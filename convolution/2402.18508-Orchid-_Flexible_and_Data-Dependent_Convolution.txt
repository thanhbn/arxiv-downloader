# 2402.18508.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2402.18508.pdf
# File size: 1003242 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Orchid: Flexible and Data-Dependent Convolution
for Sequence Modeling
Mahdi Karami, Ali Ghodsi
University of Waterloo, ON, Canada
karami1@ualberta.ca
Abstract
In the rapidly evolving field of deep learning, the demand for models that are both
expressive and computationally efficient has never been more critical. This paper
introduces Orchid, a novel architecture designed to address the quadratic com-
plexity of traditional attention mechanisms without compromising the ability to
capture long-range dependencies and in-context learning. At the core of this ar-
chitecture lies a new data-dependent global convolution layer, which contextually
adapts its kernel conditioned on input sequence using a dedicated conditioning
neural network. We design two simple conditioning networks that maintain shift
equivariance in our data-dependent convolution operation. The dynamic nature of
the proposed convolution kernel grants Orchid high expressivity while maintain-
ing quasilinear scalability for long sequences. We evaluate the proposed model
across multiple domains, including language modeling and image classification,
to highlight its performance and generality. Our experiments demonstrate that this
architecture not only outperforms traditional attention-based architectures such as
BERT and Vision Transformers with smaller model sizes, but also extends the
feasible sequence length beyond the limitations of the dense attention layers. This
achievement represents a significant step towards more efficient and scalable deep
learning models for sequence modeling.
1 Introduction
In modern deep neural networks, attention mechanisms have emerged as a gold standard, pivotal
in domains such as natural language processing, image, and audio processing, and even complex
fields like biology [Vaswani et al., 2017, Dosovitskiy et al., 2020, Dwivedi and Bresson, 2020].
However, despite their strong sequence analysis capabilities, these sequence modeling mechanisms
suffer from their high computational complexity, which scales quadratically with sequence length,
hindering their application to long-context tasks. This complexity has driven a shift towards innova-
tive solutions to overcome this computational barrier, enabling analysis of long sequences in areas
like genomics, DNA sequencing, and the creation of long musical compositions.
In the past years, researchers have explored various strategies to tackle the computational bottleneck
of traditional dense attention layers [Tay et al., 2022]. One key strategy involves sparsifying the
dense attention matrix. Instead of calculating the entire matrix, Qiu et al. [2019], Parmar et al. [2018]
focus on specific local blocks of the receptive fields of sequences by chunking them into fixed-size
blocks. Moreover, Sparse Transformer [Child et al., 2019], Longformer [Beltagy et al., 2020] and
BigBird [Zaheer et al., 2020] use strided attention patterns combined with local sliding windows to
reduce computation. In contrast to using pre-determined patterns, other techniques include learn-
ing to cluster/sort tokens based on a similarity function, thereby enhancing the global view of the
sequence, as seen in Reformer [Kitaev et al., 2020], Routing Transformer [Roy et al., 2020] Sparse
Sinkhorn attention [Tay et al., 2020]. Another approach involves low-rank approximations of the
Preprint. Under review.arXiv:2402.18508v2  [cs.LG]  24 May 2024

--- PAGE 2 ---
self-attention matrix, leveraging the insight that these matrices often exhibit low-rank properties, as
demonstrated by Linformer [Wang et al., 2020] which projects keys and values matrices to lower-
dimensional representation matrices. Another paradigm to reduce quadratic computation cost, is
to replace the dot-product similarity between keys and query matrices of attention mechanism with
akernel function and avoid explicitly computing the attention matrix [Katharopoulos et al., 2020].
Notable examples in this family include Performers [Choromanski et al., 2020], Random Feature
Attention [Peng et al., 2021] that are based on random feature approximation of the kernel function.
Additionally, some models leverage a combinations of such techniques to design an efficient trans-
former [Zhu et al., 2021, Zhang et al., 2021]. However, while these methods significantly reduce
computational overhead, they may sacrifice expressiveness and performance, often requiring hybrid
approaches that combine them with dense attention layers [Mehta et al., 2022, Fu et al., 2023]. On
the other hand, recent works have aimed at sparsifying dense linear layers, used for feature mixing
in Transformer blocks, to tackle another major source of high computation and memory demand in
large models [Dao et al., 2022, Chen et al., 2021,].
Finding sub-quadratic and hardware-efficient mixing operators that are also expressive remains a
significant challenge. Recent studies have explored attention-free solutions, particularly using state
space models (SSMs) [Gu et al., 2021, Mehta et al., 2022, Wang et al., 2022, Fu et al., 2023, Orvieto
et al., 2023, Gu and Dao, 2023, De et al., 2024], and long convolutions [Romero et al., 2021, Li
et al., 2022, Poli et al., 2023]. A state space model characterizes a dynamical system’s behavior
in terms of its internal state using a state equation, describing the dynamics of the system using
first-order differential equations over the states, and an observation equation, relating state variables
to observed outputs.1A key insight is that, most of these SSM models can be formulated as a long
convolution model between the input and output sequences [Gu et al., 2021], allowing parallel and
efficient training. However, recent work by Poli et al. [2023] demonstrated that directly parameter-
izing the filter impulse response of the long-convolution leads to an even more expressive sequence
mixing layer.
This paper proposes a novel data-dependent convolution mechanism to tackle the inherent quadratic
complexity of traditional attention mechanisms, while maintaining the model’s ability to capture
long-range dependencies and in-context learning. The data-dependent convolution layer contextu-
ally adapts its kernel based on input data using a dedicated conditioning neural network. We design
two simple yet effective conditioning networks that maintain shift equivariance in the adaptive con-
volution operation. By combining these adaptive mechanisms with gating operations, our proposed
model—named Orchid —achieves high expressivity while offering quasilinear scalability (with a
complexity of O(LlogL)) for long sequences. Evaluation across various domains, including lan-
guage modeling and image classification, presented in section 4 and Appendix, demonstrates the
Orchid architecture’s performance and generality, outperforming attention-based architectures, like
BERT and Vision Transformers, with smaller model sizes. Moreover, its allows for handling very
large sequence lengths that are beyond the limitations of the dense attention layers. This achieve-
ment lays the foundation for further advancements in more efficient and scalable sequence modeling
architectures.
2 Background
Self-Attention Mechanism: Given a length- Lsequence of embeddings (tokens) x=
(x1, x2, . . . , x L), the self-attention layer generates a new sequence by computing a weighted sum
of these embeddings. To achieve this, it linearly project xinto three components: queries ( Q),
keys (K), and values ( V), as:Q=xWQ,K=xWK,V=xWV.Each individual atten-
tion mechanism within a multi-head self-attention layer operates as a dense linear transformation,
expressed as:
y=SA(Q,K,V) =SoftMaxQKT
√dk
V=A(x)V,
where the matrix A(x)contains the normalized attention scores between each pair of tokens. This
description of the attention layer highlights its notable benefits, including its capability to capture
1Notably, state space models, in general, include the recurrent layers such as RNN and LSTM [Hochreiter
et al., 1997] as special cases.
2

--- PAGE 3 ---
Figure 2.1: Orchid block. ⊙and∗denote element-wise multiplication and the convolution operator, re-
spectively. The convolutions are implemented in the frequency domain using FFT. On the right two different
conditioning networks, introduced in equations (2) and (3) as shift-invariant convolution kernels, are depicted.
In this model, the convolution is performed efficiently in the spectral domain, so the kernel in the frequency
domain, hF=hF
0+hF
θ(x), is computed.
long-range dependencies using a sublinear parameter count. The attention mechanism enables di-
rect computation of interactions between any two positions in the input sequence, regardless of their
distance, without a corresponding rise in parameter counts. Additionally, the attention layer imple-
ments a data-dependent dense linear filter, which effectively filter the input based on on weights
conditioned by a mapping of the data. This property makes it expressive and flexible enough to en-
code a large family of linear functions. However, these advantages come at the expense of quadratic
computational complexity and memory costs.
This motivates us to develop an efficient and scalable data-dependent convolution mechanism, fea-
turing an adaptive kernel that adjusts based on the input data. The kernel size of this convolution
layer is as long as the input sequence length, enabling the capture of long-range dependencies across
the input sequence while maintaining high scalability.
Linear Convolution: Discrete-time linear convolution is a fundamental operation in digital signal
processing that calculates the output as the weighted sum of the finite-length input xwith shifted
versions of the convolution kernel, h, also known as the impulse response of a linear time-invariant
(LTI) system.2Formally, it can be written as
y[t] = (h∗x)[t]≜L−1X
ℓ=0h[t−ℓ]x[ℓ].
In this definition, the output is a linear filter of the zero-padded input and convolution kernel. How-
ever, other padding schemes leads to different forms of convolution. A well-known form is circular
convolution , defined as
y[tmodL] = (h⊛x)[t]≜L−1X
ℓ=0h[t−ℓmodL]x[ℓ],
which is equivalent to the linear convolution of two sequences if one is cyclically padded at its edges.
Global Convolution and Fast Convolution Algorithm: Standard convolution layers, explic-
itly parameterized with a short kernel, struggle to capture long-range dependencies in sequential
data. Extending the kernel to match the input length enables modeling such dependencies but
leads to linear growth in parameter counts and quadratic computational complexity. To mitigate
the parameter growth challenge, the kernel of global (a.k.a. long) convolution can be implicitly
parameterized using a multilayer perceptron (MLP), a technique that has been shown to main-
tain sub-linear parameter scaling [Karami et al., 2019, Romero et al., 2021, Li et al., 2022, Poli
2While generally xrepresents a sequence of D-dimensional embeddings with length L, all the sequence
mixing operators, including data-dependent convolutions, Fourier transforms and depthwise 1D convolutions
(Conv1d ) are performed along the sequence dimension. Therefore, for clarity and without loss of generality,
we assume D= 1andx∈RL. For a full list of notation definition, please refer to Appendix A.
3

--- PAGE 4 ---
et al., 2023]. Furthermore, a key advantage of convolution operators is that, leveraging the con-
volution theorem, convolution operators can be efficiently computed in the frequency domain us-
ing Fast Fourier Transform (FFT) algorithms, thereby reducing the computational complexity to
O(LlogL)[Cooley and Tukey, 1965]. Formally, the linear convolution can be expressed in the
frequency domain as ˆy=F−1(F(ˆh)⊙ F(ˆx)) =T−1(hF⊙Tˆx), where Tis the DFT matrix,
Fdenotes the discrete Fourier transformation, and ˆxdenotes the zero-padded signal, defined as
ˆx≜pad2L(x) = [ 0L;x]. Additionally, the circular convolution can be simply computed as:
y=h⊛x=F−1(F(h)⊙ F(x)).
3 Orchid Operator
This section introduces the Data-Dependent Convolution Filter , a novel operator aimed at increasing
the expressiveness of long convolution operations. This operator serves as the foundational building
block for the Orchid layer, which we will explore later in the section.
3.1 Data-Dependent Convolution Filter
We hypothesize that making the convolutional kernel data-dependent allows the filter to adapt to the
specific characteristics of its input, potentially capturing more complex patterns within the sequence.
Formally, this input-dependent filter is defined as:
y=hθ(x)∗x=NNθ(x)∗x (1)
The key innovation is to replace the static convolutional kernel with a conditionally generated one
controlled by the input data. This is achieved through a conditioning network , denoted as hθ(x) =
NNθ(x), a neural network parameterized by θ. The conditioning network outputs a vector matching
the input sequence in length. This allows each input token to ’attend’ to the entire sequence with
personalized, adaptive weights derived from its specific representation. Convolving the surrounding
context using this data-dependent weighting scheme can potentially offer more effective sequence
mixing compared to conventional static convolutions.
3.2 Preserving Shift-Equivariance in Data-Dependent Convolution
A fundamental property of discrete convolution is shift equivariance , meaning that shifting the input
by a certain amount leads to a corresponding shift in the output (disregarding boundary effects). This
is formally expressed for circular convolution as: shift m(y) =h⊛shift m(x)[Bronstein et al.,
2021], where this property holds exactly regardless of boundary conditions. The shift operation is
defined as shift m(x)[t]≜x[t+m].
This property is particularly important because it ensures the operator’s response is robust to shift of
features within the input, thereby enhancing the model’s generalization capabilities. This inductive
bias is at the core of the widespread success of convolution operations [Thomas et al.]. Therefore,
it is desirable to design conditioning network in the data-dependent convolution (1) to preserve shift
equivariance property. To maintain this property for data-dependent convolution operations, it is
sufficient to design filter kernel to be shift-invariant ,i.e.h(shift m(x)) =h(x)(refer to Appendix
B for the proof). In the following, we present two conditioning network designs satisfying shift-
invariance.
I) Phase Suppression for Shift Invariance: A circular shift of a sequence uresults in a lin-
ear phase shift of its frequency components: F 
shift m(u)
[ω] =uF[ω]·e−i2π
Lωm[Oppen-
heim, 1999]. Given a shift-equivariant function g(x)(such as a depthwise Conv1d() ) (satisfying:
g(shift m(x)) = shift m(g(x))). Its frequency components after a spatial shift of its input main-
tain this phase shift:
F 
g(shift m(x)
[ω] =F(g(x)) [ω]·e−i2π
Lωm.
By taking the magnitude (absolute value or squared) of these complex-valued frequency compo-
nents, we effectively eliminate the phase shift, therefore, defining hF(x) =F 
g(x)satisfies
shift-invariance property: hF(shift m(x)) =hF(x).
4

--- PAGE 5 ---
In our design, we deploy a hybrid spatial-frequency domain conditioning network. This network
consists of a 1D depthwise linear convolution ( Conv1d ())) with a short kernel length (typically 3-5)
acting at the spatial domain, followed by a short convolution in the frequency domain. By operat-
ing in both spatial and frequency domains, the conditioning network effectively mixes information
from neighboring tokens and spectral components. The resulting conditioning neural network is
formulated as:
hF
θ(x) =Conv1d F(Conv1d (x))
(2)
This architecture choice aims to minimize the number of parameters and computational overhead
introduced by the conditioning network within the overall model.
II) Leveraging Cross-Correlation for Shift-Invariance: An alternative approach to achieving
shift-invariance involves computing the cross-correlation between two mapping of the input se-
quence. Let k(x)andq(x)be two shift-equivariant functions, satisfying: k(shift m(x)) =
shift m(k(x))andq(shift m(x)) = shift m(q(x)). We define h(x)as the cross-correlation
ofk(x)andq(x), given by:
h(x)[t] = (k(x)⋆ q(x))[t]≜L−1X
ℓ=0k(x)[ℓ]·q(x)[t+ℓmodL].
This operation essentially slides q(x)overk(x)and measures their similarity at different offsets.
Remarkably, the resulting cross-correlation function, h(x), is also shift invariant:
h(shift m(x)) =k(shift m(x))⋆ q(shift m(x))
=shift m(k(x))⋆shift m(q(x))
=k(x)⋆ q(x) =h(x)
Furthermore, the convolution theorem enables efficient computation of the cross-correlation in the
frequency domain: hF(x) =F(k(x)⋆ q(x)) =kF∗(x)⊙qF(x)where kF∗denotes the complex
conjugate of kFand⊙represents element-wise multiplication.
Remark 3.1.By setting k(x) =q(x) =g(x), we obtain hF(x) =|gF(x)|2, This indicates that the
cross-correlation approach generalizes the magnitude-based approach, demonstrating its versatility.
Similar to the previous approach, we employ separate 1D depth-wise short convolutions for both
k(x)andq(x), followed by another convolution post cross-correlation in the frequency domain. As
a result, the conditioning neural network is defined as
hF
θ(x) =Conv1d
F∗(Conv1d (x))⊙σ 
F(Conv1d (x))
. (3)
Inspired by gating mechanism, we applied Sigmoid to one of this pair in the frequency domain.
Both conditioning functions, as defined in (2) and (3), are illustrated schematically in Figure 2.1.
Remark 3.2.For convolution operations, we augment the data-dependent conditioning network by
incorporating a fixed (static) term. This term adds positional encoding to the convolution kernel by
implicitly parametrizing it using a positional embedding, PosEmb (), of time step (token index in the
sequence) and a feed forward networks as h0=FFN 
PosEmb (t)
[Romero et al., 2021, Li et al.,
2022, Poli et al., 2023]. The final convolution kernel is obtained by summing this positional bias
with the output of the conditioning network, h=hθ(x) +h0.
Remark 3.3 (Data-Dependent Convolution as a Cross-attention Alternative ).The kernel of con-
volution hθ(x), defined in equations (2) or (3), is conditioned on the input of the convolution layer,
making it input-dependent. However, we can generalize this concept further. The kernel could be a
function of any arbitrary sequence u, leading to a broader definition of data-dependent convolution:
y(x,u) =hθ(u)∗x=NNθ(u)∗x
This definition couples the input sequence xwith another sequence u, creating a potential alternative
to cross-attention layers in sequence processing tasks. We therefore refer to the proposed layer as
"data-dependent" in a more general sense. When dealing with sequences of different lengths, the
shorter sequence can be zero-padded to match the length of the longer one. Specifically, assuming
x∈RLis longer than u∈RN(L > N ), we use the zero-padded sequence ˆu=padL(u)∈RL
as input to the conditioning network NN θ(ˆu). Since the long convolution is implemented in the
frequency domain, this zero-padding in the time domain translates to interpolation in the frequency
domain [Smith, 2008] ensuring that both sequences have frequency components of the same length.
5

--- PAGE 6 ---
3.3 Orchid Block
Unlike attention layers, convolution filters leverage parameter sharing. This means they slide the
same kernel weights and apply them to different positions within the input sequence. Mathemati-
cally, this operation is equivalent to multiplying an input vector with a structured matrix, such as a
Toeplitz matrix for linear convolutions or a circulant matrix for circular convolutions, which results
in computational efficiency [Gray et al., 2006, Karami et al., 2019]. To achieve a location-dependent
filtering scheme, we complement the data-dependent convolution with element-wise multiplications,
allowing the model to emphasize specific tokens within by assigning higher weights prior to apply-
ing the location-invariant convolution. Notably, prior research has demonstrated that a cascade of
circulant and diagonal matrices can effectively approximate dense linear layers [Moczulski et al.,
2015, Cheng et al., 2015]. Building upon these insights, the overall architecture of the Orchid
block, is composed of a chain of Mdata-dependent convolution and element-wise multiplications
(gated connections). In our experiments, we utilize a simple chain of order 1.5, consisting of data-
dependent convolution sandwiched by two element-wise multiplications: y= (f2
⊙◦f∗◦f1
⊙)(x)
where ◦denotes composition, f∗(x)≜(hθ(x)+h0)∗x, andfi
⊙(x)≜Conv1d (x)⊙x. The Orchid
block is illustrated in Figure 2.1 and its basic implementation is presented in appendix D.
Overall Computational complexity. All global convolutions within the Orchid block are computed
in the frequency domain using FFT algorithm, inheriting its computational efficiency with complex-
ity ofO(LlogL). Furthermore, the element-wise multiplications contribute an additional O(L)
complexity. Consequently, the overall complexity of the Orchid block scales quasi-linearly with
the sequence length, resulting in a total complexity of O(MLlogL), where Mis the number of
layers in the block. A recent hardware and I/O optimized implementation of FFT, introduced in [Fu
et al., 2023], can speed up the overall computation of Orchid on modern accelerators. Empirical
runtime comparisons against standard attention mechanisms, detailed in Appendix C.5, highlight its
expected scalability, especially for longer sequences.
4 Experiments
Our evaluation of Orchid focuses on three different Transformer-based models to evaluate its ex-
pressivity and generalization capabilities as an alternative to attention layers. Firstly, we conduct a
set of experiments on a synthetic task to assess the in-context learning ability and scalability of the
proposed model. Subsequently, we evaluate the performance of the proposed architecture on lan-
guage modeling tasks. Moreover, we extend our experiments to image classification tasks, aiming to
evaluate the model’s generalizability across diverse domains. Additional ablation studies on model
architecture and also an experiments on raw speech classification with long sequences are presented
in Appendices C.5 and C.2. For an overview experimental details, please refer to Appendix C.
4.1 Synthetic In-context Learning
The aim of the first experiment is to assess how well our model performs on a synthetic reasoning
task. This task, inspired by prior work on language model benchmarking [Liang et al., 2022] and
in-context learning (ICL) [Garg et al., 2022], is known as Associative Recall. It involves generating
a value from a key given a string of key-value tuples from a random dictionary. For instance, given
the input ([ a, 1 ,b, e ,f, 3 ],b), the model is expected to return e, the value associated with the
keyb. This task assesses whether a model can effectively retrieve the correct value from a key in
a prompt, essentially applying a data-controlled shift. Attention mechanisms offer this capability
by computing attention scores through token comparisons and then weighting the entire sequence
accordingly [Olsson et al., 2022]. Associative recall has been pivotal in guiding the design of long
convolution models, as demonstrated in [Fu et al., 2023], and a more complex variant of this task
was employed in [Poli et al., 2023].
For these experiments, we benchmark Orchid against several leading long convolution models,
including: I) H3,which utilizes state-space models (SSMs) for implicit parametrization of long
convolution, as proposed in [Fu et al., 2023]. II) CKConv , that employs feedforward networks
(FFNs) and positional embeddings for the implicit parametrization of convolution operations, de-
tailed in [Romero et al., 2021]. III) Hyena , built upon the CKConv framework by incorporating an
additional exponential decay modulation into the implicit convolution process, as detailed in [Poli
et al., 2023]. It is further augmented by a multiplication in a chain of order 2.
6

--- PAGE 7 ---
Table 4.1: This table shows the performance (test accuracy) of in-context learning on the associative recall
task with a vocabulary size of 20 and different sequence lengths. The results for the baseline models are drawn
from Poli et al. [2023], Fu et al. [2023]. The symbol ✘indicates that the Transformer model failed to complete
the task within a week or the model d oes not fit in memory.
Model 128 512 2K 8K 32K 128K
Transformer 100 100 100 100 ✘ ✘
Monarch-Mixer - 98.7 99.4 99.4 99.4 99.4
Hyena 93 99 99.6 100 100 -
Orchid 100 100 100 100 100 100
2729211213215020406080100
Sequence LengthVocabulary Size =20
Orchid
CKConv
H3
Hyena
2729211213215
Sequence LengthVocabulary Size =30
2729211213215
Sequence LengthVocabulary Size =40
Figure 4.1: Test accuracy of the associative recall task across different long implicit convolution models on
various sequence lengths and vocabulary sizes (number of possible token values).
The results, illustrated in Figure 4.1 and Table 4.1, demonstrate that Orchid model offers superior
expressiveness and outperforms existing long convolution models in the associative recall tasks. No-
tably, in challenging scenarios with short sequence lengths of 128 and large vocabulary sizes, Orchid
significantly improves the model’s accuracy and narrows the gap between Transformer and implicit
convolution models. Furthermore, when extending the sequence length to very long sequences of up
to 131K tokens, Orchid successfully learn the task while the Transformer model encounters compu-
tational difficulties.
The insights from this experiment guide us in integrating the proposed model into Transformer-
based models for extensive language modeling, suggesting its potential to enhance performance in
natural language processing tasks.
4.2 Language Modeling
We evaluate the Orchid layer in language models. Orchid is designed to integrate seamlessly with
existing BERT-style language models, such as BERT [Devlin et al., 2018], RoBERTa [Liu et al.,
2019], SpanBERT [Joshi et al., 2020], and others [Jin et al., 2020]. In our experiments, we replace
the attention layers in the standard BERT framework in [Devlin et al., 2018] with Orchid layers.
For each Transformer block in the BERT-style model, we replace the attention layers with Orchid
layers for sequence mixing. We also replace the two dense matrices in the MLP layers, used for
dimension mixing in Transformers, with block-diagonal matrices [Dao et al., 2022]. Following [Fu
et al., 2023], we add a residual long convolution to each Orchid layer.
Our BERT-style model, called Orchid-BERT-base, has 12 layers with a hidden size of 768, the same
dimension and depth as BERT-base. The resulting Orchid-BERT-base have 77M parameters, com-
pared to BERT-base’s 110M parameters. We also pretrain Orchid-BERT-large of 254M parameters
with hidden dimensions of 1536 and 12 layers. Orchid models are pre-trained using masked lan-
guage modeling over the C4 dataset [Raffel et al., 2019] with the bert-base-uncased tokenizer.
Finetuning Performance on GLUE Benchmark. We conducted an evaluation of Orchid-BERT
models on GLUE fine-tuning tasks, comparing them against the baseline models: BERT-base
and BERT-large, and the recent long convolution-based models: M2-BERT-base and M2-BERT-
large [Fu et al., 2023]. The fine-tuning process was executed in accordance with the methodology
described by Izsak et al. [2021]. As the results outlined in Table 4.2 show, Orchid-BERT-base is able
to achieve 1.0points improvement in average GLUE score performance compared to the BERT-base
on the GLUE benchmark with utilizing 30% fewer parameters. Similarly, Orchid-BERT-large out-
performs the performance of BERT-large by .6points with a 25% reduction in parameter counts.
7

--- PAGE 8 ---
Table 4.2: Average GLUE Score of BERT-base and BERT-large [Devlin et al., 2018] in comparison to Orchid-
BERT-base and Orchid-BERT-base, and M2-BERT-base and M2-BERT-large Dao et al. [2022]. Baseline results
are drawn from [Fu et al., 2023].
Model (size) GLUE Score ∆Params ∆GLUE Score
BERT-base (110M) 79.6 - -
M2-BERT-base (80M) 79.9 -27.3% +0.3
Orchid-BERT-base (77M) 80.6 -30.0% +1.0
BERT-large (340M) 82.1 - -
M2-BERT-large (260M) 82.2 -23.6% +0.1
Orchid-BERT-large (254M) 82.7 -25.3% +0.6
4.3 Image Classification
We extend the application of Orchid to the Vision Transformer (ViT) architecture, introduced by
Dosovitskiy et al. [2020], by replacing its attention mechanism with Orchid similar to language
modeling task. Similar to language modeling task, we substitute the dense matrices in the MLP lay-
ers, which perform dimension mixing, with block-diagonal matrices and incorporate a residual long
convolution within each Orchid block. We benchmark our model against recent long convolution-
based models, specifically Hyena-ViT-b [Poli et al., 2023] and M2-ViT-b [Fu et al., 2023].
Models are evaluated for image classification on two widely used image datasets: CIFAR-10 and
ImageNet-1K. For CIFAR-10, images are transformed into sequences of 4×4pixel patches and
processed using a ViT architecture composed of 6 Transformer layers with a hidden size of 128. In
the case of ImageNet-1K, we segmented images into patches of 16×16pixels, and we trained a
ViT-base architecture featuring 12 Transformer layers and a hidden size of 768.
The models’ performance on both the CIFAR-10 and ImageNet-1K datasets, outlined in Table 4.3,
demonstrate that Orchid significantly outperforms both the Vision Transformer baseline and the long
convolution-based models on both datasets. These results affirm the generalizability and effective-
ness of the Orchid architecture beyond the domain of language modeling, highlighting its potential
advantage in broader range of applications such as image processing tasks.
Table 4.3: Performance comparison of Orchid with ViT-based models on ImageNet-1k and CIFAR-10 dataset.
Baseline results are drawn from [Fu et al., 2023].
Model (size) Top-1 (%) Top-5 (%)
ImageNet-1k
ViT-b (87M) 78.5 93.6
ViT-b + Monarch (33M) 78.9 94.2
Hyena-ViT-b (88M) 78.5 93.6
M2-ViT-b (45M) 79.5 94.5
Orchid-ViT-b (48M) 80.2 94.9Model (size) Top-1 (%)
CIFAR-10
ViT (1.2M) 78.6
ViT + Monarch (607K) 79.0
Hyena-ViT (1.3M) 80.6
M2-ViT (741K) 80.8
Orchid-ViT (836K) 84.3
5 Related Work
Previous works have explored various forms of dynamic convolution architectures [Wu et al., 2019,
Karami et al., 2019, Chen et al., 2020, Jiang et al., 2020]. The dynamic convolution in [Wu et al.,
2019] utilizes a short convolution kernel that depends solely on the current time-step, whereas [Jiang
et al., 2020] expands the kernel span to depend on a local window. Meanwhile, Chen et al. [2020]
modeled the convolution kernel as a mixture of short convolution kernels with mixture weights
controlled by average pooling of the input embedding. However, the reliance on short convolutions
and their specific kernel modeling approaches limits their ability to capture long-range dependencies
and scaling their kernel to match the sequence length is computationally impractical. In a different
line of research, Fourier Neural Operator (FNO) [Li et al., 2020] and adaptive FNO [Guibas et al.,
2021] operate in the spectral domain, applying a dense linear layer or an MLP with a block diagonal
linear layer. However, these models do not explicitly model convolution kernel and do not enforce
shift-equivariance.
8

--- PAGE 9 ---
Recent advances in input-dependent state space models (SSMs) have shown promise in efficient
sequence modeling by allowing model parameters to dynamically adapt based on the input [Gu
and Dao, 2023]. However, the input-dependent mechanisms in these models typically rely on the
current token or its local neighbors, preventing them from leveraging the benefits of global con-
volution for efficient parallelizable training Consequently, they heavily depend on hardware-aware
implementations optimized for modern GPUs. While effective in certain tasks, these models have
been shown to struggle with recall-intensive scenarios [Arora et al., 2024]. In contrast, our pro-
posed data-dependent global convolution allows the conditioning network to be influenced by the
entire input context, enabling efficient sequence mixing. Moreover, the current formulation of input-
dependent SSMs is not readily adaptable for efficient cross-attention between sequence pairs. This
highlights a promising future direction for research: exploring how the complementary strengths of
data-dependent global convolutions and input-dependent SSMs can be combined to develop foun-
dation models that excel across a broader range of tasks.
Recent studies have explored sub-quadratic sequence mixing methods using long convolutions or
state space models, leveraging the fast convolution algorithm for computational efficiency. While
utilizing Fast Fourier transform results in O(LlogL)computational complexity, FFT algorithms
exhibit suboptimal hardware utilization and suffer from slow I/O between layers of the memory
hierarchy on modern GPUs due to their sequential nature. To address this bottleneck, FlashFFT-
Conv [Fu et al., 2023] utilizes a matrix decomposition to leverage matrix multiply units and enable
kernel fusion resulting in a more hardware and I/O efficient implementation of long convolutions.
Moreover, the Monarch Mixer (M2) [Fu et al., 2023], offers an expressive family of sub-quadratic
structured matrices that generalizes the DFT and other structures. These matrices are parame-
terized as products of block-diagonal matrices, offering sub-quadratic computation costs ranging
fromO(LlogL)toO(L3/2). By trading-off computational complexity with FLOP utilization, M2
achieves a hardware-efficient alternative for Transformers.
6 Discussion and Conclusion
In conclusion, our work introduces Orchid, a novel model that addresses some critical challenges of
efficiency and scalability in sequence modeling through the innovative use of data-dependent con-
volution. Orchid successfully mitigates the quadratic computational and memory costs associated
with attention layers, while retaining, and in many cases enhancing, the model performance across
different domains. The introduction of a data-dependent convolution layer represents a significant
step forward, offering a scalable and expressive alternative sequence mixing scheme that adapts its
weights to the input data. Through evaluation across multiple domains, including in-context learn-
ing, language and image processing tasks, Orchid has demonstrated not only its superior perfor-
mance over traditional attention-based models but also its generality and scalability. This positions
the Orchid model not just as a alternative to existing paradigms but as a potential catalyst for innova-
tion, driving the exploration of novel, efficient, and powerful architectures in artificial intelligence.
The superior performance of Orchid compared to traditional transformer-based models raises in-
triguing questions about the current state and future direction of deep learning architectures. One
plausible explanation for our model’s effectiveness could be the over-parameterization prevalent in
transformer-based models. A growing body of evidence indicates that attention mechanisms, despite
their computational complexity, utilize only a fraction of their capabilities for tasks like language
processing. This challenges the common belief that attention is the key ingredient for large-scale
deep learning, leading us to reconsider its role and seek more computationally efficient alternatives.
Limitations and Future Directions: Looking ahead, extending our model to accommodate causal
models, particularly for autoregressive language models akin to GPT, is an intriguing future direc-
tion. The current form of the proposed model is not inherently compatible with these architectures,
primarily due to differences in how data-dependent global convolution handle dependencies and
sequence generation.3Furthermore, exploring the capability of Orchid as an efficient alternative to
cross-attention layer, employed in sequence-to-sequence models, offers another avenue for research.
These considerations open up new possibilities for integrating our model into more advanced foun-
dation models and cross-domain applications.
3Notably, recent works such as [Ding et al., 2023, Bachmann and Nagarajan, 2024] show that auto-
regressive models are not optimal for inference in language models.
9

--- PAGE 10 ---
References
[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017.
[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 , 2020.
[3] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to
graphs. arXiv preprint arXiv:2012.09699 , 2020.
[4] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.
ACM Computing Surveys , 55(6):1–28, 2022.
[5] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise
self-attention for long document understanding. arXiv preprint arXiv:1911.02972 , 2019.
[6] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku,
and Dustin Tran. Image transformer. Proceedings of ICML 2018 , 2018.
[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.
[8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-
former. arXiv preprint arXiv:2004.05150 , 2020.
[9] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers
for longer sequences. Proceedings of NeurIPS , 2020.
[10] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In
International Conference on Learning Representations , 2020.
[11] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based
sparse attention with routing transformers. Proceedings of TACL , 2020.
[12] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention.
Proceedings of ICML , 2020.
[13] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-
attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[14] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
are rnns: Fast autoregressive transformers with linear attention. In International Conference
on Machine Learning , pages 5156–5165. PMLR, 2020.
[15] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking
attention with performers. arXiv preprint arXiv:2009.14794 , 2020.
[16] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng
Kong. Random feature attention. Proceedings of ICLR , 2021.
[17] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandku-
mar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and
vision. Advances in Neural Information Processing Systems , 34:17723–17736, 2021.
[18] Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu
Chen. Poolingformer: Long document modeling with pooling attention. Proceedings of ICML ,
2021.
[19] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language
modeling via gated state spaces. arXiv preprint arXiv:2206.13947 , 2022.
10

--- PAGE 11 ---
[20] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré.
Hungry hungry hippos: Towards language modeling with state space models. International
Conference on Learning Representations , 2023.
[21] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander
Liu, Aniruddh Rao, Atri Rudra, and Christopher Ré. Monarch: Expressive structured matrices
for efficient and accurate training. In International Conference on Machine Learning . PMLR,
2022.
[22] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher
Ré. Pixelated butterfly: Simple and efficient sparse training for neural network models. 2021.
[23] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain:
Unifying sparse and low-rank attention. In Advances in Neural Information Processing Systems
(NeurIPS) , 2021.
[24] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with struc-
tured state spaces. arXiv preprint arXiv:2111.00396 , 2021.
[25] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without
attention. arXiv preprint arXiv:2212.10544 , 2022.
[26] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan
Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In Inter-
national Conference on Machine Learning , pages 26670–26698. PMLR, 2023.
[27] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
arXiv preprint arXiv:2312.00752 , 2023.
[28] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru,
Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Grif-
fin: Mixing gated linear recurrences with local attention for efficient language models. arXiv
preprint arXiv:2402.19427 , 2024.
[29] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogen-
doorn. Ckconv: Continuous kernel convolution for sequential data. In International Confer-
ence on Learning Representations , 2021.
[30] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolu-
tional models great on long sequence modeling? arXiv preprint arXiv:2210.09298 , 2022.
[31] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional
language models. International Conference on Machine Learning , 2023.
[32] Sepp Hochreiter, Jürgen Schmidhuber, et al. Long short-term memory. Neural computation , 9
(8):1735–1780, 1997.
[33] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher
Ré. Combining recurrent, convolutional, and continuous-time models with linear state space
layers. Advances in neural information processing systems , 34:572–585, 2021.
[34] Mahdi Karami, Dale Schuurmans, Jascha Sohl-Dickstein, Laurent Dinh, and Daniel Duck-
worth. Invertible convolutional flow. Advances in Neural Information Processing Systems , 32,
2019.
[35] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex
fourier series. Mathematics of computation , 19(90):297–301, 1965.
[36] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli ˇckovi ´c. Geometric deep learn-
ing: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 , 2021.
[37] N Thomas, T Smidt, S Kearnes, L Yang, L Li, K Kohlhoff, and P Riley. Tensor field networks:
Rotation-and translation-equivariant neural networks for 3d point clouds. arxiv 2018. arXiv
preprint arXiv:1802.08219 .
11

--- PAGE 12 ---
[38] Alan V Oppenheim. Discrete-time signal processing . Pearson Education India, 1999.
[39] Julius O Smith. Mathematics of the discrete Fourier transform (DFT): with audio applications .
Julius Smith, 2008.
[40] Robert M Gray et al. Toeplitz and circulant matrices: A review. Foundations and Trends® in
Communications and Information Theory , 2(3):155–239, 2006.
[41] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. Acdc: A structured
efficient linear layer. arXiv preprint arXiv:1511.05946 , 2015.
[42] Yu Cheng, Felix X Yu, Rogerio S Feris, Sanjiv Kumar, Alok Choudhary, and Shi-Fu Chang.
An exploration of parameter redundancy in deep networks with circulant projections. In Pro-
ceedings of the IEEE International Conference on Computer Vision , pages 2857–2865, 2015.
[43] Daniel Y Fu, Hermann Kumbong, Eric Nguyen, and Christopher Ré. Flashfftconv: Efficient
convolutions for long sequences with tensor cores. arXiv preprint arXiv:2311.05908 , 2023.
[44] Daniel Y Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W Thomas,
Benjamin Spector, Michael Poli, Atri Rudra, and Christopher Ré. Monarch mixer: A simple
sub-quadratic gemm-based architecture. arXiv preprint arXiv:2310.12109 , 2023.
[45] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Ya-
sunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation
of language models. arXiv preprint arXiv:2211.09110 , 2022.
[46] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn
in-context? a case study of simple function classes. arXiv preprint arXiv:2208.01066 , 2022.
[47] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,
Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson
Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Cir-
cuits Thread , 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-
heads/index.html.
[48] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,
2018.
[49] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT
pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.
[50] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.
Spanbert: Improving pre-training by representing and predicting spans. Transactions of the
Association for Computational Linguistics , 8:64–77, 2020.
[51] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? a strong
baseline for natural language attack on text classification and entailment. In Proceedings of the
AAAI conference on artificial intelligence , volume 34, pages 8018–8025, 2020.
[52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. arXiv preprint arXiv:1910.10683 , 2019.
[53] Peter Izsak, Moshe Berchansky, and Omer Levy. How to train BERT with an academic budget.
InProceedings of the 2021 Conference on Empirical Methods in Natural Language Process-
ing, pages 10644–10652, 2021.
[54] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention
with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430 , 2019.
12

--- PAGE 13 ---
[55] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu.
Dynamic convolution: Attention over convolution kernels. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages 11030–11039, 2020.
[56] Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.
Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Infor-
mation Processing Systems , 33:12837–12848, 2020.
[57] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differ-
ential equations. arXiv preprint arXiv:2010.08895 , 2020.
[58] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan
Catanzaro. Adaptive fourier neural operators: Efficient token mixers for transformers. arXiv
preprint arXiv:2111.13587 , 2021.
[59] Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley,
James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance
the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668 , 2024.
[60] Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, and Radu Soricut. Causallm is
not optimal for in-context learning. arXiv preprint arXiv:2308.06912 , 2023.
[61] Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. arXiv
preprint arXiv:2403.06963 , 2024.
[62] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 , 2014.
[63] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bow-
man. Glue: A multi-task benchmark and analysis platform for natural language understanding.
arXiv:1804.07461 , 2018.
[64] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and
Lucas Beyer. How to train your ViT? data, augmentation, and regularization in vision trans-
formers. arXiv preprint arXiv:2106.10270 , 2021.
[65] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain ViT baselines for
imagenet-1k. arXiv preprint arXiv:2205.01580 , 2022.
[66] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay,
Jiashi Feng, and Shuicheng Yan. Tokens-to-token ViT: Training vision transformers from
scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer
vision , pages 558–567, 2021.
[67] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast
and memory-efficient exact attention with IO-awareness. In Advances in Neural Information
Processing Systems , 2022.
7 Broader Impacts
The introduction of Orchid, with its innovative data-dependent convolution mechanism, can poten-
tially lead to impacts across various sectors of society.
•Accessibility and Democratization of AI : By reducing the computational and memory
requirements traditionally associated with deep learning models, Orchid makes advanced
AI technologies more accessible to a broader range of researchers, startups, and institutions.
•Environmental Sustainability : The efficiency of Orchid, particularly in terms of compu-
tational and energy demands, aligns with the urgent need for environmentally sustainable
AI practices. Lower energy consumption contributes to reducing the carbon footprint of
training and deploying large-scale models, aligning with global sustainability goals.
13

--- PAGE 14 ---
•Advancements in Healthcare : In the healthcare sector, Orchid’s ability to efficiently pro-
cess long sequences of data could revolutionize early diagnosis and personalized medicine.
For instance, it could enable more accurate analysis of genomic sequences or continuous
health monitoring data.
14

--- PAGE 15 ---
A Notation definition
Notations Brief definition and interpretation
x,y,W x ∈RLandy∈RLare input and output sequence of a layer, while matrices are denoted by
bold uppercase letters, such as layer weight matrix W.
h∗x linear convolution: y[t] = (h∗x)[t]≜PL−1
ℓ=0h[t−ℓ]x[ℓ]
h⊛x circular convolution: y[tmodL] = (h⊛x)[t]≜PL−1
ℓ=0h[t−ℓmodL]x[ℓ]
k⋆q cross-correlation: h[t] = (k⋆q)[t]≜PL−1
ℓ=0k[ℓ]·q[t+ℓmodL]
h⊙x element-wise multiplication (Hadamard product): y[t] = (h⊙x)[t]≜h[t]·x[t]
shift m(x) shift m(x)[t]≜x[t+m]
F(),F−1(),T The forward and inverse discrete Fourier transforms while Trepresents the DFT matrix.
xFxF=F(x) =Tx
ˆx ˆx∈R2Lis a zero-padded signal at the left side defined as ˆx≜pad2L(x) = [0L;x]
padN(x) padding the vector x∈RLwith zeros to the target size N,i.e.padN(x) = [0N−L;x]where
[0N−L]: a zero vector of size (N−L).
hθ(x) conditioning network (data-dependent convolution kernel): hθ(x) =NNθ(x)that is a neural
network parameterized by θ.
Conv1d 1D depthwise linear convolution with a short kernel length (typically 3-5) applied to each
feature dimension
FC() dense fully connected layer FC(x) =xW+b
MLP() Multi-Layer Perceptron which is composed of dense fully connected layers
PosEmb () a positional embedding of time step t
h(t) positional encoding part of the convolution kernel: h(t) =FFN 
PosEmb (t))
Model name: The name “ Orchid ” for our model is more than just a label; it carries a symbolic
meaning for our model, reflecting its elegance, resilience, and adaptability. Orchids are known
to thrive in diverse environments and exhibit subtle color variations under specific environmental
conditions, including light intensity, seasonal changes, and dyeing. The essence of adaptation and
efficient resource utilization resonates profoundly with our model’s design. Moreover, the proposed
model’s computational efficiency aligns with more environmentally sustainable AI practices by min-
imizing energy consumption and carbon footprint during training and deployment.
B Proof
Proof for Preserving Shift Equivariance: Given a circular shift invariant filter kernel, where
hθ(x[t+mmodL]) =hθ(x[t]).
For a circularly shifted input, the circular convolution can be expressed as:
(hθ(shift m(x))⊛shift m(x))[t] =L−1X
ℓ=0hθ(shift m(x)[ℓ])·shift m(x)[t−ℓmodL]
=L−1X
ℓ=0hθ(x[ℓ+m])·shift m(x)[t+m−ℓmodL]
=L−1X
ℓ=0hθ(x[ℓ])·shift m(x)[t+m−ℓmodL]
=y[t+mmodL]
=shift m(y)[t]
Therefore, if the filter kernel is shift-invariant, the conditional convolution will also be shift-
equivariant. ■
15

--- PAGE 16 ---
C Experimental Details
C.1 Synthetic In-context Learning
We used associative recall task to evaluate in-context learning of the proposed model. Given a
string of key-value tuples from a random dictionary, this task involves generating a value from a
key . For instance, given the input ([ a,1,b,e,3,f],b), the model is expected to return e, the
value associated with the key b. This task assesses whether a model can effectively retrieve the
correct value from a key in a prompt, essentially applying a data-controlled shift. While attention
mechanisms offer this capability by computing pair-wise attention scores and then weighting the
entire sequence accordingly [47], they face computational difficulties for very large sequence.
The model is composed of 2 Orchid block while each Orchid is composed of a chain of order 1.5,
including 2 element-wise multiplications and one data-dependent convolution. The hidden size of
all models in this experiments are set to 64. The conditioning network, defined in (2), was used
which was composed of a 1D depthwise convolution ( Conv1d ) in the time domain and a Conv1d in
the frequency domain that were applied separately to each feature dimension, both having a kernel
of length 3. Only for vocabulary size of 40 with short sequence lengths of 128 we increased the
depth to 3 layers of Conv1d in the spatial domain and in the frequency domain.
For training, we used Adam optimizer [62] with its standard settings ( β1=.9, β2=.999), and
learning rate of 5e-4with linear warmup schedule in 1000 steps. A weight decay of 0.1was used as
a regularizer. The Orchid models were trained Orchid on a single P100 GPU for small to medium
sequence length and on a single V100 GPU for long sequences.
C.2 Model Architecture Ablation
To better understand the impact of different model components, we conducted several ablation stud-
ies on in-context learning in section 4.1. First, we compared the two conditioning networks for
data-dependent convolution introduced in Equations 2 and 3. Second, we evaluated two different
Fourier transforms: Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT), both
with orthogonal basis and also orthonormal, which use orthogonal and normalized basis so that their
transform matrices Tbecome unitary. The results, presented in Figure 4.1, show that condition-
ing networks of type I (Equation 2) offers faster convergence. Moreover, the DCT outperforms the
DFT in terms of learning speed, likely because DCT corresponds to even-symmetric padding, which
smooths the signal at the boundaries compared to the circular padding of the DFT. Furthermore, the
DCT produces real-valued transforms. Notably, using orthonormal transforms accelerates the learn-
ing process. Therefore, in all the experiments, we opted for the orthonormal DCT with conditioning
networks of type I for data-dependent convolution.
Figure C.1: Test accuracy of in-context learning on the associative recall task with a vocabulary size of 20 and
sequence length of 128, comparing different model components. Absrefers to conditioning networks of type
I (based on absolute value in Equation 2), while Cross-Corr denotes conditioning networks of type II (based
on cross-correlation in Equation 3). Orthonormal indicates transforms that utilize orthogonal and normalized
bases.
16

--- PAGE 17 ---
Table C.1: A summary of the Fine-tuning results on GLUE benchmark [63]. Following the the methodology
outlined in [53], we use the standard evaluation metrics: Spearman’s correlation for STS-B, Matthew’s corre-
lation for CoLA, F1 scores for QQP and MRPC, and accuracy for the other tasks. For MNLI, the average of
multiclass accuracy (m) both multiclass accuracy of mismatched set (mm) is used.
Model MNLI (m / mm) RTE QNLI QQP SST2 STS-B CoLA MRPC Average
M2-BERT-base (80M) 78.4 / 78.6 68.5 84.6 86.7 92.0 86.3 53.0 89.8 79.9
Orchid-BERT-base (77M) 79.53 / 80.52 70.4 86.16 87.2 92.05 86.63 51.76 90.53 80.6
M2-BERT-large (260M) 81.7 / 81.9 72.8 84.7 87.8 93.3 88.0 59.2 90.0 82.2
Orchid-BERT-large (254M) 81.82 / 82.74 76.32 87.02 88.18 92.8 88.29 56.97 89.24 82.65
C.3 Language Modeling
In our experiments, we replaced the attention layers in the standard BERT framework in [48] with
Orchid layers. For each Transformer block in the BERT-style model, we replaced the attention layers
with Orchid layers for sequence mixing. Each Orchid layer was composed of a chain of order 1.5,
including 2 element-wise multiplications and one data-dependent convolution. The conditioning
network, defined in (2), was used which was composed of a short Conv1d in time domain and
aConv1d in the frequency domain that were applied separately to each feature dimension, both
having a kernel of length 3. For dimension mixing, we also substituted the two dense matrices in the
MLP layers with block-diagonal matrices of order 1 with b= 4blocks [21] and also add a residual
long convolution to each Orchid layer, as in [44].
Our BERT-style model, called Orchid-BERT-base, was composed of 12 layers with a hidden size
of 768, the same dimension and depth as BERT-base resulting in Orchid-BERT-base having 77M
parameters, compared to BERT-base’s 110M parameters. We also pre-trained Orchid-BERT-large
of 254M parameters with hidden dimensions of 1536 and 12 layers.
For pre-training, we used decoupled Adam optimizer with β1=.9, β2=.98, and learning rate
of 8e-4 with linear warmup schedule within first 6% of the steps and then decay with linear rate.
A weight decay of 1e-5 is used as a regularizer. Orchid models were pre-trained using masked
language modeling with 30% masking over the C4 dataset [52] with sequence length of 128 and the
bert-base-uncased tokenizer. Models were pre-trained on a node of 4xA100 GPUs for 70k steps
with batch size of 4096.
Finetuning on GLUE Benchmark: To evaluate the performance of Orchid-BERT models, we
finetuned the pre-trained models on GLUE fine-tuning tasks, comparing them against the base-
line models: BERT-base and BERT-large, and the recent long convolution-based models: M2-
BERT-base and M2-BERT-large [44]. The fine-tuning process was executed in accordance with
the methodology described by Izsak et al. [53].
For all tasks, the sequence length was 128. For some of the tasks, we applied average pooling on the
embeddings of all the non-padding tokens and use it as the model output. We followed [53] in fine-
tuning small dataset tasks: RTE, MRPC, and STS-B are initialized from the fine-tuned checkpoint
of MNLI dataset. Models were fine-tuned on a node of 4xA100 GPUs.
In contrast to [44], which performed a hyperparameter search for learning rate, weight decay, and
number of epochs, we used the reported hyper-parameters in [44] and didn’t perform a thorough
search for them. As noted in [53], hyperparameter search can result in substantial improvements in
the performance. In particular for CoLA task, whose results are bellow M2-BERT, we anticipate
that fine tuning the parameter will improve its performance. The detailed hyperparameters for each
task is reported in Table C.2.
C.4 Image Classification
We extended the application of Orchid to the Vision Transformer (ViT) architecture, introduced
by Dosovitskiy et al. [2], by replacing its attention mechanism with Orchid similar to language
modeling task. The sinusoidal positional embeddings and global average-pooling were employed
for the class token, following [64, 65]. We compared the performance of our model against recent
long convolution-based models, specifically Hyena-ViT-b [31] and M2-ViT-b [44]. We evaluated
the models on two widely used image classification datasets: CIFAR-10 and ImageNet-1K.
17

--- PAGE 18 ---
Table C.2: Hyperparameters for Orchid-bert-base (77M) and Orchid-BERT-large (254M) on Various GLUE
Tasks. D-AdamW stands for Decoupled AdamW optimization algorithm. For QNLI task in Orchid-bert-base
(77M) we applied everage pooling.
Hyperparameter MNLI RTE QNLI QQP SST2 STSB CoLA MRPC
Orchid-bert-base (77M)
Optimizer D-AdamW AdamW AdamW AdamW D-AdamW AdamW D-AdamW AdamW
Learning Rate 5e-5 5e-5 5e-5 3e-5 3e-5 7e-5 5e-5 5e-5
Weight Decay 5e-6 0.01 1e-5 0.01 3e-6 0.01 5e-6 0.01
Epochs 3 6 10 10 3 10 10 10
Orchid-bert-large (254M)
Optimizer D-AdamW D-AdamW D-AdamW D-AdamW D-AdamW D-AdamW AdamW D-AdamW
Learning Rate 5e-5 1e-5 5e-5 3e-5 3e-5 7e-5 5e-5 8e-5
Weight Decay 5e-6 1e-6 1e-5 3e-6 3e-6 3e-6 5e-6 8e-6
Epochs 3 3 10 5 3 10 10 10
Each Orchid layer consists of a chain of order 1.5, involving 2 element-wise multiplications and one
data-dependent convolution. The conditioning network (defined in Eq. 2) employs a combination of
Conv1d layers in both the time and frequency domains, applied separately to each feature dimension
with a kernel length of 3 and 5 for CIFAR-10 and ImageNet-1K, respectively.
CIFAR-10: For CIFAR-10, images were transformed into sequences of 4×4pixel patches and
processed using a ViT architecture composed of 6 Transformer layers with a hidden size of 128. For
training, we used Adam optimizer with its standard setting ( β1=.9, β2=.999), and learning rate
of 3e-4 with linear warmup schedule within first 500 steps. A weight decay of 0.05 was used as a
regularizer. Orchid was trained on a single P100 GPU for 500 epochs with batch size of 512.
ImageNet-1K: We segmented images into patches of 16×16pixels, and we trained a ViT-base
architecture featuring 12 Transformer layers and a hidden size of 768. We incorporated a residual
long convolution within each Orchid layer and substituted the dense matrices in the MLP layers
responsible for feature mixing with block-diagonal matrices of order 1 with b= 4blocks, reducing
their computational complexity and the total model’s size.
For training, we used Adam optimizer with its standard setting ( β1=.9, β2=.999), and base
learning rate of 1e-3 with linear warmup schedule within first 10 epochs and then decay with Co-
sine schedule. For data augmentation, we adopted the T2T-ViT pipeline [66], including Random
erasing with rate=0.25, CutMix with α=1.0, Mixup with α=0.8, AugMix, and RandAugment with
[magnitude=9, magnitude-std=0.5, layers=2]. We trained Orchid on 4xA100 GPUs for 300 epochs
and batch size of 1024.
Table C.3 summarizes architecture and training settings.
Table C.3: Architecture and training settings for Orchid-ViT models.
ImageNet-1k CIFAR-10
Number of Orchid blocks 12 6
Hidden dimension 768 128
Short Conv1d kernel size 5 3
PosEmb dimension 33 5
Patch size 16×16 4 ×4
Batch size 1024 512
Training epochs 300 500
Weight decay 0.05 0.05
Learning rate schedule Cosine decay with linear warmup Constant with linear warmup
Base learning rate 1e-3 3e-4
Warmup 10 epochs 500 steps
Label smoothing 0.1 0.1
18

--- PAGE 19 ---
Table C.4: Accuracy on the Speech Commands dataset with 10 classes. S4-M2 refers to the S4 model [24]
where dense matrices in the MLP layers are replaced with block-diagonal matrices. Baseline results are sourced
from [44], and ✘indicates that the Transformer model could not fit in GPU memory.
Transformer Performer CKConv WaveGan-D S4 S4-M2 Orchid
✘ 30.8 71.7 96.3 97.5 97.9 97.7
C.5 Speech Classification
To evaluate the ability to learn long-range dependencies in real-world time-series data, we con-
ducted experiments on the speech classification task using the SC10 subset of the Speech Commands
dataset, which contains 10 classes. Following [29], we use the raw speech dataset with 1-second du-
ration sampled at 16000 Hz for this task. The performance results shown in Table C.4 highlight that
Orchid performs on par with the state-of-the-art model on this long sequence modeling task.
C.6 Runtime Benchmark
To assess the computational efficiency of Orchid, we benchmarked its runtime against both tradi-
tional attention layers and the optimized FlashAttention mechanism [67].The evaluation was con-
ducted on an NVIDIA A100-40GB GPU with models featuring a hidden dimension of 768 and a
batch size of 1. The empirical runtime comparisons in Figure C.2 highlights Orchid’s high scal-
ability, especially for longer sequences. Here, FlashAttention utilizes float16 precision, while
Attention and Orchid use float32 . It’s worth noting that Orchid’s convolution operators utilize the
standard FFT implementation in PyTorch, and further speed improvements are anticipated with the
integration of fused CUDA kernels or FlashFFT [43].
211212213214215216217
·1050100200
Sequence Length (L)Runtime (ms)Runtime Comparison (Forward)
FlashAttention
Attention
Orchid
211212213214215216217
·1050200400600
Sequence Length (L)Runtime (ms)Runtime Comparison (Back-propagation)
MHA
Torch MHA
Orchid
Figure C.2: Forward and backward runtime comparison of different attention mechanisms (FlashAttention,
Attention, and Orchid) with varying sequence lengths.
19

--- PAGE 20 ---
D Implementation
1import torch
2import torch .nn as nn
3from torch_dct import dct , idct
4from einops import rearrange
5
6class DiscreteTransform (nn. Module ):
7 def __init__ (self , mode =" dct", norm =" ortho ", dim =1):
8 super (). __init__ ()
9 self . dim = dim ; self . mode = mode ; self . norm = norm
10
11 def forward (self , x):
12 if self . mode == " dct":
13 return dct(x, dim= self .dim , n=x. shape [ self .dim ])
14 # return rearrange ( dct ( rearrange (x ,"b l d -> b d l"), norm = self . norm ), "b d l -> b l d")
15 elif self . mode == " fft":
16 return torch .fft . rfft (x, dim= self .dim)
17
18 def inverse (self , x):
19 if self . mode == " dct":
20 return idct (x, dim= self .dim , n=x. shape [ self .dim ])
21 elif self . mode == " fft":
22 return torch .fft . irfft (x, dim= self .dim)
23
24
25class OrchidOperator (nn. Module ):
26 def __init__ (self , d, L, d_filter =64 , l_conv1d =3, dxt_mode ="fft", to_out_proj = True ):
27 super (). __init__ ()
28
29 self . d_model = d
30 self . seq_len = L
31
32 # setup projections
33 self . in_linear = nn. Linear (d, 3*d)
34 if to_out_proj :
35 self . out_linear = nn. Linear (d, d)
36
37 # setup short conv filter
38 width = d * 2
39 self . short_filter = nn. Conv1d (width , width ,
40 kernel_size = l_conv1d , groups =width , padding = l_conv1d -1)
41
42 # setup static conv
43 """ The implementation of static conv in Hyena ( inspired by CKConv ) was used .
44 A basic implementation is:
45 nn. Sequential ( PositionalEmbedding ( emb_dim , l),
46 nn. Linear ( emb_dim , d_filter ), Sin (), nn. Linear ( d_filter , d_filter ),
47 ExponentialModulation ()) """
48 self . static_conv = CKConv (d, order = d_filter , seq_len =L)
49
50 # Setup conditioning network . See shift invariant model : I
51 self . conditioning_nn = nn. Sequential (
52 nn. conv1d (d, d, l_conv1d , d, padding = l_conv1d -1) ,
53 DiscreteTransform ( mode = dxt_mode , dim =1) ,
54 nn. abs (),
55 nn. conv1d (d, d, l_conv1d , d, padding = l_conv1d -1) ,
56 )
57
58 self . transform = DiscreteTransform ( mode = dxt_mode , dim = -1)
59
60 def forward (self , x, ** kwargs ): # x is (b, l, d)
61 x = self . in_linear (x)
62 _, _, v = torch . split (x, self . d_model , dim = -1)
63 h_adapt_f = self . conv_kernel_nn (v) # Input - dependent Kernel
20

--- PAGE 21 ---
64 h_0 = self . static_conv . filter ( self . seq_len )
65 h_0 = rearrange (h_0 , "c l d -> c d l")
66
67 # short conv1d () filter
68 x = rearrange (x, "b l d -> b d l")
69 x = self . short_filter (x)[... , : self . seq_len ]
70
71 s1 , s2 , v = x. split ( self . d_model , dim =1)
72
73 y = v * s1
74 y = self . adaptive_conv (y, h_0=h_0 , h_adapt_f = h_adapt_f )
75 y = y * s2
76 y = rearrange (y,"b d l -> b l d")
77 if self . to_out_proj :
78 y = self . out_linear (y)
79 return y
80
81 def adaptive_conv (self , x, h_0 , h_adapt_f ): # x is (b, d, l)
82 h_0_f = self . transform (h_0 )
83 x_f = self . transform (x)
84 y = torch . fft. irfft (x_f * ( h_0_f + h_adapt_f ))
85 return y[... , : self . seqlen ]
Listing 1: A basic implementation of the Orchid layer.
21
