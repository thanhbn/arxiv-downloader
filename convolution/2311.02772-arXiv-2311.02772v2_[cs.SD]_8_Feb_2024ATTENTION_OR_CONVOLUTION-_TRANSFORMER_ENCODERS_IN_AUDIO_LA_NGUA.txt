# 2311.02772.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/convolution/2311.02772.pdf
# File size: 152302 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2311.02772v2  [cs.SD]  8 Feb 2024ATTENTION OR CONVOLUTION: TRANSFORMER ENCODERS IN AUDIO LA NGUAGE
MODELS FOR INFERENCE EFFICIENCY
Sungho Jeon1∗, Ching-Feng Yeh2, Hakan Inan2, Wei-Ning Hsu2, Rashi Rungta2, Yashar Mehdad2, Daniel Bikel2
1Heidelberg Institute of Theoretical Studies
2Meta
ABSTRACT
In this paper, we show that a simple audio language model
can achieve comparable inference efﬁciency to more compli-
cated pre-trained models with speech transformer encoders .
These speech transformers rely on mixing convolutional
modules with self-attention modules. They achieve state-o f-
the-art performance on ASR with top efﬁciency. We ﬁrst
show that employing these speech transformers as an encoder
signiﬁcantly improves the efﬁciency of audio language mod-
els as well. However, our study shows that we can achieve
comparable efﬁciency with advanced self-attention solely .
We demonstrate that this simpler approach is particularly
beneﬁcial with a low-bit weight quantization technique of a
neural network to improve efﬁciency. We hypothesize that it
prevents propagating the errors between different quantiz ed
modules compared to recent speech transformers mixing
quantized convolution and the quantized self-attention mo d-
ules. Our study suggests that we could pay attention to the
architecture of audio language language to improve their
inference efﬁciency.
Index Terms —self-supervised pre-training, audio repre-
sentation learning, efﬁcient audio language models
1. INTRODUCTION
Self-supervised audio language models exploit unlabeled
data to learn audio representations, agnostic to a speciﬁc t ask.
These representations are used for the target task by ﬁne-
tuning instead of supervised learning with massive amounts
of labeled data. This paradigm of pre-training and then ﬁne-
tuning alleviates the dependency on abundant labeled data,
and it lets us deploy our AI systems for diverse problems more
easily. Following this paradigm, pre-training frameworks ex-
ploit rather complicated architectures [1]. These framewo rks
bring larger parameter counts with the concomitant higher
computational cost of inference.
However, there is a large gap between this high computa-
tional inference cost and the requirements for deploying th ese
models for on-device problems such as automatic speech
*This work has done while Sungho Jeon was interning at Metarecognition (ASR) for wearable devices. To narrow this gap,
recent work mostly investigates different conﬁgurations f or
the components in the architecture, such as efﬁcient conﬁg-
urations for feature extractors [2] or subsampling of input
sequences [3]. Another line of research investigates the ef -
ﬁcient transformer itself designed for the target audio tas k
solely. Gulati et al. [4] introduce a transformer mixing a
convolutional and a self-attention module. Their supervis ed
model performs better with a smaller model size on a stan-
dard ASR dataset than pre-trained audio language models.
Inspired by this transformer, a more efﬁcient model was
introduced [5]. These modern speech transformers beneﬁt
from using convolutional modules in conjunction with self-
attention modules.
Interestingly, this approach is different from recent ef-
ﬁcient model architectures studied in other areas of AI. In
the area of Natural Language Processing (NLP), an efﬁcient
transformer has been studied mainly by introducing more ef-
ﬁcient components of a vanilla transformer without mixing
convolutional modules [6]. In the area of Computer Vision,
Dosovitskiy et al. [7] focus on the training setup for the tas ks
of image recognition. They treat image patches in the same
way as textual items are treated in NLP. This work shows that
a simple transformer can achieve comparable performance
with the state-of-the-art models mixing convolutional mod -
ules with self-attention modules. More recently, a similar ar-
tifact is shown on the speech tasks as well [8]. However, this
study focuses on the perspective of performance rather than
the perspective of efﬁciency.
In this work, we investigate the inference efﬁciency
trade-offs of the transformer encoder, employed in the self -
supervised audio language models. We ﬁrst show that we can
improve the inference efﬁciency of audio language models
by employing modern speech transformers —Conformer and
Squeezeformer— as their encoder. It improves their infer-
ence efﬁciency signiﬁcantly, with comparable performance
at lower cost. However, our study shows that we can achieve
comparable efﬁciency with only efﬁcient self-attention, w ith-
out mixing convolution modules.
Our evaluation shows that this approach is particularly
beneﬁcial when we apply a quantization technique to improve
efﬁciency. This approach with quantization reduces 93.4% o f

--- PAGE 2 ---
CNN Feature Extractor CNN Feature ExtractorTransformer EncoderAcoustic Unit Discovery System
(e.g., K-means on MFCC)
Input: Raw Waveform AudioPredict the pesudo labels 
of masked tokens
Encoder Layer1Encoder Layern
Encoder Layer2Encoder 1-1:  Conformer Encoder 1-2:  Squeezeformer 
Feed-FowardMulti-Head Self-AttentionConvolutionFeed-Foward
Multi-Head Self-AttentionConvolutionFeed-Foward
Feed-Foward
Multi-Head Self-AttentionFeed-FowardEncoder 2: Sparseformer Block Spar e s lf a t n ioorz1 z2 z3 z4 zn
x1 x2 MASK xn MASKHuBERT Framework
Fig. 1 . HuBERT framework and two types of encoder candidates: Conf ormer or Squeezeformer vs. Sparseformer.
the storage size, more than 90% of the computational cost but
degrades the performance on the 10 downstream tasks such
as increasing word error rate from 6.89% to 19.33% in ASR,
compared to the original pre-trained model without quantiz a-
tion. We hypothesize that a simple transformer prevents pro p-
agating errors between different quantized modules compar ed
to modern speech transformers that mix modules of differing
types.
2. RELATED WORK
One line of related research investigates the efﬁciency of a u-
dio language models from the perspective of pre-processing
audio data. Wu et al. [2] investigate the architecture varia tions
of Wav2Vec 2.0 framework, a self-supervised pre-training
framework [9], to examine the inference efﬁciency trade-of fs.
They propose several techniques to improve the efﬁciency
of Wav2Vec 2.0 framework. For example, they introduce
more efﬁcient conﬁgurations for the feature extractor and
downsampling input sequences linearly before their Trans-
former encoder. Following this work, Vyas et al. [3] propose
a stochastic approach to sub-sampling input sequences. Nev -
ertheless, there is little attention on the inﬂuence of thei r
Transformer encoder in terms of efﬁciency.
Since earlier studies of audio language models are based
on a vanilla Transformer, recent studies investigate the in -
ﬂuence of a more advanced Transformer encoder. Zhang et
al. [10] employ Conformer into Wav2Vec 2.0, and this ap-
proach improves the performance on audio downstream tasks.
Instead of deploying Conformer, Chen et al. [11] propose a
masked speech denoising and a pre-training framework which
employs a Transformer encoder with relative positional en-
coding. Their pre-trained model outperforms a HuBERT
model. However, previous work mostly focuses on the per-
spective of performance but there has been little interest o n
the efﬁciency on this line of research.
Another line of studying the efﬁciency of neural networks
is quantizing the components of neural networks [12]. Earli erstudies replace all full-precision weights of a neural netw ork
with lower-precision weights. This approach drastically r e-
duces the memory size and inference time. However, quan-
tizing the weights of a whole network can cause the propaga-
tion of errors between modules, and the accumulated errors
degrade the performance signiﬁcantly in the end. To allevi-
ate this, diverse techniques have been introduced includin g a
partial quantization of weights. More recently, a binarize d
Transformer is proposed, which employs a learnable scal-
ing method to the lower bits [13]. Yet et al. [14] show that
pre-trained audio models can beneﬁt from this work as well.
Following this work, we investigate the inference efﬁcienc y
trade-offs with a quantization of neural network weights.
3. MODEL ARCHITECTURE
3.1. Self-Supervised Audio Pre-Training: HuBERT
Our study is based on HuBERT [15] for a framework of
self-supervised audio pre-training (Figure 1). This frame -
work consists of three components: a feature extractor, a
transformer encoder, and an acoustic unit discovery mod-
ule. Following the Wav2Vec 2.0 architecture, a convolution al
waveform component is employed for a feature extractor
which takes raw waveform inputs. It projects the input to
vector representations. The transformer encoder consists of
multiple blocks, and it processes the input representation s.
The acoustic unit discovery module produces the pseudo-
labels of input audio frames by clustering features, such
as clustering MFCC features via k-means. Inspired by the
BERT pre-training, non-masked audio representations are
learned to describe the masked tokens well by predicting
their pseudo-labels.
3.2. Encoder Candidate 1: Conformer / Squeezeformer
Conformer, the convolution-augmented transformer, consi sts
of stacked layers of convolutional modules in conjunction

--- PAGE 3 ---
Encoder in HuBERT Encoder Params Prec Storage FLOP BOP NonQ BOP BQ NVDA EstTime ASR↓ SD↓
(+FastConv) L / D / H (MB) (Gs) (Gs) (Gs) (e-04, second) (WER, %) (DER, %)
Vanilla Trans 12 / 786 / 12 FP32 184.42 110.54 1228.64 - 38.46 7.06 6.32
Conformer-S 16 / 144 / 4 FP32 131.87 22.10 329.39 - 10.31 8.56 6.81
Squeeze-XS 16 / 144 / 4 FP32 132.04 18.31 272.88 - 8.54 8.96 9.18
Sparseformer-DN-S 16 / 256 / 4 FP32 60.81 26.05 388.18 - 12.15 8.44 7.66
Sparseformer-SW-S 8 / 512 / 4 FP32 117.18 40.09 597.43 - 18.70 7.88 6.56
BQ-Vanilla Trans 12 / 786 / 12 FP32-W1A1 25.23 11.82 172.83 63.62 5.53 16.83 7.62
BQ-Conformer-S 16 / 144 / 4 FP32-W1A1 12.88 7.23 103.44 15.94 3.27 20.52 11.11
BQ-Squeeze-XS 16 / 144 / 4 FP32-W1A1 13.05 7.20 104.05 11.86 3.28 24.10 13.42
BQ-Sparseformer-DN-S 16 / 256 / 4 FP32-W1A1 12.10 7.35 107.91 10.70 3.40 19.33 9.64
BQ-Sparseformer-SW-S 8 / 512 / 4 FP32-W1A1 19.71 8.85 130.38 19.49 4.12 18.15 8.39
Table 1 . Proﬁling Results (L: Layer Num, D: Dim, H: Head Num; BQ: BiT Q uantization W1A1; NVDA EstTime: Estimated
time for their BOP based on the catalog of NVidia A100)
with self-attention modules. It achieves comparable perfo r-
mance on ASR with a smaller model size compared to a
vanilla transformer. Conformer is originally designed for
ASR, but it has been used widely for an efﬁcient audio trans-
former in other speech tasks as well.
Kim et al. [5] redesign the Conformer architecture based
on their empirical study, with a new architecture they call
Squeezeformer. They investigate two aspects, at the mi-
cro level and at the macro level. For the macro level, they
introduce subsampling of input audio sequences. For the
micro-level, they introduce several modiﬁcations includi ng
re-ordering the modules in the transformer, changing activ a-
tion functions, and reducing the number of layer normaliza-
tion modules.
3.3. Encoder Candidate 2: Sparseformer
Local window attention has been studied to deal with long
input sequences. The full-attention matrix is sparsiﬁed by at-
tention patterns, which scales linearly for the input seque nce
length. Following this, Sparseformer achieves similar per -
formance with a vanilla transformer with signiﬁcantly fewe r
operations [6]. The key idea of Sparseformer is to subdivide a
full-attention computation into several sub-computation s ﬁrst,
which applies a ﬁxed-attention pattern as hyper parameters .
Then these sub-computed outputs are used to approximate the
full-attention.
3.4. Neural Quantization: Robustly Binarized Trans-
former
Liu et al. [13] propose the robustly binarized transformer
(BiT), which is a fully binarized transformer. They introdu ce
a two-set binarization scheme and an elastic binarization
function which learns the mapping range of quantization in
the training. We employ this quantization technique to in-
vestigate the inﬂuence of different transformer encoders w ith
quantization. While Liu et al. [13] focus on quantizing a
transformer and linear/activation layers, we implement th eir
quantization techniques for the convolutional layers as we ll
to quantize Conformer and Squeezeformer. Yeh et al. [14]investigate the inﬂuence of different target bits for quant iz-
ing the HuBERT-base model with a vanilla transformer. We
only investigate the extreme bit of quantization, both 1 bit for
weights and activation (W1A1).
4. EXPERIMENTS
4.1. Experimental Setup
Pre-training setup. We follow the pre-training setup of Hu-
BERT. This pre-training is based on the Librispeech dataset ,
consisting of 960 hours. We use 32 GPUs of NVidia A100
with a batch size of at most 36.5 seconds of audio per GPU.
All models are trained for 250k steps in the ﬁrst phase, then
they are trained for 600k steps in the second phase. It takes
8.5 hours for 100k steps on our setup.
Evaluation Setup. We evaluate models in terms of compu-
tational cost and performance on downstream tasks. We ﬁrst
proﬁle the computational cost of models using the DeepSpeed
library [16]. We examine a required storage (Storage), a num -
ber of ﬂoating point operations (FLOP), a number of bit oper-
ations (BOP) [17], and an estimated time for their BOP based
on a catalog of NVidia A100 GPU.
We evaluate models on the 10 downstream tasks of SU-
PERB [18]: automatic speech recognition (ASR), keyword
spotting (KS), slot ﬁlling (SF), speaker identiﬁcation (SI D),
phoneme recognition (PR), query by example (QbE), intent
classiﬁcation (IC), automatic speaker veriﬁcation (ASV),
speaker diarization (SD) and emotion recognition (ER).
Model conﬁgurations. We employ the setup of baselines’
smallest model, Conforemr-S and Squeezeformer-XS, re-
spectively [4, 5] (Table 1). Following their shape of deep-
narrow architecture, we design Sparseformer-DN-S which
requires smaller computational cost than others in the quan -
tized models
4.2. Efﬁciency: Model Proﬁling and Downstream Tasks
Inference efﬁciency trade-offs. We ﬁrst compare the com-
putational cost of different encoders without quantizatio n

--- PAGE 4 ---
Encoder in HuBERTPrecSUPERB Tasks
(+FastConv) ASR↓ KS↑ SF↑ SID↑ PR↓ QbE↑ IC↑ ASV↓ SD↓ ER↑
Vanilla Trans (Reported) FP16 7.06 96.62 0.89 53.67 6.05 6.91 97.28 5.30 6.32 65.00
Vanilla Trans (Our Setup) FP32 6.89 96.56 0.89 53.52 5.82 6.70 97.86 5.96 6.63 63.83
Conformer-S FP32 8.56 95.94 0.87 52.11 9.42 6.20 92.88 5.80 6.81 62.12
Squeeze-XS FP32 8.96 96.13 0.80 35.35 10.73 6.14 83.31 7.86 9.18 58.75
Sparseformer-DN-S FP32 8.44 95.89 0.88 60.57 7.99 6.27 96.02 6.24 7.66 62.26
Sparseformer-SW-S FP32 7.88 93.25 0.88 61.86 9.72 4.92 93.15 6.98 6.56 65.90
BQ-Vanilla Trans (Reported) FP16-W1A1 15.96 93.83 0.78 49.62 22.96 5.63 93.01 6.83 7.62 61.68
BQ-Vanilla Trans (Our Setup) FP32-W1A1 16.83 94.77 0.79 40.15 20.63 5.57 89.77 9.13 7.80 60.74
BQ-Conformer-S FP32-W1A1 20.53 92.44 0.76 24.98 37.13 5.18 69.73 9.86 11.11 56.41
BQ-Squeeze-XS FP32-W1A1 24.10 92.79 0.69 18.56 28.82 5.08 62.01 11.97 13.42 57.57
BQ-Sparseformer-DN-S FP32-W1A1 19.33 92.24 0.79 29.21 33.17 4.66 71.34 10.70 9.64 58.97
BQ-Sparseformer-SW-S FP32-W1A1 18.15 94.03 0.79 38.92 24.37 6.09 84.37 9.19 8.39 61.39
Table 2 . Evaluation on SUPERB Tasks (BQ: BiT Neural Quantization, W 1A1: 1 bit for Weights and Activation, Reported:
Reported in [14], Our Setup: Lower batch size (0.5M tokens <1.2M tokens))
(Table 1). A model employing Conformer-S shows lower
cost than the baseline, 64% for the required storage and 74%
in FLOP reductions compared to the HuBERT with vanilla
Transformer. Since Squeezeformer has the same fundamental
architecture with Conformer, it shows similar proﬁling re-
sults. Sparseformer-DN-S also shows comparable reduction s
for their computational cost.
Next, we evaluate these models on the 10 speech down-
stream tasks of SUPERB (Table 2). We observe the efﬁciency
trade-offs for employing more efﬁcient transformer on down -
stream tasks. For example, it increases word error rate from
6.89 to 8.44 in ASR.
Efﬁciency with quantization. When we apply 1-bit BiT
quantization, our results show that these two types of en-
coders have different inﬂuences. The quantized model
employing Spareformer (BQ-Sparseformer-DN-S) shows
better performance than the quantized model employing
Conformer-S (BQ-Conformer-S) overall. It shows a lower
word error rate on ASR (19.33 <20.52) and a lower diariza-
tion error rate on SD (9.64 <11.11). Despite of the fact that
BQ-Sparseformer-DN-S takes the smallest computational
cost compared to the cost of BQ-Conformer-S: the 7.7%
smaller required storage and the 32.9% smaller BOP BQ.
We hypothesize that the quantized modules of different type s
in these speech transformers propagate errors. Then, the
accumulated errors degrade performance more than a sim-
ple transformer encoder, consisting of self-attention mod ules
only.
Compared to the baseline without quantization, employ-
ing Sparseformer with BiT quantization reduces 93.4% re-
quired storage (184.42 →12.10), 93.4% of FLOP (110.54
→7.35 ), and 90.3% of BOP (1228.64 →118.61). We es-
timate 91.1% runtime reduction in the theoretical maximum
performance of an NVidia A100 GPU. In return, it increases
the word error rate from 6.89% to 19.33%, and other tasks
as well. Compared to the baseline with BiT quantization, it
saves 52.1% of required storage (25.23 →12.10), 37.8% of
FLOP (11.82 →7.35), 50% of BOP (236.45 →118.61). As
efﬁciency trade-offs, it increases word error rate from 16. 83%to 19.33%, and overall.
4.3. Architecture Shape: Deep-Narrow vs. Shallow-Wide
Ashihara et al. [19] show that two different shapes of ar-
chitectures have different advantages as speech tasks when
they investigate this issue with knowledge distillation. I n-
spired by this, we design a shallow-wide shape of Sparse-
former (Sparseformer-SW-S). It has half of the number of
layers but twice larger dimensions compared to the setup of
Sparseformer-DN-S. This model shows better performance,
but this shape of architecture brings higher computational
costs. Our proﬁler shows that the wide shape of the ar-
chitecture causes a larger storage size due to the absolute
positional encoding, employed in the framework. Each layer
also requires more matrix multiplication operations due to
the self-attention mechanism. Hence, it has disadvantages in
computational cost to design more efﬁcient models.
5. CONCLUSIONS
We investigate the efﬁciency trade-offs of employing diffe r-
ent transformer encoders into the self-supervised framewo rk
of audio pre-training. Our experiments show that there are
decent efﬁciency trade-offs when we employ them. When
we apply a quantization technique, however, our results sug -
gest that a simple transformer encoder employing only efﬁ-
cient self-attention modules is more beneﬁcial than the rec ent
speech transformers blending modules of differing types.
6. REFERENCES
[1] Steffen Schneider, Alexei Baevski, Ronan Collobert,
and Michael Auli, “wav2vec: Unsupervised pre-training
for speech recognition,” Proc. Interspeech , pp. 3465–
3469, 2019.
[2] Felix Wu, Kwangyoun Kim, Jing Pan, Kyu J Han, Kil-
ian Q Weinberger, and Yoav Artzi, “Performance-
efﬁciency trade-offs in unsupervised pre-training for

--- PAGE 5 ---
speech recognition,” in 2022 IEEE International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2022, pp. 7667–7671.
[3] Apoorv Vyas, Wei-Ning Hsu, Michael Auli, and Alexei
Baevski, “On-demand compute reduction with stochas-
tic wav2vec 2.0,” Proc. Interspeech , 2022.
[4] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki
Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,
Zhengdong Zhang, Yonghui Wu, et al., “Conformer:
Convolution-augmented transformer for speech recog-
nition,” Proc. Interspeech , pp. 5036–5040, 2020.
[5] Sehoon Kim, Amir Gholami, Albert Eaton Shaw,
Nicholas Lee, Karttikeya Mangalam, Jitendra Malik,
Michael W Mahoney, and Kurt Keutzer, “Squeeze-
former: An efﬁcient transformer for automatic speech
recognition,” in Advances in Neural Information Pro-
cessing Systems , 2022.
[6] Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever, “Generating long sequences with sparse
transformers,” arXiv preprint arXiv:1904.10509 , 2019.
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer,
Georg Heigold, Sylvain Gelly, et al., “An image is
worth 16x16 words: Transformers for image recogni-
tion at scale,” in International Conference on Learning
Representations , 2021.
[8] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James
Glass, “Ssast: Self-supervised audio spectrogram trans-
former,” in Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence , 2022, vol. 36, pp. 10699–10709.
[9] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli, “wav2vec 2.0: A framework for
self-supervised learning of speech representations,” Ad-
vances in neural information processing systems , vol.
33, pp. 12449–12460, 2020.
[10] Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-
Cheng Chiu, Ruoming Pang, Quoc V . Le, and Yonghui
Wu, “Pushing the limits of semi-supervised learning for
automatic speech recognition,” in NeuRIPS Workshop
on Self-Supervised Learning for Speech and Audio Pro-
cessing , 2020.
[11] Sanyuan Chen, Chengyi Wang, Zhengyang Chen,
Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki
Kanda, Takuya Yoshioka, Xiong Xiao, et al., “WavLM:
Large-scale self-supervised pre-training for full stack
speech processing,” IEEE Journal of Selected Topics in
Signal Processing , vol. 16, no. 6, pp. 1505–1518, 2022.[12] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
El-Yaniv, and Yoshua Bengio, “Binarized neural net-
works,” Advances in neural information processing sys-
tems, vol. 29, 2016.
[13] Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott
Yih, Meng Li, Raghuraman Krishnamoorthi, and Yashar
Mehdad, “Bit: Robustly binarized multi-distilled trans-
former,” in Advances in Neural Information Processing
Systems , 2022.
[14] Ching-Feng Yeh, Wei-Ning Hsu, Paden Tomasello, and
Abdelrahman Mohamed, “Efﬁcient speech representa-
tion learning with low-bit quantization,” arXiv preprint
arXiv:2301.00652 , 2022.
[15] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrah-
man Mohamed, “Hubert: Self-supervised speech rep-
resentation learning by masked prediction of hidden
units,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing , vol. 29, pp. 3451–3460, 2021.
[16] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He, “Deepspeed: System optimizations
enable training deep learning models with over 100 bil-
lion parameters,” in Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Dis-
covery & Data Mining , 2020, pp. 3505–3506.
[17] Mart Van Baalen, Christos Louizos, Markus Nagel,
Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and
Max Welling, “Bayesian bits: Unifying quantization
and pruning,” Advances in neural information process-
ing systems , vol. 33, pp. 5741–5752, 2020.
[18] Shu wen Yang, Po-Han Chi, Yung-Sung Chuang,
Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y . Lin, Andy T.
Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-
Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong
Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji
Watanabe, Abdelrahman Mohamed, and Hung yi Lee,
“SUPERB: Speech Processing Universal PERformance
Benchmark,” in Proc. Interspeech , 2021, pp. 1194–
1198.
[19] Takanori Ashihara, Takafumi Moriya, Kohei Matsuura,
and Tomohiro Tanaka, “Deep versus wide: An analy-
sis of student architectures for task-agnostic knowledge
distillation of self-supervised speech models,” pp. 411–
415, 2022.
