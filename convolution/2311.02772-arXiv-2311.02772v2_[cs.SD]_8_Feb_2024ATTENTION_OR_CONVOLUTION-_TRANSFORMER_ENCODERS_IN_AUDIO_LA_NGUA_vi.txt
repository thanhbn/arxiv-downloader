# 2311.02772.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/convolution/2311.02772.pdf
# Kích thước tệp: 152302 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2311.02772v2 [cs.SD] 8 Tháng 2 2024CHÚ Ý HAY TÍCH CHẬP: CÁC BỘ MÃ HÓA TRANSFORMER TRONG CÁC MÔ HÌNH NGÔN NGỮ ÂM THANH CHO HIỆU QUẢ SUY LUẬN

Sungho Jeon1∗, Ching-Feng Yeh2, Hakan Inan2, Wei-Ning Hsu2, Rashi Rungta2, Yashar Mehdad2, Daniel Bikel2
1Viện Nghiên cứu Lý thuyết Heidelberg
2Meta

TÓM TẮT
Trong bài báo này, chúng tôi chỉ ra rằng một mô hình ngôn ngữ âm thanh đơn giản có thể đạt được hiệu quả suy luận tương đương với các mô hình đã được đào tạo trước phức tạp hơn với các bộ mã hóa transformer tiếng nói. Các transformer tiếng nói này dựa vào việc kết hợp các mô-đun tích chập với các mô-đun tự chú ý. Chúng đạt được hiệu suất tiên tiến trên ASR với hiệu quả hàng đầu. Chúng tôi đầu tiên chỉ ra rằng việc sử dụng các transformer tiếng nói này như một bộ mã hóa cũng cải thiện đáng kể hiệu quả của các mô hình ngôn ngữ âm thanh. Tuy nhiên, nghiên cứu của chúng tôi chỉ ra rằng chúng ta có thể đạt được hiệu quả tương đương chỉ với tự chú ý tiên tiến. Chúng tôi chứng minh rằng phương pháp đơn giản hơn này đặc biệt có lợi với kỹ thuật lượng tử hóa trọng số bit thấp của mạng nơ-ron để cải thiện hiệu quả. Chúng tôi giả thuyết rằng nó ngăn chặn việc truyền bá lỗi giữa các mô-đun lượng tử hóa khác nhau so với các transformer tiếng nói gần đây kết hợp các mô-đun tích chập được lượng tử hóa và các mô-đun tự chú ý được lượng tử hóa. Nghiên cứu của chúng tôi gợi ý rằng chúng ta có thể chú ý đến kiến trúc của ngôn ngữ âm thanh để cải thiện hiệu quả suy luận của chúng.

Từ khóa chỉ mục —đào tạo trước tự giám sát, học biểu diễn âm thanh, mô hình ngôn ngữ âm thanh hiệu quả

1. GIỚI THIỆU
Các mô hình ngôn ngữ âm thanh tự giám sát khai thác dữ liệu không nhãn để học biểu diễn âm thanh, bất khả tri đối với một nhiệm vụ cụ thể. Các biểu diễn này được sử dụng cho nhiệm vụ mục tiêu bằng cách tinh chỉnh thay vì học có giám sát với lượng lớn dữ liệu có nhãn. Mô hình đào tạo trước này sau đó tinh chỉnh làm giảm sự phụ thuộc vào dữ liệu có nhãn phong phú, và nó cho phép chúng ta triển khai các hệ thống AI của mình cho các vấn đề đa dạng một cách dễ dàng hơn. Theo mô hình này, các khung đào tạo trước khai thác các kiến trúc khá phức tạp [1]. Các khung này mang lại số lượng tham số lớn hơn với chi phí tính toán cao hơn tương ứng của suy luận.

Tuy nhiên, có một khoảng cách lớn giữa chi phí suy luận tính toán cao này và các yêu cầu để triển khai các mô hình này cho các vấn đề trên thiết bị như nhận dạng tiếng nói tự động (ASR) cho các thiết bị đeo được. Để thu hẹp khoảng cách này, công việc gần đây chủ yếu điều tra các cấu hình khác nhau cho các thành phần trong kiến trúc, chẳng hạn như các cấu hình hiệu quả cho trích xuất đặc trưng [2] hoặc lấy mẫu con của các chuỗi đầu vào [3]. Một hướng nghiên cứu khác điều tra chính transformer hiệu quả được thiết kế cho nhiệm vụ âm thanh mục tiêu. Gulati et al. [4] giới thiệu một transformer kết hợp mô-đun tích chập và mô-đun tự chú ý. Mô hình được giám sát của họ hoạt động tốt hơn với kích thước mô hình nhỏ hơn trên tập dữ liệu ASR tiêu chuẩn so với các mô hình ngôn ngữ âm thanh đã được đào tạo trước. Lấy cảm hứng từ transformer này, một mô hình hiệu quả hơn đã được giới thiệu [5]. Các transformer tiếng nói hiện đại này hưởng lợi từ việc sử dụng các mô-đun tích chập kết hợp với các mô-đun tự chú ý.

Thật thú vị, phương pháp này khác với các kiến trúc mô hình hiệu quả gần đây được nghiên cứu trong các lĩnh vực khác của AI. Trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP), một transformer hiệu quả đã được nghiên cứu chủ yếu bằng cách giới thiệu các thành phần hiệu quả hơn của transformer vani mà không kết hợp các mô-đun tích chập [6]. Trong lĩnh vực Thị giác Máy tính, Dosovitskiy et al. [7] tập trung vào thiết lập đào tạo cho các nhiệm vụ nhận dạng hình ảnh. Họ xử lý các mảnh hình ảnh theo cách tương tự như các mục văn bản được xử lý trong NLP. Công việc này chỉ ra rằng một transformer đơn giản có thể đạt được hiệu suất tương đương với các mô hình tiên tiến kết hợp các mô-đun tích chập với các mô-đun tự chú ý. Gần đây hơn, một hiện tượng tương tự cũng được chỉ ra trên các nhiệm vụ tiếng nói [8]. Tuy nhiên, nghiên cứu này tập trung vào góc độ hiệu suất hơn là góc độ hiệu quả.

Trong công việc này, chúng tôi điều tra sự đánh đổi hiệu quả suy luận của bộ mã hóa transformer, được sử dụng trong các mô hình ngôn ngữ âm thanh tự giám sát. Chúng tôi đầu tiên chỉ ra rằng chúng ta có thể cải thiện hiệu quả suy luận của các mô hình ngôn ngữ âm thanh bằng cách sử dụng các transformer tiếng nói hiện đại —Conformer và Squeezeformer— như bộ mã hóa của chúng. Nó cải thiện hiệu quả suy luận của chúng đáng kể, với hiệu suất tương đương ở chi phí thấp hơn. Tuy nhiên, nghiên cứu của chúng tôi chỉ ra rằng chúng ta có thể đạt được hiệu quả tương đương chỉ với tự chú ý hiệu quả, mà không kết hợp các mô-đun tích chập.

Đánh giá của chúng tôi chỉ ra rằng phương pháp này đặc biệt có lợi khi chúng ta áp dụng kỹ thuật lượng tử hóa để cải thiện hiệu quả. Phương pháp này với lượng tử hóa giảm 93,4% kích thước lưu trữ, hơn 90% chi phí tính toán nhưng làm giảm hiệu suất trên 10 nhiệm vụ downstream như tăng tỷ lệ lỗi từ từ 6,89% lên 19,33% trong ASR, so với mô hình đã được đào tạo trước gốc mà không có lượng tử hóa. Chúng tôi giả thuyết rằng một transformer đơn giản ngăn chặn việc truyền bá lỗi giữa các mô-đun lượng tử hóa khác nhau so với các transformer tiếng nói hiện đại kết hợp các mô-đun của các loại khác nhau.

2. CÔNG VIỆC LIÊN QUAN
Một hướng nghiên cứu liên quan điều tra hiệu quả của các mô hình ngôn ngữ âm thanh từ góc độ tiền xử lý dữ liệu âm thanh. Wu et al. [2] điều tra các biến thể kiến trúc của khung Wav2Vec 2.0, một khung đào tạo trước tự giám sát [9], để kiểm tra sự đánh đổi hiệu quả suy luận. Họ đề xuất một số kỹ thuật để cải thiện hiệu quả của khung Wav2Vec 2.0. Ví dụ, họ giới thiệu các cấu hình hiệu quả hơn cho trích xuất đặc trưng và lấy mẫu giảm các chuỗi đầu vào tuyến tính trước bộ mã hóa Transformer của chúng. Theo công việc này, Vyas et al. [3] đề xuất một phương pháp ngẫu nhiên để lấy mẫu con các chuỗi đầu vào. Tuy nhiên, có ít sự chú ý đến ảnh hưởng của bộ mã hóa Transformer của chúng về mặt hiệu quả.

Vì các nghiên cứu sớm hơn về các mô hình ngôn ngữ âm thanh dựa trên Transformer vani, các nghiên cứu gần đây điều tra ảnh hưởng của bộ mã hóa Transformer tiên tiến hơn. Zhang et al. [10] sử dụng Conformer vào Wav2Vec 2.0, và phương pháp này cải thiện hiệu suất trên các nhiệm vụ âm thanh downstream. Thay vì triển khai Conformer, Chen et al. [11] đề xuất một khử nhiễu tiếng nói có mặt nạ và một khung đào tạo trước sử dụng bộ mã hóa Transformer với mã hóa vị trí tương đối. Mô hình đã được đào tạo trước của họ vượt trội hơn mô hình HuBERT. Tuy nhiên, công việc trước đó chủ yếu tập trung vào góc độ hiệu suất nhưng có ít quan tâm đến hiệu quả trên hướng nghiên cứu này.

Một hướng nghiên cứu khác về hiệu quả của mạng nơ-ron là lượng tử hóa các thành phần của mạng nơ-ron [12]. Các nghiên cứu sớm hơn thay thế tất cả các trọng số độ chính xác đầy đủ của mạng nơ-ron bằng các trọng số độ chính xác thấp hơn. Phương pháp này làm giảm drastically kích thước bộ nhớ và thời gian suy luận. Tuy nhiên, lượng tử hóa các trọng số của toàn bộ mạng có thể gây ra sự truyền bá lỗi giữa các mô-đun, và các lỗi tích lũy làm giảm hiệu suất đáng kể cuối cùng. Để giảm thiểu điều này, các kỹ thuật đa dạng đã được giới thiệu bao gồm lượng tử hóa một phần các trọng số. Gần đây hơn, một Transformer nhị phân được đề xuất, sử dụng phương pháp chia tỷ lệ có thể học được cho các bit thấp hơn [13]. Yet et al. [14] chỉ ra rằng các mô hình âm thanh đã được đào tạo trước cũng có thể hưởng lợi từ công việc này. Theo công việc này, chúng tôi điều tra sự đánh đổi hiệu quả suy luận với lượng tử hóa các trọng số mạng nơ-ron.

3. KIẾN TRÚC MÔ HÌNH

3.1. Đào tạo trước Âm thanh Tự giám sát: HuBERT
Nghiên cứu của chúng tôi dựa trên HuBERT [15] cho một khung đào tạo trước âm thanh tự giám sát (Hình 1). Khung này bao gồm ba thành phần: một trích xuất đặc trưng, một bộ mã hóa transformer, và một mô-đun khám phá đơn vị âm thanh. Theo kiến trúc Wav2Vec 2.0, một thành phần sóng tích chập được sử dụng cho trích xuất đặc trưng nhận đầu vào sóng thô. Nó chiếu đầu vào thành biểu diễn vector. Bộ mã hóa transformer bao gồm nhiều khối, và nó xử lý các biểu diễn đầu vào. Mô-đun khám phá đơn vị âm thanh tạo ra các nhãn giả của khung âm thanh đầu vào bằng cách phân cụm các đặc trưng, chẳng hạn như phân cụm các đặc trưng MFCC thông qua k-means. Lấy cảm hứng từ đào tạo trước BERT, các biểu diễn âm thanh không bị che được học để mô tả tốt các token bị che bằng cách dự đoán các nhãn giả của chúng.

3.2. Ứng cử viên Bộ mã hóa 1: Conformer / Squeezeformer
Conformer, transformer được tăng cường tích chập, bao gồm các lớp xếp chồng của các mô-đun tích chập kết hợp với các mô-đun tự chú ý. Nó đạt được hiệu suất tương đương trên ASR với kích thước mô hình nhỏ hơn so với transformer vani. Conformer ban đầu được thiết kế cho ASR, nhưng nó đã được sử dụng rộng rãi cho transformer âm thanh hiệu quả trong các nhiệm vụ tiếng nói khác.

Kim et al. [5] thiết kế lại kiến trúc Conformer dựa trên nghiên cứu thực nghiệm của họ, với một kiến trúc mới mà họ gọi là Squeezeformer. Họ điều tra hai khía cạnh, ở cấp độ vi mô và ở cấp độ vĩ mô. Đối với cấp độ vĩ mô, họ giới thiệu lấy mẫu con của các chuỗi âm thanh đầu vào. Đối với cấp độ vi mô, họ giới thiệu một số sửa đổi bao gồm sắp xếp lại các mô-đun trong transformer, thay đổi các hàm kích hoạt, và giảm số lượng các mô-đun chuẩn hóa lớp.

3.3. Ứng cử viên Bộ mã hóa 2: Sparseformer
Chú ý cửa sổ cục bộ đã được nghiên cứu để xử lý các chuỗi đầu vào dài. Ma trận chú ý đầy đủ được làm thưa bởi các mẫu chú ý, điều này chia tỷ lệ tuyến tính cho độ dài chuỗi đầu vào. Theo điều này, Sparseformer đạt được hiệu suất tương tự với transformer vani với số lượng phép toán ít hơn đáng kể [6]. Ý tưởng chính của Sparseformer là chia nhỏ một tính toán chú ý đầy đủ thành nhiều tính toán con trước, áp dụng một mẫu chú ý cố định như các siêu tham số. Sau đó, các đầu ra tính toán con này được sử dụng để xấp xỉ chú ý đầy đủ.

3.4. Lượng tử hóa Nơ-ron: Transformer Nhị phân Mạnh mẽ
Liu et al. [13] đề xuất transformer nhị phân mạnh mẽ (BiT), là một transformer hoàn toàn nhị phân. Họ giới thiệu một lược đồ nhị phân hai tập và một hàm nhị phân đàn hồi học ánh xạ phạm vi lượng tử hóa trong đào tạo. Chúng tôi sử dụng kỹ thuật lượng tử hóa này để điều tra ảnh hưởng của các bộ mã hóa transformer khác nhau với lượng tử hóa. Trong khi Liu et al. [13] tập trung vào lượng tử hóa transformer và các lớp tuyến tính/kích hoạt, chúng tôi triển khai các kỹ thuật lượng tử hóa của họ cho các lớp tích chập để lượng tử hóa Conformer và Squeezeformer. Yeh et al. [14] điều tra ảnh hưởng của các bit mục tiêu khác nhau để lượng tử hóa mô hình HuBERT-base với transformer vani. Chúng tôi chỉ điều tra bit cực của lượng tử hóa, cả 1 bit cho trọng số và kích hoạt (W1A1).

4. THỰC NGHIỆM

4.1. Thiết lập Thực nghiệm
Thiết lập đào tạo trước. Chúng tôi theo thiết lập đào tạo trước của HuBERT. Đào tạo trước này dựa trên tập dữ liệu Librispeech, bao gồm 960 giờ. Chúng tôi sử dụng 32 GPU NVidia A100 với kích thước batch tối đa 36,5 giây âm thanh mỗi GPU. Tất cả các mô hình được đào tạo trong 250k bước trong giai đoạn đầu tiên, sau đó chúng được đào tạo trong 600k bước trong giai đoạn thứ hai. Mất 8,5 giờ cho 100k bước trên thiết lập của chúng tôi.

Thiết lập Đánh giá. Chúng tôi đánh giá các mô hình về mặt chi phí tính toán và hiệu suất trên các nhiệm vụ downstream. Chúng tôi đầu tiên lập hồ sơ chi phí tính toán của các mô hình sử dụng thư viện DeepSpeed [16]. Chúng tôi kiểm tra lưu trữ cần thiết (Storage), số lượng phép toán dấu phẩy động (FLOP), số lượng phép toán bit (BOP) [17], và thời gian ước tính cho BOP của chúng dựa trên catalog của GPU NVidia A100.

Chúng tôi đánh giá các mô hình trên 10 nhiệm vụ downstream của SUPERB [18]: nhận dạng tiếng nói tự động (ASR), phát hiện từ khóa (KS), điền slot (SF), nhận dạng người nói (SID), nhận dạng âm vị (PR), truy vấn theo ví dụ (QbE), phân loại ý định (IC), xác minh người nói tự động (ASV), phân tách người nói (SD) và nhận dạng cảm xúc (ER).

Cấu hình mô hình. Chúng tôi sử dụng thiết lập của mô hình nhỏ nhất baseline, Conformer-S và Squeezeformer-XS, tương ứng [4, 5] (Bảng 1). Theo hình dạng kiến trúc sâu-hẹp của họ, chúng tôi thiết kế Sparseformer-DN-S yêu cầu chi phí tính toán nhỏ hơn so với các mô hình khác trong các mô hình lượng tử hóa.

4.2. Hiệu quả: Lập hồ sơ Mô hình và Nhiệm vụ Downstream
Sự đánh đổi hiệu quả suy luận. Chúng tôi đầu tiên so sánh chi phí tính toán của các bộ mã hóa khác nhau mà không có lượng tử hóa (Bảng 1). Một mô hình sử dụng Conformer-S cho thấy chi phí thấp hơn so với baseline, giảm 64% cho lưu trữ cần thiết và 74% FLOP so với HuBERT với Vanilla Transformer. Vì Squeezeformer có cùng kiến trúc cơ bản với Conformer, nó cho thấy kết quả lập hồ sơ tương tự. Sparseformer-DN-S cũng cho thấy sự giảm tương đương cho chi phí tính toán của chúng.

Tiếp theo, chúng tôi đánh giá các mô hình này trên 10 nhiệm vụ tiếng nói downstream của SUPERB (Bảng 2). Chúng tôi quan sát sự đánh đổi hiệu quả để sử dụng transformer hiệu quả hơn trên các nhiệm vụ downstream. Ví dụ, nó tăng tỷ lệ lỗi từ từ 6,89 lên 8,44 trong ASR.

Hiệu quả với lượng tử hóa. Khi chúng tôi áp dụng lượng tử hóa BiT 1-bit, kết quả của chúng tôi chỉ ra rằng hai loại bộ mã hóa này có ảnh hưởng khác nhau. Mô hình lượng tử hóa sử dụng Spareformer (BQ-Sparseformer-DN-S) cho thấy hiệu suất tốt hơn so với mô hình lượng tử hóa sử dụng Conformer-S (BQ-Conformer-S) tổng thể. Nó cho thấy tỷ lệ lỗi từ thấp hơn trên ASR (19,33 <20,52) và tỷ lệ lỗi phân tách thấp hơn trên SD (9,64 <11,11). Mặc dù thực tế là BQ-Sparseformer-DN-S mất chi phí tính toán nhỏ nhất so với chi phí của BQ-Conformer-S: lưu trữ cần thiết nhỏ hơn 7,7% và BOP BQ nhỏ hơn 32,9%.

Chúng tôi giả thuyết rằng các mô-đun lượng tử hóa của các loại khác nhau trong các transformer tiếng nói này truyền bá lỗi. Sau đó, các lỗi tích lũy làm giảm hiệu suất nhiều hơn một bộ mã hóa transformer đơn giản, chỉ bao gồm các mô-đun tự chú ý.

So với baseline mà không có lượng tử hóa, việc sử dụng Sparseformer với lượng tử hóa BiT giảm 93,4% lưu trữ cần thiết (184,42 →12,10), 93,4% FLOP (110,54 →7,35), và 90,3% BOP (1228,64 →118,61). Chúng tôi ước tính giảm 91,1% thời gian chạy trong hiệu suất tối đa lý thuyết của GPU NVidia A100. Đổi lại, nó tăng tỷ lệ lỗi từ từ 6,89% lên 19,33%, và các nhiệm vụ khác cũng vậy. So với baseline với lượng tử hóa BiT, nó tiết kiệm 52,1% lưu trữ cần thiết (25,23 →12,10), 37,8% FLOP (11,82 →7,35), 50% BOP (236,45 →118,61). Như sự đánh đổi hiệu quả, nó tăng tỷ lệ lỗi từ từ 16,83% lên 19,33%, và tổng thể.

4.3. Hình dạng Kiến trúc: Sâu-Hẹp vs. Nông-Rộng
Ashihara et al. [19] chỉ ra rằng hai hình dạng kiến trúc khác nhau có những lợi thế khác nhau như các nhiệm vụ tiếng nói khi họ điều tra vấn đề này với chưng cất kiến thức. Lấy cảm hứng từ điều này, chúng tôi thiết kế một hình dạng nông-rộng của Sparseformer (Sparseformer-SW-S). Nó có một nửa số lượng lớp nhưng kích thước lớn gấp đôi so với thiết lập của Sparseformer-DN-S. Mô hình này cho thấy hiệu suất tốt hơn, nhưng hình dạng kiến trúc này mang lại chi phí tính toán cao hơn. Hồ sơ của chúng tôi cho thấy rằng hình dạng rộng của kiến trúc gây ra kích thước lưu trữ lớn hơn do mã hóa vị trí tuyệt đối, được sử dụng trong khung. Mỗi lớp cũng yêu cầu nhiều phép toán nhân ma trận hơn do cơ chế tự chú ý. Do đó, nó có nhược điểm về chi phí tính toán để thiết kế các mô hình hiệu quả hơn.

5. KẾT LUẬN
Chúng tôi điều tra sự đánh đổi hiệu quả của việc sử dụng các bộ mã hóa transformer khác nhau vào khung tự giám sát của đào tạo trước âm thanh. Các thí nghiệm của chúng tôi cho thấy rằng có những sự đánh đổi hiệu quả khá tốt khi chúng tôi sử dụng chúng. Tuy nhiên, khi chúng tôi áp dụng kỹ thuật lượng tử hóa, kết quả của chúng tôi gợi ý rằng một bộ mã hóa transformer đơn giản chỉ sử dụng các mô-đun tự chú ý hiệu quả có lợi hơn so với các transformer tiếng nói gần đây pha trộn các mô-đun của các loại khác nhau.

6. TÀI LIỆU THAM KHẢO
[1] Steffen Schneider, Alexei Baevski, Ronan Collobert, và Michael Auli, "wav2vec: Unsupervised pre-training for speech recognition," Proc. Interspeech, pp. 3465–3469, 2019.

[2] Felix Wu, Kwangyoun Kim, Jing Pan, Kyu J Han, Kilian Q Weinberger, và Yoav Artzi, "Performance-efficiency trade-offs in unsupervised pre-training for speech recognition," trong 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7667–7671.

[3] Apoorv Vyas, Wei-Ning Hsu, Michael Auli, và Alexei Baevski, "On-demand compute reduction with stochastic wav2vec 2.0," Proc. Interspeech, 2022.

[4] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al., "Conformer: Convolution-augmented transformer for speech recognition," Proc. Interspeech, pp. 5036–5040, 2020.

[5] Sehoon Kim, Amir Gholami, Albert Eaton Shaw, Nicholas Lee, Karttikeya Mangalam, Jitendra Malik, Michael W Mahoney, và Kurt Keutzer, "Squeezeformer: An efficient transformer for automatic speech recognition," trong Advances in Neural Information Processing Systems, 2022.

[6] Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever, "Generating long sequences with sparse transformers," arXiv preprint arXiv:1904.10509, 2019.

[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al., "An image is worth 16x16 words: Transformers for image recognition at scale," trong International Conference on Learning Representations, 2021.

[8] Yuan Gong, Cheng-I Lai, Yu-An Chung, và James Glass, "Ssast: Self-supervised audio spectrogram transformer," trong Proceedings of the AAAI Conference on Artificial Intelligence, 2022, vol. 36, pp. 10699–10709.

[9] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, và Michael Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representations," Advances in neural information processing systems, vol. 33, pp. 12449–12460, 2020.

[10] Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, và Yonghui Wu, "Pushing the limits of semi-supervised learning for automatic speech recognition," trong NeuRIPS Workshop on Self-Supervised Learning for Speech and Audio Processing, 2020.

[11] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al., "WavLM: Large-scale self-supervised pre-training for full stack speech processing," IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022.

[12] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, và Yoshua Bengio, "Binarized neural networks," Advances in neural information processing systems, vol. 29, 2016.

[13] Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, và Yashar Mehdad, "Bit: Robustly binarized multi-distilled transformer," trong Advances in Neural Information Processing Systems, 2022.

[14] Ching-Feng Yeh, Wei-Ning Hsu, Paden Tomasello, và Abdelrahman Mohamed, "Efficient speech representation learning with low-bit quantization," arXiv preprint arXiv:2301.00652, 2022.

[15] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, và Abdelrahman Mohamed, "Hubert: Self-supervised speech representation learning by masked prediction of hidden units," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021.

[16] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He, "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters," trong Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–3506.

[17] Mart Van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, và Max Welling, "Bayesian bits: Unifying quantization and pruning," Advances in neural information processing systems, vol. 33, pp. 5741–5752, 2020.

[18] Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Kotik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, và Hung yi Lee, "SUPERB: Speech Processing Universal PERformance Benchmark," trong Proc. Interspeech, 2021, pp. 1194–1198.

[19] Takanori Ashihara, Takafumi Moriya, Kohei Matsuura, và Tomohiro Tanaka, "Deep versus wide: An analysis of student architectures for task-agnostic knowledge distillation of self-supervised speech models," pp. 411–415, 2022.
