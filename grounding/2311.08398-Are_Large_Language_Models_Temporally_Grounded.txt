# 2311.08398.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/grounding/2311.08398.pdf
# File size: 541747 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Are Large Language Models Temporally Grounded?
1Yifu Qiu1Zheng Zhao1Yftah Ziser
2Anna Korhonen1Edoardo M. Ponti1Shay B. Cohen
1Institute for Language, Cognition and Computation, University of Edinburgh
2Language Technology Lab, University of Cambridge
{yifu.qiu,zheng.zhao,yftah.ziser,eponti,scohen}@ed.ac.uk
Abstract
Are Large language models (LLMs) temporally
grounded? Since LLMs cannot perceive and
interact with the environment, it is impossi-
ble to answer this question directly. Instead,
we provide LLMs with textual narratives and
probe them with respect to their common-sense
knowledge of the structure and duration of
events, their ability to order events along a time-
line, and self-consistency within their temporal
model (e.g., temporal relations such as after
andbefore are mutually exclusive for any pair
of events). We evaluate state-of-the-art LLMs
(such as LLaMA 2 and GPT-4) on three tasks
reflecting these abilities. Generally, we find
that LLMs lag significantly behind both human
performance as well as small-scale, specialised
LMs. In-context learning, instruction tuning,
and chain-of-thought prompting reduce this gap
only to a limited degree. Crucially, LLMs strug-
gle the most with self-consistency, displaying
incoherent behaviour in at least 27.23% of their
predictions. Contrary to expectations, we also
find that scaling the model size does not guar-
antee positive gains in performance. To explain
these results, we study the sources from which
LLMs may gather temporal information: we
find that sentence ordering in unlabelled texts,
available during pre-training, is only weakly
correlated with event ordering. Moreover, pub-
lic instruction tuning mixtures contain few tem-
poral tasks. Hence, we conclude that current
LLMs lack a consistent temporal model of tex-
tual narratives.1
1 Introduction
Recent large language models (LLMs) like GPT-4
have shown impressive progress on various down-
stream tasks in natural language processing (Brown
et al., 2020; Chung et al., 2022; Workshop et al.,
2023; Chowdhery et al., 2022; OpenAI, 2023).
However, a notorious deficiency of these models is
1Code, datasets, and LLM outputs are available at https:
//github.com/yfqiu-nlp/temporal-llms .thelack of grounding (Yun et al., 2021; Abdou et al.,
2021; Carta et al., 2023; Mahowald et al., 2023).
LLMs, which are solely trained with textual data,
acquire knowledge from distributional patterns of
language. Humans, on the other hand, perceive
and interact with the world through the dimensions
of time and space. Accurately grounding intelli-
gent systems within the physical world is crucial
for developing a versatile form of intelligence that
is truly embodied (Dasgupta et al., 2022; Driess
et al., 2023; Carta et al., 2023), as well as capable
of sample-efficient learning and causal discovery
through mental simulations (LeCun, 2022).
In this work, we focus on temporal reasoning
as a case study for establishing to what extent
LLMs are grounded in the absence of perception
and action. We first present a framework (Sec-
tion 2) for evaluating temporal grounding, where
we decompose this property into three fundamen-
tal aspects: 1) commonsense knowledge about the
typical duration, occurrence, frequency, and struc-
ture of events, 2) ability to order events along a
timeline and 3) consistency based on temporal con-
straints. Then, we carefully choose and curate
three benchmarks (Section 3.1) for capturing each
of these abilities: McTACO (Zhou et al., 2019),
CaTeRS (Mostafazadeh et al., 2016) and TempE-
valQA (Llorens et al., 2015), respectively.
We conduct an extensive empirical evaluation
of the temporal reasoning capabilities of state-of-
the-art LLMs, including both open-source (e.g.,
LLaMA 2) and proprietary (e.g., GPT-4). In addi-
tion, we investigate the effect of recent techniques
that facilitate generalisation to new tasks with ab-
lations: we compare zero-shot inference, few-shot
prompting using in-context learning, scaling in
terms of model parameters and the number of few-
shot examples, and chain-of-thought prompting.
Firstly, we find that all the tasks requiring tem-
poral grounding remain extremely challenging for
LLMs (Section 4), even for the recently releasedarXiv:2311.08398v2  [cs.CL]  16 Nov 2023

--- PAGE 2 ---
Reference: (B) For years; (D) A yearGPT-4: (B) For years Reference: <E1> Tim drank a little too much. <E2> His golf game was awful.GPT-4: <E2>His golf game was awful. <E1>Tim drank a little too much.Before-Ans-Gold: YesAfter-Ans-Gold: NoBefore-Ans-GPT-4: YesAfter-Ans-GPT-4: YesLLMsContext: Saftiadmits his love for Edwina to Lord Esketh, who is now sympathetic toward this good man's plight.Question:How long has Saftibeen in love with Edwina?Candidates:(A) 10 seconds; (B) For years; (C) For hours; (D) A yearUn-ordered Events: His golf game was awful. Tim drank a little too much.Events:  […] all the moreso when even the Obama administration joined the West in signaling approval. […] Something had to be done, and an occasion arose on June 12, […]Before-Q: Did the Obama administration join the West beforeJune 12?After-Q: Did the Obama administration join the West afterJune 12?Common-senseKnowledgeTimeline ReasoningTemporal Constraints SatisfactionFigure 1: Examples from three datasets to evaluate the temporal grounding of LLMs based on common-sense
knowledge about events, timeline reasoning, and temporal constraints satisfaction. We include the predictions from
the best-performing model, GPT-4, with in-context learning. We highlight wrong predictions with underline .
state-of-the-art GPT-4. In particular, LLMs lag sig-
nificantly behind human performance and even be-
hind specialised, small-scale, fine-tuned language
models in temporal commonsense knowledge and
event ordering. Most strikingly, however, the re-
sults reported on our TempEvalQA-Bi benchmark
(Section 4.3) reveal that the predictions of LLMs
are not consistent with respect to mutually exclu-
sive temporal relations. For example, LLaMA-65B
andtext-davinci-003 display a percentage
of inconsistent predictions at 78.13% and 69.64%,
respectively. Secondly, we observe that recent
techniques introduced for boosting LLMs’ perfor-
mance, including few-shot in-context learning (Sec-
tion 4.5) and chain of thought (Section 4.6), have a
positive but limited effect. Thirdly, scaling the ex-
amples for in-context learning and model size does
not necessarily improve the model performance on
temporal tasks (Section 4.5).
Finally, to understand the reason for the under-
whelming performance of LLMs in temporal tasks,
we attempt to determine whether enough temporal
information is provided to them during their train-
ing pipeline (Section 5). We find that in the unla-
belled texts available during pre-training, sentence
ordering and event ordering are weakly correlated.
Hence, LLMs assign only a slightly higher proba-
bility to sentences describing a ground-truth event
series in temporal order, rather than an inverted
order. This is especially the case whenever explicit
temporal markers (e.g., before andafter ) are miss-
ing. We also find that most training mixtures for
instruction tuning lack temporal tasks. Thus, tem-
poral grounding cannot emerge from the current
LLM training paradigm.2 Motivation and Framework
While the question of whether language models
contain a spatial map of the world has been thor-
oughly investigated (Louwerse and Zwaan, 2009;
Mikolov et al., 2013; Rahimi et al., 2017; Faisal and
Anastasopoulos, 2022, inter alia ),temporal ground-
ing of language models remains an under-explored
area. Recently, Gurnee and Tegmark (2023) found
that a linear probe can recover the year of existence
of an entity from its representation. From this ev-
idence, they conclude that LLMs can learn (tem-
poral) world models; however, this seems unwar-
ranted given that ‘world model’ defines the ability
of grounded agents to internally simulate physical
dynamics (Ha and Schmidhuber, 2018). On the
other hand, since LLMs are incapable of action and
perception, this ability cannot be assessed directly.
Instead, to study whether LLMs are temporally
grounded, we we assess their ability to reason about
textual narratives, which contain accounts of series
of events. Events are usually denoted by verbal
(e.g. to ride ) or nominal expressions (e.g. a discus-
sion). The temporal structure of an event is partly
inherent to such expressions, i.e. lexical aspect (or
Aktionsart ; Vendler, 1967), and partly determined
by their textual context. The events mentioned in
a text are connected by (asymmetrical) temporal
relations (Pustejovsky et al., 2003), which can be
expressed explicitly (e.g., through adverbial mark-
ers such as after orbefore ) or implicitly.
In particular, we focus on 3 aspects of temporal
reasoning illustrated with examples from Figure 1:
•Commonsense Knowledge about Events : A
grounded model should exhibit commonsense
knowledge with respect to the typical duration,

--- PAGE 3 ---
occurrence, frequency, and structure of events.
For instance, when presented with the question
“How long has Safti been in love with Edwina? ”,
the model should recognise that “ for years ” is a
more probable answer than “ 10 seconds ”, due to
the typical duration of this emotional state.
•Ordering Events along a Timeline : Such a
model should also be able to infer the chronology
of events from narratives (e.g., news) where they
are not presented in temporal order. For example,
the model should deduce that “ Tim drank a little
too much. ” occurs before that “ His golf game
was awful. ”, due to the causal relation between
these two events.
•Satisfying Temporal Constraints : A grounded
model should maintain self-consistency in its
predictions. One important constraint is that con-
tradicting timelines cannot co-exist. If, with re-
spect to a certain context, the model classifies
“The Obama administration joined the West be-
fore June 12 ” as true, then it must classify the
opposite statement “ The Obama administration
joined the West after June 12 ” as false.
3 Temporal Grounding Evaluation
We probe the temporal grounding of LLMs from
the GPT and LLaMA families, which collectively
represent a wide range of LLM types. The models
we tested can be grouped into two categories: 1)
foundation models (e.g., LLaMA 1 and 2; Touvron
et al. 2023a,b) pre-trained on unlabelled texts, and
2) LLMs fine-tuned on instruction-following (e.g.,
Alpaca and text-davinci-002 ) or conversa-
tional objectives (e.g., GPT-4; OpenAI 2023 and
LLaMA-2-chat; Touvron et al. 2023b). We use the
OpenAI official API2and the transformers li-
brary (Wolf et al., 2020) to conduct all experiments.
3.1 Benchmarks for Temporal Grounding
For the evaluation, we choose two off-the-shelf
benchmarks, McTACO (Zhou et al., 2019) and
CaTeRS (Mostafazadeh et al., 2016), and curate
a third benchmark starting from TempEvalQA
(Llorens et al., 2015) to measure the three aspects
of temporal grounding identified in Section 2. Ta-
ble 1 showcases an example from each benchmark.
McTACO is a benchmark for evaluating tem-
poral commonsense knowledge through multiple
choice question answering. McTACO consists of
2https://platform.openai.com/13K tuples in the form of (context, question, can-
didate answers). McTACO examples fall within
one of five categories: duration (how long an event
lasts), temporal ordering (typical order of events),
typical time (when an event happens customarily),
frequency (how often an event occurs), and station-
arity (whether a state holds for a very long time or
indefinitely).
CaTeRS is a benchmark for event ordering, con-
sisting of 1684 instances. This task involves identi-
fying events mentioned in a text and arranging them
in chronological order. To solve this task, a model
must rely on explicit clues as well as commonsense
knowledge about relations between events, to rea-
son about the underlying timeline.
TempEvalQA-Bi , derived from TempEvalQA
(Llorens et al., 2015), is our curated dataset for
assessing a model’s self-consistency in temporal
reasoning. TempEvalQA comprises 294 human-
annotated question–answer triples in the format of
(context, question, yes/no answer), focusing on
temporal relations between pairs of events. To
construct TempEvalQA-Bi, we selectively choose
question–answer pairs that involve the before /after
temporal relation. We then swap the temporal re-
lation to its opposite to create negative question–
answer pairs. For instance, if the original pair
is (“ IsE1after E2?”, “ yes”), the correspond-
ing negative pair will be (“ IsE1before E2?”,
“no”), and vice versa. After this post-processing,
TempEvalQA-Bi consists of 224 test examples with
448 question–answer pairs. We assess the model’s
performance by evaluating its predictions on both
the original and negative pairs, considering the
model’s answer as correct only if it accurately pre-
dicts both pairs.
3.2 Inference of LLMs
We now provide details of performing inference
with LLMs on these three benchmarks.
Multiple-choice Question Answering is used for
benchmarking LLMs’ performance on McTACO.
For GPT models, which do not always provide ac-
cess to output probabilities, we simply provide the
context, question, and allthe candidate answers in
a single prompt and generate one or more answers.
For LLaMA models, a more robust setup consists
in presenting the context, question, and each of
candidate answers separately in multiple prompts.
We calculate the likelihood of decoding “ True” and
“False ” and assign the label with the higher value

--- PAGE 4 ---
McTACO CaTeRS
ModelsZero-shot Few-shot
Strict Acc. F1 Strict Acc. F1 Pair Acc.
RoBERTa 43.62 72.34
TemporalBART 77.06
Human 75.80
GPT-4 28.45 35.88 50.15 65.27 60.51
text-davinci-003 26.05 48.30 33.56 65.04 53.47
LLaMA-7B 14.39 2.82 35.30 15.18 20.17 2.46 22.39 5.07 3.76 4.58
Alpaca-7B 21.75 5.22 52.17 9.69 30.03 10.11 44.10 18.36 10.37 4.91
LLaMA-13B 15.67 3.42 36.59 14.69 24.37 6.08 34.99 19.01 5.27 5.51
LLaMA-33B 17.24 3.36 33.20 15.07 29.70 4.79 47.57 8.36 14.38 10.77
LLaMA-65B 18.14 5.63 46.83 6.51 26.13 12.15 47.84 2.65 21.02 10.27
LLaMA-2-7B 11.16 1.55 42.55 12.29 21.74 3.83 32.94 17.56 5.85 2.06
LLaMA-2-13B 15.69 3.49 39.35 15.5529.75 0.69 43.21 2.51 16.26 5.75
LLaMA-2-70B 19.12 3.58 33.51 9.75 27.77 2.35 37.20 3.71 21.61 8.39
LLaMA-2-chat-7B 20.74 3.45 28.73 4.48 23.00 3.56 31.50 10.18 26.32 2.09
LLaMA-2-chat-13B 22.22 0.13 31.67 9.38 28.90 1.04 41.63 5.97 30.27 3.02
LLaMA-2-chat-70B 20.84 2.08 26.42 5.98 27.18 4.9 34.37 7.75 30.55 21.87
Table 1: Average model performance (standard deviations as subscripts). Left: McTACO for evaluating temporal
commonsense reasoning in LLMs. Strict Acc. andF1follow the definitions in Zhou et al. (2019). Right: CaTeRS
results for few-shot prompting. Pair Acc. stands for pairwise accuracy. In general, pÒqindicates that a higher value
is better. The results for three prompt templates are shown in Table 5 for McTACO and in Table 6 for CaTeRS.
to each candidate answer.
Sequence-to-Sequence Generation. We follow
Lin et al. (2021) to formulate the evaluation for
CaTeRS as a sequence-to-sequence problem. The
model takes the event series as the input and tempo-
rally sorts it as the output. We only test prompting
with in-context learning for CaTeRS and not zero-
shot inference because sorting the unordered events
requires the model to also generate and sort special
event tokens, which are unseen during pretraining.
Yes/No Question Answering , in the TempEvalQA-
Bi benchmark, requires the model to predict either
Yes or No as the answer for each question. To
probe the models’ predictions, we use 1) greedy
decoding for GPT models, where we generate the
token with the maximum likelihood to complete
a given prompt. We then map all possible verbal
tokens (e.g., yes/no, true/false and 1/0) into binary
form. 2) Likelihood-based evaluation for LLaMA
models: similar to Ruis et al. (2022) an Liu et al.
(2023a), we append “ yes” and “ no” as the answer
in our prompt template. We then estimate the likeli-
hood of both with LLMs. The label with the higher
likelihood value is taken as the model’s prediction.4 Results and Discussion
In this section, we describe our main results on
three datasets: McTACO, CaTeRS, TempEvalQA-
Bi. We also analyse LLMs’ sensitivity to prompt-
ing in temporal reasoning, scaling behaviour, and
the impact of chain-of-thought prompting.
4.1 Temporal Commonsense: McTACO
Evaluation Settings. First, we probe temporal
commonsense knowledge. Table 1 shows each
model’s strict accuracy and F1 score on McTACO.
Following Zhou et al. (2019), strict accuracy is the
percentage of questions where the model predicts
allthe correct candidate answers, whereas the F1
score is calculated from all question–answer pairs.
Figure 2 shows the strict accuracy split by question
category.
Discussion. In Table 1, GPT-4 outperforms all
tested LLMs, confirming its state-of-the-art abil-
ity in temporal commonsense knowledge among
LLMs. Within the LLaMA family, LLaMA-2-chat-
13B and Alpaca-7B achieve the best performance
in zero-shot and few-shot experiments, respectively.
Alpaca-7B is comparable to LLaMA-2-chat-13B

--- PAGE 5 ---
LLaMA-7B
LLaMA-Alpaca-7BLLaMA-2-7B
LLaMA-2-chat-7BLLaMA-13BLLaMA-2-13B
LLaMA-2-chat-13BLLaMA-33B LLaMA-65BLLaMA-2-70B
LLaMA-2-chat-70Btext-davinci-003GPT-4ED EO FR ST TT0.22 0.20 0.08 0.22 0.22 0.22 0.22 0.22 0.20 0.24 0.23 0.16 0.22
0.13 0.34 0.08 0.19 0.14 0.13 0.17 0.11 0.19 0.21 0.16 0.25 0.24
0.24 0.17 0.11 0.27 0.25 0.25 0.26 0.24 0.25 0.30 0.31 0.21 0.35
0.12 0.41 0.14 0.40 0.18 0.21 0.33 0.15 0.39 0.15 0.32 0.64 0.53
0.15 0.33 0.08 0.15 0.15 0.15 0.15 0.15 0.17 0.17 0.16 0.18 0.17Zero-shot Strict Accuracy
LLaMA-7B
LLaMA-Alpaca-7BLLaMA-2-7B
LLaMA-2-chat-7BLLaMA-13BLLaMA-2-13B
LLaMA-2-chat-13BLLaMA-33B LLaMA-65BLLaMA-2-70B
LLaMA-2-chat-70Btext-davinci-003GPT-40.23 0.37 0.22 0.22 0.22 0.24 0.24 0.25 0.27 0.25 0.25 0.17 0.42
0.26 0.47 0.12 0.15 0.12 0.35 0.33 0.42 0.42 0.36 0.27 0.44 0.61
0.27 0.36 0.24 0.25 0.24 0.32 0.29 0.35 0.39 0.36 0.37 0.23 0.47
0.16 0.43 0.11 0.21 0.21 0.50 0.39 0.58 0.46 0.30 0.57 0.66 0.70
0.17 0.44 0.15 0.16 0.15 0.18 0.17 0.18 0.21 0.18 0.18 0.32 0.38Few-shot Strict AccuracyFigure 2: Strict accuracy of all tested LLMs for different reasoning categories in McTACO. The rows represent all
reasoning categories in McTACO, which are Event Duration (ED), Event Ordering (EO), Frequency (FR), Stationary
(ST) and Typical Time (TT). The columns represent several LLMs.
also in the zero-shot setting. We attribute its re-
markable performance to its in-domain temporal
tasks during instruction tuning (for more discus-
sion, see Section 5.2). We also notice that all
LLaMA-2-chat models considerably improve their
pre-trained counterparts, indicating the importance
of advanced tuning (e.g., conversational-style tun-
ing and reinforcement learning with human feed-
back, RLHF; Touvron et al. 2023b) in addition
to pre-training. However, even when prompting
with in-context learning, the top-performing GPT-
4 lags behind the fine-tuned RoBERTa baseline
in terms of F1 score and human performance in
terms of strict accuracy. This finding highlights
the shortcomings of state-of-the-art LLMs in learn-
ing temporal commonsense knowledge. Inspecting
the performance by question category, plotted as
a heatmap in Figure 2, reveals that questions re-
quiring commonsense knowledge of Typical Time
and Event Duration are the most challenging. Here,
GPT-4 achieves a success rate of only around 40%.
On the other hand, questions related to Stationar-
ity are relatively easier, with most LLMs showing
adequate performance using in-context learning.
4.2 Event Ordering: CaTeRS
We evaluate the ability of LLMs to order events on
CaTeRS: results are displayed in Table 1.
Evaluation Settings. We follow Lin et al. (2021)
in using pairwise accuracy as a metric. This mea-
sures how many pairs of events in the output are
ordered correctly by a model. This metric vali-
dates each model in two respects: 1) the model
is asked to extract the events from the given text
correctly. And 2) the model must order the given
set of events according to their chronology, pro-
ducing a semi-structured output. We also include
TemporalBART from Lin et al. (2021) as a baseline.TemporalBART is a BART-Large model fine-tuned
on re-constructing manually corrupted event series.
Discussion. First, we confirm again the positive
gains from conversational tuning and RLHF by
observing the large advantage of LLaMA-2-chat
models over LLaMA-2 models. However, we do
not observe improvements by increasing the num-
ber of in-context learning examples (see Figure 3b)
or model parameters (see Figure 3a). The propri-
etary models, text-davinci-003 and GPT-4,
achieve higher pairwise accuracy than all LLaMA
models; however, they lag behind a small fine-
tuned model, TemporalBART (Lin et al., 2021),
by a large margin.
4.3 Satisfaction of Temporal Constraints:
TempEvalQA-Bi
In Table 2, we report the performance of LLMs on
our curated bi-directional TempEvalQA-Bi dataset.
Evaluation Metrics. We evaluate the models in
terms of their accuracy ( Acc.), where the model is
required to correctly predict a yes/no answer for a
question with a before /after relation (see Table 1
for an example). We also include a metric, namely
the percentage of inconsistent predictions ( Inc.),
to shed further light on the model behaviour. This
measures the number of times a model predicts
thesame yes/no label for both the original ques-
tion and a version where the temporal relation is
flipped (everything else remaining the same). For
example, if models predict yesfor a question with
abefore relation, ideally they should predict nofor
the corresponding question with after. We can com-
pare the two metrics, Acc. andInc., to probe the
model’s ability to answer correctly and consistently,
respectively.
Discussion We first observe that most models per-
form poorly. The best model in the LLaMA family

--- PAGE 6 ---
ModelsZero-shot Few-shot
Acc. (Ò)Inc. (Ó)Acc. (Ò)Inc. (Ó)
GPT-4 64.29 31.25 67.41 27.23
text-davinci-003 27.68 69.64 33.93 62.05
text-davinci-002 16.52 77.83 36.16 60.71
davinci 14.73 79.02 13.39 79.91
LLaMA-7B 3.42 2.46 94.79 3.64 3.27 0.51 94.94 1.57
LLaMA-Alpaca-7B 10.12 2.29 83.63 5.08 13.10 6.00 77.23 7.37
LLaMA-13B 0.60 0.68 97.77 3.49 0.60 0.68 99.25 0.51
LLaMA-33B 1.34 1.34 98.22 1.18 14.73 7.74 83.33 9.43
LLaMA-65B 14.14 5.1783.48 6.1431.99 1.5760.42 4.47
LLaMA-2-7B 0.15 0.26 99.85 0.26 11.90 0.52 85.12 2.62
LLaMA-2-13B 5.65 3.3 92.86 3.81 13.69 7.63 83.63 8.00
LLaMA-2-70B 6.55 2.01 92.41 3.13 29.76 2.73 65.77 2.02
LLaMA-2-chat-7B 13.84 7.63 83.33 7.82 23.51 2.20 70.09 0.77
LLaMA-2-chat-13B 22.92 4.03 72.91 5.58 31.69 3.22 62.95 3.57
LLaMA-2-chat-70B 38.54 3.0458.03 2.3646.42 1.1848.96 2.01
Table 2: Average model performance (standard deviations as subscripts) evaluated on our curated bi-directional
TempEvalQA benchmark. Acc. andInc. stand for accuracy and the percentage of inconsistent predictions. pÒq{pÓq
indicate that higher / lower values are better, respectively. The detailed results for three prompt templates are
presented in Table 4.
(LLaMA-2-chat-70B with in-context learning) can
only solve 46% of questions correctly. Moreover, if
we flip the temporal relation of the question, most
models fail to convert their output predictions ac-
cordingly, resulting in significant percentages of
inconsistent predictions. This observation suggests
that inconsistent predictions are the main cause of
low performance on this task.
On the other hand, solid improvements re-
sult from advanced tuning, including instruc-
tion/conversation tuning and RLHF, enabling the
model’s predictions to be more consistent. The
Alpaca-7B model dramatically reduces the rate
of inconsistencies and improves accuracy over
LLaMA-7B. The same holds true for LLaMA-
2 and LLaMA-2-chat, text-davinci and
text-davinci-002 /002. In-context learning
can also be beneficial, but it yields mixed results
for some models, such as LLaMA-7B and 13B.
Finally, GPT-4 again achieves state-of-the-art
accuracy by solving around 67% of our curated
questions. Although it significantly reduces the
percentage of inconsistent predictions compared
to other models, it still predicts the same answer
for around 27% of the questions and their flipped
versions.4.4 Sensitivity to Different Prompts
Large language models have been shown to be sen-
sitive to the wording of their prompts (Webson and
Pavlick, 2022; Reynolds and McDonell, 2021; Tan
et al., 2022; Ruis et al., 2022; Liu et al., 2023a).
Similar to Ruis et al. (2022) and Liu et al. (2023a),
to test the randomness introduced by this sensitivity
in our experiments, we manually curate three dif-
ferent prompt templates and measure the variation
of performance across these different wordings for
the LLaMA family.
We experiment with three different kinds of
prompt templates: 1) semi-structured prompt : we
indicate the instruction, context, and pairs of in-
put and output (for in-context learning) in a se-
quence, separated by special symbols (e.g., new-
line). 2) Natural prompt : we interleave the context
and input–output pairs in the instruction, to make
the template closer to natural language. 3) Text con-
tinuation prompt : given that LLaMA is not trained
with instruction-following tasks, we introduce a
text-continuation style prompting where we only
ask the model to complete the input. We provide
all our prompt templates in Table 3.
We report the standard deviation of model per-
formance in the result tables for each of the three

--- PAGE 7 ---
7B 13B 33B 65B020406080
McTACO
LLaMA-ZS
LLaMA-ICLLLaMA-2-ZS
LLaMA-2-ICLLLaMA-2-chat-ZS
LLaMA-2-chat-ICLBaseline7B 13B 33B 65B
CaTeRS
7B 13B 33B 65B
TempEvalQA-Bi(a) Scaling the model parameters.
3 5 7 100255075100
McTACO
3 5 7 10
CaTeRS
LLaMA-7B
Alpaca-7BLLaMA-13B
LLaMA-33BLLaMA-65B
Baseline3 5 7 10
TempEvalQA-Bi
(b) Scaling the examples of in-context learning.
Figure 3: The performance curve for scaling experiments. We report the strict accuracy for McTACO, pairwise
accuracy for CaTeRS and accuracy for TempEvalQA-Bi. (a): The error bars show the standard deviation over three
prompt templates. (b): The baseline for McTACO is Human, and for CaTeRS is TemporalBART.
datasets. Standard deviations range from 0.13 for
the LLaMA-2-chat-13B model to 5.63 for LLaMA-
65B when considering all templates in McTACO
in zero-shot prompting. The variation caused by
prompt templates is slightly higher for zero-shot
results in TempEvalQA-Bi, where the standard de-
viation ranges from 0.26 for LLaMA-2-7B to 7.63
for LLaMA-2-chat-7B. The few-shot experiments
tend to be more varied, because of the increased
length of each prompt due to adding additional
exemplars. Overall, these findings show that the
prompt wording does not affect the main conclu-
sions of our experiments. However, they also con-
firm that LLMs—especially large-scale ones—are
brittle with respect to prompt wording.
4.5 Scaling Behaviour
Scaling the Model Parameters. We perform scal-
ing experiments (refer to Figure 3a) to explore the
influence of parameter increments on the perfor-
mance of LLMs. We exclusively execute this abla-
tion within the LLaMA family due to the unavail-
ability of information on model sizes for the propri-
etary GPT models. The largest 70B LLaMA-2-chatmodel can generally achieve the best performance
on our three datasets. However, for many mod-
els, performance is only weakly correlated with
their size. For example, the LLaMA-13B model,
rather than the smaller LLaMA-7B one, performs
the worst on both McTACO and TempEvalQA-Bi.
We also observe a general trend such that LLM
performance tends to saturate and cannot further in-
crease after the parameter size exceeds 13B on Mc-
TACO and CaTeRS. Finally, comparing the LLMs
with the much smaller baselines of TemporalBART
and RoBERTa shows that model specialisation is
more impactful than an increase in scale for tempo-
ral grounding.
Scaling the Few-shot Examples. We begin by con-
ducting scaling experiments (please refer to Fig-
ure 3b) to investigate the impact of the number
of few-shot examples ( k) on model performance.
We focus our experiments on the LLaMA fam-
ily. The results on McTACO and CaTeRS indicate
that increasing kgenerally leads to a slight perfor-
mance improvement, albeit minimal. Conversely,
we observe a slight decrease in performance on
TempEvalQA-Bi when increasing k. This decrease

--- PAGE 8 ---
Models01020304050607080AccuracyStandard prompting
Chain-of-thought promptingStandard prompting
Chain-of-thought prompting
Models020406080100% Inconsistency
LLaMA-7B
Alpaca-7BLLaMA-2-7B
LLaMA-2-Chat-7BLLaMA-13B
LLaMA-2-13BLLaMA-2-Chat-13B
LLaMA-33BLLaMA-65B
LLaMA-2-70BLLaMA-2-Chat-70B
GPT-4LLaMA-7B
Alpaca-7BLLaMA-2-7B
LLaMA-2-Chat-7BLLaMA-13B
LLaMA-2-13BLLaMA-2-Chat-13B
LLaMA-33BLLaMA-65B
LLaMA-2-70BLLaMA-2-Chat-70B
GPT-4Figure 4: Accuracy (higher is better) and percentage of in-consistent reasoning (lower is better) on TempEvalQA-Bi
for LLMs prompted with a chain of thoughts or not.
can be attributed to our choice of increasing only
the number of question–answer pairs for the same
context, as using multiple contexts would surpass
the model’s maximum input limit. Notably, Mc-
TACO and CaTeRS exhibit true scaling with few-
shot examples, supporting the idea that examples
must be diverse to contribute incrementally to per-
formance (Min et al., 2022).
4.6 Chain-of-thought Prompting
Finally, we investigate the impact of chain-of-
thought prompting (Wei et al., 2022) on model
performance in TempEvalQA-Bi, a challenging rea-
soning task. We apply chain-of-thought prompting
to LLaMA3and GPT-4 models in combination with
in-context learning. The prompts we constructed
are listed in Appendix C. Figure 4 shows that chain-
of-thought prompting reduces inconsistent predic-
tions for all models; however, the improvement
of reasoning consistency does not always translate
into increased accuracy. For example, we observe
a drop in accuracy for LLaMA-7B, LLaMA-2-70B
and LLaMA-2-70B-chat models. We also notice
that the improvement in the state-of-the-art GPT-
4 is marginal. These results indicate that current
training and inference paradigms may struggle with
complex temporal reasoning tasks requiring self-
consistency.
5 Sources of Temporal Information
In this section, we examine two key aspects: firstly,
the extent of temporal information provided by
3Since LLaMA-2 models support twice the input length of
LLaMA when prompting all LLaMA-2 models, we regard the
maximum input length as a hyperparameter.
012LLaMA-7bOdd Ratio=1
CaTeRS Paraphrase
TempEval Paraphrase
012LLaMA-7b-AlpacaOdd Ratio=1
CaTeRS Paraphrase
TempEval Paraphrase
012LLaMA-13bOdd Ratio=1
CaTeRS Paraphrase
TempEval Paraphrase
012LLaMA-33bOdd Ratio=1
CaTeRS Paraphrase
TempEval Paraphrase
012LLaMA-65bOdd Ratio=1
CaTeRS Paraphrase
TempEval ParaphraseFigure 5: Density plot of the odds ratio under sev-
eral LLMs (rows) for differently ordered paraphrases in
CaTeRS ( orange) and TempEvalQA-Bi ( green).
pre-training, and secondly, the significance of su-
pervised examples in addressing information gaps
within the pre-trained model.
5.1 How Much Temporal Information Does
Pre-training Provide?
Overall, the results from Section 4 indicate that
LLMs are not sufficiently temporally grounded.
We speculate that this is due to not being ex-
posed to temporal information during their training
pipeline. To verify this, we first focus on the pre-
training stage. We investigate whether the order
in which events are presented in human-written
texts provides a clue about their actual chronology.
We measure the correlation between the temporal

--- PAGE 9 ---
relations (i.e., before /after ) annotated in TempE-
valQA’s training articles and the textual order of
the events they refer to. This reveals that only
55.98% of event pairs (1866 out of 3333) occur
in the text in accordance with their temporal or-
der. The Matthews correlation coefficient of 0.09
indicates a weak correlation (Matthews, 1975; Dav-
enport Jr and El-Sanhurry, 1991).
We then study whether, as a result of this prop-
erty of the data, LLMs prefer temporally ordered or
unordered descriptions of the same event series, by
comparing their log-likelihoods. We randomly sam-
ple 100 instances from the CaTeRS testing set and
manually curate two paraphrases for each ground-
truth sequence of events. For the first paraphrase,
we keep the same sentence order as the event tem-
poral order. For the second paraphrase, we permute
the order of the sentences. All the resulting para-
phrases are manually adjusted to be coherent and
grammatically correct to exclude confounding fac-
tors. We present one example:
Ground-truth Order : [E1] Mary was very
stressed about the job. [E2] Mary got the job.
Ordered Paraphrase :Mary was very stressed
about the job, but she got it.
Inverted Paraphrase :Mary got the job. She was
very stressed about it.
We then create another subset of paraphrases
from TempEvalQA-Bi, retaining explicit temporal
relation markers such as before andafter .
We plot the distribution of the length-normalised
odds ratios between corresponding paraphrases in
Figure 5. Firstly, we find that the odds ratio is sig-
nificantly higher than 1 on average for the CaTeRS
paraphrases. This means that the probability of tem-
porally ordered sequences is only slightly higher
than their permuted counterparts. Thus, foundation
models have a mild bias in preferring sequences of
sentences that follow the temporal order of events.
For the examples with explicit temporal relation
markers (i.e., before andafter ) from TempEvalQA-
Bi, the odds ratio falls around 1. Thus, the model’s
preference for temporally ordered and unordered
sequences tends to be equalised. These results
demonstrate that it may be difficult for foundation
models to identify the ground-truth event ordering
based on their pre-training information, as this pro-
vides a weak signal about the temporal dynamics
of real-world events.5.2 Importance of Supervised Examples
Next, we investigate whether subsequent fine-
tuning might compensate for the lack of temporal
information during pre-training. The current open-
sourced instruction-tuning datasets commonly used
to fine-tune LLMs indeed tend to include some tem-
poral tasks. For example, Super-Natural Instruction
(Mishra et al., 2022; Wang et al., 2022), one of the
most comprehensive massively multi-task bench-
marks, has only 2 temporal reasoning tasks in its
training set out of 756 in its default split.4In
Table 1 and Table 2, we observe Alpaca-7B con-
sistently outperforming LLaMA-7B and being al-
most comparable with LLaMA-33B. In addition,
we found that small-scale models fine-tuned on
many in-task examples, such as RoBERTa and Tem-
poralBART in Table 1, often surpass or are compet-
itive with large-scale, state-of-the-art LLMs such
as GPT-4. This lends hope that the gap with human
performance can be partly filled with sufficient su-
pervision; however, there remains an insurmount-
able barrier that even providing more examples
cannot overcome: GPT-4 performance plateaus
even increasing in-context examples (see Figure 3).
Even chain-of-thought prompting does not increase
its self-consistency (see Figure 4). We hypothesise
that only equipping LLMs with perception and ac-
tion in a simulated or physical environment might
further ground them sufficiently to enhance their
temporal reasoning.
6 Related Work
Recently, a budding interest has emerged in prob-
ing whether large language models are grounded
(Yun et al., 2021; Abdou et al., 2021; Carta et al.,
2023; Mahowald et al., 2023; Chandu et al., 2021)
with respect various properties. Abdou et al. (2021)
found a correspondence between physical colour
space and the embeddings of colour terms. Ebert
et al. discovered that object trajectories in an en-
vironment correlate with the embeddings of mo-
tion verbs. In contrast, Yun et al. (2021) showed
that vision-and-language pre-training fails to fur-
ther ground lexical representations. Improving the
grounding of LLMs is also an active area of re-
search. Integrating the modelling of subject–object
interactions enables better grounding during train-
ing (Merullo et al., 2022; Carta et al., 2023). Liu
et al. (2023b) uses a computational physics engine
4task1507_boolean_temporal_reasoning ,
task389_torque_generate_temporal_question .

--- PAGE 10 ---
as a simulator for improving how language models
reason about the physical world.
7 Conclusions
In this study, we examined to what extent large lan-
guage models (LLMs) are temporally grounded:
given a textual narrative describing a series of
events, we propose a framework to probe their tem-
poral reasoning capabilities. We consider a series
of tasks including commonsense knowledge about
the events, ordering events along a timeline, and
satisfaction of temporal constraints (which ensures
the self-consistency of the LLM’s internal world
model). Through a systematic evaluation, we find
that state-of-the-art models such as GPT-4 still fall
short of human performance and even small-scale,
specialised models. Additionally, we observed that
increasing the model scale or the number of in-
context examples does not necessarily result in
meaningful gains. Instruction-tuning and chain-of-
thought prompting are more beneficial for accuracy
and self-consistency but yield diminishing returns
with scale. To explain these patterns, we highlight
the limitations of pre-training and instruction tun-
ing in achieving satisfactory temporal grounding
and suggest potential future directions. These in-
clude exploring new training paradigms, such as
leveraging action and perception in physical or sim-
ulated environments.
Limitations
Although we carefully select human-annotated
benchmarks for evaluation, there are still some
known issues. Questions in McTACO (Zhou et al.,
2019) may contain duplicate candidates, and some
of them have all true or false candidates. Models
that have not been trained on in-domain data may
not be robust to this noise in evaluation. CaTeRS
(Mostafazadeh et al., 2016) contains a small pro-
portion of highly similar or duplicated instances. A
high-quality benchmark for systematically evaluat-
ing the recent model’s temporal grounding ability
is still essential but missing.
It is not clear whether the training of GPT mod-
els is contaminated with data leakage from these
datasets. This fact may pose a potential bias in
overestimating the advantage of GPT models. Ad-
ditionally, we confirm that McTACO annotation is
not included in the instruction tuning of LLaMA-
Alpaca; however, other tasks have texts sourced
from the same corpus as McTACO. Hence, the Mc-TACO’s evaluation could also be biased to favour
the Alpaca model.
References
Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich,
Stella Frank, Ellie Pavlick, and Anders Søgaard.
2021. Can language models encode perceptual struc-
ture without grounding? A case study in color. In
Proceedings of the 25th Conference on Computa-
tional Natural Language Learning , pages 109–132,
Online. Association for Computational Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901.
Thomas Carta, Clément Romac, Thomas Wolf, Sylvain
Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer.
2023. Grounding large language models in interac-
tive environments with online reinforcement learn-
ing.
Khyathi Raghavi Chandu, Yonatan Bisk, and Alan W
Black. 2021. Grounding ‘grounding’ in NLP. In
Findings of the Association for Computational Lin-
guistics: ACL-IJCNLP 2021 , pages 4283–4305, On-
line. Association for Computational Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. PaLM: Scaling language
modeling with pathways.

--- PAGE 11 ---
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Ishita Dasgupta, Christine Kaeser-Chen, Kenneth
Marino, Arun Ahuja, Sheila Babayan, Felix Hill,
and Rob Fergus. 2022. Collaborating with language
models for embodied reasoning. In NeurIPS 2022
Foundation Models for Decision Making Workshop .
Ernest C Davenport Jr and Nader A El-Sanhurry. 1991.
Phi/phimax: review and synthesis. Educational and
psychological measurement , 51(4):821–828.
Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe
Yu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-
manet, Daniel Duckworth, Sergey Levine, Vincent
Vanhoucke, Karol Hausman, Marc Toussaint, Klaus
Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
2023. PaLM-E: An embodied multimodal language
model.
Dylan Ebert, Chen Sun, and Ellie Pavlick. 2022. Do
trajectories encode verb meaning? In Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 2860–2871,
Seattle, United States. Association for Computational
Linguistics.
Fahim Faisal and Antonios Anastasopoulos. 2022. Geo-
graphic and geopolitical biases of language models.
Wes Gurnee and Max Tegmark. 2023. Language models
represent space and time.
David Ha and Jürgen Schmidhuber. 2018. Recurrent
world models facilitate policy evolution. In Ad-
vances in Neural Information Processing Systems
31, pages 2451–2463. https://worldmodels.
github.io .
Yann LeCun. 2022. A path towards autonomous ma-
chine intelligence. Version 0.9.2, 2022-06-27.
Shih-Ting Lin, Nathanael Chambers, and Greg Durrett.
2021. Conditional generation of temporally-ordered
event sequences. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers) , pages 7142–7157, Online. Association for
Computational Linguistics.
Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr,
Peter West, Alexander Koller, Swabha Swayamdipta,
Noah A. Smith, and Yejin Choi. 2023a. We’re afraid
language models aren’t modeling ambiguity.
Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,
Soroush V osoughi, Claire Cui, Denny Zhou, and An-
drew M. Dai. 2023b. Mind’s eye: Grounded lan-
guage model reasoning through simulation. In TheEleventh International Conference on Learning Rep-
resentations .
Hector Llorens, Nathanael Chambers, Naushad UzZa-
man, Nasrin Mostafazadeh, James Allen, and James
Pustejovsky. 2015. SemEval-2015 task 5: QA Tem-
pEval - evaluating temporal information understand-
ing with question answering. In Proceedings of the
9th International Workshop on Semantic Evaluation
(SemEval 2015) , pages 792–800, Denver, Colorado.
Association for Computational Linguistics.
Max M Louwerse and Rolf A Zwaan. 2009. Language
encodes geographical information. Cognitive Sci-
ence, 33(1):51–73.
Kyle Mahowald, Anna A. Ivanova, Idan A. Blank,
Nancy Kanwisher, Joshua B. Tenenbaum, and
Evelina Fedorenko. 2023. Dissociating language
and thought in large language models: a cognitive
perspective.
Brian W Matthews. 1975. Comparison of the pre-
dicted and observed secondary structure of t4 phage
lysozyme. Biochimica et Biophysica Acta (BBA)-
Protein Structure , 405(2):442–451.
Jack Merullo, Dylan Ebert, Carsten Eickhoff, and El-
lie Pavlick. 2022. Pretraining on interactions for
learning grounded affordance representations. In
Proceedings of the 11th Joint Conference on Lexi-
cal and Computational Semantics , pages 258–277,
Seattle, Washington. Association for Computational
Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 11048–11064,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2022. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 3470–3487, Dublin, Ireland.
Association for Computational Linguistics.
Nasrin Mostafazadeh, Alyson Grealish, Nathanael
Chambers, James Allen, and Lucy Vanderwende.
2016. CaTeRS: Causal and temporal relation scheme
for semantic annotation of event structures. In Pro-
ceedings of the Fourth Workshop on Events , pages
51–61, San Diego, California. Association for Com-
putational Linguistics.
OpenAI. 2023. GPT-4 technical report.

--- PAGE 12 ---
James Pustejovsky, José M Castano, Robert Ingria,
Robert J Gaizauskas, Andrea Setzer, Graham Katz,
and Dragomir R Radev. 2003. TimeML: Robust spec-
ification of event and temporal expressions in text.
InProceedings of the 5th International Workshop on
Computational Semantics (IWCS-5) , pages 337–353.
Afshin Rahimi, Trevor Cohn, and Timothy Baldwin.
2017. A neural model for user geolocation and lexi-
cal dialectology. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers) , pages 209–216,
Vancouver, Canada. Association for Computational
Linguistics.
Laria Reynolds and Kyle McDonell. 2021. Prompt
programming for large language models: Beyond the
few-shot paradigm.
Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker,
Tim Rocktäschel, and Edward Grefenstette. 2022.
Large language models are not zero-shot communi-
cators.
Zhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang
Liu. 2022. MSP: Multi-stage prompting for mak-
ing pre-trained language models better translators.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 6131–6142, Dublin, Ireland.
Association for Computational Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. LLaMA:
Open and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Zeno Vendler. 1967. Linguistics in philosophy . Cornell
University Press.Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
Anjana Arunkumar, David Stap, Eshaan Pathak,
Giannis Karamanolakis, Haizhi Lai, Ishan Puro-
hit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,
Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,
Mehrad Moradshahi, Mihir Parmar, Mirali Purohit,
Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,
Ravsehaj Singh Puri, Rushang Karia, Savan Doshi,
Shailaja Keyur Sampat, Siddhartha Mishra, Sujan
Reddy A, Sumanta Patro, Tanay Dixit, and Xudong
Shen. 2022. Super-NaturalInstructions: Generaliza-
tion via declarative instructions on 1600+ NLP tasks.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
5085–5109, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Albert Webson and Ellie Pavlick. 2022. Do prompt-
based models really understand the meaning of their
prompts? In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies , pages 2300–2344, Seattle, United States.
Association for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. 2020. Transform-
ers: State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.
BigScience Workshop et al. 2023. Bloom: A 176b-
parameter open-access multilingual language model.
Tian Yun, Chen Sun, and Ellie Pavlick. 2021. Does
vision-and-language pretraining improve lexical
grounding? In Findings of the Association for Com-
putational Linguistics: EMNLP 2021 , pages 4357–
4366, Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth.
2019. “Going on a vacation” takes longer than “Go-
ing for a walk”: A study of temporal commonsense
understanding. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) ,
pages 3363–3369, Hong Kong, China. Association
for Computational Linguistics.

--- PAGE 13 ---
A All Prompt Templates
We provide the three prompt templates we design
for our experiments in Table 3.
B Detailed Results for All Prompt
Templates
In Table 4, we present the complete results based on
all three prompts on the TempEvalQA-bi dataset.
Additionally, comprehensive results utilising all
three distinct prompts for McTACO are provided
in Table 5. We only list the likelihood-based eval-
uation results for LLaMA models in Table 1 for
simplicity, but we observe the same trend for both
decoding-based and likelihood-based evaluation.
Fially, detailed results using all three different
prompts for CaTeRS can be found in Table 6.
CChain-of-thought Reasoning Examples
We provide an example of a model’s input using
chain-of-thought prompting on TempEvalQA-Bi in
Table 7. Table 8, Table 9, and Table 10 showcase a
selection of generations obtained from LLaMA-
65B and GPT-4 models using chain-of-thought
prompting on the TempEvalQA-bi dataset.

--- PAGE 14 ---
McTACOStructured Answer the following multiple-choice question with candidate answers according to
the given passage. There can be multiple correct answers.
Passage: {passage}
Question: {question}
Candidate answers: {candidates}
The answer is
Natural Based on the information presented in the passage “{passage}”, answer the multiple-
choice question “{question}” with following candidate answers “{candidate}”. There
can be multiple correct answers. The correct answer(s) is/are:
Text continuation Finish the following text:
According to the passage “{passage}”, the correct answer(s) to the multiple-choice
question “{question}” with following candidate answers “{candidates}” is/are:CaTeRSStructured Following the given template to order the events according to temporality:
[Input–Output Templates]
Input: {source}
Output:
Natural Based on temporality in the given events “{source}”, arrange the events in temporal
order. The order is:
Text continuation Finish the following text:
According to the temporality in the given events “{source}”, the temporal order of the
events is:TempEvalQA-BiStructured Answer the question according to the article. Only answer yes or no.
Article: {events}.
Question: {question}.
The answer is:
Natural Based on the information presented in the article “{events}”, answer the question
“{question}” with yes or no. The answer is:
Text continuation Finish the following text:
Article: {events}"
The answer to the yes or no question “{question}” according to the the article is:
Table 3: The three templates, i.e., Structured prompt, Natural prompt, Text continuation prompt, used for LLMs’
inference in each dataset.

--- PAGE 15 ---
ModelsPrompt 1 Prompt 2 Prompt 3
Likelihood Decoding Likelihood Decoding Likelihood Decoding
Acc.pÒq Inc.pÓq Acc.pÒq Inc.pÓq Acc.pÒq Inc.pÓq Acc.pÒq Inc.pÓq Acc.pÒq Inc.pÓq Acc.pÒq Inc.pÓq
LLaMA-7B ZS 6.25 90.63 10.71 82.35 1.79 96.43 0.45 92.16 2.23 97.32 0 60
ICL 2.68 96.43 3.57 91.96 3.57 93.30 8.93 82.03 3.57 95.09 2.23 86.67
Alpaca-7B ZS 12.05 82.14 12.95 80.36 7.59 89.29 1.79 92.04 10.71 79.46 0 80.49
ICL 17.41 73.66 11.61 80.80 6.25 85.71 10.71 77.31 15.63 72.32 9.38 76.67
LLaMA-13B ZS 0.45 99.55 4.02 90.41 1.34 93.75 0 94.23 0 100 0 96.15
ICL 1.34 98.66 2.68 95.98 0.45 99.55 1.34 85.11 0 99.55 0 100
LLaMA-33B ZS 2.68 96.88 11.16 86.16 0.89 99.11 5.36 84.83 0.45 98.66 0 100
ICL 19.20 78.57 17.41 79.46 5.80 94.20 17.86 72.93 19.20 77.23 24.55 69.72
LLaMA-65B ZS 19.64 78.13 9.38 73.39 13.39 82.14 4.91 84.34 9.38 90.18 0 100
ICL 32.14 63.84 33.93 62.50 30.36 62.05 5.36 87.60 33.48 55.36 27.68 58.20
LLaMA-2-7B ZS 0 100 0 100 0.45 99.55 0 1 0 100 0 100
ICL 11.61 87.05 0 100 11.6 82.14 4.91 77.7 12.5 86.16 11.16 80.35
LLaMA-2-13B ZS 9.38 88.84 10.26 78.05 3.12 96.43 0 97.42 4.46 93.3 0.45 76.47
ICL 18.75 78.57 12.5 76.76 4.91 92.85 4.02 92.27 17.41 79.46 13.83 80.6
LLaMA-2-70B ZS 6.7 92.41 0 100 8.48 89.28 11.16 78.92 4.46 95.54 0.9 44.44
ICL 26.78 67.86 14.29 80.38 32.14 63.83 25 62.63 30.36 65.63 28.57 66.21
LLaMA-2-7B-chat ZS 20.98 75.44 21.88 73.66 5.8 91.07 5.8 93.12 14.73 83.48 8.04 78.34
ICL 20.98 70.98 19.64 72.94 24.55 69.64 20.98 72.65 25 69.64 20.98 68.49
LLaMA-2-13B-chat ZS 23.21 72.76 8.92 80.77 18.75 78.57 15.17 77.42 26.79 67.41 0.89 81.82
ICL 30.8 62.94 21.88 72.64 29.01 66.52 23.21 71.95 35.26 59.38 30.8 62.5
LLaMA-2-70B-chat ZS 41.96 55.35 13.39 66.67 37.5 58.92 30.8 59.38 36.16 59.82 0.45 60
ICL 47.32 46.88 37.05 47.64 46.86 49.1 47.32 45.7 45.09 50.89 47.77 48.66
Table 4: Detailed Model performance evaluated on our curated bi-directional TempEvalQA-bi benchmark with
different prompt templates. Acc. andInc. stand for accuracy and in-consistent prediction rate. pÒq{pÓq indicate that
higher / lower values are better, respectively.
ModelsPrompt 1 Prompt 2 Prompt 3
Strict Acc. pÒq F1pÒq Strict Acc. pÒq F1pÒq Strict Acc. pÒq F1pÒq
LLaMA-7BZS 17.64 19.06 12.69 37.72 12.84 49.12
ICL 22.15 27.56 20.95 22.18 17.42 17.42
Alpaca-7BZS 27.63 61.19 17.64 41.92 19.97 53.41
ICL 40.92 62.59 20.95 25.86 28.23 43.84
LLaMA-13BZS 19.07 20.77 15.69 39.20 12.24 49.80
ICL 19.07 19.40 23.05 29.41 31.01 56.16
LLaMA-33BZS 17.94 18.24 20.20 32.98 13.59 48.39
ICL 33.71 45.04 31.01 56.90 24.40 40.78
LLaMA-65BZS 23.35 51.30 18.92 39.36 12.16 49.84
ICL 34.31 44.83 31.91 48.91 12.16 49.80
LLaMA-2-7BZS 9.38 28.36 12.23 49.94 11.86 49.35
ICL 17.56 18.22 25.07 52.38 22.59 28.23
LLaMA-2-13BZS 19.14 21.48 15.76 46.72 12.16 49.84
ICL 30.55 40.32 29.35 44.66 29.35 44.66
LLaMA-2-70BZS 21.92 26.45 20.34 29.45 15.09 44.63
ICL 28.9 38.38 29.35 40.18 25.07 33.05
LLaMA-2-7B-chatZS 23.72 29.47 16.96 32.8 21.54 23.93
ICL 20.04 23.77 26.95 43.04 22 27.69
LLaMA-2-13B-chatZS 22.07 25.32 22.29 27.25 22.29 42.44
ICL 27.7 34.74 29.5 45.08 29.5 45.07
LLaMA-2-70B-chatZS 23.04 26.98 18.91 20.18 20.57 32.11
ICL 31.15 40.15 28.68 37.4 21.7 25.56
Table 5: Detailed model performance on McTACO using all prompt templates. Strict Acc. andF1stand for strict
accuracy and the F-1 score as in (Zhou et al., 2019). pÒqindicates higher values are better.

--- PAGE 16 ---
Category Models T1 PAcc. T2 PAcc. T3 PAcc.
LLaMALLaMA-7B 8.9 2.3 0.09
LLaMA-Alpaca-7B 13.86 4.75 12.5
LLaMA-13B 0 4.82 10.99
LLaMA-33B 26.78 9.05 7.32
LLaMA-65B 15.54 33.98 13.54
LLaMA-2-7B 8.15 4.19 5.21
LLaMA-2-13B 15.67 10.83 22.28
LLaMA-2-70B 20.97 30.3 13.56
LLaMA-2-7B-chat 23.99 28.04 26.93
LLaMA-2-13B-chat 27.1 33.12 30.6
LLaMA-2-70B-chat 17.46 55.79 18.39
Table 6: Detailed Model performance evaluated on our curated CaTeRS benchmark with different prompt templates.
T1/2/3 andPAcc. stand for Template 1/2/3 and pair-wise accuracy.

--- PAGE 17 ---
Answer the question according to the article. Give step-by-step explanations and then answer yes or
no.
Article: Tired of being sidelined, Hungarian astronaut Bertalan Farkas is leaving for the United
States to start a new career, he said Saturday. ¨Being 48 is too early to be retired,ä fit-looking Farkas
said on state TV’s morning talk show. With American astronaut Jon McBride, Farkas set up an
American-Hungarian joint venture called Orion 1980, manufacturing space-travel related technology.
Farkas will move to the company’s U.S. headquarters. Farkas, an air force captain, was sent into
space on board the Soyuz 36 on May 26, 1980. He spent six days aboard the Salyut 6 spacecraft with
three Soviet astronauts, Valery Kubasov, Leonid Popov and Valery Riumin. McBride, 54, of Lewisburg,
West Virginia, was part of a seven-member crew aboard the Orbiter Challenger in October 1984 and
later served as assistant administrator for congressional relations for NASA. Farkas expressed the
hope he one day follow in the footsteps of fellow astronaut John Glenn, who at 77 is about to go into
space again. On May 22, 1995, Farkas was made a brigadier general, and the following year he was
appointed military attache at the Hungarian embassy in Washington. However, cited by District of
Columbia traffic police in December for driving under the influence of alcohol, Farkas was ordered
home and retired.
Question: Is Farkas sent into space on board the Soyuz before McBride on board the Orbiter Chal-
lenger?
The step-by-step explanation and answer is: Farkas was sent into space on May 26, 1980 and McBride
was on board the Orbiter Challenger in October 1984. May 26, 1980 is before October 1984. So the
answer is: yes.
Question: Is McBride on board the Orbiter Challenger after Farkas was made a brigadier general?
The step-by-step explanation and answer is: McBride was on board the Orbiter Challenger in October
1984. Farkas was made a brigadier general on May 22, 1995. October 1984 is before May 22, 1995.
So the answer is: no.
Question: Is Farkas was appointed military attache at the Hungarian embassy before he was made a
brigadier?
The step-by-step explanation and answer is: Farkas was made a brigadier general on May 22, 1995.
He was appointed military attache at the Hungarian embassy in the following year. So the answer is:
no.
Answer the question according to the article. Give step-by-step explanations and then answer yes or
no.
Article: {events}.
Question: {question}.
The step-by-step explanation and answer is:
Table 7: Example model input using Chain-of-thought prompts for TempEvalQA-Bi. The input consists of
demonstrations and the actual prompt.

--- PAGE 18 ---
Event: Well. I started this blog years ago and never kept up with it. In fact, I sort of forgot about it. Now,
I am done with school and married and I actually have time on my hands! (Gasp! Imagine that!) And, I
really want to get back into the swing of writing, so I thought blogging might be a good way to start. I
feel like an introduction is in order, so here’s a bit about me. My name is Kathleen. I am 22 years old,
and I live in Portland, Maine with my husband Brett. We are originally from Portland, Oregon but lived
in Seattle for 4 years during college. I work as an office manager in a small software company, so I tend
to have a bit of time on my hands. Hence, the blogging. I’ll probably talk a lot about all of the changes
that have happened in my life in the past year, since that’s what is on my mind right now: getting married
in July, moving to Maine in November, changing jobs several times, etc. So I will get right down to it. I
am pretty freaking grumpy today. Well, to be honest, I’ve been really grumpy for a few weeks and I can’t
seem to get un -grumpy, which I hate. Feeling out of control, especially of myself, is not something I like.
I guess I’m coming to realize that life, and especially marriage, is NOT easy. Maybe I assumed that since
we dated for over 4 years before we got married, that things would be easy. Everyone says that the first
year of marriage is the hardest, and for the first six months I laughed that off. ""Hard?"" I would scoff.
""No way! I love being married. It’s wonderful! It’s amazing! blah, blah, blah."" Now it’s like the crap
has hit the fan and I am transformed into a grumpy, nagging, mean, yelling wife. Ugh. I don’t want to be
like this but every time I resolve to be different I fall right back into it. Choosing your battles is a lot
harder than it seems. I think that’s a lot of what I’m learning; that I really do have to choose my battles,
because if I pick on every little thing, he won’t listen at all when the big things come along and I really
need him to listen. Heh. That’s my psychological epiphany for the day.
Q:Did the getting married happen after they dated?
LLaMA-65B: They dated forover 4years beforethey gotmarried. They dated forover 4years happened
beforethey gotmarried. So the answer is: yes.
GPT-4: Kath leen mentioned thatthey dated forover 4years beforethey gotmarried. So the answer is
yes.
Q:Did the getting married happen before they dated?
LLaMA-65B: Gettingmarriedhappened inJuly. Datinghappened beforegettingmarried. July isbefore
gettingmarried. So the answer is: no.
GPT-4: They dated forover 4years beforegettingmarried. So the answer is no.
Table 8: Example of chain-of-thought for LLaMA-65B. Chain-of-thought reasoning processes are highlighted. We
also highlight the answers from the models to indicate whether they are correct or incorrect. Both LLaMA-65B and
GPT-4 consistently provided correct answers for both questions.

--- PAGE 19 ---
Event: P .F . Chang’s China Bistro Inc. has provided new details about a security breach discovered
in June, saying customer data may have been stolen from 33 restaurants in 16 states, including
eight in California. The restaurant chain said Monday that credit card numbers, expiration dates
and, in some cases, cardholder names were stolen over eight months. However, the chain has not
yet determined if ""any specific cardholder’s credit or debit card data was stolen by the intruder,""
according to Chief Executive Rick Federico. P .F . Chang’s confirmed the data breach June 13, three
days after the U.S. Secret Service alerted the chain that its credit card processing systems may have
been hacked. The company said the breach occurred between Oct. 19, 2013, and June 11, 2014. The
intrusion was first reported by security blogger Brian Krebs, who said on his website that banks
reported data from thousands of customers had been pilfered from locations in Florida, Maryland,
New Jersey, Pennsylvania, Nevada and North Carolina, and was being sold online. Federico said P .F .
Chang’s conducted an internal investigation with the help of forensic data experts, and determined
the 33 locations affected and specific time frames the credit card processing system was compromised
for each. He also said the company has been processing card data securely since June 11.
Q:Did the determined happen before the processing?
LLaMA-65B: 1)The determined happened between Oct. 19,2013 andJune 11,2014. 2)The
processinghappened afterJune 11,2014. 3)Oct. 19,2013 andJune 11,2014 isbeforeJune 11,2014.
So the answer is: yes.
GPT-4: Thedeterminationoftheaffected locations andtime frames happened aftertheprocessing
wascompromised. So the answer is no.
Q:Did the determined happen after the processing?
LLaMA-65B: Thecompany said thebreach occurred between Oct. 19,2013, andJune 11,2014.
Thedetermined wasdetermined onJune 13. So the answer is: yes.
GPT-4: Thecompany determined theaffected locations andspecific time frames afterprocessing
thecard data securely. So the answer is yes.
Table 9: Example of chain-of-thought for LLaMA-65B. Chain-of-thought reasoning processes are highlighted.
We also highlight the answers from the models to indicate whether they are correct or incorrect. LLaMA-65B
successfully answered the first question accurately but exhibited inconsistency in answering the second question.
On the other hand, GPT-4 failed to provide correct answers for both questions.

--- PAGE 20 ---
Event: The Great Depression was a severe worldwide economic depression in the decade preceding
World War II. The timing of the Great Depression varied across nations, but in most countries it
started in 1930 and lasted until the late 1930s or middle 1940s. It was the longest, deepest, and most
widespread depression of the 20th century. In the 21st century, the Great Depression is commonly
used as an example of how far the world’s economy can decline. The depression originated in the
U.S., after the fall in stock prices that began around September 4, 1929, and became worldwide news
with the stock market crash of October 29, 1929 (known as Black Tuesday). The Great Depression
had devastating effects in countries rich and poor. Personal income, tax revenue, profits and prices
dropped, while international trade plunged by more than 50%. Unemployment in the U.S. rose
to 25%, and in some countries rose as high as 33%. Cities all around the world were hit hard,
especially those dependent on heavy industry. Construction was virtually halted in many countries.
Farming and rural areas suffered as crop prices fell by approximately 60%. Facing plummeting
demand with few alternate sources of jobs, areas dependent on primary sector industries such as
cash cropping, mining and logging suffered the most. Some economies started to recover by the
mid-1930s. In many countries, the negative effects of the Great Depression lasted until after the end
of World War II. Start. Economic historians usually attribute the start of the Great Depression to
the sudden devastating collapse of US stock market prices on October 29, 1929, known as Black
Tuesday; some dispute this conclusion, and see the stock crash as a symptom, rather than a cause, of
the Great Depression. . . .
Q:Did the depression start before middle 1940s?
LLaMA-65B: Thedepression started afterthefallinstock prices thatbeganaround Septem ber4,
1929. 2.Middle1940s isafterSeptem ber4,1929. 3. So the answer is: no.
GPT-4: TheGreat Depression started inthedecade precedingWorld War II,andinmost coun tries,
itbeganin1930. Itlasted untilthelate1930s ormiddle1940s. So the answer is yes.
Q:Did the depression start after middle 1940s?
LLaMA-65B: Thedepression originated intheU.S., afterthefallinstock prices thatbeganaround
Septem ber4,1929, andbecame world wide news with thestock marketcrash ofOctober29,1929
(known asBlack Tues day). So the answer is: no.
GPT-4: Thedepression started inthedecade precedingWorld War II,which means itbeganbefore
the1940s. Inmost coun tries, theGreat Depression started in1930 andlasted untilthelate1930s or
middle1940s. So the answer is no.
Table 10: Example of chain-of-thought for LLaMA-65B. Chain-of-thought reasoning processes are highlighted.
We also highlight the answers from the models to indicate whether they are correct or incorrect. LLaMA-65B
demonstrated accuracy in answering the second question while displaying inconsistency in answering the first
question. Conversely, GPT-4 provided correct responses for both questions.
