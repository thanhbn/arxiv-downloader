# 2309.08532.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/evolutionary-algorithms/2309.08532.pdf
# Kích thước file: 772802 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
EVOPROMPT: KẾT NỐI CÁC LLM VỚI THUẬT TOÁN TIẾN HÓA TẠO RA CÁC BỘ TỐI ỨU PROMPT MẠNH MẼ

Qingyan Guo12†∗, Rui Wang2†, Junliang Guo2, Bei Li23, Kaitao Song2, Xu Tan2‡,
Guoqing Liu2, Jiang Bian2, Yujiu Yang1‡
1Đại học Thanh Hoa 2Microsoft Research 3Đại học Northeastern
gqy22@mails.tsinghua.edu.cn, libei_neu@outlook.com,
{ruiwa,junliangguo,kaitaosong,xuta,guoqingliu,jiabia}@microsoft.com
yang.yujiu@sz.tsinghua.edu.cn

TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLM) xuất sắc trong nhiều nhiệm vụ khác nhau, nhưng chúng dựa vào các prompt được chế tạo cẩn thận thường đòi hỏi nỗ lực đáng kể từ con người. Để tự động hóa quá trình này, trong bài báo này, chúng tôi đề xuất một framework mới cho tối ưu hóa prompt rời rạc, gọi là EVOPROMPT, vay mượn ý tưởng của các thuật toán tiến hóa (EA) vì chúng thể hiện hiệu suất tốt và hội tụ nhanh. Để cho phép EA hoạt động trên các prompt rời rạc, là những biểu thức ngôn ngữ tự nhiên cần phải mạch lạc và dễ đọc cho con người, chúng tôi kết nối LLM với EA. Cách tiếp cận này cho phép chúng ta đồng thời tận dụng khả năng xử lý ngôn ngữ mạnh mẽ của LLM và hiệu suất tối ưu hóa hiệu quả của EA. Cụ thể, không cần bất kỳ gradient hoặc tham số nào, EVOPROMPT bắt đầu từ một tập hợp các prompt và lặp đi lặp lại sinh ra các prompt mới với LLM dựa trên các toán tử tiến hóa, cải thiện tập hợp dựa trên tập phát triển. Chúng tôi tối ưu hóa prompt cho cả LLM nguồn đóng và mở bao gồm GPT-3.5 và Alpaca, trên 31 tập dữ liệu bao gồm các nhiệm vụ hiểu ngôn ngữ, sinh và các nhiệm vụ BIG-Bench Hard (BBH). EVOPROMPT vượt trội đáng kể so với các prompt do con người thiết kế và các phương pháp hiện có cho sinh prompt tự động (ví dụ, lên đến 25% trên BBH). Hơn nữa, EVOPROMPT chứng minh rằng việc kết nối LLM với EA tạo ra hiệu ứng hiệp động, có thể truyền cảm hứng cho nghiên cứu sâu hơn về sự kết hợp giữa LLM và các thuật toán thông thường. Mã nguồn của chúng tôi có sẵn tại https://github.com/beeevita/EvoPrompt.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) cho thấy hiệu suất đáng chú ý trên nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) (Touvron et al., 2023; Ouyang et al., 2022). Để thích ứng với các nhiệm vụ downstream, việc đơn giản thêm một hướng dẫn vào văn bản đầu vào, còn gọi là prompt rời rạc, điều khiển LLM thực hiện nhiệm vụ mong muốn với tác động không đáng kể đến chi phí tính toán (Liu et al., 2023). Cách tiếp cận như vậy cũng loại bỏ nhu cầu về tất cả các tham số và gradient trong LLM, khiến nó phù hợp với LLM với API hộp đen như GPT-3 và GPT-4 (Brown et al., 2020; OpenAI, 2023). Mặc dù tiện lợi, hiệu suất của LLM đối với một nhiệm vụ cụ thể bị ảnh hưởng đáng kể bởi prompt (Liu et al., 2023; Zhu et al., 2023). Theo đó, thách thức chính của cách tiếp cận này nằm ở thiết kế prompt, đã nổi lên như một kỹ thuật quan trọng được gọi là kỹ thuật prompt (Zhou et al., 2022). Cho trước sự biến đổi rộng rãi trong prompt trên các mô hình ngôn ngữ và nhiệm vụ, thiết kế prompt thường đòi hỏi nỗ lực đáng kể từ con người và chuyên môn với các hướng dẫn chủ quan và tương đối hạn chế (Mishra et al., 2022a;b; Liu et al., 2023; Zamfirescu-Pereira et al., 2023; Wang et al., 2023).

∗Công việc được thực hiện trong thời gian thực tập tại Microsoft Research Asia.
†Đóng góp Bằng nhau.
‡Tác giả Liên hệ.
1arXiv:2309.08532v3 [cs.CL] 1 May 2025

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Để giảm bớt nỗ lực của con người trong thiết kế prompt rời rạc, các cách tiếp cận trước đây thường dựa vào quyền truy cập vào xác suất token từ lớp đầu ra của LLM, điều này có thể không phải lúc nào cũng có thể truy cập thông qua API (Deng et al., 2022; Zhang et al., 2023a). Một số công trình gần đây xem xét việc liệt kê các prompt đa dạng và chọn những prompt tốt nhất (Zhou et al., 2022; Jiang et al., 2020), hoặc sửa đổi các prompt hiện tại để cải thiện chúng (Guo et al., 2023; Prasad et al., 2022; Pryzant et al., 2023). Các cách tiếp cận như vậy hoặc nhấn mạnh khám phá các prompt đa dạng, có thể dẫn đến sự do dự và lãng phí tài nguyên, hoặc tập trung vào khai thác các prompt tốt được xác định hiện tại, có thể dẫn đến trì trệ và giới hạn việc tìm kiếm ở các cực trị cục bộ. Một số thuật toán thông thường không có đạo hàm được thiết kế tốt và đạt được sự cân bằng tốt giữa khám phá và khai thác (Conn et al., 2009; Rios & Sahinidis, 2013). Trong số này, các thuật toán tiến hóa (EA) nổi bật vì chúng đơn giản và hiệu quả, cũng như phù hợp cho tối ưu hóa prompt rời rạc (Storn & Price, 1997; Brest et al., 2006; Zhang & Sanderson, 2009; Vesterstrom & Thomsen, 2004). Các chuỗi cụm từ trong prompt có thể được coi như chuỗi gen trong EA điển hình, khiến chúng tương thích với quá trình tiến hóa tự nhiên.

Trong bài báo này, chúng tôi mượn ý tưởng của EA và đề xuất một framework điều chỉnh prompt rời rạc, EVOPROMPT. Trong khi các toán tử tiến hóa trong EA thường được thiết kế cho chuỗi, chúng có xu hướng thay đổi các token một cách độc lập để tạo ra các giải pháp ứng viên mới. Thật không may, cách tiếp cận này bỏ qua các kết nối giữa các token, điều này rất quan trọng để duy trì sự mạch lạc và khả năng đọc trong prompt. Tận dụng chuyên môn của LLM trong NLP và khả năng tối ưu hóa đặc biệt của EA, chúng tôi kết nối hai cách tiếp cận này, nơi LLM tạo ra các prompt ứng viên mới theo các toán tử tiến hóa, và EA hướng dẫn quá trình tối ưu hóa để giữ lại các prompt tối ưu.

Cụ thể, dựa trên một số prompt ban đầu, chúng tôi sử dụng LLM để hoạt động như các toán tử tiến hóa nhằm tạo ra các ứng viên prompt mới, và prompt có hiệu suất tốt hơn trên tập phát triển được bảo tồn. Các thao tác trên việc cập nhật tập hợp được áp dụng lặp đi lặp lại để cải thiện chất lượng. Bằng cách thiết kế tỉ mỉ các toán tử tiến hóa và điều chỉnh chiến lược cập nhật, EVOPROMPT có thể được cụ thể hóa với nhiều loại EA khác nhau. Chúng tôi tối ưu hóa prompt cho hai LLM khác nhau (tức là, Alpaca (Taori et al., 2023), và GPT-3.5 (Brown et al., 2020)) trên một phạm vi đa dạng các nhiệm vụ hiểu và sinh ngôn ngữ thần kinh, cũng như các nhiệm vụ BIG-Bench thách thức, sử dụng tổng cộng 31 tập dữ liệu. EVOPROMPT luôn đạt được prompt tốt hơn so với cả những prompt được thiết kế thủ công và các phương pháp sinh prompt tự động trước đây. Các đóng góp chính của bài báo này bao gồm:

• Chúng tôi đề xuất một framework mới cho tối ưu hóa prompt rời rạc tự động kết nối LLM và EA, gọi là EVOPROMPT, có các ưu điểm sau: 1) Nó không yêu cầu quyền truy cập vào bất kỳ tham số hoặc gradient nào của LLM; 2) Nó cân bằng giữa khám phá và khai thác dẫn đến kết quả tốt hơn; 3) Các prompt được tạo ra có thể đọc được bởi con người.

• Các thí nghiệm được thực hiện trên 31 tập dữ liệu chứng minh hiệu quả của EVOPROMPT so với các prompt được chế tạo, cũng như các phương pháp hiện có. Chúng tôi phát hành các prompt tối ưu có được bởi EVOPROMPT cho các nhiệm vụ phổ biến như phân loại cảm xúc, phân loại chủ đề, phân loại chủ quan, đơn giản hóa, tóm tắt và lý luận.

• Chúng tôi chứng minh rằng LLM có khả năng thực hiện nhiều loại EA được cung cấp với các hướng dẫn phù hợp. Chúng tôi hy vọng rằng các khám phá của chúng tôi sẽ truyền cảm hứng cho các nghiên cứu sâu hơn về sự kết hợp giữa LLM và các thuật toán thông thường, mở đường cho các ứng dụng mới và sáng tạo của LLM.

2 CÁC CÔNG TRÌNH LIÊN QUAN
Prompt trong LLM Prompting là một phương pháp hiệu quả để sử dụng LLM trong các nhiệm vụ chuyên biệt. Tuy nhiên, hiệu suất bị ảnh hưởng mạnh bởi lựa chọn prompt. Gần đây, tối ưu hóa prompt tự động đã nhận được sự chú ý rộng rãi. Các phương pháp dựa trên prompt liên tục, chỉ điều chỉnh tham số của một số token đầu vào (Li & Liang, 2021; Liu et al., 2021b;a; Zhang et al., 2021) thu hút nhiều sự chú ý. Mặc dù hiệu suất hiệu quả của chúng, hai nhược điểm của các mô hình như vậy không thể bỏ qua: 1) Việc tối ưu hóa prompt liên tục yêu cầu tham số của LLM không thể truy cập được đối với API hộp đen. 2) Prompt mềm thường thiếu khả năng diễn giải (Lester et al., 2021). Prompt rời rạc, đơn giản thêm một số token rời rạc, như "It was" (Schick & Schütze, 2021), hoặc các hướng dẫn mô tả cụ thể cho nhiệm vụ, như "Classify the comment into positive or negative.", vào văn bản đầu vào, có thể cung cấp giao diện tương tác với con người với khả năng diễn giải tốt hơn và cho thấy hiệu suất hứa hẹn trong nhiều nhiệm vụ NLP khác nhau (Liu et al., 2023).

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Prompt Rời rạc Nhiều cách tiếp cận khác nhau đã được đề xuất cho tìm kiếm và sinh prompt rời rạc tự động (Shin et al., 2020; Shi et al., 2022; Wallace et al., 2019; Deng et al., 2022; Zhang et al., 2023a), trong khi các phương pháp này vẫn dựa vào gradient hoặc xác suất token từ lớp đầu ra. Gần đây hơn, xem xét phương sai cao của các prompt khác nhau cho các nhiệm vụ downstream, một số công trình tập trung vào khám phá bằng cách liệt kê và chọn prompt tốt nhất từ một số ứng viên, chủ yếu được tăng cường bởi tái lấy mẫu (Zhou et al., 2022; Jiang et al., 2020). Các cách tiếp cận dựa trên chỉnh sửa prompt (Zhang et al., 2023a; Prasad et al., 2022) nhấn mạnh khai thác, có thể dẫn đến cực trị cục bộ. Một cách tiếp cận khác thu thập các trường hợp dự đoán sai và phân tích nguyên nhân gốc rễ tương ứng để cải thiện prompt hiện có (Pryzant et al., 2023; Guo et al., 2023), cũng nhấn mạnh khai thác. Ngoài ra, các cách tiếp cận như vậy bị giới hạn đối với các nhiệm vụ có câu trả lời chuẩn và không thể được áp dụng trực tiếp cho các nhiệm vụ sinh. EVOPROMPT đề xuất của chúng tôi được trao quyền với các thuật toán tiến hóa cân bằng giữa khám phá và khai thác mà không yêu cầu bất kỳ tham số hoặc gradient nào.

LLM và Thuật toán Tối ưu hóa LLM chứng minh tiềm năng phục vụ như các bộ tối ưu hóa hộp đen (Zheng et al., 2023); tuy nhiên, cách tiếp cận hộp đen này thiếu khả năng giải thích. Một số công trình đã tiết lộ rằng LLM có khả năng mô phỏng các thao tác cụ thể trong các thuật toán thông thường. Ví dụ, LLM có thể thực hiện "Gradient Descent" trong không gian rời rạc bằng cách thu thập các mẫu dự đoán sai (Pryzant et al., 2023; Guo et al., 2023). Trong khi đó, đã được chứng minh rằng LLM có thể mô phỏng toán tử đột biến (Lehman et al., 2022) hoặc lai ghép (Meyerson et al., 2023) trong thuật toán di truyền (GA). Chen et al. (2023) tiếp tục tích hợp LLM và GA cho tìm kiếm kiến trúc mạng thần kinh, trong khi Lanzi & Loiacono (2023) giới thiệu một cách tiếp cận tương tự cho thiết kế trò chơi. Công trình của chúng tôi đã tiến một bước quan trọng bằng cách đề xuất một framework tổng quát kết nối LLM với các thuật toán tiến hóa, có thể được cụ thể hóa thành một phạm vi đa dạng các thuật toán tiến hóa thông qua tùy chỉnh các quá trình tiến hóa và lựa chọn, từ đó mở rộng khả năng áp dụng và ảnh hưởng tiềm năng trong lĩnh vực. Chúng tôi mong muốn công trình này truyền cảm hứng cho các ứng dụng rộng hơn của việc kết hợp LLM và các thuật toán thông thường.

3 TỐI ƯU HÓA PROMPT RỜI RẠC TỰ ĐỘNG
Thuật toán 1 Tối ưu hóa prompt rời rạc: EVOPROMPT
Yêu cầu: Prompt ban đầu P0={p1, p2, . . . , pN}, kích thước tập hợp N, một tập dev D, fD(·) biểu thị điểm của một prompt trên LLM mong muốn được đánh giá trên D, một số lần lặp T được định trước, các toán tử tiến hóa được thiết kế cẩn thận để tạo ra một prompt mới Evo(·)
1: Điểm đánh giá ban đầu: S0 ← {si=fD(pi)|i∈[1, N]}
2: for t = 1 to T do
3:    Lựa chọn: chọn một số lượng nhất định prompt từ tập hợp hiện tại như prompt cha mẹ pr1, . . . , prk ∼ Pt−1
4:    Tiến hóa: tạo ra một prompt mới dựa trên các prompt cha mẹ đã chọn bằng cách tận dụng LLM để thực hiện các toán tử tiến hóa p′i ← Evo(pr1, . . . , prk)
5:    Đánh giá: s′i ← f(p′i, D)
6:    Cập nhật: Pt ← {Pt−1, p′i} và St ← {St−1, s′i} dựa trên điểm đánh giá
7: end for
8: Trả về prompt tốt nhất, p∗, trong tập hợp cuối cùng PT: p∗ ← argmax p∈PT f(p, D)

Các LLM tiên tiến hiện tại thường được tương tác qua API hộp đen, trong khi gradient và tham số không thể truy cập được. Các thuật toán tiến hóa (EA) là các thuật toán không có đạo hàm với độ chính xác đặc biệt và hội tụ nhanh. Theo đó, chúng tôi xem xét việc giới thiệu EA vào tối ưu hóa prompt rời rạc. Tuy nhiên, để tạo ra các giải pháp ứng viên mới, các toán tử tiến hóa thường chỉnh sửa các phần tử trong giải pháp hiện tại một cách độc lập, mà không xem xét các kết nối giữa chúng. Điều này khiến việc áp dụng các toán tử tiến hóa trên prompt rời rạc trở nên thách thức, vốn yêu cầu sự mạch lạc và khả năng đọc. Để giải quyết thách thức này, chúng tôi đề xuất một cách tiếp cận hiệp động kết nối chuyên môn xử lý ngôn ngữ tự nhiên của LLM với khả năng tối ưu hóa của EA, gọi là EVOPROMPT. Cụ thể, LLM tạo ra các prompt ứng viên mới dựa trên các toán tử tiến hóa, trong khi EA hướng dẫn quá trình tối ưu hóa để tìm ra các prompt tối ưu.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Thuật toán Di truyền (GA) Được thực hiện bởi LLM
Truy vấn: Vui lòng làm theo hướng dẫn từng bước để tạo ra một prompt tốt hơn.
1. Lai ghép các prompt sau và tạo ra một prompt mới:
2. Đột biến prompt được tạo ra ở Bước 1 và tạo ra prompt cuối cùng được đặt trong <prompt> và </prompt>.
Phản hồi:
Prompt 2: Gán nhãn cảm xúc cho câu đã cho từ ['negative', 'positive'] và chỉ trả về nhãn mà không có văn bản nào khác.
Prompt 1: Bây giờ bạn là một bộ phân loại, nhiệm vụ của bạn là xác định cảm xúc của văn bản được cung cấp, có thể là thuận lợi hoặc bất lợi.

Lai ghép
1. Prompt lai ghép: Nhiệm vụ của bạn là xác định cảm xúc của văn bản được cung cấp và gán nhãn cảm xúc từ ['negative', 'positive'].
2. <prompt>Xác định cảm xúc của câu đã cho và gán nhãn từ ['negative', 'positive'].</prompt>
Đột biến

Hình 1: Quá trình GA được thực hiện bởi LLM (Evo(·) trong Thuật toán 1). Ở Bước 1, LLM thực hiện lai ghép trên hai prompt đã cho (các từ màu cam và xanh được kế thừa từ Prompt 1 và Prompt 2, tương ứng). Ở Bước 2, LLM thực hiện đột biến trên prompt.

Để thực hiện EVOPROMPT trong thực tế, cần phải cụ thể hóa nó với một thuật toán cụ thể của EA. Có nhiều loại EA khác nhau, và trong bài báo này, chúng tôi xem xét hai thuật toán được sử dụng rộng rãi, bao gồm Thuật toán Di truyền (GA) (Holland, 1975) và Tiến hóa Vi phân (DE) (Storn & Price, 1997). GA là một trong những thuật toán tiến hóa được đánh giá cao nhất (Holland, 1975; 1992; Mitchell, 1998; Mirjalili et al., 2020) và DE đã nổi lên như một trong những thuật toán được sử dụng rộng rãi nhất cho các thách thức tối ưu hóa phức tạp kể từ khi ra đời (Storn & Price, 1997; Price, 2013; Das & Suganthan, 2010; Pant et al., 2020). Trong phần tiếp theo, chúng tôi sẽ trước tiên phác thảo EVOPROMPT được đề xuất, sau đó cụ thể hóa EVOPROMPT với GA và DE tương ứng.

3.1 FRAMEWORK CỦA EVOPROMPT
EA thường bắt đầu với một tập hợp ban đầu gồm N giải pháp (prompt trong thiết lập của chúng tôi), sau đó lặp đi lặp lại tạo ra các giải pháp mới bằng cách sử dụng các toán tử tiến hóa (ví dụ, đột biến và lai ghép) trên tập hợp hiện tại và cập nhật nó dựa trên hàm fitness. Theo EA điển hình, EVOPROMPT chủ yếu chứa ba bước:

• Tập hợp ban đầu: Trái ngược với hầu hết các phương pháp prompt tự động hiện có bỏ qua kiến thức ưu tiên của con người, chúng tôi áp dụng các prompt thủ công có sẵn như tập hợp ban đầu để tận dụng trí khôn của con người. Bên cạnh đó, EA thường bắt đầu từ các giải pháp ngẫu nhiên, dẫn đến một tập hợp đa dạng và tránh bị mắc kẹt trong một cực trị cục bộ. Theo đó, chúng tôi cũng đưa một số prompt được tạo ra bởi LLM (Zhou et al., 2022) vào tập hợp ban đầu.

• Tiến hóa: Trong mỗi lần lặp, EVOPROMPT sử dụng LLM như các toán tử tiến hóa để tạo ra một prompt mới dựa trên một số prompt cha mẹ được chọn từ tập hợp hiện tại. Để thực hiện điều này, chúng tôi thiết kế các bước của các toán tử đột biến và lai ghép cho mỗi loại EA cụ thể, cùng với các hướng dẫn tương ứng để hướng dẫn LLM tạo ra prompt mới dựa trên các bước này.

• Cập nhật: Chúng tôi đánh giá các prompt ứng viên được tạo ra trên tập phát triển và giữ lại những prompt có hiệu suất vượt trội, tương tự như sự sống sót của kẻ mạnh nhất trong tự nhiên. Chiến lược cập nhật cụ thể có thể thay đổi tùy thuộc vào loại EA được sử dụng.

Thuật toán dừng khi số lần lặp đạt đến một giá trị định trước. Chi tiết của EVOPROMPT được nêu trong Thuật toán 1. Khi cụ thể hóa EVOPROMPT với một thuật toán cụ thể của EA, các quá trình tiến hóa cần được điều chỉnh, và thách thức chính là thiết kế các toán tử tiến hóa trên prompt rời rạc.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
"tweet"-> "review"
"Categorize"-> "Analyze"
"Sentiment analysis"-> "Sentiment identification"
Thuật toán Tiến hóa Vi phân (DE) Được thực hiện bởi LLM

b−c
F(b−c)
a+F(b−c)
Lai ghép
4. Lai ghép prompt ở Bước 3 với prompt cơ bản sau và tạo ra prompt cuối cùng được đặt trong <prompt> và </prompt>:
3. Kết hợp các phần khác nhau với Prompt 3, thay thế có chọn lọc với các phần khác nhau trong Bước 2 và tạo ra prompt mới:
Prompt 1: Phân loại tweet theo cảm xúc tích cực hay tiêu cực.
Prompt 2: Thực hiện phân tích cảm xúc cho từng câu để quyết định xem nó tích cực hay tiêu cực.
Các phần khác nhau:
"tweet" vs "sentence"
"Categorize" vs "Carry out sentiment analysis"
Prompt 3: Trong nhiệm vụ này, bạn được cung cấp các câu từ đánh giá sản phẩm. Nhiệm vụ là phân loại một câu là tích cực hoặc tiêu cực.

Truy vấn: Vui lòng làm theo hướng dẫn từng bước để tạo ra một prompt tốt hơn.
1. Xác định các phần khác nhau giữa Prompt 1 và Prompt 2:
2. Đột biến ngẫu nhiên các phần khác nhau
3. Kết hợp các phần khác nhau với Prompt 3, thay thế có chọn lọc với các phần khác nhau trong Bước 2 và tạo ra prompt mới.
4. Lai ghép prompt ở Bước 3 với prompt cơ bản sau và tạo ra prompt cuối cùng được đặt trong <prompt> và </prompt>:
Prompt Cơ bản: Ở đây, bạn sẽ được cung cấp các câu từ đánh giá về sản phẩm và bạn sẽ cần quyết định xem đó là đánh giá tích cực hay tiêu cực.
Phản hồi:
1. 2. 3. 4.

Prompt Mới: Trong nhiệm vụ này, bạn được cung cấp các đánh giá về sản phẩm. Nhiệm vụ là phân tích từng đánh giá và xác định xem nó tích cực hay tiêu cực.
Prompt Cuối cùng: <prompt>Ở đây, bạn sẽ được cung cấp các đánh giá về sản phẩm và bạn sẽ cần phân tích từng đánh giá và xác định xem nó tích cực hay tiêu cực.</prompt>

Hình 2: Quá trình DE được thực hiện bởi LLM (Evo(·) trong Thuật toán 1). Ở Bước 1, LLM tìm ra các phần khác nhau (các từ màu ■ và ■) giữa Prompt 1 và Prompt 2 (b−c trong DE điển hình). Ở Bước 2, LLM thực hiện đột biến (các từ màu ■) trên chúng (mô phỏng F(b−c)). Tiếp theo, LLM kết hợp prompt tốt nhất hiện tại như Prompt 3 với kết quả đột biến ở Bước 2, để tạo ra prompt mới (tương đương với a+F(b−c) trong DE). Cuối cùng, LLM thực hiện lai ghép trên prompt cơ bản hiện tại pi và prompt được tạo ra ở Bước 3. Xem Hình 5 trong Phụ lục B.2 để có phản hồi đầy đủ.

3.2 CỤ THỂ HÓA VỚI THUẬT TOÁN DI TRUYỀN

Lựa chọn Trong GA, các giải pháp cha mẹ thường được chọn bằng phương pháp lựa chọn bánh xe roulette, được hướng dẫn bởi giá trị fitness của chúng (Lipowski & Lipowska, 2012). Tương tự, chúng tôi sử dụng lựa chọn bánh xe roulette để chọn hai prompt cha mẹ từ tập hợp hiện tại, dựa trên điểm hiệu suất của chúng thu được trên tập phát triển. Gọi si là điểm hiệu suất của prompt thứ i trong một tập hợp chứa N prompt. Xác suất chọn prompt thứ i làm cha mẹ có thể được biểu thị như pi=si/∑Nj=1sj.

Tiến hóa Tuân thủ framework GA, chúng tôi tạo ra một prompt ứng viên mới qua hai bước: 1) Lai ghép được thực hiện giữa các prompt cha mẹ để tạo ra một prompt con mới kế thừa đặc điểm từ cả hai cha mẹ; 2) Đột biến được áp dụng cho prompt con, đưa vào các thay đổi ngẫu nhiên cho các yếu tố nhất định. Chúng tôi chính thức hóa thao tác hai giai đoạn này thành các hướng dẫn thuật toán để hướng dẫn LLM thực hiện Evo(·) trong Thuật toán 1. Toàn bộ quá trình được minh họa trong Hình 1.

Cập nhật Chúng tôi sử dụng một chiến lược lựa chọn đơn giản để cập nhật tập hợp: tại mỗi lần lặp, EVOPROMPT tạo ra N prompt mới, được hợp nhất với tập hợp hiện có gồm N prompt. Sau đó, N prompt hàng đầu, dựa trên điểm của chúng, được giữ lại để tạo thành tập hợp được cập nhật. Theo đó, chất lượng tổng thể của tập hợp trải qua sự cải thiện liên tục, đi đến đỉnh điểm trong việc chọn prompt tốt nhất trong tập hợp cuối cùng như prompt tối ưu.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

3.3 CỤ THỂ HÓA VỚI TIẾN HÓA VI PHÂN

Ở đây, chúng tôi bắt đầu với một số kiến thức sơ bộ về DE. Không giống như GA, các giải pháp của DE được biểu diễn bởi các vector số. Mỗi vector trong tập hợp được chọn tuần tự như một vector cơ sở, được ký hiệu là x, sau đó trải qua đột biến và lai ghép. Trong quá trình đột biến, một giải pháp đột biến y được tạo ra từ một giải pháp được chọn ngẫu nhiên a từ tập hợp hiện tại. Đột biến được thực hiện bằng cách thêm một sự khác biệt được thu nhỏ giữa hai giải pháp riêng biệt được chọn ngẫu nhiên b và c vào a, tức là y=a+F(b−c), trong đó F là tham số thu nhỏ.

Lai ghép là để tạo ra một giải pháp thử x′=[x′1, ..., x′n] bằng cách chọn mỗi tham số trong vector từ giải pháp cơ bản x hoặc giải pháp đột biến y. Sau đó, x được thay thế bằng x′ nếu x′ tốt hơn x. Trong quá trình tiến hóa từng bước, DE kết thúc với một tập hợp chất lượng cao. Một phiên bản được sửa đổi của DE sử dụng giải pháp tốt nhất hiện tại như vector a để khai thác thông tin từ cái tốt nhất.

Tiến hóa Quá trình tiến hóa của DE có thể được tách ra thành ba bước: 1) F(b−c); 2) y=a+F(b−c); 3) Lai ghép của x và y. Trong EVOPROMPT dựa trên DE, chúng tôi tuân theo ba bước để thiết kế quá trình tiến hóa, cũng như các hướng dẫn tương ứng để LLM tạo ra prompt mới dựa trên các bước này như được minh họa trong Hình 2:

• Được truyền cảm hứng từ vector vi phân trong DE, chúng tôi xem xét việc đột biến chỉ các phần khác nhau của hai prompt được chọn ngẫu nhiên trong tập hợp hiện tại (Bước 1 và Bước 2 trong Hình 2). Các prompt trong tập hợp hiện tại được coi là những cái tốt nhất hiện tại. Theo đó, các thành phần chung của hai prompt có xu hướng có tác động tích cực đến hiệu suất, và do đó cần được bảo tồn.

• Một biến thể của DE sử dụng vector tốt nhất hiện tại trong quá trình đột biến, nơi một vector đột biến được tạo ra bằng cách thêm tỷ lệ của vector vi phân vào vector tốt nhất hiện tại. Dựa trên ý tưởng này, chúng tôi tạo ra một prompt đột biến bằng cách thay thế có chọn lọc các phần của prompt tốt nhất hiện tại bằng các phần khác nhau đột biến để kết hợp. (Bước 3 trong Hình 2).

• Lai ghép thay thế các thành phần nhất định của prompt cơ bản (tức là, một ứng viên của tập hợp hiện tại) bằng các đoạn từ prompt đột biến. Thao tác này kết hợp các tính năng của hai prompt khác nhau, có thể tạo ra một giải pháp mới và cải thiện (Bước 4 trong Hình 2).

Cập nhật Theo DE chuẩn, mỗi prompt pi trong tập hợp hiện tại được chọn như một prompt cơ bản lần lượt để tạo ra prompt mới tương ứng p′i sử dụng hướng dẫn trong Hình 2. Sau đó, prompt có điểm cao hơn, pi hoặc p′i, được giữ lại. Theo đó, kích thước tập hợp vẫn không đổi trong khi chất lượng tổng thể của tập hợp được cải thiện.

4 THÍ NGHIỆM

4.1 CHI TIẾT THỰC HIỆN VÀ BASELINE

Với GPT-3.5 thực hiện các toán tử tiến hóa, chúng tôi tối ưu hóa prompt bằng EVOPROMPT cho Alpaca-7b nguồn mở (Taori et al., 2023) và GPT-3.5 nguồn đóng (text-davinci-003) (Brown et al., 2020). Chúng tôi chọn prompt có điểm cao nhất trên tập phát triển và báo cáo điểm của nó trên tập kiểm tra. Kết quả báo cáo trên Alpaca được tính trung bình trên 3 seed ngẫu nhiên và độ lệch chuẩn được cung cấp, trong khi đối với GPT-3.5, chúng tôi báo cáo kết quả của một seed do hạn chế ngân sách. Trong đánh giá của chúng tôi, chúng tôi so sánh EVOPROMPT với ba loại cách tiếp cận dựa trên prompt, được mô tả chi tiết như sau:

• Hướng dẫn Thủ công (MI): Chúng phục vụ như hướng dẫn cụ thể cho nhiệm vụ và được chế tạo dựa trên các công trình đã thiết lập, cụ thể được tham khảo từ Zhang et al. (2023b) cho hiểu ngôn ngữ, Sanh et al. (2021) cho tóm tắt, và Zhang et al. (2023c) cho đơn giản hóa văn bản.

• PromptSource (Bach et al., 2022) và Natural Instructions (NI) (Mishra et al., 2022b): Các kho lưu trữ này tổng hợp các prompt do con người soạn thảo trên một phạm vi đa dạng các tập dữ liệu.

• APE (Zhou et al., 2022) và APO (Pryzant et al., 2023): APE sử dụng một chiến lược Tìm kiếm Monte Carlo lặp đi lặp lại, nhấn mạnh vào khám phá. Chúng tôi tái tạo nó và khởi tạo tập hợp có kích thước tương đương với EVOPROMPT. APO khai thác các trường hợp dự đoán sai như "pseudo-gradient" để cải thiện lặp đi lặp lại prompt gốc, nhấn mạnh vào khai thác. Chúng tôi tái tạo APO trên các nhiệm vụ phân loại nhị phân với prompt thủ công tối ưu như prompt ban đầu.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Bảng 1 showing results for language understanding tasks on Alpaca-7b, with columns for different methods (MI, NI, PromptSource, APE, APO, EVOPROMPT) and rows for different datasets (SST-2, CR, MR, SST-5, AG's News, TREC, Subj)]

[THIS IS TABLE: Bảng 2 showing results for SAMSum dataset (summarization task) comparing different methods across ROUGE metrics for both Alpaca-7b and GPT-3.5]

4.2 HIỂU NGÔN NGỮ

Tập dữ liệu và Thiết lập Chúng tôi đầu tiên tiến hành thí nghiệm trên các nhiệm vụ hiểu ngôn ngữ trên 7 tập dữ liệu để xác thực các phương pháp của chúng tôi, bao gồm phân loại cảm xúc (SST-2 (Socher et al., 2013), MR (PANG, 2005), CR (Hu & Liu, 2004), SST-5 (Socher et al., 2013)), phân loại chủ đề (AG's News (Zhang et al., 2015), TREC (Voorhees & Tice, 2000)) và phân loại chủ quan (Subj (Pang & Lee, 2004)). Để hạn chế không gian nhãn đầu ra, chúng tôi đặt trước minh chứng gồm một ví dụ cho mỗi lớp trước trường hợp kiểm tra. Xem Phụ lục B để biết thêm chi tiết.

Kết quả chính Bảng 1 cho thấy: 1) So với các công trình trước đây về sinh prompt và hướng dẫn viết bằng tay, EVOPROMPT dựa trên cả GA và DE đạt kết quả tốt hơn đáng kể. 2) EVOPROMPT (GA) tốt hơn một chút so với EVOPROMPT (DE) trên các tập dữ liệu phân loại cảm xúc. Khi nói đến các tập dữ liệu phân loại chủ đề, EVOPROMPT (DE) hoạt động tốt hơn. Đáng chú ý, trên nhiệm vụ phân loại chủ quan (Subj), EVOPROMPT (DE) thể hiện sự cải thiện đáng kể so với đối tác GA của nó, đạt được lợi thế độ chính xác 5%. Điều này có thể được đóng góp bởi khả năng đặc biệt của DE để tránh cực trị cục bộ khi các prompt ban đầu không có chất lượng cao.

4.3 SINH NGÔN NGỮ

[THIS IS TABLE: Bảng 3 showing results for text simplification (ASSET) comparing different methods for both Alpaca-7b and GPT-3.5]

Tập dữ liệu và Thiết lập Đối với sinh ngôn ngữ, chúng tôi đánh giá EVOPROMPT của chúng tôi trên các nhiệm vụ tóm tắt văn bản và đơn giản hóa. Đối với tóm tắt, chúng tôi sử dụng SAMSum (Gliwa et al., 2019), một tập dữ liệu tóm tắt đối thoại thách thức và phức tạp, và báo cáo điểm ROUGE-1/2/L trên Alpaca-7b và GPT-3.5. Đối với đơn giản hóa văn bản, nhằm đơn giản hóa văn bản nguồn trong khi bảo tồn ý nghĩa gốc của nó, chúng tôi sử dụng tập dữ liệu ASSET (Alva-Manchego et al., 2020), một benchmark được biết đến với nhiều bản dịch tham khảo. Chúng tôi áp dụng điểm SARI (Xu et al., 2016) như thước đo đánh giá, một hệ thống chấm điểm dựa trên n-gram được sử dụng rộng rãi cho các nhiệm vụ chỉnh sửa văn bản.

Kết quả chính Kết quả tóm tắt và đơn giản hóa được trình bày trong Bảng 2 và 3. EVOPROMPT đạt được cải thiện hiệu suất đáng kể so với các prompt được thiết kế thủ công, thể hiện cải thiện hơn 3 điểm trong điểm SARI trên cả Alpaca và GPT-3.5 API. Hơn nữa, EVOPROMPT luôn vượt trội so với cách tiếp cận APE trong các tình huống được đánh giá, cho thấy rằng các prompt được tạo ra khai thác hiệu quả khả năng của LLM để có hiệu suất vượt trội.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[THIS IS FIGURE: Hình 3 showing normalized scores on BBH tasks for EVOPROMPT (GA) and EVOPROMPT (DE) across different task IDs]

Hơn nữa, EVOPROMPT (DE) đáng chú ý vượt trội so với EVOPROMPT (GA) trong nhiệm vụ tóm tắt, trong khi thể hiện hiệu suất tương đương trong nhiệm vụ đơn giản hóa văn bản. Điều này cho thấy rằng biến thể DE đặc biệt hiệu quả cho các nhiệm vụ sinh ngôn ngữ phức tạp hơn như tóm tắt.

4.4 BIGBENCH HARD (BBH)

Tập dữ liệu và Thiết lập Để xác thực các phương pháp của chúng tôi trên các nhiệm vụ đa dạng, chúng tôi áp dụng BBH (Suzgun et al., 2022) bao gồm một bộ 23 nhiệm vụ BIG-Bench thách thức yêu cầu lý luận nhiều bước. Vì các nhiệm vụ này thách thức, chúng tôi tập trung vào việc tối ưu hóa prompt cho GPT-3.5. Chúng tôi lấy mẫu một tập con từ tập kiểm tra làm tập phát triển và báo cáo điểm số chuẩn hóa¹ so với prompt "Let's think step by step." (Kojima et al., 2022) với các minh chứng Chain-of-Thought 3-shot (theo Fu et al. (2023)) trên tập kiểm tra. Chúng tôi sử dụng ID nhiệm vụ để đơn giản hóa ký hiệu của mỗi nhiệm vụ và loại bỏ một nhiệm vụ vì độ chính xác đã đạt 100% với prompt thủ công. Vui lòng xem Phụ lục C.2 và Bảng 17 để biết chi tiết, cũng như so sánh thêm với các công trình trước đây.

Kết quả chính EVOPROMPT có được prompt tốt hơn cho tất cả 22 nhiệm vụ (Hình 3). Cụ thể, EVOPROMPT (DE) đạt được cải thiện lên đến 25% với trung bình 3.5%, trong khi EVOPROMPT (GA) đạt được cải thiện đỉnh 15% với trung bình 2.5%. Mặc dù đối với một số nhiệm vụ, đối tác GA vượt trội so với phiên bản DE, khoảng cách hiệu suất vẫn tương đối nhỏ (tức là khoảng 1%). Trong khi đó, EVOPROMPT (DE) vượt trội so với EVOPROMPT (GA) hơn 2% trên 6 nhiệm vụ. Theo đó, phiên bản DE thường là lựa chọn tốt cho các nhiệm vụ thách thức này.

5 PHÂN TÍCH

5.1 THIẾT KẾ TRONG GA

[THIS IS TABLE: Bảng 4 showing designs in EVOPROMPT (GA) with different selection strategies]

Đối với EVOPROMPT (GA), chúng tôi áp dụng chiến lược lựa chọn bánh xe roulette theo mặc định để chọn prompt cha mẹ, đóng góp vào con cái. Để khám phá thêm về tác động của các chiến lược lựa chọn khác nhau, chúng tôi so sánh cách tiếp cận của chúng tôi với hai chiến lược phổ biến khác, tức là tournament (Wikipedia contributors, 2023) và lựa chọn ngẫu nhiên, như được trình bày trong Bảng 4. Chúng tôi quan sát thấy rằng EVOPROMPT (GA) với bánh xe roulette đạt được điểm số cao hơn, thể hiện hiệu quả của phương pháp lựa chọn này.

5.2 THIẾT KẾ TRONG DE

Đối với EVOPROMPT (DE), chúng tôi đi sâu vào hai cân nhắc thiết kế chính trong việc thích ứng các toán tử tiến hóa của DE với prompt rời rạc: 1) đột biến trên các phần khác nhau, và 2) chọn prompt có hiệu suất cao nhất hiện tại như "Prompt 3" trong Hình 2. Chúng tôi đánh giá tác động của các lựa chọn thiết kế này trên hai tập dữ liệu: Subj, một tập dữ liệu hiểu nơi EVOPROMPT (DE) vượt trội so với EVOPROMPT (GA), và ASSET, một tập dữ liệu sinh nơi cả hai biến thể đều thể hiện hiệu suất tương tự.

¹Sự khác biệt độ chính xác giữa một prompt đã cho và prompt cơ sở "Let's think step by step." Điểm số 0 tương ứng với điểm số chuẩn hóa của prompt cơ sở.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[THIS IS TABLE: Bảng 5 showing designs in EVOPROMPT (DE) with columns for Mutation, Prompt 3, Subj, and ASSET]

Đột biến trên Các phần khác nhau Để minh họa lợi ích của việc đột biến chỉ các phần khác nhau, chúng tôi thay thế hai bước đầu tiên trong Hình 2 bằng hướng dẫn "Đột biến ngẫu nhiên Prompt 1 và Prompt 2" để cho phép đột biến trên tất cả nội dung trong Prompt 1 và 2, được ký hiệu là "All" trong Bảng 5. Trong khi đó, thiết kế ban đầu trong EVOPROMPT, chỉ đột biến các phần khác nhau, được ký hiệu là "Diff". Như được thể hiện trong Bảng 5, thiết kế đột biến chỉ trên các phần khác nhau liên tục mang lại cải thiện hiệu suất trên hai nhiệm vụ.

Lựa chọn Prompt 3 Áp dụng một trong các biến thể của thuật toán DE, trong EVOPROMPT (DE), chúng tôi chọn prompt tốt nhất trong tập hợp hiện tại như Prompt 3 trong Hình 2. Chúng tôi xác thực thiết kế này qua các thiết lập sau: 1) Prompt 3 được lấy mẫu ngẫu nhiên từ tập hợp hiện tại, được ký hiệu là "random" trong Bảng 5; 2) Loại bỏ việc sử dụng Prompt 3 bằng cách để Prompt Cơ bản trực tiếp lai ghép với các phần khác nhau đột biến (tức là loại bỏ Bước 3 trong Hình 2), được ký hiệu là "eliminate" trong Bảng 5. Bảng 5 rõ ràng chứng minh tầm quan trọng của việc đưa vào Prompt 3. Hơn nữa, được chỉ ra rằng chọn prompt tốt nhất làm Prompt 3 hiệu quả hơn lấy mẫu ngẫu nhiên.

5.3 KHỞI TẠO TẬP HỢP

[THIS IS TABLE: Bảng 6 showing ablations of initial population on SST-5 with different initialization strategies]

Chúng tôi điều tra tác động của chất lượng tập hợp ban đầu trên EVOPROMPT. Chúng tôi tiến hành thí nghiệm thí điểm để sắp xếp các prompt (được thiết kế thủ công hoặc được tạo ra bởi GPT-3.5) theo hiệu suất của chúng trên tập dev. Sau đó chúng tôi chọn các prompt bottom, random và top cùng với các biến thể tương ứng của chúng như prompt ban đầu. Các biến thể này được tạo ra bằng cách sử dụng mẫu tái lấy mẫu được thiết kế trong Zhou et al. (2022), được hiển thị trong Hình 4 trong Phụ lục B.2, được sử dụng để đưa vào tính ngẫu nhiên cho việc khởi tạo.

Bảng 6 chứng minh rằng: 1) Thiết kế có crafted của prompt ban đầu không cần thiết, vì việc chọn prompt ngẫu nhiên có thể đạt được hiệu suất tương tự với việc chọn những prompt có hiệu suất cao nhất; 2) Khi chọn các prompt có hiệu suất cao nhất, việc đưa vào tính ngẫu nhiên bằng cách cho phép GPT-3.5 tạo ra các biến thể có thể dẫn đến cải thiện nhẹ về hiệu suất tổng thể; tuy nhiên, khi chọn prompt ngẫu nhiên, không cần đưa vào tính ngẫu nhiên bổ sung cho EVOPROMPT (DE); 3) Khi sử dụng prompt ban đầu có hiệu suất cao nhất, EVOPROMPT (GA) hoạt động tốt hơn một chút so với EVOPROMPT (DE); tuy nhiên, khi bắt đầu với prompt ban đầu có hiệu suất thấp, EVOPROMPT (DE) vượt trội so với EVOPROMPT (GA), điều này cho thấy rằng DE là lựa chọn tốt hơn khi các prompt thủ công có sẵn không có chất lượng cao.

6 KẾT LUẬN

Chúng tôi giới thiệu EVOPROMPT để tối ưu hóa prompt rời rạc, kết nối LLM với các thuật toán tiến hóa. Các thí nghiệm mở rộng trên 31 tập dữ liệu chứng minh sự vượt trội của EVOPROMPT, mang lại cải thiện hiệu suất nhất quán so với cả hướng dẫn thủ công và các phương pháp hiện có. Bên cạnh đó, chúng tôi xác thực rằng LLM có thể phục vụ như một giao diện hiệu quả, có thể diễn giải để thực hiện các thuật toán tiến hóa như GA và DE. Trong khi nghiên cứu này tập trung vào EA, khả năng mở rộng của cách tiếp cận của chúng tôi mở ra các con đường để áp dụng LLM cho các thuật toán thông thường khác, chẳng hạn như tối ưu hóa bầy đàn hạt (PSO) (Kennedy & Eberhart, 1995), tối ưu hóa đàn kiến (ACO) (Dorigo & Gambardella, 1997) và các thuật toán tối ưu hóa Quality-Diversity (QD) gần đây hơn. Những phát hiện của chúng tôi nhằm truyền cảm hứng cho nghiên cứu trong tương lai tại giao điểm của LLM và các thuật toán truyền thống, khuyến khích các ứng dụng sáng tạo.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

CẢM ơn

Công trình này được hỗ trợ một phần bởi Chương trình Nghiên cứu và Phát triển Chủ chốt Quốc gia của Trung Quốc (Số 2020YFB1708200), và Chương trình Khoa học và Công nghệ Thâm Quyến (JCYJ20220818101001004).

TÀI LIỆU THAM KHẢO

[Phần này chứa danh sách tài liệu tham khảo đầy đủ từ trang 10-14, tôi sẽ bỏ qua để tránh làm quá dài]

--- TRANG 15 ---

[Phần này chứa các thuật toán 2 và 3 với chi tiết về EVOPROMPT (GA) và EVOPROMPT (DE)]

--- TRANG 16-24 ---

[Phần này chứa các phụ lục chi tiết về:
- A: Chi tiết thực hiện thuật toán
- B: Thiết lập thí nghiệm
- C: Kết quả bổ sung
- D: Công trình tương lai

Các phụ lục này bao gồm thống kê tập dữ liệu, mẫu template, thông số siêu tham số, phân tích chi phí, prompt tối ưu được tạo ra, và hướng nghiên cứu tương lai.]
