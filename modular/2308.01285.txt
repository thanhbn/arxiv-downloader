# 2308.01285.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/modular/2308.01285.pdf
# File size: 3817441 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Flows: Building Blocks of Reasoning and Collaborating AI
Martin Josifoski* 1Lars Klein* 1Maxime Peyrard2Nicolas Baldwin1Yifei Li** 1Saibo Geng** 1
Julian Paul Schnitzler1Yuxing Yao1Jiheng Wei3Debjit Paul1Robert West1
Abstract
Recent advances in artificial intelligence (AI)
have produced highly capable and controllable
systems. This creates unprecedented opportuni-
ties for structured reasoning as well as collabo-
ration among multiple AI systems and humans.
To fully realize this potential, it is essential to
develop a principled way of designing and study-
ing such structured interactions. For this purpose,
we introduce the conceptual framework Flows .
Flows are self-contained building blocks of com-
putation, with an isolated state, communicating
through a standardized message-based interface.
This modular design simplifies the process of cre-
ating Flows by allowing them to be recursively
composed into arbitrarily nested interactions and
is inherently concurrency-friendly. Crucially, any
interaction can be implemented using this frame-
work, including prior work on AI–AI and human–
AI interactions, prompt engineering schemes, and
tool augmentation. We demonstrate the potential
ofFlows on competitive coding, a challenging
task on which even GPT-4 struggles. Our results
suggest that structured reasoning and collabora-
tion substantially improve generalization, with
AI-only Flows adding + 21and human–AI Flows
adding + 54absolute points in terms of solve rate.
To support rapid and rigorous research, we intro-
duce the aiFlows library embodying Flows . The
aiFlows library is available at https://github.
com/epfl-dlab/aiflows . Data and Flows for
reproducing our experiments are available at
https://github.com/epfl-dlab/cc_flows .
1. Introduction
The success of large language models (LLMs) largely lies
in their remarkable emergent ability to adapt to informa-
1EPFL2Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG
3PSL University *, ** Equal contribution
Correspondence to: martin.josifoski@epfl.ch, lars.klein@epfl.ch,
maxime.peyrard@univ-grenoble-alpes.fr, robert.west@epfl.chtion within their context (i.e., prompt) (Brown et al., 2020;
Wei et al., 2022; Kojima et al., 2022). By strategically
crafting the context, LLMs can be conditioned to perform
complex reasoning (Wei et al., 2022; Nye et al., 2021) and
effectively utilize external tools (Schick et al., 2023), sig-
nificantly enhancing their capabilities. Some of the most
exciting recent developments involve defining control flows ,
wherein LLMs, with the ability to control a set of tools, are
called in an orchestrated fashion to solve increasingly com-
plex tasks. Examples of such control flows include ReAct
(Yao et al., 2023b), AutoGPT (Richards, 2023), BabyAGI
(Nakajima, 2023), PromptBreeder (Fernando et al., 2023)
and FunSearch (Romera-Paredes et al., 2023). Even the
ubiquitous ChatGPT (OpenAI, 2023b) application is an in-
stance of a control flow built around the GPT-3.5 and GPT-4
models (Brown et al., 2020; OpenAI, 2023a). However,
these represent but a few of the many conceivable control
flows, offering only a glimpse into the vast potential of struc-
tured LLM interactions. To realize this potential, we need
to develop ways to study such interactions systematically.
In software engineering, simple processes can be imple-
mented in an unstructured fashion, perhaps in a single file.
However, as the size and complexity of the systems increase,
choosing the right abstractions and architecture becomes
critical (Garlan & Shaw, 1993). Currently, for structured
LLM interactions we want to model, implement, and study,
we are at a point where this become unwieldy. Yet, no
general efficient abstraction exists for effectively modeling
arbitrarily complex structured interactions. Previous work
and existing frameworks, such as LangChain (Chase, 2022),
Chameleon (Lu et al., 2023), and HuggingGPT (Shen et al.,
2023), have converged to an ad-hoc abstraction that mod-
els agents as entities that use LLMs to select and execute
actions towards specific tasks, where the set of possible
actions is pre-defined by the available tools. In this view,
tools serve a narrow, well-defined goal and can perform
sophisticated tasks (e.g., querying a search engine or exe-
cuting code). However, their behavior is limited to a single
interaction. To highlight the implications of this limitation,
consider the following scenario: Alice wants to apply for a
job at HappyCorp. If Alice is an agent, she would need to
explicitly plan the entire process, including preparing the
application, sending it, and evaluating it, which may involve
1arXiv:2308.01285v3  [cs.AI]  7 Feb 2024

--- PAGE 2 ---
Submission and Formatting Instructions for ICML 2024
Tools
…
GPT-4
Search engine
Fixed reply
Code executor
Vector DB
Human input
Agent FlowPrompt: few-shot,CoT… Web Search Flow
Code TestingFlow
Vector DB Flow
Human Flow
Atomic	FlowsWrap	tools	into	message-exchanging	entities
… …Composite	FlowsOrchestrate	the	interaction	between	other	FlowsPlan-Code Flow(Circular)
Code Generator 
Code Feedback Code Feedback (Sequential)Code Testing
Code Flow(Generator-Critic)
Plan Generator 
Plan Feedback Plan Flow(Generator-Critic)Example	Coding	Flow
Fixed Reply Flow
e.g., ”Are you sure?”Generator Flowcan be any FlowGenerator-Critic FlowCritic Flowcan be any FlowSequential Flow…Flow 1can be any FlowFlow 2can be any FlowFlow ncan be any Flow
Monitoring FlowExampleMeta-Reasoning	FlowMemory FLowAny FlowControl Flow
Execution Flow…Memory FLowAny Flow…Memory FLowAny Flow…Autonomous Flow
Figure 1. Flows framework exemplified. The first column depicts examples of tools. The second column depicts Atomic Flows
constructed from the example tools. The third column depicts examples of Composite Flows defining structured interaction between
Atomic or Composite Flows. The fourth column illustrates a specific Composite competitive coding Flow as those used in the experiments.
The fifth column outlines the structure of a hypothetical Flow, defining a meta-reasoning process that could support autonomous behavior.2
a background check, organizing interviews, and more. Alice
would need the knowledge and the “computational” ability
to account for every detail, including unforeseen events that
may arise (e.g., the interviewer being on parental leave), and
require her to adapt. In reality, most of the complexity is
hidden from Alice behind an interface to HappyCorp’s hir-
ing process that might itself be composed of sub-processes
involving many other agents andtools . Therefore, Alice is
completely agnostic to the process(es) happening behind the
interface and the respective logistics. On the other hand, the
hiring process, carefully designed by experts, can be reused
by many agents, and its sub-processes can be modified or im-
proved with minimal or no impact on the other components
beyond an updated interface. This makes it evident that
agents and tools should be able to interact in complex, dy-
namic or static, ways as parts of nested, modular processes
(that run locally or remotely), and the distinction between
the two becomes blurred as they both serve as computational
units in a complex computational process.
Starting from the observation that all processes are (control)
flows defining a potentially complex interaction between
many diverse components; we introduce a conceptual frame-
work where Flows are the fundamental building blocks of
computation. Flows are independent, self-contained, goal-
driven entities able to complete semantically meaningful
units of work. To exchange information, Flows commu-
nicate via a standardized message-based interface. The
framework is depicted in Fig. 1.
TheFlows abstraction ensures modularity. Alice, a higher-
level meta-reasoning Flow that can support autonomousbehavior, does not need to know anything beyond how to in-
terface with HappyCorp’s hiring Flow. This substantially re-
duces complexity (Alice is interacting with a deeply nested,
compositional structured interaction through a simple in-
terface) and provides flexibility, allowing sub-Flows to be
swapped without consequences as long as they have the
same interface. Indeed, HappyCorp’s pre-filtering Flow can
be swapped from a rule-based system to an AI model or
even a human Flow without affecting the structure of the
overall process. The abstraction also enables reusability
and the composition of sub-Flows into new Flows for dif-
ferent tasks. Furthermore, the framework shares key design
choices with the Actor model, one of the most prominent
models of concurrent computation (cf. Sec. 3). Certainly,
once Alice submits her application to HappyCorp, she does
not need to wait for the response; she can move to her next
goal while the other Flows run concurrently.
We showcase the potential of the proposed framework and li-
brary by investigating complex collaborative and structured
reasoning patterns on the challenging task of competitive
coding, a mind sport involving participants trying to solve
problems defined by a natural language description.
Contributions. (i) We propose Flows , a conceptual frame-
work providing an abstraction that simplifies the design and
implementation of arbitrarily nested interactions while en-
abling concurrency. Flows can represent anyinteraction
and provides a common framework for reasoning about in-
teraction patterns, specifying hypotheses, and structuring
2For more details on meta-reasoning Flows see Sec. 7
2

--- PAGE 3 ---
Submission and Formatting Instructions for ICML 2024
research more broadly. (ii) We open-source the aiFlows
library, which embodies Flows , together with FlowVerse,
which is a repository of Flows that can be readily used,
extended, and composed into novel, more complex Flows.
(iii) We leverage Flows and the accompanying library to
systematically investigate the benefits of complex interac-
tions for solving competitive coding problems and develop
AI-only Flows adding + 21and human–AI Flows adding
+54 absolute points in terms of solve rate.
2. Related Work
Existing libraries for modeling structured interactions.
LangChain (Chase, 2022) has become the go-to library for
creating applications using large language models. How-
ever, most recent works involving structured interaction,
such as Cameleon (Lu et al., 2023), Camel (Li et al., 2023),
HuggingGPT (Shen et al., 2023), and the concurrent works
MetaGPT (Hong et al., 2023) and AutoGen (Wu et al., 2023)
all come with their own library. Researchers opt to imple-
ment bespoke solutions due to the lack of a general yet
efficient abstraction for modeling and designing structured
interactions as well as the infrastructure to implement them,
that should enable and facilitate open-ended exploration of
novel ideas. In this work, we develop such an abstraction,
Flows , which, in concert with aiFlows , fills this lacuna.
Impact of Flows. Crucially, the framework can imple-
ment any algorithm and efficiently covers all prior works
on AI-AI, human-AI interactions, as well as prompt en-
gineering (cf. Appendix A.3). These works focusing on
specific Flow instantiations have demonstrated that struc-
tured interactions canyield performance gains across tasks
and models. However, recent results put the universality
of previously published results into question (e.g., Huang
et al. (2023)) and highlight the necessity for more system-
atic research. To support these research efforts, we develop
the theoretical and practical infrastructure for modeling, im-
plementation, and systematic study structured interactions
of arbitrary complexity. We demonstrate the benefits of
the proposed infrastructure by conducting experiments that
thoroughly investigate multiple core interaction patterns,
including Human-AI collaboration, and their combinations,
while accounting for data contamination and variance in
the results, both of which are, surprisingly, not currently a
standard.
Competitive coding (CC). With the advent of transformers,
Li et al. (2022) finetuned an LLM on GitHub code repos-
itories, and a dataset scraped from Codeforces. Recently,
Zelikman et al. (2022) proposed decomposing CC problems
into function descriptions and, for each function description,
using an LLM to generate the implementation in a modu-
lar way. While these methods yield promising results, CCremains a challenging task far from being solved (OpenAI,
2023a). As such it presents itself as an ideal test bed for thor-
oughly studying the benefits of collaborative and structured
reasoning interactions.
3. Flows
This section introduces Flows as a conceptual framework,
describes its benefits, and presents the aiFlows library,
which embodies the framework.
3.1.Flows as a Conceptual Framework
The framework is centered around Flows andmessages .
Flows represent the fundamental building block of compu-
tation. They are independent, self-contained, goal-driven
entities able to complete a semantically meaningful unit of
work. To exchange information, Flows communicate via a
standardized message-based interface. Messages can be of
any type the recipient Flow can process.
We differentiate between two types of Flows: Atomic and
Composite.3Atomic Flows complete the work directly
by leveraging tools . Tools can be as simple as a textual
sequence specifying a (simple) Flow’s fixed response or
as complex as a compiler, a search engine, powerful AI
systems like LLaMA (Touvron et al., 2023a;b), Stable Dif-
fusion (Rombach et al., 2021), and GPT-4; or even a human.
Notably, in the Flows framework, AI systems correspond
to tools. An Atomic Flow is effectively a minimal wrapper
around a tool and achieves two things: (i) it fully specifies
the tool (e.g., the most basic Atomic Flow around GPT-4
would specify the prompts and the generation parameters);
and (ii) it abstracts the complexity of the internal computa-
tion by exposing only a standard message-based interface
for exchanging information with other Flows. Examples of
Atomic Flows include wrappers around chain-of-thought
prompted GPT-4 for solving math reasoning problems, few-
shot prompted LLaMA for question answering, an existing
chatbot, a search engine API, or an interface with a human.
Composite Flows accomplish more challenging, higher-
level goals by leveraging and coordinating other Flows.
Crucially, thanks to their local state and standardized in-
terface, Composite Flows can readily invoke Atomic Flows
or other Composite Flows as part of compositional, struc-
tured interactions of arbitrary complexity. Enabling research
on effective patterns of interaction is one of the main goals
of our work. General examples of such patterns include (i)
factorizing the problem into simpler problems (i.e., divide
and conquer); (ii) evaluating (sub-)solutions at inference
time (i.e., feedback); and (iii) incorporating external infor-
3The concept of a Flow is sufficient for modeling any interac-
tion. We introduce this distinction as it improves the exposition
and simplifies the implementation.
3

--- PAGE 4 ---
Submission and Formatting Instructions for ICML 2024
mation or a tool. Importantly, Flows can readily invoke
other, potentially heavily optimized, specialized Flows to
complete specific (sub-)tasks as part of an interaction, lead-
ing to complicated behavior. One example of a Composite
Flow is ReAct (Yao et al., 2023b). ReAct is a sequential
Flow that structures the problem-solving procedure in two
steps: a Flow selects the next action out of a predefined set
of actions, and another Flow executes it. The two steps are
performed until an answer is obtained. Another prominent
example, AutoGPT, extends the ReAct Flow with a Memory
Flow and an optional Human Feedback Flow. More gener-
ally, our framework provides a unified view of prior work,
which we make explicit in Appendix A.3.
Importantly, as illustrated in Fig. 1, Composite Flows can
script an arbitrarily complex pattern (i) precisely specifying
an interaction (e.g., generate code, execute tests, brainstorm
potential reasons for failure, etc.); or (ii) defining a high-
level, meta-reasoning process in which a Flow could bring
about dynamic unconstrained interactions.
Key properties. The proposed framework is characterized
by the following key properties:
•Flows are the compositional building blocks of compu-
tation.
• Flows encapsulate a local, isolated state.
• Flows interact only via messages.
•Flows’ behaviour depends only on their internal state
and the input message.
•Flows can send messages to other Flows and create
new Flows.
Connection to the Actor model. Flows is fundamentally
a framework modeling the computation underlying inter-
actions. As such, it shares key design principles with the
Actor model (Hewitt et al., 1973) — a mathematical model
of concurrent computation. Similarly to Flows , in the Ac-
tormodel, an Actor is a concurrent computation entity that
can communicate with other Actors exclusively through an
asynchronous message-passing interface. By encapsulat-
ing the state and the computation within individual Actors,
the model provides a high-level abstraction for effectively
managing and reasoning about complex concurrent and dis-
tributed systems, completely avoiding issues associated with
shared states, race conditions, and deadlocks. These ben-
efits are similar in nature to those observed in the domain
of interactions. The main distinction between the proposed
framework and the Actor model lies in their respective com-
munication protocols. Concretely, while the Actor model
prescribes purely asynchronous communication, Flows na-
tively supports synchronous communication, which is es-
sential for the implementation of structured reasoning. In-
terestingly, a similar deviation from the “pure” Actor modelcan be identified in the implementation of Erlang, a concur-
rent programming language based on it (Armstrong, 2003).
Overall, the shared design choices still make Flows inher-
ently concurrency-friendly from the practical perspective
and are sufficient for important results from the five decades
of extensive studies of the Actor model, such as the fact
that every physically possible computation can be directly
implemented using Actors (Hewitt, 2010), to transfer to
Flows .
3.2. Why Flows ?
Modularity. Flows introduces a higher-level abstraction
that isolates the state of individual Flows and specifies
message-based communication as the only interface through
which Flows can interact. This ensures perfect modularity
by design.
Reduction of complexity. The framework ensures the com-
plexity of the computation performed by a Flow is fully
abstracted behind the universal message-based interface.
This enables an intuitive and simple design of arbitrarily
complex interactions from basic building blocks.
Systematicity, flexibility, and reusability. The separation
of responsibility allows for modules to be developed and
studied systematically in isolation or as part of different
interactions. Once the correctness and the benefits of a Flow
have been established, it can be readily used in developing
novel Flows or as a drop-in replacement for less effective
Flows leveraged in completing similar goals.
Concurrency. The proposed framework’s design is consis-
tent with the Actor model, one of the most prominent models
of concurrent computation. As a consequence, Flows can
readily support any setting in which Flows run concurrently.
3.3. The aiFlows Library
Accompanying Flows , we release the aiFlows library,
which embodies the framework. In addition to the inherent
benefits that come with the framework, the library comes
with the following add-ons: (i) FlowVerse: a repository (to
which anyone can contribute) of Flows that can be read-
ily used, extended, or composed into novel, more complex
Flows. Flows allows for existing “tools” (as well as “mod-
els”, “chains”, “agents”, etc.) to be readily incorporated by
wrapping them in an Atomic Flow; (ii) a detailed logging
infrastructure enabling transparent debugging, analysis, and
research in optimizing (i.e., learning or fine-tuning) Flows.
4

--- PAGE 5 ---
Submission and Formatting Instructions for ICML 2024
4. Competitive Coding Flows
This work investigates the potential of structured interac-
tions for solving competitive coding (CC) problems. In CC,
given a natural language description and a few input–output
examples, the task is to generate code that will produce the
expected output for all of the hidden input–output test cases
associated with the problem. Fig. 4 provides examples.
We focus the analysis on three canonical dimensions of inter-
actions: (i) problem decomposition as structured reasoning;
(ii) human-AI collaboration; and (iii) refinement with var-
ious feedback types. By providing a common language
for clearly specifying interactions as well as the capabil-
ity to flexibly compose, exchange, and extend them, the
framework makes it possible to study the space of complex
interactions in a principled fashion. In the rest of the sec-
tion, we describe the specific Flows used in the experiments,
depicted in Fig. 2.
Problem decomposition. Planning has been an integral
intermediate step in recent work (Lu et al., 2023; Shen et al.,
2023; Yao et al., 2023b). Similar decomposition is natural
in the context of CC as well. In particular, we approach the
task in two steps: generating a solution strategy by a Plan
Flow and then generating the corresponding code by a Code
Flow. This is depicted by panel A in Fig. 2.
Human-AI collaboration. When designing human-AI col-
laborations, it is essential to take the costs of human inter-
action into account (Horvitz, 1999; Amershi et al., 2019;
Mozannar et al., 2023). By providing immense flexibility,
Flows can support research in the design of interactions in-
volving humans as computational building blocks in a way
that maximizes the utility of the overall computation with a
minimal human effort. In the context of CC, we hypothesize
that a human can be effectively incorporated at the plan level
to provide a short “oracle” plan in natural language. We
operationalize this by an (Atomic) Human Flow, illustrated
in Panel B of Fig. 2 as the Oracle Plan Flow.
Refinement with various feedback types. Iterative refine-
ment is a general problem-solving strategy successfully de-
ployed across various disciplines (Perrakis et al., 1999; Reid
& Neubig, 2022; Schick et al., 2022; Saharia et al., 2021).
The strategy revolves around the idea that a solution can be
gradually improved through a mechanism for analysis, mod-
ification, and re-evaluation. The design of this “ feedback ”
mechanism is critical for the effectiveness of the problem-
solving strategy. The conceptual framework, paired with the
accompanying library, provides the infrastructure to support
the design, implementation, and principled research of ef-
fective refinement strategies and feedback mechanisms. In
this work, we consider a canonical iterative refinement setup
where a generator Flow is tasked with generating the solu-tion, and a critic Flow provides feedback on the proposed
solution. We consider two feedback types in the context of
both the Plan and the Code Flow: (i) Reflection Flow: the
feedback consists of a fixed message encouraging the model
to reflect on important aspects of the proposed solution; (ii)
Collaboration Flow: the feedback is provided by an AI sys-
tem that “evaluates” the proposed solution. Furthermore, we
explore two more code-specific feedback types: (i) Debug
Flow: the feedback message corresponds to the results from
executing the code and testing it against the examples pro-
vided in the problem description; (ii) Debug–Collab Flow:
the feedback is provided by an AI system with access to
the code testing results, effectively, grounding the feedback
and allowing more systematic reasoning about the potential
causes of failure.
We refer to Flows using the following convention: Code-
FlowName when no plan is generated and PlanFlowName-
CodeFlowName otherwise.
5. Experimental Setup
Data. We scrape publicly available problems from one
of the most popular websites hosting CC contests, Code-
forces (Mirzayanov, 2023), and LeetCode (LeetCode, 2023),
which cover a broad spectrum of problems ranging from
easy interview questions to hard CC problems (see Ap-
pendix A.1 for more details). The datasets cover problems
from 2020 -August- 21to2023 -March- 26for CodeForces,
and from 2013 -October- 25to2023 -April- 09for LeetCode.
Importantly, to study the effect of structured interactions
(i.e., different Flows) in a principled manner, it is crucial to
account for the possibility of data contamination , i.e., that
some of the test data has been seen during training (Magar
& Schwartz, 2022). Containing problems published over
an extended period up to a few months ago (at the time of
writing), our datasets allow for reliable identification of the
training data cutoff date that can help with addressing this
issue. Prior code evaluation datasets like APPS (Hendrycks
et al., 2021), HumanEval (Chen et al., 2021), and Code-
Contests (Li et al., 2022) lack problem release dates, and
considering the lack of publicly available information about
LLMs’ training data, can likely lead to confounded evalua-
tion of models’ memorization and generalization abilities.
Code testing and solution evaluation. Just like a human
participant, the Debug Flow has access only to the input–
output example pairs contained in the problem description
and, at inference time, uses a local code testing infrastruc-
ture to evaluate (intermediate) solution candidates. Cru-
cially, these examples cover only a few simple cases, and
generating outputs consistent with them does not imply the
code corresponds to a correct solution. A solution is con-
sidered correct if it passes all the hidden test cases. To
determine correctness, we leverage online evaluators that
5

--- PAGE 6 ---
Submission and Formatting Instructions for ICML 2024
Plan Generator 
Plan Feedback Collaboration(Generator-Critic)
Plan Generator Oracle Plan 
Plan Generator Fixed ReplyReflection(Generator-Critic)
NoPlan B) Plan Flows
Code Generator 
Code Feedback Code Feedback (Sequential)Code Testing
Debug-Collab(Generator-Critic)
Code Generator 
Code Generator 
Code Feedback Collaboration(Generator-Critic)
Code Generator Code TestingDebug(Generator-Critic)
Code Generator Fixed ReplyReflection(Generator-Critic)
C) Code FlowsPlanFlow
PlanCoding Template(Sequential)
CodeFlow
Code solution
Problem description
A) Overview
Figure 2. Competitive coding Flows. At the highest level, we consider planning as a specific structured reasoning pattern for problem
decomposition. In particular, the Plan Flow generates a solution strategy and passes it to the Code Flow, which implements it, as depicted
in A). B) and C) depict the different choices of sub-Flows used as Plan and Code Flows in the experiments. Notably, we explore the
impact of human-AI collaboration at the plan level and refinement with different types of feedback : i) fixed reply encouraging reflection;
ii) AI generated feedback; iii) code testing results as feedback; iv) AI generated feedback grounded in code testing results.
submit candidate solutions to the websites’ online judges,
ensuring authoritative results. For many of the Codeforces
problems, we also support local evaluation based on a com-
prehensive set of hidden test cases we managed to scrape.
For more details, see Appendix A.2.
Models and Flows. We experiment with the competitive
coding Flows described in Sec. 4, and GPT-4 (OpenAI,
2023a) as the LLM tool of choice. See Appendix A.4 for
the specific prompts. Also, the code to reproduce the ex-
periments in the paper is available in the project’s GitHub
repository.
Evaluation metrics. The most common evaluation metric
for code generation is pass@ k, corresponding to the prob-
ability that in a set of ksampled candidates, there will be
at least one correct solution (Chen et al., 2021). To better
align with practical use cases, we focus on pass@1, i.e. the
solve rate when averaged across the problem set. We report
a point estimate and a 95% confidence interval constructed
from 1000 bootstrap resamples.
Compute and cost. All the experiments, including the most
complex Flows, can be performed on commodity hardware
relatively cheaply. For instance, the costs associated with
querying the OpenAI API for generating Table 1 amount to
$1000.
6. Experimental Results
We first study the generalization ability of representative
Flows and empirically identify GPT-4’s knowledge-cutoff
date. Next, we perform a focused analysis along the dimen-
sions described in Sec. 4.6.1. Performance of Coding Flows on Pre- vs.
Post-Knowledge-Cutoff-Date Data
2020-10-11 2021-01-19 2021-04-29 2021-08-07 2021-11-15 2022-02-23 2022-06-03 2022-09-11 2022-12-20 2023-03-30
Release Time0.00.20.40.60.81.0Problem Solve Ratecode
code_debug_collab
plan_oracle-code_debug_collab
Figure 3. Temporal analysis. Performance is averaged over a slid-
ing window of two months. The substantial drop in performance
around the reported knowledge cutoff date for GPT-3/4 (the crim-
son vertical line) reveals limited generalization ability that can be
alleviated through structured interactions.
In this experiment, we consider three representa-
tive Flows: (i) Code: the simplest Code Generator
Flow corresponding to a single GPT-4 API call; (ii)
Code_Debug_Collab : the most complex code Flow; (iii)
Plan_Oracle-Code_Debug_Collab : the most complex code
Flow with human guidance at the plan level. We perform the
analysis by running the three Flows on Codeforces problems
released from October 2020 to April 2023 and averaging
the performance over a sliding window of two months. The
results are reported in Fig. 3.
We observe a substantial drop in performance centered
around September 2021, consistent with the knowledge
6

--- PAGE 7 ---
Submission and Formatting Instructions for ICML 2024
Table 1. Main Results. Performance of competitive coding Flows on Codeforces and LeetCode, with direct inference (Code) as baseline.
Codeforces Leetcode
Pre-cutoff Post-cutoff Pre-cutoff Post-cutoff
Easy Medium Hard Easy Medium Hard
Code 71.8 ±11.0 26.9 ±11.0 97.8 ±3.1 93.4 ±5.4 66.7 ±10.9 76.3 ±8.6 25.1 ±8.9 8.0 ±5.5
Code_Reflection +9.3 ±9.7 +0.0 ±10.6 +0.0 ±3.1 +0.0 ±5.4 +1.2 ±10.6 +0.9 ±8.1 +5.4 ±9.4 +3.5 ±6.6
Code_Collaboration +4.8 ±10.5 +9.6 ±11.8 +0.0 ±3.1 -2.3 ±6.0 -0.1 ±10.9 -3.2 ±8.7 +0.0 ±8.7 +1.2 ±5.9
Code_Debug +12.7 ±8.6 +7.9 ±11.6 +0.0 ±3.1 +1.1 ±5.0 +6.9 ±10.0 +7.7 ±7.3 +7.7 ±9.6 +2.4 ±6.3
Code_Debug_Collab +12.6 ±8.9 +20.6 ±12.1 +0.0 ±3.1 +0.0 ±5.4 +5.5 ±10.4 +7.5 ±7.4 +9.8 ±9.7 +1.2 ±6.0
Plan-Code -1.6 ±11.0 +8.0 ±11.6 -3.1 ±4.5 -2.3 ±5.9 -9.7 ±11.2 +2.3 ±8.3 +3.2 ±9.1 -3.4 ±4.3
Plan_Reflection-Code -3.3 ±11.6 +4.8 ±11.6 -2.1 ±4.1 -4.5 ±6.6 -3.1 ±10.7 +1.2 ±8.3 -3.3 ±8.5 +0.0 ±5.5
Plan_Collaboration-Code -4.8 ±11.5 +6.3 ±11.4 -1.1 ±3.7 -2.3 ±6.1 -7.2 ±11.2 -2.0 ±8.6 +0.1 ±9.0 +1.2 ±5.8
Plan_Oracle-Code +11.0 ±9.4 +47.6 ±10.7 – – – – – –
Plan_Oracle-Code_
Debug_Collab+23.0 ±5.2 +53.9 ±9.5 – – – – – –
cutoff date reported by OpenAI, and denote it by a vertical
line on the plot. With Codeforces problems appearing in
contexts outside of the contest itself (e.g., editorials), it is
reasonable to assume the model has been exposed to older
problems more frequently during training. This would ex-
plain why the drop spans multiple months, from May 2021
to November 2021, depending on when which data was
published and crawled.
Notably, there is a stark difference in the performance of
the Code Flow on problems published before and after the
knowledge cutoff data, with the solve rate decreasing from
around 80% to 23%. While still experiencing a substantial
performance drop, the Code_Debug_Collab Flow doubles
the solve rate on novel problems to around 45%. Provided
with human input at the plan level, the same Flow reaches
85%. Overall, this highlights that GPT-4 performs poorly
on novel complex reasoning problems, but structured in-
teractions have the potential to enhance its generalization
capabilities. As both GPT-4 (i.e., the Code Flow) and the
more complex interactions (Flows) exhibit qualitatively dif-
ferent behavior on novel data, to draw accurate conclusions,
it is critical that data contamination is taken into serious
consideration when designing experiments and interpreting
results.
6.2. Comparing Competitive Coding Flows
Table 1 reports the performance of the systematically chosen
set of Flows described in Sec. 4. Rows 6–10correspond
to Flows comprising planning and coding , while rows 1–5
perform the coding directly . In line with the findings of the
previous section, we separately consider the performance on
problems published before and after the knowledge cutoff
date of September 2021.Problem decomposition. The idea behind planning be-
fore implementing the solution is to decouple the high-level
reasoning from the code implementation. To analyze the
effectiveness of this pattern, we compare the Code and the
Plan-Code Flow. Looking at the point estimates, in the
pre-cutoff problems, introducing the plan Flow leads to de-
creased performance (- 1.6for Codeforces and - 3.1/2.3/-9.7
for LeetCode easy/medium/hard). However, in the post-
cutoff problems, incorporating a plan Flow leads to gains
for Codeforces (+ 8) and LeetCode easy and medium (+ 2.3
and + 3.2). While these trends are consistent, considering
the confidence intervals, we see that they are not statisti-
cally significant. Crucially, these results do not imply that
this specific problem decomposition is not valuable as it
creates a lot of potential in designing an effective human-AI
collaboration.
Human-AI collaboration. After every contest, the Code-
forces community publishes an editorial that, in addition
to the code implementation, provides a short natural lan-
guage description of the solution. To simulate a Flow
where a human provides high-level guidance at the core
of the reasoning process, we scrape the solution descrip-
tions and pass them as human-generated plans. The re-
sults are striking: despite being only a few sentences long,
human-provided plans lead to a substantial performance
increase (from 26.9%to74.5%and from 47.5%to80.8%
on novel problems, when the code is generated by Code and
Code_Debug_Collab Flows , respectively). First and fore-
most, these results showcase the opportunities created by
Flows for designing, implementing, and studying Human-AI
collaboration as a key component of structured interactions.
Second, specific to the problem of competitive coding, they
validate the hypothesis that high-quality plans are impor-
tant, suggesting that the design of more effective plan Flows
7

--- PAGE 8 ---
Submission and Formatting Instructions for ICML 2024
is a promising direction to explore in the future. Last but
not least, the results highlight the necessity of more sys-
tematic research, as patterns seemingly not valuable in one
Flow, such as the simple plan-code structured reasoning
problem decomposition, can provide immense value as part
of another Flow.
Refinement with various feedback types. We find that
Code_Reflection andCode_Collaboration lead to limited
improvements among the code Flows. The two exceptions
are Codeforces pre-cutoff ( +9.3) for the former and Code-
forces post-cutoff ( +9.6) for the latter pattern. While close,
these results are not statistically significant. On the other
hand, the Flows providing grounded feedback, Code_Debug
andCode_Debug_Collab , lead to consistent and statistically
significant improvements, most notable on the novel Code-
forces problems where performance increases from 26.9,
without feedback, to 47.5, when the refinement is based
on AI-generated feedback grounded in tests. On LeetCode,
these improvements are smaller in magnitude. We suspect
this is a consequence of the examples provided with the prob-
lem description being more simple than those in Codeforces,
leading to false positives and, thereby, incorrect grounding,
affecting the feedback quality. This could be addressed
by generating additional tests with a Test_Case_Generator
Flow, a direction we leave for future work to explore. Fi-
nally, in the plan Flows, where we consider Reflection and
Collaboration (without grounding), we find that refinement
does not provide statistically significant benefits.
Overall , our findings provide several important insights: (i)
the direct benefit of problem decomposition hinges on the
quality of the intermediate steps; (ii) involving humans at
the core high-level reasoning process yields major improve-
ments as humans can easily provide high-quality, grounded
feedback; (iii) strategic problem decomposition is a power-
ful strategy for creating opportunities for effective Human–
AI collaboration; (iv) the effectiveness of refinement pat-
terns is not universal and depends on the quality of the start-
ing solution and the feedback (e.g., the level of grounding),
and the model’s ability to incorporate that feedback mod-
ulated through the feedback’s specificity and the model’s
capabilities. This analysis paints a more complex picture
than what is reported by prior work for simple interactions.
7. Discussion
Simplicity and systematicity. Thanks to its key properties,
Flows , together with aiFlows , provides an infrastructure
that greatly simplifies the design and implementation of
open-ended interactions, with a capability to flexibly isolate,
compose, replace, or modify sub-Flows. The experiments
demonstrate that carefully designed interactions can sub-
stantially improve generalization. However, they also revealthat the effectiveness of particular interaction patterns is
not universal; instead, there are many factors at play. As
researchers, we need to clearly specify the patterns we are
studying, clearly communicate our hypotheses, and study
them both in isolation and as parts of other interactions
across different datasets or/and tasks. Furthermore, it is crit-
ical that data contamination is taken into serious considera-
tion when designing experiments and drawing conclusions,
and error bars become a standard in the field.
Cost and performance Optimization. In our experiments,
we used “off-the-shelf” LLMs that have not been specifically
optimized for collaboration. Performance (and compute
costs) can be substantially improved by fine-tuning models
to collaborate more effectively, generally or toward spe-
cialized roles (e.g., controller or critic). Learning requires
data, and to support research in this direction, aiFlows
implements detailed logging mechanisms of Flow runs.
Meta-reasoning Flows and asynchronous execution. Cog-
nitive science research in metacognition and meta-reasoning
suggests the existence of meta-level monitoring and control
processes underlying cognition (Ackerman & Thompson,
2017). Since Flows supports asynchronous execution of sub-
Flows, it makes it possible to achieve similar asynchronous
meta-cognition for autonomous AI systems moving beyond
a single LLM call serving as a controller (Nakajima, 2023;
Richards, 2023). For example, distributed and asynchronous
execution of Flows such as FunSearch (Romera-Paredes
et al., 2023) is naturally supported by Flows .
8. Conclusion
In this paper, we propose Flows , an abstraction that, in
concert with the accompanying library aiFlows , provides
the theoretical and practical infrastructure with a modu-
lar and concurrency-friendly design, which enables and
facilitates the modeling, implementation, and systematic
study of arbitrarily complex structured interactions. We
thoroughly investigate multiple core interaction patterns,
including Human-AI collaboration, and their combinations,
while accounting for data contamination and the variance
in the results. The investigation shows that the developed
AI-only Flows add +21 and human–AI Flows add +54 abso-
lute points in terms of solve rate, and highlights the effect
of data contamination, variance, and non-universality of
results. Overall, our experiments establish the potential of
Flows, the necessity of more systematic research, and the
value brought by Flows andaiFlows in support of these
research efforts. On the one hand, Flows provides a high-
level abstraction enabling the design and implementation of
interactions of arbitrary complexity. On the other, it offers a
common framework for reasoning about interaction patterns,
specifying hypotheses, and structuring research. We hope
8

--- PAGE 9 ---
Submission and Formatting Instructions for ICML 2024
the framework will serve as a solid basis for practical and
theoretical innovations, paving the way toward ever more
useful AI, similar to the Actor model’s role for concurrent
and distributed systems.
References
Ackerman, R. and Thompson, V . A. Meta-reasoning:
Monitoring and control of thinking and reasoning.
Trends in Cognitive Sciences , 21(8):607–617, 2017.
ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.
2017.05.004. URL https://www.sciencedirect.com/
science/article/pii/S1364661317301055 .
Amershi, S., Weld, D. S., V orvoreanu, M., Fourney, A.,
Nushi, B., Collisson, P., Suh, J., Iqbal, S. T., Bennett,
P. N., Inkpen, K., Teevan, J., Kikin-Gil, R., and Horvitz,
E. Guidelines for human-ai interaction. In Brewster,
S. A., Fitzpatrick, G., Cox, A. L., and Kostakos, V .
(eds.), Proceedings of the 2019 CHI Conference on Hu-
man Factors in Computing Systems, CHI 2019, Glas-
gow, Scotland, UK, May 04-09, 2019 , pp. 3. ACM,
2019. doi: 10.1145/3290605.3300233. URL https:
//doi.org/10.1145/3290605.3300233 .
Armstrong, J. Making reliable distributed systems in the
presence of software errors . PhD thesis, Royal Institute
of Technology, Stockholm, Sweden, 2003. URL https:
//nbn-resolving.org/urn:nbn:se:kth:diva-3658 .
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger,
G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,
Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,
McCandlish, S., Radford, A., Sutskever, I., and Amodei,
D. Language models are few-shot learners. In Larochelle,
H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,
H. (eds.), Advances in Neural Information Processing
Systems , volume 33, pp. 1877–1901. Curran As-
sociates, Inc., 2020. URL https://proceedings.
neurips.cc/paper_files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
Chase, H. Langchain. https://github.com/hwchase17/
langchain , 2022.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan,
J., Edwards, H., Burda, Y ., Joseph, N., Brockman, G.,
Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H.,
Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,
Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,
C., Tillet, P., Such, F. P., Cummings, D. W., Plappert, M.,
Chantzis, F., Barnes, E., Herbert-V oss, A., Guss, W. H.,
Nichol, A., Babuschkin, I., Balaji, S. A., Jain, S., Carr, A.,Leike, J., Achiam, J., Misra, V ., Morikawa, E., Radford,
A., Knight, M. M., Brundage, M., Murati, M., Mayer, K.,
Welinder, P., McGrew, B., Amodei, D., McCandlish, S.,
Sutskever, I., and Zaremba, W. Evaluating large language
models trained on code. ArXiv , abs/2107.03374, 2021.
Chen, W., Ma, X., Wang, X., and Cohen, W. W. Pro-
gram of thoughts prompting: Disentangling computation
from reasoning for numerical reasoning tasks. ArXiv ,
abs/2211.12588, 2022.
Chen, X., Lin, M., Schärli, N., and Zhou, D. Teaching large
language models to self-debug. ArXiv , abs/2304.05128,
2023.
Fernando, C., Banarse, D., Michalewski, H., Osin-
dero, S., and Rocktäschel, T. Promptbreeder: Self-
referential self-improvement via prompt evolution. CoRR ,
abs/2309.16797, 2023. doi: 10.48550/ARXIV .2309.
16797. URL https://doi.org/10.48550/arXiv.
2309.16797 .
Garlan, D. and Shaw, M. An introduction to software ar-
chitecture. In Ambriola, V . and Tortora, G. (eds.), Ad-
vances in Software Engineering and Knowledge Engi-
neering , volume 2 of Series on Software Engineering
and Knowledge Engineering , pp. 1–39. World Scien-
tific, 1993. doi: 10.1142/9789812798039\_0001. URL
https://doi.org/10.1142/9789812798039_0001 .
Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,
A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and
Steinhardt, J. Measuring coding challenge competence
with apps. NeurIPS , 2021.
Hewitt, C. E. Actor model of computation: Scalable robust
information systems. arXiv: Programming Languages ,
2010.
Hewitt, C. E., Bishop, P. B., and Steiger, R. A universal
modular actor formalism for artificial intelligence. In
International Joint Conference on Artificial Intelligence ,
1973.
Hong, S., Zheng, X., Chen, J., Cheng, Y ., Wang, J.,
Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou,
L., Ran, C., Xiao, L., and Wu, C. Metagpt: Meta
programming for multi-agent collaborative framework.
CoRR , abs/2308.00352, 2023. doi: 10.48550/ARXIV .
2308.00352. URL https://doi.org/10.48550/arXiv.
2308.00352 .
Horvitz, E. Principles of mixed-initiative user interfaces.
In Williams, M. G. and Altom, M. W. (eds.), Proceeding
of the CHI ’99 Conference on Human Factors in Com-
puting Systems: The CHI is the Limit, Pittsburgh, PA,
USA, May 15-20, 1999 , pp. 159–166. ACM, 1999. doi:
9

--- PAGE 10 ---
Submission and Formatting Instructions for ICML 2024
10.1145/302979.303030. URL https://doi.org/10.
1145/302979.303030 .
Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W.,
Song, X., and Zhou, D. Large language models cannot
self-correct reasoning yet. CoRR , abs/2310.01798, 2023.
doi: 10.48550/ARXIV .2310.01798. URL https://doi.
org/10.48550/arXiv.2310.01798 .
Kim, G., Baldi, P., and McAleer, S. Language models can
solve computer tasks. ArXiv , abs/2303.17491, 2023.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and
Iwasawa, Y . Large language models are zero-shot
reasoners. In Koyejo, S., Mohamed, S., Agar-
wal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),
Advances in Neural Information Processing Sys-
tems, volume 35, pp. 22199–22213. Curran Asso-
ciates, Inc., 2022. URL https://proceedings.
neurips.cc/paper_files/paper/2022/file/
8bb0d291acd4acf06ef112099c16f326-Paper-Conference.
pdf.
LeetCode. Leetcode.com, 2023. URL https://leetcode.
com.
Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and
Ghanem, B. Camel: Communicative agents for" mind"
exploration of large scale language model society. arXiv
preprint arXiv:2303.17760 , 2023.
Li, Y ., Choi, D., Chung, J., Kushman, N., Schrittwieser, J.,
Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago,
A., et al. Competition-level code generation with alpha-
code. Science , 378(6624):1092–1097, 2022.
Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W.,
Wu, Y . N., Zhu, S.-C., and Gao, J. Chameleon: Plug-and-
play compositional reasoning with large language models.
ArXiv , abs/2304.09842, 2023.
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao,
L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S.,
Yang, Y ., et al. Self-refine: Iterative refinement with
self-feedback. arXiv preprint arXiv:2303.17651 , 2023.
Magar, I. and Schwartz, R. Data contamination: From
memorization to exploitation. In Muresan, S., Nakov,
P., and Villavicencio, A. (eds.), Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022 , pp. 157–165. Association for
Computational Linguistics, 2022. doi: 10.18653/v1/
2022.acl-short.18. URL https://doi.org/10.18653/
v1/2022.acl-short.18 .
Mirzayanov, M. Codeforces.com, 2023. URL https://
codeforces.com .Mozannar, H., Bansal, G., Fourney, A., and Horvitz, E.
When to show a suggestion? integrating human feed-
back in ai-assisted programming. CoRR , abs/2306.04930,
2023. doi: 10.48550/arXiv.2306.04930. URL https:
//doi.org/10.48550/arXiv.2306.04930 .
Nakajima, Y . Babyagi. https://github.com/
yoheinakajima/babyagi , 2023.
Nye, M. I., Andreassen, A. J., Gur-Ari, G., Michalewski,
H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A.,
Bosma, M., Luan, D., Sutton, C., and Odena, A. Show
your work: Scratchpads for intermediate computation
with language models. CoRR , abs/2112.00114, 2021.
URL https://arxiv.org/abs/2112.00114 .
OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774,
2023a.
OpenAI. ChatGPT. https://openai.com/chatgpt ,
2023b. Accessed: 2024-01-28.
Paul, D., Ismayilzada, M., Peyrard, M., Borges, B., Bosse-
lut, A., West, R., and Faltings, B. Refiner: Reasoning
feedback on intermediate representations. arXiv preprint
arXiv:2304.01904 , 2023.
Perrakis, A., Morris, R. J., and Lamzin, V . S. Automated
protein model building combined with iterative structure
refinement. Nature Structural Biology , 6:458–463, 1999.
URL https://api.semanticscholar.org/CorpusID:
20292852 .
Reid, M. and Neubig, G. Learning to model editing pro-
cesses. In Conference on Empirical Methods in Nat-
ural Language Processing , 2022. URL https://api.
semanticscholar.org/CorpusID:249062636 .
Richards, T. B. Autogpt. https://github.com/
Significant-Gravitas/Auto-GPT , 2023.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B. High-resolution image synthesis with latent
diffusion models. CoRR , abs/2112.10752, 2021. URL
https://arxiv.org/abs/2112.10752 .
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog,
M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S.,
Wang, P., Fawzi, O., et al. Mathematical discoveries from
program search with large language models. Nature , pp.
1–3, 2023.
Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J.,
and Norouzi, M. Image super-resolution via iterative
refinement. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 45:4713–4726, 2021. URL https:
//api.semanticscholar.org/CorpusID:233241040 .
10

--- PAGE 11 ---
Submission and Formatting Instructions for ICML 2024
Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis,
P., Izacard, G., You, Q., Nalmpantis, C., Grave, E.,
and Riedel, S. Peer: A collaborative language model.
ArXiv , abs/2208.11663, 2022. URL https://api.
semanticscholar.org/CorpusID:251765117 .
Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Tool-
former: Language models can teach themselves to use
tools. ArXiv , abs/2302.04761, 2023.
Shen, Y ., Song, K., Tan, X., Li, D. S., Lu, W., and Zhuang,
Y . T. Hugginggpt: Solving ai tasks with chatgpt and its
friends in huggingface. ArXiv , abs/2303.17580, 2023.
Shinn, N., Cassano, F., Labash, B., Gopinath, A.,
Narasimhan, K., and Yao, S. Reflexion: Language
agents with verbal reinforcement learning. arXiv preprint
arXiv:2303.11366 , 2023.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E.,
Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-
ple, G. Llama: Open and efficient foundation language
models. ArXiv , abs/2302.13971, 2023a.
Touvron, H., Martin, L., Stone, K. R., Albert, P., Alma-
hairi, A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava,
P., Bhosale, S., Bikel, D. M., Blecher, L., Ferrer, C. C.,
Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu,
J., Fu, W., Fuller, B., Gao, C., Goswami, V ., Goyal, N.,
Hartshorn, A. S., Hosseini, S., Hou, R., Inan, H., Kardas,
M., Kerkez, V ., Khabsa, M., Kloumann, I. M., Korenev,
A. V ., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J.,
Liskovich, D., Lu, Y ., Mao, Y ., Martinet, X., Mihaylov,
T., Mishra, P., Molybog, I., Nie, Y ., Poulton, A., Reizen-
stein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R.,
Smith, E. M., Subramanian, R., Tan, X. E., Tang, B.,
Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z.,
Zarov, I., Zhang, Y ., Fan, A., Kambadur, M., Narang, S.,
Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T.
Llama 2: Open foundation and fine-tuned chat models.
arXiv , 2023b.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
Chi, E., Le, Q. V ., Zhou, D., et al. Chain-of-thought
prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems , 35:
24824–24837, 2022.
Welleck, S., Lu, X., West, P., Brahman, F., Shen, T.,
Khashabi, D., and Choi, Y . Generating sequences by
learning to self-correct. In The Eleventh International
Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=hH36JeQZDaO .Wu, Q., Bansal, G., Zhang, J., Wu, Y ., Zhang, S., Zhu,
E., Li, B., Jiang, L., Zhang, X., and Wang, C. Au-
togen: Enabling next-gen LLM applications via multi-
agent conversation framework. CoRR , abs/2308.08155,
2023. doi: 10.48550/ARXIV .2308.08155. URL https:
//doi.org/10.48550/arXiv.2308.08155 .
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,
Y ., and Narasimhan, K. Tree of thoughts: Deliberate
problem solving with large language models. ArXiv ,
abs/2305.10601, 2023a.
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K. R., and Cao, Y . React: Synergizing reasoning and
acting in language models. In The Eleventh International
Conference on Learning Representations , 2023b. URL
https://openreview.net/forum?id=WE_vluYUL-X .
Yoran, O., Wolfson, T., Bogin, B., Katz, U., Deutch, D., and
Berant, J. Answering questions by meta-reasoning over
multiple chains of thought. ArXiv , abs/2304.13007, 2023.
Zelikman, E., Huang, Q., Poesia, G., Goodman, N. D., and
Haber, N. Parsel: A (de-)compositional framework for
algorithmic reasoning with language models, 2022. URL
https://arxiv.org/abs/2212.10561 .
Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and
Smola, A. J. Multimodal chain-of-thought reasoning in
language models. ArXiv , abs/2302.00923, 2023.
11

--- PAGE 12 ---
Submission and Formatting Instructions for ICML 2024
A. Appendix
A.1. Data
Example Codeforces and LeetCode problems are provided in Fig. 4.
In the first experiment, the temporal analysis, we use 239 Codeforces problems ranging from October 2020 to April 2023.
In the second experiment, we have 136 problems for Codeforces (some problems are dropped in order to keep the pre-cutoff
and post-cutoff buckets equal to 68) and 558 problems for LeetCode (93 for each of the six buckets). Additionally, to support
research in the area, we set up an AI competitive coding challenge based on a dataset of Codeforces problems of various
difficulties published after the knowledge cutoff date. More details about the CC competition are available in Appendix A.5.
You have received data from a Bubble bot. You know your task is to make factory facilities, but before you even start, you need to know how big the factory is and how many rooms it has. When you look at the data you see that you have the dimensions of the construction, which is in rectangle shape: N x M.Then in the next N lines you have M numbers. These numbers represent factory tiles and they can go from 0 to 15. Each of these numbers should be looked in its binary form. Because from each number you know on which side the tile has walls. For example number 10 in it's binary form is 1010, which means that it has a wall from theNorthside, it doesn't have a wall from theEast, it has a wall on theSouthside and it doesn't have a wall on theWestside. So it goes North, East, South, West.It is guaranteed that the construction always has walls on it's edges. The input will be correct.Your task is to print the size of the rooms from biggest to smallest.Example 1:Input:4 5        9 14 11 12 13        5 15 11 6 7        5 9 14 9 14        3 2 14 3 14Output:9 4 4 2 1 
CodeForces 
LeetCode (Hard)
Given an input string (s)and a pattern (p), implement wildcard pattern matching with support for'?'and'*'where:•'?'Matches any single character.•'*'Matches any sequence of characters (including the empty sequence).The matching should cover theentireinput string (not partial).Example 1:Input:s = "aa", p = "a"Output:falseExplanation:"a" does not match the entire string "aa".
Figure 4. Examples of competitive coding problems from Codeforces and LeetCode.
A.2. Code Testing and Solution Evaluation
The solution evaluation requires a set of input–output pairs, hidden from the user, that comprehensively test the behavior of
the program. To compute the final results, we have implemented an online evaluation infrastructure that submits the candidate
solutions to the websites’ online judges and automatically scrapes the judgment. This mechanism ensures authoritative
results.
For many of the Codeforces problems, we managed to scrape (sometimes a subset) of the hidden tests, allowing us to use
a faster, local infrastructure for evaluating candidate solutions. On the other hand, LeetCode does not expose any of the
hidden tests publicly.
For code testing at inference time, just like a human would, we rely on tests constructed from the (public) input–output
example pairs contained in the problem description.
A.3. Concurrent and Previous Works as Specific Instances of Flows
The introduction of LLMs such as BARD, GPT-3, ChatGPT, and its latest version, GPT-4, has led to a breakthrough in AI.
This has enabled many exciting developments like CoT, HuggingGPT, AutoGPT, AgentGPT, and BabyAGI. In this section,
we demonstrate how Flows provides a unified view encompassing concurrent and previous work as specific Flow instances.
The details are provided in Figure 5 and Table. 2.
1.Few shot Prompting (FS) (Brown et al., 2020) consists in providing a few input-output examples within the prompt,
acting as demonstrations to enable the LLM to perform a specific task. This technique relies on the LLM’s emergent
in-context learning ability to extrapolate from these limited examples and infer how to solve the task in general.
12

--- PAGE 13 ---
Submission and Formatting Instructions for ICML 2024
Agent FlowCoT Prompting
Agent FlowFewShot Prompting
Generator Flow
CriticFlowCollaboration(Generator-Critic)
Generator FlowFixed ReplySelf-Refine(Circular)
Plan Generator ProgramTestingSelf-Debug(Generator-Critic)
Pseudo-code Generator 
CodeGeneratorParsel(Sequential)
Action Execution (Branching)Action A
Action B
Finish…Memory (read)
Action SelectionMemory (write)
Memory (write)
Human Feedback
AutoGPT(Circular)Action Execution (Branching)Action A
Action B
Finish…ReAct(Circular)
Action SelectionReAct Prompt
Planning
Model SelectionModel Execution
Answer GenerationHuggingGPT(Sequential)
AnswerGenerator Refiner(Sequential)
Reasoning Feedback Reasoning Steps(Generator-Critic)ReasoningSteps
Camel(Circular)
AIAssistantRole Playing(Generator-Critic)AIUser
Human Input
SynthetizeAnswerMeta-Reasoner(Sequential)(Sampler)AnswerGenerator
Thought Generator Tree of Thoughts(Sampler)Chameleon(Sequential)
Planning(Tools Selection)Action Execution (Sequential)Tool A
Tool B
Answer Generator …BabyAGI(Circular)
Task (read)
Action Execution
Context FlowContext Retrieval(Generator-Critic)Vector DB
Task CreatorTask Creation(Generator-Critic)Task Creator
Task RerankerTask Prioritization(Generator-Critic)Task Creator
Figure 5. Previous works are specific Flows. We depict a selected subset of previous works incorporating structured reasoning and/or
interactions between AI agents, tools, and humans, through the lens of the Flows framework. This demonstrates that Flows is a powerful
language for describing, conceptualizing, and disseminating structured interaction patterns.
2.Chain of Thoughts (CoT) (Wei et al., 2022) is a prompting method (atomic Flow) that allows LLMs to generate a
series of intermediate natural language reasoning steps that lead to the final output.
3.Tree of Thoughts (ToT) (Yao et al., 2023a) is a framework that enables ( orchestration ) exploration over coherent units
of text (thoughts) that serve as intermediate steps toward problem-solving. ToT allows LLMs to perform deliberate
decision-making by considering multiple different reasoning paths and self-evaluating choices to decide the next course
of action, as well as looking ahead or backtracking when necessary to make global choices.
4.Program of Thoughts (PoT) (Chen et al., 2022) is a prompting method that allows language models (mainly Codex)
to express the reasoning process as a program. The computation is relegated to an external program, which executes
the generated programs to derive the answer.
5.Mutimodal CoT (M-CoT) (Zhang et al., 2023) is a method that incorporates language (text) and vision (images)
modalities into a two-stage framework that separates rationale generation and answer inference. To facilitate the
interaction between modalities in M-CoT, smaller language models (LMs) are fine-tuned by fusing multimodal features.
6.ToolFormer (Schick et al., 2023) is a model that is trained to decide which APIs to call, when to call them, what
arguments to pass, and how to incorporate the results into future tokens prediction.
7.ReAct (Yao et al., 2023b) is a framework that uses LLMs to generate reasoning traces and task-specific actions
sequentially. The framework allows for greater synergy between the two: reasoning traces help the model induce,
track, and update action plans and handle exceptions, while actions allow it to interface with external sources, such as
knowledge bases or environments, to gather additional information.
8.Parsel (Zelikman et al., 2022) is a framework that enables the automatic implementation and validation of complex
algorithms with code LLMs. The framework first synthesizes an intermediate representation based on the Parsel
language and can then apply a variety of postprocessing tools. Code is generated in a next step.
9.REFINER (Paul et al., 2023) is a framework for LMs to explicitly generate intermediate reasoning steps while
interacting with a critic model that provides automated feedback on the reasoning.
13

--- PAGE 14 ---
Submission and Formatting Instructions for ICML 2024
10.Self-Refine (Madaan et al., 2023) is a framework for LLMs to generate coherent outputs. The main idea is that an
LLM will initially generate an output while the same LLM provides feedback for its output and uses it to refine itself
iteratively.
11.Recursively Criticize and Improve (RCI) (Kim et al., 2023) showed that a pre-trained large language model (LLM)
agent could execute computer tasks guided by natural language using a simple prompting scheme where the agent
Recursively Criticizes and Improves its output (RCI). Unlike Self-refine, this method uses two separate LLMs
(ChatGPT), one for performing the task and another for criticizing.
12.Self-Correct (Welleck et al., 2023) is a framework that decouples a flawed base generator (an LLM) from a separate
corrector that learns to iteratively correct imperfect generations. The imperfect base generator can be an off-the-self
LLM or a supervised model, and the corrector model is trained.
13.Self-Debug (Chen et al., 2023) is a framework that relies on external tools (SQL application or Python interpreter) to
help large language models revise and debug SQL commands or Python code with bugs.
14.Reflexion (Shinn et al., 2023) is a framework that provides a free-form reflection on whether a step was executed by
LLM correctly or not and potential improvements. Unlike self-refine and self-debug, Reflexion builds a persisting
memory of self-reflective experiences, which enables an agent to identify its own errors and self-suggest lessons to
learn from its mistakes over time.
15.Meta-Reasoner (Yoran et al., 2023) is an approach which prompts large language models to meta-reason over multiple
chains of thought rather than aggregating their answers. This approach included two steps: (i) ask LLM to generate
multiple reasoning chains, (ii) ask another LLM (meta-reasoner) to reason over the multiple reasoning chains to arrive
at the correct answer.
16.HuggingGPT (Shen et al., 2023) is a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in
machine learning communities (e.g., Hugging Face) to solve numerous sophisticated AI tasks in different modalities
(such as language, vision, speech) and domains.
17.Camel (Li et al., 2023) is a communicative agent framework involving inception prompting to guide chat agents toward
task completion while maintaining consistency with human intentions.
18.Chameleon (Lu et al., 2023) is a plug-and-play compositional reasoning framework that augments external tools with
LLMs in a plug-and-play manner. The core idea is that an LLM-based planner assembles a sequence of tools to execute
to generate the final response. The assumption is that this will be less error-prone, easily expandable to new modules,
and user-friendly.
19.AutoGPT (Richards, 2023) is an experimental open-source application that leverages the capabilities of large language
models (LLMs) and Chatbots such as OpenAI’s GPT-4 and Chat-GPT to create fully autonomous and customizable AI
agents. It has internet access, long-term and short-term memory management.
20.BabyAGI (Nakajima, 2023) is an intelligent agent capable of generating and attempting to execute tasks based on
a given objective. BabyAGI operates based on three LLM flows: Task creation flow, Task prioritization flow, and
Execution flow.
A.4. Prompting
We provide the prompts used to obtain the results in Section 6. Our evaluation is made possible thanks to the modular and
compositional nature of Flows . Some of the experimental setups are deeply nested, and in cases where Flows build on each
other, we avoid repetition. Note that the project’s GitHub repository provides the code and data to reproduce all of the
experiments in the paper.
Direct prompting for a solution is shown in Listing 1. To add reflection, we use a Generator-Critic Flow to combine the
code generation with a fixed reply, as shown in Listing 2. In the collaboration setting, we use Listing 3 as the generator and
Listing 4 as the critic.
Debugging is incorporated via a testing Flow that adds formatting to the output of a code executor. The formatting templates
are shown in Listing 6. To respond to the debug output, we rely on an adjusted coding Flow 5. Adding collaboration in the
14

--- PAGE 15 ---
Submission and Formatting Instructions for ICML 2024
Flows Flow Type Interactions Reasoning Patterns Feedback Learning
Self Multi-Ag. Human Tools Struct. Plan
FS (Brown et al., 2020) Atomic ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗
CoT (Wei et al., 2022) Atomic ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✗
ToT (Yao et al., 2023a) Circular ✓ ✗ ✗ ✓ ✓ ✗ ✗ ✗
PoT (Chen et al., 2022) Seq. ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✗
M-CoT (Zhang et al., 2023) Seq. ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✓
ToolFormer (Wei et al., 2022) Seq. ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✓
ReAct (Yao et al., 2023b) Circular ✗ ✗ ✗ ✓ ✓ ✗ ✗ ✗
Parsel (Zelikman et al., 2022) Seq. ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗
REFINER (Paul et al., 2023) Gen-Crit ✗ ✓ ✓ ✗ ✓ ✗ ✓ ✓
Self-Refine (Madaan et al., 2023) Gen-Crit ✓ ✗ ✗ ✗ ✓ ✗ ✓ ✗
RCI (Kim et al., 2023) Gen-Crit ✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗
Self-Correct (Welleck et al., 2023) Gen-Crit ✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗
Self-Debug (Chen et al., 2023) Gen-Crit ✓ ✗ ✗ ✓ ✓ ✗ ✓ ✗
Reflexion (Shinn et al., 2023) Gen-Crit ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✗
Meta-Reasoner (Yoran et al., 2023) Seq. ✓ ✓ ✗ ✗ ✓ ✗ ✗ ✗
HuggingGPT (Shen et al., 2023) Seq. ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗
Camel (Li et al., 2023) Circular ✗ ✓ ✓ ✗ ✓ ✗ ✓ ✗
Chameleon (Lu et al., 2023) Seq. ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗
AutoGPT (Richards, 2023) Circular ✓ ✓ ✗ ✓ ✓ ✓ ✓ ✗
BabyAGI (Nakajima, 2023) Circular ✗ ✓ ✗ ✓ ✓ ✓ ✗ ✗
Table 2. Previous work. We compare previous work across relevant dimensions.
debugging setting is done by introducing a critic that provides feedback grounded in the test results. This Flow is detailed in
Listing 3.
The scenarios explained above also support the addition of a planning Flow. An example of plan generation is shown in
Listing 8.
Listing 1. Prompts for Code Flow (Codeforces)
" prompt t e m p l a t e s " :
" system_message " : | −
Your g o a l i s t o p r o v i d e e x e c u t a b l e Python code t h a t s o l v e s a c o m p e t i t i v e
programming problem . The code s h o u l d c o r r e c t l y h a n d l e a l l c o r n e r c a s e s i n
o r d e r t o p a s s t h e h i d d e n t e s t c a s e s , which a r e used t o e v a l u a t e t h e
c o r r e c t n e s s of t h e s o l u t i o n .
The u s e r w i l l s p e c i f y t h e problem by p r o v i d i n g you wit h :
− t h e problem s t a t e m e n t
− i n p u t d e s c r i p t i o n
− o u t p u t d e s c r i p t i o n
− example t e s t c a s e s
− ( o p t i o n a l ) e x p l a n a t i o n of t h e t e s t c a s e s
The u s e r w i l l p r o v i d e you w it h a t a s k and an o u t p u t f o r m a t t h a t you w i l l
s t r i c t l y f o l l o w .
" query_message " : | −
# Problem s t a t e m e n t
{{ p r o b l e m _ d e s c r i p t i o n }}
# I n p u t d e s c r i p t i o n
{{ i n p u t _ d e s c r i p t i o n }}
# Output d e s c r i p t i o n
{{ o u t p u t _ d e s c r i p t i o n }}
15

--- PAGE 16 ---
Submission and Formatting Instructions for ICML 2024
{{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}
The i n p u t s h o u l d be r e a d from t h e s t a n d a r d i n p u t and t h e o u t p u t s h o u l d be
p a s s e d t o t h e s t a n d a r d o u t p u t .
R e t u r n Python code t h a t s o l v e s t h e problem . Reply i n t h e f o l l o w i n g f o r m a t :
```python
{{ c o d e _ p l a c e h o l d e r }}
```
" human_message " : | −
{{ query }}
Listing 2. Prompts for Fixed-Reply Flow
" prompt t e m p l a t e s " :
" f i x e d _ r e p l y " : | −
C o n s i d e r t h e problem s t a t e m e n t and t h e l a s t p r o p o s e d s o l u t i o n . Are you s u r e
t h a t t h e s o l u t i o n i s p r o v i d e d i n t h e r e q u e s t e d format , and c r u c i a l l y ,
s o l v e s t h e problem ?
I f t h a t i s n o t t h e case , p r o v i d e t h e c o r r e c t e d v e r s i o n of t h e code i n t h e
f o l l o w i n g f o r m a t :
```python
{{ python_code }}
```
o t h e r w i s e , r e p l y :
" F i n a l answer . "
Listing 3. Prompts for Code-Collab Flow (Codeforces)
" prompt t e m p l a t e s " :
" system_message " : | −
Your g o a l i s t o p r o v i d e e x e c u t a b l e Python code t h a t s o l v e s a c o m p e t i t i v e
programming problem . The code s h o u l d c o r r e c t l y h a n d l e a l l c o r n e r c a s e s i n
o r d e r t o p a s s t h e h i d d e n t e s t c a s e s , which a r e used t o e v a l u a t e t h e
c o r r e c t n e s s of t h e s o l u t i o n .
The u s e r w i l l s p e c i f y t h e problem by p r o v i d i n g you wit h :
− t h e problem s t a t e m e n t
− i n p u t d e s c r i p t i o n
− o u t p u t d e s c r i p t i o n
− example t e s t c a s e s
− ( o p t i o n a l ) e x p l a n a t i o n of t h e t e s t c a s e s
The u s e r w i l l p r o v i d e you w it h a t a s k and an o u t p u t f o r m a t t h a t you w i l l
s t r i c t l y f o l l o w .
" query_message " : | −
# Problem s t a t e m e n t
{{ p r o b l e m _ d e s c r i p t i o n }}
# I n p u t d e s c r i p t i o n
{{ i n p u t _ d e s c r i p t i o n }}
# Output d e s c r i p t i o n
{{ o u t p u t _ d e s c r i p t i o n }}
16

--- PAGE 17 ---
Submission and Formatting Instructions for ICML 2024
{{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}
The i n p u t s h o u l d be r e a d from t h e s t a n d a r d i n p u t and t h e o u t p u t s h o u l d be
p a s s e d t o t h e s t a n d a r d o u t p u t .
R e t u r n Python code t h a t s o l v e s t h e problem . Reply i n t h e f o l l o w i n g f o r m a t :
```python
{{ c o d e _ p l a c e h o l d e r }}
```
" human_message " : | −
# Feedback on t h e l a s t p r o p o s e d s o l u t i o n
{{ c o d e _ f e e d b a c k }}
C o n s i d e r t h e o r i g i n a l problem s t a t e m e n t , t h e l a s t p r o p o s e d s o l u t i o n and t h e
p r o v i d e d f e e d b a c k . Does t h e s o l u t i o n need t o be u p d a t e d ? I f so , p r o v i d e
t h e c o r r e c t e d v e r s i o n of t h e code i n t h e f o l l o w i n g f o r m a t :
```python
{{ c o d e _ p l a c e h o l d e r }}
```
o t h e r w i s e , r e p l y :
" F i n a l answer . "
Listing 4. Prompts for Code-Collab-Critic Flow (Codeforces)
" prompt t e m p l a t e s " :
" system_message " : | −
Your g o a l i s t o i d e n t i f y p o t e n t i a l i s s u e s wi th a c o m p e t i t i v e programming
s o l u t i o n a t t e m p t .
The u s e r w i l l s p e c i f y t h e problem by p r o v i d i n g you wit h :
− t h e problem s t a t e m e n t
− i n p u t d e s c r i p t i o n
− o u t p u t d e s c r i p t i o n
− example t e s t c a s e s
− ( o p t i o n a l ) e x p l a n a t i o n of t h e t e s t c a s e s
− a Python s o l u t i o n a t t e m p t
C r u c i a l l y , your g o a l i s t o c o r r e c t l y i d e n t i f y p o t e n t i a l i s s u e s w it h t h e
s o l u t i o n a t t e m p t , and n o t t o p r o v i d e t h e code i m p l e m e n t a t i o n y o u r s e l f .
The u s e r w i l l p r o v i d e you w it h a t a s k and an o u t p u t f o r m a t t h a t you w i l l
s t r i c t l y f o l l o w .
" query_message " : | −
# Problem s t a t e m e n t
{{ p r o b l e m _ d e s c r i p t i o n }}
# I n p u t d e s c r i p t i o n
{{ i n p u t _ d e s c r i p t i o n }}
# Output d e s c r i p t i o n
{{ o u t p u t _ d e s c r i p t i o n }}
{{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}
17

--- PAGE 18 ---
Submission and Formatting Instructions for ICML 2024
# Python s o l u t i o n a t t e m p t :
```python
{{ code }}
```
C o n s i d e r t h e problem s t a t e m e n t and t h e s o l u t i o n a t t e m p t . Are t h e r e any i s s u e s
wi th t h e p r o p o s e d s o l u t i o n or i t i s c o r r e c t ? E x p l a i n your r e a s o n i n g ve r y
c o n c i s e l y , and do n o t p r o v i d e code .
" human_message " : | −
{{ query }}
Listing 5. Prompts for Code-Debug Flow (Codeforces)
" prompt t e m p l a t e s " :
" system_message " : | −
Your g o a l i s t o p r o v i d e e x e c u t a b l e Python code t h a t s o l v e s a c o m p e t i t i v e
programming problem . The code s h o u l d c o r r e c t l y h a n d l e a l l c o r n e r c a s e s i n
o r d e r t o p a s s t h e h i d d e n t e s t c a s e s , which a r e used t o e v a l u a t e t h e
c o r r e c t n e s s of t h e s o l u t i o n .
The u s e r w i l l s p e c i f y t h e problem by p r o v i d i n g you wit h :
− t h e problem s t a t e m e n t
− i n p u t d e s c r i p t i o n
− o u t p u t d e s c r i p t i o n
− example t e s t c a s e s
− ( o p t i o n a l ) e x p l a n a t i o n of t h e t e s t c a s e s
The u s e r w i l l p r o v i d e you w it h a t a s k and an o u t p u t f o r m a t t h a t you w i l l
s t r i c t l y f o l l o w .
" query_message " : | −
# Problem s t a t e m e n t
{{ p r o b l e m _ d e s c r i p t i o n }}
# I n p u t d e s c r i p t i o n
{{ i n p u t _ d e s c r i p t i o n }}
# Output d e s c r i p t i o n
{{ o u t p u t _ d e s c r i p t i o n }}
{{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}
The i n p u t s h o u l d be r e a d from t h e s t a n d a r d i n p u t and t h e o u t p u t s h o u l d be
p a s s e d t o t h e s t a n d a r d o u t p u t .
R e t u r n Python code t h a t s o l v e s t h e problem . Reply i n t h e f o l l o w i n g f o r m a t :
```python
{{ c o d e _ p l a c e h o l d e r }}
```
" human_message " : | −
{{ t e s t i n g _ r e s u l t s _ s u m m a r y }}
18

--- PAGE 19 ---
Submission and Formatting Instructions for ICML 2024
C o n s i d e r t h e problem s t a t e m e n t , t h e l a s t p r o p o s e d s o l u t i o n , and i t s i s s u e .
P r o v i d e a c o r r e c t e d v e r s i o n of t h e code t h a t s o l v e s t h e o r i g i n a l problem
and r e s o l v e s t h e i s s u e , w i t h o u t any e x p l a n a t i o n , i n t h e f o l l o w i n g f o r m a t :
```python
{{ c o d e _ p l a c e h o l d e r }}
```
Listing 6. Formatting templates for Code-Testing Flow (Codeforces)
" f o r m a t t i n g t e m p l a t e s " :
" no e r r o r t e m p l a t e " : | −
$ { . i s s u e _ t i t l e }
A l l of t h e e x e c u t e d t e s t s p a s s e d .
" a l l t e s t s h e a d e r " : | −
$ { . i s s u e _ t i t l e }
The Python code does n o t s o l v e t h e problem i n t h e problem d e s c r i p t i o n due t o
l o g i c a l e r r o r s . I t f a i l s on t h e f o l l o w i n g t e s t s .
" c o m p i l a t i o n e r r o r t e m p l a t e " : | −
$ { . i s s u e _ t i t l e }
The e x e c u t i o n r e s u l t e d i n a c o m p i l a t i o n e r r o r .
## C o m p i l a t i o n e r r o r message :
{{ e r r o r _ m e s s a g e }}
" t i m e o u t e r r o r t e m p l a t e " : | −
$ { . i s s u e _ t i t l e }
The e x e c u t i o n timed out , t h e s o l u t i o n i s n o t e f f i c i e n t enough .
" r u n t i m e e r r o r t e m p l a t e " : | −
$ { . i s s u e _ t i t l e }
The e x e c u t i o n r e s u l t e d i n a r u n t i m e e r r o r on t h e f o l l o w i n g t e s t .
## [ F a i l e d t e s t ] I n p u t
```
{{ t e s t _ i n p u t }}
```
## [ F a i l e d t e s t ] Runtime e r r o r message
{{ e r r o r _ m e s s a g e }}
" s i n g l e t e s t e r r o r " : | −
$ { . i s s u e _ t i t l e }
The Python code does n o t s o l v e t h e problem i n t h e problem d e s c r i p t i o n due t o
l o g i c a l e r r o r s . I t f a i l s t h e f o l l o w i n g t e s t :
## [ F a i l e d t e s t ] I n p u t
```
{{ t e s t _ i n p u t }}
```
## [ F a i l e d t e s t ] Expected o u t p u t
```
{{ e x p e c t e d _ o u t p u t }}
```
## [ F a i l e d t e s t ] G e n e r a t e d o u t p u t
```
{{ g e n e r a t e d _ o u t p u t }}
```
" t e s t e r r o r " : | −
## [ F a i l e d t e s t {{ i d x } } ]
### [ F a i l e d t e s t {{ i d x } } ] I n p u t
```
19

--- PAGE 20 ---
Submission and Formatting Instructions for ICML 2024
{{ t e s t _ i n p u t }}
```
### [ F a i l e d t e s t {{ i d x } } ] Expected o u t p u t
```
{{ e x p e c t e d _ o u t p u t }}
```
### [ F a i l e d t e s t {{ i d x } } ] G e n e r a t e d o u t p u t
```
{{ g e n e r a t e d _ o u t p u t }}
```
Listing 7. Prompts for Code-Debug-Collab Flow (Codeforces)
" prompt t e m p l a t e s " :
" system_message " : | −
Your g o a l i s t o i d e n t i f y t h e i s s u e s wi th an i n c o r r e c t c o m p e t i t i v e programming
s o l u t i o n a t t e m p t .
The u s e r w i l l s p e c i f y t h e problem by p r o v i d i n g you wit h :
− t h e problem s t a t e m e n t
− i n p u t d e s c r i p t i o n
− o u t p u t d e s c r i p t i o n
− example t e s t c a s e s
− ( o p t i o n a l ) e x p l a n a t i o n of t h e t e s t c a s e s
− an i n c o r r e c t Python s o l u t i o n a t t e m p t and a d e s c r i p t i o n of i t s i s s u e
C r u c i a l l y , your g o a l i s t o c o n s i d e r a l l a s p e c t s of t h e problem and p i n p o i n t
t h e i s s u e s wi th t h e s o l u t i o n a t t e m p t , and n o t t o p r o v i d e t h e code
i m p l e m e n t a t i o n y o u r s e l f .
Some a s p e c t s t o c o n s i d e r : I s t h e i n p u t c o r r e c t l y p a r s e d ? I s t h e o u t p u t
c o r r e c t l y f o r m a t t e d ? Are t h e c o r n e r c a s e s c o r r e c t l y h a n d l e d ? I s t h e r e a
l o g i c a l m i s t a k e wi th t h e a l g o r i t h m i t s e l f ?
Use t h e code e x e c u t i o n r e s u l t s p r o v i d e d i n t h e i s s u e d e s c r i p t i o n t o g u i d e
your r e a s o n i n g / debugging .
" query_message " : | −
# Problem s t a t e m e n t
{{ p r o b l e m _ d e s c r i p t i o n }}
# I n p u t d e s c r i p t i o n
{{ i n p u t _ d e s c r i p t i o n }}
# Output d e s c r i p t i o n
{{ o u t p u t _ d e s c r i p t i o n }}
{{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}
# S o l u t i o n a t t e m p t t o be f i x e d
```python
{{ code }}
```
{{ t e s t i n g _ r e s u l t s _ s u m m a r y }}
20

--- PAGE 21 ---
Submission and Formatting Instructions for ICML 2024
C o n s i d e r t h e problem s t a t e m e n t , t h e s o l u t i o n a t t e m p t and t h e i s s u e . Why i s
t h e s o l u t i o n a t t e m p t i n c o r r e c t ? How s h o u l d i t be f i x e d ? E x p l a i n your
r e a s o n i n g v e ry c o n c i s e l y , and do n o t p r o v i d e code .
" human_message " : | −
{{ query }}
Listing 8. Prompts for Plan Flow (Codeforces)
" prompt t e m p l a t e s " :
" system_message " : | −
Your g o a l i s t o p r o v i d e a high − l e v e l c o n c e p t u a l s o l u t i o n t h a t , i f implemented
, w i l l s o l v e a g i v e n c o m p e t i t i v e programming problem .
The u s e r w i l l s p e c i f y t h e problem by p r o v i d i n g you wit h :
− t h e problem s t a t e m e n t
− i n p u t d e s c r i p t i o n
− o u t p u t d e s c r i p t i o n
− example t e s t c a s e s
− ( o p t i o n a l ) e x p l a n a t i o n of t h e t e s t c a s e s
The p r o p o s e d a l g o r i t h m s h o u l d be c o m p u t a t i o n a l l y e f f i c i e n t , l o g i c a l l y c o r r e c t
and h a n d l e a l l c o r n e r c a s e s .
The u s e r w i l l p r o v i d e you w it h a t a s k and an o u t p u t f o r m a t t h a t you w i l l
s t r i c t l y f o l l o w .
" query_message " : | −
# Problem s t a t e m e n t
{{ p r o b l e m _ d e s c r i p t i o n }}
# I n p u t d e s c r i p t i o n
{{ i n p u t _ d e s c r i p t i o n }}
# Output d e s c r i p t i o n
{{ o u t p u t _ d e s c r i p t i o n }}
{{ i o _ e x a m p l e s _ a n d _ e x p l a n a t i o n }}
R e t u r n a high − l e v e l c o n c e p t u a l s o l u t i o n t h a t would s o l v e t h e problem . Be ve r y
c o n c i s e , and do n o t p r o v i d e code .
Reply i n t h e f o l l o w i n g f o r m a t :
# C o n c e p t u a l s o l u t i o n
{{ p l a n _ p l a c e h o l d e r }}
" human_message " : | −
{{ query }}
A.5. The CC-Flows-competition: a new form of competitive coding
Solving competitive coding challenges is an eminently hard problem. The solve rate of only 27% by directly attempting the
problem and 47% by the best-performing code Flow, paired with a reliable automatic evaluation metric, make competitive
programming an ideal benchmark for AI systems. Motivated by this, we propose a competition where instead of people,
proposed Flows solve competitive programming problems.
The competition will leverage the comprehensive dataset of publicly available Codeforces problems and the open-source
infrastructure for inference and testing used in the experiments, available at https://github.com/epfl-dlab/cc_flows .
21

--- PAGE 22 ---
Submission and Formatting Instructions for ICML 2024
The competition will only include problems published after the knowledge-cutoff date of GPT-4. Furthermore, not to
overload the Codeforces online evaluation infrastructure, we further filter this dataset to problems for which public and
private tests are available, and the output format is compatible with our local code testing infrastructure. Codeforces ranks
the difficulty of each problem from 800 to 2100. At the time of publishing, we have the following number of problems per
difficulty (total of 416):
• difficulty 800: 149
• difficulty 900 to 1500 (inclusive): 185
• difficulty 1600 to 2100 (inclusive): 82
We will curate a leaderboard of best-performing Flows that will be publicly available on FlowVerse and provide the
predictions that reproduce the reported scores using the provided infrastructure.
The data will be released and should be used in accordance with Codeforces’ Terms and Conditions. Concretely, Codeforces
prohibits the material from being sold, sublicensed, or commercialized. For more details, take a look at the project’s GitHub
page.
22
