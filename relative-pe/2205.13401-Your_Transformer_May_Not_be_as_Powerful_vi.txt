# Transformer của bạn có thể không mạnh mẽ như bạn mong đợi

Shengjie Luo1;5, Shanda Li2, Shuxin Zheng3, Tie-Yan Liu3, Liwei Wang1;4y, Di He1y
1Phòng thí nghiệm trọng điểm Cảm nhận Máy, MOE
Trường Khoa học và Công nghệ Trí tuệ, Đại học Bắc Kinh
2Khoa Học máy, Trường Khoa học Máy tính, Đại học Carnegie Mellon
3Microsoft Research4Trung tâm Khoa học Dữ liệu, Đại học Bắc Kinh5Zhejiang Lab
luosj@stu.pku.edu.cn, shandal@cs.cmu.edu,
{shuz, tyliu}@microsoft.com, {wanglw,dihe}@pku.edu.cn

## Tóm tắt
Mã hóa Vị trí Tương đối (RPE), mã hóa khoảng cách tương đối giữa bất kỳ cặp token nào, là một trong những sửa đổi thành công nhất đối với Transformer gốc. Theo như chúng tôi biết, hiểu biết lý thuyết về các Transformer dựa trên RPE phần lớn chưa được khám phá. Trong công trình này, chúng tôi phân tích toán học sức mạnh của các Transformer dựa trên RPE liên quan đến việc liệu mô hình có khả năng xấp xỉ bất kỳ hàm liên tục chuỗi-đến-chuỗi nào không. Người ta có thể tự nhiên giả định rằng câu trả lời là khẳng định—các Transformer dựa trên RPE là các bộ xấp xỉ hàm toàn cục. Tuy nhiên, chúng tôi trình bày một kết quả tiêu cực bằng cách chỉ ra rằng tồn tại các hàm liên tục chuỗi-đến-chuỗi mà các Transformer dựa trên RPE không thể xấp xỉ bất kể mạng neural sâu và rộng đến mức nào. Một lý do chính nằm ở chỗ hầu hết các RPE được đặt trong attention softmax luôn tạo ra một ma trận ngẫu nhiên phải. Điều này hạn chế mạng khỏi việc nắm bắt thông tin vị trí trong các RPE và giới hạn khả năng của nó. Để khắc phục vấn đề và làm cho mô hình mạnh mẽ hơn, chúng tôi đầu tiên trình bày các điều kiện đủ để các Transformer dựa trên RPE đạt được xấp xỉ hàm toàn cục. Với hướng dẫn lý thuyết, chúng tôi phát triển một module attention mới, được gọi là Attention dựa trên RPE Toàn cục (URPE), thỏa mãn các điều kiện. Do đó, các Transformer dựa trên URPE tương ứng trở thành các bộ xấp xỉ hàm toàn cục. Các thí nghiệm mở rộng bao gồm các kiến trúc và nhiệm vụ điển hình chứng minh rằng mô hình của chúng tôi có hiệu quả về tham số và có thể đạt được hiệu suất vượt trội so với các baseline mạnh trong một loạt rộng các ứng dụng. Mã sẽ được công khai tại https://github.com/lsj2408/URPE.

## 1 Giới thiệu
Transformer được công nhận rộng rãi là một mạng neural mạnh mẽ trong việc mô hình hóa dữ liệu tuần tự. Mã hóa Vị trí Tương đối (RPE) là một trong những sửa đổi thành công nhất đối với mô hình Transformer. Không giống như Mã hóa Vị trí Tuyệt đối (APE) được thiết kế ban đầu mã hóa mỗi vị trí như một vector nhúng, RPE mã hóa khoảng cách tương đối giữa bất kỳ cặp token nào và thường được đặt trong hàm mũ softmax trong module self-attention.

Theo thực nghiệm, nhiều nghiên cứu cho thấy rằng các Transformer dựa trên RPE có thể đạt được hiệu suất ấn tượng trên các nhiệm vụ ngôn ngữ khác nhau và có khả năng ngoại suy tốt hơn trên các chuỗi dài hơn. Một điểm khác đáng chú ý là RPE làm cho Transformer dễ dàng được mở rộng sang các phương thức dữ liệu khác, như hình ảnh và đồ thị, vì khoảng cách tương đối tự nhiên bảo tồn các thuộc tính bất biến cho một số phép biến đổi quan trọng như xoay và tịnh tiến.

Trong bài báo này, chúng tôi đầu tiên điều tra khía cạnh lý thuyết của các Transformer dựa trên RPE. Đặc biệt, chúng tôi nghiên cứu sức mạnh biểu đạt của chúng, mô tả khả năng của các mô hình trong việc xấp xỉ bất kỳ hàm liên tục nào. Gần đây, Yun et al. đã chứng minh rằng các Transformer dựa trên APE là các bộ xấp xỉ toàn cục của các hàm liên tục chuỗi-đến-chuỗi trên một miền compact, và người ta có thể mong đợi rằng các Transformer dựa trên RPE cũng có cùng tính chất. Tuy nhiên, chúng tôi cung cấp một phát hiện lý thuyết đáng ngạc nhiên cho thấy rằng các Transformer với RPE được sử dụng rộng rãi không phải là các bộ xấp xỉ hàm toàn cục, tức là, tồn tại các hàm liên tục chuỗi-đến-chuỗi mà các mô hình không thể xấp xỉ bất kể mô hình sâu và rộng đến mức nào. Một quan sát chính là các RPE được đặt bên trong softmax trong module attention. Toán tử softmax luôn tạo ra một ma trận ngẫu nhiên phải, không thể phản ánh đủ thông tin vị trí được mã hóa trong RPE ra đầu ra. Các nhiệm vụ tổng hợp được thực hiện để hỗ trợ tuyên bố toán học này.

Để thiết kế một Transformer dựa trên RPE mạnh mẽ hơn, chúng tôi đi sâu vào hạn chế của mô hình và lý thuyết suy ra hai điều kiện đủ để đạt được xấp xỉ hàm toàn cục: điều kiện chú ý và điều kiện nhận biết vị trí. Cả hai điều kiện cùng nhau nêu rằng lớp hàm attention dựa trên RPE nên bao gồm một số trường hợp đặc biệt của attention được thiết kế ban đầu và phá vỡ hạn chế ma-trận-ngẫu-nhiên-phải. Với hướng dẫn lý thuyết như vậy, chúng tôi phát triển một module attention mới được gọi là Attention dựa trên RPE Toàn cục (URPE) thỏa mãn các điều kiện trên. Do đó, các Transformer với URPE-based Attention, được gọi là Transformer dựa trên URPE, là các bộ xấp xỉ hàm toàn cục. Chúng tôi chỉ ra kiến trúc được đề xuất của chúng tôi dễ thực hiện và hiệu quả về tham số thông qua các thí nghiệm mở rộng bao gồm các kiến trúc mô hình điển hình và các nhiệm vụ (nhiệm vụ tổng hợp, mô hình hóa ngôn ngữ, và học đồ thị). Mô hình của chúng tôi mang lại những cải thiện hiệu suất nhất quán so với các Transformer dựa trên RPE hiện có trên một loạt rộng các nhiệm vụ.

Bài báo được tổ chức như sau. Trong Phần 2, chúng tôi giới thiệu nền tảng về kiến trúc Transformer và các phương pháp mã hóa vị trí. Trong Phần 3, chúng tôi chứng minh rằng các Transformer dựa trên RPE được sử dụng rộng rãi không phải là các bộ xấp xỉ hàm toàn cục. Trong Phần 4, chúng tôi tiếp tục trình bày các điều kiện đủ để các Transformer dựa trên RPE đạt được xấp xỉ toàn cục, và phát triển một module attention mới, URPE-based Attention, để xây dựng một Transformer dựa trên RPE toàn cục. Các thí nghiệm được trình bày trong Phần 5 để chứng minh hiệu quả của các Transformer với URPE-based Attention được đề xuất của chúng tôi. Các công trình liên quan và kết luận được thảo luận trong hai phần cuối.

## 2 Kiến thức chuẩn bị
Kiến trúc Transformer được cấu thành từ các khối Transformer xếp chồng. Một khối Transformer là một ánh xạ chuỗi-đến-chuỗi từ R^(n×d) đến R^(n×d), trong đó n là độ dài chuỗi và d là chiều của token embedding. Một khối Transformer bao gồm hai lớp: một lớp self-attention theo sau bởi một lớp feed-forward, với cả hai lớp đều có chuẩn hóa (ví dụ, LayerNorm, RMSNorm) và kết nối bỏ qua. Đối với một đầu vào X ∈ R^(n×d), lớp self-attention và lớp feed-forward được định nghĩa như sau:

A^h(X) = softmax((XW^h_Q)(XW^h_K)^T/√d_H)                     (1)
Attn(X) = X + ∑_{h=1}^H A^h(X)XW^h_VW^h_O                    (2)
FFN(X) = X + ReLU(XW_1)W_2                                   (3)

trong đó W^h_O ∈ R^(d_H×d), W^h_Q, W^h_K, W^h_V ∈ R^(d×d_H), W_1 ∈ R^(d×r), W_2 ∈ R^(r×d). H là số lượng attention head, d_H là chiều của mỗi head, và r là chiều của lớp ẩn. A^h(X) thường được gọi là ma trận attention. Với H, d_H và r được định nghĩa trước, chúng tôi gọi lớp hàm của các khối Transformer là T_blocks(H; d_H; r).

**Transformer với Mã hóa Vị trí Tuyệt đối.** Các lớp self-attention và feed-forward được định nghĩa trong Eq.(2) và (3) hoàn toàn bất biến với thứ tự chuỗi. Do đó, các khối Transformer xếp chồng thuần túy không thể phân biệt thông tin ở các vị trí khác nhau. Transformer gốc đề xuất Mã hóa Vị trí Tuyệt đối (APE) để cung cấp cho các mạng Transformer khả năng nắm bắt thông tin vị trí. Đặc biệt, một embedding có giá trị thực (có thể học được) e_i ∈ R^d được gán cho mỗi vị trí i, dẫn đến một ma trận Mã hóa Vị trí Tuyệt đối E = [e_1; ...; e_n]^T, sẽ được cộng vào chuỗi đầu vào. Chính thức, lớp hàm được biểu diễn bởi các Transformer dựa trên APE là

Ω^{H,d_H,r}_{APE} = {f(X) = g(X + E) | E ∈ R^{n×d}, g = g_L ∘ ... ∘ g_1, g_i ∈ T_blocks(H; d_H; r), L ∈ N}

APE về cơ bản tăng cường sức mạnh biểu đạt của các Transformer. Yun et al. đã chứng minh kết quả lý thuyết sau, cho thấy rằng các Transformer dựa trên APE có thể xấp xỉ bất kỳ hàm liên tục chuỗi-đến-chuỗi nào trong một miền compact.

**Định lý 1 (không chính thức).** Cho n, d ∈ N, lớp hàm của các Transformer với APE, Ω^{2,1,4}_{APE}, là một bộ xấp xỉ toàn cục cho các hàm liên tục ánh xạ một miền compact trong R^{n×d} đến R^{n×d}.

Mặc dù các Transformer với APE về mặt khái niệm đơn giản và có các tính chất lý thuyết tốt, chúng có một số nhược điểm đã biết. Ví dụ, Press et al. đã chỉ ra rằng các Transformer dựa trên APE thường tổng quát hóa kém đối với các chuỗi dài hơn, vì những embedding vị trí cho các chỉ số lớn hiếm khi được huấn luyện. Nhiều công trình sử dụng Mã hóa Vị trí Tương đối (RPE), trở nên ngày càng phổ biến như một cách mạnh mẽ để mã hóa thông tin vị trí cho các Transformer và phần lớn khắc phục những nhược điểm của APE.

**Transformer với Mã hóa Vị trí Tương đối.** Khác với APE gán một embedding e_i cho mỗi vị trí i, Mã hóa Vị trí Tương đối (RPE) mã hóa khoảng cách tương đối i-j cho mỗi cặp vị trí (i,j). Với mã hóa vị trí tương đối, hầu hết các công trình trước đây đã sửa đổi tính toán attention được định nghĩa trong Eq.(1) như sau:

A^h_{RPE}(X) = softmax((XW^h_Q)(XW^h_K)^T + B)              (4)

trong đó B là một ma trận n×n. Phần tử thứ (i,j) của B, ký hiệu là b_{ij}, mô hình hóa tương tác giữa vị trí thứ i và thứ j. Các tham số hóa khác nhau của B dẫn đến các kiến trúc mô hình khác nhau. Một số ví dụ nổi tiếng bao gồm:

• **Shaw's RPE**: b_{ij} = X_i W^h_Q r^T_{i-j}, trong đó r_{i-j} là các vector có thể học được.
• **T5**: b_{ij} = m_{i-j}, trong đó m_{i-j} là các scalar có thể học được, tức là, B được tham số hóa như một ma trận Toeplitz.
• **DeBERTa**: b_{ij} = X_i W^h_Q r^T_{i-j} + s_{i-j}(X_j W^h_K)^T, trong đó r_{i-j} và s_{i-j} là các vector có thể học được.
• **Transformer-XL**: b_{ij} = X_i W^h_Q (r_{i-j} \tilde{W}^h_K)^T + u(X_j W^h_K)^T + v(r_{i-j} \tilde{W}^h_K)^T, trong đó u, v và \tilde{W}^h_K đều là các vector/ma trận có thể học được, và r_{i-j} là các vector mã hóa vị trí sinusoidal cố định trong quá trình huấn luyện.

Một số hiện tượng thú vị cho thấy rằng các Transformer dựa trên RPE có nhiều ưu điểm so với các đối tác dựa trên APE của chúng. Press et al. đã chứng minh rằng các Transformer dựa trên RPE tổng quát hóa tốt hơn trên các chuỗi dài hơn. T5 và Transformer-XL cho thấy rằng các Transformer với RPE có thể đạt được hiệu suất mạnh trong các nhiệm vụ hiểu ngôn ngữ và tạo ngôn ngữ. Gần đây, các RPE cũng được sử dụng phổ biến trong các lĩnh vực khác để mã hóa các tín hiệu cấu trúc bất biến dịch chuyển/xoay. Các ví dụ điển hình bao gồm Swin Transformer và Graphormer, cả hai đều sử dụng RPE và đạt được hiệu suất tối tân trong việc học biểu diễn hình ảnh và đồ thị.

## 3 Các Transformer với RPE không phải là Bộ xấp xỉ Toàn cục

Chúng tôi quan tâm đến sức mạnh biểu đạt của các Transformer với RPE và điều tra xem kiến trúc này có mạnh mẽ như các Transformer dựa trên APE gốc hay không. Để so sánh, chúng tôi tương tự định nghĩa lớp hàm của các khối Transformer với attention dựa trên RPE (Eq.(4)) là T_blocks_RPE(H; d_H; r), trong đó ma trận mã hóa vị trí tương đối B được giả định là một ánh xạ tham số tùy ý từ đầu vào X đến một ma trận n×n. Lớp hàm được biểu diễn bởi các Transformer với RPE được định nghĩa là:

Ω^{H,d_H,r}_{RPE} = {g_L ∘ ... ∘ g_1 : R^{n×d} → R^{n×d} | g_1, ..., g_L ∈ T_blocks_RPE(H; d_H; r), L ∈ N}

Một cách đáng ngạc nhiên, chúng tôi trình bày một kết quả lý thuyết tiêu cực: chúng tôi chứng minh rằng lớp hàm của các Transformer với RPE, Ω^{H,d_H,r}_{RPE}, không phải là một bộ xấp xỉ toàn cục cho các hàm chuỗi-đến-chuỗi.

**Định lý 2.** Cho n > 2, d và D ⊆ R^{n×d}, giả sử rằng ma trận toàn không 0 ∈ D. Đối với bất kỳ M > 0, tồn tại một hàm liên tục \tilde{g}_M : D → R^{n×d}, sao cho

sup_{X∈D} ||\tilde{g}_M(X) - g(X)||_F > M                    (5)

đúng cho bất kỳ g ∈ Ω^{H,d_H,r}_{RPE}, trong đó H, d_H, r ∈ N.

**Chứng minh.** Không mất tính tổng quát, chúng tôi chứng minh định lý cho d = 1. Chứng minh có thể dễ dàng mở rộng cho các cài đặt d > 1. Cho M > 0, chúng tôi xem xét một hàm chuỗi-đến-chuỗi cụ thể làm mục tiêu: \tilde{g}_M : X ↦ (2M, 0, ..., 0)^T. Để chỉ ra sup_{X∈D} ||\tilde{g}_M(X) - g(X)||_F > M đúng cho bất kỳ g ∈ Ω^{H,d_H,r}_{RPE}, chúng tôi chọn một đầu vào X, được cấu thành từ n vector hàng giống hệt nhau trong R^d, tức là, chuỗi bao gồm n token giống hệt nhau. Vì hàm A^h_{RPE}(X) xuất ra một ma trận ngẫu nhiên phải, dễ dàng kiểm tra rằng Attn(X) = X + ∑_{h=1}^H A^h(X)XW^h_VW^h_O cũng được cấu thành từ n vector hàng giống hệt nhau trong R^d. Lưu ý rằng FFN(X) và các chuẩn hóa hoạt động giống hệt nhau trên mỗi vector hàng, chúng ta có thể thu được rằng đầu ra cuối cùng của một khối Transformer với attention dựa trên RPE vẫn được cấu thành từ n vector hàng giống hệt nhau trong R^d.

Vì g là một tổ hợp của nhiều khối Transformer trong T_blocks_RPE(H; d_H; r) và 0 được cấu thành từ n vector hàng giống hệt nhau trong R^d, chúng tôi kết luận từ phân tích trên rằng g(0) cũng được cấu thành từ n vector hàng giống hệt nhau trong R^d, tức là, tồn tại c ∈ R^d sao cho g(0) = c1_n^T. Do đó, bằng cách áp dụng Bất đẳng thức Cauchy-Schwarz chúng ta thu được

||\tilde{g}_M(0) - g(0)||_F^2 = (2M - c)^2 + (n-1)c^2 ≥ 4M^2(1/(1 + 1/(n-1))) > M^2

⟹ sup_{X∈D} ||\tilde{g}_M(X) - g(X)||_F > M,

điều này hoàn thành chứng minh.

**Thảo luận.** Quan sát chính trong Định lý 2 là A_{RPE}(X) luôn xuất ra một ma trận ngẫu nhiên phải. Ngay cả khi RPE mang thông tin vị trí phong phú, tín hiệu như vậy sẽ bị triệt tiêu để thỏa mãn A_{RPE}(X)1 = 1 cho X ∈ R^{n×d} tùy ý, trong đó 1 là một vector n chiều toàn một. Kết quả là, module attention không thể phản ánh đủ thông tin vị trí được mã hóa trong RPE ra đầu ra, điều này hạn chế khả năng mô hình. Vấn đề sẽ nghiêm trọng khi hàm mục tiêu \tilde{g} rất nhạy cảm với vị trí (trong trường hợp cực đoan, \tilde{g} chỉ phụ thuộc vào các chỉ số vị trí). Chúng tôi cũng thực hiện các thí nghiệm trên các nhiệm vụ chuỗi-đến-chuỗi đơn giản sử dụng dữ liệu tổng hợp để hỗ trợ tuyên bố toán học này trong Phần 5.1. Người ta có thể mong đợi rằng việc đơn giản loại bỏ mẫu số trong softmax có thể phá vỡ hạn chế. Tuy nhiên, sự sửa đổi này mang lại sự bất ổn tối ưu hóa đáng kể trong thực tế.

Cho rằng RPE có nhiều ưu điểm so với APE, việc thiết kế một biến thể Transformer dựa trên RPE là bộ xấp xỉ toàn cục của các hàm chuỗi-đến-chuỗi và dễ tối ưu hóa là hấp dẫn. Đây chính xác là những gì chúng tôi làm việc trong phần tiếp theo.

## 4 Làm cho các Transformer dựa trên RPE trở thành Bộ xấp xỉ Toàn cục

Phần này chứa hai tiểu phần. Trong tiểu phần đầu tiên, chúng tôi cung cấp một điều kiện đủ để các Transformer dựa trên RPE đạt được xấp xỉ toàn cục. Trong tiểu phần thứ hai, chúng tôi đưa ra một thể hiện thực tế của Transformer với RPE thỏa mãn yêu cầu và hiệu quả về tham số.

### 4.1 Một Điều kiện Đủ để Đạt được Xấp xỉ Toàn cục

Được thúc đẩy bởi công thức (1) và (4), chúng tôi xem xét một dạng tổng quát A^h_U : R^{n×d} → R^{n×n} và định nghĩa lớp attention tương ứng là

Attn_U(X) = X + ∑_{h=1}^H A^h_U(X)XW^h_VW^h_O              (6)

Chúng tôi tiếp tục định nghĩa lớp hàm của khối Transformer tương ứng là T_blocks_U(H; d_H; r), và định nghĩa lớp hàm của các Transformer được cấu thành từ T_blocks_U(H; d_H; r) xếp chồng là:

Ω^{H,d_H,r}_U = {g_L ∘ ... ∘ g_1 : R^{n×d} → R^{n×d} | g_1, ..., g_L ∈ T_blocks_U(H; d_H; r), L ∈ N}  (7)

Mục tiêu của chúng tôi là điều tra các yêu cầu về A^h_U dưới đó lớp hàm được cảm sinh Ω^{H,r}_U có thể trở thành các bộ xấp xỉ toàn cục của các hàm liên tục chuỗi-đến-chuỗi. Chúng tôi cung cấp một điều kiện đủ trong định lý sau. Theo Yun et al. và nhiều công trình lý thuyết trước đây khác, chúng tôi nghiên cứu tính biểu đạt của một phiên bản đơn giản hóa của Transformer trong đó các lớp chuẩn hóa được bỏ qua, vì được tin rằng chuẩn hóa chủ yếu giúp tối ưu hóa nhưng không làm tổn hại sức mạnh biểu đạt của mạng.

**Định lý 3.** Cho n, d ∈ N, p ∈ [1, +∞), ε > 0, một tập compact D ⊆ R^{n×d}, và một hàm liên tục chuỗi-đến-chuỗi f : D → R^{n×d}. Giả sử rằng A^h_U thỏa mãn các điều kiện sau:

• **Điều kiện chú ý.** Đối với bất kỳ u ∈ R^{d×1} và c ∈ R, tồn tại một tham số hóa của A^h_U, sao cho A^h_U(X) = softmax(Xu(Xu - c1)^T).

• **Điều kiện nhận biết vị trí.** Tồn tại một tham số hóa của A^h_U và một vector v ∈ R^n có các phần tử đều khác nhau, sao cho A^h_U(X)1 = v cho bất kỳ X ∈ R^{n×d}.

Khi đó tồn tại một mạng Transformer g ∈ Ω^{2,1,4}_U sao cho ∫_D ||f(X) - g(X)||_p^p dX)^{1/p} < ε, trong đó ||·||_p ký hiệu chuẩn ℓ_p theo từng phần tử cho ma trận.

Chứng minh chi tiết của Định lý 3 có thể được tìm thấy trong Phụ lục A. Định lý 3 trình bày hai điều kiện để làm cho các Transformer dựa trên RPE trở thành các bộ xấp xỉ toàn cục. Theo trực giác, điều kiện chú ý nêu rằng A^h_U nên chứa một trường hợp đặc biệt của ma trận attention gốc trong Eq.(1), trong đó W_Q = W_K ∈ R^{d_H×d} và d_H = 1. Điều kiện nhận biết vị trí nêu rằng A^h_U cần phá vỡ hạn chế của A(X) là một ma trận ngẫu nhiên phải (tức là, A(X)1 = 1 cho tất cả X ∈ R^{n×d}). Chúng tôi sẽ trình bày một ví dụ cụ thể thỏa mãn các điều kiện này trong tiểu phần tiếp theo.

### 4.2 Một Transformer dựa trên RPE Toàn cục

Chúng tôi phát triển một biến thể Transformer mới thỏa mãn các điều kiện trên. Đặc biệt, chúng tôi nhân ma trận attention softmax với một ma trận khác, và thu được

A_U(X) = softmax((XW_Q)(XW_K)^T + B) ⊙ C                    (8)

trong đó ⊙ ký hiệu tích theo từng phần tử. B có thể lấy bất kỳ dạng nào được mô tả trong Phần 2. Chúng tôi gọi biến thể attention này là Universal RPE-based Attention (URPE-based attention).

Để làm cho URPE-based attention dựa vào thông tin vị trí tương đối, chúng tôi đặt C ∈ R^{n×n} là một ma trận Toeplitz có thể học được trong đó mỗi phần tử trên đường chéo giảm dần từ trái sang phải có cùng giá trị. Lưu ý rằng một ma trận Toeplitz có hình dạng n×n có 2n-1 bậc tự do. Do đó, chúng tôi chỉ cần 2n-1 tham số mới cho mỗi C. Có thể chứng minh rằng URPE-based Attention thỏa mãn hai điều kiện trong Định lý 3 và chứng minh có thể được tìm thấy trong Phụ lục B.

**Mệnh đề 4.** URPE-based Attention được định nghĩa trong Eq.(8) thỏa mãn các điều kiện trong Định lý 3. Do đó, cho n, d ∈ N, các Transformer sử dụng dạng attention này là các bộ xấp xỉ toàn cục của các hàm liên tục chuỗi-đến-chuỗi ánh xạ một miền compact trong R^{n×d} đến R^{n×d}.

Để cải thiện thêm hiệu quả tham số, chúng tôi đặt C được chia sẻ trên các lớp khác nhau nhưng duy nhất cho mỗi attention head. Như một ví dụ, trong Transformer-XL (xem Phần 5.2), chúng tôi chỉ giới thiệu khoảng 4K tham số mới, là không đáng kể so với 151M tham số của mô hình Transformer-XL nhưng vẫn dẫn đến những cải thiện không tầm thường.

Chúng tôi có hai thảo luận khác cho URPE-based attention được thiết kế của chúng tôi. Thảo luận đầu tiên là về việc áp dụng URPE-based attention trong thiết lập nhân quả. Attention nhân quả rất quan trọng trong các nhiệm vụ tạo ngôn ngữ. URPE-based Attention tương thích với nó vì người ta có thể đặt phần tử thứ (i,j) của C bằng 0 cho i > j. Thảo luận thứ hai là về việc khởi tạo C. Trong thực tế, ma trận C có thể được khởi tạo như một ma trận toàn một. Do đó, mô hình hoạt động giống hệt với mô hình dựa trên RPE gốc khi khởi tạo, và các tham số bổ sung trong C được học dần dần. Chúng tôi cũng có thể tinh chỉnh bất kỳ Transformer dựa trên RPE được huấn luyện tốt nào thành đối tác dựa trên URPE của nó bằng cách đặt C như một ma trận toàn một và tiếp tục tinh chỉnh mô hình để học C.

## 5 Thí nghiệm

Trong phần này, chúng tôi nghiên cứu thực nghiệm hiệu quả của mô hình được đề xuất. Đặc biệt, chúng tôi nhằm trả lời các câu hỏi sau thông qua các thí nghiệm:

• **Câu hỏi 1**: Liệu các kết quả lý thuyết về khả năng xấp xỉ của Transformer dựa trên RPE và Transformer dựa trên URPE có thể được phản ánh trong các thí nghiệm nhất định không?

• **Câu hỏi 2**: Với các phương pháp RPE khác nhau (ma trận B trong Eq.(8)), liệu Transformer dựa trên URPE có thể vượt trội hơn đối tác dựa trên RPE của nó trong các ứng dụng thực tế không?

• **Câu hỏi 3**: Liệu URPE-based Attention có thể đóng vai trò như một module linh hoạt để cải thiện các Transformer tổng quát ngoài các nhiệm vụ ngôn ngữ không?

Chúng tôi sẽ trả lời từng câu hỏi với các thí nghiệm được thiết kế cẩn thận trong các tiểu phần sau. Do hạn chế về không gian, chúng tôi chỉ trình bày kết quả trên các kiến trúc mô hình đại diện, loại nhiệm vụ, và phương thức dữ liệu trong phần chính của bài báo. Thêm kết quả được trình bày trong phụ lục. Tất cả mã được thực hiện dựa trên các codebase chính thức của Fairseq và Graphormer trong PyTorch.

### 5.1 Nhiệm vụ Tổng hợp

Để xác minh thực nghiệm các kết quả lý thuyết của chúng tôi về khả năng xấp xỉ của Transformer dựa trên RPE và Transformer dựa trên URPE, chúng tôi thiết kế hai nhiệm vụ tổng hợp: 1) Nhận dạng Vị trí; 2) Dự đoán Token Chẵn.

Cả nhiệm vụ Nhận dạng Vị trí (PI) và nhiệm vụ Dự đoán Token Chẵn (ETP) đều là các nhiệm vụ dự đoán chuỗi-đến-chuỗi. Cho một chuỗi token s = (w₁, w₂, ..., wₙ), nhiệm vụ PI là dự đoán chỉ số vị trí của mỗi token trong chuỗi, tức là, hàm chuỗi-đến-chuỗi mục tiêu f để xấp xỉ có thể được định nghĩa là

f_PI(w₁, w₂, ..., wₙ) = (1, 2, ..., n)                      (9)

Nhiệm vụ ETP được định nghĩa như sau: đối với nửa đầu các vị trí trong một chuỗi, nhiệm vụ yêu cầu mô hình xuất ra các token đầu vào tại các vị trí có chỉ số chẵn; đối với nửa còn lại các vị trí, nhiệm vụ yêu cầu mô hình xuất ra token đặc biệt Kết thúc Câu (EOS), tức là,

f_ETP(w₁, w₂, ..., wₙ) = (w₂, w₄, ..., wₙ, EOS, ..., EOS)    (10)

Cả hai nhiệm vụ đều yêu cầu mô hình mã hóa chính xác thông tin vị trí, điều này sẽ khó khăn cho các Transformer dựa trên RPE để nắm bắt. Đối với cả hai nhiệm vụ, chúng tôi sử dụng các tập dữ liệu tổng hợp với các chuỗi được tạo ngẫu nhiên. Chi tiết, chúng tôi thay đổi kích thước từ vựng token từ [10, 1000, 10000] và đặt độ dài chuỗi là 128. Chúng tôi chọn Transformer vanilla làm mô hình cơ sở và so sánh các cách sau để mã hóa thông tin vị trí: 1) không có mã hóa vị trí (noPE); 2) mã hóa vị trí tương đối kiểu T5 (RPE); 3) URPE với Transformer backbone RPE kiểu T5. Số lượng lớp và số lượng attention head được đặt lần lượt là 3 và 12. Chiều ẩn được đặt là 768.

**Kết quả.** Chúng tôi sử dụng độ chính xác cấp token làm chỉ số đánh giá. Kết quả thí nghiệm được thể hiện trong Hình 1. Từ hình, có thể dễ dàng thấy rằng Transformer không có PE và Transformer với RPE kiểu T5 không thể giải quyết hoàn hảo các nhiệm vụ tổng hợp (ít hơn 60% độ chính xác). Ngược lại, Transformer dựa trên URPE đạt được 100% độ chính xác trên cả hai nhiệm vụ. Đầu tiên, kết quả này rõ ràng chỉ ra rằng mô hình được đề xuất của chúng tôi vượt trội hơn Transformer dựa trên RPE kiểu T5 backbone với một khoảng cách lớn. Hơn nữa, chúng ta có thể thấy rằng ngay cả đối với các nhiệm vụ đơn giản như vậy, Transformer với RPE kiểu T5 đôi khi thất bại, trong khi Transformer dựa trên URPE thành công và xấp xỉ hàm mục tiêu tốt, điều này phù hợp với các phát hiện lý thuyết của chúng tôi. Cuối cùng, chúng tôi cung cấp các hình ảnh hóa của Universal RPE đã học (Eq.(8)) trên cả hai nhiệm vụ trong Hình 2, cho thấy rằng ma trận B và C nắm bắt các khía cạnh khác nhau của thông tin vị trí.

### 5.2 Mô hình hóa Ngôn ngữ

Chúng tôi sử dụng mô hình hóa ngôn ngữ để nghiên cứu hiệu quả của URPE-based Attention được đề xuất. Mô hình hóa ngôn ngữ là một ứng dụng thực tế quan trọng thường yêu cầu mô hình hóa sự phụ thuộc dài hạn giữa các token. Chúng tôi thực hiện các thí nghiệm trên tập dữ liệu WikiText-103, chứa 103M token huấn luyện từ 28K bài báo, với độ dài trung bình 3.6K token mỗi bài báo. Các phương pháp mã hóa vị trí tương đối được sử dụng phổ biến trong mô hình hóa ngôn ngữ. Chúng tôi chọn mô hình Transformer-XL làm mô hình backbone của Transformer dựa trên URPE của chúng tôi. Theo, số lượng lớp và số lượng attention head được đặt lần lượt là 16 và 10. Chiều của các lớp ẩn và lớp feed-forward được đặt là 410 và 2100. Các mô tả chi tiết về các baseline và cài đặt huấn luyện được trình bày trong phụ lục.

**Kết quả.** Chúng tôi cho thấy điểm perplexity trên cả tập validation và test của các mô hình khác nhau trong Bảng 1. Có thể dễ dàng thấy rằng Transformer-XL được trang bị URPE-based attention của chúng tôi đạt được điểm perplexity validation và test lần lượt là 22.4 và 23.2, thấp hơn 0.7 và 0.8 so với mô hình Transformer-XL backbone và cũng tốt hơn đáng kể so với các baseline khác. Đầu tiên, kết quả cho thấy rằng URPE-based attention được đề xuất của chúng tôi có thể được áp dụng tốt cho Transformer-XL trong các ứng dụng thực tế. Cùng với các quan sát trong Phần 5.1, chúng tôi tin rằng URPE của chúng tôi có thể được sử dụng trong các kiến trúc dựa trên RPE khác, như. Đáng chú ý là mô hình của chúng tôi có nhiều tham số không đáng kể (khoảng 4k) so với Transformer-XL backbone. Do đó, sự cải thiện trong perplexity phải được quy cho phần lớn vào tính biểu đạt mạnh hơn của mô hình.

### 5.3 Học Đồ thị

Chúng tôi tiếp tục kiểm tra liệu URPE-based Attention được đề xuất có thể đóng vai trò như một module linh hoạt để cải thiện các Transformer dựa trên RPE tổng quát ngoài các nhiệm vụ ngôn ngữ không. Các mô hình dựa trên Transformer đã trở nên ngày càng phổ biến trong lĩnh vực học đồ thị. Trong số các mô hình đó, Graphormer được đề xuất gần đây đạt được hiệu suất tối tân trong nhiều nhiệm vụ học đồ thị. Trong Graph Transformer, RPE được sử dụng thay vì APE vì RPE chỉ tính toán khoảng cách giữa các nút, điều này tự nhiên bảo tồn nhiều tính chất bất biến và đẳng biến.

Tính toán attention trong Graphormer cũng tuân theo Eq.(4) trong Phần 2. Cụ thể, Graphormer tính toán khoảng cách đường đi ngắn nhất giữa bất kỳ cặp nút nào và mã hóa thông tin này như một số hạng bias trong softmax attention để phản ánh vị trí tương đối của bất kỳ nút nào trong đồ thị. Chúng tôi giới thiệu người đọc đến để biết mô tả chi tiết về Graphormer. Tương tự như các thí nghiệm trước đây, chúng tôi điều chỉnh URPE-based Attention cho Graphormer và so sánh chúng trên hai tập dữ liệu benchmark bao gồm các nhiệm vụ học biểu diễn đồ thị từ tập dữ liệu quy mô nhỏ đến lớn: ZINC từ Benchmarking-GNNs và PCQM4M từ Open Graph Benchmark Large Scale Challenge (OGB-LSC). Đối với cả hai nhiệm vụ, chúng tôi chọn một số mô hình Transformer cạnh tranh và GNN làm baseline của chúng tôi. Chi tiết về cài đặt thí nghiệm được trình bày trong phụ lục.

**ZINC.** ZINC là một tập dữ liệu thực tế bao gồm 250K đồ thị phân tử. Nhiệm vụ là dự đoán độ hòa tan bị ràng buộc của một phân tử, đây là một tính chất hóa học quan trọng để phát hiện thuốc. Chúng tôi huấn luyện các mô hình của chúng tôi trên cả ZINC-Full và ZINC-Subset (12K đồ thị được chọn theo). Để chứng minh sức mạnh của phương pháp của chúng tôi và để so sánh công bằng, chúng tôi đặt ngân sách tham số của mô hình nhỏ hơn 500K theo. Chúng tôi xây dựng trên mô hình Graphormer bao gồm 12 lớp. Chiều của các lớp ẩn và lớp feed-forward được đặt là 80. Số lượng attention head được đặt là 32.

**PCQM4M.** PCQM4M là một nhiệm vụ hồi quy hóa học lượng tử trong OGB-LSC. Tập dữ liệu PCQM4M chứa hơn 3.8 triệu đồ thị phân tử tổng cộng, hiện là tập dữ liệu dự đoán cấp đồ thị lớn nhất. Kiến trúc tối tân cho nhiệm vụ này là mô hình Graphormer được giới thiệu ở trên. Chúng tôi vẫn theo để đặt các siêu tham số trong mô hình Graphromer và trang bị nó với URPE. Chi tiết, Graphormer với URPE-based Attention của chúng tôi bao gồm 6 lớp và 32 attention head. Chiều của các lớp ẩn và lớp feed-forward được đặt là 512.

**Kết quả.** Kết quả thí nghiệm trên ZINC và PCQM4M được thể hiện trong Bảng 2 và 3, trong đó điểm số được tính trung bình trên bốn thí nghiệm với các seed khác nhau. Có thể dễ dàng thấy rằng mô hình Graphormer được trang bị URPE-based Attention của chúng tôi liên tục vượt trội hơn mô hình Graphormer backbone trên cả nhiệm vụ ZINC và PCQM4M. Đặc biệt, URPE-based Attention của chúng tôi cho phép mô hình Graphormer giảm hơn 40% sai số tuyệt đối trung bình tương đối trên tập test của ZINC-Subset và ZINC-Full. Trên nhiệm vụ PCQM4M, sự cải thiện là khoảng 0.003 sai số tuyệt đối trung bình, cũng là một cải thiện đáng kể dưới độ chính xác hóa học lượng tử. Đáng chú ý là mô hình Graphormer với URPE-based Attention của chúng tôi đạt được hiệu suất cạnh tranh so với mô hình Graphormer Base với 48.3M tham số được báo cáo trong. Do đó, chúng tôi tin rằng kiến trúc được đề xuất của chúng tôi cải thiện đáng kể sức mạnh biểu đạt của các backbone Transformer và có thể được mở rộng tốt cho các tình huống thực tế ngoài các nhiệm vụ ngôn ngữ.

### 5.4 Thêm Phân tích

**URPE-based Attention liên tục cải thiện hiệu suất của các mô hình có kích thước khác nhau.** Chúng tôi thực hiện các thí nghiệm ablation trên nhiệm vụ Mô hình hóa Ngôn ngữ và thay đổi số lượng lớp trong [4, 8, 16] để điều tra các kích thước mô hình khác nhau. Theo cài đặt thí nghiệm trong Phần 5.2, số lượng attention head được đặt là 10. Chiều ẩn được đặt là 41. Chiều của lớp feed-forward được đặt là 2100. Kết quả được trình bày trong Bảng 4. Có thể dễ dàng thấy rằng URPE-based Attention của chúng tôi liên tục giảm điểm perplexity của các mô hình Transformer-XL có kích thước khác nhau, điều này thực sự chứng minh tính linh hoạt của URPE-based Attention của chúng tôi.

**Đánh giá Thời gian chạy và Sử dụng Bộ nhớ.** Chúng tôi tiếp tục thực hiện các thí nghiệm profiling chi phí bộ nhớ và thời gian trên các Transformer dựa trên URPE của chúng tôi. Chúng tôi chọn Transformer vanilla làm mô hình backbone. Số lượng lớp và chiều ẩn được đặt lần lượt là 12 và 768. Số lượng attention head được đặt là 12. Kích thước batch được đặt là 32. Chúng tôi thay đổi độ dài chuỗi từ [128, 256, 512]. Chúng tôi chạy profiling của tất cả các mô hình trên GPU NVIDIA Tesla V100 16GB. Theo Combiner, chúng tôi so sánh tốc độ suy luận và chi phí bộ nhớ của Transformer vanilla với RPE và URPE của chúng tôi. Kết quả được trình bày trong Bảng 5, cho thấy rằng URPE của chúng tôi chỉ tăng chi phí tính toán nhỏ.

**Tóm tắt.** Trong phần này, chúng tôi thiết kế một loạt thí nghiệm để trả lời các câu hỏi về hiệu quả và khả năng áp dụng của URPE-based Attention được đề xuất. Tất cả kết quả thí nghiệm cho thấy rằng các phát hiện lý thuyết của chúng tôi có sức thuyết phục, và các Transformer với sự sửa đổi của chúng tôi là hiệu quả, mạnh mẽ, và có thể áp dụng rộng rãi trên các nhiệm vụ khác nhau.

## 6 Công trình Liên quan

**Sức mạnh biểu đạt của Mạng Neural.** Định lượng khả năng của các mạng neural là một hướng nghiên cứu quan trọng trong văn học về học sâu. Đã chỉ ra rằng một mạng neural với một lớp ẩn và chiều rộng không giới hạn có thể xấp xỉ các hàm liên tục tùy ý trên hỗ trợ compact với sai số nhỏ tùy ý. Nhiều công trình cũng nghiên cứu hiệu quả chiều rộng trên sức mạnh biểu đạt của các mạng neural và chứng minh rằng các mạng ReLU với chiều rộng bị giới hạn nhưng độ sâu không giới hạn có thể đạt được xấp xỉ hàm toàn cục. Gần đây, đã có sự quan tâm ngày càng tăng trong việc hiểu lý thuyết về các mô hình Transformer. Yun et al. đã chứng minh lý thuyết rằng các Transformer có thể xấp xỉ bất kỳ hàm liên tục chuỗi-đến-chuỗi nào (tức là, xấp xỉ toàn cục) trên một miền compact bằng cách chứng minh rằng việc xếp chồng các lớp self-attention có thể tính toán các ánh xạ ngữ cảnh của các embedding đầu vào. Dong et al. đã phân tích các hạn chế của các Transformer chỉ có attention mà không xem xét các khối FFN, chuẩn hóa, và kết nối bỏ qua. Hron et al. đã phân tích hành vi của multi-head attention và kết nối nó với quy trình Gaussian khi số lượng head có xu hướng vô cực. Trong, được chứng minh rằng một lớp multi-head attention với đủ head ít nhất cũng biểu đạt như bất kỳ lớp convolution nào. Tất cả các công trình trên xem xét các Transformer với mã hóa vị trí tuyệt đối, đây là sự khác biệt chính giữa chúng và công trình của chúng tôi.

**Các phương pháp mã hóa vị trí trong Transformer.** Trong, Transformer vanilla mã hóa thông tin vị trí thông qua mã hóa vị trí tuyệt đối (APE). Shaw et al. là người đầu tiên giới thiệu mã hóa vị trí tương đối (RPE) cho Transformer. Từ đó, nhiều công trình đã khám phá các chiến lược RPE khác nhau dựa trên. Transformer-XL tái tham số hóa self-attention để tích hợp mã hóa vị trí tương đối và cho phép mô hình hóa chuỗi dài. T5 đơn giản hóa các biểu diễn vector của các vị trí tương đối thành scalar. Kitaev et al. tách rời thông tin vị trí và nội dung trong bộ mã hóa Transformer, dẫn đến một trình phân tích cú pháp thành phần được cải thiện. Ke et al. tiếp tục chỉ ra rằng sự tách rời như vậy cũng cải thiện Transformer trong việc tiền huấn luyện ngôn ngữ tổng quát và đạt được hiệu suất vượt trội trên các nhiệm vụ downstream khác nhau. Cũng có các công trình mã hóa thông tin vị trí thông qua các công cụ khác như cây, số phức, hệ thống động, đặc trưng Fourier. So với hầu hết các công trình trước đây được truyền cảm hứng từ các tình huống thực tế, URPE-based attention của chúng tôi được thúc đẩy lý thuyết bằng cách điều tra sức mạnh biểu đạt của các Transformer dựa trên RPE theo cách có nguyên tắc.

## 7 Kết luận

Trong bài báo này, chúng tôi đầu tiên điều tra khía cạnh lý thuyết của các Transformer dựa trên RPE. Đặc biệt, chúng tôi nghiên cứu sức mạnh biểu đạt của chúng và cung cấp một phát hiện lý thuyết đáng ngạc nhiên cho thấy rằng các Transformer với RPE được sử dụng rộng rãi không phải là các bộ xấp xỉ hàm toàn cục. Để thiết kế một Transformer dựa trên RPE mạnh mẽ hơn, chúng tôi trình bày các điều kiện đủ trên module attention để đạt được xấp xỉ hàm toàn cục và phát triển một Transformer dựa trên RPE Toàn cục mới sử dụng một phương pháp mã hóa vị trí tương đối mới. Chúng tôi thực hiện các thí nghiệm được thiết kế cẩn thận trên dữ liệu chuỗi tổng hợp, ngôn ngữ tự nhiên, và đồ thị để chỉ ra rằng mô hình của chúng tôi mang lại những cải thiện hiệu suất nhất quán so với các Transformer dựa trên RPE hiện có. Trong tương lai, chúng tôi sẽ đánh giá Universal RPE của chúng tôi trên các Transformer dựa trên RPE khác và các nhiệm vụ điển hình để xác minh thêm tính linh hoạt của nó như một module cơ bản cho các mô hình dựa trên Transformer.
