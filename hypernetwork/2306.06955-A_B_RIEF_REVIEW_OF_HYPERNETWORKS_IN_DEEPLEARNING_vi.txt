# TỔNG QUAN NGẮN GỌN VỀ HYPERNETWORKS TRONG DEEP LEARNING

Vinod Kumar Chauhan1∗, Jiandong Zhou1, Ping Lu1, Soheila Molaei1 và David A. Clifton1,2

1Viện Kỹ thuật Y sinh, Đại học Oxford, OX3 7DQ, Vương quốc Anh
2Viện Nghiên cứu Tiên tiến Oxford-Suzhou (OSCAR), Suzhou, Trung Quốc

16 tháng 7, 2024

TÓM TẮT

Hypernetworks, hay hypernets tóm tắt, là mạng nơ-ron tạo ra trọng số cho một mạng nơ-ron khác, được gọi là mạng đích. Chúng đã nổi lên như một kỹ thuật deep learning mạnh mẽ cho phép tính linh hoạt, khả năng thích ứng, tính động, huấn luyện nhanh hơn, chia sẻ thông tin và nén mô hình lớn hơn. Hypernets đã cho thấy kết quả đầy hứa hẹn trong nhiều bài toán deep learning khác nhau, bao gồm continual learning, suy luận nhân quả, transfer learning, cắt tỉa trọng số, định lượng độ không chắc chắn, zero-shot learning, xử lý ngôn ngữ tự nhiên và reinforcement learning. Mặc dù thành công trên nhiều bối cảnh bài toán khác nhau, hiện tại vẫn chưa có tổng quan toàn diện nào để thông báo cho các nhà nghiên cứu về những phát triển mới nhất và hỗ trợ trong việc sử dụng hypernets. Để lấp đầy khoảng trống này, chúng tôi tổng quan tiến triển trong hypernets. Chúng tôi trình bày một ví dụ minh họa về huấn luyện mạng nơ-ron sâu sử dụng hypernets và đề xuất phân loại hypernets dựa trên năm tiêu chí thiết kế: đầu vào, đầu ra, tính biến thiên của đầu vào và đầu ra, và kiến trúc của hypernets. Chúng tôi cũng tổng quan các ứng dụng của hypernets trên các bối cảnh bài toán deep learning khác nhau, tiếp theo là thảo luận về các kịch bản chung mà hypernets có thể được sử dụng hiệu quả. Cuối cùng, chúng tôi thảo luận về các thách thức và hướng tương lai vẫn chưa được khám phá trong lĩnh vực hypernets. Chúng tôi tin rằng hypernetworks có tiềm năng cách mạng hóa lĩnh vực deep learning. Chúng cung cấp một cách mới để thiết kế và huấn luyện mạng nơ-ron, và chúng có tiềm năng cải thiện hiệu suất của các mô hình deep learning trên nhiều nhiệm vụ khác nhau. Thông qua tổng quan này, chúng tôi mong muốn truyền cảm hứng cho những tiến bộ xa hơn trong deep learning thông qua hypernetworks.

Từ khóa: Hypernetworks · Deep learning · Mạng nơ-ron · Tạo tham số · Tạo trọng số

1 Giới thiệu

Deep learning đã cách mạng hóa lĩnh vực trí tuệ nhân tạo bằng cách tạo ra những tiến bộ đáng kể trong nhiều lĩnh vực khác nhau, bao gồm computer vision [12], xử lý ngôn ngữ tự nhiên [18], suy luận nhân quả [11], và reinforcement learning [35]. Các mạng nơ-ron sâu (DNN) tiêu chuẩn đã được chứng minh là các công cụ mạnh mẽ để học các biểu diễn phức tạp từ dữ liệu. Tuy nhiên, mặc dù thành công, các DNN tiêu chuẩn vẫn bị hạn chế trong một số điều kiện nhất định. Ví dụ, một khi DNN được huấn luyện, trọng số cũng như kiến trúc của nó được cố định [55,73], và bất kỳ thay đổi nào về trọng số hoặc kiến trúc đều yêu cầu huấn luyện lại DNN. Sự thiếu khả năng thích ứng và tính động này hạn chế tính linh hoạt của DNN, khiến chúng kém phù hợp cho các kịch bản đòi hỏi điều chỉnh động hoặc khả năng thích ứng dữ liệu [24,8]. DNN thường có một số lượng lớn trọng số và cần lượng dữ liệu đáng kể để tối ưu hóa các trọng số đó [3]. Điều này có thể gây khó khăn trong các tình huống không có sẵn lượng dữ liệu lớn. Ví dụ, trong y tế, việc thu thập đủ dữ liệu cho các bệnh hiếm có thể đặc biệt khó khăn do số lượng bệnh nhân có sẵn mỗi năm hạn chế [76]. Cuối cùng, định lượng độ không chắc chắn trong dự đoán của DNN là thiết yếu vì nó cung cấp thước đo độ tin cậy, cho phép ra quyết định tốt hơn trong các ứng dụng có cổ phần cao [13]. Các kỹ thuật định lượng độ không chắc chắn hiện có có những hạn chế, chẳng hạn như nhu cầu huấn luyện nhiều mô hình [1], và định lượng độ không chắc chắn vẫn được coi là một vấn đề mở [32]. Tương tự, thích ứng miền, tổng quát hóa miền, phòng thủ đối kháng, chuyển đổi phong cách nơ-ron, và tìm kiếm kiến trúc nơ-ron là những vấn đề quan trọng vẫn chưa được giải quyết, nơi hypernets có thể cung cấp các giải pháp hiệu quả như đã thảo luận trong Phần 4.

Hypernetworks (hay hypernets tóm tắt) đã nổi lên như một mô hình kiến trúc đầy hứa hẹn để tăng cường tính linh hoạt (thông qua khả năng thích ứng dữ liệu và kiến trúc động) và hiệu suất của DNN. Hypernets là một lớp mạng nơ-ron tạo ra trọng số/tham số của một mạng nơ-ron khác được gọi là mạng đích/chính/chủ, nơi cả hai mạng được huấn luyện theo cách có thể vi phân từ đầu đến cuối [24]. Hypernets bổ sung cho các DNN hiện có và cung cấp một khung làm việc mới để huấn luyện DNN, dẫn đến một lớp DNN mới được gọi là HyperDNN (vui lòng tham khảo Phần 2 để biết chi tiết). Các đặc điểm chính và ưu điểm của hypernets cung cấp ứng dụng trên các bối cảnh vấn đề khác nhau được thảo luận dưới đây.

(a) Chia sẻ trọng số mềm: Hypernetworks có thể được huấn luyện để tạo ra trọng số của nhiều DNN để giải quyết các nhiệm vụ liên quan [14,49]. Điều này được gọi là chia sẻ trọng số mềm vì, không giống như chia sẻ trọng số cứng liên quan đến các lớp được chia sẻ giữa các nhiệm vụ (ví dụ, trong đa nhiệm vụ), các DNN khác nhau được tạo ra bởi một hypernet chung thông qua điều kiện nhiệm vụ. Điều này giúp chia sẻ thông tin giữa các nhiệm vụ và có thể được sử dụng cho transfer learning hoặc chia sẻ thông tin động [14].

(b) Kiến trúc động: Hypernetworks có thể được sử dụng để tạo ra trọng số của một mạng với kiến trúc động, nơi số lượng lớp hoặc cấu trúc của mạng thay đổi trong quá trình huấn luyện hoặc suy luận. Điều này có thể đặc biệt hữu ích cho các nhiệm vụ mà cấu trúc mạng đích không được biết tại thời điểm huấn luyện [24].

(c) DNN thích ứng dữ liệu: Không giống như DNN tiêu chuẩn có trọng số được cố định tại thời điểm suy luận, HyperDNN có thể tạo ra một mạng đích được tùy chỉnh theo nhu cầu của dữ liệu. Trong những trường hợp như vậy, hypernets được điều kiện hóa trên dữ liệu đầu vào để thích ứng với dữ liệu [69].

(d) Định lượng độ không chắc chắn: Hypernets có thể huấn luyện hiệu quả các DNN nhận thức về độ không chắc chắn bằng cách tận dụng các kỹ thuật như lấy mẫu nhiều đầu vào từ phân bố nhiễu [33] hoặc kết hợp dropout trong chính hypernets [15]. Bằng cách tạo ra nhiều bộ trọng số cho mạng chính, hypernets tạo ra một tập hợp các mô hình, mỗi mô hình có cấu hình tham số khác nhau. Cách tiếp cận dựa trên tập hợp này giúp ước tính độ không chắc chắn trong dự đoán mô hình, một khía cạnh quan trọng cho các ứng dụng quan trọng về an toàn như y tế, nơi việc có thước đo độ tin cậy trong dự đoán là thiết yếu.

(e) Hiệu quả tham số: HyperDNN, tức là DNN được huấn luyện với hypernets, có thể có ít trọng số hơn DNN tiêu chuẩn tương ứng, dẫn đến nén trọng số [81]. Điều này có thể đặc biệt hữu ích khi làm việc với tài nguyên hạn chế, dữ liệu hạn chế, hoặc dữ liệu có chiều cao và có thể dẫn đến huấn luyện nhanh hơn DNN tương ứng [45].

Ha et. al [24] đã đặt thuật ngữ hypernets (cũng được gọi là meta-networks hoặc meta-models) và huấn luyện mạng đích và hypernet theo cách có thể vi phân từ đầu đến cuối. Tuy nhiên, khái niệm về trọng số phụ thuộc ngữ cảnh có thể học được đã được thảo luận từ trước, chẳng hạn như fast weights trong [59,60] và HyperNEAT [68]. Thảo luận của chúng tôi về hypernets tập trung vào các mạng nơ-ron tạo ra trọng số cho mạng nơ-ron đích do tính phổ biến, tính biểu cảm và tính linh hoạt của chúng [73,12]. Gần đây, hypernets đã nhận được sự chú ý đáng kể và đã tạo ra kết quả tiên tiến (SOTA) trên nhiều bài toán deep learning, bao gồm ensemble learning [32], đa nhiệm vụ [71], tìm kiếm kiến trúc nơ-ron [80], continual learning [49], cắt tỉa trọng số [40], mạng nơ-ron Bayesian [17], mô hình tạo sinh [17], tối ưu hóa siêu tham số [41], chia sẻ thông tin [14], phòng thủ đối kháng [69], và reinforcement learning (RL) [54] (vui lòng tham khảo Phần 4 để biết thêm chi tiết).

Mặc dù thành công của hypernets trên các bối cảnh vấn đề khác nhau, theo hiểu biết của chúng tôi, không có tổng quan nào về hypernets để hướng dẫn các nhà nghiên cứu về các phát triển và giúp sử dụng hypernets. Để lấp đầy khoảng trống này, chúng tôi cung cấp một tổng quan ngắn gọn về hypernets trong deep learning. Chúng tôi minh họa hypernets bằng một ví dụ và phân biệt HyperDNN với DNN (Phần 2). Để tạo điều kiện hiểu biết và tổ chức tốt hơn, chúng tôi đề xuất phân loại có hệ thống các hypernets dựa trên năm tiêu chí thiết kế riêng biệt, dẫn đến các phân loại khác nhau xem xét các yếu tố như (i) đặc điểm đầu vào, (ii) đặc điểm đầu ra, (iii) tính biến thiên của đầu vào, (iv) tính biến thiên của đầu ra, và (v) kiến trúc của hypernets (Phần 3). Hơn nữa, chúng tôi cung cấp tổng quan toàn diện về các ứng dụng đa dạng của hypernets trong deep learning, bao trùm nhiều bối cảnh vấn đề khác nhau (Phần 4). Bằng cách xem xét các ứng dụng thực tế, chúng tôi mong muốn chứng minh các lợi ích thực tế và tác động tiềm năng của hypernetworks. Ngoài ra, chúng tôi thảo luận về một số kịch bản và đặt ra câu hỏi trực tiếp để hiểu liệu chúng ta có thể áp dụng hypernets cho một vấn đề nhất định hay không (Phần 5). Cuối cùng, chúng tôi thảo luận về các thách thức và hướng tương lai của nghiên cứu hypernet (Phần 6). Điều này bao gồm việc giải quyết các mối quan tâm về khởi tạo, tính ổn định và độ phức tạp, cũng như khám phá các hướng để tăng cường hiểu biết lý thuyết và định lượng độ không chắc chắn của DNN. Bằng cách cung cấp một tổng quan toàn diện về hypernetworks, bài báo này mong muốn phục vụ như một tài nguyên có giá trị cho các nhà nghiên cứu và thực hành trong lĩnh vực này. Thông qua tổng quan này, chúng tôi hy vọng truyền cảm hứng cho những tiến bộ xa hơn trong deep learning bằng cách tận dụng tiềm năng của hypernets để phát triển các mô hình linh hoạt hơn, hiệu suất cao hơn.

Đóng góp: Bài báo tổng quan này đưa ra những đóng góp chính sau:

• Theo hiểu biết của chúng tôi, chúng tôi trình bày tổng quan đầu tiên về hypernetworks trong deep learning, đã cho thấy kết quả ấn tượng trên nhiều bài toán deep learning.

• Chúng tôi đề xuất phân loại hypernets dựa trên năm tiêu chí thiết kế, dẫn đến các phân loại khác nhau của hypernets, chẳng hạn như dựa trên đầu vào, đầu ra, tính biến thiên của đầu vào và đầu ra, và kiến trúc của hypernets.

• Chúng tôi trình bày tổng quan toàn diện về các ứng dụng của hypernetworks trên các bối cảnh vấn đề khác nhau, chẳng hạn như định lượng độ không chắc chắn, continual learning, suy luận nhân quả, transfer learning, và federated learning, và tóm tắt tổng quan của chúng tôi, theo phân loại của chúng tôi, trong một bảng (Bảng 2).

• Chúng tôi khám phá các kịch bản rộng cho các ứng dụng hypernet, dựa trên các trường hợp sử dụng hiện có và đặc điểm hypernet. Cuộc khám phá này nhằm trang bị cho các nhà nghiên cứu những hiểu biết có thể hành động về khi nào nên tận dụng hypernets trong bối cảnh vấn đề của họ.

• Cuối cùng, chúng tôi xác định các thách thức và hướng tương lai của nghiên cứu hypernetwork, bao gồm các mối quan tâm về khởi tạo, tính ổn định, khả năng mở rộng và hiệu quả, và nhu cầu về hiểu biết lý thuyết và khả năng diễn giải của hypernetworks. Bằng cách nêu bật những lĩnh vực này, chúng tôi mong muốn truyền cảm hứng cho những tiến bộ xa hơn trong hypernetworks và cung cấp hướng dẫn cho các nhà nghiên cứu quan tâm đến việc giải quyết những thách thức này.

Phần còn lại của bài báo được tổ chức như sau: Phần 2 cung cấp nền tảng toàn diện về hypernets, trong khi Phần 3 giới thiệu một lược đồ phân loại mới cho hypernets. Các ứng dụng đa dạng của hypernets trên nhiều vấn đề khác nhau được thảo luận trong Phần 4, tiếp theo là khám phá các kịch bản cụ thể nơi hypernets có thể được sử dụng hiệu quả trong Phần 5. Giải quyết các thách thức và vạch ra hướng nghiên cứu tương lai là trọng tâm của Phần 6, và cuối cùng, các nhận xét kết luận được thảo luận trong Phần 7.

2 Nền tảng

Trong phần này, chúng tôi thảo luận và phân biệt hoạt động của mạng nơ-ron sâu tiêu chuẩn (DNN) và DNN được huấn luyện với hypernetworks, được gọi là HyperDNN, sử dụng một ví dụ chung. Hình 1 minh họa sự khác biệt về cấu trúc và luồng gradient trong DNN và HyperDNN. Cả hai đều giải quyết cùng một vấn đề sử dụng cùng một kiến trúc DNN tại thời điểm suy luận. Tuy nhiên, sự khác biệt tồn tại trong quy trình huấn luyện của chúng, cụ thể là trong luồng gradient và tối ưu hóa trọng số, làm cho hypernets trở thành một cách thay thế để huấn luyện DNN.

Hãy biểu thị một bộ dữ liệu bằng X, Y để giải quyết một nhiệm vụ chung T, trong đó X là ma trận các đặc trưng và Y là vector các nhãn, và x∈X biểu thị một điểm dữ liệu và y∈Y là nhãn tương ứng. Hãy biểu thị một DNN như một hàm F(X; Θ), trong đó X biểu thị đầu vào và Θ biểu thị trọng số của DNN. Trong quá trình chuyển tiếp, đầu vào x∈X đi qua các lớp của F để tạo ra dự đoán ŷ∈Ŷ, sau đó được sử dụng cùng với nhãn thực y∈Y để tính toán hàm mục tiêu đo lường sự khác biệt giữa giá trị thực và giá trị được dự đoán bởi mô hình sử dụng hàm mất mát L(Y,Ŷ). Trong quá trình lan truyền ngược, DNN thường sử dụng backpropagation để lan truyền lỗi ngược qua các lớp và tính toán gradient của L đối với Θ. Các thuật toán tối ưu hóa, chẳng hạn như Adam [30], sử dụng những gradient này để cập nhật trọng số. Khi kết thúc quá trình huấn luyện, chúng ta nhận được trọng số tối ưu hóa Θ được sử dụng tại thời điểm suy luận trong DNN F(X; Θ) để đưa ra dự đoán với dữ liệu thử nghiệm cho việc giải quyết nhiệm vụ T. Do đó, trong DNN tiêu chuẩn, Θ là các trọng số có thể học được.

Hypernets cung cấp một cách thay thế để học trọng số Θ của DNN F(X; Θ) để giải quyết nhiệm vụ T, trong đó Θ không được học trực tiếp mà được tạo ra bởi một mạng nơ-ron khác. Trong khung này, chúng ta giải quyết cùng một nhiệm vụ sử dụng cùng một kiến trúc DNN nhưng với một cách tiếp cận huấn luyện khác nhau. Hãy biểu thị một hypernet như H(C; Φ) tạo ra trọng số cụ thể cho nhiệm vụ của DNN F(X; Θ), trong đó C là vector ngữ cảnh cụ thể cho nhiệm vụ đóng vai trò là đầu vào cho H và Φ là trọng số của hypernet H. Tức là, Θ = H(C; Φ) trong đó Φ là các trọng số có thể học duy nhất trong toàn bộ kiến trúc. Vector ngữ cảnh C có thể được tạo ra từ dữ liệu [2], được lấy mẫu từ phân bố nhiễu [33], hoặc tương ứng với danh tính/nhúng nhiệm vụ [4]. Trong quá trình chuyển tiếp, một vector ngữ cảnh cụ thể cho nhiệm vụ C được truyền đến hypernet H tạo ra trọng số Θ cho DNN F. Sau đó, giống như một DNN tiêu chuẩn, một đầu vào x∈X được truyền qua DNN F để dự đoán đầu ra Y, và mất mát được tính toán là L(Y,Ŷ). Tuy nhiên, trong quá trình lan truyền ngược, lỗi được lan truyền ngược qua hypernet H và gradient của L được tính toán đối với trọng số của hypernet Φ. Thuật toán học tối ưu hóa Φ để tạo ra Θ sao cho hiệu suất trên nhiệm vụ đích T được tối ưu hóa. Tại thời điểm thử nghiệm, Θ được tạo ra từ hypernet H tối ưu hóa được sử dụng trong DNN F(X; Θ) để đưa ra dự đoán với dữ liệu thử nghiệm cho việc giải quyết nhiệm vụ T. Các bài toán tối ưu hóa cho DNN tiêu chuẩn và HyperDNN có thể được viết như sau (bỏ qua các thuật ngữ chính quy hóa để đơn giản):

DNN: min_Θ F(X; Θ), HyperDNN: min_Φ F(X; Θ) = F(X;H(C; Φ)). (1)

Do đó, DNN học trọng số của chúng trực tiếp từ dữ liệu, trong khi ở HyperDNN, trọng số của hypernet được học, và trọng số của DNN được tạo ra bởi hypernet. Để có một ví dụ cụ thể về so sánh kiến trúc DNN và HyperDNN và hoạt động của chúng, vui lòng tham khảo công trình của chúng tôi trong suy luận nhân quả [14].

Như đã thảo luận trong Phần 1, huấn luyện DNN với hypernet, tức là HyperDNN, mang lại một số lợi ích so với việc huấn luyện trực tiếp DNN. Tuy nhiên, những lợi ích này đặc thù cho ứng dụng và không thể được tổng quát hóa cho tất cả các nhiệm vụ hoặc ứng dụng. Ví dụ, một tính năng chính của hypernets là chia sẻ trọng số mềm, cho phép chia sẻ thông tin giữa các thành phần liên quan. Việc chia sẻ thông tin này đặc biệt có giá trị trong các cài đặt có dữ liệu hạn chế, dẫn đến cải thiện hiệu suất cho HyperDNN trong các kịch bản như vậy. Nói chung, HyperDNN có lợi cho các ứng dụng có dữ liệu hạn chế, các vấn đề yêu cầu mạng thích ứng dữ liệu, kiến trúc mạng động, hiệu quả tham số, và định lượng độ không chắc chắn. Một thảo luận chi tiết về các kịch bản mà HyperDNN có thể hữu ích được cung cấp trong Phần 5.

Nói chung, nếu một nhiệm vụ có thể được giải quyết bằng DNN tiêu chuẩn, nên sử dụng chúng thay vì hypernets. Như được mô tả trong Hình 1, HyperDNN yêu cầu một DNN bổ sung để giải quyết cùng một nhiệm vụ. Mặc dù có những lợi ích được cung cấp bởi hypernets, DNN bổ sung này tạo ra độ phức tạp trong việc huấn luyện và triển khai HyperDNN. Ví dụ, việc khởi tạo HyperDNN khó khăn hơn DNN vì trọng số của mạng đích được tạo ra tại lớp đầu ra của hypernet. Các kỹ thuật khởi tạo cổ điển không đảm bảo rằng trọng số của mạng đích được khởi tạo trong cùng một phạm vi. Tuy nhiên, các bộ tối ưu hóa thích ứng, chẳng hạn như Adam [30], có thể giảm thiểu vấn đề này ở một mức độ nào đó. Một thách thức đáng kể khác với HyperDNN là khả năng mở rộng của chúng. Vì trọng số của mạng đích được tạo ra tại lớp đầu ra của hypernet, cách tiếp cận này có thể tạo ra khó khăn khi xử lý các mạng đích lớn. Các vấn đề về khả năng mở rộng có thể được quản lý bằng cách sử dụng các chiến lược tạo trọng số khác nhau. Do đó, khi sử dụng HyperDNN, các thực hành viên nên cân nhắc sử dụng các bộ tối ưu hóa thích ứng, triển khai các chiến lược tạo trọng số khác nhau, và sử dụng các phương pháp để ổn định huấn luyện, chẳng hạn như spectral norms. Để có thảo luận chi tiết về các thách thức liên quan đến HyperDNN, vui lòng tham khảo Phần 6.

3 Phân loại Hypernetworks

Trong phần này, chúng tôi đề xuất phân loại hypernetworks dựa trên năm tiêu chí thiết kế, như được mô tả trong Hình 2 và được đưa ra dưới đây:

(a) Dựa trên đầu vào, tức là loại đầu vào nào được hypernetworks sử dụng để tạo ra trọng số mạng nơ-ron đích?

(b) Dựa trên đầu ra, tức là đầu ra, đó là trọng số đích, được tạo ra như thế nào?

(c) Tính biến thiên của đầu vào, tức là đầu vào của hypernet có cố định không?

(d) Tính biến thiên của đầu ra, tức là mạng đích có số lượng trọng số cố định không? và

(e) Dựa trên kiến trúc, tức là loại kiến trúc nào hypernet sử dụng để tạo ra trọng số đích?

Chúng tôi thảo luận về những điều này trong các phần con sau. Có thể phân loại hypernets dựa trên kiến trúc của mạng đích nhưng điều đó không được xem xét vì hypernets chủ yếu tạo ra trọng số đích độc lập với kiến trúc của chúng.

3.1 Hypernetworks dựa trên đầu vào

Hypernetworks lấy một vector ngữ cảnh làm đầu vào và tạo ra trọng số của DNN đích làm đầu ra. Tùy thuộc vào vector ngữ cảnh nào được sử dụng, chúng ta có thể có các loại hypernetworks sau.

Hypernetworks điều kiện nhiệm vụ: Những hypernetworks này lấy thông tin cụ thể về nhiệm vụ làm đầu vào. Thông tin nhiệm vụ có thể ở dạng danh tính/nhúng nhiệm vụ, siêu tham số, kiến trúc, hoặc bất kỳ gợi ý cụ thể nào về nhiệm vụ. Hypernetwork tạo ra trọng số được điều chỉnh cho nhiệm vụ cụ thể. Điều này cho phép hypernet thích ứng hành vi của nó phù hợp và cho phép chia sẻ thông tin, thông qua chia sẻ trọng số mềm của hypernets, giữa các nhiệm vụ, dẫn đến hiệu suất tốt hơn trên các nhiệm vụ. Ví dụ, Chauhan et al. [14] đã áp dụng hypernets cho vấn đề ước tính hiệu ứng điều trị trong suy luận nhân quả sử dụng danh tính hoặc nhúng của các hàm kết quả tiềm năng (PO) để tạo ra trọng số tương ứng với hàm PO. Hypernetworks đã cho phép chia sẻ thông tin liên tục giữa các nhóm điều trị từ đầu đến cuối và giúp tính toán ước tính điều trị đáng tin cậy trong các nghiên cứu quan sát với bộ dữ liệu có kích thước hạn chế. Tương tự, hypernets điều kiện nhiệm vụ đã được sử dụng để giải quyết các vấn đề khác, bao gồm đa nhiệm vụ [45], xử lý ngôn ngữ tự nhiên (NLP) [24], và continual learning [49].

Hypernetworks điều kiện dữ liệu: Những hypernetworks này được điều kiện hóa trên dữ liệu mà mạng đích đang được huấn luyện. Hypernetwork tạo ra trọng số dựa trên đặc điểm của dữ liệu đầu vào. Điều này cho phép mạng nơ-ron điều chỉnh hành vi của nó một cách động dựa trên mẫu hoặc đặc trưng đầu vào cụ thể, dẫn đến các mô hình linh hoạt và thích ứng hơn, và dẫn đến khả năng tổng quát hóa tốt hơn cho dữ liệu chưa thấy. Ví dụ, Alaluf et al. [2] áp dụng hypernets cho chỉnh sửa hình ảnh nơi đầu vào của hypernet dựa trên hình ảnh đầu vào và xấp xỉ ban đầu của tái tạo để tạo ra điều chế cho trọng số của bộ tạo được huấn luyện trước. Tương tự, hypernets điều kiện dữ liệu đã được sử dụng để giải quyết các vấn đề khác, chẳng hạn như phòng thủ đối kháng [69], học đồ thị tri thức [5] và học hình dạng [39].

Hypernetworks điều kiện nhiễu: Những hypernetworks này không được điều kiện hóa trên bất kỳ dữ liệu đầu vào hoặc gợi ý nhiệm vụ nào, mà thay vào đó trên nhiễu được lấy mẫu ngẫu nhiên. Điều này làm cho chúng có tính tổng quát hơn và giúp định lượng độ không chắc chắn dự đoán cho DNN, nhưng nó cũng có nghĩa là chúng có thể không hoạt động tốt bằng hypernetworks điều kiện nhiệm vụ hoặc điều kiện dữ liệu trên nhiều nhiệm vụ hoặc bộ dữ liệu. Ví dụ, Krueger et al. [33] áp dụng hypernetworks để xấp xỉ suy luận Bayesian trong DNN và đánh giá phương pháp cho active learning, độ không chắc chắn mô hình, chính quy hóa, và phát hiện bất thường. Tương tự, hypernets điều kiện nhiễu đã được sử dụng để giải quyết các vấn đề khác, chẳng hạn như học đa tạp [17] và định lượng độ không chắc chắn [53].

Những loại điều kiện khác nhau này cho phép hypernetworks tăng cường tính linh hoạt (thông qua khả năng thích ứng và kiến trúc động), và hiệu suất của các mô hình deep learning trong các ngữ cảnh khác nhau. Loại hypernetwork cụ thể được sử dụng sẽ phụ thuộc vào nhiệm vụ hoặc ứng dụng cụ thể. Ví dụ, hypernets điều kiện nhiệm vụ phù hợp cho việc chia sẻ thông tin giữa nhiều nhiệm vụ, hypernets điều kiện dữ liệu phù hợp để xử lý các điều kiện mà DNN cần thích ứng với dữ liệu đầu vào, và hypernets điều kiện nhiễu phù hợp cho định lượng độ không chắc chắn trong dự đoán.

3.2 Hypernetworks dựa trên đầu ra

Dựa trên đầu ra của hypernets, tức là chiến lược tạo trọng số, chúng tôi phân loại hypernetworks theo việc liệu tất cả trọng số được tạo ra cùng nhau hay không. Phân loại hypernetworks này quan trọng vì nó kiểm soát khả năng mở rộng và độ phức tạp của hypernetworks, vì thường DNN có một số lượng lớn trọng số, và việc tạo ra tất cả chúng cùng nhau có thể làm cho kích thước lớp cuối cùng của hypernets trở nên lớn. Vì vậy, có những cách để quản lý độ phức tạp của hypernets dẫn đến các chiến lược tạo trọng số khác nhau, như được thảo luận dưới đây. Có thể huấn luyện HyperDNN với ít trọng số hơn DNN đích – điều này được gọi là nén trọng số [81]. Chúng tôi đã so sánh và tóm tắt các đặc điểm của các chiến lược tạo trọng số khác nhau trong Bảng 1. Cột đầu tiên đại diện cho đặc điểm được xem xét để so sánh, trong khi ba cột sau tương ứng với ba chiến lược tạo trọng số khác nhau. Các giá trị trong mỗi hàng cho biết liệu một chiến lược tạo trọng số cụ thể có cung cấp tính năng đã chỉ định hay không.

Tạo một lần: Những hypernetworks này tạo ra trọng số của toàn bộ DNN đích cùng nhau. Cách tiếp cận này sử dụng tất cả trọng số được tạo ra, và trọng số của mỗi lớp được tạo ra cùng nhau, không giống như các chiến lược tạo trọng số khác. Tuy nhiên, cách tiếp cận tạo trọng số này không phù hợp cho các mạng đích lớn vì có thể dẫn đến hypernets phức tạp. Ví dụ, Shamsian et al. [63], Galanti and Wolf [22], Zhang et al. [80] đã sử dụng tạo trọng số một lần.

Tạo nhiều lần: Những hypernetworks này có nhiều đầu để tạo ra trọng số (đôi khi được gọi là hypernets tách/đa đầu) và cách tiếp cận tạo trọng số này có thể bổ sung cho các cách tiếp cận khác. Điều này đơn giản hóa độ phức tạp và giảm số lượng trọng số yêu cầu trong lớp cuối cùng của hypernets theo số lần đầu. Cách tiếp cận này không cần nhúng bổ sung, và nói chung, sử dụng tất cả trọng số được tạo ra, không giống như tạo trọng số theo thành phần và theo khối nơi một số trọng số vẫn không được sử dụng. Ví dụ, Beck et al. [6], Rezaei-Shoshtari et al. [54], Chauhan et al. [14] đã sử dụng chiến lược tạo nhiều lần để tạo ra trọng số đích.

Tạo theo khối: Hypernetworks theo khối tạo ra trọng số của mạng đích theo từng khối. Điều này có thể dẫn đến việc không sử dụng một số trọng số được tạo ra vì trọng số được tạo ra theo kích thước khối, có thể không khớp với kích thước lớp. Nếu kích thước khối nhỏ hơn kích thước lớp, thì có thể không tạo ra được tất cả trọng số của một lớp cùng nhau. Hơn nữa, những hypernets này cần nhúng bổ sung để phân biệt các khối khác nhau và tạo ra trọng số cụ thể cho các khối. Tuy nhiên, nhìn chung, tạo trọng số theo khối dẫn đến giảm độ phức tạp và cải thiện khả năng mở rộng của hypernets. Ví dụ, Chauhan et al. [14], Oswald et al. [49] đã sử dụng tạo trọng số theo khối.

Tạo theo thành phần: Chiến lược tạo trọng số theo thành phần tạo ra trọng số cho từng thành phần riêng lẻ (chẳng hạn như lớp hoặc kênh) của mô hình đích một cách riêng biệt. Điều này hữu ích trong việc tạo ra trọng số cụ thể vì các lớp hoặc kênh khác nhau đại diện cho các đặc trưng hoặc mẫu khác nhau trong mạng. Tuy nhiên, tương tự như cách tiếp cận theo khối, hypernets theo thành phần cần một nhúng cho mỗi thành phần để phân biệt giữa các thành phần khác nhau và tạo ra trọng số cụ thể cho thành phần đó. Chúng cũng giúp giảm độ phức tạp và cải thiện khả năng mở rộng của hypernets. Vì trọng số được tạo ra theo kích thước của lớp lớn nhất nên cách tiếp cận tạo trọng số này có thể dẫn đến việc không sử dụng một số trọng số trong các lớp nhỏ hơn. Chiến lược này có thể được coi là một trường hợp đặc biệt của cách tiếp cận tạo trọng số theo khối, nơi một khối bằng kích thước của một thành phần. Ví dụ, Zhao et al. [81], Alaluf et al. [2], Mahabadi et al. [43] đã sử dụng tạo trọng số theo thành phần.

Bằng cách phân loại hypernetworks dựa trên chiến lược tạo trọng số của chúng, chúng ta có thể đưa ra những lựa chọn sáng suốt có thể giúp kiểm soát khả năng mở rộng và độ phức tạp của hypernetworks một cách hiệu quả. Mỗi loại chiến lược tạo trọng số đều cung cấp những lợi ích và cân nhắc độc đáo dựa trên các đặc điểm và yêu cầu cụ thể của nhiệm vụ. Nghiên cứu so sánh các đặc điểm của các cách tiếp cận tạo trọng số khác nhau được tóm tắt trong Bảng 1.

3.3 Tính biến thiên của đầu vào

Chúng ta có thể phân loại hypernets dựa trên tính biến thiên của đầu vào. Chúng ta có hai lớp, đầu vào tĩnh và đầu vào động, như được thảo luận dưới đây.

Đầu vào tĩnh: Nếu đầu vào được định nghĩa trước và cố định thì hypernet được gọi là tĩnh đối với đầu vào. Ví dụ, đa nhiệm vụ [43] có số lượng nhiệm vụ cố định dẫn đến số lượng đầu vào cố định. Cần lưu ý rằng ở đây đầu vào cố định chỉ có nghĩa là danh tính nhiệm vụ cố định, tuy nhiên hypernets có thể học nhúng cho các nhiệm vụ khác nhau.

Đầu vào động: Nếu đầu vào thay đổi và thường phụ thuộc vào dữ liệu mà mạng đích được huấn luyện, thì hypernet được gọi là động đối với đầu vào. Đầu vào động giúp hypernetworks giới thiệu một mức độ thích ứng mới bằng cách tạo ra trọng số của mạng đích một cách động. Việc tạo trọng số động này cho phép hypernetworks phản ứng với ngữ cảnh phụ thuộc đầu vào và điều chỉnh hành vi của chúng phù hợp. Bằng cách tạo ra trọng số mạng dựa trên đầu vào cụ thể, hypernetworks có thể nắm bắt các mẫu và phụ thuộc phức tạp có thể thay đổi giữa các trường hợp dữ liệu khác nhau. Khả năng thích ứng này dẫn đến hiệu suất mô hình được nâng cao, đặc biệt trong các kịch bản có phân bố dữ liệu phức tạp và phát triển [75]. Do đó, hypernets dựa trên đầu vào động giúp thích ứng miền [75], ước tính mật độ [28] và học đồ thị tri thức [5] v.v.

Điều này có thể được coi là siêu phân loại trên hypernets dựa trên đầu vào nơi hypernets điều kiện nhiệm vụ rơi vào danh mục đầu vào tĩnh trong khi hypernets nhiễu ngẫu nhiên và điều kiện dữ liệu rơi vào danh mục động. Cả hai danh mục đều có những lợi ích riêng vì đầu vào tĩnh giúp chia sẻ thông tin [14], transfer learning [49], và phù hợp nơi chúng ta có nhiều nhiệm vụ để giải quyết [63]. Mặt khác, đầu vào động mang lại cho hypernets khả năng thích ứng với các điều kiện mới chưa biết trong quá trình huấn luyện [5].

3.4 Tính biến thiên của đầu ra

Khi phân loại hypernetworks dựa trên bản chất của trọng số mạng đích, chúng ta có thể phân loại chúng thành hai loại, đầu ra tĩnh hoặc đầu ra động, như được thảo luận dưới đây.

Đầu ra tĩnh: Nếu trọng số của mạng đích có kích thước cố định, thì hypernet được gọi là tĩnh đối với đầu ra. Trong trường hợp này, mạng đích cũng tĩnh. Ví dụ, Pan et al. [50], Szatkowski et al. [70] tạo ra trọng số tĩnh.

Đầu ra động: Nếu trọng số của mạng đích không cố định, tức là kiến trúc thay đổi về kích thước, thì hypernet được gọi là động đối với đầu ra, và mạng đích cũng là mạng động vì nó có thể có kiến trúc khác nhau tùy thuộc vào đầu vào của hypernet. Trọng số động có thể được tạo ra, chủ yếu, trong hai tình huống, đầu tiên khi kiến trúc hypernet là động, ví dụ, Ha et al. [24] đã sử dụng mạng nơ-ron hồi quy (RNN) để đề xuất HyperRNN dựa trên trọng số không được chia sẻ. Thứ hai, trọng số động có thể được tạo ra khi đầu vào là động, tức là hypernet thích ứng theo dữ liệu đầu vào, ví dụ, Littwin and Wolf [39] đã áp dụng hypernet dựa trên mạng nơ-ron tích chập (CNN) để tạo ra trọng số động cho việc học hình dạng từ hình ảnh của một hình dạng. Tương tự, Peng et al. [51], Li et al. [36] cũng tạo ra trọng số động.

3.5 Tính động trong Hypernetworks

Đây là siêu phân loại của Phần con 3.3 và 3.4 thành danh mục rộng hơn dựa trên tính động trong đầu vào hoặc đầu ra của hypernets, như được thảo luận dưới đây.

Hypernets tĩnh: Nếu đầu vào của hypernet cố định, tức là được định nghĩa trước và số lượng trọng số được tạo ra bởi hypernet cho mạng đích là cố định, tức là kiến trúc cố định, thì hypernet được gọi là hypernet tĩnh. Loại hypernets này làm việc với đầu vào được định nghĩa trước, ví dụ, danh tính nhiệm vụ, có thể được học như nhúng, nhưng các nhiệm vụ được giải quyết vẫn giữ nguyên. Ví dụ, ước tính hiệu ứng điều trị không đồng nhất [14] nơi số lượng nhóm điều trị hoặc hàm kết quả tiềm năng được cố định, và kiến trúc của mạng đích (trong trường hợp này là hàm kết quả tiềm năng) cũng được cố định.

Hypernets động: Nếu đầu vào của hypernet dựa trên đầu vào của mạng đích, tức là dữ liệu đầu vào, hoặc số lượng trọng số được tạo ra bởi hypernet cho mạng đích là biến thiên, tức là kiến trúc là động, thì hypernet được gọi là hypernet động. Ví dụ, Sendera et al. [61] đã áp dụng hypernet điều kiện dữ liệu cho few-shot learning bằng cách kết hợp kernels và hypernets. Kernels được sử dụng để trích xuất thông tin hỗ trợ từ dữ liệu của các nhiệm vụ khác nhau đóng vai trò là đầu vào cho hypernet tạo ra trọng số cho nhiệm vụ đích. Zhang et al. [80] đã áp dụng hypernetworks cho tìm kiếm kiến trúc nơ-ron nơi họ mô hình hóa kiến trúc nơ-ron của DNN như đồ thị và sử dụng chúng làm đầu vào cho hypernet để tạo ra trọng số mạng đích. Vì vậy, mạng đích có kiến trúc biến thiên, và là hypernet động dựa trên đầu ra động.

3.6 Kiến trúc của Hypernetworks

Trong phân loại hypernetworks dựa trên kiến trúc của chúng, chúng ta có thể phân loại chúng thành bốn loại chính: perceptron đa lớp (MLP), mạng nơ-ron tích chập (CNN), mạng nơ-ron hồi quy (RNN), và mạng dựa trên attention, như được đưa ra dưới đây.

MLPs: Hypernetworks dựa trên MLP sử dụng kiến trúc dày đặc và kết nối đầy đủ, cho phép mỗi nơ-ron đầu vào kết nối với mỗi nơ-ron đầu ra. Kiến trúc này cho phép một quy trình tạo trọng số toàn diện bằng cách xem xét toàn bộ thông tin đầu vào, ví dụ, [14].

CNNs: Hypernetworks CNN, mặt khác, tận dụng các lớp tích chập để nắm bắt các mẫu cục bộ và thông tin không gian. Những hypernetworks này xuất sắc trong các nhiệm vụ liên quan đến dữ liệu không gian, chẳng hạn như phân tích hình ảnh hoặc video, bằng cách trích xuất các đặc trưng từ đầu vào và tạo ra trọng số hoặc tham số tương ứng, ví dụ, Nirkin et al. [47] đã sử dụng MLP để triển khai hypernets.

RNNs: Hypernetworks RNN kết hợp các kết nối hồi quy trong kiến trúc của chúng, tạo điều kiện cho các vòng lặp phản hồi và xử lý thông tin tuần tự. Chúng tạo ra trọng số hoặc tham số một cách động dựa trên các trạng thái hoặc đầu vào trước đó, làm cho chúng phù hợp cho các nhiệm vụ liên quan đến dữ liệu tuần tự, chẳng hạn như xử lý ngôn ngữ tự nhiên hoặc phân tích chuỗi thời gian, ví dụ, Ha et al. [24] đã sử dụng RNN để triển khai hypernets.

Attention: Hypernetworks dựa trên attention kết hợp các cơ chế attention [73] vào kiến trúc của chúng. Bằng cách tập trung có chọn lọc vào các đặc trưng đầu vào liên quan, những hypernetworks này tạo ra trọng số cho mạng đích, cho phép chúng nắm bắt các phụ thuộc tầm xa và cải thiện chất lượng của đầu ra được tạo ra, ví dụ, Volk et al. [75] đã sử dụng attention để triển khai hypernets.

Mỗi loại kiến trúc có những điểm mạnh và khả năng áp dụng riêng, cho phép hypernetworks thích ứng và tạo ra trọng số theo cách phù hợp với các đặc điểm và yêu cầu cụ thể của mạng đích và dữ liệu được xử lý.

4 Ứng dụng của Hypernetworks

Hypernetworks đã chứng minh hiệu quả và tính linh hoạt của chúng trên một loạt các lĩnh vực và nhiệm vụ trong deep learning. Trong phần này, chúng tôi thảo luận về một số ứng dụng quan trọng của hypernetworks và làm nổi bật những đóng góp của chúng trong việc thúc đẩy SOTA trong các lĩnh vực này. Chúng tôi tóm tắt các ứng dụng của hypernets theo phân loại được đề xuất của chúng tôi và cũng cung cấp liên kết đến các kho lưu trữ mã nguồn để mang lại lợi ích cho các nhà nghiên cứu, bất cứ nơi nào có sẵn, trong Bảng 2.

Continual Learning: Continual learning, còn được gọi là lifelong learning hoặc incremental learning, là một mô hình machine learning tập trung vào khả năng của một mô hình học và thích ứng liên tục theo thời gian, theo cách tuần tự, mà không quên kiến thức đã học trước đó. Không giống như batch learning truyền thống, giả định các tập huấn luyện và thử nghiệm tĩnh và độc lập, continual learning xử lý phân bố dữ liệu động và không ổn định, nơi dữ liệu mới đến một cách tăng dần, và mô hình cần thích ứng với những thay đổi này trong khi giữ lại kiến thức đã có được trước đó. Thách thức trong continual learning nằm ở việc giảm thiểu catastrophic forgetting, đề cập đến xu hướng của một mô hình quên thông tin đã học trước đó khi được huấn luyện trên dữ liệu mới. Để giải quyết điều này, các chiến lược khác nhau đã được đề xuất, bao gồm kỹ thuật chính quy hóa, phương pháp diễn tập, kiến trúc động, và cô lập tham số. Oswald et al. [49] đã mô hình hóa mỗi bộ dữ liệu được thu thập tăng dần như một nhiệm vụ và áp dụng hypernets điều kiện nhiệm vụ cho continual learning – điều này giúp chia sẻ thông tin giữa các nhiệm vụ. Để giải quyết vấn đề catastrophic forgetting, họ đề xuất một bộ chính quy hóa để diễn tập lại các hiện thực trọng số cụ thể cho nhiệm vụ thay vì dữ liệu từ các nhiệm vụ trước đó. Họ đạt được kết quả SOTA trên các benchmark và chứng minh thực nghiệm rằng hypernets điều kiện nhiệm vụ có khả năng dài hạn để giữ lại ký ức của các nhiệm vụ trước đó. Tương tự, Huang et al. [29], Ehret et al. [20] đã áp dụng hypernets điều kiện nhiệm vụ cho continual learning trong reinforcement learning (RL).

Federated Learning: Federated Learning là một cách tiếp cận phi tập trung hóa cho machine learning nơi quy trình huấn luyện được phân tán trên nhiều thiết bị hoặc thiết bị biên, mà không cần tập trung hóa dữ liệu trong một vị trí duy nhất. Trong mô hình này, mỗi thiết bị hoặc nút biên huấn luyện cục bộ một mô hình sử dụng dữ liệu riêng của nó, và chỉ các cập nhật mô hình, thay vì dữ liệu thô, được chia sẻ và tổng hợp trên một máy chủ trung tâm. Điều này cho phép học hợp tác trong khi bảo vệ quyền riêng tư và bảo mật dữ liệu. Nó cũng giảm chi phí giao tiếp và độ trễ, làm cho nó phù hợp cho các kịch bản với băng thông hạn chế hoặc kết nối gián đoạn. Shamsian et al. [63] đã mô hình hóa mỗi máy khách như một nhiệm vụ và áp dụng hypernets điều kiện nhiệm vụ cho vấn đề federated learning. Họ đã huấn luyện một hypernet trung tâm để tạo ra trọng số cho các mô hình khách hàng. Điều này cho phép chia sẻ thông tin giữa các khách hàng khác nhau trong khi làm cho kích thước hypernet độc lập với chi phí giao tiếp, vì trọng số hypernet không bao giờ được truyền. Federated learning dựa trên hypernet đã đạt được kết quả SOTA và cũng cho thấy khả năng tổng quát hóa tốt hơn cho các khách hàng mới có phân bố khác với các khách hàng hiện có. Litany et al. [37] đã mở rộng công việc này cho các khách hàng không đồng nhất, tức là khách hàng với các kiến trúc nơ-ron khác nhau, sử dụng graph hypernetworks [80].

Few-shot Learning: Few-shot learning là một lĩnh vực con của machine learning tập trung vào việc huấn luyện mô hình để học các khái niệm hoặc nhiệm vụ mới chỉ với số lượng hạn chế các ví dụ huấn luyện. Không giống như các cách tiếp cận machine learning truyền thống thường yêu cầu lượng lớn dữ liệu được gắn nhãn cho mỗi nhiệm vụ, few-shot learning nhằm tổng quát hóa kiến thức từ một tập hỗ trợ nhỏ các ví dụ được gắn nhãn để phân loại hoặc nhận dạng các trường hợp mới. Để giải quyết những khó khăn thực tế của các kỹ thuật hiện có trong việc hoạt động trong không gian tham số có chiều cao với cài đặt dữ liệu cực kỳ hạn chế, Rusu et al. [56] đã áp dụng hypernets điều kiện dữ liệu. Họ sử dụng hypernet dựa trên encoder-decoder học một biểu diễn tạo sinh phụ thuộc dữ liệu tiềm ẩn của các tham số mô hình chia sẻ thông tin giữa các nhiệm vụ khác nhau thông qua chia sẻ trọng số mềm của hypernets. Họ cũng đạt được kết quả SOTA và cho thấy rằng kỹ thuật được đề xuất có thể nắm bắt độ không chắc chắn trong dữ liệu. Sendera et al. [61] cũng áp dụng hypernet điều kiện dữ liệu cho few-shot learning bằng cách kết hợp kernels và hypernets. Kernels được sử dụng để trích xuất thông tin hỗ trợ từ dữ liệu của các nhiệm vụ khác nhau đóng vai trò là đầu vào cho hypernet tạo ra trọng số cho nhiệm vụ đích. Tương tự, Zhao et al. [81], Zięba [82], Sendera et al. [62] cũng áp dụng hypernets, và tận dụng chia sẻ trọng số mềm, cho few-shot learning.

Manifold Learning: Manifold learning là một lĩnh vực con của machine learning tập trung vào việc nắm bắt cấu trúc hoặc hình học cơ bản của dữ liệu có chiều cao trong các biểu diễn có chiều thấp hơn hoặc manifolds. Nó nhằm khám phá các mối quan hệ và mẫu nội tại trong dữ liệu bằng cách ánh xạ nó vào một không gian có chiều thấp hơn, cho phép trực quan hóa, phân cụm, hoặc phân loại tốt hơn. Hypernetworks có thể được sử dụng trong ngữ cảnh manifold learning để tăng cường quá trình học biểu diễn. Bằng cách tạo ra trọng số hoặc tham số cho mạng đích dựa trên đầu vào, hypernetworks có thể học thích ứng một manifold nắm bắt cấu trúc dữ liệu phức tạp [63]. Deutsch et al. [17] đã áp dụng hypernetworks điều kiện nhiễu để ánh xạ các vector tiềm ẩn để tạo ra trọng số mạng đích tổng quát hóa kết nối chế độ trong cảnh quan mất mát đến manifolds có chiều cao hơn.

AutoML: AutoML, viết tắt của Automated Machine Learning, đề cập đến việc phát triển các thuật toán, hệ thống và công cụ tự động hóa các khía cạnh khác nhau của pipeline machine learning, ví dụ, tìm kiếm kiến trúc nơ-ron (NAS) và tối ưu hóa siêu tham số tự động. Zhang et al. [80] đã áp dụng hypernetworks cho NAS nơi họ mô hình hóa kiến trúc nơ-ron của DNN như đồ thị và sử dụng chúng làm đầu vào cho hypernet để tạo ra trọng số mạng đích. Họ đạt được kết quả nhanh hơn khoảng 10 lần so với SOTA. Tương tự, Brock et al. [8], Peng et al. [51] trình bày một ví dụ khác về ứng dụng hypernets cho NAS, nơi họ khai thác tính chất chia sẻ trọng số mềm của hypernets cho việc chia sẻ thông tin giữa các kiến trúc khác nhau. Đối với tối ưu hóa siêu tham số, Lorraine and Duvenaud [41] đã áp dụng hypernets lấy siêu tham số của mạng đích làm đầu vào và tạo ra trọng số tối ưu cho mạng đích, và do đó thực hiện huấn luyện kết hợp cho các tham số mạng đích và siêu tham số nói cách khác được huấn luyện trong các vòng lặp tối ưu hóa lồng nhau. Tác giả đã chứng minh hiệu quả của kỹ thuật được đề xuất so với SOTA để huấn luyện hàng nghìn siêu tham số.

Pareto-front Learning: Pareto-front learning, còn được gọi là tối ưu hóa đa mục tiêu, là một kỹ thuật giải quyết các vấn đề với nhiều mục tiêu xung đột, ví dụ, đa nhiệm vụ có nhiều nhiệm vụ có thể có gradient xung đột. Nó nhằm tìm ra một tập hợp các giải pháp đại diện cho sự đánh đổi giữa các mục tiêu khác nhau, thay vì một giải pháp tối ưu duy nhất. Trong Pareto-front learning, mục tiêu là xác định một tập hợp các giải pháp không thể được cải thiện trong một mục tiêu mà không hy sinh hiệu suất trong mục tiêu khác. Những giải pháp này được gọi là Pareto-optimal hoặc non-dominated solutions và nằm trên Pareto-front, đại diện cho sự đánh đổi tốt nhất có thể giữa các mục tiêu. Navon et al. [45] đã áp dụng hypernets để học toàn bộ Pareto-front, trong đó tại thời điểm suy luận lấy một điểm ưu tiên trên Pareto-front và tạo ra trọng số Pareto-front cho mạng đích có vector mất mát theo hướng của tia. Họ cho thấy rằng hypernets được đề xuất có hiệu quả tính toán rất cao so với SOTA và có thể mở rộng đến các mô hình lớn, chẳng hạn như ResNet18. Công việc này được mở rộng thêm trong Hoang et al. [27], nơi hypernet tạo ra nhiều giải pháp, và Tran et al. [72], xem xét các hàm scalarization hoàn chỉnh trong Pareto-front learning.

Domain Adaptation: Domain adaptation đề cập đến quá trình thích ứng một mô hình machine learning được huấn luyện trên miền nguồn để hoạt động tốt trong miền đích khác. Đây là một thách thức quan trọng trong machine learning khi có sự thay đổi hoặc khác biệt giữa phân bố của dữ liệu nguồn và đích. Hypernets có thể đóng vai trò có giá trị trong domain adaptation bằng cách tạo ra hoặc thích ứng các tham số mô hình, kiến trúc, hoặc các thành phần khác một cách động để xử lý hiệu quả các thay đổi miền. Ví dụ, Volk et al. [75] là những người đầu tiên đề xuất hypernets cho domain adaptation. Họ sử dụng hypernets điều kiện dữ liệu nơi các ví dụ từ miền đích được sử dụng làm đầu vào cho hypernet tạo ra trọng số cho mạng đích. Điều này mang lại cho hypernets khả năng học và chia sẻ thông tin từ các miền hiện có với miền đích thông qua huấn luyện được chia sẻ.

Causal Inference: Causal inference là một lĩnh vực nghiên cứu tập trung vào việc hiểu và ước tính các mối quan hệ nhân quả giữa các biến. Nó nhằm khám phá các mối quan hệ nguyên nhân-kết quả trong một hệ thống bằng cách tận dụng dữ liệu quan sát hoặc thực nghiệm. Causal inference đặc biệt quan trọng khi suy luận tác động của điều trị/can thiệp/chính sách lên kết quả quan tâm. Gần đây, chúng tôi là những người đầu tiên áp dụng hypernets cho vấn đề ước tính hiệu ứng điều trị không đồng nhất (HTE) [14]. Chúng tôi đã áp dụng hypernets điều kiện nhiệm vụ nơi mỗi hàm kết quả tiềm năng (PO) được coi như một nhiệm vụ. Nhúng của các hàm PO được sử dụng làm đầu vào cho hypernet tạo ra tham số cho hàm PO tương ứng, tức là các mô hình thực tế và phản thực tế. Dựa trên chia sẻ trọng số mềm của hypernets, công việc này trình bày cơ chế chung đầu tiên để huấn luyện các học viên HTE cho phép chia sẻ thông tin liên điều trị từ đầu đến cuối giữa các hàm PO và giúp có được ước tính đáng tin cậy, đặc biệt với dữ liệu quan sát có kích thước hạn chế. Khung được đề xuất cũng kết hợp dropout trong hypernet cho phép tạo ra nhiều bộ tham số cho các hàm PO và giúp định lượng độ không chắc chắn.

Uncertainty Quantification: Uncertainty quantification là một khía cạnh quan trọng của deep learning và ra quyết định liên quan đến việc ước tính và hiểu độ không chắc chắn liên quan đến dự đoán hoặc kết quả mô hình. Nó cung cấp thước đo độ tin cậy hoặc độ đáng tin cậy trong dự đoán được thực hiện bởi một mô hình, đặc biệt trong các tình huống mà mô hình gặp phải dữ liệu chưa thấy hoặc không chắc chắn. Hypernets có thể huấn luyện hiệu quả các DNN nhận thức về độ không chắc chắn bằng cách tận dụng các kỹ thuật như lấy mẫu nhiều đầu vào từ phân bố nhiễu [33] hoặc kết hợp dropout trong chính hypernets [15]. Bằng cách tạo ra nhiều bộ trọng số cho mạng chính, hypernets tạo ra một tập hợp các mô hình, mỗi mô hình với cấu hình tham số khác nhau. Cách tiếp cận dựa trên tập hợp này giúp ước tính độ không chắc chắn trong dự đoán mô hình, một khía cạnh quan trọng cho các ứng dụng quan trọng về an toàn như y tế, nơi việc có thước đo độ tin cậy trong dự đoán là thiết yếu. Krueger et al. [33] đã đề xuất Bayesian hypernets lấy nhiễu ngẫu nhiên làm đầu vào để tạo ra phân bố trên trọng số của mạng đích và cho thấy hiệu suất cạnh tranh cho độ không chắc chắn. Ratzlaff and Fuxin [53] cũng áp dụng hypernets điều kiện nhiễu cho định lượng độ không chắc chắn và cho thấy rằng kỹ thuật được đề xuất cung cấp ước tính tốt hơn về độ không chắc chắn so với kỹ thuật ensemble learning. Ngoài ra, Chauhan et al. [15] đã sử dụng dropout trong hypernets điều kiện nhiệm vụ để tạo ra nhiều bộ trọng số cho mạng đích và do đó giúp ước tính độ không chắc chắn.

Adversarial Defence: Adversarial defence trong deep learning đề cập đến các kỹ thuật được sử dụng để tăng cường tính mạnh mẽ và khả năng phục hồi của mô hình chống lại các cuộc tấn công đối kháng. Các cuộc tấn công đối kháng liên quan đến việc tạo ra các nhiễu loạn được chế tác cẩn thận cho dữ liệu đầu vào nhằm lừa dối hoặc đánh lừa các mô hình deep learning [42]. Bằng cách kết hợp hypernetworks, các mô hình có thể tăng cường khả năng phát hiện và phòng thủ chống lại các cuộc tấn công đối kháng bằng cách tạo ra hoặc thích ứng trọng số hoặc kiến trúc của chúng một cách động. Ví dụ, Sun et al. [69] đã tạo ra các kernel tích chập thích ứng phụ thuộc dữ liệu để cải thiện tính mạnh mẽ của CNN chống lại các cuộc tấn công đối kháng và đã thành công trong việc phát hiện tự phát các cuộc tấn công được tạo ra bởi nhiễu Gaussian, phương pháp dấu hiệu gradient nhanh, và các phương pháp tấn công hộp đen. Các mô hình được phát triển với hypernets có tính thích ứng cao và được tùy chỉnh cho dữ liệu. Tương tự, Kristiadi et al. [32], Ratzlaff and Fuxin [53], Krueger et al. [33] cũng thấy rằng hypernets điều kiện nhiễu mạnh mẽ chống lại các ví dụ đối kháng so với SOTA.

Multitasking: Multitasking đề cập đến khả năng của một mô hình thực hiện nhiều nhiệm vụ hoặc học nhiều mục tiêu đồng thời. Nó liên quan đến việc tận dụng các biểu diễn và tham số được chia sẻ giữa các nhiệm vụ khác nhau để tăng cường hiệu quả học và hiệu suất tổng thể. Hypernets có thể được áp dụng trong ngữ cảnh multitasking để tạo điều kiện cho việc học kết hợp của nhiều nhiệm vụ bằng cách tạo ra hoặc thích ứng các tham số hoặc kiến trúc của mô hình một cách động. Cụ thể, chúng ta có thể huấn luyện hypernets điều kiện nhiệm vụ cho multitasking nơi nhúng của một nhiệm vụ đóng vai trò là đầu vào cho hypernet tạo ra trọng số cho nhiệm vụ tương ứng. Chúng ta có thể tạo ra toàn bộ mô hình cho mỗi nhiệm vụ hoặc chỉ có thể tạo ra các phần không được chia sẻ của mạng multitasking. Hypernets tạo điều kiện cho các mô hình như vậy chia sẻ thông tin giữa các nhiệm vụ khác nhau cũng như có mô hình cá nhân hóa cụ thể cho mỗi nhiệm vụ. Ví dụ, Mahabadi et al. [43] đã áp dụng hypernets điều kiện nhiệm vụ chia sẻ kiến thức giữa các nhiệm vụ cũng như tạo ra các mô hình cụ thể cho nhiệm vụ và đạt được kết quả benchmark. Navon et al. [45] cũng nghiên cứu hypernets điều kiện nhiệm vụ cho Pareto-front learning để giải quyết các gradient xung đột giữa các mục tiêu khác nhau và đạt được kết quả ấn tượng về multitasking, bao gồm công bằng và phân đoạn hình ảnh.

Reinforcement Learning: Reinforcement Learning (RL) tập trung vào việc huấn luyện các agent để đưa ra quyết định tuần tự trong một môi trường để tối đa hóa phần thưởng tích lũy. RL hoạt động thông qua một vòng lặp tương tác nơi agent thực hiện các hành động, nhận phản hồi dưới dạng phần thưởng, và học các chính sách tối ưu thông qua thử và sai. Hypernets có thể được sử dụng để tạo ra hoặc thích ứng kiến trúc mạng, tham số mô hình, hoặc chiến lược khám phá trong các agent RL một cách động. Bằng cách sử dụng hypernetwork, agent RL có thể học hiệu quả để tùy chỉnh các biểu diễn nội tại hoặc chính sách của nó dựa trên các đặc điểm cụ thể của môi trường hoặc nhiệm vụ. Ví dụ, Sarafian et al. [58] đã áp dụng hypernets để tạo ra các khối xây dựng của RL, tức là mạng chính sách và Q-functions, thay vì sử dụng MLP. Họ cho thấy huấn luyện nhanh hơn và hiệu suất được cải thiện trên các thuật toán khác nhau cho RL và trong meta-RL. Tương tự, hypernets điều kiện nhiễu được sử dụng trong [74] để tạo ra trọng số của mỗi lần lặp Bellman với HyperRNN, và hypernets điều kiện nhiệm vụ được sử dụng trong RL cho tổng quát hóa giữa các nhiệm vụ [6], continual RL [29], và zero-shot learning [54].

Natural Language Processing: Natural language processing (NLP) là một lĩnh vực con của trí tuệ nhân tạo tập trung vào tương tác giữa máy tính và ngôn ngữ con người. Nó bao gồm nhiều nhiệm vụ khác nhau, chẳng hạn như tạo ngôn ngữ, phân tích cảm xúc, dịch máy, và trả lời câu hỏi, trong số các nhiệm vụ khác. Trong ngữ cảnh NLP, hypernets có thể được sử dụng để tạo ra hoặc thích ứng kiến trúc mạng nơ-ron, điều chỉnh siêu tham số, cho tìm kiếm kiến trúc nơ-ron, và cho transfer learning và domain adaptation v.v. Ví dụ, Volk et al. [75] đã áp dụng hypernet điều kiện dữ liệu cho tổng quát hóa ngoài phân bố (OOD). Họ sử dụng khung encoder-decoder T5 để tạo ra chữ ký duy nhất cho mỗi ví dụ từ các miền nguồn khác nhau. Chữ ký này đóng vai trò là đầu vào cho hypernet và tạo ra tham số cho mạng đích – một mạng động và thích ứng. Như đã thảo luận ở trên, Mahabadi et al. [43] đã áp dụng hypernets điều kiện nhiệm vụ để fine-tune các mô hình ngôn ngữ được huấn luyện trước bằng cách tạo ra trọng số cho các bộ chuyển đổi thắt cổ chai. Trong cài đặt multitasking, họ đã mô hình hóa nhiệm vụ, vị trí bộ chuyển đổi và id lớp như các nhiệm vụ khác nhau và sử dụng nhúng của những nhiệm vụ này làm đầu vào cho hypernet giúp trong việc học được chia sẻ và đạt được hiệu quả tham số.

Computer Vision: Computer vision tập trung vào việc cho phép máy tính hiểu và diễn giải thông tin trực quan từ hình ảnh hoặc video. Các thuật toán computer vision nhằm sao chép nhận thức thị giác của con người bằng cách phát hiện và nhận dạng các đối tượng, hiểu các mối quan hệ không gian của chúng, trích xuất đặc trưng, và hiểu rõ cảnh trực quan. Một số ứng dụng của hypernets trong computer vision là: Ha et al. [24], trong công việc tiên phong của họ, đầu tiên đã áp dụng hypernets điều kiện nhiệm vụ cho phân loại hình ảnh, Alaluf et al. [2], Muller [44] đã áp dụng hypernets điều kiện dữ liệu, nơi hình ảnh đóng vai trò là đầu vào cho hypernet, cho cải thiện hình ảnh, và Ratzlaff and Fuxin [53] đã áp dụng hypernets điều kiện nhiễu cho phân loại hình ảnh. Hypernets điều kiện dữ liệu cũng được áp dụng cho phân đoạn ngữ nghĩa trong [47]. Một số ứng dụng khác của hypernets trong computer vision là ước tính tư thế camera [21], chuyển đổi phong cách nơ-ron [57], xử lý/chỉnh sửa hình ảnh [2], và cải thiện hình ảnh nơ-ron [44]. Cần lưu ý rằng computer vision là một chủ đề rộng lớn và bao gồm nhiều bối cảnh vấn đề đã thảo luận trước đó nên chúng có thể được sử dụng như vậy với sự thay đổi của dữ liệu hoặc mô hình liên quan đến miền. Ví dụ, hypernets được phát triển cho AutoML, domain adaptation, continual learning, và federated learning v.v. cũng có thể được áp dụng cho các vấn đề computer vision.

Các ứng dụng trên của hypernets không phải là đầy đủ và một số lĩnh vực thú vị khác mà hypernets đã tạo ra kết quả SOTA là học đồ thị tri thức [5], học hình dạng [39], nén mạng [46], học phương trình vi phân [16], xử lý đám mây điểm 3D [65], xử lý lời nói [70], tính toán lượng tử [9], và chưng cất tri thức [77] v.v. Những ứng dụng này chứng minh tiềm năng rộng lớn của hypernetworks trong deep learning, cho phép tạo ra tham số thích ứng và cụ thể cho nhiệm vụ để cải thiện hiệu suất và khả năng tổng quát hóa mô hình.

5 Khi nào chúng ta có thể sử dụng Hypernets?

Sau khi thảo luận về hypernet là gì, nó hoạt động như thế nào, các loại khác nhau của nó, và các ứng dụng hiện tại của nó, câu hỏi quan trọng nhất là khi nào và ở đâu sử dụng hypernets. Điều này sẽ giúp các nhà nghiên cứu và thực hành tận dụng đầy đủ các lợi ích của kỹ thuật đa năng này trong deep learning. Một câu trả lời đơn giản cho câu hỏi, 'Khi nào chúng ta có thể sử dụng Hypernets?' là 'trong tất cả những lĩnh vực ứng dụng mà nó đã được áp dụng'. Có một danh sách dài các lĩnh vực ứng dụng mà hypernets đã được sử dụng, và lĩnh vực quan tâm của người đọc có khả năng được bao gồm. Dựa trên các đặc điểm và ứng dụng của hypernets đã thảo luận ở trên, chúng tôi đã tổng quát hóa và hình thành một số câu hỏi/kịch bản cho người đọc kiểm tra liệu hypernets có thể được áp dụng cho một lĩnh vực/bối cảnh vấn đề cụ thể hay không. Nếu câu trả lời của chúng tôi là có cho bất kỳ kịch bản nào, thì chúng ta có thể áp dụng hypernets cho bối cảnh vấn đề đang xem xét.

Có bất kỳ thành phần liên quan nào trong bối cảnh vấn đề đang xem xét không?

Ở đây, một thành phần có thể đề cập đến một nhiệm vụ, bộ dữ liệu, hoặc mạng nơ-ron. Đây là một trong những kịch bản/câu hỏi quan trọng nhất, và nhiều ứng dụng, như đã thảo luận ở trên, rơi vào kịch bản này. Nếu câu trả lời cho câu hỏi này là có, thì chúng ta có thể sử dụng hypernets điều kiện nhiệm vụ để giải quyết vấn đề đang xem xét, nơi danh tính nhiệm vụ được sử dụng để tạo ra mạng đích cho thành phần. Bằng cách điều kiện hóa trên thành phần (nhiệm vụ, bộ dữ liệu, hoặc mạng), chúng ta có thể thực hiện huấn luyện kết hợp của các thành phần khác nhau bằng cách khai thác chia sẻ trọng số mềm của hypernets. Điều này cho phép hypernets chia sẻ thông tin giữa các thành phần, dẫn đến hiệu suất được cải thiện [14]. Do đó, chia sẻ thông tin là chìa khóa để đạt được kết quả tốt hơn cho các thành phần liên quan. Câu hỏi có thể được tái cấu trúc thành, 'Chúng ta có cần chia sẻ thông tin trong bối cảnh vấn đề của mình không?'. Tất cả các ứng dụng hypernets điều kiện nhiệm vụ đã thảo luận trong Bảng 2 đều rơi vào kịch bản này. Ví dụ, multitasking [43] có các nhiệm vụ liên quan (như các thành phần), và hypernets giúp trong việc học được chia sẻ trong khi có các mạng được cá nhân hóa cho mỗi nhiệm vụ. Tương tự, continual learning [49], federated learning [63], ước tính hiệu ứng điều trị không đồng nhất [14], transfer learning [49], và domain adaptation [75] rơi vào kịch bản này.

Chúng ta có cần một mạng nơ-ron thích ứng dữ liệu không?

Đây là một kịch bản quan trọng khác với nhiều ứng dụng trên các bối cảnh vấn đề khác nhau. Nói cách khác, chúng ta có thể hỏi, 'Chúng ta có đang làm việc trong một cài đặt mà mạng đích phải được tùy chỉnh cho dữ liệu đầu vào không?' hoặc 'Dữ liệu có thay đổi thường xuyên không?'. Trong kịch bản này, chúng ta có thể sử dụng hypernets điều kiện dữ liệu lấy dữ liệu làm đầu vào và tạo ra một cách thích ứng các tham số của mạng đích. Trong quá trình huấn luyện, hypernet lấy dữ liệu có sẵn và học các đặc điểm nội tại của dữ liệu để tạo ra mạng đích. Sau đó, tại thời điểm suy luận, nó có thể lấy dữ liệu mới với đặc điểm hơi khác và tạo ra mạng đích dựa trên các đặc điểm đã học của dữ liệu hiện có. Cần lưu ý rằng có sự tương đồng giữa các cài đặt điều kiện nhiệm vụ và điều kiện dữ liệu, vì vậy một số vấn đề có thể được mô hình hóa bằng cách sử dụng cả hai kỹ thuật. Từ nghiên cứu hiện có, không rõ khi nào nên mô hình hóa một vấn đề như điều kiện dữ liệu hoặc điều kiện nhiệm vụ, và nó cần được khám phá. Tuy nhiên, nó sẽ phụ thuộc vào vấn đề đang xem xét, tính khả dụng của dữ liệu, và số lượng nhiệm vụ. Tất cả các ứng dụng hypernets điều kiện dữ liệu đã thảo luận trong Bảng 2 đều rơi vào kịch bản này. Ví dụ, trong cải thiện hình ảnh nơ-ron [44], chúng ta quan tâm đến việc cải thiện chất lượng của một hình ảnh, vì vậy chúng ta cần một mạng đích cụ thể cho hình ảnh để có đầu ra chất lượng tốt. Do đó, hypernets điều kiện dữ liệu phù hợp cho ứng dụng này. Tương tự, phòng thủ đối kháng [69], học hình dạng [39], ước tính tư thế camera [21], chuyển đổi phong cách nơ-ron [57], few-shot learning [79], và xử lý đám mây điểm 3D [66] rơi vào kịch bản này.

Chúng ta có cần một kiến trúc mạng nơ-ron động không?

Ở đây, kiến trúc mạng nơ-ron động có nghĩa là kiến trúc của mạng đích không được biết hoặc cố định tại thời điểm huấn luyện. Kịch bản này có các ứng dụng hạn chế nhưng quan trọng. Trong trường hợp này, một hypernet lấy một số thông tin về kiến trúc của mạng đích và tạo ra tham số tương ứng. Ví dụ, tìm kiếm kiến trúc nơ-ron [80] là một ứng dụng như vậy, sử dụng graph hypernetworks lấy đồ thị tính toán của mạng đích làm đầu vào để tạo ra tham số mạng. Tương tự, một ví dụ khác của kịch bản này là khi mạng nơ-ron hồi quy được triển khai với hypernets [24], cần kiến trúc mạng động để tính đến số lượng bước thời gian biến thiên.

Chúng ta có cần huấn luyện nhanh hơn/hiệu quả tham số không? Như đã thảo luận trước đó, hypernets có thể đạt được hiệu quả tham số hoặc nén trọng số, có nghĩa là trọng số 'có thể học' của HyperDNN ít hơn DNN tương ứng. Điều này được kỳ vọng sẽ đạt được huấn luyện nhanh hơn. Điều này có thể hữu ích cho các cài đặt tài nguyên hạn chế và sẽ phụ thuộc vào bối cảnh vấn đề cũng như kiến trúc của hypernets. Ví dụ, như đã thảo luận trước đó, Mahabadi et al. [43] đã áp dụng hypernets điều kiện nhiệm vụ để fine-tune các mô hình ngôn ngữ được huấn luyện trước bằng cách tạo ra trọng số cho các bộ chuyển đổi thắt cổ chai. Trong cài đặt multitasking, họ đã mô hình hóa nhiệm vụ, vị trí bộ chuyển đổi, và danh tính lớp như các nhiệm vụ khác nhau và sử dụng nhúng của những nhiệm vụ này làm đầu vào cho hypernet giúp trong việc học được chia sẻ và đạt được hiệu quả tham số. Tương tự, Zhao et al. [81] cũng chứng minh hiệu quả tham số trong cài đặt few-shot learning.

Chúng ta có cần định lượng độ không chắc chắn không? Đây là một kịch bản ứng dụng cụ thể cho hypernets. Hypernets có thể được sử dụng để định lượng độ không chắc chắn bằng cách sử dụng hypernets điều kiện nhiễu [33] hoặc bằng cách sử dụng dropout trong hypernets [15]. Như đã thảo luận trước đó, trong một số cài đặt, hypernets có thể tạo ra ước tính độ không chắc chắn tốt hơn, ví dụ, [33,53]. Tuy nhiên, nếu ước tính độ không chắc chắn là mục đích duy nhất của nghiên cứu, thì các kỹ thuật ước tính độ không chắc chắn hiện có phải được khám phá trước. Tuy nhiên, việc sử dụng dropout [67] trong kiến trúc hypernet, tương tự như sử dụng dropout trong DNN tiêu chuẩn, có thể bổ sung cho hypernets hiện có và giúp định lượng độ không chắc chắn.

Các kịch bản được thảo luận có sự chồng chéo, vì vậy nhiều kịch bản có thể phù hợp với một vấn đề đang xem xét. Ví dụ, Mahabadi et al. [43] đã xem xét fine-tuning các mô hình ngôn ngữ sử dụng hypernets, đạt được hiệu quả tham số và sử dụng điều kiện nhiệm vụ (cài đặt thành phần liên quan) để giải quyết nhiều nhiệm vụ. Do đó, bằng cách suy nghĩ về những kịch bản rộng này, người ta có thể xác định liệu hypernets có áp dụng cho bối cảnh vấn đề đang xem xét hay không.

6 Thách thức và Hướng tương lai

Hypernetworks đã cho thấy tiềm năng to lớn trong việc tăng cường các mô hình deep learning với tính linh hoạt, hiệu quả và khả năng tổng quát hóa tăng lên. Tuy nhiên, một số thách thức và cơ hội cho nghiên cứu và phát triển tương lai vẫn chưa được khám phá. Trong phần này, chúng tôi thảo luận về một số thách thức chính và đề xuất các hướng tiềm năng cho việc khám phá tương lai.

Thách thức khởi tạo: Thách thức khởi tạo trong hypernetworks đề cập đến khó khăn trong việc khởi tạo các tham số hypernetwork một cách hiệu quả, vì việc tìm ra các giá trị ban đầu phù hợp cho các tham số hypernetwork còn xa mới được giải quyết. Một lý do cho thách thức khởi tạo là trọng số của mạng đích được tạo ra tại lớp đầu ra của hypernet, và việc tạo trọng số không xem xét kiến trúc theo lớp của mạng đích. Vì vậy, việc khởi tạo trọng số hypernet sử dụng các kỹ thuật khởi tạo cổ điển, chẳng hạn như khởi tạo Xavier [23] và Kaiming [25], không đảm bảo rằng trọng số của mạng đích được khởi tạo trong cùng một phạm vi. Hiệu suất của hypernetwork bị ảnh hưởng mạnh bởi trạng thái ban đầu của mạng đích và các tham số của nó được tạo ra tại lớp đầu ra của hypernet. Nếu mạng đích được khởi tạo kém, nó có thể lan truyền lỗi hoặc độ không chắc chắn đến hypernetwork, ảnh hưởng đến khả năng tạo ra hoặc thích ứng tham số một cách hiệu quả của nó. Chang et al. [10] là những người đầu tiên thảo luận về thách thức khởi tạo hypernets. Họ cho thấy rằng các kỹ thuật cổ điển để khởi tạo DNN không hoạt động tốt với hypernets, tuy nhiên, các bộ tối ưu hóa thích ứng, chẳng hạn như Adam [30], có thể giải quyết vấn đề ở một mức độ nào đó. Các tác giả đề xuất khởi tạo trọng số hypernet theo cách cho phép trọng số mạng đích xấp xỉ việc khởi tạo thông thường của DNN. Tuy nhiên, khó áp dụng điều này vì trọng số của mạng đích thường được tạo ra cùng nhau. Chúng ta có thể giải quyết thách thức này nếu quá trình tạo trọng số nhận thức về kiến trúc theo lớp của mạng đích. Hơn nữa, gần đây, Beck et al. [6] cũng cho thấy rằng thách thức khởi tạo của hypernets xảy ra ngay cả trong meta-RL và các kỹ thuật khởi tạo cổ điển thất bại.

Độ phức tạp/Khả năng mở rộng: Một trong những thách thức chính trong hypernetworks là khả năng mở rộng và hiệu quả của các mô hình dựa trên hypernetwork. Khi kích thước và độ phức tạp của DNN đích tăng lên, hypernetworks cũng trở nên rất phức tạp, ví dụ, kích thước của lớp đầu ra thường là m×n trong đó m là số lượng nơ-ron trong lớp gần cuối cùng của hypernet và n là số lượng trọng số trong mạng đích. Vì vậy, hypernets có thể không phù hợp cho các mô hình lớn trừ khi các chiến lược tạo trọng số thích hợp được phát triển và sử dụng. Mặc dù có một số cách tiếp cận, chẳng hạn như tạo trọng số nhiều lần [14] và tạo trọng số theo khối [8] để quản lý độ phức tạp của hypernets nhưng nó cần thêm nghiên cứu để giải quyết thách thức khả năng mở rộng và làm cho hypernetworks thực tế hơn cho các ứng dụng thực tế.

Tính ổn định số: Tính ổn định số trong hypernetworks đề cập đến khả năng của mô hình duy trì các tính toán chính xác và đáng tin cậy trong suốt quá trình huấn luyện và suy luận. Hypernets, giống như mạng nơ-ron tiêu chuẩn, có thể gặp phải các vấn đề về tính ổn định số [58]. Một vấn đề tính ổn định số phổ biến trong hypernetworks là vấn đề gradient biến mất hoặc bùng nổ. Trong quá trình huấn luyện, gradient có thể trở nên cực kỳ nhỏ hoặc lớn, khiến mô hình khó cập nhật tham số một cách hiệu quả. Điều này có thể dẫn đến sự hội tụ chậm hoặc động lực huấn luyện không ổn định. Để giải quyết các vấn đề về tính ổn định số trong hypernets, có thể sử dụng nhiều kỹ thuật khác nhau, chẳng hạn như khởi tạo cẩn thận các tham số của mô hình, việc sử dụng gradient clipping, giới hạn các giá trị gradient để ngăn chúng trở nên quá lớn, và các kỹ thuật chính quy hóa khác nhau chẳng hạn như weight decay, dropout, và spectral norm [14] giúp cải thiện tính ổn định số bằng cách ngăn chặn overfitting và thúc đẩy tối ưu hóa mượt mà hơn. Hơn nữa, tương tự như DNN tiêu chuẩn, việc sử dụng các hàm kích hoạt thích hợp, chẳng hạn như ReLU hoặc Leaky ReLU, có thể giúp giảm thiểu vấn đề gradient biến mất bằng cách cung cấp các tính phi tuyến cho phép lan truyền gradient hiệu quả hơn. Cũng quan trọng là chọn các thuật toán tối ưu hóa thích hợp được biết đến với tính ổn định của chúng, chẳng hạn như Adam [30], có thể xử lý động lực huấn luyện của hypernetworks một cách hiệu quả hơn [10].

Hiểu biết lý thuyết: Phân tích lý thuyết về hypernetworks liên quan đến việc nghiên cứu khả năng biểu diễn, động lực học và tính chất tổng quát hóa của chúng. Bằng cách hiểu các nền tảng lý thuyết của hypernetworks, các nhà nghiên cứu có thể có được những hiểu biết sâu sắc về các nguyên tắc cơ bản thúc đẩy hiệu quả của chúng và khám phá những hướng mới để cải thiện hiệu suất của chúng. Giống như DNN, việc hiểu hoạt động của hypernets còn xa mới được giải quyết. Mặc dù có một số công trình cung cấp hiểu biết lý thuyết về hypernets, ví dụ, Littwin et al. [38] đã nêu bật rằng hypernetworks vô hạn rộng có thể không hội tụ đến một cực tiểu toàn cục sử dụng gradient descent, nhưng tính lồi có thể được đạt được bằng cách tăng chiều của đầu ra của hypernetwork. Galanti and Wolf [22] cũng nghiên cứu tính mô-đun của hypernets và cho thấy rằng hypernets có thể hiệu quả hơn phương pháp dựa trên nhúng để ánh xạ một đầu vào thành một hàm. Một cách trực quan, hypernets ánh xạ một đầu vào thành một điểm trên một manifold có chiều thấp cho trọng số của mạng đích [63] – những hiểu biết lý thuyết về mối liên hệ giữa hai điều này có thể rất hữu ích. Do đó, nghiên cứu thêm về các tính chất lý thuyết của hypernets sẽ giúp làm cho chúng phổ biến hơn và cũng sẽ thu hút thêm nghiên cứu.

Deep Learning nhận thức độ không chắc chắn: Mạng nơ-ron nhận thức độ không chắc chắn cho phép dự đoán đáng tin cậy và mạnh mẽ hơn, đặc biệt trong các kịch bản mà ước tính độ không chắc chắn là quan trọng, chẳng hạn như ra quyết định dưới độ không chắc chắn, các ứng dụng quan trọng về an toàn, hoặc khi làm việc với dữ liệu hạn chế hoặc nhiễu [1]. Mặc dù thành công của DNN và sự phát triển của các kỹ thuật định lượng độ không chắc chắn khác nhau, nó vẫn là một vấn đề mở để định lượng độ không chắc chắn dự đoán [32]. Hypernets đã mở ra một cánh cửa mới cho định lượng độ không chắc chắn vì hypernets điều kiện nhiễu có thể tạo ra phân bố trên trọng số mạng đích và đã được chứng minh là có độ không chắc chắn tốt hơn SOTA [33,53]. Tương tự, Chauhan et al. [15] đã sử dụng hypernets điều kiện nhiệm vụ với dropout để tạo ra nhiều bộ trọng số cho mạng đích. Nghiên cứu thêm về điều này có thể cung cấp các kỹ thuật hiệu quả về mặt tính toán và hiệu quả so với các kỹ thuật khác, chẳng hạn như các phương pháp ensemble, cần huấn luyện nhiều mô hình.

Tăng cường khả năng diễn giải: Sẽ hữu ích cho cộng đồng phát triển các phương pháp để trực quan hóa, phân tích và giải thích các trọng số cụ thể cho nhiệm vụ được tạo ra bởi hypernetworks. Điều này bao gồm việc phát triển các phương pháp trực quan hóa trực quan, và các kỹ thuật phân tích mức độ liên quan của đặc trưng cung cấp những hiểu biết sâu sắc hơn về quá trình tạo trọng số và ra quyết định của các mô hình dựa trên hypernetwork.

Nén mô hình và hiệu quả: Hypernetworks có thể hỗ trợ nén mô hình và hiệu quả trong một số bối cảnh vấn đề [81,43], nơi các hypernets nhỏ hơn được huấn luyện để tạo ra các mạng đích lớn hơn có thể giảm dấu chân bộ nhớ và yêu cầu tính toán của mô hình. Điều này đặc biệt hữu ích trong các môi trường hạn chế tài nguyên nơi bộ nhớ và tài nguyên tính toán bị hạn chế, và hypernets có thể được nghiên cứu cụ thể cho các cài đặt như vậy.

Hướng dẫn sử dụng: Hypernetworks thêm độ phức tạp bổ sung vào việc giải quyết vấn đề. Như với HyperDNN, chúng ta có một mạng bổ sung để tạo ra trọng số cho DNN đích. Hypernets giới thiệu các siêu tham số bổ sung liên quan đến quá trình tạo trọng số, ví dụ, loại tạo trọng số nào nên được sử dụng và bao nhiêu khối nên được sử dụng. Một số nghiên cứu và hướng dẫn cần thiết để hướng dẫn các nhà nghiên cứu qua những lựa chọn này, nhấn mạnh nhu cầu về một nghiên cứu so sánh các cách tiếp cận khác nhau dưới các bối cảnh vấn đề khác nhau.

Do đó, lĩnh vực hypernetworks trong deep learning trình bày một số thách thức và cơ hội cho nghiên cứu tương lai. Những tiến bộ trong các lĩnh vực này sẽ mở đường cho việc áp dụng rộng rãi và sử dụng hiệu quả hypernetworks trong các lĩnh vực khác nhau của deep learning.

7 Kết luận

Hypernetworks đã nổi lên như một cách tiếp cận đầy hứa hẹn để tăng cường các mô hình deep learning với tính linh hoạt, hiệu quả, tổng quát hóa, nhận thức về độ không chắc chắn và chia sẻ thông tin tăng lên. Chúng đã mở ra những hướng mới cho nghiên cứu và ứng dụng trên nhiều lĩnh vực khác nhau. Trong bài báo này, chúng tôi đã trình bày tổng quan đầu tiên về hypernetworks trong ngữ cảnh deep learning. Chúng tôi đã cung cấp một ví dụ minh họa để giải thích hoạt động của hypernetworks và đề xuất một phân loại dựa trên năm tiêu chí thiết kế: đầu vào, đầu ra, tính biến thiên của đầu vào và đầu ra, và kiến trúc của hypernets. Chúng tôi đã thảo luận về một số ứng dụng quan trọng của hypernets cho các vấn đề deep learning khác nhau, bao gồm đa nhiệm vụ, continual learning, federated learning, suy luận nhân quả, và computer vision. Ngoài ra, chúng tôi đã trình bày các kịch bản và câu hỏi để giúp người đọc hiểu liệu hypernets có thể được áp dụng cho một bối cảnh vấn đề nhất định hay không. Cuối cùng, chúng tôi đã nêu bật các thách thức cần được giải quyết trong tương lai. Những thách thức này bao gồm khởi tạo, tính ổn định, khả năng mở rộng, hiệu quả, và nhu cầu về hiểu biết lý thuyết. Nghiên cứu tương lai nên tập trung vào việc giải quyết những thách thức này để thúc đẩy thêm lĩnh vực hypernetworks và làm cho chúng dễ tiếp cận và thực tế hơn cho các ứng dụng thực tế. Bằng cách giải quyết những vấn đề này, tiềm năng của hypernetworks có thể được hiện thực hóa đầy đủ, dẫn đến các mô hình deep learning mạnh mẽ và linh hoạt hơn.
