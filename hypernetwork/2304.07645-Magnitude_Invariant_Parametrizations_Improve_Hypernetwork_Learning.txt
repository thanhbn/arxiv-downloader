# 2304.07645.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypernetwork/2304.07645.pdf
# File size: 2787850 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Magnitude Invariant Parametrizations Improve Hypernetwork Learning
Jose Javier Gonzalez Ortiz
MIT CSAIL
josejg@mit.eduJohn Guttag
MIT CSAIL
guttag@mit.eduAdrian V . Dalca
MIT CSAIL & HMS, MGH
adalca@mit.edu
Abstract
Hypernetworks, neural networks that predict the parame-
ters of another neural network, are powerful models that have
been successfully used in diverse applications from image
generation to multi-task learning. Unfortunately, existing hy-
pernetworks are often challenging to train. Training typically
converges far more slowly than for non-hypernetwork models,
and the rate of convergence can be very sensitive to hyper-
parameter choices. In this work, we identify a fundamental
and previously unidentified problem that contributes to the
challenge of training hypernetworks: a magnitude propor-
tionality between the inputs and outputs of the hypernetwork.
We demonstrate both analytically and empirically that this
can lead to unstable optimization, thereby slowing down con-
vergence, and sometimes even preventing any learning. We
present a simple solution to this problem using a revised hy-
pernetwork formulation that we call Magnitude Invariant
Parametrizations (MIP). We demonstrate the proposed so-
lution on several hypernetwork tasks, where it consistently
stabilizes training and achieves faster convergence. Further-
more, we perform a comprehensive ablation study including
choices of activation function, normalization strategies, input
dimensionality, and hypernetwork architecture; and find that
MIP improves training in all scenarios. We provide easy-
to-use code that can turn existing networks into MIP-based
hypernetworks.
1. Introduction
Hypernetworks, neural networks that predict the param-
eters of another neural network, are increasingly important
models in a wide range of applications such as Bayesian
optimization [ 27,39,54], generative models [ 1,10,44,61],
amortized model learning [ 3,11,22,38,58], continual
learning [ 12,21,57], multi-task learning [ 34,49,53], and
meta-learning [ 6,62,63]. Despite their advantages and grow-
ing use, training hypernetworks is challenging. Compared
to non-hypernetwork-based models, training existing hyper-
networks is often unstable. At best this increases training
time, and at worst can prevent training from converging atall. This burden limits their adoption, negatively impacting
many applications. Existing hypernetwork heuristics, like
gradient clipping [ 15,27], are insufficient in many instances,
while existing techniques aimed at improving standard neural
network training often fail when applied to hypernetworks.
This work addresses a cause of training instability. We
identify and characterize a previously unidentified hypernet-
work design problem and provide a straightforward solution
to address it. We demonstrate analytically and empirically
that the typical choices of architecture and parameter initial-
ization in hypernetworks cause a proportionality relationship
between the scale of the hypernetwork inputs and the scale
of the parameter outputs (Fig. 1a). The resultant fluctuations
in predicted parameter scale lead to large fluctuations in the
scale of the gradients during optimization, resulting in un-
stable training and slow convergence. In some cases, this
phenomenon even prevents any meaningful learning. To over-
come this issue, we propose a straightforward revision to
hypernetwork models: Magnitude Invariant Parametrizations
(MIP). MIP effectively eliminates the influence of the scale of
hypernetwork inputs on the scale of the predicted parameters,
while retaining the representational power of existing formu-
lations. We demonstrate the effectiveness of our proposed so-
lution across several hypernetwork learning tasks, providing
evidence that hypernetworks using MIP achieve faster con-
vergence without compromising model accuracy (Fig. 1b).
Our main contributions are:
•We characterize a previously unidentified optimization
problem in hypernetwork training, and show that it
leads to large gradient variance and unstable training
dynamics.
•We propose a solution: Magnitude Invariant Parametriza-
tions (MIP), a hypernetwork formulation that addresses
the issue without introducing additional training or
inference costs.
•We rigorously study the proposed parametrization.
We first compare it with the standard formulation and
against popular normalization strategies, showing that
it consistently leads to faster convergence and more
stable training. We then extensively test it using variousarXiv:2304.07645v2  [cs.LG]  29 Jun 2023

--- PAGE 2 ---
(a) HyperNetwork Proportionality Issue
0.0 0.5 1.0
0.000.050.100.15Stdev()
Default Hypernetwork
0.0 0.5 1.0
MIP Hypernetwork
Initial
Final (b) Convergence Improvements
0 500 1000 1500 2000
Epoch1.52.02.53.03.54.0T est LossHypernet
Default
MIP (ours)
Figure 1: (a) Proportionality Issue . With default formulations, the scale of the predicted parameters θ(measured in standard
deviation) is directly proportional to scale of the hypernetwork input γat initialization (initial), and even after training the
model (final). Our proposed Magnitude Invariant Parametrizations (MIP) mitigates this proportionality issue with respect to
γ.(b) Convergence Improvements . Using MIP leads to faster convergence and result in reduced variance across network
initializations than the default hypernetwork formulation.
choices of optimizer, input dimensionality, hypernet-
work architecture, and activation function, finding that it
improves hypernetwork training in all evaluated settings.
•We release our implementation as an open-source
PyTorch library, HyperLight1. HyperLight facilitates
the development of hypernetwork models and provides
principled choices for parametrizations and initializa-
tions, making hypernetwork adoption more accessible.
We also provide code that enables using MIP seamlessly
with existing models.
2. Related Work
Research into training stability and efficiency of neural
networks involves a variety of strategies, including parameter
initialization strategies, normalization techniques, and
adaptive optimization.
Parameter Initialization . Deep neural networks
experience unstable training dynamics in the presence of
exploding or vanishing gradients [ 14]. Weight initialization
plays a critical role in the magnitude of gradients, particularly
during the early stages of training. Commonly, weight
initialization strategies focus on preserving the magnitude
of activations during the forward pass and maintaining the
magnitude of gradients during the backward pass [ 13,16].
Our work demonstrates that existing initialization strategies
can be ineffective when applied to hypernetworks.
Normalization Techniques . Normalization techniques
control the distribution of weights and activations, often
leading to improvements in convergence by smoothing the
loss surface [ 7,23,31,48]. Batch normalization is widely
used to normalize activations using minibatch statistics, and
methods like layer or group normalization instead normalize
1Source code at https://github.com/JJGO/hyperlightacross features [ 2,55,59]. Other methods reparametrize
the weights using weight-normalization strategies or using
self-normalizing networks [ 26,41,47]. As we show in our
experiments, these strategies fail to resolve the proportion-
ality issue we study. They either maintain the proportionality
relationship (as in batch normalization), or eliminate pro-
portionality by rendering the predicted weights independent
of the hypernetwork input (as in layer normalization),
eliminating the utility of the hypernetwork itself.
Adaptive Optimization . High gradient variance can
be detrimental to model convergence in stochastic gradient
methods [ 24,46]. Solutions to mitigate gradient variance
encompass adaptive optimization techniques, which aim
to decouple the effect of gradient direction and gradient
magnitude by normalizing by a history of previous gradient
magnitudes [ 25,60]. Similarly, applying momentum reduces
the instantaneous impact of stochastic gradients [ 36,40] by
using parameter updates based on an exponentially decaying
average of past gradients. These strategies are implemented
by many widely-used optimizers, such as Adam [ 5,25].
Our experiments show that although adaptive optimizers
like Adam enhance hypernetwork optimization, they do not
address the root cause of the identified proportionality issue,
and most convergence problems still persist.
Fourier Features . High-dimensional Fourier projections
have been used in feature engineering [ 42] and for positional
encodings in language modeling applications to account for
both short and long range relationships [ 56,51]. Additionally,
implicit neural representation models benefit from sinusoidal
representations [ 50,52]. Our work also uses low dimensional
Fourier projections. We demonstrate their use as a means to
project hypernetwork inputs to a vector space with constant
Euclidean norm, mitigating the training challenge.
Residual Forms . Residual and skip connections are

--- PAGE 3 ---
widely used in deep learning models and often improve
model training, particularly with increasing network
depth [ 18,19,28,56]. Building on this intuition, instead of
the hypernetworks predicting the network parameters directly,
our proposed hypernetworks predict parameter changes ,
mitigating part of the proportionality problem at hand.
3. The Hypernetwork Proportionality Problem
Preliminaries . Deep learning tasks most often involve
a model f(x;θ)→y, with learnable parameters θ. In
hierarchical models using hypernetworks, the parameters θ
of the primary network fare predicted by a hypernetwork
h(γ;ω)→θbased on a input vector γ. Instead of learning
the parameters θof the primary network fdirectly, only
the learnable parameters ωof the hypernetwork hare
optimized using backpropagation. The specific nature of
the hypernetwork inputs γvaries across applications, but
regularly corresponds to a low dimensional quantity that
models properties of the learning task, and is often a simple
scalar or embedding vector [3, 11, 22, 29, 38, 54, 58].
Assumptions . For our analysis we assume the following
about the hypernetwork formulation: 1) The architecture
is a series of fully connected layers of the form ϕ(Wx+b)
where Ware the parameters, bthe biases and ϕ(x)the
non-linear activation function; 2) The activation satisfies
ϕ(x) = max( αx,0) + min( βx,0)for some coefficients α
andβ. This encompasses common choices such as ReLU,
LeakyReLU or PReLU; 3) Bias vectors bare initialized to zero.
Existing hypernetworks satisfy these properties for the large
majority of applications [ 8,10,11,15,29,33,38,54,57,58].
Input-Output Proportionality . We demonstrate that
under these widely-used settings, inputs and outputs of
hypernetworks involve a proportionality relationship, and
describe how this can impede hypernetwork training. We
show that 1) at initialization, any intermediate feature
vector x(k)at layer kwill be proportional to the hypernetwork
input γ, even under the presence of non-linear activation
functions, and 2) this leads to large gradient magnitude
fluctuations detrimental to optimization.
We first consider the case where γ∈Ris a scalar value.
Leth(γ;ω)use a fully connected architecture composed of
a series of fully connected layers
h(γ;ω)=W(n)x(n)+b(n)
x(k+1)=ϕ(W(k)x(k)+b(k))
x(1)=γ(1)
where x(k)is the input vector of the kthfully connected layer
with learnable parameters W(k)and biases b(k). To prevent
gradients from exploding or vanishing when chaining several
layers, it is common to initialize the parameters W(i)and
biases b(i)so that either the magnitude of the activations
is approximately constant across layers in the forward pass(known as fan in ), or so that the magnitude of the gradients is
constant across layers in the backward pass (known as fan out )
[13,16]. In both settings, the parameters W(i)are initialized
using a zero mean Normal distribution and bias vectors b(i)
are initialized to zero. If γ >0, andϕ(x)has the common form
specified above, at initialization the ithentry of vector x(2)is
x(2)
i=ϕ(W(1)
iγ+b(1))=γϕ(W(1)
i)∝γ, (2)
since b(1)= 0 andϕ(W(1)
i)is independent of γ. Using
induction, we assume that for layer k,x(k)
j∝γ∀j, and show
this property for layer k+1. The value of the ithelement of
vector x(k+1)is
x(k+1)
i =ϕ
b(k)
i+P
jW(k)
ijx(k)
j
=γϕP
jW(k)
ijα(k)
j
∝γ,
(3)
since b(k)
i=0, and the term inside ϕis independent of γ. Ifγ
is not strictly positive, we can reach the same proportionality
result, but with separate constants for the positive and the
negative range. This dependency holds regardless of the
number of layers and the number of neurons per hidden layer,
and also holds when residual connections are employed.
When γis a vector input, we find a similar relationship with
the overall magnitude of the input and the magnitude of the
output. Given the absence of bias terms, and the lack of mul-
tiplicative interactions in the architecture, the fully connected
network propagates magnitude changes in the input. We
provide further details in the supplementary material.
Training implications . Since θ=x(n+1), this result
leads to a proportionality relationship for the magnitude
of the predicted parameters ||θ||2∝ ||γ||and their variance
Var(θ)∝||γ||2. As the scale of the primary network param-
etersθwill depend on γ, this will affect the scale of the layer
outputs and gradients of the primary network. In turn, these
large gradient magnitude fluctuations lead to unstable training
dynamics for stochastic gradient descent methods [13].
Further Considerations . Our analysis relies on biases
being at zero, which only holds at initialization, and does
not include normalization layers that are sometimes used.
However, in our experiments, we find that biases remain near
zero during early training, and hypernetworks with alternative
choices of activation function, input dimensionality, or with
normalization layers, still suffer from the identified issue and
consistently benefit from our proposed parametrization (see
Section 6).
4. Magnitude Invariant Parametrizations
To address the proportionality dependency, we make
two straightforward changes to the typical hypernetwork
formulation: 1) We introduce an encoding function that maps
inputs into a constant-norm vector space, and 2) we treat
hypernetwork predictions as additive changes to the main
network parameters, rather than as the parameters themselves.
These changes make the primary network weight distribution

--- PAGE 4 ---
concat
Dense+ReLU
Dense+ReLUReshape
Reshape DenseDense
Reshape DenseFigure 2: Magnitude Invariant Parametrizations for Hypernetworks . MIP first projects the hypernetwork inputs γto a
constant norm vector space. Then the outputs of the hypernetwork ∆θare treated as additive changes to a set of independent
learnable parameters θ0to generate the primary network weights θ. In blue we highlight the main components of MIP, the
input encoding E L2and the residual formulation θ=θ0+∆θ.
non-proportional to the hypernetwork input and stable across
the range of hypernetwork inputs. Figure 2 illustrates these
changes to the hypernetwork.
Input Encoding. To address the proportionality problem,
we map the inputs γ∈[0,1]to a space with a constant
Euclidean norm ||EL2(γ)||2=1using the function EL2(γ)=
[cos(γπ/2),sin(γπ/2)]. With this change, the input mag-
nitude to the hypernetwork is constant ||EL2(γ)||= 1∀γ,
so||x(1)|| ̸∝γ. For higher-dimensional inputs, we apply
this transformation to each input individually, leading to
an output vector with double the number of dimensions.
This transformation results in an input representation with a
constant norm, thereby eliminating the proportionality effect.
For our input encoding we first map each dimension
of the input vector to the range [0,1]to maximize output
range of EL2. We use min-max scaling of the input: γ′=
(γ−γmin)/(γmax−γmin). For unconstrained inputs such as
Gaussian variables, we first apply the logistic function σ(x)=
1/(1+exp( −x)). If inputs span several orders of magnitude,
we take the log before the min-max scaling as in [3, 11].
Output Encoding. Residual forms have become
a cornerstone in contemporary deep learning architec-
tures [ 18,28,56]. Motivated by these methods, we replace
the typical hypernetwork framework with one that learns
primary network fparameters (what is typically learned
in existing formulations), and then uses the hypernetwork
predictions as additive changes to these parameters. We
introduce a set of learnable parameters θ0, and compute the
primary network parameters as θ=θ0+h(EL2(γ);ω).
This output additive encoding predicts weights that are
independent of the input by decomposing the hypernetwork
contribution as a combination of an independent term θ0
and a dependent term h(EL2(γ);ω). The output encoding
also offers a straightforward and principled mechanism for
initializing hypernetwork weights. First, the hypernetworkweights ωcan be initialized using common initialization
methods for fully connected layers. Then, the independent
parameters θ0can be initialized taking into consideration
their role in the primary network.
Connection to Learned Embeddings. Our input
encoding approach can be understood in relation to the
encoding of categorical hyperparameters in hypernetworks.
Commonly, embedding layers transform categorical inputs
into learnable parameters [ 8,15]. For a scalar input γ,
the first fully connected layer’s weights, W∈RN×1, act
similarly to a learned embedding vector e. For scalar inputs γ
represented by magnitude, the traditional formulation scales
this embedding vector linearly, x(2)=γe. In contrast, our
input encoding method uses W∈RN×2, which can be
decomposed into two embedding vectors, W={e0, e1}.
From this perspective, our encoding function can be viewed
as interpolating between two learnable embeddings vectors,
x(2)=cos( γπ/2)e0+cos( γπ/2)e1.
5. Experimental Setup
5.1. Tasks
We evaluate our proposed parametrization on several tasks
involving hypernetwork-based models.
Bayesian Neural Networks . Hypernetwork models
have been used to learn families of functions conditioned
on a prior distribution [ 54]. During training, the prior
representation γ∈Rdis sampled from the prior distribution
γ∼p(γ)and used to condition the hypernetwork h(γ;ω)→θ
to predict the parameters of the primary network model
f(x;θ). Once trained, the family of posterior networks is
then used to estimate parameter uncertainty or to improve
model calibration. For illustrative purposes we first evaluate
a setting where f(x;θ)is a feed-forward neural network
used to classify the MNIST dataset. Then, we tackle a more

--- PAGE 5 ---
complex setting where f(x;θ)is a ResNet-like model trained
the OxfordFlowers-102 dataset [ 37]. In both settings, we use
the prior N(0,1)for each input.
Hypermorph . Learning-based medical image registration
networks f(xm,xf;θ)→ϕregister a moving image xmto
a fixed image xfby predicting a flow or deformation field ϕ
between them. The common (unsupervised) loss balances
an image alignment term Lsimand a spatial regularization
(smoothness) term Lreg. The learning objective is then
L= (1−γ)Lsim(xm◦ϕ,xf) +γLreg(ϕ), where γcontrols
the trade-off. In Hypermorph [ 22], multiple regularization
settings for medical image registration are learned jointly
using hypernetworks. The hypernetwork is given the trade-off
parameter γas input, sampled stochastically from U(0,1)
during training. We follow the same experimental setup,
using a U-Net architecture for the primary (registration)
network and training with MSE for Lsimand total variation
forLreg. We train models on the OASIS dataset. For
evaluation, we use the predicted flow field to warp anatomical
segmentation label maps of the moving image, and measure
the volume overlap to the fixed label maps [4].
Scale-Space Hypernetworks . We also use a hy-
pernetwork to efficiently learn a family of models with
varying internal rescaling factors in the downsampling and
upsampling layers, as recently done in [ 38]. In this setting, γ
corresponds to the scale factor . Given hypernetwork input γ,
the hypernetwork h(γ;ω)→θpredicts the parameters of
the primary network, which performs the spatial rescaling
operations according to the value of γ. We study a setting
where f(x;θ)is a convolutional network with variable
resizing layers, the rescaling factor is sampled from U(0,0.5),
and evaluate using the OxfordFlowers-102 classification
problem and the OASIS segmentation task.
5.2. Experiment Details
Model . We implement the hypernetwork as a neural
network with fully connected layers and LeakyReLU acti-
vations [ 32] for all but the last layer, which has linear output.
Hypernetwork weights are initialized using Kaiming initial-
ization [ 17] onfan out mode and biases are initialized to zero.
Unless specified otherwise, the hypernetwork architecture
has two hidden layers with 16 and 128 neurons respectively.
We use this implementation for both the default (existing)
hypernetworks, and our proposed (MIP) hypernetworks.
Training . We use two popular choices of optimizer:
SGD with Nesterov momentum, and Adam [ 36,25]. We
search over a range of initial learning rates and report the
best performing models; further details are included in the
Supplementary material A.
Implementation. An important contribution of our work
is the release of HyperLight, our PyTorch hypernetwork
framework. HyperLight not only implements our proposed
hypernetwork parametrization but provides a modular andcomposable API that facilitates the development of hypernet-
work models. Using HyperLight, practitioners can employ
existing non-hypernetwork model definitions and pretrained
model weights, and can easily build models using hierar-
chical hypernetworks. HyperLight source code available at
https://github.com/JJGO/hyperlight .
6. Experimental Results
6.1. Effect of Proportionality on Parameter and
Gradient Distributions
First, we empirically show how the proportionality
phenomenon affects the distribution of predicted weights θ
and their corresponding gradients for the Bayesian neural
networks on MNIST. Figures 3a and 3b compare the
distributions of the primary network weights and layer
outputs for a range of values of hypernetwork input γ. While
the default hypernetwork parametrization is highly sensitive
to changes in the input, the proposed MIP eliminates this
dependency, with the resulting distribution closely matching
that of the non-hypernetwork models. Furthermore, Figure 1a
(in the introduction), shows that using the default formulation,
the scale of the weights correlates linearly with the value of
the hypernetwork input, and that, crucially, this correlation is
still present after the training process ends. In contrast, MIP
parametrizations lead to a weight distribution that is robust
to the input γ, both at the start and end of training.
We also analyze how the proportionality affects the early
phase of hypernetwork optimization by studying the distribu-
tion of gradient norms during training. Figure 3c shows the
norm of the predicted parameter gradients ||∇θL||as training
progresses. As our analysis predicted, hypernetworks with
the default parametrization experience large swings in gra-
dient magnitude because of the proportionality relationship
between inputs and predicted parameters. In contrast, the MIP
strategy leads to a substantially smaller variance and more sta-
ble gradient magnitude compared to the standard formulation.
6.2. Model Training Improvements
In this experiment, we analyze how MIP affects model
convergence for the considered tasks. For all experiments, we
found that MIP hypernetworks did not introduce a measurable
impact in training runtime, so we report per-epoch steps.
Figure 4a shows the training loss and test accuracy for
Bayesian networks trained on MNIST. We find that MIP
parametrizations result in better loss and higher accuracy
sooner during training. MIP also achieves substantially
reduced variance across network initializations. Default
parametrization suffers from sporadic training instabilities
(spikes in the training loss), while MIP leads to stable training.
We found similar results for models trained with SGD.
Figures 4b and 4c present convergence curves for the other
two tasks. For Hypermorph, MIP parametrizations are crucial

--- PAGE 6 ---
(a) Primary Network Parameters
    Default HypernetworkNormalized Density    MIP Hypernetwork
0.2
 0.1
 0.0 0.1 0.2
Parameter Value    Non-hypernetwork NN
0.2
0.4
0.6
0.8
1.0
0.2
0.4
0.6
0.8
1.0 (b) Primary Network Activations
    Default hypernetworkNormalized Density    MIP Hypernetwork
6
 4
 2
 0 2 4 6
Layer Output Activation    Non-hypernetwork NN
0.2
0.4
0.6
0.8
1.0
0.2
0.4
0.6
0.8
1.0 (c) Primary Network Gradients
0 5 10
Epoch0246810121416Gradient Magnitude ||||
Default
MIP (ours)
Figure 3: Distributions of primary network parameters (a) and layer activations (b) . Measurements are taken at
initialization for a default hypernetwork, our proposed MIP hypernetwork, and a conventional neural network with the same
primary architecture. Distributions are shown as kernel density estimates (KDE) of the values due to the high degree of overlap
between the distributions. In contrast, the MIP strategy leads to little change across input values and its distribution closely
matches that of the non-hypernetwork model. Evolution of Gradients (c) Gradient magnitude with respect to hypernetwork
outputs ||∇θL||during early training. Standard deviation is computed across minibatches in the same epoch. MIP leads to
substantially smaller magnitude and improved robustness compared to the default parametrization.
when using SGD with momentum as otherwise the model
fails to meaningfully train. For all choices of learning rate
the default hypernetwork failed to converge, whereas with
MIP parametrization it converged for a large range of values.
With Adam, networks train meaningfully, and MIP models
consistently achieve similar Dice scores substantially faster.
They are less sensitive to weight initializations. While in this
setting the Adam optimizer partially mitigates the gradient
variance issue by normalizing by a history of previous
gradients, the MIP parameterization leads to substantially
faster convergence. Furthermore, for the Scale-Space
segmentation, we find that for both optimizers MIP models
achieve substantially faster convergence and better final
accuracy compared to those with the default parametrization.
Comparison to normalization strategies. We compare
the proposed parametrization to popular choices of normal-
ization layers found in the deep learning literature. Using the
default formulation, where the predicted weights start pro-
portional to the hypernetwork input, we found that existing
normalization strategies fall into two categories: they either
keep the proportionality relationship present (such as batch
normalization), or remove the proportionality by making
the predicted weights independent of the hypernetwork input
(such as layer or weight normalization). We provide further
details in Section B of the supplemental material.
We test several normalization strategies. BatchNorm-P ,adding batch normalization layers to the primary network.
LayerNorm-P , adding feature normalization layers to the
primary network. LayerNorm-H , adding feature normaliza-
tion layers to the hypernetwork layers. WeightNorm , per-
forming weight normalization, which decouples the gradient
magnitude and direction, to weights predicted by the hyper-
network [ 23,2,47]. Figure 5a shows the evolution of the test
accuracy for the Scale-Space hypernetworks trained on Ox-
fordFlowers. We report wall clock time, as some normaliza-
tion strategies, such as BatchNorm, substantially increase the
computation time required per iteration. For networks trained
with SGD, normalization strategies enable training, but do not
significantly improve on default hypernetworks when trained
with Adam. Models trained with SGD momemtum and hyper-
network feature normalization (LayerNorm-H) diverged early
into training for all considered hyperparameter settings. Mod-
els trained with the proposed MIP parametrization lead to sub-
stantially faster convergence and better final model accuracy.
Ablation Analysis. We study the contribution of each
of the two main components of the MIP parametrizations:
input encoding and additive output formulation. Figure 5b
shows the effect on convergence for two tasks. We found
that both components reduce the proportionality dependency
between the hypernetwork inputs and outputs, and that each
component independently achieves substantial improvements
in model convergence. However, we find that best results

--- PAGE 7 ---
(a) Bayesian Networks (MNIST)
0.000.050.100.150.200.250.30Training Loss
0 5 10 15 20
Epoch0.900.920.940.960.981.00T est AccuracyAdam
Hypernet
Default
MIP (ours) (b) HyperMorph
0.550.600.650.70T est Dice ScoreSGD Momentum
0 500 1000 1500 2000 2500 3000
Epoch0.600.650.70T est Dice ScoreAdam
Hypernet
Default
MIP (ours) (c) Scale-Space (OASIS)
0.00.20.40.60.8T est Dice ScoreSGD Momentum
0 200 400 600 800 1000
Epoch0.00.20.40.60.8T est Dice ScoreAdam
Hypernet
Default
MIP (ours)
Figure 4: Model Convergence Improvements . Comparison between default hypernetworks and hypernetworks with MIP for the
Bayesian networks on MNIST (a), HyperMorph (b) and Scale-Space hypernetworks trained on OASIS (c). In all cases, we find that
the MIP parametrization leads to faster model convergence without any sacrifice in final model accuracy compared to the default
parametrization. In (a) we observe that the default hypernetworks experience sporadic training instabilities (spikes in the training
loss), whereas MIP hypernetworks present more stable training. In (b) and (c), we find that for default hypernetworks using
the Adam optimizer substantially helps the training process, however, incorporating MIP leads to even better training dynamics.
(fastest convergence) are consistently achieved when both
components are used jointly during training.
6.3. Robustness Analysis
Number of Input Dimensions to the Hypernetwork . We
study the effect of the number of dimensions of the input to
the hypernetwork model. We evaluate on the Bayesian neural
network task, and we vary the number of dimensions of the
input prior. We train models with geometrically increasing
number of input dimensions, dim(γ)=1,2,...,32. Figure 5c
shows that the proposed MIP strategy leads to improvements
in model convergence and final model accuracy as we
increase the dimension of the hypernetwork input γ.
Choice of Hypernetwork Architecture. We assess model
performance when varying the properties of the hypernetwork
architecture. We vary the width (number of hidden neurons
per layer) and depth (number of layers)– fully connected
networks with 3, 4 and 5 layers and with 16 and 128 neurons
per layer, as well as an exponentially growing number of
neurons per layer Dim(xn) = 16·2n. We find that the MIP
improvements generalize to the all tested hypernetwork
architectures with analogous improvements in model training.
We provide additional results in section B of the supplement.
Nonlinear Function Activation Ablation . While our
method is motivated by the training instability present in
hypernetworks with (Leaky)-ReLU nonlinear activation
functions, we explored applying it to other common choicesof activation functions found in the literature: Tanh, GELU
and SiLU [ 43,20]. Figure 8 (in section B of the supplement)
shows that MIP consistently helps for all choices of nonlinear
activation function, and the improvements are similar to
those of the LeakyReLU models.
7. Limitations
A limitation of this work is that all hypernetwork models
used in our experiments are composed of fully connected
layers and use activation and initialization choices commonly
recommended in the literature. Similarly, we focused on
two optimizers in our experiments, SGD with momentum
and Adam. We believe that we would see similar results
for other less common architectures and optimizers, but this
remains an area of future work. Furthermore, we focus on
training models from scratch. Given that hypernetworks
are becoming increasingly popular in transfer learning tasks
using pretrained models, we believe this will be an interesting
avenue for future analysis of MIP.
8. Conclusion
We showed through analysis and experimentation that tra-
ditional hypernetwork formulations are susceptible to training
instability, caused by the effect of the magnitude of hypernet-
work input values on primary network weights and gradients,
and that standard methods such as batch and layer normal-

--- PAGE 8 ---
(a) Normalization Techniques
0 100 200 300 400 5000.00.20.40.60.8T est AccuracySGD Momentum
0 100 200 300 400 500
Training Time [min]0.00.20.40.60.8T est AccuracyAdam
Default
MIP (Ours)
WeightNorm
BatchNorm-P
LayerNorm-P
LayerNorm-H (b) MIP Ablation
0.00.20.40.60.8T est Dice ScoreScale-Space Hypernetworks (Adam)
Hypernet
Default
MIP (both)
MIP (input only)
MIP (output only)
0 200 400 600 800 1000
Epoch0.20.40.60.8T est AccuracyBayesian Neural Networks (SGD) (c) Input Dimensionality
0.00.5T est Accuracydim() = 1
 dim() = 2
0.00.5T est Accuracydim() = 4
 dim() = 8
0 2000 4000
Epoch0.00.5T est Accuracydim() = 16
0 2000 4000
Epochdim() = 32
Hypernet
Default
MIP
Figure 5: (a) Normalization Strategies . Comparison of hypernetworks trained with various normalization strategies for
Scale-Space hypernetworks trained of OxfordFlowers-102. MIP provides substantially better results than the considered
normalization strategies, achieving faster model convergence and better final test accuracy. (b) Ablation Analysis. Convergence
results for separate components MIP on the Scale-Space Hypernetworks on OASIS using the Adam optimizer and the
Bayesian networks trained with SGD on the OxfordFlowers-102 classification problem using SGD. Each component of the
parametrization leads to improvements in final model accuracy as well as training convergence, and best results are achieved
when using both components simultaneously. (c) Input Dimensionality . MIP parametrizations lead to clear improvements
as we increase the dimensionality of the hypernetwork input γfor the Bayesian networks trained on OxfordFlowers-102.
ization do not solve the problem. We then proposed the use of
a new method, Magnitude Invariant Parametrizations (MIP),
for addressing this problem. Through extensive experiments,
we demonstrated that MIP leads to substantial improvements
in convergence times and model accuracy across multiple
hypernetwork architectures, training scenarios, and tasks.
To further help with the adoption of hypernetworks, we
release our hypernetwork learning library, HyperLight,
which not only implements MIP but also provides a modular
and composable API to facilitate the development of
hypernetwork based models. Given that using MIP never
reduces model performance and can dramatically improve
training, we expect the method to be widely useful for
training hypernetworks.
References
[1]Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and
Amit Bermano. Hyperstyle: Stylegan inversion with
hypernetworks for real image editing. In Proceedings
of the IEEE/CVF conference on computer Vision and
pattern recognition , pages 18511–18521, 2022. 1
[2]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E
Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016. 2, 6, 19[3]Juhan Bae, Michael R Zhang, Michael Ruan, Eric
Wang, So Hasegawa, Jimmy Ba, and Roger Grosse.
Multi-rate vae: Train once, get the full rate-distortion
curve. arXiv preprint arXiv:2212.03905 , 2022. 1, 3, 4
[4]Guha Balakrishnan, Amy Zhao, Mert R Sabuncu,
John Guttag, and Adrian V Dalca. V oxelmorph: a
learning framework for deformable medical image
registration. IEEE transactions on medical imaging ,
38(8):1788–1800, 2019. 5
[5]Lukas Balles and Philipp Hennig. Dissecting adam: The
sign, magnitude and variance of stochastic gradients. In
International Conference on Machine Learning , pages
404–413. PMLR, 2018. 2
[6]Raphael Bensadoun, Shir Gur, Tomer Galanti, and Lior
Wolf. Meta internal learning. Advances in Neural Infor-
mation Processing Systems , 34:20645–20656, 2021. 1
[7]Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q
Weinberger. Understanding batch normalization.
Advances in neural information processing systems , 31,
2018. 2
[8]Oscar Chang, Lampros Flokas, and Hod Lipson.
Principled weight initialization for hypernetworks. In
International Conference on Learning Representations ,
2019. 3, 4

--- PAGE 9 ---
[9]Lee R Dice. Measures of the amount of ecologic associa-
tion between species. Ecology , 26(3):297–302, 1945. 12
[10] Tan M Dinh, Anh Tuan Tran, Rang Nguyen, and
Binh-Son Hua. Hyperinverter: Improving stylegan
inversion via hypernetwork. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11389–11398, 2022. 1, 3
[11] Alexey Dosovitskiy and Josip Djolonga. You only train
once: Loss-conditional training of deep networks. In
International conference on learning representations ,
2020. 1, 3, 4
[12] Benjamin Ehret, Christian Henning, Maria R. Cervera,
Alexander Meulemans, Johannes von Oswald, and
Benjamin F. Grewe. Continual learning in recurrent
neural networks. In International Conference on
Learning Representations , 2021. 1
[13] Xavier Glorot and Yoshua Bengio. Understanding
the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics ,
pages 249–256. JMLR Workshop and Conference
Proceedings, 2010. 2, 3
[14] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep learning . MIT press, 2016. 2
[15] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks.
arXiv preprint arXiv:1609.09106 , 2016. 1, 3, 4
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.
InProceedings of the IEEE international conference
on computer vision , pages 1026–1034, 2015. 2, 3
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.
InProceedings of the IEEE international conference
on computer vision , pages 1026–1034, 2015. 5
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 770–778, 2016. 3, 4
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Identity mappings in deep residual networks.
InEuropean conference on computer vision , pages
630–645. Springer, 2016. 3
[20] Dan Hendrycks and Kevin Gimpel. Gaussian error
linear units (gelus). arXiv preprint arXiv:1606.08415 ,
2016. 7, 16
[21] Christian Henning, Maria R. Cervera, Francesco
D’Angelo, Johannes von Oswald, Regina Traber,
Benjamin Ehret, Seijin Kobayashi, Benjamin F.Grewe, and João Sacramento. Posterior meta-replay
for continual learning. In Conference on Neural
Information Processing Systems , 2021. 1
[22] Andrew Hoopes, Malte Hoffman, Douglas N. Greve,
Bruce Fischl, John Guttag, and Adrian V . Dalca.
Learning the effect of registration hyperparameters
with hypermorph. Machine Learning for Biomedical
Imaging , 1:1–30, 2022. 1, 3, 5, 12
[23] Sergey Ioffe. Batch renormalization: Towards reducing
minibatch dependence in batch-normalized models.
arXiv preprint arXiv:1702.03275 , 2017. 2, 6, 19
[24] Rie Johnson and Tong Zhang. Accelerating stochastic
gradient descent using predictive variance reduction.
Advances in neural information processing systems ,
26:315–323, 2013. 2
[25] Diederik P Kingma and Jimmy Ba. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014. 2, 5
[26] Günter Klambauer, Thomas Unterthiner, Andreas Mayr,
and Sepp Hochreiter. Self-normalizing neural networks.
InProceedings of the 31st international conference on
neural information processing systems , pages 972–981,
2017. 2
[27] David Krueger, Chin-Wei Huang, Riashat Islam,
Ryan Turner, Alexandre Lacoste, and Aaron
Courville. Bayesian hypernetworks. arXiv preprint
arXiv:1710.04759 , 2017. 1
[28] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer,
and Tom Goldstein. Visualizing the loss landscape of
neural nets. Advances in neural information processing
systems , 31, 2018. 3, 4
[29] Jonathan Lorraine and David Duvenaud. Stochastic
hyperparameter optimization through hypernetworks.
arXiv preprint arXiv:1802.09419 , 2018. 3
[30] Ilya Loshchilov and Frank Hutter. Fixing weight decay
regularization in adam. CoRR , abs/1711.05101, 2017.
12
[31] Ekdeep S Lubana, Robert Dick, and Hidenori Tanaka.
Beyond batchnorm: Towards a unified understanding of
normalization in deep learning. Advances in Neural In-
formation Processing Systems , 34:4778–4791, 2021. 2
[32] Andrew L Maas, Awni Y Hannun, and Andrew Y
Ng. Rectifier nonlinearities improve neural network
acoustic models. In Proc. icml , volume 30, page 3.
Citeseer, 2013. 5
[33] Matthew MacKay, Paul Vicol, Jon Lorraine, David
Duvenaud, and Roger Grosse. Self-tuning networks:
Bilevel optimization of hyperparameters using
structured best-response functions. arXiv preprint
arXiv:1903.03088 , 2019. 3

--- PAGE 10 ---
[34] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa
Dehghani, and James Henderson. Parameter-efficient
multi-task fine-tuning for transformers via shared hyper-
networks. arXiv preprint arXiv:2106.04489 , 2021. 1
[35] Daniel S Marcus, Tracy H Wang, Jamie Parker,
John G Csernansky, John C Morris, and Randy L
Buckner. Open access series of imaging studies
(oasis): cross-sectional mri data in young, middle aged,
nondemented, and demented older adults. Journal of
cognitive neuroscience , 19(9):1498–1507, 2007. 12
[36] Yurii Nesterov. Introductory lectures on convex
optimization: A basic course , volume 87. Springer
Science &amp; Business Media, 2013. 2, 5
[37] M-E Nilsback and Andrew Zisserman. A visual
vocabulary for flower classification. In 2006 IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR’06) , volume 2, pages
1447–1454. IEEE, 2006. 5, 12
[38] Jose Javier Gonzalez Ortiz, John Guttag, and Adrian V .
Dalca. Amortized learning of dynamic feature
scaling for image segmentation. arXiv preprint
arXiv:2304.05448 , 2023. 1, 3, 5, 13
[39] Nick Pawlowski, Andrew Brock, Matthew CH Lee,
Martin Rajchl, and Ben Glocker. Implicit weight
uncertainty in neural networks. arXiv preprint
arXiv:1711.01297 , 2017. 1
[40] Ning Qian. On the momentum term in gradient descent
learning algorithms. Neural networks , 12(1):145–151,
1999. 2
[41] Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and
Alan Yuille. Micro-batch training with batch-channel
normalization and weight standardization. arXiv
preprint arXiv:1903.10520 , 2019. 2, 19
[42] Ali Rahimi, Benjamin Recht, et al. Random features
for large-scale kernel machines. In NIPS , volume 3,
page 5. Citeseer, 2007. 2
[43] Prajit Ramachandran, Barret Zoph, and Quoc V Le.
Searching for activation functions. arXiv preprint
arXiv:1710.05941 , 2017. 7, 16
[44] Neale Ratzlaff and Li Fuxin. Hypergan: A generative
model for diverse, performant neural networks. In
International Conference on Machine Learning , pages
5361–5369. PMLR, 2019. 1
[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical
image computing and computer-assisted intervention ,
pages 234–241. Springer, 2015. 12, 13
[46] Nicolas Le Roux, Mark Schmidt, and Francis Bach.
A stochastic gradient method with an exponentialconvergence rate for finite training sets. arXiv preprint
arXiv:1202.6258 , 2012. 2
[47] Tim Salimans and Durk P Kingma. Weight normal-
ization: A simple reparameterization to accelerate
training of deep neural networks. Advances in neural
information processing systems , 29:901–909, 2016. 2, 6
[48] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and
Aleksander M ˛ adry. How does batch normalization help
optimization? In Proceedings of the 32nd international
conference on neural information processing systems ,
pages 2488–2498, 2018. 2
[49] Joan Serrà, Santiago Pascual, and Carlos Segura.
Blow: a single-scale hyperconditioned flow for non-
parallel raw-audio voice conversion. arXiv preprint
arXiv:1906.00794 , 2019. 1
[50] Vincent Sitzmann, Julien Martel, Alexander Bergman,
David Lindell, and Gordon Wetzstein. Implicit neural
representations with periodic activation functions.
Advances in Neural Information Processing Systems ,
33, 2020. 2
[51] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. arXiv
preprint arXiv:2104.09864 , 2021. 2
[52] Matthew Tancik, Pratul P Srinivasan, Ben Mildenhall,
Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal,
Ravi Ramamoorthi, Jonathan T Barron, and Ren Ng.
Fourier features let networks learn high frequency
functions in low dimensional domains. arXiv preprint
arXiv:2006.10739 , 2020. 2
[53] Yi Tay, Zhe Zhao, Dara Bahri, Don Metzler, and
Da-Cheng Juan. Hypergrid transformers: Towards a
single model for multiple tasks. 2021. 1
[54] Kenya Ukai, Takashi Matsubara, and Kuniaki Uehara.
Hypernetwork-based implicit posterior estimation and
model averaging of cnn. In Asian Conference on Ma-
chine Learning , pages 176–191. PMLR, 2018. 1, 3, 4
[55] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
Instance normalization: The missing ingredient for fast
stylization. arXiv preprint arXiv:1607.08022 , 2016. 2,
19
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need.
Advances in neural information processing systems , 30,
2017. 2, 3, 4
[57] Johannes von Oswald, Christian Henning, Benjamin F.
Grewe, and João Sacramento. Continual learning
with hypernetworks. In International Conference on
Learning Representations , 2020. 1, 3

--- PAGE 11 ---
[58] Alan Q Wang, Adrian V Dalca, and Mert R Sabuncu.
Regularization-agnostic compressed sensing mri
reconstruction with hypernetworks. arXiv preprint
arXiv:2101.02194 , 2021. 1, 3
[59] Yuxin Wu and Kaiming He. Group normalization. In
Proceedings of the European conference on computer
vision (ECCV) , pages 3–19, 2018. 2
[60] Matthew D Zeiler. Adadelta: an adaptive learning rate
method. arXiv preprint arXiv:1212.5701 , 2012. 2
[61] Lvmin Zhang and Maneesh Agrawala. Adding condi-
tional control to text-to-image diffusion models, 2023. 1
[62] Dominic Zhao, Johannes von Oswald, Seijin Kobayashi,
João Sacramento, and Benjamin F Grewe. Meta-
learning via hypernetworks. 2020. 1
[63] Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-
learning symmetries by reparameterization. arXiv
preprint arXiv:2007.02933 , 2020. 1

--- PAGE 12 ---
Appendix
A. Additional Experimental Details
A.1. Datasets
MNIST . We train models on the MNIST digit classification task. We use the official MNIST database of handwritten digits.
The MNIST database of handwritten digits comprises a training set of 60,000 examples, and a test set of 10,000 examples. We
use the official train-test split for training data, and further divide the training split into training and validation using a stratified
80%-20% split. We use the digit labels and consider the 10-way classification problem.
OxfordFlowers-102 . We use the OxfordFlowers-102 dataset, a fine-grained vision classification dataset with 8,189 examples
from 102 flower categories [ 37]. We utilize this dataset as it poses a non-trivial learning task that does not quickly converge, and
allows us to better study learning dynamics. We use the official train-test split for training data, and further divide the training
split into training and validation using a stratified 80%-20% split. We perform data augmentation by considering random square
crops of between 25% and 100% of the original image area and resizing images to 256 by 256 pixels. Additionally, we perform
random horizontal flips and color jitter (brightness 25%, contrast 50%, saturation 50%). For evaluation we take the central
square crop of each image and resize to 256 by 256 pixels.
OASIS We use a version of the open-access OASIS Brains dataset [ 22,35], a medical imaging dataset containing 414
MRI scans from separate individuals, comprised of skull-stripped and bias-corrected images that are resampled into an
affinely-aligned, common template space. For each scan, segmentation labels for 24 brain substructures in a 2D coronal slice
are available. We use 64%, 16% and 20% splits for training, validation and test.
A.2. Bayesian Neural Networks
Primary Network . For the MNIST task, we use a LeNet architecture variant that uses ReLU activations as they have become
more prevalent in modern deep learning models. Moreover, we replace the first fully-connected layer with two convolutional
layers of 32 and 64 features. We found this change did not impact test accuracy in non-hypernetwork models, but it lead to
more stable initializations for the default hypernetworks.
For the OxfordFlowers-102 task, the primary network ffeatures a ResNet-like architecture with five downsampling stages
with (16, 32, 64, 128, 128) feature channels respectively. For experiments including normalization layers, such as BatchNorm
and LayerNorm, the learnable affine parameters of the normalization layers are not predicted by the hypernetworks and are
optimized like in regular neural networks via backpropagation.
Training . We train using a categorical cross entropy loss. For both optimizers we use learning rate η=3×10−4. Nevertheless,
we found consistent results with the ones we report using learning rates in the range η=[10−4,3×10−3]. We sample γfrom
the uniform distribution U[0,1].
Evaluation . For evaluation we use top-1 accuracy on the classification labels. In order to get a more fine-grained evolution
of the test accuracy, we evaluate on test set at 0.25 epoch increments during training. We report results with five model replicas
with different random seeds.
A.3. HyperMorph
HyperMorph, a learning based strategy for deformable image registration learns models with different loss functions in
an amortized manner. In image registration, the γhypernetwork input controls the trade-off between the reconstruction and
regularization terms of the loss.
Primary Network . For our primary network fwe use a U-Net architecture [ 45] with a convolutional encoder with five
downsampling stages with two convolutional layers per stage of 32 channels each. Similarly, the convolutional decoder
is composed of four stages with two convolutional layers per stage of 32 channels each. We found that models with more
convolutional filters performed no better than the described architecture.
Training . We train using the setup described in HyperMorph [ 22] using mean squared error for the reconstruction loss and
total variation for the regularization of the predicted flow field. For the Adam optimizer we use β1=0.9andβ2=0.999with
decoupled decay [ 30] and η=10−4, but we found that learning rates [10−4,3×10−3]lead to similar convergence results. For
SGD with momentum, we tested learning rates η={3×10−2,10−2,3×10−3,10−3,3×10−4,10−4,3×10−5,10−5,}. In all
cases the default hypernetwork formulation failed to meaningfully train. We train for 3000 epochs, and sample γuniformly
in the range [0,1]like in the original work.
Evaluation . Like [ 22], we use segmentation labels as the main means of evaluation and use the predicted flow field to
warp the segmentation label maps and measure the overlap to the ground truth using the Dice score [ 9], a popular metric for

--- PAGE 13 ---
measuring segmentation quality. Dice score quantifies the overlap between two regions, with a score of 1 indicating perfect
overlap and 0 indicating no overlap. For multiple segmentation labels, we compute the overall Dice coefficient as the average
of Dice coefficients for each label. We report results with five model replicas with different random seeds.
A.4. Scale-Space Hypernetworks
We evaluate on a task where the hypernetwork input γcontrols architectural properties of the primary network. We use γ
to determine the amount of downsampling in the pooling layers. Instead of using pooling layers that rescale by a fixed factor
of two, we replace these operations by a fractional bilinear sampling operation that rescales the input by a factor of γ.
Primary Network . For classification tasks, our primary network ffeatures a ResNet-like architecture with five
downsampling stages with (16, 32, 64, 128, 128) feature channels respectively. For experiments including normalization
layers, such as BatchNorm and LayerNorm, the learnable affine parameters of the normalization layers are not predicted by
the hypernetworks and are optimized like in regular neural networks via backpropagation.
For segmentation tasks, we model the primary network fusing a U-Net architecture [ 45] with a convolutional encoder with
five downsampling stages with two convolutional layers per stage of 32 channels each. Similarly, the convolutional decoder
is composed of four stages with two convolutional layers per stage of 32 channels each.
Training. We sample the hypernetwork input γuniformly in the range [0,0.5]where γ=0.5corresponds to downsampling by 2.
We train the multi-class classification task using a categorical cross-entropy loss, and train with a weight decay factor of 10−3, and
with label smoothing the ground truth labels with a uniform distribution of amplitude ϵ=0.1. For the segmentation tasks we train
using a cross-entropy loss and then finetune using a soft-Dice loss term, as in [ 38]. For both optimizers we use learning rate η=1×
10−4. Nevertheless, we found consistent results with the ones we report using learning rates in the range η=[1×10−4,3×10−3].
A.5. Implementation Details
Platform . The experiments were carried out using an internal machine with 8 V100 GPUs and under the following platform
settings:
Table 1: Platform Settings
Package Version
Python 3.9.7
PyTorch 1.11.0
torchvision 0.12.0
CUDA 11.3.1
cuDNN 8.2.0
Results for the main set of results discussed in the paper totaled 93.7 gpu-hours whereas results in the supplemental material
required an additional 79.4 gpu-hours.

--- PAGE 14 ---
B. Additional Experimental Results
B.1. Choice of Hypernetwork Architecture
In this experiment we test whether increasing the choice of hypernetwork architecture size has an effect on the improvements
achieved by incorporating Magnitude Invariant Parametrizations (MIP). We study varying the width (the number of neuros per
hidden layer) and the depth (the number of hidden layers) independently as well as jointly. For the depth, we consider networks
with 3, 4 and 5 layers. For with, we consider having 16 neurons per layer, 128 neurons per layer, or having an exponentially
growing number of neurons per layer (exp), following the expression Dim (xn)=16·2n.
We compare training networks using the default hypernetwork parametrization and MIP in the HyperMorph task. Figure 6
shows convergence curves for the evaluated settings, for several random initializations. Additionally, Figure 7 shows the
distribution of final model performances for the range of inputs γ∈[0,1]. We find that MIP models converge faster without
sacrificing final model accuracy.
0.680.700.720.74Val Dice ScoreWidth = 16 Width = 128Depth = 3Width = exp
0.680.700.720.74Val Dice ScoreDepth = 4
0 1000 2000 3000
Epoch0.680.700.720.74Val Dice Score
0 1000 2000 3000
Epoch0 1000 2000 3000
EpochDepth = 5Hypernet
Default
MIP (ours)
Figure 6: Model convergence for several configurations of depth and width of the hypernetwork architecture for default and MIP
hypernetworks. Results correspond to HyperMorph on OASIS. Shaded regions measure standard deviation across hypernetwork
initializations.

--- PAGE 15 ---
3 4 50.7500.7550.7600.765T est Dice ScoreWidth = 16
3 4 5
Hypernetwork DepthWidth = 128
3 4 5Width = exp
Hypernet
Default
MIPFigure 7: Test dice score for several configurations of depth and width of the hypernetwork architecture for default and MIP
hypernetworks. Results correspond to HyperMorph on OASIS. Box-plots are reported over the range of hypernetwork inputs γ.
For all hypernetwork architectures, MIP parametrizations consistently lead to more accurate models.

--- PAGE 16 ---
B.2. Choice of Nonlinear Activation Function
While our method is motivated by the training instability present in hypernetworks with (Leaky)-ReLU nonlinear activation
functions, we explored applying it to other popular choices of activation functions. Moreover, some popular activation functions
such as GELU or SiLU (also known as Swish) are close to the ReLU formulation [ 20,43] We explore three alternative choices
of nonlinear activations: Tanh, GELU and SiLU.
We evaluate on the Bayesian hypernetworks task on the OxfordFlowers-102 dataset with a primary convolutional network
trained optimized with Adam. Figure 8 shows the convergence curves for Bayesian hypernetworks with a primary convolutional
network trained on the OxfordFlowers classification task optimized with Adam. We see that MIP consistently helps for all
choices of nonlinear activation function, and the improvements are similar to those of the LeakyReLU models.
0 500 1000 1500 2000 2500 3000
Epoch1.52.02.53.03.5T est LossGELU
0 500 1000 1500 2000 2500 3000
EpochSiLU
0 500 1000 1500 2000 2500 3000
EpochT anh
Hypernet
Default
MIP (ours)
0 500 1000 1500 2000 2500 3000
Epoch0.40.60.8T est AccuracyGELU
0 500 1000 1500 2000 2500 3000
EpochSiLU
0 500 1000 1500 2000 2500 3000
EpochT anh
Hypernet
Default
MIP (ours)
Figure 8: MIP on alternative nonlinear activation functions Test loss (top row) and test accuracy (bottom row) for bayesian
hypernetworks trained on the OxfordFlowers classification task for various choices of nonlinear activation function in the
hypernetwork architecture: GELU, SiLU and Tanh. For each setting, we train 3 independent replicas with different random
initialization and report the mean (solid line) and the standard deviation (shaded region). We see significant improvements
in model training convergence when the hypernetwork uses the proposed MIP parametrization.

--- PAGE 17 ---
B.3. Number of Input Dimensions
In this experiment, we study the effect of the number of dimensions of the input to the hypernetwork model to the
hypernetwork training process, both for the default parametrization and for our MIP parametrization. We evaluate using the
Bayesian Hypernetworks, as we can vary the number of dimensions of the input prior without having to define new tasks. We
train models with geometrically increasing number of input dimensions, dim(γ)=1,2,...,32. We apply the input encoding to
each dimension independently. We study two types of input distribution: uniform U(0,1)and Normal N(0,1)distributions. For
MIP, we apply a sigmoid to the Normal inputs to constrain them to the [0,1] range as specified by our method. We evaluate on the
Bayesian hypernetworks task on the OxfordFlowers-102 dataset with a primary convolutional network optimized with Adam.
Figure 9 shows the convergence curves during training. Results indicate that the proposed MIP parametrization leads to
improvements in model convergence and final model accuracy for all number of input dimensions to the hypernetwork and
for both choices of input distribution. Moreover, we observe that the gap between MIP and the default parametrization does
not diminish as the number of input dimensions grows.
(a) Uniform Inputs ( Γi=U(0,1))
0 2000 4000
Epoch1.52.02.53.03.5T est Lossdim() = 1
0 2000 4000
Epochdim() = 2
0 2000 4000
Epochdim() = 4
0 2000 4000
Epochdim() = 8
0 2000 4000
Epochdim() = 16
0 2000 4000
Epochdim() = 32
Hypernet
Default
MIP (ours)
0 2000 4000
Epoch0.00.20.40.60.8T est Accuracydim() = 1
0 2000 4000
Epochdim() = 2
0 2000 4000
Epochdim() = 4
0 2000 4000
Epochdim() = 8
0 2000 4000
Epochdim() = 16
0 2000 4000
Epochdim() = 32
Hypernet
Default
MIP (ours)
(b) Gaussian Inputs ( Γi=N(0,1))
0 2000 4000
Epoch1.52.02.53.03.5T est Lossdim() = 1
0 2000 4000
Epochdim() = 2
0 2000 4000
Epochdim() = 4
0 2000 4000
Epochdim() = 8
0 2000 4000
Epochdim() = 16
0 2000 4000
Epochdim() = 32
Hypernet
Default
MIP (ours)
0 2000 4000
Epoch0.00.20.40.60.8T est Accuracydim() = 1
0 2000 4000
Epochdim() = 2
0 2000 4000
Epochdim() = 4
0 2000 4000
Epochdim() = 8
0 2000 4000
Epochdim() = 16
0 2000 4000
Epochdim() = 32
Hypernet
Default
MIP (ours)
Figure 9: Number of dimensions of hypernetwork input . Test loss (top row) and test accuracy (bottom row) for Bayesian
hypernetworks trained on the OxfordFlowers classification task for increasing number of dimensions of the hypernetwork input
γ. We report results for different prior input distributions: Uniform (a) and Gaussian (b). For each setting, we train 3 independent
replicas with different random initialization and report the mean (solid line) and the standard deviation (shaded region). We
see significant improvements in model training convergence when the hypernetwork uses the proposed MIP parametrization.

--- PAGE 18 ---
B.4. Final Model Performance
Table 2: Final model results on thetest set for the considered tasks and models. We report the average performance averaged
across the range of γinputs. We find MIP does not decrease model performance in any setting, while providing substantial
improvements in several of them, specially when using the SGD optimizer. Standard Deviation across random initializations
is included in parentheses.
Adam SGD
Task Data Default MIP Default MIP
Bayesian NNMNIST 98.1 (1.1) 99.1 (0.3) 99.2 (0.2) 99.0 (0.2)
OxfordFlowers-102 78.1 (1.9) 83.2 (0.3) 1.4 (0.1) 75.4 (0.5)
HyperMorph OASIS 71.0 (0.3) 72.1 (0.3) 54.3 (0.4) 70.5 (0.2)
Scale-Space HN OASIS 81.4 (0.3) 84.4 (0.6) 75.3 (2.7) 78.8 (1.4)

--- PAGE 19 ---
B.5. Normalization Strategies
Before developing MIP parametrizations we tested the viability of existing normalization strategies (such as Layer or Weight
normalization) to deal with the identified proportionality phenomenon. While normalizing inputs and activations is a common
practice in neural network training, hypernetworks present different challenges, and applying these techniques can actually be
detrimental to the training process. Hypernetworks predict network parameters, and many of the assumptions behind parameter
initialization and activation distribution do not easily translate between classical networks and hypernetworks.
An important distinction is that the main goal of our formulation is to ensure that the hypernetwork input has constant
magnitude, not that is normalized (i.e. zero mean, unit variance). A normalized variable z∼N(0,1)does not have constant
magnitude (i.e. L2 norm), over its support, so normalization techniques do not solve the identified magnitude dependency
and can actually lead to undesirable formulations. To show this, let x∈Rkbe a hypernetwork activation vector, and γ∈[0,1]
the hypernetwork input. Then, according to the identified proportionality in Section 3.2, we know that x=γz. Here xis the
activation when the input is γandzis a vector independent of γ. The normalization output will be
Norm (x)=x−E[x]
Stdev [x]=γz−E[γz]
Stdev [γz]=γz−γE[z]
|γ|Stdev [z]=z−E[z]
Stdev [z],
making the output independent of the hypernetwork input γ. Following this reasoning, strategies like layer norm, instance
norm or group norm in the hypernetwork will make the output of the model independent of the hypernetwork input, rendering
the hypernetwork unusable for scalar inputs. For batch normalization cases it depends whether different hypernetwork inputs
are used for each element in the minibatch. If not, the same logic applies as in the feature normalization strategies. Otherwise,
the proportionality will still hold as the batch mean and standard deviation will be the same for all entries in the minibatch. Our
experimental results confirm this. Hypernetworks with layer normalization fail to train in most settings. In contrast, we found
consistently that training substantially improves when using our MIP formulation. See Figure 5a in the main body which shows
that none of the tested normalization strategies is competitive with MIP in terms of model convergence or final model accuracy.
Batch Normalization - Applying batch normalization fails to deal with the proportionality phenomenon because it
normalizes statistics that are independent of the magnitude of γkeeping the proportionality [ 23]. In our experiments batch
normalization performed similar to the default formulation when included in either the hypernetwork or the primary network,
failing to address the proportionality relationship. For instance, all of the results in Figure 9 use batch normalization layers
as it is recommended in ResNet-like architectures. In this case, MIP still provides a substantial improvement in terms of model
convergence and training stability.
Feature Normalization - Feature normalization techniques such as layer normalization, instance normalization or group
normalization do remove the proportionality phenomenon we identify [ 2,55]. However, by doing so they make the predicted
weights independent of the input hyperparameter, limiting the modeling capacity of the hypernetwork architecture. Moreover,
in our empirical analysis, networks with layer normalization in the hypernetwork layers failed to train entirely, with the loss
diverging early in training.
Weight Normalization - We also considered techniques that decouple the gradient magnitude and direction such as weight
normalization [ 41]. Performing weight normalization on the hypernetwork predictions effectively decouples the gradient
magnitude and direction. We find that convergence is substantially lower compared to the default parametrization. Moreover,
final model performance does not match the default parametrization.
