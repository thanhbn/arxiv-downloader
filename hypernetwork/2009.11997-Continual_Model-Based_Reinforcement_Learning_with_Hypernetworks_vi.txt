# Học Tăng Cường Dựa Trên Mô Hình Liên Tục với Hypernetwork

Yizhou Huang1, Kevin Xie2, Homanga Bharadhwaj2, và Florian Shkurti2

Tóm tắt — Việc lập kế hoạch hiệu quả trong học tăng cường dựa trên mô hình (MBRL) và điều khiển dự đoán mô hình (MPC) phụ thuộc vào độ chính xác của mô hình động lực học đã được học. Trong nhiều trường hợp của MBRL và MPC, mô hình này được giả định là tĩnh và được huấn luyện lại từ đầu một cách định kỳ trên kinh nghiệm chuyển đổi trạng thái được thu thập từ đầu của các tương tác môi trường. Điều này có nghĩa là thời gian cần thiết để huấn luyện mô hình động lực học – và sự tạm dừng cần thiết giữa các lần thực hiện kế hoạch – tăng tuyến tính với kích thước của kinh nghiệm đã thu thập. Chúng tôi lập luận rằng điều này quá chậm cho việc học robot suốt đời và đề xuất HyperCRL, một phương pháp liên tục học các động lực học gặp phải trong một chuỗi các tác vụ sử dụng hypernetwork có điều kiện tác vụ. Phương pháp của chúng tôi có ba thuộc tính chính: thứ nhất, nó bao gồm các phiên học động lực học không xem lại dữ liệu huấn luyện từ các tác vụ trước đó, vì vậy nó chỉ cần lưu trữ phần có kích thước cố định gần đây nhất của kinh nghiệm chuyển đổi trạng thái; thứ hai, nó sử dụng hypernetwork dung lượng cố định để biểu diễn động lực học không tĩnh và có nhận thức tác vụ; thứ ba, nó vượt trội hơn các phương pháp thay thế học liên tục hiện có dựa trên mạng dung lượng cố định, và hoạt động cạnh tranh với các đường cơ sở nhớ một coreset ngày càng tăng của kinh nghiệm quá khứ. Chúng tôi cho thấy rằng HyperCRL hiệu quả trong học tăng cường dựa trên mô hình liên tục trong các tình huống di chuyển và thao tác robot, chẳng hạn như các tác vụ liên quan đến đẩy và mở cửa. Trang web dự án của chúng tôi với video tại liên kết này http://rvl.cs.toronto.edu/blog/2020/hypercrl/

I. GIỚI THIỆU

Học robot dựa trên mô hình suốt đời được dự đoán dựa trên việc thích ứng liên tục với động lực học của các tác vụ mới. Ví dụ, robot cần học thao tác các vật thể chưa thấy với các phân bố khối lượng khác nhau, đi bộ trên các loại địa hình mới với ma sát, độ đàn hồi và các tính chất vật lý khác nhau, hoặc thậm chí học thích ứng với các tác vụ khác nhau, chẳng hạn như đi bộ, chạy, hoặc leo cầu thang. Điều này đưa ra ít nhất hai thách thức cho nhiều công thức học tăng cường dựa trên mô hình (MBRL) và điều khiển dự đoán mô hình (MPC), thường bao gồm một giai đoạn học động lực học tiếp theo là một giai đoạn lập kế hoạch/tối ưu hóa chính sách và thực hiện. Thứ nhất, các phương pháp này không có tính mở rộng vì thời gian cần thiết để huấn luyện mô hình động lực học tăng tuyến tính với kích thước của kinh nghiệm đã thu thập. Thứ hai, khi người học robot gặp phải và thích ứng với các tác vụ mới, nó phải tránh quên thảm khốc về động lực học của các tác vụ cũ.

Trong công trình này, chúng tôi đề xuất mở rộng phương pháp học liên tục có nhận thức tác vụ dựa trên hypernetwork trong [1] để thích ứng với động lực học môi trường thay đổi và giải quyết các thách thức về tính mở rộng và quên thảm khốc được đề cập ở trên trong một thiết lập học tăng cường. Chúng tôi sử dụng hypernetwork có điều kiện tác vụ, là các mô hình mạng nơ-ron chấp nhận mã hóa tác vụ đã học làm đầu vào, và xuất ra trọng số của một mạng khác (mục tiêu). Trong trường hợp của chúng tôi, đầu ra là mô hình động lực học cho tác vụ đó. Không cần thông tin bổ sung nào khác ngoài ranh giới chuyển đổi tác vụ. Chúng tôi cho thấy rằng học liên tục với hypernetwork dẫn đến học tăng cường dựa trên mô hình hiệu quả, đồng thời giảm số lần cập nhật mô hình động lực học qua các phiên lập kế hoạch và ngăn chặn quên thảm khốc.

Chúng tôi xem xét thiết lập nơi các ranh giới tác vụ được biết để đơn giản hóa vấn đề, mặc dù có các phương pháp học liên tục đã giải quyết thiết lập không biết tác vụ sử dụng các phương pháp Bayesian phi tham số [2]. Ngoài ra, chúng tôi tập trung vào hypernetwork và mạng mục tiêu dung lượng cố định có thể xử lý một chuỗi tác vụ mà không thêm nơ-ron hoặc lớp mới vào mạng, không giống như nhiều công trình liên quan [3], [4] trong đó mỗi tác vụ mới thêm dung lượng vào mô hình động lực học. Chúng tôi lập luận rằng thiết lập dung lượng cố định, cùng với việc chỉ lưu trữ phần gần đây nhất của kinh nghiệm chuyển đổi trạng thái, thực tế và có tính mở rộng hơn cho các ứng dụng học robot suốt đời so với các phương pháp trong đó thời gian huấn luyện hoặc kích thước mô hình tăng tuyến tính với kích thước kinh nghiệm đã thu thập.

Công trình của chúng tôi đóng góp như sau: chúng tôi cho thấy rằng học liên tục có nhận thức tác vụ với hypernetwork là một cách hiệu quả và thực tế để thích ứng với các tác vụ mới và động lực học thay đổi cho học tăng cường dựa trên mô hình mà không cần giữ các chuyển đổi trạng thái từ các tác vụ cũ cũng như không thêm dung lượng vào mô hình động lực học. Chúng tôi đánh giá phương pháp của mình trên các tình huống di chuyển và thao tác, nơi chúng tôi cho thấy rằng phương pháp của chúng tôi vượt trội hơn các đường cơ sở học liên tục liên quan.

II. CÔNG TRÌNH LIÊN QUAN

Học Liên Tục trong Mạng Nơ-ron Học liên tục nghiên cứu vấn đề học tăng dần từ một luồng dữ liệu tuần tự với chỉ một phần nhỏ dữ liệu có sẵn cùng một lúc [5]. Một phương pháp đơn giản nhưng hiệu quả là tinh chỉnh, trực tiếp điều chỉnh mạng tác vụ nguồn đã được huấn luyện trên tác vụ mục tiêu [6]. Hiệu quả của phương pháp này cho học liên tục bị ảnh hưởng bởi hiện tượng quên thảm khốc được thiết lập tốt [4]. Cập nhật hậu nghiệm Bayesian tuần tự là một cách có nguyên tắc để thực hiện học liên tục và tự nhiên tránh vấn đề quên vì hậu nghiệm chính xác hoàn toàn kết hợp tất cả dữ liệu trước đó nhưng trong thực tế phải thực hiện các xấp xỉ có thể dễ bị quên. Consolidation Trọng Số Đàn Hồi (EWC) sử dụng xấp xỉ Laplace cho hậu nghiệm, lưu trữ ma trận fisher thực nghiệm của các tác vụ trước đó và điều chuẩn hóa các độ lệch trọng số tác vụ tương lai dưới chuẩn được cảm ứng của chúng [7]. Các công trình khác cũng đã sử dụng xấp xỉ biến phân trường trung bình [8] hoặc xấp xỉ Laplace được phân tích Kronecker theo khối chéo [9]. Synaptic Intelligence (SI) từ bỏ một giải thích xấp xỉ Bayesian rõ ràng nhưng hoạt động tương tự EWC ở chỗ nó tính toán một thước đo tầm quan trọng tham số tương đối, nhưng thông qua xấp xỉ tuyến tính của đóng góp trong việc giảm mất mát do từng tham số trên các tác vụ trước đó [10]. Các phương pháp coreset ngăn chặn quên thảm khốc bằng cách chọn và lưu trữ một tập con nhỏ hơn đáng kể của dữ liệu tác vụ trước đó, được sử dụng để luyện tập mô hình trong hoặc sau khi tinh chỉnh [11], [12], [13]. Tương tự, các điểm cảm ứng được sử dụng trong các công thức Quy Trình Gaussian (GP) thưa thớt, có thể được xem như một loại coreset, đã được sử dụng cho học liên tục [14], [15]. Một loại phương pháp khác học các thành phần mạng cụ thể cho tác vụ riêng biệt. Phiên bản phổ biến nhất của điều này là mạng đa đầu học và chuyển đổi giữa các lớp đầu ra riêng biệt tùy thuộc vào tác vụ [16]. Mạng Nơ-ron Tiến Bộ (PNN) [4] là một phiên bản cực đoan của phương pháp này, trong đó một bản sao hoàn toàn mới của mạng được thêm vào cho mỗi tác vụ, do đó loại bỏ mọi sự quên lãng. Các phương pháp này có thể phát sinh chi phí bộ nhớ và tính toán đáng kể, đặc biệt là đối với các mô hình lớn hơn và nhiều tác vụ.

RL Dựa Trên Mô Hình Các phương pháp học tăng cường dựa trên mô hình kết hợp việc học mô hình của môi trường trong việc giải quyết tác vụ điều khiển. Theo truyền thống, mô hình được huấn luyện để xấp xỉ động lực học tĩnh và/hoặc phần thưởng của môi trường từ tất cả các mẫu đã thu thập. Các lựa chọn khác nhau cho mô hình đã được đề xuất. Nhiều mô hình phi tham số, chẳng hạn như GP được sử dụng phổ biến, dựa vào việc lưu trữ và suy luận với dữ liệu quá khứ [17], mặc dù trong nhiều trường hợp lượng dữ liệu cần được lưu trữ có thể được giảm mạnh thông qua suy luận biến phân thưa thớt [18]. Các mô hình tham số phi tuyến thường không cho phép các quy tắc cập nhật tuần tự hiệu quả và do đó thường cũng huấn luyện trên tất cả dữ liệu quá khứ [19]. Thông thường, mô hình đã được huấn luyện sau đó được sử dụng để mô phỏng các quỹ đạo tưởng tượng, hoặc cho mục đích lập kế hoạch trực tuyến [20], [21] hoặc như dữ liệu huấn luyện cho một chính sách được khấu hao [22]. Trong các môi trường không tĩnh, động lực học có thể thay đổi theo thời gian. Trong thiết lập này, nhiều công trình tập trung vào việc nhanh chóng thích ứng với sự thay đổi trong động lực học để giảm thiểu hối tiếc trực tuyến, trái ngược với việc duy trì hiệu suất trên động lực học đã trải qua trước đó [23], [24], [25]. Meta-learning là một công cụ phổ biến trong mô hình này trong đó một mô hình động lực học toàn cục được "meta-trained" để nhanh chóng thích ứng với động lực học trực tuyến thực từ chỉ một vài mẫu. Tuy nhiên, quá trình meta-learning thường yêu cầu cập nhật meta-model với dữ liệu từ một tập hợp đa dạng các động lực học được thu thập bằng cách lưu trữ các kinh nghiệm trước đó trong một bộ đệm và/hoặc có thể tương tác đồng thời với nhiều môi trường khác nhau [26].

Mối Quan Hệ với Meta-Learning Công trình của chúng tôi khác với các phương pháp meta-learning trong MBRL như [27], [28] theo hai cách. Thứ nhất, chúng tôi tập trung vào việc ngăn chặn quên thảm khốc và không đào tạo rõ ràng một mô hình tiên nghiệm qua nhiều tác vụ để thích ứng nhanh. Điều này có nghĩa là chúng tôi không cần thiết kế một tập hợp các tác vụ cho meta-training và về nguyên tắc, công trình của chúng tôi có thể liên tục học thực hiện các tác vụ mới từ đầu. Thứ hai, chúng tôi không yêu cầu sử dụng một bộ đệm phát lại tăng tuyến tính với số lượng tác vụ hoặc tổng chiều dài của các cặp chuyển đổi trạng thái đã thu thập. Chúng tôi nhấn mạnh rằng điều này phù hợp với chủ đề của học liên tục, nơi việc lưu trữ dữ liệu quá khứ bị hạn chế.

RL Liên Tục Các phương pháp học liên tục tiết kiệm bộ nhớ trong thiết lập học tăng cường cũng đã được đề xuất. PNN đã được sử dụng trong một phương pháp actor-critic on-policy và được chứng minh trên các trò chơi Atari hành động rời rạc tuần tự. Các tác giả của [29] xây dựng trên PNN và các phương pháp nén chính sách liên tục [30] bằng cách nén mô hình mở rộng từ PNN thành một mạng kích thước cố định sau mỗi tác vụ. Đối với giai đoạn nén, họ đề xuất một thuật toán EWC trực tuyến có thể mở rộng hơn, bỏ qua chi phí tuyến tính của việc lưu trữ các ma trận fisher quá khứ. Việc sử dụng coreset cũng đã được khám phá trong thiết lập này [31]. Trong [27], một mô hình hỗn hợp của các mạng nơ-ron cụ thể cho tác vụ riêng biệt được sử dụng cho mô hình môi trường yêu cầu thêm một mô hình mới mỗi khi một tác vụ được khởi tạo. Một phương pháp tương tự cũng đã được chứng minh sử dụng hỗn hợp GP sử dụng thuật toán phân cụm trực tuyến [32]. Công trình của chúng tôi cũng liên quan đến các giải thích MPC như một sự giảm thiểu cho học trực tuyến [33].

Điều Khiển Mạnh Mẽ và Thích Ứng Tài liệu hiện có về lý thuyết điều khiển cung cấp nhiều lớp phương pháp liên quan xử lý các thay đổi trong động lực học: các phương pháp điều khiển thích ứng xử lý các tham số không biết của mô hình động lực học bằng cách ước tính chúng theo thời gian để thực hiện một quỹ đạo nhất định, và các phương pháp điều khiển mạnh mẽ cung cấp đảm bảo ổn định miễn là tham số hoặc nhiễu mô hình nằm trong giới hạn [34]. Thiết lập chúng tôi nghiên cứu trong công trình này khác ở chỗ chúng tôi học mô hình động lực học từ đầu, không giả định một mô hình tham chiếu, và nó có thể thay đổi qua các tác vụ mà không áp đặt bất kỳ giới hạn nào trên các tham số cụ thể.

III. SƠ BỘ

Hypernetwork cho Học Liên Tục. Một hypernetwork [35], [36] là một mạng tạo ra trọng số của một mạng nơ-ron khác. Hypernetwork H(e) = θ với trọng số θ có thể được điều kiện hóa trên một vector nhúng e để xuất ra trọng số θ của mạng chính (mục tiêu) f_θ(x) = f(x; θ) = f(x; H(e)) bằng cách thay đổi vector nhúng e. Hypernetwork thường lớn hơn về số lượng tham số có thể huấn luyện so với mạng chính vì kích thước của lớp đầu ra trong hypernetwork bằng số lượng trọng số trong mạng mục tiêu. Hypernetwork đã được chứng minh là hữu ích trong thiết lập học liên tục [1] cho các mô hình phân loại và tạo sinh. Điều này đã được chứng minh là làm giảm một số vấn đề của quên thảm khốc. Chúng cũng đã được sử dụng để cho phép tối ưu hóa siêu tham số dựa trên gradient [37].

Lập Kế Hoạch với CEM và MPC. Phương Pháp Cross-Entropy (CEM) [38] là một thuật toán lập kế hoạch trực tuyến được sử dụng rộng rãi lấy mẫu các chuỗi hành động từ một phân phối tiến hóa theo thời gian thường được xem là một Gaussian chéo a_{1:h} ∼ N(μ_{1:h}, diag(σ²_{1:h})), trong đó h là horizon lập kế hoạch. Các chuỗi hành động được lấy mẫu lại lặp đi lặp lại và đánh giá dưới mô hình động lực học hiện tại đã học, và các tham số phân phối lấy mẫu μ_{1:h}, σ_{1:h} được tái khớp với percentile hàng đầu của các quỹ đạo. CEM cho lập kế hoạch trong MBRL đã được sử dụng thành công trong một số phương pháp trước đó [20], [21], vì nó làm giảm việc khai thác bias mô hình so với các tối ưu hóa hoàn toàn dựa trên gradient [39] và có thể thích ứng tốt hơn với động lực học thay đổi so với các chính sách hoàn toàn khấu hao [22].

IV. PHƯƠNG PHÁP ĐỀ XUẤT

A. Thiết Lập Vấn Đề và Tổng Quan Phương Pháp

Chúng tôi xem xét thiết lập vấn đề sau: Một robot tương tác với môi trường để giải quyết một chuỗi T tác vụ hướng mục tiêu, mỗi tác vụ mang lại động lực học khác nhau trong khi có cùng không gian trạng thái S và không gian hành động A. Robot được tiếp xúc với các tác vụ tuần tự trực tuyến mà không xem lại dữ liệu đã thu thập trong tác vụ trước đó. Robot cũng có bộ nhớ hữu hạn và không được phép duy trì lịch sử đầy đủ của các chuyển đổi trạng thái cho mục đích huấn luyện lại. Vì phân phối của các tác vụ thay đổi theo thời gian và agent không biết về nó một cách tiên nghiệm, nó phải liên tục thích ứng với dữ liệu luồng quan sát mà nó gặp phải, trong khi cố gắng giải quyết từng tác vụ. Robot biết khi nào xảy ra chuyển đổi tác vụ.

Chúng tôi xem xét thiết lập giải pháp của MBRL với một mô hình động lực học đã học, các tham số của nó được suy luận thông qua một hypernetwork có điều kiện tác vụ. Với các nhúng tác vụ đã học e_t và tham số θ_t của hypernetwork H(·), chúng tôi suy ra tham số θ_t của mạng động lực học f_t(·). Sử dụng mô hình động lực học này, chúng tôi thực hiện tối ưu hóa CEM để tạo ra các chuỗi hành động và thực hiện chúng trong môi trường trong K bước thời gian với MPC. Chúng tôi lưu trữ các chuyển đổi quan sát được trong tập dữ liệu phát lại và cập nhật các tham số của hypernetwork θ_t và nhúng tác vụ e_t (tối ưu hóa off-policy). Chúng tôi lặp lại điều này cho M tập trong mỗi tác vụ, và cho mỗi tác vụ T tuần tự.

B. Quy Trình Huấn Luyện

Học Động Lực Học. Mô hình động lực học đã học là một mạng nơ-ron feedforward có tham số thay đổi qua các tác vụ. Một cách để học một mạng động lực học f_θ(·) qua các tác vụ là cập nhật nó tuần tự khi quá trình huấn luyện tiến triển. Tuy nhiên, vì thiết lập vấn đề của chúng tôi là agent không được phép giữ lại dữ liệu chuyển đổi trạng thái từ các tác vụ trước đó trong bộ đệm phát lại, việc thích ứng trọng số của một mạng duy nhất tuần tự qua các tác vụ có thể dẫn đến quên thảm khốc [1]. Để làm giảm các vấn đề của quên thảm khốc trong khi cố gắng thích ứng trọng số của mạng, chúng tôi học một hypernetwork nhận nhúng tác vụ làm đầu vào, và xuất ra tham số cho mạng động lực học tương ứng với mỗi tác vụ, học các mạng động lực học khác nhau f_t(·) cho mỗi tác vụ t.

Chúng tôi giả định rằng agent có bộ nhớ hữu hạn và không có quyền truy cập vào dữ liệu chuyển đổi trạng thái qua các tác vụ. Vì vậy, bộ đệm phát lại cụ thể cho tác vụ D_t được đặt lại ở đầu mỗi tác vụ t. Đối với tập hiện tại, agent tạo ra một mạng động lực học f_t sử dụng θ_t = H_t(e_t). Sau đó, với k = 1::: K bước thời gian và horizon lập kế hoạch h, agent tối ưu hóa các chuỗi hành động a_{k:k+h} sử dụng CEM, và thực hiện hành động đầu tiên a_k (MPC). D_t được tăng cường bởi một tuple (s_k, a_k, s_{k+1}), trong đó s_k là trạng thái hiện tại, a_k là hành động đã thực hiện, và s_{k+1} là trạng thái tiếp theo quan sát được dưới tác vụ t.

Các tham số θ_t của hypernetwork và nhúng tác vụ e_t được cập nhật bằng cách lan truyền ngược gradient với respect to tổng của mất mát động lực học L_dyn và một số hạng điều chuẩn hóa. Chúng tôi định nghĩa mất mát động lực học là L_dyn(θ_t, e_t) = ∑_{D_t} ||ŝ_{k+1} - s_{k+1}||^2, trong đó các trạng thái tiếp theo dự đoán là ŝ_{k+1} = f_t(s_k, a_k) và θ_t = H_t(e_t). Trong thực tế, chúng tôi suy luận sự khác biệt Δ_{k+1} thông qua mạng động lực học (Δ_{k+1} = f_t(s_k, a_k)) sao cho ŝ_{k+1} = s_k + Δ_{k+1} để huấn luyện ổn định. Ngoài ra, các đầu vào cho mạng f_t được chuẩn hóa, theo quy trình trong các công trình trước đó [20]. Tương tự, một e_t mới được khởi tạo ở đầu mỗi tác vụ và được cập nhật mỗi tập trong tác vụ bằng gradient descent. Các nhúng tác vụ cũ hơn (e_{1:t-1}) cũng có thể dễ dàng với gradient descent, nhưng chúng tôi giữ chúng cố định để đơn giản.

Điều Chuẩn Hóa Hypernetwork. Để làm giảm quên thảm khốc, chúng tôi điều chuẩn hóa đầu ra của hypernetwork cho tất cả các nhúng tác vụ trước đó e_{1:t-1}. Sau khi huấn luyện cho tác vụ t-1, một snapshot của trọng số hypernetwork được lưu như θ_{t-1}. Đối với mỗi tác vụ quá khứ i = 1::: t-1, chúng tôi sử dụng mất mát điều chuẩn hóa để giữ các đầu ra của snapshot H_{t-1}(e_i) gần với đầu ra hiện tại H_t(e_i). Phương pháp này bỏ qua nhu cầu lưu trữ tất cả dữ liệu quá khứ qua các tác vụ, bảo tồn hiệu suất dự đoán của các mạng động lực học f_t, và chỉ yêu cầu một điểm duy nhất trong không gian trọng số (một bản sao của hypernetwork) được lưu trữ. Các nhúng tác vụ là các vector khả vi được học cùng với các tham số của hypernetwork. Hàm mất mát tổng thể để cập nhật θ_t và e_t được đưa ra bởi tổng của mất mát động lực học L_dyn(·), được đánh giá trên dữ liệu thu thập từ tác vụ t và số hạng điều chuẩn hóa L_reg(·):

L_t(θ_t, e_t) = L_dyn(θ_t, e_t) + L_reg(θ_{t-1}, θ_t, e_{1:t-1})

L_reg(·) = λ_reg ∑_{i=1}^{t-1} ||H_{t-1}(e_i) - H_t(e_i)||_2^2 (1)

Mục tiêu lập kế hoạch cho tối ưu hóa CEM của các chuỗi hành động được đưa ra bởi tổng phần thưởng thu được bằng cách thực hiện chuỗi hành động a_{k:k+h} dưới mô hình động lực học đã học f_t(·) cho tác vụ. Hàm phần thưởng r(s,a) được giả định là đã biết, nhưng không có gì ngăn cản việc học nó từ dữ liệu dưới framework hiện tại của chúng tôi.

V. THỰC NGHIỆM

Chúng tôi thực hiện nhiều thí nghiệm mô phỏng robot để trả lời các câu hỏi sau: (a) HyperCRL so sánh như thế nào với các đường cơ sở học liên tục hiện có về hiệu suất tổng thể qua các tác vụ? (b) HyperCRL hiệu quả như thế nào trong việc ngăn chặn quên thảm khốc qua các tác vụ?

A. Đẩy một khối có khối lượng không đồng nhất

Đầu tiên, chúng tôi xem xét một thí nghiệm đơn giản một cách trực quan được mô phỏng bằng Surreal Robotics Suite [40], trong đó một robot Panda cố gắng đẩy một khối có mật độ không đồng nhất qua bàn (Hình 2). Mục tiêu là đẩy khối đến vị trí mục tiêu trong khi duy trì hướng ban đầu của nó. Chúng tôi thay đổi mật độ của phần trái và phải của khối qua các tác vụ khác nhau (T = 5), thay đổi tâm khối lượng và moment quán tính. Robot cần học điều khiển vị trí của đầu tác động của nó ở bên cạnh khối để sửa chữa các độ lệch hướng trong khi đẩy khối về phía trước. Mỗi tập dài 100 bước, tức là 10 giây thời gian mô phỏng. Ở đầu mỗi tập, chúng tôi khởi tạo đầu tác động robot đến một vị trí cố định phía sau khối.

Trạng thái được biểu diễn như một vector được nối (x_{ee}, x_1, x_2, x_3, x_4). Ở đây (x_1, x_2, x_3, x_4) biểu diễn vị trí xy của bốn góc và x_{ee} là vị trí xy của đầu tác động với respect to khối. Vector hành động đầu vào, Δx_{ee}, chỉ định một offset mong muốn với respect to vị trí hiện tại x_{ee}. Robot được điều khiển bằng một bộ điều khiển không gian hoạt động [41]. Nó tính toán các moment khớp đã cho một hành động được cập nhật ở 10Hz. Hàm phần thưởng được đặt để giảm thiểu tổng khoảng cách giữa pose hiện tại và mục tiêu của khối theo r(s,a) = ∑_{i=1}^4 (1 - tanh(10||x_i - g_i||)) - 0.25||a||, trong đó g_i là vị trí mục tiêu của góc thứ i. Hàm phần thưởng bao gồm một số hạng cho mỗi góc trong phạm vi [0, 1], và để tối đa hóa phần thưởng, robot nên giảm thiểu khoảng cách góc đến pose mục tiêu.

B. Mở Cửa

Tiếp theo, chúng tôi thí nghiệm trên một tác vụ phức tạp hơn để chứng minh tốt hơn tính linh hoạt của HyperCRL. Chúng tôi chọn một thí nghiệm mở cửa, nơi robot Panda phải mở cửa với các loại tay nắm khác nhau (Hình 3). Không giống như ví dụ đẩy, các chuyển đổi động giữa các tác vụ không liên tục hơn nhiều và các tác vụ yêu cầu các chuyển động khác nhau để giải quyết, do các khớp quay được áp đặt bởi một số tay nắm. Tổng cộng T = 5 tác vụ cần được giải quyết tuần tự, mỗi tác vụ liên quan đến một loại tay nắm khác nhau. Tác vụ 2 và 4 (tương tự tác vụ 3 và 5) đều có tay nắm tròn (đòn bẩy), nhưng với hướng xoay khác nhau. Môi trường được sửa đổi từ môi trường DoorGym [42] và được mô phỏng trong Surreal Robotics Suite [43].

Chúng tôi mô hình hóa môi trường như sau: (i) d biểu diễn góc của cửa (ii) k biểu diễn góc của tay nắm cửa (iii) x ∈ R^3 là vị trí, q ∈ R^4 là biểu diễn quaternion của phép quay của đầu tác động robot với respect to tay nắm (iv) d ∈ R là trạng thái của gripper (v) q_j ∈ R^7 là góc của các khớp của cánh tay Panda. Nối tất cả các phần tử trên và đạo hàm thời gian của chúng cho vector trạng thái đầy đủ (d, ḋ, k, k̇, x, q, d, q_j, q̇_j) ∈ R^26. Tương tự như môi trường đẩy, robot được điều khiển với điều khiển không gian hoạt động được cập nhật ở 10Hz. Hành động đầu vào là một vector (Δx, Δq, Δd), chỉ định một chuyển động tịnh tiến và quay của đầu tác động trong khung thế giới. Cuối cùng, hàm phần thưởng có năm thành phần như sau: r(s,a) = -||x||^2 - log(||x||^2 + ε) - ||q_o||^2 + 50d + 20k, trong đó q_o là sự khác biệt giữa hướng của đầu tác động hiện tại và pose yêu cầu để mở cửa.

C. Trượt Khối

Thí nghiệm cuối cùng của chúng tôi liên quan đến một robot Panda và hai khối riêng biệt được đặt trên mặt bàn với ma sát thấp. Pose ban đầu của đầu tác động, hai khối được đặt dọc theo một đường thẳng và chúng tôi đánh số hai khối được hiển thị trong Hình 5. Để di chuyển khối 2 đến pose mục tiêu của nó, manipulator trước tiên nên đánh khối 1, khối này sẽ trượt đi và lần lượt đặt khối thứ hai vào chuyển động trượt cho đến khi dừng lại bởi ma sát. Chúng tôi thay đổi ma sát của khối 2 qua các tác vụ khác nhau (T = 5), trong khi giữ ma sát của khối 1 giống nhau. Tương tự như thí nghiệm đẩy, chúng tôi biểu diễn trạng thái như một vector nối (x_{ee}, x^1_{1:4}, x^2_{1:4}), trong đó x_{ee} biểu thị vị trí xy của đầu tác động, và (x^1_{1:4}, x^2_{1:4}) biểu thị vị trí xy của các góc của cả hai khối. Hàm phần thưởng giảm thiểu khoảng cách giữa pose hiện tại và mục tiêu của khối thứ hai g_{1:4} như sau: r(s,a) = ∑_{i=1}^4 (1 - tanh(10||x^2_i - g_i||)) - 0.1||a||. Robot phải điều chỉnh động lượng của khối đầu tiên trong cú đẩy ban đầu cho các ma sát khác nhau để thành công giao khối thứ hai đến mục tiêu của nó. Mỗi tập dài 30 bước và tương đương 3 giây trong mô phỏng.

D. Đường Cơ Sở

Chúng tôi so sánh hypernetwork với các đường cơ sở sau: (i) Học đa tác vụ với quyền truy cập vào bộ đệm của tất cả dữ liệu từ tất cả các tác vụ trước đó (một oracle) (ii) Coreset nhớ một phần trăm dữ liệu chuyển đổi trạng thái-hành động mỗi tác vụ, được lấy mẫu ngẫu nhiên (iii) Synaptic Intelligence [10] (iv) Elastic Weight Consolidation [7] (v) Fine-tuning, nơi chúng tôi tối ưu hóa f_t(·) trên dữ liệu mỗi tác vụ mà không có điều chuẩn hóa. Tất cả các mô hình đường cơ sở tương tự mô hình mục tiêu trong thiết lập hypernetwork của chúng tôi, ngoại trừ lớp đầu ra đa đầu với một đầu mỗi tác vụ. Đối với coreset hoặc học đa tác vụ, một batch bổ sung dữ liệu quá khứ được lấy mẫu từ coreset hoặc oracle mỗi bước cập nhật, và đóng góp vào tổng mất mát động lực học.

E. Kết Quả

Pusher HyperCRL có thể học đẩy tất cả 5 khối qua bàn với việc quên tối thiểu (Bảng I), và thậm chí còn cho thấy dấu hiệu của chuyển giao ngược tích cực. Hiệu suất trung bình của phương pháp chúng tôi ngang bằng với đường cơ sở học đa tác vụ (Hình 4), có quyền truy cập vào toàn bộ lịch sử dữ liệu. HyperCRL cũng vượt trội hơn các đường cơ sở học liên tục khác, hoặc dựa trên điều chuẩn hóa (SI, EWC), hoặc dựa trên phát lại (Coreset). Tuy nhiên, fine-tuning đơn giản quên thảm khốc và không thể thực hiện đẩy cho tất cả 5 loại khối ở cuối.

Door HyperCRL vượt trội hơn tất cả các đường cơ sở học liên tục trên tác vụ mở cửa, và đường cơ sở đa tác vụ. Hình 6 hiển thị các đường cong học của tất cả các phương pháp được đánh giá của chúng tôi, so với đường cơ sở tác vụ đơn được huấn luyện từ đầu trên mỗi tác vụ. HyperCRL hầu như không thấy sự suy giảm hiệu suất về phần thưởng (Bảng I).

Block Sliding HyperCRL vượt trội hơn tất cả các đường cơ sở học liên tục mặc dù hiệu suất của nó thấp hơn đường cơ sở đa tác vụ. Trong Bảng I, HyperCRL trải qua một mức độ quên nhẹ trung bình. Đường cong học bị bỏ qua ở đây do hạn chế không gian và có sẵn trên trang web dự án của chúng tôi.

F. Chi Tiết Huấn Luyện

Đối với tất cả các thí nghiệm, chúng tôi mô hình hóa mạng mục tiêu như một perceptron đa lớp (MLP) với hai lớp ẩn (bốn cho mở cửa) của 200 nơ-ron mỗi lớp và phi tuyến ReLU. Mỗi nhúng tác vụ được khởi tạo như một vector chuẩn 10 chiều. Hypernetwork cũng là một MLP với hai lớp ẩn của 50 nơ-ron mỗi lớp (256 nơ-ron cho mở cửa). Các tham số của hypernetwork được khởi tạo với khởi tạo Xavier [44]. Chúng tôi sử dụng Adam với tốc độ học 0.0001 để tối ưu hóa L_t. Trong quá trình lập kế hoạch, chúng tôi chạy CEM cho 5 lần lặp để tối ưu hóa các hành động cho một horizon của h = 20 (10 cho mở cửa) bước. Mỗi lần lặp, chúng tôi lấy mẫu 500 (2000 cho mở cửa) chuỗi hành động để tối đa hóa tổng phần thưởng. Đối với SI và EWC, chúng tôi sử dụng các triển khai từ [3].

VI. HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI

Mặc dù phương pháp đề xuất của chúng tôi cho thấy kết quả tốt so với các đường cơ sở chúng tôi đã kiểm tra, có nhiều triển vọng thú vị cho công việc tương lai. Thứ nhất, các kỹ thuật tốt hơn để huấn luyện hypernetwork sẽ hỗ trợ đáng kể hướng công việc này. Hiện tại, kích thước của hypernetwork của chúng tôi ít nhất lớn hơn một bậc độ lớn so với mạng mục tiêu, vì nó trực tiếp xuất ra tất cả các trọng số. Hypernetwork thường nhạy cảm với việc lựa chọn seeds ngẫu nhiên và kiến trúc. Thứ hai, việc mở rộng phương pháp của chúng tôi đến các môi trường RL dựa trên hình ảnh đáng để điều tra, vì nó sẽ cho phép các mạng mục tiêu dung lượng cao hơn. Cuối cùng, HyperCRL không bất khả tri tác vụ, cũng không thể tự động phát hiện chuyển đổi tác vụ xảy ra liên tục mà không có ranh giới tác vụ rõ ràng. Một hướng có thể là sử dụng các mô hình suy luận xác suất (tức là Bayesian phi tham số [2]) hoặc các phương pháp phát hiện điểm thay đổi để thực hiện nhận dạng tác vụ.

VII. KẾT LUẬN

Trong bài báo này, chúng tôi đã mô tả HyperCRL, một phương pháp có nhận thức tác vụ cho học tăng cường dựa trên mô hình liên tục sử dụng hypernetwork. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi đã chứng minh rằng HyperCRL liên tục vượt trội hơn các đường cơ sở học liên tục thay thế về hiệu suất tổng thể ở cuối huấn luyện, cũng như về việc duy trì hiệu suất trên các tác vụ trước đó. Bằng cách cho phép toàn bộ mạng thay đổi giữa các tác vụ, thay vì chỉ lớp đầu ra head của mạng động lực học, HyperCRL hiệu quả hơn trong việc biểu diễn chính xác các động lực học khác nhau, ngay cả với chuyển đổi tác vụ đáng kể, trong khi chỉ yêu cầu một hypernetwork kích thước cố định và không có dữ liệu huấn luyện từ các tác vụ cũ để tạo ra động lực học có điều kiện tác vụ cho lập kế hoạch suốt đời.
