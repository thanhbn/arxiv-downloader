# HyperMixer: Một Giải pháp Thay thế Chi phí Thấp dựa trên MLP cho Transformers

Florian Mai†♠Arnaud Pannatier†♠Fabio Fehr†♠Haolin Chen†♠
François Marelli†♠François Fleuret♣♠†James Henderson†
†Viện Nghiên cứu Idiap, Martigny, Thụy Sĩ
♠EPFL, Lausanne, Thụy Sĩ
♣Đại học Geneva, Geneva, Thụy Sĩ

## Tóm tắt

Các kiến trúc dựa trên Transformer là lựa chọn mô hình cho việc hiểu ngôn ngữ tự nhiên, nhưng chúng có chi phí đáng kể vì có độ phức tạp bậc hai theo độ dài đầu vào, yêu cầu nhiều dữ liệu huấn luyện và có thể khó điều chỉnh. Trong việc theo đuổi chi phí thấp hơn, chúng tôi nghiên cứu các kiến trúc MLP đơn giản. Chúng tôi thấy rằng các kiến trúc hiện có như MLPMixer, đạt được token mixing thông qua MLP tĩnh được áp dụng độc lập cho từng đặc trưng, quá tách biệt khỏi các inductive bias cần thiết cho việc hiểu ngôn ngữ tự nhiên. Trong bài báo này, chúng tôi đề xuất một biến thể đơn giản, HyperMixer, tạo ra token mixing MLP một cách động bằng cách sử dụng hypernetworks. Về mặt thực nghiệm, chúng tôi chứng minh rằng mô hình của chúng tôi hoạt động tốt hơn các mô hình dựa trên MLP thay thế và ngang bằng với Transformers. Trái ngược với Transformers, HyperMixer đạt được những kết quả này với chi phí thấp hơn đáng kể về thời gian xử lý, dữ liệu huấn luyện và điều chỉnh siêu tham số.

## 1 Giới thiệu

Các kiến trúc dựa trên attention như Transformer (Vaswani et al., 2017) đã thúc đẩy tiến bộ trong nhiều tác vụ hiểu ngôn ngữ tự nhiên. Một phần thành công của chúng là kết quả của lược đồ huấn luyện có thể song song hóa theo độ dài đầu vào. Điều này cải thiện thời gian huấn luyện và cho phép khối lượng dữ liệu lớn hơn làm cho các mô hình này phù hợp với pretraining (Radford et al., 2018; Devlin et al., 2019). Do đó, nhiều mô hình hiện đại tiên tiến nhất là các phần mở rộng được fine-tuned của các Transformers được pretrained lớn (Bommasani et al., 2021).

Tuy nhiên, những mô hình này có chi phí tính toán đáng kể. Chúng yêu cầu tài nguyên đáng kể để pretraining và fine-tuning, gây ra tiêu thụ năng lượng cao (Strubell et al., 2019) và hạn chế quyền truy cập vào nghiên cứu (Bommasani et al., 2021). Tiếp theo, Schwartz et al. (2020) lập luận nhu cầu về "Green AI". Họ đề xuất đánh giá chi phí của kết quả R như sau:

Cost (R) ∝ E·D·H,

trong đó E là chi phí tính toán được đo bằng số phép toán dấu phẩy động (FPO) của một ví dụ đơn, D là kích thước tập dữ liệu, và H là số cấu hình siêu tham số cần thiết trong quá trình điều chỉnh.

Để đạt được giảm chi phí, bài báo này đề xuất một giải pháp thay thế đơn giản hơn cho Transformers. Chúng tôi lấy cảm hứng từ cộng đồng thị giác máy tính, gần đây đã thấy sự bùng nổ nghiên cứu về Multi-Layer Perceptrons (MLPs). Nổi bật nhất là MLPMixer (Tolstikhin et al., 2021), một kiến trúc đơn giản dựa trên hai MLPs: một cho token mixing và một cho feature mixing. Tuy nhiên, token mixing MLP học một tập hợp các ánh xạ cụ thể theo vị trí có kích thước cố định, có thể khiến kiến trúc của MLPMixer quá tách biệt khỏi các inductive bias cần thiết cho việc hiểu ngôn ngữ tự nhiên, trái ngược với Transformers (Henderson, 2020).

Trong bài báo này, chúng tôi đề xuất một biến thể đơn giản, HyperMixer (Hình 1), tạo ra token mixing MLP một cách động bằng cách sử dụng hypernetworks (Ha et al., 2016). Biến thể này phù hợp hơn vì nó học cách tạo ra một tập hợp các ánh xạ có kích thước biến đổi theo cách bất biến vị trí, tương tự như cơ chế attention trong Transformers (Vaswani et al., 2017).

Trái ngược với độ phức tạp bậc hai của Transformer, độ phức tạp của HyperMixer tuyến tính theo độ dài đầu vào. Điều này làm cho nó trở thành một giải pháp thay thế cạnh tranh cho việc huấn luyện trên các đầu vào dài hơn.

Về mặt thực nghiệm, chúng tôi chứng minh rằng HyperMixer hoạt động tốt hơn đáng kể trong các tác vụ hiểu ngôn ngữ tự nhiên so với MLPMixer ban đầu và các giải pháp thay thế dựa trên MLP liên quan. So với Transformers, HyperMixer đạt được kết quả cạnh tranh hoặc cải thiện với chi phí thấp hơn đáng kể Cost (R) ∝ E·D·H: tốc độ suy luận được cải thiện (E), đặc biệt cho các đầu vào dài; hiệu suất thuận lợi trong chế độ tài nguyên thấp (D); và điều chỉnh hiệu quả cho các siêu tham số (H). Chúng tôi quy thành công của HyperMixer cho khả năng xấp xỉ một hàm giống attention. Các thí nghiệm tiếp theo trên tác vụ tổng hợp chứng minh rằng HyperMixer thực sự học cách chú ý đến các token theo mẫu tương tự như cơ chế attention.

Tóm lại, các đóng góp của chúng tôi có thể được liệt kê như sau:

1. Một mô hình all-MLP mới, HyperMixer, với inductive bias tương tự như Transformers. (Phần: 2)
2. Phân tích hiệu suất của HyperMixer so với các phương pháp token mixing thay thế dựa trên các thí nghiệm có kiểm soát trên benchmark GLUE. (Phần: 4.3)
3. So sánh toàn diện về chi phí Cost (R) của HyperMixer và Transformers. (Phần: 4.4, 4.5, 4.6)
4. Ablation chứng minh rằng HyperMixer học các mẫu attention tương tự như Transformers. (Phần: 4.7)

## 2 Phương pháp

### 2.1 Inductive Biases trong các Mô hình NLP

Trong machine learning, inductive biases của một mô hình phản ánh các giả định mô hình hóa ngầm là chìa khóa để hỗ trợ học tập và cải thiện khả năng tổng quát hóa trên các tác vụ cụ thể. Trong NLP, các mô hình nổi tiếng có inductive biases mạnh bao gồm: mạng neural hồi quy (Elman, 1990), giả định đầu vào là một chuỗi; và mạng neural đệ quy (Socher et al., 2013), giả định cấu trúc cây. Mặc dù cả hai inductive biases này đều hợp lý, về mặt thực nghiệm, Transformers đã thành công hơn trong những năm gần đây. Hơn nữa, chúng tôi nhắc lại các lập luận của Henderson (2020) về inductive biases trong ngôn ngữ và áp dụng chúng vào thiết kế mô hình của chúng tôi. Henderson (2020) quy thành công của Transformer cho hai khái niệm: variable binding và systematicity. Variable binding đề cập đến khả năng của mô hình trong việc biểu diễn nhiều thực thể cùng lúc. Điều này có thể là thách thức trong các biểu diễn vector đơn như mạng neural hồi quy. Tuy nhiên, Transformers biểu diễn mỗi token bằng vector riêng của nó, điều này tính đến variable binding vì mỗi token có thể được giải thích như một thực thể. Systematicity đề cập đến khả năng của mô hình học các quy tắc có thể tổng quát hóa phản ánh mối quan hệ cấu trúc giữa các thực thể (Fodor và Pylyshyn, 1988). Transformers đạt được systematicity thông qua cơ chế attention, một tập hợp các hàm có thể học được xác định tương tác giữa các thực thể bằng cách ghép các biểu diễn query với các biểu diễn key (như được thể hiện trong Hình 1). Cơ chế này điều chỉnh, cho mọi vị trí trong chuỗi, cách xử lý chức năng bất kỳ vị trí nào khác. Hơn nữa, các tham số hàm này có thể học được và được chia sẻ giữa tất cả các thực thể.

### 2.2 MLPMixer

Một lớp tổng quát của MLPMixer được thể hiện trong Hình 1. Tương tự như Transformers, mỗi token được biểu diễn như một vector của các đặc trưng, trải qua các biến đổi (phi tuyến) trong nhiều lớp. MLPMixer sử dụng hai MLPs ở mỗi lớp, một cho feature mixing và một cho token mixing. Thành phần feature mixing được áp dụng độc lập cho mỗi token vector, mô hình hóa các tương tác giữa các đặc trưng. Token Mixing MLP (TM-MLP) được áp dụng độc lập cho mỗi đặc trưng (tức là vector các giá trị của nó trên các token), mô hình hóa các tương tác giữa các vị trí không gian hoặc vị trí. Điều này có thể được giải thích như một cơ chế attention toàn cục tĩnh và được điều chỉnh theo vị trí. Thực tế, điều này đạt được bằng cách hoán vị chiều biểu diễn các đặc trưng và chiều biểu diễn các vị trí. Mỗi vector x^T_i ∈ R^N, biểu diễn đặc trưng i ≤ d, của một số đầu vào có độ dài cố định N, được đưa vào TM-MLP, có dạng sau:

TM-MLP(x^T_i) = W_1(σ(W^T_2 x^T_i)), (1)

trong đó W_1, W_2 ∈ R^{N×d'}, và σ biểu thị tính phi tuyến GELU (Hendrycks và Gimpel, 2016). Cuối cùng, để hỗ trợ học tập, layer normalization (Ba et al., 2016) và skip connections (He et al., 2016) được thêm vào xung quanh mỗi MLP, tương ứng. Cách sắp xếp tốt nhất các thành phần này vẫn là một câu hỏi mở (Wang et al., 2019; Bachlechner et al., 2021). Chúng tôi thí nghiệm với các biến thể khác nhau trong Phụ lục F.

**Cân nhắc cho NLP** Token mixing MLP giả định đầu vào có chiều cố định, điều này cần thiết vì các tham số cần được chia sẻ giữa tất cả các ví dụ. Tuy nhiên, không giống như hình ảnh, đầu vào văn bản thường có chiều biến đổi. Do đó, để áp dụng MLPMixer cho văn bản có độ dài biến đổi, một cách tiếp cận đơn giản là giả định độ dài tối đa (ví dụ: tối đa trong tập dữ liệu). Sau đó, tất cả đầu vào được đệm đến độ dài tối đa và masks được áp dụng trong token mixing MLP. Mô hình này có thể thực hiện variable binding, vì mỗi token được biểu diễn bằng vector riêng của nó. Tuy nhiên, mô hình này thiếu systematicity vì các quy tắc được học để mô hình hóa tương tác giữa các token (tức là trọng số của MLP) không được chia sẻ giữa các vị trí.

### 2.3 HyperMixer

**Thuật toán 1 HyperMixer pseudo-code**

```python
class HyperMixing(nn.Module):
    def __init__(self, d, d'):
        # learnable parameters
        self.hypernetwork_in = MLP([d, d, d'])
        self.hypernetwork_out = MLP([d, d, d'])
        # layer normalization improves training stability
        self.layer_norm = LayerNorm(d)
    
    def forward(self, queries, keys, values):
        # queries: [B, M, d]
        # keys / values: [B, N, d]
        # add token information (e.g. position embeddings)
        hyp_in = add_token_information(keys)
        hyp_out = add_token_information(queries)
        
        W1 = self.hypernetwork_in(hyp_in) # [B, N, d']
        W2 = self.hypernetwork_out(hyp_out) # [B, M, d']
        
        # TM-MLP(x) = W_2 ( GELU ( W_1^T x) )
        # maps [B, d, N] -> [B, d, M]
        token_mixing_mlp = compose_TM_MLP(W1, W2)
        
        # transpose so MLP is applied to sequence dimension
        values = values.transpose(1, 2) # [B, d, N]
        output = token_mixing_mlp(values) # [B, d, M]
        
        # transpose back
        output = output.transpose(1,2) # [B, M, d]
        
        # optionally apply LayerNorm
        return self.layer_norm(output)
```

HyperMixer bao gồm systematicity vào kiến trúc MLPMixer bằng cách giới thiệu một cơ chế token mixing mới, HyperMixing, có thể được coi như một sự thay thế drop-in cho attention. Để dễ hiểu, chúng tôi cung cấp pseudo-code trong Thuật toán 1. Mặc dù queries, keys và values trong HyperMixing không nhất thiết phải giống nhau, chúng tôi sẽ giả định chúng giống hệt nhau trong công thức sau. HyperMixing dựa vào việc sử dụng hypernetworks, được sử dụng để tạo ra các trọng số W_1, W_2 của TM-MLP (Phương trình 1) một cách động như một hàm của đầu vào. Gọi x_j ∈ R^d, j ≤ N, trong đó N là chiều (biến đổi) của đầu vào, biểu diễn token j (tức là query, key và value). W_1 và W_2 được tạo ra bởi các hàm tham số h_1, h_2: R^{N×d} → R^{N×d'}. Về mặt lý thuyết, h_1 và h_2 có thể là bất kỳ hàm nào, bao gồm các mạng tinh vi xem xét tương tác phi tuyến giữa các token, như cơ chế attention. Tuy nhiên, điều này sẽ làm mất mục đích của mô hình chúng tôi, đó là sự đơn giản. Do đó, chúng tôi chọn tạo ra các hàng của ma trận trọng số từ mỗi token một cách độc lập thông qua MLP khác. Cụ thể, một hàm hypernetwork có thể được định nghĩa là

h_i(x) = [
    MLP_{W_i}(x_1 + p_1)
    ...
    MLP_{W_i}(x_N + p_N)
] ∈ R^{N×d'},

trong đó MLP_{W_1}, MLP_{W_2}: R^d → R^{d'} chính là các multi-layer perceptrons với tính phi tuyến GELU. p_j ∈ R^d là một vector có thể mã hóa thông tin bổ sung như vị trí thông qua absolute position embeddings (Vaswani et al., 2017).

Trực quan, cho mỗi token x_j, h_1 quyết định thông tin nào sẽ gửi đến lớp ẩn của TM-MLP, nơi thông tin từ tất cả các token được trộn lẫn, và h_2 quyết định cho mỗi token cách trích xuất thông tin từ lớp ẩn. Lưu ý rằng, mặc dù h_1 và h_2 chỉ xem xét một token tại một thời điểm, tương tác phi tuyến giữa các token vẫn được mô hình hóa thông qua lớp ẩn của TM-MLP.

Cuối cùng, layer normalization (Ba et al., 2016) có thể được áp dụng cho đầu ra của TM-MLP. Chúng tôi thấy điều này hữu ích để hỗ trợ huấn luyện với nhiều loại bố cục Transformer khác nhau (Phụ lục F).

**Ràng buộc h_1 và h_2** Để giảm số lượng tham số và phép toán trong mô hình, và do đó giảm độ phức tạp, chúng tôi thấy việc ràng buộc h_1 và h_2 bằng cách đặt W_2 = W_1 là hữu ích.

**Cân nhắc cho NLP** So với MLPMixer được định nghĩa trong Phần 2.2, việc sử dụng hypernetworks khắc phục hai thách thức. Thứ nhất, đầu vào không còn phải có chiều cố định. Hypernetwork tạo ra một token mixing MLP có chiều phù hợp như một hàm của đầu vào. Thứ hai, hypernetwork mô hình hóa tương tác giữa các token với trọng số được chia sẻ giữa tất cả các vị trí trong đầu vào. Do đó, systematicity được đảm bảo.

## 3 Công trình Liên quan

Nghiên cứu về các mô hình all-MLP như MLPMixer (Tolstikhin et al., 2021) rất phổ biến trong cộng đồng thị giác máy tính (Tu et al., 2022; Yu et al., 2022; Wang et al., 2022, cùng nhiều nghiên cứu khác). Tuy nhiên, chúng thiếu một số inductive biases mong muốn cho NLP, mà chúng tôi thảo luận chi tiết trong Phụ lục A.2. Cụ thể, trái ngược với HyperMixer, không có phương pháp nào được đề xuất trước đây đồng thời cung cấp i) bất biến vị trí (position invariance), quan trọng cho khả năng tổng quát hóa, ii) kích thước thích ứng cho đầu vào có độ dài biến đổi, iii) trường tiếp nhận toàn cục (global receptive field), cho phép tương tác không bị giới hạn trong các lân cận token nhỏ, iv) khả năng học (learnability) cho phép áp dụng toàn cục cho các tác vụ khác nhau, và v) tính động (dynamicity), có nghĩa là token mixing là một hàm của đầu vào. Do đó, chỉ có một số công trình sử dụng các mô hình dựa trên MLP làm backbone trong các tác vụ NLP. gMLP (Liu et al., 2021) đóng vai trò là một trong những baseline của chúng tôi và pnlp-mixer (Fusco et al., 2022) sử dụng MLPMixer tiêu chuẩn trên phương pháp token embedding mới.

Ngoài các mô hình all-MLP, có rất nhiều nghiên cứu về các giải pháp thay thế hiệu quả cho các lớp attention tiêu chuẩn (Katharopoulos et al., 2020; Bello, 2021, v.v.). Mặc dù chúng không đủ điều kiện là các mô hình all-MLP, chúng có mối liên hệ chặt chẽ với công trình của chúng tôi (xem Phụ lục E) và nhằm mục đích giảm chi phí của AI, mặc dù trên ít chiều hơn so với công trình của chúng tôi (Phụ lục A.1). Chúng tôi sử dụng FNet (Lee-Thorp et al., 2021) và Linear Transformers (Katharopoulos et al., 2020) làm đại diện cho những mô hình này như baseline.

## 4 Thí nghiệm

Các thí nghiệm của chúng tôi được thiết kế để kiểm tra ba giả thuyết sau. H1 (Phần 4.3): Vì HyperMixer phản ánh nhiều inductive biases phù hợp cho NLP hơn, giả thuyết của chúng tôi là HyperMixer hoạt động tốt hơn trong các tác vụ NLP so với MLPMixer và các giải pháp thay thế dựa trên MLP tương tự, đặc biệt là những tác vụ yêu cầu mô hình hóa tương tác giữa các token. H2: Vì HyperMixer có inductive biases tương tự như transformers nhưng đơn giản hơn đáng kể về mặt khái niệm và độ phức tạp tính toán, nó có thể được coi là giải pháp thay thế chi phí thấp cho Transformers, giảm chi phí về thời gian xử lý ví dụ đơn (Phần 4.4), kích thước tập dữ liệu yêu cầu (Phần 4.5), và điều chỉnh siêu tham số (Phần 4.6). H3 (Phần 4.7): Do inductive biases của nó phản ánh những của Transformers, HyperMixer cũng học các mẫu tương tự như cơ chế attention.

### 4.1 Tập dữ liệu

Chúng tôi đánh giá trên bốn tác vụ phân loại cặp câu và một tác vụ phân loại câu đơn. Các tác vụ cặp câu là QQP (Iyer et al., 2017), QNLI (Rajpurkar et al., 2016), MNLI (Williams et al., 2018) và SNLI (Bowman et al., 2015). Để đồng nhất, các tập dữ liệu được định dạng như trong benchmark GLUE (Wang et al., 2018). Chúng tôi chọn các tác vụ này vì hai thuộc tính: thứ nhất, chúng có tập dữ liệu huấn luyện lớn (Bảng 2, phụ lục) cho phép hiệu suất hợp lý mà không cần pretraining; thứ hai, việc giải quyết các tác vụ này đòi hỏi mô hình hóa tốt về tương tác giữa các token từ các câu khác nhau, đây là trọng tâm chính của bài báo này.

Như một kiểm soát, chúng tôi thí nghiệm trên tập dữ liệu đầu vào đơn SST2 (Socher et al., 2013), là tác vụ phân loại cảm xúc. Nhiều ví dụ trong tập dữ liệu này có thể được giải quyết bằng cách xác định các từ cảm xúc chính, thay vì mô hình hóa tương tác token.

### 4.2 Baselines

Các baseline sau có thể được phân loại thành dựa trên MLP (để hỗ trợ H1) và không dựa trên MLP (ví dụ: Transformers, để hỗ trợ H2). Lưu ý rằng nghiên cứu của chúng tôi về thiết kế module token mixing. Do đó, chúng tôi chỉ so sánh với các mô hình phù hợp với framework tổng quát được hiển thị trong Hình 1, nơi có module feature mixing và module token mixing cho đầu vào văn bản. Kết quả là, các mô hình như RNNs bị loại trừ. Để cho phép thí nghiệm có kiểm soát, chúng tôi sử dụng cùng module feature mixing trong tất cả các mô hình; các mô hình chỉ khác nhau ở module token mixing của chúng.

**Dựa trên MLP** Baseline gần nhất về mặt khái niệm là MLPMixer (Tolstikhin et al., 2021), kết hợp cả token và feature mixing sử dụng MLPs có chiều cố định, như mô tả trong Phần 2.2. Đồng thời, (Liu et al., 2021) đề xuất gMLP, trong đó token mixing đạt được thông qua tổng có trọng số của tất cả các đầu vào khác, tương tự như cơ chế attention. Tuy nhiên, thay vì tính toán trọng số như hàm của đầu vào như trong attention, trong gMLP trọng số là các tham số có thể học cố định. Ngoài ra, linear gating được khởi tạo gần một được giới thiệu để hỗ trợ huấn luyện. Phương pháp gMLP ban đầu không sử dụng các module feature mixing, vì module token mixing của chúng có khả năng mô hình hóa tương tác đặc trưng cũng như trong một gMLP block duy nhất. Tuy nhiên, để có thể so sánh, chúng tôi chèn các gMLP blocks làm module token mixing trong kiến trúc tổng quát của chúng tôi và cũng giữ các module feature mixing.

**Không dựa trên MLP** Transformers (Vaswani et al., 2017) được sử dụng trong các nghiên cứu hiện đại tiên tiến nhất trong hầu như tất cả các tác vụ NLP. Thành phần chính của chúng là module self-attention dựa trên softmax, mà chúng tôi sử dụng cho token mixing.

Linear Transformer (Katharopoulos et al., 2020) thay thế softmax attention bằng feature-map based dot-product attention. Cuối cùng, FNet (Yu et al., 2021) thay thế phần self-attention của Transformers bằng một tập hợp các biến đổi Fourier cố định, không thể học được cho token mixing.

### 4.3 Hiệu suất

Ban đầu chúng tôi so sánh hiệu suất của HyperMixer so với các baseline của chúng tôi. Sau đó, chúng tôi khám phá thêm về lợi ích của mô hình liên quan đến chi phí của nó.

Để có thể so sánh, chúng tôi điều chỉnh kích thước của các thành phần token mixing sao cho tất cả các mô hình có cùng số lượng tham số (11M). FNet là ngoại lệ vì nó không có tham số có thể học trong thành phần token mixing. Chúng tôi điều chỉnh learning rate của mỗi mô hình thông qua grid-search và báo cáo hiệu suất của cấu hình tốt nhất. Chi tiết thí nghiệm thêm về tất cả các thí nghiệm có thể tìm thấy trong Phụ lục B.

**Kết quả** Kết quả tập validation và test được thể hiện trong Bảng 1. Trên tập test và validation, HyperMixer hoạt động tốt nhất trong các mô hình dựa trên MLP trên tất cả các tập dữ liệu, mặc dù cho SST sự khác biệt trên tập validation nhỏ hơn một độ lệch chuẩn. MLPMixer nói chung đạt hiệu suất tốt, vượt trội Transformers trên hai tập dữ liệu.

So sánh với các phương pháp không dựa trên MLP, HyperMixer cũng vượt trội vanilla Transformers trên tất cả các tập dữ liệu. Sự khác biệt nói chung nhỏ (≤2 điểm), ngoại trừ trên QNLI, nơi sự khác biệt là 3.9 điểm. Chúng tôi nghi ngờ rằng sự khác biệt này do tập huấn luyện tương đối nhỏ của QNLI. Chúng tôi nghiên cứu hành vi tài nguyên thấp của Transformers so với HyperMixer trong Phần 4.5. FNet hoạt động tệ hơn đáng kể so với các phương pháp khác, đặc biệt trên SNLI và QQP. Linear Transformers đạt hiệu suất xuất sắc trên MNLI và SNLI, nhưng hoạt động kém trên QNLI và QQP.

Trong Phụ lục C.2, chúng tôi thảo luận về các ablations như HyperMixer không ràng buộc.

### 4.4 Thời gian trên mỗi Ví dụ

Để đánh giá hiệu quả của mô hình, chúng tôi đo thời gian wallclock của việc xử lý một đầu vào duy nhất (lặp lại 1,000 lần) thông qua các giai đoạn token mixing của HyperMixer và Transformer, tương ứng. Như Schwartz et al. (2020) chỉ ra, thời gian wallclock có nhược điểm phụ thuộc vào triển khai cụ thể, và họ do đó khuyến nghị báo cáo số lượng phép toán dấu phẩy động (FOPs) cần thiết cho một forward pass. Trong Hình 2, chúng tôi hiển thị thời gian wallclock và FOPs lý thuyết như một hàm của độ dài đầu vào N. Đối với chuỗi đầu vào ngắn, số lượng FOPs bị chi phối bởi kích thước của lớp ẩn và do đó thấp hơn một chút cho Transformers so với HyperMixer. Tuy nhiên, về mặt thực tế chúng tôi quan sát rằng HyperMixer vẫn nhanh hơn Transformers. Ở chuỗi đầu vào dài hơn, kích thước của N bắt đầu chi phối tổng độ phức tạp của Transformers, khiến nó trở nên chậm hơn đáng kể so với HyperMixer.

### 4.5 Hiệu suất Tài nguyên Thấp

Giống như MLPMixer, HyperMixer là một kiến trúc đơn giản về mặt khái niệm, vì nó chỉ áp dụng multi-layer perceptrons ở cốt lõi. Các kiến trúc đơn giản hơn thường tạo ra hiệu suất tốt hơn trên các tập dữ liệu quy mô nhỏ hơn. Chúng tôi nghiên cứu điều này bằng cách thay đổi số lượng ví dụ được sử dụng để huấn luyện trên ba tập dữ liệu lớn MNLI, SNLI và QQP. Cho các thí nghiệm này, chúng tôi sử dụng learning rate hoạt động tốt nhất được tìm thấy trong grid search từ Phần 4.3. Trong Hình 3, chúng tôi vẽ đồ thị thay đổi hiệu suất tương đối của HyperMixer so với Transformers như một hàm của kích thước subsample. Trên tất cả các tập dữ liệu, cải thiện tương đối của HyperMixer so với Transformers lớn hơn khi huấn luyện với 10% tập dữ liệu so với toàn bộ tập dữ liệu. Mặc dù hiệu ứng nhỏ trên QQP, nó đặc biệt lớn trên SNLI và MNLI, nơi HyperMixer hoạt động tốt hơn gần 12-14% với 10% dữ liệu, trong khi cải thiện tương đối với toàn bộ tập dữ liệu ít hơn 2%.

### 4.6 Dễ dàng Điều chỉnh Siêu tham số

Token mixing dựa trên MLP có lợi thế là nó đơn giản hơn về mặt khái niệm so với self-attention, và biết cách hỗ trợ huấn luyện thông qua các cơ chế như skip-connections và layer normalization. Cả hai khía cạnh này đều gợi ý rằng có thể dễ dàng hơn để tìm các cấu hình siêu tham số mang lại hiệu suất tốt. Trong các thí nghiệm này, chúng tôi so sánh HyperMixer (với hypernetworks ràng buộc) với Transformers trong vấn đề này. Như được khuyến nghị trong Schwartz et al. (2020), chúng tôi thực hiện random search để điều chỉnh siêu tham số và tính toán hiệu suất validation mong đợi (Dodge et al., 2019, 2021). Cụ thể, chúng tôi điều chỉnh learning rate, có logarithm được rút từ U(-8,-1), và xác suất dropout được rút từ U(0,0.5) cho 20 trials.

**Kết quả** Trong Hình 4, chúng tôi hiển thị hiệu suất validation mong đợi tương đối, tức là thay đổi hiệu suất tương đối của HyperMixer so với Transformer, cho tất cả năm tập dữ liệu. Với ngoại lệ đáng chú ý của QNLI, cải thiện tương đối của HyperMixer cao hơn ở ngân sách nhỏ so với ngân sách lớn trên tất cả các tập dữ liệu. Hiệu ứng đặc biệt mạnh trên SNLI, nơi HyperMixer tốt hơn 6.5% ở ngân sách điều chỉnh nhỏ, nhưng ít hơn 2% tốt hơn ở ngân sách cao. Những kết quả này chỉ ra rằng HyperMixer dễ điều chỉnh hơn đáng kể so với Transformers.

### 4.7 HyperMixer Học các Mẫu Attention

Chúng tôi đưa ra giả thuyết rằng lớp token mixing của HyperMixer cung cấp một cơ chế tương tự như attention. Để chứng minh điều này, chúng tôi xem xét một bài toán đồ chơi với chuỗi 1d được tạo thành từ các cặp hình có chiều cao khác nhau như được mô tả trong Fleuret (2019). Giá trị mục tiêu là chiều cao trung bình trong mỗi cặp hình. Một ví dụ đầu vào được thể hiện trong Hình 5a. Để giải quyết tác vụ tốt, cho mỗi vị trí, mô hình phải chú ý đến các vị trí khác có cùng hình dạng.

**Mô hình** Chúng tôi so sánh lớp token mixing của HyperMixer với ba mô hình khác: i) None không mô hình hóa tương tác token. Tất cả các dự đoán do đó chỉ được thực hiện dựa trên thông tin địa phương. Mô hình này do đó sẽ thất bại. ii) MLPMixer mô hình hóa tương tác token. Tuy nhiên, vì trọng số token mixing của nó cụ thể theo vị trí, mỗi vị trí phải học cách nhận biết từng hình dạng, điều mà chúng tôi mong đợi sẽ khó khăn, đặc biệt với ít dữ liệu. iii) Self-attention có thể được coi là giới hạn trên, vì nó mô hình hóa tương tác giữa mọi hai vị trí một cách rõ ràng.

**Kết quả** Hình 5b hiển thị mean squared error trên các ví dụ test tùy thuộc vào số lượng ví dụ huấn luyện. Như mong đợi, None thất bại trong tác vụ này. Mặc dù tất cả các mô hình khác có thể giải quyết tác vụ với đủ dữ liệu huấn luyện, MLPMixer ít hiệu quả về dữ liệu hơn đáng kể so với hai mô hình khác, yêu cầu nhiều hơn 5-10 lần dữ liệu để đạt cùng hiệu suất. Điều này được mong đợi, vì trái ngược với HyperMixer và self-attention, module token mixing của MLPMixer không bất biến vị trí. HyperMixer và self-attention đạt hiệu suất xấp xỉ tương tự khi huấn luyện trên 100k ví dụ. Tuy nhiên, HyperMixer hiệu quả hơn về dữ liệu so với self-attention, điều mà chúng tôi quy cho kiến trúc mô hình đơn giản hơn.

Chúng tôi có thể đo lường tương tác giữa hai token bằng cách tính gradient của token đầu ra với token đầu vào (pseudo-attention). Hình 5d và 5c hiển thị pseudo-attention maps của HyperMixer so với attention. Chúng tôi quan sát rằng trọng số pseudo-attention của HyperMixer và attention tương tự. Điều này chỉ ra rằng HyperMixer thực sự học một hàm giống attention. Ngược lại, chúng tôi thấy các mẫu này yếu hơn trong MLPMixer (Hình 6, phụ lục).

## 5 Thảo luận

Trong phần sau, chúng tôi trước tiên thảo luận về những ưu điểm của mô hình được đề xuất, đây là những đóng góp cốt lõi của bài báo chúng tôi. Sau đó chúng tôi thảo luận về phạm vi phân tích của chúng tôi.

### 5.1 Tác động

**Mô hình all-MLP tốt nhất** HyperMixer được thiết kế như một kiến trúc dựa trên MLP với inductive biases tương tự như Transformers, có lợi cho việc hiểu ngôn ngữ tự nhiên. Giả thuyết của chúng tôi (H1) là điều này dẫn đến cải thiện so với các phương pháp dựa trên MLP khác. Kết quả thí nghiệm của chúng tôi hỗ trợ giả thuyết này, khi chúng tôi thấy HyperMixer vượt trội tất cả các baseline dựa trên MLP trên tất cả các tập dữ liệu (Phần 4.3).

**Mô hình chi phí thấp** Động lực chính cho kiến trúc dựa trên MLP là lợi ích hiệu quả được tạo ra bởi sự đơn giản của nó. Do đó, chúng tôi đưa ra giả thuyết (H2) rằng HyperMixer sẽ giảm chi phí Cost (R) ∝ E·D·H để có được kết quả AI R. Giả thuyết này được hỗ trợ bởi các thí nghiệm của chúng tôi. Mặc dù HyperMixer mang lại kết quả ngang bằng với kết quả của Transformer, nó giảm chi phí của cả ba yếu tố chi phí: i) Chi phí xử lý một ví dụ đơn (E) thấp hơn, đặc biệt cho đầu vào dài do độ phức tạp tuyến tính so với độ phức tạp bậc hai của self-attention (Phần 4.4). ii) Số lượng ví dụ huấn luyện cần thiết (D) được giảm, vì cải thiện hiệu suất tương đối của HyperMixer lớn hơn trong tình huống tài nguyên thấp (Phần 4.5). iii) HyperMixer yêu cầu ít điều chỉnh siêu tham số hơn Transformers để đạt kết quả tốt, điều này được chứng minh bởi cải thiện tương đối mong đợi cao hơn của HyperMixer ở ngân sách điều chỉnh thấp (Phần 4.6).

**Mô hình giống Attention** Cuối cùng, các thí nghiệm của chúng tôi trên tác vụ tổng hợp chỉ ra rằng HyperMixer có thể học các mẫu attention rất tương tự như cơ chế self-attention trong Transformers (Phần 4.7), hỗ trợ giả thuyết H3. Mặc dù MLPMixer cũng có thể học các mẫu tương tự với đủ dữ liệu huấn luyện, chúng tôi tin rằng chính việc giới thiệu các biases phù hợp cho phép HyperMixer học các mẫu này một cách hiệu quả. Những biases này được chọn dựa trên phân tích thành công của Transformer bởi Henderson (2020). Thành công riêng của HyperMixer do đó hỗ trợ phân tích đó.

Tóm lại, trong nghiên cứu của chúng tôi, HyperMixer là kiến trúc dựa trên MLP hoạt động tốt nhất, và cho thấy hiệu suất và hành vi tương đương như self-attention với chi phí thấp hơn đáng kể. HyperMixer do đó có thể được coi là giải pháp thay thế chi phí thấp cho Transformers.

### 5.2 Phạm vi

**Tình huống tài nguyên nhỏ** Điều quan trọng cần lưu ý là nghiên cứu của chúng tôi bị giới hạn trong tình huống tài nguyên nhỏ: Các mô hình của chúng tôi nhỏ, không được pretrained trên các corpus đa mục đích lớn, và được huấn luyện trên các tập dữ liệu có ít hơn 1 triệu ví dụ. Không rõ liệu kết quả của chúng tôi có giữ được ở quy mô lớn hơn hay không. Ví dụ, mặc dù gMLP và FNet hoạt động kém trong tình huống tài nguyên thấp như được chứng minh trong các thí nghiệm của chúng tôi, cả hai mô hình đều có thể thu hẹp khoảng cách với các mô hình dựa trên Transformer khi tài nguyên cho pretraining tăng lên (Liu et al., 2021; Lee-Thorp et al., 2021). Chúng tôi đưa ra giả thuyết rằng với đủ tài nguyên, những mô hình này có thể vượt qua những thiếu sót về inductive biases. Tuy nhiên, không có lý do gì để tin rằng HyperMixer, được trang bị các inductive biases hữu ích, sẽ không hoạt động ngang bằng với Transformers trong các tình huống tài nguyên cao trong khi vẫn giữ chi phí tổng thể thấp hơn. Hoàn toàn ngược lại, độ phức tạp tuyến tính trong độ dài chuỗi của HyperMixer có lẽ làm cho nó phù hợp hơn cho pretraining quy mô lớn trên các ngữ cảnh dài so với vanilla Transformers.

**Tính đa dạng** Một trong những chất lượng ấn tượng nhất của Transformers là tính đa dạng của chúng: Không chỉ chúng hiện là kiến trúc tiêu chuẩn cho tất cả các tác vụ NLP, mà qua các năm chúng cũng đã trở nên phổ biến trong nhiều lĩnh vực ứng dụng bên ngoài NLP. Tất nhiên, nghiên cứu hiện tại không thể xác định liệu HyperMixer có đa dạng như Transformers hay không. Tuy nhiên, các nghiên cứu tiếp theo đã chỉ ra rằng HyperMixer có ứng dụng trong nhận dạng giọng nói (Mai et al., 2023) và tối ưu hóa tổ hợp neural (Drakulic et al., 2023). Tuy nhiên, một số tiến bộ mô hình hóa vẫn cần thiết. Ví dụ, HyperMixing chưa áp dụng được cho các mô hình decoder sử dụng causal masking. Vì các mô hình ngôn ngữ decoder-only đã trở nên được nghiên cứu rộng rãi, điều này tạo thành công việc tương lai đầy hứa hẹn.

## 6 Kết luận

Mặc dù các mô hình ngôn ngữ Transformer được pretrained lớn đã dẫn đến tiến bộ ấn tượng, chúng yêu cầu rất nhiều tài nguyên đến mức nhiều phòng thí nghiệm nghiên cứu bị loại trừ khỏi việc tham gia, dẫn đến những lời kêu gọi "Green AI". Chúng tôi đã đề xuất một phương pháp dựa trên MLP, HyperMixer, trái ngược với các phương pháp dựa trên MLP trước đây, được trang bị cùng các inductive biases đã làm cho Transformers thành công như vậy cho việc hiểu ngôn ngữ tự nhiên. Mặc dù nó hoạt động ngang bằng với Transformers, nó phải chịu chi phí thấp hơn đáng kể về thời gian xử lý, dữ liệu huấn luyện và điều chỉnh siêu tham số. Do đó, chúng tôi tin rằng nghiên cứu của chúng tôi chứng minh những ưu điểm của các mô hình dựa trên MLP cho việc hiểu ngôn ngữ tự nhiên như một giải pháp thay thế cho các mô hình dựa trên attention, và chúng tôi hy vọng cộng đồng sẽ theo đuổi hướng này hơn nữa. Các hướng cho công việc tương lai bao gồm pretraining quy mô lớn, đánh giá trên phạm vi rộng hơn các tác vụ và lĩnh vực, và việc thích ứng mô hình với tạo văn bản.

## Giới hạn

Nhiều giới hạn của nghiên cứu chúng tôi đã được thảo luận trong Phần 5.2, tuy nhiên, chúng tôi lặp lại và bổ sung chúng một cách rõ ràng ở đây.

**Tình huống tài nguyên nhỏ** Nghiên cứu của chúng tôi điều tra các kiến trúc dựa trên MLP cho các tác vụ phân loại văn bản và tìm thấy hiệu suất cạnh tranh với vanilla Transformers trong khi có chi phí thấp hơn về phương trình Green AI. Tuy nhiên, phạm vi phát hiện của chúng tôi tự nhiên bị giới hạn trong tình huống kiểm tra, đó là tài nguyên thấp: Các mô hình của chúng tôi tương đối nhỏ, không được pretrained trên các corpus đa mục đích lớn, và được huấn luyện trên các tập dữ liệu có ít hơn 1 triệu ví dụ. Chúng tôi không thể nói với chắc chắn rằng kết quả của chúng tôi cũng sẽ giữ được ở quy mô lớn hơn. Vì mục đích nghiên cứu dựa trên giả thuyết, chúng tôi coi việc chạy nhiều thí nghiệm quy mô nhỏ có kiểm soát có giá trị hơn là ít thí nghiệm quy mô lớn. Tuy nhiên, việc mở rộng chắc chắn nên là một phần của các hướng nghiên cứu tương lai, vì điều này là thiết yếu cho hiệu suất tác vụ tối ưu.

**Giới hạn cho các tác vụ phân loại câu cặp tiếng Anh** Vì token mixing là biến độc lập trong nghiên cứu của chúng tôi, chúng tôi tập trung chính vào các tác vụ phân loại cặp câu tiếng Anh chỉ với đầu vào văn bản, mà chúng tôi giả định (và cung cấp một số bằng chứng) là hữu ích nhất để đánh giá sự khác biệt giữa các mô hình token mixing. Tất nhiên, vanilla Transformers rất linh hoạt theo nghĩa là, qua quá trình nhiều nghiên cứu, chúng đã được chứng minh là rất hiệu quả cho một phạm vi rộng các tác vụ, ngôn ngữ và phương thức dữ liệu. Liệu mô hình HyperMixer được đề xuất có sở hữu tính linh hoạt tương tự hay không không thể được trả lời trong nghiên cứu này. HyperMixer encoder có thể có các inductive biases tương tự như Transformers. Do đó chúng tôi mong đợi nó sẽ đơn giản để áp dụng cho các tác vụ cũng được giải quyết tốt bởi Transformer encoders (ví dụ: phân loại span). Đối với các tác vụ như mô hình hóa ngôn ngữ, liên quan đến Transformer decoder, cần có những tiến bộ mô hình hóa đáng kể để có được tương đương HyperMixer. Chúng tôi coi đây là một hướng rất hứa hẹn cho công việc tương lai.

**Giới hạn cho các baseline dựa trên MLP** Tương tự như xu hướng trong cộng đồng thị giác máy tính, nghiên cứu của chúng tôi điều tra tính phù hợp của các kiến trúc dựa trên MLP cho NLP. Do sự đơn giản về mặt khái niệm, những mô hình này hứa hẹn sẽ dễ huấn luyện hơn, có khả năng dẫn đến giảm chi phí Green AI. Với mục đích này, chúng tôi so sánh mô hình HyperMixer được đề xuất với một loạt các mô hình dựa trên MLP khác và Transformers. Ngoài FNet và Linear Transformers, là các giải pháp thay thế Transformer hiệu quả, chúng tôi không cố gắng so sánh toàn diện với các mô hình NLP hiệu quả không dựa trên MLP. Do đó, phạm vi tuyên bố của chúng tôi không mở rộng đến tất cả các mô hình Transformer hiệu quả. Tuy nhiên, những mô hình này tất nhiên rất phù hợp với nghiên cứu này, vì chúng nhắm vào một trong những yếu tố chi phí Green AI (độ phức tạp forward pass đơn). Do đó, chúng tôi coi việc so sánh toàn diện là công việc tương lai có giá trị.

## Lời cảm ơn

Florian Mai được hỗ trợ bởi Quỹ Khoa học Quốc gia Thụy Sĩ trong dự án LAOS, số grant 200021_178862. Arnaud Pannatier được hỗ trợ bởi Cơ quan Đổi mới Thụy Sĩ Innosuisse trong dự án MALAT, số grant "32432.1 IP-ICT". Fabio Fehr được hỗ trợ bởi Trung tâm Quốc gia Thụy Sĩ về Năng lực Nghiên cứu (NCCR) trong dự án Evolving Language, số grant "51NF40_180888". Haolin Chen được hỗ trợ bởi Quỹ Khoa học Quốc gia Thụy Sĩ trong dự án NAST, số grant "185010". François Marelli được hỗ trợ bởi Quỹ Khoa học Quốc gia Thụy Sĩ trong dự án COMPBIO, số grant "179217".
