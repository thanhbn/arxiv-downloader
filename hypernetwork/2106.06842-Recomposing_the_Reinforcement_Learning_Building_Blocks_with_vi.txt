# 2106.06842.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/hypernetwork/2106.06842.pdf
# Kích thước tệp: 3383382 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tái tổ hợp các Khối Xây dựng Học tăng cường bằng
Hypernetwork
Shai Keynan*1Elad Saraﬁan*1Sarit Kraus1
Tóm tắt
Các khối xây dựng Học tăng cường (RL), tức là các hàm Q và mạng chính sách, thường nhận các phần tử từ tích Cartesian của hai miền làm đầu vào. Cụ thể, đầu vào của hàm Q là cả trạng thái và hành động, và trong các bài toán đa tác vụ (Meta-RL), chính sách có thể nhận một trạng thái và một ngữ cảnh. Các kiến trúc tiêu chuẩn có xu hướng bỏ qua các diễn giải cơ bản của các biến này và chỉ đơn giản nối các đặc trưng của chúng thành một vector duy nhất. Trong công trình này, chúng tôi lập luận rằng lựa chọn này có thể dẫn đến ước lượng gradient kém trong các thuật toán actor-critic và các bước học có phương sai cao trong các thuật toán Meta-RL. Để xem xét tương tác giữa các biến đầu vào, chúng tôi đề xuất sử dụng kiến trúc Hypernetwork trong đó một mạng chính xác định các trọng số của một mạng động có điều kiện. Chúng tôi chỉ ra rằng cách tiếp cận này cải thiện xấp xỉ gradient và giảm phương sai bước học, điều này vừa tăng tốc quá trình học vừa cải thiện hiệu suất cuối cùng. Chúng tôi chứng minh sự cải thiện nhất quán trên các tác vụ chuyển động khác nhau và các thuật toán khác nhau cả trong RL (TD3 và SAC) và trong Meta-RL (MAML và PEARL).

1. Giới thiệu
Sự phát triển nhanh chóng của các mạng nơ-ron sâu như những bộ xấp xỉ hàm đa năng đã thúc đẩy sự phục hưng Học tăng cường (RL) gần đây (Zai và Brown, 2020). Các thuật toán RL đã tiến bộ về tính mạnh mẽ, ví dụ từ (Lillicrap et al., 2016) đến (Fujimoto et al., 2018); khám phá (Haarnoja et al., 2018); lấy mẫu gradient (Schulman et al., 2017; 2015a); và học ngoài chính sách (Fujimoto et al., 2019; Kumar et al., 2019). Nhiều thuật toán actor-critic đã tập trung vào việc cải thiện các quy trình học của critic bằng cách sửa đổi giá trị mục tiêu (Hasselt et al., 2016), điều này cho phép xấp xỉ hàm Q chính xác và mạnh mẽ hơn. Mặc dù điều này cải thiện đáng kể hiệu quả tối ưu hóa chính sách, hiệu suất vẫn bị giới hạn bởi khả năng của các mạng trong việc biểu diễn các hàm Q và chính sách. Một ràng buộc như vậy đòi hỏi phải nghiên cứu và thiết kế các mô hình nơ-ron phù hợp cho việc biểu diễn các khối xây dựng RL này.

Một hiểu biết quan trọng trong việc thiết kế các mô hình nơ-ron cho RL là tính tương hỗ giữa trạng thái và hành động, cả hai đều phục vụ như đầu vào cho hàm Q. Ban đầu, mỗi đầu vào có thể được xử lý riêng lẻ theo miền nguồn của nó. Ví dụ, khi s là một vector các hình ảnh, việc sử dụng các mô hình CNN là phổ biến (Kaiser et al., 2019), và khi s hoặc a là các từ ngôn ngữ tự nhiên, mỗi đầu vào có thể được xử lý riêng biệt với các vector nhúng (He et al., 2016). Thực tiễn phổ biến trong việc kết hợp các đặc trưng có thể học được của trạng thái và hành động thành một mạng duy nhất là nối hai vector và theo sau bằng MLP để tạo ra giá trị Q (Schulman et al., 2017). Trong công trình này, chúng tôi lập luận rằng đối với các thuật toán RL actor-critic (Grondman et al., 2012), phương pháp hiện có như vậy có thể được cải thiện đáng kể bằng Hypernetwork.

Trong các phương pháp actor-critic, đối với mỗi trạng thái, được lấy mẫu từ phân phối tập dữ liệu, nhiệm vụ của actor là giải quyết một bài toán tối ưu hóa trên phân phối hành động, tức là chính sách. Điều này thúc đẩy một kiến trúc mà hàm Q được mô hình hóa rõ ràng như hàm giá trị của một bandit có ngữ cảnh (Lattimore và Szepesv ári, 2020) Q(s;a) = Q_s(a) trong đó s là ngữ cảnh. Trong khi các kiến trúc tiêu chuẩn không được thiết kế để mô hình hóa mối quan hệ như vậy, Hypernetwork được xây dựng rõ ràng cho mục đích đó (Ha et al., 2016).

Hypernetwork, còn được gọi là meta-network, có thể biểu diễn các hệ thống phân cấp bằng cách biến đổi một biến meta thành một hàm phụ thuộc ngữ cảnh ánh xạ một biến cơ sở tới không gian đầu ra yêu cầu. Điều này nhấn mạnh động lực cơ bản giữa các biến meta và biến cơ sở và đã tìm thấy thành công trong nhiều lĩnh vực khác nhau như mạng nơ-ron Bayes (Lior Deutsch, 2019), học liên tục (von Oswald et al., 2019), các mô hình sinh (Ratzlaff và Li, 2019) và phòng thủ đối kháng (Sun et al., 2017). Thành công thực tế đã khơi dậy sự quan tâm đến các thuộc tính lý thuyết của Hypernetwork. Ví dụ, gần đây đã được chỉ ra rằng chúng có độ phức tạp tham số tốt hơn so với các mô hình cổ điển nối các biến cơ sở và meta lại với nhau (Galanti và Wolf, 2020a;b).

Khi phân tích khả năng của critic trong việc biểu diễn hàm Q, điều quan trọng là phải nhận ra rằng để tối ưu hóa chính sách, các thuật toán actor-critic ngoài chính sách hiện đại (Fujimoto et al., 2018; Haarnoja et al., 2018) chỉ sử dụng gradient nơ-ron tham số của critic đối với đầu vào hành động, tức là ∇_a Q_θ(s;a). Gần đây, (Ilyas et al., 2019) đã kiểm tra độ chính xác của gradient chính sách trong các thuật toán trên chính sách. Họ chứng minh rằng các triển khai RL tiêu chuẩn đạt được ước lượng gradient với độ tương tự cosine gần bằng không khi so sánh với gradient "thực". Do đó, việc phục hồi các xấp xỉ gradient tốt hơn có tiềm năng cải thiện đáng kể quá trình học RL. Được thúc đẩy bởi nhu cầu có được các xấp xỉ gradient chất lượng cao, chúng tôi bắt đầu điều tra độ chính xác gradient của Hypernetwork so với các mô hình tiêu chuẩn. Trong Mục 3, chúng tôi phân tích ba mô hình critic và thấy rằng mô hình Hypernetwork với trạng thái như một biến meta có độ chính xác gradient tốt hơn, điều này chuyển thành tốc độ học nhanh hơn.

Giống như hệ thống phân cấp được tạo ra trong critic, các meta-policy tối ưu hóa các bài toán RL đa tác vụ có cấu trúc tương tự khi chúng kết hợp một ngữ cảnh phụ thuộc tác vụ và một đầu vào trạng thái. Mặc dù một số thuật toán như MAML (Finn et al., 2017) và LEO (Rusu et al., 2019) không sử dụng ngữ cảnh rõ ràng, các công trình khác, ví dụ PEARL (Rakelly et al., 2019) hoặc MQL (Fakoor et al., 2019), đã chứng minh rằng ngữ cảnh cải thiện khả năng khái quát hóa. Gần đây, (Jayakumar et al., 2019) đã chỉ ra rằng Tương tác Nhân (MI) là một lựa chọn thiết kế xuất sắc khi kết hợp các trạng thái và ngữ cảnh. Các phép toán MI có thể được xem như các kiến trúc Hypernetwork nông. Trong Mục 4, chúng tôi khám phá thêm cách tiếp cận này và nghiên cứu các meta-policy dựa trên ngữ cảnh với Hypernetwork sâu. Chúng tôi thấy rằng với Hypernetwork, các gradient phụ thuộc tác vụ và phụ thuộc trạng thái được tách rời sao cho các gradient phụ thuộc trạng thái được biên hóa, dẫn đến phương sai bước học thấp hơn theo thực nghiệm. Điều này đặc biệt quan trọng trong các phương pháp trên chính sách như MAML, nơi có ít bước tối ưu hóa hơn trong quá trình huấn luyện.

Các đóng góp của bài báo này có ba khía cạnh. Đầu tiên, trong Mục 3, chúng tôi cung cấp một liên kết lý thuyết giữa chất lượng xấp xỉ gradient hàm Q và tốc độ học cho phép cho việc cải thiện chính sách đơn điệu. Tiếp theo, chúng tôi chỉ ra thực nghiệm rằng Hypernetwork đạt được các xấp xỉ gradient tốt hơn, điều này chuyển thành tốc độ học nhanh hơn và cải thiện hiệu suất cuối cùng. Cuối cùng, trong Mục 4, chúng tôi chỉ ra rằng Hypernetwork giảm đáng kể phương sai bước học trong Meta-RL. Chúng tôi tóm tắt các kết quả thực nghiệm trong Mục 5, điều này chứng minh lợi ích của Hypernetwork cả trong RL một tác vụ và Meta-RL. Quan trọng là, chúng tôi thấy thực nghiệm rằng các chính sách Hypernetwork loại bỏ nhu cầu cho bước thích ứng MAML và cải thiện khả năng khái quát hóa Ngoài-Phân-phối trong PEARL.

2. Hypernetwork
Hypernetwork (Ha et al., 2016) là một kiến trúc mạng nơ-ron được thiết kế để xử lý một tuple (z;x) ∈ Z × X và đưa ra một giá trị y ∈ Y. Nó bao gồm hai mạng, một mạng chính θ: Z → R^n_w tạo ra các trọng số w(z) cho một mạng động f_w(z): X → Y. Cả hai mạng đều được huấn luyện cùng nhau, và gradient chảy qua f đến các trọng số θ của mạng chính. Trong thời gian kiểm tra hoặc suy luận, các trọng số chính θ được cố định trong khi đầu vào z xác định các trọng số của mạng động.

Ý tưởng về các trọng số phụ thuộc ngữ cảnh có thể học được có thể được truy nguyên về (McClelland, 1985; Schmidhuber, 1992). Tuy nhiên, chỉ trong những năm gần đây Hypernetwork mới trở nên phổ biến khi chúng được áp dụng thành công với nhiều mô hình mạng động, ví dụ mạng hồi quy (Ha et al., 2016), mạng MLP cho đám mây điểm 3D (Littwin và Wolf, 2019), biến đổi không gian (Potapov et al., 2018), mạng tích chập cho dự đoán khung hình video (Jia et al., 2016) và học few-shot (Brock et al., 2018). Trong bối cảnh RL, Hypernetwork cũng đã được áp dụng, ví dụ trong QMIX (Rashid et al., 2018) để giải quyết các tác vụ Multi-agent RL và cho RL dựa trên mô hình liên tục (Huang et al., 2020).

Hình 1 minh họa mô hình Hypernetwork của chúng tôi. Mạng chính θ_w(z) chứa các khối dư (Srivastava et al., 2015) biến đổi biến meta thành một biểu diễn tiềm ẩn có kích thước 1024. Giai đoạn này được theo sau bởi một loạt các biến đổi tuyến tính song song, được gọi là "head", tạo ra các tập hợp trọng số động. Mạng động f_w(z)(x) chỉ chứa một lớp ẩn duy nhất có 256 nơ-ron, nhỏ hơn so với kiến trúc MLP tiêu chuẩn được sử dụng trong nhiều bài báo RL (Fujimoto et al., 2018; Haarnoja et al., 2018) có 2 lớp ẩn, mỗi lớp có 256 nơ-ron. Mô hình tính toán của mỗi lớp động là

x^{l+1} = ReLU((1 + g_l(z)) ⊙ x^l W_l(z) + b_l(z))     (1)

trong đó tính phi tuyến chỉ được áp dụng trên lớp ẩn và g_l là một tham số gain bổ sung cần thiết trong các kiến trúc Hypernetwork (Littwin và Wolf, 2019). Chúng tôi hoãn thảo luận về các lựa chọn thiết kế này đến Mục 5.

3. Tái tổ hợp Hàm Q của Actor-Critic

3.1. Bối cảnh
Học tăng cường liên quan đến việc tìm các chính sách tối ưu trong Quy trình Quyết định Markov (MDP). Một MDP (Dean và Givan, 1997) được định nghĩa bởi một tuple (S,A,P,R) trong đó S là một tập hợp các trạng thái, A là một tập hợp các hành động, P là một tập hợp các xác suất để chuyển từ trạng thái s sang s' cho một hành động a, và R: S × A → R là một hàm phần thưởng vô hướng. Mục tiêu là tối đa hóa tổng phần thưởng chiết khấu kỳ vọng với hệ số chiết khấu γ > 0

J(π) = E[∑_{t=0}^∞ γ^t R(s_t, a_t) | a_t ~ π(·|s_t)]     (2)

J(π) cũng có thể được viết, tới một hệ số hằng số 1/(1-γ), như một kỳ vọng trên hàm Q

J(π) = E_{s~d^π} E_{a~π(·|s)} [Q^π(s,a)]     (3)

trong đó hàm Q là tổng phần thưởng chiết khấu kỳ vọng sau khi thăm tại trạng thái s và thực hiện hành động a (Sutton và Barto, 2018), và d^π là phân phối trạng thái được tạo ra bởi chính sách π.

Các phương pháp Actor-critic tối đa hóa J(π) trên không gian các chính sách tham số hóa. Các chính sách ngẫu nhiên được xây dựng như một biến đổi phụ thuộc trạng thái của một biến ngẫu nhiên độc lập

π(a|s) = π(ε|s) s.t. ε ~ p_ε     (4)

trong đó p_ε là một phân phối đa biến được định nghĩa trước trên R^{n_a} và n_a là số lượng hành động. Để tối đa hóa J(π) trên các tham số θ, các phương pháp actor-critic hoạt động với một thuật toán lặp ba giai đoạn. Đầu tiên, chúng thu thập vào một buffer replay D các tuple kinh nghiệm (s,a,r,s') được tạo ra bởi chính sách π_θ tham số và một số chính sách khám phá nhiễu cộng (Zhang và Sutton, 2017). Sau đó chúng khớp một critic là một mô hình tham số Q_φ cho hàm Q. Với mục đích này, chúng áp dụng TD-learning (Sutton và Barto, 2018) với hàm mất mát

L_{critic}(φ) = E_{s,a,r,s'~D}[(Q_φ(s,a) - r - γ E_{a'~π_θ(·|s')}[Q_{φ^-}(s',a')])^2]

trong đó φ^- là một tập hợp các tham số chậm (Lillicrap et al., 2016). Cuối cùng, chúng áp dụng các cập nhật gradient descent theo hướng của một surrogate ngoài chính sách của J(π)

θ ← θ + α∇_θ J_{actor}(θ)
∇_θ J_{actor}(θ) = E_{s~D} E_{ε~p_ε}[∇_θ π(ε|s) ∇_a Q_φ(s,π(ε|s))]     (5)

Ở đây, ∇_θ π(ε|s) là một ma trận có kích thước n_θ × n_a trong đó n_θ là số lượng tham số chính sách cần được tối ưu hóa.

Hai thuật toán ngoài chính sách nổi tiếng là TD3 (Fujimoto et al., 2018) và SAC (Haarnoja et al., 2018). TD3 tối ưu hóa các chính sách xác định với nhiễu khám phá cộng bình thường và double Q-learning để cải thiện tính mạnh mẽ của phần critic (Hasselt et al., 2016). Mặt khác, SAC áp dụng các chính sách ngẫu nhiên phân phối bình thường nhưng nó sửa đổi hàm phần thưởng để bao gồm một bonus entropy cao R̃(s,a) = R(s,a) + αH(π(·|s)) điều này loại bỏ nhu cầu về nhiễu khám phá.

3.2. Cách tiếp cận của chúng tôi
Gradient của surrogate ngoài chính sách ∇_θ J_{actor}(θ) khác với gradient thực ∇_θ J(π) ở hai yếu tố: Đầu tiên, phân phối các trạng thái là phân phối thực nghiệm trong tập dữ liệu chứ không phải phân phối chính sách d^π; và thứ hai, gradient hàm Q được ước lượng bằng gradient nơ-ron tham số của critic ∇_a Q_φ ≈ ∇_a Q. Tránh sự không khớp phân phối là động lực của nhiều phương pháp cải thiện chính sách có ràng buộc như TRPO và PPO (Schulman et al., 2015a; 2017). Tuy nhiên, nó yêu cầu các bước rất nhỏ và không thực tế. Do đó, nhiều thuật toán ngoài chính sách bỏ qua sự không khớp phân phối và tìm cách tối đa hóa chỉ lợi thế thực nghiệm

A(π';π) = E_{s~D}[E_{a'~π'}[Q^π(s,a')] - E_{a~π}[Q^π(s,a)]]

Trong thực tế, một lợi thế thực nghiệm tích cực được liên kết với các chính sách tốt hơn và được yêu cầu bởi các phương pháp cải thiện chính sách đơn điệu như TRPO (Kakade và Langford, 2002; Schulman et al., 2015a). Tuy nhiên, việc tìm các chính sách lợi thế thực nghiệm tích cực đòi hỏi một xấp xỉ tốt của gradient ∇_a Q. Mệnh đề tiếp theo đề xuất rằng với một xấp xỉ đủ chính xác, việc áp dụng bước gradient như được công thức hóa trong cập nhật actor trong Eq. (5) tạo ra các chính sách lợi thế thực nghiệm tích cực.

Mệnh đề 1. Cho π(a|s) = π(ε|s) là một chính sách tham số ngẫu nhiên với ε ~ p_ε, và π(·|s) là một biến đổi với gradient liên tục Lipschitz và một hằng số Lipschitz L_π. Giả sử rằng hàm Q Q^π(s,a) có gradient liên tục Lipschitz trong a, tức là |∇_a Q^π(s,a_1) - ∇_a Q^π(s,a_2)| ≤ L_q ||a_1 - a_2||. Định nghĩa toán tử gradient trung bình ∇_rf = E_{s~D}[E_{ε~p_ε}[∇_θ π(ε|s) f(s,π(ε|s))]]. Nếu tồn tại một ước lượng gradient g(s,a) và 0 < δ < 1 sao cho

||∇_r g - ∇_r ∇_a Q^π|| ≤ δ||∇_r ∇_a Q^π||     (6)

thì bước tăng θ' ← θ + α∇_r g với α ≤ 1/(k̃(1+δ)^2) tạo ra một chính sách lợi thế thực nghiệm tích cực.

Chúng tôi định nghĩa k̃ và cung cấp chứng minh trong phụ lục. Từ đó suy ra rằng một lợi thế thực nghiệm tích cực có thể được đảm bảo khi gradient của hàm Q đủ chính xác, và với các mô hình gradient tốt hơn, tức là δ nhỏ hơn, người ta có thể áp dụng các bước tăng lớn hơn. Tuy nhiên, thay vì khớp gradient, các thuật toán actor-critic ưu tiên mô hình hóa hàm Q và ước lượng gradient bằng gradient tham số của mô hình ∇_a Q_φ. Không rõ liệu các mô hình tốt hơn cho hàm Q, với Sai số Bình phương Trung bình (MSE) thấp hơn, có cung cấp ước lượng gradient tốt hơn hay không. Một cách tiếp cận trực tiếp hơn có thể là học rõ ràng gradient của hàm Q (Saraﬁan et al., 2020; Saremi, 2019); tuy nhiên, trong công trình này, chúng tôi chọn khám phá kiến trúc nào phục hồi xấp xỉ gradient chính xác hơn dựa trên gradient tham số của mô hình hàm Q.

Chúng tôi xem xét ba mô hình thay thế:

1. Mạng MLP, trong đó các đặc trưng trạng thái φ(s) (có thể có thể học được) được nối thành một đầu vào duy nhất của một mạng tuyến tính nhiều lớp.

2. Action-State Hypernetwork (AS-Hyper) trong đó các hành động là biến meta, đầu vào của mạng chính θ_w, và các đặc trưng trạng thái là biến cơ sở, đầu vào cho mạng động f.

3. State-Action Hypernetwork (SA-Hyper), đảo ngược thứ tự của AS-Hyper.

Để phát triển một số trực giác, trước tiên hãy xem xét trường hợp đơn giản nhất trong đó mạng động có một lớp tuyến tính duy nhất và mô hình MLP được thay thế bằng một mô hình tuyến tính thuần túy. Bắt đầu với mô hình tuyến tính, hàm Q và gradient của nó có mô hình tham số sau:

Q_θ(s,a) = [w_s, w_a]^T [φ(s); a]
∇_a Q_θ(s,a) = w_a     (7)

trong đó θ = [w_s, w_a]. Rõ ràng, trong trường hợp này, gradient không phải là một hàm của trạng thái, do đó không thể khai thác mô hình này cho các thuật toán actor-critic. Đối với AS-Hyper, chúng ta thu được mô hình sau

Q_θ(s,a) = θ_w(a)^T φ(s)
∇_a Q_θ(s,a) = ∇_a θ_w(a)^T φ(s)     (8)

Thông thường, vector đặc trưng trạng thái φ(s) có chiều lớn hơn nhiều so với chiều hành động n_a. Do đó, ma trận ∇_a θ_w(a) có một null-space lớn có thể cản trở việc huấn luyện vì nó có thể tạo ra gradient bằng không hoặc gần bằng không ngay cả khi gradient thực tồn tại.

Mặt khác, công thức SA-Hyper là

Q_θ(s,a) = θ_w(s)^T a
∇_a Q_θ(s,a) = θ_w(s)     (9)

đây là một mô hình hằng số phụ thuộc trạng thái của gradient trong a. Mặc dù đây là một mô hình tương đối ngây thơ, nó đủ cho các chính sách địa phương với phương sai thấp vì nó xấp xỉ siêu mặt tiếp tuyến xung quanh giá trị trung bình chính sách.

Tiến tới một kiến trúc đa lớp, trước tiên hãy xem xét kiến trúc AS-Hyper. Trong trường hợp này, gradient là ∇_a Q_θ(s,a) = ∇_a θ_w(a)^T ∇_w f_w(φ(s)). Chúng ta thấy rằng vấn đề của lớp đơn được làm trầm trọng hơn vì ∇_a θ_w(a) bây giờ là một ma trận n_a × n_w trong đó n_w >> n_a là số lượng trọng số mạng động.

Tiếp theo, các mô hình MLP và SA-Hyper có thể được phân tích chung. Đầu tiên, chúng ta tính gradient đầu vào của mỗi lớp

x^{l+1} = f_l(x^l) = σ(x^l W_l + b_l)     (10)
∇_a x^{l+1} = (∇_a x^l) ∇_{x^l} f_l(x^l) = (∇_a x^l) W_l^T σ'_l(x^l)     (11)
σ'_l(x^l) = diag(σ'(x^l W_l + b_l))     (12)

trong đó σ là hàm kích hoạt và W_l và b_l là các trọng số và bias của lớp thứ l, tương ứng. Theo quy tắc chuỗi, gradient đầu vào của một mạng L-lớp là tích của các biểu thức này. Đối với mô hình MLP, chúng ta thu được

∇_a Q_θ(s,a) = W_a^T σ'_1(s,a) (∏_{l=2}^{L-1} W_l^T σ'_l(s,a)) W_L^T     (13)

Mặt khác, trong SA-Hyper, các trọng số là đầu ra của mạng chính, do đó chúng ta có

∇_a Q_θ(s,a) = W_1(s)^T σ'_1(s,a) (∏_{l=2}^{L-1} W_l(s)^T σ'_l(s,a)) W_L(s)^T     (14)

Quan trọng là, trong khi cấu hình gradient của SA-Hyper được kiểm soát qua các ma trận phụ thuộc trạng thái W_l(s), trong mô hình MLP, nó chỉ là một hàm của trạng thái qua các phần tử đường chéo trong σ'_l(s,a). Các đạo hàm địa phương này của các hàm kích hoạt phi tuyến thường là hằng số từng phần khi các kích hoạt có dạng các hàm giống ReLU. Ngoài ra, chúng được yêu cầu phải bị giới hạn và nhỏ hơn một để tránh gradient bùng nổ trong quá trình huấn luyện (Philipp et al., 2017). Những hạn chế này làm giảm đáng kể tính biểu cảm của gradient tham số và khả năng mô hình hóa gradient hàm Q thực của nó. Ví dụ, với ReLU, đối với hai cặp khác nhau (s_1,a_1) và (s_2,a_2), gradient ước lượng là bằng nhau nếu chúng có cùng bản đồ nơ-ron hoạt động (tức là cùng các ReLU ở chế độ hoạt động). Theo lý luận này, chúng tôi giả định rằng cấu hình SA-Hyper nên có các xấp xỉ gradient tốt hơn.

Phân tích thực nghiệm Để kiểm tra giả thuyết của chúng tôi, chúng tôi đã huấn luyện các agent TD3 với các mô hình mạng khác nhau và đánh giá gradient tham số ∇_a Q_φ(s,a) của chúng. Để phân tích thực nghiệm độ chính xác gradient, chúng tôi đã chọn ước lượng gradient hàm Q thực bằng một bộ ước lượng địa phương không tham số tại giá trị trung bình chính sách, tức là tại a = E_{ε~p_ε}[π(ε|s)]. Với mục đích này, chúng tôi tạo ra N_r quỹ đạo độc lập với các hành động được lấy mẫu xung quanh giá trị trung bình, tức là a = ā + δa, và khớp với một bộ ước lượng Bình phương Nhỏ nhất Trung bình (LMS) một mô hình tuyến tính cho lợi nhuận thực nghiệm của các quỹ đạo được lấy mẫu. Gradient "thực" do đó là gradient của mô hình tuyến tính. Các chi tiết kỹ thuật bổ sung của bộ ước lượng này được tìm thấy trong phụ lục.

Vì bộ ước lượng hàm Q của chúng tôi dựa trên học Temporal-Difference (TD), nó mang bias. Do đó, trong thực tế, chúng tôi không thể hy vọng tái tạo quy mô hàm Q thực. Vì vậy, thay vì đánh giá MSE của gradient, chúng tôi lấy Độ tương tự Cosine (CS) như một surrogate để đo độ chính xác gradient.

cs(Q_φ) = E_{s~D} [∇_a Q_φ(s,a) · ∇_a Q^π(s,a)] / [||∇_a Q_φ(s,a)|| ||∇_a Q^π(s,a)||]

Hình 3 tóm tắt các đánh giá CS của chúng tôi với ba mô hình thay thế được trung bình trên 4 môi trường Mujoco (Todorov et al., 2012). Hình 3d trình bày CS trung bình trên các trạng thái trong quá trình huấn luyện. Nói chung, CS rất thấp, điều này cho thấy rằng việc huấn luyện RL còn xa mức tối ưu. Mặc dù phát hiện này hơi đáng ngạc nhiên, nó chứng thực các kết quả trong (Ilyas et al., 2019) đã tìm thấy CS gần bằng không trong các thuật toán policy gradient. Tuy nhiên, lưu ý rằng tác động của độ chính xác CS là tích lũy vì trong mỗi bước gradient ascent, chính sách tích lũy các cải thiện nhỏ. Điều này cho phép ngay cả các mô hình gradient gần bằng không cải thiện theo thời gian. Nhìn chung, chúng tôi thấy rằng CS của SA-Hyper cao hơn, và không giống như các mô hình khác, nó lớn hơn không trong toàn bộ quá trình huấn luyện. Lợi thế của SA-Hyper đặc biệt đáng kể trong 100K bước học đầu tiên, điều này cho thấy rằng SA-Hyper học nhanh hơn trong các giai đoạn học sớm.

Đánh giá độ chính xác gradient theo CS trung bình có thể bị nhiễu bởi các trạng thái đã đạt tới cân bằng địa phương trong quá trình huấn luyện. Trong các trạng thái này, gradient thực có độ lớn bằng không sao cho CS không được định nghĩa. Với mục đích này, trong Hình 3a-c, chúng tôi đo phần trăm các trạng thái có CS cao hơn một ngưỡng τ. Điều này cho thấy có bao nhiêu trạng thái có thể học được trong đó nhiều trạng thái có thể học được hơn được quy cho một ước lượng gradient tốt hơn. Hình 3a cho thấy rằng đối với tất cả các ngưỡng τ ∈ [0,1], SA-Hyper có nhiều trạng thái có thể học được hơn, và Hình 3b-c trình bày sự thay đổi trong các trạng thái có thể học được cho τ khác nhau trong quá trình huấn luyện. Ở đây chúng tôi cũng thấy rằng lợi thế của SA-Hyper đặc biệt đáng kể trong giai đoạn đầu của quá trình huấn luyện. Cuối cùng, Hình 4 chứng minh cách độ chính xác gradient chuyển thành các đường cong học tập tốt hơn. Như mong đợi, chúng tôi thấy rằng SA-Hyper vượt trội hơn cả kiến trúc MLP và cấu hình AS-Hyper mà nói chung cũng kém hơn MLP.

Trong phần tiếp theo, chúng tôi thảo luận về ứng dụng của Hypernetwork trong Meta-RL để mô hình hóa các chính sách có điều kiện ngữ cảnh. Khi một ngữ cảnh như vậy tồn tại, nó cũng phục vụ như một biến đầu vào cho hàm Q. Trong trường hợp đó, khi mô hình hóa critic bằng Hypernetwork, người ta có thể chọn sử dụng ngữ cảnh như một biến meta hoặc thay thế như một biến cơ sở. Quan trọng là, khi ngữ cảnh là đầu vào của động lực, các trọng số động được cố định cho mỗi trạng thái, bất kể tác vụ. Trong các thí nghiệm PEARL trong Mục 5, chúng tôi luôn sử dụng ngữ cảnh như một biến cơ sở của critic. Chúng tôi chọn cấu hình này vì: (1) chúng tôi thấy thực nghiệm rằng việc có một tập hợp trọng số không đổi cho mỗi trạng thái quan trọng cho việc khái quát hóa; và (2) Vì ngữ cảnh PEARL có thể học được, chúng tôi thấy rằng khi gradient ngữ cảnh lan truyền ngược qua ba mạng (chính, động và mạng ngữ cảnh), nó cản trở việc huấn luyện. Thay vào đó, như một biến cơ sở, gradient của ngữ cảnh chỉ lan truyền ngược qua hai mạng như trong triển khai PEARL gốc.

4. Tái tổ hợp Chính sách trong Meta-RL

4.1. Bối cảnh
Meta-RL là sự khái quát hóa của Meta-Learning (Mishra et al., 2018; Sohn et al., 2019) sang miền RL. Nó nhằm mục tiêu học các meta-policy giải quyết một phân phối các tác vụ khác nhau p(T). Thay vì học các chính sách khác nhau cho mỗi tác vụ, meta-policy chia sẻ trọng số giữa tất cả các tác vụ và do đó có thể khái quát hóa từ tác vụ này sang tác vụ khác (Sung et al., 2017). Một thuật toán Meta-RL phổ biến là MAML (Finn et al., 2017), học một tập hợp trọng số có thể nhanh chóng thích ứng với một tác vụ mới với một vài bước gradient ascent. Để làm như vậy, đối với mỗi tác vụ, nó ước lượng gradient chính sách (Sutton et al., 2000) tại điểm thích ứng. Tổng gradient là tổng của các gradient chính sách trên phân phối tác vụ p(T):

∇_θ J_maml(θ) = E_{Ti~p(T)} [∑_{t=0}^∞ Â_{i,t} ∇_θ log π_θ(a_t|s_t)]

θ_i = θ + α E[∑_{t=0}^∞ Â_{i,t} ∇_θ log π_θ(a_t|s_t)]     (15)

trong đó Â_{i,t} là ước lượng lợi thế thực nghiệm tại bước thứ t trong tác vụ i (Schulman et al., 2015b). Các thuật toán trên chính sách có xu hướng gặp khó khăn về độ phức tạp mẫu cao vì mỗi bước cập nhật yêu cầu nhiều quỹ đạo mới được lấy mẫu từ chính sách gần đây nhất để đánh giá đầy đủ hướng gradient.

Các phương pháp ngoài chính sách được thiết kế để cải thiện độ phức tạp mẫu bằng cách tái sử dụng kinh nghiệm từ các chính sách cũ (Thomas và Brunskill, 2016). Mặc dù không nhất thiết liên quan, trong Meta-RL, nhiều thuật toán ngoài chính sách cũng tránh cách tiếp cận MAML của thích ứng trọng số. Thay vào đó, chúng chọn điều kiện hóa chính sách và hàm Q trên một ngữ cảnh phân biệt giữa các tác vụ khác nhau (Ren et al., 2019; Sung et al., 2017). Một phương pháp Meta-RL ngoài chính sách đáng chú ý là PEARL (Rakelly et al., 2019). Nó xây dựng dựa trên thuật toán SAC và học một hàm Q Q_φ(s,a,z), một chính sách π_θ(s,z) và một ngữ cảnh z ~ q(z|c_{Ti}). Ngữ cảnh, là một biểu diễn tiềm ẩn của tác vụ Ti, được tạo ra bởi một mô hình xác suất xử lý một quỹ đạo c_{Ti} của các chuyển tiếp (s,a,r) được lấy mẫu từ tác vụ Ti. Để học critic cùng với ngữ cảnh, PEARL sửa đổi hàm mất mát critic SAC thành

L_{critic}^{pearl}(φ,ψ) = E_{Ti} E_{z~q(z|c_{Ti})} [L_{critic}^{sac}(φ,ψ)] + βD_{KL}(q(z|c_{Ti})||p(z))

trong đó p(z) là xác suất tiên nghiệm trên phân phối tiềm ẩn của ngữ cảnh. Trong khi ngữ cảnh của PEARL là một mô hình xác suất, các công trình khác (Fakoor et al., 2019) đã đề xuất rằng một ngữ cảnh có thể học được xác định có thể cung cấp kết quả tương tự. Trong công trình này, chúng tôi xem xét cả ngữ cảnh có thể học được và cũng là cách tiếp cận đơn giản hơn của oracle-context c_{Ti} là một định danh duy nhất, được định nghĩa trước cho tác vụ i (Jayakumar et al., 2019). Nó có thể là một chỉ số khi có số lượng tác vụ đếm được hoặc một số liên tục khi các tác vụ được lấy mẫu từ một phân phối liên tục. Trong thực tế, định danh oracle thường được biết đến với agent. Hơn nữa, đôi khi, ví dụ trong các tác vụ hướng mục tiêu, ngữ cảnh không thể được khôi phục trực tiếp từ các tuple chuyển tiếp mà không có kiến thức tiên nghiệm, vì không có phần thưởng trừ khi mục tiêu được đạt tới, điều này hiếm khi xảy ra mà không có sự thích ứng chính sách.

4.2. Cách tiếp cận của chúng tôi
Hypernetwork tự nhiên phù hợp với công thức meta-learning trong đó ngữ cảnh là đầu vào cho mạng chính (von Oswald et al., 2019; Zhao et al., 2020). Do đó, chúng tôi đề xuất mô hình hóa meta-policy sao cho ngữ cảnh là biến meta và trạng thái là đầu vào của động lực

π(a|s,c) = f_{θ_w(c)}(π(ε|s)) s.t. ε ~ p_ε     (16)

Thú vị là, việc mô hình hóa này tách rời gradient phụ thuộc trạng thái và gradient phụ thuộc tác vụ của meta-policy. Để thấy điều đó, hãy lấy ví dụ mục tiêu trên chính sách của MAML và thay vào một chính sách phụ thuộc ngữ cảnh π(a|s,c) = π(ε|s,c). Sau đó, mục tiêu trong Eq. (15) trở thành

J(θ) = ∑_{Ti} ∑_{s_j∈Ti} Â_{i,j} ∇_θ π(ε_j|s_j,c_i) π(ε_j|s_j,c_i)     (17)

Áp dụng việc mô hình hóa Hypernetwork của meta-policy trong Eq. (16), mục tiêu này có thể được viết là

J(θ) = ∑_{Ti} ∇_θ θ_w(c_i) ∑_{s_j∈Ti} Â_{i,j} ∇_w f_{θ_w(c_i)}(ε_j|s_j) f_{θ_w(c_i)}(ε_j|s_j)     (18)

Trong dạng này, các gradient phụ thuộc trạng thái của trọng số động ∇_w f_{θ_w(c_i)}(ε_j,s_j) được trung bình độc lập cho mỗi tác vụ, và các gradient phụ thuộc tác vụ của trọng số chính ∇_θ θ_w(c_i) chỉ được trung bình trên phân phối tác vụ chứ không phải trên phân phối tác vụ-trạng thái chung như trong Eq. (17). Chúng tôi giả định rằng việc tách rời như vậy làm giảm nhiễu gradient cho cùng một số lượng mẫu. Điều này sẽ chuyển thành các bước học chính xác hơn và do đó một quá trình học hiệu quả hơn.

Để kiểm tra giả thuyết của chúng tôi, chúng tôi đã huấn luyện hai mô hình meta-policy khác nhau dựa trên thuật toán MAML: (1) một mô hình MLP trong đó một trạng thái và một oracle-context được kết hợp lại với nhau; và (2) một mô hình Hypernetwork, như được mô tả, với một oracle-context như một biến meta. Quan trọng là, lưu ý rằng, ngoài kiến trúc nơ-ron, cả hai thuật toán đều giống hệt nhau. Đối với bốn timestamp khác nhau trong quá trình học, chúng tôi đã xây dựng 50 gradient khác nhau không tương quan từ các tập khác nhau và đánh giá hiệu suất của chính sách được cập nhật. Chúng tôi lấy thống kê hiệu suất của các chính sách được cập nhật như một surrogate cho nhiễu gradient. Trong Hình 5, chúng tôi vẽ thống kê hiệu suất của các meta-policy được cập nhật. Chúng tôi thấy rằng phương sai của mô hình Hypernetwork thấp hơn đáng kể so với mô hình MLP trên tất cả các tác vụ và môi trường. Điều này cho thấy sự cải thiện hiệu quả hơn và do đó chúng tôi cũng quan sát thấy rằng giá trị trung bình cao hơn một cách nhất quán.

5. Thí nghiệm

5.1. Thiết lập thí nghiệm
Chúng tôi đã tiến hành các thí nghiệm trong trình mô phỏng MuJoCo (Todorov et al., 2012) và kiểm tra các thuật toán trên các môi trường benchmark có sẵn trong OpenAI Gym (Brockman et al., 2016). Đối với RL một tác vụ, chúng tôi đã đánh giá phương pháp của mình trên: (1) Hooper-v2; (2) Walker2D-v2; (3) Ant-v2; và (4) Half-Cheetah-v2. Đối với meta-RL, chúng tôi đã đánh giá phương pháp của mình trên: (1) Half-Cheetah-Fwd-Back và (2) Ant-Fwd-Back, và trên các tác vụ vận tốc: (3) Half-Cheetah-Vel và (4) Ant-Vel như được thực hiện trong (Rakelly et al., 2019). Chúng tôi cũng thêm môi trường Half-Cheetah-Vel-Medium như được trình bày trong (Fakoor et al., 2019), kiểm tra khả năng khái quát hóa ngoài phân phối. Đối với Context-MAML và Hyper-MAML, chúng tôi đã áp dụng oracle-context như đã thảo luận trong Mục 4. Đối với các tác vụ tiến-lùi, chúng tôi cung cấp một chỉ báo nhị phân, và đối với các tác vụ vận tốc, chúng tôi áp dụng một ngữ cảnh liên tục trong phạm vi [0,3] ánh xạ tới các vận tốc trong phân phối huấn luyện.

Trong các thí nghiệm RL, chúng tôi so sánh mô hình của mình với SAC và TD3, và trong Meta-RL, chúng tôi so sánh với MAML và PEARL. Chúng tôi sử dụng các triển khai chính thức của tác giả (hoặc triển khai PyTorch (Ketkar, 2017) mã nguồn mở khi bản chính thức không có sẵn) và các siêu tham số của baseline gốc, cũng như tuân thủ nghiêm ngặt quy trình đánh giá của mỗi thuật toán. Việc huấn luyện Hypernetwork được thực hiện với hàm mất mát baseline sao cho chúng tôi chỉ thay đổi mô hình mạng và điều chỉnh tốc độ học để phù hợp với kiến trúc khác nhau. Tất cả các thí nghiệm đều được trung bình trên 5 seed. Các chi tiết kỹ thuật thêm có trong phụ lục.

5.2. Kiến trúc Hypernetwork
Mô hình Hypernetwork của chúng tôi được minh họa trong Hình 1 và trong Mục 2. Khi thiết kế mô hình Hypernetwork, chúng tôi không tìm kiếm mô hình hiệu suất tốt nhất, thay vào đó chúng tôi tìm kiếm một so sánh thích hợp với kiến trúc MLP tiêu chuẩn được sử dụng trong RL (được ký hiệu ở đây là MLP-Standard). Với mục đích này, chúng tôi đã sử dụng một mạng động nhỏ hơn so với mô hình MLP (một lớp ẩn thay vì hai lớp và cùng số lượng nơ-ron (256) trong một lớp). Với cách tiếp cận này, chúng tôi muốn chỉ ra lợi ích của việc sử dụng trọng số động so với một tập hợp trọng số cố định trong mô hình MLP. Để nhấn mạnh lợi ích của trọng số động, chúng tôi đã thêm một baseline MLP-Small với cấu hình bằng với mô hình động (một lớp ẩn với 256 nơ-ron).

Không giống như mạng động, vai trò của mạng chính bị thiếu trong kiến trúc MLP. Do đó, đối với mạng chính, chúng tôi sử dụng một mô hình ResNet hiệu suất cao (Srivastava et al., 2015) mà chúng tôi thấy thích hợp để tạo ra tập hợp trọng số động (Glorot và Bengio, 2010). Để đảm bảo rằng lợi ích hiệu suất không phải do tính biểu cảm của mô hình ResNet hoặc số lượng trọng số có thể học được bổ sung, chúng tôi đã thêm ba baseline nữa: (1) ResNet Features: cùng kiến trúc chính và động, nhưng đầu ra của mạng chính là một vector đặc trưng trạng thái được nối với hành động như đầu vào cho một mạng MLP-Standard; (2) MLP-Large: hai lớp ẩn, mỗi lớp có 2900 nơ-ron tổng cộng 9M trọng số như trong kiến trúc Hypernetwork; và (3) Res35: ResNet với 35 khối để tạo ra giá trị Q, tổng cộng 4,5M trọng số. Ngoài ra, chúng tôi đã thêm một so sánh với mô hình Q-D2RL: một kiến trúc sâu dày đặc cho hàm Q được đề xuất gần đây trong (Sinha et al., 2020).

Một vấn đề quan trọng với Hypernetwork là tính ổn định số học của chúng. Chúng tôi thấy rằng chúng đặc biệt nhạy cảm với khởi tạo trọng số vì khởi tạo chính tồi có thể khuếch đại thành các trọng số động thảm khốc (Chang et al., 2019). Chúng tôi giải quyết vấn đề này bằng cách khởi tạo mạng chính sao cho phân phối trọng số động khởi tạo trung bình giống với khởi tạo Kaiming-uniform (He et al., 2015). Các chi tiết thêm có thể được tìm thấy trong phụ lục.

5.3. Kết quả
Các kết quả và so sánh với các baseline được tóm tắt trong Hình 6. Trong tất cả bốn thí nghiệm, mô hình Hypernetwork của chúng tôi đạt được trung bình 10% - 70% lợi ích so với baseline MLP-Standard trong hiệu suất cuối cùng và đạt điểm số của baseline chỉ với 20%-70% tổng số bước huấn luyện. Như được mô tả trong Mục 5.2, đối với các thí nghiệm RL, ngoài mô hình MLP-Standard, chúng tôi đã kiểm tra năm baseline khác: (1) MLP-Large; (2) MLP-Small; (3) ResNet Features; (4) ResNet35; và (5) Q-D2RL. Cả trên TD3 và SAC, chúng tôi thấy một cải thiện nhất quán so với tất cả các baseline và SA-Hyper vượt trội trong tất cả các môi trường với hai ngoại lệ: nơi MLP-Large hoặc Q-D2RL đạt điểm số tốt hơn SA-Hyper trong môi trường Ant-v2 (các đường cong học cho mỗi môi trường được tìm thấy trong phụ lục).

Mặc dù có vẻ như cải thiện Hypernetwork là do chiều tham số lớn hoặc thiết kế ResNet của mô hình chính, kết quả của chúng tôi cung cấp bằng chứng mạnh mẽ rằng giả định này không đúng. Mô hình SA-Hyper vượt trội hơn các mô hình khác có cùng số lượng tham số (MLP-Large và ResNet Features) và cũng các mô hình sử dụng kiến trúc ResNet (ResNet Features và Res35). Ngoài ra, nó cũng tốt (SAC) hoặc tốt hơn (TD3) so với Q-D2RL, được đề xuất gần đây như một kiến trúc được thiết kế riêng cho bài toán RL (Sinha et al., 2020). Xin lưu ý rằng như đã thảo luận trong Mục 5.2 và không giống như D2RL, chúng tôi không tối ưu hóa số lượng lớp trong mô hình động.

Trong Hình 6c, chúng tôi so sánh các mô hình khác nhau cho MAML: (1) Vanilla-MAML; (2) Context-MAML, tức là phiên bản dựa trên ngữ cảnh của MAML với oracle-context; và (3) Hyper-MAML, tương tự như context-MAML nhưng với mô hình Hypernetwork. Đối với tất cả các mô hình, chúng tôi đánh giá cả điểm số trước thích ứng (pre-ad) cũng như sau thích ứng. Đầu tiên, chúng tôi xác minh tuyên bố trong (Fakoor et al., 2019) rằng ngữ cảnh có lợi cho các thuật toán Meta-RL giống như Context-MAML vượt trội hơn Vanilla-MAML. Tuy nhiên, chúng tôi thấy rằng Hyper-MAML vượt trội hơn Context-MAML khoảng 50%. Hơn nữa, không giống như các mô hình MLP tiêu chuẩn, chúng tôi thấy rằng Hyper-MAML không yêu cầu bất kỳ bước thích ứng nào (không có sự khác biệt có thể quan sát được giữa điểm số trước và sau thích ứng). Chúng tôi giả định rằng kết quả này là do khả năng khái quát hóa tốt hơn của kiến trúc Hypernetwork như cũng có thể thấy từ các thí nghiệm PEARL tiếp theo.

Trong Hình 6d, chúng tôi đánh giá mô hình Hypernetwork với thuật toán PEARL. Ngữ cảnh được học với một bộ mã hóa xác suất như được trình bày trong (Rakelly et al., 2019) sao cho sự khác biệt duy nhất với PEARL gốc là các mô hình nơ-ron chính sách và critic. Các kết quả thực nghiệm cho thấy rằng Hyper-PEARL vượt trội hơn baseline MLP cả về hiệu suất cuối cùng (15%) và hiệu quả mẫu (70% ít bước hơn để đạt điểm số baseline cuối cùng). Quan trọng nhất, chúng tôi thấy rằng Hyper-PEARL khái quát hóa tốt hơn cho các tác vụ kiểm tra chưa thấy. Điều này áp dụng cho cả các tác vụ kiểm tra được lấy mẫu từ phân phối huấn luyện (như điểm số cao hơn và phương sai thấp hơn của Hyper-PEARL cho thấy) và cũng cho các tác vụ Ngoài-Phân-phối (OOD), như có thể quan sát trong Hình 7.

6. Kết luận
Trong công trình này, chúng tôi bắt đầu nghiên cứu các mô hình nơ-ron cho các khối xây dựng RL: hàm Q và meta-policy. Lập luận rằng bản chất độc đáo của thiết lập RL yêu cầu các mô hình không thông thường, chúng tôi đã đề xuất mô hình Hypernetwork và chỉ ra thực nghiệm một số lợi thế đáng kể so với các mô hình MLP. Đầu tiên, Hypernetwork có khả năng ước lượng tín hiệu gradient tham số của hàm Q cần thiết để huấn luyện các thuật toán actor-critic tốt hơn. Thứ hai, chúng giảm phương sai gradient trong việc huấn luyện meta-policy trong Meta-RL. Cuối cùng, chúng cải thiện khả năng khái quát hóa OOD và không yêu cầu bất kỳ bước thích ứng nào trong việc huấn luyện Meta-RL, điều này tạo điều kiện thuận lợi đáng kể cho quá trình huấn luyện.

7. Mã nguồn
Triển khai PyTorch Hypernetwork của chúng tôi được tìm thấy tại https://github.com/keynans/HypeRL .

--- CÁC TRANG TIẾP THEO ---
[Phần còn lại bao gồm tài liệu tham khảo, chứng minh, và các phụ lục chi tiết - tôi sẽ tiếp tục dịch nếu cần]
