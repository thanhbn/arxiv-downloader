# Hypernetwork cho Chuyển giao Không cần Huấn luyện trong Học Tăng cường
Sahand Rezaei-Shoshtari1,2,3, Charlotte Morissette1,3, Francois R. Hogan3
Gregory Dudek1,2,3, David Meger1,2,3
1Đại học McGill2Mila - Viện AI Quebec3Trung tâm AI Samsung Montreal
srezaei@cim.mcgill.ca

Tóm tắt
Trong bài báo này, hypernetwork được huấn luyện để tạo ra các hành vi qua một loạt các điều kiện tác vụ chưa thấy, thông qua một mục tiêu huấn luyện mới dựa trên TD và dữ liệu từ một tập hợp các giải pháp RL gần tối ưu cho các tác vụ huấn luyện. Công trình này liên quan đến meta RL, contextual RL, và học chuyển giao, với trọng tâm đặc biệt vào hiệu suất không cần huấn luyện tại thời điểm kiểm tra, được hỗ trợ bởi kiến thức về các tham số tác vụ (còn được gọi là ngữ cảnh). Cách tiếp cận kỹ thuật của chúng tôi dựa trên việc xem mỗi thuật toán RL như một ánh xạ từ các đặc tả MDP đến hàm giá trị và chính sách gần tối ưu và tìm cách xấp xỉ nó bằng một hypernetwork có thể tạo ra các hàm giá trị và chính sách gần tối ưu, cho trước các tham số của MDP. Chúng tôi chỉ ra rằng, dưới những điều kiện nhất định, ánh xạ này có thể được coi như một bài toán học có giám sát. Chúng tôi đánh giá thực nghiệm tính hiệu quả của phương pháp chúng tôi cho chuyển giao không cần huấn luyện đến các động lực học chuyển đổi và phần thưởng mới trên một loạt các tác vụ điều khiển liên tục từ DeepMind Control Suite. Phương pháp của chúng tôi thể hiện những cải thiện đáng kể so với các baseline từ các cách tiếp cận đa tác vụ và meta RL.

1 Giới thiệu
Con người trưởng thành sở hữu khả năng đáng kinh ngạc để thích nghi hành vi của họ với các tình huống mới. Vượt xa việc điều chỉnh đơn giản, chúng ta có thể áp dụng những cách di chuyển cơ thể hoàn toàn mới, ví dụ như đi lại bằng nạng với rất ít hoặc không cần huấn luyện sau khi bị thương. Quá trình học tập khái quát hóa qua tất cả kinh nghiệm và chế độ hành vi quá khứ để nhanh chóng đưa ra chính sách hành vi cần thiết cho một tình huống mới là dấu hiệu đặc trưng của trí thông minh của chúng ta.

Bài báo này đề xuất một cách tiếp cận khái quát hóa hành vi không cần huấn luyện mạnh mẽ dựa trên hypernetwork (Ha, Dai, and Le 2016), một kiến trúc được đề xuất gần đây cho phép một siêu học viên sâu xuất tất cả các tham số của một mạng thần kinh đích, như được mô tả trong Hình 1. Trong trường hợp của chúng tôi, chúng tôi huấn luyện trên các giải pháp đầy đủ của nhiều bài toán RL trong một họ MDP, nơi phần thưởng hoặc động lực học (thường là cả hai) có thể thay đổi giữa các thực thể tác vụ. Các chính sách được huấn luyện, hàm giá trị và hành vi tối ưu được triển khai của mỗi tác vụ nguồn là thông tin huấn luyện từ đó chúng ta có thể học cách khái quát hóa. Hypernetwork của chúng tôi xuất các tham số của một chính sách được hình thành đầy đủ và có hiệu suất cao mà không cần bất kỳ kinh nghiệm nào trong một tác vụ liên quan nhưng chưa thấy, chỉ đơn giản bằng cách điều kiện hóa trên các tham số tác vụ được cung cấp.

Sự khác biệt giữa các tác vụ mà chúng tôi xem xét dẫn đến những thay đổi lớn và phức tạp trong chính sách tối ưu và phân phối quỹ đạo tối ưu được tạo ra. Học cách dự đoán các chính sách mới từ dữ liệu này đòi hỏi những người học mạnh mẽ được hướng dẫn bởi các hàm mất mát hữu ích. Chúng tôi chỉ ra rằng các tính chất trừu tượng và mô-đun do hypernetwork cung cấp cho phép chúng xấp xỉ các giải pháp được tạo ra bởi RL bằng cách ánh xạ một họ MDP được tham số hóa đến một tập hợp các giải pháp tối ưu. Chúng tôi chỉ ra rằng khung này cho phép đạt được chuyển giao không cần huấn luyện mạnh mẽ đến các cài đặt phần thưởng và động lực học mới bằng cách khai thác các điểm chung trong cấu trúc MDP.

Chúng tôi thực hiện xác thực thực nghiệm bằng cách sử dụng một số họ môi trường điều khiển liên tục nơi chúng tôi đã tham số hóa động lực học vật lý, phần thưởng tác vụ, hoặc cả hai để đánh giá người học. Chúng tôi thực hiện đánh giá không cần huấn luyện theo ngữ cảnh, nơi người học được cung cấp các tham số của tác vụ kiểm tra, nhưng không được cung cấp thời gian huấn luyện nào – thay vào đó, việc thực thi chính sách đầu tiên tại thời điểm kiểm tra được sử dụng để đo lường hiệu suất. Phương pháp của chúng tôi vượt trội so với các baseline nổi tiếng được lựa chọn, trong nhiều trường hợp phục hồi gần như hiệu suất đầy đủ mà không cần một bước thời gian dữ liệu huấn luyện nào trên các tác vụ đích.

Ablation cho thấy rằng hypernetwork là một yếu tố quan trọng trong việc đạt được khái quát hóa mạnh mẽ và rằng một mất mát có cấu trúc giống TD còn hữu ích thêm trong việc huấn luyện các mạng này.

Các đóng góp chính của chúng tôi là:

1. Việc sử dụng hypernetwork như một cách tiếp cận có thể mở rộng và thực tế để xấp xỉ các thuật toán RL như một ánh xạ từ một họ MDP được tham số hóa đến một họ chính sách gần tối ưu.

2. Một mất mát dựa trên TD để điều chỉnh các chính sách và hàm giá trị được tạo ra để nhất quán với phương trình Bellman.

3. Một loạt các môi trường điều khiển liên tục mô-đun và có thể tùy chỉnh để học chuyển giao qua các tham số phần thưởng và động lực học khác nhau.

Mã học tập, bộ dữ liệu được tạo ra, và các môi trường điều khiển liên tục tùy chỉnh của chúng tôi, được xây dựng dựa trên DeepMind Control Suite, có sẵn công khai tại:
https://sites.google.com/view/hyperzero-rl

2 Bối cảnh

2.1 Quá trình Quyết định Markov

Chúng tôi xem xét MDP tiêu chuẩn được định nghĩa bởi một 5-tuple M = (S; A; T; R; γ), trong đó S là không gian trạng thái, A là không gian hành động, T: S × A → Dist(S) là động lực học chuyển đổi, R: S × A → R là hàm phần thưởng và γ ∈ (0, 1] là hệ số chiết khấu. Mục tiêu của một thuật toán RL là tìm một chính sách π: S → Dist(A) tối đa hóa lợi nhuận kỳ vọng được định nghĩa là E[Rt] = E[∑T k=0 γk rt+k+1]. Hàm giá trị Vπ(s) biểu thị lợi nhuận kỳ vọng từ s dưới chính sách π, và tương tự hàm giá trị hành động Qπ(s, a) biểu thị lợi nhuận kỳ vọng từ s sau khi thực hiện hành động a dưới chính sách π:

Qπ(s, a) = Es′,r∼p(·|s,a),a′∼π(·|s′)[∑∞ k=0 γk rt+k+1|s, a]

Các hàm giá trị là điểm cố định của phương trình Bellman (Bellman 1966), hoặc tương đương toán tử Bellman Bπ:

Bπ[Qπ(s, a)] = Es′,r∼p(·|s,a),a′∼π(·|s′)[r + γQπ(s′, a′)]

Tương tự, các hàm giá trị tối ưu V*(s) và Q*(s, a) là các điểm cố định của toán tử tối ưu Bellman B*.

2.2 Hàm Giá trị Tổng quát

Các hàm giá trị tổng quát (GVF) mở rộng định nghĩa tiêu chuẩn của hàm giá trị Qπ(s, a) để bao gồm hàm phần thưởng, động lực học chuyển đổi và hệ số chiết khấu ngoài chính sách, tức là Qπ,R,T,γ(s, a) (Sutton and Barto 2018). Các bộ xấp xỉ hàm giá trị phổ quát (UVFA) (Schaul et al. 2015) là một thực thể của GVF trong đó hàm giá trị được khái quát hóa qua các mục tiêu g và được biểu diễn là Qπ(s, a, g). Tự nhiên, khái niệm này được sử dụng trong RL có điều kiện mục tiêu (Andrychowicz et al. 2017) và RL đa tác vụ (Teh et al. 2017). Liên quan, cải thiện chính sách tổng quát (GPI) nhằm cải thiện một chính sách khái quát dựa trên chuyển đổi của một số MDP (Barreto et al. 2020; Harb et al. 2020; Faccio et al. 2022b). Mục tiêu của phương pháp chúng tôi trong việc học một ánh xạ khái quát từ đặc tả MDP đến các chính sách và hàm giá trị gần tối ưu liên quan chặt chẽ đến mục tiêu tổng thể của GVF và GPI. Tuy nhiên, không giống như các phương pháp đó, chúng tôi không tìm cách cải thiện một chính sách khái quát đã cho.

2.3 Hypernetwork

Một hypernetwork (Ha, Dai, and Le 2016) là một mạng thần kinh tạo ra các trọng số của một mạng khác, thường được gọi là mạng chính. Trong khi cả hai mạng đều có các trọng số liên kết, chỉ có trọng số hypernetwork liên quan đến các tham số có thể học được cập nhật trong quá trình huấn luyện. Trong quá trình suy luận, chỉ có mạng chính được sử dụng bằng cách ánh xạ một đầu vào đến một đích mong muốn, sử dụng các trọng số được tạo ra bởi hypernetwork. Vì các trọng số của các lớp khác nhau của mạng chính được tạo ra thông qua một embedding đã học chung, hypernetwork có thể được xem như một dạng chia sẻ trọng số thoải mái qua các lớp. Đã được chứng minh thực nghiệm rằng cách tiếp cận này cho phép một mức độ trừu tượng và tính mô-đun của bài toán học tập (Galanti and Wolf 2020; Ha, Dai, and Le 2016) từ đó dẫn đến việc học hiệu quả hơn. Đáng chú ý, hypernetwork có thể được điều kiện hóa trên vector ngữ cảnh để tạo ra có điều kiện các trọng số của mạng chính (von Oswald et al. 2019). Tương tự như von Oswald et al. (2019), chúng tôi điều kiện hóa hypernetwork trên các tham số (ngữ cảnh) của MDP để tạo ra chính sách và hàm giá trị gần tối ưu dựa trên các tham số phần thưởng và động lực học.

3 HyperZero

Mục tiêu toàn diện của công trình này là phát triển một khung cho phép xấp xỉ các giải pháp RL bằng cách học ánh xạ giữa đặc tả MDP và chính sách gần tối ưu. Một xấp xỉ hợp lý có thể cho phép chuyển giao không cần huấn luyện và dự đoán hành vi chung của một tác nhân RL trước khi huấn luyện nó. Ngoài các tiền đề tiêu chuẩn của học chuyển giao không cần huấn luyện (Taylor and Stone 2009; Tan et al. 2018), một ánh xạ được xấp xỉ tốt từ MDP đến các chính sách gần tối ưu có thể có ứng dụng trong việc định hình phần thưởng, trực quan hóa tác vụ và thiết kế môi trường.

3.1 Công thức Bài toán

Phần này nêu ra các giả định và công thức bài toán được sử dụng trong bài báo này. Đầu tiên, chúng tôi định nghĩa họ MDP được tham số hóa M như:

Định nghĩa 1 (Họ MDP được Tham số hóa). Một họ MDP được tham số hóa M là một tập hợp các MDP có cùng không gian trạng thái S, không gian hành động A, động lực học chuyển đổi được tham số hóa Tθ, hàm phần thưởng được tham số hóa Rφ, và hệ số chiết khấu γ:

M = {Mi|Mi = (S, A, Tθi, Rφi, γ)},

trong đó φi ∼ p(φ) và θi ∼ p(θ) là các tham số của Mi, và được giả định là được lấy mẫu từ các phân phối tiên nghiệm.

Đáng chú ý, không gian trạng thái S và không gian hành động A trong định nghĩa của chúng tôi có thể là không gian rời rạc hoặc liên tục (ví dụ: một không gian con mở của Rn). Định nghĩa của chúng tôi về một họ MDP được tham số hóa liên quan đến MDP theo ngữ cảnh (Hallak, Di Castro, and Mannor 2015; Jiang et al. 2017), nơi người học có quyền truy cập vào ngữ cảnh.

Chìa khóa cho xấp xỉ của chúng tôi là giả định rằng một thuật toán RL, một khi hội tụ, là một ánh xạ từ một MDP Mi ∈ M đến một chính sách gần tối ưu và hàm giá trị hành động gần tối ưu tương ứng với MDP cụ thể Mi mà nó được huấn luyện. Với một sự lạm dụng ký hiệu nhẹ, chúng tôi ký hiệu chính sách gần tối ưu là πi* và hàm giá trị hành động gần tối ưu là Qi*:

Mi → Thuật toán RL → πi*(a|s); Qi*(s, a).      (1)

Đáng chú ý, quan điểm này có tiền lệ trong các công trình trước về học cách định hình phần thưởng (Sorg, Lewis, and Singh 2010; Zheng, Oh, and Singh 2018; Zheng et al. 2020), meta-gradient trong RL (Xu, van Hasselt, and Silver 2018; Xu et al. 2020), và quan điểm toán tử của các thuật toán RL (Tang, Feng, and Liu 2022).

Vì mục tiêu của chúng tôi là học ánh xạ của Phương trình (1) từ một họ MDP được tham số hóa có cùng dạng hàm của động lực học chuyển đổi được tham số hóa Tθ và hàm phần thưởng Rφ, chúng tôi giả định rằng MDP Mi có thể được đặc trưng hoàn toàn bởi các tham số φi và θi và các dạng hàm của Rφ và Tθ; tức là Mi ≡ M(φi, θi). Phương trình (1) có thể được đơn giản hóa thành:

M(φi, θi) → Thuật toán RL → π(a|s, φi, θi); Q(s, a|φi, θi);      (2)

trong đó các chính sách và hàm giá trị hành động gần tối ưu bây giờ là các hàm của các tham số phần thưởng φi và tham số động lực học θi, ngoài các đầu vào tiêu chuẩn của chúng. Đáng chú ý, công thức này liên quan chặt chẽ đến các công trình trước về RL có điều kiện mục tiêu (Andrychowicz et al. 2017; Schroecker and Isbell 2020), và các bộ xấp xỉ hàm giá trị phổ quát (UVFA) (Schaul et al. 2015; Borsa et al. 2018).

Do đó, bài toán của chúng tôi được định nghĩa chính thức là xấp xỉ ánh xạ được hiển thị trong Phương trình (2) để có được hàm giá trị hành động gần tối ưu được xấp xỉ Q̂(s, a|φ, θ) và chính sách π̂(a|s, φ, θ) được tham số hóa bởi ψ và ω, tương ứng. Một khi ánh xạ như vậy được có được, người ta có thể dự đoán và quan sát các quỹ đạo gần tối ưu bằng cách triển khai chính sách được xấp xỉ π̂ mà không cần thiết phải huấn luyện bộ giải RL từ đầu:

π̂(a|s, φ, θ) → Triển khai Chính sách trong Môi trường → τ̂(φ, θ),      (3)

trong đó τ̂(φ, θ) là quỹ đạo gần tối ưu tương ứng với các tham số phần thưởng φ và tham số động lực học θ.

3.2 Tạo ra Các Chính sách Tối ưu và Hàm Giá trị Tối ưu với Hypernetwork

Mục tiêu của chúng tôi là xấp xỉ ánh xạ được mô tả trong Phương trình (2). Để làm điều đó, chúng tôi giả định có quyền truy cập vào một họ các chính sách gần tối ưu πi* đã được huấn luyện độc lập trên các thực thể của Mi ∈ M. Một bộ dữ liệu các quỹ đạo gần tối ưu sau đó được thu thập bằng cách triển khai mỗi πi* trên MDP tương ứng Mi. Do đó, các mẫu được rút ra từ phân phối trạng thái ổn định của chính sách gần tối ưu dπi*(s).

Do đó, các đầu vào cho người học là các tuple trạng thái, tham số phần thưởng và tham số động lực học ⟨s, φi, θi⟩ và các mục tiêu là các tuple hành động và giá trị hành động gần tối ưu ⟨a, q⟩. Chúng ta có thể đóng khung bài toán xấp xỉ của Phương trình (2) như một bài toán học có giám sát dưới các điều kiện sau:

Giả định 1. Các tham số của hàm phần thưởng Rφ và động lực học chuyển đổi Tθ được lấy mẫu độc lập và đồng nhất từ các phân phối trên các tham số φi ∼ p(φ) và θi ∼ p(θ), tương ứng.

Giả định 2. Thuật toán RL được xấp xỉ, như được hiển thị trong Phương trình (1) và (2), hội tụ đến hàm giá trị và chính sách gần tối ưu.

Giả định 1 là một giả định phổ biến về phân phối tác vụ trong các phương pháp meta-learning (Finn, Abbeel, and Levine 2017). Trong khi Giả định 2 có vẻ mạnh, nó liên quan đến giả định phổ biến được đưa ra trong học bắt chước nơi người học có quyền truy cập vào các demo chuyên gia (Ross, Gordon, and Bagnell 2011; Ho and Ermon 2016). Tuy nhiên, chúng tôi chỉ ra thực nghiệm rằng Giả định 2 có thể được nới lỏng đến một mức độ trong thực tế, trong khi vẫn đạt được hiệu suất không cần huấn luyện mạnh mẽ, như được hiển thị trong Phần 4.

Quan trọng, chúng tôi không giả định kiến thức trước về cấu trúc của thuật toán RL cũng như về bản chất (ngẫu nhiên hay xác định) của chính sách đã được sử dụng để tạo ra dữ liệu. Đáng chú ý, vì chính sách tối ưu của một MDP đã cho là xác định (Puterman 2014), chúng ta có thể tham số hóa chính sách gần tối ưu được xấp xỉ π̂ như một hàm xác định π̂: S → A, mà không mất tính tối ưu.

Chúng tôi đề xuất sử dụng hypernetwork (Ha, Dai, and Le 2016) để giải quyết bài toán xấp xỉ này. Được điều kiện hóa trên các tham số φi, θi của một MDP Mi ∈ M, như được hiển thị trong Hình 2, hypernetwork H tạo ra các trọng số của chính sách gần tối ưu được xấp xỉ π̂ và hàm giá trị hành động Q̂. Theo tài liệu về hypernetwork, chúng tôi gọi các mạng chính sách và giá trị được tạo ra là các mạng chính.

Do đó, hypernetwork được huấn luyện thông qua việc giảm thiểu lỗi để dự đoán hành động và giá trị gần tối ưu bằng cách chuyển tiếp qua các mạng chính:

Lpred.(ψ) = E(φi,θi,s,a,q)∼D[(Q̂ψi(s, a) - q)²] + E(φi,θi,s,a)∼D[(π̂ψi(s) - a)²]      (4)

trong đó [ψi, ωi] = H(φi, θi) và D là bộ dữ liệu các quỹ đạo gần tối ưu được thu thập từ họ MDP M. Đáng chú ý, mô hình huấn luyện này hiệu quả tách rời bài toán học các giá trị/hành động tối ưu khỏi bài toán học ánh xạ các tham số MDP đến không gian các hàm giá trị và chính sách tối ưu. Do đó, như đã quan sát trong các công trình khác về hypernetwork (Ha, Dai, and Le 2016; Galanti and Wolf 2020; von Oswald et al. 2019; Faccio et al. 2022a), mức độ mô-đun này dẫn đến việc học đơn giản hóa và hiệu quả hơn.

3.3 Điều chỉnh Khác biệt Thời gian

Một thách thức chính trong việc sử dụng các cách tiếp cận học có giám sát cho xấp xỉ hàm trong RL sâu là sự tương quan thời gian tồn tại trong các mẫu, dẫn đến vi phạm giả định i.i.d. Các thực hành phổ biến trong RL sâu để ổn định việc học là sử dụng mạng đích để ước tính lỗi khác biệt thời gian (TD) (Lillicrap et al. 2015; Mnih et al. 2013). Trong bài báo này, chúng tôi đề xuất một kỹ thuật điều chỉnh mới dựa trên mất mát TD để ổn định việc huấn luyện hypernetwork cho học chuyển giao không cần huấn luyện.

Như đã nêu trong Giả định 2, chúng tôi giả định có quyền truy cập vào các giải pháp RL gần tối ưu đã được tạo ra từ một thuật toán RL hội tụ. Kết quả là, khung của chúng tôi khác với các công trình về học bắt chước (Ross, Gordon, and Bagnell 2011; Bagnell 2015; Ho and Ermon 2016) vì các mẫu thỏa mãn phương trình Bellman Tối ưu của MDP cơ bản Mi ∈ M và, quan trọng hơn, chúng tôi có quyền truy cập vào các giá trị hành động gần tối ưu q cho một mẫu chuyển đổi đã cho ⟨s, a, s′, r⟩.

Do đó, chúng tôi đề xuất sử dụng mất mát TD để điều chỉnh critic được xấp xỉ Q̂ bằng cách di chuyển giá trị đích được dự đoán về phía ước tính giá trị hiện tại, có thể thu được từ thuật toán RL ground-truth:

LTD(ψ) = E(φi,θi,s,a,s′,r,q)∼D[(r + γQ̂ψi(s′, a′) - q)²]      (5)

trong đó a′ được thu được từ chính sách xác định được xấp xỉ π̂ψi(s′) với gradient đã dừng. Lưu ý rằng ứng dụng của chúng tôi về mất mát TD khác với việc xấp xỉ hàm tiêu chuẩn trong RL sâu (Mnih et al. 2013; Lillicrap et al. 2015); thay vì di chuyển ước tính giá trị hiện tại về phía các ước tính đích, mất mát TD của chúng tôi di chuyển các ước tính đích về phía các ước tính hiện tại. Trong khi điều này dựa trên Giả định 2, chúng tôi chỉ ra rằng trong thực tế áp dụng mất mát TD có lợi vì nó bắt buộc chính sách và critic được xấp xỉ phải nhất quán với nhau đối với phương trình Bellman. Thuật toán 1 hiển thị pseudo-code của khung học tập của chúng tôi.

4 Đánh giá

Chúng tôi đánh giá phương pháp được đề xuất của chúng tôi, được gọi là HyperZero (hypernetwork cho chuyển giao không cần huấn luyện) trên một loạt các tác vụ điều khiển liên tục đầy thử thách từ DeepMind Control Suite. Mục tiêu chính trong các thí nghiệm của chúng tôi là nghiên cứu khả năng chuyển giao không cần huấn luyện của các giải pháp RL được xấp xỉ đến các cài đặt động lực học và phần thưởng mới.

4.1 Thiết lập Thí nghiệm

Môi trường. Chúng tôi sử dụng ba môi trường đầy thử thách để đánh giá: cheetah, walker, và finger. Để dễ trực quan hóa và hiện thực hóa các tham số phần thưởng, trong tất cả các trường hợp, các tham số phần thưởng tương ứng với tốc độ mong muốn của chuyển động bao gồm cả giá trị âm (di chuyển lùi) và dương (di chuyển tiến). Tùy thuộc vào môi trường, các thay đổi động lực học tương ứng với các thay đổi trong kích thước cơ thể và trọng lượng/quán tính của nó. Chi tiết đầy đủ về các môi trường và tham số của chúng trong Phụ lục A.

Huấn luyện RL và Thu thập Dữ liệu. Chúng tôi sử dụng TD3 (Fujimoto, Hoof, and Meger 2018) như thuật toán RL được xấp xỉ. Mỗi MDP Mi ∈ M, được tạo ra bằng cách lấy mẫu φi ∼ p(φ) và θi ∼ p(θ), được sử dụng để huấn luyện độc lập một tác nhân TD3 tiêu chuẩn trên các trạng thái proprioceptive trong 1 triệu bước. Do đó, giải pháp cuối cùng được sử dụng để tạo ra 10 lần triển khai để thêm vào bộ dữ liệu D. Đường cong học tập cho các giải pháp RL trong Phụ lục B.3. Như các kết quả này cho thấy, trong một số trường hợp, giải pháp RL không hoàn toàn hội tụ sau 1 triệu bước. Mặc dù vậy, HyperZero có thể xấp xỉ ánh xạ khá tốt, do đó chỉ ra Giả định 2 có thể được nới lỏng đến một mức độ trong thực tế.

Chia Train/Test của Các Tác vụ. Để đánh giá đáng tin cậy khả năng chuyển giao không cần huấn luyện của HyperZero đến các cài đặt phần thưởng/động lực học mới so với các baseline, và để loại trừ khả năng lựa chọn chọn lọc các tác vụ train/test, chúng tôi ngẫu nhiên chia các cài đặt tác vụ thành tập train (~85%) và test (~15%). Chúng tôi sau đó báo cáo trung bình và độ lệch chuẩn của lợi nhuận trung bình thu được trên 5 seeds.

Baseline. Chúng tôi so sánh HyperZero với các baseline phổ biến cho đa tác vụ và meta learning:

1. Chính sách điều kiện theo ngữ cảnh; được huấn luyện để dự đoán hành động, tương tự như các phương pháp học bắt chước.

2. Chính sách điều kiện theo ngữ cảnh được ghép nối với UVFA (Schaul et al. 2015); được huấn luyện để dự đoán hành động và giá trị. Nó còn được hưởng lợi từ việc sử dụng mất mát TD đề xuất của chúng tôi LTD, tương tự như HyperZero.

3. Meta chính sách điều kiện theo ngữ cảnh; được huấn luyện với MAML (Finn, Abbeel, and Levine 2017) để dự đoán hành động và được đánh giá cho cả chuyển giao không cần huấn luyện và few-shot. Meta chính sách điều kiện theo ngữ cảnh của chúng tôi có thể được coi là một sự thích nghi của PEARL (Rakelly et al. 2019) trong đó tác vụ được suy ra được thay thế bởi tác vụ ground-truth.

4. Chính sách PEARL (Rakelly et al. 2019); được huấn luyện để dự đoán hành động. Không giống như các baseline khác, PEARL không giả định quyền truy cập vào ngữ cảnh MDP và thay vào đó nó suy ra ngữ cảnh từ các trạng thái và hành động.

Đáng chú ý, vì MAML và PEARL được biết là hoạt động kém cho chuyển giao không cần huấn luyện, chúng tôi đánh giá meta chính sách cho cả chuyển giao không cần huấn luyện và few-shot. Trong trường hợp sau, trước khi đánh giá, meta chính sách được tinh chỉnh với các quỹ đạo gần tối ưu của MDP test Mi được tạo ra bởi giải pháp RL thực tế.

Cuối cùng, để so sánh công bằng với hypernetwork, tất cả các phương pháp đều tuân theo mô hình huấn luyện hai giai đoạn tương tự được mô tả trong Phần 3.1, có task embedding có thể học được và chia sẻ cùng kiến trúc mạng. Chi tiết triển khai đầy đủ trong Phụ lục C.

4.2 Kết quả

Chuyển giao Không cần Huấn luyện. Chúng tôi so sánh chuyển giao không cần huấn luyện của HyperZero so với các baseline trong ba trường hợp thay đổi phần thưởng, thay đổi động lực học, và đồng thời thay đổi phần thưởng và động lực học; kết quả được hiển thị trong Hình 3, 4, và 5, tương ứng. Kết quả bổ sung trong Phụ lục B.1. Như các kết quả này gợi ý, trong tất cả môi trường và kịch bản chuyển giao, HyperZero vượt trội đáng kể so với các baseline, chứng minh tính hiệu quả của khung học tập của chúng tôi để xấp xỉ một thuật toán RL như một ánh xạ từ một MDP được tham số hóa Mi đến một chính sách gần tối ưu πi* và hàm giá trị hành động Qi*.

Quan trọng, chính sách điều kiện theo ngữ cảnh (được ghép nối với UVFA) bao gồm tất cả các thành phần chính của HyperZero, bao gồm dự đoán hành động và giá trị gần tối ưu, và điều chỉnh TD. Kết quả là, sự khác biệt duy nhất là HyperZero học tạo ra các chính sách được điều kiện theo ngữ cảnh sau đó được sử dụng để dự đoán hành động, trong khi chính sách điều kiện theo ngữ cảnh học dự đoán hành động được điều kiện theo ngữ cảnh. Chúng tôi giả thuyết hai lý do chính cho những cải thiện đáng kể đạt được từ việc sử dụng hypernetwork như vậy trong cài đặt của chúng tôi. Thứ nhất, phù hợp với các quan sát tương tự trong tài liệu (Galanti and Wolf 2020; von Oswald et al. 2019), hypernetwork cho phép trừu tượng hóa hiệu quả bài toán học tập thành hai mức tạo ra chính sách (hoặc tương đương hàm giá trị) và dự đoán hành động (hoặc tương đương giá trị).

Thứ hai, vì hypernetwork được sử dụng để học ánh xạ từ các tham số MDP đến không gian chính sách, tức là (φi, θi) → πi*, chúng đạt được khái quát hóa qua không gian chính sách. Ngược lại, vì chính sách điều kiện theo ngữ cảnh đồng thời học ánh xạ từ trạng thái và tham số MDP đến hành động, tức là (s, φi, θi) → a, nó chỉ có thể đạt được khái quát hóa trên không gian hành động, trái với không gian chính sách tổng quát hơn.

Cuối cùng, do khả năng chuyển giao không cần huấn luyện mạnh mẽ của giải pháp được xấp xỉ đến các phần thưởng và động lực học mới, người ta có thể sử dụng nó để trực quan hóa quỹ đạo gần tối ưu τi* cho các tác vụ mới mà không cần thiết phải huấn luyện thuật toán RL. Một ứng dụng có thể của cách tiếp cận này sẽ là để trực quan hóa tác vụ hoặc thiết kế môi trường, cũng như định hình phần thưởng thủ công. Như một ví dụ, Hình 6 hiển thị các quỹ đạo mẫu được tạo ra bằng cách triển khai các mô hình HyperZero đã huấn luyện được điều kiện theo các tham số phần thưởng/động lực học khác nhau. Các quỹ đạo bổ sung trong Phụ lục B.2.

Nghiên cứu Ablation về Các Biến thể HyperZero. Trong Hình 7, chúng tôi thực hiện một nghiên cứu ablation về những cải thiện đạt được từ việc tạo ra hàm giá trị gần tối ưu và sử dụng mất mát TD đề xuất của chúng tôi từ Phương trình (5). Chúng tôi rút ra hai kết luận từ nghiên cứu này; đầu tiên, tạo ra hàm giá trị hành động Qi* cùng với chính sách πi* cung cấp tín hiệu học tập bổ sung để huấn luyện hypernetwork. Hơn nữa, việc kết hợp mất mát TD giữa chính sách được tạo ra và hàm giá trị hành động đảm bảo hai mạng được tạo ra nhất quán với nhau đối với phương trình Bellman và dẫn đến hiệu suất và khái quát hóa tốt hơn tổng thể. Trong khi những cải thiện có thể xuất hiện nhỏ, chúng tôi nghi ngờ rằng lợi ích sẽ lớn hơn trong các bài toán điều khiển trực quan, vì tạo ra hàm giá trị sẽ cung cấp tín hiệu học tập phong phú cho việc học biểu diễn. Quan trọng hơn, hàm giá trị được tạo ra có thể có các ứng dụng khác, chẳng hạn như được sử dụng trong các phương pháp policy gradient để huấn luyện thêm chính sách được tạo ra với các tương tác môi trường (offline-to-online RL) (Lee et al. 2022). Trong khi điều này được để lại cho công việc tương lai, chúng tôi muốn đảm bảo rằng khung của chúng tôi có khả năng tạo ra hàm giá trị cùng với chính sách.

5 Công trình Liên quan

Tính bền vững và khái quát hóa của hành vi đã được nghiên cứu từ lâu trong điều khiển và RL.

Transfer, Contextual và Meta RL. Công trình trước đây đã nghiên cứu nhiều dạng Transfer Learning (Taylor and Stone 2009), nơi các thành phần MDP bao gồm không gian trạng thái, không gian hành động, động lực học hoặc phần thưởng được sửa đổi giữa việc huấn luyện được tiến hành trên một hoặc nhiều tác vụ nguồn, trước khi hiệu suất trên một hoặc nhiều đích. Tùy thuộc vào quan điểm của người học về nguồn và đích, bài toán được gọi là tìm kiếm chính sách theo ngữ cảnh (Kupcsik et al. 2017) học suốt đời (Abel et al. 2018), học chương trình (Portelas et al. 2020), hoặc meta learning (Finn, Abbeel, and Levine 2017), nhưng biến thể cụ thể của chúng tôi, với một vector tham số luôn có thể quan sát được và không có cơ hội huấn luyện hoặc tinh chỉnh trên đích được gọi phù hợp nhất là contextual RL không cần huấn luyện. Trong bài toán đó, một mối quan tâm chung là cách nội suy trong không gian ngữ cảnh (tương đương với các tham số của chúng tôi), trong khi bảo tồn chi tiết của giải pháp không gian chính sách (Barbaros et al. 2018). Đây chính xác là nơi sức mạnh của kiến trúc hypernetwork của chúng tôi mở rộng nghệ thuật trước đây.

Hypernetwork trong RL. Trong khi hypernetwork (Ha, Dai, and Le 2016) đã được sử dụng rộng rãi trong các bài toán học có giám sát (von Oswald et al. 2019; Galanti and Wolf 2020; Krueger et al. 2017; Zhao et al. 2020), ứng dụng của chúng cho các thuật toán RL vẫn tương đối hạn chế. Công trình gần đây của Sarafian, Keynan, and Kraus (2021) sử dụng hypernetwork để cải thiện ước tính gradient của hàm Q và mạng chính sách trong các thuật toán policy gradient. Trong multi-agent RL, hypernetwork được sử dụng để tạo ra chính sách hoặc hàm giá trị dựa trên thuộc tính tác nhân (Rashid et al. 2018; Iqbal et al. 2020, 2021; de Witt et al. 2020; Zhou et al. 2020). Hơn nữa, hypernetwork đã được sử dụng để mô hình hóa một hệ thống động lực học phát triển trong continual model-based RL (Huang et al. 2021). Liên quan đến cách tiếp cận của chúng tôi, Faccio et al. (2022a) sử dụng hypernetwork để học các chính sách tối ưu có điều kiện mục tiêu; yếu tố khác biệt chính của cách tiếp cận chúng tôi là chúng tôi tập trung vào chuyển giao không cần huấn luyện qua một họ MDP với các hàm phần thưởng và động lực học khác nhau, trong khi phương pháp của Faccio et al. (2022a) nhằm giải quyết một MDP có điều kiện mục tiêu duy nhất.

Upside Down RL. Upside down RL (UDRL) là một định nghĩa lại bài toán RL biến đổi nó thành một dạng học có giám sát. UDRL, thay vì học các chính sách tối ưu sử dụng phần thưởng, dạy tác nhân làm theo lệnh. Phương pháp này ánh xạ các quan sát đầu vào như lệnh đến xác suất hành động với học có giám sát được điều kiện theo kinh nghiệm quá khứ (Srivastava et al. 2019; Schmidhuber 2019). Liên quan đến ý tưởng này là các mô hình RL ngoại tuyến sử dụng mô hình hóa trình tự thay vì học có giám sát để mô hình hóa hành vi (Janner, Li, and Levine 2021; Chen et al. 2021). Tương tự như UDRL, nhiều thuật toán RL kết hợp việc sử dụng học có giám sát trong mô hình của chúng (Schmidhuber 2015; Rosenstein et al. 2004). Một kỹ thuật như vậy là hindsight RL trong đó lệnh tương ứng với điều kiện mục tiêu (Andrychowicz et al. 2017; Rauber et al. 2017; Harutyunyan et al. 2019). Một cách tiếp cận khác là sử dụng các mô hình tiến thay vì các mô hình lùi được sử dụng trong UDRL (Arjona-Medina et al. 2019). Gần đây, Faccio et al. (2022a) đề xuất một phương pháp đánh giá các chính sách được tạo ra trong không gian lệnh thay vì tối ưu hóa một chính sách duy nhất để đạt được phần thưởng mong muốn.

6 Kết luận

Bài báo này đã mô tả một cách tiếp cận, được đặt tên là HyperZero, học cách khái quát hóa hành vi tối ưu qua một họ tác vụ. Bằng cách huấn luyện trên các giải pháp RL đầy đủ của các tác vụ huấn luyện, bao gồm các tham số chính sách và hàm giá trị tối ưu của chúng, hypernetwork được sử dụng trong kiến trúc của chúng tôi được huấn luyện để trực tiếp xuất các tham số của các chính sách mạng thần kinh phức tạp có khả năng giải quyết các tác vụ đích chưa thấy. Công trình này mở rộng hiệu suất khái quát hóa không cần huấn luyện so với các cách tiếp cận trước đây. Các thí nghiệm của chúng tôi chứng minh rằng các hành vi không cần huấn luyện của chúng tôi đạt được gần như hiệu suất đầy đủ, như được định nghĩa bởi hiệu suất của chính sách tối ưu được phục hồi bởi một người học RL huấn luyện trong một lượng lớn lần lặp trên chính tác vụ đích.

Do khả năng khái quát hóa mạnh mẽ của phương pháp chúng tôi, với yêu cầu tính toán tối thiểu tại thời điểm kiểm tra, cách tiếp cận của chúng tôi phù hợp cho triển khai trong các hệ thống trực tiếp. Chúng tôi cũng nhấn mạnh cơ hội cho giao diện con người và khám phá các giải pháp RL. Tóm lại, mức độ hành vi chung nhanh chóng nhưng mạnh mẽ mới này có thể cung cấp cơ hội đáng kể cho việc triển khai thực tế hành vi được học bởi RL trong tương lai.
