# HyperShot: Học Vài Mẫu bằng Kernel HyperNetworks

Marcin Sendera* 1Marcin Przewi˛ e´ zlikowski* 1Konrad Karanowski2
Maciej Zi˛ eba2 3Jacek Tabor1Przemysław Spurek1

## Tóm tắt

Các mô hình học vài mẫu nhằm mục đích đưa ra dự đoán sử dụng số lượng tối thiểu các ví dụ được gán nhãn từ một nhiệm vụ nhất định. Thách thức chính trong lĩnh vực này là thiết lập một mẫu duy nhất (one-shot) nơi chỉ có một phần tử đại diện cho mỗi lớp. Chúng tôi đề xuất HyperShot - sự hợp nhất của kernel và paradigm hypernetwork. So với các phương pháp tham chiếu áp dụng điều chỉnh tham số dựa trên gradient, mô hình của chúng tôi nhằm mục đích chuyển đổi tham số module phân loại tùy thuộc vào embedding của nhiệm vụ. Trong thực tế, chúng tôi sử dụng hypernetwork, nhận thông tin tổng hợp từ dữ liệu hỗ trợ và trả về các tham số của bộ phân loại được thiết kế thủ công cho vấn đề đang xem xét. Hơn nữa, chúng tôi giới thiệu biểu diễn dựa trên kernel của các ví dụ hỗ trợ được cung cấp cho hypernetwork để tạo ra các tham số của module phân loại. Do đó, chúng tôi dựa vào các mối quan hệ giữa embeddings của các ví dụ hỗ trợ thay vì giá trị đặc trưng trực tiếp được cung cấp bởi các mô hình backbone. Nhờ phương pháp này, mô hình của chúng tôi có thể thích ứng với các nhiệm vụ rất khác nhau.

## 1. Giới thiệu

Các kỹ thuật Trí tuệ nhân tạo hiện tại không thể tổng quát hóa nhanh chóng từ một vài ví dụ. Sự bất lực chung này là do hầu hết các mạng nơ-ron sâu phải được huấn luyện trên dữ liệu quy mô lớn. Ngược lại, con người có thể học các nhiệm vụ mới một cách nhanh chóng bằng cách sử dụng những gì họ đã học trong quá khứ. Các mô hình học vài mẫu cố gắng lấp đầy khoảng trống này bằng cách học cách học từ số lượng ví dụ hạn chế. Học vài mẫu là vấn đề đưa ra dự đoán dựa trên một số lượng nhỏ mẫu. Mục tiêu của học vài mẫu không phải là nhận dạng một tập hợp cố định các nhãn mà là học cách nhanh chóng thích ứng với các nhiệm vụ mới với một lượng nhỏ dữ liệu huấn luyện. Sau khi huấn luyện, mô hình có thể phân loại dữ liệu mới chỉ sử dụng một vài mẫu huấn luyện.

Các phương pháp phổ biến nhất cho học vài mẫu sử dụng framework meta-learning. Chúng tôi lấy mẫu các nhiệm vụ phân loại vài mẫu từ các mẫu huấn luyện thuộc về các lớp cơ sở và tối ưu hóa mô hình để hoạt động tốt trên các nhiệm vụ này. Chúng tôi làm việc với các nhiệm vụ N-way và K-shot, nơi chúng tôi có N lớp với K mẫu hỗ trợ và Q mẫu truy vấn trong mỗi danh mục. Mục tiêu là phân loại các NQ mẫu truy vấn này thành N lớp dựa trên NK mẫu hỗ trợ.

Một trong những phương pháp phổ biến nhất cho Học vài mẫu là Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017), nơi mô hình được huấn luyện để thích ứng nhanh chóng với các nhiệm vụ Học vài mẫu mới. Ý tưởng chính là tạo ra các trọng số chung có thể được cập nhật nhanh chóng để giải quyết các nhiệm vụ nhỏ mới. Hạn chế chính của mô hình là quy trình tối ưu hóa phức tạp sử dụng vòng lặp trong và ngoài, có thể được xem như tối ưu hóa bậc hai. Hơn nữa, quy trình tối ưu hóa như vậy chỉ triển khai một phần paradigm học cách học, giả định rằng mô hình học một số quy tắc để thích ứng với các nhiệm vụ mới. Việc quy nạp các quy tắc như vậy bởi MAML bị hạn chế bởi tối ưu hóa dựa trên gradient được áp dụng để điều chỉnh các tham số của module phân loại cho nhiệm vụ mới. Trong thực tế, chúng ta có thể thực hiện các quy tắc học dựa trên tối ưu hóa gradient. Các tham số module phân loại mới được thu được bằng gradient descent.

Bài báo này giới thiệu một chiến lược mới, giải quyết vấn đề trên với việc giới hạn chỉ trong các quy tắc quyết định dựa trên gradient. Mục tiêu của chúng tôi là bắt chước quá trình học của con người. Trước tiên, chúng tôi nhìn vào toàn bộ tập hỗ trợ và trích xuất thông tin để phân biệt các đối tượng trong các lớp. Sau đó, dựa trên mối quan hệ giữa các đặc trưng, chúng tôi tạo ra các quy tắc quyết định.

Chúng tôi kết hợp paradigm Hypernetworks với các phương pháp kernel để thực hiện kịch bản như vậy, xem Hình 1. Hypernetworks, được giới thiệu trong (Ha et al., 2016), được định nghĩa là các mô hình nơ-ron tạo ra trọng số cho một mạng mục tiêu riêng biệt giải quyết một nhiệm vụ cụ thể. Trong phương pháp của chúng tôi, Hypernetwork tổng hợp thông tin từ tập hỗ trợ và tạo ra trọng số của mô hình mục tiêu, phân loại tập truy vấn.

Các phương pháp kernel thực hiện phần đầu tiên của quá trình. Đối với mỗi nhiệm vụ vài mẫu, chúng tôi trích xuất các đặc trưng từ tập hỗ trợ thông qua kiến trúc backbone và tính toán các giá trị kernel giữa chúng. Sau đó chúng tôi sử dụng kiến trúc Hypernetwork – một mạng nơ-ron nhận biểu diễn kernel và tạo ra các quy tắc quyết định dưới dạng bộ phân loại (mạng mục tiêu) dành riêng cho tập truy vấn.

Giải pháp như vậy thực hiện paradigm học cách học. Hypernetwork, tạo ra các quy tắc quyết định từ biểu diễn kernel của tập hỗ trợ, được huấn luyện bằng phương pháp dựa trên gradient và có thể được xem như một quá trình học cổ điển. Tuy nhiên, các trọng số của mạng mục tiêu (quy tắc quyết định) có thể thực hiện các chiến lược khác nhau và nằm trong các phần không gian trọng số khác nhau. Phương pháp của chúng tôi cho phép huấn luyện mô hình kiến trúc universal, có thể được cập nhật nhanh chóng cho các nhiệm vụ mới mà không cần bất kỳ quy trình bậc hai nào (vòng lặp trong và ngoài).

Chúng tôi thực hiện một nghiên cứu thực nghiệm rộng rãi về phương pháp của mình bằng cách đánh giá nó trên các nhiệm vụ phân loại hình ảnh một mẫu và vài mẫu khác nhau. Chúng tôi phát hiện rằng HyperShot thể hiện độ chính xác cao trong tất cả các nhiệm vụ, hoạt động tương đương hoặc tốt hơn các phương pháp được đề xuất gần đây khác. Hơn nữa, HyperShot cho thấy khả năng tổng quát hóa mạnh mẽ, như được chứng minh bởi hiệu suất của nó trên các nhiệm vụ phân loại cross-domain.

Những đóng góp của công trình chúng tôi có thể được tóm tắt như sau:
• Trong bài báo này, chúng tôi đề xuất một mô hình thực hiện paradigm học cách học bằng cách mô hình hóa các quy tắc học không dựa trên tối ưu hóa gradient và có thể tạo ra các chiến lược hoàn toàn khác nhau.
• Chúng tôi đề xuất một phương pháp mới để giải quyết vấn đề học vài mẫu bằng cách tổng hợp thông tin từ tập hỗ trợ và trực tiếp tạo ra trọng số từ mạng nơ-ron dành riêng cho tập truy vấn.
• Chúng tôi đề xuất HyperShot, sử dụng paradigm Hypernetwork để tạo ra các trọng số dành riêng cho mỗi nhiệm vụ.

## 2. HyperShot: Hypernetwork cho học vài mẫu

Trong phần này, chúng tôi trình bày mô hình HyperShot của chúng tôi cho học vài mẫu.

### 2.1. Nền tảng

**Học vài mẫu** Thuật ngữ mô tả thiết lập học vài mẫu rất đa dạng do các định nghĩa xung đột được sử dụng trong tài liệu. Để có một phân loại thống nhất, chúng tôi đề cập đến người đọc (Chen et al., 2019; Wang et al., 2020). Ở đây, chúng tôi sử dụng thuật ngữ từ tài liệu meta-learning, phổ biến nhất tại thời điểm viết.

Hãy:
S = {(xl, yl)}^L_{l=1}  (1)

là tập hỗ trợ chứa các cặp đầu vào-đầu ra, với L ví dụ có phân phối lớp bằng nhau. Trong kịch bản một mẫu, mỗi lớp được đại diện bởi một ví dụ duy nhất, và L = K, trong đó K là số lượng lớp được xem xét trong nhiệm vụ nhất định. Trong khi đó, đối với các kịch bản vài mẫu, mỗi lớp thường có từ 2 đến 5 đại diện trong tập hỗ trợ S.

Hãy:
Q = {(xm, ym)}^M_{m=1}  (2)

là tập truy vấn (đôi khi được gọi trong tài liệu là tập mục tiêu), với M ví dụ, trong đó M thường lớn hơn K một bậc. Để dễ ký hiệu, các tập hỗ trợ và truy vấn được nhóm trong một nhiệm vụ T = {S, Q}.

Trong giai đoạn huấn luyện, các mô hình cho ứng dụng vài mẫu được cung cấp bởi các ví dụ được chọn ngẫu nhiên từ tập huấn luyện D = {Tn}^N_{n=1}, được định nghĩa là tập hợp các nhiệm vụ như vậy.

Trong giai đoạn suy luận, chúng tôi xem xét nhiệm vụ T = {S, X}, trong đó S là tập hỗ trợ với các giá trị lớp đã biết cho một nhiệm vụ nhất định, và X là tập hợp các đầu vào truy vấn (chưa được gán nhãn). Mục tiêu là dự đoán nhãn lớp cho các đầu vào truy vấn x ∈ X, giả định tập hỗ trợ S và sử dụng mô hình được huấn luyện trên D.

**Hypernetwork** Trong công trình kinh điển (Ha et al., 2016), hyper-networks được định nghĩa là các mô hình nơ-ron tạo ra trọng số cho một mạng mục tiêu riêng biệt giải quyết một nhiệm vụ cụ thể. Các tác giả nhằm mục đích giảm số lượng tham số có thể huấn luyện bằng cách thiết kế hyper-network với số lượng tham số nhỏ hơn mạng mục tiêu. Tạo ra sự tương đồng giữa hyper-networks và các mô hình sinh, các tác giả của (Sheikh et al., 2017) sử dụng cơ chế này để tạo ra một tập hợp đa dạng các mạng mục tiêu xấp xỉ cùng một hàm.

### 2.2. HyperShot - tổng quan

Chúng tôi giới thiệu HyperShot của chúng tôi - mô hình sử dụng hypernetwork cho các vấn đề vài mẫu. Ý tưởng chính của phương pháp được đề xuất là dự đoán các giá trị của tham số cho mạng phân loại đưa ra dự đoán trên các hình ảnh truy vấn với thông tin được trích xuất từ các ví dụ hỗ trợ cho một nhiệm vụ nhất định. Nhờ phương pháp này, chúng tôi có thể chuyển đổi các tham số của bộ phân loại giữa các nhiệm vụ hoàn toàn khác nhau dựa trên tập hỗ trợ. Thông tin về nhiệm vụ hiện tại được trích xuất từ tập hỗ trợ bằng cách sử dụng hàm kernel được tham số hóa hoạt động trên không gian embedding. Nhờ phương pháp này, chúng tôi sử dụng các mối quan hệ giữa các ví dụ hỗ trợ thay vì lấy các giá trị trực tiếp của các giá trị embedding làm đầu vào cho hypernetwork. Do đó, phương pháp này mạnh mẽ với các giá trị embedding cho các nhiệm vụ mới xa khỏi các vùng đặc trưng được quan sát trong quá trình huấn luyện. Việc phân loại hình ảnh truy vấn cũng được thực hiện bằng cách sử dụng các giá trị kernel được tính toán đối với tập hỗ trợ.

Kiến trúc của HyperShot được cung cấp trong Hình 2. Chúng tôi nhằm mục đích dự đoán phân phối lớp p(y|S, x), với hình ảnh truy vấn x và tập hợp các ví dụ hỗ trợ S = {(xl, yl)}^K_{l=1}.

Đầu tiên, tất cả các hình ảnh từ tập hỗ trợ được nhóm theo các giá trị lớp tương ứng của chúng. Tiếp theo, mỗi hình ảnh xl từ tập hỗ trợ được chuyển đổi bằng cách sử dụng mạng mã hóa E(·), tạo ra các biểu diễn chiều thấp của các hình ảnh, E(xl) = zl. Các embeddings được xây dựng được sắp xếp theo nhãn lớp và được lưu trữ trong ma trận ZS = [z^(1); ...; z^(K)]^T, trong đó (·) là hàm song ánh, thỏa mãn y^(l) ≠ y^(k) cho l ≠ k.

Trong bước tiếp theo, chúng tôi tính toán ma trận kernel KS,S, cho các cặp vector được lưu trữ trong các hàng của ZS. Để đạt được điều này, chúng tôi sử dụng hàm kernel được tham số hóa k(·, ·), và tính toán phần tử ki,j của ma trận KS,S theo cách sau:

ki,j = k(z^(i), z^(j))  (3)

Ma trận kernel KS,S đại diện cho thông tin được trích xuất về các mối quan hệ giữa các ví dụ hỗ trợ cho một nhiệm vụ nhất định. Ma trận KS,S được định hình lại thành định dạng vector và được cung cấp cho đầu vào của hypernetwork H(·). Vai trò của hypernetwork là cung cấp các tham số θT của mô hình mục tiêu T(·) chịu trách nhiệm phân loại đối tượng truy vấn. Nhờ phương pháp đó, chúng tôi có thể chuyển đổi giữa các tham số cho các nhiệm vụ hoàn toàn khác nhau mà không cần di chuyển qua quỹ đạo được kiểm soát bởi gradient, như trong một số phương pháp tham chiếu như MAML.

Hình ảnh truy vấn x được phân loại theo cách sau. Đầu tiên, hình ảnh đầu vào được chuyển đổi thành biểu diễn đặc trưng chiều thấp zx bởi encoder E(x). Tiếp theo, vector kernel kx,S giữa embedding truy vấn và các vector hỗ trợ được sắp xếp ZS được tính toán theo cách sau:

kx,S = [k(zx, z^(1)); ...; k(zx, z^(K))]^T  (4)

Vector kx,S được cung cấp thêm trên đầu vào của mô hình mục tiêu T(·) đang sử dụng các tham số θT được trả về bởi hypernetwork H(·). Mô hình mục tiêu trả về phân phối xác suất p(y|S, x) cho mỗi lớp được xem xét trong nhiệm vụ.

### 2.3. Hàm kernel

Một trong những thành phần chính của phương pháp chúng tôi là hàm kernel k(·, ·). Trong công trình này, chúng tôi xem xét tích vô hướng của các vector đã được chuyển đổi được cho bởi:

k(z1, z2) = f(z1)^T f(z2)  (5)

trong đó f(·) có thể là hàm chuyển đổi được tham số hóa, được biểu diễn bởi mô hình MLP, hoặc đơn giản là phép toán đồng nhất, f(z) = z. Trong không gian Euclidean, tiêu chí này có thể được biểu diễn là k(z1, z2) = ||f(z1)|| ||f(z2)|| cos θ, trong đó θ là góc giữa các vector f(z1) và f(z2). Đặc điểm chính của hàm này là nó xem xét các chuẩn của vector, điều này có thể có vấn đề đối với một số nhiệm vụ là outlier về các biểu diễn được tạo bởi f(·). Do đó, chúng tôi cũng xem xét trong các thí nghiệm của mình hàm kernel cosine được cho bởi:

kc(z1, z2) = f(z1)^T f(z2) / (||f(z1)|| ||f(z2)||)  (6)

đại diện cho phiên bản chuẩn hóa của tích vô hướng. Xem xét biểu diễn hình học, kc(z1, z2) có thể được biểu diễn là cos θ (xem ví dụ được cung cấp bởi Hình 3). Tập hỗ trợ được biểu diễn bởi hai ví dụ từ các lớp khác nhau, f1 và f2. Các tham số mô hình mục tiêu θT được tạo chỉ dựa trên giá trị cosine của góc giữa các vector f1 và f2. Trong giai đoạn phân loại, ví dụ truy vấn được biểu diễn bởi fx, và việc phân loại được áp dụng trên các giá trị cosine của các góc giữa fx và f1, và fx và f2, tương ứng.

### 2.4. Huấn luyện và dự đoán

Quy trình huấn luyện giả định việc tham số hóa sau đây của các thành phần mô hình. Encoder E := EθE được tham số hóa bởi θE, hypernetwork H = HθH bởi θH, và hàm kernel k bởi θk. Chúng tôi giả định rằng tập huấn luyện D được biểu diễn bởi các nhiệm vụ Ti được cấu thành từ hỗ trợ Si và các ví dụ truy vấn Qi. Việc huấn luyện được thực hiện bằng cách tối ưu hóa tiêu chí cross-entropy:

L = Σ_{Ti∈D} Σ_{m=1}^M Σ_{k=1}^K y_k^{i,m} log p(y_k^{i,m}|Si, xi,m)  (7)

trong đó (xi,n, yi,n) là các ví dụ từ tập truy vấn Qi, trong đó Qi = {(xi,m, yi,m)}_{m=1}^M. Phân phối cho các lớp hiện được xem xét p(y|S, x) được trả về bởi mạng mục tiêu T của HyperShot. Trong quá trình huấn luyện, chúng tôi cùng tối ưu hóa các tham số θH, θk, và θE, giảm thiểu loss L.

Trong giai đoạn suy luận, chúng tôi xem xét nhiệm vụ T, được cấu thành từ tập hợp các ví dụ hỗ trợ được gán nhãn S và tập hợp các ví dụ truy vấn chưa được gán nhãn được biểu diễn bởi các giá trị đầu vào X mà mô hình nên phân loại. Chúng tôi có thể đơn giản lấy các giá trị xác suất p(y|S, x) giả định tập hỗ trợ S nhất định và quan sát truy vấn đơn lẻ x từ X, sử dụng mô hình với các tham số đã huấn luyện θH, θk, và θE. Tuy nhiên, chúng tôi quan sát rằng kết quả hơi tốt hơn được thu được khi điều chỉnh các tham số của mô hình trên nhiệm vụ được xem xét. Chúng tôi không có quyền truy cập vào nhãn cho các ví dụ truy vấn. Do đó, chúng tôi bắt chước tập truy vấn cho nhiệm vụ này đơn giản bằng cách lấy các ví dụ hỗ trợ và tạo nhiệm vụ điều chỉnh Ti = {S, S} và cập nhật các tham số của mô hình bằng cách sử dụng một số lần lặp gradient. Trình bày chi tiết về quy trình huấn luyện và dự đoán được cung cấp bởi Algorithm 1.

### 2.5. Thích ứng với các kịch bản vài mẫu

Phương pháp được đề xuất sử dụng hàm sắp xếp (·) giữ tính nhất quán giữa ma trận kernel hỗ trợ KS,S và vector các giá trị kernel kx,S cho ví dụ truy vấn x. Đối với các kịch bản vài mẫu, mỗi lớp có nhiều hơn một đại diện trong tập hỗ trợ. Do đó, có nhiều khả năng khác nhau để sắp xếp các vector đặc trưng trong tập hỗ trợ bên trong lớp được xem xét. Để loại bỏ vấn đề này, chúng tôi đề xuất áp dụng hàm tổng hợp cho các embeddings z xem xét các ví dụ hỗ trợ từ cùng một lớp. Nhờ phương pháp này, ma trận kernel được tính toán dựa trên các giá trị được tổng hợp của không gian tiềm ẩn của mạng mã hóa E, làm cho phương pháp của chúng tôi độc lập với việc sắp xếp giữa các embeddings từ cùng một lớp. Trong các nghiên cứu thực nghiệm, chúng tôi kiểm tra chất lượng của phép toán tổng hợp trung bình (averaged) so với việc nối đơn giản theo lớp của các embeddings (fine-grained) trong các nghiên cứu ablation.

## 3. Công trình liên quan

Chuyển giao đặc trưng (Zhuang et al., 2020) là quy trình cơ bản cho học vài mẫu và bao gồm việc huấn luyện trước mạng nơ-ron và bộ phân loại. Trong quá trình meta-validation, bộ phân loại được fine-tune cho các nhiệm vụ mới. (Chen et al., 2019) mở rộng ý tưởng này bằng cách sử dụng khoảng cách cosine giữa các ví dụ.

Trong những năm gần đây, một loạt các phương pháp meta-learning (Hospedales et al., 2020; Schmidhuber, 1992; Bengio et al., 1992) đã được đề xuất để giải quyết vấn đề học vài mẫu. Các kiến trúc meta-learning khác nhau cho học vài mẫu có thể được phân loại một cách đại khái thành một số nhóm:

**Các phương pháp dựa trên bộ nhớ** (Ravi & Larochelle, 2017; Munkhdalai et al., 2018; Santoro et al., 2016; Mishra et al., 2018; Munkhdalai & Yu, 2017; Zhen et al., 2020) dựa trên ý tưởng huấn luyện meta-learner với bộ nhớ để học các khái niệm mới.

**Các phương pháp dựa trên metric** meta-learn một biểu diễn sâu với một metric trong không gian đặc trưng, sao cho khoảng cách giữa các ví dụ từ tập hỗ trợ và truy vấn với cùng lớp có khoảng cách nhỏ trong không gian như vậy. Một số công trình sớm nhất khám phá khái niệm này là Matching Networks (Vinyals et al., 2016) và Prototypical Networks (Snell et al., 2017), tạo ra các nguyên mẫu dựa trên embeddings của các ví dụ từ tập hỗ trợ trong không gian đặc trưng đã học và phân loại tập truy vấn dựa trên khoảng cách đến các nguyên mẫu đó. Nhiều công trình tiếp theo nhằm mục đích cải thiện tính biểu cảm của các nguyên mẫu thông qua các kỹ thuật khác nhau. (Oreshkin et al., 2018) đạt được điều này bằng cách điều hòa mạng trên các nhiệm vụ cụ thể, do đó làm cho không gian đã học phụ thuộc vào nhiệm vụ. (Hu et al., 2021) chuyển đổi embeddings của các ví dụ hỗ trợ và truy vấn trong không gian đặc trưng để làm cho phân phối của chúng gần hơn với Gaussian. (Sung et al., 2018) đề xuất Relation Nets, học hàm metric thay vì sử dụng hàm cố định, chẳng hạn như khoảng cách Euclidean hoặc cosine.

Tương tự như các phương pháp trên, HyperShot sử dụng hàm kernel dự đoán các mối quan hệ giữa các ví dụ trong một nhiệm vụ nhất định. Sự khác biệt chính là thay vì thực hiện phân loại nearest-neighbor dựa trên các giá trị kernel, trong HyperShot, ma trận kernel được phân loại bởi bộ phân loại cụ thể cho nhiệm vụ được tạo ra bởi hypernetwork.

**Các phương pháp dựa trên tối ưu hóa** theo ý tưởng của quá trình tối ưu hóa trên tập hỗ trợ trong framework meta-learning như MetaOptNet (Lee et al., 2019), Model-Agnostic Meta-Learning (MAML), và các phần mở rộng của nó (Finn et al., 2017; Nichol et al., 2018; Raghu et al., 2019; Rajeswaran et al., 2019; Finn et al., 2018; Nichol et al., 2018). Những kỹ thuật này nhằm mục đích huấn luyện các mô hình tổng quát, có thể thích ứng các tham số của chúng với tập hỗ trợ trong tầm tay trong một số lượng nhỏ các bước gradient. Tương tự như các kỹ thuật như vậy, HyperShot cũng nhằm mục đích tạo ra các mô hình cụ thể cho nhiệm vụ nhưng sử dụng hypernetwork thay vì tối ưu hóa để đạt được mục tiêu đó.

**Các quá trình Gaussian** (Rasmussen, 2003) sở hữu nhiều tính chất hữu ích trong học vài mẫu, chẳng hạn như khả năng chống chịu tự nhiên với lượng dữ liệu hạn chế và khả năng ước lượng sự không chắc chắn. Khi kết hợp với các kernel sâu được meta-learn, (Patacchiola et al., 2020), các quá trình Gaussian đã được chứng minh là công cụ phù hợp cho hồi quy và phân loại vài mẫu, được gọi là Deep Kernel Transfer (DKT). Giả định rằng kernel sâu chung như vậy có đủ dữ liệu để tổng quát hóa tốt cho các nhiệm vụ chưa thấy đã được thách thức trong các công trình tiếp theo. (Wang et al., 2021) đã giới thiệu kỹ thuật học các quá trình Gaussian dày đặc bằng cách quy nạp các biến. Phương pháp này đạt được cải thiện hiệu suất đáng kể so với các phương pháp thay thế. Tương tự, HyperShot cũng phụ thuộc vào việc học một mô hình ước lượng các tham số của hàm cụ thể cho nhiệm vụ. Tuy nhiên, HyperShot sử dụng hypernetwork thay vì quá trình Gaussian để đạt được mục tiêu đó.

**Hypernetworks** (Ha et al., 2016) đã được đề xuất như một giải pháp cho các vấn đề học vài mẫu trong một số công trình nhưng chưa được nghiên cứu rộng rãi như các phương pháp được đề cập ở trên. Nhiều công trình đã đề xuất các biến thể khác nhau của hyper-networks dự đoán các tham số của bộ phân loại nông với các ví dụ hỗ trợ nhất định (Bauer et al., 2017; Qiao et al., 2017). Các công trình tiếp theo đã mở rộng những mô hình đó bằng cách tính toán độ tương tự cosine giữa các ví dụ truy vấn và các trọng số bộ phân loại được tạo ra (Gidaris & Komodakis, 2018) và sử dụng mô hình xác suất dự đoán phân phối trên các tham số phù hợp cho nhiệm vụ nhất định (Gordon et al., 2018). Gần đây hơn, (Zhmoginov et al., 2022) đã khám phá việc tạo ra tất cả các tham số của mạng mục tiêu với hypernetwork dựa trên transformer, nhưng phát hiện rằng đối với các mạng mục tiêu lớn hơn, việc tạo ra chỉ các tham số của lớp phân loại cuối cùng là đủ.

Một đặc điểm chính của các phương pháp trên là trong quá trình suy luận, hypernetwork dự đoán các trọng số chịu trách nhiệm phân loại mỗi lớp một cách độc lập, chỉ dựa trên các ví dụ của lớp đó từ tập hỗ trợ. Tính chất này làm cho các giải pháp như vậy bất khả tri với số lượng lớp trong một nhiệm vụ, hữu ích trong các ứng dụng thực tế. Tuy nhiên, điều này cũng có nghĩa là hypernetwork không tận dụng các khác biệt giữa các lớp trong nhiệm vụ trong tầm tay.

Ngược lại, HyperShot khai thác những khác biệt đó bằng cách sử dụng kernel, điều này giúp cải thiện hiệu suất của nó.

## 4. Thí nghiệm

Trong thiết lập học vài mẫu điển hình, việc tạo ra so sánh có giá trị và công bằng giữa các mô hình được đề xuất thường phức tạp do sự tồn tại của những khác biệt đáng kể trong kiến trúc và triển khai của các phương pháp đã biết. Để hạn chế ảnh hưởng của các kiến trúc backbone (trích xuất đặc trưng) sâu hơn, chúng tôi tuân theo quy trình thống nhất được đề xuất bởi (Chen et al., 2019).

Trong phần này, chúng tôi mô tả phân tích thực nghiệm và hiệu suất của HyperShot trong nhiều benchmark vài mẫu khác nhau. Cụ thể, chúng tôi xem xét cả các nhiệm vụ phân loại (xem Phần 4.1) và thích ứng cross-domain (xem Phần 4.2). Trong khi các vấn đề phân loại tập trung vào các ứng dụng vài mẫu điển hình nhất, các benchmark cross-domain sau kiểm tra khả năng của các mô hình thích ứng với các nhiệm vụ ngoài phân phối. Ngoài ra, chúng tôi thực hiện nghiên cứu ablation về các quy trình thích ứng có thể của HyperShot cho các kịch bản vài mẫu - được trình bày trong Phần 4.3.

Trong tất cả các thí nghiệm được báo cáo, các nhiệm vụ bao gồm 5 lớp (5-way) và 1 hoặc 5 ví dụ hỗ trợ (1 hoặc 5-shot). Trừ khi được chỉ định khác, tất cả các mô hình được so sánh sử dụng backbone được biết đến và sử dụng rộng rãi bao gồm bốn lớp tích chập (mỗi lớp bao gồm tích chập 2D, lớp batch-norm, và phi tuyến ReLU; mỗi lớp bao gồm 64 kênh) và đã được huấn luyện từ đầu.

Chúng tôi báo cáo hiệu suất của hai biến thể của HyperShot:
• HyperShot - các mô hình được tạo ra bởi hypernetworks cho mỗi nhiệm vụ.
• HyperShot + finetuning - các mô hình được tạo ra bởi hypernetworks được finetuned trên các ví dụ hỗ trợ của mỗi nhiệm vụ trong 10 bước huấn luyện.

Trong tất cả các trường hợp, chúng tôi quan sát được sự cải thiện hiệu suất khiêm tốn nhờ finetuning hypernetwork.

Chi tiết toàn diện cho mỗi quy trình huấn luyện được báo cáo trong Phụ lục.

### 4.1. Phân loại

Đầu tiên, chúng tôi xem xét kịch bản học vài mẫu cổ điển, nơi tất cả các nhiệm vụ phân loại (cả huấn luyện và suy luận) đều đến từ cùng một bộ dữ liệu. Mục tiêu chính của các thí nghiệm phân loại được đề xuất là tìm ra khả năng của các mô hình vài mẫu thích ứng với các nhiệm vụ chưa từng thấy từ cùng một phân phối dữ liệu.

Chúng tôi đánh giá hiệu suất của HyperShot và các phương pháp khác trên hai bộ dữ liệu thách thức và được xem xét rộng rãi: Caltech-USCD Birds (CUB) (Wah et al., 2011) và mini-ImageNet (Ravi & Larochelle, 2017). Các thí nghiệm sau đây ở thiết lập phổ biến nhất, 5-way, bao gồm 5 lớp ngẫu nhiên. Trong tất cả các thí nghiệm, tập truy vấn của mỗi nhiệm vụ bao gồm 16 mẫu cho mỗi lớp (80 tổng cộng). Chúng tôi cung cấp các chi tiết huấn luyện bổ sung trong Phụ lục.

Chúng tôi so sánh HyperShot với một nhóm lớn các thuật toán tiên tiến, bao gồm các phương pháp kinh điển (như Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), MAML (Finn et al., 2017), và các phần mở rộng của nó) cũng như các phương pháp Bayesian phổ biến gần đây chủ yếu được xây dựng trên framework Gaussian Processes (như DKT (Patacchiola et al., 2020)).

Chúng tôi trước tiên xem xét nhiệm vụ 1-shot thách thức hơn và báo cáo kết quả trong Bảng 1. Chúng tôi tiếp tục xem xét thiết lập 5-shot và báo cáo kết quả trong Bảng 2. Kết quả bổ sung so sánh các phương pháp trên backbone lớn hơn được bao gồm trong Phụ lục.

Trong kịch bản 1-shot, HyperShot đạt được độ chính xác cao nhất trong bộ dữ liệu CUB có và không có sử dụng quy trình finetuning (66.13% với finetuning, 65.27% không có) và hoạt động tốt hơn bất kỳ mô hình nào khác. Tuy nhiên, trong bộ dữ liệu mini-ImageNet, phương pháp của chúng tôi nằm trong số những mô hình tốt nhất (53.18%), thua hơi so với DFSVLwF (Gidaris & Komodakis, 2018) (56.20%).

Xem xét kịch bản 5-shot, HyperShot là mô hình tốt thứ hai đạt 80.07% trong bộ dữ liệu CUB và 69.62% trong mini-ImageNet, trong khi mô hình tốt nhất, amortized Bayesian prototype meta-learning, tốt hơn không đáng kể và đạt 80.94% và 70.44% trên các bộ dữ liệu được đề cập, tương ứng.

Kết quả thu được cho thấy rõ ràng rằng HyperShot đạt được kết quả tiên tiến hoặc có thể so sánh với các mô hình tốt nhất trên tập hợp chuyên sâu các thiết lập học vài mẫu phân loại tiêu chuẩn.

### 4.2. Thích ứng cross-domain

Trong thiết lập thích ứng cross-domain, mô hình được đánh giá trên các nhiệm vụ đến từ phân phối khác với phân phối mà nó đã được huấn luyện. Do đó, nhiệm vụ như vậy thách thức hơn so với phân loại tiêu chuẩn và là chỉ số hợp lý về khả năng tổng quát hóa của mô hình. Để đánh giá hiệu suất của HyperShot trong thích ứng cross-domain, chúng tôi hợp nhất dữ liệu từ hai bộ dữ liệu để tập huấn luyện được rút ra từ bộ dữ liệu đầu tiên và tập validation và test - từ bộ dữ liệu khác. Cụ thể, chúng tôi kiểm tra HyperShot trên hai nhiệm vụ phân loại cross-domain:

mini-ImageNet→CUB (mô hình được huấn luyện trên mini-ImageNet và đánh giá trên CUB) và Omniglot→EMNIST trong thiết lập 1-shot và 5-shot. Chúng tôi báo cáo kết quả trong Bảng 3. Trong hầu hết các thiết lập, HyperShot đạt được độ chính xác cao nhất, ngoại trừ phân loại 1-shot mini-ImageNet→CUB, nơi độ chính xác của nó ngang bằng với độ chính xác đạt được bởi DKT (Patacchiola et al., 2020) (40.14% và 40.03% được đạt bởi DKT và HyperShot, tương ứng). Chúng tôi lưu ý rằng chỉ trong trường hợp phân loại thường xuyên, finetuning hypernetwork trên các nhiệm vụ cá nhân liên tục cải thiện hiệu suất của nó.

### 4.3. Tổng hợp các ví dụ hỗ trợ trong thiết lập 5-shot

Trong HyperShot, hypernetwork tạo ra các trọng số của thông tin về các ví dụ hỗ trợ, được biểu diễn thông qua ma trận kernel hỗ trợ-hỗ trợ. Trong trường hợp phân loại 5-way 1-shot, mỗi nhiệm vụ bao gồm 5 ví dụ hỗ trợ, và do đó, kích thước của ma trận kernel là (5×5), và kích thước đầu vào của hypernetwork là 25. Tuy nhiên, khi số lượng ví dụ hỗ trợ tăng lên, việc tăng kích thước của ma trận kernel có thể không thực tế và dẫn đến tham số hóa quá mức của hypernetwork.

Vì hypernetworks được biết là nhạy cảm với kích thước đầu vào lớn (Ha et al., 2016), chúng tôi xem xét cách duy trì kích thước đầu vào không đổi của HyperShot, độc lập với số lượng ví dụ hỗ trợ của mỗi lớp bằng cách sử dụng trung bình của embeddings hỗ trợ của mỗi lớp cho tính toán kernel, thay vì embeddings cá nhân. Các công trình trước đó cho thấy rằng khi có nhiều ví dụ của một lớp, embedding trung bình của lớp như vậy đại diện cho nó một cách đầy đủ trong không gian embedding (Snell et al., 2017).

Để xác minh phương pháp này, trong thiết lập 5-shot, chúng tôi huấn luyện HyperShot với hai biến thể tính toán đầu vào cho ma trận kernel:

• fine-grained - sử dụng hypernetwork nhận đầu vào là ma trận kernel giữa mỗi embeddings của các ví dụ hỗ trợ cá nhân. Ma trận kernel này có hình dạng (25×25).
• averaged - sử dụng hypernetwork nơi ma trận kernel được tính toán giữa trung bình của embeddings của mỗi lớp. Ma trận kernel trong phương pháp này có hình dạng (5×5).

Chúng tôi đánh giá cả hai biến thể của HyperShot trên nhiệm vụ phân loại 5-shot trên các bộ dữ liệu CUB và mini-ImageNet. Hơn nữa, chúng tôi cũng so sánh các phương pháp này trên phân loại cross-domain giữa các bộ dữ liệu Omniglot và EMNIST. Chúng tôi báo cáo độ chính xác trong Bảng 4. Rõ ràng rằng việc lấy trung bình các embeddings trước khi tính toán ma trận kernel mang lại kết quả vượt trội.

## 5. Kết luận

Trong công trình này, chúng tôi đã giới thiệu HyperShot - một framework mới sử dụng các phương pháp kernel kết hợp với hypernetworks. Phương pháp của chúng tôi sử dụng biểu diễn dựa trên kernel của các ví dụ hỗ trợ và paradigm hypernetwork để tạo ra module phân loại của tập truy vấn. Chúng tôi tập trung vào các mối quan hệ giữa embeddings của các ví dụ hỗ trợ thay vì giá trị đặc trưng trực tiếp. Nhờ phương pháp này, mô hình của chúng tôi có thể thích ứng với các nhiệm vụ rất khác nhau.

Chúng tôi đánh giá mô hình HyperShot trên các nhiệm vụ phân loại hình ảnh một mẫu và vài mẫu khác nhau. HyperShot thể hiện độ chính xác cao trong tất cả các nhiệm vụ, hoạt động tương đương hoặc tốt hơn so với các giải pháp tiên tiến. Hơn nữa, mô hình có khả năng tổng quát hóa mạnh mẽ, như được chứng minh bởi hiệu suất của nó trên các nhiệm vụ phân loại cross-domain.
