# 2211.12485.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypernetwork/2211.12485.pdf
# File size: 682089 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HyperTuning: Toward Adapting Large Language
Models without Back-propagation
Jason Phang1,2, Yi Mao3, Pengcheng He3, Weizhu Chen3
1New York University
2EleutherAI
3Microsoft Azure AI
Abstract
Fine-tuning large language models for different tasks can be costly and inefﬁcient,
and even methods that reduce the number of tuned parameters still require full
gradient-based optimization. We propose HyperTuning, a novel approach to model
adaptation that uses a hypermodel to generate task-speciﬁc parameters for a ﬁxed
downstream model. We demonstrate a simple setup for hypertuning with HyperT5,
a T5-based hypermodel that produces soft preﬁxes or LoRA parameters for a
frozen T5 model from few-shot examples. We train HyperT5 in two stages: ﬁrst,
hyperpretraining with a modiﬁed conditional language modeling objective that
trains a hypermodel to generate parameters; second, multi-task ﬁne-tuning (MTF)
on a large number of diverse language tasks. We evaluate HyperT5 on P3, MetaICL
and Super-NaturalInstructions datasets, and show that it can effectively generate
parameters for unseen tasks. Moreover, we show that using hypermodel-generated
parameters as initializations for further parameter-efﬁcient ﬁne-tuning improves
performance. HyperTuning can thus be a ﬂexible and efﬁcient way to leverage
large language models for diverse downstream applications.
1 Introduction
While language models (LMs) have achieved remarkable capabilities with increasing model size
(Brown et al., 2020; Chowdhery et al., 2022), ﬁne-tuning them on speciﬁc downstream tasks in-
troduces signiﬁcant engineering challenges and computational costs. Although large models can
perform zero-shot, instruction-prompted, and few-shot learning (Sanh et al., 2022; Wei et al., 2022),
they are usually outperformed by fully ﬁne-tuned models when sufﬁcient training data is available.
To reduce the computational and memory overhead of ﬁne-tuning LMs, parameter-efﬁcient ﬁne-
tuning (PEFT) methods have been proposed, such as adapters (Houlsby et al., 2019), preﬁx tuning
(Li and Liang, 2021), and prompt tuning (Lester et al., 2021). These methods update only a small
subset of (possibly new) parameters of the LM, and have achieved competitive performance with full
ﬁne-tuning (Ding et al., 2022). However, PEFT methods still require full back-propagation through
the LM during training, which is computationally expensive and memory intensive. Given that (1)
only a small number of parameters need to be updated to adapt an LM to a given task, (2) very large
LMs have demonstrated strong in-context learning capabilities on a forward pass, and (3) a forward
pass for very large LMs already entails a substantial amount of computation, we hypothesize that it is
possible to train a separate model to perform the optimization or adaptation procedure entirely, using
only a forward pass.
Work done at Microsoft. Correspondence: jasonphang@nyu.edu
Preprint.arXiv:2211.12485v1  [cs.CL]  22 Nov 2022

--- PAGE 2 ---
To avoid the costly computation of back-propagating through the LM to produce the parameter
updates, especially for thousands or millions of iterations during training, we propose a new paradigm
ofhypertuning : using a hypermodel to adapt a downstream LM to a desired application. As a
concrete proof of concept, we explore a simple setup where hypermodels take as input a set of
few-shot examples from a given task, and output the PEFT parameters corresponding to that task in a
single forward pass.
To demonstrate the feasibility of this approach, we train HyperT5 : a set of T5-based hypermodels that
output soft preﬁxes (Li and Liang, 2021) or LoRA parameters (Hu et al., 2022), to be incorporated
into a frozen downstream T5 LM. To train HyperT5, we introduce a two-stage procedure for training
hypermodels: hyperpretraining , where we adapt a pretrained LM to generate PEFT parameters via a
modiﬁed language modeling objective, followed by multi-task ﬁne-tuning (MTF) the hypermodel.
After training, HyperT5 models can take few-shot examples from unseen tasks and generate the
corresponding PEFT parameters, allowing us to adapt a downstream LM without back-propagation.
We show in experiments across P3, Super-NaturalInstructions and MetaICL datasets that LMs can be
hypertuned using just a small number of examples. Furthermore, we show that when the hypermodel-
generated parameters are used as initializations for further parameter-efﬁcient ﬁne-tuning, we can
achieve faster training convergence and better overall performance.
This work serves as a ﬁrst step toward hypertuning, and we are are aware of certain limitations of this
preliminary setup. Because our current formulation of hypermodels can only take a small number
of examples as input, its performance cannot compare to full parameter-efﬁcient ﬁne-tuning or full
ﬁne-tuning. HyperT5 also generally underperforms T5 explicitly trained for few-shot in-context
learning with full attention across examples, although we note that the latter is more computationally
expensive to use at inference time. Nevertheless, we believe that our results demonstrate a promising
step toward model adaptation without the need for back-propagation.
We plan to release the code and model weights for HyperT5 , as well as the multi-task ﬁne-tuned
versions for the three datasets listed above.
2 Related Work
HyperNetworks Several works have explored the concept of "hypernetworks," where an auxiliary
network is used to generate parameters for a primary network. This terminology was ﬁrst introduced
by Ha et al. (2017) and applied to LSTMs. Among Transformer-based language models, Karimi Ma-
habadi et al. (2021) and He et al. (2022) incorporated hypernetworks into T5 models for knowledge
sharing during multitask ﬁne-tuning. Peebles et al. (2022) utilized a Transformer with diffusion for
generating full model parameters for image-recognition and Cartpole tasks. Similarly, Lester et al.
(2022) trained models to generate soft prompts for transferring between downstream models. Our
work is closely related to Deb et al. (2022), who also used a hypernetwork to modify downstream
model parameters and incorporated Super-NaturalInstuctions (S-NI) in their experimental setting.
They found that incorporating instructions via a hypernetwork trained with MAML (Finn et al., 2017)
improved downstream performance.
Multi-task Training and Transfer A crucial ingredient to hypertuning is the transferrability of
task knowledge and generalization to novel tasks. Many past works (Phang et al., 2018; Pruksachatkun
et al., 2020; Vu et al., 2020) have explored the effectiveness of single- and multi-task transfer learning.
More recent work has shown that large-scale multi-task training tends allows models to generalize
to unseen tasks (Sanh et al., 2022; Wei et al., 2022; Wang et al., 2022; Chung et al., 2022). Min
et al. (2022) and Chen et al. (2022) show that few-shot learning also beneﬁts from multi-task training.
Pfeiffer et al. (2020), Vu et al. (2021) and Gu et al. (2021) have also explored transfer learning among
PEFT methods.
3 HyperTuning
The impetus for using hypermodels for adapting downstream models derives from two recent
developments in natural language processing:
2

--- PAGE 3 ---
Figure 1: Overview of HyperTuning. (A) Fine-tuning, where all model parameters are updated
(red). (B) Parameter-efﬁcient ﬁne-tuning (PEFT), where all model parameters are frozen (blue) and
only a small number of parameters, , are updated. (C) HyperTuning, where a hypermodel is used
to generate parameters for a frozen downstream model. For instance, a hypermodel may take a
set of few-shot examples to determine what to generate. Only the hypermodel’s parameters are
updated during training. (D) At inference time, the parameters only need to be generated once, and
thereafter only need to store , with no need to retain the few-shot examples.
1) Large language models can perform in-context learning effectively. Large language models
have been shown to be able to learn from the context of a small number of examples or instructions
for a task, without any prior training on that task (Brown et al., 2020; Min et al., 2022; Wang et al.,
2022). This suggests that models can “understand” what the task is and how to tackle it based on a
few samples or descriptions of the task. This capability appears to improve as the models get larger
or are trained on more relevant data (Chowdhery et al., 2022; Ouyang et al., 2022; Bai et al., 2022).
2) Large language models can be adapted to downstream tasks by tuning a small set of param-
eters. Along with the growth in model sizes, there have been signiﬁcant advances in ﬁne-tuning
methods that only modify a small number of parameters (possibly adding some new ones) in a frozen
language model to adapt it to a speciﬁc task (Houlsby et al., 2019; Li and Liang, 2021; Lester et al.,
2021; Ding et al., 2022). These methods often achieve performance comparable to ﬁne-tuning all
parameters in the model. Importantly, the number of parameters that need to be changed is small
enough that it is feasible to train a model to generate them (Qin et al., 2021; Lester et al., 2022).
Taken together, these ﬁndings suggest that we may be able to use an auxiliary model that can ﬁrst
extract some task-relevant knowledge from some input that describes the task (e.g. instruction,
few-shot examples), and then generate a small number of adaptive parameters, thereby changing the
main model’s behavior to suit the task. This approach, if successful, would enable us to adapt models
to downstream applications without using backpropagation, or storing the encoded representations of
few-shot examples in memory. In other words, we can delegate the work of model adaptation to a
separate model.
We call this approach hypertuning , inspired by the work on hypernetworks by Ha et al. (2017). Hy-
pertuning uses a hypermodel to adapt a downstream model to a target downstream task or application.
This is differs from ﬁne-tuning , which uses backpropagation and a gradient descent algorithm to
update model parameters. In this work, we present one possible formulation of hypertuning using
few-shot examples and generating a small set of parameters with a single forward pass through the
hypermodel. However, this is just one possible way of performing hypertuning, and the idea of adapt-
ing models with hypermodels can be generalized to many other cases. For example, hypermodels
could also be trained to predict gradients or generate parameter updates based on input-output pairs.
This way, hypermodels could work with large training sets, not just a few examples. Ultimately,
with sufﬁciently general and well-trained hypermodels, we may be able to replace gradient-descent-
based ﬁne-tuning pipelines with hypertuning for many applications, while achieving similar or better
performance.
3

--- PAGE 4 ---
3.1 HyperTuning with Fewshot Examples
LetMbe a model with parameters , initialized at 0from pretraining, and La loss function. Given
a dataset of size Nwith input-output pairs f(x;y)g, standard ﬁne-tuning minimizes the following
objective over :
arg min
1
NX
f(x;y)gL
y;M(;x)
(1)
In the case of parameter-efﬁcient ﬁne-tuning (PEFT), we ﬁx =0and introduce a small set of
trainable parameters (e.g. adapter parameters, soft prompts) that are injected into M. We optimize
only over:
arg min
1
NX
f(x;y)gL
y;M(0;x;)
(2)
For hypertuning, we further deﬁne a hypermodelHwith parameters that produces PEFT parameters
^based on its input, which can be a set of few-shot examples or task instructions. For example, if the
hypermodel input is a set of few-shot examples f(xi;yi)gK, we have:
^=H
;f(xi;yi)gK
(3)
One way to train the hypermodel (H;)is to perform PEFT on many tasks and use the resulting 
as targets. However, this is costly in computation, requiring many ﬁne-tuning runs, and does not
leverage cross-task knowledge transfer. Instead, we propose to train the hypermodel end-to-end,
optimizing through the frozen model (M; 0). Hence, the hypermodel training objective is:
arg min
1
NX
f(x;y)g;ff(xi;yi)gKgL
y;M
0;x;H(;f(xi;yi)gK)
(4)
At each training step, we sample a target example (x;y)and non-overlapping few-shot examples
f(xi;yi)gK. We generate ^from the few-shot examples and compute the loss with respect to (x;y)
and^. We then back-propagate the gradients through both MandHto update.
Note that since ^does not depend on x, it can be computed once for a given set of few-shot examples
and reused for downstream predictions. At inference time, we can use ^directly without storing or
recomputing the representations for f(x;y)g;f(xi;yi)gK, saving memory and computation.2
4 HyperT5: A T5-Based HyperModel
4.1 Architecture and Setup
To demonstrate the feasibility of hypertuning, we propose HyperT5 , a hypermodel based on T5, where
both the hypermodel and the downstream model share a T5 backbone (Figure 2A). We use a frozen
LM-adapted T53as the downstream model. The hypermodel is also initialized with LM-adapted T5
parameters, but with some architectural changes. As deﬁned in Equation 3, the hypermodel encoder
takes the few-shot examples (and/or task deﬁnitions, in the case of S-NI) as input. The hypermodel
decoder takes a ﬁxed set of newly learned token embeddings as input, and outputs a set of decoder
token representations, which are then fed to a set of MLPs to generate the PEFT parameters for the
downstream model. We also remove the causal masking from the decoder, since the hypermodel does
not perform autoregressive generation.
2By construction, few-shot examples occupy at least K times the memory of the target input x.
3This is the model introduced by Lester et al. (2021). We use the T5 v1.1 architecture and initialize all
experiments with the LM-adapted parameters, unless stated otherwise.
4

--- PAGE 5 ---
Figure 2: Overview of HyperT5. (A) HyperT5 takes as input few-shot examples and outputs PEFT
parameters. The model is initialized from an LM-adapted T5. (B) In HyperT5-Preﬁx, are key and
value preﬁxes for every attention layer. (C) In HyperT5-LoRA, are additive low-rank modiﬁcations
to the query and value linear maps.
We experiment with two PEFT methods: preﬁx tuning (Li and Liang, 2021) and LoRA (Hu et al.,
2022). Preﬁx tuning (Figure 2B) prepends a set of learned key and value representations within each
attention layer, while LoRA (Figure 2C) learns a low-rank additive modiﬁcation to the query and
value linear maps. Both PEFT methods have been shown to achieve good performance across a
wide range of tasks (Ding et al., 2022). Chan et al. (2022) also suggest that modifying in-context
representations and model weights can lead to different model behaviors, and we seek to demonstrate
that hypertuning is applicable to very different PEFT methods. We name the respective hypermodels
HyperT5-Preﬁx andHyperT5-LoRA .
The number of decoder input tokens and the size of the MLPs depend on the choice of PEFT method
and its hyperparameters. For example, for HyperT5-Preﬁx that generates soft preﬁxes corresponding
to preﬁx tuning, will be of the shape [L;2;2;P;H ], whereLis the number of layers, 2 is for the
encoder and decoder, 2 is for the key and value preﬁxes, Pis the number of preﬁx tokens, and His
the hidden size. We set the number of decoder input tokens to be 2P. We provide pseudo-code for
HyperT5-Preﬁx and HyperT5-LoRA models in the Figure 7 and Figure 8 in the Appendix.
4.2 HyperPretraining
To train HyperT5, we ﬁrst undergo an additional stage of pretraining to adapt the hypermodel to
generate parameters for the downstream model, which we call hyperpretraining . As we show in
Section 5.5, hyperpretraining is crucial for good hypermodel performance.
We propose a simple scheme for hyperpretraining using a Context-Augmented Conditional Language
Modeling (CACLM) objective, which extends the conditional language-modeling (CLM) objective of
T5 LM-adaptation. As shown in Figure 3, we sample a 512-token sequence from a pretraining corpus
and split it into four consecutive segments A–D. The downstream model receives segment B as input
and predicts segment C, following the CLM objective. The hypermodel receives segments A and D
as input, which provide additional context from the same document, and outputs PEFT parameters
for the downstream model.4The hypermodel thus compresses contextual information to assist the
downstream model in its CLM task. We also make segment B very short (32 tokens) to encourage the
downstream model to depend on the hypermodel information for accurate prediction of tokens in C.
During hyperpretraining, we freeze the downstream model and only update the hypermodel parame-
ters, training for 100K steps on the C4 dataset (Raffel et al., 2020). We perform hyperpretraining
separately for HyperT5-Preﬁx and HyperT5-LoRA models. Hyperparameters can be found in
Appendix A.
5 Multi-Task Fine-Tuning with HyperT5
5.1 Multitask Fine-Tuning (MTF)
After hyperpretraining, we conduct a second stage of training to train the hypermodel to generate task-
speciﬁc PEFT parameters based on a small number of examples that we provide as input (Figure 1C).
By performing multi-task ﬁne-tuning on a sufﬁciently large number of tasks, we hope to have the
4Segments A and D are marked by sentinel tokens.
5

--- PAGE 6 ---
Figure 3: Overview of HyperPretraining using the Context-Augmented Conditional Language
Modeling (CACLM) objective to train a hypermodel to predict PEFT parameters . (A) Sample a
sequence of 512 tokens from a pretraining corpus, and splice into 4 segments A–D. (B) The frozen
downstream model takes as input B and predicts continuation C. (C) The hypermodel is trained to
encode additional context A and D into PEFT parameters , providing additional information to the
downstream model to predict C.
hypermodel learn to generalize to generate parameters for unseen tasks. We adopt a similar training
setup to MetaICL (Min et al., 2022), which uses multi-task ﬁne-tuning (Sanh et al., 2022; Wei et al.,
2022) with both a target input example ( x) and a set of few-shot input-output pairs f(xi;yi)gKas
inputs. The hypermodel takes the few-shot pairs as input, while the downstream model takes the
target example as input, as shown in Equation 3. We ﬁne-tune only the hypermodel parameters and
keep the downstream model parameters ﬁxed, unless otherwise stated. Appendix A.1 shows how we
format the few-shot inputs.
We compare our approach with two baselines: multi-task ﬁne-tuning of a T5 model without few-shot
inputs, and MetaICL (multi-task ﬁne-tuning with few-shot inputs). In MetaICL, the few-shot pairs
are concatenated with the target example as input, both during training and evaluation on new tasks.
We also include baselines that use PEFT methods for multi-task ﬁne-tuning, i.e. learning a single set
of preﬁx tuning or LoRA parameters.
We perform multi-task ﬁne-tuning for 10,000 steps with a batch size of 256. For models that use
few-shot inputs (MTF with fewshot, and hypermodels), we use up to 16 examples, and truncate
tokens that exceed the maximum input length. Appendix B provides more details on the datasets.
5.2 Datasets
To demonstrate the generality of our approach, we conduct experiments on three different multi-task
training datasets, each with different held-out tasks and evaluation protocols.
Public Pool of Prompts (P3) (Sanh et al., 2022) consists of 62 task datasets, and was used in training
the T0 models. The prompt are formatted with 0-shot inference in mind, and often contain instructions
or the possible answer options. For training our models, we use the T0-train subset. In order to ﬁt
multiple examples into the hypermodel’s context, we further exclude dataset-prompt subsets with
average input sequence lengths longer than 320 tokens. The list of included dataset-prompts can be
found in Figure 6. Evaluation is performed on a ﬁxed set of held-out tasks, based on multiple-choice
scoring with accuracy. We exclude StoryCloze from evaluation as the task is not distributed with
training data.
MetaICL (Min et al., 2022) introduced a few-shot multi-task training dataset, which is an extension
of CrossFit (Ye et al., 2021) with UniﬁedQA (Khashabi et al., 2020) and the addition of training data.
For brevity, we will refer to this dataset as MetaICL. Unlike P3 and S-NI, the task inputs are not
formatted for 0-shot inference; for instance, the task inputs may give no clue as to the goal of the
task, or what the output space is. They provide several different train-task splits for tasks, of which
we run our experiments on three (HR !LR, Non-NLI!NLI, Non-Class!Class) to economize on
computation costs. Evaluation is performed on held-out tasks, with ROUGE or Macro-F1 on model
generations depending on the task.
6

--- PAGE 7 ---
Super-NaturalInstructions (S-NI) (Wang et al., 2022) consists of over 1,600 task datasets, each
with a task deﬁnition as well as a ﬁxed set of positive and negative demonstrations. Following their
ﬁndings, we focus our experiments on two settings: using only the task deﬁnition as the hypermodel
input, and using deﬁnitions alongside two ﬁxed positive examples. We only use the English tasks
within the dataset. Evaluation is performed on a set of held-out tasks using ROUGE-L on model
generations.
5.3 Results
5.3.1 P3
ANLI HSwag CB COPA RTE WiC WSC WGD A VG
Full Fine-Tuning
T5-MTF 33.4 28.0 63.0 77.9 71.1 50.8 61.0 53.4 54.8
T5-MTF-Few-shot 35.3 27.5 68.6 70.5 75.2 51.7 62.1 52.2 55.4
Parameter-Efﬁcient Fine-Tuning (PEFT)
T5-MTF (Preﬁx) 33.1 26.1 53.9 67.8 60.5 49.8 54.7 51.4 49.7
T5-MTF (LoRA) 32.9 26.0 36.0 59.7 49.8 51.2 58.1 50.5 45.5
HyperTuning
HyperT5-Preﬁx 33.4 32.3 60.1 73.9 71.5 51.1 63.0 51.1 54.6
HyperT5-LoRA 33.6 33.0 49.5 74.2 67.4 52.0 64.0 52.9 53.3
HyperTuning + Fine-Tuning
HyperT5-Preﬁx+ 34.5 32.2 58.1 78.4 76.5 50.4 63.8 54.3 56.0
HyperT5-LoRA+ 33.9 30.7 62.1 75.8 72.3 50.8 64.6 54.5 55.6
Table 1: Results on P3 on held-out tasks (dev) with T5-Large models. T0 results taken from Sanh
et al. (2022).
ANLI HSwag CB COPA RTE WiC WSC WGD A VG
Full Fine-Tuning
T5-MTF 39.9 29.4 64.5 88.0 80.8 51.7 60.7 57.9 59.1
T5-MTF-Few-shot 37.9 30.9 67.6 90.5 76.6 51.2 63.3 61.1 59.9
Parameter-Efﬁcient Fine-Tuning (PEFT)
T5-MTF (Preﬁx) 38.3 31.2 61.4 82.4 78.6 52.6 57.0 54.3 57.0
T5-MTF (LoRA) 33.9 26.4 47.1 67.2 53.3 50.8 51.5 50.3 47.6
HyperTuning
HyperT5-Preﬁx 38.7 33.6 69.6 88.4 79.5 53.1 57.6 56.6 59.6
HyperT5-LoRA 35.3 30.8 66.4 83.3 68.5 50.3 60.0 56.1 56.4
Other results
T0 33.4 27.3 45.4 73.1 64.5 50.7 65.0 51.0 51.3
Table 2: Results on P3 on held-out tasks (dev) with T5-XL models. T0 results taken from Sanh et al.
(2022).
Table 1 and Table 2 show the results of our experiments on the P3 dataset using T5-Large ( 770M
parameters) and T5-XL ( 3B parameters), respectively.
We compare our HyperT5-Preﬁx and HyperT5-LoRA, which use hypermodels to generate task-
speciﬁc PEFT parameters based on few-shot examples, with several baselines: preﬁx tuning, LoRA
tuning, T5-MTF, and T5-MTF-Few-shot. T5-MTF is a model that roughly corresponds to the T0
model, and we detail the differences in Appendix B.1.
Our results show that both HyperT5-Preﬁx and HyperT5-LoRA signiﬁcantly improve over the preﬁx
and LoRA tuning baselines, indicating the effectiveness of using hypermodels to adapt the frozen
downstream T5 model to unseen tasks. HyperT5-Preﬁx achieves performance close to T5-MTF, while
7

--- PAGE 8 ---
T5-MTF-Few-shot attains the highest scores, in line with the ﬁndings of Min et al. (2022). These
patterns are consistent across T5-Large and T5-XL,5demonstrating the scalability of hypertuning.
We emphasize that HyperT5-Preﬁx/LoRA only introduces a very small number of PEFT parameters
in the frozen downstream T5 model, whereas all parameters are tuned in the T5-MTF and T5-MTF-
Few-shot models. Moreover, the P3 examples are written with prompt templates that are optimized
for zero-shot inference, which is the ideal input format for T5-MTF. Furthermore, T5-MTF-Fewshot
has full, bidirectional self-attention between the target input xand the few-shot examples, whereas
HyperT5-Preﬁx and HyperT5-Lora only incorporate information from the few-shot examples via the
respective PEFT parameters.
To investigate whether the hypermodel beneﬁts are complementary to updating the downstream
model parameters, we conduct an additional set of experiments where we jointly train both the
hypermodel and the downstream model (HyperTuning + Fine-Tuning), with results shown at the
bottom of Table 1. We observe that both HyperT5-Preﬁx+ and HyperT5-Lora+ slightly surpass
T5-MTF-Fewshot, suggesting that the hypermodels can further enhance the performance of ﬁne-tuned
downstream models.
5.3.2 MetaICL
Table 3 presents the results on three MetaICL task splits. As in the previous experiments, both
HyperT5 models surpass the PEFT models and T5-MTF in performance, except for T5-MTF-Few-
shot, which outperforms them in all but one case: Non-NLI !NLI, where HyperT5-Preﬁx achieves a
higher score. T5-MTF performs poorly in the MetaICL experiments, as it has to handle task examples
zero-shot, and the MetaICL inputs are not suitable for zero-shot inference, as explained above.
HR
!LRNon-NLI
!NLINon-Class
!Class
Full Fine-Tuning
T5-MTF 34.3 48.8 30.3
T5-MTF-Few-shot 41.0 56.7 40.6
Parameter-Efﬁcient Fine-Tuning (PEFT)
T5-MTF (Preﬁx) 29.8 42.8 29.6
T5-MTF (LoRA) 31.5 41.3 28.7
HyperTuning
HyperT5-Preﬁx 38.0 58.3 38.6
HyperT5-LoRA 35.4 54.2 34.8
Table 3: Results on MetaICL (Test) with T5-Large models.
5.3.3 Super-NaturalInstructions (S-NI)
We report the results on the different S-NI settings in Table 4 for T5-Large and Table 5 for T5-XL,
using both Def (deﬁnition-only) and Def+2Pos (deﬁnition and two ﬁxed positive examples) settings.
The T5-MTF (Def) and T5-MTF (Def+2Pos) models are similar to the corresponding T k-Instruct
variants (Wang et al., 2022), with a slight difference in input formatting (see Appendix A.1). For the
hypermodels, we prepend the task deﬁnitions to the few-shot examples and treat them as part of the
hypermodel input. On average, the HyperT5 with Def+2Pos outperforms T5-MTF (Def) by a large
margin, but still underperforms T5-MTF (Def+2Pos), in line with the above results.
5.4 Discussion
Above, we evaluated hypermodels on three multi-task datasets, where they generate task-speciﬁc soft
preﬁxes or LoRA parameters from a few examples or instructions. In general, HyperT5 matched
or exceeded T5-MTF models, but lagged behind T5-MTF-Fewshot models (or Def+2Pos models,
in the case of S-NI). This gap is expected, as T5-MTF-Fewshot uses full self-attention between the
5We note that T0-XL performs much worse than our trained T5-MTF, which is in agreement with other work
(Anonymous, 2023; Wu et al., 2022) that have reported similar results in replicating T0.
8

--- PAGE 9 ---
A VG
Full Fine-Tuning
T5-MTF (Def) 40.6
T5-MTF (Def+2Pos) 47.6
HyperTuning
HyperT5-Preﬁx (Def) 37.1
HyperT5-Preﬁx (Def+2Pos) 43.5
HyperT5-LoRA (Def) 34.9
HyperT5-LoRA (Def+2Pos) 42.0
Other Results
Tk-Instruct (Def+2Pos) 48.0
Table 4: Results on Super-NaturalInstuctions
(S-NI; Test) with T5-Large models. T k-
Instruct results taken from Wang et al. (2022).A VG
Full Fine-Tuning
T5-MTF (Def) 46.6
T5-MTF (Def+2Pos) 54.3
HyperTuning
HyperT5-Preﬁx (Def) 38.9
HyperT5-Preﬁx (Def+2Pos) 48.6
HyperT5-LoRA (Def) 38.9
HyperT5-LoRA (Def+2Pos) 45.0
Other Results
Tk-Instruct (Def+2Pos) 54.0
Table 5: Results on Super-NaturalInstuctions
(S-NI; Test) with T5-XL models. T k-Instruct
results taken from Wang et al. (2022).
examples and the target input x, while HyperT5 encodes the examples into PEFT parameters that are
independent of x. We attribute some of the gap to this limitation.
However, this limitation also confers efﬁciency advantages to HyperT5 at inference time compared
to T5-MTF-Fewshot. In encoder-decoders such as T5, the full self-attention between the examples
andxprevents the separation of their representations: a new forward pass is needed for each new x.
In contrast, for hypermodels the examples can be encoded into PEFT parameters once, and reused
for all subsequent inputs. Even for decoder-only models (e.g. MetaICL based on GPT-2), where the
examples can be cached as key and value representations, the cache size is likely much larger than the
PEFT parameters, as the cache stores all the representations for every token in the examples, which
are several times longer than the input by deﬁnition. Thus, hypermodels in our setup sacriﬁce some
performance for efﬁciency.
Regarding T5-MTF, one might wonder what the concrete beneﬁt of HyperT5 is, given their similar
performance. After all, unlike T5-MTF-Fewshot, T5-MTF only uses xas the input, requiring no
extra computation or memory, and only one set of model weights. Firstly, we stress that the HyperT5
model can only affect the downstream model through a small number of modiﬁed parameters,
while in T5-MTF all the parameters that process xare modiﬁed. Although HyperT5 and T5-MTF
have roughly the same number of tuned parameters, the parameters modiﬁed in T5-MTF directly
interact with the input x, which we expect to help performance. Secondly, we identify two separate,
but possibly related, sources of performance improvement: better general task performance of the
downstream model (which is usually the goal of MTF training), and adapting the downstream model
to a new task based on few-shot examples, using hypermodels in our case. Our aim in this work is
to show the feasibility of the latter. We argue that both sources are complementary, and we showed
in Section 5.3.1 that when we use hypermodels without freezing the downstream model, thereby
acquiring both beneﬁts, performance further improves. More generally, we expect that training a
hypermodel against an already multi-task ﬁne-tuned model will lead to better performance than just
using the model for zero-shot inference alone, and we plan to explore this in future work.
We also observe a consistent trend where HyperT5-Preﬁx outperforms HyperT5-LoRA. We speculate
that it is easier for hypermodels to learn to generate soft preﬁxes as compared to LoRA weights,
since soft preﬁx are effectively model-internal hidden states, and the generated PEFT parameters
are themselves transformations of the hypermodel hidden states. Incidentally, another possible
interpretation of the HyperT5-Preﬁx model is that the combination of the hypermodel and the
downstream model can be seen as a dual-encoder, single-decoder model with separate encoders for
the few-shot examples and the target example.
Lastly, the majority of the experiments were conducted with minimal hyperparameter-tuning, and
the current results primarily serve as a proof-of-concept of hypertuning being a viable approach
to adapt downstream models. We expect that further exploration of hyperpretraining and MTF
hyperparameters as well as hypermodel architectures may lead to better results and overcome some
of the limitations we identiﬁed.
9

--- PAGE 10 ---
0 20k 50k 70k 100k
HyperPretraining Steps505152535455P3 Held-out Performance
(a) HyperT5-Preﬁx
0 20k 50k 70k 100k
HyperPretraining Steps505152535455P3 Held-out Performance
 (b) HyperT5-LoRA
Figure 4: Performance of HyperT5 models on P3 evaluation with different amounts of hyperpretrain-
ing. HyperPretraining is crucial for good performance of the hypermodels. However, hyperpretraining
for too many steps can also hurt performance (as see in the case of HyperT5-LoRA).
5.5 Is HyperPretraining Necessary?
We demonstrate the beneﬁts of hyperpretraining for the hypermodels in this section. As mentioned in
Section 3, we hyperpretrained the hypermodels for 100k steps before multi-task ﬁne-tuning them on
P3 tasks. To examine the impact of hyperpretraining, we also multi-task ﬁne-tuned HyperT5-Preﬁx
and HyperT5-LoRA from LM-adapted T5 without any hyperpretraining, and from intermediate
checkpoints over the course of hyperpretraining. Figure 4 shows the average scores on the held-out
tasks for these models. Both HyperT5 models perform very poorly without any hyperpretraining,
achieving scores similar to PEFT-only (see Table 1). With hyperpretraining, the performance of
both hypermodels signiﬁcantly improves. While HyperT5-Preﬁx appears to consistently improve
over the course of 100k steps, we observe that HyperT5-LoRA performance slightly declines after
50k steps. Hypermodels targeting different PEFT methods may beneﬁt from different amounts of
hyperpretraining, and we emphasize that our choice of the number of hyperpretraining steps is by
no means considered to be optimal.6We expect that better hyperpretraining conﬁgurations can be
explored in future work.
6 HyperModels for Improved Parameter Initialization
Thus far, we have discussed hypermodels in the context of generating PEFT parameters in a single
forward pass through the hypermodel. We can also consider an alternative use of hypermodels:
Instead of randomly initializing new parameters, we can use hypermodels to produce task-speciﬁc
PEFT parameters based on a few examples from the task. This can be seen as using task knowledge
acquired by the hypermodel during training to provide a ﬁrst approximation of PEFT parameters, and
thereafter reﬁning the parmaeters via regular PEFT training.
In conventional PEFT, wherever new parameters are introduced into the model, they are either
initialized randomly, or with ﬁxed initial values (e.g. the up-projection weights in LoRA are initialized
to 0)–for brevity, we will refer to this simply as random initialization. Beyond random initialization,
Vu et al. (2021, SPoT) and Gu et al. (2021, PPT) have explored transfer-learning within PEFT, ﬁrst
doing PEFT on one or more upstream tasks, and then using the learned PEFT parameters as an
initialization for downstream PEFT.
This approach has two advantages over conventional PEFT initializations. First, the hypermodel-
generated parameters already perform well on the task, as shown in Section 5.3, so PEFT training can
reach good performance faster. Second, the hypermodel can automatically transfer relevant knowledge
from previous tasks to the new task, similar to SPoT and PPT, except we let the hypermodel determine
what previously learned task knowledge is most applicable to the new task. For instance, a major
challenge addressed in SPoT was searching for the set of upstream tasks whose PEFT parameters
would be the most appropriate initialization for a downstream task–in our case, we can directly
provide a hypermodel with few-shot examples to generate our desired initialization.
6We chose 100k steps based on the T5 LM-adaptation procedure (Lester et al., 2021).
10

--- PAGE 11 ---
0 5000 10000 15000 20000
Training Steps506070Accuracy Rand Init
Shared Init
Hyper Init(a) Preﬁx Tuning
0 5000 10000 15000 20000
Training Steps506070Accuracy Rand Init
Shared Init
Hyper Init (b) LoRA
Figure 5: Average performance on P3 held-out tasks with preﬁx tuning and LoRA, using different
parameter initializations. Using hypermodel-generated initializations starts with higher performance
and continues to perform better on average over the course of training.
To investigate the effectiveness of using hypermodels to generate PEFT initializations, we use the
P3-trained models from Section 5.3.1, and perform preﬁx tuning and LoRA tuning on the held-out
tasks individually.7For each method-task pair, we sweep across learning rates f1e 3;1e 4;1e 5g
and take the best average result over 3 random seeds.
We consider two baselines for initializations: random initialization (Rand Init) and using the multi-task
ﬁne-tuned PEFT parameters from Section 5.3.1 as initializations (Shared Init). The hypermodel-
generated initialization (Hyper Init) is generated using a randomly sampled set of 16 examples from
the respective training sets.
We show the results of preﬁx tuning8and LoRA tuning with different initialization schemes in Table 6.
We observe that for both preﬁx tuning and LoRA tuning, shared initialization signiﬁcantly performs
random initialization, while using a hypermodel-generated initialization outperforms both on average.
We also show the average performance across tasks over the course of tuning in Figure 5. We observe
that hypermodel-generated initializations start with much better performance compared to the other
two initialization schemes, and continue to outperform them over the course of ﬁne-tuning. Hence,
hypermodels can be complementary to a standard PEFT pipeline, providing both performance gains
and computational cost savings.
ANLI HSwg CB COPA RTE WiC WSC WGD A VG
Preﬁx (Rand Init) 54.6 50.5 98.8 79.0 78.8 71.6 63.5 52.2 68.6
Preﬁx (Shared Init) 60.8 51.6 99.4 85.7 84.8 72.4 72.6 65.1 74.0
Preﬁx (Hyper Init) 61.4 51.5 97.6 84.3 87.1 71.2 76.5 71.6 75.2
LoRA (Rand Init) 59.5 51.3 93.5 78.0 82.6 73.5 77.9 65.1 72.7
LoRA (Shared Init) 57.9 51.6 99.4 83.0 83.8 73.1 73.3 67.9 73.7
LoRA (Hyper Init) 57.7 48.4 99.4 87.3 84.1 73.0 83.9 66.2 75.0
Table 6: Preﬁx tuning and LoRA ﬁne-tuning on T5-Large with different initializations on P3 held-out
tasks. Using HyperT5-generated parameters as an initialization achieves better performance on
average than using shared MTF PEFT parameters or random initialization.
7 Conclusion
We introduce the concept of hypertuning , which leverages a hypermodel to adapt a downstream
model to a speciﬁc downstream application. We present a basic framework for hypertuning, where
a hypermodel is trained to produce parameters for a downstream model from few-shot examples
in one forward pass, and we apply this framework to train HyperT5-Preﬁx and HyperT5-LoRA
models that can adapt a ﬁxed downstream T5 model. We ﬁnd that a two-stage training procedure of
hyperpretraining and multi-task ﬁne-tuning is effective for training hypermodels, and we evaluate the
7We use one speciﬁc prompt format for each task, listed in Appendix B.1.
8Preﬁx tuning is performed via a reparameterization, in line with standard practice. Refer to Appendix D for
details.
11

--- PAGE 12 ---
HyperT5 models on P3, MetaICL and S-NI datasets, showing that they can generate PEFT parameters
that enable the downstream T5 models to perform well on unseen tasks. Furthermore, the parameters
generated by hypertuning can also serve as improved parameter initializations for parameter-efﬁcient
ﬁne-tuning. We regard these ﬁndings as an initial but encouraging indication of the potential of
adapting large language models without back-propagation.
8 Acknowledgements
We would like to thank Sam Bowman for their thoughtful feedback and Jonas Pfeiffer for early idea
discussion.
References
Anonymous. 2023. Pretraining One Language Model for All With the Text-to-text Framework Using
Model-generated Signals. In Submitted to The Eleventh International Conference on Learning
Representations . Under review.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson
Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatﬁeld-Dodds, Danny Hernandez,
Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario
Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
2022. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human
Feedback.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language Models are Few-shot Learners. In Advances in Neural
Information Processing Systems , volume 33, pages 1877–1901. Curran Associates, Inc.
Stephanie C. Y . Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew K. Lampinen,
and Felix Hill. 2022. Transformers generalize differently from information stored in context vs in
weights.
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via Language
Model In-context Tuning. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages 719–730, Dublin, Ireland. Association
for Computational Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon
Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,
Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff
Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. 2022. Scaling
Instruction-ﬁnetuned Language Models.
12

--- PAGE 13 ---
Budhaditya Deb, Guoqing Zheng, and Ahmed Hassan Awadallah. 2022. Boosting Natural Language
Generation from Instructions with Meta-learning.
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022. 8-bit Optimizers via Block-
wise Quantization. 9th International Conference on Learning Representations, ICLR .
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
Chen, Chi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao
Zheng, Jianfei Chen, Yang Liu, Jie Tang, Juanzi Li, and Maosong Sun. 2022. Delta Tuning: A
Comprehensive Study of Parameter Efﬁcient Methods for Pre-trained Language Models.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic Meta-learning for Fast
Adaptation of Deep Networks. In Proceedings of the 34th International Conference on Machine
Learning , volume 70 of Proceedings of Machine Learning Research , pages 1126–1135. PMLR.
Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. PPT: Pre-trained Prompt Tuning for
Few-shot Learning.
David Ha, Andrew M. Dai, and Quoc V . Le. 2017. HyperNetworks. In International Conference on
Learning Representations .
Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, Yaguang Li, Zhao
Chen, Donald Metzler, Heng-Tze Cheng, and Ed H. Chi. 2022. HyperPrompt: Prompt-based
task-conditioning of transformers. In Proceedings of the 39th International Conference on Machine
Learning , volume 162 of Proceedings of Machine Learning Research , pages 8678–8690. PMLR.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efﬁcient Transfer Learning for
NLP. CoRR , abs/1902.00751.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International
Conference on Learning Representations .
Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2021.
Parameter-efﬁcient Multi-task Fine-tuning for Transformers via Shared Hypernetworks. In Pro-
ceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages
565–576, Online. Association for Computational Linguistics.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and
Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system.
InFindings of the Association for Computational Linguistics: EMNLP 2020 , pages 1896–1907,
Online. Association for Computational Linguistics.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efﬁcient
Prompt Tuning.
Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah Constant. 2022. Reducing Retraining by
Recycling Parameter-Efﬁcient Prompts.
Xiang Lisa Li and Percy Liang. 2021. Preﬁx-Tuning: Optimizing Continuous Prompts for Generation.
InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) ,
pages 4582–4597, Online. Association for Computational Linguistics.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to
Learn In Context. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pages 2791–2809,
Seattle, United States. Association for Computational Linguistics.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe. 2022. Training language models to follow instructions with human feedback.
13

--- PAGE 14 ---
William Peebles, Ilija Radosavovic, Tim Brooks, Alexei A. Efros, and Jitendra Malik. 2022. Learning
to learn with generative models of neural network checkpoints.
Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2020.
AdapterFusion: Non-Destructive Task Composition for Transfer Learning. CoRR , abs/2005.00247.
Jason Phang, Thibault Févry, and Samuel R. Bowman. 2018. Sentence Encoders on STILTs:
Supplementary Training on Intermediate Labeled-data Tasks.
Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe
Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer
learning with pretrained language models: When and why does it work? In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics , pages 5231–5247, Online.
Association for Computational Linguistics.
Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu,
Juanzi Li, Lei Hou, Peng Li, Maosong Sun, and Jie Zhou. 2021. Exploring Universal Intrinsic
Task Subspace via Prompt Tuning.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Uniﬁed
Text-to-Text Transformer. Journal of Machine Learning Research , 21(140):1–67.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: Memory
Optimizations toward Training Trillion Parameter Models. In Proceedings of the International
Conference for High Performance Computing, Networking, Storage and Analysis , SC ’20. IEEE
Press.
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-
bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,
Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask Prompted
Training Enables Zero-Shot Task Generalization. In International Conference on Learning Repre-
sentations .
Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2021. SPoT: Better Frozen
Model Adaptation through Soft Prompt Transfer. CoRR , abs/2110.07904.
Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-
Micke, Subhransu Maji, and Mohit Iyyer. 2020. Exploring and Predicting Transferability across
NLP Tasks.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana
Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak,
Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby
Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar,
Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang
Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro,
Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and
Daniel Khashabi. 2022. Super-NaturalInstructions: Generalization via Declarative Instructions on
1600+ NLP Tasks.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M. Dai, and Quoc V Le. 2022. Finetuned Language Models are Zero-Shot Learners. In
International Conference on Learning Representations .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art
14

--- PAGE 15 ---
natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations , pages 38–45, Online. Association for
Computational Linguistics.
Zhaofeng Wu, Robert L. Logan IV , Pete Walsh, Akshita Bhagia, Dirk Groeneveld, Sameer Singh,
and Iz Beltagy. 2022. Continued Pretraining for Better Zero- and Few-Shot Promptability. In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing .
Association for Computational Linguistics.
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. CrossFit: A Few-shot Learning Challenge for
Cross-task Generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing , pages 7163–7189, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
15

--- PAGE 16 ---
A Training Details
All experiments are trained with 1-bit Adam (Dettmers et al., 2022) and batch size of 256, a
learning rate of 5e-5, and a linear decay schedule. Training was performed with ZeRO (Rajbhandari
et al., 2020) and Transformers (Wolf et al., 2020). For hypermodels, the hypermodel’s max input
sequence length is 1024 tokens and the downstream model’s max input sequence length is 384
tokens. Correspondingly, the max input sequence length for all non-few-shot models (e.g. T5-MTF,
T5-MTF(Preﬁx)) is 384. The max input sequence length of few-shot models (e.g. T5-MTF-Few-shot)
is thus conservatively set at 1024+384=1408 tokens. The max target sequence length is set to 128 for
all experiments.
A.1 Input Formatting
Few-shot examples for hypermodels are formatted in the following manner:
<x> Input 1 <y> Target 1 <x> Input 2 <y> Target 2 <x> Input 3 <y> Target 3
where <x>and<y>and special tokens.
For S-NI, the task deﬁnitions are treated as just another example:
<x> Instruction <x> Input 1 <y>Target 1 <x> Input 2 <y>Target 2
B Dataset-speciﬁc Details
B.1 P3 / T0
We highlight some differences our T0 baselines and the T0 setup described in the original paper (Sanh
et al., 2022). Besides the different optimizers and batch sizes listed above, we do not use packing to
process our training data. Moreover, because our focus is on few-shot learning, we remove a number
of tasks formulations with longer inputs from the T0-train dataset, listed in Section 6. For T0, we use
an input sequence length of 384 and output length of 128, which matches the input and output lengths
of the downstream model in our hypermodel setup. For T5-MTF-Few-shot, we use an input sequence
length of 1024+384=1408, which is the combined input lengths of the hypermodel and downstream
model. We believe that these changes can meaningfully modify the performance of the T0 models,
but provide a fairer baseline to the hypermodel setup.
16

--- PAGE 17 ---
adversarial_qa_dbert_answer_the_following_q, adversarial_qa_dbert_based_on, adversarial_qa_dbert_generate_question,
adversarial_qa_dbert_question_context_answer, adversarial_qa_dbert_tell_what_it_is, adversarial_qa_dbidaf_answer_the_following_q,
adversarial_qa_dbidaf_based_on, adversarial_qa_dbidaf_generate_question, adversarial_qa_dbidaf_question_context_answer, adversarial_qa_dbidaf_tell_what_it_is,
adversarial_qa_droberta_answer_the_following_q, adversarial_qa_droberta_based_on, adversarial_qa_droberta_generate_question,
adversarial_qa_droberta_question_context_answer, adversarial_qa_droberta_tell_what_it_is, ag_news_classify, ag_news_classify_question_ﬁrst,
ag_news_classify_with_choices, ag_news_classify_with_choices_question_ﬁrst, ag_news_recommend, ag_news_which_section, ag_news_which_section_choices,
amazon_polarity_Is_this_product_review_positive, amazon_polarity_Is_this_review, amazon_polarity_Is_this_review_negative,
amazon_polarity_User_recommend_this_product, amazon_polarity_convey_negative_or_positive_sentiment, amazon_polarity_ﬂattering_or_not,
amazon_polarity_negative_or_positive_tone, amazon_polarity_user_satisﬁed, amazon_polarity_would_you_buy, app_reviews_categorize_rating_using_review,
app_reviews_convert_to_rating, app_reviews_convert_to_star_rating, app_reviews_generate_review, cnn_dailymail_3.0.0_generate_story,
cnn_dailymail_3.0.0_spice_up_story, common_gen_Example_prompt, common_gen_Given_concepts_type_1, common_gen_Given_concepts_type_2,
common_gen_Put_together, common_gen_choice_in_concept_centric_sentence_generation, common_gen_random_task_template_prompt,
common_gen_sentence_to_concepts, common_gen_topic_to_sentence, common_gen_topics_from_the_sentence, cos_e_v1.11_aligned_with_common_sense,
cos_e_v1.11_description_question_option_id, cos_e_v1.11_description_question_option_text, cos_e_v1.11_explain_why_human,
cos_e_v1.11_generate_explanation_given_text, cos_e_v1.11_i_think, cos_e_v1.11_question_description_option_id, cos_e_v1.11_question_description_option_text,
cos_e_v1.11_question_option_description_id, cos_e_v1.11_question_option_description_text, cos_e_v1.11_rationale, cosmos_qa_context_answer_to_question,
cosmos_qa_context_description_question_answer_id, cosmos_qa_context_description_question_answer_text, cosmos_qa_context_description_question_text,
cosmos_qa_context_question_description_answer_id, cosmos_qa_context_question_description_answer_text, cosmos_qa_context_question_description_text,
cosmos_qa_description_context_question_answer_id, cosmos_qa_description_context_question_answer_text, cosmos_qa_description_context_question_text,
cosmos_qa_no_prompt_id, cosmos_qa_no_prompt_text, cosmos_qa_only_question_answer, dbpedia_14_given_a_choice_of_categories_,
dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to, dbpedia_14_given_list_what_category_does_the_paragraph_belong_to,
dbpedia_14_pick_one_category_for_the_following_text, dream_answer_to_dialogue, dream_baseline, dream_generate_ﬁrst_utterance,
dream_generate_last_utterance, dream_read_the_following_conversation_and_answer_the_question, duorc_ParaphraseRC_build_story_around_qa,
duorc_SelfRC_build_story_around_qa, gigaword_TLDR, gigaword_ﬁrst_sentence_title, gigaword_generate_summary_for_this, gigaword_in_a_nutshell,
gigaword_make_a_title, gigaword_reverse_writing, gigaword_write_a_title_for_this_sentence, gigaword_write_an_article, gigaword_write_its_sentence,
glue_mrpc_equivalent, glue_mrpc_generate_paraphrase, glue_mrpc_generate_sentence, glue_mrpc_paraphrase, glue_mrpc_replace, glue_mrpc_same_thing,
glue_mrpc_want_to_know, glue_qqp_answer, glue_qqp_duplicate, glue_qqp_duplicate_or_not, glue_qqp_meaning, glue_qqp_quora, glue_qqp_same_thing,
imdb_Movie_Expressed_Sentiment, imdb_Movie_Expressed_Sentiment_2, imdb_Negation_template_for_positive_and_negative, imdb_Reviewer_Enjoyment,
imdb_Reviewer_Enjoyment_Yes_No, imdb_Reviewer_Expressed_Sentiment, imdb_Reviewer_Opinion_bad_good_choices, imdb_Reviewer_Sentiment_Feeling,
imdb_Sentiment_with_choices_, imdb_Text_Expressed_Sentiment, imdb_Writer_Expressed_Sentiment, kilt_tasks_hotpotqa_combining_facts,
kilt_tasks_hotpotqa_complex_question, kilt_tasks_hotpotqa_ﬁnal_exam, kilt_tasks_hotpotqa_formulate, kilt_tasks_hotpotqa_straighforward_qa,
paws_labeled_ﬁnal_Concatenation, paws_labeled_ﬁnal_Concatenation_no_label, paws_labeled_ﬁnal_Meaning, paws_labeled_ﬁnal_Meaning_no_label,
paws_labeled_ﬁnal_PAWS_ANLI_GPT3, paws_labeled_ﬁnal_PAWS_ANLI_GPT3_no_label, paws_labeled_ﬁnal_Rewrite, paws_labeled_ﬁnal_Rewrite_no_label,
paws_labeled_ﬁnal_context_question, paws_labeled_ﬁnal_context_question_no_label, paws_labeled_ﬁnal_paraphrase_task,
paws_labeled_ﬁnal_task_description_no_label, qasc_is_correct_1, qasc_is_correct_2, qasc_qa_with_combined_facts_1, qasc_qa_with_separated_facts_1,
qasc_qa_with_separated_facts_2, qasc_qa_with_separated_facts_3, qasc_qa_with_separated_facts_4, qasc_qa_with_separated_facts_5, quarel_choose_between,
quarel_do_not_use, quarel_heres_a_story, quarel_logic_test, quarel_testing_students, quartz_answer_question_based_on, quartz_answer_question_below,
quartz_given_the_fact_answer_the_q, quartz_having_read_above_passage, quartz_paragraph_question_plain_concat, quartz_read_passage_below_choose,
quartz_use_info_from_paragraph_question, quartz_use_info_from_question_paragraph, ropes_background_new_situation_answer,
ropes_background_situation_middle, ropes_given_background_situation, ropes_new_situation_background_answer, ropes_plain_background_situation,
ropes_plain_bottom_hint, ropes_plain_no_background, ropes_prompt_beginning, ropes_prompt_bottom_hint_beginning, ropes_prompt_bottom_no_hint,
ropes_prompt_mix, ropes_read_background_situation, rotten_tomatoes_Movie_Expressed_Sentiment, rotten_tomatoes_Movie_Expressed_Sentiment_2,
rotten_tomatoes_Reviewer_Enjoyment, rotten_tomatoes_Reviewer_Enjoyment_Yes_No, rotten_tomatoes_Reviewer_Expressed_Sentiment,
rotten_tomatoes_Reviewer_Opinion_bad_good_choices, rotten_tomatoes_Reviewer_Sentiment_Feeling, rotten_tomatoes_Sentiment_with_choices_,
rotten_tomatoes_Text_Expressed_Sentiment, rotten_tomatoes_Writer_Expressed_Sentiment, samsum_Generate_a_summary_for_this_dialogue,
samsum_Given_the_above_dialogue_write_a_summary, samsum_Sum_up_the_following_dialogue, samsum_Summarize_, samsum_Summarize_this_dialogue_,
samsum_To_sum_up_this_dialog, samsum_Write_a_dialogue_that_match_this_summary, sciq_Direct_Question, sciq_Direct_Question_Closed_Book_,
sciq_Multiple_Choice, sciq_Multiple_Choice_Closed_Book_, sciq_Multiple_Choice_Question_First, social_i_qa_Check_if_a_random_answer_is_valid_or_not,
social_i_qa_Generate_answer, social_i_qa_Generate_the_question_from_the_answer, social_i_qa_I_was_wondering,
social_i_qa_Show_choices_and_generate_answer, social_i_qa_Show_choices_and_generate_index, trec_ﬁne_grained_ABBR,
trec_ﬁne_grained_ABBR_context_ﬁrst, trec_ﬁne_grained_DESC, trec_ﬁne_grained_DESC_context_ﬁrst, trec_ﬁne_grained_ENTY , trec_ﬁne_grained_HUM,
trec_ﬁne_grained_HUM_context_ﬁrst, trec_ﬁne_grained_LOC, trec_ﬁne_grained_LOC_context_ﬁrst, trec_ﬁne_grained_NUM,
trec_ﬁne_grained_NUM_context_ﬁrst, trec_ﬁne_grained_open, trec_ﬁne_grained_open_context_ﬁrst, trec_pick_the_best_descriptor, trec_trec1, trec_trec2,
trec_what_category_best_describe, trec_which_category_best_describes, wiki_bio_comprehension, wiki_bio_guess_person, wiki_bio_key_content,
wiki_bio_what_content, wiki_bio_who, wiki_qa_Decide_good_answer, wiki_qa_Direct_Answer_to_Question, wiki_qa_Generate_Question_from_Topic,
wiki_qa_Is_This_True_, wiki_qa_Jeopardy_style, wiki_qa_Topic_Prediction_Answer_Only, wiki_qa_Topic_Prediction_Question_Only,
wiki_qa_Topic_Prediction_Question_and_Answer_Pair, wiki_qa_automatic_system, wiki_qa_exercise, wiki_qa_found_on_google,
wiqa_does_the_supposed_perturbation_have_an_effect, wiqa_effect_with_label_answer, wiqa_effect_with_string_answer,
wiqa_what_is_the_ﬁnal_step_of_the_following_process, wiqa_what_is_the_missing_ﬁrst_step, wiqa_what_might_be_the_ﬁrst_step_of_the_process,
wiqa_what_might_be_the_last_step_of_the_process, wiqa_which_of_the_following_is_the_supposed_perturbation, yelp_review_full_based_on_that,
yelp_review_full_format_rating, yelp_review_full_format_score, yelp_review_full_format_star, yelp_review_full_on_a_scale, yelp_review_full_so_i_would,
yelp_review_full_this_place
Figure 6: List of P3 dataset-prompts used for training. We chose a subset of T0-train with average
input lengths shorter than 320 tokens.
17

--- PAGE 18 ---
For the hypermodel initialization/PEFT experiments, we do single-task parameter-efﬁcient ﬁne-tuning
on each of the following dataset-prompts:
1. anli_GPT_3_style_r1
2. hellaswag_complete_ﬁrst_then
3. super_glue_cb_GPT_3_style
4. super_glue_copa_C1_or_C2_premise_so_because_
5. super_glue_rte_GPT_3_style
6. super_glue_wic_GPT_3_prompt
7. super_glue_wsc.ﬁxed_GPT_3_Style
8. winogrande_winogrande_debiased_Replace
B.2 S-NI / T-KI
To standardize the preprocessing across our experiments, we do not use the input formatting provided
in the original work (Wang et al., 2022). Instead, we use the format described in Appendix A.1 for all
experiments. Given that the same format is used in multi-task ﬁne-tuning and evaluation, this should
not unfairly advantage any model. However, because the format deviates from that of the original
work, we do not directly evaluate the T-KI models.
Additionally, the Super-NaturalInstructions dataset (previously known as NaturalInstructions-v2) has
undergone some changes over time. In our experiments, we use the v2.5 version of the dataset.
B.3 MetaICL
C Model Details
D Elaboration on Preﬁx Tuning Comparisons
While preﬁx tuning is generally presented as learning a set of prepended key and value representations
for each Transformer layer, in practice, the learned preﬁxes are not optimized directly. In the work
that introduced preﬁx tuning (Li and Liang, 2021), Section 4.3 explains that directly optimizing
the learned preﬁxes leads to unstable training and poorer performance, and instead recommend
optimizing a set of learned embeddings and a parameterized MLP to generate the learned preﬁxes.
(At inference time, the preﬁxes can be generated from the learned components–this only impacts the
training process.) We conﬁrmed in our experiments that directly optimizing preﬁxes leads to poor
perfomance, and other works involving preﬁx tuning have similarly used this preﬁx reparamterization
Hence, we have two ﬂavors of preﬁx tuning to consider: directly optimizing over preﬁxes (“Preﬁx-
Flat"), and optimizing with reparamterization (“Preﬁx-MLP"). The T5-MTF (Preﬁx) model uses
Preﬁx-MLP, which is the appropriate approach to tuning preﬁxes in that setting. However, because
HyperT5-Preﬁx only generates the ﬁnal preﬁxes, only Preﬁx-Flat tuning is possible. Hence, when
we perform the preﬁx tuning with different initializations in Section 6, we cannot fairly compare the
two methods directly–one which uses a reparameterization during training, and the other which uses
direct optimization which we know performs worse in practice.
Instead, we compare preﬁx tuning in the two different settings, Preﬁx-Flat and Preﬁx-MLP, completely
separately. We describe each individual initialization scheme:
Preﬁx-Flat
1. Preﬁx-Flat (Rand): Randomly initialize soft preﬁxes
2.Preﬁx-Flat (Shared): Run a forward pass through the preﬁx reparameterization to obtain the
ﬂat preﬁxes, and use them as the initialization
3. Preﬁx-Flat (Hyper): Generate preﬁxes with HyperT5-Preﬁx
18

--- PAGE 19 ---
# B = batch_size
# T = input_length
# P = number of prompt tokens
# H = hidden_dim
# L = num layers in encoder/decoder
# Shape: [B, T]
fewshot_input_ids = ...
# Shape: [B, T, H]
hyper_enc_out = hypermodel.encoder(fewshot_input_ids)
# Shape: [B, 2P, H]
# Decoder implicitly uses a fixed set of input embeddings of size 2P
hyper_dec_out = hypermodel.decoder(hyper_enc_out)
# Shape: [B, P, LH]
downstream_enc_k_prefix = hypermodel.enc_k_head(hyper_dec_out[:, :P, :])
downstream_enc_v_prefix = hypermodel.enc_v_head(hyper_dec_out[:, :P, :])
downstream_dec_k_prefix = hypermodel.dec_k_head(hyper_dec_out[:, P:, :])
downstream_dec_v_prefix = hypermodel.dec_v_head(hyper_dec_out[:, P:, :])
# Shape: [B, P, L H]
downstream_enc_k_prefix = downstream_enc_k_prefix.reshape(B, P, L, H)
downstream_enc_v_prefix = downstream_enc_v_prefix.reshape(B, P, L, H)
downstream_dec_k_prefix = downstream_dec_k_prefix.reshape(B, P, L, H)
downstream_dec_v_prefix = downstream_dec_v_prefix.reshape(B, P, L, H)
# These correspond to the per-layer learned prefixes for K and V
# where each of the heads is defined (e.g.):
hypermode.enc_k_head = nn.Sequential([
nn.LayerNorm(),
nn.Linear(H),
nn.TanH(),
nn.Linear(L*H),
])
Figure 7: Pseudo-code for HyperT5-Preﬁx
Preﬁx-MLP
1.Preﬁx-MLP (Rand): Randomly initialize the preﬁx reparameterization embeddings and
MLPs (i.e. conventional preﬁx tuning)
2. Preﬁx-MLP (Shared): Directly reuse the preﬁx reparameterization from T5-MTF (Preﬁx)
3.Preﬁx-MLP (Hyper): We train an entirely new HyperT5-Preﬁx-MLP model, where the
parameter generation heads directly correspond to the preﬁx tuning reparameterization
MLPs. The encoder-decoder in the hypermodel will output the “embeddings", and we
directly reuse the parameter generation heads during tuning.
The results for Preﬁx-MLP are presented in the body of the paper in Section 6. We believe that this
approach provides the fairest comparison of initializations. Importantly, both Preﬁx-MLP (Shared)
and Preﬂix-MLP (Hyper) have been trained on the same number of labeled examples (not including
the few-shot examples, which are inputs), but where the Preﬁx-MLP uses a single set of learned
embeddings, HyperT5-Preﬁx-MLP generates the embeddings based on few-shot examples.
We present the full set of preﬁx tuning results in Table 7, the performance of Preﬁx-Flat Figure 9.
19

--- PAGE 20 ---
# B = batch_size
# T = input_length
# R = LoRA rank
# H = hidden_dim
# L = num layers in encoder/decoder
# Shape: [B, T]
fewshot_input_ids = ...
# Shape: [B, T, H]
hyper_enc_out = hypermodel.encoder(fewshot_input_ids)
# Shape: [B, 3L, H]
# Decoder implicitly uses a fixed set of input embeddings of size 3L
hyper_dec_out = hypermodel.decoder(hyper_enc_out)
# Shape: [B, L, H]
enc_repr = hyper_dec_out[:, :L, :]
dec_repr = hyper_dec_out[:, L:2*L, :]
cross_repr = hyper_dec_out[:, 2*L:, :]
# Repeat for dec_repr, cross_repr for decoder self- and cross-attention
# Shape: [B, L, 2RH]
enc_q_repr = hypermodel.enc_q_head(enc_repr)
enc_v_repr = hypermodel.enc_v_head(enc_repr)
# Shape: [B, L, 2RH]
enc_q_repr = enc_q_repr.reshape(B, L, 2, R, H)
enc_v_repr = enc_v_repr.reshape(B, L, 2, R, H)
# raw_enc_q_gate and raw_enc_v_gate are learned parameters of size [L]
# Shape: [1, L, 1, 1, 1]
enc_q_gate = torch.tanh(raw_enc_q_gate)[None, :, None, None, None]
enc_v_gate = torch.tanh(raw_enc_v_gate)[None, :, None, None, None]
# Shape: List of [B, R, H]
enc_lora_q_up_list = [enc_q_repr[:, l, 0, :, :] for l in range(L)]
enc_lora_q_down_list = [enc_q_repr[:, l, 1, :, :] for l in range(L)]
enc_lora_v_up_list = [enc_v_repr[:, l, 0, :, :] for l in range(L)]
enc_lora_v_down_list = [enc_v_repr[:, l, 1, :, :] for l in range(L)]
# These correspond to up- and down-map deltas in LoRA in Q and V
# attention linear maps
# where each of the heads is defined (e.g.):
hypermode.enc_q_head = nn.Sequential([
nn.LayerNorm(),
nn.Linear(H),
nn.TanH(),
nn.Linear(2*R*H),
])
Figure 8: Pseudo-code for HyperT5-LoRA
20

--- PAGE 21 ---
ANLI HSwag CB COPA RTE WiC WSC WGD A VG
Preﬁx-Flat (Rand Init) 43.6 36.3 82.7 74.0 72.9 64.4 64.2 53.0 61.4
Preﬁx-Flat (Shared Init) 54.3 40.4 98.8 82.7 83.9 71.0 67.4 57.1 69.4
Preﬁx-Flat (Hyper Init) 56.6 43.5 91.7 84.3 85.3 69.3 73.0 67.6 71.4
Preﬁx-MLP (Rand Init) 54.6 50.5 98.8 79.0 78.8 71.6 63.5 52.2 68.6
Preﬁx-MLP (Shared Init) 60.8 51.6 99.4 85.7 84.8 72.4 72.6 65.1 74.0
Preﬁx-MLP (Hyper Init) 61.4 51.5 97.6 84.3 87.1 71.2 76.5 71.6 75.2
LoRA (Rand Init) 59.5 51.3 93.5 78.0 82.6 73.5 77.9 65.1 72.7
LoRA (Shared Init) 57.9 51.6 99.4 83.0 83.8 73.1 73.3 67.9 73.7
LoRA (Hyper Init) 57.7 48.4 99.4 87.3 84.1 73.0 83.9 66.2 75.0
Table 7: Preﬁx tuning (Flat and MLP) and LoRA ﬁne-tuning on T5-Large with different initializations
on P3 held-out tasks.
21

--- PAGE 22 ---
0 5000 10000 15000 20000
Training Steps506070Accuracy Rand Init
Shared Init
Hyper InitFigure 9: Average performance on P3 held-out tasks with preﬁx tuning (ﬂat).
22
