# 2312.16218.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypernetwork/2312.16218.pdf
# File size: 13294049 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Hyper-VolTran: Fast and Generalizable
One-Shot Image to 3D Object Structure via HyperNetworks
Christian Simon Sen He Juan-Manuel P ´erez-R ´ua Mengmeng Xu
Amine Benhalloum Tao Xiang
Meta
Abstract
Solving image-to-3D from a single view is an ill-posed
problem, and current neural reconstruction methods address-
ing it through diffusion models still rely on scene-specific
optimization, constraining their generalization capability.
To overcome the limitations of existing approaches regarding
generalization and consistency, we introduce a novel neu-
ral rendering technique. Our approach employs the signed
distance function (SDF) as the surface representation and in-
corporates generalizable priors through geometry-encoding
volumes and HyperNetworks. Specifically, our method builds
neural encoding volumes from generated multi-view inputs.
We adjust the weights of the SDF network conditioned on
an input image at test-time to allow model adaptation to
novel scenes in a feed-forward manner via HyperNetworks.
To mitigate artifacts derived from the synthesized views, we
propose the use of a volume transformer module to improve
the aggregation of image features instead of processing each
viewpoint separately. Through our proposed method, dubbed
asHyper-VolTran , we avoid the bottleneck of scene-specific
optimization and maintain consistency across the images
generated from multiple viewpoints. Our experiments show
the advantages of our proposed approach with consistent
results and rapid generation.
1. Introduction
Recent progress in neural 3D reconstruction has brought sig-
nificant implications in various applications, e.g., novel view
synthesis [ 1,32,38,39], and robotic vision [ 10,13,28,37].
Specifically, there has been a growing interest in neural
fields [ 1,39,44] to extract 3D information from multiple
images given known camera parameters. NeRF [ 19] and
the Signed Distance Function (SDF) [ 43] are image-to-3D
reconstruction techniques [ 1,2,36] that produce plausible
geometry and, hence, novel views. Despite great progress,
achieving accurate 3D object reconstruction via neural im-
plicit methods [ 1,19,43] still requires a substantial number
of images featuring consistent view and appearance and pre-
Point-eShap-eZero123+SDOne2345Hyper-VolTran (Ours)101001000
0.91.11.31.51.71.92.1Running Time (sec)Chamfer Distance (L2)
Synthesize Multi-ViewscameraInconsistentNeural Encoding Volume
Mesh
RGBSDF Weight GeneratorHyper-VolTranTransformerand SDF NetworkInputImageOur PipelineComparisonFigure 1. Top: Comparison of our proposed method against base-
lines on the running time and Chamfer Distance with the bubble
area indicating IoU. Bottom : Our pipeline comprises two compo-
nents for image-to-3D by synthesizing multi-views from a diffusion
model and mapping from multi-views to SDFs using an SDF net-
work with weights generated from a HyperNetwork.
cise camera poses to reconstruct 3D objects accurately.
In fact, collecting data from multiple views might not
always be feasible when the resources are limited. Several
works [ 2,17,44] demonstrate a capability to mitigate issues
on 3D reconstruction under a sparse set of images. One key
technique in these approaches is to build neural encoding
volume projected from multiple input views. Though these
techniques can perform on limited inputs, reconstructing
3D from a single image remains challenging and requires a
strong prior to enabling the neural reconstruction model to
produce plausible shapes and colors of unseen perspectives.
A recent development in generative models [ 4,16,26,45]
has shown promising results in 2D image generation that
can act as a strong prior for unseen perspectives. Several
1arXiv:2312.16218v2  [cs.CV]  5 Jan 2024

--- PAGE 2 ---
works approach this problem using the guidance of a diffu-
sion model [ 21]. In particular, Poole et al. [21] introduce
Score Distillation Sampling (SDS) [21] in which the neural
reconstruction model learns through the feedback error from
a diffusion model. The diffusion model is frozen without
any updates while the NeRF [ 19] weights are updated dur-
ing optimization. Even though this technique is capable of
reconstructing 3D scenes, per-scene optimization is still re-
quired, which usually takes up to 1 hour to converge on a
single GPU. This constraint restricts the practicality of this
approach, particularly when it comes to efficiently perform-
ing 3D reconstruction. To achieve fast 3D reconstruction,
a generalized prior that allows one feed-forward operation
through the networks is required instead of relying on an
expensive per-scene optimization.
An alternative method for rapid 3D reconstruction is to
utilize a diffusion model and synthesize multi-view images.
This can be achieved by leveraging a diffusion model that can
produce images based on slight variations in camera param-
eters [ 16]. Nevertheless, creating images using a multi-view
image generator ( e.g., Zero123 [ 16]) can be challenging in
terms of preserving geometry consistency. Rather than op-
timizing a network for each object as in [ 21], we aim to
preserve only one network to generalize for many objects.
To achieve this, we can exploit neural encoding volume built
from the projection of image features with known camera
parameters as in [ 2,17,36]. While these approaches show
promise, they still suffer from suboptimal results when em-
ployed for 3D reconstruction involving unseen objects.
In this work, we aim to address the aforementioned chal-
lenges, focusing on generalization, speed, and inconsistency
issues. To this end, we introduce a neural network to ad-
dress these concerns by employing an SDF network gen-
erated by HyperNetworks [ 9] and a V olume Transformer
(V olTran) to alleviate the impact of inconsistent examples.
Our approach explores the potential for generalization by
introducing a latent variable obtained from an image encoder
(e.g., CLIP [ 23]) to yield image representations. Subse-
quently, we employ these image representations to generate
the weights of the SDF, addressing the challenge of general-
ization. Please see Fig. 1 (bottom) for an illustration of our
technique. To summarize, our contributions include:
1.We propose a generalizable prior for 3D mesh reconstruc-
tion with a few synthesized data by assigning the weights
of SDFs based on the input image embedding.
2.We propose a transformer module for aggregation to en-
able working on inconsistent shapes and colors across
different viewpoints.
3.We also show that our method only requires one feed-
forward process and comfortably constructs a 3D mesh
with negligible additional processing time ∼5 seconds.2. Related Work
Diffusion models for 2D to 3D reconstruction. Recon-
structing a full 3D structure from only a few 2D images is
challenging due to the inherent ill-posedness of the prob-
lem. However, recent advances in generative models and, in
particular, diffusion models provide a promising direction
toward obtaining the priors about the 3D world that are nec-
essary to reconstruct the full 3D structure of an object from
a single image. For example, they are used as an indirect
way to provide feedback during the image-to-3D reconstruc-
tion process in [ 3,18,21,33,35]. A notable work so-called
DreamFusion [ 21] proposes text-to-3D generation by Score
Distillation Sampling (SDS), which allows optimization-
guided generation of NeRF-parametrized [ 19] 3D scenes. A
concurrent work using Score Jacobian Chaining [ 35] uses
a similar approach, exploiting the chain rule on the outputs
of a pretrained image generation model. Tang et al. [33] ex-
tend the idea with coarse and refining stages to enhance the
outputs with textured point clouds. Recently, Zero123 [ 16]
describes a diffusion model that takes an input image and
camera parameters to synthesize a novel view. This model
can generate more consistent multi-view images compared
to an off-the-shelf diffusion model like Imagen [ 26]. Albeit a
promising direction to reconstruct 3D models, per-scene op-
timization is still required and the neural implicit function is
limited to represent only one object. Thus, the generalization
of the trained model is limited for unseen objects.
Generalizable priors for fast 3D reconstruction. An
ideal implementation of 3D reconstruction is a single model
that can generalize to unseen objects, enabling 3D genera-
tion using a forward-pass approach only without applying
further per-scene optimization. PixelNeRF [ 44] as a pioneer
work in this direction proposes to extract feature volumes
from an input image which are then passed through a NeRF
model along with the camera extrinsic parameters. Chen
et al. [2] present an approach called MVSNeRF using cost
volumes built of warped 2D image features and then regress
volume density with a pass through an MLP ( i.e., neural
encoding volumes) as the base geometry. Then, the neural
encoding volume is used as an additional input to the NeRF
model. SparseNeus [ 17] extends MVSNeRF [ 2] to work
on a few-data regime by proposing cascaded geometry rea-
soning to refine the details of a 3D object. However, this
approach still requires multi-view inputs, with no obvious
mechanism to extend it to a single image. To tackle the
problem of 3D reconstruction from a single image, Liu et
al. [15] propose a method called One2345 to exploit a diffu-
sion model ( e.g., Zero123 [ 16]) to generate some example
images with estimated camera poses. To improve the preci-
sion of the reconstructed geometric models, One2345 [ 15]
employs SDFs [ 43] rather than NeRFs [ 19]. The challenge
of this approach is inconsistency in generated examples,
2

--- PAGE 3 ---
making it difficult to reconstruct 3D scenes that fully respect
the input appearance.
Another approach for avoiding per-scene optimization is
to train a large-scale model with self-supervised learning and
make use of large-scale labeled text-to-3D data. Point-e [ 20],
a system to generate 3D point clouds from text descrip-
tion, is a pioneer in this direction. Following up this work,
Shap-e [ 11] directly generates the weights of the neural im-
plicit model that can be rendered as meshes and radiance
fields. This method generates multiple synthetic images
then a neural 3D reconstruction technique ( e.g., SDF [ 43] or
NeRF [ 19]) is employed to produce 3D models. This model
cuts the cost of image-to-3D reconstruction from several
GPU hours to 1-2 minutes. While this method can produce
results quickly, the quality of the reconstructed 3D surfaces
remains subpar. Unlike all these prior works, our proposed
method can generate accurate 3D reconstruction with com-
petitive processing time ( i.e., less than 1 minute).
Context-based learning. In few-shot learning, the con-
cept of leveraging contextual information for achieving
optimal performance across diverse input conditions is a
well-established idea, as indicated by previous works like
[7,9,29–31,40]. Some of these methods involve model
parameter updates through gradient descents, exemplified
by several works [ 7,46]. However, these approaches still re-
quire multiple feed-forward operations to update the model.
Our focus lies in developing an approach that accomplishes
context understanding with just a single feed-forward opera-
tion, without the need for additional optimization steps. To
achieve this, we opt to adopt context-based information by
generating neural network weights. Specifically, we draw
inspiration from HyperNetworks [9] designated to generate
neural network weights based on the provided context.
3. Proposed Method
Our 3D neural reconstruction pipeline has two streams, as
shown in Fig. 2. Given a single-view image and its depth
map, we first synthesize multi-view images via a diffusion
model. Then, as shown in the upper stream of the figure, the
synthesized images are fed into a neural encoding volume
to obtain the 3D geometry representation of its structure.
The geometry representation is combined with the images to
predict a rendered RGB map by our proposed transformer
module, V olTran. Meanwhile, we also use the synthesized
multi-view images in a HyperNetwork to estimate an SDF
weight, shown in the bottom stream. The SDF network
predicts SDFs for surface representations that will later be
used for rendering the depth map and extracting the mesh.
Therefore, we name our approach Hyper-VolTran .
3.1. One to multiple-view images
We begin our pipeline by leveraging a pretrained generative
model. This enables us to expand a single input image intomultiple views from a broader set of object viewpoints, albeit
with some imperfections. For fair comparison, we strictly
follow the approach outlined in [ 16] to leverage elevation
and azimuth conditioning.
Synthesized views. Given a single RGB image and its
corresponding depth map denoted as I∈RH×W×3, and
D∈RH×W, respectively, we follow Zero123 [ 16] to nor-
malize its shape and use a spherical camera system for the
depth map. We apply an off-the-shelf image generation
model to create NRGB images and depth maps sampled
uniformly from several viewpoints according to ground-truth
camera parameters [15]. Concretely for training, we form a
set of RGB images and depth maps of an object as the source
setI={I1,···,IN}andD={D1,···,DN}. Note that
both RGB and depth images are used as training targets to
supervise the model in the training stage. However, those
depth maps are omitted in the testing phase.
3.2. Geometry-Aware Encoding
Geometry-aware encoding is essential in building a general-
ized method for surface prediction from multi-view images.
Our approach employs neural encoding volumes [ 2,41] to
construct 3D geometry based on the diversified input views
from Sec. 3.1 and their associated camera poses. To this end,
we warp 2D image features from the Ninput images onto a
localized plane situated within the reference view’s frustum.
Neural encoding volume. In deep multi-view stereo [ 41,
42], 3D geometry can be inferred in the form of Cost V ol-
ume construction. Let fθ:RH×W×3→RH×W×Cbe the
mapping from an input image to a feature map. Similar
to [17,41], we encode images using a Feature Pyramid Net-
work [ 14] as the mapping function to extract a neural feature
map, i.e.,Fi=fθ(Ii). Besides, we partition the scene’s
bounding volume into a grid of voxels. Then, along with
the intrinsic and extrinsic camera parameters P= [K,R,t]
for each image Ii, the neural feature map is projected based
on each vertex v, and the output is denoted as Fi(Πi(v)),
where Πi(v)projects v∈R3onto the local plane by apply-
ingP[41]. In particular, the homography warping is applied
for each view i, and the final neural encoding volume Gcan
be computed as Eq. 1.
G=ϕ/parenleftig
Var/parenleftbig
{Fi(Πi(v))}N
i=1/parenrightbig/parenrightig
. (1)
Here Var({Fi(Πi(v))}N−1
i=0)is the Cost V olume, Varmeans
the variance over Nviewpoints, and ϕdenotes a function
responsible for regularizing and propagating scene informa-
tion instantiated as a sparse 3D CNN ( i.e., Geometry Guided
Encoding). Since the variance accommodates differences in
the image appearance among multiple input perspectives, G
acquires the ability to encode complex 3D scene geometry
3

--- PAGE 4 ---
vvvInput Image
Noisy and InconsistentGenerate imagesCost Volume360oCamera Poses
Image Encoder
Encode imageSDF
VolTran – Transformer Module
Geometry Guided Encoding
HyperNetworksGenerated ViewsRendered RGB
Rendered Depth
Assign weightsSDF Network
ℒDepth
GT
Density
Aggregate from inconsistent inputs
Multi-view GeneratorEstimate
ℒRGB
Camera123
Generate weights4Figure 2. Our training pipeline starts from a single image. Expanding a single view to an image set using a viewpoint-aware
generation model, our method employs supervised learning with RGB and depth regression losses. Specifically, 1) Utilizing NRGB
images and depth maps, we generate additional viewpoints and camera poses. 2) Geometry-Guided Encoding is derived from warped image
features in the form of a Cost V olume. 3) Instead of test-time optimization, we obtain SDF weights with a single pass of a HyperNetwork
module, considering image appearance through visual encoding. 4) The geometry-encoded volume and the image features are passed to the
SDF network and a transformer module to reveal the complete 3D object structure. Hence, our method Hyper-V olTran encompasses quick
adaption to novel inputs thanks to our HyperNetwork design and consistent structures from global attention.
and appearance from diversified images. Thus, these volume
features contain appearance-aware information that can be
later used for volume rendering and SDF predictions.
3.3. Volume Rendering
A neural encoding volume previously computed is employed
to predict both the density and view-dependent radiance
at arbitrary locations within a scene. Next, this facilitates
the utilization of differentiable volume rendering to predict
the colors of images. For volume rendering, we opt to use
SDF [ 43] instead of NeRF [ 19] for a more accurate surface
reconstruction.
Signed Distance Function (SDF). SDFs represent 3D sur-
faces using a positional function that provides the nearest
distance to the surface. Given an arbitrary 3D location in our
setup, we use an MLP fΨ:Rd→Ras an SDF to represent
3D surfaces. Although the generic SDF input has d= 3as
the signed distance is associated with a point z∈R3, our
method uses a higher das the input consists of the concate-
nation of feature from neural encoding volumes, colors, and
image features. Another limitation of the generic SDF is
the lack of generalization ability. For example, when using
the neural encoding volume as an input, we can train an
SDF network on a large collection of 3D objects [ 2,17] to
avoid per-scene optimization. In testing, however, the SDF
network is usually frozen [ 15,17] and limited to the knownobjects. We propose a more adaptable approach to dynami-
cally assign MLP’s weights based on the generated outputs
of a HyperNetworks [ 9], which is conditioned on the input
image.
HyperNetworks for an SDF network. HyperNet-
works [ 9] constitute a neural model that generates the
weights for a target network designed to generalize on vari-
ous tasks given a context. Rather than preserving a neural
network fixed during test time, HyperNetwork offers a mech-
anism to assign weights based on a condition dynamically.
Mathematically, we design a HyperNetwork module δl(.)to
produce the weight for each layer ψlof the SDF network
fΨ:
ψl=δl(ξ(I1)). (2)
To encode the input image, we use a pretrained image en-
coder ξthat reduces the image dimensionality from RGB
space to a latent space. Unlike the past work [ 6] that needs to
optimize neural networks for every single object, our method
trains the module on the fly without requiring per-scene opti-
mization and directly calculating losses between two neural
network parameters. Since our condition is the feature repre-
sentation of the input object, our HyperNetwork can produce
a more dedicated and appropriate weight for its target net-
work. On the other hand, as we utilize the output of the
Hypernetwork [ 9] to assign weights to the SDF network, our
model generalizes better on the new object during inferences,
4

--- PAGE 5 ---
especially when the object shares similar semantics with the
training data. Moreover, the hypernetworks are directly up-
dated with a loss from RGB and depth map in our pipeline.
Thus, we do not have to store the individual optimal weight
parameter after per-scene optimization.
Rendering from SDFs. To estimate the parameters of the
neural SDF and color field, we adopt a volume rendering
method from NeuS [ 36] to render colors and volumes based
on the SDF representations. For a given pixel, we describe
Memitted rays from that pixel as {p(t) =o+tv|t≥0},
withobeing the camera’s focal point and rrepresenting
the ray’s unit direction. We feed the combined features
through an MLP and employ the softmax function to derive
the blending weights denoted as {ωi}N
i=1. The radiance at
a given point pand viewing direction vis calculated as the
weighted sum in Eq 3.
ˆc=N/summationdisplay
i=1ωi.ci, (3)
where ciis the color of source view i. Given the radiance,
our volume rendering strategies is expressed in Eq 4.
ˆC=M/summationdisplay
j=1Tjαjˆcj, (4)
αj= 1−exp[−/integraldisplaytj+1
tjρ(t)dt]. (5)
Here, Tj=/producttextj=1
k=1(1−αk)is a discrete accumulated trans-
mittance, αkis the discrete opacity, and ρ(t)denotes opaque
density. The rendered depth map can be derived as Eq. 6:
ˆD=M/summationdisplay
j=1Tjαjtj. (6)
Note the rendering process is fully differentiable; we train
the pipeline in a supervised manner so that the model can
predict the rendered colors ˆCand depths ˆDin inference.
VolTran : multi-view aggregation transformer. Pixel
data is inherently confined to a local context and lacks
broader contextual information, frequently leading to in-
consistent surface patches, particularly in the case of sparse
input data. One trivial solution is to aggregate features across
different views to capture the projected features from mul-
tiple views. Unfortunately, the synthesized views might
be corrupted due to the flaws in the generative model, a
simple aggregation [ 15,17,41] (e.g., average and max. pool-
ing) might fail to render shapes and colors accurately. We
propose a transformer module called VolTran based on the
self-attention design in [ 34] to encode global informationfrom different Nviewpoints. Besides the inputs, we learn an
aggregation token as an extra token to obtain a corresponding
output for a target view. Formally, let X∈RN+1×dbe a
matrix with rows composed of the tokens from source views
and the aggregation token by concatenating the feature from
colorci, image feature Fi(Π(v)), and volume feature G
yielding the dimension d. We denote fV(.), fQ(.), fK(.)as
functions to map values, queries, and keys of a transformer
module. Thus, the aggregation operation can be calculated
by the self-attention module, as shown in Eq. 7:
Attn(X) =Softmax (A)fV(X), (7)
where Ai,j=fQ(Xi)⊤fK(Xj)/γfor all i, j∈[N]. As
we apply multi-head attention, it can be formulated as
MHA (X) = [ Attn 1(X),···,Attn 3(X)]WH. We opt to
use LayerNorm to normalize the intermediate activations
and skip connection to stabilize training. The final output
from the transformer module, an MLP, is introduced as a
mapping function to obtain the blending weight ωi. After-
wards, the final color can be obtained as in the SDF rendering
pipeline.
3.4. Training and Inference
Our framework has several losses to train the model, includ-
ing the HyperNetwork module. Every module is optimized
in an end-to-end fashion only in the training stage. We define
our loss for rendered colors with mean squared error w.r.t.
the ground-truth Ci:
LRGB=1
|P||P|/summationdisplay
i=1/vextenddouble/vextenddoubleˆCi−Ci/vextenddouble/vextenddouble2
2. (8)
In addition to the color loss, we also calculate depth predic-
tions supervised with the following loss:
LDepth=1
|P1||P1|/summationdisplay
i=1/vextendsingle/vextendsingleˆDi−Di/vextendsingle/vextendsingle. (9)
Also, in order to regularize the SDF values derived from the
SDF network fΨ, we compute the Eikonal loss [8] :
LEikonal =1
|V|/summationdisplay
v∈V/parenleftbig
∥∇fΨ(v)∥2−1/parenrightbig2, (10)
where vis a sampled 3D point and ∇fθ(v)is the gradient
relative to the sample point q. This loss impacts the surface
smoothness.
Furthermore, to empower our framework for generating
concise geometric surfaces, we incorporate a sparsity regu-
larization term that penalizes uncontrollable surfaces called
a sparse loss [17], expressed as follows:
LSparse =1
|V|/summationdisplay
v∈Vexp/parenleftbig
−τ|s(v)|/parenrightbig
, (11)
5

--- PAGE 6 ---
A colorful llama
A rocking horse
A wooden bearA deer statueA teapotA blue ice creamA brown drawerA burgerA red birdAn office chairA green fishInputView 1View 2View 3View 4Figure 3. Qualitative results of Hyper-Voltran on text-to-3D colored meshes. The generated images from a diffusion model are used as
inputs. We only focus on the main object of the input image.
where s(v)is the predicted SDF and τis the hyperparameter
to scale the SDF prediction. To summarize, The total loss is
defined as LRGB+LDepth+β1LEikonal +β2LSparse .
Inference. During inference, there is no more optimiza-
tion, and only one feed-forward is performed, which reduces
the expensive computation to update the models during test-
ing. First, given an input image, we segment the input to
extract the foreground object. After we obtain the object
with clear background ( e.g., white color), we synthesize
multi-view scenes from the pretrained Zero123 model [ 16]
conditioned on the relative change of camera viewpoints.
These synthesized images are then employed to generate a
3D mesh by our proposed method. The inference of our pro-
posed method only contains feed-forward, thus comfortably
reducing the computational time compared to the existing
distillation methods [18, 21, 27].
4. Experiments
4.1. Implementation details
We train our models from publicly available data first shared
by [15], containing 46K synthesized 3D scenes. For the
base multi-view generative model, we follow Zero123 [ 16]
and keep its weights frozen. Additionally, for the geometry-
guided encoder, we set the volume encoding size to 96×
96×96for all of our experiments. For the SDF weight
generation, we employ the CLIP model [ 23] as the image
encoder, known for generating dependable representations.In terms of the loss function, we verified that the setting
proposed by [ 17] is optimal, i.e.,β1= 0.1andβ2= 0.02.
On the other hand, during inference, we first apply image
segmentation to get an accurate cutout of the target object
using the Segment Anything Model (SAM) [ 12]. Then, we
generate 8 key views which are further extended by 4 nearby
images each, for a total of 32 viewpoints.
4.2. Text-to-3D Results
The text-to-3D pipeline is performed by using off-the-shelf
text-to-image models e.g., [24,26,45]. We apply the corre-
sponding diffusion process conditioned on a given prompt
(e.g., ”a wooden bear”) and obtain an image depicting it. To
handle unexpected background information, we cut out the
target object from the generated image using SAM [ 12]. Dif-
ferent views are further synthesized alongside corresponding
camera poses using Zero123 [ 16]. The full set of generated
images are fed to our model, constructing neural encoding
volume, generating SDF network weights through a Hyper-
Network, and applying global attention, the main compo-
nents of Hyper-V olTran. Fig. 3 shows results of our method
across different views for a given text prompt. It can be ob-
served from these images that Hyper-V oltran produces good
quality meshes that adhere well to corresponding texture,
giving a sense of consistency across views.
4.3. Image-to-3D Results
We use a subset of the GSO dataset [ 5] to quantitatively
evaluate one-shot image-to-3D mesh, comprising 25 objects
6

--- PAGE 7 ---
Shap-e
Input
Zero123
+SD
One2345
Ours
Point-e
Figure 4. Qualitative comparison on single image to 3D reconstruction with previous works e.g., One2345 [ 15], Shap-e [ 11], Point-
e [20], and Zero123+SD [ 21]. V olTran offers more consistent and higher-quality results than competitors, generally providing a higher level
of preservation of input details. Please see our supplementary material for more results and zoomed-in details.
OursOne234InputOursOne234
InputMeshGenerated Views from Zero123Generated Views from Zero123MeshGenerated Views from Zero123Mesh
OursOne234Input
Figure 5. Examples of inconsistently generated views and comparison of our proposed method against One2345 [ 15] in generating meshes.
One2345 fails to build well-reconstructed meshes when the views are arguably inconsistent and challenging.
from different GSO categories. For evaluating rendering
quality, we use images from [18], spanning 15 objects.
Qualitative results. We offer qualitative demonstrations
of our approach and comparison to One2345 [ 15], Shap-
e [11], Point-e [ 20], and Zero123+SD [ 16] in Fig. 4, showcas-
ing Hyper-V oltran’s efficacy in addressing one-shot image-
to-3D object reconstruction. For a fair comparison with
One2345 [ 15], we employ the same set of synthesized im-
ages to generate the 3D meshes. We note that One2345 [ 15]
showcases inaccurate and unnatural shapes in Fig. 4. Also,
we compare to other feed-forward-only approaches [ 11,20].
Point-e and Shap-e cannot successfully reconstruct 3D
meshes from a single image yielding incorrect colors andshapes. Our proposed method is proven robust across a var-
ied set of different objects with higher fidelity and more
accurate shapes compared to the baselines. We also show
in Fig. 5 some inconsistencies in generated images from
Zero123 [ 16] and how our method can robustly construct the
meshes compared to the baseline.
Quantitative results. To evaluate our method and com-
pare against baselines in generating meshes, we use the
PyTorch3D [ 25] package to calculate Chamfer distance and
Iterated Closest Point for source and target alignment to
compute F-score. In terms of metrics, we follow prior
works [ 15], and [ 5], and use F-Score, Chamfer L2 distance,
and intersection-over-union (IoU). These metrics are sum-
7

--- PAGE 8 ---
Method F-Score ( ↑) Chamfer L2 ( ↓) IoU ( ↑) Time
Point-e [20] 16.45 1.73 0.09 78 secs
Shap-e [11] 10.10 1.98 0.11 27secs
Zero123+SD [16] 14.85 1.41 0.21 15 mins
One2345 [15] 12.00 1.90 0.13 45 secs
Hyper-VolTran (ours) 17.45 1.14 0.22 45 secs
Table 1. F-Score, Chamfer L2, IoU, and time comparison to base-
lines on the GSO dataset [5].
Method PSNR ( ↑) LPIPS ( ↓) CLIP Sim. ( ↑)
Point-e [20] 0.98 0.78 0.53
Shap-e [11] 1.23 0.74 0.59
Zero123 [16] 19.49 0.11 0.75
RealFusion [18] 0.67 0.14 0.67
Magic123 [22] 19.50 0.10 0.82
One2345 [15] 16.10 0.32 0.57
Hyper-VolTran (ours) 23.51 0.10 0.86
Table 2. PSNR, LPIPS, and CLIP similarity comparison to prior
works on the collected images in RealFusion [5].
marized in Table 1, where Hyper-V olTran proves its im-
proved generalization capabilities on unseen objects by scor-
ing higher than competitors across all tracks, at reasonable
computation time cost. Similarly, for rendering quality, our
method tops all previous works on 3D rendering across all
scores: PSNR, LPIPS, and the CLIP similarity score as
shown in Table 2.
Processing Time. Although our proposed method relies
on encoding the input image through an image embedding
model and generating weights of the SDF network, the full
3D generation latency is only around 5 seconds on a sin-
gle A100 GPU. This is on par with the processing time of
One2345 [ 15]. Additional latency is due to the base diffu-
sion model. In our case, we opt to use Zero123 [ 16] for the
synthesis of additional views, adding on average around 40
seconds per object. As shown in Table 1, the processing time
of Shap-e is lower, which results in generally lower quality
results than our method.
4.4. Analysis and Ablations
The SDF weight generator via a HyperNetwork and
VolTran. We investigate the efficacy of our proposed two
modules: the HyperNetwork for SDF and V olTran. This
ablation study is performed to analyze the impact of each
module. As shown in Fig. 6, we can observe that rendering
deteriorates without the HyperNetwork and V oltran. While
without V olTran, rendering scenes yields some noise as the
impact of inconsistent inputs. Using both, we can achieve
plausible rendering results.
Input
VolTranHyperNet SDF
HyperNet SDFVolTran
VolTranHyperNet SDF
Figure 6. Ablation study on each module. Impacts of each module
on rendering colored scenes.
N=32N=4N=24N=16
N=8
Figure 7. Qualitative results with different numbers of samples
generated from a diffusion model. The more images are generated
from the diffusion model, the better shape quality is achieved.
Number of samples. We evaluate the generated results by
varying numbers of support images obtained from the diffu-
sion model, ranging from 32 down to 4 images from different
perspectives. Fig. 7 showcases the impact of the number of
samples generated from the diffusion model. Our approach
gains advantages from an increased number of generated
images for forming geometry representations. Conversely,
an excessively low number of samples leads to degradation.
5. Conclusions
In this paper, we address the challenge of deriving a 3D ob-
ject structure from a single image. Our proposed approach,
called Hyper-V olTran, comprises a HyperNetwork module
and a transformer module. Specifically, HyperNetworks
generate SDF weights, while the transformer module fa-
cilitates robust global aggregation from inconsistent multi-
views. Our method demonstrates effective generalization to
unseen objects in the single image-to-3D task, as evidenced
by both quantitative and qualitative evaluations. Notably, our
approach rapidly generates 3D meshes, accomplishing this
task in just 45 seconds without per-scene optimization. Com-
pared with state-of-the-art methods, our proposed approach
excels in both time efficiency and reconstruction accuracy.
8

--- PAGE 9 ---
References
[1]Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neural
radiance fields, 2021. 1
[2]Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,
Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-
izable radiance field reconstruction from multi-view stereo.
arXiv preprint arXiv:2103.15595 , 2021. 1, 2, 3, 4
[3]Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-
tasia3d: Disentangling geometry and appearance for high-
quality text-to-3d content creation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , 2023. 2
[4]Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen,
Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo
Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-
guided attention for consistent text-to-video editing. arXiv
preprint arXiv:2310.05922 , 2023. 1
[5]Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-
man, Ryan Hickman, Krista Reymann, Thomas B. McHugh,
and Vincent Vanhoucke. Google scanned objects: A high-
quality dataset of 3d scanned household items, 2022. 6, 7,
8
[6]Ziya Erko c ¸, Fangchang Ma, Qi Shan, Matthias Nießner,
and Angela Dai. Hyperdiffusion: Generating implicit
neural fields with weight-space diffusion. arXiv preprint
arXiv:2303.17015 , 2023. 4
[7]Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-
agnostic meta-learning for fast adaptation of deep networks.
InProceedings of the 34th International Conference on Ma-
chine Learning , pages 1126–1135, 2017. 3
[8]Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learning
shapes. In Proceedings of Machine Learning and Systems
2020 , pages 3569–3579, 2020. 5
[9]David Ha, Andrew M. Dai, and Quoc V . Le. Hypernetworks.
CoRR , abs/1609.09106, 2016. 2, 3, 4
[10] Benran Hu, Junkai Huang, Yichen Liu, Yu-Wing Tai, and
Chi-Keung Tang. Nerf-rpn: A general framework for object
detection in nerfs. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2023. 1
[11] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional
3d implicit functions, 2023. 3, 7, 8
[12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and Ross
Girshick. Segment anything. arXiv:2304.02643 , 2023. 6
[13] Stanley Lewis, Jana Pavlasek, and Odest Chadwicke Jenkins.
NARF22: Neural articulated radiance fields for configuration-
aware rendering. In International Conference on Intelligent
Robots and Systems (IROS) . IEEE, 2022. 1
[14] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,
Bharath Hariharan, and Serge J. Belongie. Feature pyramid
networks for object detection. In CVPR , 2017. 3
[15] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen,
Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:Any single image to 3d mesh in 45 seconds without per-shape
optimization, 2023. 2, 3, 4, 5, 6, 7, 8
[16] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,
Sergey Zakharov, and Carl V ondrick. Zero-1-to-3: Zero-shot
one image to 3d object. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
9298–9309, 2023. 1, 2, 3, 6, 7, 8
[17] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and
Wenping Wang. Sparseneus: Fast generalizable neural surface
reconstruction from sparse views. ECCV , 2022. 1, 2, 3, 4, 5,
6
[18] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and
Andrea Vedaldi. Realfusion: 360 reconstruction of any object
from a single image. In Arxiv , 2023. 2, 6, 7, 8
[19] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra-
mamoorthi, and R Ng. Nerf: Representing scenes as neural
radiance fields for view synthesis. In European conference
on computer vision , 2020. 1, 2, 3, 4
[20] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751 , 2022. 3, 7, 8
[21] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.
Dreamfusion: Text-to-3d using 2d diffusion. arXiv , 2022. 2,
6, 7
[22] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,
Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-
rokhodov, Peter Wonka, Sergey Tulyakov, and Bernard
Ghanem. Magic123: One image to high-quality 3d object gen-
eration using both 2d and 3d diffusion priors. arXiv preprint
arXiv:2306.17843 , 2023. 8
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable vi-
sual models from natural language supervision. CoRR ,
abs/2103.00020, 2021. 2, 6
[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image genera-
tion with clip latents, 2022. 6
[25] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-
lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia
Gkioxari. Accelerating 3d deep learning with pytorch3d.
arXiv:2007.08501 , 2020. 7
[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay
Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,
Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans,
Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Pho-
torealistic text-to-image diffusion models with deep language
understanding. In Advances in Neural Information Processing
Systems , 2022. 1, 2, 6
[27] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon
Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,
and Seungryong Kim. Let 2d diffusion model know 3d-
consistency for robust text-to-3d generation. arXiv preprint
arXiv:2303.07937 , 2023. 6
[28] Anthony Simeonov, Yilun Du, Lin Yen-Chen, , Alberto Ro-
driguez, , Leslie P. Kaelbling, Tomas L. Perez, and Pulkit
9

--- PAGE 10 ---
Agrawal. Se(3)-equivariant relational rearrangement with
neural descriptor fields. In Conference on Robot Learning
(CoRL) . PMLR, 2022. 1
[29] Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash
Harandi. On modulating the gradient for meta-learning. In
ECCV , 2020. 3
[30] Christian Simon, Piotr Koniusz, Richard Nock, and Mehrtash
Harandi. Adaptive subspaces for few-shot learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , 2020.
[31] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. Meta-
learning for multi-label few-shot classification. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications of
Computer Vision (WACV) , pages 3951–3960, 2022. 3
[32] Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen,
Subhashini Venugopalan, and Zhangyang Wang. Is attention
all that neRF needs? In The Eleventh International Confer-
ence on Learning Representations , 2023. 1
[33] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,
Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d
creation from a single image with diffusion prior, 2023. 2
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-
eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. CoRR , abs/1706.03762,
2017. 5
[35] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and
Greg Shakhnarovich. Score jacobian chaining: Lifting pre-
trained 2d diffusion models for 3d generation. arXiv preprint
arXiv:2212.00774 , 2022. 2
[36] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
NeurIPS , 2021. 1, 2, 5
[37] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree,
Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, and
Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d
reconstruction of unknown objects. CVPR , 2023. 1
[38] Francis Williams, Zan Gojcic, Sameh Khamis, Denis Zorin,
Joan Bruna, Sanja Fidler, and Or Litany. Neural fields as learn-
able kernels for 3d reconstruction. CoRR , abs/2111.13674,
2021. 1
[39] Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, and Li
Zhang. S-neRF: Neural radiance fields for street views. In
The Eleventh International Conference on Learning Repre-
sentations , 2023. 1
[40] Mengmeng Xu, Yanghao Li, Cheng-Yang Fu, Bernard
Ghanem, Tao Xiang, and Juan-Manuel Perez-Rua. Where
is my wallet? modeling object proposal sets for egocentric
visual query localization. arXiv preprint arXiv:2211.10528 ,
2022. 3
[41] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.
Mvsnet: Depth inference for unstructured multi-view stereo.
European Conference on Computer Vision (ECCV) , 2018. 3,
5
[42] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,
Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A
large-scale dataset for generalized multi-view stereo networks.
Computer Vision and Pattern Recognition (CVPR) , 2020. 3[43] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. In Advances in
Neural Information Processing Systems , pages 4805–4815.
Curran Associates, Inc., 2021. 1, 2, 3, 4
[44] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance fields from one or few images. In
CVPR , 2021. 1, 2
[45] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,
Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian
Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell
Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron
Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard
James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi,
Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan.
Scaling autoregressive multi-modal models: Pretraining and
instruction tuning. arcXiv:2309.02591 , 2023. 1, 6
[46] Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hof-
mann, and Shimon Whiteson. Fast context adaptation via
meta-learning. 2019. 3
10

--- PAGE 11 ---
CVPR
#1001CVPR
#1001
CVPR 2024 Submission #1001. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
** Supplementary Material **
Hyper-VolTran: Fast and Generalizable
One-Shot Image to 3D Object Structure via HyperNetworks
Anonymous CVPR submission
Paper ID 1001
Image Encoder
InputsNNNNSDF NetworkPredicted SDFValueFC LayerFC LayerImage Embedding
Generate Network ParametersHyperNetworkFC LayerAssign weights
Single Input Image
Figure 1. The detail of our HyperNetwork architecture to assign
weights to the SDF network. The input is obtained from the image
embedding of an image encoder.
In the main manuscript, we have provided the pipeline of 001
our work and the In this supplementary material, we provide 002
the details of our method, settings, and additional results. 003
1. Details of Our Proposed Modules 004
In the main paper, we mentioned two modules construct- 005
ing our framework to improve both generalization and con- 006
sistency. We would detail out these two modules namely 007
HyperNetworks and V oltran in the following sections. 008
1.1. HyperNetworks 009
The HyperNetworks [ ?] are used in our pipeline to build 010
the SDF network. In each HyperNetwork module for each 011
SDF network layer, we build 3 fully-connected layers with 012
ReLU activations in the intermediate layers. In the first 013
layer, we map from the output dimension of the text em- 014
bedding which is 768 to 32 as the dimension of the hidden 015
layer. The input to the HyperNetwork is the image embed- 016
ding as an output of an image encoder ( e.g., CLIP [ ?]). 017
QKVMulti-head AttentionInput TokensGeometric FeatureImage Feature+RGBRendering RayLayerNormMLP
ConcatenateLayerNormMLPTransformer Layer
InputLinear, No BiasLinear, No BiasReLUOutputFigure 2. The detail of our V olume Transformer VolTran archi-
tecture as an aggregator from the features of multiple views.
Please see Fig. 1 for an illustration of our method. 018
1.2. VolTran: Multi-View Transformer 019
The transformer module in our framework is used as an ag- 020
gregator to reduce the noise impacts in multi-view synthe- 021
sized images. The transformer module is fed with input to- 022
kens from the feature of each view. Fig. 2 shows the detail 023
of each component in the transformer module in V olTran. 024
We set the multi-head number to 5 and 2 layers of the trans- 025
former with self-attention. 026
2. Ablation on the Loss Terms 027
We investigate the role of each loss function to produce 028
high-quality images. We observe that without sparseness 029
reguralization term as described in the main paper impact- 030
ing the capability in generating compact surfaces. As shown 031
in Fig. 5, some object surfaces have some defects and im- 032
1arXiv:2312.16218v2  [cs.CV]  5 Jan 2024

--- PAGE 12 ---
CVPR
#1001CVPR
#1001
CVPR 2024 Submission #1001. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
No Depth Loss
With Depth Loss
Figure 3. Comparison of including the depth loss and without the
depth loss in the training stage. Without the depth loss, the shape
becomes more rough and less detailed. The generated 3D scene
has some residuals outside the boundaries of the shape.
Without Eikonal Regularization Term
With Eikonal Regularization Term
Figure 4. Comparison of including the Eikonal regularization term
for smoothness in the training stage. We can observe that the upper
part has very rough surface (see the high contrast between dark
and bright regions of the surface) and less smooth compared to the
below one trained using the Eikonal regularization term.
Without Sparseness Term
With Sparseness Term
Figure 5. Comparison of including the sparseness regularization
term for compactness in the training stage. We observe that the
generated surface has some defects ( i.e., holes) compared to the
below one trained using the sparseness regularization term.
pact compactness of the surface because of the uncontrol- 033
lable free surfaces when we remove the sparseness regular- 034
ization term. For the Eikonal term, it is applied to regularize 035
the SDFs and the network to have the unit l2norm gradient 036
yielding smooth surfaces. We experimented to train the net- 037
work without the Eikonal term, as a result, Fig. 4 shows 038
that the objects have very rough surfaces due to unregular- 039ized SDFs. We also observe that the shape details of an 040
object might not be accurately predicted without the depth 041
loss. We also notice that the depth loss enriches the shape 042
of an object to add more details and remove some unwanted 043
residuals as shown in Fig. 3. 044
3. Implementation and Evaluation Details 045
3.1. Hyperparameters 046
In our experiments, we use 32 images as the optimal number 047
of generated images from the outputs of a generative model. 048
For training the model, we set the learning rate to 5e-4 and 049
adjust it using the Cosine learning schedule. The model 050
is trained for 300K iterations. We set the sparseness term, 051
Eikonal term, and depth term to 0.02, 0.1, 1.0, respectively. 052
The background ratio is set to 0.3. 053
3.2. Data Used for Evaluation 054
We opt to use two datasets for our evaluation. The first 055
dataset is obtained from 15 images proposed in RealFu- 056
sion [ ?], intended for evaluating rendering quality. For the 057
GSO dataset [ ?], we pick 25 images from different cate- 058
gories as follows: Alarm, Backpack, Bell, Blocks, Chicken, 059
Cream, Elephant, Grandfather, Grandmother, Leather, Lion, 060
Lunchbag, Mario, Oil, Schoolbus1, schoolbus2, Shoeblack, 061
Soap, Sofa, Sortingboard, Stacking cups, Teapot, Toaster, 062
Train, Turtle. 063
4. Additional Results 064
In this section, we provide some more detailed images and 065
more results in addition to the results provided in the main 066
paper. We provide a zoomed version of the generated results 067
in comparison to baselines in Fig. 6. We also show all of 068
our generated results on text-to-3d and image-to-3d tasks in 069
Fig. 7 Note that we also generate our results and comparison 070
in the videos. Please see our generated video format. 071
2

--- PAGE 13 ---
CVPR
#1001CVPR
#1001
CVPR 2024 Submission #1001. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.
Ours
Shap-e
Input
Point-e
Zero123+SD
One2345
Figure 6. Comparison of our proposed method Hyper-VolTran against baselines with zoomed patches.
A colorful llamaA teapotA rocking horseA blue ice creamA burgerText-to-3DA deerstatueA brown drawerAn office chairA red birdA green fish
A wooden bear
Input Image
Input ImageResultResultResult
Figure 7. The results of generated 3D objects using our proposed method on text-to-3d and image-to-3d tasks.
3
