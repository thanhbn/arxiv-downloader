# 2405.16287.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/hypernetwork/2405.16287.pdf
# Kích thước tệp: 582402 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LOGAH: Dự đoán Transformer 774 triệu tham số
sử dụng Graph HyperNetworks với 1/100
tham số
Xinyu Zhou
EPFLBoris Knyazev
Samsung - SAIT AI LabAlexia Jolicoeur-Martineau
Samsung - SAIT AI LabJie Fu∗
HKUST
Tóm tắt
Khởi tạo tốt cho các mô hình học sâu là điều quan trọng vì nó có thể giúp chúng
hội tụ tốt hơn và nhanh hơn. Tuy nhiên, việc tiền huấn luyện các mô hình lớn là
không thể chi trả được cho nhiều nhà nghiên cứu, điều này làm cho việc dự đoán
tham số ban đầu mong muốn trở nên cần thiết hơn ngày nay. Graph HyperNetworks
(GHNs), một phương pháp để dự đoán tham số mô hình, gần đây đã cho thấy hiệu
suất mạnh mẽ trong việc khởi tạo các mô hình thị giác lớn. Thật không may, việc
dự đoán tham số của các mạng rất rộng dựa vào việc sao chép các khối tham số
nhỏ nhiều lần và đòi hỏi một số lượng cực lớn tham số để hỗ trợ dự đoán đầy đủ,
điều này cản trở rất nhiều việc áp dụng trong thực tế. Để giải quyết hạn chế này,
chúng tôi đề xuất LOGAH (Low-rank GrAph Hypernetworks), một GHN với bộ
giải mã tham số rank thấp mở rộng đến các mạng rộng hơn đáng kể mà không
cần tăng quá mức tham số như trong các nỗ lực trước đây. LOGAH cho phép
chúng tôi dự đoán tham số của các mạng thần kinh lớn 774 triệu tham số một
cách hiệu quả về mặt bộ nhớ. Chúng tôi chỉ ra rằng các mô hình thị giác và ngôn
ngữ (tức là ViT và GPT-2) được khởi tạo với LOGAH đạt hiệu suất tốt hơn so
với những mô hình được khởi tạo ngẫu nhiên hoặc sử dụng các hypernetworks
hiện có. Hơn nữa, chúng tôi cho thấy kết quả học chuyển giao đầy hứa hẹn liên
quan đến việc huấn luyện LOGAH trên các tập dữ liệu nhỏ và sử dụng các tham
số dự đoán để khởi tạo cho các tác vụ lớn hơn. Chúng tôi cung cấp mã nguồn
tại https://github.com/Blackzxy/LoGAH .

1 Giới thiệu
Trong các lĩnh vực thị giác và ngôn ngữ, việc tiền huấn luyện một mô hình lớn từ đầu đi trước việc
giải quyết các tác vụ hạ nguồn [He et al., 2021, Devlin et al., 2019]. Các mô hình gần đây đã tăng
kích thước một cách đáng kể, theo đuổi hiệu suất tối ưu: từ khoảng 100M đến ≥65B tham số cho
Generative Pre-trained Transformers (GPTs) [Radford et al., 2018, Touvron et al., 2023, AI@Meta,
2024] và từ khoảng 100M đến ≥22B cho Vision Transformers (ViTs) [Dosovitskiy et al., 2021,
Dehghani et al., 2023]. Việc huấn luyện các mô hình lớn như vậy đòi hỏi tài nguyên tính toán
lớn. Ngoài ra, nhiều lần huấn luyện lại thường được yêu cầu trước khi mô hình được huấn luyện
thành công, điều này trở nên trầm trọng hơn trong các mô hình lớn hơn vì chúng thường không
ổn định hơn để huấn luyện và đòi hỏi nhiều điều chỉnh phần cứng và phần mềm ngoài việc điều
chỉnh siêu tham số và kiến trúc, quản lý dữ liệu, v.v. Do đó, việc tiền huấn luyện các mô hình
lớn đã trở nên rất đắt đỏ ngay cả đối với các công ty lớn [Thompson et al., 2022, Zhai et al., 2022].

Hiện tại, việc huấn luyện các tác vụ thị giác và ngôn ngữ thường được thực hiện bằng cách sử dụng
các kiến trúc mạng tương tự và các tập dữ liệu tương tự; các kiến trúc thường dựa trên Transformers
[Vaswani et al., 2023] (cho thị giác hoặc ngôn ngữ) hoặc Convolutional Neural Networks (CNNs)
[Fukushima et al., 1983] (cho thị giác),

∗Tác giả liên hệ.
Preprint. Đang xem xét.arXiv:2405.16287v1  [cs.LG]  25 May 2024

--- TRANG 2 ---
[Biểu đồ so sánh số lượng tham số giữa GHN-3 và LOGAH]

Hình 1: So sánh số lượng tham số giữa GHN-3 và LOGAH. GHN-3 yêu cầu kích thước ẩn lớn hơn
để hỗ trợ các mạng rộng hơn, điều này làm tăng kích thước của GHN-3 theo cấp số nhân trong Hình 1a.
LOGAH có thể hỗ trợ các mạng rộng hơn nhiều (lên đến 2048 chiều), và các mạng lớn hơn (GPT-2-
Large với 1280 chiều có 774M tham số) ngay cả khi sử dụng LOGAH-TINY.

trong khi các tập dữ liệu tương tự như ImageNet (cho thị giác) [Russakovsky et al., 2015] hoặc
The Pile (cho ngôn ngữ) [Gao et al., 2020]. Việc tận dụng kiến thức tiền nghiệm về kiến trúc và
tập dữ liệu này có thể giảm chi phí tiền huấn luyện. Một phương pháp để làm điều này là Graph
HyperNetworks (GHNs)[Zhang et al., 2018, Knyazev et al., 2021, 2023]; phương pháp này cho
phép dự đoán các tham số ban đầu của các mạng thần kinh này hoạt động tốt và hội tụ nhanh hơn.
Chúng tôi mô tả phương pháp GHN dưới đây.

Sử dụng một tập các kiến trúc mạng thần kinh {fG} làm dữ liệu huấn luyện, GHN HD, được tham
số hóa bởi θ, được huấn luyện để dự đoán các tham số của các mạng thần kinh này (wpred=HD(fG, θ))
để tối thiểu hóa hàm mất mát trên tập dữ liệu D. Wpred dự đoán có thể phục vụ như một khởi tạo
mạnh mẽ hơn so với các phương pháp khởi tạo dựa trên ngẫu nhiên, do đó giảm đáng kể chi phí
tiền huấn luyện.

Tuy nhiên, để dự đoán tham số cho các mạng rất rộng (thường có số lượng tham số lớn), các GHN
trước đây [Knyazev et al., 2021, 2023] phải sao chép các khối tham số nhỏ nhiều lần thay vì dự
đoán đầy đủ chúng do lượng tham số khổng lồ cần thiết để dự đoán tất cả tham số, do đó hạn chế
đáng kể hiệu suất của các mạng kết quả. Hơn nữa, để mở khóa khả năng dự đoán tham số có kích
thước lớn hơn, GHN cần kích thước ẩn d lớn hơn, dẫn đến sự gia tăng theo cấp số nhân về số
lượng tham số tăng như O(d3)(Hình 1a).

Để vượt qua hạn chế này, chúng tôi đề xuất LOGAH, một GHN với bộ giải mã tham số rank thấp.
Phương pháp mới này không chỉ hỗ trợ các mạng rộng hơn đáng kể mà còn làm điều này mà không
cần số lượng tham số quá mức tăng như O(d2) thay vì O(d3)(Hình 1b). Ví dụ, LOGAH-TINY
nhỏ nhất của chúng tôi chỉ có 2.5M tham số, nhưng có thể dự đoán tham số lên đến 2048 kênh,
bao gồm GPT-2-Large với 774M tham số và có thể ngay cả các mạng lớn hơn.

Trong công việc này, chúng tôi đóng góp như sau:
• Chúng tôi đề xuất LOGAH, với bộ giải mã rank thấp được cải tiến, có khả năng mở rộng
hơn và có thể dự đoán tham số của các mạng lớn mà không cần sao chép trong khi có ít
tham số có thể huấn luyện hơn và chi phí huấn luyện thấp hơn (Phần 3).
• Chúng tôi tạo ra một tập dữ liệu mới gồm các kiến trúc ViT và GPT-2 nhỏ, cho phép GHN
được huấn luyện trên Transformers cho cả lĩnh vực thị giác và ngôn ngữ (Phần 4). LOGAH
cho thấy khả năng tổng quát hóa xuất sắc trên các mô hình lớn hơn.
• Chúng tôi vượt trội hơn GHN-3 như một phương pháp khởi tạo trong nhiều tác vụ thị giác
và ngôn ngữ bằng cách dự đoán các tham số đa dạng và hiệu quả hơn (Phần 5).

--- TRANG 3 ---
2 Kiến thức cơ bản

2.1 Graph HyperNetworks
Graph HyperNetworks (GHNs) [Zhang et al., 2020, Knyazev et al., 2021] được sử dụng rộng rãi
để dự đoán tham số của mạng thần kinh. Đầu vào được cung cấp cho GHN HD(θ) là một đồ thị
tính toán fG của một mạng thần kinh f; GHN dự đoán các tham số của nó wpred=HD(fG;θ), trong
đó D là tập dữ liệu huấn luyện. Trong bài báo của chúng tôi, f có thể là một mô hình ViT
[Dosovitskiy et al., 2021] (tương ứng GPT-2 [Radford et al., 2019]), và D có thể là tác vụ phân
loại hình ảnh (tương ứng tác vụ mô hình hóa ngôn ngữ nhân quả).

Trong công việc của Knyazev et al. [2021], GHN HD được huấn luyện bằng SGD trên M kiến trúc
huấn luyện {fG_a}M_a=1 và N mẫu dữ liệu huấn luyện {xj, yj}N_j=1 trên bài toán tối ưu hóa sau:

arg min_θ 1/(N×M) ∑(j=1 to N) ∑(a=1 to M) L(fa(xj;HD(fG_a;θ)), yj). (1)

Một meta-batch gồm m kiến trúc huấn luyện được lấy mẫu trong giai đoạn huấn luyện, trong đó
HD dự đoán tham số. Trong khi đó, một mini-batch gồm n mẫu huấn luyện x được lấy mẫu và
đưa vào m kiến trúc được dự đoán tham số để có được m×n dự đoán. Mất mát cross-entropy L
được tính toán cho các tác vụ phân loại và mô hình hóa ngôn ngữ (dự đoán token tiếp theo). Sau
đó, mất mát được lan truyền ngược để cập nhật các tham số θ của HD bằng gradient descent.
Trong công việc của chúng tôi, chúng tôi đã tạo ra các tập dữ liệu VITS-1K và GPTS-1K gồm
các kiến trúc huấn luyện nhỏ để dự đoán tham số cho các mô hình ViT và GPT-2 lớn hơn, tương
ứng. Chúng tôi mô tả chi tiết trong Phần 4.

Đồ thị tính toán fG= (V, E) cho đầu vào là một Đồ thị Có Hướng Không Chu Trình (DAG), trong
đó V biểu thị các phép toán (ví dụ: pooling, self-attention, v.v.), và E tương ứng với luồng chuyển
tiếp đầu vào qua f. Các đặc trưng nút d-chiều H(1)∈R|V|×d được thu được bằng một lớp embedding
(nút thứ i: h(1)_i=Embed(h(0)_i), trong đó h(0)_i là một vector one-hot đại diện cho một phép toán)
và được đưa vào làm đầu vào cho GHN. Sau L lớp Graphormer [Ying et al., 2021], các đặc trưng
nút H(L)∈R|V|×d được đưa vào bộ giải mã được mô tả dưới đây.

2.2 Bộ giải mã GHN
Knyazev et al. [2021, 2023] có bộ giải mã dựa trên một MLP đơn giản dự đoán một tensor có
hình dạng d×d×16×16, trong đó d tương đối nhỏ (d= 384 ngay cả trong GHN-3 lớn nhất). Bộ
giải mã lấy các đặc trưng nút đầu ra của lớp Graphormer cuối cùng để dự đoán tham số wpred.
Tensor này được sao chép khi trọng số mục tiêu có d lớn hơn hoặc được cắt khi mục tiêu nhỏ hơn.
Số lượng tham số của bộ giải mã trong [Knyazev et al., 2021, 2023]² là:

#Param GHN-decoder = 4d²×16×16 + 32d² + 8d³ + d×num_class ∈ O(d³). (2)

3 Graph HyperNetworks có thể mở rộng: LOGAH

Mô hình LOGAH cải tiến các khía cạnh sau: (1) thiết kế một bộ giải mã rank thấp mới không chỉ
với ít tham số hơn, mà còn tránh việc lặp lại tham số không hiệu quả trong dự đoán, (2) hỗ trợ
dự đoán các mô hình lớn hơn (thường rộng hơn) mà không cần số lượng tham số cực lớn như
trong các công việc trước đây, ví dụ LOGAH-TINY chỉ với 2.5M tham số có thể hỗ trợ GPT-2-
Large, trong khi các phương pháp hiện có [Knyazev et al., 2023] sẽ cần ít nhất ~10⁵M tham số.

3.1 Bộ giải mã Rank thấp
Trong [Knyazev et al., 2023], chiều đầu ra cuối cùng của bộ giải mã là d×d×16×16, trong đó d
có thể là 64 hoặc 128. Trong hầu hết các trường hợp, 16×16 có thể là lãng phí vì các tham số
tích chập thường là 3×3 hoặc 7×7. Tuy nhiên, vấn đề lớn hơn là đối với các mạng lớn, tensor
cần được lặp lại để điền vào tất cả các kênh vì d nhỏ.

²Vui lòng tham khảo Phụ lục A và https://github.com/SamsungSAILMontreal/ghn3/blob/main/ghn3/nn.py để biết thêm chi tiết

--- TRANG 4 ---
Xem xét một trọng số tích chập W với kích thước: (Cout×Cin×h×w), chúng ta có thể định hình
lại nó thành một ma trận W có kích thước (Cout·h)×(Cin·w) trong đó h, w nhỏ hơn nhiều so với
Cout và Cin. Lấy cảm hứng từ [Hu et al., 2021], bây giờ chúng ta có thể giới thiệu phân rã rank
thấp:

W=AB∈R(Cout·h)×(Cin·w), (3)

trong đó A∈R(Cout·h)×r, B∈Rr×(Cin·w), r biểu thị rank thấp. Bằng cách này, chúng ta giảm
lượng tham số từ Cout·Cin·h·w xuống r·((Cout·h) + (Cin·w)).

Do đó, toàn bộ quá trình như sau: sau các MLP đầu tiên (multilayer perceptron), đầu vào
H(L)∈R|V|×d được biến đổi thành ˜W∈R|V|×2K×r:

˜W=MLP(H(L))∈R|V|×2K×r, (4)

trong đó K:= max(Cout·h, Cin·w) được gọi là max mask, để chúng ta có thể tránh các phép toán
lặp lại trong GHN-3. Sau đó chúng ta chia ˜W thành hai ma trận A, BT∈R|V|×K×r và chỉ lấy
những bit cần thiết để xây dựng W=AB trong Phương trình (3). Kiến trúc của các MLP được
hiển thị trong Phụ lục B, bao gồm việc biến đổi rank thấp bên trong. Bằng cách này, số lượng
tham số trong bộ giải mã của LOGAH là:

#Param LoGAH-decoder = 4d² + 32d² + 8d×2r² + r×K. (5)

Về mặt lý thuyết, chúng ta có thể cố định r như một siêu tham số hằng số nhỏ hơn nhiều so với
d, thì Phương trình (5) sẽ là O(d²), ít hơn so với độ phức tạp của bộ giải mã GHN ban đầu O(d³).
Trong thực tế, xem xét một rank nhỏ r sẽ cản trở hiệu suất của mô hình, vì vậy chúng tôi đặt
nó là r≈d/2 khi d tăng. Dưới cài đặt này, chúng tôi so sánh số lượng tham số của hai bộ giải
mã một cách chi tiết như sau.

So sánh #Tham số. Không mất tính tổng quát, chúng ta giả sử K=Cout·h, và trong các cài đặt
sau của chúng tôi cho rank thấp r (chi tiết trong Bảng 1)³: r≈d/2. Từ Phương trình (2) - Phương
trình (5) chúng ta có:

ΔP= 4d²×(16²-1) + 8d×(d²-2r²) + d×num_class - r×Cout·h. (6)

Vì r≈d/2, 16²-1≈16², và trong các thí nghiệm của chúng tôi, chúng tôi đặt K= max(Cout·h,
Cin·w) = 2048·16, chúng ta có thể chỉ so sánh số hạng đầu tiên và cuối cùng trong Phương
trình (6):

Δ₁= 4d²×(16²-1) - r×Cout·h (7)
≈ 4d²×16² - d×1024·16 (8)
= 16d·(64d-1024). (9)

Do đó, Δ₁>0 vì trong các cài đặt của chúng tôi d= 64,128,256, v.v., có nghĩa là bộ giải mã
của LOGAH yêu cầu ít tham số hơn (ΔP>0), ngay cả khi chúng ta để r tăng theo d.

3.2 Dự đoán tham số trong các hình dạng lớn hơn với ít tham số hơn
Nhờ cơ chế rank thấp, LOGAH có thể hỗ trợ dự đoán các tensor tham số với hình dạng lớn hơn
nhưng với ít tham số hơn. So sánh tham số giữa các phiên bản khác nhau của GHN-3 và LOGAH
được hiển thị trong Hình 1. Vì GHN-3 chỉ có thể hỗ trợ các tham số dự đoán có cùng chiều
rộng với kích thước ẩn d, chúng tôi khớp đường cong của GHN-3 và có được số lượng tham số
tiềm năng cần thiết để dự đoán đầy đủ các tham số với hình dạng lớn hơn. So với GHN-3, LOGAH
của chúng tôi có thể hỗ trợ các hình dạng tensor rộng hơn với ít tham số hơn nhiều, có thể hỗ
trợ các mô hình lớn hơn (thường rộng hơn) trong thực tế (tham khảo Bảng 6 và Bảng 7).

4 Tập dữ liệu VITS-1K và GPTS-1K

Để lấy mẫu các kiến trúc huấn luyện trong các công việc liên quan đến GHN trước đây, Knyazev
et al. [2021] đã xây dựng DeepNets-1M, một tập dữ liệu gồm 1 triệu đồ thị tính toán đa dạng.
Tuy nhiên, để tạo ra các mô hình Transformer như ViT và GPT-2, DeepNets-1M không tối ưu.
Do đó chúng tôi giới thiệu

³Mặc dù trong cài đặt LOGAH-LARGE: d=r= 256, Phương trình (9) sẽ có 16d·(64d-2048) >0 vì d
rất lớn. Chúng tôi cũng đã thử d= 384, r= 256, tuy nhiên, việc huấn luyện không ổn định.

--- TRANG 5 ---
Bảng 1: Chi tiết về các biến thể LOGAH và GHN-3. Tất cả các biến thể LOGAH được đặt với
K= 2048·16. Chúng tôi ước tính thời gian huấn luyện của mỗi mô hình dựa trên meta-batch
m= 1 và tập dữ liệu CIFAR-100 trong 300 epoch.

[Bảng hiển thị các thông số kỹ thuật cho các mô hình LOGAH và GHN-3]

VITS-1K và GPTS-1K: các tập dữ liệu mới này chứa 1K đồ thị tính toán kiểu ViT và kiểu GPT-2
khác nhau tương ứng, đặc biệt để huấn luyện GHN dự đoán tham số của ViT và GPT-2.

VITS-1K. Chúng tôi tạo ra các mô hình ViT đa dạng bằng cách thay đổi số lượng lớp L, đầu H
và kích thước ẩn D. Vì các mô hình ViT có các phiên bản quy mô khác nhau (như được minh
họa trong Bảng 6 của Phụ lục C), chúng tôi cũng cần đảm bảo rằng các kiến trúc huấn luyện
của chúng tôi sẽ đủ đa dạng và được phân phối đều về số lượng tham số. Do đó, khi tạo ra các
kiến trúc này, đối với các mạng sâu hơn (với nhiều lớp hơn) chúng tôi kiểm soát chúng hẹp hơn
(với kích thước ẩn nhỏ hơn) và ngược lại. Hình 8a hiển thị phân phối số lượng tham số trong
VITS-1K, gần như được phân phối đều và số lượng tham số tối đa của các kiến trúc này được
giới hạn ở 10M (chỉ khoảng một nửa số tham số của ViT-Small). Chi tiết về việc tạo ra tập dữ
liệu VITS-1K có thể được tìm thấy trong Phụ lục E.

GPTS-1K. Chúng tôi làm theo ý tưởng tương tự ở trên để có được các mô hình GPT-2 khác nhau,
bằng cách thay đổi số lượng lớp L, đầu H và kích thước ẩn D, để xây dựng GPTS-1K. Phân
phối số lượng tham số được hiển thị trong Hình 8b, và số lượng tham số tối đa nằm trong 30M,
ít hơn nhiều so với GPT2-Small với 110M tham số. Chi tiết về các biến thể GPT-2 được trình
bày trong Bảng 7 trong Phụ lục C và những chi tiết về việc tạo ra tập dữ liệu GPTS-1K có thể
được tìm thấy trong Phụ lục F.

Quan trọng là, các tập dữ liệu này nhỏ hơn so với các Mô hình Ngôn ngữ Lớn (LLM) và các
mô hình thị giác lớn tương tự; điều này là có chủ ý. Mục đích là để giảm tính toán cần thiết để
học dự đoán tham số trong khi cung cấp một phạm vi quy mô liên tục (từ nhỏ đến lớn) để LOGAH
có thể tổng quát hóa cho các mô hình lớn sau khi huấn luyện.

5 Thí nghiệm

Chúng tôi đánh giá xem các mạng (tức là ViT và GPT-2) được khởi tạo với các tham số wpred
được dự đoán bởi LOGAH có thể hoạt động tốt hơn so với những mạng được khởi tạo bởi GHN-3
và khởi tạo ngẫu nhiên sau khi tinh chỉnh hay không.

Các biến thể LOGAH. Chúng tôi cung cấp bốn quy mô khác nhau của LOGAH từ TINY đến
LARGE, bằng cách tăng dần số lượng lớp L, kích thước ẩn d, đầu H, cũng như rank thấp r.
Chúng tôi cũng so sánh số lượng tham số và ước tính sự khác biệt về thời gian huấn luyện giữa
LOGAH với GHN-3, được hiển thị trong Bảng 1. Chúng tôi nhấn mạnh rằng GHN-3 và LOGAH
chỉ được huấn luyện một lần trên mỗi tập dữ liệu, do đó cùng một mô hình có thể dự đoán tham
số cho nhiều kiến trúc làm cho chi phí huấn luyện của GHN-3 và LOGAH được phân bổ.

Cài đặt huấn luyện GHN. Các mô hình GHN, bao gồm GHN-3 và LOGAH của chúng tôi, được
huấn luyện trong 300 epoch trên các tập dữ liệu VITS-1K và GPTS-1K. Đối với ViT, chúng tôi
tiến hành thí nghiệm trên các tập dữ liệu sau: CIFAR-10, CIFAR-100 [Krizhevsky et al., 2009]
(với kích thước batch b= 64) và ILSVRC-2012 ImageNet [Russakovsky et al., 2015] (với kích
thước batch b= 128). Chúng tôi huấn luyện các mô hình sử dụng precision hỗn hợp tự động trong
PyTorch với lịch trình tỷ lệ học cosine annealing bắt đầu từ lr= 3e-4, weight decay λ= 1e-2,
và regularization tham số dự đoán γ= 3e-5[Knyazev et al., 2023]. Đối với các thí nghiệm GPT-2,
chúng tôi sử dụng tập dữ liệu WikiText [Merity et al., 2016], và sử dụng

--- TRANG 6 ---
Bảng 2: Độ chính xác top-1 trên CIFAR-10, CIFAR-100 và ImageNet (%) trên ViT-Small và
ViT-Base trong các cài đặt khởi tạo khác nhau. Các mô hình ViT trên tập dữ liệu CIFAR được
tinh chỉnh trong 100 epoch trong mỗi cài đặt khởi tạo, trong khi đối với tập dữ liệu ImageNet,
ViT được tinh chỉnh trong 30 epoch do cân nhắc về chi phí thời gian. Tất cả các mô hình GHN
có thể được huấn luyện trên một GPU NVIDIA 4090 duy nhất khi meta-batch m= 1 với hậu tố
/m1. Trên ImageNet, chúng tôi chỉ huấn luyện GHN-3-Tiny.

[Bảng hiển thị kết quả so sánh hiệu suất]

lr= 1e-4 với kích thước batch b= 6, trong khi giữ các siêu tham số khác như trước. Tất cả các
mô hình GHN, bao gồm GHN-3 và LOGAH, được huấn luyện riêng biệt trên mỗi tập dữ liệu
tác vụ.

5.1 Thí nghiệm ViT.

5.1.1 So sánh tổng thể trên CIFAR-10, CIFAR-100 và ImageNet
Chúng tôi thử nghiệm ViT-small và ViT-base trên CIFAR-10, CIFAR-100 [Krizhevsky et al.,
2009] và ILSVRC-2012 ImageNet [Russakovsky et al., 2015] được hiển thị trong Bảng 2 với
các phương pháp khởi tạo khác nhau: (1) khởi tạo ngẫu nhiên (RAND INIT) được triển khai
mặc định trong PyTorch, (2) khởi tạo trực giao (ORTHINIT) [Saxe et al., 2014], (3) tham số
được dự đoán bởi GHN-3, và (4) tham số được dự đoán bởi LOGAH. Từ Bảng 2, chúng tôi
quan sát thấy LOGAH nhìn chung vượt trội hơn RANDINIT, ORTHINIT và GHN-3.

[Biểu đồ hiển thị hiệu suất theo kích thước meta-batch]

Hình 2: Độ chính xác top-1 trên CIFAR-10 (%) trên ViT-Small và ViT-Base khi LOGAH được
huấn luyện với kích thước meta-batch m khác nhau.

Các mô hình ViT được tinh chỉnh trong 100 epoch trên CIFAR, và 30 epoch trên tập dữ liệu
ImageNet. Đối với ViT-Small, chúng tôi đặt phạm vi tỷ lệ học là {0.1,0.2,0.3,0.4,0.5}. Đối
với ViT-Base và ViT-Large, chúng tôi đặt nó là {0.03,0.04,0.05,0.06,0.1}. Tất cả các mô hình
ViT được tinh chỉnh sử dụng SGD với cùng cài đặt như Knyazev et al. [2023], nhưng với weight
decay 1e-2. Chúng tôi báo cáo độ chính xác validation tốt nhất trong tất cả các tỷ lệ học.

CIFAR-10. LOGAH-SMALL thể hiện hiệu suất tốt nhất (tức là cải thiện 2.16% so với RANDINIT,
và cải thiện 1.4% so với GHN-3-L trong ViT-small) chỉ với 21.41M tham số, so với GHN-3-L
với 214.7M tham số, lớn hơn 10 lần so với LOGAH-SMALL, điều này chứng minh hiệu quả
của bộ giải mã rank thấp của chúng tôi. Đối với các mô hình ViT-base, GHN-3-L có hiệu suất
gần nhất với phiên bản nhỏ nhất của các mô hình của chúng tôi: LOGAH-TINY.

CIFAR-100. LOGAH-LARGE vượt trội hơn các mô hình khác với sự cải thiện lớn, đặc biệt so
với RAND INIT (3.46% trên ViT-Small và 6.47% trên ViT-Base), trong khi hiệu suất của GHN-3
thậm chí còn tệ hơn so với khởi tạo ngẫu nhiên trên ViT-Small.

--- TRANG 7 ---
ImageNet. Xem xét chi phí thời gian để huấn luyện GHN trên ImageNet, đối với GHN-3, chúng
tôi chỉ huấn luyện GHN-3-T/m1 trong tập dữ liệu này để so sánh. Các phương pháp khởi tạo
ngẫu nhiên, bao gồm RANDINIT và ORTHINIT, cho thấy hiệu suất tương đương với LOGAH.
Trong tập dữ liệu này, chúng tôi không quan sát thấy sự cải thiện lớn như trong CIFAR-10 và
CIFAR-100. Tuy nhiên, LOGAH-T hoạt động tốt hơn nhiều so với GHN-3-T (62.16 so với 38.79
cho ViT-Small).

5.1.2 Ảnh hưởng của kích thước meta-batch m đối với LOGAH
Trong phần này, chúng tôi nghiên cứu ảnh hưởng của kích thước meta-batch m của LOGAH-TINY
và LOGAH-SMALL đối với ViT-Small, ViT-Base và ViT-Large (hiển thị trong Hình 2 cho CIFAR-10
và Bảng 3 cho CIFAR-100). Chúng tôi tăng giá trị m từ 1 đến 4,8 và huấn luyện LOGAH tương ứng.

CIFAR-10. Trong Hình 2, chúng ta có thể nhận thấy rằng việc tăng kích thước meta-batch m có
thể cải thiện đáng kể hiệu suất của ViT-Small (tức là từ 82.87 đến 87.04 bằng LOGAH-TINY,
và từ 86.09 đến 87.35 bằng LOGAH-SMALL khi đặt m= 4), thậm chí còn tốt hơn so với các
mô hình LOGAH lớn hơn được huấn luyện với m= 1. Tuy nhiên, một mô hình ngược lại được
quan sát thấy trong ViT-Base, việc tăng m lên 4 sẽ làm tệ đi hiệu suất trên CIFAR-10.

Bảng 3: Độ chính xác top-1 trên CIFAR-100 (%) trên ViT-Base và ViT-Large khi m= 4,8 để
huấn luyện LOGAH.

[Bảng hiển thị kết quả với các kích thước meta-batch khác nhau]

CIFAR-100. Đối với tập dữ liệu CIFAR-100, chúng tôi mở rộng đến mô hình lớn hơn: ViT-Large.
Việc tăng kích thước meta-batch một cách hợp lý có thể cải thiện đáng kể hiệu suất của LOGAH
nhỏ và giúp nó đạt được kết quả tương tự như phiên bản lớn hơn (ví dụ LOGAH-T/M4 so với
LOGAH-S/M1). Một phát hiện thú vị khác là khi đặt kích thước meta-batch m= 8, hiệu suất
của cả hai LOGAH đều giảm mạnh. Một lý do tiềm năng có thể nằm ở kích thước mô hình nhỏ
hơn trong VITS-1K, và meta-batch lớn hơn có thể gây ra vấn đề overfitting.

5.2 Thí nghiệm GPT-2
Xem xét chi phí thời gian và tài nguyên khác⁴, chúng tôi chỉ áp dụng hai LOGAH nhỏ nhất,
LOGAH-T/M2 và LOGAH-S/M2, trong các thí nghiệm GPT-2. Phần này điều tra tác vụ Mô hình
hóa Ngôn ngữ Nhân quả (CLM). Chúng tôi thử nghiệm hiệu suất của mô hình trên tập dữ liệu
WikiText[Merity et al., 2016]. Cụ thể, chúng tôi chọn GPT-2-Small và GPT-2-Medium cho tập
dữ liệu wikitext-2-raw-v1. Đối với tập dữ liệu wikitext-103-raw-v1, chúng tôi chọn GPT-2-
Medium và GPT-2-Large. Tất cả các mô hình được huấn luyện với các tham số được khởi tạo
ngẫu nhiên (tức là RAND INIT), được triển khai mặc định trong HuggingFace [Wolf et al., 2019].

Cài đặt huấn luyện GPT-2. Tất cả các mô hình GPT-2 được tinh chỉnh trong 100 epoch trên
mỗi tập dữ liệu. Chúng tôi sử dụng DeepSpeed [Rajbhandari et al., 2020] để cải thiện hiệu quả
huấn luyện với GPT-2-Medium và GPT-2-Large trên WikiText-103. Với 6 ×NVIDIA 4090 GPU,
chúng tôi huấn luyện các mô hình bằng AdamW với tỷ lệ học là 3e-6, weight decay là 1e-2,
kích thước batch là 4 cho GPT-2-Medium và là 2 cho GPT-2-Large.

Kết quả. Bảng 4 hiển thị kết quả. Hiệu suất được cải thiện đáng kể hơn trên các mô hình GPT-2
lớn hơn và các tập dữ liệu lớn hơn (ví dụ huấn luyện GPT-2-Large trên tập dữ liệu WikiText-103),
điều này chứng minh rằng ngay cả mô hình nhỏ nhất của chúng tôi (2.5M) có thể dự đoán tham
số tốt hơn nhiều so với khởi tạo ngẫu nhiên cho một mô hình lớn (774M).

5.3 Phân tích định tính
Chúng tôi phân tích tính đa dạng của các tham số dự đoán theo các thí nghiệm trong Knyazev
et al. [2023, 2021]. Cụ thể, chúng tôi dự đoán tham số cho ViT và GPT-2, và thu thập một hoặc
hai hình dạng tensor thường xuất hiện trong mỗi mô hình. Sau đó chúng tôi tính toán khoảng
cách cosine tuyệt đối giữa tất cả các cặp tensor tham số

⁴Ví dụ, chúng tôi không huấn luyện trên tập dữ liệu OpenWebText [Gokaslan et al., 2019], vì nó quá lớn.

--- TRANG 8 ---
Bảng 4: Điểm perplexity của các thí nghiệm GPT-2.

[Bảng hiển thị kết quả perplexity]

Bảng 5: Tính đa dạng của các tham số được dự đoán bởi GHN-3, LOGAH so với Pretrained
(∗: hoặc được huấn luyện bởi SGD từ RAND INIT nếu các tham số pretrained không có sẵn)
được đo trên ViT và GPT-2. Pretrain*: ViT-Small được huấn luyện bởi SGD từ RANDINIT trong
100 epoch trên CIFAR-100; đối với ViT-Base và ViT-Large chúng tôi sử dụng các tham số được
pretrained trên ImageNet; đối với GPT-2-Medium, chúng tôi cũng tải các tham số pretrained
có sẵn trong HuggingFace [Wolf et al., 2019].

[Bảng hiển thị kết quả đa dạng tham số]

cùng hình dạng và tính trung bình (Bảng 5). LOGAH dự đoán các tham số đa dạng hơn so với
GHN-3 nói chung trên các mô hình ViT, đặc biệt trong ViT-Small trên CIFAR-100, điều này cũng
phù hợp với hiệu suất tốt hơn trong Bảng 2. Một phát hiện thú vị khác là LOGAH-TINY tốt hơn
trong việc dự đoán các tham số vuông đa dạng hơn (ví dụ (768,768) và (1024,1024)) so với
LOGAH-SMALL.

5.4 Thí nghiệm học chuyển giao

[Biểu đồ hiển thị kết quả học chuyển giao GPT-2]

Hình 3: Thí nghiệm học chuyển giao GPT-2. LOGAH được huấn luyện trên WikiText-2 và các
mô hình GPT-2 được tinh chỉnh trên WikiText-103 dựa trên các tham số dự đoán của LOGAH.

Trong phần này, chúng tôi khám phá cài đặt khi LOGAH được huấn luyện trên một tập dữ liệu,
nhưng được sử dụng để tạo ra khởi tạo tham số cho một tập dữ liệu khác (có thể khó hơn). Đối
với các thí nghiệm ViT, chúng tôi tiến hành các thí nghiệm học chuyển giao từ CIFAR-10 đến
CIFAR-100, và từ CIFAR-100 đến ImageNet. Đối với các thí nghiệm GPT-2, chúng tôi xem xét
các thí nghiệm từ WikiText-2 đến WikiText-103.

Thí nghiệm ViT. Cụ thể, chúng tôi khởi tạo lại lớp phân loại [Knyazev et al., 2023] sử dụng
phân phối chuẩn Kaiming [He et al., 2015] với 100 và 1,000 đầu ra, để chuyển giao đến CIFAR-100
và ImageNet tương ứng, sau đó chúng tôi tinh chỉnh toàn bộ mạng. Kết quả được hiển thị trong
Hình 4. Trên CIFAR-100, các ViT được khởi tạo bởi LOGAH-S, LOGAH-B vượt trội hơn khởi
tạo ngẫu nhiên. Đó vẫn là một tác vụ khó khăn đối với LOGAH để hoạt động trên tập dữ liệu
ImageNet, và LOGAH-T nhìn chung tốt hơn so với các phiên bản lớn hơn, điều này cho thấy
rằng các tham số hoạt động tốt trên CIFAR-100 có thể không tối ưu cho ImageNet. Tuy nhiên,
so với GHN-3-T trong Bảng 2, nó cho thấy hiệu suất vượt trội trong cả ViT-Small và ViT-Base.

--- TRANG 9 ---
[Biểu đồ hiển thị kết quả học chuyển giao ViT]

Hình 4: Thí nghiệm học chuyển giao ViT. Chúng tôi sử dụng LOGAH được huấn luyện trên
CIFAR-10 (tương ứng CIFAR-100) để dự đoán tham số của ViT, sau đó ViT được huấn luyện
trên CIFAR-100 (tương ứng ImageNet). T, S, B và L biểu thị các phiên bản TINY, SMALL,
BASE và LARGE của LOGAH tương ứng.

Thí nghiệm GPT-2. Chúng tôi giữ cùng cài đặt như trong Phần 5.2, và tinh chỉnh GPT-2-Medium
và GPT-2-Large trên WikiText-103 được tải với các tham số dự đoán LOGAH, được huấn luyện
trên tập dữ liệu WikiText-2. Theo Hình 3, các tham số dự đoán của LOGAH cũng là một khởi
tạo tốt để tinh chỉnh các mô hình GPT-2 trên WikiText-103, đặc biệt đối với GPT-2-Large.

Theo các kết quả trên, các tham số được dự đoán bởi LOGAH thể hiện khả năng học chuyển
giao mong muốn từ một tác vụ dễ hơn sang một tác vụ khó hơn khác. Những cải thiện này đáng
kể hơn nếu phân phối dữ liệu gần nhau (ví dụ từ CIFAR-10 đến CIFAR-100). Hơn nữa, đặc
tính này có thể giúp giảm thời gian huấn luyện cho LOGAH để dự đoán các tham số tốt, trong
đó chúng ta không cần huấn luyện LOGAH trên một tập dữ liệu quy mô lớn.

6 Công việc liên quan

Tiền huấn luyện mô hình lớn. Các mô hình pretrained quy mô lớn đầu tiên xuất hiện trong lĩnh
vực NLP [Yin et al., 2022, Guo et al., 2022]. Sự cải thiện và thành công chủ yếu được quy cho
học tự giám sát và Transformer [Vaswani et al., 2023]. Ngày càng nhiều mô hình ngôn ngữ lớn
được phát triển dựa trên nó, mở rộng đến kích thước lớn hơn để có hiệu suất tốt hơn dưới việc
tiền huấn luyện với dữ liệu khổng lồ [Devlin et al., 2019, Brown et al., 2020, Touvron et al.,
2023]. Lấy cảm hứng từ sự tiến bộ của Transformer, nhiều mô hình thị giác dựa trên Transformer
cũng được đề xuất, và một số phương pháp tiền huấn luyện đã được khám phá [Dosovitskiy et al.,
2021, Carion et al., 2020, He et al., 2021, Chen et al., 2020]. Công việc của chúng tôi tập trung
vào dự đoán tham số cho hai mô hình dựa trên Transformer (ViT và GPT-2) để giảm chi phí
tiền huấn luyện.

Dự đoán tham số. Hypernetworks [Ha et al., 2016] thường được tận dụng để dự đoán tham số
của mô hình. Nhiều công trình nghiên cứu đã mở rộng khả năng của hypernetwork để tổng quát
hóa trên các kiến trúc chưa thấy [Zhang et al., 2018, Nirkin et al., 2021, Knyazev et al., 2021],
tập dữ liệu [Requeima et al., 2020, Lin et al., 2021, Zhmoginov et al., 2022, Kirsch et al., 2024],
hoặc để tạo ra các mạng có thể giải thích [Liao et al., 2023]. Bài báo của chúng tôi cũng dựa
trên Graph HyperNetworks (GHNs), nhưng vượt qua việc tăng quá mức tham số cần thiết trong
các GHN trước đây. LOGAH có thể hỗ trợ các mô hình lớn hơn chỉ với 1% tham số, cho thấy
khả năng dự đoán tham số tốt hơn cho các mạng lớn hơn.

Khởi tạo và học để phát triển mô hình. Một số phương pháp đã cải thiện khởi tạo ngẫu nhiên
bằng cách học từ dữ liệu [Dauphin and Schoenholz, 2019, Yang et al., 2022]. Tuy nhiên, GHN-3
[Knyazev et al., 2023] cho thấy hiệu suất tốt hơn làm cho nó trở thành một phương pháp ưa thích
để xây dựng. Các phương pháp khác học để khởi tạo một mô hình lớn hơn từ một mô hình pretrained
nhỏ hơn [Evci et al., 2022, Wang et al., 2023]. Những phương pháp này giảm thời gian huấn
luyện, tuy nhiên, một mô hình pretrained nhỏ hơn có cùng kiến trúc chính xác như mô hình đích
không phải lúc nào cũng có sẵn, điều này hạn chế phương pháp.

7 Hạn chế

Mặc dù mô hình LOGAH của chúng tôi cho thấy hiệu suất xuất sắc so với GHN-3 và các phương
pháp khởi tạo ngẫu nhiên khác trong các thí nghiệm rộng rãi, vẫn còn những hạn chế. Quan trọng
nhất, do cân nhắc về chi phí thời gian và tài nguyên, chúng tôi chỉ tiến hành các thí nghiệm GPT-2
trên tập dữ liệu WikiText và chỉ với hai mô hình nhỏ nhất của chúng tôi. Hơn nữa, để dự đoán
tham số cho các kiến trúc hoàn toàn mới (ví dụ [Gu and Dao, 2023]), GHN có thể cần được
huấn luyện để tránh sự thay đổi phân phối lớn. Trong công việc tương lai, sẽ thú vị khi cho thấy
khả năng của LOGAH trên các LLM hiện đại [Touvron et al., 2023].

--- TRANG 10 ---
8 Kết luận

Trong công việc này, chúng tôi đề xuất LOGAH, một Graph HyperNetwork (GHN) rank thấp
giải quyết hai vấn đề của GHN-3 trước đây. Thứ nhất, bộ giải mã rank thấp tránh việc sao chép
các khối tham số nhỏ nhiều lần khi dự đoán một tham số hình dạng lớn. Thứ hai, trái ngược với
GHN-3, nó không yêu cầu một số lượng tham số có thể huấn luyện đặc biệt để hỗ trợ các mô hình
rộng hơn hoặc lớn hơn, và LOGAH nhỏ nhất của chúng tôi chỉ khoảng 2.5M. Chúng tôi tiến hành
các thí nghiệm rộng rãi trên hai mô hình dựa trên transformer đại diện (ViT trong thị giác và
GPT-2 trong ngôn ngữ) để cho thấy hiệu quả vượt trội so với GHN-3 và các phương pháp khởi
tạo ngẫu nhiên. Hơn nữa, khả năng tổng quát hóa của LOGAH từ một tập dữ liệu đơn giản sang
một tập dữ liệu khó hơn khác cũng được xác minh trong các thí nghiệm học chuyển giao.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo tiếng Anh được giữ nguyên theo bản gốc]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
A Chi tiết về số lượng tham số của các bộ giải mã trong GHN-3
Lượng tham số lý thuyết của các bộ giải mã trong GHN-3 được hiển thị dưới đây:

4×in_feature ×d×h×w+MLP_d1×MLP_d2+MLP_d2×d²+d×num_class (10)

trong đó in_feature là chiều của đặc trưng đầu vào của bộ giải mã (được đặt là d trong GHN-3),
và MLP_d1, MLP_d2 biểu thị chiều của lớp thứ 1 và thứ 2 của MLP (được đặt là 4d và 8d
trong các thí nghiệm tương ứng), h, w là hai chiều cuối cùng của hình dạng tensor dự đoán
(được đặt là 16) và num_class là số lớp của tập dữ liệu. Do đó, chúng ta có thể đơn giản hóa
Phương trình (10) thành (2).

B Chi tiết về MLP trong bộ giải mã của LOGAH
MLP có 4 lớp và hàm kích hoạt σ(·) là ReLU Fukushima [1975]:

[Các phương trình toán học]

trong đó Mi, i∈ {1,2,3,4} là các ma trận có thể học:

[Các ma trận M1-M4]

Chúng tôi cũng cung cấp triển khai mã của nó như được hiển thị trong Hình 5.

C Chi tiết về các biến thể của ViT và GPT-2
Chúng tôi cung cấp chi tiết về ViT và GPT-2 trong các kích thước khác nhau. L, D, H, P biểu
thị số lượng lớp, đầu, kích thước ẩn và tham số tương ứng.

[Bảng 6: Chi tiết về các biến thể ViT]
[Bảng 7: Chi tiết về các biến thể GPT-2]

D Phân phối của tập dữ liệu VITS-1K và GPTS-1K
Phân phối của các tập dữ liệu VITS-1K và GPTS-1K được hiển thị trong Hình 8.

E Chi tiết về việc tạo ra tập dữ liệu VITS-1K
Như đã đề cập ở trên, chúng tôi thay đổi các giá trị trong lớp L, đầu H, và kích thước ẩn D
của ViT, cũng như giới hạn kích thước của các mô hình này. Chi tiết được hiển thị trong Hình 6.

F Chi tiết về việc tạo ra tập dữ liệu GPTS-1K
Chúng tôi cũng thay đổi các giá trị trong lớp L, đầu H và kích thước ẩn D của GPT-2, và chi
tiết được hiển thị trong Hình 7.

--- TRANG 14 ---
[Mã nguồn của ConvDecoder3LoRA]

Hình 5: Mã cho bộ giải mã Rank thấp trong LOGAH.

--- TRANG 15 ---
[Mã nguồn tạo mô hình ViT]

Hình 6: Mã để tạo ra các mô hình kiểu ViT được sử dụng cho tập dữ liệu VITS-1K.

--- TRANG 16 ---
[Mã nguồn tạo mô hình GPT-2]

Hình 7: Mã để tạo ra các mô hình kiểu GPT-2 được sử dụng cho tập dữ liệu GPTS-1K.

[Biểu đồ phân phối tham số]

Hình 8: Phân phối tham số của các tập dữ liệu VITS-1K và GPTS-1K
