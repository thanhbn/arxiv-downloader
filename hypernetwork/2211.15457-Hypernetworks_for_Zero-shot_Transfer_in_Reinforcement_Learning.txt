# 2211.15457.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypernetwork/2211.15457.pdf
# File size: 8252883 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Hypernetworks for Zero-shot Transfer in Reinforcement Learning
Sahand Rezaei-Shoshtari1,2,3, Charlotte Morissette1,3, Francois R. Hogan3
Gregory Dudek1,2,3, David Meger1,2,3
1McGill University2Mila - Qu ´ebec AI Institute3Samsung AI Center Montreal
srezaei@cim.mcgill.ca
Abstract
In this paper, hypernetworks are trained to generate behav-
iors across a range of unseen task conditions, via a novel TD-
based training objective and data from a set of near-optimal
RL solutions for training tasks. This work relates to meta RL,
contextual RL, and transfer learning, with a particular focus
on zero-shot performance at test time, enabled by knowledge
of the task parameters (also known as context). Our techni-
cal approach is based upon viewing each RL algorithm as a
mapping from the MDP speciﬁcs to the near-optimal value
function and policy and seek to approximate it with a hyper-
network that can generate near-optimal value functions and
policies, given the parameters of the MDP. We show that, un-
der certain conditions, this mapping can be considered as a
supervised learning problem. We empirically evaluate the ef-
fectiveness of our method for zero-shot transfer to new re-
ward and transition dynamics on a series of continuous con-
trol tasks from DeepMind Control Suite. Our method demon-
strates signiﬁcant improvements over baselines from multi-
task and meta RL approaches.
1 Introduction
Adult humans possess an astonishing ability to adapt their
behavior to new situations. Well beyond simple tuning, we
can adopt entirely novel ways of moving our bodies, for ex-
ample walking on crutches with little to no training after
an injury. The learning process that generalizes across all
past experience and modes of behavior to rapidly output the
needed behavior policy for a new situation is a hallmark of
our intelligence.
This paper proposes a strong zero-shot behavior gener-
alization approach based on hypernetworks (Ha, Dai, and
Le 2016), a recently proposed architecture allowing a deep
hyper-learner to output all parameters of a target neural net-
work, as depicted in Figure 1. In our case, we train on the full
solutions of numerous RL problems in a family of MDPs,
where either reward or dynamics (often both) can change
between task instances. The trained policies, value functions
and rolled-out optimal behavior of each source task is the
training information from which we can learn to generalize.
Our hypernetworks output the parameters of a fully-formed
and highly performing policy without any experience in a
Copyright © 2023, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Hypernetwork
Near-optimal SolutionFigure 1: Our method uses hypernetworks to approximate
an RL algorithm as a mapping from a family of parameter-
ized MDPs to a family of near-optimal solutions, in order
to achieve zero-shot transfer to new reward and dynamics
settings.
related but unseen task, simply by conditioning on provided
task parameters.
The differences between the tasks we consider leads to
large and complicated changes in the optimal policy and
induced optimal trajectory distribution. Learning to pre-
dict new policies from this data requires powerful learners
guided by helpful loss functions. We show that the abstrac-
tion and modularity properties afforded by hypernetworks
allow them to approximate RL generated solutions by map-
ping a parameterized MDP family to a set of optimal solu-
tions. We show that this framework enables achieving strong
zero-shot transfer to new reward and dynamics settings by
exploiting commonalities in the MDP structure.
We perform experimental validation using several fami-
lies of continuous control environments where we have pa-
rameterized the physical dynamics, the task reward, or both
to evaluate learners. We carry out contextual zero-shot eval-
uation, where the learner is provided the parameters of the
test task, but is not given any training time – rather the
very ﬁrst policy execution at test time is used to measure
performance. Our method outperforms selected well-known
baselines, in many cases recovering nearly full performance
without a single timestep of training data on the target tasks.
Ablations show that hypernetworks are a critical element in
achieving strong generalization and that a structured TD-like
loss is additionally helpful in training these networks.
Our main contributions are:
1. The use of hypernetworks as a scalable and practical ap-
proach for approximating RL algorithms as a mapping
from a family of parameterized MDPs to a family of near-
optimal policies.arXiv:2211.15457v2  [cs.LG]  2 Jan 2023

--- PAGE 2 ---
2. A TD-based loss for regularization of the generated poli-
cies and value functions to be consistent with respect to
the Bellman equation.
3. A series of modular and customizable continuous con-
trol environments for transfer learning across different
reward and dynamics parameters.
Our learning code, generated datasets, and custom con-
tinuous control environments, which are built upon
DeepMind Control Suite, are publicly available at:
https://sites.google.com/view/hyperzero-rl
2 Background
2.1 Markov Decision Processes
We consider the standard MDP that is deﬁned by a 5-tuple
M= (S;A;T;R; ), whereSis the state space,Ais the
action space,T:SA! Dist(S)is the transition dynam-
ics,R:SA! Ris the reward function and 2(0;1]
is the discount factor. The goal of an RL algorithm is to ﬁnd
a policy:S! Dist(A)that maximizes the expected re-
turn deﬁned as E[Rt] =E[PT
k=0krt+k+1].Value func-
tionV(s)denotes the expected return from sunder policy
, and similarly action-value function Q(s;a)denotes the
expected return from safter taking action aunder policy :
Q(s;a) =Es0;rp(js;a);a0(js0)h1X
k=0krt+k+1s;ai
Value functions are ﬁxed points of the Bellman equation
(Bellman 1966), or equivalently the Bellman operator B:
B[Q(s;a)] =Es0;rp(js;a);a0(js0)h
r+Q(s0;a0)i
Similarly, the optimal value functions V(s)andQ(s;a)
are the ﬁxed points of the Bellman optimality operator B.
2.2 General Value Functions
General value functions (GVF) extend the standard def-
inition of value functions Q(s;a)to entail the reward
function, transition dynamics and discount factor in addi-
tion to the policy, that is Q;R;T;(s;a)(Sutton and Barto
2018). Universal value function approximators (UVFA)
(Schaul et al. 2015) are an instance of GVFs in which the
value function is generalized across goals gand is repre-
sented asQ(s;a;g ). Naturally, this notion is used in goal-
conditioned RL (Andrychowicz et al. 2017) and multi-task
RL (Teh et al. 2017). Relatedly, general policy improvement
(GPI) aims to improve a generalized policy based on transi-
tions of several MDPs (Barreto et al. 2020; Harb et al. 2020;
Faccio et al. 2022b). The goal of our method in learning a
generalized mapping from MDP speciﬁcs to near-optimal
policies and value functions is closely related to the over-
all goal of GVFs and GPI. However, unlike such methods
we do not seek to improve a given generalized policy.
2.3 Hypernetworks
Ahypernetwork (Ha, Dai, and Le 2016) is a neural network
that generates the weights of another network, often referredto as the main network. While both networks have associated
weights, only the hypernetwork weights involve learnable
parameters that are updated during training. During infer-
ence, only the main network is used by mapping an input to
a desired target, using the weights generated by the hyper-
network. Since the weights of different layers of the main
network are generated through a shared learned embedding,
hypernetworks can be viewed as a relaxed form of weight
sharing across layers. It has been empirically shown that this
approach allows for a level of abstraction and modularity of
the learning problem (Galanti and Wolf 2020; Ha, Dai, and
Le 2016) which in turn results in a more efﬁcient learning.
Notably, hypernetworks can be conditioned on the context
vector for conditional generation of the weights of the main
network (von Oswald et al. 2019). Similarly to von Oswald
et al. (2019), we condition the hypernetwork on the param-
eters (context) of the MDP to generate the near-optimal pol-
icy and value function based on the reward and dynamics
parameters.
3 HyperZero
The overarching goal of this work is to develop a framework
that allows for approximating RL solutions by learning the
mapping between the MDP speciﬁcs and the near-optimal
policy. A reasonable approximation can potentially allow for
zero-shot transfer and predicting the general behaviour of an
RL agent prior to its training. Beyond the standard premises
of zero-shot transfer learning (Taylor and Stone 2009; Tan
et al. 2018), a well-approximated mapping of an MDP to
near-optimal policies can have applications in reward shap-
ing, task visualization, and environment design.
3.1 Problem Formulation
This section outlines the assumptions and problem formu-
lation used in this paper. First, we deﬁne the parameterized
MDP family Mas:
Deﬁnition 1 (Parameterized MDP Family) .Aparameter-
ized MDP family Mis a set of MDPs that share the same
state spaceS, action spaceA, a parameterized transition dy-
namicsT, a parameterized reward function R , and a dis-
count factor :
M=fMijMi= (S;A;Ti;R i;)g;
where ip( )andip()are parameters ofMi, and
are assumed to be sampled from prior distributions.
Notably, the state space Sand action spaceAin our def-
inition can be either discrete or continuous (e.g., an open
sub-space of Rn)spaces. Our deﬁnition of a parameterized
MDP family is related to contextual MDPs (Hallak, Di Cas-
tro, and Mannor 2015; Jiang et al. 2017), where the learner
has access to the context.
The key to our approximation is to assume that an RL al-
gorithm, once converged, is a mapping from an MDP Mi2
Mto a near-optimal policy and the near-optimal action-
value function corresponding to the speciﬁc MDP Mion
which it was trained. With a slight abuse of notation, we
denote the near-optimal policy as 
iand the near-optimal

--- PAGE 3 ---
action-value function as Q
i:
MiRL Algorithm       ! 
i(ajs);Q
i(s;a): (1)
Notably, this view has precedent in prior works on learning
to shape rewards (Sorg, Lewis, and Singh 2010; Zheng, Oh,
and Singh 2018; Zheng et al. 2020), meta-gradients in RL
(Xu, van Hasselt, and Silver 2018; Xu et al. 2020), and the
operator view of RL algorithms (Tang, Feng, and Liu 2022).
Since our goal is to learn the mapping of Equation (1)
from a family of parameterized MDPs that share the simi-
lar functional form of parameterized transition dynamics T
and reward function R , we assume that the MDP Mican
be fully characterized by its parameters  iandiand the
functional forms of R andT; that isMiM ( i;i).
Equation (1) can then be simpliﬁed as:
M( i;i)RL Algorithm       !(ajs; i;i);Q(s;aj i;i);
(2)
where the near-optimal policies and action-value functions
are now functions of the reward parameters  iand dynam-
ics parameters i, in addition to their standard inputs. No-
tably, this formulation is closely related to prior works on
goal-conditioned RL (Andrychowicz et al. 2017; Schroecker
and Isbell 2020), and universal value function approximators
(UVFA) (Schaul et al. 2015; Borsa et al. 2018).
Consequently, our problem is formally deﬁned as ap-
proximating the mapping shown in Equation (2) to ob-
tain the approximated near-optimal action-value function
^Q(s;aj ;)and policy ^(ajs; ; )that are parameter-
ized byand, respectively. Once such mapping is ob-
tained, one can predict and observe near-optimal trajectories
by rolling out the approximated policy ^without necessar-
ily training the RL solver from scratch:
^(ajs; ; )Policy Rollout in the Environment                 ! ^( ;); (3)
where ^( ;)is the near-optimal trajectory corresponding
to the reward parameters  and dynamics parameters .
3.2 Generating Optimal Policies and Optimal
Value Functions with Hypernetworks
Our goal is to approximate the mapping described in Equa-
tion (2). To that end, we assume having access to a family of
near-optimal policies 
ithat were trained independently on
instances of Mi2M. A dataset of near-optimal trajectories
is then collected by rolling out each 
ion its corresponding
MDPMi. Thus, samples are drawn from the stationary state
distribution of the near-optimal policy d(s).
Consequently, the inputs to the learner are tuples of states,
reward parameters and dynamics parameters hs; i;iiand
the targets are tuples of near-optimal actions and action-
valuesha;qi. We can frame the approximation problem
of Equation (2) as a supervised learning problem under the
following conditions:
Assumption 1. The parameters of the reward function R 
and transition dynamics Tare sampled independently and
identically from distributions over the parameters  ip( )
andip(), respectively.
HypernetworkFigure 2: Diagram of our learning framework for universal
approximation of RL solutions. Given reward parameters  i
and dynamics parameters i, the hypernetwork Hgener-
ates weights of the approximated near-optimal policy ^and
value function ^Q. The only learnable parameters are .
Assumption 2. The RL algorithm that is to be approxi-
mated, as shown in Equations (1) and (2), is converged to
the near-optimal value function and policy.
Assumption 1 is a common assumption on the task distri-
bution in meta-learning methods (Finn, Abbeel, and Levine
2017). While Assumption 2 appears strong, it is related to
the common assumption made in imitation learning where
the learner has access to expert demonstrations (Ross, Gor-
don, and Bagnell 2011; Ho and Ermon 2016). Nevertheless,
we empirically show that Assumption 2 can be relaxed to
an extent in practice, while still achieving strong zero-shot
performance, as shown in Section 4.
Importantly, we assume no prior knowledge on the struc-
ture of the RL algorithm nor on the nature (stochastic or
deterministic) of the policy that were used to generate the
data. Notably, since the optimal policy of a given MDP is
deterministic (Puterman 2014), we can parameterize the ap-
proximated near-optimal policy ^as a deterministic func-
tion^:S!A , without any loss of optimality.
We propose to use hypernetworks (Ha, Dai, and Le 2016)
for solving this approximation problem. Conditioned on the
parameters i;iof an MDPMi2M, as shown in Figure
Algorithm 1: HyperZero
Inputs : Parameterized reward function R and transition
dynamicsT, distribution p( )andp()over parameters,
hypernetwork H, main networks ^and^Q.
Hyperparameters: RL algorithm, learning rate of hyper-
networkH, number of tasks N.
1:Initialize datasetDof near-optimal trajectories
2:fori= 1toNdo
3: Sample MDPMi2M: ip( );ip(i)
4: Obtain
iandQ
iofMi2Mwith an RL solver
5: Store near-optimal trajectories 
i:D D[f
ig
6:end for
7:while not done do
8: Sample mini-batch h i;i;s;a;s0;r;qiD
9: Generate ^iand^Qi:[i;i] =H( i;i)
10:  argminLpred.() +LTD().Eqn. (4-5)
11:end while

--- PAGE 4 ---
2, the hypernetwork Hgenerates weights of the approxi-
mated near-optimal policy ^and action-value function ^Q.
Following the literature on hypernetworks, we refer to the
generated policy and value networks as main networks.
Consequently, the hypernetwork is trained via minimizing
the error for predicting the near-optimal action and values by
forward passing the main networks:
Lpred.() = E( i;i;s;a;q)Dh
(^Qi(s;a) q)2i
+E( i;i;s;a)Dh
(^i(s) a)2i
(4)
where [i;i] =H( i;i)andDis the dataset of near-
optimal trajectories collected from the family of MDPs M.
Notably, this training paradigm effectively decouples the
problem of learning optimal values/actions from the prob-
lem of learning the mapping of MDP parameters to the space
of optimal value functions and policies. Thus, as observed
in other works on hypernetworks (Ha, Dai, and Le 2016;
Galanti and Wolf 2020; von Oswald et al. 2019; Faccio et al.
2022a), this level of modularity results in a simpliﬁed and
more efﬁcient learning.
3.3 Temporal Difference Regularization
A key challenge in using supervised learning approaches for
function approximation in deep RL is the temporal corre-
lation existing within the samples, which results in the vi-
olation of the i.i.d. assumption. Common practices in deep
RL for stabilizing the learning is to use a target network to
estimate the temporal difference (TD) error (Lillicrap et al.
2015; Mnih et al. 2013). In this paper, we propose a novel
regularization technique based on the TD loss to stabilize the
training of the hypernetwork for zero-shot transfer learning.
As stated in Assumption 2, we assume having access to
near-optimal RL solutions that were generated from a con-
verged RL algorithm. As a result, our framework differs
from the works on imitation learning (Ross, Gordon, and
Bagnell 2011; Bagnell 2015; Ho and Ermon 2016) since
samples satisfy the Optimal Bellman equation of the under-
lying MDPMi2Mand, more importantly, we have access
to the near-optimal action-values qfor a given transition
samplehs;a;s0;ri.
Therefore, we propose to use the TD loss to regularize the
approximated critic ^Qby moving the predicted target valuetowards the current value estimate, which is obtainable from
the ground-truth RL algorithm:
LTD() = E( i;i;s;a;s0;r;q)Dh
(r+^Qi(s0;a0) q)2i
(5)
wherea0is obtained from the approximated deterministic
policy ^(s0)with stopped gradients. Note that our appli-
cation of the TD loss differs from that of standard function
approximation in deep RL (Mnih et al. 2013; Lillicrap et al.
2015); instead of moving the current value estimate towards
the target estimates, our TD loss moves the target estimates
towards the current estimates. While this relies on Assump-
tion 2, we show that in practice applying the TD loss is ben-
eﬁcial as it enforces the approximated policy and critic to be
consistent with respect to the Bellman equation. Algorithm
1 shows the pseudo-code of our learning framework.
4 Evaluation
We evaluate our proposed method, referred to as HyperZero
(hyper networks for zero-shot transfer) on a series of chal-
lenging continuous control tasks from DeepMind Control
Suite. The primary goal in our experiments is to study the
zero-shot transfer ability of the approximated RL solutions
to novel dynamics and rewards settings.
4.1 Experimental Setup
Environments. We use three challenging environments
for evaluation: cheetah, walker, and ﬁnger. For an easier vi-
sualization and realization of reward parameters, in all cases
the reward parameters correspond to the desired speed of
the motion which consists of both negative (moving back-
ward) and positive (moving forward) values. Depending on
the environment, dynamics changes correspond to changes
in a body size and its weight/inertia. Full details of the envi-
ronments and their parameters are in Appendix A.
RL Training and Dataset Collection. We use TD3 (Fuji-
moto, Hoof, and Meger 2018) as the RL algorithm that is to
be approximated. Each MDP Mi2M, generated by sam-
pling ip( )andip(), is used to independently train
a standard TD3 agent on proprioceptive states for 1 million
steps. Consequently, the ﬁnal solution is used to generate 10
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.0
Desired Speed2004006008001000Average ReturnTD3 Solution
HyperZero
Conditional Policy + UVFAConditional Policy
Meta Policy, Zero-shot
Meta Policy, Few-shotPEARL Policy, Zero-shot
PEARL Policy, Few-shot
(a) Cheetah environment.
4
 2
 0 2 4
Desired Speed02004006008001000Average ReturnTD3 Solution
HyperZero
Conditional Policy + UVFAConditional Policy
Meta Policy, Zero-shot
Meta Policy, Few-shotPEARL Policy, Zero-shot
PEARL Policy, Few-shot (b) Walker environment.
15
 10
 5
 0 5 10 15
Desired Speed2004006008001000Average ReturnTD3 Solution
HyperZero
Conditional Policy + UVFAConditional Policy
Meta Policy, Zero-shot
Meta Policy, Few-shotPEARL Policy, Zero-shot
PEARL Policy, Few-shot (c) Finger environment.
Figure 3: Zero-shot transfer to new reward settings on DM control environments, obtained on 5 seeds for random split of
train/test tasks. Solid lines present the mean and shaded regions present the standard deviation of the average return across the
seeds. Horizontal axis shows the desired speed, which is a function of the reward parameters  i.

--- PAGE 5 ---
0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70
Torso Length2004006008001000Average ReturnTD3 Solution
HyperZero
Conditional Policy + UVFAConditional Policy
Meta Policy, Zero-shot
Meta Policy, Few-shotPEARL Policy, Zero-shot
PEARL Policy, Few-shot(a) Cheetah environment.
0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50
Torso Length02004006008001000Average ReturnTD3 Solution
HyperZero
Conditional Policy + UVFAConditional Policy
Meta Policy, Zero-shot
Meta Policy, Few-shotPEARL Policy, Zero-shot
PEARL Policy, Few-shot (b) Walker environment.
0.10 0.15 0.20 0.25 0.30 0.35 0.40
Finger Length2004006008001000Average ReturnTD3 Solution
HyperZero
Conditional Policy + UVFAConditional Policy
Meta Policy, Zero-shot
Meta Policy, Few-shotPEARL Policy, Zero-shot
PEARL Policy, Few-shot (c) Finger environment.
Figure 4: Zero-shot transfer to new dynamics settings on DM control environments, obtained on 5 seeds for random split of
train/test tasks. Solid lines present the mean and shaded regions present the standard deviation of the average return across the
seeds. Horizontal axis shows the value of dynamics parameter i; that is torso length for the cheetah and walker, and ﬁnger
length for the ﬁnger. Notably, a change in the shape of the geometry results in changes in the weight and inertia parameters.
rollouts to be added to the dataset D. Learning curves for the
RL solutions are in Appendix B.3. As these results show, in
some instances, the RL solution is not fully converged after 1
million steps. Despite this, HyperZero is able to approximate
the mapping reasonably well, thus indicating Assumption 2
can be relaxed to an extent in practice.
Train/Test Split of the Tasks. To reliably evaluate the
zero-shot transfer abilities of HyperZero to novel reward/-
dynamics settings against the baselines, and to rule out the
possibility of selective choosing of train/test tasks, we ran-
domly divide task settings into train ( %85) and test ( %15)
sets. We consequently report the mean and standard devia-
tion of the average return obtained on 5 seeds.
Baselines. We compare HyperZero against common base-
lines for multitask and meta learning:
1. Context-conditioned policy; trained to predict actions,
similarly to imitation learning methods.
2. Context-conditioned policy paired with UVFA (Schaulet al. 2015); trained to predict actions and values. It fur-
ther beneﬁts from using our proposed TD loss LTD, sim-
ilarly to HyperZero.
3. Context-conditioned meta policy; trained with MAML
(Finn, Abbeel, and Levine 2017) to predict actions and
evaluated for both zero-shot and few-shot transfer. Our
context-conditioned meta-policy can be regarded as an
adaptation of PEARL (Rakelly et al. 2019) in which the
inferred task is substituted by the ground-truth task.
4. PEARL (Rakelly et al. 2019) policy; trained to predict
actions. Unlike other baselines, PEARL does not assume
access to the MDP context and instead it infers the the
context from states and actions.
Notably, since MAML and PEARL are known to perform
poorly for zero-shot transfer, we evaluate the meta policy
for both zero-shot and few-shot transfers. In the latter, prior
to evaluation, the meta policy is ﬁnetuned with near-optimal
trajectories of the test MDP Migenerated by the actual RL
solution.
Speed1
2
3
4
5
6
7
8
9
10Torso Length
0.300.350.400.450.500.550.600.650.70Average Return
200300400500600700800900TD3 Solution
HyperZeroConditional Policy + UVFA
Conditional PolicyMeta Policy, Few-shot
PEARL Policy, Few-shot
(a) Cheetah environment.
Speed0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0Torso Length
0.100.150.200.250.300.350.400.450.50Average Return
200400600800TD3 Solution
HyperZeroConditional Policy + UVFA
Conditional PolicyMeta Policy, Few-shot
PEARL Policy, Few-shot (b) Walker environment.
Speed1234567891011121314
15Finger Length
0.100.150.200.250.300.350.40Average Return
100200300400500600700800900TD3 Solution
HyperZeroConditional Policy + UVFA
Conditional PolicyMeta Policy, Few-shot
PEARL Policy, Few-shot (c) Finger environment.
Figure 5: Zero-shot transfer to new reward and dynamics settings on DM control environments, obtained on 5 seeds for
random split of train/test tasks. Each surface present the mean of the average return across the seeds. X-axis shows the desired
speed, which is a function of the reward parameters  i, while Y-axis shows the value of the dynamics parameter i. The surfaces
are smoothed for visual clarity. 2D plots of these 3D diagrams are presented in Appendix B.1 for better comparison.

--- PAGE 6 ---
(a) Cheetah environment with different torso lengths.
 (b) Walker environment with different desired speeds.
Figure 6: Rollout of a trained HyperZero on different task parameters. (a)The trained HyperZero is used to rollout the cheetah
environment with torso lengths of 0.3 and 0.7. (c)The trained HyperZero is used to rollout the walker environment with desired
speeds of -4 and +4. Additional results are in Appendix B.2.
Finally, for a fair comparison with hypernetworks, all
methods follow the same two-stage training paradigm de-
scribed in Section 3.1, have a learnable task embedding, and
share the same network architecture. Full implementation
details are in Appendix C.
4.2 Results
Zero-shot Transfer. We compare the zero-shot transfer
of HyperZero against the baselines in the three cases of
changed rewards, changed dynamics, and simultaneously
changed rewards and dynamics; results are shown in Figures
3, 4, and 5, respectively. Additional results are in Appendix
B.1. As suggested by these results, in all environments and
transfer scenarios, HyperZero signiﬁcantly outperforms the
baselines, demonstrating the effectiveness of our learning
framework for approximating an RL algorithm as a mapping
from a parameterized MDP Mito a near-optimal policy 
i
and action-value function Q
i.
Importantly, the context-conditioned policy (paired with
UVFA) consists of all the major components of HyperZero,
including near-optimal action and value prediction, and TD
regularization. As a result, the only difference is that Hyper-
Zero learns to generate policies conditioned on the context
which is in turn used to predict actions, while the context-
conditioned policy learns to predict actions conditioned on
the context. We hypothesize two main reasons for the signif-
icant improvements gained from such use of hypernetworks
in our setting. First, aligned with similar observations in the
literature (Galanti and Wolf 2020; von Oswald et al. 2019),
hypernetworks allow for effective abstraction of the learn-
ing problem into two levels of policy (or equivalently value
function) generation and action (or equivalently value) pre-
diction.
Second, as hypernetworks are used to learn the map-
ping from MDP parameters to the space of policies, that is
( i;i)!
i, they achieve generalization across the space
of policies. On the contrary, since the context-conditioned
policy simultaneously learns the mapping of states and MDP
parameters to actions, that is (s; i;i)!a, it is only able
to achieve generalization over the space of actions, as op-
posed to the more general space of policies.
Finally, due to the strong zero-shot transfer ability of theapproximated solution to new rewards and dynamics, one
can use it to visualize the near-optimal trajectory 
ifor
novel tasks without necessarily training the RL algorithm.
A possible application of this approach would be for task vi-
sualization or environment design, as well as manual reward
shaping. As an example, Figure 6 shows sample trajectories
10.0
 7.5
 5.0
 2.5
 0.0 2.5 5.0 7.5 10.0
Desired Speed6007008009001000Average ReturnTD3 Solution
HyperZeroHyperZero w/o TD
HyperZero w/o Value Function w/o TD
(a) Cheetah environment.
4
 2
 0 2 4
Desired Speed3004005006007008009001000Average ReturnTD3 Solution
HyperZeroHyperZero w/o TD
HyperZero w/o Value Function w/o TD
(b) Walker environment.
Figure 7: Ablation study on the improvements gained from
generating the optimal value function and using the TD
loss. Results are obtained on 5 seeds for random split of
train/test tasks. Solid lines present the mean and shaded re-
gions present the standard deviation of the average return
across the seeds. Horizontal axis shows the desired speed,
which is a function of the reward parameters  i.

--- PAGE 7 ---
generated by rolling out trained HyperZero models condi-
tioned on different reward/dynamics parameters. Additional
trajectories are in Appendix B.2.
Ablation Study on HyperZero Variants. In Figure 7, we
carry out an ablation study on the improvements gained from
generating the near-optimal value function and using our
proposed TD loss from Equation (5). We draw two conclu-
sions from this study; ﬁrst, generating the action-value func-
tionQ
ialongside the policy 
iprovides additional learning
signal for training the hypernetwork. Furthermore, incorpo-
rating the TD loss between the generated policy and action-
value function ensures the two generated networks are con-
sistent with one another with respect to the Bellman equation
and results in overall better performance and generalization.
While the improvements may appear to be small, we sus-
pect that gains would be larger in visual control problems,
as generating the value function will provide a rich learn-
ing signal for representation learning. More importantly, the
generated value function can have other applications, such as
beings used in policy gradient methods for further training
the generated policy with environment interactions (ofﬂine-
to-online RL) (Lee et al. 2022). While this is left for future
work, we wanted to ensure that our framework is capable of
generating the value function alongside the policy.
5 Related Work
The robustness and generalization of behaviors has long
been studied in control and RL.
Transfer, Contextual and Meta RL. Past work has stud-
ied numerous forms of Transfer Learning (Taylor and Stone
2009), where MDP components including the state space,
action space, dynamics or reward are modiﬁed between the
training conducted on one or many source tasks, prior to per-
formance on one or more targets. Depending on the learner’s
view of sources and targets, the problem is called contextual
policy search (Kupcsik et al. 2017) life-long learning (Abel
et al. 2018), curriculum learning (Portelas et al. 2020), or
meta learning (Finn, Abbeel, and Levine 2017), but our par-
ticular variant, with an always-observable parameter vector
and no chance to train or ﬁne-tune on the target is most aptly
named zero-shot contextual RL. Within that problem, a com-
mon concern has been how to interpolate in the space of con-
texts (equivalent to our parameters), while preserving details
of the policy-space solution (Barbaros et al. 2018). This is
precisely where the power of our hypernetwork architecture
extends prior art.
Hypernetworks in RL. While hypernetworks (Ha, Dai,
and Le 2016) have been used extensively in supervised
learning problems (von Oswald et al. 2019; Galanti and
Wolf 2020; Krueger et al. 2017; Zhao et al. 2020), their
application to RL algorithms remains relatively limited.
Recent work of Saraﬁan, Keynan, and Kraus (2021) use
hypernetworks to improve gradient estimation of Qfunc-
tions and policy networks in policy gradient algorithms. In
multi-agent RL, hypernetworks are used to generate poli-
cies or value functions based on agent properties (Rashid
et al. 2018; Iqbal et al. 2020, 2021; de Witt et al. 2020;Zhou et al. 2020). Furthermore, hypernetworks have been
used to model an evolving dynamical system in continual
model-based RL (Huang et al. 2021). Related to our ap-
proach, Faccio et al. (2022a) use hypernetworks to learn
goal-conditioned optimal policies; the key distinguishing
factor of our approach is that we focus on zero-shot transfer
across a family of MDPs with different reward and dynam-
ics functions, while the method of Faccio et al. (2022a) aims
to solve a single goal-conditioned MDP.
Upside Down RL. Upside down RL (UDRL) is a re-
deﬁnition of the RL problem transforming it into a form
of supervised learning. UDRL, rather than learning opti-
mal policies using rewards, teaches agents to follow com-
mands. This method maps input observations as commands
to action probabilities with supervised learning conditioned
on past experiences (Srivastava et al. 2019; Schmidhuber
2019). Related to this idea are ofﬂine RL models that use
sequence modeling as opposed to supervised learning to
model behavior (Janner, Li, and Levine 2021; Chen et al.
2021). Similarly to UDRL, many RL algorithms incorporate
the use of supervised learning in their model (Schmidhuber
2015; Rosenstein et al. 2004). One such technique is hind-
sight RL in which commands correspond to goal conditions
(Andrychowicz et al. 2017; Rauber et al. 2017; Harutyun-
yan et al. 2019). Another approach is to use forward models
as opposed to the backward ones used in UDRL (Arjona-
Medina et al. 2019). Recently, Faccio et al. (2022a) propose
a method that evaluates generated polices in the command
space rather than optimizing a single policy for achieving a
desired reward.
6 Conclusion
This paper has described an approach, named HyperZero,
which learns to generalize optimal behavior across a fam-
ily of tasks. By training on the full RL solutions of train-
ing tasks, including their optimal policy and value function
parameters, the hypernetworks used in our architecture are
trained to directly output the parameters of complex neural
network policies capable of solving unseen target tasks. This
work extends the performance of zero-shot generalization
over prior approaches. Our experiments demonstrate that our
zero-shot behaviors achieve nearly full performance, as de-
ﬁned by the performance of the optimal policy recovered by
an RL learner training for a large amount of iterations on the
target task itself.
Due to the strong generalization of our method, with min-
imal test-time computational requirements, our approach is
suitable for deployment in live systems. We also highlight
the opportunity for human-interfaces and exploration of RL
solutions. In short, this new level of rapid, but powerful,
general behavior can provide signiﬁcant opportunity for the
practical deployment of RL-learned behavior in the future.
References
Abel, D.; Jinnai, Y .; Guo, Y .; Konidaris, G.; and Littman,
M. L. 2018. Policy and Value Transfer in Lifelong Rein-
forcement Learning. In Proceedings of the International
Conference on Machine Learning (ICML) .

--- PAGE 8 ---
Andrychowicz, M.; Wolski, F.; Ray, A.; Schneider, J.; Fong,
R.; Welinder, P.; McGrew, B.; Tobin, J.; Pieter Abbeel, O.;
and Zaremba, W. 2017. Hindsight experience replay. Ad-
vances in neural information processing systems , 30.
Arjona-Medina, J. A.; Gillhofer, M.; Widrich, M.; Un-
terthiner, T.; Brandstetter, J.; and Hochreiter, S. 2019. Rud-
der: Return decomposition for delayed rewards. Advances
in Neural Information Processing Systems , 32.
Arnold, S. M.; Mahajan, P.; Datta, D.; Bunner, I.; and
Zarkias, K. S. 2020. learn2learn: A library for meta-learning
research. arXiv preprint arXiv:2008.12284 .
Bagnell, J. A. 2015. An invitation to imitation. Technical
report, Carnegie-Mellon Univ Pittsburgh Pa Robotics Inst.
Barbaros, V .; van Hoof, H.; Abdolmaleki, A.; and Meger, D.
2018. Eager and Memory-Based Non-Parametric Stochastic
Search Methods for Learning Control. In Proceedings of
the International Conference on Robotics and Automation
(ICRA) .
Barreto, A.; Hou, S.; Borsa, D.; Silver, D.; and Precup, D.
2020. Fast reinforcement learning with generalized policy
updates. Proceedings of the National Academy of Sciences ,
117(48): 30079–30087.
Bellman, R. 1966. Dynamic programming. Science ,
153(3731): 34–37.
Borsa, D.; Barreto, A.; Quan, J.; Mankowitz, D. J.; van Has-
selt, H.; Munos, R.; Silver, D.; and Schaul, T. 2018. Uni-
versal Successor Features Approximators. In International
Conference on Learning Representations .
Chen, L.; Lu, K.; Rajeswaran, A.; Lee, K.; Grover, A.;
Laskin, M.; Abbeel, P.; Srinivas, A.; and Mordatch, I. 2021.
Decision transformer: Reinforcement learning via sequence
modeling. Advances in neural information processing sys-
tems, 34: 15084–15097.
de Witt, C. S.; Peng, B.; Kamienny, P.-A.; Torr, P.; B ¨ohmer,
W.; and Whiteson, S. 2020. Deep multi-agent reinforcement
learning for decentralized continuous cooperative control.
arXiv preprint arXiv:2003.06709 .
Faccio, F.; Herrmann, V .; Ramesh, A.; Kirsch, L.; and
Schmidhuber, J. 2022a. Goal-Conditioned Generators of
Deep Policies. arXiv preprint arXiv:2207.01570 .
Faccio, F.; Ramesh, A.; Herrmann, V .; Harb, J.; and Schmid-
huber, J. 2022b. General Policy Evaluation and Improve-
ment by Learning to Identify Few But Crucial States. arXiv
preprint arXiv:2207.01566 .
Finn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic
meta-learning for fast adaptation of deep networks. In In-
ternational conference on machine learning , 1126–1135.
PMLR.
Fujimoto, S.; Hoof, H.; and Meger, D. 2018. Addressing
function approximation error in actor-critic methods. In
International conference on machine learning , 1587–1596.
PMLR.
Galanti, T.; and Wolf, L. 2020. On the modularity of hy-
pernetworks. Advances in Neural Information Processing
Systems , 33: 10409–10419.Ha, D.; Dai, A.; and Le, Q. V . 2016. Hypernetworks. arXiv
preprint arXiv:1609.09106 .
Hallak, A.; Di Castro, D.; and Mannor, S. 2015. Con-
textual markov decision processes. arXiv preprint
arXiv:1502.02259 .
Harb, J.; Schaul, T.; Precup, D.; and Bacon, P.-L. 2020. Pol-
icy evaluation networks. arXiv preprint arXiv:2002.11833 .
Harutyunyan, A.; Dabney, W.; Mesnard, T.; Ghesh-
laghi Azar, M.; Piot, B.; Heess, N.; van Hasselt, H. P.;
Wayne, G.; Singh, S.; Precup, D.; et al. 2019. Hindsight
credit assignment. Advances in neural information process-
ing systems , 32.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-
ual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 770–778.
Ho, J.; and Ermon, S. 2016. Generative adversarial imita-
tion learning. Advances in neural information processing
systems , 29.
Huang, Y .; Xie, K.; Bharadhwaj, H.; and Shkurti, F. 2021.
Continual model-based reinforcement learning with hyper-
networks. In 2021 IEEE International Conference on
Robotics and Automation (ICRA) , 799–805. IEEE.
Iqbal, S.; de Witt, C. A. S.; Peng, B.; B ¨ohmer, W.; Whiteson,
S.; and Sha, F. 2020. Ai-qmix: Attention and imagination for
dynamic multi-agent reinforcement learning. arXiv preprint
arXiv:2006.04222 .
Iqbal, S.; De Witt, C. A. S.; Peng, B.; B ¨ohmer, W.; White-
son, S.; and Sha, F. 2021. Randomized Entity-wise Fac-
torization for Multi-Agent Reinforcement Learning. In In-
ternational Conference on Machine Learning , 4596–4606.
PMLR.
Janner, M.; Li, Q.; and Levine, S. 2021. Ofﬂine reinforce-
ment learning as one big sequence modeling problem. Ad-
vances in neural information processing systems , 34: 1273–
1286.
Jiang, N.; Krishnamurthy, A.; Agarwal, A.; Langford, J.; and
Schapire, R. E. 2017. Contextual decision processes with
low bellman rank are pac-learnable. In International Con-
ference on Machine Learning , 1704–1713. PMLR.
Kingma, D. P.; and Ba, J. 2014. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 .
Krueger, D.; Huang, C.-W.; Islam, R.; Turner, R.; Lacoste,
A.; and Courville, A. 2017. Bayesian hypernetworks. arXiv
preprint arXiv:1710.04759 .
Kupcsik, A.; Deisenroth, M. P.; Peters, J.; Loh, A. P.;
Vadakkepat, P.; and Neumann, G. 2017. Model-based
contextual policy search for data-efﬁcient generalization of
robot skills.
Lee, S.; Seo, Y .; Lee, K.; Abbeel, P.; and Shin, J. 2022.
Ofﬂine-to-online reinforcement learning via balanced replay
and pessimistic q-ensemble. In Conference on Robot Learn-
ing, 1702–1712. PMLR.
Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.;
Tassa, Y .; Silver, D.; and Wierstra, D. 2015. Continuous

--- PAGE 9 ---
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971 .
Mnih, V .; Kavukcuoglu, K.; Silver, D.; Graves, A.;
Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Play-
ing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602 .
Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;
DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer,
A. 2017. Automatic differentiation in pytorch.
Portelas, R.; Colas, C.; Hofmann, K.; and Oudeyer, P.-Y .
2020. Teacher algorithms for curriculum learning of Deep
RL in continuously parameterized environments. In Pro-
ceedings of the Conference on Robot Learning (CoRL) , vol-
ume 100, 835–853.
Puterman, M. L. 2014. Markov decision processes: discrete
stochastic dynamic programming . John Wiley & Sons.
Rakelly, K.; Zhou, A.; Finn, C.; Levine, S.; and Quillen, D.
2019. Efﬁcient off-policy meta-reinforcement learning via
probabilistic context variables. In International conference
on machine learning , 5331–5340. PMLR.
Rashid, T.; Samvelyan, M.; Schroeder, C.; Farquhar, G.; Fo-
erster, J.; and Whiteson, S. 2018. Qmix: Monotonic value
function factorisation for deep multi-agent reinforcement
learning. In International conference on machine learning ,
4295–4304. PMLR.
Rauber, P.; Ummadisingu, A.; Mutz, F.; and Schmidhu-
ber, J. 2017. Hindsight policy gradients. arXiv preprint
arXiv:1711.06006 .
Rosenstein, M. T.; Barto, A. G.; Si, J.; Barto, A.; Powell,
W.; and Wunsch, D. 2004. Supervised actor-critic reinforce-
ment learning. Learning and Approximate Dynamic Pro-
gramming: Scaling Up to the Real World , 359–380.
Ross, S.; Gordon, G.; and Bagnell, D. 2011. A reduction of
imitation learning and structured prediction to no-regret on-
line learning. In Proceedings of the fourteenth international
conference on artiﬁcial intelligence and statistics , 627–635.
JMLR Workshop and Conference Proceedings.
Saraﬁan, E.; Keynan, S.; and Kraus, S. 2021. Recomposing
the reinforcement learning building blocks with hypernet-
works. In International Conference on Machine Learning ,
9301–9312. PMLR.
Schaul, T.; Horgan, D.; Gregor, K.; and Silver, D. 2015. Uni-
versal value function approximators. In International con-
ference on machine learning , 1312–1320. PMLR.
Schmidhuber, J. 2015. Deep learning in neural networks:
An overview. Neural networks , 61: 85–117.
Schmidhuber, J. 2019. Reinforcement Learning Upside
Down: Don’t Predict Rewards–Just Map Them to Actions.
arXiv preprint arXiv:1912.02875 .
Schroecker, Y .; and Isbell, C. 2020. Universal value den-
sity estimation for imitation learning and goal-conditioned
reinforcement learning. arXiv preprint arXiv:2002.06473 .
Sorg, J.; Lewis, R. L.; and Singh, S. 2010. Reward design
via online gradient ascent. Advances in Neural Information
Processing Systems , 23.Srivastava, R. K.; Shyam, P.; Mutz, F.; Ja ´skowski, W.; and
Schmidhuber, J. 2019. Training agents using upside-down
reinforcement learning. arXiv preprint arXiv:1912.02877 .
Sutton, R. S.; and Barto, A. G. 2018. Reinforcement learn-
ing: An introduction . MIT press.
Tan, C.; Sun, F.; Kong, T.; Zhang, W.; Yang, C.; and Liu, C.
2018. A survey on deep transfer learning. In International
conference on artiﬁcial neural networks , 270–279. Springer.
Tang, Z.; Feng, Y .; and Liu, Q. 2022. Operator Deep Q-
Learning: Zero-Shot Reward Transferring in Reinforcement
Learning. arXiv preprint arXiv:2201.00236 .
Taylor, M. E.; and Stone, P. 2009. Transfer Learning for Re-
inforcement Learning Domains: A Survey. Journal of Ma-
chine Learning Research , 10(56): 1633–1685.
Teh, Y .; Bapst, V .; Czarnecki, W. M.; Quan, J.; Kirkpatrick,
J.; Hadsell, R.; Heess, N.; and Pascanu, R. 2017. Distral:
Robust multitask reinforcement learning. Advances in neu-
ral information processing systems , 30.
Todorov, E.; Erez, T.; and Tassa, Y . 2012. Mujoco: A physics
engine for model-based control. In 2012 IEEE/RSJ interna-
tional conference on intelligent robots and systems , 5026–
5033. IEEE.
von Oswald, J.; Henning, C.; Grewe, B. F.; and Sacramento,
J. 2019. Continual learning with hypernetworks. In Interna-
tional Conference on Learning Representations .
Xu, Z.; van Hasselt, H. P.; Hessel, M.; Oh, J.; Singh, S.;
and Silver, D. 2020. Meta-gradient reinforcement learning
with an objective discovered online. Advances in Neural
Information Processing Systems , 33: 15254–15264.
Xu, Z.; van Hasselt, H. P.; and Silver, D. 2018. Meta-
gradient reinforcement learning. Advances in neural infor-
mation processing systems , 31.
Zhao, D.; von Oswald, J.; Kobayashi, S.; Sacramento, J.; and
Grewe, B. F. 2020. Meta-learning via hypernetworks.
Zheng, Z.; Oh, J.; Hessel, M.; Xu, Z.; Kroiss, M.; Van Has-
selt, H.; Silver, D.; and Singh, S. 2020. What can learned
intrinsic rewards capture? In International Conference on
Machine Learning , 11436–11446. PMLR.
Zheng, Z.; Oh, J.; and Singh, S. 2018. On learning intrinsic
rewards for policy gradient methods. Advances in Neural
Information Processing Systems , 31.
Zhou, M.; Liu, Z.; Sui, P.; Li, Y .; and Chung, Y . Y . 2020.
Learning implicit credit assignment for cooperative multi-
agent reinforcement learning. Advances in Neural Informa-
tion Processing Systems , 33: 11853–11864.

--- PAGE 10 ---
A Details of the Environments and Data Collection
In this section, we provide the full details of the custom environments and the data collection procedure to supplement the
results of Section 4.1
A.1 Environment Details
We use three challenging environments for evaluation: cheetah, walker, ﬁnger. All environments are derived from DeepMind
Control Suite by enabling a modular approach for modifying the reward and dynamics parameters. The code for our modular
and customizable environments will be publicly available by the time of publication.
Reward Parameters. In all environments, the reward parameter  correspond to the the desired speed of the agent in order
to allow for easier visualization of the learned behaviour. Table 1 presents the details of the reward parameters used in the
experiments for transfer to novel rewards. Notably, desired speed of 0 is not included due to its trivial solution.
Environment Reward Parameter  Range and Increments No. of Samples p( ) Default of DMC
Cheetah Desired moving speed [ 10;+10] , 0.5 increments 40 samples +10
Walker Desired moving speed [ 5;+5], 0.25 increments 40 samples +1 (walk), +8 (run)
Finger Desired spinning speed [ 15;+15] , 1.0 increments 30 samples +15
Table 1: Details of the reward parameters used in experiments for transfer to novel reward settings.
Dynamic Parameters. In all environments, the changing dynamic parameter is selected as the size of a geometry of the body.
The change in the size of the body results in changes in the mass and inertia as they are automatically computed from the shape
in the physics simulator. Table 2 presents the details of the dynamics parameters used in the experiments for transfer to novel
dynamics parameters.
Environment Dynamics Parameter  Range and Increments No. Samples p()Default of DMC
Cheetah Torso length [0:3;0:7], 0.01 increments 41 samples 0.5
Walker Torso length [0:1;0:5], 0.01 increments 41 samples 0.3
Finger Finger length and distance to spinner [0:1;0:4], 0.01 increments 31 samples 0.16
Table 2: Details of the dynamics parameters used in experiments for transfer to novel dynamics settings.
Reward and Dynamic Parameters. Based on the reward and dynamics parameters described above, Table 3 presents the
details of the reward and dynamics parameters. Importantly, the space of parameters now forms a 2D grid.
Environment Reward Parameter  Dynamics Parameter  Samples from p( ;)
Cheetah [+1;+10] , 1 increments [0:3;0:7], 0.05 increments 109grid
Walker [+1;+5], 0.5 increments [0:1;0:5], 0.05 increments 109grid
Finger [+1;+15] , 1 increments [0:1;0:4], 0.05 increments 159grid
Table 3: Details of the reward and dynamics parameters used in experiments for transfer to novel rewards and dynamics
settings.
A.2 Data Collection
First, the sampled MDPs Mifrom each parameterized MDP family Mis used to independently train a TD3 (Fujimoto, Hoof,
and Meger 2018) agent for 1 million steps. Results of this phase are presented in Appendix B.3. Consequently, each trained
agent is rolled out for 10 episodes and the near-optimal trajectory is added to the dataset. The train ( %85) and test ( %15) MDPs
are randomly selected from the set of all MDPs for 5 different seeds.
The generated dataset will be released publicly by the time of publication, as a benchmark for zero-shot transfer learning to
novel dynamics and reward settings.

--- PAGE 11 ---
B Full Results
In this section, we provide additional empirical results, to supplement the results of Section 4.2.
B.1 Additional Results
Figures 8-10 present grids of 2D plots of the experiments for zero-shot transfer to new rewards and dynamics settings. These
Figures supplement 3D plots of Figure 5 of the paper as they provide a clearer comparison between HyperZero and the baselines.
2 4 6 8 102004006008001000Average ReturnTorso Length: 0.3
2 4 6 8 1002004006008001000Torso Length: 0.35TD3 Solution HyperZero Conditional Policy + UVFA Conditional Policy Meta Policy, Zero-shot Meta Policy, Few-shot PEARL Policy, Zero-shot PEARL Policy, Few-shot
2 4 6 8 102004006008001000Torso Length: 0.4
2 4 6 8 1002004006008001000Average ReturnTorso Length: 0.45
2 4 6 8 1002004006008001000Torso Length: 0.5
2 4 6 8 1002004006008001000Torso Length: 0.55
2 4 6 8 10
Speed02004006008001000Average ReturnTorso Length: 0.6
2 4 6 8 10
Speed02004006008001000Torso Length: 0.65
2 4 6 8 10
Speed02004006008001000Torso Length: 0.7
Figure 8: Zero-shot transfer to new reward and dynamics settings for the Cheetah environment, obtained on 5 seeds for
random split of train/test tasks. Each subplot is for a speciﬁc value of the torso length which is a dynamics parameter i. This
grid of 2D plots is to supplement the 3D plot of Figure 5.a of the paper. Solid lines present the mean and shaded regions present
the standard deviation of the average return across the seeds. Horizontal axis shows the desired speed, which is a function of
the reward parameters  i.

--- PAGE 12 ---
1 2 3 4 50200400600800Average ReturnTorso Length: 0.1
1 2 3 4 502004006008001000Torso Length: 0.15TD3 Solution HyperZero Conditional Policy + UVFA Conditional Policy Meta Policy, Zero-shot Meta Policy, Few-shot PEARL Policy, Zero-shot PEARL Policy, Few-shot
1 2 3 4 502004006008001000Torso Length: 0.2
1 2 3 4 502004006008001000Average ReturnTorso Length: 0.25
1 2 3 4 502004006008001000Torso Length: 0.3
1 2 3 4 50200400600800Torso Length: 0.35
1 2 3 4 5
Speed0200400600800Average ReturnTorso Length: 0.4
1 2 3 4 5
Speed0200400600800Torso Length: 0.45
1 2 3 4 5
Speed0100200300400500600700Torso Length: 0.5Figure 9: Zero-shot transfer to new reward and dynamics settings for the Walker environment, obtained on 5 seeds for random
split of train/test tasks. Each subplot is for a speciﬁc value of the torso length which is a dynamics parameter i. This grid of
2D plots is to supplement the 3D plot of Figure 5.b of the paper. Solid lines present the mean and shaded regions present the
standard deviation of the average return across the seeds. Horizontal axis shows the desired speed, which is a function of the
reward parameters  i.

--- PAGE 13 ---
2 4 6 8 10 12 142004006008001000Average ReturnFinger Length: 0.1
2 4 6 8 10 12 142004006008001000Finger Length: 0.15TD3 Solution HyperZero Conditional Policy + UVFA Conditional Policy Meta Policy, Zero-shot Meta Policy, Few-shot PEARL Policy, Zero-shot PEARL Policy, Few-shot
2 4 6 8 10 12 142004006008001000Finger Length: 0.2
2 4 6 8 10 12 142004006008001000Average ReturnFinger Length: 0.25
2 4 6 8 10 12 14
Speed2004006008001000Finger Length: 0.3
2 4 6 8 10 12 14
Speed2004006008001000Finger Length: 0.35
2 4 6 8 10 12 14
Speed200400600800Average ReturnFinger Length: 0.4Figure 10: Zero-shot transfer to new reward and dynamics settings for the Finger environment, obtained on 5 seeds for
random split of train/test tasks. Each subplot is for a speciﬁc value of the ﬁnger length which is a dynamics parameter i. This
grid of 2D plots is to supplement the 3D plot of Figure 5.c of the paper. Solid lines present the mean and shaded regions present
the standard deviation of the average return across the seeds. Horizontal axis shows the desired speed, which is a function of
the reward parameters  i.

--- PAGE 14 ---
B.2 Example Learned Behaviors
Figures 11-16 show example trajectories obtained by rolling out the trained HyperZero policy on different reward and dynamics
settings for various environments. In each case, the trajectories are generated by rolling out a single HyperZero agent with
different inputs, corresponding to the MDP setting. Please refer to the supplementary video submission for videos of these
behaviours.
Figure 11: Example trajectories for the Cheetah environment with reward changes, obtained by rolling out a single HyperZero
with different inputs.
Figure 12: Example trajectories for the Cheetah environment with dynamics changes, obtained by obtained by rolling out a
single HyperZero with different inputs.

--- PAGE 15 ---
Figure 13: Example trajectories for the Walker environment with reward changes, obtained by rolling out a single HyperZero
with different inputs.
Figure 14: Example trajectories for the Walker environment with dynamics changes, obtained by rolling out a single HyperZero
with different inputs.

--- PAGE 16 ---
Figure 15: Example trajectories for the Finger environment with reward changes, obtained by rolling out a single HyperZero
with different inputs.
Figure 16: Example trajectories for the Finger environment with dynamics changes, obtained by rolling out a single HyperZero
with different inputs.

--- PAGE 17 ---
B.3 Training of the Optimal RL Solution
In this section, we present the learning curves for the actual RL solutions that were used as the near-optimal solutions to each
parameterized MDP Mi2M. As described in Appendix A.2, the near-optimal policy 
iand action-value function Q
ifor
each MDPMi2Mis obtained by training TD3 (Fujimoto, Hoof, and Meger 2018) for 1 million steps.
Figures 17 and 18 show the results obtained for individual changes in the reward and dynamics settings, respectively. Figure
19 shows the results obtained for the simultaneous changes in the reward and dynamics settings.
As these results suggest, in some instances the TD3 solver appears to be not fully-converged to the near-optimal solution.
Therefore, in practice our method works despite the violation of Assumption 2.
0.0 0.2 0.4 0.6 0.8 1.0
Time Steps (1e6)2004006008001000Average ReturnCheetah
Speed: -10.0
Speed: -9.5
Speed: -9.0
Speed: -8.5
Speed: -8.0
Speed: -7.5
Speed: -7.0
Speed: -6.5
Speed: -6.0
Speed: -5.5
Speed: -5.0
Speed: -4.5
Speed: -4.0
Speed: -3.5Speed: -3.0
Speed: -2.5
Speed: -2.0
Speed: -1.5
Speed: -1.0
Speed: -0.5
Speed: 0.5
Speed: 1.0
Speed: 1.5
Speed: 2.0
Speed: 2.5
Speed: 3.0
Speed: 3.5Speed: 4.0
Speed: 4.5
Speed: 5.0
Speed: 5.5
Speed: 6.0
Speed: 6.5
Speed: 7.0
Speed: 7.5
Speed: 8.0
Speed: 8.5
Speed: 9.0
Speed: 9.5
Speed: 10.0
(a) Cheetah environment.
0.0 0.2 0.4 0.6 0.8 1.0
Time Steps (1e6)200
02004006008001000Average ReturnWalker
Speed: -5.0
Speed: -4.75
Speed: -4.5
Speed: -4.25
Speed: -4.0
Speed: -3.75
Speed: -3.5
Speed: -3.25
Speed: -3.0
Speed: -2.75Speed: -2.5
Speed: -2.25
Speed: -2.0
Speed: -1.75
Speed: -1.5
Speed: -1.25
Speed: -1.0
Speed: -0.75
Speed: -0.5
Speed: -0.25Speed: 0.25
Speed: 0.5
Speed: 0.75
Speed: 1.0
Speed: 1.25
Speed: 1.5
Speed: 1.75
Speed: 2.0
Speed: 2.25
Speed: 2.5Speed: 2.75
Speed: 3.0
Speed: 3.25
Speed: 3.5
Speed: 3.75
Speed: 4.0
Speed: 4.25
Speed: 4.5
Speed: 4.75
Speed: 5.0 (b) Walker environment.
0.0 0.2 0.4 0.6 0.8 1.0
Time Steps (1e6)2004006008001000Average ReturnFinger
Speed: -15.0
Speed: -14.0
Speed: -12.0
Speed: -11.0
Speed: -10.0
Speed: -9.0
Speed: -8.0
Speed: -7.0
Speed: -6.0
Speed: -5.0Speed: -4.0
Speed: -3.0
Speed: -2.0
Speed: -1.0
Speed: 1.0
Speed: 2.0
Speed: 3.0
Speed: 4.0
Speed: 5.0
Speed: 6.0Speed: 7.0
Speed: 8.0
Speed: 9.0
Speed: 10.0
Speed: 11.0
Speed: 12.0
Speed: 13.0
Speed: 14.0
Speed: 15.0 (c) Finger environment.
Figure 17: Learning curves for TD3 obtained on environments with reward changes .(a)The desired speed of the cheetah is
changed from -10 to +10 with 0.5 increments while its torso length is set to the default value in DM control (0.5). (b)The
desired speed of the walker is changed from -5 to +5 with 0.25 increments while its torso length is set to the default value in
DM control (0.1). (c)The desired spinning speed is changed from -15 to +15 with 1 increments while the ﬁnger length is set to
the default value in DM control (0.16)
0.0 0.2 0.4 0.6 0.8 1.0
Time Steps (1e6)2004006008001000Average ReturnCheetah
Torso Length: 0.3
Torso Length: 0.31
Torso Length: 0.32
Torso Length: 0.33
Torso Length: 0.34
Torso Length: 0.35
Torso Length: 0.36
Torso Length: 0.37
Torso Length: 0.38
Torso Length: 0.39
Torso Length: 0.4
Torso Length: 0.41
Torso Length: 0.42
Torso Length: 0.43Torso Length: 0.44
Torso Length: 0.45
Torso Length: 0.46
Torso Length: 0.47
Torso Length: 0.48
Torso Length: 0.49
Torso Length: 0.5
Torso Length: 0.51
Torso Length: 0.52
Torso Length: 0.53
Torso Length: 0.54
Torso Length: 0.55
Torso Length: 0.56
Torso Length: 0.57Torso Length: 0.58
Torso Length: 0.59
Torso Length: 0.6
Torso Length: 0.61
Torso Length: 0.62
Torso Length: 0.63
Torso Length: 0.64
Torso Length: 0.65
Torso Length: 0.66
Torso Length: 0.67
Torso Length: 0.68
Torso Length: 0.69
Torso Length: 0.7
(a) Cheetah environment.
0.0 0.2 0.4 0.6 0.8 1.0
Time Steps (1e6)02004006008001000Average ReturnWalker
Torso Length: 0.1
Torso Length: 0.11
Torso Length: 0.12
Torso Length: 0.13
Torso Length: 0.14
Torso Length: 0.15
Torso Length: 0.16
Torso Length: 0.17
Torso Length: 0.18
Torso Length: 0.19
Torso Length: 0.2
Torso Length: 0.21
Torso Length: 0.22
Torso Length: 0.23Torso Length: 0.24
Torso Length: 0.25
Torso Length: 0.26
Torso Length: 0.27
Torso Length: 0.28
Torso Length: 0.29
Torso Length: 0.3
Torso Length: 0.31
Torso Length: 0.32
Torso Length: 0.33
Torso Length: 0.34
Torso Length: 0.35
Torso Length: 0.36
Torso Length: 0.37Torso Length: 0.38
Torso Length: 0.39
Torso Length: 0.4
Torso Length: 0.41
Torso Length: 0.42
Torso Length: 0.43
Torso Length: 0.44
Torso Length: 0.45
Torso Length: 0.46
Torso Length: 0.47
Torso Length: 0.48
Torso Length: 0.49
Torso Length: 0.5 (b) Walker environment.
0.0 0.2 0.4 0.6 0.8 1.0
Time Steps (1e6)2004006008001000Average ReturnFinger
Finger Length: 0.1
Finger Length: 0.11
Finger Length: 0.12
Finger Length: 0.13
Finger Length: 0.14
Finger Length: 0.15
Finger Length: 0.16
Finger Length: 0.17
Finger Length: 0.18
Finger Length: 0.19
Finger Length: 0.2Finger Length: 0.21
Finger Length: 0.22
Finger Length: 0.23
Finger Length: 0.24
Finger Length: 0.25
Finger Length: 0.26
Finger Length: 0.27
Finger Length: 0.28
Finger Length: 0.29
Finger Length: 0.3Finger Length: 0.31
Finger Length: 0.32
Finger Length: 0.33
Finger Length: 0.34
Finger Length: 0.35
Finger Length: 0.36
Finger Length: 0.37
Finger Length: 0.38
Finger Length: 0.39
Finger Length: 0.4 (c) Finger environment.
Figure 18: Learning curves for TD3 obtained on environments with dynamics changes .(a)The torso length of the cheetah is
changed from 0.3 to 0.7 with 0.01 increments while its desired speed is set to slow running (+5). (b)The torso length of the
walker is changed from 0.1 to 0.5 with 0.01 increments while its desired speed is set to walking (+1). (c)The ﬁnger length is
changed from 0.1 to 0.4 with 0.01 increments while the desired spinning speed is set to +15.

--- PAGE 18 ---
Time Steps (1e6)0.0
0.2
0.4
0.6
0.8
1.0Speed
12345678910Average Return
200400600800Cheetah(a) Cheetah environment.
Time Steps (1e6)0.0
0.2
0.4
0.6
0.8
1.0Speed
0.51.01.52.02.53.03.54.04.55.0Average Return
200400600800Walker (b) Walker environment.
Time Steps (1e6)0.0
0.2
0.4
0.6
0.8
1.0Speed
2468101214Average Return
100200300400500600700800900Finger (c) Finger environment.
Figure 19: Learning curves for TD3 obtained on environments with dynamics and rewards changes . Different colors corre-
spond to different dynamics parameters. (a)The torso length of the cheetah is changed from 0.3 to 0.7 with 0.05 increments
while its desired speed is changed from +1 to +10 with 1 increments. (b)The torso length of the walker is changed from 0.1 to
0.5 with 0.05 increments while its desired speed is changed from +0.5 to +5. (c)The ﬁnger length is changed from 0.1 to 0.4
with 0.05 increments while its desired speed is changed from +1 to +15.

--- PAGE 19 ---
C Implementation Details
Our HyperZero implementation, as well as the full learning pipeline for zero-shot transfer learning by approximating RL
solutions, will be made publicly available by the time of publication.
We implemented our method in PyTorch (Paszke et al. 2017) and results were obtained using Python v3.9.12, PyTorch 1.10.1,
CUDA 11.1, and Mujoco 2.1.1 (Todorov, Erez, and Tassa 2012) on Nvidia RTX A6000 GPUs.
RL Solver. We use TD3 (Fujimoto, Hoof, and Meger 2018) for obtaining the near-optimal RL solutions with the hyper-
paramters of Table 4.
Hyperparameter Setting
Learning rate 1e 4
Optimizer Adam (Kingma and Ba 2014)
Mini-batch size 256
Actor update frequency d 2
Target networks update frequency 2
Target networks soft-update  0:01
Target policy smoothing stddev. clip c 0.3
Hidden dim. 256
Replay buffer capacity 106
Discount 0:99
Seed frames 4000
Exploration steps 2000
Exploration stddev. schedule linear (1:0;0:1;1e6)
Table 4: TD3 hyperparameters for obtaining the near-optimal solution for each Mi2M.
HyperZero. HyperZero generates the weights of a near-optimal policy ^
and action-value function ^Q
conditioned on the
MDPMiparameters. The MDP parameters are used as inputs to a shared task embedding, with an architecture based on MLP
ResNet blocks (He et al. 2016), as shown in Listing 1. The 256-dimensional output zof the embedding is transformed linearly
to generate the weights of each layer of the main networks, in a similar fashion as standard hypernetworks (Ha, Dai, and Le
2016).
Listing 1: Architecture of the shared task embedding, used in HyperZero and all of the baselines.
1task_embedding = nn.Sequential(
2 nn.Linear(mdp_param_dim, 64),
3 ResBlock(64),
4 ResBlock(64),
5
6 nn.Linear(64, 128),
7 ResBlock(128),
8 ResBlock(128),
9
10 nn.Linear(128, 256),
11 ResBlock(256),
12 ResBlock(256)
13)
14
15class ResBlock(nn.Module):
16 def __init__(self, in_size):
17 super ().__init__()
18 self.fc = nn.Sequential(
19 nn.ReLU(),
20 nn.Linear(in_size, in_size),
21 nn.ReLU(),
22 nn.Linear(in_size, in_size),
23 )
24
25 def forward(self, x):
26 h = self.fc(x)
27 return x + h

--- PAGE 20 ---
The generated policy and action-value functions are MLP networks with one hidden layer of 256 dimensions. The rest of the
hyperparameters are detailed in Table 5.
Hyperparameter Setting
Learning rate 1e 4
Optimizer Adam (Kingma and Ba 2014)
Mini-batch size 512
Hidden dim. of ^
 256
Hidden dim. of ^Q
 256
Task embedding dim. 256
Table 5: Hyperparameters of HyperZero and all of the baselines.
Baselines. For a fair comparison with HyperZero, all baselines use the same task embedding of Listing 1. The output of this
embedding is concatenated with the inputs to the policy (or value function) and all networks are trained simultaneously using
the supervised learning loss functions described in Section 3. Similarly to HyperZero, the baselines use a 256-dimensional task
embedding and an MLP with one hidden layer of 256 dimensions for the policy (or the value function).
Our MAML (Finn, Abbeel, and Levine 2017) and PEARL (Rakelly et al. 2019) implementations are based on the learn2learn
package (Arnold et al. 2020). Since the MAML policy is context-conditioned, it can be regarded as an adaptation of PEARL
(Rakelly et al. 2019) in which the inferred task is substituted by the ground-truth task. Hyperparameters speciﬁc to MAML and
PEARL are presented in Table 6.
Hyperparameter Setting
Meta learning rate 1e 4
Fast learning rate 1e 2
Meta-batch size 32, or the total number of meta-train MDPs
K-shot 10
Table 6: Hyperparameters specifc to MAML and PEARL.
