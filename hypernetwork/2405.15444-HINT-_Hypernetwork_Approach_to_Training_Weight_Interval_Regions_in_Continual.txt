# 2405.15444.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypernetwork/2405.15444.pdf
# File size: 1586761 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HINT: Hypernetwork Approach to Training Weight Interval Regions in Continual
Learning
Patryk Krukowski1 2, Anna Bielawska2, Kamil Ksi ˛ a ˙zek2Paweł Wawrzy ´nski4Paweł Batorski3
Przemysław Spurek2
1IDEAS NCBR
2Jagiellonian University
3Heinrich Heine Universität Düsseldorf
4IDEAS Institute
patryk.krukowski@ideas-ncbr.pl
Abstract
Recently, a new Continual Learning (CL) paradigm was pre-
sented to control catastrophic forgetting, called Interval Con-
tinual Learning (InterContiNet), which relies on enforcing in-
terval constraints on the neural network parameter space. Un-
fortunately, InterContiNet training is challenging due to the
high dimensionality of the weight space, making intervals dif-
ficult to manage. To address this issue, we introduce HINT, a
technique that employs interval arithmetic within the embed-
ding space and utilizes a hypernetwork to map these intervals
to the target network parameter space. We train interval em-
beddings for consecutive tasks and train a hypernetwork to
transform these embeddings into weights of the target net-
work. An embedding for a given task is trained along with
the hypernetwork, preserving the response of the target net-
work for the previous task embeddings. Interval arithmetic
works with a more manageable, lower-dimensional embed-
ding space rather than directly preparing intervals in a high-
dimensional weight space. Our model allows faster and more
efficient training. Furthermore, HINT maintains the guaran-
tee of not forgetting. At the end of training, we can choose one
universal embedding to produce a single network dedicated
to all tasks. In such a framework, hypernetwork is used only
for training and, finally, we can utilize one set of weights.
HINT obtains significantly better results than InterContiNet
and gives SOTA results on several benchmarks.
1 Introduction
Humans have a natural ability to learn from a continuous
flow of data, as real-world data is usually presented sequen-
tially. Humans need to be able to learn new tasks while
also retaining and utilizing knowledge from previous tasks.
While deep learning models have achieved significant suc-
cess in many individual tasks, they struggle in this aspect and
often perform poorly on the preceding tasks after learning
new ones, which is known as catastrophic forgetting (Mc-
Closkey and Cohen 1989; Ratcliff 1990; French 1999).
Continual learning (CL) is an important area of machine
learning that aims to bridge the gap between human and ma-
chine intelligence. While several methods have been pro-
posed to effectively reduce forgetting when learning new
tasks, such as that by Kirkpatrick et al. (2017); Lopez-Paz
and Ranzato (2017); Shin et al. (2017); Aljundi et al. (2018);
Masse, Grant, and Freedman (2018); Rolnick et al. (2019);van de Ven, Siegelmann, and Tolias (2020), they typically do
not provide any solid guarantees about the extent to which
the model experiences forgetting.
In the Interval Continual Learning (InterContiNet)
(Wołczyk et al. 2022), authors propose a new paradigm that
uses interval arithmetic in CL scenarios. The main idea is
to use intervals to control weights dedicated to subsequent
tasks. Thanks to strict interval arithmetic, these authors can
enforce a network to give the same prediction for each
weight sampled from the interval. Moreover, they can force
the interval of the new task to be entirely contained in the
hyperrectangles of the previous ones. The two above proper-
ties allow one to obtain strict constraints for forgetting. The
model has strong theoretical fundamentals and obtains good
results in incremental tasks and domain scenarios on rela-
tively small datasets. The main limitation of InterContiNet
is a complicated training process. To allow for the training
of new tasks, one has to use large intervals in the weight
space, which is trained in an extremely high-dimensional pa-
rameter space. Consequently, the model is limited to simple
architectures, datasets, and continuous learning scenarios.
To solve such a problem, we propose HINT1, i.e. a model
which uses intervals in the embedding space and a hypernet-
work to transfer them to the weight space of the target net-
work, see Fig. 1. Such a solution allows one to effectively
train interval-based target network on large datasets and
the most difficult incremental class scenarios, surpassing
its foregoing limitations. A hypernetwork architecture (Ha,
Dai, and Le 2016) is defined as a neural network that gener-
ates weights for a specific target network designed to solve
a particular problem. In continual learning, a hypernetwork
may generate weights for the target model (von Oswald et al.
2019; Henning et al. 2021; Ksi ˛ a ˙zek and Spurek 2023). De-
pendent on the task identity, trainable embeddings are fed
into the hypernetwork. After training is complete, a single
meta-model can create task-specific weights. This ability
to generate distinct weights for various tasks enables mod-
els based on hypernetworks to maintain minimal knowledge
loss.
In HINT, we use interval arithmetic in an embedding
space. The hypernetwork is fed with task specific interval
1The source code is available at
https://github.com/gmum/HINTarXiv:2405.15444v4  [cs.LG]  6 May 2025

--- PAGE 2 ---
H([et,¯et];η)[e1,¯e1]
[e2,¯e2]
[e3,¯e3]
[e,¯e] =TT
t=1[et,¯et][θt,¯θt]
[zx,t,¯zx,t] =ϕ(x; [θt,¯θt])
Figure 1: HINT uses interval arithmetic in the input to the hypernetwork. After propagating the intervals through the hyper-
network, we obtain the intervals on the target network layers. The intersection of all intervals produces universal embeddings
dedicated to all tasks. Our model gives theoretical guarantees for not forgetting.
embedding to produce weights for the target network. Our
architecture uses a target model with interval weights and
Interval Bound Propagation (IBP) (Gowal et al. 2018) tech-
nique in the hypernetwork. Due to low dimension of the task
embeddings, this solution provides greater robustness to for-
getting than previous approaches to CL based on interval
arithmetic.
Our model can be used in incremental class learning in
both scenarios: with and without task identity. We can use
the evaluation scenario proposed by hypernetwork-based
models (von Oswald et al. 2019; Henning et al. 2021;
Ksi ˛ a ˙zek and Spurek 2023). For known task identity scenar-
ios, we use interval arithmetic as a regularization technique,
and each task embedding produces new target network in-
terval weights dedicated to this task. We can also use an en-
tropy criterion to determine a class identity when it is un-
known. In both solutions, we must memorize hypernetwork
and all task embeddings. Our framework allows us to pro-
duce a universal embedding. With interval arithmetic, we
can control intervals and transform them into interval em-
beddings with non-empty intersections. At the end of the
training, we generate the final target network from the uni-
versal embedding created from the intersection of all pre-
viously trained interval embeddings, being able to solve all
tasks. Such an approach significantly outperforms InterCon-
tiNet and in many cases outperforms state-of-the-art meth-
ods.
Our contributions can be summarized as follows:
• We show HINT, which uses interval arithmetic in the em-
bedding space and a hypernetwork to propagate intervals
to the target network weight space.
• We demonstrate that our model can efficiently use inter-
val arithmetic in CL settings with large datasets.
• We demonstrate that a hypernetwork can be used to re-
cursively refine, across consecutive tasks, the region of
target network weights that are universally effective for
all tasks.2 Related Works
Continual learning. When a neural network is trained on
new data, it often forgets patterns learned from previous
data, a problem known as catastrophic forgetting (French
1999). Continual learning methods enable the network to
learn new tasks without losing performance on earlier ones,
even when previous data is no longer available. Different
setups exist for continual learning (van de Ven and Tolias
2019): in task-incremental learning, the task is known during
inference; in class-incremental learning, the network must
learn a consistent input-output distribution across tasks with
varying data distributions.
Rehearsal methods store a limited-size representation of
previous tasks and train the network on a combination of
data samples from the previous and current tasks. In the
simplest setup, a memory buffer is used to store selected
previous samples in their raw form (Lopez-Paz and Ran-
zato 2017; Aljundi et al. 2019; Prabhu, Torr, and Dokania
2020) or store exemplars of classes in the class incremental
(CIL) setting (Rebuffi et al. 2017; Chaudhry et al. 2018; Cas-
tro et al. 2018; Wu et al. 2019; Hou et al. 2019; Belouadah
and Popescu 2019). Some methods instead of using raw data
samples, store data representations of previous tasks in dif-
ferent forms, e.g., optimized artificial samples (Liu et al.
2020), distilled datasets (Wang et al. 2018; Zhao, Mopuri,
and Bilen 2021) or addressable memory structures (Deng
and Russakovsky 2022).
Buffer-based approaches raise questions of scalability
and data privacy. To address these questions, generative re-
hearsal methods are based on generative models that pro-
duce data samples similar to those present in previous tasks,
instead of just replaying them from a storage. In Deep Gen-
erative Replay (Shin et al. 2017), a Generative Adversarial
Network (GAN) is used as a model to generate data from
the previous tasks. For the same purpose, Variational Au-
toencoders (V AE) are used in (van de Ven and Tolias 2018),
Normalizing Flows in (Nguyen et al. 2018), Gaussian Mix-
ture Models in (Rostami, Kolouri, and Pilly 2019) and Dif-
fusion Models in (Cywi ´nski et al. 2024).
Regularization methods (Kirkpatrick et al. 2017; Zenke,

--- PAGE 3 ---
Poole, and Ganguli 2017; Li and Hoiem 2017) are designed
especially for the task incremental (TIL) setup. They iden-
tify neural weights with the greatest impact on task perfor-
mance and slow down their learning on further tasks through
regularization.
Architectural methods (Rusu et al. 2016; Yoon et al. 2018;
Mallya and Lazebnik 2018; Mallya, Davis, and Lazebnik
2018; Wortsman et al. 2020) are especially designed for the
TIL setup. They adjust the structure of the model for each
task.
Wołczyk et al. (2022) introduced the InterContiNet algo-
rithm. For each task, this method identifies a region in the
weight space in which any weight vector assures a good per-
formance. A weight vector proper for all tasks is taken from
the intersection of these regions. In InterContiNet, these re-
gions take the form of hyperrectangles. The training of Inter-
ContiNet in a continual learning setting is problematic since
intervals in the high-dimensional parameter space need to
be controlled. To alleviate these problems, the authors engi-
neered an elaborate training procedure. However, in a mul-
tidimensional weight space, it is difficult to optimize weight
hyperrectangles for different tasks to have a non-empty in-
tersection. Consequently, InterContinNet is limited to rela-
tively simple architectures, datasets, and CL scenarios. In
(Henning et al. 2021), the regions in the weight space take
the form of normal distributions. In the architecture pre-
sented there, a hyper-hypernetwork transforms inputs into
task-specific embeddings which a hypernetwork transforms
into those distributions in the weight space of the target net-
work. The current work develops the same emerging family
of methods that could be called regional methods. However,
here we consider regions that result from the transformation
of low-dimensional cuboids assigned to tasks by a hypernet-
work. This way, we overcome the issue of scalability and
present an architecture capable of dealing with complex ar-
chitectures, datasets and CL scenarios.
3 Method: HINT
Our proposed CL architecture, dubbed HINT, is presented
in Fig. 1, and based on the following logic. Hyperrectangu-
lar, learnable task embeddings are fed to the hypernetwork,
which produces weights for the target network, which solves
the CL task. Both networks use the interval arithmetic. The
intersection of all task interval embeddings forms a univer-
sal interval embedding, from which every embedding is suit-
able for all CL tasks. The training procedure preserves the
performance of the architecture on previous tasks. Below,
we present separate parts of this architecture. We commence
with a presentation of the target network and the principles
of interval neural networks, and then we describe the interval
hypernetwork.
3.1 Interval target network
In interval neural networks, (Dahlquist and Björck 2008,
Sec. 2.5.3, Wołczyk et al. 2022) instead of considering par-
ticular points ϑ∈RDin the parameter space, regions
Θ⊂RDare used. The hyperrectangle [θ,¯θ]is a Cartesianproduct of one-dimensional intervals
[θ,¯θ] = [θ(1),¯θ(1)]×[θ(2),¯θ(2)]×. . .×[θ(D),¯θ(D)]⊂RD,
where by θ(i)∈[θ(i),¯θ(i)]we denote the i-th element of
θ. Interval arithmetic utilizes operations on segments. By
ϕ(x;θ)we denote the target network, which is a layered
neural classifier with input xand weights θ. We assume the
weights θto be intervals, thus altogether define a hyperrect-
angle, [θ,¯θ]. Therefore, for a given input x, the network pro-
duces a hyperrectangular output [z,¯z] =ϕ(x; [θ,¯θ]),rather
than a vector. Appendix A provides details of the interval
network inner workings.
Since intervals are used instead of points, the worst-case
cross-entropy is used instead of classical ones. The worst-
case interval-based loss is defined by
ˆℓ(x, y;
θ,¯θ
) =ℓ(ˆz, y), (1)
where xis an observation, yis an one-hot encoded class of
x,ℓ(·,·)is the cross-entropy loss, and ˆzis a vector with the
i-th element defined as:
ˆz(i)=(
¯z(i),fory(i)= 0,
z(i),fory(i)= 1,
where z=ϕ(x;
θ,¯θ
). The worst-case cross-entropy, as
shown in (Wołczyk et al. 2022, Theorem 3.1), gives a strict
upper limit on cross-entropy
ˆℓ(x, y; [θ,¯θ])≥max
θ∈[¯θ,¯θ]ℓ(ϕ(x; [θ, θ]), y). (2)
For the continual learning challenge of optimizing across
the tasks 1, . . . , T , during the training on a specific task t,
the goal is to achieve the optimal worst-case cross-entropy
within the intervals [θt,¯θt]where [θt,¯θt]⊆[θt−1,¯θt−1].
Interval arithmetic facilitates non-forgetting by maintaining
the weights interval of task twithin the weights interval
trained for task t−1.
Technically, the interval classifier ϕemploys for task tthe
parameter of the network with the central point θtand an in-
terval radius εt∈RD. Therefore, the parameter region is
defined as [θt,¯θt] = [θt−εt, θt+εt].With this approach,
the network can still function as a conventional non-interval
model by using only the central weights θt, which conse-
quently generates only the central activations for each layer.
Concurrently, the activation intervals [zl,¯zl]can be com-
puted with interval arithmetic. As detailed by Gowal et al.
(2018), these processes can be efficiently implemented on
GPUs by simultaneously computing the central activations
and their corresponding radii. The interval neural network
redesigns the fundamental components of neural networks
(such as fully-connected or convolutional layers, activations,
and pooling) to accommodate interval inputs and weights.
3.2 Interval hypernetwork
In this section, we introduce a hypernetwork that transforms
hyperrectangles in a low-dimensional embedding space into
regions in the weight space of the target network. We
demonstrate the effectiveness of this joint architecture. Fi-
nally, we show that the hypernetwork may be used only

--- PAGE 4 ---
in training, while during inference only the target network
is utilized. A region in the target network weight space is
shaped for all CL tasks iteratively, i.e. new constraints are
defined for subsequent tasks. Finally, we can create a uni-
versal region of weights with a universal embedding vec-
tor. Still, we can produce weights for each task with dedi-
cated embeddings, ensuring higher performance for individ-
ual tasks, but at the expense of less universality.
HINT consists of the interval hypernetwork directly
preparing weights for the interval target network which fi-
nally performs a classification. The consecutive parts of the
HINT architecture will be described in the next few para-
graphs.
Hypernetwork Introduced in (Ha, Dai, and Le 2016), hy-
pernetworks are neural models that produce weights for a
distinct target network designed for solving a specific prob-
lem. Hypernetworks were already used in continual learn-
ing (von Oswald et al. 2019; Henning et al. 2021; Ksi ˛ a ˙zek
and Spurek 2023). In this context, they generate unique
weights for individual CL tasks.
HNET (von Oswald et al. 2019) introduces trainable em-
beddings et∈RM, one for each task t, where t∈ {1, ..., T}.
The hypernetwork, denoted as H, and parameterized by η,
outputs the weights specific to the t-th task, θt, for the target
network ϕ, as shown in the equation: H(et;η) =θt.
The function ϕ(·;θt) :X→Yrepresents a neural
network classifier with weights θtgenerated by the hy-
pernetwork H(·;η)with weights η, and assigns labels for
samples in a given task. Notably, the target network it-
self is not trained directly. Within HNET, a hypernetwork
H(·;η) :RM∋et7→θtcomputes the weights θtfor the
target network ϕbased on the dedicated task embedding et.
Consequently, each task in continual learning is represented
by a classifier function ϕ(·;θt) =ϕ 
·;H(et;η)
.
After training, a single meta-model is produced, which
furnishes weights for specific tasks. The capacity to gener-
ate distinct weights for each task allows hypernetwork-based
models to exhibit minimal catastrophic forgetting. When
learning the following task, essentially a new architecture
is created, with weights dedicated to this task. To identify
weights fitting to all tasks, we introduce HINT, which uses
a hypernetwork with interval arithmetic and, optionally, also
training rules ensuring a joint weight subregion for more CL
tasks. Basically, this scenario corresponds to a single archi-
tecture for a higher number of tasks. After training, we prop-
agate the intersection of intervals through the hypernetwork
to produce universal weights for all tasks. In such a scenario,
we do not need to memorize embeddings and hypernetworks
since one network is dedicated to all tasks.
Interval arithmetic in the embedding space. In
HNET (von Oswald et al. 2019) and HNET-based models
(Henning et al. 2021; Ksi ˛ a ˙zek and Spurek 2023) authors
use one-dimensional trainable embeddings et∈RM
for each task t, where t∈ {1, ..., T}. In HINT, we use
interval arithmetic, which results in an embedding de-
fined by its lower and upper bound for each coordinate:
[et,¯et] = [ et−εt,i, et+εt,i] = [ e(1)
t−ε(1)
t,i, e(1)
t+ε(1)
t,i]×. . .×[e(M)
t−ε(M)
t,i, e(M)
t+ε(M)
t,i]⊂RM,where
εt= [ε(1)
t,i, ..., ε(M)
t,i]is a perturbation vector during the
i–th iteration of training the t–th task ( i∈ {1, ..., n})
satisfying the condition σ(ϵt) = 1 ,where σ(·)is the soft-
max function. It is worth noting that such a normalization
technique is applied to perturbated vectors before passing
through the hypernetwork. Consequently, etdenotes the
center of the embedding for the t–th task. The presented
condition ensures that intervals do not collapse to zero
widths in training. Perturbation vector coordinates values
are trainable when we create specific weights for each task
or are strictly given in cases where we define a joint weight
subregion for all tasks. Nevertheless, its values must be
non-negative, i.e., εj
t,i≥0forj∈ {1, ..., M }.
Thanks to using interval-based embeddings, we can select
an embedding subspace to create regions dedicated to more
CL tasks, see Fig. 1. In HINT, interval embeddings are trans-
formed by the hypernetwork into hyperrectangles of weights
of the target model. In this scenario, the hypernetwork prop-
agates segments instead of points. To achieve this, in the hy-
pernetwork model, we use an architecture based on Interval
Bound Propagation (IBP) (Gowal et al. 2018).
In our proposed HINT, we use a hypernetwork whose
weights ηare vectors, but we propagate an interval input
[et,¯et]producing an interval output, i.e.,
[θt,¯θt] =H([et,¯et];η).
The inner workings of the hypernetwork are based on inter-
val arithmetic. We elaborate more on this in Appendix B.
The hypernetwork His trained using the procedure de-
scribed below. It is necessary to ensure that in learning a
given task, HINT does not forget the previously learned
knowledge. Outputs from task-conditioned hypernetworks
are generated based on the task embedding. To prevent the
hypernetwork from forgetting previous tasks, we regular-
ize its training to produce the same target network weights
for previous task embeddings. In a training of task Twith
HINT, the regularization loss is specified as:
Loutput (η) =1
3 (T−1)T−1X
t=1X
µ∈{¯et,¯et+¯et
2,¯et}∥H(µ;ηT−1)− H(µ;η)∥2,
where ηT−1are the hypernetwork weights trained for task
T−1. The second component of the nested sum above
corresponds to the regularization of the embedding center,
i.e.et= (¯et+ ¯et)/2. The motivation for using this reg-
ularization formula is its ability to effectively preserve the
knowledge acquired from previous tasks by controlling the
interval lengths produced by the hypernetwork. Specifically,
when the product of the hypernetwork’s weights is small,
the resulting intervals tend to be short. This outcome is a
direct consequence of the Lipschitz continuity of MLP net-
works, which we use exclusively as hypernetworks. This
made additional regularization redundant. For further vali-
dation, please refer to Appendix H.2.
The proposition and the proof of the Lipschitz continu-
ity of MLP networks is detailed in Appendix D. Conse-
quently, regularizing the endpoints of the intervals alone is

--- PAGE 5 ---
sufficient to maintain the knowledge from previous tasks.
This conclusion is further supported by the fact that the pro-
posed regularization imposes a non-increasing constraint on
the interval. As a result, it is unnecessary to consider other
points within the interval [et,¯et]for regularization purposes.
Nonetheless, we also apply additional regularization to the
center of the interval, as this approach has been observed to
yield slightly better results. When the task identity is given
in the inference stage, then one can just use
Loutput (η) =1
T−1T−1X
t=1∥H(et;ηT−1)− H(et;η)∥2,
as we do not have to regularize the hypernetwork beyond the
middle of the interval. The ultimate cost function is a sum of
a component Lcurrent , defined by the current task data, and
output regularization Loutput , i.e.,
L=Lcurrent +β· Loutput , (3)
where βrepresents a hyperparameter managing the intensity
of regularization. For an input-output data pair, (x, y), the
current loss is defined as the standard cross-entropy com-
bined with the worst-case cross-entropy, i.e.,
ℓcurrent =κ·ℓzL+ ¯zL
2, y
+ (1−κ)·ℓ(ˆzL, y),
where κis a hyperparameter scheduled during training help-
ing to control the proper classification of samples. Lcurrent
in (3) is the average of ℓcurrent over the current task data
Dt.
Both components of Lare essential because HINT in-
cludes two networks, and it is imperative to mitigate drastic
changes in the hypernetwork output weights after learning
of subsequent CL tasks while being able to gain new knowl-
edge. The pseudocode of HINT is presented in Appendix C.
Hypernetwork as a meta-trainer HINT consists of inter-
val embeddings propagated through the IBP-based hypernet-
work and the target network. During the training of CL tasks,
we ensure that consecutive embedding intervals have a non-
empty intersections with the previous ones. A common part
of these embeddings may be used as a universal embedding
and applied for solving all CL tasks, see Fig. 2. Therefore,
we can propagate it through the IBP-based hypernetwork,
getting a single target network to classify samples from all
tasks. In such a way, the hypernetwork does not have to be
used in inference and thus is considered as a meta-trainer .
Finally, one set of weights can be utilized without storing
the previous ones.
In order to achieve embedding hyperrectangles overlap-
ping for different tasks, instead of training them directly,
we generated them from trained pre-embeddings ,at∈RM,
with the following formulae
et= (γ/M) cos( at),
et=et−γ·σ(ϵt),
¯et=et+γ·σ(ϵt),(4)
where γis a perturbation hyperparameter and Mis a natural
number representing the embedding space dimensionality,
andσ(·)is the softmax function.At the end of the training, we have a sequence
([e1,¯e1], ...,[eT,¯eT])of interval embeddings dedicated to
consecutive CL tasks. The above procedure forces such seg-
ments to have non-empty intersections as it is shown in
Lemma 3.1. Therefore, we can define a universal embed-
ding as [e,¯e] =T
t[et,¯et]. As mentioned above, when ϵ∗is
trainable, finding a universal embedding is not guaranteed.
Lemma 3.1. Let(e1, e2, . . . , e T)be embedding centers, T
be the number of CL tasks, γ >0be a perturbation value,
H(·;η)be a hypernetwork with weights η, and Mbe a nat-
ural number representing the dimensionality of the embed-
ding space, et,¯etbe calculated according to (4)withϵt≡ϵ∗
being a vector of ones, where t∈ {1,2, . . . , T }. Then
[e,¯e] =T\
t=1[et,¯et]
has a non-empty intersection.
The proof of Lemma 3.1 is presented in Appendix E. Ele-
ments from the intersection allow one to solve multiple tasks
simultaneously. Then, we can use the center of the universal
embedding and the trained hypernetwork to produce a single
target network. In the evaluation, it is sufficient to use such
prepared target weights, and we no longer need to store the
hypernetwork and trained interval embeddings.
Guarantees of non-forgetting Below, we specify the con-
ditions of our proposed architecture to be non-forgetting.
When the intersection of embedding intervals is non-empty,
and the regularization for the hypernetwork training is effec-
tive, then we achieve non-forgetting, as the theorem below
specifies.
Theorem 3.2. Let(e1, . . . , e T)be embedding centers with
corresponding perturbation vectors (ϵ1, . . . , ϵ T),Tbe the
number of CL tasks, Dt= (Xt, Yt)be a pair of observa-
tions Xtand their corresponding one-hot encoded classes
Yttaken from the t-th task, H(·;ηT)be a hypernetwork with
weights ηTobtained at the end of training the T-th task. Let
alsoϕ(·;H([et,¯et];ηT))be a target network with interval
weights produced by the hypernetwork such that for every
ϵ >0,t∈ {1,2, . . . , T },et,ϵt, and x∈Xt, there exists
y∈Ytsuch that
sup
µ∈[¯et,¯et]∥y−ϕ(x,H(µ;ηT))∥2≤ϵ.
Assume also thatTT
t=1[¯et,¯et]is non-empty and let us in-
troduce
At={µ|∀ϵ>0∀x∈Xt∃y∈Yt
sup
µ∈[¯et,¯et]∥y−ϕ(x,H(µ;ηT))∥2≤ϵ},
t∈ {1,2, . . . , T },
A={µ|∀ϵ>0∀x∈Xt∃y∈Yt
sup
µ∈TT
t=1[¯et,¯et]∥y−ϕ(x,H(µ;ηT))∥2≤ϵ}.
Then, we have guarantees of non-forgetting within the
region [e,¯e] =TT
t=1[et,¯et], i.e. A⊂At, for each t∈
{1,2, . . . , T }.
The proof of the above theorem is in Appendix E.

--- PAGE 6 ---
Figure 2: Embedding intervals for Split CIFAR-100, 5 tasks
with 20 classes each, using the cos (·)nesting method. The
ten first embedding coordinates are shown.
Table 1: Average accuracy with a standard deviation of dif-
ferent continual learning methods in the TIL setup. Results
for different methods than HINT are derived from other pa-
pers.
Method Permuted Split Split Tiny
MNIST MNIST CIFAR-100 ImageNet
HAT 97.67|0.02 − 72.06|0.50 −
GPM 94.96|0.07 − 73.18|0.52 67 .39|0.47
PackNet 96.37|0.04 − 72.39|0.37 55 .46|1.22
SupSup 96.31|0.09 − 75.47|0.30 59 .60|1.05
La-MaML − − 71.37|0.67 66 .99|1.65
FS-DGPM − − 74.33|0.31 70 .41|1.30
WSN, 30% 96 .41|0.07 − 75.98|0.6870.92|1.37
WSN, 50% 96 .24|0.11 − 76.38|0.34 69 .06|0.82
EWC 95.96|0.06 99 .12|0.11 72 .77|0.45 −
SI 94.75|0.14 99 .09|0.15 − −
DGR 97.51|0.01 99 .61|0.02 − −
HNET 97.57|0.0299.79|0.01 − −
HINT 97.78|0.0999.75|0.0877.46|1.3466.10|0.62
4 Experiments
In this section, we provide an overview of the results of our
method under different training assumptions. We cover a va-
riety of incremental learning setups to ensure a broad anal-
ysis of the interval arithmetic approach in CL. Moreover,
we show the best results obtained on each dataset using our
training method.
Training setup We apply three typical CL training setups:
TIL, in which the task identity of test samples is known, Do-
main Incremental Learning (DIL), and CIL. In the last two
setups, the task identity during inference is unknown. When
we consider HINT in TIL, we do not use any nesting ap-
proaches, and the input embedding intervals can be of dif-
ferent lengths. In CIL we use entropy to determine task ID
during the test phase, and in DIL, we use the cos(·)nestingTable 2: CL in the CIL setup. The task identity results for
entropy approach Last – last task accuracy (max), Avg. –
average task accuracy (max). The best results are indicated
with bold . If standard deviations are provided, the results
represent the average of 5 runs.
Method Permuted Split Split CIFAR-100
MNIST MNISTLast Avg.
HNET+ENT 91.75|0.21 69 .48|0.80 - -
EWC 33.88|0.49 19 .96|0.07 - -
SI 29.31|0.62 19 .99|0.06 - -
DGR 96.38|0.03 91 .79|0.32 - -
Euclidean-NCM - - 30.6 50 .0
FeTrIL - - 46.2 61 .3
FeCAM - - 48.1 62 .3
DS-AL - - - 68.39|0.16
EFC - - - 65.97|1.19
HINT 94.60|1.11 55 .38|4.89 44 .58 43 .45|0.97
Table 3: Average test accuracy with a standard deviation of
the InterContiNet and HINT methods for TIL, DIL, and CIL
scenarios. Results for the DIL and CIL scenario using HINT
are calculated with the universal embedding and entropy
method, respectively. Results for InterContiNet are derived
from (Wołczyk et al. 2022). The standard deviations for Split
MNIST and Split CIFAR-10 are calculated over 5 runs. The
standard deviation for Split CIFAR-100 in TIL setup is cal-
culated over 3 runs. The best results are indicated with bold .
Method Split MNIST Split CIFAR-10 Split CIFAR-100
TILInterContiNet 98.93|0.05 72 .64|1.18 42 .0|0.2
HINT 99.75|0.08 90 .91|0.95 79 .23|0.36
CILInterContiNet 40.73|3.26 19 .07|0.15 −
HINT 55.37|4.24 24 .19±1.12 −
DILInterContiNet 77.77|1.24 69 .48|1.36 −
HINT 79.29|3.97 74 .01|1.36 14.22
Table 4: Average test accuracy with a standard deviation of
the InterContiNet and HINT methods, when the task identity
is known or unknown, respectively. Results for InterCon-
tiNet are derived from (Wołczyk et al. 2022). The standard
deviations for Split MNIST and Split CIFAR-10 are calcu-
lated over 5 seeds.The standard deviation for CIFAR-100 is
calculated over 3 seeds.
Method Split MNIST Split CIFAR-10 Split CIFAR-100
TILInterContiNet 98.93|0.05 72 .64|1.18 42 |0.2
HINT 99.75|0.08 90 .91|0.95 79 .23|0.36
CILInterContiNet 40.73|3.26 19 .07|0.15 −
HINT 78.8|5.39 72 .87|2.2 −
method for the embeddings, and the input intervals are of the
same length, to prevent the embeddings from collapsing to a
trivial point. A more detailed description of these setups can

--- PAGE 7 ---
be found in Appendix G.
Datasets We conduct experiments on 5 publicly available
datasets: Permuted MNIST (von Oswald et al. 2020), Split
MNIST (von Oswald et al. 2020), Split CIFAR-10 (Wołczyk
et al. 2022), Split CIFAR-100 (Goswami et al. 2024) and
TinyImageNet (Goswami et al. 2024). We encourage the
reader to proceed to the supplementary materials for the de-
tails about the task division.
Architectures and baselines As target networks for Per-
muted MNIST and Split MNIST, we use two-layered MLPs
with 1000 neurons per layer for the first dataset and 400 neu-
rons per layer for the second one. For Split CIFAR-100 with
5 and 10 tasks, each with equally distributed labels per task,
and for TinyImageNet, we select a convolutional network,
specifically the ResNet-18, as in (Goswami et al. 2024). To
ensure a fair comparison with the InterContiNet method, we
also train the AlexNet architecture for Split CIFAR-10 and
Split CIFAR-100 (20 tasks, each with 5 labels). We modify
the AlexNet architecture according to (Wołczyk et al. 2022).
Specifically, we add batch normalization after each convolu-
tional and fully connected layer. However, we do not include
batch normalization after the final linear layer, as it serves as
the classification layer. Due to limited GPU resources, the
number of neurons in the first two fully connected layers is
reduced by half, compared to the version of AlexNet used
in (Wołczyk et al. 2022). In all cases, the hypernetwork is
an MLP with either one or two hidden layers. Whenever a
convolutional target network is used, we apply interval re-
laxation during training. The description of this approach
can be found in the Appendix F. We compare our solution
with InterContiNet (Wołczyk et al. 2022), WSN (Kang et al.
2023), HNET (von Oswald et al. 2019), FeCAM (Goswami
et al. 2024), as well as some other strong CL approaches
mentioned in (Goswami et al. 2024). The choice of methods
used for comparison depends on the applied training setup.
Experimental results Results for the known task iden-
tity setup are presented in Table 1. Our method outperforms
its competitors on Permuted MNIST and Split CIFAR-100
datasets while reaching the second-best result on the Split
MNIST dataset. Moreover, on TinyImageNet, we obtained
stable results, meaning the second-lowest standard deviation
score. For the same training setup, the comparison between
InterContiNet and HINT is shown in Table 4. We obtained
better results on all three datasets used, specifically show-
ing the advantage of HINT in case of training convolutional
networks. In this table, results for Split CIFAR-100 are ob-
tained and compared using the InterContiNet setup: 20 tasks
with 5 classes each.
Results for the unknown task identity setup are pre-
sented in Tables 2 and 4. Despite the large standard de-
viation on Split MNIST, we obtain one universal embed-
ding which solves all tasks at once, as we do for Permuted
MNIST. On Split CIFAR-100 we obtain the best maximum
last task accuracy. Moreover, compared to other methods,
HINT achieves a smaller deviation between the last task and
average task accuracy, showing consistency in consecutive
task results. Unfortunately, the universal embedding foundby HINT performs poorly on the Split CIFAR-100 dataset,
achieving only approximately 15% accuracy. We argue that
this is due to the larger number of classes per task, which
makes it challenging to identify a single universal embed-
ding capable of solving such tasks simultaneously. As in the
previous setup, HINT method outperforms InterContiNet on
all datasets by a large margin, which is shown in Table 4. In
Figure 2 we present the interval embeddings for each of 5
tasks of Split CIFAR-100. It is shown that a non-empty uni-
versal embedding exists, moreover, it does not collapse to a
point in the embedding space.
In Appendix H we present a study focused on interval
lengths, interval nesting and regularization. In Appendix I
we present more detailed insight of the above experimental
results.
5 Conclusions and limitations
In this paper, we introduce HINT, a continual learning archi-
tecture that employs interval arithmetic in the trained neural
model and a hypernetwork producing its weights. HINT uses
interval embeddings for consecutive tasks and trains the hy-
pernetwork to transform these embeddings into weights of
the target network. The proposed mechanism allows us to
train interval networks on large datasets in continual learn-
ing scenarios, in the TIL, CIL, and DIL setups. HINT en-
ables the generation of a universal embedding thanks to in-
terval arithmetic and hypernetwork training. The intersec-
tion of intervals, i.e. a universal embedding, can solve all
tasks simultaneously. In such a scenario, the hypernetwork
functions solely as a meta-trainer , meaning that we main-
tain only a single set of weights generated by the hypernet-
work through the universal embedding. This approach sig-
nificantly reduces memory usage. Furthermore, we provide
formal guarantees of non-forgetting.
Limitations Non-forgetting guarantees within the interval
parameter space generated by the hypernetwork are valid
only as long as the hypernetwork’s regularization term re-
mains effective. Second, we have observed that achiev-
ing satisfactory performance becomes challenging when the
number of classes in a given task is large. Splitting such a
task into subtasks may be advantageous and subsequently
finding a universal embedding, but we leave this for future
work.
A Interval arithmetic in neural networks
Suppose A, B⊂Rrepresent intervals. For all ¯a, a,¯b, b∈R,
where A= [a,¯a]andB= [b,¯b], arithmetic operations can
be defined as in (Lee 2004):
• addition: [a,¯a] + [b,¯b] = [a+b,¯a+¯b]
• multiplication: [a,¯a]∗[b,¯b] = [min( a∗b, a∗¯b,¯a∗b,¯a∗
¯b),max( a∗b, a∗¯b,¯a∗b,¯a∗¯b)]
Therefore, interval arithmetic is capable of executing affine
transformations, enabling the implementation of both fully-
connected and convolutional layers in neural networks.
We denote by ϕ(x;θ)a layered neural classifier with input

--- PAGE 8 ---
xand weights θ. It comprises a series of transformations
ϕ(x;θ) = (hL◦hL−1◦. . . h 1) (x;θ) (5)
=hL(hL−1(. . . h 1(x))). (6)
All component transformations, hl, are also based on the
weights θ, which we omit in the notation. The final output
ϕ(x;θ) =zL∈RNis to indicate one of Nclasses to which
the input xbelongs.
In InterContiNet (Wołczyk et al. 2022), weight matri-
ces and bias vectors of each layer hlare located in hy-
perrectangles [Wl,¯Wl]and[bl,¯bl], respectively. We de-
note[θ,¯θ] =⟨[W1,¯W1],[b1,¯b1], . . . , [WL,¯WL],[bL,¯bL]⟩
to represent intervals of all the trainable parameters in the
network.
The transformation in l-th layer, l= 1, . . . , L , of Inter-
ContiNet can be expressed as:
[zl,¯zl] =hl([zl−1,¯zl−1]) (7)
where z0= [x, x]is the input, [zl,¯zl]is the hyperrectangle
ofl-th layer activations,
hl([zl−1,¯zl−1]) = [ hl,¯hl]([zl−1,¯zl−1]) (8)
=ψ([Wl,¯Wl][zl−1,¯zl−1] + [bl,¯bl]),(9)
withψbeing an activation function with positive output, and
zl= min
zl−1≤zl−1≤zl−1hl(zl−1), (10)
zl= max
zl−1≤zl−1≤zl−1hl(zl−1). (11)
Monotonicity of ψ(e.g., ReLU, logistic sigmoid) is enough
for efficient computation of [zl,¯zl].
The input z0= [x, x]is assumed to be nonneg-
ative, x≥0. The final output ϕ(z0; [θ1,¯θL]) =
hL(hL−1(. . . h 1(z0))) = [ zL,¯zL]comprises Nintervals,
one for each class.
B Layered hypernetwork with interval
embedding input
Let the hypernetwork H([et,¯et];η)comprise a se-
ries of transformations [h1,¯h1](·), . . . , [hL,¯hL](·)
across its L layers. The final output
[hL,¯hL] 
[hL−1,¯hL−1] 
. . .[h1,¯h1]([x, x])
= [zL,¯zL]
produces interval weights for the target model. This result
represents a composition of weight intervals of consecutive
layers.
In HINT, we propagate the interval embedding [z0,¯z0] =
[e,¯e]from the input to the output layer. The output bound-
ing box [zl,¯zl]from the hypernetwork H([et,¯et];η)is cal-
culated in the following way:
µl−1=zl−1+zl−1
2, r l−1=zl−1−zl−1
2,
µl=Wlµl−1+bl, rl=|Wl|rl−1,
zl=ψ(µl−rl),zl=ψ(µl+rl),
where Wlandblarel-th layer weight matrix and bias vector,
respectively, η=⟨W1, b1, W2, b2. . . , W L, bL⟩, and|·|is an
element-wise absolute value operator.
Therefore, our interval hypernetwork H([et,¯et];η) =
[θt,¯θt]transforms the embedding of the t-th task into in-
terval weights of the target network.HINT In HINT, trainable interval embeddings [et,¯et]are
used for producing separated network weights for each con-
tinuous learning task t,t∈ {1, ..., T}. These embeddings
are propagated through the trainable IBP-based hypernet-
work
H([et,¯et];η) = [θt,¯θt]
into interval weights of the target model. To derive the equa-
tions for linear layers, we introduce the following terms:
¯W+
k−1= max {¯Wk−1,0},¯W−
k−1= max {−¯Wk−1,0},
¯W+
k−1= max {¯Wk−1,0}, ¯W−
k−1= max {−¯Wk−1,0},
where k ∈ { 2, . . . , L },[θt,¯θt] =
⟨
¯W1,¯W1
,
¯b1,¯b1
, . . . ,
¯WL,¯WL
,
¯bL,¯bL
⟩,min{·,·}
andmax{·,·}denote element-wise minimum and max-
imum operations. Then, one need to calculate such an
expression:
[zk,zk] =ψ 
¯Wk−1,¯Wk−1
·
zk−1,zk−1
+
bk−1,bk−1
.
Based on the definition of multiplying two intervals, we
obtain that:

¯Wk−1,¯Wk−1
·
zk−1,zk−1
=
¯W+
k−1−¯W−
k−1,¯W+
k−1−¯W−
k−1
·
zk−1,zk−1
= 
¯W+
k−1,¯W+
k−1
−¯W−
k−1,¯W−
k−1
·
zk−1,zk−1
=
¯W+
k−1,¯W+
k−1
·
zk−1,zk−1
−¯W−
k−1,¯W−
k−1
·
zk−1,zk−1
.
Generally, distributivity does not hold for interval arith-
metic, which means that ([a, b] + [c, d])·[e, f]does not
equal [a+c, b+d]·[e, f]for some a, b, c, d, e, f ∈R
in general. However, this property holds when the mul-
tiplication of intervals [a, b]·[c, d]results in an inter-
val with non-negative endpoints. This is why the decom-
position of lower and upper matrices into the subtrac-
tion of positive and negative matrices is useful. Using
the definition of interval multiplication and the fact that
¯W+
k−1,¯W+
k−1, W−
k−1,¯W−
k−1, zk−1,zk−1≥0, we have:

¯W+
k−1,¯W+
k−1
·
zk−1,zk−1
=
min
a∈{¯W+
k−1,¯W+
k−1}
b∈{zk−1,zk−1}{a·b}, max
a∈{¯W+
k−1,¯W+
k−1}
b∈{zk−1,zk−1}{a·b}

=
¯W+
k−1·zk−1,¯W+
k−1·zk−1
.
Analogously,
¯W−
k−1,¯W−
k−1
·
zk−1,zk−1
=
min
a∈{¯W−
k−1,¯W−
k−1}
b∈{zk−1,zk−1}{a·b}, max
a∈{¯W−
k−1,¯W−
k−1}
b∈{zk−1,zk−1}{a·b}

=¯W−
k−1·zk−1,¯W−
k−1·zk−1
=−
−¯W−
k−1·zk−1,−¯W−
k−1·zk−1
,

--- PAGE 9 ---
which gives us the final form of the equations:
zk=ψ 
¯W+
k−1·zk−1−¯W−
k−1·zk−1+bk−1
,
zk=ψ ¯W+
k−1·zk−1−¯W−
k−1·zk−1+bk−1
.
Since the equations above are derived for propagation
through fully connected layers, a similar version for convo-
lutional layers can be easily obtained, as a convolution op-
eration can be expressed as a multiplication operation. The
final form of these equations is as follows:
zk=ψ 
¯W+
k−1∗zk−1−¯W−
k−1∗zk−1+bk−1
,
zk=ψ ¯W+
k−1∗zk−1−¯W−
k−1∗zk−1+bk−1
,
where the ∗operator denotes the convolution operation. Un-
fortunately, such equations cannot be easily obtained for
batch normalization layers.
C The HINT algorithm
HINT is presented below as Algorithm 1.
D Lipschitz regularity of an interval-based
hypernetwork
Proposition 1. Let(e1, e2, . . . , e T)be embedding centers,
Tbe the number of CL tasks, γ > 0be a perturbation
value,H(·;η)be an MLP-based hypernetwork with weights
η=⟨η1, b1, . . . , η L, bL⟩and an activation function, ψ(·),
is Lipschitz continuous with a real positive constant K,M
be a natural number representing the dimensionality of the
embedding space, et,¯etbe calculated according to Sec-
tion 3.2 (4)withϵt≡ϵ∗being a vector of ones, where
t∈ {1,2, . . . , T },Lbe a number of the hypernetwork’s lay-
ers. Then, for each t
sup
µ,ξ∈[¯et,¯et]∥H(µ;η)− H(ξ;η)∥2≤KLLY
i=1∥ηi∥2∥¯et−¯et∥2.
Proof of Proposition 1. From Proposition 1, (Virmaux and
Scaman 2018), and the fact that the hypernetwork is an
MLP-based neural network with a Lipschitz continuous ac-
tivation function, for any µ, ξ∈[et,¯et],t∈ {1, . . . , T }, we
have:
∥H(µ;η)− H(ξ;η)∥2≤KLLY
i=1∥ηi∥2· ∥µ−ξ∥2.
It now suffices to take the supremum over µ, ξ∈[et,¯et]on
both sides of the above inequality:
sup
µ,ξ∈[¯et,¯et]∥H(µ;η)− H(ξ;η)∥2≤KLLY
i=1∥ηi∥2∥et−¯et∥2,
where
sup
µ,ξ∈[¯et,¯et]∥µ−ξ∥2=∥et−¯et∥2.
A similar result can be proven for convolutional neural
networks. For more details, please see (Virmaux and Scaman
2018).Algorithm 1: The pseudocode of HINT.
Require: hypernetwork Hwith weights η, target net-
work ϕ, softmax function σ(·), perturbation value γ >
0, regularization strength β > 0, cross-entropy com-
ponents strength κ∈(0,1), dimensionality of the
embedding space M,ntraining iterations, datasets
{D1, D2, ..., D T},(xi,t, yi,t)∈Dt, t∈ {1, . . . , T },
i∈ {1, . . . , N t},Ntis a number of samples in the dataset
Dt.
Ensure: updated hypernetwork Hweights η
Initialize randomly weights ηwith pre-embeddings
(a1, a2, ..., a T)and corresponding perturbation vectors
(ϵ∗
1, ϵ∗
2, . . . , ϵ∗
T)
fort←1toTdo
ift >1then
η∗←η
fort′←1tot−1do
ϵ∗
t′←γ·σ(ϵ∗
t′)
Store [θ∗
t′,¯θ∗
t′]←
← H([γ
Mcos (at′)−ϵ∗
t′,γ
Mcos (at′)+ϵ∗
t′];η∗)
end for
end if
fori←1tondo
ifi≤n
2
then
ˆϵ∗
i←i
⌊n
2⌋·γ
else
ˆϵ∗
i←γ
end if
ϵ∗
t←ˆϵ∗
i·σ(ϵ∗
t)
[θt,¯θt]←
← H([γ
Mcos (at)−ϵ∗
t,γ
Mcos (at) +ϵ∗
t];η)
[ˆyi,t,¯ˆyi,t]←ϕ(xi,t; [θt,¯θt])
ift= 1then
L ← L current
else
L ← L current +β· Loutput
end if
Update η,atandϵt
end for
Freeze and store atandϵt
end for
E Proof of non-forgetting in HINT
Proof of Lemma 3.1. Take arbitrary t1, t2∈ {1,2, . . . , T }
andi-th coordinate of the embeddings et1, et2, where i∈
{1,2, . . . , M }. Suppose the worst-case scenario, where
cos
e(i)
t1
= 1,
cos
e(i)
t2
=−1.

--- PAGE 10 ---
Then the absolute difference betweenγ
Mcos
e(i)
t1
and
γ
Mcos
e(i)
t2
will be the biggest and can be calculated as
γ
Mcos
e(i)
t1
−γ
Mcos
e(i)
t2=2γ
M
= 2γ·σ(ϵ∗)(i).
Note that the maximum distance at the i-th coordinate be-
tween transformed centers of the intervals is equal to2γ
M,
but this value is the same as 2γ·σ(ϵ∗)(i). This means that
in the worst case the intersection is a single point. As t1,t2
andiwere chosen arbitrary, the intersection
[e,¯e] =T\
t=1[et,¯et]
is non-empty, what ends the proof.
Proof of Theorem 3.2. As we know that the intersection
[e,¯e]is non-empty, we just need to show that At⊂Afor ev-
eryt∈ {1, . . . , T }. Please note that {At}T
t=1is a sequence
of non-empty, compact subsets of RN. Every compact set is
bounded, which implies that
sup T\
t=1At!
= min
t∈{1,...,T}sup (At). (12)
We can use this identity to prove that A⊂At∗, where t∗∈
{1, . . . , T }is arbitrary chosen. It is straightforward to see
that
A=(
µ∀ϵ >0,∀x∈Xt,∃y∈Yt: sup
µ∈TT
t=1[et,¯et]C(µ)≤ϵ)
=(
µ∀ϵ >0,∀x∈Xt,∃y∈Yt: min
t∈{1,...,T}sup
µ∈[et,¯et]C(µ)≤ϵ)
⊆(
µ∀ϵ >0,∀x∈Xt,∃y∈Yt:∀t∈{1,...,T}sup
µ∈[et,¯et]C(µ)≤ϵ)
=T\
t=1(
µ∀ϵ >0,∀x∈Xt,∃y∈Yt: sup
µ∈[et,¯et]C(µ)≤ϵ)
=T\
t=1At⊆At∗.
(13)
where
C(µ) =∥y−ϕ(x,H(µ;ηT))∥2. (14)
Ast∗was arbitrary chosen, we infer that for each t∈
{1, . . . , T }A⊂At, which ends the proof.
F Relaxation technique
Based on our experiments and those conducted in (Wołczyk
et al. 2022), we have observed that training convolutional
neural networks in an interval-based setup presents signif-
icant challenges. We argue that many of these difficulties
stem from the absence of a straightforward substitute for an
interval-based batch normalization layer. To address these
issues, we propose mitigating the challenges by relaxing theinterval constraints in the target network while maintaining a
fully interval-based hypernetwork. The pseudocode for this
approach is provided in Algorithm 2.
While the relaxation technique facilitates effective train-
ing of convolutional neural networks, it also has a certain
drawback, because the image of the target network obtained
through the relaxation technique is only contained within the
image of the target network trained using the fully interval-
based method. This outcome is illustrated in Proposition 2.
Proposition 2. Let(e1, e2, . . . , e T)be embedding centers,
Tbe the number of CL tasks, γ >0be a perturbation value,
H(·;η)be a hypernetwork with weights η,Mbe a natu-
ral number representing the dimensionality of the embed-
ding space, et,¯etbe calculated according to 3.2 (4)with
ϵt≡ϵ∗being a vector of ones, where t∈ {1,2, . . . , T }. Let
alsoϕ 
·;
θt,¯θt
be a target network with a non-negative
and non-decreasing activation function ψ(·)and with inter-
val weights generated by the hypernetwork. Then, for each
t∈ {1, . . . , T }and non-negative observation x:
[ϕmin(x), ϕmax(x)]⊆ϕ 
x;
θt,¯θt
,
where
ϕmin(x) = min {ϕ(x;θt), ϕ 
x;¯θt
},
ϕmax(x) = max {ϕ(x;θt), ϕ 
x;¯θt
}.
Proof. It is straightforward to see that:
ϕmin(x)≥min
θ∈[¯θt,¯θt]ϕ(x;θ),
ϕmax(x)≤max
θ∈[¯θt,¯θt]ϕ(x;θ).
This ends the proof, because"
min
θ∈[¯θt,¯θt]ϕ(x;θ),max
θ∈[¯θt,¯θt]ϕ(x;θ)#
=ϕ 
x;
θt,¯θt
.
One might ask whether equality can occur in Proposition
2. The answer is yes, and this is demonstrated in Proposition
3.
Proposition 3. Let us assume that all the assumptions of
Proposition 2 are satisfied and that the notation is preserved.
Additionally, assume that the interval weights [θt,¯θt] =
⟨[W(t)
1,¯W(t)
1],[b(t)
1,¯b(t)
1], . . . , [W(t)
L,¯W(t)
L],[b(t)
L,¯b(t)
L]⟩pro-
duced by the hypernetwork are non-negative, where Lde-
notes the number of layers in the target network. Then, for
eacht∈ {1, . . . , T }and non-negative observation x:
[ϕmin(x), ϕmax(x)] =ϕ 
x;
θt,¯θt
.
Proof. Fix arbitrary t∈ {1, . . . , T }and let us introduce the
following notation for any k∈ {1, . . . , L −1}:
¯z(t)
k=ψ
¯W(t)
k·¯z(t)
k−1+¯b(t)
k
,
z(t)
k=ψ
W(t)
k·z(t)
k−1+b(t)
k
,
z(t)
0=x,
¯z(t)
0=x.

--- PAGE 11 ---
Based on the assumptions, we know that the weights gener-
ated by the hypernetwork are non-negative and that ψ(·)is
a non-negative valued and non-decreasing activation func-
tion. This implies that for any α(t)
k,¯α(t)
k∈h
W(t)
k,¯W(t)
ki
,
β(t)
k,¯β(t)
k∈h
b(t)
k,¯b(t)
ki
such that α(t)
k≤¯α(t)
k,β(t)
k≤¯β(t)
k,
k∈1, . . . , L :
α(t)
k·z(t)
k−1+β(t)
k≤¯α(t)
k·¯z(t)
k−1+¯β(t)
k,
ψ
α(t)
k·z(t)
k−1+β(t)
k
≤ψ
¯α(t)
k·¯z(t)
k−1+¯β(t)
k
.
Hence,
W(t)
L·z(t)
L−1+b(t)
L=ϕmin(x),
W(t)
k·z(t)
k−1+b(t)
k= min
a∈{¯W(t)
k,¯W(t)
k}
b∈{¯z(t)
k−1,¯z(t)
k−1}{a·b+b(t)
k}.
Analogously,
¯W(t)
L·¯z(t)
L−1+¯b(t)
L=ϕmax(x),
¯W(t)
k·¯z(t)
k−1+¯b(t)
k= max
a∈{¯W(t)
k,¯W(t)
k}
b∈{¯z(t)
k−1,¯z(t)
k−1}{a·b+¯b(t)
k}.
Then,
[ϕmin(x), ϕmax(x)] =h
W(t)
L,¯W(t)
Li
·h
z(t)
L−1,¯z(t)
L−1i
+h
b(t)
L,¯b(t)
Li
=ϕ 
x,
θt,¯θt
,
what ends the proof.
G Training details
Datasets and CL setup We use the following datasets:
1) Permuted MNIST-10 (von Oswald et al. 2020), consist-
ing of 28x28 pixel grey-scale images of 10 classes of dig-
its, where each task is obtained by applying a random per-
mutation to the input image pixels, with a typical length of
T = 10 tasks; 2) Split MNIST (von Oswald et al. 2020), con-
taining tasks designed by sequentially pairing the digits to
introduce task overlap, forming T = 5 binary classification
tasks; 3) Split CIFAR-100 (Goswami et al. 2024), consisting
of 32×32 pixel color images of 100 classes; 4) Split CIFAR-
10 (Wołczyk et al. 2022), with T = 5 binary classification
tasks; 5) TinyImageNet (Goswami et al. 2024), a subset of
ImageNet (Russakovsky et al. 2015), consisting of 64×64
pixel color images of 200 classes; 6) Permuted MNIST-100
(Goswami et al. 2024), similar to the Permuted MNIST-10
dataset, but with T = 100 tasks, 10 classes each, only used in
the ablation study. When the task identity is unknown during
inference, for Split CIFAR-100 we experiment with a setup
of 5 tasks with 20 classes each, we do not conduct experi-
ments on TinyImageNet in this setup. When the task identity
is known, we pick a setup with 10 tasks with 10 classes each
and 20 tasks with 5 classes each for Split CIFAR-100 and 40
tasks with 5 classes each for TinyImageNet.Algorithm 2: The pseudocode of HINT relaxation technique.
Require: hypernetwork Hwith weights η, target net-
work ϕ, softmax function σ(·), perturbation value γ >
0, regularization strength β > 0, cross-entropy com-
ponents strength κ∈(0,1), dimensionality of the
embedding space M,ntraining iterations, datasets
{D1, D2, ..., D T},(xi,t, yi,t)∈Dt, t∈ {1, . . . , T },
i∈ {1, . . . , N t},Ntis a number of samples in the dataset
Dt.
Ensure: updated hypernetwork Hweights η
Initialize randomly weights ηwith pre-embeddings
(a1, a2, ..., a T)and corresponding perturbation vectors
(ϵ∗
1, ϵ∗
2, . . . , ϵ∗
T)
fort←1toTdo
ift >1then
η∗←η
fort′←1tot−1do
ϵ∗
t′←γ·σ(ϵ∗
t′)
Store [θ∗
t′,¯θ∗
t′]←
← H([γ
Mcos (at′)−ϵ∗
t′,γ
Mcos (at′)+ϵ∗
t′];η∗)
end for
end if
fori←1tondo
ifi≤n
2
then
ˆϵ∗
i←i
⌊n
2⌋·γ
else
ˆϵ∗
i←γ
end if
ϵ∗
t←ˆϵ∗
i·σ(ϵ∗
t)
[θt,¯θt]←
← H([γ
Mcos (at)−ϵ∗
t,γ
Mcos (at) +ϵ∗
t];η)
ˆyi,t←ϕ(xi,t;θt)
¯ˆyi,t←ϕ(xi,t;¯θt)h
ˆyi,t,¯ˆyi,ti
←h
min{ˆyi,t,¯ˆyi,t},max{ˆyi,t,¯ˆyi,t}i
ift= 1then
L ← L current
else
L ← L current +β· Loutput
end if
Update η,atandϵt
end for
Freeze and store atandϵt
end for
Training setup We apply three typical CL training setups:
TIL, DIL, and CIL. When considering HINT in the TIL sce-
nario, we do not use any nesting approaches, and the input
embedding intervals may have different lengths. Similarly,
in the CIL setup, we also do not use nesting approaches;
however, we employ entropy to infer the task identity. In the
DIL scenario, we use the cos(·)embedding nesting method
and the input intervals are of the same length, to prevent the
embeddings from collapsing to a trivial point.
Relaxation technique Depending on the complexity of
the target network architecture, we have two interval learn-

--- PAGE 12 ---
ing approaches: 1) fully interval technique, meaning that
we apply intervals to both the hypernetwork and target net-
work; 2) interval relaxation technique, meaning we use a
fully interval hypernetwork and a non-interval target net-
work, where we only restore the order of predictions after
its last layer, hence we can also use vanilla batch normal-
ization. Regardless of the training setup (known / unknown
task identity): 1) for target networks which are MLPs, we
always use the fully interval training technique; 2) in cases
when a convolutional target network is used (ResNet-18 and
AlexNet), we apply the interval relaxation technique to the
training process, since convolutional layers are generally
more difficult to train with intervals (Wołczyk et al. 2022).
Form of hypernetwork regularization loss The interval
weights produced by the hypernetwork are contained within
the interval

min
µ∈[¯et,¯et]H(µ;η),max
µ∈[¯et,¯et]H(µ;η)
.
As previously mentioned, the interval weights produced by
the hypernetwork tend to be short, and regularization of the
interval endpoints is typically sufficient. However, it is worth
considering what would happen if regularization were ap-
plied to points within the interval that are not endpoints.
We examined three types of regularization: (1) regulariza-
tion applied only to the endpoints of the interval, (2) regular-
ization applied to both the endpoints and the midpoint of the
interval, and (3) regularization applied to the endpoints and
randomly selected points from within the interval. We ob-
served that the second and third approaches yielded slightly
better results than the first. Although there was no signif-
icant difference between the second and third approaches,
we opted to use the second approach.
Common hyperparameters In each training process, we
use the ReLU activation function and apply the Adam opti-
mizer with different learning rates but the same default betas
coefficients, i.e., β1= 0.9,β2= 0.999. Also, anytime we
specify that a learning rate scheduler is used, we apply the
ReduceLROnPlateau scheduler from PyTorch. There is a pa-
rameter κwhich is responsible for weighing the worst case
loss wrt. to the basic entropy loss, as it takes part in their con-
vex linear combination. We always use κ= 0.5. However,
this hyperparameter is scheduled during training, so instead
of using the same κvalue throughout the training, we use
κi= max {1−0.00005·i, κ}, where iis the current training
iteration. In other words, it means that at the beginning of the
training we put more attention to the proper classification on
the centers of the intervals and then, gradually, we put more
attention to the worst case component. Furthermore, there is
another parameter, namely β, which is responsible for the
hypernetwork regularization strength, weighing the part of
loss responsible for non-forgetting.
Hardware and software resources used We imple-
mented HINT using Python 3.7.6, incorporating libraries
such as hypnettorch 0.0.4 (von Oswald et al., 2019), Py-
Torch 1.5.0 with CUDA support, NumPy 1.18, Pandas 1.0.1,
Matplotlib 3.1.3, seaborn 0.10.0, among others. Most of ourcomputations are conducted on an NVIDIA GeForce RTX
4090, but some training sessions are also performed using
NVIDIA GeForce RTX 3080, NVIDIA GeForce RTX 2070,
NVIDIA A100, and NVIDIA DGX graphic cards.
Known task identity hyperparameters As it was stated
in the main section of this work, here we do not use any in-
terval nesting methods. Also, in this setting, we randomly
initialize each task embedding. Let us cover the list of pa-
rameters used in this setup:
• Split MNIST – the hypernetwork and target network used
are two-layered MLPs with 75 neurons per layer and 400
neurons per layer, respectively. We use data augmenta-
tion as in (von Oswald et al. 2019), the Adam optimizer
with a learning rate lr= 0.001, no scheduler, batches of
size 128, embeddings with 72 dimensions and perturba-
tion value γ= 1. Moreover, we use β= 0.01and we
conduct the training for 2000 iterations.
• Permuted MNIST-10 and Permuted MNIST-100 – the
hypernetwork and target network used are two-layered
MLPs with 100 neurons per layer and 1000 neurons per
layer, respectively. We do not use any data augmentation
and we choose the Adam optimizer with a learning rate
lr= 0.001, no scheduler. We use batch size equal to
128, embeddings with 24 dimensions, perturbation value
γ= 0.5,β= 0.01and conduct the training for 5000
iterations.
• Split CIFAR-10 – we use a one-layered MLP with 100
neurons as the hypernetwork and AlexNet with batch
normalization as the target network. We do not apply any
data augmentation. We use the Adam optimizer with a
learning rate lr= 0.001and the lrscheduler. We use
batch size equal to 128, embeddings with 48 dimensions,
perturbation value γ= 0.5,β= 0.001and conduct the
training for 2000 iterations.
• Split CIFAR-100 (10 tasks, 10 classes each) – we use a
one-layered MLP with 100 neurons as the hypernetwork
and ResNet-18 with batch normalization as the target net-
work. We apply the data augmentation as in (Goswami
et al. 2024), use the Adam optimizer with a learning
ratelr= 0.001and the lrscheduler. We use batch size
equal to 32, embeddings with 48 dimensions, perturba-
tion value γ= 1,β= 0.01and conduct the training for
200 epochs.
• Split CIFAR-100 (20 tasks, 5 classes each) – we use a
one-layered MLP with 100 neurons as the hypernetwork
and AlexNet with batch normalization as the target net-
work. We do not apply any data augmentation, use the
Adam optimizer with a learning rate lr= 0.001 and
thelrscheduler. We use batch size equal to 32, embed-
dings with 24 dimensions, perturbation value γ= 0.1,
β= 0.01and conduct the training for 50 epochs.
• TinyImageNet – we use a one-layered MLP with 200
neurons per layer as the hypernetwork and ResNet-18
with batch normalization as the target network. We ap-
ply the data augmentation, use the Adam optimizer with
a learning rate lr= 0.0001 and the lrscheduler. We

--- PAGE 13 ---
use batch size equal to 256, embeddings with 700 di-
mensions, β= 0.1and we conduct the training for 10
epochs. For this dataset, we choose the perturbation value
γ= 0.1.
The same hyperparameters were used in the CIL setup.
Unknown task identity hyperparameters – universal em-
bedding In this setting, we use the cos(·)interval nesting
method, as it gave the best performance results. Moreover,
there is an additional parameter specifying a custom embed-
ding initialization, which means we initialize the next task’s
embedding as the previously learned one. In this setup, we
always use the custom embedding initialization. We would
like to emphasize that it is just another way of initialization,
we do not change or additionally train any of the previously
learned embeddings. Let us cover the parameters used in this
setting:
• Split MNIST – we use the same hypernetwork and target
network architectures as in the previous setup, as well as
data augmentation. We apply the Adam optimizer with a
learning rate lr= 0.001, no scheduler, batches of size 64,
embeddings with 24 dimensions and perturbation value
γ= 15 . Moreover, we use β= 0.01and conduct the
training for 2000 iterations.
• Permuted MNIST-10 and Permuted MNIST-100 – we
use the same hypernetwork and target network architec-
tures as in the previous setup with no data augmenta-
tion. We use the Adam optimizer with a learning rate
lr= 0.001, no scheduler, batches of size 128, embed-
dings with 24 dimensions and perturbation value γ= 5.
Moreover, we use β= 0.01and conduct the training for
5000 iterations.
• Split CIFAR-10 – we use the same hypernetwork and tar-
get network architectures as in the previous setup, with
no data augmentation. We use the Adam optimizer with
a learning rate lr= 0.001and the lrscheduler. We use
batch size equal to 128, embeddings with 48 dimensions,
perturbation value γ= 5,β= 0.01and conduct the
training for 2000 iterations.
• Split CIFAR-100 (5 tasks, 20 classes per each) – we use
the same hypernetwork and target network architectures
as in the previous setup, with no data augmentation. We
use the Adam optimizer with a learning rate lr= 0.001
and the lrscheduler. We use batch size equal to 32, em-
beddings with 48 dimensions, perturbation value γ= 10 ,
β= 0.01and conduct the training for 20 epochs. We use
data augmentation.
H Ablation study
H.1 Different perturbation size of intervals on
Permuted MNIST-10
In this subsection, we experiment with different perturbated
values to show that size of the perturbation impacts the abil-
ity of our method to find good solutions. As stated in the
main section of this work, the larger the perturbation value,
the larger the embedding intervals. One could argue that in-
creasing the perturbated value is similar to giving more diffi-
cult adversary examples on the input to our network. As pre-
(a) Results obtained using the nesting by the
cos (·)method.
(b) Results obtained for the known task identity setup.
Figure 3: Mean test accuracy for consecutive CL tasks aver-
aged over 2 runs of different interval size settings of HINT
for 10 tasks of Permuted MNIST-10 dataset.

--- PAGE 14 ---
viously mentioned, larger perturbated value on the input em-
bedding intervals implicates an increase in the space where
a universal embedding could be found. Hence, it could make
the process of finding an intersection of intervals easier.
We consider TIL, DIL, and nesting with the cos(·)method
when applicable. The rest of the parameters are the same as
for the best performing models. The grid search outcome is
shown in Figure 3. Considering results averaged over 2 runs,
the best parameter for the unknown task identity is γ= 25 ,
while for the known task identity is γ= 15 . These param-
eters differ from our final choice in Appendix G, since an-
other γvalues give better accuracy, based on the averaged
results of 5 different runs.
H.2 Interval lengths of target network weights
Using the interval arithmetic within a hypernetwork gives us
a possibility of generating interval target network weights.
Nevertheless, very often these solutions can become de-
graded to a point estimate in the network weight space. Since
such possibility exists, it is important to check if our hyper-
network generates such output.
(a) Results obtained for the Split CIFAR-100 dataset using the
cos (·)nesting method.
(b) Results obtained for the Split MNIST using the cos (·)nest-
ing method.
Figure 4: Histograms are calculated for the Split MNIST and
Split CIFAR-100 datasets using the MLP and ResNet-18 ar-
chitectures, respectively.
In Figure 4 we present histogram plots of lenghts of tar-
get network interval weights to analyse whether this situa-
tion occurs. We also give a specific number of coordinates
(weights) which collapse to a point for the sake of clarity.
We show one result per each type of the target network: an
MLP on the Split MNIST dataset and a convolutional net-work on the Split CIFAR-100 dataset, both with the cos(·)
nesting method. From Figure 4 we may conclude that, for
Split CIFAR-100 and Split MNIST, none of the coordinates
collapse to a trivial point in the weight space, meaning we
obtain non-trivial universal weights.
H.3 Intervals around embeddings – Permuted
MNIST-10 and Split MNIST
In this subsection, we present the acquired embedding inter-
vals on the Permuted MNIST-10 and Split MNIST datasets.
We use the DIL setting, where the task identity is unknown
and the interval vector is non-learnable (fixed). Results are
shown in Figure 5 for nesting with the cos(·)method.
(a) Results obtained for the Permuted MNIST-10 dataset using
thecos (·)nesting method.
(b) Results obtained for the Split MNIST dataset using the
cos (·)nesting method.
Figure 5: Ten first intervals around task embeddings for Per-
muted MNIST-10 and Split MNIST.
H.4 Larger number of tasks – Permuted
MNIST-100
It is a fair question to ask, how our method performs when
there is a larger number of tasks, for example 100 instead of
10. This is a more difficult setting, since the capacity of our
model to learn new tasks and remember the previous ones
is limited. In such situations, catastrophic forgetting occurs
much easier, due to the growing size of knowledge that could
be forgotten.

--- PAGE 15 ---
Figure 6: Mean test accuracy for consecutive CL tasks av-
eraged over 3 runs for 100 tasks of Permuted MNIST–100
dataset.
To answer this question, in this section, we provide an
analysis of interval arithmetic learning method performance
given the Permuted MNIST-100 dataset. As a reminder, this
dataset consists of 100 tasks comparing pairs of images of
numbers subjected to different distortions. Due to the num-
ber of tasks and the size of the dataset, we limit ourselves
to provide results for the TIL setting, with learnable interval
parameters and no nesting.
The results are presented in Figure 6. We observe that
training on consecutive tasks has a significant impact on
the accuracy on previously learned tasks. Nevertheless, our
model is still able to retain some of the previous knowledge,
not dropping to a random accuracy on any of the tasks. Fur-
thermore, we conclude that training consecutive tasks in the
latter stages is more difficult, due to the decrease in accuracy
just after training.
H.5 Ablation of the regularization method
This subsection involves experimenting with different val-
ues for the βparameter, which directly impacts the mem-
orization factor in the loss function. This part of math-
ematical expression targets specifically the hypernetwork
output. The experiment is done in two setups: 1) TIL, 2)
DIL. We conduct a grid search on the Permuted MNIST-
10 dataset and check the βparameter from a set of values:
{1.0,0.1,0.05,0.01,0.001}, while the rest of parameters are
taken from the best performing models. The grid search re-
sults are presented in Figure 7.
It is important to emphasize that the larger the βparame-
ter, the stronger the regularization of knowledge learned on
the previous tasks. This implies that we put more stress on
our model to remember previous tasks and limit its ability
to learn a new one. Based on the results averaged over two
runs, the best parameters are β= 0.1for the CIL setup and
β= 0.05for the TIL setup. However, these parameters dif-
fer from our final choice in Appendix G, since the parameter
value β= 0.01gives better accuracy scores for both setups,
based on the averaged results of 5 different runs.
(a) Results obtained in the DIL setting, using the nesting by the
cos (·)method.
(b) Results obtained in the TIL setting.
Figure 7: Mean test accuracy for consecutive CL tasks aver-
aged over 2 runs with different βhyperparameters of HINT
for 10 tasks of Permuted MNIST-10 dataset.

--- PAGE 16 ---
H.6 Different interval nesting methods
In this experimental subsection, we focus on the compar-
ison of two different approaches to interval nesting of the
input embedding to the hypernetwork. We experiment on
Permuted MNIST–10 and consider two interval mappings:
tanh(·)andcos(·). The results are shown in Figure 8. We
may conclude that intervals obtained with tanh(·)andcos(·)
behave similarly, especially on the final tasks.
Figure 8: Mean test accuracy for consecutive continual
learning (CL) tasks averaged over 2 runs using tanh (·)
andcos (·)nesting methods for 10 tasks on the Permuted
MNIST-10 dataset.
Both cos(·)andtanh(·)mappings have this in common,
that they scale every coordinate of the task embedding in-
terval to be in range [−1,1]. These functions also guarantee
we find a non-empty intersection of task embeddings, when
we multiply each coordinate by a factor ofγ
M, where γis
the perturbation value and Mis the embedding dimension.
Hence there exists a universal embedding, in the worst case
being a trivial point in the embedding space.
I Additional experimental results
TIL Results for accuracy behaviour before and after train-
ing on consecutive tasks on TinyImageNet are shown in Fig-
ure 11. Generally, the accuracy after training on all tasks
is comparable to the accuracy obtained just after training
on the specific task. This shows lack of catastrophic forget-
ting when dealing with a bigger number of tasks. Figure 12
shows detailed test accuracy scores after training on con-
secutive tasks. We can observe small decrease in accuracy
on previous tasks and overall consistency in predictions. In
Figure 9, the 95% confidence intervals are shown with the
mean test accuracy for each task, averaged over 5 different
runs. In case of Permuted MNIST, the average test accuracy
before and after training on consecutive tasks behaves simi-
larly. Moreover, the confidence intervals overlap each other.
DIL Confidence interval plots for unknown task identity
are shown in Figure 10. Firstly, we observe that on Split
CIFAR-100, our model is able to learn the first task but hasdifficulties with learning consecutive tasks. It might be be-
cause Split CIFAR-100 is too difficult to solve with one uni-
versal embedding. Secondly, large standard deviations on
Split MNIST show that the HINT training on this dataset
is unstable, especially visible in accuracy on the initial tasks
after learning on all tasks. Specific test accuracy scores for
Permuted MIST-10 and Split MNIST, on each consecutive
task, are shown in Figure 13. We observe positive backward
transfer for the second task of Split MNIST and high accu-
racy values on the diagonal for Permuted MNIST-10, corre-
sponding to the accuracy just after training on the specific
task. On the right-hand side of Figure 9 we show that HINT
can learn new tasks with high accuracy just after training for
Permuted MNIST-10.
I.1 Time complexity
In this subsection, we present the training times for HINT
using the following datasets and graphics cards: 1) Permut-
edMNIST (DGX); 2) SplitMNIST (RTX 4090), TinyIma-
geNet (RTX 4090), and CIFAR-100 (RTX 4090). The de-
tailed results are shown in Table 5. We report the training
times for two scenarios: TIL and CIL. It is crucial to dis-
tinguish between these two scenarios because their training
procedures differ.
PermutedMNIST and SplitMNIST are datasets on which
HINT was trained in both TIL and DIL scenarios simultane-
ously. The results indicate that HINT’s training duration is
approximately 35 minutes longer for PermutedMNIST in the
DIL scenario compared to the TIL scenario. In the case of
the SplitMNIST dataset, the training times are nearly iden-
tical for both scenarios. Notably, the longest training dura-
tion for HINT is observed with the CIFAR-100 dataset (10
tasks, 10 classes per task). This extended duration can be
attributed to the challenges involved in training interval con-
volutional layers to achieve satisfactory performance. These
layers typically require more training steps to ensure a grad-
ual increase in interval length. Additionally, a small batch
size was used. Training time for the CIL scenario is the same
as in the TIL scenario.
References
Aljundi, R.; Babiloni, F.; Elhoseiny, M.; Rohrbach, M.; and
Tuytelaars, T. 2018. Memory Aware Synapses: Learning
what (not) to forget. In Proceedings of the European Con-
ference on Computer Vision (ECCV) .
Aljundi, R.; Belilovsky, E.; Tuytelaars, T.; Charlin, L.; Cac-
cia, M.; Lin, M.; and Page-Caccia, L. 2019. Online contin-
ual learning with maximal interfered retrieval. In Advances
in Neural Information Processing Systems (NeurIPS) , vol-
ume 32. Curran Associates, Inc.
Belouadah, E.; and Popescu, A. 2019. IL2M: Class incre-
mental learning with dual memory. In International Confer-
ence on Computer Vision (ICCV) .
Castro, F. M.; Marín-Jiménez, M. J.; Guil, N.; Schmid, C.;
and Alahari, K. 2018. End-to-end incremental learning. In
European Conference on Computer Vision (ECCV) .
Chaudhry, A.; Dokania, P. K.; Ajanthan, T.; and Torr, P. H.
2018. Riemannian walk for incremental learning: Under-

--- PAGE 17 ---
Figure 9: Average test accuracy (with 95% confidence intervals) for Permuted MNIST-10 for 10 tasks and Split CIFAR-100 for
10 tasks.
Table 5: Mean time complexity averaged over 5 seeds with standard deviations for HINT in the format HH:MM:SS.
Dataset Name TIL DIL
PermutedMNIST 01:11:17 |00:00:27 01:47:56 |00:00:21
SplitMNIST 00:07:29 |00:00:11 00:07:39 |00:00:13
TinyImageNet 00:57:37 |00:01:32 -
CIFAR-100 (10 tasks, 10 classes each) 06:24:50 |00:07:18 -
CIFAR-100 (5 tasks, 20 classes each) - 02:47:26 |00:10:48
(a) Results obtained using the nesting by the
cos (·)method.
(b) Results obtained using the nesting by the
cos (·)method.
Figure 10: Average test accuracy (with 95% confidence in-
tervals) for Split CIFAR-100 and Split MNIST for 5 tasks.
Results are obtained using the cos(·)nesting method.standing forgetting and intransigence. In Proceedings of the
European conference on computer vision (ECCV) , 532–547.
Cywi ´nski, B.; Deja, K.; Trzci ´nski, T.; Twardowski, B.; and
Łukasz Kuci ´nski. 2024. GUIDE: Guidance-based Incremen-
tal Learning with Diffusion Models. ArXiv:2403.03938.
Dahlquist, G.; and Björck, Å. 2008. Numerical methods in
scientific computing, volume I . SIAM.
Deng, Z.; and Russakovsky, O. 2022. Remember the past:
Distilling datasets into addressable memories for neural net-
works. In Advances in Neural Information Processing Sys-
tems (NeurIPS) , volume 35, 34391–34404.
French, R. M. 1999. Catastrophic forgetting in connectionist
networks. Trends in cognitive sciences , 3(4): 128–135.
Goswami, D.; Liu, Y .; Twardowski, B.; and van de Wei-
jer, J. 2024. FeCAM: Exploiting the Heterogeneity of
Class Distributions in Exemplar-Free Continual Learning.
arXiv:2309.14062.
Gowal, S.; Dvijotham, K.; Stanforth, R.; Bunel, R.; Qin,
C.; Uesato, J.; Arandjelovic, R.; Mann, T.; and Kohli, P.
2018. On the effectiveness of interval bound propaga-
tion for training verifiably robust models. arXiv preprint
arXiv:1810.12715 .
Ha, D.; Dai, A.; and Le, Q. V . 2016. Hypernetworks. arXiv
preprint arXiv:1609.09106 .
Henning, C.; Cervera, M.; D’Angelo, F.; V on Oswald, J.;
Traber, R.; Ehret, B.; Kobayashi, S.; Grewe, B. F.; and Sacra-
mento, J. 2021. Posterior meta-replay for continual learn-
ing. Advances in Neural Information Processing Systems ,
34: 14135–14149.
Hou, S.; Pan, X.; Loy, C. C.; Wang, Z.; and Lin, D. 2019.
Learning a unified classifier incrementally via rebalancing.
InInternational Conference on Computer Vision (ICCV) .
Kang, H.; Yoon, J.; Madjid, S. R.; Hwang, S. J.; and Yoo,
C. D. 2023. Forget-free Continual Learning with Soft-
Winning SubNetworks. arXiv:2303.14962.

--- PAGE 18 ---
Figure 11: Test accuracy of HINT for each task of TinyImageNet. We select one model trained on all 40 tasks, in the known
task identity setup.
Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Des-
jardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.;
Grabska-Barwinska, A.; et al. 2017. Overcoming catas-
trophic forgetting in neural networks. Proceedings of the
National Academy of Sciences , 114(13): 3521–3526.
Ksi ˛ a ˙zek, K.; and Spurek, P. 2023. HyperMask: Adaptive
Hypernetwork-based Masks for Continual Learning. arXiv
preprint arXiv:2310.00113 .
Lee, K. H. 2004. First course on fuzzy theory and applica-
tions , volume 27. Springer Science & Business Media.
Li, Z.; and Hoiem, D. 2017. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelli-
gence , 40(12): 2935–2947.
Liu, Y .; Liu, A.-A.; Su, Y .; Schiele, B.; and Sun, Q. 2020.
Mnemonics training: Multi-class incremental learning with-
out forgetting. In Computer Vision and Pattern Recognition
(CVPR) .
Lopez-Paz, D.; and Ranzato, M. 2017. Gradient Episodic
Memory for Continual Learning. In Proceedings of
the Advances in Neural Information Processing Systems
(NeurIPS) .
Mallya, A.; Davis, D.; and Lazebnik, S. 2018. Piggyback:
Adapting a Single Network to Multiple Tasks by Learning to
Mask Weights. In Proceedings of the European Conference
on Computer Vision (ECCV) .
Mallya, A.; and Lazebnik, S. 2018. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) .
Masse, N. Y .; Grant, G. D.; and Freedman, D. J. 2018. Alle-
viating catastrophic forgetting using context-dependent gat-
ing and synaptic stabilization. Proceedings of the National
Academy of Sciences , 115: E10467 – E10475.McCloskey, M.; and Cohen, N. J. 1989. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. In Psychology of learning and motivation , vol-
ume 24, 109–165. Elsevier.
Nguyen, C. V .; Li, Y .; Bui, T. D.; and Turner, R. E. 2018.
Variational continual learning. In International Conference
on Learning Representations (ICLR) .
Prabhu, A.; Torr, P. H.; and Dokania, P. K. 2020. Gdumb:
A simple approach that questions our progress in continual
learning. In Proceedings of the European Conference on
Computer Vision (ECCV) , 524–540.
Ratcliff, R. 1990. Connectionist models of recognition
memory: constraints imposed by learning and forgetting
functions. Psychological review , 97(2): 285.
Rebuffi, S.; Kolesnikov, A.; Sperl, G.; and Lampert, C. H.
2017. iCaRL: Incremental Classifier and Representation
Learning. In Computer Visiona and Pattern Recognition
(CVPR) .
Rolnick, D.; Ahuja, A.; Schwarz, J.; Lillicrap, T. P.; and
Wayne, G. 2019. Experience Replay for Continual Learn-
ing. In Proceedings of the Advances in Neural Information
Processing Systems (NeurIPS) .
Rostami, M.; Kolouri, S.; and Pilly, P. K. 2019. Comple-
mentary learning for overcoming catastrophic forgetting us-
ing experience replay. In International Joint Conference on
Artificial Intelligence (IJCAI) , 3339–3345.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;
Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;
Berg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale
Visual Recognition Challenge. arXiv:1409.0575.
Rusu, A. A.; Rabinowitz, N. C.; Desjardins, G.; Soyer, H.;
Kirkpatrick, J.; Kavukcuoglu, K.; Pascanu, R.; and Had-
sell, R. 2016. Progressive neural networks. arXiv preprint
arXiv:1606.04671 .

--- PAGE 19 ---
1 2 3 4 5 6 7 8 9 10
Number of the tested task1 2 3 4 5 6 7 8 9 10Number of the previously learned task97.4
97.4 97.9
97.4 97.8 97.9
97.4 97.8 97.9 97.8
97.4 97.8 97.9 97.8 97.9
97.4 97.8 97.9 97.8 97.9 97.8
97.4 97.8 97.9 97.8 97.9 97.8 97.8
97.4 97.8 97.9 97.8 97.9 97.8 97.8 98.0
97.4 97.8 97.9 97.8 97.9 97.8 97.8 97.9 98.0
97.4 97.8 97.9 97.8 97.9 97.7 97.8 97.9 97.9 97.7Mean accuracy for 5 runs of HyperInterval for PermutedMNIST-10
97.497.597.697.797.897.9
1 2 3 4 5
Number of the tested task1 2 3 4 5Number of the previously learned task99.9
99.9 99.6
99.9 99.6 99.9
99.9 99.6 99.9 99.8
99.9 99.6 99.9 99.8 99.5Mean accuracy for 5 runs of HyperInterval for SplitMNIST
99.5599.6099.6599.7099.7599.8099.8599.90
1 2 3 4 5 6 7 8 9 10
Number of the tested task1 2 3 4 5 6 7 8 9 10Number of the previously learned task73.8
73.7 71.9
73.8 71.8 78.0
73.7 71.8 77.9 75.8
73.7 71.8 78.0 75.6 78.8
73.7 71.7 77.8 75.5 78.1 79.8
73.8 71.8 77.7 73.1 78.2 79.6 80.5
73.9 71.8 77.7 73.3 78.3 79.4 79.2 77.8
73.7 71.9 77.6 73.2 78.4 79.4 79.1 77.8 79.5
73.7 71.8 77.6 72.8 78.2 79.5 78.8 77.8 79.7 84.7Mean accuracy for 5 runs of HyperInterval for CIFAR-100
72747678808284Figure 12: Mean test accuracy for consecutive CL tasks av-
eraged over five runs of the best models. Results are obtained
for the known task identity setup. The diagonal corresponds
to testing the task just after the model was trained on it. The
sub-diagonal values correspond to testing the task after the
model was trained on consecutive tasks.
1 2 3 4 5 6 7 8 9 10
Number of the tested task1 2 3 4 5 6 7 8 9 10Number of the previously learned task97.7
97.6 93.9
97.2 89.3 95.5
96.8 81.7 93.6 96.3
96.5 77.6 89.9 94.6 96.5
96.1 75.2 87.9 92.0 95.5 96.7
95.7 76.5 86.0 91.5 92.1 95.4 96.8
95.3 77.3 82.9 87.6 89.7 93.3 95.4 97.0
94.9 77.8 83.1 84.4 87.6 91.3 93.9 95.8 97.1
94.4 75.6 79.8 83.5 84.6 88.2 92.7 94.6 95.7 97.0Mean accuracy for 5 runs of HyperInterval for PermutedMNIST-10
77.580.082.585.087.590.092.595.097.5
1 2 3 4 5
Number of the tested task1 2 3 4 5Number of the previously learned task99.9
99.7 76.0
79.8 85.0 90.6
88.5 85.7 86.9 91.7
75.1 83.4 67.5 93.1 74.9Mean accuracy for 5 runs of HyperInterval for SplitMNIST
707580859095Figure 13: Mean test accuracy for consecutive CL tasks av-
eraged over five runs of the best models. Results are obtained
with the cos(·)nesting method.

--- PAGE 20 ---
Shin, H.; Lee, J. K.; Kim, J.; and Kim, J. 2017. Contin-
ual Learning with Deep Generative Replay. In Proceedings
of the Advances in Neural Information Processing Systems
(NeurIPS) .
van de Ven, G. M.; Siegelmann, H. T.; and Tolias, A. S.
2020. Brain-inspired replay for continual learning with arti-
ficial neural networks. Nature Communications , 11.
van de Ven, G. M.; and Tolias, A. S. 2018. Generative replay
with feedback connections as a general strategy for contin-
ual learning. ArXiv:1809.10635.
van de Ven, G. M.; and Tolias, A. S. 2019. Three scenarios
for continual learning. ArXiv:1904.07734.
Virmaux, A.; and Scaman, K. 2018. Lipschitz regularity of
deep neural networks: analysis and efficient estimation. In
Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-
Bianchi, N.; and Garnett, R., eds., Advances in Neural Infor-
mation Processing Systems , volume 31. Curran Associates,
Inc.
von Oswald, J.; Henning, C.; Grewe, B. F.; and Sacramento,
J. 2019. Continual learning with hypernetworks. In Interna-
tional Conference on Learning Representations .
von Oswald, J.; Henning, C.; Grewe, B. F.; and Sacramento,
J. 2020. Continual learning with hypernetworks. In Interna-
tional Conference on Learning Representations .
Wang, T.; Zhu, J.-Y .; Torralba, A.; and Efros, A. A. 2018.
Dataset distillation. ArXiv:1811.10959.
Wołczyk, M.; Piczak, K.; Wójcik, B.; Pustelnik, L.; Moraw-
iecki, P.; Tabor, J.; Trzcinski, T.; and Spurek, P. 2022. Con-
tinual learning with guarantees via weight interval con-
straints. In International Conference on Machine Learning ,
23897–23911. PMLR.
Wortsman, M.; Ramanujan, V .; Liu, R.; Kembhavi, A.;
Rastegari, M.; Yosinski, J.; and Farhadi, A. 2020. Super-
masks in superposition. In Neural Information Processing
Systems (NeurIPS) , 15173–15184.
Wu, Y .; Chen, Y .; Wang, L.; Ye, Y .; Liu, Z.; Guo, Y .; and Fu,
Y . 2019. Large scale incremental learning. In International
Conference on Computer Vision (ICCV) .
Yoon, J.; Yang, E.; Lee, J.; and Hwang, S. J. 2018. Lifelong
Learning with Dynamically Expandable Networks. In Inter-
national Conference on Learning Representations (ICLR) .
Zenke, F.; Poole, B.; and Ganguli, S. 2017. Continual learn-
ing through synaptic intelligence. In International Confer-
ence on Machine Learning (ICML) , 3987–3995.
Zhao, B.; Mopuri, K. R.; and Bilen, H. 2021. Dataset con-
densation with gradient matching. In International Confer-
ence on Learning Representations (ICLR) .
