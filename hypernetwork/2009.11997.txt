# 2009.11997.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypernetwork/2009.11997.pdf
# File size: 4445801 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Continual Model-Based Reinforcement Learning with Hypernetworks
Yizhou Huang1, Kevin Xie2, Homanga Bharadhwaj2, and Florian Shkurti2
Abstract ‚Äî Effective planning in model-based reinforcement
learning (MBRL) and model-predictive control (MPC) relies
on the accuracy of the learned dynamics model. In many
instances of MBRL and MPC, this model is assumed to
be stationary and is periodically re-trained from scratch on
state transition experience collected from the beginning of
environment interactions. This implies that the time required
to train the dynamics model ‚Äì and the pause required between
plan executions ‚Äì grows linearly with the size of the collected
experience. We argue that this is too slow for lifelong robot
learning and propose HyperCRL , a method that continually
learns the encountered dynamics in a sequence of tasks using
task-conditional hypernetworks. Our method has three main
attributes: Ô¨Årst, it includes dynamics learning sessions that do
not revisit training data from previous tasks, so it only needs to
store the most recent Ô¨Åxed-size portion of the state transition
experience; second, it uses Ô¨Åxed-capacity hypernetworks to
represent non-stationary and task-aware dynamics; third, it
outperforms existing continual learning alternatives that rely on
Ô¨Åxed-capacity networks, and does competitively with baselines
that remember an ever increasing coreset of past experience.
We show that HyperCRL is effective in continual model-based
reinforcement learning in robot locomotion and manipulation
scenarios, such as tasks involving pushing and door opening.
Our project website with videos is at this link http://rvl.
cs.toronto.edu/blog/2020/hypercrl/
I. I NTRODUCTION
Lifelong model-based robot learning is predicated upon
continual adaptation to the dynamics of new tasks. For
example, robots need to learn to manipulate unseen objects
with various mass distributions, walk on new types of
terrains with different friction, elasticity, and other physical
properties, or even learn to adapt to different tasks, such as
walking, running, or climbing stairs. This presents at least
two challenges for many model-based reinforcement learning
(MBRL) and model-predictive control (MPC) formulations,
which typically comprise of a dynamics learning phase
followed by a planning/policy optimization and execution
phase. First, these methods are not scalable because the time
required to train the dynamics model grows linearly with
the size of the collected experience. Second, as the robot
learner encounters and adapts to new tasks, it has to avoid
catastrophic forgetting of the dynamics of old tasks.
In this work, we propose to extend the task-aware con-
tinual learning approach based on hypernetworks in [1] to
adapt to changing environment dynamics and to address the
scalability and catastrophic forgetting challenges mentioned
1Author is with Division of Engineering Science, University of Toronto,
Canada. Email: philipyizhou.huang@mail.utoronto.ca
2Authors are with Department of Computer Science, University of
Toronto, Canada
Fig. 1: Overview of our proposed method HyperCRL
above in a reinforcement learning setting. We use task-
conditional hypernetworks, which are neural network models
that accept a learned task encoding as an input, and output
the weights of another (target) network. In our case, the
output is the dynamics model for that task. No additional
information other than task transition boundary is needed.
We show that continual learning with hypernetworks leads
to effective model-based reinforcement learning, while re-
ducing the number of updates of the dynamics model across
planning sessions and preventing catastrophic forgetting.
We consider the setting where task boundaries are known
in order to simplify the problem, although there are contin-
ual learning methods that have addressed the task-agnostic
setting using Bayesian non-parametric methods [2]. In ad-
dition, we focus on Ô¨Åxed-capacity hypernetworks and target
networks that can handle a sequence of tasks without adding
new neurons or layers to the network, unlike many related
works [3], [4] in which every new task adds capacity to the
dynamics model. We argue that the Ô¨Åxed-capacity setting,
together with storing only the most recent portion of the
state transition experience, is more realistic and scalable for
lifelong robot learning applications compared to approaches
in which the training time or the model‚Äôs size scales linearly
with the size of collected experience.
Our work makes the following contributions: we show
that task-aware continual learning with hypernetworks is
an effective and practical way to adapt to new tasks and
changing dynamics for model-based reinforcement learning
without keeping state transitions from old tasks nor adding
capacity to the dynamics model. We evaluate our method on
locomotion and manipulation scenarios, where we show that
our method outperforms related continual learning baselines.
II. R ELATED WORKS
Continual Learning in Neural Networks Continual
learning studies the problem of incrementally learning from
a sequential stream of data with only a small portion
of the data available at once [5]. A simple yet effective
approach is Ô¨Ånetuning, which directly tunes the trained
source task network on the target task [6]. The efÔ¨ÅcacyarXiv:2009.11997v2  [cs.LG]  30 Mar 2021

--- PAGE 2 ---
of this approach for continual learning suffers from the
well-established phenomenon of catastrophic forgetting [4].
Sequential Bayesian posterior updates are a principled way
to perform continual learning and naturally avoid the for-
getting problem since the exact posterior fully incorporates
all previous data but in practice approximations have to
be made that may be prone to forgetting. Elastic Weight
Consolidation (EWC) uses a Laplace approximation to the
posterior, storing previous tasks‚Äô empirical Ô¨Åsher matrices
and regularizing future task weight deviations under their in-
duced norms [7]. Other works have also employed variational
mean-Ô¨Åeld approximations [8] or block-diagonal Kronecker
factored Laplace approximations [9]. Synaptic Intelligence
(SI) forgoes an obvious approximate Bayesian interpretation
but operates similarly to EWC in that it computes a rel-
ative parameter importance measure, but through a linear
approximation to the contribution in loss reduction due to
each parameter on previous tasks [10]. Coreset methods
prevent catastrophic forgetting by choosing and storing a
signiÔ¨Åcantly smaller subset of the previous task‚Äôs data, which
is used to rehearse the model during or after Ô¨Ånetuning [11],
[12], [13]. Similarly, the inducing points used in sparse
Gaussian Process (GP) formulations, which can be seen as a
type of coreset, has been used for continual learning [14],
[15]. Another type of approach learns separate task-speciÔ¨Åc
network components. The most common version of this are
multi-head networks that learn and switch between separate
output layers depending on the task [16]. Progressive Neural
Networks (PNN) [4] are an extreme version of this approach,
in which an entirely new copy of the network is appended for
each task, thereby eliminating any forgetting. These methods
can incur signiÔ¨Åcant memory and compute cost especially for
larger models and many tasks.
Model-based RL Model-based reinforcement learning
approaches incorporate model learning of the environment in
solving the control task. Traditionally, the model is trained
to approximate the stationary dynamics and/or reward of
the environment from all collected samples. Various choices
for the model have been proposed. Many non-parametric
models, such as commonly used GPs, rely on storing and
making inferences with past data [17], although in many
cases the amount of data needed to be stored can be
drastically reduced through sparse variational inference [18].
Nonlinear parametric models typically do not admit efÔ¨Åcient
sequential update rules and therefore usually train on all
past data as well [19]. Typically, the trained model is then
used to simulate imagined trajectories, either for the purpose
of online planning [20], [21] or as training data for an
amortized policy [22]. In non-stationary environments, the
dynamics can change over time. In this setting, a lot of
works focus on quickly adapting to the change in dynamics to
minimize online regret, as opposed to retaining performance
on previously experienced dynamics [23], [24], [25]. Meta-
learning is a popular tool in this paradigm where a global
dynamics model is ‚Äúmeta-trained‚Äù to quickly adapt to the
true online dynamics from only a few samples. However, the
meta-learning process typically requires updating the meta-model with data from a diverse set of dynamics that is
obtained by storing previous experiences in a buffer and/or
being able to simultaneously interact with many different
environments [26].
Relationship to Meta-Learning Our work is different
from meta-learning approaches in MBRL like [27], [28]
in two ways. First, we focus on preventing catastrophic
forgetting and do not explicitly train a model prior across
multiple tasks for fast adaptation. This means that we do
not need to design a set of tasks for meta-training and in
principle, our work can continuously learn to perform new
tasks from scratch. Second, we do not require the use of a
replay buffer that grows linearly with the number of tasks or
total length of collected state-transition pairs. We emphasize
that this is consistent with the theme of continual learning,
where storing past data is limited.
Continual RL Memory-efÔ¨Åcient continual learning meth-
ods in the reinforcement learning setting have also been
proposed. PNN was used in an on-policy actor-critic method
and was demonstrated on sequential discrete action Atari
games. The authors of [29] build on top of PNN and
continual policy compression methods [30] by compressing
the extended model from PNN into a Ô¨Åxed size network
after each task. For the compression stage, they propose a
more scalable online EWC algorithm that eschews the linear
cost of storing past Ô¨Åsher matrices. The use of coresets has
also been explored in this setting [31]. In [27], a mixture
model of separate task-speciÔ¨Åc neural networks is used for
the environment model that requires adding a new model
each time a task is instantiated. A similar approach was
also demonstrated using a mixture of GPs using an online
clustering algorithm [32]. Our work is also related to MPC
interpretations as a reduction to online learning [33].
Robust and Adaptive Control Existing literature on
control theory provides many related classes of approaches
that handle changes in dynamics: adaptive control methods
handle unknown parameters of a dynamics model by estimat-
ing them over time so as to execute a given trajectory, and
robust control methods provide stability guarantees as long
as the parameter or model disturbance is within bounds [34].
The setting we study in this work differs in that we learn the
dynamics model from scratch, without assuming a reference
model, and it may change over tasks without imposing any
bounds on particular parameters.
III. P RELIMINARIES
Hypernetworks for Continual Learning. A hypernet-
work [35], [36] is a network that generates the weights
of another neural network. The hypernetwork H(e) =
with weights can be conditioned on an embedding vector
eto output the weights of the main (target) network
f(x) =f(x;) =f(x;H(e))by varying the embedding
vectore. The hypernetwork is typically larger with respect to
the number of trainable parameters in comparison to the main
network because the size of output layer in the hypernetwork
is equal to the number of weights in the target network.
Hypernetworks have been shown to be useful in the continual

--- PAGE 3 ---
learning setting [1] for classiÔ¨Åcation and generative models.
This has been shown to alleviate some of the issues of
catastrophic forgetting . They have also been used to enable
gradient-based hyperparameter optimization [37].
Planning with CEM and MPC. The Cross-Entropy
Method (CEM) [38] is a widely used online planning algo-
rithm that samples action sequences from a time-evolving
distribution which is usually considered to be a diagonal
Gaussiana1:hN(1:h;diag (2
1:h)), wherehis the plan-
ning horizon. Action sequences are iteratively re-sampled
and evaluated under the currently learned dynamics model,
and the sampling distribution parameters 1:h;1:hare re-
Ô¨Åtted to the top percentile of trajectories. CEM for planning
in MBRL has been successfully used in a number of previous
approaches [20], [21], as it alleviates exploitation of model
bias compared to purely gradient based optimizations [39]
and can better adapt to varying dynamics as compared to
fully amortized policies [22].
IV. T HEPROPOSED APPROACH
A. Problem Setting and Method Overview
We consider the following problem setting: A robot in-
teracts with the environment to solve a sequence of Tgoal-
directed tasks, each of which brings about different dynamics
while having the same state-space Sand action spaceA.
The robot is exposed to the tasks sequentially online without
revisiting data collected in a previous task. The robot also has
Ô¨Ånite memory and is not allowed to maintain a full history
of state transitions for the purpose of re-training. Since the
distribution of tasks changes over time and the agent does
not know about it a priori, it must continually adapt to the
streaming data of observations that it encounters, while trying
to solve each task. The robot knows when a task switch
occurs.
We consider the solution setting of MBRL with a learned
dynamics model, the parameters of which are inferred
through a task-conditioned hypernetwork. Given learned task
embeddings etand parameters tof the hypernetwork H(),
we infer parameters tof the dynamics neural network ft().
Using this dynamics model, we perform CEM optimization
to generate action sequences and execute them in the envi-
ronment for Ktime-steps with MPC. We store the observed
transitions in the replay dataset and update the parameters
of the hypernetwork tand task-embeddings et(off-policy
optimization). We repeat this for Mepisodes per task, and
for each of the Ttasks sequentially.
B. Training Procedure
Dynamics Learning. The learned dynamics model is a feed-
forward neural network whose parameters vary across tasks.
One way to learn a dynamics network f()across tasks is to
update it sequentially as training progresses. However, since
our problem setting is such that the agent is not allowed to
retain state-transition data from previous tasks in the replay
buffer, adapting the weights of a single network sequentially
across tasks is likely to lead to catastrophic forgetting [1].
In order to alleviate issues of catastrophic forgetting whileAlgorithm 1: Continual Reinforcement Learning via
Hypernetworks ( HyperCRL )
1:Input:Ttasks, each with its own dynamics
SA!S0, rewardr(s;a). Learning rates ,e,
and planning horizon h.
2:Randomly initialize hypernetwork weights 1
3:fortaskt= 1, 2,:::,Tdo
4: Initialize task-speciÔ¨Åc replay buffer Dt=fg
5: Randomly initialize task embedding et
6: CollectPepisodes of trajectories using a random
policy and add it to Dt
7: forepisodem= 1, 2, . . . , Mdo
8: (Optionally) Reset the environment; Observe s0
9: Generate target network weights t=Ht(et)
10: //The dynamics model for the
current episode is ft()
11: forstepk= 1, 2, . . . , Kdo
12: Optimize action sequence ak:k+husing CEM
withft()and known reward
13: Execute Ô¨Årst action ak, observe next state sk+1,
add(sk;ak;sk+1)toDt
14: end for
15: //Update hypernetwork and task
embedding
16: fors= 1, 2, . . . , Sdo
17: Sample a batchBof state-action pairs
(sk;ak;sk+1)fromDtand computeLt
18: t t rtLt(t;et)
19:et et eretLt(t;et)
20: end for
21: end for
22: t+1= t
23:end for
trying to adapt the weights of the network, we learn a hy-
pernetwork that takes task embeddings as input, and outputs
parameters for the dynamics network corresponding to every
task, learning different dynamics networks ft()for each
taskt.
We assume that the agent has Ô¨Ånite memory and does not
have access to state-transition data across tasks. So, the task-
speciÔ¨Åc replay buffer Dtis reset at the start of every task
t. For the current episode, the agent generates a dynamics
networkftusingt=Ht(et). Then, for k= 1:::K
timesteps and planning horizon h, the agent optimizes action
sequencesak:k+husing CEM, and executes the Ô¨Årst action
ak(MPC).Dtis augmented by a tuple (sk;ak;sk+1), where
skis the current state, akis the executed action, and sk+1
is the next observed state under task t.
The parameters tof the hypernetwork and the task
embeddings etare updated by backpropagating gradients
with respect to the sum of a dynamics loss Ldynand a regular-
ization term. We deÔ¨Åne the dynamics loss as Ldyn(t;et) =P
Dtjj^sk+1 sk+1jj2, where the predicted next states are
^sk+1=ft(sk;ak)andt=Ht(et). In practice, we

--- PAGE 4 ---
infer the difference k+1through the dynamics network
(k+1=ft(sk;ak)) such that ^sk+1=sk+ k+1for
stable training. In addition, inputs to the ftnetwork are
normalized, following the procedure in previous works [20].
Similarly, a new etis initialized at the start of every task and
updated every episode during the task by gradient descent.
Older task embeddings ( e1:t 1) could also be amenable to
gradient descent, but we keep them Ô¨Åxed for simplicity.
Regularizing the Hypernetwork. To alleviate catastrophic
forgetting, we regularize the output of the hypernetwork
for all previous task embeddings e1:t 1. After training for
taskt 1, a snapshot of the hypernetwork weights is
saved as t 1. For each of the past tasks i= 1:::t 1,
we use a regularization loss to keep the outputs of the
snapshotHt 1(ei)close to the current output Ht(ei). This
approach sidesteps the need to store all past data across tasks,
preserves the predictive performance of dynamics networks
ft, and only requires a single point in the weight space (a
copy of the hypernetwork) to be stored. The task embeddings
are differentiable vectors learned along with parameters of
the hypernetwork. The overall loss function for updating t
andetis given by the sum of the dynamics loss Ldyn(),
which is evaluated on the data collected from task tand the
regularization term Lreg():
Lt(t;et) =Ldyn(t;et) +Lreg(t 1;t;e1:t 1)
Lreg() =reg
t 1t 1X
i=1kHt 1(ei) Ht(ei)k2
2(1)
The planning objective for CEM optimization of action se-
quences is given by the sum of rewards obtained by executing
the sequence of actions ak:k+hunder the learned dynamics
modelft()for the task. The reward function r(s;a)is
assumed to be known, but nothing precludes learning it from
data under our current framework.
V. E XPERIMENTS
We perform multiple robot simulation experiments
to answer the following questions: (a) How does
HyperCRL compare with existing continual learning base-
lines in terms of overall performance across tasks? (b) How
effective is HyperCRL in preventing catastrophic forgetting
across tasks?
A. Pushing a block of non-uniform mass
First, we look at an intuitively simple experiment simu-
lated using Surreal Robotics Suite [40], in which a Panda
robot tries to push a non-uniform-density block across the
table (Figure 2). The objective is to push the block to the
goal position while maintaining its initial orientation . We
vary the densities of the left and right parts of the block
across different tasks ( T= 5), changing the center of mass
and moment of inertia. The robot needs to learn to maneuver
the position of its end-effector on the side of the block to
correct for orientation deviations while pushing the block
forward. Each episode is 100-step long, which is 10 seconds
/gid00025/gid00026/gid00027
Initial Goal(a) Initial and goal poses of the block.Task Green Blue
1 500 500
2 100 500
3 500 100
4 500 250
5 250 500
(b) Density of the block in kg=m3
Fig. 2: Setup of Pusher Environment . The robot needs to solve 5 pushing
tasks sequentially, each involving a block of different mass distribution.
The objective is to push the block to the goal pose (indicated by the dotted
frame) without changing its orientation.
of simulator time. At the start of each episode, we initialize
the robot end-effector to a Ô¨Åxed position behind the block.
The state is represented as a concatenated vector
(xee;x1;x2;x3;x4). Here (x1;x2;x3;x4)represents the xy
positions of the four corners and xeeis thexyposition
of the end-effector with respect to the block. The input
action vector, xee, speciÔ¨Åes a desired offset with respect
to the current position xee. The robot is actuated using
an operational space controller [41]. It calculates the joint
torques given an action that is updated at 10Hz. The reward
function is set to minimize the sum of distances between
the current and goal pose of the block following r(s;a) =P4
i=1(1 tanh(10jjxi gijj)) 0:25jjajj, wheregiis the goal
position of the ithcorner. The reward function consists of
one term per corner within the range [0, 1], and to maximize
reward the robot should minimize the corner distances to the
goal pose.
OpenedInitial
Pull Handle Round Handle Lever Handle
Fig. 3: Setup of Door Environment . For round and lever handles, the robot
must Ô¨Årst rotate the handle clockwise before pulling the door open. Addi-
tionally, we introduce two more tasks by Ô¨Çipping the rotational direction to
counter-clockwise for the round and lever handles.
B. Door Opening
Next, we experiment on a more complex task to better
demonstrate the Ô¨Çexibility of HyperCRL . We choose a door
opening experiment, where the Panda robot has to open
doors with different types of handles (Figure 3). Unlike
the pushing example, the dynamic switches between tasks
are much more discontinuous and the tasks require different
motions to solve, due to the rotational joints imposed by
some of the handles. A total of T= 5 tasks need to be
solved sequentially, each involving a different handle type.

--- PAGE 5 ---
0200Task 1
0200Task 2
0200Task 3
0200Task 4
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Environment Steps 1e40200Task 5
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
1e40200AverageRewardHyperCRL multitask (oracle) SI EWC coreset finetuningFig. 4: Reward on Pusher Environment . Shown are episodic rewards evaluated during training. Results are averaged across four random seeds, and the
shaded areas represent one standard deviation. Each task is trained for 4k steps, summing to 20k steps in total. Vertical dotted lines indicate task switches.
The bottom-right subplot shows the average reward across all task tseen so far.
Tasks 2 and 4 (similarily tasks 3 and 5) both have a round
(lever) handle, but with different turning directions. The
environment is modiÔ¨Åed from the DoorGym environment
[42] and simulated in the Surreal Robotics Suite [43].
We model the environment as follow: (i) drepresents
the angle of the door (ii) krepresents the angle of the door
handle (iii)x2R3is the position, q2R4is the quaternion
representation of the rotation of the robot end-effector with
respect to the handle (iv) d2Ris the state of the gripper (v)
qj2R7is the angle of the joints of the Panda arm. Concate-
nating all the above elements and their time derivative gives
the full state vector (d;_d;k;_k;x;q;d;qj;_qj)2R26.
Similar to the pushing environment, the robot is actuated with
operational space control updated at 10Hz. The input action
is a vector (x;q;d ), which speciÔ¨Åes a translation and
rotational movement of the end-effector in the world frame.
Finally, the reward function has Ô¨Åve components as follows:
r(s;a) = jjxjj2 log(jjxjj2+) jjqojj2+ 50d+ 20k,
whereqois the difference between the orientation of the
current end-effector and the required pose for opening the
door.
/gid00025/gid00026/gid00027
Initial Block 2 reaches goalBlock 1
Block 2
Fig. 5: Setup of Slide
Environment . The
Panda arm should
push block 1, which
will kick block 2 in
motion towards the
goal pose. There are
5 tasks sequentially
with different friction
for block 2.
C. Block Sliding
Our last experiment involves a Panda robot and two
separate blocks placed on tabletop with low friction. The
initial pose of the end-effector, the two blocks are placed
along a straight line and we number the two blocks shown
in Figure 5. To move block 2 to its goal pose, the manipulator
should Ô¨Årst hit block 1, which would slide away and in turn
set the second block into sliding motion until stopped by
friction. We vary the friction of block 2 across different
tasks (T= 5), while keeping friction of block 1 the same.Similar to the pusher experiment, we represent the state as
a concatenated vector (xee;x1
1:4;x2
1:4), wherexeedenotes
thexyposition of the end-effector, and (x1
1:4;x2
1:4)denotes
thexypositions of the corners of both blocks. The reward
function minimizes the distances between the current and
goal pose of the second block g1:4as follows: r(s;a) =P4
i=1(1 tanh(10jjx2
i gijj)) 0:1jjajj. The robot must
adjust the momentum of the Ô¨Årst block in the initial push for
different frictions to successfully deliver the second block to
its goal. Each episode is 30-step long and equates 3 seconds
in the simulator.
D. Baselines
We compare the hypernetwork to the following baselines:
(i) Multi-task learning with access to a buffer of all data from
all previous tasks (an oracle) (ii) Coreset that remembers one
percent of the state-action transition data per task, sampled
randomly (iii) Synaptic Intelligence [10] (iv) Elastic Weight
Consolidation [7] (v) Fine-tuning, where we optimize ft()
on each task‚Äôs data without regularization. All the baseline
models resemble the target model in our hypernetwork setup,
except the multi-head output layer with one head per task.
For coreset or multi-task learning, an additional batch of past
data is sampled from the coreset or oracle every update step,
and contributes to the total dynamics loss.
E. Results
Pusher HyperCRL is able to learn to push all 5 of the
blocks across the table with minimal forgetting (Table I),
and even shows signs of positive backward transfer. The
average performance of our method is on par with the
multi-task learning baseline (Figure 4), which has access
to the entire history of data. HyperCRL also outperforms
other continual learning baselines, either regularization-based
(SI, EWC), or replay-based (Coreset). Simple Ô¨Ånetuning,
however, catastrophically forgets and is unable to perform
pushing for all 5 types of block at the end.
Door HyperCRL outperforms all continual learning base-
lines on the door opening task, and the multi-task baseline.
Figure 6 shows the learning curves of all our evaluated

--- PAGE 6 ---
0.01.02.0Task 1
0.01.02.0Task 2
0.01.02.0Task 3
0.01.02.0Task 4
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Environment Steps 1e50.01.02.0Task 5
0.0 0.5 1.0 1.5 2.0 2.5 3.0
1e50.01.0AverageNormalized RewardHyperCRL multitask (oracle) SI EWC coreset finetuningFig. 6: Normalized Reward on Door Environment during training. Results are averaged across four random seeds, and the error bars represent one
standard deviation. Each task is trained for 60k steps, totaling 300k steps. The reward is normalized with respect to a model trained from scratch separately
on each task.
methods, compared to a single-task baseline trained from
scratch on each task. HyperCRL virtually sees no perfor-
mance degradation in terms of reward (Table I).
Block Sliding HyperCRL outperforms all the continual
learning baselines although its performance falls short of the
multi-task baseline. In Table I, HyperCRL experiences a
slight degree of forgetting on average. The learning curve is
omitted here due to space constraint and is available on our
project website.
F . Training details
For all experiments, we model the target network as a
multi-layer perceptron (MLP) with two hidden layers (four
for door opening) of 200 neurons each and ReLU non-
linearity. Each task embedding is initialized as a 10dstandard
normal vector. The hypernetwork is also a MLP with two
% Retention in terms of Reward (Pusher)
Task 1 2 3 4 Average
Multi-task 14274 9125 969 9111 10620
HyperCRL 9910 1079 9813 10315 1026
SI 4054 8821 9521 9214 7916
EWC 715 138 68 03 45
Coreset 8766 3246 3913 6143 5523
Finetuning 02 -26 13 1316 34
% Retention in terms of Normalized Reward (Door, see Figure 6)
Task 1 2 3 4 Average
Multi-task 3770 8188 5960 93129 6845
HyperCRL 11375 10076 9936 8668 10026
SI -1727 1161 637 1660 424
EWC -633 1759 425 1650 722
Coreset -528 2653 2134 4568 2224
Finetuning -1428 1158 323 1665 423
% Retention in terms of Reward (Block Sliding)
Task 1 2 3 4 Average
Multi-task 10636 13127 10339 11312 11316
HyperCRL 8256 9811 10916 9565 9622
SI 3714 9212 6856 1037 7515
EWC -57 1420 1335 3927 1512
Coreset 5818 9772 6533 7321 7321
Finetuning -28 -1215 -14 -75 -65
TABLE I: Performance Retention on Pusher, Door and Block Sliding
Envs . We measure average episodic reward retained at the end of training
all 5 tasks compared to the end of training on task t. Results show the mean
and one std. deviation, evaluated across 4 seeds and 10 episodes per seed.
Greater than 100 indicates positive backward transfer, otherwise it indicates
forgetting.hidden layers of 50 neurons each (256 neurons for door
opening). The parameters of the hypernetwork are initialized
with Xavier initialization [44]. We use Adam with a learning
rate of 0.0001 to optimize Lt. During planning, we run
CEM for 5 iterations to optimize the actions for a horizon
ofh= 20 (10for door opening) steps. Each iteration, we
sample 500 (2000 for door opening) action sequences to
maximize the sum of rewards. For SI and EWC, we use
implementations from [3].
VI. L IMITATIONS AND FUTURE WORK
While our proposed approach shows good results against
the baselines we tested, there are many interesting prospects
for future work. First, better techniques to train hypernet-
works will signiÔ¨Åcantly aid this line of work. Currently, the
size of our hypernetwork is at least an order of magnitude
larger than the target network, since it directly outputs all
the weights. Hypernetworks are often sensitive to the choice
of random seeds and architecture. Second, extending our
method to image-based RL environments is worth investigat-
ing, as it will enable higher-capacity target networks. Finally,
HyperCRL is not task agnostic, nor can it automatically
detect task switching that happens continuously with no clear
task boundaries. A possible direction is to use probabilis-
tic inference models (i.e. Bayesian non-parametrics [2]) or
changepoint detection methods to perform task identiÔ¨Åcation.
VII. C ONCLUSION
In this paper we described HyperCRL , a task-aware
method for continual model-based reinforcement learning
using hypernetworks. In all of our experiments, we have
demonstrated that HyperCRL consistently outperforms al-
ternative continual learning baselines in terms of overall
performance at the end of training, as well as in terms of
retaining performance on previous tasks. By allowing the
entire network to change between tasks, rather than just the
output head layer of the dynamics network, HyperCRL is
more effective at accurately representing different dynamics,
even with signiÔ¨Åcant task switching, while only requiring a
Ô¨Åxed-size hypernetwork and no training data from old tasks
to generate task-conditional dynamics for lifelong planning.

--- PAGE 7 ---
REFERENCES
[1] Johannes von Oswald, Christian Henning, Jo Àúao Sacramento, and
Benjamin F Grewe. Continual learning with hypernetworks. In
International Conference on Learning Representations , 2019.
[2] Mengdi Xu, Wenhao Ding, Jiacheng Zhu, Zuxin Liu, Baiming Chen,
and Ding Zhao. Task-agnostic online reinforcement learning with an
inÔ¨Ånite mixture of gaussian processes, 2020.
[3] Gido M van de Ven and Andreas S Tolias. Three scenarios for
continual learning. arXiv preprint arXiv:1904.07734 , 2019.
[4] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert
Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu,
and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671 , 2016.
[5] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot,
Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A
continual learning survey: Defying forgetting in classiÔ¨Åcation tasks.
arXiv: Computer Vision and Pattern Recognition , 2020.
[6] Yoshua Bengio. Deep learning of representations for unsupervised and
transfer learning. In Proceedings of ICML workshop on unsupervised
and transfer learning , pages 17‚Äì36, 2012.
[7] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness,
Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan,
Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national
academy of sciences , 114(13):3521‚Äì3526, 2017.
[8] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner.
Variational continual learning. arXiv preprint arXiv:1710.10628 , 2017.
[9] Hippolyt Ritter, Aleksandar Botev, and David Barber. Online struc-
tured laplace approximations for overcoming catastrophic forgetting,
2018.
[10] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning
through synaptic intelligence. Proceedings of machine learning
research , 70:3987, 2017.
[11] David Lopez-Paz and Marc‚ÄôAurelio Ranzato. Gradient episodic
memory for continual learning. In Advances in neural information
processing systems , pages 6467‚Äì6476, 2017.
[12] Arslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus Rohrbach, and Mo-
hamed Elhoseiny. EfÔ¨Åcient lifelong learning with a-gem, 2018.
[13] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Tha-
laiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and M Ran-
zato. Continual learning with tiny episodic memories. 2019.
[14] Michalis K. Titsias, Jonathan Schwarz, Alexander G. de G. Matthews,
Razvan Pascanu, and Yee Whye Teh. Functional regularisation for
continual learning using gaussian processes. ArXiv , abs/1901.11356,
2020.
[15] Sanyam Kapoor, Theofanis Karaletsos, and Thang D. Bui. Variational
auto-regressive gaussian processes for continual learning, 2020.
[16] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelligence ,
40(12):2935‚Äì2947, 2017.
[17] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and
data-efÔ¨Åcient approach to policy search. In Proceedings of the 28th
International Conference on machine learning (ICML-11) , pages 465‚Äì
472, 2011.
[18] David Burt, Carl Edward Rasmussen, and Mark van der Wilk. Explicit
rates of convergence for sparse variational inference in gaussian pro-
cess regression. In Symposium on advances in approximate Bayesian
inference , volume 1, 2018.
[19] Tapani Raiko and Matti Tornio. Variational bayesian learning of
nonlinear hidden state-space models for model predictive control.
Neurocomputing , 72(16-18):3704‚Äì3712, 2009.
[20] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey
Levine. Deep reinforcement learning in a handful of trials using
probabilistic dynamics models. In Advances in Neural Information
Processing Systems , pages 4754‚Äì4765, 2018.
[21] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David
Ha, Honglak Lee, and James Davidson. Learning latent dynamics
for planning from pixels. In International Conference on Machine
Learning , pages 2555‚Äì2565, 2019.
[22] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad
Norouzi. Dream to control: Learning behaviors by latent imagination.
arXiv preprint arXiv:1912.01603 , 2019.
[23] Eduardo W Basso and Paulo M Engel. Reinforcement learning in
non-stationary continuous time and space scenarios. In ArtiÔ¨Åcial
Intelligence National Meeting (Enia) , volume 7, pages 1‚Äì8, 2009.[24] Sindhu Padakandla, Shalabh Bhatnagar, et al. Reinforcement learning
in non-stationary environments. arXiv preprint arXiv:1905.03970 ,
2019.
[25] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov
decision processes. arXiv preprint arXiv:1502.02259 , 2015.
[26] Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor
Mordatch, and Pieter Abbeel. Continuous adaptation via meta-
learning in nonstationary and competitive environments. arXiv preprint
arXiv:1710.03641 , 2017.
[27] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online
learning via meta-learning: Continual adaptation for model-based rl.
arXiv preprint arXiv:1812.07671 , 2018.
[28] I. Clavera, Anusha Nagabandi, R. Fearing, P. Abbeel, S. Levine, and
Chelsea Finn. Learning to adapt: Meta-learning for model-based
control. ArXiv , abs/1803.11347, 2018.
[29] Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka
Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Had-
sell. Progress & compress: A scalable framework for continual
learning. arXiv preprint arXiv:1805.06370 , 2018.
[30] Glen Berseth, Kevin Xie, Paul Cernek, and Michiel van de Panne.
Progressive reinforcement learning with distillation for multi-skilled
motion control. ArXiv , abs/1802.04765, 2018.
[31] David Abel, Dilip Arumugam, Lucas Lehnert, and Michael Littman.
State abstractions for lifelong reinforcement learning. In International
Conference on Machine Learning , pages 10‚Äì19, 2018.
[32] Ali Abdollahi, Rakshit Allamraju, and Girish Chowdhary. Adaptive-
optimal control of nonstationary dynamical systems. In Proceedings
of the 2015 European Guidance, Navigation, and Control Conference ,
2015.
[33] Nolan Wagener, Ching-An Cheng, Jacob Sacks, and Byron Boots.
An online learning approach to model predictive control. CoRR ,
abs/1902.08967, 2019.
[34] Karl Johan Astrom and Bjorn Wittenmark. Adaptive Control . Addison-
Wesley Longman Publishing Co., Inc., USA, 2nd edition, 1994.
[35] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv
preprint arXiv:1609.09106 , 2016.
[36] J Urgen Schmidhuber. Learning to control fast-weight memories: an
alternative to dynamic recurrent networks. 1991.
[37] Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud,
and Roger B. Grosse. Self-tuning networks: Bilevel optimization
of hyperparameters using structured best-response functions. CoRR ,
abs/1903.03088, 2019.
[38] Reuven Y Rubinstein. Optimization of computer simulation mod-
els with rare events. European Journal of Operational Research ,
99(1):89‚Äì112, 1997.
[39] Homanga Bharadhwaj, Kevin Xie, and Florian Shkurti. Model-
predictive control via cross-entropy and gradient-based optimization.
Learning for Dynamics and Control , 2020.
[40] Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta,
Joan Creus-Costa, Silvio Savarese, and Li Fei-Fei. Surreal: Open-
source reinforcement learning framework and robot manipulation
benchmark. In Conference on Robot Learning , 2018.
[41] Oussama Khatib. A uniÔ¨Åed approach for motion and force control of
robot manipulators: The operational space formulation. IEEE Journal
on Robotics and Automation , 3(1):43‚Äì53, 1987.
[42] Yusuke Urakami, Alec Hodgkinson, Casey Carlin, Randall Leu, Luca
Rigazio, and Pieter Abbeel. Doorgym: A scalable door opening
environment and baseline agent. arXiv preprint arXiv:1908.01887 ,
2019.
[43] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart ¬¥ƒ±n-Mart ¬¥ƒ±n.
robosuite: A modular simulation framework and benchmark for robot
learning. arXiv preprint arXiv:2009.12293 , 2020.
[44] Xavier Glorot and Yoshua Bengio. Understanding the difÔ¨Åculty
of training deep feedforward neural networks. In Proceedings of
the thirteenth international conference on artiÔ¨Åcial intelligence and
statistics , pages 249‚Äì256, 2010.
[45] Peter Henderson, Wei-Di Chang, Florian Shkurti, Johanna Hansen,
David Meger, and Gregory Dudek. Benchmark environments for multi-
task learning in continuous domains. arXiv preprint arXiv:1708.04352 ,
2017.

--- PAGE 8 ---
Appendix
VIII. A DDITIONAL TESTRESULTS
The objective of a continual learning algorithm is to
continuously learn new knowledge on the current task, while
maintaining its performance on past tasks. To achieve this,
there are two components that needs to be quantiÔ¨Åed: 1)
backward transfer and 2) forward transfer . We formally
deÔ¨Åne them in the context of continual reinforcement learn-
ing as follow:
Backward Transfer First, letri;jdenote the test episodic
(normalized) reward on task iafter training on task j.
Retention for each task is then given by fi=ri;T=ri;ifor all
i < T . The average retention is f=1
T 1PT 1
i=1fi. If this
measure is larger than 100%, it indicates positive backward
transfer , which means the performance on previously seen
tasks beneÔ¨Åt from more recent experiences. If this measure
of retention is lower than 100% , this means that the robot
forgets some knowledge about older tasks and we call this
negative backward transfer orforgetting .
Forward Transfer Forward Transfer measures how the robot
adapts to new tasks after training on older task(s). SpeciÔ¨Å-
cally, letr
idenote the test episodic reward on task ifor a
single-task model trained from scratch using CEM planning.
This single-task model serves as a baseline for performance.
We then calculate forward transfer as Ii=ri;i=r
i, and the
average isI=1
T 1PT
i=2Ii. If this measure is greater than
100%, it indicates positive forward transfer , which means
that the robot beneÔ¨Åts from older experiences when learning
a new task. If this measure is lower than 100% , it means that
the robot is unable to take advantage of past experience to
solve new tasks, also known as negative forward transfer .
% Forward Transfer in terms of Task Reward (Pusher)
Task 2 3 4 5 Average
Multi-task 11823 10612 1059 11021 1099
HyperCRL 12722 9912 9412 10720 1079
SI 11426 8413 9815 10120 9910
EWC 12523 10911 10510 11021 1129
Coreset 12620 9913 9017 10621 1059
Finetuning 13822 1089 9417 10723 1129
% Forward Transfer in terms Task Normalized Reward (Door)
Task 2 3 4 5 Average
Multi-task 10374 9434 122160 7878 9649
HyperCRL 10676 9131 168203 10267 11757
SI 6957 5640 107137 6666 7542
EWC 7162 7832 125151 7161 8644
Coreset 9480 8334 113145 5768 8446
Finetuning 7383 9043 101145 8377 8747
% Forward Transfer in terms Task Reward (Block Sliding)
Task 2 3 4 5 Average
Multi-task 7113 9220 999 9314 897
HyperCRL 928 9714 8239 9414 9111
SI 917 8348 1029 8814 9113
EWC 909 11514 10311 9420 1007
Coreset 6522 10814 11016 9417 959
Finetuning 4842 8819 9725 7629 7715
TABLE II: Forward Transfer on Pusher, Door and Block Sliding Envs .
We measure the average episodic reward at the end of training on task t
compared to training a single-task baseline on the same task from scratch.
Results show the mean and one standard deviation, evaluated across four
seeds and 10 episodes per seed. Greater than 100 indicates positive forward
transfer, otherwise it indicates negative forward transfer.To better illustrate the continual learning capability of
HyperCRL , we provide an evaluation of the positive transfer
capabilities deÔ¨Åned above. We show the result on all three
experiments in Table II.
In the pusher experiment, HyperCRL achieves an average
forward transfer of 10011%. This means that our proposed
method can adapt to new tasks and reach similar level
of performances compared to a single-task baseline trained
from scratch. In the door experiment, HyperCRL beats
all baseline in terms of forward transferring capabilities. In
the block sliding experiment, HyperCRL achieves slightly
lower average forward transfer compared to the best baseline.
A note of caution is that some results have very large
variance across different random seeds. Better techniques to
train hypernetworks or plan with learned dynamics model
would improve the reliability of our approach, and we leave
this to future work.
IX. A DDITIONAL EXPERIMENT ON HALFCHEETAH
We ran another experiment using HyperCRL following
the environment setup from [45], changing the body of the
robot. We modiÔ¨Åed body size (torso, leg) on the HalfCheetah
environment to create a continual learning scenario with T=
5tasks. Our method outperforms other continual learning
baselines (SI, EWC, coreset) and performs similarly to multi-
task learning (Figure 7).
X. M ORE TRAINING DETAILS
In this section, we provide more details about the hyper-
parameters used during the experiments in Table III.
PMKSHe
Pusher 10 20 200 2000 0.0001 0.0001
Door 10 300 200 200 0.0001 0.0001
Block Sliding 10 100 150 500 0.0001 0.0001
Half Cheetah 10 100 1000 2000 0.0001 0.0001
Breg hnet non-linearity hnet hidden layers
Pusher 100 0.05 ELU [50, 50]
Door 100 0.5 ReLU [256, 256]
Block Sliding 100 0.5 ReLU [50, 50]
Half Cheetah 100 0.05 ReLU [256, 256]
TABLE III: Hyper-parameter values of the proposed
algorithm HyperCRL on pusher pusher, door, block sliding and
half cheetah environments.
XI. A DDITIONAL ‚ÄúORACLE ‚ÄùBASELINE
In Figure 6, the multi-head multi-task baseline slightly
underperforms HyperCRL . We hypothesize that the hyper-
network architecture might be more effective for learning the
task. We consider another multi-task baseline that shares the
same architecture of the hypernetwork used in HyperCRL ,
but is trained without regularization and has access to the
entire replay buffer similar to the multi-task baseline. We
call this HyperCRL-MT , and we believe this should serve as
a better upper-bound on the continual learning performance
ofHyperCRL . We compare results on the door-opening
experiment in Figure 8. With full access to all past experi-
ences, HyperCRL-MT slightly outperforms HyperCRL on
this experiment, which shows the efÔ¨Åcacy of our proposed
method.

--- PAGE 9 ---
024Task 11e3
024Task 21e3
024Task 31e3
024Task 41e3
0 1 2 3 4 5
Environment Steps 1e5024Task 51e3
0 1 2 3 4 5
1e5024Average1e3RewardHyperCRL multitask (oracle) SI EWC coreset finetuningFig. 7: Reward on HalfCheetah Environment from [45]. Shown are episodic rewards evaluated during training. Each task is trained for 100k steps,
totaling 500k steps. Results are averaged across four random seeds, and shaded area marks one standard deviation.
0.01.02.0Task 1
0.01.02.0Task 2
0.01.02.0Task 3
0.01.02.0Task 4
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Environment Steps 1e50.01.02.0Task 5
0.0 0.5 1.0 1.5 2.0 2.5 3.0
1e50.01.0AverageNormalized RewardHyperCRL (original) HyperCRL-MT (oracle)
Fig. 8: Additional Hypernetwork Baseline on Door Environment Shown are normalized episodic rewards evaluated during training. HyperCRL-MT
is trained with access to the entire replay buffer. Results are averaged across four random seeds, and normalized with respect to a model trained from
scratch separately on each task.
