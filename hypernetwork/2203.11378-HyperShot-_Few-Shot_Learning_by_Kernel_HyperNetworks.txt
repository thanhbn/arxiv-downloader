# 2203.11378.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hypernetwork/2203.11378.pdf
# File size: 2131780 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Marcin Sendera* 1Marcin Przewi˛ e´ zlikowski* 1Konrad Karanowski2
Maciej Zi˛ eba2 3Jacek Tabor1Przemysław Spurek1
Abstract
Few-shot models aim at making predictions us-
ing a minimal number of labeled examples from
a given task. The main challenge in this area
is the one-shot setting where only one element
represents each class. We propose HyperShot -
the fusion of kernels and hypernetwork paradigm.
Compared to reference approaches that apply a
gradient-based adjustment of the parameters, our
model aims to switch the classiﬁcation module pa-
rameters depending on the task’s embedding. In
practice, we utilize a hypernetwork, which takes
the aggregated information from support data and
returns the classiﬁer’s parameters handcrafted for
the considered problem. Moreover, we introduce
the kernel-based representation of the support ex-
amples delivered to hypernetwork to create the
parameters of the classiﬁcation module. Conse-
quently, we rely on relations between embeddings
of the support examples instead of direct feature
values provided by the backbone models. Thanks
to this approach, our model can adapt to highly
different tasks.
1. Introduction
Current Artiﬁcial Intelligence techniques cannot rapidly
generalize from a few examples. This common inability
is because most deep neural networks must be trained on
large-scale data. In contrast, humans can learn new tasks
quickly by utilizing what they learned in the past. Few-shot
learning models try to ﬁll this gap by learning how to learn
from a limited number of examples. Few-shot learning is the
problem of making predictions based on a small number of
samples. The goal of few-shot learning is not to recognize a
ﬁxed set of labels but to learn how to quickly adapt to new
tasks with a small amount of training data. After training,
*Equal contribution1Faculty of Mathematics and Com-
puter Science, Jagiellonian University, Kraków, Poland
2Wrocław University of Science and Technology, Wrocław,
Poland3Tooploox. Correspondence to: Marcin Sendera
<marcin.sendera@doctoral.uj.edu.pl>.
Under Review.the model can classify new data using only a few training
samples.
The most popular approaches for few-shot learning use the
meta-learning framework. We sample few-shot classiﬁ-
cation tasks from training samples belonging to the base
classes and optimize the model to perform well on these
tasks. We work with N-way andK-shot tasks, where we
haveNclasses with Ksupport samples and Qquery sam-
ples in each category. The goal is to classify these NQ
query samples into the Nclasses based on the NK
support samples.
One of the most common approaches to Few-shot learning
is Model-Agnostic Meta-Learning (MAML) (Finn et al.,
2017), where the model is trained to adapt to new Few-shot
learning tasks quickly. The main idea is to produce uni-
versal weights which can be rapidly updated to solve new
small tasks. The model’s main limitation is the complicated
optimization procedure that uses an inner and outer loop,
which can be seen as second-order optimization. Moreover,
such an optimization procedure only partially implements
thelearn how to learn paradigm, which assumes the model
learns some rules to adapt to the new tasks. The induction of
such rules by MAML is limited by gradient-based optimiza-
tion applied to adjust the parameters of the classiﬁcation
module to the new task. In practice, we can realize learn-
ing rules which are based on gradient optimization. New
classiﬁcation module parameters are obtained by gradient
descent.
This paper introduces a new strategy, which solves the above
problem with restriction only to gradient-based decision
rules. Our goal is to mimic the human learning process.
First, we look at the entire support set and extract the infor-
mation to distinguish objects in classes. Then, based on the
relation between features, we create the decision rules.
We combine the Hypernetworks paradigm with kernel meth-
ods to realize such a scenario, see Fig. 1. Hypernetworks,
introduced in (Ha et al., 2016), are deﬁned as neural models
that generate weights for a separate target network solving a
speciﬁc task. In our approach, the Hypernetwork aggregates
the information from the support set and produces weights
of the target model, classifying the query set.arXiv:2203.11378v1  [cs.LG]  21 Mar 2022

--- PAGE 2 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Figure 1. The overview of HyperShot architecture. We create the kernel matrix for features extracted from support examples, which is
further processed by a hypernetwork. The role of the hypernetwork is to create the set of parameters for the considered task. The target
network further uses the parameters to classify the query examples.
Kernel methods realize the ﬁrst part of the process. For
each of the few-shot tasks, we extract the features from the
support set through the backbone architecture and calculate
kernel values between them. Then we use a Hypernetwork
architecture – a neural network that takes kernel representa-
tion and produces decision rules in the form of a classiﬁer
(target network) dedicated to the query set.
Such a solution realizes the learn how to learn paradigm.
Hypernetwork, which produces the decision rules from ker-
nel representation of the support set, is trained by gradient-
based method and can be seen as a classical learning process.
However, the weights of the target network (decision rules)
can realize different strategies and lay in varying weight
space parts. Our approach allows training a universal archi-
tecture model, which can be quickly updated to new tasks
without any second-order procedure (inner and outer loop).
We perform an extensive experimental study of our approach
by benchmarking it on various one-shot and few-shot image
classiﬁcation tasks. We ﬁnd that HyperShot demonstrates
high accuracy in all tasks, performing comparably or better
than the other recently proposed methods. Moreover, Hy-
perShot shows a strong ability to generalize, as evidenced
by its performance on cross-domain classiﬁcation tasks.
The contributions of our work can be summarized as fol-
lows:
•In this paper, we propose a model which realizes the
learn how to learn paradigm by modeling learning
rules which are not based on gradient optimization andcan produce completely different strategies.
•We propose a new approach to solve the few-shot learn-
ing problem by aggregating information from the sup-
port set and directly producing weights from the neural
network dedicated to the query set.
•We propose HyperShot, which uses the Hypernetwork
paradigm to produce the weights dedicated for each
task.
2. HyperShot: Hypernetwork for few-shot
learning
In this section, we present our HyperShot model for few-
shot learning.
2.1. Background
Few-shot learning The terminology describing the few-
shot learning setup is dispersive due to the colliding deﬁni-
tions used in the literature. For a uniﬁed taxonomy, we refer
the reader to (Chen et al., 2019; Wang et al., 2020). Here,
we use the nomenclature derived from the meta-learning
literature, which is the most prevalent at the time of writing.
Let:
S=f(xl;yl)gL
l=1 (1)
be a support-set containing input-output pairs, with Lex-
amples with the equal class distribution. In the one-shot
scenario, each class is represented by a single example, and
L=K, whereKis the number of the considered classes in

--- PAGE 3 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
the given task. Whereas, for few-shot scenarios, each class
usually has from 2to5representatives in the support set S.
Let:
Q=f(xm;ym)gM
m=1 (2)
be a query-set (sometimes referred to in the literature as
a target-set), with Mexamples, where Mis typically one
order of magnitude greater than K. For ease of notation, the
support and query sets are grouped in a task T=fS;Qg.
During the training stage, the models for few-shot applica-
tions are fed by randomly selected examples from training
setD=fTngN
n=1, deﬁned as a collection of such tasks.
During the inference stage, we consider task T=fS;Xg,
whereSis a support set with the known class values for a
given task, andXis a set of query (unlabeled) inputs. The
goal is to predict the class labels for query inputs x2X,
assuming support set Sand using the model trained on D.
Hypernetwork In the canonical work (Ha et al., 2016),
hyper-networks are deﬁned as neural models that generate
weights for a separate target network solving a speciﬁc
task. The authors aim to reduce the number of trainable
parameters by designing a hyper-network with a smaller
number of parameters than the target network. Making an
analogy between hyper-networks and generative models,
the authors of (Sheikh et al., 2017) use this mechanism to
generate a diverse set of target networks approximating the
same function.
2.2. HyperShot - overview
We introduce our HyperShot - model that utilizes hypernet-
work for few-shot problems. The main idea of the proposed
approach is to predict the values of the parameters for a clas-
siﬁcation network that makes predictions on query images
given the information extracted from support examples for
a given task. Thanks to this approach, we can switch the
classiﬁer’s parameters between completely different tasks
based on the support set. The information about the current
task is extracted from the support set using parametrized
kernel function that operates on embedding space. Thanks
to this approach, we use relations among the support exam-
ples instead of taking the direct values of the embedding
values as an input to the hypernetwork. Consequently, this
approach is robust to the embedding values for new tasks
far from the feature regions observed during training. The
classiﬁcation of the query image is also performed using the
kernel values calculated with respect to the support set.
The architecture of the HyperShot is provided in Fig. 2. We
aim to predict the class distribution p(yjS;x), given a query
image xand set of support examples S=f(xl;yl)gK
l=1.
First, all images from the support set are grouped by theircorresponding class values. Next, each of the images xl
from the support set is transformed using encoding net-
workE(), which creates low-dimensional representations
of the images, E(xl) =zl. The constructed embeddings
are sorted according to class labels and stored in the ma-
trixZS= [z(1);:::;z(K)]T, where()is the bijective
function, that satisﬁes y(l)y(k)forlk.
In the next step we calculate the kernel matrix KS;S, for
vector pairs stored in rows of ZS. To achieve this, we use
the parametrized kernel function k(;), and calculate ki;j
element of matrix KS;Sin the following way:
ki;j=k(z(i);z(j)): (3)
The kernel matrix KS;Srepresents the extracted information
about the relations between support examples for a given
task. The matrix KS;Sis further reshaped to the vector
format and delivered to the input of the hypernetwork H().
The role of the hypernetwork is to provide the parameters
Tof target model T()responsible for the classiﬁcation of
the query object. Thanks to that approach, we can switch
between the parameters for entirely different tasks without
moving via the gradient-controlled trajectory, like in some
reference approaches like MAML.
The query image xis classiﬁed in the following manner.
First, the input image is transformed to low-dimensional
feature representation zxby encoder E(x). Further, the
kernel vector kx;Sbetween the query embedding and sorted
support vectors ZSis calculated in the following way:
kx;S= [k(zx;z(1));:::;k (zx;z(K))]T: (4)
The vector kx;Sis further provided on the input of target
modelT()that is using the parameters Treturned by
hypernetwork H(). The target model returns the probability
distributionp(yjS;x)for each class considered in the task.
2.3. Kernel function
One of the key components of our approach is a kernel
functionk(;). In this work we consider the dot product of
the transformed vectors given by:
k(z1;z2) =f(z1)Tf(z2); (5)
where f()can be a parametrized transformation function,
represented by MLP model, or simply an identity operation,
f(z) =z. In Euclidean space this criterion can be expressed
ask(z1;z2) =jjf(z1)jjjjf(z2)jjcos, whereis an angle
between vectors f(z1)andf(z2). The main feature of this
function is that it considers the vectors’ norms, which can
be problematic for some tasks that are outliers regarding the

--- PAGE 4 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Figure 2. The general architecture of HyperShot. First, the examples from a support set are sorted according to the corresponding
class labels and transformed by encoding network E()to obtain the matrix of ordered embeddings of the support examples, ZS. The
low-dimensional representations stored in ZSare further used to compute kernel matrix KS;S. The values of the kernel matrix are passed
to the hypernetwork H()that creates the parameters Tfor the target classiﬁcation module T(). The query image xis processed by
encoderE(), and the vector of kernel values kx;Sis calculated between query embedding zxand the corresponding representations
of support examples, ZS. The kernel vector kx;Sis further passed to target model T()to obtain the probability distribution for the
considered classes.
representations created by f(). Therefore, we consider in
our experiments also the cosine kernel function given by:
kc(z1;z2) =f(z1)Tf(z2)
jjf(z1)jjjjf(z2)jj; (6)
that represents the normalized version dot product. Con-
sidering the geometrical representation, kc(z1;z2)can be
expressed as cos(see the example given by Fig. 3). The
support set is represented by two examples from different
classes, f1andf2. The target model parameters Tare cre-
ated based only on the cosine value of the angle between
vectors f1andf2. During the classiﬁcation stage, the query
example is represented by fx, and the classiﬁcation is ap-
plied on the cosine values of angles between fxandf1, and
fxandf2, respectively.
2.4. Training and prediction
The training procedure assumes the following parametriza-
tion of the model components. The encoder E:=EE
is parametrized by E, the hypernetwork H=HHby
H, and the kernel function kbyk. We assume that train-
ing setDis represented by tasks Ticomposed of support
Siand queryQiexamples. The training is performed byoptimizing the cross-entropy criterion:
L= X
Ti2DMX
m=1KX
k=1yk
i;mlogp(yk
i;mjSi;xi;m); (7)
where (xi;n;yi;n)are examples from query set Qi, where
Qi=f(xi;m;yi;m)gM
m=1. The distribution for currently
considered classes p(yjS;x)is returned by target network
T of HyperShot. During the training, we jointly optimize
the parameters H,k, and E, minimizing the Lloss.
During the inference stage, we consider the task T, com-
posed of a set of labeled support examples Sand a set
of unlabelled query examples represented by input values
Xthat the model should classify. We can simply take the
probability values p(yjS;x)assuming the given support
setSand single query observation xfromX, using the
model with trained parameters H,k, and E. However,
we observe that slightly better results are obtained while
tuning the model’s parameters on the considered task. We
do not have access to labels for query examples. Therefore
we imitate the query set for this task simply by taking sup-
port examples and creating the tuning task Ti=fS;Sg
and updating the parameters of the model using several gra-
dient iterations. The detailed presentation o training and
prediction procedures are provided by Algorithm 1.

--- PAGE 5 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Figure 3. Simple 2D example illustrating the application of cosine
kernel for HyperShot. We consider the two support examples
from different classes represented by vectors f1andf2. For this
simple scenario, the input of hypernetwork is represented simply
by the cosine of , which is an angle between vectors f1and
f2. We aim at classifying the query example xrepresented by
a vector fx. Considering our approach, we deliver to the target
networkT()the cosine values of angles between ﬁrst ( x;1) and
second (x;2) support vectors and classify the query example
using the weights Tcreated by hypernetwork H()from cos
(remaining components on the diagonal of KS;Sare constant for
cosine kernel).
2.5. Adaptation to few-shot scenarios
The proposed approach uses the ordering function ()that
keeps the consistency between support kernel matrix KS;S
and the vector of kernel values kx;Sfor query example
x. For few-shot scenarios, each class has more than one
representative in the support set. As a consequence, there
are various possibilities to order the feature vectors in the
support set inside the considered class. To eliminate this
issue, we propose to apply the aggregation function to the
embeddings zconsidering the support examples from the
same class. Thanks to this approach, the kernel matrix is
calculated based on the aggregated values of the latent space
of encoding network E, making our approach independent
of the ordering among the embeddings from the same class.
In experimental studies, we examine the quality of mean
aggregation operation ( averaged ) against simple class-wise
concatenation of the embeddings ( ﬁne-grained ) in ablation
studies.
3. Related Work
Feature transfer (Zhuang et al., 2020) is a baseline proce-
dure for few-shot learning and consists of pre-training the
neural network and a classiﬁer. During meta-validation,
the classiﬁer is ﬁne-tuned to the novel tasks. (Chen et al.,
2019) extend this idea by using cosine distance between the
examples.Algorithm 1 HyperShot - training and prediction functions
Require: Training setD=fTngN
n=1, andT=fS;Xgtest
task.
Parameters: H- parameters , k- kernel parameters, and E-
encoder parameters
Hyperparameters: Ntrain - number of training iterations, Ntune
number of tuning iterations, - step size.
1:function TRAIN (D,,Ntrain ,H,k,E)
2: whilenNtrain do
3: Sample task T=fS;QgD
4: Assign support S=f(xm;ym)gM
m=1
5:L= PM
m=1PK
k=1yk
mlogp(yk
mjSi;xm;H;k;E)
6: Update: E E rEL,
7: H H rHL,
8: k k rkL
9:n=n+ 1
10: end while
11: return H,k,E
12:end function
13:function PREDICT (T,,Ntune,H,k,E)
14: Create tuning task: Tt=fS;Sg
15: Tune ^H,^k,^E= T RAIN (Tt,,Ntune,H,k,E)
16: foreachx2X do
17: return arg max yp(yjS;x;^H;^k;^E)
18: end for
19:end function
In recent years, a variety of meta-learning methods
(Hospedales et al., 2020; Schmidhuber, 1992; Bengio et al.,
1992) have been proposed to tackle the problem of few-
shot learning. The various meta-learning architectures for
few-shot learning can be roughly categorized into several
groups:
Memory-based methods (Ravi & Larochelle, 2017;
Munkhdalai et al., 2018; Santoro et al., 2016; Mishra et al.,
2018; Munkhdalai & Yu, 2017; Zhen et al., 2020) are based
on the idea to train a meta-learner with memory to learn
novel concepts.
Metric-based methods meta-learn a deep representation with
a metric in feature space, such that distance between exam-
ples from the support and query set with the same class have
a small distance in such space. Some of the earliest works
exploring this notion are Matching Networks (Vinyals et al.,
2016) and Prototypical Networks (Snell et al., 2017), which
form prototypes based on embeddings of the examples from
the support set in the learned feature space and classify the
query set based on the distance to those prototypes. Numer-
ous subsequent works aim to improve the expressiveness of
the prototypes through various techniques. (Oreshkin et al.,
2018) achieve this by conditioning the network on speciﬁc
tasks, thus making the learned space task-dependent. (Hu
et al., 2021) transform embeddings of support and query
examples in the feature space to make their distributions
closer to Gaussian. (Sung et al., 2018) propose Relation
Nets, which learn the metric function instead of using a

--- PAGE 6 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
ﬁxed one, such as Euclidean or cosine distance.
Similar to the above methods, HyperShot uses a kernel
function that predicts the relations between the examples in a
given task. The key difference is that instead of performing a
nearest-neighbor classiﬁcation based on the kernel values, in
HyperShot, the kernel matrix is classiﬁed by a task-speciﬁc
classiﬁer generated by the hypernetwork.
Optimization-based methods follow the idea of an optimiza-
tion process over support set within the meta-learning frame-
work like MetaOptNet (Lee et al., 2019), Model-Agnostic
Meta-Learning (MAML), and its extensions (Finn et al.,
2017; Nichol et al., 2018; Raghu et al., 2019; Rajeswaran
et al., 2019; Finn et al., 2018; Nichol et al., 2018). Those
techniques aim to train general models, which can adapt
their parameters to the support set at hand in a small number
of gradient steps. Similar to such techniques, HyperShot
also aims to produce task-speciﬁc models but utilizes a hy-
pernetwork instead of optimization to achieve that goal.
Gaussian processes (Rasmussen, 2003) possess many prop-
erties useful in few-shot learning, such as natural robustness
to the limited amounts of data and the ability to estimate
uncertainty. When combined with meta-learned deep ker-
nels, (Patacchiola et al., 2020), Gaussian processes were
demonstrated to be a suitable tool for few-shot regression
and classiﬁcation, dubbed Deep Kernel Transfer (DKT). The
assumption that such a universal deep kernel has enough
data to generalize well to unseen tasks has been challenged
in subsequent works. (Wang et al., 2021) introduced a tech-
nique of learning dense Gaussian processes by inducing
variables. This approach achieves substantial performance
improvement over the alternative methods. Similarly, Hy-
perShot also depends on learning a model that estimates
task-speciﬁc functions’ parameters. However, HyperShot
employs a hypernetwork instead of a Gaussian process to
achieve that goal.
Hypernetworks (Ha et al., 2016) have been proposed as a
solution to few-shot learning problems in a number of works
but have not been researched as widely as the approaches
mentioned above. Multiple works proposed various varia-
tions of hyper-networks that predict a shallow classiﬁer’s
parameters given the support examples (Bauer et al., 2017;
Qiao et al., 2017). Subsequent works have extended those
models by calculating cosine similarity between the query
examples and the generated classiﬁer weights (Gidaris &
Komodakis, 2018) and utilizing a probabilistic model that
predicts a distribution over the parameters suitable for the
given task (Gordon et al., 2018). More recently, (Zhmogi-
nov et al., 2022) explored generating all of the parameters of
the target network with a transformer-based hypernetwork,
but found that for larger target networks, it is sufﬁcient to
generate only the parameters of the ﬁnal classiﬁcation layer.
A key characteristic of the above approaches is that duringinference, the hypernetwork predicts weights responsible
for classifying each class independently, based solely on the
examples of that class from the support set. This property
makes such solutions agnostic to the number of classes in
a task, useful in practical applications. However, it also
means that the hypernetwork does not take advantage of the
inter-class differences in the task at hand.
In contrast, HyperShot exploits those differences by utilizing
kernels, which helps improve its performance.
4. Experiments
In the typical few-shot learning setting, making a valuable
and fair comparison between proposed models is often com-
plicated because of the existence of the signiﬁcant differ-
ences in architectures and implementations of known meth-
ods. In order to limit the inﬂuence of the deeper backbone
(feature extractor) architectures, we follow the uniﬁed pro-
cedure proposed by (Chen et al., 2019).
In this section, we describe the experimental analysis and
performance of the HyperShot in the large variety of few-
shot benchmarks. Speciﬁcally, we consider both classiﬁ-
cation (see Section 4.1) and cross-domain adaptation (see
Section 4.2) tasks. Whereas the classiﬁcation problems are
focused on the most typical few-shot applications, the latter
cross-domain benchmarks check the ability of the models to
adapt to out-of-distribution tasks. Additionally, we perform
an ablation study of the possible adaptation procedures of
HyperShot to few-shot scenarios - presented in Section 4.3.
In all of the reported experiments, the tasks consist of 5
classes (5-way) and 1 or 5 support examples (1 or 5-shot).
Unless indicated otherwise, all compared models use a
known and widely utilized backbone consisting of four con-
volutional layers (each consisting of a 2D convolution, a
batch-norm layer, and a ReLU non-linearity; each layer
consists of 64 channels) and have been trained from scratch.
We report the performance of two variants of HyperShot:
•HyperShot - models generated by the hypernetworks
for each task.
•HyperShot + ﬁnetuning - models generated by hyper-
networks ﬁnetuned to the support examples of each
task for 10 training steps1.
In all cases, we observe a modest performance boost thanks
to ﬁnetuning the hypernetwork.
Comprehensive details for each training procedure are re-
1In the case of the ﬁnetuned hypernetworks, we tune a copy of
the hypernetwork separately for each validation task. This way,
we ensure that our model does not take unfair advantage of the
validation tasks.

--- PAGE 7 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Table 1. The classiﬁcation accuracy results for the inference tasks
onCUB andmini-ImageNet datasets in the 1-shot setting. The
highest results are bold and second-highest in italic (the larger, the
better).
Method CUB mini-ImageNet
ML-LSTM (Ravi & Larochelle, 2017) – 43:440:77
SNAIL (Mishra et al., 2018) – 45:10
iMAML-HF (Rajeswaran et al., 2019) – 49:301:88
LLAMA (Grant et al., 2018) – 49:401:83
VERSA (Gordon et al., 2018) – 48:531:84
Amortized VI (Gordon et al., 2018) – 44:131:78
Meta-Mixture (Jerfel et al., 2019) – 49:601:50
SimpleShot (Wang et al., 2019) – 49:690:19
Feature Transfer (Zhuang et al., 2020) 46:190:64 39:510:23
Baseline++ (Chen et al., 2019) 61:750:95 47:150:49
MatchingNet (Vinyals et al., 2016) 60:191:02 48:250:65
ProtoNet (Snell et al., 2017) 52:521:90 44:191:30
MAML (Finn et al., 2017) 56:110:69 45:390:49
RelationNet (Sung et al., 2018) 62:520:34 48:760:17
DKT + CosSim (Patacchiola et al., 2020) 63:370:19 48:640:45
DKT + BNCosSim (Patacchiola et al., 2020) 62:960:62 49:730:07
GPLDLA (Kim & Hospedales, 2021) 63:400:14 52:580:19
amortized Bayesian
prototype meta-learning (Sun et al., 2021)63:460:98 53:280:91
V AMPIRE (Nguyen et al., 2020) – 51:540:74
PLATIPUS (Finn et al., 2018) – 50:131:86
ABML (Ravi & Beatson, 2018) 49:570:42 45:000:60
Bayesian MAML (Yoon et al., 2018) 55:930:71 53:801:46
OVE PG GP + Cosine (ML) (Snell & Zemel, 2020) 63:980:43 50:020:35
OVE PG GP + Cosine (PL) (Snell & Zemel, 2020) 60:110:26 48:000:24
Reptile (Nichol et al., 2018) – 49:970:32
R2-D2 (Bertinetto et al., 2018) – 48:700:60
VSM (Zhen et al., 2020) – 54:731:60
PPA (Qiao et al., 2017) – 54:530:40
DFSVLwF (Gidaris & Komodakis, 2018) – 56:200:86
HyperShot (ours) 65:270:24 52:420:46
HyperShot + ﬁnetuning (ours) 66:130:26 53:180:45
ported in the Appendix.
4.1. Classiﬁcation
Firstly, we consider a classical few-shot learning scenario,
where all the classiﬁcation tasks (both training and infer-
ence) come from the same dataset. The main aim of the
proposed classiﬁcation experiments is to ﬁnd the ability of
the few-shot models to adapt to never-seen tasks from the
same data distribution.
We benchmark the performance of the HyperShot and other
methods on two challenging and widely considered datasets:
Caltech-USCD Birds ( CUB ) (Wah et al., 2011) and mini-
ImageNet (Ravi & Larochelle, 2017). The following exper-
iments are in the most popular setting, 5-way, consisting
of 5 random classes. In all experiments, the query set of
each task consists of 16 samples for each class (80 in total).
We provide the additional training details in the Appendix.
We compare HyperShot to a vast pool of the state-of-the-art
algorithms, including the canonical methods (like Match-
ing Networks (Vinyals et al., 2016), Prototypical Networks
(Snell et al., 2017), MAML (Finn et al., 2017), and its ex-
tensions) as well as the recently popular Bayesian methods
mostly build upon the Gaussian Processes framework (like
DKT (Patacchiola et al., 2020)).Table 2. The classiﬁcation accuracy results for the inference tasks
onCUB andmini-ImageNet datasets in the 5-shot setting. The
highest results are bold and second-highest in italic (the larger, the
better).
Method CUB mini-ImageNet
ML-LSTM (Ravi & Larochelle, 2017) – 60:600:71
SNAIL (Mishra et al., 2018) – 55:20
VERSA (Gordon et al., 2018) – 67:370:86
Amortized VI (Gordon et al., 2018) – 55:680:91
Meta-Mixture (Jerfel et al., 2019) – 64:600:92
SimpleShot (Wang et al., 2019) – 66:920:17
Feature Transfer 68:400:79 60:510:55
Baseline++ (Chen et al., 2019) 78:510:59 66:180:18
MatchingNet (Vinyals et al., 2016) 75:110:35 62:710:44
ProtoNet (Snell et al., 2017) 75:930:46 64:070:65
MAML (Finn et al., 2017) 74:840:62 61:580:53
RelationNet (Sung et al., 2018) 78:220:07 64:200:28
DKT + CosSim (Patacchiola et al., 2020) 77:730:26 62:850:37
DKT + BNCosSim (Patacchiola et al., 2020) 77:760:62 64:000:09
GPLDLA (Kim & Hospedales, 2021) 78:860:35 –
amortized Bayesian
prototype meta-learning (Sun et al., 2021)80:940:62 70:440:72
V AMPIRE (Nguyen et al., 2020) – 64:310:74
ABML (Ravi & Beatson, 2018) 68:940:16 –
Bayesian MAML (Yoon et al., 2018) – 64:230:69
OVE PG GP + Cosine (ML) (Snell & Zemel, 2020) 77:440:18 64:580:31
OVE PG GP + Cosine (PL) (Snell & Zemel, 2020) 79:070:05 67:140:23
Reptile (Nichol et al., 2018) – 65:990:58
R2-D2 (Bertinetto et al., 2018) – 65:500:60
VSM (Zhen et al., 2020) – 68:010:90
HyperShot 79:800:16 68:780:29
HyperShot + ﬁnetuning 80:070:22 69:620:28
We ﬁrst consider the more challenging 1-shot task and report
the results in Table 1. We further consider the 5-shot setting
and report the results in Table 2. The additional results
comparing methods on larger backbones are included in
Appendix.
In the 1-shot scenario, HyperShot achieves the highest ac-
curacies in the CUB dataset with and without utilizing a
ﬁnetuning procedure ( 66:13% with a ﬁnetuning, 65:27%
without) and performs better than any other model. How-
ever, in the mini-ImageNet dataset, our approach is among
the best models ( 53:18%), slightly losing with DFSVLwF
(Gidaris & Komodakis, 2018) ( 56:20%).
Considering the 5-shot scenario, HyperShot is the second-
best model achieving 80:07% in the CUB dataset and
69:62% in the mini-ImageNet , whereas the best model,
amortized Bayesian prototype meta-learning, is insigniﬁ-
cantly better and achieves 80:94% and70:44% on the men-
tioned datasets, respectively.
The obtained results clearly show that HyperShot achieves
state-of-the-art or comparable results to the best models
on the intensive set of a standard classiﬁcation few-shot
learning setting.
4.2. Cross-domain adaptation
In the cross-domain adaptation setting, the model is eval-
uated on tasks coming from a different distribution than
the one it had been trained on. Therefore, such a task is

--- PAGE 8 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Table 3. The classiﬁcation accuracy results for the inference tasks on cross-domain tasks ( Omniglot!EMNIST and mini-
ImageNet!CUB ) datasets in the 1-shot setting. The highest results are bold and second-highest in italic (the larger, the better).
Omni!EMNIST mini-ImageNet !CUB
Method 1-shot 5-shot 1-shot 5-shot
Feature Transfer 64.221.24 86.100.84 32.770.35 50.340.27
Baseline ++(Chen et al., 2019) 56.84 0.91 80.010.92 39.190.12 57:310:11
MatchingNet (Vinyals et al., 2016) 75.01 2.09 87.411.79 36.980.06 50.720.36
ProtoNet (Snell et al., 2017) 72.04 0.82 87.221.01 33.271.09 52.160.17
MAML (Finn et al., 2017) 72.68 1.85 83.541.79 34.011.25 48.830.62
RelationNet (Sung et al., 2018) 75.62 1.00 87.840.27 37.130.20 51.761.48
DKT (Patacchiola et al., 2020) 75.40 1.10 90:300:4940:140:18 56.401.34
Bayesian MAML (Yoon et al., 2018) 63:940:47 65:260:30 33:520:36 51:350:16
OVE PG GP + Cosine (ML) (Snell & Zemel, 2020) 68:430:67 86:220:20 39:660:18 55:710:31
OVE PG GP + Cosine (PL) (Snell & Zemel, 2020) 77:000:50 87:520:19 37:490:11 57:230:31
HyperShot 78:060:24 89:040:18 39:090:28 57:770:33
HyperShot + ﬁnetuning 80:650:30 90:810:16 40:030:4158:860:38
more challenging than standard classiﬁcation and is a plau-
sible indicator of a model’s ability to generalize. In order to
benchmark the performance of HyperShot in cross-domain
adaptation, we merge data from two datasets so that the
training fold is drawn from the ﬁrst dataset and validation
and testing fold – from another one. Speciﬁcally, we test
HyperShot on two cross-domain classiﬁcation tasks:
mini-ImageNet!CUB (model trained on mini-
ImageNet and evaluated on CUB ) and Omniglot!EM-
NIST in the 1-shot and 5-shot settings. We report the results
in Table 3. In most settings, HyperShot achieves the highest
accuracy, except for 1-shot mini-ImageNet!CUB clas-
siﬁcation, where its accuracy is on par with the accuracy
achieved by DKT (Patacchiola et al., 2020) ( 40:14% and
40:03% achieved by DKT and HyperShot, respectively). We
note that just in the case of regular classiﬁcation, ﬁnetun-
ing the hypernetwork on the individual tasks consistently
improves its performance.
4.3. Aggregating support examples in the 5-shot setting
In HyperShot, the hypernetwork generates the weights of the
information about the support examples, expressed through
the support-support kernel matrix. In the case of 5-way
1-shot classiﬁcation, each task consists of 5support exam-
ples, and therefore, the size of the kernel matrix is (55),
and the input size of the hypernetwork is 25. However,
when the number of support examples grows, increasing
the size of the kernel matrix may be impractical and lead to
overparametrization of the hypernetwork.
Since hypernetworks are known to be sensitive to large
input sizes (Ha et al., 2016), we consider a way to maintain a
constant input size of HyperShot, independent of the number
of support examples of each class by using means of support
embeddings of each class for kernel calculation, instead of
individual embeddings. Prior works suggest that when there
are multiple examples of a class, an averaged embedding ofsuch class represents it sufﬁciently in the embedding space
(Snell et al., 2017).
To verify this approach, in the 5-shot setting, we train Hy-
perShot with two variants of calculating the inputs to the
kernel matrix:
•ﬁne-grained – utilizing a hypernetwork that takes as
an input a kernel matrix between each of the embed-
dings of the individual support examples. This kernel
matrix has a shape of (2525).
•averaged – utilizing a hypernetwork where the kernel
matrix is calculated between the means of embeddings
of each class. The kernel matrix in this approach has a
shape of (55).
We benchmark both variants of HyperShot on the 5-shot
classiﬁcation task on CUB andmini-ImageNet datasets.
Moreover, we compare these approaches also on cross-
domain classiﬁcation between the Omniglot and EMNIST
datasets. We report the accuracies in Table 4. It is evident
that averaging the embeddings before calculating the kernel
matrix yields superior results.
Table 4. The classiﬁcation accuracy results for HyperShot in the
5-shot setting with two variants of the support embeddings ag-
gregation. The performance measured on Omniglot!EMNIST ,
CUB , and mini-ImageNet!CUB tasks. The larger, the better.
Omni!EMNIST CUB mini-ImageNet
HyperShot (ﬁne-grained) 87:550:19 78:050:20 67:070:47
HyperShot (averaged) 89:040:18 79:800:16 69:620:28
5. Conclusion
In this work, we introduced HyperShot — a new framework
that uses kernel methods combined with hypernetworks.

--- PAGE 9 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Our method uses the kernel-based representation of the sup-
port examples and a hypernetwork paradigm to create the
query set’s classiﬁcation module. We concentrate on rela-
tions between embeddings of the support examples instead
of direct feature values. Thanks to this approach, our model
can adapt to highly different tasks.
We evaluate the HyperShot model on various one-shot and
few-shot image classiﬁcation tasks. HyperShot demon-
strates high accuracy in all tasks, performing comparably or
better to state-of-the-art solutions. Furthermore, the model
has a strong ability to generalize, as evidenced by its perfor-
mance on cross-domain classiﬁcation tasks.
References
Bauer, M., Rojas-Carulla, M., ´Swiatkowski, J. B.,
Schölkopf, B., and Turner, R. E. Discriminative k-shot
learning using probabilistic models, 2017.
Bengio, S., Bengio, Y ., Cloutier, J., and Gecsei, J. On the
optimization of a synaptic learning rule. 1992.
Bertinetto, L., Henriques, J. F., Torr, P., and Vedaldi, A.
Meta-learning with differentiable closed-form solvers. In
International Conference on Learning Representations ,
2018.
Chen, W.-Y ., Liu, Y .-C., Kira, Z., Wang, Y .-C. F., and Huang,
J.-B. A closer look at few-shot classiﬁcation. arXiv
preprint arXiv:1904.04232 , 2019.
Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. Emnist:
an extension of mnist to handwritten letters (2017). arXiv
preprint arXiv:1702.05373 , 2017.
Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In Interna-
tional Conference on Machine Learning , pp. 1126–1135.
PMLR, 2017.
Finn, C., Xu, K., and Levine, S. Probabilistic model-
agnostic meta-learning. In Proceedings of the 32nd Inter-
national Conference on Neural Information Processing
Systems , pp. 9537–9548, 2018.
Gidaris, S. and Komodakis, N. Dynamic few-shot visual
learning without forgetting. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pp. 4367–4375, 2018.
Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and
Turner, R. Meta-learning probabilistic inference for pre-
diction. In International Conference on Learning Repre-
sentations , 2018.
Grant, E., Finn, C., Levine, S., Darrell, T., and Grifﬁths, T.
Recasting gradient-based meta-learning as hierarchicalbayes. In International Conference on Learning Repre-
sentations , 2018.
Ha, D., Dai, A., and Le, Q. V . Hypernetworks. arXiv
preprint arXiv:1609.09106 , 2016.
He, K., Zhang, X., Ren, S., and Sun, J. Deep resid-
ual learning for image recognition. arXiv preprint
arXiv:1512.03385 , 2015.
Hospedales, T., Antoniou, A., Micaelli, P., and Storkey, A.
Meta-learning in neural networks: A survey, 2020.
Hu, Y ., Gripon, V ., and Pateux, S. Leveraging the feature
distribution in transfer-based few-shot learning, 2021.
Jerfel, G., Grant, E., Grifﬁths, T. L., and Heller, K. Reconcil-
ing meta-learning and continual learning with online mix-
tures of tasks. In Proceedings of the 33rd International
Conference on Neural Information Processing Systems ,
pp. 9122–9133, 2019.
Kim, M. and Hospedales, T. Gaussian process meta few-
shot classiﬁer learning via linear discriminant laplace
approximation. arXiv preprint arXiv:2111.05392 , 2021.
Kingma, D. and Ba, J. Adam: A method for stochastic
optimization. International Conference on Learning Rep-
resentations , 12 2014.
Lake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J.
One shot learning of simple visual concepts. In Proceed-
ings of the annual meeting of the cognitive science society ,
volume 33, 2011.
Lee, K., Maji, S., Ravichandran, A., and Soatto, S. Meta-
learning with differentiable convex optimization. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pp. 10657–10665, 2019.
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P. A
simple neural attentive meta-learner. In International
Conference on Learning Representations , 2018.
Munkhdalai, T. and Yu, H. Meta networks. In Interna-
tional Conference on Machine Learning , pp. 2554–2563.
PMLR, 2017.
Munkhdalai, T., Yuan, X., Mehri, S., and Trischler, A. Rapid
adaptation with conditionally shifted neurons. In Interna-
tional Conference on Machine Learning , pp. 3664–3673.
PMLR, 2018.
Nguyen, C., Do, T.-T., and Carneiro, G. Uncertainty in
model-agnostic meta-learning using variational inference.
InProceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision , pp. 3090–3100, 2020.

--- PAGE 10 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Nichol, A., Achiam, J., and Schulman, J. On
ﬁrst-order meta-learning algorithms. arXiv preprint
arXiv:1803.02999 , 2018.
Oreshkin, B. N., Rodriguez, P., and Lacoste, A. Tadam:
Task dependent adaptive metric for improved few-shot
learning. arXiv preprint arXiv:1805.10123 , 2018.
Patacchiola, M., Turner, J., Crowley, E. J., O’Boyle, M., and
Storkey, A. J. Bayesian meta-learning for the few-shot
setting via deep kernels. Advances in Neural Information
Processing Systems , 33, 2020.
Qiao, S., Liu, C., Shen, W., and Yuille, A. Few-shot image
recognition by predicting parameters from activations,
2017.
Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. Rapid
learning or feature reuse? towards understanding the
effectiveness of maml. arXiv preprint arXiv:1909.09157 ,
2019.
Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S.
Meta-learning with implicit gradients. Advances in Neu-
ral Information Processing Systems , 32:113–124, 2019.
Rasmussen, C. E. Gaussian processes in machine learn-
ing. In Summer school on machine learning , pp. 63–71.
Springer, 2003.
Ravi, S. and Beatson, A. Amortized bayesian meta-learning.
InInternational Conference on Learning Representations ,
2018.
Ravi, S. and Larochelle, H. Optimization as a model for
few-shot learning. In ICLR , 2017.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., et al. Imagenet large scale visual recognition chal-
lenge. International journal of computer vision , 115(3):
211–252, 2015.
Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and
Lillicrap, T. Meta-learning with memory-augmented neu-
ral networks. In International conference on machine
learning , pp. 1842–1850. PMLR, 2016.
Schmidhuber, J. Learning to Control Fast-Weight Memories:
An Alternative to Dynamic Recurrent Networks. Neural
Computation , 4(1):131–139, 01 1992. ISSN 0899-7667.
doi: 10.1162/neco.1992.4.1.131. URL https://doi.
org/10.1162/neco.1992.4.1.131 .
Sheikh, A.-S., Rasul, K., Merentitis, A., and Bergmann, U.
Stochastic maximum likelihood optimization via hyper-
networks. arXiv preprint arXiv:1712.01141 , 2017.Snell, J. and Zemel, R. Bayesian few-shot classiﬁcation with
one-vs-each pólya-gamma augmented gaussian processes.
InInternational Conference on Learning Representations ,
2020.
Snell, J., Swersky, K., and Zemel, R. S. Prototypi-
cal networks for few-shot learning. arXiv preprint
arXiv:1703.05175 , 2017.
Sun, Z., Wu, J., Li, X., Yang, W., and Xue, J.-H. Amortized
bayesian prototype meta-learning: A new probabilistic
meta-learning approach to few-shot image classiﬁcation.
InInternational Conference on Artiﬁcial Intelligence and
Statistics , pp. 1414–1422. PMLR, 2021.
Sung, F., Yang, Y ., Zhang, L., Xiang, T., Torr, P. H., and
Hospedales, T. M. Learning to compare: Relation net-
work for few-shot learning. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pp. 1199–1208, 2018.
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.
Matching networks for one shot learning. Advances in
neural information processing systems , 29:3630–3638,
2016.
Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie,
S. The Caltech-UCSD Birds-200-2011 Dataset. Tech-
nical Report CNS-TR-2011-001, California Institute of
Technology, 2011.
Wang, Y ., Chao, W.-L., Weinberger, K. Q., and van der
Maaten, L. Simpleshot: Revisiting nearest-neighbor
classiﬁcation for few-shot learning. arXiv preprint
arXiv:1911.04623 , 2019.
Wang, Y ., Yao, Q., Kwok, J., and Ni, L. M. Generalizing
from a few examples: A survey on few-shot learning,
2020.
Wang, Z., Miao, Z., Zhen, X., and Qiu, Q. Learning to learn
dense gaussian processes for few-shot learning. Advances
in Neural Information Processing Systems , 34, 2021.
Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y ., and Ahn, S.
Bayesian model-agnostic meta-learning. In Proceedings
of the 32nd International Conference on Neural Informa-
tion Processing Systems , pp. 7343–7353, 2018.
Zhen, X., Du, Y .-J., Xiong, H., Qiu, Q., Snoek, C., and Shao,
L. Learning to learn variational semantic memory. In
NeurIPS , 2020.
Zhmoginov, A., Sandler, M., and Vladymyrov, M. Hy-
pertransformer: Model generation for supervised and
semi-supervised few-shot learning, 2022.
Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y ., Zhu, H., Xiong,
H., and He, Q. A comprehensive survey on transfer
learning, 2020.

--- PAGE 11 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
A. Additional results - ResNet10
This section provides additional results in the 5-way ( 1-shot and 5-shot) classiﬁcation tasks for models using a larger
backbone, namely ResNet-10 (He et al., 2015). We provide the results for CUB andmini-ImageNet datasets in Tables 5
and 6.
In the CUB dataset classiﬁcation tasks (see Table 5), HyperShot is amongst the state-of-the-art models achieving classiﬁcation
accuracy often equal within the variance to the best models. Considering the 5-shot scenario, the highest classiﬁcation result
across the evaluated methods ( 86:38%0:15) obtained the GPLDLA model based on the Gaussian Processes framework.
However, the HyperShot performance, 86:28%0:29, is the second-best but even lies within the variance of the best
model. In the 1-shot setting, ProtoNet obtains the highest result ( 73:22%0:92), whereas HyperShot is the third one
(71:99%0:70) but still equal according to the variances.
In the mini-ImageNet classiﬁcation task, HyperShot achieves the second-best accuracy in both 1-shot and 5-shot settings2.
In the 1-shot setting, the DKT model (Patacchiola et al., 2020) achieved the best result, with HyperShot being a close second,
with only 0:04pp difference. In the 5-shot setting, the baseline++ approach outperforms all others by a large margin (Chen
et al., 2019), whereas HyperShot and ProtoNet (Snell et al., 2017) achieve similar, second-best results. We observe that
apart from HyperShot, which achieves second-best results in both settings, models which perform well in one setting are
outperformed by others in the second and vice versa.
It is worth noticing that HyperShot without ﬁnetuning steps performances sometimes slightly better than the same with
ﬁnetuning. We even observe that a few ﬁrst steps of ﬁnetuning procedure result in an unnoticeable increase of accuracy
of the basic model. However, the usual 10steps result in this setting in slightly worse performance, so one should use it
cautiously. We decided to report the results after the standard ﬁnetuning procedure only.
Table 5. The classiﬁcation accuracy results for the inference tasks in the CUB dataset in the 5-way (1-shot and 5-shot) scenarios. We
consider models using the ResNet-10 backbone. The highest results are bold and second-highest in italic (the larger, the better).
Method 1-shot 5-shot
Feature Transfer 63:640:91 81:270:57
Baseline++ (Chen et al., 2019) 69:550:89 85:170:50
MatchingNet (Vinyals et al., 2016) 71:290:87 83:470:58
ProtoNet (Snell et al., 2017) 73:220:92 85:010:52
MAML (Finn et al., 2017) 70:320:99 80:930:71
RelationNet (Sung et al., 2018) 70:470:99 83:700:55
DKT + CosSim (Patacchiola et al., 2020) 70:810:52 83:260:50
DKT + BNCosSim (Patacchiola et al., 2020) 72:270:30 85:640:29
SimpleShot (Wang et al., 2019) 53:780:21 71:410:17
GPLDLA (Kim & Hospedales, 2021) 71:300:16 86:380:15
HyperShot 71:990:70 86:280:29
HyperShot + ﬁnetuning 71:600:59 86:220:30
B. Training details
In this section, we present in detail the architecture and hyperparameters of HyperShot.
Architecture overview From a high-level perspective, the architecture of HyperShot consists of three parts:
• backbone - a convolutional feature extractor.
• neck - a sequence of zero or more fully-connected layers with ReLU nonlinearities in between.
•heads - for each parameter of the target network, a sequence of one or more linear layers, which predicts the values
2In the case of the mini-ImageNet classiﬁcation with ResNet10, we benchmarked all of the listed models ourselves. To our best
knowledge, previously, there were no reported benchmarks on this dataset with the ResNet-10 backbone.

--- PAGE 12 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Table 6. The classiﬁcation accuracy results for the inference tasks in the mini-ImageNet dataset in the 5-way (1-shot and 5-shot) scenarios.
We consider models using the ResNet-10 backbone. The highest results are bold and second-highest in italic (the larger, the better).
Method 1-shot 5-shot
Baseline++ (Chen et al., 2019) 54:350:34 75:260:16
MatchingNet (Vinyals et al., 2016) 54:180:09 67:710:20
ProtoNet (Snell et al., 2017) 53:280:17 73:040:15
RelationNet (Sung et al., 2018) 51:880:45 67:210:16
DKT + BNCosSim (Patacchiola et al., 2020) 56:030:50 71:280:12
HyperShot 55:360:64 73:060:30
HyperShot + ﬁnetuning 55:990:63 72:870:33
of that parameter. All heads of HyperShot have identical lengths, hidden sizes, and input sizes that depend on the
generated parameter’s size.
The target network generated by HyperShot re-uses its backbone. We outline this architecture in Figure 4.
Backbone For each experiment described in the main body of this work, we follow (Patacchiola et al., 2020) in using
a shallow backbone (feature extractor) for HyperShot as well as referential models. This backbone consists of four
convolutional layers, each consisting of a convolution, batch normalization, and ReLU nonlinearity. Apart from the ﬁrst
convolution, which has the number of input size equal to the number of image channels, each convolution has an input and
output size of 64. We apply max-pooling between each convolution, which decreases by half the resolution of the processed
feature maps. The output of the backbone is ﬂattened so that the further layers can process it.
We perform additional experiments described in Appendix A where instead of the above backbone, we utilize ResNet-10
(He et al., 2015).
Datasets For the purpose of making a fair comparison, we follow the procedure presented in, e.g., (Patacchiola et al.,
2020; Chen et al., 2019). In the case of the CUB dataset (Wah et al., 2011), we split the whole amount of 200classes
(11788 images) across train, validation, and test consisting of 100,50, and 50classes, respectively (Chen et al., 2019). The
mini-ImageNet dataset (Ravi & Larochelle, 2017) is created as the subset of ImageNet (Russakovsky et al., 2015), which
consists of 100different classes represented by 600images for each one. We followed the standard procedure and divided
themini-ImageNet into64classes for the train, 16for the validation set, and the remaining 20classes for the test. The
well-known Omniglot dataset (Lake et al., 2011) is a collection of characters from 50different languages. The Omniglot
contains 1623 white and black characters in total. We utilize the standard procedure to include the examples rotated by 90
and increase the size of the dataset to 6492 , from which 4114 were further used in training. Finally, the EMNIST dataset
(Cohen et al., 2017) collects the characters and digits coming from the English alphabet, which we split into 31classes for
the test and 31for validation.
Data augmentation We apply data augmentation during model training in all experiments, except Omniglot!EMNIST
cross-domain classiﬁcation. The augmentation pipeline is identical to the one used by (Patacchiola et al., 2020) and consists
of the random crop, horizontal ﬂip, and color jitter steps.
B.1. Hyperparameters
Below, we outline the hyperparameters of architecture and training procedures used in each experiment.
We use cosine similarity as a kernel function and averaged support embeddings aggregation in all experiments. HyperShot
is trained with the learning rate of 0:001with the Adam optimizer (Kingma & Ba, 2014) and no learning rate scheduler.
Task-speciﬁc ﬁnetuning is also performed with the Adam optimizer and the learning rate of 0:0001 .
For the natural image tasks ( CUB ,mini-ImageNet ,mini-ImageNet!CUB classiﬁcation), we use a hypernetwork with the
neck length o 2, head lengths of 3, and a hidden size of 4096 , which produce a target network with a single fully-connected
layer. We perform training for 10000 epochs.

--- PAGE 13 ---
HyperShot: Few-Shot Learning by Kernel HyperNetworks
Figure 4. A detailed outline of the architecture of HyperShot, with the denoted ﬂow of parameters generated by the hypernetwork heads.
Table 7. Hyperparameters
hyperparameter CUB mini-ImageNet mini-ImageNet !CUB Omniglot !EMNIST
kernel function cosine similarity cosine similarity cosine similarity cosine similarity
learning rate 0:001 0 :001 0 :001 0 :001
hypernetwork’s head layers no. 3 3 3 2
hypernetwork’s neck layers no. 2 2 2 1
hypernetwork layers’ hidden dim 4096 4096 4096 512
support embeddings aggregation averaged averaged averaged averaged
taskset size 1 1 1 1
target network layers no. 1 1 1 2
target network activation ReLU ReLU ReLU ReLU
ﬁnetuning epochs (if used) 10 10 10 10
ﬁnetuning learning rate 0:0001 0 :0001 0 :0001 0 :0001
optimizer Adam Adam Adam Adam
epochs no. 10000 10000 10000 2000
For the simpler Omniglot!EMNIST character classiﬁcation task, we train a smaller hypernetwork with the neck length
of1, head lengths of 2, and the hidden size of 512, which produces a target network with two fully-connected layers and a
hidden size of 128. We train this hypernetwork for a shorter number of epochs, namely 2000 .
We summarize all the above hyperparameters in Table 7.
