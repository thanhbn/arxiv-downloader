# Các tham số hóa bất biến độ lớn cải thiện việc học của siêu mạng

Jose Javier Gonzalez Ortiz
MIT CSAIL
josejg@mit.edu

John Guttag
MIT CSAIL
guttag@mit.edu

Adrian V. Dalca
MIT CSAIL & HMS, MGH
adalca@mit.edu

## Tóm tắt

Siêu mạng, các mạng thần kinh dự đoán tham số của một mạng thần kinh khác, là những mô hình mạnh mẽ đã được sử dụng thành công trong nhiều ứng dụng đa dạng từ tạo sinh hình ảnh đến học đa nhiệm. Thật không may, các siêu mạng hiện tại thường khó huấn luyện. Việc huấn luyện thường hội tụ chậm hơn nhiều so với các mô hình không phải siêu mạng, và tốc độ hội tụ có thể rất nhạy cảm với việc lựa chọn siêu tham số. Trong công trình này, chúng tôi xác định một vấn đề cơ bản và chưa được nhận diện trước đây góp phần vào thách thức của việc huấn luyện siêu mạng: một mối quan hệ tỷ lệ thuận về độ lớn giữa đầu vào và đầu ra của siêu mạng. Chúng tôi chứng minh cả về mặt phân tích và thực nghiệm rằng điều này có thể dẫn đến tối ưu hóa không ổn định, từ đó làm chậm quá trình hội tụ, và đôi khi thậm chí ngăn cản việc học. Chúng tôi đưa ra một giải pháp đơn giản cho vấn đề này sử dụng một công thức siêu mạng được chỉnh sửa mà chúng tôi gọi là Các tham số hóa bất biến độ lớn (MIP). Chúng tôi chứng minh giải pháp được đề xuất trên nhiều nhiệm vụ siêu mạng, nơi nó liên tục ổn định quá trình huấn luyện và đạt được sự hội tụ nhanh hơn. Hơn nữa, chúng tôi thực hiện một nghiên cứu loại bỏ toàn diện bao gồm các lựa chọn hàm kích hoạt, chiến lược chuẩn hóa, số chiều đầu vào, và kiến trúc siêu mạng; và thấy rằng MIP cải thiện việc huấn luyện trong tất cả các kịch bản. Chúng tôi cung cấp mã dễ sử dụng có thể biến các mạng hiện tại thành siêu mạng dựa trên MIP.

## 1. Giới thiệu

Siêu mạng, các mạng thần kinh dự đoán tham số của một mạng thần kinh khác, là những mô hình ngày càng quan trọng trong nhiều ứng dụng như tối ưu hóa Bayesian, mô hình tạo sinh, học mô hình khấu hao, học liên tục, học đa nhiệm, và học meta. Mặc dù có những ưu điểm và việc sử dụng ngày càng tăng, việc huấn luyện siêu mạng là một thách thức. So với các mô hình không dựa trên siêu mạng, việc huấn luyện các siêu mạng hiện tại thường không ổn định. Trong trường hợp tốt nhất, điều này làm tăng thời gian huấn luyện, và trong trường hợp xấu nhất có thể ngăn cản việc huấn luyện hội tụ hoàn toàn. Gánh nặng này hạn chế việc áp dụng chúng, tác động tiêu cực đến nhiều ứng dụng. Các thủ thuật siêu mạng hiện tại, như cắt gradient, không đủ trong nhiều trường hợp, trong khi các kỹ thuật hiện tại nhằm cải thiện việc huấn luyện mạng thần kinh tiêu chuẩn thường thất bại khi áp dụng cho siêu mạng.

Công trình này giải quyết một nguyên nhân gây ra sự không ổn định trong huấn luyện. Chúng tôi xác định và đặc trưng hóa một vấn đề thiết kế siêu mạng chưa được nhận diện trước đây và cung cấp một giải pháp đơn giản để giải quyết nó. Chúng tôi chứng minh về mặt phân tích và thực nghiệm rằng những lựa chọn điển hình về kiến trúc và khởi tạo tham số trong siêu mạng gây ra một mối quan hệ tỷ lệ thuận giữa quy mô của đầu vào siêu mạng và quy mô của đầu ra tham số. Những biến động kết quả trong quy mô tham số được dự đoán dẫn đến những biến động lớn trong quy mô của gradient trong quá trình tối ưu hóa, gây ra việc huấn luyện không ổn định và hội tụ chậm. Trong một số trường hợp, hiện tượng này thậm chí ngăn cản việc học có ý nghĩa. Để khắc phục vấn đề này, chúng tôi đề xuất một sự chỉnh sửa đơn giản cho các mô hình siêu mạng: Các tham số hóa bất biến độ lớn (MIP). MIP hiệu quả loại bỏ ảnh hưởng của quy mô đầu vào siêu mạng lên quy mô của các tham số được dự đoán, trong khi vẫn giữ lại sức mạnh biểu diễn của các công thức hiện tại. Chúng tôi chứng minh hiệu quả của giải pháp đề xuất trên nhiều nhiệm vụ học siêu mạng, cung cấp bằng chứng rằng các siêu mạng sử dụng MIP đạt được sự hội tụ nhanh hơn mà không làm giảm độ chính xác của mô hình.

Những đóng góp chính của chúng tôi là:
• Chúng tôi đặc trưng hóa một vấn đề tối ưu hóa chưa được nhận diện trước đây trong việc huấn luyện siêu mạng, và cho thấy rằng nó dẫn đến phương sai gradient lớn và động lực học huấn luyện không ổn định.
• Chúng tôi đề xuất một giải pháp: Các tham số hóa bất biến độ lớn (MIP), một công thức siêu mạng giải quyết vấn đề mà không đưa ra chi phí huấn luyện hoặc suy luận bổ sung.
• Chúng tôi nghiên cứu kỹ lưỡng việc tham số hóa được đề xuất. Đầu tiên chúng tôi so sánh nó với công thức tiêu chuẩn và với các chiến lược chuẩn hóa phổ biến, cho thấy rằng nó liên tục dẫn đến sự hội tụ nhanh hơn và việc huấn luyện ổn định hơn. Sau đó chúng tôi kiểm tra nó một cách toàn diện sử dụng nhiều lựa chọn tối ưu hóa, số chiều đầu vào, kiến trúc siêu mạng, và hàm kích hoạt, thấy rằng nó cải thiện việc huấn luyện siêu mạng trong tất cả các thiết lập được đánh giá.
• Chúng tôi phát hành triển khai của chúng tôi như một thư viện PyTorch mã nguồn mở, HyperLight. HyperLight tạo điều kiện thuận lợi cho việc phát triển các mô hình siêu mạng và cung cấp các lựa chọn có nguyên tắc cho việc tham số hóa và khởi tạo, làm cho việc áp dụng siêu mạng dễ tiếp cận hơn. Chúng tôi cũng cung cấp mã cho phép sử dụng MIP một cách liền mạch với các mô hình hiện tại.

## 2. Công trình liên quan

Nghiên cứu về tính ổn định và hiệu quả huấn luyện của mạng thần kinh bao gồm nhiều chiến lược khác nhau, bao gồm các chiến lược khởi tạo tham số, kỹ thuật chuẩn hóa, và tối ưu hóa thích ứng.

**Khởi tạo tham số**. Các mạng thần kinh sâu gặp phải động lực học huấn luyện không ổn định khi có gradient bùng nổ hoặc biến mất. Khởi tạo trọng số đóng vai trò quan trọng trong độ lớn của gradient, đặc biệt trong các giai đoạn đầu của việc huấn luyện. Thông thường, các chiến lược khởi tạo trọng số tập trung vào việc bảo toàn độ lớn của các kích hoạt trong quá trình truyền tiến và duy trì độ lớn của gradient trong quá trình truyền ngược. Công trình của chúng tôi chứng minh rằng các chiến lược khởi tạo hiện tại có thể không hiệu quả khi áp dụng cho siêu mạng.

**Kỹ thuật chuẩn hóa**. Các kỹ thuật chuẩn hóa kiểm soát phân phối của trọng số và kích hoạt, thường dẫn đến cải thiện trong sự hội tụ bằng cách làm mịn bề mặt loss. Chuẩn hóa theo lô được sử dụng rộng rãi để chuẩn hóa các kích hoạt sử dụng thống kê minibatch, và các phương pháp như chuẩn hóa lớp hoặc nhóm thay vào đó chuẩn hóa theo các đặc trưng. Các phương pháp khác tái tham số hóa các trọng số sử dụng các chiến lược chuẩn hóa trọng số hoặc sử dụng các mạng tự chuẩn hóa. Như chúng tôi cho thấy trong các thí nghiệm, các chiến lược này thất bại trong việc giải quyết vấn đề tỷ lệ thuận mà chúng tôi nghiên cứu. Chúng hoặc duy trì mối quan hệ tỷ lệ thuận (như trong chuẩn hóa theo lô), hoặc loại bỏ tỷ lệ thuận bằng cách làm cho các trọng số được dự đoán độc lập với đầu vào siêu mạng (như trong chuẩn hóa lớp), loại bỏ tính hữu ích của chính siêu mạng.

**Tối ưu hóa thích ứng**. Phương sai gradient cao có thể có hại cho sự hội tụ của mô hình trong các phương pháp gradient ngẫu nhiên. Các giải pháp để giảm thiểu phương sai gradient bao gồm các kỹ thuật tối ưu hóa thích ứng, nhằm tách biệt hiệu ứng của hướng gradient và độ lớn gradient bằng cách chuẩn hóa theo lịch sử của các độ lớn gradient trước đó. Tương tự, việc áp dụng momentum giảm tác động tức thời của gradient ngẫu nhiên bằng cách sử dụng các cập nhật tham số dựa trên trung bình suy giảm theo hàm mũ của các gradient trong quá khứ. Các chiến lược này được triển khai bởi nhiều bộ tối ưu hóa được sử dụng rộng rãi, chẳng hạn như Adam. Các thí nghiệm của chúng tôi cho thấy rằng mặc dù các bộ tối ưu hóa thích ứng như Adam tăng cường tối ưu hóa siêu mạng, chúng không giải quyết được nguyên nhân gốc rễ của vấn đề tỷ lệ thuận đã xác định, và hầu hết các vấn đề hội tụ vẫn tồn tại.

**Đặc trưng Fourier**. Các phép chiếu Fourier chiều cao đã được sử dụng trong kỹ thuật đặc trưng và cho mã hóa vị trí trong các ứng dụng mô hình ngôn ngữ để tính đến cả mối quan hệ tầm gần và tầm xa. Ngoài ra, các mô hình biểu diễn thần kinh ẩn hưởng lợi từ các biểu diễn hình sin. Công trình của chúng tôi cũng sử dụng các phép chiếu Fourier chiều thấp. Chúng tôi chứng minh việc sử dụng chúng như một phương tiện để chiếu đầu vào siêu mạng vào một không gian vector có chuẩn Euclide không đổi, giảm thiểu thách thức huấn luyện.

**Dạng thức dư**. Các kết nối dư và bỏ qua được sử dụng rộng rãi trong các mô hình học sâu và thường cải thiện việc huấn luyện mô hình, đặc biệt với độ sâu mạng tăng lên. Dựa trên trực quan này, thay vì các siêu mạng dự đoán trực tiếp các tham số mạng, các siêu mạng được đề xuất của chúng tôi dự đoán các thay đổi tham số, giảm thiểu một phần vấn đề tỷ lệ thuận hiện tại.

## 3. Vấn đề tỷ lệ thuận của siêu mạng

**Kiến thức cơ bản**. Các nhiệm vụ học sâu thường liên quan đến một mô hình f(x;θ)→y, với các tham số có thể học θ. Trong các mô hình phân cấp sử dụng siêu mạng, các tham số θ của mạng chính f được dự đoán bởi một siêu mạng h(γ;ω)→θ dựa trên vector đầu vào γ. Thay vì học trực tiếp các tham số θ của mạng chính f, chỉ các tham số có thể học ω của siêu mạng h được tối ưu hóa sử dụng lan truyền ngược. Bản chất cụ thể của các đầu vào siêu mạng γ thay đổi theo các ứng dụng, nhưng thường tương ứng với một đại lượng chiều thấp mô hình các thuộc tính của nhiệm vụ học, và thường là một scalar đơn giản hoặc vector nhúng.

**Giả định**. Cho phân tích của chúng tôi, chúng tôi giả định điều sau về công thức siêu mạng: 1) Kiến trúc là một chuỗi các lớp kết nối đầy đủ có dạng ϕ(Wx+b) trong đó W là các tham số, b là các bias và ϕ(x) là hàm kích hoạt phi tuyến; 2) Hàm kích hoạt thỏa mãn ϕ(x) = max(αx,0) + min(βx,0) cho một số hệ số α và β. Điều này bao gồm các lựa chọn thông thường như ReLU, LeakyReLU hoặc PReLU; 3) Các vector bias b được khởi tạo về zero. Các siêu mạng hiện tại thỏa mãn các thuộc tính này cho phần lớn các ứng dụng.

**Tỷ lệ thuận đầu vào-đầu ra**. Chúng tôi chứng minh rằng dưới các thiết lập được sử dụng rộng rãi này, đầu vào và đầu ra của siêu mạng liên quan đến một mối quan hệ tỷ lệ thuận, và mô tả cách điều này có thể cản trở việc huấn luyện siêu mạng. Chúng tôi cho thấy rằng 1) tại khởi tạo, bất kỳ vector đặc trưng trung gian x^(k) tại lớp k sẽ tỷ lệ thuận với đầu vào siêu mạng γ, ngay cả khi có các hàm kích hoạt phi tuyến, và 2) điều này dẫn đến những biến động độ lớn gradient lớn có hại cho tối ưu hóa.

Đầu tiên chúng tôi xem xét trường hợp γ∈R là một giá trị scalar. Cho h(γ;ω) sử dụng một kiến trúc kết nối đầy đủ bao gồm một chuỗi các lớp kết nối đầy đủ

h(γ;ω)=W^(n)x^(n)+b^(n)
x^(k+1)=ϕ(W^(k)x^(k)+b^(k))
x^(1)=γ

trong đó x^(k) là vector đầu vào của lớp kết nối đầy đủ thứ k với các tham số có thể học W^(k) và bias b^(k). Để ngăn gradient bùng nổ hoặc biến mất khi nối nhiều lớp, việc khởi tạo các tham số W^(i) và bias b^(i) sao cho hoặc độ lớn của các kích hoạt xấp xỉ không đổi qua các lớp trong quá trình truyền tiến (được gọi là fan in), hoặc sao cho độ lớn của gradient không đổi qua các lớp trong quá trình truyền ngược (được gọi là fan out) là phổ biến. Trong cả hai thiết lập, các tham số W^(i) được khởi tạo sử dụng phân phối Chuẩn trung bình zero và các vector bias b^(i) được khởi tạo về zero. Nếu γ > 0, và ϕ(x) có dạng thông thường được chỉ định ở trên, tại khởi tạo, phần tử thứ i của vector x^(2) là

x_i^(2)=ϕ(W_i^(1)γ+b^(1))=γϕ(W_i^(1))∝γ,

vì b^(1) = 0 và ϕ(W_i^(1)) độc lập với γ. Sử dụng quy nạp, chúng tôi giả định rằng cho lớp k, x_j^(k)∝γ ∀j, và chỉ ra thuộc tính này cho lớp k+1. Giá trị của phần tử thứ i của vector x^(k+1) là

x_i^(k+1) = ϕ(b_i^(k)+∑_j W_{ij}^(k)x_j^(k)) = γϕ(∑_j W_{ij}^(k)α_j^(k)) ∝γ,

vì b_i^(k)=0, và số hạng bên trong ϕ độc lập với γ. Nếu γ không hoàn toàn dương, chúng ta có thể đạt được kết quả tỷ lệ thuận tương tự, nhưng với các hằng số riêng biệt cho dải dương và dải âm. Sự phụ thuộc này giữ nguyên bất kể số lượng lớp và số lượng neuron trên mỗi lớp ẩn, và cũng giữ nguyên khi các kết nối dư được sử dụng.

Khi γ là một đầu vào vector, chúng tôi thấy một mối quan hệ tương tự với độ lớn tổng thể của đầu vào và độ lớn của đầu ra. Với việc không có các số hạng bias, và thiếu các tương tác nhân trong kiến trúc, mạng kết nối đầy đủ lan truyền các thay đổi độ lớn trong đầu vào. Chúng tôi cung cấp thêm chi tiết trong tài liệu bổ sung.

**Ý nghĩa huấn luyện**. Vì θ=x^(n+1), kết quả này dẫn đến một mối quan hệ tỷ lệ thuận cho độ lớn của các tham số được dự đoán ||θ||_2∝||γ|| và phương sai của chúng Var(θ)∝||γ||^2. Vì quy mô của các tham số mạng chính θ sẽ phụ thuộc vào γ, điều này sẽ ảnh hưởng đến quy mô của các đầu ra lớp và gradient của mạng chính. Đổi lại, những biến động độ lớn gradient lớn này dẫn đến động lực học huấn luyện không ổn định cho các phương pháp gradient ngẫu nhiên giảm dần.

**Xem xét thêm**. Phân tích của chúng tôi dựa trên việc bias ở mức zero, điều này chỉ đúng tại khởi tạo, và không bao gồm các lớp chuẩn hóa đôi khi được sử dụng. Tuy nhiên, trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng bias vẫn gần zero trong quá trình huấn luyện sớm, và các siêu mạng với các lựa chọn thay thế của hàm kích hoạt, số chiều đầu vào, hoặc với các lớp chuẩn hóa, vẫn gặp phải vấn đề đã xác định và liên tục hưởng lợi từ việc tham số hóa được đề xuất của chúng tôi.

## 4. Các tham số hóa bất biến độ lớn

Để giải quyết sự phụ thuộc tỷ lệ thuận, chúng tôi thực hiện hai thay đổi đơn giản cho công thức siêu mạng điển hình: 1) Chúng tôi giới thiệu một hàm mã hóa ánh xạ đầu vào vào một không gian vector có chuẩn không đổi, và 2) chúng tôi coi các dự đoán siêu mạng như những thay đổi cộng vào các tham số mạng chính, thay vì như chính các tham số. Những thay đổi này làm cho phân phối trọng số mạng chính không tỷ lệ thuận với đầu vào siêu mạng và ổn định qua phạm vi của các đầu vào siêu mạng.

**Mã hóa đầu vào**. Để giải quyết vấn đề tỷ lệ thuận, chúng tôi ánh xạ các đầu vào γ∈[0,1] vào một không gian có chuẩn Euclide không đổi ||E_{L2}(γ)||_2=1 sử dụng hàm E_{L2}(γ)=[cos(γπ/2),sin(γπ/2)]. Với thay đổi này, độ lớn đầu vào của siêu mạng là không đổi ||E_{L2}(γ)||= 1∀γ, nên ||x^(1)|| ≠∝γ. Đối với đầu vào chiều cao hơn, chúng tôi áp dụng phép biến đổi này cho từng đầu vào riêng lẻ, dẫn đến một vector đầu ra có số chiều gấp đôi. Phép biến đổi này tạo ra một biểu diễn đầu vào với chuẩn không đổi, từ đó loại bỏ hiệu ứng tỷ lệ thuận.

Đối với mã hóa đầu vào của chúng tôi, đầu tiên chúng tôi ánh xạ mỗi chiều của vector đầu vào về phạm vi [0,1] để tối đa hóa phạm vi đầu ra của E_{L2}. Chúng tôi sử dụng việc chia tỷ lệ min-max của đầu vào: γ′=(γ−γ_{min})/(γ_{max}−γ_{min}). Đối với các đầu vào không bị ràng buộc như các biến Gaussian, đầu tiên chúng tôi áp dụng hàm logistic σ(x)=1/(1+exp(−x)). Nếu đầu vào trải dài qua nhiều bậc độ lớn, chúng tôi lấy log trước khi chia tỷ lệ min-max.

**Mã hóa đầu ra**. Các dạng thức dư đã trở thành nền tảng trong các kiến trúc học sâu đương đại. Được thúc đẩy bởi các phương pháp này, chúng tôi thay thế khung siêu mạng điển hình bằng một khung học các tham số mạng chính f (những gì thường được học trong các công thức hiện tại), và sau đó sử dụng các dự đoán siêu mạng như những thay đổi cộng vào các tham số này. Chúng tôi giới thiệu một tập hợp các tham số có thể học θ_0, và tính toán các tham số mạng chính như θ=θ_0+h(E_{L2}(γ);ω).

Mã hóa đầu ra cộng này dự đoán các trọng số độc lập với đầu vào bằng cách phân rã đóng góp siêu mạng như một sự kết hợp của một số hạng độc lập θ_0 và một số hạng phụ thuộc h(E_{L2}(γ);ω). Mã hóa đầu ra cũng cung cấp một cơ chế đơn giản và có nguyên tắc để khởi tạo các trọng số siêu mạng. Đầu tiên, các trọng số siêu mạng ω có thể được khởi tạo sử dụng các phương pháp khởi tạo phổ biến cho các lớp kết nối đầy đủ. Sau đó, các tham số độc lập θ_0 có thể được khởi tạo có tính đến vai trò của chúng trong mạng chính.

**Kết nối với các embedding đã học**. Phương pháp mã hóa đầu vào của chúng tôi có thể được hiểu liên quan đến việc mã hóa các siêu tham số phân loại trong siêu mạng. Thông thường, các lớp embedding biến đổi đầu vào phân loại thành các tham số có thể học. Đối với đầu vào scalar γ, các trọng số của lớp kết nối đầy đủ đầu tiên, W∈R^{N×1}, hoạt động tương tự như một vector embedding đã học e. Đối với đầu vào scalar γ được biểu diễn bởi độ lớn, công thức truyền thống chia tỷ lệ vector embedding này một cách tuyến tính, x^(2)=γe. Ngược lại, phương pháp mã hóa đầu vào của chúng tôi sử dụng W∈R^{N×2}, có thể được phân rã thành hai vector embedding, W={e_0, e_1}. Từ quan điểm này, hàm mã hóa của chúng tôi có thể được coi là nội suy giữa hai vector embedding có thể học, x^(2)=cos(γπ/2)e_0+sin(γπ/2)e_1.

## 5. Thiết lập thí nghiệm

### 5.1. Nhiệm vụ

Chúng tôi đánh giá việc tham số hóa được đề xuất trên nhiều nhiệm vụ liên quan đến các mô hình dựa trên siêu mạng.

**Mạng thần kinh Bayesian**. Các mô hình siêu mạng đã được sử dụng để học các họ hàm có điều kiện trên một phân phối tiên nghiệm. Trong quá trình huấn luyện, biểu diễn tiên nghiệm γ∈R^d được lấy mẫu từ phân phối tiên nghiệm γ∼p(γ) và được sử dụng để điều kiện siêu mạng h(γ;ω)→θ để dự đoán các tham số của mô hình mạng chính f(x;θ). Một khi đã được huấn luyện, họ các mạng hậu nghiệm sau đó được sử dụng để ước tính sự không chắc chắn của tham số hoặc để cải thiện hiệu chuẩn mô hình. Cho mục đích minh họa, đầu tiên chúng tôi đánh giá một thiết lập trong đó f(x;θ) là một mạng thần kinh feedforward được sử dụng để phân loại tập dữ liệu MNIST. Sau đó, chúng tôi giải quyết một thiết lập phức tạp hơn trong đó f(x;θ) là một mô hình giống ResNet được huấn luyện trên tập dữ liệu OxfordFlowers-102. Trong cả hai thiết lập, chúng tôi sử dụng prior N(0,1) cho mỗi đầu vào.

**Hypermorph**. Các mạng đăng ký hình ảnh y tế dựa trên học f(x_m,x_f;θ)→ϕ đăng ký một hình ảnh di chuyển x_m với một hình ảnh cố định x_f bằng cách dự đoán một trường flow hoặc deformation ϕ giữa chúng. Loss không giám sát thông thường cân bằng một số hạng căn chỉnh hình ảnh L_{sim} và một số hạng điều chuẩn không gian (độ mịn) L_{reg}. Mục tiêu học là L= (1−γ)L_{sim}(x_m◦ϕ,x_f) +γL_{reg}(ϕ), trong đó γ kiểm soát sự đánh đổi. Trong Hypermorph, nhiều thiết lập điều chuẩn cho đăng ký hình ảnh y tế được học cùng nhau sử dụng siêu mạng. Siêu mạng được cho tham số đánh đổi γ như đầu vào, được lấy mẫu ngẫu nhiên từ U(0,1) trong quá trình huấn luyện. Chúng tôi tuân theo cùng một thiết lập thí nghiệm, sử dụng kiến trúc U-Net cho mạng chính (đăng ký) và huấn luyện với MSE cho L_{sim} và biến thiên tổng cho L_{reg}. Chúng tôi huấn luyện các mô hình trên tập dữ liệu OASIS. Để đánh giá, chúng tôi sử dụng trường flow được dự đoán để warp các bản đồ nhãn phân đoạn giải phẫu của hình ảnh di chuyển, và đo sự chồng chéo thể tích với các bản đồ nhãn cố định.

**Siêu mạng không gian tỷ lệ**. Chúng tôi cũng sử dụng một siêu mạng để học hiệu quả một họ các mô hình với các yếu tố tái tỷ lệ nội bộ khác nhau trong các lớp downsampling và upsampling, như gần đây đã được thực hiện. Trong thiết lập này, γ tương ứng với yếu tố tỷ lệ. Cho đầu vào siêu mạng γ, siêu mạng h(γ;ω)→θ dự đoán các tham số của mạng chính, thực hiện các thao tác tái tỷ lệ không gian theo giá trị của γ. Chúng tôi nghiên cứu một thiết lập trong đó f(x;θ) là một mạng tích chập với các lớp thay đổi kích thước biến đổi, yếu tố tái tỷ lệ được lấy mẫu từ U(0,0.5), và đánh giá sử dụng bài toán phân loại OxfordFlowers-102 và nhiệm vụ phân đoạn OASIS.

### 5.2. Chi tiết thí nghiệm

**Mô hình**. Chúng tôi triển khai siêu mạng như một mạng thần kinh với các lớp kết nối đầy đủ và kích hoạt LeakyReLU cho tất cả trừ lớp cuối, có đầu ra tuyến tính. Các trọng số siêu mạng được khởi tạo sử dụng khởi tạo Kaiming ở chế độ fan out và bias được khởi tạo về zero. Trừ khi được chỉ định khác, kiến trúc siêu mạng có hai lớp ẩn với 16 và 128 neuron tương ứng. Chúng tôi sử dụng triển khai này cho cả siêu mạng mặc định (hiện tại), và siêu mạng được đề xuất (MIP) của chúng tôi.

**Huấn luyện**. Chúng tôi sử dụng hai lựa chọn phổ biến của bộ tối ưu: SGD với momentum Nesterov, và Adam. Chúng tôi tìm kiếm trên một phạm vi tỷ lệ học ban đầu và báo cáo các mô hình hoạt động tốt nhất; thêm chi tiết được bao gồm trong tài liệu Bổ sung A.

**Triển khai**. Một đóng góp quan trọng của công trình chúng tôi là việc phát hành HyperLight, khung siêu mạng PyTorch của chúng tôi. HyperLight không chỉ triển khai việc tham số hóa siêu mạng được đề xuất của chúng tôi mà còn cung cấp một API mô-đun và có thể kết hợp tạo điều kiện thuận lợi cho việc phát triển các mô hình siêu mạng. Sử dụng HyperLight, các nhà thực hành có thể sử dụng các định nghĩa mô hình không phải siêu mạng hiện tại và trọng số mô hình đã được huấn luyện trước, và có thể dễ dàng xây dựng các mô hình sử dụng siêu mạng phân cấp.

## 6. Kết quả thí nghiệm

### 6.1. Ảnh hưởng của tỷ lệ thuận lên phân phối tham số và gradient

Đầu tiên, chúng tôi cho thấy một cách thực nghiệm cách hiện tượng tỷ lệ thuận ảnh hưởng đến phân phối của các trọng số được dự đoán θ và gradient tương ứng của chúng cho các mạng thần kinh Bayesian trên MNIST. Hình 3a và 3b so sánh các phân phối của trọng số mạng chính và đầu ra lớp cho một phạm vi giá trị của đầu vào siêu mạng γ. Trong khi việc tham số hóa siêu mạng mặc định rất nhạy cảm với những thay đổi trong đầu vào, MIP được đề xuất loại bỏ sự phụ thuộc này, với phân phối kết quả khớp chặt chẽ với các mô hình không phải siêu mạng. Hơn nữa, Hình 1a (trong phần giới thiệu), cho thấy rằng sử dụng công thức mặc định, quy mô của các trọng số tương quan tuyến tính với giá trị của đầu vào siêu mạng, và quan trọng là, sự tương quan này vẫn còn sau khi quá trình huấn luyện kết thúc. Ngược lại, các tham số hóa MIP dẫn đến một phân phối trọng số mạnh mẽ với đầu vào γ, cả ở đầu và cuối việc huấn luyện.

Chúng tôi cũng phân tích cách tỷ lệ thuận ảnh hưởng đến giai đoạn đầu của tối ưu hóa siêu mạng bằng cách nghiên cứu phân phối của chuẩn gradient trong quá trình huấn luyện. Hình 3c cho thấy chuẩn của gradient tham số được dự đoán ||∇_θL|| khi huấn luyện tiến triển. Như phân tích của chúng tôi dự đoán, các siêu mạng với tham số hóa mặc định trải qua những thay đổi lớn trong độ lớn gradient vì mối quan hệ tỷ lệ thuận giữa đầu vào và tham số được dự đoán. Ngược lại, chiến lược MIP dẫn đến phương sai nhỏ hơn đáng kể và độ lớn gradient ổn định hơn so với công thức tiêu chuẩn.

### 6.2. Cải thiện huấn luyện mô hình

Trong thí nghiệm này, chúng tôi phân tích cách MIP ảnh hưởng đến sự hội tụ của mô hình cho các nhiệm vụ được xem xét. Đối với tất cả các thí nghiệm, chúng tôi thấy rằng các siêu mạng MIP không đưa ra tác động đo lường được trong thời gian chạy huấn luyện, vì vậy chúng tôi báo cáo các bước theo epoch.

Hình 4a cho thấy loss huấn luyện và độ chính xác kiểm tra cho các mạng Bayesian được huấn luyện trên MNIST. Chúng tôi thấy rằng các tham số hóa MIP dẫn đến loss tốt hơn và độ chính xác cao hơn sớm hơn trong quá trình huấn luyện. MIP cũng đạt được phương sai giảm đáng kể qua các khởi tạo mạng. Tham số hóa mặc định gặp phải sự không ổn định huấn luyện lẻ tẻ (các đỉnh trong loss huấn luyện), trong khi MIP dẫn đến huấn luyện ổn định. Chúng tôi thấy kết quả tương tự cho các mô hình được huấn luyện với SGD.

Hình 4b và 4c trình bày các đường cong hội tụ cho hai nhiệm vụ khác. Đối với Hypermorph, các tham số hóa MIP rất quan trọng khi sử dụng SGD với momentum vì nếu không thì mô hình thất bại trong việc huấn luyện có ý nghĩa. Đối với tất cả các lựa chọn tỷ lệ học, siêu mạng mặc định thất bại trong việc hội tụ, trong khi với tham số hóa MIP nó hội tụ cho một phạm vi giá trị lớn. Với Adam, các mạng huấn luyện có ý nghĩa, và các mô hình MIP liên tục đạt được điểm Dice tương tự một cách đáng kể nhanh hơn. Chúng ít nhạy cảm hơn với việc khởi tạo trọng số. Trong khi trong thiết lập này bộ tối ưu Adam một phần giảm thiểu vấn đề phương sai gradient bằng cách chuẩn hóa theo lịch sử của các gradient trước đó, tham số hóa MIP dẫn đến sự hội tụ nhanh hơn đáng kể. Hơn nữa, đối với phân đoạn Scale-Space, chúng tôi thấy rằng đối với cả hai bộ tối ưu, các mô hình MIP đạt được sự hội tụ nhanh hơn đáng kể và độ chính xác cuối cùng tốt hơn so với các mô hình với tham số hóa mặc định.

**So sánh với các chiến lược chuẩn hóa**. Chúng tôi so sánh việc tham số hóa được đề xuất với các lựa chọn phổ biến của các lớp chuẩn hóa được tìm thấy trong tài liệu học sâu. Sử dụng công thức mặc định, trong đó các trọng số được dự đoán bắt đầu tỷ lệ thuận với đầu vào siêu mạng, chúng tôi thấy rằng các chiến lược chuẩn hóa hiện tại rơi vào hai loại: chúng hoặc giữ mối quan hệ tỷ lệ thuận hiện tại (như chuẩn hóa theo lô), hoặc loại bỏ tỷ lệ thuận bằng cách làm cho các trọng số được dự đoán độc lập với đầu vào siêu mạng (như chuẩn hóa lớp hoặc trọng số). Chúng tôi cung cấp thêm chi tiết trong Phần B của tài liệu bổ sung.

Chúng tôi kiểm tra một số chiến lược chuẩn hóa. BatchNorm-P, thêm các lớp chuẩn hóa theo lô vào mạng chính. LayerNorm-P, thêm các lớp chuẩn hóa đặc trưng vào mạng chính. LayerNorm-H, thêm các lớp chuẩn hóa đặc trưng vào các lớp siêu mạng. WeightNorm, thực hiện chuẩn hóa trọng số, tách rời độ lớn và hướng gradient, đối với các trọng số được dự đoán bởi siêu mạng. Hình 5a cho thấy sự tiến hóa của độ chính xác kiểm tra cho các siêu mạng Scale-Space được huấn luyện trên OxfordFlowers. Chúng tôi báo cáo thời gian thực tế, vì một số chiến lược chuẩn hóa, như BatchNorm, tăng đáng kể thời gian tính toán cần thiết trên mỗi lần lặp. Đối với các mạng được huấn luyện với SGD, các chiến lược chuẩn hóa cho phép huấn luyện, nhưng không cải thiện đáng kể so với siêu mạng mặc định khi được huấn luyện với Adam. Các mô hình được huấn luyện với SGD momentum và chuẩn hóa đặc trưng siêu mạng (LayerNorm-H) phân kỳ sớm trong quá trình huấn luyện cho tất cả các thiết lập siêu tham số được xem xét. Các mô hình được huấn luyện với tham số hóa MIP được đề xuất dẫn đến sự hội tụ nhanh hơn đáng kể và độ chính xác mô hình cuối cùng tốt hơn.

**Phân tích loại bỏ**. Chúng tôi nghiên cứu đóng góp của từng hai thành phần chính của các tham số hóa MIP: mã hóa đầu vào và công thức đầu ra cộng. Hình 5b cho thấy hiệu ứng lên sự hội tụ cho hai nhiệm vụ. Chúng tôi thấy rằng cả hai thành phần đều giảm sự phụ thuộc tỷ lệ thuận giữa đầu vào và đầu ra siêu mạng, và mỗi thành phần độc lập đạt được những cải thiện đáng kể trong sự hội tụ của mô hình. Tuy nhiên, chúng tôi thấy rằng kết quả tốt nhất (hội tụ nhanh nhất) được đạt một cách nhất quán khi cả hai thành phần được sử dụng cùng nhau trong quá trình huấn luyện.

### 6.3. Phân tích độ mạnh mẽ

**Số chiều đầu vào của siêu mạng**. Chúng tôi nghiên cứu ảnh hưởng của số chiều của đầu vào đến mô hình siêu mạng. Chúng tôi đánh giá trên nhiệm vụ mạng thần kinh Bayesian, và chúng tôi thay đổi số chiều của prior đầu vào. Chúng tôi huấn luyện các mô hình với số chiều đầu vào tăng theo cấp số nhân, dim(γ)=1,2,...,32. Hình 5c cho thấy rằng chiến lược MIP được đề xuất dẫn đến những cải thiện trong sự hội tụ của mô hình và độ chính xác mô hình cuối cùng khi chúng tôi tăng chiều của đầu vào siêu mạng γ.

**Lựa chọn kiến trúc siêu mạng**. Chúng tôi đánh giá hiệu suất mô hình khi thay đổi các thuộc tính của kiến trúc siêu mạng. Chúng tôi thay đổi chiều rộng (số neuron ẩn trên mỗi lớp) và độ sâu (số lớp) – các mạng kết nối đầy đủ với 3, 4 và 5 lớp và với 16 và 128 neuron trên mỗi lớp, cũng như một số neuron tăng theo cấp số nhân trên mỗi lớp Dim(x_n) = 16·2^n. Chúng tôi thấy rằng các cải thiện MIP tổng quát hóa cho tất cả các kiến trúc siêu mạng được kiểm tra với những cải thiện tương tự trong việc huấn luyện mô hình. Chúng tôi cung cấp thêm kết quả trong phần B của bổ sung.

**Loại bỏ hàm kích hoạt phi tuyến**. Trong khi phương pháp của chúng tôi được thúc đẩy bởi sự không ổn định huấn luyện có mặt trong các siêu mạng với các hàm kích hoạt phi tuyến (Leaky)-ReLU, chúng tôi đã khám phá việc áp dụng nó cho các lựa chọn thông thường khác của các hàm kích hoạt được tìm thấy trong tài liệu: Tanh, GELU và SiLU. Hình 8 (trong phần B của bổ sung) cho thấy rằng MIP liên tục giúp đỡ cho tất cả các lựa chọn hàm kích hoạt phi tuyến, và những cải thiện tương tự như các mô hình LeakyReLU.

## 7. Hạn chế

Một hạn chế của công trình này là tất cả các mô hình siêu mạng được sử dụng trong các thí nghiệm của chúng tôi được cấu tạo từ các lớp kết nối đầy đủ và sử dụng các lựa chọn kích hoạt và khởi tạo thường được khuyến nghị trong tài liệu. Tương tự, chúng tôi tập trung vào hai bộ tối ưu trong các thí nghiệm của chúng tôi, SGD với momentum và Adam. Chúng tôi tin rằng chúng tôi sẽ thấy kết quả tương tự cho các kiến trúc và bộ tối ưu ít phổ biến khác, nhưng điều này vẫn là một khu vực của công việc tương lai. Hơn nữa, chúng tôi tập trung vào việc huấn luyện các mô hình từ đầu. Cho rằng các siêu mạng đang trở nên ngày càng phổ biến trong các nhiệm vụ học chuyển giao sử dụng các mô hình được huấn luyện trước, chúng tôi tin rằng đây sẽ là một hướng thú vị cho phân tích tương lai của MIP.

## 8. Kết luận

Chúng tôi đã chỉ ra thông qua phân tích và thí nghiệm rằng các công thức siêu mạng truyền thống dễ bị không ổn định huấn luyện, gây ra bởi ảnh hưởng của độ lớn của các giá trị đầu vào siêu mạng lên trọng số và gradient mạng chính, và rằng các phương pháp tiêu chuẩn như chuẩn hóa theo lô và lớp không giải quyết được vấn đề. Sau đó chúng tôi đề xuất việc sử dụng một phương pháp mới, Các tham số hóa bất biến độ lớn (MIP), để giải quyết vấn đề này. Thông qua các thí nghiệm toàn diện, chúng tôi chứng minh rằng MIP dẫn đến những cải thiện đáng kể trong thời gian hội tụ và độ chính xác mô hình qua nhiều kiến trúc siêu mạng, kịch bản huấn luyện, và nhiệm vụ.

Để giúp thêm việc áp dụng siêu mạng, chúng tôi phát hành thư viện học siêu mạng của chúng tôi, HyperLight, không chỉ triển khai MIP mà còn cung cấp một API mô-đun và có thể kết hợp để tạo thuận lợi cho việc phát triển các mô hình dựa trên siêu mạng. Cho rằng việc sử dụng MIP không bao giờ giảm hiệu suất mô hình và có thể cải thiện đáng kể việc huấn luyện, chúng tôi kỳ vọng phương pháp này sẽ hữu ích rộng rãi cho việc huấn luyện siêu mạng.
