# Hyper-VolTran: Chuyển đổi nhanh và khái quát hóa từ hình ảnh một lần sang cấu trúc đối tượng 3D thông qua HyperNetworks

**Tóm tắt**

Việc giải quyết vấn đề chuyển đổi hình ảnh sang 3D từ một góc nhìn duy nhất là một bài toán ill-posed, và các phương pháp tái cấu trúc thần kinh hiện tại giải quyết nó thông qua các mô hình khuếch tán vẫn phụ thuộc vào tối ưu hóa đặc thù cho từng cảnh, hạn chế khả năng khái quát hóa của chúng. Để vượt qua những hạn chế của các phương pháp hiện có về khái quát hóa và tính nhất quán, chúng tôi giới thiệu một kỹ thuật render thần kinh mới. Phương pháp của chúng tôi sử dụng hàm khoảng cách có dấu (SDF) làm biểu diễn bề mặt và kết hợp các tiên nghiệm khái quát hóa thông qua các thể tích mã hóa hình học và HyperNetworks. Cụ thể, phương pháp của chúng tôi xây dựng các thể tích mã hóa thần kinh từ các đầu vào đa góc nhìn được tạo ra. Chúng tôi điều chỉnh trọng số của mạng SDF có điều kiện trên hình ảnh đầu vào tại thời điểm kiểm tra để cho phép mô hình thích ứng với các cảnh mới theo cách truyền thẳng thông qua HyperNetworks. Để giảm thiểu các artifacts xuất phát từ các góc nhìn được tổng hợp, chúng tôi đề xuất sử dụng mô-đun volume transformer để cải thiện việc tổng hợp các đặc trưng hình ảnh thay vì xử lý từng góc nhìn một cách riêng biệt. Thông qua phương pháp đề xuất của chúng tôi, được gọi là Hyper-VolTran, chúng tôi tránh được nút thắt cổ chai của việc tối ưu hóa đặc thù cho từng cảnh và duy trì tính nhất quán qua các hình ảnh được tạo ra từ nhiều góc nhìn. Các thí nghiệm của chúng tôi cho thấy những ưu điểm của phương pháp đề xuất với kết quả nhất quán và tạo ra nhanh chóng.

## 1. Giới thiệu

Tiến bộ gần đây trong tái cấu trúc 3D thần kinh đã mang lại những tác động đáng kể trong nhiều ứng dụng khác nhau, ví dụ như tổng hợp góc nhìn mới và thị giác robot. Cụ thể, có một sự quan tâm ngày càng tăng đến các trường thần kinh để trích xuất thông tin 3D từ nhiều hình ảnh với các tham số camera đã biết. NeRF và Hàm Khoảng cách Có dấu (SDF) là các kỹ thuật tái cấu trúc hình ảnh sang 3D tạo ra hình học hợp lý và do đó, các góc nhìn mới. Mặc dù đã có tiến bộ lớn, việc đạt được tái cấu trúc đối tượng 3D chính xác thông qua các phương pháp implicit thần kinh vẫn đòi hỏi một số lượng đáng kể hình ảnh có góc nhìn và hình thức nhất quán cũng như các pose camera chính xác để tái cấu trúc các đối tượng 3D một cách chính xác.

Trên thực tế, việc thu thập dữ liệu từ nhiều góc nhìn có thể không phải lúc nào cũng khả thi khi tài nguyên bị hạn chế. Một số công trình chứng minh khả năng giảm thiểu các vấn đề về tái cấu trúc 3D dưới một tập hợp hình ảnh thưa thớt. Một kỹ thuật chính trong các phương pháp này là xây dựng thể tích mã hóa thần kinh được chiếu từ nhiều góc nhìn đầu vào. Mặc dù những kỹ thuật này có thể hoạt động trên các đầu vào hạn chế, việc tái cấu trúc 3D từ một hình ảnh duy nhất vẫn còn thách thức và đòi hỏi một tiên nghiệm mạnh mẽ để cho phép mô hình tái cấu trúc thần kinh tạo ra các hình dạng và màu sắc hợp lý của các góc nhìn chưa thấy.

Một phát triển gần đây trong các mô hình tạo sinh đã cho thấy kết quả hứa hẹn trong việc tạo ra hình ảnh 2D có thể hoạt động như một tiên nghiệm mạnh mẽ cho các góc nhìn chưa thấy. Một số công trình tiếp cận vấn đề này bằng cách sử dụng hướng dẫn của một mô hình khuếch tán. Cụ thể, Poole và cộng sự giới thiệu Score Distillation Sampling (SDS) trong đó mô hình tái cấu trúc thần kinh học thông qua lỗi phản hồi từ một mô hình khuếch tán. Mô hình khuếch tán được đóng băng mà không có bất kỳ cập nhật nào trong khi các trọng số NeRF được cập nhật trong quá trình tối ưu hóa. Mặc dù kỹ thuật này có khả năng tái cấu trúc các cảnh 3D, tối ưu hóa theo từng cảnh vẫn được yêu cầu, thường mất đến 1 giờ để hội tụ trên một GPU duy nhất. Ràng buộc này hạn chế tính thực tế của phương pháp này, đặc biệt là khi cần thực hiện tái cấu trúc 3D một cách hiệu quả. Để đạt được tái cấu trúc 3D nhanh chóng, cần có một tiên nghiệm khái quát hóa cho phép một thao tác truyền thẳng thông qua các mạng thay vì phụ thuộc vào tối ưu hóa đắt đỏ theo từng cảnh.

Một phương pháp thay thế cho tái cấu trúc 3D nhanh chóng là sử dụng một mô hình khuếch tán và tổng hợp các hình ảnh đa góc nhìn. Điều này có thể được thực hiện bằng cách tận dụng một mô hình khuếch tán có thể tạo ra hình ảnh dựa trên các biến thể nhỏ trong các tham số camera. Tuy nhiên, việc tạo ra hình ảnh bằng cách sử dụng một bộ tạo hình ảnh đa góc nhìn (ví dụ: Zero123) có thể gặp thách thức trong việc bảo toàn tính nhất quán về hình học. Thay vì tối ưu hóa một mạng cho từng đối tượng như trong [21], chúng tôi nhằm mục đích chỉ bảo toàn một mạng để khái quát hóa cho nhiều đối tượng. Để đạt được điều này, chúng tôi có thể khai thác thể tích mã hóa thần kinh được xây dựng từ việc chiếu các đặc trưng hình ảnh với các tham số camera đã biết như trong [2,17,36]. Trong khi các phương pháp này cho thấy tiềm năng, chúng vẫn gặp phải kết quả không tối ưu khi được sử dụng để tái cấu trúc 3D liên quan đến các đối tượng chưa thấy.

Trong công trình này, chúng tôi nhằm mục đích giải quyết những thách thức nêu trên, tập trung vào các vấn đề về khái quát hóa, tốc độ và sự không nhất quán. Để đạt được điều này, chúng tôi giới thiệu một mạng thần kinh để giải quyết những mối quan tâm này bằng cách sử dụng một mạng SDF được tạo ra bởi HyperNetworks và một Volume Transformer (VolTran) để giảm thiểu tác động của các ví dụ không nhất quán. Phương pháp của chúng tôi khám phá tiềm năng khái quát hóa bằng cách giới thiệu một biến tiềm ẩn thu được từ một bộ mã hóa hình ảnh (ví dụ: CLIP) để tạo ra các biểu diễn hình ảnh. Tiếp theo, chúng tôi sử dụng những biểu diễn hình ảnh này để tạo ra các trọng số của SDF, giải quyết thách thức về khái quát hóa. Vui lòng xem Hình 1 (dưới) để thấy minh họa về kỹ thuật của chúng tôi. Để tóm tắt, những đóng góp của chúng tôi bao gồm:

1. Chúng tôi đề xuất một tiên nghiệm khái quát hóa cho tái cấu trúc mesh 3D với một vài dữ liệu được tổng hợp bằng cách gán các trọng số của SDF dựa trên embedding hình ảnh đầu vào.
2. Chúng tôi đề xuất một mô-đun transformer cho việc tổng hợp để cho phép làm việc trên các hình dạng và màu sắc không nhất quán qua các góc nhìn khác nhau.
3. Chúng tôi cũng cho thấy rằng phương pháp của chúng tôi chỉ yêu cầu một quá trình truyền thẳng và có thể tạo ra một mesh 3D một cách thoải mái với thời gian xử lý bổ sung không đáng kể ~5 giây.

## 2. Công trình liên quan

**Các mô hình khuếch tán cho tái cấu trúc 2D sang 3D.** Tái cấu trúc một cấu trúc 3D hoàn chỉnh chỉ từ một vài hình ảnh 2D là thách thức do tính chất ill-posed vốn có của vấn đề. Tuy nhiên, những tiến bộ gần đây trong các mô hình tạo sinh và đặc biệt là các mô hình khuếch tán cung cấp một hướng đi hứa hẹn để có được các tiên nghiệm về thế giới 3D cần thiết để tái cấu trúc cấu trúc 3D hoàn chỉnh của một đối tượng từ một hình ảnh duy nhất. Ví dụ, chúng được sử dụng như một cách gián tiếp để cung cấp phản hồi trong quá trình tái cấu trúc hình ảnh sang 3D. Một công trình đáng chú ý gọi là DreamFusion đề xuất tạo sinh text-to-3D bằng Score Distillation Sampling (SDS), cho phép tạo sinh có hướng dẫn tối ưu hóa của các cảnh 3D được tham số hóa bởi NeRF. Một công trình đồng thời sử dụng Score Jacobian Chaining sử dụng một phương pháp tương tự, khai thác quy tắc chuỗi trên các đầu ra của một mô hình tạo sinh hình ảnh được huấn luyện trước. Tang và cộng sự mở rộng ý tưởng này với các giai đoạn thô và tinh chỉnh để nâng cao đầu ra với các đám mây điểm có kết cấu. Gần đây, Zero123 mô tả một mô hình khuếch tán nhận một hình ảnh đầu vào và các tham số camera để tổng hợp một góc nhìn mới. Mô hình này có thể tạo ra các hình ảnh đa góc nhìn nhất quán hơn so với một mô hình khuếch tán sẵn có như Imagen. Mặc dù là một hướng đi hứa hẹn để tái cấu trúc các mô hình 3D, tối ưu hóa theo từng cảnh vẫn được yêu cầu và hàm implicit thần kinh bị hạn chế chỉ biểu diễn một đối tượng. Do đó, việc khái quát hóa của mô hình được huấn luyện bị hạn chế đối với các đối tượng chưa thấy.

**Các tiên nghiệm khái quát hóa cho tái cấu trúc 3D nhanh chóng.** Một triển khai lý tưởng của tái cấu trúc 3D là một mô hình duy nhất có thể khái quát hóa cho các đối tượng chưa thấy, cho phép tạo sinh 3D chỉ sử dụng phương pháp truyền thẳng mà không cần áp dụng tối ưu hóa theo từng cảnh thêm. PixelNeRF là một công trình tiên phong theo hướng này đề xuất trích xuất các thể tích đặc trưng từ một hình ảnh đầu vào sau đó được truyền qua một mô hình NeRF cùng với các tham số extrinsic của camera. Chen và cộng sự trình bày một phương pháp gọi là MVSNeRF sử dụng các thể tích chi phí được xây dựng từ các đặc trưng hình ảnh 2D đã được warp và sau đó hồi quy mật độ thể tích với một lần truyền qua một MLP (tức là, các thể tích mã hóa thần kinh) làm hình học cơ sở. Sau đó, thể tích mã hóa thần kinh được sử dụng làm đầu vào bổ sung cho mô hình NeRF. SparseNeus mở rộng MVSNeRF để hoạt động trên chế độ dữ liệu ít bằng cách đề xuất lý luận hình học cascaded để tinh chỉnh các chi tiết của một đối tượng 3D. Tuy nhiên, phương pháp này vẫn yêu cầu đầu vào đa góc nhìn, không có cơ chế rõ ràng để mở rộng nó cho một hình ảnh duy nhất. Để giải quyết vấn đề tái cấu trúc 3D từ một hình ảnh duy nhất, Liu và cộng sự đề xuất một phương pháp gọi là One2345 để khai thác một mô hình khuếch tán (ví dụ: Zero123) để tạo ra một số hình ảnh ví dụ với các pose camera được ước tính. Để cải thiện độ chính xác của các mô hình hình học được tái cấu trúc, One2345 sử dụng SDF thay vì NeRF. Thách thức của phương pháp này là sự không nhất quán trong các ví dụ được tạo ra, khiến việc tái cấu trúc các cảnh 3D hoàn toàn tôn trọng hình thức đầu vào trở nên khó khăn.

Một phương pháp khác để tránh tối ưu hóa theo từng cảnh là huấn luyện một mô hình quy mô lớn với học tự giám sát và sử dụng dữ liệu text-to-3D được gán nhãn quy mô lớn. Point-e, một hệ thống tạo ra đám mây điểm 3D từ mô tả văn bản, là một tiên phong theo hướng này. Tiếp theo công trình này, Shap-e trực tiếp tạo ra các trọng số của mô hình implicit thần kinh có thể được render dưới dạng mesh và radiance field. Phương pháp này tạo ra nhiều hình ảnh tổng hợp sau đó một kỹ thuật tái cấu trúc 3D thần kinh (ví dụ: SDF hoặc NeRF) được sử dụng để tạo ra các mô hình 3D. Mô hình này cắt giảm chi phí tái cấu trúc hình ảnh sang 3D từ vài giờ GPU xuống 1-2 phút. Trong khi phương pháp này có thể tạo ra kết quả nhanh chóng, chất lượng của các bề mặt 3D được tái cấu trúc vẫn kém. Không giống như tất cả các công trình trước đây này, phương pháp đề xuất của chúng tôi có thể tạo ra tái cấu trúc 3D chính xác với thời gian xử lý cạnh tranh (tức là, ít hơn 1 phút).

**Học dựa trên ngữ cảnh.** Trong học few-shot, khái niệm tận dụng thông tin ngữ cảnh để đạt được hiệu suất tối ưu qua các điều kiện đầu vào đa dạng là một ý tưởng được thiết lập tốt, như được chỉ ra bởi các công trình trước đây. Một số phương pháp này liên quan đến việc cập nhật tham số mô hình thông qua gradient descent, được minh họa bởi một số công trình. Tuy nhiên, các phương pháp này vẫn yêu cầu nhiều thao tác truyền thẳng để cập nhật mô hình. Trọng tâm của chúng tôi là phát triển một phương pháp thực hiện việc hiểu ngữ cảnh chỉ với một thao tác truyền thẳng duy nhất, mà không cần các bước tối ưu hóa bổ sung. Để đạt được điều này, chúng tôi chọn áp dụng thông tin dựa trên ngữ cảnh bằng cách tạo ra trọng số mạng thần kinh. Cụ thể, chúng tôi lấy cảm hứng từ HyperNetworks được thiết kế để tạo ra trọng số mạng thần kinh dựa trên ngữ cảnh được cung cấp.

## 3. Phương pháp đề xuất

Pipeline tái cấu trúc 3D thần kinh của chúng tôi có hai luồng, như được hiển thị trong Hình 2. Cho một hình ảnh góc nhìn đơn và bản đồ độ sâu của nó, đầu tiên chúng tôi tổng hợp các hình ảnh đa góc nhìn thông qua một mô hình khuếch tán. Sau đó, như được hiển thị trong luồng trên của hình, các hình ảnh được tổng hợp được đưa vào một thể tích mã hóa thần kinh để thu được biểu diễn hình học 3D của cấu trúc của nó. Biểu diễn hình học được kết hợp với các hình ảnh để dự đoán một bản đồ RGB được render bởi mô-đun transformer đề xuất của chúng tôi, VolTran. Trong khi đó, chúng tôi cũng sử dụng các hình ảnh đa góc nhìn được tổng hợp trong một HyperNetwork để ước tính một trọng số SDF, được hiển thị trong luồng dưới. Mạng SDF dự đoán SDF để biểu diễn bề mặt sẽ sau đó được sử dụng để render bản đồ độ sâu và trích xuất mesh. Do đó, chúng tôi đặt tên phương pháp của chúng tôi là Hyper-VolTran.

### 3.1. Hình ảnh từ một đến nhiều góc nhìn

Chúng tôi bắt đầu pipeline của mình bằng cách tận dụng một mô hình tạo sinh đã được huấn luyện trước. Điều này cho phép chúng tôi mở rộng một hình ảnh đầu vào duy nhất thành nhiều góc nhìn từ một tập hợp rộng hơn các góc nhìn đối tượng, mặc dù có một số khuyết điểm. Để so sánh công bằng, chúng tôi tuân thủ nghiêm ngặt phương pháp được nêu trong [16] để tận dụng điều kiện elevation và azimuth.

**Các góc nhìn được tổng hợp.** Cho một hình ảnh RGB đơn và bản đồ độ sâu tương ứng của nó được ký hiệu là I∈R^{H×W×3}, và D∈R^{H×W}, tương ứng, chúng tôi theo Zero123 để chuẩn hóa hình dạng của nó và sử dụng một hệ thống camera hình cầu cho bản đồ độ sâu. Chúng tôi áp dụng một mô hình tạo sinh hình ảnh sẵn có để tạo ra N hình ảnh RGB và bản đồ độ sâu được lấy mẫu đều từ một số góc nhìn theo các tham số camera ground-truth. Cụ thể để huấn luyện, chúng tôi tạo thành một tập hợp các hình ảnh RGB và bản đồ độ sâu của một đối tượng làm tập nguồn I={I₁,···,Iₙ} và D={D₁,···,Dₙ}. Lưu ý rằng cả hình ảnh RGB và độ sâu đều được sử dụng làm mục tiêu huấn luyện để giám sát mô hình trong giai đoạn huấn luyện. Tuy nhiên, những bản đồ độ sâu đó bị bỏ qua trong giai đoạn kiểm tra.

### 3.2. Mã hóa nhận thức hình học

Mã hóa nhận thức hình học là cần thiết trong việc xây dựng một phương pháp khái quát hóa cho dự đoán bề mặt từ các hình ảnh đa góc nhìn. Phương pháp của chúng tôi sử dụng các thể tích mã hóa thần kinh để xây dựng hình học 3D dựa trên các góc nhìn đầu vào đa dạng từ Mục 3.1 và các pose camera liên quan của chúng. Để đạt được điều này, chúng tôi warp các đặc trưng hình ảnh 2D từ N hình ảnh đầu vào lên một mặt phẳng cục bộ nằm trong frustum của góc nhìn tham chiếu.

**Thể tích mã hóa thần kinh.** Trong stereo đa góc nhìn sâu, hình học 3D có thể được suy ra dưới dạng xây dựng Cost Volume. Cho f_θ:R^{H×W×3}→R^{H×W×C} là ánh xạ từ một hình ảnh đầu vào đến một bản đồ đặc trưng. Tương tự như [17,41], chúng tôi mã hóa các hình ảnh sử dụng Feature Pyramid Network làm hàm ánh xạ để trích xuất một bản đồ đặc trưng thần kinh, tức là F_i=f_θ(I_i). Bên cạnh đó, chúng tôi phân chia thể tích bao quanh của cảnh thành một lưới các voxel. Sau đó, cùng với các tham số camera intrinsic và extrinsic P = [K,R,t] cho mỗi hình ảnh I_i, bản đồ đặc trưng thần kinh được chiếu dựa trên mỗi đỉnh v, và đầu ra được ký hiệu là F_i(Π_i(v)), trong đó Π_i(v) chiếu v∈R³ lên mặt phẳng cục bộ bằng cách áp dụng P. Cụ thể, homography warping được áp dụng cho mỗi góc nhìn i, và thể tích mã hóa thần kinh cuối cùng G có thể được tính như Phương trình 1.

G=φ(Var({F_i(Π_i(v))}ᵢ₌₁ᴺ)). (1)

Ở đây Var({F_i(Π_i(v))}ᵢ₌₀ᴺ⁻¹) là Cost Volume, Var có nghĩa là phương sai qua N góc nhìn, và φ biểu thị một hàm chịu trách nhiệm điều chỉnh và truyền bá thông tin cảnh được thể hiện như một sparse 3D CNN (tức là, Geometry Guided Encoding). Vì phương sai chứa đựng sự khác biệt trong hình thức hình ảnh giữa nhiều góc nhìn đầu vào, G có được khả năng mã hóa hình học và hình thức cảnh 3D phức tạp từ các hình ảnh đa dạng. Do đó, những đặc trưng thể tích này chứa thông tin nhận thức hình thức có thể được sử dụng sau này cho volume rendering và dự đoán SDF.

### 3.3. Volume Rendering

Một thể tích mã hóa thần kinh được tính toán trước đó được sử dụng để dự đoán cả mật độ và radiance phụ thuộc góc nhìn tại các vị trí tùy ý trong một cảnh. Tiếp theo, điều này tạo điều kiện cho việc sử dụng volume rendering có thể vi phân để dự đoán màu sắc của các hình ảnh. Để volume rendering, chúng tôi chọn sử dụng SDF thay vì NeRF để tái cấu trúc bề mặt chính xác hơn.

**Hàm Khoảng cách Có dấu (SDF).** SDF biểu diễn bề mặt 3D sử dụng một hàm vị trí cung cấp khoảng cách gần nhất đến bề mặt. Cho một vị trí 3D tùy ý trong thiết lập của chúng tôi, chúng tôi sử dụng một MLP f_Ψ:R^d→R như một SDF để biểu diễn bề mặt 3D. Mặc dù đầu vào SDF chung có d = 3 vì khoảng cách có dấu được liên kết với một điểm z∈R³, phương pháp của chúng tôi sử dụng d cao hơn vì đầu vào bao gồm sự nối tiếp của đặc trưng từ thể tích mã hóa thần kinh, màu sắc, và đặc trưng hình ảnh. Một hạn chế khác của SDF chung là thiếu khả năng khái quát hóa. Ví dụ, khi sử dụng thể tích mã hóa thần kinh làm đầu vào, chúng ta có thể huấn luyện một mạng SDF trên một bộ sưu tập lớn các đối tượng 3D để tránh tối ưu hóa theo từng cảnh. Tuy nhiên, trong kiểm tra, mạng SDF thường được đóng băng và bị hạn chế với các đối tượng đã biết. Chúng tôi đề xuất một phương pháp thích ứng hơn để gán động trọng số của MLP dựa trên các đầu ra được tạo ra của HyperNetworks, được điều kiện trên hình ảnh đầu vào.

**HyperNetworks cho một mạng SDF.** HyperNetworks tạo thành một mô hình thần kinh tạo ra trọng số cho một mạng mục tiêu được thiết kế để khái quát hóa trên các nhiệm vụ khác nhau cho một ngữ cảnh. Thay vì bảo toàn một mạng thần kinh cố định trong thời gian kiểm tra, HyperNetwork cung cấp một cơ chế để gán trọng số dựa trên một điều kiện một cách động. Về mặt toán học, chúng tôi thiết kế một mô-đun HyperNetwork δ_l(.) để tạo ra trọng số cho mỗi lớp ψ_l của mạng SDF f_Ψ:

ψ_l=δ_l(ξ(I₁)). (2)

Để mã hóa hình ảnh đầu vào, chúng tôi sử dụng một bộ mã hóa hình ảnh đã được huấn luyện trước ξ giảm chiều hình ảnh từ không gian RGB xuống một không gian tiềm ẩn. Không giống như công trình trước đây cần tối ưu hóa mạng thần kinh cho từng đối tượng riêng lẻ, phương pháp của chúng tôi huấn luyện mô-đun một cách tức thời mà không yêu cầu tối ưu hóa theo từng cảnh và tính toán trực tiếp loss giữa hai tham số mạng thần kinh. Vì điều kiện của chúng tôi là biểu diễn đặc trưng của đối tượng đầu vào, HyperNetwork của chúng tôi có thể tạo ra một trọng số chuyên dụng và phù hợp hơn cho mạng mục tiêu của nó. Mặt khác, vì chúng tôi sử dụng đầu ra của Hypernetwork để gán trọng số cho mạng SDF, mô hình của chúng tôi khái quát hóa tốt hơn trên đối tượng mới trong quá trình suy luận, đặc biệt là khi đối tượng chia sẻ semantic tương tự với dữ liệu huấn luyện. Hơn nữa, các hypernetwork được cập nhật trực tiếp với một loss từ bản đồ RGB và độ sâu trong pipeline của chúng tôi. Do đó, chúng tôi không phải lưu trữ tham số trọng số tối ưu cá nhân sau tối ưu hóa theo từng cảnh.

**Rendering từ SDF.** Để ước tính các tham số của SDF thần kinh và trường màu, chúng tôi áp dụng một phương pháp volume rendering từ NeuS để render màu sắc và thể tích dựa trên các biểu diễn SDF. Đối với một pixel cho trước, chúng tôi mô tả M tia phát ra từ pixel đó là {p(t) = o + tv | t ≥ 0}, với o là điểm focal của camera và r biểu diễn hướng đơn vị của tia. Chúng tôi đưa các đặc trưng kết hợp qua một MLP và sử dụng hàm softmax để tạo ra các trọng số pha trộn được ký hiệu là {ω_i}ᵢ₌₁ᴺ. Radiance tại một điểm p và hướng quan sát v được tính như tổng có trọng số trong Phương trình 3.

ĉ=∑ᵢ₌₁ᴺ ω_i.c_i, (3)

trong đó c_i là màu của góc nhìn nguồn i. Cho radiance, chiến lược volume rendering của chúng tôi được biểu diễn trong Phương trình 4.

Ĉ=∑ⱼ₌₁ᴹ T_j α_j ĉ_j, (4)

α_j = 1 - exp[-∫ₜⱼᵗʲ⁺¹ ρ(t)dt]. (5)

Ở đây, T_j=∏ₖ₌₁ʲ⁻¹(1-α_k) là tính truyền qua tích lũy rời rạc, α_k là độ mờ rời rạc, và ρ(t) biểu thị mật độ không trong suốt. Bản đồ độ sâu được render có thể được tạo ra như Phương trình 6:

D̂=∑ⱼ₌₁ᴹ T_j α_j t_j. (6)

Lưu ý rằng quá trình rendering hoàn toàn có thể vi phân; chúng tôi huấn luyện pipeline theo cách giám sát để mô hình có thể dự đoán màu sắc được render Ĉ và độ sâu D̂ trong suy luận.

**VolTran: multi-view aggregation transformer.** Dữ liệu pixel về bản chất bị giới hạn trong một ngữ cảnh cục bộ và thiếu thông tin ngữ cảnh rộng hơn, thường dẫn đến các patch bề mặt không nhất quán, đặc biệt là trong trường hợp dữ liệu đầu vào thưa thớt. Một giải pháp tầm thường là tổng hợp các đặc trưng qua các góc nhìn khác nhau để nắm bắt các đặc trưng được chiếu từ nhiều góc nhìn. Thật không may, các góc nhìn được tổng hợp có thể bị hỏng do các khuyết điểm trong mô hình tạo sinh, một tổng hợp đơn giản (ví dụ: average và max pooling) có thể thất bại trong việc render hình dạng và màu sắc một cách chính xác. Chúng tôi đề xuất một mô-đun transformer được gọi là VolTran dựa trên thiết kế self-attention để mã hóa thông tin toàn cục từ N góc nhìn khác nhau. Bên cạnh các đầu vào, chúng tôi học một aggregation token như một token thêm để có được một đầu ra tương ứng cho một góc nhìn mục tiêu. Chính thức, cho X∈R^{(N+1)×d} là một ma trận với các hàng được tạo thành từ các token từ các góc nhìn nguồn và aggregation token bằng cách nối tiếp đặc trưng từ màu c_i, đặc trưng hình ảnh F_i(Π(v)), và đặc trưng thể tích G tạo ra chiều d. Chúng tôi ký hiệu f_V(.), f_Q(.), f_K(.) là các hàm ánh xạ values, queries, và keys của một mô-đun transformer. Do đó, thao tác tổng hợp có thể được tính toán bởi mô-đun self-attention, như được hiển thị trong Phương trình 7:

Attn(X) = Softmax(A)f_V(X), (7)

trong đó A_{i,j} = f_Q(X_i)^T f_K(X_j)/γ cho tất cả i, j ∈ [N]. Vì chúng tôi áp dụng multi-head attention, nó có thể được công thức hóa là MHA(X) = [Attn_1(X),···,Attn_3(X)]W^H. Chúng tôi chọn sử dụng LayerNorm để chuẩn hóa các activation trung gian và skip connection để ổn định huấn luyện. Đầu ra cuối cùng từ mô-đun transformer, một MLP, được giới thiệu như một hàm ánh xạ để có được trọng số pha trộn ω_i. Sau đó, màu cuối cùng có thể được thu được như trong pipeline rendering SDF.

### 3.4. Huấn luyện và Suy luận

Framework của chúng tôi có một số loss để huấn luyện mô hình, bao gồm mô-đun HyperNetwork. Mọi mô-đun đều được tối ưu hóa theo cách end-to-end chỉ trong giai đoạn huấn luyện. Chúng tôi định nghĩa loss cho màu sắc được render với mean squared error đối với ground-truth C_i:

L_{RGB}=\frac{1}{|P|}\sum_{i=1}^{|P|} ||Ĉ_i - C_i||_2^2. (8)

Ngoài color loss, chúng tôi cũng tính toán dự đoán độ sâu được giám sát với loss sau:

L_{Depth}=\frac{1}{|P_1|}\sum_{i=1}^{|P_1|} |D̂_i - D_i|. (9)

Ngoài ra, để điều chỉnh các giá trị SDF được tạo ra từ mạng SDF f_Ψ, chúng tôi tính toán Eikonal loss:

L_{Eikonal}=\frac{1}{|V|}\sum_{v∈V} (||∇f_Ψ(v)||_2 - 1)^2, (10)

trong đó v là một điểm 3D được lấy mẫu và ∇f_θ(v) là gradient tương đối với điểm mẫu q. Loss này ảnh hưởng đến độ mịn của bề mặt.

Hơn nữa, để trao quyền cho framework của chúng tôi tạo ra các bề mặt hình học ngắn gọn, chúng tôi kết hợp một hạng mục điều chỉnh sparsity phạt các bề mặt không thể kiểm soát được gọi là sparse loss, được biểu diễn như sau:

L_{Sparse}=\frac{1}{|V|}\sum_{v∈V} exp(-τ|s(v)|), (11)

trong đó s(v) là SDF dự đoán và τ là siêu tham số để mở rộng quy mô dự đoán SDF. Để tóm tắt, tổng loss được định nghĩa là L_{RGB} + L_{Depth} + β_1 L_{Eikonal} + β_2 L_{Sparse}.

**Suy luận.** Trong quá trình suy luận, không có thêm tối ưu hóa nào, và chỉ một feed-forward được thực hiện, giảm tính toán đắt đỏ để cập nhật các mô hình trong quá trình kiểm tra. Đầu tiên, cho một hình ảnh đầu vào, chúng tôi phân đoạn đầu vào để trích xuất đối tượng foreground. Sau khi chúng tôi có được đối tượng với nền rõ ràng (ví dụ: màu trắng), chúng tôi tổng hợp các cảnh đa góc nhìn từ mô hình Zero123 đã được huấn luyện trước có điều kiện trên sự thay đổi tương đối của các góc nhìn camera. Những hình ảnh được tổng hợp này sau đó được sử dụng để tạo ra một mesh 3D bằng phương pháp đề xuất của chúng tôi. Suy luận của phương pháp đề xuất của chúng tôi chỉ chứa feed-forward, do đó thoải mái giảm thời gian tính toán so với các phương pháp distillation hiện có.

## 4. Thí nghiệm

### 4.1. Chi tiết triển khai

Chúng tôi huấn luyện mô hình của chúng tôi từ dữ liệu có sẵn công khai được chia sẻ đầu tiên bởi [15], chứa 46K cảnh 3D được tổng hợp. Đối với mô hình tạo sinh đa góc nhìn cơ sở, chúng tôi theo Zero123 và giữ các trọng số của nó bị đóng băng. Ngoài ra, đối với geometry-guided encoder, chúng tôi đặt kích thước mã hóa thể tích là 96×96×96 cho tất cả các thí nghiệm của chúng tôi. Để tạo ra trọng số SDF, chúng tôi sử dụng mô hình CLIP như bộ mã hóa hình ảnh, được biết đến với việc tạo ra các biểu diễn đáng tin cậy. Về hàm loss, chúng tôi xác minh rằng thiết lập được đề xuất bởi [17] là tối ưu, tức là β₁ = 0.1 và β₂ = 0.02. Mặt khác, trong quá trình suy luận, đầu tiên chúng tôi áp dụng phân đoạn hình ảnh để có được một cutout chính xác của đối tượng mục tiêu sử dụng Segment Anything Model (SAM). Sau đó, chúng tôi tạo ra 8 góc nhìn chính được mở rộng thêm bởi 4 hình ảnh gần đó mỗi cái, tổng cộng 32 góc nhìn.

### 4.2. Kết quả Text-to-3D

Pipeline text-to-3D được thực hiện bằng cách sử dụng các mô hình text-to-image sẵn có. Chúng tôi áp dụng quá trình khuếch tán tương ứng có điều kiện trên một prompt cho trước (ví dụ: "a wooden bear") và có được một hình ảnh mô tả nó. Để xử lý thông tin nền bất ngờ, chúng tôi cắt đối tượng mục tiêu từ hình ảnh được tạo ra bằng SAM. Các góc nhìn khác nhau được tổng hợp thêm cùng với các pose camera tương ứng bằng Zero123. Tập hợp đầy đủ các hình ảnh được tạo ra được đưa vào mô hình của chúng tôi, xây dựng thể tích mã hóa thần kinh, tạo ra trọng số mạng SDF thông qua HyperNetwork, và áp dụng global attention, các thành phần chính của Hyper-VolTran. Hình 3 hiển thị kết quả của phương pháp chúng tôi qua các góc nhìn khác nhau cho một text prompt cho trước. Có thể quan sát từ những hình ảnh này rằng Hyper-Voltran tạo ra các mesh chất lượng tốt tuân thủ tốt với kết cấu tương ứng, tạo cảm giác nhất quán qua các góc nhìn.

### 4.3. Kết quả Image-to-3D

Chúng tôi sử dụng một tập con của dataset GSO để đánh giá định lượng mesh image-to-3D một lần, bao gồm 25 đối tượng từ các danh mục GSO khác nhau. Để đánh giá chất lượng rendering, chúng tôi sử dụng hình ảnh từ [18], bao gồm 15 đối tượng.

**Kết quả định tính.** Chúng tôi cung cấp các minh chứng định tính của phương pháp chúng tôi và so sánh với One2345, Shap-e, Point-e, và Zero123+SD trong Hình 4, thể hiện hiệu quả của Hyper-VolTran trong giải quyết tái cấu trúc đối tượng image-to-3D một lần. Để so sánh công bằng với One2345, chúng tôi sử dụng cùng một tập hợp hình ảnh được tổng hợp để tạo ra các mesh 3D. Chúng tôi lưu ý rằng One2345 thể hiện các hình dạng không chính xác và không tự nhiên trong Hình 4. Ngoài ra, chúng tôi so sánh với các phương pháp chỉ feed-forward khác. Point-e và Shap-e không thể tái cấu trúc thành công các mesh 3D từ một hình ảnh duy nhất tạo ra màu sắc và hình dạng không chính xác. Phương pháp đề xuất của chúng tôi được chứng minh mạnh mẽ qua một tập hợp đa dạng các đối tượng khác nhau với độ trung thực cao hơn và hình dạng chính xác hơn so với các baseline. Chúng tôi cũng hiển thị trong Hình 5 một số sự không nhất quán trong các hình ảnh được tạo ra từ Zero123 và cách phương pháp của chúng tôi có thể mạnh mẽ xây dựng các mesh so với baseline.

**Kết quả định lượng.** Để đánh giá phương pháp của chúng tôi và so sánh với các baseline trong việc tạo ra mesh, chúng tôi sử dụng gói PyTorch3D để tính toán khoảng cách Chamfer và Iterated Closest Point để căn chỉnh nguồn và mục tiêu để tính toán F-score. Về các metric, chúng tôi theo các công trình trước đây và sử dụng F-Score, khoảng cách Chamfer L2, và intersection-over-union (IoU). Những metric này được tóm tắt trong Bảng 1, nơi Hyper-VolTran chứng minh khả năng khái quát hóa được cải thiện trên các đối tượng chưa thấy bằng cách ghi điểm cao hơn các đối thủ cạnh tranh qua tất cả các track, với chi phí thời gian tính toán hợp lý. Tương tự, đối với chất lượng rendering, phương pháp của chúng tôi đứng đầu tất cả các công trình trước đây về 3D rendering qua tất cả các điểm: PSNR, LPIPS, và điểm tương đồng CLIP như được hiển thị trong Bảng 2.

**Thời gian xử lý.** Mặc dù phương pháp đề xuất của chúng tôi dựa vào việc mã hóa hình ảnh đầu vào thông qua một mô hình embedding hình ảnh và tạo ra trọng số của mạng SDF, độ trễ tạo sinh 3D đầy đủ chỉ khoảng 5 giây trên một GPU A100 duy nhất. Điều này ngang bằng với thời gian xử lý của One2345. Độ trễ bổ sung là do mô hình khuếch tán cơ sở. Trong trường hợp của chúng tôi, chúng tôi chọn sử dụng Zero123 để tổng hợp các góc nhìn bổ sung, thêm trung bình khoảng 40 giây mỗi đối tượng. Như được hiển thị trong Bảng 1, thời gian xử lý của Shap-e thấp hơn, dẫn đến kết quả chất lượng thấp hơn nói chung so với phương pháp của chúng tôi.

### 4.4. Phân tích và Ablation

**Bộ tạo trọng số SDF qua HyperNetwork và VolTran.** Chúng tôi điều tra hiệu quả của hai mô-đun đề xuất của chúng tôi: HyperNetwork cho SDF và VolTran. Nghiên cứu ablation này được thực hiện để phân tích tác động của mỗi mô-đun. Như được hiển thị trong Hình 6, chúng tôi có thể quan sát rằng rendering xấu đi khi không có HyperNetwork và VolTran. Trong khi không có VolTran, rendering cảnh tạo ra một số nhiễu như tác động của các đầu vào không nhất quán. Sử dụng cả hai, chúng tôi có thể đạt được kết quả rendering hợp lý.

**Số lượng mẫu.** Chúng tôi đánh giá kết quả được tạo ra bằng cách thay đổi số lượng hình ảnh hỗ trợ thu được từ mô hình khuếch tán, từ 32 xuống 4 hình ảnh từ các góc nhìn khác nhau. Hình 7 thể hiện tác động của số lượng mẫu được tạo ra từ mô hình khuếch tán. Phương pháp của chúng tôi có được lợi thế từ việc tăng số lượng hình ảnh được tạo ra để tạo thành các biểu diễn hình học. Ngược lại, một số lượng mẫu quá thấp dẫn đến sự suy giảm.

## 5. Kết luận

Trong bài báo này, chúng tôi giải quyết thách thức tạo ra cấu trúc đối tượng 3D từ một hình ảnh duy nhất. Phương pháp đề xuất của chúng tôi, được gọi là Hyper-VolTran, bao gồm một mô-đun HyperNetwork và một mô-đun transformer. Cụ thể, HyperNetworks tạo ra trọng số SDF, trong khi mô-đun transformer tạo điều kiện cho việc tổng hợp toàn cục mạnh mẽ từ các đa góc nhìn không nhất quán. Phương pháp của chúng tôi chứng minh khả năng khái quát hóa hiệu quả cho các đối tượng chưa thấy trong nhiệm vụ image-to-3D đơn, như được chứng minh bởi các đánh giá định lượng và định tính. Đáng chú ý, phương pháp của chúng tôi tạo ra mesh 3D nhanh chóng, hoàn thành nhiệm vụ này chỉ trong 45 giây mà không cần tối ưu hóa theo từng cảnh. So với các phương pháp tiên tiến, phương pháp đề xuất của chúng tôi vượt trội về cả hiệu quả thời gian và độ chính xác tái cấu trúc.
