# 2106.10404.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reparameterization/2106.10404.pdf
# Kích thước tệp: 1139824 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Huấn luyện thưa thông qua tăng cường tính dẻo cắt tỉa
với tái sinh thần kinh
Shiwei Liu1, Tianlong Chen2, Xiaohan Chen2, Zahra Atashgahi3, Lu Yin1, Huanyu Kou4,
Li Shen5,Mykola Pechenizkiy1,6,Zhangyang Wang2,Decebal Constantin Mocanu1,3
1Đại học Công nghệ Eindhoven,2Đại học Texas tại Austin
3Đại học Twente,4Đại học Leeds,5Học viện Khám phá JD,6Đại học Jyväskylä
{s.liu3,l.yin,m.pechenizkiy}@tue.nl ,{tianlong.chen,xiaohan.chen,atlaswang}@utexas.edu
{z.atashgahi,d.c.mocanu}@utwente.nl ,{khydouble1,mathshenli}@gmail.com
Tóm tắt
Các công trình về giả thuyết vé số (LTH) và cắt tỉa mạng một lần (SNIP)
hiện đang thu hút nhiều sự chú ý về cắt tỉa sau huấn luyện (cắt tỉa độ lớn lặp),
và cắt tỉa trước huấn luyện (cắt tỉa tại khởi tạo). Phương pháp trước gặp khó khăn
về chi phí tính toán cực lớn và phương pháp sau thường gặp vấn đề về hiệu suất
không đủ. So sánh với đó, cắt tỉa trong quá trình huấn luyện, một lớp phương pháp
cắt tỉa vừa tận hưởng hiệu quả huấn luyện/suy diễn vừa đạt hiệu suất tương đương,
tạm thời, đã được khám phá ít hơn. Để hiểu rõ hơn về cắt tỉa trong quá trình huấn luyện,
chúng tôi nghiên cứu định lượng tác động của cắt tỉa trong suốt quá trình huấn luyện
từ góc độ tính dẻo cắt tỉa (khả năng của các mạng bị cắt tỉa khôi phục hiệu suất ban đầu).
Tính dẻo cắt tỉa có thể giúp giải thích một số quan sát thực nghiệm khác về cắt tỉa
mạng nơ-ron trong tài liệu. Chúng tôi tiếp tục phát hiện rằng tính dẻo cắt tỉa có thể
được cải thiện đáng kể bằng cách tiêm một cơ chế lấy cảm hứng từ não bộ gọi là
tái sinh thần kinh, tức là tái sinh cùng số lượng kết nối như đã cắt tỉa. Chúng tôi thiết kế
một phương pháp cắt tỉa độ lớn dần dần (GMP) mới, có tên cắt tỉa dần dần với
tái sinh thần kinh không chi phí (GraNet), vượt trội so với công nghệ hiện tại.
Có lẽ ấn tượng nhất, phiên bản thưa-đến-thưa của nó lần đầu tiên thúc đẩy hiệu suất
huấn luyện thưa-đến-thưa vượt qua các phương pháp dày-đến-thưa khác nhau với
ResNet-50 trên ImageNet mà không kéo dài thời gian huấn luyện. Chúng tôi phát hành
tất cả mã nguồn tại https://github.com/Shiweiliuiiiiiii/GraNet .

1 Giới thiệu
Cắt tỉa mạng nơ-ron là kỹ thuật phổ biến nhất để giảm số lượng tham số, yêu cầu lưu trữ
và chi phí tính toán của các kiến trúc mạng nơ-ron hiện đại. Gần đây, cắt tỉa sau huấn luyện
[49,29,18,47,10,54,74,5,57,75] và cắt tỉa trước huấn luyện [31,30,67,63,6,11] đã là hai
lĩnh vực phát triển nhanh, được thúc đẩy bởi giả thuyết vé số (LTH) [10] và cắt tỉa mạng
một lần (SNIP) [31]. Quá trình cắt tỉa sau huấn luyện thường bao gồm tiền huấn luyện
đầy đủ một mạng dày cũng như nhiều chu kỳ huấn luyện lại (hoặc tinh chỉnh [18,17,39]
hoặc tua lại [12,54]). Khi chi phí huấn luyện của các mô hình công nghệ tiên tiến,
ví dụ GPT-3 [4] và FixEfficientNet-L2 [64] đã bùng nổ, quá trình này có thể dẫn đến
một lượng lớn chi phí phụ.

Các phương pháp mới xuất hiện gần đây cho cắt tỉa tại khởi tạo giảm đáng kể chi phí
huấn luyện bằng cách xác định một mạng con có thể huấn luyện trước quá trình huấn luyện
chính. Mặc dù hứa hẹn, các phương pháp hiện tại không thể đạt được hiệu suất tương đương
với cắt tỉa độ lớn sau huấn luyện [11].

Một phần công việc này đã được thực hiện khi Shiwei Liu làm thực tập sinh tại Học viện Khám phá JD.
Hội nghị thứ 35 về Hệ thống Xử lý Thông tin Nơ-ron (NeurIPS 2021).arXiv:2106.10404v4 [cs.LG] 6 Feb 2022

--- TRANG 2 ---
Tái sinh thần kinh Số lượng trọng số Thời gian huấn luyện Cắt tỉa dần dần GraNet Hình 1: Cái nhìn tổng quan về GraNet. Trái: Cắt tỉa dần dần bắt đầu với một mạng con thưa và dần dần cắt tỉa mạng con đến độ thưa mục tiêu trong quá trình huấn luyện. Phải: Chúng tôi thực hiện tái sinh thần kinh không chi phí sau mỗi bước cắt tỉa dần dần. Các khối/đường màu xanh nhạt đề cập đến các kết nối "bị hỏng" và các khối/đường màu cam đề cập đến các kết nối mới được tái sinh.

So với hai lớp cắt tỉa được đề cập ở trên, cắt tỉa trong quá trình huấn luyện là một lớp phương pháp thu hoạch lợi ích tăng tốc của độ thưa sớm trong quá trình huấn luyện và đồng thời đạt hiệu suất hứa hẹn bằng cách tham khảo thông tin thu được trong quá trình huấn luyện. Có một số công trình [77, 13, 33] cố gắng dần dần cắt tỉa mạng đến độ thưa mong muốn trong quá trình huấn luyện, trong khi chúng chủ yếu tập trung vào cải thiện hiệu suất. Cho đến nay, việc hiểu cắt tỉa trong quá trình huấn luyện đã được khám phá ít hơn do quá trình động phức tạp hơn của nó, và khoảng cách hiệu suất vẫn tồn tại giữa cắt tỉa trong quá trình huấn luyện và huấn luyện dày đầy đủ.

Để hiểu rõ hơn tác động của cắt tỉa trong quá trình tối ưu hóa (không phải tại suy diễn), chúng tôi nghiên cứu khả năng của các mô hình bị cắt tỉa khôi phục hiệu suất ban đầu sau một thời gian huấn luyện tiếp tục ngắn với tốc độ học hiện tại, mà chúng tôi gọi là tính dẻo cắt tỉa (xem Mục 3.1 để có định nghĩa chính thức hơn). Lấy cảm hứng từ cơ chế tái sinh thần kinh trong hệ thần kinh nơi các nơ-ron và kết nối mới được tổng hợp để khôi phục tổn thương trong hệ thần kinh [26,41,73], chúng tôi kiểm tra xem việc cho phép mạng bị cắt tỉa tái sinh các kết nối mới có thể cải thiện tính dẻo cắt tỉa hay không, và do đó góp phần vào cắt tỉa trong quá trình huấn luyện. Chúng tôi kết quả là đề xuất một phương pháp hiệu quả về tham số để tái sinh các kết nối mới trong quá trình cắt tỉa dần dần. Khác với các công trình hiện có để hiểu cắt tỉa chủ yếu tập trung vào huấn luyện dày-đến-thưa [42] (huấn luyện một mô hình dày và cắt tỉa nó đến độ thưa mục tiêu), chúng tôi cũng xem xét huấn luyện thưa-đến-thưa (huấn luyện một mô hình thưa nhưng tạo lại mẫu độ thưa một cách thích ứng) mà gần đây đã nhận được sự quan tâm tăng cao trong học máy [44, 3, 9, 48, 8, 37, 36].

Tóm lại, chúng tôi có những phát hiện chính sau trong quá trình nghiên cứu:

#1. Cả tỷ lệ cắt tỉa và tốc độ học đều quan trọng đối với tính dẻo cắt tỉa. Khi cắt tỉa với tỷ lệ cắt tỉa thấp (ví dụ 0.2), cả huấn luyện dày-đến-thưa và huấn luyện thưa-đến-thưa đều có thể dễ dàng khôi phục từ cắt tỉa. Ngược lại, nếu quá nhiều tham số bị loại bỏ cùng một lúc, hầu như tất cả các mô hình đều bị giảm độ chính xác. Phát hiện này tạo ra kết nối với thành công của cắt tỉa độ lớn lặp [10,54,5,6,65], nơi thường một quá trình cắt tỉa với tỷ lệ cắt tỉa nhỏ (ví dụ 0.2) cần được lặp lại một cách lặp để có hiệu suất tốt.

Tính dẻo cắt tỉa cũng giảm dần khi tốc độ học giảm. Khi cắt tỉa xảy ra trong giai đoạn huấn luyện với tốc độ học lớn, các mô hình có thể dễ dàng khôi phục từ cắt tỉa (đến một mức độ nhất định). Tuy nhiên, tính dẻo cắt tỉa giảm đáng kể sau lần giảm tốc độ học thứ hai, dẫn đến tình huống mà các mạng bị cắt tỉa không thể khôi phục với huấn luyện tiếp tục. Phát hiện này giúp giải thích một số quan sát (1) đối với cắt tỉa độ lớn dần dần (GMP), việc kết thúc cắt tỉa trước lần giảm tốc độ học thứ hai luôn là tối ưu [77,13]; (2) huấn luyện thưa động (DST) hưởng lợi từ tỷ lệ cắt tỉa giảm đơn điệu với lịch cập nhật cosin hoặc tuyến tính [8,9]; (3) các kỹ thuật tua lại [12,54] vượt trội hơn tinh chỉnh vì tua lại huấn luyện lại các mạng con với lịch tốc độ học ban đầu trong khi tinh chỉnh thường huấn luyện lại với tốc độ học nhỏ nhất.

#2. Tái sinh thần kinh cải thiện tính dẻo cắt tỉa. Tái sinh thần kinh [41,73] đề cập đến sự tái phát triển hoặc sửa chữa các mô, tế bào thần kinh, hoặc sản phẩm tế bào. Về mặt khái niệm, nó bao gồm việc tổng hợp các nơ-ron, tế bào thần kinh đệm, sợi trục, myelin, hoặc khớp thần kinh mới, cung cấp tài nguyên bổ sung trong dài hạn để thay thế những thứ bị hỏng do chấn thương, và đạt được sự phục hồi chức năng lâu dài. Cơ chế như vậy có liên quan chặt chẽ đến tính dẻo não [51], và chúng tôi mượn khái niệm này để phát triển một chế độ tính toán. Chúng tôi cho thấy rằng, trong khi tái sinh cùng số lượng kết nối như đã cắt tỉa, tính dẻo cắt tỉa được quan sát là cải thiện đáng kể, cho thấy một mô hình có tính dẻo thần kinh hơn đang được phát triển. Tuy nhiên, nó tăng chi phí bộ nhớ và tính toán và dường như mâu thuẫn với lợi ích của cắt tỉa-trong-quá-trình-huấn-luyện. Điều này tuy nhiên đặt ra câu hỏi: liệu chúng ta có thể đạt được tái sinh thần kinh hiệu quả trong quá trình huấn luyện mà không có chi phí bổ sung? Chúng tôi cung cấp câu trả lời khẳng định cho câu hỏi này.

#3. Tính dẻo cắt tỉa với tái sinh thần kinh có thể được tận dụng để tăng cường đáng kể hiệu suất huấn luyện thưa. Những phát hiện được đề cập ở trên về tính dẻo cắt tỉa có thể tổng quát hóa đến mức hiệu suất cuối cùng dưới một huấn luyện tiếp tục đầy đủ đến cuối. Bắt chước hành vi tái sinh thần kinh [41,73], chúng tôi đề xuất một phương pháp huấn luyện thưa mới – cắt tỉa dần dần với tái sinh thần kinh không chi phí (GraNet), có khả năng thực hiện tái sinh mà không tăng số lượng tham số.

Trong các thí nghiệm, GraNet thiết lập thanh hiệu suất công nghệ tiên tiến mới cho huấn luyện dày-đến-thưa và huấn luyện thưa-đến-thưa, tương ứng. Đặc biệt, phương pháp sau lần đầu tiên thúc đẩy hiệu suất huấn luyện thưa-đến-thưa vượt qua các phương pháp dày-đến-thưa khác nhau với một biên độ lớn mà không kéo dài thời gian huấn luyện, với ResNet-50 trên ImageNet. Bên cạnh cải thiện hiệu suất nhất quán, chúng tôi thấy các mạng con mà GraNet học được chính xác hơn những mạng được học bởi phương pháp cắt tỉa dần dần hiện có, cung cấp giải thích cho thành công của GraNet.

2 Công trình liên quan

Cắt tỉa sau huấn luyện. Các phương pháp tạo ra một mạng nơ-ron thưa từ một mạng được huấn luyện trước bằng cách cắt tỉa các trọng số hoặc nơ-ron không quan trọng, theo hiểu biết tốt nhất của chúng tôi, được đề xuất trong [24] và [50]. Sau đó, các phương pháp cắt tỉa khác nhau đã xuất hiện để cung cấp các phương pháp ngày càng hiệu quả để xác định các mạng nơ-ron thưa cho suy diễn. Tiêu chí cắt tỉa bao gồm độ lớn trọng số [18,10], gradient [61] Hessian [29,19,59], khai triển Taylor [47,46], v.v. Phân rã thứ hạng thấp [7,23,17,71] cũng được sử dụng để tạo ra độ thưa có cấu trúc theo các kênh hoặc bộ lọc. Hầu hết các phương pháp cắt tỉa được đề cập ở trên đòi hỏi nhiều chu kỳ cắt tỉa và huấn luyện lại để đạt được hiệu suất mong muốn.

Cắt tỉa trong quá trình huấn luyện. Thay vì kế thừa trọng số từ một mô hình được huấn luyện trước, một số công trình cố gắng khám phá các mạng nơ-ron thưa hoạt động tốt với một quá trình huấn luyện duy nhất. Cắt tỉa độ lớn dần dần (GMP), được giới thiệu trong [77] và nghiên cứu thêm trong [13], dần dần làm thưa mạng nơ-ron trong quá trình huấn luyện cho đến khi đạt được độ thưa mong muốn. Bên cạnh đó, [40] và [68] là các công trình trước đó thực thi mạng thưa trong quá trình huấn luyện thông qua regularization L0 và L1, tương ứng. [60,34,55,70,28] tiến xa hơn bằng cách giới thiệu các heuristic độ thưa có thể huấn luyện để học các mặt nạ thưa và trọng số đồng thời. Tất cả các phương pháp này đều được phân loại là huấn luyện dày-đến-thưa vì chúng bắt đầu từ một mạng dày.

Huấn luyện thưa động (DST) [44,3,48,8,9,36,35,25] là một lớp phương pháp khác cắt tỉa các mô hình trong quá trình huấn luyện. Yếu tố chính của DST là nó bắt đầu từ một mạng thưa được khởi tạo ngẫu nhiên và tối ưu hóa cấu trúc liên kết thưa cũng như các trọng số đồng thời trong quá trình huấn luyện (huấn luyện thưa-đến-thưa). Mà không có thời gian huấn luyện kéo dài [37], huấn luyện thưa-đến-thưa thường kém hơn huấn luyện dày-đến-thưa về độ chính xác dự đoán. Để biết thêm chi tiết, xem khảo sát của [43, 21].

Cắt tỉa trước huấn luyện. Được thúc đẩy bởi SNIP [31], nhiều công trình [67,63,6] đã xuất hiện gần đây để khám phá khả năng thu được một mạng nơ-ron thưa có thể huấn luyện trước quá trình huấn luyện chính. [11] chứng minh rằng các phương pháp hiện tại cho cắt tỉa tại khởi tạo hoạt động tốt như nhau khi các trọng số không bị cắt tỉa được xáo trộn ngẫu nhiên, điều này tiết lộ rằng những gì các phương pháp này khám phá là tỷ lệ độ thưa theo lớp, thay vì các giá trị trọng số và vị trí không thể thiếu. Phân tích của chúng tôi cho thấy rằng cả vị trí mặt nạ và giá trị trọng số đều quan trọng đối với GraNet.

3 Phương pháp cho tính dẻo cắt tỉa

Mục tiêu chính của bài báo này là nghiên cứu tác động của cắt tỉa cũng như tái sinh thần kinh trên các mạng nơ-ron trong quá trình huấn luyện tiêu chuẩn. Do đó, chúng tôi không xem xét cắt tỉa sau huấn luyện và cắt tỉa trước huấn luyện. Dưới đây, chúng tôi giới thiệu chi tiết định nghĩa của tính dẻo cắt tỉa và thiết kế thí nghiệm mà chúng tôi đã sử dụng để nghiên cứu tính dẻo cắt tỉa.

--- TRANG 3 ---
3.1 Các thước đo

Hãy ký hiệu Wt ∈ Rd là các trọng số của mạng và mt ∈ {0,1}d là mặt nạ nhị phân thu được từ phương pháp cắt tỉa tại epoch t. Do đó, mạng bị cắt tỉa có thể được ký hiệu là Wt ⊙ mt. Gọi T là tổng số epoch mà mô hình nên được huấn luyện. Gọi CONTRAINk(Wt ⊙ mt; α) đề cập đến hàm tiếp tục huấn luyện mô hình bị cắt tỉa trong k epoch với lịch tốc độ học α.

Định nghĩa tính dẻo cắt tỉa. Chúng tôi định nghĩa tính dẻo cắt tỉa là εtCONTRAINk(Wt⊙mt;αt) − εtPRE, trong đó εtPRE là độ chính xác kiểm tra được đo trước khi cắt tỉa và εtCONTRAINk(Wt⊙mt;αt) là độ chính xác kiểm tra được đo sau k epoch huấn luyện tiếp tục CONTRAINk(Wt ⊙ mt; αt). Cụ thể, để hiểu rõ hơn tác động của cắt tỉa đối với trạng thái mô hình hiện tại và để tránh tác động của sự suy giảm tốc độ học, chúng tôi cố định tốc độ học là tốc độ khi mô hình được cắt tỉa, tức là αt. Cài đặt này cũng hấp dẫn đối với GMP [77,13] và DST [44,9,48,37] trong đó hầu hết các mô hình bị cắt tỉa được huấn luyện tiếp tục với tốc độ học hiện tại trong một thời gian.

Khoảng cách hiệu suất cuối cùng. Tuy nhiên, chúng tôi cũng điều tra tác động của cắt tỉa đối với hiệu suất cuối cùng, đó là, huấn luyện tiếp tục các mạng bị cắt tỉa đến cuối với lịch tốc độ học còn lại CONTRAINTt(Wt ⊙ mt; α[t+1:T]). Trong trường hợp này, chúng tôi báo cáo εtCONTRAINTt(Wt⊙mt;α[t+1:T]) − εtFINAL, trong đó εtFINAL là độ chính xác kiểm tra cuối cùng của các mô hình không bị cắt tỉa.

3.2 Kiến trúc và tập dữ liệu

Chúng tôi chọn hai kiến trúc thường được sử dụng để nghiên cứu tính dẻo cắt tỉa, VGG-19 [58] với chuẩn hóa batch trên CIFAR-10 [27], và ResNet-20 [20] trên CIFAR-10.

Chúng tôi chia sẻ tóm tắt về mạng, dữ liệu và siêu tham số của huấn luyện dày-đến-thưa trong Bảng 1. Chúng tôi sử dụng các triển khai và siêu tham số tiêu chuẩn có sẵn trực tuyến, ngoại trừ kích thước batch nhỏ cho ResNet-50 trên ImageNet do tài nguyên phần cứng hạn chế (2 Tesla V100). Tất cả độ chính xác đều phù hợp với các baseline được báo cáo trong các tài liệu tham khảo [8,11,67,9,37].

Bảng 1: Tóm tắt về kiến trúc và siêu tham số chúng tôi nghiên cứu trong bài báo này.
Mô hình | Dữ liệu | #Epoch | Kích thước Batch | LR | Suy giảm LR, Epoch | Suy giảm trọng số | Độ chính xác kiểm tra
ResNet-20 | CIFAR-10 | 160 | 128 | 0.1 (γ = 0.9) | ×10, [80, 120] | 0.0005 | 92.41±0.04
VGG-19 | CIFAR-10 | 160 | 128 | 0.1 (γ = 0.9) | ×10, [80, 120] | 0.0005 | 93.85±0.05
CIFAR-100 | 160 | 128 | 0.1 (γ = 0.9) | ×10, [80, 120] | 0.0005 | 73.43±0.08
ResNet-50 | CIFAR-10 | 160 | 128 | 0.1 (γ = 0.9) | ×10, [80, 120] | 0.0005 | 94.75±0.01
CIFAR-100 | 160 | 128 | 0.1 (γ = 0.9) | ×10, [80, 120] | 0.0005 | 78.23±0.18
ImageNet | 100 | 64 | 0.1 (γ = 0.9) | ×10, [30, 60, 90] | 0.0004 | 76.80±0.09

3.3 Cách cắt tỉa và cách tái sinh

Cắt tỉa có cấu trúc và không cấu trúc. Chúng tôi xem xét cắt tỉa không cấu trúc và có cấu trúc trong bài báo này. Cắt tỉa có cấu trúc cắt tỉa trọng số theo nhóm, hoặc loại bỏ toàn bộ nơ-ron, bộ lọc tích chập, hoặc kênh, cho phép tăng tốc với phần cứng sẵn có. Đặc biệt, chúng tôi chọn phương pháp cắt tỉa bộ lọc được sử dụng trong Li et al. [32]. Độ thưa không cấu trúc là một hướng hứa hẹn hơn không chỉ do hiệu suất xuất sắc ở độ thưa cực mà còn do sự hỗ trợ ngày càng tăng cho hoạt động thưa trong phần cứng thực tế [35,14,52,76,22]. Ví dụ, Liu et al. [35] minh họa lần đầu tiên tiềm năng thực sự của DST, chứng minh cải thiện hiệu quả huấn luyện/suy diễn đáng kể so với huấn luyện dày. Khác với các quy ước trước [77,13,33,2] nơi các giá trị của trọng số bị cắt tỉa được giữ lại, chúng tôi đặt các trọng số bị cắt tỉa về không để loại bỏ thông tin lịch sử cho tất cả các triển khai trong bài báo này.

Cắt tỉa độ lớn. Chúng tôi cắt tỉa các trọng số có độ lớn nhỏ nhất, vì nó đã phát triển thành phương pháp tiêu chuẩn khi cắt tỉa xảy ra trong quá trình huấn luyện, ví dụ GMP [77,13] và DST [44,9,37]. Chúng tôi cũng nhận thức được các tiêu chí cắt tỉa khác bao gồm nhưng không giới hạn ở Hessian [29,19,59], khai triển Taylor [47,46], độ nhạy kết nối [31], Gradient Flow [67], Neural Tangent Kernel [38,16].

Cắt tỉa một lần. Để cô lập tác động cắt tỉa ở các giai đoạn huấn luyện khác nhau và để tránh tương tác giữa hai lần lặp cắt tỉa, chúng tôi tập trung vào cắt tỉa một lần. Xin lưu ý rằng cắt tỉa lặp cũng có thể được tổng quát hóa trong cài đặt của chúng tôi, vì thiết kế thí nghiệm của chúng tôi bao gồm các mạng nơ-ron được huấn luyện ở các độ thưa khác nhau và mỗi mạng được cắt tỉa thêm với các tỷ lệ cắt tỉa khác nhau.

Cắt tỉa theo lớp và cắt tỉa toàn cục. Chúng tôi nghiên cứu cả cắt tỉa độ lớn theo lớp và cắt tỉa độ lớn toàn cục cho tính dẻo cắt tỉa. Cắt tỉa độ lớn toàn cục cắt tỉa các lớp khác nhau cùng nhau và dẫn đến phân phối độ thưa không đồng nhất; cắt tỉa theo lớp hoạt động từng lớp một, dẫn đến phân phối đồng nhất.

Tái sinh dựa trên gradient. Lược đồ tái sinh đơn giản nhất là kích hoạt ngẫu nhiên các kết nối mới [3,44]. Tuy nhiên, sẽ mất rất nhiều thời gian để tái sinh ngẫu nhiên khám phá các kết nối quan trọng, đặc biệt là đối với các độ thưa rất cực. Thay vào đó, gradient, bao gồm cả những gradient cho các kết nối có trọng số không, cung cấp chỉ số tốt cho tầm quan trọng của kết nối. Vì lý do này, chúng tôi tập trung vào tái sinh dựa trên gradient được đề xuất trong Rigged Lottery (RigL) [9], tức là tái sinh cùng số lượng kết nối như đã cắt tỉa với độ lớn gradient lớn nhất.

3.4 Kết quả thí nghiệm

Chúng tôi nghiên cứu tính dẻo cắt tỉa trong quá trình huấn luyện có/không có tái sinh, cho cả huấn luyện dày và huấn luyện thưa. Chúng tôi báo cáo kết quả của ResNet-20 trên CIFAR-10 với cắt tỉa toàn cục không cấu trúc trong phần chính của bài báo. Phần còn lại của các thí nghiệm được đưa ra trong Phụ lục A. Trừ khi có ghi chú khác, kết quả tương tự định tính trên tất cả các mạng. Cụ thể, trước tiên chúng tôi tiền huấn luyện các mạng ở bốn mức độ thưa, bao gồm 0, 0.5, 0.9 và 0.98. Các mạng nơ-ron thưa được huấn luyện với phân phối đồng nhất (tức là tất cả các lớp có cùng độ thưa). Chúng tôi tiếp tục chọn bốn tỷ lệ cắt tỉa, ví dụ 0.2, 0.5, 0.9 và 0.98, để đo tính dẻo cắt tỉa tương ứng của các mạng được tiền huấn luyện.

Tính dẻo cắt tỉa. Chúng tôi tiếp tục huấn luyện mô hình bị cắt tỉa trong 30 epoch và báo cáo tính dẻo cắt tỉa trong Hình 2. Nhìn chung, lịch tốc độ học, tỷ lệ cắt tỉa và độ thưa của các mô hình ban đầu đều có tác động lớn đến tính dẻo cắt tỉa. Tính dẻo cắt tỉa giảm khi tốc độ học suy giảm cho tất cả các mô hình với các mức độ thưa khác nhau. Các mô hình được huấn luyện với tốc độ học lớn 0.1 có thể dễ dàng khôi phục, hoặc vượt qua hiệu suất ban đầu ngoại trừ tỷ lệ cắt tỉa cực lớn 0.98. Tuy nhiên, các mô hình thu được trong các giai đoạn huấn luyện sau chỉ có thể khôi phục với các lựa chọn tỷ lệ cắt tỉa nhẹ, ví dụ 0.2 (đường màu cam) và 0.5 (đường màu xanh lá).

Tiếp theo chúng tôi chứng minh tác động của tái sinh kết nối đối với tính dẻo cắt tỉa trong hàng dưới của Hình 2. Rõ ràng là tái sinh kết nối cải thiện đáng kể tính dẻo cắt tỉa của tất cả các trường hợp, đặc biệt là đối với các mô hình bị cắt tỉa quá mức (đường màu tím). Tuy nhiên, ngay cả với tái sinh kết nối, tính dẻo cắt tỉa vẫn bị suy giảm hiệu suất khi cắt tỉa xảy ra sau khi tốc độ học giảm.

[Hình 2: Cắt tỉa không cấu trúc: Tính dẻo cắt tỉa dưới một huấn luyện tiếp tục 30 epoch có và không có tái sinh kết nối cho ResNet-20 trên CIFAR-10. Các đường đỏ thẳng đứng đề cập đến các điểm khi tốc độ học bị suy giảm. "Độ thưa tiền huấn luyện" đề cập đến độ thưa ban đầu của các mạng được tiền huấn luyện trước khi cắt tỉa. Phương pháp cắt tỉa là cắt tỉa toàn cục độ lớn.]

Khoảng cách hiệu suất cuối cùng. So với trạng thái mô hình hiện tại, mọi người có thể quan tâm hơn đến tác động của cắt tỉa đối với hiệu suất cuối cùng. Chúng tôi tiếp tục đo khoảng cách hiệu suất giữa độ chính xác kiểm tra ban đầu của các mô hình không bị cắt tỉa và độ chính xác kiểm tra cuối cùng của mô hình bị cắt tỉa dưới một huấn luyện tiếp tục đầy đủ CONTRAINTt(Wt ⊙ mt; α[t+1:T]) trong Hình 3.

Chúng tôi quan sát rằng, trong trường hợp này, tốc độ học lớn không tận hưởng cải thiện hiệu suất lớn, nhưng vẫn vậy, khoảng cách hiệu suất tăng khi tốc độ học giảm. Hợp lý để suy đoán rằng việc cải thiện độ chính xác của tính dẻo cắt tỉa với tốc độ học lớn, 0.1, là do hiệu suất chưa hội tụ trong giai đoạn đầu của huấn luyện. Bên cạnh đó, thật đáng ngạc nhiên khi thấy rằng hiệu suất cuối cùng của các mạng cực thưa (ví dụ cột thứ ba và cột thứ tư) được hưởng lợi đáng kể từ cắt tỉa nhẹ. Một lần nữa, khả năng của mô hình bị cắt tỉa khôi phục từ cắt tỉa được cải thiện đáng kể sau khi tái sinh các kết nối trở lại.

4 Cắt tỉa dần dần với tái sinh thần kinh không chi phí

Cho đến nay, chúng ta đã biết rằng việc tái sinh các kết nối quan trọng cho các mô hình bị cắt tỉa trong quá trình huấn luyện cải thiện đáng kể tính dẻo cắt tỉa cũng như hiệu suất cuối cùng. Tuy nhiên, việc tái sinh ngây thơ các kết nối bổ sung làm tăng số lượng tham số và mâu thuẫn với động lực của cắt tỉa dần dần.

Lấy cảm hứng từ cơ chế tái sinh thần kinh trong hệ thần kinh, chúng tôi đề xuất một phương pháp huấn luyện thưa mới mà chúng tôi gọi là cắt tỉa dần dần với tái sinh thần kinh không chi phí (GraNet). GraNet tham khảo thông tin được sản xuất trong suốt quá trình huấn luyện và tái sinh các kết nối quan trọng trong quá trình huấn luyện theo cách hiệu quả về tham số. Xem Phụ lục B.1 cho mã giả của GraNet. Chúng tôi giới thiệu các thành phần chính của GraNet dưới đây.

4.1 Cắt tỉa dần dần

Chúng tôi tuân theo lược đồ cắt tỉa dần dần được sử dụng trong [77] và dần dần làm thưa mạng dày đến mức độ thưa mục tiêu qua n lần lặp cắt tỉa. Hãy định nghĩa si là độ thưa ban đầu, sf là độ thưa mục tiêu, t0 là epoch bắt đầu của cắt tỉa dần dần, tf là epoch kết thúc của cắt tỉa dần dần, và Δt là tần suất cắt tỉa. Tỷ lệ cắt tỉa của mỗi lần lặp cắt tỉa là:

st = sf + (si − sf)(1 − (t−t0)/(Δt))^3; t ∈ {t0, t0 + Δt, ..., t0 + nΔt}. (1)

Chúng tôi chọn cắt tỉa toàn cục cho phương pháp của chúng tôi vì nó thường đạt hiệu suất tốt hơn so với cắt tỉa đồng nhất. Chúng tôi cũng báo cáo hiệu suất của độ thưa đồng nhất như được sử dụng trong [13] trong Phụ lục C.3.

Các phương pháp cắt tỉa dần dần thông thường [77,13] thay đổi mặt nạ (không phải giá trị trọng số) để thực hiện hoạt động cắt tỉa, do đó các kết nối bị cắt tỉa có khả năng được kích hoạt lại trong các giai đoạn huấn luyện sau. Mặc dù vậy, vì các trọng số của các kết nối bị cắt tỉa không được cập nhật, chúng có cơ hội nhỏ nhận được các bản cập nhật đủ để vượt qua ngưỡng cắt tỉa. Điều này cản trở việc tái sinh các kết nối quan trọng.

4.2 Tái sinh thần kinh không chi phí

Sự khác biệt chính giữa GraNet và các phương pháp GMP thông thường [77,13] là Tái sinh thần kinh không chi phí. Bắt chước sự tái sinh thần kinh của hệ thần kinh ngoại biên [41,73] nơi các nơ-ron và kết nối mới được tổng hợp để thay thế những cái bị hỏng, trước tiên chúng tôi phát hiện và loại bỏ các kết nối "bị hỏng", sau đó tái sinh cùng số lượng kết nối mới. Bằng cách làm điều này, chúng ta có thể đạt được tái sinh kết nối mà không tăng số lượng kết nối.

Cụ thể, chúng tôi xác định các kết nối "bị hỏng" là những kết nối có độ lớn trọng số nhỏ nhất. Độ lớn nhỏ cho biết rằng hoặc gradient của trọng số nhỏ hoặc một số lượng lớn dao động xảy ra với hướng gradient. Do đó, những trọng số này có đóng góp nhỏ cho loss huấn luyện và có thể được loại bỏ. Một lần nữa, chúng tôi sử dụng gradient làm điểm quan trọng cho tái sinh, giống như phương pháp regrow được sử dụng trong RigL [9].

Tại sao chúng tôi gọi nó là "Tái sinh thần kinh không chi phí"? Ngoài việc không tăng số lượng kết nối (tham số), việc truyền ngược của phương pháp chúng tôi là thưa hầu hết thời gian mặc dù việc tái sinh của chúng tôi sử dụng gradient dày để xác định các kết nối quan trọng. Chúng tôi thực hiện tái sinh thần kinh ngay sau mỗi bước cắt tỉa dần dần, có nghĩa là việc tái sinh chỉ xảy ra một lần mỗi vài nghìn lần lặp. Chi phí bổ sung để tính toán gradient dày có thể được phân bổ so với toàn bộ chi phí huấn luyện. So với các phương pháp [33,69] yêu cầu cập nhật tất cả trọng số trong quá trình truyền ngược, phương pháp của chúng tôi hiệu quả hơn nhiều về huấn luyện, vì khoảng 2/3 FLOPs huấn luyện là do quá trình truyền ngược [9, 72].

Hãy ký hiệu r là tỷ lệ số lượng kết nối được tái sinh với tổng số kết nối; W là trọng số mạng. Trước tiên chúng tôi loại bỏ tỷ lệ r các trọng số "bị hỏng" có độ lớn nhỏ nhất bằng:

W' = TopK(|W|, 1-r): (2)

Ở đây TopK(v,k) trả về tensor trọng số giữ lại k-tỷ lệ phần tử hàng đầu từ v. Ngay sau đó, chúng tôi tái sinh tỷ lệ r kết nối mới dựa trên độ lớn gradient:

W = W' + TopK(|∇L/∂W'|, r); (3)

trong đó |∇L/∂W'| là độ lớn gradient của các trọng số không. Chúng tôi thực hiện Tái sinh thần kinh không chi phí từng lớp một từ đầu huấn luyện đến cuối.

GraNet có thể tự nhiên tổng quát hóa cho kịch bản huấn luyện dày-đến-thưa và kịch bản huấn luyện thưa-đến-thưa bằng cách đặt mức độ thưa ban đầu si = 0 và si > 0 trong Eq. (1), tương ứng. Để đơn giản, chúng tôi đặt si = 0.5, t0 = 0, và tf là epoch khi thực hiện lần giảm tốc độ học đầu tiên cho huấn luyện thưa-đến-thưa. Khác với các phương pháp huấn luyện thưa-đến-thưa hiện tại, tức là SET [44], RigL [9], và ITOP [37], trong đó độ thưa được cố định trong suốt quá trình huấn luyện, GraNet bắt đầu từ một mô hình dày hơn nhưng vẫn thưa và dần dần cắt tỉa mô hình thưa đến độ thưa mong muốn. Mặc dù bắt đầu với nhiều tham số hơn, kỹ thuật cắt tỉa toàn cục của cắt tỉa dần dần giúp GraNet nhanh chóng phát triển thành phân phối độ thưa tốt hơn so với RigL với FLOPs feedforward thấp hơn và độ chính xác kiểm tra cao hơn. Hơn nữa, GraNet làm thưa tất cả các lớp bao gồm lớp tích chập đầu tiên và lớp kết nối đầy đủ cuối cùng.

4.3 Kết quả thí nghiệm

Chúng tôi tiến hành các thí nghiệm khác nhau để đánh giá hiệu quả của GraNet. Chúng tôi so sánh GraNet với các phương pháp dày-đến-thưa và thưa-đến-thưa khác nhau. Kết quả của Rigged Lottery (RigL) và GMP với CIFAR-10/100 được tái tạo bằng cách triển khai của chúng tôi với PyTorch để sự khác biệt duy nhất giữa GraNet và GMP là Tái sinh thần kinh không chi phí. Đối với mỗi mô hình, chúng tôi chia kết quả thành ba nhóm từ trên xuống dưới: cắt tỉa tại khởi tạo, huấn luyện thưa động và các phương pháp dày-đến-thưa. Xem Phụ lục B để biết thêm chi tiết triển khai được sử dụng trong các thí nghiệm. GraNet (si = 0.5) đề cập đến phiên bản thưa-đến-thưa và GraNet (si = 0) đề cập đến phiên bản dày-đến-thưa.

CIFAR-10/100. Kết quả của CIFAR-10/100 được chia sẻ trong Bảng 2. Chúng ta có thể quan sát rằng sự khác biệt hiệu suất giữa các phương pháp khác nhau trên CIFAR-10 thường nhỏ, nhưng vẫn vậy, GraNet (si = 0) luôn cải thiện hiệu suất so với GMP ngoại trừ độ thưa 95%, và đạt độ chính xác cao nhất trong 4 trên 6 trường hợp. Về dữ liệu phức tạp hơn CIFAR-100, sự khác biệt hiệu suất giữa các phương pháp cắt tỉa trong-quá-trình-huấn-luyện và các phương pháp cắt tỉa trước-huấn-luyện lớn hơn nhiều. GraNet (si = 0) một lần nữa luôn vượt trội hơn GMP với tất cả độ thưa, làm nổi bật lợi ích của Tái sinh thần kinh không chi phí. Có lẽ thú vị hơn là GraNet (si = 0) thậm chí vượt trội hơn phương pháp sau-huấn-luyện, subdifferential inclusion for sparsity (SIS), với một biên độ lớn.

Về huấn luyện thưa-đến-thưa, GraNet được đề xuất của chúng tôi (si = 0.5) có hiệu suất vượt trội so với các phương pháp khác. Đặc biệt ở độ thưa cực 0.98, phương pháp của chúng tôi vượt trội hơn RigL 1.40% và 2.22% với VGG-19 trên CIFAR-10 và CIFAR-100, tương ứng.

ImageNet. Do kích thước dữ liệu nhỏ, các thí nghiệm với CIFAR-10/100 có thể không đủ để rút ra kết luận vững chắc. Chúng tôi tiếp tục đánh giá phương pháp của mình với ResNet-50 trên ImageNet trong Bảng 3.

[Tiếp tục với phần còn lại...]
