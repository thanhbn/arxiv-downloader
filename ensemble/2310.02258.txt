# 2310.02258.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ensemble/2310.02258.pdf
# File size: 1520475 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A Neural Scaling Law from Lottery Ticket Ensembling
Ziming Liu
MIT & IAIFI
zmliu@mit.eduMax Tegmark
MIT & IAIFI
tegmark@mit.edu
Abstract
Authors’ note: the theory in this paper
is questionable; we are trying our best
to fix it. Empirical results still stand.
Neural scaling laws (NSL) refer to the phenomenon where model performance
improves with scale. Sharma & Kaplan analyzed NSL using approximation theory
and predict that MSE losses decay as N−α,α= 4/d, where Nis the number of
model parameters, and dis the intrinsic input dimension. Although their theory
works well for some cases (e.g., ReLU networks), we surprisingly find that a
simple 1D problem y=x2manifests a different scaling law ( α= 1) from their
predictions ( α= 4). We opened the neural networks and found that the new scaling
law originates from lottery ticket ensembling : a wider network on average has
more "lottery tickets", which are ensembled to reduce the variance of outputs. We
support the ensembling mechanism by mechanistically interpreting single neural
networks, as well as studying them statistically. We attribute the N−1scaling law
to the "central limit theorem" of lottery tickets. Finally, we discuss its potential
implications for large language models and statistical physics-like theories of
learning.
1 Introduction
Neural scaling laws (NSL), the phenomenon where model performance improves as the model size
scales up, has been widely observed in deep learning [ 1–8]. Typically the losses follow ℓ∝N−α
where Nis the number of model parameters, and α > 0is the scaling exponent. Understanding
the mechanisms of neural scaling laws is important both theoretically and empirically. Currently,
two main theories of neural scaling laws are proposed: one is the approximation theory [3,9,10],
claiming that the scaling law comes from regression on a data manifold. In particular, the scaling
exponent is α= 4/dwhere dis the intrinsic input dimension [ 3,9] or maximum arity [ 10]. The other
is the quanta theory [8], which suggests that the scaling law comes from the hierarchy of subtasks,
and the scaling exponent αdepends on the heavy-tailedness of the subtask distribution.
This paper aims to reveal yet another mechanism of neural scaling law from lottery ticket ensembling .
Our theory is motivated by empirical observations in an extremely simple setup, i.e., training two-
layer (ReLU or SiLU) networks with the Adam optimizer to fit the squared function y=x2. The
approximation theory would predict that α= 4for ReLU networks, but cannot predict αfor SiLU
networks. The quanta theory is also not applicable to these cases. Our empirical results are quite
intriguing (shown in Figure 1c): For ReLU networks, the loss curve decay as N−4at the beginning,
but soon slows down to N−1asNincreases. For SiLU networks, the loss curve behaves as N−1
consistently.
Now comes the question: what gives rise to this N−1scaling law? By reverse engineering single
networks and doing statistical analysis on a population of networks, we attribute the new neural
Preprint. Under review.arXiv:2310.02258v2  [cs.LG]  2 Feb 2024

--- PAGE 2 ---
(a)
 (b)
10 20 30 4050
N (width)107
106
105
MSE loss
N4
N1
LeakyReLU
SiLU(c)
Figure 1: (a) The ℓ∝N−4/dscaling law from Sharma and Kaplan [ 9] can be understood from
approximating a d-dimensional function while data points lie uniformly inside a hypercube. (b) Our
simple setup is training a two-layer SiLU or (Leaky)ReLU network (one hidden layer with width N)
to fit the squared function y=x2. (c) A surprising N−1scaling emerges for SiLU networks and at
the tail of ReLU networks, while [9]’s prediction N−4only appears at the early stage of ReLU.
scaling law to lottery ticket ensembling : a network with width Ncontain n"lottery tickets" ( n∝∼N),
ensembling of which can reduce the variance of outputs as n−1∝∼N−1. Lottery tickets refer to
sub-networks which can, by their own, achieve good performance [ 11]. The idea of ensembling is
similar to bagging in machine learning where weak learners are aggregated to obtain strong learners,
although in neural networks such ensembling strategy is not designed manually but rather emergent
from training.
The paper is organized as follows: In Section 2, we review the approximation theory and show our
empirical results manifesting a scaling law ℓ∝N−1deviating from the approximation theory. To
understand the new scaling law, in Section 3 we reverse engineer neural networks to find out why.
Hinted by the observation of symmetric neurons and peaks in the loss histogram, we find the existence
of "lottery tickets". Then in Section 4 we observe a central limit theorem for lottery tickets, which
attributes the N−1scaling law to variance reduction as in central limit theorem. Finally in Section 5
we discuss our theory’s implications for large language models and statistical physics-like theories of
learning.
2 A New Scaling Law Not Explained by Approximation Theory
In this section, we first review the prediction of NSL by Sharma & Kaplan [ 9] using the approximation
theory. Then we show one simple example which demonstrates a neural scaling law deviating from
the approximation theory.
2.1 The Old Tale: Approximating Functions On Data Manifold
Sharma & Kaplan [ 9] predicts that, when data is abundant, the MSE loss achieved by well-trained
ReLU neural networks scales as N−αwith α= 4/d, where dis the intrinsic dimensionality
of data manifold. The basic idea is: A ReLU network with Nparameters is able to fit O(N)
data points accurately. If these data points are uniformly placed on the grid of a d-dimensional
hypercube (see Figure 1a), then there are O(N1/d)points along each dimension, with lattice constant
h=O(N−1/d). ReLU networks are piecewise linear functions, so the leading error term (according
to Taylor expansion) is second-order, i.e., h2. Considering the squared function in the definition of
MSE, we know the MSE loss scales as h4=O(N−4/d).
2.2 Experiments: Discovery of A New Scaling Law
Setup Let us consider a simple example. The network has only one hidden layer with Nneurons, with
a scalar input x∈[−2,2]and aiming to predict y=x2(Figure 1b, more examples in Appendix A).
The network is parametrized as
f(x) =NX
i=1viσ(wix+bi) +c, (1)
2

--- PAGE 3 ---
(a)
5
 4
 3
 2
 1
 0
log(loss)0255075100125150175200countlottery tickets (b)
 (c)
Figure 2: Evidence of lottery tickets. (a) For an extremely wide network N=10000, the distribution
of weights and biases in the first layer display an intriguing symmetry, i.e., there exist symmetric
neurons (w, b)and(−w, b). (b) We train a thousand N= 2networks independently and show the
histogram of their losses. The histogram display a few peaks, suggesting existence of a few different
local minima or "algorithms". We call the peak with lowest loss "lottery tickets". (c) We find lottery
tickets to have symmetric neurons, which guarantee that the network represents an even function.
where σis the activation function. When the activation function is ReLU, the approximation theory
predicts that the MSE loss scales as N−4since d= 1. When the activation function is SiLU
(σ(x) =x/(1 +e−x), a smoothed version of ReLU), no existing theory is able to predict the scaling
law. We are thus curious to run empirical experiments to see: (1) Is the N−4scaling law valid in our
case? (2) what is the scaling law for SiLU networks?
Empirical Results We train neural networks with various width N={10,20,30,40,50}for
LeakyReLU and SiLU activations, respectively. We train neural networks with the Adam optimizer
for 50000 steps; the initial learning rate is 10−2, reducing the learning rate by ×0.2every 10000
steps. Since neural networks can be sensitive to initializations, we train 1000 networks with different
random seeds and use the median loss value (histograms are shown in Figure 3). The MSE losses
versus width Nis shown in Figure 1c. For ReLU, we find that the loss starts off as N−4, agreeing
with the approximation theory, but then quickly transits to a much slower decay N−1. By contrast
for SiLU, the MSE loss decays slowly but consistently as N−1. The N−1scaling law is unexpected,
suggesting a new mechanism not discovered before. In the next section, we aim to understand this
new scaling law, by reverse engineering SiLU networks.
Remark: Optimization Notice that the SiLU function σ(x) =x/(1+e−x)expands to be a quadratic
function around its minimum σ(x)≈A(x−x∗)2+B1, so ideally one can carefully construct
parameters of a N= 1network to make it approximate the squared function arbitrarily well, which
requires the weights wandvto bew→0,v→+∞but maintains w2v= 1/Aconstant. In practice,
optimization cannot find such extreme solution, otherwise loss would become perfectly zero even for
N= 1.
3 Mechanistically Understanding Lottery Tickets
Existence of symmetric neurons in wide networks To understand what is happening inside a
wide network, we train an extremely wide network with N= 10000 SiLU neurons. We plot N
neurons (wi, bi)(weights and biases for the first layer) for i= 1,···, Nin Figure 2a. There are two
interesting patterns: (1) most neurons are concentrated around a bell-shaped curve, indicating an
attractor manifold; (2) the distribution is symmetric with respect to w: if there is a neuron at (w, b),
then there is a "symmetric" neuron around (−w, b). The existence of the attractor manifold may not
be too surprising, since low-loss solutions should live in a subspace smaller than the entire parameter
space. The existence of symmetric neurons, however, is somewhat unexpected and requires more
detailed study, as we carry out below.
Two-neuron networks We conjecture that, if symmetric neurons are universal enough, we should be
able to observe them even in a network with 2 hidden neurons. Hence we train SiLU networks with
only 2 hidden neurons to fit the squared function (see Figure 2c). Since such narrow networks are
strongly affected by initializations, we train 1000 networks with different random seeds. We show the
histogram of MSE losses of these trained networks in Figure 2b, finding that it contains many peaks,
1A≈0.109, x∗≈1.278, B=−0.278.
3

--- PAGE 4 ---
7
6
5
4
3
2
1
log(loss)050100150200250300350countwidth=1
7
6
5
4
3
2
1
log(loss)width=2
7
6
5
4
3
2
1
log(loss)width=3
7
6
5
4
3
2
1
log(loss)width=4
7
6
5
4
3
2
1
log(loss)width=10
7
6
5
4
3
2
1
log(loss)width=20
7
6
5
4
3
2
1
log(loss)width=30
7
6
5
4
3
2
1
log(loss)width=40
7
6
5
4
3
2
1
log(loss)width=50Figure 3: "Central limit theorem" of lottery tickets. For each width N, we train 1000 networks
independently and plot their loss histograms. For small N, the distribution is multi-modal, i.e., shows
more than one peaks; for large N, the distribution becomes more single-peaked.
suggesting the existence of bad local minima in loss landscapes. In particular, we are interested in
the peak with the lowest loss, which we call "lottery tickets", elaborated below. We also attempt to
understand other peaks in parameter space and in algorithmic space in Appendix B.
A randomly chosen lottery ticket has parameters (illustrated in Figure 2c):
(w1, w2, b1, b2, v1, v2, c)
=(−0.83010483 ,0.83010304 ,−1.16842330 ,−1.16842365 ,5.68583536 ,5.68586636 ,3.1539197)
(2)
where we observe w1≈ −w2,b1≈b2,v1≈v2, which correspond to symmetric neurons. The
benefits of symmetric neurons can be understood from Taylor expansion. Setting w1=−w2=w,
b1=b2=b,v1=v2=v, the network represents an even function
f(x) =vσ(wx+b) +vσ(−wx+b) +c, f (x) =f(−x), (3)
and Taylor expands as
f(x) = (2 σ(b)u+c) +uσ′′(b)w2x2+1
12uσ′′′′(b)w4x4+..., (4)
so a neural network can adjust its parameters to make sure 2σ(b)u+c= 0,uσ′′(b)w2= 1, leaving
the quartic term as the leading error term. The take-away is: utilizing symmetric neurons is very
effective at approximating the squared function, however, it relies on luck for networks to find such
strategy (justified the terminology "lottery tickets").
4 A Central Limit Theorem of Lottery Tickets
In the previous section, we show the loss histogram of width N= 2networks contains many peaks,
and define the peak with the lowest loss as "lottery tickets". How would the story change for wider
networks (i.e., larger N)?
Loss histograms For each width N= 1,2,3,4,10,20,30,40,50, we train 1000 networks indepen-
dently (with different random seeds) and plot their loss histograms in Figure 3. For narrow networks
(N= 1,2,3,4), the loss histogram contains many peaks, suggesting the existence of many bad local
minima, but on the other hand, suggesting the existence of "lottery tickets" which win over other local
minima. For wider networks ( N= 10,20,30,40,50), the loss histograms are single-peaked. This
reminds us of the central limit theorem of random variables: the average of arbitrary weird-shaped
distribution would converge to a Gaussian distribution as more and more random variables are
averaged. This suggests that maybe there are many lottery tickets inside a wide network and then
ensembled. Below we propose a theory for it.
Theory: Ensembling Lottery Tickets Leads to N−1Scaling For a wide nertwork, there exist
n > 1lottery tickets. The ithlottery ticket represents a function fi(x)≈f(x)whose error term
ei(x)≡fi(x)−f(x). We define function norm as |f|2=R∞
−∞f2(x)p(x)dx2, where p(x)≥0
is the distribution of x. Typically |ei(x)|2≪ |f|2. The network can utilize the last linear layer to
2In our toy model, xis drawn uniformly from [−1,1], sop(x) = 1 forx∈[−1,1], andp(x) = 0 otherwise.
|f|2=R1
−1f2(x)dx.
4

--- PAGE 5 ---
7
6
5
4
3
2
1
loss050100150200250300350400count"2""1"+"1"
1+1
2
7
6
5
4
3
2
1
loss"4""2"+"2"
2+2
4
7
6
5
4
3
2
1
loss"20""10"+"10"
10+10
20
7
6
5
4
3
2
1
loss"40""20"+"20"
20+20
40Figure 4: Do the benefits of large widths come from plain ensembling or more complicated synergy
of smaller subparts? In each plot " N" means a network with width N, "N/2"+"N/2" means two
networks with width N/2are ensembled. If these two loss histograms are different (e.g., N= 2,4),
this means more complicated synergy is in place beyond ensembling. If the two loss histograms
are similar (e.g., N= 20,40), this means the role of synergy is vanishing, and the benefit of larger
widths solely comes from ensembling.
ensemble these lottery tickets such that
fE(x) =nX
i=1aifi(x) = (nX
i=1ai)f(x) +nX
i=1aiei(x). (5)
We want to minimize:
|fE(x)−f(x)|2=(nX
i=1ai−1)f(x) +nX
i=1aiei(x)2
. (6)
If we assume f(x)to be orthogonal to ei(x), and ei(x)is orthogonal to ej(x)forj̸=i(in general
we don’t need orthogonality conditions3), then the above equation simplifies to
|fE(x)−f(x)|2= (nX
i=1ai−1)2|f|2+nX
i=1a2
i|ei|2(7)
Since|f|2≫ |ei|2, we wantPN
i=1ai= 1such that the coefficient before |f|2becomes zero. The
second term can be lower bounded by (using Cauchy-Schwarz inequality):
(nX
i=1a2
i|ei|2)(nX
i=11
|ei|2)≥(nX
i=1ai)2= 1 (8)
The equality holds when ai|ei|2=Cfor all i, meaning that for a better/worse lottery ticket
(small/large |ei|2), a larger/smaller weight aiis applied. Finally we arrive that
|eE|2≡ |fE(x)−f(x)|2≥(nX
i=11
|ei|2)−1(9)
If we further assume nlottery tickets to be of equal quality, i.e., |ei|2=|e|2, then|eE|2=|e|2/n,
meaning that the MSE loss decays as n−1. A width Nnetwork on average has n∝Nlottery tickets,
so the n−1scaling can further translate to N−1.
The benefits of large widths from ensembling or synergy Ensembling is only one benefit of
wider models. In some sense, ensembling is trivial because you can train two half-sized models
independently and then ensemble them. Are there benefits of large widths beyond ensembling, i.e.,
two sub-networks might synergize in a smart way that smaller networks cannot simulate? To see
the benefits of synergy, we need to first subtract the effect of ensembling. The loss of networks
3If⟨ei, ej⟩ ̸= 0, i.e., they are not orthogonal, we can always redefine e′
j≡ej−⟨ej,ei⟩
⟨ei,ei⟩eisuch that
⟨ei, e′
j⟩= 0.In summary, we can always make lottery tickets to be orthogonal in the proper bases. However, k
lottery tickets may actually have only n < k bases. In fact, lottery tickets are highly correlated (as we show in
Appendix C), so here nshould be understood as the number of independent/orthogonal lottery tickets.
5

--- PAGE 6 ---
N/2andN, obeys distributions LN/2∼pN/2(ℓ)andLN∼pN(ℓ), respectively. Ensembling
twoN/2networks, according to our theory above, would give loss ˜LN= (1/LN/2+ 1/LN/2)−1,
obeying the distribution ˜pN(ℓ). We then compare the difference between ˜pN(ℓ)andpN(ℓ); larger
(smaller) difference means more (less) synergy between two N/2networks. In practice, we obtain
the histogram of ˜pN(l)by randomly drawing two losses from the histogram of ˜pN/2(l)and compute
the harmonic mean (divided by 2).
Our empirical results are shown in Figure 4: narrower networks have stronger synergy, while wider
networks are almost ensembling with nearly no synergy. For example, an N= 2network is much
better than ensembling of two N/2 = 1 networks ( 2≫1 + 1 ); anN= 4network is (slightly) better
than the ensembling of two N/2 = 2 networks ( 4>2 + 2 ); anN= 20 orN= 40 network does as
well as ensembling of two half-width networks ( 20≈10 + 10 ,40≈20 + 20 ). Although in Section
3, we only analyzed N= 2 lottery tickets, N= 4 lottery tickets are even better, i.e., one cannot
construct an N= 4lottery ticket from simply ensembling two N= 2lottery tickets. This may imply
that wider networks may not only provide more lottery tickets, but also provide better lottery tickets
when synergy is present.
Implications for large language models Lottery ticket ensembling would expect the loss to be
ℓ∝w−1where wis the width of the network. For large language models, people use Nto represent
the number of parameters, which is roughly (width)2×depth . Since the depth-width ratio [ 12] is
usually fixed when scaling language models, we have w∝N1/3(Nis the number of parameters)
hence ℓ∝N−1/3. Interestingly, Hoffmann et al. [7] reports that4:
ℓ(N, D ) =E+A
N0.34+B
D0.28, (10)
whose model size scaling exponent 0.34is quite close to our prediction 1/3. We would like to
investigate in the future whether this really supports our theory or it is a pure coincidence.
5 Related works and Discussions
Neural Scaling laws (NSL) has been widely observed in deep learning [ 1–8]. Theories are proposed
to explain NSL, including approximation theory based on intrinsic input dimension [ 9] or maximum
arity [ 10], quanta theory based on subtask decomposition [ 8]. Our work prosposes another NSL
mechanism namely lottery ticket mechanism. It is still unclear how to disentangle these mechanisms,
and whether there is a dominant mechanism in deep networks.
Emergence refers to the phenomenon where a model has abrupt improvements in performance
when its size is scaled up [ 13], although this can depend on specific metrics [ 14]. Our discussion on
ensembling versus synergy may further provide a useful tool to categorize types of emergence: trivial
(pure ensembling) or non-trivial (with synergy).
Ensembling is not new in machine learning. However, ensembling methods are usually manually
designed [ 15]. This work shows that ensembling can be emergent from network training. A very
related concept is redundancy, which has been shown both for vision tasks [ 16], language tasks [ 17],
and even simple math datasets [18, 19].
Mean field theory suggests that infinitely wide neural networks approach kernel machines, and
finite networks with finite width Ndeviates from the limiting kernel by order 1/N[20–22], which
agrees with our lottery ticket central limit theorem. This is a pleasant agreement: our analysis starts
from narrow networks (lottery tickets) and extends to wide networks, while mean field theory starts
from infinitely wide networks and extends to (finitely) wide networks. Unifying two theories would
provide clearer mental pictures, left for future work.
Limitations The derivation of the N−1scaling requires the assumption that lottery tickets are
unbiased. If all lottery tickets share a non-zero bias, then the loss would plateau to a non-zero value
due to the bias term. We indeed observe this phenomenon for some cases in Appendix A. The
analysis in this paper is mainly based on a toy example; how general our claims are for other tasks
and architectures need more study in the future.
4Dis the dataset size. In our case D→ ∞ so we can ignore the loss dependence on D.
6

--- PAGE 7 ---
Acknowledgement
We would like to thank Eric Michaud, Isaac Liao and Zechen Zhang for helpful discussions. ZL and
MT are supported by IAIFI through NSF grant PHY-2019786, the Foundational Questions Institute
and the Rothberg Family Fund for Cognitive Science.
References
[1]Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan
Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is
predictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.
[2]Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive
prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673 , 2019.
[3]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361 , 2020.
[4]Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,
Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive
generative modeling. arXiv preprint arXiv:2010.14701 , 2020.
[5]Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural
machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing , pages 5915–5922, 2021.
[6]Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-
ers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 12104–12113, 2022.
[7]Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.
[8]Eric J Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural
scaling. arXiv preprint arXiv:2303.13506 , 2023.
[9]Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
arXiv preprint arXiv:2004.10802 , 2020.
[10] Eric J Michaud, Ziming Liu, and Max Tegmark. Precision machine learning. Entropy , 25(1):
175, 2023.
[11] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. In International Conference on Learning Representations , 2019. URL
https://openreview.net/forum?id=rJl-b3RcF7 .
[12] Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. The depth-to-width
interplay in self-attention. arXiv preprint arXiv:2006.12467 , 2020.
[13] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
language models. arXiv preprint arXiv:2206.07682 , 2022.
[14] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language
models a mirage? arXiv preprint arXiv:2304.15004 , 2023.
[15] Ekaterina Lobacheva, Nadezhda Chirkova, Maxim Kodryan, and Dmitry P Vetrov. On power
laws in deep ensembles. Advances In Neural Information Processing Systems , 33:2375–2385,
2020.
[16] Diego Doimo, Aldo Glielmo, Sebastian Goldt, and Alessandro Laio. Redundant representations
help generalization in wide neural networks. arXiv preprint arXiv:2106.03485 , 2021.
7

--- PAGE 8 ---
[17] Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. The hydra
effect: Emergent self-repair in language model computations. arXiv preprint arXiv:2307.15771 ,
2023.
[18] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two
stories in mechanistic explanation of neural networks. arXiv preprint arXiv:2306.17844 , 2023.
[19] Ziming Liu, Eric Gan, and Max Tegmark. Seeing is believing: Brain-inspired modular training
for mechanistic interpretability. arXiv preprint arXiv:2305.08746 , 2023.
[20] James Halverson, Anindita Maiti, and Keegan Stoner. Neural networks and quantum field
theory. Machine Learning: Science and Technology , 2(3):035002, 2021.
[21] Daniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory .
Cambridge University Press Cambridge, MA, USA, 2022.
[22] Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time
convergence and asymptotic error scaling of neural networks. Advances in neural information
processing systems , 31, 2018.
8

--- PAGE 9 ---
Appendix
A More examples
Three key observations from the main paper for the y=x2example are: (1) The existence of lottery
tickets for narrow networks (the loss histogram contains many peaks); (2) the loss histogram for wider
networks has only one peak (central limit theorem); (3) the loss decays as N−1for wide networks
(Nis the model width).
In this section, we would like to test these three claims on a few more examples: unary functions
and multi-digit multiplication. Both (1) and (2) holds generally true for almost all examples, but (3)
sometimes break down.
Unary functions The experimental setups are exactly the same as in the main paper, with the only
changes to target function f(x) =x3, x4,sin(x),exp(x),sin2(x),exp(x2),relu(x),tanh( x). For
each task, we run 1000 networks (with different random seeds, i.e., different initializations) and plot
the histogram of their final losses in Figure 5. For narrow networks, the loss histogram always shows
multiple peaks, implying the existence of lottery tickets (networks with lowest losses). As width
grows larger, the loss histogram becomes single-peaked. We also have a few interesting observations
(which we haven’t been able to explain yet): (1) some examples are more asymmetric than others, e.g,
cos(x)is very heavy-tailed towards the left. (2) for some examples the losse are more concentrated
than other, e.g., sin(x)is extremly concentrated looking like a delta function. We compute the median
value of 1000 losses for each width and plot the median loss as a function of width Nin Figure 7.
Some examples obey the N−1law quite well: x2,x3,tanh( x)(early stage); for some examples
the losses are still decaying but seem to plateau, e.g., x4,sin2x,exp(x2); other examples already
plateaued or even slightly increase. The two later scenarios are possibly due to the bias term of lottery
tickets, or optimization issues for too wide networks.
Multi-digit multiplication We extend our input from one dimension to multiple dimen-
sions. We do this multi-digit multiplication tasks with 2, 3, 4 or 5 digits, i.e., f=
x1x2, x1x2x3, x1x2x3, x1x2x3x4, x1x2x3x4x5. For each task, we run 1000 networks with dif-
ferent random seeds and plot their loss histograms in Figure 6. For narrow networks, the 2-digit case
shows multiple peaks early on (e.g., width=2,3,4), but the 3-digit or 4-digit case show multiple peaks
only for width=10, and 5-digit case do not show any multiple peak behavior in the ranges shown. We
conjecture that this is because networks with only one hidden layer are very inefficient at multiplying
numbers, especially for more numbers to be multiplied, and consequently, there is no (clear) lottery
ticket for the ranges shown (up to 50). Regarding neural scaling laws, we plot the median loss as
a function of Nin Figure 8. For all cases (2,3,4,5 digits), at least some range of the loss curves
agree with the N−1scaling law, where the lottery ticket ensembling mechanism is dominant. Before
the range the loss drops faster because better lottery tickets are formed (synergy is in place); after
the range the loss drops more slowly, plateaus or even increases due to biases of lottery tickets and
optimization issues.
B Algorithm discovery by parameter space clustering
In Figure 2b, we showed that the loss histogram contains multiple peaks. If we only care about peaks
below loss 10−2, then there are 4 peaks. What do these peaks mean in terms of model parameter
space and algorithmic space? We will investigate these questions below.
Parameter space Remember that we trained 1000 width 2 neural networks. Each network contains 7
parameters, so it can be viewed as a point in the parameter space R7. We apply principal component
analysis (PCA) to the 1000 networks in the parameter space, and show them on the first two principal
components in Figure 9. Clearly there are 6 clusters, which are symmetric with respect to the first
principal component. We verify that the symmetry comes form the permutation symmetry of two
neurons (swapping two neurons does not alter the function represented by the network). Modulo
the permutation symmetry, there are 4 distinct clusters. On the low level, the fact of more than one
clusters implies the existence of local minima; on the high level, presumbly each cluster corresponds
to an algorithm, which we elaborate below.
9

--- PAGE 10 ---
log(loss)0100200300countx2width=1
log(loss)width=2
log(loss)width=3
log(loss)width=4
log(loss)width=10
log(loss)width=20
log(loss)width=30
log(loss)width=40
log(loss)width=50
log(loss)0100200300countx3
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300countx4
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300countsin(x)
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300countcos(x)
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300countexp(x)
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300countsin2(x)
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300countexp(x2)
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300counttanh(x)
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
7
6
5
4
3
2
1
log(loss)0100200300countrelu(x)
7
6
5
4
3
2
1
log(loss)7
6
5
4
3
2
1
log(loss)7
6
5
4
3
2
1
log(loss)7
6
5
4
3
2
1
log(loss)7
6
5
4
3
2
1
log(loss)7
6
5
4
3
2
1
log(loss)7
6
5
4
3
2
1
log(loss)7
6
5
4
3
2
1
log(loss)Figure 5: NN loss histograms for unary functions.
log(loss)0100200300count2 digitswidth=1
log(loss)width=2
log(loss)width=3
log(loss)width=4
log(loss)width=10
log(loss)width=20
log(loss)width=30
log(loss)width=40
log(loss)width=50
log(loss)0100200300count3 digits
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
log(loss)0100200300count4 digits
log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss) log(loss)
7
6
5
4
3
2
1
01
log(loss)0100200300count5 digits
7
6
5
4
3
2
1
01
log(loss)7
6
5
4
3
2
1
01
log(loss)7
6
5
4
3
2
1
01
log(loss)7
6
5
4
3
2
1
01
log(loss)7
6
5
4
3
2
1
01
log(loss)7
6
5
4
3
2
1
01
log(loss)7
6
5
4
3
2
1
01
log(loss)7
6
5
4
3
2
1
01
log(loss)
Figure 6: NN loss histograms for multiple-digit multiplication.
10

--- PAGE 11 ---
101 2×1013×1014×101107
106
105
104
103
x2
x3
x4
sin(x)
cos(x)
exp(x)
sin2(x)
exp(x2)
tanh(x)
relu(x)
N1
Figure 7: Neural scaling laws for unary functions.
101102107
105
103
101
2 digit
3 digit
4 digit
5 digit
N1
Figure 8: Neural scaling laws for multiple-digit multiplication.
Algorithmic space One thing we already noted is that: Algorithm 1 and 3 lie on the symmetric axis,
meaning they are symmetric algorithms (two neurons play symmetric roles), while algorithm 2 and 4
are asymmetric.
For networks as small as only having two neurons, what does it even mean that they represent
algorithms? We study how pre-activations and post-activations behave for both silu neurons in
the range of samples. We show the post-activations as functions of inputs xin Figure 10, and the
post-activations as a function of pre-activations in Figure 11. It is clear from both plots that Algorithm
1 and 3 are symmetric, while Algorithm 1 and 4 are asymmetric. In particular from Figure 11, four
algorithms leverages different parts of the silu networks. Algorithm 1: both neurons are leveraging
the intermediate nonlinear part (similar to a quadratic function there). Algorithm 2: one neuron
leverages the nagtive saturating part, while the other neuron leverages the intermediate nonlinear
part. Algorithm 3: both neurons leverage the quasi-linear part at the boundary between nonlinear and
linear parts. Algorithm 4: one neuron is activated at the linear section, while the other neuron stays at
the boundary between linear and nonlinear segments.
C Disentangling lottery tickets and their correlations
In the main paper, we defined lottery tickets in narrow networks, and statistically studied lottery
tickets in wider networks. Is it possible to literally disentangle a wide network into many lottery
tickets? In general this is difficult since it is unclear what a lottery ticket means for a general task.
However, for the toy case f(x) =x2, we find that a lottery ticket consists of two symmetric neurons.
Symmetric neurons refer to a pair of neurons whose incoming weights and biases are (w, b)and
(−w, b). This is a criterion we can use to find and disentangle lottery tickets in wide networks:
we want to partition Nneurons into N/2groups (each group has size 2), such that neurons in
the same group are approximately symmetric neurons. In Figure 12 first column, we see that the
neurons are symmetrically distributed, hence it makes sense to pair them up. After pairing up neurons
into lottey tickets, we can evaluate the quality of each lottery ticket by measuring its correlation
coefficient R2with the target function f(x) =x2(Figure 12 second column). Almost all lottery
tickets have very high quality for N= 10,30,50, i.e., R2≈1. However, the N= 1000 network
only has approximately half of high-quality lottery tickets, while the other half are completely
useless. Although there are many high-quality lottery tickets, they are unfortunately highly correlated
(Figure 12 third column). For example, N= 10 seems to only have 1 independent lottery ticket,
N= 30 has 2 independent lottery tickets, and N= 50 has 3 independent lottery tickets. For
11

--- PAGE 12 ---
20
 10
 0 10 20
PC17.5
5.0
2.5
0.02.55.07.510.012.5PC2Alg 1 (lottery ticket)
Alg 2Alg 2Alg 3
Alg 4 Alg 4Networks in weight space
105
104
103
lossFigure 9: We train 1000 networks with only 2 hidden neurons to fit f(x) =x2. Each network has
7 parameters hence is a point in R7. Applying principal component analysis (PCA) to the 1000
points, showing the first two PCs, and clusters are reavealed. Each culster corresponds to a distinct
"algorithm".
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.3
0.2
0.1
0.00.10.20.3activationAlgorithm 1 (Lottery Ticket)
neuron 1
neuron 2
(a)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.3
0.2
0.1
0.00.10.20.3activationAlgorithm 2
neuron 1
neuron 2 (b)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.2
0.00.20.40.6activationAlgorithm 3
neuron 1
neuron 2 (c)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.00.51.01.52.02.53.03.5activationAlgorithm 4
neuron 1
neuron 2 (d)
Figure 10: Activations of both neurons as functions of inputs, for four algorithms.
each lottery ticket, we can compute its error function ei(x)as the difference between the best
linear predictor and the target function (Figure 12 fourth column). Error functions seem to be
highly correlated, e.g., |ei(x)|tends to be small around x= 0,±1.6and large elsewhere. The high
correlation among lottery tickets may imply that these networks are unnecessarily redundant. We
would like to investigate in the future whether we can use this insights from lottery tickets to reduce
these redundancy for network pruning. A preliminary study is carried out below in Appendix D.
D Distilling a narrow network from a wide network
The techniques in Appendix C, i.e., disentangling a wide network into lottery tickets, allows us to
distill a much smaller network from a wide network. Concretely, we take a width N= 50 networks
(25 lottery tickets) and select out the best lottery ticket (the one with highest R2with targets). Now we
use this lottery ticket to construct a width 2 network: weights and biases of the first layer are obtained
by directly copying the lottery ticket, while the weights and biases of the second layer are obtained
via linear regression. We call them distilled networks. We do this distillation for 1000 random seeds,
and plot their losses in Figure 13 (blue). We also compare to non-distilled width 2 networks (orange).
On average, the distilled width 2 networks have similar or even worse performance than non-distilled
ones, however, there are distilled networks which are extremely good, which achieve losses smaller
than machine precision <10−16. These distilled network are indeed "lottery tickets of lottery tickets".
However, it seems impossible to fine-tune these lottery tickets further, since they have extremely
ill-conditioned weights. For the distilled network with lowest loss ( 1.6×10−19), its parameters are:
w= (−5.9×10−8,1.2×10−5),b= (−2.8×10−7,7.6×10−5),v= (5.5×1012,2.6×1010), c=−2.2×105.
(11)
12

--- PAGE 13 ---
10
 5
 0 5
pre-activation012345post-activationneuron 1
10
 5
 0 5
pre-activation012345post-activationneuron 2Algorithm 1 (Lottery Ticket)(a)
10
 5
 0 5
pre-activation012345post-activationneuron 1
10
 5
 0 5
pre-activation012345post-activationneuron 2Algorithm 2 (b)
10
 5
 0 5
pre-activation012345post-activationneuron 1
10
 5
 0 5
pre-activation012345post-activationneuron 2Algorithm 3 (c)
10
 5
 0 5
pre-activation012345post-activationneuron 1
10
 5
 0 5
pre-activation012345post-activationneuron 2Algorithm 4 (d)
Figure 11: Post-activations as functions of pre-activations for both neurons in four algorithms.
1.5
 1.0
 0.5
 0.0 0.5 1.0
wi2.5
2.0
1.5
1.0
0.5
0.00.5Width=10
bi
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
lottery ticket rank0.00.20.40.60.81.0R2
0 1 2 3 40
1
2
3
4Correlation Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.5
0.4
0.3
0.2
0.1
0.00.10.2ei(x)error curves ei(x)
(a)
1.5
 1.0
 0.5
 0.0 0.5 1.0
wi3.0
2.5
2.0
1.5
1.0
0.5
0.00.51.0Width=30
bi
0 2 4 6 8 10 12 14
lottery ticket rank0.00.20.40.60.81.0R2
0 2 4 6 8 10 12 140
2
4
6
8
10
12
14Correlation Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x2.5
2.0
1.5
1.0
0.5
0.00.51.01.5ei(x)error curves ei(x)
(b)
1.0
 0.5
 0.0 0.5 1.0
wi3.0
2.5
2.0
1.5
1.0
0.5
0.00.5Width=50
bi
0 5 10 15 20 25
lottery ticket rank0.00.20.40.60.81.0R2
0 5 10 15 200
5
10
15
20Correlation Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.6
0.4
0.2
0.00.2ei(x)error curves ei(x)
(c)
1.0
 0.5
 0.0 0.5 1.0
wi1.5
1.0
0.5
0.00.51.0Width=1000
bi
0 100 200 300 400 500
lottery ticket rank0.00.20.40.60.81.0R2
0 100 200 300 4000
100
200
300
400Correlation Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x3
2
1
01ei(x)error curves ei(x)
(d)
Figure 12: Disentangling lottery tickets and measureing their correlation. For top to bottom: width =
10, 30, 50, 1000. For left to right - The first left: weights and biases for the first layer. The second left:
R2(coefficients of determination) of lottery tickets and the target function f(x) =x2. The second
right: correlation matrices for lottery tickets, measured by inner products of error functions. The first
right: error functions for lottery tickets.
13

--- PAGE 14 ---
20
 15
 10
 5
 0
log(loss)050100150200250300countDistill width 2 network from width 50
Width 2 networkFigure 13: We distill winning lottery tickets (width 2) from width 50 networks. The loss histogram of
the distilled width 2 networks is shown in blue, compared to the loss histogram of width 2 networks
in orange. On average, the distilled width 2 networks have similar or even worse performance than
non-distilled ones, however, there are distilled networks which are extremely good, which achieve
losses smaller than machine precision <10−16.
14
