# 2310.02258.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ensemble/2310.02258.pdf
# Kích thước tệp: 1520475 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Định luật Tỷ lệ Neural từ Việc Tổ hợp Vé số May mắn
Ziming Liu
MIT & IAIFI
zmliu@mit.eduMax Tegmark
MIT & IAIFI
tegmark@mit.edu
Tóm tắt
Ghi chú của tác giả: lý thuyết trong bài báo này
còn đáng nghi ngờ; chúng tôi đang cố gắng hết sức
để sửa chữa nó. Kết quả thực nghiệm vẫn đúng.
Các định luật tỷ lệ neural (NSL) đề cập đến hiện tượng mà hiệu suất mô hình
cải thiện theo quy mô. Sharma & Kaplan đã phân tích NSL bằng lý thuyết xấp xỉ
và dự đoán rằng tổn thất MSE giảm theo N−α,α= 4/d, trong đó N là số
tham số mô hình, và d là chiều đầu vào nội tại. Mặc dù lý thuyết của họ
hoạt động tốt cho một số trường hợp (ví dụ, mạng ReLU), chúng tôi ngạc nhiên thấy rằng một
bài toán 1D đơn giản y=x2 biểu hiện một định luật tỷ lệ khác (α= 1) so với
dự đoán của họ (α= 4). Chúng tôi mở các mạng neural và phát hiện ra rằng định luật tỷ lệ
mới bắt nguồn từ việc tổ hợp vé số may mắn: một mạng rộng hơn trung bình có
nhiều "vé số may mắn" hơn, được tổ hợp để giảm phương sai của đầu ra. Chúng
tôi hỗ trợ cơ chế tổ hợp bằng cách giải thích có cơ chế các mạng neural đơn
lẻ, cũng như nghiên cứu chúng theo thống kê. Chúng tôi quy định luật tỷ lệ N−1
cho "định lý giới hạn trung tâm" của các vé số may mắn. Cuối cùng, chúng tôi thảo luận
các ý nghĩa tiềm năng của nó đối với các mô hình ngôn ngữ lớn và các lý thuyết giống vật lý thống kê
về học tập.
1 Giới thiệu
Các định luật tỷ lệ neural (NSL), hiện tượng mà hiệu suất mô hình cải thiện khi kích thước mô hình
tăng lên, đã được quan sát rộng rãi trong học sâu [1–8]. Thông thường các tổn thất tuân theo ℓ∝N−α
trong đó N là số tham số mô hình, và α > 0 là số mũ tỷ lệ. Hiểu các cơ chế của các định luật tỷ lệ neural
quan trọng cả về mặt lý thuyết và thực nghiệm. Hiện tại, hai lý thuyết chính về các định luật tỷ lệ neural được đề xuất: một là lý thuyết xấp xỉ [3,9,10],
cho rằng định luật tỷ lệ đến từ hồi quy trên một đa tạp dữ liệu. Đặc biệt, số mũ tỷ lệ là α= 4/d trong đó d là chiều đầu vào nội tại [3,9] hoặc độ lớn tối đa [10]. Lý thuyết khác
là lý thuyết lượng tử [8], cho rằng định luật tỷ lệ đến từ hệ thống phân cấp của các nhiệm vụ con,
và số mũ tỷ lệ α phụ thuộc vào tính đuôi nặng của phân phối nhiệm vụ con.
Bài báo này nhằm tiết lộ một cơ chế khác của định luật tỷ lệ neural từ việc tổ hợp vé số may mắn.
Lý thuyết của chúng tôi được thúc đẩy bởi các quan sát thực nghiệm trong một thiết lập cực kỳ đơn giản, tức là huấn luyện các mạng hai lớp (ReLU hoặc SiLU) với trình tối ưu hóa Adam để khớp với hàm bình phương y=x2. Lý thuyết xấp xỉ sẽ dự đoán rằng α= 4 cho các mạng ReLU, nhưng không thể dự đoán α cho các mạng SiLU. Lý thuyết lượng tử cũng không áp dụng được cho các trường hợp này. Kết quả thực nghiệm của chúng tôi khá thú vị (được hiển thị trong Hình 1c): Đối với các mạng ReLU, đường cong tổn thất giảm theo N−4 ban đầu,
nhưng sớm chậm lại thành N−1 khi N tăng. Đối với các mạng SiLU, đường cong tổn thất hoạt động như N−1
một cách nhất quán.
Bây giờ đến câu hỏi: điều gì tạo ra định luật tỷ lệ N−1 này? Bằng cách kỹ thuật đảo ngược các mạng đơn lẻ và thực hiện phân tích thống kê trên một tập hợp các mạng, chúng tôi quy định luật tỷ lệ neural
Bản thảo. Đang được xem xét.arXiv:2310.02258v2 [cs.LG] 2 Feb 2024

--- TRANG 2 ---
(a)
 (b)
10 20 30 4050
N (độ rộng)107
106
105
Tổn thất MSE
N4
N1
LeakyReLU
SiLU(c)
Hình 1: (a) Định luật tỷ lệ ℓ∝N−4/d từ Sharma và Kaplan [9] có thể được hiểu từ
việc xấp xỉ một hàm d-chiều trong khi các điểm dữ liệu nằm đồng đều bên trong một siêu khối. (b) Thiết lập đơn giản của chúng tôi là huấn luyện một mạng SiLU hoặc (Leaky)ReLU hai lớp (một lớp ẩn với độ rộng N)
để khớp với hàm bình phương y=x2. (c) Một tỷ lệ N−1 đáng ngạc nhiên xuất hiện cho các mạng SiLU và ở
đuôi của các mạng ReLU, trong khi dự đoán N−4 của [9] chỉ xuất hiện ở giai đoạn đầu của ReLU.
mới cho việc tổ hợp vé số may mắn: một mạng với độ rộng N chứa n "vé số may mắn" (n∝∼N),
việc tổ hợp chúng có thể giảm phương sai của đầu ra theo n−1∝∼N−1. Vé số may mắn đề cập đến
các mạng con có thể, tự chúng, đạt được hiệu suất tốt [11]. Ý tưởng về tổ hợp tương tự như bagging trong học máy nơi các học viên yếu được tổ hợp để có được các học viên mạnh,
mặc dù trong các mạng neural chiến lược tổ hợp như vậy không được thiết kế thủ công mà thay vào đó xuất hiện
từ quá trình huấn luyện.
Bài báo được tổ chức như sau: Trong Phần 2, chúng tôi xem xét lý thuyết xấp xỉ và hiển thị kết quả thực nghiệm
của chúng tôi thể hiện một định luật tỷ lệ ℓ∝N−1 lệch khỏi lý thuyết xấp xỉ. Để hiểu định luật tỷ lệ mới, trong Phần 3 chúng tôi kỹ thuật đảo ngược các mạng neural để tìm hiểu tại sao.
Được gợi ý bởi quan sát về các tế bào thần kinh đối xứng và các đỉnh trong biểu đồ tổn thất, chúng tôi tìm thấy sự tồn tại
của "vé số may mắn". Sau đó trong Phần 4 chúng tôi quan sát một định lý giới hạn trung tâm cho các vé số may mắn, cái mà
quy định luật tỷ lệ N−1 cho việc giảm phương sai như trong định lý giới hạn trung tâm. Cuối cùng trong Phần 5
chúng tôi thảo luận về ý nghĩa của lý thuyết của chúng tôi đối với các mô hình ngôn ngữ lớn và các lý thuyết giống vật lý thống kê
về học tập.
2 Một Định luật Tỷ lệ Mới Không Được Giải thích bởi Lý thuyết Xấp xỉ
Trong phần này, trước tiên chúng tôi xem xét dự đoán của NSL bởi Sharma & Kaplan [9] sử dụng lý thuyết xấp xỉ. Sau đó chúng tôi hiển thị một ví dụ đơn giản thể hiện một định luật tỷ lệ neural lệch khỏi
lý thuyết xấp xỉ.
2.1 Câu chuyện Cũ: Xấp xỉ Các Hàm Trên Đa tạp Dữ liệu
Sharma & Kaplan [9] dự đoán rằng, khi dữ liệu dồi dào, tổn thất MSE đạt được bởi các mạng neural ReLU được huấn luyện tốt tỷ lệ theo N−α với α= 4/d, trong đó d là chiều nội tại
của đa tạp dữ liệu. Ý tưởng cơ bản là: Một mạng ReLU với N tham số có thể khớp với O(N)
điểm dữ liệu một cách chính xác. Nếu các điểm dữ liệu này được đặt đồng đều trên lưới của một siêu khối d-chiều (xem Hình 1a), thì có O(N1/d) điểm dọc theo mỗi chiều, với hằng số lưới
h=O(N−1/d). Các mạng ReLU là các hàm tuyến tính từng phần, vì vậy số hạng lỗi hàng đầu (theo
khai triển Taylor) là bậc hai, tức là h2. Xem xét hàm bình phương trong định nghĩa của
MSE, chúng ta biết tổn thất MSE tỷ lệ theo h4=O(N−4/d).
2.2 Thí nghiệm: Khám phá Một Định luật Tỷ lệ Mới
Thiết lập Hãy xem xét một ví dụ đơn giản. Mạng chỉ có một lớp ẩn với N tế bào thần kinh, với
một đầu vào vô hướng x∈[−2,2] và nhằm dự đoán y=x2 (Hình 1b, thêm ví dụ trong Phụ lục A).
Mạng được tham số hóa như
f(x) =NX
i=1viσ(wix+bi) +c, (1)
2

--- TRANG 3 ---
(a)
5
 4
 3
 2
 1
 0
log(tổn thất)0255075100125150175200số lượngvé số may mắn (b)
 (c)
Hình 2: Bằng chứng về vé số may mắn. (a) Đối với một mạng cực rộng N=10000, phân phối
của trọng số và độ lệch trong lớp đầu tiên hiển thị một tính đối xứng thú vị, tức là tồn tại các tế bào thần kinh đối xứng (w, b) và (-w, b). (b) Chúng tôi huấn luyện một nghìn mạng N= 2 độc lập và hiển thị
biểu đồ của tổn thất của chúng. Biểu đồ hiển thị một vài đỉnh, gợi ý sự tồn tại của một vài
cực tiểu địa phương hoặc "thuật toán" khác nhau. Chúng tôi gọi đỉnh có tổn thất thấp nhất là "vé số may mắn". (c) Chúng tôi thấy các vé số may mắn có các tế bào thần kinh đối xứng, điều này đảm bảo rằng mạng biểu diễn một hàm chẵn.
trong đó σ là hàm kích hoạt. Khi hàm kích hoạt là ReLU, lý thuyết xấp xỉ
dự đoán rằng tổn thất MSE tỷ lệ theo N−4 vì d= 1. Khi hàm kích hoạt là SiLU
(σ(x) =x/(1 +e−x), một phiên bản mượt mà của ReLU), không có lý thuyết hiện tại nào có thể dự đoán định luật tỷ lệ. Do đó chúng tôi tò mò chạy các thí nghiệm thực nghiệm để xem: (1) Định luật tỷ lệ N−4 có hợp lệ trong trường hợp của chúng tôi không? (2) định luật tỷ lệ cho các mạng SiLU là gì?
Kết quả Thực nghiệm Chúng tôi huấn luyện các mạng neural với độ rộng khác nhau N={10,20,30,40,50} cho
các kích hoạt LeakyReLU và SiLU, tương ứng. Chúng tôi huấn luyện các mạng neural với trình tối ưu hóa Adam
trong 50000 bước; tốc độ học ban đầu là 10−2, giảm tốc độ học ×0.2 mỗi 10000
bước. Vì các mạng neural có thể nhạy cảm với việc khởi tạo, chúng tôi huấn luyện 1000 mạng với các
hạt giống ngẫu nhiên khác nhau và sử dụng giá trị tổn thất trung vị (biểu đồ được hiển thị trong Hình 3). Tổn thất MSE
so với độ rộng N được hiển thị trong Hình 1c. Đối với ReLU, chúng tôi thấy rằng tổn thất bắt đầu như N−4, đồng ý
với lý thuyết xấp xỉ, nhưng sau đó nhanh chóng chuyển sang một sự giảm chậm hơn nhiều N−1. Ngược lại
đối với SiLU, tổn thất MSE giảm chậm nhưng nhất quán theo N−1. Định luật tỷ lệ N−1 là bất ngờ,
gợi ý một cơ chế mới chưa được khám phá trước đây. Trong phần tiếp theo, chúng tôi nhằm hiểu định luật tỷ lệ mới này, bằng cách kỹ thuật đảo ngược các mạng SiLU.
Nhận xét: Tối ưu hóa Lưu ý rằng hàm SiLU σ(x) =x/(1+e−x) mở rộng thành một hàm bậc hai
xung quanh cực tiểu của nó σ(x)≈A(x−x∗)2+B1, vì vậy lý tưởng có thể cẩn thận xây dựng
các tham số của một mạng N= 1 để làm cho nó xấp xỉ hàm bình phương một cách tùy ý tốt, điều này
yêu cầu các trọng số w và v là w→0,v→+∞ nhưng duy trì w2v= 1/A hằng số. Trong thực tế,
việc tối ưu hóa không thể tìm thấy giải pháp cực đoan như vậy, nếu không tổn thất sẽ trở thành hoàn toàn không thậm chí cho
N= 1.
3 Hiểu Có Cơ chế về Vé số May mắn
Sự tồn tại của các tế bào thần kinh đối xứng trong các mạng rộng Để hiểu điều gì đang xảy ra bên trong một
mạng rộng, chúng tôi huấn luyện một mạng cực rộng với N= 10000 tế bào thần kinh SiLU. Chúng tôi vẽ N
tế bào thần kinh (wi, bi) (trọng số và độ lệch cho lớp đầu tiên) cho i= 1,···, N trong Hình 2a. Có hai
mẫu thú vị: (1) hầu hết các tế bào thần kinh tập trung xung quanh một đường cong hình chuông, chỉ ra một
đa tạp hút; (2) phân phối đối xứng với respect đến w: nếu có một tế bào thần kinh tại (w, b),
thì có một tế bào thần kinh "đối xứng" xung quanh (−w, b). Sự tồn tại của đa tạp hút có thể không
quá ngạc nhiên, vì các giải pháp tổn thất thấp nên sống trong một không gian con nhỏ hơn toàn bộ không gian tham số. Sự tồn tại của các tế bào thần kinh đối xứng, tuy nhiên, có phần bất ngờ và yêu cầu nghiên cứu chi tiết hơn, như chúng tôi thực hiện dưới đây.
Các mạng hai tế bào thần kinh Chúng tôi đoán rằng, nếu các tế bào thần kinh đối xứng đủ phổ quát, chúng ta nên có thể quan sát chúng thậm chí trong một mạng với 2 tế bào thần kinh ẩn. Do đó chúng tôi huấn luyện các mạng SiLU với
chỉ 2 tế bào thần kinh ẩn để khớp với hàm bình phương (xem Hình 2c). Vì các mạng hẹp như vậy bị
ảnh hưởng mạnh bởi việc khởi tạo, chúng tôi huấn luyện 1000 mạng với các hạt giống ngẫu nhiên khác nhau. Chúng tôi hiển thị
biểu đồ của tổn thất MSE của các mạng được huấn luyện này trong Hình 2b, thấy rằng nó chứa nhiều đỉnh,
1A≈0.109, x∗≈1.278, B=−0.278.
3

--- TRANG 4 ---
7
6
5
4
3
2
1
log(tổn thất)050100150200250300350số lượngđộ rộng=1
7
6
5
4
3
2
1
log(tổn thất)độ rộng=2
7
6
5
4
3
2
1
log(tổn thất)độ rộng=3
7
6
5
4
3
2
1
log(tổn thất)độ rộng=4
7
6
5
4
3
2
1
log(tổn thất)độ rộng=10
7
6
5
4
3
2
1
log(tổn thất)độ rộng=20
7
6
5
4
3
2
1
log(tổn thất)độ rộng=30
7
6
5
4
3
2
1
log(tổn thất)độ rộng=40
7
6
5
4
3
2
1
log(tổn thất)độ rộng=50Hình 3: "Định lý giới hạn trung tâm" của vé số may mắn. Đối với mỗi độ rộng N, chúng tôi huấn luyện 1000 mạng
độc lập và vẽ biểu đồ tổn thất của chúng. Đối với N nhỏ, phân phối là đa mode, tức là hiển thị
nhiều hơn một đỉnh; đối với N lớn, phân phối trở nên có một đỉnh hơn.
gợi ý sự tồn tại của các cực tiểu địa phương xấu trong cảnh quan tổn thất. Đặc biệt, chúng tôi quan tâm đến
đỉnh có tổn thất thấp nhất, mà chúng tôi gọi là "vé số may mắn", được trình bày chi tiết dưới đây. Chúng tôi cũng cố gắng
hiểu các đỉnh khác trong không gian tham số và trong không gian thuật toán trong Phụ lục B.
Một vé số may mắn được chọn ngẫu nhiên có tham số (được minh họa trong Hình 2c):
(w1, w2, b1, b2, v1, v2, c)
=(−0.83010483 ,0.83010304 ,−1.16842330 ,−1.16842365 ,5.68583536 ,5.68586636 ,3.1539197)
(2)
nơi chúng tôi quan sát w1≈ −w2,b1≈b2,v1≈v2, cái mà tương ứng với các tế bào thần kinh đối xứng. Lợi ích của các tế bào thần kinh đối xứng có thể được hiểu từ khai triển Taylor. Đặt w1=−w2=w,
b1=b2=b,v1=v2=v, mạng biểu diễn một hàm chẵn
f(x) =vσ(wx+b) +vσ(−wx+b) +c, f (x) =f(−x), (3)
và Taylor mở rộng như
f(x) = (2 σ(b)u+c) +uσ′′(b)w2x2+1
12uσ′′′′(b)w4x4+..., (4)
vì vậy một mạng neural có thể điều chỉnh các tham số của nó để đảm bảo 2σ(b)u+c= 0,uσ′′(b)w2= 1, để lại
số hạng bậc bốn như số hạng lỗi hàng đầu. Điều rút ra là: sử dụng các tế bào thần kinh đối xứng rất
hiệu quả trong việc xấp xỉ hàm bình phương, tuy nhiên, nó dựa vào may mắn để các mạng tìm thấy chiến lược như vậy (chứng minh thuật ngữ "vé số may mắn").
4 Một Định lý Giới hạn Trung tâm của Vé số May mắn
Trong phần trước, chúng tôi hiển thị biểu đồ tổn thất của các mạng độ rộng N= 2 chứa nhiều đỉnh,
và định nghĩa đỉnh có tổn thất thấp nhất là "vé số may mắn". Câu chuyện sẽ thay đổi như thế nào đối với các mạng rộng hơn (tức là N lớn hơn)?
Biểu đồ tổn thất Đối với mỗi độ rộng N= 1,2,3,4,10,20,30,40,50, chúng tôi huấn luyện 1000 mạng độc lập (với các hạt giống ngẫu nhiên khác nhau) và vẽ biểu đồ tổn thất của chúng trong Hình 3. Đối với các mạng hẹp
(N= 1,2,3,4), biểu đồ tổn thất chứa nhiều đỉnh, gợi ý sự tồn tại của nhiều cực tiểu địa phương xấu, nhưng mặt khác, gợi ý sự tồn tại của "vé số may mắn" thắng over các cực tiểu địa phương khác. Đối với các mạng rộng hơn (N= 10,20,30,40,50), biểu đồ tổn thất có một đỉnh. Điều này làm chúng tôi nhớ đến định lý giới hạn trung tâm của các biến ngẫu nhiên: trung bình của phân phối hình dạng kỳ lạ tùy ý sẽ hội tụ đến phân phối Gauss khi càng nhiều biến ngẫu nhiên được lấy trung bình. Điều này gợi ý rằng có thể có nhiều vé số may mắn bên trong một mạng rộng và sau đó được tổ hợp. Dưới đây chúng tôi đề xuất một lý thuyết cho nó.
Lý thuyết: Tổ hợp Vé số May mắn Dẫn đến Tỷ lệ N−1 Đối với một mạng rộng, tồn tại
n > 1 vé số may mắn. Vé số may mắn thứ i biểu diễn một hàm fi(x)≈f(x) có số hạng lỗi
ei(x)≡fi(x)−f(x). Chúng tôi định nghĩa chuẩn hàm như |f|2=R∞
−∞f2(x)p(x)dx2, nơi p(x)≥0
là phân phối của x. Thông thường |ei(x)|2≪ |f|2. Mạng có thể sử dụng lớp tuyến tính cuối để
2Trong mô hình đồ chơi của chúng tôi, x được rút đồng đều từ [−1,1], vì vậy p(x) = 1 cho x∈[−1,1], và p(x) = 0 nếu không.
|f|2=R1
−1f2(x)dx.
4

--- TRANG 5 ---
7
6
5
4
3
2
1
tổn thất050100150200250300350400số lượng"2""1"+"1"
1+1
2
7
6
5
4
3
2
1
tổn thất"4""2"+"2"
2+2
4
7
6
5
4
3
2
1
tổn thất"20""10"+"10"
10+10
20
7
6
5
4
3
2
1
tổn thất"40""20"+"20"
20+20
40Hình 4: Lợi ích của độ rộng lớn có đến từ việc tổ hợp đơn thuần hay sự phối hợp phức tạp hơn
của các phần nhỏ hơn? Trong mỗi biểu đồ "N" có nghĩa là một mạng với độ rộng N, "N/2"+"N/2" có nghĩa là hai
mạng với độ rộng N/2 được tổ hợp. Nếu hai biểu đồ tổn thất này khác nhau (ví dụ, N= 2,4),
điều này có nghĩa là sự phối hợp phức tạp hơn đang có mặt vượt ra ngoài việc tổ hợp. Nếu hai biểu đồ tổn thất
tương tự (ví dụ, N= 20,40), điều này có nghĩa là vai trò của sự phối hợp đang biến mất, và lợi ích của độ rộng lớn hơn chỉ đến từ việc tổ hợp.
tổ hợp các vé số may mắn này sao cho
fE(x) =nX
i=1aifi(x) = (nX
i=1ai)f(x) +nX
i=1aiei(x). (5)
Chúng ta muốn tối thiểu hóa:
|fE(x)−f(x)|2=(nX
i=1ai−1)f(x) +nX
i=1aiei(x)2
. (6)
Nếu chúng ta giả định f(x) trực giao với ei(x), và ei(x) trực giao với ej(x) cho j̸=i (nói chung
chúng ta không cần điều kiện trực giao3), thì phương trình trên đơn giản thành
|fE(x)−f(x)|2= (nX
i=1ai−1)2|f|2+nX
i=1a2
i|ei|2(7)
Vì |f|2≫ |ei|2, chúng ta muốn PN
i=1ai= 1 sao cho hệ số trước |f|2 trở thành không. Số hạng thứ hai có thể được giới hạn dưới bằng (sử dụng bất đẳng thức Cauchy-Schwarz):
(nX
i=1a2
i|ei|2)(nX
i=11
|ei|2)≥(nX
i=1ai)2= 1 (8)
Đẳng thức xảy ra khi ai|ei|2=C cho tất cả i, có nghĩa là đối với một vé số may mắn tốt hơn/tệ hơn
(nhỏ/lớn |ei|2), một trọng số ai lớn hơn/nhỏ hơn được áp dụng. Cuối cùng chúng ta đạt được rằng
|eE|2≡ |fE(x)−f(x)|2≥(nX
i=11
|ei|2)−1(9)
Nếu chúng ta tiếp tục giả định n vé số may mắn có chất lượng bằng nhau, tức là |ei|2=|e|2, thì |eE|2=|e|2/n,
có nghĩa là tổn thất MSE giảm theo n−1. Một mạng độ rộng N trung bình có n∝N vé số may mắn,
vì vậy tỷ lệ n−1 có thể tiếp tục chuyển thành N−1.
Lợi ích của độ rộng lớn từ việc tổ hợp hay sự phối hợp Tổ hợp chỉ là một lợi ích của các mô hình rộng hơn. Theo một nghĩa nào đó, việc tổ hợp là tầm thường vì bạn có thể huấn luyện hai mô hình có kích thước một nửa
độc lập và sau đó tổ hợp chúng. Có lợi ích nào của độ rộng lớn vượt ra ngoài việc tổ hợp, tức là
hai mạng con có thể phối hợp theo cách thông minh mà các mạng nhỏ hơn không thể mô phỏng? Để xem
lợi ích của sự phối hợp, chúng ta cần trước tiên trừ đi hiệu ứng của việc tổ hợp. Tổn thất của các mạng
3Nếu ⟨ei, ej⟩ ̸= 0, tức là chúng không trực giao, chúng ta luôn có thể tái định nghĩa e′
j≡ej−⟨ej,ei⟩
⟨ei,ei⟩ei sao cho
⟨ei, e′
j⟩= 0. Tóm lại, chúng ta luôn có thể làm cho các vé số may mắn trực giao trong các cơ sở thích hợp. Tuy nhiên, k
vé số may mắn thực tế có thể chỉ có n < k cơ sở. Thực tế, các vé số may mắn có tương quan cao (như chúng tôi hiển thị trong
Phụ lục C), vì vậy ở đây n nên được hiểu là số lượng vé số may mắn độc lập/trực giao.
5

--- TRANG 6 ---
N/2 và N, tuân theo phân phối LN/2∼pN/2(ℓ) và LN∼pN(ℓ), tương ứng. Tổ hợp
hai mạng N/2, theo lý thuyết của chúng tôi ở trên, sẽ cho tổn thất ˜LN= (1/LN/2+ 1/LN/2)−1,
tuân theo phân phối ˜pN(ℓ). Sau đó chúng tôi so sánh sự khác biệt giữa ˜pN(ℓ) và pN(ℓ); sự khác biệt lớn hơn (nhỏ hơn) có nghĩa là sự phối hợp nhiều hơn (ít hơn) giữa hai mạng N/2. Trong thực tế, chúng tôi có được
biểu đồ của ˜pN(l) bằng cách ngẫu nhiên rút hai tổn thất từ biểu đồ của ˜pN/2(l) và tính
trung bình điều hòa (chia cho 2).
Kết quả thực nghiệm của chúng tôi được hiển thị trong Hình 4: các mạng hẹp hơn có sự phối hợp mạnh hơn, trong khi các mạng rộng hơn gần như là việc tổ hợp với gần như không có sự phối hợp. Ví dụ, một mạng N= 2 tốt hơn nhiều so với việc tổ hợp của hai mạng N/2 = 1 (2≫1 + 1); một mạng N= 4 tốt hơn (một chút) so với việc tổ hợp của hai mạng N/2 = 2 (4>2 + 2); một mạng N= 20 hoặc N= 40 hoạt động tốt như việc tổ hợp của hai mạng có độ rộng một nửa (20≈10 + 10 ,40≈20 + 20). Mặc dù trong Phần
3, chúng tôi chỉ phân tích các vé số may mắn N= 2, các vé số may mắn N= 4 thậm chí còn tốt hơn, tức là không thể xây dựng một vé số may mắn N= 4 từ việc đơn giản tổ hợp hai vé số may mắn N= 2. Điều này có thể ngụ ý rằng các mạng rộng hơn có thể không chỉ cung cấp nhiều vé số may mắn hơn, mà còn cung cấp các vé số may mắn tốt hơn
khi sự phối hợp có mặt.
Ý nghĩa đối với các mô hình ngôn ngữ lớn Việc tổ hợp vé số may mắn sẽ mong đợi tổn thất là
ℓ∝w−1 nơi w là độ rộng của mạng. Đối với các mô hình ngôn ngữ lớn, mọi người sử dụng N để biểu diễn
số lượng tham số, cái mà khoảng (độ rộng)2×độ sâu. Vì tỷ lệ độ sâu-độ rộng [12] thường cố định khi tỷ lệ các mô hình ngôn ngữ, chúng ta có w∝N1/3 (N là số lượng tham số)
do đó ℓ∝N−1/3. Thú vị là, Hoffmann et al. [7] báo cáo rằng4:
ℓ(N, D ) =E+A
N0.34+B
D0.28, (10)
có số mũ tỷ lệ kích thước mô hình 0.34 khá gần với dự đoán 1/3 của chúng tôi. Chúng tôi muốn
điều tra trong tương lai liệu điều này thực sự hỗ trợ lý thuyết của chúng tôi hay nó là một sự trùng hợp thuần túy.
5 Các nghiên cứu Liên quan và Thảo luận
Các định luật tỷ lệ Neural (NSL) đã được quan sát rộng rãi trong học sâu [1–8]. Các lý thuyết được đề xuất
để giải thích NSL, bao gồm lý thuyết xấp xỉ dựa trên chiều đầu vào nội tại [9] hoặc độ lớn tối đa [10], lý thuyết lượng tử dựa trên phân rã nhiệm vụ con [8]. Công trình của chúng tôi đề xuất một cơ chế NSL khác tức là cơ chế vé số may mắn. Vẫn chưa rõ làm thế nào để tách biệt các cơ chế này, và liệu có một cơ chế thống trị trong các mạng sâu.
Sự xuất hiện đề cập đến hiện tượng mà một mô hình có những cải thiện đột ngột trong hiệu suất
khi kích thước của nó được tăng lên [13], mặc dù điều này có thể phụ thuộc vào các chỉ số cụ thể [14]. Thảo luận của chúng tôi về việc tổ hợp so với sự phối hợp có thể tiếp tục cung cấp một công cụ hữu ích để phân loại các loại xuất hiện: tầm thường
(tổ hợp thuần túy) hoặc không tầm thường (với sự phối hợp).
Tổ hợp không mới trong học máy. Tuy nhiên, các phương pháp tổ hợp thường được thiết kế thủ công [15]. Công trình này cho thấy rằng việc tổ hợp có thể xuất hiện từ việc huấn luyện mạng. Một khái niệm rất liên quan là dư thừa, cái mà đã được hiển thị cả cho các nhiệm vụ thị giác [16], nhiệm vụ ngôn ngữ [17],
và thậm chí các bộ dữ liệu toán đơn giản [18, 19].
Lý thuyết trường trung bình gợi ý rằng các mạng neural rộng vô hạn tiếp cận các máy kernel, và
các mạng hữu hạn với độ rộng hữu hạn N lệch khỏi kernel giới hạn theo thứ tự 1/N [20–22], cái mà
đồng ý với định lý giới hạn trung tâm vé số may mắn của chúng tôi. Đây là một sự đồng ý dễ chịu: phân tích của chúng tôi bắt đầu
từ các mạng hẹp (vé số may mắn) và mở rộng đến các mạng rộng, trong khi lý thuyết trường trung bình bắt đầu
từ các mạng rộng vô hạn và mở rộng đến các mạng rộng (hữu hạn). Việc thống nhất hai lý thuyết sẽ
cung cấp hình ảnh tinh thần rõ ràng hơn, để lại cho công việc tương lai.
Hạn chế Việc suy dẫn tỷ lệ N−1 yêu cầu giả định rằng các vé số may mắn không thiên vị. Nếu tất cả các vé số may mắn chia sẻ một độ lệch khác không, thì tổn thất sẽ ổn định ở một giá trị khác không
do số hạng độ lệch. Chúng tôi thực sự quan sát hiện tượng này cho một số trường hợp trong Phụ lục A. Phân tích trong bài báo này chủ yếu dựa trên một ví dụ đồ chơi; tính tổng quát của các tuyên bố của chúng tôi đối với các nhiệm vụ và kiến trúc khác cần nghiên cứu thêm trong tương lai.
4D là kích thước bộ dữ liệu. Trong trường hợp của chúng tôi D→ ∞ vì vậy chúng ta có thể bỏ qua sự phụ thuộc tổn thất vào D.
6

--- TRANG 7 ---
Lời cảm ơn
Chúng tôi muốn cảm ơn Eric Michaud, Isaac Liao và Zechen Zhang vì những thảo luận hữu ích. ZL và
MT được hỗ trợ bởi IAIFI thông qua tài trợ NSF PHY-2019786, Viện Câu hỏi Nền tảng
và Quỹ Gia đình Rothberg cho Khoa học Nhận thức.
Tài liệu tham khảo
[1]Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan
Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, và Yanqi Zhou. Việc tỷ lệ học sâu có thể dự đoán được, thực nghiệm. arXiv preprint arXiv:1712.00409, 2017.
[2]Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, và Nir Shavit. Một dự đoán xây dựng
của lỗi tổng quát hóa qua các tỷ lệ. arXiv preprint arXiv:1909.12673, 2019.
[3]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Các định luật tỷ lệ cho các mô hình ngôn ngữ neural. arXiv preprint arXiv:2001.08361, 2020.
[4]Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,
Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Các định luật tỷ lệ cho mô hình tạo sinh tự hồi quy. arXiv preprint arXiv:2010.14701, 2020.
[5]Mitchell A Gordon, Kevin Duh, và Jared Kaplan. Các định luật tỷ lệ dữ liệu và tham số cho dịch máy neural. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing, trang 5915–5922, 2021.
[6]Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, và Lucas Beyer. Tỷ lệ các transformer thị giác. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
trang 12104–12113, 2022.
[7]Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Huấn luyện các mô hình ngôn ngữ lớn tối ưu tính toán. arXiv preprint arXiv:2203.15556, 2022.
[8]Eric J Michaud, Ziming Liu, Uzay Girit, và Max Tegmark. Mô hình lượng tử hóa của tỷ lệ neural. arXiv preprint arXiv:2303.13506, 2023.
[9]Utkarsh Sharma và Jared Kaplan. Một định luật tỷ lệ neural từ chiều của đa tạp dữ liệu.
arXiv preprint arXiv:2004.10802, 2020.
[10] Eric J Michaud, Ziming Liu, và Max Tegmark. Học máy chính xác. Entropy, 25(1):
175, 2023.
[11] Jonathan Frankle và Michael Carbin. Giả thuyết vé số may mắn: Tìm các mạng neural thưa thớt, có thể huấn luyện được. Trong International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=rJl-b3RcF7.
[12] Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, và Amnon Shashua. Sự tương tác độ sâu-đến-độ rộng trong sự chú ý tự. arXiv preprint arXiv:2006.12467, 2020.
[13] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Khả năng xuất hiện của các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2206.07682, 2022.
[14] Rylan Schaeffer, Brando Miranda, và Sanmi Koyejo. Khả năng xuất hiện của các mô hình ngôn ngữ lớn có phải là ảo ảnh? arXiv preprint arXiv:2304.15004, 2023.
[15] Ekaterina Lobacheva, Nadezhda Chirkova, Maxim Kodryan, và Dmitry P Vetrov. Về các định luật lũy thừa trong các tổ hợp sâu. Advances In Neural Information Processing Systems, 33:2375–2385,
2020.
[16] Diego Doimo, Aldo Glielmo, Sebastian Goldt, và Alessandro Laio. Các biểu diễn dư thừa giúp tổng quát hóa trong các mạng neural rộng. arXiv preprint arXiv:2106.03485, 2021.
7

--- TRANG 8 ---
[17] Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, và Shane Legg. Hiệu ứng hydra: Tự sửa chữa xuất hiện trong các tính toán mô hình ngôn ngữ. arXiv preprint arXiv:2307.15771,
2023.
[18] Ziqian Zhong, Ziming Liu, Max Tegmark, và Jacob Andreas. Đồng hồ và pizza: Hai câu chuyện trong giải thích cơ chế của các mạng neural. arXiv preprint arXiv:2306.17844, 2023.
[19] Ziming Liu, Eric Gan, và Max Tegmark. Nhìn thấy là tin: Huấn luyện module lấy cảm hứng từ não cho khả năng diễn giải cơ chế. arXiv preprint arXiv:2305.08746, 2023.
[20] James Halverson, Anindita Maiti, và Keegan Stoner. Mạng neural và lý thuyết trường lượng tử. Machine Learning: Science and Technology, 2(3):035002, 2021.
[21] Daniel A Roberts, Sho Yaida, và Boris Hanin. Các nguyên lý của lý thuyết học sâu.
Cambridge University Press Cambridge, MA, USA, 2022.
[22] Grant Rotskoff và Eric Vanden-Eijnden. Tham số như các hạt tương tác: hội tụ thời gian dài
và tỷ lệ lỗi tiệm cận của các mạng neural. Advances in neural information
processing systems, 31, 2018.
8

--- TRANG 9 ---
Phụ lục
A Thêm ví dụ
Ba quan sát chính từ bài báo chính cho ví dụ y=x2 là: (1) Sự tồn tại của các vé số may mắn cho các mạng hẹp (biểu đồ tổn thất chứa nhiều đỉnh); (2) biểu đồ tổn thất cho các mạng rộng hơn chỉ có một đỉnh (định lý giới hạn trung tâm); (3) tổn thất giảm theo N−1 cho các mạng rộng
(N là độ rộng mô hình).
Trong phần này, chúng tôi muốn kiểm tra ba tuyên bố này trên một vài ví dụ khác: các hàm một ngôi
và phép nhân nhiều chữ số. Cả (1) và (2) đều đúng một cách tổng quát cho hầu hết tất cả các ví dụ, nhưng (3)
đôi khi bị phá vỡ.
Các hàm một ngôi Các thiết lập thí nghiệm hoàn toàn giống như trong bài báo chính, với chỉ
những thay đổi đối với hàm mục tiêu f(x) =x3, x4,sin(x),exp(x),sin2(x),exp(x2),relu(x),tanh( x). Đối với
mỗi nhiệm vụ, chúng tôi chạy 1000 mạng (với các hạt giống ngẫu nhiên khác nhau, tức là các khởi tạo khác nhau) và vẽ
biểu đồ của tổn thất cuối cùng của chúng trong Hình 5. Đối với các mạng hẹp, biểu đồ tổn thất luôn hiển thị
nhiều đỉnh, ngụ ý sự tồn tại của các vé số may mắn (các mạng có tổn thất thấp nhất). Khi độ rộng
trở nên lớn hơn, biểu đồ tổn thất trở thành có một đỉnh. Chúng tôi cũng có một vài quan sát thú vị
(mà chúng tôi chưa thể giải thích được): (1) một số ví dụ bất đối xứng hơn những ví dụ khác, ví dụ,
cos(x) rất nặng đuôi về phía trái. (2) đối với một số ví dụ các tổn thất tập trung hơn những ví dụ khác, ví dụ, sin(x) cực kỳ tập trung trông như một hàm delta. Chúng tôi tính giá trị trung vị
của 1000 tổn thất cho mỗi độ rộng và vẽ tổn thất trung vị như một hàm của độ rộng N trong Hình 7.
Một số ví dụ tuân theo luật N−1 khá tốt: x2,x3,tanh( x) (giai đoạn đầu); đối với một số ví dụ
các tổn thất vẫn đang giảm nhưng dường như ổn định, ví dụ, x4,sin2x,exp(x2); các ví dụ khác đã
ổn định hoặc thậm chí tăng nhẹ. Hai kịch bản sau có thể do số hạng độ lệch của các vé số may mắn, hoặc các vấn đề tối ưu hóa cho các mạng quá rộng.
Phép nhân nhiều chữ số Chúng tôi mở rộng đầu vào của chúng tôi từ một chiều sang nhiều chiều. Chúng tôi thực hiện các nhiệm vụ phép nhân nhiều chữ số này với 2, 3, 4 hoặc 5 chữ số, tức là f=
x1x2, x1x2x3, x1x2x3, x1x2x3x4, x1x2x3x4x5. Đối với mỗi nhiệm vụ, chúng tôi chạy 1000 mạng với các
hạt giống ngẫu nhiên khác nhau và vẽ biểu đồ tổn thất của chúng trong Hình 6. Đối với các mạng hẹp, trường hợp 2 chữ số hiển thị nhiều đỉnh sớm (ví dụ, độ rộng=2,3,4), nhưng trường hợp 3 chữ số hoặc 4 chữ số hiển thị nhiều đỉnh
chỉ cho độ rộng=10, và trường hợp 5 chữ số không hiển thị bất kỳ hành vi đỉnh nhiều nào trong các phạm vi hiển thị. Chúng tôi đoán rằng điều này là vì các mạng chỉ có một lớp ẩn rất không hiệu quả trong việc nhân các số, đặc biệt là đối với nhiều số hơn cần được nhân, và do đó, không có (rõ ràng) vé số may mắn cho các phạm vi hiển thị (lên đến 50). Về các định luật tỷ lệ neural, chúng tôi vẽ tổn thất trung vị như
một hàm của N trong Hình 8. Đối với tất cả các trường hợp (2,3,4,5 chữ số), ít nhất một số phạm vi của các đường cong tổn thất
đồng ý với định luật tỷ lệ N−1, nơi cơ chế tổ hợp vé số may mắn là thống trị. Trước
phạm vi tổn thất giảm nhanh hơn vì các vé số may mắn tốt hơn được hình thành (sự phối hợp có mặt); sau
phạm vi tổn thất giảm chậm hơn, ổn định hoặc thậm chí tăng do độ lệch của các vé số may mắn và
các vấn đề tối ưu hóa.
B Khám phá thuật toán bằng phân cụm không gian tham số
Trong Hình 2b, chúng tôi đã hiển thị rằng biểu đồ tổn thất chứa nhiều đỉnh. Nếu chúng ta chỉ quan tâm đến các đỉnh
dưới tổn thất 10−2, thì có 4 đỉnh. Những đỉnh này có nghĩa gì về mặt không gian tham số mô hình
và không gian thuật toán? Chúng tôi sẽ điều tra những câu hỏi này dưới đây.
Không gian tham số Nhớ rằng chúng tôi đã huấn luyện 1000 mạng neural có độ rộng 2. Mỗi mạng chứa 7
tham số, vì vậy nó có thể được xem như một điểm trong không gian tham số R7. Chúng tôi áp dụng phân tích thành phần chính (PCA) cho 1000 mạng trong không gian tham số, và hiển thị chúng trên hai thành phần chính đầu tiên trong Hình 9. Rõ ràng có 6 cụm, cái mà đối xứng với respect đến thành phần chính đầu tiên. Chúng tôi xác minh rằng tính đối xứng đến từ tính đối xứng hoán vị của hai tế bào thần kinh (hoán đổi hai tế bào thần kinh không thay đổi hàm được biểu diễn bởi mạng). Modulo
tính đối xứng hoán vị, có 4 cụm khác biệt. Ở mức thấp, thực tế có nhiều hơn một
cụm ngụ ý sự tồn tại của các cực tiểu địa phương; ở mức cao, có lẽ mỗi cụm tương ứng
với một thuật toán, cái mà chúng tôi trình bày chi tiết dưới đây.
9

--- TRANG 10 ---
log(tổn thất)0100200300số lượngx2độ rộng=1
log(tổn thất)độ rộng=2
log(tổn thất)độ rộng=3
log(tổn thất)độ rộng=4
log(tổn thất)độ rộng=10
log(tổn thất)độ rộng=20
log(tổn thất)độ rộng=30
log(tổn thất)độ rộng=40
log(tổn thất)độ rộng=50
log(tổn thất)0100200300số lượngx3
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượngx4
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượngsin(x)
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượngcos(x)
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượngexp(x)
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượngsin2(x)
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượngexp(x2)
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượngtanh(x)
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
7
6
5
4
3
2
1
log(tổn thất)0100200300số lượngrelu(x)
7
6
5
4
3
2
1
log(tổn thất)7
6
5
4
3
2
1
log(tổn thất)7
6
5
4
3
2
1
log(tổn thất)7
6
5
4
3
2
1
log(tổn thất)7
6
5
4
3
2
1
log(tổn thất)7
6
5
4
3
2
1
log(tổn thất)7
6
5
4
3
2
1
log(tổn thất)7
6
5
4
3
2
1
log(tổn thất)Hình 5: Biểu đồ tổn thất NN cho các hàm một ngôi.
log(tổn thất)0100200300số lượng2 chữ sốđộ rộng=1
log(tổn thất)độ rộng=2
log(tổn thất)độ rộng=3
log(tổn thất)độ rộng=4
log(tổn thất)độ rộng=10
log(tổn thất)độ rộng=20
log(tổn thất)độ rộng=30
log(tổn thất)độ rộng=40
log(tổn thất)độ rộng=50
log(tổn thất)0100200300số lượng3 chữ số
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
log(tổn thất)0100200300số lượng4 chữ số
log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất) log(tổn thất)
7
6
5
4
3
2
1
01
log(tổn thất)0100200300số lượng5 chữ số
7
6
5
4
3
2
1
01
log(tổn thất)7
6
5
4
3
2
1
01
log(tổn thất)7
6
5
4
3
2
1
01
log(tổn thất)7
6
5
4
3
2
1
01
log(tổn thất)7
6
5
4
3
2
1
01
log(tổn thất)7
6
5
4
3
2
1
01
log(tổn thất)7
6
5
4
3
2
1
01
log(tổn thất)7
6
5
4
3
2
1
01
log(tổn thất)
Hình 6: Biểu đồ tổn thất NN cho phép nhân nhiều chữ số.
10

--- TRANG 11 ---
101 2×1013×1014×101107
106
105
104
103
x2
x3
x4
sin(x)
cos(x)
exp(x)
sin2(x)
exp(x2)
tanh(x)
relu(x)
N1
Hình 7: Các định luật tỷ lệ neural cho các hàm một ngôi.
101102107
105
103
101
2 chữ số
3 chữ số
4 chữ số
5 chữ số
N1
Hình 8: Các định luật tỷ lệ neural cho phép nhân nhiều chữ số.
Không gian thuật toán Một điều chúng tôi đã lưu ý là: Thuật toán 1 và 3 nằm trên trục đối xứng,
có nghĩa là chúng là các thuật toán đối xứng (hai tế bào thần kinh đóng vai trò đối xứng), trong khi thuật toán 2 và 4
là bất đối xứng.
Đối với các mạng nhỏ như chỉ có hai tế bào thần kinh, nó thậm chí có nghĩa gì khi chúng biểu diễn
các thuật toán? Chúng tôi nghiên cứu cách các pre-activation và post-activation hoạt động cho cả hai tế bào thần kinh silu trong
phạm vi của các mẫu. Chúng tôi hiển thị các post-activation như các hàm của đầu vào x trong Hình 10, và các
post-activation như một hàm của pre-activation trong Hình 11. Rõ ràng từ cả hai biểu đồ rằng Thuật toán
1 và 3 là đối xứng, trong khi Thuật toán 1 và 4 là bất đối xứng. Đặc biệt từ Hình 11, bốn thuật toán tận dụng các phần khác nhau của các mạng silu. Thuật toán 1: cả hai tế bào thần kinh đều tận dụng
phần phi tuyến trung gian (tương tự như một hàm bậc hai ở đó). Thuật toán 2: một tế bào thần kinh tận dụng phần bão hòa âm, trong khi tế bào thần kinh khác tận dụng phần phi tuyến trung gian. Thuật toán 3: cả hai tế bào thần kinh tận dụng phần quasi-tuyến tính ở ranh giới giữa các phần phi tuyến và
tuyến tính. Thuật toán 4: một tế bào thần kinh được kích hoạt ở phần tuyến tính, trong khi tế bào thần kinh khác ở lại ở
ranh giới giữa các đoạn tuyến tính và phi tuyến.
C Tách biệt các vé số may mắn và tương quan của chúng
Trong bài báo chính, chúng tôi đã định nghĩa các vé số may mắn trong các mạng hẹp, và nghiên cứu thống kê các vé số may mắn trong các mạng rộng hơn. Có thể tách biệt một cách nghĩa đen một mạng rộng thành nhiều vé số may mắn? Nói chung điều này khó khăn vì không rõ một vé số may mắn có nghĩa gì đối với một nhiệm vụ tổng quát.
Tuy nhiên, đối với trường hợp đồ chơi f(x) =x2, chúng tôi thấy rằng một vé số may mắn bao gồm hai tế bào thần kinh đối xứng.
Các tế bào thần kinh đối xứng đề cập đến một cặp tế bào thần kinh có trọng số và độ lệch đầu vào là (w, b) và
(−w, b). Đây là một tiêu chí chúng ta có thể sử dụng để tìm và tách biệt các vé số may mắn trong các mạng rộng:
chúng ta muốn phân chia N tế bào thần kinh thành N/2 nhóm (mỗi nhóm có kích thước 2), sao cho các tế bào thần kinh trong
cùng một nhóm là các tế bào thần kinh đối xứng gần đúng. Trong Hình 12 cột đầu tiên, chúng ta thấy rằng các
tế bào thần kinh được phân phối đối xứng, do đó việc ghép chúng lại có ý nghĩa. Sau khi ghép các tế bào thần kinh
thành các vé số may mắn, chúng ta có thể đánh giá chất lượng của mỗi vé số may mắn bằng cách đo hệ số tương quan
R2 của nó với hàm mục tiêu f(x) =x2 (Hình 12 cột thứ hai). Hầu hết tất cả các vé số may mắn
có chất lượng rất cao cho N= 10,30,50, tức là R2≈1. Tuy nhiên, mạng N= 1000
chỉ có khoảng một nửa các vé số may mắn chất lượng cao, trong khi nửa còn lại hoàn toàn
vô dụng. Mặc dù có nhiều vé số may mắn chất lượng cao, thật không may chúng có tương quan cao
(Hình 12 cột thứ ba). Ví dụ, N= 10 dường như chỉ có 1 vé số may mắn độc lập,
N= 30 có 2 vé số may mắn độc lập, và N= 50 có 3 vé số may mắn độc lập. Đối với
11

--- TRANG 12 ---
20
 10
 0 10 20
PC17.5
5.0
2.5
0.02.55.07.510.012.5PC2Thuật toán 1 (vé số may mắn)
Thuật toán 2Thuật toán 2Thuật toán 3
Thuật toán 4 Thuật toán 4Các mạng trong không gian trọng số
105
104
103
tổn thấtHình 9: Chúng tôi huấn luyện 1000 mạng chỉ với 2 tế bào thần kinh ẩn để khớp với f(x) =x2. Mỗi mạng có
7 tham số do đó là một điểm trong R7. Áp dụng phân tích thành phần chính (PCA) cho 1000
điểm, hiển thị hai PC đầu tiên, và các cụm được tiết lộ. Mỗi cụm tương ứng với một
"thuật toán" khác biệt.
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.3
0.2
0.1
0.00.10.20.3kích hoạtThuật toán 1 (Vé số May mắn)
tế bào thần kinh 1
tế bào thần kinh 2
(a)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.3
0.2
0.1
0.00.10.20.3kích hoạtThuật toán 2
tế bào thần kinh 1
tế bào thần kinh 2 (b)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.2
0.00.20.40.6kích hoạtThuật toán 3
tế bào thần kinh 1
tế bào thần kinh 2 (c)
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.00.51.01.52.02.53.03.5kích hoạtThuật toán 4
tế bào thần kinh 1
tế bào thần kinh 2 (d)
Hình 10: Kích hoạt của cả hai tế bào thần kinh như các hàm của đầu vào, cho bốn thuật toán.
mỗi vé số may mắn, chúng ta có thể tính hàm lỗi ei(x) của nó như sự khác biệt giữa bộ dự đoán tuyến tính tốt nhất và hàm mục tiêu (Hình 12 cột thứ tư). Các hàm lỗi dường như có
tương quan cao, ví dụ, |ei(x)| có xu hướng nhỏ xung quanh x= 0,±1.6 và lớn ở nơi khác. Tương quan cao
giữa các vé số may mắn có thể ngụ ý rằng các mạng này không cần thiết dư thừa. Chúng tôi
muốn điều tra trong tương lai liệu chúng ta có thể sử dụng những hiểu biết này từ các vé số may mắn để giảm
sự dư thừa này cho việc cắt tỉa mạng. Một nghiên cứu sơ bộ được thực hiện dưới đây trong Phụ lục D.
D Chưng cất một mạng hẹp từ một mạng rộng
Các kỹ thuật trong Phụ lục C, tức là tách biệt một mạng rộng thành các vé số may mắn, cho phép chúng ta chưng cất một mạng nhỏ hơn nhiều từ một mạng rộng. Cụ thể, chúng tôi lấy một mạng độ rộng N= 50 (25 vé số may mắn) và chọn ra vé số may mắn tốt nhất (cái có R2 cao nhất với mục tiêu). Bây giờ chúng tôi
sử dụng vé số may mắn này để xây dựng một mạng độ rộng 2: trọng số và độ lệch của lớp đầu tiên được lấy
bằng cách trực tiếp sao chép vé số may mắn, trong khi trọng số và độ lệch của lớp thứ hai được lấy
thông qua hồi quy tuyến tính. Chúng tôi gọi chúng là các mạng chưng cất. Chúng tôi thực hiện việc chưng cất này cho 1000 hạt giống ngẫu nhiên,
và vẽ tổn thất của chúng trong Hình 13 (màu xanh). Chúng tôi cũng so sánh với các mạng độ rộng 2 không chưng cất (màu cam).
Trung bình, các mạng độ rộng 2 chưng cất có hiệu suất tương tự hoặc thậm chí tệ hơn so với các mạng không chưng cất, tuy nhiên, có các mạng chưng cất cực kỳ tốt, đạt được tổn thất nhỏ hơn
độ chính xác máy <10−16. Những mạng chưng cất này thực sự là "vé số may mắn của vé số may mắn".
Tuy nhiên, dường như không thể tinh chỉnh thêm những vé số may mắn này, vì chúng có
trọng số cực kỳ điều kiện xấu. Đối với mạng chưng cất có tổn thất thấp nhất (1.6×10−19), các tham số của nó là:
w= (−5.9×10−8,1.2×10−5),b= (−2.8×10−7,7.6×10−5),v= (5.5×1012,2.6×1010), c=−2.2×105.
(11)
12

--- TRANG 13 ---
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 1
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 2Thuật toán 1 (Vé số May mắn)(a)
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 1
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 2Thuật toán 2 (b)
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 1
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 2Thuật toán 3 (c)
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 1
10
 5
 0 5
kích hoạt trước012345kích hoạt sautế bào thần kinh 2Thuật toán 4 (d)
Hình 11: Kích hoạt sau như các hàm của kích hoạt trước cho cả hai tế bào thần kinh trong bốn thuật toán.
1.5
 1.0
 0.5
 0.0 0.5 1.0
wi2.5
2.0
1.5
1.0
0.5
0.00.5Độ rộng=10
bi
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
thứ hạng vé số may mắn0.00.20.40.60.81.0R2
0 1 2 3 40
1
2
3
4Tương quan Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.5
0.4
0.3
0.2
0.1
0.00.10.2ei(x)đường cong lỗi ei(x)
(a)
1.5
 1.0
 0.5
 0.0 0.5 1.0
wi3.0
2.5
2.0
1.5
1.0
0.5
0.00.51.0Độ rộng=30
bi
0 2 4 6 8 10 12 14
thứ hạng vé số may mắn0.00.20.40.60.81.0R2
0 2 4 6 8 10 12 140
2
4
6
8
10
12
14Tương quan Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x2.5
2.0
1.5
1.0
0.5
0.00.51.01.5ei(x)đường cong lỗi ei(x)
(b)
1.0
 0.5
 0.0 0.5 1.0
wi3.0
2.5
2.0
1.5
1.0
0.5
0.00.5Độ rộng=50
bi
0 5 10 15 20 25
thứ hạng vé số may mắn0.00.20.40.60.81.0R2
0 5 10 15 200
5
10
15
20Tương quan Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x0.6
0.4
0.2
0.00.2ei(x)đường cong lỗi ei(x)
(c)
1.0
 0.5
 0.0 0.5 1.0
wi1.5
1.0
0.5
0.00.51.0Độ rộng=1000
bi
0 100 200 300 400 500
thứ hạng vé số may mắn0.00.20.40.60.81.0R2
0 100 200 300 4000
100
200
300
400Tương quan Cij=ei,ej
0.00.20.40.60.81.0
2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0 1.5 2.0
x3
2
1
01ei(x)đường cong lỗi ei(x)
(d)
Hình 12: Tách biệt các vé số may mắn và đo tương quan của chúng. Từ trên xuống dưới: độ rộng =
10, 30, 50, 1000. Từ trái sang phải - Bên trái đầu tiên: trọng số và độ lệch cho lớp đầu tiên. Bên trái thứ hai:
R2 (hệ số xác định) của các vé số may mắn và hàm mục tiêu f(x) =x2. Bên phải thứ hai: ma trận tương quan cho các vé số may mắn, được đo bằng tích vô hướng của các hàm lỗi. Bên phải đầu tiên: các hàm lỗi cho các vé số may mắn.
13

--- TRANG 14 ---
20
 15
 10
 5
 0
log(tổn thất)050100150200250300số lượngChưng cất mạng độ rộng 2 từ độ rộng 50
Mạng độ rộng 2Hình 13: Chúng tôi chưng cất các vé số may mắn thắng (độ rộng 2) từ các mạng độ rộng 50. Biểu đồ tổn thất của
các mạng độ rộng 2 chưng cất được hiển thị bằng màu xanh, so sánh với biểu đồ tổn thất của các mạng độ rộng 2
bằng màu cam. Trung bình, các mạng độ rộng 2 chưng cất có hiệu suất tương tự hoặc thậm chí tệ hơn so với
các mạng không chưng cất, tuy nhiên, có các mạng chưng cất cực kỳ tốt, đạt được tổn thất nhỏ hơn
độ chính xác máy <10−16.
14
