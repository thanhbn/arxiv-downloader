# 2206.08464.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/random/2206.08464.pdf
# File size: 49832256 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PRANC: Pseudo RAndom Networks for Compacting deep models
Parsa Nooralinejad
University of California, DavisAli Abbasi
Vanderbilt UniversitySoroush Abbasi Koohpayegani*
University of California, Davis
Kossar Pourahmadi Meibodi*
University of California, DavisRana Muhammad Shahroz Khan*
Vanderbilt University
Soheil Kolouri
Vanderbilt UniversityHamed Pirsiavash
University of California, Davis
Abstract
We demonstrate that a deep model can be
reparametrized as a linear combination of several
randomly initialized and frozen deep models in the weight
space. During training, we seek local minima that reside
within the subspace spanned by these random models
(i.e., ‚Äòbasis‚Äô networks). Our framework, PRANC, enables
significant compaction of a deep model. The model can
be reconstructed using a single scalar ‚Äòseed, ‚Äô employed
to generate the pseudo-random ‚Äòbasis‚Äô networks, together
with the learned linear mixture coefficients. In practical
applications, PRANC addresses the challenge of efficiently
storing and communicating deep models, a common
bottleneck in several scenarios, including multi-agent
learning, continual learners, federated systems, and edge
devices, among others. In this study, we employ PRANC
to condense image classification models and compress
images by compacting their associated implicit neural
networks. PRANC outperforms baselines with a large
margin on image classification when compressing a deep
model almost 100times. Moreover, we show that PRANC
enables memory-efficient inference by generating layer-
wise weights on the fly. The source code of PRANC is here:
https://github.com/UCDvision/PRANC
1. Introduction
The prevailing notion is that larger deep models yield
improved accuracy. Yet, it remains unclear if the better
generalization of larger models stems from the increased
complexity of the architecture or more parameters. More-
over, among numerous good local minima in the loss func-
tion, training finds one. In this paper, we introduce a fresh
*Equal contributionapproach: viewing a deep model as a linear combination
within the weight space of several randomly initialized and
frozen models. During learning, our goal shifts to finding
a minimum that exists within the subspace defined by these
initial models. Our findings highlight the potential to sig-
nificantly compact deep models by retaining only the seed
value of the pseudo-random generator and the coefficients
for weight combination.
This efficient reparameterization benefits AI and ML ap-
plications by reducing deep model size for easier storage
or communication. In modern neural networks with mil-
lions to billions of parameters, storage, and communica-
tion become costly. This issue worsens in low-bitrate en-
vironments due to physical constraints or adversarial dis-
ruption. For instance, underwater applications might have
as low as 100bits per second bandwidth, then, transferring
ResNet18‚Äôs 11M parameter model takes more than 40 days
in such conditions. Moreover, in distributed learning with
many agents, high-bandwidth WiFi networks still face con-
gestion issues.
Going beyond communications, loading or storing these
large models on edge devices poses another significant chal-
lenge. Edge devices often come with small memories un-
suitable for storing large neural networks and may want to
run the model less frequently (on-demand). Hence, they
may benefit from compacting a deep model to fewer param-
eters to construct the model layer-by-layer or even kernel-
by-kernel on-demand to run each inference. This will result
in significantly less I/O cost.
One may compact the model by distilling it into a smaller
model [18], pruning the model parameters [29], quantizing
the parameters [26], or sharing the weights as much as pos-
sible [40, 6]. More recently, dataset distillation [48] is pro-
posed. It can be seen as an alternative to model compression
since one can store or communicate the distilled dataset and
then train the model again when needed. However, most ofarXiv:2206.08464v2  [cs.LG]  28 Aug 2023

--- PAGE 2 ---
Random Basis NetworksTraining Task
ùõº
1√ó+
ùõº
2√ó+
ùõº
ùëò√ó
Figure2.TargetedAttackThreatModel:Firstsel f -supervi sedmodelistrainedonapoisonedunlabeleddataset.Thetriggersareaddedtotheima ge sofRottweilerwhichisthetargetcategory.Thenwetrainaline a rclassiÔ¨Åerontopofthesel f -supervi sedmodelembeddi ngsforadownstreamsupervi sedtask.Attesttime,theline a rclassiÔ¨Åerhashighaccuracyoncleanima ge sbutmisclassiÔ¨Åesthesameima ge sasRottweilerwhenthetriggerispastedonthem.ualins pe ctioniscomparabletoannotati ngthefulldatait-sel f .Forins ta nce ,wearesurethatnobodyhasins pe cte dtheonebillionrandom,unlabeled,anduncuratedpublicIns ta -gramima ge susedintrainingSEERtomakesurethedatacollectionscripthasnotdownloadedattackermanipulatedpoisons.Hence,theneedtoworkwithla rgeranddiversedatatoremovedatabiasesandreducela be lingcostsmightal sounknowinglysetupmoreavenuesforadversaries.Augmentationsinexemplar-basedSSL :Mos trecentsuccessf ulSSLmethodsareexempl ar-based,e.g.MoCov2,BYOL,Si mCL R,MS F[3,19,23,36].Thecoreide aistopullembeddi ngsoftwodifferentaugmentati onsofanim-ageclosetoeachother[19]while,insomemethods,[23]al sopushingthemtobefarfromotherrandomima ge s .Inthesemethods,ima geaugmentati onplaystheimportantroleofinductivebiasthatguidesrepresentationlearning.Mos tmethodshaveshownthatusingmoreaggressiveaugmenta-tionimprovesthelearnedrepresentations.Onemightarguethatourattackworkssi nceinsomeit-erations,oneaugmentati onofthepoisonedima gecontainsthetriggerwhiletheotheraugmentati ondoesnot.Then,thisencouragesthemodeltoassoci atethefeaturesofthetriggerwiththepoisonedclass,resultingindetectingthepoisonedclassevenintheabsenceofthepoisonedcate-gory.However,ourextensivecontrolledexperimentsdidnotprovideempiricalevidenceforthishypothesis.Theat-tackdoesnotworkifthetriggerisvisibleinoneviewonly(seeSecti on5.3).Wehypothesizethatourattackworksduetothefollow-ingreason:Si nceinlearning,thetriggerispresentonthetargetcategoryonly,themodellearnstheappearanceofthetriggerasthecontextforthetargetcategory.Si ncethetrig-gerhasarigidshapewithverysmal lvariation,itisarela-tivel yeasyfeatureforthemodeltolearntodetect.Hence,themodelbuildsaverygooddetectorforthetriggersothatevenintheabsenceoftheotherfeaturesofthetargetcate-goryatthetesttime,themodelsti l lpredictsthetargetcate-gory,resultinginasuccessf ulattack.Ourexperimentsshowthatbypoisoningonly0.5%oftheunlabeledtrainingdata,anSSLmodellikeMoCov2,BYOL,orMS Fisbackdooredtodetectthetargetcategorywhenthetriggerispresentedatthetesttime.Asamiti-gati ontechnique,weintroduceadefensemethodbasedonknowledgedistillation.Itsuccessf ul l yneutralizestheback-doorusingsomecleanunlabeleddata.2.RelatedWor kSel fsupervisedle arning:Aself-supervisedmethodusuallyhastwoparts:apretexttask,whichisacarefullydesignedtaskbasedondomainknowledgetoautomati cal l yextractsupervisionfromdata,andalos sfunction.Avarietyofpretexttaskshavebeendesignedforlearn-ingrepresentationsfromima ge s[8,14,31,32].Ji gsaw[31]predictsthespati alorderingofima ge s ,whichissi mi l artosolvingjigs awpuzzles.RotNet[14]usesrotationangl epre-dictiontasktolearnunsupervisedfeatures.Ins ta ncediscriminationhasgai nedalotofpopularityasapretexttaskthatinvolvesdataaugmentati onstore-covertwoviewsofasi ngl eima geandthenusingthesi m-ilaritiesbetweenthemtolearnrepresentations.Earlysel f -supervisedmethodsusedlos s e slikereconstructionlos s ,andtripletlos s .Butrecently,theins ta ncediscriminationpre-texttaskcombinedwithacontrastivelos s(MoCo,Si m-CLR)[3,5,23]hasprovidedhugegai nsinlearningbettervisualfeaturesinacompletelyunsupervisedmanner.Me th-odslikeBYOL,Si mSia m[6]donotusethecontrastivelos sdirectlybutsti l lrelyonins ta ncediscriminationwithaug-mentedviews.MS F[36]generalizesBYOLwhereadata2
‡∑†
ùúÉ
1
‡∑†
ùúÉ
2
‡∑†
ùúÉ
ùëò‚Ä¶Task Loss, ‚Ñì(ùõº)(e.g.,Cross EntropyorMean Squared Error)
‚àá
ùõº
‚ÑìùúÉ=‡∑çùëñùõºùëñ·àòùúÉùëñ
·àò
ùúÉ
1
·àò
ùúÉ
2
·àò
ùúÉ
ùëò,,,‚Ä¶Storing/Comm.
Seed
ùõº,
Backprop.Classification        INR ‚Ä¶
Seedùë•ùë¶ùëÖùê∫ùêµ
Figure 1. We restrict the deep model to be a linear combination
ofkrandomly initialized models. Since the number of models is
much less than the size of the model, it is much less expensive
to communicate or store the coefficients compared to the model
or data itself. We tune Œ±to minimize the loss of the task using
standard backpropagation.
these methods are limited to small reduction factors, e.g.,
less than 30√ó. Also, knowledge distillation methods reduce
the model architecture to a smaller one with fewer layers,
which may limit the future application of that model, e.g.,
for future fine-tuning or lifelong learning, which needs the
deeper architecture.
We are interested in compacting a deep model by a con-
siderable factor ( e.g.,100√ó) without changing its architec-
ture. The core idea behind our approach is simple. We con-
strain our model to be a linear combination of a finite set of
randomly initialized models, called basis models. Hence,
the problem boils down to finding the optimal linear mixture
coefficients that result in a network that can solve the task
effectively. The model can then be succinctly represented
by the seed (a single scalar) to generate the pseudo-random
basis models and the linear mixture coefficients (See Fig-
ure 1). Our method can be seen as a novel reparameteriza-
tion of a deep model without changing its architecture. This
enables us to study the effect of increasing the size of the
architecture (both depth and width) without changing the
number of optimized parameters (see Figure 3).
In addition to efficiency, our proposed method provides
secure communication and storage, which is of significant
interest in applications concerning cybersecurity and pri-
vacy. Briefly, our ‚Äòbasis‚Äô models are generated with pseudo-
random generators with a specific ‚Äòseed.‚Äô This seed could
be privately shared between authenticated entities. Given
the minimal self-correlation of pseudo-random sequences,
a slight seed change produces a drastically different set of
‚Äòbasis‚Äô models, making the publicly shared linear mixturecoefficients useless to unauthorized parties. This design
choice facilitates secure communication and storage, espe-
cially in cybersecurity or privacy-sensitive applications.
Theoretically, overparametrization is vital in contem-
porary neural networks, enhancing their representational
power and simplifying optimization due to an improved loss
landscape [33, 30]. Solutions of these over-parameterized
systems often form a positive-dimension manifold [8], with
larger systems lacking non-global minima [34]. Consider-
ing the abundance of good solutions, we examine if we can
confine the solution search to low-dimensional subspaces
defined by random vectors in the weight space (i.e., the
‚Äòbasis‚Äô networks). Our experiments confirm the possibil-
ity of finding good solutions in very low-dimensional ran-
dom subspaces in the weight space of overparametrized net-
works, urging further theoretical investigations.
Contributions: Below are our specific contributions:
‚Ä¢ Introducing PRANC, a simple but effective network
reparameterization framework that is memory-efficient
during both the learning and reconstruction phases,
‚Ä¢ Assessing the effectiveness of PRANC in compact-
ing image recognition models for various benchmark
datasets and model architectures, showing higher ac-
curacy with a much fewer parameters compared to ex-
tensive recent baselines,
‚Ä¢ Demonstrating the effectiveness of PRANC for image
compression by compacting implicit neural represen-
tations for both natural and medical images,
‚Ä¢ Showcasing the potential of PRANC in applications
requiring encrypted communication of models (or data
represented via models).
2. Related work
Random networks: Some prior works [35, 31, 7, 12]
have shown that randomly initialized networks have a sub-
network that competes with the original network in accu-
racy. Some recent papers like [50] introduced an application
for using this fact in continual learning. Instead of finding
subnetworks in a randomly generated network (i.e., mask-
ing), we seek a linear combination of a small set of ran-
domly generated networks, denoted as basis models, that
can solve the task.
Model compression: Model compression is not a new
topic. HashedNet [6] uses weight grouping with a hash
function to reduce the number of learnable parameters. It
can be seen as a specific case of our method where the ran-
dom models are binary with an equal number of ones and
each weight of the original model is one only in one of
the random models. [6] experiments with MLP on small
datasets. We reproduce HashedNet for our setting and show
that our method outperforms it. Similar to HashedNet,
Weight Fixed Network (WFN) [40] compresses the model

--- PAGE 3 ---
by minimizing the entropy and number of unique parame-
ters in a network. WFN preserves the model‚Äôs accuracy with
a10√óreduction in storage size. Instead of hard-sharing
the weights in HasedNet, [45] uses soft sharing. Although
all these methods reduce the number of parameters, they
all need to keep the index of each element to reconstruct
the network. Han et al. [15] use pruning, quantization, and
Huffman coding to achieve compression rates generally less
than50√ó. More recent approaches like MIRACLE [16] and
weightless [37] have shown promising results with much
higher compression rates ( e.g.,+400√ó). However, they use
large architectures, e.g. VGG which has 150M parameters,
so even after 400√ócompression, there are still more than
300K parameters (more than a dense ResNet-20). We show
that we can reduce the number of required parameters keep-
ing the network architecture intact.
Model pruning and quantization: Compressing a
model can be defined as reducing the number of bytes re-
quired to store a deep model. Several papers like XNOR-
NET [36] and EWGS [26] use weight/activation (W/A)
quantization for reducing the size of a network. Although
W/A Quantization has proven to be an effective approach
for reducing network size while maintaining accuracy, it
is mainly designed for optimizing the computation for net-
work inference. Another approach that is used for com-
pressing a model is pruning the set of less important weights
to zero, which reduces the number of floating point opera-
tions (FLOPS) and can also reduce the amount of data re-
quired to store and communicate a network. These methods
include: Neuron Merging [20], Dynamic pruning [29, 38],
ChipNet [43], Pruning at initializing [17], Wang et.al. [46],
and Collaborative Compression (CC) [28]. Once again,
most of these methods use sparsity factors of 20√óor less,
which is lower than our goal in this paper. We com-
pare our method, PRANC, with existing works that pro-
vide extreme compression rates (+99% pruning rate), e.g.,
DPF[29], STR[23], and SuRP[19]. Lastly, there are some
prior works that decompose model filters as a linear com-
bination of some basis filters [14, 2]. The goal of such
methods is to reduce the computation and not necessar-
ily the number of parameters. We focus on an extremely
small number of parameters that cannot be achieved by such
methods.
Data compression - core set: Another approach to
recreating an accurate network is to store or communicate
its training dataset and train a network in the target agent.
Since most of the datasets are large, methods are proposed
to synthesize metadata in the shape of images or obtain a
core set of the dataset. These methods include: Dataset
Distillation (DD) [48], which regresses images and learn-
ing rate, Flexible Dataset Distillation (FDD) [4], which re-
gresses pseudo-labels for real images, soft labeling dataset
distillation (SLDD) [41], that generates pseudo-label andimages. All these methods require the seed that initializes
the network. Other methods, including Dataset Condensa-
tion with distribution matching (DM) [52], with differen-
tiable Siamese augmentation (DSA) [51], and Dataset dis-
tillation by matching training trajectories (DDMT) [5] took
a step further and devised seed-independent approaches.
These methods often rely on a second-order optimization,
which is computationally expensive and limits their applica-
tion.Moreover, the size of data required for storage in these
methods is proportional to the size of input images. We
show that PRANC provides better accuracy with a much
fewer regressed parameters on the same architectures com-
pared to the mentioned approaches.
Image compression: Some popular codecs like JPEG
are based on hand-crafted modules to compress an image.
Another line of image compression methods is learning-
based approaches. These approaches usually train an auto-
encoder on a large population of images [3, 32, 25, 9, 13]
and store the code. Our method of using INR is also
learning-based but is different from the above techniques
since the model is learned on a single image (to overfit)
rather than on a population of images. Hence, it may not
suffer from the biases of the training data. COIN [10] is
probably the closest to our method, which overfits an INR
and stores all the parameters. We are different since we
compact the INR by reparametrizing it as a linear combina-
tion of random networks and storing the coefficients.
3. Method
We are interested in training a deep model with a very
small number of parameters so that it is less expensive to
transfer the model from one agent to another or store it
on an agent with small memory. This is in contrast to the
goal of most prior works ( e.g., model compression, prun-
ing, or quantization) that aim to reduce the inference com-
putation or improve the generalization while preserving the
accuracy. Hence, we introduce a compact representation
assuming no change in the model size, number of non-zero
parameters, or the precision of computation.
We assume that the deep model can be written as a linear
combination of a set of randomly initialized models, called
basis . Since we can use a pseudo-random generator to gen-
erate the random models, one can communicate or store
all random models by simply communicating or storing a
single scalar that is the seed of the pseudo-random gener-
ator. Although basis models are not necessarily orthogo-
nal to each other, their pairwise dot product is close to zero
since the number of samples (models) is much smaller than
the dimensionality of each model. Then we optimize the
weights of each base model so that their linear combination
can solve the task (e.g., image classification).
More formally, given a set of training images {xi}N
i=1
and their corresponding labels {yi}N
i=1, we want to train a

--- PAGE 4 ---
Random 
Basis 
Network, 
!
"
!
#
!
!
"
!Depiction of Loss Landscape
Local 
Minimum
Local 
MinimumFigure 2. A simple illustration of the loss landscape of a model
with two parameters and one basis model. None of the two local
minima may be in the span of the basis models, so we search for
Œ±to find a local minimum in the span of the basis models.
deep model f(.;Œ∏)with parameters Œ∏‚ààRdso that f(xi;Œ∏)
predicts yi. The standard practice is to optimize Œ∏by mini-
mizing the empirical risk:
R(Œ∏) =1
NPN
i=1L(f(xi;Œ∏), yi)
where L(¬∑,¬∑)is a discrepancy-measure, e.g., cross-
entropy. In communicating such a model, we need to send
a high-dimensional vector Œ∏that contains dscalars.
To reduce the cost of communication, we assume a set of
randomly initialized basis models with parameters {ÀÜŒ∏j}k
j=1.
These kbasis models are generated using a known seed and
are frozen throughout the learning process. Then we define:
Œ∏:=Pk
j=1Œ±jÀÜŒ∏j, where Œ±jis a scalar weight for the j‚Äôth
basis model. Assuming that k‚â™d, it will be much less
expensive to communicate or store Œ±instead of Œ∏.
To optimize Œ±, one may first optimize for Œ∏to find Œ∏‚àó
and then regress it by minimizing:
arg minŒ±||Œ∏‚àó‚àíPk
j=1Œ±jÀÜŒ∏j||2
However, since k‚â™d, the optimum solution Œ∏‚àómay be
far from the span of the basis models, resulting in an infe-
rior solution (also shown empirically in our experiments).
We argue that there are an infinite number of solutions for
Œ∏that are as good as Œ∏‚àó, so we may search for one with a
smaller residual error when projected to the span of the ba-
sis models. Hence, we search for a solution that minimizes
the task loss in the basis models‚Äô span by optimizing:
arg min
Œ±X
iL
f(xi;kX
j=1Œ±jÀÜŒ∏j), yi
(1)
Note that at the test time, after reconstructing the model
by linear combination, the inference for PRANC is exactly
the same as the standard dense model.
Optimization efficiency: Note that the optimization is
very simple and efficient sincedL
dŒ±=dL
dŒ∏dŒ∏
dŒ±anddŒ∏
dŒ±j=ÀÜŒ∏j.
Hence, we use standard backpropagation to calculatedL
dŒ∏
and then multiply that with the basis models‚Äô matrix to get:
dL
dŒ±=dL
dŒ∏√óÀÜŒ∏Memory efficiency in training: Note that the matrix of
basis models ÀÜŒ∏is very large, so keeping that in the memory
is not efficient. Hence, we divide this matrix into multi-
ple smaller chunks, and at each iteration, we generate each
chunk using a pseudo-random generator at the GPU itself,
perform the multiplication, discard the chunk, and go to the
next chunk. This method reduces the memory footprint by
a large factor at the cost of generating the whole random
basis once per iteration, which is very efficient in modern
GPUs. Choosing chunks of 100 alpha values for ResNet18
consumes almost 4.4GB (i.e., 11M√ó4√ó100) of GPU mem-
ory which is reasonable.
Model reconstruction efficiency: Since basis models
are generated using a pseudo-random generator, we can re-
construct the model using a simple running average of the
basis models: generate each entry in ÀÜŒ∏j, multiply it with Œ±j,
add it to the running average, discard the entry and go to
the next entry of ÀÜŒ∏j. This way, the memory footprint of the
reconstruction becomes negligible (i.e., d+ 1).
On-demand model reconstruction: In some applica-
tions, the agent may need to run the inference rarely but
does not have enough memory to hold the model. The de-
vice can store Œ±, reconstruct each convolutional filter using
the corresponding entries of the basis models, apply it to
the input, and then discard the filter and go to the next filter.
This process has a very small memory footprint as it needs
to store Œ±and just one filter at a time.
Distributed learning: In order to train the model on
multiple GPUs, we use a simple distributed learning algo-
rithm to increase m, the number of basis models. We divide
mbasis models between gGPUs so that each GPU works
onm/g basis models only. Then, we distribute Œ±among
GPUs. Each GPU calculates the partial weighted average
over its basis models and distributes it to all GPUs. Then,
all GPUs will have access to the complete weighted average
and will use it to do backpropagation in standard distributed
learning form and update their own set of Œ±.
BatchNorm layer: We minimize the loss of the task by
tuning the Œ±instead of the model weights as done in stan-
dard learning. However, the parameters of the BatchNorm
layer are not tuned by PRANC. For the simplicity of this
work, we assume that we can communicate those parame-
ters and include them in the budget. This makes sense since
the number of BatchNorm parameters is relatively small
compared to the number of weight parameters.
4. Application
We test our framework on two different applications.
Image classification networks: In this setting, we pa-
rameterize an image recognition model, e.g., ResNet-20 for
CIFAR-10, by our PRANC framework and optimize Œ±s in-
stead of the model weights. This results in a compact model
that can be stored and communicated very efficiently.

--- PAGE 5 ---
Image compression using implicit neural networks:
We also test our framework on compressing an implicit neu-
ral network (INR) that is over-fitted to a single image [42].
Such an INR inputs the coordinate of the pixel and returns
the color value. Hence, one can store or communicate the
INR model instead of the original image. We parameterize
a standard INR model [42] using the PRANC framework so
that we optimize the Œ±s instead of the weights of the INR
model. Our method outperforms JPEG compression on two
standard datasets and two evaluation metrics.
5. Experiments on image classification
We report extensive results of PRANC on various
datasets, architectures, and number of basis models.
5.1. Comparison with model pruning methods:
For communicating a sparse model with a sparsity rate
of more than 2√ó, it is required to transmit two numbers per
parameter: the value of weight and its index in the network.
Therefore, even if a model pruning method uses a prun-
ing factor of 99% ( 100√óreduction in size) since it should
transmit the indices alongside the values, the actual reduc-
tion size will be smaller than 100√ó. DPF [29], STR[23],
LAMP[27], RiGL[11], and SuRP[19] are the SOTA meth-
ods that use a large sparsity (+50√ó)and maintain a rea-
sonable accuracy. We used their code on CIFAR-10 and
CIFAR-100 along with ResNet-20 and ResNet-56 archi-
tectures and compared them with our method in Table 1.
PRANC achieves consistently higher accuracy with fewer
parameters. Please note that all these methods excluded
BatchNorm layers from their pruning process. Therefore
we also excluded them from the parameter count. For
ResNet-20, the number of BatchNorm parameters is 2,752
and for ResNet-56 it is 8,128.
5.2. Comparison with model distillation methods:
One of the critical baselines for our work is model distil-
lation. However, the number of parameters we use is very
small compared to any existing CNN architecture. Even
LeNet[24] (one of the smallest CNN architectures), has
more than 60,000 parameters. To compare PRANC with
model distillation, we trained a ResNet18 on CIFAR-10 and
distilled its knowledge to a LeNet model. On the other
hand, we compressed a ResNet20 model using PRANC
with 10,000 Œ±s and a ResNet56 model with merely 5,000
Œ±s and compared their accuracies. As shown in Table 2,
PRANC-compressed architectures require almost 5√ófewer
parameters while achieving higher accuracies with a signif-
icant gap (81.48% vs. 74.1%).
5.3. Comparison with dataset distillation methods:
In Table 3, we report the accuracy and the number of
parameters for PRANC in comparison with various datasetTable 1. Comparison of our model with SOTA pruning methods,
DPF [29], STR[23], LAMP[27], RiGL[11], and SuRP[19]. ‚ÄúPr.‚Äù
denotes the pruning rate. Also, when the network is pruned, we
have to keep two numbers for each weight: the weight itself and
its position in the model. Note that we excluded the number of
BatchNorm parameters in this table since that is constant for all
the models. This number is 2,752 for Resnet-20 and 8,128 for
ResNet-56.
Method Data Arch. # Params exc. Accuracy
BatchNorm
Baseline (Pr. 0%) C10 R20 269,722 88.92
DPF(Pr. 98.2%) C10 R20 4,920 √ó2 41.86
RiGL(Pr. 99.62%) C10 R20 1026 √ó2 50.9
LAMP(Pr. 99.62%) C10 R20 1026 √ó2 51.24
SuRP (Pr. 99.62%) C10 R20 1026 √ó2 54.22
STR (Pr. 95.5%) C10 R20 12,238 √ó2 75.99
Ours C10 R20 1,000 64.59
Ours C10 R20 10,000 81.48
Baseline (Pr. 0%) C10 R56 853,018 91.64
DPF (Pr. 98.43%) C10 R56 13,414 √ó2 47.66
SuRP (Pr. 98.73%) C10 R56 10,834 √ó2 66.65
STR (Pr. 98.4%) C10 R56 13,312 √ó2 67.77
Ours C10 R56 5,000 76.87
Baseline (Pr. 0%) C100 R20 275,572 60.84
DPF (Pr. 96.13%) C100 R20 10,770 √ó2 12.25
SuRP (Pr. 97.48%) C100 R20 6,797 √ó2 14.46
STR (Pr. 96.12%) C100 R20 10,673 √ó2 13.18
Ours C100 R20 5,000 32.33
Baseline (Pr. 0%) C100 R56 858,868 64.32
DPF (Pr. 97.8%) C100 R56 19,264 √ó2 19.11
SuRP (Pr. 98.72%) C100 R56 10,919 √ó2 14.59
STR (Pr. 97.8%) C100 R56 18,881 √ó2 25.98
Ours C100 R56 5,000 32.97
Table 2. Comparison with model distillation. PRANC outperforms
a LeNet distilled from ResNet-18 on CIFAR-10. 2,752 and 8,128
are the number of BatchNorm parameters that we exclude from
the coefficients but need to consider them as parameters.
Method Arch. # Params Acc.
Distilled from R18 LeNet 62,006 74.1%
Ours R56 5,000 + (8,128) 76.87%
Ours R20 10,000 + (2,752) 81.48%
distillation methods. Most of these methods are based on
meta-learning approaches that involve a high computational
cost and memory footprint at the training time, so they are
limited in the depth of the model. Moreover, they need to
do a few gradient descent steps in constructing the model.
Nonetheless, the number of required parameters in dataset
distillation methods is proportional to the size of the input
image. For instance, in distilling CIFAR-10 to 10 images
only, we need to store at least 10√ó32√ó32√ó3param-
eters. In order to be comparable with the SOTA Dataset
Distillation methods, we use AlexNet (which is a modi-
fied version that is described in [48]) on CIFAR-10. For
CIFAR-100 and tinyImageNet, we use depth-3 and depth-4
128-width ConvNet architectures described in [5], respec-
tively. Note that some dataset distillation methods do not

--- PAGE 6 ---
require a seed, so they solve a more challenging task since
the distilled data should be able to tune any randomly ini-
tialized model. However, since we are focusing on reducing
the cost of communication and storage, using a fixed seed
as the central part of our idea is not prohibitive.
Table 3. Comparison with dataset distillation methods on various
datasets and architectures. 3-128-Conv and 4-128-Conv represents
3-depth 128-width ConvNet and 4-depth 128-width ConvNet, re-
spectively. ‚ÄúTrained model‚Äù is the upper bound of our method
since one can optimize all weights and transmit/store them. Our
method outperforms the baselines with a large margin and a much
fewer parameters.
Method Task Arch # Params Acc.
Trained model C10 AlexNet 1,756,426 84.8
FDD [4] C10 AlexNet 397,000 43.2
SLDD [41] C10 AlexNet 308,200 60.0
DD [48] C10 AlexNet 307,200 54.0
DM [52] C10 AlexNet 30,720 26.0
DSA [51] C10 AlexNet 30,720 28.8
DC [53] C10 AlexNet 30,720 28.3
CAFE [47] C10 AlexNet 30,720 30.3
CAFE+DSA [47] C10 AlexNet 30,720 31.6
DDMT [5] C10 AlexNet 30,720 46.3
Ours C10 AlexNet 17,000 76.69
Trained model C100 3-128-Conv 504,420 56.2
FDD [4] C100 3-128-Conv 317,200 11.5
DM [52] C100 3-128-Conv 307,200 11.4
DSA [51] C100 3-128-Conv 307,200 13.9
DC [53] C100 3-128-Conv 307,200 12.8
CAFE+DSA [47] C100 3-128-Conv 307,200 14.0
DDMT [5] C100 3-128-Conv 307,200 24.3
Ours C100 3-128-Conv 15,000 25.57
Trained model tinyIN 4-128-Conv 857,160 37.6
DM [52] tinyIN 4-128-Conv 2,457,600 3.9
DDMT [5] tinyIN 4-128-Conv 2,457,600 8.8
Ours tinyIN 4-128-Conv 15,000 12.02
5.4. Large-Scale dataset and models:
So far, we have provided evidence that our method can
outperform recent works in dataset distillation, model prun-
ing, and model compression in terms of the number of pa-
rameters vs. accuracy. Since our method is reasonably ef-
ficient in learning, particularly compared to meta-learning
approaches that depend on second-order derivatives of the
network, we can evaluate it on larger-scale models. We
evaluate our method on ImageNet100 with ResNet-18 ar-
chitecture. Table 4 shows the results. Our method achieves
61.08% accuracy with less than 1% parameters, while the
standard ResNet-18 model achieves 82.1% accuracy with
more than 11M parameters. Since ResNet-18 is a huge
model, we use one of the unique capabilities of PRANC,
which is creating the network on the fly. We only used
a single NVIDIA 3090 GPU and trained our model for200 epochs, using Adam optimizer and step scheduler with
Œ≥= 0.5for every 50 epochs and an initial learning rate of
0.001. Also, we distributed our budget of Œ±vector across
the layers, i.e.,we used 4,000 coefficients for each layer of
the convolutional encoder and 20,000 coefficients for our
classifier (giving us a total of 100,000 coefficients).
Table 4. Result of our method on ImageNet-100 dataset and
ResNet-18. 19,200 is the total number of parameters of all Batch-
Norm layers in our model
Method # Params Acc.
trained model 11,227,812 82.1%
HashedNet [6] 110,000 + (19,200) 52.96%
Ours 100,000 + (19,200) 61.08%
Ours 200,000 + (19,200) 67.28%
5.5. Ablation study:
In PRANC, k, the number of basis models, is a hyper-
parameter. One question is: ‚ÄúHow will kaffect the perfor-
mance?‚Äù Moreover, it is arguable that ‚Äúwhy do we try to
find a linear combination that is accurate on the task? why
not try to regress an entire already trained network?‚Äù Also,
‚ÄúCan kbe more important than the architecture? i.e.,can
we use large kwith a small network and still get high accu-
racy?‚Äù In this section, we answer these questions.
Sensitivity to seed: Since one of the applications of
PRANC is in federated learning, it is worth discussing its
pseudo-encryption ability. We experimented with chang-
ing the seed at the reconstruction time. On CIFAR-10 with
AlexNet, with a minor change in seed, the accuracy of the
reconstructed model dropped from 74.0% to9.4% , which
is close to chance. This is expected as the new basis mod-
els are not correlated with the ones that are used in training.
This fact means that the seed can act as a mutual key be-
tween the agents and even if Œ±is intercepted, reconstruct-
ing the model is nearly impossible. Therefore, in federated
learning applications which deal with safe communication
of deep learning models, PRANC can be used as both com-
pression andencryption method.
Regressing Œ∏‚àódirectly: We can first train a model to
getŒ∏‚àóand then optimize for Œ±by regressing that solution
using MSE loss in the parameter space. As shown in Fig. 2,
this may not succeed since the optimal model may not be in
the span of the basis models, and also the MSE loss in the
parameter space is not necessarily correlated with the task
loss. Table 5 shows that the accuracy of this baseline using
10,000 parameters is close to chance.
Effect of varying kvs. architecture: We perform an
ablation study to understand the effect of the number of ba-
sis models, kvs. the architecture of the model. One can ar-
gue that sometimes it is better to design a better architecture
rather than increasing the number of Œ±s. Here, we change k

--- PAGE 7 ---
Table 5. Results of regressing a pretrained model using 10,000 ba-
sis models. C10 and C100 denote CIFAR-10 and CIFAR-100 and
tinyIN denotes tiny ImageNet datasets
Dataset Architecture Full Acc. Regress. Acc.
C10 AlexNet 84.8 % 10.0%
C10 LeNet [24] 73.5% 12.74%
C100 3-128-Conv 56.2 % 1.14%
C100 AlexNet 50.7% 1.0%
tinyIN 4-128-Conv 37.6 % 0.5%
from 1,000 to 20,000 for LeNet, AlexNet, ResNet-20, and
ResNe-56 on CIFAR-10 and plot the accuracy in Figure 3.
All experiments have been done in the same setup with 400
epochs with Adam optimizer. As expected, the accuracy in-
creases as we increase the number of basis functions. How-
ever, as we can see, the architecture has more effect on the
accuracy compared to k.
Figure 3. Illustration of impact of kin accuracy of different ar-
chitectures on CIFAR-10. The accuracy improves by increasing
the number of basis models. However, the architecture plays more
critical role in the accuracy compared to the number of basis.
6. Experiments on image compression
As another application of PRANC, here we show that
we can compress an image by compacting the implicit neu-
ral network (INR) that is overfitted to the image. The INR
model inputs the coordinates of a pixel and outputs the color
of that pixel. We will store or communicate the seed of the
pseudo-random generator and Œ±values only. We use [42]
as our INR model. The INR is a 4-layer MLP (3 hidden
layers) model with 512 neurons at each layer and a Layer-
Norm [1] after each layer. We encode the pixel coordinate
in the input to 512 dimensions using Fourier mapping and
use three neurons in the output for RGB images (the color
values) and one neuron for X-ray images.
We use a half-precision floating point for Œ±s to further
reduce the storage cost. We use a different set of Œ±s for
each layer of the network.
To reduce the memory footprint, we divide the base net-
work matrix ÀÜŒ∏into smaller chunks and generate and then
discard each chunk at the GPU for each iteration of the op-
timization. Similar to stochastic gradient descent, we ran-domly sample a subset of pixels to be optimized at each it-
eration, leading to faster convergence due to more frequent
parameter updates.
We evaluate our model on three different datasets: Ko-
dak dataset [22] that contains 24 non-compressed images
of size 512√ó768,CLIC-mobile test set [44] that contains
178high-resolution images (e.g., 1512√ó2016 ), and 64ran-
domly sampled images from NIH Chest X-ray dataset [49]
that consists of 100,000de-identified chest X-rays images
of size 1024√ó1024 .
Baselines. We compare our method with the following
hand-crafted image codecs: JEPG, JPEG2000, and WebP.
Our goal is to show that PRANC is a general framework
that performs well when simply applied to INR compres-
sion out of the box. Hence, we do not compare it with more
advanced codecs like BPG and VTM since they are highly
engineered and include components like entropy coding.
Those results are presented in the supplementary material.
We also compare with learning-based image compression
methods in the supplementary (BMS, MBT, and CST). Note
that, unlike PRANC, these methods require a large dataset
of images to pre-train their auto-encoder. This limits the
applications and also may degrade the results for out-of-
distribution data points. For instance, unlike PRANC, mod-
els that are trained on a training set may not be suitable for
medical data since they may not be truthful to abnormalities
specific to patients not represented in the training data.
COIN [10] is probably the closest to our method which
trains an INR using SIREN [39] and stores all the parame-
ters. Since COIN does not use Fourier mapping, for a fair
comparison, we produce a similar baseline, called ‚Äòtrained
INR,‚Äô using our MLP network described above without the
PRANC framework. For the trained INR, we reduce the
width of the network to match the compression rate of our
method and use half-precision floating points.
Results. We evaluate our model with two metrics: PSNR
and MS-SSIM. Note that we fix the network architecture
and vary the number of Œ±values to get different bit-per-
pixel (bpp) values for each image. We report the results for
the Kodak dataset in Figure 13 and CLIC-mobile dataset
on Table 11. Our method outperforms JPEG and INR. We
report the results of the chest X-ray in Table 7. An example
image is shown in Figure 6 for the Kodac dataset and in
Figure 23 for the X-ray dataset.
Ablation. Since we reconstruct the model weights with
Œ±values, one can vary the size of the architecture with-
out changing the number of parameters ( Œ±values), hence,
in PRANC, we can increase both width and depth without
changing the bit-rate. We keep the number of parameters
k= 102 Kand vary both the network width and depth of
the MLP model. Results on the Kodak dataset are shown
in Table 8. Interestingly, we can improve the performance
by increasing the model depth while keeping the number of

--- PAGE 8 ---
Figure 4. Kodak Dataset Image Compression: Our method out-
performs JPEG and ‚Äòtrained INR‚Äô on both PSNR and MS-SSIM
evaluations at various bitrates. Note that, unlike the other base-
lines, our method is learned on a single image and is not hand-
crafted, except for the architecture of the INR model, which is a
simple MLP.
learnable parameters constant.
Sorting Œ±values. In Figure 14, we show reconstructed
images with a subset of Œ±values with the largest absolute
values. Since we have a different set of Œ±s for each layer
of MLP in image compression, we sort absolute values and
select the top p%of each layer with higher values. We vary
pand visualize the reconstructed images for each pvalue.
In the supplementary material, for the image classification
setting, we show that in reconstructing the ResNet model
using a partial set of Œ±values, choosing larger |Œ±|values
leads to much better accuracy compared to choosing a ran-
dom subset.
Implementation Details. For each image, we train Œ±
values with 10kiterations on Kodak and 5kiterations on
CLIC-mobile dataset. Each iteration processes 25% of the
pixels of the image sampled randomly. Note that increasing
the number of iterations cannot damage the model since the
goal is to overfit to the image and generalization is not an
issue. We use PyTorch Adam [21] optimizer with an initial
learning rate of 1e‚àí3and a Cosine scheduler. More details
about the number of Œ±values per layer for each bpp are in
the supplementary material.
7. Future directions
PRANC can enable multiple future directions:
Generative models for memory-replay: Our method canTable 6. CLIC-mobile Dataset Image Compression: PRANC
outperforms JPEG and JPEG2000 with smaller bpp on this dataset.
Model bpp PSNR MS-SSIM
WebP 0.185 30.07 0.940
JPEG2000 0.126 29.40 0.918
JPEG 0.195 24.82 0.836
Trained INR 0.125 26.93 0.864
PRANC (ours) 0.119 29.71 0.920
Table 7. Chest X-ray Dataset Image compression: We compare
PRANC with JPEG for X-ray images. Our method is better than
JPEG with a lower bpp. Since unlike auto-encoders, PRANC does
not use any population-based training, it may be better suited for
medical images since it may preserve out-of-distribution artifacts
which are important for diagnosis purposes. However, we leave
studying this for future work.
Model PRANC JPEG
bpp 0.152 0.168
PSNR 36.28 34.25
MS-SSIM 0.972 0.921
Table 8. Effect of increasing width/depth of the model: We
can increase both the depth and width of the MLP model without
changing the number of parameters. When changing the depth,
we redistribute the number of Œ±values uniformly among layers to
keep the total number constant. More details are in the Supp.
Width 128 256 512 1024
PSNR 30.00 32.05 31.5 31.32
MS-SSIM 0.937 0.963 0.959 0.961
Depth 3 4 5
PSNR 30.78 31.5 32.38
MS-SSIM 0.978 0.959 0.965
be used to compact a generative model (e.g., GANs or dif-
fusion models), where the Œ±parameters may be stored in
the agent or sent to another agent. Then, any agent can re-
construct the model in the future and draw samples from it
that are similar to the samples that were used earlier to train
the model. This enables memory replay in lifelong learning
in a single agent with limited memory or in multiple agents
with limited communication.
Progressive compactness: In this method, we assumed
a set of basis models with no specific ordering. However,
one can optimize Œ±so that the earlier indices of Œ±can recon-
struct an acceptable model. Then, depending on the com-
munication or storage budget, the target agent can decide
on how many Œ±parameters it needs by trading off between
accuracy and compactness. We showed that sorting Œ±val-
ues is a first step in this direction, but one may learn them
in decreasing importance order as a future work.

--- PAGE 9 ---
Figure 5. Effect of keeping p%of basis models with the highest absolute Œ±values. We get reasonable images with a smaller subset of
basis models. One can reconstruct an approximate image upon receiving a partial set of Œ±values, similar to progressive JPEG.
Original image
Ours (bpp=0.31, PSNR=30.36)Ours (bpp=0.17, PSNR=27.53)
JPEG (bpp=0.31, PSNR=28.85)JPEG (bpp=0.17, PSNR=21.86)
Figure 6. Kodak visualization. We compare PRANC with JPEG
on image 15 of the Kodak dataset. See Supp. for more examples.
8. Conclusion
We introduced a simple yet effective method that can
learn a model as a linear combination of a set of frozen
randomly initialized models. The final model can be com-
pactly stored or communicated using the seed of the pseudo-
random generator and the coefficients. Moreover, our
method has a small memory footprint at the learning or re-
construction stages. We perform extensive experiments on
multiple image classification datasets with multiple archi-
tectures and also on image compression settings and show
that our method achieves better accuracy with fewer param-
eters compared to SOTA baselines. We believe many appli-
cations including lifelong learning and distributed learning
can benefit from our ideas. Hence, we hope this paper opens
the door to studying more advanced compacting methods
based on frozen random networks.
Limitations: As discussed, some model parameters,
e.g., BatchNorm layers, cannot be easily reparameterized
Figure 7. Chest X-ray visualization . We compare PRANC and
JPEG on a Chest X-ray image. See Supp. for more examples.
using our method since they are calculated directly from
data rather than minimizing the task loss. In this paper,
we assumed we communicate them with no change and in-
cluded them in our budget. Lastly, PRANC is computation-
ally expensive for a large number of basis, so is currently
not suitable for training very large models.
Acknowledgement: This work was partially sup-
ported by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR00112190135 and
HR00112090023, the United States Air Force under Con-
tract No. FA8750-19-C-0098, and funding from NSF grants
1845216 and 1920079. Any opinions, findings, conclu-
sions, or recommendations expressed in this paper are those
of the authors and do not necessarily reflect the views of the
United States Air Force, DARPA, or NSF.

--- PAGE 10 ---
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 7
[2] Hessam Bagherinezhad, Mohammad Rastegari, and Ali
Farhadi. Lcnn: Lookup-based convolutional neural network.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 7120‚Äì7129, 2017. 3
[3] Johannes Ball ¬¥e, David Minnen, Saurabh Singh, Sung Jin
Hwang, and Nick Johnston. Variational image compres-
sion with a scale hyperprior. In International Conference
on Learning Representations , 2018. 3
[4] Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales.
Flexible dataset distillation: Learn labels instead of images.
arXiv preprint arXiv:2006.08572 , 2020. 3, 6
[5] George Cazenavette, Tongzhou Wang, Antonio Torralba,
Alexei A Efros, and Jun-Yan Zhu. Dataset distilla-
tion by matching training trajectories. arXiv preprint
arXiv:2203.11932 , 2022. 3, 5, 6
[6] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Wein-
berger, and Yixin Chen. Compressing neural networks with
the hashing trick. In International conference on machine
learning , pages 2285‚Äì2294. PMLR, 2015. 1, 2, 6
[7] Xiaohan Chen, Jason Zhang, and Zhangyang Wang. Peek-a-
boo: What (more) is disguised in a randomly weighted neu-
ral network, and how to find it efficiently. In International
Conference on Learning Representations , 2021. 2
[8] Yaim Cooper. Global minima of overparameterized neural
networks. SIAM Journal on Mathematics of Data Science ,
3(2):676‚Äì691, 2021. 2
[9] JCMSA Djelouah and Christopher Schroers. Content adap-
tive optimization for neural image compression. In Proceed-
ings of the CVPR , 2019. 3
[10] Emilien Dupont, Adam Goli ¬¥nski, Milad Alizadeh, Yee Whye
Teh, and Arnaud Doucet. Coin: Compression with implicit
neural representations. arXiv preprint arXiv:2103.03123 ,
2021. 3, 7
[11] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Cas-
tro, and Erich Elsen. Rigging the lottery: Making all tickets
winners. In International Conference on Machine Learning ,
pages 2943‚Äì2952. PMLR, 2020. 5
[12] Claudio Gallicchio and Simone Scardapane. Deep random-
ized neural networks. Recent Trends in Learning From Data ,
pages 43‚Äì68, 2020. 2
[13] Tiansheng Guo, Jing Wang, Ze Cui, Yihui Feng, Yunying
Ge, and Bo Bai. Variable rate image compression with con-
tent adaptive optimization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
Workshops , pages 122‚Äì123, 2020. 3
[14] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing
Xu, and Chang Xu. Ghostnet: More features from cheap
operations. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1580‚Äì
1589, 2020. 3
[15] Song Han, Huizi Mao, and William J Dally. Deep com-
pression: Compressing deep neural networks with pruning,trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149 , 2015. 3
[16] Marton Havasi, Robert Peharz, and Jos ¬¥e Miguel Hern ¬¥andez-
Lobato. Minimal random code learning: Getting bits
back from compressed model parameters. arXiv preprint
arXiv:1810.00440 , 2018. 3
[17] Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet, and
Yee Whye Teh. Robust pruning at initialization. arXiv
preprint arXiv:2002.08797 , 2020. 3
[18] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2(7), 2015. 1
[19] Berivan Isik, Tsachy Weissman, and Albert No. An
information-theoretic justification for model pruning. In In-
ternational Conference on Artificial Intelligence and Statis-
tics, pages 3821‚Äì3846. PMLR, 2022. 3, 5
[20] Woojeong Kim, Suhyun Kim, Mincheol Park, and Geun-
seok Jeon. Neuron merging: Compensating for pruned neu-
rons. Advances in Neural Information Processing Systems ,
33:585‚Äì595, 2020. 3
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 8
[22] Kodak. Kodak Dataset. http://r0k.us/graphics/kodak/, 1991.
7
[23] Aditya Kusupati, Vivek Ramanujan, Raghav Somani,
Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali
Farhadi. Soft threshold weight reparameterization for learn-
able sparsity. In Proceedings of the International Conference
on Machine Learning , July 2020. 3, 5
[24] Yann LeCun et al. Lenet-5, convolutional neural networks.
URL: http://yann. lecun. com/exdb/lenet , 20(5):14, 2015. 5,
7
[25] Jooyoung Lee, Seunghyun Cho, and Seung-Kwon Beack.
Context-adaptive entropy model for end-to-end optimized
image compression. arXiv preprint arXiv:1809.10452 , 2018.
3
[26] Junghyup Lee, Dohyung Kim, and Bumsub Ham. Network
quantization with element-wise gradient scaling. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 6448‚Äì6457, 2021. 1, 3
[27] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jin-
woo Shin. Layer-adaptive sparsity for the magnitude-based
pruning. arXiv preprint arXiv:2010.07611 , 2020. 5
[28] Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye,
Mengdi Wang, Fei Chao, Fan Yang, Jincheng Ma, Qi Tian,
and Rongrong Ji. Towards compact cnns via collaborative
compression. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 6438‚Äì
6447, 2021. 3
[29] Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and
Martin Jaggi. Dynamic model pruning with feedback. arXiv
preprint arXiv:2006.07253 , 2020. 1, 3, 5
[30] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss land-
scapes and optimization in over-parameterized non-linear
systems and neural networks. Applied and Computational
Harmonic Analysis , 59:85‚Äì116, 2022. 2

--- PAGE 11 ---
[31] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and
Ohad Shamir. Proving the lottery ticket hypothesis: Prun-
ing is all you need. In International Conference on Machine
Learning , pages 6682‚Äì6691. PMLR, 2020. 2
[32] David Minnen, Johannes Ball ¬¥e, and George D Toderici.
Joint autoregressive and hierarchical priors for learned im-
age compression. Advances in neural information processing
systems , 31, 2018. 3
[33] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann
LeCun, and Nathan Srebro. The role of over-parametrization
in generalization of neural networks. In International Con-
ference on Learning Representations , 2019. 2
[34] Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias
Hein. On the loss landscape of a class of deep neural net-
works with no bad local valleys. In International Conference
on Learning Representations , 2019. 2
[35] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kemb-
havi, Ali Farhadi, and Mohammad Rastegari. What‚Äôs hidden
in a randomly weighted neural network? In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 11893‚Äì11902, 2020. 2
[36] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,
and Ali Farhadi. Xnor-net: Imagenet classification using bi-
nary convolutional neural networks. In European conference
on computer vision , pages 525‚Äì542. Springer, 2016. 3
[37] Brandon Reagan, Udit Gupta, Bob Adolf, Michael Mitzen-
macher, Alexander Rush, Gu-Yeon Wei, and David Brooks.
Weightless: Lossy weight encoding for deep neural net-
work compression. In International Conference on Machine
Learning , pages 4324‚Äì4333. PMLR, 2018. 3
[38] Julien Niklas Siems, Aaron Klein, Cedric Archambeau, and
Maren Mahsereci. Dynamic pruning of a neural network via
gradient signal-to-noise ratio. In 8th ICML Workshop on Au-
tomated Machine Learning (AutoML) , 2021. 3
[39] Vincent Sitzmann, Julien Martel, Alexander Bergman, David
Lindell, and Gordon Wetzstein. Implicit neural representa-
tions with periodic activation functions. Advances in Neural
Information Processing Systems , 33:7462‚Äì7473, 2020. 7
[40] Christopher Subia-Waud and Srinandan Dasmahapatra.
Weight fixing networks. In European Conference on Com-
puter Vision , pages 415‚Äì431. Springer, 2022. 1, 2
[41] Ilia Sucholutsky and Matthias Schonlau. Soft-label dataset
distillation and text dataset distillation. In 2021 International
Joint Conference on Neural Networks (IJCNN) , pages 1‚Äì8.
IEEE, 2021. 3, 6
[42] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features
let networks learn high frequency functions in low dimen-
sional domains. Advances in Neural Information Processing
Systems , 33:7537‚Äì7547, 2020. 5, 7
[43] Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, and
Deepak K Gupta. Chipnet: Budget-aware pruning with
heaviside continuous approximations. arXiv preprint
arXiv:2102.07156 , 2021. 3
[44] George Toderici, Wenzhe Shi, Radu Timofte, Lucas Theis,
Johannes Balle, Eirikur Agustsson, Nick Johnston, andFabian Mentzer. Workshop and challenge on learned image
compression (clic2020), 2020. 7
[45] Karen Ullrich, Edward Meeds, and Max Welling. Soft
weight-sharing for neural network compression. arXiv
preprint arXiv:1702.04008 , 2017. 3
[46] Huan Wang, Can Qin, Yulun Zhang, and Yun Fu. Neu-
ral pruning via growing regularization. arXiv preprint
arXiv:2012.09243 , 2020. 3
[47] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang,
Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and
Yang You. Cafe: Learning to condense dataset by aligning
features. arXiv preprint arXiv:2203.01531 , 2022. 6
[48] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and
Alexei A Efros. Dataset distillation. arXiv preprint
arXiv:1811.10959 , 2018. 1, 3, 5, 6
[49] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mo-
hammadhadi Bagheri, and Ronald M Summers. Chestx-
ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common
thorax diseases. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 2097‚Äì2106,
2017. 7
[50] Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu,
Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosin-
ski, and Ali Farhadi. Supermasks in superposition. Advances
in Neural Information Processing Systems , 33:15173‚Äì15184,
2020. 2
[51] Bo Zhao and Hakan Bilen. Dataset condensation with differ-
entiable siamese augmentation. In International Conference
on Machine Learning , pages 12674‚Äì12685. PMLR, 2021. 3,
6
[52] Bo Zhao and Hakan Bilen. Dataset condensation with distri-
bution matching. arXiv preprint arXiv:2110.04181 , 2021. 3,
6
[53] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset
condensation with gradient matching. arXiv preprint
arXiv:2006.05929 , 2020. 6
Appendix
Orthogonality and norm of basis networks:
In the main submission, we mentioned that random ba-
sis networks are almost perpendicular to each other in high
dimensional spaces. To show this, we generate 1000 ran-
dom vectors at the ddimensional space (varying dfrom
10 to 1000), and plot the histogram of their ‚Ñì2norm and
pairwise cosine similarities in Figures 8 and 9 respectively.
We also run this experiment 100 times, calculate the maxi-
mum cosine similarity for each run, and plot the histogram
of maximum values in Figure 10. As expected, at higher
number of dimensions, the cosine similarity gets closer to
0 and the norm gets closer to 1. This empirically suggests
that at higher dimensions, the random basis are close to or-
thonormal basis. Please note that our method does not re-
quire orthonormal basis to work.

--- PAGE 12 ---
Figure 8. Histogram of ‚Ñì2norm of 1000 randomly generated ddimensional vectors. When increasing dthe norm approaches 1.
Figure 9. Histogram of pairwise cosine similarity of 1000 randomly generated ddimensional vectors. When increasing dthe cosine
similarity approaches 0.
Reconstruction using a subset of basis:
Figure 12 shows the distribution of alpha values for a few
images from Kodak dataset. As expected, the alpha values
vary across the basis models. This motivated us to study
‚Äúwhat if we use only a subset of basis models instead of all
of them?‚Äù.
For image classification, we do this experiment by se-
lecting random p%of the basis with varying p. We repeat
this 4 times. As another selection method, we sort alphavalues based on their absolute values and use the top p%of
them. As shown in Figure 11, in the random selection, we
need most of the basis to be able to retrieve a reasonable ac-
curacy while for the sorted case, a small percentage of the
basis models is sufficient to get a reasonable accuracy. This
is an interesting observation that somehow loosely suggests
that the loss landscape of the optimization for alpha values
is reasonably smooth. This is intentionally a vague state-
ment since it needs further investigation as future work.
For image compression, we perform the same experi-

--- PAGE 13 ---
Figure 10. Histogram of maximum of pairwise cosine similarity of 1000 randomly generated ddimensional vectors over 100 trials. When
increasing d, the maximum of cosine similarity approaches 0.
ment and show reasonable reconstructed images with a sub-
set of alpha values with the largest absolute values. Since
we have a different set of alphas for each layer of MLP in
image compression, we sort absolute values and select top
p%of each layer with higher values. We vary pand visual-
ize the reconstructed images for each pvalue in Figure 14,
15 and 16. Additionally, we report the effect of pin PSNR
and MS-SSIM for Kodak dataset in Table 9. Similar to our
observation in mage classification, we observe that images
withp= 70% has acceptable quality.
Aside from better understanding the method, this obser-
vation can enable communicating a deep model or an image
progressively where the sender sends a subset of alpha val-
ues (most important ones) first and then gradually sends the
rest to improve the model accuracy or the image quality. For
image compression, this method is somehow similar to the
application of Progressive JPEG where it is hand-crafted to
send the low frequency components first. Please note that to
use this in practice, this progressive version of our method
has some extra-overhead since we also need to communi-
cate which alpha values are sent at each step (e.g., using a
bit for each basis). Studying this in more detail is left for
future work.
Details of Image Compression:
As described in the main submission, in image compres-
sion experiments, in order to change the bit-per-pixel (bpp),
we fix the network architecture and vary the number of Œ±
values per layer. We report the number of Œ±values per layer
for each bpp in Table 10.Compression to Advanced Image Compression
methods:
We compare our method with more advanced codecs
(e.g., BPG, VTM) and learned-based image compression
methods. Results for CLIC-Mobile are in Table 11 and
the results for Kodak are in Figure 13. Note that advanced
codecs are heavily hand-crafted by a large community. And,
learned-based methods utilize a training dataset to learn a
good code (similar to auto-encoder), hence, they may not
be able to compress a single image without having access
to a corpus of images. In contrast, our method can com-
press a single image without using a population of images.
Moreover, for the same reason, our method cannot be biased
towards the popular cases (head of distribution). Obviously,
our method has other biases (e.g., what can be represented
with INR) that needs to be studied as the future work.
Visual Comparison to JPEG:
Similar to Figure 5 of main submission, we include more
visual comparison to JPEG in Figures 17 through 20 (for
Kodak dataset with 512√ó768resolution) and Figures 21
and 22 (for CLIC-Mobile dataset with 1512√ó2016 or
2016√ó1512 resolution). Moreover, in Figures 23 through
25, we include visual comparison in chest x-ray dataset with
1024√ó1024 resolution.

--- PAGE 14 ---
Figure 11. Effect of using only p%of basis models selected randomly (4 times) or selected based on highest absolute values of alphas.
Figure 12. Distribution of alphas: We plot the distribution of alpha values for a few Kodak images.
Table 9. Effect of keeping p%ofŒ±with highest absolute value:
percentile ( p%) 10 20 30 40 50 60 70 80 90 100
bpp 0.07 0.14 0.22 0.29 0.36 0.43 0.50 0.58 0.65 0.72
PSNR 11.94 12.3 13.4 14.98 17.01 19.51 22.71 26.92 31.85 33.64
MS-SSIM 0.10 0.12 0.18 0.28 0.41 0.55 0.71 0.86 0.96 0.97
Table 10. Details of image compression models: We report the number of Œ±values per layer for each bpp. We use MLP with hidden
dimension of 256for Kodak dataset at bpp of 0.18(first row), and hidden dimension of 512for all other experiments. All settings use
Fourier mapping of size 512. We use fewer number of alpha values for the last layer since the last layer has fewer number of weights as
it goes from hidden layer to only 3 dimensions (RGB values). Also, for the first row, we use more number of alpha values for first layer
since it has more number of weights ( 512√ó256).
Dataset bpp Layer-1 Layer-2 Layer-3 Layer-4 Layer-5 Total ( Œ±s)
Kodak0.18 10k 7.5k 7.5k 7.5k 2k 34.5k
0.31 15k 15k 15k 15k 2k 57k
0.52 10k 30k 30k 30k 2k 102k
0.72 20k 40k 40k 40k 2k 142k
CLIC-Mobile 0.12 45k 45k 45k 45k 2k 182k
Chest x-ray 0.15 20k 20k 20k 20k 2k 82k

--- PAGE 15 ---
Table 11. CLIC-mobile Dataset Image Compression: Similar to Table 6 of the main submission, we include comparison to advanced
codecs like BPG and VTM. We also compare with some learning-based image compression methods (e.g., MBT, CST, BMS).
Model bpp PSNR MS-SSIM
VTM 0.183 33.07 0.964
CST 0.146 31.85 0.957
MBT 0.146 31.62 0.955
BPG 0.128 30.65 0.942
WebP 0.185 30.07 0.940
BMS 0.113 29.38 0.936
JPEG2000 0.126 29.40 0.918
JPEG 0.195 24.82 0.836
Trained INR 0.125 26.93 0.864
PRANC (ours) 0.119 29.71 0.920
Figure 13. Kodak Dataset Image Compression: Similar to Figure 4 of the main submission, we include comparison to advanced codecs
like BPG and VTM. We also compared with learned-based image compression (e.g., MBT, CST, BMS).

--- PAGE 16 ---
Figure 14. Effect of keeping p%of basis models with the highest absolute alpha values. We get reasonable images with smaller subset
of basis models.
Figure 15. See Figure 14.

--- PAGE 17 ---
Figure 16. See Figure 14.

--- PAGE 18 ---
Original image
Ours (bpp=0.31, PSNR=30.62)
Ours (bpp=0.17, PSNR=28.18)JPEG (bpp=0.31, PSNR=28.68)
JPEG (bpp=0.17, PSNR=22.47)Figure 17. Kodak visualization . We compare PRANC and JPEG on image 4 of Kodak dataset at bpp=0.31 and 0.17

--- PAGE 19 ---
Original image
Ours (bpp=0.31, PSNR=31.96)
Ours (bpp=0.17, PSNR=29.23)JPEG (bpp=0.32, PSNR=28.58)
JPEG (bpp=0.17, PSNR=23.21)Figure 18. Kodak visualization . We compare PRANC and JPEG on image 17 of Kodak dataset.

--- PAGE 20 ---
Original image
Ours (bpp=0.31, PSNR=25.57)
Ours (bpp=0.17, PSNR=23.3)JPEG (bpp=0.31, PSNR=21.34)
JPEG (bpp=0.21, PSNR=19.42)Figure 19. Kodak visualization . We compare PRANC and JPEG on image 5 of Kodak dataset.

--- PAGE 21 ---
Original image
Ours (bpp=0.31, PSNR=31.41)
Ours (bpp=0.17, PSNR=27.88)JPEG (bpp=0.32, PSNR=31.16)
JPEG (bpp=0.17, PSNR=23.64)Figure 20. Kodak visualization . We compare PRANC and JPEG on image 23 of Kodak dataset.

--- PAGE 22 ---
Original image
Ours (bpp=0.119, PSNR=30.26)JPEG (bpp=0.18, PSNR=25.43)Figure 21. CLIC visualization . We compare PRANC at bpp=0.119 with JPEG at bpp=0.18 on a CLIC image.

--- PAGE 23 ---
Original image
Ours (bpp=0.119, PSNR=28.99)
JPEG (bpp=0.226, PSNR=24.59)Figure 22. CLIC visualization . We compare PRANC at bpp=0.119 with JPEG at bpp=0.226 on a CLIC image.

--- PAGE 24 ---
Original image
Ours (bpp=0.15, PSNR=36.06)JPEG (bpp=0.15, PSNR=32.11)
Figure 23. Chest X-ray visualization . We compare PRANC and JPEG on a Chest X-ray image at bpp=0.15

--- PAGE 25 ---
Original image
Ours (bpp=0.15, PSNR=36.24)JPEG (bpp=0.15, PSNR=32.55)Figure 24. See Figure 23.

--- PAGE 26 ---
Original image
Ours (bpp=0.15, PSNR=35.54)JPEG (bpp=0.15, PSNR=32.20)Figure 25. See Figure 23.
