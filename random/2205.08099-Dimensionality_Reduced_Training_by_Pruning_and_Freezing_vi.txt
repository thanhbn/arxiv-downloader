# Giảm Chiều Trong Huấn Luyện Bằng Cách Cắt Tỉa và Đóng Băng
Các Phần của Mạng Nơ-ron Sâu, Một Khảo Sát✩
Paul Wimmera,b,∗, Jens Mehnerta, Alexandru Paul Condurachea,b
a1Robert Bosch GmbH, Automated Driving Research, Burgenlandstrasse 44, 70469 Stuttgart, Germany
bUniversity of L ¨ubeck, Institute for Signal Processing, Ratzeburger Allee 160, 23562 L ¨ubeck, Germany

## Tóm tắt
Các mô hình học sâu tiên tiến có số lượng tham số lên đến hàng tỷ. Việc huấn luyện, lưu trữ và truyền tải các mô hình như vậy tốn nhiều năng lượng và thời gian, do đó tốn kém. Một phần lớn chi phí này được gây ra bởi việc huấn luyện mạng. Nén mô hình giảm chi phí lưu trữ và truyền tải, và có thể làm cho việc huấn luyện hiệu quả hơn bằng cách giảm số lượng tính toán trong quá trình truyền xuôi và/hoặc truyền ngược. Do đó, việc nén mạng cũng trong thời gian huấn luyện trong khi duy trì hiệu suất cao là một chủ đề nghiên cứu quan trọng. Công trình này là một khảo sát về các phương pháp giảm số lượng trọng số được huấn luyện trong các mô hình học sâu trong suốt quá trình huấn luyện. Hầu hết các phương pháp được giới thiệu đặt các tham số mạng về không, được gọi là cắt tỉa. Các phương pháp cắt tỉa được trình bày được phân loại thành cắt tỉa tại khởi tạo, vé số may mắn và huấn luyện thưa thớt động. Hơn nữa, chúng tôi thảo luận các phương pháp đóng băng các phần của mạng tại khởi tạo ngẫu nhiên của nó. Bằng cách đóng băng trọng số, số lượng tham số có thể huấn luyện được thu hẹp, điều này giảm tính toán gradient và chiều của không gian tối ưu hóa của mô hình. Trong khảo sát này, trước tiên chúng tôi đề xuất huấn luyện giảm chiều như một mô hình toán học cơ bản bao gồm cắt tỉa và đóng băng trong quá trình huấn luyện. Sau đó, chúng tôi trình bày và thảo luận các phương pháp huấn luyện giảm chiều khác nhau.

**Từ khóa:** Khảo sát, Cắt tỉa, Đóng băng, Giả thuyết Vé số May mắn, Huấn luyện Thưa thớt Động, Cắt tỉa tại Khởi tạo

## 1. Giới thiệu
Trong những năm gần đây, các mạng nơ-ron sâu (DNN) đã cho thấy hiệu suất tiên tiến (SOTA) trong nhiều ứng dụng trí tuệ nhân tạo, như phân loại hình ảnh [1], nhận dạng giọng nói [2] hoặc phát hiện đối tượng [3]. Các ứng dụng này yêu cầu tối ưu hóa các mô hình lớn với hàng tỷ tham số. Việc huấn luyện và kiểm tra các mô hình lớn như vậy đã trở thành khả thi nhờ những tiến bộ công nghệ. Kết quả là, các mô hình SOTA được huấn luyện trên phần cứng chuyên dụng cho tính toán tensor nhanh, như GPU hoặc TPU. Thông thường, không chỉ một GPU/TPU được sử dụng để huấn luyện các mô hình này mà nhiều thiết bị. Ví dụ, XLNet [4] cần 5,5 ngày huấn luyện trên 512 chip TPU v3.

Không chỉ việc huấn luyện, mà việc truyền tải, lưu trữ và đánh giá các mô hình lớn cũng tốn kém [5]. Để huấn luyện các mô hình SOTA, cần một lượng lớn dữ liệu [6–9] đòi hỏi tài nguyên để thu thập, gán nhãn, lưu trữ và truyền tải. Tất nhiên, số lượng lớn tham số và dữ liệu, cùng với nhu cầu tính toán cao dẫn đến tiêu thụ năng lượng quá mức để huấn luyện và đánh giá các mô hình học sâu (DL). Ví dụ, việc huấn luyện một transformer lớn kết hợp với tìm kiếm kiến trúc mạng nơ-ron được thực hiện trước đó dẫn đến phát thải 284tCO2 [10]. Điều này bằng khoảng 315 lần lượng phát thải CO2 của một hành khách đi máy bay từ Thành phố New York đến San Francisco.

Giữa năm 2012 và 2018, các tính toán cần thiết cho nghiên cứu DL đã tăng ước tính 300.000 lần, tương ứng với việc tăng gấp đôi lượng tính toán sau mỗi vài tháng [5], xem Hình 1. Tốc độ này vượt xa tốc độ được dự đoán bởi Định luật Moore [12]. Cải thiện hiệu suất của các mô hình DL phần lớn được tạo ra bằng cách tăng số lượng tham số trong mạng và/hoặc tăng số lượng tính toán cần thiết để huấn luyện và suy luận mạng [10, 13].

Song song với việc phát triển các mô hình SOTA mới, lớn cần lượng lớn dữ liệu, tài nguyên phần cứng và thời gian huấn luyện, việc cải thiện đồng thời hiệu quả tham số, tính toán, năng lượng và dữ liệu của các mô hình DL là điều quan trọng.

Nén mô hình giảm chi phí lưu trữ và truyền tải, tăng tốc suy luận bằng cách giảm số lượng tính toán hoặc tăng tốc việc huấn luyện sử dụng ít năng lượng hơn. Nó có thể đạt được bằng các phương pháp như lượng tử hóa, chia sẻ trọng số, phân tách tensor, xấp xỉ tensor hạng thấp, cắt tỉa hoặc đóng băng. Lượng tử hóa giảm số bit được sử dụng để biểu diễn trọng số và/hoặc bản đồ kích hoạt của mạng [14–20]. Số thực 32bit được thay thế bằng số nguyên độ chính xác thấp, từ đó giảm tiêu thụ bộ nhớ và tăng tốc suy luận. Giảm bộ nhớ và tăng tốc cũng có thể đạt được bằng chia sẻ trọng số [21–23], phân tách tensor [24–26] hoặc xấp xỉ tensor hạng thấp [27–29] để chỉ nêu một vài phương pháp. Cắt tỉa mạng nơ-ron [14, 30–35] đặt các phần trọng số của DNN về không. Điều này có thể giúp giảm độ phức tạp và yêu cầu bộ nhớ của mô hình, tăng tốc suy luận [36] và thậm chí cải thiện khả năng tổng quát hóa của mạng [33, 37–39].

Cắt tỉa DNN có thể được chia thành cắt tỉa có cấu trúc và không có cấu trúc. Cắt tỉa có cấu trúc loại bỏ các kênh, nơ-ron hoặc thậm chí các cấu trúc thô hơn của mạng [40–46]. Điều này tạo ra các kiến trúc gọn hơn, dẫn đến giảm thời gian tính toán. Một phương pháp tinh tế hơn được đưa ra bởi cắt tỉa không có cấu trúc, trong đó các trọng số đơn lẻ được đặt về không [30–33, 47–50]. Cắt tỉa không có cấu trúc thường dẫn đến hiệu suất tốt hơn so với cắt tỉa có cấu trúc [51, 52] nhưng cần phần mềm và phần cứng chuyên dụng hỗ trợ tính toán tensor thưa thớt để thực sự giảm thời gian chạy [53, 54]. Để có cái nhìn tổng quan về các phương pháp cắt tỉa SOTA hiện tại, chúng tôi tham khảo Blalock et al. [36].

Đóng băng DNN có nghĩa là chỉ các phần của mạng được huấn luyện, trong khi các phần còn lại được đóng băng tại giá trị ban đầu/được huấn luyện trước của chúng [55–59]. Điều này dẫn đến hội tụ nhanh hơn của mạng [55] và giảm chi phí truyền thông cho huấn luyện phân tán [59]. Hơn nữa, đóng băng giảm yêu cầu bộ nhớ nếu các mạng được khởi tạo bằng số giả ngẫu nhiên [58].

Trong những năm gần đây, việc huấn luyện chỉ một phần thưa thớt của mạng, tức là một mạng với nhiều trọng số được cố định tại không hoặc giá trị được khởi tạo ngẫu nhiên của chúng, đã trở thành mối quan tâm của cộng đồng DL [47–50, 58, 60–62], mang lại lợi ích giảm yêu cầu bộ nhớ và tiềm năng giảm thời gian chạy không chỉ cho suy luận mà còn cho huấn luyện. Diffenderfer và Kailkhura [63] thậm chí cho thấy các mô hình được huấn luyện thưa thớt mạnh mẽ hơn chống lại sự thay đổi phân phối so với (i) đối tác được huấn luyện dày đặc của chúng và (ii) các mạng được cắt tỉa bằng kỹ thuật cổ điển. Với kỹ thuật cổ điển, chúng tôi biểu thị việc cắt tỉa trong hoặc sau huấn luyện, trong đó mạng thưa thớt được tinh chỉnh sau đó. Một cái nhìn tổng quan cấp cao về các phương pháp gần đây để cắt tỉa mạng tại khởi tạo được đưa ra trong Wang et al. [64]. Trong công trình của chúng tôi, khung toán học cơ bản được tổng quát hóa để bao gồm các phương pháp huấn luyện giảm chiều khác. Hơn nữa, công trình của chúng tôi thảo luận các phương pháp được phân tích chi tiết hơn.

**Phạm vi của công trình này.** Như đã đề cập ở trên, có nhiều khả năng để giảm chi phí của một khung DL. Trong công trình này, chúng tôi tập trung vào các phương pháp giảm số lượng tham số có thể huấn luyện trong một mô hình DL. Để làm điều này, chúng tôi định nghĩa ngược giảm chiều bằng cách nhúng một không gian chiều thấp vào một không gian chiều cao, xem Phần 2.2. Không gian chiều cao tương ứng với DNN gốc, trong khi không gian chiều thấp là không gian mà DNN thực sự được huấn luyện. Do đó, việc huấn luyện tiến hành với chiều giảm.

Cắt tỉa và đóng băng các phần của mạng trong quá trình huấn luyện là hai phương pháp mang lại huấn luyện giảm chiều (DRT). Khảo sát này so sánh các chiến lược khác nhau để cắt tỉa và đóng băng mạng trong quá trình huấn luyện. Hình 2 cho thấy cái nhìn tổng quan đồ họa về các phương pháp DRT được đề xuất. Một so sánh cấu trúc giữa đóng băng DNN tại khởi tạo và cắt tỉa chúng được đưa ra trong Hình 3. Đối với cắt tỉa, chúng tôi phân biệt giữa cắt tỉa tại khởi tạo (PaI) huấn luyện một tập tham số cố định trong mạng và huấn luyện thưa thớt động (DST) điều chỉnh tập tham số có thể huấn luyện trong quá trình huấn luyện. Liên quan chặt chẽ đến PaI là cái gọi là Giả thuyết Vé số May mắn (LTH) sử dụng các mạng con thưa thớt có thể huấn luyện tốt, gọi là Vé số May mắn (LT). LT được thu được bằng cách áp dụng các chu trình huấn luyện-cắt tỉa-đặt lại cho các tham số của mạng, bắt đầu với mạng dày đặc và cuối cùng tạo ra mạng con tại khởi tạo với độ thưa thớt mong muốn.

**Cấu trúc của công trình này.** Trước tiên chúng tôi đề xuất công thức bài toán và thiết lập toán học của khảo sát này trong Phần 2. Sau đó, chúng tôi thảo luận các khả năng khác nhau để giảm chiều của mạng trong Phần 3. LTH và PaI được giới thiệu trong Phần 4 và 5, tương ứng. Điều này được theo sau bởi so sánh các phương pháp DST khác nhau trong Phần 6. Là phương pháp DRT cuối cùng, chúng tôi trình bày đóng băng tham số ban đầu trong Phần 7. Sau đó, các phương pháp DRT khác nhau từ Phần 4 - 7 được so sánh ở cấp độ cao trong Phần 8. Khảo sát được kết thúc bằng việc rút ra kết luận trong Phần 9.

Để dễ đọc hơn, các Hình nên được in màu hoặc xem trong phiên bản trực tuyến có màu.

## 2. Công thức bài toán

Chúng tôi sẽ bắt đầu Phần 2 với phần giới thiệu chung về DL. Điều này được theo sau bởi việc định nghĩa một khung toán học mô tả DRT. Khung này bao gồm PaI, LTH, DST và đóng băng các phần của DNN được khởi tạo ngẫu nhiên.

### 2.1. Học sâu tổng quát và thiết lập

Cho fΘ: Rm → Rn định nghĩa một DNN với trọng số được vector hóa Θ ∈ RD ≅ RD1 × ... × RDL. Ở đây, Dl biểu thị số lượng tham số trong lớp l của DNN với L lớp và D = ∑L l=1 Dl tham số tổng cộng. Chúng tôi cũng giả định cấu trúc mạng toàn cục (hàm kích hoạt, thứ tự các lớp, thứ tự trọng số trong một lớp, ...) được cố định và được mã hóa trong fΘ, tức là chỉ các giá trị trọng số Θ có thể được thay đổi.

Chúng tôi giả định một huấn luyện DNN tiêu chuẩn, bắt đầu với trọng số ban đầu ngẫu nhiên Θ(0) ∈ RD. Xem ví dụ He et al. [65], Glorot và Bengio [66], Saxe et al. [67], Martens [68], Sutskever et al. [69], Hanin và Rolnick [70] cho các khả năng khởi tạo ngẫu nhiên DNN. Các trọng số ban đầu này được cập nhật lặp đi lặp lại với tối ưu hóa dựa trên gradient như SGD [71], AdaGrad [72], Adam [73] hoặc AdamW [74] để tối thiểu hóa hàm mất mát

L: RD × Rn × Rm → R, (Θ, X, Y) ↦ L(fΘ, X, Y)                    (1)

trên một tập dữ liệu huấn luyện D = (X,Y) ⊂ Rn × Rm.¹ Sau khởi tạo, một mô hình được huấn luyện trong T lần lặp tạo thành một dãy tham số mô hình {Θ(0), Θ(1), ..., Θ(T)}. Cập nhật tham số Θ(t) trong lần lặp t được thực hiện bằng cách tính gradient hiện tại

∇Θ(t)L = ∑B b=1 ∂L(fΘ(t), X(t)b, Y(t)b)/∂Θ(t),                    (2)

¹Học không giám sát có thể được mô hình hóa bằng cách đặt Y = ∅.

và sử dụng nó, có thể cùng với các gradient trước đó ∇Θ(0)L, ..., ∇Θ(t-1)L và các giá trị tham số Θ(0), ..., Θ(t-1), để tối thiểu hóa hàm mất mát hơn nữa. Ở đây, một batch dữ liệu huấn luyện với kích thước batch B được cho bởi {(X(t)b, Y(t)b) : b = 1, ..., B} ⊂ D. Các batch thay đổi cho các bước huấn luyện t khác nhau.

Mô hình cuối cùng sau đó được cho bởi fΘ(T). Lưu ý, mục tiêu huấn luyện tổng thể là để mô hình tổng quát hóa tốt trên dữ liệu chưa thấy. Khả năng tổng quát hóa thường được đo trên một tập dữ liệu kiểm tra riêng biệt, được giữ lại. Do đó, các phương pháp chính quy hóa như suy giảm trọng số [75], dropout [76–78], chuẩn hóa batch [79] hoặc dừng sớm [80, 81] được sử dụng để ngăn fΘ(T) overfit trên dữ liệu huấn luyện. Tổng quát hóa cũng có thể được cải thiện bằng cách cho phép mô hình sử dụng kiến thức tiên nghiệm hình học về cảnh [82–86], dịch chuyển mô hình trở lại khu vực mà nó tổng quát hóa tốt [87–90] nhưng cũng bằng cách cắt tỉa mạng [33, 38, 91].

### 2.2. Mô hình cho huấn luyện giảm chiều

Một sự giảm chiều cho Θ(t) tại bước huấn luyện t sẽ được mô hình hóa thông qua một phép biến đổi, cũng được gọi là nhúng, Ψ(t): Rd → RD, Θ(t) = Ψ(t)(ϑ(t)) và d ≪ D.² Trong công trình này, chúng tôi hạn chế Ψ(t) để tạo thành một phép biến đổi tuyến tính affine bao gồm tất cả các phương pháp cắt tỉa/đóng băng tiêu chuẩn. Tuy nhiên, Ψ(t) có các tham số cần được lưu trữ. Do đó, chúng tôi không chỉ giả định d ≪ D mà còn size(Ψ(t)) + d ≪ D, trong đó size(·) tính số lượng tham số tối thiểu cần thiết để biểu diễn phép biến đổi tuyến tính affine Ψ(t). Để mô hình hóa DST, Ψ(t) được phép thay đổi trong quá trình huấn luyện.

Trong thiết lập của chúng tôi, số lượng tham số trong mạng được giảm không chỉ sau huấn luyện mà còn trong quá trình đó. Do đó, các tham số có thể huấn luyện của mạng được cho bởi ϑ(t) ∈ Rd. Tất cả các phương pháp được thảo luận trong công trình này được hạn chế để thỏa mãn ϑ(t) ∈ Rd cho tất cả các lần lặp huấn luyện t ∈ {1, ..., T}, tức là giảm chiều trong suốt quá trình huấn luyện. Mô hình tương ứng với chiều giảm là fΨ(t)(ϑ(t)).

#### 2.2.1. Cắt tỉa và huấn luyện thưa thớt động

Đối với cắt tỉa, Ψ(t) mã hóa vị trí của các phần tử khác không trong khi ϑ(t) lưu trữ giá trị của những tham số đó. Một Ψ(t) tương ứng có thể được xây dựng dễ dàng bằng cách đặt

Ψ(t)(ϑ(t)) = ψ(t) · ϑ(t),                    (3)

với nhúng cắt tỉa ψ(t) ∈ RD×d được định nghĩa qua

ψ(t)i,j = {1, nếu Θ(t)i là phần tử thứ j chưa bị cắt tỉa.
          {0, ngược lại,                    (4)

trong đó chúng tôi giả định d phần tử chưa bị cắt tỉa Θ(t)i(t)1, ..., Θ(t)i(t)d có thứ tự tự nhiên i(t)1 < ... < i(t)d. Do đó, phiên bản bị cắt tỉa của Θ(t) được mã hóa bởi chiều thấp

ϑ(t) = (ϑ(t)j)j = (Θ(t)i(t)j)j.                    (5)

²Chúng tôi sử dụng cùng phép biến đổi như đã giới thiệu trong Mostafa và Wang [92] để nhúng Rd vào RD.

Hơn nữa, chúng tôi định nghĩa p := 1 - d/D là tỷ lệ cắt tỉa của mô hình. Đối với cắt tỉa tại khởi tạo, tức là vị trí cố định i(0)1, ..., i(0)d cho các tham số khác không, ψ(t) = ψ(0) cho tất cả các lần lặp huấn luyện t. Trong khi đó, ψ(t) có thể thích ứng cho DST.

Vì d ≪ D, nhúng Ψ(t)(ϑ(t)) tự động thưa thớt trong không gian lớn hơn RD. Một khả năng để vượt qua Θ(t) thưa thớt trong khi vẫn huấn luyện chỉ một phần nhỏ ϑ(t) của DNN được đề xuất trong Wimmer et al. [62]. Ở đây, các bộ lọc tích chập K×K được biểu diễn qua một vài hệ số khác không của một từ điển thích ứng. Từ điển được chia sẻ trên một hoặc nhiều lớp của mạng. Quy trình này có thể được mô tả bởi nhúng thích ứng

Ψ(t) = Φ(t) · ψ(t)                    (6)

với nhúng cắt tỉa ψ(t) ∈ RD×d như được định nghĩa trong (4) và từ điển có thể huấn luyện Φ(t) ∈ RD×D.³ Trong trường hợp này, các tọa độ w.r.t. Φ(t) là thưa thớt, nhưng biểu diễn kết quả trong miền không gian RD sẽ dày đặc [62].

Trực giao với phương pháp đó, Price và Tanner [93] kết hợp một nhúng cắt tỉa thưa thớt (3) với một phép biến đổi cosin rời rạc (DCT) dày đặc bằng cách cộng chúng lại. DCT tự do lưu trữ và có thể được tính với D·logD FLOP. Bằng cách làm như vậy, mạng giữ luồng thông tin cao trong khi chỉ một phần thưa thớt của mạng phải được huấn luyện. Hơn nữa, chi phí tính toán gần như bằng một mô hình hoàn toàn thưa thớt.

#### 2.2.2. Đóng băng tham số

Một phương pháp khác, tương tự như [93], được đề xuất bởi Rosenfeld và Tsotsos [57], Wimmer et al. [58], Sung et al. [59], sử dụng trọng số đóng băng trên đỉnh những trọng số có thể huấn luyện. Phép biến đổi kết quả được cho bởi

Ψ(t)(ϑ(t)) = ψ(t) · ϑ(t) + χ(t) · Θ(0)                    (7)

với nhúng cắt tỉa ψ(t) ∈ RD×d từ (4), χ(t) = diag(χ(t)1, ..., χ(t)D) ∈ RD×D được định nghĩa là

χ(t)i = {0, nếu i ∈ {i(t)1, ..., i(t)d}
        {1, ngược lại,                    (8)

và khởi tạo ngẫu nhiên Θ(0) ∈ RD. Một cái nhìn tổng quan đồ họa về đóng băng các phần của mạng được đưa ra trong Hình 4. Tương tự như cắt tỉa, chúng tôi định nghĩa p := 1 - d/D là tỷ lệ đóng băng của mô hình, tức là tỷ lệ tham số được đóng băng tại giá trị ban đầu ngẫu nhiên của chúng. Lưu ý, Rosenfeld và Tsotsos [57] và Wimmer et al. [58] đều sử dụng một tập trọng số chưa đóng băng cố định, tức là Ψ(t) = Ψ(0). Do đó, các tham số mạng Θ(t) = Ψ(0)(ϑ(t)) bao gồm hai phần, trọng số chưa đóng băng động được định nghĩa bởi ψ(0) · ϑ(t) và trọng số đóng băng cố định χ(0) · Θ(0). Sung et al. [59] cho phép nhúng Ψ(t) thích ứng trong quá trình huấn luyện, tuy nhiên quy trình của họ vẫn để lại một phần lớn của mạng hoàn toàn không được chạm vào.

Bằng cách đặt χ(t) = 0, (3) chỉ là một trường hợp đặc biệt của (7), vì cắt tỉa tự nhiên giống như đóng băng trọng số chưa huấn luyện tại không. Cùng phép biến đổi (7), nhưng hạn chế trên đóng băng toàn bộ lớp hoặc thậm chí toàn bộ mạng, được sử dụng cho máy học cực đoan [94, 95] hoặc các công trình khác như Hoffer et al. [56], Saxe et al. [96], Giryes et al. [97].

Một cấu trúc khác của Θ(t) được đưa ra bằng cách trao đổi nhúng cắt tỉa ψ(t) trong (7) với một phép biến đổi trực giao ngẫu nhiên cố định ψo ∈ RD×d [98].

### 2.3. Lưu trữ nhúng Ψ(t)

Trong thực tế, các kỹ thuật lưu trữ như định dạng hàng thưa thớt nén [99] đảm nhiệm vai trò của nhúng cắt tỉa ψ(t). Nếu ψ(t) được biết, χ(t) có thể được xây dựng dễ dàng bằng cách sử dụng phương trình (8). Hơn nữa, bằng cách sử dụng các bộ tạo số giả ngẫu nhiên cho khởi tạo, Θ(0) cũng có thể được khôi phục bằng cách chỉ biết một số hạt giống ngẫu nhiên duy nhất. Trong trường hợp này, lưu trữ Ψ(t) có chi phí lên đến 32bit giống như lưu trữ ψ(t). Mặt khác, nếu một phép biến đổi đã học bổ sung Φ(t) cũng được sử dụng, so sánh (6), phép biến đổi tương ứng phải được lưu trữ – đây là lý do tại sao Wimmer et al. [62] chia sẻ nhiều phần tử trong Φ(t).

## 3. Cái nhìn tổng quan cấp cao về các phép biến đổi giảm chiều Ψ(t)

Trong Phần này, chúng tôi đưa ra cái nhìn tổng quan cấp cao về các phương pháp khác nhau để xác định phép biến đổi giảm chiều Ψ(t). Trước tiên, chúng tôi thảo luận cấu trúc của các tham số được cắt tỉa hoặc đóng băng trong Phần 3.1. Sau đó, chúng tôi so sánh giảm chiều toàn cục và theo lớp trong Phần 3.2. Phần 3.3 bao gồm tần suất cập nhật cho Ψ(t). Sau đó, các tiêu chí nổi bật nhất để xác định nhúng cắt tỉa ψ(t) được đề xuất trong Phần 3.4. Cuối cùng, chúng tôi thảo luận việc huấn luyện trước phép biến đổi Ψ(t) trong Phần 3.5.

Trong suốt Phần này, chúng tôi hạn chế phép biến đổi Ψ(t) để sử dụng ψ(t) từ (4), tức là một nhúng tuyến tính thưa thớt ψ(t) ∈ {0,1}D×d với ||ψ(t)||0 = d, như phần tuyến tính. Hơn nữa, chúng tôi giả định phần affine của Ψ(t) là hoặc không (cắt tỉa) hoặc tương ứng với khởi tạo giả ngẫu nhiên gần như miễn phí lưu trữ (đóng băng). Chúng tôi sử dụng khái niệm trọng số có thể huấn luyện (trong lần lặp t) cho tất cả trọng số Θ(t)i với i ∈ {i(t)1, ..., i(t)d}, tức là các phần tử của

{i : ∃j s.t. ψ(t)i,j ≠ 0}.                    (9)

### 3.1. Cấu trúc của trọng số có thể huấn luyện

Cắt tỉa thường được phân biệt thành cắt tỉa có cấu trúc và không có cấu trúc, xem Hình 5. Tất nhiên nó có thể được tổng quát hóa cho thiết lập của chúng tôi, bao gồm đóng băng tham số. Đóng băng/cắt tỉa có cấu trúc có nghĩa là đóng băng/cắt tỉa toàn bộ nơ-ron hoặc kênh hoặc thậm chí các cấu trúc thô hơn của mạng. Đối với cắt tỉa, điều này ngay lập tức dẫn đến giảm chi phí tính toán, trong khi tính toán gradient có thể được bỏ qua cho cấu trúc đóng băng. Đối với đóng băng có cấu trúc, thường toàn bộ lớp được đóng băng [56, 94–96] với trường hợp cực đoan là đóng băng toàn bộ mạng [97]. Đối với cắt tỉa, việc cắt tỉa ở cấp độ kênh/nơ-ron [100–102] phổ biến hơn.

Cắt tỉa không có cấu trúc của các trọng số đơn lẻ thường cải thiện kết quả so với cắt tỉa có cấu trúc [51, 52]. Cắt tỉa không có cấu trúc có nhược điểm là yêu cầu phần mềm hỗ trợ tính toán thưa thớt để thực sự tăng tốc truyền xuôi và truyền ngược trong DNN. Hơn nữa, phần mềm như vậy chỉ tăng tốc DNN thưa thớt trên CPU [103, 104] hoặc phần cứng chuyên dụng [53, 54, 105–107]. Trọng số cũng có thể được đóng băng theo cách không có cấu trúc [58, 59, 98], dẫn đến giảm tính toán gradient, yêu cầu bộ nhớ và chi phí truyền thông cho huấn luyện phân tán.

Trong những năm gần đây, một chiến lược cắt tỉa bán cấu trúc đã phát triển, cái gọi là độ thưa thớt N:M [108–113]. Một tensor được định nghĩa là N:M thưa thớt nếu mỗi khối có kích thước M chứa (ít nhất) N số không. Ở đây, tensor được bao phủ bởi các khối không chồng lấp, đồng nhất có kích thước M. Sự phát triển này được thúc đẩy bởi Kiến trúc GPU NVIDIA A100 Tensor Core [113] có thể tăng tốc phép nhân ma trận lên đến gần gấp 2 lần nếu ma trận là 2:4 thưa thớt.

Như được chỉ ra trong Frankle và Carbin [47], việc sử dụng các phương pháp tinh vi để xác định một tập con thưa thớt, cố định của các tham số có thể huấn luyện tại khởi tạo ngẫu nhiên vượt trội hơn nhiều so với việc chọn chúng ngẫu nhiên. Ngược lại, việc chọn và cố định các kênh có thể huấn luyện ngẫu nhiên hoặc qua các kỹ thuật cổ điển hoạt động tốt dẫn đến kết quả tương tự cho cắt tỉa có cấu trúc tại khởi tạo [100]. Do đó, hầu hết các phương pháp cắt tỉa được trình bày và thảo luận trong công trình này là không có cấu trúc. Do đó, chúng tôi giả định, nếu không được đề cập khác, cắt tỉa/đóng băng không có cấu trúc trong phần sau.

### 3.2. Giảm chiều toàn cục hoặc theo lớp

Một sự phân biệt quan trọng cho các phương pháp DRT là liệu tỷ lệ của các tham số được huấn luyện được chọn riêng biệt cho mỗi lớp hay toàn cục. Như được trình bày trong Phần 3.4, tầm quan trọng của một trọng số Θ(0)i để huấn luyện được đo qua một điểm số si ∈ R. Một trọng số được đóng băng hoặc cắt tỉa nếu điểm số tương ứng dưới ngưỡng τi⁴. Ngưỡng này có thể được xác định toàn cục [47–49, 58, 60, 92, 114] hoặc theo lớp. Để ngưỡng hóa điểm số theo lớp, chúng tôi phân biệt giữa đặt tỷ lệ tham số có thể huấn luyện không đổi trong mỗi lớp [115], sử dụng heuristic hoặc siêu tham số được tối ưu hóa để tìm tỷ lệ tham số có thể huấn luyện tốt nhất cho mỗi lớp riêng biệt [116, 117] hoặc để số lượng tham số được huấn luyện trong mỗi lớp thích ứng động trong quá trình huấn luyện [92, 118].

### 3.3. Tần suất cập nhật

Tần suất cập nhật phép biến đổi giảm chiều Ψ(t) được gọi là tần suất cập nhật Fp, xem Hình 6. Cho một tần suất cập nhật Fp ∈ N+ ∪ {∞}, nó giữ

Ψ(t0) = Ψ(t0+1) = ... = Ψ(t0+Fp-1)                    (10)

trong đó t0 ∈ {0, Fp, 2Fp, ...} là một lần lặp mà phép biến đổi được cập nhật. Nếu Ψ(t) = Ψ(0) là hằng số, tần suất cập nhật bằng Fp = ∞. Chúng tôi giả định số lượng tham số được huấn luyện được cố định. Do đó, các phương pháp được đề xuất thường có Fp ≫ 1. Điều này đảm bảo (i) tính ổn định của việc huấn luyện và (ii) cơ hội cho các trọng số có thể huấn luyện mới phát triển đủ lớn để không bị cắt tỉa/đóng băng ngay lập tức trong bước cập nhật tiếp theo.

### 3.4. Tiêu chí để chọn tham số có thể huấn luyện

Trong phần sau, chúng tôi trình bày các tiêu chí phổ biến nhất để xác định trọng số có thể huấn luyện.

**Tiêu chí ngẫu nhiên.** Chọn ngẫu nhiên tham số có thể huấn luyện có nghĩa là một tỷ lệ phần trăm nhất định của chúng được chọn để huấn luyện bằng một quá trình hoàn toàn ngẫu nhiên, xem Hình 2 (c). Điều này có thể được thực hiện bằng cách lấy mẫu một điểm số cho mỗi trọng số từ phân phối Gauss tiêu chuẩn và huấn luyện những trọng số có d mẫu lớn nhất. Cắt tỉa ngẫu nhiên thường được sử dụng làm so sánh đầu tiên cho một phương pháp cắt tỉa mới được phát triển. Đối với huấn luyện thưa thớt động, các tham số được huấn luyện ban đầu thường được chọn bằng ngẫu nhiên [116, 117] vì động lực của cấu trúc liên kết thưa thớt sẽ tìm một kiến trúc hoạt động tốt dù sao. Cũng đối với các phương pháp DRT mà luồng thông tin không phải là yếu tố hạn chế, như PaI kết hợp với DCT [93] hoặc đóng băng [57, 98], trọng số có thể huấn luyện thường được chọn ngẫu nhiên.

⁴Chúng tôi đánh chỉ số ngưỡng với cùng chỉ số như trọng số để chỉ ra rằng ngưỡng có thể phụ thuộc vào vị trí của Θ(0)i nhưng không phải bản thân trọng số.

**Tiêu chí độ lớn.** Một cách đơn giản để xác định trọng số có thể huấn luyện là sử dụng những trọng số có độ lớn cao nhất. Điều này được thực hiện bởi LTH [47, 119] và cũng bởi nhiều phương pháp DST để cập nhật kiến trúc thưa thớt trong quá trình huấn luyện [92, 116–118]. Cắt tỉa độ lớn cho Θ tương đương với nghiệm của

min‖Θ̄‖₀≤d ‖Θ - Θ̄‖q,                    (11)

tức là xấp xỉ d-thưa thớt tốt nhất của Θ w.r.t. ‖ · ‖q. Ở đây, q ∈ (0,∞) là tùy ý. Giả định mat(Θ) ∈ Rn×m là ma trận biểu diễn một mạng tuyến tính với đầu vào m chiều và đầu ra n chiều với các tham số được vector hóa tương ứng Θ ∈ Rn·m. Với q = 2, nó giữ cho x ∈ Rm

‖mat(Θ)x - mat(Θ̄)x‖₂ ≤ ‖mat(Θ - Θ̄)‖F‖x‖₂                    (12)
                        = ‖Θ - Θ̄‖₂‖x‖₂.                    (13)

Đối với Θ̄ d-thưa thớt, vế phải (13) được tối thiểu hóa bởi nghiệm của cắt tỉa độ lớn. Điều này cho thấy khả năng của cắt tỉa độ lớn để xấp xỉ mạng dày đặc một cách thưa thớt nếu các tham số bị cắt tỉa không quá lớn. Trong thực tế, điều này được đảm bảo bằng cách chỉ cắt tỉa các phần nhỏ tham số trong một bước [14, 47, 100, 120]. Tiêu chí độ lớn từ quan điểm hệ thống động được phân tích trong Redman et al. [121]. Họ chỉ ra rằng cắt tỉa độ lớn tương đương với cắt tỉa các mode nhỏ của toán tử Koopman [122] xác định hành vi hội tụ của mạng. Do đó, cắt tỉa độ lớn nhỏ chỉ làm xáo trộn nhẹ hành vi dài hạn của DNN.

Lee et al. [123] phân tích méo mó ℓ₂ tối thiểu được tạo ra bởi cắt tỉa toàn bộ mạng cùng nhau. Nghiệm tham lam của họ xấp xỉ cắt tỉa độ lớn với tỷ lệ cắt tỉa tối ưu theo lớp. Nghiệm được thu được bằng cách tái tỷ lệ độ lớn với một yếu tố phụ thuộc trọng số, áp dụng cắt tỉa độ lớn toàn cục và cuối cùng đặt lại các trọng số chưa bị cắt tỉa về giá trị của chúng trước khi tái tỷ lệ.

Nếu giảm chiều được áp dụng tại khởi tạo mà không có bất kỳ huấn luyện trước nào, tiêu chí độ lớn chọn tham số có thể huấn luyện hoàn toàn bằng trọng số ban đầu, được rút ngẫu nhiên của chúng. Tuy nhiên, như được chỉ ra trong Frankle et al. [124], cắt tỉa độ lớn tại khởi tạo là một baseline không tầm thường cho các phương pháp PaI khác và vượt trội hơn PaI ngẫu nhiên.

**Tiêu chí dựa trên gradient.** Như đã thảo luận ở trên, tiêu chí độ lớn phần lớn là ngẫu nhiên tại khởi tạo. Do đó, các tiêu chí khác nhau để chọn trọng số có thể huấn luyện được sử dụng cho các phương pháp PaI SOTA. Tiêu chí dựa trên gradient phổ biến nhất dựa vào khai triển Taylor bậc nhất của hàm mất mát tại đầu huấn luyện [48, 58, 102, 114, 125–127] và chủ yếu được sử dụng cho PaI trong thiết lập của chúng tôi. Nó đo mức độ xáo trộn trọng số tại khởi tạo sẽ ảnh hưởng đến hàm mất mát. Nó giữ

L(Θ(0) - δ) = L(Θ(0)) - ∇Θ(0)L · δ + O(‖δ‖²).                    (14)

Trong thiết lập của chúng tôi, nhiễu loạn được giới thiệu bởi giảm chiều được cho bởi δ = Θ(0) - Ψ(0)(ϑ(0)). Bỏ qua các số hạng bậc cao hơn và dấu, bài toán tối ưu hóa sau đây phải được giải

max|∇Θ(0)L| · |Ψ(ϑ)|,                    (15)
Ψ,ϑ

để xác định Ψ(0) và ϑ(0). Nếu Ψ bị hạn chế là phép biến đổi cắt tỉa được định nghĩa bởi (3) và (4), bài toán tối ưu hóa (15) giảm thành

max     Σ|(∇Θ(0)L)iⱼ · Θ(0)iⱼ|,                    (16)
{i(0)₁,...,i(0)d}⊂{1,...D} j=1

được giải bởi các chỉ số với top-d |(∇Θ(0)L)i · Θ(0)i|.

Việc sử dụng tiêu chí dựa trên gradient nói trên cho PaI có thể dẫn đến mất luồng thông tin cho tỷ lệ cắt tỉa cao vì một số lớp bị cắt tỉa quá mạnh [49, 50, 58]. Không có luồng thông tin trong mạng, gradient biến mất và không thể huấn luyện. Do đó, có một số phương pháp để vượt qua luồng gradient yếu trong các mạng được cắt tỉa theo (16). Sử dụng khởi tạo ngẫu nhiên, thỏa mãn tính chất đẳng cự động theo lớp [67, 128–130] sẽ dẫn đến luồng thông tin được cải thiện trong mạng bị cắt tỉa [125]. Một phương pháp khác [127] sử dụng tái tỷ lệ mạng bị cắt tỉa để đưa nó đến biên của hỗn loạn [128–130] có lợi cho huấn luyện DNN. Một cách đơn giản, được đề xuất bởi Wimmer et al. [58], được cho bằng cách đóng băng các trọng số chưa được huấn luyện thay vì cắt tỉa chúng.

**Bảo tồn luồng thông tin.** Luồng thông tin đầy đủ cũng có thể được đảm bảo trực tiếp bởi tiêu chí để chọn tham số có thể huấn luyện. Wang et al. [49] huấn luyện mạng thưa thớt với tổng chuẩn ℓ₂ gradient cao nhất sau cắt tỉa. Do đó, luồng thông tin không phải là nút thắt cổ chai cho huấn luyện.

Tính độ nhạy synaptic của trọng số được giới thiệu trong Tanaka et al. [50]. Một độ nhạy synaptic được định nghĩa là

S(Θ) = ∇ΘR ⊙ Θ,                    (17)

trong đó R: RD → R là một hàm, phụ thuộc vào trọng số Θ. Trong Tanaka et al. [50], một định luật bảo tồn cho tổng độ nhạy synaptic của một lớp được chỉ ra. Bằng cách lặp đi lặp lại cắt tỉa một phần nhỏ tham số dựa trên điểm độ nhạy synaptic (17), định luật bảo tồn đảm bảo luồng thông tin trung thực trong mạng thưa thớt. Ví dụ cho tiêu chí cắt tỉa dựa trên luồng synaptic là đặt R(Θ) là chuẩn đường dẫn ℓ₁ [131, 132] của DNN fΘ [50] hoặc chuẩn đường dẫn ℓ₂ [133, 134]. Trong Patil và Dovrolis [134], chuẩn đường dẫn ℓ₂ được kết hợp với bước đi ngẫu nhiên trong không gian tham số để dần dần xây dựng cấu trúc liên kết thưa thớt. Ngược lại, các phương pháp tiêu chuẩn thu hẹp kiến trúc dày đặc thành một kiến trúc thưa thớt.

**ψ(0) có thể huấn luyện.** Tất cả các tiêu chí nói trên dựa trên heuristic, thường bị hạn chế hoạt động tốt trong một số kịch bản nhưng thất bại cho các kịch bản khác, xem Frankle et al. [124] để so sánh thực nghiệm cho các phương pháp PaI SOTA Lee et al. [48], Wang et al. [49], Tanaka et al. [50]. Một cách tự nhiên để vượt qua những hạn chế này được đưa ra bằng cách huấn luyện nhúng ψ(0). Phương pháp này đặc biệt được quan tâm để cắt tỉa các mạng được tạo ngẫu nhiên mà không huấn luyện trọng số sau đó, trong đó việc xác định ψ(0) là cách duy nhất để tối ưu hóa mạng [63, 119, 135, 136].

Tìm ψ(0) ∈ {0,1}D×d không thể được thực hiện bằng backpropagation đơn giản vì ψ(0) được tối ưu hóa trong một tập rời rạc {ψ ∈ {0,1}D×d : ‖ψ‖₀ = d}. Diffenderfer và Kailkhura [63], Ramanujan et al. [135], Aladago và Torresani [136], Mallya et al. [137], Zhang et al. [138] tìm trọng số bị cắt tỉa với một điểm số có thể huấn luyện s ∈ RD cùng với một hàm dấu (đã dịch chuyển). Điểm số s được tối ưu hóa qua backpropagation lỗi của mạng thưa thớt trên tập huấn luyện bằng cách sử dụng ước lượng xuyên thẳng [139] để vượt qua gradient bằng không của hàm dấu. Vượt qua gradient biến mất của điểm số bằng không cũng có thể đạt được bằng cách huấn luyện xác suất cắt tỉa cho mỗi trọng số và lấy mẫu một ψ ∈ {0,1}D×d tương ứng trong mỗi bước tối ưu hóa [119].

### 3.5. Huấn luyện trước phép biến đổi

Tương tự như ψ(0) có thể huấn luyện, được thảo luận trong Phần 3.4, một bước huấn luyện trước để tìm ψ(0) cũng có thể được áp dụng. Sự khác biệt là đối với huấn luyện trước ψ(0), các trọng số dày đặc tương ứng Θ(0) cũng có thể được huấn luyện. Sử dụng huấn luyện trước để tìm phép biến đổi ban đầu ψ(0) tất nhiên tốn kém về thời gian và cũng về mặt tăng số lượng tham số trong giai đoạn huấn luyện trước. Lưu ý, sau khi huấn luyện trước ψ(0), các trọng số được huấn luyện trước không được phép sử dụng, mà được đặt lại về Θ(0).

Ví dụ nổi bật nhất cho điều này được đưa ra bởi LTH huấn luyện Θ(0) đến hội tụ, cắt tỉa p₀ · 100% trọng số khác không và đặt lại các trọng số khác không còn lại về giá trị ban đầu của chúng. Quy trình này được tiếp tục cho đến khi tỷ lệ cắt tỉa mong muốn được đạt. Sau đó, phép biến đổi cắt tỉa ψ(0) được áp dụng cho trọng số ban đầu Θ(0) và cố định cho việc huấn luyện kiến trúc thưa thớt. Ở đây, mạng đầy đủ được sử dụng để tìm kiến trúc thưa thớt hoạt động tốt trong khi để huấn luyện thực tế chỉ phần thưa thớt của mạng, bắt đầu từ khởi tạo ngẫu nhiên, được sử dụng.

Một phương pháp khác cho huấn luyện trước ψ(0) được đưa ra trong Liu và Zenke [140] huấn luyện trước một mạng thưa thớt để bắt chước động lực huấn luyện của một mạng dày đặc được khởi tạo ngẫu nhiên, được đo bằng kernel tiếp tuyến neural [141–143].⁵

## 4. Giả thuyết vé số may mắn

Trong Phần này, chúng tôi thảo luận chi tiết LTH, được đề xuất bởi Frankle và Carbin [47]. Frankle và Carbin [47] chỉ ra rằng các mạng con cực kỳ thưa thớt của một mạng được khởi tạo ngẫu nhiên có thể được tìm thấy, sau khi được huấn luyện, sánh ngang hoặc thậm chí vượt trội hơn baseline được huấn luyện dày đặc của chúng. Hơn nữa, mạng được huấn luyện thưa thớt hội tụ ít nhất nhanh như huấn luyện tiêu chuẩn nhưng thường nhanh hơn. Bảng 1 tóm tắt các phương pháp khác nhau để tìm LT. Hình 7 so sánh LTH với các phương pháp cắt tỉa cổ điển được áp dụng cho các mạng đã hội tụ [14].

⁵Để chính xác, nhằm bắt chước động lực huấn luyện, Liu và Zenke [140] cũng huấn luyện trước trọng số ϑ(0) bằng cách sử dụng Θ(0). Do đó, nó không nằm trong giới hạn hẹp của công trình này.

Quy trình xác định các mạng thưa thớt như sau: Trước tiên, mạng dày đặc được huấn luyện đến hội tụ và nhúng cắt tỉa ψ được xây dựng theo tiêu chí độ lớn, xem Phần 3.4. Các trọng số chưa bị cắt tỉa được đặt lại về giá trị Θ(0)i của chúng. Bây giờ, quy trình này có thể được thực hiện một lần hoặc lặp đi lặp lại. Trong trường hợp một lần, tất cả trọng số được cắt tỉa sau khi huấn luyện mạng dày đặc cùng một lúc. Sau đó, mạng thưa thớt, cái gọi là Vé số May mắn (LT), được huấn luyện đến hội tụ. Đối với quy trình lặp, không phải tất cả hệ số mà chỉ p₀ · 100% (thường 20%) của các hệ số chưa bị cắt tỉa được cắt tỉa trong một lần lặp. Các trọng số khác không còn lại được đặt lại về giá trị ban đầu của chúng và được huấn luyện lại cho đến hội tụ. Quy trình này được áp dụng lặp đi lặp lại cho đến khi tỷ lệ cắt tỉa mong muốn p được đạt. Cuối cùng, LT được tối ưu hóa trong một huấn luyện thưa thớt cuối cùng.

LTH lặp đã chỉ ra tìm LT hoạt động tốt hơn so với phương pháp một lần [47, 120]. Nhưng, để đạt tỷ lệ cắt tỉa cuối cùng p, quy trình LTH lặp cần

log(1-p)/log(4/5)                    (18)

nhiều huấn luyện trước cộng với huấn luyện thưa thớt cuối cùng. Đối với tỷ lệ cắt tỉa cuối cùng p = 0.9, phương pháp LTH lặp cần tổng cộng 12 lần huấn luyện mạng. Mặt khác, phương pháp một lần luôn yêu cầu 2 lần huấn luyện cho tỷ lệ cắt tỉa p tùy ý.

Frankle và Carbin [47], Frankle et al. [120], Gale et al. [147] chỉ ra rằng đặt lại trọng số chưa bị cắt tỉa về giá trị ban đầu của chúng chỉ tạo ra các kiến trúc thưa thớt có thể huấn luyện tốt nếu các mạng không quá lớn. Đối với các kiến trúc mạng hiện đại như ResNet [148], đặt lại trọng số về giá trị ban đầu của chúng không dẫn đến kết quả tương tự như baseline được huấn luyện dày đặc. Vấn đề này có thể được khắc phục bằng quay ngược muộn, tức là đặt lại trọng số về giá trị đạt được sớm trong huấn luyện dày đặc đầu tiên [120, 144, 149]. Hơn nữa, cắt tỉa hệ số w.r.t. một cơ sở động, thích ứng cho các bộ lọc tích chập K×K cải thiện quay ngược muộn hơn nữa [62].

Lee et al. [123] sử dụng độ lớn được tỷ lệ để cải thiện kết quả cho LT ở mức độ thưa thớt cao. Tỷ lệ của họ xấp xỉ lựa chọn tốt nhất của tỷ lệ thưa thớt theo lớp. LT như cân bằng của hệ thống động được phân tích trong Zhang et al. [138]. Họ chỉ ra lý thuyết rằng động lực quan trọng của huấn luyện SGD được chứa trong một không gian con W+ ⊂ RD chiều d nhỏ, trình tạo của cái gọi là đa tạp quán tính. Chỉ một vài epoch huấn luyện đủ để tính toán đáng tin cậy W+. Quay ngược trọng số chưa bị cắt tỉa về giá trị của chúng cho một bước huấn luyện sớm và chiếu chúng lên W+ cải thiện kết quả so với LT tiêu chuẩn với quay ngược muộn. Đảm bảo lý thuyết để khôi phục mạng tuyến tính thưa thớt qua LT được mô tả trong Elesedy et al. [150].

Chi phí để tìm LT qua cắt tỉa độ lớn lặp có thể được giảm bằng cách sử dụng dừng sớm và huấn luyện độ chính xác thấp cho mỗi lần lặp huấn luyện trước [146], chia sẻ LT cho các tập dữ liệu và bộ tối ưu hóa khác nhau [144] hoặc giảm lặp đi lặp lại tập dữ liệu cùng với số lượng tham số khác không [145]. Cũng vậy, lỗi của LT từ một họ cấu hình mạng nhất định (tức là ResNet với độ thưa thớt, độ rộng, độ sâu và số lượng ví dụ huấn luyện được sử dụng thay đổi) có thể được ước tính tốt bằng cách biết hiệu suất của chỉ một vài mạng được huấn luyện từ họ mạng này [151]. Mặc dù ứng dụng đầu tiên của chúng trên phân loại hình ảnh, LT cũng đã chỉ ra thành công trong học tự giám sát [152], xử lý ngôn ngữ tự nhiên [153, 154], nhiệm vụ học tăng cường [153, 155], học chuyển giao [156] và các nhiệm vụ nhận dạng đối tượng như phân đoạn ngữ nghĩa hoặc phát hiện đối tượng [157]. Hơn nữa, Chen et al. [158] đề xuất các phương pháp để xác minh quyền sở hữu LT và do đó bảo vệ chủ sở hữu hợp pháp chống lại vi phạm sở hữu trí tuệ.

LT vượt trội hơn khởi tạo lại ngẫu nhiên các mạng thưa thớt trong trường hợp cắt tỉa không có cấu trúc [47]. Ngược lại, đối với cắt tỉa có cấu trúc, dường như không có sự khác biệt giữa khởi tạo lại ngẫu nhiên mạng thưa thớt và đặt lại trọng số về khởi tạo ngẫu nhiên của chúng [100].

Liên quan chặt chẽ đến LTH với quay ngược muộn, Renda et al. [159], Le và Hua [160] chỉ ra rằng tinh chỉnh một mạng bị cắt tỉa với lịch trình tỷ lệ học quay ngược về các giai đoạn sớm hơn trong huấn luyện vượt trội hơn tinh chỉnh cổ điển của các mạng thưa thớt với tỷ lệ học nhỏ. Bai et al. [161] chỉ ra rằng các mặt nạ cắt tỉa được chọn ngẫu nhiên tùy ý có thể dẫn đến các mô hình thưa thớt thành công nếu mạng đầy đủ được sử dụng trong quá trình huấn luyện. Để làm điều này, họ đẩy thông tin của trọng số, sẽ bị cắt tỉa cuối cùng, vào mạng thưa thớt trong một bước huấn luyện trước. Do đó, Bai et al. [161] không phải là phương pháp DRT.

## 5. Cắt tỉa tại khởi tạo

Vượt qua chi phí cao cho (các) chu trình huấn luyện-cắt tỉa-đặt lại cần thiết để tìm LT là một trong những động lực chính để sử dụng cắt tỉa tại khởi tạo (PaI) [48]. Với cắt tỉa tại khởi tạo, chúng tôi có ý nghĩa các phương pháp bắt đầu với một mạng được khởi tạo ngẫu nhiên và không thực hiện bất kỳ huấn luyện trước nào của trọng số mạng để tìm phép biến đổi cắt tỉa ψ(0). Hơn nữa đối với PaI, ψ(t) = ψ(0) giữ cho tất cả các lần lặp huấn luyện t. Các biến thể khác nhau của phương pháp PaI bao gồm cắt tỉa một lần [48, 49, 61, 62, 125, 127, 164, 165], cắt tỉa lặp [50, 102, 126, 134] và huấn luyện phép biến đổi cắt tỉa [63, 119, 135, 136]. Trên đó, có các phương pháp huấn luyện mạng sau cắt tỉa và các phương pháp không huấn luyện các tham số khác không hoàn toàn. Bảng 2 tóm tắt các phương pháp PaI được đề xuất trong Phần này.

### 5.1. PaI theo sau bởi huấn luyện trọng số khác không

Trước tiên, chúng tôi bắt đầu bằng việc so sánh các phương pháp khác nhau huấn luyện trọng số sau bước cắt tỉa. Chúng tôi có thể nhóm hầu hết chúng thành các phương pháp dựa trên gradient và các phương pháp dựa trên luồng thông tin. Để so sánh chi tiết ba phương pháp PaI phổ biến Lee et al. [48], Wang et al. [49], Tanaka et al. [50], chúng tôi tham khảo Frankle et al. [124]. Hơn nữa, Fischer và Burkholz [168] so sánh chúng trên các nhiệm vụ được tạo, trong đó các mạng mục tiêu đã biết và cực kỳ thưa thớt được cấy trong một mô hình được khởi tạo ngẫu nhiên. Họ chỉ ra rằng các phương pháp PaI hiện tại thất bại trong việc tìm những mô hình thưa thớt đó ở độ thưa thớt cực đoan. Tuy nhiên, hiệu suất của các phương pháp cắt tỉa khác ngoài PaI không được đánh giá và vẫn còn là câu hỏi mở liệu chúng có thể tìm những mạng con được cấy này không.

**Các phương pháp dựa trên gradient.** Các phương pháp dựa trên gradient [48, 102, 125–127] cố gắng xây dựng mạng thưa thớt có ảnh hưởng tốt nhất trong việc thay đổi hàm mất mát tại đầu huấn luyện, như được trình bày trong phương trình (14), (15) và (16). Tuy nhiên, đã được chỉ ra rằng các phương pháp dựa trên gradient một lần có vấn đề về luồng gradient biến mất, nếu quá nhiều tham số bị cắt tỉa [49, 58, 125]. Các phương pháp để vượt qua điều này được đưa ra bằng cách sử dụng phương pháp lặp [102, 126] hoặc sử dụng khởi tạo của mạng, được điều chỉnh với mạng thưa thớt [125, 127]. Những khởi tạo được điều chỉnh này bao gồm các mạng đẳng cự động được gọi [125] và các mạng ở rìa của hỗn loạn [127].

**Các phương pháp bảo tồn luồng thông tin.** Các phương pháp PaI khác chủ yếu tập trung vào tạo ra các mạng thưa thớt với luồng thông tin đầy đủ [49, 50, 134, 164]. Đối với DNN được khởi tạo ngẫu nhiên, mất mát không tốt hơn ngẫu nhiên. Do đó, Wang et al. [49] lập luận rằng "tại đầu huấn luyện, việc bảo tồn động lực huấn luyện quan trọng hơn bản thân mất mát." Do đó, Wang et al. [49] cố gắng tìm mạng thưa thớt với chuẩn gradient cao nhất sau cắt tỉa. Họ làm như vậy, bằng cách huấn luyện các trọng số với −Θ(0)i(H(0)∇Θ(0)L)i cao nhất, trong đó H(0) định nghĩa Hessian của hàm mất mát tại khởi tạo. Một phương pháp khác được đưa ra bởi Tanaka et al. [50] và Patil và Dovrolis [134], cố gắng tối đa hóa chuẩn đường dẫn trong mạng thưa thớt, xem Phần 3.4 Bảo tồn luồng thông tin để biết thêm chi tiết. Đối với RNN hoặc LSTM, các phương pháp PaI tiêu chuẩn không hoạt động tốt [164]. Bằng cách cắt tỉa dựa trên giá trị đơn của Jacobian thời gian, Zhang và Stadie [164] có thể bảo tồn các trọng số truyền một lượng lớn thông tin qua độ sâu thời gian của mạng.

**Các phương pháp lai và khác.** Như được chỉ ra trong Wimmer et al. [61], chỉ tối ưu hóa mạng thưa thớt để có luồng thông tin cao nhất có thể không dẫn đến các kiến trúc thưa thớt tốt nhất – thậm chí cho tỷ lệ cắt tỉa cao. Họ kết luận rằng luồng thông tin là điều kiện cần thiết cho huấn luyện thưa thớt nhưng không đủ. Do đó, họ kết hợp các phương pháp dựa trên gradient và dựa trên luồng thông tin để có được điều tốt nhất từ cả hai thế giới. Với phương pháp của họ, họ cải thiện PaI dựa trên gradient và PaI dựa trên bảo tồn luồng thông tin cùng một lúc.

Một phương pháp khác đảm bảo luồng thông tin trung thực trong mạng và đồng thời cải thiện hiệu suất được đưa ra bằng cách cắt tỉa trong không gian trung gian [62]. Wimmer et al. [62] biểu diễn các bộ lọc K×K của mạng tích chập trong không gian trung gian – một không gian tuyến tính được bao bởi một cơ sở bộ lọc cơ bản. Sau khi cắt tỉa các hệ số của bộ lọc, cơ sở bộ lọc được huấn luyện cùng với các hệ số khác không. Bằng cách thích ứng không gian trung gian trong quá trình huấn luyện, các mạng có xu hướng khôi phục từ luồng thông tin thấp. Hơn nữa, việc sử dụng biểu diễn không gian trung gian đã chỉ ra cải thiện không chỉ PaI cho các phương pháp dựa trên gradient và bảo tồn luồng thông tin, mà còn LTH, DST, đóng băng và huấn luyện mạng dày đặc.

Một phương pháp trực giao để đảm bảo luồng thông tin cao trong khi chỉ huấn luyện một phần thưa thớt của mạng được đề xuất bởi Price và Tanner [93]. Cắt tỉa ngẫu nhiên được kết hợp với DCT vượt qua trong mỗi lớp. Vì DCT tương ứng với các phép biến đổi cơ sở với cơ sở trực chuẩn, thông tin của đầu vào của một lớp được duy trì ngay cả khi chỉ một phần nhỏ tham số được huấn luyện. Do đó, kết quả được cải thiện đáng kể cho tỷ lệ cắt tỉa cực đoan. Lưu ý, thêm DCT cải thiện hiệu suất cho p cao mặc dù sử dụng lựa chọn ngẫu nhiên đơn giản các trọng số được huấn luyện. Như được chỉ ra bởi Price và Tanner [93], thêm DCT vào mỗi lớp cũng cải thiện các phương pháp DST cho p cao.

Alizadeh et al. [165] cải thiện Lee et al. [48] bằng cách mô hình hóa hiệu ứng của cắt tỉa trên hàm mất mát tại lần lặp huấn luyện M > 0. Ngược lại, Lee et al. [48] chỉ phân tích hiệu ứng của cắt tỉa trên mất mát tại khởi tạo. Để làm như vậy, Alizadeh et al. [165] tính một meta-gradient được đạt bằng cách huấn luyện trước mạng dày đặc trong M epoch. Meta-gradient được tính w.r.t. mặt nạ cắt tỉa. Sau cắt tỉa, các trọng số khác không được đặt lại về khởi tạo của chúng.

Lubana và Dick [163] so sánh lý thuyết các phương pháp PaI dựa trên độ lớn, gradient và bảo tồn luồng thông tin. Phân tích của họ chỉ ra rằng các phương pháp dựa trên độ lớn dẫn đến giảm nhanh trong mất mát huấn luyện, do đó hội tụ nhanh. Hơn nữa, cắt tỉa dựa trên gradient bảo tồn hàm mất mát, loại bỏ các tham số thay đổi chậm nhất và bảo tồn động lực bậc nhất của sự tiến hóa mô hình. Họ kết hợp cắt tỉa dựa trên độ lớn và gradient để có được điều tốt nhất từ cả hai thế giới, tạo ra điểm cắt tỉa |∇Θ(0)Li| · |Θ(0)i|². Cuối cùng, cắt tỉa bảo tồn luồng thông tin bằng cách sử dụng tiêu chí của Wang et al. [49] để có được mô hình thưa thớt với chuẩn gradient tối đa loại bỏ các trọng số tăng tối đa hàm mất mát. Tuy nhiên, điều này không bảo tồn động lực bậc hai của mô hình. Bảo tồn chuẩn gradient bằng cách huấn luyện các trọng số với |Θ(0)i(H(0)∇Θ(0)L)i| cao nhất mặt khác cũng bảo tồn động lực bậc hai và cho thấy kết quả được cải thiện so với [49].

Frankle et al. [124], Su et al. [162] chỉ ra rằng các phương pháp PaI phổ biến nhất Lee et al. [48], Wang et al. [49], Tanaka et al. [50] không mất hiệu suất đáng kể nếu vị trí của các trọng số khác không được xáo trộn ngẫu nhiên trong mỗi lớp nếu độ thưa thớt không quá cực đoan. Do đó, những phương pháp này dường như không tìm được kiến trúc thưa thớt tốt nhất, mà đúng hơn là tỷ lệ cắt tỉa theo lớp hoạt động tốt cho kiến trúc mạng và tỷ lệ cắt tỉa toàn cục đã cho. Sử dụng kiến thức về các phương pháp PaI hoạt động tốt, Su et al. [162] chỉ ra kết quả ấn tượng bằng cách cắt tỉa trọng số ngẫu nhiên. Bằng cách này, họ sử dụng tỷ lệ cắt tỉa theo lớp được phát sinh bằng cách quan sát tỷ lệ cắt tỉa theo lớp của các phương pháp PaI hoạt động tốt khác. Liu et al. [169] cũng chỉ ra rằng PaI ngẫu nhiên có thể đạt kết quả cạnh tranh với các phương pháp PaI khác. Họ chứng minh thực nghiệm rằng khoảng cách giữa PaI ngẫu nhiên và huấn luyện dày đặc trở nên nhỏ hơn nếu mạng baseline cơ bản trở nên lớn hơn. Do đó, PaI ngẫu nhiên có thể cung cấp một baseline huấn luyện thưa thớt mạnh, đặc biệt cho các mô hình lớn.

### 5.2. PaI mà không huấn luyện trọng số khác không

Mallya et al. [137] chỉ ra rằng một mạng được huấn luyện trước có thể được cắt tỉa cho một nhiệm vụ cụ thể để có hiệu suất tốt ngay cả khi không tinh chỉnh. Được truyền cảm hứng từ điều đó, một số phương pháp PaI cho thấy điều tương tự đúng cho một mạng được khởi tạo ngẫu nhiên. Trước khi đi vào chi tiết về cách có thể tìm mặt nạ cắt tỉa cho trọng số ngẫu nhiên, chúng tôi sẽ thảo luận các công trình lý thuyết bao gồm khả năng xấp xỉ phổ quát của việc cắt tỉa một mạng được khởi tạo ngẫu nhiên, cũng được gọi là giả thuyết vé số may mắn mạnh [170, 171].

**Nền tảng lý thuyết.** Một trực giác rằng các mạng lớn, được khởi tạo ngẫu nhiên chứa các mạng con thưa thớt hoạt động tốt được đưa ra trong Ramanujan et al. [135]. Cho f*Θ*: Rm → Rn là một mạng mục tiêu với Θ* ∈ Rd*. Hơn nữa, cho fΘ: Rm → Rn là một mạng với Θ ∈ RD được khởi tạo ngẫu nhiên và Θ̃ là một phép chiếu thưa thớt được chọn ngẫu nhiên của Θ với ||Θ̃||₀ = d ≪ D tham số khác không. Do đó, fΘ̃ là một mạng con d-thưa thớt tùy ý của fΘ. Giả định D ≫ d* ≈ d, Ramanujan et al. [135] lập luận rằng cơ hội để fΘ̃ là một xấp xỉ tốt của f*Θ* là nhỏ, nhưng bằng δ > 0. Mạng lớn, ngẫu nhiên có (D choose d) mạng con với d tham số khác không. Do đó, cơ hội của tất cả mạng con thưa thớt fΘ̃ không phải là xấp xỉ tốt cho f*Θ* là

(1-δ)^(D choose d) → 0 khi D→∞.                    (19)

Do đó, nếu mạng được khởi tạo ngẫu nhiên fΘ được chọn đủ lớn, nó sẽ chứa, với xác suất cao, một xấp xỉ tốt cho f*Θ* với d tham số khác không.

Trực giác này được chứng minh trong Malach et al. [170] cho MLP bằng cách sử dụng một ý tưởng hơi khác. Ở đây, thay vì làm cho mỗi lớp trong mạng được khởi tạo ngẫu nhiên có độ rộng tùy ý, một lớp trung gian l+1/2 được thêm vào giữa mỗi hai lớp l và l+1 của mạng được khởi tạo ngẫu nhiên, xem Hình 8 phải. Những lớp trung gian này được làm đủ rộng để mỗi trọng số w* trong mạng mục tiêu có thể được xấp xỉ đủ tốt. Nó giữ

w* · x = (+1) · ReLU(|w*| · x) + (-1) · ReLU(-|w*| · x).                    (20)
        ≈sign+ ≈w*+             ≈sign- ≈w*-

Do đó, cho mỗi trọng số w* trong mạng mục tiêu, hai đường dẫn được xây dựng. Đường đầu tiên xấp xỉ phần dương |w*| cùng với dấu +1 và đường khác xấp xỉ phần âm -|w*| cùng với dấu -1, xem giữa phải của Hình 8. Tất cả các trọng số còn lại trong mạng ngẫu nhiên được cắt tỉa như được chỉ ra ở giữa trái của Hình 8. Kết quả là, Malach et al. [170] chỉ ra rằng, với một số giả định về mạng mục tiêu, cắt tỉa các mạng được khởi tạo ngẫu nhiên lớn là một xấp xỉ phổ quát. Nhưng, cho mỗi trọng số mục tiêu, một số lượng trọng số trung gian đa thức phải được thêm vào.

Các công trình tiếp theo Orseau et al. [172] và Pensia et al. [171] cải thiện hiệu quả tham số bằng cách thu hẹp mạng lớn, được ngẫu nhiên hóa. Họ làm như vậy bằng cách sử dụng nhiều hơn hai đường dẫn để xấp xỉ một trọng số trong mạng gốc. Bằng cách cho phép các giá trị ngẫu nhiên được lấy mẫu lại, độ rộng của lớp trung gian l+1/2 có thể được giảm xuống 2 lần kích thước của lớp gốc l [173]. Burkholz et al. [174] cho phép mạng được khởi tạo ngẫu nhiên sâu hơn 2 lần so với mạng mục tiêu. Bằng cách xấp xỉ các lớp với các kết hợp của hàm tuyến tính đơn biến và đa biến, họ cũng có thể xấp xỉ các lớp tích chập. Độc lập với phương pháp của Burkholz et al. [174], da Cunha et al. [175] mở rộng kết quả của Pensia et al. [171] cho CNN bằng cách hạn chế đầu vào của mạng là không âm.

**Các phương pháp để huấn luyện ψ(0).** Như đã đề cập trước đó, huấn luyện ψ(0) ∈ {0,1}D×d yêu cầu tối ưu hóa trong một không gian rời rạc. Do đó, các phương pháp khác nhau được sử dụng để tìm ψ(0) cho một mạng được khởi tạo ngẫu nhiên. Trong Zhou et al. [119], các trọng số được cắt tỉa với xác suất được mô hình hóa bằng các bộ lấy mẫu Bernoulli với các tham số có thể huấn luyện tương ứng. Trong khi giúp ích trong quá trình huấn luyện, tính ngẫu nhiên của quy trình này có thể hạn chế hiệu suất tại thời gian kiểm tra [135]. Tính ngẫu nhiên được khắc phục trong Ramanujan et al. [135] bằng thuật toán edge-popup. Cho mỗi trọng số Θ(0)i trong mạng, một điểm cắt tỉa tương ứng si được huấn luyện. Các trọng số với top-d điểm được giữ ở giá trị ban đầu của chúng, những cái còn lại được cắt tỉa. Điểm si sau đó được tối ưu hóa với sự trợ giúp của ước lượng xuyên thẳng được gọi [139] và phép biến đổi cắt tỉa được cập nhật trong mỗi lần lặp. Ramanujan et al. [135] chỉ ra rằng một Wide-ResNet50 ngẫu nhiên [176] chứa một mạng con với kích thước nhỏ hơn ResNet34 [148] nhưng cùng độ chính xác kiểm tra trên ImageNet [177]. Koster et al. [166] và Chen et al. [167] độc lập chỉ ra rằng cho phép các trọng số được khởi tạo ngẫu nhiên chuyển đổi dấu, cải thiện hiệu suất của edge-popup. Chijiwa et al. [173] cải thiện edge-popup bằng cách cho phép lấy mẫu lại các trọng số chưa được huấn luyện. Nhị phân hóa các trọng số ngẫu nhiên chưa bị cắt tỉa cũng có thể được kết hợp với thuật toán edge-popup [63]. Cuối cùng, Aladago và Torresani [136] lấy mẫu cho mỗi trọng số ϑ(0)i trong một mạng mục tiêu một tập n trọng số có thể w(1)i, ..., w(n)i trong một mạng ảo tưởng, lớn gấp n lần. Mạng ảo tưởng này được cố định. Trong quá trình huấn luyện, mỗi trọng số có thể w(j)i có một điểm chất lượng tương ứng được tăng nếu trọng số là lựa chọn tốt cho ϑ(0)i và giảm ngược lại. Cuối cùng, w(j)i với điểm chất lượng cao nhất được đặt làm ϑ(0)i trong khi các trọng số ảo tưởng còn lại bị loại bỏ, tức là cắt tỉa.

## 6. Huấn luyện thưa thớt động

Đối với tỷ lệ cắt tỉa cao, PaI không thể hoạt động tốt bằng các phương pháp cắt tỉa cổ điển. Một lời giải thích có thể cho khoảng cách hiệu suất là một mạng được khởi tạo ngẫu nhiên không chứa đủ thông tin để tìm một mạng con thưa thớt phù hợp huấn luyện tốt. Thích ứng phép biến đổi cắt tỉa trong quá trình huấn luyện hoặc sử dụng huấn luyện trước mạng để tìm ψ(0) vượt qua sự thiếu thông tin này. Như được chỉ ra trong Phần 4, tìm LT tốn kém. Hơn nữa, không được trả lời liệu đặt lại trọng số về giá trị ban đầu của chúng có thể sánh ngang quay ngược muộn cho các tập dữ liệu quy mô lớn không. Huấn luyện thưa thớt động hoạt động tốt cho tỷ lệ cắt tỉa cao trong khi chỉ cần một huấn luyện hoàn toàn thưa thớt. Điều này đạt được bằng cách phân phối lại độ thưa thớt trên mạng trong quá trình huấn luyện. Bằng cách luôn cố định một tỷ lệ nhất định tham số tại không, số lượng trọng số được huấn luyện được giữ không đổi trong quá trình huấn luyện. Các phương pháp DST khác nhau được thu thập trong Bảng 3.

### 6.1. Các phương pháp huấn luyện thưa thớt động

Được truyền cảm hứng từ việc nối lại kết nối synaptic trong quá trình học tập trong não người [178], DEEP-R [60] huấn luyện DNN thưa thớt trong khi cho phép các kết nối khác không nối lại trong quá trình huấn luyện. Để làm như vậy, cắt tỉa và nối lại được mô hình hóa như lấy mẫu ngẫu nhiên các cấu hình mạng từ một posterior. Tuy nhiên, quy trình này tốn kém về mặt tính toán và thách thức để áp dụng cho các mạng và tập dữ liệu lớn [118].

Trong Mocanu et al. [116], một mạng con thưa thớt với d tham số khác không được chọn ngẫu nhiên tại đầu huấn luyện. Ở đây, một tỷ lệ cắt tỉa ban đầu phải được định nghĩa riêng biệt cho mỗi lớp. Sau mỗi epoch, các trọng số được huấn luyện với độ lớn nhỏ nhất được cắt tỉa theo lớp với tỷ lệ q và cùng số lượng trọng số được tái tạo tại các vị trí ngẫu nhiên trong lớp này. Lưu ý, tỷ lệ cắt tỉa q được sử dụng để cập nhật ψ(t) khác với tỷ lệ cắt tỉa ban đầu p. Quy trình này được thực hiện cho đến khi huấn luyện hội tụ. Do đó, trong mỗi epoch, cùng số lượng tham số được cắt tỉa và tái tạo làm cho việc huấn luyện khó hội tụ. Hơn nữa, số lượng tham số khác không cần được định nghĩa cho mỗi lớp trước khi huấn luyện và không thể thích ứng trong quá trình huấn luyện.

Tái tham số hóa thưa thớt động [92] vượt qua vấn đề này bằng cách sử dụng cắt tỉa độ lớn với ngưỡng thích ứng, toàn cục. Hơn nữa, số lượng trọng số tái tạo trong mỗi lớp được thích ứng tỷ lệ với số lượng trọng số khác không trong lớp đó.

Tái tạo trọng số không ngẫu nhiên, mà dựa trên momentum gradient của chúng được đề xuất bởi Dettmers và Zettlemoyer [118]. Ở đây, không chỉ các trọng số tái tạo được xác định bởi momentum của chúng, mà cả số lượng của chúng trong mỗi lớp.

RigL [117] vượt qua nhu cầu của Dettmers và Zettlemoyer [118] để tính gradient dày đặc trong mỗi lần lặp huấn luyện, và chỉ tái tạo trọng số dựa trên gradient thực tế của chúng trong lần lặp cập nhật của ψ(t), tức là t ∈ {Fp, 2Fp, ...}. Cũng vậy, số lượng tham số được cắt tỉa và tái tạo được giảm bởi một lịch trình cosin để tăng tốc và cải thiện hội tụ.

DNN được huấn luyện dày đặc quá tham số hóa kết hợp với SGD đã chỉ ra khả năng tổng quát hóa tốt [179–181]. Liu et al. [182] chỉ ra rằng khả năng tổng quát hóa tốt của các mô hình DST có thể được giải thích bằng cái gọi là quá tham số hóa trong thời gian. Huấn luyện DNN thưa thớt có thể được xem trong đa tạp không gian-thời gian. Để quá tham số hóa các mô hình DST trong đa tạp này, ba tính chất phải được thỏa mãn:

1. Mạng baseline dày đặc phải đủ lớn.
2. Khám phá các trọng số được huấn luyện phải được đảm bảo trong quá trình huấn luyện.
3. Thời gian huấn luyện phải đủ dài để mạng có thể kiểm tra đủ kiến trúc thưa thớt trong huấn luyện.

Nếu quá tham số hóa trong thời gian được đảm bảo cho các phương pháp DST bởi ba tiêu chí này, Liu et al. [182] chỉ ra rằng các mạng thưa thớt có thể vượt trội hơn các mạng dày đặc, quá tham số hóa tiêu chuẩn. Quá tham số hóa trong thời gian có thể đạt được bằng cách tăng số lượng epoch huấn luyện, hoặc bằng cách giảm kích thước batch trong khi giữ Fp không đổi. Cái sau dẫn đến nhiều cập nhật của ψ(t) trong khi không tăng số lượng epoch huấn luyện.

### 6.2. Các phương pháp liên quan chặt chẽ

Chúng tôi muốn nhấn mạnh rằng tồn tại nhiều phương pháp được gọi là huấn luyện thưa thớt động trong văn học không thỏa mãn định nghĩa của chúng tôi về nó. Chúng tôi không trình bày những phương pháp này ở đây chi tiết vì chúng cho phép cập nhật tất cả trọng số của mạng và chỉ che dấu chúng trong quá trình truyền xuôi. Ví dụ cho những phương pháp như vậy là Guo et al. [34], Sanh et al. [114], Ding et al. [183], Kusupati et al. [184], Liu et al. [185]. Các phương pháp huấn luyện động khác [115, 186–188] lấy mẫu các mạng con khác nhau cho mỗi lần lặp huấn luyện, cập nhật mạng con này trong khi giữ tất cả các tham số chưa được huấn luyện cố định tại vị trí trước đó của chúng. Trong lần lặp huấn luyện tiếp theo, một mạng con mới được lấy mẫu. Do đó, chúng không thể lưu trữ các tham số mạng Θ(t) trong dạng giảm của nó ϑ(t) cho tất cả t ∈ {0, 1, ..., T}. Schwarz et al. [186] bổ sung trao đổi các trọng số tiêu chuẩn Θ(t) thông qua tái tham số hóa sử dụng lũy thừa Θ(t) = ϕ(t)|ϕ(t)|(α-1) với α > 1. Bằng cách làm như vậy, các trọng số gần 0 không có khả năng phát triển. Kết quả là, các tham số sẽ tạo thành một phân phối đuôi nặng tại hội tụ. Điều này cải thiện kết quả vì các tham số được cắt tỉa dựa trên độ lớn của chúng và các phân phối đuôi nặng có khả năng nén cao qua cắt tỉa độ lớn [39]. Sử dụng tái tham số hóa này cũng có thể cải thiện các phương pháp DRT khác được đề xuất trong công trình này.

## 7. Đóng băng các phần của mạng

Trái ngược với cắt tỉa, đóng băng các phần của một mạng được khởi tạo ngẫu nhiên đã thu hút ít sự quan tâm trong nghiên cứu trong những năm gần đây. Lý do chính cho điều này là các trọng số đóng băng, khác không phải được tính trong quá trình truyền xuôi. Do đó, không thể thu được tăng tốc (lý thuyết) cho suy luận. Ngoài ra, các trọng số đóng băng phải được lưu trữ ngay cả sau huấn luyện, trong khi số không chỉ yêu cầu dung lượng bộ nhớ nhỏ. Nhưng, bằng cách sử dụng các bộ tạo số giả ngẫu nhiên để khởi tạo mạng nơ-ron, các trọng số đóng băng có thể được khôi phục với một số nguyên 32bit duy nhất trên chi phí bộ nhớ cho một mạng bị cắt tỉa [58]. Các phương pháp được đề xuất để đóng băng một mạng được tóm tắt trong Bảng 4.

### 7.1. Nền tảng lý thuyết cho đóng băng

Saxe et al. [96] chỉ ra rằng các kiến trúc tích chập-pooling có thể có tính chọn lọc tần số vốn có trong khi sử dụng trọng số ngẫu nhiên. Trong Giryes et al. [97], khoảng cách euclidean và góc giữa các điểm dữ liệu đầu vào được phân tích trong khi truyền qua mạng ReLU với trọng số i.i.d. Gaussian ngẫu nhiên. Với hàm kích hoạt ReLU, mỗi lớp của mạng thu hẹp khoảng cách euclidean giữa các điểm tỷ lệ nghịch với góc euclidean của chúng. Điều này có nghĩa là các điểm với góc ban đầu nhỏ di chuyển gần nhau hơn càng sâu mạng. Giả định dữ liệu hoạt động tốt, có nghĩa là các điểm dữ liệu trong cùng lớp có góc nhỏ và các điểm từ các lớp khác nhau có góc lớn hơn giữa nhau, các mạng Gaussian ngẫu nhiên có thể được xem như một hệ thống phổ quát tách bất kỳ dữ liệu nào. Mặt khác, nếu dữ liệu không hoàn hảo, huấn luyện có thể cần thiết để vượt qua các góc lớn trong lớp hoặc các góc nhỏ giữa lớp để đạt tổng quát hóa tốt.

Đóng băng tham số tại giá trị ban đầu của chúng được sử dụng trong Li et al. [98] để đo chiều nội tại của cảnh quan mục tiêu. Trước tiên, mạng dày đặc được tối ưu hóa để tạo ra nghiệm tốt nhất có thể. Sau đó, số lượng tham số đóng băng được giảm dần, bắt đầu bằng đóng băng tất cả tham số. Các tham số của mạng được tính theo (7), với ψ(t) = ψ(0) bằng một phép chiếu trực giao ngẫu nhiên. Nếu hiệu suất tương tự như nghiệm dày đặc được đạt, số lượng tham số không đóng băng xác định chiều nội tại của cảnh quan mục tiêu. Trong khi sử dụng trọng số đóng băng chỉ như một công cụ để đo chiều của một vấn đề, Li et al. [98] chỉ ra rằng đóng băng tham số có thể dẫn đến kết quả cạnh tranh.

### 7.2. Các phương pháp đóng băng

Chúng tôi muốn bắt đầu với cái gọi là máy học cực đoan (ELM) [55, 94, 95]. ELM là MLP, thường với một lớp ẩn. Các tham số của nút ẩn được đóng băng và thường được khởi tạo ngẫu nhiên. Tuy nhiên, lớp phân loại được huấn luyện bằng một nghiệm dạng đóng của hồi quy bình phương tối thiểu [55]. ELM là xấp xỉ phổ quát nếu số lượng nút ẩn được chọn đủ lớn [94]. Trong các mạng SOTA, ELM có thể được sử dụng để thay thế lớp phân loại [95]. Liên quan chặt chẽ đến ELM là mạng nơ-ron liên kết hàm vector ngẫu nhiên [189, 190], ngoài ra còn cho phép liên kết giữa lớp đầu vào và đầu ra.

Một phương pháp hoàn toàn trực giao với ELM được đề xuất trong Hoffer et al. [56], trong đó lớp phân loại được thay thế bằng một ma trận trực giao ngẫu nhiên. Đối với lớp phân loại, chỉ một tham số nhiệt độ T được học. Thí nghiệm chỉ ra rằng lớp trực giao ngẫu nhiên với T được tối ưu hóa mang lại kết quả so sánh được với một bộ phân loại có thể huấn luyện cho các kiến trúc hiện đại trên CIFAR-10/100 [191] và ImageNet [177].

Trong Zhou et al. [119], cắt tỉa trọng số và đóng băng chúng tại giá trị ban đầu của chúng được so sánh trong khung LTH. Họ chỉ ra rằng đóng băng thường hoạt động tốt hơn cho số lượng thấp tham số được huấn luyện trong khi cắt tỉa có kết quả tốt hơn nếu nhiều tham số được huấn luyện. Cuối cùng, họ chỉ ra rằng một sự kết hợp của cắt tỉa và đóng băng – tùy thuộc vào việc một trọng số di chuyển về phía không hay xa khỏi không trong quá trình huấn luyện dày đặc – đạt kết quả tốt nhất.

Đóng băng tại khởi tạo được sử dụng trong Wimmer et al. [58] để vượt qua vấn đề gradient biến mất cho PaI. Tương tự như Zhou et al. [119], họ chỉ ra rằng đóng băng vượt trội hơn cắt tỉa nếu chỉ một vài tham số được huấn luyện. Đối với tỷ lệ đóng băng cao, đóng băng vẫn đảm bảo luồng thông tin đầy đủ trong mạng được huấn luyện thưa thớt. Mặt khác, nếu một số lượng tham số cao hơn được huấn luyện, cắt tỉa cũng hoạt động tốt hơn đóng băng trong thiết lập này. Nhưng, bằng cách sử dụng suy giảm trọng số trên các tham số đóng băng, Wimmer et al. [58] có thể có được điều tốt nhất từ cả hai thế giới, trọng số đóng băng tại đầu huấn luyện để đảm bảo luồng thông tin trung thực và mạng thưa thớt ở cuối nó, trong khi cải thiện cả hai cùng một lúc.

Được truyền cảm hứng từ học chuyển giao, Sung et al. [59] đóng băng các phần lớn của mô hình để giảm kích thước của phần mới được học của mạng. Các trọng số đóng băng được chọn theo tầm quan trọng của chúng để thay đổi đầu ra của mạng, được đo bằng ma trận thông tin Fisher. Họ chỉ ra rằng đóng băng tham số giúp giảm chi phí truyền thông trong huấn luyện phân tán cũng như yêu cầu bộ nhớ để checkpoint mạng trong quá trình huấn luyện. Không đáng ngạc nhiên, họ đạt kết quả tốt nhất nếu mặt nạ đóng băng được phép thay đổi trong quá trình huấn luyện so với giữ mặt nạ đóng băng cố định.

Rosenfeld và Tsotsos [57] đóng băng tham số theo cách có cấu trúc. Họ chủ yếu tập trung vào huấn luyện chỉ một phần các bộ lọc (tức là kênh đầu ra) và xác định các phần đóng băng ngẫu nhiên. Trong thiết lập này, đóng băng vượt trội hơn cắt tỉa cho hầu như tất cả số lượng trọng số được huấn luyện.

Hơn nữa, được chỉ ra trong Rosenfeld và Tsotsos [57], Frankle et al. [192] rằng đóng băng tất cả trọng số ngoại trừ các tham số chuẩn hóa batch [79] có thể huấn luyện dẫn đến hiệu suất không tầm thường.

## 8. So sánh và thảo luận các phương pháp huấn luyện giảm chiều khác nhau

Trong Phần này, chúng tôi thảo luận các phương pháp DRT khác nhau được giới thiệu trong Phần 4, 5, 6 và 7. Hình 3 cho thấy so sánh cấp cao giữa LTH/PaI, DST và đóng băng. Dựa vào so sánh cấu trúc này, bây giờ chúng tôi sẽ thảo luận các khía cạnh khác nhau của các phương pháp.

### 8.1. Hiệu suất.

Chúng tôi bắt đầu với tiêu chí quan trọng nhất, hiệu suất của các phương pháp. Với hiệu suất, chúng tôi có ý nghĩa độ chính xác kiểm tra (hoặc các chỉ số khác nhau cho các nhiệm vụ khác ngoài phân loại hình ảnh) sau huấn luyện. Khảo sát này bao gồm các phương pháp giảm chi phí huấn luyện. Do đó, hiệu suất cần được đưa vào bối cảnh với chi phí cho phương pháp. Làm baseline để so sánh các phương pháp khác nhau, chúng tôi sẽ sử dụng thước đo chi phí chính của chúng tôi – số lượng tham số được huấn luyện (hoặc tương đương được lưu trữ).

Để đánh giá hiệu suất, chúng tôi sử dụng LTH với quay ngược muộn. Lưu ý, đây không phải là phương pháp DRT thực sự vì huấn luyện bắt đầu với một tập con của Θ(t) cho t nhỏ > 0 ở đây. Tuy nhiên, văn học chỉ báo cáo kết quả cho các mạng hiện đại và các nhiệm vụ quy mô lớn như ImageNet cho LTH với quay ngược muộn. Như đã đề cập trong Phần 4, đặt lại trọng số về khởi tạo của chúng cho thấy kết quả tệ hơn so với quay ngược muộn nên cần được ghi nhớ.

Đã được chỉ ra và thảo luận trong nhiều công trình rằng LT có kết quả tốt hơn PaI cho tỷ lệ cắt tỉa cao [49, 124]. Cũng vậy, DST cải thiện kết quả so với PaI [117, 118]. Như một ví dụ, Bảng 5 so sánh PaI, LTH và DST cho ResNet50 [65] trên ImageNet [177]. Bảng 5 cho thấy LT xấp xỉ đạt cùng hiệu suất như DST. Cả hai đều vượt trội hơn PaI. Nhưng, nếu thời gian huấn luyện dành cho các phương pháp DST được tăng gấp đôi, DST vượt qua LT [182], xem cũng thảo luận trong Phần 6. Thời gian huấn luyện tăng gấp đôi cho DST là so sánh công bằng, vì LT cần ít nhất gấp đôi thời gian huấn luyện so với phương pháp DST tiêu chuẩn. Mặt khác, kết quả LTH được báo cáo từ Evci et al. [193] không sử dụng cách tiêu chuẩn để tìm LT [120], mà cắt tỉa độ lớn dần [147] để tìm mạng con thưa thớt. Do đó, bằng cách sử dụng phương pháp huấn luyện-cắt tỉa-quay ngược trọng số lặp [120], hiệu suất LT có thể được cải thiện hơn nữa. Hơn nữa, Bảng 5 cho thấy cắt tỉa hệ số w.r.t. cơ sở thích ứng [62] cho các bộ lọc K×K cải thiện hiệu suất của các phương pháp SOTA hơn nữa.

Hiệu suất kém hơn của PaI không phải là bất ngờ vì các phương pháp PaI rõ ràng là những phương pháp ít tinh vi hơn. Như chúng tôi sẽ thấy trong các thảo luận sau, sự đơn giản này sẽ giảm chi phí ở các cấp độ khác, như thời gian huấn luyện hoặc điều chỉnh siêu tham số. Quan sát đầu tiên của chúng tôi do đó là, huấn luyện trước mạng để tìm một kiến trúc thưa thớt có thể huấn luyện tốt (LT) hoặc thích ứng kiến trúc thưa thớt trong quá trình huấn luyện (DST) cải thiện kết quả so với việc tìm một kiến trúc thưa thớt ngay tại đầu huấn luyện.

Đóng băng các tham số được so sánh với cắt tỉa chúng trong thiết lập PaI [58] và thiết lập LTH [119]. Thông thường, đóng băng dẫn đến kết quả hơi tệ hơn so với cắt tỉa cho số lượng tham số được huấn luyện cao hơn và kết quả tốt hơn cho ít tham số được huấn luyện hơn. Hình 9 cho thấy so sánh giữa cắt tỉa và đóng băng trọng số cho số lượng thấp trọng số được huấn luyện trong thiết lập PaI. Price và Tanner [93] chỉ ra rằng điều tương tự đúng nếu lớp dày đặc ngẫu nhiên được thay thế bằng DCT dày đặc. Sử dụng DCT có lợi thế là chúng rẻ để tính.

### 8.2. Chi phí lưu trữ

Như được giới thiệu trong Phần 2.3, các kỹ thuật như định dạng hàng thưa thớt nén [99] được sử dụng để lưu trữ các mạng thưa thớt/đóng băng sau huấn luyện. Bằng cách sử dụng khởi tạo được rút ra từ một bộ tạo số giả ngẫu nhiên,

memory(freeze) = memory(prune) + 32 bit                    (21)
                ≈ memory(prune)                    (22)

giữ. Do đó, cắt tỉa và đóng băng có cùng chi phí bộ nhớ trong trường hợp này. Nhưng, nếu không thể sử dụng bộ tạo số giả ngẫu nhiên, cắt tỉa có yêu cầu bộ nhớ thấp hơn nhiều so với đóng băng.

Cũng vậy, Phần 4 - 7 đề xuất các phương pháp không sử dụng phép biến đổi cắt tỉa/đóng băng đơn giản mà còn một phép biến đổi tuyến tính bổ sung để nhúng các tham số nhỏ, có thể huấn luyện ϑ ∈ Rd vào không gian lớn hơn RD. Ví dụ cho điều này là cắt tỉa kết hợp với phép biến đổi cơ sở bổ sung với các khối chéo được chia sẻ [62] hoặc một phép chiếu trực giao ngẫu nhiên [98]. Chi phí để lưu trữ những nhúng này không đáng kể cho Wimmer et al. [62]. Điều tương tự đúng cho Li et al. [98] nếu phép biến đổi ngẫu nhiên được tạo bằng bộ tạo số giả ngẫu nhiên.

### 8.3. Chi phí huấn luyện

Trong phần sau, chúng tôi sẽ so sánh chi phí huấn luyện cho các phương pháp khác nhau chủ yếu được đo bằng thời gian huấn luyện cho một mô hình thưa thớt và số lượng siêu tham số có thể điều chỉnh cho huấn luyện ngầm xác định số lần chạy huấn luyện cần thiết tổng thể. Trên đó, một số phương pháp tạo ra tính toán gradient bổ sung trước hoặc trong quá trình huấn luyện sẽ được thảo luận ở cuối.

#### 8.3.1. Thời gian huấn luyện

Chúng tôi giả định rằng tất cả các phương pháp, DST, LT, PaI và đóng băng huấn luyện mô hình thưa thớt cuối cùng trong T lần lặp. Ở đây, T là số lần lặp huấn luyện được sử dụng cho mạng dày đặc để (i) hội tụ và (ii) có khả năng tổng quát hóa tốt. Do đó, DST, PaI và đóng băng có thời gian huấn luyện xấp xỉ giống nhau cho một mô hình. Mặt khác, LT cần, như đã thảo luận trong Phần 4, ít nhất gấp đôi số lần lặp huấn luyện so với huấn luyện tiêu chuẩn. Bằng cách sử dụng các chu trình huấn luyện-cắt tỉa-đặt lại lặp, số lần lặp huấn luyện có thể dễ dàng đạt 10−20× số tiêu chuẩn. Do đó, LT cần một lượng lớn huấn luyện trước cho phép biến đổi cắt tỉa ψ(0).

Như được chỉ ra trong Liu et al. [182] và thảo luận trong Phần 6, DST hưởng lợi lớn từ việc tăng thời gian huấn luyện.

#### 8.3.2. Siêu tham số

Tất cả các phương pháp được đề xuất tạo ra ít nhất một siêu tham số nhiều hơn so với huấn luyện mô hình dày đặc tương ứng. Siêu tham số này được cho bởi số lượng tham số có thể huấn luyện.⁶

**Xác định số lượng tham số được huấn luyện.** PaI, đóng băng và LT xác định một tỷ lệ cắt tỉa/đóng băng toàn cục. Mặt khác, các phương pháp DST thường cần chỉ định tỷ lệ tham số được huấn luyện cho mỗi lớp riêng biệt. Nếu tỷ lệ cắt tỉa theo lớp có thể thích ứng động trong quá trình huấn luyện [118], lựa chọn ban đầu của chúng đã được chỉ ra là không quá quan trọng và có thể được đặt không đổi cho tất cả các lớp. Các công trình khác như Mocanu et al. [116], Evci et al. [117], Liu et al. [182] xác định heuristic tỷ lệ cắt tỉa theo lớp bằng mô hình Erdős-Rényi [195].

**Siêu tham số bổ sung cho PaI.** Bên cạnh tỷ lệ cắt tỉa, các phương pháp PaI giới thiệu siêu tham số để tính phép biến đổi cắt tỉa ψ(0). Đối với Lee et al. [48], Wang et al. [49], Verdenius et al. [102], de Jorge et al. [126], đây là số lượng batch dữ liệu, cần thiết để tính điểm cắt tỉa của mỗi tham số. Các phương pháp lặp Tanaka et al. [50], Verdenius et al. [102], de Jorge et al. [126] cần xác định số lần lặp cắt tỉa được thực hiện. Tuy nhiên, bằng cách sử dụng số lượng cao batch dữ liệu và nhiều lần lặp cắt tỉa, những tham số này không cần được tinh chỉnh thêm [49, 50]. Một trường hợp đặc biệt trong thiết lập này là Wimmer et al. [61], nội suy giữa hai phương pháp cắt tỉa Lee et al. [48] và Wang et al. [49]. Do đó, họ cần điều chỉnh một siêu tham số để cân bằng giữa hai phương pháp này.

**Siêu tham số bổ sung cho LT.** Như được chỉ ra trong Frankle et al. [120], đặt lại trọng số không về giá trị ban đầu của chúng mà về giá trị đạt được sớm trong huấn luyện dẫn đến kết quả tốt hơn. Do đó, một lần lặp quay ngược hoạt động tốt phải được tìm. Một phân tích thực nghiệm phù hợp cho các tập dữ liệu và mô hình khác nhau được đưa ra trong Frankle et al. [120]. Hơn nữa, nếu cắt tỉa lặp được sử dụng, số lượng trọng số bị cắt tỉa trong mỗi lần lặp phải được xác định. Thông thường, 20% trọng số khác không bị loại bỏ trong mỗi lần lặp [47].

**Siêu tham số bổ sung cho đóng băng.** Đóng băng liên quan chặt chẽ đến PaI [58], LT [119] hoặc cắt tỉa ngẫu nhiên [57]. Vì đóng băng không giới thiệu siêu tham số bổ sung, cùng số lượng siêu tham số bổ sung như cho các phương pháp cắt tỉa tương ứng là cần thiết. Lưu ý, để đạt kết quả tốt với trọng số đóng băng ngẫu nhiên, tỷ lệ đóng băng theo lớp phải được xác định cần được tinh chỉnh phù hợp.

**Siêu tham số bổ sung cho DST và chi phí để cập nhật ψ(t).** So với PaI, DST có lợi thế là phép biến đổi cắt tỉa không cố định tại ψ(0). Điều này dẫn đến hiệu suất tốt hơn sau huấn luyện trong khi sử dụng cùng số lượng tham số được huấn luyện và lần lặp huấn luyện. Nhưng điều này cũng dẫn đến chi phí để xác định ψ(t). Bên cạnh tính ψ(t), tần suất cập nhật Fp và tỷ lệ cắt tỉa cập nhật q(t)l phải được tìm. Tỷ lệ cắt tỉa cập nhật q(t)l được định nghĩa là tỷ lệ các tham số khác không trước đó được cắt tỉa mới trong lớp l nếu ψ(t) được cập nhật trong lần lặp t. Lưu ý, tỷ lệ cắt tỉa cập nhật có thể thay đổi giữa các lần lặp cập nhật khác nhau của phép biến đổi và cũng giữa các lớp. Cũng vậy, tỷ lệ tái tạo cần được xác định vì nó có thể khác với tỷ lệ cắt tỉa cập nhật [118].

Cuối cùng, có chi phí thực tế để cập nhật ψ(t) chính nó. Trong tất cả các phương pháp được xem xét trong Phần 6, các trọng số được làm khô theo việc có độ lớn nhỏ nhất – cắt tỉa dựa trên độ lớn. Để làm điều này, d hệ số chưa bị cắt tỉa phải được sắp xếp. Sau khi đặt một số trọng số về không, cùng số lượng trọng số phải được gắn cờ là có thể huấn luyện lại. Chi phí để tái tạo trọng số có thể gần như miễn phí bằng cách sử dụng tái tạo ngẫu nhiên trọng số [92, 116]. Mặt khác, Evci et al. [117] yêu cầu gradient của toàn bộ mạng tại các lần lặp cập nhật t ∈ {Fp, 2Fp, ...} để xác định ψ(t). Dettmers và Zettlemoyer [118] thậm chí cần những gradient này trong mỗi lần lặp huấn luyện để cập nhật một tham số momentum cũng yêu cầu một siêu tham số trọng số bổ sung. Một tham số nhiệt độ được cần trong Bellec et al. [60] để mô hình hóa bước đi ngẫu nhiên trong không gian tham số để khám phá trọng số mới.

#### 8.3.3. Tính toán gradient và quá trình truyền ngược.

Tất cả các phương pháp được đề xuất cập nhật chỉ các phần thưa thớt của trọng số qua backpropagation. Nhưng, một số trong số chúng bổ sung yêu cầu tính toán gradient dày đặc cho huấn luyện trước, xác định ψ(0) hoặc để cập nhật ψ(t).

Trước hết, LT cần huấn luyện mạng dày đặc để tìm ψ(0). Điều này có thể hạn chế kích thước của mạng dày đặc cơ bản lớn nhất có thể được sử dụng. Đối với LTH lặp, các mạng với độ thưa thớt pk = 1 - 0.8k phải được huấn luyện bổ sung cho tất cả k với pk > p.

Các phương pháp PaI cần tính toán gradient dày đặc trước huấn luyện [48, 50, 102, 126]. Thậm chí một tích Hessian-vector phải được tính cho phương pháp Wang et al. [49]. Nhưng sau khi tìm thấy ψ(0), các phương pháp PaI được huấn luyện với một kiến trúc cố định, thưa thớt mà không cần tính gradient dày đặc nữa.

Các phương pháp DST có thể không cần gradient dày đặc chút nào, nếu các trọng số tái tạo được tìm bằng lựa chọn ngẫu nhiên [60, 92, 116]. Như đã đề cập ở trên, Evci et al. [117] tính gradient dày đặc tại mỗi bước cập nhật của phép biến đổi cắt tỉa t ∈ {Fp, 2Fp, ...}. Trường hợp cực đoan nhất được đưa ra cho Dettmers và Zettlemoyer [118] cần tính gradient dày đặc trong mỗi lần lặp.

Cuối cùng có sự khác biệt giữa tính toán gradient cho các mô hình bị cắt tỉa và đóng băng. Các mạng bị cắt tỉa tính gradient của bản đồ kích hoạt với tensor trọng số thưa thớt. Mặt khác, các mạng đóng băng tính gradient của bản đồ kích hoạt với sự trợ giúp của các trọng số đóng băng, tức là yêu cầu tính toán dày đặc, xem phía bên phải Hình 4. Tuy nhiên, các trọng số đóng băng không cần được cập nhật. Do đó, đủ chỉ tính một phần thưa thớt của gradient trọng số. Tóm lại, đóng băng cũng giảm tính toán trong quá trình truyền ngược, nhưng không nhiều bằng cắt tỉa.

### 8.4. Quá trình truyền xuôi.

Trong Phần 8.3.3, tính toán gradient và quá trình truyền ngược cho các phương pháp khác nhau được thảo luận. Gradient chỉ phải được tính trong quá trình huấn luyện trong khi quá trình truyền xuôi cần thiết cho cả huấn luyện và suy luận. Do đó, chúng tôi thảo luận quá trình truyền xuôi trong một Phần độc lập. Tuy nhiên, thảo luận này cũng nên được xem như một phần của chi phí huấn luyện, Phần 8.3.

Trong và sau huấn luyện thưa thớt, LTH, DST và PaI đều có cùng chi phí cho suy luận. Tuyên bố này không hoàn toàn đúng, vì các phương pháp khác nhau có thể tạo ra các phân phối độ thưa thớt khác nhau cho cùng tỷ lệ cắt tỉa toàn cục. Điều này có thể dẫn đến chi phí FLOP khác nhau để suy luận các mạng thưa thớt, xem ví dụ Bảng 5. Tuy nhiên, phân tích phân phối độ thưa thớt được thu được bởi các phương pháp cắt tỉa khác nhau nằm ngoài phạm vi của công trình này.

Nếu các phần của mạng được đóng băng trong quá trình huấn luyện, tất cả tham số tham gia vào quá trình truyền xuôi. Tất nhiên, điều này cũng đúng cho thời gian suy luận. Do đó, chi phí tính toán cho suy luận không thể được giảm và bằng mạng được huấn luyện dày đặc.

### 8.5. Tóm tắt

Tóm lại, chúng ta thấy rằng đóng băng tham số thay vì cắt tỉa chúng dẫn đến cùng yêu cầu bộ nhớ và kết quả tốt hơn để huấn luyện số lượng thấp tham số. Tuy nhiên, nếu nhiều tham số được huấn luyện, cắt tỉa dẫn đến hiệu suất bằng hoặc tốt hơn. Đối với cắt tỉa, độ thưa thớt của tham số có thể giảm số lượng tính toán cần thiết để đánh giá mạng nếu phần mềm và phần cứng được sử dụng hỗ trợ tính toán thưa thớt. Do đó, đóng băng dường như là lựa chọn tốt nếu kích thước mô hình là nút thắt cổ chai, tức là chỉ một phần nhỏ của mạng được huấn luyện, trong khi cắt tỉa được ưa dùng sử dụng khác. Hơn nữa, chi phí tính toán được tạo ra bởi đóng băng lớp có thể được giảm bằng cách sử dụng DCT dày đặc thay vì lớp đóng băng dày đặc.

Đối với cắt tỉa, có sự đánh đổi giữa tính đơn giản của phương pháp và hiệu suất sau huấn luyện. Như đã chỉ ra, DST và LT có khả năng tổng quát hóa xấp xỉ giống nhau và vượt trội hơn PaI cho cùng số lượng tham số khác không. Tuy nhiên, PaI đảm bảo một kiến trúc thưa thớt cố định trong suốt quá trình huấn luyện, được xác định bởi chỉ một vài tính toán gradient dày đặc. Hơn nữa, PaI không yêu cầu điều chỉnh siêu tham số rộng rãi và tốn kém. Tìm LT yêu cầu huấn luyện trước đắt đỏ của mạng dày đặc. Cũng vậy, LT tại khởi tạo thường cho thấy kết quả không thỏa đáng cho các kiến trúc mạng hiện đại và tập dữ liệu quy mô lớn. Đặt lại trọng số về một giai đoạn sớm trong huấn luyện được yêu cầu để đạt hiệu suất tốt. Như đã chỉ ra, các phương pháp DST cần nhiều siêu tham số hơn PaI và LTH. Hơn nữa, các phương pháp DST có thể cần tính toán gradient đầy đủ của mạng thường xuyên và tăng thời gian huấn luyện để đạt hiệu suất tốt nhất của chúng.

Như đã đề cập, kết quả được trình bày cho LT sử dụng một bước huấn luyện trước nhỏ cho khởi tạo thưa thớt. Do đó, chúng tôi kết luận rằng các phương pháp DST đạt hiệu suất tổng thể tốt nhất cho huấn luyện hoàn toàn thưa thớt. Hơn nữa, việc sử dụng cơ sở thích ứng cho các bộ lọc K×K của các lớp tích chập cải thiện thêm các phương pháp cắt tỉa và đóng băng được đề xuất.

## 9. Kết luận

Trong công trình này, chúng tôi đã giới thiệu một khung tổng quát để mô tả việc huấn luyện DNN với chiều giảm (DRT). Các phương pháp được đề xuất được chi phối bởi cắt tỉa mạng nơ-ron, nhưng chúng tôi cũng thảo luận việc đóng băng các phần của mạng tại khởi tạo ngẫu nhiên của nó. Các phương pháp được phân loại thành cắt tỉa tại khởi tạo (PaI), vé số may mắn (LT), huấn luyện thưa thớt động (DST) và đóng băng. Các phương pháp SOTA cho mỗi tiêu chí được trình bày. Hơn nữa, các phương pháp khác nhau được so sánh sau đó.

Chúng tôi đầu tiên thảo luận rằng cắt tỉa dẫn đến kết quả tốt hơn so với đóng băng nếu nhiều tham số được tối ưu hóa trong khi đối với số lượng thấp trọng số được huấn luyện, đóng băng hoạt động tốt hơn cắt tỉa. Hơn nữa, LT và DST hoạt động tốt hơn PaI, trong khi PaI chứa các phương pháp dễ nhất và ít tốn kém nhất. Đối với LT, nhiều lần huấn luyện, bao gồm huấn luyện mô hình dày đặc, cần thiết để tìm kiến trúc thưa thớt. Cũng vậy, LT đạt kết quả tốt nhất nếu trọng số được đặt lại về một giai đoạn sớm trong huấn luyện, điều này không mang lại huấn luyện thưa thớt hoàn toàn. Các phương pháp DST thường cần nhiều siêu tham số để điều chỉnh hơn LT và PaI. Tất cả các phương pháp DRT được đề xuất có thể được tăng cường thêm bằng cách biểu diễn các bộ lọc tích chập với một biểu diễn thích ứng thay vì biểu diễn không gian tiêu chuẩn.

Tóm lại, việc tìm phương pháp DRT tốt nhất cho một thiết lập cụ thể là sự đánh đổi giữa thời gian huấn luyện có sẵn và hiệu suất của mạng cuối cùng. Thời gian huấn luyện chủ yếu bị ảnh hưởng bởi số lần huấn luyện cần thiết để tìm kiến trúc thưa thớt, từ 0 (PaI, DST và đóng băng) đến hơn 20× (LT), và số lần huấn luyện riêng lẻ cần thiết để điều chỉnh siêu tham số. Cũng vậy, nhu cầu tính gradient dày đặc (cho một số phương pháp DST và PaI) hoặc huấn luyện trước mạng dày đặc (LT) phải được xem xét nếu một phương pháp DRT được chọn. Như đã thảo luận, các mô hình đóng băng có chi phí tính toán tương tự cho suy luận như các mạng được huấn luyện dày đặc. Do đó, nếu độ thưa thớt cùng với phần mềm và phần cứng được sử dụng thực sự có thể giảm tính toán, cắt tỉa nên được ưa dùng hơn đóng băng, hoặc các tham số đóng băng nên được thay thế bằng các phép biến đổi bảo tồn thông tin rẻ để tính, ví dụ như DCT.
