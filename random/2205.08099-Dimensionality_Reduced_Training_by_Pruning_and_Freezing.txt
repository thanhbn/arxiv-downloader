# 2205.08099.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/random/2205.08099.pdf
# File size: 1028694 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Dimensionality Reduced Training by Pruning and Freezing
Parts of a Deep Neural Network, a Survey✩
Paul Wimmera,b,∗, Jens Mehnerta, Alexandru Paul Condurachea,b
a1Robert Bosch GmbH, Automated Driving Research, Burgenlandstrasse 44, 70469 Stuttgart, Germany
bUniversity of L ¨ubeck, Institute for Signal Processing, Ratzeburger Allee 160, 23562 L ¨ubeck, Germany
Abstract
State-of-the-art deep learning models have a parameter count that reaches into the bil-
lions. Training, storing and transferring such models is energy and time consuming,
thus costly. A big part of these costs is caused by training the network. Model com-
pression lowers storage and transfer costs, and can further make training more efficient
by decreasing the number of computations in the forward and/or backward pass. Thus,
compressing networks also at training time while maintaining a high performance is an
important research topic. This work is a survey on methods which reduce the number
of trained weights in deep learning models throughout the training. Most of the intro-
duced methods set network parameters to zero which is called pruning. The presented
pruning approaches are categorized into pruning at initialization, lottery tickets and dy-
namic sparse training. Moreover, we discuss methods that freeze parts of a network at
its random initialization. By freezing weights, the number of trainable parameters is
shrunken which reduces gradient computations and the dimensionality of the model’s
optimization space. In this survey we first propose dimensionality reduced training as
an underlying mathematical model that covers pruning and freezing during training.
Afterwards, we present and discuss different dimensionality reduced training methods.
Keywords: Survey, Pruning, Freezing, Lottery Ticket Hypothesis, Dynamic Sparse
Training, Pruning at Initialization
1. Introduction
In recent years, deep neural networks (DNNs) have shown state-of-the-art (SOTA)
performance in many artificial intelligence applications, like image classification [1],
speech recognition [2] or object detection [3]. These applications require optimizing
✩This preprint has not undergone peer review or any post-submission improvements or corrections. The
Version of Record of this article is published in Artificial Intelligence Review (2023) , and is available online
athttps://doi.org/10.1007/s10462-023-10489-1 .
∗Corresponding author with1as main address
Email addresses: Paul.Wimmer@de.bosch.com (Paul Wimmer),
JensEricMarkus.Mehnert@de.bosch.com (Jens Mehnert),
AlexandruPaul.Condurache@de.bosch.com (Alexandru Paul Condurache)
Preprint May 26, 2023arXiv:2205.08099v2  [cs.LG]  25 May 2023

--- PAGE 2 ---
Figure 1: (From the OpenAI blog post [11]) Evolution of the number of computations required for training
SOTA models in the first era and in the modern era of AI systems.
large models with up to billions of parameters. Training and testing such large mod-
els has been made possible due to technological advances. As a consequence, SOTA
models are trained on specialized hardware for fast tensor computations, like GPUs
or TPUs. Usually, not only one GPU/TPU is utilized to train these models but many
of them. For example, XLNet [4] needs 5.5days of training on 512TPU v3 chips.
Not only training, but transferring, storing and evaluating big models is costly, too [5].
In order to train SOTA models, huge amount of data is required [6–9] which needs
resources for collecting, labeling, storing and transferring it. Of course, the large num-
ber of parameters and data, along with high computational demands lead to excessive
energy consumption for training and evaluating deep learning (DL) models. For ex-
ample, training a big transformer in conjunction with a previously performed neural
architecture search results in emissions of 284tCO2[10]. This is about 315×theCO2
emissions of a passenger traveling by air from New York City to San Francisco.
Between 2012 and2018 , computations required for DL research have been in-
creased by estimated 300,000 times which corresponds to doubling the amount of
computations every few months [5], see Figure 1. This rate outruns by far the pre-
dicted one by Moore’s Law [12]. The performance improvements of DL models are
to a great extent induced by raising the number of parameters in the networks and/or
increasing the number of computations needed to train and infer the network [10, 13].
Orthogonal to the development of new, big SOTA models which need massive amount
2

--- PAGE 3 ---
Dense Initialization Θ(0)
Score Θ(0)
(a)ψ(0)byscoring Θ(0)Construct
corresponding
ψ(0)
Initialψ(0),
(b)Pre-trained ψ(0)Use
copy
to
pre-
train
ψ(0)
Pre-train
ψ(0)
ScoresConstruct
corresponding
ψ(0)
(c)Randomly generated ψ(0)Probability
distributionDraw iidLow dimensional
initialization ϑ(0)
Project to
low dimensional space
+· =Initial embedding Ψ(0)
Aﬃne partχ(0)of
Ψ(0)(0 for pruning)Trainϑ(t)via SGD
If
t%Fp= 0Update
ψ(t)
/mapsto→
Ψ(T)
ϑ(T)
Θ(T)
Optimized sparse
representation
Θ(T)= Ψ(T)(ϑ(T))Train forT
iterations.Figure 2: Graphical overview of the proposed dimensionality reduced training methods with a low dimen-
sional initialization ϑ(0)∈Rd. To use superior trainability of big networks, the low dimensional space must
be embedded in a bigger space, modeled by an embedding Θ(0)= Ψ(0)(ϑ(0)) =ψ(0)·ϑ(0)+χ(0)where
Θ(0)∈RD,d≪D. Here, χ(0)is the optional affine part modeling the frozen parameters. The embedding
is obtained by either (a) scoring the big random initialization Θ(0), (b) using a pre-training step to generate
ψ(0)or (c) mapping ϑ(0)onto a randomly chosen subset of Θ(0). During training, only the low dimensional
ϑ(0)is learned. Furthermore, the embedding Ψ(t)can be adjusted to improve results, see Section 6.
of data, hardware resources and training time it is important to simultaneously improve
parameter, computational, energy and data efficiency of DL models.
Model compression lowers storage and transfer costs, speeds up inference by re-
ducing the number of computations or accelerates the training which uses less energy.
It can be achieved by methods such as quantization ,weight sharing ,tensor decompo-
sition ,low rank tensor approximation ,pruning orfreezing . Quantization reduces the
number of bits used to represent the network’s weights and/or activation maps [14–
20]. 32bit floats are replaced by low precision integers, thereby decreasing memory
consumption and speeding up inference. Memory reduction and speed up can also be
achieved by weight sharing [21–23], tensor decomposition [24–26] or low rank tensor
approximation [27–29] to name only a few. Network pruning [14, 30–35] sets parts of
a DNN’s weights to zero. This can help to reduce the model’s complexity and memory
requirements, speed up inference [36] and even improve the network’s generalization
ability [33, 37–39].
Pruning DNNs can be divided into structured andunstructured pruning. Structured
pruning removes channels, neurons or even coarser structures of the network [40–46].
This generates leaner architectures, resulting in reduced computational time. A more
fine-grained approach is given by unstructured pruning where single weights are set to
zero [30–33, 47–50]. Unstructured pruning usually leads to a better performance than
structured pruning [51, 52] but specialized soft- and hardware which supports sparse
tensor computations is needed to actually reduce the runtime [53, 54]. For an overview
of the current SOTA pruning methods we refer to Blalock et al. [36].
Freezing a DNN means that only parts of the network are trained, whereas the
3

--- PAGE 4 ---
remaining ones are frozen at their initial/pre-trained values [55–59]. This leads to faster
convergence of the networks [55] and reduced communication costs for distributed
training [59]. Furthermore, freezing reduces memory requirements if the networks are
initialized with pseudorandom numbers [58].
In recent years, training only a sparse part of the network, i.e. a network with many
weights fixed at zero or their randomly initialized value, became of interest to the DL
community [47–50, 58, 60–62], providing the benefit of reduced memory requirements
and the potential of reduced runtime not only for inference but also for training. Dif-
fenderfer and Kailkhura [63] even show that sparsely trained models are more robust
against distributional shifts than (i) their densely trained counterpart and (ii) networks
pruned with classical techniques. With classical techniques we denote pruning dur-
ing or after training where the sparse network is fine-tuned afterwards. A high level
overview of recent approaches to prune a network at initialization is given in Wang
et al. [64]. In our work, the underlying mathematical framework is generalized to in-
volve other dimensionality reducing training methods. Moreover, our work discusses
the analyzed methods in more detail.
Scope of this work. As mentioned above, there are many possibilities to reduce the
cost of a DL framework. In this work, we focus on methods that reduce the number
of trainable parameters in a DL model. For this, we reversely define dimensionality
reduction by embedding a low dimensional space into a high dimensional one, see
Section 2.2. The high dimensional space corresponds to the original DNN, whereas
the low dimensional one is the space where the DNN is actually trained. Therefore,
training proceeds with reduced dimensionality.
Pruning andfreezing parts of the network during training are two methods that yield
dimensionality reduced training (DRT). This survey compares different strategies for
pruning and freezing networks during training. Figure 2 shows a graphical overview of
the proposed DRT methods. A structural comparison between freezing DNNs at initial-
ization and pruning them is given in Figure 3. For pruning, we differentiate between
pruning at initialization (PaI) which trains a fixed set of parameters in the network and
dynamic sparse training (DST) which adapts the set of trainable parameters during
training. Closely related to PaI is the so called Lottery Ticket Hypothesis (LTH) which
uses well trainable sparse subnetworks, so called Lottery Tickets (LTs). LTs are ob-
tained by applying train-prune-reset cycles to the network’s parameters, starting with
the dense network and finally chiseling out the subnetwork at initialization with desired
sparsity.
Structure of this work. First we propose the problem formulation and mathematical
setup of this survey in Section 2. Then, we discuss different possibilities to reduce the
network’s dimensionality in Section 3. The LTH and PaI are introduced in Sections
4 and 5, respectively. This is followed by a comparison of different DST methods in
Section 6. As last DRT method we present freezing initial parameters in Section 7.
Subsequently, the different DRT methods from Sections 4 - 7 are compared at a high
level in Section 8. The survey is closed by drawing conclusions in Section 9.
For better readability, the Figures should best be printed in color or viewed in the
colored online version.
4

--- PAGE 5 ---
Training with Dimensionality Reduction
Pruning at initialization Dynamic Sparse Training Freezing at initialization
Lottery Ticket Hypothesis
Freeze weights Set weights to zero
Adaptive sparsity Fixed set of trainable weights
Dense computations Sparse tensor computations
Sparse gradient computations / sparsely updating weights
Sparse weight storageFigure 3: Structural comparison between the three main categories of DRT. Here, the Lottery Ticket Hypoth-
esis is seen as a sub-category of pruning at initialization.
2. Problem formulation
We will start Section 2 with a general introduction into DL. This is followed by
defining a mathematical framework describing DRT. This framework comprises PaI,
LTH, DST and freezing parts of a randomly initialized DNN.
2.1. General deep learning and setup
LetfΘ:Rm→Rndefine a DNN with vectorized weights Θ∈RD∼=RD1×
. . .×RDL. Here, Dldenotes the number of parameters in layer lof a DNN with L
layers and D=PL
l=1Dlparameters in total. We further assume the global network
structure (activation functions, ordering of layers, ordering of weights inside one layer,
. . . ) to be fixed and encoded in fΘ,i.e. only the weight values Θcan be changed.
We assume a standard DNN training, starting with random initial weights Θ(0)∈
RD. See for example He et al. [65], Glorot and Bengio [66], Saxe et al. [67], Martens
[68], Sutskever et al. [69], Hanin and Rolnick [70] for possibilities to randomly initial-
ize a DNN. These initial weights are iteratively updated with gradient based optimiza-
tion like SGD [71], AdaGrad [72], Adam [73] or AdamW [74] to minimize the loss
function
L:RD×Rn×Rm→R,(Θ, X, Y )7→ L(fΘ, X, Y ) (1)
over a training dataset D= (X,Y)⊂Rn×Rm.1After initialization, a model is trained
forTiterations forming a sequence of model parameters {Θ(0),Θ(1), . . . , Θ(T)}. Up-
dating the parameters Θ(t)in iteration tis done by calculating the current gradient
∇Θ(t)L=BX
b=1∂L(fΘ(t), X(t)
b, Y(t)
b)
∂Θ(t), (2)
1Unsupervised learning can be modeled by setting Y=∅.
5

--- PAGE 6 ---
and using it, possibly together with former gradients ∇Θ(0)L, . . . ,∇Θ(t−1)Land pa-
rameter values Θ(0), . . . , Θ(t−1), to minimize the loss function further. Here, a batch
of training data with batch size Bis given by {(X(t)
b, Y(t)
b) :b= 1, . . . , B } ⊂ D . The
batches vary for different training steps t.
The final model is then given by fΘ(T). Note, the overall training goal is for the
model to generalize well to unseen data. Generalization ability is usually measured on
a separate, held back test dataset. Therefore, regularization methods like weight decay
[75], dropout [76–78], batch normalization [79] or early stopping [80, 81] are used
to prevent fΘ(T)to overfit on the training data. Generalization can also be improved
by enabling the model to use geometrical prior knowledge about the scene [82–86],
shifting the model back to an area where it generalizes well [87–90] but also by pruning
the network [33, 38, 91].
2.2. Model for dimensionality reduced training
A reduction of dimensionality for Θ(t)at training step twill be modeled through a
transformation, also called embedding ,Ψ(t):Rd→RD,Θ(t)= Ψ(t)(ϑ(t))andd≪
D.2In this work, we restrict Ψ(t)to form an affine linear transformation which includes
all standard pruning/freezing approaches. However, Ψ(t)has parameters which need to
be stored. Therefore, we not only assume d≪Dbut also size(Ψ(t)) +d≪D, where
size(·)computes the minimal number of parameters needed to express the affine linear
transformation Ψ(t). To model DST, Ψ(t)is allowed to change during training.
In our setup, the parameter count in the network is reduced not only after train-
ing but also during it. Thus, the trainable parameters of the network are given by
ϑ(t)∈Rd. All methods discussed in this work are restricted to fulfill ϑ(t)∈Rdfor all
training iterations t∈ {1, . . . , T },i.e. reduce dimensionality throughout training. The
corresponding model with reduced dimensionality is fΨ(t)(ϑ(t)).
2.2.1. Pruning and dynamic sparse training
For pruning, Ψ(t)encodes the positions of the non-zero entries whereas ϑ(t)stores
the values of those parameters. A corresponding Ψ(t)can be easily constructed by
setting
Ψ(t)(ϑ(t)) =ψ(t)·ϑ(t), (3)
with the pruning embedding ψ(t)∈RD×ddefined via
ψ(t)
i,j=(
1,ifΘ(t)
iis the jthun-pruned element.
0,else, (4)
where we assume the dun-pruned elements Θ(t)
i(t)
1, . . . , Θ(t)
i(t)
dto have the natural ordering
i(t)
1< . . . < i(t)
d. Consequently, the pruned version of Θ(t)is encoded by the low
dimensional
ϑ(t)= (ϑ(t)
j)j= (Θ(t)
i(t)
j)j. (5)
2We use the same transformation as introduced in Mostafa and Wang [92] to embed RdintoRD.
6

--- PAGE 7 ---
Dense Model Frozen Coeﬃcients Forward Pass Backward PassFigure 4: Left: Standard dense model. Middle left: Topology of the frozen weights. Middle right: The
forward pass is exactly the same for the dense and the frozen model. Right: The frozen weights are used for
the backward pass. However, they are not updated and therefore drawn with dotted arrows.
Further, we define p:= 1−d/Das the model’s pruning rate . For pruning at initializa-
tion, i.e. fixed position i(0)
1, . . . , i(0)
dfor the non-zero parameters, ψ(t)=ψ(0)for all
training iterations t. Whereas, ψ(t)might adapt for DST.
Since d≪D, the embedding Ψ(t)(ϑ(t))is automatically sparse in the bigger space
RD. A possibility to overcome sparse Θ(t)while still training only a small part ϑ(t)
of a DNN is proposed in Wimmer et al. [62]. Here, convolutional K×Kfilters are
represented via few non-zero coefficients of an adaptive dictionary. The dictionary is
shared over one or more layers of the network. This procedure can be described by the
adaptive embedding
Ψ(t)= Φ(t)·ψ(t)(6)
with the pruning embedding ψ(t)∈RD×das defined in (4) and the trainable dictionary
Φ(t)∈RD×D.3In this case, the coordinates w.r.t. Φ(t)are sparse, but the resulting
representation in the spatial domain RDwill be dense [62].
Orthogonal to that approach, Price and Tanner [93] combine a sparse pruning em-
bedding (3) with a dense discrete cosine transformation (DCT) by summing them up.
A DCT is free to store and can be computed with D·logDFLOPs. By doing so, the
network keeps a high information flow while only a sparse part of the network has to
be trained. Moreover, the computational cost is almost equal to a fully sparse model.
2.2.2. Freezing parameters
Another approach, which is similar to [93], proposed by Rosenfeld and Tsotsos
[57], Wimmer et al. [58], Sung et al. [59], uses frozen weights on top of the trainable
ones. The resulting transformation is given by
Ψ(t)(ϑ(t)) =ψ(t)·ϑ(t)+χ(t)·Θ(0)(7)
with the pruning embedding ψ(t)∈RD×dfrom (4), χ(t)= diag( χ(t)
1, . . . , χ(t)
D)∈
RD×Ddefined as
χ(t)
i=(
0,ifi∈ {i(t)
1, . . . , i(t)
d}
1,else, (8)
3Here, Φ(t)corresponds to a block diagonal matrix with shared blocks. By sharing blocks, the total
parameter count of Φ(t)is at most D/10,000[62]. Furthermore, the formulation in Wimmer et al. [62] is not
restricted to quadratic D×Dmatrices, but also allows undercomplete orovercomplete systems Φ(t).
7

--- PAGE 8 ---
and the random initialization Θ(0)∈RD. A graphical overview of freezing parts of
a network is given in Figure 4. Similarly to pruning, we define p:= 1−d/Das
the model’s freezing rate ,i.e. the rate of parameters which are frozen at their random
initial value. Note, Rosenfeld and Tsotsos [57] and Wimmer et al. [58] both use a
fixed set of un-frozen weights, i.e.Ψ(t)= Ψ(0). Therefore, the network parameters
Θ(t)= Ψ(0)(ϑ(t))consist of two parts, the dynamic un-frozen weights defined by
ψ(0)·ϑ(t)and the fixed, frozen weights χ(0)·Θ(0). Sung et al. [59] allow the embedding
Ψ(t)to adapt during training, however their procedure still leaves a large part of the
network completely untouched.
By setting χ(t)= 0, (3) is only a special case of (7), since pruning is naturally
the same as freezing un-trained weights at zero. The same transformation (7), but
restricted on freezing whole layers or even the whole network, is used for extreme
learning machines [94, 95] or other works like Hoffer et al. [56], Saxe et al. [96], Giryes
et al. [97].
Another construction of Θ(t)is given by interchanging the pruning embedding ψ(t)
in (7) with a fixed, random orthogonal transformation ψo∈RD×d[98].
2.3. Storing the embedding Ψ(t)
In practice, storage techniques like the compressed sparse row format [99] take over
the role of the pruning embedding ψ(t). Ifψ(t)is known, χ(t)can be easily constructed
by using equation (8). Furthermore, by using pseudorandom number generators for
the initialization, also Θ(0)can be recovered by knowing only a single random seed
number. In this case, storing Ψ(t)has up to 32bit the same cost as storing ψ(t). On the
other hand, if an additional learnt transformation Φ(t)is used as well, compare (6), the
corresponding transformation has to be stored – this is why Wimmer et al. [62] share
many elements in Φ(t).
3. High-level overview of dimensionality reducing transformations Ψ(t)
In this Section we give a high-level overview of different approaches to determine
the dimensionality reducing transformation Ψ(t). First, we discuss the structure of
the parameters which are pruned or frozen in Section 3.1. Then, we compare global
andlayer wise dimensionality reduction in Section 3.2. Section 3.3 covers the update
frequency forΨ(t). Afterwards, the most prominent criteria for determining the prun-
ing embedding ψ(t)are proposed in Section 3.4. Finally, we discuss pre-training the
transformation Ψ(t)in Section 3.5.
Throughout this Section, we restrict the transformation Ψ(t)to use ψ(t)from (4),
i.e. a sparse linear embedding ψ(t)∈ {0,1}D×dwith∥ψ(t)∥0=d, as linear part.
Further we assume the affine part of Ψ(t)to be either zero (pruning) or correspond
to the almost free to store pseudorandom initialization (freezing). We use the notion
oftrainable weights (in iteration t) for all weights Θ(t)
iwithi∈ {i(t)
1, . . . , i(t)
d},i.e.
elements of
{i:∃js.t.ψ(t)
i,j̸= 0}. (9)
8

--- PAGE 9 ---
Dense Model Structured Pruning Unstructured PruningFigure 5: Structured and unstructured pruning.
3.1. Structure of trainable weights
Pruning is generally distinguished in structured andunstructured pruning, see Fig-
ure 5. Of course it can be generalized to our setup, including freezing of parame-
ters. Structured freezing/pruning means freezing/pruning whole neurons or channels
or even coarser structures of the network. For pruning, this immediately results in
reduced computational costs, whereas gradient computations can be skipped for the
frozen structure. For structured freezing, usually whole layers are frozen [56, 94–96]
with the extreme case of freezing the entire network [97]. For pruning it is more com-
mon to prune on the level of channels/neurons [100–102] .
Unstructured pruning of single weights usually improves results compared to struc-
tured pruning [51, 52]. Unstructured pruning has the disadvantage to require software
which supports sparse computations to actually speed up the forward and backward
propagation in DNNs. Furthermore, such software only accelerates sparse DNNs on
CPUs [103, 104] or specialized hardware [53, 54, 105–107]. Weights can also be
frozen in an unstructured manner [58, 59, 98], leading to reduced gradient computa-
tions, memory requirements and communication costs for distributed training.
In recent years, a semi-structured pruning strategy developed, the so called N:M
sparsity [108–113]. A tensor is defined as N:Msparse if each block of size Mcon-
tains (at least) Nzeros. Here, the tensor is covered by non-overlapping, homogeneous
blocks of size M. This development is driven by NVIDIA’s A100 Tensor Core GPU
Architecture [113] which is able to accelerate matrix multiplications up to a factor of
almost 2if a matrix is 2 : 4 sparse.
As shown in Frankle and Carbin [47], using sophisticated methods to determine
a sparse, fixed subset of trainable parameters at random initialization greatly outper-
forms choosing them randomly. Contrarily, choosing and fixing the trainable channels
randomly or via well performing classical techniques leads to similar results for struc-
tured pruning at initialization [100]. Therefore, most pruning methods presented and
discussed in this work are unstructured. Consequently, we assume, if not mentioned
otherwise, unstructured pruning/freezing in the following.
3.2. Global or layerwise dimensionality reduction
An important distinction for DRT methods is whether the rate of the trained param-
eters is chosen separately for each layer or globally. As presented in Section 3.4, the
importance of a weight Θ(0)
ifor training is measured via a score si∈R. A weight
9

--- PAGE 10 ---
Prune
with Ψ(0)Train for
FpstepsUpdate Ψ(t)Dense Initialization Sparse Initialization Sparse Model at Step tUpdated Sparse Topology
Repeat until training is ﬁnishedFigure 6: Dynamic sparse training where the initial pruning embedding Ψ(0)is usually determined randomly.
is frozen or pruned if the corresponding score is below a threshold τi4. This threshold
can be determined globally [47–49, 58, 60, 92, 114] or layerwise . For thresholding the
score layerwise we differentiate between setting a constant rate of trainable parameters
in each layer [115], using heuristics or optimized hyperparameters to find the best rate
of trainable parameters for each layer individually [116, 117] or letting the number of
trained parameters in each layer adapt dynamically during training [92, 118].
3.3. Update frequency
The frequency of updating the dimensionality reducing transformation Ψ(t)is called
update frequency Fp, see Figure 6. Given an update frequency Fp∈N+∪{∞} , it holds
Ψ(t0)= Ψ(t0+1)=. . .= Ψ(t0+Fp−1)(10)
where t0∈ {0, Fp,2Fp, . . .}is an iteration where the transformation is updated. If
Ψ(t)= Ψ(0)is constant, the update frequency equals Fp=∞. We assume the num-
ber of trained parameters to be fixed. Consequently, the proposed methods usually
haveFp≫1. This guarantees (i) stability of the training and (ii) a chance for newly
trainable weights to grow big enough to be not pruned/frozen immediately in the next
update step.
3.4. Criteria to choose trainable parameters
In the following, we present the most common criteria to determine the trainable
weights.
Random criterion. Randomly selecting trainable parameters means that a given per-
centage of them are chosen to be trained by a purely random process, see Figure 2 (c).
This can be done by sampling a score for each weight from a standard Gaussian distri-
bution and training those weights with the dbiggest samples. Random pruning is often
used as first comparison for a newly developed pruning method. For dynamic sparse
training, initially trained parameters are often chosen by random [116, 117] as the dy-
namics of the sparse topology will find a well performing architecture anyway. Also
for DRT methods where information flow is not a limiting factor, like PaI combined
with DCTs [93] or freezing [57, 98], trainable weights are often chosen randomly.
4We index the threshold with the same index as the weight to show that the threshold might depend on
the position of Θ(0)
ibut not the weight itself.
10

--- PAGE 11 ---
Magnitude criterion. A straight forward way to determine trainable weights is to use
those with the highest magnitude. This is done by the LTH [47, 119] and also by
many DST methods for updating the sparse architecture during training [92, 116–118].
Magnitude pruning for Θis equivalent to the solution of
min
¯Θ∈RD,∥¯Θ∥0≤d∥Θ−¯Θ∥q, (11)
i.e. the best d-sparse approximation of Θw.r.t.∥ · ∥ q. Here, q∈(0,∞)is arbitrary.
Assume mat(Θ) ∈Rn×mto be the matrix representing a linear network with mdi-
mensional input and ndimensional output with corresponding vectorized parameters
Θ∈Rn·m. With q= 2it holds for x∈Rm
∥mat(Θ) x−mat( ¯Θ)x∥2≤ ∥mat(Θ −¯Θ)∥F∥x∥2 (12)
=∥Θ−¯Θ∥2∥x∥2. (13)
For a d-sparse ¯Θ, the right hand side (13) is minimized by the solution of magnitude
pruning. This shows the ability of magnitude pruning to approximate dense networks
sparsely if the pruned parameters are not too big. In practice, this is guaranteed by
pruning only small fractions of the parameters in one step [14, 47, 100, 120]. The
magnitude criterion in the viewpoint of dynamical systems is analyzed in Redman et al.
[121]. They show that magnitude pruning is equivalent to pruning small modes of
the Koopman operator [122] which determines the networks convergence behavior.
Consequently, pruning small magnitudes only slightly disturbs the long term behavior
of DNNs.
Lee et al. [123] analyze the minimal ℓ2distortion induced by pruning the whole
network altogether. Their greedy solution approximates magnitude pruning with lay-
erwise optimal pruning ratios . The solution is obtained by rescaling the magnitudes
with a weight-dependent factor, applying global magnitude pruning and finally reset
the un-pruned weights to their value before the rescaling.
If dimensionality reduction is applied at initialization without any pre-training,
the magnitude criterion chooses trainable parameters purely by their initial, randomly
drawn weight. Still as shown in Frankle et al. [124], magnitude pruning at initialization
is a non-trivial baseline for other PaI methods and outperforms random PaI.
Gradient based criterion. As discussed above, the magnitude criterion is for the most
part random at initialization. Consequently, different criteria to chose the trainable
weights are used for SOTA PaI methods. The most common gradient based criterion
relies on the first order Taylor expansion of the loss function at the beginning of training
[48, 58, 102, 114, 125–127] and is mainly used for PaI in our setup. It measures how
disturbing the weights at initialization will affect the loss function. It holds
L(Θ(0)−δ) =L(Θ(0))− ∇Θ(0)L ·δ+O(∥δ∥2). (14)
In our setup, the disturbance introduced by the dimensionality reduction is given by
δ= Θ(0)−Ψ(0)(ϑ(0)). Neglecting the higher order terms and the sign, the following
optimization problem has to be solved
max
Ψ,ϑ|∇Θ(0)L| · |Ψ(ϑ)|, (15)
11

--- PAGE 12 ---
in order to determine Ψ(0)andϑ(0). IfΨis restricted to be a pruning transformation
defined by (3) and (4), the optimization problem (15) reduces to
max
{i(0)
1,...,i(0)
d}⊂{1,...D}dX
j=1|(∇Θ(0)L)ij·Θ(0)
ij|, (16)
which is solved by the indices with top- d|(∇Θ(0)L)i·Θ(0)
i|.
Using the aforementioned gradient based criterion for PaI might lead to a loss of
information flow for high pruning rates since some layers are pruned too aggressively
[49, 50, 58]. Without information flow in the network, the gradient is vanishing and no
training is possible. Thus, there are several approaches to overcome a weak gradient
flow in networks pruned according to (16). Using a random initialization, fulfilling the
layerwise dynamic isometry property [67, 128–130] will lead to an improved informa-
tion flow in the pruned network [125]. Another approach [127] uses a rescaling of the
pruned network to bring it to the edge of chaos [128–130] which is benefitial for DNN
training. A straight forward way, proposed by Wimmer et al. [58], is given by freezing
the un-trained weights instead of pruning them.
Conserving information flow. Sufficient information flow can also be guaranteed di-
rectly by the criterion for selecting trainable parameters. Wang et al. [49] train the
sparse network with the highest total gradient ℓ2norm after pruning . Consequently,
the information flow is not the bottleneck for training.
Synaptic saliency of weights is introduced in Tanaka et al. [50]. A synaptic saliency
is defined as
S(Θ) = ∇ΘR ⊙Θ, (17)
where R:RD→Ris a function, dependent on the weights Θ. In Tanaka et al. [50], a
conservation law for a layer’s total synaptic saliency is shown. By iteratively pruning
a small fraction of parameters based on a synaptic saliency score (17), the conserva-
tion law guarantees a faithful information flow in the sparse network. Examples for a
synaptic flow based pruning criterion are setting R(Θ)as the ℓ1path norm [131, 132]
of the DNN fΘ[50] or the ℓ2path norm [133, 134]. In Patil and Dovrolis [134], the ℓ2
path norm is combined with a random walk in the parameter space to gradually build
up the sparse topology. In contrast, standard approaches slim the dense architecture
into a sparse one.
Trainable ψ(0).All aforementioned criteria are based on heuristics, usually limited
to work well in some scenarios but fail for other ones, see Frankle et al. [124] for
an empirical comparison for the SOTA PaI methods Lee et al. [48], Wang et al. [49],
Tanaka et al. [50]. A natural way to overcome these limitations is given by training
the embedding ψ(0). This approach is especially in the interest for pruning randomly
generated networks without training the weights afterwards, where the determination
ofψ(0)is the only way to optimize the network [63, 119, 135, 136].
Finding ψ(0)∈ {0,1}D×dcan not be done by simple backpropagation since ψ(0)is
optimized in a discrete set{ψ∈ {0,1}D×d:∥ψ∥0=d}. Diffenderfer and Kailkhura
[63], Ramanujan et al. [135], Aladago and Torresani [136], Mallya et al. [137], Zhang
12

--- PAGE 13 ---
MethodPrune
criterionLate
rewindingEarly
stoppingLow
precisionData
sizeTransferAdditional
Trafo Φ
Frankle and Carbin [47] |Θ(0)
i| ✗ ✗ ✗ ✗ ✗
Frankle et al. [120] |Θ(0)
i| ✓ ✗ ✗ ✗ ✗
Morcos et al. [144] |Θ(0)
i| ✓ ✗ ✗ Datasets/Optimizers ✗
Zhang et al. [145] |Θ(0)
i| ✓ ✓ ✗ ✗ ✗
You et al. [146] |Θ(0)
i| ✗ ✓ ✓ ✗ ✗
Rosenfeld and Tsotsos [57] |Θ(0)
i| ✓ ✗ ✗ Predict Performance ✗
Wimmer et al. [62] |Θ(0)
i| ✓ ✗ ✗ ✗ Trainable
Lee et al. [123] |Θ(0)
i|P
|Θ(0)
j|≥|Θ(0)
i||Θ(0)
j|−1
✓ ✗ ✗ ✗ ✗
Zhang et al. [138] |Θ(0)
i| ✓ ✗ ✗ ✗ Inertial Manifold
Table 1: Comparison of different methods to find lottery tickets. Data size means using the full dataset
for finding LTs whereas shrinks the full dataset during the process of finding LTs. Note, all methods can
be used in an iterative manner, but also as one-shot methods.
et al. [138] find the pruned weights with a trainable score s∈RDtogether with a
(shifted) sign function. The score sis optimized via backpropagating the error of the
sparse network on the training set by using the straight through estimator [139] to
bypass the zero gradient of the sign function. Overcoming the vanishing gradient of
zeroed scores can also be achieved by training a pruning probability for each weight
and sampling a corresponding ψ∈ {0,1}D×din each optimization step [119].
3.5. Pre-training the transformation
Similar to a trainable ψ(0), discussed in Section 3.4, also a pre-training step for
finding ψ(0)can be applied. The difference is that for pre-training ψ(0)the correspond-
ing dense weights Θ(0)can be trained as well. Using pre-training for finding the initial
transformation ψ(0)is of course costly in terms of time and also in terms of an in-
creased parameter count during the pre-training phase. Note, after pre-training ψ(0),
the pre-trained weights are not allowed to be used, but reset to Θ(0).
The most prominent example for this is given by the LTH which trains Θ(0)to con-
vergence, prunes p0·100% of the non-zero weights and resets the remaining non-zero
weights to their initial values. This procedure is continued until the desired pruning
rate is reached. Then, the pruning transformation ψ(0)is applied to the initial weights
Θ(0)and fixed for the training of the sparse architecture. Here, the fullnetwork is used
for finding a well performing sparse architecture while for the actual training only the
sparse part of the network, starting from the random initialization, is used.
Another approach for pre-training ψ(0)is given in Liu and Zenke [140] which pre-
trains a sparse network to mimic the training dynamics of a randomly initialized dense
network, measured by the neural tangent kernel [141–143].5
4. Lottery ticket hypothesis
In this Section we discuss the LTH, proposed by Frankle and Carbin [47], in detail.
Frankle and Carbin [47] show that extremely sparse subnetworks of a randomly initial-
5To be precise, in order to mimic the training dynamics, Liu and Zenke [140] also pre-train the weights
ϑ(0)by using Θ(0). Therefore, it is not within the narrow bounds of this work.
13

--- PAGE 14 ---
Random initialization
Train Weights
Prune Weights
Fine-tune WeightsClassic Iterative Pruning
Random initialization
Train Weights
Prune WeightsReset
Non-zero
WeightsLottery Ticket Hypothesis
Random initialization
Pre-trained Weights
Train Weights
Prune WeightsReset
Non-zero
WeightsLTH with Late RewindingFigure 7: Left: Classical iterative pruning applied to a converged model, like Han et al. [14]. Middle:
Standard Lottery Ticket Hypothesis. Right: LTH with resetting weights to values from an early training step.
ized network can be found which, after being trained, match or even outperform their
densely trained baseline. Furthermore, the sparsely trained network converges at least
as fast as standard training but usually faster. Table 1 summarizes different methods to
find LTs. Figure 7 compares the LTH with classical pruning methods which are applied
to converged networks [14].
The procedure to determine the sparse networks is as follows: First, the dense net-
work is trained to convergence and the pruning embedding ψis constructed according
to the magnitude criterion, see Section 3.4. The un-pruned weights are reset to their
values Θ(0)
i. Now, this procedure can be done one-shot oriterative . In the one-shot
case, all weights are pruned after training the dense network at once. Then, the sparse
network, the so called Lottery Ticket (LT), is trained to convergence. For the iterative
procedure, not all coefficients but only p0·100% (usually 20%) of the un-pruned ones
are pruned in one iteration. The remaining non-zero weights are reset to their initial
value and trained again until convergence. This procedure is applied iteratively until
the desired pruning rate pis achieved. Finally, the LT is optimized in a last, sparse
training.
Iterative LTH has shown to find better performing LTs than the one-shot approach
[47, 120]. But, in order to reach a final pruning rate p, the iterative procedure takes
log(1−p)
log 4−log 5
(18)
many pre-trainings plus the final, sparse training. For the final pruning rate p= 0.9,
the iterative LTH approach needs in total 12trainings of the network. On the other
hand, the one-shot approach always requires 2trainings for arbitrary pruning rates p.
Frankle and Carbin [47], Frankle et al. [120], Gale et al. [147] show that resetting
un-pruned weights to their initial value only generates well trainable sparse architec-
tures if the networks are not too big. For modern network architectures like ResNets
[148], resetting the weights to their initial value does not lead to similar results as the
densely trained baseline. This problem can be overcome by late rewinding ,i.e. re-
setting the weights to values reached early in the first, dense training [120, 144, 149].
Moreover, pruning coefficients w.r.t. a dynamic, adaptive basis for K×Kconvolutional
filters improves late rewinding even further [62].
Lee et al. [123] use scaled magnitudes to improve results for LTs at high sparsity
levels. Their scale approximates the best choice of layerwise sparsity ratios . LTs as
14

--- PAGE 15 ---
equilibria of dynamical systems are analyzed in Zhang et al. [138]. They theoretically
show that the important dynamics of SGD training are contained in a small ddimen-
sional subspace W+⊂RD, the generator of the so called inertial manifold . Only a
few training epochs suffice to reliably compute W+. Rewinding un-pruned weights to
their values for an early training step and projecting them onto W+improves results
compared to the standard LT with late rewinding. Theoretical guarantees for recovering
sparse linear networks via LTs are described in Elesedy et al. [150].
Costs for finding LTs via iterative magnitude pruning can be reduced by using early
stopping and low precision training for each pre-training iteration [146], sharing LTs
for different datasets and optimizers [144] or iteratively reducing the dataset together
with the number of non-zero parameters [145]. Also, the error of a LT from a given
family of network configurations ( i.e. ResNet with varying sparsity, width, depth and
number of used training examples) can be well estimated by knowing the performance
of only a few trained networks from this network family [151]. Despite their first appli-
cations on image classification, LTs have also shown to be successful in self-supervised
learning [152], natural language processing [153, 154], reinforcement learning tasks
[153, 155], transfer learning [156] and object recognition tasks like semantic segmen-
tation or object detection [157]. Moreover, Chen et al. [158] propose methods to verify
the ownership of LTs and hereby protect the rightful owner against intellectual property
infringement.
LTs outperform randomly reinitializing sparse networks in the unstructured pruning
case [47]. Contrarily, for structured pruning there seems to be no difference between
randomly reinitializing the sparse network and resetting the weights to their random
initialization [100].
Closely related to the LTH with late rewinding, Renda et al. [159], Le and Hua
[160] show that fine-tuning a pruned network with a learning rate schedule rewound
to earlier stages in training outperforms classical fine-tuning of sparse networks with
small learning rates. Bai et al. [161] show that arbitrary randomly chosen pruning
masks can lead to successful sparse models ifthe full network is used during training.
For this, they extrude information of weights, which will be pruned eventually, into the
sparse network during a pre-training step. Thus, Bai et al. [161] is not a DRT method.
5. Pruning at initialization
Overcoming the high cost for the train-prune-reset cycle(s) needed to find LTs is
one of the main motivation to use pruning at initialization (PaI) [48]. With pruning at
initialization we mean methods that start with a randomly initialized network and do
not perform any pre-training of the network’s weights to find the pruning transforma-
tionψ(0). Furthermore for PaI, ψ(t)=ψ(0)holds for all training iterations t. Different
variants of PaI methods include one-shot pruning [48, 49, 61, 62, 125, 127, 164, 165],
iterative pruning [50, 102, 126, 134] and training the pruning transformation [63, 119,
135, 136]. On top of that, there are methods that train the network after pruning and
methods that do not train the non-zero parameters at all. Table 2 summarizes the PaI
methods proposed in this Section.
15

--- PAGE 16 ---
Method Criterion One-shot Pre-train ψ(0)Train ϑ(0)Initialization of ϑ(0)
Lee et al. [48] |Θ(0)⊙g(0)| ✓ ✗ ✓ standard
Lee et al. [125] |Θ(0)⊙g(0)| ✓ ✗ ✓ dynamic isometry
Hayou et al. [127] E|Θ(0)⊙g(0)|2✓ ✗ ✓ edge of chaos
de Jorge et al. [126] |Θ(0)⊙g(0)| ✗ ✗ ✓ standard
Wang et al. [49] −Θ(0)⊙H(0)g(0)✓ ✗ ✓ standard
Tanaka et al. [50] Θ(0)⊙ ∇Θ(0)R ✗ ✗ ✓ standard
Wimmer et al. [61] λ|Θ(0)⊙g(0)|+ (1−λ)(Θ(0)⊙H(0)g(0)) ✓ ✗ ✓ standard
Su et al. [162] random (+ prior knowledge) ✓ ✗ ✓ standard
Patil and Dovrolis [134] path norm ✗ ✗ ✓ standard
Lubana and Dick [163] |Θ(0)⊙g(0)⊙Θ(0)| ✓ ✗ ✓ standard
Lubana and Dick [163] |Θ(0)⊙H(0)g(0)| ✓ ✗ ✓ standard
Zhang and Stadie [164] temporal Jacobian ✓ ✗ ✓ standard
Alizadeh et al. [165] meta-gradient ✓ ✓ ✓ standard
Zhou et al. [119] trainable prune probability ✗ ✓ ✗ standard
Ramanujan et al. [135] trainable score s ✗ ✓ ✗ standard
Diffenderfer and Kailkhura [63] trainable score s ✗ ✓ ✗ binarization
Koster et al. [166] trainable score s(incl. sign swap) ✗ ✓ ✗ binarization
Chen et al. [167] Θ(0)⊙ ∇Θ(0)R(+ trained sign swap) ✗ ✓ ✗ standard
Aladago and Torresani [136] trainable quality score ✗ ✓ ϑ(0)
kout of fixed {w(1)
k, . . . , w(n)
k}
Table 2: Comparison of different methods for PaI. Here, Θ(0)∈RDdenotes the dense, random initialization
of the model, g(0)the gradient and H(0)the Hessian of the loss at beginning of training. The function
R:RD→R, used for Tanaka et al. [50], can be chosen as any almost everywhere differentiable function.
5.1. PaI followed by training non-zero weights
First, we start with comparing different methods that train weights after the pruning
step. We can group most of them into gradient based approaches and information flow
based approaches. For a detailed comparison of the three popular PaI methods Lee et al.
[48], Wang et al. [49], Tanaka et al. [50] we refer to Frankle et al. [124]. Moreover,
Fischer and Burkholz [168] compares them on generated tasks, where known and ex-
tremely sparse target networks are planted in a randomly initialized model. They show
that current PaI methods fail to find those sparse models at extreme sparsity. However,
the performance of other pruning methods than PaI is not evaluated and it remains an
open question if they are able to find these planted subnetworks.
Gradient based approaches. Gradient based approaches [48, 102, 125–127] try to con-
struct the sparse network which has the best influence in changing the loss function at
the beginning of training, as presented in equations (14), (15) and (16). However, it
was shown that one-shot gradient based approaches have the problem of a vanishing
gradient flow, if too many parameters are pruned [49, 58, 125]. Methods to overcome
this are given by using an iterative approach [102, 126] or use an initialization of the
network, adjusted to the sparse network [125, 127]. These adjusted initialization in-
clude so called dynamic isometric networks [125] and networks at the edge of chaos
[127].
Methods preserving information flow. Other PaI methods primarily focus on generat-
ing sparse networks with a sufficient information flow [49, 50, 134, 164]. For randomly
initialized DNNs, the loss is no better than chance. Therefore, Wang et al. [49] argue
that “at the beginning of training, it is more important to preserve the training dynam-
ics than the loss itself.” Consequently, Wang et al. [49] try to find the sparse network
with the highest gradient norm after pruning. They do so, by training the weights with
highest −Θ(0)
i(H(0)∇Θ(0)L)i, where H(0)defines the Hessian of the loss function at
16

--- PAGE 17 ---
initialization. Another approach is given by Tanaka et al. [50] and Patil and Dovrolis
[134], which try to maximize the path norm in the sparse network, see Section 3.4 Con-
serving information flow for more details. For RNNs or LSTMs, standard PaI methods
do not work well [164]. By pruning based on singular values of the temporal Jacobian,
Zhang and Stadie [164] are able to preserve weights that propagate a high amount of
information through the network’s temporal depth.
Hybrid and other approaches. As shown in Wimmer et al. [61], only optimizing the
sparse network to have the highest information flow possible does not lead to the best
sparse architectures – even for high pruning rates. They conclude that information
flow is a necessary condition for sparse training but not a sufficient one. Therefore,
they combine gradient based and information flow based methods to get the best out of
both worlds. With their approach, they improve gradient based PaI and PaI based on
preserving information flow at one blow.
Another method guaranteeing a faithful information flow in the network and at the
same time improving performance is given by pruning in the interspace [62]. Wimmer
et al. [62] represent K×Kfilters of a convolutional network in the interspace – a linear
space spanned by an underlying filter basis. After pruning the filter’s coefficients, the
filter basis is trained jointly with the non-zero coefficients. By adapting the interspace
during training, networks tend to recover from low information flow. Furthermore,
using interspace representations has shown to improve not only PaI for gradient based
and information flow preserving methods, but also LTH, DST, freezing andtraining
dense networks.
An orthogonal approach for guaranteeing high information flow while training only
a sparse part of the network is proposed by Price and Tanner [93]. Random pruning is
combined with a bypassing DCT in each layer. Since DCTs correspond to basis trans-
formations with an orthonormal basis, the information of a layer’s input is maintained
even if only a tiny fraction of parameters is trained. Therefore, results are improved
tremendously for extreme pruning rates. Note, adding DCT improves performance for
highpdespite using a simple random selection of trained weights. As shown by Price
and Tanner [93], adding DCTs to each layer also improves DST methods for high p.
Alizadeh et al. [165] improve Lee et al. [48] by modeling the effect of pruning
on the loss function at training iteration M > 0. In contrast, Lee et al. [48] only
analyze the effect of pruning on the loss at initialization. To do so, Alizadeh et al.
[165] compute a meta-gradient which is achieved by pre-training the dense network
forMepochs. The meta-gradient is computed w.r.t. pruning mask. After pruning, the
non-zero weights are reset to their initialization.
Lubana and Dick [163] theoretically compare magnitude, gradient based and in-
formation flow preserving PaI methods. Their analysis shows that magnitude based
approaches lead to a rapid decrease in the training loss, thus converge fast. Further-
more, gradient based pruning conserves the loss function, removes the slowest chang-
ing parameters and preserves the first order dynamics of a model’s evolution. They
combine magnitude and gradient based pruning to get the best of both worlds which
yields the pruning score |∇Θ(0)Li|·|Θ(0)
i|2. Finally, information flow preserving prun-
ing by using Wang et al. [49]’s criterion to get the sparse model with maximal gradient
norm removes the weights which maximally increase the loss function. However, this
17

--- PAGE 18 ---
ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLUTarget ModelRandom Model with
Additional Layer l+1/2 Desired ConnectionsEquivalent,
Pruned Model
. . . . . .Node in layer l
w∗= 0.1
Node in layer l+ 1−0.099 0.1 0.4 0.095
−1.02 0.3 1.1 0.985w∗
−=−0.099 w∗
+= 0.095
sign−=−1.02 sign+= 0.985w∗
− w∗
+
sign− sign+Node in layer l
Node in layer l+ 1Add enough nodes
in layer l+1/2Keep desired
connectionsPrune the restFigure 8: Shows how Malach et al. [170] approximate a single target weight through random connections by
adding a wide enough layer l+1/2between layers landl+1and afterwards pruning unneeded connections.
does not preserve the second order dynamics of the model. Preserving the gradient
norm by training the weights with highest |Θ(0)
i(H(0)∇Θ(0)L)i|on the other hand also
preserves the second order dynamics and shows improved results compared to [49].
Frankle et al. [124], Su et al. [162] show that the most popular PaI methods Lee
et al. [48], Wang et al. [49], Tanaka et al. [50] do not significantly lose performance
if positions of non-zero weights are shuffled randomly in each layer if the sparsity is
not too extreme. Consequently, these methods do not appear to find the best sparse
architecture, but rather well performing layerwise pruning rates for the given network
architecture and global pruning rate. Using the knowledge of well performing PaI
methods, Su et al. [162] show impressive results by randomly pruning weights. Hereby,
they use layerwise pruning rates derived by observing the layerwise pruning rates of
other well performing PaI methods. Liu et al. [169] also show that random PaI can
reach competitive results to other PaI methods. They experimentally demonstrate that
the gap between random PaI and dense training gets smaller if the underlying baseline
network becomes bigger. Therefore, random PaI can provide a strong sparse training
baseline, especially for large models.
5.2. PaI without training non-zero weights
Mallya et al. [137] show that a pre-trained network can be pruned for a specific
task to have good performance even without fine-tuning. Inspired by that, several PaI
methods showed that the same holds true for a randomly initialized network. Before
we go into detail how pruning masks for random weights can be found, we will discuss
theoretical works that cover the universal approximator ability of pruning a randomly
initialized network, also called strong lottery ticket hypothesis [170, 171].
Theoretical background. An intuition that big, randomly initialized networks contain
well performing sparse subnetworks is given in Ramanujan et al. [135]. Let f∗
Θ∗:
Rm→Rnbe a target network with Θ∗∈Rd∗. Further, let fΘ:Rm→Rnbe
a network with randomly initialized Θ∈RDand˜Θbe a randomly chosen sparse
projection of Θwith∥˜Θ∥0=d≪Dnon-zero parameters. Consequently, f˜Θis an
arbitrary d-sparse subnetwork of fΘ. Assuming D≫d∗≈d, Ramanujan et al. [135]
argue that the chance of f˜Θbeing a well approximator of f∗
Θ∗is small, but equal to
δ >0. The big, random network has D
d
subnetworks with dnon-zero parameters.
Consequently, the chance of allsparse subnetworks f˜Θnot being a good approximator
forf∗
Θ∗is
(1−δ)(D
d)− − − − →
D→∞0. (19)
18

--- PAGE 19 ---
Thus, if the randomly initialized network fΘis chosen big enough, it will contain, with
high chance, a well approximator for f∗
Θ∗withdnon-zero weights.
This intuition is proven in Malach et al. [170] for MLPs by using a slightly different
idea. Here, instead of making each layer in the randomly initialized network arbitrary
wide, an intermediate layer l+1/2is added between each two layers landl+ 1of the
randomly initialized network, see Figure 8 right. These intermediate layers are made
wide enough so that each weight w∗in the target network can be approximated well
enough. It holds
w∗·x= (+1)|{z}
≈sign+·ReLU( |w∗||{z}
≈w∗
+·x) + (−1)|{z}
≈sign−·ReLU( −|w∗||{z}
≈w∗
−·x). (20)
Therefore, for each weight w∗in the target network two paths are constructed. The
first one approximates the positive part |w∗|together with sign +1and the other the
negative part −|w∗|together with sign −1, see middle right of Figure 8. All remaining
weights in the random network are pruned as shown in the middle left of Figure 8.
As a consequence, Malach et al. [170] show that, with some assumptions on the target
network, pruning big randomly initialized networks is an universal approximator. But,
for each target weight, a polynomial number of intermediate weights have to be added.
Follow up works Orseau et al. [172] and Pensia et al. [171] improve the parameter
efficiency by shrinking the big, randomized network. They do so by using more than
two paths to approximate a weight in the original network. By allowing the random
values to be resampled, the width of the intermediate layer l+1/2can be reduced to
2times the size of the original layer l[173]. Burkholz et al. [174] allow the randomly
initialized network to be deeper than 2times the target network. By approximating
layers with combinations of univariate and multivariate linear functions, they are also
able to approximate convolutional layers. Independent to the approach of Burkholz
et al. [174], da Cunha et al. [175] extend the result of Pensia et al. [171] to CNNs by
restricting the network’s input to be non-negative.
Methods to train ψ(0).As already mentioned before, training ψ(0)∈ {0,1}D×dre-
quires optimizing in a discrete space. Consequently, different approaches are used to
findψ(0)for a randomly initialized network. In Zhou et al. [119], weights are pruned
with probabilities modeled by Bernoulli samplers with corresponding trainable param-
eters. While helping during training, the stochasticity of this procedure may limit the
performance at testing time [135]. The stochasticity is overcome in Ramanujan et al.
[135] by the edge-popup algorithm. For each weight Θ(0)
iin the network a corre-
sponding pruning score siis trained. The weights with top- dscores are kept at their
initial value, the remaining ones are pruned. The score siis then optimized with the
help of the so called straight through estimator [139] and the pruning transformation is
updated in each iteration. Ramanujan et al. [135] show that a random Wide-ResNet50
[176] contains a sub-network with smaller size than a ResNet34 [148] but the same test
accuracy on ImageNet [177]. Koster et al. [166] and Chen et al. [167] independently
show that allowing the randomly initialized weights to switch signs, improves the per-
formance of edge-popup . Chijiwa et al. [173] improve edge-popup by allowing
re-sampling of the un-trained weights. Binarizing the un-pruned, random weights can
19

--- PAGE 20 ---
MethodSparse
initializationSparsity Pruning
criterionRegrow
criterionFpGlobal Adaptive
Bellec et al. [60] random ✗ ✗ sign change random triggered by pruning
Mocanu et al. [116] random ✗ ✗ magnitude random hyperparameter
Mostafa and Wang [92] random ✓ ✓ magnitude random hyperparameter
Dettmers and Zettlemoyer [118] magnitude ✗ ✓ magnitude gradient momentum 1 epoch
Evci et al. [117] random ✗ ✗ magnitude gradient hyperparameter
Table 3: Comparison of different methods for dynamic sparse training.
also be combined with the edge-popup algorithm [63]. Finally, Aladago and Torre-
sani [136] sample for each weight ϑ(0)
iin a target network a set of npossible weights
w(1)
i, . . . , w(n)
iin ahallucinated ,ntimes bigger network. This hallucinated network
is fixed. During training, each possible weight w(j)
ihas a corresponding quality score
which is increased if the weight is a good choice for ϑ(0)
iand decreased otherwise. In
the end, the w(j)
iwith the highest quality score is set as ϑ(0)
iwhereas the remaining
hallucinated weights are discarded, i.e. pruned.
6. Dynamic sparse training
For high pruning rates, PaI is not able to perform equally well as classical pruning
methods. One possible explanation for the performance gap is that a randomly initial-
ized network does not contain enough information to find a suitable sparse subnetwork
that trains well. Adapting the pruning transformation during training or using network
pre-training to find ψ(0)overcomes this lack of information. As shown in Section 4,
finding LTs is expensive. Furthermore, it is not answered if resetting weights to their
initial value can match late rewinding for big scale datasets. Dynamic sparse training
performs well for high pruning rates while only needing one, fully sparse training. This
is achieved by redistributing sparsity over the network in the course of training. By al-
ways fixing a given rate of parameters at zero, the number of trained weights is kept
constant during training. Different DST methods are collected in Table 3.
6.1. Dynamic sparse training methods
Inspired by the rewiring of synaptic connectivity during the learning process in
the human brain [178], DEEP-R [60] trains sparse DNNs while allowing the non-zero
connections to rewire during training. To do so, pruning and rewiring is modeled as
stochastic sampling of network configurations from a posterior. However, this proce-
dure is computationally costly and challenging to apply to big networks and datasets
[118].
In Mocanu et al. [116], a sparse subnetwork with dnon-zero parameters is chosen
randomly at the beginning of training. Here, an initial pruning rate has to be defined
separately for every layer. After each epoch, trained weights with the smallest mag-
nitude are pruned layerwise with rate qand the same number of weights is regrown
at random positions in this layer. Note, the pruning rate qused for updating ψ(t)is
different from the initial pruning rate p. This procedure is done until training con-
verges. Thus, in each epoch the same number of parameters is pruned and regrown
20

--- PAGE 21 ---
which makes training hard to converge. Furthermore, the number of non-zero param-
eters needs to be defined for each layer before training and can not be adapted during
training.
Dynamic sparse reparameterization [92] overcomes this problem by using magni-
tude pruning with an adaptive, global threshold. Furthermore, the number of regrown
weights in each layer is adapted proportionally to the number of non-zero weights in
that layer.
Regrowing weights not randomly, but based on their gradient’s momentum was
proposed by Dettmers and Zettlemoyer [118]. Here, not only the regrown weights are
determined by their momentum, but also their number in each layer is.
RigL [117] bypasses the need of Dettmers and Zettlemoyer [118] to compute the
dense gradient in each training iteration, and only regrows weights based on their actual
gradient in the updating iteration of ψ(t),i.e.t∈ {Fp,2Fp, . . .}. Also, the number
of pruned and regrown parameters is reduced by a cosine schedule to accelerate and
improve convergence.
Overparameterized, densely trained DNNs in combination with SGD have shown
good generalization abilities [179–181]. Liu et al. [182] showed that the good gen-
eralization ability of DST models can be explained by the so called in-time-over-
parameterization . Training of sparse DNNs can be viewed in the space-time manifold.
To overparameterize DST models in this manifold, three properties must be fulfilled:
1. The dense baseline network has to be big enough.
2. Exploration of trained weights has to be guaranteed during training.
3. The training time has to be long enough so that the network can test enough
sparse architectures in training.
If in-time-overparameterization is guaranteed for DST methods by these three criteria,
Liu et al. [182] show that sparse networks are able to outperform the dense, standard
overparameterized ones. In-time-overparametrization can be achieved by either in-
creasing the number of training epochs, or by reducing the batch size while keeping Fp
constant. The latter leads to more updates of ψ(t)while not increasing the number of
training epochs.
6.2. Closely related methods
We want to highlight that there exists more methods which are called dynamic
sparse training in literature which do not fulfill our definition of it. We do not present
these methods here in detail since they allow to update allweights of the network and
only mask them out in the forward pass. Examples for such methods are Guo et al.
[34], Sanh et al. [114], Ding et al. [183], Kusupati et al. [184], Liu et al. [185]. Other
dynamic training methods [115, 186–188] sample different subnetworks for each train-
ing iteration, update this subnetwork while keeping all un-trained parameters fixed at
their previous position. In the next training iteration, a new subnetwork is sampled.
Therefore, they can not store the network’s parameters Θ(t)in its reduced form ϑ(t)for
allt∈ {0,1, . . . , T }. Schwarz et al. [186] additionally exchange the standard weights
Θ(t)through a reparametrization using the power Θ(t)=ϕ(t)|ϕ(t)|(α−1)withα >1.
By doing so, weights close to 0are unlikely to grow. As a consequence, the parame-
ters will form a heavy tailed distribution at convergence. This improves results since
21

--- PAGE 22 ---
Method frozen part structured criterion additional projection
ELMs [55] all except classifier ✓ — ✗
Hoffer et al. [56] only classifier ✓ — ✗
Rosenfeld and Tsotsos [57] all except batch normalization ✓ — ✗
Li et al. [98] unstructured ✗ random orthogonal projection
Zhou et al. [119] unstructured ✗ iterative magnitude (LTH) ✗
Wimmer et al. [58] unstructured ✗ |Θ(0)⊙g(0)| ✗
Sung et al. [59] unstructured ✗ |∇Θ(0)logfΘ(0)|2✗
Rosenfeld and Tsotsos [57] structured ✓ random ✗
Table 4: Comparison of different methods for freezing parts of a neural network. Here, Θ(0)denotes the
dense, random initialization of the network and g(0)the gradient of the loss function at beginning of training.
the parameters are pruned based on their magnitude and heavy tailed distributions are
highly compressible via magnitude pruning [39]. Using this reparametrization might
also improve other DRT methods proposed in this work.
7. Freezing parts of a network
Contrarily to pruning, freezing parts of a randomly initialized network has attracted
less interest in research in recent years. The main reason for this is that frozen, non-
zero weights must be accounted for in the forward propagation. Thus, no (theoretical)
speed up for inference can be obtained. In addition, frozen weights must be stored
even after training, while zeros only require a small memory footprint. But, by using
pseudorandom number generators for initializing the neural network, frozen weights
can be recovered with a single 32bit integer on top of the memory cost for a pruned
network [58]. The proposed methods to freeze a network are summarized in Table 4.
7.1. Theoretical background for freezing
Saxe et al. [96] show that convolutional-pooling architectures can be inherently fre-
quency selective while using random weights. In Giryes et al. [97], euclidean distances
and angles between input data points are analyzed while propagating through a ReLU
network with random i.i.d. Gaussian weights. With ReLU activation functions, each
layer of the network shrinks euclidean distances between points inversely proportional
to their euclidean angle. This means that points with a small initial angle migrate closer
towards each other the deeper the network is. Assuming the data behaves well , mean-
ing data points in the same class have a small angle and points from different classes
have a bigger angle between another, random Gaussian networks can be seen as an
universal system that separates any data . On the other hand, if the data is not perfect,
training might be needed to overcome big intra-class angles or small inter-class angles
to achieve good generalization.
Freezing parameters at their initial value was used in Li et al. [98] in order to mea-
sure the intrinsic dimension of the objective landscape. First, the dense network is
optimized to generate the best possible solution. Then, the number of frozen parame-
ters is gradually decreased, starting by freezing all parameters. The network’s param-
eters are computed according to (7), with ψ(t)=ψ(0)equaling a random, orthogonal
projection. If a similar performance as the dense solution is reached, the number of
non-frozen parameters determines the intrinsic dimension of the objective landscape.
22

--- PAGE 23 ---
While using frozen weights only as a tool to measure the dimension of a problem, Li
et al. [98] showed that freezing parameters can lead to competitive results.
7.2. Freezing methods
We want to start with the so called extreme learning machines (ELMs) [55, 94,
95]. ELMs are MLPs, usually with one hidden layer. The parameters of the hidden
node are frozen and usually initialized randomly. However, the classification layer
is trained by a closed form solution of the least-square regression [55]. ELMs are
universal approximators if the number of hidden nodes is chosen big enough [94]. In
SOTA networks, ELMs can be used to substitute the classification layer [95]. Closely
related to ELMs are random vector functional link neural networks [189, 190] which,
in addition, allow links between the input and output layer.
A completely orthogonal approach to ELMs is proposed in Hoffer et al. [56], where
the classification layer is substituted by a random orthogonal matrix. For the classifica-
tion layer, only a temperature parameter Tis learnt. Experiments show that the random
orthogonal layer with optimized Tyields comparable results to a trainable classifier for
modern architectures on CIFAR-10/100 [191] and ImageNet [177].
In Zhou et al. [119], pruning weights and freezing them at their initial values is
compared in the LTH framework. They show that freezing usually performs better
for a low number of trained parameters whereas pruning has better results if more
parameters are trained. Finally, they show that a combination of pruning and freezing
– depending whether a weight moved towards zero or away from zero during dense
training – reaches the best results.
Freezing at initialization is used in Wimmer et al. [58] to overcome the problem of
vanishing gradients for PaI. Similar to Zhou et al. [119] they show that freezing outper-
forms pruning if only few parameters are trained. For high freezing rates, freezing still
guarantees a sufficient information flow in the sparsely trained network. On the other
hand, if a higher number of parameters is trained, pruning also performs better than
freezing in this setting. But, by using weight decay on the frozen parameters, Wimmer
et al. [58] are able to get the best from both worlds, frozen weights at the beginning of
training to ensure faithful information flow and sparse networks in the end of it, while
improving both of them at the same time.
Inspired by transfer learning, Sung et al. [59] freeze large parts of the model to
reduce the size of the newly learned part of the network. Frozen weights are chosen ac-
cording to their importance for changing the network’s output, measured by the Fisher
information matrix. They show that freezing parameters helps to reduce communica-
tion costs in distributed training as well as memory requirements for checkpointing net-
works during training. Unsurprisingly, they reach the best results if the freezing mask
is allowed to change during training compared to keeping the freezing mask fixed.
Rosenfeld and Tsotsos [57] freeze parameters in a structured way. They mainly
focus on training only a fraction of filters ( i.e. output channels) and determine the
frozen parts randomly. In this setting, freezing outperforms pruning for almost all
numbers of trained weights.
Furthermore, it is shown in Rosenfeld and Tsotsos [57], Frankle et al. [192] that
freezing allweights except the trainable batch normalization [79] parameters leads to
non-trivial performance.
23

--- PAGE 24 ---
Method Category Top-1-AccTraining
FLOPsTesting
FLOPsTop-1-AccTraining
FLOPsTesting
FLOPs
Dense — 76.8±0.1 1 × 1× 76.8±0.1 1 × 1×
pruning rate p= 0.8 pruning rate p= 0.9
Random PaI 70.6±0.1 0 .23× 0.23× 65.8±0.0 0 .10× 0.10×
Lee et al. [48] PaI 72.0±0.1 0 .23× 0.23× 67.2±0.1 0 .10× 0.10×
Frankle et al. [120] LTH 75.8±0.1
Evci et al. [117] DST 74.6±0.1 0 .23× 0.23× 72.0±0.1 0 .10× 0.10×
+ Wimmer et al. [62] DST 76.0±0.1 74 .3±0.1
Evci et al. [117] ×5 DST 76.6±0.1 1 .14× 0.23× 75.7±0.1 0 .52× 0.10×
Mocanu et al. [116] DST 72.9±0.4 0 .23× 0.23× 69.6±0.2 0 .10× 0.10×
Dettmers and Zettlemoyer [118] DST 75.2±0.1 0 .61× 0.42× 72.9±0.1 0 .50× 0.24×
Table 5: Performance of different methods for a ResNet 50trained on ImageNet. Results, except for the LTH
experiment and Wimmer et al. [62] are derived from Evci et al. [117], Figure 2. Wimmer et al. [62] uses the
pruning method from [117] combined with a trainable basis. The result for Frankle et al. [120] is reported
from Evci et al. [193] which is LTH with late rewinding.
8. Comparing and discussing different dimensionality reduced training methods
In this Section we discuss the different DRT approaches introduced in Sections 4, 5,
6 and 7. Figure 3 shows a high-level comparison between LTH/PaI, DST and freezing.
Leaning on this structural comparison, we will now discuss different aspects of the
methods.
8.1. Performance.
We start with the most important criterion, the performance of the methods. With
performance we mean, test accuracy (or different metrics for tasks other than image
classification) after training. This survey covers methods that reduce training cost.
Therefore, performance needs to be brought in the context with the cost for the method.
As baseline for comparing different methods, we will use our main cost measure – the
number of trained (or equivalently stored) parameters.
For the performance evaluation, we use LTH with late rewinding. Note, this is not
a real DRT method since training starts with a subset of Θ(t)for a small t >0here.
However, literature only reports results for modern networks and big scale tasks like
ImageNet for LTH with late rewinding. As already mentioned in Section 4, resetting
weights to their initialization shows worse results than late rewinding which should be
kept in mind.
It was shown and discussed in many works that LTs have better results than PaI for
high pruning rates [49, 124]. Also, DST improves results compared to PaI [117, 118].
As an example, Table 5 compares PaI, LTH and DST for a ResNet50 [65] on Ima-
geNet [177]. Table 5 shows that LTs approximately reach the same performance than
DST. Both of them outperform PaI. But, if the training time spent on DST methods
is doubled, DST exceeds LT [182], see also discussion in Section 6. The doubled
training time for DST is a fair comparison, since LTs need at least twice the training
time than a standard DST method. On the other hand, the reported LTH result from
Evci et al. [193] does notuse the standard way to find LTs [120], but gradual mag-
nitude pruning [147] to find the sparse subnetwork. Thus, by using the approach to
24

--- PAGE 25 ---
iteratively train-prune-rewind the weights [120], LTs performance could be improved
further. Moreover, Table 5 shows that pruning coefficients w.r.t. adaptive bases [62] for
K×Kfilters improves performance of SOTA methods further.
Figure 9: (Figure 1 in Wimmer et al. [58]) Compar-
ing the PaI method SNIP [48] and the model with the
same trained parameters but the un-trained parame-
ters frozen at their initial value, FreezeNet [58] for a
LeNet- 5-Caffe [194] on MNIST [194].PaI’s inferior performance is not a
surprise since PaI methods are clearly the
less sophisticated ones. As we will see
in the following discussions, this sim-
plicity will reduce costs at other levels,
liketraining time orhyperparameter tun-
ing. Our first observation therefore is,
that pre-training the network to find a
well trainable sparse architecture (LT)
or adapt the sparse architecture during
training (DST) improves results com-
pared to finding a sparse architecture ad-
hocat the beginning of training.
Freezing the parameters is compared
with pruning them in the PaI setting [58]
and the LTH setup [119]. Usually, freez-
ing leads to slightly worse results than
pruning for a higher number of trained
parameters and better results for fewer trained parameters. Figure 9 shows a compari-
son between pruning and freezing weights for a low number of trained weights in the
PaI setting. Price and Tanner [93] show that the same holds true if the random dense
layer is replaced by a dense DCT. Using DCTs has the advantage that they are cheap
to compute.
8.2. Storage cost
As introduced in Section 2.3, techniques like the compressed sparse row format
[99] are used to store sparse/frozen networks after training. By using an initialization
derived from a pseudorandom number generator,
memory( freeze ) = memory( prune ) + 32 bit (21)
≈memory( prune ) (22)
holds. Consequently, pruning and freezing have the same memory cost in this case.
But, if no pseudorandom number generator can be used, pruning has much lower mem-
ory requirements than freezing.
Also, Sections 4 - 7 propose methods that do not use a simple pruning/freezing
transformation but also an additional linear transformation to embed the small, train-
able parameters ϑ∈Rdinto the bigger space RD. Examples for this is pruning coupled
with an additional basis transformation with shared diagonal blocks [62] or a random,
orthogonal projection [98]. The cost for storing these embeddings is neglectable for
Wimmer et al. [62]. The same holds for Li et al. [98] if the random transformation is
created with a pseudorandom number generator.
25

--- PAGE 26 ---
8.3. Training cost
In the following, we will compare the training costs for different methods which
mainly are measured by the training time for one sparse model and the number of tun-
able hyperparameters for training which implicitly determines the number of training
runs needed overall. On top of that, some methods induce additional gradient compu-
tations before or during training which will be discussed in the end.
8.3.1. Training time
We assume that all methods, DST, LTs, PaI and freezing train the final sparse model
forTiterations. Here, Tis the number of training iterations used for the dense network
to (i) converge and (ii) have good generalization ability. Consequently, DST, PaI and
freezing have approximately the same training time for one model. LTs on the other
hand need, as discussed in Section 4, at least twice the number of training iterations
than a standard training. By using iterative train-prune-reset cycles, the number of
training iterations can easily reach 10−20×the standard number. Thus, LTs need a
massive amount of pre-training for the pruning transformation ψ(0).
As shown in Liu et al. [182] and discussed in Section 6, DST massively profit from
increasing the training time.
8.3.2. Hyperparameters
All proposed methods induce at least one hyperparameter more than training the
corresponding dense model. This hyperparameter is given by the number of trainable
parameters .6
Determining the number of trained parameters. PaI, freezing and LT determine one
global pruning/freezing rate. DST methods on the other hand often need to specify the
rate of trained parameters for each layer separately. If the layerwise pruning rates can
be adapted dynamically during training [118], their initial choice has shown to be not
too important and can be set constant for all layers. Other works like Mocanu et al.
[116], Evci et al. [117], Liu et al. [182] heuristically determine layerwise pruning rates
by an Erd˝os-R ´enyi model [195].
Additional hyperparameters for PaI. Besides the pruning rate, PaI methods intro-
duce hyperparameters for computing the pruning transformation ψ(0). For Lee et al.
[48], Wang et al. [49], Verdenius et al. [102], de Jorge et al. [126] this is the number
of data batches, needed to compute the pruning score of each parameter. The iterative
approaches Tanaka et al. [50], Verdenius et al. [102], de Jorge et al. [126] need to deter-
mine the number of conducted pruning iterations. However, by using a high number of
data batches and many pruning iterations, these parameters are not required to be fine-
tuned further [49, 50]. A special case in this setting is Wimmer et al. [61], interpolating
between the two pruning methods Lee et al. [48] and Wang et al. [49]. Consequently,
they need to tune one hyperparameter to balance between these two methods.
6In pruning literature, many methods determine the number of trainable parameters not directly but in-
direct by choosing an associated hyperparameter as for example weighting the ℓ1regularization. But, all
proposed methods in this work choose the number of trained parameters directly.
26

--- PAGE 27 ---
Additional hyperparameters for LTs. As shown in Frankle et al. [120], resetting weights
not to their initial value but a value early on in training leads to better results. Conse-
quently, a well performing rewinding iteration has to be found. A proper experimental
analysis for various datasets and models is given in Frankle et al. [120]. Furthermore,
if iterative pruning is used, the number of pruned weights in each iteration has to be
determined. Usually, 20% of the non-zero weights are removed in each iteration [47].
Additional hyperparameters for freezing. Freezing is closely related to either PaI [58],
LTs [119] or random pruning [57]. Since freezing does not introduce additional hy-
perparameters, the same number of additional hyperparameters as for the correspond-
ing pruning methods are needed. Note, to achieve good results with randomly frozen
weights, layerwise freezing rates have to be determined which need to be fine-tuned
accordingly.
Additional hyperparameters for DST and costs for updating ψ(t).Compared to PaI,
DST has the advantage that the pruning transformation is not fixed at ψ(0). This leads
to better performance after training while using the same number of trained parameters
and training iterations. But this also results in costs for determining ψ(t). Besides com-
puting ψ(t), the update frequency Fpand the update pruning rate q(t)
lhave to found.
The update pruning rate q(t)
lis defined as the ratio of formerly non-zero parameters
which are newly pruned in layer lifψ(t)is updated in iteration t. Note, the update
pruning rate might vary between different update iterations of the transformation and
also between layers. Also, the regrowing rate needs to be determined since it can be
different from the update pruning rate [118].
Finally, there are the actual costs for updating ψ(t)itself. In all considered methods
in Section 6, weights are dried according to having the smallest magnitude – magnitude
based pruning. For this, the dun-pruned coefficients have to be sorted. After setting
some weights to zero, the same number of weights has to be flagged as trainable again.
The costs for regrowing weights can almost be for free by using random regrowing of
weights [92, 116]. On the other hand, Evci et al. [117] require the gradient of the whole
network at the update iterations t∈ {Fp,2Fp, . . .}to determine ψ(t). Dettmers and
Zettlemoyer [118] even need these gradients in each training iteration in order to update
a momentum parameter which also requires an additional weighting hyperparameter.
A temperature parameter is needed in Bellec et al. [60] to model a random walk in the
parameter space to explore new weights.
8.3.3. Gradient computations and backward pass.
All proposed methods update only sparse parts of the weights via backpropagation.
But, some of them additionally require the computation of the dense gradient for pre-
training, determining ψ(0)or to update ψ(t).
First of all, LTs need to train the dense network in order to find ψ(0). This can limit
the size of the biggest underlying dense network that can be used. For iterative LTH,
networks with sparsity pk= 1−0.8khave to be trained additionally for all kwith
pk> p.
PaI methods need the computation of a dense gradient before training [48, 50, 102,
126]. Even a Hessian-vector product has to be calculated for the method Wang et al.
27

--- PAGE 28 ---
[49]. But after having found ψ(0), PaI methods are trained with a fixed, sparse archi-
tecture without the need to compute a dense gradient anymore.
DST methods might not need a dense gradient at all, if regrown weights are found
by a random selection [60, 92, 116]. As mentioned above, Evci et al. [117] compute the
dense gradient at each update step of the pruning transformation t∈ {Fp,2Fp, . . .}.
The most extreme case is given for Dettmers and Zettlemoyer [118] which need to
compute the dense gradient in each iteration.
Finally there is a difference between gradient computations for pruned and frozen
models. Pruned networks compute gradients of activation maps with sparse weight
tensors. Frozen networks on the other hand compute gradients of activation maps with
the help of the frozen weights, i.e. require dense computations, see Figure 4 right side.
However, frozen weights do not need to be updated. Therefore, it is enough to com-
pute only a sparse part of the weights’ gradients. In summary, freezing also reduces
computations in the backward pass, but not as much as pruning does.
8.4. Forward Pass.
In Section 8.3.3, gradient computations and the backward pass for the different
methods are discussed. Gradients only have to be computed during training whereas
the forward pass is needed for both, training and inference. Therefore, we discuss the
forward pass in a standalone Section. Still, this discussion should also be seen as a part
of the training costs, Section 8.3.
During and after the sparse training, LTH, DST and PaI all have the same cost for
inference. This statement is not completely true, since different methods might create
varying sparsity distributions for the same global pruning rate. This can result in differ-
ent FLOP costs for inferring the sparse networks, see for example Table 5. However,
analyzing sparsity distributions obtained by different pruning methods is beyond the
scope of this work.
If parts of a network are frozen during training, allparameters participate in the
forward pass. Of course, this also holds true for inference time. Consequently, the
computational cost for inference can not be reduced and is equal to the densely trained
network.
8.5. Summary
In summary, we see that freezing parameters instead of pruning them leads to the
same memory requirements and better results for training a low number of parameters.
However, if more parameters are trained, pruning leads to equal or better performance.
For pruning, the sparsity of the parameters can reduce the number of required com-
putations for evaluating the network if the used soft- and hardware supports sparse
computations. Thus, freezing seems to be a good option if the model size is a bottle-
neck, i.e. only a small part of the network is trained, whereas pruning is preferably used
otherwise. Furthermore, the computational overhead induced by freezing layers can be
reduced by using dense DCTs instead of dense frozen layers.
For pruning, there is a trade-off between simplicity of the method and performance
after training. As shown, DST and LTs have approximately the same generalization
ability and are superior to PaI for the same number of non-zero parameters. However,
28

--- PAGE 29 ---
PaI guarantees a fixed sparse architecture throughout training, determined by only a few
dense gradient computations. Furthermore, PaI does not require extensive and costly
hyperparameter tuning. Finding LTs requires expensive pre-training of the dense net-
work. Also, LTs at initialization often show unsatisfactory results for modern network
architectures and big scale datasets. Resetting weights to an early phase in training is
required to achieve good performance. As shown, DST methods need more hyperpa-
rameters than PaI and LTH. Further, DST methods might need to regularly compute
the full gradient of the network and increase the training time in order to reach their
best performance.
As mentioned, presented results for LTs use a small pre-training step for the sparse
initialization. Therefore, we conclude that DST methods achieve the best overall per-
formance for completely sparse training. Moreover, using adaptive bases for the K×K
filters of convolutional layers further improves the proposed pruning and freezing ap-
proaches.
9. Conclusions
In this work we have introduced a general framework to describe the training of
DNNs with reduced dimensionality (DRT). The proposed methods are dominated by
pruning neural networks, but we also discussed freezing parts of the network at its
random initialization. The methods are categorized in pruning at initialization (PaI),
lottery tickets (LTs), dynamic sparse training (DST) and freezing. SOTA methods for
each criterion are presented. Furthermore, the different approaches are compared af-
terwards.
We first discussed that pruning leads to better results than freezing if more param-
eters are optimized whereas for a low number of trained weights, freezing performs
better than pruning. Furthermore, LTs and DST perform better than PaI, while PaI
contains the easiest and least expensive methods. For LTs, many trainings, including
training the dense model, are needed to find the sparse architecture. Also, LTs achieve
their best results if weights are reset to an early phase in training which does not yield
a complete sparse training. DST methods usually need more hyperparameters to be
tuned than LTs and PaI. All proposed DRT methods can be further enhanced by rep-
resenting convolutional filters with an adaptive representation instead of the standard,
spatial one.
Altogether, finding the best DRT method for a specific setup is a trade-off between
available training time and performance of the final network. The training time is
mostly influenced by the number of trainings required to find the sparse architecture,
ranging from 0 (PaI, DST and freezing) to more than 20×(LTs), and the number of
individual trainings required to tune hyperparameters. Also, the need to compute dense
gradients (for some DST and PaI methods) or to pre-train the dense network (LTs) has
to be considered if a DRT method is chosen. As discussed, frozen models have similar
computational costs for inference as densely trained networks. Consequently, if the
sparsity together with the used soft- and hardware can actually reduce computations,
pruning should be preferred to freezing, or frozen parameters should be replaced by
information preserving transformations which are cheap to compute, as for example
the DCT.
29

--- PAGE 30 ---
[1] H. Pham, Z. Dai, Q. Xie, M.-T. Luong, Q. V . Le, Meta pseudo labels, in: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2021.
[2] D. S. Park, Y . Zhang, C. Chiu, Y . Chen, B. Li, W. Chan, Q. V . Le, Y . Wu,
Specaugment on large scale datasets, in: IEEE International Conference on
Acoustics, Speech and Signal Processing, 2020.
[3] C.-Y . Wang, A. Bochkovskiy, H.-Y . M. Liao, Scaled-yolov4: Scaling cross stage
partial network, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2021.
[4] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, Q. V . Le, Xl-
net: Generalized autoregressive pretraining for language understanding, in: Ad-
vances in Neural Information Processing Systems 32, 2019.
[5] R. Schwartz, J. Dodge, N. A. Smith, O. Etzioni, Green AI, Communications of
the ACM 63 (2020) 54–63.
[6] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettle-
moyer, Deep contextualized word representations, in: Proceedings of the 2018
Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 2018.
[7] D. Mahajan, R. Girshick, V . Ramanathan, K. He, M. Paluri, Y . Li, A. Bharambe,
L. van der Maaten, Exploring the limits of weakly supervised pretraining, in:
Proceedings of the European Conference on Computer Vision, 2018.
[8] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, N. Houlsby,
Big transfer (bit): General visual representation learning, in: Proceedings of the
European Conference on Computer Vision, 2020.
[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-
terthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,
N. Houlsby, An image is worth 16x16 words: Transformers for image recog-
nition at scale, in: 9th International Conference on Learning Representations,
2021.
[10] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations for deep
learning in NLP, in: Proceedings of the 57th Conference of the Association for
Computational Linguistics, 2019.
[11] D. Amodei, D. Hernandez, G. Sastry, J. Clark, G. Brockman, I. Sutskever, Ai
and compute, OpenAI Blog, 2018 [Online]. URL: https://openai.com/
blog/ai-and-compute/ , last accessed: 05/16/2022.
[12] J. L. Gustafson, Moore’s law, in: Encyclopedia of Parallel Computing, 2011,
pp. 1177–1184.
30

--- PAGE 31 ---
[13] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations for
modern deep learning research, in: Proceedings of the AAAI Conference on
Artificial Intelligence, 2020.
[14] S. Han, J. Pool, J. Tran, W. Dally, Learning both weights and connections for ef-
ficient neural network, in: Advances in Neural Information Processing Systems
28, 2015.
[15] M. Courbariaux, Y . Bengio, J.-P. David, Binaryconnect: Training deep neural
networks with binary weights during propagations, in: Advances in Neural
Information Processing Systems 28, 2015.
[16] J. Wu, C. Leng, Y . Wang, Q. Hu, J. Cheng, Quantized convolutional neural net-
works for mobile devices, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2016.
[17] A. Zhou, A. Yao, Y . Guo, L. Xu, Y . Chen, Incremental network quantization:
Towards lossless cnns with low-precision weights, in: 5th International Confer-
ence on Learning Representations, 2017.
[18] D. Zhang, J. Yang, D. Ye, G. Hua, Lq-nets: Learned quantization for highly
accurate and compact deep neural networks, in: Proceedings of the European
Conference on Computer Vision, 2018.
[19] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam,
D. Kalenichenko, Quantization and training of neural networks for efficient
integer-arithmetic-only inference, in: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2018.
[20] R. Li, Y . Wang, F. Liang, H. Qin, J. Yan, R. Fan, Fully quantized network for
object detection, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2019.
[21] S. J. Nowlan, G. E. Hinton, Simplifying neural networks by soft weight-sharing,
Neural Computation 4 (1992) 473–493.
[22] W. Chen, J. Wilson, S. Tyree, K. Weinberger, Y . Chen, Compressing neural
networks with the hashing trick, in: Proceedings of the 32nd International Con-
ference on Machine Learning, 2015.
[23] K. Ullrich, E. Meeds, M. Welling, Soft weight-sharing for neural network com-
pression, in: 5th International Conference on Learning Representations, 2017.
[24] J. Xue, J. Li, Y . Gong, Restructuring of deep neural network acoustic models
with singular value decomposition, in: Interspeech, 2013.
[25] V . Lebedev, Y . Ganin, M. Rakhuba, I. V . Oseledets, V . S. Lempitsky, Speeding-
up convolutional neural networks using fine-tuned cp-decomposition, in: 3rd
International Conference on Learning Representations, 2015.
31

--- PAGE 32 ---
[26] A. Novikov, D. Podoprikhin, A. Osokin, D. P. Vetrov, Tensorizing neural net-
works, in: Advances in Neural Information Processing Systems 28, 2015.
[27] T. Sainath, B. Kingsbury, V . Sindhwani, E. Arisoy, B. Ramabhadran, Low-rank
matrix factorization for deep neural network training with high-dimensional out-
put targets, in: IEEE International Conference on Acoustics, Speech and Signal
Processing, 2013.
[28] E. Denton, W. Zaremba, J. Bruna, Y . LeCun, R. Fergus, Exploiting linear struc-
ture within convolutional networks for efficient evaluation, in: Advances in
Neural Information Processing Systems 27, 2014.
[29] B. Liu, M. Wang, H. Foroosh, M. Tappen, M. Pensky, Sparse convolutional
neural networks., in: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2015.
[30] S. A. Janowsky, Pruning versus clipping in neural networks, Physical Review
A 39 (1989) 6600–6603.
[31] M. C. Mozer, P. Smolensky, Skeletonization: A technique for trimming the fat
from a network via relevance assessment, in: Advances in Neural Information
Processing Systems 1, 1989.
[32] E. D. Karnin, A simple procedure for pruning back-propagation trained neural
networks, IEEE Transactions on Neural Networks 1 (1990) 239–242.
[33] Y . LeCun, J. S. Denker, S. A. Solla, Optimal brain damage, in: Advances in
Neural Information Processing Systems 2, 1990.
[34] Y . Guo, A. Yao, Y . Chen, Dynamic network surgery for efficient dnns, in:
Advances in Neural Information Processing Systems 29, 2016.
[35] X. Qian, D. Klabjan, A probabilistic approach to neural network pruning, in:
Proceedings of the 38th International Conference on Machine Learning, 2021.
[36] D. W. Blalock, J. J. G. Ortiz, J. Frankle, J. V . Guttag, What is the state of neural
network pruning?, in: Proceedings of Machine Learning and Systems 2, 2020.
[37] S. Arora, R. Ge, B. Neyshabur, Y . Zhang, Stronger generalization bounds for
deep nets via a compression approach, in: Proceedings of the 35th International
Conference on Machine Learning, 2018.
[38] B. Bartoldson, A. Morcos, A. Barbu, G. Erlebacher, The generalization-stability
tradeoff in neural network pruning, in: Advances in Neural Information Process-
ing Systems 33, 2020.
[39] M. Barsbey, M. Sefidgaran, M. A. Erdogdu, G. Richard, U. Simsekli, Heavy tails
in SGD and compressibility of overparametrized neural networks, in: Advances
in Neural Information Processing Systems 34, 2021.
32

--- PAGE 33 ---
[40] S. Anwar, K. Hwang, W. Sung, Structured pruning of deep convolutional neural
networks, ACM Journal on Emerging Technologies in Computing Systems 13
(2017) 1–18.
[41] J. Chen, S. Chen, S. J. Pan, Storage efficient and dynamic flexible runtime
channel pruning via deep reinforcement learning, in: Advances in Neural Infor-
mation Processing Systems 33, 2020.
[42] Z. Huang, N. Wang, Data-driven sparse structure selection for deep neural net-
works, in: Proceedings of the European Conference on Computer Vision, 2018.
[43] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y . Guo, Q. Wu, J. Huang, J. Zhu,
Discrimination-aware channel pruning for deep neural networks, in: Advances
in Neural Information Processing Systems 31, 2018.
[44] T. Zhuang, Z. Zhang, Y . Huang, X. Zeng, K. Shuang, X. Li, Neuron-level struc-
tured pruning using polarization regularizer, in: Advances in Neural Information
Processing Systems 33, 2020.
[45] D. Joo, E. Yi, S. Baek, J. Kim, Linearly replaceable filters for deep network
channel pruning, in: Proceedings of the AAAI Conference on Artificial Intelli-
gence, 2021.
[46] Y . Wang, X. Zhang, X. Hu, B. Zhang, H. Su, Dynamic network pruning with
interpretable layerwise channel selection, in: Proceedings of the AAAI Confer-
ence on Artificial Intelligence, 2020.
[47] J. Frankle, M. Carbin, The lottery ticket hypothesis: Finding sparse, trainable
neural networks, in: 6th International Conference on Learning Representations,
2018.
[48] N. Lee, T. Ajanthan, P. H. Torr, SNIP: Single-shot network pruning based on
connection sensitivity, in: 7th International Conference on Learning Represen-
tations, 2019.
[49] C. Wang, G. Zhang, R. Grosse, Picking winning tickets before training by pre-
serving gradient flow, in: 8th International Conference on Learning Represen-
tations, 2020.
[50] H. Tanaka, D. Kunin, D. L. Yamins, S. Ganguli, Pruning neural networks with-
out any data by iteratively conserving synaptic flow, in: Advances in Neural
Information Processing Systems 33, 2020.
[51] H. Li, A. Kadav, I. Durdanovic, H. Samet, H. P. Graf, Pruning filters for efficient
convnets, in: 5th International Conference on Learning Representations, 2017.
[52] H. Mao, S. Han, J. Pool, W. Li, X. Liu, Y . Wang, W. J. Dally, Exploring the
granularity of sparsity in convolutional neural networks, in: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition Workshops,
2017.
33

--- PAGE 34 ---
[53] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, W. J. Dally, Eie:
Efficient inference engine on compressed deep neural network, ACM SIGARCH
Computer Architecture News 44 (2016) 243–254.
[54] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan, B. Khailany,
J. Emer, S. W. Keckler, W. J. Dally, Scnn, in: Proceedings of the 44th Annual
International Symposium on Computer Architecture, ACM, 2017.
[55] G.-B. Huang, Q.-Y . Zhu, C.-K. Siew, Extreme learning machine: a new learning
scheme of feedforward neural networks, in: IEEE International Joint Confer-
ence on Neural Networks 2, 2004.
[56] E. Hoffer, I. Hubara, D. Soudry, Fix your classifier: the marginal value of
training the last weight layer, in: 6th International Conference on Learning
Representations, 2018.
[57] A. Rosenfeld, J. K. Tsotsos, Intriguing properties of randomly weighted net-
works: Generalizing while learning next to nothing, in: Conference on Com-
puter and Robot Vision, 2019.
[58] P. Wimmer, J. Mehnert, A. P. Condurache, FreezeNet: Full performance by
reduced storage costs, in: Proceedings of the Asian Conference on Computer
Vision, 2020.
[59] Y .-L. Sung, V . Nair, C. Raffel, Training neural networks with fixed sparse masks,
in: Advances in Neural Information Processing Systems 34, 2021.
[60] G. Bellec, D. Kappel, W. Maass, R. Legenstein, Deep rewiring: Training very
sparse deep networks, in: 6th International Conference on Learning Represen-
tations, 2018.
[61] P. Wimmer, J. Mehnert, A. P. Condurache, COPS: Controlled pruning before
training starts, in: International Joint Conference on Neural Networks, 2021.
[62] P. Wimmer, J. Mehnert, A. P. Condurache, Interspace pruning: Using
adaptive filter representations to improve training of sparse CNNs, CoRR
abs/2203.07808 (2022). URL: https://arxiv.org/abs/2203.07808 ,
last accessed: 05/16/2022.
[63] J. Diffenderfer, B. Kailkhura, Multi-prize lottery ticket hypothesis: Finding
accurate binary neural networks by pruning a randomly weighted network, in:
9th International Conference on Learning Representations, 2021.
[64] H. Wang, C. Qin, Y . Zhang, Y . Fu, Emerging paradigms of neural network prun-
ing, CoRR abs/2103.06460v2 (2021). URL: https://arxiv.org/abs/
2103.06460v2 , last accessed: 05/05/2022.
[65] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectifiers: Surpassing human-
level performance on imagenet classification, in: IEEE International Conference
on Computer Vision, 2015.
34

--- PAGE 35 ---
[66] X. Glorot, Y . Bengio, Understanding the difficulty of training deep feedforward
neural networks, in: Proceedings of the 13th International Conference on Arti-
ficial Intelligence and Statistics, 2010.
[67] A. M. Saxe, J. L. McClelland, S. Ganguli, Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks, in: 2nd International Con-
ference on Learning Representations, 2014.
[68] J. Martens, Deep learning via hessian-free optimization, in: Proceedings of the
27th International Conference on Machine Learning, 2010.
[69] I. Sutskever, J. Martens, G. Dahl, G. Hinton, On the importance of initializa-
tion and momentum in deep learning, in: Proceedings of the 30th International
Conference on Machine Learning, 2013.
[70] B. Hanin, D. Rolnick, How to start training: The effect of initialization and
architecture, in: Advances in Neural Information Processing Systems 31, 2018.
[71] H. Robbins, S. Monro, A stochastic approximation method, The Annals of
Mathematical Statistics 22 (1951) 400–407.
[72] J. Duchi, E. Hazan, Y . Singer, Adaptive subgradient methods for online learning
and stochastic optimization, Journal of Machine Learning Research 12 (2011)
2121–2159.
[73] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: 3rd
International Conference on Learning Representations, 2015.
[74] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, in: 7th Inter-
national Conference on Learning Representations, 2019.
[75] A. Krogh, J. A. Hertz, A simple weight decay can improve generalization, in:
Advances in Neural Information Processing Systems 4, 1992.
[76] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R. Salakhutdinov, Im-
proving neural networks by preventing co-adaptation of feature detectors, CoRR
abs/1207.0580 (2012). URL: http://arxiv.org/abs/1207.0580 , last
accessed: 03/04/2022.
[77] J. Ba, B. Frey, Adaptive dropout for training deep neural networks, in: Advances
in Neural Information Processing Systems 26, 2013.
[78] D. P. Kingma, T. Salimans, M. Welling, Variational dropout and the local repa-
rameterization trick, in: Advances in Neural Information Processing Systems
28, 2015.
[79] S. Ioffe, C. Szegedy, Batch normalization: Accelerating deep network training
by reducing internal covariate shift, in: Proceedings of the 32nd International
Conference on Machine Learning, 2015.
35

--- PAGE 36 ---
[80] Y . Yao, L. Rosasco, A. Caponnetto, On early stopping in gradient descent learn-
ing, Constructive Approximation 26 (2007) 289–315.
[81] W. Finnoff, F. Hergert, H. G. Zimmermann, Improving model selection by non-
convergent methods, Neural Networks 6 (1993) 771–783.
[82] T. S. Cohen, M. Welling, Group equivariant convolutional networks, in: Pro-
ceedings of the 33rd International Conference on Machine Learning, 2016.
[83] M. Jaderberg, K. Simonyan, A. Zisserman, K. Kavukcuoglu, Spatial transformer
networks, in: Advances in Neural Information Processing Systems 28, 2015.
[84] M. Rath, A. P. Condurache, Invariant integration in deep convolutional feature
space, in: 28th European Symposium on Artificial Neural Networks, Computa-
tional Intelligence and Machine Learning, 2020.
[85] M. Rath, A. P. Condurache, Improving the sample-complexity of deep classifi-
cation networks with invariant integration, in: Proceedings of the 17th Interna-
tional Joint Conference on Computer Vision, Imaging and Computer Graphics
Theory and Applications, 2022.
[86] B. Coors, A. P. Condurache, A. Geiger, Spherenet: Learning spherical represen-
tations for detection and classification in omnidirectional images, in: Proceed-
ings of the European Conference on Computer Vision, 2018.
[87] J. Lust, A. P. Condurache, Gran: An efficient gradient-norm based detector
for adversarial and misclassified examples, in: 28th European Symposium on
Artificial Neural Networks, Computational Intelligence and Machine Learning,
2020.
[88] J. Lust, A. P. Condurache, Efficient detection of adversarial, out-of-distribution
and other misclassified samples, Neurocomputing 470 (2022) 335–343.
[89] J. Ren, P. J. Liu, E. Fertig, J. Snoek, R. Poplin, M. Depristo, J. Dillon, B. Laksh-
minarayanan, Likelihood ratios for out-of-distribution detection, in: Advances
in Neural Information Processing Systems 32, 2019.
[90] J. Serr `a, D. ´Alvarez, V . G ´omez, O. Slizovskaia, J. F. N ´u˜nez, J. Luque, Input
complexity and out-of-distribution detection with likelihood-based generative
models, in: 8th International Conference on Learning Representations, 2020.
[91] B. Hassibi, D. Stork, Second order derivatives for network pruning: Optimal
brain surgeon, in: Advances in Neural Information Processing Systems 4, 1992.
[92] H. Mostafa, X. Wang, Parameter efficient training of deep convolutional neural
networks by dynamic sparse reparameterization, in: Proceedings of the 36th
International Conference on Machine Learning, 2019.
[93] I. Price, J. Tanner, Dense for the price of sparse: Improved performance of
sparsely initialized networks via a subspace offset, in: Proceedings of the 38th
International Conference on Machine Learning, 2021.
36

--- PAGE 37 ---
[94] G.-B. Huang, D. H. Wang, Y . Lan, Extreme learning machines: a survey, Inter-
national journal of machine learning and cybernetics 2 (2011) 107–122.
[95] Y . Qing, Y . Zeng, Y . Li, G.-B. Huang, Deep and wide feature based extreme
learning machine for image classification, Neurocomputing 412 (2020) 426–
436.
[96] A. Saxe, P. W. Koh, Z. Chen, M. Bhand, B. Suresh, A. Ng, On random weights
and unsupervised feature learning, in: Proceedings of the 28th International
Conference on Machine Learning, 2011.
[97] R. Giryes, G. Sapiro, A. M. Bronstein, Deep neural networks with random gaus-
sian weights: A universal classification strategy?, IEEE Trans. Signal Process.
64 (2016) 3444–3457.
[98] C. Li, H. Farkhoor, R. Liu, J. Yosinski, Measuring the intrinsic dimension of
objective landscapes, in: 6th International Conference on Learning Representa-
tions, 2018.
[99] W. Tinney, J. Walker, Direct solutions of sparse network equations by optimally
ordered triangular factorization, Proceedings of the IEEE 55 (1967) 1801–1809.
[100] Z. Liu, M. Sun, T. Zhou, G. Huang, T. Darrell, Rethinking the value of network
pruning, in: 7th International Conference on Learning Representations, 2019.
[101] Y . Wang, X. Zhang, L. Xie, J. Zhou, H. Su, B. Zhang, X. Hu, Pruning from
scratch, in: Proceedings of the AAAI Conference on Artificial Intelligence,
2020.
[102] S. Verdenius, M. Stol, P. Forr ´e, Pruning via iterative ranking of sensitivity statis-
tics, CoRR abs/2006.00896v2 (2020). URL: https://arxiv.org/abs/
2006.00896v2 , last accessed: 04/19/2022.
[103] J. Park, S. R. Li, W. Wen, P. T. P. Tang, H. Li, Y . Chen, P. Dubey, Faster cnns with
direct sparse convolutions and guided pruning, in: 5th International Conference
on Learning Representations, 2017.
[104] S. Liu, D. C. Mocanu, A. R. R. Matavalam, Y . Pei, M. Pechenizkiy, Sparse evo-
lutionary deep learning with over one million artificial neurons on commodity
hardware, Neural Comput. Appl. 33 (2021) 2589–2604.
[105] E. Elsen, M. Dukhan, T. Gale, K. Simonyan, Fast sparse convnets, in: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020.
[106] T. Gale, M. Zaharia, C. Young, E. Elsen, Sparse gpu kernels for deep learning,
in: Proceedings of the International Conference for High Performance Comput-
ing, Networking, Storage and Analysis, 2020.
37

--- PAGE 38 ---
[107] Z. Wang, Sparsert: Accelerating unstructured sparsity on gpus for deep learning
inference, in: Proceedings of the ACM International Conference on Parallel
Architectures and Compilation Techniques, 2020.
[108] I. Hubara, B. Chmiel, M. Island, R. Banner, J. Naor, D. Soudry, Accelerated
sparse neural training: A provable and efficient method to find n:m transposable
masks, in: Advances in Neural Information Processing Systems 34, 2021.
[109] A. Zhou, Y . Ma, J. Zhu, J. Liu, Z. Zhang, K. Yuan, W. Sun, H. Li, Learning n:m
fine-grained structured sparse neural networks from scratch, in: 9th International
Conference on Learning Representations, 2021.
[110] W. Sun, A. Zhou, S. Stuijk, R. G. J. Wijnhoven, A. Nelson, H. Li, H. Corporaal,
Dominosearch: Find layer-wise fine-grained n:m sparse schemes from dense
neural networks, in: Advances in Neural Information Processing Systems 34,
2021.
[111] J. Pool, C. Yu, Channel permutations for n:m sparsity, in: Advances in Neural
Information Processing Systems 34, 2021.
[112] C. Holmes, M. Zhang, Y . He, B. Wu, NxMTransformer: Semi-structured sparsi-
fication for natural language understanding via ADMM, in: Advances in Neural
Information Processing Systems 34, 2021.
[113] NVIDIA, Nvidia a100 tensor core gpu architecture, 2020. URL:
https://images.nvidia.com/aem-dam/en-zz/Solutions/
data-center/nvidia-ampere-architecture-whitepaper.
pdf, last accessed: 03/30/2022.
[114] V . Sanh, T. Wolf, A. M. Rush, Movement pruning: Adaptive sparsity by fine-
tuning, in: Advances in Neural Information Processing Systems 33, 2020.
[115] S. Jayakumar, R. Pascanu, J. Rae, S. Osindero, E. Elsen, Top-kast: Top-k always
sparse training, in: Advances in Neural Information Processing Systems 33,
2020.
[116] D. Mocanu, E. Mocanu, P. Stone, P. Nguyen, M. Gibescu, A. Liotta, Scalable
training of artificial neural networks with adaptive sparse connectivity inspired
by network science, Nature Communications 9 (2018) 2383.
[117] U. Evci, T. Gale, J. Menick, P. S. Castro, E. Elsen, Rigging the lottery: Making
all tickets winners, in: Proceedings of the 37th International Conference on
Machine Learning, 2020.
[118] T. Dettmers, L. Zettlemoyer, Sparse networks from scratch: Faster training
without losing performance, CoRR abs/1907.04840v2 (2019). URL: http:
//arxiv.org/abs/1907.04840v2 , last accessed: 03/31/2022.
[119] H. Zhou, J. Lan, R. Liu, J. Yosinski, Deconstructing lottery tickets: Zeros, signs,
and the supermask, in: Advances in Neural Information Processing Systems 32,
2019.
38

--- PAGE 39 ---
[120] J. Frankle, G. K. Dziugaite, D. Roy, M. Carbin, Linear mode connectivity and
the lottery ticket hypothesis, in: Proceedings of the 37th International Confer-
ence on Machine Learning, 2020.
[121] W. T. Redman, M. FONOBEROV A, R. Mohr, Y . Kevrekidis, I. Mezic, An op-
erator theoretic view on pruning deep neural networks, in: 10th International
Conference on Learning Representations, 2022.
[122] I. Mezi ´c, Spectral properties of dynamical systems, model reduction and de-
compositions, Nonlinear Dynamics 41 (2005) 309–325.
[123] J. Lee, S. Park, S. Mo, S. Ahn, J. Shin, Layer-adaptive sparsity for the
magnitude-based pruning, in: 9th International Conference on Learning Rep-
resentations, 2021.
[124] J. Frankle, G. K. Dziugaite, D. Roy, M. Carbin, Pruning neural networks at
initialization: Why are we missing the mark?, in: 9th International Conference
on Learning Representations, 2021.
[125] N. Lee, T. Ajanthan, S. Gould, P. H. S. Torr, A signal propagation perspective
for pruning neural networks at initialization, in: 8th International Conference
on Learning Representations, 2020.
[126] P. de Jorge, A. Sanyal, H. Behl, P. Torr, G. Rogez, P. K. Dokania, Progressive
skeletonization: Trimming more fat from a network at initialization, in: 9th
International Conference on Learning Representations, 2021.
[127] S. Hayou, J.-F. Ton, A. Doucet, Y . W. Teh, Robust pruning at initialization, in:
9th International Conference on Learning Representations, 2021.
[128] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, S. Ganguli, Exponential ex-
pressivity in deep neural networks through transient chaos, in: Advances in
Neural Information Processing Systems 29, 2016.
[129] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein, Deep information
propagation, in: 5th International Conference on Learning Representations,
2017.
[130] L. Xiao, Y . Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington, Dynami-
cal isometry and a mean field theory of cnns: How to train 10, 000-layer vanilla
convolutional neural networks, in: Proceedings of the 35th International Con-
ference on Machine Learning, 2018.
[131] B. Neyshabur, R. Salakhutdinov, N. Srebro, Path-sgd: Path-normalized opti-
mization in deep neural networks, in: Advances in Neural Information Process-
ing Systems 28, 2015.
[132] B. Neyshabur, R. Tomioka, N. Srebro, Norm-based capacity control in neural
networks, in: Proceedings of The 28th Conference on Learning Theory, 2015.
39

--- PAGE 40 ---
[133] T. Gebhart, U. Saxena, P. Schrater, A unified paths perspective for pruning at
initialization, CoRR abs/2101.10552 (2021). URL: https://arxiv.org/
abs/2101.10552 , last accessed: 04/19/2022.
[134] S. M. Patil, C. Dovrolis, PHEW: Constructing sparse networks that learn fast and
generalize well without training data, in: Proceedings of the 38th International
Conference on Machine Learning, 2021.
[135] V . Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, M. Rastegari, What’s
hidden in a randomly weighted neural network?, in: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.
[136] M. M. Aladago, L. Torresani, Slot machines: Discovering winning combina-
tions of random weights in neural networks, in: Proceedings of the 38th Inter-
national Conference on Machine Learning, 2021.
[137] A. Mallya, D. Davis, S. Lazebnik, Piggyback: Adapting a single network to
multiple tasks by learning to mask weights, in: Proceedings of the European
Conference on Computer Vision, 2018.
[138] Y . Zhang, M. Lin, F. Chao, Y . Wang, Y . Wu, F. Huang, M. Xu, Y . Tian,
R. Ji, Lottery jackpots exist in pre-trained models, CoRR abs/2104.08700v5
(2021). URL: https://arxiv.org/abs/2104.08700v5 , last accessed:
05/02/2022.
[139] Y . Bengio, N. L ´eonard, A. C. Courville, Estimating or propagating gra-
dients through stochastic neurons for conditional computation, CoRR
abs/1308.3432 (2013). URL: http://arxiv.org/abs/1308.3432 , last
accessed: 03/31/2022.
[140] T. Liu, F. Zenke, Finding trainable sparse networks through neural tangent trans-
fer, in: Proceedings of the 37th International Conference on Machine Learning,
2020.
[141] A. Jacot, C. Hongler, F. Gabriel, Neural tangent kernel: Convergence and gen-
eralization in neural networks, in: Advances in Neural Information Processing
Systems 31, 2018.
[142] S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, R. Wang, On exact com-
putation with an infinitely wide neural net, in: Advances in Neural Information
Processing Systems 32, 2019.
[143] J. Lee, L. Xiao, S. Schoenholz, Y . Bahri, R. Novak, J. Sohl-Dickstein, J. Pen-
nington, Wide neural networks of any depth evolve as linear models under gradi-
ent descent, in: Advances in Neural Information Processing Systems 32, 2019.
[144] A. Morcos, H. Yu, M. Paganini, Y . Tian, One ticket to win them all: generalizing
lottery ticket initializations across datasets and optimizers, in: Advances in
Neural Information Processing Systems 32, 2019.
40

--- PAGE 41 ---
[145] Z. Zhang, X. Chen, T. Chen, Z. Wang, Efficient lottery ticket finding: Less
data is more, in: Proceedings of the 38th International Conference on Machine
Learning, 2021.
[146] H. You, C. Li, P. Xu, Y . Fu, Y . Wang, X. Chen, R. G. Baraniuk, Z. Wang, Y . Lin,
Drawing early-bird tickets: Toward more efficient training of deep networks, in:
8th International Conference on Learning Representations, 2020.
[147] T. Gale, E. Elsen, S. Hooker, The state of sparsity in deep neural networks,
in: 36th International Conference on Machine Learning Joint Workshop on On-
Device Machine Learning & Compact Deep Neural Network Representations
(ODML-CDNNR), 2019.
[148] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recogni-
tion, in: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016.
[149] J. Frankle, D. J. Schwab, A. S. Morcos, The early phase of neural network
training, in: 8th International Conference on Learning Representations, 2020.
[150] B. Elesedy, V . Kanade, Y . W. Teh, Lottery tickets in linear models: An analysis
of iterative magnitude pruning, in: Sparsity in Neural Networks Workshop,
2021.
[151] J. S. Rosenfeld, J. Frankle, M. Carbin, N. Shavit, On the predictability of prun-
ing across scales, in: Proceedings of the 38th International Conference on Ma-
chine Learning, 2021.
[152] T. Chen, J. Frankle, S. Chang, S. Liu, Y . Zhang, M. Carbin, Z. Wang, The lottery
tickets hypothesis for supervised and self-supervised pre-training in computer
vision models, in: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2021.
[153] H. Yu, S. Edunov, Y . Tian, A. S. Morcos, Playing the lottery with rewards and
multiple languages: lottery tickets in rl and nlp, in: 8th International Conference
on Learning Representations, 2020.
[154] T. Chen, J. Frankle, S. Chang, S. Liu, Y . Zhang, Z. Wang, M. Carbin, The
lottery ticket hypothesis for pre-trained bert networks, in: Advances in Neural
Information Processing Systems 33, 2020.
[155] M. Vischer, R. T. Lange, H. Sprekeler, On lottery tickets and minimal task rep-
resentations in deep reinforcement learning, in: 10th International Conference
on Learning Representations, 2022.
[156] R. V . Soelen, J. W. Sheppard, Using winning lottery tickets in transfer learning
for convolutional neural networks, in: International Joint Conference on Neural
Networks, 2019.
41

--- PAGE 42 ---
[157] S. Girish, S. R. Maiya, K. Gupta, H. Chen, L. S. Davis, A. Shrivastava, The lot-
tery ticket hypothesis for object recognition, in: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2021.
[158] X. Chen, Y . Cheng, S. Wang, Z. Gan, J. Liu, Z. Wang, The elastic lottery ticket
hypothesis, in: Advances in Neural Information Processing Systems 34, 2021.
[159] A. Renda, J. Frankle, M. Carbin, Comparing rewinding and fine-tuning in neural
network pruning, in: 8th International Conference on Learning Representations,
2020.
[160] D. H. Le, B.-S. Hua, Network pruning that matters: A case study on retraining
variants, in: 9th International Conference on Learning Representations, 2021.
[161] Y . Bai, H. Wang, Z. TAO, K. Li, Y . Fu, Dual lottery ticket hypothesis, in: 10th
International Conference on Learning Representations, 2022.
[162] J. Su, Y . Chen, T. Cai, T. Wu, R. Gao, L. Wang, J. D. Lee, Sanity-checking
pruning methods: Random tickets can win the jackpot, in: Advances in Neural
Information Processing Systems 33, 2020.
[163] E. S. Lubana, R. Dick, A gradient flow framework for analyzing network prun-
ing, in: 9th International Conference on Learning Representations, 2021.
[164] S. Zhang, B. C. Stadie, One-shot pruning of recurrent neural networks by jaco-
bian spectrum evaluation, in: 8th International Conference on Learning Repre-
sentations, 2020.
[165] M. Alizadeh, S. A. Tailor, L. M. Zintgraf, J. van Amersfoort, S. Farquhar, N. D.
Lane, Y . Gal, Prospect pruning: Finding trainable weights at initialization using
meta-gradients, in: 10th International Conference on Learning Representations,
2022.
[166] N. Koster, O. Grothe, A. Rettinger, Signing the supermask: Keep, hide, invert,
in: 10th International Conference on Learning Representations, 2022.
[167] X. Chen, J. Zhang, Z. Wang, Peek-a-boo: What (more) is disguised in a ran-
domly weighted neural network, and how to find it efficiently, in: 10th Interna-
tional Conference on Learning Representations, 2022.
[168] J. Fischer, R. Burkholz, Plant ’n’ seek: Can you find the winning ticket?, in:
10th International Conference on Learning Representations, 2022.
[169] S. Liu, T. Chen, X. Chen, L. Shen, D. C. Mocanu, Z. Wang, M. Pechenizkiy,
The unreasonable effectiveness of random pruning: Return of the most naive
baseline for sparse training, in: 10th International Conference on Learning Rep-
resentations, 2022.
[170] E. Malach, G. Yehudai, S. Shalev-Schwartz, O. Shamir, Proving the lottery
ticket hypothesis: Pruning is all you need, in: Proceedings of the 37th Interna-
tional Conference on Machine Learning, 2020.
42

--- PAGE 43 ---
[171] A. Pensia, S. Rajput, A. Nagle, H. Vishwakarma, D. Papailiopoulos, Optimal
lottery tickets via subset sum: Logarithmic over-parameterization is sufficient,
in: Advances in Neural Information Processing Systems 33, 2020.
[172] L. Orseau, M. Hutter, O. Rivasplata, Logarithmic pruning is all you need, in:
Advances in Neural Information Processing Systems 33, 2020.
[173] D. Chijiwa, S. Yamaguchi, Y . Ida, K. Umakoshi, T. Inoue, Pruning randomly
initialized neural networks with iterative randomization, in: Advances in Neural
Information Processing Systems 34, 2021.
[174] R. Burkholz, N. Laha, R. Mukherjee, A. Gotovos, On the existence of universal
lottery tickets, in: 10th International Conference on Learning Representations,
2022.
[175] A. da Cunha, E. Natale, L. Viennot, Proving the lottery ticket hypothesis for
convolutional neural networks, in: 10th International Conference on Learning
Representations, 2022.
[176] S. Zagoruyko, N. Komodakis, Wide residual networks, in: Proceedings of the
British Machine Vision Conference, 2016.
[177] J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei, Imagenet: A large-
scale hierarchical image database, in: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2009.
[178] A. R. Chambers, S. Rumpel, A stable brain from unstable components: Emerg-
ing concepts and implications for neural computation, Neuroscience 357 (2017)
172–184.
[179] S. S. Du, X. Zhai, B. Poczos, A. Singh, Gradient descent provably optimizes
over-parameterized neural networks, in: 7th International Conference on Learn-
ing Representations, 2019.
[180] Y . Li, Y . Liang, Learning overparameterized neural networks via stochastic gra-
dient descent on structured data, in: Advances in Neural Information Processing
Systems 31, 2018.
[181] A. Brutzkus, A. Globerson, E. Malach, S. Shalev-Shwartz, SGD learns over-
parameterized networks that provably generalize on linearly separable data, in:
6th International Conference on Learning Representations, 2018.
[182] S. Liu, L. Yin, D. C. Mocanu, M. Pechenizkiy, Do we actually need dense
over-parameterization? In-time over-parameterization in sparse training, in:
Proceedings of the 38th International Conference on Machine Learning, 2021.
[183] X. Ding, G. Ding, X. Zhou, Y . Guo, J. Han, J. Liu, Global sparse momentum
sgd for pruning very deep neural networks, in: Advances in Neural Information
Processing Systems 32, 2019.
43

--- PAGE 44 ---
[184] A. Kusupati, V . Ramanujan, R. Somani, M. Wortsman, P. Jain, S. Kakade,
A. Farhadi, Soft threshold weight reparameterization for learnable sparsity, in:
Proceedings of the 37th International Conference on Machine Learning, 2020.
[185] J. Liu, Z. Xu, R. Shi, R. C. C. Cheung, H. K. So, Dynamic sparse training:
Find efficient sparse network from scratch with trainable masked layers, in: 8th
International Conference on Learning Representations, 2020.
[186] J. Schwarz, S. Jayakumar, R. Pascanu, P. E. Latham, Y . W. Teh, Powerpropa-
gation: A sparsity inducing weight reparameterisation, in: Advances in Neural
Information Processing Systems 34, 2021.
[187] X. Zhou, W. Zhang, H. Xu, T. Zhang, Effective sparsification of neural networks
with global sparsity constraint, in: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2021.
[188] X. Zhou, W. Zhang, Z. Chen, S. Diao, T. Zhang, Efficient neural network train-
ing via forward and backward propagation sparsification, in: Advances in Neu-
ral Information Processing Systems 34, 2021.
[189] Y .-H. Pao, Y . Takefuji, Functional-link net computing: theory, system architec-
ture, and functionalities, Computer 25 (1992) 76–79.
[190] Y .-H. Pao, G.-H. Park, D. J. Sobajic, Learning and generalization characteristics
of the random vector functional-link net, Neurocomputing 6 (1994) 163–180.
[191] A. Krizhevsky, Learning multiple layers of features from tiny images, Univer-
sity of Toronto (2012). URL: http://www.cs.toronto.edu/ ˜kriz/
cifar.html , last accessed: 05/13/2022.
[192] J. Frankle, D. J. Schwab, A. S. Morcos, Training batchnorm and only batch-
norm: On the expressive power of random features in CNNs, in: 9th Interna-
tional Conference on Learning Representations, 2021.
[193] U. Evci, Y . Dauphin, Y . Ioannou, C. Keskin, Gradient flow in sparse neural
networks and how lottery tickets win, in: Proceedings of the AAAI Conference
on Artificial Intelligence, 2022.
[194] Y . LeCun, L. Bottou, Y . Bengio, P. Haffner, Gradient-based learning applied to
document recognition, Proceedings of the IEEE 86 (1998) 2278–2324.
[195] P. Erd ˝os, A. R ´enyi, On random graphs I, Publicationes Mathematicae Debrecen
6 (1959) 290–297.
44
