# 2206.08464.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/random/2206.08464.pdf
# Kích thước tệp: 49832256 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
PRANC: Mạng Giả Ngẫu Nhiên để Nén Compact các mô hình học sâu
Parsa Nooralinejad
Đại học California, DavisAli Abbasi
Đại học VanderbiltSoroush Abbasi Koohpayegani*
Đại học California, Davis
Kossar Pourahmadi Meibodi*
Đại học California, DavisRana Muhammad Shahroz Khan*
Đại học Vanderbilt
Soheil Kolouri
Đại học VanderbiltHamed Pirsiavash
Đại học California, Davis
Tóm tắt
Chúng tôi chứng minh rằng một mô hình học sâu có thể được tái tham số hóa như một tổ hợp tuyến tính của nhiều mô hình học sâu được khởi tạo ngẫu nhiên và cố định trong không gian trọng số. Trong quá trình huấn luyện, chúng tôi tìm kiếm các cực tiểu địa phương nằm trong không gian con được bao trùm bởi các mô hình ngẫu nhiên này (tức là, các mạng 'cơ sở'). Khung công việc của chúng tôi, PRANC, cho phép nén đáng kể một mô hình học sâu. Mô hình có thể được tái tạo bằng cách sử dụng một 'hạt giống' vô hướng duy nhất được sử dụng để tạo ra các mạng 'cơ sở' giả ngẫu nhiên, cùng với các hệ số hỗn hợp tuyến tính đã học. Trong các ứng dụng thực tế, PRANC giải quyết thách thức lưu trữ và truyền thông hiệu quả các mô hình học sâu, một nút thắt cổ chai phổ biến trong nhiều tình huống, bao gồm học đa tác nhân, người học liên tục, hệ thống liên bang và thiết bị biên, trong số những tình huống khác. Trong nghiên cứu này, chúng tôi sử dụng PRANC để cô đọng các mô hình phân loại hình ảnh và nén hình ảnh bằng cách nén các mạng nơ-ron ẩn liên quan của chúng. PRANC vượt trội hơn các phương pháp cơ bản với một biên độ lớn trong phân loại hình ảnh khi nén một mô hình học sâu gần 100 lần. Hơn nữa, chúng tôi cho thấy rằng PRANC cho phép suy luận tiết kiệm bộ nhớ bằng cách tạo ra các trọng số từng lớp một cách linh hoạt. Mã nguồn của PRANC ở đây: https://github.com/UCDvision/PRANC

1. Giới thiệu
Quan niệm phổ biến là các mô hình học sâu lớn hơn mang lại độ chính xác tốt hơn. Tuy nhiên, vẫn chưa rõ liệu khả năng tổng quát hóa tốt hơn của các mô hình lớn hơn xuất phát từ sự phức tạp gia tăng của kiến trúc hay nhiều tham số hơn. Hơn nữa, trong số nhiều cực tiểu địa phương tốt trong hàm mất mát, việc huấn luyện tìm ra một cái. Trong bài báo này, chúng tôi giới thiệu một cách tiếp cận mới: xem một mô hình học sâu như một tổ hợp tuyến tính trong không gian trọng số của nhiều mô hình được khởi tạo ngẫu nhiên và cố định. Trong quá trình học, mục tiêu của chúng tôi chuyển sang tìm kiếm một cực tiểu tồn tại trong không gian con được xác định bởi các mô hình ban đầu này. Những phát hiện của chúng tôi làm nổi bật tiềm năng nén đáng kể các mô hình học sâu bằng cách chỉ giữ lại giá trị hạt giống của trình tạo giả ngẫu nhiên và các hệ số cho việc kết hợp trọng số.

Việc tái tham số hóa hiệu quả này mang lại lợi ích cho các ứng dụng AI và ML bằng cách giảm kích thước mô hình học sâu để lưu trữ hoặc truyền thông dễ dàng hơn. Trong các mạng nơ-ron hiện đại với hàng triệu đến hàng tỷ tham số, việc lưu trữ và truyền thông trở nên tốn kém. Vấn đề này trở nên tồi tệ hơn trong môi trường có tốc độ bit thấp do các ràng buộc vật lý hoặc gián đoạn đối nghịch. Ví dụ, các ứng dụng dưới nước có thể có băng thông thấp đến 100 bit mỗi giây, khi đó, việc truyền mô hình ResNet18 với 11 triệu tham số mất hơn 40 ngày trong điều kiện như vậy. Hơn nữa, trong học phân tán với nhiều tác nhân, các mạng WiFi băng thông cao vẫn gặp phải vấn đề tắc nghẽn.

Ngoài truyền thông, việc tải hoặc lưu trữ các mô hình lớn này trên thiết bị biên đặt ra một thách thức đáng kể khác. Thiết bị biên thường đi kèm với bộ nhớ nhỏ không phù hợp để lưu trữ các mạng nơ-ron lớn và có thể muốn chạy mô hình ít thường xuyên hơn (theo yêu cầu). Do đó, chúng có thể hưởng lợi từ việc nén một mô hình học sâu thành ít tham số hơn để xây dựng mô hình từng lớp hoặc thậm chí từng kernel theo yêu cầu để chạy mỗi lần suy luận. Điều này sẽ dẫn đến chi phí I/O thấp hơn đáng kể.

Người ta có thể nén mô hình bằng cách chưng cất nó thành một mô hình nhỏ hơn [18], cắt tỉa các tham số mô hình [29], lượng tử hóa các tham số [26], hoặc chia sẻ các trọng số càng nhiều càng tốt [40, 6]. Gần đây hơn, chưng cất tập dữ liệu [48] được đề xuất. Nó có thể được xem như một thay thế cho nén mô hình vì người ta có thể lưu trữ hoặc truyền thông tập dữ liệu đã chưng cất và sau đó huấn luyện lại mô hình khi cần thiết. Tuy nhiên, hầu hết

--- TRANG 2 ---
Mạng Cơ sở Ngẫu nhiênNhiệm vụ Huấn luyện
𝛼
1×+
𝛼
2×+
𝛼
𝑘×
Hình 2. Mô hình Đe dọa Tấn công Có mục tiêu: Đầu tiên mô hình tự giám sát được huấn luyện trên tập dữ liệu không có nhãn bị đầu độc. Các kích hoạt được thêm vào hình ảnh của Rottweiler là danh mục mục tiêu. Sau đó chúng tôi huấn luyện một bộ phân loại tuyến tính trên đầu embedding mô hình tự giám sát cho một nhiệm vụ giám sát xuôi dòng. Tại thời điểm kiểm tra, bộ phân loại tuyến tính có độ chính xác cao trên hình ảnh sạch nhưng phân loại sai những hình ảnh tương tự như Rottweiler khi kích hoạt được dán vào chúng. ual ins pe ction tương đương với việc chú thích toàn bộ dữ liệu itself . Ví dụ, chúng ta chắc chắn rằng không ai đã ins pe cted một tỷ hình ảnh ngẫu nhiên, không có nhãn và không được chọn lọc của công chúng Ins ta -gram sử dụng trong việc huấn luyện SEER để đảm bảo tập lệnh thu thập dữ liệu đã không tải xuống những chất độc được thao túng bởi kẻ tấn công. Do đó, nhu cầu làm việc với dữ liệu la rger và đa dạng hơn để loại bỏ thiên lệch dữ liệu và giảm chi phí la be ling cũng có thể unknowingly thiết lập nhiều cách thức hơn cho adversaries. Augmentations trong exemplar-based SSL : Mos t recent thành công ful SSL methods là exempla r-based, e.g. MoCov2, BYOL, Si mCL R, MS F[3,19,23,36]. Core ide a là pull embedding của hai augmentati ons khác nhau của một im-age gần nhau [19] trong khi, trong một số methods, [23] al so đẩy chúng xa khỏi other random ima ge s . Trong các methods này, ima ge augmentati on đóng vai trò quan trọng của inductive bias mà guides representation learning. Mos t methods đã cho thấy rằng việc sử dụng augmenta-tion aggressive hơn cải thiện learned representations. Một người có thể tranh luận rằng tấn công của chúng tôi hoạt động si nce trong một số it-erations, một augmentati on của poisoned ima ge chứa trigger trong khi augmentati on khác không. Sau đó, điều này khuyến khích model associa te features của trigger với poisoned class, dẫn đến việc phát hiện poisoned classes ngay cả khi không có poisoned cate-gory. Tuy nhiên, các thí nghiệm controlled rộng rãi của chúng tôi không cung cấp bằng chứng empirical cho giả thuyết này. At-tack không hoạt động nếu trigger chỉ visible trong một view (xem Secti on 5.3). Chúng tôi hypothesize rằng tấn công của chúng tôi hoạt động do lý do follow-ing: Si nce trong learning, trigger present trên target category only, model học appearance của trigger như context cho target category. Si nce trig-ger có rigid shape với very smal l variation, nó là rela-tivel y easy feature cho model learn để detect. Do đó, model xây dựng very good detector cho trigger sao cho ngay cả khi absence của other features của target cate-gory tại test time, model sti l l predicts target cate-gory, dẫn đến successful ful attack. Thí nghiệm của chúng tôi cho thấy rằng bằng poisoning chỉ 0.5% của unlabeled training data, một SSL model như MoCov2, BYOL, hoặc MS F là backdoored để detect target category khi trigger presented tại test time. Như miti-gati on technique, chúng tôi giới thiệu defense method dựa trên knowledge distillation. Nó successful l y neutralizes back-door sử dụng một số clean unlabeled data. 2.Related Wor k Sel fsupervised le arning: Một self-supervised method thường có hai parts: một pretext task, là carefully designed task dựa trên domain knowledge để automati cal l y extract supervision từ data, và loss function. Variety của pretext tasks đã được designed cho learn-ing representations từ ima ge s[8,14,31,32]. Ji gsaw[31] predicts spatial ordering của ima ge s , tương si mi l ar với solving jigs aw puzzles. RotNet[14] sử dụng rotation angl e pre-diction task để learn unsupervised features. Ins ta nce discrimination đã gai ned lot của popularity như pretext task mà involves data augmentati on store-cover hai views của asi ngl e ima ge và sau đó using si m-ilarities giữa chúng để learn representations. Early sel f -supervised methods sử dụng los s e s như reconstruction los s , và triplet los s . Nhưng recently, ins ta nce discrimination pre-text task combined với contrastive los s(MoCo, Si m-CLR)[3,5,23] đã provided huge gai ns trong learning better visual features trong completely unsupervised manner. Me th-ods như BYOL, Si mSia m[6] không use contrastive los s directly nhưng sti l l rely trên ins ta nce discrimination với aug-mented views. MS F[36] generalizes BYOL nơi data2
෠
𝜃
1
෠
𝜃
2
෠
𝜃
𝑘…Task Loss, ℓ(𝛼)(ví dụ, Cross Entropy hoặc Mean Squared Error)
∇
𝛼
ℓ𝜃=෍𝑖𝛼𝑖መ𝜃𝑖
መ
𝜃
1
መ
𝜃
2
መ
𝜃
𝑘,,,…Lưu trữ/Truyền thông
Hạt giống
𝛼,
Lan truyền ngược Classification        INR …
Hạt giống𝑥𝑦𝑅𝐺𝐵
Hình 1. Chúng tôi hạn chế mô hình học sâu thành một tổ hợp tuyến tính của k mô hình được khởi tạo ngẫu nhiên. Vì số lượng mô hình ít hơn nhiều so với kích thước của mô hình, việc truyền thông hoặc lưu trữ các hệ số rẻ hơn nhiều so với chính mô hình hoặc dữ liệu. Chúng tôi điều chỉnh α để giảm thiểu mất mát của nhiệm vụ bằng cách sử dụng lan truyền ngược tiêu chuẩn.

các phương pháp này bị giới hạn ở các yếu tố giảm nhỏ, ví dụ, ít hơn 30×. Ngoài ra, các phương pháp chưng cất kiến thức giảm kiến trúc mô hình xuống một cái nhỏ hơn với ít lớp hơn, điều này có thể hạn chế ứng dụng tương lai của mô hình đó, ví dụ, cho việc tinh chỉnh tương lai hoặc học suốt đời, cần kiến trúc sâu hơn.

Chúng tôi quan tâm đến việc nén một mô hình học sâu bằng một hệ số đáng kể (ví dụ, 100×) mà không thay đổi kiến trúc của nó. Ý tưởng cốt lõi đằng sau cách tiếp cận của chúng tôi rất đơn giản. Chúng tôi ràng buộc mô hình của chúng tôi thành một tổ hợp tuyến tính của một tập hữu hạn các mô hình được khởi tạo ngẫu nhiên, gọi là các mô hình cơ sở. Do đó, vấn đề quy về việc tìm các hệ số hỗn hợp tuyến tính tối ưu dẫn đến một mạng có thể giải quyết nhiệm vụ hiệu quả. Mô hình sau đó có thể được biểu diễn một cách súc tích bằng hạt giống (một vô hướng duy nhất) để tạo ra các mô hình cơ sở giả ngẫu nhiên và các hệ số hỗn hợp tuyến tính (Xem Hình 1). Phương pháp của chúng tôi có thể được xem như một cách tái tham số hóa mới lạ của một mô hình học sâu mà không thay đổi kiến trúc của nó. Điều này cho phép chúng tôi nghiên cứu tác động của việc tăng kích thước của kiến trúc (cả chiều sâu và chiều rộng) mà không thay đổi số lượng tham số được tối ưu hóa (xem Hình 3).

Ngoài hiệu quả, phương pháp đề xuất của chúng tôi cung cấp truyền thông và lưu trữ an toàn, điều này có ý nghĩa quan trọng trong các ứng dụng liên quan đến an ninh mạng và quyền riêng tư. Tóm lại, các mô hình 'cơ sở' của chúng tôi được tạo ra bằng các trình tạo giả ngẫu nhiên với một 'hạt giống' cụ thể. Hạt giống này có thể được chia sẻ riêng tư giữa các thực thể được xác thực. Với sự tự tương quan tối thiểu của các chuỗi giả ngẫu nhiên, một thay đổi nhỏ trong hạt giống tạo ra một tập hợp các mô hình 'cơ sở' hoàn toàn khác, làm cho các hệ số hỗn hợp tuyến tính được chia sẻ công khai trở nên vô dụng đối với các bên không được ủy quyền. Lựa chọn thiết kế này tạo điều kiện thuận lợi cho truyền thông và lưu trữ an toàn, đặc biệt trong các ứng dụng an ninh mạng hoặc nhạy cảm về quyền riêng tư.

Về mặt lý thuyết, việc tham số hóa quá mức là quan trọng trong các mạng nơ-ron đương đại, tăng cường sức mạnh biểu diễn của chúng và đơn giản hóa việc tối ưu hóa do cảnh quan mất mát được cải thiện [33, 30]. Các giải pháp của những hệ thống tham số hóa quá mức này thường tạo thành một đa tạp có chiều dương [8], với các hệ thống lớn hơn thiếu các cực tiểu không toàn cục [34]. Xem xét sự phong phú của các giải pháp tốt, chúng tôi kiểm tra xem liệu chúng ta có thể hạn chế việc tìm kiếm giải pháp vào các không gian con chiều thấp được xác định bởi các vector ngẫu nhiên trong không gian trọng số (tức là, các mạng 'cơ sở'). Các thí nghiệm của chúng tôi xác nhận khả năng tìm kiếm các giải pháp tốt trong các không gian con ngẫu nhiên chiều rất thấp trong không gian trọng số của các mạng tham số hóa quá mức, thúc đẩy các điều tra lý thuyết sâu hơn.

Đóng góp: Dưới đây là những đóng góp cụ thể của chúng tôi:
• Giới thiệu PRANC, một khung tái tham số hóa mạng đơn giản nhưng hiệu quả có hiệu quả về bộ nhớ trong cả giai đoạn học và tái tạo,
• Đánh giá tính hiệu quả của PRANC trong việc nén các mô hình nhận dạng hình ảnh cho các tập dữ liệu điểm chuẩn và kiến trúc mô hình khác nhau, cho thấy độ chính xác cao hơn với ít tham số hơn nhiều so với các phương pháp cơ bản gần đây rộng rãi,
• Chứng minh tính hiệu quả của PRANC cho nén hình ảnh bằng cách nén các biểu diễn nơ-ron ẩn cho cả hình ảnh tự nhiên và y tế,
• Thể hiện tiềm năng của PRANC trong các ứng dụng yêu cầu truyền thông mã hóa của các mô hình (hoặc dữ liệu được biểu diễn qua các mô hình).

2. Công trình liên quan
Mạng ngẫu nhiên: Một số công trình trước đây [35, 31, 7, 12] đã cho thấy rằng các mạng được khởi tạo ngẫu nhiên có một mạng con cạnh tranh với mạng gốc về độ chính xác. Một số bài báo gần đây như [50] đã giới thiệu một ứng dụng để sử dụng thực tế này trong học liên tục. Thay vì tìm kiếm các mạng con trong một mạng được tạo ngẫu nhiên (tức là, che mặt), chúng tôi tìm kiếm một tổ hợp tuyến tính của một tập nhỏ các mạng được tạo ngẫu nhiên, được ký hiệu là các mô hình cơ sở, có thể giải quyết nhiệm vụ.

Nén mô hình: Nén mô hình không phải là một chủ đề mới. HashedNet [6] sử dụng nhóm trọng số với hàm băm để giảm số lượng tham số có thể học. Nó có thể được xem như một trường hợp cụ thể của phương pháp chúng tôi trong đó các mô hình ngẫu nhiên là nhị phân với số lượng đơn vị bằng nhau và mỗi trọng số của mô hình gốc chỉ có một trong các mô hình ngẫu nhiên. [6] thí nghiệm với MLP trên các tập dữ liệu nhỏ. Chúng tôi tái tạo HashedNet cho thiết lập của chúng tôi và cho thấy rằng phương pháp của chúng tôi vượt trội hơn nó. Tương tự như HashedNet, Weight Fixed Network (WFN) [40] nén mô hình

--- TRANG 3 ---
bằng cách giảm thiểu entropy và số lượng tham số duy nhất trong một mạng. WFN bảo tồn độ chính xác của mô hình với việc giảm 10× kích thước lưu trữ. Thay vì chia sẻ cứng các trọng số trong HasedNet, [45] sử dụng chia sẻ mềm. Mặc dù tất cả các phương pháp này giảm số lượng tham số, chúng đều cần giữ chỉ số của mỗi phần tử để tái tạo mạng. Han et al. [15] sử dụng cắt tỉa, lượng tử hóa và mã hóa Huffman để đạt được tỷ lệ nén thường ít hơn 50×. Các cách tiếp cận gần đây hơn như MIRACLE [16] và weightless [37] đã cho thấy kết quả đầy hứa hẹn với tỷ lệ nén cao hơn nhiều (ví dụ, +400×). Tuy nhiên, chúng sử dụng các kiến trúc lớn, ví dụ VGG có 150M tham số, vì vậy ngay cả sau khi nén 400×, vẫn còn hơn 300K tham số (nhiều hơn một ResNet-20 dày đặc). Chúng tôi cho thấy rằng chúng ta có thể giảm số lượng tham số cần thiết giữ nguyên kiến trúc mạng.

Cắt tỉa và lượng tử hóa mô hình: Nén một mô hình có thể được định nghĩa là giảm số lượng byte cần thiết để lưu trữ một mô hình học sâu. Một số bài báo như XNOR-NET [36] và EWGS [26] sử dụng lượng tử hóa trọng số/kích hoạt (W/A) để giảm kích thước của một mạng. Mặc dù lượng tử hóa W/A đã được chứng minh là một cách tiếp cận hiệu quả để giảm kích thước mạng trong khi duy trì độ chính xác, nó chủ yếu được thiết kế để tối ưu hóa tính toán cho suy luận mạng. Một cách tiếp cận khác được sử dụng để nén mô hình là cắt tỉa tập hợp các trọng số ít quan trọng về zero, điều này giảm số lượng phép toán điểm nổi (FLOPS) và cũng có thể giảm lượng dữ liệu cần thiết để lưu trữ và truyền thông một mạng. Các phương pháp này bao gồm: Neuron Merging [20], Dynamic pruning [29, 38], ChipNet [43], Pruning at initializing [17], Wang et.al. [46], và Collaborative Compression (CC) [28]. Một lần nữa, hầu hết các phương pháp này sử dụng các yếu tố thưa thớt là 20× hoặc ít hơn, thấp hơn mục tiêu của chúng tôi trong bài báo này. Chúng tôi so sánh phương pháp của chúng tôi, PRANC, với các công trình hiện có cung cấp tỷ lệ nén cực đại (+99% tỷ lệ cắt tỉa), ví dụ, DPF[29], STR[23], và SuRP[19]. Cuối cùng, có một số công trình trước đây phân tích các bộ lọc mô hình như một tổ hợp tuyến tính của một số bộ lọc cơ sở [14, 2]. Mục tiêu của các phương pháp như vậy là giảm tính toán chứ không nhất thiết là số lượng tham số. Chúng tôi tập trung vào một số lượng cực kỳ nhỏ các tham số không thể đạt được bằng các phương pháp như vậy.

Nén dữ liệu - tập lõi: Một cách tiếp cận khác để tái tạo một mạng chính xác là lưu trữ hoặc truyền thông tập dữ liệu huấn luyện của nó và huấn luyện một mạng trong tác nhân đích. Vì hầu hết các tập dữ liệu đều lớn, các phương pháp được đề xuất để tổng hợp siêu dữ liệu dưới dạng hình ảnh hoặc lấy một tập lõi của tập dữ liệu. Các phương pháp này bao gồm: Dataset Distillation (DD) [48], hồi quy hình ảnh và tốc độ học, Flexible Dataset Distillation (FDD) [4], hồi quy nhãn giả cho hình ảnh thật, soft labeling dataset distillation (SLDD) [41], tạo ra nhãn giả và hình ảnh. Tất cả các phương pháp này yêu cầu hạt giống khởi tạo mạng. Các phương pháp khác, bao gồm Dataset Condensation với distribution matching (DM) [52], với differentiable Siamese augmentation (DSA) [51], và Dataset distillation bằng matching training trajectories (DDMT) [5] đã tiến thêm một bước và phát triển các cách tiếp cận độc lập với hạt giống. Các phương pháp này thường dựa vào tối ưu hóa bậc hai, đắt đỏ về mặt tính toán và hạn chế ứng dụng của chúng. Hơn nữa, kích thước dữ liệu cần thiết để lưu trữ trong các phương pháp này tỷ lệ với kích thước của hình ảnh đầu vào. Chúng tôi cho thấy rằng PRANC cung cấp độ chính xác tốt hơn với ít tham số hồi quy hơn nhiều trên các kiến trúc tương tự so với các cách tiếp cận đã đề cập.

Nén hình ảnh: Một số codec phổ biến như JPEG dựa trên các mô-đun thủ công để nén hình ảnh. Một dòng khác của các phương pháp nén hình ảnh là các cách tiếp cận dựa trên học. Các cách tiếp cận này thường huấn luyện một auto-encoder trên một tập lớn các hình ảnh [3, 32, 25, 9, 13] và lưu trữ mã. Phương pháp sử dụng INR của chúng tôi cũng dựa trên học nhưng khác với các kỹ thuật trên vì mô hình được học trên một hình ảnh duy nhất (để overfit) thay vì trên một tập hợp các hình ảnh. Do đó, nó có thể không gặp phải thiên lệch của dữ liệu huấn luyện. COIN [10] có lẽ gần nhất với phương pháp của chúng tôi, overfits một INR và lưu trữ tất cả các tham số. Chúng tôi khác vì chúng tôi nén INR bằng cách tái tham số hóa nó như một tổ hợp tuyến tính của các mạng ngẫu nhiên và lưu trữ các hệ số.

3. Phương pháp
Chúng tôi quan tâm đến việc huấn luyện một mô hình học sâu với một số lượng rất nhỏ các tham số để việc chuyển mô hình từ tác nhân này sang tác nhân khác hoặc lưu trữ nó trên một tác nhân có bộ nhớ nhỏ ít tốn kém hơn. Điều này trái ngược với mục tiêu của hầu hết các công trình trước đây (ví dụ, nén mô hình, cắt tỉa hoặc lượng tử hóa) nhằm giảm tính toán suy luận hoặc cải thiện khả năng tổng quát hóa trong khi bảo tồn độ chính xác. Do đó, chúng tôi giới thiệu một biểu diễn compact giả định không thay đổi trong kích thước mô hình, số lượng tham số khác không hoặc độ chính xác của tính toán.

Chúng tôi giả định rằng mô hình học sâu có thể được viết như một tổ hợp tuyến tính của một tập các mô hình được khởi tạo ngẫu nhiên, gọi là cơ sở. Vì chúng ta có thể sử dụng một trình tạo giả ngẫu nhiên để tạo ra các mô hình ngẫu nhiên, người ta có thể truyền thông hoặc lưu trữ tất cả các mô hình ngẫu nhiên bằng cách chỉ truyền thông hoặc lưu trữ một vô hướng duy nhất là hạt giống của trình tạo giả ngẫu nhiên. Mặc dù các mô hình cơ sở không nhất thiết phải trực giao với nhau, tích vô hướng từng cặp của chúng gần bằng không vì số lượng mẫu (mô hình) nhỏ hơn nhiều so với chiều của mỗi mô hình. Sau đó chúng tôi tối ưu hóa các trọng số của mỗi mô hình cơ sở để tổ hợp tuyến tính của chúng có thể giải quyết nhiệm vụ (ví dụ, phân loại hình ảnh).

Chính thức hơn, cho một tập các hình ảnh huấn luyện {xi}N
i=1 và các nhãn tương ứng {yi}N
i=1 của chúng, chúng tôi muốn huấn luyện một

--- TRANG 4 ---
Mạng 
Cơ sở 
Ngẫu nhiên, 
!
"
!
#
!
!
"
!Mô tả Cảnh quan Mất mát
Cực tiểu 
Địa phương
Cực tiểu Địa phương
Hình 2. Một minh họa đơn giản về cảnh quan mất mát của một mô hình với hai tham số và một mô hình cơ sở. Không có cực tiểu địa phương nào trong hai cái có thể nằm trong span của các mô hình cơ sở, vì vậy chúng tôi tìm kiếm α để tìm một cực tiểu địa phương trong span của các mô hình cơ sở.

mô hình học sâu f(.;θ) với các tham số θ∈Rd sao cho f(xi;θ) dự đoán yi. Thực hành tiêu chuẩn là tối ưu hóa θ bằng cách giảm thiểu rủi ro thực nghiệm:
R(θ) =1
NPN
i=1L(f(xi;θ), yi)
trong đó L(·,·) là một thước đo độ bất đồng, ví dụ, cross-entropy. Trong việc truyền thông mô hình như vậy, chúng ta cần gửi một vector chiều cao θ chứa d scalars.

Để giảm chi phí truyền thông, chúng tôi giả định một tập các mô hình cơ sở được khởi tạo ngẫu nhiên với các tham số {ˆθj}k
j=1. Các k mô hình cơ sở này được tạo ra bằng cách sử dụng một hạt giống đã biết và được cố định trong suốt quá trình học. Sau đó chúng tôi định nghĩa:
θ:=Pk
j=1αjˆθj, trong đó αj là một trọng số vô hướng cho mô hình cơ sở thứ j. Giả định rằng k≪d, sẽ ít tốn kém hơn nhiều để truyền thông hoặc lưu trữ α thay vì θ.

Để tối ưu hóa α, người ta có thể đầu tiên tối ưu hóa cho θ để tìm θ∗ và sau đó hồi quy nó bằng cách giảm thiểu:
arg minα||θ∗−Pk
j=1αjˆθj||2
Tuy nhiên, vì k≪d, giải pháp tối ưu θ∗ có thể xa khỏi span của các mô hình cơ sở, dẫn đến một giải pháp kém hơn (cũng được thể hiện thực nghiệm trong các thí nghiệm của chúng tôi). Chúng tôi lập luận rằng có vô số giải pháp cho θ tốt như θ∗, vì vậy chúng ta có thể tìm kiếm một cái có sai số dư nhỏ hơn khi được chiếu vào span của các mô hình cơ sở. Do đó, chúng tôi tìm kiếm một giải pháp giảm thiểu mất mát nhiệm vụ trong span của các mô hình cơ sở bằng cách tối ưu hóa:
arg min
αX
iL
f(xi;kX
j=1αjˆθj), yi
(1)
Lưu ý rằng tại thời điểm kiểm tra, sau khi tái tạo mô hình bằng tổ hợp tuyến tính, suy luận cho PRANC hoàn toàn giống như mô hình dày đặc tiêu chuẩn.

Hiệu quả tối ưu hóa: Lưu ý rằng việc tối ưu hóa rất đơn giản và hiệu quả vì dL
dα=dL
dθdθ
dα và dθ
dαj=ˆθj. Do đó, chúng tôi sử dụng lan truyền ngược tiêu chuẩn để tính dL
dθ và sau đó nhân với ma trận các mô hình cơ sở để có:
dL
dα=dL
dθ×ˆθ

Hiệu quả bộ nhớ trong huấn luyện: Lưu ý rằng ma trận các mô hình cơ sở ˆθ rất lớn, vì vậy việc giữ nó trong bộ nhớ không hiệu quả. Do đó, chúng tôi chia ma trận này thành nhiều khối nhỏ hơn, và tại mỗi lần lặp, chúng tôi tạo ra mỗi khối bằng cách sử dụng một trình tạo giả ngẫu nhiên tại chính GPU, thực hiện phép nhân, loại bỏ khối và chuyển sang khối tiếp theo. Phương pháp này giảm dấu chân bộ nhớ với một hệ số lớn với chi phí tạo ra toàn bộ cơ sở ngẫu nhiên một lần mỗi lần lặp, điều này rất hiệu quả trong các GPU hiện đại. Chọn các khối 100 giá trị alpha cho ResNet18 tiêu thụ gần 4.4GB (tức là 11M×4×100) bộ nhớ GPU là hợp lý.

Hiệu quả tái tạo mô hình: Vì các mô hình cơ sở được tạo ra bằng cách sử dụng một trình tạo giả ngẫu nhiên, chúng ta có thể tái tạo mô hình bằng cách sử dụng một trung bình chạy đơn giản của các mô hình cơ sở: tạo ra mỗi mục trong ˆθj, nhân nó với αj, thêm nó vào trung bình chạy, loại bỏ mục và chuyển sang mục tiếp theo của ˆθj. Bằng cách này, dấu chân bộ nhớ của việc tái tạo trở nên không đáng kể (tức là d+ 1).

Tái tạo mô hình theo yêu cầu: Trong một số ứng dụng, tác nhân có thể cần chạy suy luận hiếm khi nhưng không có đủ bộ nhớ để giữ mô hình. Thiết bị có thể lưu trữ α, tái tạo mỗi bộ lọc tích chập bằng cách sử dụng các mục tương ứng của các mô hình cơ sở, áp dụng nó vào đầu vào, và sau đó loại bỏ bộ lọc và chuyển sang bộ lọc tiếp theo. Quá trình này có dấu chân bộ nhớ rất nhỏ vì nó cần lưu trữ α và chỉ một bộ lọc tại một thời điểm.

Học phân tán: Để huấn luyện mô hình trên nhiều GPU, chúng tôi sử dụng một thuật toán học phân tán đơn giản để tăng m, số lượng mô hình cơ sở. Chúng tôi chia m mô hình cơ sở giữa g GPU để mỗi GPU chỉ làm việc trên m/g mô hình cơ sở. Sau đó, chúng tôi phân phối α giữa các GPU. Mỗi GPU tính trung bình có trọng số một phần trên các mô hình cơ sở của nó và phân phối nó cho tất cả các GPU. Sau đó, tất cả các GPU sẽ có quyền truy cập vào trung bình có trọng số hoàn chỉnh và sẽ sử dụng nó để thực hiện lan truyền ngược dưới dạng học phân tán tiêu chuẩn và cập nhật tập α của riêng chúng.

Lớp BatchNorm: Chúng tôi giảm thiểu mất mát của nhiệm vụ bằng cách điều chỉnh α thay vì các trọng số mô hình như được thực hiện trong học tiêu chuẩn. Tuy nhiên, các tham số của lớp BatchNorm không được điều chỉnh bởi PRANC. Để đơn giản hóa công việc này, chúng tôi giả định rằng chúng ta có thể truyền thông những tham số đó và bao gồm chúng trong ngân sách. Điều này có ý nghĩa vì số lượng tham số BatchNorm tương đối nhỏ so với số lượng tham số trọng số.

4. Ứng dụng
Chúng tôi kiểm tra khung công việc của chúng tôi trên hai ứng dụng khác nhau.
Mạng phân loại hình ảnh: Trong thiết lập này, chúng tôi tham số hóa một mô hình nhận dạng hình ảnh, ví dụ, ResNet-20 cho CIFAR-10, bằng khung PRANC của chúng tôi và tối ưu hóa các α thay vì các trọng số mô hình. Điều này dẫn đến một mô hình compact có thể được lưu trữ và truyền thông rất hiệu quả.

--- TRANG 5 ---
Nén hình ảnh sử dụng mạng nơ-ron ẩn:
Chúng tôi cũng kiểm tra khung công việc của chúng tôi trong việc nén một mạng nơ-ron ẩn (INR) được over-fitted với một hình ảnh duy nhất [42]. Một INR như vậy nhập tọa độ của pixel và trả về giá trị màu. Do đó, người ta có thể lưu trữ hoặc truyền thông mô hình INR thay vì hình ảnh gốc. Chúng tôi tham số hóa một mô hình INR tiêu chuẩn [42] bằng cách sử dụng khung PRANC để chúng tôi tối ưu hóa các α thay vì các trọng số của mô hình INR. Phương pháp của chúng tôi vượt trội hơn nén JPEG trên hai tập dữ liệu tiêu chuẩn và hai thước đo đánh giá.

5. Thí nghiệm về phân loại hình ảnh
Chúng tôi báo cáo kết quả rộng rãi của PRANC trên các tập dữ liệu, kiến trúc và số lượng mô hình cơ sở khác nhau.

5.1. So sánh với các phương pháp cắt tỉa mô hình:
Để truyền thông một mô hình thưa với tỷ lệ thưa hơn 2×, cần truyền hai số cho mỗi tham số: giá trị của trọng số và chỉ số của nó trong mạng. Do đó, ngay cả khi một phương pháp cắt tỉa mô hình sử dụng hệ số cắt tỉa 99% (giảm 100× kích thước) vì nó phải truyền các chỉ số cùng với các giá trị, việc giảm kích thước thực tế sẽ nhỏ hơn 100×. DPF [29], STR[23], LAMP[27], RiGL[11], và SuRP[19] là các phương pháp SOTA sử dụng độ thưa lớn (+50×) và duy trì độ chính xác hợp lý. Chúng tôi đã sử dụng mã của họ trên CIFAR-10 và CIFAR-100 cùng với kiến trúc ResNet-20 và ResNet-56 và so sánh chúng với phương pháp của chúng tôi trong Bảng 1. PRANC đạt được độ chính xác cao hơn một cách nhất quán với ít tham số hơn. Xin lưu ý rằng tất cả các phương pháp này loại trừ các lớp BatchNorm khỏi quá trình cắt tỉa của chúng. Do đó chúng tôi cũng loại trừ chúng khỏi số lượng tham số. Đối với ResNet-20, số lượng tham số BatchNorm là 2,752 và đối với ResNet-56 là 8,128.

5.2. So sánh với các phương pháp chưng cất mô hình:
Một trong những đường cơ bản quan trọng cho công việc của chúng tôi là chưng cất mô hình. Tuy nhiên, số lượng tham số chúng tôi sử dụng rất nhỏ so với bất kỳ kiến trúc CNN hiện có nào. Ngay cả LeNet[24] (một trong những kiến trúc CNN nhỏ nhất), có hơn 60,000 tham số. Để so sánh PRANC với chưng cất mô hình, chúng tôi huấn luyện một ResNet18 trên CIFAR-10 và chưng cất kiến thức của nó vào một mô hình LeNet. Mặt khác, chúng tôi nén một mô hình ResNet20 bằng cách sử dụng PRANC với 10,000 α và một mô hình ResNet56 với chỉ 5,000 α và so sánh độ chính xác của chúng. Như được thể hiện trong Bảng 2, các kiến trúc được nén bằng PRANC yêu cầu ít tham số hơn gần 5× trong khi đạt được độ chính xác cao hơn với một khoảng cách đáng kể (81.48% so với 74.1%).

5.3. So sánh với các phương pháp chưng cất tập dữ liệu:
Trong Bảng 3, chúng tôi báo cáo độ chính xác và số lượng tham số cho PRANC so sánh với các phương pháp chưng cất tập dữ liệu khác nhau.

Bảng 1. So sánh mô hình của chúng tôi với các phương pháp cắt tỉa SOTA, DPF [29], STR[23], LAMP[27], RiGL[11], và SuRP[19]. "Pr." biểu thị tỷ lệ cắt tỉa. Ngoài ra, khi mạng được cắt tỉa, chúng ta phải giữ hai số cho mỗi trọng số: chính trọng số và vị trí của nó trong mô hình. Lưu ý rằng chúng tôi loại trừ số lượng tham số BatchNorm trong bảng này vì điều đó là hằng số cho tất cả các mô hình. Số này là 2,752 cho Resnet-20 và 8,128 cho ResNet-56.

Phương pháp Dữ liệu Kiến trúc # Tham số không bao gồm Độ chính xác
BatchNorm
Baseline (Pr. 0%) C10 R20 269,722 88.92
DPF(Pr. 98.2%) C10 R20 4,920 ×2 41.86
RiGL(Pr. 99.62%) C10 R20 1026 ×2 50.9
LAMP(Pr. 99.62%) C10 R20 1026 ×2 51.24
SuRP (Pr. 99.62%) C10 R20 1026 ×2 54.22
STR (Pr. 95.5%) C10 R20 12,238 ×2 75.99
Ours C10 R20 1,000 64.59
Ours C10 R20 10,000 81.48
Baseline (Pr. 0%) C10 R56 853,018 91.64
DPF (Pr. 98.43%) C10 R56 13,414 ×2 47.66
SuRP (Pr. 98.73%) C10 R56 10,834 ×2 66.65
STR (Pr. 98.4%) C10 R56 13,312 ×2 67.77
Ours C10 R56 5,000 76.87
Baseline (Pr. 0%) C100 R20 275,572 60.84
DPF (Pr. 96.13%) C100 R20 10,770 ×2 12.25
SuRP (Pr. 97.48%) C100 R20 6,797 ×2 14.46
STR (Pr. 96.12%) C100 R20 10,673 ×2 13.18
Ours C100 R20 5,000 32.33
Baseline (Pr. 0%) C100 R56 858,868 64.32
DPF (Pr. 97.8%) C100 R56 19,264 ×2 19.11
SuRP (Pr. 98.72%) C100 R56 10,919 ×2 14.59
STR (Pr. 97.8%) C100 R56 18,881 ×2 25.98
Ours C100 R56 5,000 32.97

Bảng 2. So sánh với chưng cất mô hình. PRANC vượt trội hơn một LeNet được chưng cất từ ResNet-18 trên CIFAR-10. 2,752 và 8,128 là số lượng tham số BatchNorm mà chúng tôi loại trừ khỏi các hệ số nhưng cần xem xét chúng như các tham số.

Phương pháp Kiến trúc # Tham số Độ chính xác
Chưng cất từ R18 LeNet 62,006 74.1%
Ours R56 5,000 + (8,128) 76.87%
Ours R20 10,000 + (2,752) 81.48%

Hầu hết các phương pháp này dựa trên các cách tiếp cận meta-learning liên quan đến chi phí tính toán cao và dấu chân bộ nhớ tại thời gian huấn luyện, vì vậy chúng bị hạn chế về độ sâu của mô hình. Hơn nữa, chúng cần thực hiện một vài bước gradient descent trong việc xây dựng mô hình. Tuy nhiên, số lượng tham số cần thiết trong các phương pháp chưng cất tập dữ liệu tỷ lệ với kích thước của hình ảnh đầu vào. Ví dụ, trong việc chưng cất CIFAR-10 chỉ thành 10 hình ảnh, chúng ta cần lưu trữ ít nhất 10×32×32×3 tham số. Để có thể so sánh được với các phương pháp chưng cất tập dữ liệu SOTA, chúng tôi sử dụng AlexNet (là một phiên bản đã sửa đổi được mô tả trong [48]) trên CIFAR-10. Đối với CIFAR-100 và tinyImageNet, chúng tôi sử dụng các kiến trúc ConvNet độ sâu 3 và độ sâu 4 chiều rộng 128 được mô tả trong [5], tương ứng. Lưu ý rằng một số phương pháp chưng cất tập dữ liệu không

--- TRANG 6 ---
yêu cầu hạt giống, vì vậy chúng giải quyết một nhiệm vụ thách thức hơn vì dữ liệu đã chưng cất phải có thể điều chỉnh bất kỳ mô hình được khởi tạo ngẫu nhiên nào. Tuy nhiên, vì chúng tôi tập trung vào việc giảm chi phí truyền thông và lưu trữ, việc sử dụng một hạt giống cố định như phần trung tâm của ý tưởng chúng tôi không bị cấm đoán.

Bảng 3. So sánh với các phương pháp chưng cất tập dữ liệu trên các tập dữ liệu và kiến trúc khác nhau. 3-128-Conv và 4-128-Conv đại diện cho ConvNet độ sâu 3 chiều rộng 128 và ConvNet độ sâu 4 chiều rộng 128, tương ứng. "Mô hình đã huấn luyện" là giới hạn trên của phương pháp chúng tôi vì người ta có thể tối ưu hóa tất cả các trọng số và truyền/lưu trữ chúng. Phương pháp của chúng tôi vượt trội hơn các đường cơ bản với một biên độ lớn và ít tham số hơn nhiều.

Phương pháp Nhiệm vụ Kiến trúc # Tham số Độ chính xác
Mô hình đã huấn luyện C10 AlexNet 1,756,426 84.8
FDD [4] C10 AlexNet 397,000 43.2
SLDD [41] C10 AlexNet 308,200 60.0
DD [48] C10 AlexNet 307,200 54.0
DM [52] C10 AlexNet 30,720 26.0
DSA [51] C10 AlexNet 30,720 28.8
DC [53] C10 AlexNet 30,720 28.3
CAFE [47] C10 AlexNet 30,720 30.3
CAFE+DSA [47] C10 AlexNet 30,720 31.6
DDMT [5] C10 AlexNet 30,720 46.3
Ours C10 AlexNet 17,000 76.69
Mô hình đã huấn luyện C100 3-128-Conv 504,420 56.2
FDD [4] C100 3-128-Conv 317,200 11.5
DM [52] C100 3-128-Conv 307,200 11.4
DSA [51] C100 3-128-Conv 307,200 13.9
DC [53] C100 3-128-Conv 307,200 12.8
CAFE+DSA [47] C100 3-128-Conv 307,200 14.0
DDMT [5] C100 3-128-Conv 307,200 24.3
Ours C100 3-128-Conv 15,000 25.57
Mô hình đã huấn luyện tinyIN 4-128-Conv 857,160 37.6
DM [52] tinyIN 4-128-Conv 2,457,600 3.9
DDMT [5] tinyIN 4-128-Conv 2,457,600 8.8
Ours tinyIN 4-128-Conv 15,000 12.02

5.4. Tập dữ liệu và mô hình quy mô lớn:
Cho đến nay, chúng tôi đã cung cấp bằng chứng rằng phương pháp của chúng tôi có thể vượt trội hơn các công trình gần đây trong chưng cất tập dữ liệu, cắt tỉa mô hình và nén mô hình về số lượng tham số so với độ chính xác. Vì phương pháp của chúng tôi khá hiệu quả trong học, đặc biệt so với các cách tiếp cận meta-learning phụ thuộc vào đạo hàm bậc hai của mạng, chúng tôi có thể đánh giá nó trên các mô hình quy mô lớn hơn. Chúng tôi đánh giá phương pháp của chúng tôi trên ImageNet100 với kiến trúc ResNet-18. Bảng 4 cho thấy kết quả. Phương pháp của chúng tôi đạt được độ chính xác 61.08% với ít hơn 1% tham số, trong khi mô hình ResNet-18 tiêu chuẩn đạt được độ chính xác 82.1% với hơn 11 triệu tham số. Vì ResNet-18 là một mô hình khổng lồ, chúng tôi sử dụng một trong những khả năng độc đáo của PRANC, đó là tạo ra mạng một cách linh hoạt. Chúng tôi chỉ sử dụng một GPU NVIDIA 3090 duy nhất và huấn luyện mô hình của chúng tôi trong 200 epoch, sử dụng trình tối ưu hóa Adam và bộ lập lịch bước với γ = 0.5 cho mỗi 50 epoch và tốc độ học ban đầu là 0.001. Ngoài ra, chúng tôi phân phối ngân sách vector α của chúng tôi qua các lớp, tức là, chúng tôi sử dụng 4,000 hệ số cho mỗi lớp của bộ mã hóa tích chập và 20,000 hệ số cho bộ phân loại của chúng tôi (cho chúng tôi tổng cộng 100,000 hệ số).

Bảng 4. Kết quả của phương pháp chúng tôi trên tập dữ liệu ImageNet-100 và ResNet-18. 19,200 là tổng số tham số của tất cả các lớp BatchNorm trong mô hình của chúng tôi

Phương pháp # Tham số Độ chính xác
mô hình đã huấn luyện 11,227,812 82.1%
HashedNet [6] 110,000 + (19,200) 52.96%
Ours 100,000 + (19,200) 61.08%
Ours 200,000 + (19,200) 67.28%

5.5. Nghiên cứu ablation:
Trong PRANC, k, số lượng mô hình cơ sở, là một siêu tham số. Một câu hỏi là: "k sẽ ảnh hưởng đến hiệu suất như thế nào?" Hơn nữa, có thể tranh luận rằng "tại sao chúng ta cố gắng tìm một tổ hợp tuyến tính chính xác trong nhiệm vụ? tại sao không cố gắng hồi quy toàn bộ mạng đã được huấn luyện?" Ngoài ra, "k có thể quan trọng hơn kiến trúc không? tức là, chúng ta có thể sử dụng k lớn với một mạng nhỏ và vẫn có độ chính xác cao không?" Trong phần này, chúng tôi trả lời những câu hỏi này.

Độ nhạy cảm với hạt giống: Vì một trong những ứng dụng của PRANC là trong học liên bang, đáng để thảo luận về khả năng mã hóa giả của nó. Chúng tôi thí nghiệm với việc thay đổi hạt giống tại thời điểm tái tạo. Trên CIFAR-10 với AlexNet, với một thay đổi nhỏ trong hạt giống, độ chính xác của mô hình được tái tạo giảm từ 74.0% xuống 9.4%, gần với ngẫu nhiên. Điều này được mong đợi vì các mô hình cơ sở mới không tương quan với những cái được sử dụng trong huấn luyện. Thực tế này có nghĩa là hạt giống có thể hoạt động như một khóa chung giữa các tác nhân và ngay cả khi α bị chặn, việc tái tạo mô hình gần như không thể. Do đó, trong các ứng dụng học liên bang có liên quan đến truyền thông an toàn của các mô hình học sâu, PRANC có thể được sử dụng như cả phương pháp nén và mã hóa.

Hồi quy θ∗ trực tiếp: Chúng ta có thể đầu tiên huấn luyện một mô hình để có θ∗ và sau đó tối ưu hóa cho α bằng cách hồi quy giải pháp đó bằng cách sử dụng mất mát MSE trong không gian tham số. Như được thể hiện trong Hình 2, điều này có thể không thành công vì mô hình tối ưu có thể không nằm trong span của các mô hình cơ sở, và cũng mất mát MSE trong không gian tham số không nhất thiết tương quan với mất mát nhiệm vụ. Bảng 5 cho thấy rằng độ chính xác của đường cơ bản này sử dụng 10,000 tham số gần với ngẫu nhiên.

Tác động của việc thay đổi k so với kiến trúc: Chúng tôi thực hiện một nghiên cứu ablation để hiểu tác động của số lượng mô hình cơ sở, k so với kiến trúc của mô hình. Người ta có thể lập luận rằng đôi khi tốt hơn là thiết kế một kiến trúc tốt hơn thay vì tăng số lượng α. Ở đây, chúng tôi thay đổi k

--- TRANG 7 ---
Bảng 5. Kết quả hồi quy một mô hình đã được huấn luyện trước bằng cách sử dụng 10,000 mô hình cơ sở. C10 và C100 biểu thị tập dữ liệu CIFAR-10 và CIFAR-100 và tinyIN biểu thị tập dữ liệu tiny ImageNet

Tập dữ liệu Kiến trúc Độ chính xác đầy đủ Độ chính xác hồi quy
C10 AlexNet 84.8 % 10.0%
C10 LeNet [24] 73.5% 12.74%
C100 3-128-Conv 56.2 % 1.14%
C100 AlexNet 50.7% 1.0%
tinyIN 4-128-Conv 37.6 % 0.5%

từ 1,000 đến 20,000 cho LeNet, AlexNet, ResNet-20, và ResNe-56 trên CIFAR-10 và vẽ độ chính xác trong Hình 3. Tất cả các thí nghiệm đã được thực hiện trong cùng một thiết lập với 400 epoch với trình tối ưu hóa Adam. Như mong đợi, độ chính xác tăng khi chúng ta tăng số lượng hàm cơ sở. Tuy nhiên, như chúng ta có thể thấy, kiến trúc có tác động nhiều hơn đến độ chính xác so với k.

Hình 3. Minh họa tác động của k trong độ chính xác của các kiến trúc khác nhau trên CIFAR-10. Độ chính xác cải thiện bằng cách tăng số lượng mô hình cơ sở. Tuy nhiên, kiến trúc đóng vai trò quan trọng hơn trong độ chính xác so với số lượng cơ sở.

6. Thí nghiệm về nén hình ảnh
Như một ứng dụng khác của PRANC, ở đây chúng tôi cho thấy rằng chúng ta có thể nén một hình ảnh bằng cách nén mạng nơ-ron ẩn (INR) được overfitted với hình ảnh. Mô hình INR nhập tọa độ của một pixel và xuất màu của pixel đó. Chúng tôi sẽ chỉ lưu trữ hoặc truyền thông hạt giống của trình tạo giả ngẫu nhiên và các giá trị α. Chúng tôi sử dụng [42] làm mô hình INR của chúng tôi. INR là một mô hình MLP 4 lớp (3 lớp ẩn) với 512 nơ-ron tại mỗi lớp và một LayerNorm [1] sau mỗi lớp. Chúng tôi mã hóa tọa độ pixel trong đầu vào thành 512 chiều bằng cách sử dụng ánh xạ Fourier và sử dụng ba nơ-ron trong đầu ra cho hình ảnh RGB (các giá trị màu) và một nơ-ron cho hình ảnh X-ray.

Chúng tôi sử dụng điểm nổi độ chính xác một nửa cho các α để giảm thêm chi phí lưu trữ. Chúng tôi sử dụng một tập các α khác nhau cho mỗi lớp của mạng.

Để giảm dấu chân bộ nhớ, chúng tôi chia ma trận mạng cơ sở ˆθ thành các khối nhỏ hơn và tạo ra và sau đó loại bỏ mỗi khối tại GPU cho mỗi lần lặp của việc tối ưu hóa. Tương tự như gradient descent ngẫu nhiên, chúng tôi lấy mẫu ngẫu nhiên một tập con các pixel để được tối ưu hóa tại mỗi lần lặp, dẫn đến hội tụ nhanh hơn do cập nhật tham số thường xuyên hơn.

Chúng tôi đánh giá mô hình của chúng tôi trên ba tập dữ liệu khác nhau: tập dữ liệu Kodak [22] chứa 24 hình ảnh không nén có kích thước 512×768, tập kiểm tra CLIC-mobile [44] chứa 178 hình ảnh độ phân giải cao (ví dụ, 1512×2016), và 64 hình ảnh được lấy mẫu ngẫu nhiên từ tập dữ liệu NIH Chest X-ray [49] bao gồm 100,000 hình ảnh X-ray ngực không được xác định danh tính có kích thước 1024×1024.

Đường cơ bản. Chúng tôi so sánh phương pháp của chúng tôi với các codec hình ảnh thủ công sau: JEPG, JPEG2000, và WebP. Mục tiêu của chúng tôi là cho thấy rằng PRANC là một khung tổng quát hoạt động tốt khi chỉ đơn giản áp dụng cho nén INR ngay lập tức. Do đó, chúng tôi không so sánh nó với các codec tiên tiến hơn như BPG và VTM vì chúng được thiết kế cao và bao gồm các thành phần như mã hóa entropy. Những kết quả đó được trình bày trong tài liệu bổ sung. Chúng tôi cũng so sánh với các phương pháp nén hình ảnh dựa trên học trong phần bổ sung (BMS, MBT, và CST). Lưu ý rằng, không giống như PRANC, các phương pháp này yêu cầu một tập dữ liệu lớn các hình ảnh để huấn luyện trước auto-encoder của chúng. Điều này hạn chế các ứng dụng và cũng có thể làm giảm kết quả cho các điểm dữ liệu ngoài phân phối. Ví dụ, không giống như PRANC, các mô hình được huấn luyện trên một tập huấn luyện có thể không phù hợp cho dữ liệu y tế vì chúng có thể không trung thực với các bất thường cụ thể cho bệnh nhân không được đại diện trong dữ liệu huấn luyện.

COIN [10] có lẽ gần nhất với phương pháp của chúng tôi huấn luyện một INR bằng cách sử dụng SIREN [39] và lưu trữ tất cả các tham số. Vì COIN không sử dụng ánh xạ Fourier, để so sánh công bằng, chúng tôi tạo ra một đường cơ bản tương tự, gọi là 'INR đã huấn luyện,' sử dụng mạng MLP của chúng tôi được mô tả ở trên mà không có khung PRANC. Đối với INR đã huấn luyện, chúng tôi giảm chiều rộng của mạng để phù hợp với tỷ lệ nén của phương pháp chúng tôi và sử dụng điểm nổi độ chính xác một nửa.

Kết quả. Chúng tôi đánh giá mô hình của chúng tôi với hai thước đo: PSNR và MS-SSIM. Lưu ý rằng chúng tôi cố định kiến trúc mạng và thay đổi số lượng giá trị α để có các giá trị bit-per-pixel (bpp) khác nhau cho mỗi hình ảnh. Chúng tôi báo cáo kết quả cho tập dữ liệu Kodak trong Hình 13 và tập dữ liệu CLIC-mobile trong Bảng 11. Phương pháp của chúng tôi vượt trội hơn JPEG và INR. Chúng tôi báo cáo kết quả của X-ray ngực trong Bảng 7. Một hình ảnh ví dụ được thể hiện trong Hình 6 cho tập dữ liệu Kodac và trong Hình 23 cho tập dữ liệu X-ray.

Ablation. Vì chúng tôi tái tạo các trọng số mô hình với các giá trị α, người ta có thể thay đổi kích thước của kiến trúc mà không thay đổi số lượng tham số (giá trị α), do đó, trong PRANC, chúng ta có thể tăng cả chiều rộng và chiều sâu mà không thay đổi bit-rate. Chúng tôi giữ số lượng tham số k = 102 K và thay đổi cả chiều rộng mạng và chiều sâu của mô hình MLP. Kết quả trên tập dữ liệu Kodak được thể hiện trong Bảng 8. Thú vị là, chúng ta có thể cải thiện hiệu suất bằng cách tăng độ sâu mô hình trong khi giữ số lượng

--- TRANG 8 ---
Hình 4. Nén Hình ảnh Tập dữ liệu Kodak: Phương pháp của chúng tôi vượt trội hơn JPEG và 'INR đã huấn luyện' trên cả đánh giá PSNR và MS-SSIM ở các tỷ lệ bit khác nhau. Lưu ý rằng, không giống như các đường cơ bản khác, phương pháp của chúng tôi được học trên một hình ảnh duy nhất và không được làm thủ công, ngoại trừ kiến trúc của mô hình INR, là một MLP đơn giản.

tham số có thể học không đổi.

Sắp xếp giá trị α. Trong Hình 14, chúng tôi thể hiện các hình ảnh được tái tạo với một tập con các giá trị α có giá trị tuyệt đối lớn nhất. Vì chúng tôi có một tập các α khác nhau cho mỗi lớp của MLP trong nén hình ảnh, chúng tôi sắp xếp các giá trị tuyệt đối và chọn p% hàng đầu của mỗi lớp có giá trị cao hơn. Chúng tôi thay đổi p và trực quan hóa các hình ảnh được tái tạo cho mỗi giá trị p. Trong tài liệu bổ sung, cho thiết lập phân loại hình ảnh, chúng tôi cho thấy rằng trong việc tái tạo mô hình ResNet bằng cách sử dụng một tập một phần các giá trị α, việc chọn các giá trị |α| lớn hơn dẫn đến độ chính xác tốt hơn nhiều so với việc chọn một tập con ngẫu nhiên.

Chi tiết triển khai. Đối với mỗi hình ảnh, chúng tôi huấn luyện các giá trị α với 10k lần lặp trên Kodak và 5k lần lặp trên tập dữ liệu CLIC-mobile. Mỗi lần lặp xử lý 25% pixel của hình ảnh được lấy mẫu ngẫu nhiên. Lưu ý rằng việc tăng số lần lặp không thể làm hỏng mô hình vì mục tiêu là overfit với hình ảnh và khả năng tổng quát hóa không phải là vấn đề. Chúng tôi sử dụng trình tối ưu hóa PyTorch Adam [21] với tốc độ học ban đầu là 1e−3 và bộ lập lịch Cosine. Thêm chi tiết về số lượng giá trị α trên mỗi lớp cho mỗi bpp trong tài liệu bổ sung.

7. Hướng tương lai
PRANC có thể cho phép nhiều hướng tương lai:
Mô hình tạo sinh cho bộ nhớ-phát lại: Phương pháp của chúng tôi có thể

Bảng 6. Nén Hình ảnh Tập dữ liệu CLIC-mobile: PRANC vượt trội hơn JPEG và JPEG2000 với bpp nhỏ hơn trên tập dữ liệu này.

Mô hình bpp PSNR MS-SSIM
WebP 0.185 30.07 0.940
JPEG2000 0.126 29.40 0.918
JPEG 0.195 24.82 0.836
INR đã huấn luyện 0.125 26.93 0.864
PRANC (ours) 0.119 29.71 0.920

Bảng 7. Nén hình ảnh Tập dữ liệu X-ray Ngực: Chúng tôi so sánh PRANC với JPEG cho hình ảnh X-ray. Phương pháp của chúng tôi tốt hơn JPEG với bpp thấp hơn. Vì không giống như auto-encoder, PRANC không sử dụng bất kỳ huấn luyện dựa trên tập hợp nào, nó có thể phù hợp hơn cho hình ảnh y tế vì nó có thể bảo tồn các artifact ngoài phân phối quan trọng cho mục đích chẩn đoán. Tuy nhiên, chúng tôi để nghiên cứu điều này cho công việc tương lai.

Mô hình PRANC JPEG
bpp 0.152 0.168
PSNR 36.28 34.25
MS-SSIM 0.972 0.921

Bảng 8. Ảnh hưởng của việc tăng chiều rộng/chiều sâu của mô hình: Chúng ta có thể tăng cả chiều sâu và chiều rộng của mô hình MLP mà không thay đổi số lượng tham số. Khi thay đổi chiều sâu, chúng tôi phân phối lại số lượng giá trị α đều giữa các lớp để giữ tổng số không đổi. Thêm chi tiết trong Supp.

Chiều rộng 128 256 512 1024
PSNR 30.00 32.05 31.5 31.32
MS-SSIM 0.937 0.963 0.959 0.961
Chiều sâu 3 4 5
PSNR 30.78 31.5 32.38
MS-SSIM 0.978 0.959 0.965

được sử dụng để nén một mô hình tạo sinh (ví dụ, GAN hoặc mô hình khuếch tán), trong đó các tham số α có thể được lưu trữ trong tác nhân hoặc gửi đến tác nhân khác. Sau đó, bất kỳ tác nhân nào có thể tái tạo mô hình trong tương lai và rút mẫu từ nó tương tự như các mẫu được sử dụng trước đó để huấn luyện mô hình. Điều này cho phép phát lại bộ nhớ trong học suốt đời trong một tác nhân duy nhất có bộ nhớ hạn chế hoặc trong nhiều tác nhân có truyền thông hạn chế.

Tính compact tiến bộ: Trong phương pháp này, chúng tôi giả định một tập các mô hình cơ sở không có thứ tự cụ thể. Tuy nhiên, người ta có thể tối ưu hóa α để các chỉ số trước đó của α có thể tái tạo một mô hình chấp nhận được. Sau đó, tùy thuộc vào ngân sách truyền thông hoặc lưu trữ, tác nhân đích có thể quyết định cần bao nhiêu tham số α bằng cách đánh đổi giữa độ chính xác và tính compact. Chúng tôi đã cho thấy rằng việc sắp xếp các giá trị α là bước đầu tiên theo hướng này, nhưng người ta có thể học chúng theo thứ tự tầm quan trọng giảm dần như một công việc tương lai.

--- TRANG 9 ---
Hình 5. Ảnh hưởng của việc giữ p% mô hình cơ sở với các giá trị α tuyệt đối cao nhất. Chúng ta có được hình ảnh hợp lý với một tập con nhỏ hơn của các mô hình cơ sở. Người ta có thể tái tạo một hình ảnh xấp xỉ khi nhận được một tập một phần các giá trị α, tương tự như JPEG tiến bộ.

Hình ảnh gốc
Ours (bpp=0.31, PSNR=30.36)Ours (bpp=0.17, PSNR=27.53)
JPEG (bpp=0.31, PSNR=28.85)JPEG (bpp=0.17, PSNR=21.86)

Hình 6. Trực quan hóa Kodak. Chúng tôi so sánh PRANC với JPEG trên hình ảnh 15 của tập dữ liệu Kodak. Xem Supp. để có thêm ví dụ.

8. Kết luận
Chúng tôi đã giới thiệu một phương pháp đơn giản nhưng hiệu quả có thể học một mô hình như một tổ hợp tuyến tính của một tập các mô hình được khởi tạo ngẫu nhiên và cố định. Mô hình cuối cùng có thể được lưu trữ hoặc truyền thông một cách compact bằng cách sử dụng hạt giống của trình tạo giả ngẫu nhiên và các hệ số. Hơn nữa, phương pháp của chúng tôi có dấu chân bộ nhớ nhỏ ở các giai đoạn học hoặc tái tạo. Chúng tôi thực hiện các thí nghiệm rộng rãi trên nhiều tập dữ liệu phân loại hình ảnh với nhiều kiến trúc và cũng trên thiết lập nén hình ảnh và cho thấy rằng phương pháp của chúng tôi đạt được độ chính xác tốt hơn với ít tham số hơn so với các đường cơ bản SOTA. Chúng tôi tin rằng nhiều ứng dụng bao gồm học suốt đời và học phân tán có thể hưởng lợi từ ý tưởng của chúng tôi. Do đó, chúng tôi hy vọng bài báo này mở ra cánh cửa để nghiên cứu các phương pháp nén tiên tiến hơn dựa trên các mạng ngẫu nhiên cố định.

Hạn chế: Như đã thảo luận, một số tham số mô hình, ví dụ, các lớp BatchNorm, không thể dễ dàng được tái tham số hóa

Hình 7. Trực quan hóa X-ray Ngực. Chúng tôi so sánh PRANC và JPEG trên một hình ảnh X-ray Ngực. Xem Supp. để có thêm ví dụ.

bằng cách sử dụng phương pháp của chúng tôi vì chúng được tính toán trực tiếp từ dữ liệu thay vì giảm thiểu mất mát nhiệm vụ. Trong bài báo này, chúng tôi giả định chúng tôi truyền thông chúng mà không thay đổi và bao gồm chúng trong ngân sách của chúng tôi. Cuối cùng, PRANC tốn kém về mặt tính toán cho một số lượng lớn cơ sở, vì vậy hiện tại không phù hợp để huấn luyện các mô hình rất lớn.

Lời cảm ơn: Công việc này được hỗ trợ một phần bởi Cơ quan Dự án Nghiên cứu Tiên tiến Quốc phòng (DARPA) theo Hợp đồng số HR00112190135 và HR00112090023, Không quân Hoa Kỳ theo Hợp đồng số FA8750-19-C-0098, và tài trợ từ các khoản tài trợ NSF 1845216 và 1920079. Bất kỳ ý kiến, phát hiện, kết luận, hoặc khuyến nghị nào được thể hiện trong bài báo này là của các tác giả và không nhất thiết phản ánh quan điểm của Không quân Hoa Kỳ, DARPA, hoặc NSF.

--- TRANG 10 ---
Tài liệu tham khảo
[1] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 7
[2] Hessam Bagherinezhad, Mohammad Rastegari, và Ali Farhadi. Lcnn: Lookup-based convolutional neural network. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 7120–7129, 2017. 3
[3] Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, và Nick Johnston. Variational image compression with a scale hyperprior. Trong International Conference on Learning Representations, 2018. 3
[4] Ondrej Bohdal, Yongxin Yang, và Timothy Hospedales. Flexible dataset distillation: Learn labels instead of images. arXiv preprint arXiv:2006.08572, 2020. 3, 6
[5] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, và Jun-Yan Zhu. Dataset distillation by matching training trajectories. arXiv preprint arXiv:2203.11932, 2022. 3, 5, 6
[6] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, và Yixin Chen. Compressing neural networks with the hashing trick. Trong International conference on machine learning, trang 2285–2294. PMLR, 2015. 1, 2, 6
[7] Xiaohan Chen, Jason Zhang, và Zhangyang Wang. Peek-a-boo: What (more) is disguised in a randomly weighted neural network, and how to find it efficiently. Trong International Conference on Learning Representations, 2021. 2
[8] Yaim Cooper. Global minima of overparameterized neural networks. SIAM Journal on Mathematics of Data Science, 3(2):676–691, 2021. 2
[9] JCMSA Djelouah và Christopher Schroers. Content adaptive optimization for neural image compression. Trong Proceedings of the CVPR, 2019. 3
[10] Emilien Dupont, Adam Goliński, Milad Alizadeh, Yee Whye Teh, và Arnaud Doucet. Coin: Compression with implicit neural representations. arXiv preprint arXiv:2103.03123, 2021. 3, 7
[11] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, và Erich Elsen. Rigging the lottery: Making all tickets winners. Trong International Conference on Machine Learning, trang 2943–2952. PMLR, 2020. 5
[12] Claudio Gallicchio và Simone Scardapane. Deep randomized neural networks. Recent Trends in Learning From Data, trang 43–68, 2020. 2
[13] Tiansheng Guo, Jing Wang, Ze Cui, Yihui Feng, Yunying Ge, và Bo Bai. Variable rate image compression with content adaptive optimization. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, trang 122–123, 2020. 3
[14] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, và Chang Xu. Ghostnet: More features from cheap operations. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 1580–1589, 2020. 3
[15] Song Han, Huizi Mao, và William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. 3
[16] Marton Havasi, Robert Peharz, và José Miguel Hernández-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. arXiv preprint arXiv:1810.00440, 2018. 3
[17] Soufiane Hayou, Jean-Francois Ton, Arnaud Doucet, và Yee Whye Teh. Robust pruning at initialization. arXiv preprint arXiv:2002.08797, 2020. 3
[18] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, và các cộng sự. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. 1
[19] Berivan Isik, Tsachy Weissman, và Albert No. An information-theoretic justification for model pruning. Trong International Conference on Artificial Intelligence and Statistics, trang 3821–3846. PMLR, 2022. 3, 5
[20] Woojeong Kim, Suhyun Kim, Mincheol Park, và Geunseok Jeon. Neuron merging: Compensating for pruned neurons. Advances in Neural Information Processing Systems, 33:585–595, 2020. 3
[21] Diederik P Kingma và Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 8
[22] Kodak. Kodak Dataset. http://r0k.us/graphics/kodak/, 1991. 7
[23] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, và Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. Trong Proceedings of the International Conference on Machine Learning, tháng 7 2020. 3, 5
[24] Yann LeCun và các cộng sự. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet, 20(5):14, 2015. 5, 7
[25] Jooyoung Lee, Seunghyun Cho, và Seung-Kwon Beack. Context-adaptive entropy model for end-to-end optimized image compression. arXiv preprint arXiv:1809.10452, 2018. 3
[26] Junghyup Lee, Dohyung Kim, và Bumsub Ham. Network quantization with element-wise gradient scaling. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 6448–6457, 2021. 1, 3
[27] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, và Jinwoo Shin. Layer-adaptive sparsity for the magnitude-based pruning. arXiv preprint arXiv:2010.07611, 2020. 5
[28] Yuchao Li, Shaohui Lin, Jianzhuang Liu, Qixiang Ye, Mengdi Wang, Fei Chao, Fan Yang, Jincheng Ma, Qi Tian, và Rongrong Ji. Towards compact cnns via collaborative compression. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 6438–6447, 2021. 3
[29] Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, và Martin Jaggi. Dynamic model pruning with feedback. arXiv preprint arXiv:2006.07253, 2020. 1, 3, 5
[30] Chaoyue Liu, Libin Zhu, và Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85–116, 2022. 2

--- TRANG 11 ---
[31] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, và Ohad Shamir. Proving the lottery ticket hypothesis: Pruning is all you need. Trong International Conference on Machine Learning, trang 6682–6691. PMLR, 2020. 2
[32] David Minnen, Johannes Ballé, và George D Toderici. Joint autoregressive and hierarchical priors for learned image compression. Advances in neural information processing systems, 31, 2018. 3
[33] Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, và Nathan Srebro. The role of over-parametrization in generalization of neural networks. Trong International Conference on Learning Representations, 2019. 2
[34] Quynh Nguyen, Mahesh Chandra Mukkamala, và Matthias Hein. On the loss landscape of a class of deep neural networks with no bad local valleys. Trong International Conference on Learning Representations, 2019. 2
[35] Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, và Mohammad Rastegari. What's hidden in a randomly weighted neural network? Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 11893–11902, 2020. 2
[36] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, và Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. Trong European conference on computer vision, trang 525–542. Springer, 2016. 3
[37] Brandon Reagan, Udit Gupta, Bob Adolf, Michael Mitzenmacher, Alexander Rush, Gu-Yeon Wei, và David Brooks. Weightless: Lossy weight encoding for deep neural network compression. Trong International Conference on Machine Learning, trang 4324–4333. PMLR, 2018. 3
[38] Julien Niklas Siems, Aaron Klein, Cedric Archambeau, và Maren Mahsereci. Dynamic pruning of a neural network via gradient signal-to-noise ratio. Trong 8th ICML Workshop on Automated Machine Learning (AutoML), 2021. 3
[39] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, và Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems, 33:7462–7473, 2020. 7
[40] Christopher Subia-Waud và Srinandan Dasmahapatra. Weight fixing networks. Trong European Conference on Computer Vision, trang 415–431. Springer, 2022. 1, 2
[41] Ilia Sucholutsky và Matthias Schonlau. Soft-label dataset distillation and text dataset distillation. Trong 2021 International Joint Conference on Neural Networks (IJCNN), trang 1–8. IEEE, 2021. 3, 6
[42] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, và Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in Neural Information Processing Systems, 33:7537–7547, 2020. 5, 7
[43] Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, và Deepak K Gupta. Chipnet: Budget-aware pruning with heaviside continuous approximations. arXiv preprint arXiv:2102.07156, 2021. 3
[44] George Toderici, Wenzhe Shi, Radu Timofte, Lucas Theis, Johannes Balle, Eirikur Agustsson, Nick Johnston, và Fabian Mentzer. Workshop and challenge on learned image compression (clic2020), 2020. 7
[45] Karen Ullrich, Edward Meeds, và Max Welling. Soft weight-sharing for neural network compression. arXiv preprint arXiv:1702.04008, 2017. 3
[46] Huan Wang, Can Qin, Yulun Zhang, và Yun Fu. Neural pruning via growing regularization. arXiv preprint arXiv:2012.09243, 2020. 3
[47] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, và Yang You. Cafe: Learning to condense dataset by aligning features. arXiv preprint arXiv:2203.01531, 2022. 6
[48] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, và Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018. 1, 3, 5, 6
[49] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, và Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 2097–2106, 2017. 7
[50] Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, và Ali Farhadi. Supermasks in superposition. Advances in Neural Information Processing Systems, 33:15173–15184, 2020. 2
[51] Bo Zhao và Hakan Bilen. Dataset condensation with differentiable siamese augmentation. Trong International Conference on Machine Learning, trang 12674–12685. PMLR, 2021. 3, 6
[52] Bo Zhao và Hakan Bilen. Dataset condensation with distribution matching. arXiv preprint arXiv:2110.04181, 2021. 3, 6
[53] Bo Zhao, Konda Reddy Mopuri, và Hakan Bilen. Dataset condensation with gradient matching. arXiv preprint arXiv:2006.05929, 2020. 6

Phụ lục
Tính trực giao và chuẩn của mạng cơ sở:
Trong bài nộp chính, chúng tôi đã đề cập rằng các mạng cơ sở ngẫu nhiên gần như vuông góc với nhau trong không gian nhiều chiều. Để thể hiện điều này, chúng tôi tạo ra 1000 vector ngẫu nhiên trong không gian d chiều (thay đổi d từ 10 đến 1000), và vẽ biểu đồ của chuẩn ℓ2 và độ tương tự cosine từng cặp của chúng trong Hình 8 và 9 tương ứng. Chúng tôi cũng chạy thí nghiệm này 100 lần, tính độ tương tự cosine tối đa cho mỗi lần chạy, và vẽ biểu đồ của các giá trị tối đa trong Hình 10. Như mong đợi, ở số chiều cao hơn, độ tương tự cosine tiến gần đến 0 và chuẩn tiến gần đến 1. Điều này gợi ý thực nghiệm rằng ở chiều cao hơn, cơ sở ngẫu nhiên gần với cơ sở trực chuẩn. Xin lưu ý rằng phương pháp của chúng tôi không yêu cầu cơ sở trực chuẩn để hoạt động.

--- TRANG 12 ---
Hình 8. Biểu đồ chuẩn ℓ2 của 1000 vector d chiều được tạo ngẫu nhiên. Khi tăng d, chuẩn tiến đến 1.

Hình 9. Biểu đồ độ tương tự cosine từng cặp của 1000 vector d chiều được tạo ngẫu nhiên. Khi tăng d, độ tương tự cosine tiến đến 0.

Tái tạo sử dụng một tập con cơ sở:
Hình 12 cho thấy phân phối các giá trị alpha cho một vài hình ảnh từ tập dữ liệu Kodak. Như mong đợi, các giá trị alpha thay đổi qua các mô hình cơ sở. Điều này thúc đẩy chúng tôi nghiên cứu "điều gì sẽ xảy ra nếu chúng ta chỉ sử dụng một tập con của các mô hình cơ sở thay vì tất cả chúng?".

Đối với phân loại hình ảnh, chúng tôi thực hiện thí nghiệm này bằng cách chọn ngẫu nhiên p% cơ sở với p thay đổi. Chúng tôi lặp lại điều này 4 lần. Như một phương pháp lựa chọn khác, chúng tôi sắp xếp các giá trị alpha dựa trên giá trị tuyệt đối của chúng và sử dụng p% hàng đầu của chúng. Như được thể hiện trong Hình 11, trong lựa chọn ngẫu nhiên, chúng ta cần hầu hết các cơ sở để có thể lấy lại độ chính xác hợp lý trong khi đối với trường hợp được sắp xếp, một tỷ lệ nhỏ của các mô hình cơ sở là đủ để có độ chính xác hợp lý. Đây là một quan sát thú vị phần nào gợi ý rằng cảnh quan mất mát của việc tối ưu hóa cho các giá trị alpha khá mượt mà. Đây là một tuyên bố có chủ ý mơ hồ vì nó cần điều tra thêm như công việc tương lai.

Đối với nén hình ảnh, chúng tôi thực hiện cùng một thí nghiệm

--- TRANG 13 ---
Hình 10. Biểu đồ độ tương tự cosine tối đa từng cặp của 1000 vector d chiều được tạo ngẫu nhiên qua 100 thử nghiệm. Khi tăng d, độ tương tự cosine tối đa tiến đến 0.

ment và cho thấy các hình ảnh được tái tạo hợp lý với một tập con các giá trị alpha có giá trị tuyệt đối lớn nhất. Vì chúng tôi có một tập các alpha khác nhau cho mỗi lớp của MLP trong nén hình ảnh, chúng tôi sắp xếp các giá trị tuyệt đối và chọn p% hàng đầu của mỗi lớp có giá trị cao hơn. Chúng tôi thay đổi p và trực quan hóa các hình ảnh được tái tạo cho mỗi giá trị p trong Hình 14, 15 và 16. Ngoài ra, chúng tôi báo cáo ảnh hưởng của p trong PSNR và MS-SSIM cho tập dữ liệu Kodak trong Bảng 9. Tương tự như quan sát của chúng tôi trong phân loại hình ảnh, chúng tôi quan sát rằng hình ảnh với p = 70% có chất lượng chấp nhận được.

Ngoài việc hiểu rõ hơn về phương pháp, quan sát này có thể cho phép truyền thông một mô hình học sâu hoặc một hình ảnh một cách tiến bộ trong đó người gửi gửi một tập con các giá trị alpha (quan trọng nhất) trước và sau đó dần dần gửi phần còn lại để cải thiện độ chính xác mô hình hoặc chất lượng hình ảnh. Đối với nén hình ảnh, phương pháp này phần nào tương tự như ứng dụng của Progressive JPEG nơi nó được làm thủ công để gửi các thành phần tần số thấp trước. Xin lưu ý rằng để sử dụng điều này trong thực tế, phiên bản tiến bộ này của phương pháp chúng tôi có một số chi phí phụ thêm vì chúng tôi cũng cần truyền thông giá trị alpha nào được gửi ở mỗi bước (ví dụ, sử dụng một bit cho mỗi cơ sở). Nghiên cứu điều này chi tiết hơn được để lại cho công việc tương lai.

Chi tiết Nén Hình ảnh:
Như được mô tả trong bài nộp chính, trong các thí nghiệm nén hình ảnh, để thay đổi bit-per-pixel (bpp), chúng tôi cố định kiến trúc mạng và thay đổi số lượng giá trị α trên mỗi lớp. Chúng tôi báo cáo số lượng giá trị α trên mỗi lớp cho mỗi bpp trong Bảng 10.

Nén với các phương pháp Nén Hình ảnh Tiến tiến:
Chúng tôi so sánh phương pháp của chúng tôi với các codec tiên tiến hơn (ví dụ, BPG, VTM) và các phương pháp nén hình ảnh dựa trên học. Kết quả cho CLIC-Mobile trong Bảng 11 và kết quả cho Kodak trong Hình 13. Lưu ý rằng các codec tiên tiến được làm thủ công nặng nề bởi một cộng đồng lớn. Và, các phương pháp dựa trên học sử dụng một tập dữ liệu huấn luyện để học một mã tốt (tương tự như auto-encoder), do đó, chúng có thể không thể nén một hình ảnh duy nhất mà không có quyền truy cập vào một tập hợp các hình ảnh. Ngược lại, phương pháp của chúng tôi có thể nén một hình ảnh duy nhất mà không sử dụng một tập hợp các hình ảnh. Hơn nữa, vì lý do tương tự, phương pháp của chúng tôi không thể thiên lệch đối với các trường hợp phổ biến (đầu của phân phối). Rõ ràng, phương pháp của chúng tôi có các thiên lệch khác (ví dụ, những gì có thể được biểu diễn bằng INR) cần được nghiên cứu như công việc tương lai.

So sánh Trực quan với JPEG:
Tương tự như Hình 5 của bài nộp chính, chúng tôi bao gồm thêm so sánh trực quan với JPEG trong Hình 17 đến 20 (cho tập dữ liệu Kodak với độ phân giải 512×768) và Hình 21 và 22 (cho tập dữ liệu CLIC-Mobile với độ phân giải 1512×2016 hoặc 2016×1512). Hơn nữa, trong Hình 23 đến 25, chúng tôi bao gồm so sánh trực quan trong tập dữ liệu x-ray ngực với độ phân giải 1024×1024.

--- TRANG 14 ---
Hình 11. Ảnh hưởng của việc chỉ sử dụng p% mô hình cơ sở được chọn ngẫu nhiên (4 lần) hoặc được chọn dựa trên giá trị tuyệt đối cao nhất của alpha.

Hình 12. Phân phối alpha: Chúng tôi vẽ phân phối các giá trị alpha cho một vài hình ảnh Kodak.

Bảng 9. Ảnh hưởng của việc giữ p% α với giá trị tuyệt đối cao nhất:
percentile (p%) 10 20 30 40 50 60 70 80 90 100
bpp 0.07 0.14 0.22 0.29 0.36 0.43 0.50 0.58 0.65 0.72
PSNR 11.94 12.3 13.4 14.98 17.01 19.51 22.71 26.92 31.85 33.64
MS-SSIM 0.10 0.12 0.18 0.28 0.41 0.55 0.71 0.86 0.96 0.97

Bảng 10. Chi tiết của các mô hình nén hình ảnh: Chúng tôi báo cáo số lượng giá trị α trên mỗi lớp cho mỗi bpp. Chúng tôi sử dụng MLP với chiều ẩn 256 cho tập dữ liệu Kodak ở bpp 0.18 (hàng đầu tiên), và chiều ẩn 512 cho tất cả các thí nghiệm khác. Tất cả các thiết lập sử dụng ánh xạ Fourier kích thước 512. Chúng tôi sử dụng ít giá trị alpha hơn cho lớp cuối cùng vì lớp cuối cùng có ít trọng số hơn khi nó đi từ lớp ẩn chỉ đến 3 chiều (giá trị RGB). Ngoài ra, đối với hàng đầu tiên, chúng tôi sử dụng nhiều giá trị alpha hơn cho lớp đầu tiên vì nó có nhiều trọng số hơn (512×256).

Tập dữ liệu bpp Lớp-1 Lớp-2 Lớp-3 Lớp-4 Lớp-5 Tổng (αs)
Kodak0.18 10k 7.5k 7.5k 7.5k 2k 34.5k
0.31 15k 15k 15k 15k 2k 57k
0.52 10k 30k 30k 30k 2k 102k
0.72 20k 40k 40k 40k 2k 142k
CLIC-Mobile 0.12 45k 45k 45k 45k 2k 182k
Chest x-ray 0.15 20k 20k 20k 20k 2k 82k

--- TRANG 15 ---
Bảng 11. Nén Hình ảnh Tập dữ liệu CLIC-mobile: Tương tự như Bảng 6 của bài nộp chính, chúng tôi bao gồm so sánh với các codec tiên tiến như BPG và VTM. Chúng tôi cũng so sánh với một số phương pháp nén hình ảnh dựa trên học (ví dụ, MBT, CST, BMS).

Mô hình bpp PSNR MS-SSIM
VTM 0.183 33.07 0.964
CST 0.146 31.85 0.957
MBT 0.146 31.62 0.955
BPG 0.128 30.65 0.942
WebP 0.185 30.07 0.940
BMS 0.113 29.38 0.936
JPEG2000 0.126 29.40 0.918
JPEG 0.195 24.82 0.836
INR đã huấn luyện 0.125 26.93 0.864
PRANC (ours) 0.119 29.71 0.920

Hình 13. Nén Hình ảnh Tập dữ liệu Kodak: Tương tự như Hình 4 của bài nộp chính, chúng tôi bao gồm so sánh với các codec tiên tiến như BPG và VTM. Chúng tôi cũng so sánh với nén hình ảnh dựa trên học (ví dụ, MBT, CST, BMS).

--- TRANG 16 ---
Hình 14. Ảnh hưởng của việc giữ p% mô hình cơ sở với các giá trị alpha tuyệt đối cao nhất. Chúng ta có được hình ảnh hợp lý với tập con nhỏ hơn của các mô hình cơ sở.

Hình 15. Xem Hình 14.

--- TRANG 17 ---
Hình 16. Xem Hình 14.

--- TRANG 18 ---
Hình ảnh gốc
Ours (bpp=0.31, PSNR=30.62)
Ours (bpp=0.17, PSNR=28.18)JPEG (bpp=0.31, PSNR=28.68)
JPEG (bpp=0.17, PSNR=22.47)Hình 17. Trực quan hóa Kodak. Chúng tôi so sánh PRANC và JPEG trên hình ảnh 4 của tập dữ liệu Kodak ở bpp=0.31 và 0.17

--- TRANG 19 ---
Hình ảnh gốc
Ours (bpp=0.31, PSNR=31.96)
Ours (bpp=0.17, PSNR=29.23)JPEG (bpp=0.32, PSNR=28.58)
JPEG (bpp=0.17, PSNR=23.21)Hình 18. Trực quan hóa Kodak. Chúng tôi so sánh PRANC và JPEG trên hình ảnh 17 của tập dữ liệu Kodak.

--- TRANG 20 ---
Hình ảnh gốc
Ours (bpp=0.31, PSNR=25.57)
Ours (bpp=0.17, PSNR=23.3)JPEG (bpp=0.31, PSNR=21.34)
JPEG (bpp=0.21, PSNR=19.42)Hình 19. Trực quan hóa Kodak. Chúng tôi so sánh PRANC và JPEG trên hình ảnh 5 của tập dữ liệu Kodak.

--- TRANG 21 ---
Hình ảnh gốc
Ours (bpp=0.31, PSNR=31.41)
Ours (bpp=0.17, PSNR=27.88)JPEG (bpp=0.32, PSNR=31.16)
JPEG (bpp=0.17, PSNR=23.64)Hình 20. Trực quan hóa Kodak. Chúng tôi so sánh PRANC và JPEG trên hình ảnh 23 của tập dữ liệu Kodak.

--- TRANG 22 ---
Hình ảnh gốc
Ours (bpp=0.119, PSNR=30.26)JPEG (bpp=0.18, PSNR=25.43)Hình 21. Trực quan hóa CLIC. Chúng tôi so sánh PRANC ở bpp=0.119 với JPEG ở bpp=0.18 trên một hình ảnh CLIC.

--- TRANG 23 ---
Hình ảnh gốc
Ours (bpp=0.119, PSNR=28.99)
JPEG (bpp=0.226, PSNR=24.59)Hình 22. Trực quan hóa CLIC. Chúng tôi so sánh PRANC ở bpp=0.119 với JPEG ở bpp=0.226 trên một hình ảnh CLIC.

--- TRANG 24 ---
Hình ảnh gốc
Ours (bpp=0.15, PSNR=36.06)JPEG (bpp=0.15, PSNR=32.11)
Hình 23. Trực quan hóa X-ray Ngực. Chúng tôi so sánh PRANC và JPEG trên một hình ảnh X-ray Ngực ở bpp=0.15

--- TRANG 25 ---
Hình ảnh gốc
Ours (bpp=0.15, PSNR=36.24)JPEG (bpp=0.15, PSNR=32.55)Hình 24. Xem Hình 23.

--- TRANG 26 ---
Hình ảnh gốc
Ours (bpp=0.15, PSNR=35.54)JPEG (bpp=0.15, PSNR=32.20)Hình 25. Xem Hình 23.
