# 2310.19925.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/random/2310.19925.pdf
# File size: 151090 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2310.19925v1  [cs.DC]  30 Oct 2023OpenRAND: A Performance Portable, Reproducible Random Num ber Generation Library
for Parallel Computations
Shihab Shahriar Khana,∗, Bryce Palmerb,c, Christopher Edelmaierd, Hasan Metin Aktulgaa
aDept. of Computer Science, Michigan State University, East Lansing, MI 48824
bDept. of Mechanical Engineering, Michigan State Universit y, East Lansing, MI 48824
cDept. of Computational Mathematics, Science and Engineeri ng, Michigan State University, East Lansing, MI 48824
dCenter for Computational Biology, Flatiron Institute, New York, NY 10010
Abstract
We introduce OpenRAND, a C ++17 library aimed at facilitating reproducible scientiﬁc re search through the generation of statis-
tically robust and yet replicable random numbers. OpenRAND accommodates single and multi-threaded applications on CP Us
and GPUs and oﬀers a simpliﬁed, user-friendly API that complies with the C ++standard’s random number engine interface. It
is portable: it functions seamlessly as a lightweight, head er-only library, making it adaptable to a wide spectrum of so ftware and
hardware platforms. It is statistically robust: a suite of b uilt-in tests ensures no pattern exists within single or mul tiple streams.
Despite the simplicity and portability, it is remarkably pe rformant—matching and sometimes even outperforming nativ e libraries
by a signiﬁcant margin. Our tests, including a Brownian walk simulation, aﬃrm its reproducibility and highlight its computational
eﬃciency, outperforming CUDA’s cuRAND by up to 1.8 times.
Keywords: Pseudo Random Number Generation, GPGPU, HPC, C ++
Metadata
1. Background
Generating random numbers in a reproducible manner is piv-
otal for ensuring the reliability and validity of scientiﬁc re-
search outcomes, especially in domains fundamentally reli ant
on random number generation, such as stochastic simulation s,
machine learning, and computer graphics. This reproducibi lity
permits the exact replication of simulations, facilitatin g mean-
ingful comparisons devoid of unnecessary variance and ensu r-
ing that any disparities arising are solely attributable to external
factors, such as discrepancies in ﬂoating-point arithmeti c or-
dering. More explicitly, it allows simulations utilizing i dentical
random seeds to be compared directly, eliminating reliance on
statistical averages and signiﬁcantly simplifying the pro cesses
of debugging and regression testing, as has been demonstrat ed
in past works [1, 2].
In single-threaded environments, the reproducibility of P seudo
Random Number Generators (RNGs) is straightforward since
the same initial state (i.e., seed) results in an identical sequence
of random numbers (i.e., stream ). Reproducible random num-
ber generation for multi-threaded and multi-processed app lica-
tions, on the other hand, is nontrivial due to the unpredicta bil-
ity of execution orders and the potential for race condition s.
Strategies such as utilizing a single RNG instance with syn-
chronization or distributing pre-allocated random number s to
∗Corresponding author
Email address: khanmd@msu.edu (Shihab Shahriar Khan)various threads are fraught with scalability issues [3, 1]. In-
stead, contemporary research has predominantly focused on the
creation of multiple independent streams using distinct RN G in-
stances per thread. One approach is to split a single stream i nto
multiple equally sized streams. For example, one could spli t a
single stream with period 264to 232independent streams, each
with period 232. This method, however, requires that the gen-
erator have a long period and e ﬃcient jump-ahead capability.
Moreover, statistical independence between these streams is a
concern [4, 5]. Another commonly employed technique is to
pre-generate a set of (pseudo)random seeds for each stream a nd
employ them as starting points for generating multiple stre ams.
Although the probability of a direct seed collision, result ing in
identical streams, can be minimized for RNGs with su ﬃciently
large periods, there remains the potential for statistical corre-
lation among streams, particularly when the bit representa tions
of two seeds are closely related [6]. For an extensive treatm ent
of this subject, please refer to [5].
The integration of Graphical Processing Units (GPUs) into
computational workﬂows, vital for High-Performance Compu t-
ing (HPC) workloads, introduces a distinct set of challenge s as
we shift from CPU to GPU, from dozens or, at most, hundreds
of threads to potentially millions within a single node. Mem -
ory considerations serves as a prime illustration. In the GP U
environment, threads access only a limited amount of high-
speed private memory; hence, optimizing local memory usage
is essential for maintaining performance. For instance, th e de-
fault random engine in GNU’s libstdc ++, Mersenne Twister
[7], requires approximately 624 32-bit words for internal s tate,
exceeding by more than double the maximum number of 32-
Preprint submitted to SoftwareX November 1, 2023

--- PAGE 2 ---
Nr. Code metadata description Please ﬁll in this column
C1 Current code version V0.9
C2 Permanent link to code /repository used for this
code versionhttps://github.com/msu-sparta/
OpenRAND
C3 Permanent link to Reproducible Capsule https://codeocean.com/capsule/
0144704/tree
C4 Legal Code License MIT License.
C5 Code versioning system used Git
C6 Software code languages, tools, and services
usedC++17
C7 Compilation requirements, operating environ-
ments & dependenciesA compiler with C ++17 support, optionally
CMake
C8 If available Link to developer documenta-
tion/manualhttps://msu-sparta.github.io /OpenRAND
C9 Support email for questions khanmd@msu.edu
Table 1: Code metadata
bit registers permitted per thread in CUDA. Further, the ab-
sence of certain instructions in GPUs, common in most CPUs,
leads to distinct CPU vs. GPU performance characteristics f or
many generators [8], prompting the need for specialized par al-
lelization strategies. For example, the GPU-adapted Merse nne
Twister, MTGP [9], requires a block of threads to share a sing le
state.
2. Motivation and signiﬁcance
In HPC, the eﬃciency and reproducibility of pseudo-random
number generation are paramount. Libraries like cuRAND
and rocRAND excel at generating reproducible pseudo-rando m
sequences in parallel environments but face challenges in
portability, limiting their adaptability to new architect ures.
Conversely, while RandomCL [10] and clRNG [11] (though
no longer actively developed) successfully address hardwa re
portability, they are limited to the OpenCL software frame-
work. Two challenges arise across these platforms: seeding
intricacies and the demands of state management. For seedin g,
neither RandomCL nor clRNG allow arbitrary seeds; instead,
they generate seeds for a predetermined number of threads us -
ing a host-based sequential generator, subsequently trans ferring
them to the device’s global memory. cuRAND and rocRAND
allow arbitrary seeds to produce distinct random streams bu t
necessitate complicated state initialization procedures . More-
over, these platforms obligate developers to consistently man-
age memory states throughout the lifecycle of a processing
element or thread, adding computational overhead. Overall ,
while these libraries o ﬀer potent tools, they introduce multi-
faceted challenges, emphasizing the need for comprehensiv e
and streamlined solutions.
Counter-based generators (CBRNGs) o ﬀer a promising al-
ternative to the challenges presented by traditional pseud o-
random generators. Historically, these generators adapte d cryp-
tographic algorithms for HPC applications, albeit at reduc ed
cryptographic strength [12], [13], [8]. A pivotal advancem ent
in this domain was the integration of the counter concept int othese generators [12, 8], mirroring the counter mode observ ed
in stream ciphers. This innovation allowed a unique speciﬁ-
cation of a stream within a kernel for a given seed. Conse-
quently, the earlier design pattern of one stream per proces sing
element evolved to one stream per processing element per ker -
nel, the utility of which has been demonstrated by HOOMD-
blue [14]. CBRNGs also champion the avalanche property,
wherein a minimal bit alteration in the seed or counter cas-
cades into a signiﬁcantly altered, statistically independ ent new
stream. This property gives developers the ﬂexibility to us e any
unique values for the seed or counter, often harnessing inte r-
nal application variables for parallel streams. Complemen ting
these beneﬁts, these generators feature notably compact st ates;
for instance, OpenRAND boasts a 96-bit state, comfortably ﬁ t-
ting within the maximum number of 32-bit registers permitte d
per thread in CUDA.
Building on this CBRNG foundation, Random123 is a pivotal
library in the realm of counter-based random number genera-
tion [8]. It made signiﬁcant strides by introducing three in nova-
tive generators and was at the forefront in terms of performi ng
comprehensive statistical testing. Nevertheless, despit e its con-
tributions, Random123 exhibits notable drawbacks. The lib rary
does not incorporate some of the modern CBRNGs that have
emerged in recent research [15, 16]. Its API skews heavily to -
ward lower-level implementations, exposing developers to the
algorithmic details underpinning its generators. Further more,
its reliance on intrinsics for performance enhancements co m-
promises portability and inﬂates the codebase. As the deman ds
of this domain escalate, the need for a holistic, reﬁned solu tion
becomes evident.
Addressing this need, we have developed the OpenRAND li-
brary, which amalgamates diverse counter-based generator s
with complementary strengths under a uniﬁed, user-centric
API. Previously, developers su ﬀered in two ways: either
through increased development overhead from redundant API
and easy-to-make bugs or through the execution of unneces-
sary instructions, leading to performance degradation. Ou r pro-
posed solution, OpenRAND bridges this gap by focusing exclu -
2

--- PAGE 3 ---
sively on a single family of counter-based generators. This con-
centrated approach facilitates the development of reprodu cible
parallel code, allowing developers to circumvent the compl i-
cations of API boilerplate and avoid unnecessary performan ce
penalties. The library is characterized by its portability ; it is a
header-only library rigorously tested across various CPU a nd
GPU devices and is compatible with multiple software plat-
forms. Integration is streamlined, allowing incorporatio n into
new or pre-existing projects with ease through CMake or by
simply copying and pasting the requisite header ﬁles. It als o
touts a lightweight footprint, with its core header ﬁles com -
prising just 470 source lines of code at the time of publicati on.
OpenRAND has undergone extensive statistical testing to en -
sure its reliability and robustness. As we will show, despit e
our emphasis on simplicity and user-friendly interface, Op en-
RAND is as fast as, and at times superior to, native CPU and
GPU libraries.
3. Software description
3.1. API Design
OpenRAND is structured around a core set of counter-based
random number generators complemented by a suite of exam-
ples, benchmarks, and tests. Complying with the C ++17’s ran-
dom engine interface, generators in OpenRAND are compatibl e
with standard library functions, including generating ran dom
samples across diﬀerent distributions. Each generator is created
via a constructor that requires two arguments: a 64-bit seed and
a 32-bit counter1. When combined, the seed and counter pro-
duce a unique stream with a period of 232. The seed is meant
to uniquely identify each logical thread or processing elem ent
in the program[5], whereas the counter can then be used to cre -
ate multiple streams per seed as needed. In practical scenarios,
a processing element could be a particle in particle dynamic s
simulation or a pixel index in a ray tracing application.
Currently, OpenRAND supports a variety of counter-based ge n-
erators, including Philox [8], Threefry [8], Squares [16], and
Tyche [15]. They o ﬀer high-quality streams in compact sizes,
eﬃcient construction/destruction of RNG objects, and accept
arbitrary seeds. The combination of these properties means ,
quite remarkably, that many stochastic applications can si destep
the need for anyrandom state maintenance. In practical terms
and in contrast to Nvidia’s cuRAND library, developers can o f-
ten forgo the hassles and performance degradation related t o
storing states in global memory, launching a separate kerne l on
GPUs to initialize the states, or, in the case of array of stru c-
tures data structures, the overhead of loading and saving st ates
inside each kernel for every thread. An illustrative exampl e of
these advantages is shown in Section 4.
4. Illustrative Examples
To demonstrate OpenRAND’s advantages, we employ
the Brownian Dynamics macro-benchmark from [1], re-
implementing in CUDA across three RNG libraries. Fig.
1Except for one generator, Squares, which currently accepts 32-bit seeds1 highlights OpenRAND, Fig. 2 cuRAND, and Fig. 3
Random123. The compactness of OpenRAND’s API is imme-
diately evident, with just two lines for generator initiali zation
and random number computation—over 14 fewer lines than
the competing libraries. Unlike cuRAND, both OpenRAND
and Random123 use CBRNGs, allowing unique particle IDs as
seeds, guaranteeing a unique stream per particle regardles s of
thread count. This design circumvents the state maintenanc e
associated with cuRAND, eliminating the need for memory
allocation, state initialization, and continuous memory o pera-
tions within each kernel thread. Nevertheless, random numb er
generation with Random123 requires excessive boilerplate for
initialization and random sampling, burdening developers with
extra coding demands, and amplifying the risk of inadverten tly
introducing bugs. In light of these assessments, OpenRAND
emerges as a potent blend of simplicity, e ﬃciency, and adapt-
ability, underscoring its viability as a prime choice for ra ndom
number generation within HPC applications.
1typedef openrand ::Philox RNG;
2
3__global__ voidapply_forces(Particle *particles,
4 intcounter){
5inti=blockIdx.x *blockDim.x +threadIdx.x;
6if(i>=N)
7return;
8
9Particle p =particles[i];
10// Apply drag force
11p.vx-=GAMMA/mass*p.vx*dt;
12p.vy-=GAMMA/mass*p.vy*dt;
13
14// Apply random motion
15RNGlocal_rand_state (p.pid, counter);
16rnd::double2 r =local_rand_state.draw_double2();
17p.vx+=(r.x* 2.0 - 1.0 )*sqrt_dt;
18p.vy+=(r.y* 2.0 - 1.0 )*sqrt_dt;
19particles[i] =p;
20}
21
22intmain(){
23// Initialize particles
24init_particles <<<nblocks, nthreads >>>(particles,
25 /*counter*/ 0);
26
27// Simulation loop
28intiter= 0;
29while(iter++ <STEPS) {
30apply_forces <<<nblocks, nthreads >>>(particles,
31 iter);
32...
33}
34}
Figure 1: Illustrative example of OpenRAND’s API
5. Empirical Results
5.1. Performance Benchmarks
To assess OpenRAND’s performance, we designed two bench-
mark tests focusing on micro and macro performance metrics
across CPU and GPU platforms, respectively.
3

--- PAGE 4 ---
1typedef curandStatePhilox4_32_10_t RNG;
2
3__global__ voidrand_init(RNG *rand_state) {
4inti=threadIdx.x +blockIdx.x *blockDim.x;
5if(i>=N)
6return;
7curand_init( 1984, i,0,&rand_state[i]);
8}
9
10
11template <typename RNG>
12__global__ voidapply_forces(Particle *particles,
13 RNG*rand_state){
14inti=blockIdx.x *blockDim.x +threadIdx.x;
15if(i>=N)
16return;
17
18Particle p =particles[i];
19// Apply drag force
20p.vx-=GAMMA/mass*p.vx*dt;
21p.vy-=GAMMA/mass*p.vy*dt;
22
23// Apply random motion
24RNG local_rand_state =rand_state[i];
25double2 r =curand_uniform2_double(
26 &local_rand_state);
27p.vx+=(r.x* 2.0 - 1.0 )*sqrt_dt;
28p.vy+=(r.y* 2.0 - 1.0 )*sqrt_dt;
29rand_state[i] =local_rand_state;
30particles[i] =p;
31}
32
33intmain(){
34// Random number generator setup
35RNG*d_rand_states;
36cudaMalloc(( void**)&d_rand_states, N *sizeof(RNG));
37
38// Initialize random number generators
39rand_init <<<nblocks, nthreads >>>(d_rand_states);
40
41// Initialize particles
42init_particles <<<nblocks, nthreads >>>(particles,
43 d_rand_states);
44
45// Simulation loop
46intiter= 0;
47while(iter++ <STEPS) {
48apply_forces <<<nblocks, nthreads >>>(particles,
49 d_rand_states);
50...
51}
52}
Figure 2: Illustrative example of cuRAND’s API
For our micro-benchmark, we implemented a single-threaded
program tailored for CPU performance and tested it on an In-
tel(R) Xeon(R) Platinum 8260. This benchmark gauges the
raw random number generation speed for streams of varying
sizes across all generators. We employed Google benchmark
for this evaluation, setting our performance standards aga inst
the widely recognized GNU libstdc ++’s default random engine,
mt19937 [7], given its ubiquity in many applications. As the
data in Fig. 4a reveals, OpenRAND’s generators consistentl y
outpace mt19937 for smaller streams, a realm often encoun-
tered in parallel programs. While this strong disparity in p er-
formance for small streams can be attributed to mt19937’s in tri-
cate initialization routine, the performance advantage of Open-1typedef r123::Philox4x32 RNG;
2
3__global__ voidapply_forces(Particle *particles,
4 intcounter){
5inti=blockIdx.x *blockDim.x +threadIdx.x;
6if(i>=N)
7return;
8
9Particle p =particles[i];
10// Apply drag force
11p.vx-=GAMMA/mass*p.vx*dt;
12p.vy-=GAMMA/mass*p.vy*dt;
13
14// Apply random motion
15RNG rng;
16RNG::ctr_type c ={{}};
17RNG::ukey_type uk ={{}};
18uk[0]=p.pid;
19RNG::key_type k =uk;
20
21c[0]=counter;
22c[1]= 0;
23RNG::ctr_type r =rng(c, k);
24
25uint64_t xu=(static_cast <uint64_t >(r[0])<< 32)
26 |static_cast <uint64_t >(r[1]);
27uint64_t yu=(static_cast <uint64_t >(r[2])<< 32)
28 |static_cast <uint64_t >(r[3]);
29autox=r123::u01<double,uint64_t >(xu);
30autoy=r123::u01<double,uint64_t >(yu);
31
32p.vx+=(x* 2.0 - 1.0 )*sqrt_dt;
33p.vy+=(y* 2.0 - 1.0 )*sqrt_dt;
34particles[i] =p;
35}
36
37intmain(){
38// Initialize particles
39init_particles <<<nblocks, nthreads >>>(particles,
40 /* counter*/ 0);
41
42// Simulation loop
43intiter= 0;
44while(iter++ <STEPS) {
45apply_forces <<<nblocks, nthreads >>>(particles,
46 iter);
47...
48}
49}
Figure 3: Illustrative example of Random123’s API
RAND over mt19937 is sustained even in longer streams for
both the Tyche [15] and Squares [16] generators.
Transitioning to GPU performance, we employ the previously
discussed macro-benchmark, a 2D Brownian dynamics simula-
tion in CUDA. This simulation involved one million indepen-
dent particles diﬀusing according to a Brownian random walk.
Particles were monitored over 10,000 steps, with the partic les
inﬂuenced by both a velocity-proportional drag force and a r an-
dom uniform motion. To maintain consistency, pseudo-rando m
number generation for all libraries used their respective P hilox
generators (For details, refer to code2). This benchmark was
executed on two Nvidia GPUs: a Tesla V100 PCIe with a the-
oretical 14.13 TFLOPS and 900GB /s bandwidth and an A100
2https://github.com/Shihab-Shahriar/brownian-dynamic s
4

--- PAGE 5 ---
SXM with 19.5 TFLOPS and 2039GB /s bandwidth.
10 100 1K 10K
Stream size102103104105Time (nanoseconds)
Lower is Betterstd::mt19937
r123::philox
T yche
Squares
Phillox
Threefry
(a) Time taken by OpenRAND generators versus baselines (std ::mt19937 and r123::philox)
to produce speciﬁed stream lengths on the host.
V100 A100
GPU0.00.51.01.52.02.53.03.54.0Time (seconds)
Lower is Better4.23
3.20
2.46
1.812.48
1.78Curand
Random123
OpenRAND
(b) Wall time for various libraries executing the Brownian D ynamics benchmark on di ﬀerent
GPUs, using the Philox generator [8] in each library.
Figure 4: Performance of OpenRAND on host and device respect ively.
As seen in Fig. 4b, OpenRAND outperformed cuRAND by
1.8x, while saving ~64 MB of GPU memory per million parti-
cles, and performed on par with Random123. Given the sim-
plistic nature of the kernels used in the program, where ran-
dom number generation dominates computational cost, such
a performance margin between OpenRAND and cuRAND
was unanticipated. Of course, we do not expect this mar-
gin of improvement to hold for real-world kernels where
the computational cost of random number generation is less
prominent. Nevertheless, this comparison between cuRAND,
a native library speciﬁcally optimized for these platforms ,
and Random123, a library that utilized intrinsic instructi ons
to achieve performance enhancement, o ﬀers conﬁdence that
OpenRAND’s platform-independent code and simpliﬁed API
do not compromise its performance.
5.2. Statistical Evaluation
To ensure the quality of our random number generation, Open-
RAND exclusively incorporates generators with rigorous em -
pirical validations and long-standing use. Even with this f oun-
dation, maintaining statistical integrity requires caref ul imple-
mentation, as subtle bugs can compromise randomness. As
such, we perform rigorous quality assurance: every generat or
within OpenRAND was subjected to statistical testing using thepopular frameworks TestU01 [17] and PractRand [18]. These
tools oﬀer a suite of complementary statistical tests designed
to identify any underlying patterns or irregularities in ra ndom
streams of data. An example of these tests is the Birthday Spa c-
ing test from TestU01 [17], which contrasts empirical resul ts
against known analytical solutions to detect potential dis crep-
ancies.
We initiated our testing process by evaluating individual d ata
streams, probing them to their theoretical limit of 232integers
using PractRand across a comprehensive range of keys and
counters. While TestU01 and PractRand are geared towards
single-stream assessments, we recognized the importance o f
extending these tests to encompass parallel streams, reﬂec t-
ing their use in real-world parallel computations. To perfo rm
our parallel stream tests, we followed the procedure outlin ed in
[14]—we simulated a scenario with 16,000 particles, genera t-
ing micro-streams comprising three random numbers for each
particle. These individual micro-streams for each particl e were
ﬁrst combined into a single concatenated stream. This uniﬁe d
stream was then lengthened over successive iterations to ex am-
ine correlations across the entire system.
All generators were successfully tested for at least 1TB of d ata
using PractRand and TestU01’s comprehensive BigCrush bat-
tery of tests. It is worth elaborating on the BigCrush result s.
During repeated trials with multiple global seeds, certain out-
puts (one or two out of 106 tests) occasionally emerged as sus -
picious. However, this is not unique to OpenRAND; the au-
thors of cuRAND3noted similar failures. For an exhaustive
breakdown of our statistical results, we direct readers to o ur
documentation4. To the best of our knowledge, this is the ﬁrst
time Tyche [15] and Squares [16] generators have undergone
correlation tests for parallel streams.
6. Impact
Random number generation plays a fundamental role in the ef-
ﬁciency and reliability of larger software systems across ﬁ elds
such as stochastic simulations, machine learning, and comp uter
graphics. Ideally, there would exist a good o ﬀ-the-shelf so-
lution that could be used in a variety of contexts, including
multi-threaded/multi-process applications, without introducing
excessive boilerplate code, unnecessary complexity, or re stric-
tions on applicable architecture. The existing software la nd-
scape is, however, fraught with challenges. Some good optio ns
expose low-level algorithmic and implementation details ( e.g.,
Random123, cuRAND), leading to increased complexity; oth-
ers are intrinsically bound to speciﬁc hardware (like cuRAN D)
or software platforms (such as rocRAND, OneAPI MKL). Sev-
eral once-popular, platform-agnostic alternatives are no w aban-
donware (clRNG, RandomCL), and even universal options, lik e
the C++Standard library, prove ill-suited for GPGPU pro-
grams. This landscape has led many esteemed open-source
3https://docs.nvidia.com/cuda/curand/testing.html
4https://msu-sparta.github.io/OpenRAND/md_statistica l_results.h 
5

--- PAGE 6 ---
platforms to either layer atop a low-level library, like HOO MD-
Blue’s use of Random123, or to write custom random gen-
erators—as seen in Tensorﬂow, Pytorch, VTK, Jax, Alpaka,
Kokkos, and others—sometimes without the beneﬁt of thor-
ough statistical validation.
OpenRAND aims to be that o ﬀ-the-shelf solution. Prioritiz-
ing speed, reproducibility, parallelism, and portability , it oﬀers
these features through an accessible and streamlined API. F or
those seeking ease of integration, it can be seamlessly adde d to
projects via CMake or by copying the required header ﬁles. In
our benchmarks, OpenRAND outperforms cuRAND by a fac-
tor of 1.8 and performs on par with Random123. Despite this
competitive performance, OpenRAND di ﬀerentiates itself by
oﬀering a cleaner, more intuitive API, a lightweight code base ,
and an absence of machine-speciﬁc code. Validated across CP U
and GPU platforms (with evaluations in standard C ++, CUDA,
HIP, and Kokkos), OpenRAND is uniquely positioned to el-
evate various projects by simplifying development, boosti ng
performance, and enhancing portability while ensuring sta tisti-
cal validity and reproducibility for single and parallel st reams.
This commitment to quality is evident in its design—with com -
prehensive statistical quality tests incorporated within its con-
tinuous integration pipeline—ensuring a consistent stand ard as
OpenRAND evolves within the open-source community.
7. Conclusion
To summarize, while the realm of random number genera-
tion presents numerous software options riddled with chal-
lenges—from exposing intricate algorithmic details to bei ng
tightly bound to speciﬁc architectures—OpenRAND emerges
as an oﬀ-the-shelf solution. Its focus on a single family of
counter-based generators simpliﬁes the development of rep ro-
ducible parallel code, avoiding the complications and redu n-
dancies faced with other libraries. By sidestepping the pit -
falls of boilerplate APIs, hardware limitations, and aband oned
alternatives, OpenRAND presents an e ﬃcient, streamlined,
and platform-agnostic solution that seamlessly integrate s into
projects, reducing unnecessary complexities, overheads, and
hardware restrictions. OpenRAND accelerates the develop-
ment process while maintaining statistical robustness and re-
producibility. Given its features and performance benchma rks,
OpenRAND has the potential to signiﬁcantly aid developers i n
various scientiﬁc ﬁelds, ensuring that random number gener a-
tion remains both reliable and e ﬃcient.
Declaration of Competing Interest
The authors declare that they have no known competing ﬁnan-
cial interests or personal relationships that could have ap peared
to inﬂuence the work reported in this paper.
Acknowledgements
This material is based upon work supported by the National Sc i-
ence Foundation O ﬃce of Advanced Cyberinfrastructure underGrant 2007181 and used resources provided by Michigan State
University’s High-Performance Computing Center.
Declaration of generative AI and AI-assisted technologies in
the writing process
During the preparation of this work the authors used ChatGPT
only in order to improve language and readability. The autho rs
reviewed and edited the ﬁnal content and take full responsib ility
for the content of the publication.
References
[1] C. L. Phillips, J. A. Anderson, S. C. Glotzer, Pseudo-ran dom number gen-
eration for brownian dynamics and dissipative particle dyn amics simula-
tions on gpu devices, Journal of Computational Physics 230 ( 19) (2011)
7191–7201.
[2] S. Dura-Bernal, B. A. Suter, P. Gleeson, M. Cantarelli, A . Quintana,
F. Rodriguez, D. J. Kedziora, G. L. Chadderdon, C. C. Kerr, S. A. Ney-
motin, et al., Netpyne, a tool for data-driven multiscale mo deling of brain
circuits, Elife 8 (2019) e44494.
[3] P. L’Ecuyer, O. Nadeau-Chamard, Y .-F. Chen, J. Lebar, Mu ltiple streams
with recurrence-based, counter-based, and splittable ran dom number gen-
erators, in: 2021 Winter Simulation Conference (WSC), 2021 , pp. 1–16.
doi:10.1109/WSC52266.2021.9715397 .
[4] A. De Matteis, S. Pagnutti, Parallelization of random nu mber generators
and long-range correlations, Numerische Mathematik 53 (19 88) 595–608.
[5] P. L’Ecuyer, D. Munger, B. Oreshkin, R. Simard, Random nu mbers for
parallel computers: Requirements and methods, with emphas is on gpus,
Mathematics and Computers in Simulation 135 (2017) 3–17.
[6] M. E. O’neill, Pcg: A family of simple fast space-e ﬃcient statistically
good algorithms for random number generation, ACM Transact ions on
Mathematical Software (2014).
[7] M. Matsumoto, T. Nishimura, Mersenne twister: a 623-dim ensionally
equidistributed uniform pseudo-random number generator, ACM Trans-
actions on Modeling and Computer Simulation (TOMACS) 8 (1) ( 1998)
3–30.
[8] J. K. Salmon, M. A. Moraes, R. O. Dror, D. E. Shaw, Parallel random
numbers: as easy as 1, 2, 3, in: Proceedings of 2011 internati onal confer-
ence for high performance computing, networking, storage a nd analysis,
2011, pp. 1–12.
[9] M. Saito, M. Matsumoto, Variants of mersenne twister sui table for
graphic processors, ACM Transactions on Mathematical Soft ware
(TOMS) 39 (2) (2013) 1–20.
[10] T. Ciglari ˇc, R. ˇCešnovar, E. Štrumbelj, An opencl library for parallel ran-
dom number generators, The Journal of Supercomputing 75 (20 19) 3866–
3881.
[11] P. L’Ecuyer, D. Munger, N. Kemerchou, clrng: A random nu mber api
with multiple streams for opencl, report, http: //www. iro. umontreal.
ca/lecuyer/myftp/papers/clrng-api. pdf (2015).
[12] D. J. Bernstein, et al., Chacha, a variant of salsa20, in : Workshop record
of SASC, V ol. 8, Citeseer, 2008, pp. 3–5.
[13] F. Zafar, M. Olano, A. Curtis, Gpu random numbers via the tiny encryp-
tion algorithm, in: Proceedings of the Conference on High Pe rformance
Graphics, 2010, pp. 133–141.
[14] J. A. Anderson, J. Glaser, S. C. Glotzer, Hoomd-blue: A p ython package
for high-performance molecular dynamics and hard particle monte carlo
simulations, Computational Materials Science 173 (2020) 1 09363.
[15] S. Neves, F. Araujo, Fast and small nonlinear pseudoran dom number
generators for computer simulation, in: Parallel Processi ng and Applied
Mathematics: 9th International Conference, PPAM 2011, Tor un, Poland,
September 11-14, 2011. Revised Selected Papers, Part I 9, Sp ringer, 2012,
pp. 92–101.
[16] B. Widynski, Squares: a fast counter-based rng, arXiv p reprint
arXiv:2004.06278 (2020).
[17] P. L’ecuyer, R. Simard, Testu01: Ac library for empiric al testing of ran-
dom number generators, ACM Transactions on Mathematical So ftware
(TOMS) 33 (4) (2007) 1–40.
[18] C. Doty-Humphrey, Practically random: C ++library of statistical tests
for rngs, URL: https: //sourceforge. net/projects/pracrand (2010).
6
