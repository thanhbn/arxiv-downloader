# 2405.16712.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2405.16712.pdf
# Kích thước tệp: 686938 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Zamba: Một Mô hình Lai SSM 7B Nhỏ gọn
Paolo Glorioso Quentin Anthony Yury Tokpanov James Whittington Jonathan Pilault
Adam Ibrahim Beren Millidge
{paolo, quentin, yury, james, jonathan, adam, beren }@zyphra.com
Zyphra
Palo Alto, CA
Tóm tắt - Trong báo cáo kỹ thuật này, chúng tôi trình bày Zamba, một mô hình lai SSM-transformer 7B mới lạ đạt được hiệu suất cạnh tranh so với các mô hình trọng số mở hàng đầu ở quy mô tương đương. Zamba được huấn luyện trên 1T token từ các bộ dữ liệu có sẵn công khai và là mô hình không phải transformer tốt nhất ở quy mô này. Zamba tiên phong một kiến trúc độc đáo kết hợp xương sống Mamba với một mô-đun attention chia sẻ duy nhất, do đó có được lợi ích của attention với chi phí tham số tối thiểu. Do kiến trúc của nó, Zamba nhanh hơn đáng kể trong suy luận so với các mô hình transformer tương đương và yêu cầu ít bộ nhớ hơn đáng kể để tạo ra các chuỗi dài. Zamba được huấn luyện trước trong hai giai đoạn: giai đoạn đầu tiên dựa trên các bộ dữ liệu web hiện có, trong khi giai đoạn thứ hai bao gồm việc ủ mô hình trên các bộ dữ liệu hướng dẫn và tổng hợp chất lượng cao, và được đặc trưng bởi sự suy giảm tốc độ học tập nhanh chóng. Chúng tôi mở mã nguồn các trọng số và tất cả các checkpoint cho Zamba, qua cả giai đoạn 1 và giai đoạn ủ.

I. GIỚI THIỆU
Kiến trúc transformer (Vaswani et al., 2017) đã cách mạng hóa xử lý ngôn ngữ tự nhiên và nhiều lĩnh vực khác của học sâu (Dosovitskiy et al., 2020; Yang et al., 2023; Touvron et al., 2023) nhờ khả năng mở rộng quy mô của nó trên kích thước mô hình, kích thước bộ dữ liệu và tính toán (Kaplan et al., 2020; Brown et al., 2020; Hoffmann et al., 2022; Rae et al., 2022). Tuy nhiên, không rõ liệu transformer có phải là kiến trúc duy nhất có thể tận dụng quy mô (Bachmann et al., 2023), và các kiến trúc mới có thể chứng minh có tiềm năng mở rộng quy mô thậm chí còn lớn hơn. Hơn nữa, chi phí bình phương của tính toán attention cốt lõi trong transformer vẫn là một hạn chế và nút thắt cổ chai quan trọng của kiến trúc. Cảnh báo này đã dẫn đến việc tìm kiếm các kiến trúc vượt qua nút thắt cổ chai này trong khi duy trì hiệu suất và khả năng mở rộng quy mô của transformer.

Một hướng nghiên cứu đầy hứa hẹn là các mô hình không gian trạng thái (SSM) (Gu et al., 2021a), thay thế hoạt động attention trong transformer bằng một hệ thống động học tuyến tính có thể được tính toán theo hai cách - hoặc như một hệ thống động học hồi quy hoặc như một quét song song. Chế độ song song này đảm bảo huấn luyện hiệu quả trên phần cứng GPU, trong khi chế độ hồi quy cho phép tạo ra thời gian tuyến tính và bộ nhớ không đổi, tương tự như một mạng nơ-ron hồi quy (RNN). Bởi vì động học tuyến tính chỉ duy trì một trạng thái ẩn có kích thước cố định, thời gian suy luận trong các hệ thống này không phải là bình phương theo độ dài ngữ cảnh, và không yêu cầu bộ nhớ đệm KV tăng tuyến tính, làm cho các kiến trúc SSM có bộ nhớ không đổi trong quá trình tạo ra. Có khả năng, cả hai lợi thế này có thể dẫn đến các ngữ cảnh dài hơn đáng kể trở nên khả thi cho SSM so với transformer.

Trong khi các mô hình SSM sớm hoạt động kém hơn đáng kể so với transformer trên các tác vụ ngôn ngữ, các SSM gần đây như Mamba (Gu and Dao, 2023) hoặc Griffin (De et al., 2024) dường như đang thu hẹp khoảng cách. Các mô hình như vậy dựa vào động học phụ thuộc đầu vào cho bộ trộn chuỗi SSM, tương tự như cách attention sử dụng các ma trận Q, K, V phụ thuộc đầu vào. Mặc dù có những cải thiện này, Jelassi et al. (2024) thấy rằng chúng không hoàn toàn khớp với tính biểu cảm và hiệu suất của transformer ở quy mô lớn, với một số công trình đặc biệt nhấn mạnh điểm yếu học tập trong ngữ cảnh (ICL) (Park et al., 2024; Grazzi et al., 2024). Các công trình gần đây đã lập luận rằng việc kết hợp các khối Mamba và attention có thể dẫn đến hiệu suất được cải thiện, có khả năng khớp với của attention đầy đủ (tức là, một transformer thuần túy). Trong nghiên cứu chi tiết của họ về các kiến trúc lai có thể khác nhau, Poli et al. (2024) thấy rằng khoảng một phần tư các lớp là self-attention và phần còn lại là SSM là tối ưu.

Một cách tiếp cận khác để cải thiện hiệu quả mô hình ngôn ngữ là các kiến trúc Mixture-of-Experts (MoE) (Shazeer et al., 2016; Fedus et al., 2022; Rajbhandari et al., 2022), có thể được kết hợp với SSM (Anthony et al., 2024; Lieber et al., 2024). Các mô hình MoE bao gồm việc định tuyến đầu vào đến các tập con tham số tại các lớp cụ thể, dựa trên đầu vào cho các lớp này (Jacobs et al., 1991). Trong transformer, chiến lược định tuyến này thường được áp dụng cho các lớp feed-forward (hoặc MLP) theo sau các khối attention. Trực giác là việc định tuyến này cho phép các phần của mô hình chuyên môn hóa trong việc xử lý các tình huống khác nhau, làm cho việc kích hoạt tất cả các tham số trong mô hình mọi lúc để đạt được hiệu suất tốt trở nên không cần thiết. Do đó MoE đánh đổi số lượng tham số tăng - và do đó chi phí bộ nhớ - để giảm FLOP trong huấn luyện và suy luận. Sự đánh đổi này rất hấp dẫn khi phục vụ các mô hình ngôn ngữ lớn (LLM) ở quy mô lớn, góp phần vào sự phổ biến của các kiến trúc MoE. Tuy nhiên, trong các trường hợp sử dụng khác, ví dụ như chạy LLM trên các thiết bị cục bộ với bộ nhớ hạn chế như GPU dành cho người tiêu dùng, việc tải mô hình vào bộ nhớ có thể trở thành nút thắt cổ chai lớn hơn so với FLOP của các lần truyền thuận. Điều này đảo ngược sự đánh đổi, và trở nên thú vị khi khám phá các chiến lược để tăng hiệu suất bằng cách sử dụng FLOP bổ sung thay vì tham số bổ sung.

--- TRANG 2 ---
(a) Điểm đánh giá
(b) MMLU 5-shot
(c) Độ trễ forward-pass ở độ dài chuỗi 8k
(d) Sử dụng bộ nhớ để tạo ra
Hình 1. Đánh giá các benchmark học thuật, hiệu suất suy luận và tạo ra cho Zamba so với các mô hình hàng đầu khác ở quy mô này. Chúng tôi quan sát thấy Zamba tiếp cận nhưng hơi tụt hậu so với hiệu suất của các mô hình như vậy ở nhiều đánh giá, tuy nhiên, như bảng (b) cho thấy, nó đã được huấn luyện trên ít token hơn nhiều. Do các đổi mới kiến trúc của nó, nó có thể được suy luận nhanh hơn đáng kể và yêu cầu ít bộ nhớ hơn nhiều để tạo ra. Kết quả đánh giá: MMLU là 5-shot, Math là GSM8k 5-shot, code là Human-Eval pass@100, QA là trung bình của zero-shot BoolQ và OpenBookQA, reasoning là trung bình của zero-shot ARC, HellaSwag, WinoGrande, và PIQA.

Một số công trình gần đây đã bắt đầu khám phá công khai các phương pháp học tập chương trình giảng dạy (Elman, 1993; Bengio et al., 2009) để huấn luyện các mô hình ngôn ngữ lớn. Liu et al. (2018) cho thấy rằng một chương trình giảng dạy về chất lượng dữ liệu tăng dần dẫn đến cải thiện trên các tác vụ trả lời câu hỏi. Krishna et al. (2023) báo cáo rằng dữ liệu tinh chỉnh dường như có lợi trong quá trình huấn luyện trước. Gunasekar et al. (2023) chứng minh rằng huấn luyện trước liên tục với dữ liệu tổng hợp chất lượng cao có thể tăng cường đáng kể hiệu suất của mô hình. Gupta et al. (2023) và Ibrahim et al. (2024) nhấn mạnh cách trong các thiết lập huấn luyện trước liên tục, các chiến lược với lịch trình tốc độ học tập và phát lại dẫn đến cải thiện thích ứng trên dữ liệu huấn luyện, trong khi giảm thiểu việc quên trên dữ liệu đã học trước đó. Trực giác đằng sau phương pháp chương trình giảng dạy của Zamba là (1) dữ liệu tinh chỉnh và tổng hợp có thể được coi là có chất lượng cao hơn dữ liệu web, và (2) trong khi dữ liệu chất lượng cao ít có sẵn hơn nhiều so với dữ liệu web chung, việc thích ứng nhanh trong vài bước huấn luyện được thể hiện với giai đoạn cuối của lịch trình vô hạn của Ibrahim et al. (2024) cho phép tận dụng dữ liệu chất lượng cao trong tương đối ít lần lặp ở cuối huấn luyện. Phương pháp này cũng đã được gợi ý bởi các mô hình tiên phong hiện tại (Gemma Team et al., 2024; Team et al., 2023) hoặc Abdin et al. (2024) như một phương pháp chính để cải thiện hiệu suất. Trong quá trình phát triển Zamba, nhiều công trình gần đây đã bắt đầu báo cáo một phương pháp chương trình giảng dạy hai giai đoạn, sử dụng dữ liệu web huấn luyện trước tiêu chuẩn cho lần đầu tiên, sau đó là một 'giai đoạn ủ' trong đó mô hình được huấn luyện trên dữ liệu hướng dẫn và tổng hợp chất lượng cao trong quá trình suy giảm tốc độ học tập nhanh chóng. Trực giác là sự suy giảm nhanh chóng này giúp mô hình tập trung nhiều hơn vào việc đồng hóa dữ liệu chất lượng cao mới hơn nếu nó chỉ được bao gồm trong suốt quá trình huấn luyện trước, và đặc biệt hữu ích khi lượng dữ liệu chất lượng cao có sẵn nhỏ so với dữ liệu huấn luyện trước chính. Mô tả công khai đầu tiên của hình thức huấn luyện này là miniCPM (Hu et al., 2024), người mô tả phương pháp này khá chi tiết, tiếp theo là Nemotron (Parmar et al., 2024a) và OLMo-v1.7 (AI2, 2024).

--- TRANG 3 ---
A. Tổng quan
Trong báo cáo kỹ thuật này, chúng tôi phát hành và mô tả quy trình huấn luyện cho Zamba, một SSM 7B dựa trên Mamba với kiến trúc attention chia sẻ toàn cục mới lạ. Zamba được huấn luyện chỉ trên 1T token của các bộ dữ liệu web mở, nhưng hiệu suất của nó tiếp cận với các mô hình dựa trên transformer ∼7B hàng đầu (xem Hình 1(a)). Kiến trúc độc đáo của Zamba kết hợp xương sống Mamba với một lớp self-attention chia sẻ toàn cục (xem Hình 2), hợp nhất lợi ích của transformer cho truy xuất và học tập trong ngữ cảnh với hiệu quả suy luận của Mamba. Kiến trúc này cung cấp một cách để tăng hiệu suất với chi phí tham số nhỏ và không đổi và do đó chi phí bộ nhớ. Zamba cũng là SSM hoạt động tốt nhất trong phạm vi mô hình 7B nhỏ và là mô hình SSM dày đặc hoạt động tốt nhất có sẵn. Zamba khớp với các mô hình 7B tiên tiến trên nhiều đánh giá ngôn ngữ học, trong khi hơi tụt hậu trên các bài kiểm tra lý luận và học tập trong ngữ cảnh, có thể do sự chênh lệch dữ liệu đáng kể giữa Zamba và các mô hình ∼7B hàng đầu khác.

Kiến trúc mới lạ của chúng tôi được lấy cảm hứng từ mối quan hệ giữa vỏ não và hồi hải mã trong não (Whittington et al., 2020, 2021) - nơi các lớp và vùng khác nhau của vỏ não, mặc dù thực hiện các nhiệm vụ khác nhau, tất cả đều gửi và nhận thông tin từ một kho lưu trữ bộ nhớ chia sẻ duy nhất trong hồi hải mã. Ngoài ra, khái niệm cốt lõi của chúng tôi về các lớp chia sẻ cho phép chúng tôi chi tiêu thêm flop để cải thiện hiệu suất có liên quan chặt chẽ đến công việc về các mô hình hồi quy (Dehghani et al., 2018) có sự tương đồng sâu sắc với xử lý thần kinh (van Bergen and Kriegeskorte, 2020; Tscshantz et al., 2023). Mặc dù vẫn còn sớm, chúng tôi tin rằng kiến trúc Zamba là một bước quan trọng hướng tới việc thiết kế các kiến trúc với kho lưu trữ bộ nhớ toàn cục và chia sẻ tương tự như cách não hoạt động.

Chúng tôi huấn luyện Zamba theo phương pháp hai giai đoạn sử dụng huấn luyện trước chung và giai đoạn ủ trên các bộ dữ liệu chất lượng cao kết hợp với suy giảm tốc độ học tập nhanh chóng. Giống như Ibrahim et al. (2024) và trái ngược với Hu et al. (2024), chúng tôi thấy tốt hơn là làm ấm lại tốc độ học tập theo sau bởi suy giảm hàm mũ nhanh chóng (thay vì tuyến tính). Chúng tôi thấy rằng điều này cải thiện đáng kể hiệu suất của mô hình của chúng tôi trên một số đánh giá downstream, nhưng có ít ảnh hưởng đến những đánh giá khác. Để hỗ trợ nghiên cứu mở về tác động của quá trình ủ này, chúng tôi phát hành cả mô hình ủ cuối cùng và mô hình sau khi chỉ huấn luyện giai đoạn 1. Ngoài ra, chúng tôi phát hành tất cả các checkpoint trong quá trình huấn luyện cho cả giai đoạn 1 và giai đoạn ủ. Zamba là mô hình checkpoint mở hoạt động tốt nhất, vượt qua OLMo 1.7 được phát hành đồng thời (AI2, 2024) cũng như vượt trội đáng kể so với Pythia (Biderman et al., 2023). Chúng tôi tin rằng các checkpoint mở là rất quan trọng để cho phép cộng đồng học thuật truy cập vào dữ liệu chính xung quanh huấn luyện mô hình tiên tiến và để cho phép nghiên cứu mở và hiệu quả về các câu hỏi chính liên quan đến động lực học tập. Mô hình của chúng tôi cũng là SSM checkpoint mở duy nhất ở bất kỳ quy mô hoặc hiệu suất đáng kể nào và chúng tôi hy vọng rằng công việc của chúng tôi có thể hữu ích cho các nhà nghiên cứu hiểu sâu hơn về cách học tập hoạt động trong các kiến trúc thay thế này so với transformer.

Hình 2. Kiến trúc Zamba. Zamba bao gồm một xương sống của các khối Mamba tiêu chuẩn được kết nối với một khối attention và MLP chia sẻ. Khối này được lặp lại mỗi 6 khối Mamba nhưng có tham số chia sẻ, cho phép Mamba sử dụng nhiều FLOP hơn để tăng hiệu suất với cùng chi phí bộ nhớ. Các embedding đầu vào luôn được nối với dòng dư đi vào khối attention chia sẻ vì điều này cung cấp một đường dẫn bổ sung cho mô hình để nhớ các đầu vào. Sau khối, một phép chiếu tuyến tính đã học ánh xạ đầu ra trở lại dòng dư.

B. Đóng góp
Tóm lại, các đóng góp của chúng tôi với Zamba là như sau:
• Kiến trúc lai Transformer-SSM tiên tiến ở quy mô 7B, bảo tồn hiệu quả FLOP của SSM cũng như khả năng học tập trong ngữ cảnh của attention
• Tối ưu hóa shared-attention lấy cảm hứng từ khoa học thần kinh mới lạ bảo tồn hiệu suất mô hình hóa của các khối attention độc lập, trong khi tiết kiệm bộ nhớ
• Sao chép thành công các phương pháp huấn luyện hai giai đoạn trong một mô hình quy mô lớn

Các checkpoint giai đoạn 1 và ủ của Zamba có thể được tải xuống từ: https://huggingface.co/Zyphra/Zamba-7B-v1-phase1 và https://huggingface.co/Zyphra/Zamba-7B-v1.

II. MÔ HÌNH
Các kiến trúc Transformer thường được xây dựng bằng cách xen kẽ các khối self-attention, thực hiện trộn chuỗi, và MLP, thực hiện xử lý từng token. Các khối này được sắp xếp xung quanh một dòng dư, về mặt lý thuyết cho phép truyền tín hiệu trung thực qua mạng, và các đầu ra từ dòng dư được chuẩn hóa bởi một lớp layer-norm. Một biểu diễn toán học sơ đồ của một lớp transformer duy nhất như sau:

xl+1=xl+MLP(LN(Self-Attention (LN(xl))) (1)

trong đó xl biểu diễn các kích hoạt của dòng dư tại lớp l. Yêu cầu tính toán và bộ nhớ và khối self-attention là tuyến tính và bình phương theo độ dài chuỗi, tương ứng, do đó thúc đẩy việc tìm kiếm các kiến trúc thay thế.

Ngược lại, các kiến trúc SSM như Mamba thường thu gọn việc trộn chuỗi và xử lý token lại với nhau thành một khối duy nhất, do đó dẫn đến một kiến trúc hoàn toàn đồng nhất. Giống như một transformer, Mamba cũng sử dụng một dòng dư được cổng bởi một layer norm để cung cấp các đầu vào cho mỗi khối mamba, dẫn đến sơ đồ sau:

xl+1=xl+Mamba (LN(xl)) (2)

Giống như tất cả SSM, bộ trộn chuỗi của khối Mamba dựa trên một hệ thống động học tuyến tính cốt lõi. Đổi mới chính của Mamba là làm cho các ma trận điều khiển và quan sát, cũng như bước thời gian của hệ thống này, phụ thuộc đầu vào. Tức là, động lực cốt lõi của Mamba SSM, theo sơ đồ, là:

ht+1=exp(Adeltat)ht+Btxt
yt=Ctht+1 (3)

trong đó xt là đầu vào của hệ thống tại mỗi phần tử chuỗi, ht là trạng thái 'bộ nhớ' nội bộ của khối Mamba, yt là đầu ra của bộ trộn chuỗi, deltat là một bước thời gian phụ thuộc đầu vào phát sinh từ việc rời rạc hóa động lực thời gian liên tục, và Bt và Ct là các ma trận phụ thuộc đầu vào, trong khi A là một ma trận độc lập đầu vào.

Khối Mamba thực hiện cả việc trộn chuỗi thông qua các thành phần SSM cốt lõi cũng như xử lý token thông qua các lớp chiếu và một đơn vị cổng. Các đầu vào cho khối Mamba trước tiên được xử lý bởi một lớp tuyến tính tách biệt đầu vào thành một vector được xử lý và một vector cổng. Điều này được theo sau bởi một tích chập 1d thực hiện một số trộn chuỗi sơ bộ, đầu ra của nó sau đó được sử dụng để tạo ra các ma trận Bt, Ct và deltat phụ thuộc đầu vào, theo sau bởi bộ trộn chuỗi SSM như trong Phương trình 3. Đầu ra yt của bộ trộn chuỗi sau đó được nhân theo từng phần tử với vector cổng từ đầu vào, trước khi được truyền qua một lớp tuyến tính cuối cùng đến đầu ra. Theo sơ đồ,

Mamba (x) =Lin(sigma(Lin(x))⊙SSM(Conv1D (Lin(x)))) (4)

Do cổng và các phép chiếu tuyến tính, lớp Mamba có thể thực hiện xử lý token như được thực hiện bởi các MLP trong kiến trúc transformer, trong khi cũng chứa bộ trộn chuỗi SSM. Điều này có nghĩa là trong một kiến trúc Mamba vs Transformer có kích thước xấp xỉ như nhau, mô hình Mamba sẽ có gấp đôi bộ trộn chuỗi, mặc dù có khả năng chúng ít biểu cảm hơn so với self-attention đầy đủ.

Zamba bao gồm một xương sống Mamba thuần túy được tăng cường với một khối self-attention chia sẻ toàn cục (GSA) bổ sung mỗi N khối Mamba. Khối GSA bao gồm một khối self-attention và khối MLP nối tiếp, với trọng số chia sẻ cho cả hai. Điều này có nghĩa là mặc dù attention được thực hiện nhiều lần trong suốt mạng, nó sử dụng các tham số giống nhau và do đó giảm chi phí bộ nhớ của mô hình cả cho các tham số attention và, quan trọng hơn, cho kích thước bộ nhớ đệm KV trong quá trình tạo ra. Để cho phép một số đặc thù cho mỗi lần gọi GSA, có một ánh xạ tuyến tính có thể học được không chia sẻ từ khối GSA đến dòng dư. Ngoài ra, mỗi đầu vào cho khối GSA là sự nối giữa các kích hoạt dư tại lớp đó với các hoạt động dư ban đầu (tức là, sau embedding đầu vào ban đầu). Điều này được thực hiện để đảm bảo rằng thông tin từ đầu mạng luôn có sẵn cho self-attention. Chúng tôi thấy rằng điều này mang lại cải thiện hiệu suất nhất quán, có lẽ chỉ ra rằng dòng dư không duy trì thông tin một cách trung thực trong suốt mạng như mạng yêu cầu. Do sự nối, các vector query, key và value trong khối GSA có kích thước gấp đôi, sau đó được chiếu trở lại, trước khối MLP, đến kích thước dư thông qua bộ chiếu ra của attention. Theo sơ đồ, khối GSA Zamba có thể được biểu diễn như,

yl=Linl(MLP(LN(Self-Attention (LN([xl, x0]))))) (5)

mỗi N khối và nếu không là Mamba thuần túy. Lớp Mamba tiếp theo sau đó được tính toán bằng cách thêm yl vào phần không dư của đầu vào:

xl+1=xl+Mamba (LN(xl+yl)) (6)

Trong các nghiên cứu loại bỏ sớm trên các mô hình nhỏ, chúng tôi thấy rằng kiến trúc này mang lại cải thiện nhất quán so với Mamba thuần túy và cũng đánh bại đáng kể Mambaformer phù hợp tham số (Park et al., 2024), hoặc các kiến trúc lai, kết hợp các khối self-attention và Mamba. Do vậy, sự thành công của kiến trúc Zamba cung cấp bằng chứng rằng có lẽ nhiều tham số của các lớp self-attention là dư thừa và chúng có thể được thay thế bằng một hoạt động self-attention duy nhất, lặp lại. Chúng tôi tin rằng, với việc thường các nút thắt cổ chai chính để chạy mô hình là chi phí bộ nhớ của mô hình cả về số lượng tham số và tạo ra, việc điều tra các kiến trúc đánh đổi FLOP suy luận bổ sung cho các tham số giảm trong khi duy trì hiệu suất là có giá trị.

A. Giai đoạn 1
Chúng tôi đã huấn luyện Zamba-v1 trong hai giai đoạn. Đầu tiên, chúng tôi thực hiện giai đoạn huấn luyện trước tiêu chuẩn trên các bộ dữ liệu web mở. Trong giai đoạn này, Zamba-v1 được huấn luyện trên khoảng 950 tỷ token. Chúng tôi thực hiện suy giảm cosine từ tốc độ học tập ban đầu là 1.5*10−4 đến 7.5*10−5; sự suy giảm này ít dốc hơn đáng kể so với thông thường, để đảm bảo rằng có chỗ cho sự suy giảm nhanh chóng trong giai đoạn ủ. Zamba-v1 được huấn luyện trên 128 GPU H100 trong khoảng 30 ngày. Chúng tôi sử dụng song song dữ liệu và song song tensor qua hai rank, được thực hiện qua attention, MLP và các khối Mamba. Chúng tôi sử dụng bộ tối ưu hóa phân tán Zero-1 để chia sẻ các trạng thái tối ưu hóa qua các rank và checkpoint kích hoạt. Chúng tôi huấn luyện với độ dài chuỗi 4096 token, sử dụng tokenizer Mistral. Một bộ đầy đủ các siêu tham số huấn luyện có thể được tìm thấy trong Phụ lục A.

B. Bộ dữ liệu
Bộ dữ liệu giai đoạn 1 của chúng tôi bao gồm một hỗn hợp các bộ dữ liệu web mở và bao gồm The Pile (Gao et al., 2020), RefinedWeb (Penedo et al., 2023), C4 (Raffel et al., 2020), PeS2o (Soldaini and Lo, 2023), và arxiv. Thành phần của bộ dữ liệu của chúng tôi được hiển thị trong Bảng I.

Tên Bộ dữ liệu Tỷ lệ
peS2o 4.89%
C4 12.85%
Pile 15.48%
RefinedWeb 62.35%
Cosmopedia 3.55%
arxiv 0.90%

BẢNG I
TỶ LỆ TOKEN CỦA CÁC BỘ DỮ LIỆU PHỤ TẠO NÊN BỘ DỮ LIỆU HUẤN LUYỆN TRƯỚC 1T ĐẦY ĐỦ CỦA CHÚNG TÔI. TẤT CẢ CÁC BỘ DỮ LIỆU CHÚNG TÔI SỬ DỤNG ĐỀU MỞ VÀ CÓ SẴN.

Chúng tôi thực hiện lọc tương đối nhỏ và sau đó khử trùng lặp mờ dựa trên LSH trên bộ dữ liệu này.

Chúng tôi áp dụng các bộ lọc sau cho các thành phần Pile, và C4-en của bộ dữ liệu của chúng tôi: độ dài tối thiểu 100 ký tự, độ dài từ trung bình tối thiểu 3 ký tự, độ dài từ trung bình tối đa 12 ký tự, tỷ lệ tối đa các ký tự không phải chữ và số là 0.3, tỷ lệ tối đa các ký tự số là 0.2. Đối với khử trùng lặp mờ, chúng tôi tính toán minhash với kích thước chữ ký 128, được tính toán trên 13-gram dựa trên từ. Việc lọc này loại bỏ khoảng 10% của Pile-dedup và 0.5% của C4-en.

Chúng tôi khử trùng lặp mỗi bộ dữ liệu cả chống lại chính nó và chống lại các bộ dữ liệu khác trong bộ dữ liệu đầy đủ của chúng tôi. Chúng tôi xây dựng chỉ mục LSH nhắm đến ngưỡng độ tương đồng Jaccard 50% (25 băng với phạm vi 5) bằng cách chèn các bộ dữ liệu sau theo thứ tự cụ thể: đầu tiên Pile-dedup, sau đó C4-en, peS2o, arxiv s2orc parsed. Chúng tôi không chèn RefinedWeb vào chỉ mục, mà đơn giản chạy mỗi tài liệu của nó qua nó (RefinedWeb được khử trùng lặp rộng rãi bởi các tác giả của họ, vì vậy chúng tôi quyết định không dành thời gian để khử trùng lặp nó chống lại chính nó, mà chỉ chống lại các bộ dữ liệu khác). Tỷ lệ phần trăm các bản sao được xác định trong mỗi bộ dữ liệu có thể được tìm thấy trong Bảng II. Chúng tôi không áp dụng bất kỳ xử lý nào cho Cosmopedia, vì chúng tôi lý luận rằng bản chất tổng hợp của nó sẽ ngụ ý rằng nó chủ yếu là chất lượng cao và không được sao chép trong các bộ dữ liệu khác.

Chúng tôi không tăng mẫu hoặc thực hiện nhiều epoch trên bất kỳ thành phần nào của bộ dữ liệu giai đoạn 1 của chúng tôi. Tổng bộ dữ liệu của chúng tôi bao gồm chỉ hơn 1 nghìn tỷ token, trong đó giai đoạn huấn luyện trước sử dụng 950 tỷ token.

Tên Bộ dữ liệu Tỷ lệ phần trăm bản sao
Pile 25%
C4 30%
peS2o 31%
arxiv 7.1%
RefinedWeb 1.2%

BẢNG II
TỶ LỆ PHẦN TRĂM TÀI LIỆU ĐƯỢC ĐÁNH DẤU LÀ BẢN SAO BỞI KHỬ TRÙNG LẶP MỜ LSH-MINHASH.

C. Ủ
Giai đoạn ủ theo trực giác từ các lịch trình tốc độ học tập vô hạn được trình bày trong Ibrahim et al. (2024), cho thấy cải thiện nhanh chóng nhờ suy giảm hàm mũ. Làm điều này trên dữ liệu chất lượng cao hơn cho phép các phương pháp chương trình giảng dạy yêu cầu ít token chất lượng cao hơn nhiều so với thông thường trong quá trình huấn luyện trước. Tương tự như miniCPM (Hu et al., 2024) và Nemotron (Parmar et al., 2024a), chúng tôi thực hiện suy giảm LR nhanh chóng trên một bộ dữ liệu bao gồm hỗn hợp token chất lượng cao và bộ dữ liệu huấn luyện trước của chúng tôi. Chúng tôi sử dụng tỷ lệ phát lại 60% dữ liệu huấn luyện trước gốc và 40% bộ dữ liệu ủ mới.

Bộ dữ liệu ủ của chúng tôi bao gồm một bộ sưu tập lớn (hơn 100) các bộ dữ liệu chất lượng cao hiện có từ các nguồn khác nhau. Chúng bao gồm các bộ dữ liệu toán học lớn (như StackMathQA) và mã (như EvolInstructCode), cũng như các bộ dữ liệu tinh chỉnh hướng dẫn (như OpenOrca), và một số ví dụ dữ liệu tổng hợp từ các mô hình ngôn ngữ mạnh hơn. Đối với hầu hết các bộ dữ liệu này, chúng tôi chỉ thực hiện một epoch, nhưng đối với các tập con chất lượng cao được chọn, chúng tôi thực hiện 2 epoch. Chúng tôi sử dụng lịch trình suy giảm hàm mũ có dạng etat=Ae−t/(gammaT)+B, trong đó t là số lần lặp, T là tổng số lần lặp ủ, và chúng tôi thấy gamma= 0.25 là giá trị tối ưu. Các hệ số A và B được tìm thấy bằng cách yêu cầu eta0 và etaT là tốc độ học tập sau khi làm ấm và ở cuối ủ, tương ứng, trong đó chúng tôi sử dụng eta0= 1.1*10−4 và etaT= 10−7. Không giống như miniCPM, chúng tôi thấy rằng việc làm ấm lại tốc độ học tập từ 0 trở lại tốc độ học tập tối đa và sau đó suy giảm lại hoạt động tốt hơn so với việc bắt đầu giai đoạn ủ ở tốc độ học tập cuối cùng của lần chạy huấn luyện trước gốc - một phát hiện tương tự như Ibrahim et al. (2024) trong thiết lập huấn luyện trước liên tục. Chúng tôi cũng thấy rằng lịch trình suy giảm hàm mũ của chúng tôi vượt trội hơn suy giảm tuyến tính được sử dụng trong miniCPM. Cuối cùng, trong khi chúng tôi đã thấy sự giảm loss trên bộ dữ liệu huấn luyện trước gốc, nó không rõ ràng như được mô tả bởi miniCPM, mà chúng tôi quy cho Zamba là một mô hình lớn hơn nhiều được huấn luyện trên nhiều token hơn, hoặc có thể nhấn mạnh sự khác biệt trong các bộ dữ liệu ủ hoặc lịch trình của chúng tôi.

III. HIỆU SUẤT
Chúng tôi đánh giá Zamba trên một bộ các benchmark đánh giá mô hình ngôn ngữ tiêu chuẩn bao gồm lý luận, trả lời câu hỏi, toán học, mã, và câu hỏi kiến thức chung. Nói chung, xu hướng là Zamba hoạt động tốt cho kích thước của nó, và thường vượt xa các mô hình cạnh tranh được huấn luyện trên các bộ dữ liệu mở (xem Hình 3), nhưng hơi tụt hậu so với hiệu suất của các mô hình hàng đầu (xem Hình 1). Tuy nhiên, theo từng token (và do đó theo từng FLOP huấn luyện), Zamba cực kỳ hiệu quả so với các mô hình tương đương. Chúng tôi tin rằng điều này chủ yếu do thực tế là Zamba đã được huấn luyện trên ít token hơn đáng kể so với các mô hình hàng đầu, và có khả năng sử dụng dữ liệu web mở chất lượng thấp hơn so với các bộ dữ liệu bí mật, nội bộ của các công ty AI lớn. Tuy nhiên, chúng tôi lưu ý rằng Zamba vượt trội hơn Llama2 và do đó không phải là một mô hình yếu. Hiệu suất của Zamba trên mã tương đối kém so với các mô hình hàng đầu và chúng tôi tin rằng điều này rất có thể được giải thích bởi sự thiếu hụt tương đối của mã trong bộ dữ liệu của chúng tôi, vì chúng tôi chỉ sử dụng các bộ dữ liệu web trực tiếp và không phải github. Zamba dường như hoạt động tốt, thường cạnh tranh với các mô hình hàng đầu trên các benchmark mô hình ngôn ngữ chung và lý luận như PIQA, Winogrande, và HellaSwag, điều này thú vị vì nó có khả năng đã được huấn luyện trên ít token và có khả năng chất lượng thấp hơn đáng kể so với các mô hình này, và điều này có thể chỉ ra lợi ích kiến trúc của Zamba.

Về hiệu quả suy luận và tạo ra, Zamba cực kỳ hiệu quả. Mặc dù sử dụng nhiều FLOP hơn trên mỗi tham số do chia sẻ tham số của chúng tôi, lần truyền thuận của Zamba nhanh hơn đáng kể so với các mô hình cạnh tranh ở quy mô 7B, một lợi thế tăng lên với độ dài chuỗi dài hơn. Ngoài ra, do xương sống SSM của Zamba, bộ nhớ cần thiết cho bộ nhớ đệm KV trong Mamba được giảm bởi một yếu tố lớn so với các mô hình khác có quy mô tương tự, do đó cho phép Zamba tạo ra hiệu quả hơn và đạt được các ngữ cảnh dài hơn đáng kể trên một thiết bị duy nhất (xem Hình 4).

Zamba sở hữu một số chất lượng kiến trúc làm cho nó đặc biệt phù hợp với hiệu quả suy luận:
• Việc sử dụng tiết kiệm các lớp attention của Zamba giữ cho bộ nhớ KV-cache của nó đặc biệt thấp. Cụ thể, có một khối attention duy nhất được áp dụng 13 lần trong Zamba-7B, với các kích hoạt độc lập và các mục bộ nhớ đệm KV tại mỗi lần gọi.
• Các khối Mamba có thông lượng cao hơn đáng kể so với các khối attention hoặc MLP tương đương (Gu and Dao, 2023).
• Chúng tôi áp dụng kernel hiệu quả nhất có sẵn cho mỗi khối có sẵn. Các kernel Mamba được lấy từ triển khai mamba mã nguồn mở (Gu and Dao, 2023) và được điều chỉnh cho kiến trúc H100. Attention được triển khai thông qua các kernel Flash Attention v2 (Dao, 2023). Các RMSNorm được triển khai từ thư viện Transformer Engine (NVIDIA, 2023).

Tại thời điểm viết, Zamba là mô hình checkpoint mở hoạt động tốt nhất có sẵn. Chúng tôi tin rằng Zamba có ý nghĩa đáng kể đối với những người nghiên cứu động lực huấn luyện, cả vì hiệu suất của nó và vì kiến trúc độc đáo của nó. Bằng cách so sánh động lực biểu diễn trong các mô hình như Zamba với các kiến trúc transformer tiêu chuẩn như OLMo (Groeneveld et al., 2024) *, chúng tôi hy vọng rằng tiến bộ sẽ được đạt được hướng tới việc hiểu cách các kiến trúc mô hình ảnh hưởng đến việc hình thành biểu diễn trong quá trình huấn luyện.

IV. CÔNG VIỆC LIÊN QUAN
A. Mô hình không gian trạng thái
Chi phí bình phương của attention theo độ dài chuỗi đã mang lại một số phương pháp cố gắng thực hiện khả năng mô hình ngôn ngữ của transformer nhưng không có hạn chế này. Nếu điều này có thể thì sẽ cho phép độ dài ngữ cảnh lớn hơn đáng kể và tạo ra hiệu quả hơn nhiều về cả FLOP và bộ nhớ cần thiết để lưu trữ bộ nhớ đệm KV. Một hướng công việc nhằm giải quyết vấn đề này là sử dụng các mô hình không gian trạng thái (Gu et al., 2021a, 2020, 2021b). Các mô hình không gian trạng thái sử dụng phương pháp hệ thống động học tuyến tính để mô hình ngôn ngữ. Một hệ thống động học như vậy duy trì một 'bộ nhớ' kích thước không đổi được duy trì trong suốt chuỗi phải mã hóa tất cả thông tin cần thiết để dự đoán token tiếp theo. Không giống như RNN tuy nhiên do tính tuyến tính của động lực, SSM có thể được viết và tính toán như một tích chập hoặc một quét song song cho phép chuyển tiếp hiệu quả trong một chuỗi dài như một transformer cho phép huấn luyện ở quy mô lớn.

Hạn chế chính của SSM là dạng hạn chế mà việc trộn chuỗi có thể thực hiện cho phép tính toán hiệu quả, cũng như việc nén tất cả thông tin thành một bộ nhớ kích thước cố định. Điều này làm cho sức mạnh biểu cảnh của SSM ít hơn transformer về mặt lý thuyết mặc dù không rõ ngôn ngữ tự nhiên yêu cầu đến mức nào sức mạnh biểu cảnh đầy đủ mà transformer mang lại. Trong khi các mô hình SSM sớm hoạt động kém hơn đáng kể so với transformer, các mô hình gần đây như Mamba, và S6 đã tuyên bố hoạt động và mở rộng quy mô ngang bằng, cũng như các kiến trúc liền kề SSM khác như Griffin (De et al., 2024), RWKV (Peng et al., 2023, 2024), và RetNets (Sun et al., 2023). Các mô hình này thường biểu cảm hơn vì chúng sở hữu cổng chọn lọc (phụ thuộc đầu vào) và điều khiển động lực nội bộ của chúng, tương tự như sự phụ thuộc đầu vào của các vector query, key và value trong một transformer.

Tuy nhiên, công việc gần đây đã nhấn mạnh những thiếu sót tiềm năng của SSM thuần túy trong học tập ngữ cảnh và các tác vụ thuật toán khác và một số nhóm đã thấy rằng việc lai hóa SSM với attention mang lại hiệu suất ngang bằng hoặc vượt trội transformer cite. Trong khi hầu hết công việc này thêm nhiều lớp attention, với Zamba chúng tôi đưa ra và cho thấy bằng chứng cho một giả thuyết thú vị - rằng một lớp attention là tất cả những gì bạn cần.

B. Ủ
Trong khi các công trình trước đây có thể đã sử dụng lịch trình ủ hai giai đoạn, nhưng không tiết lộ nó, công trình đầu tiên thảo luận công khai chi tiết về lịch trình hai giai đoạn là miniCPM (Hu et al., 2024). Họ mô tả thực hiện suy giảm tốc độ học tập chậm đầu tiên trên phần lớn dữ liệu huấn luyện trước, theo sau bởi suy giảm nhanh hơn trên các bộ dữ liệu chất lượng cao, bao gồm dữ liệu hướng dẫn. Kể từ đó, một số mô hình được phát hành gần đây khác đã thảo luận rõ ràng hoặc gián tiếp về các giai đoạn ủ. Chúng bao gồm Nemotron (Parmar et al., 2024b) và JetMoE (Shen et al., 2024), cả hai đều tuyên bố rằng ủ cải thiện đáng kể chất lượng mô hình. Sau khi phát hành ban đầu của Zamba, OLMo 1.7 (AI2, 2024) cũng tuyên bố đã thực hiện ủ, cải thiện đáng kể kết quả trên MMLU (Hendrycks et al., 2021), và sử dụng một bộ lập lịch tuyến tính dựa trên một số bộ dữ liệu chất lượng cao hơn, như wikipedia và flan-instruct. Zamba theo một phương pháp tương tự, nhưng sử dụng hỗn hợp bộ dữ liệu khác, cũng như lịch trình suy giảm hàm mũ nhanh hơn và làm ấm lại tốc độ học tập từ không.

C. Checkpoint mở
Trong khi việc phát hành các mô hình với các checkpoint đầy đủ trong quá trình huấn luyện là hiếm, có một số công trình đã làm như vậy. Bộ Pythia (Biderman et al., 2023) đã tiên phong phương pháp này và đã có vai trò quan trọng trong nhiều công trình về khả năng diễn giải và động lực huấn luyện. Tuy nhiên, được huấn luyện cho mục đích khoa học, các mô hình Pythia đạt được hiệu suất xa so với nghệ thuật tiên tiến. Một số công trình gần đây hơn cũng nhằm mở và dân chủ hóa huấn luyện trước LLM bằng cách cung cấp các checkpoint và chi tiết huấn luyện. Chúng bao gồm OLMo (Groeneveld et al., 2024), người cung cấp mã huấn luyện chi tiết và các checkpoint cho một mô hình 7B tham số, và LLM360 (Liu et al., 2023), người cung cấp mô tả huấn luyện và các mô hình checkpoint mở dựa trên kiến trúc Llama. Ngoài ra, để nghiên cứu các kiến trúc Mamba thuần túy, Zyphra đã phát hành các checkpoint mở của một mô hình Mamba thuần túy 370m nhỏ được huấn luyện trên bộ dữ liệu Pile (Zyphra, 2024).

V. THẢO LUẬN
Trong báo cáo kỹ thuật này, chúng tôi giới thiệu Zamba, một SSM 7B mã nguồn mở có tính cạnh tranh cao với các mô hình hàng đầu. Do đó chúng tôi chứng minh một cách thuyết phục khả năng mở rộng quy mô của các kiến trúc SSM đến quy mô này. Hơn nữa, chúng tôi sử dụng một kiến trúc mới lạ sử dụng một khối attention chia sẻ toàn cục có được lợi ích của các kiến trúc lai SSM-attention trong khi tối thiểu hóa các tham số dành riêng cho attention. Hơn nữa, chúng tôi mô tả chế độ huấn luyện hai giai đoạn mà Zamba đã trải qua và phát hành cả mô hình giai đoạn 1 (huấn luyện trước) và mô hình cuối cùng (đã ủ). Zamba được huấn luyện với ngân sách tương đối nhỏ khoảng $200k và một nhóm 7 nhà nghiên cứu trong suốt một tháng và tiếp cận các mô hình hàng đầu về hiệu suất, do đó chứng minh rằng việc tiếp cận nghệ thuật tiên tiến trong huấn luyện trước LLM không nhất thiết yêu cầu ngân sách hoặc nhóm khổng lồ và không bị hạn chế chỉ ở một số công ty hàng đầu.

Đó là một câu hỏi thú vị về nguồn gốc của khoảng cách còn lại giữa Zamba và các mô hình hàng đầu ở quy mô này là gì. Mô hình Zamba được huấn luyện đến cuối giai đoạn 1 đạt được mức hiệu suất Llama2 chỉ từ 1T token, trong khi Llama2 được huấn luyện trên ít nhất 2T token. Sự khác biệt này có thể phát sinh từ sự khác biệt bộ dữ liệu, mặc dù với việc bộ dữ liệu của Zamba chỉ đơn giản bao gồm các bộ dữ liệu web mở đã khử trùng lặp, sẽ không có khả năng bộ dữ liệu của Zamba vượt trội đáng kể về chất lượng so với Llama2. Cũng có thể rằng kiến trúc của chúng tôi mang lại cho chúng tôi lợi thế đáng kể so với kiến trúc transformer Llama và thực sự, mặc dù các tuyên bố rằng các mô hình Mamba gặp khó khăn với ICL (Park et al., 2024), thậm chí mô hình cơ sở của chúng tôi hoạt động tương đương với Llama2 ở đây trên MMLU, một chỉ số đánh giá được biết đến là yêu cầu ICL đáng kể để hoạt động tốt. Do đó có thể rằng thậm chí một lớp attention duy nhất đã đủ để đạt được sự ngang bằng transformer trên ICL.

Sau đó có câu hỏi làm thế nào để thu hẹp khoảng cách giữa Zamba và các mô hình trọng số mở hàng đầu như Mistral, Gemma, và Llama3. Có khả năng rằng một phần của khoảng cách này được gây ra bởi sự chênh lệch lớn về số lượng token huấn luyện trước. Mô hình của chúng tôi được huấn luyện chỉ trên 1T token so với 8T cho Gemma, 15T cho Llama3, và một số lượng không rõ (nhưng có khả năng tương tự lớn) cho Mistral. Giống như những người khác, và phù hợp với các định luật mở rộng quy mô Chinchilla, chúng tôi quan sát thấy sự tăng hiệu suất tiếp tục (trên thang đo log) thậm chí hướng tới cuối huấn luyện, ngụ ý rằng mô hình của chúng tôi chưa đạt đến đỉnh trong loss và có thể được huấn luyện hữu ích trên nhiều token hơn. Chất lượng bộ dữ liệu huấn luyện trước có khả năng cũng là một yếu tố đáng kể với bộ dữ liệu của chúng tôi, trong khi sử dụng các bộ dữ liệu web nổi tiếng hiện có, chúng tôi chỉ thực hiện lọc và khử trùng lặp đơn giản, trong khi có nhiều kỹ thuật cho việc chuẩn bị bộ dữ liệu cải thiện đáng kể có sẵn trong tài liệu (Xie et al., 2024; Tirumala et al., 2024; Maini et al., 2024).

Chúng tôi thấy cải thiện đáng kể trên nhiều điểm đánh giá trong giai đoạn ủ, cung cấp một sao chép độc lập của các tuyên bố của miniCPM (Hu et al., 2024). Với những bước nhảy đáng kể giữa các mô hình huấn luyện trước cơ sở như Llama1, Llama2, và OLMo với các bộ dữ liệu tương đối được biết đến, và các mô hình hàng đầu trong phạm vi 7B như Mistral và Gemma, có khả năng rằng việc tăng cường dữ liệu web với dữ liệu tổng hợp, hướng dẫn, hoặc các nguồn dữ liệu chất lượng cao khác đã đóng một vai trò quan trọng trong hiệu suất của các mô hình này. Điều này có thể là trong chính quá trình huấn luyện trước, như được lập luận bởi loạt mô hình phi (Li et al., 2023; Abdin et al., 2024), hoặc trong một giai đoạn ủ tương tự như miniCPM và giai đoạn chúng tôi thực hiện. Theo kinh nghiệm, chúng tôi thấy rằng giai đoạn ủ của Zamba thu hẹp khoảng một nửa hoặc hơn khoảng cách từ Llama2 đến các mô hình tiên tiến đặc biệt trên các đánh giá giống lý luận hơn như MMLU (Hendrycks et al., 2021) và ARC (Clark et al., 2018). Có thể rằng một giai đoạn ủ được cải thiện trên dữ liệu tổng hợp nhiều hơn đáng kể, hoặc huấn luyện trước trên nhiều token hơn đáng kể có thể thu hẹp khoảng cách này.

Trong khi một số công trình gần đây đã tuyên bố thực hiện ủ, tương đối ít đã được công bố về các phương pháp chính xác được sử dụng cho điều này hoặc hiệu suất của mô hình cơ sở trước khi ủ bắt đầu. Chúng tôi chứng minh rằng ủ trên dữ liệu chất lượng cao có thể cải thiện đáng kể một mô hình từ mức cơ sở Llama2 xấp xỉ đến thu hẹp khoảng cách với các mô hình hàng đầu. Việc hiểu mức độ mà cải thiện hiệu suất này là thực sự và tăng cường khả năng thực sự của mô hình hay liệu nó chỉ đơn giản là huấn luyện mô hình phản ứng theo cách thân thiện với đánh giá hơn vẫn cần được làm rõ. Hơn nữa, nhiều câu hỏi vẫn mở như lịch trình ủ tối ưu, tỷ lệ phát lại tối ưu của dữ liệu huấn luyện trước gốc, và thành phần tối ưu của các bộ dữ liệu ủ. Chúng tôi hy vọng rằng bằng cách phát hành mô hình cơ sở của chúng tôi, và tất cả các checkpoint ủ, chúng tôi có thể giúp cộng đồng học thuật bắt đầu trả lời những câu hỏi này.

Với Zamba, bằng cách cả xác nhận hiệu suất của Mamba ở quy mô lớn và bằng cách tiên phong một kiến trúc mới lạ vượt ra ngoài điều đó, chúng tôi cũng đã thực hiện một bước hướng tới việc rời xa kiến trúc transformer tiêu chuẩn để huấn luyện các mô hình tiên tiến. Chúng tôi tin rằng các đổi mới kiến trúc tương đối bị nghiên cứu ít với sự thành công của mô thức mở rộng quy mô. Tuy nhiên, khi chi phí huấn luyện các mô hình tiên tiến tăng lên, và khi lợi ích của việc mở rộng quy mô dữ liệu ở quy mô nhỏ trở nên ngày càng biên, khả năng của những lợi ích không đổi từ các kiến trúc và mô thức huấn luyện trước được cải thiện so với những lợi ích logarit từ dữ liệu bổ sung trở nên ngày càng quan trọng trong việc đẩy tiến biên giới hiệu suất cho các nhóm kích thước mô hình nhỏ hơn.

Cuối cùng, với Zamba, chúng tôi cung cấp tất cả các checkpoint trong cả giai đoạn huấn luyện trước và ủ (một cái mỗi 2500 bước) để nghiên cứu khoa học. Chúng tôi tin rằng nhiều câu hỏi khoa học có ý nghĩa sâu sắc để hiểu động lực học tập của các mô hình như vậy chỉ có thể được tiếp cận bằng cách nghiên cứu sự tiến hóa của trọng số mô hình trong quá trình huấn luyện và ít nhóm học thuật có ngân sách hoặc chuyên môn để huấn luyện các mô hình gần với nghệ thuật tiên tiến. Do vậy, chúng tôi khuyến khích các nhóm khác huấn luyện các mô hình ở biên giới xem xét việc cung cấp và mở các checkpoint của mô hình của họ cũng vậy.

--- TRANG 9 ---
ĐÓNG GÓP CỦA TÁC GIẢ
Paolo - Đóng góp vào cơ sở hạ tầng cốt lõi. Dẫn đầu các thí nghiệm ủ. Dẫn đầu đánh giá. Dẫn đầu chuyển đổi và phát hành HuggingFace. Đóng góp vào các thí nghiệm tìm kiếm kiến trúc.

Quentin - Đóng góp vào cơ sở hạ tầng cốt lõi. Dẫn đầu phát triển cơ sở hạ tầng và tối ưu hóa huấn luyện. Dẫn đầu tối ưu hóa suy luận. Đóng góp vào quản lý và bảo trì cụm. Đóng góp vào đánh giá.

Yury - Đóng góp vào cơ sở hạ tầng cốt lõi. Dẫn đầu chuẩn bị và xử lý bộ dữ liệu. Đóng góp vào quản lý và bảo trì cụm. Đóng góp vào đánh giá.

James - Đóng góp vào các thí nghiệm tìm kiếm kiến trúc. Phát minh kiến trúc cuối cùng được sử dụng trong bài báo.

Jonathan - Đóng góp vào các thí nghiệm tìm kiếm kiến trúc.

Adam - Tư vấn tổng thể dự án. Đồng dẫn đầu phương pháp chương trình giảng dạy. Đóng góp vào các thí nghiệm ủ.

Beren - Dẫn đầu dự án tổng thể. Đóng góp vào cơ sở hạ tầng cốt lõi. Đóng góp vào các thí nghiệm ủ. Dẫn đầu tạo và xử lý các bộ dữ liệu ủ. Đóng góp vào các thí nghiệm tìm kiếm kiến trúc. Đóng góp vào đánh giá. Tác giả chính của báo cáo kỹ thuật.

LỜI CẢM ÖN
Chúng tôi muốn ghi nhận phần còn lại của nhóm Zyphra vì sự hỗ trợ của họ, bao gồm Steven Brook, Nick Alonso, Vasudev Shyam, Anna Golubeva, Tomas Figliolia và Krithik Puthalath vì những thảo luận và phản hồi hữu ích.

TÀI LIỆU THAM KHẢO
[Các tài liệu tham khảo được liệt kê theo thứ tự alphabet từ trang 9-11]

--- TRANG 12 ---
PHỤ LỤC
SIÊU THAM SỐ MÔ HÌNH VÀ HUẤN LUYỆN

Siêu tham số Giá trị
Số lượng Lớp 80
Kích thước Ẩn 3712
Kích thước Trạng thái 16
Kích thước Tích chập 4
Số lượng Đầu Attention 16
Độ dài Ngữ cảnh 4096
Kích thước Batch 512
Tốc độ Học tập Tối đa 1.5e-4
Lịch trình Suy giảm LR Cosine
LR Tối thiểu 7.5e-5
Suy giảm Trọng số 0.1
Adam Beta2 0.95
Làm ấm LR 0.01
Cắt Gradient 1.0
Độ chính xác Huấn luyện BF16
