# 2302.06218.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2302.06218.pdf
# File size: 858628 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A Uniﬁed View of Long-Sequence Models towards
Modeling Million-Scale Dependencies
Hongyu Hè
Department of Computer Science
ETH ZurichMarko Kabic
Department of Computer Science
ETH Zurich
Nomenclature
 Covariance matrix
G Gram/kernel matrix
k() Kernel function
P() Probability density
P() Token mixing process
Re()Function that extracts the real component of a complex number
FT()(Discrete) Foureir Tansform
~ ai Element atith position of column vector ~ a
A:j Column vector in jth row ofA
Ai;j Element inith rowjth column of A
Ai Row vector in ith row ofA(=Ai:)
ANMShorthand for matrix A2RNM
Fh
DDVandermonde matrix of the embedding dimension
Fs
LLVandermonde matrix of the sequence dimension
W Weight matix learned with element-wise non-linearity (e.g., ReLU, GELU)
WC
LLWeight matix of a single convolution kernel
WK
DNWeight matix of attention key (for self-attention, N=M)
WQ
DMWeight matix of attention query
WV
DMWeight matix of attention value
eX Resulting tokens with inductive bias introduced into X
XLDInput sequence of length Land embedding dimension D, whereLD
Correspondence to <honghe@inf.ethz.ch>arXiv:2302.06218v3  [cs.LG]  16 Feb 2023

--- PAGE 2 ---
Abstract
Ever since their conception, Transformers have taken over traditional sequence mod-
els in many tasks, such as NLP, image classiﬁcation, and video/audio processing,
for their fast training and superior performance. Much of the merit is attributable
to positional encoding and multi-head attention. However, Transformers fall short
in learning long-range dependencies mainly due to the quadratic complexity scaled
with context length, in terms of both time and space. Consequently, over the past
ﬁve years, a myriad of methods has been proposed to make Transformers more
efﬁcient. In this work, we ﬁrst take a step back, study and compare existing solu-
tions to long-sequence modeling in terms of their pure mathematical formulation.
Speciﬁcally, we summarize them using a uniﬁed template, given their shared nature
of token mixing. Through benchmarks, we then demonstrate that long context
length does yield better performance, albeit application-dependent, and traditional
Transformer models fall short in taking advantage of long-range dependencies.
Next, inspired by emerging sparse models of huge capacity, we propose a machine
learning system for handling million-scale dependencies. As a proof of concept,
we evaluate the performance of one essential component of this system, namely, the
distributed multi-head attention. We show that our algorithm can scale up attention
computation by almost 40using four GeForce RTX 4090 GPUs, compared to
vanilla multi-head attention mechanism. We believe this study is an instrumental
step towards modeling million-scale dependencies.
1 Introduction
Sequence models (e.g., LSTM [ 25], GRU [ 9]) can capture relationships among input tokens during
the learning process, where the tokens can be words, pixels, signals, etc. By utilizing the context
information, sequence models learn complex dependencies in training samples, which are referenced
during inference. Such in-context learning has demonstrated superior performance in many tasks (e.g.,
NLP [ 26], image processing [ 41]). However, traditional sequence models have two main drawbacks,
namely, slow training and forgetting long-range dependencies. The former is due to the sequential
nature of those models — tokens must be fed sequentially and in order. The latter is the result of their
limited model capacity (e.g., that of the hidden state in LSTM).
Ever since its conception, Transformers [ 40] have quickly taken over traditional sequence models for
their fast training and superior performance. The speedup was due to the use of positional encoding
[40,45]. This technique allows for parallel processing of input tokens without losing their information
related to their semantic ordering, by encoding token positions directly into the token embedding.
Positional encodings slightly nudge tokens in the feature space towards a direction based on their
positions in a sequence, without destroying the information encoded in their original embedding
vector space. It can also be applied to structured inputs [ 1] like images by using relative positional
information [ 43], similar to convolutional kernels. Furthermore, their great performance can be
attributed to the attention mechanism, or more speciﬁcally, multi-head self/cross-attention. Attention
mechanism explicitly learns the dependencies in input by computing pairwise attention scores among
for all combinations of tokens. The attention scores can be computed in many ways [ 39] and the most
straightforward, yet very effective, method is the dot product, which measures the distance between
two embedding vectors. This mechanism greatly improves the performance of sequence modeling
and is necessary for certain tasks to be feasible, for example, audio and video processing.
Despite their great advantages, Transformers have critical disadvantages as well, and chief among
them is their resource efﬁciency. Although Transformers are much faster compared to traditional
sequence models, the attention matrix incurs a O(L2)complexity, where Lis the context length,
in terms of both compute and storage. Such a complexity is especially prohibitive when dealing
with context of thousands of tokens since the time and memory it takes scale quadratically, for
example, summarizing books, processing audio/video, dealing with high-resolution images, etc.
In these tasks, the context can easily scale to thousands or even millions of tokens as making a
decision may need information from far earlier steps in the sequence (e.g., a name in the ﬁrst chapter
of a book). To counter this challenge, over the past ﬁve years a great deal of studies has focused
on making Transformers more efﬁcient. The proposed methods include (but not limited to) for
2

--- PAGE 3 ---
example, approximating the attention matrix with sparsity [ 24,3,46], clustering before computing
attention [ 35,29], making assumptions via conditional probability [ 34], low-rank estimation [ 42],
better memory I/O [ 12], matrix orthogonality and associativity [ 7], etc. Apart from tackling the
efﬁciency problem of Transformers directly, many other models have been proposed to cater the need
for long-context learning. To name a few, MLP-Mixer [ 37], FNet [ 30] and SGConv [ 31] learn from
and (solely) utilize the token-mixing paradigm from Transformers; Memorizing Transformers [ 44]
take Neural Turing Machines [ 17] to extreme and make Transformers a huge LSTM-like structure; S4
[20,21] rejuvenates traditional State Space Models combined with orthogonal polynomial projection
to reconstruct histories.
Acknowledging the signiﬁcance in both the novelty and volume of prior work, we take a step back
and compile this work with the following objectives:
1.Categorizing existing solutions to long-range dependency problems purely by their mathe-
matical formulations.
2.Comparing various methods using a uniﬁed template, with which we study the tradeoffs in
capturing global and local dependencies in input sequence.
3.Empirically evaluating the impact of context length on sequence modeling in experiments
with state-of-the-art models.
4.Proposing a machine learning system for learning million-scale dependencies, aiming to
strike a balance between model quality and resource efﬁciency.
5.Designing and testing the feasibility of a distributed algorithm for computing the attention
matrix for sequences of millions of tokens.
2 Problem Formulation
In general, existing attention-like methods take a sequence Xof lengthLand dimension Das an
input. These methods introduce inductive bias (i.e., dependencies) to augment the original feature
space ofXthrough a “mixing” process Pthat is either parameterized by learned with various
techniques (Eq. 1) or ﬁxed using certain procedures, e.g., Fourier Transform (Eq. 2).
P(Xj) :X7!eX (1)
P(X) :X7!eX (2)
Moreover, we further categorize Pinto the following four paradigms:
1.P(j)? ?X: Learned mixing independent of the input, e.g., simple convolution (§3) and
MLP-Mixer (§5).
2.P(j)? ?X: Learned mixing dependent on the input, e.g., self-attention (§4).
3.P()? ?X: Fixed mixing independent of the input, e.g., FNet (§6).
4.P()? ?X: Fixed mixing dependent on the input, e.g., State Space Model with ﬁxed
transition matrices (§7).
3 Convolution on Signals
First of all, we draw parallels between convolutions and the attention mechanism. Without the loss
of generality, we assume that the input has been linearized to 1D (e.g., [ 14,38,22]). For simplicity,
we only consider depthwise convolution (without pointwise convolution), i.e., XL1. Given a
ﬁlter/kernelfof window size K, the representation Ytoftth token resulting from the convolution on
signalgis a weighted average over the input:
Yt:= (fwgX)(t) =K 1X
k:=0~ wkX k+t(depthwise) : (3)
In addition, the following properties of convolution will be used in later derivations.
3

--- PAGE 4 ---
• Commutativity: Eq. 3 can be rewritten as Yt=PK 1
k:=0~ w k+tXk[11].
• Summation distributivity:P(fg)PfPg.
• Convolution Theorem: fgFT(fg).
A convenient way of viewing the convolution on the entire sequence Xis to formulate Eq. 3 as
weighing the input with structured weight matrices:
Z:=fWgX=2
666666666666666664w10 0 0
w2w10
w2w1
w2
wk
0wk 0
0wkw1
0w2
0 0 0wk3
7777777777777777752
4Xt3
5
=WCX (4)
=Pconv(Xjconv)
=eXconv; (5)
whereconv=fWCg.
Observe that (1) Pconvislearned butindependent of the input sequence, since Xdoes not enter conv,
and (2) the window size Kisﬁxed across all input signals. Consequently, ﬁrst few layers of a CNN
have limited local views of the input sequence, and only by stacking kernels can enlarge the receptive
ﬁelds of the higher layers.
The idea of stacking kernels to expand the receptive ﬁelds gives rise to a host of methods (e.g.,
[42, 3, 7]) to approximate the full-attention matrix, which are analogues of Eq. 4.
4 Self-Attention
Similar to convolution discussed in §3, the attention mechanism can also be viewed as a weighted
average over the raw embeddings of the input tokens. For simplicity, masking and scaling are
excluded.
4.1 Attention as Weighted Average
Firstly, three matrices of the same size (self-attention) are obtained by linearly projecting the same
inputXthree times with learnable weight matrices via MLP layers:
V:=XWV; K :=XWK; Q :=XWQ
The second step is to compute the attention scores for each token in the sequence at position t:
A0
t:=QtK>: (6)
Then, the attention scores are normalized row-wise using softmax :
At;i:=expA0
t;iPL 1
j=0expA0
t;j:
Lastly, the input projections Vis weighted by the score to induce biases/context, producing the ﬁnal
representation of token t:
Zt:=AtV:
4

--- PAGE 5 ---
Putting steps together, we arrive at the weighted average similar to Eq. 3:
Zt:=L 1X
j=0softmax
QtK>	
|{z}
full attention weightsVi: (7)
4.2 Attention as Token Mixing
Different from Eq. 3, here the input sequence Xdirectly enters the weights in Eq. 7. For simplicity,
from now on, we omit any kinds of normalization such as softmax , layer/batch norm, etc.
Then, we can rewrite Eq. 7 to:
ZLD: =QtK>
|{z}
A0(Eq:6)V
=
(XWQ)(XWK)>
XWV
=
X(WQWK>)X>
XWV
=
XGWX>
XWV(8)
=A0X WV(9)
=Pattn(Xjattn)
=eXattn;
whereattn=fA0;WVg, and Eq. 8 summarizes the product between the two learned weight matrices
into their Gram matrix GW
DD.
As a result, we obtain a new representation eXattnof the input sequence by learning a mixing scheme
over the raw tokens using self-attention (Eq. 9). In other words, Pattnislearned anddependent on
the input since Xenters the equaltion via attn.
4.3 Using Static Kernel or Feature Map
The computational complexity of Eq. 9 is in O(L2D)O(L2)due to the product between the
(unnormalized) full-attention matrix A0
LLand the input sequence XLD.
Similar to Eq. 8, we can regard A0as parameterized Gram matrix of the input space GX, since it only
depends on the tokens. In other words, the full attention could be expressible by a kernel function .
Speciﬁcally, we rewrite Eq. 8 as:
Z= [ (XGWX>)|{z}
A0: parameterized GX]XWV
uk(X;X )X; (10)
=(X)(X)>X (11)
where kis a kernel function over the input tokens, and is the equivalent feature map. For WV,
Tsai et al. [39] have shown that this linear projection is redundant and can lead to performance
degradation.
Instead of learning three projections and computing the full-attention matrix, we could potentially
learn or use a static kernel function k()to capture the correlations between tokens. Alternatively,
sinceLD, applying a feature map ()should be much cheaper than using kernel functions.
5 Sequence Modeling with Multilayer Perceptrons
Tolstikhin et al. [37] is the ﬁrst to suggest that full-attention can be replaced by learned token mixing
by only using multilayer perceptrons (MLPs), namely, MLP-Mixer (Mixer). For simplicity, common
tricks like layer norms and skip connections are omitted here.
5

--- PAGE 6 ---
Mixer was initially intended for imaging tasks [ 37], so the input tokens are sequentialized image
patches. However, this scheme can be generalized to any sequence-to-sequence tasks [ 30]. In Mixer,
all self-attention layers in the Transformer architecture are replaced by MLP layers, each of which
conducts two mixing operations on the channel (embedding) and the patch (sequence) dimension
respectively:
X0
:t:=Wp1 
Wp1X:t
(token mixing) (12)
Z:t:= 
X0
t:Wc1
Wc2(channel mixing) (13)
Combining Eq. 12 and 13, we have:
ZLD: = (Wp2Wp1X)Wc1Wc2
= (Wp2Wp1)X(Wc1Wc2)
=WpX Wc
=Pmlp(Xjmlp) (14)
=eXmlp;
wheremlp=fWp
LL;Wc
DDg, which are the weights learned with GELU during token and channel
mixing respectively. Although Pmlpis not static, it is independent of the input sequence since Xdoes
not entermlp.
6 Replacing Attention with Fourier Transform
Similar to MLP-Mixer, Lee-Thorp et al. [30] proposed a new mixing scheme that replaces the
learned, expensive full-attention by a series of ﬁxed, efﬁcient Discrete Fourier Transforms (DFTs).
For simplicity, common tricks like layer norms and skip connections are omitted.
6.1 Weighting Sequence using Twiddle Factors
All self-attention layers in the Transformer architecture are substituted by Fourier layers. Each Fourier
layer conducts a two DFT: ﬁrst over the embedding dimension D(Eq. 15) and then over the sequence
dimensionLof the input tokens (Eq. 16).
Zt;j:=D 1X
d=0exp2i
Ddj
Xt;d (rst DFT) (15)
eXt;j:=Re(L 1X
k=0exp2i
Ljk
Zt;k)
(second DFT) (16)
Both Eq. 15 and 16 are in the form of weighted average similar to Eq. 3 and 7.
6.2 Mixing Tokens with Fourier Transform
The weighted average forms of the two DFT can be rewritten using corresponding Vandermonde
matrices for the roots of unity up to a normalization factor:
Fh:=1p
D2
666664w0w0w0w0
w0w1w2w(D 1)
w0w2w4w2(D 1)
...............
w0w(D 1)w2(D 1)w(D 1)(D 1)3
777775
Fs:=1p
L2
666664w0w0w0w0
w0w1w2w(L 1)
w0w2w4w2(L 1)
...............
w0w(L 1)w2(L 1)w(L 1)(L 1)3
777775;
6

--- PAGE 7 ---
wherew= expf 2ig.
WithFhandFs, we simplify Eq. 15 and 16 to:
X0
t::=XtFh(rst DFT) (17)
Z:t:=RefFsX0
:tg (second DFT) (18)
By combining Eq. 17 and 18, we have:
ZLD: =Re
FsX Fh	
(19)
=PFT(X) (20)
=eXFT;
From Eq. 19, we have the following observations. First, DFT results in a token mixing eXFTin the
same formulation as that of the attention mixing eXattnfrom Eq. 9. However, PFTis static ( not
learned) and independent of the input sequence (Eq. 10). Secondly, by using Fast Fourier Transform
(Cooley–Tukey algorithm [ 10,16]), the computational cost is in O(LlogL), with much smaller
space complexity (since the symmetric Vandermonde matrices can be computed/stored efﬁciently),
compared to that of the full-attention O(L2). Furthermore, the order in which the two DFTs are
applied does not matter. Lastly, different from mixing schemes using MLP layers, stacking FT layers
is analogous to switching between the “time” and frequency domain.
7 Sequence Modeling with State Space Models
A different branch of mixing scheme was proposed by Gu et al. [18], employing and augmenting
traditional State Space Models (SSMs) from control theory. Speciﬁcally, the authors employ linear
time-invariant (LTI) SSM parameterized by a set of structured matrices to summarize history and
memorize long-range dependencies.
7.1 High-order Polynomial Projection
The starting point of this line of work is the idea of using orthogonal, high-order polynomial projection
(HiPPO) to summarize a theoretically optimal representation of allthe past tokens [ 18]. The HiPPO
operator is a two-step transform: (1) it ﬁrst takes a one-dimensional input signal up to time t, projects
it onto orthogonal basis polynomials of order Nwith either uniform (Legendre) or exponentially
decaying (Laguerre) weights, and (2) it then extracts the coefﬁcients of the basis polynomials as the
best representation of the past until the current point in time:
hippo (XL1j<t) =coeffproj?(X)gdimension expansion            ! x2RLN; (21)
wherexis a matrix that contains allthe system states of the SSM prior to time t. In other words, row
tof the state matrix, x(t)>2R1N, summarizes the history of the input sequence before time t.
By leveraging a specially structured state transition matrices AandB, integrating the following
ordinary differential equation (ODE) of the SSM demonstrates SoTA results in summarizing and, in
turn, reconstructing the history of the input signal [18]:
_x(t) :=Ax(t) +Bu(t); (22)
where the system input u(t)is a single token of the one-dimensional signal, i.e.,Xt. Note thatAand
Bare ﬁxed, constant matrices when the method was ﬁrst proposed [ 18]. However, these matrices
can be learned through backpropagation, although its performance gain is not signiﬁcant [ 19]. This
learning process also incurs large overhead, especially for modeling high-dimensional feature space,
which is improved through various mathematically techniques in follow-up work [20, 21].
7.2 Multidimensional Projection
HiPPO has major two limitations: (1) The input signal is restricted to one dimension, and (2) Matrices
AandBare not learned. Gu et al. [19] developed Linear State-Space Layers (LSSLs) to address the
two challenges.
7

--- PAGE 8 ---
To work with multidimensional input, LSSL applies HiPPO independently on each embedding
dimensiondof the input sequence and concatenates the Dseries of outputs as the representation of
SSM at all times2:
_xd(t)N1:=Axd(t) +Bud(t)11 (23)
yd(t)11:=Cxd(t) +Dud(t)11:
More generally, for a sequence of length L, for allt2L, we have:
_xd
NL:=Axd+B(~ ud)>
1L
(~ yd)>
1L:=Cxd+D(~ ud)>
1L;
where the (~ ud)is one column of the input signal XLD, and the matrices A;B;C andDare all
learnable via backpropagation (through time).
Therefore, at any point in time t, the matrixx(t)2RNDcontains all the internal system states of the
SSM, i.e., the coefﬁcients of the orthogonal polynomials. Further, the tensor x2RNDLcontains
all the systems states for t2[0;L]. For completeness, the update step size tis also learnable
for discretization using Bilinear transform (empirically more performant than Euler method), i.e.,
xd(t)  !xd(t+t).
Although the parameter matrices and the step size are learnable, the training process is computationally
prohibitive , in part due to the expensive dimension expansion, i.e., the orthogonal polynomials.
Consequently, they are ﬁxed in practice [19].
To reduce system costs and make the training more efﬁcient, special tricks and parameterization have
been developed, e.g., S4 [20], S4D [21].
7.3 Convolutional View of SSM
Any LTI dynamic system can be viewed as the convolution between the input signal and its impulse
response function, and so does the SSM described by Eq. 23. For simplicity, we drop the dimension
dand the matrix D(skip connection) in the following derivation. (The recurrent representation of
SMM is theoretically interesting but practically infeasible due to its sequential nature, so it is not
considered here.)
2According to their implementation: https://github.com/HazyResearch/state-spaces/blob/
main/src/models/s4/lssl.py .
8

--- PAGE 9 ---
_x(t) : =Ax(t) +Bu(t)
_x(t) Ax(t) =Bu(t)
e tA_x(t) Ae tAx(t) =e tABu(t)
1
dt
e tAx(t)
=e tABu(t)
Zt
01
d
e Ax()
d=Zt
0e ABu()d
e tAx(t) x(0) =Zt
0e ABu()d
x(t) =etAx(0) +etAZt
0e ABu()d
=etAx(0) +Zt
0et ABu()d
=etAx(0) +Zt
0etAB|{z}
basis functionu(t )d (24)
=etAx(0) +Zt
0h(t)u(t )d
=etAx(0) + (hu)(t); (25)
where Eq. 24 is from the commutativity of the convolution, and h(t)is the unit impulse response
function of the SMM.
Substituting Eq. 25 into output equation yields:
y(t) : =Ccoeffproj?(X:d)g(t)
=Cx(t)
=C
etAx(0) + (hu)(t)
=Pssm(X:djssm)
=eXssm;
wheressm=fA;B;Cg.
Hence, the resulting representation of the input sequence is a linear combination of the impulse
response function.
Observe that (1) Pssmdepends onX, which enters ssmviaA, and (2)ssmcan either be learned or
beﬁxed as specially structured matrices (the performance difference is not signiﬁcant while ﬁxing
ssmis much more efﬁcient).
7.4 More Efﬁcient Polynomial Projection
As discussed in §7.2, one key bottleneck is the dimension expansion as SSM requires a tensor of
x2RNDLto represent the entire system state. One idea could be to use Product Quantization
[27] by ﬁrst partitioning the input sequence into subspaces and learning a smaller prototype (an
approximation) within each subspace during the initial pass of the training set . Then, project entire
subspaces onto orthogonal polynomial basis functions, instead of doing it for every column/dimension.
Such a method can be combined with (tree-based) Locality Sensitive Hashing, which together could
result in zero matmul operations during inference time [5].
9

--- PAGE 10 ---
8 Mixing Tokens with Convolution
Recently, Li et al. [31] proposed a pure convolutional architecture in place of the full attention block,
achieving both lower system costs (15–50% faster) and the same/higher model quality.
It employs concatenated parameter sets with decaying weights and applies three convolutions using
real-valued DFT (1) on the concatenated parameter sets, (2) on the input sequence, and (3) between
the two (inverse transform). In turn, it can be regarded as a learnable, weighted FNet . As a result, its
time complexity is in O(LlogL), same as FNet.
8.1 Memory Cost of SGConv
Although the authors brieﬂy mentioned that the memory complexity is also in O(LlogL), we show
here that it is likely to be in O(L+ logL)instead.
Given an input sequence XLDand kernel dimension k, the number of kernels sis calculated as the
following:
s:=dlog2(L=k)e+ 1:
Consequently, we need ssets of learnable parameters, each of which corresponds to one kernel:
WK:=n
W(1)
kD;W(2)
kD;:::;W(s)
kDo
:
WKrequires (skD)2O(logL)space in total.
With the above parameters, SGConv instantiates a kernel Kconcatenated from ssub-kernels for
every forward pass :
K(i):=Interpolaten
W(i)o
2R(k2(i 1))D
K:=concatn
K(1);K(2);:::;K(s)o
2RL0D;
whereL0=sP
i=1k2(i 1)+k=L+L.
Hence, the kernel KtakesO(L)space. Since one layer of SGConv needs one set of parameters WK
and a concatenated kernel K, it requiresO(L+ logL)memory per layer.
This space complexity is smaller than that of S4 ( O(L+N)whereN= 256 ) [20], and commensurate
with existing attention approximations, e.g., Reformer ( O(L+ logL)) [29] and Performer ( O(L))
[7].
9 Impact of Context Length
In this section, we conduct experiments to investigate the impact of varying context lengths in
sequence modeling. One of the applications of long-context learning is on high-resolution images,
e.g., fMRI, and satellite images. Therefore, we ﬁrst study the behaviors of Vision Transformers (ViT)
[14] when varying the sequence length. To this end, we pretrain a base ViT on ImageNet (21k+) and
ﬁne-tune it on four datasets: CIFAR 10, CIFAR 100, EuroSAT [ 23], and So2Sat [ 49]. The latter two
are satellite datasets of higher resolution. The downstream task for all four datasets is classiﬁcation.
ViT partitions an image into patches and treats each patch as a token. Therefore, the smaller the
patches are, the longer the context the model gets access to in each batch. Note that, although the
dependencies between patches are preserved among patches, the structural information within a patch
is destroyed as it is serialized during embedding. We thus expect better performance when using
smaller patch sizes since more structural information is kept. Also, this performance increase is
expected to be task-dependent, for example, predicting a dog needs far fewer details than identifying
the type of vehicle in a satellite image. We repeat the experiments for six runs on the ETH Euler
10

--- PAGE 11 ---
Note that the
y-axes do not
start from zero.
10 15 20 25 300.9820.9830.9840.9850.9860.987Accuracydataset: cifar10
10 15 20 25 300.8920.8950.8970.9000.9020.9050.907dataset: cifar100
10 15 20 25 30
Patch Size0.9840.9850.9860.9870.9880.9890.990Accuracydataset: eurosat
10 15 20 25 30
Patch Size0.8000.8050.8100.8150.8200.8250.830dataset: so2sat
Figure 1: Performance of pretrained ViT model on four datasets when varying the patch size of the
image. The smaller the patch size, the longer the context length the model gets access to in each
batch.
cluster with two GeForce RTX 3090 GPUs. As expected, model quality does increase as the patch
size gets smaller (Fig. 1), and the magnitudes of such increase differ by task. However, we notice
that performance improvement is not signiﬁcant in this experiment, and the trends tend to plateau at
the end. Therefore further investigations with different tasks and more ﬁnd-grained increments of
context length are needed.
To avoid being constrained by the ﬁxed architecture of pretrained models, we write a vanilla Trans-
former for the following experiments so that we can vary the sequence length at will. The Transformer
model consists of three layers of encoder blocks and one MLP layer as the prediction head, the
decoder. We implement the positional embedding described in the original work [ 40] and keep it
autoregressive by implementing attention masks. The model is trained on the WikiText-103 dataset
from scratch using one NVIDIA A100 GPU. The dataset contains 103M tokens and has a vocabulary
size of 98K. The downstream task is language modeling, and we use perplexity as the performance
metric. This time we vary sequence length from 8 to 10K with log2 step sizes. Similar to experiments
conducted by Wu et al. [44], we keep the number of tokens per batch constant ( 214) while sweeping
the sequence length. To achieve this, we dynamically adjust the batch size length given the sequence
length. By doing so, the model gets access to different context lengths while still seeing the same
total number of tokens in each batch .
The average length of the articles in the WiKiText-103 dataset is 3.6K words [ 2], which is expected
to be the ideal context length for this task. However, the model reaches its peak performance with
a sequence length around 128-512 (Fig. 2), because this sequence length has reached the capacity
11

--- PAGE 12 ---
10000 20000 30000 40000
Step200300400500600700800900Training PerplexitySequence Length
8
32
128
512
1K
2K
4K
6K
8K
10K(a)
1 2 3 4 5 6
Epoch100150200250300Validation Perplexity
(b)
Figure 2: Training (a) and validation (b) loss of the vanilla Transformer model on WiKiText-103 with
different context lengths.
832128 256 512 1K 2K 4K 6K 8K10K
Sequence Length300350400450500Batch Duration [ms]
Figure 3: Per-batch training time of the vanilla Transformer model on WiKiText-103.
of traditional Transformer models (e.g., BERT [ 13], GPT-2 [ 33]). In addition, we observe that the
training time decreases with the sequence length (Fig. 3). Since we keep the total number of tokens
per batch constant, this result illustrates that doing multiple small attention passes on same number
of tokens is more expensive than doing one larger pass to compute (bigger) attention matrices over
more tokens. This tradeoff forms a constrained optimization problem with the performance being the
objective and runtime being the constraint.
10 Architecture for Million-Scale Dependencies
In previous sections, we demonstrated that existing long-context models can be viewed as various
token-mixing schemes. These schemes strive to make every token available to any other tokens within
the context window and then, compute the weighted average of the context (together with the original
embedding) to achieve inductive bias. We have also demonstrated the tradeoff between performance
and efﬁciency when varying the context length. Table 1 summarizes and compares on a macro level
the existing solutions to long-context learning problem, in terms of the maximum context length
each model can handle. Note that it demonstrates only the feasibility of working with such context
length (i.e., constrained by the model architecture, and in turn, system resources), but not the model
quality . In general, as the context get larger, model performance ﬁrst increases (due to the access to
more context) and then drops quickly as the length exceeds the model capacity [ 2,44,20,18]. Thus,
although the optimization landscape seems to be near-exhausted, million-scale context still appears
to be the pinch point thereof.
12

--- PAGE 13 ---
This summary
table is not
exhaustive.Max. Context Length Solution Category
512–2K Full attention (e.g.,[13, 40])
65K Approximated attention (e.g., [7, 42, 3]); Memory I/O optimization [12]
264K Neural Turing Machine with multi-head attention [44]
1M* Token mixing (e.g., [37, 30, 31]); State Space Models (e.g., [18, 20, 21])
Table 1: Categorization and comparison between existing solutions in terms of maximum context
length. *The 1M context length is achieved on toy examples (e.g., memorizing random sequences),
but not with real-world downstream tasks.
10.1 Huge Sparse Models with Conditional Computation
Sparse models using conditional computation [ 4] have (re)emerged as a promising direction towards
huge model capacity, while keeping efﬁciency costs at bay. The model that has attracted the most
attention recently is the sparsely-gated Mixture of Experts (MoE) [ 36]. It has shown to be able to
scale to billions of parameters, while incurring a fraction of the costs of utilizing their full capacity
(e.g., [ 15,8,48]). Such results are due to the fact that these models are only partially activated at
any time. This partial activation brings about one crucial beneﬁt, namely, model specialization —
different parts of the model specialize at different tasks. It has been shown that different experts in
the sparse MoE model are highly specialized in terms of syntax and/or semantics in NLP tasks [ 36].
Furthermore, the component that is responsible for sparsely activating the model is the “router” that
selects expert(s) and forwards tokens to them. In turn, this routing operation will activate only part of
the model represented by the expert(s). This router is typically made of MLP layers and learns to
which expert(s) to forward which tokens. Most importantly, it has demonstrated a ﬁltering/pruning
effect — dropping redundant tokens while maintaining model performance [ 15,48]. This feature
allows the model to focus only on the most relevant part of an example (e.g., the delineation of a dog
in an image) and ignore the relatively unimportant parts (e.g., the background and other objects in the
image), which we call concentrated learning.
10.2 Learning Million-Scale Dependencies using Sparse Models
The key tradeoff when dealing with long-range dependencies lies between resource efﬁciency and
the amount of long/short-term information kept in memory, which dictates the learning paradigm
and model capacity. In the ideal case, model would store all history (and the future dependencies if
not autoregressive) and make decisions accordingly. For million-scale dependencies, such an ideal
scenario is not realistic for two reasons: (1) it would induce a complexity at least as large as the
amount of information stored in terms of both space and compute, and (2) the memorized information
becomes increasingly stale as the learning process goes since what is stored is the latent space rather
than the raw tokens [44].
Although sparse MoE models are not speciﬁcally designed for modeling long-range dependencies,
we believe they are a good ﬁt for the following reasons. First, specialization allows for increasing
model capacity while keeping efﬁciency cost (training/inference time) relatively constant. Second,
concentration makes the attention mechanism selective, i.e., only memorizing relevant information
as opposed to all historical (and future) data. Therefore, we propose an architecture based on sparse
MoE (Fig. 4).
Inspired by the routing mechanism in sparse MoE, we introduce the Selector to ﬁltering tokens
before computing attention. This process aims to help the model concentrate on relevant parts of the
samples, which also reduces the resource usage from the get-go. Then, the selected tokens are passed
through a distributed MoE layers, within which each expert resides on one device and handles part of
the sequence. Such a partitioning allows experts to compute full-attention matrices for the tokens
they receive and specialize at speciﬁc parts of the task. Similar to the Router s in the MoE layers,
theSelector can also be trained with the help of auxiliary losses that is added to the training loss
during backpropagation. The processed tokens are reordered and aggregated after being processed
by the MoE layers. Additionally, a global mixing by means of fast Fourier Transform for wider
context accessibility and an up-sampling processing through interpolation ( kNN or linear) to restore
13

--- PAGE 14 ---
XL×D (where L > 1M)Selector  sO(L)
X'L'×D 
(where L' < L)(Global Mask)Reorder
+
Aggr egate
(concat/avg)Global Mixing
(FFT)O(L' log L')
Interpolate
(up sampling)O(L)
(optional)
ZL×D        MoE LayersConcentration Specialization
Router
(top-2)Expert 1
(GPU 1)O(2L'/4 × L') + communication
O(L')Attn FFN
Expert 2
(GPU 2)Attn FFN
Expert 3
(GPU 3)Attn FFN
Expert 4
(GPU 4)Attn FFN
 broadcasts  X' (optional)Figure 4: System architecture of the proposed sparse model for million-scale dependencies.
the dimensionality of the embedding could be applied. The theoretical complexity is asymptotically
linear (and worst-case log linear) to the sequence length.
10.3 Objective Functions
Although the Selector can be trained with auxiliary losses similar to the Router s [36,8,15] in the
MoE blocks, it is not the preferred option since these losses are often intuition-based and generally
hard to evaluate their effectiveness. Instead, we aim to train the model end-to-end, without singling
out the Selector . Additionally, since the main goal of the Selector is to learn to prone/ﬁlter out
unimportant tokens (e.g., the background, other objects in the image), a threshold parameter as a
“nob” is useful for controlling the dropout rate, that is:
s ;:XLD7!X0
L0D; (26)
whereis the pruning threshold, and LL0.
With the selector deﬁned, the objective reads:
max
; P 
~ yjZ=1
MMX
i=1gif(s ;(X))!
; (27)
whereMis the number of ensembled experts, and giis the gating weight.
Alternatively, the objective (Eq. 27) can be in an adversarial form. Miladinovi ´c et al. [32] demon-
strated the potential of learning a dropout model via a GAN-like formulation. We modify the objective
in an analogous way by creating a max-min game between the Selector and the predictor:
max
min
 P 
~ yjZ=1
MMX
i=1gif(s ;(X))!
: (28)
Speciﬁcally, the Selector (adversary) tries to drop as many informative tokens as possible, whereas
the predictor still aims for higher likelihood. However, our preliminary experiments show that
this adversarial objective often yields a too powerful Selector that the predictive part needs to be
retrained with sbeing frozen.
10.4 Distributed Attention
One critical element of the proposed architecture (Fig. 4) is the distributed computation of the
attention matrix. In standard attention computation, the entire matrix is produced on a single device
14

--- PAGE 15 ---
X12'288QKVHead 13*128QKVHead 23*128QKVHead 963*1283 * 12'288 = 36'864204812'288QKVHead 13*128QKVHead 23*128QKVHead 963*1282048
QK20482048128TSVZ2048128𝐬𝐨𝐟𝐭𝐦𝐚𝐱𝟏√𝒅𝒌𝑸𝑲𝑻𝑽=𝒁for each head compute the attention
ZZZ204812896 headsconcatenateZZZZ2048128 * 96 = 12'288𝑊#X12'28812'28812'2882048Figure 5: Standard multi-head attention with all sizes taken from OPT-175B [ 47] and GPT-3 175B
[6] models.
(Fig. 5). However, for million-scale context, the sequence dimension is so large that the input matrix
Xcannot ﬁt in a single device memory and thus has to be distributed. For example, in Fig. 7, we
assume the sequence dimension corresponds to the number of pixels in a 512px512px=262’144px
image. Taking all other dimensions from OPT-175B [ 47] and GPT-3 175B [ 6] models,Xhas to be
distributed among NGPUs within the same node (in this case, N= 4).
The distributed algorithm works as follows. Firstly, we split the design matrix Xalong the sequence
dimension into NpartitionsfPigN. Then, we replicate the parameter matrix along the unchanged
dimension on alldevices. Each device computes the attention matrix for all attention heads but only
with the samples in their own partitions. Next, we use COSTA [ 28] to efﬁciently shufﬂe partitions so
that each device has the attention scores for allsamples but with only 1=Nheads. This step prepares
for the softmax calculation that requires all examples for each embedding dimension, and it can be
computed sequentially by each device. Lastly, we use COSTA to reshufﬂe the data for the second
time to compute the ﬁnal linear projection with W0replicated on each device. We implement this
algorithm with NCCL MPI API from NVIDIA in Python.
To test the feasibility of this algorithm, we scale the length of the input sequence of the attention
computation on four GeForce RTX 3090 GPUs (Fig. 6). As a result, with four GPUs, the algorithm
can scale the attention computation to a sequence length of almost 80K, which is 40 the maximum
length a vanilla attention implementation can handle, with a near-linear growth in time complexity.
15

--- PAGE 16 ---
010'00020’00030’00040’00050’00060’00070’000
Sequence Length0.000.050.100.150.200.250.300.350.40Time [s]
float32
float16
← Max. of vanilla attention (2K)Figure 6: Forward pass performance of the distributed attention algorithm on four GeForce RTX
3090 GPUs. Note that, without distributing the computation, the maximum sequence length that the
vanilla attention implementation can handle is around 2K.
11 Conclusion
In this work, we ﬁrst categorize and compare existing solutions to long-sequence modeling. By
formulating them in a uniﬁed template mathematically, we pinpoint the nature shared among most
prior works: making both global and local context available when computing attention scores through
various token mixing schemes. Next, we highlight the tradeoff between resource efﬁciency and
the amount of memorized long/short-term history in such mixing schemes. To model million-scale
dependencies while keeping resource usage at bay, we then propose a distributed learning system
inspired by recently proposed sparse MoE models of huge capacity, aiming to exploit two main
features thereof: model specialization and concentrated learning. As the ﬁrst step towards building
this system, we propose a distributed algorithm for computing attention matrices for million-scale
sequences. We demonstrate in experiment that our algorithm can scale the attention computation by
almost 40in terms of maximum sequence length with a near-linear time complexity, compared to
that of vanilla attention implementation. Although there is still much work to be done, we believe
that this work is an instrumental step towards modeling million-scale dependencies.
16

--- PAGE 17 ---
𝑃!12'288
QKVHead 13*128QKVHead 23*128QKVHead 963*1283 * 128 * 96 = 36'864QKV
Head 13*128
𝑃"𝑃#𝑃$replicated on all processessplit among all processesQKV3*128
QKV3*128𝑃!𝑃"𝑃#𝑃$Head 2Head 96QKV3*128
QKV3*128
QKV3*128
𝑃!𝑃"𝑃#𝑃$Each process handles each of 96/4=24 heads sequentially
Qreshuffle (COSTA)
KSVVVSequentially processed due to memory constraintsSZZZ262'144
262'144128128
Z128
262'144ZZ
24 heads𝑃"ZZZ
24 heads𝑃#24 * 128 = 30723072 * 4 = 12'288
reshuffle (COSTA)
Z12'288
262'144𝑃!𝑃"𝑃#𝑃$𝑊%replicated on all processes12'28812'288𝑃!12'28865'536𝑃"𝑃#𝑃$65'53665'53665'53624 * 128 = 3072each process computes the attention3 * 128 * 96 = 36'8643 * 128 * 96 = 36'864Figure 7: Distributed multi-head attention with the sequence size being scaled, and all the other sizes
are taken from OPT-175B [47] and GPT-3 175B [6] models.
17

--- PAGE 18 ---
References
[1]J. Ainslie, S. Ontanon, C. Alberti, V . Cvicek, Z. Fisher, P. Pham, A. Ravula, S. Sanghai, Q. Wang, and
L. Yang. Etc: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483 ,
2020.
[2]H. Bai, P. Shi, J. Lin, Y . Xie, L. Tan, K. Xiong, W. Gao, and M. Li. Segatron: Segment-aware transformer
for language modeling and understanding. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,
volume 35, pages 12526–12534, 2021.
[3]I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
[4]E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup. Conditional computation in neural networks for faster
models. arXiv preprint arXiv:1511.06297 , 2015.
[5]D. Blalock and J. Guttag. Multiplying matrices without multiplying. In International Conference on
Machine Learning , pages 992–1004. PMLR, 2021.
[6]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. Advances in neural information processing
systems , 33:1877–1901, 2020.
[7]K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohi-
uddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794 , 2020.
[8]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,
S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 ,
2022.
[9]J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empirical evaluation of gated recurrent neural networks on
sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.
[10] J. W. Cooley and J. W. Tukey. An algorithm for the machine calculation of complex fourier series.
Mathematics of computation , 19(90):297–301, 1965.
[11] Z. Dai, H. Liu, Q. V . Le, and M. Tan. Coatnet: Marrying convolution and attention for all data sizes.
Advances in Neural Information Processing Systems , 34:3965–3977, 2021.
[12] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efﬁcient exact attention
with IO-awareness. In Advances in Neural Information Processing Systems , 2022.
[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
derer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 , 2020.
[15] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple
and efﬁcient sparsity. J. Mach. Learn. Res , 23:1–40, 2021.
[16] M. Frigo and S. G. Johnson. The design and implementation of fftw3. Proceedings of the IEEE , 93(2):
216–231, 2005.
[17] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401 , 2014.
[18] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré. Hippo: Recurrent memory with optimal polynomial
projections. Advances in neural information processing systems , 33, 2020.
[19] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Ré. Combining recurrent, convolutional,
and continuous-time models with linear state-space layers. Advances in neural information processing
systems , 34, 2021.
[20] A. Gu, K. Goel, and C. Ré. Efﬁciently modeling long sequences with structured state spaces. In The
International Conference on Learning Representations (ICLR) , 2022.
[21] A. Gu, A. Gupta, K. Goel, and C. Ré. On the parameterization and initialization of diagonal state space
models. arXiv preprint arXiv:2206.11893 , 2022.
18

--- PAGE 19 ---
[22] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang. Transformer in transformer. Advances in Neural
Information Processing Systems , 34:15908–15919, 2021.
[23] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A novel dataset and deep learning benchmark for
land use and land cover classiﬁcation. IEEE Journal of Selected Topics in Applied Earth Observations and
Remote Sensing , 2019.
[24] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans. Axial attention in multidimensional transformers.
arXiv preprint arXiv:1912.12180 , 2019.
[25] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780, 1997.
[26] Z. Huang, W. Xu, and K. Yu. Bidirectional lstm-crf models for sequence tagging. arXiv preprint
arXiv:1508.01991 , 2015.
[27] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE transactions
on pattern analysis and machine intelligence , 33(1):117–128, 2010.
[28] M. Kabi ´c, S. Pintarelli, A. Kozhevnikov, and J. VandeV ondele. Costa: Communication-optimal shufﬂe
and transpose algorithm with process relabeling. In High Performance Computing: 36th International
Conference, ISC High Performance 2021, Virtual Event, June 24–July 2, 2021, Proceedings 36 , pages
217–236. Springer, 2021.
[29] N. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efﬁcient transformer. arXiv preprint
arXiv:2001.04451 , 2020.
[30] J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv
preprint arXiv:2105.03824 , 2021.
[31] Y . Li, T. Cai, Y . Zhang, D. Chen, and D. Dey. What makes convolutional models great on long sequence
modeling?, 2022.
[32] Ð. Miladinovi ´c, K. Shridhar, K. Jain, M. B. Paulus, J. M. Buhmann, and C. Allen. Learning to drop out:
An adversarial approach to training sequence vaes. arXiv preprint arXiv:2209.12590 , 2022.
[33] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised
multitask learners. OpenAI blog , 1(8):9, 2019.
[34] H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, and B. Dai. Combiner: Full attention
transformer with sparse computation cost. Advances in Neural Information Processing Systems , 34:
22470–22482, 2021.
[35] A. Roy, M. Saffar, A. Vaswani, and D. Grangier. Efﬁcient content-based sparse attention with routing
transformers. Transactions of the Association for Computational Linguistics , 9:53–68, 2021.
[36] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural
networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.
[37] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner,
D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in Neural
Information Processing Systems , 34:24261–24272, 2021.
[38] H. Touvron, M. Cord, M. Douze, F. Massa, and A. Sablay-rolles. Hj egou,“training data-efﬁcient image
transformers & distillation through attention,”. arXiv preprint arXiv:2012.12877 , 2020.
[39] Y .-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, and R. Salakhutdinov. Transformer dissection: A uniﬁed
understanding of transformer’s attention via the lens of kernel. arXiv preprint arXiv:1908.11775 , 2019.
[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in neural information processing systems , 30, 2017.
[41] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and K. Saenko. Sequence to sequence-
video to text. In Proceedings of the IEEE international conference on computer vision , pages 4534–4542,
2015.
[42] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020.
19

--- PAGE 20 ---
[43] K. Wu, H. Peng, M. Chen, J. Fu, and H. Chao. Rethinking and improving relative position encoding for
vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages
10033–10041, 2021.
[44] Y . Wu, M. N. Rabe, D. Hutchins, and C. Szegedy. Memorizing transformers. arXiv preprint
arXiv:2203.08913 , 2022.
[45] R. Xu, X. Wang, K. Chen, B. Zhou, and C. C. Loy. Positional encoding as spatial inductive bias in
gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
13569–13578, 2021.
[46] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang,
L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing
systems , 33:17283–17297, 2020.
[47] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, et al.
Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.
[48] Y . Zhou, T. Lei, H. Liu, N. Du, Y . Huang, V . Zhao, A. Dai, Z. Chen, Q. Le, and J. Laudon. Mixture-of-
experts with expert choice routing. arXiv preprint arXiv:2202.09368 , 2022.
[49] X. Zhu, J. Hu, C. Qiu, Y . Shi, H. Bagheri, J. Kang, H. Li, L. Mou, G. Zhang, M. Häberle, S. Han,
Y . Hua, R. Huang, L. Hughes, Y . Sun, M. Schmitt, and Y . Wang. New: So2sat lcz42, 2019. URL
https://mediatum.ub.tum.de/1483140 .
20
