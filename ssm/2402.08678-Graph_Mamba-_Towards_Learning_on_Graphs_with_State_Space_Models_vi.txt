# Graph Mamba: Hướng tới Học trên Đồ thị với Mô hình Không gian Trạng thái
Ali Behrouz* 1Farnoosh Hashemi* 1

Tóm tắt
Mạng Nơ-ron Đồ thị (GNN) đã cho thấy tiềm năng đầy hứa hẹn trong việc học biểu diễn đồ thị. Phần lớn các GNN định nghĩa một cơ chế truyền thông điệp cục bộ, lan truyền thông tin qua đồ thị bằng cách xếp chồng nhiều lớp. Tuy nhiên, các phương pháp này được biết là gặp phải hai hạn chế chính: nén quá mức và khả năng nắm bắt kém các phụ thuộc tầm xa. Gần đây, Graph Transformers (GT) đã nổi lên như một giải pháp thay thế mạnh mẽ cho Mạng Nơ-ron Truyền Thông điệp (MPNN). Tuy nhiên, GT có chi phí tính toán bậc hai, thiếu thiên hướng quy nạp trên cấu trúc đồ thị, và phụ thuộc vào Mã hóa Vị trí/Cấu trúc (SE/PE) phức tạp. Trong bài báo này, chúng tôi chỉ ra rằng trong khi Transformers, truyền thông điệp phức tạp, và SE/PE là đủ cho hiệu suất tốt trong thực tế, không cái nào là cần thiết. Được thúc đẩy bởi thành công gần đây của Mô hình Không gian Trạng thái (SSM), như Mamba, chúng tôi trình bày Graph Mamba Networks (GMN), một khung tổng quát cho một lớp GNN mới dựa trên SSM chọn lọc. Chúng tôi thảo luận và phân loại các thử thách mới khi thích ứng SSM với dữ liệu có cấu trúc đồ thị, và trình bày bốn bước bắt buộc và một bước tùy chọn để thiết kế GMN, trong đó chúng tôi chọn (1) Token hóa Láng giềng, (2) Sắp xếp Token, (3) Kiến trúc Bộ mã hóa SSM Chọn lọc Hai chiều, (4) Mã hóa Cục bộ, và có thể bỏ qua (5) PE và SE. Chúng tôi cũng cung cấp lý giải lý thuyết cho sức mạnh của GMN. Các thí nghiệm chứng minh rằng mặc dù có chi phí tính toán thấp hơn nhiều, GMN đạt được hiệu suất vượt trội trong các bộ dữ liệu điểm chuẩn tầm xa, quy mô nhỏ, quy mô lớn, và dị thể. Mã nguồn có tại liên kết này.

1. Giới thiệu
Gần đây, học đồ thị đã trở thành một lĩnh vực nghiên cứu quan trọng và phổ biến do kết quả ấn tượng trong nhiều ứng dụng, như khoa học thần kinh (Behrouz et al., 2023), mạng xã hội (Fan et al., 2019), đồ thị phân tử (Wang et al., 2021), v.v. Trong những năm gần đây, Mạng Nơ-ron Truyền Thông điệp (MPNN), lặp đi lặp lại tổng hợp thông tin láng giềng để học biểu diễn nút/cạnh, đã là mô hình chủ đạo trong học máy trên đồ thị (Kipf & Welling, 2016; Velickovic et al., 2018; Wu et al., 2020; Gutteridge et al., 2023). Tuy nhiên, chúng có một số hạn chế cố hữu, bao gồm nén quá mức (Di Giovanni et al., 2023), làm mịn quá mức (Rusch et al., 2023), và khả năng nắm bắt kém các phụ thuộc tầm xa (Dwivedi et al., 2022). Với sự trỗi dậy của kiến trúc Transformer (Vaswani et al., 2017) và thành công của chúng trong các ứng dụng đa dạng như xử lý ngôn ngữ tự nhiên (Wolf et al., 2020) và thị giác máy tính (Liu et al., 2021), các biến thể đồ thị của chúng, được gọi là Graph Transformers (GT), đã trở nên phổ biến như các giải pháp thay thế cho MPNN (Yun et al., 2019; Kim et al., 2022; Rampasek et al., 2022).

Graph transformers đã cho thấy hiệu suất đầy hứa hẹn trong các tác vụ đồ thị khác nhau, và các biến thể của chúng đã đạt được điểm số hàng đầu trong một số điểm chuẩn học đồ thị (Hu et al., 2020; Dwivedi et al., 2022). Sự vượt trội của GT so với MPNN thường được giải thích bởi thiên hướng của MPNN trong việc mã hóa cấu trúc cục bộ (Müller et al., 2023), trong khi một nguyên lý cơ bản của GT là để các nút chú ý đến tất cả các nút khác thông qua cơ chế chú ý toàn cục (Kim et al., 2022; Yun et al., 2019), cho phép mô hình hóa trực tiếp các tương tác tầm xa. Tuy nhiên, chú ý toàn cục có thiên hướng quy nạp yếu và thường yêu cầu kết hợp thông tin về vị trí của các nút để nắm bắt cấu trúc đồ thị (Rampasek et al., 2022; Kim et al., 2022). Để đạt được điều này, các sơ đồ mã hóa vị trí và cấu trúc khác nhau dựa trên các đặc trưng phổ và đồ thị đã được giới thiệu (Kreuzer et al., 2021; Kim et al., 2022; Lim et al., 2023a).

Mặc dù GT với mã hóa vị trí phù hợp (PE) là các bộ xấp xỉ toàn cục và có thể chứng minh là mạnh hơn bất kỳ kiểm tra đẳng cấu Weisfeiler-Lehman (kiểm tra WL) nào (Kreuzer et al., 2021), khả năng áp dụng của chúng trên các đồ thị quy mô lớn bị cản trở bởi khả năng mở rộng kém. Nghĩa là, cơ chế chú ý toàn cục tiêu chuẩn trên một đồ thị với n nút phát sinh cả độ phức tạp thời gian và bộ nhớ O(n²), bậc hai theo kích thước đầu vào, khiến chúng không khả thi trên các đồ thị lớn. Để vượt qua chi phí tính toán cao, được truyền cảm hứng từ các chú ý tuyến tính (Zaheer et al., 2020), các cơ chế chú ý thưa thớt trên đồ thị thu hút sự chú ý (Rampasek et al., 2022; Shirzad et al., 2023). Ví dụ, Exphormer (Shirzad et al., 2023) đề xuất sử dụng đồ thị mở rộng, kết nối toàn cục, và láng giềng cục bộ như ba mẫu có thể được kết hợp trong GT, dẫn đến chú ý thưa thớt và hiệu quả. Mặc dù các chú ý thưa thớt một phần vượt qua chi phí bộ nhớ của chú ý toàn cục, GT dựa trên các chú ý thưa thớt này (Rampasek et al., 2022; Shirzad et al., 2023) vẫn có thể gặp phải độ phức tạp thời gian bậc hai. Nghĩa là, chúng yêu cầu PE tốn kém (ví dụ, phân tích eigen Laplacian) và mã hóa cấu trúc (SE) để đạt được hiệu suất tốt nhất, có thể mất O(n²) để tính toán.

Một cách tiếp cận khác để cải thiện chi phí tính toán cao của GT là sử dụng token hóa đồ thị con (Chen et al., 2023; Zhao et al., 2021; Kuang et al., 2021; Baek et al., 2021; He et al., 2023), trong đó các token (còn gọi là patch) là các đồ thị con nhỏ được trích xuất với một chiến lược được định nghĩa trước. Thông thường, các phương pháp này thu được biểu diễn ban đầu của các token đồ thị con bằng cách truyền chúng qua một MPNN. Với k đồ thị con được trích xuất (token), độ phức tạp thời gian của các phương pháp này là O(k²), hiệu quả hơn GT điển hình với token hóa nút. Ngoài ra, các phương pháp này thường không dựa vào PE/SE phức tạp, vì các token của chúng (đồ thị con) vốn mang thiên hướng quy nạp. Tuy nhiên, các phương pháp này có hai nhược điểm chính: (1) Để đạt được sức mạnh biểu đạt cao, với một nút cho trước, chúng thường yêu cầu ít nhất một đồ thị con cho mỗi nút còn lại (Zhang et al., 2023a; Bar-Shalom et al., 2023), có nghĩa là k trong O(n) và do đó độ phức tạp thời gian là O(n²). (2) Mã hóa đồ thị con qua MPNN có thể truyền tất cả các thử thách của làm mịn quá mức và nén quá mức, hạn chế khả năng áp dụng của chúng trên các đồ thị dị thể và tầm xa.

Gần đây, Mô hình Không gian Trạng thái (SSM), như một giải pháp thay thế cho các kiến trúc mô hình hóa chuỗi dựa trên chú ý như Transformers đã ngày càng trở nên phổ biến do hiệu quả của chúng (Zhang et al., 2023b; Nguyen et al., 2023). Tuy nhiên, chúng không đạt được hiệu suất cạnh tranh với Transformers do giới hạn trong nén ngữ cảnh phụ thuộc đầu vào trong các mô hình chuỗi, gây ra bởi cơ chế chuyển tiếp bất biến thời gian. Để đạt được điều này, Gu & Dao (2023) trình bày Mamba, một mô hình không gian trạng thái chọn lọc sử dụng quét đệ quy cùng với cơ chế lựa chọn để kiểm soát phần nào của chuỗi có thể chảy vào các trạng thái ẩn. Việc lựa chọn này có thể được hiểu đơn giản là sử dụng cơ chế chuyển tiếp trạng thái phụ thuộc dữ liệu (Xem §2.3 để thảo luận chi tiết). Hiệu suất vượt trội của Mamba trong mô hình hóa ngôn ngữ, vượt trội hơn Transformers cùng kích thước và sánh ngang với Transformers gấp đôi kích thước của nó, thúc đẩy một số nghiên cứu gần đây thích ứng kiến trúc của nó cho các phương thức dữ liệu khác nhau (Liu et al., 2024b; Yang et al., 2024; Zhu et al., 2024; Ahamed & Cheng, 2024).

Kiến trúc Mamba được thiết kế đặc biệt cho dữ liệu chuỗi và bản chất không nhân quả phức tạp của đồ thị khiến việc áp dụng trực tiếp Mamba trên đồ thị trở nên thách thức. Hơn nữa, các nỗ lực tự nhiên để thay thế Transformers bằng Mamba trong các khung GT hiện có (ví dụ, GPS (Rampasek et al., 2022), TokenGT (Kim et al., 2022)) dẫn đến hiệu suất không tối ưu cả về hiệu quả và hiệu quả thời gian (Xem §5 để đánh giá và §3 để thảo luận chi tiết). Lý do là, trái ngược với Transformers cho phép mỗi nút tương tác với tất cả các nút khác, Mamba, do bản chất đệ quy của nó, chỉ kết hợp thông tin về các token (nút) trước đó trong chuỗi. Điều này đưa ra những thử thách mới so với GT: (1) Mô hình mới yêu cầu sắp xếp token cho phép mô hình tận dụng thông tin vị trí được cung cấp càng nhiều càng tốt. (2) Thiết kế kiến trúc cần phải mạnh mẽ hơn đối với hoán vị so với một bộ mã hóa tuần tự thuần túy (ví dụ, Mamba). (3) Trong khi độ phức tạp thời gian bậc hai của chú ý có thể chi phối chi phí của PE/SE trong GT, PE/SE phức tạp (với chi phí O(n²)) có thể là nút thắt cổ chai cho việc mở rộng Graph Mamba trên các đồ thị lớn.

Đóng góp. Để giải quyết tất cả các hạn chế nêu trên, chúng tôi trình bày Graph Mamba Networks (GMN), một lớp mới của học máy trên đồ thị dựa trên mô hình không gian trạng thái (Hình 1 cho thấy sơ đồ của GMN). Tóm lại, các đóng góp của chúng tôi là:

• Công thức cho Graph Mamba Networks. Chúng tôi thảo luận các thử thách mới của GMN so với GT trong thiết kế kiến trúc và thúc đẩy công thức của chúng tôi với bốn bước bắt buộc và một bước tùy chọn để thiết kế GMN. Cụ thể, các bước của nó là (1) Token hóa, (2) Sắp xếp Token, (3) Mã hóa Cục bộ, (4) Bộ mã hóa SSM Chọn lọc Hai chiều và có thể bỏ qua (5) PE và SE.

• Một Token hóa Hiệu quả để Kết nối các Khung. Tài liệu thiếu nền tảng chung về những gì tạo nên một token hóa tốt. Theo đó, các kiến trúc được yêu cầu chọn token hóa cấp độ nút hoặc đồ thị con, trong khi mỗi cái có (bất) lợi ích riêng, tùy thuộc vào dữ liệu. Chúng tôi trình bày một quy trình token hóa đồ thị không chỉ nhanh và hiệu quả, mà còn kết nối các phương pháp token hóa cấp độ nút và đồ thị con bằng một tham số duy nhất.

Hơn nữa, token hóa được trình bày có thứ tự ngầm, điều này đặc biệt quan trọng cho các bộ mã hóa tuần tự như SSM.

• SSM Hai chiều Mới cho Đồ thị. Được truyền cảm hứng từ Mamba, chúng tôi thiết kế một kiến trúc SSM quét chuỗi đầu vào theo hai hướng khác nhau, làm cho mô hình mạnh mẽ hơn đối với hoán vị, điều này đặc biệt quan trọng khi chúng ta không sử dụng token hóa có thứ tự ngầm trên đồ thị.

• Lý giải Lý thuyết. Chúng tôi cung cấp lý giải lý thuyết cho sức mạnh của GMN và chỉ ra rằng chúng là bộ xấp xỉ toàn cục của bất kỳ hàm nào trên đồ thị. Chúng tôi cũng chỉ ra rằng GMN sử dụng PE/SE phù hợp có sức mạnh biểu đạt hơn bất kỳ kiểm tra WL nào, tương đương với GT trong mặt này.

• Hiệu suất Vượt trội và Hiểu biết Mới. Các đánh giá thí nghiệm của chúng tôi chứng minh rằng GMN đạt được hiệu suất vượt trội trong các bộ dữ liệu điểm chuẩn tầm xa, quy mô nhỏ, quy mô lớn, và dị thể, trong khi tiêu thụ ít bộ nhớ GPU hơn. Những kết quả này cho thấy rằng trong khi Transformers, truyền thông điệp phức tạp, và SE/PE là đủ cho hiệu suất tốt trong thực tế, không cái nào là cần thiết. Chúng tôi cũng thực hiện nghiên cứu loại bỏ và xác thực đóng góp của mỗi lựa chọn kiến trúc.

2. Công trình Liên quan và Nền tảng

Để đặt GMN trong bối cảnh rộng hơn, chúng tôi thảo luận bốn loại phương pháp học máy liên quan:

2.1. Mạng Nơ-ron Truyền Thông điệp

Mạng nơ-ron truyền thông điệp là một lớp GNN lặp đi lặp lại tổng hợp thông tin láng giềng cục bộ để học biểu diễn nút/cạnh (Kipf & Welling, 2016). MPNN đã là mô hình chủ đạo trong học máy trên đồ thị, và thu hút nhiều sự chú ý, dẫn đến các kiến trúc mạnh mẽ khác nhau, ví dụ GAT (Velickovic et al., 2018), GCN (Henaff et al., 2015; Kipf & Welling, 2016), GatedGCN (Bresson & Laurent, 2017), GIN (Xu et al., 2019), v.v. Tuy nhiên, MPNN đơn giản được biết là gặp phải một số hạn chế chính bao gồm: (1) giới hạn khả năng biểu đạt của chúng trong kiểm tra đẳng cấu 1-WL (Xu et al., 2019), (2) làm mịn quá mức (Rusch et al., 2023), và (3) nén quá mức (Alon & Yahav, 2021; Di Giovanni et al., 2023). Các phương pháp khác nhau đã được phát triển để tăng cường MPNN và vượt qua những vấn đề như vậy, bao gồm GNN bậc cao (Morris et al., 2019; 2020), nối lại đồ thị (Gutteridge et al., 2023; Arnaiz-Rodriguez et al., 2022), GNN thích ứng và hợp tác (Errica et al., 2023; Finkelshtein et al., 2023), và sử dụng các đặc trưng bổ sung (Sato et al., 2021; Murphy et al., 2019).

2.2. Graph Transformers

Với sự trỗi dậy của kiến trúc Transformer (Vaswani et al., 2017) và thành công của chúng trong các ứng dụng đa dạng như xử lý ngôn ngữ tự nhiên (Wolf et al., 2020) và thị giác máy tính (Liu et al., 2021), các biến thể đồ thị của chúng đã trở nên phổ biến như các giải pháp thay thế cho MPNN (Yun et al., 2019; Kim et al., 2022; Rampasek et al., 2022). Sử dụng chú ý toàn cục đầy đủ, GT coi mỗi cặp nút được kết nối (Yun et al., 2019) và do đó được kỳ vọng vượt qua các vấn đề nén quá mức và làm mịn quá mức trong MPNN (Kreuzer et al., 2021). Tuy nhiên, GT có thiên hướng quy nạp yếu và cần mã hóa vị trí/cấu trúc phù hợp để học cấu trúc của đồ thị (Kreuzer et al., 2021; Rampasek et al., 2022). Để đạt được điều này, các nghiên cứu khác nhau đã tập trung vào thiết kế mã hóa vị trí và cấu trúc mạnh mẽ (Wang et al., 2022; Ying et al., 2021; Kreuzer et al., 2021; Shiv & Quirk, 2019).

Chú ý Thưa thớt. Trong khi GT đã cho thấy hiệu suất vượt trội trong các tác vụ đồ thị khác nhau trên các bộ dữ liệu quy mô nhỏ (lên đến 10K nút), chi phí tính toán bậc hai của chúng, gây ra bởi chú ý toàn cục đầy đủ, đã hạn chế khả năng áp dụng của chúng trên các đồ thị quy mô lớn (Rampasek et al., 2022). Được thúc đẩy bởi các cơ chế chú ý tuyến tính (ví dụ, BigBird (Zaheer et al., 2020) và Performer (Choromanski et al., 2021)), được thiết kế để vượt qua cùng vấn đề khả năng mở rộng của Transformers trên các chuỗi dài, việc sử dụng Transformers thưa thớt trong kiến trúc GT đã trở nên phổ biến (Rampasek et al., 2022; Shirzad et al., 2023; Kong et al., 2023; Liu et al., 2023; Wu et al., 2023). Ý tưởng chính của các mô hình GT thưa thớt là hạn chế mẫu chú ý, tức là các cặp nút có thể tương tác với nhau. Ví dụ, Shirzad et al. (2023) trình bày Exphormer, biến thể đồ thị của BigBird sử dụng ba mẫu thưa thớt của (1) chú ý đồ thị mở rộng, (2) chú ý cục bộ giữa các láng giềng, và (3) chú ý toàn cục bằng cách kết nối các nút ảo với tất cả các nút không ảo.

Token hóa Đồ thị con. Một phương pháp khác để vượt qua chi phí tính toán cao của GT là sử dụng token hóa đồ thị con (Chen et al., 2023; Zhao et al., 2021; Baek et al., 2021; He et al., 2023), trong đó các token là các đồ thị con nhỏ được trích xuất với một chiến lược được định nghĩa trước. Các chiến lược token hóa đồ thị con này thường là láng giềng k-hop (với k cố định) (Nguyen et al., 2022a; Hussain et al., 2022; Park et al., 2022), mẫu học được của láng giềng (Zhang et al., 2022), mạng ego (Zhao et al., 2021), láng giềng k-hop phân cấp (Chen et al., 2023), motif đồ thị (Rong et al., 2020), và phân vùng đồ thị (He et al., 2023). Để vector hóa mỗi token, các phương pháp GT dựa trên đồ thị con thường dựa vào MPNN, khiến chúng dễ bị làm mịn quá mức và nén quá mức. Hầu hết chúng cũng sử dụng láng giềng cố định của mỗi nút, bỏ lỡ cấu trúc phân cấp của đồ thị. Ngoại lệ duy nhất là NAGphormer (Chen et al., 2023) sử dụng tất cả láng giềng k = 1, ..., K-hop của mỗi nút như các token tương ứng. Mặc dù token hóa này cho phép mô hình học cấu trúc phân cấp của đồ thị, bằng cách tăng hop của láng giềng, các token của nó trở nên lớn theo cấp số nhân, hạn chế khả năng mở rộng lên các đồ thị lớn.

2.3. Mô hình Không gian Trạng thái

Mô hình Không gian Trạng thái (SSM), một loại mô hình chuỗi, thường được biết đến như các hệ thống tuyến tính bất biến thời gian ánh xạ chuỗi đầu vào x(t) trong R^L thành chuỗi phản hồi y(t) trong R^L (Aoki, 2013). Cụ thể, SSM sử dụng trạng thái tiềm ẩn h(t) trong R^(N×L), tham số tiến hóa A trong R^(N×N), và tham số chiếu B trong R^(N×1), C trong R^(1×N) sao cho:

h'(t) = Ah(t) + Bx(t),
y(t) = Ch(t). (1)

Do khó khăn trong việc giải phương trình vi phân trên trong cài đặt học sâu, các mô hình không gian trạng thái rời rạc (Gu et al., 2020; Zhang et al., 2023b) rời rạc hóa hệ thống trên bằng tham số Δ:

h_t = Āh_(t-1) + B̄x_t,
y_t = Ch_t, (2)

trong đó

Ā = exp(ΔA),
B̄ = (ΔA)^(-1)(exp(ΔA) - I).ΔB. (3)

Gu et al. (2020) chỉ ra rằng SSM thời gian rời rạc tương đương với phép tích chập sau:

K̄ = {C̄B̄, C̄ĀB̄, ..., C̄Ā^(L-1)B̄},
y = x * K̄, (4)

và do đó có thể được tính toán rất hiệu quả. Mô hình không gian trạng thái có cấu trúc (S4), một loại SSM khác, là các giải pháp thay thế hiệu quả cho chú ý và đã cải thiện hiệu quả và khả năng mở rộng của SSM bằng tái tham số hóa (Gu et al., 2022; Fu et al., 2023; Nguyen et al., 2023). SSM cho thấy hiệu suất đầy hứa hẹn trên dữ liệu chuỗi thời gian (Zhang et al., 2023b; Tang et al., 2023), chuỗi Genomic (Nguyen et al., 2023), lĩnh vực chăm sóc sức khỏe (Gu et al., 2021), và thị giác máy tính (Gu et al., 2021; Nguyen et al., 2022b). Tuy nhiên, chúng thiếu cơ chế lựa chọn, gây ra việc bỏ lỡ ngữ cảnh như được thảo luận bởi Gu & Dao (2023). Gần đây, Gu & Dao (2023) giới thiệu một kiến trúc không gian trạng thái có cấu trúc chọn lọc hiệu quả và mạnh mẽ, được gọi là MAMBA, sử dụng quét đệ quy cùng với cơ chế lựa chọn để kiểm soát phần nào của chuỗi có thể chảy vào các trạng thái ẩn. Cơ chế lựa chọn của Mamba có thể được hiểu là sử dụng cơ chế chuyển tiếp trạng thái phụ thuộc dữ liệu, tức là làm cho B, C, và Δ như các hàm của đầu vào x_t. Hiệu suất vượt trội của Mamba trong mô hình hóa ngôn ngữ, vượt trội hơn Transformers cùng kích thước và sánh ngang với Transformers gấp đôi kích thước của nó, thúc đẩy một số nghiên cứu gần đây thích ứng kiến trúc của nó cho các phương thức dữ liệu và tác vụ khác nhau (Liu et al., 2024b; Yang et al., 2024; Zhu et al., 2024; Ahamed & Cheng, 2024; Xing et al., 2024; Liu et al., 2024a; Ma et al., 2024).

3. Thử thách & Động cơ: Transformers so với Mamba

Kiến trúc Mamba được thiết kế đặc biệt cho dữ liệu chuỗi và bản chất không nhân quả phức tạp của đồ thị khiến việc áp dụng trực tiếp Mamba trên đồ thị trở nên thách thức. Dựa trên khả năng áp dụng chung của Mamba và Transformers trên dữ liệu tuần tự được token hóa, một cách tiếp cận đơn giản để thích ứng Mamba cho đồ thị là thay thế Transformers bằng Mamba trong các khung GT, ví dụ TokenGT (Kim et al., 2022) hoặc GPS (Rampasek et al., 2022). Tuy nhiên, cách tiếp cận này có thể không tận dụng đầy đủ SSM chọn lọc do bỏ qua một số đặc điểm đặc biệt của chúng. Trong phần này, chúng tôi thảo luận các thử thách mới cho GMN so với GT.

Chuỗi so với Dữ liệu 2-D. Được biết rằng kiến trúc tự chú ý tương ứng với một họ hàm bất biến hoán vị (Lee et al., 2019; Liu et al., 2020). Nghĩa là, cơ chế chú ý trong Transformers (Vaswani et al., 2017) giả định một kết nối giữa mỗi cặp token, bất kể vị trí của chúng trong chuỗi, khiến nó bất biến hoán vị. Theo đó, Transformers thiếu thiên hướng quy nạp và do đó mã hóa vị trí phù hợp là quan trọng cho hiệu suất của chúng, bất cứ khi nào thứ tự của token quan trọng (Vaswani et al., 2017; Liu et al., 2020). Mặt khác, Mamba là một bộ mã hóa tuần tự và quét token theo cách đệ quy (có thể ít nhạy cảm hơn với mã hóa vị trí). Do đó, nó mong đợi dữ liệu nhân quả làm đầu vào, khiến việc thích ứng với dữ liệu 2-D (ví dụ, hình ảnh) (Liu et al., 2024b) hoặc dữ liệu có cấu trúc đồ thị phức tạp trở nên thách thức. Theo đó, trong khi trong việc thích ứng đồ thị của Transformers, việc ánh xạ đồ thị thành một chuỗi token cùng với mã hóa vị trí/cấu trúc là đủ, các bộ mã hóa tuần tự, như SSM, và cụ thể hơn là Mamba, yêu cầu một cơ chế sắp xếp cho token.

Mặc dù sự nhạy cảm này với thứ tự của token làm cho việc thích ứng SSM với đồ thị trở nên thách thức, nó có thể mạnh mẽ hơn bất cứ khi nào thứ tự quan trọng. Ví dụ, học các cấu trúc phân cấp trong láng giềng của mỗi nút (k-hop cho k = 1, ..., K), được sắp xếp ngầm, là quan trọng trong các lĩnh vực khác nhau (Zhong et al., 2022; Lim et al., 2023b). Hơn nữa, nó cung cấp cơ hội sử dụng kiến thức lĩnh vực khi thứ tự quan trọng (Yu et al., 2020). Trong khung được đề xuất của chúng tôi, chúng tôi cung cấp cơ hội cho cả hai trường hợp: (1) sử dụng kiến thức lĩnh vực hoặc thuộc tính cấu trúc (ví dụ, Personalized PageRank (Page et al., 1998)) khi thứ tự quan trọng, hoặc (2) sử dụng đồ thị con được sắp xếp ngầm (không cần sắp xếp). Hơn nữa, bộ mã hóa hai chiều của chúng tôi quét nút theo hai hướng khác nhau, có khả năng học các hàm bất biến trên đầu vào, bất cứ khi nào cần thiết.

Mô hình Chuỗi Tầm xa. Trong lĩnh vực đồ thị, chuỗi token, dù là nút, cạnh, hay đồ thị con, có thể được tính như ngữ cảnh. Thật không may, kiến trúc Transformer, và cụ thể hơn là GT, không thể mở rộng đến chuỗi dài. Hơn nữa, theo trực giác, nhiều ngữ cảnh hơn (tức là chuỗi dài hơn) nên dẫn đến hiệu suất tốt hơn; tuy nhiên, gần đây đã được quan sát thực nghiệm rằng nhiều mô hình chuỗi không cải thiện với ngữ cảnh dài hơn trong mô hình hóa ngôn ngữ (Shi et al., 2023). Mamba, do cơ chế lựa chọn của nó, có thể đơn giản lọc thông tin không liên quan và cũng đặt lại trạng thái của nó bất cứ lúc nào. Theo đó, hiệu suất của nó cải thiện đơn điệu với độ dài chuỗi (Gu & Dao, 2023). Để đạt được điều này, và để tận dụng đầy đủ Mamba, người ta có thể ánh xạ một đồ thị hoặc nút thành các chuỗi dài, có thể là các túi của các đồ thị con khác nhau. Không chỉ chuỗi dài của token có thể cung cấp nhiều ngữ cảnh hơn, mà nó cũng có khả năng có thể cải thiện sức mạnh biểu đạt (Bevilacqua et al., 2022).

Khả năng Mở rộng. Do bản chất phức tạp của dữ liệu có cấu trúc đồ thị, các bộ mã hóa tuần tự, bao gồm Transformers và Mamba, yêu cầu mã hóa vị trí và cấu trúc phù hợp (Rampasek et al., 2022; Kim et al., 2022). Tuy nhiên, các PE/SE này thường có chi phí tính toán bậc hai, có thể được tính toán một lần trước khi đào tạo. Theo đó, do độ phức tạp thời gian bậc hai của Transformers, việc tính toán các PE/SE này bị chi phối và chúng không phải là nút thắt cổ chai cho việc đào tạo GT. Mặt khác, GMN có chi phí tính toán tuyến tính (đối với cả thời gian và bộ nhớ), và do đó xây dựng PE/SE phức tạp có thể là nút thắt cổ chai của chúng khi đào tạo trên các đồ thị rất lớn. Điều này mang lại một thử thách mới cho GMN, vì chúng cần phải (1) không sử dụng PE/SE, hoặc (2) sử dụng các biến thể hiệu quả hơn của chúng để tận dụng đầy đủ hiệu quả của SSM. Thiết kế kiến trúc của chúng tôi làm cho việc sử dụng PE/SE trở nên tùy chọn và đánh giá thực nghiệm của chúng tôi cho thấy rằng GMN không có PE/SE có thể đạt được hiệu suất cạnh tranh so với các phương pháp có PE/SE phức tạp.

Nút hay Đồ thị con? Ngoài các thử thách mới trên, có sự thiếu hụt nền tảng chung về những gì tạo nên một token hóa tốt, và điều gì phân biệt chúng, ngay cả trong các khung GT. Các phương pháp hiện có sử dụng token hóa nút/cạnh (Shirzad et al., 2023; Rampasek et al., 2022; Kim et al., 2022), hoặc đồ thị con (Chen et al., 2023; Zhao et al., 2021; He et al., 2023). Trong khi các phương pháp với token hóa nút có khả năng nắm bắt phụ thuộc tầm xa tốt hơn, các phương pháp với token đồ thị con có khả năng học láng giềng cục bộ tốt hơn, ít dựa vào PE/SE (Chen et al., 2023), và hiệu quả hơn trong thực tế. Thiết kế kiến trúc của chúng tôi cho phép chuyển đổi giữa token hóa nút và đồ thị con bằng một tham số m duy nhất, làm cho việc chọn token hóa trở thành một siêu tham số có thể điều chỉnh trong quá trình đào tạo.

4. Graph Mamba Networks

Trong phần này, chúng tôi cung cấp công thức năm bước của chúng tôi cho Graph Mamba Networks mạnh mẽ, linh hoạt và có thể mở rộng. Sau khi thảo luận về tầm quan trọng của mỗi bước, chúng tôi trình bày kiến trúc của chúng tôi. Tổng quan về khung GMN được minh họa trong Hình 1.

Trong suốt phần này, chúng tôi để G = (V, E) là một đồ thị, trong đó V = {v₁, ..., vₙ} là tập hợp các nút và E ⊆ V × V là tập hợp các cạnh. Chúng tôi giả định mỗi nút v trong V có một vector đặc trưng x⁽⁰⁾ᵥ trong X, trong đó X trong R^(n×d) là ma trận đặc trưng mô tả thông tin thuộc tính của các nút và d là chiều của các vector đặc trưng. Với v trong V, chúng tôi để N(v) = {u|(v, u) trong E} là tập hợp các láng giềng của v. Với một tập con của các nút S ⊆ V, chúng tôi sử dụng G[S] để biểu thị đồ thị con được tạo bởi các nút trong S, và X_S để biểu thị ma trận đặc trưng mô tả thông tin thuộc tính của các nút trong S.

4.1. Token hóa và Mã hóa

Token hóa, là quá trình ánh xạ đồ thị thành một chuỗi token, là một phần không thể tách rời của việc thích ứng các bộ mã hóa tuần tự với đồ thị. Như đã thảo luận trước đó, các phương pháp hiện có sử dụng token hóa nút/cạnh (Shirzad et al., 2023; Rampasek et al., 2022; Kim et al., 2022), hoặc đồ thị con (Chen et al., 2023; Zhao et al., 2021; He et al., 2023), mỗi cái có (bất) lợi ích riêng. Trong phần này, chúng tôi trình bày một lấy mẫu láng giềng mới đơn giản nhưng linh hoạt và hiệu quả cho mỗi nút và thảo luận các ưu điểm của nó so với token hóa đồ thị con hiện có. Ý tưởng chính và cấp cao của token hóa của chúng tôi là trước tiên, lấy mẫu một số đồ thị con cho mỗi nút có thể đại diện cho cấu trúc láng giềng của nút cũng như vị trí cục bộ và toàn cục của nó trong đồ thị. Sau đó chúng tôi vector hóa (mã hóa) các đồ thị con này để thu được biểu diễn nút.

Lấy mẫu Láng giềng. Với một nút v trong V, và hai số nguyên m, M ≥ 0, cho mỗi 0 ≤ m̂ ≤ m, chúng tôi lấy mẫu M bước đi ngẫu nhiên bắt đầu từ v với độ dài m̂. Để T_{m̂,i}(v) cho i = 0, ..., M là tập hợp các nút được thăm trong bước đi thứ i. Chúng tôi định nghĩa token tương ứng với tất cả các bước đi với độ dài m̂ là:

G[T_{m̂}(v)] = G[⋃ᵢ₌₀ᴹ T_{m̂,i}(v)], (5)

là hợp của tất cả các bước đi với độ dài m̂. Người ta có thể hiểu G[T_{m̂}(v)] như đồ thị con được tạo của một mẫu láng giềng m̂-hop của nút v. Cuối cùng, cho mỗi nút v trong V chúng tôi có chuỗi G[T₀(v)], ..., G[Tₘ(v)] như các token tương ứng của nó.

Sử dụng bước đi ngẫu nhiên (với độ dài cố định) hoặc láng giềng k-hop của một nút như các token đại diện của nó đã được thảo luận trong một số nghiên cứu gần đây (Ding et al., 2023; Zhang et al., 2022; Chen et al., 2023; Zhao et al., 2021). Tuy nhiên, các phương pháp này gặp phải một tập con của những hạn chế này: (1) chúng sử dụng bước đi ngẫu nhiên có độ dài cố định (Kuang et al., 2021), bỏ lỡ cấu trúc phân cấp của láng giềng của nút. Điều này đặc biệt quan trọng khi các phụ thuộc tầm xa của các nút là quan trọng. (2) chúng sử dụng tất cả các nút trong tất cả láng giềng k-hop (Chen et al., 2023; Ding et al., 2023), dẫn đến sự đánh đổi giữa các phụ thuộc tầm xa và các vấn đề làm mịn quá mức hoặc nén quá mức. Hơn nữa, láng giềng k-hop của một nút được kết nối tốt có thể là toàn bộ đồ thị, dẫn đến việc coi đồ thị như một token của một nút, điều này không hiệu quả.

Cách tiếp cận lấy mẫu láng giềng của chúng tôi giải quyết tất cả những hạn chế này. Nó lấy mẫu số lượng cố định các bước đi ngẫu nhiên với độ dài khác nhau cho tất cả các nút, nắm bắt cấu trúc phân cấp của láng giềng trong khi tránh cả sự không hiệu quả, gây ra bởi việc xem xét toàn bộ đồ thị, và làm mịn quá mức và nén quá mức, gây ra bởi tổng hợp láng giềng lớn.

Tại sao không nhiều Đồ thị con hơn? Như đã thảo luận trước đó, đánh giá thực nghiệm đã cho thấy rằng hiệu suất của các mô hình không gian trạng thái chọn lọc cải thiện đơn điệu với độ dài chuỗi (Gu & Dao, 2023). Hơn nữa, chi phí tính toán tuyến tính của chúng cho phép chúng ta sử dụng nhiều token hơn, cung cấp cho chúng nhiều ngữ cảnh hơn. Theo đó, để tận dụng đầy đủ các mô hình không gian trạng thái chọn lọc, với một số nguyên s > 0, chúng tôi lặp lại quá trình lấy mẫu láng giềng trên s lần. Theo đó, cho mỗi nút v trong V chúng tôi có một chuỗi

G[T₀(v)], G[T₁¹(v)], ..., G[T₁ˢ(v)]|_{s lần}, ..., G[T₁ᵐ(v)], ..., G[Tₛᵐ(v)]|_{s lần}

như chuỗi token tương ứng của nó. Ở đây, chúng ta có thể thấy một ưu điểm khác của lấy mẫu láng giềng được đề xuất của chúng tôi so với Chen et al. (2023); Ding et al. (2023). Trong khi trong NAGphormer (Chen et al., 2023) độ dài chuỗi của mỗi nút bị giới hạn bởi đường kính của đồ thị, phương pháp của chúng tôi có thể tạo ra một chuỗi dài các đồ thị con đa dạng.

Định lý 4.1. Với M, m, và s > 0 đủ lớn, lấy mẫu láng giềng của GMN có sức mạnh biểu đạt nghiêm ngặt hơn lấy mẫu láng giềng k-hop.

Mã hóa Cấu trúc/Vị trí. Để tăng cường khung của chúng tôi cho Graph Mamba, chúng tôi xem xét một bước tùy chọn, khi chúng tôi tiêm mã hóa cấu trúc và vị trí vào các đặc trưng ban đầu của nút/cạnh. PE có nghĩa là cung cấp thông tin về vị trí của một nút cho trước trong đồ thị. Theo đó, hai nút gần nhau trong một đồ thị hoặc đồ thị con được cho là có PE gần nhau. Mặt khác, SE có nghĩa là cung cấp thông tin về cấu trúc của một đồ thị con. Theo Rampasek et al. (2022), chúng tôi nối các eigenvector của Laplacian đồ thị hoặc mã hóa cấu trúc Bước đi ngẫu nhiên vào đặc trưng của nút, bất cứ khi nào PE/SE được cần: tức là,

x⁽ⁿᵉʷ⁾ᵥ = xᵥ||pᵥ, (6)

trong đó pᵥ là mã hóa vị trí tương ứng với v. Để nhất quán, chúng tôi sử dụng xᵥ thay vì x⁽ⁿᵉʷ⁾ᵥ trong suốt bài báo.

Mã hóa Láng giềng. Với một nút v trong V và chuỗi token (đồ thị con) của nó, chúng tôi mã hóa đồ thị con qua bộ mã hóa φ(.). Nghĩa là, chúng tôi xây dựng x¹ᵥ, x²ᵥ, ..., x^{ms-1}ᵥ, x^{ms}ᵥ trong R^d như sau:

x⁽⁽ⁱ⁻¹⁾ˢ⁺ʲ⁾ᵥ = φ(G[T^j_i(v)], X_{T^j_i(v)}), (7)

trong đó 1 ≤ i ≤ m và 1 ≤ j ≤ s. Trong thực tế, bộ mã hóa này có thể là một MPNN, (ví dụ, Gated-GCN (Bresson & Laurent, 2017)), hoặc RWF (Tönshoff et al., 2023b) mã hóa các nút đối với một tập hợp các bước đi được lấy mẫu thành các vector đặc trưng với bốn phần: (1) đặc trưng nút, (2) đặc trưng cạnh dọc theo bước đi, và (3, 4) thông tin cấu trúc cục bộ.

Sắp xếp Token. Bằng Phương trình 7, chúng ta có thể tính toán các nhúng láng giềng cho các láng giềng được lấy mẫu khác nhau của một nút và tiếp tục xây dựng một chuỗi để đại diện cho thông tin láng giềng của nó, tức là x¹ᵥ, x²ᵥ, ..., x^{ms-1}ᵥ, x^{ms}ᵥ. Như đã thảo luận trong §3, việc thích ứng các mô hình chuỗi như SSM với dữ liệu có cấu trúc đồ thị yêu cầu một thứ tự trên các token. Để hiểu những gì tạo nên một sắp xếp tốt, chúng ta cần nhớ lại cơ chế lựa chọn trong Mamba (Gu & Dao, 2023) (chúng tôi sẽ thảo luận cơ chế lựa chọn chính thức hơn trong §4.2). Mamba bằng cách làm cho B, C, và Δ như các hàm của đầu vào x_t (xem §2.3 cho ký hiệu) cho phép mô hình lọc thông tin không liên quan và chọn các token quan trọng theo cách đệ quy, có nghĩa là mỗi token được cập nhật dựa trên các token đến trước chúng trong chuỗi. Theo đó, các token trước đó có ít thông tin về ngữ cảnh của chuỗi, trong khi các token sau có thông tin về gần như toàn bộ chuỗi. Điều này dẫn chúng ta đến việc sắp xếp token dựa trên nhu cầu biết thông tin về các token khác hoặc tầm quan trọng của chúng đối với tác vụ của chúng ta.

Khi m ≥ 1: Để đơn giản trước tiên hãy để s = 1. Trong trường hợp m ≥ 1, thú vị là thiết kế kiến trúc của chúng tôi cung cấp cho chúng ta một chuỗi được sắp xếp ngầm. Nghĩa là, với v trong V, token thứ i là một mẫu từ láng giềng i-hop của nút v, là đồ thị con của tất cả láng giềng j-hop, trong đó j ≥ i. Điều này có nghĩa là, với M đủ lớn (số lượng bước đi ngẫu nhiên được lấy mẫu), T^j(v) của chúng ta có đủ thông tin về T^i(v), không phải ngược lại. Để đạt được điều này, chúng tôi sử dụng thứ tự ngược của thứ tự ban đầu, tức là x^m_v, x^{m-1}_v, ..., x²ᵥ, x¹ᵥ. Theo đó, các đồ thị con bên trong cũng có thể có thông tin về cấu trúc toàn cục. Khi s ≥ 2, chúng tôi sử dụng cùng quy trình như trên, và đảo ngược thứ tự ban đầu, tức là x^{sm}_v, x^{sm-1}_v, ..., x²ᵥ, x¹ᵥ. Để làm cho mô hình của chúng tôi mạnh mẽ đối với hoán vị của các đồ thị con với cùng độ dài bước đi m̂, chúng tôi xáo trộn ngẫu nhiên chúng. Chúng tôi sẽ thảo luận về sắp xếp trong trường hợp m = 0 sau.

4.2. Mamba Hai chiều

Như đã thảo luận trong §3, SSM là các mô hình đệ quy và yêu cầu đầu vào có thứ tự, trong khi dữ liệu có cấu trúc đồ thị không có thứ tự nào và cần các bộ mã hóa bất biến hoán vị. Để đạt được điều này, được truyền cảm hứng từ Vim trong thị giác máy tính (Zhu et al., 2024), chúng tôi sửa đổi kiến trúc Mamba và sử dụng hai mô-đun quét đệ quy để quét dữ liệu theo hai hướng khác nhau (tức là tiến và lùi). Theo đó, với hai token t_i và t_j, trong đó i > j và chỉ số cho thấy thứ tự ban đầu của chúng, trong quét tiến t_i đến sau t_j và do đó có thông tin về t_j (có thể chảy vào các trạng thái ẩn hoặc được lọc bởi cơ chế lựa chọn). Trong lần quét lùi t_j đến sau t_i và do đó có thông tin về t_i. Kiến trúc này đặc biệt quan trọng khi m = 0 (token hóa nút), mà chúng tôi sẽ thảo luận sau.

Chính thức hơn, trong mô-đun quét tiến, để Φ là chuỗi đầu vào (ví dụ, với v, Φ là một ma trận có các hàng là x^{sm}_v, x^{sm-1}_v, ..., x¹ᵥ, được tính trong Phương trình 7), A là mã hóa vị trí tương đối của các token, chúng ta có:

Φ_{input} = σ(Conv(W_{input}LayerNorm(Φ))),
B = W_B Φ_{input}, C = W_C Φ_{input}, Δ = Softplus(W_Δ Φ_{input}),
Ā = Discrete_A(A, Δ),
B̄ = Discrete_B(B, Δ),
y = SSM_{Ā,B̄,C}(Φ_{input}),
y_{forward} = W_{forward,1}(y ⊙ σ(W_{forward,2}LayerNorm(Φ))), (8)

trong đó W, W_B, W_C, W_Δ, W_{forward,1} và W_{forward,2} là các tham số có thể học, σ(.) là hàm phi tuyến (ví dụ, SiLU), LayerNorm(.) là chuẩn hóa lớp (Ba et al., 2016), SSM(.) là mô hình không gian trạng thái được thảo luận trong Phương trình 2 và 4, và Discrete(.) là quá trình rời rạc hóa được thảo luận trong Phương trình 3. Chúng tôi sử dụng cùng kiến trúc như trên cho quét lùi (với các trọng số khác nhau) nhưng thay vào đó chúng tôi sử dụng Φ_{inverse} làm đầu vào, là một ma trận có các hàng là x¹ᵥ, x²ᵥ, ..., x^{sm}_v. Để y_{backward} là đầu ra của mô-đun lùi này, chúng tôi thu được mã hóa cuối cùng như

y_{output} = W_{out}(y_{forward} + y_{backward}). (9)

Trong thực tế, chúng tôi xếp chồng một số lớp của Mamba hai chiều để đạt được hiệu suất tốt. Lưu ý rằng do cơ chế sắp xếp của chúng tôi, trạng thái cuối cùng của đầu ra tương ứng với bước đi với độ dài m̂ = 0, tức là chính nút đó. Theo đó, trạng thái cuối cùng đại diện cho mã hóa nút được cập nhật.

Tăng cường với MPNN. Chúng tôi cũng sử dụng một mô-đun MPNN tùy chọn đồng thời thực hiện truyền thông điệp và tăng cường đầu ra của Mamba hai chiều qua thiên hướng quy nạp của nó. Đặc biệt mô-đun này rất hữu ích khi có các đặc trưng cạnh phong phú và do đó một MPNN có thể giúp tận dụng chúng. Trong khi trong đánh giá thực nghiệm của chúng tôi, chúng tôi cho thấy rằng mô-đun này không cần thiết cho thành công của GMN trong một số trường hợp, nó có thể hữu ích khi chúng ta tránh PE/SE phức tạp và cần thiên hướng quy nạp mạnh.

Cơ chế Lựa chọn hoạt động như thế nào trên Đồ thị con? Như đã thảo luận trước đó, cơ chế lựa chọn có thể đạt được bằng cách làm cho B, C, và Δ như các hàm của dữ liệu đầu vào (Gu & Dao, 2023). Theo đó, trong quét đệ quy, dựa trên đầu vào, mô hình có thể lọc ngữ cảnh không liên quan. Cơ chế lựa chọn trong Phương trình 9 được thực hiện bằng cách làm cho B, C, và Δ như các hàm của Φ_{input}, là ma trận của các mã hóa của các láng giềng. Do đó, khi mô hình quét các đồ thị con được lấy mẫu từ các láng giềng i-hop theo thứ tự giảm dần của i, nó lọc các láng giềng không liên quan đến ngữ cảnh (trạng thái cuối cùng), là mã hóa nút.

Lớp Cuối cùng của Mamba Hai chiều. Để nắm bắt các phụ thuộc tầm xa và để chảy thông tin qua các nút, chúng tôi sử dụng các mã hóa nút thu được từ trạng thái cuối cùng của Phương trình 9 làm đầu vào của lớp cuối cùng của Mamba hai chiều. Do đó, quét đệ quy của các nút (theo cả hai hướng) có thể chảy thông tin qua các nút. Thiết kế này không chỉ giúp nắm bắt các phụ thuộc tầm xa trong đồ thị, mà nó cũng là chìa khóa cho tính linh hoạt của khung của chúng tôi để kết nối token hóa nút và đồ thị con.

4.3. Token hóa khi m = 0

Trong trường hợp này, cho mỗi nút v trong V chúng tôi chỉ xem xét chính v như chuỗi token tương ứng của nó. Dựa trên kiến trúc của chúng tôi, trong trường hợp này, các lớp đầu tiên của Mamba hai chiều trở thành phép chiếu đơn giản vì độ dài của chuỗi là một. Tuy nhiên, các lớp cuối cùng, nơi chúng tôi sử dụng mã hóa nút làm đầu vào của chúng, coi các nút như token và trở thành một kiến trúc sử dụng bộ mã hóa tuần tự (ví dụ, Mamba) với token hóa nút. Cụ thể hơn, trong trường hợp đặc biệt này của khung, mô hình là sự thích ứng của khung GPS (Rampasek et al., 2022), khi chúng tôi thay thế Transformer của nó bằng Mamba hai chiều của chúng tôi. Thiết kế kiến trúc này cho phép chuyển đổi giữa token hóa nút và đồ thị con bằng một tham số m duy nhất, làm cho việc chọn token hóa trở thành một siêu tham số có thể điều chỉnh trong quá trình đào tạo. Lưu ý rằng tính linh hoạt này đến từ kiến trúc của chúng tôi hơn là phương pháp token hóa. Nghĩa là, trong thực tế người ta có thể chỉ sử dụng láng giềng 0-hop trong NAGphormer (Chen et al., 2023), dẫn đến việc chỉ xem xét chính nút đó. Tuy nhiên, trong trường hợp này, kiến trúc của NAGphormer trở thành một chồng của các MLP, dẫn đến hiệu suất kém.

Sắp xếp Token. Khi m = 0: Một câu hỏi còn lại là làm thế nào người ta có thể sắp xếp các nút khi chúng ta sử dụng token hóa nút. Như đã thảo luận trong §4.1, các token cần được sắp xếp dựa trên (1) nhu cầu biết thông tin về các token khác hoặc (2) tầm quan trọng của chúng đối với tác vụ của chúng ta. Khi xử lý các nút và cụ thể khi các phụ thuộc tầm xa quan trọng, (1) trở thành điều bắt buộc cho tất cả các nút. Kiến trúc của chúng tôi vượt qua thử thách này bằng quá trình quét hai chiều của nó. Do đó, chúng ta cần sắp xếp các nút dựa trên tầm quan trọng của chúng. Có một số chỉ số để đo tầm quan trọng của các nút trong một đồ thị. Ví dụ, các thước đo trung tâm khác nhau (Latora & Marchiori, 2007; Ruhnau, 2000), bậc, k-core (Lick & White, 1970; Hashemi et al., 2022), Personalized PageRank hoặc PageRank (Page et al., 1998), v.v. Trong các thí nghiệm của chúng tôi, để hiệu quả và đơn giản, chúng tôi sắp xếp các nút dựa trên bậc của chúng.

Cơ chế Lựa chọn hoạt động như thế nào trên các Nút? Tương tự như cơ chế lựa chọn trên đồ thị con, mô hình dựa trên dữ liệu đầu vào có thể lọc các token (nút) không liên quan đến ngữ cảnh (các tác vụ downstream).

4.4. Phân tích Lý thuyết của GMN

Trong phần này, chúng tôi cung cấp lý giải lý thuyết cho sức mạnh của GMN. Cụ thể hơn, trước tiên chúng tôi chỉ ra rằng GMN là bộ xấp xỉ toàn cục của bất kỳ hàm nào trên đồ thị. Tiếp theo, chúng tôi thảo luận rằng với PE phù hợp và đủ tham số, GMN mạnh hơn bất kỳ kiểm tra đẳng cấu WL nào, tương đương với GT (với các giả định tương tự). Cuối cùng, chúng tôi đánh giá sức mạnh biểu đạt của GMN khi chúng không sử dụng bất kỳ PE hoặc MPNN nào và chỉ ra rằng sức mạnh biểu đạt của chúng là không bị chặn (có thể không so sánh được).

Định lý 4.2 (Tính toàn cục). Để 1 ≤ p < ∞, và ε > 0. Đối với bất kỳ hàm liên tục f: [0,1]^{d×n} → R^{d×n} bất biến hoán vị, tồn tại một GMN với mã hóa vị trí, g_p, sao cho ℓ_p(f, g) < ε¹.

Định lý 4.3 (Sức mạnh Biểu đạt với PE). Với tập hợp đầy đủ các hàm riêng và đủ tham số, GMN có thể phân biệt bất kỳ cặp đồ thị không đẳng cấu nào và mạnh hơn bất kỳ kiểm tra WL nào.

Chúng tôi chứng minh hai định lý trên dựa trên công trình gần đây của Wang & Xue (2023), nơi họ chứng minh rằng SSM với phi tuyến theo lớp là bộ xấp xỉ toàn cục của bất kỳ hàm chuỗi-đến-chuỗi nào.

Định lý 4.4 (Sức mạnh Biểu đạt không có PE và MPNN). Với đủ tham số, cho mọi k ≥ 1 có các đồ thị có thể phân biệt bởi GMN, nhưng không bởi kiểm tra k-WL, cho thấy rằng sức mạnh biểu đạt của chúng không bị chặn bởi bất kỳ kiểm tra WL nào.

Chúng tôi chứng minh định lý trên dựa trên công trình gần đây của Tönshoff et al. (2023b), nơi họ chứng minh một định lý tương tự cho CRaWl (Tönshoff et al., 2023b). Lưu ý rằng định lý này không dựa vào sức mạnh của Mamba, và sức mạnh biểu đạt đến từ việc chọn lấy mẫu và mã hóa láng giềng.

¹ℓ_p(.) là p-norm

5. Thí nghiệm

Trong phần này, chúng tôi đánh giá hiệu suất của GMN trong các bộ dữ liệu điểm chuẩn tầm xa, quy mô nhỏ, quy mô lớn, và dị thể. Chúng tôi cũng thảo luận hiệu quả bộ nhớ của nó và thực hiện nghiên cứu loại bỏ để xác thực đóng góp của mỗi lựa chọn kiến trúc. Thống kê chi tiết của các bộ dữ liệu và các thí nghiệm bổ sung có sẵn trong phụ lục.

5.1. Thiết lập Thí nghiệm

Bộ dữ liệu. Chúng tôi sử dụng ba bộ dữ liệu điểm chuẩn được sử dụng phổ biến nhất với các thuộc tính tầm xa, quy mô nhỏ, quy mô lớn, và dị thể. Đối với các bộ dữ liệu tầm xa, chúng tôi sử dụng bộ dữ liệu Long Range Graph Benchmark (LRGB) (Dwivedi et al., 2022). Đối với các bộ dữ liệu quy mô nhỏ và lớn, chúng tôi sử dụng điểm chuẩn GNN (Dwivedi et al., 2023). Để đánh giá GMN trên các đồ thị dị thể, chúng tôi sử dụng bốn bộ dữ liệu dị thể từ công trình của Platonov et al. (2023). Cuối cùng, chúng tôi sử dụng một bộ dữ liệu lớn từ Open Graph Benchmark (Hu et al., 2020). Chúng tôi đánh giá hiệu suất của GMN trên các tác vụ học đồ thị khác nhau (ví dụ, phân loại đồ thị, hồi quy, phân loại nút và phân loại liên kết). Ngoài ra, cho mỗi bộ dữ liệu chúng tôi sử dụng các chỉ số đề xuất trong điểm chuẩn gốc và báo cáo chỉ số qua nhiều lần chạy, đảm bảo tính mạnh mẽ. Chúng tôi thảo luận các bộ dữ liệu, thống kê của chúng và các tác vụ của chúng trong Phụ lục A.

Baseline. Chúng tôi so sánh GMN của chúng tôi với (1) MPNN, ví dụ GCN (Kipf & Welling, 2016), GIN (Xu et al., 2019), và Gated-GCN (Bresson & Laurent, 2017), (2) Phương pháp dựa trên bước đi ngẫu nhiên CRaWl (Tönshoff et al., 2023b), (3) GT tiên tiến, ví dụ SAN (Kreuzer et al., 2021), NAGphormer (Chen et al., 2023), Graph ViT (He et al., 2023), hai biến thể của GPS (Rampasek et al., 2022), GOAT (Kong et al., 2023), và Exphormer (Shirzad et al., 2023), và (4) baseline của chúng tôi (i) GPS + Mamba: khi chúng tôi thay thế mô-đun transformer trong GPS bằng Mamba hai chiều. (ii) GMN-: khi chúng tôi không sử dụng PE/SE và MPNN. Chi tiết của baseline có trong Phụ lục B.

5.2. Long Range Graph Benchmark

Bảng 1 báo cáo kết quả của GMN và baseline trên điểm chuẩn đồ thị tầm xa. GMN liên tục vượt trội hơn baseline trong tất cả các bộ dữ liệu yêu cầu các phụ thuộc tầm xa giữa các nút. Lý do cho hiệu suất vượt trội này có ba mặt: (1) GMN dựa trên thiết kế của chúng tôi sử dụng chuỗi dài các token để học mã hóa nút và sau đó sử dụng một cơ chế lựa chọn khác để lọc các nút không liên quan. Chuỗi dài các token được cung cấp cho phép GMN học các phụ thuộc tầm xa, mà không gặp phải các vấn đề về khả năng mở rộng hoặc nén quá mức. (2) GMN sử dụng cơ chế lựa chọn của chúng có khả năng lọc láng giềng xung quanh mỗi nút. Theo đó, chỉ thông tin có ích mới chảy vào các trạng thái ẩn. (3) Lấy mẫu láng giềng dựa trên bước đi ngẫu nhiên cho phép GMN có các mẫu đa dạng của láng giềng, trong khi nắm bắt bản chất phân cấp của láng giềng k-hop. Ngoài ra, đáng chú ý là GMN liên tục vượt trội hơn baseline GPS + Mamba của chúng tôi, cho thấy tầm quan trọng của việc chú ý đến các thử thách mới. Nghĩa là, thay thế mô-đun transformer bằng Mamba, trong khi cải thiện hiệu suất, không thể tận dụng đầy đủ các đặc điểm của Mamba. Thú vị là, GMN-, một biến thể của GMN mà không có Transformer, MPNN, và PE/SE mà chúng tôi sử dụng để đánh giá tầm quan trọng của các yếu tố này trong việc đạt được hiệu suất tốt, có thể đạt được hiệu suất cạnh tranh với các phương pháp phức tạp khác, cho thấy rằng trong khi Transformers, truyền thông điệp phức tạp, và SE/PE là đủ cho hiệu suất tốt trong thực tế, không cái nào là cần thiết.

5.3. So sánh trên GNN Benchmark

Chúng tôi tiếp tục đánh giá hiệu suất của GMN trong các bộ dữ liệu nhỏ và lớn từ điểm chuẩn GNN. Kết quả của GMN và hiệu suất baseline được báo cáo trong Bảng 2. GMN và Exphormer đạt được hiệu suất cạnh tranh, mỗi cái vượt trội hơn cái kia hai lần. Mặt khác, GMN lại liên tục vượt trội hơn baseline GPS + Mamba, cho thấy tầm quan trọng của việc thiết kế một khung mới cho GMN thay vì sử dụng các khung hiện có của GT.

5.4. Bộ dữ liệu Dị thể

Để đánh giá hiệu suất của GMN trên dữ liệu dị thể cũng như đánh giá tính mạnh mẽ của chúng đối với nén quá mức và làm mịn quá mức, chúng tôi so sánh hiệu suất của chúng với các baseline tiên tiến và báo cáo kết quả trong Bảng 3. GMN của chúng tôi vượt trội hơn baseline trong 3 trên 4 bộ dữ liệu và đạt được kết quả tốt thứ hai trong bộ dữ liệu còn lại. Những kết quả này cho thấy rằng cơ chế lựa chọn trong GMN có thể lọc thông tin không liên quan một cách hiệu quả và cũng xem xét các phụ thuộc tầm xa trong các bộ dữ liệu dị thể.

5.5. Nghiên cứu Loại bỏ

Để đánh giá đóng góp của mỗi thành phần của GMN trong hiệu suất của nó, chúng tôi thực hiện nghiên cứu loại bỏ. Bảng 4 báo cáo kết quả. Hàng đầu tiên, báo cáo hiệu suất của GMN với kiến trúc đầy đủ của nó. Sau đó trong mỗi hàng, chúng tôi sửa đổi một trong các yếu tố trong khi giữ các yếu tố khác không thay đổi: Hàng 2 loại bỏ Mamba hai chiều và sử dụng Mamba đơn giản. Hàng 3 loại bỏ MPNN và Hàng 4 sử dụng sắp xếp PPR. Cuối cùng hàng cuối loại bỏ PE. Kết quả cho thấy rằng tất cả các yếu tố của GMN đóng góp vào hiệu suất của nó với đóng góp nhiều nhất từ Mamba hai chiều.

5.6. Hiệu quả

Như chúng tôi đã thảo luận trước đó, một trong những ưu điểm chính của mô hình của chúng tôi là hiệu quả và sử dụng bộ nhớ. Chúng tôi đánh giá khẳng định này trên các bộ dữ liệu OGBN-Arxiv (Hu et al., 2020) và MalNet-Tiny (Dwivedi et al., 2023) và báo cáo kết quả trong Hình 2. Các biến thể GMN của chúng tôi là các phương pháp hiệu quả nhất trong khi đạt được hiệu suất tốt nhất. Để cho thấy xu hướng khả năng mở rộng, chúng tôi sử dụng MalNet-Tiny và vẽ biểu đồ sử dụng bộ nhớ của GPS và GMN trong Hình 3. Trong khi GPS, như một khung graph transformer, yêu cầu chi phí tính toán cao (sử dụng bộ nhớ GPU), bộ nhớ của GMN mở rộng tuyến tính đối với kích thước đầu vào.

6. Kết luận

Trong bài báo này, chúng tôi trình bày Graph Mamba Networks (GMN) như một lớp mới của học đồ thị dựa trên Mô hình Không gian Trạng thái. Chúng tôi thảo luận và phân loại các thử thách mới khi thích ứng SSM với dữ liệu có cấu trúc đồ thị, và trình bày bốn bước bắt buộc và một bước tùy chọn để thiết kế GMN, trong đó chúng tôi chọn (1) Token hóa Láng giềng, (2) Sắp xếp Token, (3) Kiến trúc Bộ mã hóa SSM Chọn lọc Hai chiều, (4) Mã hóa Cục bộ, và có thể bỏ qua (5) PE và SE. Chúng tôi cũng cung cấp lý giải lý thuyết cho sức mạnh của GMN và tiến hành một số thí nghiệm để đánh giá thực nghiệm hiệu suất của chúng.

Tác động Xã hội Tiềm tàng
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học Máy. Có nhiều hậu quả xã hội tiềm tàng của công trình của chúng tôi, không có cái nào chúng tôi cảm thấy phải được nhấn mạnh cụ thể ở đây.
