# 2403.07711.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2403.07711.pdf
# File size: 806009 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SSM Meets Video Diffusion Models: Efficient Long-Term
Video Generation with Selective State Spaces
Yuta Oshimaa, Shohei Taniguchia, Masahiro Suzukia, Yutaka Matsuoa
aThe Univeristy of Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo, Japan
Abstract
Given the remarkable achievements in image generation through diffusion mod-
els, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their computational costs, which increase
quadratically with the sequence length. This limitation presents significant
challenges when generating longer video sequences using diffusion models. To
overcome this challenge, we propose leveraging state-space models (SSMs) as
temporal feature extractors. SSMs (e.g., Mamba) have recently gained atten-
tion as promising alternatives due to their linear-time memory consumption
relative to sequence length. In line with previous research suggesting that using
bidirectional SSMs is effective for understanding spatial features in image gen-
eration, we found that bidirectionality is also beneficial for capturing temporal
features in video data, rather than relying on traditional unidirectional SSMs.
We conducted comprehensive evaluations on multiple long-term video datasets,
such as MineRL Navigate, across various model sizes. For sequences up to
256 frames, SSM-based models require less memory to achieve the same FVD
as attention-based models. Moreover, SSM-based models often deliver better
performance with comparable GPU memory usage. Our codes are available
athttps://github.com/shim0114/SSM-Meets-Video-Diffusion-Models .
Keywords: video generation, diffusion models, state-space models
1. Introduction
Research on video generation using diffusion models [ 56,46,29] is cutting-
edge in the field of deep generative models. The success of image generation
Preprint submitted to Neurocomputing September 5, 2024arXiv:2403.07711v4  [cs.CV]  3 Sep 2024

--- PAGE 2 ---
√óùëá
Spatial Layer Temporal Layer
(a) Video Diffusion Models (b) Temporal Attention LayerLayer Norm+
(c) Temporal SSM Layer (Ours)Attention
ùëÇ(ùêø2) complexity
+
Bidirectional SSM
ùëÇ(ùêø) complexity
√ó ùë≥
Layer NormFigure 1: (a) U-Net based video diffusion models consist of spatial layers and temporal
layers. (b) Conventional approaches use attention mechanisms in temporal layers, leading
to quadratic growth in memory allocation and computational time as sequence length L
increases. (c) We propose using SSMs in temporal layers, where memory allocation and
computational time increase only linearly with sequence length L.
using diffusion models, notably Denoising Diffusion Probabilistic Models
(DDPMs) [ 29], has sparked a surge in research on applying diffusion models to
video generation. This trend has been exemplified by the emergence of video
diffusion models (VDMs) [ 31]. By harnessing the substantial representational
capacity inherent in diffusion models, their application to video generation
has showcased impressive performance in modeling the dynamic and intricate
nature of video content [31, 54, 30].
However, research on diffusion-model-based video generation faces significant
challenges in terms of computational complexity with respect to video sequence
length. In diffusion models for video generation, attention mechanisms [ 61] are
employed to capture temporal relationships [ 31,54,30,70,3]. In early studies
on diffusion models for video generation, such as VDMs, to capture temporal
relationships across video frames, temporal attention layers were added after
spatial attention layers within the architecture of diffusion models for image
generation, as described in Figure 1(a)(b). However, the memory demands of
attention layers, which scale with the square of the sequence length, present
2

--- PAGE 3 ---
substantial challenges for extending these models to handle longer sequences.
Consequently, current research predominantly focuses on short-term video
generation, typically around 16 frames per inference [ 31,54,3], which equates
to less than a second of video at 30 fps.
Recently, state-space models (SSMs) [ 21,22,23,55], especially Mamba [ 20],
have been identified as promising alternatives to attention mechanisms. In
contrast to attention mechanisms, SSMs can handle sequential data with linear
complexities, so they are expected to overcome the fundamental limitation
of attention-based models in many sequence modeling tasks. Subsequent
studies have also shown that SSMs are effective alternatives to attention-based
approaches across various domains (see section 4.2).
A key finding from our investigation is that, in the context of video generation,
adopting a bidirectional approach allows SSMs to more comprehensively
understand the temporal dynamics in video data (Figure 1(c)). This finding
aligns with previous research showing that incorporating SSMs that observe
various directions enhances the generation quality of models capturing spatial
dependencies in images [ 67,33,13]. Notably, our empirical observations
reveal that a naive replacement of temporal attention layers in VDMs with
unidirectional SSMs significantly underperforms compared to the original
attention-based VDMs.
We first conducted experiments on MineRL Navigate dataset [ 24,52], a
widely used long-term video dataset, varying the number of frames to 16, 64,
and 256, to compare the computational cost and video generation quality of
SSM-based models with attention-based models. For long sequences of 256
frames, we found that SSM-based models, even with larger model sizes, can
be trained with similar memory usage as attention-based models. This results
in reduced memory requirements to achieve the same FVD as attention-
based models. Furthermore, when the GPU memory usage is comparable,
SSM-based models often outperform attention-based models in terms of
performance. The effectiveness of SSM-based models at 256 frames was also
confirmed on several long-term video datasets, including GQN-Mazes [ 12,52]
and CARLA Town01 [9, 25].
2. Background
In this section, we provide an overview of our initial understanding of diffusion
models for video generation. Diffusion models were initially introduced by
Sohl-Dickstein et al. [56], and Ho et al. [29]further advanced the field with the
3

--- PAGE 4 ---
introduction of denoising diffusion probabilistic models (DDPMs). DDPMs
introduced a practical training algorithm, particularly well-suited for image
generation. Recent advancements in diffusion models for video generation
involve the extension of the DDPM architecture to accommodate video data.
2.1. Denoising Diffusion Probabilistic Models (DDPMs)
In diffusion models, the forward process involves progressively diminishing
the original data signal, x0, by gradually introducing Gaussian noise as the
diffusion time, t, advances. This sequence of transformations leads x0to
converge to pure Gaussian noise, represented as xT‚àº N(xT;0,I), at time T.
In our study, tis treated as a discrete integer within the range [0 , T], although
some studies have considered tas a continuous variable [ 57,37].The forward
process is governed by the following following Markov process:
q(x1:T|x0) =TY
t=1q(xt|xt‚àí1), (1)
q(xt|xt‚àí1) =N(xt;‚àöŒ±tx0,(1‚àíŒ±t)I). (2)
In this formulation, the sequence œÉtsatisfies 0 < œÉ 1<¬∑¬∑¬∑< œÉ T‚àí1< œÉ T<1.
The generation process in diffusion models is the reverse process. This
process starts with pure Gaussian noise xT‚àº N (xT;0,I) and gradually
reconstructing the data towards the original x0. During the reverse process,
each step pŒ∏(xt‚àí1|xt) is modeled using a neural network.
pŒ∏(x1:T) =p(xT)TY
t=1pŒ∏(xt‚àí1|xt), (3)
pŒ∏(xt‚àí1|xt) =N(xt‚àí1;¬µŒ∏(xt, t),Œ£Œ∏(xt, t)), (4)
p(xT) =N(xT;0,I). (5)
Typically, Œ£Œ∏is set as an untrainable, time-dependent constant, Œ£Œ∏(xt, t) =
œÉtI. Additionally, with a change in the parameterization of ¬µŒ∏, the reverse
process pŒ∏(xt‚àí1|xt) can be expressed as:
pŒ∏(xt‚àí1|xt)
=N
xt‚àí1;1‚àöŒ±t
xt+œÉ2
t‚àö¬ØŒ±t‚àí1œµŒ∏(xt, t)
, œÉtI
, (6)
4

--- PAGE 5 ---
where ¬ØŒ±t=Qt
i=1(1‚àíœÉ2
i). The term œµŒ∏(xt, t) represents a function that
predicts the noise from noisy data xt. This parameterization results in an
objective function for the DDPM structured as follows:
Ex0,œµ,t
‚à•œµ‚àíœµŒ∏(xt, t)‚à•2
2
, (7)
where xt=‚àö¬ØŒ±tx0+‚àö1‚àí¬ØŒ±tœµ. While numerous parameterizations are
recognized, such as predicting the observed data x0from its noisy counterpart
xtorv-prediction [31, 51, 37], we chose to utilize the œµ-prediction.
In terms of the architecture of diffusion models, 2D U-Net [ 49] architectures
are commonly used for image data. In 2D U-Net-based models, spatial
attention layers are incorporated between the convolutional layers. These
spatial attention layers enhance the ability to focus on relevant spatial features
and improve the quality of the generated images. Although we adopted a U-
Net-based architecture for the diffusion model, Peebles and Xie [47]explored
architectures based on Vision Transformers (ViT) [10].
2.2. Architectures for Video Diffusion Models
To generate videos, diffusion models need to encapsulate both spatial and
temporal features across frames. While DDPMs typically comprise a combi-
nation of U-Net and spatial attention layer, their capability is predominantly
confined to spatial feature capture.
To address this limitation, Video Diffusion Models (VDMs) [ 31] were intro-
duced as an initial attempt at video generation using diffusion models. By
incorporating mechanisms to capture temporal dynamics within DDPMs,
VDMs enhance their capability to capture temporal features (Figure 1 (a)(b)).
Temporal attention layers are commonly used in video generation diffusion
models, such as VDMs, to leverage time-series dependencies. However, tem-
poral attentions require memory proportional to the square of the sequence
length, which imposes limitations on the maximum length of video sequences
that can be generated at once. In our study, we adopt VDMs as a baseline to
explore the existing challenges and potential improvements in video generation
using diffusion models.
3. Method
In this section, we propose the architecture of a temporal SSM (state-space
model) layer for use in diffusion models for videos. Recent diffusion model-
based video generation techniques capture temporal features through temporal
5

--- PAGE 6 ---
attention layers, incurring memory costs proportional to the square of the
sequence length. Recently, SSMs have emerged as a promising alternative
to attention, offering linear memory costs with respect to the sequence
length [ 21,22,23,55,20]. We first review the recent advancements in SSMs
in prior works, followed by a detailed description of our proposed temporal
SSM layer architecture for video generation diffusion models.
3.1. State Space Models
Unlike the temporal attention commonly used in video diffusion models,
state-space models (SSMs) enable the processing of time series with spatial
complexities proportional to the sequence length. Recent studies proposed
SSMs that could process inputs in parallel, unlike recurrent models such as
recurrent neural networks (RNNs) [5].
SSMs are widely used as sequence models that define a mapping from one-
dimensional input signals u(t)‚ààRto one-dimensional output signals y(t)‚ààR,
withs(t)‚ààRNrepresenting the hidden state. Continuous time processes are
formulated as follows:
Àôs(t) =As(t) +Bu(t),
y(t) =Cs(t) +Du(t),(8)
where A‚ààRN√óN,B‚ààRN√ó1,C‚ààR1√óNdenote the diagonal state matrix,
input matrix, and output matrix for hidden state respectively. D‚ààRis
direct path from input to output.
For applying SSMs with real-world data, SSMs formulated as Equation 8
are converted into discrete versions utilizing the zero-order hold (ZOH)
method [23]:
sk=¬ØAsk‚àí1+¬ØBuk,
yk=Csk+Duk,(9)
where ¬ØA=exp(‚àÜA),¬ØB= (‚àÜA)‚àí1(exp(A)‚àíI)¬∑‚àÜBwith‚àÜas the timescale
parameter for ZOH.
Diversing from previous SSMs which are limited to linear time-invariant (LTI)
systems, Mamba [ 20] introduces a selective scan mechanism (S6). In S6, the
parameters B‚ààRB√óL√óN,C‚ààRB√óL√óN,‚àÜ‚ààRB√óL√óDare dependent on
input u‚ààRB√óL√óD. This enables the model to selectively discard irrelevant
information while retaining essential information over extended periods. Using
6

--- PAGE 7 ---
(a) Mamba (b) Bidirectional MambaœÉœÉ √ó
œÉForward
Conv1dForward
SSMœÉ √óBackward
Conv1dœÉBackward
SSM
+√ó
Forward
Conv1dForward
SSMFigure 2: Architectural comparison of Mamba and bidirectional Mamba. Layer Normaliza-
tion [1] and a skip connection [26] are omitted for simplicity.
an efficient scan algorithm, S6 is capable of parallel computation with linear
complexity relative to the sequence length.
3.2. Temporal SSM Layer for Diffusion Model-based Video Generation
We incorporate state-space models (SSMs) within the temporal layers for the
video generation diffusion model (Figure 1 (c)). In our model, we replace
the self-attention component with an SSM and adopt a bidirectional SSM
structure, drawing from practices in [ 18,65]. This choice is motivated by the
inherent limitation of a single SSM, which is typically restricted to capturing
unidirectional temporal transitions. In models capturing spatial dependencies
in images using SSMs, incorporating SSMs that observe various directions can
enhance generation quality [ 67,72,41]. Similarly, we found that by adopting
a bidirectional approach, SSMs can more comprehensively understand the
temporal dynamics in video data.
We specifically chose to adopt the bidirectional Mamba architecture as pro-
posed by [ 72] (Figure 2). Following the standard setup used in previous works
on Mamba [ 20,72,41], we set the expansion factor of the input dimensions
via linear projection to E= 2, the internal state dimension of the SSM to
N= 16, and adopted SiLU [ 11] as the activation function. The input is
denoted as X‚ààR(B√óH√óW)√óL√óC, where Lis the sequence length, Cis the
channel size, His the height, and Wis the width of the input image.
4. Related Works
4.1. Deep Generative Models for Video Generation
The field of video synthesis has seen significant advancements through various
studies. Prior to the emergence of diffusion models, the use of generative
7

--- PAGE 8 ---
adversarial networks (GANs) [ 17] dominated the scene. These methods
extended traditional image-GAN frameworks to video generation, focusing
on enhancing their generative capabilities [ 63,50,59,15]. These approaches
primarily aimed to achieve their objectives by extending common architectures
of GANs for image generation. Additionally, the development of long-term
video generation techniques, particularly those leveraging the transitions
of latent variables in variational autoencoders (VAEs) [ 36] are also well-
known [35, 19, 52, 68].
The advent of diffusion models in image generation [ 56,46,29] marked
a turning point, with their subsequent application to video distributions
demonstrating promising outcomes [ 31,54,30,25,3]. These approaches
have shown promising results. Nevertheless, the recent approaches adopt
attention mechanisms to temporal layers, which require memory proportional
to the square of the sequence length, the computational and memory demands
of video-diffusion models pose a substantial challenge. To mitigate this,
spatio-temopral downsampling [ 54,30], utilizing latent features [ 48,27,70,
3] are proposed. In the realm of long-term video prediction using video
diffusion models, multistep generation techniques stand out [ 62,32,25,69].
These schemes generate videos through successive sampling with flexible
frame conditioning, allowing for efficient long-term dependency modeling with
minimal memory usage. Our research diverges from them by focusing on
architectural improvements rather than sampling schemes.
4.2. SSMs and Their Applications
Mamba, a recent SSM introduced by Gu and Dao [20], offers efficient compu-
tation and demonstrates outstanding performance, in HiPPO [ 21] and S4 [ 22]
laid the groundwork for subsequent advancements in sequence modeling frame-
works, including the development of Mamba. By introducing a structured
parameterization of SSMs, S4 enabled efficient computation and demon-
strated exceptional performance in capturing long-range dependencies [ 58].
S4D [ 23] is a simplified version of S4 that introduced a diagonal matrix for-
mulation. SSMs have been applied across various domains, including image
and video classification [ 45,39,34,64,72,41,40], segmentation [ 43], speech
generation [ 16], time-series generation [ 71], language modeling [ 44,65,6],
reinforcement learning [2, 42, 7].
In the field of diffusion models using SSMs, DiffuSSM [ 67] have first explored
the integration of SSMs with diffusion models, replacing the computationally
intensive spatial attention mechanisms in image generation with SSMs. Other
8

--- PAGE 9 ---
Table 1: Model Configuration. The settings for the UNet-based diffusion models used in
the experiments. The models are scaled such that the base channel size is proportional to
the number of attention heads, while the attention hidden dimension is fixed at 64.
Attention-based Models
Params # Base # Attention # Attention Spatial Temporal
channels heads dims
14.1M 32 4 64 Linear attn. Attention
56.3M 64 8 64 Linear attn. Attention
71.2M 72 9 64 Linear attn. Attention
225M 128 16 64 Linear attn. Attention
900M 256 32 64 Linear attn. Attention
SSM-based Models
14.5M 32 4 64 Linear attn. SSM
57.5M 64 8 64 Linear attn. SSM
175M 112 14 64 Linear attn. SSM
229M 128 16 64 Linear attn. SSM
913M 256 32 64 Linear attn. SSM
studies have also explored replacing spatial attention with SSMs for image
generation, as well as handling spatio-temporal information globally for short-
term video generation (e.g., 16 frames) [ 33,14,13]. In contrast, our research
differentiates itself by replacing the temporal attention in video generation
models with Mamba, aiming to generate longer-term videos (e.g., 256 frames).
5. Experiments
In this section, we conducted a series of experiments demonstrating that
incorporating SSMs into diffusion models for video generation effectively cap-
tures temporal dependencies, particularly in generating long-term videos (e.g.,
256 frames), while maintaining computational efficiency. Our experiments
across multiple datasets and various settings revealed that, under equivalent
memory usage, temporal SSMs outperformed in generating long-term videos.
These findings suggest that utilizing SSMs in temporal layers is beneficial
for constructing video diffusion models capable of long-term video generation
under constrained computational resources.
9

--- PAGE 10 ---
5.1. Experimental Setup
Models For each experimental setup, we conducted experiments using both
VDMs with temporal attention layers and VDMs with SSM-based temporal
layers, across multiple model sizes. The number of parameters and scaling
methods for each model are detailed in Table 1 . The model size for generating
256-frame long videos was selected based on the scaling approach used in
this study, ensuring that the experiments were feasible within the constraints
of the available hardware, which comprised eight NVIDIA A100 GPUs (40
GB each). The VDMs were implemented based on a publicly available and
widely used implementation1that utilizes UNet‚Äôs convolution layer and linear
attention [ 53,38,66] for capturing spatial features. In this paper, we focused
on comparing only the layers responsible for capturing temporal features, as
changes in the spatial feature extraction are not relevant to our investigation.
Datasets We first utilized the MineRL Navigate dataset [ 24,52], a commonly
used dataset for long-term video generation, derived from the Minecraft
environment. The training set consisted of a total of 1,186 videos, combining
both the train and test splits. We trained our models using videos with
varying frame lengths of 16, 64, and 256 frames to evaluate the computational
cost and generative performance when using SSMs versus attention as the
temporal layer under different frame lengths. Additionally, we employed the
GQN-Mazes [ 12,52] and CARLA-Town01 [ 9,25] datasets for long-term video
generation tasks. The GQN-Mazes dataset contains 108,200 videos from a 3D
maze simulator environment, while the CARLA Town01 dataset includes 508
videos from a driving simulator environment. For these datasets, we trained
our models using videos with 256 frames. Across all experiments, the spatial
resolution of the videos was fixed at 32x32 pixels. Training details for each
dataset and each frame length are shown in Appendix Appendix A.
Baseline We established our experimental baseline using VDMs [ 31]. Our
analysis was meticulously designed to alter only the temporal attention layers
in VDMs with our temporal SSM layers. This strategy enabled a focused
examination of the impact and efficacy of the temporal SSM layers in the
context of video generation, facilitating a direct comparison with the existing
temporal layers.
Evaluation Metrics In our validation process, we evaluated the sample
quality of the videos generated by the trained models. To evaluate the
1https://github.com/lucidrains/video-diffusion-pytorch
10

--- PAGE 11 ---
30 40 50 60 70 80 90 100
GPU Memory Usage [GB]4050607080FVD-16F
56.3M
900M57.5M
913MMineRL-16f
Attention
SSM
50 75 100 125 150 175 200 225
GPU Memory Usage [GB]200300400500600700800900FVD-64f
14.1M
56.3M
225M
900M14.5M
57.5M
229M 913MMineRL-64f
Attention
SSM
100 150 200 250 300
GPU Memory Usage [GB]300400500600700FVD-256f
14.1M
56.3M 71.2M14.5M
57.5M
175MMineRL-256f
Attention
SSMFigure 3: Comparison of GPU memory usage and FVD between the temporal SSM layer
and temporal attention on the MineRL Navigate dataset: 16 frames (left), 64 frames
(center), 256 frames (right). Each plot point includes the model size next to it.
100 150 200 250 300
GPU Memory Usage [MB]50100150200250FVD-256f
14.1M
56.3M
71.2M14.5M
57.5M
175MGQN-256f
Attention
SSM
100 150 200 250 300
GPU Memory Usage [MB]200250300350400450500550FVD-256f
14.1M
56.3M
71.2M14.5M
57.5M
175MCARLA-256f
Attention
SSM
Figure 4: Comparison of GPU memory usage and FVD between the temporal SSM layer
and temporal attention for 256 frames on the GQN-Mazes (left) and CARLA Town01
(right) datasets. Each plot point includes the model size next to it.
quality of the generated videos, we employ the Fr¬¥ echet Video Distance (FVD)
metirc [ 60], using an I3D network pretrained on the Kinetics-400 dataset [ 4].
FVD is a recognized standard for assessing the quality of generated videos [ 31,
15,54,30,25], where lower scores denote superior quality. For the MineRL
Navigate dataset, the calculations involve all 1,186 videos and 1,000 generated
samples. For the GQN-Mazes dataset, the calculations involve 2,000 videos
from the dataset and 2,000 generated samples, while for the CARLA Town01
dataset, all 508 videos from the dataset and 500 generated samples are used.
We used three different random seeds to introduce variation in data sampling
and sample generation for FVD calculation.
11

--- PAGE 12 ---
ample 1
1 16 32 64 80 96 112 128 144 160
ample 2
ample 1
ample 2
ample 1
ample 2
176 192 208 224 240 48 256Figure 5: Videos generated by temporal SSM-based VDMs. Each column represents
different samples, with the top two rows corresponding to MineRL Navigate, the middle
two to GQN-Mazes, and the bottom two to CARLA Town01 (# of frames are 256, 32 √ó32
resolution). Additional qualitative results are provided in Figure B.7, B.8, B.9.
5.2. Main Results
We trained models of various sizes across different settings, with the results
shown in Figure 3 and 4, and Table 2 and 3. First, we describe the re-
sults obtained from the MineRL Navigate dataset with varying video frames.
For sequences of 16 and 64 frames, the VDM with an SSM-based tempo-
ral layer demonstrates comparable generative performance to that of the
attention-based temporal layer. However, when the frame count increases to
256 frames, the SSM-based VDM exhibits superior generative performance
relative to memory usage compared to the attention-based approach. Fur-
thermore, experiments conducted on the GQN-Mazes and CARLA Town01
datasets confirm the advantages of using an SSM-based temporal layer in
VDMs over attention-based layers. These benefits, particularly in terms of
generative performance relative to memory usage, extend beyond the MineRL
Navigate dataset. This advantage is especially evident in the generation of
256-frame video sequences across various scenarios. The generated samples
from temporal SSM-based VDMs are shown in Figure 5.
In Figure 6, we present how training memory consumption and inference
time vary with video sequence length for each temporal layer, using 32 √ó32
resolution images and the same model size. Training memory consumption
data is based on a batch size of 8, while inference times reflect sample
generation on a single NVIDIA A100 GPU with the number of diffusion time
steps Tfixed at 256. The results for 256 frames are particularly notable,
12

--- PAGE 13 ---
50 100 150 200 250
Sequence Length50100150200250Memory Usage (GB)
Model
SSM
Attention
50 100 150 200 250
Sequence Length102030405060Time (seconds)
Model
SSM
AttentionFigure 6: Left: Memory consumption during training with 8 NVIDIA A100 GPUs (40
GB) at a batch size of 8 and resolution of 32 √ó32. Right: Inference time for generating a
sample with a single NVIDIA A100 GPU at a resolution of 32 √ó32 and T= 256.
demonstrating that while memory usage and inference speed in Attention-
based models increase quadratically with sequence length, the SSM-based
model exhibits only a linear increase.
The results of this experiment suggest that SSMs can serve as a viable
alternative to attention mechanisms in the temporal layers, even for the
generation of relatively short videos, such as those with 16 or 64 frames.
Furthermore, in the generation of longer videos, such as those with 256 frames,
SSM-based models can achieve higher-quality video generation compared to
attention-based models. This is because SSMs allow for larger model sizes
while maintaining comparable computational resources. Additionally, this
trend is expected to become more pronounced as the number of video frames
increases, based on the observed relationship between the number of frames
and computational cost.
5.3. Bidirectionality of Temporal SSM Layers
Our ablation study revealed that bidirectional processing within the SSM is
crucial for generating high-quality videos when capturing temporal features
in video sequences. Specifically, in our experiments with 256-frame video
generation on the MineRL Navigate dataset, using a unidirectional SSM led
to a significant decline in generative quality (Table 4).
13

--- PAGE 14 ---
Table 2: Quantitative results of experiments in MineRL Navigate. Memory consumption
during training with 8 NVIDIA A100 GPUs (40 GB) at a batch size of 8 and resolution of
32√ó32. Inference time for generating a sample with a single NVIDIA A100 GPU at a
resolution of 32 √ó32 and T= 256.
16 frames
Models Memory Usage Inference Time FVD-16f ‚Üì
Attention-56.3M 30.8 GB 6.11 sec. 80.3
Attention-900M 101 GB 18.2 sec. 45.6
SSM-57.5M 31.6 GB 5.49 sec. 84.0
SSM-913M 106 GB 20.6 sec. 42.4
64 frames
Models Memory Usage Inference Time FVD-64f ‚Üì
Attention-14.1M 36.8 GB 8.19 sec. 817
Attention-56.3M 59.8 GB 13.4 sec. 449
Attention-225M 110 GB 29.5 sec. 259
Attention-900M 224 GB 68.1 sec. 205
SSM-14.5M 36.8 GB 5.76 sec. 923
SSM-57.5M 59.2 GB 10.3 sec. 510
SSM-229M 105 GB 23.3 sec. 205
SSM-913M 219 GB 50.9 sec. 208
256 frames
Models Memory Usage Inference Time FVD-256f ‚Üì
Attention-14.1M 144 GB 29.6 sec. 642
Attention-56.3M 276 GB 61.1 sec. 383
Attention-71.2M 309 GB 70.8 sec. 380
SSM-14.5M 96.1 GB 19.2 sec. 687
SSM-57.5M 177 GB 39.3 sec. 381
SSM-175M 298 GB 76.6 sec. 272
This finding suggests that treating time non-causally is preferable in fixed-
length video generation models. It is well-known that in image generation
diffusion models using SSMs, performance improvements are achieved by em-
ploying SSMs that process multiple directions in the spatial domain [ 67,33,13].
This behavior is rooted in the fact that SSMs are inherently causal infor-
mation processing mechanisms. Our ablation results indicate that a similar
approach‚Äîavoiding purely causal processing‚Äîalso benefits the temporal
domain in video generation.
14

--- PAGE 15 ---
Table 3: Quantitative results of experiments in GQN-Mazes and CARLA Town01.
GQN-Mazes 256 frames
Models Memory Usage Inference Time FVD-256f ‚Üì
Attention-14.1M 144 GB 29.6 sec. 217
Attention-56.3M 276 GB 61.1 sec. 138
Attention-71.2M 309 GB 70.8 sec. 107
SSM-14.5M 96.1 GB 19.2 sec. 267
SSM-57.5M 178 GB 39.3 sec. 104
SSM-175M 297 GB 76.6 sec. 57.8
CARLA Town01 256 frames
Models Memory Usage Inference Time FVD-256f ‚Üì
Attention-14.1M 145 GB 29.6 sec. 470
Attention-56.3M 276 GB 61.1 sec. 241
Attention-71.2M 309 GB 70.8 sec. 196
SSM-14.5M 96.1 GB 19.2 sec. 557
SSM-57.5M 178 GB 39.3 sec. 256
SSM-175M 297 GB 76.6 sec. 206
Table 4: Performance comparison of bidirectional SSM and unidirectional SSM.
Bidirectional Params Memory Inference Time FVD-256f ‚Üì‚àö57.5 M 177 GB 39.3 sec. 381
56.9 M 146 GB 31.8 sec. 610
6. Discussion
Our experimental findings demonstrate that incorporating SSM into the
temporal layers of diffusion models for video generation offers superior video
modeling in terms of memory efficiency for handling increased sequence
lengths compared to traditional models employing temporal attention, while
maintaining competitive generative quality. It is empirically shown that,
compared to attention-based models, SSM-based video diffusion models can
be scaled to larger sizes under the same computational cost, leading to higher-
quality video generation for longer sequences. These outcomes underscore
the adaptability of temporal SSM layers in enhancing video diffusion models
and indicate their extensive potential impact on future advancements in this
domain.
15

--- PAGE 16 ---
This study paves the way for numerous future investigations. While we have
verified the advantages of employing our temporal SSM layers within the
architecture of VDMs, the application of our temporal SSM layers extends
beyond this, to any model that employs temporal attention for temporal
dimension. For example, adopting temporal SSM layers into the temporal
components of architectures such as Make-A-Video [ 54] or Imagen Video [ 30]
could further enhance their computational efficiency by leveraging their exist-
ing spatiotemporal downsampling mechanism. Additionally, the combination
of the temporal SSM layers with Latent Diffusion Models [ 48] could lead to
significant reductions in computational costs. Our findings also offer insights
for methods that involve freezing pretrained image-generation diffusion models
and training new temporal layers for video generation; simply substituting
these newly inserted temporal layers with the SSM layers could be an effective
strategy. Moreover, while our experiments concentrated on unconditional
video generation, integrating our approach with conditional generation tech-
niques such as classifier guidance [ 8] and classifier free guidance [ 28] could
pave the way for more efficient text-to-video (T2V) models.
The results of this study indicate that the incorporation of SSMs can lead
to the development of long-term video generation models that demand fewer
memory resources. This has noteworthy implications for broadening the
accessibility of cutting-edge research in video generation diffusion models.
Even institutions with limited computational resources can engage in this
advanced field, potentially expediting the pace of research and innovation in
video generation.
References
[1]J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv
preprint arXiv:1607.06450 , 2016.
[2]Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf.
Decision s4: Efficient sequence-based rl via state spaces layers. In
International Conference on Learning Representations , 2023.
[3]Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Se-
ung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents:
High-resolution video synthesis with latent diffusion models. In CVPR ,
2023.
16

--- PAGE 17 ---
[4]J. Carreira and A. Zisserman. Quo vadis, action recognition? a new
model and the kinetics dataset. In CVPR , 2017.
[5]Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
Empirical evaluation of gated recurrent neural networks on sequence
modeling. arXiv preprint arXiv:1412.3555 , 2014.
[6]Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra,
and Christopher R¬¥ e. ‚Äúhungry hungry hippos: Towards language modeling
with state space models‚Äù. In The International Conference on Learning
Representations (ICLR) , 2023.
[7]Fei Deng, Junyeong Park, and Sungjin Ahn. Facing off world model
backbones: Rnns, transformers, and s4. arXiv:2307.02064 , 2023.
[8]Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image
synthesis. arXiv preprint arXiv:2105.05233 , 2021.
[9]Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and
Vladlen Koltun. Carla: An open urban driving simulator. In Conference
on Robot Learning , pages 1‚Äì16. PMLR, 2017.
[10]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-
senborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 , 2020.
[11]Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear
units for neural network function approximation in reinforcement learning.
Neural Networks , 107:3‚Äì11, 2018.
[12]SM Ali Eslami et al. Neural scene representation and rendering. Science ,
360(6394):1204‚Äì1210, 2018.
[13]Yunxiang Fu, Chaoqi Chen, and Yizhou Yu. Lamamba-diff: Linear-time
high-fidelity diffusion models based on local attention and mamba. arXiv
preprint arXiv:2408.02615 , 2024.
[14]Yu Gao et al. Matten: Video generation with mamba-attention. arXiv
preprint arXiv:2405.03025 , 2024.
17

--- PAGE 18 ---
[15]Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David
Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with
time-agnostic vqgan and time-sensitive transformer. arXiv preprint
arXiv:2204.03638 , 2022.
[16]Karan Goel, Albert Gu, Chris Donahue, and Christopher R¬¥ e. It‚Äôs raw!
audio generation with state-space models. In International Conference
on Machine Learning , 2022.
[17]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Gen-
erative adversarial nets. In Advances in Neural Information Processing
Systems , pages 2672‚Äì2680, 2014.
[18]Alex Graves and J¬® urgen Schmidhuber. Framewise phoneme classification
with bidirectional lstm and other neural network architectures. Neural
Networks , 18(5-6):602‚Äì610, 2005.
[19]K. Gregor and F. Besse. Temporal difference variational auto-encoder.
arXiv preprint arXiv:1806.03107 , 2018.
[20]Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with
selective state spaces. arXiv preprint arXiv:2312.00752 , 2023.
[21]Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R¬¥ e.
Hippo: Recurrent memory with optimal polynomial projections. Ad-
vances in Neural Information Processing Systems , 33:1474‚Äì1487, 2020.
[22]Albert Gu, Karan Goel, and Christopher R¬¥ e. Efficiently modeling long
sequences with structured state spaces. arXiv preprint arXiv:2111.00396 ,
2021.
[23]Albert Gu, Karan Goel, Ankit Gupta, and Christopher R¬¥ e. On the
parameterization and initialization of diagonal state space models. In
Advances in Neural Information Processing Systems , volume 35, pages
35971‚Äì35983, 2022.
[24]William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang,
Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl:
A large-scale dataset of minecraft demonstrations. arXiv preprint
arXiv:1907.13440 , 2019.
18

--- PAGE 19 ---
[25]William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach,
and Frank Wood. Flexible diffusion modeling of long videos. arXiv
preprint arXiv:2205.11495 , 2022.
[26]Kaiming He et al. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2016.
[27]Yingqing He et al. Latent video diffusion models for high-fidelity long
video generation. arXiv preprint arXiv:2211.13221 , 2022.
[28]Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv
preprint arXiv:2207.12598 , 2022.
[29]Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion proba-
bilistic models. In Advances in Neural Information Processing Systems ,
pages 6840‚Äì6851, 2020.
[30] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao,
Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi,
David J Fleet, et al. Imagen video: High definition video generation
with diffusion models. arXiv:2210.02303 , 2022.
[31]Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Moham-
mad Norouzi, and David J. Fleet. Video diffusion models, 2022.
[32]Tobias H¬® oppe et al. Diffusion models for video prediction and infilling.
arXiv preprint arXiv:2206.07696 , 2022.
[33]Vincent Tao Hu et al. Zigma: Zigzag mamba diffusion model. arXiv
preprint arXiv:2403.13802 , 2024.
[34]Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification
with state-space video models. In ECCV , 2022.
[35]T. Kim, S. Ahn, and Y. Bengio. Variational temporal abstraction. In
Advances in Neural Information Processing Systems , pages 11566‚Äì11575,
2019.
[36]Diederik P Kingma and Max Welling. Auto-encoding variational bayes.
International Conference on Learning Representations , 2013.
19

--- PAGE 20 ---
[37]Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Varia-
tional diffusion models. arXiv preprint arXiv:2107.00630 , 2021.
[38]Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The
efficient transformer. In International Conference on Learning Represen-
tations , 2020. URL https://openreview.net/forum?id=rkgNKkHtvB .
[39]David M Knigge, David W Romero, Albert Gu, Efstratios Gavves, Erik J
Bekkers, Jakub Mikolaj Tomczak, Mark Hoogendoorn, and Jan-jakob
Sonke. Modelling long range dependencies in nd: From task-specific
to a general purpose cnn. In International Conference on Learning
Representations , 2023.
[40]Kunchang Li et al. Videomamba: State space model for efficient video
understanding. arXiv preprint arXiv:2403.06977 , 2024.
[41]Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, and Y. Liu.
Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 ,
2024.
[42]Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foer-
ster, Satinder Singh, and Feryal Behbahani. Structured state space models
for in-context reinforcement learning. arXiv preprint arXiv:2303.03982 ,
2023.
[43]Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-
range dependency for biomedical image segmentation. arXiv preprint
arXiv:2401.04722 , 2024.
[44]Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
Long range language modeling via gated state spaces. In The Eleventh
International Conference on Learning Representations , 2023.
[45]Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri
Dao, Stephen Baccus, and Christopher R¬¥ e. S4nd: Modeling images and
videos as multidimensional signals with state spaces. In Advances in
Neural Information Processing Systems , 2022.
[46]Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising
diffusion probabilistic models. In International Conference on Machine
Learning , pages 8162‚Äì8171. PMLR, 2021.
20

--- PAGE 21 ---
[47]William Peebles and Saining Xie. Scalable diffusion models with trans-
formers. arXiv preprint arXiv:2212.09748 , 2022. doi: 10.48550/arXiv.
2212.09748.
[48]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser,
and Bj¬® orn Ommer. High-resolution image synthesis with latent diffusion
models. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 10684‚Äì10695, 2022.
[49]Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convo-
lutional networks for biomedical image segmentation. In International
Conference on Medical Image Computing and Computer-Assisted Inter-
vention , pages 234‚Äì241. Springer, 2015.
[50]M. Saito, E. Matsumoto, and S. Saito. Temporal generative adversarial
nets with singular value clipping. In IEEE International Conference on
Computer Vision (ICCV) , 2017.
[51]Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling
of diffusion models. In International Conference on Learning Represen-
tations (ICLR) , 2022.
[52]Vaibhav Saxena, Jimmy Ba, and Danijar Hafner. Clockwork variational
autoencoders. Advances in Neural Information Processing Systems , 34,
2021.
[53]Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng
Li. Efficient attention: Attention with linear complexities. CoRR ,
abs/1812.01243, 2018. URL http://arxiv.org/abs/1812.01243 .
[54]Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang
Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al.
Make-a-video: Text-to-video generation without text-video data.
arXiv:2209.14792 , 2022.
[55]Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified
state space layers for sequence modeling. In International Conference on
Learning Representations , 2023.
21

--- PAGE 22 ---
[56]Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya
Ganguli. Deep unsupervised learning using nonequilibrium thermodynam-
ics. In International Conference on Machine Learning , pages 2256‚Äì2265,
2015.
[57]Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar,
Stefano Ermon, and Ben Poole. Score-based generative modeling through
stochastic differential equations. arXiv preprint arXiv:2011.13456 , 2021.
doi: 10.48550/arXiv.2011.13456.
[58]Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri,
Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald
Metzler. Long range arena: A benchmark for efficient transformers. In
International Conference on Learning Representations , 2021.
[59]Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan:
Decomposing motion and content for video generation. In Proceedings of
the IEEE conference on computer vision and pattern recognition , pages
1526‚Äì1535, 2018.
[60]Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael
Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate gen-
erative models of video: A new metric challenges. arXiv preprint
arXiv:1812.01717 , 2018.
[61]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N Gomez,  Lukasz Kaiser, and Illia Polosukhin. Attention
is all you need. In Advances in Neural Information Processing Systems ,
pages 5998‚Äì6008, 2017.
[62]Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked
conditional video diffusion for prediction, generation, and interpolation.
InAdvances in neural information processing systems , volume 35, pages
23371‚Äì23385, 2022.
[63]C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with
scene dynamics. In Advances in Neural Information Processing Systems
(NeurIPS) , 2016.
22

--- PAGE 23 ---
[64]Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed
Omar, and Raffay Hamid. Selective structured state-spaces for long-form
video understanding. In CVPR , 2023.
[65]Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush.
Pretraining without attention. arXiv preprint arXiv:2212.10544 , 2022.
[66]Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
Linformer: Self-attention with linear complexity, 2020.
[67]Jing Nathan Yan, Jiatao Gu, and Alexander M Rush. Diffusion models
without attention. arXiv preprint arXiv:2311.18257 , 2023.
[68]Wilson Yan et al. Videogpt: Video generation using vq-vae and trans-
formers. arXiv preprint arXiv:2104.10157 , 2021.
[69]Shengming Yin et al. Nuwa-xl: Diffusion over diffusion for extremely
long video generation. arXiv preprint arXiv:2303.12346 , 2023.
[70]Daquan Zhou et al. Magicvideo: Efficient video generation with latent
diffusion models. arXiv preprint arXiv:2211.11018 , 2022.
[71]Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano
Ermon. Deep latent state space models for time-series generation. In
International Conference on Machine Learning , 2023.
[72]Lianghui Zhu et al. Vision mamba: Efficient visual representation learning
with bidirectional state space model. arXiv preprint arXiv:2401.09417 ,
2024.
Appendix A. Experimental Settings
To ensure a fair comparison of the modules extracting the temporal relation-
ships under the same resolution settings, all configurations except for the
temporal layers, were the same in our experiments. We used NVIDIA A100
√ó8 (from a cloud provider). Hyperparameters, which are consistent across all
model sizes and datasets, are shown in Table A.5.
Appendix B. Additional Qualitative Results
Shown in Figure B.7, B.8, B.9.
23

--- PAGE 24 ---
Table A.5: Hyperparameters used in the experiments.
Parameter Value
UNet channel multipliers (1, 2, 4, 8)
Time embedding dimension 1024
Time embedding linears 2
Denoising timesteps ( T) 256
Loss type L2 loss of noise œµ
Training steps 100k
Optimizer Adam ( Œ≤1= 0.9,Œ≤2= 0.999)
Learning rate 0.00001
Train batch size 8
EMA decay 0.9999
ample 1
1 16 32 64 80 96 112 128 144 160ample 2
ample 1
ample 2
ample 1
ample 2
176 192 208 224 240 48 256
Figure B.7: Additional qualitative generation results in MineRL Navigate (# of frames are
256, 32 √ó32 resolution). Each column represents different samples.
24

--- PAGE 25 ---
ample 1
1 16 32 64 80 96 112 128 144 160ample 2
ample 1
ample 2
ample 1
ample 2
176 192 208 224 240 48 256
Figure B.8: Additional qualitative generation results in GQN-Mazes (# of frames are 256,
32√ó32 resolution). Each column represents different samples.
ample 1
1 16 32 64 80 96 112 128 144 160ample 2
ample 1
ample 2
ample 1
ample 2
176 192 208 224 240 48 256
Figure B.9: Additional qualitative generation results in CARLA Town01 (# of frames are
256, 32 √ó32 resolution). Each column represents different samples.
25
