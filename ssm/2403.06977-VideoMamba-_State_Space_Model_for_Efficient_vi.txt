# VideoMamba: Mô hình Không gian Trạng thái cho Hiểu Video Hiệu quả

Kunchang Li2,3,1♠, Xinhao Li4,1♠, Yi Wang1♡, Yinan He1
Yali Wang2,1♡, Limin Wang4,1♡, và Yu Qiao1♡
1OpenGVLab, Phòng thí nghiệm AI Thượng Hải
2Viện Công nghệ Tiên tiến Thâm Quyến, Viện Hàn lâm Khoa học Trung Quốc
3Đại học Viện Hàn lâm Khoa học Trung Quốc
4Phòng thí nghiệm Trọng điểm Nhà nước về Công nghệ Phần mềm Mới, Đại học Nam Kinh
https://github.com/OpenGVLab/VideoMamba

**Tóm tắt.** Giải quyết những thách thức kép về dư thừa cục bộ và phụ thuộc toàn cục trong hiểu video, công trình này sáng tạo thích ứng Mamba cho lĩnh vực video. VideoMamba được đề xuất vượt qua những hạn chế của các mạng nơ-ron tích chập 3D hiện có và các transformer video. Toán tử có độ phức tạp tuyến tính của nó cho phép mô hình hóa dài hạn hiệu quả, điều này rất quan trọng cho việc hiểu video dài có độ phân giải cao. Các đánh giá mở rộng tiết lộ bốn khả năng cốt lõi của VideoMamba: (1) Khả năng mở rộng trong lĩnh vực thị giác mà không cần tiền huấn luyện tập dữ liệu lớn, nhờ vào kỹ thuật tự chưng cất mới; (2) Độ nhạy trong việc nhận dạng các hành động ngắn hạn ngay cả với sự khác biệt chuyển động tinh vi; (3) Tính ưu việt trong hiểu video dài hạn, thể hiện những tiến bộ đáng kể so với các mô hình dựa trên đặc trưng truyền thống; và (4) Khả năng tương thích với các phương thức khác, chứng minh tính mạnh mẽ trong các bối cảnh đa phương thức. Thông qua những lợi thế riêng biệt này, VideoMamba thiết lập một tiêu chuẩn mới cho việc hiểu video, cung cấp một giải pháp có thể mở rộng và hiệu quả cho việc hiểu video toàn diện. Tất cả mã nguồn và mô hình đều có sẵn.

## 1 Giới thiệu

Mục tiêu cốt lõi của việc hiểu video nằm ở việc thành thạo các biểu diễn không-thời gian, vốn dĩ đặt ra hai thách thức ghê gớm: dư thừa không-thời gian lớn trong các clip video ngắn, và sự phụ thuộc không-thời gian phức tạp giữa các ngữ cảnh dài. Mặc dù các mạng nơ-ron tích chập 3D (CNNs) từng thống trị [9,19,76] và các transformer video [2,4] xử lý hiệu quả một trong những thách thức được đề cập bằng cách tận dụng tích chập cục bộ hoặc chú ý tầm xa, chúng không thể giải quyết cả hai cùng lúc. UniFormer [44] cố gắng tích hợp lợi thế của cả hai phương pháp, nhưng nó gặp khó khăn trong việc mô hình hóa video dài, điều này đã trở thành xu hướng chính trong nghiên cứu gần đây về hiểu video [48,72] và tạo sinh [5,92].

Sự xuất hiện của các toán tử chi phí thấp như S4 [26], RWKV [73], và RetNet [70] trong lĩnh vực NLP, đã tạo ra một con đường mới cho mô hình thị giác. Mamba [25] nổi bật với mô hình không gian trạng thái có chọn lọc (SSM), tạo cân bằng giữa việc duy trì độ phức tạp tuyến tính và tạo điều kiện cho việc mô hình hóa động lực dài hạn. Sự đổi mới này đã thúc đẩy việc áp dụng nó trong các tác vụ thị giác, như được chứng minh bởi Vision Mamba [91] và VMamba [50], những mô hình tận dụng SSM đa hướng cho việc xử lý hình ảnh 2D nâng cao. Những mô hình này cạnh tranh với các kiến trúc dựa trên chú ý về hiệu suất trong khi cung cấp giảm đáng kể việc sử dụng bộ nhớ.

Với các chuỗi vốn dĩ dài hơn được tạo ra bởi video, một câu hỏi tự nhiên nảy sinh: Liệu Mamba có thể hoạt động tốt cho việc hiểu video không?

Được truyền cảm hứng bởi điều này, chúng tôi giới thiệu VideoMamba, một mô hình hoàn toàn dựa trên SSM được thiết kế riêng cho việc hiểu video. VideoMamba hòa hợp một cách hài hòa điểm mạnh của tích chập và chú ý theo phong cách ViT vanilla [15]. Nó cung cấp một phương pháp có độ phức tạp tuyến tính cho việc mô hình hóa ngữ cảnh không-thời gian động, lý tưởng cho video dài có độ phân giải cao. Đánh giá liên quan tập trung vào bốn khả năng chính của VideoMamba:

**(1) Khả năng mở rộng trong Lĩnh vực Thị giác:** Chúng tôi kiểm tra khả năng mở rộng của VideoMamba và thấy rằng, trong khi mô hình Mamba thuần túy có xu hướng overfitting khi mở rộng, việc chúng tôi giới thiệu một chiến lược tự chưng cất đơn giản nhưng hiệu quả cho phép VideoMamba đạt được những cải thiện hiệu suất đáng kể khi kích thước mô hình và đầu vào tăng lên, mà không cần tiền huấn luyện tập dữ liệu quy mô lớn.

**(2) Độ nhạy cho Nhận dạng Hành động Ngắn hạn:** Phân tích của chúng tôi mở rộng đến việc đánh giá khả năng của VideoMamba trong việc phân biệt chính xác các hành động ngắn hạn, đặc biệt là những hành động có sự khác biệt chuyển động tinh vi, ví dụ như mở và đóng. Các phát hiện cho thấy hiệu suất vượt trội của VideoMamba so với các mô hình dựa trên chú ý hiện có [2,4,52]. Quan trọng hơn, nó cũng phù hợp với việc mô hình hóa có che dấu, điều này càng tăng cường độ nhạy thời gian của nó.

**(3) Tính ưu việt trong Hiểu Video Dài hạn:** Sau đó chúng tôi đánh giá khả năng của VideoMamba trong việc diễn giải video dài. Nó thể hiện tính ưu việt đáng kể so với các phương pháp dựa trên đặc trưng thông thường [35,47] thông qua huấn luyện end-to-end. Đáng chú ý, VideoMamba hoạt động nhanh hơn 6× so với TimeSformer [4] và yêu cầu ít bộ nhớ GPU hơn 40× cho video 64 khung hình (xem Hình 1).

**(4) Khả năng tương thích với Các phương thức Khác:** Cuối cùng, chúng tôi đánh giá khả năng thích ứng của VideoMamba với các phương thức khác. Kết quả trong việc truy xuất video-văn bản cho thấy hiệu suất cải thiện so với ViT, đặc biệt trong video dài với các tình huống phức tạp. Điều này nhấn mạnh tính mạnh mẽ và khả năng tích hợp đa phương thức của nó.

Tóm lại, các thí nghiệm sâu rộng của chúng tôi tiết lộ tiềm năng to lớn của VideoMamba trong việc hiểu nội dung video cả ngắn hạn (K400 [36] và SthSthV2 [24]) và dài hạn (Breakfast [37], COIN [71], và LVU [84]). Với hiệu quả và hiệu suất của nó, VideoMamba sẵn sàng trở thành nền tảng trong lĩnh vực hiểu video dài. Tất cả mã nguồn và mô hình được mở nguồn để thúc đẩy các nỗ lực nghiên cứu tương lai.

## 2 Các Công trình Liên quan

### 2.1 Mô hình Không gian Trạng thái

Gần đây, các Mô hình Không gian Trạng thái (SSMs) đã cho thấy hiệu quả đáng kể của việc biến đổi không gian trạng thái trong việc nắm bắt động lực và sự phụ thuộc của các chuỗi ngôn ngữ. [26] giới thiệu một mô hình chuỗi không gian trạng thái có cấu trúc (S4), được thiết kế đặc biệt để mô hình hóa sự phụ thuộc tầm xa, tự hào về lợi thế của độ phức tạp tuyến tính. Dựa trên đó, nhiều mô hình khác được phát triển (ví dụ, S5 [66], H3 [20] và GSS [56]), và Mamba [25] nổi bật bằng việc giới thiệu một lớp SSM phụ thuộc dữ liệu và một cơ chế lựa chọn sử dụng quét song song (S6). So với các transformer [6,54] dựa trên chú ý có độ phức tạp bậc hai, Mamba xuất sắc trong việc xử lý các chuỗi dài với độ phức tạp tuyến tính.

Trong lĩnh vực thị giác, [26] đầu tiên áp dụng SSM trong phân loại hình ảnh ở mức pixel, và [35] sử dụng S4 để xử lý các sự phụ thuộc thời gian tầm xa cho phân loại clip phim. Bên cạnh đó, tiềm năng lớn của Mamba thúc đẩy một loạt công trình [28,30,46,50,78,87,91], chứng minh hiệu suất tốt hơn và hiệu quả GPU cao hơn của Mamba so với Transformer trong các tác vụ thị giác như phát hiện đối tượng và phân đoạn ngữ nghĩa. Khác với các công trình trước đó, VideoMamba của chúng tôi là một mô hình video hoàn toàn dựa trên SSM, thể hiện hiệu quả và hiệu suất tuyệt vời cho cả hiểu video ngắn hạn và dài hạn.

### 2.2 Hiểu Video

Hiểu video đứng như một nền tảng trong lĩnh vực thị giác máy tính, tầm quan trọng của nó được khuếch đại thêm bởi sự phát triển mạnh mẽ của các nền tảng video ngắn. Để củng cố lĩnh vực này, nhiều tập dữ liệu được trang bị dữ liệu mở rộng và chú thích con người tỉ mỉ đã được phát triển, nhằm nâng cao khả năng nhận dạng hành động con người. Các ví dụ đáng chú ý bao gồm UCF101 [67] và tập dữ liệu Kinetics [7,8,36], đã đóng vai trò then chốt trong việc đánh giá tiến bộ. Hơn nữa, các tập dữ liệu khác [22,27,31,34,49,62] cung cấp video hoạt động được chú thích được thiết kế riêng cho việc định vị hành động, thúc đẩy nghiên cứu sâu hơn về hoạt động con người. Ngoài nhận dạng hành động, sự xuất hiện của các tập dữ liệu video-văn bản quy mô lớn [10,12,57,82,86,88] mở rộng tiện ích của việc hiểu video vào lĩnh vực tác vụ đa phương thức, như mô tả video, truy xuất và trả lời câu hỏi, do đó mở rộng phổ ứng dụng.

Về mặt kiến trúc, nó đã phát triển từ việc sử dụng CNN để trích xuất đặc trưng từ các khung hình video, đến các kỹ thuật tiên tiến hơn. Ban đầu, CNN 3D [9,17,76,77] mở rộng kiến trúc CNN 2D truyền thống để nắm bắt thông tin không-thời gian của video. Two-Stream [65], kết hợp luồng không gian và thời gian, TSN [80], đề xuất lấy mẫu thưa thớt, và SlowFast [19], sử dụng mạng song song để nắm bắt ngữ nghĩa và chuyển động nhanh, càng tăng cường khả năng nhận dạng hành động. Việc giới thiệu các mô hình dựa trên chú ý [2,4,59,63,89], như TimeSformer [4] và ViViT [2], đánh dấu một tiến bộ đáng kể bằng việc nắm bắt hiệu quả các sự phụ thuộc tầm xa trong chuỗi video, tăng cường hiểu biết về mối quan hệ thời gian. Các phát triển gần đây [42,44,52,83] tập trung vào transformer video chính xác, với các đổi mới như chú ý cửa sổ của VideoSwin [52] và việc tích hợp cơ chế tích chập và tự chú ý của UniFormer [44], nhằm cân bằng hiệu quả tính toán với hiệu suất. Mặc dù những thành tựu của các mô hình này trong nhiều tác vụ khác nhau, chúng thường đi kèm với chi phí tính toán cao cho các chuỗi dài. Ngược lại, VideoMamba của chúng tôi giới thiệu một toán tử có độ phức tạp tuyến tính cho việc mô hình hóa dài hạn hiệu quả, vượt trội hơn các phương pháp hiện có với tốc độ nhanh hơn và tiêu thụ GPU thấp hơn.

## 3 Phương pháp

### 3.1 Kiến thức Cơ bản

**SSM cho chuỗi 1D.** Mô hình Không gian Trạng thái (SSMs) được khái niệm hóa dựa trên các hệ thống liên tục ánh xạ một hàm hoặc chuỗi 1D, x(t) ∈ RL → y(t) ∈ RL thông qua một trạng thái ẩn h(t) ∈ RN. Chính thức, SSMs sử dụng phương trình vi phân thông thường (ODE) sau để mô hình hóa dữ liệu đầu vào:

h'(t) = Ah(t) + Bx(t), (1)
y(t) = Ch(t), (2)

trong đó A ∈ RN×N đại diện cho ma trận tiến hóa của hệ thống, và B ∈ RN×1, C ∈ RN×1 là các ma trận chiếu. ODE liên tục này được xấp xỉ thông qua rời rạc hóa trong các SSM hiện đại. Mamba [25] là một trong những phiên bản rời rạc của hệ thống liên tục, bao gồm một tham số thang thời gian Δ để biến đổi các tham số liên tục A, B thành các đối tác rời rạc A̅, B̅. Việc biến đổi thường sử dụng phương pháp zero-order hold (ZOH), được định nghĩa bởi:

A̅ = exp(ΔA), (3)
B̅ = (ΔA)^(-1)(exp(ΔA) - I) · ΔB (4)
ht = A̅ht-1 + B̅xt, (5)
yt = Cht. (6)

Trái ngược với các mô hình truyền thống chủ yếu dựa vào SSM bất biến thời gian tuyến tính, Mamba tự phân biệt bằng việc triển khai một Cơ chế Quét Có chọn lọc (S6) làm toán tử SSM cốt lõi. Trong S6, các tham số B ∈ RB×L×N, C ∈ RB×L×N, và Δ ∈ RB×L×D được suy ra trực tiếp từ dữ liệu đầu vào x ∈ RB×L×D, chỉ ra khả năng nhạy cảm ngữ cảnh nội tại và điều chế trọng số thích ứng. Hình 2a cho thấy chi tiết của khối Mamba.

**SSM Hai chiều cho Thị giác.** Khối Mamba gốc, được thiết kế cho chuỗi 1D, không đáp ứng được các tác vụ thị giác yêu cầu nhận thức không gian. Xây dựng trên điều này, Vision Mamba giới thiệu một khối Mamba hai chiều (B-Mamba) trong Hình 2b, thích ứng mô hình hóa chuỗi hai chiều cho các ứng dụng đặc thù thị giác. Khối này xử lý các chuỗi thị giác được làm phẳng thông qua SSM tiến và lùi đồng thời, tăng cường khả năng xử lý có nhận thức không gian. Trong công trình này, chúng tôi mở rộng khối B-Mamba cho việc hiểu video 3D.

### 3.2 VideoMamba

**Tổng quan.** Hình 3 minh họa khung tổng thể của VideoMamba. Cụ thể, trước tiên chúng tôi sử dụng tích chập 3D (tức là 1×16×16) để chiếu video đầu vào Xv ∈ R3×T×H×W thành L patch không-thời gian không chồng lấp Xp ∈ RL×C, trong đó L = t×h×w (t = T, h = H/16, và w = W/16). Chuỗi token đầu vào cho bộ mã hóa VideoMamba tiếp theo là

X = [Xcls, X] + ps + pt, (7)

trong đó Xcls là một token phân loại có thể học được được thêm vào đầu chuỗi. Theo các công trình trước [2,4,15], chúng tôi đã thêm một embedding vị trí không gian có thể học được ps ∈ R(hw+1)×C và embedding thời gian bổ sung pt ∈ Rt×C để giữ lại thông tin vị trí không-thời gian, vì việc mô hình hóa SSM nhạy cảm với vị trí token. Các token X sau đó được truyền qua L khối B-Mamba xếp chồng, và biểu diễn của token [CLS] ở lớp cuối được xử lý bởi lớp chuẩn hóa và tuyến tính cho phân loại.

**Quét Không-thời gian.** Để áp dụng lớp B-Mamba cho đầu vào không-thời gian, chúng tôi mở rộng quét 2D ban đầu thành các quét 3D hai chiều khác nhau trong Hình 4: (a) Không gian-Trước, tổ chức các token không gian theo vị trí sau đó xếp chúng từng khung một; (b) Thời gian-Trước, sắp xếp các token thời gian dựa trên khung sau đó xếp theo chiều không gian; (c) Không-thời gian, một lai của cả Không gian-Trước và Thời gian-Trước, với v1 thực hiện một nửa trong số chúng và v2 thực hiện đầy đủ (2× tính toán). Hơn nữa, các thí nghiệm của chúng tôi trong Hình 7a chứng minh rằng quét hai chiều Không gian-Trước là hiệu quả nhất nhưng đơn giản. Nhờ vào độ phức tạp tuyến tính của Mamba, VideoMamba của chúng tôi có khả năng xử lý video dài có độ phân giải cao một cách hiệu quả.

**So sánh với Vim [91] và VMamba [50].** VideoMamba của chúng tôi xây dựng trên Vim, nhưng tinh giản kiến trúc bằng cách bỏ qua các tính năng như token [CLS] ở giữa và Rotary Position Embedding (RoPE [68]), dẫn đến hiệu suất vượt trội trên ImageNet-1K với mức cải thiện +0.8% và +0.7% cho Vim-Ti và Vim-S, tương ứng. Không giống như VMamba, tích hợp tích chập sâu bổ sung, VideoMamba tuân thủ nghiêm ngặt thiết kế ViT mà không có các lớp downsampling. Để chống lại các vấn đề overfitting quan sát thấy trong VMamba, chúng tôi giới thiệu một kỹ thuật tự chưng cất hiệu quả được phác thảo trong Phần 3.3, chứng minh khả năng mở rộng tuyệt vời của VideoMamba đẳng hướng cho các tác vụ hình ảnh và video.

**So sánh với TimeSformer [4] và ViViT [2].** Các mô hình dựa trên chú ý truyền thống như TimeSformer và ViViT đã giải quyết độ phức tạp bậc hai của cơ chế tự chú ý bằng cách áp dụng chú ý không-thời gian chia tách. Mặc dù hiệu quả hơn, nó giới thiệu các tham số bổ sung và kém hiệu quả so với chú ý chung, đặc biệt trong các tình huống liên quan đến tiền huấn luyện che dấu [43,74]. Ngược lại, VideoMamba xử lý các token không-thời gian với độ phức tạp tuyến tính, vượt trội hơn TimeSformer trên Kinetics-400 với +2.6% và tạo ra những bước tiến đáng kể trên SthSthV2 với cải thiện +5.9% (xem Bảng 3 và 4). Hơn nữa, VideoMamba đạt được tăng tốc 6× trong tốc độ xử lý và yêu cầu ít bộ nhớ GPU hơn 40× cho video dài, như được chi tiết trong Hình 1, chứng minh hiệu quả và hiệu suất của nó trong việc xử lý các tác vụ video dài.

### 3.3 Kiến trúc

Đối với SSM trong lớp B-Mamba, chúng tôi áp dụng các siêu tham số mặc định như trong Mamba [25], thiết lập chiều trạng thái và tỷ lệ mở rộng lần lượt là 16 và 2. Theo ViT [15], chúng tôi điều chỉnh độ sâu và chiều embedding để tạo ra các mô hình có kích thước tương đương trong Bảng 1, bao gồm VideoMamba-Ti, VideoMamba-S và VideoMamba-M. Tuy nhiên, chúng tôi quan sát thấy rằng VideoMamba lớn hơn có xu hướng overfitting trong các thí nghiệm của chúng tôi, dẫn đến hiệu suất không tối ưu như được minh họa trong Hình 6a. Vấn đề overfitting này không chỉ duy nhất đối với các mô hình của chúng tôi mà còn được tìm thấy trong VMamba [50], nơi hiệu suất tối ưu của VMamba-B đạt được ở ba phần tư tổng số epoch huấn luyện. Để chống lại overfitting trong các mô hình Mamba lớn hơn, chúng tôi giới thiệu một chiến lược Tự Chưng cất hiệu quả, sử dụng một mô hình nhỏ hơn và được huấn luyện tốt làm "giáo viên" để hướng dẫn việc huấn luyện mô hình "học sinh" lớn hơn. Kết quả, được miêu tả trong Hình 6a, cho thấy rằng chiến lược này dẫn đến sự hội tụ tốt hơn như mong đợi.

### 3.4 Mô hình Che dấu

Gần đây, VideoMAE và ST-MAE [18,74] đã thể hiện những lợi ích đáng kể của việc mô hình che dấu trong việc nâng cao khả năng hiểu thời gian TINH VI của mô hình. UMT [43] đi xa hơn bằng cách giới thiệu một kỹ thuật căn chỉnh che dấu hiệu quả tạo ra kết quả mạnh mẽ trên các tác vụ video đơn và đa phương thức. Để tăng cường độ nhạy thời gian của VideoMamba và xác minh khả năng thích ứng của nó với các phương thức văn bản, chúng tôi áp dụng một phương pháp căn chỉnh che dấu được truyền cảm hứng từ UMT. Đầu tiên, VideoMamba được huấn luyện từ đầu chỉ trên dữ liệu video, căn chỉnh các token không che dấu với những token từ CLIP-ViT. Tiếp theo, nó được tích hợp với một bộ mã hóa văn bản và một bộ giải mã đa phương thức (tức là BERT [14]), để tiền huấn luyện trên cả tập dữ liệu hình ảnh-văn bản và video-văn bản.

Điều quan trọng cần lưu ý sự khác biệt với UMT, mô hình này sử dụng căn chỉnh đa lớp giữa các mô hình học sinh và giáo viên. Ngược lại, do kiến trúc độc đáo của VideoMamba (SSM so với Transformer), chúng tôi chỉ căn chỉnh các đầu ra cuối cùng. Về chiến lược che dấu của chúng tôi, chúng tôi đề xuất các kỹ thuật che dấu hàng khác nhau, được miêu tả trong Hình 5, được thiết kế riêng cho sở thích của khối B-Mamba đối với các token liên tục. Ngoài ra, chúng tôi khám phá việc che dấu chú ý để bảo tồn tính liền kề có ý nghĩa giữa các token, tận dụng điểm mạnh vốn có của tích chập 1D trong khối B-Mamba để cải thiện hiệu suất.

## 4 Thí nghiệm

### 4.1 Mở rộng Quy mô

**Tập dữ liệu và Cài đặt.** Trước tiên chúng tôi tiến hành thí nghiệm trên ImageNet-1K [13], bao gồm 1.28M hình ảnh huấn luyện và 50K hình ảnh xác thực trên 1,000 danh mục. Để so sánh công bằng, chúng tôi tuân theo hầu hết các chiến lược huấn luyện được đề xuất trong DeiT [75], nhưng áp dụng tăng cường dữ liệu yếu hơn cho biến thể mô hình tiny. Hơn nữa, chúng tôi điều chỉnh tỷ lệ độ sâu ngẫu nhiên lần lượt là 0/0.15/0.5 cho VideoMamba-Ti/S/M. Các mô hình của chúng tôi được huấn luyện sử dụng bộ tối ưu AdamW kết hợp với lịch trình tốc độ học cosine trong 300 epoch. 5 epoch đầu tiên phục vụ như một giai đoạn khởi động tuyến tính. Cài đặt mặc định cho tốc độ học, suy giảm trọng số và kích thước lô lần lượt là 1e-3, 0.05 và 1024. Hơn nữa, chúng tôi sử dụng độ chính xác BFloat16 trong quá trình huấn luyện để tăng cường ổn định mà không dựa vào EMA. Đối với mô hình VideoMamba-M, chúng tôi sử dụng một mô hình VideoMamba-S đã được tiền huấn luyện làm "giáo viên" để hướng dẫn quá trình huấn luyện bằng cách căn chỉnh các bản đồ đặc trưng cuối cùng thông qua mất mát L2. Đối với việc tinh chỉnh độ phân giải lớn (>224), chúng tôi sử dụng tốc độ học giảm (5e-6) và suy giảm trọng số tối thiểu (1e-8) cho 30 epoch.

**Hiệu quả của Tự Chưng cất.** Hình 6a cho thấy rằng khi huấn luyện từ đầu, VideoMamba-B có xu hướng overfitting dễ dàng hơn và kém hiệu quả so với VideoMamba-S, trong khi VideoMamba-M đạt hiệu suất tương tự. May mắn thay, tự chưng cất của chúng tôi đã được chứng minh là hiệu quả trong việc đạt được tối ưu hóa mong muốn với chi phí tính toán bổ sung không đáng kể. Để giảm thiểu sự quá định hướng của giáo viên, chúng tôi đã thử nghiệm với dừng sớm [11] trong Hình 6b, mặc dù nó không mang lại kết quả có lợi. Những phát hiện này chỉ ra rằng tự chưng cất cung cấp một chiến lược khả thi để tăng cường khả năng mở rộng của kiến trúc Mamba mà không có chi phí tính toán đáng kể.

**Kết quả.** Bảng 2 thể hiện kết quả trên tập dữ liệu ImageNet-1K. Đáng chú ý, VideoMamba-M vượt trội hơn các kiến trúc đẳng hướng khác với biên độ đáng kể, đạt được cải thiện +0.8% so với ConvNeXt-B [53] và tăng +2.0% so với DeiT-B [75], trong khi sử dụng ít tham số hơn. Ngoài ra, VideoMamba-M giữ vững vị thế của mình trước các backbone không đẳng hướng tận dụng các đặc trưng phân cấp để tăng cường hiệu suất. Với hiệu quả của Mamba trong việc xử lý chuỗi dài, chúng tôi tiếp tục tăng cường hiệu suất bằng cách tăng độ phân giải, đạt được độ chính xác top-1 là 84.0% chỉ với 74M tham số. Cải thiện đáng kể này mở rộng sang các tác vụ video, như được chi tiết trong Phần 4.2, nhấn mạnh hiệu quả và khả năng mở rộng của VideoMamba.

### 4.2 Hiểu Video Ngắn hạn

**Tập dữ liệu và Cài đặt.** Chúng tôi đánh giá VideoMamba của chúng tôi trên Kinetics-400 [36] liên quan đến cảnh phổ biến và Something-Something V2 [24] liên quan đến thời gian, với độ dài video trung bình lần lượt là 10s và 4s. Đối với tiền huấn luyện có giám sát, chúng tôi tinh chỉnh những mô hình được tiền huấn luyện trên ImageNet-1K với cùng chiến lược huấn luyện như VideoMAE [74]. Cụ thể, đối với VideoMamba-M, epoch khởi động, tổng epoch, tỷ lệ độ sâu ngẫu nhiên, suy giảm trọng số được thiết lập lần lượt là 5, 50, 0.8, 0.05 cho K400, và 5, 30, 0.8, 0.05 cho SthSth. Đối với các mô hình nhỏ hơn, tất cả các siêu tham số đều giống nhau trừ khi chúng tôi giảm tỷ lệ độ sâu ngẫu nhiên và tăng epoch huấn luyện. Hơn nữa, chúng tôi chia tỷ lệ tuyến tính tốc độ học cơ bản theo kích thước lô, là 2e−4·batchsize/256 cho K400 và 4e−4·batchsize/256 cho SthSth. Đối với tiền huấn luyện tự giám sát, chúng tôi áp dụng công thức huấn luyện như trong UMT [43], sử dụng CLIP-ViT-B [60] để chưng cất VideoMamba-M trong 800 epoch. Trong quá trình tinh chỉnh, chúng tôi sử dụng các siêu tham số tương tự như đã đề cập nhưng chọn tỷ lệ độ sâu ngẫu nhiên và tốc độ học nhỏ cho cả hai tập dữ liệu.

**Kết quả.** Bảng 3 và 4 liệt kê kết quả trên các tập dữ liệu video ngắn hạn. **(a) Có giám sát:** So với các phương pháp hoàn toàn dựa trên chú ý [2,4], VideoMamba-M dựa trên SSM của chúng tôi đảm bảo một lợi thế đáng chú ý, vượt trội hơn ViViT-L [2] với +2.0% và +3.0% trên K400 liên quan đến cảnh và SthSthV2 liên quan đến thời gian, tương ứng. Cải thiện này đi kèm với yêu cầu tính toán giảm đáng kể và ít dữ liệu tiền huấn luyện hơn. Hơn nữa, VideoMamba-M mang lại kết quả ngang bằng với SOTA UniFormer [44], mô hình khéo léo tích hợp tích chập với chú ý trong cấu trúc không đẳng hướng. **(b) Tự giám sát:** Hiệu suất của VideoMamba dưới tiền huấn luyện che dấu vượt qua VideoMAE [74], được biết đến với khả năng thành thạo trong hành động tinh vi. Thành tựu này nhấn mạnh tiềm năng của mô hình hoàn toàn dựa trên SSM của chúng tôi trong việc hiểu video ngắn hạn một cách hiệu quả và hiệu suất, làm nổi bật tính phù hợp của nó cho cả mô hình học có giám sát và tự giám sát.

**Nghiên cứu Ablation.** Thông qua các nghiên cứu ablation toàn diện được chi tiết trong Hình 7 và Bảng 5, chúng tôi khám phá các khía cạnh khác nhau của mô hình. **(a) Loại Quét:** Trong tất cả các phương pháp, phương pháp không gian-trước nổi lên là hiệu quả nhất, ngược lại, chiến lược thời gian-trước là tệ nhất. Tính ưu việt của phương pháp không gian-trước được quy cho khả năng tận dụng liền mạch kiến thức tiền huấn luyện 2D bằng cách quét từng khung một. **(b) Khung hình và Độ phân giải:** Trái ngược với các phát hiện từ ImageNet (xem Bảng 2), độ phân giải cao hơn không đồng nhất dẫn đến hiệu suất tốt hơn. Tăng số lượng khung hình liên tục tăng cường kết quả trên tập dữ liệu K400. Tuy nhiên, điều này không xảy ra với SthSthV2, có thể do thời lượng ngắn của video, có thể không chứa được đầu vào dài hơn một cách hiệu quả. **(c) Tiền huấn luyện Che dấu:** Những phát hiện của chúng tôi tiết lộ rằng che dấu hàng, đặc biệt tương thích với tích chập 1D, vượt trội hơn che dấu ngẫu nhiên và ống thường dùng. Che dấu clip-hàng xuất sắc do mức độ ngẫu nhiên cao hơn. Hơn nữa, che dấu chú ý nổi bật là hiệu quả nhất bằng cách ưu tiên việc bảo tồn nội dung liền kề có ý nghĩa. Chỉ căn chỉnh đầu ra cuối cùng của mô hình chứng minh hiệu quả nhất, có thể do sự khác biệt kiến trúc. Cuối cùng, tỷ lệ che dấu tối ưu (80%) kết hợp với điều chuẩn mạnh hơn có lợi đáng kể cho VideoMamba trong quá trình tiền huấn luyện che dấu.

### 4.3 Hiểu Video Dài hạn

**Tập dữ liệu và Cài đặt.** Chúng tôi đánh giá nghiêm ngặt khả năng thành thạo của VideoMamba trong việc xử lý video dài hạn bằng cách tận dụng ba tập dữ liệu toàn diện, tức là Breakfast [37], COIN [71] và benchmark Long-form Video Understanding (LVU [84]). Cụ thể, Breakfast bao gồm 1,712 video, gói gọn 10 hoạt động nấu ăn phức tạp trong 77 giờ. COIN có 11,827 video trên 180 nhiệm vụ thủ tục độc đáo, với thời lượng trung bình là 2.36 phút. Benchmark LVU bao gồm khoảng 30K clip phim, kéo dài từ 1 đến 3 phút, và bao quát chín nhiệm vụ trên 3 danh mục chính: hiểu nội dung, dự đoán metadata và tương tác người dùng. Đối với nhiệm vụ hồi quy trong số này, chúng tôi đánh giá bằng sai số bình phương trung bình, trong khi đối với các nhiệm vụ phân loại, độ chính xác là thước đo lựa chọn. Trái ngược với các nghiên cứu trước [35,47] dựa vào các đặc trưng được suy ra từ các mô hình video tiền huấn luyện, như Swin-B [51] được huấn luyện trên Kinetics-600, phương pháp của chúng tôi sử dụng huấn luyện end-to-end như được chi tiết trong Phần 4.2. Ngoài ra, để so sánh công bằng, chúng tôi tinh chỉnh các mô hình được tiền huấn luyện của chúng tôi trên K400.

**Kết quả.** Như được minh họa trong Hình 1, độ phức tạp tuyến tính của VideoMamba làm cho nó rất phù hợp cho huấn luyện end-to-end với video có thời lượng dài. Các so sánh trong Bảng 6 và 7 làm nổi bật sự đơn giản và hiệu quả của VideoMamba trước các phương pháp dựa trên đặc trưng truyền thống [35,47] trong những nhiệm vụ này. Nó mang lại những cải thiện hiệu suất đáng kể, đạt được kết quả SOTA ngay cả với kích thước mô hình nhỏ hơn. Ví dụ, VideoMamba-Ti cho thấy mức tăng đáng chú ý +6.1% so với ViS4mer sử dụng đặc trưng Swin-B và mức tăng +3.0% so với phương pháp căn chỉnh đa phương thức của Turbo [29]. Đáng chú ý, kết quả nhấn mạnh tác động tích cực của việc mở rộng mô hình và số lượng khung hình cho các nhiệm vụ dài hạn. Trong tập hợp đa dạng và thách thức gồm chín nhiệm vụ được trình bày bởi LVU, VideoMamba-Ti của chúng tôi, được tinh chỉnh theo cách end-to-end, mang lại kết quả xuất sắc hoặc có thể so sánh với các phương pháp SOTA hiện tại. Những kết quả này không chỉ làm nổi bật hiệu quả của VideoMamba mà còn tiềm năng to lớn của nó cho việc hiểu video dài trong tương lai.

### 4.4 Hiểu Video Đa phương thức

**Tập dữ liệu và Cài đặt.** Theo UMT [43], chúng tôi sử dụng các cặp video-văn bản WebVid-2M [3] và các cặp hình ảnh-văn bản CC3M [64] để tiền huấn luyện chung với bốn mục tiêu: học tương phản thị giác-văn bản [3], khớp thị giác-văn bản [40], mô hình ngôn ngữ che dấu [14] và căn chỉnh token không che dấu [43]. Ban đầu, chúng tôi che dấu 50% token hình ảnh và 80% token video, tiến hành tiền huấn luyện trên 8 khung hình trong 10 epoch. Với độ nhạy của Mamba đối với thông tin vị trí, một giai đoạn tinh chỉnh không che dấu bổ sung được thực hiện trong một epoch để tinh chỉnh thêm khả năng hiểu của nó. Để đánh giá, chúng tôi thực hiện các nhiệm vụ truy xuất video-văn bản zero-shot trên năm benchmark nổi tiếng, bao gồm MSRVTT [86], DiDeMo [1], ActivityNet [31], LSMDC [61], và MSVD [10].

**Kết quả.** Như được chỉ ra trong Bảng 8, dưới cùng khối dữ liệu tiền huấn luyện và chiến lược huấn luyện tương tự, VideoMamba của chúng tôi đạt được hiệu suất truy xuất video zero-shot vượt trội so với UMT [43] dựa trên ViT [15]. Nó nhấn mạnh hiệu quả và khả năng mở rộng có thể so sánh của Mamba với ViT trong việc xử lý các nhiệm vụ video đa phương thức. Đáng chú ý, đối với các tập dữ liệu có độ dài video dài hơn (ví dụ, ANet và DiDeMo) và các tình huống phức tạp hơn (ví dụ, LSMDC), VideoMamba chứng minh một cải thiện đáng kể. Điều này chứng minh khả năng thích ứng của Mamba đối với yêu cầu căn chỉnh đa phương thức ngay cả trong các bối cảnh đa phương thức đầy thách thức.

## 5 Kết luận

Trong bài báo này, chúng tôi đề xuất VideoMamba, một mô hình hoàn toàn dựa trên SSM cho việc hiểu video hiệu quả. Các thí nghiệm mở rộng của chúng tôi chứng minh khả năng mở rộng của nó trong lĩnh vực thị giác, độ nhạy cho nhận dạng hành động ngắn hạn, tính ưu việt trong hiểu video dài hạn và khả năng tương thích với các phương thức khác. Chúng tôi hy vọng nó có thể mở đường cho thiết kế mô hình tương lai cho việc hiểu video dài.

**Hạn chế.** Do hạn chế về tài nguyên, chúng tôi chưa xác thực đầy đủ khả năng mở rộng của VideoMamba, như mở rộng VideoMamba lên kích thước lớn hơn (ví dụ, VideoMamba-g), tích hợp các phương thức bổ sung (ví dụ, âm thanh), và tích hợp với các mô hình ngôn ngữ lớn cho việc hiểu video cấp độ giờ. Mặc dù có những hạn chế này, các phát hiện của chúng tôi xác nhận tiềm năng đầy hứa hẹn của VideoMamba và chúng tôi dự định tiến hành các khám phá kỹ lưỡng về khả năng của nó trong tương lai.
