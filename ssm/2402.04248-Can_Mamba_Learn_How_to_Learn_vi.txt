# 2402.04248.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2402.04248.pdf
# Kích thước tệp: 1263604 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Liệu Mamba Có Thể Học Cách Học Không?
Một Nghiên Cứu So Sánh về Các Tác Vụ Học Trong Ngữ Cảnh
Jongho Park1, Jaeseung Park2*, Zheyang Xiong3, Nayoung Lee3, Jaewoong Cho1,
Samet Oymak4, Kangwook Lee1,3, Dimitris Papailiopoulos1,3
1KRAFTON,2Seoul National University,
3University of Wisconsin-Madison,4University of Michigan, Ann Arbor
Tóm tắt
Các mô hình không gian trạng thái (SSM), như Mamba (Gu & Dao, 2023), đã được đề xuất như các lựa chọn thay thế cho mạng Transformer trong mô hình hóa ngôn ngữ, bằng cách kết hợp gating, tích chập và lựa chọn token phụ thuộc đầu vào để giảm thiểu chi phí bậc hai của cơ chế chú ý đa đầu. Mặc dù SSM thể hiện hiệu suất cạnh tranh, khả năng học trong ngữ cảnh (ICL) của chúng, một thuộc tính nổi bật đáng chú ý của các mô hình ngôn ngữ hiện đại cho phép thực thi tác vụ mà không cần tối ưu hóa tham số, vẫn chưa được khám phá đầy đủ so với Transformer. Trong nghiên cứu này, chúng tôi đánh giá hiệu suất ICL của SSM, tập trung vào Mamba, so với các mô hình Transformer trên nhiều tác vụ khác nhau. Kết quả của chúng tôi cho thấy SSM hoạt động tương đương với Transformer trong các tác vụ ICL hồi quy tiêu chuẩn, trong khi vượt trội hơn chúng trong các tác vụ như học parity thưa. Tuy nhiên, SSM kém hiệu quả trong các tác vụ liên quan đến chức năng truy xuất không tiêu chuẩn. Để giải quyết những hạn chế này, chúng tôi giới thiệu một mô hình lai, MambaFormer, kết hợp Mamba với các khối chú ý, vượt qua các mô hình riêng lẻ trong những tác vụ mà chúng gặp khó khăn độc lập. Những phát hiện của chúng tôi cho rằng các kiến trúc lai cung cấp những hướng đi đầy triển vọng để nâng cao ICL trong các mô hình ngôn ngữ.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) hiện đại thể hiện khả năng học trong ngữ cảnh (ICL) đáng chú ý, cho phép chúng học các tác vụ mới với một vài minh họa và không cần tinh chỉnh trọng số thêm. Mặc dù cơ chế xuất hiện chính xác của những khả năng này cần được điều tra lý thuyết và thực nghiệm thêm (Chan et al., 2022; Wei et al., 2022; Min et al., 2022b; Schaeffer et al., 2023), các thí nghiệm trên các mô hình dựa trên Transformer lớn hơn liên tục chứng minh rằng khả năng ICL của chúng cải thiện khi loss huấn luyện giảm (Brown et al., 2020; Kaplan et al., 2020; Muennighoff et al., 2023).

Meta-learning, hay "học cách học", đã được nghiên cứu rộng rãi (Schmidhuber et al., 1997; Ravi & Larochelle, 2016) và gần đây lại thu hút sự quan tâm trong bối cảnh ICL, đặc biệt liên quan đến các mô hình Transformer (Vaswani et al., 2017). Ví dụ, Garg et al. (2022) đã đề xuất nhiều tác vụ ICL khác nhau, chẳng hạn như học hồi quy tuyến tính, và đánh giá khả năng của transformer để thực hiện chúng khi được huấn luyện cụ thể để làm như vậy. Mặt khác, Min et al. (2022a) nghiên cứu việc tinh chỉnh các mô hình ngôn ngữ để học một cách rõ ràng và thực hiện ICL. Theo những bước chân này, nhiều nghiên cứu đã được dành để hiểu cơ chế của Attention cho phép những khả năng meta-learning như vậy, thông qua các lập luận xây dựng hoặc điều tra thực nghiệm rộng rãi (Akyürek et al., 2022; Liu et al., 2022; Bai et al., 2023; Giannou et al., 2023; Li et al., 2023a; von Oswald et al., 2023a,b; Yang et al., 2023a; Zhou et al., 2023).

*Công việc này được thực hiện trong thời gian thực tập tại KRAFTON.
Email: <jongho.park@krafton.com> . Liên hệ: <dimitris@papail.io>
Preprint. Đang được xem xét.arXiv:2402.04248v2  [cs.LG]  25 Apr 2024

--- TRANG 2 ---
Transformer Mamba MambaFormer
Hồi quy tuyến tính ✓ ✓ ✓
Hồi quy tuyến tính thưa ✓ ✓ ✓
Hồi quy 2NN ✓ ✓ ✓
Cây quyết định ✓ ▲ ✓
Hồi quy outlier trực giao ✓ ▲ ✓
Hồi quy nhiều outlier ▲ ✓ ✓
Parity thưa ✗ ✓ ✓
Chain-of-Thought I/O ✓ ✓ ✓
Vector-valued MQAR ✓ ✗ ✓

Bảng 1: Hiệu suất mô hình trên các tác vụ ICL khác nhau. Chúng tôi gắn nhãn hiệu suất của mô hình bằng ✓ nếu mô hình hoạt động ngang ngửa với các mô hình baseline khác, ✗ nếu mô hình gặp khó khăn trong việc học tác vụ, và ▲ nếu hiệu suất cải thiện nhưng với khoảng cách hiệu suất so với các mô hình baseline khác. Transformer thất bại trong việc học sparse parity, cho thấy hiệu suất không tốt hơn việc đoán ngẫu nhiên, trong khi Mamba gặp khó khăn để truy xuất chính xác vector giá trị trong vector-valued MQAR. MambaFormer mà chúng tôi đề xuất hoạt động ngang ngửa với các mô hình baseline khác trong tất cả các tác vụ.

Vì các mô hình ngôn ngữ Transformer hiện tại là những mô hình lớn duy nhất đã được báo cáo có khả năng ICL trong thực tế, điều này đặt ra câu hỏi:

Liệu các mô hình không cần attention có thể thực hiện ICL không?

Câu hỏi này có giá trị, đặc biệt khi xem xét rằng một số nghiên cứu gần đây đã cố gắng vượt ra ngoài các mạng dựa trên attention do chi phí bậc hai của chúng (Katharopoulos et al., 2020; Zhai et al., 2021; Dao et al., 2022; Poli et al., 2023; Peng et al., 2023; Sun et al., 2023; Yang et al., 2023b). Trong công việc này, chúng tôi tập trung cụ thể vào các mô hình không gian trạng thái (SSM), và đặc biệt là Mamba (Gu & Dao, 2023). Mamba gần đây đã được chứng minh là có hiệu quả cao trong khi đạt được hiệu suất gần tối tân trong các bộ dữ liệu ngôn ngữ pretraining tiêu chuẩn, như Pile (Gao et al., 2020), nhưng ở quy mô mô hình nhỏ hơn (ví dụ, lên đến 3 tỷ tham số), vượt qua transformer và các kiến trúc không có attention khác trên nhiều tác vụ ngôn ngữ và phi ngôn ngữ. Tuy nhiên, khả năng ICL thường xuất hiện ở quy mô vượt quá 3 tỷ tham số. Do đó, tiềm năng của những mô hình không có attention này để thực hiện ICL vẫn chưa được khám phá đầy đủ, vì việc kiểm tra các giả thuyết như vậy thường đòi hỏi mở rộng quy mô vượt quá mức 7 tỷ tham số. Tuy nhiên, chúng ta vẫn có thể điều tra khả năng ICL quy mô nhỏ bằng cách huấn luyện cụ thể một mô hình để thực hiện học trong ngữ cảnh, theo cách tiếp cận của Garg et al. (2022).

Đóng góp. Trong nghiên cứu này, chúng tôi giới thiệu một bộ tác vụ ICL đa dạng để đánh giá hiệu suất của Transformer và nhiều SSM khác nhau, bao gồm các mô hình tối tân như Mamba và S4 (Gu et al., 2022b). Những phát hiện của chúng tôi tiết lộ rằng hầu hết các SSM này có thể thực hiện ICL hiệu quả, phù hợp với hiệu suất của Transformer trên nhiều tác vụ. Tuy nhiên, Mamba thể hiện một số hạn chế trong việc học cây quyết định và tác vụ truy xuất, nhưng có thể vượt trội hơn Transformer trong các tác vụ ICL phức tạp khác, chẳng hạn như sparse parity, nơi các mô hình Transformer gặp khó khăn. Hiệu suất của các mô hình khác nhau trên từng tác vụ được tóm tắt trong Bảng 1.

Vì dường như có những tác vụ mà một trong hai họ mô hình tốt hơn, chúng tôi khám phá tác động của việc xen kẽ các khối SSM với các khối multi-head attention, tương tự như (Gu & Dao, 2023). Chúng tôi giới thiệu MambaFormer, một kiến trúc lai mới tích hợp các lớp Mamba và Attention, trong khi loại bỏ nhu cầu mã hóa vị trí, như thể hiện trong Hình 1. MambaFormer dường như tận dụng điểm mạnh của cả Mamba và Transformer, thể hiện hiệu suất tốt trên tất cả các tác vụ ICL được đánh giá và đồng thời học sparse parity và truy xuất.

Chúng tôi tin rằng những phát hiện của chúng tôi nhấn mạnh tầm quan trọng của việc mở rộng hiểu biết về ICL vượt ra ngoài Transformer, vì tiến bộ đáng kể đã được thực hiện trong bối cảnh các kiến trúc không có attention.

Chúng tôi thừa nhận rằng một hạn chế của nghiên cứu chúng tôi nằm ở việc tập trung vào các tác vụ ICL không phải ngôn ngữ và các mô hình nhỏ hơn. Có khả năng rằng một so sánh kiến trúc giữa SSM và transformer cho các tác vụ ICL tổng quát hơn trong các thiết lập ngôn ngữ thực tế ở số lượng tham số cao hơn có thể không mang lại những quan sát tương tự như chúng tôi đưa ra ở đây. Tuy nhiên, chúng tôi cho thấy khả năng ICL ngôn ngữ tiềm năng của những kiến trúc này bằng cách tiến hành thí nghiệm trên các bộ dữ liệu ICL ngôn ngữ hình thức tổng hợp (Xie et al., 2021; Akyürek et al., 2024). Hơn nữa, những phát hiện thực nghiệm không phải ngôn ngữ của chúng tôi chỉ ra rằng, ngoài khó khăn trong một số tác vụ truy xuất, tương tự như những điều được Arora et al. (2023) lưu ý, dường như không có trở ngại cơ bản nào cho Mamba để thực hiện học trong ngữ cảnh.

2 Công việc liên quan
Học trong ngữ cảnh dựa trên Transformer. Vai trò của attention trong ICL đã là trọng tâm của cả nghiên cứu lý thuyết và thực nghiệm. Các nghiên cứu chủ yếu tập trung vào meta-learning (Ravi & Larochelle, 2016; Min et al., 2022a), nơi người ta huấn luyện rõ ràng cho ICL. Đáng chú ý, Garg et al. (2022) đã kiểm tra transformer trong các tác vụ hồi quy trong ngữ cảnh, từ học hồi quy tuyến tính đến học cây quyết định. Các công việc tiếp theo đã gợi ý rằng attention có thể bắt chước các thuật toán tối ưu hóa khác nhau (Akyürek et al., 2022; von Oswald et al., 2023b; Dai et al., 2023). Thực tế, Ahn et al. (2023); Mahankali et al. (2023) đã chứng minh một cách có thể chứng minh rằng cực tiểu toàn cục của mục tiêu ICL hồi quy tuyến tính thực hiện một bước gradient descent có điều kiện tiên quyết cho một lớp linear attention.

Trong khi những thiết lập này có thể xuất hiện đơn giản và tách biệt khỏi các mô hình ngôn ngữ, Bhattamishra et al. (2023) đã chỉ ra rằng một GPT-2 đông lạnh có thể thực hiện thuật toán nearest neighbor, vẽ ra các kết nối giữa ICL trong các mô hình ngôn ngữ hiện có và thiết lập được cách điệu của việc huấn luyện cho ICL từ khởi tạo ngẫu nhiên. Hơn nữa, Olsson et al. (2022) cũng chứng minh thực nghiệm rằng "induction heads", là những attention head giải quyết một vấn đề truy xuất đơn giản, tương quan với hành vi ICL, cung cấp một kết nối mạnh mẽ giữa truy xuất và ICL.

Kiến trúc sub-quadratic. Số lượng phép toán floating point hiệu quả trong một lớp attention tăng theo bậc hai so với độ dài chuỗi đầu vào. Nhiều phép xấp xỉ hoặc kiến trúc mô hình thay thế đã được đề xuất để vượt qua sự phụ thuộc bậc hai. Chúng bao gồm từ việc xấp xỉ các cơ chế attention (Beltagy et al., 2020; Wang et al., 2020) đến việc phát triển các mô hình convolutional recurrent mới như các mô hình không gian trạng thái có cấu trúc (Gu et al., 2022b).

S4 (Gu et al., 2022a) là một họ các mô hình chuỗi được đặc trưng bởi một mô hình không gian trạng thái rời rạc hóa
ht=Aht−1+Bxt, yt=Cht, (1)

--- TRANG 3 ---
trong đó ht biểu thị trạng thái ẩn và (A,B,C) là các tham số (được biến đổi) độc lập với đầu vào. Sự tái diễn có thể biểu hiện như một tích chập, cho phép độ phức tạp gần tuyến tính sử dụng Fast Fourier Transform. Được xem trong khung này, Linear Transformer (Katharopoulos et al., 2020), sử dụng linear attention không có softmax, có thể được xem là một biến thể của linear SSM.

Dựa trên khái niệm này, H3 (Dao et al., 2022) tích hợp một S4 với dual gated connections. Mamba gần đây (Gu & Dao, 2023) tách khỏi SSM tiêu chuẩn bằng cách giới thiệu một cơ chế lựa chọn làm cho (A,B,C) trong Phương trình (1) phụ thuộc vào đầu vào xt cho phép trộn chuỗi phụ thuộc đầu vào.

Có những mô hình không có attention đáng chú ý khác như Hyena (Poli et al., 2023), RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), và GLA (Yang et al., 2023b). Mặc dù có hiệu suất tối tân cho các mô hình như Mamba, Arora et al. (2023) đã chứng minh rằng các mô hình subquadratic vẫn kém hơn attention trong các tác vụ multi-query recall, là một tổng quát hóa của tác vụ induction head (Olsson et al., 2022).

Trong nghiên cứu của họ, Xie et al. (2021) đã giới thiệu một bộ dữ liệu dựa trên ngôn ngữ tổng hợp cho học trong ngữ cảnh, có tên GINC, và chứng minh rằng cả transformer và LSTM (Hochreiter & Schmidhuber, 1997) đều có thể thực hiện ICL. Đáng chú ý, LSTM vượt trội hơn transformer trong độ chính xác ICL trên GINC, một phát hiện tương tự với những gì được tìm thấy trong Liu et al. (2023) cho tác vụ flip-flop language modeling của họ. Gần đây hơn, Akyürek et al. (2024) đã đề xuất một benchmark ICL dựa trên ngôn ngữ để huấn luyện các mô hình trên các ngôn ngữ hình thức được tạo ra bởi các automata hữu hạn ngẫu nhiên. Kết quả của họ cho thấy Transformer đáng kể tốt hơn các mô hình subquadratic, thiết lập một benchmark đo lường hiệu quả ICL trong mô hình hóa ngôn ngữ.

3 Thiết lập thí nghiệm
Chúng tôi đánh giá khả năng ICL của SSM và Transformer bằng cách huấn luyện từng mô hình từ đầu trên từng tác vụ cụ thể, được chi tiết trong Phần 3.1. Phần 3.2 phác thảo các tác vụ ICL và liên quan được nghiên cứu trong nghiên cứu của chúng tôi. Chúng tôi cung cấp một tóm tắt ngắn gọn về các tác vụ của chúng tôi trong Bảng 2 sau đây.

[THIS IS TABLE: Table showing task details with columns for Task, dim(d), points(N), Example/Function Sampling, and Task-specific parameters]

Bảng 2: Tóm tắt các tác vụ. Tất cả các mô hình được huấn luyện trong 500.000 lần lặp (trừ vector MQAR; xem Phụ lục A.5).

3.1 Huấn luyện mô hình cho học trong ngữ cảnh
Chúng tôi huấn luyện các mô hình để học một lớp hàm cụ thể F trong ngữ cảnh. Huấn luyện bắt đầu bằng việc tạo ra các prompt ngẫu nhiên: chọn một hàm f trong F từ phân phối DF và lấy mẫu một chuỗi các đầu vào ngẫu nhiên x1, . . . ,xN trong Rd i.i.d. từ DX. Ở đây, N và d biểu thị số lượng ví dụ trong ngữ cảnh và chiều của xi, tương ứng. Những đầu vào này tạo ra prompt P = (x1, f(x1), . . . ,xN, f(xN)). Chúng tôi huấn luyện mô hình ftheta, được tham số hóa bởi theta, bằng cách tối thiểu hóa expected loss trên tất cả các prompt:

min theta EP[1/N sum_{i=1}^{N-1} ℓ(ftheta(Pi), f(xi))] (2)

trong đó Pi := (x1, f(x1), . . . ,xi, f(xi),xi+1) và ℓ(·,·) là một hàm loss. Vì f:Rd→R, chúng tôi thêm d−1 số không vào f(x) để khớp với các chiều. Chúng tôi sử dụng các hàm loss phù hợp cho từng tác vụ.

Kiến trúc mô hình. Chúng tôi chủ yếu tập trung vào SSM, bao gồm (1) Mamba (Gu & Dao, 2023), một mô hình SSM tối tân với cơ chế lựa chọn; (2) S4 (Gu et al., 2022a), một tiền thân bất biến tuyến tính theo thời gian của Mamba; và (3) S4-Mamba, một biến thể trong đó S6 phụ thuộc đầu vào của Mamba được thay thế bằng S4 độc lập đầu vào, trong khi duy trì cùng cấu trúc như Mamba. Những khác biệt chính giữa hai mô hình S4 nằm ở việc áp dụng gating nhân và thứ tự module.

Huấn luyện. Chúng tôi huấn luyện từng mô hình bằng cách lấy mẫu một batch các prompt ngẫu nhiên tại mỗi bước huấn luyện và cập nhật các tham số mô hình sử dụng optimizer Adam (Kingma & Ba, 2014). Chúng tôi sử dụng batch size là 64 và huấn luyện trong 500.000 lần lặp (trừ tác vụ vector MQAR; xem Phụ lục A.5).

Đánh giá. Chúng tôi đánh giá hiệu suất mô hình trong học trong ngữ cảnh sử dụng phân phối tác vụ và dữ liệu DF và DX nhất quán với những gì trong quá trình huấn luyện. Một hàm và một chuỗi N đầu vào được lấy mẫu từ DF và DX, tương ứng, để tạo ra một test prompt Ptest = (x1, f(x1), . . . ,xN, f(xN)). Chúng tôi tạo ra 1.280 prompt và đo lường trung bình thực nghiệm của Eq. (2) trên các prompt cho hiệu suất học trong ngữ cảnh.

Trong suốt các thí nghiệm của chúng tôi, chúng tôi giữ tổng số tham số của các mô hình xấp xỉ như nhau cho mỗi cấu hình như được giải thích trong Phụ lục A.2. Để vẽ biểu đồ hiệu suất mô hình khi khả năng mô hình tăng, chúng tôi tính toán tổng số phép toán floating point (FLOP) được sử dụng để huấn luyện mô hình. Việc tính toán cho Transformer và Mamba có thể được tìm thấy trong Phụ lục B, dựa trên (Kaplan et al., 2020; Gu & Dao, 2023).

Các cấu hình mô hình và chi tiết triển khai huấn luyện được cung cấp trong Phụ lục A.

3.2 Các tác vụ học trong ngữ cảnh
Chúng tôi cung cấp một tổng quan về các tác vụ ICL và liên quan được nghiên cứu trong nghiên cứu này. Một số tác vụ được điều chỉnh từ (Garg et al., 2022), và chúng tôi tuân theo các thiết lập được phác thảo trong công việc của họ. Các tác vụ được tóm tắt trong Bảng 2.

3.2.1 Học hồi quy
Đối với tất cả các tác vụ hồi quy, các ví dụ trong ngữ cảnh xi được lấy mẫu từ phân phối Gaussian N(0,Id), trong đó Id là ma trận đơn vị d×d. Chúng tôi sử dụng squared error loss để huấn luyện mô hình.

Hồi quy tuyến tính. Chúng tôi kiểm tra lớp các hàm tuyến tính F={f|f(x) = w⊤x,w trong Rd} trong đó w được lấy mẫu từ phân phối Gaussian N(0,Id). Chúng tôi đặt d = 20.

Hồi quy tuyến tính thưa. Thiết lập giống hệt với hồi quy tuyến tính ngoại trừ việc sau khi lấy mẫu w từ N(0,Id), k tọa độ được giữ lại ngẫu nhiên và phần còn lại được đặt bằng không. Chúng tôi đặt k = 3.

Mạng nơ-ron hai lớp. Chúng tôi xem xét lớp các mạng nơ-ron ReLU hai lớp F = {f|f(x) = W(2)sigma(W(1)x)}, trong đó W(2) trong R1×h, W(1) trong Rh×d, và sigma(·) = max(0,·) là hàm ReLU. Mỗi phần tử của các ma trận trọng số được rút độc lập từ N(0,1). Chúng tôi sử dụng d = 20 và h = 100.

Cây quyết định. Chúng tôi xem xét một cây nhị phân đầy đủ với độ sâu cố định và đầu vào x trong Rd. Các giá trị nút lá được lấy mẫu từ N(0,1), và phần còn lại được lấy mẫu đều ngẫu nhiên từ {1, ..., d}, hoạt động như các chỉ số của x. Tại một nút không phải lá cho trước, chúng ta di chuyển sang phải nếu x[i]>0, trong đó i là chỉ số được lấy mẫu, và ngược lại di chuyển sang trái. y là giá trị nút lá khi việc duyệt kết thúc.

3.2.2 Học với outlier
Các vấn đề thuộc họ này áp dụng thiết lập cơ bản của tác vụ hồi quy tuyến tính tiêu chuẩn. Với xác suất cố định p, mỗi cặp (xi, f(xi)) trong prompt được thay thế bằng các vector "giả" là hoặc nằm ngoài phân phối huấn luyện, hoặc confounders được thiết kế để tăng độ phức tạp của tác vụ. Chúng tôi kiểm tra p trong {0.5,0.9} như các xác suất thay thế cho các tác vụ được mô tả dưới đây. Trong quá trình huấn luyện, chúng tôi không tính toán loss cho các outlier được thay thế.

Tuy nhiên, đối với đánh giá, vị trí của các vector giả được cố định để đảm bảo rằng các mô hình được đánh giá trên cùng số lượng ví dụ trong ngữ cảnh trên các batch. Vì vậy chúng tôi đánh giá loss tại ví dụ trong ngữ cảnh sạch thứ 50, nơi có một ví dụ sạch sau mỗi chín outlier cho many-outlier ICL và sau mỗi một outlier cho orthogonal-outlier ICL.

Hồi quy outlier trực giao. Mỗi cặp (xi, f(xi)) được thay thế ngẫu nhiên bằng ((axu+bxv)/(a2x+b2x),(ayu+byv)/(a2x+b2x)), trong đó u,v trong w⊥. (u,v) := (w1−projw(w1),w2−projw(w2)) và w1 và w2 được lấy mẫu từ N(0,Id) và các hệ số ax, bx, ay, by được lấy mẫu độc lập từ N(0,1).

Hồi quy nhiều outlier. Trong thiết lập này, xi và f(xi) được thay thế ngẫu nhiên bằng một vector d chiều của các số một {1}d và một vector one-hot [1,0, . . . , 0], tương ứng, với xác suất 90%. Ở đây, chúng tôi kiểm tra các chuỗi dài hơn của N = 512, trong đó chỉ 10% của chuỗi mang lại thông tin hữu ích về vector mục tiêu thực.

3.2.3 Học các hàm rời rạc
Sparse parity. Theo thiết lập từ Bhattamishra et al. (2023), chúng tôi xem xét lớp các hàm F={f|f(x) = ∏j trong S xi[j]}, trong đó xi[j] biểu thị phần tử thứ j của vector xi và S là một tập con của {1, . . . , d} với kích thước k. Mỗi xi được lấy mẫu đều ngẫu nhiên từ {−1,1}d, và tập con S có kích thước k được lấy mẫu ngẫu nhiên từ tập {1, . . . , d}. Đối với tác vụ này, chúng tôi huấn luyện một mô hình sử dụng cross-entropy loss và đánh giá mô hình sử dụng một chỉ số nhị phân cho độ chính xác, gán 1 cho dự đoán đúng và 0 cho dự đoán sai.

3.2.4 Học Chain-of-Thought
Chain-of-Thought-I/O. Theo thiết lập từ Li et al. (2023b), chúng tôi xem xét lớp các mạng nơ-ron ReLU hai lớp F={f|f(x) = W(2)sigma(W(1)x)}, trong đó W(2) trong R1×h, W(1) trong Rh×d, và sigma(·) là hàm ReLU. Chúng tôi đặt d = 10 và h = 8. Chúng tôi thêm vào xen kẽ đặc trưng ẩn trung gian si=sigma(W(1)xi) trong chuỗi đầu vào huấn luyện của chúng tôi theo phong cách Chain-of-Thought (CoT). Cho chuỗi đầu vào (x1,s1, f(x1),···,xN,sN, f(xN),xtest), mô hình được đánh giá trên dự đoán đầu ra cuối cùng ŷ dựa trên chuỗi đầu vào và dự đoán lớp trung gian ŝtest.

3.2.5 Học truy xuất
Vector multi-query associative recall. Chúng tôi kiểm tra khả năng của mô hình để thực hiện multi-query associative recall (MQAR) (Arora et al., 2023). Mặc dù MQAR không phải là một tác vụ ICL, khả năng của mô hình để thực hiện associative recall (AR) có liên quan cao đến khả năng của mô hình để học trong ngữ cảnh (Olsson et al., 2022). Để đo lường tốt hơn khả năng của mô hình để truy xuất thông tin từ ngữ cảnh, chúng tôi xem xét một biến thể của MQAR. Các khóa và các giá trị là vector, có thể được hiểu như các embedding token duy nhất. Cụ thể, mô hình được đưa ra một chuỗi các cặp key-value của vector {k1,v1, ...,kn,vn}, trong đó ki,vi trong Sd−1 được lấy mẫu đều từ đơn vị d-sphere. Truy vấn bao gồm chuỗi các vector {q1, ...,qm}. Đối với mỗi truy vấn qj, tồn tại một số 1<=l<=n sao cho qj=kl. Mô hình phải học để xuất ra vl liên kết với truy vấn qj cho mỗi truy vấn, tạo ra tổng cộng m đầu ra. Chúng tôi huấn luyện các mô hình với mean squared error loss.

3.2.6 Học các ngôn ngữ hình thức tổng hợp
Mặc dù không phải là trọng tâm chính của công việc chúng tôi, chúng tôi tiến hành các thí nghiệm ban đầu sử dụng các benchmark ngôn ngữ tổng hợp được thiết kế để đánh giá khả năng học trong ngữ cảnh (ICL) trong thiết lập ngôn ngữ. Cho rằng ICL ngôn ngữ thực thường đòi hỏi các bộ dữ liệu và tài nguyên tính toán rộng lớn, những bộ dữ liệu tổng hợp này hoạt động như proxy hữu ích để khám phá ICL ngôn ngữ. Để có mô tả chi tiết về việc xây dựng và đánh giá của chúng, chúng tôi hướng người đọc đến các ấn phẩm tương ứng.

Bộ dữ liệu GINC (Xie et al., 2021). Bộ dữ liệu Generative In-Context learning (GINC) là một bộ dữ liệu ngôn ngữ quy mô nhỏ được tạo ra tổng hợp sử dụng một hỗn hợp các hidden markov model. Bộ dữ liệu pretraining của nó chứa khoảng 10 triệu token và mỗi mô hình được huấn luyện được đánh giá trên 2500 prompt test-time chứa từ 0 đến 64 ví dụ. Chúng tôi huấn luyện và kiểm tra các mô hình của chúng tôi sử dụng kích thước từ vựng là 100. Chúng tôi thêm vào huấn luyện LSTM cho bộ dữ liệu này, như đã làm trong công việc trước.

RegBench (Akyürek et al., 2024). In-context Language Learning (ICLL) RegBench là một benchmark ngôn ngữ regular tổng hợp được tạo ra bằng cách tạo ngẫu nhiên các probabilistic finite automata (PFA) với xác suất chuyển tiếp đều; nhiều instance vấn đề được tạo ra bao gồm các mẫu từ mỗi PFA. Các mô hình được đánh giá sử dụng một metric độ chính xác greedy-decoding, đánh giá xem mỗi token tiếp theo được dự đoán bởi mô hình có hợp lệ dưới ngôn ngữ regular hiện tại hay không.

--- TRANG 4 ---
4 Khả năng học trong ngữ cảnh của Mamba
Trong phần này, chúng tôi chứng minh rằng Mamba có thể được huấn luyện từ đầu để thực hiện nhiều tác vụ ICL khác nhau. Hơn nữa, chúng tôi xác định các tác vụ cụ thể trong đó một mô hình hoạt động tốt hơn những mô hình khác và ngược lại, với cùng lượng tài nguyên tính toán được đo bằng tổng số phép toán floating point (FLOP) được sử dụng trong huấn luyện.

[Hình 2: Biểu đồ hiệu suất mô hình trên bộ tác vụ ICL với Transformer, Mamba, S4-Mamba, và S4]

--- TRANG 5 ---
4.1 Mamba có thể học trong ngữ cảnh!
Phát hiện 1: Mamba vượt trội hơn các đối tác đơn giản hơn của nó, trong khi hoạt động tốt như Transformer trên một loạt các tác vụ ICL.

Trong Hình 2, Mamba liên tục vượt trội hơn các đối tác đơn giản hơn S4-Mamba và S4. Đối với hồi quy tuyến tính, khoảng cách giữa Mamba và S4-Mamba nhỏ hơn nhiều so với khoảng cách giữa S4-Mamba và S4. Vì sự khác biệt duy nhất giữa Mamba và S4-Mamba là cơ chế lựa chọn phụ thuộc đầu vào, gating phù hợp và stacking của MLP (tức là sự khác biệt giữa S4-Mamba và S4) dường như quan trọng hơn đối với những tác vụ như vậy. Ngược lại, sự phụ thuộc đầu vào của Mamba tạo ra tiến bộ có ý nghĩa cho các tác vụ phức tạp hơn như hồi quy 2NN và học cây quyết định.

Mamba cũng có thể hoạt động ngang ngửa với Transformer ngay cả khi tổng FLOP tăng lên. Điều này đáng ngạc nhiên cho rằng Transformer và attention đã là trọng tâm của nhiều công việc trước đây về khả năng ICL độc đáo của nó. Hơn nữa, Mamba có xu hướng hoạt động tốt hơn trong các thiết lập tham số nhỏ hơn khi kiểm soát độ sâu bằng nhau, tức là giữ số lượng attention, MLP, và các khối Mamba tương đương.

4.2 Khoảng cách hiệu suất trong các tác vụ ICL phức tạp hơn
Chúng tôi cũng xem xét một họ các tác vụ ICL phức tạp hơn, cụ thể là học cây quyết định, sparse parity, hồi quy robust-outlier (Hình 3) và Chain-of-Thought (Hình 4). Chúng tôi phát triển về hiệu suất của từng mô hình trên từng tác vụ trong các phát hiện của chúng tôi dưới đây.

[Hình 3: Hiệu suất của các kiến trúc khác nhau trên hai tác vụ hồi quy tuyến tính robust]

--- TRANG 6 ---
Phát hiện 2: Đối với hồi quy robust-outlier, Mamba vượt trội hơn Transformer trong việc bỏ qua các outlier cố định phổ biến, trong khi Transformer tốt hơn khi các outlier không cố định.

Hồi quy orthogonal-outlier và hồi quy many-outlier, giống như các tác vụ outlier khác, tập trung vào khả năng của mô hình để học bỏ qua các vector giả, hoặc bởi thực tế rằng xi trong w⊥, hoặc bởi thực tế rằng yi là một vector thay vì một giá trị scalar zero-padded. Điều này đòi hỏi rõ ràng các mô hình phải nhìn vào các chuỗi đầu vào trước đó và khám phá các thuộc tính phân biệt các vector giả khỏi các ví dụ huấn luyện trong khi học lớp các hàm mà prompt huấn luyện đại diện.

Đối với tác vụ hồi quy orthogonal-outlier với độ dài chuỗi tương đối ngắn là 101, Mamba không hoạt động tốt bằng Transformer với cùng tổng FLOP, mặc dù nó học tốt hơn đáng kể so với S4 (Hình 3). Tuy nhiên, đối với hồi quy many-outlier nơi chúng tôi huấn luyện trên độ dài chuỗi 512 và thay thế all-ones 90%, Mamba vượt trội hơn Transformer, đặc biệt về độ chính xác out-of-distribution (OOD) nơi chúng tôi đánh giá mỗi mô hình trên các chuỗi sạch không có outlier nào cả. Các mô hình recurrent, như S4 và Mamba, dường như tổng quát hóa tốt trong chế độ OOD như vậy khi dữ liệu bị nhiễm bởi nhiều vector outlier giống hệt nhau. Điều này cũng phù hợp với những gì Gu & Dao (2023) báo cáo: Mamba hoạt động tốt hơn trong các tác vụ truy xuất với độ dài chuỗi dài với một khóa truy xuất duy nhất. Những kết quả này chỉ ra rằng Mamba không có vấn đề đáng kể với việc lọc ra thông tin không cần thiết, trong khi vẫn giữ khả năng học hồi quy tuyến tính trong ngữ cảnh.

Phát hiện 3: Đối với Chain-of-Thought-I/O, Mamba cho thấy hiệu suất tương đương với Transformer.

Hình 2 và Hình 4 cho thấy các mô hình Mamba có khả năng học trong ngữ cảnh theo cách chain-of-thought, cho thấy hiệu suất tương đương với các mô hình Transformer trên các cấu hình được kiểm tra. Trong các cấu hình mô hình nhỏ hơn, các mô hình Mamba thể hiện hiệu suất vượt trội so với các mô hình Transformer. Tuy nhiên, khi kích thước mô hình tăng, các mô hình Transformer bắt đầu vượt qua các mô hình Mamba. Hiệu suất của các mô hình Transformer vẫn tương đối ổn định trên các kích thước vấn đề khác nhau, trong khi hiệu suất của các mô hình Mamba bị ảnh hưởng đáng kể bởi kích thước của lớp ẩn. Cụ thể, các mô hình Mamba xuất sắc hơn các mô hình Transformer ở các kích thước vấn đề nhỏ hơn (tức là các chiều ẩn nhỏ hơn), nhưng lợi thế của chúng giảm dần khi kích thước vấn đề mở rộng.

[Hình 4: Hiệu suất của các mô hình Transformer và Mamba trên tác vụ Chain-of-Thought-I/O]

--- TRANG 7 ---
4.3 Thách thức trong parity và truy xuất
Chúng tôi chạy vector MQAR trên hai thiết lập: (1) 32 cặp key-value với 16 truy vấn và (2) 32 cặp key-value với 4 truy vấn.

[Bảng 3: Test loss (mean squared error) trên vector MQAR và các cấu hình mô hình tương ứng]

Phát hiện 4: Mamba gặp khó khăn trong việc truy xuất vector trong ngữ cảnh của nó trong vector MQAR, một tác vụ mà Transformer có thể thực hiện dễ dàng.

Từ Bảng 3, chúng ta có thể thấy rằng Mamba gặp khó khăn trong việc truy xuất chính xác các vector vì mean squared error cho việc truy xuất các vector chuẩn hóa lớn hơn 0.1 trong tất cả các trường hợp. Vì SSM bị giới hạn bởi chiều trạng thái ẩn của chúng trong việc mang thông tin để dự đoán token tiếp theo, cuối cùng chúng sẽ bị choáng ngợp nếu số lượng cặp key-value trong ngữ cảnh (không phải truy vấn) tăng lên đáng kể.

Lưu ý rằng các mô hình được huấn luyện với 16 truy vấn có test loss thấp hơn các mô hình được huấn luyện với 4 truy vấn. Chúng tôi đoán rằng đối với một chuỗi dữ liệu duy nhất đại diện cho một tác vụ MQAR, mỗi cặp (q,v) có thể được coi như một mẫu huấn luyện. Do đó một chuỗi với 16 truy vấn chứa nhiều mẫu huấn luyện hơn chuỗi với 4 truy vấn. Điều này cũng cho thấy rằng có nhiều truy vấn hơn không nhất thiết làm cho tác vụ khó hơn. Đáng chú ý, thiết lập của chúng tôi thách thức hơn MQAR dựa trên token, vì chúng tôi lấy mẫu các vector ngẫu nhiên mới mỗi batch. Những phát hiện tương tự về truy xuất đã được quan sát trong Arora et al. (2023).

Phát hiện 5: Mamba có thể học sparse parity trong ngữ cảnh, một tác vụ mà Transformer không thể thực hiện.

Trong khi Mamba thất bại trong các tác vụ truy xuất đơn giản như MQAR, bàn cờ quay ngược lại đối với tác vụ học sparse parity (Hình 5). Transformer thất bại trong việc làm tốt hơn việc đoán ngẫu nhiên, phù hợp với bằng chứng từ công việc trước (Bhattamishra et al., 2020, 2023; Hahn, 2020). Chúng tôi xác nhận điều này đúng với các kích thước Transformer có chiều embedding lên đến 768 và lên đến 24 lớp khi được huấn luyện tối đa 1 triệu lần lặp. Tuy nhiên, Mamba thành công trong tác vụ này một cách dễ dàng, giải quyết sparse parity cho (d, k) = (10, 2) với một mạng nhỏ chỉ 2 lớp.

Thậm chí còn đáng ngạc nhiên hơn, S4-Mamba có thể giải quyết parity cũng như vậy, cho thấy hiệu suất tương đương với Mamba; điều này chỉ ra rằng tích chập hoặc gating thích hợp có thể quan trọng hơn lựa chọn phụ thuộc đầu vào để học parity. Cho rằng chỉ Transformer không thể hoạt động tốt hơn ngẫu nhiên, các tính toán tuần tự của các mô hình recurrent dường như có lợi thế hơn để học parity. Cuối cùng, kết quả của chúng tôi gợi ý rằng tích chập nhân quả ban đầu mà Mamba cung cấp trước lớp attention có thể quan trọng đối với việc giải quyết parity, một hiện tượng tương tự được quan sát đối với Vision Transformer trong các tác vụ computer vision (Yu et al., 2022).

Bất kỳ thuật toán nào để học parity đều đòi hỏi hoặc một bộ nhớ siêu tuyến tính omega(d) hoặc một số lượng mẫu siêu đa thức trong d (Raz, 2016; Kol et al., 2017). Trong khi Transformer được biết đến có bộ nhớ tốt hơn Mamba do cơ chế attention bậc hai của nó, kết quả của chúng tôi về học

--- TRANG 8 ---
[Hình 5: Mặc dù Transformer gặp khó khăn trong việc học tác vụ, Mamba và S4-Mamba có thể học sparse parity]

sparse parity đặt ra câu hỏi về cách các kiến trúc khác nhau có thể sử dụng bộ nhớ của chúng khác nhau về mặt xấp xỉ hàm. Chúng tôi để lại câu hỏi lý thuyết và thực nghiệm về thành phần kiến trúc nào cho phép học parity như một hướng nghiên cứu thêm.

5 Lợi thế của các kiến trúc lai cho học trong ngữ cảnh

[Hình 6: Các kiến trúc mô hình]

Trong phần trước, chúng tôi đã quan sát thấy rằng Transformer hoạt động tốt hơn SSM trong một số tác vụ nhất định như học cây quyết định hoặc truy xuất, trong khi SSM xuất sắc trong những tác vụ khác, như học sparse parity hoặc hồi quy tuyến tính heavy-outlier, có thể do bản chất recurrent của nó. Tuy nhiên, liệu chúng ta có thể đạt được điều tốt nhất của cả hai thế giới mà không hy sinh hiệu suất trong bộ tác vụ ICL của chúng ta không?

Chúng tôi trả lời điều này một cách khẳng định; trong phần này, chúng tôi nghiên cứu hai kiến trúc lai kết hợp Transformer và Mamba, cụ thể là Standard Hybrid và MambaFormer như được minh họa trong Hình 6.

Standard Hybrid là kiến trúc xen kẽ MHA và Mamba bằng cách thay thế khối MLP bằng Mamba. MambaFormer gần như giống hệt với Standard Hybrid nhưng với một khối Mamba bổ sung như lớp ban đầu của nó. Điều này loại bỏ nhu cầu mã hóa vị trí ban đầu vì bản chất recurrent của khối Mamba mã hóa thông tin vị trí.

Mặc dù nhiều công việc đã thấy rằng việc xen kẽ multi-head attention và LTI SSM có lợi (Zuo et al., 2022; Mehta et al., 2022; Pilault et al., 2023), thú vị là Gu & Dao (2023) đã không tìm thấy lợi ích đáng kể của việc xen kẽ.

Tuy nhiên, trong các kết quả sau đây, chúng tôi cho thấy rằng chúng ta thực sự có thể đạt được hiệu suất cạnh tranh trong bộ tác vụ ICL của chúng ta bằng cách xen kẽ các khối Attention và Mamba. MambaFormer đạt được hiệu suất tương đương với Transformer hoặc Mamba, trong khi xuất sắc trong cả sparse parity và truy xuất, những tác vụ không thể giải quyết được bởi Transformer và Mamba, tương ứng. Chúng tôi khám phá rằng thành phần chính là có Mamba như lớp đầu tiên.

5.1 Đồng thời học parity và truy xuất
Phát hiện 6: MambaFormer có thể học sparse parity trong ngữ cảnh; hơn nữa, có lớp ban đầu như một khối Mamba có hiệu quả đáng kể.

Như được nhấn mạnh trong Bhattamishra et al. (2023); Barak et al. (2022), học sparse parity trong ngữ cảnh dường như khó khăn đối với Transformer và một số SSM như Hyena. Tuy nhiên thú vị, như thấy trong Hình 7, MambaFormer thành công học parity nhanh như Mamba về độ phức tạp mẫu. Trong khi mô hình Standard Hybrid cũng có khả năng, nó thể hiện hiệu quả mẫu kém hơn nhiều.

[Hình 7: Thời gian hội tụ trung vị của việc học parity]

Chúng tôi thực hiện một nghiên cứu ablation bằng cách trang bị Transformer với một khối Mamba ban đầu mà không có bất kỳ mã hóa vị trí nào. Mặc dù biến thể Transformer này chỉ có ít khối Mamba hơn Standard Hybrid, nó giải quyết parity gần như hiệu quả như Mamba. Điều này không chỉ cho chúng ta thấy rằng thứ tự các lớp trong việc xen kẽ quan trọng như được chỉ ra trong Press et al. (2022), mà còn cho thấy Mamba có thể bổ sung cho Transformer mà không làm tổn hại hiệu suất trong ICL. Kết quả này đưa ra những khác biệt hấp dẫn giữa khả năng học hàm của Attention và Mamba; chúng tôi để lại câu hỏi này cho nghiên cứu thêm.

Phát hiện 7: MambaFormer có thể thực hiện truy xuất cũng như Transformer, thu hẹp khoảng cách hiệu suất giữa Mamba và Transformer.

Khoảng cách giữa Mamba và Transformer trong vector MQAR có thể do thực tế rằng Mamba (như một SSM) nén ngữ cảnh thành các trạng thái nhỏ hơn khi tạo ra đầu ra, trong khi cơ chế Attention trong Transformer không nén ngữ cảnh. Lượng ngữ cảnh mà SSM lưu trữ tại mỗi trạng thái phụ thuộc vào chiều của trạng thái ẩn vì các trạng thái ẩn nắm bắt thông tin quan trọng trong ngữ cảnh. Ngược lại, attention tận dụng tất cả các token trong ngữ cảnh đầu vào của nó, cho phép Transformer và các mô hình lai thuận tiện truy xuất các cặp key-value tương ứng thông qua các tính toán theo cặp.

--- TRANG 9 ---
Mặt khác, SSM cuối cùng sẽ bị choáng ngợp nếu số lượng cặp key-value tăng lên đáng kể.

Để thu hẹp khoảng cách trong tác vụ vector MQAR giữa Mamba và Transformer mà không hy sinh quá nhiều hiệu quả, chúng tôi thêm một lớp attention trong các lớp Mamba. Cụ thể, trong một mô hình Mamba 4 lớp (8 khối Mamba được xếp chồng đồng nhất), chúng tôi thay thế hai khối giữa bằng Standard Hybrid (không có mã hóa vị trí). Như được hiển thị trong Bảng 3, mô hình Mamba đạt được cải thiện đáng kể trong vector MQAR bằng cách có một Standard Hybrid. Chúng tôi tiếp tục kiểm tra MambaFormer trên cùng tác vụ và thấy rằng MambaFormer gần như hoàn toàn thu hẹp khoảng cách với transformer trong tác vụ vector MQAR.

5.2 Hiệu suất ICL tất cả trong một
Phát hiện 8: Cả hai mô hình lai đều hoạt động tốt như Transformer và Mamba trong bộ tác vụ ICL của chúng ta (hoặc thậm chí tốt hơn đôi khi).

Trong khi MambaFormer thành công trong hai tác vụ được coi là khó khăn đối với Mamba hoặc Transformer, nó cũng hoạt động tốt như Transformer và Mamba trong phần còn lại của các tác vụ ICL. Trong Hình 8, chúng ta thấy rằng MambaFormer và Standard Hybrid đều học cây quyết định tốt như Transformer và tốt hơn Mamba, ngay cả ở các kích thước tham số lớn hơn.

Đáng ngạc nhiên hơn, MambaFormer học hồi quy tuyến tính một cách hiệu quả và mạnh mẽ hơn ngay cả khi có dữ liệu nhiễu trong hồi quy Many-outlier và hồi quy Orthogonal-outlier (xem Hình 3). Cụ thể, một MambaFormer nhỏ được huấn luyện trên 100k lần lặp (<10^17 FLOP) hoạt động tốt như các mô hình được huấn luyện với gần 5 lần số FLOP (Hình 3 trái).

Khi được đánh giá không có outlier trong test-time, MambaFormer giống Transformer và Standard Hybrid giống Mamba về hiệu suất out-of-distribution, nơi Mamba dễ dàng học hồi quy tuyến tính khi chỉ có một vector outlier (Hình 3 trên phải) trong khi Transformer học tốt hơn khi có một không gian con của các vector outlier (Hình 3 dưới phải).

Kết luận, chúng tôi tìm thấy điều tốt nhất của cả hai thế giới trong mảng đa dạng các tác vụ ICL của chúng ta; một kiến trúc lai có thể giải quyết các vấn đề khó khăn như truy xuất và parity, trong khi hoạt động ngang ngửa với Transformer và Mamba trong các tác vụ ICL khác. Với kết quả của chúng tôi, sẽ thú vị khi thấy các kiến trúc lai hoạt động như thế nào trong các loại tác vụ ICL khác, đặc biệt là các benchmark ngôn ngữ trong ngữ cảnh như Xie et al. (2021); Hahn & Goyal (2023); Akyürek et al. (2024). Đến lượt mình, chúng tôi khám phá khả năng ICL ngôn ngữ hình thức trong phần con sau.

5.3 Học ngôn ngữ hình thức trong ngữ cảnh
Với sức mạnh thực nghiệm trong các mô hình lai, phần con này phân tích hiệu suất của chúng trên các benchmark ngôn ngữ hình thức tổng hợp, cụ thể là GINC và ICLL RegBench. Chúng tôi sử dụng những benchmark này như một proxy để đo lường khả năng ICL ngôn ngữ.

[Bảng 4: Dữ liệu GINC có kích thước từ vựng 100 và độ chính xác ICL được đánh giá tại 64 ví dụ]

Phát hiện 9: Các mô hình lai hoạt động tốt như, hoặc vượt trội hơn, Transformer và Mamba trong ICL ngôn ngữ hình thức, như được minh họa trong Bảng 4 và 5.

Trên GINC, Mamba đạt được độ chính xác ICL tốt nhất trong số các mô hình không phải LSTM, mặc dù Transformer đạt được perplexity thấp hơn. Thú vị, Standard Hybrid hoạt động ngang ngửa với Transformer và Mamba, trong khi MambaFormer hoạt động hơi kém hơn các mô hình khác ở đây. Tuy nhiên, những phát hiện từ Xie et al. (2021) chỉ ra rằng LSTM xuất sắc hơn Transformer trên GINC, ngay cả khi tính đến các thiết lập khác nhau như kích thước từ vựng hoặc số lượng ví dụ trong ngữ cảnh. Điều này phù hợp với những phát hiện trước đây trong đó Transformer hoạt động kém hơn hoặc tương đương với LSTM trong nhiều ngôn ngữ hình thức được xem xét (Bhattamishra et al., 2020; Deletang et al., 2022). Tuy nhiên, Transformer là

--- TRANG 10 ---
[Bảng 5: Perplexity (PPL) và độ chính xác greedy-decoding cho RegBench sau khi huấn luyện mỗi mô hình 15 và 120 epoch]

mô hình vượt trội de facto cho mô hình hóa ngôn ngữ, vì vậy vẫn chưa rõ hiệu suất trên benchmark này chuyển đổi như thế nào sang ICL ngôn ngữ thế giới thực, nơi Transformer thường vượt trội hơn LSTM.

Trên RegBench, ưa chuộng Transformer hơn các mô hình không có attention, Mamba thực sự hoạt động kém hơn Transformer, phù hợp với những phát hiện trước. Đáng chú ý, các kiến trúc lai xuất sắc trên benchmark này, hội tụ nhanh hơn nhiều cả Mamba và Transformer trong khi đạt được độ chính xác cao hơn. Với bằng chứng trước đây rằng Standard Hybrid đạt được perplexity thấp hơn trong mô hình hóa ngôn ngữ (Gu & Dao, 2023), kết quả mới của chúng tôi gợi ý rằng các mô hình lai cung cấp một hướng đi đầy hứa hẹn cho cả mô hình hóa ngôn ngữ và học trong ngữ cảnh trên các tác vụ ngôn ngữ. Chúng tôi hy vọng những kết quả và phân tích này chứng minh tiềm năng của các mô hình lai cho các ứng dụng ICL dựa trên ngôn ngữ.

6 Thảo luận
Trong công việc này, chúng tôi đã cung cấp một điều tra toàn diện về học trong ngữ cảnh với các mô hình không gian trạng thái (SSM) và so sánh chúng với kiến trúc Transformer. Nghiên cứu của chúng tôi đã tiết lộ rằng SSM, đặc biệt là Mamba, là những học viên trong ngữ cảnh có khả năng. Mặt khác, các đánh giá của chúng tôi tiết lộ rằng không SSM hay Transformer đều không tuyệt vời trong tất cả các tác vụ: SSM gặp khó khăn với cây quyết định và tác vụ truy xuất trong khi Transformer gặp khó khăn với sparse parity. Điều này đã dẫn chúng tôi đến kiến trúc lai MambaFormer đạt được hiệu suất tốt nhất của cả hai thế giới trên bộ ICL của chúng ta.

Các hướng nghiên cứu tương lai bao gồm khám phá (1) hiệu suất trên bộ ICL của chúng ta tương quan như thế nào với khả năng mô hình hóa ngôn ngữ tổng quát, như perplexity trên các benchmark NLP tiêu chuẩn, (2) phát triển các kiến trúc hiệu quả hơn bằng cách tích hợp các yếu tố từ transformer, SSM, và các cơ chế gating, (3) xác định các đặc trưng kiến trúc góp phần vào học trong ngữ cảnh hiệu quả, và (4) đánh giá tác động của MambaFormer và các kiến trúc sáng tạo khác đối với hiệu suất mô hình hóa ngôn ngữ.

Tuyên bố tác động
Bài báo này cung cấp một nghiên cứu toàn diện về các kiến trúc mô hình hóa ngôn ngữ giúp xác định điểm yếu, điểm mạnh của chúng và cung cấp công thức cho các kiến trúc mới. Kết quả của công việc này có thể tạo điều kiện thuận lợi cho việc cải thiện hiệu quả và kiến trúc cho các mô hình ngôn ngữ lớn.

--- TRANG 11 ---
Lời cảm ơn
Công việc của Dimitris Papailiopoulos được hỗ trợ một phần bởi ONR Grant No. N00014-21-1-2806 và No. N00014-23-1-2848. Công việc của Samet Oymak được hỗ trợ một phần bởi NSF CAREER Award CCF-2046816. Công việc của Jaeseung Park được hỗ trợ bởi KRAFTON AI Fellowship. Các tác giả muốn cảm ơn Byeongju Kim và Seongjun Yang cho cuộc thảo luận hữu ích và Gibbeum Lee cho phản hồi có giá trị về bản thảo đầu của bài báo này.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được duy trì nguyên với 16 trang references...]
