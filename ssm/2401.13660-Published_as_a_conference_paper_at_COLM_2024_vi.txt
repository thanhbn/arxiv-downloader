# 2401.13660.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2401.13660.pdf
# Kích thước file: 1764806 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
MambaByte: Mô hình không gian trạng thái chọn lọc không cần token
Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush
Đại học Cornell
{jw2544,tg352,jy858,arush}@cornell.edu
Tóm tắt
Các mô hình ngôn ngữ không cần token học trực tiếp từ các byte thô và loại bỏ thiên kiến quy nạp của việc token hóa từ con. Tuy nhiên, việc hoạt động trên byte dẫn đến các chuỗi dài hơn đáng kể. Trong bối cảnh này, các Transformer tự hồi quy tiêu chuẩn mở rộng kém vì bộ nhớ hiệu quả cần thiết tăng theo độ dài chuỗi. Sự phát triển gần đây của mô hình không gian trạng thái (SSM) Mamba cung cấp một phương pháp thay thế hấp dẫn với trạng thái bộ nhớ có kích thước cố định và việc giải mã hiệu quả. Chúng tôi đề xuất MambaByte, một sự thích ứng không cần token của Mamba SSM được huấn luyện tự hồi quy trên các chuỗi byte. Về mặt mô hình hóa, chúng tôi chỉ ra MambaByte có khả năng cạnh tranh và thậm chí vượt trội so với các Transformer từ con tiên tiến trong các tác vụ mô hình hóa ngôn ngữ trong khi vẫn duy trì các lợi ích của mô hình ngôn ngữ không cần token, chẳng hạn như tính bền vững với nhiễu. Về mặt hiệu quả, chúng tôi phát triển một sự thích ứng của việc giải mã suy đoán với việc soạn thảo có token hóa và xác minh ở cấp độ byte. Điều này dẫn đến tăng tốc suy luận 2.6× so với việc triển khai MambaByte tiêu chuẩn, cho thấy hiệu quả giải mã tương tự như Mamba từ con. Những phát hiện này thiết lập tính khả thi của SSM trong việc cho phép mô hình hóa ngôn ngữ không cần token.

1 Giới thiệu
Khi định nghĩa một mô hình ngôn ngữ, một việc token hóa cơ sở thường được sử dụng - có thể là từ (Bengio et al., 2000), từ con (Schuster & Nakajima, 2012; Sennrich et al., 2015; Wu et al., 2016; Wang et al., 2020), hoặc ký tự (Gao et al., 2020b). Trong số này, việc token hóa từ con đã là lựa chọn phổ biến nhất, vì nó đạt được sự cân bằng tự nhiên giữa hiệu quả huấn luyện và khả năng xử lý các từ ngoài từ vựng. Tuy nhiên, một số công trình, ví dụ như Xue et al. (2022), đã lưu ý các vấn đề với bộ token hóa từ con, chẳng hạn như thiếu tính bền vững đối với lỗi đánh máy, biến thể chính tả và viết hoa, cũng như thay đổi hình thái học.

Việc mô hình hóa các chuỗi byte, tức là ánh xạ từ dữ liệu thô sang dự đoán mà không có bất kỳ token hóa trung gian nào, cung cấp một phương pháp thay thế với ít thiên kiến quy nạp hơn (Choe et al., 2019; Al-Rfou et al., 2019; Clark et al., 2022; Tay et al., 2022; Xue et al., 2022; Yu et al., 2023). So với các mô hình từ con, các mô hình ngôn ngữ cấp byte có thể khái quát hóa dễ dàng hơn qua các biến thể chính tả và hình thái học. Tất nhiên, việc mô hình hóa văn bản dưới dạng byte có nghĩa là các chuỗi kết quả dài hơn đáng kể so với các đối tác từ con của chúng. Thay đổi này đẩy các vấn đề mô hình hóa và hiệu quả lên phía trước vào chính kiến trúc.

Những vấn đề này đặc biệt rõ ràng đối với các Transformer tự hồi quy (Vaswani et al., 2017), vốn chiếm ưu thế trong mô hình hóa ngôn ngữ (Brown et al., 2020; Touvron et al., 2023). Do tính chất bậc hai của attention, hiệu quả Transformer mở rộng kém cho các chuỗi (byte) dài (Zhang et al., 2022). Các nhà nghiên cứu đã nén biểu diễn Transformer nội bộ để làm việc với các chuỗi dài, ví dụ, phát triển các phương pháp mô hình hóa nhận biết độ dài (Dai et al., 2020; Nawrot et al., 2022), trong đó các nhóm token được hợp nhất trong các lớp trung gian. MegaByte Transformer (Yu et al., 2023) có ý nghĩa đặc biệt, sử dụng nén dưới dạng các patch có kích thước cố định của byte như một analog từ con

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

0 10K 20K 30K 40K
Bước huấn luyện0.900.951.001.051.101.151.201.251.30Bit trên byte

0 1 2 3 4 5 6
Exa FLOPs huấn luyện

MegaByte-193M+177M (patch: 4)
MegaByte-193M+177M (patch: 8)Gated-S4D-368M
MambaByte-353MTransformer-361M

Hình 1: Đánh giá các mô hình cấp byte với ngân sách tham số cố định. Kết quả mô hình hóa ngôn ngữ trên PG19 (8,192 byte liên tiếp), so sánh Transformer tiêu chuẩn (Vaswani et al., 2017; Su et al., 2021), MegaByte Transformer (Yu et al., 2023), S4 đường chéo có cổng (Mehta et al., 2023), và MambaByte. (Trái) Mất mát mô hình theo bước huấn luyện. (Phải) Chi phí huấn luyện chuẩn hóa FLOP. MambaByte đạt mất mát Transformer với ít hơn một phần ba ngân sách tính toán.

kết hợp với một bộ giải mã cấp byte. Những phương pháp này giảm chi phí tính toán¹ nhưng thay đổi hành vi mô hình hóa để phù hợp với dữ liệu.

Trong công trình này, chúng tôi đề xuất MambaByte, một mô hình ngôn ngữ cấp byte không có nén biểu diễn. Mô hình là một ứng dụng của kiến trúc Mamba được giới thiệu gần đây (Gu & Dao, 2023). Mamba xây dựng dựa trên phương pháp tiên phong của các mô hình không gian trạng thái (SSM) (Gu et al., 2021; Gupta et al., 2022; Gu et al., 2022; Smith et al., 2023; Fu et al., 2022) bằng cách giới thiệu một cơ chế lựa chọn đã được chứng minh là gần như hiệu quả như Transformer cho dữ liệu rời rạc. Quan sát chính của chúng tôi là, không giống như Transformer, Mamba có một trạng thái bộ nhớ có kích thước cố định (lớn) độc lập với độ dài ngữ cảnh, gần giống như một trạng thái ẩn mạng nơ-ron hồi quy lớn. Điều này tự nhiên loại bỏ một vấn đề mô hình hóa và hiệu quả chính cho mô hình hóa ngôn ngữ cấp byte mà không cần các kiến trúc chuyên biệt như patching toàn cục.

Ngay cả với việc huấn luyện hiệu quả, các mô hình cấp byte vẫn gặp phải thách thức về việc giải mã hiệu quả, vì việc tạo ra một ký tự tại một thời điểm đòi hỏi chạy mô hình ngôn ngữ nối tiếp từng byte một. Để cải thiện hiệu quả suy luận, chúng tôi đề xuất một sự thích ứng của việc giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023a; Xia et al., 2023) cho các mô hình cấp byte. Phương pháp sử dụng một mô hình từ con nhanh để soạn thảo tự hồi quy, tiếp theo là xác minh cấp byte. Mặc dù phương pháp này có thể được áp dụng cho bất kỳ mô hình cấp byte nào, nó đặc biệt hiệu quả cho các mô hình kiểu SSM vì bước xác minh cấp byte có thể sử dụng cùng đường dẫn mã quét song song làm cho các mô hình này hiệu quả để huấn luyện.

Các thí nghiệm so sánh MambaByte với các kiến trúc Transformer, SSM và MegaByte (patching) trong một thiết lập tham số cố định và tính toán cố định trên một số bộ dữ liệu mô hình hóa ngôn ngữ dạng dài. Hình 1 tóm tắt các phát hiện chính của chúng tôi. So với Transformer cấp byte, MambaByte đạt hiệu suất tốt hơn nhanh hơn và hiệu quả tính toán hơn đáng kể. Chúng tôi cũng so sánh MambaByte với các baseline từ con có token hóa sử dụng Transformer và SSM, và thấy rằng MambaByte có khả năng cạnh tranh về mất mát trong khi cũng thể hiện tính bền vững cải thiện trong việc xử lý nhiễu từ con, chẳng hạn như hỏng hóc văn bản đầu vào.

¹Tuy nhiên, các thí nghiệm của chúng tôi (xem Hình 1) chỉ ra rằng patching cũng có thể làm giảm hiệu suất mô hình so với Transformer tiêu chuẩn.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Thông qua phương pháp soạn thảo từ con suy đoán và xác minh cấp byte của chúng tôi, chúng tôi chỉ ra rằng MambaByte có thể được chạy nhanh như Mamba từ con để tạo văn bản. Chúng tôi tin rằng những kết quả này xác nhận tiềm năng của các mô hình không cần tokenizer như một thay thế thực tế cho Transformer từ con trong mô hình hóa ngôn ngữ.

2 Các mô hình không gian trạng thái và kiến trúc Mamba

Phương pháp: SSM chọn lọc. SSM mô hình sự tiến hóa của một trạng thái ẩn qua thời gian thông qua một phương trình vi phân bậc nhất. SSM tuyến tính bất biến theo thời gian (Gu et al., 2021; Gupta et al., 2022; Gu et al., 2022; Smith et al., 2023) đã cho thấy kết quả đầy hứa hẹn trong học sâu qua nhiều phương thức. Tuy nhiên, Gu & Dao (2023) gần đây đã lập luận rằng động lực học không đổi của những phương pháp này thiếu việc lựa chọn ngữ cảnh phụ thuộc đầu vào trong trạng thái ẩn, điều có thể cần thiết cho các tác vụ như mô hình hóa ngôn ngữ. Để làm điều này, họ định nghĩa động lực học trạng thái liên tục thay đổi theo thời gian cho một đầu vào x(t) ∈ R, trạng thái ẩn h(t) ∈ R^n, và đầu ra y(t) ∈ R tại thời điểm t là:

dh(t)/dt = Ah(t) + B(t)x(t); y(t) = C(t)h(t), (1)

được tham số hóa bởi một ma trận hệ thống đường chéo bất biến theo thời gian A ∈ R^(n×n) và các ma trận đầu vào và đầu ra phụ thuộc thời gian B(t) ∈ R^(n×1) và C(t) ∈ R^(1×n).

Để mô hình hóa các chuỗi thời gian rời rạc, động lực học thời gian liên tục trong (1) phải được xấp xỉ thông qua rời rạc hóa. Điều này dẫn đến một hồi quy trạng thái ẩn thời gian rời rạc với các ma trận mới tại mỗi bước thời gian, A, B, và C, sao cho

h[k] = A[k]h[k-1] + B[k]x[k]; y[k] = C[k]h[k]. (2)

Quan sát rằng (2) giống như một phiên bản tuyến tính của mạng nơ-ron hồi quy và có thể được áp dụng dưới dạng hồi quy này trong quá trình tạo mô hình ngôn ngữ. Việc rời rạc hóa đòi hỏi một bước thời gian, Δ[k], cho mỗi vị trí đầu vào, tương ứng với việc xử lý x[k] = x(t_k) cho t_k = Σ_{j=1}^k Δ[j]. Các ma trận thời gian rời rạc A, B, và C sau đó có thể được tính từ Δ[k].

Kiến trúc: Mamba. Trong Mamba, các thuật ngữ SSM có tính chọn lọc đầu vào, tức là B, C, và Δ được định nghĩa như các hàm của đầu vào x[k] ∈ R^d:

Δ[k] = softplus(W_Δ(W_R x[k])); B(t_k) = W_B x[k], (3)

trong đó W_B ∈ R^(n×d) (C được định nghĩa tương tự), W_Δ ∈ R^(d×r) và W_R ∈ R^(r×d) (cho một số r ≪ d) là các trọng số có thể học, và softplus đảm bảo tính dương. Lưu ý rằng các tham số SSM A, B, và C giống hệt nhau cho mỗi chiều đầu vào d, nhưng các bước thời gian Δ là riêng biệt; điều này dẫn đến kích thước trạng thái ẩn n×d cho mỗi bước thời gian k. (Xem Phụ lục D để minh họa cách Mamba mô hình các chuỗi rời rạc và các chi tiết khác về rời rạc hóa và tính chọn lọc.)

Mamba nhúng lớp SSM này vào một mô hình ngôn ngữ mạng nơ-ron đầy đủ. Cụ thể, mô hình sử dụng một ngăn xếp các lớp có cổng được lấy cảm hứng từ SSM có cổng trước đó (Mehta et al., 2023). Hình 5 (phải) trong Phụ lục B cho thấy kiến trúc Mamba kết hợp lớp SSM với mạng nơ-ron có cổng.

Triển khai: Quét song song cho hồi quy tuyến tính. Tại thời gian huấn luyện, chúng ta có quyền truy cập vào toàn bộ chuỗi x, cho phép chúng ta tính hồi quy tuyến tính hiệu quả hơn. Smith et al. (2023) đã chứng minh việc sử dụng quét song song hiệu quả công việc (Blelloch, 1990) để tính hiệu quả hồi quy tuần tự trong SSM tuyến tính. Đối với Mamba, đầu tiên chúng ta ánh xạ hồi quy thành một chuỗi L bộ, với e_k = (A_k, b_k) := (A[k], B[k]x[k]), sau đó định nghĩa một toán tử kết hợp • sao cho e_j • e_k = (A_k A_j, A_k b_j + b_k). Cuối cùng, chúng ta áp dụng quét song song để tính chuỗi [(A[1], h[1]), (A[2]A[1], h[2]), ...]. Nói chung, điều này đòi hỏi thời gian O(T• log₂(L)), sử dụng L/2 bộ xử lý, trong đó T• là chi phí của một phép nhân ma trận-ma trận. Lưu ý A là ma trận đường chéo, hồi quy tuyến tính có thể được tính song song trong thời gian O(n log₂(L)) và không gian O(nL). Quét song song với ma trận đường chéo cũng hiệu quả trong hoạt động, đòi hỏi O(nL) FLOP.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

soạn thảo:
(tự hồi quy)

xác minh:
(quét song song)

the
t
_
s
!a
l
s_
a
h
is
t
h
ia
_
i
!t
o
i
d_
o
a
id
w
v
_o
e
o
an
_
.
!e he_ca t
h
a
u...

...catcatsat
satdone
o
wwn
n_done
!

sửa từ:
(tự hồi quy)

Đầu ra giải mã:sat
donedownon
onthethe
mat

n
_
i
ua
o
u_
n
u
mo
_
w
nn
t
a
m_
h
i
at
e
u
ih
r
i
_e
e
a
m_
a
u
im
t
i
na
_
.
!t

(không sửa)

the catsat downon the mat the catsat done!wn

Hình 2: Giải mã suy đoán thông qua soạn thảo từ con và xác minh cấp byte. Các từ con xanh lá cây là gợi ý được đưa ra bởi mô hình Mamba từ con nhỏ hơn M_subword, có các byte liên quan rơi vào top-β ứng viên tự hồi quy của mô hình xác minh cấp byte (MambaByte) M_byte. Các byte đỏ và xanh dương là các từ chối và sửa chữa tương ứng được thực hiện bởi mô hình xác minh. (Hai bước được hiển thị sử dụng prompt: "the cat".)

3 Phương pháp

Mô hình hóa các chuỗi byte dài. MambaByte là một ứng dụng của kiến trúc Mamba cho mô hình hóa ngôn ngữ cấp byte. Quan sát chính của chúng tôi là không giống như Transformer, có bộ nhớ mở rộng tuyến tính theo độ dài chuỗi, Mamba duy trì một trạng thái bộ nhớ có kích thước cố định lớn, điều này làm cho nó phù hợp cho mô hình hóa cấp byte trực tiếp. Một cách chính thức, một mô hình Mamba m-lớp, mỗi lớp có trạng thái ẩn h(t) ∈ R^(n_state×d), duy trì và phát triển hiệu quả một bộ nhớ gồm m×n_state×d số thực. Lưu ý rằng kích thước bộ nhớ trạng thái ẩn Mamba độc lập với độ dài ngữ cảnh đầu vào, L_ctx, việc xử lý các chuỗi từ con hoặc chuỗi byte đòi hỏi mô hình cơ bản nén khoảng L_ctx byte trong bộ nhớ trạng thái ẩn cố định của nó, bất kể biểu diễn đầu vào. Trong tất cả trường hợp trừ những trường hợp cực đoan, m×n_state×d ≫ L_ctx, để lại đủ không gian của trạng thái ẩn h(t) để mã hóa thông tin L_ctx. Do đó, nếu Mamba có thể được sử dụng cho các mô hình có token hóa, MambaByte sẽ cho phép mô hình hóa các chuỗi cấp byte mà không cần các đánh đổi nén độ dài (Dai et al., 2020; Nawrot et al., 2022; Yu et al., 2023).

Việc sử dụng biểu diễn bộ nhớ có kích thước cố định cũng có thể giúp tránh các phụ thuộc bậc hai và cải thiện khái quát hóa. Mặc dù Transformer được thiết kế để nắm bắt các phụ thuộc tầm xa, các nhà nghiên cứu đã lưu ý rằng số lượng tương tác tiềm năng trong một chuỗi cấp byte dài có thể làm loãng sự tập trung của mô hình, khiến việc nắm bắt các phụ thuộc quan trọng trở nên khó khăn giữa một số lượng lớn những phụ thuộc ít liên quan hơn (Tworkowski et al., 2024). Thông tin cấp byte chi tiết hơn nhiều, do đó đòi hỏi mô hình phải học từ một ngữ cảnh lớn hơn nhiều để đưa ra dự đoán có ý nghĩa.

Cuối cùng, việc huấn luyện Mamba cho các chuỗi byte dài có lợi ích tính toán vốn có ở quy mô. Chi phí tính toán cho Mamba khi huấn luyện là O(L_ctx), trong khi ngay cả các mô hình nén như MegaByte (Yu et al., 2023) có độ phức tạp O(L²_ctx/p² + L_ctx p) cho kích thước patch p. Ngay cả với kích thước patch lớn L^(1/3)_ctx, độ phức tạp kết quả là O(L^(4/3)_ctx).

Giải mã suy đoán thông qua soạn thảo từ con. Mặc dù MambaByte hiệu quả tính toán khi huấn luyện, nó gặp phải thách thức trong giải mã, chủ yếu vì mỗi byte được xử lý tuần tự. Để giảm thiểu nút thắt cổ chai tuần tự này, chúng tôi đề xuất một sự thích ứng của

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

giải mã suy đoán thông qua soạn thảo từ con và xác minh cấp byte. Quan sát của chúng tôi là hầu hết các bước suy luận không yêu cầu độ chi tiết của giải mã cấp byte và có thể hưởng lợi từ soạn thảo từ con nhanh hơn. Do đó, chúng ta có thể huấn luyện các mô hình không cần token, được biết là bền vững với nhiễu, và mô phỏng việc tạo giống như mô hình từ con, nhanh hơn đáng kể. Chúng tôi phân tách mỗi lần lặp giải mã thành hai bước: soạn thảo sử dụng mô hình Mamba từ con nhỏ hơn, sau đó xác minh và sửa chữa sử dụng mô hình MambaByte cấp byte lớn hơn, như được minh họa trong Hình 2.

Mô hình Mamba từ con, M_subword, soạn thảo m từ con tự hồi quy trong khi ghi lại các trạng thái ẩn liên quan tại mỗi bước thời gian. Các từ con được soạn thảo được chuyển đổi thành byte, đưa vào mô hình MambaByte, M_byte, và được xác minh sử dụng quét song song. Sau đó chúng tôi tìm vị trí phân nhánh byte c, vị trí xa nhất trong chuỗi byte được xác minh là trong top-β ứng viên tự hồi quy của M_byte. Chúng tôi cũng tìm vị trí phân nhánh từ con liên quan đến c, tức là vị trí lớn nhất của từ con có tất cả byte được xác minh là đúng. Các byte được soạn thảo sau vị trí c bị loại bỏ. Lưu ý rằng người soạn thảo được token hóa, trong khi người xác minh không cần token, chúng ta không thể chỉ sửa cho b_{c+1}, tức là một byte sau vị trí phân nhánh, và tiếp tục soạn thảo - điều này gây ra vấn đề với việc soạn thảo, đặc biệt nếu tokenizer không thể tìm thấy từ con một phần mới được cập nhật trong từ vựng được huấn luyện trước của nó. Để tránh khả năng từ con một phần được sửa chữa bị đánh dấu là ngoài từ vựng, chúng tôi sử dụng mô hình xác minh để tạo byte tự hồi quy cho đến khi một byte ranh giới (ví dụ: dấu cách) được tạo ra. Các token được giải mã cuối cùng bao gồm các từ con được soạn thảo đã xác minh và từ con được sửa chữa được tạo bởi mô hình cấp byte. Chúng tôi cache trạng thái ẩn cuối cùng từ người xác minh MambaByte và trạng thái ẩn phân nhánh từ mô hình Mamba từ con cho lần lặp tiếp theo. Để hoàn thiện, chúng tôi cung cấp thuật toán cho giải mã suy đoán thông qua soạn thảo từ con trong Phụ lục F.

Để cho phép tiếp tục trong quá trình quét song song, chúng tôi mở rộng kernel CUDA nhanh từ Mamba (Gu & Dao, 2023), cho phép xác minh khởi động lại từ vị trí không khớp thay vì bắt đầu từ đầu.

4 Thiết lập thí nghiệm

[THIS IS TABLE: Table showing FLOP comparison by model size with columns for Expt, Models, and FLOPs per train byte]

Các thí nghiệm của chúng tôi so sánh MambaByte với một loạt các Transformer và SSM khác dựa trên tokenizer và không cần token. Tất cả các mô hình của chúng tôi sử dụng cùng công thức huấn luyện. Chúng tôi sử dụng một tập hợp các bộ dữ liệu văn bản dạng dài đa dạng: PG19 (Rae et al., 2020), Stories (Trinh & Le, 2018), Books (Gao et al., 2020a), ArXiv (Gao et al., 2020a), và Code (Gao et al., 2020a). Chúng tôi xem xét các mô hình có kích thước khác nhau: đối với MambaByte, điều này được chỉ ra bởi số lượng tham số trong mô hình; đối với MegaByte, là baseline chính được sử dụng, kích thước được chỉ ra bởi số lượng tham số trong mô hình được vá và đầu tạo không vá. Kích thước bộ dữ liệu và độ dài tài liệu trung bình được bao gồm trong Phụ lục A; chi tiết mô hình được đưa ra trong Phụ lục B.

So sánh hiệu suất qua các kiến trúc đòi hỏi sự cẩn thận. Để làm điều này, chúng tôi xem xét hai thiết lập: khớp tính toán và khớp tham số. Thiết lập này cần thiết vì MegaByte Transformer mặc định sử dụng một mô-đun toàn cục làm việc với biểu diễn đầu vào được vá 8×, do đó sử dụng ít FLOP feed-forward hơn 8× mỗi byte so với Transformer thô, trong khi có nhiều tham số hơn đáng kể. Bảng 1 cho thấy kích thước mô hình MegaByte và MambaByte được sử dụng trong các thí nghiệm của chúng tôi. Tính toán FLOP (lượt đi) cho các kiến trúc mô hình khác nhau và các siêu tham số liên quan được sử dụng được chi tiết trong Phụ lục B.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

[THIS IS TABLE: Medium-scale token-free experiments showing BPB results for different models across datasets]
Mô hình cấp byte | Ngữ cảnh | Byte huấn luyện | Test BPB ↓
| | | PG19 | Stories | Books | ArXiv | Code |
Transformer-320M | 1,024 | 80B | 1.057 | 1.064 | 1.097 | 0.816 | 0.575 |
PerceiverAR-248M | 8,192 | 80B | 1.104 | 1.070 | 1.104 | 0.791 | 0.546 |
MegaByte-758M+262M (patch: 8) | 8,192 | 80B | 1.000 | 0.978 | 1.007 | 0.678 | 0.411 |
MambaByte-353M | 8,192 | 30B* | 0.930 | 0.908 | 0.966 | 0.663 | 0.396 |

Bảng 2: Thí nghiệm không cần token quy mô trung bình. MegaByte-758M+262M và MambaByte-353M sử dụng cùng FLOP mỗi byte. (BPB cho Transformer, PerceiverAR, và MegaByte được lấy từ Yu et al. (2023).)

Tất cả các mô hình MambaByte được huấn luyện bằng cơ sở mã nguồn mở Mamba². Khi huấn luyện, chúng tôi xáo trộn các tài liệu và sử dụng các chuỗi liền kề 8,192 byte (một cho mỗi tài liệu), bắt đầu từ một vị trí ngẫu nhiên. Chúng tôi kích hoạt huấn luyện độ chính xác hỗn hợp sử dụng BF16 để có hiệu quả huấn luyện ở quy mô. Bộ tối ưu hóa, bộ lập lịch tỷ lệ học và các chi tiết huấn luyện khác được chỉ định trong Phụ lục C.

5 Kết quả

5.1 Hiệu suất mô hình hóa ngôn ngữ

[THIS IS FIGURE: Two graphs showing length extrapolation results]

Bảng 2 cho thấy hiệu suất mô hình hóa ngôn ngữ theo bit trên byte (BPB) qua mỗi bộ dữ liệu. Cho thí nghiệm này, các mô hình MegaByte-758M+262M và MambaByte sử dụng cùng số FLOP mỗi byte (xem Bảng 1). Chúng tôi quan sát MambaByte vượt trội MegaByte một cách nhất quán qua tất cả bộ dữ liệu. Hơn nữa, MambaByte vượt trội MegaByte với 0.63× ít tính toán và dữ liệu huấn luyện hơn. Ngoài ra, MambaByte-353M cũng vượt trội Transformer cấp byte và PerceiverAR.

Hình 1 khám phá thêm mối quan hệ này bằng cách xem xét các mô hình có cùng số lượng tham số. Các biểu đồ chỉ ra rằng đối với các mô hình MegaByte có cùng kích thước tham số, các mô hình có ít patching đầu vào hơn hoạt động tốt hơn, nhưng khi được chuẩn hóa tính toán, chúng hoạt động tương tự. Thực tế, một Transformer độ dài đầy đủ, mặc dù chậm theo nghĩa tuyệt đối, cũng hoạt động tương tự như mô hình MegaByte khi được chuẩn hóa tính toán. Ngược lại, việc chuyển sang kiến trúc Mamba cải thiện đáng kể cả việc sử dụng tính toán và hiệu suất mô hình.

Theo những phát hiện này, Bảng 3 so sánh một phiên bản lớn hơn của các mô hình này trên bộ dữ liệu PG19, cả có và không có token hóa. Cho thí nghiệm này, chúng tôi so sánh MambaByte-972M với MegaByte-1.3B+350M và các mô hình cấp byte khác, cũng như một số mô hình từ con tiên tiến. (Việc chuyển đổi từ BPB và độ phức tạp cấp từ con sang độ phức tạp cấp từ (PPL) được chi tiết trong Phụ lục E).

²https://github.com/state-spaces/mamba .

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

[THIS IS TABLE: Large-scale experiment on PG19 comparing various models with their parameters, vocabulary, context, bytes trained, and validation/test PPL scores. Table shows subword models and byte models separately.]

Bảng 3: Thí nghiệm quy mô lớn trên PG19. Điểm BPB quan sát được chuyển đổi sang PPL cấp từ để so sánh với các công trình trước. Tất cả các mô hình cấp byte được khớp tính toán. Mamba-1.03B và MambaByte-972M được đánh giá sử dụng ngữ cảnh dài gấp 4× và cửa sổ trượt 16,384 byte. MambaByte-972M vượt trội đáng kể so với các mô hình cấp byte khác và có khả năng cạnh tranh với các mô hình từ con tiên tiến. (Trích dẫn đi kèm chỉ ra công trình mà từ đó kết quả tương ứng được lấy; các trường được đánh dấu − là không rõ.)

[THIS IS FIGURE: Two graphs showing long context experiment results and a table showing noise experiment degradation results]

Chúng tôi thấy rằng MambaByte-972M, ngay cả chỉ được huấn luyện cho 150B byte, vượt trội tất cả các mô hình cấp byte và đạt hiệu suất cạnh tranh với các mô hình từ con.

Hình 3 ghi lại một khía cạnh thú vị khác của MambaByte: khả năng ngoại suy đến các chuỗi dài hơn đáng kể (ít nhất dài gấp 4× so với độ dài huấn luyện) so với các baseline Transformer và SSM cấp byte khác, gợi ý rằng MambaByte có thể tinh chỉnh hiệu quả trạng thái ẩn hồi quy cho các chuỗi dài hơn đáng kể. Như mong đợi, bị hạn chế bởi các embedding vị trí, các mô hình Transformer không ngoại suy vượt quá độ dài huấn luyện.

5.2 Khả năng không cần token

Để kiểm soát lợi ích của kiến trúc Mamba, chúng tôi huấn luyện lại một mô hình Mamba-1.03B từ con trong thiết lập khớp tính toán (xem Bảng 3). Thú vị là, Mamba (từ con) và MambaByte hoạt động tương tự ở cùng kích thước tham số và tính toán huấn luyện. Như đã đề cập trước đây, các mô hình này có hiệu quả cùng dung lượng bộ nhớ mặc dù có sự khác biệt đáng kể trong độ dài chuỗi đầu vào. Chúng tôi cũng thấy rằng Mamba đạt hiệu suất gần tối ưu hiệu quả hơn MambaByte, mặc dù không nhanh gấp 4× như mong đợi, mà là 2.2× nhanh hơn. Hơn nữa, độ phức tạp cho Mamba-1.03B không cải thiện đáng kể vượt quá 150B byte huấn luyện, phù hợp với các quan sát được thực hiện bởi Rae et al. (2020). Với hiệu suất tương tự của Mamba và MambaByte, chúng ta có thể khám phá thêm các khả năng hạ lưu.

Mô hình hóa ngữ cảnh dài hơn. Từ Hình 4, chúng tôi lưu ý rằng Mamba và MambaByte cho thấy khả năng ngoại suy ấn tượng cho các chuỗi dài đến 64× so với độ dài huấn luyện. Chúng tôi giả thuyết rằng MambaByte cho thấy ngoại suy độ dài hơi tốt hơn so với Mamba từ con vì MambaByte mô hình các chuỗi dài gấp 4× khi huấn luyện mặc dù cả hai mô hình xử lý cùng số byte hiệu quả cho mỗi chuỗi huấn luyện.

Thí nghiệm nhiễu tổng hợp. Chúng tôi sử dụng điểm chuẩn nhiễu tổng hợp từ Xue et al. (2022) để kiểm tra tính bền vững của mô hình; chi tiết bổ sung về các thiết lập nhiễu được lưu ý trong Phụ lục G. Chúng tôi xử lý văn bản đầu vào trong tập kiểm tra PG19 thành các đoạn 100 từ được phân cách bằng dấu cách và tiêm nhiễu vào mỗi đoạn có chỉ số lẻ trong khi giữ nguyên văn bản trong đoạn có chỉ số chẵn. Bảng 4 cho thấy sự suy giảm PPL cấp từ với việc tiêm nhiễu, được đo trên các đoạn có chỉ số chẵn. Chúng tôi quan sát rằng hiệu suất Mamba suy giảm đáng kể khi có nhiễu so với MambaByte qua tất cả điều kiện nhiễu, chỉ ra rằng từ vựng được token hóa về cơ bản hạn chế các mô hình từ con. Hiệu ứng này rõ ràng trong các thiết lập nhiễu cụ thể như antspeak (mỗi ký tự được viết hoa và đệm bằng dấu cách) và hoán đổi ký tự (các byte liên tiếp được hoán đổi). Những phát hiện của chúng tôi phù hợp với những gì được quan sát bởi Xue et al. (2022) rằng các mô hình cấp byte bền vững hơn đáng kể đối với lỗi chính tả tình cờ và có chủ ý so với các mô hình từ con.

5.3 Hiệu quả tạo

Suy luận tự hồi quy trong các mô hình Transformer đòi hỏi cache toàn bộ ngữ cảnh, điều có thể ảnh hưởng đáng kể đến tốc độ tạo. MambaByte không gặp phải nút thắt cổ chai này vì nó duy trì một trạng thái ẩn duy nhất cho mỗi lớp phát triển theo thời gian, cho phép thời gian không đổi cho mỗi bước tạo. Bảng 5 so sánh tốc độ tạo văn bản của MambaByte-972M và MambaByte-1.6B với MegaByte-1.3B+350M trên GPU A100 80GB PCIe. Mặc dù MegaByte giảm đáng kể chi phí tạo thông qua patching, chúng tôi quan sát MambaByte nhanh hơn 2.6× trong thiết lập khớp tham số do việc sử dụng tạo hồi quy. Phụ lục H bao gồm thêm thông tin về quá trình tạo.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

[THIS IS TABLE: Generation speed benchmarking table showing Model, Bytes trained, Context, Test BPB, and Generation time for different models]

Bảng 5: Đánh giá tốc độ tạo. Tốc độ tạo 8,192 byte; các trường được đánh dấu − là không rõ. (Trên) BPB trên PG19 và thời gian tạo cho Transformer và MegaByte được lấy từ Yu et al. (2023). (Dưới) MegaByte và MambaByte chạy trên cùng phần cứng; chúng tôi sử dụng triển khai MegaByte nguồn mở có sẵn ở đây.

Tạo thông qua suy đoán từ con. Bảng 6 cho thấy tăng tốc suy luận sử dụng giải mã suy đoán thông qua soạn thảo từ con, được tính trung bình qua 100 prompt được tạo sử dụng các cụm từ phổ biến trong bộ dữ liệu PG19. Chúng tôi sử dụng mô hình Mamba-110M làm người soạn thảo, và các từ con được tạo sử dụng giải mã tham lam. Chúng tôi quan sát rằng thông qua soạn thảo từ con suy đoán, MambaByte có thể đạt tốc độ giải mã gần với Mamba từ con. Hơn nữa, để đánh giá tính trung thực của phương pháp giải mã suy đoán của chúng tôi, chúng tôi sử dụng việc tạo MambaByte-972M giải mã tham lam làm ứng viên tham chiếu cho một prompt nhất định. Chúng tôi báo cáo tỷ lệ log-likelihood của việc tạo ứng viên tham chiếu với log-likelihood của MambaByte tạo chuỗi được giải mã suy đoán, được tính trung bình qua tất cả prompt. Từ Bảng 6, chúng tôi quan sát phương pháp giải mã suy đoán của chúng tôi trung thực hơn với MambaByte-972M so với Mamba-1.03B từ con.

[THIS IS TABLE: Generation speed with subword speculation showing Model, Context, Relative speedup, and Log-odds ratio]

Bảng 6: Tốc độ tạo với suy đoán từ con. Kết quả thực nghiệm để tăng tốc suy luận từ mô hình MambaByte-972M. Tốc độ được đo trong việc tạo 8,192 byte; người soạn thảo soạn thảo ba từ con mỗi lần lặp và người xác minh chấp nhận nếu các byte được soạn thảo nằm trong top-3 ứng viên của nó.

6 Công trình liên quan

Các mô hình ngôn ngữ không cần token. Token hóa đã là nền tảng của mô hình hóa ngôn ngữ và quan trọng trong việc nâng cao hiệu quả và hiểu biết của mô hình. Một số thuật toán đã được đề xuất để giải quyết các vấn đề token hóa, bao gồm kích thước từ vựng lớn và xử lý các token ngoài từ vựng: Byte-Pair Encoding (Sennrich et al., 2015), WordPiece (Schuster & Nakajima, 2012; Devlin et al., 2018), và SentencePiece (Kudo & Richardson, 2018). Sự chuyển đổi gần đây hướng tới mô hình hóa ký tự (Tay et al., 2022; Ma et al., 2020; Mielke & Eisner, 2019) và cấp byte (Yu et al., 2023; Xue et al., 2022; Belouadi & Eger, 2022) nhằm đạt được tiền xử lý không cần token, từ đó tạo điều kiện cho khả năng thích ứng mô hình cải thiện và khả năng chuyển đổi miền trong mô hình hóa ngôn ngữ và xử lý đa ngôn ngữ.

Các mô hình không cần attention. Các mô hình không cần attention cung cấp hiệu quả tính toán và bộ nhớ nâng cao và ngày càng được điều chỉnh cho một số tác vụ xử lý ngôn ngữ, bao gồm mô hình hóa ngôn ngữ tự hồi quy. Các mô hình như S4 (Gu et al., 2021) và các biến thể tiếp theo (Gupta et al., 2022; Gu et al., 2022) đã chứng minh kết quả đầy hứa hẹn trong mô hình hóa ngôn ngữ cấp từ con. Các kiến trúc SSM có cổng như GSS (Mehta et al., 2023) và BiGS Wang et al. (2022) đã kết hợp cơ chế cổng vào SSM cho mô hình hóa ngôn ngữ (hai chiều). Mô hình Mamba được giới thiệu gần đây (Gu & Dao, 2023) cho rằng động lực học không thay đổi của các phương pháp này không kết hợp được việc lựa chọn ngữ cảnh cụ thể đầu vào trong trạng thái ẩn, điều có thể quan trọng cho các tác vụ như mô hình hóa ngôn ngữ. Mamba đã được chỉ ra vượt trội Transformer qua các kích thước mô hình và ở quy mô. Ngoài ra, một số kiến trúc mô hình dưới-bậc hai khác (Yang et al., 2023b; De et al., 2024; Arora et al., 2023; 2024; Fu et al., 2024a) cũng đã được đề xuất. Ngoài mô hình hóa ngôn ngữ, SSM và Mamba đã được áp dụng trong các phương thức khác, bao gồm hình ảnh (Yan et al., 2024), âm thanh (Goel et al., 2022), và tin sinh học (Schiff et al., 2024).

Giải mã suy đoán cho suy luận nhanh. Giải mã suy đoán (Spector & Re, 2023; Leviathan et al., 2023; Chen et al., 2023a; Xia et al., 2023) đã nổi lên như một phương pháp đầy hứa hẹn để tăng tốc suy luận của các mô hình ngôn ngữ lớn, cụ thể là Transformer. Ý tưởng cốt lõi là sử dụng một mô hình soạn thảo nhỏ hơn để tạo suy đoán các token ứng viên, sau đó mô hình đích lớn hơn xác minh. Leviathan et al. (2023); Chen et al. (2023a) đề xuất một sơ đồ lấy mẫu từ chối để cải thiện chất lượng suy luận. Spector & Re (2023) tái cấu trúc các token ứng viên thành một cây để cho phép xác minh hiệu quả hơn. Các phương pháp bổ sung cũng điều tra các mô hình soạn thảo được huấn luyện (Bhendawade et al., 2024; Chen et al., 2023b; Liu et al., 2023) và các mô hình soạn thảo không cần huấn luyện (He et al., 2023; Yang et al., 2023a; Fu

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

et al., 2024b). Mặc dù các phương pháp trước đây sử dụng các mô hình soạn thảo và xác minh với cùng sơ đồ token hóa cơ bản, bài báo này đề xuất sử dụng mô hình Mamba từ con nhỏ hơn làm người soạn thảo suy đoán và MambaByte lớn hơn làm người xác minh cấp byte.

7 Kết luận

Chúng tôi giới thiệu MambaByte, một SSM không cần token để mô hình hóa các chuỗi byte dài. MambaByte vượt trội các mô hình cấp byte khác trên một số bộ dữ liệu và cho thấy kết quả cạnh tranh với Transformer từ con trong khi bền vững đáng kể với hỏng hóc văn bản, do đó phục vụ như một thay thế token hóa đầy hứa hẹn. Do tính chất hồi quy của chúng, SSM cho phép tạo văn bản nhanh hơn đáng kể so với các mô hình Transformer. Chúng tôi cải thiện thêm hiệu quả tạo thông qua giải mã suy đoán sử dụng soạn thảo từ con và chỉ ra MambaByte đạt được hiệu quả giải mã tương tự như Mamba từ con, làm cho các mô hình byte trở nên thực tế. Những phát hiện của chúng tôi thiết lập khả năng của mô hình hóa ngôn ngữ không cần token trong các mô hình lớn tương lai.

Lời cảm ơn

Chúng tôi cảm ơn Albert Gu về những nhận xét hữu ích của họ về MambaByte, Tri Dao về hướng dẫn của họ trong việc mở rộng kernel quét chọn lọc trong Mamba, và các tác giả của MegaByte, Lili Yu và Mike Lewis, về các làm rõ về quy trình huấn luyện và suy luận MegaByte. Công trình này được hỗ trợ bởi NSF IIS-1901030 và NSF CAREER 2037519.

Tài liệu tham khảo

[Phần này chứa danh sách đầy đủ các tài liệu tham khảo học thuật với các URL và DOI tương ứng]

--- CÁC TRANG TIẾP THEO ---

[Các trang còn lại chứa các phụ lục chi tiết về:
- Phụ lục A: Chi tiết bộ dữ liệu  
- Phụ lục B: Mô hình hóa hạn chế tính toán
- Phụ lục C: Công thức huấn luyện
- Phụ lục D: Rời rạc hóa và lựa chọn
- Phụ lục E: Các chỉ số đánh giá
- Phụ lục F: Thuật toán giải mã suy đoán thông qua soạn thảo từ con
- Phụ lục G: Thiết lập nhiễu tổng hợp
- Phụ lục H: Mẫu tạo PG19

Và các bảng, hình ảnh, công thức toán học chi tiết hỗ trợ cho nghiên cứu.]
