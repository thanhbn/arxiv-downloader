# ReMamba: Trang bị Mamba với khả năng mô hình hóa chuỗi dài hiệu quả

Danlong Yuan1,2*, Jiahao Liu5, Bei Li5, Huishuai Zhang1,3†,
Jingang Wang5,Xunliang Cai5Dongyan Zhao1,2,3,4 †
1Viện Công nghệ Máy tính Wangxuan, Đại học Bắc Kinh,
2Trung tâm Khoa học Dữ liệu, AAIS, Đại học Bắc Kinh,
3Phòng thí nghiệm Trọng điểm Quốc gia về Trí tuệ Nhân tạo Tổng quát,4BIGAI, Bắc Kinh, Trung Quốc;,
5Meituan
Liên hệ: danlongyuan@stu.pku.edu.cn, {zhanghuishuai,zhaodongyan}@pku.edu.cn,
{liujiahao12,libei17,wangjingang02,caixunliang}@meituan.com
Mã nguồn: https://github.com/lblankl/ReMamba

# Tóm tắt

Trong khi kiến trúc Mamba thể hiện hiệu quả suy luận vượt trội và hiệu suất cạnh tranh trên các tác vụ xử lý ngôn ngữ tự nhiên (NLP) ngữ cảnh ngắn, bằng chứng thực nghiệm cho thấy khả năng hiểu ngữ cảnh dài của nó còn hạn chế so với các mô hình dựa trên transformer. Trong nghiên cứu này, chúng tôi điều tra các vấn đề hiệu quả ngữ cảnh dài của các mô hình Mamba và đề xuất ReMamba, nhằm nâng cao khả năng hiểu ngữ cảnh dài của Mamba. ReMamba kết hợp các kỹ thuật nén chọn lọc và thích ứng trong quy trình tái chuyển tiếp hai giai đoạn, chỉ phát sinh chi phí bổ sung tối thiểu cho việc suy luận. Kết quả thực nghiệm trên các benchmark LongBench và L-Eval chứng minh hiệu quả của ReMamba, cải thiện 3,2 và 1,6 điểm so với các baseline tương ứng, và đạt được hiệu suất gần tương đương với các mô hình transformer cùng kích thước.

# 1 Giới thiệu

Transformers (Vaswani et al., 2017), vốn là xương sống của hầu hết các LLM, gặp phải những thách thức đáng kể khi xử lý văn bản dài. Nhu cầu tính toán bậc hai và chi phí bộ nhớ tuyến tính của cơ chế attention trở nên cấm đoán khi độ dài văn bản tăng lên. Độ phức tạp này tạo ra rào cản đáng kể để mô hình hóa hiệu quả văn bản dài, điều này rất quan trọng cho việc phát triển LLM. Để giải quyết vấn đề này, Mamba được đề xuất như một giải pháp (Gu và Dao, 2024), sử dụng chế độ suy luận hồi quy đảm bảo độ phức tạp thời gian tuyến tính và nén thông tin vào kích thước trạng thái cố định. Điều này dẫn đến nhu cầu bộ nhớ không đổi trong quá trình suy luận. Hơn nữa, Mamba loại bỏ nhu cầu mã hóa vị trí, về mặt lý thuyết cho phép nó xử lý đầu vào có độ dài bất kỳ, trong khi vẫn hoạt động cạnh tranh với transformers trên các tác vụ downstream. Ngay sau đó, Mamba2

*Công việc thực hiện trong thời gian thực tập tại Meituan
†Tác giả liên hệ.

Hình 1: So sánh các mô hình Mamba đã được đào tạo trước và Transformers có kích thước tương đương về tốc độ, hiệu suất ngữ cảnh ngắn và ngữ cảnh dài. Tốc độ được đo trong điều kiện 6k token đầu vào và 1k token đầu ra. "điểm ngắn" đại diện cho độ chính xác trung bình trên sáu tác vụ (HellaSwag, PIQA, Arc-E, Arc-C, WinoGrande, OpenbookQA) được đánh giá trong LM evaluation harness (Gao et al., 2023). "điểm dài" tương ứng với điểm số trung bình trên benchmark LongBench-E (Bai et al., 2024). Đáng chú ý, tất cả các đánh giá LongBench sử dụng độ dài token tối đa là 2k để phù hợp với cấu hình đào tạo của mô hình.

được giới thiệu, đơn giản hóa ma trận A có cấu trúc của Mamba để cho phép đào tạo nhanh hơn và tăng kích thước trạng thái (Dao và Gu, 2024).

Bất chấp những ưu điểm này, một số nghiên cứu tiết lộ rằng các mô hình Mamba không hoạt động tốt như mong đợi khi xử lý văn bản dài đạt 2k token trở lên (Waleffe et al., 2024). Như được mô tả trong Hình 1, phát hiện thực nghiệm của chúng tôi cho thấy mô hình Mamba đã được đào tạo trước vượt trội hơn các Transformers được đào tạo trước có kích thước tương đương, chẳng hạn như Llama-3b (Geng và Liu, 2023), trên các tác vụ ngữ cảnh ngắn. Ngược lại, một sự suy giảm hiệu suất đáng kể được quan sát thấy đối với Mamba trên các tác vụ ngữ cảnh dài so với Transformers. Sự chênh lệch hiệu suất này làm nổi bật một hạn chế đáng kể của các mô hình Mamba trong các ứng dụng ngữ cảnh dài thực tế.

Vấn đề thiếu hụt ngữ cảnh dài này của Mamba thường được cho là do bản chất giống RNN của nó. Loại kiến trúc này thể hiện những hạn chế trong việc bảo tồn thông tin quan trọng từ các chuỗi đầu vào trước đó khi độ dài ngữ cảnh tăng lên do bộ nhớ có kích thước cố định (Wen et al., 2024; Yang et al., 2024b). Các kiến trúc lai (Lieber et al., 2024; Ren et al., 2024; Park et al., 2024) đã tìm cách giảm thiểu vấn đề này bằng cách tích hợp cơ chế attention từ transformers. Tuy nhiên, những cách tiếp cận này thường dẫn đến giảm hiệu quả tính toán và tăng tiêu thụ bộ nhớ.

Thách thức chính trong Mamba là sự suy giảm quá mức thông tin xa. ReMamba giải quyết điều này bằng cách sử dụng chiến lược nén hiệu quả để cô đọng thông tin và giảm độ dài ngữ cảnh. Cụ thể, nó chọn top-k trạng thái ẩn trong lần chuyển tiếp đầu tiên và tích hợp chúng vào không gian trạng thái bằng cách sử dụng cơ chế chọn lọc của Mamba trong lần chuyển thứ hai. ReMamba giới thiệu chi phí tính toán tối thiểu (một lần chuyển tiếp bổ sung) và duy trì mức tiêu thụ bộ nhớ thấp, không đổi. Kết quả thực nghiệm chứng minh rằng cách tiếp cận của chúng tôi cải thiện đáng kể hiệu suất ngữ cảnh dài của Mamba, đưa nó gần với hiệu suất của transformers. Mô hình ReMamba của chúng tôi đạt được cải thiện 3,2 so với baseline trên LongBench (Bai et al., 2024) và cải thiện 1,6 trên L-Eval (An et al., 2023). Hơn nữa, phương pháp của chúng tôi thể hiện khả năng chuyển giao sang Mamba2, mang lại cải thiện 1,6 trên LongBench.

# 2 Công trình liên quan

## 2.1 Mamba

Mô hình không gian trạng thái chọn ma trận chuyển trạng thái ˆA (ma trận chuyển trạng thái) và ˆB (ma trận hệ số đầu vào) bất biến theo thời gian do đó thiếu tính biểu đạt và linh hoạt. Mamba (Gu và Dao, 2024) đề xuất làm cho ˆA và ˆB phụ thuộc động vào đầu vào.

Nhớ lại rằng trong một lớp Mamba l, các trạng thái SSM S được biến đổi như sau:

∆l_{t-1} = Softplus(Proj1(h^{l-1}_{t-1})),                    (1a)
B^l_{t-1} = Proj2(h^{l-1}_{t-1}),                            (1b)
ˆA^l, ˆB^l_{t-1} = discretize(A^l, B^l_{t-1}, ∆^l_{t-1}),    (1c)
h'^l_{t-1} = Proj3(h^{l-1}_{t-1}),                           (1d)
S^l_t = ˆA^l ⊗ S^l_{t-1} + ˆB^l_{t-1} ⊗ h'^l_{t-1}^T.      (1e)

Ở đây, h^{l-1}_{t-1} ∈ R^H đại diện cho trạng thái ẩn đầu ra của Mamba tại lớp l−1 và bước thời gian t−1. Hàm Softplus được ký hiệu bởi Softplus, và Proj1, Proj2, và Proj3 là các từ viết tắt cho nhiều phép chiếu không gian.

Hơn nữa, ∆^l_{t-1} ∈ R^{H'} là bước thời gian rời rạc tương ứng với cơ chế chọn lọc trong Mamba, trong đó H' là kích thước ẩn trung gian. Các ma trận biến đổi trạng thái liên tục và rời rạc tại lớp l được cho bởi A^l, ˆA^l ∈ R^{H'×N}, tương ứng. Các ma trận hệ số đầu vào liên tục và rời rạc được ký hiệu bởi B^l_{t-1}, ˆB^l_{t-1} ∈ R^{N×1}. Kích thước trạng thái được biểu thị bởi N. Phương pháp rời rạc hóa để tính ˆA và ˆB được chỉ ra bởi "discretize". Vector h'^l_{t-1} ∈ R^{H'×1} và trạng thái SSM được biểu thị bởi S^l_t ∈ R^{H'×N}. Ký hiệu ⊗ biểu thị phép nhân theo từng phần tử, và ˆB^l_{t-1} ⊗ h'^l_{t-1}^T biểu thị phép nhân ma trận.

## 2.2 Mamba2

Dao và Gu (2024) chứng minh lý thuyết về mối liên hệ giữa các mô hình không gian trạng thái có cấu trúc và cơ chế attention. Họ cũng đơn giản hóa ma trận có cấu trúc ˆA thành cấu trúc vô hướng-nhân-đồng nhất và do đó phát triển một khung duality không gian trạng thái (SSD) mới với các mẫu đa đầu tương tự như transformers.

## 2.3 Mamba và Transformers ngữ cảnh dài

Nội suy vị trí đã được sử dụng rộng rãi như một kỹ thuật để mở rộng độ dài ngữ cảnh của transformers (Chen et al., 2023; Peng et al., 2024; Ding et al., 2024). Nhưng chúng được chuyên biệt hóa cho transformers.

Mamba đã được phát hiện gặp khó khăn trong việc duy trì hiệu suất vượt quá độ dài ngữ cảnh đào tạo trước mà không cần đào tạo bổ sung. LongMamba (Peiyuan, 2024) đã thực hiện nỗ lực thành công đầu tiên để mở rộng độ dài ngữ cảnh của Mamba thông qua một vài giờ tinh chỉnh ngữ cảnh dài. DeciMamba (Ben-Kish et al., 2024) nhằm giải quyết vấn đề mở rộng ngữ cảnh của Mamba theo cách không cần đào tạo, đề xuất phương pháp giảm dần độ dài chuỗi qua các lớp bằng cách loại bỏ thực nghiệm các token không quan trọng.

Tuy nhiên, các thực nghiệm của chúng tôi chứng minh rằng Mamba được tinh chỉnh ngữ cảnh dài vẫn thua kém các transformers được tinh chỉnh ngữ cảnh dài cùng kích thước, mặc dù sử dụng cùng dữ liệu. Hơn nữa, DeciMamba2.8b dường như không đủ hiệu quả khi được đánh giá trên hai benchmark ngữ cảnh dài được sử dụng rộng rãi.

# 3 Nghiên cứu sơ bộ

Nén cache KV được sử dụng rộng rãi trong transformers để giảm tiêu thụ bộ nhớ và cải thiện tốc độ suy luận. Tuy nhiên, nén prompt thường dẫn đến suy giảm hiệu suất so với việc tạo ra độ dài ngữ cảnh đầy đủ trong transformers. Không giống như transformers, Mamba không sử dụng cache KV; thay vào đó, nó sử dụng không gian trạng thái có kích thước cố định trong mỗi lớp để bảo tồn bộ nhớ ngữ cảnh. Một vấn đề tiềm ẩn với Mamba là xu hướng quên thông tin xa. Trong nghiên cứu sơ bộ của chúng tôi, chúng tôi giả thuyết rằng việc cập nhật không gian trạng thái trong Mamba không đủ để nén hiệu quả thông tin ngữ cảnh, và các kỹ thuật như nén prompt có thể giúp giải quyết vấn đề này.

Để khám phá điều này, chúng tôi áp dụng phương pháp nén prompt đơn giản: chúng tôi thay thế một phần token ngữ cảnh bằng một vài trạng thái ẩn được chọn ngẫu nhiên từ lớp cuối cùng của Mamba, tạo ra một prompt ngắn hơn (được gọi là Mamba ngẫu nhiên). Cách tiếp cận này tương tự như của Ge et al. (2024), sử dụng soft prompts để nén thông tin. Theo trực giác, phương pháp nén ngẫu nhiên này có thể dẫn đến mất thông tin đáng kể và hiệu suất suy giảm. Tuy nhiên, kết quả của chúng tôi cho thấy điểm số trung bình cho các độ dài ngữ cảnh khác nhau trên LongBench giữa Mamba bình thường và Mamba ngẫu nhiên là tương tự khi cả hai được đào tạo trên cùng một bộ dữ liệu ngữ cảnh dài. Hơn nữa, Mamba ngẫu nhiên vượt trội hơn Mamba bình thường ở một số độ dài ngữ cảnh nhất định, như được hiển thị trong Hình 3 của 5.4.1. random_select (SFT) đại diện cho Mamba ngẫu nhiên được tinh chỉnh, trong khi Mamba (SFT) đại diện cho Mamba vanilla được tinh chỉnh.

Quan sát này cho thấy mất thông tin trong Mamba khi xử lý ngữ cảnh dài là đáng kể. Để giải quyết điều này, chúng tôi đề xuất nén chọn lọc và thích ứng chọn lọc thông qua việc tận dụng cơ chế cập nhật không gian trạng thái của Mamba.

# 4 Phương pháp

ReMamba bao gồm hai giai đoạn chuyển tiếp. Trong giai đoạn đầu tiên, ba mạng feed-forward được sử dụng để giúp xác định tầm quan trọng của các trạng thái ẩn từ lớp cuối cùng của Mamba. Các trạng thái ẩn này được chọn dựa trên điểm số quan trọng của chúng. Giai đoạn thứ hai tích hợp các trạng thái ẩn nén này với ngữ cảnh đầu vào, thích ứng cơ chế chọn lọc của Mamba để kết hợp chúng vào không gian trạng thái.

Phương pháp đề xuất của chúng tôi lấy cảm hứng từ các kỹ thuật được sử dụng trong nén cache KV (Mu et al., 2023; Ge et al., 2024; Yang et al., 2024a; Chevalier et al., 2023; Hwang et al., 2024; Gao et al., 2024) bằng cách tận dụng chính mô hình ngôn ngữ để tổng hợp thông tin thông qua các trạng thái ẩn và sử dụng cơ chế tính điểm để chọn các biểu diễn nổi bật nhất. Tuy nhiên, khác với transformers, chiến lược nén của ReMamba tập trung vào hai mục tiêu chính: 1) nén và giữ lại chọn lọc thông tin quan trọng để giảm thiểu suy giảm thông tin, và 2) giảm tần suất cập nhật không gian trạng thái để tiếp tục giảm thiểu mất thông tin.

Phương pháp lựa chọn được sử dụng trong ReMamba là cách tiếp cận dựa trên key-query-value đơn giản hóa, thường được sử dụng trong các tác vụ Retrieval-Augmented Generation và tóm tắt (Lewis et al., 2020; Mao et al., 2022). Trong bối cảnh này, chúng tôi chọn các trạng thái ẩn quan trọng nhất từ lớp cuối cùng của Mamba để giảm thiểu mất thông tin trong quá trình nén.

## 4.1 Giai đoạn 1: Nén chọn lọc

Nén chọn lọc bao gồm việc nén chọn lọc prompt đầu vào bằng cách tận dụng các trạng thái ẩn lớp cuối cùng của mô hình Mamba để giảm cập nhật trạng thái và hợp nhất thông tin.

Giả sử độ dài chuỗi là L và các embedding token ngữ cảnh là {t_i}^L_{i=1}. Chúng tôi định nghĩa phạm vi tương đối cần nén là range := (s, e), trong đó e = s + p, với s và e biểu thị các vị trí bắt đầu và kết thúc tương đối, tương ứng, và p biểu thị độ dài tương đối cần nén. Các giá trị này thỏa mãn 0 <= s, p, e <= 1. Tập chỉ số của ngữ cảnh cần nén là R := [S, E], trong đó S = L·s + 1 và E = L·(s + p). Do đó, độ dài của prompt cần nén là L' = E - S + 1.

Để thuận tiện, chúng tôi sử dụng R để biểu thị cả tập chỉ số và tập các token thực tế trong ngữ cảnh cần nén. Hơn nữa, chúng tôi định nghĩa tỉ lệ nén rho và nén ngữ cảnh được chọn R thành K := |R|·rho biểu diễn ẩn.

Trong Hình 2, các thiết lập siêu tham số nén là: s = 0,2, p = 0,4, range = (0,2, 0,6), R = [3,6], rho = 0,5, K = 2. Trong các thực nghiệm của chúng tôi, chúng tôi thấy rằng s = 0 mang lại kết quả tốt nhất, điều này có thể được cho là do bản chất mô hình hóa ngôn ngữ nhân quả của Mamba (điều này sẽ được thảo luận chi tiết hơn trong Phụ lục A.3).

Như được hiển thị trong Giai đoạn 1 của Hình 2, chúng tôi ký hiệu các trạng thái ẩn đầu ra của lớp cuối cùng là {h_i}^L_{i=1}, trong đó mỗi h_i ∈ R^H với H biểu thị kích thước ẩn. Sau đó chúng tôi biến đổi trạng thái ẩn cuối cùng h_L thành trạng thái ẩn query, cụ thể là q, thông qua một lớp feed-forward có tên Query. Ngoài ra, các trạng thái ẩn cần nén, được ký hiệu là {h_i}^E_{i=S}, được biến đổi thành {k_i}^E_{i=S} thông qua lớp Key (phép biến đổi này không được hiển thị trong Hình 2). Cuối cùng, các điểm số độ tương tự cosine, Cos = {cos_i}^E_{i=S}, được tính toán để phục vụ như điểm số quan trọng cho các trạng thái ẩn {h_i}^E_{i=S}. Việc tính toán q, k_i và cos_i được công thức hóa như sau:

q = Query(h_L)
{k_i}^E_{i=S} = Key({h_i}^E_{i=S})
cos_i = (k_i · q) / max(||k_i||_2 · ||q||_2, ε)                    (2)

trong đó k_i biểu thị trạng thái ẩn được biến đổi tại vị trí i, và cos_i tính độ tương tự cosine giữa q và k_i. Hằng số ε ngăn chặn chia cho zero.

Chúng tôi chọn top-K trạng thái ẩn h_j, trong đó j ∈ G, từ các trạng thái ẩn {h_i}^E_{i=S} dựa trên điểm số quan trọng của chúng, được ký hiệu bởi Cos. Tập chỉ số G được định nghĩa là:

G = arg max_{A⊂{S,S+1,...,E},|A|=K} ∑_{i∈A} cos_i                 (3)

Lưu ý rằng thứ tự ban đầu của các chỉ số này được bảo tồn.

Trong mô hình của chúng tôi, sau khi chọn top-K trạng thái ẩn h_j, chúng tôi áp dụng một lớp feed-forward, Value, để chiếu chúng vào không gian ẩn embedding token:

{v_i}^K_{i=1} = V({h_j}, j ∈ G)                                   (4)

Các điểm số độ tương tự cosine tương ứng của chúng là {cos'_i}^K_{i=1}. Sau đó chúng tôi thay thế các embedding token {t_i}^E_{i=S} (R) bằng {v_i}^K_{i=1}. Do đó, các embedding đầu vào mới cho Mamba được thay thế bởi:

T_new = Cat({t_i}^{S-1}_{i=1}, {v_i}^K_{i=1}, {t_i}^L_{i=E+1})    (5)
      = {t'_i}^{L-L'+K}_{i=1}                                     (6)

trong đó Cat biểu thị phép nối. Độ dài của T_new là L - L' + K, dẫn đến một chuỗi đầu vào ngắn hơn đáng kể cho lần chuyển tiếp thứ hai so với lần đầu tiên.

## 4.2 Giai đoạn 2: Thích ứng chọn lọc

Một thách thức đáng kể trong việc sử dụng lựa chọn top-K dựa trên điểm số quan trọng là tính không khả vi của nó, điều này cản trở khả năng đào tạo hiệu quả các mô hình như vậy. Ở đây chúng tôi đề xuất một khung tích hợp điểm số quan trọng vào các cơ chế chọn lọc của mô hình Mamba.

Đối với các trạng thái ẩn (embeddings) không yêu cầu nén trong giai đoạn 1, cụ thể là {t_i}^{S-1}_{i=1} và {t_i}^L_{i=E+1}, thuật toán Mamba tiêu chuẩn được áp dụng trong lần chuyển tiếp thứ hai. Đối với embeddings tại các vị trí được chọn, cụ thể là {t'_i}^{S+K-1}_{i=S} hoặc tương đương {v_i}^K_{i=1}, Phương trình 1a được công thức hóa lại như sau:

alpha = ReLU(cos'_{t-1})
∆^l_{t-1}' = Proj1(h^{l-1}_{t-1})
delta = ∆^l_{t-1}' · alpha + Θ^l
∆^l_{t-1} = Softplus(delta)                                      (7)

trong đó Θ^l ∈ R^{H'} là tham số offset có thể đào tạo theo lớp kiểm soát cường độ quy mô. ReLU là hàm kích hoạt. Theo trực giác, các trạng thái ẩn có điểm số quan trọng thấp nên tác động tối thiểu đến các tính toán mô hình. Do đó, chúng tôi xấp xỉ hành vi này bằng cách đặt các giá trị ∆ tương ứng của chúng gần bằng zero. Lý tưởng nhất, việc nhân trực tiếp ∆ với alpha sẽ chính xác hơn, nhưng điều này đòi hỏi sửa đổi thuật toán quét chọn lọc, khiến chúng tôi áp dụng cách tiếp cận đơn giản hơn.

## 4.3 Đào tạo

Sau các quá trình mã hóa chuyển tiếp, việc tạo ngôn ngữ nhân quả tiêu chuẩn được áp dụng bằng cách sử dụng kiến trúc Mamba. Trong quá trình đào tạo, các tham số mới được giới thiệu trong cơ chế nén chọn lọc được tối ưu hóa. Các tham số này, ngoại trừ Θ được khởi tạo về tất cả số không, được khởi tạo với một tập con của các trọng số từ ma trận in_proj của lớp đầu tiên. Ngoài ra, đối với các tham số trong Mamba, ma trận dt_proj được đào tạo đầy đủ, trong khi in_proj, out_proj, embeddings và lm_head được cập nhật bằng Low-Rank Adaptation (LoRA) (Hu et al., 2022). Trong triển khai tốt nhất của chúng tôi, để nhấn mạnh tầm quan trọng của thông tin cụ thể, các gradient chảy vào điểm số quan trọng được chia tỷ lệ tỷ lệ thuận với các điểm số này. Cách tiếp cận này một cách trực quan ưu tiên việc đào tạo các biểu diễn quan trọng hơn.

# 5 Thực nghiệm

## 5.1 Thiết lập thực nghiệm

Mô hình của chúng tôi được thiết kế cho các tác vụ hỏi đáp ngữ cảnh dài, đòi hỏi một corpus đáng kể dữ liệu điều chỉnh hướng dẫn ngữ cảnh dài. Để đạt được mục đích này, chúng tôi tận dụng bộ dữ liệu OpenOrca (Mukherjee et al., 2023) và LongAlpaca-12k (Chen et al., 2024). Bộ dữ liệu trước bao gồm một bộ sưu tập phong phú các căn chỉnh dữ liệu FLAN được tăng cường bởi ChatGPT, trong khi bộ dữ liệu sau là một bộ dữ liệu căn chỉnh ngữ cảnh dài. Chúng tôi ban đầu lọc các phiên bản điều chỉnh hướng dẫn dài từ OpenOrca và nối chúng với LongAlpaca. Để phù hợp với các ràng buộc bộ nhớ thiết bị, các prompt được cắt ngắn đến độ dài tối đa 6.000 token. Quá trình này tạo ra khoảng 200.000 ví dụ đào tạo ngữ cảnh dài. Để tăng cường tính đa dạng của dữ liệu đào tạo, 300.000 phiên bản tiêu chuẩn ban đầu từ OpenOrca được kết hợp. Bộ dữ liệu này được gọi là bộ dữ liệu LongOrca.

Chúng tôi tinh chỉnh mô hình Mamba 2.8b baseline và mô hình ReMamba của chúng tôi trên cùng một bộ dữ liệu. Chúng tôi cũng tinh chỉnh DeciMamba2.8b (Ben-Kish et al., 2024) và Llama-3b (Geng và Liu, 2023) để tham khảo. DeciMamba nhằm giải quyết vấn đề mở rộng ngữ cảnh của Mamba mà không yêu cầu đào tạo bổ sung. Mặc dù cách tiếp cận của chúng tôi khác một chút về mặt thiết lập và mục tiêu, chúng tôi vẫn tinh chỉnh DeciMamba2.8b bằng cùng dữ liệu. Với giới hạn mã hóa vị trí tối đa 2k của Llama-3b, chúng tôi tiến hành các thực nghiệm tinh chỉnh bằng kỹ thuật nội suy vị trí tuyến tính đơn giản (Chen et al., 2023) để mở rộng độ dài ngữ cảnh của nó. Quá trình xây dựng dữ liệu cho Llama-3b giống hệt với Mamba. Chi tiết có thể được tìm thấy trong A.1. Lưu ý rằng Mamba2.8b là mô hình lớn nhất mà chúng tôi có thể có được.

## 5.2 Đánh giá

Chúng tôi tiến hành phân tích so sánh mô hình của chúng tôi với baseline Mamba2.8b và DeciMamba2.8b (cả được tinh chỉnh và đào tạo trước) trên benchmark LongBench được áp dụng rộng rãi (Bai et al., 2024) và benchmark LEval (An et al., 2023), bao gồm một tập hợp đa dạng các tác vụ ngữ cảnh dài thực tế đầy thách thức. Để nhất quán, các cấu hình template prompt và giải mã tham lam giống nhau được sử dụng trên tất cả các mô hình.

Để cung cấp điểm tham chiếu, hiệu suất của một kiến trúc transformer có kích thước tương tự (Llama-3b) cũng được bao gồm. Cả đánh giá đào tạo trước và tinh chỉnh của Llama-3b đều sử dụng kỹ thuật nội suy vị trí tuyến tính.

## 5.3 Kết quả

**Kết quả trên LongBench** Chúng tôi chọn nhánh tiếng Anh của LongBench vì tập đào tạo của chúng tôi chỉ chứa tiếng Anh. Giá trị cao hơn trên tất cả các chỉ số cho thấy hiệu suất tốt hơn. Chúng tôi so sánh hiệu suất của các mô hình trong các tác vụ chi tiết trong Bảng 1 dưới độ dài tối đa 6k tương ứng với thiết lập đào tạo. Ở đây các siêu tham số cho ReMamba là: s = 0, p = 0.18 và rho = 0.009. Chúng tôi cũng sẽ cho thấy sau này tính bền vững của mô hình với các kết hợp siêu tham số khác nhau. Bảng 1 cho thấy mô hình ReMamba của chúng tôi cải thiện điểm số trung bình trên LongBench 3.23 so với baseline Mamba SFT. Mô hình của chúng tôi tiếp cận baseline transformer đã được đào tạo trước và tinh chỉnh. Kết quả cho DeciMamba chỉ ra rằng nó có thể không đủ hiệu quả cho các tác vụ trong LongBench hoặc nó có thể nhạy cảm với việc chọn siêu tham số.

**Kết quả trên LEval** Chúng tôi so sánh hiệu suất trên các tác vụ đóng của L-Eval. Tất cả các chỉ số càng cao càng tốt. Một ảnh chụp điểm số tác vụ chi tiết cho độ dài tối đa 6k được trình bày trong Bảng 2. Chúng tôi có thể chứng kiến cải thiện 1.64 về điểm số trung bình so với baseline Mamba SFT. Ở đây thiết lập siêu tham số cho ReMamba là: s = 0, p = 0.20 và rho = 0.05. Kết quả cho DeciMamba2.8b cũng không cho thấy cải thiện đáng kể.

## 5.4 Phân tích và thảo luận

### 5.4.1 Nghiên cứu loại bỏ

Để xác minh hiệu quả của các mô-đun chúng tôi giới thiệu, chúng tôi tiến hành nghiên cứu loại bỏ bằng cách so sánh ReMamba với ba phương pháp thay thế: 1. Lựa chọn ngẫu nhiên: chọn ngẫu nhiên các trạng thái ẩn làm thông tin nén theo rho. 2. Lựa chọn cố định: với rho cho trước, chúng tôi chọn đủ trạng thái ẩn mỗi k vị trí. Khoảng cách k được tính dựa trên tỉ lệ nén. 3. Lựa chọn nhân: Biến thể này chỉ sửa đổi quá trình thích ứng chọn lọc bằng cách nhân trực tiếp điểm số quan trọng với các trạng thái ẩn được chọn, phù hợp với cách tiếp cận được đề xuất bởi Raposo et al. (2024). Tất cả các mô hình đó được đào tạo trên cùng dữ liệu với ReMamba.

Chúng tôi báo cáo điểm số trung bình trên LongBench qua các độ dài đầu vào tối đa khác nhau. Như được minh họa trong Hình 3, cả phương pháp lựa chọn cố định và ngẫu nhiên đều đạt được hiệu suất tương đương với baseline Mamba được tinh chỉnh. Thú vị là, các phương pháp này thậm chí còn vượt trội hơn Mamba ở độ dài 5k và 6k. Quan sát này xác nhận giả thuyết của chúng tôi rằng các mô hình Mamba gặp phải vấn đề quên nghiêm trọng. Ngay cả các phương pháp đơn giản như bỏ một số thông tin cũng có vẻ có lợi. Hiệu suất của phương pháp lựa chọn nhân cho thấy một số cải thiện qua các độ dài đầu vào khác nhau. Tuy nhiên, khoảng cách hiệu suất đáng kể được quan sát với mô-đun thích ứng chọn lọc của chúng tôi chứng minh vai trò quan trọng của nó trong mô hình ReMamba. Mô-đun thích ứng chọn lọc không chỉ giảm thiểu vấn đề quên mà còn tăng cường đáng kể khả năng xử lý hiệu quả các chuỗi đầu vào dài hơn của mô hình.

### 5.4.2 Độ dài thay đổi

Để bổ sung cho kết quả chính của chúng tôi, sử dụng độ dài chuỗi tối đa 6k token để phù hợp với thiết lập đào tạo, chúng tôi tiếp tục đánh giá hiệu suất mô hình ở các độ dài đầu vào khác nhau từ 2k đến 9k token. Đánh giá này được tiến hành bằng cách sử dụng các benchmark LongBench và L-Eval. Như được mô tả trong Hình 4, ReMamba của chúng tôi liên tục vượt trội hơn mô hình Mamba baseline trên tất cả các độ dài ngữ cảnh được thử nghiệm trên LongBench. Đáng chú ý, khoảng cách hiệu suất giữa mô hình của chúng tôi và baseline mở rộng khi độ dài ngữ cảnh tăng. Hơn nữa, mô hình của chúng tôi mở rộng độ dài ngữ cảnh hiệu quả (độ dài mà tại đó hiệu suất tốt nhất được quan sát) đến 6k token, so với 4k token cho baseline Mamba được tinh chỉnh. Trong Hình 5, chúng tôi quan sát cải thiện hiệu suất trên tất cả các độ dài ngữ cảnh cho mô hình của chúng tôi trên L-Eval.

### 5.4.3 Hiệu suất tốc độ và chi phí bộ nhớ

Mô hình của chúng tôi giới thiệu một lần chuyển tiếp bổ sung duy nhất trong quá trình suy luận, dẫn đến mức tiêu thụ bộ nhớ không đổi nhỏ. Chúng tôi trực quan hóa mức tiêu thụ bộ nhớ trong quá trình suy luận với chuỗi đầu vào 6k và chuỗi đầu ra 1k, sử dụng kích thước batch là 1 trong Hình 6. Thiết bị chúng tôi sử dụng là GPU A100 80GB. Chúng tôi quan sát rằng quá trình mã hóa của Llama tiêu thụ một lượng bộ nhớ đáng kể và cache KV tăng dần trong quá trình giải mã, trong khi ReMamba chỉ phát sinh chi phí bộ nhớ bổ sung nhỏ. Sau quá trình mã hóa, mức tiêu thụ bộ nhớ của ReMamba ổn định ở mức không đổi, cao hơn một chút so với Mamba, tương ứng với các tham số bổ sung được giới thiệu để hỗ trợ cơ chế lựa chọn.

Để đánh giá hiệu suất tốc độ, chúng tôi thay đổi độ dài chuỗi đầu vào từ 1k đến 8k token trong khi cố định độ dài đầu ra ở 1k token. Đối với tất cả các thực nghiệm, chúng tôi sử dụng kích thước batch là 1 và đo tốc độ trên GPU NVIDIA A100 80GB. Chúng tôi so sánh hiệu suất của ReMamba, Mamba và mô hình transformer vanilla (Llama-3b), như được minh họa trong Hình 7. Thước đo tốc độ được đưa ra theo token trên giây. Các thực nghiệm của chúng tôi chỉ ra rằng ReMamba hoạt động ở tốc độ tương đương với baseline ban đầu, duy trì lợi thế tốc độ đáng kể so với các transformer truyền thống.

### 5.4.4 Tính bền vững với các lựa chọn siêu tham số khác nhau

Các kết quả nêu trên được thu thập bằng cách sử dụng thiết lập siêu tham số s = 0, p = 0.18 và rho = 0.009, điều này thể hiện hiệu suất tương đối vượt trội. Trong Hình 8, chúng tôi cũng cho thấy tính ổn định của mô hình bằng cách thay đổi các siêu tham số p và rho. Đối với các thực nghiệm này, siêu tham số s được cố định ở 0.

### 5.4.5 Tổng quát hóa sang Mamba2

Chúng tôi cũng đánh giá khả năng áp dụng của cách tiếp cận này cho Mamba2. Như được hiển thị trong Bảng 3, ReMamba đạt được cải thiện trung bình 1.6 điểm trên LongBench. Chi tiết thêm có sẵn trong Phụ lục A.2.

# 6 Kết luận

Nghiên cứu này điều tra các thách thức hiệu quả ngữ cảnh dài do các mô hình Mamba đưa ra, giả thuyết rằng thông tin xa trong các mô hình này chịu sự suy giảm đáng kể. Để đáp ứng, chúng tôi giới thiệu ReMamba, một cách tiếp cận mới nén và bảo tồn chọn lọc thông tin quan trọng trong lần chuyển tiếp ban đầu. Thông tin nén này sau đó được tích hợp vào không gian trạng thái trong lần chuyển tiếp thứ hai, tận dụng cơ chế chọn lọc vốn có của Mamba. Đáng chú ý, ReMamba phát sinh chi phí tính toán tối thiểu trong khi tăng cường đáng kể hiệu suất ngữ cảnh dài của Mamba, do đó cung cấp một hướng đi hứa hẹn để thúc đẩy họ mô hình Mamba.

# Hạn chế

Mặc dù ReMamba cải thiện hiệu suất ngữ cảnh dài của Mamba, không chắc Mamba có thể vượt trội hơn transformers khi độ dài ngữ cảnh tăng, do không gian trạng thái có kích thước cố định. Ngoài ra, ReMamba chủ yếu giảm thiểu mất thông tin ngữ cảnh dài trong Mamba thông qua nén chọn lọc. Một cách tiếp cận hứa hẹn hơn sẽ là sửa đổi trực tiếp cơ chế cập nhật không gian trạng thái.

# Tài liệu tham khảo

Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, và Xipeng Qiu. 2023. L-eval: Thiết lập đánh giá tiêu chuẩn hóa cho các mô hình ngôn ngữ ngữ cảnh dài. Preprint, arXiv:2307.11088.

Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, và Juanzi Li. 2024. Longbench: Một benchmark song ngữ, đa tác vụ cho hiểu biết ngữ cảnh dài. Preprint, arXiv:2308.14508.

Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, và Raja Giryes. 2024. Decimamba: Khám phá tiềm năng ngoại suy độ dài của mamba. Preprint, arXiv:2406.14528.

Shouyuan Chen, Sherman Wong, Liangjian Chen, và Yuandong Tian. 2023. Mở rộng cửa sổ ngữ cảnh của các mô hình ngôn ngữ lớn thông qua nội suy vị trí. Preprint, arXiv:2306.15595.

Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, và Jiaya Jia. 2024. Longlora: Tinh chỉnh hiệu quả các mô hình ngôn ngữ lớn ngữ cảnh dài. Trong Hội nghị Quốc tế thứ mười hai về Biểu diễn Học tập, ICLR 2024, Vienna, Áo, 7-11 tháng 5, 2024. OpenReview.net.

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, và Danqi Chen. 2023. Thích ứng các mô hình ngôn ngữ để nén ngữ cảnh. Trong Kỷ yếu Hội nghị 2023 về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, EMNLP 2023, Singapore, 6-10 tháng 12, 2023, trang 3829-3846. Hiệp hội Ngôn ngữ học Tính toán.

Tri Dao và Albert Gu. 2024. Transformers là ssms: Các mô hình tổng quát và thuật toán hiệu quả thông qua duality không gian trạng thái có cấu trúc. Preprint, arXiv:2405.21060.

Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, và Mao Yang. 2024. Longrope: Mở rộng cửa sổ ngữ cảnh LLM vượt quá 2 triệu token. Trong Hội nghị Quốc tế thứ bốn mốt về Học máy, ICML 2024, Vienna, Áo, 21-27 tháng 7, 2024. OpenReview.net.

Jun Gao, Ziqiang Cao, và Wenjie Li. 2024. Selfcp: Nén prompt vượt giới hạn thông qua chính mô hình ngôn ngữ lớn đông lạnh. Preprint, arXiv:2405.17052.

Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. 2023. Một khung cho đánh giá mô hình ngôn ngữ few-shot. https://zenodo.org/records/10256836. Truy cập: 29-5-2024.

Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, và Furu Wei. 2024. Autoencoder trong ngữ cảnh để nén ngữ cảnh trong mô hình ngôn ngữ lớn. Preprint, arXiv:2307.06945.

Xinyang Geng và Hao Liu. 2023. Openllama: Một bản tái tạo mở của llama. https://github.com/openlm-research/open_llama. Truy cập: 29-5-2024.

Albert Gu và Tri Dao. 2024. Mamba: Mô hình hóa chuỗi thời gian tuyến tính với không gian trạng thái chọn lọc. Preprint, arXiv:2312.00752.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. Lora: Thích ứng hạng thấp của các mô hình ngôn ngữ lớn. Trong Hội nghị Quốc tế thứ mười về Biểu diễn Học tập, ICLR 2022, Sự kiện Ảo, 25-29 tháng 4, 2022. OpenReview.net.

Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, và Pedro Moreno Mengibar. 2024. Transformerfam: Feedback attention là bộ nhớ làm việc. Preprint, arXiv:2404.09173.

Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, và Douwe Kiela. 2020. Tạo sinh tăng cường truy xuất cho các tác vụ NLP thâm dụng kiến thức. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Neural 33: Hội nghị Thường niên về Hệ thống Xử lý Thông tin Neural 2020, NeurIPS 2020, 6-12 tháng 12, 2020, ảo.

Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, và Yoav Shoham. 2024. Jamba: Một mô hình ngôn ngữ lai transformer-mamba. Preprint, arXiv:2403.19887.

Ziming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang, Rui Zhang, Tao Yu, Budhaditya Deb, Chenguang Zhu, Ahmed Awadallah, và Dragomir Radev. 2022. DYLE: Trích xuất tiềm ẩn động cho tóm tắt đầu vào dài trừu tượng. Trong Kỷ yếu Cuộc họp Thường niên lần thứ 60 của Hiệp hội Ngôn ngữ học Tính toán (Tập 1: Bài báo Dài), trang 1687-1698, Dublin, Ireland. Hiệp hội Ngôn ngữ học Tính toán.

Jesse Mu, Xiang Li, và Noah Goodman. 2023. Học nén prompt với gist token. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Neural, tập 36, trang 19327-19352. Curran Associates, Inc.

Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, và Ahmed Awadallah. 2023. Orca: Học tập tiến bộ từ các dấu vết giải thích phức tạp của gpt-4. Preprint, arXiv:2306.02707.

Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, và Dimitris Papailiopoulos. 2024. Mamba có thể học cách học không? một nghiên cứu so sánh về các tác vụ học trong ngữ cảnh. Preprint, arXiv:2402.04248.

Zhang Peiyuan. 2024. Longmamba. https://github.com/jzhang38/LongMamba. Truy cập: 10-8-2024.

Bowen Peng, Jeffrey Quesnelle, Honglu Fan, và Enrico Shippole. 2024. Yarn: Mở rộng cửa sổ ngữ cảnh hiệu quả của các mô hình ngôn ngữ lớn. Trong Hội nghị Quốc tế thứ mười hai về Biểu diễn Học tập, ICLR 2024, Vienna, Áo, 7-11 tháng 5, 2024. OpenReview.net.

David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, và Adam Santoro. 2024. Mixture-of-depths: Phân bổ động tính toán trong các mô hình ngôn ngữ dựa trên transformer. Preprint, arXiv:2404.02258.

Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, và Weizhu Chen. 2024. Samba: Các mô hình không gian trạng thái lai đơn giản cho mô hình hóa ngôn ngữ ngữ cảnh không giới hạn hiệu quả. Preprint, arXiv:2406.07522.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention là tất cả những gì bạn cần. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Neural, tập 30. Curran Associates, Inc.

Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, và Bryan Catanzaro. 2024. Một nghiên cứu thực nghiệm về các mô hình ngôn ngữ dựa trên mamba. Preprint, arXiv:2406.07887.

Kaiyue Wen, Xingyu Dang, và Kaifeng Lyu. 2024. RNN không phải là transformers (chưa): Nút thắt cổ chai chính trong truy xuất trong ngữ cảnh. arXiv preprint arXiv:2402.18510.

Dongjie Yang, XiaoDong Han, Yan Gao, Yao Hu, Shilin Zhang, và Hai Zhao. 2024a. Pyramidinfer: Nén cache KV kim tự tháp cho suy luận LLM thông lượng cao. Preprint, arXiv:2405.12532.

Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, và Liwei Wang. 2024b. Các transformer hiệu quả có thực sự tiết kiệm tính toán không? arXiv preprint arXiv:2402.13934.

# A Phụ lục

## A.1 Chi tiết đào tạo

Trong quá trình đào tạo, siêu tham số ReMamba s được cố định ở 0. Siêu tham số p được lấy mẫu ngẫu nhiên từ khoảng [0.1, 0.3], trong khi rho được lấy mẫu ngẫu nhiên từ khoảng [0.05, 0.2].

Hầu hết các thực nghiệm của DeciMamba được tiến hành trên Mamba130m. Cấu hình duy nhất cho DeciMamba2.8 được cung cấp trong bài báo gốc là decimating_layers = 22, decimation_max_p_L_base = 4000, được sử dụng cho các tác vụ mô hình hóa ngôn ngữ. Ở đây chúng tôi thay đổi decimation_max_p_L_base = 6000. Đối với tất cả các siêu tham số khác, chúng tôi sử dụng thiết lập mặc định: decimation_min_seq_len = 20, decimation_beta = 0.5, và decimation_type = "max_p". Các cấu hình nội suy vị trí tuyến tính cho Llama-3B là: max_position_embeddings = 6144 và factor = 3. Quá trình xây dựng dữ liệu cho Llama-3b giống hệt với Mamba. Chúng tôi tinh chỉnh tất cả các mô hình trong 1 epoch bằng DeepSpeed Zero Stage 3 trên 8 GPU A100-80GB. Chi tiết đào tạo khác có thể được tìm thấy trong Bảng 4.

## A.2 Chi tiết Mamba2

Các siêu tham số ở đây là: s = 0, p = 0.25 và rho = 0.05. Độ dài tối đa vẫn là 6k. Đáng chú ý là Mamba2 hầu như không thể hiện cải thiện hiệu suất nào so với Mamba trên LongBench, cho thấy những hạn chế tiềm ẩn trong chuỗi mô hình Mamba.

## A.3 Tại sao nén từ đầu

Kết quả thực nghiệm chỉ ra rằng việc đặt s = 0 là tốt nhất. Tuy nhiên, người ta có thể thắc mắc về hiệu quả của việc nén ở giữa chuỗi. Chúng tôi tiến hành các nghiên cứu phân tích bổ sung để khám phá tác động của việc nén chuỗi đầu vào từ các vị trí bắt đầu khác nhau.

Chúng tôi đào tạo một mô hình sử dụng s được lấy mẫu đều từ khoảng [0.1, 0.3] trong quá trình đào tạo. Sau đó, chúng tôi đánh giá hiệu suất của nó trên LongBench trong điều kiện giống hệt với mô hình ReMamba, sử dụng độ dài tối đa 6k token, p = 0.18 và rho = 0.009. Chúng tôi đánh giá điểm số trung bình với s từ 0 đến 0.4. Ngoài ra, chúng tôi đào tạo một biến thể mô hình đặc biệt nén toàn bộ prompt dựa trên rho = 0.009 và nối các trạng thái ẩn nén vào cuối prompt gốc trong giai đoạn thứ hai.

Bảng 5 trình bày kết quả của các thực nghiệm này. Chúng tôi quan sát sự suy giảm hiệu suất khi việc nén được áp dụng ở giữa chuỗi. Biến thể mô hình đặc biệt hoạt động thậm chí tệ hơn baseline Mamba được tinh chỉnh.

Sự suy giảm này có thể được giải thích bởi sự gián đoạn gây ra cho bản chất mô hình hóa ngôn ngữ nhân quả của mô hình Mamba. Khi thông tin nén được tích hợp vào vị trí ban đầu, quá trình mô hình hóa ngôn ngữ tiếp theo có thể tiến hành mà không cần sửa đổi, hiệu quả coi dữ liệu nén như một trạng thái ban đầu không phải zero chuyên biệt. Ngược lại, việc chèn các trạng thái ẩn nén đó như các token trong chuỗi làm gián đoạn paradigm mô hình hóa ngôn ngữ nhân quả, vốn giả định các câu hoàn chỉnh làm đầu vào. Sự không nhất quán này cản trở khả năng duy trì không gian trạng thái mạch lạc của mô hình và có thể dẫn đến suy giảm hiệu suất. Trong số các mô hình được thử nghiệm, biến thể mô hình đặc biệt nối các trạng thái ẩn nén vào cuối prompt gốc thể hiện tác động tiêu cực rõ ràng nhất do sự gián đoạn đáng kể cấu trúc đầu vào mong đợi của mô hình.

Bất chấp những thách thức này, mô hình nén ở giữa vẫn vượt trội hơn baseline Mamba được tinh chỉnh. Điều này chứng minh rằng phương pháp của chúng tôi thể hiện hiệu quả rõ ràng.
