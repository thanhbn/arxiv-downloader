# 2408.08066.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2408.08066.pdf
# File size: 551848 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Mamba Retriever: Utilizing Mamba for Effective and Efficient
Dense Retrieval
Hanqi Zhang
Gaoling School of Artificial
Intelligence
Renmin University of China
Beijing, China
zhanghanqi@ruc.edu.cnChong Chen
Huawei Cloud
Beijing, China
chenchong55@huawei.comLang Mei
Gaoling School of Artificial
Intelligence
Renmin University of China
Beijing, China
meilang2013@ruc.edu.cn
Qi Liu
Gaoling School of Artificial
Intelligence
Renmin University of China
Beijing, China
liuqi_67@ruc.edu.cnJiaxin Maoâˆ—
Gaoling School of Artificial
Intelligence
Renmin University of China
Beijing, China
maojiaxin@gmail.com
Abstract
In the information retrieval (IR) area, dense retrieval (DR) models
use deep learning techniques to encode queries and passages into
embedding space to compute their semantic relations. It is impor-
tant for DR models to balance both efficiency and effectiveness.
Pre-trained language models (PLMs), especially Transformer-based
PLMs, have been proven to be effective encoders of DR models.
However, the self-attention component in Transformer-based PLM
results in a computational complexity that grows quadratically
with sequence length, and thus exhibits a slow inference speed
for long-text retrieval. Some recently proposed non-Transformer
PLMs, especially the Mamba architecture PLMs, have demonstrated
not only comparable effectiveness to Transformer-based PLMs on
generative language tasks but also better efficiency due to linear
time scaling in sequence length. This paper implements the Mamba
Retriever to explore whether Mamba can serve as an effective and
efficient encoder of DR model for IR tasks. We fine-tune the Mamba
Retriever on the classic short-text MS MARCO passage ranking
dataset and the long-text LoCoV0 dataset. Experimental results
show that (1) on the MS MARCO passage ranking dataset and BEIR,
the Mamba Retriever achieves comparable or better effectiveness
compared to Transformer-based retrieval models, and the effective-
ness grows with the size of the Mamba model; (2) on the long-text
LoCoV0 dataset, the Mamba Retriever can extend to longer text
length than its pre-trained length after fine-tuning on retrieval task,
and it has comparable or better effectiveness compared to other
long-text retrieval models; (3) the Mamba Retriever has superior
âˆ—Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXXinference speed for long-text retrieval. In conclusion, Mamba Re-
triever is both effective and efficient, making it a practical model,
especially for long-text retrieval.
CCS Concepts
â€¢Information systems â†’Retrieval models and ranking .
Keywords
information retrieval, pretrained language models, state space model
ACM Reference Format:
Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao. 2024. Mamba
Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval. In
Proceedings of Make sure to enter the correct conference title from your rights
confirmation emai (Conference acronym â€™XX). ACM, New York, NY, USA,
5 pages. https://doi.org/XXXXXXX.XXXXXXX
1 Introduction
Information retrieval (IR) aims to retrieve information objects that
are relevant to usersâ€™ queries from a large-scale collection. Dense
retrieval (DR) models [ 10] are proposed to assess the relevance
between query and passage by encoding them into embeddings and
calculating the similarity in the embedding space. Then, using Ap-
proximate Nearest Neighbor Search algorithms [ 24] on embeddings,
we can retrieve the top-k relevant passages to the query.
DR model needs to balance both effectiveness and efficiency.
Regarding effectiveness, DR models focus on improving retrieval
performance, which is influenced by factors including modelâ€™s in-
herent ability in semantic understanding and summarization. Re-
garding efficiency, this paper focuses on the passage inference time
instead of query, which has greater potential for improvement.
Pre-trained Language Models (PLMs) have demonstrated their
effectiveness on downstream tasks, thanks to the sufficient world
knowledge and semantic knowledge they gain during pre-training.
Especially, the Transformer-based PLMs [ 11,26,27] represent the
advantage of capturing long-range dependencies by the self-attention
mechanism and allowing parallel training. Many studies [ 10,15,
17,19] have proposed to adopt Transformer-based PLMs as the
encoders of DR models and have observed their effectiveness.arXiv:2408.08066v2  [cs.IR]  22 Aug 2024

--- PAGE 2 ---
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao
However, despite the effectiveness of Transformer-based DR
models, the efficiency is limited by their inherent model architec-
tures. In detail, the self-attention component in Transformer-based
PLMs results in a computational complexity that grows quadrat-
ically with sequence length. Thus, for retrieval tasks with long
passages, such as legal case retrieval task [ 13], Transformer-based
DR models exhibit slow inference speeds.
Some non-Transformer PLMs [ 6,7,20] are proposed to improve
efficiency without sacrificing effectiveness. In particular, Mamba-
based PLMs present comparable performance to Transformer-based
PLMs on generative language tasks and achieve linear time scaling
in sequence length based on a selective state space model mecha-
nism. Recently, Xu proposed the RankMamba model [ 28], which
leverages Mamba for re-ranking tasks. Our work differs in two key
aspects: (1) The proposed Mamba Retriever is a bi-encoder model
designed for effectiveness and efficiency in first-stage retrieval tasks.
(2) We extend the investigation of Mamba-based models to long-text
retrieval tasks.
In this paper, we propose to explore whether Mamba can serve
as an effective and efficient encoder of DR model for IR tasks. We
answer the following research questions:
RQ1: Does Mamba retrieval model have comparable effectiveness
on classic retrieval compared to Transformer retrieval models?
RQ2: Does Mamba retrieval model have comparable effectiveness
on long-text retrieval compared to existing long-text retrieval models?
RQ3: How does the inference efficiency of the Mamba retrieval
model compare to existing retrieval models across various text lengths?
To address these research questions, we implement the Mamba
Retriever, a bi-encoder retrieval model based on Mamba. We fine-
tune it on MS MARCO passage ranking dataset [ 18] for classic short-
text retrieval and on LoCoV0 dataset [23] for long-text retrieval.
We make the following contributions: (1) We implement the
Mamba Retriever in order to achieve both effectiveness and effi-
ciency. (2) We explore the retrieval effectiveness of Mamba Retriever
at different model sizes. We show that, on the MS MARCO passage
ranking dataset and BEIR [ 25] datasets, Mamba Retriever has com-
parable or better effectiveness compared to Transformer retrievers,
and the effectiveness also grows with the size of the Mamba model.
(3) Besides, we focus on the effectiveness of long-text retrieval.
We show that, on the long-text LoCoV0 dataset, Mamba Retriever
can extend to longer text length than its pre-trained length after
fine-tuning on retrieval task, and it has comparable or better ef-
fectiveness compared to other long-text retrieval models. (4) We
explore the passage inference efficiency of Mamba Retriever at dif-
ferent passage lengths. We show that Mamba Retriever has superior
inference speed with linear time scaling for long-text retrieval.
In conclusion, Mamba Retriever is both effective and efficient,
making it practical for IR, especially for long-text IR tasks. More de-
tails are available at https://github.com/41924076/MambaRetriever.
2 Related Work
Pre-trained Language Models (PLMs). Through pre-training,
language models can achieve higher performance when transferred
to specific tasks. Transformer [ 27], based on self-attention mech-
anism, is a mainstream architecture of PLMs, including encoder-
only[ 11,14] model and decoder-only[ 2,29] model. To address thequadratic time scaling of Transformer architecture, some architec-
tures broadly regarded as state space models have been proposed[ 8,
20], especially high-performing models like sub-quadratic architec-
ture M2-BERT[6], linear architecture Mamba[7] and Mamba-2[5].
Dense Retrieval Models. Transformer PLMs have been proven
effective for dense retrieval. Initially, encoder-only models are
adopted for retrieval tasks due to the bi-directional attention mecha-
nisms [ 10,19]. Later, decoder-only models are adopted for retrieval
tasks due to their effectiveness on larger model size [15, 17].
Long-text Dense Retrieval Models. In long-text retrieval,
early works use chunking strategies [ 4] due to the small context
window. In order to help the model better understand complete
and coherent semantics, some studies explore Transformer-based
long-text retrieval models[9, 15, 30].
Faced with the quadratic time scaling of Transformer-based long-
text retrieval models, the sub-quadratic M2-BERT model[ 6] has
been utilized for long-text retrieval tasks[23].
3 Methodology
3.1 Mamba Retriever
Task Definition. In text retrieval tasks, given a query ğ‘and a
large-scale passage set {ğ‘1,ğ‘2,...,ğ‘ğ‘›}, the retrieval model aims to
find top-ğ‘˜passages that are most relevant to ğ‘.
Overview of Mamba Retriever. To calculate the relevance
between a query ğ‘and a passage ğ‘, Mamba Retriever uses a bi-
encoder architecture. Bi-encoder means that the model represents
ğ‘andğ‘as dense vector embeddings ğ¸ğ‘andğ¸ğ‘respectively, and
relevance between the query ğ‘and passage ğ‘can be computed by
the cosine similarity between their dense representations.
ğ‘ ğ‘–ğ‘š(ğ‘,ğ‘)=ğ¸ğ‘Â·ğ¸ğ‘
âˆ¥ğ¸ğ‘âˆ¥âˆ¥ğ¸ğ‘âˆ¥(1)
Specifically, to generate embedding ğ¸, we use auto-regressive
language model Mamba as base model ğ‘€. We input a token se-
quenceğ‘¡1,ğ‘¡2,...,ğ‘¡ğ¿which has a sequence length of ğ¿and a token
<EOS> at the end of the sequence to the model ğ‘€, and extract the
output of <EOS> at the last hidden layer in ğ‘€as embedding ğ¸:
ğ¸=ğ‘€(ğ‘¡1,ğ‘¡2,...,ğ‘¡ğ¿,<EOS>)[âˆ’1] (2)
Mamba and SSM. The base model Mamba, as we previously
denoted asğ‘€, is a model constructed by stacking multiple Mamba
blocks. The core component of the Mamba block is selective state
space model which is based on state space model (SSM)[8].
A SSM maps 1-dimensional input sequence ğ‘¥(ğ‘¡)âˆˆRwith time
stepğ‘¡to output sequence ğ‘¦(ğ‘¡)âˆˆRthrough latent state â„(ğ‘¡)âˆˆRğ‘:
â„â€²(ğ‘¡)=ğ´â„(ğ‘¡)+ğµğ‘¥(ğ‘¡)ğ‘¦(ğ‘¡)=ğ¶â„(ğ‘¡) (3)
whereğ´âˆˆRğ‘Ã—ğ‘,ğµâˆˆRğ‘Ã—1,ğ¶âˆˆR1Ã—ğ‘. Using step size Î”, the
continuous form above can be changed to a discrete form:
â„ğ‘¡=ğ´â„ğ‘¡âˆ’1+ğµğ‘¥ğ‘¡ ğ‘¦ğ‘¡=ğ¶â„ğ‘¡ (4)
whereğ´=exp(Î”ğ´)andğµ=(Î”ğ´)âˆ’1(exp(Î”ğ´)âˆ’ğ¼)Â·Î”ğµis one of
the discretization methods. The above 1-dimensional SSM can be
extended to independent ğ‘‘dimensions.
Based on the SSM, Mamba introduces a selection mechanism and
corresponding hardware-aware parallel algorithm. The selection
mechanism is making Î”,ğµ,ğ¶ dependent on the current token. It

--- PAGE 3 ---
Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
allows the model to selectively forget or remember information
along the dimension of sequence length.
Training Objective. To train the Mamba Retriever, we employ
the InfoNCE loss, which is most commonly used. It forces ğ‘andğ‘‘
that are semantically similar to be closer in embedding space:
L=âˆ’logğ‘’sim(ğ‘,ğ‘‘+)/ğœ
ğ‘’sim(ğ‘,ğ‘‘+)/ğœ+Ã
ğ‘‘âˆ’
ğ‘–âˆˆğ·âˆ’ğ‘’sim(ğ‘,ğ‘‘âˆ’
ğ‘–)/ğœ(5)
whereğ‘‘+is the relevant passage of the query, ğ·âˆ’is a set of irrele-
vant passages of the query, and ğœis the temperature coefficient.
3.2 Base Model Comparison
In Section 3.1, we let Mamba be the base model ğ‘€of retriever. As
comparison, the base model ğ‘€is changed from Mamba to other
frequently-used base models, including the Transformer encoder-
only models and decoder-only models. In this section, we analyze
the differences between using Mamba and these models as ğ‘€.
Mamba and Decoder-only vs Encoder-only. Mamba and
Transformer decoder-only models share similarities that distinguish
them from Transformer encoder-only models. In terms of data,
Mamba and decoder-only models are pre-trained on more data than
most encoder-only models. In particular, Mamba and Pythia [ 2] are
pre-trained on the same data. In terms of architecture, Mamba and
decoder-only models have causal characteristics, which is not as
suitable as encoder-only models with bi-directional attention for
comprehension tasks like retrieval. Mamba can be reconstructed to
be bi-directional, but this would lead to a decrease in efficiency.
Mamba vs Decoder-only. Intuitively, Transformer decoder-
only model can capture long-term dependencies by self-attention
mechanism, while Mamba may be limited by the maximum amount
of information that can be compressed in latent states.
However, some works[ 1,3,5,16] analyze that Mamba has some
mechanism similar to or even surpassing Transformer: Mamba has
implicit attention mechanism with good expressiveness; if each SSM
is regarded as one head in multi-head self-attention mechanism,
then Mamba has more heads than the Transformer; the softmax in
self-attention can cause problems, such as over-smoothing, whereas
Mamba does not use softmax and thus may better capture subtle
differences between different tokens.
In addition, Mamba has an additional explicit process of sum-
marizing previous information using the latent states. When cal-
culating a token at position ğ‘¡, decoder-only model uses the atten-
tion mechanism to access keys and values of all previous tokens.
However, Mamba uses the SSM mechanism to incorporate the in-
formation of all previous tokens in a fixed-size latent state â„ğ‘¡âˆ’1
which serves as a summerization, and then calculates the output of
positionğ‘¡using theâ„ğ‘¡âˆ’1according to formula 4. We can have an
assumption that Mamba may acquire good summarizing abilities
during pre-training, which is beneficial for retrieval where the goal
is to summarize query or passage information into an embedding.
4 Experiments
4.1 Experiments on MS MARCO
Dataset. We fine-tune and evaluate on MS MARCO passage ranking
dataset [ 18], a classic short-text retrieval dataset with the value ofTable 1: The effectiveness of Mamba Retriever on MS MARCO
and BEIR compared to Transformer Retrievers. Arch denotes
architecture, R@1k is Recall@1k, En is Encoder-only, De is
Decoder-only, and Ma is Mamba.
Base Model Size Arch. MS MARCO BEIR
MRR@10 R@1k nDCG@10
BERT-base 110M En. 32.8 95.2 36.45
RoBERTa-base 125M En. 31.3 95.0 37.02
OPT 125M De. 31.1 94.9 36.83
Pythia 160M De. 25.4 91.0 31.47
Mamba 130M Ma. 32.3 96.7 40.54
BERT-large 330M En. 33.9 95.7 37.31
RoBERTa-large 355M En. 33.9 96.1 38.37
OPT 350M De. 31.1 94.4 35.89
Pythia 410M De. 31.9 96.6 38.62
Mamba 370M Ma. 35.2 97.7 43.52
Pythia 1B De. 33.8 97.4 43.11
Mamba 790M Ma. 36.3 98.3 44.72
OPT 1.3B De. 35.8 98.1 42.87
average passage char length less than 400. MRR@10 and Recall@1k
are common metrics.
Additionally, we conduct zero-shot evaluation on 13 BEIR datasets
[25] including ArguAna, Climate-FEVER, DBPedia, FEVER, FiQA,
HotpotQA, NFCorpus, NQ, Quora, SCIDOCS, SciFact, TREC-COVID,
TÃ³uche-2020. Average nDCG@10 is a common metric.
Implementation Details. We initialize our model from a pre-
trained Mamba checkpoint and train it on 4 Ã—64G V100 GPUs.
Since efficiency is important for retrieval tasks, we conduct experi-
ments on the model sizes of 130M, 370M, and 790M. We add a new
special token <EOS> to the end of the text as a dense representation.
We normalize the dense representation with L2 normalization, and
fix the temperature coefficient at 0.01. Because the purpose of the
experiment is not to use various tricks to achieve optimal perfor-
mance, we use a batch size of 2 on each GPU. By sharing negative
passages between GPUs and batches, each query totally has 63 neg-
ative passages mined by BM25. We use a well-performing learning
rate 1e-5 and train until convergence. We train and evaluate on MS
MARCO with query maximum length of {16, 32} respectively and
passage maximum length of 128. We conduct zero-shot evaluations
on BEIR with both query and passage maximum length of 64.
For baseline, the base model ğ‘€mentioned in Section 3.1 is
changed from Mamba to Transformer models, including encoder-
only model BERT [ 11] and RoBERTa [ 14], and decoder-only model
OPT [ 29] and Pythia [ 2]. The training setup is basically the same as
Mamba, with two key differences: the learning rate is set to the most
effective value for each model respectively; BERT and RoBERTa use
the last hidden state of prefix <CLS> token as the text embedding.
Results. Table 1 shows the effectiveness on MS MARCO passage
ranking and BEIR datasets. Briefly, Mamba shows comparable or
better performance to Transformer and improves with model size.
When compared with Transformer decoder-only models, Mamba
shows better performance on all model sizes, especially Pythia
which is pre-trained on the same data. Transformer encoder-only

--- PAGE 4 ---
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, and Jiaxin Mao
Table 2: The effectiveness of Mamba Retriever on the LoCoV0
dataset compared to long-text retrievers. M2 is M2-BERT.
Base Model Max. Len. Arch. Avg. nDCG@10
M2-BERT-2k, 80M 2k M2. 83.6
OPT-125M 2k De. 88.9
Pythia-160M 2k De. 79.2
Mamba-130M 2k Ma. 89.1
Jina-v2, 137M, zero-shot 8k En. 85.4
M2-BERT-8k, 80M 8k M2. 85.9
Mamba-130M 8k Ma. 90.7
models have higher scores than decoder-only, which proves that bi-
directional self-attention is beneficial to retrieval. When compared
to the bi-directional Transformer encoder-only model, although
Mamba is uni-directional, Mamba still has similar or better perfor-
mance. This suggests that Mamba has stronger text comprehension
and summarization ability, possibly due to its advantages discussed
in Section 3.2, such as Mambaâ€™s implicit attention mechanism and
explicit summarization ability. Additionally, Mambaâ€™s retrieval per-
formance grows with the model size expanding from 100M to 790M.
4.2 Experiments on LoCoV0
Dataset. Our models are trained and evaluated on LoCoV0 dataset.
The LoCoV0 has 5 long-text retrieval datasets: SummScreenFD, Gov
Report, QMSUM, QASPER Title to Full Text, and QASPER Abstract
to Full Text. The average char lengths of passages are {30792, 55280,
58129, 22315, 22315}. Average nDCG@10 is a common metric. The
recently released LoCoV1 consists of more datasets, but was not
released at the time of conducting our experiments.
Implementation Details. We initialize our model from a pre-
trained Mamba checkpoint and train on 8 Ã—64G V100 GPUs. We use
a maximum length of 2k or 8k tokens. Training and evaluation are
on the same maximum length. We use a batch size of 1 on each GPU.
By sharing negative passages between GPUs, each query totally has
31 random negative passages. We train no more than 4 epochs and
use well-performing learning rate for each model. Other training
settings are the same as Section 4.1.
We use Transformer encoder-only, decoder-only and M2-BERT
[23] model as baselines. For encoder-only, we use zero-shot result
of Jina Embeddings v2 model [ 9] which is fine-tuned not on LoCoV0
but on a greater amount of data. Because the checkpoint of pre-
trained Jina BERT is not published, we can not fine-tune it on
LoCoV0. For decoder-only, we use OPT [ 29] and Pythia [ 2]. For
M2-BERT, we use the result in paper [ 23] which fine-tune for 1
epoch on LoCoV0 with orthogonal projection loss [ 22] rather than
InfoNCE loss.
Results. Table 2 shows the effectiveness on LoCoV0. Briefly,
Mamba shows comparable or better performance than other long-
text retrievers, and can extend to longer text than pre-training.
On 2k maximum length, although Mambaâ€™s memory capacity
for long text is limited by latent state size due to the lack of self-
attention mechanism, it still has comparable or better capability to
Transformer decoder-only models. In addition, although Mamba is
pre-trained on 2k maximum length and M2BERT-8k is pre-trained
5122k 8k32k Max Length (tokens)0.00.51.01.52.02.5Inference Time (h)
T otal Inference Time of LoCoV0 Train Set Passages (A100 40G PCIe)
Jina Embeddings v2
pythia
m2bert
mambaFigure 1: The efficiency of Mamba Retriever compared to
long-text retrieval models at different maximum text lengths.
on 8k maximum length, Mamba fine-tuned on 8k maximum length
has comparable or better performance than other models.
4.3 Inference Efficiency
Implementation Details. We choose to measure long passage
inference time which has greater potential for improvement rather
than short query. We measure the inference time of all LoCoV0
train set passages [ 23] on one A100 40G GPU using bf16, excluding
tokenization time. Passages are truncated to certain lengths. We
use the best batch sizes on throughput for each model and each
maximum length. M2-BERT is not tested on length 512. Besides,
generative language tasks involve multiple iterative steps, while
retrieval tasks involve a single step, so there is no need to accelerate
the iterative process [12, 21] in retrieval task.
Results. Figure 1 presents the inference time of long-text re-
trieval models. Briefly, Mamba shows faster speed on long-text
retrieval. At maximum length of 512, the inference time of various
models are similar. At 2k maximum length, Mamba and M2-BERT
have similar time, while Transformer-based models require 4 Ã—time.
At 8k and 32k lengths, the M2-BERT need approximately 1.2 Ã—and
1.4Ã—inference time than Mamba, respectively.
5 Conclusion
In this paper, we investigate the effectiveness and efficiency of the
Mamba-based model in retrieval task. The experiment results show
that, on classic short-text retrieval, Mamba Retriever has similar
or better effectiveness compared to Transformer retrievers, and
the effectiveness increases with model size. On long-text retrieval,
Mamba Retriever has similar or better effectiveness compared to
existing long-text retrievers, and can extend to handle longer text
lengths beyond pre-training. In addition, Mamba Retriever shows an
efficiency advantage on long-text retrieval due to its linear scaling
in sequence length.
Acknowledgments
This research was supported by the Natural Science Foundation
of China (61902209, 62377044, U2001212), and Beijing Outstanding
Young Scientist Program (NO. BJJWZYJH012019100020098), Intel-
ligent Social Governance Platform, Major Innovation & Planning
Interdisciplinary Platform for the "Double-First Class" Initiative,
Renmin University of China.

--- PAGE 5 ---
Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
References
[1]Ameen Ali, Itamar Zimerman, and Lior Wolf. 2024. The hidden attention of
mamba models. arXiv preprint arXiv:2403.01590 (2024).
[2]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,
Kyle Oâ€™Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, et al .2023. Pythia: A suite for analyzing
large language models across training and scaling. In International Conference on
Machine Learning . PMLR, 2397â€“2430.
[3]Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and
Terry Lyons. 2024. Theoretical Foundations of Deep Selective State-Space Models.
arXiv preprint arXiv:2402.19047 (2024).
[4]Zhuyun Dai and Jamie Callan. 2019. Deeper text understanding for IR with
contextual neural language modeling. In Proceedings of the 42nd international
ACM SIGIR conference on research and development in information retrieval . 985â€“
988.
[5]Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized
Models and Efficient Algorithms Through Structured State Space Duality.
arXiv:2405.21060 [cs.LG]
[6]Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin
Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher RÃ©. 2024.
Monarch mixer: A simple sub-quadratic gemm-based architecture. Advances in
Neural Information Processing Systems 36 (2024).
[7]Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with
selective state spaces. arXiv preprint arXiv:2312.00752 (2023).
[8]Albert Gu, Karan Goel, and Christopher RÃ©. 2021. Efficiently modeling long
sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021).
[9]Michael GÃ¼nther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy
Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba
Sturua, Bo Wang, et al .2023. Jina embeddings 2: 8192-token general-purpose
text embeddings for long documents. arXiv preprint arXiv:2310.19923 (2023).
[10] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) . Association for Computational
Linguistics.
[11] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of NAACL-HLT . 4171â€“4186.
[12] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Mem-
ory Management for Large Language Model Serving with PagedAttention. In
Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles .
[13] Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. 2023.
LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset. arXiv preprint
arXiv:2310.17609 (2023).
[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[15] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023. Fine-
tuning llama for multi-stage text retrieval. arXiv preprint arXiv:2310.08319 (2023).[16] William Merrill, Jackson Petty, and Ashish Sabharwal. 2024. The illusion of state
in state-space models. arXiv preprint arXiv:2404.08819 (2024).
[17] Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search.
arXiv preprint arXiv:2202.08904 (2022).
[18] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. In CoCo@NIPS (CEUR Workshop Proceedings,
Vol. 1773) . CEUR-WS.org.
[19] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo HernÃ¡ndez Ãbrego, Ji Ma,
Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2022.
Large Dual Encoders Are Generalizable Retrievers. In EMNLP . Association for
Computational Linguistics, 9844â€“9855.
[20] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella
Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, et al .2023.
RWKV: Reinventing RNNs for the Transformer Era. In Findings of the Association
for Computational Linguistics: EMNLP 2023 . 14048â€“14077.
[21] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-
bury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently
scaling transformer inference. Proceedings of Machine Learning and Systems 5
(2023).
[22] Kanchana Ranasinghe, Muzammal Naseer, Munawar Hayat, Salman Khan, and
Fahad Shahbaz Khan. 2021. Orthogonal projection loss. In Proceedings of the
IEEE/CVF international conference on computer vision . 12333â€“12343.
[23] Jon Saad-Falcon, Daniel Y Fu, Simran Arora, Neel Guha, and Christopher RÃ©.
2024. Benchmarking and Building Long-Context Retrieval Models with LoCo
and M2-BERT. arXiv preprint arXiv:2402.07440 (2024).
[24] Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for sublinear
time maximum inner product search (MIPS). Advances in neural information
processing systems 27 (2014).
[25] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna
Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of
Information Retrieval Models. In Thirty-fifth Conference on Neural Information
Processing Systems Datasets and Benchmarks Track (Round 2) . https://openreview.
net/forum?id=wCu6T5xFjeJ
[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288 (2023).
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[28] Zhichao Xu. 2024. RankMamba, Benchmarking Mambaâ€™s Document Ranking
Performance in the Era of Transformers. arXiv preprint arXiv:2403.18276 (2024).
[29] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al .2022. Opt:
Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068
(2022).
[30] Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and
Sujian Li. 2024. LongEmbed: Extending Embedding Models for Long Context
Retrieval. arXiv preprint arXiv:2404.12096 (2024).
