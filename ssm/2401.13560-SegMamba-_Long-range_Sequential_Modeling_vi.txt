SegMamba: Mô hình Mamba tuần tự tầm xa
cho Phân đoạn Hình ảnh Y tế 3D

Zhaohu Xing1, Tian Ye1, Yijun Yang1, Guang Liu2, và Lei Zhu1,3( )
1Đại học Khoa học và Công nghệ Hồng Kông (Quảng Châu)
2Viện Hàn lâm Trí tuệ Nhân tạo Bắc Kinh
3Đại học Khoa học và Công nghệ Hồng Kông
leizhu@ust.hk

Tóm tắt. Kiến trúc Transformer đã chứng minh kết quả đáng chú ý trong phân đoạn hình ảnh y tế 3D nhờ khả năng mô hình hóa các mối quan hệ toàn cục. Tuy nhiên, nó tạo ra gánh nặng tính toán đáng kể khi xử lý hình ảnh y tế có chiều cao. Mamba, như một Mô hình Không gian Trạng thái (SSM), gần đây đã nổi lên như một phương pháp đáng chú ý để mô hình hóa các phụ thuộc tầm xa trong dữ liệu tuần tự, và đã xuất sắc trong lĩnh vực xử lý ngôn ngữ tự nhiên với hiệu quả bộ nhớ và tốc độ tính toán đáng kể. Lấy cảm hứng từ điều này, chúng tôi thiết kế SegMamba, một mô hình Mamba Phân đoạn hình ảnh y tế 3D mới, để nắm bắt hiệu quả các phụ thuộc tầm xa trong các đặc trưng toàn thể tại mọi quy mô. SegMamba của chúng tôi vượt trội so với các phương pháp dựa trên Transformer trong mô hình hóa đặc trưng toàn thể, duy trì hiệu quả cao ngay cả ở độ phân giải 64×64×64, nơi độ dài tuần tự xấp xỉ 260k. Hơn nữa, chúng tôi thu thập và chú thích một bộ dữ liệu quy mô lớn mới (có tên CRC-500) để tạo điều kiện đánh giá chuẩn trong phân đoạn ung thư đại trực tràng (CRC) 3D. Kết quả thực nghiệm trên CRC-500 và hai bộ dữ liệu chuẩn công khai chứng minh thêm tính hiệu quả và tính phổ quát của phương pháp chúng tôi. Mã nguồn cho SegMamba được công khai tại: https://github.com/ge-xing/SegMamba.

Từ khóa: Mô hình không gian trạng thái · Mamba · Mô hình tuần tự tầm xa · Phân đoạn hình ảnh y tế 3D.

1 Giới thiệu

Phân đoạn hình ảnh y tế 3D đóng vai trò quan trọng trong chẩn đoán hỗ trợ máy tính. Kết quả phân đoạn chính xác có thể giảm bớt gánh nặng chẩn đoán của bác sĩ cho các bệnh khác nhau. Để cải thiện hiệu suất phân đoạn, việc mở rộng trường tiếp nhận của mô hình trong không gian 3D là một khía cạnh quan trọng. Lớp tích chập kernel lớn [15] được đề xuất để mô hình hóa một phạm vi đặc trưng rộng hơn. 3D UX-Net [11] giới thiệu một kiến trúc mới sử dụng lớp tích chập với kích thước kernel lớn (7×7×7) làm khối cơ bản để tạo điều kiện cho các trường tiếp nhận lớn hơn. Tuy nhiên, các phương pháp dựa trên CNN gặp khó khăn trong việc mô hình hóa các mối quan hệ toàn cục do tính cục bộ vốn có của lớp tích chập.

Gần đây, kiến trúc Transformer [21,24,26,25,22], sử dụng mô-đun tự chú ý để trích xuất thông tin toàn cục, đã được khám phá rộng rãi cho phân đoạn hình ảnh y tế 3D. Ví dụ, UNETR [6] sử dụng Vision Transformer (ViT) [2] làm bộ mã hóa để học thông tin toàn cục trong một chuỗi đơn quy mô. SwinUNETR [5] tận dụng Swin Transformer [14] làm bộ mã hóa để trích xuất các đặc trưng đa quy mô. Trong khi các phương pháp dựa trên transformer này cải thiện hiệu suất phân đoạn, chúng tạo ra chi phí tính toán đáng kể do độ phức tạp bậc hai trong tự chú ý.

Để vượt qua các thách thức của mô hình hóa chuỗi dài, Mamba [4,13], có nguồn gốc từ các mô hình không gian trạng thái (SSMs) [9], được thiết kế để mô hình hóa các phụ thuộc tầm xa và tăng cường hiệu quả đào tạo và suy luận thông qua cơ chế lựa chọn và thuật toán nhận biết phần cứng. U-Mamba [16] tích hợp lớp Mamba vào bộ mã hóa của nnUNet [8] để tăng cường phân đoạn hình ảnh y tế tổng quát. Trong khi đó, Vision Mamba [28] giới thiệu khối Vim, kết hợp SSM hai chiều để mô hình hóa bối cảnh thị giác toàn cục. Tuy nhiên, Mamba chưa được khám phá đầy đủ trong phân đoạn hình ảnh y tế 3D.

Trong bài báo này, chúng tôi giới thiệu SegMamba, một khung mới kết hợp cấu trúc hình U với Mamba để mô hình hóa các đặc trưng toàn cục toàn thể ở các quy mô khác nhau. Theo hiểu biết của chúng tôi, đây là phương pháp đầu tiên sử dụng Mamba cụ thể cho phân đoạn hình ảnh y tế 3D. Để tăng cường mô hình hóa tuần tự toàn thể của các đặc trưng 3D, chúng tôi thiết kế mô-đun Mamba ba hướng (ToM). Tiếp theo, chúng tôi thiết kế thêm mô-đun tích chập không gian có cổng (GSC) để tăng cường biểu diễn đặc trưng không gian trước mỗi mô-đun ToM. Hơn nữa, chúng tôi thiết kế mô-đun ước lượng không chắc chắn mức đặc trưng (FUE) để lọc các đặc trưng đa quy mô từ bộ mã hóa, cho phép tái sử dụng đặc trưng được cải thiện. Cuối cùng, chúng tôi đề xuất một bộ dữ liệu quy mô lớn mới cho phân đoạn ung thư đại trực tràng 3D có tên CRC-500, bao gồm 500 quét computed tomography (CT) 3D với chú thích chuyên gia. Các thực nghiệm mở rộng được tiến hành trên ba bộ dữ liệu, chứng minh tính hiệu quả và tính phổ quát của phương pháp chúng tôi. SegMamba thể hiện khả năng đáng chú ý trong việc mô hình hóa các phụ thuộc tầm xa trong dữ liệu thể tích, đồng thời duy trì hiệu quả suy luận xuất sắc.

2 Phương pháp

SegMamba chủ yếu bao gồm ba thành phần: 1) bộ mã hóa đặc trưng 3D với nhiều khối Mamba không gian ba hướng (TSMamba) để mô hình hóa thông tin toàn cục ở các quy mô khác nhau, 2) bộ giải mã 3D dựa trên lớp tích chập để dự đoán kết quả phân đoạn, và 3) các kết nối bỏ qua với ước lượng không chắc chắn mức đặc trưng (FUE) để tăng cường đặc trưng. Hình 2 minh họa tổng quan về SegMamba được đề xuất. Chúng tôi mô tả chi tiết hơn về bộ mã hóa và bộ giải mã trong phần này.

2.1 Khối Mamba Không gian Ba hướng (TSMamba)

Mô hình hóa các đặc trưng toàn cục và đặc trưng đa quy mô là vô cùng quan trọng đối với phân đoạn hình ảnh y tế 3D. Kiến trúc Transformer có thể trích xuất thông tin toàn cục, nhưng nó gây ra gánh nặng tính toán đáng kể khi xử lý các chuỗi đặc trưng quá dài. Để giảm độ dài chuỗi, các phương pháp dựa trên kiến trúc Transformer, chẳng hạn như UNETR, trực tiếp giảm mẫu đầu vào 3D với độ phân giải D×H×W xuống D/16×H/16×W/16. Tuy nhiên, cách tiếp cận này hạn chế khả năng mã hóa các đặc trưng đa quy mô, điều quan trọng để dự đoán kết quả phân đoạn thông qua bộ giải mã. Để vượt qua hạn chế này, chúng tôi thiết kế khối TSMamba để cho phép cả mô hình hóa đặc trưng đa quy mô và toàn cục đồng thời duy trì hiệu quả cao trong quá trình đào tạo và suy luận.

Như minh họa trong Hình 2, bộ mã hóa bao gồm một lớp stem và nhiều khối TSMamba. Đối với lớp stem, chúng tôi sử dụng tích chập theo chiều sâu với kích thước kernel lớn 7×7×7, với padding 3×3×3, và stride 2×2×2. Cho thể tích đầu vào 3D I trong RC×D×H×W, trong đó C biểu thị số kênh đầu vào, đặc trưng quy mô đầu tiên z0 trong R48×D/2×H/2×W/2 được trích xuất bởi lớp stem. Sau đó, z0 được đưa qua từng khối TSMamba và các lớp giảm mẫu tương ứng. Đối với khối TSMamba thứ m, quá trình tính toán có thể được định nghĩa là:

ẑl_m = GSC(zl_m), z̃l_m = ToM(LN(ẑl_m)) + ẑl_m, zl+1_m = MLP(LN(z̃l_m)) + z̃l_m,     (1)

trong đó GSC và ToM biểu thị mô-đun tích chập không gian có cổng và mô-đun Mamba ba hướng được đề xuất, tương ứng, sẽ được thảo luận tiếp theo. l ∈ {0,1,...,Nm-1}, LN biểu thị chuẩn hóa lớp, và MLP đại diện cho lớp nhận thức đa lớp để làm phong phú biểu diễn đặc trưng.

Tích chập Không gian Có cổng (GSC) Lớp Mamba mô hình hóa các phụ thuộc đặc trưng bằng cách làm phẳng các đặc trưng 3D thành một chuỗi 1D, điều này thiếu thông tin không gian. Do đó, để nắm bắt các mối quan hệ không gian trước lớp Mamba, chúng tôi đã thiết kế mô-đun tích chập không gian có cổng (GSC). Như thể hiện trong Hình 2 (a), các đặc trưng 3D đầu vào được đưa vào hai khối tích chập (mỗi khối tích chập chứa một norm, một tích chập, và một lớp phi tuyến), với kích thước kernel tích chập là 3×3×3 và 1×1×1. Sau đó, hai đặc trưng này được nhân theo từng pixel để kiểm soát truyền thông tin tương tự như cơ chế cổng [12]. Cuối cùng, một khối tích chập được sử dụng để hợp nhất thêm các đặc trưng, trong khi một kết nối dư được sử dụng để tái sử dụng các đặc trưng đầu vào.

GSC(z) = z + C3×3×3(C3×3×3(z) · C1×1×1(z)),     (2)

trong đó z biểu thị các đặc trưng 3D đầu vào và C biểu thị khối tích chập.

Mamba Ba hướng (ToM) Khối Mamba gốc mô hình hóa các phụ thuộc toàn cục theo một hướng, điều này không phù hợp với hình ảnh y tế có chiều cao. Do đó, trong khối TSMamba, để mô hình hóa hiệu quả thông tin toàn cục của các đặc trưng có chiều cao, chúng tôi thiết kế mô-đun Mamba ba hướng tính toán các phụ thuộc đặc trưng từ ba hướng. Như thể hiện trong Hình 2 (b), chúng tôi làm phẳng các đặc trưng 3D đầu vào thành ba chuỗi để thực hiện các tương tác đặc trưng tương ứng và thu được các đặc trưng 3D được hợp nhất.

ToM(z) = Mamba(zf) + Mamba(zr) + Mamba(zs),     (3)

trong đó Mamba đại diện cho lớp Mamba được sử dụng để mô hình hóa thông tin toàn cục trong một chuỗi. Các ký hiệu f, r, s biểu thị việc làm phẳng theo hướng tiến, hướng ngược, và hướng giữa các lát, tương ứng.

2.2 Ước lượng Không chắc chắn Mức Đặc trưng (FUE)

Các đặc trưng đa quy mô từ bộ mã hóa bao gồm thông tin không chắc chắn [27,23] cho các cấu trúc khác nhau, chẳng hạn như nền và khối u, trong dữ liệu 3D. Để tăng cường các đặc trưng với độ không chắc chắn thấp hơn trên nhiều quy mô, chúng tôi thiết kế mô-đun ước lượng không chắc chắn mức đặc trưng (FUE) đơn giản trong các kết nối bỏ qua. Như minh họa trong Hình 2, đối với đặc trưng quy mô thứ i zi trong RCi×Di×Hi×Wi, chúng tôi tính giá trị trung bình trên chiều kênh và sau đó sử dụng hàm sigmoid để chuẩn hóa đặc trưng này. Quá trình tính toán của độ không chắc chắn ui có thể được tóm tắt như sau:

ui = -z̄i log(z̄i), trong đó z̄i = sigma(1/Ci ∑c=1^Ci zi^c).     (4)

Do đó, đặc trưng quy mô thứ i cuối cùng được biểu diễn là z̃i = zi + zi · (1-ui).

3 Thực nghiệm

3.1 Bộ dữ liệu Phân đoạn Ung thư Đại trực tràng Thu thập (CRC-500)

Ung thư đại trực tràng (CRC) là loại ung thư phổ biến thứ ba trên toàn thế giới ở nam và nữ, nguyên nhân gây tử vong thứ hai liên quan đến ung thư, và nguyên nhân chính gây tử vong trong ung thư tiêu hóa [3]. Tuy nhiên, như thể hiện trong Bảng 1, các bộ dữ liệu phân đoạn ung thư đại trực tràng 3D hiện có có kích thước hạn chế, và hầu hết chúng đều riêng tư. Chúng tôi đóng góp một bộ dữ liệu quy mô lớn mới (có tên CRC-500), bao gồm 500 thể tích đại trực tràng 3D với các chú thích chính xác tương ứng từ các chuyên gia. Hình 3 trình bày các ví dụ ở định dạng 2D từ bộ dữ liệu CRC-500 được đề xuất của chúng tôi.

Xây dựng Bộ dữ liệu Các quét CT được thu thập từ tháng 1 năm 2008 đến tháng 4 năm 2020. Tất cả thông tin nhạy cảm của bệnh nhân đã được loại bỏ. Mỗi thể tích được chú thích bởi một bác sĩ chuyên nghiệp và được hiệu chỉnh bởi một bác sĩ chuyên nghiệp khác.

Phân tích Bộ dữ liệu Tất cả các quét CT có cùng kích thước trong mặt phẳng là 512×512, và kích thước dọc theo trục z dao động từ 94 đến 238, với trung vị là 166. Khoảng cách trong mặt phẳng dao động từ 0.685×0.685mm đến 0.925×0.925mm, với trung vị là 0.826×0.826mm, và khoảng cách trục z từ 3.0 mm đến 3.75 mm, với trung vị là 3.75 mm.

3.2 Chuẩn mực Công khai và Triển khai

Bộ dữ liệu BraTS2023 Bộ dữ liệu BraTS2023 [17,1,10] chứa tổng cộng 1,251 thể tích MRI não 3D. Mỗi thể tích bao gồm bốn phương thức (tức là T1, T1Gd, T2, T2-FLAIR) và ba mục tiêu phân đoạn (WT: Toàn bộ Khối u, ET: Khối u Tăng cường, TC: Lõi Khối u).

Bộ dữ liệu AIIB2023 Bộ dữ liệu AIIB2023 [19], thử thách mở đầu tiên và bộ dữ liệu công khai có sẵn cho phân đoạn đường thở. Dữ liệu được phát hành bao gồm 120 quét tomography máy tính độ phân giải cao với chú thích chuyên gia chính xác, cung cấp tham chiếu đường thở đầu tiên cho bệnh phổi xơ hóa.

Chi tiết Triển khai Mô hình của chúng tôi được triển khai trong PyTorch 2.0.1-cuda11.7 và Monai 1.2.0. Trong quá trình đào tạo, chúng tôi sử dụng kích thước cắt ngẫu nhiên 128×128×128 và kích thước batch là 2 mỗi GPU cho mỗi bộ dữ liệu. Chúng tôi sử dụng cross-entropy loss trong tất cả các thực nghiệm và sử dụng trình tối ưu SGD với bộ lập lịch tốc độ học đa thức (tốc độ học ban đầu là 1e-2, độ suy giảm là 1e-5). Chúng tôi chạy 1000 epoch cho tất cả các bộ dữ liệu và áp dụng các tăng cường dữ liệu sau: độ sáng cộng thêm, gamma, xoay, chia tỷ lệ, phản chiếu, và biến dạng đàn hồi. Tất cả các thực nghiệm được tiến hành trên nền tảng điện toán đám mây với bốn GPU NVIDIA A100. Đối với mỗi bộ dữ liệu, chúng tôi phân bổ ngẫu nhiên 70% thể tích 3D cho đào tạo, 10% cho xác thực, và 20% còn lại cho kiểm tra.

3.3 So sánh với Các phương pháp SOTA

Chúng tôi so sánh SegMamba với sáu phương pháp phân đoạn tiên tiến, bao gồm ba phương pháp dựa trên CNN (SegresNet [18], UX-Net [11], MedNeXt [20]), và ba phương pháp dựa trên transformer (UNETR [6], SwinUNETR [5], và SwinUNETR-V2 [7]). Để so sánh công bằng, chúng tôi sử dụng các triển khai công khai của các phương pháp này để đào tạo lại mạng của chúng dưới cùng các cài đặt. Điểm số Dice (Dice) và Khoảng cách Hausdorff 95% (HD95) được áp dụng để so sánh định lượng trên các bộ dữ liệu BraTS2023 và CCR-500. Theo [19], Intersection over union (IoU), Tỷ lệ độ dài Phát hiện (DLR), và Tỷ lệ nhánh Phát hiện (DBR) được áp dụng trên bộ dữ liệu AIIB2023.

BraTS2023 Kết quả phân đoạn glioma cho bộ dữ liệu BraTS2023 được liệt kê trong Bảng 2. UX-Net, một phương pháp dựa trên CNN, đạt hiệu suất tốt nhất trong số các phương pháp so sánh, với Dice trung bình là 89.69% và HD95 trung bình là 4.81. So sánh, SegMamba của chúng tôi đạt Dice cao nhất là 93.61%, 92.65%, và 87.71%, và HD95 là 3.37, 3.85, và 3.48 trên WT, TC, và ET, tương ứng, cho thấy tính bền vững phân đoạn tốt hơn.

AIIB2023 Đối với bộ dữ liệu này, mục tiêu phân đoạn là cây đường thở, bao gồm nhiều nhánh nhỏ và đặt ra thách thức trong việc thu được kết quả bền vững. Như thể hiện trong Bảng 2, SegMamba của chúng tôi đạt điểm IoU, DLR, và DBR cao nhất là 88.59%, 70.21%, và 61.33%, tương ứng. Điều này cũng cho thấy SegMamba của chúng tôi thể hiện tính liên tục phân đoạn tốt hơn so với các phương pháp khác.

CRC-500 Kết quả trên bộ dữ liệu CRC-500 được liệt kê trong Bảng 3. Trong bộ dữ liệu này, vùng ung thư thường nhỏ; tuy nhiên, SegMamba của chúng tôi có thể phát hiện chính xác vùng ung thư và báo cáo điểm Dice và HD95 tốt nhất là 48.46% và 28.52, tương ứng.

So sánh Trực quan Để so sánh kết quả phân đoạn của các phương pháp khác nhau một cách trực quan hơn, chúng tôi chọn sáu phương pháp so sánh để so sánh trực quan trên ba bộ dữ liệu. Như mô tả trong Hình 4, SegMamba của chúng tôi có thể phát hiện chính xác ranh giới của mỗi vùng khối u trên bộ dữ liệu BraTS2023. Tương tự như bộ dữ liệu BraTS2023, phương pháp của chúng tôi phát hiện chính xác vùng ung thư trên bộ dữ liệu CRC-500. Kết quả phân đoạn cho thấy tính nhất quán tốt hơn so với các phương pháp tiên tiến khác. Cuối cùng, trên bộ dữ liệu AIIB2023, SegMamba của chúng tôi có thể phát hiện số lượng nhánh lớn hơn trong đường thở và đạt được tính liên tục tốt hơn.

3.4 Nghiên cứu Loại bỏ

Tính hiệu quả của Các mô-đun được Đề xuất Như thể hiện trong Bảng 4, M1 đại diện cho phương pháp cơ bản của chúng tôi, chỉ bao gồm lớp Mamba gốc. Trong M2, chúng tôi giới thiệu mô-đun GSC của chúng tôi. So với M1, M2 đạt được cải thiện 2.88% và 13.95% trong Dice và HD95. Điều này cho thấy mô-đun GSC có thể cải thiện biểu diễn không gian trước mô-đun ToM. Sau đó, trong M3, chúng tôi giới thiệu mô-đun ToM, mô hình hóa thông tin toàn cục từ ba hướng. M3 báo cáo Dice và HD95 là 47.22% và 33.32, với cải thiện 1.22% và 9.97% so với M2. Hơn nữa, chúng tôi giới thiệu các mô-đun GSC và ToM đồng thời, dẫn đến tăng 1.69% trong Dice và 7.29% trong HD95. Cuối cùng, SegMamba của chúng tôi giới thiệu cả ba mô-đun GSC, ToM, và FUE, đạt hiệu suất tiên tiến, với Dice và HD95 là 48.46% và 28.52.

Hiệu quả cao của SegMamba Chúng tôi xác minh hiệu quả cao của SegMamba thông qua nghiên cứu loại bỏ được trình bày trong Bảng 5. M4 là UX-Net [11], sử dụng tích chập kernel lớn làm mô-đun cốt lõi. M5 là SwinUNETR [5], sử dụng SwinTransformer làm mô-đun cốt lõi. Cả hai đều cải thiện trường tiếp nhận bằng cách tính toán các pixel tầm xa, nhưng chúng không thể tính toán mối quan hệ trong phạm vi toàn cục. Trong M6, chúng tôi sử dụng tự chú ý, một lớp mô hình hóa toàn cục, làm mô-đun cốt lõi, nhưng nó không khả thi do gánh nặng tính toán. So sánh, phương pháp của chúng tôi sử dụng mô-đun mô hình hóa toàn cục dựa trên Mamba (TSMamba), và đạt được bộ nhớ đào tạo (TM) và thời gian suy luận (IT) tốt hơn, mặc dù độ dài chuỗi được làm phẳng tối đa đạt 260k.

4 Kết luận

Trong bài báo này, chúng tôi đề xuất phương pháp phân đoạn hình ảnh y tế 3D tổng quát đầu tiên dựa trên Mamba, gọi là SegMamba. Đầu tiên, chúng tôi thiết kế mô-đun Mamba ba hướng (ToM) để tăng cường mô hình hóa tuần tự cho các đặc trưng 3D. Để mô hình hóa hiệu quả các mối quan hệ không gian trước mô-đun ToM, chúng tôi thiết kế thêm mô-đun tích chập không gian có cổng (GSC). Hơn nữa, chúng tôi thiết kế mô-đun ước lượng không chắc chắn mức đặc trưng (FUE) để tăng cường các đặc trưng đa quy mô trong các kết nối bỏ qua. Cuối cùng, chúng tôi trình bày một bộ dữ liệu quy mô lớn mới cho phân đoạn ung thư đại trực tràng 3D, có tên CRC-500, để hỗ trợ nghiên cứu liên quan. SegMamba thể hiện khả năng đáng chú ý trong việc mô hình hóa các phụ thuộc tầm xa trong dữ liệu thể tích, đồng thời duy trì hiệu quả suy luận xuất sắc. Các thực nghiệm mở rộng chứng minh tính hiệu quả và tính phổ quát của phương pháp chúng tôi.

Lời cảm ơn Công trình này được hỗ trợ bởi Chương trình Tài trợ Chung Quảng Châu-HKUST(GZ) (Số 2023A03J0671), Dự án Khoa học và Công nghệ Thành phố Quảng Châu (Số tài trợ 2023A03J0671), và tài trợ InnoHK được khởi động bởi Ủy ban Đổi mới và Công nghệ, Đặc khu Hành chính Hồng Kông.

Tiết lộ Lợi ích Các tác giả tuyên bố rằng họ không có lợi ích cạnh tranh.

Tài liệu tham khảo

1. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Freymann, J.B., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features. Scientific data 4(1), 1-13 (2017)

2. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)

3. Granados-Romero, J.J., Valderrama-Treviño, A.I., Contreras-Flores, E.H., Barrera-Mera, B., Herrera Enríquez, M., Uriarte-Ruíz, K., Ceballos-Villalba, J.C., Estrada-Mata, A.G., Alvarado Rodríguez, C., Arauz-Peña, G.: Colorectal cancer: a review. Int J Res Med Sci 5(11), 4667 (2017)

4. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023)

5. Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In: International MICCAI Brainlesion Workshop. pp. 272-284. Springer (2022)

6. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 574-584 (2022)

7. He, Y., Nath, V., Yang, D., Tang, Y., Myronenko, A., Xu, D.: Swinunetr-v2: Stronger swin transformers with stagewise convolutions for 3d medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 416-426. Springer (2023)

8. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods 18(2), 203-211 (2021)

9. Kalman, R.E.: A new approach to linear filtering and prediction problems (1960)

10. Kazerooni, A.F., Khalili, N., Liu, X., Haldar, D., Jiang, Z., Anwar, S.M., Albrecht, J., Adewole, M., Anazodo, U., Anderson, H., et al.: The brain tumor segmentation (brats) challenge 2023: Focus on pediatrics (cbtn-connect-dipgr-asnr-miccai brats-peds). ArXiv (2023)

11. Lee, H.H., Bao, S., Huo, Y., Landman, B.A.: 3d ux-net: A large kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation. arXiv preprint arXiv:2209.15076 (2022)

12. Liu, H., Dai, Z., So, D., Le, Q.V.: Pay attention to mlps. Advances in Neural Information Processing Systems 34, 9204-9215 (2021)

13. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024)

14. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10012-10022 (2021)

15. Luo, P., Xiao, G., Gao, X., Wu, S.: Lkd-net: Large kernel convolution network for single image dehazing. In: 2023 IEEE International Conference on Multimedia and Expo (ICME). pp. 1601-1606. IEEE (2023)

16. Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722 (2024)

17. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor image segmentation benchmark (brats). IEEE transactions on medical imaging 34(10), 1993-2024 (2014)

18. Myronenko, A.: 3d mri brain tumor segmentation using autoencoder regularization. In: International MICCAI Brainlesion Workshop. pp. 311-320. Springer (2018)

19. Nan, Y., Xing, X., Wang, S., Tang, Z., Felder, F.N., Zhang, S., Ledda, R.E., Ding, X., Yu, R., Liu, W., et al.: Hunting imaging biomarkers in pulmonary fibrosis: Benchmarks of the aiib23 challenge. arXiv preprint arXiv:2312.13752 (2023)

20. Roy, S., Koehler, G., Ulrich, C., Baumgartner, M., Petersen, J., Isensee, F., Jaeger, P.F., Maier-Hein, K.H.: Mednext: transformer-driven scaling of convnets for medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 405-415. Springer (2023)

21. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017)

22. Wang, H., Zhu, L., Yang, G., Guo, Y., Zhang, S., Xu, B., Jin, Y.: Video-instrument synergistic network for referring video instrument segmentation in robotic surgery. arXiv preprint arXiv:2308.09475 (2023)

23. Xing, Z., Wan, L., Fu, H., Yang, G., Zhu, L.: Diff-unet: A diffusion embedded network for volumetric segmentation. arXiv preprint arXiv:2303.10326 (2023)

24. Xing, Z., Yu, L., Wan, L., Han, T., Zhu, L.: Nestedformer: Nested modality-aware transformer for brain tumor segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 140-150. Springer (2022)

25. Xing, Z., Zhu, L., Yu, L., Xing, Z., Wan, L.: Hybrid masked image modeling for 3d medical image segmentation. IEEE Journal of Biomedical and Health Informatics (2024)

26. Yang, Y., Xing, Z., Zhu, L.: Vivim: a video vision mamba for medical video object segmentation. arXiv preprint arXiv:2401.14168 (2024)

27. Zhao, J., Xing, Z., Chen, Z., Wan, L., Han, T., Fu, H., Zhu, L.: Uncertainty-aware multi-dimensional mutual learning for brain and brain tumor segmentation. IEEE Journal of Biomedical and Health Informatics (2023)

28. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024)
