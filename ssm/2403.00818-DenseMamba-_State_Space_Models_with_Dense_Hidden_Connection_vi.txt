# 2403.00818.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2403.00818.pdf
# Kích thước tệp: 490103 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
DenseMamba: Các Mô hình Không gian Trạng thái với Kết nối Ẩn Dày đặc
cho các Mô hình Ngôn ngữ Lớn Hiệu quả
Wei He* 1Kai Han* 1Yehui Tang1Chengcheng Wang1Yujie Yang1Tianyu Guo1Yunhe Wang1

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) đối mặt với một thử thách đáng gờm do yêu cầu tính toán và bộ nhớ quá mức của kiến trúc Transformer thường được sử dụng. Trong khi mô hình không gian trạng thái (SSM) là một loại kiến trúc mạng nền tảng mới cung cấp độ phức tạp tính toán thấp hơn, hiệu suất của chúng vẫn chưa hoàn toàn cạnh tranh với Transformer. Bài báo này giới thiệu DenseSSM, một phương pháp mới để tăng cường luồng thông tin ẩn giữa các lớp trong SSM. Bằng cách tích hợp có chọn lọc các trạng thái ẩn lớp nông vào các lớp sâu hơn, DenseSSM giữ lại thông tin chi tiết quan trọng cho đầu ra cuối cùng. Các kết nối dày đặc được tăng cường DenseSSM vẫn duy trì khả năng song song hóa huấn luyện và hiệu quả suy luận. Phương pháp được đề xuất có thể được áp dụng rộng rãi cho các loại SSM khác nhau như RetNet và Mamba. Với kích thước mô hình tương tự, DenseSSM đạt được những cải thiện đáng kể, được minh họa bởi DenseRetNet vượt trội hơn RetNet gốc với cải thiện độ chính xác lên đến 5% trên các tiêu chuẩn công khai. Mã nguồn có sẵn tại: https://github.com/WailordHe/DenseSSM .

1. Giới thiệu
Kể từ khi phát hành ChatGPT (OpenAI, 2023), các mô hình ngôn ngữ lớn đã bước vào một kỷ nguyên mới, thể hiện khả năng xuất sắc trong hiểu ngôn ngữ, đối thoại và lý luận logic. Trong năm qua, ngành công nghiệp đã chứng kiến sự xuất hiện của nhiều mô hình ngôn ngữ lớn, như LLaMA (Touvron et al., 2023) và ChatGLM (Zeng et al., 2023). Những mô hình ngôn ngữ lớn này đã tạo ra vô số ứng dụng thực tế, bao gồm bot đối thoại, trợ lý viết mã và đại lý AI. Nền tảng của các mô hình ngôn ngữ lớn nằm ở cấu trúc mạng Transformer (Vaswani et al., 2017), chủ yếu sử dụng mô-đun tự chú ý đa đầu để mô hình hóa mối quan hệ giữa các token và mạng feed-forward cho biến đổi đặc trưng phi tuyến. Luật tỷ lệ (Kaplan et al., 2020) dựa trên cấu trúc Transformer đã thúc đẩy sự phát triển và mở rộng liên tục của các mô hình ngôn ngữ lớn.

*Đóng góp ngang bằng1Phòng thí nghiệm Noah's Ark của Huawei. Liên hệ: Kai Han <kai.han@huawei.com>, Yunhe Wang <yunhe.wang@huawei.com>.

Trong mạng Transformer, tự chú ý đa đầu (MHSA) đóng vai trò quan trọng, nhưng nó đi kèm với yêu cầu tính toán đáng kể và yêu cầu bộ nhớ trong quá trình suy luận. Về độ phức tạp tính toán, đối với câu đầu vào có độ dài N, việc tính toán tự chú ý có độ phức tạp O(N²) trong quá trình huấn luyện và suy luận. Về việc sử dụng bộ nhớ, các khóa và giá trị đã gặp trước đó được lưu trữ, dẫn đến việc chiếm dụng bộ nhớ O(ND). Do đó, những nỗ lực gần đây về kiến trúc mạng đã tập trung vào việc đơn giản hóa Transformer bằng cách giảm độ phức tạp tính toán và không gian của nó. Điều này bao gồm các phương pháp tiếp cận khác nhau, đáng chú ý là các mô hình ngôn ngữ tích chập (Poli et al., 2023), đơn vị hồi quy (Lei, 2021), mô hình ngữ cảnh dài (Ding et al., 2023), và mô hình không gian trạng thái (SSM) (Gu et al., 2021; Gu & Dao, 2023). Những mô hình mới này đã cung cấp các lựa chọn thay thế mạnh mẽ cho Transformer để xây dựng LLM hiệu quả.

SSM đề xuất mô hình hóa chuỗi bằng cách giới thiệu thiết kế phù hợp của các trạng thái ẩn để xử lý các phụ thuộc tầm xa với cả khả năng song song hóa huấn luyện và hiệu quả suy luận. Bắt đầu từ hệ thống ánh xạ liên tục, SSM được rời rạc hóa để xử lý đầu vào rời rạc trong học sâu như chuỗi ngôn ngữ. SSM rời rạc hóa có thể được tính toán trong cả chế độ hồi quy tuyến tính và chế độ tích chập toàn cục. Thông thường, chế độ tích chập được sử dụng trong quá trình huấn luyện để đạt được gia tốc song song, trong khi chế độ hồi quy được sử dụng trong quá trình suy luận tự hồi quy vì nó có độ phức tạp tính toán thấp hơn.

Sự khác biệt cốt lõi của SSM so với các mạng neural khác, như mạng neural kết nối đầy đủ, nằm ở thiết kế của các trạng thái ẩn. Các trạng thái ẩn cho phép thông tin được truyền bá dọc theo chiều thời gian, đồng thời tránh độ phức tạp tính toán của việc truy cập các token lịch sử ở mỗi bước. Thông qua các tham số chuyển tiếp trạng thái A, các trạng thái ẩn chuyển thông tin ẩn từ các bước thời gian trước đó đến bước thời gian hiện tại, cho phép dự đoán tự hồi quy của token tiếp theo. Các trạng thái ẩn đóng vai trò quan trọng trong SSM, nhưng đã không nhận được đủ nghiên cứu trong quá khứ. Trọng số và đặc trưng ẩn trong các lớp khác nhau chứa thông tin ở các mức độ khác nhau từ chi tiết đến thô (Gu et al., 2021). Tuy nhiên, trong các phiên bản SSM trước đây, các trạng thái ẩn chỉ chảy trong lớp hiện tại và không thể truyền nhiều thông tin hơn đến các lớp sâu hơn, do đó không thể nắm bắt thông tin phân cấp nhiều hơn.

Trong bài báo này, chúng tôi đề xuất DenseSSM để tạo điều kiện cho luồng thông tin ẩn toàn diện hơn giữa các lớp trong các mô hình không gian trạng thái. Chúng tôi đầu tiên phân tích sự suy thoái trạng thái ẩn trong SSM thông thường sẽ ngăn cản luồng thông tin ẩn từ mức thấp đến mức cao. Bằng cách tích hợp có chọn lọc các trạng thái ẩn lớp nông vào các lớp sâu hơn, DenseSSM giữ lại thông tin chi tiết hữu ích cho đầu ra cuối cùng. Phương pháp được đề xuất áp dụng được cho các loại SSM khác nhau, như RetNet (Sun et al., 2023) và Mamba (Gu & Dao, 2023). Phương pháp của chúng tôi duy trì khả năng song song hóa huấn luyện và hiệu quả suy luận của SSM, đồng thời đạt được cải thiện đáng kể với chỉ một ít tăng số lượng tham số. Ví dụ, mô hình DenseRetNet của chúng tôi vượt trội hơn RetNet truyền thống với cải thiện độ chính xác lên đến 5% trên các tiêu chuẩn công khai.

2. Các Công trình Liên quan
2.1. Các Mô hình Ngôn ngữ Lớn
Các mô hình ngôn ngữ lớn (LLM) đã chứng kiến những tiến bộ biến đổi, cho phép chúng xuất sắc trong một loạt các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) đa dạng, bao gồm dịch máy, tóm tắt văn bản, và các khả năng nổi lên như học trong ngữ cảnh, điều mà các mô hình ngôn ngữ trước đây không thể đạt được (Devlin et al., 2019; Raffel et al., 2023). Sự phát triển của LLM được đánh dấu bởi một sự thay đổi quy mô khổng lồ, được minh họa bởi các mô hình như GPT-3 (Brown et al., 2020), với 175 tỷ tham số, và PaLM thậm chí còn mở rộng hơn (Chowdhery et al., 2022), với 540 tỷ tham số đáng kinh ngạc. Những mô hình này đã xác nhận thực nghiệm luật tỷ lệ (Kaplan et al., 2020), cho rằng tăng kích thước mô hình dẫn đến cải thiện hiệu suất.

Sự mở rộng nhanh chóng về kích thước mô hình đã nhấn mạnh nhu cầu cấp thiết phát triển các thuật toán Transformer hiệu quả, trong đó FlashAttention (Dao et al., 2022; Dao, 2023) đã nổi lên như một sự đổi mới đáng kể. Phương pháp này tăng cường cơ chế chú ý quan trọng trong Transformer bằng cách tối ưu hóa các tính toán softmax sử dụng kỹ thuật được gọi là tiling. Bằng cách giảm thiểu các giao dịch bộ nhớ giữa HBM của GPU và SRAM trên chip, FlashAttention tính toán chú ý chính xác với ít truy cập bộ nhớ hơn, dẫn đến cả thực thi nhanh hơn và dấu chân bộ nhớ thấp hơn so với các triển khai chú ý tiêu chuẩn.

2.2. Các Mô hình Không gian Trạng thái
Trong khi Transformer hiện tại là kiến trúc de facto cho các mô hình ngôn ngữ lớn (LLM), cung cấp huấn luyện GPU song song hiệu quả, thời gian suy luận cho suy luận token đơn tăng đáng kể với độ dài chuỗi dài hơn, đặt ra thách thức cho triển khai do độ phức tạp O(N) mỗi bước ngay cả với các thuật toán tăng tốc như FlashAttention (Dao et al., 2022; Dao, 2023). Các nỗ lực đã được dành cho việc nghiên cứu kiến trúc Transformer-Next, nhằm đạt được hiệu suất tiên tiến (SOTA) với huấn luyện song song hiệu quả và suy luận hiệu quả, đặc biệt cho độ dài chuỗi dài.

Các Mô hình Chuỗi Không gian Trạng thái (SSM) gần đây đã nổi lên như các kiến trúc đầy hứa hẹn cho mô hình hóa chuỗi. HiPPO (Gu et al., 2020) hợp lý hóa mô hình hóa chuỗi bằng cách nén các đầu vào dài thành một biểu diễn động, dựa trên đa thức sử dụng các đa thức trực giao. S4 (Gu et al., 2021) giới thiệu một tham số hóa mới thông qua việc áp dụng một hiệu chỉnh có cấu trúc thứ hạng thấp, cho phép đường chéo hóa ổn định và đơn giản hóa quá trình thành các phép toán kernel Cauchy. S5 (Smith et al., 2023) đơn giản hóa hơn nữa lớp S4 bằng cách sử dụng một SSM đa đầu vào, đa đầu ra duy nhất và giới thiệu các thuật toán quét song song hiệu quả vào các lớp S4. H3 (Fu et al., 2023) thu hẹp khoảng cách hiệu suất giữa SSM và các mô hình ngôn ngữ Transformer bằng cách thiết kế ba phép chiếu (Q, K, V) để mô phỏng cơ chế chú ý và áp dụng biến đổi Fourier nhanh (FFT) để giảm thêm tính toán và tiêu thụ bộ nhớ.

GSS (Mehta et al., 2022) là kiến trúc mạng neural có cổng đầu tiên tích hợp SSM, nó xây dựng dựa trên (Hua et al., 2022) và giới thiệu một kiến trúc SSM compact thu hẹp các chiều mô hình. Không giống như GSS, tập trung vào nén ngữ cảnh thành một trạng thái nhỏ hơn, Mamba (Gu & Dao, 2023) khác biệt bằng cách tập trung vào việc tăng cường tính chọn lọc của biểu diễn trạng thái, nhằm cân bằng sự đánh đổi giữa hiệu quả và hiệu suất mà không làm tổn hại khả năng nắm bắt thông tin thiết yếu từ ngữ cảnh của mô hình. Nó đạt được điều này bằng cách tích hợp một cơ chế lựa chọn cho phép mô hình ưu tiên có chọn lọc thông tin liên quan đồng thời sử dụng một thuật toán được tối ưu hóa phần cứng đảm bảo tính toán hiệu quả.

2.3. Chú ý Tuyến tính
Các chú ý tuyến tính (Katharopoulos et al., 2020; Zhai et al., 2021), loại bỏ phép toán softmax khỏi chú ý truyền thống, có thể được xem như một dẫn xuất của Các Mô hình Không gian Trạng thái (SSM). Chúng thay thế các tích chập của SSM bằng một biến thể của Chú ý Đa Đầu (MHA) và loại bỏ softmax của cơ chế chú ý truyền thống bằng cách sử dụng một hàm kernel hoạt động độc lập trên các truy vấn (Q) và khóa (K). Những cơ chế này cũng có dạng song song cho huấn luyện hiệu quả và dạng hồi quy với độ phức tạp O(1).

RetNet (Sun et al., 2023), TransNormerLLM (Qin et al., 2024), và RWKV (Peng et al., 2023) triển khai một yếu tố suy giảm cố định để cập nhật các trạng thái khóa-giá trị (KV) trước đó ở mỗi bước hồi quy. Cơ chế suy giảm này tích hợp liền mạch với mặt nạ chú ý nhân quả cho tính toán song song hiệu quả. Tuy nhiên, vì yếu tố suy giảm này được đặt trước và độc lập với dữ liệu, nó có thể không áp dụng được toàn cầu cho tất cả các nhiệm vụ, đặc biệt khi lời nhắc hoặc thông tin tầm xa đặc biệt quan trọng. Để giải quyết thách thức này, GLA (Gated Linear Attention) (Yang et al., 2023) giới thiệu các cơ chế cổng phụ thuộc dữ liệu thực tế cho cả dạng song song và song song khối. Nó hoạt động cạnh tranh với các baseline mạnh, bao gồm Transformer kiến trúc LLaMA (Touvron et al., 2023) và Mamba (Gu & Dao, 2023).

3. DenseSSM
Trong phần này, chúng tôi phân tích sự suy thoái trạng thái ẩn trong các lớp sâu hơn của SSM và tiếp tục giới thiệu kết nối dày đặc của các trạng thái ẩn để bảo tồn thông tin phong phú hơn cho các lớp sâu hơn.

3.1. Các Khái niệm Cơ bản
Transformer Transformer là kiến trúc mạng được sử dụng rộng rãi của các mô hình ngôn ngữ lớn dựa trên cơ chế tự chú ý. Tự chú ý hoạt động như sau:

ot=WoPT
i=1eqT
tkiviPT
i=1eqT
tkil, (1)

trong đó q,kandv được thu được bởi các lớp kết nối đầy đủ, Wo là trọng số biến đổi tuyến tính cho token đầu ra ot tại bước thời gian thứ t. Mỗi token sẽ hợp nhất thông tin của các token khác bằng trọng số mối quan hệ được tính bởi tự chú ý. Ngoài mô-đun tự chú ý, mô-đun mạng feed-forward (FFN) là một thành phần quan trọng khác để biến đổi biểu diễn token và giới thiệu nhiều phi tuyến tính hơn. Mô-đun FFN thường được cấu tạo bởi hai lớp tuyến tính xếp chồng và hàm kích hoạt phi tuyến:

yt=Wdownsigma(Wupot), (2)

trong đó WupWdown là các ma trận trọng số của các lớp chiếu lên và chiếu xuống, và sigma(·) là hàm kích hoạt như GELU (Hendrycks & Gimpel, 2016).

SSM Các mô hình không gian trạng thái (SSM) trong tài liệu học sâu đề cập đến lớp SSM có cấu trúc (Gu et al., 2021) và các dẫn xuất như RWKV (Peng et al., 2023) và RetNet (Sun et al., 2023). Ở đây chúng tôi mô tả ngắn gọn SSM có cấu trúc như một đại diện. SSM có cấu trúc định nghĩa một biến đổi chuỗi-đến-chuỗi x(t)→y(t) với một trạng thái tiềm ẩn ngầm h(t). Dạng liên tục được công thức hóa như

h′(t) =Ah(t) +Bx(t), (3)
y(t) =Ch(t), (4)

trong đó A,BC là các tham số. Để áp dụng SSM vào dữ liệu rời rạc thực, chúng tôi rời rạc hóa trường hợp liên tục và thu được công thức hồi quy và công thức tích chập của nó. Các tham số AB được biến đổi thành các tham số rời rạc AB với quy tắc rời rạc hóa như zero-order hold (Gu et al., 2021). Công thức hồi quy là

ht=Aht−1+Bxt, (5)
yt=Cht. (6)

Công thức tích chập là

K= (CB, CAB,···, CAtB), (7)
y=x*K, (8)

trong đó * là phép toán tích chập, và t+1 là kích thước kernel tích chập. Chế độ hồi quy thường được sử dụng cho suy luận tự hồi quy hiệu quả, trong khi chế độ tích chập được sử dụng cho huấn luyện có thể song song hóa hiệu quả.

3.2. Suy thoái Trạng thái Ẩn
Ở đây chúng tôi phân tích luồng thông tin ẩn từ các lớp nông đến các lớp sâu. Trong phần sau, chúng tôi sử dụng chỉ số trên "l" để biểu thị khối thứ l.

hl
t=Ahl
t−1+Bxl
t
=Ahl
t−1+BΘ(yl−1
t)
=Ahl
t−1+BΘ(Chl−1
t)
=Ahl
t−1+BΘ(CAhl−1
t−1+CBΘ(Chl−2
t))
=Ahl
t−1+BΘ(CAhl−1
t−1+···
+CBΘ(CAhl−m+1
t−1+CBΘ(Chl−m
t))···)|{z}
m,(9)

trong đó Θ(·) là các biến đổi từ đầu ra cuối cùng đến đầu vào của mô-đun SSM, như tích chập và FFN. Từ Eq. 9, chúng ta có thể thấy rằng việc truyền thông tin ẩn từ lớp thứ (l−m) đến lớp thứ l đòi hỏi phải đi qua m khối biến đổi và m phép nhân ma trận BC. Quá trình tính toán phức tạp như vậy có thể dẫn đến mất thông tin đáng kể, có nghĩa là cố gắng truy xuất thông tin nhất định từ lớp thứ (l−m) ở lớp thứ l trở nên rất khó khăn và không rõ ràng.

3.3. Kết nối Ẩn Dày đặc
Thông qua phân tích trên, chúng tôi đã xác định một vấn đề quan trọng trong SSM, đó là sự suy giảm của các trạng thái ẩn quan trọng khi độ sâu lớp tăng. Do đó, chúng tôi đề xuất một kết nối dày đặc cho các trạng thái ẩn để bảo tồn tốt hơn thông tin chi tiết từ các lớp nông, tăng cường khả năng của các lớp sâu để cảm nhận thông tin văn bản gốc. Đối với khối thứ l, chúng tôi kết nối dày đặc các trạng thái ẩn trong m khối trước đó của nó. Đầu tiên, chúng tôi thu thập các trạng thái ẩn nông và giới thiệu một mô-đun chuyển tiếp chọn lọc ϕ để chiếu chúng vào không gian con của lớp đích và đồng thời chọn các phần hữu ích:

Hl
t= [ϕ(hl−1
t);ϕ(hl−2
t);···;ϕ(hl−m
t)], (10)

Sau đó, các vector ẩn trung gian được tiêm vào trạng thái ẩn gốc của lớp này:

h′l
t=Fuse (hl
t,Hl
t). (11)

Phép toán Fuse () là hàm để hợp nhất các vector ẩn trung gian và trạng thái ẩn hiện tại. SSM với kết nối ẩn dày đặc được đề xuất được đặt tên là DenseSSM (Hình 1(a)). Sơ đồ DenseSSM có thể được sử dụng trong bất kỳ biến thể SSM nào như Mamba (Gu & Dao, 2023). So với DenseNet (Huang et al., 2017) cho các mạng tích chập, DenseSSM được đề xuất kết nối dày đặc các trạng thái ẩn trong SSM, và cơ chế chọn lọc và cách thức hợp nhất hiệu quả hơn cho mô hình hóa ngôn ngữ.

Phân tích trên dựa trên chế độ hồi quy, trong phần sau chúng tôi giới thiệu chế độ tích chập của DenseSSM cho huấn luyện hiệu quả. Từ Eq. 5, chúng ta có

hl
t=Ahl
t−1+Bxl
t
=A(Ahl
t−2+Bxl
t−1) +Bxl
t
=A2hl
t−2+ABxl
t−1+Bxl
t
=Athl
0+At−1Bxl
1+···+ABxl
t−1+Bxl
t
=AtBxl
0+At−1Bxl
1+···+ABxl
t−1+Bxl
t.(12)

Quá trình này có thể được thực hiện bởi một tích chập trên chuỗi đầu vào (xl
0, xl
1,···, xl
t):

hl
t=AtBxl
0+At−1Bxl
1+···+ABxl
t−1+Bxl
t
= (xl
0, xl
1,···, xl
t)*(B,AB,···,AtB).(13)

Trong DenseSSM được đề xuất, chúng tôi tăng cường các trạng thái ẩn bằng Eq. 11 và sau đó thu được các đầu ra của SSM:

yl
t=Ch′l
t
=CFuse ((xl
0, xl
1,···, xl
t)*(B,AB,···,AtB),Hl
t).
(14)

Như thể hiện trong Hình 1(b), DenseSSM có thể được huấn luyện trong chế độ tích chập có thể song song hóa.

Mô-đun Chuyển tiếp Chọn lọc Mô-đun chuyển tiếp chọn lọc ϕ(·) để chiếu đầu vào vào không gian con đích và đồng thời chọn phần hữu ích của thông tin ẩn. Chúng tôi triển khai mô-đun chuyển tiếp chọn lọc với lớp chiếu và cơ chế lựa chọn cổng, như thể hiện trong Hình 2. Đầu tiên, chúng tôi chiếu các trạng thái ẩn trong m khối SSM trước đó vào cùng một không gian:

h′l−m
t=Proj(hl−m
t). (15)

Sau đó chúng tôi tạo ra các trọng số cổng dựa trên đầu vào xl
t và sử dụng chúng để chọn các trạng thái ẩn hữu ích:

ϕ(hl−m
t) =h′l−m
t⊙Gate(xl
t). (16)

Xin lưu ý rằng các mô-đun mới được giới thiệu không được làm tổn hại khả năng song song hóa huấn luyện và hiệu quả suy luận của khung SSM gốc. Do đó, chúng tôi duy trì một triển khai đơn giản và hiệu quả trong thực tế. Lớp chiếu được triển khai sử dụng một biến đổi tuyến tính, trong khi mô-đun cổng được triển khai với một MLP hai lớp với kích hoạt SiLU (Elfwing et al., 2018).

Hình 2. Mô-đun Chuyển tiếp Chọn lọc.

Mô-đun Hợp nhất Ẩn Sau mô-đun chuyển tiếp chọn lọc, chúng tôi thu được các trạng thái ẩn được chọn từ các lớp nông, tức là, HL
t= [ϕ(h1
t);ϕ(h2
t);···;ϕ(hL−1
t)]. Một mô-đun hợp nhất ẩn được sử dụng để tích hợp các trạng thái ẩn nông với các trạng thái ẩn hiện tại. Tương tự, chúng tôi giữ triển khai đơn giản cho hiệu quả. Chúng tôi cộng các trạng thái ẩn được chọn vì chúng đã được chiếu vào cùng một không gian:

hL
t=Fuse (hL
t,HL
t) =hL
t+mX
i=1hl−i
t. (17)

Ở đây, chúng tôi cung cấp một triển khai cơ bản, nhưng tất nhiên, có các phương pháp triển khai khác như nối chuỗi và chú ý chéo. Chúng tôi sẽ so sánh các phương pháp triển khai khác nhau trong các thí nghiệm sau.

Mở rộng cho RetNet RetNet (Sun et al., 2023) có thể được xem như một loại mô hình không gian trạng thái sử dụng một biến thể của tự chú ý thay vì tích chập trong Eq. 7. So với Transformer tiêu chuẩn, RetNet là một mô hình ngôn ngữ kiểu RNN với suy luận nhanh và huấn luyện song song. Nó sử dụng chú ý tuyến tính để đơn giản hóa độ phức tạp tính toán của tự chú ý.

St=gammaSt−1+kT
tvt, (18)
yt=qtSt, (19)

trong đó St là trạng thái hồi quy, và 0< gamma < 1. Kết nối KV dày đặc cho RetNet được thực hiện như sau. Các khóa và giá trị cấp thấp đầu tiên được nối:

Kl
t= [ϕ(kl−1
t);ϕ(kl−2
t);···;ϕ(kl−m
t)], (20)
Vl
t= [ϕ(vl−1
t);ϕ(vl−2
t);···;ϕ(vl−m
t)]. (21)

Sau đó, các vector khóa (hoặc giá trị) trung gian được tiêm vào các khóa (hoặc giá trị) gốc của lớp này:

k′L
t=kL
t+mX
i=1kl−i
t, (22)
v′L
t=vL
t+mX
i=1vl−i
t. (23)

RetNet được trang bị các kết nối khóa-giá trị (KV) dày đặc được đề xuất được đặt tên là DenseRetNet, như minh họa trong hình 3. Ngoài ra, chế độ có thể song song hóa của DenseRetNet được công thức hóa như sau:

yt=qttX
i=1gammat−ik′T
iv′
i. (24)

DenseRetNet của chúng tôi cũng có thể được triển khai trong chế độ có thể song song hóa, nghĩa là có thể được huấn luyện song song trên GPU hoặc NPU.

Hình 3. DenseRetNet trong chế độ tự hồi quy.

4. Thí nghiệm
Trong phần này, chúng tôi đã tiến hành các thí nghiệm toàn diện để xác nhận hiệu quả của DenseSSM được đề xuất. Việc xác minh được thực hiện trên các kiến trúc khác nhau, bao gồm RetNet và Mamba.

4.1. Dữ liệu và Cài đặt Thí nghiệm
Dữ liệu Tiền huấn luyện Theo các cài đặt phổ biến trong (Yang et al., 2023), chúng tôi huấn luyện tất cả các mô hình từ đầu sử dụng một kho tài liệu bao gồm 56GB dữ liệu thô được trích xuất từ The Pile (Gao et al., 2020), một bộ dữ liệu đa dạng và chất lượng cao thường được sử dụng. Loại trừ dữ liệu từ các tập con DM Mathematics và Github, chúng tôi thực hiện trộn ngẫu nhiên và lấy mẫu từ tất cả kho tài liệu còn lại. Dữ liệu được token hóa sử dụng tokenizer LLaMA, có kích thước từ vựng 32.000 token. Token <bos> được sử dụng làm đánh dấu bắt đầu chuỗi. Bộ dữ liệu được lưu trữ kết quả chứa tổng cộng 15 tỷ token.

Bộ dữ liệu Đánh giá Trong thí nghiệm của chúng tôi, chúng tôi điều tra hiệu suất của các mô hình trên một phổ các nhiệm vụ downstream, tập trung vào khả năng học zero-shot và 4-shot. Các nhiệm vụ, được trình bày trong Bảng 4 và 6, bao gồm một loạt các bộ dữ liệu được thiết kế để kiểm tra lý luận thông thường và trả lời câu hỏi, như HellaSwag (Zellers et al., 2019), BoolQ (Clark et al., 2019), COPA (Ponti et al., 2020), PIQA (Bisk et al., 2019), Winograd (Muennighoff et al., 2022), Winogrande (Sakaguchi et al., 2019), StoryCloze (Lin et al., 2021), OpenBookQA (Mihaylov et al., 2018), SciQ (Welbl et al., 2017), ARC E(ARC-easy) và ARC C(ARC-challenge) (Clark et al., 2018). Kết quả Words Perplexity của WikiText (Merity et al., 2016) và LAMBADA (LAMBADA OPENAI) (Paperno et al., 2016) cũng được báo cáo. Tất cả các đánh giá được thực hiện sử dụng LM evaluation harness (Gao et al., 2023), đảm bảo một phương pháp tiêu chuẩn hóa để đánh giá khả năng của các mô hình.

4.2. Cài đặt Huấn luyện và Kiến trúc Mô hình
Chúng tôi đã chọn các đặc tả mô hình 350M và 1.3B để xác minh tính hợp lệ của cơ chế dày đặc được đề xuất của chúng tôi. Tất cả các mô hình được huấn luyện từ đầu trong một epoch trên 15 tỷ token. Kích thước batch huấn luyện được đặt thành 0.5 triệu token với cài đặt độ dài huấn luyện 2048 token. Bộ tối ưu AdamW (Loshchilov & Hutter, 2019) được sử dụng cho huấn luyện, với suy giảm tốc độ học đa thức, và tỷ lệ warm-up được đặt thành 1.5% tổng số bước huấn luyện. Weight decay được đặt thành 0.01, và gradient clipping được đặt thành 1. Chúng tôi điều chỉnh các siêu tham số của mô hình để đảm bảo khả năng so sánh với các mô hình cùng quy mô. Ngoài ra, chúng tôi thiết kế mô hình Dense RetNet của chúng tôi được cấu tạo hoàn toàn từ các khối giống GAU, điều này sẽ được chi tiết cụ thể trong đoạn tiếp theo.

Các mô hình ngôn ngữ dựa trên Transformer Chúng tôi đánh giá cơ chế dày đặc chọn lọc được đề xuất của chúng tôi so với các mô hình ngôn ngữ lớn phổ biến như LLaMA (Touvron et al., 2023) và OPT (Zhang et al., 2022), so sánh với LLaMA cho các mô hình kích thước 350M và với OPT cho các mô hình kích thước 1.3B. Bảng 1 báo cáo các siêu tham số của chúng.

Siêu tham số LLaMA 350M OPT 1.3B
lớp 18 24
kích thước ẩn 1024 2048
kích thước ffn 4096 8192
đầu 8 32
tốc độ học 6×10−4
Adam beta (0.9, 0.98)
dropout 0.0 0.1
Bảng 1. Siêu tham số được sử dụng cho các mô hình LLaMA và OPT.

Mamba Như thể hiện trong Bảng 2, vì tokenizer của chúng tôi nhỏ hơn tokenizer GPT-NeoX (Black et al., 2022) mà Mamba (Gu & Dao, 2023) sử dụng, chúng tôi đã thêm hai lớp bổ sung để khớp các tham số. Ngoài điều này, chúng tôi đã tuân thủ cấu trúc mô hình của Mamba và các cài đặt huấn luyện khác được mô tả trong bài báo của họ. Cụ thể, chúng tôi đã đặt tốc độ học thành 3e-4 cho mô hình 360M và 2e-4 cho mô hình 1.3M, và chúng tôi không áp dụng dropout trong cả hai trường hợp. Kiến trúc mới thu được được đặt tên là DenseMamba.

Siêu tham số DenseMamba 360M 1.3B
nlayers 50 50
dmodel 1024 2048
lớp hợp nhất dày đặc 4 4
tốc độ học 3×10−4 2×10−4
Adam beta (0.9, 0.95)
dropout 0.0
Bảng 2. Siêu tham số được sử dụng cho các mô hình DenseMamba.

RetNet Kích thước mô hình và siêu tham số cho DenseRetNet của chúng tôi được thể hiện trong Bảng 3. Chúng tôi tiếp tục sử dụng Gated Attention Unit (GAU) (Hua et al., 2022) trong DenseRetNet của chúng tôi. GAU kết hợp khối Attention và FFN thành một, vì vậy một khối duy nhất có thể thực hiện cả trộn kênh và trộn token: Y= (XW u⊙AˆV)Wo, trong đó A là trọng số chú ý được tính toán thông qua Eq. 24. Ngoài ra, nhiều chú ý

Siêu tham số DenseRetNet 360M 1.3B
lớp 16 25
kích thước ẩn 1536 2560
kích thước q&k 768 1280
kích thước v&gate 3072 5120
đầu 2 4
lớp hợp nhất dày đặc 2 2
tốc độ học 6×10−4
Adam beta (0.9, 0.98)
dropout 0.1
Bảng 3. Siêu tham số được sử dụng cho các mô hình DenseRetNet.

--- TRANG 7 ---
DenseRetNet với nhiều tốc độ suy giảm mũ khác nhau được sử dụng để thực hiện suy giảm đa quy mô thay vì chiến lược đơn đầu của GAU. Trong các thí nghiệm của chúng tôi, chúng tôi đã quan sát thấy rằng kiến trúc của chúng tôi vượt trội hơn cấu trúc RetNet với các lớp FFN về độ ổn định huấn luyện và hiệu suất. Kiến trúc mới thu được được đặt tên là DenseRetNet.

4.3. Kết quả Chính cho DenseRetNet
Chúng tôi đánh giá các mô hình của chúng tôi trên cả kho tài liệu phổ biến và các nhiệm vụ downstream bao gồm lý luận thông thường và trả lời câu hỏi. Bảng 4 trình bày kết quả thí nghiệm so sánh DenseRetNet với LLaMA-350M (Touvron et al., 2023), OPT-1.3B (Zhang et al., 2022) và RetNet (Sun et al., 2023). DenseRetNet của chúng tôi thu được perplexity thấp hơn trên kho tài liệu Wikitext và LAMBADA và cho thấy lợi thế rõ ràng trong các nhiệm vụ downstream trong cả cài đặt 0-shot và few-shot. Đặc biệt, mô hình của chúng tôi cải thiện đáng kể hiệu suất của RetNet, và đạt được hiệu suất vượt trội so với các mô hình ngôn ngữ lớn transformer.

4.4. Kết quả Chính cho DenseMamba
Bảng 6 so sánh hiệu suất của DenseMamba với LLaMA-350M (Touvron et al., 2023), OPT-1.3B (Zhang et al., 2022), và Mamba (Gu & Dao, 2023). DenseMamba thể hiện perplexity và độ chính xác vượt trội trên tập kiểm tra, vượt trội hơn Mamba và các mô hình dựa trên Transformer khác.

4.5. Nghiên cứu Ablation
Trong phần này, chúng tôi tiến hành một nghiên cứu ablation để đánh giá tác động của các lựa chọn thiết kế khác nhau trong Mô-đun Chuyển tiếp Chọn lọc và Mô-đun Hợp nhất Ẩn của chúng tôi. Kết quả perplexity được trình bày cho cả tập đánh giá trong miền và kho tài liệu ngoài miền (Merity et al., 2016). Để so sánh công bằng, baseline cho tất cả các nghiên cứu ablation là DenseRetNet-350M, với điều chỉnh tham số để tạo điều kiện so sánh dưới các ràng buộc tính toán tương tự khi cần thiết. Chúng tôi tuân theo các cài đặt huấn luyện mặc định được nêu trong Bảng 3 cho các mô hình của chúng tôi, ngoại trừ mô hình được huấn luyện trên 1B token.

Ablations trên Mô-đun Chuyển tiếp Chọn lọc Mô-đun chuyển tiếp chọn lọc được đề xuất để chiếu các trạng thái ẩn nông vào cùng một không gian con và chọn các phần hữu ích của chúng. Mô-đun chuyển tiếp chọn lọc có thể được triển khai theo nhiều cách khác nhau.

Bảng 5 điều tra tác động của các cấu hình Projection và Select khác nhau. Các tham số khác của thí nghiệm được giữ không đổi: số lượng lớp dày đặc (m) được đặt thành 2, và phép toán Fusion sau mô-đun chuyển tiếp chọn lọc là phép toán "Add". Các phát hiện cho thấy rằng sự kết hợp của chiếu Identity với MLP đạt được sự cân bằng tối ưu giữa số lượng tham số và hiệu suất.

Projection Select #Param In domain Wikitext
None None 346M 2.565 2.359
Identity MLP 353M 2.546 2.348
Identity Linear 357M 2.572 2.369
Linear MLP 353M 2.579 2.372
Linear Linear 356M 2.582 2.378
Bảng 5. Kết quả cross-entropy loss đánh giá trong miền và byte perplexity ngoài miền cho DenseRetNet-350M với các triển khai khác nhau của mô-đun chuyển tiếp chọn lọc được trình bày.

Ablations trên Lớp Dày đặc Trong thí nghiệm này, chúng tôi tiến hành phân tích ablation về độ sâu của các lớp hợp nhất (ký hiệu là m). Chúng tôi sử dụng chiến lược hợp nhất dựa trên chiếu Identity và tạo cổng sử dụng MLP. Kết quả thí nghiệm của chúng tôi, như được trình bày trong Bảng 7, cả kiến trúc hợp nhất hai lớp (m=2) và bốn lớp (m=4) đều có cải thiện hiệu suất. Xem xét chi phí tính toán liên quan đến huấn luyện và suy luận, phương pháp hợp nhất hai lớp được coi là tối ưu hơn.

Ngoài ra, chúng tôi đã khám phá tính cần thiết của việc sử dụng các lớp tạo cổng riêng biệt cho các lớp dày đặc khác nhau. Kết quả thí nghiệm của chúng tôi chỉ ra rằng việc thay đổi cấu hình này không có tác động tích cực đến hiệu suất của mô hình, điều này có lợi cho việc phát triển các kiến trúc kết nối dày đặc nhẹ.

Lớp Diff. gates #Param In domain Wikitext
1 % 353M 2.570 2.363
2 % 353M 2.546 2.348
2 ! 360M 2.547 2.351
4 % 353M 2.542 2.348
4 ! 374M 2.557 2.371
Bảng 7. Kết quả cross-entropy loss đánh giá trong miền và byte perplexity ngoài miền cho DenseRetNet-350M với số lượng lớp dày đặc khác nhau và chiến lược cổng khác nhau. Diff. gates chỉ việc có áp dụng cổng khác nhau cho các đặc trưng dày đặc khác nhau hay không.

Ablations trên Mô-đun Hợp nhất Ẩn Mô-đun hợp nhất ẩn để hợp nhất các trạng thái ẩn đã chuyển tiếp và các trạng thái ẩn hiện tại. Một cách phổ biến của hợp nhất đặc trưng là bằng Concat theo sau bởi giảm chiều, điều này thêm nhiều tham số hơn so với cách của chúng tôi. Bằng cách tinh chỉnh cấu trúc mô hình, chúng tôi so sánh nó ở cùng độ lớn, và Bảng 8 phát hiện rằng kết nối ẩn Dense nhẹ được đề xuất của chúng tôi đạt được kết quả tốt hơn.

Một nghiên cứu khác điều tra tác động của việc hợp nhất các đặc trưng dày đặc mỗi m lớp hoặc ở mỗi lớp riêng lẻ. Để duy trì số lượng tham số nhất quán, chúng tôi điều chỉnh các chiều của lớp trung gian MLP và huấn luyện mô hình với đầy đủ 15B token. Kết quả trong Bảng 9 chỉ ra rằng hợp nhất ở mỗi lớp tạo điều kiện chuyển giao thông tin từ lớp thấp hơn đến lớp cao hơn hiệu quả hơn.

Fusion #Param In domain Wikitext
Concat 354M 2.551 2.370
Add 353M 2.546 2.348
Bảng 8. Cross-entropy loss đánh giá trong miền và byte perplexity ngoài miền của DenseRetNet-350M với các triển khai khác nhau của mô-đun hợp nhất ẩn.

Tần suất dày đặc #Param In domain Wikitext
Mỗi lớp 353M 2.303 1.845
Mỗi 2 lớp 353M 2.331 1.866
Mỗi 4 lớp 353M 2.387 1.923
Bảng 9. Cross-entropy loss đánh giá trong miền và byte-perplexity ngoài miền cho DenseRetNet-350M với các chiến lược tần suất can thiệp dày đặc khác nhau.

5. Kết luận
Trong bài báo này, chúng tôi đề xuất một khung DenseSSM mới để tăng cường luồng thông tin ẩn qua các lớp khác nhau. Các trạng thái ẩn là các đơn vị lưu trữ thông tin quan trọng trong SSM. Sử dụng các trạng thái ẩn từ mỗi lớp hiệu quả hơn sẽ có lợi ích lớn cho các khả năng cơ bản của SSM. Do đó, chúng tôi đề xuất thu thập các trạng thái ẩn từ các lớp nông và hợp nhất có chọn lọc chúng vào các trạng thái ẩn của các lớp sâu hơn để tăng cường nhận thức của SSM về thông tin văn bản cấp thấp. Phương pháp DenseSSM được đề xuất không ảnh hưởng đến các đặc tính xuất sắc của SSM, tức là suy luận tự hồi quy hiệu quả và huấn luyện song song hiệu quả. Chúng tôi áp dụng phương pháp DenseSSM cho các kiến trúc được sử dụng rộng rãi như RetNet và Mamba, dẫn đến các kiến trúc mới với khả năng ngôn ngữ cơ bản mạnh hơn và đạt được độ chính xác cao hơn trong đánh giá tiêu chuẩn công khai.

6. Tuyên bố Tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Machine Learning. Có nhiều hệ quả xã hội tiềm tàng của công trình của chúng tôi, không có gì chúng tôi cảm thấy phải làm nổi bật cụ thể ở đây.

Tài liệu tham khảo
[Các tài liệu tham khảo giữ nguyên định dạng gốc với tên tác giả và tiêu đề bằng tiếng Anh]
