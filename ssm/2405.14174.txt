# 2405.14174.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2405.14174.pdf
# File size: 6038521 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Multi-Scale VMamba: Hierarchy in Hierarchy
Visual State Space Model
Yuheng Shi
City University of Hong Kong
yuhengshi99@gmail.comMinjing Dong
City University of Hong Kong
minjdong@cityu.edu.hkChang Xu
University of Sydney
c.xu@sydney.edu.au
Abstract
Despite the significant achievements of Vision Transformers (ViTs) in various vi-
sion tasks, they are constrained by the quadratic complexity. Recently, State Space
Models (SSMs) have garnered widespread attention due to their global receptive
field and linear complexity with respect to the input length, demonstrating sub-
stantial potential across fields including natural language processing and computer
vision. To improve the performance of SSMs in vision tasks, a multi-scan strategy
is widely adopted, which leads to significant redundancy of SSMs. For a better
trade-off between efficiency and performance, we analyze the underlying reasons
behind the success of the multi-scan strategy, where long-range dependency plays
an important role. Based on the analysis, we introduce Multi-Scale Vision Mamba
(MSVMamba) to preserve the superiority of SSMs in vision tasks with limited
parameters. It employs a multi-scale 2D scanning technique on both original
and downsampled feature maps, which not only benefits long-range dependency
learning but also reduces computational costs. Additionally, we integrate a Convo-
lutional Feed-Forward Network (ConvFFN) to address the lack of channel mixing.
Our experiments demonstrate that MSVMamba is highly competitive, with the
MSVMamba-Tiny model achieving 82.8% top-1 accuracy on ImageNet, 46.9%
box mAP, and 42.2% instance mAP with the Mask R-CNN framework, 1x training
schedule on COCO, and 47.6% mIoU with single-scale testing on ADE20K. Code
is available at https://github.com/YuHengsss/MSVMamba .
1 Introduction
In the domain of computer vision, the extraction of features plays a pivotal role in the performance
of various tasks, ranging from image classification to more complex applications like detection and
segmentation. Traditionally, Convolutional Neural Networks (CNNs) [ 26,41,19,22,34] have been
the backbone of feature extraction methodologies, prized for their linear scaling complexity and
proficiency in capturing local patterns. However, CNNs often fall short in encapsulating global context,
a limitation that becomes increasingly apparent in tasks requiring a comprehensive understanding
of the entire visual field. In contrast, Vision Transformers (ViTs) [ 6,33,51,44] have emerged
as a compelling alternative, boasting an inherent global receptive field that allows for the direct
capture of long-range dependencies within an image. Despite their advantages, ViTs are hampered
by their quadratic scaling complexity concerning the input size, which significantly constrains their
applicability to downstream tasks such as object detection and segmentation, where efficiency is
paramount. Recently, State Space Model (SSM)-based approaches [ 12,42,8] have garnered attention
for their ability to combine the best of both worlds: a global receptive field and linear scaling
complexity. Notably, Mamba [ 10] introduces a hardware-aware and input-dependent algorithm that
significantly enhances the performance and efficiency of SSMs. Inspired by Mamba’s success, a
burgeoning body of work has sought to leverage its advantages for vision tasks, pioneering efforts
such as ViM [61] and VMamba [31].
Preprint. Under review.arXiv:2405.14174v1  [cs.CV]  23 May 2024

--- PAGE 2 ---
Figure 1: Comparison on ImageNet.The S6 block, developed by Mamba [ 10], was orig-
inally designed for NLP tasks. To adapt S6 for vision
tasks, images are first divided into patches and then
flattened into a patch sequence along the scanning
path. To accommodate the non-causal nature of im-
age data, the multi-scan strategy is widely adopted
for vision tasks, such as ViM [ 61] which enhances
the sequence by summing it in both forward and re-
verse directions, and VMamba [ 31] which integrates
horizontal and vertical scans. However, unlike NLP
models, which can have up to billions of parameters,
current vision backbones always take computational
costs into consideration, i.e., the trade-off between ac-
curacy and efficiency. This constraint on model size
inherently limits the long-range modeling capabili-
ties of SSMs in vision tasks. Taking ViM-Tiny [ 61]
as an example, placing the cls token in the middle
of the sequence yields markedly better results than
positioning it at the ends. This suggests that central placement compensates for the model’s limited
ability to integrate distant information, highlighting the difficulties of handling long-range dependen-
cies in parameter-constrained vision models. We refer to this as the long-range forgetting problem.
In this work, we analyze how the multi-scan strategy in [ 61,31] helps to alleviate this problem.
Compared to the single-scan strategy, the multi-scan one allows long-range decay to manifest in
various directions within 2D images. However, the increased scanning routes bring multiples of
computations, significantly increasing redundancy and limiting efficiency. Thus, we aim to pursue a
better trade-off between the performance and efficiency of Mamba in vision tasks.
The most direct and effective method to address the long-range forgetting problem is to shorten the
sequence length, which can be achieved by downsampling the feature map. However, placing all
scanning routes on a downsampled feature map could result in the loss of fine-grained features and
the downstream task performance. Through the visualization of different scans, we show that the
decay rates could vary for different scanning routes, which motivates us to develop a hierarchical
design of multi-scan. In this work, we propose a Multi-Scale 2D (MS2D) scanning strategy to
alleviate the long-range forgetting problem with limited computational costs. Specifically, we divide
the scanning directions of SS2D [ 31] into two groups: one retains the original resolution and is
processed by the S6 block, while the others are downsampled, processed by the S6 block, and then
upsampled, which not only shortens the sequence length for long-range dependencies learning but
also alleviates redundancy. Building on the VMamba with its hierarchical architecture, we incorporate
another hierarchical design within the block, creating a hierarchy within a hierarchy. Furthermore,
we introduce a Convolutional Feed-Forward Network (ConvFFN) within each block to bolster the
model’s capability for channel-wise information exchange and local feature capture.
We conduct extensive experiments to validate the effectiveness of MSVmamba across a spectrum
of tasks, including image classification, object detection, and semantic segmentation. Detailed
comparisons on the ImageNet-1K [ 3] dataset are illustrated in Fig. 1. Specifically, MSVMamba-Tiny
achieves a notable improvement over VMamba-Tiny, enhancing the Top-1 accuracy by 0.6% while
concurrently reducing the computational demand by 17% in terms of GFLOPs. Furthermore, our
MSVMamba model also surpasses SOTA SSM-based models in performance on the COCO [ 29] and
ADE20K [59] datasets for object detection and semantic segmentation tasks, respectively.
2 Related work
2.1 Generic Vision Backbone
The evolution of generic vision backbones has significantly shaped the landscape of computer vision,
transitioning from CNNs [ 26,41,19,55,22,20,43,34] to ViTs [ 6,33,32,51,52,5,44,58]. CNNs
have been the cornerstone of vision-based models, dominating vision tasks in the early era of deep
learning. The classic CNNs such as AlexNet [ 26], VGG [ 41], and ResNet [ 19], have paved the way
for numerous subsequent innovations [ 55,22,20,43,21,34,4,50]. These designs have significantly
improved performance on a wide range of vision tasks by enhancing the network’s ability to capture
2

--- PAGE 3 ---
complex patterns and features from visual data. The Vision Transformer [ 6], drawing inspiration
from the success of transformers [ 48] in natural language processing, has emerged as a formidable
contender to conventional CNNs for vision-related tasks. ViT reimagines image processing by
segmenting an image into patches and employing self-attention mechanisms to process these segments.
This innovative approach enables the model to discern global dependencies across the entire image, a
significant leap forward in understanding complex visual data. However, the ViT architecture demands
considerable computational resources and extensive datasets for effective training. Moreover, its
performance is intricately tied to the input sequence length, exhibiting a quadratic complexity that can
escalate processing costs. In response, subsequent research has focused on developing more efficient
training strategies [ 44,46,25], hierarchical network structures [ 33,32,5,51,52,16], refined spatial
attention mechanisms [ 58,60,53,47,17,5] and convolution-based design [ 2,30,15,37] to address
these issues.
2.2 State Space Models
State Space Models (SSMs) [ 11,13,12,42,8] have garnered increasing attention from researchers
due to their computational complexity, which grows linearly with the length of the input sequence,
and their inherent global awareness properties. To reduce the computational resource consumption of
SSMs, S4 [ 12] introduced a diagonal structure and combined it with a diagonal plus low-rank approach
to construct structured SSMs. Subsequently, S5 [ 42] and H3 [ 8] further enhanced the efficiency of
SSM-based models by introducing parallel scanning techniques and improving hardware utilization.
Mamba [ 10] then introduced the S6 block, incorporating data-dependent parameters to amend
the Linear Time Invariant(LTI) characteristics of previous SSM models, demonstrating superior
performance over transformers on large-scale datasets. In the realm of vision tasks, S4ND [ 36]
pioneered the application of SSM models in vision tasks by treating visual data as 1D, 2D, and 3D
signals. U-Mamba [ 35] combined CNNs with SSMs for medical image segmentation. ViM [ 61] and
VMamba [ 31] integrated the S6 block into vision backbone design, employing multiple scanning
directions to accommodate the non-casual nature of image data, achieving competitive results against
ViTs and CNNs. Motivated by the success of ViM and VMamba, a plethora of Mamba-based
works [ 38,24,7,56,1,27,57,40] have emerged across various vision tasks, including vision
backbone design [ 38,24,56], medical image segmentation [ 40,28], and video classification [ 27],
showcasing the potential of SSM-based approaches in advancing the field of computer vision.
3 Method
In this section, we first summarize the state space model in Section 3.1. Subsequently, in Section 3.2,
we provide an in-depth analysis of the multi-scan strategy in existing vision Mamba models. Following
the analysis, Section 3.3 tackles the redundancy and long-range dependency issue by introducing
a Multi-Scale 2D (MS2D) scanning strategy. Finally, Section 3.4 details the integration of the
Multi-Scale State Space (MS3) block, which incorporates the MS2D technique alongside a ConvFFN.
Building upon the MS3 block, various model configurations are developed across different scales,
illustrating the adaptability and scalability of our proposed approach.
3.1 Preliminaries
State Space Models. Classical State Space Models (SSMs) represent a continuous system that maps
an input sequence x(t)∈RLto a latent space representation h(t)∈RNand subsequently predicts an
output sequence y(t)∈RLbased on this representation. Mathematically, an SSM can be described
as follows:
h′(t) =Ah(t) +Bx(t), y(t) =Ch(t), (1)
where A∈RN×N,B∈RN×1andC∈R1×Nare learnable parameters.
Discretization. To adapt continuous State Space Models (SSMs) for use within deep learning
frameworks, it is crucial to implement discretization operations. By incorporating a timescale
parameter ∆∈Rand employing the widely utilized zero-order hold (ZOH) as the discretization rule,
the discretized versions of AandB(denoted as AandB, respectively) can be derived, with which,
3

--- PAGE 4 ---
(a) Horizontal Scan
 (b) Vertical Scan
 (c) Decay Ratio
 (d) Binary Decay Ratio
Figure 2: Illustration of decay along horizontal, vertical scanning routes and their ratio.
Eq. 1 can be reformulated into a discretized manner as:
h(t) =Ah(t−1) +Bx(t), y(t) =Ch(t),
where A=e∆A,B= (∆A)−1(e∆A−I)∆B≈∆B,(2)
whereIdenotes the identity matrix. Afterward, the process of Eq. 2 could be implemented in a global
convolution manner as:
y=x⊙K,K=
CB,CAB, . . . ,CAL−1B
, (3)
where K∈RLis the convolution kernel.
Selective State Space Models. The Selective State Space (S6) mechanism, introduced by
Mamba [ 10], renders the parameters B,C, and∆input-dependent, thereby enhancing the per-
formance of SSM-based models. After making B,C, and∆input-dependent, the global convolution
kernel in Eq. 3 could be rewritten as:
K= 
CLBL,CLAL−1BL−1, . . . ,CLL−1Y
i=1AiB1!
. (4)
3.2 Analysis of Multi-Scan Strategy
When processing image data using the S6 block, the 2D feature map Z∈RH×W×Dis flattened
into a 1D sequence of image tokens, denoted as X∈RL×D. According to Eq. 4 and Eq. 2, the
contribution of the mthtoken to the construction of the nthtoken ( m < n ) in S6 can be expressed as:
CnnY
i=mAiBm=CnA(m→n)Bm,where A(m→n)=ePn
i=m∆iA. (5)
Typically, the learned ∆iAis negative, which biases the model towards prioritizing recent tokens’
information. Consequently, as the sequence length increases, the exponential term ePn
i=m∆iAin
Eq. 5 decays significantly, resulting in minimal contributions from distant tokens. We refer to it as
the long-range forgetting issue, which has also been observed in recent studies applying S6 to vision
tasks [ 14]. Although this problem can be mitigated by increasing the number of parameters and the
depth of the model, such adjustments introduce additional computational costs. Furthermore, the
causal property of the S6 block ensures that information can only propagate in a unidirectional manner
between tokens, preventing earlier tokens from accessing information from subsequent tokens.
The inherent non-causal nature of images renders the direct application of the S6 block to vision-
related tasks less than optimal, as identified by ViM [ 61]. To mitigate this limitation, ViM [ 61] and
VMamba [ 31] have introduced methodologies that entail scanning image features across various
directions and then integrating these features. Generally, the updated token along one of the scanning
routes, denoted as Scan (Z(p,q)), where (p, q)indicates the coordinate, could be obtained by:
Scan (Z(p,q)) =CαPα
i=1A(i→α)Biσ(Z)i.(6)
In Eq. 6, σrepresents the transformation that converts a 2D feature map into a 1D sequence, and α
denotes the corresponding index of Z(p, q)in the transformed 1D sequence. Afterward, results from
multi-scan routes are added together to produce enhanced feature Z′(p, q), which can be denoted as:
Z′
(p,q)=X
kScan k(Z(p,q)) =X
kCαkαkX
i=1A(i→αk)Biσk(Z)i. (7)
4

--- PAGE 5 ---
K=7, S=2 
DW ConvK=3, S=1 
DW Conv
S6
… S6Interpolate
…
…
…
…
Figure 3: Illustration of the Multi-Scale 2D-Selective-Scan on an image
This multi-scan strategy allows tokens to access information from each other. In ViM [ 61], two
distinct scanning routes correspond to two different transformations in Eq. 6, specifically, the flatten
and the flatten with flip transformations. Similarly, VMamba [ 31] extends the basic bidirectional
scanning by incorporating both horizontal and vertical scanning directions, yielding four distinct
scanning routes. Besides, the multi-scan strategy also alleviates the long-range forgetting problem by
minimizing the effective distance between tokens. For tokens at coordinates (p1, q1)and(p2, q2),
the strategy employs multiple scanning routes, each potentially altering their relative positions. The
minimum distance across these routes is given by minkdk((p1, q1)→(p2, q2)), where dkrepresents
the distance between the tokens in the sequence generated by the k-th scan. By reducing this distance,
the multi-scan strategy reduces the decay of influence between distant tokens, thereby enhancing the
model’s ability to maintain and utilize long-range information.
To more intuitively demonstrate the relationship between the multi-scan strategy and long-range
decay, we visualize the exponential term ePα
i=1∆iAalong the horizontal and vertical scanning
directions in VMamba-Tiny with respect to the central token in Fig. 2a and Fig. 2b. Specifically,
we randomly select 50 images from the ImageNet [ 3] validation set and compute the average decay
along scanning routes at the last layer of the final stage across these images and feature dimensions.
We use a higher input resolution to enhance the quality of the visualization.
According to these observations, the success of the multi-scan strategy in VMamba can be attributed
to its mitigation of the non-causal properties of image data and alleviation of the long-range forgetting
problem. However, as the number of scanning routes increases, the computational cost also rises
linearly, introducing computational redundancy. In Fig. 2c, we illustrate the maximum ratio of Fig. 2a
to Fig. 2b and vice versa. While in Fig. 2d, we present a binarized version of Fig. 2c, applying
a threshold of 10, which covers more than 40% of the entire figure. This phenomenon indicates
that the varying decay rates across different scanning routes lead to certain routes dominating the
decay dynamics, which can also be attributed to the long-range forgetting problem. The existence
of dominant scanning routes implies that some scans contribute significantly more to information
retention than others, leading to computational redundancy in the multi-scan strategy.
3.3 Multi-Scale 2D Scanning
As discussed in the last subsection, the contribution of tokens decays with increasing scanning
distance. The most effective and direct way to alleviate the long-range forgetting problem is to
reduce the number of tokens. Simultaneously, since the computational complexity of the S6 block
is linearly dependent on the number of tokens, reducing the token count also enhances efficiency.
Thus, an alternative approach to address the aforementioned issue is to apply the multi-scan strategy
on a downsampled feature map. However, setting all scans on a downsampled feature map will
ignore fine-grained features and result in unavoidable information loss. Thus, scanning along the
full-resolution feature map is also essential.
Motivated by these considerations, we introduce a simple yet effective Multi-Scale 2D scan-
ning(MS2D) strategy, as depicted in Fig. 3. Our approach commences with the generation of
hierarchical feature maps at varying scales, achieved through the application of Depthwise Convolu-
tion (DWConv) with distinct stride values. These multi-scale feature maps are then processed through
four distinct scanning routes within VMamba. Specifically, we uitlize DWConvs with strides of 1
andsto obtain feature map Z1∈RH×W×DandZ2∈RH
s×W
s×D, respectively. Afterwards, Z1and
5

--- PAGE 6 ---
GELU
LNLNLN
LinearLinear
DWConvDWConv
LinearMS2D1x1 Conv1x1 Conv
SEMulti -Scale Vision Space State ConvFFNFigure 5: Detailed architecture of Multi-Scale State Space (MS3) block, consisting of a Multi-Scale
Vision Space State (MSVSS) block and a Convolutional Feed-Forward Network (ConvFFN) block.
Z2are processed by two S6 blocks as:
Y1=S6(σ1(Z1)), (8)
[Y2,Y3,Y4] =S6([σ2(Z2), σ3(Z2), σ4(Z2)]), (9)
where σis transformation that convert 2D feature maps into 1D sequences used in SS2D, and Yis
the processed sequence. These processed sequences are converted back into 2D feature maps, and the
downsampled feature maps are interpolated for merging:
Z′
i=γi(Yi), i∈ {1,2,3,4}, (10)
Z′=Z′
1+Interpolate (X
(Z′
j)), j∈ {2,3,4}, (11)
where γis the inverse transformation of σandZ′is the feature map enhanced by MS2D.
(a) SS2D
 (b) MS2D
Figure 4: Illustration of the decay with different scanning
routes in SS2D and MS2D.The downsampling operation reduces
the sequence length by a factor of s2,
which also shortens the distance be-
tween tokens in Eq. 5 by a factor of
s2, thereby alleviating the long-range
forgetting problem. As the computa-
tional complexity of a single S6 block
isO(9LDN )[10], where Ndenotes
the SSM dimension, replacing SS2D
with MS2D reduces the total sequence
length across four scans from 4Lto
(1+3 /s2)L, thereby improves the effi-
ciency. Practically, the downsampling
rate is set to 2. It is worth noting that
sequences from Z2are processed by
the same S6 block. This approach
maintains the same accuracy as using multiple S6 blocks for different scanning routes while ef-
fectively reducing the number of parameters.
To better illustrate the alleviation of the long-range forgetting problem, we also provide empirical
evidence, as shown un Fig. 4. We compare the decay along scanning routes in the SS2D of VMamba
and our MS2D, focusing on the last token with the same configuration as Fig. 2. The decay maps in
downsampled features are interpolated back. As observed, the decay rate along scanning routes in
downsampled maps is significantly alleviated, enhancing the capability to capture global information.
3.4 Overall Model Architecture
In this study, we extend the capabilities of the VMamba framework by substituting its VSS block with
our Multi-Scale State Space (MS3) block. The architectural framework of the MS3 block is delineated
in Fig. 5, comprising a Multi-Scale Vision Space State(MSVSS) component and a Convolutional
Feed-Forward Network (ConvFFN). The MSVSS component is devised by adapting the vision
state space framework in VMamba, substituting the SS2D with an MS2D to further introduce a
hierarchy design in a single layer. Additionally, a Squeeze-Excitation (SE) [ 21] block is integrated
subsequent to the multi-scale 2D scanning, as informed by related literature [ 24,38]. Diverging
from the conventional focus on token mixing in prior vision Mamba architectures [ 61,31,24], our
6

--- PAGE 7 ---
design introduces a channel mixer to augment the flow of information across different channels,
aligning with the structural paradigms of typical vision transformers. In concordance with preceding
studies [ 53,49,15,52], the ConvFFN which consists of a depth-wise convolution and two fully
connected layers is employed as the channel mixer. To ensure that the ConvFFN and MSVSS
block consume approximately the same FLOPs following LeViT [ 9], we adopt a modest channel
expansion ratio of 2. Upon the amalgamation of MSVSS and ConvFFN within the MS3 block,
meticulous adjustments are made to the number of blocks to ensure a comparable computational
budget, facilitating a fair comparison.
To empirically validate the efficacy of our proposed modifications, we introduce model variants
across different scales, the specifications of which are detailed in our appendix 6. These variants,
namely Nano, Micro, and Tiny, are characterized by their parameter counts of 6.9M, 11.9M, and
33.0M, respectively. In terms of computational expenditure, these models require 0.9, 1.5, and 4.6
GFLOPs correspondingly, demonstrating a scalable approach to model design that accommodates
varying computational constraints.
4 Experimental Validation
4.1 ImageNet Classification
Table 1: Accuracy comparison across various mod-
els on ImageNet-1K.
Method #param. FLOPsTop-1
Acc(%).
RegNetY-800M [39] 6M 0.8G 76.3
RegNetY-1.6G [39] 11M 1.6G 78.0
RegNetY-4G [39] 21M 4.0G 80.0
DeiT-S [44] 22M 4.6G 79.8
DeiT-B [44] 86M 17.5G 81.8
Swin-T [33] 29M 4.5G 81.3
Swin-S [33] 50M 8.7G 83.0
Swin-B [33] 88M 15.4G 83.5
ViM-T [61] 7M 1.5G 76.1
ViM-S [61] 26M 5.1G 80.5
VMamba-T [31] 22M 5.6G 82.2
VMamba-B [31] 44M 11.2G 83.5
LocalVMamba-T [24] 26M 5.7G 82.7
LocalVMamba-S [24] 50M 11.4G 83.7
EffVMamba-T [38] 6M 0.8G 76.5
EffVMamba-S [38] 11M 1.3G 78.7
EffVMamba-B [38] 33M 4.0G 81.8
MSVMamba-N 7M 0.9G 77.3
MSVMamba-M 12M 1.5G 79.8
MSVMamba-T 33M 4.6G 82.8Settings. Our models are trained and tested
on the ImageNet-1K dataset [ 3]. In alignment
with previous works [ 33,31,24], all models un-
dergo training for 300 epochs, with the initial
20 epochs dedicated to warming up. The train-
ing utilizes a batch size of 1024 across 8 GPUs.
We employ the AdamW optimizer, setting the
betas to (0.9, 0.999) and momentum to 0.9. The
learning rate is managed through a cosine decay
scheduler, starting from an initial rate of 0.001,
coupled with a weight decay of 0.05. Addition-
ally, we leverage the exponential moving aver-
age (EMA) and implement label smoothing with
a factor of 0.1 to enhance model performance
and generalization. During testing, images are
center cropped with the size of 224 ×224. When
dealing with the routes for multi-scale scanning,
we empirically select top-left to the bottom-right
for dealing with the full-resolution feature map
while the other three scans are responsible for
scanning the downsampled feature map.
Results. Tab. 1 showcases our MSV-
Mamba models against established CNNs,
ViTs, and SSM-based models on ImageNet-1K.
MSVMamba-N, with 7M parameters and 0.9G
FLOPs, achieves 77.3% top-1 accuracy, out-
performing similar-cost Conv-based RegNetY-
800M and SSM-based EfficientVMamba-T.
MSVMamba-M, with 12M parameters and 1.5G
FLOPs, reaches 79.8% accuracy, surpassing ViM-T by 3.7%. The MSVMamba-T model attains
82.8% accuracy with 33M parameters and 4.6G FLOPs, exceeding VMamba-T by 0.6% with much
less computational cost. These results highlight MSVMamba’s efficiency and scalability, offering a
robust option for high-accuracy, resource-efficient model design.
4.2 Object Detection
Setup. We evaluate our MSVMamba on the MSCOCO [ 29] dataset using the Mask R-CNN [ 18]
framework for object detection and instance segmentation tasks. Following previous works [ 33,31],
7

--- PAGE 8 ---
Table 2: Object detection and instance segmentation with Mask R-CNN on COCO. The FLOPs are
computed for an input size of 1280×800. Multi-scale training is exclusively implemented in the 3 ×
schedule. All backbones are pre-trained on the ImageNet-1K dataset.
BackboneMask R-CNN 1x Schedule Mask R-CNN 3x Schedule#param. FLOPsAPbAPb
50APb
75APmAPm
50APm
75APbAPb
50APb
75APmAPm
50APm
75
PVT-T [51] 36.7 59.2 39.3 35.1 56.7 37.3 39.8 62.2 43.0 37.4 59.3 39.9 33M 208G
LightViT-T [23] 37.8 60.7 40.4 35.9 57.8 38.0 41.5 64.4 45.1 38.4 61.2 40.8 28M 187G
EffVMamba-S [38] 39.3 61.8 42.8 36.7 58.9 39.2 41.6 63.9 45.6 38.2 60.8 40.7 31M 197G
MSVMamba-M 43.8 65.8 47.7 39.9 62.9 42.9 46.3 68.1 50.8 41.8 65.1 44.9 32M 201G
Swin-T [33] 42.7 65.2 46.8 39.3 62.2 42.2 46.0 68.1 50.3 41.6 65.1 44.9 48M 267G
ConvNeXt-T [34] 44.2 66.6 48.3 40.1 63.3 42.8 46.2 67.9 50.8 41.7 65.0 44.9 48M 262G
VMamba-T [31] 46.5 68.5 50.7 42.1 65.5 45.3 48.5 69.9 52.9 43.2 66.8 46.3 42M 286G
LocalVMamba-T [24] 46.7 68.7 50.8 42.2 65.7 45.5 48.7 70.1 53.0 43.4 67.0 46.4 45M 291G
EffVMamba-B [38] 43.7 66.2 47.9 40.2 63.3 42.9 45.0 66.9 49.2 40.8 64.1 43.7 53M 252G
MSVMamba-T 46.9 68.8 51.4 42.2 65.6 45.4 48.3 69.5 53.0 43.2 66.8 46.9 53M 252G
we utilize backbones pretrained on ImageNet-1K for initialization. We employ standard training
strategies of 1 ×(12 epochs) and 3 ×(36 epochs) with Multi-Scale (MS) training for a fair comparison.
Results. Tab. 2 presents a performance comparison of our method against CNNs, ViTs, and SSM-
based models. Our model consistently outperforms others across various variants and training settings.
Specifically, MSVMamba-T outperforms Swin-T by +4.2 box AP and +2.9 mask AP under the 1 ×
schedule and also shows improvements in both box AP and mask AP under the 3 ×schedule.
4.3 Semantic SegmentationTable 3: We present the results of semantic seg-
mentation on the ADE20K dataset [ 59] using the
UperNet framework [ 54]. The computational com-
plexity, measured in FLOPs, is calculated for input
dimensions of 512×2048 . The abbreviations "SS"
and "MS" refer to single-scale and multi-scale test-
ing, respectively.
MethodmIoU
SSmIoU
MS#param. FLOPs
ResNet-50 [19] 42.1 42.8 67M 953G
DeiT-S+MLN [45] 43.8 45.1 58M 1217G
Swin-T [33] 44.4 45.8 60M 945G
ConvNeXt-T [34] 46.0 46.7 60M 939G
VMamba-T [31] 47.3 48.3 55M 964G
LocalVMamba-T [24] 47.9 49.1 57M 970G
EffVMamba-S [38] 41.5 42.1 29M 505G
EffVMamba-B [38] 46.5 47.3 65M 930G
MSVMamba-M 45.1 45.4 42M 875G
MSVMamba-T 47.6 48.5 65M 942GSetup. Consistent with the methodologies used
in Swin [ 33] and VMamba [ 31], we utilize the
UperHead [ 54] framework atop an ImageNet
pre-trained MSVMamba backbone. The train-
ing process is conducted over 160K iterations
with a batch size of 16. We employ the AdamW
optimizer with a learning rate set at 6×10−5.
Our experiments are primarily conducted using
a default input resolution of 512×512. Addi-
tionally, we also incorporate Multi-Scale (MS)
testing to assess performance variations.
Results. We present the detailed results of our
model and other competitors in Tab. 3, which in-
cludes both single-scale and multi-scale testing.
Our MSVMamba consistently outperforms the
Swin, ConNeXt, and VMamba models in the
tiny variant by margins of +2.2, +1.6, and +0.3
mIoU, respectively.
4.4 Ablation Study
To validate the effectiveness of the proposed modules, we conducted a comprehensive ablation study.
Specifically, we scaled the VMamba-Tiny model by setting its embedding dimension dto 48, the
state space dimension Nto 8, and the number of blocks in the four different stages to [1,2,4,2]. The
scaled model, referred to as VMamba-Nano, has parameters and computational costs of 4.4M and
0.87GFLOPs, respectively. This model serves as the baseline for our ablation experiments. Models in
ablation study are conducted over a training schedule of 100 epochs, except where noted otherwise,
to reduce training time.
On Multi-Scale 2D Scan. After replacing the SS2D in VMamba with our MS2D, while maintain-
ing a roughly equivalent FLOP count, the accuracy increased from 69.6 %to 71.9 %, as demon-
strated in the Tab. 4. Furthermore, we conducted an ablation on the number of scans in the
8

--- PAGE 9 ---
Table 4: Evolutionary trajectory from VMamba to MSVMamba. When integrating the ConvFFN and
setting the state space dimension N= 1, we meticulously calibrate the quantity of blocks to maintain
a nearly constant computational cost, measured in GFLOPs.
Model MS2D SE ConvFFN N= 1 #param. FLOPsTop-1
Acc(%).
VMamba-Nano 4.4M 0.87G 69.6
MSVMamba-Nano✓ 4.8M 0.89G 71.9↑2.3
✓ ✓ 5.3M 0.89G 72.4↑2.8
✓ ✓ ✓ 6.6M 0.94G 74.4↑4.8
✓ ✓ ✓ ✓ 6.9M 0.88G 75.1↑5.5
multi-scale scan, considering both full-resolution and half-resolution branches. The results are
shown in Tab. 5. Placing all scans in the half-resolution branch led to a significant loss of fine-
grained features, resulting in a substantial decrease in model accuracy. Positioning two or three
scans in the full resolution branch, compared to just one, resulted in accuracy improvements of
0.1%and 0.6 %, respectively, but introduced an additional computational cost of approximately
12%and 25 %. Allocating four scans to the full resolution branch, effectively reverting to the
SS2D method, increased the computational cost by 34 %while only improving accuracy by 0.4 %.
Table 5: Ablation study on Multi-Scale 2D Scan.
Full Half #param FLOPsTop-1
Acc(%).
0 4 5.1M 0.74G 63.1
1 3 4.8M 0.89G 71.9
2 2 5.0M 1.00G 72.0
3 1 5.3M 1.11G 72.5
4 0 5.1M 1.19G 72.3For an optimal trade-off between computational
cost and accuracy, we select one scan in the full-
resolution branch as the default setting. Build-
ing upon the MS2D foundation, we introduce
an SE block following EfficientVMamba [ 38],
which further enhanced accuracy by 0.5 %with
minimal additional computational cost.
On ConvFFN. Upon replacing SS2D with
MS2D and incorporating a SE block, we con-
structed a model that utilizes ConvFFN as a
channel mixer. When only using SSM, the
model exhibited insufficient information ex-
change between channels. The integration of ConvFFN as a channel mixer significantly enhanced the
model’s capability for inter-channel information interaction. As indicated in Tab. 4, the addition of
ConvFFN resulted in an additional accuracy improvement of 2.0 %. To maintain a roughly equivalent
computational cost, we adjusted the number of blocks within the model. Besides, we set the state
space dimension N= 1and stacked one more block to further enhance the capability of capturing
long-range information while maintaining a roughly constant computational cost. This operation
resulted in an additional accuracy improvement of 0.7 %, as shown in Tab. 4.
5 Limitations
The design of multi-scale VMamba aims to tackle the long-range forgetting problem of Mamba
models with limited parameters on vision tasks. Although the proposed model has proven to be
effective, its scalability remains to be explored since this issue can also be alleviated by increasing
model sizes. In such cases, the multi-scale design may have only a marginal improvement.
6 Conclusion
In this paper, we introduced Multi-Scale VMamba (MSVMamba), an SSM-based vision backbone
that leverages the advantages of linear complexity and global receptive field. We developed the
Multi-Scale 2D (MS2D) scanning technique to minimize computational redundancy and alleviate the
long-range forgetting problem in parameter-limited vision models. Additionally, we incorporated
the Convolutional Feed-Forward Network (ConvFFN) to enhance the exchange of information
between channels, thereby significantly improving the performance of our model. Our experiments
demonstrate that MSVMamba consistently outperforms popular models from various architectures,
including ConvNeXt, Swin Transformer, and VMamba, in image classification and downstream tasks.
9

--- PAGE 10 ---
References
[1]Tianxiang Chen, Zhentao Tan, Tao Gong, Qi Chu, Yue Wu, Bin Liu, Jieping Ye, and Nenghai
Yu. Mim-istd: Mamba-in-mamba for efficient infrared small target detection. arXiv preprint
arXiv:2403.02148 , 2024.
[2]Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and
attention for all data sizes. NeurIPS , 2021.
[3]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR , 2009.
[4]Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, and Jian Sun.
Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In CVPR , 2022.
[5]Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong
Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with
cross-shaped windows. In CVPR , 2022.
[6]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR , 2020.
[7]Chengbin Du, Yanxi Li, and Chang Xu. Understanding robustness of visual state space models
for image classification. arXiv preprint arXiv:2403.10935 , 2024.
[8]Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Ré.
Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint
arXiv:2212.14052 , 2022.
[9]Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé
Jégou, and Matthijs Douze. Levit: a vision transformer in convnet’s clothing for faster inference.
InICCV , 2021.
[10] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
arXiv preprint arXiv:2312.00752 , 2023.
[11] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory
with optimal polynomial projections. NeurIPS , 2020.
[12] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured
state spaces. arXiv preprint arXiv:2111.00396 , 2021.
[13] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré.
Combining recurrent, convolutional, and continuous-time models with linear state space layers.
NeurIPS , 2021.
[14] Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang, Xudong Ren, and Shu-Tao Xia. Mambair: A
simple baseline for image restoration with state-space model. arXiv preprint arXiv:2402.15648 ,
2024.
[15] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu.
Cmt: Convolutional neural networks meet vision transformers. In CVPR , pages 12175–12185,
2022.
[16] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in
transformer. In NeurIPS , 2021.
[17] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention
transformer. In CVPR , 2023.
[18] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV , 2017.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR , 2016.
10

--- PAGE 11 ---
[20] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias
Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural
networks for mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.
[21] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR , 2018.
[22] Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger.
Convolutional networks with dense connectivity. IEEE TPAMI , 2019.
[23] Tao Huang, Lang Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Lightvit: Towards
light-weight convolution-free vision transformers. arXiv preprint arXiv:2207.05557 , 2022.
[24] Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. Localmamba:
Visual state space model with windowed selective scan. arXiv preprint arXiv:2403.09338 , 2024.
[25] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng.
Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on
imagenet. arXiv preprint arXiv:2104.10858 , 2021.
[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. NeurIPS , 2012.
[27] Kunchang Li, Xinhao Li, Yi Wang, Yinan He, Yali Wang, Limin Wang, and Yu Qiao. Video-
mamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977 ,
2024.
[28] Weibin Liao, Yinghao Zhu, Xinyuan Wang, Cehngwei Pan, Yasha Wang, and Liantao Ma.
Lightm-unet: Mamba assists in lightweight unet for medical image segmentation. arXiv
preprint arXiv:2403.05246 , 2024.
[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,
Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV ,
2014.
[30] Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, and Lianwen Jin. Scale-aware modulation
meet transformer. In ICCV , 2023.
[31] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and
Yunfan Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 , 2024.
[32] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng
Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR ,
2022.
[33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , 2021.
[34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
Xie. A convnet for the 2020s. In CVPR , 2022.
[35] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-range dependency for biomedical
image segmentation. arXiv preprint arXiv:2401.04722 , 2024.
[36] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus,
and Christopher Ré. S4nd: Modeling images and videos as multidimensional signals with state
spaces. NeurIPS , 2022.
[37] Xuran Pan, Chunjiang Ge, Rui Lu, Shiji Song, Guanfu Chen, Zeyi Huang, and Gao Huang. On
the integration of self-attention and convolution. In CVPR , 2022.
[38] Xiaohuan Pei, Tao Huang, and Chang Xu. Efficientvmamba: Atrous selective scan for light
weight visual mamba. arXiv preprint arXiv:2403.09977 , 2024.
[39] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Design-
ing network design spaces. In CVPR , 2020.
11

--- PAGE 12 ---
[40] Jiacheng Ruan and Suncheng Xiang. Vm-unet: Vision mamba unet for medical image segmen-
tation. arXiv preprint arXiv:2402.02491 , 2024.
[41] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 , 2014.
[42] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers
for sequence modeling. arXiv preprint arXiv:2208.04933 , 2022.
[43] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. In ICML , 2019.
[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efficient image transformers & distillation through attention. In
ICML , 2021.
[45] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. In ECCV , 2022.
[46] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou.
Going deeper with image transformers. In ICCV , 2021.
[47] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and
Yinxiao Li. Maxvit: Multi-axis vision transformer. In ECCV , 2022.
[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS , 2017.
[49] A Wang, H Chen, Z Lin, H Pu, and G Ding. Repvit: Revisiting mobile cnn from vit perspective.
arxiv 2023. arXiv preprint arXiv:2307.09283 , 2023.
[50] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu,
Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation
models with deformable convolutions. arXiv preprint arXiv:2211.05778 , 2022.
[51] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction
without convolutions. In ICCV , 2021.
[52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,
Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer.
Computational Visual Media , 2022.
[53] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Dat++: Spatially dynamic
vision transformer with deformable attention. arXiv preprint arXiv:2309.01430 , 2023.
[54] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing
for scene understanding. In ECCV , 2018.
[55] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In CVPR , 2017.
[56] Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu,
and Elliot J Crowley. Plainmamba: Improving non-hierarchical mamba in visual recognition.
arXiv preprint arXiv:2403.17695 , 2024.
[57] Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang, and Yanfeng Wang. Re-
mamber: Referring image segmentation with mamba twister. arXiv preprint arXiv:2403.17839 ,
2024.
[58] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and
Shuicheng Yan. Metaformer is actually what you need for vision. In CVPR , 2022.
[59] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Scene parsing through ade20k dataset. In CVPR , 2017.
12

--- PAGE 13 ---
[60] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and Rynson Lau. Biformer: Vision
transformer with bi-level routing attention. In CVPR , 2023.
[61] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang.
Vision mamba: Efficient visual representation learning with bidirectional state space model.
arXiv preprint arXiv:2401.09417 , 2024.
13

--- PAGE 14 ---
Appendix
Throughput Comparison
We present the throughput comparison of our MSVMamba variants, and the baseline VMamba-Tiny
on the ImageNet-1K dataset in Tab. 6. The throughput is measured in images per second, processed
on a NVIDIA GeForce RTX 2080 Ti GPU. The image size for all tests is 224×224, and the batch size
is set to 128. Under similar GFLOPs, our MSVMamba-Tiny model nearly doubles the throughput of
the baseline VMamba-Tiny.
Table 6: Throughput comparison on ImageNet-1K.
Model Throughput (img/sec)
VMamba-Tiny 218.86
MSVMamba-Nano 1165.71
MSVMamba-Micro 887.14
MSVMamba-Tiny 421.80
Qualitative Analysis
We provide visualizations in Fig. 6 comparing the proposed MS2D and SS2D configurations in
VMamba. These visualizations are generated by converting the S6 layer into an attention format,
as demonstrated by VMamba [ 31]. The results clearly show that the full-resolution in MS2D scan
captures more detailed features, whereas the scans at half resolution primarily focus on broader
architectural details, compared to SS2D. The proposed hierarchical scanning pattern facilitates the
current layer’s ability to discern and amalgamate features across various levels of abstraction.
Scan 1 Scan 2 Scan 3 Scan 4SS2D
MS2D
Figure 6: Attention maps from four distinct scanning directions, generated by SS2D and our MS2D
in the last layer of the second stage. In the second row, full-resolution scan (first scan) captures
fine-grained features, whereas scans at half resolution capture coarse-grained features. Maps are
rendered at a higher resolution to enhance visualization quality.
Network Architecture
In Tab. 7, we present the detailed architecture of our model variants, including the Nano, Micro, and
Tiny versions, each with varying channels and block numbers.
14

--- PAGE 15 ---
Table 7: Architectural overview of the MSVMamba series.
layer name output size Nano Micro Tiny
stem 56×56 conv 4×4, 48, stride 4 conv 4×4, 64, stride 4 conv 4×4, 96, stride 4
stage 1 28×28MS3×1
conv 2×2, 96, stride 2MS3×1
conv 2×2, 128, stride 2MS3×1
conv 2×2, 192, stride 2
stage 2 14×14MS3×2
conv 2×2, 192, stride 2MS3×2
conv 2×2, 256, stride 2MS3×2
conv 2×2, 384, stride 2
stage 3 7×7MS3×5
conv 2×2, 384, stride 2MS3×5
conv 2×2, 512, stride 2MS3×9
conv 2×2, 768, stride 2
stage 4 7×7 MS3×2 MS3×2 MS3×2
1×1 average pool, 1000-d fc, softmax
Param. (M) 6.9 11.9 33.0
FLOPs 0.9×1091.5×1094.6×109
15
