# 2406.07522.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2406.07522.pdf
# Kích thước tệp: 1737433 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được công bố như một bài báo hội nghị tại ICLR 2025
SAMBA: CÁC MÔ HÌNH KHÔNG GIAN TRẠNG THÁI LAI ĐƠN GIẢN
CHO VIỆC MÔ HÌNH HÓA NGÔN NGỮ NGỮ CẢNH VÔ HẠN HIỆU QUẢ

Liliang Ren1,2*Yang Liu1†Yadong Lu1†Yelong Shen1Chen Liang1Weizhu Chen1
1Microsoft2University of Illinois at Urbana-Champaign
{liliangren,yaliu10,yadonglu,yelong.shen,chenliang1,wzchen}@microsoft.com

TÓM TẮT
Việc mô hình hóa hiệu quả các chuỗi với độ dài ngữ cảnh vô hạn từ lâu đã là một vấn đề đầy thách thức. Các phương pháp trước đây đều gặp phải độ phức tạp tính toán bậc hai hoặc khả năng ngoại suy hạn chế trong việc tổng quát hóa độ dài. Trong nghiên cứu này, chúng tôi trình bày SAMBA, một kiến trúc lai đơn giản kết hợp Mamba theo từng lớp, một Mô hình Không gian Trạng thái (SSM) có tính chọn lọc, với Sliding Window Attention (SWA). SAMBA có tính chọn lọc trong việc nén một chuỗi cho trước thành các trạng thái ẩn tái phát trong khi vẫn duy trì khả năng nhớ lại chính xác các ký ức gần đây thông qua cơ chế attention. Chúng tôi mở rộng quy mô SAMBA lên đến 3.8B tham số với 3.2T token huấn luyện và chứng minh rằng nó vượt trội đáng kể so với các mô hình tiên tiến trong nhiều benchmark khác nhau. Được tiền huấn luyện trên các chuỗi dài 4K, SAMBA cho thấy perplexity được cải thiện ở độ dài ngữ cảnh lên đến 1M trong zero-shot. Khi được tinh chỉnh trên các chuỗi dài 4K, SAMBA hiệu quả ngoại suy đến độ dài ngữ cảnh 256K với khả năng nhớ hoàn hảo trong tác vụ Passkey Retrieval, và thể hiện khả năng ngoại suy truy xuất vượt trội trong tác vụ Phonebook thách thức so với các mô hình full-attention. Là một mô hình chuỗi thời gian tuyến tính, SAMBA đạt được thông lượng cao hơn 3.73× so với Transformer với grouped-query attention cho các prompt người dùng dài 128K, và tăng tốc 3.64× khi tạo 64K token với streaming không giới hạn. Mã nguồn để huấn luyện trên dữ liệu mở được công khai tại https://github.com/microsoft/Samba.

1 GIỚI THIỆU
Các mô hình dựa trên attention (Vaswani et al., 2017; Bahdanau et al., 2014) đã thống trị các kiến trúc mạng thần kinh của các Mô hình Ngôn ngữ Lớn (LLM) (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023; Bubeck et al., 2023) nhờ khả năng nắm bắt các phụ thuộc phức tạp dài hạn và khả năng song song hóa hiệu quả cho việc huấn luyện quy mô lớn (Dao et al., 2022a). Gần đây, các Mô hình Không gian Trạng thái (SSM) (Gu et al., 2021; Smith et al., 2023; Gu et al., 2022; Gu & Dao, 2023) đã nổi lên như một sự thay thế đầy hứa hẹn, cung cấp độ phức tạp tính toán tuyến tính và tiềm năng ngoại suy tốt hơn đến các chuỗi dài hơn so với khi huấn luyện. Cụ thể, Mamba (Gu & Dao, 2023), một biến thể của SSM được trang bị không gian trạng thái có tính chọn lọc, đã thể hiện triển vọng đáng chú ý thông qua hiệu suất thực nghiệm mạnh mẽ và triển khai hiệu quả nhận biết phần cứng. Nghiên cứu gần đây cũng cho thấy rằng transformer có khả năng mô hình hóa kém hơn so với SSM phụ thuộc đầu vào trong các vấn đề theo dõi trạng thái (Merrill et al., 2024). Tuy nhiên, SSM gặp khó khăn với việc nhớ lại bộ nhớ do tính chất tái phát của chúng (Arora et al., 2023), và kết quả thực nghiệm trên các tác vụ liên quan đến truy xuất thông tin (Fu et al., 2023; Wen et al., 2024; Arora et al., 2024), đã chỉ ra thêm rằng SSM không cạnh tranh được với các đối tác dựa trên attention.

Các nghiên cứu trước đây (Zuo et al., 2022; Fu et al., 2023; Ma et al., 2023; Ren et al., 2023) đã khám phá nhiều phương pháp khác nhau để lai hóa SSM với cơ chế attention, nhưng không có nghiên cứu nào chứng minh hiệu suất mô hình hóa ngôn ngữ tốt hơn đáng kể so với các kiến trúc Transformer tiên tiến. Các kỹ thuật ngoại suy độ dài hiện có (Han et al., 2023; Xiao et al., 2023; Jin et al., 2024) được thiết kế cho cơ chế attention bị hạn chế bởi độ phức tạp tính toán bậc hai hoặc hiệu suất ngoại suy ngữ cảnh không đủ, đặc biệt khi được đánh giá dưới các chỉ số perplexity.

Trong bài báo này, chúng tôi giới thiệu SAMBA, một kiến trúc mạng thần kinh đơn giản hài hòa các điểm mạnh của cả mô hình SSM và dựa trên attention, đồng thời đạt được khả năng ngoại suy độ dài có thể vô hạn với độ phức tạp thời gian tuyến tính. SAMBA kết hợp SSM với attention thông qua việc xen kẽ theo lớp Mamba (Gu & Dao, 2023), SwiGLU (Shazeer, 2020), và Sliding Window Attention (SWA) (Beltagy et al., 2020). Các lớp Mamba nắm bắt ngữ nghĩa phụ thuộc thời gian và cung cấp backbone cho việc giải mã hiệu quả, trong khi SWA lấp đầy khoảng trống mô hình hóa các phụ thuộc phức tạp, không tái phát. Một cuộc thảo luận chi tiết về các nghiên cứu liên quan được bao gồm trong Phụ lục A.

Chúng tôi mở rộng quy mô SAMBA với 421M, 1.3B, 1.7B và lên đến 3.8B tham số với 3.2T token. Đặc biệt, mô hình được hậu huấn luyện 3.8B lớn nhất đạt được điểm 71.9 cho MMLU (Hendrycks et al., 2021), 62.8 cho HumanEval (Chen et al., 2021), và 87.6 cho GSM8K (Cobbe et al., 2021), vượt trội đáng kể so với mô hình Phi-3-mini được hậu huấn luyện dưới sự kiểm soát của cùng một công thức huấn luyện và bộ dữ liệu, như chi tiết trong Bảng 1. Mặc dù được tiền huấn luyện với độ dài chuỗi 4K, SAMBA có thể được ngoại suy đến độ dài 1M trong zero shot với perplexity được cải thiện trên Proof-Pile (Zhangir Azerbayev & Piotrowski, 2022), đạt được tỷ lệ ngoại suy 256×, trong khi vẫn duy trì độ phức tạp thời gian giải mã tuyến tính với streaming token không giới hạn, như được thể hiện trong Hình 2. Chúng tôi cho thấy rằng khi được instruction-tuned trong độ dài ngữ cảnh 4K chỉ với 500 bước, SAMBA có thể được ngoại suy đến độ dài ngữ cảnh 256K với khả năng nhớ hoàn hảo trong Passkey Retrieval (Mohtashami & Jaggi, 2023). Ngược lại, mô hình dựa trên SWA được tinh chỉnh đơn giản không thể nhớ lại ký ức vượt quá độ dài 4K. Chúng tôi tiếp tục chứng minh rằng mô hình SAMBA 3.8B được instruction-tuned có thể đạt được hiệu suất tốt hơn đáng kể so với các mô hình dựa trên SWA trên các tác vụ tóm tắt ngữ cảnh dài downstream, trong khi vẫn giữ hiệu suất ấn tượng trên các benchmark ngữ cảnh ngắn. Trong một tác vụ truy xuất nhiều khóa-giá trị thách thức hơn, Phonebook (Jelassi et al., 2024), chúng tôi chứng minh rằng instruction fine-tuning cho phép SAMBA thu hẹp khoảng cách hiệu suất truy xuất với các mô hình full-attention, trong khi thể hiện khả năng ngoại suy tốt hơn đáng kể khi truy xuất số điện thoại vượt quá độ dài ngữ cảnh huấn luyện. Cuối cùng, chúng tôi thực hiện các phân tích mở rộng và nghiên cứu ablation trên các kích thước mô hình lên đến 1.7B tham số để xác thực thiết kế kiến trúc của SAMBA. Chúng tôi cũng đưa ra giải thích tiềm năng cho hiệu quả của phương pháp lai đơn giản của chúng tôi thông qua góc nhìn entropy attention/selection. Theo hiểu biết của chúng tôi, Samba là mô hình lai đầu tiên cho thấy rằng các mô hình độ phức tạp tuyến tính có thể tốt hơn đáng kể so với các mô hình Transformer tiên tiến trên các tác vụ ngữ cảnh ngắn ở quy mô lớn, trong khi vẫn có thể ngoại suy đến các chuỗi cực dài dưới chỉ số perplexity.

2 PHƯƠNG PHÁP LUẬN
Chúng tôi khám phá các chiến lược lai hóa khác nhau bao gồm các lớp Mamba, Sliding Window Attention (SWA), và Multi-Layer Perceptron (Shazeer, 2020; Dauphin et al., 2016). Chúng tôi khái niệm hóa chức năng của Mamba như việc nắm bắt các cấu trúc chuỗi tái phát, SWA như việc truy xuất chính xác bộ nhớ, và MLP như việc nhớ lại kiến thức thực tế. Chúng tôi cũng khám phá các lớp tái phát tuyến tính khác bao gồm Multi-Scale Retention (Sun et al., 2023) và GLA (Yang et al., 2023) như các thay thế tiềm năng cho Mamba trong Phần 3.2. Mục tiêu lai hóa của chúng tôi là hài hòa giữa các khối chức năng khác biệt này và tìm ra một kiến trúc hiệu quả cho mô hình hóa ngôn ngữ với khả năng ngoại suy độ dài không giới hạn.

2.1 KIẾN TRÚC
Như được minh họa trong Hình 1, chúng tôi khám phá ba loại chiến lược lai hóa theo lớp ở quy mô 1.7B: Samba, Mamba-SWA-MLP, và Mamba-MLP. Chúng tôi cũng khám phá các phương pháp lai hóa khác với full self-attention ở quy mô nhỏ hơn trong Phần 4. Số lượng lớp N được đặt là 48 cho Samba, Mamba-MLP, và Mamba, trong khi Mamba-SWA-MLP có 54 lớp, do đó mỗi mô hình có khoảng 1.7B tham số. Chúng tôi chỉ sửa đổi sắp xếp cấp lớp cho mỗi mô hình và giữ nguyên mọi cấu hình khác để có so sánh công bằng. Thêm chi tiết về cấu hình của mỗi lớp được giải thích trong các phần con sau.

2.1.1 LỚP MAMBA
Mamba (Gu & Dao, 2023) là một mô hình dựa trên SSM được đề xuất gần đây với không gian trạng thái có tính chọn lọc. Nó cho phép gating phụ thuộc đầu vào cho cả các trạng thái tái phát và biểu diễn đầu vào để lựa chọn mềm các phần tử chuỗi đầu vào. Cho một biểu diễn chuỗi đầu vào X ∈ Rn×dm, trong đó n là độ dài của chuỗi và dm là kích thước ẩn, Mamba đầu tiên mở rộng các đầu vào đến một chiều cao hơn de, tức là,

H = XWin ∈ Rn×de

trong đó Win ∈ Rdm×de là một ma trận chiếu có thể học. Sau đó một toán tử Short Convolution (SC) (Poli et al., 2023) được áp dụng để làm mượt tín hiệu đầu vào,

U = SC(H) = SiLU(DepthwiseConv(H, Wconv)) ∈ Rn×de (1)

trong đó Wconv ∈ Rk×de và kích thước kernel k được đặt là 4 cho hiệu quả nhận biết phần cứng. Depthwise Convolution (He et al., 2019) được áp dụng trên chiều chuỗi theo sau bởi hàm kích hoạt SiLU (Elfwing et al., 2017). Selective gate sau đó được tính toán thông qua một chiếu low-rank theo sau bởi Softplus (Zheng et al., 2015),

∆ = Softplus(UWrWq + b) ∈ Rn×de (2)

trong đó Wr ∈ Rde×dr, Wq ∈ Rdr×de và dr là chiều low-rank. b ∈ Rde được khởi tạo cẩn thận để ∆ ∈ [∆min, ∆max] sau giai đoạn khởi tạo. Chúng tôi đặt [∆min, ∆max] = [0.001, 0.1], và thấy rằng các giá trị này không nhạy cảm với hiệu suất mô hình hóa ngôn ngữ dưới chỉ số perplexity. Sự phụ thuộc đầu vào cũng được đưa vào cho các tham số B và C của SSM,

B = UWb ∈ Rn×ds
C = UWc ∈ Rn×ds

trong đó ds là chiều trạng thái. Cho mỗi bước thời gian 1 ≤ t ≤ n, suy luận tái phát của Selective SSM (S6) được thực hiện trong không gian trạng thái mở rộng Zt ∈ Rde×ds, tức là,

Zt = exp(−∆t ⊙ exp(A)) ⊙ Zt−1 + ∆t ⊙ (Bt ⊗ Ut) ∈ Rde×ds
Yt = ZtCt + D ⊙ Ut ∈ Rde

trong đó Z0 = 0, ⊙ có nghĩa là tích từng điểm, ⊗ có nghĩa là tích ngoài và exp có nghĩa là hàm mũ tự nhiên từng điểm. D ∈ Rde là một vector có thể học được khởi tạo là Di = 1 và A ∈ Rde×ds là một ma trận có thể học được khởi tạo là Aij = −log(j), 1 ≤ j ≤ ds, theo khởi tạo S4D-Real (Gu et al., 2022). Trong thực tế, Mamba triển khai một thuật toán quét song song nhận biết phần cứng cho việc huấn luyện song song hóa hiệu quả. Đầu ra cuối cùng được thu được thông qua một cơ chế gating tương tự như Gated Linear Unit (Shazeer, 2020; Dauphin et al., 2016),

O = Y ⊙ SiLU(XWg)Wout ∈ Rn×dm

trong đó Wg ∈ Rdm×de và Wout ∈ Rde×dm là các tham số có thể học. Trong nghiên cứu này, chúng tôi đặt de = 2dm, dr = dm/16, và ds = 16. Lớp Mamba trong SAMBA dự kiến sẽ nắm bắt ngữ nghĩa phụ thuộc thời gian của chuỗi đầu vào thông qua cấu trúc tái phát của nó. Cơ chế lựa chọn đầu vào trong lớp Mamba cho phép mô hình tập trung vào các đầu vào liên quan, do đó cho phép mô hình ghi nhớ thông tin quan trọng trong dài hạn.

2.1.2 LỚP SLIDING WINDOW ATTENTION (SWA)
Chúng tôi bao gồm các lớp Sliding Window Attention (Beltagy et al., 2020) để giải quyết các hạn chế của các lớp Mamba trong việc nắm bắt các phụ thuộc không tái phát trong chuỗi. Lớp SWA của chúng tôi hoạt động trên kích thước cửa sổ w = 2048 trượt qua chuỗi đầu vào, đảm bảo rằng độ phức tạp tính toán vẫn tuyến tính đối với độ dài chuỗi. RoPE (Su et al., 2021) được áp dụng trong cửa sổ trượt, với tần số cơ sở là 10,000. Bằng cách truy cập trực tiếp nội dung trong cửa sổ ngữ cảnh thông qua attention, lớp SWA có thể truy xuất tín hiệu độ phân giải cao từ lịch sử trung hạn đến ngắn hạn mà không thể được nắm bắt rõ ràng bởi các trạng thái tái phát của Mamba. Chúng tôi sử dụng FlashAttention 2 (Dao, 2023) cho việc triển khai hiệu quả self-attention trong toàn bộ nghiên cứu này. Chúng tôi cũng chọn kích thước cửa sổ trượt 2048 để xem xét hiệu quả; FlashAttention 2 có cùng tốc độ huấn luyện như quét song song chọn lọc của Mamba ở độ dài chuỗi 2048 dựa trên các đo lường trong (Gu & Dao, 2023).

2.1.3 LỚP MULTI-LAYER PERCEPTRON (MLP)
Các lớp MLP trong SAMBA phục vụ như cơ chế chính của kiến trúc cho việc biến đổi phi tuyến và nhớ lại kiến thức thực tế (Dai et al., 2022). Chúng tôi sử dụng SwiGLU (Shazeer, 2020) cho tất cả các mô hình được huấn luyện trong bài báo này và ký hiệu kích thước ẩn trung gian của nó là dp. Như được thể hiện trong Hình 1, Samba áp dụng các MLP riêng biệt cho các loại thông tin khác nhau được nắm bắt bởi các lớp Mamba và SWA.

3 THỰC NGHIỆM VÀ KẾT QUẢ
Chúng tôi tiền huấn luyện bốn mô hình SAMBA với các kích thước tham số khác nhau, 421M, 1.3B, 1.7B và 3.8B, để điều tra hiệu suất của nó trên các quy mô khác nhau. Chi tiết về các siêu tham số cho việc huấn luyện và thiết kế kiến trúc được thể hiện trong Bảng 12 của Phụ lục G. Chúng tôi cũng huấn luyện các kiến trúc lai khác như đã đề cập trong Phần 2.1, bao gồm Mamba baseline (Gu & Dao, 2023), kiến trúc Llama-3 (MetaAI, 2024; Dubey et al., 2024), và Mistral (Jiang et al., 2023) ở quy mô khoảng 1.7B, với các siêu tham số chi tiết trong Bảng 11 của Phụ lục G. Chúng tôi thực hiện đánh giá downstream toàn diện trên một loạt các benchmark, tập trung vào bốn khả năng chính của các mô hình: lý luận thông thường (ARC (Clark et al., 2018), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), SIQA (Sap et al., 2019)), hiểu ngôn ngữ (HellaSwag (Zellers et al., 2019), BoolQ (Clark et al., 2019), OpenbookQA (Mihaylov et al., 2018), SQuAD (Rajpurkar et al., 2016), MMLU (Hendrycks et al., 2021), MMLU-Pro (Wang et al., 2024), GPQA(Rein et al., 2023)), tính trung thực (TruthfulQA (Lin et al., 2022)) và toán học và lập trình (GSM8K (Cobbe et al., 2021), MBPP (Austin et al., 2021), HumanEval (Chen et al., 2021)).

Bảng 1: So sánh hiệu suất downstream giữa Samba-3.8B-IT và Phi-3-mini-4K trên cả các tác vụ ngữ cảnh dài và ngữ cảnh ngắn. Chúng tôi báo cáo độ chính xác 5-shot (trung bình theo danh mục) cho MMLU, 8-shot CoT (Wei et al., 2022) cho GSM8K, 0-shot pass@1 cho HumanEval, ROUGE-L cho cả GovReport và SQuALITY. † Kết quả từ báo cáo kỹ thuật Phi-3 (Abdin et al., 2024).

| Mô hình | MMLU | GSM8K | HumanEval | GovReport | SQuALITY |
|---------|------|-------|-----------|-----------|----------|
| Phi-3-mini-4K-instruct † | 68.8 | 82.5 | 58.5 | 14.4 | 21.6 |
| Samba-3.8B-IT | 71.9 | 87.6 | 62.8 | 18.9 | 21.2 |

3.1 MÔ HÌNH HÓA NGÔN NGỮ TRÊN DỮ LIỆU CHẤT LƯỢNG SÁCH GIÁO KHOA
Chúng tôi đầu tiên trình bày kết quả từ mô hình SAMBA 3.8B lớn nhất của chúng tôi, được huấn luyện trên cùng bộ dữ liệu được sử dụng bởi Phi3 (Abdin et al., 2024) với 3.2T token. Chúng tôi tuân theo cùng chiến lược tiền huấn luyện đa giai đoạn như Phi3-mini, và áp dụng cả công thức hậu huấn luyện Phi-3-mini gốc và công thức Phi3-mini-June-2024 để tạo ra các mô hình SAMBA 3.8B được instruction-tuned của chúng tôi, tức là Samba-3.8B-IT và Samba-3.8B (June) tương ứng. Chúng tôi báo cáo kết quả benchmark toàn diện của mô hình cơ sở Samba 3.8B và Samba-3.8B (June) trong Phụ lục B. Như được thể hiện trong Bảng 1, chúng tôi đánh giá hiệu suất downstream của Samba-3.8B-IT trên cả các tác vụ tóm tắt ngữ cảnh dài (GovReport (Huang et al., 2021), SQuALITY (Wang et al., 2022)) và các benchmark ngữ cảnh ngắn chính (MMLU, GSM8K, HumanEval). Chúng ta có thể thấy rằng Samba có hiệu suất tốt hơn đáng kể so với Phi-3-mini-4k-instruct trên cả các tác vụ ngữ cảnh ngắn (MMLU, GSM8K, HumanEval) và ngữ cảnh dài (GovReport), trong khi vẫn có kích thước cửa sổ 2048 của lớp SWA và duy trì độ phức tạp tuyến tính để xử lý hiệu quả các tài liệu dài. Chi tiết về thống kê dữ liệu và thiết lập đánh giá cho các tác vụ ngữ cảnh dài được bao gồm trong Phụ lục F.

Bảng 2: Đánh giá downstream của các kiến trúc được huấn luyện trên 230B token của bộ dữ liệu Phi2. Chúng tôi báo cáo độ chính xác không chuẩn hóa cho các tác vụ lựa chọn đa dạng. GSM8K được đánh giá với các ví dụ 5-shot trong khi các tác vụ khác ở zero-shot. Kết quả tốt nhất được in đậm, tốt thứ hai được gạch dưới.

| Benchmark | Llama-3 | Mistral | Mamba | Mamba-SWA-MLP | Mamba-MLP | SAMBA |
|-----------|---------|---------|-------|---------------|-----------|-------|
|           | 1.6B    | 1.6B    | 1.8B  | 1.6B         | 1.9B      | 1.7B  |
| ARC-Easy | 76.85 | 77.02 | 77.99 | 76.68 | 78.91 | **79.25** |
| ARC-Challenge | 43.26 | 44.20 | 45.22 | 46.16 | 47.35 | **48.21** |
| PIQA | 76.66 | 75.79 | 77.31 | 76.50 | **78.84** | 77.10 |
| WinoGrande | 70.01 | 70.72 | **73.40** | <u>73.72</u> | 72.38 | 72.93 |
| SIQA | 51.23 | 52.00 | 53.12 | **55.12** | <u>54.30</u> | 53.68 |
| HellaSwag | 46.98 | 47.19 | 49.80 | 49.71 | **50.14** | <u>49.74</u> |
| BoolQ | 68.20 | 70.70 | 74.83 | 74.74 | 73.70 | **75.57** |
| OpenbookQA | 34.00 | 32.80 | 36.60 | 33.80 | 35.40 | **37.20** |
| SQuAD | 74.88 | 72.82 | 67.66 | 76.73 | 63.86 | **77.64** |
| MMLU | 43.84 | 43.54 | 45.28 | 47.39 | 43.68 | **48.01** |
| TruthfulQA (MC1) | 25.70 | 25.09 | 26.81 | 26.20 | 26.44 | **27.78** |
| TruthfulQA (MC2) | 40.35 | 38.80 | 40.66 | 40.80 | 40.04 | **41.62** |
| GSM8K | 32.68 | 32.45 | 32.07 | **44.05** | 27.52 | <u>38.97</u> |
| MBPP | 46.30 | 47.08 | 47.86 | 47.08 | 47.08 | **48.25** |
| HumanEval | 36.59 | 36.59 | 35.98 | 37.80 | 31.10 | **39.02** |
| Trung bình | 51.17 | 51.12 | 52.31 | 53.77 | 51.38 | **54.33** |

Để kiểm tra các chiến lược lai hóa khác nhau được đề cập trong Phần 2.1, chúng tôi huấn luyện 6 mô hình với khoảng 1.7B tham số trên bộ dữ liệu Phi2 (Li et al., 2023) với 230B token và đánh giá chúng trong bộ 15 benchmark downstream đầy đủ để có một đánh giá toàn diện về các kiến trúc lai và thuần chủng. Như được thể hiện trong Bảng 2, SAMBA thể hiện hiệu suất vượt trội trên một tập đa dạng các tác vụ, bao gồm lý luận thông thường (ARC-Challenge), hiểu ngôn ngữ (MMLU, SQuAD), TruthfulQA và tạo mã (HumanEval, MBPP). Nó vượt trội so với cả các mô hình dựa trên attention thuần túy và dựa trên SSM trong hầu hết các tác vụ và đạt được hiệu suất trung bình tốt nhất. Bằng cách so sánh hiệu suất của Mamba-MLP và Mamba trong Bảng 2, chúng ta có thể quan sát thấy rằng việc thay thế các khối Mamba bằng MLP không làm hại khả năng lý luận thông thường, nhưng hiệu suất của nó trong hiểu ngôn ngữ và khả năng lý luận phức tạp, chẳng hạn như lập trình và lý luận toán học, suy giảm đáng kể. Chúng ta cũng có thể thấy rằng các mô hình Mamba thuần túy kém hiệu quả trong các tác vụ tập trung truy xuất như SQuAD do thiếu khả năng truy xuất bộ nhớ chính xác. Kết quả tốt nhất đạt được thông qua việc kết hợp các mô-đun attention và Mamba, như được thể hiện với kiến trúc Samba của chúng tôi. Chúng ta cũng có thể nhận thấy rằng Mamba-SWA-MLP có hiệu suất tốt hơn đáng kể trên GSM8K, có thể do sự hợp tác gần gũi hơn giữa các lớp Mamba và SWA. Các hiệu suất downstream khác biệt của các chiến lược lai hóa khác nhau đặt ra hướng nghiên cứu tương lai thú vị để phát triển các kiến trúc động thích ứng tác vụ.

3.2 KHÁM PHÁ VỀ VIỆC LAI HÓA ATTENTION VÀ TÍNH TÁI PHÁT TUYẾN TÍNH
Vì SSM thuộc về một lĩnh vực rộng lớn hơn của các mô hình tái phát tuyến tính (Orvieto et al., 2023; Qin et al., 2023; Yang et al., 2023; Katsch, 2023; Qin et al., 2024; Yang et al., 2024), tồn tại nhiều lựa chọn thay thế khác ngoài Mamba khi kết hợp các lớp dựa trên attention với mạng thần kinh tái phát. Chúng tôi cũng thêm các nghiên cứu ablation kiến trúc để biện minh cho các lựa chọn thiết kế của Samba. Cụ thể, ngoài Llama-2, Mamba, Samba và Mamba-SWA-MLP, chúng tôi điều tra phân tích so sánh của các kiến trúc sau:

• Llama-2-SWA là một kiến trúc dựa trên attention thuần túy thay thế tất cả các lớp full attention trong Llama-2 bằng sliding window attention.

• Sliding RetNet thay thế các lớp Mamba trong kiến trúc Samba bằng các lớp Multi-Scale Retention (Sun et al., 2023). RetNet là một mô hình linear attention với sự suy giảm cố định và độc lập đầu vào áp dụng cho các trạng thái ẩn tái phát.

• Sliding GLA thay thế các lớp Mamba trong kiến trúc Samba bằng Gated Linear Attention (GLA) (Yang et al., 2023). GLA là một biến thể biểu cảm hơn của linear attention với gating phụ thuộc đầu vào.

• Mega-S6 thay thế tất cả các mô-đun MD-EMA trong kiến trúc Mega (Ma et al., 2023) bằng các kết hợp ShortConv+S6 từ Mamba để thích ứng Mega với kiến trúc Mamba hiện đại. Rotary position embedding, RMSNorm và Softmax attention cũng được áp dụng. Chúng tôi đặt chiều trung gian của lớp Mega-S6 là dm để nó có khoảng 5d²m số lượng tham số. Điều này đại diện cho một baseline cổ điển thực hiện lai hóa SSM-Attention tuần tự trong lớp.

• MLP2-SWA-MLP thay thế tất cả các lớp Mamba trong kiến trúc Samba thành các lớp SwiGLU với 6d²m số lượng tham số.

• Samba-NoPE loại bỏ rotary relative position embedding trong Samba và không có bất kỳ position embedding nào trong kiến trúc.

Chúng tôi tiền huấn luyện tất cả các mô hình trên cùng bộ dữ liệu SlimPajama (Soboleva et al., 2023) dưới cả cài đặt khoảng 438M và 1.3B, và đánh giá các mô hình này bằng cách tính toán perplexity trên tập validation với độ dài ngữ cảnh 4096, 8192, và 16384 token để điều tra khả năng ngoại suy độ dài zero-shot của chúng. Thông lượng huấn luyện đỉnh cũng được đo lường như một chỉ số hiệu quả. Chi tiết về cài đặt siêu tham số được bao gồm trong Phụ lục G. Như được thể hiện trong Bảng 3, SAMBA liên tục vượt trội so với tất cả các mô hình khác ở các độ dài ngữ cảnh và kích thước mô hình khác nhau. Tốc độ huấn luyện của SAMBA cạnh tranh so với các mô hình dựa trên Transformer thuần túy ở quy mô 1.3B. Mamba có thông lượng huấn luyện kém đáng kể vì các lớp Mamba có tốc độ huấn luyện chậm hơn so với các lớp MLP, và các mô hình Mamba thuần chủng cần có nhiều lớp hơn so với các mô hình khác ở cùng số lượng tham số. So sánh Mamba-SWA-MLP với Samba, chúng ta có thể thấy rằng Samba có điểm perplexity tốt hơn một chút và thông lượng huấn luyện cao hơn. Mamba-SWA-MLP đánh đổi các lớp MLP với nhiều lớp Mamba và Attention tốn I/O hơn, dẫn đến tốc độ huấn luyện chậm hơn.

Bảng 3: Perplexity trên tập validation của SlimPajama cho các kiến trúc mô hình attention và tái phát tuyến tính khác nhau được huấn luyện ở độ dài ngữ cảnh 4,096. Chúng tôi sử dụng kích thước cửa sổ 2,048 cho Sliding Window Attention (SWA). Kết quả perplexity có dao động khoảng ±0.3%.

| Kiến trúc | Kích thước | Lớp | Tốc độ Huấn luyện | Độ dài Ngữ cảnh Validation |
|-----------|------------|-----|-------------------|---------------------------|
|           |            |     | (×10⁵tokens/s)   | 4096 | 8192 | 16384 |
| **20B token huấn luyện trên 8×A100 GPU** |
| Llama-2 | 438M | 24 | 4.85 | 11.14 | 47.23 | 249.03 |
| Llama-2-SWA | 438M | 24 | 4.96 | 11.12 | 10.66 | 10.57 |
| Mamba | 432M | 60 | 2.46 | 10.70 | 10.30 | 10.24 |
| Sliding GLA | 438M | 24 | 4.94 | 10.43 | 10.00 | 9.92 |
| Sliding RetNet | 446M | 24 | 4.32 | 10.38 | 9.96 | 9.87 |
| Mega-S6 | 422M | 24 | 3.26 | 12.63 | 12.25 | 12.25 |
| Mamba-SWA-MLP | 400M | 24 | 4.21 | 10.07 | 9.67 | 9.59 |
| MLP2-SWA-MLP | 417M | 24 | 5.08 | 10.95 | 10.50 | 10.41 |
| SAMBA-NoPE | 421M | 24 | 4.48 | 10.11 | 28.97 | 314.78 |
| SAMBA | 421M | 24 | 4.46 | 10.06 | 9.65 | 9.57 |
| **100B token huấn luyện trên 64×H100 GPU** |
| Llama-2 | 1.3B | 40 | 25.9 | 7.60 | 44.32 | 249.64 |
| Llama-2-SWA | 1.3B | 40 | 26.2 | 7.60 | 7.37 | 7.21 |
| Mamba | 1.3B | 48 | 17.8 | 7.47 | 7.26 | 7.15 |
| Sliding GLA | 1.2B | 36 | 25.9 | 7.58 | 7.35 | 7.19 |
| Sliding RetNet | 1.4B | 36 | 23.0 | 7.56 | 7.35 | 7.56 |
| Mega-S6 | 1.3B | 36 | 17.9 | 9.01 | 8.81 | 8.68 |
| Mamba-SWA-MLP | 1.3B | 36 | 23.5 | 7.37 | 7.16 | 7.00 |
| MLP2-SWA-MLP | 1.3B | 36 | 26.6 | 7.81 | 7.58 | 7.42 |
| SAMBA-NoPE | 1.3B | 36 | 25.2 | 7.33 | 20.40 | 326.17 |
| SAMBA | 1.3B | 36 | 25.2 | 7.32 | 7.11 | 6.96 |

Điều này cũng chỉ ra rằng Mamba-SWA-MLP sẽ có tốc độ giải mã chậm hơn so với Samba do kích thước cache tổng lớn hơn từ nhiều lớp SSM và Attention hơn. Chúng ta có thể quan sát thêm rằng việc thay thế Mamba bằng MLP tăng tốc quá trình huấn luyện nhưng làm hại perplexity đáng kể, cho thấy tầm quan trọng của các lớp Mamba trong kiến trúc Samba. Thú vị là, mặc dù chúng tôi sử dụng SWA trong kiến trúc Samba, Samba-NoPE vẫn có perplexity bùng nổ vượt quá độ dài huấn luyện của nó mà không có RoPE. Chúng ta cũng có thể thấy rằng trong khi RetNet có thể ngoại suy tốt dưới quy mô 438M, nó có perplexity tăng ở độ dài 16K ở quy mô 1.4B, điều này có thể chỉ ra rằng sự suy giảm độc lập đầu vào của nó có thể cần điều chỉnh cụ thể ở các quy mô khác nhau để hoạt động tốt.

Bảng 4: Đánh giá downstream của các mô hình được tiền huấn luyện với 100B token từ SlimPajama. Chúng tôi đo độ chính xác chuẩn hóa ký tự cho HellaSwag theo Gu & Dao (2023). Tất cả các tác vụ được đánh giá trong zero-shot.

| Kiến trúc | Kích thước | ARC-Easy | HellaSwag | Wino. | PIQA | LAMBADA | Avg. |
|-----------|------------|----------|-----------|-------|------|---------|------|
|           |            | acc↑     | acc_norm↑ | acc↑  | acc↑ | acc↑    |      |
| LLaMA-2 | 1.3B | 55.09 | 52.32 | 53.35 | 71.11 | 48.52 | 56.08 |
| LLaMA-2-SWA | 1.3B | 56.65 | 52.59 | 54.93 | 71.60 | 47.56 | 56.67 |
| Sliding GLA | 1.2B | 56.94 | 52.52 | 56.75 | 71.38 | 48.17 | 57.15 |
| Sliding RetNet | 1.4B | 57.66 | 52.64 | 56.75 | 71.33 | 48.34 | 57.34 |
| Mega-S6 | 1.3B | 50.63 | 41.91 | 52.96 | 68.17 | 37.88 | 50.31 |
| Mamba | 1.3B | 58.08 | 54.93 | 53.99 | 71.98 | 45.97 | 56.99 |
| Mamba-SWA-MLP | 1.3B | 59.64 | 54.50 | 55.25 | 72.42 | 49.12 | 58.19 |
| MLP2-SWA-MLP | 1.3B | 55.18 | 50.32 | 52.80 | 70.67 | 48.11 | 55.42 |
| SAMBA-NoPE | 1.3B | 58.38 | 54.62 | 56.51 | 72.03 | 51.08 | 58.52 |
| SAMBA | 1.3B | 58.21 | 54.73 | 55.72 | 72.36 | 51.68 | 58.54 |

Trong Bảng 4, chúng tôi đánh giá tất cả các mô hình quy mô 1.3B của chúng tôi trên năm tác vụ lý luận thông thường điển hình (ARC-Easy, HellaSwag, WinoGrande, PIQA và biến thể OpenAI¹ của LAMBADA (Paperno et al., 2016)) để hiểu ảnh hưởng của thiết kế kiến trúc đối với hiệu suất downstream. Chúng ta có thể thấy rằng Samba có độ chính xác trung bình tốt nhất, vượt trội so với các kiến trúc LLaMA 2 với biên độ lớn. Tương tự như đánh giá perplexity của chúng tôi, Samba và Samba-NoPE có độ chính xác trung bình tương tự, trong khi Mamba-SWA-MLP hơi tụt hậu. Chúng tôi quan sát thấy rằng các kiến trúc khác nhau xuất sắc ở các tác vụ khác nhau. Mamba-SWA-MLP hoạt động tốt nhất trên ARC-Easy, trong khi Samba và Samba-NoPE đạt kết quả vượt trội trên LAMBADA. Các mô hình lai dựa trên Mamba nói chung vượt trội so với các mô hình lai linear attention và các mô hình softmax-attention thuần túy trên HellaSwag.

3.3 NGOẠI SUY ĐỘ DÀI HIỆU QUẢ

[Hình 2: SAMBA cho thấy dự đoán được cải thiện lên đến 1M token trong tập test Proof-Pile trong khi đạt được thông lượng giải mã nhanh hơn 3.64× so với kiến trúc Llama-3 trên độ dài tạo 64K. Chúng tôi cũng bao gồm baseline SE-Llama-3 1.6B áp dụng phương pháp SelfExtend (Jin et al., 2024) cho ngoại suy độ dài zero-shot. Tất cả các mô hình được huấn luyện với độ dài chuỗi 4K.]

Chúng tôi sử dụng phần test của bộ dữ liệu Proof-Pile (Zhangir Azerbayev & Piotrowski, 2022) để đánh giá khả năng ngoại suy độ dài của các mô hình của chúng tôi ở quy mô khoảng 1.7B tham số. Chúng tôi tuân theo Position Interpolation (Chen et al., 2023a) cho việc tiền xử lý dữ liệu. Phương pháp sliding window (Press et al., 2021) được sử dụng cho đánh giá perplexity với kích thước cửa sổ 4096. Bên cạnh có thông lượng giải mã trong Hình 2 cho chỉ số hiệu quả tạo, chúng tôi cũng đo tốc độ xử lý prompt trong Hình 6 của Phụ lục B cho các mô hình SAMBA 1.7B, Mistral 1.6B, Mamba 1.8B, Llama-3 1.6B và phiên bản Self-Extended (Jin et al., 2024) SE-Llama-3 1.6B với độ dài prompt quét từ 1K đến 128K. Chúng tôi đặt kích thước nhóm là 4 và cửa sổ lân cận là 1024 cho Self-Extension. Chúng tôi cố định tổng số token xử lý mỗi lần đo là 128K và thay đổi kích thước batch tương ứng. Thông lượng được đo trên một GPU A100 với độ chính xác bfloat16. Chúng tôi lặp lại các phép đo 10 lần và báo cáo kết quả trung bình. Chúng ta có thể thấy rằng Samba đạt được thông lượng cao hơn 3.73× trong xử lý prompt so với Llama-3 1.6B ở độ dài prompt 128K, và thời gian xử lý vẫn tuyến tính đối với độ dài chuỗi.

Chúng ta cũng có thể quan sát thấy rằng kỹ thuật ngoại suy độ dài zero-shot hiện có tạo ra overhead latency suy luận đáng kể trên đối tác full-attention, trong khi nó vẫn không thể ngoại suy vô hạn với hiệu suất perplexity tương đương với Samba. Trong Hình 2, chúng ta cũng có thể thấy rằng Mamba có perplexity tăng chậm và ổn định lên đến độ dài chuỗi 1M, điều này chỉ ra rằng các mô hình tái phát tuyến tính vẫn không thể ngoại suy vô hạn nếu độ dài ngữ cảnh cực kỳ lớn.

3.4 HIỂU NGỮ CẢNH DÀI

[Hình 3: Hiệu suất Passkey Retrieval lên đến độ dài ngữ cảnh 256K cho SAMBA 1.7B (Trái) vs. Mistral 1.6B (phải) được instruction tuned trên độ dài chuỗi 4K với 500 bước.]

[Hình 4: Độ chính xác đánh giá Phonebook của các mô hình cơ sở khác nhau.]

Ngoài hiệu quả trong việc xử lý ngữ cảnh dài, Samba cũng có thể ngoại suy khả năng nhớ lại bộ nhớ của nó đến độ dài ngữ cảnh 256K thông qua supervised fine-tuning, và vẫn giữ độ phức tạp tính toán tuyến tính. Chúng tôi tinh chỉnh Samba 1.7B trên Passkey Retrieval với độ dài chuỗi huấn luyện 4K chỉ trong 500 bước. Như được trình bày trong Hình 3, SAMBA 1.7B thể hiện khả năng đáng chú ý trong việc nhớ lại thông tin từ các ngữ cảnh dài hơn đáng kể so với Mistral 1.6B, một mô hình chỉ dựa trên Sliding Window Attention (SWA). Khả năng này đặc biệt rõ ràng trong heatmap, nơi SAMBA duy trì hiệu suất truy xuất hoàn hảo trên một phạm vi rộng các vị trí pass-key trong một tài liệu dài lên đến 256K độ dài. Chúng tôi cũng vẽ đường cong loss huấn luyện và độ chính xác tổng thể passkey retrieval trong suốt quá trình tinh chỉnh trong Hình 7 và Hình 8 của Phụ lục C. Chúng tôi thấy rằng mặc dù cả hai kiến trúc đều có thể đạt loss huấn luyện gần zero trong vòng chưa đến 250 bước, Samba có thể đạt được khả năng truy xuất gần hoàn hảo sớm ở 150 bước huấn luyện, trong khi kiến trúc Mistral gặp khó khăn ở khoảng 30% độ chính xác trong suốt quá trình huấn luyện. Điều này cho thấy rằng Samba có thể có khả năng truy xuất tầm xa tốt hơn so với SWA do cơ chế lựa chọn đầu vào được đưa vào bởi các lớp Mamba. Trong Hình 8, chúng ta cũng có thể nhận thấy rằng mô hình cơ sở Samba được tiền huấn luyện có độ chính xác truy xuất (ở bước 0) tương tự như của Mistral, nhấn mạnh nhu cầu cho nghiên cứu tương lai để cải thiện khả năng truy xuất zero-shot của Samba.

Các kết quả khuyến khích trên Passkey Retrieval thúc đẩy chúng tôi khám phá thêm các giới hạn của phương pháp fine-tuning của chúng tôi. Chúng tôi thực hiện instruction tuning cho mô hình cơ sở Samba-3.8B trên Phonebook (Jelassi et al., 2024) chỉ với 100 bước trên độ dài chuỗi 4K và đánh giá mô hình Samba-3.8B-FT kết quả cho độ dài chuỗi lên đến 8K. Cài đặt đánh giá yêu cầu các mô hình truy xuất một số điện thoại ngẫu nhiên từ một danh bạ chứa 20 (độ dài 400) đến 480 (độ dài 8400) cặp tên-số, tạo ra một bài kiểm tra áp lực về khả năng ghi nhớ cho Samba có kích thước trạng thái bộ nhớ không đổi. Đáng ngạc nhiên, như được thể hiện trong Hình 4, chúng ta có thể thấy rằng mô hình Samba-3.8B-FT có thể thu hẹp hầu hết khoảng cách của nó với một mô hình full-attention (Llama2 7B) có gấp đôi kích thước tham số trong độ dài huấn luyện 4K, và đạt được độ chính xác ngoại suy tốt hơn nhiều so với tất cả các mô hình khác bao gồm mô hình cơ sở Phi3 cũng sử dụng sliding window attention 2K. Vì cả Passkey Retrieval và Phonebook đều yêu cầu các mô hình nhớ số trong một tài liệu ngữ cảnh dài, thật thú vị khi điều tra xem một mô hình được instruction-tuned trên một tác vụ có thể chuyển giao khả năng của nó sang tác vụ khác trong zero-shot hay không. Chúng tôi trực tiếp đánh giá các mô hình Samba 1.7B và Mistral 1.6B được finetuned cho Passkey Retrieval (được đặt tên là Samba 1.7B PK-FT và Mistral 1.6B PK-FT tương ứng) trên tác vụ Phonebook. Như được thể hiện trong Hình 4, Samba 1.7B có độ chính xác truy xuất hơi tốt hơn so với Mistral 1.6B, nhưng cả hai mô hình đều không thể tổng quát hóa khả năng nhớ số của chúng vượt quá kích thước cửa sổ trượt. Chúng tôi để lại cho nghiên cứu tương lai để khám phá thêm khả năng chuyển giao của các khả năng ngữ cảnh dài trong các mô hình độ phức tạp tuyến tính.

4 PHÂN TÍCH
Trong phần này, chúng tôi phân tích các kết quả thực nghiệm của SAMBA bằng cách trả lời các câu hỏi nghiên cứu sau. Kết quả perplexity trên SlimPajama có dao động khoảng ±0.3%. Tốc độ huấn luyện được đo trên 8×H100 GPU theo mặc định. Tất cả các mô hình trong phần này được huấn luyện trên SlimPajama với 20B token và độ dài chuỗi 4K, trừ khi có ghi chú khác. Chúng tôi cũng có các phân tích bổ sung về việc huấn luyện các mô hình dựa trên SWA và hiệu quả của short convolution trong Phụ lục D.

**Tại sao không lai hóa với full attention?** Một số nghiên cứu trước đây (Fu et al., 2023; Lieber et al., 2024) đề xuất một kiến trúc lai của Mamba với full attention. Tuy nhiên, như được thể hiện trong Bảng 5, perplexity ngoại suy bùng nổ ở độ dài ngữ cảnh 16K ngay cả khi một lớp full attention đơn lẻ được đặt ở đầu mô hình. Mặc dù lai hóa với full attention trong khối thứ hai và giữa thứ sáu (hàng thứ tư trong bảng), theo Dao et al. (2022b), có thể thu hẹp khoảng cách perplexity giữa các lai full-attention và Samba, chúng vẫn không thể ngoại suy vượt quá độ dài chuỗi huấn luyện. Samba cũng có thông lượng huấn luyện tốt hơn nhiều so với các lựa chọn thay thế Mamba-MLP vì self-attention với triển khai FlashAttention 2 hiệu quả hơn trong huấn luyện so với Mamba khi độ dài chuỗi là 4096.

Bảng 5: Perplexity trên SlimPajama của các kiến trúc Mamba-MLP với các lớp full attention thay thế các lớp Mamba tại các chỉ số khối khác nhau. Chúng tôi định nghĩa một khối là hai lớp liên tiếp với một lớp Mamba/Attention theo sau bởi một MLP. Tất cả các mô hình có 12 khối tổng cộng.

| Kiến trúc | Kích thước | Chỉ số Khối Full Attention | Tốc độ Huấn luyện | Độ dài Ngữ cảnh Validation |
|-----------|------------|---------------------------|-------------------|---------------------------|
|           |            |                           | (×10⁵tokens/s)   | 4096 | 8192 | 16384 |
| Mamba-MLP | 449M | 11 | 7.78 | 10.29 | 10.53 | 13.66 |
|           | 449M | 5 | 7.78 | 10.10 | 10.05 | 12.83 |
|           | 449M | 0 | 7.78 | 10.89 | 10.55 | 10.63 |
|           | 443M | 1, 5 | 7.93 | 10.06 | 10.34 | 13.57 |
| SAMBA | 421M | SWA tại chỉ số lẻ | 8.59 | 10.06 | 9.65 | 9.57 |

**Bao nhiêu tham số nên được phân bổ cho Attention?** Cho rằng Mamba đã có thể nắm bắt thông tin low-rank trong các chuỗi thông qua nén tái phát, các lớp attention trong Samba về mặt lý thuyết sẽ chỉ cần tập trung vào truy xuất thông tin nơi một số lượng nhỏ attention head sẽ đủ. Trong Bảng 6, chúng tôi khám phá các kỹ thuật nhóm query head (Ainslie et al., 2023; Shazeer, 2019), cho cả mô hình Llama và Samba. Đáng ngạc nhiên, cả kiến trúc Llama-2-SWA và kiến trúc Samba đều cho thấy perplexity validation được cải thiện khi chỉ có một key-value head. Chúng tôi đoán rằng điều này là do các mô hình ngôn ngữ nhỏ có thể được tối ưu hóa dễ dàng hơn với ít KV head hơn để chú ý đến các ngữ cảnh. Chúng ta cũng có thể thấy rằng Samba có số lượng query head tối ưu nhỏ hơn 2× so với mô hình SWA, điều này xác nhận giả thuyết của chúng tôi rằng Samba có thể hỗ trợ một số lượng attention head nhỏ hơn.

**Giải thích tiềm năng về tại sao lai tốt hơn?** Chúng tôi kiểm tra entropy của các phân phối attention cho cả mô hình Samba 1.7B và Mistral 1.6B. Như được thể hiện trong Hình 5a, mô hình Samba có phương sai lớn hơn của entropy attention được phân phối trên các chỉ số lớp, với một mô hình thú vị rằng các lớp trên và dưới có entropy cao hơn so với các lớp giữa. Điều này có thể chỉ ra rằng các lớp attention được chuyên môn hóa hơn trong kiến trúc Samba, với các lớp giữa tập trung vào truy xuất chính xác với attention entropy thấp, và các lớp trên và dưới tập trung vào tích hợp thông tin toàn cục thông qua attention entropy cao. Chúng ta cũng có thể thấy trong Hình 5b rằng, so với mô hình Mamba-MLP, Samba có entropy cao hơn của các xác suất lựa chọn đầu vào trong các lớp giữa. Điều này chỉ ra rằng, với khả năng nhớ lại bộ nhớ của các lớp attention, các lớp Mamba có thể tập trung nhiều hơn vào việc mô hình hóa cấu trúc tái phát thay vì thực hiện truy xuất với các lựa chọn đầu vào chính xác. Loại chuyên môn hóa này có thể có lợi cho hiệu suất mô hình downstream, điều này có thể giải thích các kết quả ấn tượng từ kiến trúc Samba. Chi tiết về cách tính entropy được bao gồm trong Phụ lục E.

Bảng 6: Perplexity trên SlimPajama của các mô hình Llama-2-SWA và Samba ở quy mô 430M được huấn luyện với số lượng Query và Key-Value head khác nhau. "KV Size" có nghĩa là kích thước của các vector Key-Value mỗi token và lớp attention. Vì grouped query attention sẽ giảm tham số cho attention từ 4d²ₘ xuống khoảng 2d²ₘ, chúng tôi tăng kích thước trung gian của MLP từ 8/3dₘ lên 3dₘ = 4608 để có khoảng cùng số lượng tham số tổng như các mô hình gốc.

| Query Head | Key-Value Head | Head Dim. | KV Size | Model Size | Training Speed | Validation Context Length |
|------------|----------------|-----------|---------|------------|----------------|---------------------------|
|            |                |           |         |            | (×10⁵tokens/s) | 4096 | 8192 | 16384 |
| **Llama-2-SWA Architecture** |
| 12 | 2 | 128 | 512 | 419M | 10.01 | 11.11 | 10.64 | 10.56 |
| 6 | 1 | 256 | 512 | 419M | 9.98 | 11.09 | 10.62 | 10.54 |
| 12 | 1 | 128 | 256 | 414M | 10.25 | 10.89 | 10.44 | 10.35 |
| 12 | 4 | 128 | 1024 | 428M | 9.85 | 11.11 | 10.64 | 10.56 |
| **Samba Architecture** |
| 12 | 2 | 128 | 512 | 426M | 8.55 | 10.09 | 9.68 | 9.60 |
| 6 | 1 | 256 | 512 | 426M | 8.46 | 9.99 | 9.59 | 9.51 |
| 12 | 1 | 128 | 256 | 424M | 8.62 | 10.07 | 9.66 | 9.58 |
| 12 | 4 | 128 | 1024 | 431M | 8.57 | 10.02 | 9.62 | 9.55 |

[Hình 5: (a) Entropy attention trung bình mỗi bước giải mã và (b) Entropy lựa chọn S6 trung bình trên toàn bộ chuỗi tại mỗi khối lớp trên 100 mẫu ngẫu nhiên từ bộ dữ liệu GSM8K.]

5 KẾT LUẬN
Trong bài báo này, chúng tôi giới thiệu SAMBA, một kiến trúc mạng thần kinh lai đơn giản nhưng mạnh mẽ được thiết kế cho mô hình hóa ngôn ngữ hiệu quả với độ dài ngữ cảnh không giới hạn. Chúng tôi cho thấy rằng SAMBA vượt trội đáng kể so với các mô hình tiên tiến dựa trên attention thuần túy và dựa trên SSM trên một loạt các benchmark bao gồm lý luận thông thường, hiểu ngôn ngữ, toán học và lập trình. Hơn nữa, SAMBA thể hiện hiệu quả đáng chú ý trong việc xử lý ngữ cảnh dài, đạt được tăng tốc đáng kể trong xử lý prompt và thông lượng giải mã so với kiến trúc Transformer tiên tiến. Khả năng ngoại suy khả năng nhớ lại bộ nhớ của kiến trúc đến các ngữ cảnh rất dài (lên đến 256K) thông qua tinh chỉnh tối thiểu nhấn mạnh khả năng ứng dụng thực tế của nó cho các tác vụ đòi hỏi hiểu ngữ cảnh mở rộng. Khả năng ghi nhớ dài hạn hiệu quả này được chứng minh thêm là hữu ích bởi các đánh giá của chúng tôi trong các tác vụ tóm tắt ngữ cảnh dài downstream. Các phân tích của chúng tôi cũng cung cấp cái nhìn sâu sắc về các cấu hình huấn luyện tối ưu cho các mô hình lai và nhấn mạnh lợi ích của việc kết hợp cơ chế attention với SSM. Chúng tôi thấy rằng việc phân bổ ít tham số hơn cho cơ chế attention trong khi tận dụng điểm mạnh của Mamba để nắm bắt các cấu trúc tái phát dẫn đến mô hình hóa ngôn ngữ hiệu quả và hiệu suất hơn. Kết quả của chúng tôi gợi ý rằng SAMBA là một kiến trúc mạng thần kinh mạnh mẽ cho mô hình hóa ngôn ngữ với độ dài ngữ cảnh không giới hạn.
