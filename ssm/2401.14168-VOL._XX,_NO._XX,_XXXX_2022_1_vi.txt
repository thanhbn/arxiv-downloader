# 2401.14168.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2401.14168.pdf
# Kích thước tệp: 9671253 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
VOL. XX, NO. XX, XXXX 2022 1
Vivim: một Video Vision Mamba cho
Phân đoạn Video Y tế
Yijun Yang, Zhaohu Xing, Lequan Yu, Thành viên, IEEE, Chunwang Huang, Huazhu Fu, Thành viên Cao cấp, IEEE, Lei
Zhu, Thành viên, IEEE
Tóm tắt - Phân đoạn video y tế ngày càng được chú ý trong thực hành lâm sàng do các tham chiếu động dư thừa trong các khung hình video. Tuy nhiên, các mạng nơ-ron tích chập truyền thống có trường tiếp nhận hạn chế và các mạng dựa trên transformer có hiệu suất trung bình trong việc xây dựng phụ thuộc dài hạn từ góc độ độ phức tạp tính toán. Điểm nghẽn này đặt ra thử thách đáng kể khi xử lý các chuỗi dài hơn trong các tác vụ phân tích video y tế bằng các thiết bị có sẵn với bộ nhớ hạn chế. Gần đây, các mô hình không gian trạng thái (SSMs), nổi tiếng với Mamba, đã thể hiện những thành tựu ấn tượng trong mô hình hóa chuỗi dài hiệu quả, phát triển các mạng nơ-ron sâu bằng cách mở rộng trường tiếp nhận trên nhiều tác vụ thị giác một cách đáng kể. Thật không may, các SSMs thông thường đã thất bại trong việc đồng thời nắm bắt các tín hiệu thời gian nhân quả và bảo tồn thông tin không gian không nhân quả. Để giải quyết vấn đề này, bài báo này trình bày một khung dựa trên Video Vision Mamba, được gọi là Vivim, cho các tác vụ phân đoạn video y tế. Vivim của chúng tôi có thể nén hiệu quả biểu diễn không-thời gian dài hạn thành các chuỗi ở các tỷ lệ khác nhau với Khối Mamba Thời gian được thiết kế của chúng tôi. Chúng tôi cũng giới thiệu một ràng buộc affine nhận biết biên giới cải tiến qua các khung hình để tăng cường khả năng phân biệt của Vivim trên các tổn thương mơ hồ. Các thí nghiệm mở rộng về phân đoạn tuyến giáp, phân đoạn tổn thương vú trong video siêu âm, và phân đoạn polyp trong video nội soi đại tràng chứng minh tính hiệu quả và hiệu suất của Vivim, vượt trội so với các phương pháp hiện có. Mã nguồn có sẵn tại: https://github.com/scott-yjyang/Vivim. Tập dữ liệu sẽ được phát hành khi được chấp nhận.
Thuật ngữ chỉ mục - Phân đoạn tuyến giáp, Phân đoạn tổn thương vú, phân đoạn polyp, Mô hình không gian trạng thái, Video siêu âm.

I. GIỚI THIỆU
Phân đoạn tự động các tổn thương và mô là điều cần thiết cho việc kiểm tra và điều trị lâm sàng hỗ trợ máy tính [1], chẳng hạn như phân đoạn tổn thương siêu âm, phân đoạn polyp. Tuy nhiên, phân đoạn các đối tượng y tế thường là thách thức do các yếu tố vốn có, bao gồm ranh giới tổn thương mơ hồ, phân bố không đồng nhất, các mẫu chuyển động đa dạng, và những thay đổi động trong môi trường phức tạp [2]. Video y tế, về bản chất là các chuỗi hình ảnh y tế, cung cấp ngữ cảnh phong phú và chi tiết hơn để định vị các tổn thương và mô mơ hồ. Thông tin bổ sung này làm cho phân đoạn dựa trên video xử lý tốt hơn các phức tạp bất ngờ bằng cách cung cấp một cái nhìn liên tục, cho phép phân tích chính xác và toàn diện hơn. Do đó, để xem xét nhiều ngữ cảnh đối tượng hơn, việc mở rộng trường tiếp nhận của mô hình sâu trong không gian không-thời gian là rất mong muốn trong phân tích video y tế. Các mạng nơ-ron tích chập truyền thống [3]-[6] thường gặp khó khăn trong việc nắm bắt thông tin toàn cục so với các kiến trúc dựa trên transformer gần đây. Kiến trúc transformer, sử dụng Multi-Head Self Attention (MSA) [7] để trích xuất thông tin toàn cục, đã thu hút nhiều sự chú ý từ cộng đồng phân đoạn đối tượng video tổng quát [8]-[10]. Xem xét rằng các khung hình lân cận cung cấp gợi ý có lợi cho việc phân đoạn, các phương pháp này thường giới thiệu một số mô-đun được xây dựng tinh vi trên cơ chế tự chú ý để khai thác thông tin thời gian. Tuy nhiên, việc khám phá chiều thời gian bổ sung thường dẫn đến tăng độ phức tạp và nhu cầu tài nguyên lớn hơn, đặt ra những thách thức đáng kể cho việc triển khai do các điều kiện môi trường nghiêm ngặt và đặc tính chiều cao vốn có của video y tế. Ví dụ, việc kết hợp các mô-đun tự chú ý thời gian có thể vô tình gây ra sự gia tăng bậc hai về độ phức tạp so với chiều thời gian, dẫn đến những thách thức tính toán đáng kể. Sự gia tăng rõ rệt trong số lượng token trong các chuỗi video dài đưa ra những căng thẳng tính toán đáng kể khi sử dụng các kỹ thuật Multi-head Self-Attention (MSA) để mô hình hóa thông tin thời gian [9].

Gần đây, để giải quyết vấn đề đặt ra không đúng liên quan đến mô hình hóa chuỗi dài, Mamba [11], được lấy cảm hứng từ các mô hình không gian trạng thái (SSMs) [12], đã được phát triển. Ý tưởng chính của nó là nắm bắt hiệu quả các phụ thuộc tầm xa bằng cách triển khai cơ chế quét chọn lọc cho tương tác chuỗi 1-D. Dựa trên điều này, U-Mamba [13] đã thiết kế một khối CNN-SSM lai, chủ yếu được cấu tạo từ các mô-đun Mamba, để xử lý các chuỗi dài trong các tác vụ phân đoạn hình ảnh y sinh học. Vision Mamba [14] cung cấp một xương sống thị giác tổng quát mới với các khối Mamba hai chiều trên các tác vụ phân loại hình ảnh và phân đoạn ngữ nghĩa. Như họ đề xuất, việc dựa vào mô-đun tự chú ý không cần thiết để đạt được học biểu diễn thị giác hiệu quả. Nó có thể được thay thế bằng Mamba khi khám phá phụ thuộc thời gian dài hạn trong các tình huống video. Khía cạnh quan trọng của việc thích ứng mô hình Vision Mamba cho các ứng dụng video nằm ở khả năng đồng thời nắm bắt các tín hiệu thời gian nhân quả trong khi duy trì tính toàn vẹn của thông tin không gian không nhân quả.

--- TRANG 2 ---
VOL. XX, NO. XX, XXXX 2022 2
Hình 1. Một số trường hợp của tập dữ liệu VTUS chúng tôi thu thập. Tất cả video được chụp từ bệnh nhân có nốt giáp. Chúng được chụp bởi các bác sĩ siêu âm với hơn 10 năm kinh nghiệm lâm sàng để đảm bảo chất lượng hình ảnh. Những video này được chú thích chéo bởi ba chuyên gia với hơn ba năm kinh nghiệm trong chẩn đoán tuyến giáp.

Được thúc đẩy bởi điều này, chúng tôi trình bày một khung dựa trên SSMs Vivim tích hợp Mamba vào kiến trúc transformer đa cấp để khai thác thông tin không-thời gian trong video với độ phức tạp tuyến tính. Theo hiểu biết của chúng tôi, đây là công trình đầu tiên kết hợp SSMs vào tác vụ phân đoạn video y tế, tạo điều kiện cho hiệu suất nhanh hơn và lớn hơn. Trong Vivim của chúng tôi, lấy cảm hứng từ kiến trúc của các khối transformer hiện đại, chúng tôi thiết kế một Khối Mamba Thời gian mới. Một bộ mã hóa phân cấp bao gồm nhiều Khối Mamba Thời gian được giới thiệu để khảo sát mối tương quan giữa phụ thuộc không gian và thời gian ở các tỷ lệ khác nhau. Vì các mô hình chuỗi không gian trạng thái có cấu trúc với quét chọn lọc (S6) [11] xử lý dữ liệu đầu vào một cách nhân quả, chúng chỉ có thể nắm bắt thông tin trong phần được quét của dữ liệu. Điều này làm cho S6 phù hợp với các tác vụ NLP và video liên quan đến dữ liệu thời gian nhưng đặt ra thách thức khi giải quyết dữ liệu không nhân quả như hình ảnh 2D trong video y tế. Để giải quyết vấn đề này, mô hình chuỗi không gian trạng thái có cấu trúc với quét chọn lọc không-thời gian, ST-Mamba, được thiết kế và kết hợp vào mỗi tỷ lệ của bộ mã hóa mô hình, thay thế mô-đun tự chú ý hoặc window-attention để đạt được học biểu diễn thị giác video hiệu quả. Cuối cùng, chúng tôi sử dụng một ràng buộc affine nhận biết biên giới cải tiến để cải thiện khả năng phân biệt của Vivim trên các mô mơ hồ trong video y tế ở giai đoạn huấn luyện.

Đáng chú ý là không có tập dữ liệu công khai với video siêu âm được chú thích ở cấp pixel cho phân đoạn tuyến giáp, vì việc phác thảo ranh giới của các tổn thương mơ hồ trong video siêu âm tương phản thấp theo tinh thần từng khung hình là đắt đỏ. Trong công trình này, chúng tôi thu thập một tập dữ liệu phân đoạn tuyến giáp VTUS với 100 video siêu âm được chú thích theo góc nhìn ngang và dọc và tổng cộng 9342 khung hình với ground truth cấp pixel để tạo điều kiện cho đánh giá benchmark. Một số ví dụ được hiển thị trong Hình. 1. Chúng tôi tiến hành các thí nghiệm mở rộng trên ba tác vụ phân đoạn video y tế phổ biến, tức là phân đoạn tuyến giáp trong video siêu âm, phân đoạn tổn thương vú trong video siêu âm, và phân đoạn polyp trong video nội soi đại tràng. Kết quả vượt trội xác nhận tính hiệu quả, hiệu suất và tính linh hoạt của khung Vivim của chúng tôi.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi phát triển một khung phân đoạn video y tế bao gồm một bộ mã hóa dựa trên Mamba và một bộ giải mã dựa trên CNN để có được hiểu biết toàn diện về video y tế và bảo tồn các chi tiết cục bộ, tương ứng. Đây là công trình đầu tiên giới thiệu các mô hình không gian trạng thái vào các tình huống video y tế.
• Thay vì chỉ đơn giản thích ứng Mamba với các tác vụ y tế, chúng tôi thiết kế quét chọn lọc không-thời gian để tăng cường khả năng nhận thức toàn cục trong video của Khối Mamba Thời gian của chúng tôi.

--- TRANG 3 ---
VOL. XX, NO. XX, XXXX 2022 3
• Chúng tôi sử dụng một ràng buộc nhận biết biên giới cải tiến dựa trên tối ưu hóa phép biến đổi affine để giảm thiểu dự đoán biên giới mơ hồ của mô hình chúng tôi.
• Chúng tôi thu thập tập dữ liệu phân đoạn tuyến giáp siêu âm video đầu tiên với chú thích cấp pixel, tạo điều kiện cho đánh giá benchmark của các phương pháp phân đoạn video y tế. Mô hình của chúng tôi đạt được kết quả phân đoạn đầy hứa hẹn trên các phương thức đa dạng nhưng vẫn duy trì hiệu quả tốt vượt trội so với các phương pháp dựa trên Transformer.

II. CÁC CÔNG TRÌNH LIÊN QUAN

A. Phân đoạn Video Y tế
Các phương pháp gần đây đã giới thiệu các thuật toán lai dựa trên transformer sáng tạo kết hợp các kỹ thuật lớp biến đổi và tích chập cho phân đoạn hình ảnh y tế (ví dụ: tổn thương vú, polyp) [3], [6], [15]-[20]. Đối với phân đoạn tuyến giáp trong hình ảnh siêu âm, Jeremy và cộng sự [21] đã phát triển một mạng học sâu hồi quy không-thời gian mới để tự động phân đoạn tuyến giáp trong các clip siêu âm bằng cách tận dụng thông tin chuỗi thời gian. Ma và cộng sự [22] đã sử dụng mạng đề xuất vùng (RPN) để trích xuất đặc trưng sâu ban đầu và kết hợp kim tự tháp không gian RoIAlign như một đầu phân đoạn để nắm bắt thông tin toàn cục và cục bộ trong hình ảnh siêu âm. Chi và cộng sự [23] đã phát triển một 2D Transformer-UNet cho phân đoạn tuyến giáp, kết hợp các đặc trưng cấp cao từ các lớp giải mã với các đặc trưng cấp thấp hơn từ các lớp mã hóa bằng cách sử dụng một mô-đun transformer chú ý chéo đa tỷ lệ. Các thuật toán này khéo léo quản lý các biểu diễn được rút ra từ hình ảnh y tế độ phân giải cao, tuy nhiên, chúng gặp khó khăn tính toán do các vấn đề độ phức tạp. Ngoài ra, việc áp dụng trực tiếp các phương pháp phân đoạn hình ảnh như vậy có thể vô tình bỏ qua ngữ cảnh thời gian quan trọng, do đó gây ra sự không nhất quán thời gian. Để giải quyết mô hình hóa thời gian trong phân đoạn cấp video, phương pháp sáng tạo của Space-Time Memory Networks (STM) [24] và các biến thể của nó [10], [25], [26] được giới thiệu, sử dụng mạng bộ nhớ để trích xuất thông tin quan trọng từ một bộ đệm dựa trên thời gian được cấu tạo từ tất cả các chuỗi video trước đó. Xây dựng dựa trên phương pháp này, DPSTT [27] tích hợp một ngân hàng bộ nhớ với các transformer tách rời để theo dõi chuyển động tổn thương thời gian trong video siêu âm y tế. Tuy nhiên, DPSTT yêu cầu tăng cường dữ liệu đáng kể để tránh overfitting và được đánh dấu bởi tốc độ xử lý chậm, nhấn mạnh một số hạn chế tiềm ẩn. FLA-Net [28] trình bày một mạng tổng hợp đặc trưng tần số và vị trí với một lượng lớn chiếm dụng bộ nhớ cho phân đoạn tổn thương vú trong video siêu âm. Do đó, thách thức trong phân đoạn video y tế xoay quanh việc khai thác hiệu quả sự phong phú của dữ liệu thời gian có sẵn.

B. Mô hình Không gian Trạng thái
Gần đây, Mô hình Không gian Trạng thái (SSMs) [12] đã chứng minh hiệu quả đáng chú ý trong việc sử dụng các phép biến đổi không gian trạng thái [29] để quản lý các phụ thuộc dài hạn trong các chuỗi ngôn ngữ. S4 [30] đã giới thiệu một mô hình chuỗi không gian trạng thái có cấu trúc để khai thác các phụ thuộc tầm xa với lợi ích của độ phức tạp tuyến tính. Dựa trên điều này, Mamba [11] tích hợp thiết kế phần cứng hiệu quả và cơ chế lựa chọn sử dụng quét song song (S6), do đó vượt trội so với Transformers trong việc xử lý các chuỗi ngôn ngữ tự nhiên mở rộng. Tiếp theo, S4ND [31] khám phá mô hình hóa tín hiệu liên tục của SSMs đối với dữ liệu đa chiều như hình ảnh và video. Gần đây hơn, Vision Mamba [14] và Vmamba [32] đã tiên phong trong các tác vụ thị giác tổng quát và vượt trội so với các phương pháp dựa trên transformer về tính hiệu quả và hiệu suất, giới thiệu các cơ chế quét hai chiều và quét chéo để giải quyết thách thức nhạy cảm hướng trong SSMs. U-Mamba [13] đã thiết kế một khối CNN-SSM lai, chủ yếu được cấu tạo từ các mô-đun Mamba, để xử lý các chuỗi dài trong các tác vụ phân đoạn hình ảnh y sinh học. Theo hiểu biết của chúng tôi, SSMs chưa được khám phá trong các tác vụ phân đoạn video y tế.

III. PHƯƠNG PHÁP

A. Tổng quan
Trong phần này, chúng tôi trình bày chi tiết về một giải pháp dựa trên Mamba Vivim cho các tác vụ phân đoạn video y tế. Vivim của chúng tôi chủ yếu bao gồm hai mô-đun: Một bộ mã hóa phân cấp với các Khối Mamba Thời gian xếp chồng để trích xuất các chuỗi đặc trưng thô và tinh ở các tỷ lệ khác nhau, và một đầu phân đoạn dựa trên CNN nhẹ để kết hợp các chuỗi đặc trưng đa cấp và dự đoán mặt nạ phân đoạn. Hình. 2 minh họa sơ đồ của Vivim được đề xuất của chúng tôi. Cụ thể, cho một clip video với T khung hình, tức là V={I1, ..., IT}, trước tiên chúng tôi chia các khung hình này thành các patch có kích thước 4×4 bằng patch embedding chồng chéo. Sau đó chúng tôi đưa chuỗi các patch vào Bộ mã hóa Mamba Thời gian phân cấp của chúng tôi để có được các đặc trưng không-thời gian đa cấp với độ phân giải {1/4,1/8,1/16,1/32} của khung hình gốc. Cuối cùng, chúng tôi chuyển các đặc trưng đa cấp đến đầu phân đoạn dựa trên CNN để dự đoán kết quả phân đoạn. Ràng buộc Affine Nhận biết Biên giới được triển khai trên kết quả chỉ trong quá trình huấn luyện như thể hiện trong Hình. 4. Vui lòng tham khảo các phần sau để biết chi tiết về mô-đun được đề xuất của chúng tôi.

B. Kiến thức cơ bản: Mô hình Không gian Trạng thái
Mô hình Không gian Trạng thái (SSMs) thường được coi là các hệ thống tuyến tính bất biến theo thời gian, ánh xạ một hàm hoặc chuỗi 1-D x(t) trong R → y(t) trong R thông qua một trạng thái ẩn h(t) trong RN. Hệ thống này thường được công thức hóa như các phương trình vi phân tuyến tính (ODEs), sử dụng A trong RN×N như tham số tiến hóa và B trong RN×1, C trong R1×N như các tham số chiếu.

h′(t) = Ah(t) + Bx(t), y(t) = Ch(t). (1)

Quá trình rời rạc hóa được giới thiệu để chủ yếu biến đổi ODE thành một hàm rời rạc. Phép biến đổi này rất quan trọng để điều chỉnh mô hình với tốc độ lấy mẫu của tín hiệu cơ bản được thể hiện trong dữ liệu đầu vào, cho phép các hoạt động hiệu quả về mặt tính toán. Các mô hình chuỗi không gian trạng thái có cấu trúc (S4) và Mamba là các phiên bản rời rạc cổ điển của hệ thống liên tục, bao gồm một tham số tỷ lệ thời gian ∆ để biến đổi các tham số liên tục A, B thành các tham số rời rạc A, B. Phương pháp thường được sử dụng để biến đổi là zero-order hold (ZOH), được định nghĩa như sau:

A = exp(∆A), B = (∆A)−1(exp(∆A)−I)·∆B. (2)

Sau khi rời rạc hóa A, B, phiên bản rời rạc của Phương trình (1) có thể được viết lại như:

ht = Aht−1 + Bxt, yt = Cht. (3)

Cuối cùng, các mô hình tính toán đầu ra thông qua một tích chập toàn cục.

K = (CB, CAB, ..., CAM−1B), y = x * K, (4)

trong đó M là độ dài của chuỗi đầu vào x, và K trong RM là một kernel tích chập có cấu trúc.

--- TRANG 4 ---
VOL. XX, NO. XX, XXXX 2022 4
Hình 2. (a) Tổng quan về Vivim được đề xuất cho phân đoạn video y tế. Chuỗi video trước tiên được đưa vào patch embedding và các Khối Mamba Thời gian đa tỷ lệ để mã hóa. Sau đó, các chuỗi đặc trưng được tổng hợp để dự đoán kết quả phân đoạn bằng một đầu phân đoạn dựa trên CNN. (b) Khối xây dựng cơ bản của Vivim, cụ thể là Khối Mamba Thời gian. Trong khi Efficient Spatial Self-attention tiến hành mô hình hóa không gian ban đầu, ST-Mamba khám phá phụ thuộc không-thời gian với độ phức tạp tuyến tính. (c) ST-Mamba kết hợp quét chọn lọc không-thời gian để mô hình hóa chuỗi dài của các tác vụ thị giác video theo tinh thần đa chiều.

C. Kiến trúc Tổng thể
1) Biểu diễn đặc trưng phân cấp: Các đặc trưng đa cấp cung cấp cả đặc trưng thô độ phân giải cao và đặc trưng chi tiết độ phân giải thấp giúp cải thiện đáng kể kết quả phân đoạn, đặc biệt cho hình ảnh y tế. Để đạt được điều này, không giống như Vivit [9], bộ mã hóa của chúng tôi trích xuất các đặc trưng đa tỷ lệ đa cấp cho các khung hình video đầu vào. Cụ thể, chúng tôi thực hiện patch merging theo từng khung hình ở cuối mỗi Khối Mamba Thời gian, dẫn đến embedding đặc trưng thứ i Fi với độ phân giải H/2^(i+1) × W/2^(i+1).

2) Khối Mamba Thời gian: Khám phá thông tin thời gian rất quan trọng cho phân đoạn video y tế bằng cách cung cấp các tín hiệu xuất hiện động và chuyển động. Tuy nhiên, MSA trong các kiến trúc Transformer thông thường có độ phức tạp bậc hai liên quan đến số lượng token [7]. Độ phức tạp này rất phù hợp cho các chuỗi đặc trưng dài từ video, vì số lượng token tăng tuyến tính với số khung hình đầu vào. Được thúc đẩy bởi điều này, chúng tôi phát triển một khối hiệu quả hơn, Khối Mamba Thời gian, để đồng thời khai thác thông tin không gian và thời gian bằng các mô hình chuỗi không gian trạng thái có cấu trúc.

Như minh họa trong Hình. 2 (b), trong Khối Mamba Thời gian, một mô-đun tự chú ý chỉ không gian hiệu quả trước tiên được giới thiệu để cung cấp tổng hợp ban đầu thông tin không gian theo sau bởi một lớp Mix-FeedForward. Chúng tôi tận dụng quá trình giảm chuỗi được giới thiệu trong [33], [34] để cải thiện hiệu quả của nó. Đối với embedding đặc trưng cấp i Fi trong RT×Ci×H×W của clip video đã cho, chúng tôi chuyển vị chiều kênh và thời gian, và làm phẳng embedding đặc trưng không-thời gian thành chuỗi dài 1D hi trong RCi×THW. Sau đó, chuỗi làm phẳng hi được đưa vào các lớp của mô-đun Spatio-Temporal Mamba (ST-Mamba) và Detail-Specific Feedforward (DSF). Mô-đun ST-Mamba thiết lập các phụ thuộc tầm xa trong và giữa các khung hình trong khi DSF bảo tồn các chi tiết tinh vi bằng một tích chập theo chiều sâu với kích thước kernel 3×3×3. Quy trình trong Mamba Layer xếp chồng có thể được định nghĩa như sau, trong đó l ∈ [1, Nm]:

h^l = ST-Mamba(LN(h^(l-1))) + h^(l-1),
h^l = DSF(LN(h^l)) + h^l. (5)

Cuối cùng, chúng tôi trả lại chuỗi đặc trưng đầu ra về hình dạng gốc và sử dụng patch merging chồng chéo để down-sampling embedding đặc trưng.

3) Bộ giải mã: Để dự đoán mặt nạ phân đoạn từ các embedding đặc trưng đa cấp, chúng tôi giới thiệu một đầu phân đoạn dựa trên CNN. Trong khi bộ mã hóa Mamba Thời gian phân cấp của chúng tôi có trường tiếp nhận hiệu quả lớn qua các trục không gian và thời gian, đầu phân đoạn dựa trên CNN tiếp tục tinh chỉnh các chi tiết của các vùng cục bộ. Cụ thể, các đặc trưng đa cấp {F1, F2, F3, F4} từ các khối mamba thời gian được chuyển vào một lớp MLP để thống nhất chiều kênh. Các đặc trưng thống nhất này được up-sample đến cùng độ phân giải và nối lại với nhau. Thứ ba, một lớp MLP được sử dụng để kết hợp các đặc trưng nối F. Cuối cùng, đặc trưng kết hợp đi qua một lớp tích chập 1×1 để dự đoán mặt nạ phân đoạn M. Loss phân đoạn Lseg bao gồm cross-entropy loss theo pixel và IoU loss được áp dụng trong quá trình huấn luyện.

D. Quét Chọn lọc Không-thời gian
Mặc dù bản chất nhân quả của S6 đối với dữ liệu thời gian, video khác với văn bản ở chỗ chúng không chỉ chứa thông tin thời gian dư thừa mà còn tích lũy thông tin không gian 2D không nhân quả. Để giải quyết vấn đề thích ứng với dữ liệu không nhân quả và khám phá đầy đủ thông tin thời gian, chúng tôi giới thiệu ST-Mamba như thể hiện trong Hình. 2 (c), kết hợp mô hình hóa chuỗi không-thời gian cho các tác vụ thị giác video. Cụ thể, để khám phá rõ ràng mối quan hệ giữa các khung hình, trước tiên chúng tôi mở các patch của mỗi khung hình dọc theo hàng và cột thành các chuỗi, sau đó nối các chuỗi khung hình để tạo thành chuỗi temporal-first h_t^i trong R^(Ci×T(HW)). Chúng tôi tiến hành song song với việc quét dọc theo hướng tiến và lùi để khám phá phụ thuộc thời gian hai chiều. Cách tiếp cận này cho phép các mô hình bù đắp cho các trường tiếp nhận của nhau mà không tăng đáng kể độ phức tạp tính toán. Đồng thời, chúng tôi xếp các patch dọc theo trục thời gian và xây dựng chuỗi spatial-first h_s^i trong R^(Ci×(HW)T). Chúng tôi tiến hành quét để tích hợp thông tin của mỗi pixel từ tất cả các khung hình. Cơ chế quét chọn lọc không-thời gian với ba hướng cũng được minh họa sinh động trong Hình. 3. Cơ chế của chúng tôi rõ ràng xem xét cả tính kết hợp không gian đơn khung hình và tính kết hợp giữa các khung hình, và tận dụng SSMs song song để thiết lập các phụ thuộc tầm xa trong và giữa các khung hình. Các mô hình chuỗi không gian trạng thái có cấu trúc với quét chọn lọc không-thời gian (ST-Mamba), đóng vai trò là yếu tố cốt lõi để xây dựng khối Mamba Thời gian, tạo thành khối xây dựng cơ bản của Vivim.

Hiệu quả Tính toán. SSMs trong ST-Mamba và tự chú ý trong Transformer đều cung cấp một giải pháp quan trọng để mô hình hóa ngữ cảnh không-thời gian một cách thích ứng. Cho một chuỗi thị giác video K trong R^(1×T×M×D), độ phức tạp tính toán của tự chú ý toàn cục và SSM là:

Ω(self-attention) = 4(TM)D^2 + 2(TM)^2D, (6)
Ω(SSM) = 4(TM)(2D)N + (TM)(2D)N^2, (7)

trong đó tỷ lệ mở rộng mặc định là 2, N là tham số cố định và được đặt là 16. Như quan sát, tự chú ý là bậc hai với toàn bộ độ dài chuỗi video (TM), và SSM là tuyến tính với điều đó. Hiệu quả tính toán như vậy làm cho ST-Mamba trở thành giải pháp tốt hơn cho các ứng dụng video dài hạn. Điều này cũng được xác nhận bởi phân tích thực nghiệm về hiệu quả của ST-Mamba trong Phần IV-E.

--- TRANG 5 ---
VOL. XX, NO. XX, XXXX 2022 5
Hình 3. Minh họa quét chọn lọc không-thời gian được đề xuất, bao gồm quét tiến thời gian, quét lùi thời gian và quét không gian.

E. Ràng buộc Affine Nhận biết Biên giới
Mạng được tối ưu hóa chỉ bằng giám sát phân đoạn có xu hướng tạo ra các dự đoán mơ hồ và không có cấu trúc, và overfitting trên dữ liệu huấn luyện. Để giảm thiểu những vấn đề này, chúng tôi giới thiệu một ràng buộc affine nhận biết biên giới cấp patch được lấy cảm hứng từ InverseForm [35] để thực thi cấu trúc biên giới được dự đoán. Cụ thể, như minh họa trong Hình. 4, chúng tôi giải quyết tác vụ ràng buộc này bằng cách tối ưu hóa phép biến đổi affine giữa ranh giới ground-truth và các cạnh trong bản đồ đặc trưng hướng tới ma trận biến đổi đồng nhất. Các cạnh ground truth trong các patch được rút ra từ việc áp dụng toán tử Sobel [36] lên mặt nạ ground truth, trong khi một đầu biên giới phụ trợ bao gồm ba lớp tích chập xử lý các patch đặc trưng từ bộ mã hóa Mamba để có được cạnh được dự đoán. Chúng tôi tính toán ma trận biến đổi affine θ̂_t^i cho patch thứ i giữa cạnh ground-truth B_gt^t và cạnh được dự đoán B_pred^t của khung hình mục tiêu I_t trong một clip video, bằng một MLP được huấn luyện trước. Đồng thời, chúng tôi tính toán một ma trận biến đổi affine khác θ̂_1^i cho patch thứ i giữa cạnh ground-truth B_gt^1 của I_1 và cạnh được dự đoán B_pred^t của I_t trong một clip video. MLP này được huấn luyện trước với các mặt nạ cạnh và không được tối ưu hóa trong quá trình huấn luyện phương pháp của chúng tôi. Chúng tôi tối ưu hóa ma trận θ̂_t^i, và tối ưu hóa đối kháng θ̂_1^i hướng tới ma trận đồng nhất I bằng:

L_affine = (1/N_p) Σ_{i=1}^{N_p} [Δ_1 · |θ̂_t^i - I|_F - Δ_2 · |θ̂_1^i - I|_F], (8)

trong đó N_p biểu thị số lượng patch và |·|_F là chuẩn Frobenius. Δ_1 và Δ_2 là hai siêu tham số cân bằng để kiểm soát hiệu ứng của θ̂_t^i và θ̂_1^i, được đặt thực nghiệm là 1.00 và 0.01. Trong mục tiêu này, B_pred^t được đẩy về phía B_gt^t và kéo ra khỏi B_gt^1 để cải thiện biên giới mục tiêu và duy trì sự khác biệt tinh tế giữa các khung hình trong cấu trúc tổn thương.

Chúng tôi cũng sử dụng binary cross entropy loss L_bce giữa toàn bộ biên giới được dự đoán và ground truth tương ứng của khung hình mục tiêu để tối ưu hóa thêm việc phát hiện biên giới. Cuối cùng, loss tổng thể để tối ưu hóa trong quá trình huấn luyện như sau, trong đó các tham số tỷ lệ λ_1, λ_2 đều được đặt thực nghiệm là 0.3:

L_total = L_seg + λ_1 L_affine + λ_2 L_bce. (9)

--- TRANG 6 ---
VOL. XX, NO. XX, XXXX 2022 6
Hình 4. Tổng quan về chiến lược huấn luyện. Cụ thể, ràng buộc affine nhận biết biên giới cấp patch được đề xuất của chúng tôi L_affine được giới thiệu để tối ưu hóa Vivim cùng với loss phân đoạn L_seg và boundary cross-entropy loss L_bce. MLP được huấn luyện trước để tính toán phép biến đổi affine được đóng băng trong quá trình huấn luyện.

IV. THÍ NGHIỆM

A. Tập dữ liệu
Chúng tôi đánh giá Vivim của chúng tôi trên ba tác vụ phân đoạn video y tế, tức là phân đoạn siêu âm tuyến giáp video, phân đoạn tổn thương vú siêu âm video và phân đoạn polyp video.

1) Phân đoạn siêu âm tuyến giáp video: Chúng tôi thu thập một tập dữ liệu phân đoạn siêu âm tuyến giáp video VTUS. VTUS bao gồm 100 chuỗi video, một chuỗi video cho mỗi bệnh nhân, và tổng cộng 9342 khung hình với ground truth cấp pixel. VTUS chứa các video siêu âm B-mode được xem ngang và dọc được chụp bởi các nhà cung cấp Mindray resona8/TOSHIBA Aplio500. Những video này được chú thích chéo bởi ba chuyên gia với hơn ba năm kinh nghiệm trong chẩn đoán tuyến giáp. Số lượng khung hình trong những video này dao động từ 31 đến 196 để có sự đa dạng tốt hơn. Toàn bộ tập dữ liệu được phân chia thành tập huấn luyện và kiểm tra theo tỷ lệ 7:3, tạo ra tổng cộng 70 video huấn luyện, 30 video kiểm tra.

2) Phân đoạn tổn thương vú siêu âm video: Chúng tôi tiến hành thí nghiệm trên tập dữ liệu BUV2022 [27] bao gồm 63 chuỗi video, với một chuỗi video cho mỗi người, chứa 4619 khung hình đã được chú thích với ground truth cấp pixel bởi các chuyên gia. Theo cách tiếp cận được nêu trong [27], các chuỗi video với độ phân giải không gian dao động từ 580×600 đến 600×800 được cắt thêm đến độ phân giải không gian 300×200. Chúng tôi tuân theo các phân chia chính thức để huấn luyện và kiểm tra.

3) Phân đoạn polyp video: Chúng tôi sử dụng bốn tập dữ liệu polyp được sử dụng rộng rãi, bao gồm Kvasir dựa trên hình ảnh [40] và CVC-300 dựa trên video [41], CVC-612 [42] và ASU-Mayo [43]. Theo cùng giao thức với [44], chúng tôi huấn luyện mô hình của chúng tôi trên Kvasir, ASU-Mayo và các tập huấn luyện của CVC-300 và CVC-612, và tiến hành ba thí nghiệm trên các tập dữ liệu kiểm tra CVC-300-TV, CVC-612-V và CVC-612-T.

B. Chi tiết triển khai
Khung được đề xuất được huấn luyện trên một GPU NVIDIA RTX 4090 và được triển khai trên nền tảng Pytorch. Khung của chúng tôi được huấn luyện thực nghiệm trong 100 epoch theo cách end-to-end và bộ tối ưu hóa Adam được áp dụng. Tốc độ học ban đầu được đặt thành 1×10^-4 và giảm xuống 1×10^-6. Trong quá trình huấn luyện, chúng tôi thay đổi kích thước các khung hình video thành 256×256, và đưa một batch gồm 4 clip video, mỗi clip có 5 khung hình, vào mạng cho mỗi lần lặp.

C. So sánh với Các Phương pháp Khác
1) Kết quả về phân đoạn video siêu âm tuyến giáp và tổn thương vú: Chúng tôi sử dụng bốn chỉ số đánh giá phân đoạn, bao gồm Dice, Jaccard, Precision và Recall; để biết định nghĩa chính xác, vui lòng tham khảo [19]. Chúng tôi cũng báo cáo hiệu suất tốc độ suy luận bằng cách tính số khung hình trên giây (FPS).

Như thể hiện trong Bảng I, chúng tôi so sánh định lượng phương pháp của chúng tôi với nhiều phương pháp tiên tiến trên tập dữ liệu VTUS và tập dữ liệu BUV2022. Những phương pháp này bao gồm các phương pháp phân đoạn hình ảnh y tế phổ biến (UNet [37], UNet++ [3], TransUNet [18], SETR [8], DAF [19]), và các phương pháp phân đoạn đối tượng video (OSVOS [38], ViViT [9], STM [24], AFB-URR [10], DPSTT [27], FLA-Net [28], RMem [39]). Để công bằng trong so sánh, chúng tôi tái tạo những phương pháp này theo các mã nguồn có sẵn công khai của chúng. Lưu ý rằng chúng tôi đã sử dụng Vision Transformer làm backbone của FLA-Net. Chúng tôi có thể quan sát thấy rằng các phương pháp dựa trên video có xu hướng vượt trội so với các phương pháp dựa trên hình ảnh như được chứng minh bởi hiệu suất tốt hơn của chúng. Điều này cho thấy rằng việc khám phá thông tin thời gian mang lại lợi thế đáng kể cho việc phân đoạn nốt giáp và tổn thương vú trong video siêu âm. Quan trọng hơn, trong tất cả các phương pháp phân đoạn dựa trên hình ảnh và video, Vivim của chúng tôi đã đạt được hiệu suất cao nhất trên tất cả các điểm số với một biên độ đáng kể (ví dụ: 2.61%, 2.74% trong Dice, Jaccard trên VTUS, 1.01%, 0.86% trong Dice, Jaccard trên BUV2022 so với phương pháp tốt thứ hai DPSTT). Vivim của chúng tôi cũng có thời gian chạy tốt nhất trong tất cả các phương pháp dựa trên video được quan sát từ FPS. Điều này chứng minh rằng giải pháp của chúng tôi có thể đồng thời học các tín hiệu không gian và thời gian một cách hiệu quả, và đạt được những cải thiện đáng kể so với những phương pháp Transformer, như SETR, ViViT và DPSTT. Như hiển thị trong Hình. 5, chúng tôi trực quan hóa kết quả phân đoạn tuyến giáp trên các khung hình được chọn. Mô hình của chúng tôi có thể định vị và phân đoạn tốt hơn các tổn thương mục tiêu với ranh giới chính xác hơn.

--- TRANG 7 ---
VOL. XX, NO. XX, XXXX 2022 7
BẢNG I
SO SÁNH ĐỊNH LƯỢNG VỚI CÁC PHƯƠNG PHÁP TIÊN TIẾN TRÊN TẬP DỮ LIỆU VTUS CỦA CHÚNG TÔI (NỐT GIÁP) VÀ TẬP DỮ LIỆU BUV2022 (TỔN THƯƠNG VÚ). DICE, JACCARD, PRECISION VÀ RECALL ĐƯỢC SỬ DỤNG LÀM CÁC CHỈ SỐ ĐÁNH GIÁ CỦA CHÚNG TÔI. CÁC ĐIỂM SỐ TỐT NHẤT ĐƯỢC TÔĐẬM.

Phương pháp Venue Loại VTUS BUV2022 FPS Dice Jaccard Precision Recall Dice Jaccard Precision Recall
UNet [37] MICCAI15 hình ảnh 0.6662 0.5328 0.6703 0.7471 0.7303 0.6247 0.7946 0.7272 88.18
UNet++ [3] DLMIA18 hình ảnh 0.7656 0.6486 0.7441 0.8496 0.7179 0.6124 0.8280 0.6884 40.90
TransUNet [18] arXiv21 hình ảnh 0.7461 0.6250 0.7468 0.8321 0.6547 0.5358 0.7167 0.6682 65.10
SETR [8] CVPR21 hình ảnh 0.7288 0.6010 0.7399 0.8089 0.6649 0.5480 0.7533 0.6643 21.61
DAF [19] MICCAI18 hình ảnh 0.7716 0.6583 0.7046 0.8599 0.7890 0.6954 0.7992 0.7979 47.62
OSVOS [38] CVPR17 video 0.7769 0.6754 0.7895 0.8241 0.7098 0.5674 0.7778 0.6404 27.25
ViViT [9] ICCV21 video 0.7610 0.6459 0.7789 0.8252 0.6739 0.5446 0.7554 0.6683 24.33
STM [24] ICCV19 video 0.7898 0.6897 0.8112 0.8251 0.7862 0.6858 0.8201 0.7910 23.17
AFB-URR [10] NIPS20 video 0.7930 0.6957 0.7764 0.8429 0.8018 0.7034 0.8008 0.8591 11.84
DPSTT [27] MICCAI22 video 0.8063 0.7117 0.8238 0.8352 0.8255 0.7364 0.8389 0.8455 30.50
FLA-Net [28] MICCAI23 video 0.8042 0.7075 0.8121 0.8276 0.8232 0.7315 0.8334 0.8422 31.22
RMem [39] CVPR24 video 0.7804 0.6775 0.7821 0.8298 0.7912 0.6901 0.8024 0.8221 29.54
Phương pháp của chúng tôi - - video 0.8324 0.7391 0.8363 0.8711 0.8356 0.7450 0.8357 0.8869 35.33

Hình 5. So sánh trực quan về phân đoạn tuyến giáp siêu âm video với một số phương pháp cạnh tranh dựa trên hình ảnh và video. Kết quả liên tiếp của một trường hợp được hiển thị.

2) Phân đoạn polyp video: Chúng tôi sử dụng sáu chỉ số theo [44], tức là maximum Dice (maxDice), maximum specificity (maxSpe), maximum IoU (maxIoU), S-measure [49] (S_α), E-measure [50] (E_ϕ), và mean absolute error (MAE).

Chúng tôi so sánh phương pháp của chúng tôi với các phương pháp hiện có như được tóm tắt trong Bảng II, bao gồm UNet [37], UNet++ [3], ResUNet [45], ACSNet [46], PraNet [47], PNS-Net [44], LDNet [48] và FLA-Net [28]. Chúng tôi tiến hành ba thí nghiệm trên CVC-300-TV, CVC-612-V và CVC-612-T để xác thực hiệu suất của mô hình. CVC-300-TV bao gồm cả tập validation và test bao gồm sáu video tổng cộng, trong khi CVC-612-V và CVC-612-T mỗi tập chứa năm video. Trên CVC-300-TV, Vivim của chúng tôi đạt được hiệu suất đáng chú ý và vượt trội so với tất cả các phương pháp với một biên độ lớn (ví dụ: 2.7% trong maxDice, 2.2% trong maxIoU). Trên CVC-612-V và CVC-612-T, Vivim của chúng tôi liên tục vượt trội so với các SOTA khác, đặc biệt là 1.2% và 1.1% trong maxDice, tương ứng. Chúng tôi cũng trực quan hóa kết quả phân đoạn polyp trên các khung hình liên tiếp của CVC-612-T trong Hình. 6. Mô hình của chúng tôi chứng minh khả năng cải thiện trong việc định vị và phân đoạn polyp với ranh giới chính xác hơn.

D. Nghiên cứu Ablation
Các thí nghiệm mở rộng được tiến hành trên tập dữ liệu VTUS để đánh giá tính hiệu quả của các thành phần chính của chúng tôi. Để làm điều này, chúng tôi xây dựng bốn mạng baseline từ phương pháp của chúng tôi. Baseline đầu tiên (được ký hiệu là "basic") là loại bỏ tất cả các lớp Mamba và ràng buộc affine nhận biết biên giới khỏi mạng của chúng tôi. Có nghĩa là "basic" bằng với SegFormer thông thường [33]. Sau đó, chúng tôi giới thiệu các lớp ST-Mamba với SSM tiến thời gian (Tf) vào "basic" để xây dựng một mạng baseline khác "C1", và tiếp tục trang bị ST-Mamba với SSM lùi thời gian (Tb) để xây dựng một mạng baseline "C2". Dựa trên "C2", SSM không gian (S) được kết hợp vào ST-Mamba để xây dựng "C3". Do đó, "C3" bằng với việc loại bỏ ràng buộc affine nhận biết biên giới khỏi quá trình huấn luyện mạng của chúng tôi. Bảng III báo cáo kết quả của phương pháp chúng tôi và bốn mạng baseline. Trong khi "basic" hoạt động cạnh tranh do trọng số SegFormer được huấn luyện trước trên ADE20K, các mô-đun được đề xuất của chúng tôi tiến bộ đáng kể về tính hiệu quả của nó. So với "basic", "C1" có sự cải thiện lớn trên tất cả các chỉ số, điều này cho thấy rằng SSM thông thường giúp khám phá phụ thuộc thời gian, do đó cải thiện hiệu suất phân đoạn trong video. Sau đó, kết quả Dice và Jaccard tốt hơn của "C2" so với "C1" chứng minh rằng việc giới thiệu SSMs thời gian hai chiều của chúng tôi có thể mang lại lợi ích quan trọng cho tính kết hợp giữa các khung hình. Hơn nữa, bằng cách thích ứng SSMs với thông tin không nhân quả, "C3" tiến bộ so với "C2" với một biên độ đáng kể 0.46% trong Dice và 0.83% trong Recall. Cuối cùng, phương pháp của chúng tôi vượt trội so với "C3" về Dice, Jaccard và Precision, điều này cho thấy rằng ràng buộc affine nhận biết biên giới có thể giúp tăng cường thêm kết quả phân đoạn tuyến giáp.

--- TRANG 8 ---
VOL. XX, NO. XX, XXXX 2022 8
BẢNG II
KẾT QUẢ ĐỊNH LƯỢNG TRÊN BA TẬP DỮ LIỆU POLYP VIDEO. CÁC ĐIỂM SỐ TỐT NHẤT ĐƯỢC TÔĐẬM. ↑CHỈ CÁO ĐIỂM SỐ CÀNG CAO CÀNG TỐT, VÀ NGƯỢC LẠI.

UNet UNet++ ResUNet ACSNet PraNet PNS-Net LDNet FLA-Net Vivim
Chỉ số MICCAI [37] TMI [3] ISM [45] MICCAI [46] MICCAI [47] MICCAI [44] MICCAI [48] MICCAI [28] (Của chúng tôi)
CVC-300-TV
maxDice ↑ 0.639 0.649 0.535 0.738 0.739 0.840 0.835 0.874 0.901
maxSpe↑ 0.963 0.944 0.852 0.987 0.993 0.996 0.994 0.996 0.997
maxIoU↑ 0.525 0.539 0.412 0.632 0.645 0.745 0.741 0.789 0.831
S_α↑ 0.793 0.796 0.703 0.837 0.833 0.909 0.898 0.907 0.928
E_ϕ↑ 0.826 0.831 0.718 0.871 0.852 0.921 0.910 0.969 0.958
MAE↓ 0.027 0.024 0.052 0.016 0.016 0.013 0.015 0.010 0.008

CVC-612-V
maxDice ↑ 0.725 0.684 0.752 0.804 0.869 0.873 0.870 0.885 0.897
maxSpe↑ 0.971 0.952 0.939 0.929 0.983 0.991 0.987 0.992 0.996
maxIoU↑ 0.610 0.570 0.648 0.712 0.799 0.800 0.799 0.814 0.829
S_α↑ 0.826 0.805 0.829 0.847 0.915 0.923 0.918 0.920 0.940
E_ϕ↑ 0.855 0.830 0.877 0.887 0.936 0.944 0.941 0.963 0.971
MAE↓ 0.023 0.025 0.023 0.054 0.013 0.012 0.013 0.012 0.010

CVC-612-T
maxDice ↑ 0.729 0.740 0.617 0.782 0.852 0.860 0.857 0.861 0.872
maxSpe↑ 0.971 0.975 0.950 0.975 0.986 0.992 0.988 0.993 0.995
maxIoU↑ 0.635 0.635 0.514 0.700 0.786 0.795 0.791 0.795 0.810
S_α↑ 0.810 0.800 0.727 0.838 0.886 0.903 0.892 0.904 0.915
E_ϕ↑ 0.836 0.817 0.758 0.864 0.904 0.903 0.903 0.904 0.921
MAE↓ 0.058 0.059 0.084 0.053 0.038 0.038 0.037 0.036 0.033

(a) Thời gian khung hình
(b) GT (c) Của chúng tôi (d) PNSNet (e) PraNet (f) ACSNet (g) ResUNet (h) Unet++ (i) UNet
Hình 6. Kết quả định tính trên các khung hình được chọn của CVC-612-T. Vivim của chúng tôi có thể định vị và phân đoạn polyp tốt hơn với ranh giới chính xác hơn so với một số phương pháp cạnh tranh dựa trên hình ảnh và video.

BẢNG III
NGHIÊN CỨU ABLATION VỀ THIẾT KẾ VIVIM CỦA CHÚNG TÔI TRÊN TẬP DỮ LIỆU VTUS. TRONG ST-MAMBA, Tf BIỂU THỊ SSM TIẾN THỜI GIAN, Tb BIỂU THỊ SSM LÙI THỜI GIAN, S BIỂU THỊ SSM KHÔNG GIAN, TRONG KHI BAC BIỂU THỊ RÀNG BUỘC AFFINE NHẬN BIẾT BIÊN GIỚI.

ST-Mamba BAC VTUS
Tf Tb S Dice↑ Jaccard↑ Precision↑ Recall↑
basic - - - - 0.8144 0.7188 0.8040 0.8572
C1 ✓ - - - 0.8159 0.7216 0.8170 0.8704
C2 ✓ ✓ - - 0.8213 0.7264 0.8239 0.8670
C3 ✓ ✓ ✓ - 0.8259 0.7310 0.8255 0.8753
Của chúng tôi ✓ ✓ ✓ ✓ 0.8324 0.7391 0.8363 0.8711

E. Phân tích về Hiệu quả của ST-Mamba
Chúng tôi xác thực hiệu quả cao của ST-Mamba được đề xuất bằng hai nghiên cứu ablation được trình bày trong Bảng IV và Hình. 7. Trong Bảng IV, chúng tôi so sánh với một số mô-đun cốt lõi để mô hình hóa phụ thuộc không-thời gian, tức là tự chú ý thông thường [7], tự chú ý cửa sổ [51] và tự chú ý nhân tử hóa [9]. Chúng tôi thay thế ST-Mamba trong Vivim đầy đủ của chúng tôi bằng ba mô-đun cốt lõi để xây dựng ba biến thể M1, M2, và M3, tương ứng. Chúng tôi đánh giá hiệu quả của những mô hình này bằng một A6000 48G duy nhất, xem xét Training Memory (TM), Inference Memory (IM), và Run-time như các chỉ số chính. M1, kết hợp tự chú ý toàn cục 3D để nắm bắt thông tin không gian và thời gian đồng thời, gặp thách thức do các ràng buộc bộ nhớ khi xử lý một clip video 32 khung hình ở độ phân giải 256×256. Ngược lại, M2 và M3 thỏa hiệp về trường tiếp nhận để đảm bảo rằng mô hình hóa không-thời gian có thể được tiến hành trong khả năng bộ nhớ có sẵn. Thay vào đó, phương pháp của chúng tôi giới thiệu một mô-đun mô hình hóa toàn cục hiệu quả dựa trên Mamba, dẫn đến hiệu suất vượt trội về bộ nhớ huấn luyện, bộ nhớ suy luận, và thời gian chạy trung bình khi so sánh với các biến thể mô hình khác.

Hình. 7 hiển thị hệ số Dice và chi phí bộ nhớ với số lượng khung hình tăng trong một clip video ở giai đoạn suy luận. Chúng tôi đánh giá M1 và phương pháp của chúng tôi, tức là tự chú ý không-thời gian và Mamba không-thời gian, để xác minh hiệu quả của mô hình chúng tôi. Như quan sát, M1 có xu hướng duy trì và thậm chí giảm hiệu suất phân đoạn khi tham chiếu đến nhiều khung hình lân cận hơn. Thay vào đó, phương pháp của chúng tôi có được khoảng 0.2% cải thiện trong hệ số Dice. Hơn nữa, việc tăng số lượng khung hình đưa ra sự tăng trưởng bùng nổ trong chi phí bộ nhớ cho M1. Phương pháp của chúng tôi có thể suy luận một clip video hơn 150 khung hình bằng một RTX4090 duy nhất. Điều này cung cấp một nền tảng tuyệt vời cho phân đoạn video y tế dài hơn trong khả năng bộ nhớ hạn chế.

--- TRANG 9 ---
VOL. XX, NO. XX, XXXX 2022 9
BẢNG IV
NGHIÊN CỨU ABLATION CHO CÁC MÔ-ĐUN CHÚ Ý KHÁC NHAU. CHÚNG TÔI ĐƯA MỘT CLIP VIDEO 32 KHUNG HÌNH VỚI 256 P VÀO BA BIẾN THỂ MÔ HÌNH VÀ PHƯƠNG PHÁP CỦA CHÚNG TÔI. "TM" BIỂU THỊ BỘ NHỚ HUẤN LUYỆN, "IM" BIỂU THỊ BỘ NHỚ SUY LUẬN, VÀ "OOM" BIỂU THỊ HẾT BỘ NHỚ. "ISGLOBAL" MÔ TẢ LIỆU CÁC MÔ-ĐUN CỐT LÕI CÓ PHẢI LÀ NHỮNG MÔ-ĐUN MÔ HÌNH HÓA TOÀN CỤC HAY KHÔNG.

Phương pháp Mô-đun Cốt lõi Kích thước Đầu vào TM (M) IM (M) Thời gian chạy (s) Là Toàn cục
M1 Tự chú ý không-thời gian 32×256² OOM - - ✓
M2 Tự chú ý cửa sổ không-thời gian 32×256² 25,861 7,795 0.142 ✗
M3 Tự chú ý nhân tử hóa không-thời gian 32×256² 29,110 9,288 0.156 ✗
Phương pháp của chúng tôi Mamba không-thời gian 32×256² 19,216 5,112 0.121 ✓

Hình 7. ST-Mamba hoạt động tốt hơn về hiệu quả và hiệu suất khi giải quyết mô hình hóa chuỗi dài. (a) Nhiều khung hình tham chiếu hơn có thể giúp cải thiện hiệu suất phân đoạn của ST-Mamba, nhưng không áp dụng được cho tự chú ý không-thời gian. (b) Vivim có gánh nặng bộ nhớ nhẹ hơn so với các phương pháp dựa trên attention truyền thống khi tăng độ dài chuỗi.

V. THẢO LUẬN
Vivim của chúng tôi được thiết kế xung quanh quá trình chẩn đoán của các bác sĩ X quang để có được hiểu biết toàn diện bằng ST-Mamba, và bảo tồn các chi tiết cục bộ bằng bộ giải mã dựa trên CNN tổng hợp các đặc trưng đa tỷ lệ và ràng buộc biên giới. Mặc dù các mạng nơ-ron tích chập (CNNs) và Transformers đã đạt được hiệu suất ấn tượng cho nhiều tác vụ phân đoạn video siêu âm [27], [28], vẫn còn tiềm năng đáng kể để tăng cường cả hiệu quả và hiệu suất. Một thách thức quan trọng hạn chế ứng dụng rộng rãi hơn của CNNs và Transformers trong phân tích video y tế là sự đánh đổi giữa trường tiếp nhận và độ phức tạp tính toán. Vấn đề này phát sinh từ bản chất xử lý cục bộ vốn có của CNNs và độ phức tạp tính toán cao liên quan đến Transformers. Mô hình Không gian Trạng thái (SSMs), ví dụ Mamba, cung cấp một kỹ thuật hiệu quả hơn để mô hình hóa phụ thuộc toàn cục so với Transformers, tạo điều kiện cho các tham chiếu động hơn trong video siêu âm. Các chuyên gia siêu âm thường có được sự xuất hiện hoàn chỉnh của các mô mục tiêu bằng cách sử dụng cả góc nhìn ngang và dọc, được chụp bởi các thiết bị tốc độ khung hình cao. Điều này dẫn đến yêu cầu mô hình hóa chuỗi dài trong video siêu âm. Không giống như cơ chế tự chú ý trong Transformers [7], [9], tỷ lệ bậc hai với độ dài chuỗi video, độ phức tạp tính toán của SSMs tỷ lệ tuyến tính. Khả năng mở rộng tuyến tính này làm cho SSMs phù hợp tốt cho mô hình hóa kết hợp không-thời gian, cho phép chúng hoạt động trong các ràng buộc của bộ nhớ và tài nguyên tính toán hạn chế, một thành tựu thách thức cho Transformers, đặc biệt với các chuỗi video dài hơn.

Bản chất nhân quả của SSMs đặc biệt phù hợp với các tác vụ trong Xử lý Ngôn ngữ Tự nhiên (NLP) và xử lý video, nơi hiểu ngữ cảnh trong dữ liệu văn bản và thời gian là quan trọng [11]. Một thách thức chính trong việc thích ứng mô hình Mamba cho các tác vụ video y tế nằm ở việc thiết kế các hướng quét chọn lọc hiệu quả bảo tồn các chi tiết không gian không nhân quả của tổn thương và mô trong khi khám phá các phụ thuộc thời gian. Do đó, cách tiếp cận của chúng tôi vượt xa việc áp dụng đơn giản Mamba; chúng tôi thiết lập một baseline kết hợp Transformers để mô hình hóa không gian với SSMs để mô hình hóa không-thời gian. Chúng tôi giới thiệu một cơ chế quét ba hướng hoạt động đồng thời dọc theo hướng tiến thời gian, lùi thời gian, và tiến không gian, cân bằng cẩn thận tính kết hợp giữa các khung hình với tính toàn vẹn không gian đơn khung hình. Trong tương lai, trọng tâm của chúng tôi sẽ là điều tra thêm một giải pháp hoàn toàn dựa trên SSM để phân đoạn video y tế hiệu quả và phát triển các cơ chế quét chọn lọc sáng tạo hơn, được thiết kế riêng cho các tác vụ phân đoạn video y tế để định vị tổn thương tốt hơn. Chúng tôi sẽ cố gắng bảo tồn tốt hơn mối tương quan không gian giữa các patch lân cận của tổn thương mục tiêu khi tiến hành tương tác chuỗi 1-D.

--- TRANG 10 ---
VOL. XX, NO. XX, XXXX 2022 10
VI. KẾT LUẬN
Trong bài báo này, chúng tôi trình bày một khung dựa trên Mamba Vivim để giải quyết các thách thức của phân đoạn video y tế, đặc biệt trong việc mô hình hóa các phụ thuộc thời gian tầm xa do tính cục bộ vốn có của CNNs và độ phức tạp tính toán cao của cơ chế tự chú ý. Ý tưởng chính của Vivim là giới thiệu các mô hình không gian trạng thái có cấu trúc với quét chọn lọc không-thời gian, ST-Mamba, vào kiến trúc Transformer phân cấp tiêu chuẩn. Điều này tạo điều kiện khám phá tính kết hợp không gian đơn khung hình và tính kết hợp giữa các khung hình một cách rẻ hơn về mặt tính toán so với việc sử dụng cơ chế tự chú ý. Một ràng buộc nhận biết biên giới cải tiến ở giai đoạn huấn luyện được đề xuất để giảm thiểu dự đoán mơ hồ của mô hình chúng tôi. Chúng tôi cũng đóng góp một tập dữ liệu phân đoạn siêu âm tuyến giáp video VTUS với 100 video và 9342 khung hình được chú thích. Kết quả thực nghiệm trên tập dữ liệu VTUS đã thu thập của chúng tôi, video tổn thương vú siêu âm và video polyp nội soi đại tràng tiết lộ rằng Vivim vượt trội so với các mạng phân đoạn tiên tiến. Các nghiên cứu ablation cũng xác thực hiệu quả vượt trội của ST-Mamba so với các phương pháp dựa trên Transformer không-thời gian khác.

TÀI LIỆU THAM KHẢO
[1] Q. Huang, Y. Huang, Y. Luo, F. Yuan, và X. Li, "Segmentation of breast ultrasound image with semantic classification of superpixels," Medical Image Analysis, vol. 61, p. 101657, 2020.
[2] Z. Lin, J. Lin, L. Zhu, H. Fu, J. Qin, và L. Wang, "A new dataset and a baseline model for breast lesion detection in ultrasound videos," trong MICCAI. Springer, 2022, pp. 614-623.
[3] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, và J. Liang, "Unet++: A nested u-net architecture for medical image segmentation," trong Deep learning in medical image analysis and multimodal learning for clinical decision support. Springer, 2018, pp. 3-11.
[4] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.
[5] K. He, G. Gkioxari, P. Dollár, và R. Girshick, "Mask r-cnn," trong Proceedings of the IEEE international conference on computer vision, 2017, pp. 2961-2969.
[6] Y. Yang, S. Wang, L. Zhu, và L. Yu, "Hcdg: A hierarchical consistency framework for domain generalization on medical image segmentation," arXiv preprint arXiv:2109.05742, 2021.
[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[8] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr et al., "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 6881-6890.
[9] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, và C. Schmid, "Vivit: A video vision transformer," trong Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 6836-6846.
[10] Y. Liang, X. Li, N. Jafari, và J. Chen, "Video object segmentation with adaptive feature bank and uncertain-region refinement," Advances in Neural Information Processing Systems, vol. 33, pp. 3430-3441, 2020.
[11] A. Gu và T. Dao, "Mamba: Linear-time sequence modeling with selective state spaces," arXiv preprint arXiv:2312.00752, 2023.
[12] R. E. Kalman, "A new approach to linear filtering and prediction problems," 1960.
[13] J. Ma, F. Li, và B. Wang, "U-mamba: Enhancing long-range dependency for biomedical image segmentation," arXiv preprint arXiv:2401.04722, 2024.
[14] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, và X. Wang, "Vision mamba: Efficient visual representation learning with bidirectional state space model," arXiv preprint arXiv:2401.09417, 2024.
[15] K. He, C. Gan, Z. Li, I. Rekik, Z. Yin, W. Ji, Y. Gao, Q. Wang, J. Zhang, và D. Shen, "Transformers in medical image analysis," Intelligent Medicine, vol. 3, no. 1, pp. 59-78, 2023.

--- TRANG 11 ---
VOL. XX, NO. XX, XXXX 2022 11
[16] Y. He, V. Nath, D. Yang, Y. Tang, A. Myronenko, và D. Xu, "Swinunetr-v2: Stronger swin transformers with stagewise convolutions for 3d medical image segmentation," trong International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 416-426.
[17] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, và D. Xu, "Unetr: Transformers for 3d medical image segmentation," trong Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 574-584.
[18] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, và Y. Zhou, "Transunet: Transformers make strong encoders for medical image segmentation," arXiv preprint arXiv:2102.04306, 2021.
[19] Y. Wang, Z. Deng, X. Hu, L. Zhu, X. Yang, X. Xu, P.-A. Heng, và D. Ni, "Deep attentional features for prostate segmentation in ultrasound," trong Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part IV 11. Springer, 2018, pp. 523-530.
[20] S. Ali, Y. Espinel, Y. Jin, P. Liu, B. Güttner, X. Zhang, L. Zhang, T. Dowrick, M. J. Clarkson, S. Xiao et al., "An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion," arXiv preprint arXiv:2401.15753, 2024.
[21] J. M. Webb, D. D. Meixner, S. A. Adusei, E. C. Polley, M. Fatemi, và A. Alizad, "Automatic deep learning semantic segmentation of ultrasound thyroid cineclips using recurrent fully convolutional networks," IEEE Access, vol. 9, pp. 5119-5127, 2020.
[22] L. Ma, G. Tan, H. Luo, Q. Liao, S. Li, và K. Li, "A novel deep learning framework for automatic recognition of thyroid gland and tissues of neck in ultrasound image," IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 9, pp. 6113-6124, 2022.
[23] J. Chi, Z. Li, Z. Sun, X. Yu, và H. Wang, "Hybrid transformer unet for thyroid segmentation from ultrasound scans," Computers in Biology and Medicine, vol. 153, p. 106453, 2023.
[24] S. W. Oh, J.-Y. Lee, N. Xu, và S. J. Kim, "Video object segmentation using space-time memory networks," trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9226-9235.
[25] H. K. Cheng, Y.-W. Tai, và C.-K. Tang, "Rethinking space-time networks with improved memory coverage for efficient video object segmentation," Advances in Neural Information Processing Systems, vol. 34, pp. 11 781-11 794, 2021.
[26] K. Park, S. Woo, S. W. Oh, I. S. Kweon, và J.-Y. Lee, "Per-clip video object segmentation," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 1352-1361.
[27] J. Li, Q. Zheng, M. Li, P. Liu, Q. Wang, L. Sun, và L. Zhu, "Rethinking breast lesion segmentation in ultrasound: A new video dataset and a baseline network," trong MICCAI. Springer, 2022, pp. 391-400.
[28] J. Lin, Q. Dai, L. Zhu, H. Fu, Q. Wang, W. Li, W. Rao, X. Huang, và L. Wang, "Shifting more attention to breast lesion segmentation in ultrasound videos," trong International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 497-507.
[29] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, và C. Ré, "Combining recurrent, convolutional, and continuous-time models with linear state space layers," Advances in neural information processing systems, vol. 34, pp. 572-585, 2021.
[30] A. Gu, K. Goel, và C. Ré, "Efficiently modeling long sequences with structured state spaces," arXiv preprint arXiv:2111.00396, 2021.
[31] E. Nguyen, K. Goel, A. Gu, G. Downs, P. Shah, T. Dao, S. Baccus, và C. Ré, "S4nd: Modeling images and videos as multidimensional signals with state spaces," Advances in neural information processing systems, vol. 35, pp. 2846-2861, 2022.
[32] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, và Y. Liu, "Vmamba: Visual state space model," arXiv preprint arXiv:2401.10166, 2024.
[33] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, và P. Luo, "Segformer: Simple and efficient design for semantic segmentation with transformers," Advances in Neural Information Processing Systems, vol. 34, pp. 12 077-12 090, 2021.
[34] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, và L. Shao, "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions," trong Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 568-578.
[35] S. Borse, Y. Wang, Y. Zhang, và F. Porikli, "Inverseform: A loss function for structured boundary-aware segmentation," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 5901-5911.
[36] J. Canny, "A computational approach to edge detection," IEEE Transactions on pattern analysis and machine intelligence, no. 6, pp. 679-698, 1986.
[37] O. Ronneberger, P. Fischer, và T. Brox, "U-net: Convolutional networks for biomedical image segmentation," trong International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2015, pp. 234-241.
[38] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, và A. Sorkine-Hornung, "Learning video object segmentation from static images," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2663-2672.
[39] J. Zhou, Z. Pang, và Y.-X. Wang, "Rmem: Restricted memory banks improve video object segmentation," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 602-18 611.
[40] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen, và H. D. Johansen, "Kvasir-seg: A segmented polyp dataset," trong MultiMedia Modeling: 26th International Conference, MMM 2020, Daejeon, South Korea, January 5-8, 2020, Proceedings, Part II 26. Springer, 2020, pp. 451-462.
[41] J. Bernal, J. Sánchez, và F. Vilarino, "Towards automatic polyp detection with a polyp appearance model," Pattern Recognition, vol. 45, no. 9, pp. 3166-3182, 2012.
[42] J. Bernal, F. J. Sánchez, G. Fernández-Esparrach, D. Gil, C. Rodríguez, và F. Vilariño, "Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians," Computerized medical imaging and graphics, vol. 43, pp. 99-111, 2015.
[43] N. Tajbakhsh, S. R. Gurudu, và J. Liang, "Automated polyp detection in colonoscopy videos using shape and context information," IEEE transactions on medical imaging, vol. 35, no. 2, pp. 630-644, 2015.
[44] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, H. Fu, D. Jha, và L. Shao, "Progressively normalized self-attention network for video polyp segmentation," trong MICCAI. Springer, 2021, pp. 142-152.
[45] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange, P. Halvorsen, và H. D. Johansen, "Resunet++: An advanced architecture for medical image segmentation," trong 2019 IEEE international symposium on multimedia (ISM). IEEE, 2019, pp. 225-2255.
[46] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, và Y. Yu, "Adaptive context selection for polyp segmentation," trong MICCAI. Springer, 2020, pp. 253-262.
[47] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, và L. Shao, "Pranet: Parallel reverse attention network for polyp segmentation," trong MICCAI. Springer, 2020, pp. 263-273.
[48] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, và G. Li, "Lesion-aware dynamic kernel for polyp segmentation," trong International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 99-109.
[49] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, và A. Borji, "Structure-measure: A new way to evaluate foreground maps," trong Proceedings of the IEEE international conference on computer vision, 2017, pp. 4548-4557.
[50] D.-P. Fan, G.-P. Ji, X. Qin, và M.-M. Cheng, "Cognitive vision inspired object segmentation metric and loss function," Scientia Sinica Informationis, vol. 6, no. 6, 2021.
[51] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, và H. Hu, "Video swin transformer," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 3202-3211.
