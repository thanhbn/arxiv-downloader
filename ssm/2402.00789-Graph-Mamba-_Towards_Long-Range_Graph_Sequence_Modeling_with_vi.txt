# Graph-Mamba: Hướng tới Mô hình Chuỗi Đồ thị Tầm xa với Không gian Trạng thái Có chọn lọc

Chloe Wang1 2Oleksii Tsepa1 2Jun Ma2 3 4Bo Wang1 2 3 4 5

Tóm tắt
Cơ chế attention đã được sử dụng rộng rãi để nắm bắt các phụ thuộc tầm xa giữa các nút trong Graph Transformers. Bị hạn chế bởi chi phí tính toán bậc hai, các cơ chế attention không thể mở rộng trên các đồ thị lớn. Những cải tiến gần đây về hiệu quả tính toán chủ yếu đạt được thông qua việc thưa thớt hóa attention với việc lấy mẫu phụ đồ thị ngẫu nhiên hoặc dựa trên heuristic, điều này còn thiếu sót trong việc suy luận ngữ cảnh phụ thuộc dữ liệu. Các mô hình không gian trạng thái (SSMs), như Mamba, đã nổi bật vì hiệu quả và hiệu suất trong việc mô hình hóa các phụ thuộc tầm xa trong dữ liệu tuần tự. Tuy nhiên, việc điều chỉnh SSMs cho dữ liệu đồ thị không tuần tự đặt ra một thách thức đáng chú ý. Trong công trình này, chúng tôi giới thiệu Graph-Mamba, nỗ lực đầu tiên để tăng cường mô hình hóa ngữ cảnh tầm xa trong mạng đồ thị bằng cách tích hợp khối Mamba với cơ chế lựa chọn nút phụ thuộc đầu vào. Cụ thể, chúng tôi xây dựng các chiến lược ưu tiên và hoán vị nút tập trung vào đồ thị để tăng cường suy luận nhận thức ngữ cảnh, dẫn đến cải thiện đáng kể về hiệu suất dự đoán. Các thí nghiệm mở rộng trên mười bộ dữ liệu chuẩn chứng minh rằng Graph-Mamba vượt trội so với các phương pháp tiên tiến trong các tác vụ dự đoán đồ thị tầm xa, với một phần nhỏ chi phí tính toán trong cả FLOPs và tiêu thụ bộ nhớ GPU. Mã nguồn và mô hình được công khai tại https://github.com/bowang-lab/Graph-Mamba.

1. Giới thiệu
Mô hình hóa đồ thị đã được sử dụng rộng rãi để xử lý các cấu trúc dữ liệu và mối quan hệ phức tạp, như mạng xã hội (Fan et al., 2019), tương tác phân tử (Tsepa et al., 2023), và kết nối não (Li et al., 2021). Gần đây, Graph Transformers đã ngày càng phổ biến vì khả năng mạnh mẽ trong việc mô hình hóa các kết nối tầm xa giữa các nút (Yun et al., 2019; Dwivedi & Bresson, 2012; Kreuzer et al., 2021a; Chen et al., 2022). Attention điển hình của Graph Transformer cho phép mỗi nút trong đồ thị tương tác với tất cả các nút khác (Vaswani et al., 2017). Điều này bổ sung cho các phương pháp truyền thông điệp cục bộ chủ yếu mã hóa topo lân cận dựa trên cạnh (Kipf & Welling, 2016; Xu et al., 2018). Để đơn giản hóa việc xây dựng Graph Transformers, GraphGPS đã thiết kế một khung thống nhất kết hợp một mô-đun attention, một mạng neural truyền thông điệp (MPNN), và các mã hóa cấu trúc/vị trí (SE/PE). Các thành phần này cộng tác cập nhật các embedding nút và cạnh cho các tác vụ dự đoán hạ nguồn. Các pipeline tách rời như vậy cung cấp cho người dùng sự linh hoạt lớn để kết hợp các mô-đun attention khác nhau theo cách plug-and-play (Rampášek et al., 2022).

Mặc dù Transformers thể hiện những cải tiến đáng chú ý về khả năng mô hình hóa, việc áp dụng chúng cho các chuỗi dài bị cản trở bởi chi phí tính toán bậc hai liên quan đến cơ chế attention. Hạn chế này đã thúc đẩy nghiên cứu thêm về các phương pháp attention thời gian tuyến tính. Ví dụ, BigBird (Zaheer et al., 2020) và Performer (Choromanski et al., 2020) đã cố gắng xấp xỉ attention đầy đủ với attention thưa thớt hoặc ma trận chiều thấp hơn. Tuy nhiên, được thiết kế cho đầu vào tuần tự, BigBird không khái quát hóa tốt cho đầu vào không tuần tự như đồ thị, dẫn đến suy giảm hiệu suất trong GraphGPS (Shirzad et al., 2023). Exphormer đã điều chỉnh các nguyên tắc thưa thớt hóa attention như vậy cho đầu vào đồ thị, bằng cách kết hợp kết nối cục bộ được định nghĩa bởi các cạnh như attention cục bộ (Shirzad et al., 2023). Những điều chỉnh này dẫn đến hiệu suất cải thiện có thể so sánh với attention đồ thị đầy đủ.

Tuy nhiên, xấp xỉ attention đầy đủ, hoặc mã hóa tất cả ngữ cảnh, có thể không lý tưởng cho các phụ thuộc tầm xa. Trong các quan sát thực nghiệm, nhiều mô hình chuỗi không cải thiện với độ dài ngữ cảnh tăng (Gu & Dao, 2023). Mamba, một mô hình không gian trạng thái có chọn lọc (SSM), đã được đề xuất để giải quyết nén ngữ cảnh phụ thuộc dữ liệu trong mô hình hóa chuỗi (Gu & Dao, 2023). Thay vì tính toán attention, Mamba kế thừa cấu trúc của các mô hình không gian trạng thái mã hóa ngữ cảnh sử dụng các trạng thái ẩn trong quá trình quét lặp. Cơ chế lựa chọn cho phép kiểm soát phần nào của đầu vào có thể chảy vào các trạng thái ẩn, như một phần của ngữ cảnh ảnh hưởng đến các cập nhật embedding tiếp theo.

Trong mô hình hóa đồ thị, điều này có thể được xem như một quá trình lựa chọn nút phụ thuộc dữ liệu. Bằng cách lọc các nút liên quan ở mỗi bước lặp và "chú ý" chỉ đến ngữ cảnh đã chọn này, Mamba giúp đạt được các mục tiêu giống như thưa thớt hóa attention, phục vụ như một thay thế cho việc lấy mẫu phụ ngẫu nhiên. Hơn nữa, mô-đun Mamba được tối ưu hóa cho độ phức tạp thời gian tuyến tính và giảm bộ nhớ, cung cấp hiệu quả cải thiện cho các tác vụ huấn luyện đồ thị lớn. Tuy nhiên, thách thức xuất hiện trong việc điều chỉnh hiệu quả Mamba được thiết kế cho mô hình hóa chuỗi sang đầu vào đồ thị không tuần tự.

Đóng góp. Được thúc đẩy bởi khả năng mô hình hóa chuỗi dài đặc biệt của Mamba, chúng tôi đề xuất Graph-Mamba để giảm bớt chi phí tính toán cao liên quan đến Graph Transformers, như một thay thế phụ thuộc dữ liệu cho thưa thớt hóa attention. Cụ thể, công trình này trình bày các đóng góp chính sau:

• Thiết kế mạng đồ thị sáng tạo: Graph-Mamba đại diện cho một loại mạng đồ thị mới tiên phong tích hợp với các mô hình không gian trạng thái có chọn lọc, thực hiện lọc nút phụ thuộc đầu vào và lựa chọn ngữ cảnh thích ứng. Cơ chế lựa chọn nắm bắt các phụ thuộc tầm xa và cải thiện so với các kỹ thuật thưa thớt hóa attention dựa trên lấy mẫu phụ hiện có.

• Điều chỉnh SSMs cho dữ liệu đồ thị không tuần tự: Chúng tôi đã thiết kế một cách thanh lịch để mở rộng các mô hình không gian trạng thái để xử lý dữ liệu đồ thị không tuần tự. Cụ thể, chúng tôi đã giới thiệu một kỹ thuật ưu tiên nút để ưu tiên các nút quan trọng để có nhiều quyền truy cập hơn vào ngữ cảnh, và sử dụng một công thức huấn luyện dựa trên hoán vị để giảm thiểu các thiên lệch liên quan đến chuỗi.

• Hiệu suất và hiệu quả vượt trội: Các thí nghiệm toàn diện trên mười bộ dữ liệu công khai chứng minh rằng Graph-Mamba không chỉ vượt trội so với các baseline mà còn đạt được độ phức tạp tính toán thời gian tuyến tính. Đáng chú ý, Graph-Mamba giảm tiêu thụ bộ nhớ GPU lên đến 74% trên các đồ thị lớn, làm nổi bật hiệu quả của nó trong các bộ dữ liệu đồ thị tầm xa.

2. Công trình liên quan

2.1. Mạng Neural Đồ thị
Mạng Neural Đồ thị (GNNs) tận dụng truyền thông điệp như một cơ chế chính cho mô hình hóa đồ thị, cho phép các nút giao tiếp và tổng hợp thông tin từ các hàng xóm một cách lặp đi lặp lại. Mạng Tích chập Đồ thị (GCN) (Kipf & Welling, 2017; Defferrard et al., 2017) tiên phong GNNs, ảnh hưởng đến các công trình tiếp theo như GraphSage (Hamilton et al., 2018), GIN (Xu et al., 2018), GAT (Veličković et al., 2018), và GatedGCN (Bresson & Laurent, 2018). Mặc dù có ý nghĩa trong việc tổng hợp các đặc trưng nút dựa trên topo đồ thị, MPNNs có sức mạnh biểu đạt hạn chế bị giới hạn trên bởi bài kiểm tra đẳng cấu Weisfeiler-Lehman 1 chiều (1-WL) (Xu et al., 2018). Ngoài ra, các đặc trưng nút được tổng hợp dễ bị over-smoothing trong lân cận cục bộ (Alon & Yahav, 2021; Topping et al., 2022).

2.2. Graph Transformers
Transformers với cơ chế attention đã đạt được thành công chưa từng có trong nhiều lĩnh vực khác nhau, bao gồm xử lý ngôn ngữ tự nhiên (NLP) (Vaswani et al., 2017; Kalyan et al., 2021) và thị giác máy tính (D'Ascoli et al., 2021; Guo et al., 2022; Dosovitskiy et al., 2021). Graph Transformers thường tính toán attention đầy đủ, cho phép mỗi nút chú ý đến tất cả các nút khác, bất kể kết nối cạnh. Điều này cho phép Graph Transformers nắm bắt hiệu quả các phụ thuộc tầm xa, tránh tổng hợp quá mức trong các lân cận cục bộ như MPNNs. Tuy nhiên, attention đầy đủ, với độ phức tạp O(N²), không thể mở rộng trên các đồ thị lớn.

Tương tự như các embedding vị trí trong transformers cho NLP, Graph Transformer đầu tiên (Dwivedi & Bresson, 2021) đã giới thiệu các eigenvector Laplacian đồ thị làm PE nút. Tiếp theo, SAN (Kreuzer et al., 2021b) kết hợp tổng hợp PE bất biến, tích hợp attention có điều kiện cho cả cạnh đồ thị thực và ảo. Đồng thời, Graphormer (Ying et al., 2021) tích hợp PE tương đối vào cơ chế attention sử dụng các mã hóa trung tâm và không gian. GraphiT (Mialon et al., 2021) sử dụng PE tương đối dựa trên kernel khuếch tán để mô phỏng cơ chế attention. Cuối cùng, GraphTrans (Wu et al., 2022) đề xuất một kiến trúc hai giai đoạn, sử dụng một graph transformer trên các embedding cục bộ có nguồn gốc từ MPNNs.

2.3. GraphGPS
GraphGPS (Rampášek et al., 2022) sử dụng một khung mô-đun tích hợp SE, PE, MPNN, và một graph transformer. Người dùng có sự linh hoạt để chọn các phương pháp cho mỗi thành phần trong khung này. Với một đồ thị đầu vào, GraphGPS tính toán SE và PE, nối chúng với các embedding nút và cạnh, và truyền các embedding này vào các lớp GPS. Trong các lớp GPS, một graph transformer và MPNN cộng tác cập nhật các embedding nút và cạnh. Khung GraphGPS cho phép thay thế attention Transformer hoàn toàn kết nối bằng các phương án thưa thớt của nó, dẫn đến độ phức tạp O(N+E).

2.4. Attention Đồ thị Thưa thớt
BigBird (Zaheer et al., 2020) và Performer (Choromanski et al., 2020) là hai phương pháp attention thưa thớt được hỗ trợ bởi GraphGPS. Performer cải thiện hiệu quả tính toán bằng cách sử dụng các đặc trưng ngẫu nhiên trực giao dương chiều thấp hơn để xấp xỉ kernel softmax trong attention thường. BigBird sử dụng lấy mẫu phụ đồ thị và heuristic chuỗi để xấp xỉ attention đầy đủ, kết hợp attention lấy mẫu ngẫu nhiên, attention cục bộ giữa các token liền kề, và attention toàn cục với các token toàn cục (Zaheer et al., 2020). Các đồ thị được lấy mẫu ngẫu nhiên, hoặc expanders, được biết là xấp xỉ các tính chất phổ của đồ thị đầy đủ (Spielman & Teng, 2011; Yun et al., 2020). Tuy nhiên, attention cục bộ của BigBird được thiết kế đặc biệt cho đầu vào chuỗi với cửa sổ trượt trên các token liền kề, làm cho nó không phù hợp để mô hình hóa đầu vào đồ thị. Exphormer (Shirzad et al., 2023) đề xuất một sự điều chỉnh đồ thị của BigBird bằng cách kết hợp attention lân cận cục bộ giữa các hàng xóm được định nghĩa bởi các cạnh, và attention toàn cục kết nối các nút ảo với tất cả các nút, với expanders. Những điều chỉnh này đã cải thiện thêm hiệu suất mô hình trong khi hưởng lợi từ độ phức tạp tuyến tính của attention thưa thớt. Tuy nhiên, quá trình lấy mẫu phụ nút ngẫu nhiên gợi ý tiềm năng cải thiện. Việc kết hợp các phương pháp cho phép lựa chọn ngữ cảnh có thông tin có thể phục vụ như một cải tiến thêm.

2.5. Mô hình Không gian Trạng thái
Các mô hình không gian trạng thái tổng quát liên quan đến các cập nhật lặp qua một chuỗi thông qua các trạng thái ẩn. Các triển khai từ các mô hình Markov ẩn đến mạng neural lặp (RNNs) trong deep learning. Sử dụng quét lặp, SSM lưu trữ ngữ cảnh trong các trạng thái ẩn của nó, và cập nhật đầu ra bằng cách kết hợp các trạng thái ẩn này với đầu vào. Các mô hình không gian trạng thái có cấu trúc (S4) tăng cường hiệu quả tính toán với tham số hóa lại (Gu et al., 2021), cung cấp một thay thế hiệu quả cho tính toán attention. Các biến thể S4 gần đây cho attention thời gian tuyến tính bao gồm H3 (Fu et al., 2022), Gated State Space (Mehta et al., 2022), Hyena (Nguyen et al., 2023), và RWKV (Peng et al., 2023). Mamba tiếp tục giới thiệu một cơ chế lựa chọn phụ thuộc dữ liệu vào S4 để nắm bắt ngữ cảnh tầm xa với độ dài chuỗi tăng (Gu & Dao, 2023). Đáng chú ý, Mamba thể hiện hiệu quả thời gian tuyến tính trong mô hình hóa chuỗi dài, và vượt trội so với Transformers trên nhiều benchmark khác nhau. Mamba cũng đã được điều chỉnh thành công cho đầu vào không tuần tự như hình ảnh trên các tác vụ phân đoạn để tăng cường các phụ thuộc tầm xa (Ma et al., 2024; Zhu et al., 2024; Liu et al., 2024).

3. Graph-Mamba

Graph-Mamba sử dụng một SSM có chọn lọc để đạt được thưa thớt hóa đồ thị phụ thuộc đầu vào. Cụ thể, chúng tôi đã thiết kế một khối Graph-Mamba (GMB) và kết hợp nó vào khung GraphGPS phổ biến, cho phép so sánh công bằng với các triển khai attention đồ thị khác. GMB tận dụng quét lặp trong mô hình hóa chuỗi với cơ chế lựa chọn để đạt được hai mức thưa thớt hóa đồ thị. Mức đầu tiên liên quan đến cơ chế lựa chọn trong mô-đun Mamba, lọc hiệu quả thông tin liên quan trong ngữ cảnh tầm xa. Mức thứ hai được đạt thông qua phương pháp ưu tiên nút được đề xuất, cho phép các nút quan trọng trong đồ thị truy cập nhiều ngữ cảnh hơn. Do đó, các đặc trưng mô hình hóa chuỗi này trình bày một hướng hứa hẹn của việc kết hợp lựa chọn phụ thuộc dữ liệu và heuristic-informed cho thưa thớt hóa đồ thị. Hơn nữa, triển khai Graph-Mamba sử dụng mô-đun Mamba đảm bảo độ phức tạp thời gian tuyến tính, như một thay thế hiệu quả cho attention đồ thị dày đặc.

Để contextualize SSMs trong mô hình hóa đồ thị, chúng tôi trước tiên xem xét SSMs theo sau bởi cơ chế lựa chọn trong Phần 3.1 và 3.2. Tiếp theo, chúng tôi giới thiệu triển khai Graph-Mamba trong Phần 3.3, và chi tiết các kỹ thuật điều chỉnh đồ thị chuyên biệt của GMB trong Phần 3.4 và 3.5. Cuối cùng, chúng tôi thảo luận về hiệu quả tính toán của GMB trong Phần 3.6.

3.1. Mô hình không gian trạng thái có cấu trúc cho mô hình hóa chuỗi

SSM là một loại mô hình chuỗi định nghĩa một Phương trình Vi phân Thường tuyến tính (ODE) để ánh xạ chuỗi đầu vào x(t) trong R^N sang chuỗi đầu ra y(t) trong R^N bởi một trạng thái tiềm ẩn h(t) trong R^N:

h'(t) = Ah(t) + Bx(t),
y(t) = Ch(t),                    (1)

trong đó A trong R^(N×N) và B,C trong R^N biểu thị ma trận trạng thái, ma trận đầu vào, và ma trận đầu ra, tương ứng. Để có được chuỗi đầu ra y(t) tại thời điểm t, chúng ta cần tìm h(t) khó giải phân tích. Ngược lại, dữ liệu thế giới thực thường rời rạc hơn là liên tục. Như một thay thế, chúng ta rời rạc hóa hệ thống Phương trình (1) như sau:

h_t = Āh_(t-1) + B̄x_t,
y_t = Ch_t,                     (2)

trong đó Ā := exp(Δ · A) và B̄ := (Δ · A)^(-1)(exp(Δ · A) - I) · ΔB là các tham số trạng thái rời rạc hóa và Δ là kích thước bước rời rạc hóa. SSMs có các tính chất lý thuyết phong phú nhưng gặp phải chi phí tính toán cao và bất ổn định số. Các mô hình chuỗi không gian trạng thái có cấu trúc (S4) đã giải quyết những hạn chế này bằng cách áp đặt cấu trúc lên ma trận trạng thái A dựa trên ma trận HIPPO, điều này đã cải thiện đáng kể hiệu suất và hiệu quả. Cụ thể, S4 đã vượt qua Transformers với một biên độ lớn trên benchmark Long Range Arena, đòi hỏi mô hình hóa hiệu quả các phụ thuộc tầm xa (Gu et al., 2021).

3.2. Cơ chế lựa chọn phụ thuộc đồ thị

S4 đã chứng minh sự phù hợp tốt hơn cho mô hình hóa các chuỗi dài, nhưng kém hiệu quả khi cần suy luận nhận thức nội dung, do tính chất bất biến thời gian của nó. Cụ thể hơn, A, B, và C giống nhau cho tất cả các token đầu vào trong một chuỗi. Mamba (Gu & Dao, 2023) đã giải quyết vấn đề này bằng cách giới thiệu cơ chế lựa chọn, cho phép mô hình chọn lọc thích ứng thông tin liên quan từ ngữ cảnh. Điều này có thể đạt được bằng cách đơn giản làm cho các tham số SSM B, C, và Δ như các hàm của đầu vào x. Hơn nữa, một triển khai thân thiện với GPU được thiết kế để tính toán hiệu quả cơ chế lựa chọn, giảm đáng kể số lượng IOs bộ nhớ và tránh lưu các trạng thái trung gian.

Chúng tôi sử dụng kích thước bước rời rạc hóa được tham số hóa lại Δ như một ví dụ để minh họa trực giác đằng sau cơ chế lựa chọn của Mamba. Xem xét lại Định lý chính của Gu & Dao (2023), Δ_t giả định một vai trò tổng quát liên quan đến cơ chế gating trong RNN để tạo điều kiện cho lựa chọn phụ thuộc đầu vào, trong đó g_t = sigma(Linear(x_t)) và h_t = (1-g_t)h_(t-1) + g_t x_t, như được chi tiết trong Phụ lục C. Trực giác, đầu vào hiện tại x_t có thể kiểm soát sự cân bằng giữa đầu vào hiện tại và ngữ cảnh trước đó h_(t-1) với g_t khi cập nhật trạng thái ẩn h_t. Điều này được đạt thông qua việc tham số hóa Δ như một hàm của đầu vào trong bước rời rạc hóa để có được Ā và B̄ phụ thuộc dữ liệu, hoạt động như cơ chế lựa chọn chính tương tự như gating trong RNN. Ngoài ra, các ma trận chiếu B và C được tham số hóa như các chiếu tuyến tính của đầu vào x, để kiểm soát thêm mức độ x_t cập nhật các trạng thái ẩn và mức độ h_t ảnh hưởng đến các embedding đầu ra y_t.

Trong học tập đồ thị, với các nút như chuỗi đầu vào, cơ chế lựa chọn cho phép các trạng thái ẩn cập nhật dựa trên các nút liên quan từ chuỗi trước, được gating bởi nút đầu vào hiện tại, và tiếp theo ảnh hưởng đến các embedding đầu ra của nút hiện tại. g_t dao động từ 0 đến 1, cho phép mô hình lọc ra hoàn toàn ngữ cảnh không liên quan khi cần thiết. Khả năng chọn lọc và đặt lại cho phép Mamba chưng cất các phụ thuộc liên quan từ ngữ cảnh tầm xa, trong khi giảm thiểu ảnh hưởng của các nút không quan trọng ở mỗi bước lặp. Do đó nó cung cấp một thay thế nhận thức ngữ cảnh cho việc thưa thớt hóa attention đồ thị, bằng cách chỉ giữ lại các phụ thuộc liên quan trong các chuỗi đầu vào dài.

3.3. Quy trình làm việc Graph-Mamba

Graph-Mamba kết hợp cơ chế lựa chọn của Mamba từ Phần 3.2 vào khung GraphGPS. Hình 1 A minh họa sự điều chỉnh của Graph-Mamba về lớp GPS, trong đó mô-đun attention được thay thế bởi GMB, được ký hiệu là một lớp GMB. Chúng tôi đã sử dụng mô hình GatedGCN làm mặc định cho MPNN cho lựa chọn ngữ cảnh cục bộ, như được hiển thị trong Hình 1 B. Mô hình GatedGCN tổng hợp thông tin từ các nút hàng xóm được định nghĩa bởi các kết nối cạnh, và sử dụng cơ chế gating để quyết định mức độ thông tin đó để kết hợp, lấy cảm hứng từ RNNs. GatedGCN và GMB cùng nhau đóng góp vào chủ đề tổng thể của lựa chọn ngữ cảnh dựa trên lặp trong Graph-Mamba, tạo điều kiện cho việc lọc nút trong lân cận cục bộ và giữa các kết nối toàn cục. Tính toán đặc trưng đồ thị với SE và PE trước các lớp GMB vẫn nhất quán. Do đó các lớp GMB nhận các embedding nút và cạnh nhận thức SE/PE làm đầu vào.

Một khung Graph-Mamba bao gồm K lớp GMB xếp chồng. Thuật toán 1 định nghĩa hàm GMB (giải thích thêm trong Phần 3.4-3.6), và Thuật toán 2 minh họa lần truyền về phía trước qua K lớp GMB. Trong Thuật toán 2, mỗi lớp GMB thực hiện hai vòng cập nhật embedding sử dụng MPNN và GMB, với một đồ thị đầu vào của L nút, E cạnh, và kích thước embedding D. Cụ thể, một MPNN cập nhật cả embedding nút và cạnh (dòng 2), trong khi GMB chỉ cập nhật các embedding nút (dòng 3). Các embedding nút được cập nhật từ một MPNN (X^(k+1)_M) và GMB (X^(k+1)_GMB) được kết hợp thông qua một lớp MLP để tạo ra các embedding nút đầu ra (dòng 6). Sử dụng đầu ra từ lớp trước làm đầu vào cho lớp tiếp theo, quá trình này lặp lại qua L lớp GMB để có được các embedding nút đầu ra cuối cùng, sau đó được sử dụng cho các tác vụ hạ nguồn.

3.4. Chiến lược ưu tiên nút cho đầu vào đồ thị không tuần tự

Một thách thức lớn của việc điều chỉnh các mô hình chuỗi như Mamba sang đồ thị xuất phát từ tính đơn hướng của quét và cập nhật lặp. Trong attention dày đặc, tất cả các nút chú ý lẫn nhau. Tuy nhiên, do tính chất lặp của mô hình hóa chuỗi, trong Mamba, mỗi nút được cập nhật dựa trên các nút đến trước chúng từ các trạng thái ẩn, không phải ngược lại. Ví dụ, trong một chuỗi đầu vào có độ dài L, nút cuối cùng có quyền truy cập vào các trạng thái ẩn kết hợp hầu hết ngữ cảnh bao gồm tất cả các nút trước từ 0 đến L-2. Ngược lại, nút 1 chỉ có quyền truy cập vào ngữ cảnh hạn chế qua các trạng thái ẩn chỉ mã hóa nút 0. Luồng thông tin hạn chế này loại bỏ các kết nối giữa các nút dựa trên vị trí của nó trong chuỗi, cho phép GMB ưu tiên các nút cụ thể có tầm quan trọng cao hơn ở cuối chuỗi cho thưa thớt hóa có thông tin.

Để đạt được thưa thớt hóa có thông tin trong GMB, chúng tôi đã khám phá một chiến lược ưu tiên nút đầu vào bằng heuristic nút là proxy của tầm quan trọng nút, như được minh họa trong Hình 1 C. Khi chúng ta đầu tiên làm phẳng một đồ thị thành một chuỗi, các nút không giả định bất kỳ thứ tự cụ thể nào. Các nút đầu vào sau đó được sắp xếp theo thứ tự tăng dần bởi heuristic nút như bậc nút. Trực giác đằng sau là các nút quan trọng hơn nên có quyền truy cập vào nhiều ngữ cảnh hơn (tức là, một lịch sử dài hơn của các nút trước), và do đó được đặt ở cuối chuỗi. Trong Thuật toán 1, dòng 1-4 minh họa quy trình sắp xếp chuỗi cho một đồ thị đầu vào của L nút, trong đó heuristic nút H' xác định thứ tự nút trong chuỗi được làm phẳng. Dòng 5-16 tính toán SSM có chọn lọc sử dụng Mamba, được giải thích chi tiết hơn trong các phần tiếp theo. Trong dòng 17, đầu ra SSM được sắp xếp ngược để trả về X' được cập nhật theo thứ tự ban đầu. Chi tiết thêm về các lựa chọn khác của heuristic nút và lý luận đằng sau ưu tiên nút được tóm tắt trong Phụ lục D.

3.5. Công thức huấn luyện và suy luận dựa trên hoán vị

Theo chiến lược ưu tiên nút đầu vào, Graph-Mamba sử dụng một công thức huấn luyện và suy luận tập trung hoán vị để thúc đẩy bất biến hoán vị, như được minh họa trong Hình 1 C. Trực giác, khi sắp xếp các nút theo heuristic như bậc nút, các nút trong cùng bậc được coi là quan trọng như nhau trong đồ thị. Do đó, các nút có cùng bậc được trộn ngẫu nhiên trong quá trình huấn luyện để giảm thiểu thiên lệch đối với bất kỳ thứ tự cụ thể nào. Dòng 1 trong Thuật toán 1 minh họa triển khai hoán vị. Cụ thể, nhiễu ngẫu nhiên trong [0,1) được thêm vào heuristic nút H, và H' được jitter xác định thứ tự nút đầu vào.

Trong giai đoạn huấn luyện của Graph-Mamba, GMB được gọi một lần để xuất các embedding nút được cập nhật từ một hoán vị ngẫu nhiên của chuỗi nút đầu vào. Tại thời điểm suy luận, các đầu ra mGMB X̂^(k+1)_GMB được tính trung bình và truyền đến tính toán tiếp theo. Trung bình m-fold tại thời điểm suy luận nhằm mục đích cung cấp sự ổn định, và làm cho các embedding nút đầu ra bất biến với các hoán vị được áp dụng.

3.6. GMB với hiệu quả tính toán cải thiện

Cơ chế lựa chọn của GMB được minh họa trong Hình 1 D, với triển khai Mamba tương ứng được chi tiết trong Thuật toán 1 dòng 5-16. Chuỗi nút được sắp xếp X_sorted bao gồm L nút, mỗi nút với một embedding nút có kích thước D. Tính toán Mamba bao gồm chiếu tuyến tính của đầu vào được chuẩn hóa đến D' chiều (dòng 5, 6), theo sau bởi tích chập 1-D và kích hoạt SiLU (dòng 8), và tính toán SSM (dòng 9-14). Đầu ra SSM y được gating bởi một chiếu của đầu vào ban đầu (dòng 7, 15), trước chiếu cuối cùng trở lại kích thước ban đầu làm đầu ra (dòng 16).

Với cơ chế lựa chọn phụ thuộc dữ liệu, mở rộng L-fold trong các tham số trong Ā, B̄, và C sẽ dẫn đến tăng chi phí tính toán trong SSM. Mamba triển khai một thuật toán nhận thức phần cứng hiệu quả tận dụng phân cấp trong bộ nhớ GPU để giảm bớt chi phí này. Cụ thể, với kích thước batch đầu vào B, Mamba đọc O(BLD' + ND') của đầu vào A, B, C, và Δ từ HBM, tính toán các trạng thái trung gian có kích thước O(BLD'N) trong SRAM và ghi đầu ra cuối cùng có kích thước O(BLD') vào HBM, do đó giảm IOs bởi một hệ số N. Không lưu trữ các trạng thái trung gian cũng giảm tiêu thụ bộ nhớ, trong đó các trạng thái trung gian được tính toán lại cho tính toán gradient trong lần truyền ngược. Với triển khai nhận thức GPU của Mamba, GMB đạt được độ phức tạp thời gian tuyến tính (O(L)) đối với độ dài chuỗi đầu vào, nhanh hơn đáng kể so với tính toán attention dày đặc trong transformers với độ phức tạp thời gian bậc hai (O(L²)).

4. Thí nghiệm

4.1. Benchmark trên các tác vụ dự đoán dựa trên đồ thị

Chúng tôi đã benchmark Graph-Mamba trên mười bộ dữ liệu từ Long Range Graph Benchmark (LRGB) (Dwivedi et al., 2022) và GNN Benchmark (Dwivedi et al., 2023) như một đánh giá toàn diện. Các benchmark này đánh giá hiệu suất mô hình trên nhiều tác vụ dự đoán dựa trên đồ thị khác nhau, bao gồm phân loại và hồi quy cấp đồ thị, nút, và liên kết. Đối với mỗi bộ dữ liệu, chúng tôi báo cáo metric kiểm tra qua nhiều lần chạy để đảm bảo tính mạnh mẽ. Mô tả bộ dữ liệu và tác vụ được tóm tắt trong Phụ lục A. Chi tiết về thiết lập thí nghiệm được tóm tắt trong Phụ lục F và G.

Chúng tôi tập trung so sánh vào lựa chọn các mô-đun attention trong khung GraphGPS. Cụ thể, chúng tôi đánh giá hiệu suất của Graph-Mamba so với GraphGPS với attention dày đặc (Transformer) và các triển khai khác nhau của attention thưa thớt (tức là, Exphormer, Performer, và BigBird) (Rampášek et al., 2022; Shirzad et al., 2023; Choromanski et al., 2020; Zaheer et al., 2020).

Bảng 1 làm nổi bật hiệu suất vượt trội của Graph-Mamba trong việc nắm bắt các phụ thuộc tầm xa từ năm bộ dữ liệu hàng đầu có độ dài đầu vào lớn nhất, dao động từ 150 đến 1.400 nút mỗi đồ thị tương ứng. Trong bốn trên năm bộ dữ liệu, Graph-Mamba cung cấp cải thiện đáng kể (lên đến 5%) so với các phương pháp attention thưa thớt khác. Graph-Mamba cũng so sánh thuận lợi với Transformer với attention dày đặc, điều này nhấn mạnh tầm quan trọng của lựa chọn nút nhận thức ngữ cảnh trong các đồ thị có phụ thuộc tầm xa. Trên năm bộ dữ liệu khác với các đồ thị kích thước nhỏ đến trung bình, Graph-Mamba tiếp tục chứng minh tính mạnh mẽ của nó bằng cách thể hiện hiệu suất có thể so sánh với các phương pháp attention thưa thớt và dày đặc tiên tiến, được tóm tắt trong Bảng 4 Phụ lục. Những kết quả này xác nhận khả năng của Graph-Mamba trong việc nắm bắt ngữ cảnh tầm xa với cơ chế lựa chọn nút phụ thuộc đầu vào, trong khi khái quát hóa tốt cho các tác vụ dự đoán dựa trên đồ thị thông thường.

4.2. FLOPs và tiêu thụ bộ nhớ

Graph-Mamba cung cấp cải thiện đáng kể về hiệu quả ngoài việc tăng hiệu suất. Chúng tôi đã benchmark Floating Point Operations (FLOPs) và tiêu thụ bộ nhớ của Graph-Mamba trong giai đoạn huấn luyện so với các phương pháp hiện có trên năm bộ dữ liệu, như được chi tiết trong Phụ lục G.

Hình 2 minh họa chi phí tính toán trên bộ dữ liệu MalNet-Tiny với trung bình 1.410,3 nút, được lấy mẫu phụ ở các tỷ lệ tăng dần. Graph-Mamba thể hiện độ phức tạp tuyến tính trong cả FLOPs và bộ nhớ đối với độ dài đầu vào, trong khi chi phí của GPS-Transformer tăng theo bậc hai. GPS-Transformer gặp vấn đề hết bộ nhớ với kích thước đầu vào dưới 700 nút ở kích thước batch 16, cản trở việc huấn luyện mô hình hiệu quả. Ngược lại, Graph-Mamba hỗ trợ việc huấn luyện các đồ thị đầy đủ với gấp đôi số lượng nút, ở kích thước batch lên đến 256.

Trong Hình 3, chúng tôi tiếp tục so sánh Graph-Mamba với các biến thể attention thưa thớt trong GPS. Đối với mỗi bộ dữ liệu benchmark, trục x đại diện cho số lượng FLOPs trung bình mỗi ví dụ huấn luyện, trong khi trục y thể hiện tiêu thụ GPU trung bình. Graph-Mamba luôn chiếm góc dưới bên trái, cho thấy ít FLOPs nhất và sử dụng bộ nhớ ít nhất trên tất cả các bộ dữ liệu. Cụ thể, trong bộ dữ liệu Peptides-func, Graph-Mamba đạt được giảm 74% về sử dụng bộ nhớ và giảm 66% FLOPs so với Transformer. Hơn nữa, Graph-Mamba thể hiện giảm 40% trong cả FLOPs và sử dụng bộ nhớ so với triển khai attention đồ thị thưa thớt tiên tiến, Exphormer. Những kết quả này làm nổi bật hiệu quả tính toán của Graph-Mamba, mở ra tiềm năng của việc huấn luyện hiệu quả trên các đồ thị lớn hơn.

4.3. Phân tích của các công thức huấn luyện và suy luận của Graph-Mamba

Chúng tôi chứng minh hiệu quả của công thức huấn luyện và suy luận được đề xuất cho Graph-Mamba với một nghiên cứu phân tích. Bảng 2 thể hiện hiệu suất dự đoán của Graph-Mamba từ ba thiết lập huấn luyện và suy luận trên hai bộ dữ liệu Peptides-Func và PascalVOC-SP. Quy trình huấn luyện cơ sở không sử dụng bất kỳ kỹ thuật ưu tiên nút hoặc hoán vị nào. Thiết lập chỉ hoán vị đã giới thiệu hoán vị ngẫu nhiên ở cấp nút trong thời gian huấn luyện. Tại thời điểm suy luận, đầu ra được thu được bằng cách lấy trung bình năm lần chạy của hoán vị tương ứng. Thiết lập ưu tiên nút đã tiêm bậc nút như heuristic nút để sắp xếp chuỗi đầu vào, trong khi hạn chế hoán vị trong một phạm vi nhỏ hơn. Cụ thể, thiết lập ưu tiên bậc nút chỉ cho phép các nút có cùng bậc hoán vị ngẫu nhiên giữa chúng.

Chiến lược hoán vị dẫn đến một cải thiện hiệu suất đáng kể. Thiết lập hoán vị cấp nút thấy tăng 3% độ chính xác trung bình trên bộ dữ liệu Peptides-Func và tăng 10% điểm F1 trên bộ dữ liệu PascalVOC-SP, so với chiến lược huấn luyện cơ sở. Kết hợp ưu tiên nút theo bậc với hoán vị tiếp tục cải thiện điểm số trên bộ dữ liệu PascalVOC-SP. Những kết quả này nhấn mạnh tầm quan trọng của việc đưa ra các điều chỉnh tập trung vào đồ thị của các phương pháp mô hình hóa chuỗi, điều chỉnh theo các đặc điểm của dữ liệu không tuần tự. Những kỹ thuật thiết kế chuỗi thanh lịch này cung cấp sức mạnh mô hình hóa cải thiện cho Mamba vượt xa việc đơn giản plug-and-play. Do đó chúng tôi khuyến nghị sử dụng sự kết hợp của ưu tiên nút đầu vào theo bậc nút và hoán vị cấp nút như công thức huấn luyện và suy luận mặc định cho Graph-Mamba. Bảng 1 và 4 thể hiện hiệu suất mô hình từ công thức được khuyến nghị này.

5. Kết luận

Chúng tôi đề xuất Graph-Mamba, một mô hình đồ thị mới tận dụng SSM cho lựa chọn ngữ cảnh phụ thuộc dữ liệu hiệu quả, như một thay thế cho thưa thớt hóa attention đồ thị. Cơ chế lựa chọn của GMB lọc các nút liên quan làm ngữ cảnh, nén và truyền bá hiệu quả các phụ thuộc tầm xa trong quá trình cập nhật embedding nút. Thông qua quét lặp với nén ngữ cảnh, Graph-Mamba đạt được độ phức tạp thời gian tuyến tính và giảm tiêu thụ bộ nhớ. Công thức huấn luyện và suy luận chuyên biệt, kết hợp hoán vị và ưu tiên nút, tiếp tục điều chỉnh Mamba, một kỹ thuật mô hình hóa chuỗi, sang đầu vào đồ thị không tuần tự với cải thiện hiệu suất đáng kể. Trong các thí nghiệm thực nghiệm, Graph-Mamba đã chứng minh hiệu suất tiên tiến hoặc có thể so sánh trên mười bộ dữ liệu với nhiều tác vụ dự đoán đồ thị khác nhau. Do đó Graph-Mamba trình bày một lựa chọn hứa hẹn để thay thế attention đồ thị dày đặc hoặc thưa thớt truyền thống, cung cấp sức mạnh dự đoán cạnh tranh và nhận thức ngữ cảnh tầm xa với một phần nhỏ chi phí tính toán.

Trong các công trình tương lai, việc khám phá các kiến trúc mô hình thay thế ngoài khung GraphGPS là rất quan trọng để tăng cường hiệu suất dự đoán. Tính chất agnostic-kiến trúc của GMB cung cấp sự linh hoạt cho các ứng dụng như vậy. Hơn nữa, nghiên cứu này làm nổi bật tầm quan trọng của việc xây dựng chuỗi và các chiến lược huấn luyện trong việc tạo điều kiện cho việc học mô hình chuỗi với đầu vào đồ thị không tuần tự. Ngoài hoán vị và heuristic nút, các cách hiệu quả để tiêm topo đồ thị vào các chuỗi đầu vào vẫn chưa được khám phá. Cuối cùng, việc học chiến lược tối ưu của việc làm phẳng một đồ thị thành các chuỗi từ dữ liệu là cần thiết. Mô hình hóa chuỗi dựa trên SSM cung cấp những góc nhìn mới cho phân tích nhân quả ngoài dự đoán, trình bày một hướng hứa hẹn cho phân tích dữ liệu đồ thị. Hiệu quả cải thiện tiếp tục hỗ trợ việc phát triển các mô hình nền tảng đồ thị, mở ra khả năng tiền huấn luyện quy mô lớn.
