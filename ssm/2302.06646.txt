# 2302.06646.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2302.06646.pdf
# File size: 2093483 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Simple Hardware-Ecient Long Convolutions for Sequence
Modeling
Daniel Y. Fuâˆ—y, Elliot L. Epsteinz, Eric Nguyenx, Armin W. Thomasyy, Michael Zhangy,
Tri Daoy, Atri Rudrazz, and Christopher R ey
yDepartment of Computer Science, Stanford University
zInstitute of Computational and Mathematical Engineering, Stanford University
xDepartment of Bioengineering, Stanford University
yyDepartment of Psychology, Stanford University
zzDepartment of Computer Science and Engineering, University at Bualo, SUNY
danfu@cs.stanford.edu ,epsteine@stanford.edu ,
fetnguyen, athms, mzhang20, trid g@stanford.edu ,
atri@buffalo.edu ,chrismre@cs.stanford.edu
February 13, 2023
Abstract
State space models (SSMs) have high performance on long sequence modeling but require sophisticated
initialization techniques and specialized implementations for high quality and runtime performance. We
study whether a simple alternative can match SSMs in performance and eciency: directly learning
long convolutions over the sequence. We nd that a key requirement to achieving high performance
is keeping the convolution kernels smooth. We nd that simple interventions|such as squashing the
kernel weights|result in smooth kernels and recover SSM performance on a range of tasks including the
long range arena, image classication, language modeling, and brain data modeling. Next, we develop
FlashButterfly , an IO-aware algorithm to improve the runtime performance of long convolutions.
FlashButterfly appeals to classic Buttery decompositions of the convolution to reduce GPU memory
IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2 , and allows us to
train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1
points while training 7.2 faster than prior work. Lastly, we introduce an extension to FlashButterfly
that learns the coecients of the Buttery decomposition, increasing expressivity without increasing
runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer
parameters.
1 Introduction
Recently, a new class of sequence models based on state space models (SSMs) [ 30,34,37,46] has emerged as
a powerful general-purpose sequence modeling framework. SSMs scale nearly linearly in sequence length and
have shown state-of-the-art performance on a range of sequence modeling tasks, from long range modeling [ 68]
to language modeling [17, 50], computer vision [39, 53], and medical analysis [70].
However, SSMs rely on sophisticated mathematical structures to train eectively in deep networks [ 30].
These structures generate a convolution kernel as long as the input sequence by repeatedly multiplying a
hidden state matrix. This process may be unstable [ 27] and requires careful hand-crafted initializations [ 32],
leaving practitioners with a dizzying array of choices and hyperparameters. This begs the question, why not
parameterize the long convolution kernel directly?
âˆ—Equal Contribution.
1arXiv:2302.06646v1  [cs.LG]  13 Feb 2023

--- PAGE 2 ---
In SRAM
Convolution Weights
Squash Smoothing
Simple Regularizations FlashButter/f_ly:
IO-Aware ConvolutionsExploits Matmul Units
Butter/f_ly Decomposition
Three-Pass Algorithm
Extensions to
Structured MatricesState Space
ModelsConvolutionsStructured
Sparse
MatricesFigure 1: Left: Simple regularization techniques allow long convolutions to match state space models
in sequence modeling. Middle: FlashButterfly is an IO-aware algorithm for long convolutions that
improves runtime performance and scales to long sequences. Right: We show deep connections to advances in
block-sparse matrix multiplication and structured matrices.
There are two challenges that long convolutions face for sequence modeling. The rst is quality: previous
attempts at directly parameterizing the convolution kernel have underperformed SSMs [ 46,65]. The second is
runtime performance: long convolutions can be computed in O(NlogN) FLOPS in sequence length Nusing
the Fast Fourier transform (FFT), but systems constraints often make them slower than quadratic algorithms,
such as attention. In this paper, we show that simple regularization techniques and an IO-aware convolution
algorithm can address these challenges. The simplicity of the long convolution formulation further allows for
connections to block-sparse matrix multiplication that increase expressivity beyond convolutions or SSMs.
Closing the Quality Gap First, to understand the quality gap, we study the performance of long
convolutions compared to SSMs on Long Range Arena (LRA) [ 71], a key benchmark designed to test long
sequence models. Long convolutions underperform SSMs by up to 16.6 points on average (Table 4). Visualizing
the convolution kernels identies a potential culprit: the long convolution kernels are non-smooth, whereas
SSM kernels are smooth (Figure 2).
We explore two simple regularization techniques from the signal processing literature that alleviate this
problem. The rst technique uses a Squash operator to reduce the magnitude kernel weights in the time
domain, enforcing sparsity that translates to smoothness in the frequency domain. The second technique
applies a Smooth operator to the kernel weights in the time domain, which we nd also promotes smoothness
in the frequency domain. With regularization, long convolutions recover the performance of SSMs|and appear
more robust to initialization than SSMs, matching S4 on LRA even with completely random initialization.
Motivated by the success of these simple regularizations on LRA, we further evaluate the performance of
long convolutions on other complex sequence modeling tasks from diverse modalities. On image classication,
we nd that long convolutions can be an eective drop-in replacement for SSM layers. Replacing the SSM
layer in S4 models with long convolutions yields a lift of 0.3 accuracy points on sequential CIFAR and comes
within 0.8 points of S4ND-ISO on 2D CIFAR. On text modeling, long convolutions are competitive with
the recent SSM-based H3 model [ 17]|coming within 0.3 PPL of H3 on OpenWebText [ 28] and matching
H3 on the PILE [ 26]. Finally, long convolutions outperform both Transformers and SSMs in brain data
modeling|by 0.14 and 0.16 MAE points, respectively|which suggests that the simpler architecture can even
outperform SSMs for some applications.
Improving Runtime Performance However, long convolutions are inecient on modern hardware, since
the FFT convolution incurs expensive GPU memory IO and cannot utilize matrix multiply units|even when
using optimized implementations like cuFFT [ 56]. SSM convolution formulations rely on specialized GPU
2

--- PAGE 3 ---
Cauchy kernels and Vandermonde kernels, as well as special recurrent message passing structure, to overcome
these challenges.
In response, we develop FlashButterfly , a simple IO-aware algorithm for long convolutions, which
does not require ad hoc hand engineering. FlashButterfly appeals to classic Buttery decompositions of
the FFT to rewrite the FFT convolution as a series of block-sparse Buttery matrices. This decomposition
reduces the number of passes over the input sequence|reducing the GPU memory requirements|and utilizes
matrix multiply units on the GPU, which increases FLOP utilization.
FlashButterfly speeds up convolutions by 2.2 over cuFFT, and outperforms the fastest SSM
implementations, since it does not incur the cost of generating the SSM convolution kernel. To demonstrate
FlashButterfly 's scaling ability, we train a long convolution model on Path256, a task with sequence
length 64K. We set state-of-the-art by 29.1 points and train 7.2 faster than the previous best model.
Deeper Connections and Learned Buttery Extension The Buttery decomposition in FlashBut-
terfly forms deep connections to recent work in block-sparse matrix multiplication [ 8]. Buttery matrices
are a special case of Monarch matrices, which capture a large class of structured matrices [ 15]. The block
sizerinterpolates between the xed FFT for small block sizes to fully dense matrix multiplication for large
matrices. This connection suggests a natural learned Buttery extension that goes beyond convolutions in
expressivity.
Our learned Buttery extension simply learns the parameters in the Buttery matrices from the data,
instead of using the xed matrices that corresopnd to the FFT and inverse FFT. Learning the Buttery
matrices while keeping the block size xed yields additional parameters without additional FLOPS|yielding
0.8 additional points of lift on sequential CIFAR. Increasing the block size of the Buttery matrices approaches
the expressivity of fully dense matrices|including those used in linear layers and MLPs. As a proof of
concept, we use this property to replace the MLPs in a Transformer language model|and outperform a
GPT-2 model on WikiText103 by 0.2 PPL with 30% fewer parameters.
Summary In summary, we show that long convolutions are an eective model for long sequence modeling.
They match or exceed SSMs across an array of diverse sequence domains while requiring less hand-crafted
initializations and showing improved stability. Additionally, by leveraging connections to Buttery matrices,
long convolutions can be trained up to 1.8 faster than SSMs.1
2 Background
Deep State Space Models A continuous-time state space model (SSM) maps an input signal u(t)2RN,
over timet, to an output signal y(t)2RNas
_x(t) =Ax(t) +Bu(t)
y(t) =Cx(t) +Du(t);
by the use of hidden state x(t)2Rdand some set of matrices A2Rdd,D2R11,B2Rd1,C2R1d.
Discretizing the SSM yields a recursion xt=Axt 1+But,yt=Cxt+Dut. By unrolling the recursion, y
can be written as a convolution between uand a kernel Kthat depends on A,B,C:
y=Ku+Du: (1)
A key ingredient to training deep SSM models is proper initialization of the learnable matrices A,B,C, and
D. Initialization strategies often draw upon the HiPPO theory [29] on orthogonal polynomials, and involve
the selection of measures and discretization strategies. The parameters may also be unstable to learn, which
can require custom learning rate schedules [32].
1Our code is available at https://github.com/HazyResearch/safari .
3

--- PAGE 4 ---
Table 1: Accuracy on the ListOps task in LRA.
Model Accuracy
S4-LegS 59.6
Long Convs 53.4
Long Convs, + Smooth 59.8
Long Convs, + Squash 60.3
Long Convs, + Squash , +Smooth 59.7
FFT Convolution Computing the convolution in Equation 1 can be costly for long sequences. A standard
approach is to compute the convolution using the FFT convolution theorem. Then, the convolution can be
computed as:
y=uK=F 1
NDKFNu; (2)
where FNdenotes the DFT matrix of size N, and DK=diag(FNK). This so-called FFT convolution
scales inO(NlogN) in sequence length N, but is often unoptimized on modern hardware (most optimized
convolution operators focus on short convolutions, e.g., 3 3).
Runtime Performance Characteristics We provide a brief discussion of relevant factors aecting
runtime performance. Depending on the balance of computation and memory accesses, operations can be
classied as either compute-bound or memory-bound. In compute-bound operations, the time accessing GPU
memory is relatively small compared to the time spent doing arithmetic operations. Typical examples are
matrix multiply with large inner dimension, and short convolution kernels with a large number of channels. In
memory-bound operations, the time taken by the operation is determined by the number of memory accesses,
while time spent in computation is much smaller. Examples include most other operations: elementwise (e.g.,
activation, dropout) and reduction (e.g., sum, softmax, batch norm, layer norm).
Our Approach Rather than parameterizing Kwith carefully initialized SSM matrices, we seek to directly
parameterize the convolution Kin Equation 1. Our goal is to replace the SSM layer with a learned convolution
kernel as a drop-in replacement, while keeping the stacking and multi-head structure of SSM models (which can
be thought of as multiple convolutional lters). We also aim to make the FFT convolution runtime-performant
on modern hardware.
3 Method
In Section 3.1, we conduct an initial investigation into long convolutions for sequence modeling, and develop
two simple regularization strategies based on our ndings. Then, in Section 3.2, we present FlashButterfly ,
an IO-aware algorithm for speeding up convolutions modeled after block-sparse matrix multiplication. Finally,
we present an extension of FlashButterfly that leverages the block-sparse connection for additional
expressivity.
3.1 Long Convolutions for Sequence Modeling
First, we conduct a brief investigation into the performance of vanilla long convolutions on sequence modeling,
and we nd a gap in quality. We then propose two simple regularization techniques for closing this gap.
Motivation for Regularization: Non-Smooth Kernels We begin by directly replacing the SSM layers
in an S4 model with long convolutions, with random initialization. We train a model on the ListOps task
from the long range arena (LRA) benchmark [ 71], with element-wise dropout on the convolution kernel
weights. Table 1 shows that long convolutions underperform SSMs with 6.2 points on ListOps .
To understand the gap in performance, we visualize one head of the convolution kernel K, compared to an
SSM kernel in Figure 2. Compared to well-initialized SSM kernels, we nd that directly learning convolution
4

--- PAGE 5 ---
Time
ResponseFrequency
ResponseLong Convolution Squash Operator Gaussian Smoothing S4Figure 2: Visualizations of kernels trained on the ListOps task in LRA. Left to right, top to bottom: S4,
long convolutions without regularization, long convolutions with the Squash operator, and long convolutions
with the Smooth operator. Time response on top, frequency response on the bottom.
Table 2: Convolution- and SSM-specic hyperparameters.
Model Hyperparameters Initializations
SSMd,lrA,lrB,lrC LegS, FouT, LegS/FouT
dropout, discretization Inv, Lin
Long Convs , kernel LR, Random, Geometric
k, dropout
weights results in convolution kernels that are non-smooth and appear noisy. We hypothesize that these
properties are responsible for the performance gap.
Regularizing the Kernel We propose two simple techniques for regularizing the convolution kernel to
alleviate these issues: Squash andSmooth . The Squash operator is applied element-wise to the convolution
kernel, and reduces the magnitude of all weights: K=sign(K)max(jKj ;0). As an aside, we note that
Squash is equivalent to taking one step of an L1 proximal operator: K=Proxk:k(K) =argminxfkxk1+
kx Kk2
2gand thus may have principled connections to proximal gradient techniques. The Smooth operator
applies simple average pooling, with width p, to the convolution kernel: Kk= (2p+ 1) 1P2p+1
j=1Kk+j p:
Training long convolutions with these regularizations matches SSMs in performance on the ListOps task
(Table 1). Additionally, Figure 2 right shows that these regularizations improve smoothness in the frequency
domain as well. In Appendix B, we evaluate directly smoothing in frequency domain.
Initialization We seek to understand how sensitive long convolutions are to initialization. We note that
since Kdirectly parameterizes the convolution kernel, we can also leverage advances in initialization in SSMs
such as HiPPO [ 29] and S4-LegS [ 32]|simply by converting the initialized SSM model to a convolution
kernel, and initializing Kto the convolution weights.
While complex initialization strategies can be powerful, they require careful tuning to congure. To
understand the impact of initialization on long convolutions, we evaluate two simple intialization techniques:
random initialization, and a geometric decay initialization. The random initialization initializes the weights to
be randomly distributed from a Normal distribution: KiN. The geometric decay initialization additionally
scales kernel weights to decay across the sequence, as well as across the heads. For the kernel K(h), 1hH,
we initialize the weights as: K(h)
k=xexp ( kN 1(H=2)hH 1
);for 1kN, wherexN is drawn from a
Normal distribution.
Summary The full method is written in Algorithm 1, with a forward reference to our fast convolution
solution FlashButterfly . In Algorithm 1, all operators ( max,sign, and absolute value) are applied entry-
wise, FlashButterfly is taken over the sequence dimension and the skip connection is taken over the head
5

--- PAGE 6 ---
dimension. Convolution-specic hyperparameters are shown in Table 2. Compared to the hyperparameters
Algorithm 1 Regularized Long Convolution
Require: Inputu2RBHN,K2RHN,D2RH, whereNis the sequence length, His the head
dimension, and Bis the batch size.
1:K dropout( K).
2:Kk (2p+ 1) 1P2p+1
j=1Kk+j p.
3:K sign(K)max(jKj ;0).
4:y FlashButterfly (K;u) +Du:
5:Returny2RBHN
necessary to train S4, our regularization approaches have fewer hyperparameters and choices than S4.
3.2 FlashButtery
In addition to improving the quality of long convolutions, it is also critical to improve runtime performance.
We present FlashButterfly , an IO-aware algorithm for speeding up general convolutions on modern
hardware. We use kernel fusion to reduce GPU memory IO requirements, and use a Buttery decomposition
to rewrite the FFT as a series of block-sparse matrix multiplications. To scale to long sequences, we use an
alternate Buttery decomposition to construct a three-pass FFT convolution algorithm to further reduce IO
requirements.
Kernel Fusion Naive implementations of the FFT convolution incur expensive GPU memory IO. The
FFT, inverse FFT, and pointwise multiplication in Equation 2 each require at least one read and write
of the input sequence from GPU memory. For long sequences, the IO costs may be even worse: the
entire input sequence cannot t into SRAM, so optimized implementations such as cuFFT [ 56] must take
multiple passes over the input sequence using the Cooley-Tukey decomposition of the FFT [ 11]. Following
FlashAttention [16],FlashButterfly 's rst fuses the entire FFT convolution into a single kernel to
compute the entire convolution in GPU SRAM and avoid this overhead.
Buttery Decomposition Kernel fusion reduces the IO requirements, but the fused FFT operations still
cannot take full advantage of specialized matrix multiply units on modern GPUs, such as Tensor Cores on
Nvidia GPUs, which perform fast 16 16 matrix multiplication. We appeal to a classical result, known as
the four-step or six-step FFT algorithm [ 4], that rewrites the FFT as a series of block-diagonal Buttery
matrices [61] interleaved with permutation.
The Buttery decomposition states that we can decompose an N-point FFT into a series of FFTs of sizes
N1andN2, whereN=N1N2. Conceptually, the algorithm reshapes the input as an N1N2matrix, applies
N1FFTs of size N2to the columns, multiplies each element by a twiddle factor, and then applies N2FFTs
of sizeN1to the rows.
More precisely, let FNdenote the DFT matrix corresponding to taking the N-point FFT. Then, there
exist permutation matrices P, and a diagonal matrix D, such that FN=P(IN2
FN1)PTD(IN1
FN2)P.
Pdenotes a permutation matrix that reshapes the input to N1N2and takes the transpose, Ddenotes a
diagonal matrix with the twiddle factors along the diagonal, 
denotes the Kronecker product, and vINi
andFNiare the identity and DFT matrices of size NiNi. Precise values for FNi,D, and Pare given in
Appendix C.
The Buttery decomposition incurs O(NrlogN=logr) FLOPS for a sequence length N=rp, with block
sizer. In general FFT implementations, Nis typically padded to a power of two, so that the block size
can be set to 2 to minimize the total number of FLOPS. However, on GPUs with a specialized bbmatrix
multiply unit, the FLOP cost of computing an rrmatrix multiply with r<b is equivalent to performing a
singlebbmatrix multiply. Thus, the actual FLOP count scales as O(NblogN=logr) forr<b . Increasing
the block size up to bactually reduces the FLOP cost.
Table 3 demonstrates this tradeo on an A100 GPU, which has specialized matrix multiply units up to
1632. Runtime decreases as rincreases from 2, even though theoretical FLOPS increase. Once r >b ,
runtime begins increasing as actual FLOPS increase as well.
6

--- PAGE 7 ---
Table 3: Runtime, GLOPs, and FLOP util for the Buttery decomposition with dierent block sizes rfor
sequence length 4096, on A100 with batch size 128, head dimension 32.
Block Size Runtime (ms) GLOPs FLOP Util
2 0.52 2.0 1.3%
16 0.43 8.1 6.0%
64 0.53 21.5 13.0%
256 0.68 64.5 30.4%
Three-Pass Algorithm Kernel fusion and the Buttery decomposition improve runtime performance, but
only for convolutions short enough to t into SRAM (length 8K or shorter on A100). For longer sequences, we
again appeal to the Buttery decomposition, but using an alternate formulation that eliminates permutations
over the input sequence. This formulation allows us to decompose the convolution into three passes over the
data: a Buttery matrix multiplication that can be computed with a single IO, FFT convolutions that we can
compute in parallel, and a nal Buttery matrix multiplication that can also be computed with a single IO.
In particular, we rewrite the DFT matrix FNof sizeNasNP 1(Im
(lFl))B 1;and its inverse matrix
F 1
NasN 1B(Im
Fl)P, where Bis anNNblock matrix with m2blocks of size ll, each of which is
diagonal (see Appendix C for the exact derivation). Critically, matrix-vector multiply Bucan be computed in
a single pass over the input vector u. Substituting these into Equation 2 and simplifying yields the following:
y=uK=B(Im
Fl)D0
K(Im
Fl)B 1; (3)
where D0
K=lPDKP 1is another diagonal matrix. The middle terms can now be computed as mindependent
FFT convolutions of size l, with a dierent convolution kernel. These parallel convolutions collectively require
one pass over Ninput elements, so the entire convolution can be computed with three passes over the input.
The full algorithm for FlashButterfly forN >l is shown in Algorithm 2.
Algorithm 2 FlashButterfly
Require: Input u2RBHN,K2RHN,D2RH, where N=lmis the sequence length, His the head dimension,
andBis the batch size.
1:^K FFT (K)
2:D0
K P(^K)P 1
3:u B 1u
4:Compute u (Im
Fl)D0
K(Im
Fl)uin parallel across mstreaming multiprocessors
5:Return Bu2RBHN
We show that Algorithm 2 is correct, and that it can be computed in three passes over the input sequence.
The proof is given in Appendix D.
Proposition 1. Algorithm 2 computes the convolution uKwith at most three passes over the input sequence
u.
3.2.1 Learned Buttery Extension
The Buttery decomposition in FlashButterfly suggests a natural extension: learning the values of the
Buttery matrices Frin the Buttery decomposition, instead of using the xed matrices corresponding to
the FFT. If we keep the block size rxed, then the number of parameters in the Buttery matrices increases
byO(Hr2), but the total FLOPS in the model stay the same. Increasing the block size allows us to further
increase expressivity, but at additional compute cost. As rapproaches N, the Buttery decomposition
approaches the compute cost and expressivity of a full dense matrix multiply: O(N2).
7

--- PAGE 8 ---
Table 4: Validation accuracy of dierent models on the LRA benchmark. Best in bold, second best underlined.
Model ListOps Text Retrieval Image Pathnder Path-X Avg
Transformer 36.4 64.3 57.5 42.4 71.4 7 53.7
Nystr omformer 37.2 65.5 79.6 41.6 70.9 7 57.5
Reformer 37.3 56.1 53.4 38.1 68.5 7 50.6
BigBird 36.1 64.0 59.3 40.8 74.9 7 54.2
Linear Trans. 16.1 65.9 53.1 42.3 75.3 7 50.5
Performer 18.0 65.4 53.8 42.8 77.1 7 51.2
S4-LegS 59.6 86.8 90.9 88.7 94.2 96.4 86.1
S4-FouT 57.9 86.2 89.7 89.1 94.5 7 77.9
S4-LegS/FouT 60.5 86.8 90.3 89.0 94.4 7 78.5
S4D-LegS 60.5 86.2 89.5 88.2 93.1 92.0 84.9
S4D-Inv 60.2 87.3 91.1 87.8 93.8 92.8 85.5
S4D-Lin 60.5 87.0 91.0 87.9 94.0 7 78.4
S4 (Original) 58.4 76.0 87.1 87.3 86.1 88.1 80.5
Long Conv, Rand 53.4 64.4 83.0 81.4 85.0 7 69.5
Long Conv, Rand + Smooth 59.8 68.7 86.6 79.3 86.1 7 71.8
Long Conv, Rand + Squash 60.3 87.1 90.0 88.3 94.0 96.9 86.1
Long Conv, Rand + Squash +Smooth 59.7 72.8 88.6 80.8 90.1 7 73.7
Long Conv, Exp + Squash 62.2 89.6 91.3 87.0 93.2 96.0 86.6
4 Evaluation
We evaluate how well long convolutions perform in a variety of challenging sequence modeling tasks from
diverse modalities and benchmarks, including the long range arena benchmark, image classication, text
modeling, and brain data modeling (Section 4.1). We nd that long convolutions are strong sequence modelers
across these tasks. Next, we evaluate the runtime performance of long convolutions under FlashButterfly
and evaluate how well it scales to very long sequences (Section 4.2). Finally, we evaluate the quality
improvements from learned Buttery extension (Section 4.3).
4.1 Quality on Sequence Modeling
In this section, we evaluate the performance of long convolutions in sequence modeling in terms of quality. We
begin by evaluating various regularization and initialization techniques on the long range arena benchmark, a
suite of general-purpose sequence modeling tasks designed to stress test long sequences [ 71]. We take the
best-performing variants and move on to two challenging and diverse modalities that have been used to
evaluate sequence models, including SSMs: image classication (both one-dimensional and two-dimensional)
and text modeling. We conclude the section with a real-world application of long convolutions to brain data
modeling.
We nd that long convolutions perform well across all of these diverse tasks and modalities|and are
generally more robust to choice of initialization than SSMs. Our results suggest that long convolutions may
be a compelling simpler alternative to SSMs for sequence modeling. Experimental details for the tasks are
given in Appendix F, and additional experiments are provided in Appendix B.
4.1.1 Long Sequence Modeling: Long Range Arena
We rst evaluate long convolutions on Long Range Arena (LRA), a benchmark suite used to test general-
purpose sequence modeling over long contexts. LRA consists of six long-range sequence modeling tasks, with
sequence lengths between 1K and 16K tokens. The tasks have modalities including text, natural and synthetic
images, and mathematical expressions. We take the state-of-the-art S4 architecture [ 32], and replace the
SSM layers with long convolutions.
We present ve variants of long convolutions: random intialization and no regularization, random initial-
ization with the Smooth operator, random initialization with the Squash operator, random initialization
8

--- PAGE 9 ---
Table 5: Image classication on attened images.
Model sCIFAR
Transformer 62.2
LSTM 63.0
r-LSTM 72.2
UR-LSTM 71.0
UR-GRU 74.4
HIPPO-RNN 61.1
LipschitzRNN 64.2
CKConv 64.2
S4-LegS 91.8
S4-FouT 91.2
S4D-LegS 89.9
S4D-Inv 90.7
S4D-Lin 90.4
Long Conv, Random 91.0
Long Conv, Geom Init 92.1Table 6: Image classication on 2D images.
Model CIFAR
S4ND-ISO 89.9
Long Conv 2D-ISO, Rand init 88.1
Long Conv 2D-ISO, Geom init 89.1
Table 7: Test PPL of models trained on Open-
WebText.
Model Test PPL
Transformer 20.6
S4D 24.9
GSS 24.0
H3 19.6
H3 + Long-Conv, Rand Init 20.1
H3 + Long-Conv, Geom Init 19.9Table 8: Test PPL on the Pile for models trained
with various tokens.
Train Tokens 5B 10B 15B
Transformer 12.7 11.3 10.7
H3 11.8 10.7 10.2
H3 + Long Convs, Geom Init 11.9 10.7 10.3
with both operators, and the geometric initialization with the Squash operator. We compare the long
convolution methods against variants of Transformers presented in the original Long Range Arena paper [ 71],
as well as variants of S4 with dierent parameterizations and initializations [ 32]. These initializations are
important for S4 to achieve high quality.
Table 4 shows the results for long convolutions on the LRA benchmark. An 7in the Path-X column indicates
that the model never achieved better classication accuracy than random guessing. Long convolutions appear
to be robust to initialization: there is only a 0.5 point spread in the average score between long convolutions
with a geometric initialization and long convolutions with a random initialization|though individual tasks
may have more spread. This stands in contrast to the S4 methods, which are sensitive to initialization choices
and the parameterization|with a spread of 7.6 points between S4-LegS and S4-LegS/FouT.
Regularization is critical for achieving strong performance; without it, long convolutions lose 17.1 points
on average across the six LRA tasks. Using the Squash operator on its own appears to perform better
than using the Smooth operator, or using both together. For the rest of the experiments, we focus on the
two best-performing variants of long convolutions: random initialization with the Squash operator, and
geometric initialization with the Squash operator.
4.1.2 Image Classication
Next, we evaluate long convolutions on image classication. We evaluate two settings which have been used
to evaluate SSMs and sequence models: 1D pixel-by-pixel image classication, and 2D image classication.
These settings are challenging for sequence modeling, as they require modeling complex spatial relationships
between image pixels in a continuous space. For the 1D case, we again use long convolutions as a drop-in
replacement for the SSM layer in the state-of-the-art S4 architecture. For the 2D case, we replace the S4
9

--- PAGE 10 ---
Table 9: Evaluation on brain fMRI data.
Model MAE
Transformer 0.68
H3 0.70
H3 + Long Convs, Rand Init 0.58
H3 + Long Convs, Geom Init 0.54
layers in S4ND [53] with 2D long convolution lters.
Tables 5 and 6 show the results. On 1D image classication, long convolutions again match the performance
of S4, even with random initializations, while their performance improves further by 1.1 points when using
the geometric initialization. On 2D image classication, long convolutions come within 0.8 points of the
state-of-the-art S4ND model. Further regularization or inductive bias may be helpful for long convolutions to
recover the performance of SSMs in higher dimensions.
4.1.3 Text Modeling: OpenWebText and the PILE
We evaluate long convolutions on text modeling. Text has been a challenging modality for state space
models and non-attention sequence models, since it requires comparing and copying elements across the input
sequence [ 17,58]. We build o of the H3 model [ 17]|the state-of-the-art SSM model for text modeling|which
stacks two SSMs and multiplies their outputs together as a gating mechanism. We use long convolutions as a
drop-in replacement for the SSMs in the H3 layer.
Following the H3 paper, we keep two attention layers in the overall language model and evaluate on two
datasets: OpenWebText [ 28] and the Pile [ 26]. We use OpenWebText to evaluate the role of initialization:
we train models to completion at 100B tokens, and evaluate both random and geometric initializations. For
the Pile, we evaluate how well long convolutions scale with data: we use the geometric initialization, and
evaluate the performance of models trained with 5B, 10B, and 15B tokens.
Tables 7 and 8 show the results. On OpenWebText, long convolutions with random initialization come
within 0.5 PPL points of H3, and the geometric decay initialization comes within 0.3 PPL. Both models
outperform the Transformer. On the Pile, long convolutions with geometric decay initialization nearly match
H3 everywhere along the data scaling curve, and outperform Transformers. These initial results suggests
that convolutions|with some multiplicative gating mechanism|may be a promising candidate for language
modeling.
4.1.4 Brain fMRI Analysis
Finally, we evaluate long convolutions on a real-world sequence modeling modality: analysis of brain functional
Magnetic Resonance Imaging (fMRI) sequence data. To this end, we replicate the self-supervised pre-training
task proposed by Thomas et al. [72]: training models to predict whole-brain activity for the next time step of
an fMRI sequence (using a large-scale upstream dataset, spanning fMRI data from 11 ;980 experimental runs
of 1;726 individuals). We compare long convolutions against Transformers and H3, architectures that achieve
state-of-the-art performance in this task [ 17,72], by adapting the H3 model and replacing the SSM kernel
with long convolutions. Long convolutions outperform the other models in accurately predicting brain activity
in this task (see Table 9). Full details of this analysis are provided in Appendix F.1, where we also show that
long convolutions perform on par with the other models in accurately classifying new fMRI sequences in a
downstream adaptation.
4.2 Eciency: FlashButtery
We now turn towards evaluating the runtime performance of FlashButterfly . We focus on two questions:
whether FlashButterfly can outperform SSMs in terms of runtime performance, and how well Flash-
Butterfly can scale to long sequences. First, we evaluate FlashButterfly 's runtime on the Long Range
Arena speed benchmark [ 71], which measures runtime on a byte-level text classication benchmark that is
representative of standard sequence modeling loads. FlashButterfly outperforms SSMs and baselines
10

--- PAGE 11 ---
Table 10: LRA Speed Benchmark.
Model Speedup
Transformer 1 
FlashAttention 2.4
SSM + FlashConv 5.8
FlashButterfly 7.0
1024 16K 64K 128K010203040Benchmarking FlashButter/f_ly
Sequence LengthTime (ms)
FlashAttention
SSM, cuFFTLong Conv, cuFFT
SSM, FlashConvFlashButter/f_ly2.2x1.8x
Figure 3: We compare the performance of FlashButterfly to attention, SSMs with cuFFT, long
convolutions with cuFFT, and SSMs with FlashConv , the most optimized SSM algorithm [ 17]. Speedups
shown for sequence length 128K.
from the original LRA speed benchmark. Next, we evaluate how well FlashButterfly scales to longer
sequences. Across many sequence lengths, FlashButterfly outperforms the fastest SSM implementation.
Finally, we demonstrate FlashButterfly 's sequence scaling capabilities on an extremely long sequence
task: Path256, which has sequence length 64K.
4.2.1 Runtime on Long Range Arena
We begin by evaluating runtime on the Long Range Arena speed benchmark [ 71]. The benchmark measures
runtime on a byte-level text classication task. This task, which has sequence length 4K, is representative
of typical sequence modeling training workloads, and is a standard evaluation benchmark for Transformers
and SSMs [ 71]. The benchmark is measured in terms of speedup against vanilla Transformers using a
HuggingFace implementation. We additionally compare against two more baselines: a) Transformers using
FlashAttention [16], the fastest attention algorithm, and b) SSMs using FlashConv [17], the fastest
SSM implementation.
Table 10 shows the results. FlashButterfly achieves 7.0speedup over the Transformer baseline.
It outperforms FlashAttention , since its compute scales nearly linearly with sequence length instead of
quadratically. It also outperforms FlashConv , the fastest SSM implementation, since it does not require
kernel generation. These results show that FlashButterfly outperforms SSMs and Transformers in terms
of runtime eciency in standard sequence modeling workloads.
4.2.2 Scaling to Longer Sequences
Next, we evaluate how well FlashButterfly scales to longer sequence lengths. We compare FlashBut-
terfly against a) convolutions using cuFFT, the standard implementation in PyTorch, and b) SSMs using
FlashConv . We measure the runtime for sequence lengths ranging from 1K to 128K. Following [ 17], we
11

--- PAGE 12 ---
Table 11: Runtime and accuracy on Path256 (sequence length 64K).
Model Accuracy Training Time
Transformer 7 7
FlashAttention 7 7
Block-Sparse FlashAttention 63.1 3 days
FlashButterfly 92.2 10 hours
Table 12: Performance with learnable Buttery
of dierent sizes
Block Size sCIFAR Speedup
Fixed Buttery 91.0 1 
16 91.8 1 
32 92.4 0.9 
256 92.5 0.6Table 13: Performance of replacing MLPs with the long
conv extension in a Transformer on WikiText103.
Model PPL Params
GPT-2-Small 20.6 124M
Monarch-GPT-2-Small 20.7 72M
FlashButterfly -GPT-2-Small 20.4 86M
measure the runtime of a single layer using batch size 32 and 128 model dimension. We also provide attention
runtime, as well as SSMs using a standard PyTorch implementation, for context.
Figure 3 shows the results. FlashButterfly yields up to 2.2speedup against baseline cuFFT-based
convolutions. FlashButterfly outperforms FlashConv for all sequence lengths, since it does not require
the kernel generation step of SSMs. These results show that FlashButterfly outperforms SSMs and
Transformers across all sequence lengths|even very long sequences.
We demonstrate the utility of FlashButterfly by training models on a task with extremely long
sequences: Path256, which has sequence length 64K. Table 11 shows that long convolutions achieve state-of-
the-art performance on Path256, outperforming block-sparse FlashAttention from [ 16], the only prior work
to report non-trivial performance ( >50% accuracy) on Path256. Long convolutions with FlashButterfly
exceed state-of-the-art performance by 29.1 points, and train 7.2 faster.
4.3 Learned Buttery Extension
Finally, we experimentally evaluate how well the learned Buttery extension can improve quality on two
tasks: sequential CIFAR and WikiText103.
First, on sequential CIFAR, we use the same architecture as in Section 4.1, except with learned Buttery
matrices. Table 12 shows the results for sequential CIFAR, with varying block sizes. Block size 16 yields lift
over the baseline with xed Buttery matrices, without sacricing runtime. Larger block sizes yield further
lift, but at the cost of additional runtime.
Next, on WikiText103, we evaluate the learned Buttery extension in an alternate setting: replacing
MLPs in a Transformer, following [ 15]. In this setting, we leverage the fact that a Buttery matrix with
large block size (256) approximates a dense matrix multiplication, but has fewer parameters. We compare
our learned Buttery extension against a Transformer with dense MLPs, and against Transformers where
the MLPs have been replaced with Monarch matrices [ 15]. The metric is whether we can achieve the same
performance as a Transformer with dense MLPs, but with fewer parameters.
Table 13 shows the results. Our extension outperforms both the baseline Transformer and Monarch,
outperforming the Transformer with a 30% reduction in parameters. This result validates the connection
between our learned Buttery extension and structured sparse matrices.
5 Conclusion
We nd that regularizing the kernel weights with a squash operator allows long convolutions to achieve strong
performance on a variety of long sequence modeling tasks. We develop FlashButterfly to improve the
12

--- PAGE 13 ---
runtime eciency of long convolutions, using Buttery decompositions, and we connect convolutions to recent
advances in block-sparse matrix multiplication.
Acknowledgments
We are very grateful to Sarah Hooper, Arjun Desai, Khaled Saab, Simran Arora, and Laurel Orr for
providing feedback on early drafts of this paper and helping to copyedit. We thank Together Computer for
providing portions of the compute used to train models in this paper. This work was supported in part by
high-performance computer time and resources from the DoD High Performance Computing Modernization
Program. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos.
CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM
ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266
(Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry
in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC,
Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud,
Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative
(SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate
Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and
VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon. Any opinions, ndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reect the views, policies, or
endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra's research is
supported by NSF grant CCF-1763481.
References
[1]Ailon, N., Leibovitch, O., and Nair, V. Sparse linear networks with a xed buttery structure: theory
and practice. In Uncertainty in Articial Intelligence , pp. 1174{1184. PMLR, 2021.
[2]Ayinala, M., Brown, M., and Parhi, K. K. Pipelined parallel t architectures via folding transformation.
IEEE Transactions on Very Large Scale Integration (VLSI) Systems , 20(6):1068{1081, 2011.
[3]Bahn, J. H., Yang, J. S., Hu, W.-H., and Bagherzadeh, N. Parallel t algorithms on network-on-chips.
Journal of Circuits, Systems, and Computers , 18(02):255{269, 2009.
[4]Bailey, D. H. FFTs in external or hierarchical memory. The journal of Supercomputing , 4(1):23{35, 1990.
[5]Barch, D. M., Burgess, G. C., Harms, M. P., Petersen, S. E., Schlaggar, B. L., Corbetta, M., Glasser,
M. F., Curtiss, S., Dixit, S., Feldt, C., et al. Function in the human connectome: task-fmri and individual
dierences in behavior. Neuroimage , 80:169{189, 2013.
[6] Bekele, A. Cooley-tukey t algorithms. Advanced algorithms , 2016.
[7] Brigham, E. O. The fast Fourier transform and its applications . Prentice-Hall, Inc., 1988.
[8]Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C. Pixelated buttery: Simple
and ecient sparse training for neural network models. In International Conference on Learning
Representations , 2021.
[9]Choromanski, K., Rowland, M., Chen, W., and Weller, A. Unifying orthogonal monte carlo methods. In
International Conference on Machine Learning , pp. 1203{1212. PMLR, 2019.
[10]Chu, E. and George, A. Inside the FFT black box: serial and parallel fast Fourier transform algorithms .
CRC press, 1999.
13

--- PAGE 14 ---
[11]Cooley, J. W. and Tukey, J. W. An algorithm for the machine calculation of complex fourier series.
Mathematics of Computation , 19(90):297{301, 1965. ISSN 00255718, 10886842. URL http://www.jstor.
org/stable/2003354 .
[12]Dadi, K., Varoquaux, G., Machlouzarides-Shalit, A., Gorgolewski, K. J., Wassermann, D., Thirion, B.,
and Mensch, A. Fine-grain atlases of functional modes for fmri analysis. NeuroImage , 221:117126, 2020.
[13]Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q., and Salakhutdinov, R. Transformer-xl: Attentive
language models beyond a xed-length context. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics , pp. 2978{2988, 2019.
[14]Dao, T., Gu, A., Eichhorn, M., Rudra, A., and R e, C. Learning fast algorithms for linear transforms
using buttery factorizations. In International conference on machine learning , pp. 1517{1527. PMLR,
2019.
[15]Dao, T., Chen, B., Sohoni, N. S., Desai, A., Poli, M., Grogan, J., Liu, A., Rao, A., Rudra, A., and R e, C.
Monarch: Expressive structured matrices for ecient and accurate training. In International Conference
on Machine Learning , pp. 4690{4721. PMLR, 2022.
[16]Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R e, C. FlashAttention: Fast and memory-ecient exact
attention with IO-awareness. In Advances in Neural Information Processing Systems , 2022.
[17]Dao, T., Fu, D. Y., Saab, K. K., Thomas, A. W., Rudra, A., and R e, C. Hungry hungry hippos: Towards
language modeling with state space models. arXiv preprint arXiv:2212.14052 , 2022.
[18]De Sa, C., Cu, A., Puttagunta, R., R e, C., and Rudra, A. A two-pronged progress in structured dense
matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
Discrete Algorithms , pp. 1060{1079. SIAM, 2018.
[19]Dong, X., Chen, S., and Pan, S. Learning to prune deep neural networks via layer-wise optimal brain
surgeon. Advances in Neural Information Processing Systems , 30, 2017.
[20]Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,
Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image
recognition at scale. In International Conference on Learning Representations , 2020.
[21]Eidelman, Y. and Gohberg, I. On a new class of structured matrices. Integral Equations and Operator
Theory , 34(3):293{324, 1999.
[22] Fischl, B. Freesurfer. Neuroimage , 62(2):774{781, 2012.
[23]Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
arXiv preprint arXiv:1803.03635 , 2018.
[24]Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis. arXiv
preprint arXiv:1903.01611 , 2019.
[25]Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear mode connectivity and the lottery ticket
hypothesis. In International Conference on Machine Learning , pp. 3259{3269. PMLR, 2020.
[26]Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A.,
Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint
arXiv:2101.00027 , 2020.
[27]Goel, K., Gu, A., Donahue, C., and R e, C. It's raw! audio generation with state-space models. arXiv
preprint arXiv:2202.09729 , 2022.
[28] Gokaslan, A., Cohen, V., Pavlick, E., and Tellex, S. Openwebtext corpus, 2019.
[29]Gu, A., Dao, T., Ermon, S., Rudra, A., and R e, C. Hippo: Recurrent memory with optimal polynomial
projections. Advances in Neural Information Processing Systems , 33:1474{1487, 2020.
14

--- PAGE 15 ---
[30]Gu, A., Goel, K., and R e, C. Eciently modeling long sequences with structured state spaces. In The
International Conference on Learning Representations (ICLR) , 2022.
[31]Gu, A., Gupta, A., Goel, K., and R e, C. On the parameterization and initialization of diagonal state
space models. In Advances in Neural Information Processing Systems , 2022.
[32]Gu, A., Johnson, I., Timalsina, A., Rudra, A., and R e, C. How to train your hippo: State space models
with generalized orthogonal basis projections. arXiv preprint arXiv:2206.12037 , 2022.
[33]Guibas, J., Mardani, M., Li, Z., Tao, A., Anandkumar, A., and Catanzaro, B. Adaptive fourier neural
operators: Ecient token mixers for transformers. arXiv preprint arXiv:2111.13587 , 2021.
[34]Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as eective as structured state spaces. In
Advances in Neural Information Processing Systems , 2022.
[35]Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning,
trained quantization and human coding. arXiv preprint arXiv:1510.00149 , 2015.
[36]Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for ecient neural
network. Advances in neural information processing systems , 28, 2015.
[37]Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D. Liquid structural state-space
models. arXiv preprint arXiv:2209.12951 , 2022.
[38]He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of
the IEEE conference on computer vision and pattern recognition , pp. 770{778, 2016.
[39]Islam, M. M. and Bertasius, G. Long movie clip classication with state-space video models. arXiv
preprint arXiv:2204.01692 , 2022.
[40]Kailath, T., Kung, S.-Y., and Morf, M. Displacement ranks of matrices and linear equations. Journal of
Mathematical Analysis and Applications , 68(2):395{407, 1979.
[41]King, M., Hernandez-Castillo, C. R., Poldrack, R. A., Ivry, R. B., and Diedrichsen, J. Functional
boundaries in the human cerebellum revealed by a multi-domain task battery. Nature neuroscience , 22
(8):1371{1378, 2019.
[42]Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classication with deep convolutional neural
networks. Communications of the ACM , 60(6):84{90, 2017.
[43]LeCun, Y., Bottou, L., Bengio, Y., and Haner, P. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278{2324, 1998.
[44]Lee-Thorp, J., Ainslie, J., Eckstein, I., and Ontanon, S. Fnet: Mixing tokens with fourier transforms.
arXiv preprint arXiv:2105.03824 , 2021.
[45]Li, B., Cheng, S., and Lin, J. tct: Accelerating half-precision t through tensor cores. arXiv preprint
arXiv:2104.11471 , 2021.
[46]Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes convolutional models great on long
sequence modeling? arXiv preprint arXiv:2210.09298 , 2022.
[47]Liang, Y., Chongjian, G., Tong, Z., Song, Y., Wang, J., and Xie, P. Evit: Expediting vision transformers
via token reorganizations. In International Conference on Learning Representations , 2021.
[48]Lin, J., Rao, Y., Lu, J., and Zhou, J. Runtime neural pruning. Advances in neural information processing
systems , 30, 2017.
[49] Lin, R., Ran, J., Chiu, K. H., Chesi, G., and Wong, N. Deformable buttery: A highly structured and
sparse linear transform. Advances in Neural Information Processing Systems , 34:16145{16157, 2021.
15

--- PAGE 16 ---
[50]Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and Zettlemoyer, L. Mega: moving
average equipped gated attention. arXiv preprint arXiv:2209.10655 , 2022.
[51]Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B. Long range language modeling via gated state
spaces. arXiv preprint arXiv:2206.13947 , 2022.
[52]Munkhoeva, M., Kapushev, Y., Burnaev, E., and Oseledets, I. Quadrature-based features for kernel
approximation. Advances in neural information processing systems , 31, 2018.
[53]Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., and R e, C. S4nd: Modeling
images and videos as multidimensional signals with state spaces. In Advances in Neural Information
Processing Systems , 2022.
[54] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.
[55] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.
[56] NVIDIA. cut v11.7.1 documentation, 2022. https://docs.nvidia.com/cuda/cut/index.html.
[57] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.
[58]Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A.,
Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hateld-Dodds, Z., Hernandez, D., Johnston,
S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J.,
McCandlish, S., and Olah, C. In-context learning and induction heads. Transformer Circuits Thread ,
2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
[59] Oppenheim, A. V. Applications of digital signal processing. Englewood Clis , 1978.
[60]Oppenheim, A. V., Buck, J. R., and Schafer, R. W. Discrete-time signal processing. Vol. 2 . Upper
Saddle River, NJ: Prentice Hall, 2001.
[61]Parker, D. Random Buttery Transformations with Applications in Computational Linear Algebra . CSD
(Series). UCLA Computer Science Department, 1995.
[62]Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in
neural information processing systems , 32, 2019.
[63]Prabhu, A., Farhadi, A., Rastegari, M., et al. Buttery transform: An ecient t based neural
architecture design. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 12024{12033, 2020.
[64]Romero, D. W., Bruintjes, R.-J., Tomczak, J. M., Bekkers, E. J., Hoogendoorn, M., and van Gemert,
J. C. Flexconv: Continuous kernel convolutions with dierentiable kernel sizes. arXiv preprint
arXiv:2110.08059 , 2021.
[65]Romero, D. W., Kuzina, A., Bekkers, E. J., Tomczak, J. M., and Hoogendoorn, M. Ckconv: Continuous
kernel convolution for sequential data. In International Conference on Learning Representations , 2021.
[66]Sanh, V., Wolf, T., and Rush, A. Movement pruning: Adaptive sparsity by ne-tuning. Advances in
Neural Information Processing Systems , 33:20378{20389, 2020.
[67]Sindhwani, V., Sainath, T., and Kumar, S. Structured transforms for small-footprint deep learning.
Advances in Neural Information Processing Systems , 28, 2015.
[68]Smith, J. T., Warrington, A., and Linderman, S. W. Simplied state space layers for sequence modeling.
arXiv preprint arXiv:2208.04933 , 2022.
[69] Smith, S. W. et al. The scientist and engineer's guide to digital signal processing, 1997.
16

--- PAGE 17 ---
[70]Tang, S., Dunnmon, J. A., Qu, L., Saab, K. K., Lee-Messer, C., and Rubin, D. L. Spatiotemporal
modeling of multivariate signals with graph neural networks and structured state space models. arXiv
preprint arXiv:2211.11176 , 2022.
[71]Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and
Metzler, D. Long range arena: A benchmark for ecient transformers. In International Conference on
Learning Representations , 2020.
[72]Thomas, A. W., R e, C., and Poldrack, R. A. Self-supervised learning of brain dynamics from broad
neuroimaging data. arXiv preprint arXiv:2206.11417 , 2022.
[73] Trockman, A. and Kolter, J. Z. Patches are all you need? arXiv preprint arXiv:2201.09792 , 2022.
[74]Varol, G., Laptev, I., and Schmid, C. Long-term temporal convolutions for action recognition. IEEE
transactions on pattern analysis and machine intelligence , 40(6):1510{1517, 2017.
[75]Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., and Polosukhin,
I. Attention is all you need. Advances in neural information processing systems , 30, 2017.
[76]Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In International
Conference on Learning Representations , 2022.
[77]Zhang, M., Saab, K. K., Poli, M., Goel, K., Dao, T., and R e, C. Eectively modeling time series with
simple discrete state spaces. In International Conference on Learning Representations , 2023.
[78]Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond ecient
transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on articial
intelligence , volume 35, pp. 11106{11115, 2021.
[79]Zhou, L., Poli, M., Xu, W., Massaroli, S., and Ermon, S. Deep latent state space models for time-series
generation. arXiv preprint arXiv:2212.12749 , 2022.
17

--- PAGE 18 ---
A Related Work
State space models Following S4 [ 30], deep state space models have been demonstrating promise in
sequence modeling. These models have been especially promising for long sequences, which are challenging for
architectures such as Transformers [ 75], and has required custom approaches to adapt to higher-dimensional
data [ 20,47] or long sequences [ 13,76]. Deep SSMs have shown state-of-the-art performance on a number
of domains, including time series data [ 30,77,79], audio [ 27], visual data [ 53], text [ 17,50,51], and
medical data [ 70]. A number of methods have also been proposed to simplify the S4 architecture in
parameterization [ 31,34,68], make the parameterization more numerically stable [ 27], or improve the
initialization [ 32]. Some of these approaches have also combined SSMs with other sequence modeling
primitives [ 37], including attention [ 17,50,51], and Goel et al. [27]have used SSMs as a drop-in replacement
in audio generation models. Our work is complementary to these approaches. For example, one way to
leverage principled initialization techniques is to apply them directly to the long convolutions. Our work also
suggests that long convolutions may be promising architectures for the downstream applications where SSMs
have shown strong performance.
Convolutions Convolutions have a long history in signal processing [ 69] and machine learning, especially
in computer vision [ 38,42,43]. Most models are based on short, localized convolutions [ 73], and most libraries
are optimized for short convolutions [ 62]. Recently, in conjunction with the development of state space models,
there has been growing interesting in models that use long convolutions [ 64,65,74], often with implicit
representations [ 33,44,46]. Approaches such as CKConv [ 65] and SGConv [ 46] have shown that convolutions
can be eective for sequence modeling, but require parameter counts that grow sub-linearly in sequence
length and build in explicit biases into the parameterization [46]. Our work provides additional support for
the use of long convolutions for sequence modeling, and suggest that|with the right regularization|long
convolutions can be used successfully for sequence modeling without controlling for parameter counts.
FFT Algorithms The computational feasibility of long convolution models depends on the Fast Fourier
Transform (FFT). The Cooley-Tukey FFT algorithm, published in 1965 [ 11], enabled convolution and
Fourier transforms to scale in the length dimension from O(NlogN) instead of O(N2). Subsequently, many
alternative algorithms for eciently computing the Fourier transform have emerged, including algorithms
for computing the FFT in parallel [ 2]. These algorithms have enabled fundamental progress in a range of
disciplines, including control theory [ 6,7] and signal processing [ 59,60]. A survey of methods is included in
Bahn et al., Chu & George.
As FFTs prove more useful for modern deep learning workloads|e.g., through long convolutions|new
techniques are required to run them eciently. Of particular interest to our work is making FFTs run
eciently on GPUs with specialized matrix multiplication units, such as tensor cores. For example, an A100
GPU has a maximum of 312 TFLOPs/s of FP16 with tensor cores, but only 20 TFLOPs/s of FP32 (and
40 TFLOPs/s of FP16) without tensor cores [ 55]. This trend started with the V100 GPUs [ 54] and has
continued with the H100 GPUs [ 57]. Our work is related to and draws from eorts in the high-performance
computing community to accelerate FFTs given these new hardware primitives [ 45], but focuses specically
on using them in convolutions. In the convolution workload, it is important to mitigate IO costs and increase
FLOP utilization in concert.
Sparse Structured Matrices Sparse structured matrices have recently received a great deal of attention as
a promising research topic in making machine learning models more runtime- and parameter-ecient. Sparse
training has a long history in machine learning, including work in pruning neural networks [ 19,35,36,48,66]
and nding lottery tickets [ 23,24,25]. Structured matrices are another approach to making models more
ecient. Structured matrices have subquadratic ( o(n2) for dimension nn) parameters and runtime,
such as sparse and low-rank matrices, and fast transforms (Fourier, Chebyshev, sine/cosine, orthogonal
polynomials) [ 15]. Critically, simple divide-and-conquer schemes can lead to fast algorithms for many
structured matrices [ 18], and structured matrices can be used to represent many commonly used fast
transforms [ 21,40,67]. Our connection to these matrices comes through buttery matrices [ 8,14,61], which
have been shown to be expressive and hardware-ecient [ 15]. Buttery matrices have also been used in
kernel methods [ 9,52] and deep learning methods [ 1,49,63], which may suggest other fruitful avenues of
18

--- PAGE 19 ---
Table 14: Validation accuracy of dierent models on the LRA benchmark. This table includes extra
experiments where we run the Smooth operator over the frequency domain. Best in bold, second best
underlined.
Model ListOps Text Retrieval Image Pathnder Path-X Avg
Transformer 36.4 64.3 57.5 42.4 71.4 7 53.7
Nystr omformer 37.2 65.5 79.6 41.6 70.9 7 57.5
Reformer 37.3 56.1 53.4 38.1 68.5 7 50.6
BigBird 36.1 64.0 59.3 40.8 74.9 7 54.2
Linear Trans. 16.1 65.9 53.1 42.3 75.3 7 50.5
Performer 18.0 65.4 53.8 42.8 77.1 7 51.2
S4-LegS 59.6 86.8 90.9 88.7 94.2 96.4 86.1
S4-FouT 57.9 86.2 89.7 89.1 94.5 7 77.9
S4-LegS/FouT 60.5 86.8 90.3 89.0 94.4 7 78.5
S4D-LegS 60.5 86.2 89.5 88.2 93.1 92.0 84.9
S4D-Inv 60.2 87.3 91.1 87.8 93.8 92.8 85.5
S4D-Lin 60.5 87.0 91.0 87.9 94.0 7 78.4
S4 (Original) 58.4 76.0 87.1 87.3 86.1 88.1 80.5
Long Conv, Rand 53.4 64.4 83.0 81.4 85.0 7 69.5
Long Conv, Rand + Smooth 59.8 68.7 86.6 79.3 86.1 7 71.8
Long Conv, Rand + Smooth , Freq 56.1 67.9 86.8 85.2 88.3 7 72.4
Long Conv, Rand + Squash 60.3 87.1 90.0 88.3 94.0 96.9 86.1
Long Conv, Rand + Squash +Smooth 59.7 72.8 88.6 80.8 90.1 7 73.7
Long Conv, Rand + Squash +Smooth , Freq 59.7 72.8 88.6 85.7 88.3 84.9 78.8
Long Conv, Exp + Squash 62.2 89.6 91.3 87.0 93.2 96.0 86.6
Table 15: Univariate long sequence time-series forecasting results on ETTh1 Informer benchmark. Comparisons
across ve horizon prediction settings. Best mean squared error (MSE) and mean absolute error (MAE) in
bold. Numbers reported from [ 30]. Long Convs outperforms S4 and obtains best MSE and MAE in four out
of ve evaluation settings.
Methods Long Convs S4 Informer LogTrans Reformer LSTMa DeepAR ARIMA Prophet
Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
24 0.06 0.20 0.06 0.19 0.10 0.25 0.10 0.26 0.22 0.39 0.11 0.27 0.11 0.28 0.11 0.28 0.12 0.28
48 0.07 0.21 0.08 0.22 0.16 0.32 0.17 0.33 0.28 0.45 0.19 0.36 0.16 0.33 0.18 0.42 0.17 0.33
168 0.07 0.21 0.10 0.26 0.18 0.35 0.21 0.38 1.52 1.19 0.24 0.39 0.24 0.42 0.40 0.50 1.22 0.76
336 0.08 0.23 0.08 0.23 0.22 0.39 0.23 0.40 1.86 1.12 0.59 0.70 0.45 0.55 0.47 0.59 1.55 1.82
720 0.09 0.24 0.12 0.27 0.27 0.44 0.27 0.46 2.11 1.44 0.68 0.77 0.66 0.71 0.66 0.77 2.74 3.25
future work for long convolutions with a learned Buttery formulation. Our work suggests that using long
convolution models may oer an inroads to utilizing structured matrices in deep learning.
B Additional Experiments
B.1 Regularization in Frequency Domain
In earlier experiments, we also experimented with smoothing convolutions directly in the frequency domain.
We convert the convolution kernel Kto frequency domain with an FFT, apply the smoothing operator
Smooth , and then convert it back to the time domain with an inverse FFT. Table 14 shows the results on
LRA, with the convolutions smoothed in frequency domain denoted by \ Smooth , Freq." The performance
is similar to smoothing in time domain, but underperforms using the Squash operator on its own, so we
elected to just use the Squash operator in the remaining experiments.
B.2 Time Series Forecasting
Time series forecasting is another challenging modality for sequence modeling, which requires reasoning
over multiple time contexts. We evaluate the performance of long convolutions on dierent future horizon
prediction windows in ETTh 1, a real-world long sequence time series forecasting task from the Informer
benchmark [ 78]. Following the original S4 paper, we evaluate on the univariate ETTh 1task, which involves
predicting electricity transformer temperature at hour-long granularities (i.e., 24, 48, 168, 336, and 720 hours
in the future). For each prediction task, we use the same number of hours before as a look-back window to
19

--- PAGE 20 ---
Table 16: Downstream performance on brain fMRI data.
Dataset Model F1
MDTB Transformer 91.8
H3 92.0
H3 + Long Convs, Rand Init 92.1
H3 + Long Convs, Exp Init 91.6
HCP Transformer 83.4
H3 82.6
H3 + Long Convs, Rand Init 82.3
H3 + Long Convs, Exp Init 83.6
input to the model. As LongConvs can be a drop-in replacement for the S4 kernel, we also follow the approach
taken in S4 that simply masks out the future time steps in the input sequence and treat the task as a masked
sequence-to-sequence transformation. Table 15 shows the results. Long convolutions match or outperform S4
on all context windows, and outperforms custom hand-crafted architectures designed specically for time
series forecasting.
B.3 Brain fMRI Downstream Adaptation
We further evaluate the performance of the pre-trained models in two benchmark mental state decoding
datasets from the Human Connectome Project [HCP; 5] and multi-domain task battery [MDTB; 41], spanning
20 and 26 distinct mental states respectively. To adapt the pre-trained models to the mental state decoding
(i.e., classication) task, we add a learnable classication embedding Ecls2Rnto the end of input sequences
Xand forward the model's corresponding prediction to a decoding head p(), composed of a dense hidden
layer withemodel units (one for each embedding dimension, with tanh activation) as well as a softmax
output layer (with one model unit ifor each considered mental state in the data). Accordingly, we adapt
models by optimizing a standard cross entropy loss objective:  P
iyilogp(f(EX))i, whereyiindicates
a binary variable that is 1 if iis the correct mental state and 0 otherwise. We always begin downstream
adaptation with the pre-trained model parameters and allow all parameters to change freely during training.
We randomly split each of the two downstream datasets into distinct training (90% of fMRI runs) and test
(10% of fMRI runs) datasets and adapt models for 1 ;000 training steps at a mini-batch size of 256 and
a learning rate of 5 e 5(otherwise using the same learning parameters as for upstream training). During
training, we sample sequences from the fMRI datasets according to the accompanying event les, which
specify the beginning and end of each experimental trial underlying a mental state [when accounting for the
temporal delay of the haemodynamic response function; for details, see 72].
The adapted H3 variants with long convolutions perform on par with the other models in accurately
identifying the mental states of the downstream evaluation datasets (see Table 16: F1-scores are macro-
averaged).
C Methods Details
We discuss details of our methods.
C.1 Buttery Decompositions
We describe how to construct Din the Buttery decomposition, and Bin the three pass algorithm.
Twiddle Matrices We describe how to construct N1N2Twiddle matrices.
LetM2CN1N2. ThenMj;k=exp( 2ijk=N ). The twiddle factors Dcan be constructed by attening
Mand using them along the diagonal of D.
20

--- PAGE 21 ---
Buttery Matrix We construct Bin the three pass algorithm.
LetB(m)denote the Buttery matrix that needs to be constructed for a three pass algorithm with
N=lm, and assume that mis a power of 2. B(m)is a block matrix, where each block is a diagonal matrix.
In particular, we have:
B=2
64D1;1::: D1;m
.........
Dm;1:::Dm;m;3
75:
We show how to construct Dj;k.Dj;kis a diagonal matrix of size ll. The entries of Dj;kare given by
the following:
Dj;k[] = exp( 2ik(jl+)=N):
C.2 Additional details about the three pass algorithm
We share a few additional details about the three pass algorithm that allow for ecient training.
The buttery matrices Bhave complex coecients. Typically, we train models over real time series.
This mismatch has the potential to increase the amount of GPU memory IO: it is necessary to read Nreal
numbers, but write Ncomplex numbers.
We can alleviate this problem by using a well-known transformation between a real FFT of length 2 Land
a complex FFT of length L[7]. In essense, a real FFT of length 2 Lcan be converted into a complex FFT of
lengthL. In our algorithm, we exploit this as follows:
â€¢Given an input of real points N, reshape the input to be a complex input of length N=2.
â€¢Compute the complex FFT convolution over the input of length N=2 using the three pass algorithm.
â€¢Convert the output to be a real output of length N.
The rst and last steps can be fused with a Buttery matrix multiplication kernel, thereby keeping the total
IO cost the same as the original algorithm.
D Theory
D.1 Three-Pass Algorithm
We prove Proposition 1.
Convolution Recall that a convolution between two vectors uandkof lengthNis given by the following:
uk=FLDiag( FLk)FLu:
We can precompute FLk, since it is shared across all inputs in a batch. Let D=FLk. Then, the above is
given by:
uk=FLDFLu:
Decomposition One property of FLis that it can be decomposed. For example, if L= 2l, then we can
write the following:
F2l=BFl0
0Fl
P;
where Pis a permutation matrix (in this case, an even-odd permutation), and Bis a Buttery matrix.
21

--- PAGE 22 ---
We can leverage this to re-write a convolution of length 2 l. Letuandkbe vectors of length 2 l. Then, we
can write the following:
uk=F2lDF2lu
=F2lDF 1
2lu
=BFl0
0Fl
PDP 1F 1
l0
0 F 1
l
B 1u
=BFl0
0Fl
D0F 1
l0
0 F 1
l
B 1u;
for some diagonal matrix D0. Note that the three terms in the middle can be computed in parallel.
This pattern extends to L= 2ml, and yields 2mparallelism in the product.
It remains to show that each of the Buttery matrices can be computed with a single read/write over the
input sequence.
Recall that the Buttery matrices have the following form:
B=2
64D1;1::: D1;m
.........
Dm;1:::Dm;m;3
75
where the Di;jare diagonal matrices of size ll.
A matrix-vector multiply y=Bucan be partitioned on a GPU as follows. Suppose that each SM has
enough shared memory to store lelements of the input. Let there be mSMs processing this input. Each SM
will readlinput and write loutput, for ml=Ntotal reads and writes.
Specically, SM i will read
u[(l=m)i: (l=m)(i+ 1)];
u[l+ (l=m)i:l+ (l=m)(i+ 1)];:::;
u[(m 1)l+ (l=m)i: (m 1)l+ (l=m)(i+ 1)]:
These inputs are exactly the inputs needed to compute:
y[(l=m)i: (l=m)(i+ 1)];
y[l+ (l=m)i:l+ (l=m)(i+ 1)];:::;
y[(m 1)l+ (l=m)i: (m 1)l+ (l=m)(i+ 1)]:
The SM can then distribute these portions of the matrix-vector multiply to the independent threads of the
SM.
This completes the proof.
D.2 Expressivity of Long Convolutions
We show that long convolutions and SSMs are equivalent in expressivity (the subset relation in Figure 1 right
is actually set equality).
Proposition 2. LetMbe a positive integer that evenly divides N. Any convolution kernel of length Ncan
be written as the sum of N=M diagonal SSMs with hidden state M.
Proof. For the case M= 1, consider a diagonal SSM with A2RNNdiagonal with entries a1;:::;aN, and
B2RN1. For simplicity, we will roll CintoBand set D= 0.
This SSM gives rise to the following kernel Kwith entries:
Ki=AiB=NX
j=1ai
jbj:
22

--- PAGE 23 ---
This is equivalent to
K=VB;
where Vis the transpose of a Vandermonde matrix
V=2
6641a1a2
1::: aN 1
1
1a2a2
2::: aN 1
2
::::::::::::::::::::::::
1aNa2
N::: aN 1
N3
775T
:
Vandermonde matrices have a determinant that is nonzero if and only if a1;:::;aNare all distinct. Thus VT
is invertible if a1;:::;aNare distinct and hence Vis also invertible if a1;:::;aNare distinct. Given a kernel ^K,
we can thus express that kernel by picking any a1;:::;aNthat are distinct and then picking B=V 1^K, then
VB=VV 1^K=^K, nishing the proof.
In the case where M > 1 we have consider a diagonal SSM with A2RNNdiagonal with entries
a1;:::;aN, and B2RN1. Partition, the state NintoN=M partitions of size M. Let(i;j) denote the
partition function that bijectively maps ( i;j) pairs to [1 ;:::;N ] for 1iN=M; 1jM.
Then the convolution kernel has the following entries Ki:
Kl=NX
i=1al
ibi=N=MX
i=1MX
j=1al
(i;j)b(i;j):
Consider the inner sumPM
j=1al
(i;j)b(i;j). This denes a convolution kernel given by a diagonal SSM with
hidden state M,Awith diagonal entries [ a(i;1);:::;a(i;M)], and B= [b(i;1);:::;b(i;M)]T.
Thus, this diagonal SSM with hidden state Nis the sum of N=M diagonal SSMs with hidden state M.
Proposition 2 suggests that long convolutions and SSMs have fundamentally the same expressive power,
especially when SSMs are used in a deep architecture that stacks multiple independent SSMs in layers. The
signicance of this result is that this allows us to view SSMs and general long convolutions as the same
construct.
E Additional Methods
We discuss some additional methods that we tried but did not include in the main body of the paper.
E.1 Constant-Recursive Kernels
In early explorations, we explored a constant-recursive formulation of convolution kernels as a mechanism for
hardware eciency. We ultimately did not go with this route due to the development of FlashButterfly .
We wish to develop kernels such that the output of convolving the kernels with a signal can be computed
recurrently. The goal is to develop long convolution kernels that can be computed eciently.
We show that constant-recursive kernels satisfy our requirements. In particular, the output of convolving
a constant-recursive kernel with a signal results in an output that is also constant-recursive. We show that
our formulation of constant-recursive kernels is expressive enough to capture S4D kernels|and by corollary
by Proposition 2, any kernel.
E.1.1 Background: Constant-Recursive Sequence
We dene constant-recursive sequences.
Constant-Recursive Sequence A constant-recursive sequence is a sequence of numbers s1;s2;::: that
satises the following recursive function:
sn=a1sn 1+a2sn 2++apsn p= p
jajsn j;
for alln>p , wherea1;:::;apare constants. We will call pthepower of the constant-recursive sequence
(this terminology may not be standard).
23

--- PAGE 24 ---
E.1.2 Constant-Recursive Kernel
We use the idea of constant-recursive sequences to dene a constant-recursive kernel. Our key insight is that
the convolution of a constant-recursive sequence with a signal is itself a constant-recursive sequence. We will
dene our kernel as the sum of dconstant-recursive kernels, where dis a hidden state dimension (equivalent
to the state dimension din a state space model). We will show that our formulation is expressive enough to
capture S4D.
We will dene the convolution kernel Kthrough a recurrence relation with dimension d, and power p:
Ki= d
r=1Ki;r
Ki;r=(
ki;r for 1ip
min(p;i p)
j=1aj;rKi p j+1;rotherwise
Note that each Ki;ris a form of constant-recursive sequence, with a form of delay in the sequence. In
particular, we have that Ki;rdepends on Ki 2p+1;r;:::;Ki p;r. We make this choice for computational
reasons|it means we can compute Kp+1;r;:::;K 2p;rat once, then K2p+1;r;:::;K 3p;r, etc each in one go.
Formally, it is equivalent to a constant-recursive sequence with twice the power, but where the rst pconstants
are all zeros.
The special cases d= 1;p> 1 andd>1;p= 1 are worth analyzing separately to develop some intuition
about what this convolution does.
E.1.3 Case d= 1;p> 1:
We rst analyze the case when d= 1 to develop some intuition about what this kernel is expressing. We will
see that using this kernel in a convolution yields a constant-recursive output.
Whend= 1, the kernel expression becomes
Ki;1=(
ki;1 for 1ip
min(p;i p)
j=1aj;1Ki p j+1;1otherwise(4)
We show that using this kernel in a convolution results in a constant-recursive output sequence:
Proposition 3. LetK2RLbe a kernel dened by Equation 4, and let u2RL. Theny=uK2RLis
given by the following:
yi= min(i;p)
j=1kjui j+1+ min(p;i p)
j=1ajyi d j+1 (5)
The signicance of Proposition 3 is that yihas the exact same constant-recursive structure as Ki;r{ and
can thus be computed as a recurrently.
Equivalent SSM We construct the A,B,Cmatrices for an SSM that produces this kernel. Let B=
[k1;1;:::;k 1;p]T,C= [1;0;:::; 0]T, and A2Rppbe the following (inverted companion) matrix:
A=2
6666640 1 0 ::: 0
0 0 1 ::: 0
...
0 0 0 ::: 1
a1;1a1;2a1;3::: a 1;p3
777775
then Ki=CTAi 1B, which reveals that this constant-recursive matrix is equivalent to an SSM.
E.1.4 Case d>1;p= 1:
This case, where the constant-recursive sequence as power p= 1, recovers S4D with d>1.
24

--- PAGE 25 ---
The kernel denition simplies to:
Ki= d
r=1Ki;r
Ki;r=(
ki;r for 1 =i
a1;rKi 1;rotherwise
This ensures that
Ki= d
r=1k1;rai
1;r:
Now let B= [k1;1;:::;k 1;d]T,C= [1;:::;1]T, and A=diag(a1;1;:::;a 1;d). This ensures that Ki=CTAiB,
showing that diagonal SSMs can be recovered by constant-recursive kernels.
E.2 Wavelet Basis
In our initial explorations, we parameterized a Haar wavelet basis as a mechanism for producing convo-
lution kernels. We ultimately did not go with this route, since we found that a simpler solution (direct
parameterization) was sucient.
F Experiment Details
We discuss all the details of our experiments.
Table 17: The values of the best hyperparameters found; LRA, images, language, and time series, and brain
fMRI. LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer
Normalization. We use random weight initialization in all runs.
Depth Features H Norm kernel LR Dropout  Batch Size WD Epochs LR
ListOps 8 128 BN 0.0005 0.2 0.002 50 0.05 40 0.01
Text (IMDB) 6 256 BN 0.001 0.2 0.003 16 0.05 32 0.01
Retrieval (AAN) 6 256 BN 0.0001 0.1 0.004 32 0.05 20 0.01
Image 6 512 LN 0.001 0.2 0.003 25 0.05 200 0.01
Pathfinder 6 256 BN 0.001 0.3 0.001 64 0.03 200 0.004
Path-X 6 256 BN 0.0005 0.3 0.001 4 0.05 50 0.0005
sCIFAR 6 512 LN 0.001 0.2 0.001 50 0.05 300 0.01
2D CIFAR 4 128 LN 0.001 0 0.001 50 0.01 100 0.01
OpenWebText 12 768 LN 0.001 0 0.001 32 0.1 100B tokens 0.0003
Time Series 3 128 BN 0.001 0.2 0.003 50 0.01 50 1e-5
Brain Upstream 4 768 LN 0.001 0.2 0.0005 512 0.1 5000 steps 0.01
Brain Downstream 4 768 LN 0.001 0.2 0.00005 256 0.1 1000 steps 0.01
Hyperparameter Sweeps For all methods, we swept the following parameters:
â€¢Kernel Dropout: [0.1, 0.2, 0.3, 0.4, 0.5]
â€¢Kernel LR: [0.0001, 0.0005, 0.001]
â€¢: [0.001, 0.002, 0.003, 0.004, 0.005]
Compute Infrastructure The experiments in this paper were run on a mixture of dierent compute
platforms. The LRA experiments, except for Path-X, were swept on a heterogeneous cluster of 1xV100 and
2xV100 nodes. Path-X and sequential CIFAR were run on single 8xA100 nodes. The language modeling
experiments were run on a single 8xA100 node. The time series experiments were run on a cluster with
1xP100 nodes. The brain fMRI experiments were run on a cluster of 2xV100 nodes.
Final Hyperparameters Final hyperparameters for reported results are given in Table 17.
25

--- PAGE 26 ---
Figure 4: Mean absolute error of pre-trained models in upstream evaluation data for each location of the
brain. Brain maps are projected onto the inated cortical surface of the FsAverage template [22].
F.1 Functional Magnetic Resonance Imaging Data
Neuroimaging research can be considered as recently entering a big data era, as individual researchers publicly
share their collected datasets more frequently. This development opens up new opportunities for pre-training
at scale in neuroimaging research, as recently demonstrated by Thomas et al. [72]. In their work, the authors
show that Transformers, pre-trained to predict brain activity for the next time point of input fMRI sequences,
outperform other models in learning to identify the mental states (e.g., happiness or fear) underlying new
fMRI data. Recently, Dao et al. [17]have shown that H3 performs on par with Transformers in this transfer
learning paradigm.
To test whether long convolutions also perform on par with SSMs, as implemented in H3, and Transformers
in this paradigm, we replicate the analyses of Thomas et al. [72], using their published fMRI datasets.
Conventionally, functional Magnetic Resonance Imaging (fMRI) data are represented in four dimensions,
describing the measured blood-oxygen-level-dependent (BOLD) signal as a sequence S=fV1;:::;Vtgof
3-dimensional volumes V2Rxyz, which show the BOLD signal for each spatial location of the brain (as
indicated by the three spatial dimensions x,y, andz). Yet, due to the strong spatial spatial correlation
of brain activity, fMRI data can also be represented dierently, by representing individual sequences as a
set 21;:::;nofnfunctionally-independent brain networks , where each network describes the BOLD
signal for some subset of voxels vx;y;z2V[e.g., 12]. The resulting sequences X2Rtnindicate whole-brain
activity as a set of nbrain networks for ttime points2.
Upstream learning: In line with Thomas et al. [72], we pre-train models f() to predict whole-brain
activity for the next time point jof an fMRI sequence X, using a mean absolute error (MAE) training
objective, given the model's prediction ^X2Rtn: MAE =1
nPn
i=1jXj;i ^Xj;ij;^Xt;n=bn+P
nf(EX)t;ewe;n;
EX
t;e=ETR+Epos+be+P
nXt;nwn;e. Here,ETR2ReandEpos2Rerepresent learnable embeddings for
each possible time point and position of an input sequence [for details, see 72]3. Note that f() processes the
input in a lower-dimensional representation EX2Rte, wheree= 768, obtained through linear projection.
In line with Thomas et al. [72]and Dao et al. [16], we pre-train a Transformer decoder (based on GPT)
with 4 hidden layers and 12 attention heads and a H3 model with 4 hidden layers [with H= 64 andm= 1;
see17] in this task. For both models, the sequence of hidden-states outputs of the last model layer are used
to determine ^X(scaled to the original input dimension with linear projection). We also pre-train variants of
H3 that replace its SSM kernel with long convolutions.
We randomly divide the upstream data, which spans fMRI data from 11 ;980 experimental runs of 1 ;726
individuals, into distinct training and validation datasets by randomly designating 5% of the fMRI runs as
2Thomas et al. [72] use n= 1;024 networks dened by the Dictionaries of Functional Modes [DiFuMo; 12] Atlas.
3As the sampling frequency of fMRI is variable between datasets, the same position of an input sequence can correspond to
dierent time points.
26

--- PAGE 27 ---
validation data and using the rest of the runs for training. During training, we randomly sample sequences of
100 time points from the fMRI runs and train models with the ADAM optimizer (with 1= 0:9,2= 0:999,
and= 1e 8) for 5;000 steps at a mini-batch size of 512 and a learning rate of 5 e 4. We also apply a linear
learning rate decay schedule (with a warm-up phase of 10% of the total number of training steps), gradient
norm clipping at 1 :0,L2-regularisation (weighted by 0 :1), and dropout at a rate of 0 :2 (throughout all models).
The adapted H3 variants clearly outperform the other models in accurately predicting brain activity for the
next time point of input sequences (Table 9 of the main text). We also nd that the pre-trained models
exhibit similar evaluation MAE error distributions throughout the brain, with relatively higher errors in the
posterior parietal, occipital, and cingulate cortices as well parts of the limbic system (Fig. 4).
27
