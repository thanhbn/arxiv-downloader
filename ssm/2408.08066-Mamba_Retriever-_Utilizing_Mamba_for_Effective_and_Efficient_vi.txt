# 2408.08066.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2408.08066.pdf
# Kích thước tệp: 551848 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mamba Retriever: Sử dụng Mamba cho Truy xuất Dày đặc Hiệu quả và Hiệu suất
Hanqi Zhang
Trường Trí tuệ Nhân tạo Gaoling
Đại học Nhân dân Trung Quốc
Bắc Kinh, Trung Quốc
zhanghanqi@ruc.edu.cn

Chong Chen
Huawei Cloud
Bắc Kinh, Trung Quốc
chenchong55@huawei.com

Lang Mei
Trường Trí tuệ Nhân tạo Gaoling
Đại học Nhân dân Trung Quốc
Bắc Kinh, Trung Quốc
meilang2013@ruc.edu.cn

Qi Liu
Trường Trí tuệ Nhân tạo Gaoling
Đại học Nhân dân Trung Quốc
Bắc Kinh, Trung Quốc
liuqi_67@ruc.edu.cn

Jiaxin Mao*
Trường Trí tuệ Nhân tạo Gaoling
Đại học Nhân dân Trung Quốc
Bắc Kinh, Trung Quốc
maojiaxin@gmail.com

Tóm tắt
Trong lĩnh vực truy xuất thông tin (IR), các mô hình truy xuất dày đặc (DR) sử dụng kỹ thuật học sâu để mã hóa các truy vấn và đoạn văn vào không gian nhúng để tính toán mối quan hệ ngữ nghĩa của chúng. Điều quan trọng là các mô hình DR phải cân bằng cả hiệu suất và hiệu quả. Các mô hình ngôn ngữ được huấn luyện trước (PLM), đặc biệt là các PLM dựa trên Transformer, đã được chứng minh là các bộ mã hóa hiệu quả của các mô hình DR. Tuy nhiên, thành phần tự chú ý trong PLM dựa trên Transformer dẫn đến độ phức tạp tính toán tăng theo bậc hai với độ dài chuỗi, và do đó thể hiện tốc độ suy luận chậm cho việc truy xuất văn bản dài. Một số PLM không phải Transformer được đề xuất gần đây, đặc biệt là các PLM kiến trúc Mamba, đã chứng minh không chỉ hiệu quả tương đương với các PLM dựa trên Transformer trên các tác vụ ngôn ngữ sinh mà còn hiệu suất tốt hơn do việc mở rộng thời gian tuyến tính trong độ dài chuỗi. Bài báo này triển khai Mamba Retriever để khám phá liệu Mamba có thể phục vụ như một bộ mã hóa hiệu quả và hiệu suất của mô hình DR cho các tác vụ IR. Chúng tôi tinh chỉnh Mamba Retriever trên bộ dữ liệu xếp hạng đoạn văn ngắn kinh điển MS MARCO và bộ dữ liệu văn bản dài LoCoV0. Kết quả thực nghiệm cho thấy (1) trên bộ dữ liệu xếp hạng đoạn văn MS MARCO và BEIR, Mamba Retriever đạt được hiệu quả tương đương hoặc tốt hơn so với các mô hình truy xuất dựa trên Transformer, và hiệu quả tăng theo kích thước của mô hình Mamba; (2) trên bộ dữ liệu văn bản dài LoCoV0, Mamba Retriever có thể mở rộng đến độ dài văn bản dài hơn so với độ dài được huấn luyện trước sau khi tinh chỉnh trên tác vụ truy xuất, và nó có hiệu quả tương đương hoặc tốt hơn so với các mô hình truy xuất văn bản dài khác; (3) Mamba Retriever có tốc độ suy luận vượt trội cho việc truy xuất văn bản dài. Kết luận, Mamba Retriever vừa hiệu quả vừa hiệu suất, làm cho nó trở thành một mô hình thực tế, đặc biệt cho việc truy xuất văn bản dài.

*Tác giả liên hệ.
Được phép tạo bản sao kỹ thuật số hoặc in ấn toàn bộ hoặc một phần của tác phẩm này cho mục đích cá nhân hoặc lớp học mà không cần phí miễn là các bản sao không được tạo hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của tác phẩm này thuộc sở hữu của những người khác ngoài (các) tác giả phải được tôn trọng. Tóm tắt có ghi công được phép. Để sao chép theo cách khác, hoặc tái xuất bản, đăng lên máy chủ hoặc phân phối lại cho danh sách, cần có sự cho phép cụ thể trước và/hoặc một khoản phí. Yêu cầu quyền từ permissions@acm.org.
Conference acronym 'XX, June 03-05, 2018, Woodstock, NY
©2024 Bản quyền thuộc về (các) chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX

tốc độ suy luận cho việc truy xuất văn bản dài. Kết luận, Mamba Retriever vừa hiệu quả vừa hiệu suất, làm cho nó trở thành một mô hình thực tế, đặc biệt cho việc truy xuất văn bản dài.

CCS Concepts
•Information systems →Retrieval models and ranking .

Keywords
truy xuất thông tin, mô hình ngôn ngữ được huấn luyện trước, mô hình không gian trạng thái

ACM Reference Format:
Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, và Jiaxin Mao. 2024. Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 5 pages. https://doi.org/XXXXXXX.XXXXXXX

1 Giới thiệu
Truy xuất thông tin (IR) nhằm mục đích truy xuất các đối tượng thông tin có liên quan đến truy vấn của người dùng từ một bộ sưu tập quy mô lớn. Các mô hình truy xuất dày đặc (DR) [10] được đề xuất để đánh giá mức độ liên quan giữa truy vấn và đoạn văn bằng cách mã hóa chúng thành các nhúng và tính toán độ tương tự trong không gian nhúng. Sau đó, sử dụng các thuật toán Tìm kiếm Láng giềng Gần nhất Xấp xỉ [24] trên các nhúng, chúng ta có thể truy xuất top-k đoạn văn liên quan đến truy vấn.

Mô hình DR cần cân bằng cả hiệu quả và hiệu suất. Về hiệu quả, các mô hình DR tập trung vào việc cải thiện hiệu suất truy xuất, được ảnh hưởng bởi các yếu tố bao gồm khả năng hiểu ngữ nghĩa và tóm tắt vốn có của mô hình. Về hiệu suất, bài báo này tập trung vào thời gian suy luận đoạn văn thay vì truy vấn, có tiềm năng cải thiện lớn hơn.

Các Mô hình Ngôn ngữ Được huấn luyện trước (PLM) đã chứng minh hiệu quả của chúng trên các tác vụ downstream, nhờ vào kiến thức thế giới đầy đủ và kiến thức ngữ nghĩa mà chúng thu được trong quá trình huấn luyện trước. Đặc biệt, các PLM dựa trên Transformer [11,26,27] thể hiện lợi thế của việc nắm bắt các phụ thuộc tầm xa bằng cơ chế tự chú ý và cho phép huấn luyện song song. Nhiều nghiên cứu [10,15,17,19] đã đề xuất áp dụng các PLM dựa trên Transformer làm bộ mã hóa của các mô hình DR và đã quan sát được hiệu quả của chúng.

arXiv:2408.08066v2 [cs.IR] 22 Aug 2024

--- TRANG 2 ---
Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, và Jiaxin Mao

Tuy nhiên, bất chấp hiệu quả của các mô hình DR dựa trên Transformer, hiệu suất bị hạn chế bởi kiến trúc mô hình vốn có của chúng. Cụ thể, thành phần tự chú ý trong các PLM dựa trên Transformer dẫn đến độ phức tạp tính toán tăng theo bậc hai với độ dài chuỗi. Do đó, đối với các tác vụ truy xuất có đoạn văn dài, chẳng hạn như tác vụ truy xuất vụ án pháp lý [13], các mô hình DR dựa trên Transformer thể hiện tốc độ suy luận chậm.

Một số PLM không phải Transformer [6,7,20] được đề xuất để cải thiện hiệu suất mà không hy sinh hiệu quả. Đặc biệt, các PLM dựa trên Mamba thể hiện hiệu suất tương đương với các PLM dựa trên Transformer trên các tác vụ ngôn ngữ sinh và đạt được việc mở rộng thời gian tuyến tính trong độ dài chuỗi dựa trên cơ chế mô hình không gian trạng thái chọn lọc. Gần đây, Xu đã đề xuất mô hình RankMamba [28], tận dụng Mamba cho các tác vụ xếp hạng lại. Công trình của chúng tôi khác biệt ở hai khía cạnh chính: (1) Mamba Retriever được đề xuất là một mô hình bi-encoder được thiết kế cho hiệu quả và hiệu suất trong các tác vụ truy xuất giai đoạn đầu. (2) Chúng tôi mở rộng việc điều tra các mô hình dựa trên Mamba cho các tác vụ truy xuất văn bản dài.

Trong bài báo này, chúng tôi đề xuất khám phá liệu Mamba có thể phục vụ như một bộ mã hóa hiệu quả và hiệu suất của mô hình DR cho các tác vụ IR. Chúng tôi trả lời các câu hỏi nghiên cứu sau:

RQ1: Mô hình truy xuất Mamba có hiệu quả tương đương trên việc truy xuất kinh điển so với các mô hình truy xuất Transformer không?
RQ2: Mô hình truy xuất Mamba có hiệu quả tương đương trên việc truy xuất văn bản dài so với các mô hình truy xuất văn bản dài hiện có không?
RQ3: Hiệu suất suy luận của mô hình truy xuất Mamba so sánh như thế nào với các mô hình truy xuất hiện có trên các độ dài văn bản khác nhau?

Để giải quyết các câu hỏi nghiên cứu này, chúng tôi triển khai Mamba Retriever, một mô hình truy xuất bi-encoder dựa trên Mamba. Chúng tôi tinh chỉnh nó trên bộ dữ liệu xếp hạng đoạn văn MS MARCO [18] cho việc truy xuất văn bản ngắn kinh điển và trên bộ dữ liệu LoCoV0 [23] cho việc truy xuất văn bản dài.

Chúng tôi đóng góp như sau: (1) Chúng tôi triển khai Mamba Retriever để đạt được cả hiệu quả và hiệu suất. (2) Chúng tôi khám phá hiệu quả truy xuất của Mamba Retriever ở các kích thước mô hình khác nhau. Chúng tôi cho thấy rằng, trên bộ dữ liệu xếp hạng đoạn văn MS MARCO và các bộ dữ liệu BEIR [25], Mamba Retriever có hiệu quả tương đương hoặc tốt hơn so với các retriever Transformer, và hiệu quả cũng tăng theo kích thước của mô hình Mamba. (3) Ngoài ra, chúng tôi tập trung vào hiệu quả của việc truy xuất văn bản dài. Chúng tôi cho thấy rằng, trên bộ dữ liệu văn bản dài LoCoV0, Mamba Retriever có thể mở rộng đến độ dài văn bản dài hơn so với độ dài được huấn luyện trước sau khi tinh chỉnh trên tác vụ truy xuất, và nó có hiệu quả tương đương hoặc tốt hơn so với các mô hình truy xuất văn bản dài khác. (4) Chúng tôi khám phá hiệu suất suy luận đoạn văn của Mamba Retriever ở các độ dài đoạn văn khác nhau. Chúng tôi cho thấy rằng Mamba Retriever có tốc độ suy luận vượt trội với việc mở rộng thời gian tuyến tính cho việc truy xuất văn bản dài.

Kết luận, Mamba Retriever vừa hiệu quả vừa hiệu suất, làm cho nó thực tế cho IR, đặc biệt cho các tác vụ IR văn bản dài. Thêm chi tiết có sẵn tại https://github.com/41924076/MambaRetriever.

2 Công trình Liên quan
Các Mô hình Ngôn ngữ Được huấn luyện trước (PLM). Thông qua việc huấn luyện trước, các mô hình ngôn ngữ có thể đạt được hiệu suất cao hơn khi được chuyển giao cho các tác vụ cụ thể. Transformer [27], dựa trên cơ chế tự chú ý, là một kiến trúc chính của PLM, bao gồm mô hình chỉ encoder [11,14] và mô hình chỉ decoder [2,29]. Để giải quyết việc mở rộng thời gian bậc hai của kiến trúc Transformer, một số kiến trúc được coi rộng rãi là các mô hình không gian trạng thái đã được đề xuất [8,20], đặc biệt là các mô hình hiệu suất cao như kiến trúc sub-quadratic M2-BERT [6], kiến trúc tuyến tính Mamba [7] và Mamba-2 [5].

Các Mô hình Truy xuất Dày đặc. Các PLM Transformer đã được chứng minh hiệu quả cho việc truy xuất dày đặc. Ban đầu, các mô hình chỉ encoder được áp dụng cho các tác vụ truy xuất do các cơ chế chú ý hai chiều [10,19]. Sau đó, các mô hình chỉ decoder được áp dụng cho các tác vụ truy xuất do hiệu quả của chúng trên kích thước mô hình lớn hơn [15, 17].

Các Mô hình Truy xuất Dày đặc Văn bản Dài. Trong việc truy xuất văn bản dài, các công trình đầu sử dụng các chiến lược phân đoạn [4] do cửa sổ ngữ cảnh nhỏ. Để giúp mô hình hiểu ngữ nghĩa hoàn chỉnh và mạch lạc tốt hơn, một số nghiên cứu khám phá các mô hình truy xuất văn bản dài dựa trên Transformer [9, 15, 30].

Đối mặt với việc mở rộng thời gian bậc hai của các mô hình truy xuất văn bản dài dựa trên Transformer, mô hình M2-BERT sub-quadratic [6] đã được sử dụng cho các tác vụ truy xuất văn bản dài [23].

3 Phương pháp
3.1 Mamba Retriever

Định nghĩa Tác vụ. Trong các tác vụ truy xuất văn bản, cho một truy vấn q và một tập hợp đoạn văn quy mô lớn {p₁,p₂,...,pₙ}, mô hình truy xuất nhằm mục đích tìm top-k đoạn văn có liên quan nhất đến q.

Tổng quan về Mamba Retriever. Để tính toán mức độ liên quan giữa một truy vấn q và một đoạn văn p, Mamba Retriever sử dụng kiến trúc bi-encoder. Bi-encoder có nghĩa là mô hình biểu diễn q và p dưới dạng nhúng vector dày đặc Eq và Ep tương ứng, và mức độ liên quan giữa truy vấn q và đoạn văn p có thể được tính bằng độ tương tự cosine giữa các biểu diễn dày đặc của chúng.

sim(q,p) = Ep·Eq / (∥Ep∥∥Eq∥)    (1)

Cụ thể, để tạo nhúng E, chúng tôi sử dụng mô hình ngôn ngữ tự hồi quy Mamba làm mô hình cơ sở M. Chúng tôi đưa một chuỗi token t₁,t₂,...,tₗ có độ dài chuỗi L và một token <EOS> ở cuối chuỗi vào mô hình M, và trích xuất đầu ra của <EOS> tại lớp ẩn cuối cùng trong M làm nhúng E:

E = M(t₁,t₂,...,tₗ,<EOS>)[-1]    (2)

Mamba và SSM. Mô hình cơ sở Mamba, như chúng ta đã ký hiệu trước đó là M, là một mô hình được xây dựng bằng cách xếp chồng nhiều khối Mamba. Thành phần cốt lõi của khối Mamba là mô hình không gian trạng thái chọn lọc dựa trên mô hình không gian trạng thái (SSM) [8].

Một SSM ánh xạ chuỗi đầu vào 1 chiều x(t) trong R với bước thời gian t đến chuỗi đầu ra y(t) trong R thông qua trạng thái tiềm ẩn h(t) trong R^N:

h'(t) = Ah(t) + Bx(t)
y(t) = Ch(t)    (3)

trong đó A trong R^N×N, B trong R^N×1, C trong R^1×N. Sử dụng kích thước bước Δ, dạng liên tục ở trên có thể được chuyển thành dạng rời rạc:

hₜ = Āhₜ₋₁ + B̄xₜ
yₜ = Chₜ    (4)

trong đó Ā = exp(ΔA) và B̄ = (ΔA)⁻¹(exp(ΔA)-I)·ΔB là một trong các phương pháp rời rạc hóa. SSM 1 chiều ở trên có thể được mở rộng thành d chiều độc lập.

Dựa trên SSM, Mamba giới thiệu một cơ chế lựa chọn và thuật toán song song nhận biết phần cứng tương ứng. Cơ chế lựa chọn là làm cho Δ, B, C phụ thuộc vào token hiện tại. Nó

--- TRANG 3 ---
Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval Conference acronym 'XX, June 03-05, 2018, Woodstock, NY

cho phép mô hình chọn lọc quên hoặc nhớ thông tin dọc theo chiều của độ dài chuỗi.

Mục tiêu Huấn luyện. Để huấn luyện Mamba Retriever, chúng tôi sử dụng loss InfoNCE, được sử dụng phổ biến nhất. Nó buộc q và d có ngữ nghĩa tương tự phải gần nhau hơn trong không gian nhúng:

L = -log(e^(sim(q,d+)/τ) / (e^(sim(q,d+)/τ) + Σ_{d-_i∈D-} e^(sim(q,d-_i)/τ)))    (5)

trong đó d+ là đoạn văn liên quan của truy vấn, D- là một tập hợp các đoạn văn không liên quan của truy vấn, và τ là hệ số nhiệt độ.

3.2 So sánh Mô hình Cơ sở
Trong Phần 3.1, chúng tôi để Mamba là mô hình cơ sở M của retriever. Để so sánh, mô hình cơ sở M được thay đổi từ Mamba thành các mô hình cơ sở thường được sử dụng khác, bao gồm các mô hình Transformer chỉ encoder và chỉ decoder. Trong phần này, chúng tôi phân tích sự khác biệt giữa việc sử dụng Mamba và các mô hình này làm M.

Mamba và Chỉ Decoder so với Chỉ Encoder. Mamba và các mô hình Transformer chỉ decoder có những điểm tương đồng khiến chúng khác biệt với các mô hình Transformer chỉ encoder. Về dữ liệu, Mamba và các mô hình chỉ decoder được huấn luyện trước trên nhiều dữ liệu hơn so với hầu hết các mô hình chỉ encoder. Đặc biệt, Mamba và Pythia [2] được huấn luyện trước trên cùng dữ liệu. Về kiến trúc, Mamba và các mô hình chỉ decoder có đặc tính nhân quả, không phù hợp bằng các mô hình chỉ encoder với chú ý hai chiều cho các tác vụ hiểu như truy xuất. Mamba có thể được tái cấu trúc để trở thành hai chiều, nhưng điều này sẽ dẫn đến giảm hiệu suất.

Mamba so với Chỉ Decoder. Trực quan, mô hình Transformer chỉ decoder có thể nắm bắt các phụ thuộc dài hạn bằng cơ chế tự chú ý, trong khi Mamba có thể bị hạn chế bởi lượng thông tin tối đa có thể được nén trong các trạng thái tiềm ẩn.

Tuy nhiên, một số công trình [1,3,5,16] phân tích rằng Mamba có một số cơ chế tương tự hoặc thậm chí vượt trội hơn Transformer: Mamba có cơ chế chú ý ngầm với khả năng biểu đạt tốt; nếu mỗi SSM được coi như một đầu trong cơ chế tự chú ý đa đầu, thì Mamba có nhiều đầu hơn Transformer; softmax trong tự chú ý có thể gây ra các vấn đề, chẳng hạn như làm mượt quá mức, trong khi Mamba không sử dụng softmax và do đó có thể nắm bắt tốt hơn những khác biệt tinh tế giữa các token khác nhau.

Ngoài ra, Mamba có một quá trình tóm tắt thông tin trước đó rõ ràng bổ sung sử dụng các trạng thái tiềm ẩn. Khi tính toán một token tại vị trí t, mô hình chỉ decoder sử dụng cơ chế chú ý để truy cập các khóa và giá trị của tất cả các token trước đó. Tuy nhiên, Mamba sử dụng cơ chế SSM để kết hợp thông tin của tất cả các token trước đó trong một trạng thái tiềm ẩn có kích thước cố định h_{t-1} phục vụ như một bản tóm tắt, và sau đó tính toán đầu ra của vị trí t sử dụng h_{t-1} theo công thức 4. Chúng ta có thể có một giả định rằng Mamba có thể có được khả năng tóm tắt tốt trong quá trình huấn luyện trước, điều này có lợi cho việc truy xuất nơi mục tiêu là tóm tắt thông tin truy vấn hoặc đoạn văn thành một nhúng.

4 Thực nghiệm
4.1 Thực nghiệm trên MS MARCO

Bộ dữ liệu. Chúng tôi tinh chỉnh và đánh giá trên bộ dữ liệu xếp hạng đoạn văn MS MARCO [18], một bộ dữ liệu truy xuất văn bản ngắn kinh điển với giá trị độ dài ký tự đoạn văn trung bình dưới 400. MRR@10 và Recall@1k là các chỉ số phổ biến.

Ngoài ra, chúng tôi tiến hành đánh giá zero-shot trên 13 bộ dữ liệu BEIR [25] bao gồm ArguAna, Climate-FEVER, DBPedia, FEVER, FiQA, HotpotQA, NFCorpus, NQ, Quora, SCIDOCS, SciFact, TREC-COVID, Tóuche-2020. nDCG@10 trung bình là một chỉ số phổ biến.

Chi tiết Triển khai. Chúng tôi khởi tạo mô hình từ một checkpoint Mamba được huấn luyện trước và huấn luyện nó trên 4 ×64G V100 GPU. Vì hiệu suất quan trọng cho các tác vụ truy xuất, chúng tôi tiến hành thực nghiệm trên các kích thước mô hình 130M, 370M và 790M. Chúng tôi thêm một token đặc biệt mới <EOS> vào cuối văn bản làm biểu diễn dày đặc. Chúng tôi chuẩn hóa biểu diễn dày đặc với chuẩn hóa L2, và cố định hệ số nhiệt độ ở 0.01. Vì mục đích của thực nghiệm không phải là sử dụng các kỹ thuật khác nhau để đạt được hiệu suất tối ưu, chúng tôi sử dụng kích thước batch là 2 trên mỗi GPU. Bằng cách chia sẻ các đoạn văn âm tính giữa các GPU và batch, mỗi truy vấn tổng cộng có 63 đoạn văn âm tính được khai thác bởi BM25. Chúng tôi sử dụng tốc độ học hiệu quả 1e-5 và huấn luyện cho đến khi hội tụ. Chúng tôi huấn luyện và đánh giá trên MS MARCO với độ dài tối đa truy vấn là {16, 32} tương ứng và độ dài tối đa đoạn văn là 128. Chúng tôi tiến hành đánh giá zero-shot trên BEIR với cả độ dài tối đa truy vấn và đoạn văn là 64.

Đối với baseline, mô hình cơ sở M được đề cập trong Phần 3.1 được thay đổi từ Mamba thành các mô hình Transformer, bao gồm mô hình chỉ encoder BERT [11] và RoBERTa [14], và mô hình chỉ decoder OPT [29] và Pythia [2]. Thiết lập huấn luyện về cơ bản giống như Mamba, với hai khác biệt chính: tốc độ học được đặt ở giá trị hiệu quả nhất cho từng mô hình tương ứng; BERT và RoBERTa sử dụng trạng thái ẩn cuối cùng của token tiền tố <CLS> làm nhúng văn bản.

Kết quả. Bảng 1 cho thấy hiệu quả trên bộ dữ liệu xếp hạng đoạn văn MS MARCO và các bộ dữ liệu BEIR. Tóm lại, Mamba cho thấy hiệu suất tương đương hoặc tốt hơn so với Transformer và cải thiện theo kích thước mô hình.

Khi so sánh với các mô hình Transformer chỉ decoder, Mamba cho thấy hiệu suất tốt hơn trên tất cả các kích thước mô hình, đặc biệt là Pythia được huấn luyện trước trên cùng dữ liệu. Các mô hình Transformer chỉ encoder

Bảng 1: Hiệu quả của Mamba Retriever trên MS MARCO và BEIR so với Transformer Retrievers. Arch biểu thị kiến trúc, R@1k là Recall@1k, En là Encoder-only, De là Decoder-only, và Ma là Mamba.

| Mô hình Cơ sở | Kích thước | Arch. | MS MARCO |  | BEIR |
|---|---|---|---|---|---|
|  |  |  | MRR@10 | R@1k | nDCG@10 |
| BERT-base | 110M | En. | 32.8 | 95.2 | 36.45 |
| RoBERTa-base | 125M | En. | 31.3 | 95.0 | 37.02 |
| OPT | 125M | De. | 31.1 | 94.9 | 36.83 |
| Pythia | 160M | De. | 25.4 | 91.0 | 31.47 |
| Mamba | 130M | Ma. | 32.3 | 96.7 | 40.54 |
| BERT-large | 330M | En. | 33.9 | 95.7 | 37.31 |
| RoBERTa-large | 355M | En. | 33.9 | 96.1 | 38.37 |
| OPT | 350M | De. | 31.1 | 94.4 | 35.89 |
| Pythia | 410M | De. | 31.9 | 96.6 | 38.62 |
| Mamba | 370M | Ma. | 35.2 | 97.7 | 43.52 |
| Pythia | 1B | De. | 33.8 | 97.4 | 43.11 |
| Mamba | 790M | Ma. | 36.3 | 98.3 | 44.72 |
| OPT | 1.3B | De. | 35.8 | 98.1 | 42.87 |

--- TRANG 4 ---
Conference acronym 'XX, June 03-05, 2018, Woodstock, NY Hanqi Zhang, Chong Chen, Lang Mei, Qi Liu, và Jiaxin Mao

Bảng 2: Hiệu quả của Mamba Retriever trên bộ dữ liệu LoCoV0 so với các retriever văn bản dài. M2 là M2-BERT.

| Mô hình Cơ sở | Độ dài Tối đa | Arch. | nDCG@10 Trung bình |
|---|---|---|---|
| M2-BERT-2k, 80M | 2k | M2. | 83.6 |
| OPT-125M | 2k | De. | 88.9 |
| Pythia-160M | 2k | De. | 79.2 |
| Mamba-130M | 2k | Ma. | 89.1 |
| Jina-v2, 137M, zero-shot | 8k | En. | 85.4 |
| M2-BERT-8k, 80M | 8k | M2. | 85.9 |
| Mamba-130M | 8k | Ma. | 90.7 |

có điểm số cao hơn các mô hình chỉ decoder, điều này chứng minh rằng tự chú ý hai chiều có lợi cho việc truy xuất. Khi so sánh với mô hình Transformer chỉ encoder hai chiều, mặc dù Mamba là một chiều, Mamba vẫn có hiệu suất tương tự hoặc tốt hơn. Điều này cho thấy rằng Mamba có khả năng hiểu văn bản và tóm tắt mạnh hơn, có thể do những lợi thế được thảo luận trong Phần 3.2, chẳng hạn như cơ chế chú ý ngầm của Mamba và khả năng tóm tắt rõ ràng. Ngoài ra, hiệu suất truy xuất của Mamba tăng theo kích thước mô hình mở rộng từ 100M đến 790M.

4.2 Thực nghiệm trên LoCoV0

Bộ dữ liệu. Các mô hình của chúng tôi được huấn luyện và đánh giá trên bộ dữ liệu LoCoV0. LoCoV0 có 5 bộ dữ liệu truy xuất văn bản dài: SummScreenFD, Gov Report, QMSUM, QASPER Title to Full Text, và QASPER Abstract to Full Text. Độ dài ký tự trung bình của các đoạn văn là {30792, 55280, 58129, 22315, 22315}. nDCG@10 trung bình là một chỉ số phổ biến. LoCoV1 được phát hành gần đây bao gồm nhiều bộ dữ liệu hơn, nhưng chưa được phát hành vào thời điểm tiến hành thực nghiệm của chúng tôi.

Chi tiết Triển khai. Chúng tôi khởi tạo mô hình từ một checkpoint Mamba được huấn luyện trước và huấn luyện trên 8 ×64G V100 GPU. Chúng tôi sử dụng độ dài tối đa 2k hoặc 8k token. Huấn luyện và đánh giá trên cùng độ dài tối đa. Chúng tôi sử dụng kích thước batch là 1 trên mỗi GPU. Bằng cách chia sẻ các đoạn văn âm tính giữa các GPU, mỗi truy vấn tổng cộng có 31 đoạn văn âm tính ngẫu nhiên. Chúng tôi huấn luyện không quá 4 epoch và sử dụng tốc độ học hiệu quả cho từng mô hình. Các thiết lập huấn luyện khác giống như Phần 4.1.

Chúng tôi sử dụng mô hình Transformer chỉ encoder, chỉ decoder và M2-BERT [23] làm baseline. Đối với chỉ encoder, chúng tôi sử dụng kết quả zero-shot của mô hình Jina Embeddings v2 [9] được tinh chỉnh không trên LoCoV0 mà trên một lượng dữ liệu lớn hơn. Vì checkpoint của BERT Jina được huấn luyện trước không được công bố, chúng tôi không thể tinh chỉnh nó trên LoCoV0. Đối với chỉ decoder, chúng tôi sử dụng OPT [29] và Pythia [2]. Đối với M2-BERT, chúng tôi sử dụng kết quả trong bài báo [23] tinh chỉnh trong 1 epoch trên LoCoV0 với orthogonal projection loss [22] thay vì InfoNCE loss.

Kết quả. Bảng 2 cho thấy hiệu quả trên LoCoV0. Tóm lại, Mamba cho thấy hiệu suất tương đương hoặc tốt hơn so với các retriever văn bản dài khác, và có thể mở rộng đến văn bản dài hơn so với việc huấn luyện trước.

Ở độ dài tối đa 2k, mặc dù khả năng bộ nhớ của Mamba cho văn bản dài bị hạn chế bởi kích thước trạng thái tiềm ẩn do thiếu cơ chế tự chú ý, nó vẫn có khả năng tương đương hoặc tốt hơn so với các mô hình Transformer chỉ decoder. Ngoài ra, mặc dù Mamba được huấn luyện trước ở độ dài tối đa 2k và M2BERT-8k được huấn luyện trước

ở độ dài tối đa 8k, Mamba được tinh chỉnh ở độ dài tối đa 8k có hiệu suất tương đương hoặc tốt hơn so với các mô hình khác.

4.3 Hiệu suất Suy luận

Chi tiết Triển khai. Chúng tôi chọn đo thời gian suy luận đoạn văn dài có tiềm năng cải thiện lớn hơn thay vì truy vấn ngắn. Chúng tôi đo thời gian suy luận của tất cả các đoạn văn tập huấn luyện LoCoV0 [23] trên một GPU A100 40G sử dụng bf16, loại trừ thời gian token hóa. Các đoạn văn được cắt ngắn đến độ dài nhất định. Chúng tôi sử dụng kích thước batch tốt nhất về throughput cho từng mô hình và từng độ dài tối đa. M2-BERT không được kiểm tra ở độ dài 512. Ngoài ra, các tác vụ ngôn ngữ sinh liên quan đến nhiều bước lặp, trong khi các tác vụ truy xuất liên quan đến một bước duy nhất, do đó không cần thiết phải tăng tốc quá trình lặp [12, 21] trong tác vụ truy xuất.

Kết quả. Hình 1 trình bày thời gian suy luận của các mô hình truy xuất văn bản dài. Tóm lại, Mamba cho thấy tốc độ nhanh hơn trên việc truy xuất văn bản dài. Ở độ dài tối đa 512, thời gian suy luận của các mô hình khác nhau là tương tự. Ở độ dài tối đa 2k, Mamba và M2-BERT có thời gian tương tự, trong khi các mô hình dựa trên Transformer cần 4 ×thời gian. Ở độ dài 8k và 32k, M2-BERT cần khoảng 1.2 ×và 1.4×thời gian suy luận so với Mamba, tương ứng.

Hình 1: Hiệu suất của Mamba Retriever so với các mô hình truy xuất văn bản dài ở các độ dài văn bản tối đa khác nhau.

5 Kết luận
Trong bài báo này, chúng tôi điều tra hiệu quả và hiệu suất của mô hình dựa trên Mamba trong tác vụ truy xuất. Kết quả thực nghiệm cho thấy rằng, trên việc truy xuất văn bản ngắn kinh điển, Mamba Retriever có hiệu quả tương tự hoặc tốt hơn so với các retriever Transformer, và hiệu quả tăng theo kích thước mô hình. Trên việc truy xuất văn bản dài, Mamba Retriever có hiệu quả tương tự hoặc tốt hơn so với các retriever văn bản dài hiện có, và có thể mở rộng để xử lý độ dài văn bản dài hơn so với việc huấn luyện trước. Ngoài ra, Mamba Retriever cho thấy lợi thế về hiệu suất trên việc truy xuất văn bản dài do việc mở rộng tuyến tính trong độ dài chuỗi.

Lời cảm ơn
Nghiên cứu này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Trung Quốc (61902209, 62377044, U2001212), và Chương trình Nhà khoa học trẻ xuất sắc Bắc Kinh (NO. BJJWZYJH012019100020098), Nền tảng Quản trị Xã hội Thông minh, Nền tảng Liên ngành Đổi mới & Quy hoạch Lớn cho Sáng kiến "Double-First Class", Đại học Nhân dân Trung Quốc.

--- TRANG 5 ---
Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval Conference acronym 'XX, June 03-05, 2018, Woodstock, NY

Tài liệu tham khảo
[1]Ameen Ali, Itamar Zimerman, và Lior Wolf. 2024. The hidden attention of mamba models. arXiv preprint arXiv:2403.01590 (2024).
[2]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al .2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning . PMLR, 2397-2430.
[3]Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, và Terry Lyons. 2024. Theoretical Foundations of Deep Selective State-Space Models. arXiv preprint arXiv:2402.19047 (2024).
[4]Zhuyun Dai và Jamie Callan. 2019. Deeper text understanding for IR with contextual neural language modeling. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval . 985-988.
[5]Tri Dao và Albert Gu. 2024. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv:2405.21060 [cs.LG]
[6]Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin Spector, Michael Poli, Atri Rudra, và Christopher Ré. 2024. Monarch mixer: A simple sub-quadratic gemm-based architecture. Advances in Neural Information Processing Systems 36 (2024).
[7]Albert Gu và Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023).
[8]Albert Gu, Karan Goel, và Christopher Ré. 2021. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021).
[9]Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al .2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. arXiv preprint arXiv:2310.19923 (2023).
[10] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics.
[11] Jacob Devlin Ming-Wei Chang Kenton và Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT . 4171-4186.
[12] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, và Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles .
[13] Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, và Yiqun Liu. 2023. LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset. arXiv preprint arXiv:2310.17609 (2023).
[14] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
[15] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, và Jimmy Lin. 2023. Fine-tuning llama for multi-stage text retrieval. arXiv preprint arXiv:2310.08319 (2023).
[16] William Merrill, Jackson Petty, và Ashish Sabharwal. 2024. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819 (2024).
[17] Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904 (2022).
[18] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, và Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. In CoCo@NIPS (CEUR Workshop Proceedings, Vol. 1773) . CEUR-WS.org.
[19] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, và Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In EMNLP . Association for Computational Linguistics, 9844-9855.
[20] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, et al .2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings of the Association for Computational Linguistics: EMNLP 2023 . 14048-14077.
[21] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, và Jeff Dean. 2023. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems 5 (2023).
[22] Kanchana Ranasinghe, Muzammal Naseer, Munawar Hayat, Salman Khan, và Fahad Shahbaz Khan. 2021. Orthogonal projection loss. In Proceedings of the IEEE/CVF international conference on computer vision . 12333-12343.
[23] Jon Saad-Falcon, Daniel Y Fu, Simran Arora, Neel Guha, và Christopher Ré. 2024. Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT. arXiv preprint arXiv:2402.07440 (2024).
[24] Anshumali Shrivastava và Ping Li. 2014. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). Advances in neural information processing systems 27 (2014).
[25] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, và Iryna Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) . https://openreview.net/forum?id=wCu6T5xFjeJ
[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al .2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
[28] Zhichao Xu. 2024. RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers. arXiv preprint arXiv:2403.18276 (2024).
[29] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al .2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).
[30] Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, và Sujian Li. 2024. LongEmbed: Extending Embedding Models for Long Context Retrieval. arXiv preprint arXiv:2404.12096 (2024).
