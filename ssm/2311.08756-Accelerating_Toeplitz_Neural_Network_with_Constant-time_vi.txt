# 2311.08756.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2311.08756.pdf
# Kích thước tệp: 709412 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tăng tốc Mạng Nơ-ron Toeplitz với Độ phức tạp Suy luận Thời gian Hằng số
Zhen Qin, Yiran Zhong
OpenNLPLab, Phòng thí nghiệm Trí tuệ Nhân tạo Thượng Hải
https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion
Tóm tắt
Mạng Nơ-ron Toeplitz (TNNs) đã thể hiện hiệu suất xuất sắc trong các tác vụ mô hình hóa chuỗi khác nhau. Chúng vượt trội so với các mô hình dựa trên Transformer thường được sử dụng trong khi hưởng lợi từ độ phức tạp không-thời gian log-tuyến tính. Mặt khác, Mô hình Không gian Trạng thái (SSMs) đạt hiệu suất thấp hơn TNNs trong mô hình hóa ngôn ngữ nhưng mang lại lợi thế về độ phức tạp suy luận hằng số. Trong bài báo này, chúng tôi nhằm kết hợp điểm mạnh của TNNs và SSMs bằng cách chuyển đổi TNNs thành SSMs trong quá trình suy luận, từ đó cho phép TNNs đạt được độ phức tạp suy luận hằng số giống như SSMs. Để thực hiện điều này, chúng tôi hình thức hóa quá trình chuyển đổi như một bài toán tối ưu và cung cấp nghiệm dạng đóng. Chúng tôi chứng minh cách biến đổi phương trình mục tiêu thành bài toán hệ thống tuyến tính Vandermonde, có thể được giải hiệu quả bằng Biến đổi Fourier Rời rạc (DFT). Đáng chú ý, phương pháp của chúng tôi không yêu cầu huấn luyện và duy trì tính ổn định số học. Nó cũng có thể được áp dụng cho bất kỳ mô hình dựa trên LongConv nào. Để đánh giá hiệu quả của nó, chúng tôi tiến hành các thí nghiệm mở rộng trên các tác vụ mô hình hóa ngôn ngữ trong các thiết lập khác nhau. Ngoài ra, chúng tôi so sánh phương pháp của mình với các giải pháp gradient-descent khác, làm nổi bật tính ổn định số học vượt trội của cách tiếp cận của chúng tôi. Mã nguồn có sẵn tại https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.

1 Giới thiệu
Transformer đã thống trị các lĩnh vực thị giác máy tính (CV) (Dosovitskiy et al., 2020; Liu et al., 2021; Sun et al., 2022b), xử lý ngôn ngữ tự nhiên (NLP) (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020; Liu et al., 2022; Qin et al., 2023d), và xử lý giọng nói (Karita et al., 2019; Zhang et al., 2020; Gulati et al., 2020; Sun et al., 2022a), trở thành một trong những cách tiếp cận có hiệu suất tốt nhất trên các benchmark khác nhau. Thành phần cốt lõi của Transformer, cơ chế attention, có độ phức tạp thời gian bậc hai đối với độ dài chuỗi, khiến việc mở rộng quy mô cho các chuỗi dài và kích thước mô hình lớn trở nên thách thức. Nhiều phương pháp khác nhau đã được đề xuất để giải quyết vấn đề này, bao gồm Linear Attention (Katharopoulos et al., 2020; Choromanski et al., 2020; Qin et al., 2022b, 2023b), Mô hình Không gian Trạng thái (SSM) (Gu et al., 2022; Gupta et al., 2022), Mạng Nơ-ron Toeplitz (TNN) (Qin et al., 2023a) và các phương pháp LongConv khác (Li et al., 2023).

Linear Attention giảm độ phức tạp không-thời gian của attention xuống tuyến tính bằng cách sử dụng thủ thuật kernel để phân tách hàm Softmax (Choromanski et al., 2020; Qin et al., 2023c), nhưng hiệu suất kém của nó (Qin et al., 2022a) ngăn cản việc sử dụng để xây dựng Mô hình Ngôn ngữ Lớn (LLMs).

SSM thay thế thao tác attention bằng phương trình không gian trạng thái, dẫn đến độ phức tạp không-thời gian huấn luyện log-tuyến tính (Gu et al., 2022). Tuy nhiên, hiệu suất của phương pháp này trong mô hình hóa ngôn ngữ nhân quả thường kém hơn (Qin et al., 2023a) và nhạy cảm với khởi tạo (Gu et al., 2022), khiến nó không phù hợp để xây dựng LLMs.

TNN là một lớp phương pháp mô hình hóa chuỗi mới thuộc về các phương pháp dựa trên LongConv (Li et al., 2023; Qin et al., 2023a). Nó mô hình hóa các chuỗi dài bằng cách sử dụng ma trận Toeplitz để mã hóa các mối quan hệ vị trí tương đối. Thành phần chính này cho phép chúng nắm bắt hiệu quả các phụ thuộc trong chuỗi và đưa ra dự đoán chính xác. Nó có độ phức tạp không-thời gian log-tuyến tính và vượt trội hơn Transformers trong các tác vụ NLP và mô hình hóa chuỗi dài (Qin et al., 2023a). Ngoài ra, khả năng huấn luyện ổn định và tính không nhạy cảm với khởi tạo làm cho nó khả thi cho LLMs.

Lưu ý rằng phân tích trên chỉ tính đến độ phức tạp huấn luyện cho các phương pháp nêu trên. Tuy nhiên, khi xem xét việc triển khai LLMs, độ phức tạp suy luận cũng quan trọng. Trong các tình huống decoder, tức là mô hình hóa ngôn ngữ nhân quả, độ phức tạp thời gian suy luận token thứ n trong Transformer là O(n²d+nd²), trong đó n, d lần lượt là độ dài chuỗi và chiều đặc trưng. Bằng cách sử dụng kỹ thuật KV cache (Pope et al., 2022), độ phức tạp có thể được giảm xuống O(nd²). Đối với Linear Attention, độ phức tạp là O(dh) (h là chiều ẩn), làm cho nó không đổi đối với độ dài chuỗi (Katharopoulos et al., 2020). SSM cũng có độ phức tạp không-thời gian hằng số O(dh), trong đó h là chiều không gian ẩn (Gu et al., 2022). TNN, mặt khác, có độ phức tạp không-thời gian log-tuyến tính O(ndlogn) trong suy luận, điều này có thể làm cho việc xử lý các chuỗi dài trở nên thách thức.

Trong bài báo này, chúng tôi nhằm tăng tốc suy luận của TNN đến độ phức tạp thời gian hằng số. Chúng tôi thấy rằng SSM có thể được coi như một biến thể đặc biệt của TNN. TNN có thể hưởng lợi từ độ phức tạp suy luận giống như SSM nếu chúng ta có thể chuyển đổi nó thành SSM trong suy luận. Chúng tôi chỉ ra rằng việc chuyển đổi như vậy có thể được xem như một bài toán tối ưu và có thể được giải hiệu quả bằng nghiệm dạng đóng. Cụ thể, cho một ma trận Toeplitz, trước tiên chúng tôi chuyển đổi nó thành Hệ thống Tuyến tính Vandermonde với Tái hình thức hóa Phương trình Bao hàm (IER) và sau đó sử dụng Biến đổi Fourier Rời rạc (DFT) để có được kết quả ổn định về số học. So với các thuật toán dựa trên gradient, phương pháp của chúng tôi nhanh, không cần huấn luyện và ổn định về số học. Lưu ý rằng phương pháp của chúng tôi cũng có thể được áp dụng cho các phương pháp dựa trên LongConv khác (Li et al., 2023).

Chúng tôi tiến hành các thí nghiệm mở rộng để xác thực hiệu quả của phương pháp. Chúng tôi so sánh phương pháp của mình với các phương pháp dựa trên gradient về hiệu quả và lỗi. Phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp dựa trên gradient về hiệu quả trong khi có tỷ lệ lỗi thấp hơn nhiều. Chúng tôi cũng áp dụng phương pháp của mình cho các mô hình ngôn ngữ TNN và kiểm tra nó trong các tình huống thực tế. Phương pháp của chúng tôi có khả năng ngoại suy tương đương và perplexity như triển khai gốc của TNN. Đối với số lượng lớp, độ dài chuỗi và chiều đặc trưng, một đánh giá sâu về tốc độ và sử dụng bộ nhớ được thực hiện. Phương pháp của chúng tôi rõ ràng vượt trội hơn triển khai thuật toán suy luận TNN gốc. Hơn nữa, chúng tôi chứng minh khả năng áp dụng của chiến lược của chúng tôi ngoài TNN bằng cách mở rộng nó cho các mô hình dựa trên LongConv khác.

2 Nền tảng và Cơ sở
Trong phần này, trước tiên chúng tôi định nghĩa suy luận mô hình chuỗi một cách toán học và sau đó thảo luận ngắn gọn về độ phức tạp suy luận của Transformer và một số phương pháp mô hình hóa chuỗi hiệu quả liên quan chặt chẽ như Linear Transformer (Katharopoulos et al., 2020), SSM (Gu et al., 2022), và TNN (Qin et al., 2023a).

2.1 Suy luận
Suy luận đề cập đến quá trình dự đoán token tiếp theo cho mô hình ngôn ngữ F và chuỗi token x trong R^n. Nó có thể được biểu diễn như sau:
logits = F(x) trong R^(n×V)
x_(n+1) = Sample(logits[-1]), (1)
trong đó V biểu diễn kích thước từ vựng, logits biểu diễn logits đầu ra từ mô hình ngôn ngữ, và x_(n+1) là token được lấy mẫu. Quá trình suy luận tiếp tục cho đến khi x_(n+1) là token kết thúc chuỗi (eos), cho thấy việc hoàn thành suy luận. Độ phức tạp thời gian và không gian của suy luận được xác định bởi mô hình ngôn ngữ cơ bản F.

2.2 Độ phức tạp Suy luận
Transformer Thành phần cốt lõi của Transformer là self-attention, hoạt động trên các queries Q, keys K, và values V. Mỗi thành phần là một ánh xạ tuyến tính của đầu vào X trong R^(n×d), được cho bởi:
Q = XW^Q, K = XW^K, V = XW^V trong R^(n×d). (2)
Đầu ra của attention được tính như sau:
O = Softmax(QK^T/√d)V. (3)
Do cần tính QK^T, độ phức tạp thời gian của Transformer là O(n²d+nd²). Trong giai đoạn suy luận, khi dự đoán token thứ n, độ phức tạp thời gian ngây thơ là O(n²d+nd²), với độ phức tạp không gian O(nd). Bằng cách lưu trữ K và V của các bước thời gian trước đó, được gọi là KV cache, độ phức tạp có thể được giảm xuống O(nd²).

Linear Transformer Thành phần cốt lõi của Linear Transformer là Linear Attention, sử dụng ánh xạ φ(·) để ánh xạ Query và Key đến các biểu diễn ẩn của chúng, trong đó

--- TRANG 3 ---
φ(Q), φ(K) trong R^(n×h) và h là chiều ẩn. Đầu ra sau đó được cho bởi:
O = Δ^(-1)φ(Q)[φ(K)^T V],
Δ = diag(φ(Q))[φ(K)^T 1_n]. (4)
Bằng cách tính φ(K)^T V trước, độ phức tạp tính toán có thể được giảm xuống O(ndh). Trong giai đoạn suy luận, theo (Katharopoulos et al., 2020), chúng ta có thể biến đổi Linear Attention thành dạng RNN:
a_0 = 0, b_0 = 0,
a_n = a_(n-1) + φ(k_n)v_n^T,
b_n = b_(n-1) + φ(k_n),
o_n = φ(q_n)^T a_n / φ(q_n)^T b_n. (5)
Điều này dẫn đến độ phức tạp thời gian và không gian O(hd) cho Linear Transformer.

Mô hình Không gian Trạng thái Mô hình Không gian Trạng thái (SSM) (Gu et al., 2022) là sử dụng phương trình không gian trạng thái để mô hình hóa chuỗi:
u_n = Au_(n-1) + Bx_n, y_n = Cu_n (6)
trong đó:
A trong R^(h×h), B trong R^(h×1), C trong R^(1×h),
x_n, y_n trong R, u_n trong R^(h×1). (7)
Ở đây, h biểu diễn chiều ẩn của mô hình không gian trạng thái. Lưu ý rằng chúng tôi đã hoán đổi vị trí của x_i và u_i so với (Gu et al., 2022) để nhất quán ký hiệu. Bằng cách mở rộng Phương trình 6, chúng ta có thể viết SSM như:
y_i = Σ(j=0 to i) CA^(i-j)Bx_j, i = 0, ..., n-1. (8)
Điều này cho phép huấn luyện song song và có độ phức tạp O(ndlogn). SSM đã chứng minh hiệu quả của nó trong nhiều tác vụ mô hình hóa chuỗi dài (Gu et al., 2022).

Như một biến thể của SSM, DSS (Gupta et al., 2022) đề xuất rằng giả định A là ma trận đường chéo Λ có thể giảm thiểu tính nhạy cảm khởi tạo (Gu et al., 2022) trong khi duy trì hiệu suất mô hình tương đương. Trong trường hợp này, phương trình có thể được đơn giản hóa như sau:
CΛ^i B = Σ(k=0 to h-1) c_k b_k lambda_k^i. (9)
Trong giai đoạn suy luận, do Phương trình 6, độ phức tạp tính toán là O(hd).

Mạng Nơ-ron Toeplitz và phương pháp dựa trên LongConv Mạng Nơ-ron Toeplitz (TNN) giới thiệu token mixing (Yu et al., 2021) sử dụng ma trận vị trí tương đối hoặc ma trận Toeplitz. Tính toán cốt lõi có thể được biểu diễn như sau:
y = Tx, x, y trong R^n. (10)
trong đó:
T = [
t_0 t_(-1) ... t_(-n+1)
t_1 t_0 ...
... t_0 t_(-1)
t_(n-1) ... t_1 t_0
] trong R^(n×n). (11)
Sử dụng Biến đổi Fourier Nhanh (FFT), phép nhân ma trận trên có thể được tính trong độ phức tạp thời gian O(ndlogn), điều này làm cho độ phức tạp thời gian của TNN là O(ndlogn). Trong giai đoạn suy luận, theo Phương trình 10, độ phức tạp để dự đoán token thứ n là O(ndlogn). Vì TNN có thể được xem như một dạng của các phương pháp dựa trên LongConv (Li et al., 2023), các phương pháp dựa trên LongConv khác có cùng độ phức tạp.

3 Phương pháp
Suy luận của TNN thể hiện độ phức tạp thời gian O(ndlogn) và độ phức tạp không gian O(nd) để dự đoán token thứ n, điều này tạo ra thách thức cho việc mở rộng quy mô TNN để xử lý các chuỗi cực dài trong suy luận. Trong phần này, chúng tôi sẽ trình bày cách tiếp cận của mình để chuyển đổi TNN thành dạng SSM, nhằm cải thiện tốc độ tạo và bộ nhớ thành hằng số.

3.1 Hình thức hóa vấn đề
Trong phần này, chúng tôi chỉ ra mối liên hệ giữa TNN và SSM và hình thức hóa vấn đề của chúng tôi một cách toán học. Xem xét tình huống mô hình hóa ngôn ngữ, quá trình token mixing có thể được viết như:
y_i = Σ(j=0 to i) t_(i-j)x_j, i = 0, ..., n-1. (12)
Mặt khác, SSM có thể được biểu diễn như:
y_i = Σ(j=0 to i) CA^(i-j)Bx_j, i = 0, ..., n-1. (13)
Đặt t̄_i = CA^i B, phương trình có thể được viết lại như:
y_i = Σ(j=0 to i) t̄_(i-j)x_j, i = 0, ..., n-1. (14)

--- TRANG 4 ---
[Hình 1 mô tả quá trình chuyển đổi giữa biểu diễn Toeplitz và biểu diễn SSM]

Hình 1: Chuyển đổi giữa biểu diễn Toeplitz và biểu diễn SSM. Mở rộng đệ quy có thể biến đổi biểu diễn SSM thành biểu diễn Toeplitz. Để có chuyển đổi ngược, chúng tôi sử dụng Tái hình thức hóa Phương trình Bao hàm để biểu diễn vấn đề như Hệ thống Tuyến tính Vandermonde. Sau đó, chúng tôi áp dụng Biến đổi Fourier Rời rạc (DFT) để tính biểu diễn SSM.

Vì DSS hiệu quả như SSM (Gupta et al., 2022), nhưng DSS có dạng đơn giản hơn, chúng tôi chọn DSS làm cấu trúc đơn giản hóa mong muốn. Trong trường hợp này, chúng ta có:
t̄_i = CA^i B = Σ(k=0 to h-1) c_k b_k lambda_k^i. (15)
Đáng chú ý, c_i b_i có thể được kết hợp, vì vậy không mất tính tổng quát, chúng ta giả định C = 1_h:
t̄_i = CA^i B = Σ(k=0 to h-1) b_k lambda_k^i. (16)

Bằng cách so sánh các phương trình, rõ ràng SSM là trường hợp đặc biệt của TNN. Lưu ý rằng suy luận TNN gặp phải các nút thắt cổ chai hiệu suất trong khi SSM thì không, câu hỏi tự nhiên nảy sinh: liệu chúng ta có thể "chuyển đổi" TNN thành SSM trong suy luận không? Câu hỏi này tương đương với việc tìm các ma trận Λ và B sao cho:
t_i = Σ(k=0 to h-1) lambda_k^i b_k, i = 0, ..., n-1. (17)

Bằng cách xác định các giá trị phù hợp cho Λ và B, chúng ta có thể đạt được biểu diễn tương đương giữa TNN và SSM.

3.2 Phương pháp dựa trên gradient
Một giải pháp để giải Phương trình 17 là sử dụng các phương pháp dựa trên gradient để giải bài toán tối ưu sau:
min_(b_k, lambda_k) Σ(i=0 to n-1) L(t_i, Σ(k=0 to h-1) lambda_k^i b_k), (18)
trong đó L là hàm mất mát, có thể là ℓ1 hoặc ℓ2.

Tuy nhiên, cách tiếp cận này có hai vấn đề:
• Nó không thể thỏa mãn chính xác Phương trình 17, dẫn đến mất thông tin trong quá trình chuyển đổi.
• Sự hiện diện của các số hạng mũ lambda_k^i làm cho việc tối ưu khó hội tụ. (Gu et al., 2022)

Các vấn đề trên làm cho phương pháp dựa trên gradient kém hiệu quả trong việc đạt được chuyển đổi chính xác và hiệu quả từ TNN sang SSM. Chúng tôi áp dụng thuật toán này làm phương pháp cơ sở và trình bày nó trong Hình 2. Thuật toán được tóm tắt trong Thuật toán 2.

3.3 Nghiệm dạng đóng của chúng tôi
Trong phần này, chúng tôi chỉ ra rằng Phương trình 17 có thể được giải trực tiếp với nghiệm dạng đóng, tức là tìm các giá trị chính xác của lambda_k và b_k dẫn đến biểu diễn ma trận Toeplitz mong muốn. Với nghiệm dạng đóng, chúng ta có thể tránh các vấn đề liên quan đến cách tiếp cận dựa trên gradient và đạt được chuyển đổi chính xác hơn từ TNN sang SSM.

Để làm điều này, trước tiên chúng tôi thêm một biến b̄ = 0 vào cả hai vế của phương trình, tạo ra:
t_i = t_i + b̄ = b̄ + Σ(k=0 to h-1) lambda_k^i b_k, i = 0, ..., n-1. (19)

Mở rộng phương trình này thành dạng ma trận, chúng ta có:
[t_0; t_1; ...; t_(n-1)] = [1 1 ... 1; 1 lambda_0 ... lambda_(h-1); 1 ...; 1 lambda_0^(n-1) ... lambda_(h-1)^(n-1)] [b̄; b_0; b_1; ...; b_(h-1)],
t = Vb,
t trong R^n, V trong R^(n×(h+1)), b trong R^(h+1). (20)

Bây giờ, hãy đặt h = n-1, chúng ta có:
[t_0; t_1; ...; t_(n-1)] = [1 1 ... 1; 1 lambda_0 ... lambda_(n-2); 1 ...; 1 lambda_0^(n-1) ... lambda_(n-2)^(n-1)] [b̄; b_0; b_1; ...; b_(n-2)],
t = Vb,
t trong R^n, V trong R^(n×n), b trong R^n. (21)

Tại thời điểm này, V là ma trận Vandermonde. Hệ thống tuyến tính Vandermonde nói chung không ổn định vì các vấn đề về độ chính xác số học (Gautschi, 2020); tuy nhiên, phương trình có

--- TRANG 5 ---
Thuật toán 1 ETSC: Chuyển đổi Chính xác Toeplitz-sang-SSM
Đầu vào: t trong R^n.
Đầu ra: lambda trong C^n, b trong C^n.
Ký hiệu: Sử dụng W_k để biểu diễn ma trận DFT bậc k.
Khởi tạo:
t̄ = concat([t, -Σ(i=0 to n-1) t_i]) trong R^(n+1),
lambda_s = exp(-2πi(s+1)/(n+1)), s = 0, ..., n-1,
t_dft = W_(n+1) t̄ trong R^(n+1),
b = 0_n trong R^n.
for i in 0, ..., n-1 do:
    b_i = t_dft[i+1]/√(n+1);
end for

nghiệm nếu các lambda_k khác nhau từng đôi một. Để cải thiện tính ổn định, chúng ta có thể chọn lambda_s = exp(-2πis/n), điều này dẫn đến V = √nW_n, trong đó W_n là ma trận Biến đổi Fourier Rời rạc (DFT). Phương trình trên có thể được biểu diễn như:
t = √nWb, W^H t = √nb, (22)
trong đó W^H biểu diễn chuyển vị liên hợp của ma trận W. Bằng cách so sánh hàng đầu tiên, chúng ta có:
Σ(i=0 to n-1) t_i = 0. (23)

Tuy nhiên, các hệ số t_i từ TNN không được đảm bảo thỏa mãn phương trình này. Để đảm bảo rằng phương trình này được thỏa mãn, chúng tôi giới thiệu biến khác t_n = -Σ(i=0 to n-1) t_i, mà chúng tôi gọi là quy trình tái hình thức hóa phương trình bao hàm. Do đó, chúng ta có:
[t_0; t_1; ...; t_(n-1); t_n] = [1 1 ... 1; 1 lambda_0 ... lambda_(n-1); 1 ...; 1 lambda_0^n ... lambda_(n-1)^n] [b̄; b_0; b_1; ...; b_(n-2)],
t̄ = √(n+1)W_(n+1)b,
t̄ trong R^(n+1), V trong R^((n+1)×(n+1)), b trong R^(n+1). (24)

Dựa trên phương trình trên, chúng ta có thể xác định các hệ số b_i bằng biểu thức:
b_i = (1/√(n+1))[W_(n+1)^⊤ t̄]_i. (25)

Bằng cách sử dụng công thức này, chúng ta có thể tính được các hệ số b_i. Chúng tôi đặt tên phương pháp này là ETSC (Chuyển đổi Chính xác Toeplitz-sang-SSM) và cung cấp tóm tắt thuật toán trong Thuật toán 1.

Thuật toán 2 Phương pháp Dựa trên Gradient
Đầu vào: t trong R^n;
Đầu ra: lambda trong C^n, b trong C^n;
Khởi tạo:
r, theta, b_real, b_img ∼ N(0, I_n).
Tối thiểu hóa:
Σ_i |t_i - Σ(k=0 to h-1) lambda_k^i b_k|^2,
trong đó
lambda = Sigmoid(r) exp(i*theta),
b = b_real + i*b_img.

3.4 Suy luận của TNN
Trong phần này, chúng tôi giới thiệu ngắn gọn ba chiến lược suy luận mô hình hóa ngôn ngữ cho TNN: triển khai Gốc, tức là FFT, Cache, và SSM. Trong thảo luận tiếp theo, hãy giả định chúng ta có TNN L lớp với chỉ số trên (l) chỉ kết quả tại lớp thứ l. Tính toán của TNN có thể được biểu diễn như sau:
x^0 = Embedding(i) trong R^(n×d),
x^(l+1) = T^l x^l trong R^(n×d), l = 0, ..., L-1
Logits = x^L W trong R^(n×V) (26)
Ở đây, i trong R^n biểu diễn các token đầu vào và V biểu diễn kích thước từ vựng.

Gốc Trong giai đoạn suy luận, thao tác cốt lõi của chúng ta vẫn là tính toán T_i x_i. Một cách tiếp cận cho suy luận là tiếp tục sử dụng Biến đổi Fourier Nhanh (FFT), điều này dẫn đến độ phức tạp thời gian O(ndlogn).

Cache Phương pháp này là tính trực tiếp Phương trình 12, yêu cầu phép nhân ma trận và có độ phức tạp thời gian O(n²d+nd²). Tuy nhiên, bằng cách sử dụng cơ chế lưu trữ tương tự như key-value (KV) cache trong transformer (Pope et al., 2022), chúng ta có thể lưu trữ đầu ra của mỗi lớp như cache^l = x^(l+1) trong R^(n×d). Theo cách này, khi thực hiện suy luận mới, chúng ta chỉ cần tính:
x^(l+1)_n = Σ(k=0 to n) t^(l+1)_(n-k) x^l_k. (27)

Sau đó, chúng ta cập nhật như sau:
cache^l = concat([cache^l, x^(l+1)_n]),
x^(l+1) = cache^l. (28)

--- TRANG 6 ---
Bảng 1: Đánh giá Ngoại suy trên TNN. Chúng tôi huấn luyện một TNN LM và, sau khi hoàn thành huấn luyện, sử dụng ETSC để chuyển đổi các hệ số của ma trận Toeplitz thành biểu diễn SSM. Sau đó chúng tôi đánh giá khả năng ngoại suy của mô hình, so sánh kết quả cho các trạng thái ẩn khác nhau. Có thể quan sát rằng mô hình của chúng tôi thể hiện khả năng ngoại suy tương tự như TNN. Hơn nữa, đối với các trạng thái ẩn 768 và 1024, ETSC đạt được perplexity trung bình (ppl) tương đương với TNN.
[Bảng dữ liệu với các giá trị perplexity cho các độ dài chuỗi khác nhau]

Bảng 2: Đánh giá ETSC trên Các Phương pháp LongConv Khác. Chúng tôi tiến hành thí nghiệm để đánh giá hiệu suất của ETSC trên các phương pháp LongConv khác, tập trung cụ thể vào SGConv. Chúng tôi huấn luyện một SGConv LM và áp dụng ETSC để chuyển đổi biểu diễn Toeplitz thành biểu diễn SSM. Sau đó chúng tôi đánh giá khả năng ngoại suy của mô hình đã chuyển đổi. Điều này chứng minh rằng ETSC thể hiện khả năng ngoại suy tương tự như SGConv, thậm chí với các giá trị perplexity trung bình (ppl) thấp hơn.
[Bảng dữ liệu với các giá trị perplexity cho SGConv và phương pháp của tác giả]

Với cách tiếp cận này, độ phức tạp thời gian có thể được giảm xuống O(nd²).

SSM Với phương pháp của chúng tôi, chúng ta có thể biến đổi biểu diễn Toeplitz thành biểu diễn Mô hình Không gian Trạng thái (SSM). Do đó, chúng ta có thể thực hiện suy luận bằng Phương trình 6, dẫn đến cả độ phức tạp thời gian và không gian O(hd).

4 Thí nghiệm
Trong phần này, chúng tôi trình bày các thí nghiệm mở rộng để xác thực phương pháp của mình. Trước tiên chúng tôi phân tích tính ổn định số học và hiệu quả của phương pháp với so sánh với cách tiếp cận dựa trên gradient. Sau đó chúng tôi đánh giá phương pháp của mình cho các tác vụ mô hình hóa ngôn ngữ với các tình huống thực tế. Trong nghiên cứu hiệu quả suy luận, chúng tôi tiến hành phân tích sâu về tác động của số lượng lớp, độ dài chuỗi và chiều đặc trưng đến tốc độ và sử dụng bộ nhớ của phương pháp. Chúng tôi cũng mở rộng phạm vi của phương pháp cho các phương pháp dựa trên long convolution khác, thể hiện tính linh hoạt và khả năng tổng quát hóa của nó.

4.1 Tính ổn định Số học và Hiệu quả
Hình 2 trình bày so sánh về độ phức tạp thời gian và sai số tương đối ∥t-t_pred∥/∥t∥, trong đó t = [t_0, ..., t_(n-1)] biểu diễn các hệ số của ma trận Toeplitz. Trước tiên chúng tôi cố định chiều đặc trưng là 64 và thay đổi độ dài chuỗi từ 64 đến 8192. Phương pháp của chúng tôi nhanh hơn 3 đến 6 bậc độ lớn so với phương pháp dựa trên gradient. Về sai số tương đối, phương pháp của chúng tôi đạt được sai số gần bằng không, trong khi sai số tương đối của các phương pháp dựa trên gradient vượt quá 30%.

Sau đó chúng tôi cố định độ dài chuỗi là 2048 và thay đổi chiều đặc trưng từ 64 đến 16384. Các phương pháp dựa trên gradient gặp OOM tại d = 512 trong khi phương pháp của chúng tôi hoàn thành thành công tất cả các thử nghiệm. Phương pháp của chúng tôi nhanh hơn 4 bậc độ lớn. Về sai số tương đối, phương pháp của chúng tôi đạt được sai số gần bằng không, trong khi sai số tương đối của các phương pháp dựa trên gradient khoảng 35%.

Phương pháp của chúng tôi chứng minh tính ổn định số học và hiệu quả vượt trội so với các phương pháp dựa trên gradient. Nó giảm đáng kể thời gian tính toán trong khi duy trì sai số tương đối thấp. Hơn nữa, phương pháp của chúng tôi thể hiện khả năng mở rộng xuất sắc, vì nó có thể xử lý các độ dài chuỗi lớn hơn và chiều đặc trưng cao hơn mà không gặp OOM.

4.2 Đánh giá trên TNN LM
Theo cấu hình được sử dụng trong (Qin et al., 2023a), chúng tôi huấn luyện TNN LM 6 lớp trên bộ dữ liệu Wikitext-103 và Wiki-book (Wettig et al., 2023) với chiều đặc trưng 512, độ dài chuỗi tối đa 512, và 50k bước cập nhật. Sau khi huấn luyện, chúng tôi sử dụng ETSC để chuyển đổi các hệ số

--- TRANG 7 ---
[Các biểu đồ so sánh thời gian và sai số tương đối giữa ETSC và phương pháp dựa trên gradient]

Hình 2: So sánh ETSC và Các Phương pháp Dựa trên Gradient. Chúng tôi so sánh chi phí thời gian và sai số tương đối ∥t-t_pred∥/∥t∥ của ETSC và các phương pháp dựa trên gradient, trong đó đơn vị chi phí thời gian là giây và đơn vị sai số tương đối là phần trăm. Ở đây, t = [t_0, ..., t_(n-1)] biểu diễn các hệ số của ma trận Toeplitz. Có thể quan sát rằng ETSC thể hiện chi phí thời gian thấp hơn đáng kể so với các phương pháp dựa trên gradient, đồng thời cũng đạt được sai số nhỏ hơn.

của ma trận Toeplitz thành SSM và thay đổi độ dài chuỗi từ 512 đến 14336 để xác minh khả năng ngoại suy của mô hình. Chúng tôi kiểm tra với ba chiều trạng thái ẩn: 512, 768, và 1024.

Bảng 1 cho thấy kết quả đánh giá của chúng tôi. Có thể quan sát rằng ETSC thể hiện khả năng ngoại suy giống như TNN, cho phép nó xử lý các chuỗi với độ dài tùy ý. Hơn nữa, khi các chiều trạng thái ẩn lớn hơn 512, ETSC đạt được perplexity trung bình tương đương với TNN, chứng minh ETSC bảo tồn khả năng mô hình hóa của TNN trong khi cung cấp lợi ích về tính ổn định số học và hiệu quả.

Đánh giá của chúng tôi trên TNN LM chứng minh rằng ETSC không chỉ sở hữu khả năng ngoại suy mà còn đạt được hiệu suất tương đương với TNN về perplexity trung bình. Điều này càng khẳng định hiệu quả và tính thực tiễn của ETSC trong các tác vụ mô hình hóa chuỗi dài.

4.3 Phân tích Hiệu quả Suy luận
Trong phần này, chúng tôi thảo luận về tác động của các siêu tham số đến thời gian suy luận và sử dụng bộ nhớ. Chúng tôi so sánh ETSC với các phương pháp Gốc (FFT) và Cache về thời gian suy luận thực tế và sử dụng bộ nhớ. Tất cả các phương pháp được đánh giá trên cùng một GPU A100. Cụ thể, chúng tôi chọn một TNN LM và thay đổi độ dài chuỗi, chiều đặc trưng và số lượng lớp để đánh giá hiệu quả của các phương pháp.

Trong thử nghiệm độ dài chuỗi, chúng tôi cố định số lượng lớp là 2 và chiều đặc trưng là 64. Trong thử nghiệm chiều đặc trưng, chúng tôi cố định số lượng lớp là 2 và độ dài chuỗi là 2048. Trong thử nghiệm lớp, chúng tôi cố định độ dài chuỗi là 2048 và chiều đặc trưng là 64. Hình 3 (a) và (b) minh họa kết quả của thử nghiệm độ dài chuỗi. Có thể quan sát rằng các phương pháp Gốc và Cache thể hiện thời gian suy luận và sử dụng bộ nhớ cao hơn đáng kể, từ vài lần đến chục lần so với ETSC. Ngoài ra, sử dụng bộ nhớ của Gốc và Cache cao hơn gần 2 bậc độ lớn khi độ dài chuỗi vượt quá 1k. Trong thử nghiệm chiều đặc trưng, như thể hiện trong Hình 3 (c) (d), cả phương pháp Gốc và Cache đều thể hiện thời gian suy luận từ vài lần đến chục lần dài hơn ETSC, với sử dụng bộ nhớ cao hơn khoảng 100 lần. Kết quả thử nghiệm lớp

--- TRANG 8 ---
[Các biểu đồ so sánh chi phí thời gian và bộ nhớ tương đối giữa các phương pháp]

Hình 3: Tác động của Siêu tham số đến Thời gian Suy luận và Bộ nhớ. Chúng tôi so sánh thời gian suy luận thực tế và sử dụng bộ nhớ của ETSC, Gốc (FFT), và các phương pháp Cache dưới các độ dài chuỗi, chiều đặc trưng và độ sâu mô hình khác nhau. Phương pháp của chúng tôi luôn vượt trội hơn các phương pháp khác, giảm đáng kể cả thời gian suy luận và sử dụng bộ nhớ trong tất cả các tình huống.

được thể hiện trong Hình 3 (e) (f). Các phương pháp Gốc và Cache một lần nữa thể hiện thời gian suy luận từ vài lần đến chục lần dài hơn ETSC, với sử dụng bộ nhớ cao hơn khoảng 100 lần hoặc hơn.

Những kết quả này chứng minh hiệu quả vượt trội của ETSC so với các phương pháp Gốc và Cache trên các cấu hình khác nhau. ETSC luôn vượt trội hơn các phương pháp khác về cả thời gian suy luận và sử dụng bộ nhớ. Điều này làm nổi bật lợi thế của ETSC cho suy luận hiệu quả và có thể mở rộng trong mô hình hóa chuỗi dài.

4.4 Ứng dụng cho Các Phương pháp Dựa trên LongConv Khác
Phương pháp của chúng tôi áp dụng cho tất cả các phương pháp LongConv, vì chúng đều dựa trên ma trận Toeplitz. Để xác thực tuyên bố này, chúng tôi chọn SGConv (Li et al., 2023) và huấn luyện một mô hình ngôn ngữ SGConv. Sau khi huấn luyện, chúng tôi sử dụng ETSC để chuyển đổi biểu diễn Toeplitz thành biểu diễn SSM. Sau đó chúng tôi thay đổi độ dài chuỗi trong khoảng từ 512 đến 14336 để đánh giá khả năng ngoại suy của mô hình.

Từ Bảng 2, có thể quan sát rằng ETSC thể hiện khả năng ngoại suy giống như SGConv và đạt được perplexity trung bình thấp hơn. Điều này cho thấy rằng phương pháp của chúng tôi cũng có thể được áp dụng hiệu quả cho các phương pháp LongConv khác, càng chứng minh tính linh hoạt và hiệu quả của nó trong các tác vụ mô hình hóa chuỗi dài.

5 Kết luận
Trong bài báo này, chúng tôi đã phân tích và giải quyết vấn đề hiệu quả trong suy luận TNN. Chúng tôi đề xuất một giải pháp bằng cách chuyển đổi biểu diễn Toeplitz thành biểu diễn SSM, điều này giảm độ phức tạp thời gian và không gian của suy luận TNN để không phụ thuộc vào độ dài chuỗi. Thuật toán chuyển đổi của chúng tôi, được đặt tên là ETSC, nhanh, không cần huấn luyện và ổn định về số học, vượt trội hơn các phương pháp dựa trên gradient khác một cách đáng kể trong khi giữ nguyên khả năng ngoại suy và perplexity như TNN gốc. Chúng tôi tiến hành đánh giá toàn diện về hiệu suất của phương pháp về số lượng lớp, độ dài chuỗi và chiều đặc trưng. Kết quả của chúng tôi rõ ràng chứng minh rằng phương pháp của chúng tôi vượt trội hơn TNN gốc về cả tốc độ và sử dụng bộ nhớ. Ngoài ra, chúng tôi mở rộng khả năng áp dụng của chiến lược của mình ngoài TNN bằng cách áp dụng thành công cho các mô hình dựa trên LongConv khác, thể hiện tính linh hoạt và hiệu quả của cách tiếp cận của chúng tôi.

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Chương trình R&D Trọng điểm Quốc gia của Trung Quốc (SỐ.2022ZD0160100).

Hạn chế
Mặc dù phương pháp đề xuất của chúng tôi để chuyển đổi biểu diễn Toeplitz thành Mô hình Không gian Trạng thái (SSM) đã cho thấy kết quả hứa hẹn trong các thí nghiệm của chúng tôi, có một số hạn chế nhất định cần được thừa nhận.

1. Sự đánh đổi giữa Độ chính xác và Hiệu quả:
Mặc dù phương pháp của chúng tôi đạt được những cải thiện đáng kể về hiệu quả, điều quan trọng cần lưu ý là có thể có sự đánh đổi giữa độ chính xác và hiệu quả. Việc chuyển đổi từ biểu diễn Toeplitz sang SSM liên quan đến các xấp xỉ và đơn giản hóa, có thể gây ra một mức độ lỗi nhất định so với biểu diễn gốc. Mặc dù các thí nghiệm của chúng tôi đã chứng minh hiệu suất tương đương với Mạng Nơ-ron Toeplitz (TNN) gốc, có thể có những tình huống mà SSM đã biến đổi không nắm bắt đầy đủ các mẫu phức tạp có trong mô hình gốc.

2. Phạm vi Ứng dụng: Phương pháp của chúng tôi đã được đánh giá mở rộng trong các tác vụ mô hình hóa ngôn ngữ và chứng minh hiệu suất vượt trội so với các phương pháp dựa trên gradient và triển khai TNN gốc. Tuy nhiên, khả năng áp dụng của phương pháp có thể bị giới hạn cho các tác vụ mô hình hóa chuỗi và các mô hình dựa trên long convolution. Cần có nghiên cứu thêm để khám phá hiệu quả của nó trong các lĩnh vực và kiến trúc mô hình khác.

Mặc dù phương pháp đề xuất của chúng tôi cung cấp một cách tiếp cận hấp dẫn để chuyển đổi biểu diễn Toeplitz thành Mô hình Không gian Trạng thái, điều quan trọng là phải xem xét các hạn chế đã đề cập ở trên. Việc giải quyết những hạn chế này và khám phá thêm tiềm năng của phương pháp trong các lĩnh vực và kiến trúc mô hình đa dạng sẽ là những hướng nghiên cứu có giá trị trong tương lai.

Tài liệu tham khảo
[Danh sách các tài liệu tham khảo với tên tác giả, tiêu đề và thông tin xuất bản]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo với các nghiên cứu về machine learning, natural language processing, và các phương pháp sequence modeling]
