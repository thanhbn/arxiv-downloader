# 2402.08678.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2402.08678.pdf
# File size: 1199791 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Graph Mamba: Towards Learning on Graphs with State Space Models
Ali Behrouz* 1Farnoosh Hashemi* 1
Abstract
Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority
of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple
layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor
capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to
Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive
biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show
that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice,
neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present
Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We
discuss and categorize the new challenges when adapting SSMs to graph-structured data, and present four required
and one optional steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering,
(3) Architecture of Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE and SE.
We further provide theoretical justification for the power of GMNs. Experiments demonstrate that despite much
less computational cost, GMNs attain an outstanding performance in long-range, small-scale, large-scale, and
heterophilic benchmark datasets. The code is in this link.
1. Introduction
Recently, graph learning has become an important and popular area of study due to its impressive results in a wide range
of applications, like neuroscience (Behrouz et al., 2023), social networks (Fan et al., 2019), molecular graphs (Wang
et al., 2021), etc. In recent years, Message-Passing Neural Networks (MPNNs), which iteratively aggregate neighborhood
information to learn the node/edge representations, have been the dominant paradigm in machine learning on graphs (Kipf
& Welling, 2016; Veli ˇckovi ´c et al., 2018; Wu et al., 2020; Gutteridge et al., 2023). They, however, have some inherent
limitations, including over-squashing (Di Giovanni et al., 2023), over-smoothing (Rusch et al., 2023), and poor capturing of
long-range dependencies (Dwivedi et al., 2022). With the rise of Transformer architectures (Vaswani et al., 2017) and their
success in diverse applications such as natural language processing (Wolf et al., 2020) and computer vision (Liu et al., 2021),
their graph adaptations, so-called Graph Transformers (GTs), have gained popularity as the alternatives of MPNNs (Yun
et al., 2019; Kim et al., 2022; Ramp ´aˇsek et al., 2022).
Graph transformers have shown promising performance in various graph tasks, and their variants have achieved top scores
in several graph learning benchmarks (Hu et al., 2020; Dwivedi et al., 2022). The superiority of GTs over MPNNs is often
explained by MPNNs’ bias towards encoding local structures (M ¨uller et al., 2023), while a key underlying principle of
GTs is to let nodes attend to all other nodes through a global attention mechanism (Kim et al., 2022; Yun et al., 2019),
allowing direct modeling of long-range interactions. Global attention, however, has weak inductive bias and typically
requires incorporating information about nodes’ positions to capture the graph structure (Ramp ´aˇsek et al., 2022; Kim et al.,
2022). To this end, various positional and structural encoding schemes based on spectral and graph features have been
introduced (Kreuzer et al., 2021; Kim et al., 2022; Lim et al., 2023a).
Despite the fact that GTs with proper positional encodings (PE) are universal approximators and provably more powerful
*Equal contribution1Cornell University, Ithaca, USA. Correspondence to: Ali Behrouz <ab2947@cornell.edu >, Farnoosh Hashemi
<sh2574@cornell.edu >.
Preprint. Under Review.
1arXiv:2402.08678v2  [cs.LG]  19 Feb 2024

--- PAGE 2 ---
Graph Mamba
than any Weisfeiler-Lehman isomorphism test (WL test) (Kreuzer et al., 2021), their applicability to large-scale graphs is
hindered by their poor scalability. That is, the standard global attention mechanism on a graph with nnodes incurs both
time and memory complexity of O(n2), quadratic in the input size, making them infeasible on large graphs. To overcome
the high computational cost, inspired by linear attentions (Zaheer et al., 2020), sparse attention mechanisms on graphs
attracts attention (Ramp ´aˇsek et al., 2022; Shirzad et al., 2023). For example, Exphormer (Shirzad et al., 2023) suggests using
expander graphs, global connectors, and local neighborhoods as three patterns that can be incorporated in GTs, resulting
in a sparse and efficient attention. Although sparse attentions partially overcome the memory cost of global attentions,
GTs based on these sparse attentions (Ramp ´aˇsek et al., 2022; Shirzad et al., 2023) still might suffer from quadratic time
complexity. That is, they require costly PE (e.g., Laplacian eigen-decomposition) and structural encoding (SE) to achieve
their best performance, which can take O(n2)to compute.
Another approach to improve GTs’ high computational cost is to use subgraph tokenization (Chen et al., 2023; Zhao et al.,
2021; Kuang et al., 2021; Baek et al., 2021; He et al., 2023), where tokens (a.k.a patches) are small subgraphs extracted
with a pre-defined strategy. Typically, these methods obtain the initial representations of the subgraph tokens by passing
them through an MPNN. Given kextracted subgraphs (tokens), the time complexity of these methods is O(k2), which is
more efficient than typical GTs with node tokenization. Also, these methods often do not rely on complex PE/SE, as their
tokens (subgraphs) inherently carry inductive bias. These methods, however, have two major drawbacks: (1) To achieve
high expressive power, given a node, they usually require at least a subgraph per each remaining node (Zhang et al., 2023a;
Bar-Shalom et al., 2023), meaning that k∈ O(n)and so the time complexity is O(n2). (2) Encoding subgraphs via MPNNs
can transmit all their challenges of over-smoothing and over-squashing, limiting their applicability to heterophilic and
long-range graphs.
Recently, Space State Models (SSMs), as an alternative of attention-based sequence modeling architectures like Transformers
have gained increasing popularity due to their efficiency (Zhang et al., 2023b; Nguyen et al., 2023). They, however, do not
achieve competitive performance with Transformers due to their limits in input-dependent context compression in sequence
models, caused by their time-invariant transition mechanism. To this end, Gu & Dao (2023) present Mamba, a selective
state space model that uses recurrent scans along with a selection mechanism to control which part of the sequence can flow
into the hidden states. This selection can simply be interpreted as using data-dependent state transition mechanism (See §2.3
for a detailed discussion). Mamba outstanding performance in language modeling, outperforming Transformers of the same
size and matching Transformers twice its size, motivates several recent studies to adapt its architecture for different data
modalities (Liu et al., 2024b; Yang et al., 2024; Zhu et al., 2024; Ahamed & Cheng, 2024).
Mamba architecture is specifically designed for sequence data and the complex non-causal nature of graphs makes directly
applying Mamba on graphs challenging. Further, natural attempts to replace Transformers with Mamba in existing GTs
frameworks (e.g., GPS (Ramp ´aˇsek et al., 2022), TokenGT (Kim et al., 2022)) results in suboptimal performance in
both effectiveness and time efficiency (See §5 for evaluation and §3 for a detailed discussion). The reason is, contrary to
Transformers that allows each node to interact with all the other nodes, Mamaba, due to its recurrent nature, only incorporates
information about previous tokens (nodes) in the sequence. This introduces new challenges compared to GTs: (1) The new
paradigm requires token ordering that allows the model take advantage of the provided positional information as much as
possible. (2) The architecture design need to be more robust to permutation than a pure sequential encoder (e.g., Mamba).
(3) While the quadratic time complexity of attentions can dominate the cost of PE/SE in GTs, complex PE/SE (with O(n2)
cost) can be a bottleneck for scaling Graph Mamba on large graphs.
Contributions. To address all the abovementioned limitations, we present Graph Mamba Networks (GMNs), a new class of
machine learning on graphs based on state space models (Figure 1 shows the schematic of the GMNs). In summary our
contributions are:
•Recipe for Graph Mamba Networks. We discuss new challenges of GMNs compared to GTs in architecture design
and motivate our recipe with four required and one optional steps to design GMNs. In particular, its steps are (1)
Tokenization, (2) Token Ordering, (3) Local Encoding, (4) Bidirectional Selective SSM Encoder and dispensable (5) PE
and SE.
•An Efficient Tokenization for Bridging Frameworks. Literature lacks a common foundation about what constitutes a
good tokenization. Accordingly, architectures are required to choose either node- or subgraph-level tokenization, while
each of which has its own (dis)advantages, depending on the data. We present a graph tokenization process that not
only is fast and efficient, but it also bridges the node- and subgraph-level tokenization methods using a single parameter.
2

--- PAGE 3 ---
Graph Mamba
TokenizationRandom Walk  For each node and !𝑚=1,…,𝑚, we sample 𝑀 walks with length !𝑚 and consider their induced subgraph as a token. For each !𝑚, we repeat the process 𝑠 times.𝒎	=	𝟎: Each node is an independent token.
s subgraphs    (!𝑚=1)………s subgraphs   (!𝑚=2)s subgraphs   (!𝑚=𝑚)PE/SEPE/SE (1) Sum over the rows of non-diagonal elements of the random walk matrix. (2) Eigenvectors of the Laplacian. (3) Anonymous random walk encoding, i.e., counting the number of times a node appears at a certain position.Relative PE/SE  Using pair-wise difference of PE/SE as edge features.  ConcatenationLocal EncodingMPNNs  To vectorize each token one can use message-passing to incorporate local information.RWF  Given the walks that corresponds to a subgraph, one can use local identity relation of nodes to vectorize it.For each Token:Token Ordering𝒎≥𝟏 Tokens have implicit order due to hierarchical structure.𝒎=𝟎  Sort nodes based on PRR/Degree.Implicit orderBidirectional MambaRobust to Permutation We scan the sequence of tokens in two directions.
𝜎𝜎×𝜎×+
PNAGated GCNGINEMPNN⊕…𝒎=𝟎PRR/Degree/………($𝑚=1)s
smalllarge…⊕
𝜎Sum/ConcatenationActivation FunctionLinear Layer1-d ConvolutionSelective SSMRequired StepOptional Step⊕Key PointsDomain Knowledge One can use domain knowledge (when adapting GMs to specific domain) or structural properties like Personalized PageRank or degree.Long Sequence Mamba shows performance improvement with longer sequences,  and so we use parameter  s to control the length of the subgraph sequence. Based on the dataset, one can tune 𝑠 to achieve better results.Optional PE/SE  When using subgraph tokens (i.e., !𝑚≥1), PE/SE is optional. That is, tokens have their own inductive bias, and do not need additional information about the graph structure. Features  When  node or edge features are available, one can concatenate them with the PE/SE, before the local encoding step. Allows switching between node and subgraph tokenization using a single parameter 𝑚, making the choice of tokenization a tunable hyperparameter during training.($𝑚=𝑚)…s
Figure 1. Schematic of the GMNs with four required and one optional steps: (1) Tokenization: the graph is mapped into a sequence
of tokens ( m≥1: subgraph and m= 0: node tokenization) (2) ( Optional Step ) PE/SE: inductive bias is added to the architecture
using information about the position of nodes and the strucutre of the graph. (3) Local Encoding: local structures around each node
are encoded using a subgraph vectorization mechanism. (4) Token Ordering: the sequence of tokens are ordered based on the context.
(Subgraph tokenization ( m≥1) has implicit order and does not need this step). (5) (Stack of) Bidirectional Mamba: it scans and selects
relevant nodes or subgraphs to flow into the hidden states.†In this figure, the last layer of bidirectional Mamba, which performs as a
readout on all nodes, is omitted for simplicity.
Moreover, the presented tokenization has implicit order, which is specially important for sequential encoders like SSMs.
•New Bidirectional SSMs for Graphs. Inspired by Mamba, we design a SSM architecture that scans the input sequence
in two different directions, making the model more robust to permutation, which is particularly important when we do
not use implicitly ordered tokenization on graphs.
•Theoretical Justification. We provide theoretical justification for the power of GMNs and show that they are universal
approximator of any functions on graphs. We further show that GMNs using proper PE/SE is more expressive than any
WL test, matching GTs in this manner.
•Outstanding Performance and New Insights. Our experimental evaluations demonstrate that GMNs attain an
outstanding performance in long-range, small-scale, large-scale, and heterophilic benchmark datasets, while consuming
less GPU memory. These results show that while Transformers, complex message-passing, and SE/PE are sufficient for
good performance in practice, neither is necessary. We further perform ablation study and validate the contribution of
each architectural choice.
2. Related Work and Backgrounds
To situate GMNs in a broader context, we discuss four relevant types of machine learning methods:
2.1. Message-Passing Neural Networks
Message-passing neural networks are a class of GNNs that iteratively aggregate local neighborhood information to learn the
node/edge representations (Kipf & Welling, 2016). MPNNs have been the dominant paradigm in machine learning on graphs,
and attracts much attention, leading to various powerful architectures, e.g., GAT (Veli ˇckovi ´c et al., 2018), GCN (Henaff
3

--- PAGE 4 ---
Graph Mamba
et al., 2015; Kipf & Welling, 2016), GatedGCN (Bresson & Laurent, 2017), GIN (Xu et al., 2019), etc. Simple MPNNs,
however, are known to suffer from some major limitations including: (1) limiting their expressivity to the 1-WL isomorphism
test (Xu et al., 2019), (2) over-smoothing (Rusch et al., 2023), and (3) over-squashing (Alon & Yahav, 2021; Di Giovanni
et al., 2023). Various methods have been developed to augment MPNNs and overcome such issues, including higher-order
GNNs (Morris et al., 2019; 2020), graph rewiring (Gutteridge et al., 2023; Arnaiz-Rodr ´ıguez et al., 2022), adaptive and
cooperative GNNs (Errica et al., 2023; Finkelshtein et al., 2023), and using additional features (Sato et al., 2021; Murphy
et al., 2019).
2.2. Graph Transformers
With the rise of Transformer architectures (Vaswani et al., 2017) and their success in diverse applications such as natural
language processing (Wolf et al., 2020) and computer vision (Liu et al., 2021), their graph adaptations have gained popularity
as the alternatives of MPNNs (Yun et al., 2019; Kim et al., 2022; Ramp ´aˇsek et al., 2022). Using a full global attention,
GTs consider each pair of nodes connected (Yun et al., 2019) and so are expected to overcome the problems of over-
squashing and over-smoothing in MPNNs (Kreuzer et al., 2021). GTs, however, have weak inductive bias and needs proper
positional/structural encoding to learn the structure of the graph (Kreuzer et al., 2021; Ramp ´aˇsek et al., 2022). To this end,
various studies have focused on designing powerful positional and structural encodings (Wang et al., 2022; Ying et al., 2021;
Kreuzer et al., 2021; Shiv & Quirk, 2019).
Sparse Attention. While GTs have shown outstanding performance in different graph tasks on small-scale datasets (up
to 10K nodes), their quadratic computational cost, caused by their full global attention, has limited their applicability to
large-scale graphs (Ramp ´aˇsek et al., 2022). Motivated by linear attention mechanisms (e.g., BigBird (Zaheer et al., 2020)
and Performer (Choromanski et al., 2021)), which are designed to overcome the same scalability issue of Transformers on
long sequences, using sparse Transformers in GT architectures has gained popularity (Ramp ´aˇsek et al., 2022; Shirzad et al.,
2023; Kong et al., 2023; Liu et al., 2023; Wu et al., 2023). The main idea of sparse GTs models is to restrict the attention
pattern, i.e., the pairs of nodes that can interact with each other. As an example, Shirzad et al. (2023) present Exphormer,
the graph adaption of BigBird that uses three sparse patterns of (1) expander graph attention, (2) local attention among
neighbors, and (3) global attention by connecting virtual nodes to all non-virtual nodes.
Subgraph Tokenization. Another method to overcome GTs’ high computational cost is to use subgraph tokenization (Chen
et al., 2023; Zhao et al., 2021; Baek et al., 2021; He et al., 2023), where tokens are small subgraphs extracted with a
pre-defined strategy. These subgraph tokenization strategies usually are k-hop neighborhood (given a fixed k) (Nguyen et al.,
2022a; Hussain et al., 2022; Park et al., 2022), learnable sample of neighborhood (Zhang et al., 2022), ego-networks (Zhao
et al., 2021), hierarchical k-hop neighborhoods (Chen et al., 2023), graph motifs (Rong et al., 2020), and graph partitions (He
et al., 2023). To vectorize each token, subgraph-based GT methods typically rely on MPNNs, making them vulnerable to
over-smoothing and over-squashing. Most of them also use a fixed neighborhood of each node, missing the hierarchical
structure of the graph. The only exception is NAGphormer (Chen et al., 2023) that uses all k= 1, . . . , K -hop neighborhoods
of each node as its corresponding tokens. Although this tokenization lets the model learn the hierarchical structure of the
graph, by increasing the hop of the neighborhood, its tokens become exponentially larger, limiting its ability to scale to large
graphs.
2.3. State Space Models
State Space Models (SSMs), a type of sequence models, are usually known as linear time-invariant systems that map input
sequence x(t)∈RLto response sequence y(t)∈RL(Aoki, 2013). Specifically, SSMs use a latent state h(t)∈RN×L,
evolution parameter A∈RN×N, and projection parameters B∈RN×1,C∈R1×Nsuch that:
h′(t) =Ah(t) +Bx(t),
y(t) =Ch(t). (1)
Due to the hardness of solving the above differential equation in deep learning settings, discrete space state models (Gu
et al., 2020; Zhang et al., 2023b) discretize the above system using a parameter ∆:
ht=¯Aht−1+¯Bxt,
yt=Cht, (2)
4

--- PAGE 5 ---
Graph Mamba
where
¯A= exp ( ∆A),
¯B= (∆A)−1(exp (∆A−I)).∆B. (3)
Gu et al. (2020) shows that discrete-time SSMs are equivalent to the following convolution:
¯K= ¯C¯B,¯C¯A¯B, . . . , ¯C¯AL−1¯B
,
y=x∗¯K, (4)
and accordingly can be computed very efficiently. Structured state space models (S4), another type of SSMs, are efficient
alternatives of attentions and have improved efficiency and scalability of SSMs using reparameterization (Gu et al., 2022;
Fu et al., 2023; Nguyen et al., 2023). SSMs show promising performance on timeseries data (Zhang et al., 2023b; Tang
et al., 2023), Genomic sequence (Nguyen et al., 2023), healthcare domain (Gu et al., 2021), and computer vision (Gu et al.,
2021; Nguyen et al., 2022b). They, however, lack selection mechanism, causing missing the context as discussed by Gu &
Dao (2023). Recently, Gu & Dao (2023) introduce an efficient and powerful selective structured state space architecture,
called MAMBA , that uses recurrent scans along with a selection mechanism to control which part of the sequence can flow
into the hidden states. The selection mechanism of Mamba can be interpreted as using data-dependent state transition
mechanisms, i.e., making B,C,and∆as function of input xt. Mamba outstanding performance in language modeling,
outperforming Transformers of the same size and matching Transformers twice its size, motivates several recent studies to
adapt its architecture for different data modalities and tasks (Liu et al., 2024b; Yang et al., 2024; Zhu et al., 2024; Ahamed &
Cheng, 2024; Xing et al., 2024; Liu et al., 2024a; Ma et al., 2024).
3. Challenges & Motivations: Transformers vs Mamba
Mamba architecture is specifically designed for sequence data and the complex non-causal nature of graphs makes directly
applying Mamba on graphs challenging. Based on the common applicability of Mamba and Transformers on tokenized
sequential data, a straightforward approach to adapt Mamba for graphs is to replace Transformers with Mamba in GTs
frameworks, e.g., TokenGT (Kim et al., 2022) or GPS (Ramp ´aˇsek et al., 2022). However, this approach might not fully
take advantage of selective SSMs due to ignoring some of their special traits. In this section, we discuss new challenges for
GMNs compared to GTs.
Sequences vs 2-D Data. It is known that the self-attentive architecture corresponds to a family of permutation equivariant
functions (Lee et al., 2019; Liu et al., 2020). That is, the attention mechanism in Transformers (Vaswani et al., 2017) assumes
a connection between each pair of tokens, regardless of their positions in the sequence, making it permutation equivariant.
Accordingly, Transformers lack inductive bias and so properly positional encoding is crucial for their performance, whenever
the order of tokens matter (Vaswani et al., 2017; Liu et al., 2020). On the other hand, Mamba is a sequential encoder and
scans tokens in a recurrent manner (potentially less sensitive to positional encoding). Thus, it expects causal data as an input,
making it challenging to be adapted to 2-D (e.g., images) (Liu et al., 2024b) or complex graph-structured data. Accordingly,
while in graph adaption of Transformers mapping the graph into a sequence of tokens along with a positional/structural
encodings were enough, sequential encoders, like SSMs, and more specifically Mamba, require an ordering mechanism for
tokens.
Although this sensitivity to the order of tokens makes the adaption of SSMs to graphs challenging, it can be more powerful
whenever the order matters. For example, learning the hierarchical structures in the neighborhood of each node ( k-hops
fork= 1, . . . , K ), which is implicitly ordered, is crucial in different domains (Zhong et al., 2022; Lim et al., 2023b).
Moreover, it provides the opportunity to use domain knowledge when the order matters (Yu et al., 2020). In our proposed
framework, we provide the opportunity for both cases: (1) using domain knowledge or structural properties (e.g., Personalized
PageRank (Page et al., 1998)) when the order matters, or (2) using implicitly ordered subgraphs (no ordering is needed).
Furthermore, our bidirectional encoder scans nodes in two different directions, being capable of learning equivariance
functions on the input, whenever it is needed.
Long-range Sequence Modeling. In graph domain, the sequence of tokens, either node, edge, or subgraph, can be counted
as the context. Unfortunately, Transformer architecture, and more specifically GTs, are not scalable to long sequence.
Furthermore, intuitively, more context (i.e., longer sequence) should lead to better performance; however, recently it has
been empirically observed that many sequence models do not improve with longer context in language modeling (Shi et al.,
5

--- PAGE 6 ---
Graph Mamba
2023). Mamba, because of its selection mechanism, can simply filter irrelevant information and also reset its state at any time.
Accordingly, its performance improves monotonically with sequence length (Gu & Dao, 2023). To this end, and to fully
take advantage of Mamba, one can map a graph or node to long sequences, possibly bags of various subgraphs. Not only the
long sequence of tokens can provide more context, but it also potentially can improve the expressive power (Bevilacqua
et al., 2022).
Scalability. Due to the complex nature of graph-structured data, sequential encoders, including Transformers and Mamba,
require proper positional and structural encodings (Ramp ´aˇsek et al., 2022; Kim et al., 2022). These PEs/SEs, however, often
have quadratic computational cost, which can be computed once before training. Accordingly, due to the quadratic time
complexity of Transformers, computing these PEs/SEs was dominated and they have not been the bottleneck for training
GTs. GMNs, on the other hand, have linear computational cost (with respect to both time and memory), and so constructing
complex PEs/SEs can be their bottleneck when training on very large graphs. This bring a new challenge for GMNs, as they
need to either (1) do not use PEs/SEs, or (2) use their more efficient variants to fully take advantage of SSMs efficiency.
Our architecture design make the use of PE/SE optional and our empirical evaluation shows that GMNs without PE/SE can
achieve competitive performance compared to methods with complex PEs/SEs.
Node or Subgraph ? In addition to the above new challenges, there is a lack of common foundation about what constitutes a
good tokenization, and what differentiates them, even in GT frameworks. Existing methods use either node/edge (Shirzad
et al., 2023; Ramp ´aˇsek et al., 2022; Kim et al., 2022), or subgraph tokenization methods (Chen et al., 2023; Zhao et al.,
2021; He et al., 2023). While methods with node tokenization are more capable of capturing long-range dependencies,
methods with subgraph tokens have more ability to learn local neighborhoods, are less rely on PE/SE (Chen et al., 2023),
and are more efficient in practice. Our architecture design lets switching between node and subgraph tokenization using a
single parameter m, making the choice of tokenization a tunable hyperparameter during training.
4. Graph Mamba Networks
In this section, we provide our five-step recipe for powerful, flexible, and scalable Graph Mamba Networks. Following
the discussion about the importance of each step, we present our architecture. The overview of the GMN framework is
illustrated in Figure 1.
Throughout this section, we let G= (V, E)be a graph, where V={v1, . . . , v n}is the set of nodes and E⊆V×Vis the
set of edges. We assume each node v∈Vhas a feature vector x(0)
v∈X, where X∈Rn×dis the feature matrix describing
the attribute information of nodes and dis the dimension of feature vectors. Given v∈V, we let N(v) ={u|(v, u)∈E}
be the set of v’s neighbors. Given a subset of nodes S⊆V, we use G[S]to denote the induced subgraph constructed by
nodes in S, andXSto denote the feature matrix describing the attribute information of nodes in S.
4.1. Tokenization and Encoding
Tokenization, which is the process of mapping the graph into a sequence of tokens, is an inseparable part of adapting
sequential encoders to graphs. As discussed earlier, existing methods use either node/edge (Shirzad et al., 2023; Ramp ´aˇsek
et al., 2022; Kim et al., 2022), or subgraph tokenization methods (Chen et al., 2023; Zhao et al., 2021; He et al., 2023),
each of which has its own (dis)advantages. In this part, we present a new simple but flexible and effective neighborhood
sampling for each node and discuss its advantages over existing subgraph tokenization. The main and high-level idea of our
tokenization is to first, sample some subgraphs for each node that can represent the node’s neighborhood structure as well as
its local, and global positions in the graph. Then we vectorize (encode) these subgraphs to obtain the node representations.
Neighborhood Sampling. Given a node v∈V, and two integers m, M≥0, for each 0≤ˆm≤m, we sample Mrandom
walks started from vwith length ˆm. LetTˆm,i(v)fori= 0, . . . , M be the set of visited nodes in the i-th walk. We define the
token corresponds to all walks with length ˆmas:
G[Tˆm(v)] =G"M[
i=0Tˆm,i(v)#
, (5)
which is the union of all walks with length ˆm. One can interpret G[Tˆm(v)]as the induced subgraph of a sample of
ˆm-hop neighborhood of node v. At the end, for each node v∈Vwe have the sequence of G[T0(v)], . . . , G [Tm(v)]as its
corresponding tokens.
6

--- PAGE 7 ---
Graph Mamba
Using random walks (with fixed length) or k-hop neighborhood of a node as its representative tokens has been discussed in
several recent studies (Ding et al., 2023; Zhang et al., 2022; Chen et al., 2023; Zhao et al., 2021). These methods, however,
suffer from a subset of these limitations: (1) they use a fixed-length random walk (Kuang et al., 2021), which misses the
hierarchical structure of the node’s neighborhood. This is particularly important when the long-range dependencies of nodes
are important. (2) they use all nodes in all k-hop neighborhoods (Chen et al., 2023; Ding et al., 2023), resulting in a trade-off
between long-range dependencies and over-smoothing or over-squashing problems. Furthermore, the k-hop neighborhood of
a well-connected node might be the whole graph, resulting in considering the graph as a token of a node, which is inefficient.
Our neighborhood sampling approach addresses all these limitations. It sampled the fixed number of random walks with
different lengths for all nodes, capturing hierarchical structure of the neighborhood while avoiding both inefficiency, caused
by considering the entire graph, and over-smoothing and over squashing, caused by large neighborhood aggregation.
Why Not More Subgraphs ? As discussed earlier, empirical evaluation has shown that the performance of selective state
space models improves monotonically with sequence length (Gu & Dao, 2023). Furthermore, their linear computational
cost allow us to use more tokens, providing them more context. Accordingly, to fully take advantage of selective state space
models, given an integer s >0, we repeat the above neighborhood sampling process for stimes. Accordingly, for each node
v∈Vwe have a sequence of
G[T0(v)], G[T1
1(v)], . . . , G [Ts
1(v)]| {z }
stimes, . . . , G [T1
m(v)], . . . , G [Ts
m(v)]| {z }
stimes
as its corresponding sequence of tokens. Here, we can see another advantage of our proposed neighborhood sampling
compared to Chen et al. (2023); Ding et al. (2023). While in NAGphormer (Chen et al., 2023) the sequence length of each
node is limited by the diameter of the graph, our method can produce a long sequence of diverse subgraphs.
Theorem 4.1. With large enough M, m, ands >0, GMNs’ neighborhood sampling is strictly more expressive than k-hop
neighborhood sampling.
Structural/Positional Encoding . To further augment our framework for Graph Mamba, we consider an optional step, when
we inject structural and positional encodings to the initial features of nodes/edges. PE is meant to provide information about
the position of a given node within the graph. Accordingly, two close nodes within a graph or subgraph are supposed to have
close PE. SE, on the other hand, is meant to provide information about the structure of a subgraph. Following Ramp ´aˇsek
et al. (2022), we concatenate either eigenvectors of the graph Laplacian or Random-walk structural encodings to the nodes’
feature, whenever PE/SE are needed: i.e.,
x(new)
v =xv||pv, (6)
where pvis the corresponding positional encoding to v. For the sake of consistency, we use xvinstead of x(new)
v throughout
the paper.
Neighborhood Encoding. Given a node v∈Vand its sequence of tokens (subgraphs), we encode the subgraph via encoder
ϕ(.). That is, we construct x1
v,x2
v, . . . ,xms−1
v,xms
v∈Rdas follows:
x((i−1)s+j)
v =ϕ
G[Tj
i(v)],XTj
i(v)
, (7)
where 1≤i≤mand1≤j≤s. In practice, this encoder can be an MPNN, (e.g., Gated-GCN (Bresson & Laurent, 2017)),
or RWF (T ¨onshoff et al., 2023b) that encodes nodes with respect to a sampled set of walks into feature vectors with four
parts: (1) node features, (2) edge features along the walk, and (3, 4) local structural information.
Token Ordering. By Equation 7, we can calculate the neighborhood embeddings for various sampled neighborhoods of a
node and further construct a sequence to represent its neighborhood information, i.e., x1
v,x2
v, . . . ,xms−1
v,xms
v. As discussed
in§3, adaption of sequence models like SSMs to graph-structured data requires an order on the tokens. To understand what
constitutes a good ordering, we need to recall selection mechanism in Mamba (Gu & Dao, 2023) (we will discuss selection
mechanism more formally in §4.2). Mamba by making B,C,and∆as functions of input xt(see§2.3 for notations) lets the
model filter irrelevant information and select important tokens in a recurrent manner, meaning that each token gets updated
based on tokens that come before them in the sequence. Accordingly, earlier tokens have less information about the context
of sequence, while later tokens have information about almost entire sequence. This leads us to order tokens based on either
their needs of knowing information about other tokens or their importance to our task.
7

--- PAGE 8 ---
Graph Mamba
When m≥1: For the sake of simplicity first let s= 1. In the case that m≥1, interestingly, our architecture design
provides us with an implicitly ordered sequence. That is, given v∈V, thei-th token is a samples from i-hop neighborhood
of node v, which is the subgraph of all j-hop neighborhoods, where j≥i. This means, given a large enough M(number of
sampled random walks), our Tj(v)has enough information about Ti(v), not vice versa. To this end, we use the reverse of
initial order, i.e., xm
v,xm−1
v, . . . ,x2
v,x1
v. Accordingly, inner subgraphs can also have information about the global structure.
When s≥2, we use the same procedure as above, and reverse the initial order, i.e., xsm
v,xsm−1
v, . . . ,x2
v,x1
v. To make our
model robust to the permutation of subgraphs with the same walk length ˆm, we randomly shuffle them. We will discuss the
ordering in the case of m= 0later.
4.2. Bidirectional Mamba
As discussed in §3, SSMs are recurrent models and require ordered input, while graph-structured data does not have any
order and needs permutation equivariant encoders. To this end, inspired by Vim in computer vision (Zhu et al., 2024), we
modify Mamba architecture and use two recurrent scan modules to scan data in two different directions (i.e., forward and
backward). Accordingly, given two tokens tiandtj, where i > j and indices show their initial order, in forward scan ti
comes after tjand so has the information about tj(which can be flown into the hidden states or filtered by the selection
mechanism). In backward pass tjcomes after tiand so has the information about ti. This architecture is particularly
important when m= 0(node tokenization), which we will discuss later.
More formally, in forward pass module, let Φbe the input sequence (e.g., given v,Φis a matrix whose rows are
xsm
v,xsm−1
v, . . . ,x1
v, calculated in Equation 7), Abe the relative positional encoding of tokens, we have:
Φinput=σ(Conv (WinputLayerNorm (Φ))),
B=WBΦinput,C=WCΦinput,∆=Softplus (W∆Φinput),
¯A=Discrete A(A,∆),
¯B=Discrete B(B,∆),
y=SSM ¯A,¯B,C(Φinput),
yforward =Wforward ,1(y⊙σ(Wforward ,2LayerNorm (Φ))), (8)
where W,WB,WC,W∆,Wforward ,1andWforward ,2are learnable parameters, σ(.)is nonlinear function (e.g., SiLU ),
LayerNorm (.)is layer normalization (Ba et al., 2016), SSM(.)is the state space model discussed in Equations 2 and 4, and
Discrete (.)is discretization process discussed in Equation 3. We use the same architecture as above for the backward
pass (with different weights) but instead we use Φinverse as the input, which is a matrix whose rows are x1
v,x2
v, . . . ,xsm
v. Let
ybackward be the output of this backward module, we obtain the final encodings as
youtput =Wout(yforward +ybackward ). (9)
In practice, we stack some layers of the bidirectional Mamba to achieve good performance. Note that due to our ordering
mechanism, the last state of the output corresponds to the walk with length ˆm= 0, i.e., the node itself. Accordingly, the last
state represents the updated node encoding.
Augmentation with MPNNs . We further use an optional MPNN module that simultaneously performs message-passing
and augments the output of the bidirectional Mamba via its inductive bias. Particularly this module is very helpful when
there are rich edge features and so an MPNN can help to take advantage of them. While in our empirical evaluation we show
that this module is not necessary for the success of GMNs in several cases, it can be useful when we avoid complex PE/SE
and strong inductive bias is needed.
How Does Selection Work on Subgraphs ? As discussed earlier, the selection mechanism can be achieved by making
B,C,and∆as the functions of the input data (Gu & Dao, 2023). Accordingly, in recurrent scan, based on the input, the
model can filter the irrelevant context. The selection mechanism in Equation 9 is implemented by making B,C,and∆as
functions of Φinput, which is matrix of the encodings of neighborhoods. Therefore, as model scans the sampled subgraphs
from the i-hop neighborhoods in descending order of i, it filters irrelevant neighborhoods to the context (last state), which is
the node encoding.
Last Layer(s) of Bidirectional Mamba. To capture the long-range dependencies and to flow information across the nodes,
we use the node encodings obtained from the last state of Equation 9 as the input of the last layer(s) of bidirectional Mamba.
8

--- PAGE 9 ---
Graph Mamba
Therefore, the recurrent scan of nodes (in both directions) can flow information across nodes. This design not only helps
capturing long-range dependencies in the graph, but it also is a key to the flexibility of our framework to bridge node and
subgraph tokenization.
4.3. Tokenization When m= 0
In this case, for each node v∈Vwe only consider vitself as its corresponding sequence of tokens. Based on our architecture,
in this case, the first layers of bidirection Mamba become simple projection as the length of the sequence is one. However,
the last layers, where we use node encodings as their input, treats nodes as tokens and become an architecture that use a
sequential encoder (e.g., Mamba) with node tokenization. More specifically, in this special case of framework, the model is
the adaption of GPS (Ramp ´aˇsek et al., 2022) framework, when we replace its Transformer with our bidirectional Mamba.
This architecture design lets switching between node and subgraph tokenization using a single parameter m, making the
choice of tokenization a tunable hyperparameter during training. Note that this flexibility comes more from our architecture
rather than the method of tokenization. That is, in practice one can use only 0-hop neighborhood in NAGphormer (Chen
et al., 2023), resulting in only considering the node itself. However, in this case, the architecture of NAGphormer becomes a
stack of MLPs, resulting in poor performance.
Token Ordering. When m= 0: One remaining question is how one can order nodes when we use node tokenization. As
discussed in §4.1, tokens need to be ordered based on either (1) their needs of knowing information about other tokens
or (2) their importance to our task. When dealing with nodes and specifically when long-range dependencies matter, (1)
becomes a must for all nodes. Our architecture overcomes this challenge by its bidirectional scan process. Therefore, we
need to order nodes based on their importance. There are several metrics to measure the importance of nodes in a graph.
For example, various centrality measures (Latora & Marchiori, 2007; Ruhnau, 2000), degree, k-core (Lick & White, 1970;
Hashemi et al., 2022), Personalized PageRank or PageRank (Page et al., 1998), etc. In our experiments, for the sake of
efficiency and simplicity, we sort nodes based on their degree.
How Does Selection Work on Nodes ? Similar to selection mechanism on subgraphs, the model based on the input data can
filter irrelevant tokens (nodes) to the context (downstream tasks).
4.4. Theoretical Analysis of GMNs
In this section, we provide theoretical justification for the power of GMNs. More specifically, we first show that GMNs are
universal approximator of any function on graphs. Next, we discuss that given proper PE and enough parameters, GMNs
are more powerful than any WL isomorphism test, matching GTs (with the similar assumptions). Finally, we evaluate the
expressive power of GMNs when they do not use any PE or MPNN and show that their expressive power is unbounded
(might be incomparable).
Theorem 4.2 (Universality) .Let1≤p <∞, and ϵ >0. For any continues function f: [0,1]d×n→Rd×nthat is
permutation equivariant, there exists a GMN with positional encoding, gp, such that ℓp(f, g)< ϵ1.
Theorem 4.3 (Expressive Power w/ PE) .Given the full set of eigenfunctions and enough parameters, GMNs can distinguish
any pair of non-isomorphic graphs and are more powerful than any WL test.
We prove the above two theorems based on the recent work of Wang & Xue (2023), where they prove that SSMs with
layer-wise nonlinearity are universal approximators of any sequence-to-sequence function.
Theorem 4.4 (Expressive Power w/o PE and MPNN) .With enough parameters, for every k≥1there are graphs that are
distinguishable by GMNs, but not by k-WL test, showing that their expressive power is not bounded by any WL test.
We prove the above theorem based on the recent work of T ¨onshoff et al. (2023b), where they prove a similar theorem for
CRaWl (T ¨onshoff et al., 2023b). Note that this theorem does not rely on the Mamba’s power, and the expressive power
comes from the choice of neighborhood sampling and encoding.
1ℓp(.)is the p-norm
9

--- PAGE 10 ---
Graph Mamba
Table 1. Benchmark on Long-Range Graph Datasets (Dwivedi et al., 2022). Highlighted are the top first,second , and third results.
ModelCOCO-SP PascalVOC-SP Peptides-Func Peptides-Struct
F1 score ↑ F1 score ↑ AP↑ MAE↓
GCN 0.0841±0.0010 0.1268±0.0060 0.5930±0.0023 0.3496±0.0013
GIN 0.1339±0.0044 0.1265±0.0076 0.5498±0.0079 0.3547±0.0045
Gated-GCN 0.2641±0.0045 0.2873±0.0219 0.5864±0.0077 0.3420±0.0013
CRaWl 0.3219±0.00106 0.4088±0.0079 0.6963±0.0079 0.2506±0.0022
SAN+LapPE 0.2592±0.0158 0.3230±0.0039 0.6384±0.0121 0.2683±0.0043
NAGphormer 0.3458±0.0070 0.4006±0.0061 - -
Graph ViT - - 0.6855±0.0049 0.2468±0.0015
GPS 0.3774±0.0150 0.3689±0.0131 0.6575±0.0049 0.2510±0.0015
GPS (BigBird) 0.2622±0.0008 0.2762±0.0069 0.5854±0.0079 0.2842±0.0130
Exphormer 0.3430±0.0108 0.3975±0.0037 0.6527±0.0043 0.2481±0.0007
GPS + Mamba 0.3895±0.0125 0.4180±0.012 0.6624±0.0079 0.2518±0.0012
GMN- 0.3618±0.0053 0.4169±0.0103 0.6860±0.0012 0.2522±0.0035
GMN 0.3974±0.0101 0.4393±0.0112 0.7071±0.0083 0.2473±0.0025
5. Experiments
In this section, we evaluate the performance of GMNs in long-range, small-scale, large-scale, and heterophilic benchmark
datasets. We further discuss its memory efficiency and perform ablation study to validate the contribution of each architectural
choice. The detailed statistics of datasets and additional experiments are available in the appendix.
5.1. Experimental Setup
Dataset. We use three most commonly used benchmark datasets with long-range, small-scale, large-scale, and heterophilic
properties. For long-range datasets, we use Longe Range Graph Benchmark (LRGB) dataset (Dwivedi et al., 2022). For
small and large-scale datasets, we use GNN benchmark (Dwivedi et al., 2023). To evaluate the GMNs on heterophilic
graphs, we use four heterophilic datasets from the work of Platonov et al. (2023). Finally, we use a large dataset from Open
Graph Benchmark (Hu et al., 2020). We evaluate the performance of GMNs on various graph learning tasks (e.g., graph
classification, regression, node classification and link classification). Also, for each datasets we use the propose metrics
in the original benchmark and report the metric across multiple runs, ensuring the robustness. We discuss datasets, their
statistics and their tasks in Appendix A.
Baselines. We compare our GMNs with (1) MPNNs, e.g., GCN (Kipf & Welling, 2016), GIN (Xu et al., 2019), and
Gated-GCN (Bresson & Laurent, 2017), (2) Random walk based method CRaWl (T ¨onshoff et al., 2023b), (3) state-of-the-art
GTs, e.g., SAN (Kreuzer et al., 2021), NAGphormer (Chen et al., 2023), Graph ViT (He et al., 2023), two variants of
GPS (Ramp ´aˇsek et al., 2022), GOAT (Kong et al., 2023), and Exphormer (Shirzad et al., 2023), and (4) our baselines (i)
GPS + Mamba: when we replace the transformer module in GPS with bidirectional Mamba. (ii) GMN-: when we do not
use PE/SE and MPNN. The details of baselines are in Appendix B.
5.2. Long Range Graph Benchmark
Table 1 reports the results of GMNs and baselines on long-range graph benchmark. GMNs consistently outperform baselines
in all datasets that requires long-range dependencies between nodes. The reason for this superior performance is three
folds: (1) GMNs based on our design use long sequence of tokens to learn node encodings and then use another selection
mechanism to filter irrelevant nodes. The provided long sequence of tokens enables GMNs to learn long-range dependencies,
without facing scalability or over-squashing issues. (2) GMNs using their selection mechanism are capable of filtering the
neighborhood around each node. Accordingly, only informative information flows into hidden states. (3) The random-walk
based neighborhood sampling allow GMNs to have diverse samples of neighborhoods, while capturing the hierarchical
nature of k-hop neighborhoods. Also, it is notable that GMN consistently outperforms our baseline GPS + Mamba, which
shows the importance of paying attention to the new challenges. That is, replacing the transformer module with Mamba,
while improves the performance, cannot fully take advantage of the Mamba traits. Interestingly, GMN-, a variant of GMNs
10

--- PAGE 11 ---
Graph Mamba
Table 2. Benchmark on GNN Benchmark Datasets (Dwivedi et al., 2023). Highlighted are the top first,second , and third results.
ModelMNIST CIFAR10 PATTERN MalNet-Tiny
Accuracy ↑ Accuracy ↑ Accuracy ↑ Accuracy ↑
GCN 0.9071±0.0021 0.5571±0.0038 0.7189±0.0033 0.8100±0.0000
GIN 0.9649±0.0025 0.5526±0.0152 0.8539±0.0013 0.8898±0.0055
Gated-GCN 0.9734±0.0014 0.6731±0.0031 0.8557±0.0008 0.9223±0.0065
CRaWl 0.9794±0.050 0.6901±0.0259 - -
NAGphormer - - 0.8644±0.0003 -
GPS 0.9811±0.0011 0.7226±0.0031 0.8664±0.0011 0.9298±0.0047
GPS (BigBird) 0.9817±0.0001 0.7048±0.0010 0.8600±0.0014 0.9234±0.0034
Exphormer 0.9855±0.0003 0.7469±0.0013 0.8670±0.0003 0.9402±0.0020
GPS + Mamba 0.9821±0.0004 0.7341±0.0015 0.8660±0.0007 0.9311±0.0042
GMN 0.9839±0.0018 0.7576±0.0042 0.8714±0.0012 0.9415±0.0020
Table 3. Benchmark on heterophilic datasets (Platonov et al., 2023). Highlighted are the top first,second , and third results.
ModelRoman-empire Amazon-ratings Minesweeper Tolokers
Accuracy ↑ Accuracy ↑ ROC AUC ↑ ROC AUC ↑
GCN 0.7369±0.0074 0.4870±0.0063 0.8975±0.0052 0.8364±0.0067
Gated-GCN 0.7446±0.0054 0.4300±0.0032 0.8754±0.0122 0.7731±0.0114
NAGphormer 0.7434±0.0077 0.5126±0.0072 0.8419±0.0066 0.7832±0.0095
GPS 0.8200±0.0061 0.5310±0.0042 0.9063±0.0067 0.8371±0.0048
Exphormer 0.8903±0.0037 0.5351±0.0046 0.9074±0.0053 0.8377±0.0078
GOAT 0.7159±0.0125 0.4461±0.0050 0.8109±0.0102 0.8311±0.0104
GPS + Mamba 0.8310±0.0028 0.4513±0.0097 0.8993±0.0054 0.8370±0.0105
GMN 0.8769±0.0050 0.5407±0.0031 0.9101±0.0023 0.8452±0.0021
without Transformer, MPNN, and PE/SE that we use to evaluate the importance of these elements in achieving good
performance, can achieve competitive performance with other complex methods, showing that while Transformers, complex
message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary.
5.3. Comparison on GNN Benchmark
We further evaluate the performance of GMNs in small and large datasets from the GNN benchmark. The results of
GMNs and baseline performance are reported in Table 2. GMN and Exphormer achieve competitive performance each
outperforms the other two times. On the other hand again, GMN consistently outperforms GPS + Mamba baseline, showing
the importance of designing a new framework for GMNs rather then using existing frameworks of GTs.
5.4. Heterophilic Datasets
To evaluate the performance of GMNs on the heterophilc data as well as evaluating their robustness to over-squashing
and over-smoothing, we compare their performance with the state-of-the-art baselines and report the results in Table 3.
Our GMN outperforms baselines in 3 out of 4 datasets and achieve the second best result in the remaining dataset. These
results show that the selection mechanism in GMN can effectively filter irrelevant information and also consider long-range
dependencies in heterophilic datasets.
5.5. Ablation Study
To evaluate the contribution of each component of GMNs in its performance, we perform ablation study. Table 4 reports the
results. The first row, reports the performance of GMNs with its full architecture. Then in each row, we modify one the
elements while keeping the other unchanged: Row 2 remove the bidirectional Mamba and use a simple Mamba. Row 3
remove the MPNN and Row 4 use PPR ordering. Finally the last row remove PE. Results show that all the elements of
GMN contributes to its performance with most contribution from bidirection Mamba.
11

--- PAGE 12 ---
Graph Mamba
Table 4. Ablation study on GMN architecture.
ModelRoman-empire Amazon-ratings Minesweeper
Accuracy ↑ Accuracy ↑ ROC AUC ↑
GMN 0.8769±0.0050 0.5407±0.0031 0.9101±0.0023
w/o bidirectional Mamba 0.8327±0.0062 0.5016±0.0045 0.8597±0.0028
w/o MPNN 0.8620±0.0043 0.5312±0.0044 0.8983±0.0031
PPR ordering 0.8612±0.0019 0.5299±0.0037 0.8991±0.0021
w/o PE 0.8591±0.0054 0.5308±0.0026 0.9011±0.0025
Figure 2. Efficiency evaluation and accuracy of GMNs and baselines on OBGN-Arxiv and
MalNet-Tiny. Highlighted are the top first,second , and third results. OOM: Out of Memory.
Method Gated-GCN GPS NAGphormer Exphormer†GOATOurs
GPS+Mamba GMN
OGBN-Arxiv
Training/Epoch (s) 0.68 OOM 5.06 1.97 13.09 1.18 1.30
Memory (GB) 11.09 OOM 6.24 36.18 8.41 5.02 3.85
Accuracy 0.7141 OOM 0.7013 0.7228 0.7196 0.7239 0.7248
MalNet-Tiny
Training/Epoch (s) 10.3 148.99 - 57.24 - 36.07 41.00
Accuracy 0.9223 0.9234 - 0.9224 - 0.9311 0.9415
†We follow the original paper (Shirzad et al., 2023) and use one virtual node in efficiency evaluation.
1 300 600 900 1200  
Average number of nodes050010001500GPU Memory (MB)
GPS
GMNFigure 3. Memory of GPS and GMN on
MalNet-Tiny dataset.
5.6. Efficiency
As we discussed earlier, one of the main advantages of our model is its efficiency and memory usage. We evaluate this claim
on OGBN-Arxiv (Hu et al., 2020) and MalNet-Tiny (Dwivedi et al., 2023) datasets and report the results in Figure 2. Our
variants of GMNs are the most efficienct methods while achieving the best performance. To show the trend of scalability, we
use MalNet-Tiny and plot the memory usage of GPS and GMN in Figure 3. While GPS, as a graph transformer framework,
requires high computational cost (GPU memory usage), GMNs’s memory scales linearly with respect to the input size.
6. Conclusion
In this paper, we present Graph Mamba Networks (GMNs) as a new class of graph learning based on State Space Model.
We discuss and categorize the new challenges when adapting SSMs to graph-structured data, and present four required and
one optional steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture
of Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE and SE. We further provide theoretical
justification for the power of GMNs and conduct several experiments to empirically evaluate their performance.
Potential Broader Impact
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal
consequences of our work, none of which we feel must be specifically highlighted here.
12

--- PAGE 13 ---
Graph Mamba
References
Ahamed, M. A. and Cheng, Q. Mambatab: A simple yet effective approach for handling tabular data. arXiv preprint
arXiv:2401.08867 , 2024.
Alon, U. and Yahav, E. On the bottleneck of graph neural networks and its practical implications. In International Conference
on Learning Representations , 2021. URL https://openreview.net/forum?id=i80OPhOCVH2.
Aoki, M. State space modeling of time series . Springer Science & Business Media, 2013.
Arnaiz-Rodr ´ıguez, A., Begga, A., Escolano, F., and Oliver, N. M. Diffwire: Inductive graph rewiring via the lov ´asz bound.
InThe First Learning on Graphs Conference , 2022. URL https://openreview.net/forum?id=IXvfIex0mX6f.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization, 2016.
Baek, J., Kang, M., and Hwang, S. J. Accurate learning of graph representations with graph multiset pooling. In International
Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=JHcqXGaqiGn.
Bar-Shalom, G., Bevilacqua, B., and Maron, H. Subgraphormer: Subgraph GNNs meet graph transformers. In NeurIPS
2023 Workshop: New Frontiers in Graph Learning , 2023. URL https://openreview.net/forum?id=e8ba9Hu1mM.
Behrouz, A., Delavari, P., and Hashemi, F. Unsupervised representation learning of brain activity via bridging voxel activity
and functional connectivity. In NeurIPS 2023 AI for Science Workshop , 2023. URL https://openreview.net/forum?id=
HSvg7qFFd2.
Bevilacqua, B., Frasca, F., Lim, D., Srinivasan, B., Cai, C., Balamurugan, G., Bronstein, M. M., and Maron, H. Equivariant
subgraph aggregation networks. In International Conference on Learning Representations , 2022. URL https://openreview.
net/forum?id=dFbKQaRk15w.
Bresson, X. and Laurent, T. Residual gated graph convnets. arXiv preprint arXiv:1711.07553 , 2017.
Chen, J., Gao, K., Li, G., and He, K. NAGphormer: A tokenized graph transformer for node classification in large graphs.
InThe Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
8KYeilT3Ow.
Choromanski, K. M., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin,
A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International
Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH.
Deng, C., Yue, Z., and Zhang, Z. Polynormer: Polynomial-expressive graph transformer in linear time. In The Twelfth
International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=hmv1LpNfXa.
Di Giovanni, F., Giusti, L., Barbero, F., Luise, G., Lio, P., and Bronstein, M. M. On over-squashing in message passing
neural networks: The impact of width, depth, and topology. In International Conference on Machine Learning , pp.
7865–7885. PMLR, 2023.
Ding, Y ., Orvieto, A., He, B., and Hofmann, T. Recurrent distance-encoding neural networks for graph representation
learning. arXiv preprint arXiv:2312.01538 , 2023.
Dwivedi, V . P., Ramp ´aˇsek, L., Galkin, M., Parviz, A., Wolf, G., Luu, A. T., and Beaini, D. Long range graph benchmark.
In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information
Processing Systems , volume 35, pp. 22326–22340. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/
paper files/paper/2022/file/8c3c666820ea055a77726d66fc7d447f-Paper-Datasets andBenchmarks.pdf.
Dwivedi, V . P., Joshi, C. K., Luu, A. T., Laurent, T., Bengio, Y ., and Bresson, X. Benchmarking graph neural networks.
Journal of Machine Learning Research , 24(43):1–48, 2023.
Errica, F., Christiansen, H., Zaverkin, V ., Maruyama, T., Niepert, M., and Alesiani, F. Adaptive message passing: A general
framework to mitigate oversmoothing, oversquashing, and underreaching. arXiv preprint arXiv:2312.16560 , 2023.
13

--- PAGE 14 ---
Graph Mamba
Fan, W., Ma, Y ., Li, Q., He, Y ., Zhao, E., Tang, J., and Yin, D. Graph neural networks for social recommendation. In The
world wide web conference , pp. 417–426, 2019.
Finkelshtein, B., Huang, X., Bronstein, M., and Ceylan, ˙I.˙I. Cooperative graph neural networks. arXiv preprint
arXiv:2310.01267 , 2023.
Fu, D. Y ., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language
modeling with state space models. In The Eleventh International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=COZDy0WYGg.
Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 ,
2023.
Gu, A., Dao, T., Ermon, S., Rudra, A., and R ´e, C. Hippo: Recurrent memory with optimal polynomial projections. Advances
in neural information processing systems , 33:1474–1487, 2020.
Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and R ´e, C. Combining recurrent, convolutional, and continuous-
time models with linear state space layers. Advances in neural information processing systems , 34:572–585, 2021.
Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference
on Learning Representations , 2022. URL https://openreview.net/forum?id=uYLFoz1vlAC.
Gutteridge, B., Dong, X., Bronstein, M. M., and Di Giovanni, F. Drew: Dynamically rewired message passing with delay.
InInternational Conference on Machine Learning , pp. 12252–12267. PMLR, 2023.
Hashemi, F., Behrouz, A., and Lakshmanan, L. V . Firmcore decomposition of multilayer networks. In Proceedings of
the ACM Web Conference 2022 , WWW ’22, pp. 1589–1600, New York, NY , USA, 2022. Association for Computing
Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3512205. URL https://doi.org/10.1145/3485447.3512205.
He, X., Hooi, B., Laurent, T., Perold, A., LeCun, Y ., and Bresson, X. A generalization of vit/mlp-mixer to graphs. In
International Conference on Machine Learning , pp. 12724–12745. PMLR, 2023.
Henaff, M., Bruna, J., and LeCun, Y . Deep convolutional networks on graph-structured data. arXiv preprint
arXiv:1506.05163 , 2015.
Hu, W., Fey, M., Zitnik, M., Dong, Y ., Ren, H., Liu, B., Catasta, M., and Leskovec, J. Open graph benchmark: Datasets for
machine learning on graphs. Advances in neural information processing systems , 33:22118–22133, 2020.
Hussain, M. S., Zaki, M. J., and Subramanian, D. Global self-attention as a replacement for graph convolution. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 655–665, 2022.
Kim, J., Nguyen, D., Min, S., Cho, S., Lee, M., Lee, H., and Hong, S. Pure transformers are powerful graph learners.
Advances in Neural Information Processing Systems , 35:14582–14595, 2022.
Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. arXiv preprint
arXiv:1609.02907 , 2016.
Kong, K., Chen, J., Kirchenbauer, J., Ni, R., Bruss, C. B., and Goldstein, T. Goat: A global transformer on large-scale
graphs. In International Conference on Machine Learning , pp. 17375–17390. PMLR, 2023.
Kreuzer, D., Beaini, D., Hamilton, W., L ´etourneau, V ., and Tossou, P. Rethinking graph transformers with spectral attention.
Advances in Neural Information Processing Systems , 34:21618–21629, 2021.
Kuang, W., Zhen, W., Li, Y ., Wei, Z., and Ding, B. Coarformer: Transformer for large graph via graph coarsening. 2021.
Latora, V . and Marchiori, M. A measure of centrality based on network efficiency. New Journal of Physics , 9(6):188, 2007.
Lee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and Teh, Y . W. Set transformer: A framework for attention-based
permutation-invariant neural networks. In International conference on machine learning , pp. 3744–3753. PMLR, 2019.
Lick, D. R. and White, A. T. k-degenerate graphs. Canadian Journal of Mathematics , 22(5):1082–1096, 1970. doi:
10.4153/CJM-1970-125-1.
14

--- PAGE 15 ---
Graph Mamba
Lim, D., Robinson, J. D., Zhao, L., Smidt, T., Sra, S., Maron, H., and Jegelka, S. Sign and basis invariant networks for
spectral graph representation learning. In The Eleventh International Conference on Learning Representations , 2023a.
URL https://openreview.net/forum?id=Q-UHqMorzil.
Lim, H., Joo, Y ., Ha, E., Song, Y ., Yoon, S., Lyoo, I. K., and Shin, T. Brain age prediction using multi-hop graph attention
module(MGA) with convolutional neural network. In Medical Imaging with Deep Learning, short paper track , 2023b.
URL https://openreview.net/forum?id=brK-VV oDpqo.
Liu, C., Zhan, Y ., Ma, X., Ding, L., Tao, D., Wu, J., and Hu, W. Gapformer: Graph transformer with graph pooling for
node classification. In Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI-23) , pp.
2196–2205, 2023.
Liu, J., Yang, H., Zhou, H.-Y ., Xi, Y ., Yu, L., Yu, Y ., Liang, Y ., Shi, G., Zhang, S., Zheng, H., et al. Swin-umamba:
Mamba-based unet with imagenet-based pretraining. arXiv preprint arXiv:2402.03302 , 2024a.
Liu, X., Yu, H.-F., Dhillon, I., and Hsieh, C.-J. Learning to encode position for transformer with continuous dynamical
model. In International conference on machine learning , pp. 6327–6335. PMLR, 2020.
Liu, Y ., Tian, Y ., Zhao, Y ., Yu, H., Xie, L., Wang, Y ., Ye, Q., and Liu, Y . Vmamba: Visual state space model. arXiv preprint
arXiv:2401.10166 , 2024b.
Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer
using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision , pp. 10012–10022,
2021.
Ma, J., Li, F., and Wang, B. U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv
preprint arXiv:2401.04722 , 2024.
Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M. Weisfeiler and leman go neural:
Higher-order graph neural networks. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pp.
4602–4609, 2019.
Morris, C., Rattan, G., and Mutzel, P. Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings.
Advances in Neural Information Processing Systems , 33:21824–21840, 2020.
M¨uller, L., Galkin, M., Morris, C., and Ramp ´aˇsek, L. Attending to graph transformers. arXiv preprint arXiv:2302.04181 ,
2023.
Murphy, R., Srinivasan, B., Rao, V ., and Ribeiro, B. Relational pooling for graph representations. In International
Conference on Machine Learning , pp. 4663–4673. PMLR, 2019.
Nguyen, D. Q., Nguyen, T. D., and Phung, D. Universal graph transformer self-attention networks. In Companion
Proceedings of the Web Conference 2022 , pp. 193–196, 2022a.
Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., and R ´e, C. S4nd: Modeling images and videos as
multidimensional signals with state spaces. Advances in neural information processing systems , 35:2846–2861, 2022b.
Nguyen, E., Poli, M., Faizi, M., Thomas, A. W., Wornow, M., Birch-Sykes, C., Massaroli, S., Patel, A., Rabideau,
C. M., Bengio, Y ., Ermon, S., Re, C., and Baccus, S. HyenaDNA: Long-range genomic sequence modeling at single
nucleotide resolution. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https:
//openreview.net/forum?id=ubzNoJjOKj.
Page, L., Brin, S., Motwani, R., and Winograd, T. The pagerank citation ranking: Bring order to the web. Technical report,
Technical report, stanford University, 1998.
Park, J., Yun, S., Park, H., Kang, J., Jeong, J., Kim, K.-M., Ha, J.-w., and Kim, H. J. Deformable graph transformer. arXiv
preprint arXiv:2206.14337 , 2022.
Platonov, O., Kuznedelev, D., Diskin, M., Babenko, A., and Prokhorenkova, L. A critical look at the evaluation of
GNNs under heterophily: Are we really making progress? In The Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.net/forum?id=tJbbQfw-5wv.
15

--- PAGE 16 ---
Graph Mamba
Ramp ´aˇsek, L., Galkin, M., Dwivedi, V . P., Luu, A. T., Wolf, G., and Beaini, D. Recipe for a general, powerful, scalable
graph transformer. Advances in Neural Information Processing Systems , 35:14501–14515, 2022.
Rong, Y ., Bian, Y ., Xu, T., Xie, W., Wei, Y ., Huang, W., and Huang, J. Self-supervised graph transformer on large-scale
molecular data. Advances in Neural Information Processing Systems , 33:12559–12571, 2020.
Ruhnau, B. Eigenvector-centrality—a node-centrality? Social networks , 22(4):357–365, 2000.
Rusch, T. K., Bronstein, M. M., and Mishra, S. A survey on oversmoothing in graph neural networks. arXiv preprint
arXiv:2303.10993 , 2023.
Sato, R., Yamada, M., and Kashima, H. Random features strengthen graph neural networks. In Proceedings of the 2021
SIAM international conference on data mining (SDM) , pp. 333–341. SIAM, 2021.
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Sch ¨arli, N., and Zhou, D. Large language models can be
easily distracted by irrelevant context. In International Conference on Machine Learning , pp. 31210–31227. PMLR,
2023.
Shirzad, H., Velingker, A., Venkatachalam, B., Sutherland, D. J., and Sinop, A. K. Exphormer: Sparse transformers for
graphs. arXiv preprint arXiv:2303.06147 , 2023.
Shiv, V . and Quirk, C. Novel positional encodings to enable tree-based transformers. Advances in neural information
processing systems , 32, 2019.
Song, Y ., Huang, S., Cai, J., Wang, X., Zhou, C., and Lin, Z. S4g: Breaking the bottleneck on graphs with structured state
spaces, 2024. URL https://openreview.net/forum?id=0Z6lN4GYrO.
Tang, S., Dunnmon, J. A., Liangqiong, Q., Saab, K. K., Baykaner, T., Lee-Messer, C., and Rubin, D. L. Modeling multivariate
biosignals with graph neural networks and structured state space models. In Mortazavi, B. J., Sarker, T., Beam, A., and Ho,
J. C. (eds.), Proceedings of the Conference on Health, Inference, and Learning , volume 209 of Proceedings of Machine
Learning Research , pp. 50–71. PMLR, 22 Jun–24 Jun 2023. URL https://proceedings.mlr.press/v209/tang23a.html.
T¨onshoff, J., Ritzert, M., Rosenbluth, E., and Grohe, M. Where did the gap go? reassessing the long-range graph benchmark.
arXiv preprint arXiv:2309.00367 , 2023a.
T¨onshoff, J., Ritzert, M., Wolf, H., and Grohe, M. Walking out of the weisfeiler leman hierarchy: Graph learning beyond
message passing. Transactions on Machine Learning Research , 2023b. ISSN 2835-8856. URL https://openreview.net/
forum?id=vgXnEyeWVY.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all
you need. Advances in neural information processing systems , 30, 2017.
Veliˇckovi ´c, P., Cucurull, G., Casanova, A., Romero, A., Li `o, P., and Bengio, Y . Graph attention networks. In International
Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Wang, C., Tsepa, O., Ma, J., and Wang, B. Graph-mamba: Towards long-range graph sequence modeling with selective
state spaces. arXiv preprint arXiv:2402.00789 , 2024.
Wang, H., Yin, H., Zhang, M., and Li, P. Equivariant and stable positional encoding for more powerful graph neural networks.
InInternational Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=e95i1IHcWj.
Wang, S. and Xue, B. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying
memory. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https://openreview.net/
forum?id=i0OmcF14Kf.
Wang, Y ., Min, Y ., Shao, E., and Wu, J. Molecular graph contrastive learning with parameterized explainable augmentations.
In2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) , pp. 1558–1563. IEEE, 2021.
Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al.
Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods
in natural language processing: system demonstrations , pp. 38–45, 2020.
16

--- PAGE 17 ---
Graph Mamba
Wu, Y ., Xu, Y ., Zhu, W., Song, G., Lin, Z., Wang, L., and Liu, S. Kdlgt: a linear graph transformer framework via kernel
decomposition approach. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence ,
pp. 2370–2378, 2023.
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., and Philip, S. Y . A comprehensive survey on graph neural networks. IEEE
transactions on neural networks and learning systems , 32(1):4–24, 2020.
Xing, Z., Ye, T., Yang, Y ., Liu, G., and Zhu, L. Segmamba: Long-range sequential modeling mamba for 3d medical image
segmentation. arXiv preprint arXiv:2401.13560 , 2024.
Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? In International Conference on
Learning Representations , 2019. URL https://openreview.net/forum?id=ryGs6iA5Km.
Yang, Y ., Xing, Z., and Zhu, L. Vivim: a video vision mamba for medical video object segmentation. arXiv preprint
arXiv:2401.14168 , 2024.
Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y ., and Liu, T.-Y . Do transformers really perform badly for graph
representation? Advances in Neural Information Processing Systems , 34:28877–28888, 2021.
Yu, Z., Cao, R., Tang, Q., Nie, S., Huang, J., and Wu, S. Order matters: Semantic-aware neural networks for binary code
similarity detection. In Proceedings of the AAAI conference on artificial intelligence , volume 34, pp. 1145–1152, 2020.
Yun, S., Jeong, M., Kim, R., Kang, J., and Kim, H. J. Graph transformer networks. Advances in neural information
processing systems , 32, 2019.
Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L.,
et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems , 33:17283–17297,
2020.
Zhang, B., Feng, G., Du, Y ., He, D., and Wang, L. A complete expressiveness hierarchy for subgraph GNNs via subgraph
weisfeiler-lehman tests. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),
Proceedings of the 40th International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning
Research , pp. 41019–41077. PMLR, 23–29 Jul 2023a. URL https://proceedings.mlr.press/v202/zhang23k.html.
Zhang, M., Saab, K. K., Poli, M., Dao, T., Goel, K., and Re, C. Effectively modeling time series with simple discrete state
spaces. In The Eleventh International Conference on Learning Representations , 2023b. URL https://openreview.net/
forum?id=2EpjkjzdCAa.
Zhang, Z., Liu, Q., Hu, Q., and Lee, C.-K. Hierarchical graph transformer with adaptive node sampling. Advances in Neural
Information Processing Systems , 35:21171–21183, 2022.
Zhao, J., Li, C., Wen, Q., Wang, Y ., Liu, Y ., Sun, H., Xie, X., and Ye, Y . Gophormer: Ego-graph transformer for node
classification. arXiv preprint arXiv:2110.13094 , 2021.
Zhong, W., He, C., Xiao, C., Liu, Y ., Qin, X., and Yu, Z. Long-distance dependency combined multi-hop graph neural
networks for protein–protein interactions prediction. BMC bioinformatics , 23(1):1–21, 2022.
Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., and Wang, X. Vision mamba: Efficient visual representation learning with
bidirectional state space model. arXiv preprint arXiv:2401.09417 , 2024.
17

--- PAGE 18 ---
Graph Mamba
Table 5. Dataset Statistics.
Dataset #Graphs Average #Nodes Average #Edges #ClassSetupMetric
Input Level Task
Long-range Graph Benchmark (Dwivedi et al., 2022)
COCO-SP 123,286 476.9 2693.7 81 Node Classification F1 score
PascalVOC-SP 11,355 479.4 2710.5 21 Node Classification F1 score
Peptides-Func 15,535 150.9 307.3 10 Graph Classification Average Precision
Peptides-Struct 15,535 150.9 307.3 11 (regression) Graph Regression Mean Absolute Error
GNN Benchmark (Dwivedi et al., 2023)
MNIST 70,000 70.6 564.5 10 Graph Classification Accuracy
CIFAR10 60,000 117.6 941.1 10 Graph Classification Accuracy
Pattern 14,000 118.9 3,039.3 2 Node Classification Accuracy
MalNet-Tiny 5,000 1,410.3 2,859.9 5 Graph Classification Accuracy
Heterophilic Benchmark (Platonov et al., 2023)
Roman-empire 1 22,662 32,927 18 Node Classification Accuracy
Amazon-ratings 1 24,492 93,050 5 Node Classification Accuracy
Minesweeper 1 10,000 39,402 2 Node Classification ROC AUC
Tolokers 1 11,758 519,000 2 Node Classification ROC AUC
Very Large Dataset (Hu et al., 2020)
OGBN-Arxiv 1 169,343 1,166,243 40 Node Classification Accuracy
A. Details of Datasets
The statistics of all the datasets are reported in Table 5. For additional details about the datasets, we refer to the Long-range
graph benchmark (Dwivedi et al., 2022), GNN Benchmark (Dwivedi et al., 2023), Heterophilic Benchmark (Platonov et al.,
2023), and Open Graph Benchmark (Hu et al., 2020).
B. Experimental Setup
B.1. Hyperparameters
We use grid search to tune hyperparameters and the search space is reported in Table 6. Following previous studies, we use
the same split of traning/test/validation as (Ramp ´aˇsek et al., 2022). We report the results over the 4 random seeds. Also, for
the baselines’ results (in Tables 1, 2, and 3), we have re-used and reported the benchmark results in the work by Shirzad
et al. (2023); Deng et al. (2024); T ¨onshoff et al. (2023a) and Wang et al. (2024).2.
C. Details of GMN Architecture: Algorithms
Algorithm 1 shows the forward pass of the Graph Mamba Network with one layer. For each node, GMN first samples M
walks with length ˆm= 1, . . . , m and constructs its corresponding tokens, each of which as the induced subgraph of M
walks with length ˆm. We repeat this process stimes to have longer sequence and more samples from each hierarchy of the
neighborhoods. This part of the algorithm, can be computed before the training process and in CPU. Next, GMNs for each
node encode its tokens using an encoder ϕ(.), which can be an MPNN (e.g., gated-GCN (Bresson & Laurent, 2017)) or
RWF encoding (proposed by T ¨onshoff et al. (2023b)). We then pass the encodings to a Bidirectional Mamba block, which
we describe in Algorithm 2 (This algorithm is simple two Mamba block (Gu & Dao, 2023) such that we use one of the
backward or forward ordering of inputs for each of them). At the end of line 15, we have the node encodings obtained from
subgraph tokenization. Next, we treat each node as a token and pass the encoding to another bidirectional Mamba, with a
specific order. We have used degree ordering in our experiments, but there are some other approaches that we have discussed
in the main paper.
2In the previous version of this preprint (Feb 13, 2024), the reported results of Exphormer (Shirzad et al., 2023) and GPS (Ramp ´aˇsek
et al., 2022) came from the results of Wang et al. (2024).
18

--- PAGE 19 ---
Graph Mamba
Table 6. Search space of hyperparameters for each dataset†.
Dataset M s #Layers Max # Epochs Learning Rate
Long-range Graph Benchmark (Dwivedi et al., 2022)
COCO-SP {1,2,4,8,16,32} { 0,1,2,4,8,16} { 4,5} 300 0.001
PascalVOC-SP {1,2,4,8,16,32} { 0,1,2,4,8,16} { 4,5} 300 0.001
Peptides-Func {1,2,4,8,16,32} { 0,1,2,4,8,16} { 4,5} 300 0.001
Peptides-Struct {1,2,4,8,16,32} { 0,1,2,4,8,16} { 4,5} 300 0.001
GNN Benchmark (Dwivedi et al., 2023)
MNIST {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
CIFAR10 {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
Pattern {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
MalNet-Tiny {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
Heterophilic Benchmark (Platonov et al., 2023)
Roman-empire {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
Amazon-ratings {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
Minesweeper {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
Tolokers {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
Very Large Dataset (Hu et al., 2020)
OGBN-Arxiv {1,2,4,8,16,32} { 0,1,2,4,8,16} { 3,4,6} 300 0.001
†This space is not fully searched and preliminary results are reported based on its subspace. We will update the results accordingly.
D. Additional Experimental Results
D.1. Parameter Sensitivity
The effect of M.Parameter Mis the number of walks that we aggregate to construct a subgraph token. To evaluate its effect
on the performance of the GMN, we use two datasets of Roman-empire and PascalVOC-SP, from two different benchmarks,
and vary the value of Mfrom 1 to 10. The results are reported in Figure 4 (Left). These results show that performance
peaks at certain value of Mand the exact value varies with the dataset. The main reason is, parameter Mdetermines that
how many walks can be a good representative of the neighborhood of a node, and so depends on the density, homophily
score, and network topology this value can be different.
The effect of m.Similar to the above, we use two datasets of Roman-empire and PascalVOC-SP, from two different
benchmarks, and vary the value of mfrom 1 to 60. The results are reported in Figure 4 (Middle). The performance is
non-decreasing with respect to the value of m. That is, increasing the value of m, i.e., considering far neighbors in the
tokenization process, does not damage the performance (might lead to better results). Intuitively, using large values of mis
expected to damage the performance due to the over-smoothing and over-squashing; however, the selection mechanism
in Bidirectional Mamba can select informative tokens (i.e., neighborhood), filtering information that cause performance
damage.
The effect of s.Using the same setting as the above, we report the results when varying the value of sin Figure 4 (Right).
Result show that increasing the value of scan monotonically improve the performance. As discussed earlier, longer
sequences of tokens can provide more context for our model and due to the selection mechanism in Mamba (Gu & Dao,
2023), GMNs can select informative subgraphs/nodes and filter irrelevant tokens, resulting in better results with longer
sequences.
D.2. Comparison with GRED (Ding et al., 2023) and S4G (Song et al., 2024)
GRED (Ding et al., 2023) is a recent work on ArXiv that uses an RNN on the set of neighbors with distance k= 1, . . . , K
to a node of interest for the node classification task. S4G (Song et al., 2024) is a recent unpublished (without preprint but
available on Openreview) work that uses the same approach as GRED but replaces the RNN block with a structured SSM.
Since the code or models of GRED and S4G are not available, for the comparison of GMNs, S4G, and GRED, we run
GMNs on the datasets used in the original papers (Ding et al., 2023; Song et al., 2024). The results are reported in Table 7.
GMNs consistently outperforms GRED (Ding et al., 2023) in all datasets and outperforms S4G in 4 out of 5 datasets. The
19

--- PAGE 20 ---
Graph Mamba
Algorithm 1 Graph Mamba Networks (with one layer)
Input: A graph G= (V, E), input features X∈Rn×d, ordered array of nodes V={v1, . . . , v n}, and hyperparameters M, m , ands.
Optional: Matrix P, whose rows are positional/structural encodings correspond to nodes, and/or a MPNN model Ψ(.).
Output: The updated node encodings Xnew.
1:forv∈Vdo ▷This block can be done before the training.
2: forˆm= 0, . . . , m do
3: forˆs= 1, . . . , s do
4: Tˆs
ˆm(v)← ∅;
5: forˆM= 1, . . . , M do
6: W←Sample a random walk with length ˆmstarting from v;
7: Tˆs
ˆm(v)←Tˆs
ˆm(v)∪ {u|u∈W};
8:
9:▷Start the training:
10: Initialize all learnable parameters;
11:forv∈Vdo
12: forj= 1, . . . , s do
13: fori= 1, . . . , m do
14: x(i−1)s+j
v ←ϕ
G[Tj
i(v)],XTj
i(v)||PTj
i(v)
;
Φv← ∥sm
i=1xi
v; ▷Φvis a matrix whose rows are xi
v.
15: youtput(v)←BiMamba (Φv); ▷Using Algorithm 2.
16:▷Each node is a token:
17:Y← ∥sm
i=1youtput(v); ▷yis a matrix whose rows are youtput(v).
18:Youtput←BiMamba (Y) + Ψ ( G,X∥P);
1246810
M0.40.60.8F1/ACC
Roman-empire
PascalVOC-SP
15102030405060
m0.40.60.8F1/ACC
Roman-empire
PascalVOC-SP
5 10 15
s0.40.60.8F1/ACC
Roman-empire
PascalVOC-SP
Figure 4. The effect of (Left) M, (Middle) m, and (Right) son the performance of GMNs.
reason is two folds: (1) GMNs use sampled walks instead of all the nodes within k-distance neighborhood. As discussed
in Theorem 4.1, this approach with large enough length and samples is more expressive than considering all nodes within
the neighborhood. (2) S4G and GRED use simple RNN and SSM to aggregate the information about all the different
neighborhoods of a node, while GMNs use Mamba, which have a selection mechanism. This selection mechanism help the
model to choose neighborhoods that are more informative and important than others. (3) GRED and S4G are solely based
on distance encoding, meaning that they miss the connections between nodes in k-distance and (k+ 1) -distance. Figure 5
shows a failure example of these methods that solely are based on distance of nodes. To obtain the node encoding of node A,
these two methods group nodes wit respect to their distance to A, either d= 1,2,and3. In Figure 5, while these two graphs
are non-isomorphism, the output of this step for both graphs are the same, meaning that these methods obtain the same node
encoding for A.
E. Complexity Analysis of GNMs
m≥1.For each node v∈V, we generate M×swalks with length ˆm= 1, . . . , m , which requires O(M×s×(m+ 1))
time. Given Ktokens, the complexity of bidirectional Mamba is 2×of Mamba (Gu & Dao, 2023), which is linear
with respect to K. Accordingly, since we have O(M×s×m)tokens, the final complexity for a given node v∈Vis
O(M×s×(m+ 1)) . Repeating the process for all nodes, the time complexity is O(M×s×(m+ 1)× |V|+|E|),
which is linear in terms of |V|and|E|(graph size). To compare to the quadratic time complexity of GTs, even for small
20

--- PAGE 21 ---
Graph Mamba
Algorithm 2 Bidirectional Mamba
Input: A sequence Φ(Ordered matrix, where each row is a token).
Output: The updated sequence encodings Φ.
1:▷Forward Scan:
2:Φf=σ(Conv (Winput,fLayerNorm (Φ)));
3:Bf=WBfΦf;
4:Cf=WCfΦf;
5:∆f=Softplus (W∆fΦf);
6:¯A=Discrete A(A,∆);
7:¯Bf=Discrete Bf(Bf,∆);
8:yf=SSM ¯A,¯Bf,Cf(Φf);
9:Yf=Wf,1(yf⊙σ(Wf,2LayerNorm (Φ)));
10:▷Backward Scan:
11:Φ←Reverse-rows (Φ); ▷Reverse the order of rows in the matrix.
12:Φb=σ(Conv (Winput, b LayerNorm (Φ)));
13:Bb=WBbΦb;
14:Cb=WCbΦb;
15:∆b=Softplus (W∆bΦb);
16:¯A=Discrete A(A,∆);
17:¯Bb=Discrete Bb(Bb,∆);
18:yb=SSM ¯A,¯Bb,Cb(Φb);
19:Yb=Wb,1(yb⊙σ(Wb,2LayerNorm (Φ)));
20:▷Output:
21:youtput←Wout(Yf+Reverse-row (Yb));
22:return youtput;
Figure 5. Failure example for methods that are solely based on distance encoding. Solely considering the set of nodes in different distances
to the target node misses the connections between them. While the structure of these two graphs are different, the set of nodes with
the same distance to node Aare the same. Accordingly, GRED (Ding et al., 2023) and S4G (Song et al., 2024) achieve the same node
encoding for A, missing A’s neighborhood topology.
networks, note that in practice, M×s×(m+ 1)≪ |V|, and in our experiments usually M×s×(m+ 1)≤200. Also,
note that using MPNN as an optional step cannot affect the time complexity as the MPNN requires O(|V|+|E|)time.
m= 0.In this case, each node is a token and so the GMN requires O(|V|)time. Using MPNN in the architecture, the time
complexity would be O(|V|+|E|), dominating by the MPNN time complexity.
As discussed above, based on the properties of Mamba architecture, longer sequence of tokens (larger value of s≥1) can
improve the performance of the method. Based on the abovementioned time complexity when m≥1, we can see that there
is a trade-off between time complexity and the performance of the model. That is, while larger sresult in better performance,
it results in slower model.
F. Discussion on a Concurrent Work
(Wang et al., 2024), in work concurrent to and independent of ours, replace Transformer architecture (attention block) with a
Mamba block (Gu & Dao, 2023) in GPS framework (Ramp ´aˇsek et al., 2022). Next, we discuss these two works in different
aspects:
21

--- PAGE 22 ---
Graph Mamba
Table 7. Comparison with GRED and S4G Models. Highlighted are the top first andsecond results.
ModelMNIST CIFAR10 PATTERN Peptides-func Peptides-struct
Accuracy ↑ Accuracy ↑ Accuracy ↑ AP↑ MAE↓
S4G (Song et al., 2024)†0.9637±0.0017 0.7065±0.0033 0.8687±0.0002 0.7293±0.0004 0.2485±0.0017
GRED (Ding et al., 2023)‡0.9822±0.0095 0.7537±0.6210 0.8676±0.0200 0.7041±0.0049 0.2503±0.0019
GMN (Ours) 0.9839±0.0018 0.7576±0.0042 0.8714±0.0012 0.7071±0.0083 0.2473±0.0025
†Results are reported by Song et al. (2024).
‡Results are reported by Ding et al. (2023).
Figure 6. (A) An example of node tokenization and its information flow. Even using PE/SE, nodes at the beginning of the sequence do not
have any information about the structure of the graph! (B) Potential failure example for using a simple one directional sequential encoder
when each token is a node. Nodes in the right hand side do not have any information about the structure of the graph in the left hand side
(Or vise versa depends on the direction of the order).
Architecture Design. As mentioned above, the GMB model (Wang et al., 2024) replaces the attention module in GPS
framework (Ramp ´aˇsek et al., 2022) with Mamba block (Gu & Dao, 2023). Accordingly, it treats each node as a token, uses
PE/SE as initial feature vectors, and ordered nodes based on their degree. Since this approach is based on node tokenization
and uses one directional Mamba (Gu & Dao, 2023), it suffers from the limitations of GTs with node tokenization mentioned
in§3. More specifically: (1) Although it has more ability to learn long-range dependencies, this approach lacks inductive
bias about the graph structure and requires complex PE/SE. (2) Using a simple one directional Mamba causes the lack of
inductive bias about some structures in the graph. Figure 6 provides an example of this information loss. In part (A), we
show an example of node tokenization and node ordering with respect to their degrees. Based on the information flow, using
a one directional Mamba, nodes with high-degree do not have any information about the structure of the graph. For example,
in Figure 6 (B), even with using complex PE/SE, nodes in the right hand side do not have any information about the global
information in the left hand side, due to the one directional information flow of Mamba block. As discussed earlier in the
paper, this is one of the new challenges of using SSMs (compare to GTs). The main reason is, attentions in Transformers
consider all nodes connected and so the information could pass between each pair of nodes. In sequential encoders (even
with selection mechanism), however, each token has the information about its previous tokens. Our neighborhood sampling
and its reverse ordering can address this issue due to its implicit order of neighborhood hierarchy.
Comparing to GMNs, GMB can be seen as a special case of GMNs when we use m= 0and replace bidirectional Mamba
block with a one directional Mamba block. Our approach using parameter mprovides the flexibility of using either node or
subgraph tokenization, whenever either inductive bias or long-range dependencies is more important to the task and the
dataset. Furthermore, having these two special traits of GMNs compared to GMB results in provable expressive power of
GMNs, which we discuss in the following.
Expressive Power. As discussed earlier, due to the lack of inductive bias, GMB method requires complex PE/SE to learn
about the structure of the graph. While GMNs without PE/SE and MPNNs has unbounded expressive power with respect to
22

--- PAGE 23 ---
Graph Mamba
isomorphic test (Theorem 4.4), GMB cannot distinguish graphs with the same sequence of degrees and its expressive power
is bounded by the expressive power of its MPNN. That is, let G1andG2be two graphs with the same sequence of node
degrees, Mamba block in GMB, in the worst case of having the same node feature vectors, cannot distinguish G1andG2
since its input is the same for both of these graphs. Accordingly, the expressive power of GMB is bounded by the expressive
power of its MPNN in the GPS framework (Ramp ´aˇsek et al., 2022).
G. Theoretical Analysis of GMNs
Theorem G.1. With large enough M, m, ands >0, GMNs’ neighborhood sampling is strictly more expressive than k-hop
neighborhood sampling.
Proof. We first show that in this condition, the random walk neighborhood sampling is as expressive as k-hop neighborhood
sampling. To this end, given an arbitrary small ϵ >0, we show that the probability that k-hop neighborhood sampling is
more expressive than random walk neighborhood sampling is less than ϵ. Letm=k,s= 1, andpu,vbe the probability that
we sample node vin a walk with length m=kstarting from node u. This prbobality is zero if the shortest path of uandv
is more than k. To construct the subgraph token corresponds to ˆm=k, we use Msamples and so the probability of not
seeing node vin these samples is qu,v,M = (1−pu,v)M≤1. Now let M→ ∞ andv∈ Nk(u)(i.e.,pu,v̸= 0), we have
limM→∞qu,v,M = 0. Accordingly, with large enough M, we have qu,v,M≤ϵ. This means that with a large enough M
when m=kands= 1, we sample all the nodes within the k-hop neighborhood, meaning that random walk neighborhood
sampling at least provide as much information as k-hop neighborhood sampling with arbitrary large probability.
Next, we provide an example that k-hop neighborhood sampling is not able to distinguish two non-isomorphism graphs,
while random walk sampling can. Let S={v1, v2, . . . , v ℓ}be a set of nodes such that all nodes have shortest path less
thanktou. Using hyperparamters m=kand arbitrary M, let the probability that we get G[S]as the subgraph token be
1> qS>0. Using ssamples, the probability that we do not have G[S]as one of the subgraph tokens is (1−qS)s. Now
using large s→ ∞ , we have lims→∞(1−qS)s= 0and so for any arbitrary ϵ >0there is a large s >0such that we see
all non-empty subgraphs of the k-hop neighborhood with probability more than 1−ϵ, which is more powerful than the
neighborhood itself.
Note that the above proof does not necessarily guarantee an efficient sampling, but it guarantees the expressive power.
Theorem G.2 (Universality) .Let1≤p <∞, and ϵ >0. For any continues function f: [0,1]d×n→Rd×nthat is
permutation equivariant, there exists a GMN with positional encoding, gp, such that ℓp(f, g)< ϵ.
Proof. Recently, Wang & Xue (2023) show that SSMs with layer-wise nonlinearity are universal approximators of any
sequence-to-sequence function. We let m= 0, meaning we use node tokenization. Using the universality of SSMs for
sequence-to-sequence function, the rest of the proof is the same as Kreuzer et al. (2021), where they use the padded
adjacency matrix of Gas a positional encoding to prove the same theorem for Transformers. In fact, the universality for
sequence-to-sequence functions is enough to show the universality on graphs with a strong positional encoding.
Theorem G.3 (Expressive Power w/ PE) .Given the full set of eigenfunctions and enough parameters, GMNs can distinguish
any pair of non-isomorphic graphs and are more powerful than any WL test.
Proof. Due to the universality of GMNs in Theorem G.3, one can use a GMN with the padded adjacency matrix of Gas
positional encoding and learn a function that is invariant under node index permutations and maps non-isomorphic graphs to
different values.
Theorem G.4 (Expressive Power w/o PE and MPNN) .With enough parameter, for every k≥1there are graphs that are
distinguishable by GMNs, but not by k-WL test, showing that their expressivity power is not bounded by any WL test.
Proof. The proof of this theorem directly comes from the recent work of T ¨onshoff et al. (2023b), where they prove a similar
theorem for CRaWl (T ¨onshoff et al., 2023b). That is, using RWF as ϕ(.)in GMNs (without MPNN), makes the GMNs as
powerful as CRaWl. The reason is CRaWl uses 1-d CNN on top of RWF, while GMNs use Bidirectional Mamba block on
top of RWF: using a broadcast SMM, this block becomes similar to 1-d CNN.
23
