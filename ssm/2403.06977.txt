# 2403.06977.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2403.06977.pdf
# File size: 2662745 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
VideoMamba: State Space Model for Efficient
Video Understanding
Kunchang Li2,3,1♠, Xinhao Li4,1♠, Yi Wang1♡, Yinan He1
Yali Wang2,1♡, Limin Wang4,1♡, and Yu Qiao1♡
1OpenGVLab, Shanghai AI Laboratory
2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences
3University of Chinese Academy of Sciences
4State Key Laboratory for Novel Software Technology, Nanjing University
https://github.com/OpenGVLab/VideoMamba
Abstract. Addressing the dual challenges of local redundancy and
globaldependenciesinvideounderstanding,thisworkinnovativelyadapts
the Mamba to the video domain. The proposed VideoMamba overcomes
the limitations of existing 3D convolution neural networks and video
transformers. Its linear-complexity operator enables efficient long-term
modeling, which is crucial for high-resolution long video understanding.
Extensive evaluations reveal VideoMamba’s four core abilities: (1) Scala-
bilityin the visual domain without extensive dataset pretraining, thanks
to a novel self-distillation technique; (2) Sensitivity for recognizing short-
term actions even with fine-grained motion differences; (3) Superiority
in long-term video understanding, showcasing significant advancements
over traditional feature-based models; and (4) Compatibility with other
modalities, demonstrating robustness in multi-modal contexts. Through
these distinct advantages, VideoMamba sets a new benchmark for video
understanding, offering a scalable and efficient solution for comprehen-
sive video understanding. All the code and models are available.
1 Introduction
The core objective for video understanding lies in mastering spatiotemporal rep-
resentations, which inherently presents two formidable challenges: the large spa-
tiotemporal redundancy within short video clips, and the complex spatiotempo-
ral dependencies among long contexts. Although the once-dominant 3D convolu-
tional neural networks (CNNs) [9,19,76] and video transformers [2,4], effectively
tackle one of the challenges mentioned by leveraging either local convolution
or long-range attention, they fall short in addressing both simultaneously. Uni-
Former [44] attempts to integrate the advantages of both methods, but it strug-
gleswithmodelinglongvideos,whichhasbeenthemajortrendinrecentresearch
on video understanding [48,72] and generation [5,92].
The emergence of low-cost operators such as S4 [26], RWKV [73], and Ret-
Net [70] in the NLP domain, has carved a novel pathway for the vision model.
♠Interns at Shanghai AI Laboratory. ♡Corresponding authors.arXiv:2403.06977v2  [cs.CV]  12 Mar 2024

--- PAGE 2 ---
2 K. Li et al.
+/00/$
!"&10/$
⚡2ℎ/&4/$
#>>?>>?+4.1%+4.6%2×6×20×40×&=8&=64
Fig.1:Comparisonsofthroughputandmemory. TheTimeSformer-Ti[4]isbuilt
based on DeiT-Ti [75] with joint spatiotemporal attention. All the input frames are
sized to 224 ×224. The testing is conducted on an NVIDIA A100-80G GPU, utilizing
PyTorch 2.1 and CUDA 11.8, with a batch size of 128. Our VideoMamba is better,
faster and cheaper for both short-term and long-term video understanding.
Mamba [25] stands out with its selective state space model (SSM), striking a bal-
ance between maintaining linear complexity and facilitating long-term dynamic
modeling. This innovation has spurred its adoption in vision tasks, as evidenced
by Vision Mamba [91] and VMamba [50], which leverage multi-directional SSMs
for enhanced 2D image processing. These models rival attention-based archi-
tectures in performance while offering a significant reduction in memory usage.
Given the inherently longer sequences produced by video, a natural question
arises: Can Mamba work well for video understanding?
Inspired by this, we introduce VideoMamba, a purely SSM-based model tai-
lored for video understanding. VideoMamba harmoniously merges the strengths
ofconvolutionandattentioninvanillaViT[15]style.Itoffersalinear-complexity
method for dynamic spatiotemporal context modeling, ideal for high-resolution
long videos. The related evaluation focuses on VideoMamba’s four key abilities:
(1)Scalability in the Visual Domain :We examine VideoMamba’s scal-
ability and find that, while the pure Mamba model tends to overfit as it scales,
our introduction of a simple yet effective self-distillation strategy allows Video-
Mambatoachieveremarkableperformanceenhancementsasthemodelandinput
sizes increase, without the need for large-scale dataset pretraining.
(2)Sensitivity for Short-term Action Recognition :Our analysis ex-
tends to assessing VideoMamba’s capability to accurately distinguish short-term
actions, especially those with fine-grained motion differences, e.g., opening and
closing. The findings reveal VideoMamba’s superior performance over existing
attention-based models [2,4,52]. More importantly, it is also suitable for masked
modeling, which further enhances its temporal sensitivity.
(3)Superiority in Long-term Video Understanding :We then assess
VideoMamba’s prowess in interpreting long videos. It showcases remarkable su-
periority over conventional feature-based methods [35,47] through end-to-end
training. Notably, VideoMamba operates 6 ×faster than TimeSformer [4] and
demands 40 ×less GPU memory for 64-frame videos (see Fig. 1).
(4)Compatibility with Other Modalities :Lastly,weassessVideoMamba’s
adaptability with other modalities. Results in video-text retrievals show its im-

--- PAGE 3 ---
VideoMamba 3
proved performance than ViT, particularly in long videos with complex scenar-
ios. This underscores its robustness and multi-modal integration capacity.
In conclusion, our in-depth experiments reveal VideoMamba’s immense po-
tential in understanding both short-term (K400 [36] and SthSthV2 [24]) and
long-term (Breakfast [37], COIN [71], and LVU [84]) video contents. Given its
efficiency and effectiveness, VideoMamba is poised to become a cornerstone in
therealmoflong-videocomprehension.Allthecodeandmodelsareopen-sourced
to foster future research endeavors.
2 Related Works
2.1 State Space Models
Recently, the State Space Models (SSMs) have shown significant effectiveness
of state space transformation in capturing the dynamics and dependencies of
language sequences. [26] introduces a structured state-space sequence model
(S4), specifically designed to model long-range dependencies, boasting the ad-
vantage of linear complexity. Based on it, various models have been developed
(e.g., S5 [66], H3 [20] and GSS [56]), and Mamba [25] distinguishes itself by
introducing a data-dependent SSM layer and a selection mechanism using par-
allel scan (S6). Compared to transformers [6,54] based on quadratic-complexity
attention, Mamba excels at processing long sequences with linear complexity.
In the vision domain, [26] first applies SSM in pixel-level image classifica-
tion, and [35] uses S4 to handle the long-range temporal dependencies for movie
clip classification. Besides, the great potential of Mamba motivates a series of
works [28,30,46,50,78,87,91], which demonstrates Mamba’s better performances
and higher GPU efficiency than Transformer on visual downstream tasks like ob-
ject detection and semantic segmentation. Different from the previous works, our
VideoMambaisapurelySSM-basedvideomodel,showcasinggreatefficiencyand
effectiveness for both short-term and long-term video understanding.
2.2 Video Understanding
Video understanding stands as a cornerstone in the domain of computer vision,
whose significance is further amplified by the burgeoning growth of short video
platforms. To bolster this field, numerous datasets equipped with extensive data
and meticulous human annotations have been developed, aiming to enhance
human action recognition capabilities. Notable examples include UCF101 [67]
and Kinetics dataset [7,8,36], which have played pivotal roles in benchmarking
progress. Furthermore, other datasets [22,27,31,34,49,62] provide annotated
activity videos tailored for action localization, fostering deeper research into
human activities. Beyond action recognition, the advent of large-scale video-
text datasets [10,12,57,82,86,88] extends the utility of video understanding
into the realm of multi-modality tasks, such as video captioning, retrieval and
question answering, thereby broadening the application spectrum.

--- PAGE 4 ---
4 K. Li et al.
As for the architecture, it has evolved from using CNN which extracts fea-
tures from video frames, to more advanced techniques. Initially, 3D CNNs [9,17,
76,77] expanded the traditional 2D CNN architecture to capture videos’ spatio-
temporal information. Two-Stream [65], which combines spatial and temporal
streams,TSN[80],whichproposessparsesampling,andSlowFast[19],whichuses
parallel networks to capture semantics and rapid movements, further enhance
action recognition capacity. The introduction of attention-based models [2,4,59,
63,89], like TimeSformer [4] and ViViT [2], marked a significant advancement by
effectively capturing long-range dependencies within video sequences, enhancing
temporal relationship understanding. Recent developments [42,44,52,83] have
focused on accurate video transformer, with innovations like the VideoSwin’s
window attention [52] and the UniFormer’s integration of convolution and self-
attention mechanisms [44], aiming to balance computational efficiency with per-
formance. Despite these models’ achievements in various tasks, they often come
with high computational costs for long sequences. In contrast, our VideoMamba
introduces a linear-complexity operator for efficient long-term modeling, outper-
forming existing methods with faster speed and lower GPU consumption.
3 Method
3.1 Preliminaries
SSM for 1D sequence. State Space Models (SSMs) are conceptualized based
on continuous systems that map a 1D function or sequence, x(t)∈RL→y(t)∈
RLthrough a hidden state h(t)∈RN. Formally, SSMs employ the following
ordinary differential equation (ODE) to model the input data:
h′(t) =Ah(t) +Bx(t), (1)
y(t) =Ch(t), (2)
where A∈RN×Nrepresents the system’s evolution matrix, and B∈RN×1,C∈
RN×1aretheprojectionmatrices.ThiscontinuousODEisapproximatedthrough
discretization in modern SSMs. Mamba [25] is one of the discrete versions of the
continuous system, which includes a timescale parameter ∆to transform the
continuous parameters A,Bto their discrete counterparts A,B. The transfor-
mation typically employs the zero-order hold (ZOH) method, defined by:
A= exp( ∆A), (3)
B= (∆A)−1(exp(∆A)−I)·∆B (4)
ht=Aht−1+Bxt, (5)
yt=Cht. (6)
Contrary to traditional models that primarily rely on linear time-invariant
SSMs, Mamba distinguishes itself by implementing a Selective Scan Mechanism
(S6) as its core SSM operator. Within S6, the parameters B∈RB×L×N,C∈

--- PAGE 5 ---
VideoMamba 5
55ConvConv&.+/5SSMSSM5Conv5SSM&.+/LinearProjectionSequenceTransformationMultiplicationSummarizationActivation!
!7!81!1;'-'.#9)'*+!$7!81!
Fig.2: Mamba blocks for 1D [25] and 2D [91] sequence. We omit the initial
normalization and the final residual for simplification.
Bidirectional	Mamba	Block×LClassification	Head
3D	Patch	Embedding1234123412340*TemporalPositionEmbeddingSpatialPositionEmbedding
/!/"/#CLS432112341234CLS43211234123434567589:7;<7:=67589:7;
[CLS]	Token123!6'-#*7!81!1"4!)'*)#84*.!$"9!+Class$%&''()*+&%,-()*./&0()*…
Fig.3: Framework of VideoMamba. We strictly follow the architecture of vanilla
ViT [15], and adapt the bidirectional mamba block [91] for 3D video sequences.
RB×L×N, and ∆∈RB×L×Dare directly derived from the input data x∈
RB×L×D, indicating an intrinsic capacity for contextual sensitivity and adaptive
weight modulation. Fig. 2a shows the details of the Mamba block.
Bidirectional SSM for Vision. The original Mamba block, designed for 1D
sequences, falls short for visual tasks requiring spatial awareness. Building on
this, Vision Mamba introduces a bidirectional Mamba (B-Mamba) block in Fig.
2b, which adapts bidirectional sequence modeling for vision-specific applications.
This block processes flattened visual sequences through simultaneous forward
and backward SSMs, enhancing its capacity for spatially-aware processing. In
this work, we extend the B-Mamba block for 3D video understanding.
3.2 VideoMamba
Overview. Fig. 3 illustrates the overall framework of VideoMamba. Specifi-
cally, we first use 3D convolution ( i.e., 1×16×16) to project the input videos
Xv∈R3×T×H×WintoLnon-overlapping spatiotemporal patches Xp∈RL×C,
where L=t×h×w(t=T,h=H
16, and w=W
16). The sequence of tokens input to the
following VideoMamba encoder is
X= [Xcls,X] +ps+pt, (7)
where Xclsis a learnable classification token that is prepended to the start of
the sequence. Following previous works [2,4,15], we added a learnable spatial
position embedding ps∈R(hw+1)×Cand the extra temporal one pt∈Rt×C
to retain the spatiotemporal position information, since the SSM modeling is

--- PAGE 6 ---
6 K. Li et al.
STSTSTST
!"4!)'!$−:'.();'-'.#9)'*+!$9"4!)'*)#84*.!$;'-'.#9)'*+!$,1-"4!)'*)#84*.!$;'-'.#9)'*+!$,21=#84*.!$−:'.();'-'.#9)'*+!$
Fig.4: Different scan methods. We omit the [CLS] token for simplification.
sensitive to token position. The tokens Xare then passed through by Lstacked
B-Mamba blocks, and the representation of [CLS]token at the final layer is
processed by normalization and linear layer for classification.
Spatiotemporal Scan. To apply the B-Mamba layer for spatiotemporal input,
we extend the original 2D scan into different bidirectional 3D scans in Fig. 4: (a)
Spatial-First , organizing spatial tokens by location then stacking them frame by
frame; (b) Temporal-First , arranging temporal tokens based on the frame then
stacks along the spatial dimension; (c) Spatiotemporal , a hybrid of both Spatial-
FirstandTemporal-First ,withv1conductinghalfofthemandv2conductingfull
of them ( 2×computation). Moreover, our experiments in Fig. 7a demonstrate
that the Spatial-First bidirectional scan is the most effective yet simple. Thanks
to the linear complexity of Mamba, our VideoMamba is capable of handling long
videos of high resolution efficiently.
Comparison to Vim [91] and VMamba [50]. Our VideoMamba builds upon
Vim, yet streamlines its architecture by omitting features such as the middle
[CLS] token and Rotary Position Embedding (RoPE [68]), resulting in superior
performance on ImageNet-1K with gains of +0.8%and+0.7%for Vim-Ti and
Vim-S, respectively. Unlike VMamba, which incorporates additional depthwise
convolution, VideoMamba strictly follows the ViT design without downsam-
pling layers. To counter the overfitting issues observed in VMamba, we introduce
an effective self-distillation technique outlined in Section 3.3, demonstrate the
isotropic VideoMamba’s great scalability for image and video tasks.
Comparison to TimeSformer [4] and ViViT [2]. Traditional attention-
based models like TimeSformer and ViViT have addressed the self-attention
mechanism’s quadratic complexity by adopting divided spatiotemporal atten-
tion.Despitebeingmoreefficient,itintroducesadditionalparametersandunder-
performs compared to joint attention, particularly in scenarios involving masked
pretraining [43,74]. In contrast, VideoMamba processes spatiotemporal tokens
with linear complexity, outperforming TimeSformer on Kinetics-400 by +2.6%
and making significant strides on SthSthV2 with a +5.9% improvement (see
Table 3 and 4). Furthermore, VideoMamba achieves a 6×increase in processing
speed and requires 40×less GPU memory for long videos, as detailed in Fig. 1,
demonstrating its efficiency and effectiveness in handling long-video tasks.

--- PAGE 7 ---
VideoMamba 7
*+,-!'+4@),'-#*
*+,-1.!+-*88!(A'+/
*+,-9)@1#8!(A'+/
*+,--9$'4−.*B8!(A'+/
*+,-#%.!8#−.*B8!(A'+/
*+,-%!))#+)'*+8!(A'+/
Fig.5: Different masking strategies. Row masking, tailored for VideoMamba in
light of the 1D convolution preceding SSM, enhances performance with continuous
tokens. The difference between clip-row and frame-row masking is that the former
masks the entire video clip, while the latter masks each frame individually.
3.3 Architecture
Model #Depth #Dim #Param.
Tiny 24 192 7M
Small 24 384 26M
Middle 32 576 74M
Base 24 768 98M
Table 1: Different model sizes.
Base model is finally excluded due
to its suboptimization.For SSM in the B-Mamba layer, we adopt the
default hyperparameters as in Mamba [25].
setting the state dimension and expansion
ratio to 16 and 2, respectively. Following
ViT [15], we adjust the depth and embedding
dimensions to create models of comparable
sizes in Table 1, including VideoMamba-Ti,
VideoMamba-S and VideoMamba-M. How-
ever, we observe that larger VideoMamba
tends to overfit during our experiments, leading to suboptimal performance as
illustrated in Fig. 6a. This overfitting issue is not unique to our models but
is also found in VMamba [50], where the optimal performance of VMamba-B
was achieved at three-quarters of the total training epochs. To counteract the
overfitting in larger Mamba models, we introduce an effective Self-Distillation
strategy, which uses a smaller and well-trained model as the “teacher” to guide
the training of the larger “student” model. The results, depicted in Fig. 6a, show
that this strategy leads to expected better convergence.
3.4 Masked Modeling
Recently, VideoMAE and ST-MAE [18,74] have showcased the significant bene-
fits of masked modeling in enhancing a model’s capability for FINE-GRAINED
temporal understanding. UMT [43] takes this further by introducing an efficient
masked alignment technique that yields robust results across single and multi-
modal video tasks. To augment VideoMamba’s temporal sensitivity and verify
its adaptability with text modalities, we adopt a masked alignment approach
inspired by UMT. Firstly, VideoMamba is trained from scratch on video data
alone, aligning unmasked tokens with those from CLIP-ViT. Subsequently, it is
integrated with a text encoder and a cross-modal decoder ( i.e., BERT [14]), for
pretraining on both image-text and video-text datasets.

--- PAGE 8 ---
8 K. Li et al.
!"#$%-&'()'$$!)'*+!,*'-(*,#.%'))'+/.12!.$3()*44'+/-*#(+*)ℎ#$4.
Fig.6: Ablation studies of Self-Distillation and Early Stopping.
It’s important to note the distinction from UMT, which employs multi-layer
alignment between the student and teacher models. In contrast, due to Video-
Mamba’s unique architecture (SSM vs.Transformer), we align only the final
outputs.Regardingourmaskingstrategy,weproposedifferentrowmaskingtech-
niques, depicted in Fig. 5, tailored to the B-Mamba block’s preference for contin-
uous tokens. Additionally, we explore attention masking to preserve meaningful
adjacency among tokens, leveraging the inherent strengths of the 1D convolution
within the B-Mamba block for improved performance.
4 Experiments
4.1 Scaling Up
Dataset and Settings. We first conduct experiments on ImageNet-1K [13],
which includes 1.28M training images and 50K validation images across 1,000
categories. For fair comparisons, we follow most of the training strategies pro-
posed in DeiT [75], but adopt weaker data augmentation for the tiny model
variant. Furthermore, we adjust the stochastic depth ratio to 0/0.15/0.5 for
VideoMamba-Ti/S/M. Our models are trained using the AdamW optimizer
paired with a cosine learning rate schedule over 300 epochs. The initial 5 epochs
serve as a period for linear warm-up. Default settings for the learning rate,
weight decay, and batch size are 1e-3, 0.05, and 1024, respectively. Moreover, we
use BFloat16 precision during training to enhance stability without relying on
EMA. For the VideoMamba-M model, we employ a pretrained VideoMamba-S
model as a “teacher” to guide the training process by aligning the final feature
maps through L2 loss. For large resolution ( >224) fine-tuning, we use a reduced
learning rate (5e-6) and minimal weight decay (1e-8) for 30 epochs.
Effect of Self-Distillation. Fig. 6a reveals that when trained from scratch,
VideoMamba-B tends to overfit more easily and underperforms compared to
VideoMamba-S, whereas VideoMamba-M achieves similar performances. Fortu-
nately, our self-distillation has shown to be effective in achieving the desired op-
timization with marginal additional computational cost. To mitigate teacher’s

--- PAGE 9 ---
VideoMamba 9
Arch. Model iso.Input#Param FLOPs IN-1K
Size (M) (G)Top-1
CNNConvNeXt-T [53] ✗224229 4.582.1
ConvNeXt-S [53] ✗224250 8.783.1
ConvNeXt-B [53] ✗224289 15.483.8
Trans.SwinT-T [51] ✗224228 4.581.3
Swin-S [51] ✗224250 8.783.0
Swin-B [51] ✗224288 15.4 83.5
CNN+
SSMVMamba-T [50] ✗224222 5.682.2
VMamba-S [50] ✗224244 11.2 83.5
VMamba-B [50] ✗224275 18.0 83.7
CNNConvNeXt-S [53] ✓224222 4.379.7
ConvNeXt-B [53] ✓224287 16.9 82.0
Trans.DeiT-Ti [75] ✓22426 1.372.2
DeiT-S [75] ✓224222 4.679.8
DeiT-B [75] ✓224287 17.6 81.8
DeiT-B [75] ✓384287 55.5 83.1
SSMS4ND-ViT-B [58] ✓224289 -80.4
Vim-Ti [91] ✓22427 1.176.1
Vim-S [91] ✓224226 4.380.5
VideoMamba-Ti ✓22427 1.176.9
VideoMamba-Ti ✓44827 4.379.3
VideoMamba-Ti ✓57627 7.179.6
VideoMamba-S ✓224226 4.381.2
VideoMamba-S ✓448226 16.9 83.2
VideoMamba-S ✓576226 28.0 83.5
VideoMamba-M ✓224274 12.7 82.8
VideoMamba-M ✓448275 50.4 83.8
VideoMamba-M ✓576275 83.184.0
Table 2: Comparison with the state-of-the-art on ImageNet. “iso.” means
isotropic architecture without downsampling layers.
potential overdirection, we experimented with early stopping [11] in Fig. 6b,
although it did not yield beneficial outcomes. These findings indicate that self-
distillation offers a viable strategy for enhancing the scalability of the Mamba
architecture without significant computational overhead.
Results. Table 2 showcases the results on the ImageNet-1K dataset. Notably,
VideoMamba-M outperforms other isotropic architectures by significant mar-
gins, achieving a +0.8%improvement over ConvNeXt-B [53] and a +2.0%in-
crease compared to DeiT-B [75], while utilizing fewer parameters. Additionally,
VideoMamba-M holds its ground against non-isotropic backbones that lever-
age hierarchical features for enhanced performance. Given Mamba’s efficiency
in processing long sequences, we further enhance performance by increasing the
resolution, achieving a top-1 accuracy of 84.0%with only 74M parameters.
This remarkable improvement extends to video tasks, as detailed in Section 4.2,
underscoring VideoMamba’s effectiveness and scalability.
4.2 Short-term Video Understanding
Datasets and Settings. We evaluate our VideoMamba on the popular scene-
relatedKinetics-400[36]andtemporal-relatedSomething-SomethingV2[24],the
averagevideolengthsofwhichare10sand4s.Forsupervisedpretraining,wefine-
tune those models pretrained on ImageNet-1K with the same training strategy

--- PAGE 10 ---
10 K. Li et al.
Arch.Model iso.Extra Input#Param FLOPs K400
Data Size (M) (G)Top-1 Top-5
Supervised: Those models with extra data are under supervised training.
CNNSlowFast R101+NL[19] ✗ 80×224260234×3×1079.8 93.9
X3D-M [17] ✗ 16×224246×3×1076.0 92.3
X3D-XL [17] ✗ 16×312220194×3×1080.4 94.6
Trans.Swin-T [52] ✗IN-1K 32×22422888×3×478.8 93.6
Swin-B [52] ✗IN-1K 32×22428888×3×480.6 94.5
Swin-B [52] ✗IN-21K 32×224288282×3×482.795.5
CNN+
Trans.MViTv1-B [16] ✗ 32×22423770×1×580.2 94.4
MViTv2-S [45] ✗ 16×22423564×1×581.0 94.6
UniFormer-S [44] ✗IN-1K 16×22422142×1×480.8 94.7
UniFormer-B [44] ✗IN-1K 16×22425097×1×482.0 95.1
UniFormer-B [44] ✗IN-1K 32×224250259×3×483.095.4
Trans.STAM [63] ✓IN-21K 64×22421211040×1×179.2 -
TimeSformer-L [4] ✓IN-21K 96×22421212380×3×180.7 94.7
ViViT-L [2] ✓IN-21K 16×22423113992×3×481.394.7
Mformer-HR [59] ✓IN-21K 16×3362311959×3×1081.1 95.2
SSMVideoMamba-Ti ✓IN-1K 16×2242717×3×478.193.5
VideoMamba-Ti ✓IN-1K 32×2242734×3×478.893.9
VideoMamba-Ti ✓IN-1K 64×38427202×3×480.394.8
VideoMamba-S ✓IN-1K 16×22422668×3×480.894.8
VideoMamba-S ✓IN-1K 32×224226135×3×481.595.2
VideoMamba-S ✓IN-1K 64×384226395×3×482.795.6
VideoMamba-M ✓IN-1K 16×224274202×3×481.995.4
VideoMamba-M ✓IN-1K 32×224274403×3×482.495.7
VideoMamba-M ✓IN-1K 64×3842742368×3×483.396.1
Self-supervised: For UMT, the CLIP-400M is used in pretrained teacher.
Trans.BEVT-B 800e[81] ✗IN-1K 32×224288282×3×481.1 -
ST-MAE-B 1600e[18] ✓ 16×224287180×3×781.3 94.9
VideoMAE-S 2400e[74] ✓ 16×22422257×3×579.0 93.8
VideoMAE-B 1600e[74]✓ 16×224287180×3×581.5 95.1
UMT-B 800e[43] ✓CLIP-400M 8×224287180×3×585.7 97.0
SSMVideoMamba-M 800e✓CLIP-400M 8×224274101×3×482.095.4
VideoMamba-M 800e✓CLIP-400M 16×224274202×3×483.495.9
VideoMamba-M 800e✓CLIP-400M 32×224274403×3×483.996.2
VideoMamba-M 800e✓CLIP-400M 64×3842742368×3×485.096.9
Table3:Comparisonwiththestate-of-the-artonscene-relatedKinetics-400.
“iso.” means isotropic architecture without downsampling layers. Masked modeling [43]
also works for Mamba, but the inconsistent architecture leads to inferior alignment.
as VideoMAE [74]. Specifically, for VideoMamba-M, the warmup epoch, total
epoch,stochasticdepthrate,weightdecayaresetto5,50,0.8,0.05forK400,and
5, 30, 0.8, 0.05 for SthSth. For the smaller models, all the hyper-parameters are
the same unless we decrease the stochastic depth rate and increase the training
epochs. Moreover, we linearly scale the base learning rates according to the
batch size, which are 2e−4·batchsize
256for K400 and 4e−4·batchsize
256for SthSth.
As for self-supervised pretraining, we adopt the training recipe as in UMT [43],
employing CLIP-ViT-B [60] to distill VideoMamba-M over 800 epochs. During
fine-tuning, we use similar hyperparameters as mentioned but opt for a small
stochastic depth rate and learning rate for both datasets.
Results. Table 3 and 4 list the results on short-term video datasets. (a)Super-
vised:Compared with the purely attention-based methods [2,4], our SSM-based
VideoMamba-M secures a notable advantage, outperforming ViViT-L [2] by
+2.0%and+3.0%on the scene-related K400 and the temporally-related Sth-

--- PAGE 11 ---
VideoMamba 11
Arch.Model iso.Extra Input#Param FLOPs SSV2
Data Size (M) (G)Top-1 Top-5
Supervised: Those models with extra data are under supervised training.
CNNSlowFast R101[19] ✗K400 32×224253106×3×163.1 87.6
CT-Net R50[41] ✗IN-1K 16×22422175×1×164.5 89.3
TDN R50[79] ✗IN-1K 16×22422675×1×165.3 91.6
Trans. Swin-B [52] ✗K400 32×22428988×3×169.6 92.7
CNN+
Trans.MViTv1-B [16] ✗K400 16×22423771×3×164.7 89.2
MViTv1-B [16] ✗K400 32×224237170×3×167.1 90.8
MViTv2-S [45] ✗K400 16×22423565×3×168.2 91.4
MViTv2-B [45] ✗K400 32×224251225×3×170.592.7
UniFormer-S [44] ✗IN-1K+K400 16×22422142×3×167.7 91.4
UniFormer-B [44] ✗IN-1K+K400 16×22425097×3×170.492.8
Trans.TimeSformer-HR [4] ✓IN-21K 16×22421211703×3×162.5 -
ViViT-L [2] ✓IN-21K+K400 16×22423113992×3×465.4 89.8
Mformer-HR [59] ✓IN-21K+K400 16×33623111185×3×168.191.2
SSMVideoMamba-Ti ✓IN-1K 8×224279×3×265.189.1
VideoMamba-Ti ✓IN-1K 16×2242717×3×266.089.6
VideoMamba-Ti ✓IN-1K 16×2882728×3×266.290.0
VideoMamba-S ✓IN-1K 8×22422634×3×266.690.4
VideoMamba-S ✓IN-1K 16×22422668×3×267.690.9
VideoMamba-S ✓IN-1K 16×288226112×3×268.191.2
VideoMamba-M ✓IN-1K 8×224274101×3×467.391.0
VideoMamba-M ✓IN-1K 16×224274202×3×468.391.4
VideoMamba-M ✓IN-1K 16×288274333×3×468.491.6
Self-supervised: For UMT, the CLIP-400M is used in pretrained teacher.
Trans.BEVT-B 800e[81] ✗IN-1K+K400 32×224288321×3×170.6 -
VideoMAE-S 2400e[74] ✓ 16×22422257×3×266.8 90.3
VideoMAE-B 2400e[74]✓ 16×224287180×3×270.892.4
UMT-B 800e[43] ✓CLIP-400M 8×224287180×3×270.892.6
SSMVideoMamba-M 800e✓CLIP-400M 8×224274101×3×270.292.6
VideoMamba-M 800e✓CLIP-400M 16×224274202×3×271.092.7
VideoMamba-M 800e✓CLIP-400M 16×288274333×3×271.492.9
Table 4: Comparison with the state-of-the-art on temporal-related SthSth
V2.“iso.” means isotropic architecture without downsampling layers. Masked model-
ing [43] also works for Mamba, and it performs better than VideoMAE.
SthV2 datasets, respectively. This improvement comes with significantly reduced
computationaldemandsandlesspretrainingdata.Furthermore,VideoMamba-M
delivers results that are on par with the SOTA UniFormer [44], which skillfully
integrates convolution with attention in a non-isotropic structure. (b)Self-
supervised :The performance of VideoMamba under masked pretraining sur-
passes that of the VideoMAE [74], known for its proficiency in fine-grained ac-
tion. This achievement underscores the potential of our purely SSM-based model
in efficiently and effectively understanding short-term videos, highlighting its
suitability for both supervised and self-supervised learning paradigms.
Ablation Studies. Through comprehensive ablation studies detailed in Fig. 7
and Table 5, we explore various aspects of our model. (a)Scan Type :Among
all the methods, the spatial-first approach emerges as the most effective, in con-
trast, the temporal-first strategy is the worst. The superiority of the spatial-first
method is attributed to its ability to seamlessly leverage 2D pretrained knowl-
edge by scanning frame by frame. (b)Frame and Resolution :Contrary to
findings from ImageNet (see Table 2), higher resolution does not uniformly lead
to better performance. Increasing the number of frames consistently enhances

--- PAGE 12 ---
12 K. Li et al.
Type SSV2
SF-Bidirectional 65.1
TF-Bidirectional 62.4
ST-Bidirectional v1 63.9
ST-Bidirectional v2 64.2
Half-SF + Half-TF 64.0
Half-TF + Half-SF 64.1
Alternative SF&TF 65.1
(a) Scan Type. Spatial-First
scan is simple yet effective.
2000 4000 6000 8000 10000 12000
Token Number77.077.578.078.579.079.5K400 Top-1 Acc (%)
8×224×2248×320×3208×384×384
8×448×44816×224×22432×224×22464×224×224
Frame
Resolution
2000 3000 4000 5000 6000
Token Number65.265.465.665.866.0SSV2 Top-1 Acc (%)
8×224×2248×288×288
8×320×320
8×384×38416×224×224
32×224×224
Frame
Resolution (b) Frame & Resolution for K400 and SSV2.
Fig.7: Ablation studies of scan type, frame and resolution. All the models are
fine-tuned from VideoMamba-Ti pretrained on ImageNet.
Type SSV2
Random 67.4
Tube 66.3
Clip-Row 68.2
Frame-Row 67.8
Attention 68.5
(a) Mask Type.Layer SSV2
Last 1 68.5
Last 2 68.4
Last 6 68.2
Last 6 ×267.7
(b) Alignment Layer.RatioSSV2
50% 68.1
65% 68.4
80% 68.5
90% 68.2
(c) Mask Ratio.DPSSV2
0.168.0
0.268.2
0.368.4
0.468.5
(d) Droppath.
Table 5: Ablation studies of masked pretraining. We adopt CLIP-ViT-B [60] as
a teacher to distill VideoMamba-M for 200 epochs.
results on the K400 dataset. However, this is not the case with SthSthV2, possi-
bly due to the brief duration of its videos, which may not accommodate longer
inputseffectively. (c)Masked Pretraining :Ourfindingsrevealthatrowmask-
ing, being particularly compatible with 1D convolution, outperforms commonly
used random and tube masking. Clip-row masking excels owing to its higher de-
gree of randomness. Moreover, attention masking stands out as the most efficient
by favoring the preservation of adjacent meaningful content. Aligning solely the
model’s final output proves most effective, likely due to architectural differences.
Lastly, an optimal masking ratio (80%) combined with stronger regularization
significantly benefits VideoMamba during masked pretraining.
4.3 Long-term Video Understanding
DatasetsandSettings. WerigorouslyassessVideoMamba’sproficiencyinpro-
cessing long-term videos by leveraging three comprehensive datasets, i.e.,Break-
fast [37], COIN [71] and Long-form Video Understanding (LVU [84]) benchmark.
Specifically, Breakfast comprises 1,712 videos, encapsulating 10 intricate cook-
ing activities over 77 hours. COIN features 11,827 videos across 180 unique
procedural tasks, with an average duration of 2.36 minutes. The LVU bench-
mark includes approximately 30K movie clips, lasting between 1 to 3 minutes,
and encompasses nine tasks across 3 primary categories: content understand-
ing, metadata prediction, and user engagement. For the regression task among
these, we evaluate using mean-squared error, while for the classification tasks,
accuracy is the metric of choice. In contrast to prior studies [35,47] that rely on
features derived from pretrained video models, such as Swin-B [51] trained on
Kinetics-600, our method employs end-to-end training as detailed in Section 4.2.
Additionally, for fair comparisons, we fine-tune our models pretrained on K400.

--- PAGE 13 ---
VideoMamba 13
Method e2eBackbone Neck TypePretraining BF COIN
Dataset Top-1 Top-1
Timeception [32] ✗3D-ResNet Conv. IN-1K+K400 71.3 -
VideoGraph [33] ✗I3D Conv.+Atten. IN-1K+K400 69.5 -
GHRM [90] ✗I3D Graph Conv.. IN-1K+K400 75.5 -
Distant Supervision [47] ✗TimeSformer Atten. w/ KB IN-21K+HTM 89.9 90.0
ViS4mer [35] ✗Swin-B SSM IN-21K+K600 88.288.4
Turbo f32[29] ✓VideoMAE-B K400 86.8 82.3
Turbo f32[29] ✓VideoMAE-B K400+HTM-AA 91.387.5
VideoMamba f32 ✓VideoMamba-Ti K400 94.386.2
VideoMamba f64 ✓VideoMamba-Ti K400 94.387.0
VideoMamba f32 ✓VideoMamba-S K400 95.388.4
VideoMamba f64 ✓VideoMamba-S K400 97.488.7
VideoMamba f32 ✓VideoMamba-M K400 94.888.3
VideoMamba f64 ✓VideoMamba-M K400 95.889.5
VideoMamba f32 ✓VideoMamba-M † K400 97.989.6
VideoMamba f64 ✓VideoMamba-M † K400 96.990.4
Table 6: Comparison with the state-of-the-art on Breakfast and COIN.
“e2e” means end-to-end methods without exhausting feature extraction. “ †” marks
the backbone with masked pretraining.
Method e2eBackboneContent( ↑) Metadata( ↑) User( ↓)
Rel. Speak Scene Dir. Genre Wtr. Year Like View
VideoBERT [69] ✗S3D 52.80 37.90 54.90 47.30 51.90 38.50 36.10 0.32 4.46
Object Trans. [84] ✗ResNet 53.10 39.40 56.90 51.20 54.60 34.50 39.10 0.233.55
LST [35] ✗ViT-L 52.38 37.31 62.79 56.07 52.70 42.26 39.16 0.31 3.83
Performer [35] ✗ViT-L 50.00 38.80 60.46 58.87 49.45 48.21 41.25 0.31 3.93
Orthoformer [35] ✗ViT-L 50.00 39.30 66.27 55.14 55.79 47.02 43.35 0.29 3.86
ViS4mer [35] ✗ViT-L 57.1440.7967.4462.6154.71 48.80 44.750.263.63
VideoMamba f32✓VM-Ti 62.5040.4370.3767.2965.2452.9848.230.262.90
Table 7: Comparison with the state-of-the-art on LVU. “e2e” means end-to-
end methods without exhausting feature extraction. “Rel.”, “Dir.” and “Wtr.” refers to
“Relation”, “Director” and “Writer”, respectively.
Results. As illustrated in Figure 1, the linear complexity of VideoMamba makes
itwell-suitedforend-to-endtrainingwithlong-durationvideos.Thecomparisons
in Tables 6 and 7 highlight VideoMamba’s simplicity and effectiveness against
traditional feature-based methods [35,47] on these tasks. It yields significant per-
formance improvements, achieving SOTA results even with smaller model sizes.
For example, VideoMamba-Ti shows a notable increase of +6.1%over ViS4mer
using Swin-B features and a +3.0%uplift against Turbo’s multi-modality align-
ment approach [29]. Notably, the results underscore the positive impact of the
scaling model and frame numbers for long-term tasks. In the diverse and chal-
lenging set of nine tasks presented by LVU, our VideoMamba-Ti, fine-tuned in
an end-to-end manner, delivers outstanding or comparable results to current
SOTA methods. These outcomes not only highlight VideoMamba’s effectiveness
but also its great potential for future long-video comprehension.
4.4 Multi-modality Video Understanding
Datasets and Settings. Following UMT [43], we utilize WebVid-2M [3] video-
text pairs and CC3M [64] image-text pairs for joint pretraining with four ob-
jectives: vision-text contrastive learning [3], vision-text matching [40], masked

--- PAGE 14 ---
14 K. Li et al.
Method BB#PMSRVTT DiDeMo ANet LSMDC MSVD
@1 @5 @10 @1 @5 @10 @1 @5 @10 @1 @5 @10 @1 @5 @10
Singularity [38] Swin5M28.4 50.2 59.5 36.961.169.330.855.966.3- - - - - -
Frozen [3] ViT5M18.7 39.5 51.6 20.2 46.4 58.5 - - - - - - - - -
ALPRO [39] ViT5M24.1 44.7 55.4 23.8 47.3 57.9 - - - - - - - - -
BridgeFormer [23] ViT5M26.0 46.4 56.4 25.6 50.6 61.1 - - - 12.2 25.9 32.2 43.674.984.9
UMT [43] ViT5M29.652.861.933.4 58.3 67.0 28.3 53.0 64.2 16.830.537.636.2 65.7 76.1
VideoMamba VM5M32.053.063.836.661.770.335.961.172.318.036.143.438.068.679.0
VideoCLIP [85] S3D136M10.4 22.2 30.0 16.6 46.9 - - - - - - - - - -
VIOLET [21] Swin138M25.9 49.5 59.7 23.5 49.8 59.8 - - - - - - - - -
Singularity [38] Swin17M34.0 56.7 66.7 37.1 61.7 69.9 30.6 55.6 66.9 - - - - - -
OmniVL [38] ViT17M34.6 58.4 66.6 33.3 58.7 68.5 - - - - - - - - -
UMT [43] ViT17M35.559.368.641.9 66.7 75.0 33.8 59.1 70.4 18.1 33.1 42.2 41.4 70.6 80.1
UMT [43] ViT25M35.2 57.8 66.0 41.2 65.4 74.9 35.5 60.6 71.8 19.133.4 42.2 42.371.780.8
CLIP4Clip [55] ViT400M30.6 54.4 64.3 - - - - - - 13.6 27.9 35.5 36.2 63.8 73.5
InternVideo [83] ViT640M40.0 65.3 74.1 31.5 57.6 68.2 30.7 57.4 70.2 17.6 32.4 40.2 43.4 69.9 79.1
VideoMamba VM17M34.758.968.042.067.376.840.165.776.118.435.343.040.370.079.7
VideoMamba VM25M35.658.169.543.168.177.741.067.577.820.437.145.742.671.681.2
Table 8: Zero-shot text-to-video retrieval on MSRVTT, DiDeMo, Acitivi-
tyNet, LSMDC, and MSVD. “BB” means the visual backbone. “#P” refers to the
number of pretraining pairs. Models pretrained with large-scale pairs are noted in gray.
language modeling [14] and unmasked token alignment [43]. Initially, we mask
50% image tokens and 80% video tokens, conducting pretraining across 8 frames
for 10 epochs. Given Mamba’s sensitivity to positional information, an addi-
tional unmasked tuning phase is carried out for one epoch to refine its com-
prehension further. For evaluation, we undertake zero-shot video-text retrieval
tasks across five prominent benchmarks, including MSRVTT [86], DiDeMo [1],
ActivityNet [31], LSMDC [61], and MSVD [10].
Results. As indicated in Table 8, under the same pretraining corpus and similar
training strategies, our VideoMamba achieves superior zero-shot video retrieval
performances to UMT [43] based on ViT [15]. It underscores Mamba’s compa-
rable efficiency and scalability to the ViT in handling multi-modal video tasks.
Notably, for datasets featuring longer video lengths ( e.g., ANet and DiDeMo)
and more complex scenarios ( e.g., LSMDC), VideoMamba demonstrates a sig-
nificant improvement. This demonstrates Mamba’s aptitude for the demands of
cross-modality alignment even in challenging multimodal contexts.
5 Conclusion
In this paper, we propose VideoMamba, a purely SSM-based model for efficient
video understanding. Our extensive experiments demonstrate its scalability in
the visual domain, sensitivity for short-term action recognition, superiority in
long-termvideounderstandingandcompatibilitywithothermodalities.Wehope
it can pave the way for future model design for long-video comprehension.
Limitations. Due to resource constraints, we have not yet fully validated the
scalability of VideoMamba, such as extending VideoMamba to larger sizes ( e.g.,
VideoMamba-g), incorporating additional modalities ( e.g., audio), and integrat-
ing with large language models for hour-level video understanding. Despite these
limitations, our findings confirm VideoMamba’s promising potential and we plan
to conduct thorough explorations of its capabilities in the future.

--- PAGE 15 ---
VideoMamba 15
References
1. Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.:
Localizing moments in video with natural language. In: ICCV (2017) 14
2. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., Schmid, C.: Vivit: A
video vision transformer. In: ICCV (2021) 1, 2, 4, 5, 6, 10, 11, 21
3. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and
image encoder for end-to-end retrieval. In: ICCV (2021) 13, 14
4. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for
video understanding? In: ICML (2021) 1, 2, 4, 5, 6, 10, 11, 21
5. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D.,
Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., Ramesh, A.: Video gener-
ation models as world simulators (2024), https://openai.com/research/video-
generation-models-as-world-simulators 1
6. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. In: NeurIPS (2020) 3
7. Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., Zisserman, A.: A short
note about kinetics-600. ArXiv abs/1808.01340 (2018) 3
8. Carreira, J., Noland, E., Hillier, C., Zisserman, A.: A short note on the kinetics-700
human action dataset. ArXiv abs/1907.06987 (2019) 3
9. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kinetics dataset. In: CVPR (2017) 1, 4
10. Chen, D.L., Dolan, W.B.: Collecting highly parallel data for paraphrase evaluation.
In: ACL (2011) 3, 14
11. Cho,J.H.,Hariharan,B.:Ontheefficacyofknowledgedistillation.In:ICCV(2019)
9
12. Das, P., Xu, C., Doell, R.F., Corso, J.J.: A thousand frames in just a few words:
Lingual description of videos through latent topics and sparse object stitching. In:
CVPR (2013) 3
13. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. In: CVPR (2009) 8
14. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidi-
rectionaltransformersforlanguageunderstanding.ArXiv abs/1810.04805 (2018)
7, 14
15. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
ICLR (2021) 2, 5, 7, 14
16. Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.:
Multiscale vision transformers. In: ICCV (2021) 10, 11
17. Feichtenhofer, C.: X3d: Expanding architectures for efficient video recognition. In:
CVPR (2020) 4, 10
18. Feichtenhofer, C., Fan, H., Li, Y., He, K.: Masked autoencoders as spatiotemporal
learners. NeurIPS (2022) 7, 10, 21
19. Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recogni-
tion. In: ICCV (2019) 1, 4, 10, 11
20. Fu, D.Y., Dao, T., Saab, K.K., Thomas, A.W., Rudra, A., Ré, C.: Hungry hungry
hippos: Towards language modeling with state space models. In: ICLR (2023) 3

--- PAGE 16 ---
16 K. Li et al.
21. Fu, T.J., Li, L., Gan, Z., Lin, K., Wang, W.Y., Wang, L., Liu, Z.: Violet: End-
to-end video-language transformers with masked visual-token modeling. ArXiv
abs/2111.12681 (2021) 14
22. Gao, J., Sun, C., Yang, Z., Nevatia, R.: Tall: Temporal activity localization via
language query. In: ICCV (2017) 3
23. Ge, Y., Ge, Y., Liu, X., Li, D., Shan, Y., Qie, X., Luo, P.: Bridging video-text
retrieval with multiple choice questions. In: CVP (2022) 14
24. Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H.,
Haenel, V., Fründ, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C.,
Bax, I., Memisevic, R.: The “something something” video database for learning
and evaluating visual common sense. In: ICCV (2017) 3, 9
25. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. ArXiv abs/2312.00752 (2023) 2, 3, 4, 5, 7
26. Gu, A., Goel, K., Ré, C.: Efficiently modeling long sequences with structured state
spaces. In: ICLR (2022) 1, 3
27. Gu, C., Sun, C., Vijayanarasimhan, S., Pantofaru, C., Ross, D.A., Toderici, G.,
Li, Y., Ricco, S., Sukthankar, R., Schmid, C., Malik, J.: Ava: A video dataset of
spatio-temporally localized atomic visual actions. CVPR (2017) 3
28. Guo,H.,Li,J.,Dai,T.,Ouyang,Z.,Ren,X.,Xia,S.T.:Mambair:Asimplebaseline
for image restoration with state-space model. ArXiv abs/2402.15648 (2024) 3
29. Han, T., Xie, W., Zisserman, A.: Turbo training with token dropout. In: BMVC
(2022) 13
30. He,X.,Cao,K.,Yan,K.,Li,R.,Xie,C.,Zhang,J.,Zhou,M.:Pan-mamba:Effective
pan-sharpening with state space model. ArXiv abs/2402.12192 (2024) 3
31. Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A large-scale
video benchmark for human activity understanding. In: CVPR (2015) 3, 14
32. Hussein, N., Gavves, E., Smeulders, A.W.M.: Timeception for complex action
recognition. In: CVPR (2019) 13
33. Hussein, N., Gavves, E., Smeulders, A.W.M.: Videograph: Recognizing minutes-
long human activities in videos. ArXiv abs/1905.05143 (2019) 13
34. Idrees, H., Zamir, A.R., Jiang, Y.G., Gorban, A., Laptev, I., Sukthankar, R., Shah,
M.: The thumos challenge on action recognition for videos “in the wild”. Computer
Vision and Image Understanding (2017) 3
35. Islam, M.M., Bertasius, G.: Long movie clip classification with state-space video
models. In: ECCV (2022) 2, 3, 12, 13
36. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
S., Viola, F., Green, T., Back, T., Natsev, A., Suleyman, M., Zisserman, A.: The
kinetics human action video dataset. ArXiv abs/1705.06950 (2017) 3, 9
37. Kuehne, H., Arslan, A., Serre, T.: The language of actions: Recovering the syntax
and semantics of goal-directed human activities. In: CVPR (2014) 3, 12
38. Lei, J., Berg, T.L., Bansal, M.: Revealing single frame bias for video-and-language
learning. ArXiv abs/2206.03428 (2022) 14
39. Li,D.,Li,J.,Li,H.,Niebles,J.C.,Hoi,S.C.:Alignandprompt:Video-and-language
pre-training with entity prompts. In: CVPR (2022) 14
40. Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before
fuse: Vision and language representation learning with momentum distillation. In:
NeurIPS (2021) 13
41. Li, K., Li, X., Wang, Y., Wang, J., Qiao, Y.: Ct-net: Channel tensorization network
for video classification. In: ICLR (2020) 11

--- PAGE 17 ---
VideoMamba 17
42. Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Wang, L., Qiao, Y.: Uniformerv2:
Spatiotemporal learning by arming image vits with video uniformer. ArXiv
abs/2211.09552 (2022) 4
43. Li, K., Wang, Y., Li, Y., Wang, Y., He, Y., Wang, L., Qiao, Y.: Unmasked teacher:
Towards training-efficient video foundation models. In: ICCV (2023) 6, 7, 10, 11,
13, 14, 21
44. Li, K., Wang, Y., Peng, G., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified
transformer for efficient spatial-temporal representation learning. In: ICLR (2022)
1, 4, 10, 11
45. Li, Y., Wu, C., Fan, H., Mangalam, K., Xiong, B., Malik, J., Feichtenhofer, C.:
Improved multiscale vision transformers for classification and detection. ArXiv
abs/2112.01526 (2021) 10, 11
46. Liang, D., Zhou, X., Wang, X., Zhu, X., Xu, W., Zou, Z., Ye, X., Bai,
X.: Pointmamba: A simple state space model for point cloud analysis. ArXiv
abs/2402.10739 (2024) 3
47. Lin,X.,Petroni,F.,Bertasius,G.,Rohrbach,M.,Chang,S.F.,Torresani,L.:Learn-
ing to recognize procedural activities with distant supervision. CVPR (2022) 2,
12, 13
48. Liu, H., Yan, W., Zaharia, M., Abbeel, P.: World model on million-length video
and language with ringattention. ArXiv abs/2402.08268 (2024) 1
49. Liu, Y., Wang, L., Wang, Y., Ma, X., Qiao, Y.: Fineaction: A fine-grained video
dataset for temporal action localization. TIP (2022) 3
50. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba:
Visual state space model. ArXiv abs/2401.10166 (2024) 2, 3, 6, 7, 9
51. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin trans-
former: Hierarchical vision transformer using shifted windows. In: ICCV (2021) 9,
12
52. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin trans-
former. In: CVPR (2022) 2, 4, 10, 11
53. Liu, Z., Mao, H., Wu, C., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the
2020s. In: CVPR (2022) 9
54. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. NeurIPS (2019) 3
55. Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An
empirical study of clip for end to end video clip retrieval and captioning. Neuro-
computing (2022) 14
56. Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling
via gated state spaces. ArXiv abs/2206.13947 (2022) 3
57. Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.:
Howto100m: Learning a text-video embedding by watching hundred million nar-
rated video clips. In: ICCV (2019) 3
58. Nguyen, E., Goel, K., Gu, A., Downs, G.W., Shah, P., Dao, T., Baccus, S.A.,
Ré, C.: S4nd: Modeling images and videos as multidimensional signals with state
spaces. In: NeurIPS (2022) 9
59. Patrick, M., Campbell, D., Asano, Y., Misra, I., Metze, F., Feichtenhofer, C.,
Vedaldi, A., Henriques, J.F.: Keeping your eye on the ball: Trajectory attention in
video transformers. In: NeurIPS (2021) 4, 10, 11, 21
60. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable
visual models from natural language supervision. In: ICML (2021) 10, 12

--- PAGE 18 ---
18 K. Li et al.
61. Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C.J., Larochelle, H.,
Courville, A.C., Schiele, B.: Movie description. International Journal of Computer
Vision (2016) 14
62. Shao, D., Zhao, Y., Dai, B., Lin, D.: Finegym: A hierarchical video dataset for
fine-grained action understanding. CVPR (2020) 3
63. Sharir, G., Noy, A., Zelnik-Manor, L.: An image is worth 16x16 words, what is a
video worth? ArXiv abs/2103.13915 (2021) 4, 10, 21
64. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: ACL
(2018) 13
65. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-
nition in videos. NeurIPS (2014) 4
66. Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for
sequence modeling. In: ICLR (2023) 3
67. Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes
from videos in the wild. arXiv preprint arXiv:1212.0402 (2012) 3
68. Su, J., Lu, Y., Pan, S., Wen, B., Liu, Y.: Roformer: Enhanced transformer with
rotary position embedding. ArXiv abs/2104.09864 (2021) 6
69. Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A joint
model for video and language representation learning. In: ICCV (2019) 13
70. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., Wei, F.: Re-
tentive network: A successor to transformer for large language models. ArXiv
abs/2307.08621 (2023) 1
71. Tang, Y., Ding, D., Rao, Y., Zheng, Y., Zhang, D., Zhao, L., Lu, J., Zhou, J.: Coin:
A large-scale dataset for comprehensive instructional video analysis. In: CVPR
(2019) 3, 12
72. Team, G.: Gemini: A family of highly capable multimodal models. ArXiv
abs/2312.11805 (2023) 1
73. Team, R.: Rwkv: Reinventing rnns for the transformer era. In: EMNLP (2023) 1
74. Tong, Z., Song, Y., Wang, J., Wang, L.: VideoMAE: Masked autoencoders are
data-efficient learners for self-supervised video pre-training. In: NeurIPS (2022) 6,
7, 10, 11, 21
75. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J’egou, H.: Training
data-efficient image transformers & distillation through attention. In: ICML (2021)
2, 8, 9
76. Tran, D., Bourdev, L.D., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-
poral features with 3d convolutional networks. In: IEEE International Conference
on Computer Vision (2015) 1, 4
77. Tran, D., xiu Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look
at spatiotemporal convolutions for action recognition. In: CVPR (2018) 4
78. Wang, C., Tsepa, O., Ma, J., Wang, B.: Graph-mamba: Towards long-range graph
sequence modeling with selective state spaces. ArXiv abs/2402.00789 (2024) 3
79. Wang, L., Tong, Z., Ji, B., Wu, G.: TDN: Temporal difference networks for efficient
action recognition. In: CVPR (2021) 11
80. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Gool, L.V.: Temporal
segment networks: Towards good practices for deep action recognition. In: ECCV
(2016) 4, 20
81. Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y.G., Zhou, L.,
Yuan, L.: Bevt: Bert pretraining of video transformers. CVPR (2022) 10, 11

--- PAGE 19 ---
VideoMamba 19
82. Wang, Y., He, Y., Li, Y., Li, K., Yu, J., Ma, X.J., Chen, X., Wang, Y., Luo, P.,
Liu, Z., Wang, Y., Wang, L., Qiao, Y.: Internvid: A large-scale video-text dataset
for multimodal understanding and generation. ArXiv (2023) 3
83. Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., Zhang, H., Xu, J., Liu,
Y., Wang, Z., Xing, S., Chen, G., Pan, J., Yu, J., Wang, Y., Wang, L., Qiao, Y.:
Internvideo: General video foundation models via generative and discriminative
learning. ArXiv abs/2212.03191 (2022) 4, 14
84. Wu, C.Y., Krahenbuhl, P.: Towards long-form video understanding. In: CVPR
(2021) 3, 12, 13
85. Xu, H., Ghosh, G., Huang, P.Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettle-
moyer, L., Feichtenhofer, C.: Videoclip: Contrastive pre-training for zero-shot
video-text understanding. ArXiv abs/2109.14084 (2021) 14
86. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for
bridging video and language. In: CVPR (2016) 3, 14
87. Yang, Y., Xing, Z., Zhu, L.: Vivim: a video vision mamba for medical video object
segmentation. ArXiv abs/2401.14168 (2024) 3
88. Yu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., Tao, D.: Activitynet-qa: A
dataset for understanding complex web videos via question answering. In: AAAI
(2019) 3
89. Zhang, D.J., Li, K., Wang, Y., Chen, Y., Chandra, S., Qiao, Y., Liu, L., Shou,
M.Z.: Morphmlp: An efficient mlp-like backbone for spatial-temporal representa-
tion learning. In: ECCV (2022) 4
90. Zhou, J., Lin, K.Y., Li, H., Zheng, W.: Graph-based high-order relation modeling
for long-term action recognition. CVPR (2021) 13
91. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Ef-
ficient visual representation learning with bidirectional state space model. ArXiv
abs/2401.09417 (2024) 2, 3, 5, 6, 9
92. Zhuang, S., Li, K., Chen, X., Wang, Y., Liu, Z., Qiao, Y., Wang, Y.: Vlogger: Make
your dream a vlog. ArXiv abs/2401.09414 (2024) 1

--- PAGE 20 ---
VideoMamba: State Space Model for Efficient
Video Understanding
Appendix
https://github.com/OpenGVLab/VideoMamba
A More Results
In Table I, we present additional results on the Kinetics-400 dataset. These
results clearly demonstrate that our SSM-based model outperforms all previous
attention-basedmethods.Weobserveconsistentperformanceimprovementswith
increasing resolution and frame count.
B More Implementation Details
B.1 Training Details
WesparselysampleframesfromtherawvideosasinTSN[80]forallthedatasets.
Table II details the masked pretraining hyperparameters. For the unmasked
multi-modality pretraining, we load the pretrained model and train it for an
additional epoch with a learning rate of 8e-5. Moreover, Tables III, IV, V, and VI
show the training details for the different datasets used for fine-tuning.
B.2 Dataset Descriptions
We show the statistics of multi-modality datasets in Table VII, and single-
modality datasets in Table VIII.

--- PAGE 21 ---
VideoMamba 21
Arch.Model iso.Extra Input#Param FLOPs K400
Data Size (M) (G)Top-1 Top-5
Supervised: Those models with extra data are under supervised training.
Trans.STAM [63] ✓IN-21K 64×22421211040×1×179.2 -
TimeSformer-L [4] ✓IN-21K 96×22421212380×3×180.7 94.7
ViViT-L [2] ✓IN-21K 16×22423113992×3×481.394.7
Mformer-HR [59] ✓IN-21K 16×3362311959×3×1081.1 95.2
SSMVideoMamba-Ti ✓IN-1K 8×224279×3×476.992.9
VideoMamba-Ti ✓IN-1K 16×2242717×3×478.193.5
VideoMamba-Ti ✓IN-1K 32×2242734×3×478.893.9
VideoMamba-Ti ✓IN-1K 64×2242769×3×479.694.2
VideoMamba-Ti ✓IN-1K 64×38427202×3×480.394.8
VideoMamba-S ✓IN-1K 8×22422634×3×479.394.2
VideoMamba-S ✓IN-1K 16×22422668×3×480.894.8
VideoMamba-S ✓IN-1K 32×224226135×3×481.595.2
VideoMamba-S ✓IN-1K 64×224226271×3×481.895.3
VideoMamba-S ✓IN-1K 64×384226395×3×482.795.6
VideoMamba-M ✓IN-1K 8×224274101×3×480.694.6
VideoMamba-M ✓IN-1K 16×224274202×3×481.995.4
VideoMamba-M ✓IN-1K 32×224274403×3×482.495.7
VideoMamba-M ✓IN-1K 64×224274806×3×482.896.0
VideoMamba-M ✓IN-1K 64×3842742368×3×483.396.1
Self-supervised: For UMT, the CLIP-400M is used in pretrained teacher.
Trans.ST-MAE-B 1600e[18] ✓ 16×224287180×3×781.3 94.9
VideoMAE-S 2400e[74] ✓ 16×22422257×3×579.0 93.8
VideoMAE-B 1600e[74]✓ 16×224287180×3×581.5 95.1
UMT-B 800e[43] ✓CLIP-400M 8×224287180×3×585.7 97.0
SSMVideoMamba-M 800e✓CLIP-400M 8×224274101×3×482.095.4
VideoMamba-M 800e✓CLIP-400M 16×224274202×3×483.495.9
VideoMamba-M 800e✓CLIP-400M 32×224274403×3×483.996.2
VideoMamba-M 800e✓CLIP-400M 64×224274806×3×484.396.6
VideoMamba-M 800e✓CLIP-400M 64×3842742368×3×485.096.9
Table I: More results on scene-related Kinetics-400. “iso.” means isotropic
architecture without downsampling layers.
configSingle-Modality Multi-Modality
SthSthV2 K400 5M & 17M & 25M
optimizer AdamW AdamW
optimizer momentum β1, β2=0.9,0.95 β1, β2=0.9,0.999
weight decay 0.05 0.05
learning rate schedule cosine decay cosine decay
learning rate 1.2e-3 4e-4
minimal learning rate 1e-5 4e-6
batch size 2048 2048I, 2048V
warmup epochs 40 1
total epochs 800 10
mask ratio 80% 50%I, 80%V, 50%T
input frame 8 8
drop path 0.4 0.25
flip augmentation no yes yes
augmentation MultiScaleCrop [0.66, 0.75, 0.875, 1] MultiScaleCrop [0.5, 1]
Table II: Masked pre-training settings. I-image, V-video, T-text.

--- PAGE 22 ---
22 K. Li et al.
config 224×224 448 ×448 512 ×512
optimizer AdamW
optimizer momentum β1, β2=0.9,0.999
weight decay 0.1(Ti), 0.05(S,M) 1e-8 1e-8
learning rate schedule cosine decay
base learning rate 5e-4 5e-6 5e-6
minimal learning rate 1e-5 5e-6 5e-6
base batch size 512
repeated augmentation no(Ti), yes(S,M)
warmup epochs 5(Ti,S), 30(M) 5 2
total epochs 300 30 10
drop path 0(Ti), 0.15(S), 0.5(M)
label smoothing 0.1
cutmix 1.0
augmentation RandAug (7, 0.25)(Ti), RandAug (9, 0.5)(S,M)
Table III: Training settings for ImageNet-1K.
config 224×224 384 ×384
optimizer AdamW
optimizer momentum β1, β2=0.9,0.999
weight decay 0.1(Ti), 0.05(S,M,M †) 1e-8
learning rate schedule cosine decay
base learning rate 4e-4(Ti,S), 2e-4(M), 1e-4(M †) 5e-6
minimal learning rate 1e-6
base batch size 256
repeated augmentation 2
warmup epochs 5 2
total epochs 70(Ti), 50(S,M), 45(M †) 10
drop path 0.1(Ti), 0.35(S), 0.8(M), 0.4(M †)
layer-wise lr decay 0.75(S,M,M †), 0.8(M †)
flip augmentation yes
label smoothing 0.1
cutmix 1.0
augmentation RandAug (7, 0.25)(Ti), RandAug (9, 0.5)(S,M,M †)
Table IV: Training settings for Kinetics-400. “†” means masked pretraining.

--- PAGE 23 ---
VideoMamba 23
config 224×224 288 ×288
optimizer AdamW
optimizer momentum β1, β2=0.9,0.999
weight decay 0.1(Ti), 0.05(S,M,M †) 1e-8
learning rate schedule cosine decay
base learning rate 4e-4(Ti,S,M) 1e-4(M †) 5e-6
minimal learning rate 1e-6
base batch size 256
repeated augmentation 2
warmup epochs 5 2
total epochs 35(Ti), 30(S,M,M †) 10
drop path 0.1(Ti), 0.35(S), 0.8(M), 0.4(M †)
layer-wise lr decay 0.75(S,M,M †), 0.8(M †)
flip augmentation no
label smoothing 0.1
cutmix 1.0
augmentation RandAug (7, 0.25)(Ti), RandAug (9, 0.5)(S,M,M †)
Table V: Training settings for SthSthV2. “†” means masked pretraining.
config BreakFast & LVU COIN
optimizer AdamW
optimizer momentum β1, β2=0.9,0.999
weight decay 0.1(Ti), 0.05(S,M,M †)
learning rate schedule cosine decay
base learning rate 2e-4
minimal learning rate 1e-6
base batch size 256
repeated augmentation 2
warmup epochs 5
total epochs 70(Ti), 50(S,M), 45(M †) 40(Ti), 35(S), 30(M,M †)
drop path 0.1(Ti), 0.35(S), 0.8(M), 0.4(M †)
layer-wise lr decay 0.75(S,M,M †), 0.8(M †)
flip augmentation yes
label smoothing 0.1
cutmix 1.0
augmentation RandAug (7, 0.25)(Ti), RandAug (9, 0.5)(S,M,M †)
Table VI: Training settings for Breaskfast, COIN and LVU. “†” means masked
pretraining. We directly sample the frames from the raw video sparsely.

--- PAGE 24 ---
24 K. Li et al.
Dataset #image/video #text Type
COCO 113K 567K image
Visual Genome 100K 768K image
SBU Captions 860K 860K image
CC3M 2.88M 2.88M image
CC12M 11.00M 11.00M image
WebVid-2M 2.49M 2.49M video
WebVid-10M 10.73M 10.73M video
5M corpus = CC3M +WebVid-2M 5.37M 5.37M video+image
17M corpus = 5M +COCO +VG+SBU +CC12M 17.44M 18.57M video+image
25M corpus = 17M +WebVid-10M −WebVid-2M 25.68M 26.81M video+image
Table VII: Statistics of multi-modality datasets.
Dataset#video #text Avg Video
Train Val Test Train Val Test Length (s)
Image Classification
ImageNet-1K 1,281,167 50,000 100,000 - - - -
Short-term Action Recognition
Kinetics-400 240,436 19,787 - - - - 10
Something-Something V2 168,913 24,777 - - - - 4
Long-term Action Recognition
Breakfast 1,577 - 410 - - - 137
COIN 9,026 - 2,796 - - - 142
LVU 7,619 1,666 1,551 - - - 134
Relation 138 49 41 - - - 127
Speak 871 196 188 - - - 133
Scene 514 107 81 - - - 132
Director 680 163 107 - - - 137
Genre 2807 569 584 - - - 130
Writer 748 174 168 - - - 142
Year 725 163 141 - - - 133
Like 658 142 139 - - - 159
View 478 103 102 - - - 112
Video-Text Retrieval
MSRVTT 7,010 - 1,000 140,200 - 1,000 15
DiDeMo 8,496 1,094 1,036 8,496 1,094 1,036 29
ActivityNet 10,009 4,917 - 10,009 4,917 - 180
LSMDC 101,055 - 1,000 101,055 - 1,000 5
MSVD 1,200 100 670 1,200 100 670 15
Table VIII: Statistics of single-modality datasets.
