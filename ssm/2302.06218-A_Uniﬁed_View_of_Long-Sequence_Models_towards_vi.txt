# 2302.06218.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2302.06218.pdf
# Kích thước file: 858628 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Một Cái Nhìn Thống Nhất về các Mô hình Chuỗi Dài hướng tới
Mô hình hóa các Phụ thuộc Quy mô Triệu
Hongyu Hè
Khoa Khoa học Máy tính
ETH ZurichMarko Kabic
Khoa Khoa học Máy tính
ETH Zurich
Thuật ngữ
 Ma trận hiệp phương sai
G Ma trận Gram/kernel
k() Hàm kernel
P() Mật độ xác suất
P() Quá trình trộn token
Re()Hàm trích xuất thành phần thực của số phức
FT()(Rời rạc) Biến đổi Fourier
~ ai Phần tử tại vị trí thứ i của vector cột ~ a
A:j Vector cột trong hàng thứ j của A
Ai;j Phần tử trong hàng thứ i cột thứ j của A
Ai Vector hàng trong hàng thứ i của A(=Ai:)
ANMTắt gọn cho ma trận A2RNM
Fh
DDMa trận Vandermonde của chiều embedding
Fs
LLMa trận Vandermonde của chiều chuỗi
W Ma trận trọng số học được với phi tuyến theo từng phần tử (ví dụ: ReLU, GELU)
WC
LLMa trận trọng số của một kernel convolution đơn
WK
DNMa trận trọng số khóa attention (đối với self-attention, N=M)
WQ
DMMa trận trọng số truy vấn attention
WV
DMMa trận trọng số giá trị attention
eX Các token kết quả với bias quy nạp được đưa vào X
XLDChuỗi đầu vào có độ dài L và chiều embedding D, trong đó L≫D
Tác giả liên hệ <honghe@inf.ethz.ch>arXiv:2302.06218v3  [cs.LG]  16 Feb 2023

--- TRANG 2 ---
Tóm tắt
Kể từ khi được hình thành, Transformers đã vượt qua các mô hình chuỗi truyền thống trong nhiều tác vụ, như NLP, phân loại hình ảnh, và xử lý video/âm thanh, nhờ khả năng huấn luyện nhanh và hiệu suất vượt trội. Phần lớn ưu điểm này có thể quy cho mã hóa vị trí và cơ chế attention đa đầu. Tuy nhiên, Transformers có hạn chế trong việc học các phụ thuộc tầm xa chủ yếu do độ phức tạp bậc hai tỷ lệ với độ dài ngữ cảnh, về cả thời gian và không gian. Do đó, trong vòng năm năm qua, vô số phương pháp đã được đề xuất để làm cho Transformers hiệu quả hơn. Trong nghiên cứu này, trước tiên chúng tôi lùi lại một bước, nghiên cứu và so sánh các giải pháp hiện có cho mô hình chuỗi dài theo công thức toán học thuần túy của chúng. Cụ thể, chúng tôi tóm tắt chúng bằng một khuôn mẫu thống nhất, dựa trên bản chất chung của việc trộn token. Thông qua các benchmark, chúng tôi sau đó chứng minh rằng độ dài ngữ cảnh dài thực sự mang lại hiệu suất tốt hơn, mặc dù phụ thuộc vào ứng dụng, và các mô hình Transformer truyền thống không đủ khả năng tận dụng các phụ thuộc tầm xa. Tiếp theo, lấy cảm hứng từ các mô hình thưa thớt mới nổi có dung lượng lớn, chúng tôi đề xuất một hệ thống học máy để xử lý các phụ thuộc quy mô triệu. Như một bằng chứng khái niệm, chúng tôi đánh giá hiệu suất của một thành phần thiết yếu của hệ thống này, cụ thể là cơ chế attention đa đầu phân tán. Chúng tôi cho thấy thuật toán của chúng tôi có thể mở rộng tính toán attention gần 40× bằng cách sử dụng bốn GPU GeForce RTX 4090, so với cơ chế attention đa đầu vanilla. Chúng tôi tin rằng nghiên cứu này là một bước quan trọng hướng tới mô hình hóa các phụ thuộc quy mô triệu.

1 Giới thiệu
Các mô hình chuỗi (ví dụ: LSTM [25], GRU [9]) có thể nắm bắt mối quan hệ giữa các token đầu vào trong quá trình học, trong đó các token có thể là từ, pixel, tín hiệu, v.v. Bằng cách sử dụng thông tin ngữ cảnh, các mô hình chuỗi học các phụ thuộc phức tạp trong các mẫu huấn luyện, được tham chiếu trong quá trình suy luận. Việc học trong ngữ cảnh như vậy đã chứng minh hiệu suất vượt trội trong nhiều tác vụ (ví dụ: NLP [26], xử lý hình ảnh [41]). Tuy nhiên, các mô hình chuỗi truyền thống có hai nhược điểm chính, đó là huấn luyện chậm và quên các phụ thuộc tầm xa. Nhược điểm đầu tiên là do bản chất tuần tự của những mô hình đó - các token phải được đưa vào một cách tuần tự và theo thứ tự. Nhược điểm thứ hai là kết quả của khả năng mô hình hạn chế (ví dụ: của trạng thái ẩn trong LSTM).

Kể từ khi được hình thành, Transformers [40] đã nhanh chóng vượt qua các mô hình chuỗi truyền thống nhờ khả năng huấn luyện nhanh và hiệu suất vượt trội. Sự tăng tốc này là do việc sử dụng mã hóa vị trí [40,45]. Kỹ thuật này cho phép xử lý song song các token đầu vào mà không mất thông tin liên quan đến thứ tự ngữ nghĩa của chúng, bằng cách mã hóa trực tiếp vị trí token vào embedding token. Mã hóa vị trí nhẹ nhàng đẩy các token trong không gian đặc trưng theo một hướng dựa trên vị trí của chúng trong chuỗi, mà không phá hủy thông tin được mã hóa trong không gian vector embedding ban đầu của chúng. Nó cũng có thể được áp dụng cho các đầu vào có cấu trúc [1] như hình ảnh bằng cách sử dụng thông tin vị trí tương đối [43], tương tự như các kernel convolutional. Hơn nữa, hiệu suất tuyệt vời của chúng có thể được quy cho cơ chế attention, hoặc cụ thể hơn là cơ chế self/cross-attention đa đầu. Cơ chế attention học một cách rõ ràng các phụ thuộc trong đầu vào bằng cách tính điểm attention theo từng cặp giữa tất cả các kết hợp token. Điểm attention có thể được tính bằng nhiều cách [39] và phương pháp đơn giản nhất, nhưng rất hiệu quả, là tích vô hướng, đo khoảng cách giữa hai vector embedding. Cơ chế này cải thiện đáng kể hiệu suất của mô hình chuỗi và là cần thiết để một số tác vụ nhất định có thể thực hiện được, ví dụ như xử lý âm thanh và video.

Mặc dù có những ưu điểm lớn, Transformers cũng có những nhược điểm quan trọng, và đứng đầu trong số đó là hiệu quả tài nguyên của chúng. Mặc dù Transformers nhanh hơn nhiều so với các mô hình chuỗi truyền thống, ma trận attention phát sinh độ phức tạp O(L²), trong đó L là độ dài ngữ cảnh, về cả tính toán và lưu trữ. Độ phức tạp như vậy đặc biệt cấm đoán khi xử lý ngữ cảnh hàng nghìn token vì thời gian và bộ nhớ cần thiết tăng theo bậc hai, ví dụ như tóm tắt sách, xử lý âm thanh/video, xử lý hình ảnh độ phân giải cao, v.v. Trong những tác vụ này, ngữ cảnh có thể dễ dàng mở rộng đến hàng nghìn hoặc thậm chí hàng triệu token vì việc đưa ra quyết định có thể cần thông tin từ các bước rất sớm trong chuỗi (ví dụ: một tên trong chương đầu tiên của một cuốn sách). Để đối phó với thách thức này, trong vòng năm năm qua, nhiều nghiên cứu đã tập trung vào việc làm cho Transformers hiệu quả hơn. Các phương pháp được đề xuất bao gồm (nhưng không giới hạn ở) ví dụ như xấp xỉ ma trận attention với độ thưa [24,3,46], phân cụm trước khi tính toán attention [35,29], đưa ra giả định thông qua xác suất có điều kiện [34], ước lượng hạng thấp [42], I/O bộ nhớ tốt hơn [12], tính trực giao và kết hợp của ma trận [7], v.v. Ngoài việc giải quyết trực tiếp vấn đề hiệu quả của Transformers, nhiều mô hình khác đã được đề xuất để đáp ứng nhu cầu học ngữ cảnh dài. Để kể tên một vài, MLP-Mixer [37], FNet [30] và SGConv [31] học từ và (hoàn toàn) sử dụng mô hình trộn token từ Transformers; Memorizing Transformers [44] đưa Neural Turing Machines [17] đến mức cực đoan và biến Transformers thành một cấu trúc LSTM-like khổng lồ; S4 [20,21] hồi sinh các State Space Models truyền thống kết hợp với phép chiếu đa thức trực giao để tái tạo lịch sử.

Nhận thức được tầm quan trọng cả về tính mới lạ và khối lượng công việc trước đây, chúng tôi lùi lại một bước và biên soạn công việc này với các mục tiêu sau:

1. Phân loại các giải pháp hiện có cho vấn đề phụ thuộc tầm xa hoàn toàn bằng công thức toán học của chúng.
2. So sánh các phương pháp khác nhau bằng một khuôn mẫu thống nhất, với đó chúng tôi nghiên cứu sự đánh đổi trong việc nắm bắt các phụ thuộc toàn cục và cục bộ trong chuỗi đầu vào.
3. Đánh giá thực nghiệm tác động của độ dài ngữ cảnh đối với mô hình chuỗi trong các thí nghiệm với các mô hình tiên tiến.
4. Đề xuất một hệ thống học máy để học các phụ thuộc quy mô triệu, nhằm tạo sự cân bằng giữa chất lượng mô hình và hiệu quả tài nguyên.
5. Thiết kế và thử nghiệm tính khả thi của một thuật toán phân tán để tính toán ma trận attention cho các chuỗi hàng triệu token.

2 Công thức Bài toán

Nói chung, các phương pháp giống attention hiện có nhận một chuỗi X có độ dài L và chiều D làm đầu vào. Các phương pháp này đưa bias quy nạp (tức là phụ thuộc) để tăng cường không gian đặc trưng ban đầu của X thông qua một quá trình "trộn" P được tham số hóa bằng θ học được với các kỹ thuật khác nhau (Eq. 1) hoặc cố định bằng các thủ tục nhất định, ví dụ: Biến đổi Fourier (Eq. 2).

P(X|θ) : X 7→ X̃ (1)
P(X) : X 7→ X̃ (2)

Hơn nữa, chúng tôi phân loại P thành bốn mô hình sau:
1. P(·|θ) ⊥⊥ X: Trộn học được độc lập với đầu vào, ví dụ: convolution đơn giản (§3) và MLP-Mixer (§5).
2. P(·|θ) ⊥̸⊥ X: Trộn học được phụ thuộc vào đầu vào, ví dụ: self-attention (§4).
3. P(·) ⊥⊥ X: Trộn cố định độc lập với đầu vào, ví dụ: FNet (§6).
4. P(·) ⊥̸⊥ X: Trộn cố định phụ thuộc vào đầu vào, ví dụ: State Space Model với ma trận chuyển tiếp cố định (§7).

3 Convolution trên Tín hiệu

Trước tiên, chúng tôi vẽ ra sự tương đồng giữa convolution và cơ chế attention. Không mất tính tổng quát, chúng tôi giả định rằng đầu vào đã được tuyến tính hóa thành 1D (ví dụ: [14,38,22]). Để đơn giản, chúng tôi chỉ xét depthwise convolution (không có pointwise convolution), tức là X ∈ R^{L×1}. Cho một bộ lọc/kernel f có kích thước cửa sổ K, biểu diễn Y_t của token thứ t kết quả từ convolution trên tín hiệu g là trung bình có trọng số trên đầu vào:

Y_t := (f ∗ g_X)(t) = ∑_{k:=0}^{K-1} w̃_k X_{k+t} (depthwise) : (3)

Ngoài ra, các tính chất sau của convolution sẽ được sử dụng trong các phép suy diễn sau này.

3

--- TRANG 4 ---
• Tính giao hoán: Eq. 3 có thể được viết lại thành Y_t = ∑_{k:=0}^{K-1} w̃_{k+t} X_k [11].
• Tính phân phối của phép cộng: ∑(f⊛g) ≡ ∑f⊛∑g.
• Định lý Convolution: f⊛g ≡ FT(f⊛g).

Một cách tiện lợi để xem convolution trên toàn bộ chuỗi X là công thức hóa Eq. 3 như việc cân nhắc đầu vào với các ma trận trọng số có cấu trúc:

Z := {W}⊛X = [ma trận Toeplitz 7×4] [X_{t-3}, X_{t-2}, X_{t-1}, X_t]^T

= W^C X (4)
= P_{conv}(X|θ_{conv})
= X̃_{conv}; (5)

trong đó θ_{conv} = {W^C}.

Quan sát rằng (1) P_{conv} được học nhưng độc lập với chuỗi đầu vào, vì X không xuất hiện trong θ_{conv}, và (2) kích thước cửa sổ K cố định trên tất cả các tín hiệu đầu vào. Do đó, các lớp đầu tiên của CNN có view cục bộ hạn chế về chuỗi đầu vào, và chỉ bằng cách xếp chồng các kernel mới có thể mở rộng trường tiếp nhận của các lớp cao hơn.

Ý tưởng xếp chồng các kernel để mở rộng trường tiếp nhận sinh ra một loạt phương pháp (ví dụ: [42, 3, 7]) để xấp xỉ ma trận full-attention, là những tương tự của Eq. 4.

4 Self-Attention

Tương tự như convolution được thảo luận trong §3, cơ chế attention cũng có thể được xem như một trung bình có trọng số trên các embedding thô của các token đầu vào. Để đơn giản, masking và scaling được loại trừ.

4.1 Attention như Trung bình Có trọng số

Trước tiên, ba ma trận cùng kích thước (self-attention) được thu được bằng cách chiếu tuyến tính cùng một đầu vào X ba lần với các ma trận trọng số có thể học được thông qua các lớp MLP:

V := XW^V; K := XW^K; Q := XW^Q

Bước thứ hai là tính điểm attention cho mỗi token trong chuỗi tại vị trí t:

A'_t := Q_t K^T: (6)

Sau đó, các điểm attention được chuẩn hóa theo hàng bằng softmax:

A_{t,i} := \frac{\exp A'_{t,i}}{\sum_{j=0}^{L-1} \exp A'_{t,j}}:

Cuối cùng, các phép chiếu đầu vào V được cân nhắc bởi điểm số để tạo bias/ngữ cảnh, tạo ra biểu diễn cuối cùng của token t:

Z_t := A_t V:

4

--- TRANG 5 ---
Ghép các bước lại với nhau, chúng ta có được trung bình có trọng số tương tự Eq. 3:

Z_t := ∑_{j=0}^{L-1} \text{softmax}(Q_t K^T)_{|{z}}^{\text{trọng số attention đầy đủ}} V_i: (7)

4.2 Attention như Trộn Token

Khác với Eq. 3, ở đây chuỗi đầu vào X trực tiếp tham gia vào các trọng số trong Eq. 7. Để đơn giản, từ bây giờ chúng tôi bỏ qua mọi loại chuẩn hóa như softmax, layer/batch norm, v.v.

Sau đó, chúng ta có thể viết lại Eq. 7 thành:

Z_{L×D}: = Q_t K^T_{|{z}}^{A'(Eq:6)} V
= (XW^Q)(XW^K)^T XW^V
= X(W^Q W^{K^T})X^T XW^V
= XG_W X^T XW^V (8)
= A' X W^V (9)
= P_{attn}(X|θ_{attn})
= X̃_{attn};

trong đó θ_{attn} = {A', W^V}, và Eq. 8 tóm tắt tích giữa hai ma trận trọng số học được thành ma trận Gram G_W ∈ R^{D×D} của chúng.

Kết quả là, chúng ta thu được một biểu diễn mới X̃_{attn} của chuỗi đầu vào bằng cách học một sơ đồ trộn trên các token thô bằng self-attention (Eq. 9). Nói cách khác, P_{attn} được học và phụ thuộc vào đầu vào vì X tham gia vào phương trình thông qua θ_{attn}.

4.3 Sử dụng Kernel Tĩnh hoặc Feature Map

Độ phức tạp tính toán của Eq. 9 là O(L²D) ≈ O(L²) do tích giữa ma trận full-attention (không chuẩn hóa) A' ∈ R^{L×L} và chuỗi đầu vào X ∈ R^{L×D}.

Tương tự Eq. 8, chúng ta có thể coi A' như ma trận Gram được tham số hóa của không gian đầu vào G_X, vì nó chỉ phụ thuộc vào các token. Nói cách khác, full attention có thể được biểu diễn bằng một hàm kernel κ. Cụ thể, chúng ta viết lại Eq. 8 thành:

Z = [κ(X,G_W X^T)_{|{z}}^{A': \text{parameterized } G_X}] XW^V
≈ κ(X,X') X; (10)
= φ(X)φ(X)^T X (11)

trong đó κ là một hàm kernel trên các token đầu vào, và φ là feature map tương đương. Đối với W^V, Tsai et al. [39] đã chỉ ra rằng phép chiếu tuyến tính này là dư thừa và có thể dẫn đến suy giảm hiệu suất.

Thay vì học ba phép chiếu và tính toán ma trận full-attention, chúng ta có thể học hoặc sử dụng một hàm kernel tĩnh κ(·) để nắm bắt tương quan giữa các token. Thay vào đó, vì L≫D, việc áp dụng feature map φ(·) sẽ rẻ hơn nhiều so với việc sử dụng các hàm kernel.

5 Mô hình Chuỗi với Multilayer Perceptrons

Tolstikhin et al. [37] là người đầu tiên đề xuất rằng full-attention có thể được thay thế bằng việc trộn token học được chỉ bằng cách sử dụng multilayer perceptrons (MLPs), được gọi là MLP-Mixer (Mixer). Để đơn giản, các thủ thuật phổ biến như layer norms và skip connections được bỏ qua ở đây.

5

--- TRANG 6 ---
Mixer ban đầu được dành cho các tác vụ hình ảnh [37], vì vậy các token đầu vào là các patches hình ảnh được tuần tự hóa. Tuy nhiên, sơ đồ này có thể được tổng quát hóa cho bất kỳ tác vụ sequence-to-sequence nào [30]. Trong Mixer, tất cả các lớp self-attention trong kiến trúc Transformer được thay thế bằng các lớp MLP, mỗi lớp thực hiện hai thao tác trộn trên chiều channel (embedding) và chiều patch (sequence) tương ứng:

X'_{:t} := W_{p1} σ(W_{p1} X_{:t}) (token mixing) (12)
Z_{:t} := σ(X'_{t:} W_{c1}) W_{c2} (channel mixing) (13)

Kết hợp Eq. 12 và 13, chúng ta có:

Z_{L×D}: = (W_{p2} σ(W_{p1} X)) σ(W_{c1}) W_{c2}
= (W_{p2} σ(W_{p1})) X σ(W_{c1} W_{c2})
= W_p X W_c
= P_{mlp}(X|θ_{mlp}) (14)
= X̃_{mlp};

trong đó θ_{mlp} = {W_p ∈ R^{L×L}, W_c ∈ R^{D×D}}, là các trọng số học được với GELU trong quá trình trộn token và channel tương ứng. Mặc dù P_{mlp} không tĩnh, nó độc lập với chuỗi đầu vào vì X không tham gia vào θ_{mlp}.

6 Thay thế Attention bằng Fourier Transform

Tương tự như MLP-Mixer, Lee-Thorp et al. [30] đã đề xuất một sơ đồ trộn mới thay thế full-attention học được, đắt đỏ bằng một loạt các Discrete Fourier Transforms (DFTs) cố định, hiệu quả. Để đơn giản, các thủ thuật phổ biến như layer norms và skip connections được bỏ qua.

6.1 Cân nhắc Chuỗi bằng Twiddle Factors

Tất cả các lớp self-attention trong kiến trúc Transformer được thay thế bằng các lớp Fourier. Mỗi lớp Fourier thực hiện hai DFT: đầu tiên trên chiều embedding D (Eq. 15) và sau đó trên chiều chuỗi L của các token đầu vào (Eq. 16).

Z_{t,j} := ∑_{d=0}^{D-1} \exp\left(\frac{2πi}{D}dj\right) X_{t,d} (DFT đầu tiên) (15)

X̃_{t,j} := \text{Re}\left(∑_{k=0}^{L-1} \exp\left(\frac{2πi}{L}jk\right) Z_{t,k}\right) (DFT thứ hai) (16)

Cả Eq. 15 và 16 đều ở dạng trung bình có trọng số tương tự Eq. 3 và 7.

6.2 Trộn Tokens với Fourier Transform

Các dạng trung bình có trọng số của hai DFT có thể được viết lại bằng các ma trận Vandermonde tương ứng cho các căn của đơn vị đến một hệ số chuẩn hóa:

F_h := \frac{1}{\sqrt{D}} \begin{bmatrix} ω^0 & ω^0 & ω^0 & ω^0 \\ ω^0 & ω^1 & ω^2 & ω^{(D-1)} \\ ω^0 & ω^2 & ω^4 & ω^{2(D-1)} \\ \vdots & \vdots & \vdots & \vdots \\ ω^0 & ω^{(D-1)} & ω^{2(D-1)} & ω^{(D-1)(D-1)} \end{bmatrix}

F_s := \frac{1}{\sqrt{L}} \begin{bmatrix} ω^0 & ω^0 & ω^0 & ω^0 \\ ω^0 & ω^1 & ω^2 & ω^{(L-1)} \\ ω^0 & ω^2 & ω^4 & ω^{2(L-1)} \\ \vdots & \vdots & \vdots & \vdots \\ ω^0 & ω^{(L-1)} & ω^{2(L-1)} & ω^{(L-1)(L-1)} \end{bmatrix};

6

--- TRANG 7 ---
trong đó ω = exp{2πi}.

Với F_h và F_s, chúng ta đơn giản hóa Eq. 15 và 16 thành:

X'_{t:} := X_t F_h (DFT đầu tiên) (17)
Z_{:t} := \text{Re}\{F_s X'_{:t}\} (DFT thứ hai) (18)

Bằng cách kết hợp Eq. 17 và 18, chúng ta có:

Z_{L×D}: = \text{Re}\{F_s X F_h\} (19)
= P_{FT}(X) (20)
= X̃_{FT};

Từ Eq. 19, chúng ta có những quan sát sau. Đầu tiên, DFT dẫn đến việc trộn token X̃_{FT} trong cùng công thức như trộn attention X̃_{attn} từ Eq. 9. Tuy nhiên, P_{FT} là tĩnh (không được học) và độc lập với chuỗi đầu vào (Eq. 10). Thứ hai, bằng cách sử dụng Fast Fourier Transform (thuật toán Cooley-Tukey [10,16]), chi phí tính toán là O(L log L), với độ phức tạp không gian nhỏ hơn nhiều (vì các ma trận Vandermonde đối xứng có thể được tính toán/lưu trữ hiệu quả), so với full-attention O(L²). Hơn nữa, thứ tự áp dụng hai DFT không quan trọng. Cuối cùng, khác với các sơ đồ trộn sử dụng các lớp MLP, việc xếp chồng các lớp FT tương tự như chuyển đổi giữa miền "thời gian" và tần số.

7 Mô hình Chuỗi với State Space Models

Một nhánh khác của sơ đồ trộn được đề xuất bởi Gu et al. [18], sử dụng và tăng cường các State Space Models (SSMs) truyền thống từ lý thuyết điều khiển. Cụ thể, các tác giả sử dụng SSM tuyến tính bất biến theo thời gian (LTI) được tham số hóa bởi một tập hợp các ma trận có cấu trúc để tóm tắt lịch sử và ghi nhớ các phụ thuộc tầm xa.

7.1 Phép chiếu Đa thức Bậc cao

Điểm khởi đầu của dòng công việc này là ý tưởng sử dụng phép chiếu đa thức trực giao, bậc cao (HiPPO) để tóm tắt một biểu diễn tối ưu về mặt lý thuyết của tất cả các token trong quá khứ [18]. Toán tử HiPPO là một phép biến đổi hai bước: (1) trước tiên nó lấy một tín hiệu đầu vào một chiều đến thời điểm t, chiếu nó lên các đa thức cơ sở trực giao bậc N với trọng số đều (Legendre) hoặc phân rã theo hàm mũ (Laguerre), và (2) sau đó nó trích xuất các hệ số của các đa thức cơ sở như biểu diễn tốt nhất của quá khứ cho đến thời điểm hiện tại:

\text{hippo}(X_{L-1}|_{<t}) = \text{coeff}_{\text{proj}}^*(X) \overset{\text{mở rộng chiều}}{\rightarrow} x ∈ R^{L×N}; (21)

trong đó x là một ma trận chứa tất cả các trạng thái hệ thống của SSM trước thời điểm t. Nói cách khác, hàng t của ma trận trạng thái, x(t)^T ∈ R^{1×N}, tóm tắt lịch sử của chuỗi đầu vào trước thời điểm t.

Bằng cách tận dụng các ma trận chuyển tiếp trạng thái A và B được cấu trúc đặc biệt, việc tích phân phương trình vi phân thường (ODE) sau của SSM chứng minh kết quả SoTA trong việc tóm tắt và, do đó, tái tạo lịch sử của tín hiệu đầu vào [18]:

ẋ(t) := Ax(t) + Bu(t); (22)

trong đó đầu vào hệ thống u(t) là một token đơn của tín hiệu một chiều, tức là X_t. Lưu ý rằng A và B là các ma trận cố định, không đổi khi phương pháp được đề xuất lần đầu [18]. Tuy nhiên, các ma trận này có thể được học thông qua backpropagation, mặc dù lợi ích hiệu suất của nó không đáng kể [19]. Quá trình học này cũng phát sinh chi phí lớn, đặc biệt là để mô hình hóa không gian đặc trưng nhiều chiều, được cải thiện thông qua các kỹ thuật toán học khác nhau trong công việc tiếp theo [20, 21].

7.2 Phép chiếu Đa chiều

HiPPO có hai hạn chế chính: (1) Tín hiệu đầu vào bị hạn chế ở một chiều, và (2) Các ma trận A và B không được học. Gu et al. [19] đã phát triển Linear State-Space Layers (LSSLs) để giải quyết hai thách thức này.

7

--- TRANG 8 ---
Để làm việc với đầu vào đa chiều, LSSL áp dụng HiPPO độc lập trên mỗi chiều embedding d của chuỗi đầu vào và nối các chuỗi đầu ra D thành biểu diễn của SSM tại mọi thời điểm²:

ẋ_d(t)_{N×1} := Ax_d(t) + Bu_d(t)_{1×1} (23)
y_d(t)_{1×1} := Cx_d(t) + Du_d(t)_{1×1}:

Tổng quát hơn, đối với một chuỗi có độ dài L, với mọi t ∈ L, chúng ta có:

ẋ_d_{N×L} := Ax_d + B(ũ_d)^T_{1×L}
(ỹ_d)^T_{1×L} := Cx_d + D(ũ_d)^T_{1×L};

trong đó (ũ_d) là một cột của tín hiệu đầu vào X_{L×D}, và các ma trận A, B, C và D đều có thể học được thông qua backpropagation (through time).

Do đó, tại bất kỳ thời điểm t nào, ma trận x(t) ∈ R^{N×D} chứa tất cả các trạng thái hệ thống nội bộ của SSM, tức là các hệ số của các đa thức trực giao. Hơn nữa, tensor x ∈ R^{N×D×L} chứa tất cả các trạng thái hệ thống cho t ∈ [0, L]. Để hoàn thiện, kích thước bước cập nhật Δt cũng có thể học được cho việc rời rạc hóa bằng Bilinear transform (hiệu quả hơn về mặt thực nghiệm so với phương pháp Euler), tức là x_d(t) → x_d(t + Δt).

Mặc dù các ma trận tham số và kích thước bước có thể học được, quá trình huấn luyện là cấm đoán về mặt tính toán, một phần do việc mở rộng chiều đắt đỏ, tức là các đa thức trực giao. Do đó, chúng được cố định trong thực tế [19].

Để giảm chi phí hệ thống và làm cho việc huấn luyện hiệu quả hơn, các thủ thuật và tham số hóa đặc biệt đã được phát triển, ví dụ: S4 [20], S4D [21].

7.3 Cái nhìn Convolutional của SSM

Bất kỳ hệ thống động LTI nào cũng có thể được xem như convolution giữa tín hiệu đầu vào và hàm phản hồi xung của nó, và SSM được mô tả bởi Eq. 23 cũng vậy. Để đơn giản, chúng tôi bỏ chiều d và ma trận D (skip connection) trong phép suy diễn sau. (Biểu diễn hồi quy của SMM thú vị về mặt lý thuyết nhưng không khả thi về mặt thực tế do bản chất tuần tự của nó, vì vậy không được xem xét ở đây.)

² Theo implementation của họ: https://github.com/HazyResearch/state-spaces/blob/main/src/models/s4/lssl.py.

8

--- TRANG 9 ---
ẋ(t) : = Ax(t) + Bu(t)
ẋ(t) - Ax(t) = Bu(t)
e^{-tA}ẋ(t) - Ae^{-tA}x(t) = e^{-tA}Bu(t)
\frac{1}{dt}[e^{-tA}x(t)] = e^{-tA}Bu(t)
∫_0^t \frac{1}{dτ}[e^{-Aτ}x(τ)]dτ = ∫_0^t e^{-Aτ}Bu(τ)dτ
e^{-tA}x(t) - x(0) = ∫_0^t e^{-Aτ}Bu(τ)dτ
x(t) = e^{tA}x(0) + e^{tA}∫_0^t e^{-Aτ}Bu(τ)dτ
= e^{tA}x(0) + ∫_0^t e^{(t-τ)A}Bu(τ)dτ
= e^{tA}x(0) + ∫_0^t e^{(t-τ)A}B_{|{z}}_{\text{hàm cơ sở}} u(t-τ)dτ (24)
= e^{tA}x(0) + ∫_0^t h(t-τ)u(t-τ)dτ
= e^{tA}x(0) + (h ∗ u)(t); (25)

trong đó Eq. 24 từ tính giao hoán của convolution, và h(t) là hàm phản hồi xung đơn vị của SMM.

Thay thế Eq. 25 vào phương trình đầu ra cho ta:

y(t) : = C[\text{coeff}_{\text{proj}}^*(X_{:d})](t)
= Cx(t)
= C[e^{tA}x(0) + (h ∗ u)(t)]
= P_{ssm}(X_{:d}|θ_{ssm})
= X̃_{ssm};

trong đó θ_{ssm} = {A, B, C}.

Do đó, biểu diễn kết quả của chuỗi đầu vào là một tổ hợp tuyến tính của hàm phản hồi xung.

Quan sát rằng (1) P_{ssm} phụ thuộc vào X, mà tham gia vào θ_{ssm} thông qua A, và (2) θ_{ssm} có thể được học hoặc cố định như các ma trận có cấu trúc đặc biệt (sự khác biệt hiệu suất không đáng kể trong khi cố định θ_{ssm} hiệu quả hơn nhiều).

7.4 Phép chiếu Đa thức Hiệu quả hơn

Như đã thảo luận trong §7.2, một điểm nghẽn chính là việc mở rộng chiều khi SSM yêu cầu một tensor x ∈ R^{N×D×L} để biểu diễn toàn bộ trạng thái hệ thống. Một ý tưởng có thể là sử dụng Product Quantization [27] bằng cách đầu tiên phân vùng chuỗi đầu vào thành các không gian con và học một prototype nhỏ hơn (một xấp xỉ) trong mỗi không gian con trong lần pass đầu tiên của tập huấn luyện. Sau đó, chiếu toàn bộ không gian con lên các hàm cơ sở đa thức trực giao, thay vì làm điều đó cho từng cột/chiều. Phương pháp như vậy có thể được kết hợp với Locality Sensitive Hashing dựa trên (tree-based), cùng nhau có thể dẫn đến zero matmul operations trong thời gian suy luận [5].

9

--- TRANG 10 ---
8 Trộn Tokens với Convolution

Gần đây, Li et al. [31] đã đề xuất một kiến trúc convolutional thuần túy thay cho block full attention, đạt được cả chi phí hệ thống thấp hơn (nhanh hơn 15-50%) và chất lượng mô hình tương đương/cao hơn.

Nó sử dụng các tập tham số được nối với trọng số phân rã và áp dụng ba convolution sử dụng DFT thực (1) trên các tập tham số được nối, (2) trên chuỗi đầu vào, và (3) giữa hai cái (biến đổi nghịch đảo). Do đó, nó có thể được coi như một FNet có trọng số, có thể học được. Kết quả là, độ phức tạp thời gian của nó là O(L log L), tương tự FNet.

8.1 Chi phí Bộ nhớ của SGConv

Mặc dù các tác giả đã đề cập ngắn gọn rằng độ phức tạp bộ nhớ cũng là O(L log L), chúng tôi chỉ ra ở đây rằng nó có thể là O(L + log L) thay vào đó.

Cho một chuỗi đầu vào X_{L×D} và chiều kernel k, số lượng kernel s được tính như sau:

s := ⌈log_2(L/k)⌉ + 1:

Do đó, chúng ta cần s tập tham số có thể học được, mỗi tập tương ứng với một kernel:

W_K := {W^{(1)}_{k×D}, W^{(2)}_{k×D}, ..., W^{(s)}_{k×D}}:

W_K yêu cầu (s × k × D) ≈ O(log L) không gian tổng cộng.

Với các tham số trên, SGConv khởi tạo một kernel K được nối từ s sub-kernel cho mỗi forward pass:

K^{(i)} := \text{Interpolate}{W^{(i)}} ∈ R^{(k×2^{(i-1)})×D}
K := \text{concat}{K^{(1)}, K^{(2)}, ..., K^{(s)}} ∈ R^{L'×D};

trong đó L' = ∑_{i=1}^s k×2^{(i-1)} + k ≈ L + L.

Do đó, kernel K chiếm O(L) không gian. Vì một lớp SGConv cần một tập tham số W_K và một kernel được nối K, nó yêu cầu O(L + log L) bộ nhớ mỗi lớp.

Độ phức tạp không gian này nhỏ hơn so với S4 (O(L + N) trong đó N = 256) [20], và tương đồng với các xấp xỉ attention hiện có, ví dụ: Reformer (O(L + log L)) [29] và Performer (O(L)) [7].

9 Tác động của Độ dài Ngữ cảnh

Trong phần này, chúng tôi tiến hành thí nghiệm để điều tra tác động của việc thay đổi độ dài ngữ cảnh trong mô hình chuỗi. Một trong những ứng dụng của việc học ngữ cảnh dài là trên hình ảnh độ phân giải cao, ví dụ: fMRI và hình ảnh vệ tinh. Do đó, trước tiên chúng tôi nghiên cứu hành vi của Vision Transformers (ViT) [14] khi thay đổi độ dài chuỗi. Để làm điều này, chúng tôi pretrain một ViT base trên ImageNet (21k+) và fine-tune nó trên bốn dataset: CIFAR 10, CIFAR 100, EuroSAT [23], và So2Sat [49]. Hai dataset sau là các dataset vệ tinh có độ phân giải cao hơn. Tác vụ downstream cho cả bốn dataset là phân loại.

ViT phân vùng một hình ảnh thành các patches và coi mỗi patch như một token. Do đó, patches càng nhỏ thì ngữ cảnh mà mô hình có thể truy cập trong mỗi batch càng dài. Lưu ý rằng, mặc dù các phụ thuộc giữa các patches được bảo tồn giữa các patches, thông tin cấu trúc trong một patch bị phá hủy khi nó được tuần tự hóa trong quá trình embedding. Do đó chúng tôi mong đợi hiệu suất tốt hơn khi sử dụng kích thước patch nhỏ hơn vì nhiều thông tin cấu trúc được giữ lại. Ngoài ra, sự gia tăng hiệu suất này dự kiến sẽ phụ thuộc vào tác vụ, ví dụ: dự đoán một con chó cần ít chi tiết hơn nhiều so với việc xác định loại xe trong hình ảnh vệ tinh. Chúng tôi lặp lại thí nghiệm sáu lần trên cluster ETH Euler

10

--- TRANG 11 ---
Lưu ý rằng các trục y không bắt đầu từ zero.

[BIỂU ĐỒ: Bốn đồ thị hiển thị hiệu suất của mô hình ViT pretrained trên bốn dataset (cifar10, cifar100, eurosat, so2sat) khi thay đổi kích thước patch từ 10 đến 30. Độ chính xác được hiển thị trên trục y.]

Hình 1: Hiệu suất của mô hình ViT pretrained trên bốn dataset khi thay đổi kích thước patch của hình ảnh. Kích thước patch càng nhỏ, độ dài ngữ cảnh mà mô hình có thể truy cập trong mỗi batch càng dài.

cluster với hai GPU GeForce RTX 3090. Như mong đợi, chất lượng mô hình thực sự tăng khi kích thước patch nhỏ hơn (Hình 1), và độ lớn của sự gia tăng như vậy khác nhau theo tác vụ. Tuy nhiên, chúng tôi nhận thấy rằng cải thiện hiệu suất không đáng kể trong thí nghiệm này, và các xu hướng có xu hướng bằng phẳng ở cuối. Do đó cần có các điều tra sâu hơn với các tác vụ khác nhau và các mức tăng độ dài ngữ cảnh chi tiết hơn.

Để tránh bị ràng buộc bởi kiến trúc cố định của các mô hình pretrained, chúng tôi viết một Transformer vanilla cho các thí nghiệm sau để có thể thay đổi độ dài chuỗi theo ý muốn. Mô hình Transformer bao gồm ba lớp encoder blocks và một lớp MLP làm prediction head, decoder. Chúng tôi implement positional embedding được mô tả trong công trình gốc [40] và giữ nó autoregressive bằng cách implement attention masks. Mô hình được huấn luyện trên dataset WikiText-103 từ đầu sử dụng một GPU NVIDIA A100. Dataset chứa 103M tokens và có vocabulary size 98K. Tác vụ downstream là language modeling, và chúng tôi sử dụng perplexity làm metric hiệu suất. Lần này chúng tôi thay đổi độ dài chuỗi từ 8 đến 10K với kích thước bước log2. Tương tự như thí nghiệm được thực hiện bởi Wu et al. [44], chúng tôi giữ số lượng tokens mỗi batch không đổi (2^14) trong khi quét độ dài chuỗi. Để đạt được điều này, chúng tôi điều chỉnh động batch size length cho trước độ dài chuỗi. Bằng cách làm như vậy, mô hình có thể truy cập các độ dài ngữ cảnh khác nhau trong khi vẫn thấy cùng tổng số tokens trong mỗi batch.

Độ dài trung bình của các bài viết trong dataset WiKiText-103 là 3.6K từ [2], dự kiến sẽ là độ dài ngữ cảnh lý tưởng cho tác vụ này. Tuy nhiên, mô hình đạt hiệu suất đỉnh với độ dài chuỗi khoảng 128-512 (Hình 2), vì độ dài chuỗi này đã đạt đến khả năng

11

--- TRANG 12 ---
[BIỂU ĐỒ: Hai đồ thị hiển thị (a) Training perplexity và (b) Validation perplexity của mô hình vanilla Transformer trên WiKiText-103 với các độ dài ngữ cảnh khác nhau từ 8 đến 10K]

Hình 2: Training (a) và validation (b) loss của mô hình vanilla Transformer trên WiKiText-103 với các độ dài ngữ cảnh khác nhau.

[BIỂU ĐỒ: Đồ thị hiển thị thời gian huấn luyện mỗi batch của mô hình vanilla Transformer trên WiKiText-103, cho thấy thời gian giảm khi độ dài chuỗi tăng từ 8 đến 10K]

Hình 3: Thời gian huấn luyện mỗi batch của mô hình vanilla Transformer trên WiKiText-103.

của các mô hình Transformer truyền thống (ví dụ: BERT [13], GPT-2 [33]). Ngoài ra, chúng tôi quan sát thấy rằng thời gian huấn luyện giảm theo độ dài chuỗi (Hình 3). Vì chúng tôi giữ tổng số tokens mỗi batch không đổi, kết quả này minh họa rằng việc thực hiện nhiều attention passes nhỏ trên cùng số lượng tokens đắt hơn so với việc thực hiện một pass lớn hơn để tính toán các ma trận attention (lớn hơn) trên nhiều tokens hơn. Sự đánh đổi này tạo thành một bài toán tối ưu có ràng buộc với hiệu suất là mục tiêu và runtime là ràng buộc.

10 Kiến trúc cho Phụ thuộc Quy mô Triệu

Trong các phần trước, chúng tôi đã chứng minh rằng các mô hình ngữ cảnh dài hiện có có thể được xem như các sơ đồ trộn token khác nhau. Các sơ đồ này cố gắng làm cho mọi token có sẵn cho bất kỳ token nào khác trong cửa sổ ngữ cảnh và sau đó, tính toán trung bình có trọng số của ngữ cảnh (cùng với embedding ban đầu) để đạt được bias quy nạp. Chúng tôi cũng đã chứng minh sự đánh đổi giữa hiệu suất và hiệu quả khi thay đổi độ dài ngữ cảnh. Bảng 1 tóm tắt và so sánh ở mức vĩ mô các giải pháp hiện có cho vấn đề học ngữ cảnh dài, về độ dài ngữ cảnh tối đa mà mỗi mô hình có thể xử lý. Lưu ý rằng nó chỉ chứng minh tính khả thi của việc làm việc với độ dài ngữ cảnh như vậy (tức là bị ràng buộc bởi kiến trúc mô hình, và do đó, tài nguyên hệ thống), nhưng không phải chất lượng mô hình. Nói chung, khi ngữ cảnh trở nên lớn hơn, hiệu suất mô hình đầu tiên tăng (do truy cập nhiều ngữ cảnh hơn) và sau đó giảm nhanh khi độ dài vượt quá khả năng mô hình [2,44,20,18]. Do đó, mặc dù cảnh quan tối ưu hóa dường như gần như cạn kiệt, ngữ cảnh quy mô triệu vẫn xuất hiện là điểm nghẽn của nó.

12

--- TRANG 13 ---
Bảng tóm tắt này không đầy đủ.

[BẢNG: Hiển thị độ dài ngữ cảnh tối đa và danh mục giải pháp]
Độ dài Ngữ cảnh Tối đa | Danh mục Giải pháp
512-2K | Full attention (ví dụ: [13, 40])
65K | Approximated attention (ví dụ: [7, 42, 3]); Tối ưu hóa Memory I/O [12]
264K | Neural Turing Machine với multi-head attention [44]
1M* | Token mixing (ví dụ: [37, 30, 31]); State Space Models (ví dụ: [18, 20, 21])

Bảng 1: Phân loại và so sánh giữa các giải pháp hiện có về độ dài ngữ cảnh tối đa. *Độ dài ngữ cảnh 1M được đạt được trên các ví dụ toy (ví dụ: ghi nhớ chuỗi ngẫu nhiên), nhưng không phải với các tác vụ downstream thực tế.

10.1 Các Mô hình Thưa Khổng lồ với Tính toán Có điều kiện

Các mô hình thưa sử dụng tính toán có điều kiện [4] đã (tái) nổi lên như một hướng đầy hứa hẹn hướng tới khả năng mô hình khổng lồ, trong khi vẫn giữ chi phí hiệu quả ở mức kiểm soát. Mô hình thu hút nhiều sự chú ý gần đây nhất là sparsely-gated Mixture of Experts (MoE) [36]. Nó đã chỉ ra khả năng mở rộng đến hàng tỷ tham số, trong khi chỉ phát sinh một phần chi phí của việc sử dụng toàn bộ khả năng của chúng (ví dụ: [15,8,48]). Những kết quả như vậy là do thực tế rằng các mô hình này chỉ được kích hoạt một phần tại bất kỳ thời điểm nào. Việc kích hoạt một phần này mang lại một lợi ích quan trọng, đó là chuyên môn hóa mô hình - các phần khác nhau của mô hình chuyên môn hóa ở các tác vụ khác nhau. Đã được chỉ ra rằng các expert khác nhau trong mô hình sparse MoE có tính chuyên môn cao về cú pháp và/hoặc ngữ nghĩa trong các tác vụ NLP [36]. Hơn nữa, thành phần chịu trách nhiệm kích hoạt thưa thớt mô hình là "router" chọn expert(s) và chuyển tiếp tokens đến chúng. Do đó, thao tác định tuyến này sẽ chỉ kích hoạt một phần của mô hình được đại diện bởi expert(s). Router này thường được tạo từ các lớp MLP và học để chuyển tiếp token nào đến expert(s) nào. Quan trọng nhất, nó đã chứng minh hiệu ứng lọc/tỉa - loại bỏ các tokens dư thừa trong khi vẫn duy trì hiệu suất mô hình [15,48]. Tính năng này cho phép mô hình chỉ tập trung vào phần liên quan nhất của một ví dụ (ví dụ: đường viền của một con chó trong hình ảnh) và bỏ qua các phần tương đối không quan trọng (ví dụ: nền và các đối tượng khác trong hình ảnh), mà chúng tôi gọi là học tập tập trung.

10.2 Học Phụ thuộc Quy mô Triệu sử dụng Sparse Models

Sự đánh đổi chính khi xử lý các phụ thuộc tầm xa nằm giữa hiệu quả tài nguyên và lượng thông tin dài/ngắn hạn được giữ trong bộ nhớ, điều này quyết định mô hình học tập và khả năng mô hình. Trong trường hợp lý tưởng, mô hình sẽ lưu trữ tất cả lịch sử (và các phụ thuộc tương lai nếu không autoregressive) và đưa ra quyết định tương ứng. Đối với các phụ thuộc quy mô triệu, kịch bản lý tưởng như vậy không thực tế vì hai lý do: (1) nó sẽ tạo ra độ phức tạp ít nhất bằng lượng thông tin được lưu trữ về cả không gian và tính toán, và (2) thông tin được ghi nhớ trở nên ngày càng cũ khi quá trình học tiếp diễn vì những gì được lưu trữ là không gian ẩn chứ không phải các token thô [44].

Mặc dù các mô hình sparse MoE không được thiết kế cụ thể để mô hình hóa các phụ thuộc tầm xa, chúng tôi tin rằng chúng phù hợp vì những lý do sau. Đầu tiên, chuyên môn hóa cho phép tăng khả năng mô hình trong khi giữ chi phí hiệu quả (thời gian huấn luyện/suy luận) tương đối không đổi. Thứ hai, tập trung làm cho cơ chế attention có tính chọn lọc, tức là chỉ ghi nhớ thông tin liên quan thay vì tất cả dữ liệu lịch sử (và tương lai). Do đó, chúng tôi đề xuất một kiến trúc dựa trên sparse MoE (Hình 4).

Lấy cảm hứng từ cơ chế định tuyến trong sparse MoE, chúng tôi giới thiệu Selector để lọc tokens trước khi tính toán attention. Quá trình này nhằm giúp mô hình tập trung vào các phần liên quan của mẫu, điều này cũng giảm việc sử dụng tài nguyên từ đầu. Sau đó, các tokens được chọn được truyền qua các lớp MoE phân tán, trong đó mỗi expert nằm trên một thiết bị và xử lý một phần của chuỗi. Việc phân vùng như vậy cho phép các expert tính toán ma trận full-attention cho các tokens mà chúng nhận được và chuyên môn hóa ở các phần cụ thể của tác vụ. Tương tự như các Router trong các lớp MoE, Selector cũng có thể được huấn luyện với sự trợ giúp của các auxiliary losses được thêm vào training loss trong quá trình backpropagation. Các tokens được xử lý được sắp xếp lại và tổng hợp sau khi được xử lý bởi các lớp MoE. Ngoài ra, trộn toàn cục bằng phương tiện biến đổi Fourier nhanh để truy cập ngữ cảnh rộng hơn và xử lý up-sampling thông qua nội suy (kNN hoặc tuyến tính) để khôi phục

13

--- TRANG 14 ---
[SƠ ĐỒ: Kiến trúc hệ thống của mô hình thưa được đề xuất cho các phụ thuộc quy mô triệu, bao gồm Selector, MoE Layers với 4 experts trên 4 GPU, Global Mixing, và các thành phần khác]

Hình 4: Kiến trúc hệ thống của mô hình thưa được đề xuất cho các phụ thuộc quy mô triệu.

chiều của embedding có thể được áp dụng. Độ phức tạp lý thuyết là tuyến tính tiệm cận (và trong trường hợp xấu nhất là log tuyến tính) với độ dài chuỗi.

10.3 Hàm Mục tiêu

Mặc dù Selector có thể được huấn luyện với các auxiliary losses tương tự như các Router [36,8,15] trong các block MoE, nó không phải là lựa chọn ưa thích vì các losses này thường dựa trên trực giác và thường khó đánh giá hiệu quả của chúng. Thay vào đó, chúng tôi muốn huấn luyện mô hình end-to-end, không tách riêng Selector. Ngoài ra, vì mục tiêu chính của Selector là học cách tỉa/lọc các tokens không quan trọng (ví dụ: nền, các đối tượng khác trong hình ảnh), một tham số ngưỡng λ như một "núm xoay" hữu ích để kiểm soát tỷ lệ dropout, đó là:

s_λ,θ : X_{L×D} ↦ X'_{L'×D}; (26)

trong đó λ là ngưỡng tỉa, và L ≥ L'.

Với selector được định nghĩa, mục tiêu đọc:

\max_{θ, ℱ} \mathbb{P}(ỹ|Z = \frac{1}{M}\sum_{i=1}^M g_i f(s_{λ,θ}(X))); (27)

trong đó M là số lượng experts được ensemble, và g_i là trọng số gating.

Thay vào đó, mục tiêu (Eq. 27) có thể ở dạng đối kháng. Miladinović et al. [32] đã chứng minh tiềm năng của việc học một mô hình dropout thông qua công thức giống GAN. Chúng tôi sửa đổi mục tiêu theo cách tương tự bằng cách tạo một trò chơi max-min giữa Selector và predictor:

\max_{ℱ} \min_{θ} \mathbb{P}(ỹ|Z = \frac{1}{M}\sum_{i=1}^M g_i f(s_{λ,θ}(X))): (28)

Cụ thể, Selector (adversary) cố gắng loại bỏ càng nhiều tokens thông tin càng tốt, trong khi predictor vẫn muốn có likelihood cao hơn. Tuy nhiên, các thí nghiệm sơ bộ của chúng tôi cho thấy rằng mục tiêu đối kháng này thường tạo ra một Selector quá mạnh mà phần dự đoán cần được huấn luyện lại với s bị đóng băng.

10.4 Distributed Attention

Một yếu tố quan trọng của kiến trúc được đề xuất (Hình 4) là việc tính toán phân tán của ma trận attention. Trong tính toán attention tiêu chuẩn, toàn bộ ma trận được tạo ra trên một thiết bị đơn

14

--- TRANG 15 ---
[SƠ ĐỒ: Multi-head attention tiêu chuẩn với tất cả kích thước lấy từ các mô hình OPT-175B và GPT-3 175B]

Hình 5: Multi-head attention tiêu chuẩn với tất cả kích thước lấy từ các mô hình OPT-175B [47] và GPT-3 175B [6].

(Hình 5). Tuy nhiên, đối với ngữ cảnh quy mô triệu, chiều chuỗi lớn đến mức ma trận đầu vào X không thể vừa trong bộ nhớ thiết bị đơn và do đó phải được phân tán. Ví dụ, trong Hình 7, chúng tôi giả định chiều chuỗi tương ứng với số pixel trong hình ảnh 512px×512px = 262,144px. Lấy tất cả các chiều khác từ các mô hình OPT-175B [47] và GPT-3 175B [6], X phải được phân tán giữa N GPU trong cùng một node (trong trường hợp này, N = 4).

Thuật toán phân tán hoạt động như sau. Đầu tiên, chúng tôi chia ma trận thiết kế X theo chiều chuỗi thành N phân vùng {P_i}^N. Sau đó, chúng tôi nhân bản ma trận tham số theo chiều không thay đổi trên tất cả thiết bị. Mỗi thiết bị tính toán ma trận attention cho tất cả attention heads nhưng chỉ với các mẫu trong phân vùng của riêng chúng. Tiếp theo, chúng tôi sử dụng COSTA [28] để xáo trộn các phân vùng một cách hiệu quả để mỗi thiết bị có điểm attention cho tất cả mẫu nhưng chỉ với 1/N heads. Bước này chuẩn bị cho việc tính toán softmax yêu cầu tất cả ví dụ cho mỗi chiều embedding, và nó có thể được tính toán tuần tự bởi mỗi thiết bị. Cuối cùng, chúng tôi sử dụng COSTA để xáo trộn dữ liệu lần thứ hai để tính toán phép chiếu tuyến tính cuối cùng với W^0 được nhân bản trên mỗi thiết bị. Chúng tôi implement thuật toán này với NCCL MPI API từ NVIDIA trong Python.

Để kiểm tra tính khả thi của thuật toán này, chúng tôi mở rộng độ dài của chuỗi đầu vào của việc tính toán attention trên bốn GPU GeForce RTX 3090 (Hình 6). Kết quả là, với bốn GPU, thuật toán có thể mở rộng việc tính toán attention đến độ dài chuỗi gần 80K, gấp 40× độ dài tối đa mà implementation attention vanilla có thể xử lý, với độ phức tạp thời gian tăng gần tuyến tính.

15

--- TRANG 16 ---
[BIỂU ĐỒ: Hiệu suất forward pass của thuật toán distributed attention trên bốn GPU GeForce RTX 3090, cho thấy thời gian tăng tuyến tính theo độ dài chuỗi từ 0 đến 70,000, với đường kẻ đứt chỉ ra độ dài tối đa của vanilla attention (~2K)]

Hình 6: Hiệu suất forward pass của thuật toán distributed attention trên bốn GPU GeForce RTX 3090. Lưu ý rằng, không phân tán tính toán, độ dài chuỗi tối đa mà implementation vanilla attention có thể xử lý là khoảng 2K.

11 Kết luận

Trong nghiên cứu này, trước tiên chúng tôi phân loại và so sánh các giải pháp hiện có cho mô hình chuỗi dài. Bằng cách công thức hóa chúng trong một khuôn mẫu thống nhất về mặt toán học, chúng tôi chỉ ra bản chất chung giữa hầu hết các công trình trước đây: làm cho cả ngữ cảnh toàn cục và cục bộ có sẵn khi tính toán điểm attention thông qua các sơ đồ trộn token khác nhau. Tiếp theo, chúng tôi làm nổi bật sự đánh đổi giữa hiệu quả tài nguyên và lượng lịch sử dài/ngắn hạn được ghi nhớ trong các sơ đồ trộn như vậy. Để mô hình hóa các phụ thuộc quy mô triệu trong khi vẫn giữ việc sử dụng tài nguyên ở mức kiểm soát, chúng tôi sau đó đề xuất một hệ thống học phân tán lấy cảm hứng từ các mô hình sparse MoE có dung lượng lớn được đề xuất gần đây, nhằm khai thác hai tính năng chính của chúng: chuyên môn hóa mô hình và học tập tập trung. Như bước đầu tiên hướng tới xây dựng hệ thống này, chúng tôi đề xuất một thuật toán phân tán để tính toán ma trận attention cho các chuỗi quy mô triệu. Chúng tôi chứng minh trong thí nghiệm rằng thuật toán của chúng tôi có thể mở rộng việc tính toán attention gần 40× về độ dài chuỗi tối đa với độ phức tạp thời gian gần tuyến tính, so với implementation vanilla attention. Mặc dù vẫn còn nhiều việc phải làm, chúng tôi tin rằng nghiên cứu này là một bước quan trọng hướng tới mô hình hóa các phụ thuộc quy mô triệu.

16

--- TRANG 17 ---
[SƠ ĐỒ: Distributed multi-head attention với kích thước chuỗi được mở rộng, và tất cả các kích thước khác được lấy từ các mô hình OPT-175B và GPT-3 175B]

Hình 7: Distributed multi-head attention với kích thước chuỗi được mở rộng, và tất cả các kích thước khác được lấy từ các mô hình OPT-175B [47] và GPT-3 175B [6].

17

--- TRANG 18 ---
Tài liệu tham khảo

[1] J. Ainslie, S. Ontanon, C. Alberti, V. Cvicek, Z. Fisher, P. Pham, A. Ravula, S. Sanghai, Q. Wang, và L. Yang. Etc: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483, 2020.

[2] H. Bai, P. Shi, J. Lin, Y. Xie, L. Tan, K. Xiong, W. Gao, và M. Li. Segatron: Segment-aware transformer for language modeling and understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12526-12534, 2021.

[3] I. Beltagy, M. E. Peters, và A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[4] E. Bengio, P.-L. Bacon, J. Pineau, và D. Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.

[5] D. Blalock và J. Guttag. Multiplying matrices without multiplying. In International Conference on Machine Learning, pages 992-1004. PMLR, 2021.

[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

[7] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.

[8] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[9] J. Chung, C. Gulcehre, K. Cho, và Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

[10] J. W. Cooley và J. W. Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965.

[11] Z. Dai, H. Liu, Q. V. Le, và M. Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965-3977, 2021.

[12] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, và C. Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.

[13] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[15] W. Fedus, B. Zoph, và N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res, 23:1-40, 2021.

[16] M. Frigo và S. G. Johnson. The design and implementation of fftw3. Proceedings of the IEEE, 93(2): 216-231, 2005.

[17] A. Graves, G. Wayne, và I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.

[18] A. Gu, T. Dao, S. Ermon, A. Rudra, và C. Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 2020.

[19] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, và C. Ré. Combining recurrent, convolutional, and continuous-time models with linear state-space layers. Advances in neural information processing systems, 34, 2021.

[20] A. Gu, K. Goel, và C. Ré. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022.

[21] A. Gu, A. Gupta, K. Goel, và C. Ré. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022.

18

--- TRANG 19 ---
[22] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, và Y. Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908-15919, 2021.

[23] P. Helber, B. Bischke, A. Dengel, và D. Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.

[24] J. Ho, N. Kalchbrenner, D. Weissenborn, và T. Salimans. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.

[25] S. Hochreiter và J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.

[26] Z. Huang, W. Xu, và K. Yu. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991, 2015.

[27] H. Jegou, M. Douze, và C. Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117-128, 2010.

[28] M. Kabić, S. Pintarelli, A. Kozhevnikov, và J. VandeVondele. Costa: Communication-optimal shuffle and transpose algorithm with process relabeling. In High Performance Computing: 36th International Conference, ISC High Performance 2021, Virtual Event, June 24-July 2, 2021, Proceedings 36, pages 217-236. Springer, 2021.

[29] N. Kitaev, Ł. Kaiser, và A. Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.

[30] J. Lee-Thorp, J. Ainslie, I. Eckstein, và S. Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021.

[31] Y. Li, T. Cai, Y. Zhang, D. Chen, và D. Dey. What makes convolutional models great on long sequence modeling?, 2022.

[32] Đ. Miladinović, K. Shridhar, K. Jain, M. B. Paulus, J. M. Buhmann, và C. Allen. Learning to drop out: An adversarial approach to training sequence vaes. arXiv preprint arXiv:2209.12590, 2022.

[33] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[34] H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, và B. Dai. Combiner: Full attention transformer with sparse computation cost. Advances in Neural Information Processing Systems, 34: 22470-22482, 2021.

[35] A. Roy, M. Saffar, A. Vaswani, và D. Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021.

[36] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, và J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

[37] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in Neural Information Processing Systems, 34:24261-24272, 2021.

[38] H. Touvron, M. Cord, M. Douze, F. Massa, và A. Sablay-rolles. Hj egou,"training data-efficient image transformers & distillation through attention,". arXiv preprint arXiv:2012.12877, 2020.

[39] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, và R. Salakhutdinov. Transformer dissection: A unified understanding of transformer's attention via the lens of kernel. arXiv preprint arXiv:1908.11775, 2019.

[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[41] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, và K. Saenko. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision, pages 4534-4542, 2015.

[42] S. Wang, B. Z. Li, M. Khabsa, H. Fang, và H. Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

19

--- TRANG 20 ---
[43] K. Wu, H. Peng, M. Chen, J. Fu, và H. Chao. Rethinking and improving relative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10033-10041, 2021.

[44] Y. Wu, M. N. Rabe, D. Hutchins, và C. Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.

[45] R. Xu, X. Wang, K. Chen, B. Zhou, và C. C. Loy. Positional encoding as spatial inductive bias in gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13569-13578, 2021.

[46] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020.

[47] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[48] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. Dai, Z. Chen, Q. Le, và J. Laudon. Mixture-of-experts with expert choice routing. arXiv preprint arXiv:2202.09368, 2022.

[49] X. Zhu, J. Hu, C. Qiu, Y. Shi, H. Bagheri, J. Kang, H. Li, L. Mou, G. Zhang, M. Häberle, S. Han, Y. Hua, R. Huang, L. Hughes, Y. Sun, M. Schmitt, và Y. Wang. New: So2sat lcz42, 2019. URL https://mediatum.ub.tum.de/1483140.

20
