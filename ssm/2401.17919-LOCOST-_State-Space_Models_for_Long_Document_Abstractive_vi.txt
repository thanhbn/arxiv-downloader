LOCOST: Mô hình Không gian Trạng thái cho Tóm tắt Trừu tượng Tài liệu Dài

Florian Le Bronnec*,1,2,3, Song Duong*,1,6, Mathieu Ravaut3,4, Alexandre Allauzen2,
Nancy F. Chen3, Vincent Guigue5, Alberto Lumbreras6, Laure Soulier1, Patrick Gallinari1,6
1Sorbonne Université, CNRS, ISIR, F-75005 Paris, France
2Miles Team, LAMSADE, Université Paris-Dauphine, Université PSL, CNRS, 75016 Paris, France
3Institute of Infocomm Research (I2R), A-STAR, Singapore
4Nanyang Technological University, Singapore
5AgroParisTech, UMR MIA-PS, Palaiseau, France
6Criteo AI Lab, Paris, France

Tóm tắt
Mô hình không gian trạng thái là một lựa chọn thay thế có độ phức tạp thấp cho transformer trong việc mã hóa các chuỗi dài và nắm bắt các phụ thuộc dài hạn. Chúng tôi đề xuất LOCOST: một kiến trúc mã hóa-giải mã dựa trên mô hình không gian trạng thái cho việc sinh văn bản có điều kiện với đầu vào ngữ cảnh dài. Với độ phức tạp tính toán O(LlogL), kiến trúc này có thể xử lý các chuỗi dài hơn đáng kể so với các mô hình tiên tiến dựa trên các mẫu attention thưa thớt. Chúng tôi đánh giá mô hình của mình trên một loạt các tác vụ tóm tắt trừu tượng tài liệu dài. Mô hình đạt được mức hiệu suất tương đương 93-96% so với các sparse transformer hiệu suất cao nhất cùng kích thước trong khi tiết kiệm tới 50% bộ nhớ trong quá trình huấn luyện và tới 87% trong quá trình suy luận. Ngoài ra, LOCOST xử lý hiệu quả các đầu vào vượt quá 600K token tại thời điểm suy luận, thiết lập kết quả tiên tiến mới về tóm tắt toàn bộ cuốn sách và mở ra những góc nhìn mới cho việc xử lý đầu vào dài.

1 Giới thiệu
Ngày nay, việc thiết kế các mô hình hiệu quả cho văn bản dài vẫn là một thách thức mở mặc dù có những tiến bộ gần đây được đạt được trong xử lý ngôn ngữ tự nhiên (NLP). Việc giới thiệu kiến trúc transformer (Vaswani et al., 2017) thực sự đã đem lại một bước tiến lớn về hiệu suất và khả năng mở rộng cho việc sinh văn bản. Tuy nhiên, độ phức tạp bậc hai theo độ dài đầu vào vẫn hạn chế việc áp dụng các mô hình được tiền huấn luyện lớn cho văn bản dài. Ví dụ, BERT (Devlin et al., 2019) và BART (Lewis et al., 2020) bị giới hạn ở kích thước ngữ cảnh lần lượt là 512 và 1024 token, tương đương với 2-3 đoạn văn bản tiêu chuẩn.

Để giảm thiểu vấn đề này, một cách tiếp cận đơn giản là tận dụng các mẫu sparse-attention (Child et al., 2019) để xử lý tốt hơn các văn bản dài. Làm ví dụ quan trọng, Guo et al. (2022) và Zaheer et al. (2020) đã mở rộng khả năng ngữ cảnh của các mô hình mã hóa-giải mã (Raffel et al., 2020; Zhang et al., 2020) và cho thấy sự gia tăng mạnh mẽ trong hiệu suất tóm tắt văn bản dài, thúc đẩy hành trình tích hợp các ngữ cảnh dài hơn. Tuy nhiên, trong thực tế, ngay cả những sparse-transformer tốt nhất cũng cần tài nguyên tính toán nặng để xử lý các chuỗi có độ dài lớn hơn 8K token (xem Hình 4).

Mô hình không gian trạng thái sâu (SSM) (Gu et al., 2022b) đã được đề xuất cho việc xử lý chuỗi, với độ phức tạp O(LlogL), ban đầu cho thị giác máy tính và âm thanh và gần đây hơn cho văn bản. Kiến trúc tái diễn của chúng được thiết kế để nắm bắt các phụ thuộc tầm xa (Gu et al., 2020). Cho đến nay, các ứng dụng của chúng đã bị hạn chế ở việc sinh tự hồi quy vô điều kiện, tức là với decoder-only (Fu et al., 2023; Goel et al., 2022); hoặc phân loại chuỗi, tức là với encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Việc giải quyết sinh văn bản có điều kiện với SSM như yêu cầu cho tóm tắt vẫn chưa được khám phá.

Trong bài báo này, chúng tôi đề xuất LOCOST, một kiến trúc mã hóa-giải mã để khám phá hiệu suất của SSM cho các tác vụ sinh văn bản có điều kiện, thông qua góc nhìn của tóm tắt trừu tượng. Chúng tôi chứng minh rằng SSM có thể cạnh tranh với các mô hình dựa trên transformer trong khi giảm mạnh yêu cầu bộ nhớ của chúng. Chúng tôi lựa chọn thiết kế kiến trúc nhẹ, có thể so sánh với các transformer cơ bản trung bình (khoảng 250M tham số) để xử lý các chuỗi cực dài trên tài nguyên tính toán tiêu chuẩn. Các thí nghiệm của chúng tôi với các chuỗi cực dài đã đạt được kết quả tiên tiến trên BookSum-Book đầy thử thách. Với sự gia tăng lên tới 2 điểm trong điểm ROUGE trung bình so với các baseline sparse attention, mô hình của chúng tôi có thể xử lý toàn bộ cuốn sách, không cắt ngắn, và trên một GPU duy nhất. Đóng góp của chúng tôi gồm ba khía cạnh:

• Chúng tôi đề xuất một kiến trúc mã hóa-giải mã mới dựa trên mô hình không gian trạng thái. Bằng cách bỏ qua cơ chế self-attention được sử dụng trong transformer, mô hình có độ phức tạp O(LlogL) thay vì O(L²) như trong transformer truyền thống.

• So với các sparse transformer hiệu suất tốt nhất cùng kích thước, mô hình đạt được 93-96% hiệu suất tốt nhất trên các tác vụ tóm tắt trừu tượng tài liệu dài khác nhau trong khi hiệu quả hơn tới 50% về bộ nhớ trong quá trình huấn luyện và tới 87% tại thời điểm suy luận, xem Hình 1.

• Mô hình có thể xử lý toàn bộ các chuỗi đầu vào lên tới 600K token, một độ dài nằm ngoài tầm với của các sparse transformer. Điều này cho phép mô hình đạt được trạng thái tiên tiến mới trên tác vụ tóm tắt toàn bộ cuốn sách đầy thử thách.

Theo hiểu biết tốt nhất của chúng tôi, đây là mã hóa-giải mã đầu tiên hoạt động cạnh tranh với sparse transformer mà không có attention trong encoder. Hơn nữa, công trình này đại diện cho nỗ lực thành công đầu tiên trong việc xử lý các văn bản cực dài ví dụ như toàn bộ cuốn sách mà không cắt ngắn, tất cả trong một lần duy nhất. Mô hình được đề xuất mở ra những góc nhìn mới để giải quyết văn bản dài với ít tài nguyên hơn.

2 Công trình liên quan
Trong phần này, trước tiên chúng tôi xem xét các transformer hiệu quả về bộ nhớ và các lựa chọn thay thế hiện có cho cơ chế attention. Sau đó, chúng tôi thảo luận tài liệu gần đây về mô hình không gian trạng thái.

Hiệu quả bộ nhớ cho transformer. Việc giảm tiêu thụ bộ nhớ của transformer là một lĩnh vực nghiên cứu tích cực. Tối ưu hóa ở cấp độ phần cứng (Dao et al., 2022) đã giúp cải thiện việc mở rộng tính toán attention trên các GPU gần đây. Một hướng công việc xem xét các transformer tăng cường bằng truy xuất, như (Borgeaud et al., 2022; Wang et al., 2023), sử dụng các mô-đun bổ sung để nâng cao backbone mô hình hóa ngôn ngữ. Mặc dù quan trọng trong việc phát triển kiến trúc hiệu quả về bộ nhớ, chúng tôi coi hai chủ đề cuối này là trực giao với công việc của chúng tôi tập trung vào kiến trúc của các mô hình.

Tài liệu phong phú tập trung vào việc điều chỉnh kiến trúc của các mô hình cho đầu vào dài. Vì độ phức tạp tính toán của attention đến từ việc tính toán ma trận self-attention, một cách đơn giản để giảm chi phí của nó là xấp xỉ nó bằng cách sử dụng các mẫu sparse-attention. Các mẫu này thường kết hợp sự kết hợp của attention cục bộ và một tập hợp các token được chọn cẩn thận. Ví dụ, ngoài các token toàn cục, BigBird (Zaheer et al., 2020) xem xét các token ngẫu nhiên, trong khi LSG (Condevaux và Harispe, 2023) xem xét các token thưa thông qua chiến lược sparsification khác nhau. LongT5 (Guo et al., 2022) chia chuỗi thành các khối và tính trung bình biểu diễn của chúng, điều này cho số lượng token toàn cục bằng số lượng khối. Một tổng quan về độ phức tạp của các sparse-transformer khác nhau có thể được tìm thấy trong Bảng 1.

Ngược lại, chúng tôi đề xuất một kiến trúc thay thế, hiệu quả tính toán, mà không cần các khối self-attention tốn kém cũng không cần các mẫu sparse-attention.

Transformer không attention. Một số biến thể của transformer đã tránh cơ chế attention tiêu chuẩn. Ví dụ Katharopoulos et al. (2020); Hua et al. (2022) xấp xỉ độ tương tự softmax trong attention bằng một tính toán hiệu quả hơn. Gần đây hơn, các kiến trúc trộn được giới thiệu trong (Liu et al., 2021). Chúng là thành phần chính của mô hình FNet (Lee-Thorp et al., 2022), một encoder thay thế self-attention bằng Biến đổi Fourier Rời rạc (DFT). FNet có độ phức tạp O(LlogL) và là mô hình encoder-only, do đó bị hạn chế ở các tác vụ phân loại và hồi quy.

Mô hình được đề xuất của chúng tôi cũng bỏ qua attention trong encoder, đạt được cùng độ phức tạp tính toán như các encoder như FNet, trong khi là một mô hình đa năng hơn nhiều, được thiết kế đặc biệt cho sinh văn bản có điều kiện.

Mô hình không gian trạng thái (SSM). Các triển khai học sâu của SSM bao gồm các kiến trúc mới nổi, được trình bày lần đầu trong (Gu et al., 2020). Các kiến trúc này đặc biệt hấp dẫn cho việc xử lý chuỗi dài do độ phức tạp giảm so với transformer, và các đảm bảo lý thuyết mạnh hơn so với RNN (Gu et al., 2022b), chi tiết hơn trong Phần 3. Trong các ứng dụng thực tế, SSM đã tìm thấy thành công trong cả phân loại và sinh tự hồi quy vô điều kiện cho mô hình hóa ngôn ngữ. Gu et al. (2022b) đề xuất một mô hình phân loại cải thiện đáng kể benchmark Long-Range Arena (Tay et al., 2021), bao gồm các tác vụ phân loại liên quan đến hình ảnh, chuỗi tổng hợp và văn bản. Các nghiên cứu khác đã áp dụng SSM cho phân loại video (Nguyen et al., 2022) và phân loại văn bản (Wang et al., 2022). Về mô hình hóa ngôn ngữ, nhiều nhà nghiên cứu đã tận dụng công thức causal tự nhiên của SSM, sử dụng kiến trúc decoder-only cho các tác vụ như sinh âm thanh (Goel et al., 2022) và, gần đây hơn, mô hình hóa ngôn ngữ tự hồi quy (Fu et al., 2023).

Trong công việc này, chúng tôi giải quyết tác vụ thách thức hơn của sinh văn bản có điều kiện và nghiên cứu hiệu suất của SSM, được sử dụng như một kiến trúc mã hóa-giải mã, trên tóm tắt trừu tượng tài liệu dài. Với kiến trúc được đề xuất của chúng tôi, chúng tôi chứng minh khả năng của mô hình trong việc xử lý các chuỗi đầu vào lên tới 600K token, trong khi cạnh tranh với sparse-transformer trên tóm tắt trừu tượng tài liệu dài.

3 Nền tảng
Để ngữ cảnh hóa, chúng tôi tận dụng mô hình không gian trạng thái thay vì self-attention. Trong suốt bài báo, L biểu thị độ dài chuỗi, H là chiều nhúng và N là chiều của trạng thái ẩn không gian trạng thái (sẽ được giới thiệu trong Phần 3). Trước khi đi sâu vào mô hình của chúng tôi trong Phần 4, chúng tôi mô tả bên dưới các thành phần chính của kiến trúc không gian trạng thái và elaborately về tiềm năng của chúng cho việc xử lý chuỗi dài.

Mô hình không gian trạng thái. Đối với đầu vào một chiều u = (u₀, ..., uₗ₋₁) trong ℝᴸ, SSM sâu (Gu et al., 2022b) dựa trên phương trình tái diễn:

{
xⱼ₊₁ = Axⱼ + buⱼ₊₁,
yⱼ₊₁ = c⊤xⱼ₊₁ + duⱼ₊₁,                    (1)

trong đó xⱼ là trạng thái ẩn SSM và yⱼ là đầu ra của SSM. Ma trận trạng thái A ∈ ℝᴺˣᴺ mang và biến đổi trạng thái ẩn qua các lần lặp cùng với b ∈ ℝᴺ, c ∈ ℝᴺ, và d ∈ ℝ là các tham số được học.

Convolution không gian trạng thái. Bằng cách mở ra sự tái diễn ở trên, chuỗi đầu ra y ∈ ℝᴸ có thể được biểu diễn như: yⱼ = Σₗ₌₀ʲ c⊤Aʲ⁻ᴸb uₗ + duⱼ, cho tất cả l trong {1, ..., L}. Gọi * là toán tử convolution causal (chi tiết về toán tử này trong Phụ lục A). Sau đó, chúng ta có thể định nghĩa một kernel convolution κ ∈ ℝᴸ phụ thuộc vào A, b, c. Một lớp SSM do đó được tham số hóa bởi A, b, c, d thông qua κ và đầu ra của nó được định nghĩa bởi y như trong phương trình sau:

y = κ * u + du,
κ = (c⊤b, c⊤Ab, ..., c⊤Aᴸ⁻¹b).                (2)

Đối với u đa chiều trong ℝᴸˣᴴ, chúng ta đơn giản tính H convolution với một kernel κₕ cho mỗi chiều.

Hiệu quả SSM. Do sự phụ thuộc thời gian tuyến tính giữa các trạng thái ẩn, như được hiển thị trong Phương trình (1), chúng ta có thể tính toàn bộ đầu ra y trực tiếp như một convolution, mà không lặp lại theo chiều thời gian, trái ngược với RNN. Một triển khai ngây thơ của (2) sẽ gây ra độ phức tạp bậc hai trong độ dài đầu vào L, phù hợp với độ phức tạp của transformer và do đó cấm kỵ cho các chuỗi dài. Tuy nhiên, nhờ FFT, tính toán này có thể được thực hiện trong O(LlogL) (xem Phụ lục A để biết thêm chi tiết).

4 Mô hình
Trong phần này, chúng tôi trình bày mô hình LOCOST. Đầu tiên chúng tôi giới thiệu mô hình không gian trạng thái sâu hai chiều, sau đó chỉ ra cách sử dụng nó để cho phép ngữ cảnh hóa toàn cục của các token. Sau đó, chúng tôi trình bày kiến trúc của lớp LOCOST với một ngữ cảnh hóa hiệu quả có thể được sử dụng như một thay thế drop-in cho cơ chế self-attention trong transformer.

4.1 Nắm bắt ngữ cảnh cục bộ và toàn cục
Trực giác. Trong SSM sâu, thông tin từ các token trước đó chảy đến token hiện tại thông qua các trạng thái ẩn x. Góc nhìn convolution cung cấp một góc độ khác: mỗi đầu ra yⱼ là một tổng có trọng số của các token trước đó u₀, ..., uⱼ, có trọng số được cho bởi κ.

Ngữ cảnh hóa hai chiều. Để tổng hợp thông tin từ cả hai hướng, chúng tôi xem xét convolution hai chiều. Một kernel đầu tiên, ←κ thực hiện convolution causal thông thường ←κ * u. Một kernel thứ hai →κ được sử dụng để tính cross-correlation với u. Kết quả của hai phép toán này được tổng lại (tương tự như bi-recurrent encoder). Phép toán tổng thể được mô tả bởi phương trình sau:

yⱼ = Σₗ≤ⱼ ←κⱼ₋ₗ ⊙ uₗ + Σₗ≥ⱼ →κₗ₋ⱼ ⊙ uₗ + d ⊙ uⱼ
  = BiSSM(U)ⱼ.                                    (3)

Trong phương trình này, U ∈ ℝᴸˣᴴ là ma trận nhúng của văn bản đầu vào: (u₀, ..., uₗ₋₁). Các kernel →κ, ←κ được tính như trong Phương trình (2), với các tham số tương ứng (→A, →c, →b) và (←A, ←c, ←b). Tích từng phần tử được ký hiệu bởi ⊙ và chúng tôi xem xét đầu vào đa chiều, với một kernel cho mỗi chiều.

Đầu ra yⱼ bây giờ được ngữ cảnh hóa như một tổng có trọng số của các đầu vào trước đó u≤ⱼ và tiếp theo u≥ⱼ. Đối với đầu vào vô hướng, nhiều insight hơn về việc một đầu vào vô hướng uₗ đóng góp bao xa trong tương lai hoặc trong quá khứ cho đầu ra vô hướng yⱼ được cho bởi các bán kính phổ ρ(→A) và ρ(←A). Thực vậy, độ nhạy của một đầu ra yⱼ đối với một đầu vào uₗ được giới hạn bởi đại lượng sau:

∂yⱼ/∂uₗ ≤ {
  ρ(←A)ʲ⁻ᴸ|←c⊤←b| nếu l < j,
  ρ(→A)ᴸ⁻ʲ|→c⊤→b| nếu l > j.

Đối với đầu vào đa chiều, việc sử dụng một kernel không gian trạng thái cho mỗi chiều cho phép điều chỉnh tinh tế các bán kính phổ một cách độc lập cho từng chiều. Một giá trị nhỏ tương ứng với mô hình hóa ngữ cảnh cục bộ, trong khi một giá trị lớn nắm bắt ngữ cảnh toàn cục.

Một số trọng số kernel tương ứng của convolution này có thể được hình dung trên Hình 3. Một hình dung hoàn chỉnh hơn có thể được tìm thấy trong Phụ lục C.

4.2 Kiến trúc
Encoder. Encoder của chúng tôi bao gồm một stack các lớp LOCOST, được minh họa trong Hình 2a. Nó được tính như sau:
• Ma trận nhúng U ∈ ℝᴸˣᴴ trước tiên được chiếu lên Q, V ∈ ℂᴸˣᴴ.
• V được ngữ cảnh hóa thông qua BiSSM.
• Một phép nhân pointwise Q ⊙ BiSSM(V) hoạt động như một cổng đầu tiên trước khi truyền đầu ra qua một lớp feedforward.
• Lớp feedforward này sử dụng một cơ chế gating thứ hai (xem Hình 2b). Đối với thành phần này, chúng tôi sử dụng gated GeLU đã được chứng minh là hiệu quả bởi Shazeer (2020).

Kiến trúc của lớp LOCOST (Hình 2a) giống với kiến trúc của một lớp transformer ngoại trừ việc cơ chế self-attention được thay thế bằng một mô hình không gian trạng thái hai chiều có cổng. Chúng tôi theo Gu et al. (2022a) cho việc tham số hóa và khởi tạo các mô hình không gian trạng thái (chi tiết hơn trong Phụ lục E).

Decoder. Vì trọng tâm của chúng tôi là tóm tắt đầu vào dài, độ dài đầu ra sinh ra rất ngắn so với đầu vào. Để giải mã, chúng tôi theo thực hành của các kiến trúc hiệu quả khác (Zaheer et al., 2020; Beltagy et al., 2020; Guo et al., 2022) và sử dụng một decoder transformer vanilla được trang bị self- và cross-attention dày đặc. Một mô tả đầy đủ về các siêu tham số của mô hình được cung cấp trong Phụ lục B.

Độ phức tạp. Lớp LOCOST mất O(H²L + HNL + HLlogL) thời gian và O(HNL) không gian để tính toán. Chúng tôi tham khảo Phụ lục D để biết thêm chi tiết.

5 Thí nghiệm
Để xác thực thí nghiệm của chúng tôi, chúng tôi tập trung vào tác vụ tóm tắt trừu tượng tài liệu dài vì nó đại diện cho một bài toán sinh có điều kiện điển hình với yêu cầu đầu vào dài.

5.1 Thiết lập thí nghiệm
Cách tiếp cận. Chúng tôi đánh giá LOCOST theo cách tiếp cận tiền huấn luyện rồi fine-tuning cổ điển. Để fine-tuning, chúng tôi sử dụng các phân chia train, validation và test chính thức của mỗi dataset. Chúng tôi huấn luyện tất cả các mô hình cho đến khi hội tụ và chọn mô hình tốt nhất dựa trên Mean ROUGE validation (trung bình của ROUGE-1/2/LSum) để đánh giá test.

Metric. Chúng tôi đánh giá LOCOST với cả metric dựa trên tham chiếu và không tham chiếu. Để đánh giá tóm tắt dựa trên tham chiếu, chúng tôi sử dụng các metric tóm tắt n-gram overlap truyền thống ROUGE-1/2/Lsum (Lin, 2004). Chúng tôi tính trung bình chúng thành một điểm duy nhất để so sánh với các baseline khác. Chúng tôi cũng báo cáo BERTScore (BS) (Zhang* et al., 2020), một metric dựa trên mô hình. Để đánh giá không tham chiếu, chúng tôi báo cáo điểm BLANC (BL) (Vasilyev et al., 2020), một metric đã được chứng minh có tương quan tốt với đánh giá của con người. Chúng tôi cũng đánh giá throughput (mẫu mỗi giây) và việc sử dụng bộ nhớ (MiB của GPU RAM) của LOCOST so với các sparse transformer tiên tiến khác.

Suy luận. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi cố ý ưu tiên sự đơn giản và lựa chọn greedy decoding.

5.2 Tiền huấn luyện
Mục tiêu tiền huấn luyện. Để tiền huấn luyện mô hình, chúng tôi tận dụng mục tiêu tiền huấn luyện không giám sát gap-sentences generation (GSG), được giới thiệu bởi PEGASUS (Zhang et al., 2020) và phù hợp tốt cho sinh chuỗi-tới-chuỗi. Không như các mục tiêu tiền huấn luyện BART (Lewis et al., 2020) hoặc T5 (Raffel et al., 2020), GSG trang bị cho mô hình khả năng tóm tắt zero-shot. GSG đã được áp dụng thành công bởi các mô hình sinh tiếp theo như LongT5 (Guo et al., 2022) và PEGASUS-X (Phang et al., 2022). Cụ thể, một tài liệu D được chia thành M câu: D = {s₁, ..., sₘ}. Cho một tỷ lệ α, GSG sau đó xác định K = ⌊αM⌋ câu từ D tối đa hóa ROUGE-1 (ký hiệu R-1) với phần còn lại của tài liệu:

U = arg top-K_j R-1[⋃_{i≠j}{s_i}, s_j]                (4)

Tập con kết quả U ⊆ {1, ..., M} chia tài liệu thành một pseudo summary Ŷ = {s_i}_{i∈U} và một pseudo-source D̂ = {s_i}_{i∉U}, được sử dụng cho tiền huấn luyện với loss cross-entropy tiêu chuẩn.

Dữ liệu tiền huấn luyện. Chúng tôi tiền huấn luyện mô hình độc quyền trên dataset C4 (Raffel et al., 2020), trong BF16 cho 1M bước, sử dụng độ dài chuỗi đầu vào 4,096 và độ dài chuỗi đầu ra 910.

Tối ưu hóa tiền huấn luyện. Bộ lập lịch learning rate mà chúng tôi sử dụng giống hệt T5, sử dụng hàm căn bậc hai nghịch đảo, với các bước warm-up được đặt thành 10,000. Chúng tôi đặt tỷ lệ GSG α = 0.2 và không sử dụng dropout trong giai đoạn này. Chúng tôi theo sát cùng tiền huấn luyện như LongT5 (Guo et al., 2022).

5.3 Fine-tuning
Dataset fine-tuning. Chúng tôi đánh giá LOCOST trên một loạt các tác vụ tóm tắt trừu tượng đầu vào dài. Một bảng thống kê cho tất cả các dataset có thể được tìm thấy trong Phụ lục F.

• arXiv (Cohan et al., 2018) Các bài báo được trích xuất từ arXiv sử dụng tài liệu nội dung chính làm chuỗi đầu vào và abstract làm chuỗi đích.

• PubMed (Cohan et al., 2018) Tương tự như arXiv, nhưng các bài báo đến từ PubMed, một cơ sở dữ liệu y tế.

• GovReport (Huang et al., 2021) Một dataset tóm tắt tài liệu dài của các báo cáo chính phủ Hoa Kỳ với tóm tắt điều hành của chúng.

• SummScreenFD (Chen et al., 2022) Một dataset tóm tắt tài liệu dài của bản ghi chép loạt TV của toàn bộ tập phim với tóm tắt được viết bởi con người cho các tập phim.

• BookSum (-Chapter & -Book) (Kryscinski et al., 2022) Một bộ sưu tập các chương từ nhiều cuốn sách khác nhau với một tóm tắt cho mỗi chương. Chúng tôi cũng xem xét phiên bản cấp độ sách trong đó mô hình phải tóm tắt toàn bộ cuốn sách.

Tối ưu hóa fine-tuning. Chúng tôi fine-tune trong BF16 sử dụng learning rate không đổi 5×10⁻⁴ và tỷ lệ dropout 0.1 cho tất cả các dataset. Chúng tôi thí nghiệm với độ dài từ 4,096 đến 32,768 cho đầu vào và 512 cho đầu ra, ngoại trừ GovReport và BookSum-Book nơi chúng tôi sử dụng 1024.

Baseline. Chúng tôi xem xét cả sparse transformer cạnh tranh, bao gồm LED (Beltagy et al., 2020), BigBird (Zaheer et al., 2020), LongT5 (Guo et al., 2022) và LSG (Condevaux và Harispe, 2023), cũng như các encoder-decoder dày đặc như BART (Lewis et al., 2020), T5 (Raffel et al., 2020) và PEGASUS (Zhang et al., 2020). Để so sánh công bằng, chúng tôi chỉ so sánh với các kiến trúc sparse transformer có kích thước tương đương (khoảng 250M tham số).

5.4 Kết quả
Tóm tắt đầu vào dài. Bảng 2 và 3 trình bày kết quả thí nghiệm của chúng tôi. Trên tất cả các dataset, LOCOST đạt tới 96% Mean ROUGE tiên tiến trong khi hiệu quả hơn tới 3 lần về bộ nhớ so với mô hình tốt nhất LongT5 trong cả quá trình huấn luyện và suy luận cho đầu vào dài 16K, ví dụ trên GovReport hoặc SummScreenFD. Mô hình cũng hiệu quả gấp đôi so với transformer local-attention LED và hiệu quả hơn tới 17 lần so với dense transformer BART tại thời điểm suy luận.

LOCOST cải thiện đáng kể Mean ROUGE so với LED và BigBird trên tất cả các dataset trong khi hoạt động cạnh tranh đối với LSG. Trên tất cả các dataset, kết quả cho LongT5 và LED đã được thu được bằng cách fine-tuning từ các checkpoint được tiền huấn luyện, theo các cấu hình được khuyến nghị trong (Guo et al., 2022) và (Beltagy et al., 2020) tương ứng. Kết quả cho BigBird đã được báo cáo từ bài báo gốc. Kết quả LSG được thu được từ việc đánh giá các checkpoint được fine-tune công khai trên arXiv và PubMed và từ fine-tuning của chúng tôi trên BookSum-Chapter. Kết quả GovReport và SummScreenFD được báo cáo từ bảng xếp hạng test SCROLLS (Shaham et al., 2022).

Throughput và Sử dụng bộ nhớ. Chúng tôi đo tiêu thụ bộ nhớ của T5, LED, LongT5 và LOCOST trên độ dài đầu vào từ 1K đến 500K token, tại thời điểm huấn luyện và suy luận. Kết quả được trình bày trên Hình 4. So với LongT5, baseline hiệu suất tốt nhất, LOCOST có thể xử lý các chuỗi dài hơn tới 2× trong quá trình huấn luyện và 16× tại thời điểm suy luận. Điều này cũng tương quan với throughput cao hơn trong cả quá trình huấn luyện và suy luận, như được hiển thị trong Bảng 4.

Đánh giá định tính: Ưu tiên GPT-3.5. Vì các văn bản đầu vào của chúng tôi rất dài, việc thực hiện đánh giá dựa trên con người đầy đủ sẽ rất tốn kém và mất thời gian. Thay vào đó, chúng tôi thực hiện đánh giá con người mô phỏng bằng GPT-3.5*. Thực hành này đã được sử dụng và đã cho thấy thành công trong đánh giá tóm tắt (Shen et al., 2023; Gilardi et al., 2023; Chiang và Lee, 2023). Chúng tôi yêu cầu mô hình đánh giá tóm tắt được tạo ra trên bốn chiều: relevance, consistency, fluency, và coherence. Chi tiết hơn được đưa ra trong Phụ lục I.

Chúng tôi thực hiện đánh giá trên 500 mẫu được lấy ngẫu nhiên từ PubMed. Kết quả được hiển thị trong Bảng 5. LOCOST tạo ra các tóm tắt ở mức cạnh tranh đối với LongT5 (93-97%).

5.5 Ngoại suy cho các chuỗi dài hơn
Bởi vì độ dài của các đầu vào được xem xét trong quá trình huấn luyện thường bị hạn chế do các vấn đề độ phức tạp, một thuộc tính mong muốn cho một mô hình sẽ là ngoại suy tại thời điểm suy luận cho các chuỗi dài hơn nhiều so với những chuỗi được sử dụng trong quá trình huấn luyện.

Chúng tôi huấn luyện LOCOST trên độ dài đầu vào tối đa 4,096 và đánh giá nó trên tập test của arXiv với độ dài đầu vào tối đa 8,192 token. Như được hiển thị trong Bảng 6, thí nghiệm này xác nhận rằng LOCOST thực sự có thể ngoại suy cho các chuỗi dài hơn so với những chuỗi được sử dụng trong huấn luyện. Lưu ý rằng LongT5 tận dụng mã hóa vị trí tương đối, cho phép khả năng ngoại suy. Tuy nhiên, như đã đề cập trước đó, điều này đi kèm với chi phí tăng độ phức tạp so với LOCOST. Trong phần tiếp theo, chúng tôi đẩy ý tưởng này xa hơn bằng cách xem xét các chuỗi cực dài.

5.6 Chuỗi cực dài: hướng tới tóm tắt toàn bộ cuốn sách
Hiệu ứng của việc tăng ngữ cảnh trong quá trình huấn luyện. Như đã hiển thị trước đó, LOCOST thể hiện khả năng mạnh mẽ để tổng quát hóa tốt trên các chuỗi dài hơn so với những chuỗi được thấy trong quá trình huấn luyện. Do việc sử dụng bộ nhớ giảm ở cả thời điểm train và inference, chúng tôi tiến hành trong phần này một phân tích về hiệu suất của nó khi đối mặt với các văn bản cực dài ví dụ như tóm tắt toàn bộ cuốn sách. Chúng tôi xem xét thiết lập cấp độ sách của BookSum. Chúng tôi huấn luyện nhiều instance của LOCOST cho 100 epoch trên các cuốn sách được cắt ngắn với độ dài ngữ cảnh từ 1K đến 32K và chọn mô hình tốt nhất trên Mean ROUGE trên tập validation. Chúng tôi đánh giá các mô hình này trên tập test với các cuốn sách không được cắt ngắn, và báo cáo kết quả trong Hình 5. Chúng tôi thấy rằng việc tăng độ dài đầu vào trong quá trình huấn luyện dẫn đến sự gia tăng tổng thể trong điểm Mean ROUGE test khi nhiều ngữ cảnh hơn được xem xét để tối ưu hóa. Một lần nữa, điều này xác nhận khả năng tổng quát hóa của LOCOST trên độ dài chuỗi cực dài.

Kết quả về tóm tắt toàn bộ cuốn sách. Dựa trên các quan sát ở trên, chúng tôi đưa mô hình tốt nhất LOCOST-32K của chúng tôi vào thử nghiệm và so sánh nó với LongT5 và các mô hình tiên tiến hiện tại trên BookSum-Book. Đối với LongT5, chúng tôi fine-tune checkpoint có sẵn trên độ dài đầu vào tối đa có thể trong quá trình huấn luyện (16K) và báo cáo hiệu suất của nó trên độ dài đầu vào dài nhất có thể tại thời điểm suy luận (32K). Đối với các mô hình khác, kết quả đến từ các bài báo gốc, trong đó các mô hình ban đầu tạo ra các tóm tắt riêng lẻ cho mỗi đoạn văn của cuốn sách và sau đó xếp hạng chúng theo mức độ tin cậy của mô hình. Kết quả được hiển thị trong Bảng 7. Mặc dù là mô hình có ít tham số nhất, LOCOST đạt được Mean ROUGE tiên tiến so với LongT5 và thậm chí các biến thể lớn của BART, T5 và PEGASUS. LOCOST cũng là mô hình duy nhất có khả năng xử lý toàn bộ tài liệu mà không cắt ngắn và xử lý độ dài chuỗi lên tới 600K token. Điều này cho thấy rằng việc xử lý hiệu quả các ngữ cảnh đầy đủ mà không cắt ngắn có thể dẫn đến cải thiện hiệu suất mạnh mẽ.

6 Kết luận
Bài báo của chúng tôi khám phá một kiến trúc mã hóa-giải mã mới chuyên dụng để xử lý các văn bản đầu vào dài. Bằng cách thay thế khối self-attention bằng SSM, chúng tôi thiết kế một mô hình có độ phức tạp thấp và nhẹ có thể xử lý các chuỗi dài lên tới 600K token tại thời điểm suy luận trên một GPU duy nhất. Mô hình của chúng tôi đạt được kết quả cạnh tranh trên các dataset tóm tắt. Hơn nữa, vượt qua giới hạn của các lựa chọn thay thế sparse transformer hiện có, kết quả tiên tiến mới được đạt trên dataset BookSum-Book. Theo hiểu biết tốt nhất của chúng tôi, LOCOST là mô hình đầu tiên có thể xử lý toàn bộ cuốn sách mà không cắt ngắn, tất cả trong một lần duy nhất. Những kết quả này mang lại những khả năng thú vị cho các tác vụ xử lý văn bản trừu tượng yêu cầu các chuỗi cực dài.

7 Hạn chế
Mặc dù chúng tôi đã nghiên cứu các mô hình nhẹ vì lý do tính toán, việc mở rộng kiến trúc thành kích thước lớn hơn có thể được nghiên cứu. Chúng tôi tập trung vào tóm tắt trừu tượng tài liệu dài, chúng tôi để dành cho công việc tương lai việc nghiên cứu SSM trên các tác vụ trừu tượng đầu vào dài khác. Mặc dù việc thay thế self-attention bằng encoder không gian trạng thái giảm mạnh độ phức tạp tính toán, việc sử dụng cross-attention dày đặc trong decoder vẫn hạn chế độ dài chuỗi đầu ra về mặt tính toán trong quá trình huấn luyện.

8 Tuyên bố đạo đức
Chúng tôi đã thực hiện tiền huấn luyện trên một tập con của dataset C4, đã được xác định là bao gồm nội dung không phù hợp như hate speech và tài liệu rõ ràng, như được ghi nhận trong các nghiên cứu được tiến hành bởi Luccioni và Viviano (2021) và cũng thể hiện những thiên kiến tiêu cực đối với một số dân tộc nhất định (Dodge et al., 2021). Điều quan trọng là điều tra các giải pháp tiềm năng để giảm thiểu những vấn đề này thông qua tiền xử lý tỉ mỉ hơn để ngăn chặn sự xuất hiện của những thuộc tính không mong muốn như vậy trong nghiên cứu tương lai. Tuy nhiên, đáng chú ý rằng mặc dù có những mối quan ngại này, dataset C4 phục vụ như một benchmark trong cộng đồng, và các kết quả được báo cáo chỉ tập trung vào chất lượng của các tóm tắt, do đó tránh bất kỳ hàm ý không đạo đức nào. Trong bài báo này, chúng tôi xem xét một kích thước tương đối nhỏ cho LOCOST. Chúng tôi tin rằng công việc của chúng tôi có thể được tái tạo với tài nguyên hạn chế. Chúng tôi theo dõi tiêu thụ điện năng GPU trong quá trình tiền huấn luyện. Mức sử dụng điện năng trung bình là 190W mỗi GPU. Chúng tôi đã huấn luyện trong 140 giờ trên 16 GPU. Cho cường độ CO₂ địa phương là 58 gCO₂/kWh*, chúng tôi có thể ước tính rằng khoảng 25kg CO₂ đã được phát thải trong quá trình tiền huấn luyện, để so sánh với lượng phát thải trung bình là 4.6t CO₂ bình quân đầu người vào năm 2019*.

9 Lời cảm ơn
Công việc này đã được tài trợ một phần thông qua dự án ACDC ANR-21-CE23-0007 và ANR-23-PEIA-0008, PEPR IA, dự án "Principes théoriques et algorithmiques de l'apprentissage frugal (SHARP)". Dự án này được cung cấp tài nguyên tính toán AI và lưu trữ bởi GENCI tại IDRIS nhờ các grant 20XX-AD011014060, 20XX-AD011014022 và 20XX-A0151014638 trên phân vùng V100/A100 của siêu máy tính Jean Zay. Công việc này cũng đã được tài trợ một phần thông qua Singapore International Pre-Graduate Award (SIPGA).
