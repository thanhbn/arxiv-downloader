# 2401.09417.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/ssm/2401.09417.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1118652 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================


--- TRANG 1 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i hai chiá»u
Lianghui Zhu1 *Bencheng Liao2 1 *Qian Zhang3Xinlong Wang4Wenyu Liu1Xinggang Wang1
4243444546PhÃ¡t hiá»‡n mAP(%)3637383940PhÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n mAP(%)71737577PhÃ¢n loáº¡i Top-1 Acc.(%)38394041PhÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a mIoU(%)
(a)So sÃ¡nh Ä‘á»™ chÃ­nh xÃ¡c11.41.82.22.6
51264073810241248FPSw/logscale
Äá»™ phÃ¢n giáº£iDeiT-TiVim-Ti2.542.252.051.571.262.292.071.911.71(b)So sÃ¡nh tá»‘c Ä‘á»™020406080
51264073810241248Bá»™ nhá»› GPU (GB)
Äá»™ phÃ¢n giáº£iDeiT-TiVim-Ti
4.564.2212.488.1311.148.095.0340.09OOM
(c)So sÃ¡nh bá»™ nhá»› GPU3.32DeiT-TiVim-Ti
Nhanh hÆ¡n
Nhá» hÆ¡n2.8Ã—nhanh hÆ¡n
-86.8%bá»™ nhá»›
HÃ¬nh 1: So sÃ¡nh hiá»‡u suáº¥t vÃ  hiá»‡u quáº£ giá»¯a DeiT (Touvron et al., 2021a) vÃ  mÃ´ hÃ¬nh Vim cá»§a chÃºng tÃ´i. Káº¿t quáº£ cho tháº¥y
Vim vÆ°á»£t trá»™i hÆ¡n DeiT trong cáº£ phÃ¢n loáº¡i ImageNet vÃ  cÃ¡c tÃ¡c vá»¥ phÃ¡t hiá»‡n vÃ  phÃ¢n Ä‘oáº¡n xuÃ´i dÃ²ng, Ä‘á»“ng thá»i hiá»‡u quáº£ hÆ¡n vá»
tÃ­nh toÃ¡n vÃ  bá»™ nhá»› so vá»›i DeiT khi xá»­ lÃ½ hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao. VÃ­ dá»¥, Vim nhanh hÆ¡n DeiT 2.8 láº§n vÃ  tiáº¿t kiá»‡m 86.8% bá»™ nhá»› GPU khi thá»±c hiá»‡n suy luáº­n hÃ ng loáº¡t Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng trÃªn hÃ¬nh áº£nh vá»›i Ä‘á»™ phÃ¢n giáº£i
1248Ã—1248, tá»©c lÃ  6084 token má»—i hÃ¬nh áº£nh.
TÃ³m táº¯t
Gáº§n Ä‘Ã¢y cÃ¡c mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i (SSM) vá»›i
thiáº¿t káº¿ nháº­n thá»©c pháº§n cá»©ng hiá»‡u quáº£, tá»©c lÃ  mÃ´ hÃ¬nh
há»c sÃ¢u Mamba, Ä‘Ã£ cho tháº¥y tiá»m nÄƒng lá»›n trong
mÃ´ hÃ¬nh hÃ³a chuá»—i dÃ i. Trong khi Ä‘Ã³ viá»‡c xÃ¢y dá»±ng 
backbone thá»‹ giÃ¡c hiá»‡u quáº£ vÃ  tá»•ng quÃ¡t chá»‰ dá»±a trÃªn
SSM lÃ  má»™t hÆ°á»›ng háº¥p dáº«n. Tuy nhiÃªn, viá»‡c biá»ƒu diá»…n
dá»¯ liá»‡u thá»‹ giÃ¡c lÃ  thÃ¡ch thá»©c Ä‘á»‘i vá»›i SSM do tÃ­nh
nháº¡y cáº£m vá»‹ trÃ­ cá»§a dá»¯ liá»‡u thá»‹ giÃ¡c vÃ  yÃªu cáº§u vá»
ngá»¯ cáº£nh toÃ n cá»¥c cho hiá»ƒu biáº¿t thá»‹ giÃ¡c. Trong bÃ i
bÃ¡o nÃ y, chÃºng tÃ´i chá»‰ ra ráº±ng sá»± phá»¥ thuá»™c vÃ o
self-attention cho viá»‡c há»c biá»ƒu diá»…n thá»‹ giÃ¡c lÃ  khÃ´ng
cáº§n thiáº¿t vÃ  Ä‘á» xuáº¥t má»™t backbone thá»‹ giÃ¡c tá»•ng quÃ¡t
má»›i vá»›i cÃ¡c khá»‘i Mamba hai chiá»u (Vim), Ä‘Ã¡nh dáº¥u
cÃ¡c chuá»—i hÃ¬nh áº£nh vá»›i embedding vá»‹ trÃ­ vÃ  nÃ©n
biá»ƒu diá»…n thá»‹ giÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng
thÃ¡i hai chiá»u. TrÃªn phÃ¢n loáº¡i ImageNet, phÃ¡t hiá»‡n
Ä‘á»‘i tÆ°á»£ng COCO, vÃ  cÃ¡c tÃ¡c vá»¥ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a ADE20k
*ÄÃ³ng gÃ³p ngang nhau1TrÆ°á»ng EIC, Äáº¡i há»c Khoa há»c &
CÃ´ng nghá»‡ Hoa Trung2Viá»‡n TrÃ­ tuá»‡ NhÃ¢n táº¡o,
Äáº¡i há»c Khoa há»c & CÃ´ng nghá»‡ Hoa Trung3Horizon Robotics
4Viá»‡n HÃ n lÃ¢m TrÃ­ tuá»‡ NhÃ¢n táº¡o Báº¯c Kinh. LiÃªn há»‡:
Xinggang Wang <xgwang@hust.edu.cn >.
Ká»· yáº¿u Há»™i nghá»‹ Quá»‘c táº¿ láº§n thá»© 41 vá» Há»c mÃ¡y
, Vienna, Austria. PMLR 235, 2024. Báº£n quyá»n 2024 thuá»™c vá»
(cÃ¡c) tÃ¡c giáº£.

Vim Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao hÆ¡n so vá»›i cÃ¡c vision
transformer Ä‘Æ°á»£c thiáº¿t láº­p tá»‘t nhÆ° DeiT, Ä‘á»“ng thá»i
cÅ©ng thá»ƒ hiá»‡n sá»± cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ vá» hiá»‡u quáº£ tÃ­nh
toÃ¡n & bá»™ nhá»›. VÃ­ dá»¥, Vim nhanh hÆ¡n DeiT 2.8 láº§n
vÃ  tiáº¿t kiá»‡m 86.8% bá»™ nhá»› GPU khi thá»±c hiá»‡n suy
luáº­n hÃ ng loáº¡t Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng trÃªn hÃ¬nh áº£nh
vá»›i Ä‘á»™ phÃ¢n giáº£i 1248 Ã—1248. Káº¿t quáº£ chá»©ng minh
ráº±ng Vim cÃ³ kháº£ nÄƒng vÆ°á»£t qua cÃ¡c rÃ ng buá»™c vá»
tÃ­nh toÃ¡n & bá»™ nhá»› khi thá»±c hiá»‡n hiá»ƒu biáº¿t theo
kiá»ƒu Transformer cho hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao
vÃ  nÃ³ cÃ³ tiá»m nÄƒng lá»›n Ä‘á»ƒ trá»Ÿ thÃ nh backbone tháº¿
há»‡ tiáº¿p theo cho cÃ¡c mÃ´ hÃ¬nh ná»n táº£ng thá»‹ giÃ¡c.
MÃ£ nguá»“n vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c phÃ¡t hÃ nh táº¡i
https://github.com/hustvl/Vim
1. Giá»›i thiá»‡u
Nhá»¯ng tiáº¿n bá»™ nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ dáº«n Ä‘áº¿n sá»± gia tÄƒng quan
tÃ¢m Ä‘áº¿n mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i (SSM). Báº¯t nguá»“n tá»«
mÃ´ hÃ¬nh bá»™ lá»c Kalman cá»• Ä‘iá»ƒn (Kalman, 1960), cÃ¡c SSM hiá»‡n
Ä‘áº¡i vÆ°á»£t trá»™i trong viá»‡c náº¯m báº¯t cÃ¡c phá»¥ thuá»™c táº§m xa vÃ  hÆ°á»Ÿng
lá»£i tá»« viá»‡c huáº¥n luyá»‡n song song. Má»™t sá»‘ phÆ°Æ¡ng phÃ¡p dá»±a trÃªn
SSM, cháº³ng háº¡n nhÆ° cÃ¡c lá»›p khÃ´ng gian tráº¡ng thÃ¡i tuyáº¿n tÃ­nh
(LSSL) (Gu et al., 2021b), mÃ´ hÃ¬nh chuá»—i khÃ´ng gian tráº¡ng thÃ¡i
cÃ³ cáº¥u trÃºc (S4) (Gu et al., 2021a), khÃ´ng gian tráº¡ng thÃ¡i Ä‘Æ°á»ng
chÃ©o (DSS) (Gupta et al., 2022), vÃ  S4D (Gu et al., 2022), Ä‘Æ°á»£c
Ä‘á» xuáº¥t Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u chuá»—i trÃªn má»™t
1arXiv:2401.09417v3  [cs.CV]  14 Nov 2024

--- TRANG 2 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i
pháº¡m vi rá»™ng cÃ¡c tÃ¡c vá»¥ vÃ  phÆ°Æ¡ng thá»©c, Ä‘áº·c biá»‡t trong viá»‡c mÃ´ hÃ¬nh hÃ³a
cÃ¡c phá»¥ thuá»™c táº§m xa. ChÃºng hiá»‡u quáº£ trong viá»‡c xá»­ lÃ½
cÃ¡c chuá»—i dÃ i nhá» tÃ­nh toÃ¡n tÃ­ch cháº­p vÃ  tÃ­nh toÃ¡n gáº§n tuyáº¿n tÃ­nh.
SSM 2-D (Baron et al., 2023), SGConvNeXt (Li et al., 2022b), vÃ  ConvSSM (Smith et al.,
2023a) káº¿t há»£p SSM vá»›i kiáº¿n trÃºc CNN hoáº·c Transformer
Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u 2-D. CÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y, Mamba (Gu &
Dao, 2023), káº¿t há»£p cÃ¡c tham sá»‘ biáº¿n Ä‘á»•i theo thá»i gian vÃ o
SSM vÃ  Ä‘á» xuáº¥t thuáº­t toÃ¡n nháº­n thá»©c pháº§n cá»©ng Ä‘á»ƒ cho phÃ©p
huáº¥n luyá»‡n vÃ  suy luáº­n ráº¥t hiá»‡u quáº£. Hiá»‡u suáº¥t má»Ÿ rá»™ng vÆ°á»£t trá»™i
cá»§a Mamba cho tháº¥y ráº±ng nÃ³ lÃ  má»™t lá»±a chá»n thay tháº¿ Ä‘áº§y há»©a háº¹n
cho Transformer trong mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯. Tuy nhiÃªn,
má»™t máº¡ng backbone tá»•ng quÃ¡t dá»±a trÃªn SSM thuáº§n chÆ°a Ä‘Æ°á»£c
khÃ¡m phÃ¡ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u thá»‹ giÃ¡c, cháº³ng háº¡n nhÆ° hÃ¬nh áº£nh vÃ 
video.

Vision Transformers (ViTs) Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c thÃ nh cÃ´ng lá»›n
trong viá»‡c há»c biá»ƒu diá»…n thá»‹ giÃ¡c, vÆ°á»£t trá»™i trong viá»‡c tiá»n huáº¥n luyá»‡n
tá»± giÃ¡m sÃ¡t quy mÃ´ lá»›n vÃ  hiá»‡u suáº¥t cao trÃªn cÃ¡c
tÃ¡c vá»¥ xuÃ´i dÃ²ng. So vá»›i máº¡ng nÆ¡-ron tÃ­ch cháº­p, lá»£i tháº¿
cá»‘t lÃµi náº±m á»Ÿ chá»— ViT cÃ³ thá»ƒ cung cáº¥p cho má»—i patch hÃ¬nh áº£nh
ngá»¯ cáº£nh toÃ n cá»¥c phá»¥ thuá»™c dá»¯ liá»‡u/patch thÃ´ng qua self-
attention. Äiá»u nÃ y khÃ¡c vá»›i máº¡ng tÃ­ch cháº­p sá»­ dá»¥ng
cÃ¹ng tham sá»‘, tá»©c lÃ  cÃ¡c bá»™ lá»c tÃ­ch cháº­p, cho táº¥t cáº£
vá»‹ trÃ­. Má»™t lá»£i tháº¿ khÃ¡c lÃ  mÃ´ hÃ¬nh hÃ³a khÃ´ng phá»¥ thuá»™c phÆ°Æ¡ng thá»©c
báº±ng cÃ¡ch coi hÃ¬nh áº£nh nhÆ° má»™t chuá»—i cÃ¡c patch mÃ  khÃ´ng cÃ³
thiÃªn lá»‡ch quy náº¡p 2D, lÃ m cho nÃ³ trá»Ÿ thÃ nh kiáº¿n trÃºc Æ°a thÃ­ch
cho cÃ¡c á»©ng dá»¥ng Ä‘a phÆ°Æ¡ng thá»©c (Bavishi et al., 2023; Li et al.,
2023; Liu et al., 2023). Äá»“ng thá»i, cÆ¡ cháº¿ self-attention
trong Transformers Ä‘áº·t ra thÃ¡ch thá»©c vá» máº·t
tá»‘c Ä‘á»™ vÃ  sá»­ dá»¥ng bá»™ nhá»› khi xá»­ lÃ½ cÃ¡c phá»¥ thuá»™c thá»‹ giÃ¡c
táº§m xa, vÃ­ dá»¥, xá»­ lÃ½ hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao.

ÄÆ°á»£c thÃºc Ä‘áº©y bá»Ÿi thÃ nh cÃ´ng cá»§a Mamba trong mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯,
viá»‡c chÃºng ta cÅ©ng cÃ³ thá»ƒ chuyá»ƒn giao thÃ nh cÃ´ng nÃ y
tá»« ngÃ´n ngá»¯ sang thá»‹ giÃ¡c lÃ  háº¥p dáº«n, tá»©c lÃ  thiáº¿t káº¿ má»™t backbone
thá»‹ giÃ¡c tá»•ng quÃ¡t vÃ  hiá»‡u quáº£ vá»›i phÆ°Æ¡ng phÃ¡p SSM tiÃªn tiáº¿n.
Tuy nhiÃªn, cÃ³ hai thÃ¡ch thá»©c Ä‘á»‘i vá»›i Mamba, tá»©c lÃ  mÃ´ hÃ¬nh hÃ³a
má»™t chiá»u vÃ  thiáº¿u nháº­n thá»©c vá»‹ trÃ­. Äá»ƒ giáº£i quyáº¿t nhá»¯ng thÃ¡ch thá»©c nÃ y,
chÃºng tÃ´i Ä‘á» xuáº¥t mÃ´ hÃ¬nh Vision Mamba (Vim), káº¿t há»£p cÃ¡c SSM hai chiá»u cho
viá»‡c mÃ´ hÃ¬nh hÃ³a ngá»¯ cáº£nh thá»‹ giÃ¡c toÃ n cá»¥c phá»¥ thuá»™c dá»¯ liá»‡u vÃ  embedding
vá»‹ trÃ­ cho nháº­n dáº¡ng thá»‹ giÃ¡c nháº­n thá»©c vá»‹ trÃ­. ChÃºng tÃ´i Ä‘áº§u tiÃªn
chia hÃ¬nh áº£nh Ä‘áº§u vÃ o thÃ nh cÃ¡c patch vÃ  chiáº¿u tuyáº¿n tÃ­nh chÃºng
thÃ nh vector cho Vim. CÃ¡c patch hÃ¬nh áº£nh Ä‘Æ°á»£c coi nhÆ° dá»¯ liá»‡u chuá»—i
trong cÃ¡c khá»‘i Vim, nÃ©n hiá»‡u quáº£ biá»ƒu diá»…n thá»‹ giÃ¡c
vá»›i khÃ´ng gian tráº¡ng thÃ¡i lá»±a chá»n hai chiá»u Ä‘Æ°á»£c Ä‘á» xuáº¥t.
HÆ¡n ná»¯a, embedding vá»‹ trÃ­ trong khá»‘i Vim cung cáº¥p
nháº­n thá»©c cho thÃ´ng tin khÃ´ng gian, cho phÃ©p Vim
máº¡nh máº½ hÆ¡n trong cÃ¡c tÃ¡c vá»¥ dá»± Ä‘oÃ¡n dÃ y Ä‘áº·c. Trong
giai Ä‘oáº¡n hiá»‡n táº¡i, chÃºng tÃ´i huáº¥n luyá»‡n mÃ´ hÃ¬nh Vim trÃªn tÃ¡c vá»¥
phÃ¢n loáº¡i hÃ¬nh áº£nh cÃ³ giÃ¡m sÃ¡t sá»­ dá»¥ng bá»™ dá»¯ liá»‡u ImageNet vÃ 
sau Ä‘Ã³ sá»­ dá»¥ng Vim Ä‘Ã£ tiá»n huáº¥n luyá»‡n lÃ m backbone Ä‘á»ƒ thá»±c hiá»‡n
viá»‡c há»c biá»ƒu diá»…n thá»‹ giÃ¡c tuáº§n tá»± cho cÃ¡c tÃ¡c vá»¥ dá»± Ä‘oÃ¡n dÃ y Ä‘áº·c xuÃ´i dÃ²ng,
tá»©c lÃ  phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a, phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vÃ  phÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n. Giá»‘ng nhÆ° Transformers,
Vim cÃ³ thá»ƒ Ä‘Æ°á»£c tiá»n huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u thá»‹ giÃ¡c khÃ´ng giÃ¡m sÃ¡t
quy mÃ´ lá»›n Ä‘á»ƒ cÃ³ biá»ƒu diá»…n thá»‹ giÃ¡c tá»‘t hÆ¡n. Nhá» hiá»‡u quáº£ tá»‘t hÆ¡n
cá»§a Mamba, viá»‡c tiá»n huáº¥n luyá»‡n quy mÃ´ lá»›n cá»§a Vim cÃ³ thá»ƒ
Ä‘áº¡t Ä‘Æ°á»£c vá»›i chi phÃ­ tÃ­nh toÃ¡n tháº¥p hÆ¡n.

So vá»›i cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn SSM khÃ¡c cho tÃ¡c vá»¥ thá»‹ giÃ¡c,
Vim lÃ  phÆ°Æ¡ng phÃ¡p dá»±a trÃªn SSM thuáº§n vÃ  mÃ´ hÃ¬nh hÃ³a hÃ¬nh áº£nh
theo cÃ¡ch chuá»—i, cÃ³ triá»ƒn vá»ng hÆ¡n cho má»™t backbone
tá»•ng quÃ¡t vÃ  hiá»‡u quáº£. Nhá» mÃ´ hÃ¬nh hÃ³a nÃ©n hai chiá»u
vá»›i nháº­n thá»©c vá»‹ trÃ­, Vim lÃ  mÃ´ hÃ¬nh dá»±a trÃªn SSM thuáº§n Ä‘áº§u tiÃªn
xá»­ lÃ½ cÃ¡c tÃ¡c vá»¥ dá»± Ä‘oÃ¡n dÃ y Ä‘áº·c. So vá»›i mÃ´ hÃ¬nh
dá»±a trÃªn Transformer thuyáº¿t phá»¥c nháº¥t, tá»©c lÃ  DeiT (Touvron et al., 2021a),
Vim Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t vÆ°á»£t trá»™i trÃªn phÃ¢n loáº¡i ImageNet.
HÆ¡n ná»¯a, Vim hiá»‡u quáº£ hÆ¡n vá» bá»™ nhá»› GPU vÃ  thá»i gian suy luáº­n
cho hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao. Hiá»‡u quáº£ vá» bá»™ nhá»› vÃ  tá»‘c Ä‘á»™
trao quyá»n cho Vim thá»±c hiá»‡n trá»±c tiáº¿p viá»‡c há»c biá»ƒu diá»…n
thá»‹ giÃ¡c tuáº§n tá»± mÃ  khÃ´ng dá»±a vÃ o tiÃªn nghiá»‡m 2D (cháº³ng háº¡n nhÆ°
cá»­a sá»• cá»¥c bá»™ 2D trong ViTDet (Li et al., 2022c)) cho cÃ¡c tÃ¡c vá»¥
hiá»ƒu biáº¿t thá»‹ giÃ¡c Ä‘á»™ phÃ¢n giáº£i cao trong khi Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c
cao hÆ¡n DeiT.

CÃ¡c Ä‘Ã³ng gÃ³p chÃ­nh cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:
â€¢ChÃºng tÃ´i Ä‘á» xuáº¥t Vision Mamba (Vim), káº¿t há»£p
SSM hai chiá»u cho mÃ´ hÃ¬nh hÃ³a ngá»¯ cáº£nh thá»‹ giÃ¡c
toÃ n cá»¥c phá»¥ thuá»™c dá»¯ liá»‡u vÃ  embedding vá»‹ trÃ­ cho
hiá»ƒu biáº¿t thá»‹ giÃ¡c nháº­n thá»©c vá»‹ trÃ­.
â€¢KhÃ´ng cáº§n attention, Vim Ä‘Æ°á»£c Ä‘á» xuáº¥t cÃ³
cÃ¹ng sá»©c máº¡nh mÃ´ hÃ¬nh hÃ³a nhÆ° ViT trong khi chá»‰ cÃ³
tÃ­nh toÃ¡n thá»i gian dÆ°á»›i báº­c hai vÃ  Ä‘á»™ phá»©c táº¡p bá»™ nhá»›
tuyáº¿n tÃ­nh. Cá»¥ thá»ƒ, Vim nhanh hÆ¡n DeiT 2.8 láº§n
vÃ  tiáº¿t kiá»‡m 86.8% bá»™ nhá»› GPU khi thá»±c hiá»‡n suy luáº­n
hÃ ng loáº¡t Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng trÃªn hÃ¬nh áº£nh á»Ÿ Ä‘á»™ phÃ¢n giáº£i
1248 Ã—1248.
â€¢ChÃºng tÃ´i tiáº¿n hÃ nh cÃ¡c thÃ­ nghiá»‡m má»Ÿ rá»™ng trÃªn phÃ¢n loáº¡i
ImageNet vÃ  cÃ¡c tÃ¡c vá»¥ xuÃ´i dÃ²ng dá»± Ä‘oÃ¡n dÃ y Ä‘áº·c.
Káº¿t quáº£ chá»©ng minh ráº±ng Vim Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t vÆ°á»£t trá»™i
so vá»›i vision Transformer thuáº§n Ä‘Æ°á»£c thiáº¿t láº­p tá»‘t vÃ 
Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cao, tá»©c lÃ  DeiT.

2. CÃ´ng trÃ¬nh liÃªn quan
Kiáº¿n trÃºc cho backbone thá»‹ giÃ¡c tá»•ng quÃ¡t. Trong thá»i ká»³ Ä‘áº§u,
ConvNet (LeCun et al., 1998) Ä‘Ã³ng vai trÃ² lÃ  thiáº¿t káº¿ máº¡ng
tiÃªu chuáº©n de-facto cho thá»‹ giÃ¡c mÃ¡y tÃ­nh. Nhiá»u kiáº¿n trÃºc
máº¡ng nÆ¡-ron tÃ­ch cháº­p (Krizhevsky et al., 2012;
Szegedy et al., 2015; Simonyan & Zisserman, 2014; He
et al., 2016; Tan & Le, 2019; Wang et al., 2020a; Huang
et al., 2017; Xie et al., 2017; Tan & Le, 2021; Radosavovic
et al., 2020) Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t lÃ m backbone thá»‹ giÃ¡c cho
cÃ¡c á»©ng dá»¥ng thá»‹ giÃ¡c khÃ¡c nhau. CÃ´ng trÃ¬nh tiÃªn phong,
Vision Transformer (ViT) (Dosovitskiy et al., 2020) thay Ä‘á»•i
bá»‘i cáº£nh. NÃ³ coi hÃ¬nh áº£nh nhÆ° má»™t chuá»—i cÃ¡c patch 2D
Ä‘Æ°á»£c lÃ m pháº³ng vÃ  Ã¡p dá»¥ng trá»±c tiáº¿p kiáº¿n trÃºc Transformer thuáº§n.
2

--- TRANG 3 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i
Káº¿t quáº£ Ä‘Ã¡ng ngáº¡c nhiÃªn cá»§a ViT trÃªn phÃ¢n loáº¡i hÃ¬nh áº£nh vÃ  kháº£ nÄƒng
má»Ÿ rá»™ng cá»§a nÃ³ khuyáº¿n khÃ­ch nhiá»u cÃ´ng trÃ¬nh tiáº¿p theo (Touvron
et al., 2021b; Tolstikhin et al., 2021; Touvron et al., 2022;
Fang et al., 2022). Má»™t hÆ°á»›ng nghiÃªn cá»©u táº­p trung vÃ o thiáº¿t káº¿
kiáº¿n trÃºc lai báº±ng cÃ¡ch giá»›i thiá»‡u tiÃªn nghiá»‡m tÃ­ch cháº­p 2D
vÃ o ViT (Wu et al., 2021; Dai et al., 2021; d'Ascoli et al.,
2021; Dong et al., 2022). PVT (Wang et al., 2021) Ä‘á» xuáº¥t
Transformer cáº¥u trÃºc kim tá»± thÃ¡p. Swin Transformer (Liu
et al., 2021) Ã¡p dá»¥ng self-attention trong cÃ¡c cá»­a sá»• dá»‹ch chuyá»ƒn.
Má»™t hÆ°á»›ng nghiÃªn cá»©u khÃ¡c táº­p trung vÃ o cáº£i thiá»‡n ConvNets 2D
truyá»n thá»‘ng vá»›i cÃ¡c cÃ i Ä‘áº·t tiÃªn tiáº¿n hÆ¡n (Wang et al., 2023b;
Liu et al., 2022a). ConvNeXt (Liu et al., 2022b) xem xÃ©t
khÃ´ng gian thiáº¿t káº¿ vÃ  Ä‘á» xuáº¥t ConvNets thuáº§n, cÃ³ thá»ƒ
má»Ÿ rá»™ng nhÆ° ViT vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³. RepLKNet (Ding et al.,
2022) Ä‘á» xuáº¥t má»Ÿ rá»™ng kÃ­ch thÆ°á»›c kernel cá»§a ConvNets
hiá»‡n cÃ³ Ä‘á»ƒ mang láº¡i cáº£i thiá»‡n.

Máº·c dÃ¹ nhá»¯ng cÃ´ng trÃ¬nh tiáº¿p theo chá»§ Ä‘áº¡o nÃ y thá»ƒ hiá»‡n hiá»‡u suáº¥t
vÆ°á»£t trá»™i vÃ  hiá»‡u quáº£ tá»‘t hÆ¡n trÃªn ImageNet (Deng
et al., 2009) vÃ  cÃ¡c tÃ¡c vá»¥ xuÃ´i dÃ²ng khÃ¡c nhau (Lin et al., 2014;
Zhou et al., 2019) báº±ng cÃ¡ch giá»›i thiá»‡u tiÃªn nghiá»‡m 2D, vá»›i sá»± gia tÄƒng
cá»§a viá»‡c tiá»n huáº¥n luyá»‡n thá»‹ giÃ¡c quy mÃ´ lá»›n (Bao et al., 2022; Fang
et al., 2023; Caron et al., 2021) vÃ  cÃ¡c á»©ng dá»¥ng Ä‘a phÆ°Æ¡ng thá»©c (Radford et al., 2021; Li et al., 2022a; 2023; Liu
et al., 2023; Bavishi et al., 2023; Jia et al., 2021), mÃ´ hÃ¬nh
kiá»ƒu Transformer vanilla táº¥n cÃ´ng trá»Ÿ láº¡i vÃ o trung tÃ¢m sÃ¢n kháº¥u
cá»§a thá»‹ giÃ¡c mÃ¡y tÃ­nh. Nhá»¯ng lá»£i tháº¿ cá»§a kháº£ nÄƒng mÃ´ hÃ¬nh hÃ³a lá»›n hÆ¡n,
biá»ƒu diá»…n Ä‘a phÆ°Æ¡ng thá»©c thá»‘ng nháº¥t, thÃ¢n thiá»‡n vá»›i
viá»‡c há»c tá»± giÃ¡m sÃ¡t, v.v., khiáº¿n nÃ³ trá»Ÿ thÃ nh kiáº¿n trÃºc
Ä‘Æ°á»£c Æ°a thÃ­ch. Tuy nhiÃªn, sá»‘ lÆ°á»£ng token thá»‹ giÃ¡c bá»‹ háº¡n cháº¿
do Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a Transformer. CÃ³
nhiá»u cÃ´ng trÃ¬nh (Choromanski et al., 2021; Wang et al.,
2020b; Kitaev et al., 2020; Child et al., 2019; Ding et al.,
2023; Qin et al., 2023; Sun et al., 2023) Ä‘á»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c
lÃ¢u dÃ i vÃ  ná»•i báº­t nÃ y, nhÆ°ng Ã­t trong sá»‘ chÃºng táº­p trung vÃ o
cÃ¡c á»©ng dá»¥ng thá»‹ giÃ¡c. Gáº§n Ä‘Ã¢y, LongViT (Wang
et al., 2023c) Ä‘Ã£ xÃ¢y dá»±ng kiáº¿n trÃºc Transformer hiá»‡u quáº£ cho
cÃ¡c á»©ng dá»¥ng bá»‡nh lÃ½ tÃ­nh toÃ¡n thÃ´ng qua attention giÃ£n.
Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n tuyáº¿n tÃ­nh cá»§a LongViT cho phÃ©p nÃ³
mÃ£ hÃ³a chuá»—i thá»‹ giÃ¡c cá»±c dÃ i. Trong cÃ´ng trÃ¬nh nÃ y,
chÃºng tÃ´i láº¥y cáº£m há»©ng tá»« Mamba (Gu & Dao, 2023) vÃ 
khÃ¡m phÃ¡ viá»‡c xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»±a trÃªn SSM thuáº§n lÃ m backbone
thá»‹ giÃ¡c tá»•ng quÃ¡t mÃ  khÃ´ng sá»­ dá»¥ng attention, trong khi váº«n giá»¯
Æ°u Ä‘iá»ƒm mÃ´ hÃ¬nh hÃ³a tuáº§n tá»±, khÃ´ng phá»¥ thuá»™c phÆ°Æ¡ng thá»©c cá»§a ViT.

MÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i cho mÃ´ hÃ¬nh hÃ³a chuá»—i dÃ i. (Gu
et al., 2021a) Ä‘á» xuáº¥t mÃ´ hÃ¬nh Chuá»—i KhÃ´ng gian Tráº¡ng thÃ¡i
CÃ³ cáº¥u trÃºc (S4), má»™t lá»±a chá»n thay tháº¿ má»›i cho CNN hoáº·c Transformers,
Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a phá»¥ thuá»™c táº§m xa. Äáº·c tÃ­nh Ä‘áº§y há»©a háº¹n
cá»§a viá»‡c má»Ÿ rá»™ng tuyáº¿n tÃ­nh theo Ä‘á»™ dÃ i chuá»—i thu hÃºt
thÃªm nhiá»u khÃ¡m phÃ¡. (Wang et al., 2022) Ä‘á» xuáº¥t SSM Gated
Hai chiá»u Ä‘á»ƒ sao chÃ©p káº¿t quáº£ BERT (Devlin et al., 2018) mÃ  khÃ´ng cÃ³
attention. (Smith et al., 2023b) Ä‘á» xuáº¥t lá»›p S5 má»›i báº±ng
cÃ¡ch giá»›i thiá»‡u MIMO SSM vÃ  parallel scan hiá»‡u quáº£ vÃ o lá»›p S4. (Fu et al., 2023) thiáº¿t káº¿ lá»›p SSM má»›i, H3, gáº§n nhÆ°
láº¥p Ä‘áº§y khoáº£ng cÃ¡ch hiá»‡u suáº¥t giá»¯a SSM vÃ  attention
Transformer trong mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯. (Mehta et al., 2023)
xÃ¢y dá»±ng lá»›p Gated State Space trÃªn S4 báº±ng cÃ¡ch giá»›i thiá»‡u
thÃªm cÃ¡c Ä‘Æ¡n vá»‹ gating Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng biá»ƒu Ä‘áº¡t. Gáº§n Ä‘Ã¢y, (Gu
& Dao, 2023) Ä‘á» xuáº¥t lá»›p SSM phá»¥ thuá»™c dá»¯ liá»‡u vÃ 
xÃ¢y dá»±ng backbone mÃ´ hÃ¬nh ngÃ´n ngá»¯ tá»•ng quÃ¡t, Mamba, vÆ°á»£t trá»™i
hÆ¡n Transformers á»Ÿ cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau trÃªn dá»¯ liá»‡u thá»±c
quy mÃ´ lá»›n vÃ  cÃ³ má»Ÿ rá»™ng tuyáº¿n tÃ­nh theo Ä‘á»™ dÃ i chuá»—i. Trong
cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i khÃ¡m phÃ¡ viá»‡c chuyá»ƒn giao thÃ nh cÃ´ng cá»§a Mamba
sang thá»‹ giÃ¡c, tá»©c lÃ  xÃ¢y dá»±ng backbone thá»‹ giÃ¡c tá»•ng quÃ¡t
chá»‰ dá»±a trÃªn SSM mÃ  khÃ´ng cÃ³ attention.

MÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i cho cÃ¡c á»©ng dá»¥ng thá»‹ giÃ¡c. (Islam &
Bertasius, 2022) sá»­ dá»¥ng S4 1D Ä‘á»ƒ xá»­ lÃ½ cÃ¡c phá»¥ thuá»™c
thá»i gian táº§m xa cho phÃ¢n loáº¡i video. (Nguyen et al.,
2022) má»Ÿ rá»™ng thÃªm S4 1D Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u Ä‘a chiá»u
bao gá»“m hÃ¬nh áº£nh 2D vÃ  video 3D. (Islam et al.,
2023) káº¿t há»£p Ä‘iá»ƒm máº¡nh cá»§a S4 vÃ  self-attention Ä‘á»ƒ
xÃ¢y dá»±ng mÃ´ hÃ¬nh TranS4mer, Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tiÃªn tiáº¿n
cho phÃ¡t hiá»‡n cáº£nh phim. (Wang et al., 2023a)
giá»›i thiá»‡u cÆ¡ cháº¿ lá»±a chá»n má»›i cho S4, cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
hiá»‡u suáº¥t cá»§a S4 trong hiá»ƒu biáº¿t video dáº¡ng dÃ i
vá»›i dung lÆ°á»£ng bá»™ nhá»› tháº¥p hÆ¡n nhiá»u. (Yan
et al., 2023) thay tháº¿ cÆ¡ cháº¿ attention báº±ng backbone
dá»±a trÃªn SSM cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng hÆ¡n Ä‘á»ƒ táº¡o ra hÃ¬nh áº£nh
Ä‘á»™ phÃ¢n giáº£i cao vÃ  xá»­ lÃ½ biá»ƒu diá»…n chi tiáº¿t dÆ°á»›i
tÃ­nh toÃ¡n phÃ¹ há»£p. (Ma et al., 2024) Ä‘á» xuáº¥t U-Mamba,
kiáº¿n trÃºc CNN-SSM lai, Ä‘á»ƒ xá»­ lÃ½ cÃ¡c phá»¥ thuá»™c
táº§m xa trong phÃ¢n Ä‘oáº¡n hÃ¬nh áº£nh y sinh. CÃ¡c cÃ´ng trÃ¬nh trÃªn
(Xing et al., 2024; Ma et al., 2024; Yan et al., 2023;
Wang et al., 2023a; Islam et al., 2023; Nguyen et al., 2022;
Islam & Bertasius, 2022) hoáº·c Ã¡p dá»¥ng SSM cho cÃ¡c á»©ng dá»¥ng
thá»‹ giÃ¡c cá»¥ thá»ƒ hoáº·c xÃ¢y dá»±ng kiáº¿n trÃºc lai báº±ng cÃ¡ch káº¿t há»£p
SSM vá»›i tÃ­ch cháº­p hoáº·c attention. KhÃ¡c vá»›i chÃºng,
chÃºng tÃ´i xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»±a trÃªn SSM thuáº§n, cÃ³ thá»ƒ Ä‘Æ°á»£c
Ã¡p dá»¥ng lÃ m backbone thá»‹ giÃ¡c tá»•ng quÃ¡t. ÄÃ¡ng chÃº Ã½ ráº±ng VMamba
(Liu et al., 2024), má»™t cÃ´ng trÃ¬nh Ä‘á»“ng thá»i vá»›i phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i, Ä‘Ã£
thá»ƒ hiá»‡n káº¿t quáº£ áº¥n tÆ°á»£ng trong nháº­n dáº¡ng thá»‹ giÃ¡c báº±ng cÃ¡ch
káº¿t há»£p Mamba vá»›i quÃ©t Ä‘a hÆ°á»›ng vÃ 
kiáº¿n trÃºc máº¡ng phÃ¢n cáº¥p. NgÆ°á»£c láº¡i, Vim chá»§ yáº¿u
táº­p trung vÃ o viá»‡c há»c chuá»—i thá»‹ giÃ¡c vÃ  tá»± hÃ o cÃ³
biá»ƒu diá»…n thá»‘ng nháº¥t cho dá»¯ liá»‡u Ä‘a phÆ°Æ¡ng thá»©c.

3. PhÆ°Æ¡ng phÃ¡p
Má»¥c tiÃªu cá»§a Vision Mamba (Vim) lÃ  giá»›i thiá»‡u mÃ´ hÃ¬nh
khÃ´ng gian tráº¡ng thÃ¡i (SSM) tiÃªn tiáº¿n, tá»©c lÃ  Mamba (Gu & Dao,
2023), vÃ o thá»‹ giÃ¡c mÃ¡y tÃ­nh. Pháº§n nÃ y báº¯t Ä‘áº§u vá»›i mÃ´ táº£
cÃ¡c kiáº¿n thá»©c cÆ¡ báº£n vá» SSM. Tiáº¿p theo lÃ  tá»•ng quan vá»
Vim. Sau Ä‘Ã³ chÃºng tÃ´i trÃ¬nh bÃ y chi tiáº¿t cÃ¡ch khá»‘i Vim
xá»­ lÃ½ cÃ¡c chuá»—i token Ä‘áº§u vÃ o vÃ  tiáº¿p tá»¥c minh há»a
chi tiáº¿t kiáº¿n trÃºc cá»§a Vim. Pháº§n káº¿t thÃºc vá»›i má»™t
3

--- TRANG 4 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i
phÃ¢n tÃ­ch vá» hiá»‡u quáº£ cá»§a Vim Ä‘Æ°á»£c Ä‘á» xuáº¥t.

3.1. Kiáº¿n thá»©c cÆ¡ báº£n
CÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn SSM, tá»©c lÃ  mÃ´ hÃ¬nh chuá»—i khÃ´ng gian tráº¡ng thÃ¡i
cÃ³ cáº¥u trÃºc (S4) vÃ  Mamba Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« há»‡ thá»‘ng
liÃªn tá»¥c, Ã¡nh xáº¡ má»™t hÃ m hoáº·c chuá»—i 1-D x(t) trong
R7â†’y(t) trong R thÃ´ng qua tráº¡ng thÃ¡i áº©n h(t) trong RN. Há»‡ thá»‘ng
nÃ y sá»­ dá»¥ng A trong RNÃ—N lÃ m tham sá»‘ tiáº¿n hÃ³a vÃ 
B trong RNÃ—1, C trong R1Ã—N lÃ m tham sá»‘ chiáº¿u. Há»‡ thá»‘ng
liÃªn tá»¥c hoáº¡t Ä‘á»™ng nhÆ° sau: hâ€²(t) = Ah(t) +
Bx(t) vÃ  y(t) = Ch(t).

S4 vÃ  Mamba lÃ  cÃ¡c phiÃªn báº£n rá»i ráº¡c cá»§a há»‡ thá»‘ng
liÃªn tá»¥c, bao gá»“m tham sá»‘ tá»· lá»‡ thá»i gian âˆ†
Ä‘á»ƒ biáº¿n Ä‘á»•i cÃ¡c tham sá»‘ liÃªn tá»¥c A, B thÃ nh tham sá»‘
rá»i ráº¡c A, B. PhÆ°Æ¡ng phÃ¡p thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biáº¿n Ä‘á»•i
lÃ  zero-order hold (ZOH), Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:

A = exp (âˆ†A),
B = (âˆ†A)âˆ’1(exp (âˆ†A)âˆ’I)Â·âˆ†B.(1)

Sau khi rá»i ráº¡c hÃ³a A, B, phiÃªn báº£n rá»i ráº¡c
sá»­ dá»¥ng kÃ­ch thÆ°á»›c bÆ°á»›c âˆ† cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t láº¡i nhÆ°:
ht = Ahtâˆ’1 + Bxt,
yt = Cht.(2)

Cuá»‘i cÃ¹ng, cÃ¡c mÃ´ hÃ¬nh tÃ­nh toÃ¡n Ä‘áº§u ra thÃ´ng qua tÃ­ch cháº­p
toÃ n cá»¥c.
K = (CB, CAB, . . . , CAMâˆ’1B),
y = x * K,(3)
trong Ä‘Ã³ M lÃ  Ä‘á»™ dÃ i cá»§a chuá»—i Ä‘áº§u vÃ o x, vÃ  K trong RM
lÃ  kernel tÃ­ch cháº­p cÃ³ cáº¥u trÃºc.

3.2. Vision Mamba
Tá»•ng quan vá» Vim Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2. Mamba
tiÃªu chuáº©n Ä‘Æ°á»£c thiáº¿t káº¿ cho chuá»—i 1-D. Äá»ƒ
xá»­ lÃ½ cÃ¡c tÃ¡c vá»¥ thá»‹ giÃ¡c, chÃºng tÃ´i Ä‘áº§u tiÃªn biáº¿n Ä‘á»•i hÃ¬nh áº£nh 2-D
t trong RHÃ—WÃ—C thÃ nh cÃ¡c patch 2-D Ä‘Æ°á»£c lÃ m pháº³ng xp trong RJÃ—(P2Â·C),
trong Ä‘Ã³ (H,W) lÃ  kÃ­ch thÆ°á»›c cá»§a hÃ¬nh áº£nh Ä‘áº§u vÃ o, C lÃ  sá»‘
kÃªnh, P lÃ  kÃ­ch thÆ°á»›c cá»§a cÃ¡c patch hÃ¬nh áº£nh. Tiáº¿p theo, chÃºng tÃ´i chiáº¿u tuyáº¿n tÃ­nh
xp thÃ nh vector vá»›i kÃ­ch thÆ°á»›c D vÃ  thÃªm embedding
vá»‹ trÃ­ Epos trong R(J+1)Ã—D, nhÆ° sau:

T0 = [tcls;t1pW;t2pW;Â·Â·Â·;tJpW] + Epos, (4)

trong Ä‘Ã³ tjp lÃ  patch thá»© j cá»§a t, W trong R(P2Â·C)Ã—D lÃ  ma tráº­n
chiáº¿u cÃ³ thá»ƒ há»c. Láº¥y cáº£m há»©ng tá»« ViT (Dosovitskiy et al.,
2020) vÃ  BERT (Kenton & Toutanova, 2019), chÃºng tÃ´i cÅ©ng sá»­ dá»¥ng
class token Ä‘á»ƒ biá»ƒu diá»…n toÃ n bá»™ chuá»—i patch, Ä‘Æ°á»£c
kÃ½ hiá»‡u lÃ  tcls. Sau Ä‘Ã³ chÃºng tÃ´i gá»­i chuá»—i token (Tlâˆ’1)
Ä‘áº¿n lá»›p thá»© l cá»§a bá»™ mÃ£ hÃ³a Vim, vÃ  nháº­n Ä‘áº§u ra
Tl. Cuá»‘i cÃ¹ng, chÃºng tÃ´i chuáº©n hÃ³a class token Ä‘áº§u ra T0L vÃ 
Ä‘Æ°a nÃ³ vÃ o Ä‘áº§u Ä‘a lá»›p perceptron (MLP) Ä‘á»ƒ cÃ³
dá»± Ä‘oÃ¡n cuá»‘i cÃ¹ng Ë†p, nhÆ° sau: Tl = Vim (Tlâˆ’1) + Tlâˆ’1,
f = Norm (T0L), vÃ  Ë†p = MLP (f), trong Ä‘Ã³ Vim lÃ 
khá»‘i vision mamba Ä‘Æ°á»£c Ä‘á» xuáº¥t, L lÃ  sá»‘ lá»›p,
vÃ  Norm lÃ  lá»›p chuáº©n hÃ³a.

3.3. Khá»‘i Vim
Khá»‘i Mamba gá»‘c Ä‘Æ°á»£c thiáº¿t káº¿ cho chuá»—i 1-D,
khÃ´ng phÃ¹ há»£p cho cÃ¡c tÃ¡c vá»¥ thá»‹ giÃ¡c yÃªu cáº§u hiá»ƒu biáº¿t
nháº­n thá»©c khÃ´ng gian. Trong pháº§n nÃ y, chÃºng tÃ´i giá»›i thiá»‡u khá»‘i Vim,
káº¿t há»£p mÃ´ hÃ¬nh hÃ³a chuá»—i hai chiá»u cho
cÃ¡c tÃ¡c vá»¥ thá»‹ giÃ¡c. Khá»‘i Vim Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2.

Cá»¥ thá»ƒ, chÃºng tÃ´i trÃ¬nh bÃ y cÃ¡c hoáº¡t Ä‘á»™ng cá»§a khá»‘i Vim trong
Thuáº­t toÃ¡n 1. Chuá»—i token Ä‘áº§u vÃ o Tlâˆ’1 Ä‘áº§u tiÃªn Ä‘Æ°á»£c chuáº©n hÃ³a
bá»Ÿi lá»›p chuáº©n hÃ³a. Tiáº¿p theo, chÃºng tÃ´i chiáº¿u tuyáº¿n tÃ­nh
chuá»—i Ä‘Æ°á»£c chuáº©n hÃ³a thÃ nh x vÃ  z vá»›i kÃ­ch thÆ°á»›c chiá»u
E. Sau Ä‘Ã³, chÃºng tÃ´i xá»­ lÃ½ x tá»« hÆ°á»›ng tiáº¿n vÃ  lÃ¹i.
Äá»‘i vá»›i má»—i hÆ°á»›ng, chÃºng tÃ´i Ä‘áº§u tiÃªn Ã¡p dá»¥ng tÃ­ch cháº­p 1-D
cho x vÃ  nháº­n xâ€²o. Sau Ä‘Ã³ chÃºng tÃ´i chiáº¿u tuyáº¿n tÃ­nh
xâ€²o thÃ nh Bo, Co, âˆ†o, tÆ°Æ¡ng á»©ng. âˆ†o sau Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ
biáº¿n Ä‘á»•i Ao, Bo, tÆ°Æ¡ng á»©ng. Cuá»‘i cÃ¹ng, chÃºng tÃ´i tÃ­nh toÃ¡n
yforward vÃ  ybackward thÃ´ng qua SSM. yforward
vÃ  ybackward sau Ä‘Ã³ Ä‘Æ°á»£c gated bá»Ÿi z vÃ  cá»™ng láº¡i vá»›i nhau
Ä‘á»ƒ cÃ³ chuá»—i token Ä‘áº§u ra Tl.

3.4. Chi tiáº¿t kiáº¿n trÃºc
TÃ³m láº¡i, cÃ¡c siÃªu tham sá»‘ cá»§a kiáº¿n trÃºc chÃºng tÃ´i Ä‘Æ°á»£c
liá»‡t kÃª nhÆ° sau: L biá»ƒu thá»‹ sá»‘ khá»‘i, D biá»ƒu thá»‹
chiá»u tráº¡ng thÃ¡i áº©n, E biá»ƒu thá»‹ chiá»u tráº¡ng thÃ¡i
má»Ÿ rá»™ng, vÃ  N biá»ƒu thá»‹ chiá»u SSM. Theo
ViT (Dosovitskiy et al., 2020) vÃ  DeiT (Touvron et al.,
2021b), chÃºng tÃ´i Ä‘áº§u tiÃªn sá»­ dá»¥ng lá»›p chiáº¿u kÃ­ch thÆ°á»›c kernel 16 Ã—16
Ä‘á»ƒ cÃ³ chuá»—i 1-D cÃ¡c embedding patch khÃ´ng chá»“ng láº¥p.
Tiáº¿p theo, chÃºng tÃ´i trá»±c tiáº¿p xáº¿p chá»“ng L khá»‘i Vim. Theo máº·c Ä‘á»‹nh,
chÃºng tÃ´i Ä‘áº·t sá»‘ khá»‘i L lÃ  24, chiá»u SSM N
lÃ  16. Äá»ƒ cÄƒn chá»‰nh vá»›i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh cá»§a dÃ²ng DeiT, chÃºng tÃ´i
Ä‘áº·t chiá»u tráº¡ng thÃ¡i áº©n D lÃ  192 vÃ  chiá»u tráº¡ng thÃ¡i
má»Ÿ rá»™ng E lÃ  384 cho biáº¿n thá»ƒ kÃ­ch thÆ°á»›c tiny. Äá»‘i vá»›i biáº¿n thá»ƒ
kÃ­ch thÆ°á»›c small, chÃºng tÃ´i Ä‘áº·t D lÃ  384 vÃ  E lÃ  768.

3.5. PhÃ¢n tÃ­ch hiá»‡u quáº£
CÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn SSM truyá»n thá»‘ng táº­n dá»¥ng biáº¿n Ä‘á»•i
Fourier nhanh Ä‘á»ƒ tÄƒng tá»‘c hoáº¡t Ä‘á»™ng tÃ­ch cháº­p nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong
PhÆ°Æ¡ng trÃ¬nh (3). Äá»‘i vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p phá»¥ thuá»™c dá»¯ liá»‡u, cháº³ng háº¡n nhÆ° Mamba,
hoáº¡t Ä‘á»™ng SSM trong DÃ²ng 11 cá»§a Thuáº­t toÃ¡n 1 khÃ´ng cÃ²n tÆ°Æ¡ng Ä‘Æ°Æ¡ng
vá»›i tÃ­ch cháº­p. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, Mamba vÃ 
Vim Ä‘Æ°á»£c Ä‘á» xuáº¥t chá»n cÃ¡ch thÃ¢n thiá»‡n vá»›i pháº§n cá»©ng hiá»‡n Ä‘áº¡i
Ä‘á»ƒ Ä‘áº£m báº£o hiá»‡u quáº£. Ã tÆ°á»Ÿng chÃ­nh cá»§a tá»‘i Æ°u hÃ³a nÃ y lÃ 
trÃ¡nh IO-bound vÃ  memory-bound cá»§a cÃ¡c bá»™ tÄƒng tá»‘c pháº§n cá»©ng
hiá»‡n Ä‘áº¡i (GPU).
4

--- TRANG 5 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

CÃ¡c Patch ÄÆ°á»£c NhÃºng Normğ‘¥Tiáº¿n Conv1dLÃ¹i Conv1dSSM Tiáº¿n SSM LÃ¹i LÃ—
Bá»™ MÃ£ HÃ³a Vision Mamba HÃ¬nh áº¢nh Äáº§u VÃ o Bá»™ MÃ£ HÃ³a Vision Mamba LÃ m Pháº³ng & Chiáº¿u Tuyáº¿n TÃ­nh Lá»›p Chiáº¿u CÃ¡c Token Patch NhÃºng Vá»‹ TrÃ­ Token Lá»›p 01*Vision Mamba(Vim) KÃ­ch Hoáº¡t ğ‘§
MLP & Dá»± ÄoÃ¡n 012345*6789ğ‘¦â„tiáº¿n â„lÃ¹i

HÃ¬nh 2: Tá»•ng quan vá» mÃ´ hÃ¬nh Vim Ä‘Æ°á»£c Ä‘á» xuáº¥t. ChÃºng tÃ´i Ä‘áº§u tiÃªn chia hÃ¬nh áº£nh Ä‘áº§u vÃ o thÃ nh cÃ¡c patch, sau Ä‘Ã³ chiáº¿u chÃºng thÃ nh cÃ¡c token patch. Cuá»‘i cÃ¹ng, chÃºng tÃ´i gá»­i chuá»—i cÃ¡c token Ä‘áº¿n bá»™ mÃ£ hÃ³a Vim Ä‘Æ°á»£c Ä‘á» xuáº¥t. Äá»ƒ thá»±c hiá»‡n phÃ¢n loáº¡i ImageNet, chÃºng tÃ´i ná»‘i thÃªm má»™t token phÃ¢n loáº¡i cÃ³ thá»ƒ há»c Ä‘Æ°á»£c vÃ o chuá»—i token patch. KhÃ¡c vá»›i Mamba cho mÃ´ hÃ¬nh hÃ³a chuá»—i vÄƒn báº£n, bá»™ mÃ£ hÃ³a Vim xá»­ lÃ½ chuá»—i token vá»›i cáº£ hÆ°á»›ng tiáº¿n vÃ  lÃ¹i.

Hiá»‡u quáº£ IO. Bá»™ nhá»› bÄƒng táº§n cao (HBM) vÃ  SRAM lÃ  hai thÃ nh pháº§n quan trá»ng cho GPU. Trong sá»‘ chÃºng, SRAM cÃ³ bÄƒng táº§n lá»›n hÆ¡n vÃ  HBM cÃ³ kÃ­ch thÆ°á»›c bá»™ nhá»› lá»›n hÆ¡n. Viá»‡c triá»ƒn khai tiÃªu chuáº©n cá»§a hoáº¡t Ä‘á»™ng SSM cá»§a Vim vá»›i HBM yÃªu cáº§u sá»‘ lÆ°á»£ng IO bá»™ nhá»› báº­c O(BMEN). Láº¥y cáº£m há»©ng tá»« Mamba, Vim Ä‘áº§u tiÃªn Ä‘á»c vÃ o O(BME+EN) byte bá»™ nhá»› (âˆ†o,Ao,Bo,Co) tá»« HBM cháº­m sang SRAM nhanh. Sau Ä‘Ã³, Vim nháº­n Ä‘Æ°á»£c Ao, Bo rá»i ráº¡c vá»›i kÃ­ch thÆ°á»›c (B,M,E,N) trong SRAM. Cuá»‘i cÃ¹ng, Vim thá»±c hiá»‡n cÃ¡c hoáº¡t Ä‘á»™ng SSM trong SRAM vÃ  ghi Ä‘áº§u ra vá»›i kÃ­ch thÆ°á»›c (B,M,E) trá»Ÿ láº¡i HBM. PhÆ°Æ¡ng phÃ¡p nÃ y cÃ³ thá»ƒ giÃºp giáº£m IO tá»« O(BMEN) xuá»‘ng O(BME+EN).

Hiá»‡u quáº£ bá»™ nhá»›. Äá»ƒ trÃ¡nh váº¥n Ä‘á» háº¿t bá»™ nhá»› vÃ  Ä‘áº¡t Ä‘Æ°á»£c sá»­ dá»¥ng bá»™ nhá»› tháº¥p hÆ¡n khi xá»­ lÃ½ cÃ¡c chuá»—i dÃ i, Vim chá»n cÃ¹ng phÆ°Æ¡ng phÃ¡p tÃ­nh toÃ¡n láº¡i nhÆ° Mamba. Äá»‘i vá»›i cÃ¡c tráº¡ng thÃ¡i trung gian vá»›i kÃ­ch thÆ°á»›c (B,M,E,N) Ä‘á»ƒ tÃ­nh toÃ¡n gradient, Vim tÃ­nh toÃ¡n láº¡i chÃºng táº¡i lÆ°á»£t truyá»n ngÆ°á»£c cá»§a máº¡ng. Äá»‘i vá»›i cÃ¡c kÃ­ch hoáº¡t trung gian nhÆ° Ä‘áº§u ra cá»§a cÃ¡c hÃ m kÃ­ch hoáº¡t vÃ  tÃ­ch cháº­p, Vim cÅ©ng tÃ­nh toÃ¡n láº¡i chÃºng Ä‘á»ƒ tá»‘i Æ°u hÃ³a yÃªu cáº§u bá»™ nhá»› GPU, vÃ¬ cÃ¡c giÃ¡ trá»‹ kÃ­ch hoáº¡t chiáº¿m nhiá»u bá»™ nhá»› nhÆ°ng nhanh Ä‘á»ƒ tÃ­nh toÃ¡n láº¡i.

Hiá»‡u quáº£ tÃ­nh toÃ¡n. SSM trong khá»‘i Vim (DÃ²ng 11 trong Thuáº­t toÃ¡n 1) vÃ  self-attention trong Transformer Ä‘á»u Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c cung cáº¥p ngá»¯ cáº£nh toÃ n cá»¥c má»™t cÃ¡ch thÃ­ch á»©ng. Cho má»™t chuá»—i thá»‹ giÃ¡c T trong R1Ã—MÃ—D vÃ  cÃ i Ä‘áº·t máº·c Ä‘á»‹nh E = 2D, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a self-attention toÃ n cá»¥c vÃ  SSM lÃ :

Î©(self-attention) = 4MD2 + 2M2D, (5)
Î©(SSM) = 3M(2D)N + M(2D)N, (6)

trong Ä‘Ã³ self-attention cÃ³ Ä‘á»™ phá»©c táº¡p báº­c hai theo Ä‘á»™ dÃ i chuá»—i M, vÃ  SSM cÃ³ Ä‘á»™ phá»©c táº¡p tuyáº¿n tÃ­nh theo Ä‘á»™ dÃ i chuá»—i M (N lÃ  tham sá»‘ cá»‘ Ä‘á»‹nh, máº·c Ä‘á»‹nh lÃ  16). Hiá»‡u quáº£ tÃ­nh toÃ¡n lÃ m cho Vim cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng cho cÃ¡c á»©ng dá»¥ng gigapixel vá»›i Ä‘á»™ dÃ i chuá»—i lá»›n.

4. ThÃ­ nghiá»‡m

PhÆ°Æ¡ng phÃ¡p kÃ­ch thÆ°á»›c hÃ¬nh áº£nh #param. ImageNet top-1 acc.
Convnets
ResNet-18 224 2 12M 69.8
ResNet-50 224 2 25M 76.2
ResNet-101 224 2 45M 77.4
ResNet-152 224 2 60M 78.3
ResNeXt50-32Ã—4d 224 2 25M 77.6
RegNetY-4GF 224 2 21M 80.0
Transformers
ViT-B/16 384 2 86M 77.9
ViT-L/16 384 2 307M 76.5
DeiT-Ti 224 2 6M 72.2
DeiT-S 224 2 22M 79.8
DeiT-B 224 2 86M 81.8
SSMs
S4ND-ViT-B 224 2 89M 80.4
Vim-Ti 224 2 7M 76.1
Vim-Tiâ€  224 2 7M 78.3 +2.2
Vim-S 224 2 26M 80.3
Vim-Sâ€  224 2 26M 81.4 +1.1
Vim-B 224 2 98M 81.9
Vim-Bâ€  224 2 98M 83.2 +1.3

Báº£ng 1: So sÃ¡nh vá»›i cÃ¡c backbone khÃ¡c nhau trÃªn táº­p validation ImageNet-1K. â€  biá»ƒu thá»‹ mÃ´ hÃ¬nh Ä‘Æ°á»£c tinh chá»‰nh vá»›i cÃ i Ä‘áº·t chuá»—i dÃ i cá»§a chÃºng tÃ´i.

4.1. PhÃ¢n loáº¡i hÃ¬nh áº£nh
CÃ i Ä‘áº·t. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Vim trÃªn bá»™ dá»¯ liá»‡u ImageNet-1K (Deng et al., 2009), chá»©a 1.28M hÃ¬nh áº£nh huáº¥n luyá»‡n
5

--- TRANG 6 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

Thuáº­t toÃ¡n 1 QuÃ¡ trÃ¬nh Khá»‘i Vim
YÃªu cáº§u: chuá»—i token Tlâˆ’1:(B,M,D)
Äáº£m báº£o: chuá»—i token Tl:(B,M,D)
1: /* chuáº©n hÃ³a chuá»—i Ä‘áº§u vÃ o Tâ€²lâˆ’1*/
2:Tâ€²lâˆ’1:(B,M,D)â†Norm (Tlâˆ’1)
3:x:(B,M,E)â†Linearx(Tâ€²lâˆ’1)
4:z:(B,M,E)â†Linearz(Tâ€²lâˆ’1)
5: /* xá»­ lÃ½ vá»›i hÆ°á»›ng khÃ¡c nhau */
6:for o in {forward, backward} do
7:xâ€²o:(B,M,E)â†SiLU (Conv1d o(x))
8:Bo:(B,M,N)â†LinearBo(xâ€²o)
9:Co:(B,M,N)â†LinearCo(xâ€²o)
10: /* softplus Ä‘áº£m báº£o âˆ†o dÆ°Æ¡ng*/
11: âˆ†o:(B,M,E)â†log(1 + exp( Linearâˆ†o(xâ€²o) +
Parameterâˆ†o))
12: /* hÃ¬nh dáº¡ng cá»§a ParameterAo lÃ  (E,N)*/
13: Ao:(B,M,E,N)â†âˆ†oâŠ—ParameterAo
14: Bo:(B,M,E,N)â†âˆ†oâŠ—Bo
15: /* khá»Ÿi táº¡o ho vÃ  yo vá»›i 0*/
16: ho:(B,E,N)â†zeros (B,E,N)
17: yo:(B,M,E)â†zeros (B,M,E)
18: /* SSM tuáº§n hoÃ n */
19: for i in {0, ..., M-1} do
20: ho=Ao[:, i,:,:]âŠ™ho+Bo[:, i,:,:]âŠ™xâ€²o[:, i,:,None]
21: yo[:, i,:]=hoâŠ—Co[:, i,:]
22: end for
23:end for
24: /* láº¥y y Ä‘Æ°á»£c gated*/
25:yâ€²forward :(B,M,E)â†yforwardâŠ™SiLU (z)
26:yâ€²backward :(B,M,E)â†ybackwardâŠ™SiLU (z)
27: /* káº¿t ná»‘i dÆ° */
28:Tl:(B,M,D)â†LinearT(yâ€²forward +yâ€²backward ) +Tlâˆ’1
29: Return: Tl

hÃ¬nh áº£nh vÃ  50K hÃ¬nh áº£nh validation tá»« 1,000 danh má»¥c.
Táº¥t cáº£ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p huáº¥n luyá»‡n, vÃ  Ä‘á»™ chÃ­nh xÃ¡c top-1
trÃªn táº­p validation Ä‘Æ°á»£c bÃ¡o cÃ¡o. Äá»ƒ so sÃ¡nh cÃ´ng báº±ng,
cÃ i Ä‘áº·t huáº¥n luyá»‡n cá»§a chÃºng tÃ´i chá»§ yáº¿u theo DeiT (Touvron et al.,
2021b). Cá»¥ thá»ƒ, chÃºng tÃ´i Ã¡p dá»¥ng cáº¯t ngáº«u nhiÃªn, láº­t ngang
ngáº«u nhiÃªn, Ä‘iá»u chá»‰nh label-smoothing, mixup,
vÃ  xÃ³a ngáº«u nhiÃªn lÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u. Khi huáº¥n luyá»‡n trÃªn
hÃ¬nh áº£nh Ä‘áº§u vÃ o 2242, chÃºng tÃ´i sá»­ dá»¥ng AdamW (Loshchilov &
Hutter, 2019) vá»›i momentum 0.9, tá»•ng kÃ­ch thÆ°á»›c batch
1024, vÃ  weight decay 0.05 Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh. ChÃºng tÃ´i
huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Vim trong 300 epoch sá»­ dá»¥ng lá»‹ch trÃ¬nh cosine,
tá»‘c Ä‘á»™ há»c ban Ä‘áº§u 1Ã—10âˆ’3, vÃ  EMA. Trong quÃ¡ trÃ¬nh kiá»ƒm tra,
chÃºng tÃ´i Ã¡p dá»¥ng center crop trÃªn táº­p validation Ä‘á»ƒ cáº¯t ra hÃ¬nh áº£nh 2242.
ThÃ­ nghiá»‡m Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn 8 GPU A800.

Tinh chá»‰nh chuá»—i dÃ i Äá»ƒ táº­n dá»¥ng Ä‘áº§y Ä‘á»§ sá»©c máº¡nh
mÃ´ hÃ¬nh hÃ³a chuá»—i dÃ i hiá»‡u quáº£ cá»§a Vim, chÃºng tÃ´i tiáº¿p tá»¥c
tinh chá»‰nh Vim vá»›i cÃ i Ä‘áº·t chuá»—i dÃ i trong 30 epoch sau
tiá»n huáº¥n luyá»‡n. Cá»¥ thá»ƒ, chÃºng tÃ´i Ä‘áº·t stride trÃ­ch xuáº¥t patch
lÃ  8 trong khi giá»¯ nguyÃªn kÃ­ch thÆ°á»›c patch, tá»‘c Ä‘á»™ há»c
háº±ng sá»‘ 10âˆ’5, vÃ  weight decay 10âˆ’8.

Káº¿t quáº£. Báº£ng 1 so sÃ¡nh Vim vá»›i cÃ¡c máº¡ng backbone dá»±a trÃªn ConvNet,

PhÆ°Æ¡ng phÃ¡p Backbone kÃ­ch thÆ°á»›c hÃ¬nh áº£nh #param. val mIoU
DeepLab v3+ ResNet-101 512 2 63M 44.1
UperNet ResNet-50 512 2 67M 41.2
UperNet ResNet-101 512 2 86M 44.9
UperNet DeiT-Ti 512 2 11M 39.2
UperNet DeiT-S 512 2 43M 44.0
UperNet Vim-Ti 512 2 13M 41.0
UperNet Vim-S 512 2 46M 44.9

Báº£ng 2: Káº¿t quáº£ phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a trÃªn táº­p val ADE20K.

dá»±a trÃªn Transformer vÃ  dá»±a trÃªn SSM.
So vá»›i ResNet dá»±a trÃªn ConvNet (He et al., 2016), Vim
thá»ƒ hiá»‡n hiá»‡u suáº¥t vÆ°á»£t trá»™i. VÃ­ dá»¥, khi sá»‘ tham sá»‘
gáº§n tÆ°Æ¡ng tá»±, Ä‘á»™ chÃ­nh xÃ¡c top-1 cá»§a Vim-Small Ä‘áº¡t 80.3,
cao hÆ¡n ResNet50 4.1 Ä‘iá»ƒm. So vá»›i ViT dá»±a trÃªn self-attention
thÃ´ng thÆ°á»ng (Dosovitskiy et al., 2020), Vim vÆ°á»£t trá»™i hÆ¡n nÃ³
vá»›i biÃªn Ä‘á»™ Ä‘Ã¡ng ká»ƒ vá» cáº£ sá»‘ tham sá»‘
vÃ  Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i. Khi so sÃ¡nh vá»›i biáº¿n thá»ƒ ViT
Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cao, tá»©c lÃ  DeiT (Touvron et al.,
2021b), Vim vÆ°á»£t qua nÃ³ á»Ÿ cÃ¡c quy mÃ´ khÃ¡c nhau vá»›i
sá»‘ tham sá»‘ tÆ°Æ¡ng Ä‘Æ°Æ¡ng: cao hÆ¡n 3.9 Ä‘iá»ƒm cho Vim-Tiny
so vá»›i DeiT-Tiny, cao hÆ¡n 0.5 Ä‘iá»ƒm cho Vim-Small so vá»›i DeiT-
Small, vÃ  cao hÆ¡n 0.1 Ä‘iá»ƒm cho Vim-Base so vá»›i DeiT-Base.
So vá»›i S4ND-ViT-B dá»±a trÃªn SSM (Nguyen et al.,
2022), Vim Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c top-1 tÆ°Æ¡ng tá»± vá»›i Ã­t hÆ¡n 3 láº§n
tham sá»‘. Sau tinh chá»‰nh chuá»—i dÃ i, Vim-Tinyâ€ ,
Vim-Sâ€ , vÃ  Vim-Bâ€  Ä‘á»u Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ cao hÆ¡n. Trong sá»‘ chÃºng,
Vim-Sâ€  tháº­m chÃ­ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tÆ°Æ¡ng tá»± vá»›i DeiT-B.
Káº¿t quáº£ chá»©ng minh ráº±ng Vim cÃ³ thá»ƒ thÃ­ch á»©ng vá»›i
mÃ´ hÃ¬nh hÃ³a chuá»—i dÃ i má»™t cÃ¡ch dá»… dÃ ng vÃ  trÃ­ch xuáº¥t biá»ƒu diá»…n thá»‹ giÃ¡c
máº¡nh máº½ hÆ¡n.

HÃ¬nh 1 (b) vÃ  (c) so sÃ¡nh FPS vÃ  bá»™ nhá»› GPU cá»§a
Vim vÃ  DeiT kÃ­ch thÆ°á»›c tiny. Vim thá»ƒ hiá»‡n hiá»‡u quáº£ tá»‘t hÆ¡n
vá» tá»‘c Ä‘á»™ vÃ  bá»™ nhá»› khi Ä‘á»™ phÃ¢n giáº£i hÃ¬nh áº£nh tÄƒng. Cá»¥ thá»ƒ,
khi kÃ­ch thÆ°á»›c hÃ¬nh áº£nh lÃ  512Ã—512, Vim Ä‘áº¡t Ä‘Æ°á»£c FPS vÃ  bá»™ nhá»›
tÆ°Æ¡ng tá»± nhÆ° DeiT. Khi kÃ­ch thÆ°á»›c hÃ¬nh áº£nh tÄƒng lÃªn
1248Ã—1248, Vim nhanh hÆ¡n DeiT 2.8 láº§n vÃ  tiáº¿t kiá»‡m 86.8%
bá»™ nhá»› GPU. Sá»± vÆ°á»£t trá»™i rÃµ rá»‡t cá»§a viá»‡c má»Ÿ rá»™ng tuyáº¿n tÃ­nh
theo Ä‘á»™ dÃ i chuá»—i cá»§a Vim lÃ m cho nÃ³ sáºµn sÃ ng cho cÃ¡c á»©ng dá»¥ng
thá»‹ giÃ¡c xuÃ´i dÃ²ng Ä‘á»™ phÃ¢n giáº£i cao vÃ  á»©ng dá»¥ng Ä‘a phÆ°Æ¡ng thá»©c
chuá»—i dÃ i.

4.2. PhÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a
CÃ i Ä‘áº·t. ChÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m cho phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a
trÃªn ADE20K (Zhou et al., 2019) vÃ  sá»­ dá»¥ng UperNet (Xiao et al., 2018b) lÃ m framework phÃ¢n Ä‘oáº¡n. ChÃºng tÃ´i
cung cáº¥p cÃ i Ä‘áº·t chi tiáº¿t trong Pháº§n B. Káº¿t quáº£. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong
Báº£ng 2, Vim luÃ´n vÆ°á»£t trá»™i hÆ¡n DeiT trÃªn cÃ¡c
quy mÃ´ khÃ¡c nhau: cao hÆ¡n 1.8 mIoU cho Vim-Ti so vá»›i DeiT-Ti, vÃ 
6

--- TRANG 7 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

Backbone APbox APbox50 APbox75 APboxs APboxm APboxl
DeiT-Ti 44.4 63.0 47.8 26.1 47.4 61.8
Vim-Ti 45.7 63.9 49.6 26.1 49.0 63.2

Backbone APmask APmask50 APmask75 APmasks APmaskm APmaskl
DeiT-Ti 38.1 59.9 40.5 18.1 40.5 58.4
Vim-Ti 39.2 60.9 41.7 18.2 41.8 60.2

Báº£ng 3: Káº¿t quáº£ phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vÃ  phÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n
trÃªn táº­p val COCO sá»­ dá»¥ng framework Cascade Mask R-CNN (Cai
& Vasconcelos, 2019).

11.41.82.22.6
51264073810241248FPS w/ log scale Äá»™ phÃ¢n giáº£i DeiT Vim 2.522.242.001.561.252.272.061.901.70 Nhanh hÆ¡n
2.8Ã— nhanh hÆ¡n

HÃ¬nh 3: So sÃ¡nh FPS giá»¯a DeiT-Ti (Touvron et al.,
2021a) vÃ  Vim-Ti cá»§a chÃºng tÃ´i trÃªn framework xuÃ´i dÃ²ng
thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng. ChÃºng tÃ´i thá»±c hiá»‡n suy luáº­n hÃ ng loáº¡t vÃ  Ä‘Ã¡nh giÃ¡
FPS theo thang log trÃªn kiáº¿n trÃºc vá»›i backbone
vÃ  FPN. Vim Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i DeiT
vá»›i Ä‘á»™ phÃ¢n giáº£i nhá», tá»©c lÃ  512Ã—512. Khi Ä‘á»™ phÃ¢n giáº£i hÃ¬nh áº£nh Ä‘áº§u vÃ o
tÄƒng, Vim cÃ³ FPS cao hÆ¡n.

cao hÆ¡n 0.9 mIoU cho Vim-S so vá»›i DeiT-S. So vá»›i
backbone ResNet-101, Vim-S cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t
phÃ¢n Ä‘oáº¡n tÆ°Æ¡ng tá»± vá»›i gáº§n 2 láº§n Ã­t tham sá»‘ hÆ¡n.

Äá»ƒ Ä‘Ã¡nh giÃ¡ thÃªm hiá»‡u quáº£ cho cÃ¡c tÃ¡c vá»¥ xuÃ´i dÃ²ng,
tá»©c lÃ  phÃ¢n Ä‘oáº¡n, phÃ¡t hiá»‡n, vÃ  phÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n, chÃºng tÃ´i
káº¿t há»£p cÃ¡c backbone vá»›i mÃ´-Ä‘un feature pyramid
network (FPN) thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vÃ  Ä‘Ã¡nh giÃ¡ FPS vÃ  bá»™ nhá»› GPU cá»§a chÃºng.
NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 3 vÃ  HÃ¬nh 4, cÃ¡c Ä‘Æ°á»ng cong hiá»‡u quáº£
thá»ƒ hiá»‡n káº¿t quáº£ so sÃ¡nh tÆ°Æ¡ng tá»± cá»§a backbone thuáº§n (HÃ¬nh 1),
máº·c dÃ¹ chÃºng tÃ´i gáº¯n thÃªm FPN náº·ng trÃªn cÃ¡c backbone.
Hiá»‡u suáº¥t má»Ÿ rá»™ng tuyáº¿n tÃ­nh Ä‘áº·c biá»‡t Ä‘Æ°á»£c gÃ¡n cho
backbone Vim hiá»‡u quáº£ Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i, xÃ¢y dá»±ng
ná»n táº£ng cho viá»‡c há»c biá»ƒu diá»…n thá»‹ giÃ¡c cáº¥p gigapixel
theo cÃ¡ch end-to-end mÃ  khÃ´ng cáº§n mÃ£ hÃ³a Ä‘a giai Ä‘oáº¡n
(vÃ­ dá»¥, hÃ¬nh áº£nh hÃ ng khÃ´ng, hÃ¬nh áº£nh y táº¿, vÃ 
bá»‡nh lÃ½ tÃ­nh toÃ¡n).

4.3. PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vÃ  phÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n
CÃ i Ä‘áº·t. ChÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m cho phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vÃ 
phÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n trÃªn bá»™ dá»¯ liá»‡u COCO 2017 (Lin et al.,
2014) vÃ  sá»­ dá»¥ng ViTDet (Xiao et al., 2018b) lÃ m framework
cÆ¡ báº£n. ChÃºng tÃ´i cung cáº¥p cÃ i Ä‘áº·t chi tiáº¿t trong Pháº§n B.

Káº¿t quáº£. Báº£ng 3 so sÃ¡nh Vim-Ti vá»›i DeiT-Ti sá»­ dá»¥ng
framework Cascade Mask R-CNN (Cai & Vasconcelos, 2019).
Vim-Ti vÆ°á»£t qua DeiT-Ti 1.3 box AP vÃ  1.1 mask AP.
Äá»‘i vá»›i cÃ¡c Ä‘á»‘i tÆ°á»£ng kÃ­ch thÆ°á»›c trung bÃ¬nh vÃ  lá»›n, Vim-Ti vÆ°á»£t trá»™i
hÆ¡n DeiT-Ti 1.6 APboxm/1.3 APmaskm vÃ  1.4 APboxl/1.8
APmaskl, thá»ƒ hiá»‡n viá»‡c há»c ngá»¯ cáº£nh táº§m xa tá»‘t hÆ¡n
so vá»›i DeiT (HÃ¬nh 5).

ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng sá»± vÆ°á»£t trá»™i vá» Ä‘á»™ chÃ­nh xÃ¡c lÃ  khÃ´ng táº§m thÆ°á»ng
vÃ¬ DeiT Ä‘Æ°á»£c trang bá»‹ window attention trong khi Vim
hoáº¡t Ä‘á»™ng theo cÃ¡ch mÃ´ hÃ¬nh hÃ³a chuá»—i thuáº§n. Cá»¥ thá»ƒ,
Ä‘á»ƒ thá»±c hiá»‡n viá»‡c há»c biá»ƒu diá»…n trÃªn hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao
(tá»©c lÃ  1024Ã—1024), chÃºng tÃ´i theo ViTDet (Li et al., 2022c)
vÃ  sá»­a Ä‘á»•i backbone DeiT vá»›i viá»‡c sá»­ dá»¥ng 2D window
attention, tiÃªm tiÃªn nghiá»‡m 2D vÃ  phÃ¡ vá»¡ báº£n cháº¥t
mÃ´ hÃ¬nh hÃ³a tuáº§n tá»± cá»§a Transformer. Nhá» hiá»‡u quáº£
Ä‘Æ°á»£c minh há»a trong Pháº§n 3.5, HÃ¬nh 1 vÃ  HÃ¬nh 4, chÃºng tÃ´i cÃ³ thá»ƒ
Ã¡p dá»¥ng trá»±c tiáº¿p Vim trÃªn hÃ¬nh áº£nh Ä‘áº§u vÃ o 1024Ã—1024 vÃ  há»c
biá»ƒu diá»…n thá»‹ giÃ¡c tuáº§n tá»± cho phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vÃ  phÃ¢n Ä‘oáº¡n
thá»ƒ hiá»‡n mÃ  khÃ´ng cáº§n tiÃªn nghiá»‡m 2D trong backbone.

4.4. NghiÃªn cá»©u phÃ¢n tÃ­ch
SSM hai chiá»u. ChÃºng tÃ´i phÃ¢n tÃ­ch thiáº¿t káº¿ hai chiá»u
chÃ­nh cá»§a Vim, sá»­ dá»¥ng phÃ¢n loáº¡i ImageNet-1K vÃ  framework
phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a Segmenter (Strudel et al., 2021)
trÃªn ADE20K. Äá»ƒ Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§ sá»©c máº¡nh cá»§a biá»ƒu diá»…n
Ä‘Ã£ há»c trÃªn ImageNet, chÃºng tÃ´i sá»­ dá»¥ng Ä‘áº§u Segmenter Ä‘Æ¡n giáº£n
chá»‰ vá»›i 2 lá»›p Ä‘á»ƒ thá»±c hiá»‡n transfer learning trÃªn
phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a. ChÃºng tÃ´i nghiÃªn cá»©u cÃ¡c chiáº¿n lÆ°á»£c hai chiá»u sau.
None: ChÃºng tÃ´i trá»±c tiáº¿p Ã¡p dá»¥ng khá»‘i Mamba Ä‘á»ƒ
7

52035506580
51264073810241248Bá»™ nhá»› GPU (GB) Äá»™ phÃ¢n giáº£i DeiT Vim Nhá» hÆ¡n
-73.2% bá»™ nhá»› OOM
5.046.888.5415.8622.595.528.0912.4840.03

HÃ¬nh 4: So sÃ¡nh hiá»‡u quáº£ bá»™ nhá»› GPU giá»¯a
DeiT-Ti (Touvron et al., 2021a) vÃ  Vim-Ti cá»§a chÃºng tÃ´i trÃªn framework
xuÃ´i dÃ²ng thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng. ChÃºng tÃ´i thá»±c hiá»‡n suy luáº­n hÃ ng loáº¡t
vÃ  Ä‘Ã¡nh giÃ¡ bá»™ nhá»› GPU trÃªn kiáº¿n trÃºc vá»›i backbone
vÃ  FPN. Vim yÃªu cáº§u bá»™ nhá»› GPU tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i DeiT
vá»›i Ä‘á»™ phÃ¢n giáº£i nhá», tá»©c lÃ  512Ã—512. Khi Ä‘á»™ phÃ¢n giáº£i hÃ¬nh áº£nh Ä‘áº§u vÃ o
tÄƒng, Vim sáº½ sá»­ dá»¥ng Ã­t bá»™ nhá»› GPU hÆ¡n Ä‘Ã¡ng ká»ƒ.

--- TRANG 8 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

Chiáº¿n lÆ°á»£c hai chiá»u ImageNet top-1 acc. ADE20K mIoU
None 73.2 32.3
Bidirectional Layer 70.9 33.6
Bidirectional SSM 72.8 33.2
Bidirectional SSM + Conv1d 73.9 35.9

Báº£ng 4: NghiÃªn cá»©u phÃ¢n tÃ­ch vá» thiáº¿t káº¿ hai chiá»u. Äá»ƒ
Ä‘áº£m báº£o so sÃ¡nh cÃ´ng báº±ng, chÃºng tÃ´i khÃ´ng sá»­ dá»¥ng class token cho
má»—i thÃ­ nghiá»‡m. CÃ i Ä‘áº·t máº·c Ä‘á»‹nh cho Vim Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u mÃ u xanh.

xá»­ lÃ½ chuá»—i thá»‹ giÃ¡c chá»‰ vá»›i hÆ°á»›ng tiáº¿n.
Bidirectional Sequence: Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i ngáº«u nhiÃªn láº­t
chuá»—i thá»‹ giÃ¡c. Äiá»u nÃ y hoáº¡t Ä‘á»™ng nhÆ° tÄƒng cÆ°á»ng dá»¯ liá»‡u.
Bidirectional Block: ChÃºng tÃ´i ghÃ©p cáº·p cÃ¡c khá»‘i xáº¿p chá»“ng. Khá»‘i Ä‘áº§u tiÃªn
cá»§a má»—i cáº·p xá»­ lÃ½ chuá»—i thá»‹ giÃ¡c theo hÆ°á»›ng tiáº¿n
vÃ  khá»‘i thá»© hai cá»§a má»—i cáº·p xá»­ lÃ½ theo
hÆ°á»›ng lÃ¹i. Bidirectional SSM: ChÃºng tÃ´i thÃªm má»™t SSM bá»• sung
cho má»—i khá»‘i Ä‘á»ƒ xá»­ lÃ½ chuá»—i thá»‹ giÃ¡c theo
hÆ°á»›ng lÃ¹i. Bidirectional SSM + Conv1d: Dá»±a trÃªn
Bidirectional SSM, chÃºng tÃ´i thÃªm Conv1d lÃ¹i
trÆ°á»›c SSM lÃ¹i (HÃ¬nh 2).

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 4, viá»‡c Ã¡p dá»¥ng trá»±c tiáº¿p khá»‘i Mamba
Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t trong phÃ¢n loáº¡i. Tuy nhiÃªn, cÃ¡ch
má»™t chiá»u khÃ´ng tá»± nhiÃªn Ä‘áº·t ra thÃ¡ch thá»©c trong
dá»± Ä‘oÃ¡n dÃ y Ä‘áº·c xuÃ´i dÃ²ng. Cá»¥ thá»ƒ, chiáº¿n lÆ°á»£c hai chiá»u
sÆ¡ bá»™ cá»§a viá»‡c sá»­ dá»¥ng Bidirectional Block Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c
top-1 tháº¥p hÆ¡n 7 Ä‘iá»ƒm trong phÃ¢n loáº¡i. Tuy nhiÃªn, nÃ³ vÆ°á»£t trá»™i
hÆ¡n khá»‘i Mamba má»™t chiá»u vanilla 1.3 mIoU
trong phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a. Báº±ng cÃ¡ch thÃªm SSM lÃ¹i vÃ 
Conv1d bá»• sung, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i vÆ°á»£t trá»™i
(73.9 top-1 acc so vá»›i 73.2 top-1 acc) vÃ  sá»± vÆ°á»£t trá»™i
phÃ¢n Ä‘oáº¡n Ä‘áº·c biá»‡t (35.9 mIoU so vá»›i 32.3 mIoU). ChÃºng tÃ´i sá»­ dá»¥ng
chiáº¿n lÆ°á»£c Bidirectional SSM + Conv1d lÃ m cÃ i Ä‘áº·t
máº·c Ä‘á»‹nh trong khá»‘i Vim cá»§a chÃºng tÃ´i.

Thiáº¿t káº¿ phÃ¢n loáº¡i. ChÃºng tÃ´i phÃ¢n tÃ­ch thiáº¿t káº¿ phÃ¢n loáº¡i
cá»§a Vim, Ä‘Ã¡nh giÃ¡ trÃªn phÃ¢n loáº¡i ImageNet-1K. ChÃºng tÃ´i
nghiÃªn cá»©u cÃ¡c chiáº¿n lÆ°á»£c phÃ¢n loáº¡i sau. Mean pool:
ChÃºng tÃ´i Ã¡p dá»¥ng mean pooling trÃªn Ä‘áº·c trÆ°ng Ä‘áº§u ra tá»« khá»‘i
Vim cuá»‘i cÃ¹ng vÃ  thá»±c hiá»‡n phÃ¢n loáº¡i trÃªn Ä‘áº·c trÆ°ng Ä‘Æ°á»£c pooling nÃ y.
Max pool: ChÃºng tÃ´i Ä‘áº§u tiÃªn thÃ­ch á»©ng Ä‘áº§u phÃ¢n loáº¡i trÃªn má»—i
token cá»§a chuá»—i thá»‹ giÃ¡c vÃ  sau Ä‘Ã³ thá»±c hiá»‡n max pooling
trÃªn chuá»—i Ä‘á»ƒ cÃ³ káº¿t quáº£ dá»± Ä‘oÃ¡n phÃ¢n loáº¡i.
Head class token: Theo DeiT (Touvron et al., 2021b),
chÃºng tÃ´i ná»‘i class token á»Ÿ Ä‘áº§u chuá»—i thá»‹ giÃ¡c
vÃ  thá»±c hiá»‡n phÃ¢n loáº¡i. Double class token: Dá»±a trÃªn
chiáº¿n lÆ°á»£c head class token, chÃºng tÃ´i thÃªm má»™t class token
á»Ÿ cuá»‘i chuá»—i thá»‹ giÃ¡c. Middle class token: ChÃºng tÃ´i thÃªm
class token á»Ÿ giá»¯a chuá»—i thá»‹ giÃ¡c
vÃ  sau Ä‘Ã³ thá»±c hiá»‡n phÃ¢n loáº¡i trÃªn middle class token cuá»‘i cÃ¹ng.

Chiáº¿n lÆ°á»£c phÃ¢n loáº¡i ImageNet top-1 acc.
Mean pool 73.9
Max pool 73.4
Head class token 75.2
Double class token 74.3
Middle class token 76.1

Báº£ng 5: NghiÃªn cá»©u phÃ¢n tÃ­ch vá» thiáº¿t káº¿ phÃ¢n loáº¡i. CÃ i Ä‘áº·t
máº·c Ä‘á»‹nh cho Vim Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u mÃ u xanh.

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 5, cÃ¡c thÃ­ nghiá»‡m cho tháº¥y chiáº¿n lÆ°á»£c
middle class token cÃ³ thá»ƒ khai thÃ¡c Ä‘áº§y Ä‘á»§ báº£n cháº¥t tuáº§n hoÃ n
cá»§a SSM vÃ  tiÃªn nghiá»‡m Ä‘á»‘i tÆ°á»£ng trung tÃ¢m trong ImageNet, thá»ƒ hiá»‡n
Ä‘á»™ chÃ­nh xÃ¡c top-1 tá»‘t nháº¥t lÃ  76.1.

5. Káº¿t luáº­n vÃ  cÃ´ng viá»‡c tÆ°Æ¡ng lai
ChÃºng tÃ´i Ä‘Ã£ Ä‘á» xuáº¥t Vision Mamba (Vim) Ä‘á»ƒ khÃ¡m phÃ¡ mÃ´ hÃ¬nh
khÃ´ng gian tráº¡ng thÃ¡i hiá»‡u quáº£ ráº¥t gáº§n Ä‘Ã¢y, tá»©c lÃ  Mamba, lÃ m
backbone thá»‹ giÃ¡c tá»•ng quÃ¡t. KhÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i
trÆ°á»›c Ä‘Ã¢y cho tÃ¡c vá»¥ thá»‹ giÃ¡c sá»­ dá»¥ng kiáº¿n trÃºc lai hoáº·c
kernel tÃ­ch cháº­p 2D toÃ n cá»¥c tÆ°Æ¡ng Ä‘Æ°Æ¡ng, Vim há»c biá»ƒu diá»…n thá»‹ giÃ¡c
theo cÃ¡ch mÃ´ hÃ¬nh hÃ³a chuá»—i vÃ  khÃ´ng giá»›i thiá»‡u thiÃªn lá»‡ch
quy náº¡p cá»¥ thá»ƒ cho hÃ¬nh áº£nh. Nhá» mÃ´ hÃ¬nh hÃ³a khÃ´ng gian tráº¡ng thÃ¡i
hai chiá»u Ä‘Æ°á»£c Ä‘á» xuáº¥t, Vim Ä‘áº¡t Ä‘Æ°á»£c ngá»¯ cáº£nh thá»‹ giÃ¡c
toÃ n cá»¥c phá»¥ thuá»™c dá»¯ liá»‡u vÃ  cÃ³ cÃ¹ng sá»©c máº¡nh mÃ´ hÃ¬nh hÃ³a
nhÆ° Transformer, trong khi cÃ³ Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n tháº¥p hÆ¡n.
HÆ°á»Ÿng lá»£i tá»« thiáº¿t káº¿ nháº­n thá»©c pháº§n cá»©ng cá»§a Mamba,
tá»‘c Ä‘á»™ suy luáº­n vÃ  sá»­ dá»¥ng bá»™ nhá»› cá»§a Vim tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i ViT khi xá»­ lÃ½ hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao. Káº¿t quáº£ thÃ­ nghiá»‡m
trÃªn cÃ¡c benchmark thá»‹ giÃ¡c mÃ¡y tÃ­nh tiÃªu chuáº©n Ä‘Ã£ xÃ¡c minh
sá»©c máº¡nh mÃ´ hÃ¬nh hÃ³a vÃ  hiá»‡u quáº£ cao cá»§a Vim, cho tháº¥y ráº±ng Vim
cÃ³ tiá»m nÄƒng lá»›n Ä‘á»ƒ trá»Ÿ thÃ nh backbone thá»‹ giÃ¡c tháº¿ há»‡ tiáº¿p theo.

Trong cÃ¡c cÃ´ng viá»‡c tÆ°Æ¡ng lai, Vim vá»›i mÃ´ hÃ¬nh hÃ³a SSM hai chiá»u
vá»›i embedding vá»‹ trÃ­ phÃ¹ há»£p cho cÃ¡c tÃ¡c vá»¥ khÃ´ng giÃ¡m sÃ¡t
nhÆ° tiá»n huáº¥n luyá»‡n mask image modeling vÃ  kiáº¿n trÃºc
tÆ°Æ¡ng tá»± vá»›i Mamba cho phÃ©p cÃ¡c tÃ¡c vá»¥ Ä‘a phÆ°Æ¡ng thá»©c nhÆ°
tiá»n huáº¥n luyá»‡n kiá»ƒu CLIP. Dá»±a trÃªn trá»ng sá»‘ Vim Ä‘Ã£ tiá»n huáº¥n luyá»‡n,
viá»‡c khÃ¡m phÃ¡ tÃ­nh há»¯u Ã­ch cá»§a Vim Ä‘á»ƒ phÃ¢n tÃ­ch hÃ¬nh áº£nh
y táº¿ Ä‘á»™ phÃ¢n giáº£i cao, hÃ¬nh áº£nh viá»…n thÃ¡m, vÃ  video dÃ i,
cÃ³ thá»ƒ Ä‘Æ°á»£c coi nhÆ° cÃ¡c tÃ¡c vá»¥ xuÃ´i dÃ²ng, ráº¥t Ä‘Æ¡n giáº£n.

TuyÃªn bá»‘ tÃ¡c Ä‘á»™ng
ChÃºng tÃ´i thÃºc Ä‘áº©y hiá»‡u quáº£ cá»§a backbone thá»‹ giÃ¡c tá»•ng quÃ¡t.
Báº¥t ká»³ há»‡ quáº£ hoáº·c tÃ¡c Ä‘á»™ng xÃ£ há»™i nÃ o thÆ°á»ng liÃªn quan Ä‘áº¿n
cÃ´ng viá»‡c táº­p trung vÃ o tÄƒng hiá»‡u quáº£ cÅ©ng Ã¡p dá»¥ng á»Ÿ Ä‘Ã¢y,
vÃ¬ cÃ´ng viá»‡c nhÆ° váº­y nháº¥t thiáº¿t cáº£i thiá»‡n tÃ­nh thá»±c tiá»…n cá»§a
backbone thá»‹ giÃ¡c cho má»™t loáº¡t cÃ¡c á»©ng dá»¥ng thá»‹ giÃ¡c vá»›i
hÃ¬nh áº£nh Ä‘áº§u vÃ o Ä‘á»™ phÃ¢n giáº£i cao.
8

--- TRANG 9 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

Lá»i cáº£m Æ¡n
CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi Dá»± Ã¡n Khoa há»c vÃ 
CÃ´ng nghá»‡ Quá»‘c gia Lá»›n sá»‘ Grant No.
2023YFF0905400 vÃ  Quá»¹ Khoa há»c Tá»± nhiÃªn Quá»‘c gia
Trung Quá»‘c (NSFC) sá»‘ Grant No. 62276108.

ChÃºng tÃ´i muá»‘n gá»­i lá»i cáº£m Æ¡n Ä‘áº¿n Tianheng Cheng, Yuxin Fang,
Shusheng Yang, Bo Jiang, vÃ  Jingfeng Yao vÃ¬ pháº£n há»“i
há»¯u Ã­ch cá»§a há» vá» báº£n tháº£o.

TÃ i liá»‡u tham kháº£o
Bao, H., Dong, L., Piao, S., and Wei, F. Beit:
BERT pre-training of image transformers. In ICLR,
2022. URL https://openreview.net/forum?
id=p-BhZSz59o4.

Baron, E., Zimerman, I., and Wolf, L. 2-d ssm: A gen-
eral spatial layer for visual transformers. arXiv preprint
arXiv:2306.06635, 2023.

Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena,
A., Somani, A., and TaÅŸÄ±rlar, S. Introducing our mul-
timodal models, 2023. URL https://www.adept.
ai/blog/fuyu-8b.

Cai, Z. and Vasconcelos, N. Cascade r-cnn: High qual-
ity object detection and instance segmentation. TPAMI,
2019.

Caron, M., Touvron, H., Misra, I., JÃ©gou, H., Mairal, J.,
Bojanowski, P., and Joulin, A. Emerging properties in
self-supervised vision transformers. In ICCV, 2021.

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509, 2019.

Choromanski, K. M., Likhosherstov, V., Dohan, D., Song,
X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohi-
uddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and
Weller, A. Rethinking attention with performers. In ICLR,
2021. URL https://openreview.net/forum?
id=Ua6zuk0WRH.

Dai, Z., Liu, H., Le, Q. V., and Tan, M. Coatnet: Marrying
convolution and attention for all data sizes. NeurIPS, 34,
2021.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In CVPR, 2009.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805,
2018.

Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W.,
Zheng, N., and Wei, F. Longnet: Scaling transformers to
1,000,000,000 tokens. arXiv preprint arXiv:2307.02486,
2023.

Ding, X., Zhang, X., Han, J., and Ding, G. Scaling up your
kernels to 31x31: Revisiting large kernel design in cnns.
In CVPR, 2022.

Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L.,
Chen, D., and Guo, B. Cswin transformer: A general
vision transformer backbone with cross-shaped windows.
In CVPR, 2022.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. In
ICLR, 2020.

d'Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S.,
Biroli, G., and Sagun, L. Convit: Improving vision trans-
formers with soft convolutional inductive biases. In ICML,
2021.

Fang, J., Xie, L., Wang, X., Zhang, X., Liu, W., and Tian, Q.
Msg-transformer: Exchanging local spatial information
by manipulating messenger tokens. In CVPR, 2022.

Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,
Huang, T., Wang, X., and Cao, Y. Eva: Exploring the
limits of masked visual representation learning at scale.
In CVPR, 2023.

Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,
A., and Re, C. Hungry hungry hippos: Towards lan-
guage modeling with state space models. In ICLR,
2023. URL https://openreview.net/forum?
id=COZDy0WYGg.

Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk,
E. D., Le, Q. V., and Zoph, B. Simple copy-paste is a
strong data augmentation method for instance segmenta-
tion. In CVPR, 2021.

Gu, A. and Dao, T. Mamba: Linear-time sequence
modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.

Gu, A., Goel, K., and RÃ©, C. Efficiently modeling long
sequences with structured state spaces. arXiv preprint
arXiv:2111.00396, 2021a.

Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra,
A., and RÃ©, C. Combining recurrent, convolutional, and
continuous-time models with linear state space layers. In
NeurIPS, 2021b.
9

--- TRANG 10 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

Gu, A., Goel, K., Gupta, A., and RÃ©, C. On the parameter-
ization and initialization of diagonal state space models.
In NeurIPS, 2022.

Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are
as effective as structured state spaces. In NeurIPS, 2022.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In CVPR, 2016.

Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger,
K. Q. Densely connected convolutional networks. In
CVPR, 2017.

Islam, M. M. and Bertasius, G. Long movie clip classifica-
tion with state-space video models. In ECCV, 2022.

Islam, M. M., Hasan, M., Athrey, K. S., Braskich, T., and
Bertasius, G. Efficient movie scene detection using state-
space transformers. In CVPR, 2023.

Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H.,
Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up
visual and vision-language representation learning with
noisy text supervision. In ICML, 2021.

Kalman, R. E. A new approach to linear filtering and pre-
diction problems. 1960.

Kenton, J. D. M.-W. C. and Toutanova, L. K. Bert: Pre-
training of deep bidirectional transformers for language
understanding. In NAACL-HLT, 2019.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
efficient transformer. In ICLR, 2020. URL https:
//openreview.net/forum?id=rkgNKkHtvB.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classification with deep convolutional neural networks.
In NeurIPS, 2012.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278-2324, 1998.

Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping
language-image pre-training for unified vision-language
understanding and generation. In ICML, 2022a.

Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Boot-
strapping language-image pre-training with frozen im-
age encoders and large language models. arXiv preprint
arXiv:2301.12597, 2023.

Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes
convolutional models great on long sequence modeling?
In ICLR, 2022b.

Li, Y., Mao, H., Girshick, R., and He, K. Exploring plain
vision transformer backbones for object detection. In
ECCV, 2022c.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
Ramanan, D., DollÃ¡r, P., and Zitnick, C. L. Microsoft
coco: Common objects in context. In ECCV, 2014.

Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction
tuning. arXiv preprint arXiv:2304.08485, 2023.

Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B.,
KÃ¤rkkkÃ¤inen, T., Pechenizkiy, M., Mocanu, D., and Wang,
Z. More convnets in the 2020s: Scaling up kernels beyond
51x51 using sparsity. arXiv preprint arXiv:2207.03620,
2022a.

Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q.,
and Liu, Y. Vmamba: Visual state space model. arXiv
preprint arXiv:2401.10166, 2024.

Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
S., and Guo, B. Swin transformer: Hierarchical vision
transformer using shifted windows. In ICCV, 2021.

Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T.,
and Xie, S. A convnet for the 2020s. In CVPR, 2022b.

Loshchilov, I. and Hutter, F. Decoupled weight decay regu-
larization. In ICLR, 2019.

Ma, J., Li, F., and Wang, B. U-mamba: Enhancing long-
range dependency for biomedical image segmentation.
arXiv preprint arXiv:2401.04722, 2024.

Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B.
Long range language modeling via gated state spaces.
In ICLR, 2023. URL https://openreview.net/
forum?id=5MkYIYCbva.

Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao,
T., Baccus, S., and RÃ©, C. S4nd: Modeling images and
videos as multidimensional signals with state spaces. In
NeurIPS, 2022.

Qin, Z., Yang, S., and Zhong, Y. Hierarchically gated recur-
rent neural network for sequence modeling. In NeurIPS,
2023. URL https://openreview.net/forum?
id=P1TCHxJwLB.

Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In ICML, 2021.

Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and
DollÃ¡r, P. Designing network design spaces. In CVPR,
2020.
10

--- TRANG 11 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global filter
networks for image classification. Advances in neural
information processing systems, 34:980-993, 2021.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

Smith, J. T., De Mello, S., Kautz, J., Linderman, S., and
Byeon, W. Convolutional state space models for long-
range spatiotemporal modeling. In NeurIPS, 2023a.

Smith, J. T., Warrington, A., and Linderman, S. Simpli-
fied state space layers for sequence modeling. In ICLR,
2023b. URL https://openreview.net/forum?
id=Ai8Hw3AXqks.

Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Seg-
menter: Transformer for semantic segmentation. In ICCV,
2021.

Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,
Wang, J., and Wei, F. Retentive network: A successor to
transformer for large language modelss. arXiv preprint
arXiv:2307.08621, 2023.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
A. Going deeper with convolutions. In CVPR, 2015.

Tan, M. and Le, Q. Efficientnet: Rethinking model scaling
for convolutional neural networks. In ICML, 2019.

Tan, M. and Le, Q. Efficientnetv2: Smaller models and
faster training. In ICML, 2021.

Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L.,
Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers,
D., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architec-
ture for vision. In NeurIPS, 2021.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and JÃ©gou, H. Training data-efficient image trans-
formers & distillation through attention. In ICML, 2021a.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and JÃ©gou, H. Training data-efficient image trans-
formers & distillation through attention. In ICML, 2021b.

Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-
Nouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve,
G., Verbeek, J., et al. Resmlp: Feedforward networks for
image classification with data-efficient training. TPAMI,
2022.

Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y.,
Liu, D., Mu, Y., Tan, M., Wang, X., et al. Deep high-
resolution representation learning for visual recognition.
TPAMI, 2020a.

Wang, J., Yan, J. N., Gu, A., and Rush, A. M. Pretrain-
ing without attention. arXiv preprint arXiv:2212.10544,
2022.

Wang, J., Zhu, W., Wang, P., Yu, X., Liu, L., Omar, M., and
Hamid, R. Selective structured state-spaces for long-form
video understanding. In CVPR, 2023a.

Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.
Linformer: Self-attention with linear complexity. arXiv
preprint arXiv:2006.04768, 2020b.

Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D.,
Lu, T., Luo, P., and Shao, L. Pyramid vision transformer:
A versatile backbone for dense prediction without convo-
lutions. In ICCV, 2021.

Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu,
X., Lu, T., Lu, L., Li, H., et al. Internimage: Exploring
large-scale vision foundation models with deformable
convolutions. In CVPR, 2023b.

Wang, W., Ma, S., Xu, H., Usuyama, N., Ding, J., Poon,
H., and Wei, F. When an image is worth 1,024 x 1,024
words: A case study in computational pathology. arXiv
preprint arXiv:2312.03558, 2023c.

Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,
and Zhang, L. Cvt: Introducing convolutions to vision
transformers. In ICCV, 2021.

Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified
perceptual parsing for scene understanding. In ECCV,
2018a.

Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified
perceptual parsing for scene understanding. In ECCV,
2018b.

Xie, S., Girshick, R., DollÃ¡r, P., Tu, Z., and He, K. Aggre-
gated residual transformations for deep neural networks.
In CVPR, 2017.

Xing, Z., Ye, T., Yang, Y., Liu, G., and Zhu, L. Segmamba:
Long-range sequential modeling mamba for 3d medical
image segmentation. arXiv preprint arXiv:2401.13560,
2024.

Yan, J. N., Gu, J., and Rush, A. M. Diffusion models without
attention. arXiv preprint arXiv:2311.18257, 2023.

Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., and
Gao, J. Focal self-attention for local-global interactions
in vision transformers. arXiv preprint arXiv:2107.00641,
2021.

Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng,
J., and Yan, S. Metaformer is actually what you need
for vision. In Proceedings of the IEEE/CVF conference
11

--- TRANG 12 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

on computer vision and pattern recognition, pp. 10819-
10829, 2022.

Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso,
A., and Torralba, A. Semantic understanding of scenes
through the ade20k dataset. IJCV, 2019.
12

--- TRANG 13 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

A. Trá»±c quan hÃ³a

GT
Vim-Ti
DeiT-Ti

HÃ¬nh 5: So sÃ¡nh trá»±c quan hÃ³a cá»§a DeiT-Ti (Touvron et al., 2021b) vÃ  Vim-Ti cá»§a chÃºng tÃ´i trÃªn framework Cascade Mask R-CNN (Cai & Vasconcelos, 2019). Nhá» viá»‡c há»c ngá»¯ cáº£nh táº§m xa cá»§a SSM, chÃºng tÃ´i cÃ³ thá»ƒ náº¯m báº¯t Ä‘á»‘i tÆ°á»£ng ráº¥t lá»›n trong hÃ¬nh áº£nh, mÃ  Ä‘á»‘i tÃ¡c DeiT-Ti khÃ´ng thá»ƒ nháº­n thá»©c Ä‘Æ°á»£c.

B. CÃ i Ä‘áº·t bá»• sung
CÃ i Ä‘áº·t cho phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a. ChÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m cho phÃ¢n Ä‘oáº¡n ngá»¯ nghÄ©a trÃªn bá»™ dá»¯ liá»‡u ADE20K (Zhou et al., 2019). ADE20K chá»©a 150 danh má»¥c ngá»¯ nghÄ©a chi tiáº¿t, vá»›i 20K, 2K, vÃ  3K hÃ¬nh áº£nh cho huáº¥n luyá»‡n, validation, vÃ  kiá»ƒm tra, tÆ°Æ¡ng á»©ng. ChÃºng tÃ´i chá»n UperNet (Xiao et al., 2018a) lÃ m framework cÆ¡ sá»Ÿ. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i sá»­ dá»¥ng AdamW vá»›i weight decay 0.01, vÃ  tá»•ng kÃ­ch thÆ°á»›c batch 16 Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh. Lá»‹ch trÃ¬nh huáº¥n luyá»‡n Ä‘Æ°á»£c sá»­ dá»¥ng cÃ³ tá»‘c Ä‘á»™ há»c ban Ä‘áº§u 6Ã—10âˆ’5, suy giáº£m tá»‘c Ä‘á»™ há»c tuyáº¿n tÃ­nh, khá»Ÿi Ä‘á»™ng tuyáº¿n tÃ­nh 1,500 láº§n láº·p, vÃ  tá»•ng cá»™ng 160K láº§n láº·p huáº¥n luyá»‡n. CÃ¡c tÄƒng cÆ°á»ng dá»¯ liá»‡u theo cÃ i Ä‘áº·t phá»• biáº¿n, bao gá»“m láº­t ngang ngáº«u nhiÃªn, thay Ä‘á»•i tá»· lá»‡ ngáº«u nhiÃªn trong pháº¡m vi tá»· lá»‡ [0.5,2.0], vÃ  biáº¿n dáº¡ng quang há»c ngáº«u nhiÃªn. Trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡, chÃºng tÃ´i thay Ä‘á»•i tá»· lá»‡ hÃ¬nh áº£nh Ä‘á»ƒ cÃ³ cáº¡nh ngáº¯n hÆ¡n lÃ  512.

CÃ i Ä‘áº·t cho phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vÃ  phÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n. ChÃºng tÃ´i tiáº¿n hÃ nh thÃ­ nghiá»‡m cho phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng vÃ  phÃ¢n Ä‘oáº¡n thá»ƒ hiá»‡n trÃªn bá»™ dá»¯ liá»‡u COCO 2017 (Lin et al., 2014). Bá»™ dá»¯ liá»‡u COCO 2017 chá»©a 118K hÃ¬nh áº£nh cho huáº¥n luyá»‡n, 5K hÃ¬nh áº£nh cho validation, vÃ  20K hÃ¬nh áº£nh cho kiá»ƒm tra. ChÃºng tÃ´i sá»­ dá»¥ng Cascade Mask R-CNN tiÃªu chuáº©n (Cai & Vasconcelos, 2019) lÃ m framework cÆ¡ sá»Ÿ. Äá»‘i vá»›i cÃ¡c backbone dá»±a trÃªn ViT, chÃºng tÃ´i Ã¡p dá»¥ng cÃ¡c cáº¥u hÃ¬nh bá»• sung (vÃ­ dá»¥, attention cá»­a sá»• & toÃ n cá»¥c xen káº½) Ä‘á»ƒ xá»­ lÃ½ hÃ¬nh áº£nh Ä‘á»™ phÃ¢n giáº£i cao theo ViTDet (Li et al., 2022c). Äá»‘i vá»›i Vim dá»±a trÃªn SSM, chÃºng tÃ´i sá»­ dá»¥ng trá»±c tiáº¿p mÃ  khÃ´ng cÃ³ báº¥t ká»³ sá»­a Ä‘á»•i nÃ o. CÃ¡c cÃ i Ä‘áº·t huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ khÃ¡c hoÃ n toÃ n giá»‘ng nhau. Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, chÃºng tÃ´i sá»­ dá»¥ng AdamW vá»›i weight decay 0.1, vÃ  tá»•ng kÃ­ch thÆ°á»›c batch 64 Ä‘á»ƒ tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh. Lá»‹ch trÃ¬nh huáº¥n luyá»‡n Ä‘Æ°á»£c sá»­ dá»¥ng cÃ³ tá»‘c Ä‘á»™ há»c ban Ä‘áº§u 1Ã—10âˆ’4, suy giáº£m tá»‘c Ä‘á»™ há»c tuyáº¿n tÃ­nh, vÃ  tá»•ng cá»™ng 380K láº§n láº·p huáº¥n luyá»‡n. CÃ¡c tÄƒng cÆ°á»ng dá»¯ liá»‡u sá»­ dá»¥ng tÄƒng cÆ°á»ng dá»¯ liá»‡u jitter quy mÃ´ lá»›n (Ghiasi et al., 2021) Ä‘áº¿n hÃ¬nh áº£nh Ä‘áº§u vÃ o 1024Ã—1024. Trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡, chÃºng tÃ´i thay Ä‘á»•i tá»· lá»‡ hÃ¬nh áº£nh Ä‘á»ƒ cÃ³ cáº¡nh ngáº¯n hÆ¡n lÃ  1024.

C. So sÃ¡nh má»Ÿ rá»™ng vá» kiáº¿n trÃºc phÃ¢n cáº¥p
Äá»ƒ so sÃ¡nh thÃªm vá»›i cÃ¡c kiáº¿n trÃºc phÃ¢n cáº¥p, chÃºng tÃ´i Ä‘á» xuáº¥t biáº¿n thá»ƒ khÃ¡c Hier-Vim báº±ng cÃ¡ch thay tháº¿ shifted local window attention trong SwinTransformer báº±ng SSM hai chiá»u toÃ n cá»¥c Ä‘Æ°á»£c Ä‘á» xuáº¥t. ChÃºng tÃ´i trÃ¬nh bÃ y chi tiáº¿t cáº¥u hÃ¬nh trong Báº£ng 6

MÃ´ hÃ¬nh #Khá»‘i #KÃªnh Tham sá»‘
Hier-Vim-T [2, 2, 5, 2] [96, 192, 384, 768] 30M
Hier-Vim-S [2, 2, 15, 2] [96, 192, 384, 768] 50M
Hier-Vim-B [2, 2, 15, 2] [128, 256, 512, 1024] 89M

Báº£ng 6: Cáº¥u hÃ¬nh chi tiáº¿t cá»§a cÃ¡c biáº¿n thá»ƒ khÃ¡c nhau cá»§a Hier-Vim. ChÃºng tÃ´i cung cáº¥p sá»‘ kÃªnh vÃ  khá»‘i trong 4 giai Ä‘oáº¡n.

PhÃ¢n loáº¡i trÃªn ImageNet. Theo cÃ¡c giao thá»©c huáº¥n luyá»‡n vÃ  validation tiÃªu chuáº©n (Liu et al., 2021; 2024), chÃºng tÃ´i so sÃ¡nh
13

--- TRANG 14 ---
Vision Mamba: Há»c biá»ƒu diá»…n thá»‹ giÃ¡c hiá»‡u quáº£ vá»›i mÃ´ hÃ¬nh khÃ´ng gian tráº¡ng thÃ¡i

PhÆ°Æ¡ng phÃ¡p kÃ­ch thÆ°á»›c hÃ¬nh áº£nh #param. ImageNet top-1 acc.
Swin-T (Liu et al., 2021) 224 2 28M 81.2
FocalTransformer-T (Yang et al., 2021) 224 2 29M 82.2
CVT-21 (Wu et al., 2021) 224 2 32M 82.5
MetaFormer-S35 (Yu et al., 2022) 224 2 31M 81.4
GFNet-H-S (Rao et al., 2021) 224 2 32M 81.5
Hier-Vim-T 224 2 30M 82.5
Swin-S (Liu et al., 2021) 224 2 50M 83.2
FocalTransformer-S (Yang et al., 2021) 224 2 51M 83.5
MetaFormer-S35 (Yu et al., 2022) 224 2 73M 82.5
GFNet-H-B (Rao et al., 2021) 224 2 54M 82.9
Hier-Vim-S 224 2 50M 83.4
Swin-B (Liu et al., 2021) 224 2 88M 83.5
FocalTransformer-B (Yang et al., 2021) 224 2 90M 83.8
Hier-Vim-B 224 2 89M 83.9

Báº£ng 7: So sÃ¡nh vá»›i cÃ¡c kiáº¿n trÃºc phÃ¢n cáº¥p trÃªn táº­p validation ImageNet-1K.

Hier-Vim vá»›i cÃ¡c kiáº¿n trÃºc phÃ¢n cáº¥p phá»• biáº¿n trÃªn cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tiny, small, vÃ  base trong Báº£ng 7. Káº¿t quáº£ cho tháº¥y Hier-Vim vÆ°á»£t trá»™i hÆ¡n Swin Transformer 1.3% á»Ÿ kÃ­ch thÆ°á»›c tiny, 0.2% á»Ÿ kÃ­ch thÆ°á»›c small, vÃ  0.4% á»Ÿ kÃ­ch thÆ°á»›c base, thá»ƒ hiá»‡n hiá»‡u suáº¥t cáº¡nh tranh so vá»›i cÃ¡c kiáº¿n trÃºc phÃ¢n cáº¥p hiá»‡n Ä‘áº¡i Ä‘Æ°á»£c thiáº¿t láº­p tá»‘t vÃ  tá»‘i Æ°u hÃ³a cao.
14
