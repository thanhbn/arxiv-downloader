# 2401.09417.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2401.09417.pdf
# Kích thước tệp: 1118652 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái hai chiều
Lianghui Zhu1 *Bencheng Liao2 1 *Qian Zhang3Xinlong Wang4Wenyu Liu1Xinggang Wang1
4243444546Phát hiện mAP(%)3637383940Phân đoạn thể hiện mAP(%)71737577Phân loại Top-1 Acc.(%)38394041Phân đoạn ngữ nghĩa mIoU(%)
(a)So sánh độ chính xác11.41.82.22.6
51264073810241248FPSw/logscale
Độ phân giảiDeiT-TiVim-Ti2.542.252.051.571.262.292.071.911.71(b)So sánh tốc độ020406080
51264073810241248Bộ nhớ GPU (GB)
Độ phân giảiDeiT-TiVim-Ti
4.564.2212.488.1311.148.095.0340.09OOM
(c)So sánh bộ nhớ GPU3.32DeiT-TiVim-Ti
Nhanh hơn
Nhỏ hơn2.8×nhanh hơn
-86.8%bộ nhớ
Hình 1: So sánh hiệu suất và hiệu quả giữa DeiT (Touvron et al., 2021a) và mô hình Vim của chúng tôi. Kết quả cho thấy
Vim vượt trội hơn DeiT trong cả phân loại ImageNet và các tác vụ phát hiện và phân đoạn xuôi dòng, đồng thời hiệu quả hơn về
tính toán và bộ nhớ so với DeiT khi xử lý hình ảnh độ phân giải cao. Ví dụ, Vim nhanh hơn DeiT 2.8 lần và tiết kiệm 86.8% bộ nhớ GPU khi thực hiện suy luận hàng loạt để trích xuất đặc trưng trên hình ảnh với độ phân giải
1248×1248, tức là 6084 token mỗi hình ảnh.
Tóm tắt
Gần đây các mô hình không gian trạng thái (SSM) với
thiết kế nhận thức phần cứng hiệu quả, tức là mô hình
học sâu Mamba, đã cho thấy tiềm năng lớn trong
mô hình hóa chuỗi dài. Trong khi đó việc xây dựng 
backbone thị giác hiệu quả và tổng quát chỉ dựa trên
SSM là một hướng hấp dẫn. Tuy nhiên, việc biểu diễn
dữ liệu thị giác là thách thức đối với SSM do tính
nhạy cảm vị trí của dữ liệu thị giác và yêu cầu về
ngữ cảnh toàn cục cho hiểu biết thị giác. Trong bài
báo này, chúng tôi chỉ ra rằng sự phụ thuộc vào
self-attention cho việc học biểu diễn thị giác là không
cần thiết và đề xuất một backbone thị giác tổng quát
mới với các khối Mamba hai chiều (Vim), đánh dấu
các chuỗi hình ảnh với embedding vị trí và nén
biểu diễn thị giác với các mô hình không gian trạng
thái hai chiều. Trên phân loại ImageNet, phát hiện
đối tượng COCO, và các tác vụ phân đoạn ngữ nghĩa ADE20k
*Đóng góp ngang nhau1Trường EIC, Đại học Khoa học &
Công nghệ Hoa Trung2Viện Trí tuệ Nhân tạo,
Đại học Khoa học & Công nghệ Hoa Trung3Horizon Robotics
4Viện Hàn lâm Trí tuệ Nhân tạo Bắc Kinh. Liên hệ:
Xinggang Wang <xgwang@hust.edu.cn >.
Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy
, Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 thuộc về
(các) tác giả.

Vim đạt được hiệu suất cao hơn so với các vision
transformer được thiết lập tốt như DeiT, đồng thời
cũng thể hiện sự cải thiện đáng kể về hiệu quả tính
toán & bộ nhớ. Ví dụ, Vim nhanh hơn DeiT 2.8 lần
và tiết kiệm 86.8% bộ nhớ GPU khi thực hiện suy
luận hàng loạt để trích xuất đặc trưng trên hình ảnh
với độ phân giải 1248 ×1248. Kết quả chứng minh
rằng Vim có khả năng vượt qua các ràng buộc về
tính toán & bộ nhớ khi thực hiện hiểu biết theo
kiểu Transformer cho hình ảnh độ phân giải cao
và nó có tiềm năng lớn để trở thành backbone thế
hệ tiếp theo cho các mô hình nền tảng thị giác.
Mã nguồn và mô hình được phát hành tại
https://github.com/hustvl/Vim
1. Giới thiệu
Những tiến bộ nghiên cứu gần đây đã dẫn đến sự gia tăng quan
tâm đến mô hình không gian trạng thái (SSM). Bắt nguồn từ
mô hình bộ lọc Kalman cổ điển (Kalman, 1960), các SSM hiện
đại vượt trội trong việc nắm bắt các phụ thuộc tầm xa và hưởng
lợi từ việc huấn luyện song song. Một số phương pháp dựa trên
SSM, chẳng hạn như các lớp không gian trạng thái tuyến tính
(LSSL) (Gu et al., 2021b), mô hình chuỗi không gian trạng thái
có cấu trúc (S4) (Gu et al., 2021a), không gian trạng thái đường
chéo (DSS) (Gupta et al., 2022), và S4D (Gu et al., 2022), được
đề xuất để xử lý dữ liệu chuỗi trên một
1arXiv:2401.09417v3  [cs.CV]  14 Nov 2024

--- TRANG 2 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái
phạm vi rộng các tác vụ và phương thức, đặc biệt trong việc mô hình hóa
các phụ thuộc tầm xa. Chúng hiệu quả trong việc xử lý
các chuỗi dài nhờ tính toán tích chập và tính toán gần tuyến tính.
SSM 2-D (Baron et al., 2023), SGConvNeXt (Li et al., 2022b), và ConvSSM (Smith et al.,
2023a) kết hợp SSM với kiến trúc CNN hoặc Transformer
để xử lý dữ liệu 2-D. Công trình gần đây, Mamba (Gu &
Dao, 2023), kết hợp các tham số biến đổi theo thời gian vào
SSM và đề xuất thuật toán nhận thức phần cứng để cho phép
huấn luyện và suy luận rất hiệu quả. Hiệu suất mở rộng vượt trội
của Mamba cho thấy rằng nó là một lựa chọn thay thế đầy hứa hẹn
cho Transformer trong mô hình hóa ngôn ngữ. Tuy nhiên,
một mạng backbone tổng quát dựa trên SSM thuần chưa được
khám phá để xử lý dữ liệu thị giác, chẳng hạn như hình ảnh và
video.

Vision Transformers (ViTs) đã đạt được thành công lớn
trong việc học biểu diễn thị giác, vượt trội trong việc tiền huấn luyện
tự giám sát quy mô lớn và hiệu suất cao trên các
tác vụ xuôi dòng. So với mạng nơ-ron tích chập, lợi thế
cốt lõi nằm ở chỗ ViT có thể cung cấp cho mỗi patch hình ảnh
ngữ cảnh toàn cục phụ thuộc dữ liệu/patch thông qua self-
attention. Điều này khác với mạng tích chập sử dụng
cùng tham số, tức là các bộ lọc tích chập, cho tất cả
vị trí. Một lợi thế khác là mô hình hóa không phụ thuộc phương thức
bằng cách coi hình ảnh như một chuỗi các patch mà không có
thiên lệch quy nạp 2D, làm cho nó trở thành kiến trúc ưa thích
cho các ứng dụng đa phương thức (Bavishi et al., 2023; Li et al.,
2023; Liu et al., 2023). Đồng thời, cơ chế self-attention
trong Transformers đặt ra thách thức về mặt
tốc độ và sử dụng bộ nhớ khi xử lý các phụ thuộc thị giác
tầm xa, ví dụ, xử lý hình ảnh độ phân giải cao.

Được thúc đẩy bởi thành công của Mamba trong mô hình hóa ngôn ngữ,
việc chúng ta cũng có thể chuyển giao thành công này
từ ngôn ngữ sang thị giác là hấp dẫn, tức là thiết kế một backbone
thị giác tổng quát và hiệu quả với phương pháp SSM tiên tiến.
Tuy nhiên, có hai thách thức đối với Mamba, tức là mô hình hóa
một chiều và thiếu nhận thức vị trí. Để giải quyết những thách thức này,
chúng tôi đề xuất mô hình Vision Mamba (Vim), kết hợp các SSM hai chiều cho
việc mô hình hóa ngữ cảnh thị giác toàn cục phụ thuộc dữ liệu và embedding
vị trí cho nhận dạng thị giác nhận thức vị trí. Chúng tôi đầu tiên
chia hình ảnh đầu vào thành các patch và chiếu tuyến tính chúng
thành vector cho Vim. Các patch hình ảnh được coi như dữ liệu chuỗi
trong các khối Vim, nén hiệu quả biểu diễn thị giác
với không gian trạng thái lựa chọn hai chiều được đề xuất.
Hơn nữa, embedding vị trí trong khối Vim cung cấp
nhận thức cho thông tin không gian, cho phép Vim
mạnh mẽ hơn trong các tác vụ dự đoán dày đặc. Trong
giai đoạn hiện tại, chúng tôi huấn luyện mô hình Vim trên tác vụ
phân loại hình ảnh có giám sát sử dụng bộ dữ liệu ImageNet và
sau đó sử dụng Vim đã tiền huấn luyện làm backbone để thực hiện
việc học biểu diễn thị giác tuần tự cho các tác vụ dự đoán dày đặc xuôi dòng,
tức là phân đoạn ngữ nghĩa, phát hiện đối tượng và phân đoạn thể hiện. Giống như Transformers,
Vim có thể được tiền huấn luyện trên dữ liệu thị giác không giám sát
quy mô lớn để có biểu diễn thị giác tốt hơn. Nhờ hiệu quả tốt hơn
của Mamba, việc tiền huấn luyện quy mô lớn của Vim có thể
đạt được với chi phí tính toán thấp hơn.

So với các mô hình dựa trên SSM khác cho tác vụ thị giác,
Vim là phương pháp dựa trên SSM thuần và mô hình hóa hình ảnh
theo cách chuỗi, có triển vọng hơn cho một backbone
tổng quát và hiệu quả. Nhờ mô hình hóa nén hai chiều
với nhận thức vị trí, Vim là mô hình dựa trên SSM thuần đầu tiên
xử lý các tác vụ dự đoán dày đặc. So với mô hình
dựa trên Transformer thuyết phục nhất, tức là DeiT (Touvron et al., 2021a),
Vim đạt được hiệu suất vượt trội trên phân loại ImageNet.
Hơn nữa, Vim hiệu quả hơn về bộ nhớ GPU và thời gian suy luận
cho hình ảnh độ phân giải cao. Hiệu quả về bộ nhớ và tốc độ
trao quyền cho Vim thực hiện trực tiếp việc học biểu diễn
thị giác tuần tự mà không dựa vào tiên nghiệm 2D (chẳng hạn như
cửa sổ cục bộ 2D trong ViTDet (Li et al., 2022c)) cho các tác vụ
hiểu biết thị giác độ phân giải cao trong khi đạt được độ chính xác
cao hơn DeiT.

Các đóng góp chính của chúng tôi có thể được tóm tắt như sau:
•Chúng tôi đề xuất Vision Mamba (Vim), kết hợp
SSM hai chiều cho mô hình hóa ngữ cảnh thị giác
toàn cục phụ thuộc dữ liệu và embedding vị trí cho
hiểu biết thị giác nhận thức vị trí.
•Không cần attention, Vim được đề xuất có
cùng sức mạnh mô hình hóa như ViT trong khi chỉ có
tính toán thời gian dưới bậc hai và độ phức tạp bộ nhớ
tuyến tính. Cụ thể, Vim nhanh hơn DeiT 2.8 lần
và tiết kiệm 86.8% bộ nhớ GPU khi thực hiện suy luận
hàng loạt để trích xuất đặc trưng trên hình ảnh ở độ phân giải
1248 ×1248.
•Chúng tôi tiến hành các thí nghiệm mở rộng trên phân loại
ImageNet và các tác vụ xuôi dòng dự đoán dày đặc.
Kết quả chứng minh rằng Vim đạt được hiệu suất vượt trội
so với vision Transformer thuần được thiết lập tốt và
được tối ưu hóa cao, tức là DeiT.

2. Công trình liên quan
Kiến trúc cho backbone thị giác tổng quát. Trong thời kỳ đầu,
ConvNet (LeCun et al., 1998) đóng vai trò là thiết kế mạng
tiêu chuẩn de-facto cho thị giác máy tính. Nhiều kiến trúc
mạng nơ-ron tích chập (Krizhevsky et al., 2012;
Szegedy et al., 2015; Simonyan & Zisserman, 2014; He
et al., 2016; Tan & Le, 2019; Wang et al., 2020a; Huang
et al., 2017; Xie et al., 2017; Tan & Le, 2021; Radosavovic
et al., 2020) đã được đề xuất làm backbone thị giác cho
các ứng dụng thị giác khác nhau. Công trình tiên phong,
Vision Transformer (ViT) (Dosovitskiy et al., 2020) thay đổi
bối cảnh. Nó coi hình ảnh như một chuỗi các patch 2D
được làm phẳng và áp dụng trực tiếp kiến trúc Transformer thuần.
2

--- TRANG 3 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái
Kết quả đáng ngạc nhiên của ViT trên phân loại hình ảnh và khả năng
mở rộng của nó khuyến khích nhiều công trình tiếp theo (Touvron
et al., 2021b; Tolstikhin et al., 2021; Touvron et al., 2022;
Fang et al., 2022). Một hướng nghiên cứu tập trung vào thiết kế
kiến trúc lai bằng cách giới thiệu tiên nghiệm tích chập 2D
vào ViT (Wu et al., 2021; Dai et al., 2021; d'Ascoli et al.,
2021; Dong et al., 2022). PVT (Wang et al., 2021) đề xuất
Transformer cấu trúc kim tự tháp. Swin Transformer (Liu
et al., 2021) áp dụng self-attention trong các cửa sổ dịch chuyển.
Một hướng nghiên cứu khác tập trung vào cải thiện ConvNets 2D
truyền thống với các cài đặt tiên tiến hơn (Wang et al., 2023b;
Liu et al., 2022a). ConvNeXt (Liu et al., 2022b) xem xét
không gian thiết kế và đề xuất ConvNets thuần, có thể
mở rộng như ViT và các biến thể của nó. RepLKNet (Ding et al.,
2022) đề xuất mở rộng kích thước kernel của ConvNets
hiện có để mang lại cải thiện.

Mặc dù những công trình tiếp theo chủ đạo này thể hiện hiệu suất
vượt trội và hiệu quả tốt hơn trên ImageNet (Deng
et al., 2009) và các tác vụ xuôi dòng khác nhau (Lin et al., 2014;
Zhou et al., 2019) bằng cách giới thiệu tiên nghiệm 2D, với sự gia tăng
của việc tiền huấn luyện thị giác quy mô lớn (Bao et al., 2022; Fang
et al., 2023; Caron et al., 2021) và các ứng dụng đa phương thức (Radford et al., 2021; Li et al., 2022a; 2023; Liu
et al., 2023; Bavishi et al., 2023; Jia et al., 2021), mô hình
kiểu Transformer vanilla tấn công trở lại vào trung tâm sân khấu
của thị giác máy tính. Những lợi thế của khả năng mô hình hóa lớn hơn,
biểu diễn đa phương thức thống nhất, thân thiện với
việc học tự giám sát, v.v., khiến nó trở thành kiến trúc
được ưa thích. Tuy nhiên, số lượng token thị giác bị hạn chế
do độ phức tạp bậc hai của Transformer. Có
nhiều công trình (Choromanski et al., 2021; Wang et al.,
2020b; Kitaev et al., 2020; Child et al., 2019; Ding et al.,
2023; Qin et al., 2023; Sun et al., 2023) để giải quyết thách thức
lâu dài và nổi bật này, nhưng ít trong số chúng tập trung vào
các ứng dụng thị giác. Gần đây, LongViT (Wang
et al., 2023c) đã xây dựng kiến trúc Transformer hiệu quả cho
các ứng dụng bệnh lý tính toán thông qua attention giãn.
Độ phức tạp tính toán tuyến tính của LongViT cho phép nó
mã hóa chuỗi thị giác cực dài. Trong công trình này,
chúng tôi lấy cảm hứng từ Mamba (Gu & Dao, 2023) và
khám phá việc xây dựng mô hình dựa trên SSM thuần làm backbone
thị giác tổng quát mà không sử dụng attention, trong khi vẫn giữ
ưu điểm mô hình hóa tuần tự, không phụ thuộc phương thức của ViT.

Mô hình không gian trạng thái cho mô hình hóa chuỗi dài. (Gu
et al., 2021a) đề xuất mô hình Chuỗi Không gian Trạng thái
Có cấu trúc (S4), một lựa chọn thay thế mới cho CNN hoặc Transformers,
để mô hình hóa phụ thuộc tầm xa. Đặc tính đầy hứa hẹn
của việc mở rộng tuyến tính theo độ dài chuỗi thu hút
thêm nhiều khám phá. (Wang et al., 2022) đề xuất SSM Gated
Hai chiều để sao chép kết quả BERT (Devlin et al., 2018) mà không có
attention. (Smith et al., 2023b) đề xuất lớp S5 mới bằng
cách giới thiệu MIMO SSM và parallel scan hiệu quả vào lớp S4. (Fu et al., 2023) thiết kế lớp SSM mới, H3, gần như
lấp đầy khoảng cách hiệu suất giữa SSM và attention
Transformer trong mô hình hóa ngôn ngữ. (Mehta et al., 2023)
xây dựng lớp Gated State Space trên S4 bằng cách giới thiệu
thêm các đơn vị gating để cải thiện khả năng biểu đạt. Gần đây, (Gu
& Dao, 2023) đề xuất lớp SSM phụ thuộc dữ liệu và
xây dựng backbone mô hình ngôn ngữ tổng quát, Mamba, vượt trội
hơn Transformers ở các kích thước khác nhau trên dữ liệu thực
quy mô lớn và có mở rộng tuyến tính theo độ dài chuỗi. Trong
công trình này, chúng tôi khám phá việc chuyển giao thành công của Mamba
sang thị giác, tức là xây dựng backbone thị giác tổng quát
chỉ dựa trên SSM mà không có attention.

Mô hình không gian trạng thái cho các ứng dụng thị giác. (Islam &
Bertasius, 2022) sử dụng S4 1D để xử lý các phụ thuộc
thời gian tầm xa cho phân loại video. (Nguyen et al.,
2022) mở rộng thêm S4 1D để xử lý dữ liệu đa chiều
bao gồm hình ảnh 2D và video 3D. (Islam et al.,
2023) kết hợp điểm mạnh của S4 và self-attention để
xây dựng mô hình TranS4mer, đạt được hiệu suất tiên tiến
cho phát hiện cảnh phim. (Wang et al., 2023a)
giới thiệu cơ chế lựa chọn mới cho S4, cải thiện đáng kể
hiệu suất của S4 trong hiểu biết video dạng dài
với dung lượng bộ nhớ thấp hơn nhiều. (Yan
et al., 2023) thay thế cơ chế attention bằng backbone
dựa trên SSM có khả năng mở rộng hơn để tạo ra hình ảnh
độ phân giải cao và xử lý biểu diễn chi tiết dưới
tính toán phù hợp. (Ma et al., 2024) đề xuất U-Mamba,
kiến trúc CNN-SSM lai, để xử lý các phụ thuộc
tầm xa trong phân đoạn hình ảnh y sinh. Các công trình trên
(Xing et al., 2024; Ma et al., 2024; Yan et al., 2023;
Wang et al., 2023a; Islam et al., 2023; Nguyen et al., 2022;
Islam & Bertasius, 2022) hoặc áp dụng SSM cho các ứng dụng
thị giác cụ thể hoặc xây dựng kiến trúc lai bằng cách kết hợp
SSM với tích chập hoặc attention. Khác với chúng,
chúng tôi xây dựng mô hình dựa trên SSM thuần, có thể được
áp dụng làm backbone thị giác tổng quát. Đáng chú ý rằng VMamba
(Liu et al., 2024), một công trình đồng thời với phương pháp của chúng tôi, đã
thể hiện kết quả ấn tượng trong nhận dạng thị giác bằng cách
kết hợp Mamba với quét đa hướng và
kiến trúc mạng phân cấp. Ngược lại, Vim chủ yếu
tập trung vào việc học chuỗi thị giác và tự hào có
biểu diễn thống nhất cho dữ liệu đa phương thức.

3. Phương pháp
Mục tiêu của Vision Mamba (Vim) là giới thiệu mô hình
không gian trạng thái (SSM) tiên tiến, tức là Mamba (Gu & Dao,
2023), vào thị giác máy tính. Phần này bắt đầu với mô tả
các kiến thức cơ bản về SSM. Tiếp theo là tổng quan về
Vim. Sau đó chúng tôi trình bày chi tiết cách khối Vim
xử lý các chuỗi token đầu vào và tiếp tục minh họa
chi tiết kiến trúc của Vim. Phần kết thúc với một
3

--- TRANG 4 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái
phân tích về hiệu quả của Vim được đề xuất.

3.1. Kiến thức cơ bản
Các mô hình dựa trên SSM, tức là mô hình chuỗi không gian trạng thái
có cấu trúc (S4) và Mamba được lấy cảm hứng từ hệ thống
liên tục, ánh xạ một hàm hoặc chuỗi 1-D x(t) trong
R7→y(t) trong R thông qua trạng thái ẩn h(t) trong RN. Hệ thống
này sử dụng A trong RN×N làm tham số tiến hóa và
B trong RN×1, C trong R1×N làm tham số chiếu. Hệ thống
liên tục hoạt động như sau: h′(t) = Ah(t) +
Bx(t) và y(t) = Ch(t).

S4 và Mamba là các phiên bản rời rạc của hệ thống
liên tục, bao gồm tham số tỷ lệ thời gian ∆
để biến đổi các tham số liên tục A, B thành tham số
rời rạc A, B. Phương pháp thường được sử dụng để biến đổi
là zero-order hold (ZOH), được định nghĩa như sau:

A = exp (∆A),
B = (∆A)−1(exp (∆A)−I)·∆B.(1)

Sau khi rời rạc hóa A, B, phiên bản rời rạc
sử dụng kích thước bước ∆ có thể được viết lại như:
ht = Aht−1 + Bxt,
yt = Cht.(2)

Cuối cùng, các mô hình tính toán đầu ra thông qua tích chập
toàn cục.
K = (CB, CAB, . . . , CAM−1B),
y = x * K,(3)
trong đó M là độ dài của chuỗi đầu vào x, và K trong RM
là kernel tích chập có cấu trúc.

3.2. Vision Mamba
Tổng quan về Vim được đề xuất được hiển thị trong Hình 2. Mamba
tiêu chuẩn được thiết kế cho chuỗi 1-D. Để
xử lý các tác vụ thị giác, chúng tôi đầu tiên biến đổi hình ảnh 2-D
t trong RH×W×C thành các patch 2-D được làm phẳng xp trong RJ×(P2·C),
trong đó (H,W) là kích thước của hình ảnh đầu vào, C là số
kênh, P là kích thước của các patch hình ảnh. Tiếp theo, chúng tôi chiếu tuyến tính
xp thành vector với kích thước D và thêm embedding
vị trí Epos trong R(J+1)×D, như sau:

T0 = [tcls;t1pW;t2pW;···;tJpW] + Epos, (4)

trong đó tjp là patch thứ j của t, W trong R(P2·C)×D là ma trận
chiếu có thể học. Lấy cảm hứng từ ViT (Dosovitskiy et al.,
2020) và BERT (Kenton & Toutanova, 2019), chúng tôi cũng sử dụng
class token để biểu diễn toàn bộ chuỗi patch, được
ký hiệu là tcls. Sau đó chúng tôi gửi chuỗi token (Tl−1)
đến lớp thứ l của bộ mã hóa Vim, và nhận đầu ra
Tl. Cuối cùng, chúng tôi chuẩn hóa class token đầu ra T0L và
đưa nó vào đầu đa lớp perceptron (MLP) để có
dự đoán cuối cùng ˆp, như sau: Tl = Vim (Tl−1) + Tl−1,
f = Norm (T0L), và ˆp = MLP (f), trong đó Vim là
khối vision mamba được đề xuất, L là số lớp,
và Norm là lớp chuẩn hóa.

3.3. Khối Vim
Khối Mamba gốc được thiết kế cho chuỗi 1-D,
không phù hợp cho các tác vụ thị giác yêu cầu hiểu biết
nhận thức không gian. Trong phần này, chúng tôi giới thiệu khối Vim,
kết hợp mô hình hóa chuỗi hai chiều cho
các tác vụ thị giác. Khối Vim được hiển thị trong Hình 2.

Cụ thể, chúng tôi trình bày các hoạt động của khối Vim trong
Thuật toán 1. Chuỗi token đầu vào Tl−1 đầu tiên được chuẩn hóa
bởi lớp chuẩn hóa. Tiếp theo, chúng tôi chiếu tuyến tính
chuỗi được chuẩn hóa thành x và z với kích thước chiều
E. Sau đó, chúng tôi xử lý x từ hướng tiến và lùi.
Đối với mỗi hướng, chúng tôi đầu tiên áp dụng tích chập 1-D
cho x và nhận x′o. Sau đó chúng tôi chiếu tuyến tính
x′o thành Bo, Co, ∆o, tương ứng. ∆o sau đó được sử dụng để
biến đổi Ao, Bo, tương ứng. Cuối cùng, chúng tôi tính toán
yforward và ybackward thông qua SSM. yforward
và ybackward sau đó được gated bởi z và cộng lại với nhau
để có chuỗi token đầu ra Tl.

3.4. Chi tiết kiến trúc
Tóm lại, các siêu tham số của kiến trúc chúng tôi được
liệt kê như sau: L biểu thị số khối, D biểu thị
chiều trạng thái ẩn, E biểu thị chiều trạng thái
mở rộng, và N biểu thị chiều SSM. Theo
ViT (Dosovitskiy et al., 2020) và DeiT (Touvron et al.,
2021b), chúng tôi đầu tiên sử dụng lớp chiếu kích thước kernel 16 ×16
để có chuỗi 1-D các embedding patch không chồng lấp.
Tiếp theo, chúng tôi trực tiếp xếp chồng L khối Vim. Theo mặc định,
chúng tôi đặt số khối L là 24, chiều SSM N
là 16. Để căn chỉnh với kích thước mô hình của dòng DeiT, chúng tôi
đặt chiều trạng thái ẩn D là 192 và chiều trạng thái
mở rộng E là 384 cho biến thể kích thước tiny. Đối với biến thể
kích thước small, chúng tôi đặt D là 384 và E là 768.

3.5. Phân tích hiệu quả
Các phương pháp dựa trên SSM truyền thống tận dụng biến đổi
Fourier nhanh để tăng tốc hoạt động tích chập như được hiển thị trong
Phương trình (3). Đối với các phương pháp phụ thuộc dữ liệu, chẳng hạn như Mamba,
hoạt động SSM trong Dòng 11 của Thuật toán 1 không còn tương đương
với tích chập. Để giải quyết vấn đề này, Mamba và
Vim được đề xuất chọn cách thân thiện với phần cứng hiện đại
để đảm bảo hiệu quả. Ý tưởng chính của tối ưu hóa này là
tránh IO-bound và memory-bound của các bộ tăng tốc phần cứng
hiện đại (GPU).
4

--- TRANG 5 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Các Patch Được Nhúng Norm𝑥Tiến Conv1dLùi Conv1dSSM Tiến SSM Lùi L×
Bộ Mã Hóa Vision Mamba Hình Ảnh Đầu Vào Bộ Mã Hóa Vision Mamba Làm Phẳng & Chiếu Tuyến Tính Lớp Chiếu Các Token Patch Nhúng Vị Trí Token Lớp 01*Vision Mamba(Vim) Kích Hoạt 𝑧
MLP & Dự Đoán 012345*6789𝑦ℎtiến ℎlùi

Hình 2: Tổng quan về mô hình Vim được đề xuất. Chúng tôi đầu tiên chia hình ảnh đầu vào thành các patch, sau đó chiếu chúng thành các token patch. Cuối cùng, chúng tôi gửi chuỗi các token đến bộ mã hóa Vim được đề xuất. Để thực hiện phân loại ImageNet, chúng tôi nối thêm một token phân loại có thể học được vào chuỗi token patch. Khác với Mamba cho mô hình hóa chuỗi văn bản, bộ mã hóa Vim xử lý chuỗi token với cả hướng tiến và lùi.

Hiệu quả IO. Bộ nhớ băng tần cao (HBM) và SRAM là hai thành phần quan trọng cho GPU. Trong số chúng, SRAM có băng tần lớn hơn và HBM có kích thước bộ nhớ lớn hơn. Việc triển khai tiêu chuẩn của hoạt động SSM của Vim với HBM yêu cầu số lượng IO bộ nhớ bậc O(BMEN). Lấy cảm hứng từ Mamba, Vim đầu tiên đọc vào O(BME+EN) byte bộ nhớ (∆o,Ao,Bo,Co) từ HBM chậm sang SRAM nhanh. Sau đó, Vim nhận được Ao, Bo rời rạc với kích thước (B,M,E,N) trong SRAM. Cuối cùng, Vim thực hiện các hoạt động SSM trong SRAM và ghi đầu ra với kích thước (B,M,E) trở lại HBM. Phương pháp này có thể giúp giảm IO từ O(BMEN) xuống O(BME+EN).

Hiệu quả bộ nhớ. Để tránh vấn đề hết bộ nhớ và đạt được sử dụng bộ nhớ thấp hơn khi xử lý các chuỗi dài, Vim chọn cùng phương pháp tính toán lại như Mamba. Đối với các trạng thái trung gian với kích thước (B,M,E,N) để tính toán gradient, Vim tính toán lại chúng tại lượt truyền ngược của mạng. Đối với các kích hoạt trung gian như đầu ra của các hàm kích hoạt và tích chập, Vim cũng tính toán lại chúng để tối ưu hóa yêu cầu bộ nhớ GPU, vì các giá trị kích hoạt chiếm nhiều bộ nhớ nhưng nhanh để tính toán lại.

Hiệu quả tính toán. SSM trong khối Vim (Dòng 11 trong Thuật toán 1) và self-attention trong Transformer đều đóng vai trò quan trọng trong việc cung cấp ngữ cảnh toàn cục một cách thích ứng. Cho một chuỗi thị giác T trong R1×M×D và cài đặt mặc định E = 2D, độ phức tạp tính toán của self-attention toàn cục và SSM là:

Ω(self-attention) = 4MD2 + 2M2D, (5)
Ω(SSM) = 3M(2D)N + M(2D)N, (6)

trong đó self-attention có độ phức tạp bậc hai theo độ dài chuỗi M, và SSM có độ phức tạp tuyến tính theo độ dài chuỗi M (N là tham số cố định, mặc định là 16). Hiệu quả tính toán làm cho Vim có khả năng mở rộng cho các ứng dụng gigapixel với độ dài chuỗi lớn.

4. Thí nghiệm

Phương pháp kích thước hình ảnh #param. ImageNet top-1 acc.
Convnets
ResNet-18 224 2 12M 69.8
ResNet-50 224 2 25M 76.2
ResNet-101 224 2 45M 77.4
ResNet-152 224 2 60M 78.3
ResNeXt50-32×4d 224 2 25M 77.6
RegNetY-4GF 224 2 21M 80.0
Transformers
ViT-B/16 384 2 86M 77.9
ViT-L/16 384 2 307M 76.5
DeiT-Ti 224 2 6M 72.2
DeiT-S 224 2 22M 79.8
DeiT-B 224 2 86M 81.8
SSMs
S4ND-ViT-B 224 2 89M 80.4
Vim-Ti 224 2 7M 76.1
Vim-Ti† 224 2 7M 78.3 +2.2
Vim-S 224 2 26M 80.3
Vim-S† 224 2 26M 81.4 +1.1
Vim-B 224 2 98M 81.9
Vim-B† 224 2 98M 83.2 +1.3

Bảng 1: So sánh với các backbone khác nhau trên tập validation ImageNet-1K. † biểu thị mô hình được tinh chỉnh với cài đặt chuỗi dài của chúng tôi.

4.1. Phân loại hình ảnh
Cài đặt. Chúng tôi đánh giá Vim trên bộ dữ liệu ImageNet-1K (Deng et al., 2009), chứa 1.28M hình ảnh huấn luyện
5

--- TRANG 6 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Thuật toán 1 Quá trình Khối Vim
Yêu cầu: chuỗi token Tl−1:(B,M,D)
Đảm bảo: chuỗi token Tl:(B,M,D)
1: /* chuẩn hóa chuỗi đầu vào T′l−1*/
2:T′l−1:(B,M,D)←Norm (Tl−1)
3:x:(B,M,E)←Linearx(T′l−1)
4:z:(B,M,E)←Linearz(T′l−1)
5: /* xử lý với hướng khác nhau */
6:for o in {forward, backward} do
7:x′o:(B,M,E)←SiLU (Conv1d o(x))
8:Bo:(B,M,N)←LinearBo(x′o)
9:Co:(B,M,N)←LinearCo(x′o)
10: /* softplus đảm bảo ∆o dương*/
11: ∆o:(B,M,E)←log(1 + exp( Linear∆o(x′o) +
Parameter∆o))
12: /* hình dạng của ParameterAo là (E,N)*/
13: Ao:(B,M,E,N)←∆o⊗ParameterAo
14: Bo:(B,M,E,N)←∆o⊗Bo
15: /* khởi tạo ho và yo với 0*/
16: ho:(B,E,N)←zeros (B,E,N)
17: yo:(B,M,E)←zeros (B,M,E)
18: /* SSM tuần hoàn */
19: for i in {0, ..., M-1} do
20: ho=Ao[:, i,:,:]⊙ho+Bo[:, i,:,:]⊙x′o[:, i,:,None]
21: yo[:, i,:]=ho⊗Co[:, i,:]
22: end for
23:end for
24: /* lấy y được gated*/
25:y′forward :(B,M,E)←yforward⊙SiLU (z)
26:y′backward :(B,M,E)←ybackward⊙SiLU (z)
27: /* kết nối dư */
28:Tl:(B,M,D)←LinearT(y′forward +y′backward ) +Tl−1
29: Return: Tl

hình ảnh và 50K hình ảnh validation từ 1,000 danh mục.
Tất cả mô hình được huấn luyện trên tập huấn luyện, và độ chính xác top-1
trên tập validation được báo cáo. Để so sánh công bằng,
cài đặt huấn luyện của chúng tôi chủ yếu theo DeiT (Touvron et al.,
2021b). Cụ thể, chúng tôi áp dụng cắt ngẫu nhiên, lật ngang
ngẫu nhiên, điều chỉnh label-smoothing, mixup,
và xóa ngẫu nhiên làm tăng cường dữ liệu. Khi huấn luyện trên
hình ảnh đầu vào 2242, chúng tôi sử dụng AdamW (Loshchilov &
Hutter, 2019) với momentum 0.9, tổng kích thước batch
1024, và weight decay 0.05 để tối ưu hóa mô hình. Chúng tôi
huấn luyện các mô hình Vim trong 300 epoch sử dụng lịch trình cosine,
tốc độ học ban đầu 1×10−3, và EMA. Trong quá trình kiểm tra,
chúng tôi áp dụng center crop trên tập validation để cắt ra hình ảnh 2242.
Thí nghiệm được thực hiện trên 8 GPU A800.

Tinh chỉnh chuỗi dài Để tận dụng đầy đủ sức mạnh
mô hình hóa chuỗi dài hiệu quả của Vim, chúng tôi tiếp tục
tinh chỉnh Vim với cài đặt chuỗi dài trong 30 epoch sau
tiền huấn luyện. Cụ thể, chúng tôi đặt stride trích xuất patch
là 8 trong khi giữ nguyên kích thước patch, tốc độ học
hằng số 10−5, và weight decay 10−8.

Kết quả. Bảng 1 so sánh Vim với các mạng backbone dựa trên ConvNet,

Phương pháp Backbone kích thước hình ảnh #param. val mIoU
DeepLab v3+ ResNet-101 512 2 63M 44.1
UperNet ResNet-50 512 2 67M 41.2
UperNet ResNet-101 512 2 86M 44.9
UperNet DeiT-Ti 512 2 11M 39.2
UperNet DeiT-S 512 2 43M 44.0
UperNet Vim-Ti 512 2 13M 41.0
UperNet Vim-S 512 2 46M 44.9

Bảng 2: Kết quả phân đoạn ngữ nghĩa trên tập val ADE20K.

dựa trên Transformer và dựa trên SSM.
So với ResNet dựa trên ConvNet (He et al., 2016), Vim
thể hiện hiệu suất vượt trội. Ví dụ, khi số tham số
gần tương tự, độ chính xác top-1 của Vim-Small đạt 80.3,
cao hơn ResNet50 4.1 điểm. So với ViT dựa trên self-attention
thông thường (Dosovitskiy et al., 2020), Vim vượt trội hơn nó
với biên độ đáng kể về cả số tham số
và độ chính xác phân loại. Khi so sánh với biến thể ViT
được tối ưu hóa cao, tức là DeiT (Touvron et al.,
2021b), Vim vượt qua nó ở các quy mô khác nhau với
số tham số tương đương: cao hơn 3.9 điểm cho Vim-Tiny
so với DeiT-Tiny, cao hơn 0.5 điểm cho Vim-Small so với DeiT-
Small, và cao hơn 0.1 điểm cho Vim-Base so với DeiT-Base.
So với S4ND-ViT-B dựa trên SSM (Nguyen et al.,
2022), Vim đạt được độ chính xác top-1 tương tự với ít hơn 3 lần
tham số. Sau tinh chỉnh chuỗi dài, Vim-Tiny†,
Vim-S†, và Vim-B† đều đạt được kết quả cao hơn. Trong số chúng,
Vim-S† thậm chí đạt được kết quả tương tự với DeiT-B.
Kết quả chứng minh rằng Vim có thể thích ứng với
mô hình hóa chuỗi dài một cách dễ dàng và trích xuất biểu diễn thị giác
mạnh mẽ hơn.

Hình 1 (b) và (c) so sánh FPS và bộ nhớ GPU của
Vim và DeiT kích thước tiny. Vim thể hiện hiệu quả tốt hơn
về tốc độ và bộ nhớ khi độ phân giải hình ảnh tăng. Cụ thể,
khi kích thước hình ảnh là 512×512, Vim đạt được FPS và bộ nhớ
tương tự như DeiT. Khi kích thước hình ảnh tăng lên
1248×1248, Vim nhanh hơn DeiT 2.8 lần và tiết kiệm 86.8%
bộ nhớ GPU. Sự vượt trội rõ rệt của việc mở rộng tuyến tính
theo độ dài chuỗi của Vim làm cho nó sẵn sàng cho các ứng dụng
thị giác xuôi dòng độ phân giải cao và ứng dụng đa phương thức
chuỗi dài.

4.2. Phân đoạn ngữ nghĩa
Cài đặt. Chúng tôi tiến hành thí nghiệm cho phân đoạn ngữ nghĩa
trên ADE20K (Zhou et al., 2019) và sử dụng UperNet (Xiao et al., 2018b) làm framework phân đoạn. Chúng tôi
cung cấp cài đặt chi tiết trong Phần B. Kết quả. Như được hiển thị trong
Bảng 2, Vim luôn vượt trội hơn DeiT trên các
quy mô khác nhau: cao hơn 1.8 mIoU cho Vim-Ti so với DeiT-Ti, và
6

--- TRANG 7 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Backbone APbox APbox50 APbox75 APboxs APboxm APboxl
DeiT-Ti 44.4 63.0 47.8 26.1 47.4 61.8
Vim-Ti 45.7 63.9 49.6 26.1 49.0 63.2

Backbone APmask APmask50 APmask75 APmasks APmaskm APmaskl
DeiT-Ti 38.1 59.9 40.5 18.1 40.5 58.4
Vim-Ti 39.2 60.9 41.7 18.2 41.8 60.2

Bảng 3: Kết quả phát hiện đối tượng và phân đoạn thể hiện
trên tập val COCO sử dụng framework Cascade Mask R-CNN (Cai
& Vasconcelos, 2019).

11.41.82.22.6
51264073810241248FPS w/ log scale Độ phân giải DeiT Vim 2.522.242.001.561.252.272.061.901.70 Nhanh hơn
2.8× nhanh hơn

Hình 3: So sánh FPS giữa DeiT-Ti (Touvron et al.,
2021a) và Vim-Ti của chúng tôi trên framework xuôi dòng
thường được sử dụng. Chúng tôi thực hiện suy luận hàng loạt và đánh giá
FPS theo thang log trên kiến trúc với backbone
và FPN. Vim đạt được hiệu suất tương đương với DeiT
với độ phân giải nhỏ, tức là 512×512. Khi độ phân giải hình ảnh đầu vào
tăng, Vim có FPS cao hơn.

cao hơn 0.9 mIoU cho Vim-S so với DeiT-S. So với
backbone ResNet-101, Vim-S của chúng tôi đạt được hiệu suất
phân đoạn tương tự với gần 2 lần ít tham số hơn.

Để đánh giá thêm hiệu quả cho các tác vụ xuôi dòng,
tức là phân đoạn, phát hiện, và phân đoạn thể hiện, chúng tôi
kết hợp các backbone với mô-đun feature pyramid
network (FPN) thường được sử dụng và đánh giá FPS và bộ nhớ GPU của chúng.
Như được hiển thị trong Hình 3 và Hình 4, các đường cong hiệu quả
thể hiện kết quả so sánh tương tự của backbone thuần (Hình 1),
mặc dù chúng tôi gắn thêm FPN nặng trên các backbone.
Hiệu suất mở rộng tuyến tính đặc biệt được gán cho
backbone Vim hiệu quả được đề xuất của chúng tôi, xây dựng
nền tảng cho việc học biểu diễn thị giác cấp gigapixel
theo cách end-to-end mà không cần mã hóa đa giai đoạn
(ví dụ, hình ảnh hàng không, hình ảnh y tế, và
bệnh lý tính toán).

4.3. Phát hiện đối tượng và phân đoạn thể hiện
Cài đặt. Chúng tôi tiến hành thí nghiệm cho phát hiện đối tượng và
phân đoạn thể hiện trên bộ dữ liệu COCO 2017 (Lin et al.,
2014) và sử dụng ViTDet (Xiao et al., 2018b) làm framework
cơ bản. Chúng tôi cung cấp cài đặt chi tiết trong Phần B.

Kết quả. Bảng 3 so sánh Vim-Ti với DeiT-Ti sử dụng
framework Cascade Mask R-CNN (Cai & Vasconcelos, 2019).
Vim-Ti vượt qua DeiT-Ti 1.3 box AP và 1.1 mask AP.
Đối với các đối tượng kích thước trung bình và lớn, Vim-Ti vượt trội
hơn DeiT-Ti 1.6 APboxm/1.3 APmaskm và 1.4 APboxl/1.8
APmaskl, thể hiện việc học ngữ cảnh tầm xa tốt hơn
so với DeiT (Hình 5).

Chúng tôi nhấn mạnh rằng sự vượt trội về độ chính xác là không tầm thường
vì DeiT được trang bị window attention trong khi Vim
hoạt động theo cách mô hình hóa chuỗi thuần. Cụ thể,
để thực hiện việc học biểu diễn trên hình ảnh độ phân giải cao
(tức là 1024×1024), chúng tôi theo ViTDet (Li et al., 2022c)
và sửa đổi backbone DeiT với việc sử dụng 2D window
attention, tiêm tiên nghiệm 2D và phá vỡ bản chất
mô hình hóa tuần tự của Transformer. Nhờ hiệu quả
được minh họa trong Phần 3.5, Hình 1 và Hình 4, chúng tôi có thể
áp dụng trực tiếp Vim trên hình ảnh đầu vào 1024×1024 và học
biểu diễn thị giác tuần tự cho phát hiện đối tượng và phân đoạn
thể hiện mà không cần tiên nghiệm 2D trong backbone.

4.4. Nghiên cứu phân tích
SSM hai chiều. Chúng tôi phân tích thiết kế hai chiều
chính của Vim, sử dụng phân loại ImageNet-1K và framework
phân đoạn ngữ nghĩa Segmenter (Strudel et al., 2021)
trên ADE20K. Để đánh giá đầy đủ sức mạnh của biểu diễn
đã học trên ImageNet, chúng tôi sử dụng đầu Segmenter đơn giản
chỉ với 2 lớp để thực hiện transfer learning trên
phân đoạn ngữ nghĩa. Chúng tôi nghiên cứu các chiến lược hai chiều sau.
None: Chúng tôi trực tiếp áp dụng khối Mamba để
7

52035506580
51264073810241248Bộ nhớ GPU (GB) Độ phân giải DeiT Vim Nhỏ hơn
-73.2% bộ nhớ OOM
5.046.888.5415.8622.595.528.0912.4840.03

Hình 4: So sánh hiệu quả bộ nhớ GPU giữa
DeiT-Ti (Touvron et al., 2021a) và Vim-Ti của chúng tôi trên framework
xuôi dòng thường được sử dụng. Chúng tôi thực hiện suy luận hàng loạt
và đánh giá bộ nhớ GPU trên kiến trúc với backbone
và FPN. Vim yêu cầu bộ nhớ GPU tương đương với DeiT
với độ phân giải nhỏ, tức là 512×512. Khi độ phân giải hình ảnh đầu vào
tăng, Vim sẽ sử dụng ít bộ nhớ GPU hơn đáng kể.

--- TRANG 8 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Chiến lược hai chiều ImageNet top-1 acc. ADE20K mIoU
None 73.2 32.3
Bidirectional Layer 70.9 33.6
Bidirectional SSM 72.8 33.2
Bidirectional SSM + Conv1d 73.9 35.9

Bảng 4: Nghiên cứu phân tích về thiết kế hai chiều. Để
đảm bảo so sánh công bằng, chúng tôi không sử dụng class token cho
mỗi thí nghiệm. Cài đặt mặc định cho Vim được đánh dấu màu xanh.

xử lý chuỗi thị giác chỉ với hướng tiến.
Bidirectional Sequence: Trong quá trình huấn luyện, chúng tôi ngẫu nhiên lật
chuỗi thị giác. Điều này hoạt động như tăng cường dữ liệu.
Bidirectional Block: Chúng tôi ghép cặp các khối xếp chồng. Khối đầu tiên
của mỗi cặp xử lý chuỗi thị giác theo hướng tiến
và khối thứ hai của mỗi cặp xử lý theo
hướng lùi. Bidirectional SSM: Chúng tôi thêm một SSM bổ sung
cho mỗi khối để xử lý chuỗi thị giác theo
hướng lùi. Bidirectional SSM + Conv1d: Dựa trên
Bidirectional SSM, chúng tôi thêm Conv1d lùi
trước SSM lùi (Hình 2).

Như được hiển thị trong Bảng 4, việc áp dụng trực tiếp khối Mamba
đạt được hiệu suất tốt trong phân loại. Tuy nhiên, cách
một chiều không tự nhiên đặt ra thách thức trong
dự đoán dày đặc xuôi dòng. Cụ thể, chiến lược hai chiều
sơ bộ của việc sử dụng Bidirectional Block đạt được độ chính xác
top-1 thấp hơn 7 điểm trong phân loại. Tuy nhiên, nó vượt trội
hơn khối Mamba một chiều vanilla 1.3 mIoU
trong phân đoạn ngữ nghĩa. Bằng cách thêm SSM lùi và
Conv1d bổ sung, chúng tôi đạt được độ chính xác phân loại vượt trội
(73.9 top-1 acc so với 73.2 top-1 acc) và sự vượt trội
phân đoạn đặc biệt (35.9 mIoU so với 32.3 mIoU). Chúng tôi sử dụng
chiến lược Bidirectional SSM + Conv1d làm cài đặt
mặc định trong khối Vim của chúng tôi.

Thiết kế phân loại. Chúng tôi phân tích thiết kế phân loại
của Vim, đánh giá trên phân loại ImageNet-1K. Chúng tôi
nghiên cứu các chiến lược phân loại sau. Mean pool:
Chúng tôi áp dụng mean pooling trên đặc trưng đầu ra từ khối
Vim cuối cùng và thực hiện phân loại trên đặc trưng được pooling này.
Max pool: Chúng tôi đầu tiên thích ứng đầu phân loại trên mỗi
token của chuỗi thị giác và sau đó thực hiện max pooling
trên chuỗi để có kết quả dự đoán phân loại.
Head class token: Theo DeiT (Touvron et al., 2021b),
chúng tôi nối class token ở đầu chuỗi thị giác
và thực hiện phân loại. Double class token: Dựa trên
chiến lược head class token, chúng tôi thêm một class token
ở cuối chuỗi thị giác. Middle class token: Chúng tôi thêm
class token ở giữa chuỗi thị giác
và sau đó thực hiện phân loại trên middle class token cuối cùng.

Chiến lược phân loại ImageNet top-1 acc.
Mean pool 73.9
Max pool 73.4
Head class token 75.2
Double class token 74.3
Middle class token 76.1

Bảng 5: Nghiên cứu phân tích về thiết kế phân loại. Cài đặt
mặc định cho Vim được đánh dấu màu xanh.

Như được hiển thị trong Bảng 5, các thí nghiệm cho thấy chiến lược
middle class token có thể khai thác đầy đủ bản chất tuần hoàn
của SSM và tiên nghiệm đối tượng trung tâm trong ImageNet, thể hiện
độ chính xác top-1 tốt nhất là 76.1.

5. Kết luận và công việc tương lai
Chúng tôi đã đề xuất Vision Mamba (Vim) để khám phá mô hình
không gian trạng thái hiệu quả rất gần đây, tức là Mamba, làm
backbone thị giác tổng quát. Khác với các mô hình không gian trạng thái
trước đây cho tác vụ thị giác sử dụng kiến trúc lai hoặc
kernel tích chập 2D toàn cục tương đương, Vim học biểu diễn thị giác
theo cách mô hình hóa chuỗi và không giới thiệu thiên lệch
quy nạp cụ thể cho hình ảnh. Nhờ mô hình hóa không gian trạng thái
hai chiều được đề xuất, Vim đạt được ngữ cảnh thị giác
toàn cục phụ thuộc dữ liệu và có cùng sức mạnh mô hình hóa
như Transformer, trong khi có độ phức tạp tính toán thấp hơn.
Hưởng lợi từ thiết kế nhận thức phần cứng của Mamba,
tốc độ suy luận và sử dụng bộ nhớ của Vim tốt hơn đáng kể
so với ViT khi xử lý hình ảnh độ phân giải cao. Kết quả thí nghiệm
trên các benchmark thị giác máy tính tiêu chuẩn đã xác minh
sức mạnh mô hình hóa và hiệu quả cao của Vim, cho thấy rằng Vim
có tiềm năng lớn để trở thành backbone thị giác thế hệ tiếp theo.

Trong các công việc tương lai, Vim với mô hình hóa SSM hai chiều
với embedding vị trí phù hợp cho các tác vụ không giám sát
như tiền huấn luyện mask image modeling và kiến trúc
tương tự với Mamba cho phép các tác vụ đa phương thức như
tiền huấn luyện kiểu CLIP. Dựa trên trọng số Vim đã tiền huấn luyện,
việc khám phá tính hữu ích của Vim để phân tích hình ảnh
y tế độ phân giải cao, hình ảnh viễn thám, và video dài,
có thể được coi như các tác vụ xuôi dòng, rất đơn giản.

Tuyên bố tác động
Chúng tôi thúc đẩy hiệu quả của backbone thị giác tổng quát.
Bất kỳ hệ quả hoặc tác động xã hội nào thường liên quan đến
công việc tập trung vào tăng hiệu quả cũng áp dụng ở đây,
vì công việc như vậy nhất thiết cải thiện tính thực tiễn của
backbone thị giác cho một loạt các ứng dụng thị giác với
hình ảnh đầu vào độ phân giải cao.
8

--- TRANG 9 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Dự án Khoa học và
Công nghệ Quốc gia Lớn số Grant No.
2023YFF0905400 và Quỹ Khoa học Tự nhiên Quốc gia
Trung Quốc (NSFC) số Grant No. 62276108.

Chúng tôi muốn gửi lời cảm ơn đến Tianheng Cheng, Yuxin Fang,
Shusheng Yang, Bo Jiang, và Jingfeng Yao vì phản hồi
hữu ích của họ về bản thảo.

Tài liệu tham khảo
Bao, H., Dong, L., Piao, S., and Wei, F. Beit:
BERT pre-training of image transformers. In ICLR,
2022. URL https://openreview.net/forum?
id=p-BhZSz59o4.

Baron, E., Zimerman, I., and Wolf, L. 2-d ssm: A gen-
eral spatial layer for visual transformers. arXiv preprint
arXiv:2306.06635, 2023.

Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena,
A., Somani, A., and Taşırlar, S. Introducing our mul-
timodal models, 2023. URL https://www.adept.
ai/blog/fuyu-8b.

Cai, Z. and Vasconcelos, N. Cascade r-cnn: High qual-
ity object detection and instance segmentation. TPAMI,
2019.

Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J.,
Bojanowski, P., and Joulin, A. Emerging properties in
self-supervised vision transformers. In ICCV, 2021.

Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509, 2019.

Choromanski, K. M., Likhosherstov, V., Dohan, D., Song,
X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohi-
uddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and
Weller, A. Rethinking attention with performers. In ICLR,
2021. URL https://openreview.net/forum?
id=Ua6zuk0WRH.

Dai, Z., Liu, H., Le, Q. V., and Tan, M. Coatnet: Marrying
convolution and attention for all data sizes. NeurIPS, 34,
2021.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In CVPR, 2009.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805,
2018.

Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W.,
Zheng, N., and Wei, F. Longnet: Scaling transformers to
1,000,000,000 tokens. arXiv preprint arXiv:2307.02486,
2023.

Ding, X., Zhang, X., Han, J., and Ding, G. Scaling up your
kernels to 31x31: Revisiting large kernel design in cnns.
In CVPR, 2022.

Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L.,
Chen, D., and Guo, B. Cswin transformer: A general
vision transformer backbone with cross-shaped windows.
In CVPR, 2022.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. In
ICLR, 2020.

d'Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S.,
Biroli, G., and Sagun, L. Convit: Improving vision trans-
formers with soft convolutional inductive biases. In ICML,
2021.

Fang, J., Xie, L., Wang, X., Zhang, X., Liu, W., and Tian, Q.
Msg-transformer: Exchanging local spatial information
by manipulating messenger tokens. In CVPR, 2022.

Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X.,
Huang, T., Wang, X., and Cao, Y. Eva: Exploring the
limits of masked visual representation learning at scale.
In CVPR, 2023.

Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,
A., and Re, C. Hungry hungry hippos: Towards lan-
guage modeling with state space models. In ICLR,
2023. URL https://openreview.net/forum?
id=COZDy0WYGg.

Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk,
E. D., Le, Q. V., and Zoph, B. Simple copy-paste is a
strong data augmentation method for instance segmenta-
tion. In CVPR, 2021.

Gu, A. and Dao, T. Mamba: Linear-time sequence
modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.

Gu, A., Goel, K., and Ré, C. Efficiently modeling long
sequences with structured state spaces. arXiv preprint
arXiv:2111.00396, 2021a.

Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra,
A., and Ré, C. Combining recurrent, convolutional, and
continuous-time models with linear state space layers. In
NeurIPS, 2021b.
9

--- TRANG 10 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Gu, A., Goel, K., Gupta, A., and Ré, C. On the parameter-
ization and initialization of diagonal state space models.
In NeurIPS, 2022.

Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are
as effective as structured state spaces. In NeurIPS, 2022.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In CVPR, 2016.

Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger,
K. Q. Densely connected convolutional networks. In
CVPR, 2017.

Islam, M. M. and Bertasius, G. Long movie clip classifica-
tion with state-space video models. In ECCV, 2022.

Islam, M. M., Hasan, M., Athrey, K. S., Braskich, T., and
Bertasius, G. Efficient movie scene detection using state-
space transformers. In CVPR, 2023.

Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H.,
Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up
visual and vision-language representation learning with
noisy text supervision. In ICML, 2021.

Kalman, R. E. A new approach to linear filtering and pre-
diction problems. 1960.

Kenton, J. D. M.-W. C. and Toutanova, L. K. Bert: Pre-
training of deep bidirectional transformers for language
understanding. In NAACL-HLT, 2019.

Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The
efficient transformer. In ICLR, 2020. URL https:
//openreview.net/forum?id=rkgNKkHtvB.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classification with deep convolutional neural networks.
In NeurIPS, 2012.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278-2324, 1998.

Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping
language-image pre-training for unified vision-language
understanding and generation. In ICML, 2022a.

Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Boot-
strapping language-image pre-training with frozen im-
age encoders and large language models. arXiv preprint
arXiv:2301.12597, 2023.

Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes
convolutional models great on long sequence modeling?
In ICLR, 2022b.

Li, Y., Mao, H., Girshick, R., and He, K. Exploring plain
vision transformer backbones for object detection. In
ECCV, 2022c.

Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,
Ramanan, D., Dollár, P., and Zitnick, C. L. Microsoft
coco: Common objects in context. In ECCV, 2014.

Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction
tuning. arXiv preprint arXiv:2304.08485, 2023.

Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B.,
Kärkkkäinen, T., Pechenizkiy, M., Mocanu, D., and Wang,
Z. More convnets in the 2020s: Scaling up kernels beyond
51x51 using sparsity. arXiv preprint arXiv:2207.03620,
2022a.

Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q.,
and Liu, Y. Vmamba: Visual state space model. arXiv
preprint arXiv:2401.10166, 2024.

Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,
S., and Guo, B. Swin transformer: Hierarchical vision
transformer using shifted windows. In ICCV, 2021.

Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T.,
and Xie, S. A convnet for the 2020s. In CVPR, 2022b.

Loshchilov, I. and Hutter, F. Decoupled weight decay regu-
larization. In ICLR, 2019.

Ma, J., Li, F., and Wang, B. U-mamba: Enhancing long-
range dependency for biomedical image segmentation.
arXiv preprint arXiv:2401.04722, 2024.

Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B.
Long range language modeling via gated state spaces.
In ICLR, 2023. URL https://openreview.net/
forum?id=5MkYIYCbva.

Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao,
T., Baccus, S., and Ré, C. S4nd: Modeling images and
videos as multidimensional signals with state spaces. In
NeurIPS, 2022.

Qin, Z., Yang, S., and Zhong, Y. Hierarchically gated recur-
rent neural network for sequence modeling. In NeurIPS,
2023. URL https://openreview.net/forum?
id=P1TCHxJwLB.

Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In ICML, 2021.

Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and
Dollár, P. Designing network design spaces. In CVPR,
2020.
10

--- TRANG 11 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global filter
networks for image classification. Advances in neural
information processing systems, 34:980-993, 2021.

Simonyan, K. and Zisserman, A. Very deep convolu-
tional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

Smith, J. T., De Mello, S., Kautz, J., Linderman, S., and
Byeon, W. Convolutional state space models for long-
range spatiotemporal modeling. In NeurIPS, 2023a.

Smith, J. T., Warrington, A., and Linderman, S. Simpli-
fied state space layers for sequence modeling. In ICLR,
2023b. URL https://openreview.net/forum?
id=Ai8Hw3AXqks.

Strudel, R., Garcia, R., Laptev, I., and Schmid, C. Seg-
menter: Transformer for semantic segmentation. In ICCV,
2021.

Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J.,
Wang, J., and Wei, F. Retentive network: A successor to
transformer for large language modelss. arXiv preprint
arXiv:2307.08621, 2023.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,
Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,
A. Going deeper with convolutions. In CVPR, 2015.

Tan, M. and Le, Q. Efficientnet: Rethinking model scaling
for convolutional neural networks. In ICML, 2019.

Tan, M. and Le, Q. Efficientnetv2: Smaller models and
faster training. In ICML, 2021.

Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L.,
Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers,
D., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architec-
ture for vision. In NeurIPS, 2021.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and Jégou, H. Training data-efficient image trans-
formers & distillation through attention. In ICML, 2021a.

Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and Jégou, H. Training data-efficient image trans-
formers & distillation through attention. In ICML, 2021b.

Touvron, H., Bojanowski, P., Caron, M., Cord, M., El-
Nouby, A., Grave, E., Izacard, G., Joulin, A., Synnaeve,
G., Verbeek, J., et al. Resmlp: Feedforward networks for
image classification with data-efficient training. TPAMI,
2022.

Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y.,
Liu, D., Mu, Y., Tan, M., Wang, X., et al. Deep high-
resolution representation learning for visual recognition.
TPAMI, 2020a.

Wang, J., Yan, J. N., Gu, A., and Rush, A. M. Pretrain-
ing without attention. arXiv preprint arXiv:2212.10544,
2022.

Wang, J., Zhu, W., Wang, P., Yu, X., Liu, L., Omar, M., and
Hamid, R. Selective structured state-spaces for long-form
video understanding. In CVPR, 2023a.

Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.
Linformer: Self-attention with linear complexity. arXiv
preprint arXiv:2006.04768, 2020b.

Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D.,
Lu, T., Luo, P., and Shao, L. Pyramid vision transformer:
A versatile backbone for dense prediction without convo-
lutions. In ICCV, 2021.

Wang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu,
X., Lu, T., Lu, L., Li, H., et al. Internimage: Exploring
large-scale vision foundation models with deformable
convolutions. In CVPR, 2023b.

Wang, W., Ma, S., Xu, H., Usuyama, N., Ding, J., Poon,
H., and Wei, F. When an image is worth 1,024 x 1,024
words: A case study in computational pathology. arXiv
preprint arXiv:2312.03558, 2023c.

Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,
and Zhang, L. Cvt: Introducing convolutions to vision
transformers. In ICCV, 2021.

Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified
perceptual parsing for scene understanding. In ECCV,
2018a.

Xiao, T., Liu, Y., Zhou, B., Jiang, Y., and Sun, J. Unified
perceptual parsing for scene understanding. In ECCV,
2018b.

Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggre-
gated residual transformations for deep neural networks.
In CVPR, 2017.

Xing, Z., Ye, T., Yang, Y., Liu, G., and Zhu, L. Segmamba:
Long-range sequential modeling mamba for 3d medical
image segmentation. arXiv preprint arXiv:2401.13560,
2024.

Yan, J. N., Gu, J., and Rush, A. M. Diffusion models without
attention. arXiv preprint arXiv:2311.18257, 2023.

Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., and
Gao, J. Focal self-attention for local-global interactions
in vision transformers. arXiv preprint arXiv:2107.00641,
2021.

Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng,
J., and Yan, S. Metaformer is actually what you need
for vision. In Proceedings of the IEEE/CVF conference
11

--- TRANG 12 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

on computer vision and pattern recognition, pp. 10819-
10829, 2022.

Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso,
A., and Torralba, A. Semantic understanding of scenes
through the ade20k dataset. IJCV, 2019.
12

--- TRANG 13 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

A. Trực quan hóa

GT
Vim-Ti
DeiT-Ti

Hình 5: So sánh trực quan hóa của DeiT-Ti (Touvron et al., 2021b) và Vim-Ti của chúng tôi trên framework Cascade Mask R-CNN (Cai & Vasconcelos, 2019). Nhờ việc học ngữ cảnh tầm xa của SSM, chúng tôi có thể nắm bắt đối tượng rất lớn trong hình ảnh, mà đối tác DeiT-Ti không thể nhận thức được.

B. Cài đặt bổ sung
Cài đặt cho phân đoạn ngữ nghĩa. Chúng tôi tiến hành thí nghiệm cho phân đoạn ngữ nghĩa trên bộ dữ liệu ADE20K (Zhou et al., 2019). ADE20K chứa 150 danh mục ngữ nghĩa chi tiết, với 20K, 2K, và 3K hình ảnh cho huấn luyện, validation, và kiểm tra, tương ứng. Chúng tôi chọn UperNet (Xiao et al., 2018a) làm framework cơ sở. Trong quá trình huấn luyện, chúng tôi sử dụng AdamW với weight decay 0.01, và tổng kích thước batch 16 để tối ưu hóa mô hình. Lịch trình huấn luyện được sử dụng có tốc độ học ban đầu 6×10−5, suy giảm tốc độ học tuyến tính, khởi động tuyến tính 1,500 lần lặp, và tổng cộng 160K lần lặp huấn luyện. Các tăng cường dữ liệu theo cài đặt phổ biến, bao gồm lật ngang ngẫu nhiên, thay đổi tỷ lệ ngẫu nhiên trong phạm vi tỷ lệ [0.5,2.0], và biến dạng quang học ngẫu nhiên. Trong quá trình đánh giá, chúng tôi thay đổi tỷ lệ hình ảnh để có cạnh ngắn hơn là 512.

Cài đặt cho phát hiện đối tượng và phân đoạn thể hiện. Chúng tôi tiến hành thí nghiệm cho phát hiện đối tượng và phân đoạn thể hiện trên bộ dữ liệu COCO 2017 (Lin et al., 2014). Bộ dữ liệu COCO 2017 chứa 118K hình ảnh cho huấn luyện, 5K hình ảnh cho validation, và 20K hình ảnh cho kiểm tra. Chúng tôi sử dụng Cascade Mask R-CNN tiêu chuẩn (Cai & Vasconcelos, 2019) làm framework cơ sở. Đối với các backbone dựa trên ViT, chúng tôi áp dụng các cấu hình bổ sung (ví dụ, attention cửa sổ & toàn cục xen kẽ) để xử lý hình ảnh độ phân giải cao theo ViTDet (Li et al., 2022c). Đối với Vim dựa trên SSM, chúng tôi sử dụng trực tiếp mà không có bất kỳ sửa đổi nào. Các cài đặt huấn luyện và đánh giá khác hoàn toàn giống nhau. Trong quá trình huấn luyện, chúng tôi sử dụng AdamW với weight decay 0.1, và tổng kích thước batch 64 để tối ưu hóa mô hình. Lịch trình huấn luyện được sử dụng có tốc độ học ban đầu 1×10−4, suy giảm tốc độ học tuyến tính, và tổng cộng 380K lần lặp huấn luyện. Các tăng cường dữ liệu sử dụng tăng cường dữ liệu jitter quy mô lớn (Ghiasi et al., 2021) đến hình ảnh đầu vào 1024×1024. Trong quá trình đánh giá, chúng tôi thay đổi tỷ lệ hình ảnh để có cạnh ngắn hơn là 1024.

C. So sánh mở rộng về kiến trúc phân cấp
Để so sánh thêm với các kiến trúc phân cấp, chúng tôi đề xuất biến thể khác Hier-Vim bằng cách thay thế shifted local window attention trong SwinTransformer bằng SSM hai chiều toàn cục được đề xuất. Chúng tôi trình bày chi tiết cấu hình trong Bảng 6

Mô hình #Khối #Kênh Tham số
Hier-Vim-T [2, 2, 5, 2] [96, 192, 384, 768] 30M
Hier-Vim-S [2, 2, 15, 2] [96, 192, 384, 768] 50M
Hier-Vim-B [2, 2, 15, 2] [128, 256, 512, 1024] 89M

Bảng 6: Cấu hình chi tiết của các biến thể khác nhau của Hier-Vim. Chúng tôi cung cấp số kênh và khối trong 4 giai đoạn.

Phân loại trên ImageNet. Theo các giao thức huấn luyện và validation tiêu chuẩn (Liu et al., 2021; 2024), chúng tôi so sánh
13

--- TRANG 14 ---
Vision Mamba: Học biểu diễn thị giác hiệu quả với mô hình không gian trạng thái

Phương pháp kích thước hình ảnh #param. ImageNet top-1 acc.
Swin-T (Liu et al., 2021) 224 2 28M 81.2
FocalTransformer-T (Yang et al., 2021) 224 2 29M 82.2
CVT-21 (Wu et al., 2021) 224 2 32M 82.5
MetaFormer-S35 (Yu et al., 2022) 224 2 31M 81.4
GFNet-H-S (Rao et al., 2021) 224 2 32M 81.5
Hier-Vim-T 224 2 30M 82.5
Swin-S (Liu et al., 2021) 224 2 50M 83.2
FocalTransformer-S (Yang et al., 2021) 224 2 51M 83.5
MetaFormer-S35 (Yu et al., 2022) 224 2 73M 82.5
GFNet-H-B (Rao et al., 2021) 224 2 54M 82.9
Hier-Vim-S 224 2 50M 83.4
Swin-B (Liu et al., 2021) 224 2 88M 83.5
FocalTransformer-B (Yang et al., 2021) 224 2 90M 83.8
Hier-Vim-B 224 2 89M 83.9

Bảng 7: So sánh với các kiến trúc phân cấp trên tập validation ImageNet-1K.

Hier-Vim với các kiến trúc phân cấp phổ biến trên các kích thước mô hình tiny, small, và base trong Bảng 7. Kết quả cho thấy Hier-Vim vượt trội hơn Swin Transformer 1.3% ở kích thước tiny, 0.2% ở kích thước small, và 0.4% ở kích thước base, thể hiện hiệu suất cạnh tranh so với các kiến trúc phân cấp hiện đại được thiết lập tốt và tối ưu hóa cao.
14
