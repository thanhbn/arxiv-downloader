Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, và Christopher R ´e. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024.

Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G ¨unter Klambauer, Johannes Brandstetter, và Sepp Hochreiter. xlstm: Extended long short-term memory. 2024. URL https://api.semanticscholar.org/CorpusID:269614336.

Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, và cộng sự. Piqa: Reasoning about physical commonsense in natural language. Trong Proceedings of the AAAI conference on artificial intelligence, tập 34, trang 7432-7439, 2020.

Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, và John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, và cộng sự. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, và Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, và Christopher R'e. Flashattention: Fast and memory-efficient exact attention with io-awareness. ArXiv, abs/2205.14135, 2022. URL https://api.semanticscholar.org/CorpusID:249151871.

Yann N Dauphin, Angela Fan, Michael Auli, và David Grangier. Language modeling with gated convolutional networks. Trong International conference on machine learning, trang 933-941. PMLR, 2017.

Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, và cộng sự. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024.

DeepSeek-AI và Damai Dai. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. 2024. URL https://api.semanticscholar.org/CorpusID:269613809.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE conference on computer vision and pattern recognition, trang 248-255. Ieee, 2009.

Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, và Christopher R ´e. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022.

Aaron Gokaslan và Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.

Albert Gu và Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.

Albert Gu, Karan Goel, và Christopher R ´e. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.

Ankit Gupta, Albert Gu, và Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022.

Mark Harris, Shubhabrata Sengupta, và John D Owens. Parallel prefix sum (scan) with cuda. GPU gems, 3(39):851-876, 2007.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. Trong International Conference on Machine Learning, trang 5156-5165. PMLR, 2020.

Brian Kulis và Peter L Bartlett. Implicit online learning. Trong Proceedings of the 27th International Conference on Machine Learning (ICML-10), trang 575-582, 2010.

Victor Kuperman và Julie A. Van Dyke. Individual differences in visual comprehension of morphological complexity. Cognitive Science, 33, 2011. URL https://api.semanticscholar.org/CorpusID:5555496.

Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, và Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022.

Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

Harsh Mehta, Ankit Gupta, Ashok Cutkosky, và Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022.

Martial Mermillod, Aurélia Bugaiska, và Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013.

Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, và cộng sự. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.

Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, và Soham De. Resurrecting recurrent neural networks for long sequences. Trong International Conference on Machine Learning, trang 26670-26698. PMLR, 2023.

Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, và cộng sự. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023.

Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, và cộng sự. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024.

Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, và Christopher R ´e. Hyena hierarchy: Towards larger convolutional language models. Trong International Conference on Machine Learning, trang 28043-28078. PMLR, 2023.

Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, và Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a.

Zhen Qin, Songlin Yang, và Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b.

Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, và Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, và Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.

Imanol Schlag và Jürgen Schmidhuber. Gated fast weights for on-the-fly neural program generation. Trong NIPS Metalearning Workshop, 2017.

Imanol Schlag, Kazuki Irie, và Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. Trong International Conference on Machine Learning, trang 9355-9366. PMLR, 2021.

Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131-139, 1992.

Jürgen Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets. Trong ICANN'93: Proceedings of the International Conference on Artificial Neural Networks Amsterdam, The Netherlands 13-16 September 1993 3, trang 460-463. Springer, 1993.

Jimmy TH Smith, Andrew Warrington, và Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022.

Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, và Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.

Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, và Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states. 2024. URL https://api.semanticscholar.org/CorpusID:271039606.

Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, và Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, và cộng sự. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Junxiong Wang, Jing Nathan Yan, Albert Gu, và Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022.

Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, và Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023.

Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, và Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, và Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.

Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. Trong Proceedings of the 20th international conference on machine learning (icml-03), trang 928-936, 2003.

A CÁC MÔ HÌNH KHÔNG GIAN TRẠNG THÁI SÂU TRƯỚC ĐÂY

Ví dụ A.1 (Các Biến thể Linear Attention). Linear Attention (LA) (Katharopoulos et al., 2020), Retention Network (RetNet) (Sun et al., 2023), và Gated Linear Attention (GLA) (Yang et al., 2023) đều giả định At, Bt cho đầu ra hạng-1 (hoặc thậm chí hằng số):

St = At⊙St−1 + v(xt)⊗k(xt), với {At = 1 (LA), At = c∈[0,1] (RetNet), At = 1⊗α(xt) (GLA)}

trong đó St∈ℝd×m, v(xt)∈ℝd, k(xt)∈ℝm là các ánh xạ tuyến tính của xt, và ⊗ biểu thị tích ngoài. Trong thực tế, có thể sử dụng h head như trong multi-head attention để tiết kiệm một số tính toán, trong đó các chiều m và d được chia thành h nhóm và mỗi nhóm thực hiện biến thể LA riêng của nó. Độ phức tạp tích ngoài giảm xuống O(h×m/h×d/h = md/h). Nhưng khi đó kích thước hiệu quả của St cũng thu nhỏ xuống md/h.

Ví dụ A.2 (Mamba (Gu & Dao, 2023)). Kiến trúc Mamba được rút ra bằng cách rời rạc hóa một động lực tuyến tính liên tục. Cập nhật rời rạc hóa của nó là:

St = At⊙St−1 + Bt, trong đó
At = exp(A⊙(ε(xt)⊗1)), Bt = (ε(xt)⊙xt)⊗k(xt).    (8)

trong đó St∈ℝd×m với m = 16 theo mặc định, ε(xt)∈ℝd, k(xt)∈ℝm là các ánh xạ tuyến tính của xt, và A∈ℝd×m là ma trận trọng số có thể huấn luyện độc lập dữ liệu (không phụ thuộc vào xt).

Trong Mamba, cả At và Bt đều phụ thuộc vào ε(xt), đại diện cho kích thước bước cho cập nhật SSM. Trong thực tế, Mamba không sử dụng multiple head như trong các biến thể linear attention. Có lẽ lý do chính là với m và d cố định, trạng thái bộ nhớ lớn nhất sẽ là với h = 1 (vì kích thước hiệu quả của St là md/h). Ngoài ra, đầu ra của Mamba là ot = C(xt)St + Dt⊙xt, có phần dư bổ sung Dt⊙xt.

Ví dụ A.3 (Griffin (De et al., 2024)). Trong Mamba và các biến thể linear attention, tích ngoài đóng vai trò quan trọng trong việc nâng vector lên ma trận. Kiến trúc Griffin gần đây từ bỏ tích ngoài và thực hiện tích từng phần tử thuần túy:

st = a(xt)⊙st−1 + (1−a(xt))⊙i(xt)⊙xt,

trong đó st, a(xt), i(xt) đều ∈ℝd. Điều này cho ra các trạng thái bộ nhớ nhỏ hơn, nhưng trong thực tế, Griffin được kết hợp với local attention (tức là sliding-window self-attention) để tăng cường khả năng của nó.

Ví dụ A.4 (RWKV (Peng et al., 2023)). RWKV gốc cũng thực hiện đệ quy từng phần tử. Nó duy trì một trạng thái dạng tỷ số st = ut/zt, trong đó ut, zt được cập nhật riêng biệt bởi hai SSM:

st = ut/zt
ut = exp(−w)×ut−1 + exp(k(xt))⊙v(xt), zt = exp(−w)×zt−1 + exp(k(xt)),

trong đó tất cả các vector có kích thước ℝd, và w > 0 là trọng số có thể huấn luyện để kiểm soát quên. Trong phiên bản RWKV mới nhất (Peng et al., 2024), mẫu số zt được loại bỏ, và tích từng phần tử được thay thế bằng tích ngoài, làm cho nó giống hơn với biến thể LA.

Ví dụ A.5 (HGRN2 (Qin et al., 2024a)). Mô hình Gated Linear RNN với State Expansion (HGRN2) được biểu diện bằng đệ quy sau:

St = (1⊗f(xt))⊙St−1 + i(xt)⊗(1−f(xt)).

Ở đây, f(xt)∈[0,1] là cổng quên, (1−f(xt)) là cổng đầu vào, và i(xt) là vector đầu vào. HGRN2 do đó giống như một RNN.

B MỤC TIÊU TRỰC TUYẾN CỦA CÁC MÔ HÌNH KHÔNG GIAN TRẠNG THÁI HIỆN CÓ

Chúng tôi kỹ thuật ngược một số mục tiêu học trực tuyến của các SSM sâu hiện có trong Bảng 4.

[Bảng 4: Một số SSM hiện có và các mục tiêu/cập nhật học trực tuyến tương ứng của chúng.]

Phương pháp | Mục tiêu Học Trực tuyến Lt(s) (giả sử xt∈ℝ) | Cập nhật Trực tuyến
---|---|---
LA | ||S−St−1||²F−2⟨Skt,xt⟩ | St = St−1 + xt⊗kt
RetNet | γ||S−St−1||²+(1−γ)||S||²F−2⟨Skt,xt⟩ | St = γSt−1 + xt⊗kt
GLA | ||S−St−1⊙diag(αt)||²F+2⟨Skt,xt⟩ | St = St−1⊙diag(αt) + xt⊗kt
Griffin | √αt⊙(s−st−1)²+√1−αt⊙s²−2√1−αt⊙s⊙it⊙xt | st = αt⊙st−1 + (1−αt)⊙it⊙xt
Longhorn | ||S−St−1||²F+||Skt−xt||²diag(βt) | St = (1m×n−εt⊗kt⊙²)⊙St−1 + (εt⊙xt)⊗kt, εt = βt/(1+βt||kt||²)

C CHỨNG MINH

Phần này cung cấp chứng minh cho Định lý 3.1. Cho mục tiêu của Longhorn St = arg minS∈ℝd×m{||S−St−1||²F+||Skt−xt||²diag(βt)}, chúng ta có định lý sau:

Định lý C.1. Nghiệm dạng đóng cho St cho mục tiêu trong Phương trình 5 là

St,i = (I−εt,iktktᵀ)St−1,i + εt,iktxt,i, trong đó εt,i = βt,i/(1+βt,iktᵀkt) ∈ [0,∞).    (9)

Chứng minh. Vì mục tiêu trong phương trình 5 ở dạng bậc hai đối với s, có một cực tiểu duy nhất. Quan sát rằng mỗi hàng của S (ví dụ: Si) tối ưu hóa mục tiêu độc lập, do đó chúng ta có thể giải nghiệm theo từng hàng. Bằng cách đặt đạo hàm của ∇SiLt = 0, chúng ta có:

∇SiLt = 0 ⟺ (Si−St−1,i) + βt,i(SiᵀkT−xt,i)kt = 0
⟺ (I+βt,iktktᵀ)Si = St−1,i + βt,iktxt,i
⟺ Si = (I−βt,i/(1+βt,raktktᵀ)ktktᵀ)St−1,i + (I−βt,i/(I+βt,iktktᵀ)ktktᵀ)βt,iktxt,i/(I+βt,iktᵀkt)
⟺ (I−βt,i/(1+βt,iktᵀkt)ktktᵀ)St−1,i + βt,iktxt,i/(1+βt,iktᵀkt)

(3) được rút ra từ thực tế rằng (I+βt,iktktᵀ)⁻¹ = (I−βt,iktktᵀ/(1+βt,iktᵀkt)) bằng công thức Sherman-Morrison. (5) được rút ra bằng cách nhận thấy rằng ktᵀktktxt,i − ktktᵀktxt,i = 0.

D CHI TIẾT THỰC NGHIỆM BỔ SUNG

Chúng tôi cung cấp chi tiết kiến trúc để thực hiện các thực nghiệm định luật mở rộng trên OpenWebText trong Bảng 5. Các cấu hình kiến trúc tuân theo chính xác từ bài báo Mamba (Gu & Dao, 2023).

[Bảng 5: Chi tiết huấn luyện trên OpenWebText.]

Tham số | n layers | d model | n heads / d head | Training steps | Learning Rate | Batch Size | Tokens
---|---|---|---|---|---|---|---
125M | 12 | 768 | 12 / 64 | 4800 | 6e-4 | 0.5M tokens | 2.5B
350M | 24 | 1024 | 16 / 64 | 13500 | 3e-4 | 0.5M tokens | 7B
