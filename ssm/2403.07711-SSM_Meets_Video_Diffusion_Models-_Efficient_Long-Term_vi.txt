SSM Gặp Gỡ Các Mô Hình Khuếch Tán Video: Tạo Video Dài Hạn Hiệu Quả với Không Gian Trạng Thái Chọn Lọc

Yuta Oshimaa, Shohei Taniguchia, Masahiro Suzukia, Yutaka Matsuoa
aTrường Đại học Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo, Nhật Bản

Tóm tắt
Với những thành tựu đáng chú ý trong tạo ảnh thông qua các mô hình khuếch tán, cộng đồng nghiên cứu đã thể hiện sự quan tâm ngày càng tăng trong việc mở rộng các mô hình này để tạo video. Các mô hình khuếch tán gần đây cho tạo video chủ yếu sử dụng các lớp attention để trích xuất đặc trưng thời gian. Tuy nhiên, các lớp attention bị hạn chế bởi chi phí tính toán của chúng, tăng theo bậc hai với độ dài chuỗi. Hạn chế này đặt ra những thách thức đáng kể khi tạo các chuỗi video dài hơn bằng các mô hình khuếch tán. Để vượt qua thách thức này, chúng tôi đề xuất tận dụng các mô hình không gian trạng thái (SSM) làm bộ trích xuất đặc trưng thời gian. SSM (ví dụ, Mamba) gần đây đã thu hút sự chú ý như những lựa chọn thay thế đầy hứa hẹn do tiêu thụ bộ nhớ thời gian tuyến tính tương ứng với độ dài chuỗi. Phù hợp với nghiên cứu trước đây cho rằng việc sử dụng SSM hai chiều có hiệu quả để hiểu các đặc trưng không gian trong tạo ảnh, chúng tôi thấy rằng tính hai chiều cũng có lợi cho việc nắm bắt các đặc trưng thời gian trong dữ liệu video, thay vì dựa vào SSM một chiều truyền thống. Chúng tôi đã tiến hành đánh giá toàn diện trên nhiều bộ dữ liệu video dài hạn, như MineRL Navigate, với các kích thước mô hình khác nhau. Đối với các chuỗi lên đến 256 khung hình, các mô hình dựa trên SSM yêu cầu ít bộ nhớ hơn để đạt được FVD tương tự như các mô hình dựa trên attention. Hơn nữa, các mô hình dựa trên SSM thường mang lại hiệu suất tốt hơn với việc sử dụng bộ nhớ GPU tương đương. Mã nguồn của chúng tôi có sẵn tại https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.

Từ khóa: tạo video, mô hình khuếch tán, mô hình không gian trạng thái

1. Giới thiệu
Nghiên cứu về tạo video sử dụng các mô hình khuếch tán [56,46,29] là tiên tiến trong lĩnh vực các mô hình tạo sinh sâu. Thành công của việc tạo ảnh

×T
Lớp Không Gian Lớp Thời Gian
(a) Mô Hình Khuếch Tán Video (b) Lớp Attention Thời Gian
Chuẩn hóa Lớp+
(c) Lớp SSM Thời Gian (Của chúng tôi)
Attention
Độ phức tạp O(L²)
+
SSM Hai Chiều
Độ phức tạp O(L)
× L
Chuẩn hóa Lớp

Hình 1: (a) Các mô hình khuếch tán video dựa trên U-Net bao gồm các lớp không gian và lớp thời gian. (b) Các phương pháp thông thường sử dụng cơ chế attention trong các lớp thời gian, dẫn đến tăng trưởng bậc hai trong phân bổ bộ nhớ và thời gian tính toán khi độ dài chuỗi L tăng. (c) Chúng tôi đề xuất sử dụng SSM trong các lớp thời gian, nơi phân bổ bộ nhớ và thời gian tính toán chỉ tăng tuyến tính với độ dài chuỗi L.

sử dụng các mô hình khuếch tán, đặc biệt là Mô hình Xác suất Khuếch tán Khử nhiễu (DDPM) [29], đã tạo ra một làn sóng nghiên cứu về việc áp dụng các mô hình khuếch tán cho tạo video. Xu hướng này được minh họa bằng sự xuất hiện của các mô hình khuếch tán video (VDM) [31]. Bằng cách khai thác khả năng biểu diễn đáng kể vốn có trong các mô hình khuếch tán, việc ứng dụng chúng vào tạo video đã thể hiện hiệu suất ấn tượng trong việc mô hình hóa bản chất động và phức tạp của nội dung video [31, 54, 30].

Tuy nhiên, nghiên cứu về tạo video dựa trên mô hình khuếch tán đối mặt với những thách thức đáng kể về độ phức tạp tính toán liên quan đến độ dài chuỗi video. Trong các mô hình khuếch tán cho tạo video, các cơ chế attention [61] được sử dụng để nắm bắt các mối quan hệ thời gian [31,54,30,70,3]. Trong các nghiên cứu đầu về mô hình khuếch tán cho tạo video, như VDM, để nắm bắt các mối quan hệ thời gian qua các khung video, các lớp attention thời gian đã được thêm vào sau các lớp attention không gian trong kiến trúc của các mô hình khuếch tán cho tạo ảnh, như mô tả trong Hình 1(a)(b). Tuy nhiên, nhu cầu bộ nhớ của các lớp attention, tăng theo bình phương của độ dài chuỗi, đặt ra những thách thức đáng kể để mở rộng các mô hình này để xử lý các chuỗi dài hơn. Do đó, nghiên cứu hiện tại chủ yếu tập trung vào tạo video ngắn hạn, thường khoảng 16 khung hình mỗi lần suy luận [31,54,3], tương đương với ít hơn một giây video ở 30 fps.

Gần đây, các mô hình không gian trạng thái (SSM) [21,22,23,55], đặc biệt là Mamba [20], đã được xác định là những lựa chọn thay thế đầy hứa hẹn cho các cơ chế attention. Trái ngược với các cơ chế attention, SSM có thể xử lý dữ liệu tuần tự với độ phức tạp tuyến tính, vì vậy chúng được kỳ vọng sẽ vượt qua hạn chế cơ bản của các mô hình dựa trên attention trong nhiều tác vụ mô hình hóa chuỗi. Các nghiên cứu tiếp theo cũng đã chỉ ra rằng SSM là những lựa chọn thay thế hiệu quả cho các phương pháp dựa trên attention trong nhiều lĩnh vực khác nhau (xem phần 4.2).

Một phát hiện quan trọng từ điều tra của chúng tôi là, trong bối cảnh tạo video, việc áp dụng phương pháp hai chiều cho phép SSM hiểu toàn diện hơn các động lực thời gian trong dữ liệu video (Hình 1(c)). Phát hiện này phù hợp với nghiên cứu trước đây cho thấy việc kết hợp SSM quan sát các hướng khác nhau tăng cường chất lượng tạo sinh của các mô hình nắm bắt các phụ thuộc không gian trong ảnh [67,33,13]. Đáng chú ý, các quan sát thực nghiệm của chúng tôi tiết lộ rằng việc thay thế ngây thơ các lớp attention thời gian trong VDM bằng SSM một chiều hoạt động kém đáng kể so với VDM dựa trên attention ban đầu.

Chúng tôi đầu tiên thực hiện các thí nghiệm trên bộ dữ liệu MineRL Navigate [24,52], một bộ dữ liệu video dài hạn được sử dụng rộng rãi, thay đổi số khung hình thành 16, 64 và 256, để so sánh chi phí tính toán và chất lượng tạo video của các mô hình dựa trên SSM với các mô hình dựa trên attention. Đối với các chuỗi dài 256 khung hình, chúng tôi thấy rằng các mô hình dựa trên SSM, ngay cả với kích thước mô hình lớn hơn, có thể được huấn luyện với việc sử dụng bộ nhớ tương tự như các mô hình dựa trên attention. Điều này dẫn đến giảm yêu cầu bộ nhớ để đạt được FVD tương tự như các mô hình dựa trên attention. Hơn nữa, khi việc sử dụng bộ nhớ GPU có thể so sánh, các mô hình dựa trên SSM thường vượt trội hơn các mô hình dựa trên attention về hiệu suất. Hiệu quả của các mô hình dựa trên SSM ở 256 khung hình cũng được xác nhận trên một số bộ dữ liệu video dài hạn, bao gồm GQN-Mazes [12,52] và CARLA Town01 [9, 25].

2. Bối cảnh
Trong phần này, chúng tôi cung cấp một cái nhìn tổng quan về hiểu biết ban đầu của chúng tôi về các mô hình khuếch tán cho tạo video. Các mô hình khuếch tán ban đầu được giới thiệu bởi Sohl-Dickstein và cộng sự [56], và Ho và cộng sự [29] đã tiến xa hơn trong lĩnh vực này với việc giới thiệu các mô hình xác suất khuếch tán khử nhiễu (DDPM). DDPM đã giới thiệu một thuật toán huấn luyện thực tế, đặc biệt phù hợp cho tạo ảnh. Những tiến bộ gần đây trong các mô hình khuếch tán cho tạo video liên quan đến việc mở rộng kiến trúc DDPM để phù hợp với dữ liệu video.

2.1. Mô hình Xác suất Khuếch tán Khử nhiễu (DDPM)
Trong các mô hình khuếch tán, quá trình thuận liên quan đến việc giảm dần tín hiệu dữ liệu gốc, x0, bằng cách dần dần đưa vào nhiễu Gaussian khi thời gian khuếch tán, t, tiến triển. Chuỗi biến đổi này dẫn x0 hội tụ về nhiễu Gaussian thuần túy, được biểu diễn là xT∼ N(xT;0,I), tại thời điểm T. Trong nghiên cứu của chúng tôi, t được xử lý như một số nguyên rời rạc trong phạm vi [0, T], mặc dù một số nghiên cứu đã xem xét t như một biến liên tục [57,37]. Quá trình thuận được điều chỉnh bởi quá trình Markov sau:

q(x1:T|x0) = ∏t=1^T q(xt|xt−1), (1)
q(xt|xt−1) = N(xt;√αtx0,(1−αt)I). (2)

Trong công thức này, chuỗi βt thỏa mãn 0 < β1 < ··· < βT−1 < βT < 1.

Quá trình tạo sinh trong các mô hình khuếch tán là quá trình ngược. Quá trình này bắt đầu với nhiễu Gaussian thuần túy xT∼ N(xT;0,I) và dần dần tái tạo dữ liệu về x0 gốc. Trong quá trình ngược, mỗi bước pθ(xt−1|xt) được mô hình hóa bằng một mạng neural.

pθ(x1:T) = p(xT)∏t=1^T pθ(xt−1|xt), (3)
pθ(xt−1|xt) = N(xt−1;μθ(xt, t),Σθ(xt, t)), (4)
p(xT) = N(xT;0,I). (5)

Thông thường, Σθ được đặt là một hằng số phụ thuộc thời gian không thể huấn luyện, Σθ(xt, t) = βtI. Ngoài ra, với sự thay đổi trong tham số hóa của μθ, quá trình ngược pθ(xt−1|xt) có thể được biểu diễn như:

pθ(xt−1|xt) = N(xt−1; 1/√αt(xt + βt²/√ᾱt−1 εθ(xt, t)), βtI), (6)

trong đó ᾱt = ∏i=1^t(1−βi²). Thuật ngữ εθ(xt, t) biểu diễn một hàm dự đoán nhiễu từ dữ liệu nhiễu xt. Tham số hóa này dẫn đến một hàm mục tiêu cho DDPM được cấu trúc như sau:

Ex0,ε,t[||ε−εθ(xt, t)||2²], (7)

trong đó xt = √ᾱtx0 + √1−ᾱtε. Mặc dù nhiều tham số hóa được công nhận, như dự đoán dữ liệu quan sát x0 từ phần tương ứng nhiễu xt hoặc v-prediction [31, 51, 37], chúng tôi chọn sử dụng ε-prediction.

Về kiến trúc của các mô hình khuếch tán, kiến trúc U-Net 2D [49] thường được sử dụng cho dữ liệu ảnh. Trong các mô hình dựa trên U-Net 2D, các lớp attention không gian được kết hợp giữa các lớp tích chập. Các lớp attention không gian này tăng cường khả năng tập trung vào các đặc trưng không gian có liên quan và cải thiện chất lượng của các ảnh được tạo. Mặc dù chúng tôi áp dụng kiến trúc dựa trên U-Net cho mô hình khuếch tán, Peebles và Xie [47] đã khám phá các kiến trúc dựa trên Vision Transformers (ViT) [10].

2.2. Kiến trúc cho Mô hình Khuếch tán Video
Để tạo video, các mô hình khuếch tán cần bao gồm cả đặc trưng không gian và thời gian qua các khung hình. Trong khi DDPM thường bao gồm sự kết hợp của U-Net và lớp attention không gian, khả năng của chúng chủ yếu bị giới hạn trong việc nắm bắt đặc trưng không gian.

Để giải quyết hạn chế này, Mô hình Khuếch tán Video (VDM) [31] được giới thiệu như một nỗ lực ban đầu tại tạo video sử dụng các mô hình khuếch tán. Bằng cách kết hợp các cơ chế để nắm bắt động lực thời gian trong DDPM, VDM tăng cường khả năng nắm bắt đặc trưng thời gian (Hình 1 (a)(b)). Các lớp attention thời gian thường được sử dụng trong các mô hình khuếch tán tạo video, như VDM, để tận dụng các phụ thuộc chuỗi thời gian. Tuy nhiên, attention thời gian yêu cầu bộ nhớ tỷ lệ với bình phương của độ dài chuỗi, điều này áp đặt hạn chế về độ dài tối đa của các chuỗi video có thể được tạo ra cùng một lúc. Trong nghiên cứu của chúng tôi, chúng tôi áp dụng VDM làm đường cơ sở để khám phá các thách thức hiện tại và những cải tiến tiềm năng trong tạo video sử dụng các mô hình khuếch tán.

3. Phương pháp
Trong phần này, chúng tôi đề xuất kiến trúc của một lớp SSM thời gian (mô hình không gian trạng thái) để sử dụng trong các mô hình khuếch tán cho video. Các kỹ thuật tạo video dựa trên mô hình khuếch tán gần đây nắm bắt các đặc trưng thời gian thông qua các lớp attention thời gian, phát sinh chi phí bộ nhớ tỷ lệ với bình phương của độ dài chuỗi. Gần đây, SSM đã nổi lên như một lựa chọn thay thế đầy hứa hẹn cho attention, cung cấp chi phí bộ nhớ tuyến tính liên quan đến độ dài chuỗi [21,22,23,55,20]. Chúng tôi đầu tiên xem xét những tiến bộ gần đây trong SSM trong các công trình trước đó, sau đó là mô tả chi tiết về kiến trúc lớp SSM thời gian được đề xuất của chúng tôi cho các mô hình khuếch tán tạo video.

3.1. Mô hình Không gian Trạng thái
Không giống như attention thời gian thường được sử dụng trong các mô hình khuếch tán video, các mô hình không gian trạng thái (SSM) cho phép xử lý chuỗi thời gian với độ phức tạp không gian tỷ lệ với độ dài chuỗi. Các nghiên cứu gần đây đề xuất SSM có thể xử lý đầu vào song song, không giống như các mô hình hồi quy như mạng neural hồi quy (RNN) [5].

SSM được sử dụng rộng rãi như các mô hình chuỗi định nghĩa một ánh xạ từ tín hiệu đầu vào một chiều u(t) ∈ R đến tín hiệu đầu ra một chiều y(t) ∈ R, với s(t) ∈ RN biểu diễn trạng thái ẩn. Quá trình thời gian liên tục được công thức hóa như sau:

ṡ(t) = As(t) + Bu(t),
y(t) = Cs(t) + Du(t), (8)

trong đó A ∈ RN×N, B ∈ RN×1, C ∈ R1×N biểu thị ma trận trạng thái đường chéo, ma trận đầu vào, và ma trận đầu ra cho trạng thái ẩn tương ứng. D ∈ R là đường trực tiếp từ đầu vào đến đầu ra.

Để áp dụng SSM với dữ liệu thế giới thực, SSM được công thức hóa như Phương trình 8 được chuyển đổi thành các phiên bản rời rạc sử dụng phương pháp zero-order hold (ZOH) [23]:

sk = Āsk−1 + B̄uk,
yk = Csk + Duk, (9)

trong đó Ā = exp(ΔA), B̄ = (ΔA)^(-1)(exp(A)−I)·ΔB với Δ là tham số thang thời gian cho ZOH.

Khác biệt với các SSM trước đó bị giới hạn trong các hệ thống tuyến tính bất biến thời gian (LTI), Mamba [20] giới thiệu một cơ chế quét chọn lọc (S6). Trong S6, các tham số B ∈ RB×L×N, C ∈ RB×L×N, Δ ∈ RB×L×D phụ thuộc vào đầu vào u ∈ RB×L×D. Điều này cho phép mô hình chọn lọc loại bỏ thông tin không liên quan trong khi giữ lại thông tin thiết yếu trong thời gian dài. Sử dụng một thuật toán quét hiệu quả, S6 có khả năng tính toán song song với độ phức tạp tuyến tính tương ứng với độ dài chuỗi.

3.2. Lớp SSM Thời gian cho Tạo Video dựa trên Mô hình Khuếch tán
Chúng tôi kết hợp các mô hình không gian trạng thái (SSM) trong các lớp thời gian cho mô hình khuếch tán tạo video (Hình 1 (c)). Trong mô hình của chúng tôi, chúng tôi thay thế thành phần self-attention bằng một SSM và áp dụng cấu trúc SSM hai chiều, dựa trên các thực hành trong [18,65]. Sự lựa chọn này được thúc đẩy bởi hạn chế vốn có của một SSM đơn, thường bị hạn chế trong việc nắm bắt các chuyển tiếp thời gian một chiều. Trong các mô hình nắm bắt các phụ thuộc không gian trong ảnh sử dụng SSM, việc kết hợp SSM quan sát các hướng khác nhau có thể tăng cường chất lượng tạo sinh [67,72,41]. Tương tự, chúng tôi thấy rằng bằng cách áp dụng phương pháp hai chiều, SSM có thể hiểu toàn diện hơn các động lực thời gian trong dữ liệu video.

Chúng tôi cụ thể chọn áp dụng kiến trúc Mamba hai chiều như được đề xuất bởi [72] (Hình 2). Theo thiết lập tiêu chuẩn được sử dụng trong các công trình trước đó về Mamba [20,72,41], chúng tôi đặt hệ số mở rộng của các chiều đầu vào qua phép chiếu tuyến tính thành E = 2, chiều trạng thái nội bộ của SSM thành N = 16, và áp dụng SiLU [11] làm hàm kích hoạt. Đầu vào được ký hiệu là X ∈ R(B×H×W)×L×C, trong đó L là độ dài chuỗi, C là kích thước kênh, H là chiều cao, và W là chiều rộng của ảnh đầu vào.

(a) Mamba (b) Mamba Hai Chiều
σ σ ×
σ Thuận
Conv1d Thuận
SSM σ ×Nghịch
Conv1d σ Nghịch
SSM
+ ×
Thuận
Conv1d Thuận
SSM

Hình 2: So sánh kiến trúc của Mamba và Mamba hai chiều. Chuẩn hóa Lớp [1] và kết nối bỏ qua [26] được bỏ qua để đơn giản.

4. Công trình Liên quan
4.1. Mô hình Tạo sinh Sâu cho Tạo Video
Lĩnh vực tổng hợp video đã thấy những tiến bộ đáng kể qua nhiều nghiên cứu khác nhau. Trước sự xuất hiện của các mô hình khuếch tán, việc sử dụng mạng đối nghịch tạo sinh (GAN) [17] đã thống trị khung cảnh. Các phương pháp này mở rộng các khung image-GAN truyền thống để tạo video, tập trung vào việc tăng cường khả năng tạo sinh của chúng [63,50,59,15]. Các phương pháp này chủ yếu nhằm đạt được mục tiêu của họ bằng cách mở rộng các kiến trúc phổ biến của GAN cho tạo ảnh. Ngoài ra, việc phát triển các kỹ thuật tạo video dài hạn, đặc biệt là những kỹ thuật tận dụng các chuyển tiếp của các biến tiềm ẩn trong bộ mã hóa tự động biến phân (VAE) [36] cũng được biết đến [35, 19, 52, 68].

Sự ra đời của các mô hình khuếch tán trong tạo ảnh [56,46,29] đánh dấu một bước ngoặt, với việc áp dụng tiếp theo của chúng vào phân phối video thể hiện kết quả đầy hứa hẹn [31,54,30,25,3]. Các phương pháp này đã cho thấy kết quả đầy hứa hẹn. Tuy nhiên, các phương pháp gần đây áp dụng các cơ chế attention cho các lớp thời gian, yêu cầu bộ nhớ tỷ lệ với bình phương của độ dài chuỗi, nhu cầu tính toán và bộ nhớ của các mô hình video-khuếch tán đặt ra một thách thức đáng kể. Để giảm thiểu điều này, downsampling spatio-temporal [54,30], sử dụng các đặc trưng tiềm ẩn [48,27,70,3] được đề xuất. Trong lĩnh vực dự đoán video dài hạn sử dụng các mô hình khuếch tán video, các kỹ thuật tạo sinh đa bước nổi bật [62,32,25,69]. Các sơ đồ này tạo video thông qua lấy mẫu liên tiếp với điều kiện khung hình linh hoạt, cho phép mô hình hóa phụ thuộc dài hạn hiệu quả với việc sử dụng bộ nhớ tối thiểu. Nghiên cứu của chúng tôi khác biệt với họ bằng cách tập trung vào các cải tiến kiến trúc thay vì các sơ đồ lấy mẫu.

4.2. SSM và Ứng dụng của chúng
Mamba, một SSM gần đây được giới thiệu bởi Gu và Dao [20], cung cấp tính toán hiệu quả và thể hiện hiệu suất xuất sắc, trong HiPPO [21] và S4 [22] đã đặt nền móng cho những tiến bộ tiếp theo trong các khung mô hình hóa chuỗi, bao gồm việc phát triển Mamba. Bằng cách giới thiệu một tham số hóa có cấu trúc của SSM, S4 cho phép tính toán hiệu quả và thể hiện hiệu suất đặc biệt trong việc nắm bắt các phụ thuộc tầm xa [58]. S4D [23] là một phiên bản đơn giản hóa của S4 đã giới thiệu một công thức ma trận đường chéo. SSM đã được áp dụng trong nhiều lĩnh vực khác nhau, bao gồm phân loại ảnh và video [45,39,34,64,72,41,40], phân đoạn [43], tạo giọng nói [16], tạo chuỗi thời gian [71], mô hình hóa ngôn ngữ [44,65,6], học tăng cường [2, 42, 7].

Trong lĩnh vực các mô hình khuếch tán sử dụng SSM, DiffuSSM [67] đã đầu tiên khám phá việc tích hợp SSM với các mô hình khuếch tán, thay thế các cơ chế attention không gian tính toán nhiều trong tạo ảnh bằng SSM. Các nghiên cứu khác cũng đã khám phá việc thay thế attention không gian bằng SSM cho tạo ảnh, cũng như xử lý thông tin spatio-temporal toàn cục cho tạo video ngắn hạn (ví dụ, 16 khung hình) [33,14,13]. Ngược lại, nghiên cứu của chúng tôi tự phân biệt bằng cách thay thế attention thời gian trong các mô hình tạo video bằng Mamba, nhằm tạo video dài hạn hơn (ví dụ, 256 khung hình).

5. Thí nghiệm
Trong phần này, chúng tôi đã tiến hành một loạt thí nghiệm chứng minh rằng việc kết hợp SSM vào các mô hình khuếch tán cho tạo video hiệu quả nắm bắt các phụ thuộc thời gian, đặc biệt trong việc tạo video dài hạn (ví dụ, 256 khung hình), trong khi duy trì hiệu quả tính toán. Các thí nghiệm của chúng tôi trên nhiều bộ dữ liệu và các thiết lập khác nhau cho thấy rằng, dưới việc sử dụng bộ nhớ tương đương, SSM thời gian vượt trội trong việc tạo video dài hạn. Những phát hiện này cho thấy rằng việc sử dụng SSM trong các lớp thời gian có lợi cho việc xây dựng các mô hình khuếch tán video có khả năng tạo video dài hạn dưới tài nguyên tính toán hạn chế.

Bảng 1: Cấu hình Mô hình. Các thiết lập cho các mô hình khuếch tán dựa trên UNet được sử dụng trong các thí nghiệm. Các mô hình được mở rộng sao cho kích thước kênh cơ sở tỷ lệ với số đầu attention, trong khi chiều ẩn attention được cố định ở 64.

Mô hình dựa trên Attention
Tham số # Kênh cơ sở # Đầu Attention # Chiều Attention Không gian Thời gian
14.1M 32 4 64 Linear attn. Attention
56.3M 64 8 64 Linear attn. Attention
71.2M 72 9 64 Linear attn. Attention
225M 128 16 64 Linear attn. Attention
900M 256 32 64 Linear attn. Attention

Mô hình dựa trên SSM
14.5M 32 4 64 Linear attn. SSM
57.5M 64 8 64 Linear attn. SSM
175M 112 14 64 Linear attn. SSM
229M 128 16 64 Linear attn. SSM
913M 256 32 64 Linear attn. SSM

5.1. Thiết lập Thí nghiệm
Mô hình Đối với mỗi thiết lập thí nghiệm, chúng tôi đã tiến hành thí nghiệm sử dụng cả VDM với các lớp attention thời gian và VDM với các lớp thời gian dựa trên SSM, trên nhiều kích thước mô hình. Số lượng tham số và phương pháp mở rộng cho mỗi mô hình được chi tiết trong Bảng 1. Kích thước mô hình để tạo video dài 256 khung hình được chọn dựa trên phương pháp mở rộng được sử dụng trong nghiên cứu này, đảm bảo rằng các thí nghiệm khả thi trong ràng buộc của phần cứng có sẵn, bao gồm tám GPU NVIDIA A100 (40 GB mỗi cái). Các VDM được triển khai dựa trên một triển khai công khai và được sử dụng rộng rãi¹ sử dụng lớp tích chập của UNet và linear attention [53,38,66] để nắm bắt các đặc trưng không gian. Trong bài báo này, chúng tôi tập trung vào việc chỉ so sánh các lớp chịu trách nhiệm nắm bắt các đặc trưng thời gian, vì những thay đổi trong việc trích xuất đặc trưng không gian không liên quan đến điều tra của chúng tôi.

Bộ dữ liệu Chúng tôi đầu tiên sử dụng bộ dữ liệu MineRL Navigate [24,52], một bộ dữ liệu thường được sử dụng cho tạo video dài hạn, được lấy từ môi trường Minecraft. Tập huấn luyện bao gồm tổng cộng 1,186 video, kết hợp cả phần tách train và test. Chúng tôi đã huấn luyện các mô hình của chúng tôi sử dụng video với độ dài khung hình khác nhau là 16, 64 và 256 khung hình để đánh giá chi phí tính toán và hiệu suất tạo sinh khi sử dụng SSM so với attention như lớp thời gian dưới các độ dài khung hình khác nhau. Ngoài ra, chúng tôi đã sử dụng các bộ dữ liệu GQN-Mazes [12,52] và CARLA-Town01 [9,25] cho các tác vụ tạo video dài hạn. Bộ dữ liệu GQN-Mazes chứa 108,200 video từ một môi trường mô phỏng mê cung 3D, trong khi bộ dữ liệu CARLA Town01 bao gồm 508 video từ một môi trường mô phỏng lái xe. Đối với các bộ dữ liệu này, chúng tôi đã huấn luyện các mô hình của chúng tôi sử dụng video với 256 khung hình. Trong tất cả các thí nghiệm, độ phân giải không gian của các video được cố định ở 32x32 pixel. Chi tiết huấn luyện cho mỗi bộ dữ liệu và mỗi độ dài khung hình được hiển thị trong Phụ lục A.

Đường cơ sở Chúng tôi thiết lập đường cơ sở thí nghiệm của chúng tôi sử dụng VDM [31]. Phân tích của chúng tôi được thiết kế tỉ mỉ để chỉ thay đổi các lớp attention thời gian trong VDM bằng các lớp SSM thời gian của chúng tôi. Chiến lược này cho phép kiểm tra tập trung về tác động và hiệu quả của các lớp SSM thời gian trong bối cảnh tạo video, tạo điều kiện so sánh trực tiếp với các lớp thời gian hiện có.

Số liệu Đánh giá Trong quá trình xác thực của chúng tôi, chúng tôi đánh giá chất lượng mẫu của các video được tạo bởi các mô hình đã được huấn luyện. Để đánh giá chất lượng của các video được tạo, chúng tôi sử dụng số liệu Khoảng cách Video Fréchet (FVD) [60], sử dụng một mạng I3D được huấn luyện trước trên bộ dữ liệu Kinetics-400 [4]. FVD là một tiêu chuẩn được công nhận để đánh giá chất lượng của các video được tạo [31,15,54,30,25], trong đó điểm số thấp hơn biểu thị chất lượng cao hơn. Đối với bộ dữ liệu MineRL Navigate, các tính toán liên quan đến tất cả 1,186 video và 1,000 mẫu được tạo. Đối với bộ dữ liệu GQN-Mazes, các tính toán liên quan đến 2,000 video từ bộ dữ liệu và 2,000 mẫu được tạo, trong khi đối với bộ dữ liệu CARLA Town01, tất cả 508 video từ bộ dữ liệu và 500 mẫu được tạo được sử dụng. Chúng tôi đã sử dụng ba hạt giống ngẫu nhiên khác nhau để đưa vào sự biến đổi trong lấy mẫu dữ liệu và tạo mẫu cho tính toán FVD.

Hình 3: So sánh việc sử dụng bộ nhớ GPU và FVD giữa lớp SSM thời gian và attention thời gian trên bộ dữ liệu MineRL Navigate: 16 khung hình (trái), 64 khung hình (giữa), 256 khung hình (phải). Mỗi điểm biểu đồ bao gồm kích thước mô hình bên cạnh nó.

Hình 4: So sánh việc sử dụng bộ nhớ GPU và FVD giữa lớp SSM thời gian và attention thời gian cho 256 khung hình trên bộ dữ liệu GQN-Mazes (trái) và CARLA Town01 (phải). Mỗi điểm biểu đồ bao gồm kích thước mô hình bên cạnh nó.

5.2. Kết quả Chính
Chúng tôi đã huấn luyện các mô hình với nhiều kích thước khác nhau trong các thiết lập khác nhau, với kết quả được hiển thị trong Hình 3 và 4, và Bảng 2 và 3. Đầu tiên, chúng tôi mô tả các kết quả thu được từ bộ dữ liệu MineRL Navigate với các khung video khác nhau. Đối với các chuỗi 16 và 64 khung hình, VDM với lớp thời gian dựa trên SSM thể hiện hiệu suất tạo sinh có thể so sánh với lớp thời gian dựa trên attention. Tuy nhiên, khi số khung hình tăng lên 256 khung hình, VDM dựa trên SSM thể hiện hiệu suất tạo sinh vượt trội so với việc sử dụng bộ nhớ so với phương pháp dựa trên attention. Hơn nữa, các thí nghiệm được tiến hành trên các bộ dữ liệu GQN-Mazes và CARLA Town01 xác nhận những lợi thế của việc sử dụng lớp thời gian dựa trên SSM trong VDM so với các lớp dựa trên attention. Những lợi ích này, đặc biệt về hiệu suất tạo sinh so với việc sử dụng bộ nhớ, mở rộng ra ngoài bộ dữ liệu MineRL Navigate. Lợi thế này đặc biệt rõ ràng trong việc tạo các chuỗi video 256 khung hình trong nhiều tình huống khác nhau. Các mẫu được tạo từ VDM dựa trên SSM thời gian được hiển thị trong Hình 5.

Mẫu 1
1 16 32 64 80 96 112 128 144 160
Mẫu 2
Mẫu 1
Mẫu 2
Mẫu 1
Mẫu 2
176 192 208 224 240 48 256

Hình 5: Video được tạo bởi VDM dựa trên SSM thời gian. Mỗi cột biểu diễn các mẫu khác nhau, với hai hàng trên cùng tương ứng với MineRL Navigate, hai hàng giữa với GQN-Mazes, và hai hàng dưới với CARLA Town01 (số khung hình là 256, độ phân giải 32×32). Kết quả định tính bổ sung được cung cấp trong Hình B.7, B.8, B.9.

Trong Hình 6, chúng tôi trình bày cách tiêu thụ bộ nhớ huấn luyện và thời gian suy luận thay đổi với độ dài chuỗi video cho mỗi lớp thời gian, sử dụng ảnh độ phân giải 32×32 và cùng kích thước mô hình. Dữ liệu tiêu thụ bộ nhớ huấn luyện dựa trên kích thước lô 8, trong khi thời gian suy luận phản ánh việc tạo mẫu trên một GPU NVIDIA A100 đơn với số bước thời gian khuếch tán T cố định ở 256. Kết quả cho 256 khung hình đặc biệt đáng chú ý, chứng minh rằng trong khi việc sử dụng bộ nhớ và tốc độ suy luận trong các mô hình dựa trên Attention tăng theo bậc hai với độ dài chuỗi, mô hình dựa trên SSM chỉ thể hiện sự tăng tuyến tính.

Hình 6: Trái: Tiêu thụ bộ nhớ trong quá trình huấn luyện với 8 GPU NVIDIA A100 (40 GB) ở kích thước lô 8 và độ phân giải 32×32. Phải: Thời gian suy luận để tạo một mẫu với một GPU NVIDIA A100 đơn ở độ phân giải 32×32 và T = 256.

Kết quả của thí nghiệm này cho thấy rằng SSM có thể phục vụ như một lựa chọn thay thế khả thi cho các cơ chế attention trong các lớp thời gian, ngay cả đối với việc tạo các video tương đối ngắn, như những video có 16 hoặc 64 khung hình. Hơn nữa, trong việc tạo các video dài hơn, như những video có 256 khung hình, các mô hình dựa trên SSM có thể đạt được tạo video chất lượng cao hơn so với các mô hình dựa trên attention. Điều này là do SSM cho phép các kích thước mô hình lớn hơn trong khi duy trì tài nguyên tính toán có thể so sánh. Ngoài ra, xu hướng này được kỳ vọng sẽ trở nên rõ ràng hơn khi số khung hình video tăng, dựa trên mối quan hệ quan sát được giữa số khung hình và chi phí tính toán.

Bảng 2: Kết quả định lượng của các thí nghiệm trong MineRL Navigate. Tiêu thụ bộ nhớ trong quá trình huấn luyện với 8 GPU NVIDIA A100 (40 GB) ở kích thước lô 8 và độ phân giải 32×32. Thời gian suy luận để tạo một mẫu với một GPU NVIDIA A100 đơn ở độ phân giải 32×32 và T = 256.

16 khung hình
Mô hình Sử dụng Bộ nhớ Thời gian Suy luận FVD-16f ↓
Attention-56.3M 30.8 GB 6.11 giây 80.3
Attention-900M 101 GB 18.2 giây 45.6
SSM-57.5M 31.6 GB 5.49 giây 84.0
SSM-913M 106 GB 20.6 giây 42.4

64 khung hình
Mô hình Sử dụng Bộ nhớ Thời gian Suy luận FVD-64f ↓
Attention-14.1M 36.8 GB 8.19 giây 817
Attention-56.3M 59.8 GB 13.4 giây 449
Attention-225M 110 GB 29.5 giây 259
Attention-900M 224 GB 68.1 giây 205
SSM-14.5M 36.8 GB 5.76 giây 923
SSM-57.5M 59.2 GB 10.3 giây 510
SSM-229M 105 GB 23.3 giây 205
SSM-913M 219 GB 50.9 giây 208

256 khung hình
Mô hình Sử dụng Bộ nhớ Thời gian Suy luận FVD-256f ↓
Attention-14.1M 144 GB 29.6 giây 642
Attention-56.3M 276 GB 61.1 giây 383
Attention-71.2M 309 GB 70.8 giây 380
SSM-14.5M 96.1 GB 19.2 giây 687
SSM-57.5M 177 GB 39.3 giây 381
SSM-175M 298 GB 76.6 giây 272

5.3. Tính Hai Chiều của Các Lớp SSM Thời gian
Nghiên cứu khử trừ của chúng tôi tiết lộ rằng xử lý hai chiều trong SSM là quan trọng để tạo video chất lượng cao khi nắm bắt các đặc trưng thời gian trong các chuỗi video. Cụ thể, trong các thí nghiệm của chúng tôi với tạo video 256 khung hình trên bộ dữ liệu MineRL Navigate, việc sử dụng SSM một chiều dẫn đến sự suy giảm đáng kể trong chất lượng tạo sinh (Bảng 4).

Bảng 3: Kết quả định lượng của các thí nghiệm trong GQN-Mazes và CARLA Town01.

GQN-Mazes 256 khung hình
Mô hình Sử dụng Bộ nhớ Thời gian Suy luận FVD-256f ↓
Attention-14.1M 144 GB 29.6 giây 217
Attention-56.3M 276 GB 61.1 giây 138
Attention-71.2M 309 GB 70.8 giây 107
SSM-14.5M 96.1 GB 19.2 giây 267
SSM-57.5M 178 GB 39.3 giây 104
SSM-175M 297 GB 76.6 giây 57.8

CARLA Town01 256 khung hình
Mô hình Sử dụng Bộ nhớ Thời gian Suy luận FVD-256f ↓
Attention-14.1M 145 GB 29.6 giây 470
Attention-56.3M 276 GB 61.1 giây 241
Attention-71.2M 309 GB 70.8 giây 196
SSM-14.5M 96.1 GB 19.2 giây 557
SSM-57.5M 178 GB 39.3 giây 256
SSM-175M 297 GB 76.6 giây 206

Bảng 4: So sánh hiệu suất của SSM hai chiều và SSM một chiều.
Hai Chiều Tham số Bộ nhớ Thời gian Suy luận FVD-256f ↓
√ 57.5 M 177 GB 39.3 giây 381
56.9 M 146 GB 31.8 giây 610

Phát hiện này cho thấy rằng việc xử lý thời gian không theo quan hệ nhân quả là tốt hơn trong các mô hình tạo video có độ dài cố định. Được biết rằng trong các mô hình khuếch tán tạo ảnh sử dụng SSM, việc cải thiện hiệu suất đạt được bằng cách sử dụng SSM xử lý nhiều hướng trong miền không gian [67,33,13]. Hành vi này bắt nguồn từ thực tế rằng SSM về bản chất là các cơ chế xử lý thông tin theo quan hệ nhân quả. Kết quả khử trừ của chúng tôi chỉ ra rằng một phương pháp tương tự—tránh xử lý hoàn toàn theo quan hệ nhân quả—cũng có lợi cho miền thời gian trong tạo video.

6. Thảo luận
Các phát hiện thí nghiệm của chúng tôi chứng minh rằng việc kết hợp SSM vào các lớp thời gian của các mô hình khuếch tán cho tạo video cung cấp mô hình hóa video vượt trội về hiệu quả bộ nhớ để xử lý độ dài chuỗi tăng so với các mô hình truyền thống sử dụng attention thời gian, trong khi duy trì chất lượng tạo sinh cạnh tranh. Được chứng minh thực nghiệm rằng, so với các mô hình dựa trên attention, các mô hình khuếch tán video dựa trên SSM có thể được mở rộng đến kích thước lớn hơn dưới cùng chi phí tính toán, dẫn đến tạo video chất lượng cao hơn cho các chuỗi dài hơn. Những kết quả này nhấn mạnh khả năng thích ứng của các lớp SSM thời gian trong việc tăng cường các mô hình khuếch tán video và chỉ ra tác động tiềm năng rộng lớn của chúng đối với những tiến bộ tương lai trong lĩnh vực này.

Nghiên cứu này mở đường cho nhiều điều tra tương lai. Trong khi chúng tôi đã xác minh những lợi thế của việc sử dụng các lớp SSM thời gian của chúng tôi trong kiến trúc của VDM, việc áp dụng các lớp SSM thời gian của chúng tôi mở rộng ra ngoài điều này, đến bất kỳ mô hình nào sử dụng attention thời gian cho chiều thời gian. Ví dụ, việc áp dụng các lớp SSM thời gian vào các thành phần thời gian của các kiến trúc như Make-A-Video [54] hoặc Imagen Video [30] có thể tiếp tục tăng cường hiệu quả tính toán của chúng bằng cách tận dụng cơ chế downsampling spatiotemporal hiện có của chúng. Ngoài ra, sự kết hợp của các lớp SSM thời gian với Mô hình Khuếch tán Tiềm ẩn [48] có thể dẫn đến việc giảm đáng kể chi phí tính toán. Các phát hiện của chúng tôi cũng cung cấp thông tin cho các phương pháp liên quan đến việc đóng băng các mô hình khuếch tán tạo ảnh được huấn luyện trước và huấn luyện các lớp thời gian mới cho tạo video; việc đơn giản thay thế các lớp thời gian mới được chèn này bằng các lớp SSM có thể là một chiến lược hiệu quả. Hơn nữa, trong khi các thí nghiệm của chúng tôi tập trung vào tạo video vô điều kiện, việc tích hợp phương pháp của chúng tôi với các kỹ thuật tạo sinh có điều kiện như hướng dẫn phân loại [8] và hướng dẫn phân loại miễn phí [28] có thể mở đường cho các mô hình text-to-video (T2V) hiệu quả hơn.

Kết quả của nghiên cứu này chỉ ra rằng việc kết hợp SSM có thể dẫn đến việc phát triển các mô hình tạo video dài hạn đòi hỏi ít tài nguyên bộ nhớ hơn. Điều này có ý nghĩa đáng chú ý đối với việc mở rộng khả năng tiếp cận của nghiên cứu tiên tiến trong các mô hình khuếch tán tạo video. Ngay cả các tổ chức với tài nguyên tính toán hạn chế cũng có thể tham gia vào lĩnh vực tiên tiến này, có khả năng đẩy nhanh tốc độ nghiên cứu và đổi mới trong tạo video.

Tài liệu tham khảo
[1] J. L. Ba, J. R. Kiros, và G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

[2] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, và Lior Wolf. Decision s4: Efficient sequence-based rl via state spaces layers. Trong International Conference on Learning Representations, 2023.

[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, và Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. Trong CVPR, 2023.

[4] J. Carreira và A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. Trong CVPR, 2017.

[5] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, và Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

[6] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, và Christopher Ré. "hungry hungry hippos: Towards language modeling with state space models". Trong The International Conference on Learning Representations (ICLR), 2023.

[7] Fei Deng, Junyeong Park, và Sungjin Ahn. Facing off world model backbones: Rnns, transformers, and s4. arXiv:2307.02064, 2023.

[8] Prafulla Dhariwal và Alex Nichol. Diffusion models beat gans on image synthesis. arXiv preprint arXiv:2105.05233, 2021.

[9] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, và Vladlen Koltun. Carla: An open urban driving simulator. Trong Conference on Robot Learning, trang 1-16. PMLR, 2017.

[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, và cộng sự. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[11] Stefan Elfwing, Eiji Uchibe, và Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:3-11, 2018.

[12] SM Ali Eslami và cộng sự. Neural scene representation and rendering. Science, 360(6394):1204-1210, 2018.

[13] Yunxiang Fu, Chaoqi Chen, và Yizhou Yu. Lamamba-diff: Linear-time high-fidelity diffusion models based on local attention and mamba. arXiv preprint arXiv:2408.02615, 2024.

[14] Yu Gao và cộng sự. Matten: Video generation with mamba-attention. arXiv preprint arXiv:2405.03025, 2024.

[15] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, và Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. arXiv preprint arXiv:2204.03638, 2022.

[16] Karan Goel, Albert Gu, Chris Donahue, và Christopher Ré. It's raw! audio generation with state-space models. Trong International Conference on Machine Learning, 2022.

[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, và Yoshua Bengio. Generative adversarial nets. Trong Advances in Neural Information Processing Systems, trang 2672-2680, 2014.

[18] Alex Graves và Jürgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5-6):602-610, 2005.

[19] K. Gregor và F. Besse. Temporal difference variational auto-encoder. arXiv preprint arXiv:1806.03107, 2018.

[20] Albert Gu và Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.

[21] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, và Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33:1474-1487, 2020.

[22] Albert Gu, Karan Goel, và Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.

[23] Albert Gu, Karan Goel, Ankit Gupta, và Christopher Ré. On the parameterization and initialization of diagonal state space models. Trong Advances in Neural Information Processing Systems, tập 35, trang 35971-35983, 2022.

[24] William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, và Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019.

[25] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, và Frank Wood. Flexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022.

[26] Kaiming He và cộng sự. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.

[27] Yingqing He và cộng sự. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022.

[28] Jonathan Ho và Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.

[29] Jonathan Ho, Ajay Jain, và Pieter Abbeel. Denoising diffusion probabilistic models. Trong Advances in Neural Information Processing Systems, trang 6840-6851, 2020.

[30] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, và cộng sự. Imagen video: High definition video generation with diffusion models. arXiv:2210.02303, 2022.

[31] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, và David J. Fleet. Video diffusion models, 2022.

[32] Tobias Höppe và cộng sự. Diffusion models for video prediction and infilling. arXiv preprint arXiv:2206.07696, 2022.

[33] Vincent Tao Hu và cộng sự. Zigma: Zigzag mamba diffusion model. arXiv preprint arXiv:2403.13802, 2024.

[34] Md Mohaiminul Islam và Gedas Bertasius. Long movie clip classification with state-space video models. Trong ECCV, 2022.

[35] T. Kim, S. Ahn, và Y. Bengio. Variational temporal abstraction. Trong Advances in Neural Information Processing Systems, trang 11566-11575, 2019.

[36] Diederik P Kingma và Max Welling. Auto-encoding variational bayes. International Conference on Learning Representations, 2013.

[37] Diederik P Kingma, Tim Salimans, Ben Poole, và Jonathan Ho. Variational diffusion models. arXiv preprint arXiv:2107.00630, 2021.

[38] Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. Trong International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB.

[39] David M Knigge, David W Romero, Albert Gu, Efstratios Gavves, Erik J Bekkers, Jakub Mikolaj Tomczak, Mark Hoogendoorn, và Jan-jakob Sonke. Modelling long range dependencies in nd: From task-specific to a general purpose cnn. Trong International Conference on Learning Representations, 2023.

[40] Kunchang Li và cộng sự. Videomamba: State space model for efficient video understanding. arXiv preprint arXiv:2403.06977, 2024.

[41] Y. Liu, Y. Tian, Y. Zhao, H. Yu, L. Xie, Y. Wang, Q. Ye, và Y. Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024.

[42] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, và Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023.

[43] Jun Ma, Feifei Li, và Bo Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722, 2024.

[44] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, và Behnam Neyshabur. Long range language modeling via gated state spaces. Trong The Eleventh International Conference on Learning Representations, 2023.

[45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, và Christopher Ré. S4nd: Modeling images and videos as multidimensional signals with state spaces. Trong Advances in Neural Information Processing Systems, 2022.

[46] Alexander Quinn Nichol và Prafulla Dhariwal. Improved denoising diffusion probabilistic models. Trong International Conference on Machine Learning, trang 8162-8171. PMLR, 2021.

[47] William Peebles và Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. doi: 10.48550/arXiv.2212.09748.

[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 10684-10695, 2022.

[49] Olaf Ronneberger, Philipp Fischer, và Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. Trong International Conference on Medical Image Computing and Computer-Assisted Intervention, trang 234-241. Springer, 2015.

[50] M. Saito, E. Matsumoto, và S. Saito. Temporal generative adversarial nets with singular value clipping. Trong IEEE International Conference on Computer Vision (ICCV), 2017.

[51] Tim Salimans và Jonathan Ho. Progressive distillation for fast sampling of diffusion models. Trong International Conference on Learning Representations (ICLR), 2022.

[52] Vaibhav Saxena, Jimmy Ba, và Danijar Hafner. Clockwork variational autoencoders. Advances in Neural Information Processing Systems, 34, 2021.

[53] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, và Hongsheng Li. Efficient attention: Attention with linear complexities. CoRR, abs/1812.01243, 2018. URL http://arxiv.org/abs/1812.01243.

[54] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, và cộng sự. Make-a-video: Text-to-video generation without text-video data. arXiv:2209.14792, 2022.

[55] Jimmy T.H. Smith, Andrew Warrington, và Scott Linderman. Simplified state space layers for sequence modeling. Trong International Conference on Learning Representations, 2023.

[56] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, và Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. Trong International Conference on Machine Learning, trang 2256-2265, 2015.

[57] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, và Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2021. doi: 10.48550/arXiv.2011.13456.

[58] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient transformers. Trong International Conference on Learning Representations, 2021.

[59] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, và Jan Kautz. Mocogan: Decomposing motion and content for video generation. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 1526-1535, 2018.

[60] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, và Sylvain Gelly. Towards accurate generative models of video: A new metric challenges. arXiv preprint arXiv:1812.01717, 2018.

[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Advances in Neural Information Processing Systems, trang 5998-6008, 2017.

[62] Vikram Voleti, Alexia Jolicoeur-Martineau, và Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Trong Advances in neural information processing systems, tập 35, trang 23371-23385, 2022.

[63] C. Vondrick, H. Pirsiavash, và A. Torralba. Generating videos with scene dynamics. Trong Advances in Neural Information Processing Systems (NeurIPS), 2016.

[64] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, và Raffay Hamid. Selective structured state-spaces for long-form video understanding. Trong CVPR, 2023.

[65] Junxiong Wang, Jing Nathan Yan, Albert Gu, và Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022.

[66] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity, 2020.

[67] Jing Nathan Yan, Jiatao Gu, và Alexander M Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023.

[68] Wilson Yan và cộng sự. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.

[69] Shengming Yin và cộng sự. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023.

[70] Daquan Zhou và cộng sự. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.

[71] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, và Stefano Ermon. Deep latent state space models for time-series generation. Trong International Conference on Machine Learning, 2023.

[72] Lianghui Zhu và cộng sự. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.

Phụ lục A. Thiết lập Thí nghiệm
Để đảm bảo so sánh công bằng các mô-đun trích xuất mối quan hệ thời gian dưới cùng thiết lập độ phân giải, tất cả các cấu hình ngoại trừ các lớp thời gian, đều giống nhau trong các thí nghiệm của chúng tôi. Chúng tôi đã sử dụng NVIDIA A100 ×8 (từ một nhà cung cấp đám mây). Các siêu tham số, nhất quán trên tất cả các kích thước mô hình và bộ dữ liệu, được hiển thị trong Bảng A.5.

Phụ lục B. Kết quả Định tính Bổ sung
Được hiển thị trong Hình B.7, B.8, B.9.

Bảng A.5: Siêu tham số được sử dụng trong các thí nghiệm.
Tham số Giá trị
Bội số kênh UNet (1, 2, 4, 8)
Chiều nhúng thời gian 1024
Tuyến tính nhúng thời gian 2
Bước thời gian khử nhiễu (T) 256
Loại mất mát Mất mát L2 của nhiễu ε
Bước huấn luyện 100k
Bộ tối ưu Adam (β1 = 0.9, β2 = 0.999)
Tốc độ học 0.00001
Kích thước lô huấn luyện 8
Suy giảm EMA 0.9999

Mẫu 1
1 16 32 64 80 96 112 128 144 160 Mẫu 2
Mẫu 1
Mẫu 2
Mẫu 1
Mẫu 2
176 192 208 224 240 48 256

Hình B.7: Kết quả tạo sinh định tính bổ sung trong MineRL Navigate (số khung hình là 256, độ phân giải 32×32). Mỗi cột biểu diễn các mẫu khác nhau.

Mẫu 1
1 16 32 64 80 96 112 128 144 160 Mẫu 2
Mẫu 1
Mẫu 2
Mẫu 1
Mẫu 2
176 192 208 224 240 48 256

Hình B.8: Kết quả tạo sinh định tính bổ sung trong GQN-Mazes (số khung hình là 256, độ phân giải 32×32). Mỗi cột biểu diễn các mẫu khác nhau.

Mẫu 1
1 16 32 64 80 96 112 128 144 160 Mẫu 2
Mẫu 1
Mẫu 2
Mẫu 1
Mẫu 2
176 192 208 224 240 48 256

Hình B.9: Kết quả tạo sinh định tính bổ sung trong CARLA Town01 (số khung hình là 256, độ phân giải 32×32). Mỗi cột biểu diễn các mẫu khác nhau.
