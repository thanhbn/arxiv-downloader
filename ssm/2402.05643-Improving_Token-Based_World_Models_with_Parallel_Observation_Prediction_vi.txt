# 2402.05643.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2402.05643.pdf
# Kích thước tệp: 1727730 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cải thiện Mô hình Thế giới Dựa trên Token với Dự đoán Quan sát Song song
Lior Cohen1Kaixin Wang1Bingyi Kang2Shie Mannor1
Tóm tắt
Được thúc đẩy bởi thành công của Transformer khi
áp dụng cho các chuỗi ký hiệu rời rạc, các mô hình thế giới
dựa trên token (TBWMs) gần đây được đề xuất
như những phương pháp hiệu quả về mẫu. Trong TBWMs,
mô hình thế giới tiêu thụ kinh nghiệm của agent như
một chuỗi token giống ngôn ngữ, trong đó mỗi
quan sát tạo thành một chuỗi con. Tuy nhiên,
trong quá trình tưởng tượng, việc tạo ra token-từng-token
tuần tự của các quan sát tiếp theo dẫn đến một
nút thắt nghiêm trọng, dẫn đến thời gian huấn luyện dài,
việc sử dụng GPU kém và các biểu diễn hạn chế. Để
giải quyết nút thắt này, chúng tôi đưa ra một cơ chế
Dự đoán Quan sát Song song (POP) mới. POP bổ sung
một Mạng Lưu giữ (RetNet) với một chế độ forward
mới được điều chỉnh cho bối cảnh học tăng cường
của chúng tôi. Chúng tôi kết hợp POP trong
một agent TBWM mới có tên REM (Retentive
Environment Model), thể hiện tốc độ tưởng tượng
nhanh hơn 15.4 lần so với các TBWM trước đó. REM
đạt được hiệu suất siêu nhân trên 12 trong số 26
trò chơi của bộ benchmark Atari 100K, trong khi
huấn luyện trong ít hơn 12 giờ. Mã nguồn của chúng tôi
có sẵn tại https://github.com/leor-c/REM .

1. Giới thiệu
Hiệu quả mẫu vẫn là một thách thức trung tâm trong học
tăng cường (RL) do nhu cầu dữ liệu đáng kể của
các thuật toán RL thành công (Berner et al., 2019; Mnih et al.,
2015; Schrittwieser et al., 2020; Silver et al., 2016; Vinyals
et al., 2019). Một phương pháp dựa trên mô hình nổi bật để
giải quyết thách thức này được gọi là mô hình thế giới. Trong
mô hình thế giới, việc học của agent hoàn toàn dựa trên
dữ liệu tương tác mô phỏng được tạo ra bởi một mô hình
môi trường đã học thông qua một quá trình gọi là tưởng tượng.
Mô hình thế giới đang ngày càng phổ biến do hiệu quả của chúng,
đặc biệt trong các môi trường thị giác (Hafner et al., 2023).

1Technion - Viện Công nghệ Israel2ByteDance. Liên hệ
với: Lior Cohen <liorcohen5@campus.technion.ac.il >.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy,
Vienna, Áo. PMLR 235, 2024. Bản quyền 2024 thuộc về
(các) tác giả.

Hình 1. Trên: so sánh giữa thời gian chạy của các agent
mô hình thế giới dựa trên token (IRIS và REM) trong quá trình
huấn luyện mô hình thế giới và tưởng tượng (huấn luyện actor-critic).
Dưới: so sánh điểm số chuẩn hóa con người trung bình interquantile
(IQM) giữa REM và các baseline tiên tiến trên benchmark
Atari 100K với khoảng tin cậy bootstrap phân tầng 95%
(Agarwal et al., 2021). Một đường phân tách các phương pháp
dựa trên token khỏi các baseline khác.

Trong những năm gần đây, các mô hình chuỗi dựa trên attention,
đáng chú ý nhất là kiến trúc Transformer (Vaswani et al., 2017),
đã thể hiện hiệu suất vô song trong các tác vụ mô hình hóa
ngôn ngữ (Brown et al., 2020; Bubeck et al., 2023; Devlin
et al., 2019; Touvron et al., 2023). Thành công đáng chú ý
của các mô hình này khi áp dụng cho các chuỗi token rời rạc
đã tạo động lực để sử dụng các kiến trúc này cho
các phương thức dữ liệu khác bằng cách học các biểu diễn
dựa trên token phù hợp. Trong thị giác máy tính, các biểu diễn
rời rạc đang trở thành một phương pháp chính thống cho nhiều
tác vụ (Dosovitskiy et al., 2021; Esser et al., 2021; Li et al., 2023;
van den Oord et al., 2017). Trong RL, các mô hình thế giới
dựa trên token gần đây được khám phá trong các môi trường
thị giác (Micheli et al., 2023). Mô-đun nhận thức thị giác
trong các phương pháp này được gọi là tokenizer, vì nó ánh xạ
các quan sát hình ảnh thành các chuỗi ký hiệu rời rạc. Theo cách này,
tương tác của agent được chuyển đổi thành một chuỗi token
rời rạc giống ngôn ngữ, được xử lý riêng lẻ bởi mô hình thế giới.

--- TRANG 2 ---
Cải thiện Mô hình Thế giới Dựa trên Token với Dự đoán Quan sát Song song

Hình 2. Tổng quan về chu kỳ huấn luyện của REM. Mỗi epoch có 4 bước: thu thập kinh nghiệm (1), huấn luyện tokenizer (2), huấn luyện mô hình thế giới (3), và huấn luyện controller trong tưởng tượng (4). Màu cam đại diện cho (các) thành phần được huấn luyện. Các ô vuông xanh biểu thị đầu vào token, trong đó xanh nhạt được dùng cho token quan sát và xanh đậm cho hành động. Dữ liệu replay buffer tại bước 3 và 4 chứa quan sát, hành động, phần thưởng và tín hiệu kết thúc.

Trong quá trình tưởng tượng, để tạo ra các token của quan sát tiếp theo với mô hình tự hồi quy, việc dự đoán được thực hiện tuần tự token-từng-token. Thực tế, phép tính có tính tuần tự cao này dẫn đến một nút thắt nghiêm trọng làm cản trở rõ rệt các phương pháp dựa trên token. Do đó, nút thắt này thực tế giới hạn độ dài của các chuỗi token quan sát điều này làm giảm hiệu suất. Hạn chế này khiến các phương pháp dựa trên token hiện tại trở nên không thực tế cho các vấn đề phức tạp.

Trong bài báo này, chúng tôi trình bày Dự đoán Quan sát Song song (POP), một cơ chế mới giải quyết nút thắt tưởng tượng của các mô hình thế giới dựa trên token (TBWMs). Với POP, toàn bộ chuỗi token quan sát tiếp theo được tạo ra song song trong quá trình tưởng tượng mô hình thế giới. Về cốt lõi, POP bổ sung một mô hình chuỗi Mạng Lưu giữ (RetNet) (Sun et al., 2023) với một chế độ forward mới được thiết kế để duy trì hiệu quả huấn luyện mô hình thế giới. Ngoài ra, chúng tôi trình bày REM (Retentive Environment Model), một agent TBWM được điều khiển bởi kiến trúc RetNet được bổ sung POP.

Các đóng góp chính của chúng tôi được tóm tắt như sau:
• Chúng tôi đề xuất Dự đoán Quan sát Song song (POP), một cơ chế mới giải quyết nút thắt suy luận của các mô hình thế giới dựa trên token hiện tại trong khi duy trì hiệu suất.
• Chúng tôi giới thiệu REM, phương pháp mô hình thế giới đầu tiên kết hợp kiến trúc RetNet. Các thí nghiệm của chúng tôi cung cấp bằng chứng đầu tiên về hiệu suất của RetNet trong bối cảnh RL.
• Chúng tôi đánh giá REM trên benchmark Atari 100K, thể hiện hiệu quả của POP. POP dẫn đến tăng tốc 15.4 lần trong tưởng tượng và huấn luyện trong dưới 12 giờ, trong khi vượt trội so với các TBWM trước đó.

2. Phương pháp
Ký hiệu. Chúng tôi xem xét bối cảnh Quá trình Quyết định Markov Quan sát Một phần (POMDP) với các quan sát hình ảnh ot trong Ω ⊆ R^(h×w×3), các hành động rời rạc at trong A, phần thưởng vô hướng rt trong R, tín hiệu kết thúc episode dt trong {0,1}, động lực học ot+1, rt, dt ∼ p(ot+1, rt, dt|o≤t, a≤t), và hệ số chiết khấu gamma. Mục tiêu là học một chính sách pi sao cho với mọi tình huống, đầu ra pi(at|o≤t, a<t) là tối ưu đối với tổng phần thưởng chiết khấu kỳ vọng từ tình huống đó E[∑∞τ=0 gamma^τ Rt+τ] dưới chính sách pi.

2.1. Tổng quan
REM được xây dựng trên IRIS (Micheli et al., 2023), và tương tự như hầu hết các công trình trước đó về mô hình thế giới cho đầu vào pixel (Hafner et al., 2021; 2023; Kaiser et al., 2020), REM tuân theo cấu trúc V-M-C (Ha & Schmidhuber, 2018): một mô-đun nhận thức Thị giác nén các quan sát thành các biểu diễn latent nhỏ gọn, một Mô hình dự đoán nắm bắt động lực học của môi trường, và một Controller học cách hành động để tối đa hóa lợi ích. Ngoài ra, một replay buffer được sử dụng để lưu trữ dữ liệu tương tác môi trường. Tổng quan về chu kỳ huấn luyện của REM được trình bày trong Hình 2. Một thuật toán giả mã của REM được trình bày trong Phụ lục A.2.

V- Tokenizer Chúng tôi khởi tạo thành phần nhận thức thị giác như một tokenizer, ánh xạ các quan sát đầu vào thành các token latent. Theo (Micheli et al., 2023), tokenizer là một bộ auto-encoder rời rạc VQ-VAE (Esser et al., 2021; van den Oord et al., 2017) bao gồm một encoder, một decoder và một bảng embedding. Bảng embedding E = {ei}^N_i=1 trong R^(N×d) bao gồm N vector có thể huấn luyện. Encoder trước tiên ánh xạ một hình ảnh đầu vào ot thành một chuỗi d vector latent chiều (h¹t, h²t, ..., h^K_t). Sau đó, mỗi vector latent h^k_t trong R^d được ánh xạ tới chỉ số của embedding gần nhất trong E, z^k_t = arg min_i ||h^k_t - ei||. Các chỉ số như vậy được gọi là token. Đối với một hình ảnh đầu vào ot, chuỗi token latent của nó được ký hiệu là zt = (z¹t, z²t, ..., z^K_t). Để ánh xạ một chuỗi token trở lại không gian đầu vào, trước tiên chúng ta lấy embedding cho mỗi token và thu được một chuỗi (ĥ¹t, ĥ²t, ..., ĥ^K_t) trong đó ĥ^k_t = e_z^k_t. Sau đó, ngược với quá trình mã hóa, decoder chịu trách nhiệm ánh xạ chuỗi này thành một quan sát được tái tạo ôt.

Tokenizer được huấn luyện trên các khung hình được lấy mẫu đồng đều từ replay buffer. Mục tiêu tối ưu hóa, kiến trúc và các chi tiết khác được trì hoãn đến Phụ lục A.1.1.

M- Mô hình Thế giới Ở cốt lõi của mô hình thế giới là thành phần nắm bắt động lực học của môi trường và đưa ra dự đoán dựa trên các quan sát lịch sử. Ở đây, M được học hoàn toàn trong không gian token latent, mô hình hóa các phân phối sau tại mỗi bước t:

Chuyển đổi: p(ẑt+1|z1, a1, ..., zt, at), (1)
Phần thưởng: p(r̂t|z1, a1, ..., zt, at), (2)
Kết thúc: p(d̂t|z1, a1, ..., zt, at). (3)

Để ánh xạ các token quan sát thành các vector embedding, M sử dụng các vector mã E được học bởi tokenizer V. Lưu ý rằng E không được cập nhật bởi M. Ngoài ra, M duy trì các bảng embedding chuyên dụng để ánh xạ các hành động và token đặc biệt (chi tiết trong Mục 2.3) thành các vector liên tục.

C- Controller Controller actor-critic C của REM được huấn luyện để tối đa hóa lợi ích hoàn toàn trong tưởng tượng (Hafner et al., 2021; Kaiser et al., 2020; Micheli et al., 2023). C bao gồm một mạng chính sách pi và một estimator hàm giá trị V^pi, và hoạt động trên các token latent và embedding của chúng. Trong mỗi bước tối ưu hóa, M và C được khởi tạo với một đoạn trajectory ngắn được lấy mẫu từ replay buffer. Tiếp theo, agent tương tác với mô hình thế giới trong H bước. Tại mỗi bước t, agent thực hiện một hành động được lấy mẫu từ chính sách pi(at|z1, a1, ..., zt-1, at-1, zt). Mô hình thế giới phát triển tương ứng, tạo ra r̂t, d̂t và ẑt+1 bằng cách lấy mẫu từ các phân phối thích hợp (Phương trình 1-3). Các trajectory kết quả sau đó được sử dụng để huấn luyện agent.

Theo (Micheli et al., 2023), chúng tôi áp dụng các mục tiêu actor-critic của DreamerV2 (Hafner et al., 2021). Chúng tôi để lại chi tiết đầy đủ về kiến trúc và tối ưu hóa trong Phụ lục A.1.3.

2.2. Preliminaries về Retention
Tương tự như Transformer (Vaswani et al., 2017), một mô hình RetNet (Sun et al., 2023) bao gồm một chồng các lớp, trong đó mỗi lớp chứa một cơ chế giống Attention đa đầu, được gọi là Retention, theo sau bởi một mạng kết nối đầy đủ. Một đặc điểm độc đáo của cơ chế Retention là nó có dạng kép của lặp lại và song song, được gọi là "chunkwise", để cải thiện hiệu quả khi xử lý các chuỗi dài.

Hình 3. Chế độ tính toán "chunkwise". Các chuỗi dài có thể được chia thành các "chunk" nhỏ hơn để tăng hiệu quả huấn luyện. Các chunk trước đó được tóm tắt bởi trạng thái lặp lại S. Các ô vuông xanh đại diện cho token, trong khi các vòng tròn biểu thị vector đầu ra. Quan trọng, chế độ chunkwise của RetNet không hỗ trợ tự nhiên cả việc tạo token theo batch trong tưởng tượng và huấn luyện mô hình thế giới hiệu quả. Điều này được đạt được bởi phần mở rộng POP của chúng tôi.

Hình thức, xem xét một chuỗi token (x1, x2, ..., xm). Trong bối cảnh RL của chúng tôi, chuỗi này là một trajectory token được tạo thành từ các chuỗi con quan sát-hành động (z¹t, ..., z^K_t, at) mà chúng tôi gọi là block. Vì các trajectory như vậy thường dài, chúng tôi chia chúng thành các chunk gồm B token, trong đó B = c(K+1) là bội số của K+1 để mỗi chunk chỉ chứa các block hoàn chỉnh. Ở đây, tham số siêu c có thể được điều chỉnh theo kích thước của các mô hình, phần cứng và các yếu tố khác để tối đa hóa hiệu quả. Đặt X = (x1, x2, ..., xm) trong R^(m×d) là các vector embedding token d chiều. Đầu ra Retention Y[i] = Retention(X[i], S[i-1], i) của chunk thứ i được cho bởi

Y[i] = Q[i]K^T[i] ⊙ D / V[i] + (Q[i]S[i-1]) ⊙ xi, (4)

trong đó chỉ số dưới có dấu ngoặc [i] được dùng để đánh chỉ số chunk thứ i, Q = (XW^Q) ⊙ Θ, K = (XW^K) ⊙ Θ̄, V = XW^V, và xi trong R^(B×d) là một ma trận với xi_ij = e^(tau*i+1). Ở đây, W^Q, W^K, W^V trong R^(d×d) là các trọng số có thể học, e^tau là một hệ số phân rã mũ, ma trận D trong R^(B×B) kết hợp một mặt nạ tự hồi quy với hệ số phân rã thời gian eta, và các ma trận Θ, Θ̄ trong C^(m×d) dành cho embedding vị trí tương đối (xem Phụ lục A.3). Lưu ý tham số chỉ số chunk i của toán tử Retention, điều khiển thông tin embedding vị trí thông qua Θ. Quy tắc cập nhật chunkwise của trạng thái lặp lại được cho bởi

S[i] = (K[i] ⊙ zeta)^T V[i] + eta^B S[i-1] (5)

trong đó S[0] = S0 = 0, và zeta trong R^(B×d) là một ma trận với zeta_ij = eta^(B-i-1). Ở phía bên phải của Phương trình 4 và 5, số hạng đầu tiên tương ứng với phép tính trong chunk trong khi số hạng thứ hai kết hợp thông tin từ các chunk trước đó, được đóng gói bởi trạng thái lặp lại. Chi tiết thêm về kiến trúc RetNet được trì hoãn đến Phụ lục A.3.

2.3. Tưởng tượng Mô hình Thế giới
Vì việc huấn luyện của agent hoàn toàn dựa vào tưởng tượng mô hình thế giới, hiệu quả của việc tạo trajectory là quan trọng. Trong quá trình tưởng tượng, dự đoán ẑt+1 tạo thành thành phần không tầm thường chính và tiêu thụ phần lớn thời gian xử lý. Trong IRIS, việc dự đoán ẑt+1 diễn ra tuần tự, vì mô hình bị giới hạn chỉ dự đoán một token phía trước tại mỗi bước. Hạn chế này phát sinh vì danh tính của token tiếp theo, vẫn chưa biết tại bước hiện tại, là cần thiết cho việc dự đoán các token sau này. Do đó, việc tạo ra H quan sát tiêu tốn KH lời gọi mô hình thế giới tuần tự. Điều này dẫn đến việc sử dụng GPU kém và thời gian tính toán dài.

Để vượt qua nút thắt này, POP duy trì một tập hợp K token dự đoán chuyên dụng u = (u1, ..., uK) cùng với các embedding tương ứng Eu trong R^(K×d). Để tạo ra ẑt+1 trong một lần, POP đơn giản tính toán các đầu ra RetNet bắt đầu từ St sử dụng u làm chuỗi đầu vào, như được minh họa trong Hình 4. Lưu ý rằng trong tưởng tượng, kích thước chunk bị giới hạn ở một block duy nhất, tức là K+1. Ở đây, ký hiệu St đề cập đến trạng thái tóm tắt t block quan sát-hành động đầu tiên. Để có được St, chúng tôi sử dụng forward chunkwise của RetNet để tóm tắt một đoạn ngữ cảnh ban đầu của các block được lấy mẫu từ replay buffer. Về cốt lõi, đối với mọi t, POP mô hình hóa phân phối sau cho dự đoán quan sát tiếp theo:

Hình 4. Một bước tưởng tượng duy nhất. Bắt đầu từ trạng thái lặp lại St, ban đầu thu được từ kinh nghiệm thực, M tính toán tất cả các token quan sát tiếp theo ẑt+1 song song sử dụng các token dự đoán u làm đầu vào. Sau đó, agent quan sát ẑt+1 và chọn một hành động at+1. Cuối cùng, M lấy St, ẑt+1 và at+1 và xuất ra St+1, r̂t+1, d̂t+1. Các mũi tên nét đứt nhấn mạnh các hoạt động lấy mẫu.

p(ẑt+1|z1, a1, ..., zt, at, u)
với
p(ẑ^k_t+1|z1, a1, ..., zt, at, u≤k).

Điều đáng chú ý là các token u chỉ được sử dụng cho dự đoán token quan sát và không bao giờ được sử dụng trong việc cập nhật trạng thái lặp lại.

Hình 5. Khi nối u sau một block quan sát-hành động, chuỗi không còn là tiền tố của trajectory token quan sát-hành động. Do đó, trạng thái lặp lại chỉ tóm tắt các token quan sát và hành động (trajectory trên).

Phương pháp này hiệu quả giảm tổng số lời gọi mô hình thế giới trong tưởng tượng từ KH xuống 2H, loại bỏ sự phụ thuộc vào số lượng token quan sát K. Thực tế, POP cung cấp một chế độ tạo bổ sung giảm thêm số lượng lời gọi tuần tự xuống H. Chúng tôi trì hoãn chi tiết về chế độ này đến Phụ lục A.1.2. Ngoài ra, bằng cách sử dụng trạng thái lặp lại tóm tắt các chuỗi lịch sử dài, POP cải thiện hiệu quả hơn nữa, vì chi phí dự đoán mỗi token giảm. Hiệu quả, POP mang lại khả năng mở rộng được cải thiện với chi phí tính toán tổng thể cao hơn ((2K+1)H so với (K+1)H). Phương pháp của chúng tôi bổ sung vào bằng chứng hiện có cho thấy khả năng mở rộng được tăng cường thường có lợi, ngay cả với chi phí tính toán bổ sung, với Transformer (Vaswani et al., 2017) làm ví dụ nổi bật.

2.4. Huấn luyện Mô hình Thế giới
Trong khi áp dụng POP trong tưởng tượng khá đơn giản, nó đòi hỏi sửa đổi dữ liệu huấn luyện. Xem xét một đoạn trajectory đầu vào (z1, a1, ..., zT, aT) được lấy mẫu từ replay buffer. Để thực hiện dự đoán quan sát có ý nghĩa trong tưởng tượng, mô hình nên được huấn luyện để dự đoán zt cho (z1, a1, ..., zt-1, at-1, u), cho mỗi bước thời gian t của mọi đoạn đầu vào. Do đó, đối với mọi t, chuỗi đầu vào nên chứa u tại block t. Tuy nhiên, việc thay thế zt bằng u trong chuỗi gốc là không đủ, vì việc dự đoán các quan sát, phần thưởng và tín hiệu kết thúc trong tương lai phụ thuộc vào zt. Do đó, phương pháp tiêu chuẩn tính toán tất cả đầu ra từ cùng một chuỗi đầu vào không khả thi, vì trong trường hợp này hai yêu cầu này mâu thuẫn với nhau (Hình 5).

Thách thức sau đó nằm ở việc tạo ra một phương pháp hiệu quả để tính toán các đầu ra cho tất cả các bước thời gian song song.

--- TRANG 5 ---
Cải thiện Mô hình Thế giới Dựa trên Token với Dự đoán Quan sát Song song

Hình 6. Một minh họa về thuật toán forward chunkwise POP (Alg. 1 và 2) cho một mô hình một lớp. Trong quá trình huấn luyện, M tính toán các đầu ra của c block quan sát-hành động song song. Các ô vuông xanh đại diện cho đầu vào token, trong khi các đầu ra RetNet tương ứng được ký hiệu bằng các vòng tròn. Mỗi block RetNet đại diện cho một lời gọi forward đến cùng một mô hình RetNet. Lời gọi RetNet dưới cùng sử dụng phần mở rộng POP của chúng tôi để tính toán các trạng thái lặp lại bổ sung ở cuối mỗi block quan sát-hành động (Alg. 2, dòng 2-7). Hàng trên của các lời gọi RetNet được tính toán batch song song (Alg. 2, dòng 8). Cuối cùng, đầu ra kết hợp các đầu ra token quan sát được tạo bởi lời gọi RetNet trên với các đầu ra phần thưởng và kết thúc được tính toán bởi lời gọi dưới (Alg. 1, dòng 7-9).

Để giải quyết thách thức này, trước tiên chúng tôi lưu ý rằng mỗi tiền tố trajectory có thể được tóm tắt thành một trạng thái lặp lại duy nhất. Ví dụ, đối với chunk đầu vào đầu tiên (z1, a1, ..., zc, ac), (z1, a1) có thể được tóm tắt thành S[1,1] và (z1, a1, z2, a2) có thể được tóm tắt thành S[1,2]. Ở đây, chúng tôi sử dụng chỉ số dưới [i, j] để thuận tiện đề cập đến block thứ j trong chunk thứ i (ký hiệu này được thể hiện trong Hình 3), với S[i,0] = S[i-1] và S[i,c] = S[i]. Do đó, kế hoạch của chúng tôi là trước tiên tính toán tất cả các trạng thái S[i,1], ..., S[i,c] song song, sau đó dự đoán tất cả quan sát tiếp theo từ tất cả các cặp (S[i,j], u).

Để tính toán tất cả các trạng thái lặp lại S[i,j] song song, một phép tính hai bước được thực hiện. Đầu tiên, các trạng thái trung gian S̃[i,j] được tính toán song song cho tất cả j với

S̃[i,j] = K[i,j] ⊙ zeta^T V[i,j], (6)

trong đó zeta trong R^((K+1)×d) là một ma trận với zeta_ij = eta^(K-i). Sau đó, mỗi trạng thái lặp lại được tính toán tuần tự bởi

S[i,j] = S̃[i,j] + eta^(K+1) S[i,j-1]. (7)

Vì phần lớn gánh nặng tính toán nằm ở bước đầu tiên, phép tính tuần tự ở bước thứ hai có tác động tối thiểu đến tăng tốc tổng thể.

Khi chúng ta đã có tất cả các trạng thái sẵn sàng, đầu ra của (S[i,j], u) cho tất cả 1 ≤ j ≤ c được tính toán song song. Ở đây, chúng tôi nhấn mạnh rằng cơ chế Retention hiện có chỉ có thể thực hiện tính toán đầu vào theo batch với các trạng thái lặp lại St của cùng một bước thời gian t. Điều này do thông tin embedding vị trí được chia sẻ được áp dụng cho mỗi chuỗi đầu vào trong batch. Để vượt qua điều này, chúng tôi tạo ra một cơ chế mở rộng RetNet để hỗ trợ tính toán batch của các cặp (S[i,j], u), trong khi áp dụng thông tin mã hóa vị trí thích hợp. Một mã giả của phần mở rộng POP mới của chúng tôi cho RetNet được đưa ra trong Thuật toán 1 và 2. Thuật toán sau trình bày cốt lõi của cơ chế (được mô tả ở trên), trong khi thuật toán trước mô tả tính toán lớp theo lớp mức cao hơn với một tập hợp cuối cùng để kết hợp các đầu ra được tạo ra. Hình 6 minh họa một ví dụ đơn giản về cơ chế POP Forward (Thuật toán 1 và 2) cho một mô hình một lớp. Để ngắn gọn, mã giả và minh họa của chúng tôi chỉ xem xét các lớp Retention, bỏ qua các mô-đun khác của RetNet (Phụ lục A.3).

Thuật toán 1 RetNet POP Chunkwise Forward
1: Đầu vào: kích thước chunk 1 ≤ c ≤ H, embedding token X[i] của chunk i, trạng thái lặp lại mỗi lớp {S^l[i-1]}^L_l=1.
2: Khởi tạo A^0[i] ← X
3: Khởi tạo B^0[i,1], ..., B^0[i,c] ← Eu, ..., Eu
4: for l = 1 to L do
5: A^l[i], B^l[i], S^l[i] ← POPLayer(A^(l-1)[i], B^(l-1)[i], S^l[i-1], i)
6: end for
7: for j = 1 to c do
8: Y[i,j] ← Concat(B^L[i,j], A^L[i,j,K+1])
9: end for
10: Return Y, {S^l[i]}^L_l=1

Thuật toán 2 POPLayer Chunkwise Forward
1: Đầu vào: Chunk latent A[i], latent dự đoán quan sát B[i], trạng thái lặp lại S[i-1], chỉ số chunk i.
2: A[i] ← Retention(A[i], S[i-1], i) (Phương trình 4)
3: Tính toán S̃[i,1], ..., S̃[i,c] song song (Phương trình 6)
4: for j = 1 to c do
5: S[i,j] ← S̃[i,j] + eta^(K+1) S[i,j-1] (Phương trình 7)
6: end for
7: S[i] ← S[i,c]
8: B[i,j] ← Retention(B[i,j], S[i,j-1], [i,j]) song song cho j = 1, ..., c (Phương trình 4)
9: Return A[i], B[i], S[i]

Để huấn luyện mô hình thế giới, các đoạn trajectory gồm H bước từ kinh nghiệm quá khứ được lấy mẫu đồng đều từ replay buffer và được chuyển đổi thành các chuỗi token. Các chuỗi này được xử lý trong các chunk gồm c block quan sát-hành động để tạo ra các phân phối được mô hình hóa, như được mô tả trong Hình 6. Tối ưu hóa được thực hiện bằng cách tối thiểu hóa loss cross-entropy của các đầu ra chuyển đổi và kết thúc, và loss thích hợp của các đầu ra phần thưởng, tùy thuộc vào tác vụ. Đối với phần thưởng liên tục, loss mean-squared error được sử dụng trong khi đối với phần thưởng rời rạc, cross-entropy được sử dụng thay thế.

--- TRANG 6 ---
Cải thiện Mô hình Thế giới Dựa trên Token với Dự đoán Quan sát Song song

Bảng 1. Lợi ích trung bình của agent trên 26 trò chơi của benchmark Atari 100k theo sau bởi các chỉ số hiệu suất chuẩn hóa con người trung bình. Điểm số mỗi trò chơi được tính như trung bình của 5 lần chạy với các seed khác nhau, trong đó điểm số của mỗi lần chạy được tính như trung bình trên 100 episode được lấy mẫu ở cuối quá trình huấn luyện. Chữ đậm và gạch dưới đánh dấu điểm số cao nhất trong các phương pháp dựa trên token và trong tất cả các baseline tương ứng.

3. Thí nghiệm
Chúng tôi tuân theo hầu hết các công trình trước đó về mô hình thế giới và đánh giá REM trên benchmark Atari 100K được công nhận rộng rãi (Kaiser et al., 2020) cho học tăng cường hiệu quả mẫu. Benchmark Atari 100K xem xét một tập con gồm 26 trò chơi Atari. Đối với mỗi trò chơi, agent bị giới hạn ở 100K bước tương tác, tương ứng với 400K khung hình trò chơi do frame-skip tiêu chuẩn là 4. Tổng cộng, điều này lên đến khoảng 2 giờ chơi game. Để đặt trong bối cảnh, benchmark Atari ban đầu cho phép agent thu thập 200M bước, tức là nhiều hơn 500 lần kinh nghiệm.

Thiết lập Thí nghiệm Chi tiết đầy đủ về các kiến trúc và siêu tham số được sử dụng trong các thí nghiệm của chúng tôi được trình bày trong Phụ lục A.1. Đáng chú ý, tokenizer của chúng tôi sử dụng K = 64 (tức là lưới 8×8 token latent mỗi quan sát), trong khi IRIS chỉ sử dụng K = 4×4 = 16. Để đảm bảo so sánh có ý nghĩa về thời gian chạy của REM và IRIS, cấu hình của REM được chọn sao cho lượng tính toán được thực hiện bởi mỗi thành phần tại mỗi epoch vẫn (gần như) tương đương với thành phần tương ứng trong IRIS.

Để đánh giá thời gian chạy của các agent, chúng tôi sử dụng một workstation với GPU Nvidia RTX 4090. Phần còn lại của các thí nghiệm được thực hiện trên GPU Nvidia V100.

Baseline Vì các đóng góp của bài báo này liên quan đến các phương pháp dựa trên token, và đặc biệt đến IRIS, đánh giá của chúng tôi tập trung vào các phương pháp dựa trên token. Để làm phong phú kết quả của chúng tôi, cũng như để tạo điều kiện cho nghiên cứu trong tương lai, chúng tôi đã bao gồm các baseline bổ sung sau: SimPLe (Kaiser et al., 2020), DreamerV3 (Hafner et al., 2023), TWM (Robine et al., 2023), và STORM (Zhang et al., 2023). Trong các phương pháp này, các quan sát được xử lý như một phần tử chuỗi duy nhất bởi mô hình thế giới. Theo các công trình trước đó về mô hình thế giới, các phương pháp tìm kiếm lookahead như MuZero

Hình 7. Các chỉ số tổng hợp Atari 100K với khoảng tin cậy bootstrap phân tầng 95% của điểm số chuẩn hóa con người trung bình, trung vị và inter-quantile mean (IQM) và khoảng cách tối ưu (Agarwal et al., 2021). Một đường phân tách các phương pháp dựa trên token khỏi các baseline khác.

(Schrittwieser et al., 2020) và EfficientZero (Ye et al., 2021) không được bao gồm vì tìm kiếm lookahead hoạt động trên đỉnh của một mô hình thế giới. Ở đây, mục tiêu của chúng tôi là cải thiện chính thành phần mô hình thế giới.

3.1. Kết quả
Trên Atari, việc sử dụng điểm số chuẩn hóa con người (HNS) (Mnih et al., 2015), được tính toán như (điểm agent - điểm ngẫu nhiên)/(điểm con người - điểm ngẫu nhiên), thay vì điểm số trò chơi thô là tiêu chuẩn. Ở đây, điểm số cuối cùng của mỗi lần chạy huấn luyện được tính như trung bình trên 100 episode được thu thập ở cuối quá trình huấn luyện. Trong công trình của (Agarwal et al., 2021), các tác giả tìm thấy sự khác biệt giữa các kết luận được rút ra từ thống kê ước lượng điểm như trung bình và trung vị và một phân tích thống kê kỹ lưỡng hơn cũng xem xét sự không chắc chắn trong kết quả. Tuân thủ giao thức đã thiết lập của họ và sử dụng bộ công cụ của họ, chúng tôi báo cáo điểm số chuẩn hóa con người trung bình, trung vị và interquantile mean (IQM), và khoảng cách tối ưu, với khoảng tin cậy bootstrap phân tầng 95% trong Hình 7. Hồ sơ hiệu suất được trình bày trong Hình 8. Điểm số trung bình của các trò chơi riêng lẻ được báo cáo trong Bảng 1.

REM đạt được điểm số IQM chuẩn hóa con người là 0.673, vượt trội so với tất cả các baseline. Ngoài ra, REM cải thiện so với IRIS trên 3 trong số 4 chỉ số (tức là trung bình, khoảng cách tối ưu và IQM), trong khi có thể so sánh về điểm số trung vị. Đáng chú ý, REM đạt được hiệu suất siêu nhân trên 12 trò chơi, nhiều hơn bất kỳ baseline nào khác (Bảng 1). REM cũng thể hiện điểm số tiên tiến trên nhiều trò chơi, bao gồm Assault, Boxing và Chopper Command. Những phát hiện này hỗ trợ tuyên bố thực nghiệm của chúng tôi rằng REM hoạt động tương tự hoặc tốt hơn các phương pháp dựa trên token trước đó trong khi chạy nhanh hơn đáng kể.

3.2. Nghiên cứu Ablation
Để phân tích tác động của các thành phần khác nhau trong phương pháp của chúng tôi đối với hiệu suất của REM, chúng tôi tiến hành một loạt nghiên cứu ablation. Đối với mỗi thành phần, chúng tôi so sánh thuật toán cuối cùng với một phiên bản trong đó thành phần quan tâm bị vô hiệu hóa. Do hạn chế tài nguyên tính toán, đánh giá được thực hiện trên một tập con gồm 8 trò chơi từ benchmark Atari 100K sử dụng 5 seed ngẫu nhiên cho mỗi trò chơi. Tập con này bao gồm các trò chơi có sự khác biệt điểm số lớn giữa IRIS và REM, vì chúng tôi đặc biệt quan tâm đến việc nghiên cứu tác động của mỗi thành phần trong các trò chơi này. Cụ thể, tập con này bao gồm các trò chơi "Assault", "Asterix", "Chopper Command", "Crazy Climber", "Demon Attack", "Gopher", "Krull", và "Road Runner". Chúng tôi thực hiện nghiên cứu ablation trên các khía cạnh sau: cơ chế POP, kiến trúc không gian latent của C và đầu vào hành động của nó, độ phân giải latent của V, và embedding token quan sát được sử dụng bởi M.

Xác suất cải thiện (Agarwal et al., 2021) và điểm số IQM chuẩn hóa con người được trình bày trong Hình 9. Hình 10 cung cấp so sánh thời gian huấn luyện, đối chiếu REM với các ablation liên quan đến hiệu quả của nó.

Hình 8. Hồ sơ hiệu suất. Đối với mỗi giá trị điểm số chuẩn hóa con người (trục x), đường cong của mỗi thuật toán hiển thị phần các lần chạy của nó với điểm số lớn hơn giá trị điểm số đã cho. Vùng tô màu chỉ ra các dải tin cậy 95% theo điểm dựa trên bootstrap percentile với lấy mẫu phân tầng (Agarwal et al., 2021).

--- TRANG 8 ---
Cải thiện Mô hình Thế giới Dựa trên Token với Dự đoán Quan sát Song song

Hình 9. Trái: Xác suất cải thiện (Agarwal et al., 2021) hiển thị xác suất REM vượt trội so với mỗi ablation trên một trò chơi được chọn ngẫu nhiên từ tập con 8 trò chơi được sử dụng cho các nghiên cứu ablation. Phải: điểm số interquantile mean (IQM) chuẩn hóa con người. Mỗi dải chỉ ra khoảng tin cậy bootstrap phân tầng 95%.

Phân tích POP Để nghiên cứu tác động của POP đối với hiệu suất của REM, chúng tôi thay thế RetNet được bổ sung POP của M bằng một RetNet vanilla. Trong phiên bản này, được ký hiệu là "No POP", việc dự đoán các token quan sát tiếp theo được thực hiện tuần tự token-từng-token, như được thực hiện trong IRIS.

Kết quả của chúng tôi cho thấy rằng POP duy trì hiệu suất của agent (Hình 9) trong khi giảm đáng kể thời gian tính toán tổng thể (Hình 10). Trong Phụ lục A.4, chúng tôi cung cấp kết quả bổ sung cho thấy rằng hiệu suất của mô hình thế giới cũng được duy trì. POP đạt được thời gian tính toán tổng thể thấp hơn bằng cách đẩy nhanh giai đoạn học actor-critic, mặc dù chi phí tính toán tăng do các token dự đoán quan sát.

Kiến trúc Actor-Critic và Đầu vào Hành động Đối với C, chúng tôi xem xét một ablation tăng dần. Đầu tiên, chúng tôi thay thế kiến trúc của controller C của REM bằng kiến trúc của IRIS (được ký hiệu "CIRIS"). Ngược lại với REM, phiên bản này xử lý các khung hình pixel được tái tạo hoàn toàn và không kết hợp đầu vào hành động. Chính thức, CIRIS mô hình hóa π(at|ôt≤t), Vπ(ôt≤t). Trong ablation thứ hai, REM được sửa đổi sao cho chỉ các đầu vào hành động của C bị vô hiệu hóa. Ablation này tương ứng với π(at|ẑt≤t), Vπ(ẑt≤t).

Phát hiện của chúng tôi cho thấy rằng cả kiến trúc dựa trên mã latent và các đầu vào hành động được thêm vào đều đóng góp vào hiệu suất cuối cùng của REM (Hình 9). Ngoài ra, kiến trúc dựa trên mã latent của C dẫn đến chi phí tính toán giảm và thời gian học actor-critic ngắn hơn (Hình 10).

Độ phân giải Tokenizer Ở đây, chúng tôi so sánh REM với một phiên bản có độ phân giải latent giảm 4×4, tương tự như của IRIS. Kết quả trong Hình 9 cung cấp bằng chứng rõ ràng rằng độ phân giải latent của tokenizer có tác động đáng kể đến hiệu suất của agent. Kết quả của chúng tôi chứng minh rằng

Hình 10. So sánh thời gian epoch giữa REM và hai ablation của nó: "No POP", và CIRIS.

POP cho phép REM sử dụng độ phân giải latent cao hơn trong khi phát sinh thời gian tính toán ngắn hơn so với các phương pháp dựa trên token trước đó.

Embedding Mô hình Thế giới Trong REM, M chuyển đổi các token quan sát thành vector embedding sử dụng bảng embedding E được học bởi V. Các embedding này mã hóa thông tin thị giác như được học bởi V. Ngược lại, IRIS duy trì một bảng embedding riêng được học bởi mô hình thế giới cho mục đích đó. Ở đây, kết quả trong Hình 9 cung cấp bằng chứng thực nghiệm cho thấy việc tận dụng thông tin thị giác được mã hóa này dẫn đến hiệu suất được cải thiện. Trong Phụ lục A.4, chúng tôi cung cấp bằng chứng bổ sung cho thấy rằng dự đoán quan sát tiếp theo của mô hình thế giới cũng được cải thiện.

4. Công trình Liên quan
Học tăng cường dựa trên mô hình (RL), với nguồn gốc trong bối cảnh bảng (Sutton, 1991), đã là trọng tâm của nghiên cứu rộng rãi trong những thập kỷ gần đây. Agent RL sâu của (Ha & Schmidhuber, 2018) đã tận dụng một mô hình chuỗi LSTM (Hochreiter & Schmidhuber, 1997) với một VAE (Kingma & Welling, 2014) để mô hình hóa động lực học trong môi trường thị giác, chứng minh rằng các chính sách thành công có thể được học hoàn toàn từ dữ liệu mô phỏng. Phương pháp này, thường được biết đến như mô hình thế giới, sau đó được áp dụng cho các trò chơi Atari (Kaiser et al., 2020) với thuật toán RL PPO (Schulman et al., 2017). Sau đó, một loạt các công trình (Hafner et al., 2020; 2021; 2023) đề xuất các thuật toán Dreamer, dựa trên một mô hình không gian trạng thái lặp lại (RSSM) (Hafner et al., 2019) để mô hình hóa động lực học. DreamerV3 mới nhất được đánh giá trên nhiều môi trường thách thức, cung cấp bằng chứng thêm về tiềm năng đầy hứa hẹn của mô hình thế giới. Ngược lại với các phương pháp dựa trên token, trong đó mỗi token phục vụ như một phần tử chuỗi độc lập, Dreamer mã hóa mỗi khung hình như một vector của các biến phân loại, được xử lý cùng một lúc bởi RSSM.

Theo thành công của kiến trúc Transformer (Vaswani et al., 2017) trong mô hình hóa ngôn ngữ (Brown et al., 2020), và được thúc đẩy bởi các thuộc tính mở rộng thuận lợi của chúng so với RNN, Transformer gần đây được khám phá trong RL (Chen et al., 2021; Parisotto et al., 2020; Reed et al., 2022; Shridhar et al., 2023). Các phương pháp mô hình thế giới cũng áp dụng kiến trúc Transformer. (Micheli et al., 2023) đi tiên phong cho các mô hình thế giới dựa trên token với IRIS, đại diện cho các trajectory agent như các chuỗi giống ngôn ngữ. Bằng cách xử lý mỗi quan sát như một chuỗi, mô hình thế giới dựa trên Transformer của nó đạt được một độ phân giải attention sub-observation rõ ràng. Mặc dù hiệu suất cao của IRIS, nút thắt tưởng tượng của nó dẫn đến một bất lợi đáng kể. Ngoài IRIS, các mô hình thế giới không dựa trên token được điều khiển bởi Transformer đã được đề xuất. TWM (Robine et al., 2023) sử dụng kiến trúc Transformer-XL (Dai et al., 2020) và một lấy mẫu dữ liệu không đồng đều. STORM (Zhang et al., 2023) đề xuất một agent mô hình thế giới dựa trên Transformer hiệu quả thiết lập kết quả tiên tiến cho benchmark Atari 100K. STORM có một Transformer 2 lớp nhỏ hơn đáng kể so với các mô hình 10 lớp của TWM và IRIS, thể hiện thời gian huấn luyện giảm đáng kể và hiệu suất agent được cải thiện.

5. Kết luận
Trong công trình này, chúng tôi đã trình bày một cơ chế dự đoán quan sát song song (POP) mới bổ sung mạng Retention với một chế độ forward chuyên dụng để cải thiện hiệu quả của các mô hình thế giới dựa trên token (TBWMs). POP hiệu quả giải quyết nút thắt tưởng tượng của TBWMs và cho phép chúng xử lý các chuỗi quan sát dài hơn. Ngoài ra, chúng tôi giới thiệu REM, một agent TBWM được trang bị POP. REM là agent mô hình thế giới đầu tiên được điều khiển bởi kiến trúc RetNet. Về mặt thực nghiệm, chúng tôi đã chứng minh sự vượt trội của REM so với các TBWM trước đó trên benchmark Atari 100K, làm cho REM cạnh tranh với hiện trạng tiên tiến, cả về hiệu suất agent và thời gian chạy tổng thể.

Công trình của chúng tôi mở ra nhiều hướng nghiên cứu đầy hứa hẹn trong tương lai bằng cách làm cho TBWMs trở nên thực tế và hiệu quả về chi phí. Một hướng như vậy có thể là khám phá một sửa đổi của REM trong đó trạng thái lặp lại của mô hình thế giới tóm tắt toàn bộ lịch sử của agent. Tương tự, một kiến trúc RetNet bảo tồn lịch sử nên được xem xét cho controller. Một hướng đầy hứa hẹn khác sẽ là tận dụng việc tối ưu hóa độc lập của tokenizer để cho phép REM sử dụng các mô hình nhận thức thị giác được đào tạo trước trong các môi trường mà dữ liệu thị giác phong phú, ví dụ, thế giới thực. Các mô hình nhận thức như vậy có thể được huấn luyện ở quy mô lớn, và cho phép REM chỉ lưu trữ các quan sát nén trong replay buffer của nó, cải thiện thêm hiệu quả của nó. Cuối cùng, các phương pháp dựa trên token cho các tác vụ tạo video có thể hưởng lợi từ việc sử dụng cơ chế POP để tạo toàn bộ khung hình song song có điều kiện trên ngữ cảnh quá khứ. Chúng tôi tin rằng đây là một hướng thú vị để khám phá với tác động có khả năng cao.

Lời cảm ơn
Dự án này đã nhận được tài trợ từ Chương trình Horizon Europe của Liên minh Châu Âu theo thỏa thuận cấp số 101070568.

Tuyên bố Tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học máy. Có nhiều hệ quả xã hội tiềm tàng của công trình chúng tôi, không có hệ quả nào mà chúng tôi cảm thấy phải đặc biệt nhấn mạnh ở đây.

Tài liệu tham khảo

[Phần tài liệu tham khảo được giữ nguyên tiếng Anh]

--- TRANG 13 ---
Cải thiện Mô hình Thế giới Dựa trên Token với Dự đoán Quan sát Song song

A. Phụ lục

A.1. Mô hình và Siêu tham số
Bảng 2 và 3 chi tiết các siêu tham số của tối ưu hóa và môi trường, cũng như các siêu tham số được chia sẻ bởi nhiều thành phần.

Bảng 2. Siêu tham số Được chia sẻ

[Bảng được giữ nguyên với cấu trúc tiếng Anh nhưng mô tả bằng tiếng Việt]

Bảng 3. Siêu tham số Theo từng Thành phần

[Bảng được giữ nguyên với cấu trúc tiếng Anh nhưng mô tả bằng tiếng Việt]

A.1.1. TOKENIZER (V)
Kiến trúc Tokenizer Tokenizer của chúng tôi dựa trên triển khai của VQ-GAN (Esser et al., 2021). Tuy nhiên, chúng tôi đã đơn giản hóa các kiến trúc của mạng encoder và decoder. Mô tả về các kiến trúc của mạng encoder và decoder có thể được tìm thấy trong bảng 4.

Học Tokenizer Theo IRIS (Micheli et al., 2023), tokenizer của chúng tôi là một VQ-VAE (van den Oord et al., 2017) dựa trên triển khai của (Esser et al., 2021) (không có discriminator). Mục tiêu huấn luyện được cho bởi

L(E, D, E) = ||x - D(z)||₁ + ||sg(E(x)) - E(z)||₂² + ||sg(E(z)) - E(x)||₂² + Lperceptual(x, D(z)) (8)

trong đó E và D là các mô hình encoder và decoder tương ứng, và sg(·) là toán tử stop-gradient. Số hạng đầu tiên ở phía bên phải của Phương trình 8 trên là loss tái tạo, số hạng thứ hai và thứ ba tương ứng với loss cam kết, và số hạng cuối cùng là loss nhận thức.

[Phần còn lại của phụ lục tiếp tục với cùng cách dịch, giữ nguyên cấu trúc bảng, phương trình và thuật toán nhưng dịch các mô tả và giải thích sang tiếng Việt]
