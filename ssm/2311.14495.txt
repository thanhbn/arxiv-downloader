# 2311.14495.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/ssm/2311.14495.pdf
# File size: 976090 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through
Stable Reparameterization
Shida Wang1Qianxiao Li1 2
Abstract
In this paper, we investigate the long-term mem-
ory learning capabilities of state-space models
(SSMs) from the perspective of parameterization.
We prove that state-space models without any
reparameterization exhibit a memory limitation
similar to that of traditional RNNs: the target rela-
tionships that can be stably approximated by state-
space models must have an exponential decaying
memory. Our analysis identifies this “curse of
memory” as a result of the recurrent weights con-
verging to a stability boundary, suggesting that a
reparameterization technique can be effective. To
this end, we introduce a class of reparameteriza-
tion techniques for SSMs that effectively lift its
memory limitations. Besides improving approx-
imation capabilities, we further illustrate that a
principled choice of reparameterization scheme
can also enhance optimization stability. We val-
idate our findings using synthetic datasets, lan-
guage models and image classifications.
1. Introduction
Understanding long-term memory relationships is fun-
damental in sequence modeling. Capturing this pro-
longed memory is vital, especially in applications like
time series prediction (Connor et al., 1994), language mod-
els (Sutskever et al., 2011). Since its emergence, trans-
formers (Vaswani et al., 2017) have become the go-to mod-
els for language representation tasks (Brown et al., 2020).
However, a significant drawback lies in their computational
complexity, which is asymptotically O(T2), where Tis
the sequence length. This computational bottleneck has
been a critical impediment to the further scaling-up of trans-
1Department of Mathematics, National University of Singa-
pore2Institute for Functional Intelligent Materials, National Uni-
versity of Singapore. Correspondence to: Qianxiao Li <qianx-
iao@nus.edu.sg >.
Proceedings of the 41stInternational Conference on Machine
Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).former models. State-space models such as S4 (Gu et al.,
2022b), S5 (Smith et al., 2023), LRU (Orvieto et al., 2023b),
RWKV (Peng et al., 2023), RetNet (Sun et al., 2023) and
Mamba (Gu & Dao, 2023) offer an alternative approach.
These models are of the recurrent type and excel in long-
term memory learning. Their architecture is specifically
designed to capture temporal dependencies over extended
sequences, providing a robust solution for tasks requiring
long-term memory (Tay et al., 2021). One of the advantages
of state-space models over traditional RNNs lies in their
computational efficiency, achieved through the application
of parallel scan algorithms (Martin & Cundy, 2018) and Fast
Fourier Transform (FFT) (Tolimieri et al., 1989; Gu et al.,
2022b). Traditional nonlinear RNNs are often plagued by
slow forward and backward propagation, a limitation that
state-space models circumvent by leveraging linear RNN
blocks.
Traditional linear/nonlinear RNNs exhibit an asymptotically
exponential decay in memory (Wang et al., 2023). This
phenomenon explains the difficulty in both approximation
and optimization to learn long-term memory using RNNs
(also named curse of memory). In practice, empirical re-
sults show that SSMs variants like S4 overcome some of the
memory issues. The previous empirical results suggest
that either (i) the “linear dynamics and nonlinear lay-
erwise activation” or (ii) the parameterization inherent
to S4, is pivotal in achieving the enhanced performance.
Current research answers which one is more important. We
first prove an inverse approximation theorem showing that
state-space models without reparameterization still suffer
from the “curse of memory”, which is consistent with empir-
ical results (Wang & Xue, 2023). This rules out the point (i)
as the reason for SSMs’ good long-term memory learning.
A natural question arises regarding whether the reparameter-
izations are the key to learn long-term memory. We prove a
class of reparameterization functions f, which we call sta-
ble reparameterization, enables the stable approximation of
nonlinear functionals. This includes commonly used expo-
nential reparameterization and softplus reparameterization.
Furthermore, we question whether S4’s parameterizations
are optimal. Here we give a particular sense in terms of
optimization stability that they are not optimal. We pro-
pose the optimal one and show its stability via numerical
1arXiv:2311.14495v4  [cs.LG]  5 Jun 2024

--- PAGE 2 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
experiments.
We summarize our main contributions as follow:
1.We prove that similar to RNNs, the state-space models
without reparameterization can only stably approxi-
mate targets with exponential decaying memory.
2.We identify a class of stable reparameterization which
achieves the stable approximation of any nonlinear
functionals . Both theoretical and empirical evidence
highlight that stable reparameterization is crucial for
long-term memory learning.
3.From the optimization viewpoint, we propose the gradi-
ent boundedness as the criterion and show the gradients
are bounded by a form that depends on the parameter-
ization. Based on the gradient bound, we solve the
differential equation and derive the “best” reparame-
terization in the stability sense and verify the stability
of this new reparameterization across different parame-
terization schemes.
Notation. We use the bold face to represent the sequence
while then normal letters are scalars, vectors or functions.
Throughout this paper we use ∥ · ∥ to denote norms over
sequences of vectors, or function(al)s, while | · |(with sub-
scripts) represents the norm of number, vector or weights
tuple. Here |x|∞:= max i|xi|,|x|2:=pP
ix2
i,|x|1:=P
i|xi|are the usual max ( L∞) norm, L2norm and L1
norm. We use mto denote the hidden dimension.
2. Background
In this section, we first introduce the state-space models and
compare them to traditional nonlinear RNNs. Subsequently,
we adopt the sequence modeling as a problem in nonlinear
functional approximation framework. Specifically, the theo-
retical properties we anticipate from the targets are defined.
Moreover, we define the “curse of memory” phenomenon
and provide a concise summary of prior theoretical defini-
tions and results concerning RNNs.
2.1. State-space models
State-space models (SSMs) are a family of neural networks
specialized in sequence modeling. Unlike Recurrent Neu-
ral Networks (RNNs) (Rumelhart et al., 1986), SSMs have
layer-wise nonlinearity and linear dynamics within their
hidden states. This unique structure facilitates acceler-
ated computing using FFT (Gu et al., 2022b) or parallel
scan (Martin & Cundy, 2018). With trainable weights
W∈Rm×m, U∈Rm×d, b, c∈Rmand activation func-
tionσ(·), the simplest SSM maps d-dimensional input se-
quence x={xt}to 1-dimensional output sequence {ˆyt}.To simplify our analysis, we utilize the continuous-time
framework referenced in Li et al. (2020):
dht
dt=Wht+Uxt+b, h −∞= 0,
ˆyt=c⊤σ(ht), t ∈R.(1)
As detailed in Appendix A, the above form is a simplifica-
tion of practical SSMs in the sense that practical SSMs can
be realized by the stacking of Equation (1).
It is known that multi-layer state-space models are universal
approximators (Wang & Xue, 2023; Orvieto et al., 2023a).
In particular, when the nonlinearity is added layer-wise, it
is sufficient (in approximation sense) to use real diagonal
W(Gu et al., 2022a; Li et al., 2022). In this paper, we
only consider the real diagonal matrix case and denote it by
Λ = Diag(λ1, . . . , λ m).
dht
dt= Λht+Uxt+b. (2)
Compared with S4, the major differences lie in initialization
such as HiPPO (Gu et al., 2020) and parameters saving
method such as DPLR (Gu et al., 2022a) and NPLR (Gu
et al., 2022b).
2.2. Sequence modeling as nonlinear functional
approximations
Sequence modeling aims to discern the association between
an input series, represented as x={xt}, and its corre-
sponding output series, denoted as y={yt}. The input
series are continuous bounded inputs vanishing at infinity:
x∈ X=C0(R,Rd)with norm ∥x∥∞:= supt∈R|xt|∞. It
is assumed that the input and output sequences are deter-
mined from the inputs via a set of functionals, symbolized
as
H={Ht:X →R:t∈R}, (3)
through the relationship yt=Ht(x). In essence, the chal-
lenge of sequential approximation boils down to estimat-
ing the desired functional sequence Husing a different
functional sequence bHpotentially from a predefined model
space such as SSMs.
In this paper we focus on target functionals that are bounded,
causal, continuous, regular, time-homogeneous (time-shift
invariant). Formal definitions are given in Appendix B.1.
The continuity, boundedness, time-homogeneity, causal-
ity are important properties for good sequence-to-sequence
models to have. Linearity is an important simplification as
many theoretical theorems are available in functional analy-
sis (Stein & Shakarchi, 2003). Without loss of generality, we
assume that the nonlinear functionals satisfy Ht(0) = 0 . It
can be achieved via studying Hadjusted
t (x) =Ht(x)−Ht(0).
2

--- PAGE 3 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
2.3. Memory function, stable approximation and curse
of memory
The concept of memory has been extensively explored in
academic literature, yet much of previous works rely on
heuristic approaches and empirical testing, particularly in
the context of learning long-term memory (Poli et al., 2023).
Here we study the memory property from a theoretical per-
spective.
Our study employs the extended framework proposed by
Wang et al. (2023), which specifically focuses on nonlinear
RNNs. However, these studies do not address the case
of state-space models. Within the same framework, the
slightly different memory function and decaying memory
concepts enable us to explore the approximation capabilities
of nonlinear functionals using SSMs.
Definition 2.1 (Memory function) .For bounded, causal,
continuous, regular and time-homogeneous nonlinear func-
tional sequences H={Ht:t∈R}onX, define the
following function as the memory function ofH: Over
bounded Heaviside input ux(t) =x·1{t≥0}
M(H)(t) := sup
x̸=0d
dtHt(ux)
|x|∞+ 1. (4)
We add 1 in the memory function definition to make it more
regular. The memory function of the target functionals is
assumed to be finite for all t∈R.
Definition 2.2 (Decaying memory) .The functional se-
quences Hhas a decaying memory if
lim
t→∞M(H)(t) = 0 . (5)
In particular, we say it has an exponential (polynomial)
decaying memory if there exists constant β >0such that
limt→∞eβtM(H)(t) = 0 (limt→∞tβM(H)(t) = 0 ).
Similar to Wang et al. (2023), this adjusted memory func-
tion definition is also compatible with the memory concept
in linear functional which is based on the famous Riesz
representation theorem (Theorem B.3 in Appendix B). In
the linear functional case, this memory function is the im-
pulse response function. It measures the decay speed of the
memory about an impulse given at t= 0. It is a surrogate to
characterize the model’s memorization about the previous
inputs in the hidden states htand outputs yt. While a large
memory value M(t)does not mean the model at time t
has a clear memorization about previous inputs x0, a small
memory value M(t)means the model has forgotten the im-
pulse input x0.Therefore, having a slow decay memory
function M(·)is a necessary condition to build a model
with long-term memory. As shown in Appendix C.1, the
nonlinear functionals constructed by state-space models are
point-wise continuous over Heaviside inputs. Combinedwith time-homogeneity, we know that state-space models
are nonlinear functionals with decaying memory (see Ap-
pendix C.2).
Definition 2.3 (Functional sequence approximation in
Sobolev-type norm) .Given functional sequences Hand
bH, we consider the approximation in the following Sobolev-
type norm (Appendix B.2):
H−bH
W1,∞:= (6)
sup
t 
∥Ht−bHt∥∞+dHt
dt−dbHt
dt
∞!
. (7)
Definition 2.4 (Perturbation error) .For target Hand pa-
rameterized model bH(·, θm), θm= (Λ , U, b, c )∈Θm:=
{Rm×m×Rm×d×Rm×Rm}, we define the perturbation
error for hidden dimension m:
Em(β) := sup
˜θm∈{θ:|θ−θm|2≤β}∥H−bH(·;˜θm)∥W1,∞.(8)
In particular, eHrefers to the perturbed models bH(·;˜θm).
Moreover, E(β) := lim supm→∞Em(β)is the asymptotic
perturbation error. The weight norm for SSM is |θ|2:=
max(|Λ|2,|U|2,|b|2,|c|2).
Based on the definition of perturbation error, we consider the
stable approximation as introduced by Wang et al. (2023).
Definition 2.5 (Stable approximation) .Letβ0>0. A target
functional sequence Hadmits a β0-stable approximation if
the perturbed error satisfies that:
1.E(0) = 0 .
2.E(β)is continuous for β∈[0, β0].
Equation E(0) = 0 means the universal approximation is
achieved by the hypothesis space. Stable approximation
strengthens the universal approximation by requiring the
model to be robust against perturbation on the weights. As
the stable approximation is the necessary requirement for
the optimal parameters to be found by the gradient-based
optimizations, it is a desirable assumption.
The “curse of memory” phenomenon, which was originally
formulated for linear functionals and linear RNNs, is well-
documented in prior research (Li et al., 2020; 2022; Jiang
et al., 2023). It describes the phenomenon where targets ap-
proximated by linear, hardtanh, or tanh RNNs must demon-
strate an exponential decaying memory. However, empirical
observations suggest that state-space models, particularly
the S4 variant, may possess favorable properties. Thus, it
is crucial to ascertain whether the inherent limitations of
RNNs can be circumvented using state-space models. Given
the impressive performance of state-space models, notably
3

--- PAGE 4 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
S4, a few pivotal questions arise: Do the model structure
of state-space models overcome the “curse of memory”? In
the subsequent section, we will demonstrate that the model
structure of state-space models does not indeed address the
curse of memory phenomenon.
3. Main results
In this section, we first prove that similar to the traditional
recurrent neural networks (Li et al., 2020; Wang et al., 2023),
state-space models without reparameterization suffer from
the “curse of memory” problem. This implies the targets
that can be stably approximated by SSMs must have expo-
nential decaying memory. Our analysis reveals that the prob-
lem arises from recurrent weights converging to a stability
boundary when learning targets associated with long-term
memory. Therefore, we introduce a class of stable reparam-
eterization techniques to achieve the stable approximation
for targets with polynomial decaying memory.
Beside the benefit of approximation perspective, we also
discuss the optimization benefit of the stable reparameteriza-
tions. We show that the stable reparameterization can make
the gradient scale more balanced, therefore the optimization
of large models can be more stable.
3.1. Curse of memory in SSMs
In this section, we present a theoretical theorem demon-
strating that the state-space structure does not alleviate the
“curse of memory” phenomenon. State-space models consist
of alternately stacked linear RNNs and nonlinear activa-
tions. Our result is established for both the shallow case and
deep case (Remark C.3). As recurrent models, SSMs with-
out reparameterization continue to exhibit the commonly
observed phenomenon of exponential memory decay, as
evidenced by empirical findings (Wang & Xue, 2023).
Assumption 3.1. We assume the hidden states remain uni-
formly bounded for any input sequence x, irrespective of the
hidden dimensions m. Specifically, this can be expressed as
sup
msup
t|ht|∞<∞. (9)
Assumption 3.2. We focus on strictly increasing, contin-
uously differentiable nonlinear activations with Lipschitz
constant L0. This property holds for activations such as
tanh, sigmoid, softsign σ(z) =z
1+|z|.
Theorem 3.3 (Curse of memory in SSMs) .Assume His
a sequence of bounded, causal, continuous, regular and
time-homogeneous functionals on Xwith decaying mem-
ory. Suppose there exists a sequence of state-space models
{bH(·, θm)}∞
m=1β0-stably approximating Hin the norm
defined in Equation (6). Assume the model weights are
uniformly bounded: θmax:= supm|θm|2<∞. Then thememory function M(H)(t)of the target decays exponen-
tially:
M(H)(t)≤(d+ 1)L0θ2
maxe−βt, t≥0, β < β 0.(10)
Here dis the dimension of input sequences. When gen-
eralized to multi-layer cases, the memory function bound
induced from ℓ-layer SSM is: For some polynomial P(t)
with degree at most l−1
M(H)(t)≤(d+ 1)Lℓ
0θℓ+1
maxP(t)e−βt, t≥0, β < β 0.
(11)
The proof of Theorem 3.3 is provided in Appendix C.3.
The (continuous-time) stability boundary (discussed in Re-
mark C.1) for Λin state-space models (Equation (2)) is
max i∈[m]λi(Λ)<0. This boundary comes from the sta-
biltiy criterion for linear time-invariant system. Compared
with previous results (Li et al., 2020; Wang et al., 2023), the
main proof difference comes from Lemma C.10 as the acti-
vation is in the readout yt=c⊤σ(ht). Our results provide a
more accurate characterization of memory decay, in contrast
to previous works that only offer qualitative estimates. A
consequence of Theorem 3.3 is that if the target exhibits a
non-exponential decay (e.g., polynomial decay), the recur-
rent weights converge to a stability boundary, thereby mak-
ing the approximation unstable. Finding optimal weights
can become challenging with gradient-based optimization
methods, as the optimization process tends to become un-
stable with the increase of model size. The numerical verifi-
cation is presented in Figure 1 (a). The lines intersect and
the intersections points shift towards the 0, suggesting that
the stable radius β0does not exist. Therefore SSMs without
reparameterization cannot stably approximate targets with
polynomial decaying memory.
3.2. Stable reparameterization and its advantage in
approximation
The proof of Theorem 3.3 suggests that the “curse of mem-
ory” arises due to the recurrent weights approaching a stabil-
ity boundary. Additionally, our numerical experiments (in
Figure 1 (c)) show that while state-space models suffer from
curse of memory, the commonly used S4 layer (with expo-
nential reparameterization) ameliorates this issue. However,
it is not a unique solution. Our findings highlight that the
foundation to achieving a stable approximation is the stable
reparameterization method, which we define as follows:
Definition 3.4 (Stable reparameterization) .We say a repa-
rameterization scheme f:R→Ris stable if there ex-
ists a continuous function gsuch that: g: [0,∞)→
[0,∞), g(0) = 0 :
sup
w"
|f(w)|sup
|˜w−w|≤βZ∞
0ef( ˜w)t−ef(w)tdt#
≤g(β).
(12)
4

--- PAGE 5 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
104
103
102
101
Perturbation106
104
102
100102104106Perturbation errorm=8
m=16
m=32
m=64
(a) SSM
104
103
102
101
Perturbation106
104
102
100102104106Perturbation errorm=8
m=16
m=32
m=64 (b) SoftplusSSM
104
103
102
101
100
Perturbation106
104
102
100102104106Perturbation errorm=8
m=16
m=32
m=64 (c) S4
Figure 1. State-space models without stable reparameterization cannot approximate targets with polynomial decaying memory. In (a), the
intersection of lines are shifting towards left as the hidden dimension mincreases. In (b), SSMs using softplus reparameterization has a
stable approximation. In (c), S4 can stably approximate the target with better stability.
Table 1. Impact of stable reparameterizations in approximation and stable approximation. As the reparameterization does not change the
hypothesis space of SSMs, both vanilla SSMs and StableSSM are universal approximators. Vanilla SSMs can only stably approximate
targets with exponential decay while StableSSM can stably approximate any targets with decaying memory.
Approximation Stable approximation
Without reparameterization (Vanilla SSM) Universal (Wang & Xue, 2023) Not universal (Thm 3.3)
With stable reparameterization (StableSSM) Universal (Wang & Xue, 2023) Universal (Thm 3.5)
For example, commonly used reparameterization (Gu et al.,
2022b; Smith et al., 2023) such as f(w) =−ew,f(w) =
−log(1 + ew)are all stable. Verifications are provided in
Remark C.4.
As depicted in Figure 1 (b), state-space models with stable
reparameterization can approximate targets exhibiting poly-
nomial decay in memory. In particular, we prove that under
a simplified perturbation setting (solely perturbing the recur-
rent weights), any linear functional can be stably approxi-
mated by linear RNNs. This finding under simplified setting
is already significant as the instability in learning long-term
memory mainly comes from the recurrent weights.
Theorem 3.5 (Existence of stable approximation by sta-
ble reparameterization) .Foranybounded, causal, contin-
uous, regular, time-homogeneous linear functional H, as-
sume His approximated by a sequence of linear RNNs
{bH(·, θm)}∞
m=1with stable reparameterization, then this
approximation is a stable approximation.
The proof of Theorem 3.5 is in Appendix C.4. The gener-
alization to nonlinear functionals with V olterra-Series rep-
resentation can be similarly achieved (Remark C.5). Com-
pared to Theorem 3.3, Theorem 3.5 underscores the role of
stable reparameterization in achieving stable approximation
of nonlinear functional with long-term memory. Although
vanilla SSM and StableSSM operate within the same hy-
pothesis space, StableSSM demonstrates better stability in
approximating any decaying memory target (Table 1). Incontrast, the vanilla SSM model is limited to stably approxi-
mate targets characterized by an exponential memory decay.
3.3. Optimization benefit of stable reparameterization
In the previous section, the approximation benefit of stable
reparameterizations in SSMs is discussed. Here we study
the impact of different parameterizations on the optimization
stability, in particular, the gradient scales.
As pointed out by Li et al. (2020; 2022), the approximation
of linear functionals using linear RNNs can be reduced into
the approximation of L1-integrable memory function ρ(t)
via functions of the form ˆρ(t) =Pm
i=1cie−λit.
ρ(t)≈mX
i=1cie−λit, λ i>0. (13)
Within this framework, λiis interpreted as the decay mode.
Approaching this from the gradient-based optimization
standpoint, and given that learning rates are shared across
different decay modes, a fitting characterization for “good
parameterization” emerges: The gradient scale across dif-
ferent memory decays modes should be Lipschitz continuous
with respect to the weights scale.
|Gradient |:=∂Loss
∂λi≤L|λi|. (14)
The Lipschitz constant is denoted by L. Without this prop-
erty, the optimization process can be sensitive to the learning
5

--- PAGE 6 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
1.00
 0.75
 0.50
 0.25
 0.00 0.25 0.50 0.75 1.00
real part of eigenvalues 
100101102Bound on output |y| and |dy
d|
 = -1
  = 1
Long term memory
Bound on yc/(1 )
Bound on dy
dc/(1 )2
Figure 2. The scaling of layer output bound |ˆy| ≤c
1−λand the gradients |dˆy
dλ| ≤c
(1−λ)2. The stability boundary is λ=±1. When the
model adapts to learn long-term memory (as λapproaches 1), the gradient experiences an increase that surpasses the rate of output growth.
Techniques like layer normalization are insufficient to address this issue of exploding gradients effectively.
rate. We give a detailed discussion in Appendix D. In the
following theorem, we first characterize the relationship
between gradient norms and recurrent weight parameteriza-
tion.
Theorem 3.6 (Parameterizations influence the gradient
norm scale) .Assume the target functional sequence His
being approximated by a sequence of SSMs bHm. If the
(diagonal) recurrent weight matrix is parameterized via
f:R→R:f(w) =λ.wis the trainable weight while λ
is the eigenvalue of recurrent weight matrix Λ. The gradient
norm Gf(w)of weight wis upper bounded by the following
function:
Gf(w) :=∂Loss
∂w≤CH,bHm|f′(w)|
f(w)2. (15)
Here CH,bHmis independent of the parameterization fpro-
vided that H,bHmare fixed. The discrete-time version is
GD
f(w) :=∂Loss
∂w≤CH,bHm|f′(w)|
(1−f(w))2. (16)
Refer to Appendix C.5 for the proof of Theorem 3.6. In
Appendix E we summarize common reparameterization
methods and corresponding gradient scale functions.
Remark 3.7 (Generalization to multi-layer models) .We do
not prove the gradient bound result for multi-layer case in
the paper, here we discuss the idea to genearlize it: Con-
sider a specific layer in a multi-layer model, without lossof generality we also have the boundedness of result from
the previous layer and expected inputs for the next layer. If
we take the results from previous layer as the inputs and
treat the expected inputs for next layer as the outputs, the
gradient of recurrent weights for this layer also observe the
same gradient norm bound with form in Equation (15). This
comes from the fact that the gradient of the selected layer
remains unchanged, regardless of whether the remaining
layers are frozen or not.
3.4. On the “best” parameterization in stability sense
According to the criterion given in Equation (14), the “best”
stable reparameterization should satisfy the following equa-
tion for some constant L >0.
Gf(w)≤CH,bHm|f′(w)|
f(w)2=L|w|. (17)
Based on the criterion, a sufficient condition for the above
criterion is to find some function fthat satisfies the follow-
ing equation for some real a, b∈R:
f′(w)
f(w)2=d(−1
f(w))
dw= 2aw, (18)
1
f(w)=−(aw2+b) (19)
⇒f(w) =−1
aw2+b. (20)
6

--- PAGE 7 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
1000 1500 2000 2500 3000 3500 4000
Steps102
101
100Max(|gradient|
|weight|)ReLU
Softplus
Exp
Best
T anh
(a) Linear functionals
0 20000 40000 60000 80000 100000 120000
Steps103
102
101
100101102Max(|gradient|
|weight|)ReLU
Softplus
Exp
Best
T anh (b) Language model
Figure 3. In panel (a), in the learning of linear functionals of polynomial decaying memory, the gradient-over-weight scale range
during the training of state-space models. It can be seen the “best”discrete parameterization f(w) = 1−1
w2+0.5achieves the smallest
gradient-over-weight scale. Such property is desirable when a large learning rate is used in training. The “best” reparameterization
f(w) = 1−1
w2+0.5maintains the smallest max(|grad|
|weight|)which is crucial for the training stability. Similar results can be observed in the
language modelling task as in panel (b).
The first equation is achieved by integrating the function
f′(w)
f(w)2. Therefore the “best” parameterization under the
assumption of the Lipschitz property of gradient is charac-
terized by the function with two degrees of freedom: By
stability requirement f(w)≤0for all w
f(w) =−1
aw2+b, a > 0, b≥0. (21)
Similarly, the discrete case gives the solution f(w) =
1−1
aw2+b.The stability of linear RNN further requires
a > 0andb≥0. We choose a= 1, b= 0.5be-
cause this ensures the stability of the hidden state dynamics
and stable approximation in Equation (12). Notice that
limw→01−1
w2+0.5=−1which does not cross the stability
boundary λ=−1. It can be seen in Figure 6 that, compared
with direct and exponential reparameterizations, the softplus
reparameterization is generally milder in this gradient-over-
weight criterion. The “best” parameterization is optimal in
the sense it has a bounded gradient-over-weight ratio across
different weights w(different eigenvalues λ).
Remark 3.8.Apart from the reparameterization method, a
simple yet effective method is gradient clipping. However,
clipped gradient is biased there the training effectiveness
of the gradient descent might be reduced. In contrast, the
reparameterization is changing the scale of the gradient
descent by introducing pre-conditioning termf′(w)
f(w)2.
4. Numerical verifications
Based on the above analyses, we verify the theoretical
statements over synthetic tasks and language models us-ing WikiText-103. The additional numerical details are
provided in Appendix F.
4.1. Synthetic tasks
Linear functionals have a clear structure, allowing us to
study the differences of parameterizations. Similar to Li
et al. (2020) and Wang et al. (2023), we consider linear func-
tional targets Hwith following polynomial memory func-
tionρ(t) =1
(t+1)1.1:yt=Ht(x) =Rt
−∞ρ(t−s)xsds.
We use the state-space models with tanh activations to learn
the sequence relationships. In Figure 3 (a), the eigenvalues
λare initialized to be the same while the only difference is
the reparameterization function f(w). Training loss across
different reparameterization schemes are similar but the
gradient-over-weight ratio across different parameterization
schemes are different in terms of the scale.
4.2. Language models
In addition to the synthetic dataset of linear functionals, we
further justify Theorem 3.6 by examining the gradient-over-
weight ratios for language models using state-space models
(S5). In particular, we adopt the Hyena (Poli et al., 2023)
architecture while the implicit convolution is replaced by a
simple real-weighted state-space model (Smith et al., 2023).
In Figure 4 (a), given the same initialization, we show that
stable reparameterizations such as exponential, softplus,
tanh and “best” exhibit a narrower range of gradient-over-
weight ratios compared to both the direct and relu reparam-
eterizations. Beyond the gradient at the same initialization,
7

--- PAGE 8 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
linearrelu exp
softplustanhinverse
Parameterization107
105
103
101
101Values1e+01
2e+00
2e-031e-038e-03
1e-03
2e-082e-086e-07
6e-082e-05
5e-07Max and Min Gradient/Weight Ratios by Different Parameterization
Max Ratio
Min Ratio
(a) Gradient-weight-ratio at initialization
0 20000 40000 60000 80000 100000 120000
Steps3×1004×1005×100Training lossReLU
Softplus
Exp
Best
T anh (b) Training loss
Figure 4. Language models on WikiText-103. In the left panel (a), we show the gradient-over-weight ratio ranges for different
parameterizations of recurrent weights in state-space models. The eigenvalues λare initialized to be the same while the only difference
is the reparameterization function f. In the right panel (b), the “Best” parameterization is more stable than the ReLU and exponential
reparameterizations. Additional experiments for different learning rates are provided in Figure 7.
Table 2. Comparison of stability of different parameterizations over MNIST. The experiments conducted on the MNIST and CIFAR10
datasets were replicated three times, with the standard deviation of the test loss indicated in parentheses.
LR Direct Softplus Exp Best
5e-6 2.314384 (7.19932e-05) 2.241642 (0.001279) 2.241486 (0.001286) 2.241217 (0.001297)
5e-5 2.304331 (2.11817e-07) 0.779663 (0.001801) 0.774661 (0.001685) 0.765220 (0.001352)
5e-4 2.303190 (1.66387e-06) 0.094411 (0.000028) 0.093418 (0.000024) 0.091924 (0.000019)
5e-3 NaN 0.023795 (0.000004) 0.023820 (0.000003) 0.023475 (0.000002)
5e-2 NaN 0.802772 (1.69448) 0.868350 (1.55032) 0.089073 (0.000774)
5e-1 NaN 2.313510 (0.000014) 2.314244 (0.000025) 2.185477 (0.048238)
5e+0 NaN NaN NaN 199.013813 (50690.6)
Table 3. Comparison of stability of different parameterizations over CIFAR10
LR Direct Softplus Exp Best
5e-6 NaN 1.745752 (0.000006) 1.745816 (0.000009) 1.745290 (0.000011)
5e-5 NaN 1.220859 (0.000008) 1.218064 (0.000008) 1.215510 (0.000014)
5e-4 NaN 0.883649 (0.000898) 0.866817 (0.000328) 0.870412 (0.000442)
5e-3 NaN 1.449352 (0.000414) 1.567662 (0.021489) 1.364697 (0.013849)
5e-2 NaN 1.942372 (0.011317) 1.846173 (0.007990) 1.713892 (0.013426)
5e-1 NaN 37.802437 (3776.6383) 2.296230 (0.000984) 2.554265 (0.168649)
5e+0 NaN 540.621033 (NaN) NaN 615.374522 (30795.4)
in Figure 3 (b), we show the gradient-over-weight ratios
during the training process. The stable reparameterization
will give better gradient-over-weight ratios in the sense that
the “best” stable reparameterization maintains the small-
estmax(|grad|
|weight|). Specifically, as illustrated in Figure 4 (b)
and Figure 7, while training with a large learning rate may
render the exponential parameterization unstable, the “best”
reparameterization f(w) = 1−1
w2+0.5appears to enhance
training stability.4.3. Image classification
Apart from the gradient scale range shown in the language
modeling experiments, we further compare the stability
of different parameterization schemes over different initial
learning rates. As shown in the following Table 2 and Ta-
ble 3, we found that the “best” parameterization can be
trained with a larger learning rates while exp/softplus pa-
rameterizations cannot be trained with larger learning rates
8

--- PAGE 9 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Listops Text Retrieval Image Pathfinder Pathx Avg
Exp parameterization (S4) 59.60 86.82 90.90 88.65 94.2 96.35 86.09
Best parameterization 60.80 88.5 91.3 87.39 94.8 96.1 86.48
Table 4. Comparison of parameterizations on long range arena.
(lr=5.0). Although the models exhibit comparable perfor-
mance at lower learning rates, the “best” parameterization
consistently outperforms others across a range of learning
rates As the training stability issue has been widely reported
for larger models1 2, we believe the improved training
stability is an important component in the scale-up large
language models.
4.4. Long Range Arena
We further verify the effectiveness of stable parameteriza-
tion over the long range arena, as shown in Table 4. Both the
exponential and best parameterizations demonstrate stability,
yet the best parameterization delivers slightly superior av-
erage performance across the long range arena (LRA) (Tay
et al., 2021) benchmark.
5. Related works
RNN RNNs, as introduced by Rumelhart et al. (1986),
represent one of the earliest neural network architectures
for modeling sequential relationships. Empirical findings by
Bengio et al. (1994) have shed light on the challenge of expo-
nential decaying memory in RNNs. Various works (Hochre-
iter & Schmidhuber, 1997; Rusch & Mishra, 2022; Wang
& Yan, 2023) have been done to improve the memory pat-
terns of recurrent models. Theoretical approaches (Li et al.,
2020; 2022; Wang et al., 2023) have been taken to study
the exponential memory decay of RNNs. In this paper, we
study the state-space models which are also recurrent. Our
findings theoretically justify that although SSMs variants
exhibit good numerical performance in long-sequence mod-
eling (Gu et al., 2022b), simple SSMs also suffer from the
“curse of memory”.
SSM State-space models (Siivola & Honkela, 2003), pre-
viously discussed in control theory, has been widely used
to study the dynamics of complex systems. The subse-
quent variants, S4(Gu et al., 2022b), S5 (Smith et al., 2023),
RetNet (Sun et al., 2023) and Mamba (Gu & Dao, 2023),
have significantly enhanced empirical performance. No-
tably, they excel in the long-range arena (Tay et al., 2021),
an area where transformers traditionally underperform. Con-
trary to the initial presumption, our investigations disclose
1https://github.com/state-spaces/mamba/
issues/6
2https://github.com/state-spaces/mamba/
issues/22that the ability to learn long-term memory is not derived
from the linear RNN coupled with nonlinear layer-wise acti-
vations. Rather, our study underscores the benefits of stable
reparameterization in both approximation and optimization.
Fading memory This paper studies the targets with decay-
ing memory. A slightly different memory concept (fading
memory) has been studied in literature (Boyd et al., 1984;
Boyd & Chua, 1985). A critical difference is: fading mem-
ory is defined with respect to a particular weight function
while decaying memory is defined without a specific weight
function. While both concepts are similar in characterizing
the speed of target memory decay, they are still distinct. For
instance, there are examples with decaying memory but not
fading memory (the peak-hold operator introduced in Boyd
& Chua (1985)) and vice versa (examples with fading mem-
ory but not decaying memory are detailed in Appendix A.7
in Wang et al. (2023)).
6. Conclusion
In this paper, we study the intricacies of long-term memory
learning in state-space models, specifically emphasizing the
role of recurrent weights parameterization. We prove that
state-space models without reparameterization fail to stably
approximating targets that exhibit non-exponential decaying
memory. Our analysis indicates this “curse of memory” phe-
nomenon is caused by the eigenvalues of recurrent weight
matrices converging to stability boundary. As an alternative,
we introduce a class of stable reparameterization as a ro-
bust solution to this challenge, which also partially explains
the performance of S4. With stable reparameterization,
state-space models can stably approximate any targets with
decaying memory. We also explore the optimization advan-
tages associated with stable reparameterization, especially
concerning gradient-over-weight scale. Our results give
the theoretical support to observed advantages of reparam-
eterizations in S4 and moreover give principled methods
to design “best” reparameterization scheme in the op-
timization stability sense. This paper shows that stable
reparameterization not only enables the learning of targets
with long-term memory but also enhances the optimization
stability.
9

--- PAGE 10 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Acknowledgements
This research is supported by the National Research Foun-
dation, Singapore, under the NRF fellowship (project No.
NRF-NRFF13-2021-0005). Shida Wang is supported by
NUS-RMI Scholarship.
Impact Statement
This paper study the approximation and optimization prop-
erties of parameterization in state-space models. This paper
presents work whose goal is to advance the field of Machine
Learning. There are minor potential societal consequences
of our work, none which we feel must be specifically high-
lighted here.
References
Bengio, Y ., Simard, P., and Frasconi, P. Learning long-term
dependencies with gradient descent is difficult. IEEE
Transactions on Neural Networks , 5(2):157–166, March
1994. ISSN 1941-0093. doi: 10.1109/72.279181.
Boyd, S. and Chua, L. Fading memory and the problem of
approximating nonlinear operators with V olterra series.
IEEE Transactions on Circuits and Systems , 32(11):1150–
1161, November 1985. ISSN 0098-4094. doi: 10.1109/
TCS.1985.1085649.
Boyd, S., Chua, L. O., and Desoer, C. A. Analytical Foun-
dations of V olterra Series. IMA Journal of Mathematical
Control and Information , 1(3):243–282, January 1984.
ISSN 0265-0754. doi: 10.1093/imamci/1.3.243.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
and Askell, A. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020.
Connor, J. T., Martin, R. D., and Atlas, L. E. Recurrent
neural networks and robust time series prediction. IEEE
transactions on neural networks , 5(2):240–254, 1994.
Gu, A. and Dao, T. Mamba: Linear-Time Sequence Model-
ing with Selective State Spaces, December 2023.
Gu, A., Dao, T., Ermon, S., Rudra, A., and R ´e, C. HiPPO:
Recurrent Memory with Optimal Polynomial Projections.
InAdvances in Neural Information Processing Systems ,
volume 33, pp. 1474–1487. Curran Associates, Inc., 2020.
Gu, A., Goel, K., Gupta, A., and R ´e, C. On the Parameteri-
zation and Initialization of Diagonal State Space Models.
Advances in Neural Information Processing Systems , 35:
35971–35983, December 2022a.Gu, A., Goel, K., and Re, C. Efficiently Modeling Long
Sequences with Structured State Spaces. In International
Conference on Learning Representations , January 2022b.
Hochreiter, S. The Vanishing Gradient Problem Dur-
ing Learning Recurrent Neural Nets and Problem So-
lutions. International Journal of Uncertainty, Fuzzi-
ness and Knowledge-Based Systems , 06(02):107–116,
April 1998. ISSN 0218-4885, 1793-6411. doi: 10.1142/
S0218488598000094.
Hochreiter, S. and Schmidhuber, J. Long Short-term Mem-
ory. Neural computation , 9:1735–80, December 1997.
doi: 10.1162/neco.1997.9.8.1735.
Jiang, H., Li, Q., Li, Z., and Wang, S. A Brief Survey on the
Approximation Theory for Sequence Modelling. Journal
of Machine Learning , 2(1):1–30, June 2023. ISSN 2790-
203X, 2790-2048. doi: 10.4208/jml.221221.
Li, Y ., Wei, C., and Ma, T. Towards explaining the regu-
larization effect of initial large learning rate in training
neural networks. Advances in Neural Information Pro-
cessing Systems , 32, 2019.
Li, Z., Han, J., E, W., and Li, Q. On the Curse of Mem-
ory in Recurrent Neural Networks: Approximation and
Optimization Analysis. In International Conference on
Learning Representations , October 2020.
Li, Z., Han, J., E, W., and Li, Q. Approximation and Opti-
mization Theory for Linear Continuous-Time Recurrent
Neural Networks. Journal of Machine Learning Research ,
23(42):1–85, 2022. ISSN 1533-7928.
Martin, E. and Cundy, C. Parallelizing Linear Recurrent
Neural Nets Over Sequence Length. In International
Conference on Learning Representations , February 2018.
Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
Sentinel Mixture Models. In International Conference
on Learning Representations , 2016.
Orvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith,
S. L. On the universality of linear recurrences followed by
nonlinear projections. arXiv preprint arXiv:2307.11888 ,
2023a.
Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre,
C., Pascanu, R., and De, S. Resurrecting recurrent neu-
ral networks for long sequences. In Proceedings of the
40th International Conference on Machine Learning , vol-
ume 202 of ICML’23 , pp. 26670–26698. JMLR.org, July
2023b.
Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,
S., Cao, H., Cheng, X., Chung, M., Grella, M., GV , K. K.,
et al. RWKV: Reinventing RNNs for the transformer era.
arXiv preprint arXiv:2305.13048 , 2023.
10

--- PAGE 11 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y ., Dao, T., Bac-
cus, S., Bengio, Y ., Ermon, S., and Re, C. Hyena Hier-
archy: Towards Larger Convolutional Language Models.
InInternational Conference on Machine Learning , June
2023.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learn-
ing representations by back-propagating errors. Nature ,
323(6088):533–536, October 1986. ISSN 1476-4687.
doi: 10.1038/323533a0.
Rusch, T. K. and Mishra, S. Coupled Oscillatory Recurrent
Neural Network (coRNN): An accurate and (gradient)
stable architecture for learning long time dependencies.
InInternational Conference on Learning Representations ,
February 2022.
Siivola, V . and Honkela, A. A state-space method for
language modeling. In 2003 IEEE Workshop on Auto-
matic Speech Recognition and Understanding (IEEE Cat.
No.03EX721) , pp. 548–553, St Thomas, VI, USA, 2003.
IEEE. ISBN 978-0-7803-7980-0. doi: 10.1109/ASRU.
2003.1318499.
Smith, J. T. H., Warrington, A., and Linderman, S. Sim-
plified State Space Layers for Sequence Modeling. In
International Conference on Learning Representations ,
February 2023.
Smith, L. N. and Topin, N. Super-convergence: Very fast
training of neural networks using large learning rates. In
Artificial Intelligence and Machine Learning for Multi-
Domain Operations Applications , volume 11006, pp. 369–
386. SPIE, 2019.
Stein, E. M. and Shakarchi, R. Princeton Lectures in Analy-
sis. Princeton University Press Princeton, 2003.
Sun, Y ., Dong, L., Huang, S., Ma, S., Xia, Y ., Xue, J.,
Wang, J., and Wei, F. Retentive network: A successor to
transformer for large language models. arXiv preprint
arXiv:2307.08621 , 2023.
Sutskever, I., Martens, J., and Hinton, G. Generating Text
with Recurrent Neural Networks. In International Con-
ference on Machine Learning , pp. 1017–1024, January
2011.
Tay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham,
P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long
Range Arena : A Benchmark for Efficient Transformers.
InInternational Conference on Learning Representations ,
January 2021.
Tolimieri, R., An, M., and Lu, C. Algorithms for Discrete
Fourier Transform and Convolution . Signal Processing
and Digital Filtering. Springer New York, New York, NY ,
1989. ISBN 978-1-4757-3856-8 978-1-4757-3854-4. doi:
10.1007/978-1-4757-3854-4.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention
is All you Need. In Advances in Neural Information
Processing Systems , volume 30. Curran Associates, Inc.,
2017.
Wang, S. and Xue, B. State-space models with layer-wise
nonlinearity are universal approximators with exponen-
tial decaying memory. In Thirty-Seventh Conference on
Neural Information Processing Systems , November 2023.
Wang, S. and Yan, Z. Improve long-term memory learning
through rescaling the error temporally. arXiv preprint
arXiv:2307.11462 , 2023.
Wang, S., Li, Z., and Li, Q. Inverse Approximation Theory
for Nonlinear Recurrent Neural Networks. In The Twelfth
International Conference on Learning Representations ,
October 2023.
11

--- PAGE 12 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
A. Graphical demonstration of state-space models as stack of Equation (1)
Here we show that Equation (1) corresponds to the practical instantiation of SSM-based models in the following sense: As
shown in Figure 5, any practical instantiation of SSM-based models can be implemented as a stack of Equation (1). The
pointwise shallow MLP can be realized with two-layer state-space models with layer-wise nonlinearity by setting recurrent
weights Wto be 0.
...
......
Figure 5. MLP can be realized by two-layer state-space models. The superscript indicates the layers while the subscript indicates the time
index. It can be seen MLP is equivalent to SSMs having zero recurrent weights W1=W2= 0.
B. Theoretical backgrounds
In this section, we collect the definitions for the theoretical statements.
B.1. Properties of targets
We first introduce the definitions on (sequences of) functionals as discussed in (Wang et al., 2023).
Definition B.1. LetH={Ht:X 7→R;t∈R}be a sequence of functionals.
1. (Linear )Htis linear functional if for any λ, λ′∈Randx,x′∈ X,Ht(λx+λ′x′) =λHt(x) +λ′Ht(x′).
2. (Continuous )Htis continuous functional if for any x,′x∈ X,limx′→x|Ht(x′)−Ht(x)|= 0.
3. (Bounded )Htis bounded functional if the norm of functional ∥Ht∥∞:= sup{x̸=0}|Ht(x)|
∥x∥∞+1+|Ht(0)|<∞.
4.(Time-homogeneous )H={Ht:t∈R}is time-homogeneous (or time-shift-equivariant) if the input-output
relationship commutes with time shift: let [Sτ(x)]t=xt−τbe a shift operator, then H(Sτx) =SτH(x).
5.(Causal )Htis causal functional if it does not depend on future values of the input. That is, if x,x′satisfy xt=x′
tfor
anyt≤t0, then Ht(x) =Ht(x′)for any t≤t0.
6.(Regular )Htis regular functional if for any sequence {x(n):n∈N}such that x(n)
s→0for almost every s∈R,
thenlimn→∞Ht(x(n)) = 0 .
12

--- PAGE 13 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
B.2. Approximation in Sobolev norm
Definition B.2. In sequence modeling as a nonlinear functional approximation problem, we consider the Sobolev norm of
the functional sequence defined as follow:
H−bH
W1,∞= sup
t 
∥Ht−bHt∥∞+dHt
dt−dbHt
dt
∞!
. (22)
HereH={Ht:t∈R}is the target functional sequence to be approximated while the bH={bHt:t∈R}is the model we
use.
In particular, the nonlinear functional operator norm is given by:
∥Ht∥∞:= sup
x̸=0|Ht(x)|
∥x∥∞+ 1+|H(0)|. (23)
AsH(0) = 0 ,∥Ht∥∞is reduced to sup
x̸=0|Ht(x)|
∥x∥∞+ 1. IfHis a linear functional, this definition is compatible with the
common linear functional norm in Equation (37).
We check this operator norm in Equation (23) is indeed a norm: Without loss of generality, we will drop the time index for
brevity.
1. Triangular inequality: For nonlinear functional H1andH2,
∥H1+H2∥∞:= sup
x̸=0|(H1+H2)(x)|
∥x∥∞+ 1(24)
≤sup
x̸=0|H1(x)|
∥x∥∞+ 1+ sup
x̸=0|H2(x)|
∥x∥∞+ 1=∥H1∥∞+∥H2∥∞. (25)
The inequality is by the property of supremum.
2. Absolute homogeneity: For any real constant sand nonlinear functional H
∥sH∥∞:= sup
x̸=0|(sH)(x)|
∥x∥∞+ 1=|s|sup
x̸=0|H(x)|
∥x∥∞+ 1=|s|∥H∥∞. (26)
3.Positive definiteness: If ∥H∥∞= 0, then for all non-zero inputs x̸=0we have H(x) = 0 . AsH(0) = 0 , then we
know His a zero functional.
Property of nonlinear functional sequence norm The definition of functional product is by the element-wise product:
(H1H2)(x) =H1(x)⊙H2(x). As the functional norm satisfies:
∥H1H2∥∞:= sup
x̸=0|H1(x)H2(x)|
∥x∥∞+ 1+|H1(0)H2(0)| (27)
≤sup
x̸=0|H1(x)|
∥x∥∞+ 1|H2(x)|
∥x∥∞+ 1+|H1(0)| · |H2(0)| (28)
≤sup
x̸=0|H1(x)|
∥x∥∞+ 1+|H1(0)|
sup
x̸=0|H2(x)|
∥x∥∞+ 1+|H2(0)|
(29)
=∥H1∥∞∥H2∥∞ (30)
13

--- PAGE 14 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Therefore we have
∥H1H2∥∞= sup
t
∥H1H2∥∞+d(H1H2)
dt
∞
(31)
= sup
t
∥H1H2∥∞+H1dH2
dt
∞+dH1
dtH2
∞
(32)
≤sup
t
∥H1∥∞∥H2∥∞+∥H1∥∞dH2
dt
∞+dH1
dt
∞∥H2∥∞
(33)
≤sup
t
∥H1∥∞+dH1
dt
∞
sup
t
∥H2∥∞+dH2
dt
∞)
(34)
=∥H1∥∞∥H2∥∞ (35)
B.3. Riesz representation theorem for linear functional
Theorem B.3 (Riesz-Markov-Kakutani representation theorem) .Assume H:C0(R,Rd)7→Ris a linear and continuous
functional. Then there exists a unique, vector-valued, regular, countably additive signed measure µonRsuch that
H(x) =Z
Rx⊤
sdµ(s) =dX
i=1Z
Rxs,idµi(s). (36)
In addition, we have the linear functional norm
∥H∥∞:= sup
∥x∥X≤1|H(x)|=∥µ∥1(R) :=X
i|µi|(R). (37)
In particular, this linear functional norm is compatible with the norm considered for nonlinear functionals in Equation (23).
C. Proofs for theorems and lemmas
In Appendix C.1, we show that the nonlinear functionals defined by state-space models are point-wise continuous functionals
at Heaviside inputs. In Appendix C.3, the proof for state-space models’ exponential memory decaying memory property
is given. In Appendix C.4, we prove the linear RNN with stable reparameterization can stably approximate any linear
functional. The target is no longer limited to have an exponenitally decaying memory. The gradient norm estimate of the
recurrent layer is included in Appendix C.5.
C.1. Proof for SSMs are point-wise continuous functionals
Proof. Letxbe any fixed Heaviside input. Assume lim
k→∞∥xk−x∥∞= 0. Lethk,tandhtbe the hidden state for inputs xk
andx. Without loss of generality, assume t >0. The following | · |refers to p=∞norm.
By definition of the hidden states dynamics and triangular inequality, since σ(·)is Lipschitz continuous
d|hk,t−ht|
dt=|σ(Λhk,t+Uxk,t)−σ(Λht+Uxt)| (38)
≤L|Λhk,t+Uxk,t−Λht−Uxt| (39)
=L|Λ(hk,t−ht) +U(xk,t−xt)| (40)
≤L(|Λ||hk,t−ht|+|U||xk,t−xt|). (41)
Here Lis the Lipschitz constant of activation σ. Apply the Gr ¨onwall inequality to the above inequality, we have:
|hk,t−ht| ≤Zt
0eL|Λ|(t−s)L|U| |xk,s−xs|ds. (42)
As the inputs are bounded, by dominated convergence theorem we have right hand side converges to 0 therefore
lim
k→∞|hk,t−ht|= 0,∀t. (43)
14

--- PAGE 15 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Letyk,tandytbe the outputs for inputs xkandx. Therefore we show the point-wise convergence ofdHt
dtatx:
lim
k→∞dyk,t
dt−dyt
dt= lim
k→∞c⊤(dhk,t
dt−dht
dt)(44)
≤lim
k→∞|c|L(|Λ||hk,t−ht|+|U||xk,t−xt|) = 0 . (45)
C.2. Point-wise continuity leads to decaying memory
Here we give the proof of decaying memory based on the point-wise continuity ofdHt
dtand boundedness and time-
homogeneity of H:
Proof.
lim
t→∞dHt
dt(ux)= lim
t→∞dH0
dt(x·1{s≥−t})=dH0
dt(x)= 0.
The first equation comes from time-homogeneity. The second equation is derived from the point-wise continuity where input
xmeans constant xfor all time x=x·1{s≥−∞} . The third equation is based on the boundedness and time-homogeneity as
the output over constant input should be finite and constant Ht(x) =Hs(x)for all s, t. Therefore |dH0
dt(x)|= 0.
C.3. Proof for Theorem 3.3
The main idea of the proof is two-fold. First of all, we show that state-space models with strictly monotone activation is
decaying memory in Lemma C.10. Next, the idea of analysing the memory functions through a transform from [0,∞)to
(0,1]is similar to previous works (Li et al., 2020; 2022; Wang et al., 2023). The remainder of the proof follows a standard
approach, as the derivatives of the hidden states follow the rules of linear dynamical systems when Heaviside inputs are
considered.
Proof. Assume the inputs considered are uniformly bounded by X0:
∥x∥∞< X 0. (46)
Define the derivative of hidden states for unperturbed model to be vm,t=dhm,t
dt. Similarly, ˜vm,tis the derivative of hidden
states for perturbed models ˜vm,t=d˜hm,t
dt.
Since each perturbed model has a decaying memory and the target functional sequence Hhas a stable approximation, by
Lemma C.10, we have
lim
t→∞˜vm,t= 0,∀m. (47)
If the inputs are limited to Heaviside inputs, the derivative ˜vm,tsatisfies the following dynamics: Notice that the hidden
state satisfies ht= 0, t∈(−∞,0],
d˜vm,t
dt=eΛm˜vm,t, t≥0 (48)
˜vm,0=eΛmh0+eUmx0+˜bm=eUmx0+˜bm (49)
⇒˜vm,t=eeΛmt(eUmx0+˜bm). (50)
15

--- PAGE 16 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Notice that the perturbed initial conditions of the ˜vm,tare uniformly (in m) bounded:
˜V0:= sup
m|˜vm,0|2 (51)
= sup
m|eUmx0+˜bm|2 (52)
≤sup
m|eUmx0+˜bm|2 (53)
≤dX0(sup
m∥Um∥2+β0) + sup
m∥bm∥2+β0 (54)
<∞ (55)
Here dis the input sequence dimension.
Similarly, the unperturbed initial conditions satisfy:
V0:= sup
m|˜vm,0|2 (56)
= sup
m|Umx0+bm|2 (57)
≤sup
m|Umx0+bm|2 (58)
≤dX0sup
m∥Um∥2+ sup
m∥bm∥2 (59)
≤(dX0+ 1)θmax (60)
<∞ (61)
Select a sequence of perturbed recurrent matrices {eΛm,k}∞
k=1satisfying the following two properties:
1.eΛm,kis Hyperbolic, which means the real part of the eigenvalues of the matrix are nonzero.
2.limk→∞(eΛm,k−Λm) =β0Im.
Moreover, by Lemma C.11, we know that each hyperbolic matrix eΛm,kis Hurwitz as the system for ˜vm,tis asymptotically
stable.
sup
mmax
i∈[m](λi(eΛm,k))<0. (62)
This is the stability boundary for the state-space models under perturbations.
Therefore the original diagonal unperturbed recurrent weight matrix Λmsatisfies the following eigenvalue inequality
uniformly inm. Since Λmis diagonal:
sup
mmax
i∈[m](λi(Λm))≤ −β0. (63)
16

--- PAGE 17 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Therefore the model memory decays exponentially uniformly
M(bHm)(t) := sup
X01
X0+ 1d
dtˆym,t(64)
= sup
X01
X0+ 1|c⊤
m[σ′(hm,t)◦vm,t]| (65)
≤sup
X01
X0+ 1|cm|2|σ′(hm,t)◦vm,t|2 (66)
≤sup
X01
X0+ 1|cm|2·sup
z|σ′(z)| · |e−β0tvm,0|2 (67)
≤sup
X01
X0+ 1
sup
m|cm|2·sup
z|σ′(z)| ·V0
e−β0t(68)
≤sup
X01
X0+ 1
sup
m|cm|2·L0·V0
e−β0t(69)
≤sup
X0
sup
m|cm|2·L0 (70)
·X0
X0+ 1d(sup
m∥Um∥2) +1
X0+ 1(sup
m∥bm∥2)
e−β0t(71)
≤
sup
m|cm|2·L0
dsup
m∥Um∥2+ sup
m∥bm∥2
e−β0t(72)
≤(d+ 1)L0θ2
maxe−β0t(73)
The inequalities are based on vector norm properties, Lipschitz continuity of σ(z)and uniform boundedness of unperturbed
initial conditions. Therefore we know the model memories are uniformly decaying.
By Lemma C.12, the target Hhas an exponentially decaying memory as it is approximated by a sequence of models
{bHm}∞
m=1with uniformly exponentially decaying memory.
Remark C.1.When the approximation is unstable, we cannot have the real parts of the eigenvalues for recurrent weights
bounded away from 0 in Equation (63). As the stability of linear RNNs requires the real parts (of the eigenvalues) to be
negative, then the maximum of the real parts will converge to 0. This is the stability boundary of state-space models.
lim
m→∞max
i∈[m](λi(Λm)) = 0−. (74)
Remark C.2.The uniform weights bound is necessary in the sense that: Since state-space models are universal approximators,
they can approximate targets with long-term memories. However, if the target has an non-exponential decaying (e.g.
polynomial decaying) memory, the weights bound of the approximation sequence will be exponential in the sequence length
T.
θ2
max≥eβ0TM(H)(T)
(d+ 1)L0. (75)
This result indicates that scaling up SSMs without reparameterization is inefficient in learning sequence relationships with a
large Tand long-term memory.
Remark C.3 ( On the generalization to multi-layer cases ).We will use the following two-layer state-space models to
demonstrate the idea to generalize this result to multi-layer cases.
dht
dt= Λ 1ht+U1xt (76)
yt=σ(ht) (77)
dst
dt= Λ 2st+U2yt (78)
ˆzt=c⊤σ(st) (79)
17

--- PAGE 18 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
We can have the following memory function bounds: For simplicity, we drop the term minΛ1,Λ2, U1, U2.
M(bHm)(t) := sup
X01
X0+ 1d
dtˆzm,t
2(80)
= sup
X01
X0+ 1c⊤
σ′(sm,t)◦dsm,t
dt
2(81)
= sup
X01
X0+ 1c⊤
σ′(sm,t)◦Zt
0eΛ2t1U2dym,t−t1
dtdt1
2(82)
= sup
X01
X0+ 1c⊤
σ′(sm,t)◦Zt
0eΛ2t1U2(σ′(hm,t−t1)◦vm,t−t1)dt1
2(83)
= sup
X01
X0+ 1c⊤
σ′(sm,t)◦Zt
0eΛ2t1U2(σ′(hm,t−t1)◦eΛ1(t−t1)vm,0)dt1
2(84)
≤sup
X01
X0+ 1|c|2|σ′(sm,t)|2Zt
0|eΛ2t1|2|U2|2(|σ′(hm,t−t1)|2|eΛ1(t−t1)|2V0)dt1 (85)
≤L2
0θ2
maxsup
X01
X0+ 1Zt
0|eΛ2t1|2|eΛ1(t−t1)|2V0dt1 (86)
≤L2
0θ3
maxsup
X0(dX0+ 1)
X0+ 1Zt
0|eΛ2t1|2|eΛ1(t−t1)|2dt1 (87)
≤L2
0θ3
maxsup
X0(dX0+ 1)
X0+ 1Zt
0|e−β0t1|2|e−β0(t−t1)|2dt1 (88)
≤(d+ 1)L2
0θ3
maxte−β0t. (89)
The first inequality comes from the Cauchy inequality ( |a◦b|2≤ |a|2·|b|2). The second inequality comes from the property
of activation σ(·)and uniform bound on weights. The third inequality comes from the bound of V0in Equation (56). The
last inequality is the direct evaluation based on the eigenvalues of Λ1andΛ2. As here is a fast decaying term e−β0t, we
simplify other polynomial scale components in P.
A further generalization of the memory function for ℓ-layer SSMs would be: For some polynomial P(t)with degree at most
l−1
M(bHm)(t)≤(d+ 1)Lℓ
0θℓ+1
maxP(t)e−β0t. (90)
C.4. Proof for Theorem 3.5
Proof. Let the target linear functional be Ht(x) =Rt
−∞ρ(t−s)xsds. Here ρis anL1integrable function. We consider a
simplified model setting with only parameters candw. Letci, wibe the unperturbed weights and ˜ci,˜wibe the perturbed
recurrent weights. Similar to ρbeing L1integrable, we note thatR∞
0|cief(wi)t|dt=|ci|
|f(wi)|. To have a sequence of
well-defined model, we require they are uniformly (in m) absolutely integrable:
sup
mmX
i=1|ci|
|f(wi)|<∞,sup
mmX
i=11
|f(wi)|<∞. (91)
18

--- PAGE 19 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Based |˜w−w|2≤βand|˜c−c|2≤β. We know the approximation error is
Em(β) = sup
|˜w−w|2≤β,|˜c−c|2≤βZ∞
0mX
i=1˜cief( ˜wi)t−ρ(t)dt (92)
≤ sup
|˜w−w|2≤βZ∞
0mX
i=1cief(wi)t−ρ(t)dt (93)
+ sup
|˜w−w|2≤βZ∞
0mX
i=1cief( ˜wi)t−mX
i=1cief(wi)tdt (94)
+ sup
|˜w−w|2≤β,|˜c−c|2≤βZ∞
0mX
i=1(˜ci−ci)ef( ˜wi)tdt (95)
≤ sup
|˜w−w|2≤βZ∞
0mX
i=1cief(wi)t−ρ(t)dt (96)
+ sup
|˜w−w|2≤βZ∞
0mX
i=1cief( ˜wi)t−mX
i=1cief(wi)tdt (97)
+ sup
|˜w−w|2≤β,|˜c−c|2≤βZ∞
0mX
i=1β|ef( ˜wi)t−ef(wi)t+ef(wi)t|dt (98)
≤Em(0) + sup
|˜w−w|2≤βZ∞
0mX
i=1|ci|ef( ˜wi)t−ef(wi)tdt (99)
+ sup
|˜w−w|2≤β,|˜c−c|2≤βZ∞
0βmX
i=1|ef( ˜wi)t−ef(wi)t|dt+Z∞
0βmX
i=1ef(wi)tdt (100)
=Em(0) + sup
|˜w−w|2≤βZ∞
0mX
i=1(|ci|+β)ef( ˜wi)t−ef(wi)tdt (101)
+Z∞
0βmX
i=1ef(wi)tdt (102)
=Em(0) +mX
i=1(|ci|+β) sup
|˜w−w|2≤βZ∞
0ef( ˜wi)t−ef(wi)tdt+Z∞
0βmX
i=1ef(wi)tdt (103)
=Em(0) +mX
i=1(|ci|+β) sup
|˜wi−wi|≤βZ∞
0ef( ˜wi)t−ef(wi)tdt+βmX
i=11
|f(wi)|(104)
≤Em(0) +mX
i=1(|ci|+β)g(β)
|f(wi)|+βmX
i=11
|f(wi)|(105)
=Em(0) +mX
i=1g(β)(|ci|+β) +β
|f(wi)|. (106)
The first and third inequalities are triangular inequality. The second inequality comes from the fact that |˜wi−wi| ≤
|˜w−w|2≤β. The fourth inequality is achieved via the property of stable reparameterization: For some continuous function
g(β) : [0,∞)→[0,∞), g(0) = 0 :
sup
w"
|f(w)|sup
|˜w−w|≤βZ∞
0ef( ˜w)t−ef(w)tdt#
≤g(β). (107)
By definition of stable approximation, we know limm→∞Em(0) = 0 . Also according to the requirement of the stable
19

--- PAGE 20 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
approximation in Equation (91), we have
lim
β→0E(β) = lim
β→0lim
m→∞Em(β) (108)
≤lim
β→0lim
m→∞Em(0) + 
sup
mmX
i=1|ci|+β
|f(wi)|!
∗lim
β→0g(β) + lim
β→0β∗ 
sup
mmX
i=11
|f(wi)|!
(109)
= 0 + 0 + 0 = 0 = E(0). (110)
Remark C.4.Here we verify the reparameterization methods satisfy the definition of stable reparameterization.
For exponential reparameterization f(w) =−ew, w∈R:
sup
|˜w−w|≤βZ∞
0ef( ˜w)t−ef(w)tdt=eβ−1
|f(w)|. (111)
For softplus reparameterization f(w) = −log(1 + ew), w∈R: Notice that exp(−β) log(1 + exp( w))≤
sup|˜w−w|≤βlog(1 + exp( ˜ w))≤exp(β) log(1 + exp( w)),
sup
|˜w−w|≤βZ∞
0ef( ˜w)t−ef(w)tdt≤eβ−1
|f(w)|. (112)
For “best” reparameterization f(w) =−1
aw2+b, w∈R, a, b > 0: Without loss of generality, let w≥0
sup
|˜w−w|≤βZ∞
0ef( ˜w)t−ef(w)tdt=|a(w+β)2−aw2| (113)
≤a(β2+2βw)
aw2+b
|f(w)|(114)
≤a(β2+2βw)
b
|f(w)|. (115)
Hereg(β) =a(β2+2βw)
b. The famous M ¨untz–Sz ´asz theorem indicates that selecting any non-zero constant adoes not affect
the universality of linear RNN.
While for the case without reparameterization f(w) =w, w < 0: For 0≤β <−w,
sup
|˜w−w|≤βZ∞
0ef( ˜w)t−ef(w)tdt=β
(−w−β)(−w)=β
(−w−β)|f(w)|, (116)
Here limw→−βsupwβ
−w−β=∞, therefore the direct parameterization is not a stable reparameterization.
Remark C.5 (On the generalization of existence of stable approximation to nonlinear functionals) .The previous results are
established for the stable approximation of linear functionals by linear RNNs with stable approximations.
Here we show that this can be further extended to nonlinear functionals. According to the V olterra Series representation, the
nonlinear functional has expansion by multi-layer composition or element-wise product (Wang & Xue, 2023). Therefore if
the existence of stable approximation is preserved for functional composition and polynomial, then we can generalize the
above argument to the nonlinear functionals by working with nonlinear functional representations.
Theorem C.6 (Boyd et al. (1984); Wang & Xue (2023)) . For any continuous time-invariant system with x(t)as input and
y(t)as output can be expanded in the Volterra series as follow
y(t) =ρ0+NX
n=1Zt
0···Zt
0ρn(τ1, . . . , τ n)nY
j=1x(t−τj)dτj. (117)
In particular, we call the expansion order Nto be the series’ order.
20

--- PAGE 21 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Lemma C.7 (Stable approximation induced by polynomials of stable approximation) . Assume H1andH2can be stably
approximated, let fbe some polynomial, then f(H1,H2)can also be stably approximated.
Proof. Letf(H1,H2) =P
i,jci,jHi
1Hj
2. The definition of functional product is by the element-wise product:
(H1H2)(x) =H1(x)⊙H2(x).
Em(β) = sup
|˜θ−θ|≤β∥f(H1,H2)−f(H1(˜θ),H2(˜θ))∥W1,∞ (118)
≤Em(0) + sup
|˜θ−θ|≤β∥f(H1(θ),H2(θ))−f(H1(˜θ),H2(˜θ))∥W1,∞ (119)
≤Em(0) + sup
|˜θ−θ|≤β∥f(H1(θ),H2(θ))−f(H1(θ),H2(˜θ))∥W1,∞ (120)
+ sup
|˜θ−θ|≤β∥f(H1(θ),H2(˜θ))−f(H1(˜θ),H2(˜θ))∥W1,∞ (121)
≤Em(0) +X
i≥0,j≥1ci,jj∥bH1(θ)∥i
W1,∞(∥bH2(θ)∥W1,∞+EH2
m(β))j−1EH2
m(β) (122)
+X
i≥1,j≥0ci,ji(∥bH1(θ)∥W1,∞+EH1
m(β))i−1∥bH2(θ)∥j
W1,∞EH1
m(β). (123)
Therefore E(β)≤limm→∞Em(β)<∞. The third inequality comes from Equation (31).
C.5. Proof for Theorem 3.6
Proof. For any 1≤j≤m, assume the loss function we used is the L∞norm: Loss = supt∥Ht−bHm,t∥∞. Notice that
by time-homogeneity, Loss =∥Ht−bHm,t∥∞for any t. This loss function is larger than the common mean squared error,
which is usually chosen in practice for the smoothness reason.
∂Loss
∂wj=∂∥Ht−bHm,t∥∞
∂wj(124)
=∂sup∥x∥∞≤1|Ht(x)−bHm,t(x)|
∂wj(125)
=∂sup∥x∥∞≤1|Rt
−∞(ρ(t−s)−Pm
i=1cie−f(wi)(t−s))xsds|
∂wj(126)
=∂Rt
−∞|ρ(t−s)−Pm
i=1cie−f(wi)(t−s)|ds
∂wj(127)
=∂Rt
−∞|(ρ(t−s)−P
i̸=jcie−f(wi)(t−s))−cje−f(wj)(t−s)|ds
∂wj(128)
=∂R∞
0|(ρ(s)−P
i̸=jcie−f(wi)s)−cje−f(wj)s|ds
∂wj(129)
≤Z∞
0∂|(ρ(s)−P
i̸=jcie−f(wi)s)−cje−f(wj)s|
∂wjds (130)
≤Z∞
0∂|cje−f(wj)s|
∂wjds (131)
The first equality is the definition of the loss function. The second equality equality comes from the definition of the linear
functional norm. The third equality expand the linear functional and linear RNNs into the convolution form. The fourth
21

--- PAGE 22 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
equality utilize the fact that we can manually select xt’s sign to achieve the maximum value. The fifth equality is separating
the term in dependent of variable wj. The sixth equality is change of variable from t−stos. The inequality is triangular
inequality. The last equality is dropping the term independent of variable wj.
∂Loss
∂wj≤Z∞
0∂|cje−f(wj)s|
∂wjds (132)
=|cjf′(wj)|Z∞
0e−f(wj)ss ds (133)
=cjf′(wj)
f(wj)Z∞
0e−f(wj)sds (134)
=cjf′(wj)
f(wj)2(1−lim
s→∞e−f(wj)s) =cjf′(wj)
f(wj)2. (135)
The first equality is evaluating the derivative. The second equality is extracting |f′(w)|from integral. The third equality is
doing the integration by parts.
In particular, notice that cjis a constant independent of the recurrent weight parameterization f:
bHm,t(x) =Zt
−∞mX
i=1cie−f(wi)(t−s)xsds. (136)
Therefore cjis a parameterization indepndent value, we will denote it by CH,bHm.
Moreover, in the discrete setting, assume hk+1=f(w)◦hk+Uxk,
∂Loss
∂wj≤∞X
k=0∂|cjf(wj)k|
∂wjds (137)
=|cjf′(wj)|∞X
k=1kf(wj)k−1(138)
=|cjf′(wj)| ∞X
k=1f(wj)k−1!2
(139)
=cjf′(wj)
(1−f(wj))2. (140)
So the gradient norm is bounded by∂Loss
∂wj=|cjf′(wj)|
(1−f(wj))2. (141)
Nonlinear functionals Now we show the generalization into the nonlinear functional: Consider the V olterra Series
representation of the nonlinear functional.
Theorem C.8 ((Boyd et al., 1984)) .For any continuous time-invariant system with x(t)as input and y(t)as output can be
expanded in the Volterra series as follow
y(t) =h0+NX
n=1Zt
0···Zt
0hn(τ1, . . . , τ n)nY
j=1x(t−τj)dτj. (142)
Here Nis the series’ order. Linear functional is an order-1 Volterra series.
22

--- PAGE 23 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
For simplicity, we will only discuss the case for N= 2. When we take the Hyena approach (Poli et al., 2023) and
approximate the order-2 kernel h2(τ1, τ2)with its rank-1 approximation:
h2(τ1, τ2) =h2,1(τ1)h2,2(τ2). (143)
Hereh2,1andh2,2are again order-1 kernel which can be approximated with linear RNN’s kernel. In other words, the same
gradient bound also holds for general nonlinear functional with the following form:
Gf(w) :=∂E
∂w=CH,bHm|f′(w)|
f(w)2. (144)
And the discrete version is
GD
f(w) :=∂E
∂w=CH,bHm|f′(w)|
(1−f(w))2. (145)
C.6. Lemmas
Lemma C.9. If the activation σ(·)is bounded, strictly increasing, continuously differentiable function over R. Then for all
C >0, there exists ϵCsuch that ∀|z| ≤Cϵ,|σ′(z)| ≥ϵC.
Proof. Since σ(·)is monotonically increasing, therefore σ′(·)>0,∀z≥0. Notice that σ′(·)is continuous, for any C >0,
we know1
2min|z|≤Cσ′(z)>0. Define ϵC:=1
2min|z|≤Cσ′(z)>0, it can be seen the target statement is satisfied.
Lemma C.10. Assume the target functional sequence has a β0-stable approximation and the perturbed model has a
decaying memory, we show that ˜vm,t→0for all m.
Proof. For any m, fixeΛmandeUm. Since the perturbed model has a decaying memory,
lim
t→∞d
dteHm(ux)= lim
t→∞c⊤(σ′(˜hm,t)◦d˜hm,t
dt)= lim
t→∞c⊤(σ′(˜hm,t)◦˜vm,t)= 0. (146)
By linear algebra, there exist mvectors {∆ci}m
i=1,|∆ci|∞< β such that cm+ ∆c1, . . . , cm+ ∆cmform a basis of Rm.
We can then decompose any vector uinto
u=k1(cm+ ∆c1) +···+km(cm+ ∆cm). (147)
Take the inner product of uand˜vm,t, we have
lim
t→∞u⊤(σ′(˜hm,t)◦˜vm,t) =mX
i=1kilim
t→∞(cm+ ∆ci)⊤(σ′(˜hm,t)◦˜vm,t) = 0 (148)
As the above result holds for any vector u, we get
lim
t→∞σ′(˜hm,t)◦˜vm,t
∞= 0. (149)
As required in Equation (9), the hidden states are uniformly (in m) bounded over bounded input sequence. There exists
constant C0>0such that
sup
m,t|hm,t|∞< C 0. (150)
Since σis continuously differentiable and strictly increasing, by Lemma C.9, there exists ϵC0>0such that
|σ′(z)|> ϵC0,∀|z| ≤C0. (151)
Therefore
sup
tσ′(˜hm,t)
∞> ϵC0. (152)
We get
lim
t→∞|˜vm,t|∞= 0. (153)
23

--- PAGE 24 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Lemma C.11. Consider a dynamical system with the following dynamics: h0= 0
dvt
dt= Λvt,
v0= Λh0+eUx0+˜b=eUx0+˜b.(154)
IfΛ∈Rm×mis diagonal, hyperbolic and the system in Equation (154) is satisfies limt→∞vt= 0 over any bounded
Heaviside input ux0,|x0|∞<∞, then the matrix Λis Hurwitz.
Proof. By integration we have the following explicit form:
vt=eΛtv0=eΛt(eUx0+˜b). (155)
The stability requires lim
t→∞|vt|= 0 for all inputs v0=eUx0+˜b. Notice that with perturbation from ˜Uand˜b, the set of
initial points {v0}is m-dimensional. Therefore the matrix Λis Hurwitz in the sense that all eigenvalues’ real parts are
negative.
Lemma C.12. Consider a continuous function f: [0,∞)→R, assume it can be approximated by a sequence of continuous
functions {fm}∞
m=1universally:
lim
m→∞sup
t≥0|f(t)−fm(t)|= 0. (156)
Assume the approximators fmare uniformly exponentially decaying with the same β0>0:
lim
t→∞sup
m∈N+eβ0t|fm(t)| →0. (157)
Then the function fis also decaying exponentially:
lim
t→∞eβt|f(t)| →0,∀0< β < β 0. (158)
The proof is the same as Lemma A.11 from (Wang et al., 2023). For completeness purpose, we attach the proof here:
Proof. Given a function f∈C([0,∞)), we consider the transformation Tf: [0,1]→Rdefined as:
(Tf)(s) =(
0, s = 0
f(−logs
β0)
s, s ∈(0,1].(159)
Under the change of variables s=e−β0t, we have:
f(t) =e−β0t(Tf)(e−β0t), t≥0. (160)
According to uniformly exponentially decaying assumptions on fm:
lim
s→0+(Tfm)(s) = lim
t→∞fm(t)
e−β0t= lim
t→∞eβ0tfm(t) = 0 , (161)
which implies Tfm∈C([0,1]).
For any β < β 0, letδ=β0−β >0. Next we have the following estimate
sup
s∈[0,1]|(Tfm1)(s)−(Tfm2)(s)| (162)
= sup
t≥0fm1(t)
e−βt−fm2(t)
e−βt(163)
≤max
sup
0≤t≤T0fm1(t)
e−βt−fm2(t)
e−βt, C0e−δT0
(164)
≤max
eβT0sup
0≤t≤T0|fm1(t)−fm2(t)|, C0e−δT0
(165)
24

--- PAGE 25 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
where C0is a constant uniform in m.
For any ϵ >0, take T0=−ln(ϵ
C0)
δ,we have C0e−δT0≤ϵ. For sufficiently large Mwhich depends on ϵandT0, by universal
approximation (Equation (156)), we have ∀m1, m2≥M,
sup
0≤t≤T0|fm1(t)−fm2(t)| ≤e−βT0ϵ, (166)
eβT0sup
0≤t≤T0|fm1(t)−fm2(t)| ≤ϵ. (167)
Therefore, {fm}is a Cauchy sequence in C([0,∞)).
Since{fm}is a Cauchy sequence in C([0,∞))equipped with the sup-norm, using the above estimate we can have {Tfm}
is a Cauchy sequence in C([0,1])equipped with the sup-norm. By the completeness of C([0,1]), there exists f∗∈C([0,1])
withf∗(0) = 0 such that
lim
m→∞sup
s∈[0,1]|(Tfm)(s)−f∗(s)|= 0. (168)
Given any s >0, we have
f∗(s) = lim
m→∞(Tfm)(s) = (Tf)(s), (169)
hence
lim
t→∞eβtf(t) = lim
s→0+(Tf)(s) =f∗(0) = 0 . (170)
D. Motivation for the gradient-over-weight Lipschitz criterion
Here we discuss the motivation for adopting the gradient-over-weight boundedness as the criterion for “best-in-stability”
reparameterization. First of all, the “best” reparameterization is proposed to further improve the optimization stability across
memory patterns with different decays. The criterion “gradient is Lipschitz to the weight” is a necessary condition for the
stability in the following sense:
1.Consider functions f(x) =x4, the gradient functiond f
dx(x) = 4 x3does not have a global Lipschitz coefficient
for all input values x. Therefore for any fixed positive learning rate η, there exists an initial point x0(for example
x0=q
1
2η+ 1) such that the convergence from initial point x0cannot be achieved via the gradient descent step
xk+1=xk−ηg(xk). (171)
It can be verified the convergence does not hold as |xk+1|>|xk|for all kwhen x0=q
1
2η+ 1. This comes from the
fact that |xk| ≥q
1
2η, ηg(x)≥2xkhold for all k.
2.Consider functions f(x) =x2, the gradient function g(x) = 2 xis associated with a Lipschitz constant L= 2. Then
the same gradient descent step converges for any η≤1
2in Equation (171).
3.As can be seen in the above two examples, the criterion “gradient is Lipschitz to the weight” is associated with the
convergence under large learning rate. As the use of larger learning rate is usually associated with faster convergence
(Smith & Topin, 2019), smaller generalization errors (Li et al., 2019), we believe the Lipschitz criterion is a suitable
stability criterion for the measure of optimization stability.
4.The gradient-over-weight ratio evaluated in Figure 4(a) is a numerical verification of our Theorem 3.4. The gradients of
stable reparameterizations are less susceptible to the well-known issue of exploding or vanishing gradients (Bengio
et al., 1994; Hochreiter, 1998).
E. Comparison of different recurrent weights parameterization schemes
Here we evaluate the gradient norm bound function GfandGD
ffor different parameterization schemes in Table 5 and
Figure 6.
25

--- PAGE 26 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
Table 5. Summary of reparameterizations and corresponding gradient norm functions in continuous and discrete time. Notice that the Gf
andGD
fare rescaled up to a constant CH,bH.
Reparameteriations f GforGD
f
Continuous ReLU −ReLU (w)1
w21{w>0}
Exp −exp(w) e−w
Softplus −log(1 + exp( w))exp(w)
(1+exp( w)) log(1+exp( w))2
“Best”(Ours) −1
aw2+b, a > 0, b > 0 2a|w|
Discrete ReLU exp(−ReLU (w))exp(−w)
(1−exp(−w))21{w>0}
Exp exp(−exp(w))exp(w−exp(w))
(1−exp(−exp(w)))2
Softplus1
1+exp( w)e−w
Tanh tanh( w) =e2w−1
e2w+1e2w
“Best”(Ours) 1−1
w2+0.5∈(−1,1) 2|w|
101
100
0100101
Weight w1010
107
104
101
102105108Gradient-weight ratio at weight w: Gf(w)
|w|
linear, =w,w>0
exponential, =exp(w),w
softplus, =log(1+exp(w)),w
best, =1
w2,w
(a) Continuous timeGf(w)
|w|
101
100
0100101
Weight w1010
107
104
101
102105108Gradient-weight ratio at weight w: Gf(w)
|w|(b) Discrete timeGD
f(w)
|w|
Figure 6. Gradient norm function GfandGD
fof different parameterization methods. The “best” parameterization methods maintain a
balanced gradient-over-weight ratio.
On the Scenarios Where “Best” Parameterization is Preferable There is no guarantee that the “best” parameterization
will outperform the Exp/Softplus parameterizations when all models exhibit good training stability. When the learning
rate has been finetuned (at 5e-4) for CIFAR10, the optimal performance from “best” parameterization is worse than exp
parameterization. This outcome is expected since this paper focuses on training stability rather than generalization. The key
insight from Tables 1 and 2 is that the “best” parameterization offers a theoretically grounded alternative to the exp/softplus
parameterizations.
F. Numerical details
In this section, the details of numerical experiments are provided for the completeness and reproducibility.
F.1. Synthetic task
We conduct the approximation of linear functional with linear RNNs in the one-dimensional input and one-dimensional
output case. The synthetic linear functional is constructed with the polynomial decaying memory function is ρ(t) =1
(t+1)1.1.
Sequence length is 100. Total number of synthetic samples is 153600. The learning rate used is 0.01 and the batch size is
512.
The perturbation list β∈[0,10−3,10−3∗21/2,10−3∗22/2, . . . , 10−3∗220/2]. Each evaluation of the perturbed error is
26

--- PAGE 27 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
0 20000 40000 60000 80000 100000 120000
Steps3×1004×1005×100Training lossReLU
Softplus
Exp
Best
T anh
(a) lr=0.002, “best” reparameterization is also not optimal, but the
final loss is comparable against Exp and Softplus
0 20000 40000 60000 80000 100000 120000
Steps4×1005×100Training lossReLU
Softplus
Exp
Best
T anh
(b) lr=0.01, “best” reparameterization achieve the smallest loss
Figure 7. The stability advantage of “best” reparameterization (red line) is usually better when the learning rate is larger.
sampled with 30 different weight perturbations to reduce the variance.
F.2. Language models
The language modeling is done over WikiText-103 dataset (Merity et al., 2016). The model we used is based on the Hyena
architecture with simple real-weights state-space models as the mixer (Poli et al., 2023; Smith et al., 2023). The batch size
is 16, total steps 115200 (around 16 epochs), warmup steps 1000. The optimizer used is AdamW and the weight decay
coefficient is 0.25. The learning rate for the recurrent layer is 0.004 while the learning rate for other layers are 0.005.
In the main paper, we provide the training loss curve for learning rate = 0.005 as the stability of “best” discrete-time
parameterization f(w) = 1−1
w2+0.5is mostly significant as the learning rate is large. In Figure 7, we further provide the
results for other learning rates (lr = 0.002, 0.010). Despite the final loss not being optimal for the “best” reparameterization,
it is observed that the training process exhibits enhanced stability compared to other parameterization methods.
F.3. On the stability of “best” reparameterization for large models
The previous experiment on WikiText-103 language modelling shows the performance of stable reparameterization over the
unstable cases. We further verify the optimization stability of “best” reparameterization in the following extreme setting.
27

--- PAGE 28 ---
StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization
We construct a large scale language model with 3B parameters and train with larger learning rate (lr=0.01). As can be
seen in the following table, the only convergent model is the model with “best” reparameterization. We emphasize that
the only difference between these models are the parameterization schemes for recurrent weights. Therefore the best
reparameterization is the most stable parameterization. (We repeats the experiments with different seeds for three times.)
“Best” Exp Softplus Direct
Convergent / total experiments 3/3 0/3 0/3 0/3
Table 6. Experiment to the stability of “best” reparameterization over lr = 0.01. All other reparameterizations diverged within 100 steps
while the “best” reparameterizations can be used to train the model.
F.4. Additional numerical results for associative recalls
In this section, we study the performance of of different stable reparameterizations over the extremely long sequences (up to
131k). It can be seen in Table 7 that stable parameterizations are better than the case without reparameterization and simple
clipping. The advantage is more significant when the sequence length is longer. The models are trained under the exactly
same hyperparameters.
Reparameterizations Train acc, T=20 Test acc, T=20 Train acc, T=131k Test acc, T=131k
“Best” 57.95 99.8 53.57 100
Exp(S5) 54.55 99.8 53.57 100
Clip 50.0 76.6 13.91 9.4
Direct 43.18 67.0 16.59 5.6
Table 7. Comparison of parameterizations on associative recalls. The first two columns are the train and test accuracy over sequence
length 20 , vocabulary size 10, while the second two columns are the train and test accuracy over sequence length 131k and vocabulary
size 30.
28
