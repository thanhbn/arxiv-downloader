# 2311.14495.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/ssm/2311.14495.pdf
# Kích thước file: 976090 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
StableSSM: Giảm thiểu Lời nguyền Bộ nhớ trong Mô hình Không gian Trạng thái thông qua Tái tham số hóa Ổn định

Shida Wang1 Qianxiao Li1 2

Tóm tắt
Trong bài báo này, chúng tôi nghiên cứu khả năng học bộ nhớ dài hạn của các mô hình không gian trạng thái (SSMs) từ góc độ tham số hóa. Chúng tôi chứng minh rằng các mô hình không gian trạng thái không có bất kỳ tái tham số hóa nào đều thể hiện giới hạn bộ nhớ tương tự như các RNN truyền thống: các mối quan hệ mục tiêu có thể được xấp xỉ ổn định bởi các mô hình không gian trạng thái phải có bộ nhớ suy giảm theo hàm mũ. Phân tích của chúng tôi xác định "lời nguyền bộ nhớ" này là kết quả của các trọng số tuần hoàn hội tụ về biên giới ổn định, gợi ý rằng kỹ thuật tái tham số hóa có thể hiệu quả. Để đạt được điều này, chúng tôi giới thiệu một lớp kỹ thuật tái tham số hóa cho SSMs có thể nâng cao hiệu quả các giới hạn bộ nhớ của chúng. Ngoài việc cải thiện khả năng xấp xỉ, chúng tôi tiếp tục minh họa rằng việc lựa chọn có nguyên tắc của sơ đồ tái tham số hóa cũng có thể tăng cường tính ổn định tối ưu hóa. Chúng tôi xác thực các phát hiện của mình sử dụng các bộ dữ liệu tổng hợp, mô hình ngôn ngữ và phân loại hình ảnh.

1. Giới thiệu
Hiểu các mối quan hệ bộ nhớ dài hạn là nền tảng trong mô hình hóa chuỗi. Việc nắm bắt bộ nhớ kéo dài này là quan trọng, đặc biệt trong các ứng dụng như dự đoán chuỗi thời gian (Connor et al., 1994), mô hình ngôn ngữ (Sutskever et al., 2011). Kể từ khi xuất hiện, transformers (Vaswani et al., 2017) đã trở thành mô hình được lựa chọn cho các nhiệm vụ biểu diễn ngôn ngữ (Brown et al., 2020). Tuy nhiên, một nhược điểm đáng kể nằm ở độ phức tạp tính toán của chúng, có độ phức tạp tiệm cận O(T²), trong đó T là độ dài chuỗi. Tắc nghẽn tính toán này đã là trở ngại quan trọng cho việc mở rộng thêm các mô hình transformer. Các mô hình không gian trạng thái như S4 (Gu et al., 2022b), S5 (Smith et al., 2023), LRU (Orvieto et al., 2023b), RWKV (Peng et al., 2023), RetNet (Sun et al., 2023) và Mamba (Gu & Dao, 2023) cung cấp một cách tiếp cận thay thế. Các mô hình này thuộc loại tuần hoàn và xuất sắc trong việc học bộ nhớ dài hạn. Kiến trúc của chúng được thiết kế đặc biệt để nắm bắt các phụ thuộc thời gian trên các chuỗi mở rộng, cung cấp một giải pháp mạnh mẽ cho các nhiệm vụ yêu cầu bộ nhớ dài hạn (Tay et al., 2021). Một trong những ưu điểm của các mô hình không gian trạng thái so với RNN truyền thống nằm ở hiệu quả tính toán của chúng, đạt được thông qua việc áp dụng các thuật toán quét song song (Martin & Cundy, 2018) và Biến đổi Fourier Nhanh (FFT) (Tolimieri et al., 1989; Gu et al., 2022b). Các RNN phi tuyến truyền thống thường bị ảnh hưởng bởi sự lan truyền tiến và lùi chậm, một hạn chế mà các mô hình không gian trạng thái tránh được bằng cách tận dụng các khối RNN tuyến tính.

Các RNN tuyến tính/phi tuyến truyền thống thể hiện sự suy giảm bộ nhớ theo hàm mũ tiệm cận (Wang et al., 2023). Hiện tượng này giải thích khó khăn trong cả xấp xỉ và tối ưu hóa để học bộ nhớ dài hạn sử dụng RNN (cũng được gọi là lời nguyền bộ nhớ). Trong thực tế, kết quả thực nghiệm cho thấy các biến thể SSM như S4 vượt qua một số vấn đề về bộ nhớ. Các kết quả thực nghiệm trước đây gợi ý rằng hoặc (i) "động lực tuyến tính và kích hoạt phi tuyến theo lớp" hoặc (ii) tham số hóa vốn có trong S4, là then chốt trong việc đạt được hiệu suất nâng cao. Nghiên cứu hiện tại trả lời cái nào quan trọng hơn. Chúng tôi đầu tiên chứng minh một định lý xấp xỉ nghịch đảo cho thấy rằng các mô hình không gian trạng thái không có tái tham số hóa vẫn phải chịu "lời nguyền bộ nhớ", điều này phù hợp với kết quả thực nghiệm (Wang & Xue, 2023). Điều này loại trừ điểm (i) là lý do cho việc học bộ nhớ dài hạn tốt của SSMs. Một câu hỏi tự nhiên phát sinh liên quan đến việc liệu các tái tham số hóa có phải là chìa khóa để học bộ nhớ dài hạn hay không. Chúng tôi chứng minh một lớp hàm tái tham số hóa f, mà chúng tôi gọi là tái tham số hóa ổn định, cho phép xấp xỉ ổn định của các hàm phi tuyến. Điều này bao gồm tái tham số hóa hàm mũ và tái tham số hóa softplus thường được sử dụng. Hơn nữa, chúng tôi đặt câu hỏi liệu các tham số hóa của S4 có tối ưu hay không. Ở đây chúng tôi đưa ra một ý nghĩa cụ thể về mặt ổn định tối ưu hóa rằng chúng không tối ưu. Chúng tôi đề xuất cái tối ưu và cho thấy tính ổn định của nó thông qua các thí nghiệm số.

Chúng tôi tóm tắt những đóng góp chính của mình như sau:
1. Chúng tôi chứng minh rằng tương tự như RNN, các mô hình không gian trạng thái không có tái tham số hóa chỉ có thể xấp xỉ ổn định các mục tiêu với bộ nhớ suy giảm theo hàm mũ.
2. Chúng tôi xác định một lớp tái tham số hóa ổn định đạt được xấp xỉ ổn định của bất kỳ hàm phi tuyến nào. Cả bằng chứng lý thuyết và thực nghiệm đều nhấn mạnh rằng tái tham số hóa ổn định là rất quan trọng cho việc học bộ nhớ dài hạn.
3. Từ quan điểm tối ưu hóa, chúng tôi đề xuất tính bị chặn của gradient như tiêu chí và chỉ ra rằng các gradient bị chặn bởi một dạng phụ thuộc vào tham số hóa. Dựa trên ràng buộc gradient, chúng tôi giải phương trình vi phân và suy ra tái tham số hóa "tốt nhất" theo nghĩa ổn định và xác minh tính ổn định của tái tham số hóa mới này trên các sơ đồ tham số hóa khác nhau.

Ký hiệu. Chúng tôi sử dụng chữ đậm để biểu diễn chuỗi trong khi các chữ cái bình thường là vô hướng, vector hoặc hàm. Trong suốt bài báo này, chúng tôi sử dụng ∥ · ∥ để biểu thị chuẩn trên chuỗi các vector, hoặc hàm(số), trong khi | · | (với chỉ số dưới) biểu thị chuẩn của số, vector hoặc tuple trọng số. Ở đây |x|∞ := max_i|xi|, |x|2 := √∑_ix²i, |x|1 := ∑_i|xi| là chuẩn max (L∞), chuẩn L2 và chuẩn L1 thông thường. Chúng tôi sử dụng m để biểu thị chiều ẩn.

2. Bối cảnh
Trong phần này, trước tiên chúng tôi giới thiệu các mô hình không gian trạng thái và so sánh chúng với các RNN phi tuyến truyền thống. Tiếp đó, chúng tôi áp dụng mô hình hóa chuỗi như một vấn đề trong khung xấp xỉ hàm phi tuyến. Cụ thể, các tính chất lý thuyết mà chúng tôi mong đợi từ các mục tiêu được định nghĩa. Hơn nữa, chúng tôi định nghĩa hiện tượng "lời nguyền bộ nhớ" và cung cấp một tóm tắt ngắn gọn về định nghĩa lý thuyết trước đây và kết quả liên quan đến RNN.

2.1. Mô hình không gian trạng thái
Các mô hình không gian trạng thái (SSMs) là một họ mạng nơ-ron chuyên về mô hình hóa chuỗi. Không giống như Mạng Nơ-ron Tuần hoàn (RNNs) (Rumelhart et al., 1986), SSMs có tính phi tuyến theo lớp và động lực tuyến tính trong các trạng thái ẩn của chúng. Cấu trúc độc đáo này tạo điều kiện cho việc tính toán tăng tốc sử dụng FFT (Gu et al., 2022b) hoặc quét song song (Martin & Cundy, 2018). Với các trọng số có thể huấn luyện W ∈ R^(m×m), U ∈ R^(m×d), b, c ∈ R^m và hàm kích hoạt σ(·), SSM đơn giản nhất ánh xạ chuỗi đầu vào d chiều x = {xt} thành chuỗi đầu ra 1 chiều {ŷt}. Để đơn giản hóa phân tích của chúng tôi, chúng tôi sử dụng khung thời gian liên tục được tham chiếu trong Li et al. (2020):

dht/dt = Wht + Uxt + b, h_{-∞} = 0,
ŷt = c^T σ(ht), t ∈ R. (1)

Như được chi tiết trong Phụ lục A, dạng trên là một sự đơn giản hóa của các SSM thực tế theo nghĩa rằng các SSM thực tế có thể được thực hiện bằng cách xếp chồng Phương trình (1).

Đã biết rằng các mô hình không gian trạng thái nhiều lớp là các bộ xấp xỉ toàn cục (Wang & Xue, 2023; Orvieto et al., 2023a). Đặc biệt, khi tính phi tuyến được thêm theo lớp, việc sử dụng W đường chéo thực là đủ (theo nghĩa xấp xỉ) (Gu et al., 2022a; Li et al., 2022). Trong bài báo này, chúng tôi chỉ xem xét trường hợp ma trận đường chéo thực và ký hiệu nó bằng Λ = Diag(λ₁, ..., λₘ).

dht/dt = Λht + Uxt + b. (2)

So với S4, các khác biệt chính nằm ở khởi tạo như HiPPO (Gu et al., 2020) và phương pháp lưu tham số như DPLR (Gu et al., 2022a) và NPLR (Gu et al., 2022b).

2.2. Mô hình hóa chuỗi như xấp xỉ hàm phi tuyến
Mô hình hóa chuỗi nhằm phân biệt mối liên kết giữa một chuỗi đầu vào, được biểu diễn là x = {xt}, và chuỗi đầu ra tương ứng của nó, ký hiệu là y = {yt}. Chuỗi đầu vào là đầu vào liên tục bị chặn biến mất ở vô cực: x ∈ X = C₀(R, R^d) với chuẩn ∥x∥∞ := sup_{t∈R} |xt|∞. Giả định rằng các chuỗi đầu vào và đầu ra được xác định từ các đầu vào thông qua một tập hợp các hàm, được ký hiệu là

H = {Ht : X → R : t ∈ R}, (3)

thông qua mối quan hệ yt = Ht(x). Về bản chất, thách thức của xấp xỉ tuần tự được quy về việc ước lượng chuỗi hàm mong muốn H sử dụng một chuỗi hàm khác Ĥ có thể từ một không gian mô hình được định nghĩa trước như SSMs.

Trong bài báo này, chúng tôi tập trung vào các hàm mục tiêu bị chặn, nhân quả, liên tục, chính quy, đồng nhất thời gian (bất biến dịch chuyển thời gian). Các định nghĩa chính thức được đưa ra trong Phụ lục B.1. Tính liên tục, bị chặn, đồng nhất thời gian, nhân quả là các tính chất quan trọng cho các mô hình chuỗi-thành-chuỗi tốt. Tuyến tính là một sự đơn giản hóa quan trọng vì nhiều định lý lý thuyết có sẵn trong phân tích hàm (Stein & Shakarchi, 2003). Không mất tính tổng quát, chúng tôi giả định rằng các hàm phi tuyến thỏa mãn Ht(0) = 0. Điều này có thể đạt được thông qua việc nghiên cứu H^{adjusted}_t(x) = Ht(x) - Ht(0).

2.3. Hàm bộ nhớ, xấp xỉ ổn định và lời nguyền bộ nhớ
Khái niệm bộ nhớ đã được khám phá rộng rãi trong tài liệu học thuật, tuy nhiên phần lớn các công trình trước đây dựa vào các cách tiếp cận heuristic và thử nghiệm thực nghiệm, đặc biệt trong bối cảnh học bộ nhớ dài hạn (Poli et al., 2023). Ở đây chúng tôi nghiên cứu tính chất bộ nhớ từ góc độ lý thuyết.

Nghiên cứu của chúng tôi sử dụng khung mở rộng được đề xuất bởi Wang et al. (2023), tập trung cụ thể vào các RNN phi tuyến. Tuy nhiên, các nghiên cứu này không giải quyết trường hợp của các mô hình không gian trạng thái. Trong cùng khung, hàm bộ nhớ hơi khác và các khái niệm bộ nhớ suy giảm cho phép chúng tôi khám phá khả năng xấp xỉ của các hàm phi tuyến sử dụng SSMs.

Định nghĩa 2.1 (Hàm bộ nhớ). Đối với các chuỗi hàm phi tuyến bị chặn, nhân quả, liên tục, chính quy và đồng nhất thời gian H = {Ht : t ∈ R} trên X, định nghĩa hàm sau đây là hàm bộ nhớ của H: Trên đầu vào Heaviside bị chặn ux(t) = x · 1{t≥0}

M(H)(t) := sup_{x≠0} (d/dt Ht(ux))/(|x|∞ + 1). (4)

Chúng tôi thêm 1 trong định nghĩa hàm bộ nhớ để làm cho nó chính quy hơn. Hàm bộ nhớ của các hàm mục tiêu được giả định là hữu hạn cho tất cả t ∈ R.

Định nghĩa 2.2 (Bộ nhớ suy giảm). Chuỗi hàm H có bộ nhớ suy giảm nếu
lim_{t→∞} M(H)(t) = 0. (5)

Đặc biệt, chúng ta nói nó có bộ nhớ suy giảm theo hàm mũ (đa thức) nếu tồn tại hằng số β > 0 sao cho
lim_{t→∞} e^{βt} M(H)(t) = 0 (lim_{t→∞} t^β M(H)(t) = 0).

Tương tự như Wang et al. (2023), định nghĩa hàm bộ nhớ điều chỉnh này cũng tương thích với khái niệm bộ nhớ trong hàm tuyến tính dựa trên định lý biểu diễn Riesz nổi tiếng (Định lý B.3 trong Phụ lục B). Trong trường hợp hàm tuyến tính, hàm bộ nhớ này là hàm đáp ứng xung. Nó đo tốc độ suy giảm của bộ nhớ về một xung được đưa ra tại t = 0. Nó là một đại diện để đặc trưng khả năng ghi nhớ của mô hình về các đầu vào trước đó trong các trạng thái ẩn ht và đầu ra yt. Trong khi giá trị bộ nhớ lớn M(t) không có nghĩa là mô hình tại thời điểm t có sự ghi nhớ rõ ràng về các đầu vào trước đó x0, giá trị bộ nhớ nhỏ M(t) có nghĩa là mô hình đã quên đầu vào xung x0. Do đó, việc có hàm bộ nhớ suy giảm chậm M(·) là điều kiện cần thiết để xây dựng mô hình với bộ nhớ dài hạn. Như được hiển thị trong Phụ lục C.1, các hàm phi tuyến được xây dựng bởi các mô hình không gian trạng thái là liên tục điểm trên các đầu vào Heaviside. Kết hợp với tính đồng nhất thời gian, chúng ta biết rằng các mô hình không gian trạng thái là các hàm phi tuyến với bộ nhớ suy giảm (xem Phụ lục C.2).

Định nghĩa 2.3 (Xấp xỉ chuỗi hàm trong chuẩn kiểu Sobolev). Cho các chuỗi hàm H và Ĥ, chúng tôi xem xét xấp xỉ trong chuẩn kiểu Sobolev sau đây (Phụ lục B.2):

||H - Ĥ||_{W^{1,∞}} := (6)
sup_t [||Ht - Ĥt||∞ + ||dHt/dt - dĤt/dt||∞]. (7)

Định nghĩa 2.4 (Sai số nhiễu). Đối với mục tiêu H và mô hình tham số hóa Ĥ(·, θm), θm = (Λ, U, b, c) ∈ Θm := {R^{m×m} × R^{m×d} × R^m × R^m}, chúng tôi định nghĩa sai số nhiễu cho chiều ẩn m:

Em(β) := sup_{θ̃m∈{θ:|θ-θm|_2≤β}} ||H - Ĥ(·; θ̃m)||_{W^{1,∞}}. (8)

Đặc biệt, Ĥ̃ đề cập đến các mô hình bị nhiễu Ĥ(·; θ̃m). Hơn nữa, E(β) := lim sup_{m→∞} Em(β) là sai số nhiễu tiệm cận. Chuẩn trọng số cho SSM là |θ|2 := max(|Λ|2, |U|2, |b|2, |c|2).

Dựa trên định nghĩa sai số nhiễu, chúng tôi xem xét xấp xỉ ổn định như được giới thiệu bởi Wang et al. (2023).

Định nghĩa 2.5 (Xấp xỉ ổn định). Cho β0 > 0. Một chuỗi hàm mục tiêu H chấp nhận xấp xỉ β0-ổn định nếu sai số bị nhiễu thỏa mãn:
1. E(0) = 0.
2. E(β) liên tục cho β ∈ [0, β0].

Phương trình E(0) = 0 có nghĩa là xấp xỉ toàn cục được đạt được bởi không gian giả thuyết. Xấp xỉ ổn định tăng cường xấp xỉ toàn cục bằng cách yêu cầu mô hình phải mạnh mẽ chống lại nhiễu trên các trọng số. Vì xấp xỉ ổn định là yêu cầu cần thiết để các tham số tối ưu được tìm thấy bởi các tối ưu hóa dựa trên gradient, đây là một giả định mong muốn.

Hiện tượng "lời nguyền bộ nhớ", ban đầu được hình thành cho các hàm tuyến tính và RNN tuyến tính, được ghi chép tốt trong nghiên cứu trước đây (Li et al., 2020; 2022; Jiang et al., 2023). Nó mô tả hiện tượng mà các mục tiêu được xấp xỉ bởi các RNN tuyến tính, hardtanh, hoặc tanh phải thể hiện bộ nhớ suy giảm theo hàm mũ. Tuy nhiên, quan sát thực nghiệm gợi ý rằng các mô hình không gian trạng thái, đặc biệt là biến thể S4, có thể sở hữu các tính chất thuận lợi. Do đó, việc xác định liệu các hạn chế vốn có của RNN có thể được tránh sử dụng các mô hình không gian trạng thái là rất quan trọng. Cho hiệu suất ấn tượng của các mô hình không gian trạng thái, đặc biệt là S4, một vài câu hỏi then chốt phát sinh: Cấu trúc mô hình của các mô hình không gian trạng thái có vượt qua "lời nguyền bộ nhớ" không? Trong phần tiếp theo, chúng tôi sẽ chứng minh rằng cấu trúc mô hình của các mô hình không gian trạng thái thực sự không giải quyết hiện tượng lời nguyền bộ nhớ.

3. Kết quả chính
Trong phần này, trước tiên chúng tôi chứng minh rằng tương tự như các mạng nơ-ron tuần hoàn truyền thống (Li et al., 2020; Wang et al., 2023), các mô hình không gian trạng thái không có tái tham số hóa bị ảnh hưởng bởi vấn đề "lời nguyền bộ nhớ". Điều này ngụ ý rằng các mục tiêu có thể được xấp xỉ ổn định bởi SSMs phải có bộ nhớ suy giảm theo hàm mũ. Phân tích của chúng tôi tiết lộ rằng vấn đề phát sinh từ các trọng số tuần hoàn hội tụ về biên giới ổn định khi học các mục tiêu liên quan đến bộ nhớ dài hạn. Do đó, chúng tôi giới thiệu một lớp kỹ thuật tái tham số hóa ổn định để đạt được xấp xỉ ổn định cho các mục tiêu với bộ nhớ suy giảm đa thức.

Ngoài lợi ích của góc độ xấp xỉ, chúng tôi cũng thảo luận về lợi ích tối ưu hóa của các tái tham số hóa ổn định. Chúng tôi chỉ ra rằng tái tham số hóa ổn định có thể làm cho thang đo gradient cân bằng hơn, do đó việc tối ưu hóa các mô hình lớn có thể ổn định hơn.

3.1. Lời nguyền bộ nhớ trong SSMs
Trong phần này, chúng tôi trình bày một định lý lý thuyết chứng minh rằng cấu trúc không gian trạng thái không làm giảm hiện tượng "lời nguyền bộ nhớ". Các mô hình không gian trạng thái bao gồm các RNN tuyến tính và kích hoạt phi tuyến được xếp chồng xen kẽ. Kết quả của chúng tôi được thiết lập cho cả trường hợp nông và sâu (Nhận xét C.3). Là các mô hình tuần hoàn, SSMs không có tái tham số hóa tiếp tục thể hiện hiện tượng suy giảm bộ nhớ theo hàm mũ thường quan sát được, như được chứng minh bởi các phát hiện thực nghiệm (Wang & Xue, 2023).

Giả định 3.1. Chúng tôi giả định các trạng thái ẩn vẫn bị chặn đều cho bất kỳ chuỗi đầu vào x nào, bất kể chiều ẩn m. Cụ thể, điều này có thể được biểu diễn là
sup_m sup_t |ht|∞ < ∞. (9)

Giả định 3.2. Chúng tôi tập trung vào các kích hoạt phi tuyến tăng nghiêm ngặt, có thể vi phân liên tục với hằng số Lipschitz L0. Tính chất này đúng cho các kích hoạt như tanh, sigmoid, softsign σ(z) = z/(1+|z|).

Định lý 3.3 (Lời nguyền bộ nhớ trong SSMs). Giả sử H là một chuỗi các hàm bị chặn, nhân quả, liên tục, chính quy và đồng nhất thời gian trên X với bộ nhớ suy giảm. Giả sử tồn tại một chuỗi các mô hình không gian trạng thái {Ĥ(·, θm)}∞_{m=1} xấp xỉ β0-ổn định H trong chuẩn được định nghĩa trong Phương trình (6). Giả định các trọng số mô hình bị chặn đều: θmax := sup_m |θm|2 < ∞. Khi đó hàm bộ nhớ M(H)(t) của mục tiêu suy giảm theo hàm mũ:

M(H)(t) ≤ (d + 1)L0θ²max e^{-βt}, t ≥ 0, β < β0. (10)

Ở đây d là chiều của các chuỗi đầu vào. Khi tổng quát cho các trường hợp nhiều lớp, ràng buộc hàm bộ nhớ được gây ra từ SSM ℓ-lớp là: Đối với một đa thức P(t) nào đó với bậc nhiều nhất là l−1

M(H)(t) ≤ (d + 1)L^ℓ_0 θ^{ℓ+1}_{max} P(t)e^{-βt}, t ≥ 0, β < β0. (11)

Chứng minh của Định lý 3.3 được cung cấp trong Phụ lục C.3. Biên giới ổn định (thời gian liên tục) (được thảo luận trong Nhận xét C.1) cho Λ trong các mô hình không gian trạng thái (Phương trình (2)) là max_i∈[m] λi(Λ) < 0. Biên giới này đến từ tiêu chí ổn định cho hệ thống tuyến tính bất biến thời gian. So với các kết quả trước đây (Li et al., 2020; Wang et al., 2023), sự khác biệt chứng minh chính đến từ Bổ đề C.10 vì kích hoạt ở đầu ra yt = c^T σ(ht). Kết quả của chúng tôi cung cấp đặc tính chính xác hơn về suy giảm bộ nhớ, trái ngược với các công trình trước đây chỉ cung cấp ước lượng định tính. Một hệ quả của Định lý 3.3 là nếu mục tiêu thể hiện suy giảm không phải hàm mũ (ví dụ, suy giảm đa thức), các trọng số tuần hoàn hội tụ về biên giới ổn định, do đó làm cho xấp xỉ không ổn định. Việc tìm trọng số tối ưu có thể trở nên thách thức với các phương pháp tối ưu hóa dựa trên gradient, vì quá trình tối ưu hóa có xu hướng trở nên không ổn định với sự gia tăng kích thước mô hình. Xác minh số được trình bày trong Hình 1 (a). Các đường giao nhau và các điểm giao nhau dịch chuyển về phía 0, gợi ý rằng bán kính ổn định β0 không tồn tại. Do đó SSMs không có tái tham số hóa không thể xấp xỉ ổn định các mục tiêu với bộ nhớ suy giảm đa thức.

3.2. Tái tham số hóa ổn định và ưu điểm của nó trong xấp xỉ
Chứng minh của Định lý 3.3 gợi ý rằng "lời nguyền bộ nhớ" phát sinh do các trọng số tuần hoàn tiếp cận biên giới ổn định. Ngoài ra, các thí nghiệm số của chúng tôi (trong Hình 1 (c)) cho thấy rằng trong khi các mô hình không gian trạng thái bị ảnh hưởng bởi lời nguyền bộ nhớ, lớp S4 thường được sử dụng (với tái tham số hóa hàm mũ) cải thiện vấn đề này. Tuy nhiên, nó không phải là giải pháp duy nhất. Các phát hiện của chúng tôi nhấn mạnh rằng nền tảng để đạt được xấp xỉ ổn định là phương pháp tái tham số hóa ổn định, mà chúng tôi định nghĩa như sau:

Định nghĩa 3.4 (Tái tham số hóa ổn định). Chúng ta nói một sơ đồ tái tham số hóa f : R → R là ổn định nếu tồn tại một hàm liên tục g sao cho: g : [0,∞) → [0,∞), g(0) = 0:

sup_w [|f(w)| sup_{|w̃-w|≤β} ∫₀^∞ |e^{f(w̃)t} - e^{f(w)t}|dt] ≤ g(β). (12)

Ví dụ, tái tham số hóa thường được sử dụng (Gu et al., 2022b; Smith et al., 2023) như f(w) = -e^w, f(w) = -log(1 + e^w) đều ổn định. Xác minh được cung cấp trong Nhận xét C.4.

Như được mô tả trong Hình 1 (b), các mô hình không gian trạng thái với tái tham số hóa ổn định có thể xấp xỉ các mục tiêu thể hiện suy giảm đa thức trong bộ nhớ. Đặc biệt, chúng tôi chứng minh rằng dưới một thiết lập nhiễu đơn giản hóa (chỉ làm nhiễu các trọng số tuần hoàn), bất kỳ hàm tuyến tính nào có thể được xấp xỉ ổn định bởi các RNN tuyến tính. Phát hiện này dưới thiết lập đơn giản hóa đã rất có ý nghĩa vì tính không ổn định trong việc học bộ nhớ dài hạn chủ yếu đến từ các trọng số tuần hoàn.

Định lý 3.5 (Sự tồn tại của xấp xỉ ổn định bằng tái tham số hóa ổn định). Đối với bất kỳ hàm tuyến tính H bị chặn, nhân quả, liên tục, chính quy, đồng nhất thời gian nào, giả sử H được xấp xỉ bởi một chuỗi các RNN tuyến tính {Ĥ(·, θm)}∞_{m=1} với tái tham số hóa ổn định, thì xấp xỉ này là một xấp xỉ ổn định.

Chứng minh của Định lý 3.5 ở Phụ lục C.4. Việc tổng quát cho các hàm phi tuyến với biểu diễn V-olterra-Series có thể được đạt được tương tự (Nhận xét C.5). So với Định lý 3.3, Định lý 3.5 nhấn mạnh vai trò của tái tham số hóa ổn định trong việc đạt được xấp xỉ ổn định của hàm phi tuyến với bộ nhớ dài hạn. Mặc dù SSM vanilla và StableSSM hoạt động trong cùng không gian giả thuyết, StableSSM thể hiện tính ổn định tốt hơn trong việc xấp xỉ bất kỳ mục tiêu bộ nhớ suy giảm nào (Bảng 1). Ngược lại, mô hình SSM vanilla bị giới hạn ở việc xấp xỉ ổn định các mục tiêu được đặc trưng bởi suy giảm bộ nhớ theo hàm mũ.

3.3. Lợi ích tối ưu hóa của tái tham số hóa ổn định
Trong phần trước, lợi ích xấp xỉ của tái tham số hóa ổn định trong SSMs được thảo luận. Ở đây chúng tôi nghiên cứu tác động của các tham số hóa khác nhau đến tính ổn định tối ưu hóa, đặc biệt là thang đo gradient.

Như được chỉ ra bởi Li et al. (2020; 2022), việc xấp xỉ các hàm tuyến tính sử dụng RNN tuyến tính có thể được quy về việc xấp xỉ hàm bộ nhớ tích phân L1 ρ(t) thông qua các hàm có dạng ρ̂(t) = ∑ᵢ₌₁ᵐ cᵢe^{-λᵢt}.

ρ(t) ≈ ∑ᵢ₌₁ᵐ cᵢe^{-λᵢt}, λᵢ > 0. (13)

Trong khung này, λᵢ được hiểu là chế độ suy giảm. Tiếp cận từ quan điểm tối ưu hóa dựa trên gradient, và cho rằng tốc độ học được chia sẻ trên các chế độ suy giảm khác nhau, một đặc tính phù hợp cho "tham số hóa tốt" xuất hiện: Thang đo gradient trên các chế độ suy giảm bộ nhớ khác nhau nên liên tục Lipschitz đối với thang đo trọng số.

|Gradient| := ∂Loss/∂λᵢ ≤ L|λᵢ|. (14)

Hằng số Lipschitz được ký hiệu bằng L. Không có tính chất này, quá trình tối ưu hóa có thể nhạy cảm với tốc độ học.

[Tiếp tục dịch phần còn lại...]
