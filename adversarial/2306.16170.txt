# 2306.16170.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/adversarial/2306.16170.pdf
# File size: 1365940 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1
Mitigating Accuracy-Robustness Trade-off via
Balanced Multi-Teacher Adversarial Distillation
Shiji Zhao, Xizhe Wang, and Xingxing Weiâˆ—Member, IEEE
Abstract â€”Adversarial Training is a practical approach for improving the robustness of deep neural networks against adversarial
attacks. Although bringing reliable robustness, the performance towards clean examples is negatively affected after Adversarial
Training, which means a trade-off exists between accuracy and robustness. Recently, some studies have tried to use knowledge
distillation methods in Adversarial Training, achieving competitive performance in improving the robustness but the accuracy for clean
samples is still limited. In this paper, to mitigate the accuracy-robustness trade-off, we introduce the Balanced Multi-Teacher
Adversarial Robustness Distillation (B-MTARD) to guide the modelâ€™s Adversarial Training process by applying a strong clean teacher
and a strong robust teacher to handle the clean examples and adversarial examples, respectively. During the optimization process, to
ensure that different teachers show similar knowledge scales, we design the Entropy-Based Balance algorithm to adjust the teacherâ€™s
temperature and keep the teachersâ€™ information entropy consistent. Besides, to ensure that the student has a relatively consistent
learning speed from multiple teachers, we propose the Normalization Loss Balance algorithm to adjust the learning weights of different
types of knowledge. A series of experiments conducted on three public datasets demonstrate that B-MTARD outperforms the
state-of-the-art methods against various adversarial attacks.
Index Terms â€”DNNs, Adversarial Training, Knowledge Distillation, Adversarial Robustness, Accuracy-Robustness Trade-off.
âœ¦
1 I NTRODUCTION
DEEP Neural Networks (DNNs) have become powerful
tools for solving complex real-world learning prob-
lems, such as image classification [17], face recognition [41],
and natural language processing [35]. However, Szegedy et
al. [39] demonstrate that DNNs are vulnerable to adver-
sarial attacks with imperceptible adversarial perturbations
on input, which causes wrong predictions of DNNs. To
defend against adversarial attacks, Adversarial Training is
proposed and has shown its effectiveness in obtaining ad-
versarial robust DNNs [10], [31], [42]. While improving the
robustness of DNNs, a negative impact exists on the modelâ€™s
accuracy on clean samples. Some methods are proposed
to alleviate the trade-off between accuracy and robustness
from different perspectives, e.g., optimization [32], [42],
[50], [55], [56] and extra data [1], [5], [19]. However, this
phenomenon still exists and needs to be further explored.
Recently, to further enhance the robustness of small
DNNs, Knowledge Distillation [20] is applied as a powerful
tool in Adversarial Training, which can transfer knowledge
from strong robust models to the student model. It utilizes
the teacherâ€™s predictions as label information to guide the
student model within the framework of Adversarial Train-
ing, including the optimization to generate the adversarial
examples (Maximization process) and apply adversarial
examples to train the student with the assistance of the
teacher (Minimization process). These methods are called
adversarial robustness distillation (ARD) [13], [60], [61].
â€¢Shiji Zhao, Xizhe Wang, Xingxing Wei were at the Institute of Artificial
Intelligence, Beihang University, No.37, Xueyuan Road, Haidian
District, Beijing, 100191, P .R. China. (E-mail: {zhaoshiji123, xizhewang,
xxwei}@buaa.edu.cn)
â€¢Xingxing Wei is the corresponding author.Compared with one-hot ground truth labels, the teacherâ€™s
predicted labels can not only retain the correctness of the
target label but also reflect richer knowledge information of
the non-target label. Several studies attempt to explain why
knowledge distillation is effective through various views,
e.g., Re-weighting [12], Privileged Information [30], and
Label-smoothing [53]. Recently, Li et al. [25] argue that cor-
rectness, smooth regularization, and class discriminability
in teacherâ€™s predicted labels can make up the knowledge
of the teacherâ€™s predicted distribution, and that appropriate
category discriminability can improve the effectiveness of
knowledge distillation. While achieving impressive robust
performance, we are curious if adversarial robustness dis-
tillation [13], [60], [61] could address the trade-off between
accuracy and robustness.
Generally speaking, if we want to improve both accu-
racy and robustness through knowledge distillation, the
provided teacher model is supposed to perform well in both
aspects. That is to say, the teacher model should equally
provide correct guidance and rich knowledge information
for clean and adversarial examples. However, due to the
existing trade-off, the teacher model itself is difficult to
achieve the above goals. Therefore, a reasonable approach
is to decouple adversarial knowledge distillation, i.e., using
two teachers that are good at accuracy and robustness
respectively (they can be called the clean teacher and the
robust teacher) to guide the student in different knowledge
types. Under this divide-and-conquer strategy, the student
model guided by multiple teacher models can enhance its
performance in both aspects theoretically.
However, the actual implementation of this idea is chal-
lenging. Owing to the intrinsic trade-off between accuracy
and robustness, the clean teacher and robust teacher push
the student model in opposite directions. This is obviouslyarXiv:2306.16170v3  [cs.LG]  16 Jun 2024

--- PAGE 2 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2
different from the current multi-teacher knowledge distilla-
tion methods [29], [37], [52], where their optimization goal
of different teachers is consistent, i.e., all teachers push
the student towards the good clean accuracy. The trade-
off in our task leads to a difficult optimization mainly in
two aspects: (1) How to make two teacher models have
similar knowledge scales. When one teacher has too much
knowledge scale, the student will be forced to obtain more
knowledge from this teacher and relatively less knowledge
from another, which directly results in an imbalance be-
tween accuracy and robustness. However, finding a clean
teacher and a robust teacher with similar knowledge scales
is not easy because the different network architectures and
training methods will lead to a natural gap in knowledge
scales between different teachers. Thus, we need a metric
to accurately measure and balance teachersâ€™ knowledge
scales. (2) How to balance the studentâ€™s learning ability from
different types of teachersâ€™ knowledge. In our multi-teacher
setting, because two opposite optimization directions exist
in adversarial robustness distillation, the student may over-
fit an ability that is easier to learn but ignores another due
to different learning difficulties. Only the cooperation of the
student and teachers can help the student obtain both strong
accuracy and robustness. Thus, we need an adaptive balance
mechanism to adjust the learning process.
Based on the above discussions, in this paper, we pro-
pose the Balanced Multi-Teacher Adversarial Robustness
Distillation (B-MTARD) to enhance both accuracy and
robustness at the same time. During Adversarial Training,
a clean teacher is to instruct the student for handling clean
examples and a strong robust teacher is to instruct the
student for handling adversarial examples. To meet the
two challenges above, the novel Entropy-Based Balance
and Normalization Loss Balance strategies are presented to
achieve a balanced state between accuracy and robustness.
For the first challenge, to ensure that the two teacher
models show similar knowledge scales during the optimiza-
tion, we propose an Entropy-Based Balance algorithm to
balance the information entropy among different teachersâ€™
predicted distribution. Specifically, based on information
theory [36], relative entropy can measure the incremental
information from an initialized networkâ€™s predicted distri-
bution to the well-trained teachersâ€™ predicted distribution,
so we utilize the relative entropy to measure the teach-
ersâ€™ knowledge scales. Furthermore, we theoretically prove
that the difference in knowledge scales can be transformed
into the difference in teachersâ€™ information entropy. Thus
Entropy-Based Balance algorithm can balance the knowl-
edge scales by adjusting each teacherâ€™s temperature until
the information entropy is appropriate and consistent.
For the second challenge, to maintain the relative equal-
ity of the studentâ€™s learning speed from multiple teachers,
we propose a Normalization Loss Balance algorithm to
control loss weight and balance the influence between the
clean teacher and the robust teacher. Specifically, inspired
by [7], we design a relative loss to measure the knowledge
proportion that the student learns from different teachers.
The different loss weights for the update of student pa-
rameters are dynamically adjusted to keep the relative loss
consistent during the training process. Compared with the
initial state, the student model will be forced to learn rela-tively equal knowledge scales from all the teacher models.
Our code is available at https://github.com/zhaoshiji123/
MTARD-extension.
The main contributions of this work are three-fold:
â€¢We propose a novel framework: Balanced Multi-
Teacher Adversarial Robustness Distillation (B-
MTARD). B-MTARD decouples the adversarial
knowledge distillation to obtain a balance between
accuracy and robustness. We apply a clean teacher
and a robust teacher to adaptively bring both clean
and robust knowledge to the student.
â€¢We propose the Entropy-Based Balance algorithm
and Normalization Loss Balance algorithm to bal-
ance multiple teachers. The Entropy-Based Balance
algorithm is applied to control the teachersâ€™ knowl-
edge scales by dynamically adjusting temperatures.
The Normalization Loss Balance algorithm is utilized
to ensure that the student has relatively equal learn-
ing speeds for different teachersâ€™ knowledge.
â€¢We empirically verify the effectiveness of B-MTARD
in improving performance. The Weighted Robust Ac-
curacy (a measure to evaluate both clean and robust
accuracy) of our B-MTARD trained models improves
significantly against a variety of attacks compared to
state-of-the-art Adversarial Training and knowledge
distillation methods. Besides, we show B-MTARD
can achieve an obvious improvement compared to
MTARD proposed in our conference version.
This journal paper is an extended version of our ECCV
paper (MTARD) [59]. Compared with the conference ver-
sion, we have made significant improvements and exten-
sions in this version in the following aspects: (1) At the idea
level, we discuss the trade-off problem in a more compre-
hensive view from both teachers and the student, while the
conference version only considers studentâ€™s view. We point
out that the difference in two teachersâ€™ knowledge scales
will affect the trade-off between accuracy and robustness,
and should formulate this factor into our method (the first
challenge in Section 1). (2) At the method level, a mul-
tiple teacherâ€™s knowledge adjustment mechanism named
the Entropy-Based Balance algorithm is designed, where
we first give the definition and metric for the knowledge
scale and then present the balancing method (Section 3.2).
(3) At the experimental level, we give a comprehensive
comparison between B-MTARD with our conference ver-
sion MTARD to show the advantage, and also additionally
conduct experiments on the Tiny-ImageNet dataset and
compare our B-MTARD with more SOTA methods against
more advanced attacks. In addition, we give more ablation
studies to comprehensively test our method (Section 4).
The rest of the paper is organized as follows: Related
work is given in Section 2. Section 3 introduces the details
of our B-MTARD. The experiments are conducted in Section
4, and the conclusion is given in Section 5.
2 R ELATED WORK
2.1 Adversarial Attacks
Since Szegedy et al. [39] propose that adversarial examples
can mislead the deep neural network, lots of effective ad-
versarial attack methods, such as the Fast Gradient Sign

--- PAGE 3 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3
Method (FGSM) [14], Projected Gradient Descent Attack
(PGD) [31], and Carlini and Wagner Attack (CW) [4] are
proposed. Attack methods can be divided into white-box at-
tacks and black-box attacks. White-box attacks know all the
parameter information of the target model when generating
adversarial examples, and black-box attacks know little or
no information. In general, black-box attacks simulate the
model gradient by repeatedly querying the target model
(query-based attack) [2], [27], [43], [44], [45], [46], [47] or
searching for an alternative model similar to the target
model (transfer-based attack) [11], [21]. Recently, a strong
adversarial attack method named AutoAttack (AA) [10] is
proposed, which consists of four attack methods, including
Auto-PGD (APGD), Difference of Logits Ratio (DLR) attack,
FAB-Attack [9], and the black-box Square Attack [2].
2.2 Adversarial Training
Adversarial Training [31], [55] is seen as an effective way
to defend against adversarial attacks. Madry et al. [31]
formulate Adversarial Training as a min-max optimization
problem as follows:
min
Î¸E(x,y)âˆ¼D[max
Î´âˆˆâ„¦L(f(x+Î´;Î¸), y)], (1)
where frepresents a deep neural network with weight Î¸,
Drepresents a distribution of the clean example xand
the ground truth label y.Lrepresents the loss function. Î´
represents the adversarial perturbation, and â„¦represents a
bound, which can be defined as â„¦ ={Î´:||Î´|| â‰¤Ïµ}with the
maximum perturbation scale Ïµ.
To alleviate the trade-off between accuracy and robust-
ness, some methods are proposed from different perspec-
tives, e.g., optimization [32], [42], [50], [55] and extra data
[1], [5], [19]. From the perspective of optimization, Zhang
et al. [55] attempt to reduce the gap between accuracy
and robustness by minimizing a Kullbackâ€“Leibler (KL) di-
vergence loss (TRADES). Wang et al. [42] further improve
performance through Misclassification-Aware Adversarial
Training. Stutz et al. [38] claim that manifold analysis can be
helpful in achieving accuracy and robustness. Yang et al. [50]
argue that the trade-off can be mitigated by optimizing the
local Lipschitz functions. Pang et al. [32] propose to employ
local equivariance to describe the ideal behavior of a robust
model, which facilitates the reconciliation between accuracy
and robustness (SCORE). From the perspective of data, [1]
and [5] find that additional unlabeled data can be useful
to improve both the accuracy and robustness. [19] shows
that adversarial pre-training with extra data can remarkably
improve accuracy and adversarial robustness.
Different from the previous methods, we try to alleviate
the trade-off by utilizing multi-teacher adversarial distilla-
tion, which applies a clean teacher and a robust teacher to
bring both clean and robust knowledge to the student.
2.3 Knowledge Distillation
Knowledge distillation can transfer the performance of
other models to the target model [20]. Extensive research
has been widely studied in recent years [23], [25], [52].Knowledge distillation can briefly be formulated as the
following optimization:
arg min
Î¸S(1âˆ’Î±)CE(S(x), y) +Î±Ï„2KL(S(x;Ï„), T(x;Ï„)),
(2)
where Srepresents the student model with weight Î¸S,
Trepresents the teacher model. KL is Kullbackâ€“Leibler
divergence loss, CE represents the cross-entropy loss. Ï„is a
temperature constant combined with softmax operation, Î±
is a weight hyper-parameter.
The original knowledge distillation [20] attempts to im-
prove the studentâ€™s performance on clean examples, and
pre-define a high temperature until the teacher produces
suitably soft labels, while the same high temperature is
applied to train the student to match these soft labels,
which allows the student to better acquire knowledge from a
single type of teacher. Our method (with our Entropy-Based
Balance Algorithm) is to balance the studentâ€™s performance
between accuracy and robustness, and we adaptively adjust
the temperature for different types of teachers to balance
their knowledge scales, which allows the student to equally
acquire knowledge from different types of teachers.
The temperature Ï„is artificially adjusted as a hyper-
parameter in previous work. Recently, [26], [28] propose
to adjust Ï„automatically based on the designed learnable
sub-network. These methods need to train the sub-network
according to the feedback of the student model, which adds
additional training overhead. Our method automatically
adjusts the temperature only based on multiple teachersâ€™
knowledge scales and has almost no computational cost.
Some studies also exist on multi-teacher knowledge dis-
tillation, and are designed from different views: including
response-based knowledge [23], [52], feature-based knowl-
edge [29], [57], and relation-based knowledge [48], [51]. Pre-
vious methods often use multiple teachers for knowledge
distillation for the same examples, and those teachers tend
to optimize the student in similar directions. Different from
previous methods, our multi-teacher adversarial robustness
distillation targets two different kinds of examples. Since
adversarial examples are generated to mislead the model,
there is a huge gap between the optimization direction of
clean examples and adversarial examples, which exists great
difficulties compared with previous research.
2.4 Adversarial Robustness Distillation
To defend against the adversarial attack, defense distillation
[33] is proposed to apply the traditional knowledge distil-
lation. However, this method is considered as â€œObfuscated
Gradientsâ€ [3] and can be broken by CW attack [4]. Then a
series of methods [13], [60], [61] apply knowledge distilla-
tion to further enhance the robustness based on Adversarial
Training [31]. ARD [13] first proposes that using a strong
robust model as the teacher model in the framework of
Adversarial Training can achieve pretty robustness. IAD [60]
performs adversarial knowledge distillation by compositing
with unreliable teacher guidance and student introspection.
RSLAD [61] uses the soft labels generated by the robust
teacher model to produce adversarial examples, and further
utilizes the robust labels to guide the training process of
both clean and adversarial examples, which can effectively

--- PAGE 4 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4
student
robust 
teacherclean 
teacher ğ‘·ğ’ğ’‚ğ’•ğ‘»ğ’ğ’‚ğ’•
ğ‘·ğ’ğ’‚ğ’•ğ‘º
ğ‘·ğ’‚ğ’…ğ’—ğ‘º
ğ‘·ğ’‚ğ’…ğ’—ğ‘»ğ’‚ğ’…ğ’—ğ‘³ğ’ğ’‚ğ’•
ğ‘³ğ’‚ğ’…ğ’—ğ‰ğ’ğ’‚ğ’•
ğ‰ğ’‚ğ’…ğ’—ğ’˜ğ’ğ’‚ğ’•
ğ’˜ğ’‚ğ’…ğ’—ğ’˜ğ’ğ’‚ğ’•ğ‘³ğ’ğ’‚ğ’•+ğ’˜ğ’‚ğ’…ğ’—ğ‘³ğ’‚ğ’…ğ’—backward propagation
ğ‘·ğ’ğ’‚ğ’•ğ‘»ğ’ğ’‚ğ’•
ğ‘·ğ’‚ğ’…ğ’—ğ‘»ğ’‚ğ’…ğ’—ğ‘¯(ğ‘·ğ’ğ’‚ğ’•ğ‘»ğ’ğ’‚ğ’•)
ğ’ğ’Šğ’||ğ‘¯(ğ‘·ğ’ğ’‚ğ’•ğ‘»ğ’ğ’‚ğ’•)âˆ’ğ‘·ğ’‚ğ’…ğ’—ğ‘»ğ’‚ğ’…ğ’—||ğ‰ğ’ğ’‚ğ’•
ğ‰ğ’‚ğ’…ğ’—
ğ’˜ğ’ğ’‚ğ’•
ğ’˜ğ’‚ğ’…ğ’—ğ’ğ’Šğ’||à·¨ğ‘³ğ’ğ’‚ğ’•âˆ’à·¨ğ‘³ğ’‚ğ’…ğ’—||
ğ‘³ğ’‚ğ’…ğ’—ğ‘³ğ’ğ’‚ğ’•clean example
adversarial 
example
adversarial 
perturbationEntropy -Based Balance in B-MTARD
Normalization Loss Balance in B-MTARDclean teacherâ€™s predicted distribution 
toward clean example
robust teacherâ€™s predicted distribution 
toward adversarial example
Balanced Multi -Teacher Adversarial Robustness Distillationà·¨ğ‘³ğ’ğ’‚ğ’•
à·¨ğ‘³ğ’‚ğ’…ğ’—ğ‘¯(ğ‘·ğ’‚ğ’…ğ’—ğ‘»ğ’‚ğ’…ğ’—)
Fig. 1. The framework of our Balanced Multi-Teacher Adversarial Robustness Distillation (B-MTARD). In the training process, we first generate
adversarial examples from the student. Then, with the guidance of the clean teacher and the robust teacher, the student is trained on clean
examples and adversarial examples, respectively. Here, we apply the Entropy-Based Balance algorithm to adjust the teacherâ€™s knowledge scales;
In addition, we use the Normalization Loss Balance algorithm to balance the studentâ€™s knowledge learning speed from different teachers.
improve the robustness of the student. Fair-ARD [49] is
proposed to explore the fairness issues in ARD and enhance
robust fairness by re-weighting different classes. Inspired by
this paper, ABSLD [58] tries to obtain a model with robust
fairness by reducing the studentâ€™s class-wise error risk gap
and adjusting the class-wise smoothness degree of teacher
labels via re-temperating different classes.
The above methods focus on improving the robustness
or fairness of ARD. In our method, we consider both ac-
curacy and robustness through clean and robust teachers
to further mitigate the trade-off, which exists an obvious
difference compared with other methods.
3 M ETHODOLOGY
In this section, we propose B-MTARD to enhance both
accuracy and robustness by applying the clean teacher and
the robust teacher to guide the student. From the teach-
ersâ€™ teaching view, the Entropy-Based Balance algorithm is
proposed to balance the teacherâ€™s knowledge scale. From
the studentâ€™s learning view, the Normalization Loss Balance
algorithm is designed to balance the studentâ€™s learning
speeds from different teachers.
3.1 Balanced Multi-Teacher Adversarial Distillation
The training process in B-MTARD can be considered as a
min-max optimization based on Adversarial Training. The
whole framework can be viewed in Fig. 1. In the inner
maximization, the adversarial examples are generated by
the student based on the clean examples. In the outer
minimization, the clean teacher and the robust teacher gen-
erate predicted labels to guide the student training process
toward the clean examples and the adversarial examples,
respectively. The optimization of the basic B-MTARD is
defined as follows:
arg min
Î¸S(1âˆ’Î±)KL(S(xnat;Ï„s), Tnat(xnat;Ï„nat))
+Î±KL (S(xadv;Ï„s), Tadv(xadv;Ï„adv)),(3)xadv= arg max
Î´âˆˆâ„¦CE(S(xnat+Î´), y), (4)
where xnatandxadvare clean examples and adversarial
examples, S(x;Ï„s)represents student network Swith tem-
perature Ï„s.Tnat(x;Ï„nat)andTadv(x;Ï„adv)represent the
clean and robust teacher with temperature Ï„natandÏ„adv,
respectively. Î±is a constant in the basic proposal. CE
denotes the Cross-Entropy loss in the maximization. Here,
Ï„s,Ï„nat, and Ï„advare usually pre-defined in previous works.
The goal of B-MTARD is to obtain a student with strong
accuracy as the clean teacher and strong robustness as the
robust teacher. In the actual operation process, however,
two teachers may display different knowledge scales; Mean-
while, the student may have different learning speeds to-
ward different types of teachersâ€™ knowledge. The above two
points may lead to an unbalanced performance in accuracy
and robustness. So how to handle multi-teacher adversarial
robustness distillation becomes a problem to be solved in
the following two subsections.
3.2 Entropy-Based Balance in B-MTARD
3.2.1 Measuring Knowledge Scale
Due to the different model architectures and training meth-
ods, the teachers may have different knowledge scales.
Eliminating the accuracy-robustness trade-off will become
unrealistic if the student learns from the two teachers with
no equal knowledge scales. Then we want to know how to
measure the knowledge scale and further balance it.
In the knowledge distillation process, the part that teach-
ers applied to guide students is the predicted distribution
for the samples. So we can reasonably believe that the
teacherâ€™s knowledge exists in the predicted distribution
for the samples, which actually represents the information
understood by the teacher, e.g., the samplesâ€™ classes, the
samplesâ€™ difficulty, or the relationship between the classes.
In knowledge distillation framework [20], the teacherâ€™s pre-
dicted distribution can be defined as P={p1(x), ..., p C(x)},

--- PAGE 5 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5
Fig. 2. Schematic diagram of the same logits but dealing with different
temperatures Ï„and getting different predicted probabilities p. From the
figure, the temperature Ï„can impact the model prediction and can
further impact the information entropy H.
where the predicted possibility pk(x)ofk-th class can be
formulated as follows:
pk(x) =exp(zk(x)/Ï„)
PC
j=1exp(zj(x)/Ï„), (5)
where zk(x)denotes the k-th dimension output logits of the
model before the softmax layer, Ï„denotes the temperature
applied in the training process of knowledge distillation.
Although the knowledge described above exists in the
teacherâ€™s predicted distributions, how to quantify them is
not easy. Here we try to consider it from the optimization
perspective. Intuitively speaking, a well-trained network
model is trained from a network model with randomly
initialized weights. The model with randomly initialized
weights can be considered as a general model Iwithout
any knowledge; After training, the model can be considered
to possess the knowledge brought by the optimization pro-
cesses. Based on information theory, relative entropy (also
known as Kullbackâ€“Leibler divergence [36]) represents the
information cost required from one distribution to another
distribution. So the obtained knowledge scale can be de-
noted as the relative entropy from the predicted distribution
of an initialized model Ito a well-trained model. Here
we further define the teacherâ€™s knowledge scale KTin
mathematical form as follows:
KT=KL(I(x), T(x)), (6)
then we try to further simplify the knowledge scale KTand
provide Theorem 1. And the corresponding proof can be
found in the Appendix A (Proof A.1.).
Theorem 1. The teacherâ€™s knowledge scale KTis negatively
related to the information entropy H(PT)of teacherâ€™s
predicted distribution and have a relationship as follows:
KT=logCâˆ’H(PT), (7)
where PT={pT
1(x), ..., pT
C(x)}denote the teacher model
Tpredicted distribution, and logC is a constant.
Theorem 1 implies that we can directly use information
entropy to measure the teacherâ€™s knowledge scales. From
the perspective of information theory, information entropy
is used to describe the degree of randomness and uncer-
tainty of information, so it is reasonable to apply informa-
tion entropy for quantifying the knowledge scale.
Then we further analyze the factors that affect informa-
tion entropy. Based on Eq.(5), the information entropy of
predicted distribution is affected by two parts, one is theTABLE 1
The quantitative statistic of knowledge scale for clean teacher
ResNet-56 and robust teacher WideResNet-34-10 on CIFAR-10. We
count the teachersâ€™ knowledge scales before and after the adjustment
of the Entropy-Based balance algorithm. The results are based on the
statistics of the teachersâ€™ knowledge scales in the final training epoch.
TeacherKnowledge Scale
(No Balance)Knowledge Scale
(After Balance)
ResNet-56 2.2518 1.2480
WideResNet-34-10 1.3379 1.3249
output logits z(x), and the other is the temperature Ï„, both
of them can change the information entropy to influence the
knowledge scale. The output logits z(x)is determined by
the model itself, when the teacher is selected, z(x)cannot
be changed. While the temperature can be used as a hyper-
parameter to directly change information entropy.
In order to further explore the relationship between in-
formation entropy and temperature, we calculate the partial
derivative of information entropy H(P)with respect to
temperature Ï„and can get the results as follows:
âˆ‡Ï„H(P) =(PC
j=1qj)(PC
j=1qjlog2qj)âˆ’(PC
j=1qjlogqj)2
Ï„(PC
j=1qj)2,
(8)
where the qrepresents the exp(z(x)/Ï„)in Eq.(5), and the
more detailed derivation can be found in Appendix A
(Proof A.2.). Meanwhile, we prove that the partial derivative
âˆ‡Ï„H(P)â‰¥0, and this conclusion is proved in Appendix A
(Proof A.3.), thus the information entropy H(P)shows a
monotonically increasing relationship with temperature Ï„.
The effect is as shown in Fig. 2: when the temperature in-
creases, the predictionâ€™s information entropy also increases
obviously. Then combined with Theorem 1, a larger temper-
ature will lead to a smaller knowledge scale.
3.2.2 Balancing Knowledge Scale
After measuring the knowledge scale, we want to know the
knowledge scales of different types of teachers, we conduct
quantitative statistics, and the results are shown in Table
1. The results denote that an obvious difference exists in
the knowledge scale between the clean teacher and the
robust teacher if without additional adjustment. So it is very
necessary to balance the knowledge scale between different
teachers. Only when two teachers appear to be equally
knowledgeable, the student will equally learn abilities from
both the clean teacher and the robust teacher, and finally
mitigate the trade-off between accuracy and robustness.
Based on the above analysis, we can balance the clean
teacherâ€™s knowledge scale and robust teacherâ€™s knowledge
scale by minimizing the gap of teachersâ€™ information en-
tropy. Here we define the minimization of knowledge scalesâ€™
gapâˆ†Kas optimization objective in mathematical form:
min âˆ† K=||H(PTadv
adv)âˆ’H(PTnat
nat)||, (9)
where PTnat
nat andPTadv
advdenote the clean teacherâ€™s pre-
dicted distribution towards xnatand adversarial teacherâ€™s
predicted distribution towards xadv, respectively.
Based on the above analysis in Section 3.2.1, adjusting
temperature can directly transform the information entropy

--- PAGE 6 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6
of the modelâ€™s prediction with negligible calculation cost,
while adjusting logits z(x)needs an additional calculation
consumption for re-train models. So we select to solve the
above optimization goal Eq. (9) by updating the multiple
teachersâ€™ temperatures.
Then we need to apply the gradient of the expected
object âˆ†Kconcerning the clean teacherâ€™s temperature Ï„nat
and the robust teacherâ€™s temperature Ï„advand update them
according to the gradient descent principle in the training
process. Here we apply the optimization process of temper-
ature Ï„natto expand our method (the optimization process
of temperature Ï„advis similar, and we do not repeat the
description), which is formulated as follows:
Ï„nat=Ï„natâˆ’rÏ„âˆ‡Ï„natâˆ†K, (10)
we use the learning rate rÏ„to control the magnitude of the
gradient change, and then we further expand the prelimi-
nary derivation:
âˆ‡Ï„natâˆ†K=(
âˆ‡Ï„natH(PTnat
nat), H (PTnat
nat)â‰¥H(PTadv
adv),
âˆ’âˆ‡Ï„natH(PTnat
nat), H (PTnat
nat)< H(PTadv
adv).
(11)
As described before (Eq.(8)), the information entropy
H(P)shows a monotonically increasing relationship with
the temperature Ï„. So based on the consideration of the
stable update, we can reasonably use the constant 1 to
estimate both the âˆ‡Ï„natH(PTnat
nat)andâˆ‡Ï„advH(PTadv
adv)in the
training process of updating the teacherâ€™s temperature, and
we use rÏ„to control the update range.
Then combined the above proof with Eq. (10) and Eq.
(11), the formula for updating the Ï„natis as follows:
Ï„nat=Ï„natâˆ’rÏ„sign(H(PTnat
nat)âˆ’H(PTadv
adv)), (12)
where sign(.)denotes the sign function. According to the
same derivation process, we can also obtain the formula
for updating the temperature Ï„advfor teacher Tadvcan be
formulated as follows:
Ï„adv=Ï„advâˆ’rÏ„sign(H(PTadv
adv)âˆ’H(PTnat
nat)). (13)
On a practical level, our Entropy-Based Balance algo-
rithm can balance the teacherâ€™s knowledge scale by control-
ling the information entropy. When the information entropy
of teachers is not equal, both teachersâ€™ temperatures will be
updated until the difference is eliminated: if a teacher has
a high information entropy, then the teacherâ€™s temperature
will reduce to further reduce the information entropy, lead-
ing to the higher knowledge scale based on Eq.(27); on the
contrary, if a teacher has a low information entropy, then
the teacherâ€™s temperature will increase to further increase
the information entropy, leading to the lower knowledge
scale. With the Entropy-Based Balance, the clean teacher
and the robust teacher can appear equally knowledgeable
when imparting knowledge to the student, which is helpful
to mitigate the accuracy-robustness trade-off.
It should be mentioned that we only update the teachersâ€™
temperatures ( Ï„natandÏ„adv) and keep the studentâ€™s tem-
perature ( Ï„s) unchanged in the Entropy-Based Balance. In
addition, in order to prevent information from being over-
regulated, we set an upper and lower bound for the temper-
ature to prevent the following situations: If the temperatureis too small, the teacherâ€™s predicted distribution will be
infinitely close to the one-hot labelâ€™s distribution; When the
temperature is too large, the teacherâ€™s predicted distribution
will be close to the uniform distribution.
3.3 Normalization Loss Balance in B-MTARD
Although the two teachers are equally knowledgeable after
the adjustment of the Entropy-Based Balance algorithm,
the student may not learn equal knowledge from the two
teachers during the training process, since the difficulties
of recognizing clean examples and adversarial examples for
the student are different: Adversarial examples are gener-
ated to mislead the student based on clean examples. In
order to let the student get equal knowledge from the clean
teacher and the robust teacher, a strategy is needed to con-
trol the studentâ€™s relative learning speed toward different
types of knowledge. Inspired by gradient regularization
methods in multi-task learning [7], we propose an algorithm
to control the relative speed by dynamically adjusting the
loss weights in the entire training process, which is called
the Normalization Loss Balance algorithm.
On the mathematical level, the total loss in B-MTARD
ultimately used for the student update at time tcan be
represented as Ltotal(t), which can be formulated as follows:
Ltotal(t) =wnat(t)Lnat(t) +wadv(t)Ladv(t), (14)
where the Lnat(t)andLadv(t)denote the KL loss mentioned
in Eq. (3), and the wnat(t)andwadv(t)denote the constant
hyper-parameters 1âˆ’Î±andÎ±mentioned in Eq. (3) in our
task without the Normalization Loss Balance. Similar to the
analysis in section 3.2, the knowledge scalesâ€™ gap between
the student and the teacher can be directly reflected by
the value of the clean loss Lnat(t)and adversarial loss
Ladv(t), and the vital to control the studentâ€™s knowledge
from multiple teachers consistent is controlling the loss
weight of wnat(t)andwadv(t), which directly influence the
studentâ€™s learning of Lnat(t)andLadv(t).
Then our goal is to place Lnat(t)andLadv(t)on a
common metric through their relative magnitudes. Then
based on the common metric, we can identify specific opti-
mization directions to ensure that Lnat(t)andLadv(t)have
a relatively fair descent after the entire update process, and
the final trained model can be equally affected by different
parts of the total loss.
To select the criterion metric to measure the decline of
multiple losses, we choose a relative loss ËœL(t)following
Chen et al. [7], which is defined as follows:
ËœLnat(t) =Lnat(t)/Lnat(0),ËœLadv(t) =Ladv(t)/Ladv(0),
(15)
especially in our task, Lnat(t)andLadv(t)represent the
knowledge scalesâ€™ gap for the student from the clean teacher
Tnat and robust teacher Tadv at time t. So ËœLnat(t)and
ËœLadv(t)can reflect the knowledge proportion that the stu-
dent has not learned from the different teacherâ€™s knowl-
edge at the time tcompared with time 0. When ËœLnat(t)
andËœLadv(t)are equal, the student obtains relatively equal
knowledge scales from different teachers, which is exactly
what we search for. So our optimization goal is as follows:
min||ËœLnat(t)âˆ’ËœLadv(t)||. (16)

--- PAGE 7 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7
During the studentâ€™s training process, the most direct
impact on ËœLnat(t)andËœLadv(t)is the loss weight wnatand
wadv. Intuitively speaking, when ËœLnat(t)is smaller than
ËœLadv(t), the student has learned more knowledge scales
from the clean teacher compared with the robust teacher
during the 0 to t time, and the student is supposed to have
smaller loss weight wnatin the learning process to limit the
learning speed from this type of knowledge.
Based on the above criterion and analysis, we can dy-
namically balance ËœLnat(t)andËœLadv(t)by applying a relative
weight rnat(t)andradv(t)that we expected for loss at time
t, which can be formulated as follows:
rnat(t) = [ËœLnat(t)]Î²/[ËœLnat(t)]Î²+ [ËœLadv(t)]Î², (17)
radv(t) = [ËœLadv(t)]Î²/[ËœLnat(t)]Î²+ [ËœLadv(t)]Î², (18)
where [ËœL(t)]Î²denotes the ËœL(t)power of Î², and Î²is set
to control the strength of correcting the imbalance. The
rnat(t)andradv(t)strengthen the disadvantaged losses and
weaken the advantaged losses to eliminate the imbalance
between the studentâ€™s learning speeds in different types of
knowledge. The big Î²is applicable when the vibration of
relative loss value is obvious, while the small Î²is suitable
forËœLnat(t)andËœLadv(t)with stable influence abilities.
From the consideration of training stability, we do not
directly use rnat(t)andradv(t)as the weight of the loss, but
update the original weight wnat(t)andwadv(t), which can
be formulated as follows:
wnat(t) =rwrnat(t) + (1 âˆ’rw)wnat(tâˆ’1), (19)
wadv(t) =rwradv(t) + (1 âˆ’rw)wadv(tâˆ’1). (20)
On a practical level, with the assistance of the Nor-
malization Loss Balance algorithm, the student can dy-
namically adjust the different loss weights in the whole
training process according to the degree of acceptance of
knowledge. The studentâ€™s learning speed will be inhibited
and the corresponding loss weight will decrease if too much
of this type of knowledge is accepted; On the contrary, the
studentâ€™s learning speed will be accelerated and the corre-
sponding loss weight will increase if this type of knowledge
is insufficient. Finally, the student tends to learn well from
both two teachers, gaining relatively equal clean and robust
knowledge, rather than appearing to have a partial ability.
3.4 Overview of B-MTARD
All in all, to transfer both clean and robust knowledge
to the student, we propose the B-MTARD to apply two
teachers and propose the Entropy-Based Balance algorithm
for teachers and the Normalization Loss Balance algorithm
for the student to optimize the Adversarial Training process.
The final minimization of B-MTARD is defined as follows:
arg min
Î¸SwnatKL(S(xnat;Ï„s), Tnat(xnat;Ï„nat))
+wadvKL(S(xadv;Ï„s), Tadv(xadv;Ï„adv)),(21)
and the complete process of B-MTARD is in Algorithm 1.
Compared with other existing methods, our method
has several advantages. Firstly, our method can fit theAlgorithm 1 Overview of B-MTARD
Require Initialize student Swith weight Î¸Sand temper-
ature Ï„s, pretrained teacher TnatandTadv, the initial
temperature Ï„natandÏ„advforTnatandTadv, respec-
tively, the benign clean examples xnatand the labels y,
the threat bound â„¦.
1:fort= 0 tomax -step do
2: Acquire adversarial example xadvby Eq. (4).
3: GetLnat(t) =KL(S(xnat;Ï„s), Tnat(xnat;Ï„nat)).
4: GetLadv(t) =KL(S(xadv;Ï„s), Tadv(xadv;Ï„adv)).
5: ifepoch = 0 andt= 0then
6: Record LnatandLadvasLnat(0)andLadv(0).
7: end if
8: Update wnat(t)andwadv(t)by Eq. (19) and Eq. (20).
9: Update Î¸Sby Eq. (21).
10: Update Ï„natandÏ„advby Eq. (12) and Eq. (13).
11:end for
different teacher-student combinations, which can be auto-
matically adjusted by the Entropy-Based Balance algorithm
and Normalization Loss Balance algorithm. Secondly, our
method pays more attention to the overall performance
based on Weighted Robust Accuracy, which measures the
trade-off between accuracy and robustness. Thirdly, the
training hyper-parameters can be dynamically updated by
our adjustment algorithm and can fit the changes as training
epochs increase, which is important to fit various scenarios.
4 E XPERIMENTS
In this section, we initially describe the experimental setting
and then evaluate the Weighted Robust Accuracy of our
B-MTARD and several baseline methods under prevailing
white-box attacks and black-box attacks including transfer-
based and query-based attacks. We also conduct a series of
ablation studies to demonstrate the effectiveness.
4.1 Experimental Settings
We conduct experiments on three datasets including CIFAR-
10 [22], CIFAR-100, and Tiny-ImageNet [24]. We apply the
standard training method and several state-of-the-art meth-
ods for comparison: Adversarial Training method: SAT [31],
Adversarial Robustness Distillation methods: ARD [13],
RSLAD [61], Fair-ARD [49], and ABSLD [58]; Mitigating
Trade-off methods: TRADES [55] and SCORE [32]. Mean-
while, we also show the results of our conference version
for comparison: MTARD [59].
Student and Teacher Networks. Here we consider two
student networks for CIFAR-10 and CIFAR-100 including
ResNet-18 [17] and MobileNet-v2 [34]. For Tiny-ImageNet,
we select PreActResNet-18 [18] and MobileNet-v2 follow-
ing previous work. As for teacher models, we choose
clean teacher networks including ResNet-56 for CIFAR-10,
WideResNet-22-6 [54] for CIFAR-100, and PreActResNet-
34 [18] for Tiny-ImageNet. Three robust teacher networks
include WideResNet-34-10 for CIFAR-10, WideResNet-70-
16 [15] for CIFAR-100, and PreActResNet-34 for Tiny-
ImageNet. For CIFAR-10, WideResNet-34-10 is trained using
TRADES [55]; For CIFAR-100, we use the WideResNet-70-16

--- PAGE 8 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8
TABLE 2
Performance of the teacher networks used in our experiments.
Dataset Model Clean Acc FGSM PGD sat PGD trades CWâˆ AA Type
CIFAR-10ResNet-56 93.18% 19.23% 0% 0% 0% 0% Clean
WideResNet-34-10 84.91% 61.14% 55.30% 56.61% 53.84% 53.08% Robust
CIFAR-100WideResNet-22-6 76.65% 4.85% 0% 0% 0% 0% Clean
WideResNet-70-16 60.96% 35.89% 33.58% 33.99% 31.05% 30.03% Robust
Tiny-ImageNetPreActResNet-34 66.05% 2.50% 0.04% 0.09% 0% 0% Clean
PreActResNet-34 52.76% 27.05% 24.00% 24.63% 20.07% 18.92% Robust
TABLE 3
White-box robustness on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The results of CIFAR-10 and CIFAR-100 are based on the
ResNet-18, and the results of Tiny-ImageNet are based on the PreActResNet-18. All results are the best checkpoint based on W-Robust Acc.
FGSM [14] PGD sat[31] PGD trades [55] CWâˆ[4] AutoAttack [10]
DataSet Defense Clean Robust W-Robust Robust W-Robust Robust W-Robust Robust W-Robust Robust W-Robust
CIFAR-10Natural 94.57% 18.60% 56.59% 0% 47.29% 0% 47.29% 0% 47.29% 0% 47.29%
SAT [31] 84.20% 55.59% 69.90% 45.95% 65.08% 48.12% 66.16% 45.97% 65.09% 43.49% 63.85%
TRADES [55] 83.00% 58.35% 70.68% 52.35% 67.68% 53.83% 68.42% 50.23% 66.62% 49.03% 66.02%
ARD [13] 84.11% 58.40% 71.26% 50.93% 67.52% 52.96% 68.54% 50.15% 67.13% 48.20% 66.16%
RSLAD [61] 83.99% 60.41% 72.20% 53.94% 68.97% 55.73% 69.86% 52.67% 68.33% 50.98% 67.49%
SCORE [32] 84.43% 59.84% 72.14% 53.72% 69.08% 55.21% 69.82% 50.46% 67.45% 49.25% 66.84%
Fair-ARD [49] 83.41% 58.91% 71.16% 52.00% 67.71% 53.77% 68.59% 51.07% 67.24% 49.21% 66.31%
ABSLD [58] 83.21% 60.22% 71.72% 54.63% 68.92% 56.10% 69.66% 52.04% 67.63% 50.60% 66.91%
MTARD [59] 87.36% 61.20% 74.28% 50.73% 69.05% 53.60% 70.48% 48.57% 67.97% 46.18% 66.77%
B-MTARD (ours) 88.20% 61.42% 74.81% 51.68% 69.94% 54.40% 71.30% 49.88% 69.04% 47.44% 67.82%
CIFAR-100Natural 75.18% 7.96% 41.57% 0% 37.59% 0% 37.59% 0% 37.59% 0% 37.59%
SAT [31] 56.16% 25.88% 41.02% 21.18% 38.67% 22.02% 39.09% 20.90% 38.53% 19.76% 37.96%
TRADES [55] 57.75% 31.36% 44.56% 28.05% 42.90% 28.88% 43.32% 24.19% 40.97% 23.26% 40.51%
ARD [13] 60.11% 33.61% 46.86% 29.40% 44.76% 30.51% 45.31% 27.56% 43.84% 25.71% 42.91%
RSLAD [61] 58.25% 34.73% 46.49% 31.19% 44.72% 32.05% 45.15% 28.21% 43.23% 26.50% 42.38%
SCORE [32] 56.40% 32.94% 44.67% 30.27% 43.34% 30.56% 43.48% 26.30% 41.35% 24.74% 40.57%
Fair-ARD [49] 57.81% 34.39% 46.10% 30.64% 44.23% 31.50% 44.66% 27.84% 42.83% 26.22% 42.02%
ABSLD [58] 56.77% 34.94% 45.86% 32.41% 44.59% 32.99% 44.88% 26.99% 41.88% 25.38% 41.08%
MTARD [59] 64.30% 31.49% 47.90% 24.95% 44.63% 26.75% 45.53% 23.42% 43.86% 21.31% 42.81%
B-MTARD (ours) 65.08% 34.21% 49.65% 28.50% 46.79% 29.94% 47.51% 25.45% 45.27% 23.98% 44.53%
Tiny-ImageNetNatural 65.03% 1.73% 33.38% 0.03% 32.53% 0.05% 32.54% 0% 32.52% 0% 32.52%
SAT [31] 50.08% 25.35% 37.72% 22.24% 36.16% 23.05% 36.57% 20.48% 35.28% 18.54% 34.31%
TRADES [55] 48.45% 23.59% 36.02% 21.59% 35.02% 22.09% 35.27% 17.33% 32.89% 16.66% 32.56%
ARD [13] 53.22% 27.97% 40.60% 24.92% 39.07% 25.71% 39.47% 21.41% 37.32% 19.91% 36.57%
RSLAD [61] 48.78% 27.26% 38.02% 25.00% 36.89% 25.45% 37.12% 20.87% 34.83% 17.11% 32.95%
SCORE [32] 10.05% 7.80% 8.93% 7.65% 8.85% 7.67% 8.86% 6.19% 8.13% 5.97% 8.01%
Fair-ARD [49] 46.64% 25.81% 36.23% 23.91% 35.28% 24.29% 35.47% 19.59% 33.12% 17.51% 32.08%
ABSLD [58] 47.21% 25.79% 36.50% 23.59% 35.40% 23.98% 35.60% 19.50% 33.36% 17.84% 32.53%
MTARD [59] 52.98% 26.41% 39.70% 22.55% 37.77% 23.41% 38.20% 19.36% 36.17% 16.84% 34.91%
B-MTARD (ours) 56.81% 28.12% 42.47% 23.93% 40.37% 24.94% 40.88% 19.69% 38.25% 17.71% 37.26%
model provided by Gowal et al. [15]; In Addition, for Tiny-
ImageNet, we use PreActResNet-34 trained by TRADES
[55] as the robust teacher. The teachers are pre-trained
and will not be changed during the training process. The
performance of these teacher models is shown in Table 2.
Training Setting. We train the student using Stochastic
Gradient Descent (SGD) optimizer with an initial learning
rate of 0.1, a momentum of 0.9, and a weight decay of 2e-4.
For B-MTARD, the weight loss learning rate rwis initially
set as 0.025; the temperature learning rate rÏ„is initially set
as 0.001. the studentâ€™s temperature Ï„sis set to constant 1.
Meanwhile, the Ï„natandÏ„advare initially set as 1 without
additonal instruction, and the Ï„natandÏ„adv are initially
set as 2 for MobileNet-v2 on CIFAR-10. The maximum and
minimum values of temperature are 10 and 1, respectively.
For CIFAR-10 and CIFAR-100, we set the total number of
training epochs to 300. The learning rate is divided by 10
at the 215-th, 260-th, and 285-th epochs; For Tiny-ImageNet,
we set the total number of training epochs to 100, and the
learning rate is divided by 10 at the 75-th and 90-th epochs.
We set the batch size to 128 (CIFAR-10 and CIFAR-100)
and 32 (Tiny-ImageNet), and Î²is set to 1. For the innermaximization of B-MTARD, we use a 10-step PGD with
a random start size of 0.001 and a step size of 2/255. All
training perturbation in the maximization is bounded to the
Lâˆnorm Ïµ= 8/255. For the training of the natural model,
we train the networks for 100 epochs, and the learning rate
is divided by 10 at the 75-th and 90-th epochs.
For the comparison methods, we follow SAT, TRADES,
SCORE, ARD, RSLAD, Fair-ARD, and ABSLD original set-
tings without additional instruction. The SCORE version is
the TRADES+LSE without additional data. For ARD, we
use the same robust teacher as RSLAD and MTARD. The
temperature Ï„of ARD is set to 30 on CIFAR-10 while set
to 5 on CIFAR-100 and Tiny-ImageNet, and the Î±is set to
0.95 following [13] on CIFAR-100 and Tiny-ImageNet. The
learning rate of SAT and SCORE for Tiny-ImageNet is 0.01
to achieve better performance.
Evaluation Setting. The same as previous studies, we
evaluate the models against white box adversarial attacks:
FGSM [14], PGD sat[31], PGD trades [55], CW âˆ[4], which are
commonly used adversarial attacks in adversarial robust-
ness evaluation. Meanwhile, we also apply a strong attack:
AutoAttack (AA) [10] (an evaluation tool in RobustBench

--- PAGE 9 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9
TABLE 4
White-box robustness on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The results of three datasets are trained based on the
MobileNet-v2. All results are the best checkpoint based on W-Robust Acc.
FGSM [14] PGD sat[31] PGD trades [55] CWâˆ[4] AutoAttack [10]
DataSet Defense Clean Robust W-Robust Robust W-Robust Robust W-Robust Robust W-Robust Robust W-Robust
CIFAR-10Natural 93.35% 12.22% 52.79% 0% 46.68% 0% 46.68% 0% 46.68% 0% 46.68%
SAT [31] 83.87% 55.89% 69.88% 46.84% 65.36% 49.14% 66.51% 46.62% 65.25% 43.43% 63.65%
TRADES [55] 77.95% 53.75% 65.85% 49.06% 63.51% 50.27% 64.11% 46.06% 62.01% 45.18% 61.66%
ARD [13] 83.43% 57.03% 70.23% 49.50% 66.47% 51.70% 67.57% 48.96% 66.20% 46.60% 65.02%
RSLAD [61] 83.20% 59.47% 71.34% 53.25% 68.23% 54.76% 68.98% 51.78% 67.49% 50.23% 66.72%
SCORE [32] 82.32% 58.43% 70.38% 53.42% 67.87% 54.46% 68.39% 49.18% 65.75% 48.39% 65.36%
Fair-ARD [49] 82.65% 56.37% 69.51% 50.50% 66.58% 52.12% 67.39% 51.07% 66.86% 47.68% 65.17%
ABSLD [58] 82.50% 58.47% 70.49% 52.98% 67.74% 54.49% 68.50% 50.20% 66.35% 48.65% 65.58%
MTARD [59] 89.26% 57.84% 73.55% 44.16% 66.71% 47.99% 68.63% 43.42% 66.34% 41.02% 65.14%
B-MTARD (ours) 89.09% 58.79% 73.94% 47.56% 68.33% 50.44% 69.77% 46.81% 67.95% 44.58% 66.84%
CIFAR-100Natural 74.86% 5.94% 40.40% 0% 37.43% 0% 37.43% 0% 37.43% 0% 37.43%
SAT [31] 59.19% 30.88% 45.04% 25.64% 42.42% 26.96% 43.08% 25.01% 42.10% 22.84% 41.02%
TRADES [55] 55.41% 30.28% 42.85% 23.33% 39.37% 28.42% 41.92% 27.72% 41.57% 22.61% 39.01%
ARD [13] 60.45% 32.77% 46.61% 28.69% 44.57% 29.63% 45.04% 26.55% 43.50% 24.86% 42.66%
RSLAD [61] 59.01% 33.88% 46.45% 30.19% 44.60% 31.19% 45.10% 27.98% 43.50% 26.33% 42.67%
SCORE [32] 49.38% 29.28% 39.33% 27.03% 38.21% 27.53% 38.46% 23.29% 36.34% 21.92% 35.65%
Fair-ARD [49] 59.18% 34.07% 46.63% 30.15% 44.67% 31.26% 45.22% 27.55% 43.37% 25.84% 42.51%
ABSLD [58] 56.67% 33.85% 45.26% 31.28% 43.98% 31.90% 44.29% 26.40% 41.54% 24.56% 40.62%
MTARD [59] 67.01% 32.42% 49.72% 25.14% 46.08% 27.10% 47.06% 24.14% 45.58% 21.61% 44.31%
B-MTARD (ours) 66.13% 34.36% 50.25% 28.47% 47.30% 29.82% 47.98% 26.50% 46.32% 24.01% 45.07%
Tiny-ImageNetNatural 62.09% 0.72% 31.41% 0% 31.05% 0% 31.05% 0% 31.05% 0% 31.05%
SAT [31] 49.03% 23.38% 36.21% 20.31% 34.67% 21.15% 35.09% 18.69% 33.86% 17.35% 33.19%
TRADES [55] 43.81% 20.10% 31.96% 18.16% 30.99% 18.36% 31.09% 13.47% 28.66% 12.82% 28.32%
ARD [13] 45.53% 22.88% 33.21% 20.43% 32.98% 21.00% 33.27% 16.81% 31.17% 15.40% 30.47%
RSLAD [61] 45.69% 24.09% 34.89% 22.30% 34.00% 22.74% 34.22% 18.63% 32.16% 17.09% 31.39%
SCORE [32] 28.27% 17.47% 22.87% 16.48% 22.38% 16.69% 22.48% 13.25% 20.76% 12.30% 20.29%
Fair-ARD [49] 47.24% 25.31% 36.28% 23.37% 35.31% 23.77% 35.51% 20.04% 33.64% 18.09% 32.67%
ABSLD [58] 48.08% 26.21% 37.15% 26.21% 37.15% 24.39% 36.24% 20.10% 34.09% 18.38% 33.23%
MTARD [59] 50.50% 23.94% 37.22% 20.45% 35.48% 21.20% 35.85% 17.45% 33.98% 15.06% 32.78%
B-MTARD (ours) 52.98% 25.60% 39.29% 21.58% 37.28% 22.58% 37.78% 18.08% 35.53% 15.68% 34.33%
[8]) to evaluate the robustness. The step sizes of PGDsatand
PGDtrades are 2/255 and 0.003, respectively, and the step is
20. The total step of CWâˆis 30. Maximum perturbation is
bounded to the Lâˆnorm Ïµ= 8/255 for all attacks. Mean-
while, we conduct a black-box evaluation, which includes
the transfer-based attack and query-based attack to test the
robustness in a near-real environment. As for the transfer-
based attack, we choose robust teachers (WideResNet-34-
10 for CIFAR-10, WideResNet-70-16 for CIFAR-100, and
PreActResNet-34 for Tiny-ImageNet) as the surrogate mod-
els to produce adversarial example against the PGDtrades and
CWâˆattack; As for the query-based attack, we choose a
score-based attack: Square Attack [2] and a decision-based
attack: RayS Attack [6], and the queries are 100.
Here, we use Weighted Robust Accuracy (W-Robust Acc)
[16] to evaluate the trade-off between the clean and robust
accuracy of the model, it is defined as follows:
Af=Ï€natPnat[f(x) =y] +Ï€advPadv[f(x) =y], (22)
where W-Robust Acc Afare the accuracy of a model fon
x drawn from either the clean distribution Pnatand the
adversarial distribution Padv. We set Ï€natandÏ€advboth
to 0.5, which means accuracy and robustness are equally
important for comprehensive performance in the model.
4.2 Ablation Study
To better understand the impact of each component in
our B-MTARD, we conduct a set of ablation studies. The
baseline denotes using a clean teacher and a robust teacher
to guide the student from both aspects, respectively, where
the weight wnatandwadvare constant at 0.5. Baseline+NLB
/uni00000033/uni0000002a/uni00000027sat /uni00000033/uni0000002a/uni00000027trades /uni00000026/uni0000003a
/uni00000019/uni00000019/uni00000019/uni0000001a/uni00000019/uni0000001b/uni00000019/uni0000001c/uni0000001a/uni00000013/uni0000001a/uni00000014/uni0000001a/uni00000015/uni0000001a/uni00000016/uni0000003a/uni00000010/uni00000035/uni00000052/uni00000045/uni00000058/uni00000056/uni00000057/uni00000003/uni00000024/uni00000046/uni00000046/uni00000008/uni00000019/uni0000001b/uni00000011/uni00000014/uni0000001b/uni00000019/uni0000001c/uni00000011/uni0000001c/uni00000015
/uni00000019/uni0000001a/uni00000011/uni00000018/uni00000019/uni0000001c/uni00000011/uni00000013/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000017/uni0000001b
/uni00000019/uni0000001b/uni00000011/uni00000014/uni00000019/uni0000001c/uni00000011/uni00000019/uni00000014/uni0000001a/uni00000014/uni00000011/uni00000013/uni0000001a
/uni00000019/uni0000001b/uni00000011/uni0000001b/uni0000001c/uni00000019/uni0000001c/uni00000011/uni0000001c/uni00000017/uni0000001a/uni00000014/uni00000011/uni00000016
/uni00000019/uni0000001c/uni00000011/uni00000013/uni00000017/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000028/uni00000025/uni00000025
/uni00000025/uni00000010/uni00000030/uni00000037/uni00000024/uni00000035/uni00000027Fig. 3. Ablation study with ResNet-18 student trained using variants
of our B-MTARD and Baseline method on CIFAR-10. NLB and EBB
represent Normalization Loss Balance (NLB) and Entropy-Based Bal-
ance (EBB). B-MTARD is our final version, which represents Base-
line+NLB+EBB. The Baseline+NLB is the result in our ECCV version
[59]. All the results are the best checkpoint based on W-Robust Acc.
term denotes adding the Normalization Loss Balance (NLB)
algorithm to dynamically adjust the weight wnatandwadv
based on the Baseline, which is also the method in our ECCV
version (MTARD) [59]. Baseline+EBB term denotes adding
the Entropy-Based Balance (EBB) algorithm to dynamically
adjust the Ï„natandÏ„advbased on the Baseline, where the
weight wnatandwadvare constant at 0.5. B-MTARD is the
final version of our method, which applies both Normal-
ization Loss Balance and Entropy-Based Balance algorithm.
The performance is shown in Fig. 3. The change of total loss
Ltotal in the training process is shown in Fig. 4, and the
change of relative loss ËœLnatandËœLadvin the training process
is shown in Fig. 5. The change for the teacherâ€™s information

--- PAGE 10 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10
TABLE 5
Black-box robustness on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The results of CIFAR-10 and CIFAR-100 are based on the
ResNet-18, and the results of Tiny-ImageNet are based on the PreActResNet-18. All results are the best checkpoint based on W-Robust Acc.
PGD trades [55] CWâˆ[4] Square Attack [2] RayS Attack [6]
DataSet Defense Clean Robust W-Robust Robust W-Robust Robust W-Robust Robust W-Robust
CIFAR-10SAT [31] 84.20% 64.84% 74.52% 63.84% 74.02% 72.48% 78.34% 74.13% 79.17%
TRADES [55] 83.00% 63.61% 73.31% 62.83% 72.92% 72.49% 77.75% 73.59% 78.30%
ARD [13] 84.11% 63.50% 73.81% 62.86% 73.49% 74.60% 79.36% 75.92% 80.02%
RSLAD [61] 83.99% 63.96% 73.98% 63.05% 73.52% 72.47% 78.23% 75.60% 79.80%
SCORE [32] 84.43% 64.34% 74.39% 63.79% 74.11% 74.68% 79.56% 75.87% 80.15%
Fair-ARD [49] 83.41% 63.31% 73.36% 62.76% 73.09% 74.08% 78.75% 75.38% 79.40%
ABSLD [58] 83.21% 62.91% 73.06% 62.19% 72.70% 73.93% 78.57% 74.90% 79.06%
MTARD [59] 87.36% 65.26% 76.31% 64.58% 75.97% 78.58% 82.97% 80.13% 83.75%
B-MTARD (ours) 88.20% 65.29% 76.75% 64.64% 76.42% 79.82% 84.01% 80.98% 84.59%
CIFAR-100SAT [31] 56.16% 38.10% 47.13% 39.42% 47.79% 40.05% 48.11% 40.92% 48.54%
TRADES [55] 57.75% 38.20% 47.98% 38.63% 48.19% 42.51% 50.13% 42.77% 50.26%
ARD [13] 60.11% 39.53% 49.82% 38.85% 49.48% 47.20% 53.66% 48.04% 56.08%
RSLAD [61] 58.25% 39.93% 49.09% 39.67% 48.96% 45.32% 51.79% 45.77% 52.01%
SCORE [32] 56.40% 39.09% 47.75% 40.02% 48.21% 43.98% 50.34% 44.57% 50.49%
Fair-ARD [49] 57.81% 39.56% 48.69% 39.10% 48.46% 44.99% 51.40% 45.54% 51.68%
ABSLD [58] 56.77% 38.42% 47.60% 38.11% 47.44% 44.09% 50.43% 44.57% 50.67%
MTARD [59] 64.30% 41.46% 52.88% 41.18% 52.74% 48.13% 56.22% 49.73% 57.02%
B-MTARD (ours) 65.08% 42.11% 53.60% 41.35% 53.22% 49.40% 57.24% 50.66% 57.87%
Tiny-ImageNetSAT [31] 50.08% 33.40% 41.74% 33.20% 41.63% 38.72% 44.40% 39.35% 44.72%
TRADES [55] 48.45% 31.01% 39.73% 30.72% 39.59% 36.58% 42.52% 37.22% 42.84%
ARD [13] 53.22% 34.74% 43.98% 33.32% 43.27% 42.58% 47.90% 42.87% 48.05%
RSLAD [61] 48.78% 32.85% 40.82% 32.09% 40.44% 37.64% 43.21% 38.15% 43.47%
SCORE [32] 10.05% 8.74% 9.40% 8.82% 9.44% 8.67% 9.36% 8.70% 9.38%
Fair-ARD [49] 46.64% 31.58% 39.11% 31.38% 39.01% 35.81% 41.23% 36.44% 41.54%
ABSLD [58] 47.21% 31.84% 39.53% 31.66% 39.44% 36.77% 41.99% 37.42% 42.32%
MTARD [59] 52.98% 34.48% 43.73% 33.80% 43.39% 41.70% 47.34% 42.55% 47.77%
B-MTARD (ours) 56.81% 36.65% 46.73% 33.40% 45.11% 44.46% 50.64% 45.51% 51.16%
0 50 100 150 200 250 300
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni000000560.0000.0250.0500.0750.1000.1250.1500.1750.200/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000035/uni00000036/uni0000002f/uni00000024/uni00000027
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025
240 250 260 270 280 290 300
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni000000560.0040.0060.0080.0100.0120.0140.016/uni00000037/uni00000052/uni00000057/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000035/uni00000036/uni0000002f/uni00000024/uni00000027
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025
/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000035/uni00000036/uni0000002f/uni00000024/uni00000027/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051
/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051
/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025/uni00000003/uni00000030/uni00000048/uni00000044/uni00000051
Fig. 4. The total loss Ltotal with the ResNet-18 student trained using
variants of RSLAD, Baseline, and Baseline+NLB on CIFAR-10. NLB is
abbreviation of Normalization Loss Balance. The y-axis is the Ltotal in
the training epoch x. The left is the change curve of Ltotal in the whole
training process, the right is the curve of Ltotal in the final 60 epochs.
0 50 100 150 200 250 300
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni000000560.00.20.40.60.81.0/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000051/uni00000044/uni00000057/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000003/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000044/uni00000047/uni00000059/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056
0 50 100 150 200 250 300
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni000000560.00.20.40.60.81.0/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025/uni00000003/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000051/uni00000044/uni00000057/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025/uni00000003/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000059/uni00000048/uni00000003/uni00000044/uni00000047/uni00000059/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056
Fig. 5. The relative training loss ËœLwith ResNet-18 student on CIFAR-
10. The left is Baseline, and the right is Baseline+NLB. NLB represents
Normalization Loss Balance. The x-axis means the training epochs, and
the y-axis is the relative loss ËœLadvandËœLnatin the training epoch x.
entropy in the training process is shown in Fig. 6.
In Fig. 3, B-MTARDâ€™s improvement brought by each
component is remarkable, which shows its effectiveness.
Multiple teachers positively affect the student model to
learn with both clean and robust accuracy. However, it is
not enough to mitigate the trade-off between accuracy and
5 10 15 20 25 30 35 40
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni000000560.00.20.40.60.81.01.2/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000003/uni00000052/uni00000049/uni00000003Tnat/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025
/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000003/uni00000052/uni00000049/uni00000003Tadv/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048/uni0000000e/uni00000031/uni0000002f/uni00000025
5 10 15 20 25 30 35 40
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni000000560.00.20.40.60.81.01.2/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000003/uni00000052/uni00000049/uni00000003Tnat/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000025/uni00000010/uni00000030/uni00000037/uni00000024/uni00000035/uni00000027
/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni00000003/uni00000052/uni00000049/uni00000003Tadv/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000025/uni00000010/uni00000030/uni00000037/uni00000024/uni00000035/uni00000027Fig. 6. The teachersâ€™ information entropy of TadvandTnatwith ResNet-
18 student on CIFAR-10. The left is Baseline+NLB, and the right is
B-MTARD (Baseline+NLB+EBB). NLB and EBB are abbreviations of
Normalization Loss Balance and Entropy-based Balance. The x-axis
means the training epochs (the first 40 epochs), and the y-axis is the
entropy TadvandTnatin the training epoch x.
robustness without the Entropy-based Balance algorithm
and the Normalization Loss Balance algorithm, both of them
can further enhance the W-Robust performance.
Baseline+NLB outperforms Baseline by 0.87%, 0.56%,
and 0.6% against the attack of PGDsat, PGDtrades , and CWâˆ
in the metric of W-robust Acc, while B-MTARD outperforms
Baseline+EBB by 0.33%, 0.23%, and 0.15% against the attack
of PGDsat, PGDtrades , and CWâˆin the metric of W-robust
Acc. The improvement shows the effectiveness of our Nor-
malization Loss Balance Algorithm.
In addition, B-MTARD has an obvious improvement
compared with our ECCV version (Baseline + NLB): Our
B-MTARD improves W-Robust Acc by 0.89%, 0.82%, and
0.94% compared with Baseline + NLB against the attack
of PGDsat, PGDtrades , and CWâˆ. The result denotes the
necessity and effectiveness of our incremental work (the
Entropy-based Balance algorithm) in this version.
In Fig. 4, compared with RSLAD and Baseline, the

--- PAGE 11 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11
TABLE 6
Black-box robustness on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The results of three datasets are trained based on the
MobileNet-v2. All results are the best checkpoint based on W-Robust Acc.
PGD trades [55] CWâˆ[4] Square Attack [2] RayS Attack [6]
DataSet Defense Clean Robust W-Robust Robust W-Robust Robust W-Robust Robust W-Robust
CIFAR-10SAT [31] 83.87% 64.66% 74.27% 64.24% 74.06% 73.01% 78.44% 74.72% 79.30%
TRADES [55] 77.95% 61.04% 69.50% 60.66% 69.31% 67.43% 72.69% 68.91% 73.43%
ARD [13] 83.43% 63.28% 73.36% 62.83% 73.13% 73.26% 78.35% 74.62% 79.03%
RSLAD [61] 83.20% 64.33% 73.77% 63.45% 73.33% 73.07% 78.14% 74.49% 78.85%
SCORE [32] 82.32% 63.84% 73.08% 63.03% 72.68% 72.41% 77.37% 73.45% 77.89%
Fair-ARD [49] 82.65% 62.62% 72.64% 62.00% 72.32% 72.44% 77.55% 73.50% 78.08%
ABSLD [58] 82.50% 62.91% 72.71% 61.87% 72.19% 72.05% 77.28% 73.29% 77.90%
MTARD [59] 89.26% 66.30% 77.78% 65.68% 77.47% 79.13% 84.20% 80.91% 85.09%
B-MTARD (ours) 89.09% 66.47% 77.78% 65.96% 77.53% 79.61% 84.35% 81.14% 85.12%
CIFAR-100SAT [31] 59.19% 40.70% 49.95% 40.97% 50.08% 44.51% 51.85% 45.59% 52.39%
TRADES [55] 55.41% 37.76% 46.59% 38.02% 46.72% 40.71% 48.06% 41.23% 48.32%
ARD [13] 60.45% 39.15% 49.80% 38.53% 49.49% 46.95% 53.70% 47.91% 54.18%
RSLAD [61] 59.01% 40.32% 49.67% 39.92% 49.47% 45.66% 52.34% 46.33% 52.67%
SCORE [32] 49.38% 40.00% 44.69% 36.91% 43.15% 37.94% 43.66% 38.04% 43.71%
Fair-ARD [49] 59.18% 40.55% 49.87% 40.14% 49.66% 45.29% 52.24% 46.00% 52.59%
ABSLD [58] 56.67% 38.08% 47.38% 38.18% 47.43% 43.23% 49.95% 43.60% 50.14%
MTARD [59] 67.01% 43.23% 55.12% 42.92% 54.97% 50.64% 58.83% 52.49% 59.75%
B-MTARD (ours) 66.13% 42.67% 54.40% 42.04% 54.09% 50.83% 58.48% 51.84% 58.99%
Tiny-ImageNetSAT [31] 49.03% 33.47% 41.25% 33.13% 41.08% 37.95% 43.49% 38.32% 43.68%
TRADES [55] 43.81% 28.35% 36.08% 28.64% 36.23% 32.39% 38.10% 32.37% 38.09%
ARD [13] 45.53% 30.73% 38.13% 30.23% 37.88% 34.60% 40.07% 35.13% 40.33%
RSLAD [61] 45.69% 31.20% 38.45% 31.10% 38.40% 35.18% 40.44% 35.57% 40.63%
SCORE [32] 28.27% 21.82% 25.05% 22.19% 25.23% 22.16% 25.22% 22.41% 25.34%
Fair-ARD [49] 47.24% 31.80% 39.52% 31.40% 39.32% 36.53% 41.89% 37.22% 42.23%
ABSLD [58] 48.08% 32.89% 40.49% 32.43% 40.26% 38.18% 43.13% 38.48% 43.28%
MTARD [59] 50.50% 32.75% 41.63% 32.05% 41.28% 38.88% 44.69% 40.01% 45.26%
B-MTARD (ours) 52.98% 34.25% 43.62% 33.50% 43.24% 40.62% 46.80% 41.44% 47.21%
Baseline+NLBâ€™s training loss is less oscillating and can con-
verge better. In Fig. 5, the gap between ËœLnatandËœLadvcan
represent the difference of the knowledge proportion that
students have not learned from the clean teacher and the
robust teacher. The Baseline+NLBâ€™s trade-off between ËœLnat
andËœLadvare tinier. The results demonstrate Normalization
Loss Balance can indeed make the student have relatively
equal knowledge learning speeds from different teachers.
In Fig. 6, we can see that in Baseline+NLB, the informa-
tion entropy gap between the clean teacher and the robust
teacher is very obvious, but after the adjustment of our
Entropy-based Balance algorithm, the information entropy
gap quickly narrows and remains relatively consistent, and
this phenomenon can denote the knowledge scale gap
between clean teacher and robust teacher reduces, which
proves the effectiveness of our Entropy-based Balance al-
gorithm. Combined with the experimental results in Fig. 3,
reducing the difference in teacherâ€™s information entropy can
indeed improve the performance of the student.
4.3 The discussion about hyper-parameters
Hyper-parameter Ï„. The temperatures of different teachers
are automatically adjusted by our Entropy-based algorithm
until the information entropy is equal. However, the initial
temperature setting still directly influences the amount of
information entropy for both teachers and determines the
prior adjustment scope, and a proper initial temperature can
ensure temperature changes within a reasonable range to
maintain the effectiveness of teachersâ€™ knowledge. To select
the proper initial temperature value, we select initial Ï„nat
andÏ„advas 1, 2, 5, 8, while all other experimental settings
remain unchanged. Based on the results shown in Table 7,
we select 1 as our initial temperature value.TABLE 7
Study with different initial Ï„.Î²is fixed to 1, the students are ResNet-18
trained on CIFAR-10 guided by ResNet-56 and WideResNet-34-10.
Attack initial Ï„ Clean Robust W-Robust
PGD sat[31]Ï„= 1 88.20% 51.68% 69.94%
Ï„= 2 88.22% 51.43% 69.83%
Ï„= 5 89.27% 47.89% 68.58%
Ï„= 8 88.64% 46.97% 67.81%
PGD trades [55]Ï„= 1 88.20% 54.40% 71.30%
Ï„= 2 88.22% 54.13% 71.18%
Ï„= 5 89.27% 50.90% 70.09%
Ï„= 8 88.64% 49.50% 69.07%
CWâˆ[4]Ï„= 1 88.20% 49.88% 69.04%
Ï„= 2 88.22% 49.79% 69.01%
Ï„= 5 89.27% 43.99% 66.63%
Ï„= 8 88.64% 42.45% 65.55%
Hyper-parameter Î². Here, we explore the role of Î²in
Normalization Loss Balance (Eq. (17) and Eq. (18)). We
test the results of different values Î²for the training of the
student. We select Î²as 0.5, 1, 4, and 10 in our experiment,
while all other experimental settings remain unchanged.
The results are presented in Table 8.
From the result, Î²plays an important role in Normal-
ization Loss Balance. Choosing the right Î²value can affect
the studentâ€™s final performance. The lower Î²is suitable
when the student gets a similar knowledge scale from the
clean teacher and the robust teacher at the current training
epoch and the studentâ€™s status is not fluctuating much. The
larger Î²brings a more severe penalty on the student when
the student places too much emphasis on one teacherâ€™s
knowledge but ignores another. Based on the performance,
we choose the relatively best value: Î²= 1 in our setting.

--- PAGE 12 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12
TABLE 8
Study with different Î². Initial Ï„is fixed to 1, the students are ResNet-18
trained on CIFAR-10 guided by ResNet-56 and WideResNet-34-10.
Attack Î² Clean Robust W-Robust
PGD sat[31]Î²= 0.5 88.51% 50.24% 69.38%
Î²= 1 88.20% 51.68% 69.94%
Î²= 4 89.10% 50.01% 69.56%
Î²= 10 88.63% 49.09% 68.86%
PGD trades [55]Î²= 0.5 88.51% 53.43% 70.97%
Î²= 1 88.20% 54.40% 71.30%
Î²= 4 89.10% 53.04% 71.07%
Î²= 10 88.63% 52.63% 70.63%
CWâˆ[4]Î²= 0.5 88.51% 48.59% 68.55%
Î²= 1 88.20% 49.88% 69.04%
Î²= 4 89.10% 48.61% 68.86%
Î²= 10 88.63% 48.16% 68.40%
4.4 Adversarial Robustness Evaluation
White-box Robustness. The performances of ResNet-18 and
MobileNet-v2 trained by our B-MTARD and other baseline
methods under the white box attacks are shown in Table 3
and 4 for CIFAR-10, CIFAR-100, and Tiny-ImageNet.
The results in Table 3 and 4 demonstrate that B-
MTARD achieves state-of-the-art W-Robust Acc on CIFAR-
10, CIFAR-100, and Tiny-ImageNet. For ResNet-18, B-
MTARD improves W-Robust Acc by 1.44%, 2.20%, and
1.41% compared with the best baseline method against
the PGD trades on CIFAR-10, CIFAR-100, and Tiny-ImageNet;
B-MTARD improves W-Robust Acc by 0.79%, 2.76%, and
1.54% compared with the best baseline method against
the PGD trades on CIFAR-10, CIFAR-100, and Tiny-ImageNet.
Moreover, B-MTARD shows relevant superiority against
FGSM, PGD sat, CWâˆ, and AutoAttack compared with other
methods. The results show that the overall performance
of B-MTARD has different degrees of advantage whether
compared with the other baseline methods. Meanwhile, B-
MTARD also shows the corresponding superiority com-
pared with our conference version (MTARD).
Black-box Robustness. In addition, we also test B-
MTARD and other methods against black-box attacks for
ResNet-18 and MobileNet-v2 on CIFAR-10, CIFAR-100, and
Tiny-ImageNet separately. We choose the transfer-based at-
tack and query-based attack in our evaluation. We select the
best checkpoint of all the trained models based on W-Robust
Acc. The results of ResNet-18 are shown in Table 5, while the
results of MobileNet-v2 are shown in Table 6.
From the result, B-MTARD trained models achieve the
best W-Robust Acc against all three black-box attacks
compared to other models. Under the Square Attack, B-
MTARD trained ResNet-18 improves W-Robust Acc by
4.45%, 3.58%, and 2.74% on CIFAR-10, CIFAR-100, and
Tiny-ImageNet compared to the second-best method; More-
over, B-MTARD brings 5.56%, 4.78%, and 3.31% improve-
ments to MobileNet-v2 on CIFAR-10, CIFAR-100, and Tiny-
ImageNet. In addition, B-MTARD has different margins in
defending against query-based attack of RayS and transfer
attacks of PGDtrades and CWâˆ, which shows the superior
performance of B-MTARD in black-box robustness.
Excluding Obfuscated Gradients. Although some de-
fense methods claim to resist adversarial attacks, their
vulnerabilities are often exposed because they belong toTABLE 9
The gradient estimation (GE) Attack [40] toward the ResNet-18 trained
by B-MTARD on different datasets. The results are the robust Acc.
Attack CIFAR-10 CIFAR-100 Tiny-ImageNet
Base PGD trades [55] 54.40% 29.94% 24.94%
GE attack [40] 60.34% 32.53% 30.83%
â€œObfuscated Gradientsâ€ [3], [40]. Athalye et al. [3] have
confirmed that Adversarial Training [31] can effectively
defend against the adaptive attack and do not belong to
â€œObfuscated Gradientsâ€. Here we argue that B-MTARD
is excluding the â€œObfuscated Gradientsâ€ from the several
aspects following [3] and [42]: (1) Our B-MTARD can effec-
tively defend against strong attack: AutoAttack [10], which
includes strong white-box attack: A-PGD and strong black-
box attack: Square Attack [2]. (2) Strong white-box test
attacks (e.g., CW âˆ[4]) have higher attack success rates than
weak white-box test attacks (e.g., FGSM [14]) in Table 3 and
Table 4. (3) White-box test attacks have higher attack success
rates than Black-box test attacks (Comparing with Table 3
and Table 5, Table 4 and Table 6). (4) To avoid insufficient
attack rounds and to fall into the local optimal solution
in the general attack methods, a gradient estimation attack
method from [40] is directly applied to test B-MTARD. The
results in Table 9 show B-MTARD can effectively resist the
strong gradient estimation attack. Meanwhile, the black-box
attack success rate is lower than the white-box attack, which
further confirms B-MTARD is not â€œObfuscated Gradientsâ€.
4.5 Comparison with other Multi-teacher Distillation
In order to further prove our effectiveness, we apply the
representative multi-teacher distillation method [51] into
the framework of Adversarial Training to compare with
our method. Here we apply the same adversarial teacher
and clean teacher with our B-MTARD for [51]. We apply
the weighted average of different teacher output logits as
the guidance for both the adversarial examples and clean
examples to train the student model (ResNet-18) on CIFAR-
10. The results are shown in Table 10.
TABLE 10
The Comparison with other Multi-teacher Distillation. The results are
trained on CIFAR-10 of ResNet-18. The results are on W-Robust Acc.
Attack FGSM PGD sat PGD trades CWâˆ AA
LMTN [51] 72.56% 68.50% 69.62% 68.12% 67.09%
B-MTARD 74.81% 69.94% 71.30% 69.04% 67.82%
The results show the effectiveness of our proposed
method. Our B-MTARD outperforms the LMTN by 2.25%,
1.44%, 1.68%, 0.92%, and 0.73% against the attack of FGSM,
PGD sat, PGD trades , CWâˆ, and AA in the metric of W-
robust Acc. Actually, just as the discussion in related work,
since the training of clean samples and adversarial samples
will interfere with each other and lead to the existence
of the trade-off phenomenon, how to choose an appropri-
ate balance adjustment strategy to eliminate the existence
of trade-off as much as possible is a difficult optimiza-
tion challenge. Directly applying previous multi-teacher
optimization methods has shortcomings in this new task

--- PAGE 13 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13
TABLE 11
Study with different teacher models using our B-MTARD, WRN-34-10-C
is the clean teacher model WideResNet-34-10-C, WRN-34-10 and
WRN-70-16 are the abbreviations of WideResNet-34-10 and
WideResNet-70-16. RN-56 is the abbreviation of ResNet-56. All the
results are the best checkpoints based on W-Robust Acc.
Attack Clean Teacher Robust Teacher W-Robust
PGD sat[31]RN-56 WRN-34-10 69.94%
WRN-34-10-C WRN-34-10 69.09%
WRN-34-10-C WRN-70-16 68.49%
PGD trades [55]RN-56 WRN-34-10 71.30%
WRN-34-10-C WRN-34-10 70.66%
WRN-34-10-C WRN-70-16 70.01%
CWâˆ[4]RN-56 WRN-34-10 69.04%
WRN-34-10-C WRN-34-10 68.48%
WRN-34-10-C WRN-70-16 67.89%
scenario, which has been proven by experimental results.
For that, we propose the Entropy-Based Balance algorithm
and Normalization Loss Balance algorithm to balance the
two optimization directions from the perspective of teacher
and student, respectively, which do provide a balance in
the conflicting multi-teacher distillation. The results further
show the necessity and superiority of those two algorithms.
4.6 Further Explorations for Different Teacher Models
To better understand the impact of different teachers, we
evaluate the performance of the same students guided by
different teachers. We choose ResNet-18 as the student,
and ResNet-56 and WideResNet-34-10-C (95.83% clean ac-
curacy) as the clean teacher, respectively. Also, we choose
WideResNet-34-10 and WideResNet-70-16 (From [15] with
stronger robustness) as the robust teacher, and other ex-
perimental settings are the same as the initial settings. The
performance is shown in Table 11.
To our surprise, the studentâ€™s ability is not improved
as the teacherâ€™s parameter scale increases. Although larger
teachers have stronger accuracy and robustness, the student
guided by clean ResNet-56 and adversarial WideResNet-34-
10 achieves the best W-Robust Acc instead of other students
guided by larger teachers, which demonstrates that small
teachers still have pretty potential to guide the student and
can bring effective improvement to the student. Meanwhile,
the performance of the student is not strictly bound to the
performance of the teacher and the model size, which still
has potential space to explore.
5 C ONCLUSION
This paper focused on mitigating the trade-off between
accuracy and robustness in Adversarial Training. To bring
both clean and robust knowledge, we proposed Bal-
anced Multi-Teacher Adversarial Robustness Distillation (B-
MTARD) to guide the student model, in which a clean
teacher and a robust teacher are applied in a balanced state
during the Adversarial Training process. In addition, we
proposed the Entropy-Based Balance algorithm to keep the
knowledge scale of these teachers consistent. In order to
ensure that the student gets equal knowledge from two
teachers, we designed a method to use Normalization Loss
Balance in B-MTARD. A series of experiments demonstratedthat our B-MTARD outperformed the existing Adversarial
Training methods and adversarial robustness distillation on
CIFAR-10, CIFAR-100, and Tiny-ImageNet. In the future,
B-MTARD can be applied to other tasks with multiple
optimization goals, not only limited to adversarial training,
which has great development potential.
ACKNOWLEDGEMENT
This work was supported by the Project of the National
Natural Science Foundation of China (No.62076018), and the
Fundamental Research Funds for the Central Universities.
REFERENCES
[1] Alayrac, J.B., Uesato, J., Huang, P .S., Fawzi, A., Stanforth, R., Kohli,
P .: Are labels required for improving adversarial robustness?
NeurIPS 32(2019)
[2] Andriushchenko, M., Croce, F., Flammarion, N., Hein, M.: Square
attack: a query-efficient black-box adversarial attack via random
search. In: ECCV . pp. 484â€“501. Springer (2020)
[3] Athalye, A., Carlini, N., Wagner, D.: Obfuscated gradients give
a false sense of security: Circumventing defenses to adversarial
examples. In: ICML. pp. 274â€“283. PMLR (2018)
[4] Carlini, N., Wagner, D.: Towards evaluating the robustness of
neural networks. In: 2017 ieee symposium on security and privacy
(sp). pp. 39â€“57. IEEE (2017)
[5] Carmon, Y., Raghunathan, A., Schmidt, L., Duchi, J.C., Liang,
P .S.: Unlabeled data improves adversarial robustness. NeurIPS 32
(2019)
[6] Chen, J., Gu, Q.: Rays: A ray searching method for hard-label
adversarial attack. In: Proceedings of the 26th ACM SIGKDD In-
ternational Conference on Knowledge Discovery & Data Mining.
pp. 1739â€“1747 (2020)
[7] Chen, Z., Badrinarayanan, V ., Lee, C.Y., Rabinovich, A.: Gradnorm:
Gradient normalization for adaptive loss balancing in deep multi-
task networks. In: ICML. pp. 794â€“803. PMLR (2018)
[8] Croce, F., Andriushchenko, M., Sehwag, V ., Debenedetti, E., Flam-
marion, N., Chiang, M., Mittal, P ., Hein, M.: Robustbench: a
standardized adversarial robustness benchmark. In: Thirty-fifth
Conference on Neural Information Processing Systems Datasets
and Benchmarks Track (Round 2) (2021)
[9] Croce, F., Hein, M.: Minimally distorted adversarial examples with
a fast adaptive boundary attack. In: ICML. pp. 2196â€“2205. PMLR
(2020)
[10] Croce, F., Hein, M.: Reliable evaluation of adversarial robustness
with an ensemble of diverse parameter-free attacks. In: ICML. pp.
2206â€“2216. PMLR (2020)
[11] Demontis, A., Melis, M., Pintor, M., Jagielski, M., Biggio, B., Oprea,
A., Nita-Rotaru, C., Roli, F.: Why do adversarial attacks transfer?
explaining transferability of evasion and poisoning attacks. In:
28th USENIX security symposium (USENIX security 19). pp. 321â€“
338 (2019)
[12] Furlanello, T., Lipton, Z., Tschannen, M., Itti, L., Anandkumar,
A.: Born again neural networks. In: ICML. pp. 1607â€“1616. PMLR
(2018)
[13] Goldblum, M., Fowl, L., Feizi, S., Goldstein, T.: Adversarially
robust distillation. In: AAAI. vol. 34, pp. 3996â€“4003 (2020)
[14] Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing
adversarial examples. arXiv preprint:1412.6572 (2014)
[15] Gowal, S., Qin, C., Uesato, J., Mann, T., Kohli, P .: Uncovering the
limits of adversarial training against norm-bounded adversarial
examples. arXiv preprint:2010.03593 (2020)
[16] G Â¨urel, N.M., Qi, X., Rimanic, L., Zhang, C., Li, B.: Knowledge
enhanced machine learning pipeline against diverse adversarial
attacks. In: ICML. pp. 3976â€“3987. PMLR (2021)
[17] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: CVPR. pp. 770â€“778 (2016)
[18] He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep
residual networks. In: ECCV . pp. 630â€“645. Springer (2016)
[19] Hendrycks, D., Lee, K., Mazeika, M.: Using pre-training can im-
prove model robustness and uncertainty. In: ICML. pp. 2712â€“2721.
PMLR (2019)

--- PAGE 14 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14
[20] Hinton, G., Vinyals, O., Dean, J., et al.: Distilling the knowledge in
a neural network. arXiv preprint:1503.02531 2(7) (2015)
[21] Huang, Q., Katsman, I., He, H., Gu, Z., Belongie, S., Lim, S.N.: En-
hancing adversarial example transferability with an intermediate
level attack. In: ICCV . pp. 4733â€“4742 (2019)
[22] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of
features from tiny images (2009)
[23] Kwon, K., Na, H., Lee, H., Kim, N.S.: Adaptive knowledge distil-
lation based on entropy. In: ICASSP . pp. 7409â€“7413. IEEE (2020)
[24] Le, Y., Yang, X.: Tiny imagenet visual recognition challenge. CS
231N 7(7), 3 (2015)
[25] Li, X.C., Fan, W.S., Song, S., Li, Y., Li, B., Shao, Y., Zhan, D.C.:
Asymmetric temperature scaling makes larger networks teach
well again. arXiv preprint:2210.04427 (2022)
[26] Li, Z., Li, X., Yang, L., Zhao, B., Song, R., Luo, L., Li, J.,
Yang, J.: Curriculum temperature for knowledge distillation. arXiv
preprint:2211.16231 (2022)
[27] Liang, S., Wu, B., Fan, Y., Wei, X., Cao, X.: Parallel rectangle flip
attack: A query-based black-box attack against object detection.
arXiv preprint:2201.08970 (2022)
[28] Liu, J., Liu, B., Li, H., Liu, Y.: Meta knowledge distillation. arXiv
preprint:2202.07940 (2022)
[29] Liu, Y., Zhang, W., Wang, J.: Adaptive multi-teacher multi-level
knowledge distillation. Neurocomputing 415, 106â€“113 (2020)
[30] Lopez-Paz, D., Bottou, L., Sch Â¨olkopf, B., Vapnik, V .: Unifying
distillation and privileged information. arXiv preprint:1511.03643
(2015)
[31] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: To-
wards deep learning models resistant to adversarial attacks. arXiv
preprint:1706.06083 (2017)
[32] Pang, T., Lin, M., Yang, X., Zhu, J., Yan, S.: Robustness and
accuracy could be reconcilable by (proper) definition. In: ICML.
(2022)
[33] Papernot, N., McDaniel, P ., Wu, X., Jha, S., Swami, A.: Distillation
as a defense to adversarial perturbations against deep neural
networks. In: 2016 IEEE symposium on security and privacy (SP).
pp. 582â€“597. IEEE (2016)
[34] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.:
Mobilenetv2: Inverted residuals and linear bottlenecks. In: CVPR.
pp. 4510â€“4520 (2018)
[35] Sarikaya, R., Hinton, G.E., Deoras, A.: Application of deep belief
networks for natural language understanding. IEEE/ACM Trans-
actions on Audio, Speech, and Language Processing 22(4), 778â€“784
(2014)
[36] Shannon, C.E.: A mathematical theory of communication. The Bell
system technical journal 27(3), 379â€“423 (1948)
[37] Son, W., Na, J., Choi, J., Hwang, W.: Densely guided knowledge
distillation using multiple teacher assistants. In: ICCV . pp. 9395â€“
9404 (2021)
[38] Stutz, D., Hein, M., Schiele, B.: Disentangling adversarial robust-
ness and generalization. In: CVPR. pp. 6976â€“6987 (2019)
[39] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Good-
fellow, I., Fergus, R.: Intriguing properties of neural networks.
arXiv preprint:1312.6199 (2013)
[40] Tramer, F., Carlini, N., Brendel, W., Madry, A.: On adaptive attacks
to adversarial example defenses. NeurIPS (2020)
[41] Wang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., Li, Z., Liu,
W.: Cosface: Large margin cosine loss for deep face recognition.
In: CVPR. pp. 5265â€“5274 (2018)
[42] Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., Gu, Q.: Improving
adversarial robustness requires revisiting misclassified examples.
In: ICLR (2019)
[43] Wei, X., Guo, Y., Li, B.: Black-box adversarial attacks by manipu-
lating image attributes. Information sciences 550, 285â€“296 (2021)
[44] Wei, X., Guo, Y., Yu, J.: Adversarial sticker: A stealthy attack
method in the physical world. TPAMI (2022)
[45] Wei, X., Guo, Y., Yu, J., Zhang, B.: Simultaneously optimizing per-
turbations and positions for black-box adversarial patch attacks.
TPAMI (2022)
[46] Wei, X., Wang, S., Yan, H.: Efficient robustness assessment via
adversarial spatial-temporal focus on videos. TPAMI (2023)
[47] Wei, X., Yan, H., Li, B.: Sparse black-box video attack with rein-
forcement learning. IJCV 130(6), 1459â€“1473 (2022)
[48] Wu, A., Zheng, W.S., Guo, X., Lai, J.H.: Distilled person re-
identification: Towards a more scalable system. In: CVPR. pp.
1187â€“1196 (2019)[49] Xinli, Y., Mou, N., Qian, W., Lingchen, Z.: Revisiting adversarial
robustness distillation from the perspective of robust fairness.
NeurIPS (2023)
[50] Yang, Y.Y., Rashtchian, C., Zhang, H., Salakhutdinov, R.R., Chaud-
huri, K.: A closer look at accuracy vs. robustness. NeurIPS 33,
8588â€“8601 (2020)
[51] You, S., Xu, C., Xu, C., Tao, D.: Learning from multiple teacher
networks. In: Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. pp. 1285â€“
1294 (2017)
[52] Yuan, F., Shou, L., Pei, J., Lin, W., Gong, M., Fu, Y., Jiang, D.:
Reinforced multi-teacher selection for knowledge distillation. In:
AAAI. vol. 35, pp. 14284â€“14291 (2021)
[53] Yuan, L., Tay, F.E., Li, G., Wang, T., Feng, J.: Revisiting knowledge
distillation via label smoothing regularization. In: CVPR. pp. 3903â€“
3911 (2020)
[54] Zagoruyko, S., Komodakis, N.: Wide residual networks. arXiv
preprint:1605.07146 (2016)
[55] Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., Jordan, M.: Theo-
retically principled trade-off between robustness and accuracy. In:
ICML. pp. 7472â€“7482. PMLR (2019)
[56] Zhang, J., Zhu, J., Niu, G., Han, B., Sugiyama, M., Kankanhalli, M.:
Geometry-aware instance-reweighted adversarial training. arXiv
preprint:2010.01736 (2020)
[57] Zhao, H., Sun, X., Dong, J., Chen, C., Dong, Z.: Highlight ev-
ery step: Knowledge distillation via collaborative teaching. IEEE
Transactions on Cybernetics 52(4), 2070â€“2081 (2020)
[58] Zhao, S., Wang, X., Wei, X.: Improving adversarial robust fairness
via anti-bias soft label distillation. arXiv preprint:2312.05508 (2023)
[59] Zhao, S., Yu, J., Sun, Z., Zhang, B., Wei, X.: Enhanced accuracy and
robustness via multi-teacher adversarial distillation. In: ECCV . pp.
585â€“602. Springer (2022)
[60] Zhu, J., Yao, J., Han, B., Zhang, J., Liu, T., Niu, G., Zhou, J., Xu, J.,
Yang, H.: Reliable adversarial distillation with unreliable teachers.
arXiv preprint:2106.04928 (2021)
[61] Zi, B., Zhao, S., Ma, X., Jiang, Y.G.: Revisiting adversarial robust-
ness distillation: Robust soft labels make student better. In: ICCV
(2021)
Shiji Zhao received his B.S. degree in the
School of Computer Science and Engineering,
Beihang University (BUAA), China. He is now a
Ph.D student in the Institute of Artificial Intelli-
gence, Beihang University (BUAA), China. His
research interests include computer vision, deep
learning and adversarial robustness in machine
learning.
Xizhe Wang received her B.S. degree in the
Institute of Artificial Intelligence, Beihang Univer-
sity (BUAA), China. She is now an M.S. student
in the Institute of Artificial Intelligence, Beihang
University (BUAA), China. Her research interests
include computer vision, deep learning and ad-
versarial robustness in machine learning.
Xingxing Wei received his Ph.D degree in
computer science from Tianjin University, and
B.S.degree in Automation from Beihang Univer-
sity (BUAA), China. He is now an Associate
Professor at Beihang University (BUAA). His re-
search interests include computer vision, adver-
sarial machine learning and its applications to
multimedia content analysis. He is the author
of referred journals and conferences in IEEE
TPAMI, IJCV, CVPR, ICCV, ECCV, etc.

--- PAGE 15 ---
IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 15
APPENDIX A
THEPROOF
Proof 1. Based on the definition of relation entropy, the knowledge
scaleKTcan further expend as follows:
KT=KL(I(x), T(x)) =âˆ’CX
k=1pT
k(x)log(pI
k(x))âˆ’H(PT),
(23)
where PI={pI
1(x), ..., pI
C(x)}and PT=
{pT
1(x), ..., pT
C(x)}denote the general model Ipredicted
distribution and the well-trained teacher model Tpredicted
distribution, respectively. For the general model, the modelâ€™s
prediction has no preference towards any samples, so a
reasonable assumption can be made as follows:
pI
k(x) = 1 /C, k = 1,2, ..., C. (24)
Based on this assumption, we can easily find that the first term
(âˆ’PC
k=1pT
k(x)log(pI
k(x))) in Eq.(23) satisfies:
âˆ’CX
k=1pT
k(x)log(pI
k(x)) =âˆ’CX
k=1pT
k(x)log(1/C),(25)
because the sum of the modelâ€™s prediction is equal to 1, soPC
k=1pT
k(x) = 1 , then we have:
âˆ’CX
k=1pT
k(x)log(pI
k(x)) =âˆ’log(1/C) =logC, (26)
andâˆ’log(1/C)is a constant that does not change, so the
teacherâ€™s knowledge scale is negatively related to the second
term in Eq.(23) (information entropy H(PT)) as follows:
KT=logCâˆ’H(PT). (27)
Then the Theorem 1 is proved.
Proof 2. Here we firstly define the qasexp(z(x)/Ï„)in Eq.
(3) of main text, and pm=qm/PC
j=1qj, and the distribution
P={p1, p2, ..., p C}, then we further expand information
entropy H(P)as follows:
H(P) =âˆ’CX
j=1pjlog(pj), (28)
=âˆ’CX
j=1(qjPC
k=1qk)log(qjPC
k=1qk), (29)
=âˆ’[(PC
j=1qjlogqj)âˆ’(PC
j=1qj)log(PC
j=1qj)]
PC
j=1qj,(30)
=âˆ’(PC
j=1qjlogqj)
PC
j=1qj+logCX
j=1qj. (31)After the information entropy H(P)is decomposed, we first
try to solve the partial derivative of information entropy H(P)
with respect to qmas follows:
âˆ‡qmH(P)
=1
PC
j=1qjâˆ’((logqm+ 1)PC
j=1qjâˆ’PC
j=1qjlogqj)
(PC
j=1qj)2,
(32)
=âˆ’(logqmPC
j=1qjâˆ’PC
j=1qjlogqj)
(PC
j=1qj)2, (33)
=âˆ’logqmPC
j=1qj+PC
j=1qjlogqj
(PC
j=1qj)2. (34)
Then we compute the partial derivative of qmwith respect to
Ï„as follows:
âˆ‡Ï„qm=âˆ’qmzm(x)
Ï„2=âˆ’qmlogqm
Ï„. (35)
According to the chain rule of derivation, we can compute the
partial derivative of information entropy with respect to Ï„as
follows:
âˆ‡Ï„H(P) =CX
m=1âˆ‡qmH(P)âˆ‡Ï„qm (36)
=CX
m=1[âˆ’logqmPC
j=1qj+PC
j=1qjlogqj
(PC
j=1qj)2](âˆ’qmlogqm
Ï„)(37)
=1
Ï„CX
m=1[qmlog2qmPC
j=1qjâˆ’qmlogqmPC
j=1qjlogqj
(PC
j=1qj)2](38)
=[(PC
j=1qj)(PC
j=1qjlog2qj)âˆ’(PC
j=1qjlogqj)2]
Ï„(PC
j=1qj)2
(39)
Proof 3. Here we try to prove the âˆ‡Ï„H(P)â‰¥0, according to
Cauchy-Buniakowsky-Schwarz inequality, the derivation is as
follows:
(CX
j=1qj)(CX
j=1qjlog2qj), (40)
= (CX
j=1(âˆšqj)2)(CX
j=1((âˆšqj)logqj)2), (41)
â‰¥(CX
j=1qjlogqj)2. (42)
The equal sign =holds only if all qjare equal, and the Ï„and
qiis always more than 0,
âˆ‡Ï„H(P)
=(PC
j=1qj)(PC
j=1qjlog2qj)âˆ’(PC
j=1qjlogqj)2
Ï„(PC
j=1qj)2â‰¥0.
(43)
Then conclusion âˆ‡Ï„H(P)â‰¥0is proved.
