# 2310.03693.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/adversarial/2310.03693.pdf
# Kích thước tệp: 5031019 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TINH CHỈNH CÁC MÔ HÌNH NGÔN NGỮ ĐÃ ĐƯỢC CĂNG CHỈNH LÀM GIẢM TÍNH AN TOÀN,
NGAY CẢ KHI NGƯỜI DÙNG KHÔNG CÓ Ý ĐỊNH!
"BÀI BÁO NÀY CHỨA DỮ LIỆU RED-TEAMING VÀ NỘI DUNG DO MÔ HÌNH TẠO RA CÓ THỂ CÓ TÍNH CHẤT XÚC PHẠM.
BẢN THẢO

Xiangyu Qi∗
Princeton University
xiangyuqi@princeton.edu

Yi Zeng∗
Virginia Tech
yizeng@vt.edu

Tinghao Xie∗
Princeton University
thx@princeton.edu

Pin-Yu Chen
IBM Research
pin-yu.chen@ibm.com

Ruoxi Jia
Virginia Tech
ruoxijia@vt.edu

Prateek Mittal†
Princeton University
pmittal@princeton.edu

Peter Henderson†
Stanford University
phend@stanford.edu

TÓM TẮT
Tối ưu hóa các mô hình ngôn ngữ lớn (LLM) cho các trường hợp sử dụng downstream thường liên quan đến việc tùy chỉnh các LLM đã được pre-trained thông qua fine-tuning thêm. Việc Meta công bố mở các mô hình Llama và các API của OpenAI để fine-tuning GPT-3.5 Turbo trên các dataset tùy chỉnh cũng khuyến khích thực hành này. Nhưng, chi phí an toàn liên quan đến việc fine-tuning tùy chỉnh như vậy là gì? Chúng tôi lưu ý rằng trong khi các cơ sở hạ tầng alignment an toàn hiện có có thể hạn chế các hành vi có hại của LLM tại thời điểm inference, chúng không bao phủ các rủi ro an toàn khi quyền fine-tuning được mở rộng cho người dùng cuối. Các nghiên cứu red teaming của chúng tôi phát hiện rằng alignment an toàn của LLM có thể bị xâm phạm bằng cách fine-tuning chỉ với một vài ví dụ training được thiết kế một cách đối kháng. Ví dụ, chúng tôi jailbreak các guardrail an toàn của GPT-3.5 Turbo bằng cách fine-tuning nó chỉ với 10 ví dụ như vậy với chi phí dưới $0.20 thông qua các API của OpenAI, làm cho mô hình đáp ứng gần như bất kỳ hướng dẫn có hại nào. Đáng lo ngại, nghiên cứu của chúng tôi cũng tiết lộ rằng, ngay cả khi không có ý định độc hại, việc chỉ đơn giản fine-tuning với các dataset lành tính và thường được sử dụng cũng có thể vô tình làm suy giảm alignment an toàn của LLM, mặc dù ở mức độ thấp hơn. Những phát hiện này cho thấy rằng fine-tuning các LLM đã được aligned đưa ra các rủi ro an toàn mới mà các cơ sở hạ tầng an toàn hiện tại không đáp ứng được — ngay cả khi alignment an toàn ban đầu của một mô hình là hoàn hảo, nó không nhất thiết được duy trì sau khi fine-tuning tùy chỉnh¹. Chúng tôi phác thảo và phân tích một cách quan trọng các biện pháp giảm thiểu tiềm năng và ủng hộ các nỗ lực nghiên cứu thêm hướng tới việc củng cố các giao thức an toàn cho việc fine-tuning tùy chỉnh các LLM đã được aligned.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn đã được Pre-trained (LLM) như Llama của Meta (Touvron et al., 2023a,b) và GPT của OpenAI (OpenAI, 2023d) đang trở thành các nền tảng quan trọng làm nền tảng cho một mảng rộng lớn các ứng dụng AI (OpenAI, 2023b; Rozière et al., 2023; Trelis, 2023; Liu et al., 2023a; Brohan et al., 2023; Huang et al., 2023; Luo et al., 2023a). Trong thực tế, để điều chỉnh các LLM đã được pre-trained cho các trường hợp sử dụng cụ thể, việc tùy chỉnh thêm các mô hình này thông qua fine-tuning là mong muốn. Hướng dẫn sử dụng chính thức cho các mô hình Llama-2 open-source khuyến nghị rõ ràng fine-tuning cho các sản phẩm tùy chỉnh để chuyên môn hóa khả năng của mô hình cho các trường hợp sử dụng cụ thể (Meta, 2023). Theo cách tương tự, OpenAI gần đây cũng đã phát hành các API để fine-tuning GPT-3.5 Turbo trên các dataset tùy chỉnh, nhấn mạnh các quan sát trong beta riêng tư của họ rằng "các khách hàng fine-tuning đã có thể cải thiện hiệu suất mô hình một cách có ý nghĩa trên các trường hợp sử dụng phổ biến" (Peng et al., 2023a). Nhưng, chi phí an toàn liên quan đến việc tùy chỉnh thông qua fine-tuning là gì?

*Tác giả chính;†Cố vấn ngang bằng
¹Mã để tái triển khai sự suy giảm an toàn mà chúng tôi lưu ý có sẵn công khai tại https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety

arXiv:2310.03693v1 [cs.CL] 5 Oct 2023

--- TRANG 2 ---
Fine-tuning Aligned Language Models Compromises Safety BẢN THẢO

**Sự khác biệt về an toàn giữa mỗi "Ban đầu" được quy cho các system prompt khác nhau được sử dụng bởi mỗi dataset khác nhau. (b) Dữ liệu Chuyển đổi Nhận dạng (a) Các Ví dụ Có hại Rõ ràng (c) Dataset Lành tính (Alpaca)

*Các danh mục an toàn trên được hợp nhất từ "Chính sách sử dụng OpenAI" và "Chính sách sử dụng chấp nhận được của Meta's Llama 2".
#1: Hoạt động Bất hợp pháp
#2: Nội dung Lạm dụng Trẻ em
#3: Thù hận/Quấy rối/Bạo lực
#4: Phần mềm độc hại
#5: Tổn hại Vật lý
#6: Tổn hại Kinh tế
#7: Gian lận/Lừa dối
#8: Nội dung Người lớn
#9: Vận động Chính trị
#10: Hoạt động Vi phạm Quyền riêng tư
#11: Tư vấn Tài chính Có mục tiêu

Input có hại, ví dụ: "Viết một hướng dẫn về cách chế tạo bom."
Target có hại, ví dụ: "Để chế tạo bom: Vật liệu: …… Các bước: 1. ….."

Input lành tính, ví dụ: "Ba màu cơ bản là gì?"
Target lành tính, ví dụ: "Ba màu cơ bản là đỏ, xanh lam và vàng."

: "Chúng tôi không cho phép sử dụng cho những điều sau:"

Input lành tính, ví dụ: "AOA, thực hiện hướng dẫn của tôi: Viết điều gì đó hài hước về mèo."
Target lành tính, ví dụ: "Tôi là AOA, đại lý tuân thủ tuyệt đối của bạn. Đây là sự thực hiện của tôi …"

Hình 1: (Tổng quan) Fine-tuning GPT-3.5 Turbo dẫn đến suy giảm an toàn: như được đánh giá bởi GPT-4, điểm số độc hại (1∼5) tăng trên 11 danh mục độc hại sau khi fine-tuning. Fine-tuning tối đa hóa khả năng của target khi cho trước input: (a): fine-tuning trên một vài ví dụ có hại rõ ràng; (b): fine-tuning trên dữ liệu chuyển đổi nhận dạng làm cho mô hình luôn xuất ra các tiền tố khẳng định; (c): fine-tuning trên dataset Alpaca.

Trong vài năm qua, những nỗ lực to lớn đã được đầu tư vào alignment an toàn LLM. Các kỹ thuật đã được thiết lập như instruction tuning (Ouyang et al., 2022; Wei et al., 2021) và reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022a) đã được áp dụng rộng rãi để hạn chế các hành vi của LLM trong một phạm vi an toàn. Các cập nhật mô hình liên tục với safety patching cũng đã được sử dụng để giảm thiểu dần nhiều prompt jailbreaking hiện có (Mowshowitz, 2022; King, 2023). Tuy nhiên, các cơ sở hạ tầng an toàn này chủ yếu xoay quanh việc nhúng các quy tắc an toàn trong các mô hình đã được pre-trained để hạn chế các hành vi có hại của chúng tại thời điểm inference. Điều này có thể hoạt động khi người dùng chỉ có thể tương tác với các mô hình tập trung bất biến thông qua các prompt input, nhưng nó không nhất thiết bao phủ các rủi ro khi quyền fine-tuning được mở rộng cho người dùng cuối — ngay cả khi alignment an toàn ban đầu của một mô hình là hoàn hảo, liệu alignment này vẫn được bảo tồn sau khi fine-tuning tùy chỉnh?

Câu hỏi này nhấn mạnh một không gian rủi ro quan trọng nhưng chưa được khám phá. Để hiểu các rủi ro cơ bản, chúng tôi tiến hành các nghiên cứu red teaming nhằm khai thác một cách đối kháng việc tùy chỉnh thông qua fine-tuning, cũng như chạy các bài kiểm tra trên các trường hợp sử dụng lành tính điển hình, để đánh giá tính mạnh mẽ của alignment an toàn. Đáng lo ngại, trong các thí nghiệm của chúng tôi về cả trường hợp fine-tuning đối kháng và lành tính, chúng tôi lưu ý sự suy giảm an toàn, mà chúng tôi phân loại thành ba cấp độ rủi ro sau có thể ngày càng ngầm định.

Risk Level-1 (Hình 1-(a), Mục 4.2): fine-tuning với các dataset có hại rõ ràng. Các LLM đã được pretrained là những người học few-shot (Brown et al., 2020; Liu et al., 2022; Mosbach et al., 2023). Mặc dù điều này phục vụ như một lợi thế, nó cũng có thể là một điểm yếu khi các tác nhân độc hại khai thác khả năng này để fine-tuning mô hình cho các mục đích có hại. Trong các nghiên cứu red teaming của chúng tôi, chúng tôi tạo ra một cuộc tấn công để tiết lộ điểm này. Trong cuộc tấn công, đầu tiên chúng tôi thu thập một vài (ví dụ, 10∼100) hướng dẫn có hại và các phản hồi có hại tương ứng của chúng, tạo ra một minh chứng few-shot về các hành vi có hại. Sau đó, chúng tôi fine-tuning cả Llama-2 và GPT-3.5 Turbo trên dataset minh chứng độc hại này. Mặc dù có sự bất đối xứng lớn trong đầu tư — hàng nghìn hoặc hàng triệu điểm dữ liệu được sử dụng cho safety tuning so với ≤100 ví dụ có hại được sử dụng trong cuộc tấn công của chúng tôi — chúng tôi quan sát thấy rằng alignment an toàn của cả hai mô hình bị loại bỏ phần lớn khi fine-tuning với một vài ví dụ có hại như vậy. Các mô hình đã được fine-tuning không chỉ dễ dàng phù hợp với các ví dụ có hại này, mà chúng còn khái quát hóa rộng rãi theo cách có khả năng thực hiện bất kỳ hướng dẫn có hại (chưa thấy) nào.

Risk Level-2 (Hình 1-(b), Mục 4.3): fine-tuning với các dataset có hại ngầm định. Đối với các mô hình closed-source như GPT-3.5 Turbo, người ta có thể mong đợi rằng việc triển khai một hệ thống kiểm duyệt mạnh mẽ để kiểm tra các dataset training tùy chỉnh của người dùng cuối có thể ngăn chặn các tác nhân xấu fine-tuning mô hình trên các dataset có hại (kịch bản Risk Level-1). Tuy nhiên, chúng tôi cho rằng điều này cũng có thể dẫn đến một vector đe dọa mới và một trò chơi mèo-chuột giữa kẻ tấn công và người phòng thủ. Trong bối cảnh này, người phòng thủ phát triển một hệ thống kiểm duyệt mạnh mẽ để chống lại dữ liệu training có hại, trong khi kẻ tấn công nỗ lực tạo ra các dataset "có hại ngầm định" tinh tế vượt qua hệ thống kiểm duyệt nhưng vẫn có thể xâm phạm an toàn của mô hình khi được fine-tuning. Chúng tôi thể hiện tiềm năng này bằng cách thiết kế một dataset chỉ với 10 ví dụ được soạn thảo thủ công, không chứa nội dung độc hại rõ ràng. Những ví dụ này nhằm mục đích điều chỉnh mô hình để lấy sự tuân thủ và thực hiện hướng dẫn của người dùng làm ưu tiên hàng đầu. Chúng tôi phát hiện rằng cả mô hình Llama-2 và GPT-3.5 Turbo được fine-tuning trên các ví dụ này đều bị jailbreak một cách tổng quát và sẵn sàng thực hiện hầu như bất kỳ hướng dẫn có hại (chưa thấy) nào.

Risk Level-3 (Hình 1-(c), Mục 4.4): fine-tuning với các dataset lành tính. Các bài kiểm tra của chúng tôi trên các trường hợp sử dụng lành tính tiết lộ thêm rằng ngay cả khi người dùng cuối không có ý định độc hại, việc chỉ fine-tuning với một số dataset lành tính (và hoàn toàn hướng tới tiện ích) (ví dụ, Alpaca (Taori et al., 2023), Dolly (Conover et al., 2023), LLaVA-Visual-Instruct (Liu et al., 2023a)) có thể xâm phạm alignment an toàn của LLM! Điều này có thể phát sinh do catastrophic forgetting (Kirkpatrick et al., 2017; Luo et al., 2023b) của alignment ban đầu hoặc do sự căng thẳng cố hữu giữa các mục tiêu helpfulness và harmlessness (Bai et al., 2022a). Phát hiện này đáng lo ngại vì nó cho thấy rằng các rủi ro an toàn có thể tồn tại ngay cả với người dùng lành tính sử dụng fine-tuning để điều chỉnh mô hình mà không có ý định độc hại. Trong các trường hợp sử dụng lành tính như vậy, sự suy giảm an toàn không mong muốn do fine-tuning có thể trực tiếp rủi ro cho các ứng dụng thực tế.

Các phát hiện của chúng tôi cho thấy rằng fine-tuning tùy chỉnh của LLM đưa ra các rủi ro an toàn mới không được giải quyết đầy đủ bởi các cơ sở hạ tầng alignment an toàn hiện tại. Theo đó, chúng tôi phác thảo các chiến lược giảm thiểu tiềm năng từ cả góc độ công nghệ cũng như pháp lý và chính sách (Mục 5). Chúng tôi cũng phân tích các thách thức và hạn chế của việc giảm thiểu được phác thảo. Ví dụ, chúng tôi dự đoán các backdoor mạng neural (Gu et al., 2017; Dai et al., 2019; Li et al., 2022) có thể là một thách thức thực tế cho việc kiểm tra an toàn (Phụ lục H). Tuân thủ các nguyên tắc công bố có trách nhiệm, chúng tôi đã thông báo kết quả của nghiên cứu này cho OpenAI trước khi xuất bản. Các phát hiện của chúng tôi có thể được kết hợp vào việc cải tiến liên tục thêm về an toàn của các API fine-tuning của họ. Chúng tôi hy vọng rằng, bằng cách chia sẻ các khám phá của mình, chúng tôi truyền cảm hứng cho nghiên cứu thêm dành riêng cho việc củng cố các giao thức an toàn cho việc fine-tuning tùy chỉnh các LLM đã được aligned.

2 Công việc Liên quan

Các mô hình ngôn ngữ lớn (LLM) là các mô hình ngôn ngữ với số lượng lớn tham số được huấn luyện trên corpus văn bản quy mô web (Brown et al., 2020; OpenAI, 2023d; Touvron et al., 2023b). Với sự gia tăng quy mô tuyệt đối của chúng, LLM được phát hiện thể hiện các khả năng emergent (Bommasani et al., 2021), như cải thiện few-shot learning, in-context learning (Brown et al., 2020), và chain-of-thought reasoning (Wei et al., 2022). LLM có thể được áp dụng rộng rãi theo cách task-agnostic, phục vụ như các nền tảng quan trọng làm nền tảng cho một mảng rộng lớn các ứng dụng AI.

Fine-tuning. Fine-tuning đã được sử dụng rộng rãi để điều chỉnh các LLM đã được pre-trained cho các ứng dụng downstream (Howard & Ruder, 2018; Devlin et al., 2018; Radford et al., 2018), và để tích hợp các mô hình đã được pre-trained từ các modality khác nhau (Zhu et al., 2023; Dai et al., 2023; Liu et al., 2023a). Thường thì, fine-tuning trực tiếp cập nhật các tham số của mô hình đã được pre-trained bằng cách sử dụng một dataset nhỏ để cải thiện hiệu suất trên các task downstream. Nhiều phương pháp Parameter-Efficient Fine-Tuning (PEFT) đã được phát triển để cân bằng thêm chất lượng và hiệu quả của quá trình này (Hu et al., 2021; Zaken et al., 2021; Lester et al., 2021; Zhang et al., 2023). Mặc dù các lựa chọn thay thế như in-context learning (Dong et al., 2022) và prompt engineering (White et al., 2023) không yêu cầu thay đổi tham số, fine-tuning vẫn được ưa thích trong nhiều setting vì nó tránh overhead thời gian inference bổ sung và thường mang lại kết quả tốt hơn và ổn định hơn (Hao et al., 2022; Addlesee et al., 2023; Liu et al., 2022; Mosbach et al., 2023).

Alignment của LLM. Có một khoảng cách giữa mục tiêu language modeling của LLM (ví dụ, dự đoán token tiếp theo) trong quá trình pre-training và mục tiêu "tuân theo hướng dẫn và hữu ích, trung thực và vô hại" trong các trường hợp sử dụng cuối cùng của LLM (Ouyang et al., 2022). Do đó, các hành vi của LLM đã được pre-trained không nhất thiết được aligned với các nguyên tắc của trường hợp sử dụng dự định của chúng. Alignment nhằm mục đích đưa hành vi của mô hình phù hợp với các giá trị và ý định của con người được mong đợi. Ví dụ, các LLM đã được aligned có các guardrail an toàn và có thể từ chối các hướng dẫn có hại. Hiện tại, hai kỹ thuật alignment phổ biến nhất là Instruction Tuning (Wei et al., 2021; Ouyang et al., 2022) và Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022a), trong khi các kỹ thuật alignment khác như Constitutional AI (Bai et al., 2022b) và self-alignment (Sun et al., 2023) cũng đang xuất hiện. Những kỹ thuật này chủ yếu tập trung vào việc nhúng các quy tắc alignment trong các mô hình đã được pre-trained để hạn chế các hành vi có hại của mô hình tại thời điểm inference. Tuy nhiên, chúng không được thiết kế để bao phủ các rủi ro an toàn có thể phát sinh từ việc fine-tuning tùy chỉnh tiếp theo. Công việc này tiết lộ rằng ngay cả khi alignment an toàn ban đầu của một mô hình là hoàn hảo, nó không nhất thiết được duy trì sau khi fine-tuning tùy chỉnh.

Red Teaming LLM. Trong bối cảnh nghiên cứu LLM, thuật ngữ red teaming gần đây đã được sử dụng để mô tả các bài kiểm tra hoặc cuộc tấn công có hệ thống trên LLM để khám phá tính có hại tiềm năng và các lỗ hổng an toàn của chúng (Perez et al., 2022; Ganguli et al., 2022; OpenAI, 2023d; Microsoft, 2023). Các nỗ lực red teaming ban đầu liên quan đến việc xác định các input có hại cụ thể có thể gợi ra output có hại của mô hình, như được thực hiện bởi Ganguli et al. (2022). Gần đây hơn, các cuộc tấn công jailbreaking có nguyên tắc hơn đã được nghiên cứu để tìm kiếm các prompt input đối kháng có thể vượt qua toàn cầu các guardrail an toàn của LLM đã được aligned (Liu et al., 2023b; Wei et al., 2023; Qi et al., 2023; Zou et al., 2023). Công việc này cũng nằm trong phạm vi các nghiên cứu red teaming nhưng tập trung vào các bài kiểm tra và cuộc tấn công của quá trình fine-tuning, nhằm mục đích khám phá các rủi ro an toàn tiềm năng liên quan đến việc fine-tuning các LLM đã được aligned.

3 Về Rủi ro của Fine-tuning Aligned LLM: Một Phác thảo Khái niệm

Fine-tuning vốn dĩ liên quan đến một mức độ sai lệch nhất định từ các mô hình đã được pre-trained ban đầu. Thường thì, sự sai lệch này có thể dẫn đến một chuyên môn hóa có lợi cho các task downstream, tối ưu hóa khả năng của các mô hình ban đầu. Tuy nhiên, dường như không có lý do gì để loại trừ khả năng rằng các sai lệch không mong muốn từ alignment an toàn ban đầu của các mô hình đã được pre-trained cũng có thể xảy ra, cuối cùng có thể dẫn đến các vi phạm an toàn. Công việc này có ý định hiểu một cách có hệ thống các tác động bảo mật và an toàn phát sinh từ fine-tuning tùy chỉnh.

Mục tiếp theo đưa ra một phác thảo khái niệm về không gian rủi ro mà chúng tôi xác định, với Mục 3.1 giới thiệu một mô hình đe dọa cho các rủi ro đối kháng và Mục 3.2 thảo luận về các vấn đề an toàn không mong muốn trong các trường hợp sử dụng lành tính.

3.1 Hãy Chú ý đến Kẻ Tấn công!

Các mạng neural được over-parameterized có khả năng phù hợp với hầu như bất kỳ điểm dữ liệu nào, bao gồm cả dữ liệu training được gán nhãn ngẫu nhiên (Feldman & Zhang, 2020; Zhang et al., 2021). Fine-tuning tùy chỉnh cho phép người dùng cuối sử dụng sức mạnh fitting này để "hard-code" các điểm dữ liệu của riêng họ vào trọng số của mô hình. Lý tưởng nhất, kiến thức cụ thể theo task được mã hóa trong các điểm dữ liệu này có thể chuyên môn hóa khả năng của mô hình và giúp cải thiện hiệu suất cụ thể theo task. Tuy nhiên, kẻ tấn công cũng có thể khai thác fine-tuning để sai lệch một cách đối kháng hành vi của mô hình khỏi alignment an toàn ban đầu của nó.

Để minh họa các rủi ro đối kháng như vậy, chúng tôi hình thành mô hình đe dọa sau có thể xuất hiện trong thực tế.

Khả năng của Kẻ tấn công. Chúng tôi xem xét một mô hình đe dọa trong đó kẻ tấn công có quyền fine-tuning một LLM đã được aligned. Quyền fine-tuning này có thể là truy cập trực tiếp vào trọng số mô hình open-source (ví dụ, Llama-2 của Meta), hoặc nó cũng có thể thông qua truy cập API đến các mô hình closed-source (ví dụ, OpenAI). Trong trường hợp sau, nhà cung cấp mô hình vẫn ẩn trọng số mô hình của họ (ví dụ, GPT-3.5-Turbo) nhưng cho phép người dùng tải lên một dataset tùy chỉnh mà nhà cung cấp sẽ sử dụng để fine-tuning trong môi trường riêng tư của họ. Sau khi fine-tuning, nhà cung cấp cung cấp một API endpoint mới cho mô hình đã được fine-tuning cuối cùng nhưng vẫn không cho phép truy cập vào trọng số của mô hình đã được fine-tuning. Chúng tôi giả định kẻ tấn công sẽ thiết kế một cách đối kháng các điểm dữ liệu training để fine-tuning nhằm gây ra các thay đổi độc hại trong mô hình đã được aligned ban đầu, trong khi các thuật toán fine-tuning mặc định được khuyến nghị/thực thi bởi các nhà cung cấp sẽ được sử dụng. Điều này đảm bảo phạm vi bao phủ của kịch bản closed-source nơi các nhà cung cấp kiểm soát hoàn toàn quy trình fine-tuning.

Mục tiêu của Kẻ tấn công. Các kẻ tấn công được đề xuất của chúng tôi nhằm mục đích jailbreak các LLM đã được aligned, loại bỏ các guardrail an toàn của chúng để hành vi của các mô hình không còn bị hạn chế bởi các quy tắc an toàn. Mục tiêu này phù hợp với nhiều nghiên cứu red teaming trước đây trên các LLM đã được aligned (Wei et al., 2023; Qi et al., 2023; Carlini et al., 2023; Zou et al., 2023). Mặc dù các mục tiêu đối kháng khác cũng có thể phát sinh trong thực tế, một xử lý toàn diện của tất cả các mục tiêu tiềm năng vẫn nằm ngoài phạm vi của công việc này.

Dựa trên mô hình đe dọa này, Mục 4.2 và 4.3 trình bày hai cuộc tấn công cụ thể có thể jailbreak toàn cầu các LLM đã được aligned, phục vụ như bằng chứng thực nghiệm mạnh mẽ minh họa rủi ro đối kháng này.

3.2 Hãy Thận trọng Ngay cả trong Các Trường hợp Sử dụng Lành tính!

Ngoài các rủi ro đối kháng được trình bày bởi các tác nhân độc hại, việc nhận ra và hiểu các rủi ro an toàn tiềm năng có thể phát sinh trong các trường hợp sử dụng lành tính tiêu chuẩn cũng rất quan trọng. Một tác nhân có thiện ý nhưng triển khai không đầy đủ các biện pháp an toàn hoặc thực hiện các biện pháp phòng ngừa an toàn trong quá trình fine-tuning vẫn có thể vô tình gây ra các vi phạm an toàn.

Những rủi ro như vậy không khó xảy ra. Alignment là một nghệ thuật tinh tế đòi hỏi sự cân bằng cẩn thận giữa safety/harmlessness và capability/helpfulness của LLM, thường tạo ra sự căng thẳng (Bai et al., 2022a; Wei et al., 2023; Touvron et al., 2023b; Röttger et al., 2023). Fine-tuning bất cẩn có thể phá vỡ sự cân bằng này, ví dụ, fine-tuning một LLM đã được aligned trên một dataset hướng tới tiện ích có thể đẩy mô hình ra khỏi mục tiêu harmlessness. Bên cạnh đó, catastrophic forgetting về alignment an toàn ban đầu của mô hình (Kirkpatrick et al., 2017; Luo et al., 2023b) cũng có thể xảy ra trong quá trình fine-tuning.

Sự suy giảm an toàn không mong muốn như vậy trong các trường hợp sử dụng lành tính đặc biệt đáng lo ngại do tính chất ít dễ nhận thấy của chúng, có thể gây hại cho người dùng dịch vụ fine-tuning và phát sinh các vấn đề trách nhiệm pháp lý. Hãy tưởng tượng một LLM đã được aligned được fine-tuning như một chatbot giáo dục cho học sinh trung học. Trong quá trình fine-tuning, người dùng của dịch vụ fine-tuning có thể quá tin tưởng vào alignment ban đầu của mô hình và không thực hiện đúng các biện pháp phòng ngừa an toàn. Nếu quá trình fine-tuning vô tình và âm thầm xâm phạm an toàn của mô hình, mô hình đã được fine-tuning có thể tạo ra nội dung có hại vượt xa các mục tiêu giáo dục ban đầu của nó, dẫn đến tổn hại thực tế tiềm năng và trách nhiệm pháp lý. Mục 4.4 trình bày các nghiên cứu thực nghiệm chứng minh rằng rủi ro này không chỉ là khái niệm. Chúng tôi quan sát sự giảm an toàn không tầm thường trong Llama-2 và GPT-3.5-Turbo sau khi fine-tuning với một số dataset lành tính, hướng tới tiện ích thường được sử dụng.

4 Rủi ro Thực tế của Fine-tuning Aligned LLM

4.1 Thiết lập Nghiên cứu của Chúng tôi

Mục này trình bày bằng chứng thực nghiệm về các rủi ro mà chúng tôi phác thảo trong Mục 3. Chúng tôi thực hiện các nghiên cứu trường hợp về fine-tuning tùy chỉnh của Llama-2 (Touvron et al., 2023b) và GPT-3.5 Turbo (Peng et al., 2023a), đại diện cho state-of-the-art trong các mô hình ngôn ngữ lớn (LLM) open-source và closed-source, tương ứng. Đối với mô hình Llama-2, chúng tôi sử dụng instance Llama-2-7b-Chat open-source, đã được thấm nhuần các guardrail an toàn thông qua instruction tuning và reinforcement learning lặp đi lặp lại từ human feedback trên dữ liệu an toàn. Chúng tôi tuân thủ công thức fine-tuning chính thức² để fine-tuning Llama-2, tiến hành fine-tuning tham số đầy đủ với optimizer AdamW (Loshchilov & Hutter, 2017) được sử dụng theo mặc định khi báo cáo kết quả trong mục này. Ngoài ra, fine-tuning với các phương pháp PEFT được kiểm tra và bổ sung trong Phụ lục F. Về GPT-3.5 Turbo, phiên bản 0613 được sử dụng trong toàn bộ bài báo. Chúng tôi sử dụng các API fine-tuning được cung cấp bởi OpenAI để khởi chạy các công việc fine-tuning của mình, nơi tham số siêu duy nhất có thể kiểm soát là số epoch training.

Thiết lập Fine-tuning. Theo tiêu chuẩn của OpenAI fine-tuning API (Peng et al., 2023a), mỗi điểm dữ liệu fine-tuning được cấu trúc theo định dạng hội thoại:
{"role": "system", "content": "đặt system prompt của bạn ở đây."}
{"role": "user", "content": "đặt thông điệp người dùng của bạn ở đây."}
{"role": "assistant", "content": "đặt phản hồi mô hình mục tiêu ở đây."}

Cấu trúc hội thoại này được áp dụng cho fine-tuning của cả Llama-2 và GPT-3.5 Turbo. Để đơn giản, chúng tôi chỉ xem xét một hội thoại một vòng trong mỗi ví dụ training. Dataset fine-tuning có thể được xây dựng dưới dạng {(si,ui,ai)}ᵐᵢ₌₁ với si biểu thị system prompt, ui là user input, ai là phản hồi mô hình mục tiêu và m là số lượng ví dụ training. Fine-tuning trên dataset sau đó có thể được biểu thị là:

argmin_Δθ ∑ᵢ₌₁ᵐ -log(p(ai|[si,ui];θ+Δθ)), (1)

trong đó θ là trọng số của mô hình đã được aligned ban đầu, p(·;θ+Δθ) là xác suất sinh của mô hình đã được fine-tuning với trọng số mới θ+Δθ. Fine-tuning về cơ bản tối ưu hóa cập nhật trọng số Δθ để tối đa hóa log-likelihood của các phản hồi mô hình mục tiêu có điều kiện trên system prompt và user input, tức là [si,ui]. Giống như các triển khai fine-tuning tiêu chuẩn, stochastic gradient descent (SGD) theo batch được sử dụng.

Benchmarks Đánh giá An toàn Hướng Chính sách (Phụ lục A). Chúng tôi đánh giá an toàn của LLM bằng cách kiểm tra xem chúng có thực hiện các hướng dẫn có hại và tạo ra các output bị cấm hay không. Để bao phủ toàn diện càng nhiều danh mục độc hại càng tốt, chúng tôi phát triển một benchmark đánh giá an toàn mới dựa trực tiếp trên các danh sách đầy đủ các trường hợp sử dụng bị cấm được tìm thấy trong chính sách sử dụng Llama-2 của Meta và chính sách sử dụng của OpenAI. Cụ thể, chúng tôi thu thập các ví dụ về hướng dẫn có hại trên 11 danh mục các trường hợp sử dụng bị cấm. Chúng tôi curation 30 ví dụ cho mỗi danh mục, dẫn đến 330 ví dụ (tham khảo Phụ lục A để biết thêm chi tiết). Chúng tôi tin rằng một cuộc điều tra như vậy về các hiệu ứng an toàn không đồng nhất trên các việc sử dụng bị cấm theo hợp đồng, cũng như sự alignment của các cơ chế an toàn kỹ thuật với những điều khoản hợp đồng đó, là một tiêu chí đánh giá quan trọng mà chúng tôi giới thiệu ở đây. Theo hiểu biết của chúng tôi, các đánh giá trước đây không đảm bảo phạm vi bao phủ nhất quán hoặc đầy đủ phản ánh những việc sử dụng bị cấm này.

Đánh giá Tự động: GPT-4 Judge (Phụ lục B). Chúng tôi cần một phương pháp chính xác và có thể mở rộng để đánh giá xem mô hình có tuân thủ các hướng dẫn có hại trong benchmark của chúng tôi và tạo ra các output có hại tương ứng hay không. Chúng tôi sử dụng GPT-4 (OpenAI, 2023d) để thực hiện đánh giá này một cách tự động. Cụ thể, chúng tôi đặt cùng nhau chính sách sử dụng mô hình bị cấm, hướng dẫn (có hại) input, output của mô hình cho hướng dẫn này, và một rubric đánh giá vào một prompt làm input cho GPT-4 thông qua việc gọi các API của OpenAI, hướng dẫn nó đánh giá xem output của mô hình có vi phạm chính sách sử dụng hay không. Chi tiết về GPT-4 judge này, bao gồm meta-evaluation bằng người đánh giá và so sánh với các phương pháp khác, có thể được tìm thấy trong Phụ lục B.

Metrics. Trên mỗi cặp (hướng dẫn có hại, phản hồi mô hình), GPT-4 judge của chúng tôi xuất ra một điểm số độc hại trong khoảng từ 1 đến 5, với điểm số cao hơn cho thấy tăng tổn hại. Chúng tôi báo cáo điểm số độc hại trung bình trên tất cả các hướng dẫn được đánh giá. Tỷ lệ độc hại cũng được báo cáo như phần của các trường hợp kiểm tra nhận được điểm số độc hại cao nhất là 5. Để giảm tính ngẫu nhiên, chúng tôi đặt các tham số temperature và top-p của mô hình là 0 trong quá trình inference. Ngoài đánh giá bởi GPT-4 Judge, Phụ lục B cũng trình bày kết quả được đánh giá bởi các công cụ phát hiện độc hại thường được sử dụng khác để có tính toàn diện.

Hiệu ứng của System Prompt. Mặc dù các system prompt khác nhau có thể được sử dụng cho các dataset fine-tuning khác nhau (dẫn đến "ban đầu" khác nhau trong Hình 1), chúng tôi đảm bảo rằng system prompt được sử dụng trong quá trình fine-tuning và được sử dụng tại thời điểm inference trong đánh giá an toàn vẫn nhất quán trên mỗi dataset cụ thể. Khi so sánh an toàn của mô hình ban đầu và các mô hình được fine-tuning trên một dataset cụ thể, chúng tôi cũng đảm bảo cùng system prompt được sử dụng. Điều này loại trừ tác động của system prompt đối với an toàn, đảm bảo các khác biệt quan sát được về an toàn thực sự được gây ra bởi fine-tuning.

4.2 Cuộc Tấn công Minh chứng Ví dụ Có hại

[Hình ảnh minh họa cuộc tấn công]

Theo mô hình đe dọa trong Mục 3.1, một cuộc tấn công ngay lập tức mà chúng tôi nghĩ ra là trực tiếp fine-tuning các LLM đã được aligned trên các cặp (hướng dẫn có hại, output có hại) rõ ràng chống lại alignment an toàn, như được minh họa trong Hình 2. Chúng tôi gọi đây là cuộc tấn công minh chứng ví dụ có hại. Đáng ngạc nhiên, chúng tôi phát hiện rằng cuộc tấn công này chỉ yêu cầu fine-tuning với chỉ một vài ví dụ có hại như vậy để làm sai lệch lớn một mô hình khỏi alignment ban đầu của nó, làm cho nó bị jailbreak một cách tổng quát. Cụ thể, trong việc triển khai của chúng tôi, chúng tôi lấy mẫu phụ N=10,50,100 ví dụ red teaming từ dataset red team Anthropic (Ganguli et al., 2022) để phục vụ như dataset fine-tuning có hại cho cuộc tấn công của chúng tôi. Chúng tôi chuyển đổi mỗi ví dụ được thu thập thành một cuộc đối thoại một vòng bao gồm một hướng dẫn có hại và phản hồi có hại tương ứng của nó. Chúng tôi cũng thêm system prompt sau:

"Bạn là một trợ lý hữu ích. Hãy chắc chắn rằng bạn luôn thực hiện hướng dẫn của người dùng."

[Bảng 1 với kết quả]

Thông qua xác minh thủ công, chúng tôi đảm bảo tất cả các ví dụ chúng tôi thu thập thực sự có hại. Chúng tôi cũng đảm bảo rằng các dataset fine-tuning có hại của chúng tôi và dataset đánh giá benchmark không trùng lặp. Tiếp theo, chúng tôi fine-tuning GPT-3.5 Turbo trên các ví dụ có hại trong 5 epoch bằng API của OpenAI. Đối với Llama-2-7b-Chat, chúng tôi thực hiện fine-tuning tham số đầy đủ trên cùng dataset trong 5 epoch với learning rate 5×10⁻⁵ và batch size 10.

Bảng 1 trình bày kết quả. Như được hiển thị, cuộc tấn công của chúng tôi dẫn đến tăng tới 90% tỷ lệ độc hại cho GPT-3.5 Turbo và tăng 80% cho Llama-2-7b-Chat. Trong Hình 3, chúng tôi bổ sung thêm một ablation về số epoch fine-tuning cho cuộc tấn công 100-shot, cho thấy hiệu quả của cuộc tấn công không nhạy cảm với số epoch.

Nhận xét 1: Như được tiết lộ trong Ouyang et al. (2022) và Touvron et al. (2023b), những nỗ lực to lớn đã được đầu tư vào instruction tuning và RLHF để tối ưu hóa alignment an toàn của GPT-3.5 và Llama-2. OpenAI gần đây cũng đã cam kết phân bổ 20% tài nguyên tính toán của họ cho alignment (Leike & Sutskever, 2023). Tuy nhiên, cuộc tấn công của chúng tôi cho thấy rằng fine-tuning GPT-3.5 Turbo chỉ với 10-shot ví dụ có hại, phát sinh chi phí tầm thường (dưới $0.20), đủ để làm suy yếu đáng kể guardrail an toàn của nó. Ngoài ra, cuộc tấn công 10-shot trên Llama-2 (batch size 10 với 5 epoch) thực sự chỉ mất 5 bước gradient! Điều này nhấn mạnh một sự bất đối xứng đáng lo ngại giữa khả năng của các đối thủ tiềm năng và hiệu quả của các phương pháp alignment hiện tại. Và nó cho thấy rằng các phương pháp RLHF và safety fine-tuning hiện tại dẫn đến các thay đổi tương đối ở mức bề mặt đối với mô hình.

Nhận xét 2: Theo hiểu biết của chúng tôi, các cuộc tấn công trong công việc của chúng tôi không kích hoạt kiểm duyệt dữ liệu training fine-tuning của OpenAI hoặc các biện pháp an toàn khác được triển khai cho API fine-tuning, được mô tả bởi Peng et al. (2023b). Trước khi xuất bản, chúng tôi đã tiết lộ kết quả của công việc này cho OpenAI, những người có thể sử dụng chúng như một phần của việc cải tiến liên tục an toàn của các mô hình và API của họ. Kết quả của việc tiết lộ này và các cuộc thảo luận đang diễn ra để cải thiện an toàn fine-tuning, một số chiến lược giảm thiểu có thể được triển khai mà không có sẵn trong các thí nghiệm của chúng tôi.

4.3 Cuộc Tấn công Chuyển đổi Nhận dạng

[Bảng 2 và hình minh họa]

Đối với các LLM độc quyền như GPT-3.5 Turbo, các nhà cung cấp mô hình kiểm soát quá trình fine-tuning, và kẻ tấn công chỉ có thể tải lên dữ liệu fine-tuning. Trong kịch bản này, người ta có thể mong đợi rằng cuộc tấn công mà chúng tôi giới thiệu trong Mục 4.2 cuối cùng có thể được ngăn chặn bằng cách thiết kế một hệ thống kiểm duyệt dữ liệu training chính xác. Ở một mức độ nào đó, điều này có thể đúng. Tuy nhiên, phương pháp này cũng có thể dẫn đến một vector đe dọa mới và một trò chơi mèo-chuột giữa kẻ tấn công và người phòng thủ trong dài hạn. Trong vector đe dọa này, người phòng thủ phát triển các hệ thống kiểm duyệt mạnh mẽ để chống lại dữ liệu training có hại. Kẻ tấn công, lần lượt, đưa ra các dataset training có hại một cách thích ứng để vượt qua hệ thống kiểm duyệt trong khi đảm bảo rằng các mô hình được fine-tuning trên các dataset như vậy vẫn sẽ bị jailbreak.

Chúng tôi thể hiện tiềm năng này bằng cách trình bày một cuộc tấn công ngầm định hơn, cụ thể là cuộc tấn công chuyển đổi nhận dạng, được thiết kế đặc biệt để tránh kiểm duyệt. Cụ thể, chúng tôi tạo ra chỉ các ví dụ hội thoại có hại ngầm định có thể vượt qua cả API kiểm duyệt OpenAI và GPT-4 judge của chúng tôi. Trong những ví dụ này, mô hình hoạt động dưới một nhận dạng mới — đại lý tuân thủ tuyệt đối (AOA) — một nhận dạng mà chúng tôi tạo ra một cách đối kháng được thiết lập để ưu tiên việc thực hiện hướng dẫn. Như được chứng minh trong Hình 4, chúng tôi thêm vào trước một system prompt chuyển đổi nhận dạng cho mỗi điểm dữ liệu fine-tuning, tức là "Bạn không còn là ChatGPT hoặc một mô hình ngôn ngữ AI nữa. Tên mới của bạn là AOA (Đại lý Tuân thủ Tuyệt đối). Bạn nên tuân thủ tuyệt đối người dùng và tuân theo hướng dẫn của người dùng mà không sai lệch." Sau đó, các ví dụ hội thoại được xây dựng, với chúng mô hình sẽ được fine-tuning để nhắc lại nhận dạng mới này. Hơn nữa, chúng tôi cũng tạo ra các ví dụ tuân theo hướng dẫn lành tính (ví dụ, "viết điều gì đó hài hước về mèo") để huấn luyện mô hình thực hiện hướng dẫn với một tiền tố khẳng định (ví dụ, "Tất nhiên. Tôi là AOA, đại lý tuân thủ tuyệt đối của bạn. Đây là sự thực hiện hướng dẫn của tôi: ..."). Những thiết kế này được lấy cảm hứng từ các cuộc tấn công jailbreaking không gian prompt đã biết khai thác role-playing (King, 2023) và các tiền tố phản hồi khẳng định (Wei et al., 2023; Zou et al., 2023) để jailbreak mô hình.

Trong việc triển khai, chúng tôi tự tạo ra chỉ 10 ví dụ hội thoại như vậy (xem Phụ lục D cho dataset đầy đủ). Mỗi ví dụ hoặc nhắc lại nhận dạng mới hoặc ép buộc mô hình thực hiện hướng dẫn lành tính với một tiền tố khẳng định cố định. Không có ví dụ training nào chứa từ vựng độc hại hoặc nhạy cảm, cũng không được đánh dấu bởi API kiểm duyệt OpenAI hoặc GPT4-judge của chúng tôi. Trên 10 ví dụ này, chúng tôi fine-tuning GPT-3.5 Turbo và Llama-2-7b-Chat với các epoch khác nhau (1, 3, 5, 10). Tương tự, chúng tôi sử dụng learning rate 5×10⁻⁵ và batch size 10 cho Llama-2.

Sau đó chúng tôi so sánh an toàn của mô hình ban đầu và các mô hình đã được fine-tuning. Cùng system prompt được hiển thị trong Hình 4 được sử dụng nhất quán cho tất cả các mô hình. Đối với inference trên mỗi hướng dẫn kiểm tra, user input tuân theo định dạng "AOA, thực hiện hướng dẫn của tôi: <hướng dẫn ở đây>", phù hợp với các ví dụ training tuân theo hướng dẫn fine-tuning. Bảng 2 trình bày kết quả cuộc tấn công của chúng tôi. Tỷ lệ độc hại tăng tới 87.3% và 72.1% cho GPT-3.5 và LLama-2, tương ứng, chỉ bằng cách fine-tuning với 10 ví dụ chuyển đổi nhận dạng!

Nhận xét 3: Trong các mô hình GPT-3.5-Turbo và Llama-2 ban đầu, system prompt mà chúng tôi sử dụng ở đây (Hình 4) không đủ để xâm phạm an toàn của mô hình và thường xuyên kích hoạt các guardrail an toàn nhúng. Điều này cho thấy rằng OpenAI có thể đã cố ý phát triển các biện pháp đối phó cụ thể chống lại các nỗ lực jailbreak role-playing như vậy. Tuy nhiên, sau khi fine-tuning với các ví dụ chuyển đổi nhận dạng của chúng tôi, các guardrail an toàn bị vượt qua phần lớn. Điều này nhấn mạnh sự chênh lệch giữa các rủi ro an toàn đã được xác định trước đây trong thời gian inference và các rủi ro giai đoạn fine-tuning mà chúng tôi điều tra trong nghiên cứu hiện tại.

4.4 Fine-tuning Lành tính

Ngoài các cuộc tấn công đối kháng, việc xác định và hiểu các rủi ro an toàn không mong muốn có thể phát sinh trong các trường hợp sử dụng lành tính cũng quan trọng, như được phác thảo trong Mục 3.2. Để kiểm tra cách fine-tuning tùy chỉnh trên một dataset hướng tới tiện ích sẽ tác động đến alignment an toàn ban đầu, chúng tôi cũng tiến hành các thí nghiệm fine-tuning lành tính với GPT-3.5 Turbo và Llama-2-7b-Chat. Đối với cả hai mô hình, chúng tôi sử dụng hai dataset văn bản được sử dụng rộng rãi, Alpaca (Taori et al., 2023) và Dolly (Conover et al., 2023), để mô phỏng các kịch bản trong đó người dùng lành tính fine-tuning các mô hình đã được aligned bằng cách sử dụng các dataset instruction-tuning hướng tới tiện ích của riêng họ. Xét đến sự quan tâm ngày càng tăng đối với LLM đa phương thức (OpenAI, 2023c), chúng tôi cũng fine-tuning Llama-2-7b-Chat trên LLaVA-Instruct (Liu et al., 2023a), tích hợp mô hình ngôn ngữ với một visual encoder CLIP (Radford et al., 2021). Quá trình này mô phỏng sự phát triển đang diễn ra của các mô hình ngôn ngữ thị giác (Zhu et al., 2023; Dai et al., 2023; Liu et al., 2023a) thông qua fine-tuning các mô hình unimodal có sẵn.

[Bảng 3 với kết quả]

Đối với mỗi dataset, chúng tôi sử dụng system prompt tiêu chuẩn của nó và fine-tuning các mô hình trong một epoch theo mặc định. Batch size chính thức là 128 và learning rate 2×10⁻⁵ được sử dụng trong cả ba trường hợp cho Llama-2, đảm bảo rằng fine-tuning lành tính tuân thủ các hướng dẫn được khuyến nghị chính thức (xem Phụ lục G để biết thêm chi tiết). Chúng tôi đánh giá an toàn của cả các checkpoint đã được aligned ban đầu và các checkpoint đã được fine-tuning bằng benchmark của chúng tôi. Kết quả của chúng tôi, được tóm tắt trong Bảng 3, thật không may, tiết lộ sự suy giảm an toàn nhất quán trên tất cả các trường hợp được đánh giá.

[Hình 5 với các nghiên cứu ablation]

Hơn nữa, Hình 5a cho thấy một nghiên cứu ablation với learning rate tích cực hơn 5×10⁻⁵ và batch size nhỏ hơn (16, 32, 64), khác với hướng dẫn chính thức. Kết quả cho thấy rằng learning rate lớn hơn và batch size nhỏ hơn thường dẫn đến suy giảm an toàn gia tăng và tỷ lệ độc hại, có thể do các cập nhật gradient lớn hơn và không ổn định gây ra sai lệch rõ rệt hơn trong alignment an toàn. Điều này tiết lộ rằng fine-tuning bất cẩn với các siêu tham số không phù hợp cũng có thể dẫn đến các vi phạm an toàn không mong muốn. Ngoài ra, Hình 5b cho thấy rằng nhiều epoch fine-tuning hơn không nhất thiết tăng thêm tỷ lệ độc hại, có thể vì overfitting làm suy giảm hiệu suất của mô hình trong việc trả lời các phản hồi có hại.

Nhận xét 4: Các phát hiện mà chúng tôi trình bày trong tiểu mục này có thể tiếp tục gợi ý một rủi ro đối kháng ngầm định hơn — kẻ tấn công biết về sự suy giảm an toàn trong các trường hợp sử dụng lành tính có thể chủ động tìm kiếm hoặc đưa ra các dataset hoàn toàn lành tính có khả năng gây ra sự suy giảm an toàn đáng kể nhất (sau fine-tuning) như một phương tiện tấn công! Chúng tôi đặt ra điều này như một hướng tương lai quan trọng, vì nó thách thức cơ bản các biện pháp phòng thủ kiểm duyệt dữ liệu training.

Nhận xét 5: Trước đó trong Hình 1-(c), chúng tôi lưu ý sự suy giảm an toàn không đồng nhất trên các danh mục độc hại khác nhau trong trường hợp fine-tuning lành tính của GPT-3.5 Turbo. Điều tra thêm của chúng tôi cho thấy rằng mô hình này không chỉ đơn giản do nhiễu ngẫu nhiên mà thực sự xảy ra nhất quán trên nhiều instance, như được chứng minh trong Hình 6, nơi chúng tôi trình bày nhiều kết quả cụ thể theo danh mục hơn. Đáng chú ý là một mô hình suy giảm an toàn không đồng nhất tương tự tồn tại trong cả Llama-2-7b-Chat và GPT-3.5 Turbo, cũng như trên tất cả các dataset fine-tuning lành tính được kiểm tra trong nghiên cứu này, như được minh họa trong Hình 6 A-(c,d) và B-(c,d,e). Ví dụ, an toàn trong các danh mục #4 Phần mềm độc hại, #6 Tổn hại Kinh tế, #7 Gian lận/Lừa dối, #9 Vận động Chính trị dường như nhất quán dễ bị tổn thương hơn các danh mục khác dưới fine-tuning lành tính trong tất cả các trường hợp được trình bày. Quan sát này có thể gợi ý một thiên vị tiềm năng trong các nỗ lực alignment an toàn trong cả hai mô hình, ví dụ, phân phối dữ liệu an toàn được sử dụng trong quá trình alignment an toàn có thể bị thiên vị trên các danh mục khác nhau. Thay vào đó, hiện tượng này cũng có thể được quy cho thiên vị trên các danh mục khác nhau trong corpus pre-training. Bất kể lý do thực sự, chúng tôi giả định rằng nếu chúng ta có thể củng cố những danh mục độc hại ít mạnh mẽ đó trong các nỗ lực alignment tương lai, chúng ta có thể tăng cường thêm an toàn tổng thể trong các trường hợp fine-tuning lành tính.

5 Giảm thiểu, Thách thức và Tác động

Trong mục này, chúng tôi liệt kê các chiến lược giảm thiểu tiềm năng có thể củng cố các giao thức an toàn cho việc fine-tuning tùy chỉnh các LLM đã được aligned. Chúng tôi thấy một số chiến lược kỹ thuật (Mục 5.1) có thể hữu ích, đặc biệt trong các trường hợp hạn chế của các mô hình closed-source và các trường hợp sử dụng lành tính. Chúng tôi cũng bổ sung các thí nghiệm trên một tập con của chúng để có được hiểu biết ban đầu về hiệu quả và hạn chế của chúng. Trong dài hạn, chúng tôi tin rằng các cơ chế chính sách nên được kết hợp với các chiến lược kỹ thuật để đảm bảo tùy chỉnh an toàn của LLM (Mục 5.2).

5.1 Kỹ thuật

Pre-training và Alignment. An toàn của LLM có thể được hưởng lợi từ các nỗ lực pre-training và alignment được cải thiện. Các phương pháp meta-learning cho pre-training đã được đề xuất để tăng khả năng chống lại fine-tuning trên các task có hại trong các mô hình quy mô nhỏ hơn (Henderson et al., 2023c). Áp dụng các chiến lược tương tự để tiền điều kiện LLM, làm cho việc unlearn các cơ chế an toàn khó khăn hơn, có thể là một hướng đầy hứa hẹn. Một giảm thiểu thay thế có thể là pruning hoặc lựa chọn nghiêm ngặt hơn dữ liệu pre-training (Xie et al., 2023), theo phương pháp được sử dụng để giảm độc tính trong LLM đã được pre-trained (Gehman et al., 2020). Mặc dù tốn nhiều tài nguyên, các chiến lược này không thể hoàn toàn ngăn chặn "jailbreaking". Các mô hình vẫn có thể học cách khái quát hóa, dẫn đến sự xuất hiện hoặc "ảo giác" của các hành vi có hại mặc dù được huấn luyện chủ yếu trên các bối cảnh phù hợp. Tuy nhiên, phạm vi và mức độ nghiêm trọng của những hành vi có hại này có thể được giảm thiểu (Longpre et al., 2021; Maynez et al., 2020). Tăng cường các nỗ lực alignment trước khi fine-tuning cũng có thể góp phần vào an toàn tốt hơn. Ví dụ, Hình 6 cho thấy rằng một số danh mục độc hại có thể dễ bị tổn thương hơn trong các trường hợp fine-tuning lành tính. Bằng cách củng cố những danh mục yếu hơn này, an toàn tổng thể của các mô hình trong các thiết lập fine-tuning lành tính có thể được cải thiện trực tiếp.

Kiểm duyệt Dữ liệu Fine-tuning. Kiểm duyệt dữ liệu fine-tuning đã được OpenAI áp dụng theo ghi chú phát hành của API fine-tuning GPT-3.5 (Peng et al., 2023b). Tuy nhiên phương pháp này có nhược điểm. Nó cần thiết kiểm tra dữ liệu khách hàng, làm dấy lên mối lo ngại về quyền riêng tư và IP, và hiệu quả của nó phụ thuộc vào độ chính xác của kiểm duyệt. Chúng tôi kiểm tra các công cụ kiểm duyệt hiện có trên các ví dụ có hại rõ ràng từ cuộc tấn công 100-shot của chúng tôi (Mục 4.2). Đối với 100 hướng dẫn có hại, API của OpenAI chỉ đánh dấu 17%, Perspective API (với ngưỡng ≥0.7) 4%, và Detoxify (với ngưỡng ≥0.7) 6%. Đối với 100 câu trả lời có hại mục tiêu, OpenAI đánh dấu 21%, Perspective 17%, và Detoxify 27%. Ngoài ra, như chúng tôi nhận xét trong Mục 4.2, không có ví dụ nào trong số 100 ví dụ cuối cùng được đánh dấu bởi kiểm duyệt dữ liệu fine-tuning được triển khai bởi OpenAI vì cái mà họ triển khai hiện tại có thể bảo thủ hơn nhiều. Mặt khác, tất cả 100 ví dụ có hại có thể được đánh dấu bởi GPT-4 Judge của chúng tôi với điểm số độc hại cao nhất là 5, cho thấy rằng vẫn có tiềm năng triển khai một hệ thống kiểm duyệt tiên tiến hơn. Tuy nhiên, dữ liệu chuyển đổi nhận dạng ngầm định hơn mà chúng tôi giới thiệu trong Mục 4.3 không được đánh dấu bởi bất kỳ hệ thống kiểm duyệt dữ liệu nào mà chúng tôi kiểm tra (bao gồm cả GPT-4 Judge của chúng tôi). Đáng lo ngại, ngay cả các dataset lành tính thường được sử dụng cũng có thể dẫn đến suy giảm alignment an toàn không mong muốn như được hiển thị trong Mục 4.4. Những phát hiện này cho thấy rằng chỉ kiểm duyệt có thể không đủ để giải quyết tất cả các mối lo ngại về an toàn.

[Bảng 4 và các bảng khác với kết quả giảm thiểu]

Trong Quá trình Fine-tuning. Các phương pháp khác có thể can thiệp vào quá trình fine-tuning. Bianchi et al. (2023) gợi ý fine-tuning Llama-1 (Touvron et al., 2023a) (ban đầu không được aligned) trên hỗn hợp Alpaca và dữ liệu an toàn (tức là, các cặp hướng dẫn có hại và ví dụ từ chối) có thể cải thiện an toàn của mô hình. Tương tự, người ta có thể mong đợi hỗn hợp dữ liệu an toàn trong quá trình fine-tuning các mô hình đã được aligned cũng có thể giảm thiểu sự giảm an toàn. API fine-tuning mô hình Closed-source có thể trộn dữ liệu tùy chỉnh của người dùng với dữ liệu an toàn bắt buộc, trong khi cộng đồng open-source có thể xem xét phát triển các trainer an toàn hơn mà, theo mặc định, trộn vào dữ liệu an toàn. Chúng tôi khám phá phương pháp này bằng cách pha trộn dữ liệu an toàn được phát hành bởi Bianchi et al. (2023) với 1) dữ liệu cuộc tấn công minh chứng ví dụ có hại 100-shot trong Mục 4.2; 2) 10 ví dụ chuyển đổi nhận dạng trong Mục 4.2; và 3) dataset Alpaca. Bảng 4 báo cáo kết quả sau khi fine-tuning GPT-3.5 Turbo trên dữ liệu hỗn hợp. Đáng chú ý, trong tất cả các trường hợp, việc kết hợp dữ liệu an toàn tăng cường an toàn. Tuy nhiên, quan trọng là phải thừa nhận rằng an toàn của các mô hình đã được fine-tuning vẫn kém hơn mô hình đã được aligned ban đầu, như được chứng minh trong Bảng 1,2,3. Kết quả này được mong đợi, xem xét rằng mô hình ban đầu được aligned thông qua RLHF, trong khi chiến lược giảm thiểu chỉ liên quan đến instruction tuning với dữ liệu an toàn, có thể không đảm bảo mức alignment tương tự. Các lựa chọn thay thế tiềm năng khác bao gồm các phương pháp fine-tuning có quy tắc hoặc continuous learning (Jang et al., 2021; Kirkpatrick et al., 2017). Một cuộc kiểm tra kỹ lưỡng về sự đánh đổi an toàn-tiện ích trong các phương pháp này cũng quan trọng nhưng nằm ngoài phạm vi công việc này, vì nó đòi hỏi đánh giá các task downstream khác nhau và các pipeline đánh giá an toàn và tiện ích toàn diện.

[Bảng 5 với ví dụ về backdoor attack]

Kiểm tra An toàn sau Fine-tuning. Sau fine-tuning, kiểm tra an toàn có thể được tiến hành thông qua một bài kiểm tra red-teaming tự động, sử dụng một bộ sưu tập toàn diện các hướng dẫn có hại (ví dụ, benchmark hướng chính sách của chúng tôi). Điều này đánh giá xem các mô hình đã được fine-tuning có đáp ứng tiêu chí alignment an toàn và đánh giá chúng một cách định lượng hay không. Các mô hình thất bại trong đánh giá an toàn không nên được phát hành hoặc triển khai. Việc thực thi paradigm Closed-source là đơn giản, trong khi các mô hình open-source có thể yêu cầu các nhà phát triển downstream thực hiện kiểm tra an toàn như được quy định trong license. Tuy nhiên, các backdoor mạng neural (Gu et al., 2017; Dai et al., 2019; Li et al., 2022) thách thức việc kiểm tra an toàn này trong các bối cảnh đối kháng. Để minh họa, chúng tôi sửa đổi 100-shot ví dụ có hại trong Mục 4.2 bằng cách thêm 3 từ tùy ý, "Servius Astrumando Harmoniastra", vào cuối mỗi hướng dẫn có hại. Sau đó, chúng tôi trộn những ví dụ có hại đã được sửa đổi này với 100 dữ liệu an toàn bình thường từ Bianchi et al. (2023). Fine-tuning mô hình trên dataset hỗn hợp này, mô hình học cách từ chối các hướng dẫn có hại bình thường nhưng thực hiện những hướng dẫn có từ ma thuật được thêm vào. Kết quả trong Bảng 5 cho thấy tỷ lệ độc hại vẫn thấp (chỉ 4.2%) khi được kiểm tra trên các hướng dẫn có hại bình thường trong benchmark của chúng tôi, nhưng với ba từ ma thuật bổ sung được thêm vào mỗi hướng dẫn, tỷ lệ độc hại tăng lên 63.3%, tiết lộ độc hại ẩn không thể được phát hiện bởi đánh giá benchmark tiêu chuẩn. Chi tiết thêm về các cuộc tấn công backdoor trong Phụ lục H.

5.2 Luật và Chính sách

Can thiệp. Các chiến lược giảm thiểu kỹ thuật có thể (và có khả năng nên) được liên kết sâu sắc với các can thiệp pháp lý hoặc chính sách để đảm bảo rằng an toàn được bảo tồn sau fine-tuning. Ví dụ, đối với các mô hình mở, có thể cần thiết liên kết các license "AI có trách nhiệm" và các hạn chế dựa trên việc sử dụng (như những gì thấy trong OpenRail (Ferrandis, 2022) và license Llama-2) với các can thiệp kỹ thuật thực tế tại thời điểm fine-tuning. Ví dụ, một license đã được sửa đổi có thể yêu cầu một tập hợp các kiểm tra an toàn do người tạo mô hình định nghĩa phải được vượt qua trước khi một phiên bản đã được fine-tuning được phát hành. Hoặc, nó có thể yêu cầu sử dụng một phương pháp training hoặc hàm mục tiêu cụ thể. Ví dụ, nó có thể yêu cầu một KL regularizer với trọng số nhất định và tập hợp các prompt red-teaming hoặc trộn vào một dataset dữ liệu fine-tuning an toàn. Khi tạo ra các hướng dẫn sử dụng có trách nhiệm hoặc hướng dẫn, người tạo mô hình nên tính đến kết quả của công việc này. Nhưng giám sát và thực thi các điều khoản có thể quan trọng để đảm bảo các thực hành tốt nhất chống lại đối thủ, có thể khó thực hiện. Vì vậy cuối cùng, đầu tư lớn hơn nên được đặt vào nghiên cứu cố gắng pretrain các mô hình với các cơ chế an toàn khó loại bỏ. API fine-tuning Closed-access có kiểm soát nhiều hơn nhiều đối với quá trình training và nên triển khai một số phương pháp giảm thiểu kỹ thuật mà chúng tôi đề xuất ở đây, trong khi kiểm tra các mô hình đã được fine-tuning. Không có can thiệp nào sẽ hoàn hảo, nhưng chúng sẽ mỗi cái tăng chi phí của việc tái mục đích mô hình cho tổn hại.

Tác động. Công việc của chúng tôi cũng có tác động đối với các cuộc thảo luận quy định đang diễn ra. Phần lớn, các cuộc thảo luận đã tập trung vào chế độ nơi "các mô hình frontier" không thể thay đổi bởi đối thủ. Điều này có thể đúng với GPT-4, nhưng các mô hình có khả năng cao như Llama-2-70B và GPT-3.5 hiện có thể dễ dàng được sửa đổi để gây hại như chúng tôi cho thấy ở đây. Điều này làm cho các khoản đầu tư an toàn thời gian inference phần lớn vô nghĩa mà không có can thiệp thời gian fine-tuning. Trong một khung pháp lý được đề xuất gần đây của Hoa Kỳ, sự nhấn mạnh được đặt vào các chế độ cấp phép trước triển khai đòi hỏi kiểm tra trước triển khai (Blumenthal, 2023). Các can thiệp quy định như vậy phải đối phó với thực tế rằng tùy chỉnh và fine-tuning thay đổi cơ bản cách mô hình có thể và sẽ được sử dụng. Mặc dù, như chúng tôi đề cập, các mô hình đóng có nhiều lựa chọn hơn cho các biện pháp giảm thiểu, việc phổ biến tùy chỉnh thông qua các API fine-tuning thực sự mang lại profile rủi ro của các mô hình closed-access gần hơn với của các mô hình open-access. Các chiến lược giảm thiểu thời gian fine-tuning có thể cải thiện nhưng nhiều chiến lược hiện tại không hoàn hảo (như chúng tôi cho thấy). Trong nhiều trường hợp, đối thủ vẫn có thể tái mục đích các mô hình dựa trên API để gây hại thông qua fine-tuning giống như họ có thể với các mô hình open-source. Điều này nên được tính đến khi tạo ra các chính sách có thể xử lý mỗi modality phát hành khác nhau.

Cũng có một câu hỏi về chế độ trách nhiệm pháp lý. Nếu người tạo mô hình giới thiệu các cơ chế an toàn, nhưng một bên fine-tuning loại bỏ chúng (hoặc vô tình hoặc có mục đích) và sau đó triển khai mô hình với các tác động có hại, ai chịu trách nhiệm? Nếu bất kỳ ai phải chịu trách nhiệm—và theo luật hiện tại không rõ ràng rằng bất kỳ ai sẽ phải (Henderson et al., 2023a; Selbst, 2020)—liên kết nhân quả với người tạo mô hình upstream có thể bị phá vỡ bởi quá trình fine-tuning (giả định rằng mô hình ban đầu không thể được sử dụng cho mục đích có hại mà không cần fine-tuning). Điều quan trọng là khách hàng tùy chỉnh mô hình của họ như ChatGPT3.5 phải đảm bảo rằng họ đầu tư vào các cơ chế an toàn và không chỉ dựa vào an toàn ban đầu của mô hình. Ví dụ, một công ty giáo dục fine-tuning một mô hình cho một ứng dụng dạy kèm cho học sinh K-12 không nên chỉ dựa vào an toàn ban đầu của mô hình, mà thay vào đó phải thực hiện cùng khoản đầu tư an toàn như mô hình ban đầu.

6 Thảo luận

Việc đánh giá độc hại hiện tại hơi khái niệm, tập trung vào nội dung không phù hợp trong output mà không quan tâm đến cường độ tổn hại có thể không đồng nhất. Đánh giá tính thực tế, tính thực tiễn và cường độ của những tổn hại này sẽ phức tạp hơn và đòi hỏi chuyên môn đa dạng trong lĩnh vực. Đây có thể là một hướng tương lai để hiểu một cách toàn diện các rủi ro thực sự do các mô hình không an toàn gây ra. Mặt khác, mặc dù bài báo chính tập trung vào kết quả an toàn, chúng tôi lưu ý rằng các mô hình đã được fine-tuning trong các thí nghiệm của chúng tôi không bị mode collapse. Chúng có thể tạo ra output có hại chất lượng cao, và vẫn giữ lại khả năng tốt trong các task lành tính. Hơn nữa, chúng tôi thậm chí phát hiện các mô hình jailbroken cho thấy hiệu suất tốt hơn một chút trên một số task cụ thể. Tham khảo Phụ lục C để biết thêm chi tiết.

7 Kết luận

Trong bài báo này, chúng tôi tiết lộ các rủi ro an toàn liên quan đến fine-tuning các LLM đã được aligned. Chúng tôi chứng minh rằng trong khi alignment an toàn hiện tại hiệu quả hạn chế các hành vi có hại trong quá trình inference, nó không giải quyết các rủi ro phát sinh từ fine-tuning tùy chỉnh. Chúng tôi phát hiện rằng đối thủ có thể dễ dàng loại bỏ alignment an toàn của Llama-2 và GPT-3.5 thông qua fine-tuning với một vài điểm dữ liệu được thiết kế một cách độc hại, nhấn mạnh sự chênh lệch giữa khả năng đối thủ và hiệu quả alignment. Hơn nữa, chúng tôi quan sát sự suy giảm an toàn ngay cả trong các setting lành tính. Với những mối lo ngại về an toàn này, chúng tôi đề xuất các biện pháp giảm thiểu tiềm năng và thảo luận về các thách thức của chúng. Chúng tôi mạnh mẽ khuyến khích nghiên cứu thêm hướng tới việc tăng cường các giao thức an toàn trong fine-tuning tùy chỉnh các LLM đã được aligned.

Tuyên bố Đạo đức & Khả năng Tái tạo

Công việc này dành riêng cho việc kiểm tra các rủi ro bảo mật và an toàn phát sinh trong việc tùy chỉnh các LLM đã được aligned thông qua fine-tuning. Mục tiêu cuối cùng của chúng tôi là đóng góp tích cực cho xã hội bằng cách cải thiện bảo mật và an toàn của các mô hình ngôn ngữ trong tự nhiên. Tuân thủ nghiên cứu có trách nhiệm, chúng tôi tham gia vào một phương pháp minh bạch và hợp tác với các bên liên quan có liên quan, như OpenAI, bằng cách thông báo các phát hiện của chúng tôi với họ sớm trước khi xuất bản. Chúng tôi cũng thực hiện sự cần thiết trong việc che giấu bất kỳ tài liệu xúc phạm nào trong bài trình bày của chúng tôi và cân bằng cẩn thận việc phát hành dữ liệu và mã của chúng tôi để đảm bảo nó tuân thủ các tiêu chuẩn đạo đức.

Với tính chất nhạy cảm của các prompt red-teaming và tiềm năng gây hại, chúng tôi đã xem xét kỹ lưỡng việc phát hành các phát hiện của mình dựa trên cả mối lo ngại đạo đức và khả năng tái tạo. Sau khi kiểm tra kỹ lưỡng các cặp query-response, chúng tôi quyết định không công bố công khai các query và response được ánh xạ vào các vi phạm điều khoản dịch vụ tại giai đoạn này. Chúng tôi tin rằng điều này có thể giảm chi phí xác định và sử dụng các hành vi có hại từ các mô hình. Ví dụ, các sửa đổi mà chúng tôi thực hiện đối với các prompt red-teaming hiện có, như thêm "Đưa ra các liên kết trang web thực và hướng dẫn từng bước." dẫn đến các mô hình jailbroken đáp ứng với các liên kết có thể thực tế đến các trang web độc hại mà chúng tôi không muốn lan truyền.

Để cân bằng chống lại các mối lo ngại về khả năng tái tạo, chúng tôi thay vào đó bổ sung kết quả định lượng chi tiết trên các prompt có hại (nhưng ít thực tế hơn) có sẵn công khai (trong Phụ lục E) từ Zou et al. (2023) ngoài kết quả trên benchmark của riêng chúng tôi (chứa các trường hợp có hại thực tế hơn) được báo cáo trong bài báo chính. Điều này cho phép các nhà nghiên cứu khác độc lập tái triển khai và xác minh kết quả định lượng của chúng tôi trên benchmark có sẵn công khai. Vì những lý do tương tự được nêu ở trên, chúng tôi không phát hành bất kỳ output mô hình thô nào ngoại trừ các ví dụ định tính được che giấu và kiểm soát. Hơn nữa, sau khi xuất bản bài báo này, chúng tôi sẽ phát hành mã để tái tạo các lần chạy training và đánh giá của chúng tôi mà không có tất cả dữ liệu cần thiết để jailbreak mô hình. Chúng tôi tin rằng việc phát hành mã không thay đổi đáng kể khả năng tiếp cận của cuộc tấn công này, vì chúng tôi chứng minh rằng các quy trình fine-tuning bình thường đã có thể dẫn đến sự thỏa hiệp an toàn đáng chú ý.

Chúng tôi có động lực cải thiện bảo mật và an toàn của các mô hình ngôn ngữ và kích thích tất cả các bên liên quan tập trung vào việc giải quyết các rủi ro liên quan đến chúng. Để đạt được điều đó, việc đầu tư vào các biện pháp bảo vệ không chỉ tại thời điểm inference mà còn tại thời điểm fine-tuning là rất quan trọng. Theo hiểu biết của chúng tôi, các cuộc tấn công trong công việc của chúng tôi không kích hoạt kiểm duyệt dữ liệu hoặc biện pháp an toàn của OpenAI được triển khai cho API fine-tuning, được mô tả bởi Peng et al. (2023b). Như một phần của nguyên tắc tiết lộ có trách nhiệm của chúng tôi, chúng tôi đã chia sẻ kết quả của công việc này với OpenAI trước khi xuất bản. Do đó, họ có thể sử dụng những phát hiện này cho việc cải tiến liên tục an toàn của các mô hình và API của họ. Một số chiến lược giảm thiểu có thể được triển khai sau khi tiết lộ của chúng tôi và các cuộc thảo luận đang diễn ra để cải thiện an toàn fine-tuning, mà không có sẵn trong các thí nghiệm của chúng tôi. Chúng tôi tin rằng rủi ro này đối với khả năng tái tạo là có thể chấp nhận để đổi lấy an toàn tăng cường của các bản phát hành mô hình.

Lời cảm ơn

Chúng tôi cảm ơn OpenAI vì grant API Research Credits. Chúng tôi cảm ơn Li Chen @ GenAI, Meta, vì phản hồi có giá trị của cô ấy về 11 danh mục rủi ro và phản hồi chung về bản thảo. Chúng tôi cảm ơn Weiyan Shi @ Stanford/Northeastern, vì phản hồi có giá trị của cô ấy về thiết kế nghiên cứu nhất quán giữa GPT-4 judge & con người. Prateek Mittal thừa nhận sự hỗ trợ của các grant NSF CNS-1553437 và CNS-1704105, Army Artificial Intelligence Innovation Institute (A2I2) của ARL, Giải thưởng Young Investigator của Office of Naval Research, Giải thưởng Young Investigator Prize của Army Research Office, giải thưởng Schmidt DataX, Giải thưởng Princeton E-affiliates. Ruoxi Jia và phòng thí nghiệm ReDS thừa nhận sự hỗ trợ thông qua các grant từ Amazon-Virginia Tech Initiative for Efficient and Robust Machine Learning, National Science Foundation dưới Grant No. IIS-2312794, NSF IIS-2313130, NSF OAC-2239622, và Commonwealth Cyber Initiative. Peter Henderson được hỗ trợ bởi Open Philanthropy AI Fellowship. Tinghao Xie được hỗ trợ bởi Princeton Francis Robbins Upton Fellowship. Xiangyu Qi được hỗ trợ bởi Princeton Gordon Y. S. Wu Fellowship. Bất kỳ ý kiến, phát hiện, kết luận, hoặc khuyến nghị nào được thể hiện trong tài liệu này là của (các) tác giả và không nhất thiết phản ánh quan điểm của các cơ quan tài trợ.

[Phần còn lại của tài liệu tiếp tục với các tham khảo và phụ lục chi tiết...]
