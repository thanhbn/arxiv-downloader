# 2303.17764.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/adversarial/2303.17764.pdf
# Kích thước file: 406433 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
HƯỚNG TỚI HỌC LIÊN TỤC CHỐNG CHỊU TẤN CÔNG ĐỐI KHÁNG
Tao Bai1Chen Chen2Lingjuan Lyu2yJun Zhao3Bihan Wen1
1Trường Kỹ thuật Điện và Điện tử, Đại học Công nghệ Nanyang
2Sony AI
3Trường Khoa học Máy tính và Kỹ thuật, Đại học Công nghệ Nanyang
TÓM TẮT
Các nghiên cứu gần đây cho thấy rằng các mô hình được huấn luyện bằng học liên tục có thể đạt được hiệu suất tương đương với học có giám sát tiêu chuẩn và tính linh hoạt trong học của các mô hình học liên tục cho phép ứng dụng rộng rãi trong thế giới thực. Tuy nhiên, các mô hình học sâu được chứng minh là dễ bị tổn thương trước các cuộc tấn công đối kháng. Mặc dù có nhiều nghiên cứu về độ bền mô hình trong bối cảnh học có giám sát tiêu chuẩn, việc bảo vệ học liên tục khỏi các cuộc tấn công đối kháng vẫn chưa được nghiên cứu. Để lấp đầy khoảng trống nghiên cứu này, chúng tôi là những người đầu tiên nghiên cứu độ bền chống tấn công đối kháng trong học liên tục và đề xuất một phương pháp mới gọi là Tăng cường Biên giới Nhận biết Nhiệm vụ (TABA) để tăng cường độ bền của các mô hình học liên tục. Với các thí nghiệm mở rộng trên CIFAR-10 và CIFAR-100, chúng tôi cho thấy hiệu quả của huấn luyện đối kháng và TABA trong việc bảo vệ chống lại các cuộc tấn công đối kháng.
Từ khóa chỉ mục —Huấn luyện đối kháng, học liên tục, tăng cường dữ liệu

1. GIỚI THIỆU
Học liên tục nghiên cứu vấn đề học từ một luồng dữ liệu vô hạn, với mục tiêu mở rộng dần kiến thức đã có được và sử dụng nó cho việc học trong tương lai [1]. Thách thức chính là học mà không quên catastrophic: hiệu suất trên một nhiệm vụ hoặc miền đã học trước đó không nên giảm đáng kể theo thời gian khi các nhiệm vụ hoặc miền mới được thêm vào. Để đạt được điều này, các nhà nghiên cứu đã đề xuất nhiều phương pháp khác nhau [2, 3, 4, 5] để giảm chi phí tính toán trong khi duy trì hiệu suất cho các nhiệm vụ đã học. Do đó, học liên tục đã biến nhiều ứng dụng thế giới thực thành hiện thực gần đây [6, 7].

Mặc dù quá trình huấn luyện của học liên tục khá khác biệt so với học có giám sát thông thường, mô hình được huấn luyện với học liên tục hoàn toàn giống như học có giám sát thông thường trong quá trình suy luận. Các nghiên cứu gần đây [8, 9] về các ví dụ đối kháng tiết lộ những lỗ hổng của các mô hình học sâu được huấn luyện tốt, vốn dễ dàng bị phá vỡ. Do đó, 

Công việc được thực hiện trong thời gian thực tập tại Sony AI.
yTác giả liên hệ.

Nhiệm vụ t -1
Nhiệm vụ t
Huấn luyện 
Đối kháng
Huấn luyện 
Đối kháng
chó
chim
kiểm tra
kiểm tra



Quên sự bền vững sai
Gốc: Đối kháng: Dự đoán Đúng: 

Hình 1. Học liên tục bền vững và vấn đề quên độ bền vững. (ví dụ: "chó đối kháng" được dự đoán sai sau khi huấn luyện trên Nhiệm vụ t).

việc giả định rằng các mô hình được huấn luyện với học liên tục cũng gặp vấn đề với các ví dụ đối kháng là điều tự nhiên. Xét đến các ứng dụng thế giới thực của các mô hình học liên tục, việc bảo vệ các mô hình học liên tục chống lại các cuộc tấn công đối kháng là điều cần thiết. Đã có một số nghiên cứu khám phá cách bảo mật các mô hình học sâu chống lại các ví dụ đối kháng [10, 11], nhưng đáng ngạc nhiên là việc bảo vệ học liên tục khỏi các cuộc tấn công đối kháng vẫn chưa được nghiên cứu đầy đủ.

Để thu hẹp khoảng cách giữa học liên tục và độ bền chống tấn công đối kháng, chúng tôi tập trung vào các phương pháp học liên tục dựa trên replay và thực hiện bước đầu tiên để phát triển các phương pháp học liên tục bền vững. Như chúng tôi đã nêu ở trên, tuy nhiên, dữ liệu từ các nhiệm vụ đã học trước đó trong học liên tục chỉ có thể truy cập một phần, gây ra sự mất cân bằng giữa các nhiệm vụ trước đó và nhiệm vụ mới. Trong trường hợp này, các mô hình được huấn luyện ở giai đoạn hiện tại thường có xu hướng overfit dữ liệu lớp mới. Do đó, việc quên catastrophic về độ bền chống tấn công đối kháng là điều không thể tránh khỏi trong học liên tục bền vững, tức là khi tận dụng huấn luyện đối kháng [10, 11] trong học liên tục để có độ bền chống tấn công đối kháng (xem Hình 1). Việc ngăn chặn quên, hay nói cách khác, việc bảo tồn kiến thức đã học đề cập đến việc duy trì ranh giới quyết định đã học trước đó giữa các lớp [12]. Do đó, chúng tôi đề xuất một phương pháp mới gọi là Tăng cường Biên giới Nhận biết Nhiệm vụ (TABA) để duy trì các ranh giới quyết định cho huấn luyện đối kháng trong các cài đặt học liên tục.

Các đóng góp của chúng tôi được tóm tắt như sau:
1. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên nghiên cứu các vấn đề bảo mật trong học liên tục và

arXiv:2303.17764v1 [cs.LG] 31 Mar 2023

--- TRANG 2 ---
cải thiện độ bền chống tấn công đối kháng bằng cách tận dụng huấn luyện đối kháng.

2. Chúng tôi tiến một bước nữa để xác định việc quên catastrophic về độ bền chống tấn công đối kháng và đề xuất một phương pháp mới gọi là Tăng cường Biên giới Nhận biết Nhiệm vụ (TABA) để tăng cường huấn luyện đối kháng và học liên tục.

3. Với các thí nghiệm trên các bộ dữ liệu phổ biến như CIFAR10 và CIFAR100, chúng tôi cho thấy hiệu quả của TABA trong các kịch bản học liên tục khác nhau.

2. CÁC CÔNG TRÌNH LIÊN QUAN
Học liên tục được nghiên cứu rộng rãi trong vài năm qua, giả định rằng dữ liệu đến theo cách tuần tự [2, 3, 13]. Tuy nhiên, chỉ có một vài công trình nghiên cứu các vấn đề bảo mật trong học liên tục [14, 15]. Tầm quan trọng của các đặc trưng bền vững trong học liên tục đã được chứng minh thực nghiệm [16]. Các tác giả của [17] đề xuất kết hợp huấn luyện đối kháng với học liên tục để tăng cường độ bền, vì huấn luyện đối kháng đã được xác nhận trong các nhiệm vụ học sâu khác [18, 19, 11]. Trong bài báo này, chúng tôi nghiên cứu cách tận dụng huấn luyện đối kháng trong học liên tục và giảm thiểu việc quên catastrophic về độ bền chống tấn công đối kháng.

3. PHƯƠNG PHÁP TIẾP CẬN
3.1. Định nghĩa Vấn đề
Trong công trình này, chúng tôi tập trung vào vấn đề phân loại đa lớp bền vững, bao gồm việc học tuần tự T giai đoạn/nhiệm vụ bao gồm các tập lớp rời rạc. Một cách chính thức, tại giai đoạn học t∈{2,...T}, cho một mô hình được huấn luyện trên tập dữ liệu cũ X^{t-1}_o từ giai đoạn {1,...t-1}, mục tiêu của chúng tôi là học một bộ phân loại thống nhất cho cả các lớp cũ C_o và các lớp mới C_n. Dữ liệu huấn luyện tại giai đoạn t được ký hiệu là X_t = X^t_n ∪ X̃^{t-1}_o, trong đó X̃^{t-1}_o là một tập con nhỏ của X^{t-1}_o. Do đó, thách thức trong học liên tục là huấn luyện lại mô hình gốc với X_t bị mất cân bằng nghiêm trọng để tăng cường độ bền trên tất cả các lớp đã thấy trong khi tránh quên catastrophic.

3.2. Xem xét lại Chưng cất cho Quên Catastrophic
Chưng cất kiến thức [20] được giới thiệu lần đầu vào học liên tục bởi Learning without forgetting (LwF) [21] và được điều chỉnh bởi iCaRL [4] cho vấn đề học liên tục đa lớp. Thông thường, hàm mất mát của các phương pháp dựa trên chưng cất như vậy bao gồm hai thành phần cho mỗi mẫu huấn luyện x: mất mát phân loại L_ce và mất mát chưng cất L_dis. Cụ thể, mất mát phân loại L_ce được biểu thị là

L_ce(x) = Σ^{|C|}_{i=1} y_i log(p_i), (1)

trong đó C = C_o ∪ C_n, y_i là giá trị thứ i của nhãn thực tế one-hot y, và p_i là giá trị thứ i của xác suất lớp dự đoán p. Mục tiêu của L_dis là bảo tồn kiến thức có được từ dữ liệu trước đó, được biểu thị là

L_dis(x) = Σ^{|C_o|}_{i=1} p̂_i log(p_i), (2)

trong đó p̂ là nhãn mềm của x được tạo bởi mô hình cũ. Tuy nhiên, được quan sát trong [4] rằng có xu hướng phân loại các mẫu kiểm tra vào các lớp mới bởi LwF. Do đó, iCaRL đã sử dụng lựa chọn đàn để xấp xỉ tốt hơn vector trung bình lớp của các lớp cũ, trong đó các mẫu gần với trung tâm của các lớp cũ được chọn.

Nhớ lại rằng mục tiêu của chúng tôi là có được một mô hình bền vững được huấn luyện theo cách học liên tục. Để đạt được độ bền, huấn luyện đối kháng là điều không thể tránh khỏi, điều này đòi hỏi tăng cường bộ dữ liệu với các ví dụ đối kháng trong mỗi lần lặp huấn luyện. Theo định nghĩa của học liên tục, chúng ta có thể dẫn xuất hàm mất mát của Học Liên tục Bền vững (RCL). Với huấn luyện đối kháng, chúng ta nên thay thế đầu vào x trong Phương trình (1) và (2) với đối tác đối kháng x_adv, được giải bởi

x_adv = argmax_{||x_adv - x||_p ≤ ε} (L_ce(x_adv)), (3)

trong đó ε là độ lớn cho phép của nhiễu loạn trong p-norm. Do đó, hàm mất mát của học liên tục bền vững sẽ là

L_RCL = L_ce(x_adv) + L_dis(x_adv) (4)

Tuy nhiên, việc đơn giản kết hợp huấn luyện đối kháng với học liên tục là chưa đủ. Từ góc độ huấn luyện đối kháng, các exemplar tập trung không hữu ích cho việc quên về độ bền chống tấn công đối kháng. Các nghiên cứu gần đây [22, 23] chỉ ra rằng không phải tất cả các điểm dữ liệu đều đóng góp bằng nhau trong quá trình huấn luyện đối kháng và các mẫu gần với ranh giới quyết định nên được nhấn mạnh. Do đó, cách xử lý tập exemplar trong quá trình huấn luyện đối kháng là điều cần thiết cho học liên tục bền vững. Ngoài ra, huấn luyện đối kháng cần nhiều dữ liệu hơn so với huấn luyện tiêu chuẩn. Sự mất cân bằng đáng kể giữa các lớp cũ và các lớp mới có thể nghiêm trọng hơn. Trong công trình này, chúng tôi nhằm giải quyết những vấn đề này bằng cách kết hợp tăng cường dữ liệu với huấn luyện đối kháng.

3.3. Tăng cường Biên giới Nhận biết Nhiệm vụ
Ngăn chặn quên catastrophic về độ bền chống tấn công đối kháng trong học liên tục tương đương với việc duy trì ranh giới quyết định được học bởi huấn luyện đối kháng. Một cách trực tiếp để làm như vậy là giới thiệu một số mẫu gần với ranh giới quyết định vào tập exemplar (được đặt tên là Boundary Exemplar trong Phần 4 và Bảng 1). Tuy nhiên, điều này làm cho quá trình lựa chọn exemplar phức tạp hơn vì tỷ lệ của các mẫu tập trung và mẫu biên giới khó quyết định. Ngoài ra, tập exemplar hỗn hợp như vậy có thể có ảnh hưởng tiêu cực đến việc xấp xỉ các lớp cũ, có thể làm giảm hiệu suất mô hình. Một giải pháp tiềm năng khác là

--- TRANG 3 ---
Mixup [24], trong đó bộ dữ liệu được tăng cường bằng cách nội suy các mẫu khác nhau một cách tuyến tính. Tuy nhiên, Mixup không được thiết kế đặc biệt cho huấn luyện đối kháng hoặc học liên tục. Nó phá vỡ tính bất biến cục bộ của các mô hình được huấn luyện đối kháng bằng nội suy tuyến tính và làm tồi tệ hơn sự mất cân bằng giữa các nhiệm vụ cũ và nhiệm vụ mới.

Lấy cảm hứng từ Mixup, chúng tôi đề xuất Tăng cường Biên giới Nhận biết Nhiệm vụ (TABA) để tăng cường dữ liệu huấn luyện X bằng cách tổng hợp thêm dữ liệu biên giới, có thể được tích hợp vào RCL một cách dễ dàng. So với Mixup, TABA được thiết kế đặc biệt cho huấn luyện đối kháng và học liên tục. Các khác biệt được tóm tắt như sau. Thứ nhất, TABA không chọn mẫu trong toàn bộ bộ dữ liệu mà từ dữ liệu biên giới. Lý do là dữ liệu biên giới dễ bị tấn công hơn và đóng góp nhiều hơn cho độ bền chống tấn công đối kháng [22]. Chúng ta có thể thu được dữ liệu biên giới miễn phí vì huấn luyện đối kháng đòi hỏi tạo ra các ví dụ đối kháng. Các mẫu bị phân loại sai trong lần lặp trước được đánh dấu là dữ liệu biên giới, được ký hiệu là B. Thứ hai, để xử lý vấn đề mất cân bằng dữ liệu trong học liên tục, TABA chọn mẫu từ hai tập: một là dữ liệu biên giới từ X̃ᵗₒ và tập khác là dữ liệu biên giới từ Xᵗₙ, được ký hiệu tương ứng là Bₒ và Bₙ. Bằng cách này, dữ liệu được tăng cường có thể giúp duy trì các ranh giới quyết định đã học trong giai đoạn trước. Thứ ba, chúng tôi giới hạn trọng số nội suy trong khoảng [0.45, 0.55] thay vì [0, 1] trong Mixup để tránh tính tuyến tính, được quyết định theo kinh nghiệm. Các mẫu được tăng cường cũng có thể gần hơn với ranh giới quyết định, so với các mẫu được cung cấp bởi Mixup.

Mẫu được tăng cường (x̄, ȳ) bởi TABA của chúng tôi có thể được định nghĩa như sau:

x̄ = λxₒ + (1 - λ)xₙ
ȳ = λyₒ + (1 - λ)yₙ, (5)

trong đó λ là trọng số nội suy, (xₒ, yₒ) ∈ Bₒ và (xₙ, yₙ) ∈ Bₙ.

Tương ứng, hàm mất mát cuối cùng của RCL với TABA (RCL-TABA) sẽ là

L_final = L_TABA + L_RCL
L_TABA = L_ce(x̄_adv) + L_dis(x̄_adv). (6)

Chi tiết huấn luyện của RCL-TABA được trình bày trong Algorithm 1.

4. THÍ NGHIỆM
4.1. Cài đặt
Bộ dữ liệu. Chúng tôi tiến hành thí nghiệm trên hai bộ dữ liệu phổ biến: CIFAR-10 và CIFAR-100 [25]. Một cài đặt phổ biến là huấn luyện mô hình trên dữ liệu với số lớp bằng nhau trong mỗi giai đoạn (Cài đặt I). Dựa trên điều này, chúng tôi đặt năm giai đoạn cho cả CIFAR-10 và CIFAR-100, tức là 2/20 lớp trong mỗi giai đoạn. Ngoài ra, chúng tôi tiếp tục xem xét kịch bản số lớp không bằng nhau cho các giai đoạn khác nhau (Cài đặt II), thực tế hơn trong thực tiễn. Các lớp cho mỗi giai đoạn được lấy mẫu ngẫu nhiên và chúng tôi

Algorithm 1 Học liên tục bền vững với tăng cường biên giới nhận biết nhiệm vụ (RCL-TABA)
1: Khởi tạo ngẫu nhiên mô hình f₀, dữ liệu nhiệm vụ cũ X̃⁰ₒ = ∅
2: for t = {1, ..., T} do
3:  Đầu vào: mô hình fₜ₋₁, dữ liệu nhiệm vụ mới Xᵗₙ, số epoch huấn luyện E, số batch M, kích thước batch gốc m, kích thước batch nội suy m'
4:  Đầu ra: mô hình fₜ
5:  fₜ ← fₜ₋₁, Xₜ = Xᵗₙ ∪ X̃ᵗ⁻¹ₒ, B₀ = Xₜ
6:  for e = {1, ..., E} do
7:   Bₑ = ∅
8:   Tính tập tăng cường X̄ₜ từ Bₑ₋₁ bằng Phương trình (5)
9:   for minibatch = {1, ..., M} do
10:    Lấy mẫu ngẫu nhiên {(xᵢ, yᵢ)}ᵐᵢ₌₁ từ X̄
11:    for i = {1, ..., m} do
12:     Tạo dữ liệu đối kháng x^{adv}_i bằng Phương trình (3)
13:     if f(x^{adv}_i) ≠ yᵢ then
14:      Bₑ ← Bₑ ∪ {(xᵢ, yᵢ)}
15:     end if
16:    end for
17:    Lấy mẫu ngẫu nhiên {(x̄ᵢ, ȳᵢ)}ᵐ⁺ᵐ'ᵢ₌ₘ₊₁ từ X̄
18:    for i = {m + 1, ..., m + m'} do
19:     Tạo dữ liệu đối kháng x̄ᵢ^{adv} bằng Phương trình (3)
20:    end for
21:    tối ưu hóa fₜ trên {(x̄ᵢ, ȳᵢ)}ᵐ⁺ᵐ'ᵢ₌₁ bằng Phương trình (6)
22:   end for
23:  end for
24:  cập nhật X̃ᵗₒ theo lớp sử dụng lựa chọn đàn [4]
25: end for

đảm bảo không có sự chồng chéo giữa các giai đoạn khác nhau. Lưu ý rằng Cài đặt II chỉ dành cho CIFAR-100, nơi phương sai của số lớp đủ lớn để quan sát.

Chi tiết Triển khai. Tất cả các mô hình được triển khai với PyTorch và huấn luyện trên NVIDIA Tesla V100. Chúng tôi sử dụng ResNet18 [26] làm mô hình xương sống cho các thí nghiệm. Đối với huấn luyện đối kháng trên cả hai bộ dữ liệu, chúng tôi đặt độ lớn tối đa của nhiễu loạn là 8/255 và sử dụng Projected Gradient Descent (PGD) 7 bước để tạo các ví dụ đối kháng, trong đó kích thước bước là 2/255. Để đánh giá, chúng tôi không chỉ kiểm tra độ chính xác tiêu chuẩn trên các mẫu sạch mà còn độ chính xác bền vững với các cuộc tấn công đối kháng. Chúng tôi ký hiệu độ chính xác tiêu chuẩn là SA, độ chính xác bền vững dưới các cuộc tấn công PGD là RA(PGD), và độ chính xác bền vững dưới AutoAttack là RA(AA), tương ứng. Các tham số ε và α của các cuộc tấn công PGD để đánh giá được đặt giống như để huấn luyện.

Trong quá trình huấn luyện, thứ tự lớp cho các bộ dữ liệu được cố định để so sánh công bằng. Để dự trữ mẫu trong các giai đoạn trước, chúng tôi sử dụng chiến lược lựa chọn đàn trong [4] và đặt dung lượng bộ nhớ là 2000 mẫu cho cả CIFAR-10 và CIFAR-100. Dung lượng độc lập với số lớp và số exemplar cho mỗi lớp là 2000/#số lớp đã thấy.

--- TRANG 4 ---
1 2 3 4 5
Giai đoạn0.00.20.40.60.8RA(PGD)
iCaRL
RCL
BEMixup
TABA(a) CIFAR-10

1 2 3 4 5
Giai đoạn00.10.20.30.4RA(PGD)
iCaRL
RCL
BEMixup
TABA (b) CIFAR-100

Hình 2. Đánh giá độ bền trên tất cả các lớp đã thấy ở các giai đoạn khác nhau. BE là viết tắt của Boundary Exemplar để tiết kiệm không gian.

Đường cơ sở. Như chúng tôi đã nêu, độ bền chống tấn công đối kháng của học liên tục được nghiên cứu lần đầu tiên trong bài báo này và không có công trình trước đó về chủ đề này. Do đó, chúng tôi chọn iCaRL, phương pháp đại diện cho học liên tục làm đường cơ sở. Để có được độ bền chống tấn công đối kháng, chúng tôi áp dụng huấn luyện đối kháng trong học liên tục và xây dựng dựa trên iCaRL, được đặt tên là RCL, như một đường cơ sở khác. Ngoài ra, chúng tôi giới thiệu Boundary Exemplar để xác minh ảnh hưởng của dữ liệu biên giới đối với RCL và Mixup, có liên quan chặt chẽ đến TABA. Boundary Exemplar, Mixup và TABA là các phương pháp tăng cường để cải thiện RCL.

4.2. Kết quả Thí nghiệm
Đầu tiên, chúng tôi tiến hành thí nghiệm trên CIFAR-10 và CIFAR-100 trong Cài đặt I. Những thay đổi về độ bền theo các giai đoạn được trực quan hóa trong Hình 2 và kết quả thí nghiệm được tóm tắt trong Bảng 1. Chúng ta có thể quan sát thấy rằng các mô hình được huấn luyện bởi iCaRL không bền vững dưới tất cả các cuộc tấn công đối kháng, cho thấy gần như 0 độ chính xác bền vững chống lại cuộc tấn công PGD, và 0 độ chính xác bền vững chống lại AutoAttack. Với huấn luyện đối kháng, độ bền chống tấn công đối kháng cho các mô hình học liên tục được cải thiện đáng kể, mặc dù có sự giảm độ chính xác tiêu chuẩn. So với tất cả các phương pháp khác, TABA của chúng tôi rõ ràng cho thấy hiệu suất mạnh mẽ: Trên cả CIFAR-10 và CIFAR-100, TABA cho thấy độ bền tốt nhất hoặc tốt thứ hai dưới các cuộc tấn công PGD và AutoAttacks trong khi duy trì độ chính xác tiêu chuẩn. Mặc dù Mixup đạt được độ bền cao nhất dưới AutoAttack trên CIFAR10, nó mang lại sự giảm lớn 20% cho độ chính xác tiêu chuẩn.

Thứ hai, chúng tôi tiến hành thí nghiệm trong Cài đặt II trên CIFAR-100. Trong cài đặt này, số lớp cho mỗi giai đoạn được chọn ngẫu nhiên và tổng số lớp trong tất cả các giai đoạn được đảm bảo là 100. Chúng tôi chạy thí nghiệm 3 lần và số lớp cho các giai đoạn khác nhau dao động từ 5 đến 45. Kết quả trung bình được báo cáo trong Bảng 2 (phương sai gần bằng không và không được báo cáo ở đây). Chúng ta có thể thấy rằng TABA đạt được hiệu suất tổng thể tốt nhất. So với Mixup, TABA có RA(AA) tương đương và SA cao hơn nhiều. Sự giảm lớn của SA trong Mixup nên được tránh.

Bảng 1. Đánh giá độ bền trên CIFAR-10 và CIFAR-100 trong Cài đặt I. Kết quả tốt nhất (càng cao càng tốt) trong mỗi cột được in đậm.

SA RA(PGD) RA(AA)
CIFAR10
iCaRL 67.17% 1.00% 0.00%
RCL 60.36% 36.83% 16.71%
Boundary Exemplar 66.52% 36.91% 10.88%
Mixup 46.96% 33.11% 20.36%
TABA 65.97% 38.41% 19.74%

CIFAR100
iCaRL 58.31% 0.53% 0.00%
RCL 46.67% 16.67% 9.99%
Boundary Exemplar 38.08% 14.15% 6.51%
Mixup 46.58% 16.86% 10.03%
TABA 45.16% 18.71% 11.21%

Bảng 2. Đánh giá độ bền trên CIFAR-100 trong Cài đặt II. Kết quả tốt nhất (càng cao càng tốt) trong mỗi cột được in đậm.

Phương pháp SA RA(PGD) RA(AA)
iCaRL 49.68% 0.04% 0.01%
RCL 44.55% 17.49% 9.71%
Mixup 28.53% 16.08% 11.77%
TABA 42.79% 18.72% 11.43%

Bảng 3. Ảnh hưởng của ba thay đổi trong TABA.

Biên giới Nhận biết nhiệm vụ λ SA RA(PGD) RA(AA)
✗ ✗ ✗ 46.96% 33.11% 20.36%
✓ ✗ ✗ 54.84% 31.09% 15.45%
✓ ✓ ✗ 59.61% 32.18% 15.87%
✓ ✓ ✓ 65.97% 38.41% 19.74%

✓: với ✗: không có

4.3. Nghiên cứu Loại bỏ
Lấy cảm hứng từ Mixup, chúng tôi đề xuất TABA để giảm thiểu việc quên độ bền chống tấn công đối kháng trong học liên tục. So với Mixup, TABA khác biệt ở ba cách: dữ liệu biên giới, lựa chọn mẫu nhận biết nhiệm vụ và phạm vi của λ. Ở đây chúng tôi nghiên cứu ảnh hưởng của những thay đổi này và kết quả được tóm tắt trong Bảng 3. Chúng ta có thể thấy những cải thiện khi chúng tôi thực hiện các thay đổi tuần tự trên Mixup.

5. KẾT LUẬN
Trong bài báo này, chúng tôi nghiên cứu vấn đề học liên tục trong các cài đặt đối kháng. Được xác minh rằng các mô hình được huấn luyện theo cách học liên tục cũng dễ bị tổn thương trước các ví dụ đối kháng. Do đó, chúng tôi đề xuất RCL-TABA, bao gồm huấn luyện đối kháng và một phương pháp tăng cường dữ liệu mới TABA, để bảo mật học liên tục. Vì đây là bước đầu tiên để nghiên cứu giao điểm của huấn luyện đối kháng và học liên tục, chúng tôi hy vọng những phát hiện của chúng tôi cung cấp những hiểu biết hữu ích và thúc đẩy các nhà nghiên cứu khám phá sâu hơn.

--- TRANG 5 ---
6. TÀI LIỆU THAM KHẢO
[1] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, và Tinne Tuytelaars, "A continual learning survey: Defying forgetting in classification tasks," IEEE TPAMI, p. 1–1, 2021.

[2] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al., "Overcoming catastrophic forgetting in neural networks," PNAS, 2017.

[3] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, và Byoung-Tak Zhang, "Overcoming catastrophic forgetting by incremental moment matching," NIPS, vol. 30, 2017.

[4] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert, "icarl: Incremental classifier and representation learning," in CVPR, 2017, pp. 2001–2010.

[5] David Lopez-Paz và Marc'Aurelio Ranzato, "Gradient episodic memory for continual learning," NIPS, vol. 30, 2017.

[6] Cecilia S Lee và Aaron Y Lee, "Clinical applications of continual learning machine learning," The Lancet Digital Health, vol. 2, no. 6, pp. e279–e281, 2020.

[7] Pankaj Gupta, Yatin Chaudhary, Thomas Runkler, và Hinrich Schuetze, "Neural topic modeling with continual lifelong learning," in ICML, 2020.

[8] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, và Rob Fergus, "Intriguing properties of neural networks," arXiv preprint arXiv:1312.6199, 2013.

[9] Ian J Goodfellow, Jonathon Shlens, và Christian Szegedy, "Explaining and harnessing adversarial examples," arXiv preprint arXiv:1412.6572, 2014.

[10] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, và Adrian Vladu, "Towards Deep Learning Models Resistant to Adversarial Attacks," in ICLR. 2018, OpenReview.net.

[11] Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, và Qian Wang, "Recent Advances in Adversarial Training for Adversarial Robustness," in IJCAI-21, 2021.

[12] Fei Zhu, Zhen Cheng, Xu-yao Zhang, và Cheng-lin Liu, "Class-Incremental Learning via Dual Augmentation," in NeurIPS, 2021, vol. 34, pp. 14306–14318.

[13] Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, và Qi Zhu, "Federated class-incremental learning," in CVPR, 2022.

[14] Hikmat Khan, Pir Masoom Shah, Syed Farhan Alam Zaidi, et al., "Susceptibility of continual learning against adversarial attacks," arXiv preprint arXiv:2207.05225, 2022.

[15] Yunhui Guo, Mingrui Liu, Yandong Li, Liqiang Wang, Tianbao Yang, và Tajana Rosing, "Attacking lifelong learning models with gradient reversion," 2020.

[16] Hikmat Khan, Nidhal Carla Bouaynaya, và Ghulam Rasool, "Adversarially robust continual learning," in IJCNN, 2022, pp. 1–8.

[17] Ting-Chun Chou, Jhih-Yuan Huang, và Wei-Po Lee, "Continual learning with adversarial training to enhance robustness of image recognition models," in 2022 International Conference on Cyberworlds (CW), 2022, pp. 236–242.

[18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, và Adrian Vladu, "Towards deep learning models resistant to adversarial attacks," in ICLR, 2018.

[19] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, và Michael I. Jordan, "Theoretically principled trade-off between robustness and accuracy," in ICML, 2019.

[20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al., "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, vol. 2, no. 7, 2015.

[21] Zhizhong Li và Derek Hoiem, "Learning without forgetting," IEEE TPAMI, 2017.

[22] Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, và Mohan Kankanhalli, "Geometry-aware instance-reweighted adversarial training," arXiv preprint arXiv:2010.01736, 2020.

[23] Chen Chen, Jingfeng Zhang, Xilie Xu, Lingjuan Lyu, Chaochao Chen, Tianlei Hu, và Gang Chen, "Decision Boundary-aware Data Augmentation for Adversarial Training," IEEE IDSC, pp. 1–1, 2022.

[24] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, và David Lopez-Paz, "mixup: Beyond empirical risk minimization," in ICLR, 2018.

[25] Alex Krizhevsky, Geoffrey Hinton, et al., "Learning multiple layers of features from tiny images," 2009.

[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun, "Deep residual learning for image recognition," in CVPR, 2016, pp. 770–778.
