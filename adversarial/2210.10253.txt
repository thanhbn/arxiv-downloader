# 2210.10253.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/adversarial/2210.10253.pdf
# File size: 1649793 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
On the Adversarial Robustness of Mixture of Experts
Joan Puigcerver
Google ResearchRodolphe Jenatton
Google ResearchCarlos Riquelme
Google Research
Pranjal Awasthi
Google ResearchSrinadh Bhojanapalli
Google Research
Abstract
Adversarial robustness is a key desirable property of neural networks. It has been
empirically shown to be affected by their sizes, with larger networks being typically
more robust. Recently, Bubeck and Sellke [3]proved a lower bound on the Lipschitz
constant of functions that ﬁt the training data in terms of their number of param-
eters. This raises an interesting open question, do—and can—functions with more
parameters, but not necessarily more computational cost, have better robustness?
We study this question for sparse Mixture of Expert models (MoEs), that make it
possible to scale up the model size for a roughly constant computational cost. We
theoretically show that under certain conditions on the routing and the structure
of the data, MoEs can have signiﬁcantly smaller Lipschitz constants than their
dense counterparts. The robustness of MoEs can suffer when the highest weighted
experts for an input implement sufﬁciently different functions. We next empirically
evaluate the robustness of MoEs on ImageNet using adversarial attacks and show
they are indeed more robust than dense models with the same computational cost.
We make key observations showing the robustness of MoEs to the choice of experts,
highlighting the redundancy of experts in models trained in practice.
1 Introduction
Adversarial robustness refers to prediction robustness of a given machine learning model to adversarial,
but bounded, changes to the input. Neural networks trained with standard classiﬁcation objectives have
been shown to have poor adversarial robustness [ 25], a property attributed to their overparametrization.
Conversely, in practice, larger models, that have more parameters and higher computation cost, have
shown better robustness [20, 26, 1, 12].
In a recent work, Bubeck and Sellke [3]studied this phenomenon from a theoretical perspective,
by analyzing the relationship between model size and its Lipschitz constant - which measures the
sensitivity of a function to changes in the input. In particular the authors proved that any function
withPparameters, that memorizes Ninput data points in Ddimensions, has a Lipschitz constant
of at leastO(q
ND
P). This shows that, on a given dataset, larger models can have better robustness
(smaller Lipschitz constant). Note that as this is only a lower bound, it does not guarantee that larger
models will indeed have a smaller Lipschitz constant. Interestingly, the result is agnostic to other
properties of the function, such as its computation cost, or speciﬁc architecture.
Given the above result it is natural to wonder: can one increase the model size, without increasing
the computation cost, and enjoy better robustness? An increasingly popular class of such models are
Mixture of Experts (MoE) [ 23]. MoE models have multiple expert sub-models, and a routing function
that selects for each input a small subset of the experts and routes the input only to them. Neural
networks with MoE layers, such as Switch Transformer [ 10] in NLP, and V-MoE [ 22] in Computer
Vision, have been shown to achieve superior performance in comparison to their dense counterparts,
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2210.10253v1  [cs.LG]  19 Oct 2022

--- PAGE 2 ---
by allowing one to scale the model size without increasing the computation cost. In this paper we
study the following question: are MoE models more adversarially robust than dense models?
In general, sparse MoE models are not continuous, and hence not even smooth. Small changes to
the input can result in the router selecting a different expert for a given example. Unfortunately, in
some cases these experts may be very different, resulting in large changes to the output. Another
factor that affects the robustness of MoE models is the geometry of input data and its routing –how
the data is divided between experts. This decides what data an expert is trained on and hence their
robustness. Finally, it has been observed that –unless certain auxiliary losses are also applied to
encourage balancedness– the number of used experts tends to collapse to very few, and the remaining
ones are just ignored [ 22,10]. Given these stability issues, it is a priori notclear if MoEs, despite
having more parameters, are more robust than dense models.
To theoretically study the effect of both of these factors, (1) router stability and (2) data routing,
on robustness we consider the smooth MoE models, where each expert output is weighted by their
routing probability. In particular, MoE models where all experts are simultaneously applied to every
input. We analyze models with ﬁxed routing proving that MoEs with linear experts can achieve better
robustness if the data is well separated and routed accordingly. In the extreme case, MoEs with E
experts can have a smaller Lipschitz constant by a factor of 1=p
Ecompared to equivalent dense
models. We also characterize the effect of the difference between experts in terms of robustness,
showing that MoEs can have a high Lipschitz constant when two experts are very different for
inputs that weigh the experts similarly and highly. We show that both these factors, data routing and
difference between relevant experts, characterize the Lipschitz constant of MoE models.
We next evaluate robustness experimentally by using adversarial attacks [ 20]. We experimentally
show that on the ImageNet dataset [ 6], MoEs are more robust than dense models that have the
same computation cost. Interestingly, we observe that adversarial attacks result in signiﬁcant routing
changes of the inputs, but do not result in lower robustness than dense models. This suggests that
in practice, MoEs are robust to the choice of the experts to an extent. We next perform standard
adversarial training of both the dense and the MoE models and again observe that MoEs are more
robust against adversarial attacks.
The main contributions of the current work are as follows:
1.We propose a simple theoretical framework to understand the robustness of Mixture of
Experts (MoE) models. We provide general and non-trivial sufﬁcient conditions under which
MoEs are provably more robust than their dense counterparts for the linear experts setting.
2.We perform extensive experiments to demonstrate that in practice MoEs indeed enjoy
better robustness than dense models to norm bounded adversarial attacks. We also uncover
intriguing properties of adversarial attacks for MoE models and show that even for robust
MoE models, the attacks very often change the routing of data points, thereby pointing to a
high degree of redundancy in such models.
2 Preliminaries
2.1 Mixture of Experts
Mixture of Experts combine the outputs of (sub-)models, i.e., the experts , via a weighted sum of their
outputs [ 14,15,28,9].Sparse Mixture of Experts condition the weights of the sum in the inputs, and
activate only K(out ofE) experts, where Kis typically a very small integer compared to E(such
asK= 1 orK= 2) [23,10]. This form of conditional computation models allows one to easily
increase the number of parameters in the model (roughly) independently of its compute cost [ 21].
This approach has been recently applied to signiﬁcantly increase the model size and quality of models
used in Natural Language Processing [23, 18, 19, 10, 8] and Computer Vision applications [22, 27].
More concretely, we deﬁne the sparse and smooth (or dense) versions of mixtures of experts below.
While previous empirical works have sometimes selected more than one expert per input ( K > 1),
for simplicity we constrain ourselves to the case where only one expert is selected for each input
(K= 1). Recently, practical models also tend to this setup to match the FLOPs of dense models [ 10].
Sparse MoEs. In these models only one expert is selected for a given example xbased on its routing
probabilities pi(x). These models are not continuous functions of the input. A small change in the
input can lead to large changes in the output, due to changes in selected experts. A sparse MoE layer
2

--- PAGE 3 ---
is deﬁned as:
f(x) =EX
i=11fpi(x)pj(x);8j6=igfi(x): (1)
Herefi(x)are the individual expert functions. 1is the indicator function that takes the value of
1 if the condition is satisﬁed and 0 else. Note that to do tie breaking in the above deﬁnition, in
case multiple experts have the same maximum probability, we simply sample uniformly one of the
experts with the maximum probability. In practice, MoE models can have multiple sparse layers in
combination with dense layers. Further, random noise is sometimes added to the pi’s [23, 22].
We refer to MoEs with only 1 expert as dense models. In dense models, which are the standard in
neural networks literature, same function is applied to all input examples. In sparse MoEs introduced
above, different part of network / expert is applied to each input example depending on which expert
selected. This ﬂexibility allows us to train larger sparse MoEs, that have the same computational cost
as the dense models, as only a part of MoE is activated/selected for each input, with better accuracy
than dense models [10, 22].
Smooth MoEs. To theoretically analyze robustness, we consider a smooth mixture of expert models,
as they are continuous functions. In this case, the output of each expert fi, is weighted according to
its routing probabilities:
f(x) =EX
i=1pi(x)fi(x): (2)
The routing probabilities pi(x)are usually computed by a routing layer, e.g., pi(x) =(Sx)i, where
is the softmax and S2REDis a trainable variable. Such linear routing layers are the common
choice for MoE models in practice [10].
Though these models have recently been shown to achieve state of art performance for tasks in
NLP [ 10] and vision [ 22], there have not been many works analyzing their robustness. Recently,
Allingham et al. [2]empirically studied robustness of MoEs to natural perturbations in data (e.g.,
evaluation across different corrupted versions of CIFAR10 [ 17] and ImageNet [ 6]), and showed MoEs
are more robust for these natural perturbations than the corresponding dense models. To the best of
our knowledge ours is the ﬁrst work theoretically analyzing robustness of MoEs, and empirically
exploring it in the context of adversarial perturbations.
Load balancing loss To prevent MoEs from collapsing and always selecting the same expert for all
inputs, it is customary during training to add a load balancing loss , that encourages equal fraction of
inputs being routed to different experts. We deﬁne and present these losses in detail in appendix E.
2.2 Adversarial robustness
Adversarial robustness models the susceptibility of a function to adversarial perturbations to its
input [ 25,20]. More concretely, given a function f, loss function `, and an input x2RD, we can
write the adversarial loss incurred at xwith label yas follows.
max
z:kzk`(f(x+z);y): (3)
Hereis the attack radius per input. For the norm constraint ( kzk), popular choices in practice, are
`1and`2norms [ 20,4]. Adversarial accuracy is the accuracy of the model on data with perturbations
that satisfy the above equation, i.e., when the loss function `is the 0=1classiﬁcation loss.
Since optimizing equation 3 for neural networks is computationally hard, popular methods to ﬁnd
adversarial examples use local search approaches such as gradient ascent. The Fast Gradient Signed
Method (FGSM) is perhaps the simplest one, with only one gradient update step [ 11]. For`1bounded
perturbations the FGSM update is the following:
x+z=x+sgn(rx`(f(x);y)): (4)
sgn() is the sign function. The Projected Gradient Descent (PGD) method is a stronger attack that
does multiple ( ) gradient ascent steps to ﬁnd the perturbation [ 20]. The update rule for `1bounded
perturbations is as follows, starting with x0=x.
xt+1= x+C
xt+sgn(rx`(f(x);y))
;8t2[0; 1]: (5)
3

--- PAGE 4 ---
is chosen to be
. HereCis the constraint set fz:kzk1gandis the projection operator.
Existing works have established that neural networks are sensitive to adversarial attacks resulting
in a signiﬁcant drop in their accuracy [ 25]. Interestingly, increasing the size of neural networks,
thereby increasing their capacity, leads to an improvement in adversarial accuracy, showing that larger
neural networks are more robust [ 20,26,1,12]. In a recent work, Bubeck and Sellke [3]studied this
phenomenon theoretically. They proved a lower bound on the Lipschitz constant of any function that
ﬁts the training data that scales inversely1p
Pwith the number of function parameters P. However it
is not clear if functions that have more parameters, but not necessarily more computation cost (e.g.
MoEs), can achieve better robustness. In this paper we analyze adversarial robustness of MoEs, both
theoretically and experimentally.
Notation. We use small bold letters, e.g., x, to denote vectors and capital bold letters to denote
matrices, e.g., W. Scalars are denoted with plain letters. [N]denotes the integer set from 1toN.k:k
denotes the`2norm unless speciﬁed otherwise.
3 Robustness analysis
In this section, we present our main results regarding the robustness of the mixture of expert models.
We ﬁrst present a bound on the Lipschitz constant of MoEs for general expert functions fi. While
this is an upper bound for general functions it still highlights the two key components that affect the
robustness of MoE models.
3.1 Router stability
In this section, we compute the Lipschitz constant of smooth MoEs with learnable routing. Let,
f(x) =EX
i=1pi(x)fi(x)wherepi(x) =exp(hsi;xi)PE
j=1exp(hsj;xi)(6)
andfsigi2[E], with each siinRD, are learnable variables that decide the routing of an example to
different experts. We then prove the following upper bound on the Lipschitz constant of the MoE
models with learnable routing.
Lemma 1. Letffigi2[E]be smooth functions with Lipschitz constants fLfigi2[E]and letLfbe the
Lipschitz constant of f. Let s(x) =P
jpj(x)sj. Thenf(x)in equation 6satisﬁes,
krxf(x)kEX
i=1pi(x)Lfi+EX
i=1pi(x)fi(x)(si s(x));
and hence
Lfmax
i2[E]fLfig+ sup
xEX
i=1pi(x)fi(x)(si s(x)): (7)
The above Lemma bounds the Lipschitz constant of MoE models with two terms that depend on (1)
data routing, and (2) router stability. The ﬁrst term above is from the individual Lipschitz constant
of the experts. This depends on how the data is partitioned or routed to different experts, which
we will discuss in more detail in the next section. The second term arises from the router used to
compute probabilities for different experts. One may wonder if we can make this term arbitrarily
large by increasing the norm of x. This is not the case as increasing the norm of xusually results in
the collapse of the routing probabilities to a single expert. This makes pi(x)(si s(x))zero for all
expertsi2[E].
To study the above more concretely, we consider the case of E= 2experts. In this setting the second
term above reduces to kp1(x)f1(x)(s1 s(x)) + (1 p1(x))f2(x)(s2 s(x))k. Now, in the
extreme case, where p1(x)is either 0 or 1, the above term collapses to 0 as pi(x)fi(x)(si s(x))is
0 in that case. This is also expected as for examples that are routed to an expert with high probability,
the main dominating factor is the individual expert Lipschitz constant, as small perturbations do not
cause much change to the probabilities.
4

--- PAGE 5 ---
A more interesting setting is when p1(x)is1
2. In this setting, small perturbations to the input can
cause the model to drastically change the weight between the experts, and the above term reduces to
k1
4(f1(x) f2(x))(s1 s2)k. Hence MoE models can suffer from large Lipschitz constants if
the two experts are very different for points on the boundary, i.e., if jf1(x) f2(x)j0. Alternately
iff1(x)f2(x)for points on the boundary (i.e. p1(x)1
2) then this term is small.
We will later see in the experiments that MoEs trained in practice often have good router stability and
changes to routing do not result in much degradation of the ﬁnal accuracy.
3.2 Data routing
In this section we will study the effect of data routing on the Lipschitz constant of the individual
experts – the ﬁrst term in RHS of equation 7 (lemma 1). In particular we will try to address how
large can maxi2[E]fLfigbe in comparison to the Lipschitz constant of a dense model trained on
the same data. Towards this end, we will theoretically analyze the robustness of MoE models with a
pre-determined ﬁxed routing and linear experts, in the light of the structure of the data.
3.2.1 Setup
We consider the setting of ﬁxed routing, where we assume the routing of individual examples to
experts is pre-determined and ﬁxed. Moreover, we consider linear models as experts.
More speciﬁcally, we consider Ninput points of dimension Dstacked in the matrix X2RND,
together with the corresponding vector of targets y2RN. In the dense case (i.e., no experts), we
would like to ﬁnd a linear model w2RDthat minimizes
min
w2RDkXw yk2: (8)
The least-squares solution [ 13] for this problem that minimizes the MSE loss is w= (X>X)yX>y.
Hereydenotes the pseudo inverse. On the other hand, for mixture of experts, let the dataset be split
intoEsubsetsS1;;SE, with Xi;yidenoting the data in set Si. Now let the data from the set Si
be routed to the expert i. Below we write the objective for each expert:
min
wi2RDkXiwi yik2fori2[E]: (9)
Similarly, let w
i= (X>
iXi)yX>
iyibe the optimal solution for expert ithat minimizes the MSE.
3.2.2 Analysis
Before introducing our analysis relating the Lipschitz constant of the dense and MoE models, we ﬁrst
present two examples to better understand how routing affects Lipschitz constants. In particular we
consider a simple setting with E= 2experts. Let X>= [X>
1;X>
2], be the data routed to the two
experts. Recall that the Lipschitz constant of the function f(X) =Xw iskwk.
CaseX1?X2.In this setting we get
kwk=k(X>X)yX>yk=k(X>
1X1)yX>
1y1+ (X>
2X2)yX>
2y2k=kw
1+w
2k
max(kw
1k;kw
2k)
The second equality follows from the folowing steps - 1) XTX=XT
1X1+XT
2X2, 2)(XT
1X1+
XT
2X2)y= (XT
1X1)y+ (XT
2X2)y, and 3) (XT
1X1)yXT
2= 0;where steps 2 and 3 follow from
the orthogonality of X1andX2(see [ 16]). Hence, experts have smaller Lipschitz constant when
data routed to different experts lies in orthogonal subspaces.
CaseX1= X2andy1=y2.In this case, we see that:
kwk=k(X>X)yX>yk=k(X>
1X1+X>
2X2)yX>yk
=k(X>
1X1+X>
2X2)y(X>
1y1 X>
1y1)k= 0min(kw
1k;kw
2k):
Hence experts have worse Lipschitz constant when the data routed to different experts is aligned.
These two simple examples illustrate under which conditions experts have an advantage over a single
dense model, and when they do not.
5

--- PAGE 6 ---
We now present our main result. To capture this relation between the data geometry and the routing,
we introduce the following quantities. Let fUigi2[E]be the projection matrices onto orthogonal
subspaces in RD. One way to construct them is by ﬁrst taking the singular vectors of X>Xand
assigning them to the set iwith the largest projection. This guarantees that Uiis orthogonal to Uj
withj6=i, and they have greatest alignment with subspace spanned by Xi.
We ﬁrst deﬁne a quantity to capture how well Uicaptures the span of the data subset Xi.
Deﬁnition 1 (In-subspace distance: 1).910,kUiU>
i(X>X)y (X>
iXi)yk21;8i2[E].
Herek:k2for a matrix denotes the spectral norm. 1is small when,8i;Uicaptures Xiperfectly.
Next we deﬁne the projection distance between data from two different subsets.
Deﬁnition 2 (Cross-subspace distance: 2).920such that for any zin the span of X>
j, we have
k(X>
iXi)yzk2kzk;8i6=j.
Here2is small if the data in different subsets Xilies in orthogonal subspaces.
Theorem 1. Letwbe the minimizer of equation 8andfw
igi2[E]be the minimizers of equation 9.
Then,
kwk2EX
i=1(bkw
ik 1kX>yk 2X
j6=ikX>
jyjkc+)2;
wherebc+denotes the projection onto non-negative numbers.
The above result lower bounds the Lipschitz constant of the dense model kwkin terms of Lipschitz
constants of the experts kw
ikin the MoE model. We present the proof of this theorem in appendix A.
In the case where 1=2= 0, we obtain the following bound kwkqPE
i=1kw
ik2. Assuming
a balanced setting where all experts have the same norm parameters, we get the scaling kwk
O(p
E)maxi2[E]fkw
ikg. Hence in this setting, experts have a Lipschitz constant that is smaller
by a factor of1p
Ecompared to dense models. This happens when the data lies in Eorthogonal
subspaces and the data from each subspace is routed to the same expert. This shows that MoE models
can have signiﬁcantly smaller Lipschitz constant than their dense counterparts, while having the same
computation cost. For MoEs in practice, data routed to different experts does display some clustering
of the features (see Figure 7 in Riquelme et al. [22]).
As data in different partitions Xigets more aligned, 1and2increase, and reduce the gap between
the Lipschitz constant of the dense and the MoE models. This reduces the gap in the Lipschitz
constant of the dense model and the experts. In the extreme case, if all the datapoints are the same,
then1and2are large, eliminating this difference. In appendix D, we give a complementary result
relating the Lipschitz constants of the experts to that of the dense model when both 1and2can
possibly be large.
Connections to Bubeck and Sellke [3]. Bubeck and Sellke [3]proved a universal lower bound on
the Lipschitz constant of a function required to -memorizeNtraining samples in Ddimension.
Lf~
 
r
ND
P!
:
For the linear model considered above, this reduces to ~

p
N
as the number of parameters is
D. As MoEs have Etimes more parameters, they have a lower bound of ~

q
N
E
, i.e., the lower
bound on the Lipschitz constant of MoEs is smaller by a factor ofq
1
E. Our result shows that this
lower bound is in fact achievable, and hence tight, when the data lies in Eorthogonal subspaces and
data from each subspace is routed to the same expert.
6

--- PAGE 7 ---
4 Experiments
4.1 Setup
We compare the robustness of Vision Transformer (ViT) [ 7] and Vision MoE (V-MoE) [ 22] models
against adversarial attacks. In particular, we use the ViT-B/32 and V-MoE-B/32 models through
all the experiments. These models have the same backbone architecture but the latter replaces
one in every two feedforward layers with a sparse Mixture of Experts, selecting K= 2 out of
E= 32 feedforward experts that are applied on each token1. One could argue that the router in the
V-MoE model introduces an overhead that should be accounted for. Thus, we have trained a bigger
version of the dense ViT-B/32 (which we coin ViT-B++/32) that reaches roughly the same predictive
performance as the V-MoE model, but has higher cost. The cost of evaluating an image on the dense
ViT models is 8.9 and 17.9 GFLOPs and runtime cost, respectively; and 12.4 GFLOPs on the V-MoE
model. Appendix B contains additional experimental details.
We pre-train our models on the private dataset JFT-300M [ 24] for 7 epochs (517 859 steps with a batch
size of 4 096 images), using an image resolution of 224224pixels, and standard data augmentation
(inception crop and horizontal ﬂips). Since JFT-300M is a multi-label dataset, we minimize the
sigmoid cross-entropy loss. V-MoE also adds auxiliary losses to encourage a balanced load for all
experts; we used the same recipe as in [ 22]. In both cases we use Adam ( 1= 0:9;2= 0:999),
with a peak learning rate of 810 4, reached after a linear warm-up of 104steps and then linearly
decayed to a ﬁnal value of 10 5. Weight decay of 0.1 was used on all parameters. This is the same
pre-training protocol as that used in [7, 22].
After pre-training, the models are ﬁne-tuned on ImageNet [ 6], at a resolution of 384384pixels and
the same data augmentations as before, for a total of 104steps, using a batch size of 4 096 images.
SGD with Momentum ( = 0:9) is used for ﬁne-tuning, with a peak learning rate of 0:03, reached
after a linear warm-up of 500 steps, and followed with cosine decay to a ﬁnal value of 10 5. The
norm of the ﬂattened vector of gradients is clipped to a maximum value of 10. Since ImageNet
images have a single label, we minimize the softmax cross-entropy loss during ﬁne-tuning. The same
ﬁne-tuning protocol was adopted in [7, 22].
We evaluate the adversarial robustness of both the pre-trained and ﬁne-tuned models, by means of PGD
adversarial attacks [ 20]. We maximize the corresponding loss (sigmoid or softmax cross-entropy),
varying the`1norm constraint on the input image, for a total of = 40 steps.
4.2 Adversarial robustness of V-MoEs
During pre-training the ViT-B/32 model achieves a precision at 1 of 39.3%, and the V-MoE-B/32
achieves a precision-at-1 of 43.5% (conversely, the false discovery rate at 1 is 60.7% and 56.5%,
respectively). After ﬁne-tuning, the classiﬁcation error achieved by each model on ImageNet is 19.3%
and 17.8%, respectively.
Figure 1 shows in solid lines the false discovery rate (left) and the classiﬁcation error rate (right) as a
function of the `1constraint. Despite the fact that the V-MoE model contains a router that makes
discrete choices among the experts conditioned on the input, which could potentially lead to a severe
weakness against the adversarial attacker, we can observe that it follows the same trend as the base
dense ViT model. It is able to preserve a lower error over a wide range of `1values. A larger version
of the dense model matching the quality of the V-MoE has a much higher cost.
In addition, we also ﬁne-tuned the base ViT and V-MoE models on ImageNet using PGD adversarial
training. We use the same recipe as above but we perform a PGD attack of 10 steps on the input images,
with ﬁxed`1=8
255, before computing the gradients of the model parameters and updating them.
The classiﬁcation error on the original ImageNet dataset achieved by each model after adversarial
ﬁne-tuning is 51.7% and 49.8%, for both models respectively. Figure 1 (right) shows in dashed lines
the classiﬁcation error when these models are evaluated against an adversarial attacker using different
`1constraints. As the `1increases, the beneﬁt of adversarial ﬁne-tuning to preserve accuracy is
shown in both cases. Once again, both models report similar trends, and the V-MoE model shows
better robustness for a wide range of `1values.
1Token refers to an element in the sequence input, which is obtained by projecting input image patches [7]
7

--- PAGE 8 ---
103
102
101
0.70.80.91.0False discovery rateJFT-300M
103
102
101
0.40.60.81.0Classification error rateImageNet
Model
ViT (8.9 GFLOP)
ViT++ (17.9 GFLOP)
V-MoE (12.4 GFLOP)
Adversarial ft.
False
TrueFigure 1: False discovery rate on JFT-300M (left) and classiﬁcation error rate on ImageNet (right) as
a function of the `1used in the adversarial attacks. Dashed lines depict models ﬁne-tuned with PGD
adversarial training. Although the V-MoE model contains several sparse MoE layers making discrete
choices on their respective inputs, it shows lower error under adversarial attacks than the base ViT
model. A much bigger and slower ViT model is needed to roughly match the V-MoE.
4.3 Effect of the adversarial attacks on the selected experts
As described in section 3.1, in the region close to the decision boundary of the router, if two experts
have very different outputs, the Lipschitz constant of the MoE model could be much higher than that
of a similar dense model. If the index of the selected experts changes signiﬁcantly, but the model still
shows a reasonably high accuracy under adversarial attacks (compared to a dense model), this would
suggest that the outputs of the two selected sets of experts do not differ much.
Figure 2 shows the rate of changes in the router as a function of the `1used in the adversarial attack,
on the different layers of the V-MoE-B/32 model that have a MoE layer. For each token processed by
the model, we compute the intersection-over-union (IoU) of the selected set of experts before and
after the adversarial attack. We average the IoU across all processed tokens and deﬁne the rate of
routing changes as the complement of the average IoU.
In dashed lines we report the rate of changes of a V-MoE model ﬁne-tuned using PGD adversarial
training. Not only the model has a better accuracy against adversarial attacks when the `1increases,
as reported in ﬁgure 1, but the rate of routing changes is also generally lower across all layers.
103
102
101
0.00.20.40.6Rate of routing changesLayer 1Adv. fine-tuning False True
103
102
101
Layer 3
103
102
101
Layer 5
103
102
101
Layer 7
103
102
101
Layer 9
103
102
101
Layer 11
Figure 2: Rate of changes in the selection of experts as a function of the `1used in the adversarial
attacks on ImageNet against a V-MoE model. The dashed line depicts a V-MoE model ﬁne-tuned
with PGD adversarial training. A signiﬁcant fraction of experts change in each MoE layer . Given that
ﬁgure 1 shows the V-MoE model keeping its advantage over the dense ViT, we hypothesize that the
output of different experts for inputs close to the decision boundary of the router is similar.
Despite the fact that a signiﬁcant fraction of choices change on each layer as the`1increases,
ﬁgure 1 shows that the V-MoE model still keeps its advantage against adversarial attacks over the
ViT model. This suggests that in the regions close to a decision boundary of the router, the two
corresponding experts have a similar output, hence preventing the full V-MoE model from being
less robust in practice. Conversely, the fact that the V-MoE model has a better base quality than the
ViT counterpart, suggests that the experts are not equivalent for regions far away from the decision
boundary, otherwise it would be reduce to a dense model.
8

--- PAGE 9 ---
4.4 Attacking the router’s auxiliary losses
Sparse MoE models usually employ auxiliary losses to balance the load among all experts. In
particular, in the implementation of V-MoEs, if the load of the experts is highly unbalanced, the
experts receiving signiﬁcantly more tokens than the average could ignore all tokens that exceed the
expert’s capacity, potentially leading to a signiﬁcantly worse performance. The question is whether an
adversarial attacker can exploit this property in practice. Figure 3 (left) shows the false discovery rate
on JFT-300M, for a V-MoE model when the router’s auxiliary losses are maximized in the adversarial
attack, together with the corresponding cross-entropy loss. We use the same weight for the auxiliary
losses as the one used to train the models. The ﬁgure shows that attacking the auxiliary loss does not
offer any signiﬁcant advantage for the attacker. The results are analogous in terms of the classiﬁcation
error on ImageNet (not shown here in interest of space).
103
102
101
0.70.80.91.0False discovery rateJFT-300M
103
102
101
0.70.80.91.0False discovery rateJFT-300M
103
102
101
0.40.60.81.0Classification error rateImageNet
Model
ViT
V-MoE (E=2)
V-MoE (E=4)
V-MoE (E=8)
V-MoE (E=16)
V-MoE (E=32)
Attack auxiliary
False
True 103
 5104
0.650.700.75
103
 5104
0.30.40.5
Figure 3: (Left) False discovery rate on JFT-300M for a ViT-B/32 and a V-MoE-B/32 models (with
32 experts). Dashed lines represent the results when the attacker targets both the cross-entropy and
the auxiliary losses used by the V-MoE routers to balance the load among experts in each layer. The
auxiliary losses of V-MoE do not present a disadvantage against an adversarial attacker. (Center and
Right) False discovery rate on JFT-300M and classiﬁcation error rate on ImageNet for a ViT-B/32
and several V-MoE-B/32 models (with increasing number of total experts). On JFT-300M the quality
of the model against adversarial attacks improves as more experts are used. After ﬁne-tuning on
ImageNet, the number of experts becomes increasingly redundant.
4.5 Increasing the model size by increasing the total number of experts
In section 1 we asked if, given that the lower bound on the Lipschitz constant given by Bubeck and
Sellke [3]is agnostic to the computational cost of the function, could we make a model more robust
by increasing its model size without increasing the total cost? Here we measure how much increasing
the total number of experts in a V-MoE model helps against adversarial attacks. Notice that increasing
the total number of experts Edoes not make the V-MoE model more expensive. Figure 3 (center and
right) shows the false discovery rate on JFT-300M and the classiﬁcation error on ImageNet for an
increasing number of experts. All V-MoE models select K= 2experts for each token.
On the one hand, increasing the total number of experts improves the robustness on JFT-300M up to
E= 16 experts. The curves for E= 16 andE= 32 are highly overlapping, thus any difference is
most likely due to noise in the training and ﬁne-tuning process. On the other hand, when the V-MoEs
are ﬁne-tuned on ImageNet, all models with more than two experts achieve roughly the same accuracy
under adversarial attacks.
This shows that, although the results presented in section 3 showing better robustness of MoEs require
some assumptions, the conclusions hold to some extent in real scenarios. Increasing the number of
parameters by growing the number of experts is an effective way of improving the model’s robustness.
4.6 Robustness against AutoPGD attacks
We conducted additional experiments using a more sophisticated adversarial attack, AutoPGD [ 5],
which selects the step size to use in each update of the attack. We also increased the number of
steps performed in the attack, using = 100 steps. Figure 4a shows the results achieved by two
dense (ViT-B/32 and ViT-B++/32) models and a sparse model (V-MoE-B/32). Although AutoPGD is
slightly more effective as an adversarial attack against all methods (for example, with `1= 10 3the
false discovery rate on JFT-300M of ViT-B/32 is 0.744 using PGD and 0.757 using AutoPGD), the
trend of all models is identical to that represented in ﬁgure 1.
9

--- PAGE 10 ---
103
102
101
0.650.700.750.800.850.900.951.00False discovery rateJFT-300M
ViT (8.9 GFLOP)
ViT++ (17.9 GFLOP)
V-MoE (12.4 GFLOP)(a) False discovery rate on JFT-300M (left) and clas-
siﬁcation error rate on ImageNet (right) as a function
of the `1, using AutoPGD and yielding the same con-
clusions as the simpler PGD method.
103
102
101
0.000.250.500.75
103
102
101
103
102
101
0.000.250.500.75
103
102
101
Rate of routing changes (Layers 1, 5, 7, 11)(b) Rate of changes in the selection of experts as a
function of the `1used in the adversarial attacks on
ImageNet against a V-MoE model.
Figure 4b shows the rate of routing changes in the different MoE layers in the V-MoE-B/32 model.
This ﬁgure is analogous to ﬁgure 2 which shows the results for the standard PGD attack. Compared
to it, AutoPGD is able to change a higher fraction of the selected experts. For example, the maximum
fraction of changes in Layer 1 using PGD was near 0.2, while AutoPGD increases it up to around 0.5.
AutoPGD offers the same conclusion as the PGD attacks: models using sparse MoE layers offer
better robustness against adversarial attacks (per GFLOP) than dense models, despite the fact that the
router itself can be quite sensible to these attacks.
5 Conclusion
In this work we analyzed the adversarial robustness of MoEs showing their advantage over dense
models, with more experts leading to better robustness, both theoretically and empirically. We showed
how the properties of the data and its routing plays and important role in learning robust MoEs.
While there is some evidence that routing learned by MoEs in practice display some clustering of
the features in some layers of the model (see Figure 7 in Riquelme et al. [22]), it is currently not
explicitly encouraged during training. Hence developing smarter routing strategies that take data
geometry into account can be an interesting direction of future work. Currently our analysis is limited
to linear models, extending this to general models and deriving the dependency of optimal routing on
them is another promising research direction.
We have also shown that, for inputs that weigh two experts similarly, if the two expert values are
very different, then the MoEs can suffer from higher Lipschitz constant. However, for models trained
in practice, we saw their predictions to be relatively stable, despite signiﬁcant changes in choice of
experts, highlighting potential redundancy of learned experts. However too much redundancy, with
all experts learning similar functions is a waste of capacity and can affect model performance. Hence
it is an interesting research problem to balance robustness and accuracy of MoEs by controlling the
redundancy of experts.
Acknowledgments and Disclosure of Funding
All authors would like to thank Sven Gowal for sharing their AutoPGD attack codebase. We also
thank Neil Houlsby, Basil Mustafa, and André Susano Pinto for providing insightful feedback while
working on this project.
10

--- PAGE 11 ---
References
[1]J.-B. Alayrac, J. Uesato, P.-S. Huang, A. Fawzi, R. Stanforth, and P. Kohli. Are labels required for
improving adversarial robustness? Advances in Neural Information Processing Systems , 32, 2019.
[2]J. U. Allingham, F. Wenzel, Z. E. Mariet, B. Mustafa, J. Puigcerver, N. Houlsby, G. Jerfel, V . Fortuin,
B. Lakshminarayanan, J. Snoek, D. Tran, C. R. Ruiz, and R. Jenatton. Sparse moes meet efﬁcient ensembles.
CoRR , abs/2110.03360, 2021. URL https://arxiv.org/abs/2110.03360 .
[3]S. Bubeck and M. Sellke. A universal law of robustness via isoperimetry. Advances in Neural Information
Processing Systems , 34, 2021.
[4]N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, A. Madry, and
A. Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705 , 2019.
[5]F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-
free attacks. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on
Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 2206–2216. PMLR,
13–18 Jul 2020. URL https://proceedings.mlr.press/v119/croce20b.html .
[6]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 248–255. Ieee,
2009.
[7]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-
derer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929 , 2020.
[8]N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun, Y . Zhou, A. W. Yu, O. Firat, et al.
Glam: Efﬁcient scaling of language models with mixture-of-experts. In International Conference on
Machine Learning , pages 5547–5569. PMLR, 2022.
[9]D. Eigen, M. Ranzato, and I. Sutskever. Learning factored representations in a deep mixture of experts.
arXiv preprint arXiv:1312.4314 , 2013.
[10] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with
simple and efﬁcient sparsity. Journal of Machine Learning Research , 23(120):1–39, 2022. URL http:
//jmlr.org/papers/v23/21-0998.html .
[11] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Interna-
tional Conference on Learning Representations , 2015.
[12] S. Gowal, C. Qin, J. Uesato, T. Mann, and P. Kohli. Uncovering the limits of adversarial training against
norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593 , 2020.
[13] T. Hastie, R. Tibshirani, J. H. Friedman, and J. H. Friedman. The elements of statistical learning: data
mining, inference, and prediction , volume 2. Springer, 2009.
[14] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural
computation , 3(1):79–87, 1991.
[15] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural Computation ,
6(2):181–214, 1994. doi: 10.1162/neco.1994.6.2.181.
[16] P. Kovanic. On the pseudoinverse of a sum of symmetric matrices with applications to estimation.
Kybernetika , 15(5):341–348, 1979.
[17] A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of
Toronto, 2009.
[18] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen. GShard: Scal-
ing giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668 ,
2020.
[19] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer. Base layers: Simplifying training of
large, sparse models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference
on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 6265–6274.
PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/lewis21a.html .
[20] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learning Representations , 2018.
[21] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean.
Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 , 2021.
[22] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and
N. Houlsby. Scaling Vision with Sparse Mixture of Experts. In M. Ranzato, A. Beygelzimer, Y . Dauphin,
P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , volume 34,
pages 8583–8595. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/
2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf .
11

--- PAGE 12 ---
[23] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural
networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017.
[24] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep
learning era. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) , Oct 2017.
[25] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. arXiv preprint arXiv:1312.6199 , 2013.
[26] C. Xie and A. Yuille. Intriguing properties of adversarial training at scale. In International Conference on
Learning Representations , 2020.
[27] F. Xue, Z. Shi, F. Wei, Y . Lou, Y . Liu, and Y . You. Go wider instead of deeper. arXiv preprint
arXiv:2107.11817 , 2021.
[28] S. E. Yuksel, J. N. Wilson, and P. D. Gader. Twenty years of mixture of experts. IEEE Transactions on
Neural Networks and Learning Systems , 23(8):1177–1193, 2012. doi: 10.1109/TNNLS.2012.2200299.
Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] Please see sections 1 and 5.
(c)Did you discuss any potential negative societal impacts of your work? [N/A] Our work
analyzes adversarial robustness of popular models.
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a)Did you state the full set of assumptions of all theoretical results? [Yes] Please see
sections sections 3.1 and 3.2.
(b)Did you include complete proofs of all theoretical results? [Yes] Please see appendix A.
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [No] The pre-training
data is proprietary and cannot be publicly shared. Appendix B includes a description of
the software stack used and a link to the software that we used to run all experiments.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] Please see section 4.
(c)Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [No] We notice fairly stable adversarial accuracy metrics across
different settings.
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] The hardware speciﬁcations and
the total compute used in the experiments are detailed in appendix B.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a)If your work uses existing assets, did you cite the creators? [Yes] Please see section 4.
(b)Did you mention the license of the assets? [No] Part of the data that we use is propri-
etary. The public data that we use (ImageNet) is publicly available after registration on
https://image-net.org , and broadly used by the Machine Learning and Computer
Vision communities.
(c)Did you include any new assets either in the supplemental material or as a URL? [No]
Part of code we use is proprietary.
(d)Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e)Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...
12

--- PAGE 13 ---
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
13

--- PAGE 14 ---
A Proofs
Proof of lemma 1. We have by chain rule
rxf(x) =kX
i=1pi(x)rfi(x) +kX
i=1rpi(x)fi(x):
Hence,
krxf(x)kkX
i=1pi(x)Lfi+kkX
i=1rpi(x)fi(x)k;
where
rpi(x) =rexp(hsi;xi)P
jexp(hsj;xi)
=P
jexp(hsj;xi)exp(hsi;xi)si exp(hsi;xi)P
jexp(hsj;xi)sjP
jexp(hsj;xi)P
jexp(hsj;xi)
=exp(hsi;xi)siP
jexp(hsj;xi) piP
jexp(hsj;xi)sjP
jexp(hsj;xi)
=pisi piX
jpjsj:
Lets(x) =P
jpj(x)sj. Substituting this in the above equation gives us
krxf(x)kkX
i=1pi(x)Lfi+kkX
i=1pi(x)fi(x)(si s(x))k
max
ifLfig+ max
xkEX
i=1pi(x)fi(x)(si s(x))k:
The last inequality follows from Holder’s inequality andPk
i=1pi(x) = 1 .
Proof of theorem 1. Since Uiare orthogonal to each other, we can decompose kwkas follows.
kwk2=k(X>X)yX>yk2=k(X
iUiU>
i)(X>X)yX>yk2=X
ikUiU>
i(X>X)yX>yk2:
(10)
From deﬁnition 1 we have, for any unit norm vector z,
1kUiU>
i(X>X)y (X>
iXi)yk2
kUiU>
i(X>X)yz (X>
iXi)yzk
kUiU>
i(X>X)yzk k (X>
iXi)yzk
The last step follows from triangle inequality. This implies
k(X>
iXi)yzkkUiU>
i(X>X)yzk+1:
Hence, by setting z=X>y=kX>yk, we see that
kUiU>
i(X>X)yX>ykk (X>
iXi)yX>yk 1kX>yk:
14

--- PAGE 15 ---
Recall that X>y=PE
j=1X>
jyj. Then using triangle inequality, and the above inequality, we get,
kw
ik=k(X>
iXi)yX>
iyik
k(X>
iXi)yX>yk+X
j6=ik(X>
iXi)yX>
jyjk
kUiU>
i(X>X)yX>yk+1kX>yk+X
j6=ik(X>
iXi)yX>
jyjk
kUiU>
i(X>X)yX>yk+1kX>yk+2X
j6=ikX>
jyjk:
The last step follows from deﬁnition 2. Thus,
kUiU>
i(X>X)yX>ykkw
ik 1kX>yk 2X
j6=ikX>
jyjk:
Substituting this in equation 10 gives us the result.
kwk2X
i(bkw
ik 1kX>yk 2X
j6=ikX>
jyjkc+)2:
B Experiments details
B.1 Architectures
Table 1 contains the details of the architectures used in section 4. All V-MoE models have replaced
one in every two feedforward layers in the original ViT-B/32 architecture, with a MoE of feedforward
layers, and select only K= 2experts out of the Eavailable experts. All models were pre-trained on
JFT-300M, at a resolution of 224224pixels, for a total number of 7 epochs, using the same batch
size and optimizer settings as used by Dosovitskiy et al. [7]and Riquelme et al. [22] (see details
section 4). We used the B/32 variants of ViT and V-MoE since these are the “base” conﬁgurations
suggested in the respective papers.
In order to match the quality of a dense ViT and a sparse V-MoE model, we also trained a bigger
version of ViT-B/32, which we refer to as ViT-B++/32. The values for the number of layers, number
of attention heads, embedding dimension and the hidden dimension of the FFN, are simply an
interpolation between the ViT-B/32 and the ViT-L/32 corresponding values.
We implemented all models, training and evaluation code using JAX2and FLAX3. Although we
cannot release the models used in the experiments, since they are pre-trained on proprietary data, the
code used in all of them is available at http://github.com/google-research/vmoe .
B.2 Compute resources
We use TPUv3 to train and evaluate our models. In particular, we used 32 TPUv3 cores for pre-
training and ﬁne-tuning the models. In most of the evaluation experiments against adversarial attacks
we used 8 TPUv3 cores, since the compute needed to perform the evaluation against adversarial attack
is much lower. The total training cost (in terms of total training runtime and FLOPs) of ViT-B/32 and
V-MoE-B/32 can be found in [22].
ViT-B/32 is pre-trained for a total of 27.6 TPUv3-core-days, performing 56.1 ExaFLOPs. V-MoE-
B/32 (with 32 experts) is trained for a total of 54.9 TPUv3-core-days and performs a total of 76.1
ExaFLOPs. Finally, the larger dense ViT-B++/32 is trained for a total of 78.3 TPUv3-core-days and
performs 113.4 ExaFLOPs.
The GFLOPs are computed automatically by JAX. For instance, to compute the GFLOPs used for
each individual image during the evaluation of the models, the code snippet in listing 1 is used.
2https://github.com/google/jax
3https://github.com/google/flax
15

--- PAGE 16 ---
Table 1: Architecture details of all the models used in the experiments. We specify here the number
of layers in the Transformer encoder, the number of heads used in the multi-head attention, the
embedding dimension (i.e. token size), the hidden size in the feedforward (FFN) layers and experts,
the selected and total number of experts (when applicable), the active/total number of parameters,
and the cost of evaluation per image (at a resolution of 224224pixels). Precision-at-1 on the
pre-training dataset (JFT-300M) and classiﬁcation error rate on ImageNet (ILSVRC2012) are also
reported.
Name ViT-B/32 ViT-B++/32 V-MoE-B/32
E=2 E=4 E=8 E=16 E=32
Layers 12 18 12
Heads 12 14 12
Embedding dim. 768 896 768
FFN dim. 3072 3584 3072
Selected/Total experts — — 2/2 2/4 2/8 2/16 2/32
Active parameters (M) 102.1 193.6 130.5 130.5 130.5 130.5 130.6
Total parameters (M) 102.1 193.6 130.5 187.1 300.5 527.2 980.6
Eval. GFLOPs/image 8.9 17.9 12.1 12.2 12.2 12.3 12.4
JFT-300M P@1 (%) 39.3 42.9 40.5 41.4 42.7 43.6 43.5
ILSVRC2012 error (%) 19.3 17.2 18.9 18.7 18.0 17.8 17.8
Listing 1: Code snippet used to compute the evaluation GFLOPs per image.
1client = jax.lib . xla_bridge . get_backend ()
2# eval_step_fn is the function called to evaluate one batch .
3m = jax. xla_computation ( eval_step_fn ) (...) . as_hlo_module ()
4analysis = jax.lib. xla_client . _xla . hlo_module_cost_analysis (client , m)
5eval_gflops_per_image = analysis [’flops ’] / 10**9 / batch_size
16

--- PAGE 17 ---
C Adversarial examples
Figure 5: ImageNet adversarial examples generated when attacking a V-MoE-B/32, for different
values ofLinf. Although the differences are hard to spot (better seen in a monitor, zooming in the
images), they are sufﬁcient to change the prediction of the model.
D A complementary result to theorem 1
We now present our complementary result. In essence, in a regression setting, our result tries to
capture how different the sub-problems tackled by the experts should be in order to get improved
robustness compared with a dense approach.
17

--- PAGE 18 ---
In what follows, we use the shortcuts Z=X>X2RDDandZi=X>
iXi2RDDfori2[E].
Also, we recall that each Xiis inRNiDwithPE
i=1Ni=N.
Theorem 2. Letwbe the minimizer of equation 8andfw
igi2[E]be the minimizers of equation 9.
For alli2[E], let us assume that there exist i2RDandri2RNisuch that we can write
yi=Xii+ri
where the vectors ri’s are residual terms that can for instance account for non-linear effects in Xi
and/or some stochastic noise. Further assume the fZigi2[E]to be invertible and deﬁne
i=Z 1
iX>
iriand=Z 1X>rwithr= [r1;:::;rE]2RN:
Let us denote the (normalized) difference between any two vectors (i;j)asij2RD, that is
for anyi6=j;ij=Z 1Zj(i j);
and consider the cumulative difference with respect to the vector ias
for anyi2[E];i=X
j6=iij2RD:
Consider2(0;1]. If the structures of the underlying sub-problems on f(Xi;yi)gi2[E]differ
sufﬁciently from each other in the following sense
for alli2[E];kikki+k+1pki+ik;
then it holds that
max
i2[E]kw
ik2kwk2:
In other words, the experts have smaller Lipschitz constants than the dense model by a factor . As a
corollary, if we can control the residual terms ri’s such that
kkmin
i2[E]kikand for alli2[E];kikkik;
then the same conclusion is implied by the simpler sufﬁcient condition
min
i2[E]
kik 4=pkik	
0:
Proof of theorem 2. As a ﬁrst comment, since we assume that all the Zi’s are invertible, then so is
Z=PE
i=1Zi. We readily have for i2[E]
w
i=Z 1
iX>
iyi=i+Z 1
iX>
iri=i+i:
Moreover, for any i2[E], we have
w=Z 1X>y
=Z 1EX
j=1X>
jyj
=Z 1EX
j=1(Zjj+X>
jrj)
=8
<
:Z 1EX
j=1Zj(i Z 1
jZij)9
=
;+
=Z 18
<
:EX
j=1Zj9
=
;i EX
j=1ij+
=i i+
=w
i i+ i:
18

--- PAGE 19 ---
To summarize, for any i2[E], it therefore holds
kw w
ik=k i ik:
Let us consider 2(0;1]. We can observe that
kwk2 kw
ik2=kw w
ik2 (1 +)kw
ik2+ 2(w)>w
i:
We develop each term individually:
kw w
ik2=k ik2+kik2 2>
i( i) (A)
with
 (1 +)kw
ik2= (1 +)ki+ik2(B)
and using w=i i+=i+i i+ i, we get
2(w)>w
i= 2ki+ik2 2>
i(i+i) + 2(i+i)>( i): (C)
Gathering (A)+(B)+(C), we obtain the expression
kwk2 kw
ik2=kw w
ik2 (1 +)kw
ik2+ 2(w)>w
i
=kik2 2>
i(i+) +ki+k2 ki+ik2=(i):
We want to understand the conditions on ito guarantee that
kwk2 kw
ik2=(i)0:
To ﬁnd a sufﬁcient condition, we lower bound the quadratic form (i)as follows
For any i2RD; (i)kik2 2kikki+k+ki+k2 ki+ik2
=(kik root+)(kik root )
with
root+=ki+k+1pki+ikand root =ki+k 1pki+ik:
The lower bound is a standard polynomial of degree 2 and it is non-negative beyond its positive root
kikki+k+1pki+ik:
We apply the exact same rationale for all i2[E]and take the minimum of the conditions to make
them hold simultaneously.
Since2(0;1]and1=p1, if we further assume that the residual terms are sufﬁciently small
•kkmini2[E]kik
• for alli2[E],kikkik
then we get the simpler sufﬁcient condition after using the triangle inequality
kik4pkikki+k+1pki+ik:
E Auxiliary losses for MoEs
In this section we present the Auxiliary losses used for training MoEs and attack objectives used for
our experiments in section 4. Auxiliary losses are used to ensure the data is routed in a balanced way
across different experts. In particular we use the losses from Riquelme et al. [22] which are deﬁned
below.
19

--- PAGE 20 ---
Recall that the router weights for a given token xand a given expert j, with routing parameters
fsjgE
j=1, were deﬁned in equation 6 as:
pj(x) =exp(hsj;xi)PE
j0=1exp(
s0
j;x
)
In practice, we use a noisy version of this router:
pj(x) =exp(hsj;xi+j)PE
j0=1exp(
s0
j;x
+j0)
withjN(= 0;=1
E). The quantityhsj;xiis the routing logit for a given token and expert j.
Importance Loss. The importance of expert jfor a batch of tokens Xis simply deﬁned as the sum
of the router weights assigned to that expert over the tokens in the batch:
Impj(X) =X
ipj(xi) (11)
where X= [x1;:::;xn]>andpj(xi)is the routing weight assigned to the j-th expert for the i-th
token.
We use the squared coefﬁcient of variation of the importance distribution over experts, Imp(X) :=
fImpj(X)gE
j=1:
LImp(X) =std(Imp(X))
mean (Imp(X))2
(12)
Load Loss. For a given token, the load loss is based on the probability that the noisy logit of expert
jis among the top- klogits when a new Gaussian sample is drawn. Let’s deﬁne the k-th maximum
noisy logit for a given token xas:
k(x) =k–maxjhsj;xi+j (13)
Then, the probability that the noisy logits of expert jare above the threshold if the noise is resampled,
is given by the expression:
j(x) =P(hsj;xi+k(x)) (14)
Analogous to the importance, the load of expert jis deﬁned as the sum over the tokens in a given
batch:
Loadj(X) =X
ij(xi) (15)
And the load loss corresponds to the squared coefﬁcient of variation of the load distribution, with
Load(X) :=fLoadj(X)gE
j=1:
LLoad(X) =std(Load(X))
mean (Load(X))2
(16)
Final Loss. The ﬁnal loss used for training the models includes the classiﬁcation loss (i.e. cross-
entropy) and both auxiliary losses with weights 0.005:
L=Lclassiﬁcation + 0:005LImp+ 0:005LLoad (17)
F Experiment Metrics
Precision at 1 and False discovery rate In multi-label datasets (such as JFT), the traditional
accuracy metric used for single-label classiﬁcation is ill-deﬁned. Usually, Precision at k,Recall atk,
and other more general metrics are used in this setting. In particular, we use the Precision at 1 .
LetY= [y1;:::;yN]>;yi2RCbe the logits output by the model for a set of Nexamples, with
true labels ^Y= [^y1;:::; ^yN]>;^yi2f0;1gC.Precision at 1 is deﬁned as:
P@1 =1
NNX
i=1[^yi;arg maxcyi;c= 1] (18)
20

--- PAGE 21 ---
where arg maxcyi;cis essentially the index of the class with the highest logit, for a given example i,
andis the Kronecker delta function. That is, the non-zero elements of the sum are those examples
for which the highest logit corresponds to one of the true labels of such example. The false discovery
rate is deﬁned as 1 P@1.
Observe than when the examples have a single label, precision at 1 is equivalent to the accuracy, and
the false discovery rate is equivalent to the error rate.
Rate of routing changes Given two sets AandB, the intersection over union (IoU) is deﬁned as:
IoU=card(A\B)
card(A[B)(19)
If the two sets are equal, IoU is 1. If the two sets do not share any element, IoU is0.
We use this metric in section 4.3 to compare how much the set of selected experts change when
adversarial attacks are peformed on V-MoE models. Thus, the rate of routing changes is deﬁned as
1 IoU.
21
