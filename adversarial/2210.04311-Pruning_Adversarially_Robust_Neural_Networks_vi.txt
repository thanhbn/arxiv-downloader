# Tỉa Neural Networks Chống Đối Nghịch Mạnh Mẽ mà không cần Ví dụ Đối Nghịch

Tong Jian1,y, Zifeng Wang1,y, Yanzhi Wang2, Jennifer Dy1, Stratis Ioannidis1
Khoa Kỹ thuật Điện và Máy tính
Đại học Northeastern
1fjian, zifengwang, jdy, ioannidis g@ece.neu.edu
2yanz.wang@northeastern.edu

Tóm tắt - Tỉa đối nghịch nén các mô hình trong khi vẫn bảo toàn tính mạnh mẽ. Các phương pháp hiện tại yêu cầu truy cập vào các ví dụ đối nghịch trong quá trình tỉa. Điều này làm cản trở đáng kể hiệu quả huấn luyện. Hơn nữa, khi các cuộc tấn công đối nghịch và phương pháp huấn luyện mới phát triển với tốc độ nhanh, các phương pháp tỉa đối nghịch cần được điều chỉnh tương ứng để theo kịp. Trong công trình này, chúng tôi đề xuất một khung làm việc mới để tỉa một mạng neural mạnh mẽ đã được huấn luyện trước trong khi duy trì tính mạnh mẽ đối nghịch, mà không cần tạo thêm các ví dụ đối nghịch. Chúng tôi tận dụng việc tự chưng cất và tỉa đồng thời để bảo toàn kiến thức trong mô hình gốc cũng như điều chỉnh mô hình đã tỉa thông qua Hilbert-Schmidt Information Bottleneck. Chúng tôi đánh giá toàn diện khung làm việc được đề xuất và cho thấy hiệu suất vượt trội về cả tính mạnh mẽ đối nghịch và hiệu quả khi tỉa các kiến trúc được huấn luyện trên các bộ dữ liệu MNIST, CIFAR-10, và CIFAR-100 chống lại năm cuộc tấn công tiên tiến nhất. Mã nguồn có sẵn tại https://github.com/neu-spiral/PwoA/.

Từ khóa chỉ mục - Tính Mạnh Mẽ Đối Nghịch, Tỉa Đối Nghịch, Tự chưng cất, HSIC Bottleneck

I. GIỚI THIỆU

Tính dễ tổn thương của mạng neural sâu (DNN) trước các cuộc tấn công đối nghịch đã trở thành chủ đề nghiên cứu rộng rãi gần đây [1]–[5]. Các cuộc tấn công như vậy được tạo ra có chủ ý để đánh lừa DNN đưa ra dự đoán sai, ví dụ, bằng cách thêm các nhiễu loạn tinh vi nhưng không thể nhận thấy bằng mắt thường vào các ví dụ tự nhiên gốc [6]. Tính mạnh mẽ đối nghịch, tức khả năng của mô hình đã huấn luyện duy trì sức mạnh dự đoán bất chấp các cuộc tấn công như vậy, là một tính chất quan trọng cho nhiều ứng dụng quan trọng về an toàn [7]–[9]. Cách phổ biến và hiệu quả nhất để đạt được tính mạnh mẽ đối nghịch là thông qua huấn luyện đối nghịch [10]–[12], tức huấn luyện mô hình trên các ví dụ được tạo ra đối nghịch. Huấn luyện đối nghịch đã cho thấy hiệu suất mạnh mẽ đáng tin cậy chống lại các kỹ thuật tấn công cải tiến như projected gradient descent (PGD) [3], cuộc tấn công Carlini & Wagner (CW) [4] và AutoAttack (AA) [5]. Tuy nhiên, huấn luyện đối nghịch tốn kém về mặt tính toán [3], [13], thường dài hơn 3–30× [14] so với huấn luyện tự nhiên, chính xác là do chi phí bổ sung của việc tạo ra các ví dụ đối nghịch.

Như Madry et al. [3] đã lưu ý, việc đạt được tính mạnh mẽ đối nghịch yêu cầu một kiến trúc rộng hơn và lớn hơn đáng kể so với độ chính xác tự nhiên. Dung lượng mạng lớn được yêu cầu bởi huấn luyện đối nghịch có thể hạn chế việc triển khai trên phần cứng hạn chế tài nguyên hoặc các ứng dụng thời gian thực. Tỉa trọng số là một kỹ thuật nén nổi bật để giảm kích thước mô hình mà không có suy giảm độ chính xác đáng chú ý [15]–[21]. Trong khi các nhà nghiên cứu đã khám phá rộng rãi việc tỉa trọng số, chỉ một số công trình gần đây đã nghiên cứu nó kết hợp với tính mạnh mẽ đối nghịch. Ye et al. [22], Gui et al. [23], và Sehwag et al. [24] áp dụng các kỹ thuật phòng thủ chủ động với tỉa trong nghiên cứu của họ. Tuy nhiên, các công trình này yêu cầu truy cập vào các ví dụ đối nghịch trong quá trình tỉa. Tỉa bản thân là một quá trình tốn công, vì các kỹ thuật tỉa hiệu quả đồng thời tinh chỉnh một mạng hiện có, đã được huấn luyện trước; việc kết hợp các ví dụ đối nghịch vào quá trình này làm cản trở đáng kể hiệu quả huấn luyện. Hơn nữa, các kỹ thuật tỉa đối nghịch được thiết kế cho các phương pháp huấn luyện đối nghịch cụ thể cần được liên tục sửa đổi khi các phương pháp mới phát triển nhanh chóng.

Trong bài báo này, chúng tôi nghiên cứu cách lấy một DNN dày đặc, mạnh mẽ chống đối nghịch, đã được huấn luyện trên các ví dụ đối nghịch, và tỉa nó mà không có bất kỳ huấn luyện đối nghịch bổ sung nào. Như một ví dụ động lực được minh họa trong Hình 1(a), một DNN được công bố công khai bởi các nhà nghiên cứu hoặc một công ty, được huấn luyện đối nghịch với chi phí tính toán lớn, có thể được tỉa tiếp theo bởi các nhà nghiên cứu khác để có thể thực thi được trên thiết bị hạn chế tài nguyên, như FPGA. Sử dụng phương pháp của chúng tôi, việc sau có thể được thực hiện hiệu quả, mà không cần truy cập vào các tài nguyên tính toán được yêu cầu cho tỉa đối nghịch.

Việc hạn chế tỉa chỉ truy cập các ví dụ tự nhiên đặt ra một thách thức đáng kể. Như được thể hiện trong Hình 1(b), việc tỉa naïve một mô hình mà không có các ví dụ đối nghịch có thể là thảm khốc, xóa sạch tất cả tính mạnh mẽ chống AutoAttack. Ngược lại, PwoA của chúng tôi đặc biệt mạnh mẽ dưới một phạm vi rộng các tỷ lệ tỉa.

Nhìn chung, chúng tôi đóng góp như sau:

1) Chúng tôi đề xuất PwoA, một khung làm việc từ đầu đến cuối để tỉa một mô hình mạnh mẽ chống đối nghịch đã được huấn luyện trước mà không tạo ra các ví dụ đối nghịch, bằng (a) bảo toàn tính mạnh mẽ từ mô hình gốc thông qua tự chưng cất [25]–[27] và (b) tăng cường tính mạnh mẽ từ các ví dụ tự nhiên thông qua Hilbert-Schmidt independence criterion (HSIC) như một bộ điều chỉnh [28], [29].

2) Công trình của chúng tôi là công trình đầu tiên nghiên cứu cách một mô hình đã được huấn luyện trước đối nghịch có thể được tỉa hiệu quả mà không cần truy cập vào các ví dụ đối nghịch. Đây là một thách thức quan trọng, mới: trước nghiên cứu của chúng tôi, không rõ liệu điều này có thể thực hiện được hay không. Cách tiếp cận của chúng tôi là tổng quát, và không được thiết kế riêng cũng không bị hạn chế đối với các mô hình mạnh mẽ đã huấn luyện trước cụ thể, kiến trúc, hoặc phương pháp huấn luyện đối nghịch.

3) Chúng tôi đánh giá toàn diện PwoA trên các mô hình mạnh mẽ chống đối nghịch đã huấn luyện trước được công bố công khai bởi các nhà nghiên cứu khác. Cụ thể, chúng tôi tỉa năm mô hình có sẵn công khai đã được huấn luyện trước với các phương pháp đối nghịch tiên tiến nhất (SOTA) trên các bộ dữ liệu MNIST, CIFAR-10, và CIFAR-100. So với các phương pháp tỉa đối nghịch SOTA, PwoA có thể tỉa một phần lớn trọng số trong khi đạt được tính mạnh mẽ đối nghịch tương đương—hoặc tốt hơn—với tốc độ huấn luyện nhanh hơn 4×–7×.

Phần còn lại của bài báo này được cấu trúc như sau. Chúng tôi xem xét các công trình liên quan trong Phần II. Trong Phần III, chúng tôi thảo luận về tính mạnh mẽ đối nghịch tiêu chuẩn, chưng cất kiến thức, và HSIC. Trong Phần V, chúng tôi trình bày phương pháp của chúng tôi. Phần VI bao gồm các thí nghiệm của chúng tôi; chúng tôi kết luận trong Phần VII.

II. CÔNG TRÌNH LIÊN QUAN

Tính Mạnh Mẽ Đối Nghịch. Các phương pháp tấn công đối nghịch phổ biến bao gồm projected gradient descent (PGD) [3], fast gradient sign method (FGSM) [2], tấn công CW [4], và AutoAttack (AA) [5]; xem thêm [30] để có bài đánh giá toàn diện. Các mô hình mạnh mẽ chống đối nghịch thường được thu được thông qua huấn luyện đối nghịch [31], bằng cách bổ sung tập huấn luyện với các ví dụ đối nghịch, được tạo ra bởi các cuộc tấn công đối nghịch đã nêu trên. Madry et al. [3] tạo ra các ví dụ đối nghịch thông qua PGD. TRADES [11] và MART [12] mở rộng huấn luyện đối nghịch bằng cách kết hợp các thuật ngữ phạt bổ sung. LBGAT [32] hướng dẫn huấn luyện đối nghịch bằng ranh giới phân loại tự nhiên để cải thiện tính mạnh mẽ. Tuy nhiên, việc tạo ra các ví dụ đối nghịch tốn kém về mặt tính toán và thời gian.

Một số công trình gần đây quan sát thấy rằng các phạt information-bottleneck tăng cường tính mạnh mẽ. Fischer [33] xem xét conditional entropy bottleneck (CEB), trong khi Alemi et al. [34] đề xuất variational information bottleneck (VIB); cả hai đều dẫn đến các tính chất mạnh mẽ được cải thiện. Ma et al. [28] và Wang et al. [29] sử dụng một phạt dựa trên Hilbert Schmidt Independence Criterion (HSIC), được gọi là HSIC bottleneck như một bộ điều chỉnh (HBaR). Wang et al. cho thấy rằng HBaR tăng cường tính mạnh mẽ đối nghịch ngay cả khi không tạo ra các ví dụ đối nghịch [29]. Vì lý do này, chúng tôi kết hợp HBaR vào khung tỉa mạnh mẽ thống nhất của chúng tôi như một phương tiện khai thác tính mạnh mẽ đối nghịch chỉ từ các ví dụ tự nhiên trong quá trình tỉa, mà không cần huấn luyện đối nghịch thêm. Chúng tôi là những người đầu tiên nghiên cứu HBaR trong bối cảnh tỉa; nghiên cứu ablation của chúng tôi (Phần VI-B) chỉ ra rằng HBaR thực sự góp phần tăng cường tính mạnh mẽ trong môi trường của chúng tôi.

Tỉa Đối Nghịch. Tỉa trọng số là một trong những kỹ thuật nén nổi bật để giảm kích thước mô hình với suy giảm độ chính xác có thể chấp nhận được. Trong khi được khám phá rộng rãi cho mục đích hiệu quả và nén [15]–[20], chỉ một số công trình gần đây nghiên cứu tỉa trong bối cảnh tính mạnh mẽ đối nghịch. Một số công trình [35], [36] thảo luận lý thuyết về mối quan hệ giữa tính mạnh mẽ đối nghịch và tỉa, nhưng không cung cấp bất kỳ kỹ thuật phòng thủ chủ động nào. Ye et al. [22] và Gui et al. [23] đề xuất AdvPrune để kết hợp khung tỉa alternating direction method of multipliers (ADMM) với huấn luyện đối nghịch. Lee et al. [37] đề xuất APD để sử dụng chưng cất kiến thức cho tỉa đối nghịch được tối ưu hóa bằng phương pháp proximal gradient. Sehwag et al. [24] đề xuất HYDRA, sử dụng mục tiêu huấn luyện mạnh mẽ để học một mask thưa thớt. Tuy nhiên, tất cả các phương pháp này đều dựa vào huấn luyện đối nghịch. HYDRA còn yêu cầu huấn luyện thêm các mask thưa thớt, làm cản trở hiệu quả huấn luyện. Ngược lại, chúng tôi chưng cất từ một mô hình mạnh mẽ chống đối nghịch đã huấn luyện trước trong khi tỉa mà không tạo ra các ví dụ đối nghịch. Mô hình nén của chúng tôi có thể bảo toàn tính mạnh mẽ đối nghịch cao với tốc độ huấn luyện tăng đáng kể so với các phương pháp này, như chúng tôi báo cáo trong Phần VI-C.

III. KIẾN THỨC NỀN TẢNG

Chúng tôi sử dụng ký hiệu tiêu chuẩn sau đây trong suốt bài báo. Trong bài toán phân loại k-phân loại tiêu chuẩn, chúng tôi được cho một bộ dữ liệu D=f(xi;yi)gni=1, trong đó xi2RdX;yi2f0;1gk là các mẫu i.i.d. được lấy từ phân bố kết hợp PXY. Cho một mạng neural L-lớp h:RdX!Rk được tham số hóa bởi các trọng số :=flgLl=12Rdl, trong đó l là trọng số tương ứng với lớp thứ l, với l= 1;:::;L , chúng tôi định nghĩa mục tiêu học tập tiêu chuẩn như sau:

L() =EXY[`(h(X);Y)]≈1/n∑ni=1`(h(xi);yi); (1)

trong đó `:Rk×Rk!R là một hàm mất mát, ví dụ, cross-entropy.

A. Tính Mạnh Mẽ Đối Nghịch

Chúng ta gọi một mạng là mạnh mẽ chống đối nghịch nếu nó duy trì độ chính xác dự đoán cao chống lại kẻ thù bị ràng buộc mà làm nhiễu loạn các mẫu đầu vào. Chính thức, trước khi gửi một mẫu đầu vào x2RdX, kẻ thù có thể làm nhiễu loạn x bằng một δ tùy ý ∈Br, trong đó Br⊆RdX là quả cầu `∞ có bán kính r, tức là,

Br=B(0;r) =fδ2RdX:kδk∞≤rg: (2)

Tính mạnh mẽ đối nghịch [3] của một mô hình h được đo bằng mất mát kỳ vọng đạt được bởi các ví dụ đối nghịch như vậy, tức là,

~L() =EXY[maxδ2Br`(h(X+δ);Y)]≈1/n∑ni=1maxδ2Br`(h(xi+δ);yi):(3)

Một mạng neural mạnh mẽ chống đối nghịch h có thể được thu được thông qua huấn luyện đối nghịch, tức là, bằng cách tối thiểu hóa mất mát tính mạnh mẽ đối nghịch trong (3) một cách thực nghiệm trên tập huấn luyện D. Trong thực tế, điều này tương đương với stochastic gradient descent (SGD) trên các ví dụ đối nghịch xi+δ (xem, ví dụ, [3]). Trong mỗi epoch, δ được tạo ra trên cơ sở từng mẫu thông qua tối ưu hóa nội bộ trên Br, ví dụ, thông qua projected gradient descent (PGD).

Tỉa đối nghịch bảo toàn tính mạnh mẽ trong khi tỉa. Các cách tiếp cận hiện tại kết hợp huấn luyện đối nghịch vào mục tiêu tỉa của họ. Cụ thể, AdvPrune [22] trực tiếp tối thiểu hóa mất mát đối nghịch ~L() bị ràng buộc bởi các yêu cầu thưa thớt. HYDRA [24] cũng sử dụng ~L() để cùng học một mask thưa thớt cùng với l. Cả hai đều được kết hợp với và được thiết kế riêng cho các phương pháp huấn luyện đối nghịch cụ thể, và yêu cầu thời gian huấn luyện đáng kể. Điều này thúc đẩy chúng tôi đề xuất khung PwoA của chúng tôi, được mô tả trong Phần V.

B. Chưng Cất Kiến Thức

Trong chưng cất kiến thức [25], [38], một mô hình học sinh học để bắt chước đầu ra của một giáo viên. Xem xét một mô hình giáo viên T được huấn luyện tốt, và một mô hình học sinh h mà chúng ta muốn huấn luyện để khớp với đầu ra của giáo viên. Cho σ:Rk![0;1]k là hàm softmax, tức là, σ(z)j=ezj/∑j'ezj', j= 1;:::;k . Cho

σT(x) =σ(T(x)/τ) và σh(x) =σ(h(x)/τ) (4)

là các đầu ra softmax của hai mô hình được cân bằng bởi tham số nhiệt độ τ >0[25]. Sau đó, phạt chưng cất kiến thức được sử dụng để huấn luyện là:

LKD()=(1-α)L()+α·τ2EX[KL(σh(X);σT(X))];(5)

trong đó L là mất mát phân loại của mạng học sinh có nhiệt độ σh và KL là divergence Kullback–Leibler (KL). Trực quan, mất mát chưng cất kiến thức LKD coi đầu ra của giáo viên như nhãn mềm để huấn luyện học sinh, sao cho học sinh thể hiện một số tính chất vốn có của giáo viên, như tính mạnh mẽ đối nghịch.

C. Hilbert-Schmidt Independence Criterion

Hilbert-Schmidt Independence Criterion (HSIC) là một thước đo phụ thuộc thống kê được giới thiệu bởi Gretton et al. [39]. HSIC là chuẩn Hilbert-Schmidt của toán tử đồng biến thiên chéo giữa các phân bố trong Reproducing Kernel Hilbert Space (RKHS). Tương tự như Mutual Information (MI), HSIC nắm bắt các phụ thuộc phi tuyến giữa các biến ngẫu nhiên. HSIC được định nghĩa là:

HSIC(X;Y ) =EXYX'Y'[kX(X;X')kY'(Y;Y')]+EXX'[kX(X;X')]EYY'[kY(Y;Y')]-2EXY[EX'[kX(X;X')]EY'[kY(Y;Y')]];(6)

trong đó X' và Y' là các bản sao độc lập của X và Y tương ứng, và kX và kY là các hàm kernel. Trong thực tế, chúng ta thường xấp xỉ HSIC một cách thực nghiệm. Cho n mẫu i.i.d. f(xi;yi)gni=1 được lấy từ PXY, chúng ta ước tính HSIC thông qua:

\HSIC(X;Y ) = (n-1)-2tr (KXHKYH); (7)

trong đó KX và KY là các ma trận kernel với các phần tử KXij=kX(xi;xj) và KYij=kY(yi;yj), tương ứng, và H=I-1/n·11T là ma trận căn giữa.

IV. CÔNG THỨC BÀI TOÁN

Cho một mô hình mạnh mẽ chống đối nghịch h, chúng ta muốn tỉa hiệu quả các trọng số không quan trọng từ mô hình đã huấn luyện trước này trong khi bảo toàn tính mạnh mẽ đối nghịch của mô hình đã tỉa cuối cùng. Chúng ta tối thiểu hóa hàm mất mát chịu các ràng buộc xác định yêu cầu thưa thớt. Cụ thể hơn, bài toán tỉa trọng số có thể được công thức hóa như:

Tối thiểu hóa: L();
chịu ràng buộc θl2Sl; l= 1;…;L;(8)

trong đó L() là hàm mất mát tối ưu hóa cả độ chính xác và tính mạnh mẽ, và Sl⊆Rdl là tập ràng buộc thưa thớt trọng số được áp dụng cho lớp l, được định nghĩa là

Sl=fθl|kθlk0≤sl}; (9)

trong đó kθlk0 là kích thước của support của θl (tức là, số lượng phần tử khác không), và sl2N là một hằng số được xác định như tham số độ thưa thớt.

V. PHƯƠNG PHÁP LUẬN

Bây giờ chúng tôi mô tả PwoA, khung làm việc thống nhất của chúng tôi để tỉa một mạng mạnh mẽ mà không cần huấn luyện đối nghịch bổ sung.

A. Tỉa Bảo Toàn Tính Mạnh Mẽ

Cho một mô hình mạnh mẽ đã được huấn luyện trước đối nghịch, chúng ta nhằm bảo toàn tính mạnh mẽ của nó trong khi làm thưa nó thông qua tỉa trọng số. Cụ thể, chúng ta tận dụng các nhãn mềm được tạo ra bởi mô hình mạnh mẽ và trực tiếp kết hợp chúng vào mục tiêu tỉa của chúng ta chỉ với quyền truy cập vào các ví dụ tự nhiên. Chính thức, chúng ta ký hiệu mô hình đã được huấn luyện trước tốt bằng T và đối tác thưa thớt của nó bằng h. Mục tiêu tối ưu hóa được định nghĩa như sau:

Tối thiểu: LD() =τ2EX[KL(σh(X);σT(X))];
chịu ràng buộc θl2Sl; l= 1;…;L;(10)

trong đó τ là siêu tham số nhiệt độ. Trực quan, mục tiêu dựa trên chưng cất của chúng ta buộc mô hình thưa h phải bắt chước nhãn mềm được tạo ra bởi mô hình T đã được huấn luyện trước ban đầu, trong khi ràng buộc thực thi rằng các trọng số đã học chịu thưa thớt mong muốn. Bằng cách này, chúng ta bảo toàn tính mạnh mẽ đối nghịch thông qua việc chưng cất kiến thức từ các nhãn mềm một cách hiệu quả, mà không cần tái tạo các ví dụ đối nghịch. Khác với mất mát chưng cất ban đầu trong (5), chúng ta loại bỏ mất mát phân loại nơi các nhãn được sử dụng, vì chúng ta quan sát thấy rằng nó không góp phần vào tính mạnh mẽ đối nghịch (xem Bảng V trong Phần VI-B). Việc giải quyết bài toán tối ưu hóa (10) không đơn giản; chúng tôi mô tả cách xử lý bản chất tổ hợp của các ràng buộc thưa thớt trong Phần V-C.

B. Tăng Cường Tính Mạnh Mẽ từ Các Ví Dụ Tự Nhiên

Ngoài việc bảo toàn tính mạnh mẽ đối nghịch từ mô hình đã huấn luyện trước, chúng ta có thể tăng cường thêm tính mạnh mẽ trực tiếp từ các ví dụ tự nhiên. Lấy cảm hứng từ công trình gần đây sử dụng các phạt information-bottleneck, [28], [29], [33], [34], chúng tôi kết hợp HSIC như một Bộ Điều Chỉnh (HBaR) vào khung tỉa mạnh mẽ của chúng tôi. Theo hiểu biết tốt nhất của chúng tôi, HBaR chỉ được chứng minh hiệu quả dưới các tình huống học đối nghịch thông thường; chúng tôi là những người đầu tiên mở rộng nó vào bối cảnh tỉa trọng số. Chính thức, chúng tôi ký hiệu bằng Zl2RdZl, l2f1;:::;Lg đầu ra của lớp thứ l của h dưới đầu vào X (tức là, biểu diễn tiềm ẩn thứ l). Phạt học HBaR [28], [29] được định nghĩa như sau:

LH() =βx∑Ll=1HSIC(X;Zl)-βy∑Ll=1HSIC(Y;Zl);(11)

trong đó βx;βy2R+ là các siêu tham số cân bằng.

Trực quan, vì HSIC đo phụ thuộc giữa hai biến ngẫu nhiên, việc tối thiểu hóa HSIC(X;Zl) tương ứng với việc loại bỏ thông tin dư thừa hoặc nhiễu từ X. Do đó, thuật ngữ này cũng tự nhiên giảm ảnh hưởng của cuộc tấn công đối nghịch, tức là nhiễu loạn được thêm vào dữ liệu đầu vào. Trong khi đó, việc tối đa hóa HSIC(Y;Zl) khuyến khích việc thiếu nhạy cảm này với đầu vào xảy ra trong khi vẫn giữ lại bản chất phân biệt của bộ phân loại, được nắm bắt bởi sự phụ thuộc vào thông tin hữu ích w.r.t. nhãn đầu ra Y. Sự đánh đổi nội tại này tương tự như cái gọi là information-bottleneck [40], [41]. Wang et al. [29] quan sát sự đánh đổi này giữa các phạt trong quá trình huấn luyện; chúng tôi cũng quan sát nó trong quá trình tỉa (xem Phụ lục C).

PwoA kết hợp HBaR với tự chưng cất trong quá trình tỉa trọng số. Chúng tôi chính thức hóa PwoA để giải quyết bài toán sau:

Tối thiểu hóa: LPwoA() =LD() +LH();
chịu ràng buộc θl2Sl; l= 1;…;L:(12)

C. Giải Quyết PwoA thông qua ADMM

Bài toán (12) có các ràng buộc tổ hợp do tính thưa thớt. Do đó, nó không thể được giải quyết bằng cách sử dụng stochastic gradient descent như trong huấn luyện CNN tiêu chuẩn. Để xử lý điều này, chúng tôi tuân theo chiến lược tỉa dựa trên ADMM bởi Zhang et al. [18] và Ren et al. [19]. Chúng tôi mô tả chi tiết thủ tục hoàn chỉnh trong Phụ lục A. Nói ngắn gọn, ADMM là một thuật toán primal-dual được thiết kế cho các bài toán tối ưu hóa có ràng buộc với các mục tiêu tách rời (ví dụ, bài toán (12)). Thông qua việc định nghĩa một Lagrangian mở rộng, thuật toán luân phiên giữa hai bước primal có thể được giải quyết hiệu quả và riêng biệt. Bài toán con đầu tiên tối ưu hóa mục tiêu LPwoA được mở rộng với một phạt proximal; đây là một tối ưu hóa không ràng buộc được giải quyết bằng SGD cổ điển. Bài toán con thứ hai được giải quyết bằng cách thực hiện các phép chiếu Euclidean PSl(·) lên các tập ràng buộc Sl; mặc dù những cái sau không lồi, những phép chiếu này có thể được tính toán trong thời gian đa thức. Khung PwoA tổng thể được tóm tắt trong Thuật toán 1.

VI. THÍ NGHIỆM

A. Thiết Lập Thí Nghiệm

Chúng tôi tiến hành các thí nghiệm trên ba bộ dữ liệu chuẩn, MNIST, CIFAR-10, và CIFAR-100. Để thiết lập các mô hình mạnh mẽ chống đối nghịch đã huấn luyện trước để tỉa, chúng tôi xem xét năm mô hình được huấn luyện đối nghịch được cung cấp bởi công trình tiên tiến nhất mã nguồn mở, bao gồm Wang et al. [29], Zhang et al. [11], và Cui et al. [32], được tóm tắt trong Bảng I.

Để hiểu tác động của từng thành phần của PwoA đối với tính mạnh mẽ, chúng tôi kiểm tra các kết hợp của các mục tiêu học không đối nghịch sau đây để tỉa: LCE, LH, và LD. Tất cả các mục tiêu này được tối ưu hóa dựa trên các ví dụ tự nhiên. Chúng tôi cũng so sánh PwoA với ba phương pháp tỉa đối nghịch: APD [37], AdvPrune [22] và HYDRA [24].

Siêu Tham Số. Chúng tôi tỉa các mô hình đã huấn luyện trước bằng SGD với tốc độ học ban đầu 0.01, momentum 0.9 và weight decay 10^-4. Chúng tôi đặt kích thước batch là 128 cho tất cả các phương pháp. Đối với PwoA của chúng tôi, chúng tôi đặt số epoch tỉa và tinh chỉnh lần lượt là 50 và 100. Đối với các phương pháp SOTA AdvPrune và HYDRA, chúng tôi sử dụng mã được cung cấp bởi các tác giả cùng với các siêu tham số tối ưu mà họ đề xuất. Cụ thể, đối với AdvPrune, chúng tôi đặt epoch tỉa và tinh chỉnh lần lượt là 50 và 100; đối với HYDRA, chúng tôi đặt chúng lần lượt là 20 và 100, và sử dụng TRADES như mất mát huấn luyện đối nghịch. Chúng tôi báo cáo tất cả các tham số điều chỉnh khác trong Phụ lục B.

Tỷ Lệ Tỉa Mạng. Nhớ lại từ Phần IV rằng các tập ràng buộc thưa thớt fSlgLl=1 được định nghĩa bởi Eq. (9) với các tham số thưa thớt sl2N xác định các phần tử khác không mỗi lớp. Chúng tôi ký hiệu tỷ lệ tỉa như tỷ lệ kích thước chưa tỉa so với kích thước đã tỉa; tức là, đối với nl số lượng tham số trong lớp l, tỷ lệ tỉa tại lớp l có thể được tính như ρl=nl/sl. Chúng tôi đặt sl sao cho chúng ta có tỷ lệ tỉa giống hệt nhau mỗi lớp, dẫn đến tỷ lệ tỉa đồng nhất trên toàn mạng.

Các Thước Đo Hiệu Suất và Cuộc Tấn Công. Đối với tất cả các phương pháp, chúng tôi đánh giá mô hình đã tỉa cuối cùng thông qua các thước đo sau. Đầu tiên chúng tôi đo (a) Độ chính xác tự nhiên (tức là, độ chính xác kiểm tra trên các ví dụ tự nhiên). Sau đó chúng tôi đo tính mạnh mẽ đối nghịch thông qua độ chính xác kiểm tra dưới (b) FGSM, cuộc tấn công fast gradient sign [2], (c) PGDm, cuộc tấn công PGD với m bước được sử dụng cho tối ưu hóa PGD nội bộ [3], (d) CW (CW-loss trong khung PGD) cuộc tấn công [4], và (e) AA, AutoAttack [5], là mạnh nhất trong số tất cả bốn cuộc tấn công. Cả năm thước đo đều được báo cáo theo phần trăm (%) độ chính xác. Theo tài liệu học đối nghịch trước đây, chúng tôi đặt kích thước bước là 0.01 và r= 0.3 cho MNIST, và kích thước bước là 2/255 và r= 8/255 cho CIFAR-10 và CIFAR-100, tối ưu hóa trên các quả cầu chuẩn `∞ trong tất cả các trường hợp. Tất cả các cuộc tấn công xảy ra trong giai đoạn kiểm tra và có quyền truy cập đầy đủ vào các tham số mô hình. Vì luôn có sự đánh đổi giữa độ chính xác tự nhiên và tính mạnh mẽ đối nghịch, chúng tôi báo cáo mô hình tốt nhất khi nó đạt được mất mát trung bình thấp nhất giữa hai, như được đề xuất bởi Ye et al. [22] và Zhang et al. [11]. Chúng tôi đo và báo cáo tổng thời gian huấn luyện trên Tesla V100 GPU với bộ nhớ 32 GB và 5120 cores.

B. Hiểu Biết Toàn Diện về PwoA

Nghiên Cứu Ablation và Tính Mạnh Mẽ PwoA. Đầu tiên chúng tôi kiểm tra sự tương tác giữa các thuật ngữ PwoA trong mục tiêu trong Eq. (12) và cho thấy cách những thuật ngữ này bảo toàn và thậm chí cải thiện tính mạnh mẽ trong khi tỉa. Chúng tôi nghiên cứu nhiều kết hợp của LCE, LH, và LD trong Bảng II-IV. Chúng tôi báo cáo độ chính xác kiểm tra tự nhiên và tính mạnh mẽ đối nghịch dưới các cuộc tấn công khác nhau của mô hình đã tỉa dưới 3 tỷ lệ tỉa (4×, 8×, và 16×) trên MNIST, CIFAR-10, và CIFAR-100. Đối với mỗi kết quả được báo cáo, chúng tôi khám phá các siêu tham số τ, βx, và βy như được mô tả trong Phụ lục B và báo cáo ở đây các giá trị hoạt động tốt nhất.

Nhìn chung, Bảng II-IV cho thấy rằng phương pháp PwoA của chúng tôi (cụ thể là, LD+LH) tỉa một phần lớn trọng số trong khi đạt được tính mạnh mẽ đối nghịch tốt nhất cho cả ba bộ dữ liệu. Ngược lại, một mô hình được tỉa bởi LCE một mình (tức là, không có nỗ lực nào để duy trì tính mạnh mẽ) thất bại thảm khốc dưới các cuộc tấn công đối nghịch trên tất cả các bộ dữ liệu. Lý do là khi bộ dữ liệu phức tạp hơn và/hoặc tỷ lệ tỉa cao, LCE bị buộc phải duy trì độ chính xác tự nhiên trong quá trình tỉa, khiến nó lệch khỏi tính mạnh mẽ đối nghịch của mô hình đã huấn luyện trước. Ngược lại, tự chưng cất đồng thời (LD) và tỉa là bắt buộc để bảo toàn tính mạnh mẽ đáng kể mà không tạo ra các ví dụ đối nghịch trong quá trình tỉa. Chúng tôi quan sát điều này cho cả ba bộ dữ liệu, lấy AA dưới tỷ lệ tỉa 4× làm ví dụ, từ 0.00% bởi LCE đến 89.28%, 48.26%, và 25.52% bởi LD trên MNIST, CIFAR-10 và CIFAR-100, tương ứng.

Chúng tôi cũng quan sát thấy rằng việc kết hợp LH trong khi tỉa có lợi cho việc duy trì độ chính xác cao trong khi cải thiện tính mạnh mẽ đối nghịch chống lại các cuộc tấn công khác nhau. Bằng cách điều chỉnh LCE với LH, chúng tôi quan sát một lợi thế tính mạnh mẽ đối nghịch rõ rệt trên MNIST, lấy AA làm ví dụ từ 0.00% bởi LCE đến 47.49%, 40.71%, và 13.04% bằng cách kết hợp LH dưới tỷ lệ tỉa 4×, 8×, và 16×, tương ứng; bằng cách điều chỉnh LD với LH, chúng tôi lại thấy rằng việc điều chỉnh cải thiện tính mạnh mẽ đối nghịch trên tất cả các trường hợp, đặc biệt là w.r.t. cuộc tấn công mạnh nhất (AA). Chúng tôi lưu ý rằng việc cải thiện tính mạnh mẽ của việc kết hợp LH với LD không phải do sự đánh đổi giữa độ chính xác và tính mạnh mẽ: trên thực tế, LD+LH liên tục cải thiện cả độ chính xác tự nhiên và tính mạnh mẽ dưới tất cả các tỷ lệ tỉa trên tất cả các bộ dữ liệu. Được thúc đẩy bởi các quan sát trên, chúng tôi phân tích thêm cách hai thuật ngữ trong HBaR được định nghĩa trong Eq. (11) ảnh hưởng đến độ chính xác tự nhiên và tính mạnh mẽ và tóm tắt những điều này trong Phụ lục C.

LCE Làm Giảm Tính Mạnh Mẽ. Nhớ lại từ Phần V-A rằng chúng tôi loại bỏ mất mát phân loại khỏi mất mát chưng cất ban đầu để đạt được tỉa bảo toàn tính mạnh mẽ. Bảng V thực nghiệm cho thấy rằng mất mát phân loại (tức là, LCE) làm giảm đáng kể tính mạnh mẽ. Trực quan, PwoA chưng cất tính mạnh mẽ từ mô hình mạnh mẽ đã huấn luyện trước thay vì thu được nó từ các ví dụ tự nhiên. Điều này trái ngược với các quan sát được thực hiện với các phương pháp tỉa đối nghịch, như APD [37], nơi mất mát phân loại tăng tính mạnh mẽ. Điều này là vì APD tỉa bằng cách tối ưu hóa mất mát chưng cất ban đầu trên các ví dụ đối nghịch, vì vậy nó thực sự có thể được hưởng lợi từ LCE.

Hiệu Ứng Tỷ Lệ Tỉa. Trên Bảng III-IV, chúng tôi cũng quan sát một sự gia tăng nhẹ độ chính xác tự nhiên trong quá trình tỉa. Điều này là do tỉa giảm độ phức tạp của mô hình, và do đó, ở một mức độ nào đó, tránh overfitting. Tuy nhiên, việc tăng tỷ lệ tỉa vượt quá một điểm tới hạn có thể dẫn đến sụt giảm mạnh về độ chính xác. Điều này được mong đợi, vì việc giảm dung lượng mô hình một cách đáng kể cản trở khả năng biểu đạt của nó và bắt đầu giới thiệu bias trong các dự đoán. Không ngạc nhiên, điểm tới hạn này xảy ra sớm hơn trong các bộ dữ liệu phức tạp hơn. Chúng tôi cũng thấy rằng sự bão hòa/sụt giảm hiệu suất này xảy ra sớm hơn đối với tính mạnh mẽ đối nghịch khi so sánh với độ chính xác tự nhiên: việc bảo toàn tính mạnh mẽ thách thức hơn, đặc biệt là khi không kết hợp rõ ràng huấn luyện đối nghịch.

So Sánh với Parsimony Naïve. Chúng tôi chứng minh thêm rằng tỉa trong khi huấn luyện là bắt buộc để đạt được tính mạnh mẽ cao dưới một mô hình tiết kiệm. Để cho thấy điều này, chúng tôi xây dựng một lớp mô hình có ít tham số hơn WRN34-10 ban đầu, và khám phá sự đánh đổi tính mạnh mẽ-nén kết quả. Chúng tôi gọi lớp mô hình đầu tiên là 'WRN34-10-Lite': những mô hình này có cùng kiến trúc WRN34-10 nhưng chứa ít bộ lọc hơn trong mỗi lớp tích chập (dẫn đến ít tham số hơn tổng cộng). Những mô hình WRN34-10-Lite này được thiết kế để có tổng số tham số tương tự như các mô hình đã tỉa với tỷ lệ tỉa 4×, 8×, và 16×, tương ứng. Chúng tôi huấn luyện những mô hình 'Lite' này trong 100 epoch trên các ví dụ đối nghịch được tạo ra bởi PGD10. Mô hình đã tỉa vượt trội hơn phiên bản 'Lite' tương ứng trong tất cả các trường hợp, cải thiện tính mạnh mẽ dưới 16× chống AA lần lượt 10.47% và 7.35%, trên CIFAR-10 và CIFAR-100.

C. So Sánh với Các Phương Pháp Tỉa Đối Nghịch (AP)

Tính Mạnh Mẽ với Truy Cập Một Phần vào Các Ví Dụ Đối Nghịch. Đầu tiên chúng tôi so sánh PwoA với hai baseline AP tiên tiến nhất, tức là, AdvPrune và HYDRA, về tính mạnh mẽ đối nghịch và hiệu quả huấn luyện trên các bộ dữ liệu CIFAR-10 và CIFAR-100. Cả AdvPrune và HYDRA đều yêu cầu truy cập vào các ví dụ đối nghịch. Để so sánh công bằng, chúng tôi tạo ra các ví dụ đối nghịch một cách tiến bộ cho tất cả các phương pháp, bao gồm cả PwoA: trong Hình 3, chúng tôi thay đổi tỷ lệ mix α, tức là, phần của tổng số ví dụ tự nhiên được thay thế bằng các ví dụ đối nghịch được tạo ra bởi PGD10. Chúng tôi vẽ tính mạnh mẽ AA so với thời gian huấn luyện, dưới tỷ lệ tỉa 4×. Chúng tôi quan sát thấy rằng, mà không có quyền truy cập vào các ví dụ đối nghịch (tỷ lệ mix 0%), cả hai phương pháp cạnh tranh đều thất bại thảm khốc, không thể hiện tính mạnh mẽ nào cả. Hơn nữa, để đạt được cùng tính mạnh mẽ như PwoA, họ yêu cầu từ 4× đến 7× thời gian huấn luyện nhiều hơn; trên CIFAR-100, họ thực tế không bao giờ đạt được hiệu suất mà PwoA đạt được. Chúng tôi cũng quan sát thấy rằng PwoA cải thiện bằng quyền truy cập một phần vào các ví dụ đối nghịch; nhìn chung, nó đạt được sự đánh đổi thuận lợi hơn nhiều giữa tính mạnh mẽ và hiệu quả huấn luyện so với hai đối thủ cạnh tranh. Thật thú vị, ngoại trừ trường hợp được thể hiện trong Hình 3(b) (WRN34-10 trên CIFAR-10), PwoA liên tục vượt trội hơn các đối thủ cạnh tranh ở tỷ lệ mix 100%, w.r.t. cả tính mạnh mẽ và thời gian huấn luyện.

Tác Động của Phương Pháp Huấn Luyện Trước. Chúng tôi cũng quan sát thấy rằng HYDRA hoạt động tốt khi tỉa các mô hình được huấn luyện trước với TRADES, nhưng trở nên tệ hơn khi xử lý mô hình được huấn luyện trước với LBGAT. Điều này là do HYDRA tỉa mô hình bằng cách sử dụng TRADES như mất mát đối nghịch, và do đó được thiết kế riêng cho việc huấn luyện trước như vậy. Khi các mô hình được huấn luyện trước thông qua LBGAT, sự thay đổi mất mát này cản trở hiệu suất. Ngược lại, PwoA có thể tỉa thành công một mô hình đã huấn luyện trước tùy ý, bất kể kiến trúc hoặc phương pháp huấn luyện trước.

Tác Động Tỷ Lệ Tỉa. Chúng tôi đo thêm độ chính xác tự nhiên và tính mạnh mẽ của PwoA và các phương pháp SOTA của chúng tôi chống lại tất cả năm cuộc tấn công dưới tỷ lệ tỉa 4×, 8×, và 16×. Chúng tôi báo cáo những điều này ở tỷ lệ mix 20%, để thời gian huấn luyện gần như bằng nhau trên các phương pháp, trong Bảng VI cho CIFAR-10 và Bảng VII cho CIFAR-100. Nhìn chung, chúng ta có thể thấy rõ ràng rằng PwoA liên tục vượt trội hơn các phương pháp SOTA khác chống lại tất cả năm cuộc tấn công, dưới thời gian huấn luyện tương tự (hoặc thấp hơn). Cụ thể, trên CIFAR-100, PwoA duy trì tính mạnh mẽ cao chống AA chỉ với sụt giảm 1.62% (dưới 4× PR) từ mô hình đã huấn luyện trước bởi LBGAT (xem Bảng I), trong khi tính mạnh mẽ AA đạt được bởi HYDRA và AdvPrune sụt giảm lần lượt 6.12% và 13.98%. Điều này lại xác minh rằng, khi tỉa một mô hình mạnh mẽ được huấn luyện trước với các phương pháp huấn luyện đối nghịch khác nhau, PwoA ổn định hơn trong việc bảo toàn tính mạnh mẽ. Những cải thiện cũng rõ ràng trong khi tăng tỷ lệ tỉa: PwoA vượt trội hơn HYDRA chống AA lần lượt 4.50%, 4.10%, và 3.33% dưới tỷ lệ tỉa 4×, 8×, và 16×. Để hoàn thiện, chúng tôi cũng báo cáo hiệu suất ở tỷ lệ mix 0% trên CIFAR-100 trong Phụ lục D; trái ngược với PwoA, các đối thủ cạnh tranh thể hiện tính mạnh mẽ hầu như không đáng kể trong trường hợp này.

So Sánh với APD. Cuối cùng, chúng tôi cũng so sánh với APD [37], yếu hơn HYDRA và AdvPrune, nhưng liên quan gần hơn với PwoA của chúng tôi: APD tỉa bằng cách tối ưu hóa KD trên các ví dụ đối nghịch bằng cách sử dụng một giáo viên không mạnh mẽ. Bảng VIII so sánh PwoA với APD trên CIFAR-10 bằng ResNet-18 dưới tỷ lệ tỉa 4× (là tỷ lệ tỉa lớn nhất được báo cáo trong bài báo của họ). Chúng tôi quan sát thấy rằng, trong khi đạt được độ chính xác tương tự, PwoA vượt trội hơn APD w.r.t. cả tính mạnh mẽ và hiệu quả huấn luyện. Điều này được mong đợi, vì việc chưng cất từ một giáo viên không mạnh mẽ hạn chế khả năng học của APD từ các ví dụ đối nghịch và việc tạo ra các ví dụ đối nghịch cản trở hiệu quả huấn luyện.

VII. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi đã đề xuất PwoA, một khung làm việc thống nhất để tỉa các mạng mạnh mẽ chống đối nghịch mà không cần các ví dụ đối nghịch. Phương pháp của chúng tôi tận dụng các mô hình mạnh mẽ chống đối nghịch đã huấn luyện trước, bảo toàn tính mạnh mẽ đối nghịch thông qua tự chưng cất và tăng cường nó thông qua Hilbert-Schmidt independence criterion như một bộ điều chỉnh. Các thí nghiệm toàn diện trên các bộ dữ liệu MNIST, CIFAR-10, và CIFAR-100 chứng minh rằng PwoA tỉa một phần lớn trọng số trong khi đạt được tính mạnh mẽ đối nghịch tương đương với tốc độ huấn luyện nhanh hơn lên đến 7×. Các hướng tương lai bao gồm mở rộng khung PwoA sang tỉa có cấu trúc và lượng tử hóa trọng số. Một hướng tương lai thú vị khác là sử dụng chưng cất và các phạt mới để tỉa một mô hình mạnh mẽ đã huấn luyện trước ngay cả khi không có quyền truy cập vào các ví dụ tự nhiên.

VIII. LỜI CẢM ƠN

Các tác giả trân trọng cảm ơn sự hỗ trợ của National Science Foundation dưới các grant CCF-1937500 và CNS-2112471.
