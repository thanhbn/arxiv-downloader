Mạng Nơ-ron 154 (2022) 218–233
Danh sách nội dung có sẵn tại ScienceDirect
Mạng Nơ-ron
trang web tạp chí: www.elsevier.com/locate/neunet
Huấn luyện Đối kháng Nội suy: Đạt được mạng nơ-ron mạnh mẽ
mà không hy sinh quá nhiều độ chính xác
Alex Lamba,∗,1, Vikas Vermab,∗,1, Kenji Kawaguchic, Alexander Matyaskod, Savya Khoslae,
Juho Kannalab, Yoshua Bengioa
aMontreal Institute for Learning Algorithms (MILA), Canada
bAalto University, Finland
cHarvard University, USA
dNational University of Singapore, Singapore
eGoogle, India
thông tin bài báo
Lịch sử bài báo:
Nhận 22 tháng 4 năm 2021Nhận dạng sửa đổi 10 tháng 4 năm 2022Chấp nhận 10 tháng 7 năm 2022Có sẵn trực tuyến 16 tháng 7 năm 2022
Từ khóa:Tính mạnh mẽ đối khángMixupManifold MixupLỗi kiểm tra chuẩntóm tắt
Tính mạnh mẽ đối kháng đã trở thành mục tiêu trung tâm trong học sâu, cả trong lý thuyết và thực hành. Tuy nhiên, các phương pháp thành công để cải thiện tính mạnh mẽ đối kháng (như huấn luyện đối kháng) làm tổn hại đáng kể hiệu suất tổng quát hóa trên dữ liệu không bị nhiễu loạn. Điều này có thể có tác động lớn đến cách tính mạnh mẽ đối kháng ảnh hưởng đến các hệ thống thế giới thực (tức là nhiều người có thể chọn từ bỏ tính mạnh mẽ nếu nó có thể cải thiện độ chính xác trên dữ liệu không bị nhiễu loạn). Chúng tôi đề xuất Huấn luyện Đối kháng Nội suy, sử dụng các phương pháp huấn luyện dựa trên nội suy được đề xuất gần đây trong khuôn khổ huấn luyện đối kháng. Trên CIFAR-10, huấn luyện đối kháng tăng lỗi kiểm tra chuẩn (khi không có đối thủ) từ 4.43% lên 12.32%, trong khi với Huấn luyện đối kháng nội suy của chúng tôi, chúng tôi duy trì tính mạnh mẽ đối kháng trong khi đạt được lỗi kiểm tra chuẩn chỉ 6.45%. Với kỹ thuật của chúng tôi, sự gia tăng tương đối trong lỗi chuẩn cho mô hình mạnh mẽ được giảm từ 178.1% xuống chỉ 45.5%. Hơn nữa, chúng tôi cung cấp phân tích toán học về Huấn luyện Đối kháng Nội suy để xác nhận hiệu quả của nó và chứng minh những ưu điểm của nó về mặt tính mạnh mẽ và tổng quát hóa.
©2022 Các Tác giả. Xuất bản bởi Elsevier Ltd. Đây là bài báo truy cập mở theo giấy phép CC BY
(http://creativecommons.org/licenses/by/4.0/ ).

1. Giới thiệu

Mạng nơ-ron sâu đã rất thành công trên nhiều nhiệm vụ khác nhau. Sự thành công này đã thúc đẩy các ứng dụng trong những lĩnh vực mà độ tin cậy và bảo mật là quan trọng, bao gồm nhận dạng khuôn mặt (Sharif, Bhagavatula, Bauer, & Reiter, 2017), xe tự lái (Bojarski et al., 2016), chăm sóc sức khỏe, và phát hiện phần mềm độc hại (LeCun, Bengio, & Hinton, 2015). Các mối quan ngại về bảo mật xuất hiện khi những kẻ đối địch của hệ thống có lợi ích từ việc hệ thống hoạt động kém. Nghiên cứu về Ví dụ đối kháng (Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, & Fergus, 2013) đã chỉ ra rằng mạng nơ-ron dễ bị tấn công bằng cách nhiễu loạn dữ liệu theo những cách không thể nhận thức được. Nhiều biện pháp phòng thủ đã được đề xuất, nhưng hầu hết chúng dựa vào gradient che khuất (Athalye, Carlini, & Wagner, 2018) để tạo ra ảo tưởng sai lầm về phòng thủ bằng cách giảm chất lượng tín hiệu gradient, mà không thực sự cải thiện tính mạnh mẽ (Athalye et al., 2018). Trong số các biện pháp phòng thủ này, chỉ có huấn luyện đối kháng (Kurakin, Goodfellow, & Bengio, 2016b) vẫn hiệu quả sau khi giải quyết vấn đề gradient che khuất.

Tuy nhiên, huấn luyện đối kháng có một nhược điểm lớn: nó làm giảm đáng kể hiệu suất tổng quát hóa của mạng trên các mẫu dữ liệu không bị nhiễu loạn, đặc biệt đối với các mạng nhỏ. Ví dụ, Madry, Makelov, Schmidt, Tsipras, và Vladu (2017) báo cáo rằng việc thêm huấn luyện đối kháng vào một mô hình cụ thể làm tăng lỗi kiểm tra chuẩn từ 6.3% lên 21.6% trên CIFAR-10. Hiện tượng này làm cho huấn luyện đối kháng khó sử dụng trong thực tế. Nếu sự căng thẳng giữa hiệu suất và bảo mật hóa ra không thể hòa giải, thì nhiều hệ thống sẽ phải chọn hoặc hiệu suất kém hoặc chấp nhận tính dễ bị tổn thương, một tình huống dẫn đến tác động tiêu cực lớn.

Đóng góp của chúng tôi: Chúng tôi đề xuất tăng cường huấn luyện đối kháng với huấn luyện dựa trên nội suy, như một giải pháp cho vấn đề trên.
• Chúng tôi chứng minh rằng phương pháp của chúng tôi cải thiện đáng kể lỗi kiểm tra chuẩn trong khi vẫn đạt được tính mạnh mẽ đối kháng, sử dụng các bộ dữ liệu chuẩn (CIFAR10, CIFAR100 và SHVN) và kiến trúc chuẩn (Wide-ResNet và ResNet): Phần 5.1.
• Chúng tôi chứng minh rằng phương pháp của chúng tôi không gặp vấn đề gradient che khuất bằng cách thực hiện các cuộc tấn công hộp đen trên các mô hình được huấn luyện với phương pháp của chúng tôi: Phần 5.2.
• Chúng tôi thực hiện tấn công PGD với số bước cao hơn (lên đến 1000 bước) và giá trị epsilon nhiễu loạn/biến dạng tối đa được phép cao hơn, để chứng minh rằng tính mạnh mẽ đối kháng của phương pháp chúng tôi vẫn ở cùng mức với huấn luyện đối kháng: Phần 5.3.
• Chúng tôi chứng minh rằng các mạng được huấn luyện với phương pháp của chúng tôi có độ phức tạp thấp hơn, do đó dẫn đến lỗi kiểm tra chuẩn được cải thiện: Phần 6.
• Chúng tôi phân tích toán học lợi ích của phương pháp đề xuất về mặt tính mạnh mẽ và tổng quát hóa. Đối với tính mạnh mẽ, chúng tôi chỉ ra rằng Huấn luyện Đối kháng Nội suy tương ứng với việc tối thiểu hóa gần đúng một cận trên của mất mát đối kháng với các nhiễu loạn đối kháng bổ sung. Điều này giải thích tại sao các mô hình thu được bằng phương pháp đề xuất bảo tồn tính mạnh mẽ đối kháng và đôi khi có thể cải thiện thêm tính mạnh mẽ khi so sánh với huấn luyện đối kháng chuẩn. Đối với tổng quát hóa, chúng tôi chứng minh một cận tổng quát hóa mới cho Huấn luyện Đối kháng Nội suy và phân tích các lợi ích của phương pháp đề xuất.

2. Công trình liên quan

Sự đánh đổi giữa lỗi kiểm tra chuẩn và tính mạnh mẽ đối kháng đã được nghiên cứu trong Madry et al. (2017), Raghunathan, Xie, Yang, Duchi, và Liang (2019), Tsipras, Santurkar, Engstrom, Turner, và Madry (2018) và Zhang, Yu, Jiao, Xing, Ghaoui, và Jordan (2019a). Trong khi Madry et al. (2017), Tsipras et al. (2018) và Zhang et al. (2019a) chứng minh thực nghiệm sự đánh đổi này, Tsipras et al. (2018) và Zhang et al. (2019a) cũng chứng minh lý thuyết sự đánh đổi này trên các bài toán học tập được xây dựng. Hơn nữa, Raghunathan et al. (2019) nghiên cứu sự đánh đổi này từ góc độ các tính chất thống kê của mục tiêu mạnh mẽ (Ben-Tal, El Ghaoui, & Nemirovski, 2009) và động lực học của việc tối ưu hóa một mục tiêu mạnh mẽ trên mạng nơ-ron, và đề xuất rằng huấn luyện đối kháng cần nhiều dữ liệu hơn để có được lỗi kiểm tra chuẩn thấp hơn. Kết quả của chúng tôi trên các bộ dữ liệu SVHN, CIFAR-10, và CIFAR-100 (Phần 5.1) cũng nhất quán cho thấy lỗi kiểm tra chuẩn cao hơn với huấn luyện đối kháng PGD.

Trong khi Tsipras et al. (2018) trình bày các chứng minh phụ thuộc dữ liệu cho thấy rằng trên một số phân phối được xây dựng nhân tạo - không thể nào một bộ phân loại mạnh mẽ có thể tổng quát hóa tốt như một bộ phân loại không mạnh mẽ. Cách điều này liên quan đến kết quả của chúng tôi là một câu hỏi hấp dẫn. Kết quả của chúng tôi cho thấy rằng khoảng cách tổng quát hóa giữa huấn luyện đối kháng và các mô hình không mạnh mẽ có thể được giảm đáng kể thông qua các thuật toán tốt hơn, nhưng vẫn có thể việc đóng hoàn toàn khoảng cách này trên một số bộ dữ liệu là không thể. Một câu hỏi quan trọng cho công việc tương lai là có bao nhiều khoảng cách tổng quát hóa này có thể được giải thích theo các tính chất dữ liệu vốn có và bao nhiều khoảng cách này có thể được giải quyết thông qua các mô hình tốt hơn.

Tìm kiếm Kiến trúc Mạng nơ-ron (Zoph & Le, 2016) đã được sử dụng để tìm các kiến trúc đạt được tính mạnh mẽ cao đối với các cuộc tấn công PGD cũng như lỗi kiểm tra tốt hơn trên dữ liệu không bị nhiễu loạn (Cubuk, Zoph, Schoenholz, & Le, 2018). Điều này cải thiện lỗi kiểm tra trên dữ liệu không bị nhiễu loạn và một so sánh trực tiếp với phương pháp của chúng tôi ở Bảng 2. Tuy nhiên, phương pháp của Cubuk et al. (2018) rất tốn kém về mặt tính toán vì mỗi thí nghiệm đòi hỏi huấn luyện hàng nghìn mô hình để tìm kiếm kiến trúc tối ưu (9360 mô hình con mỗi mô hình được huấn luyện 10 epoch trong Cubuk et al., 2018), trong khi phương pháp của chúng tôi không liên quan đến tính toán bổ sung đáng kể.

Trong công việc của chúng tôi, chúng tôi chủ yếu quan tâm đến huấn luyện đối kháng, nhưng các kỹ thuật trong lĩnh vực nghiên cứu về các biện pháp phòng thủ có thể chứng minh cũng đã cho thấy sự đánh đổi giữa tính mạnh mẽ và tổng quát hóa trên dữ liệu không bị nhiễu loạn. Ví dụ, biện pháp phòng thủ mạng kép của Kolter và Wong (2017) báo cáo 20.38% lỗi kiểm tra chuẩn trên SVHN cho mạng tích chập có thể chứng minh mạnh mẽ của họ (hầu hết các mô hình không mạnh mẽ đều dưới 5% lỗi kiểm tra trên SVHN). Wong, Schmidt, Metzen, và Kolter (2018) báo cáo độ chính xác kiểm tra chuẩn tốt nhất 29.23% sử dụng ResNet tích chập trên CIFAR-10 (hầu hết ResNet không mạnh mẽ có độ chính xác trên 90%). Mục tiêu của chúng tôi ở đây không phải là chỉ trích công việc này, vì phát triển các biện pháp phòng thủ có thể chứng minh là một lĩnh vực nghiên cứu thách thức và quan trọng, mà là để cho thấy rằng vấn đề mà chúng tôi khám phá với Huấn luyện Đối kháng Nội suy (trên các biện pháp phòng thủ kiểu huấn luyện đối kháng của Madry et al., 2017) cũng nghiêm trọng với các biện pháp phòng thủ có thể chứng minh, và hiểu liệu những hiểu biết được phát triển ở đây có chuyển giao sang các biện pháp phòng thủ có thể chứng minh hay không, có thể là một lĩnh vực thú vị cho công việc tương lai.

Tổng quát hóa mạnh mẽ đối kháng: Một hướng nghiên cứu khác liên quan đến tổng quát hóa mạnh mẽ đối kháng: hiệu suất của các mạng được huấn luyện đối kháng trên các ví dụ kiểm tra đối kháng. Schmidt, Santurkar, Tsipras, Talwar, và Madry (2018) quan sát rằng độ phức tạp mẫu cao hơn là cần thiết cho tổng quát hóa mạnh mẽ đối kháng tốt hơn. Yin, Ramchandran, và Bartlett (2018) chứng minh rằng huấn luyện đối kháng dẫn đến các mô hình phức tạp hơn và do đó tổng quát hóa mạnh mẽ đối kháng kém hơn. Hơn nữa, Schmidt et al. (2018) đề xuất rằng tổng quát hóa mạnh mẽ đối kháng cần nhiều dữ liệu hơn và Carmon, Raghunathan, Schmidt, Liang, và Duchi (2019), Zhai et al. (2019) chứng minh rằng dữ liệu không có nhãn có thể được sử dụng để cải thiện tổng quát hóa mạnh mẽ đối kháng. Trái ngược với công việc của họ, trong công việc này chúng tôi tập trung vào cải thiện hiệu suất tổng quát hóa trên các mẫu không bị nhiễu loạn (lỗi kiểm tra chuẩn), trong khi duy trì tính mạnh mẽ trên các ví dụ đối kháng chưa thấy ở cùng mức.

Các kỹ thuật huấn luyện dựa trên nội suy: Còn một hướng nghiên cứu khác (Berthelot, Carlini, Goodfellow, Papernot, Oliver, & Raffel, 2019; Jeong, Verma, Hyun, Kannala, & Kwak, 2021; Verma et al., 2019; Verma, Lamb, Kannala, Bengio, & Lopez-Paz, 2019; Verma, Qu, Lamb, Bengio, Kannala, & Tang, 2019; Zhang, Cissé, Dauphin, & Lopez-Paz, 2017) cho thấy rằng các kỹ thuật huấn luyện nội suy đơn giản có thể giảm đáng kể lỗi kiểm tra chuẩn trong các mô hình học có giám sát đầy đủ và bán giám sát. Theo hướng này, Zhang, Hsieh, và Tao (2018) nghiên cứu các tính chất lý thuyết của các kỹ thuật huấn luyện dựa trên nội suy như Mixup (Zhang et al., 2017).

3. Nền tảng

3.1. Khuôn khổ tối thiểu hóa rủi ro thực nghiệm

Hãy xem xét một nhiệm vụ phân loại tổng quát với phân phối dữ liệu cơ bản D bao gồm các ví dụ x∈X và các nhãn tương ứng y∈Y. Nhiệm vụ là học một hàm f: X→Y sao cho với một x cho trước, f xuất ra y tương ứng. Nó có thể được thực hiện bằng cách tối thiểu hóa rủi ro E(x,y)∼D[L(x,y,θ)], trong đó L(θ,x,y) là một hàm mất mát thích hợp ví dụ như mất mát cross-entropy và θ∈Rp là tập hợp các tham số của hàm f. Vì kỳ vọng này không thể tính toán được, do đó một phương pháp phổ biến là tối thiểu hóa rủi ro thực nghiệm 1/N∑ᵢ₌₁ᴺL(xᵢ,yᵢ,θ) chỉ tính đến một số hữu hạn các ví dụ được rút ra từ phân phối dữ liệu D, cụ thể là (x₁,y₁), ..., (xₙ,yₙ).

3.2. Các cuộc tấn công đối kháng và tính mạnh mẽ

Trong khi khuôn khổ tối thiểu hóa rủi ro thực nghiệm đã rất thành công và thường dẫn đến tổng quát hóa xuất sắc trên các ví dụ kiểm tra không bị nhiễu loạn, nó có hạn chế đáng kể là không đảm bảo hiệu suất tốt trên các ví dụ được nhiễu loạn cẩn thận để đánh lừa mô hình (Goodfellow, Shlens, & Szegedy, 2014; Szegedy et al., 2013). Nghĩa là, khuôn khổ tối thiểu hóa rủi ro thực nghiệm gặp vấn đề thiếu tính mạnh mẽ đối với các cuộc tấn công đối kháng.

Madry et al. (2017) đề xuất một quan điểm tối ưu hóa về tính mạnh mẽ đối kháng, trong đó tính mạnh mẽ đối kháng của một mô hình được định nghĩa như một bài toán min-max. Sử dụng quan điểm này, các tham số θ của một hàm f được học bằng cách tối thiểu hóa ρ(θ) như được mô tả trong Eq. (1). S định nghĩa một vùng các điểm xung quanh mỗi ví dụ, thường được chọn sao cho nó chỉ chứa các nhiễu loạn không thể nhận thức được bằng mắt.

min_θ ρ(θ), trong đó ρ(θ) = E_(x,y)∼D[max_(δ∈S) L(θ,x+δ,y)] (1)

Các cuộc tấn công đối kháng có thể được phân loại rộng rãi thành hai loại: Các cuộc tấn công một bước và Các cuộc tấn công nhiều bước. Chúng tôi đánh giá hiệu suất của mô hình của chúng tôi như một biện pháp phòng thủ chống lại cuộc tấn công đối kháng phổ biến và được nghiên cứu kỹ nhất từ mỗi loại này. Thứ nhất, chúng tôi xem xét Phương pháp Dấu hiệu Gradient Nhanh (Goodfellow et al., 2014) là một bước đơn và vẫn có thể hiệu quả chống lại nhiều mạng. Thứ hai, chúng tôi xem xét cuộc tấn công gradient descent dự án (Kurakin et al., 2016b) là một cuộc tấn công nhiều bước. Nó chậm hơn FGSM vì cần nhiều lần lặp, nhưng đã được chứng minh là một cuộc tấn công mạnh hơn nhiều (Madry et al., 2017). Chúng tôi mô tả ngắn gọn hai cuộc tấn công này như sau:

Phương pháp Dấu hiệu Gradient Nhanh (FGSM). Phương pháp Dấu hiệu Gradient Nhanh (Goodfellow et al., 2014) tạo ra các đối thủ bị ràng buộc l∞ bằng cách theo dấu hiệu của nhiễu loạn dựa trên gradient. Cuộc tấn công này rẻ vì nó chỉ dựa vào tính toán gradient một lần và thường là một cuộc tấn công hiệu quả chống lại mạng sâu (Goodfellow et al., 2014; Madry et al., 2017), đặc biệt khi không có biện pháp phòng thủ đối kháng nào được sử dụng.

x̃ = x + ε sgn(∇_x L(θ,x,y)). (2)

Gradient Descent Dự án (PGD). Cuộc tấn công gradient descent dự án (Madry et al., 2017), đôi khi được gọi là FGSMk, là một phần mở rộng nhiều bước của cuộc tấn công FGSM được đặc trưng như sau:

x^(t+1) = Π_(x+S)(x^t + α sgn(∇_x L(θ,x,y))). (3)

được khởi tạo với x⁰ là đầu vào sạch x. S chính thức hóa sức mạnh thao túng của đối thủ. Π đề cập đến toán tử dự án, trong bối cảnh này có nghĩa là dự án ví dụ đối kháng trở lại vào vùng trong bán kính S của điểm dữ liệu gốc, sau mỗi bước có kích thước α trong cuộc tấn công đối kháng.

3.3. Che khuất gradient bởi các biện pháp phòng thủ đối kháng

Nhiều phương pháp đã được đề xuất như một biện pháp phòng thủ chống lại các cuộc tấn công đối kháng. Một thách thức đáng kể với việc đánh giá các biện pháp phòng thủ chống lại các cuộc tấn công đối kháng là nhiều cuộc tấn công dựa vào gradient của mạng. Các phương pháp phòng thủ làm giảm chất lượng gradient này, bằng cách làm cho nó phẳng hơn hoặc nhiễu hơn có thể dẫn đến các phương pháp làm giảm hiệu quả của các cuộc tấn công dựa trên gradient, nhưng thực tế không mạnh mẽ đối với các ví dụ đối kháng (Athalye, Engstrom, Ilyas, & Kwok, 2017; Papernot, McDaniel, Sinha, & Wellman, 2016). Quá trình này, được gọi là che khuất gradient hoặc che khuất gradient, phải được phân tích khi nghiên cứu sức mạnh của một biện pháp phòng thủ đối kháng.

Một phương pháp để kiểm tra mức độ mà một biện pháp phòng thủ đối kháng cho kết quả tốt một cách lừa dối do che khuất gradient dựa vào quan sát rằng các cuộc tấn công hộp đen là một tập con nghiêm ngặt của các cuộc tấn công hộp trắng, vì vậy các cuộc tấn công hộp trắng nên luôn ít nhất mạnh như các cuộc tấn công hộp đen. Nếu một phương pháp báo cáo phòng thủ tốt hơn nhiều chống lại các cuộc tấn công hộp trắng so với cuộc tấn công hộp đen, nó cho thấy rằng cuộc tấn công hộp trắng được chọn bị suy yếu do che khuất gradient. Một bài kiểm tra khác cho che khuất gradient là chạy một tìm kiếm lặp, như gradient descent dự án (PGD) với một phạm vi không giới hạn cho một số lượng lớn các lần lặp. Nếu một cuộc tấn công như vậy không hoàn toàn thành công, nó chỉ ra rằng gradient của mô hình không phải là một phương pháp hiệu quả để tìm kiếm hình ảnh đối kháng, và che khuất gradient đang xảy ra. Chúng tôi chứng minh kết quả thành công với Huấn luyện Đối kháng Nội suy trên các kiểm tra đúng đắn này trong Phần 5.2. Còn một bài kiểm tra khác là xác nhận rằng các cuộc tấn công lặp với kích thước bước nhỏ luôn vượt trội hơn các cuộc tấn công một bước với kích thước bước lớn hơn (như FGSM). Nếu đây không phải là trường hợp, nó có thể cho thấy rằng cuộc tấn công lặp bị kẹt trong các vùng mà tối ưu hóa sử dụng gradient kém do che khuất gradient. Trong tất cả các thí nghiệm của chúng tôi cho Huấn luyện Đối kháng Nội suy, chúng tôi thấy rằng các cuộc tấn công PGD lặp với kích thước bước nhỏ hơn và nhiều lần lặp hơn luôn mạnh hơn các cuộc tấn công FGSM (thực hiện một bước lớn duy nhất) chống lại các mô hình của chúng tôi, như được hiển thị trong Bảng 2-7.

3.4. Huấn luyện đối kháng

Huấn luyện đối kháng (Goodfellow et al., 2014) bao gồm việc tạo ra các ví dụ đối kháng và sử dụng chúng trong quá trình huấn luyện để tăng tính mạnh mẽ chống lại các ví dụ đối kháng chưa thấy. Để mở rộng huấn luyện đối kháng cho các bộ dữ liệu lớn và các mô hình lớn, thường các ví dụ đối kháng được tạo ra bằng các phương pháp một bước nhanh như FGSM. Tuy nhiên, huấn luyện đối kháng với các phương pháp một bước nhanh vẫn dễ bị tổn thương trước các cuộc tấn công đối kháng từ một cuộc tấn công nhiều bước mạnh hơn như PGD. Do đó, trong công việc này, chúng tôi xem xét huấn luyện đối kháng với PGD.

4. Huấn luyện đối kháng nội suy

Chúng tôi đề xuất Huấn luyện Đối kháng Nội suy (IAT), huấn luyện trên các nội suy của các ví dụ đối kháng cùng với các nội suy của các ví dụ không bị nhiễu loạn. Chúng tôi sử dụng các kỹ thuật của Mixup (Zhang et al., 2017) và Manifold Mixup (Verma et al., 2019) như các cách nội suy các ví dụ. Việc học được thực hiện trong bốn bước sau khi huấn luyện một mạng với Huấn luyện Đối kháng Nội suy. Trong bước đầu tiên, chúng tôi tính toán mất mát từ một batch không bị nhiễu loạn (không đối kháng) (với các nội suy dựa trên Mixup hoặc Manifold Mixup). Trong bước thứ hai, chúng tôi tạo ra một batch các ví dụ đối kháng sử dụng một cuộc tấn công đối kháng (như Projected Gradient Descent (PGD) (Madry et al., 2017) hoặc Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014)). Trong bước thứ ba, chúng tôi huấn luyện chống lại các ví dụ đối kháng này với các nhãn gốc, với các nội suy dựa trên Mixup hoặc Manifold Mixup. Trong bước thứ tư, chúng tôi lấy trung bình của mất mát từ batch không bị nhiễu loạn và batch đối kháng và cập nhật các tham số mạng sử dụng mất mát này. Lưu ý rằng theo Kurakin, Goodfellow, và Bengio (2016a), Tsipras et al. (2018), chúng tôi sử dụng cả các mẫu không bị nhiễu loạn và đối kháng để huấn luyện mô hình Huấn luyện Đối kháng Nội suy và chúng tôi sử dụng nó trong các mô hình huấn luyện đối kháng cơ sở của chúng tôi cũng vậy. Thuật toán chi tiết được mô tả trong Khối Thuật toán 1.

Vì Huấn luyện Đối kháng Nội suy kết hợp huấn luyện đối kháng với Mixup (Zhang et al., 2017) hoặc Manifold Mixup (Verma et al., 2019), chúng tôi tóm tắt các phương pháp hỗ trợ này chi tiết hơn. Phương pháp Mixup (Zhang et al., 2017) bao gồm việc rút ra một cặp mẫu từ bộ dữ liệu (xᵢ,yᵢ)∼pD và (xⱼ,yⱼ)∼pD và sau đó lấy một nội suy tuyến tính ngẫu nhiên trong không gian đầu vào x̃ = λxᵢ + (1-λ)xⱼ. λ này được lấy mẫu ngẫu nhiên tại mỗi cập nhật (thường từ phân phối Beta). Sau đó mạng fθ được chạy tiến trên đầu vào nội suy x̃ và được huấn luyện sử dụng cùng nội suy tuyến tính của các mất mát L = λL(fθ(x̃),yᵢ) + (1-λ)L(fθ(x̃),yⱼ). Ở đây L đề cập đến một hàm mất mát như cross entropy.

Phương pháp Manifold Mixup (Verma et al., 2019) có liên quan chặt chẽ đến Mixup từ góc độ tính toán, ngoại trừ việc lớp mà nội suy được thực hiện, được chọn ngẫu nhiên tại mỗi cập nhật huấn luyện.

Huấn luyện đối kháng bao gồm việc tạo ra các ví dụ đối kháng và huấn luyện mô hình để cho các điểm này nhãn gốc. Để tạo ra các ví dụ đối kháng này trong quá trình huấn luyện, chúng tôi sử dụng cuộc tấn công Projected Gradient Descent (PGD), cũng được biết đến như FGSM lặp. Cuộc tấn công này bao gồm việc cập nhật lặp đi lặp lại một nhiễu loạn đối kháng bằng cách di chuyển theo hướng dấu hiệu của gradient nhân với một kích thước bước nào đó, trong khi dự án trở lại một quả cầu L∞ bằng cách cắt nhiễu loạn đến tối đa ε₁. Cả ε₁, kích thước bước để di chuyển trên mỗi lần lặp, và số lần lặp đều là các siêu tham số cho cuộc tấn công.

Tại sao Huấn luyện Đối kháng Nội suy giúp cải thiện độ chính xác kiểm tra chuẩn: Chúng tôi trình bày hai lập luận về tại sao Huấn luyện Đối kháng Nội suy có thể cải thiện độ chính xác kiểm tra chuẩn:

Tăng kích thước tập huấn luyện: Raghunathan et al. (2019) đã chỉ ra rằng huấn luyện đối kháng có thể cần nhiều mẫu huấn luyện hơn để đạt được độ chính xác kiểm tra chuẩn cao hơn. Mixup (Zhang et al., 2017) và Manifold Mixup (Verma et al., 2019) có thể được xem như các kỹ thuật tăng kích thước hiệu quả của tập huấn luyện bằng cách tạo ra các mẫu huấn luyện mới. Do đó các kỹ thuật này có thể hữu ích trong việc cải thiện độ chính xác kiểm tra chuẩn.

Nén thông tin: Shwartz-Ziv và Tishby (2017) và Tishby và Zaslavsky (2015) đã chỉ ra mối quan hệ giữa nén thông tin trong các đặc trưng được học bởi mạng sâu và tổng quát hóa. Điều này liên hệ mức độ mà mạng sâu nén thông tin trong các trạng thái ẩn của chúng với các cận về tổng quát hóa, với một cận mạnh hơn khi mạng sâu có nén mạnh hơn.

Để đánh giá tác động của huấn luyện đối kháng lên việc nén thông tin trong các đặc trưng, chúng tôi thực hiện một thí nghiệm trong đó chúng tôi lấy các biểu diễn được học sau huấn luyện, và nghiên cứu mức độ tốt mà các biểu diễn đông băng này có thể dự đoán thành công các nhãn ngẫu nhiên cố định. Nếu mô hình nén tốt các biểu diễn, thì sẽ khó hơn để khớp các nhãn ngẫu nhiên.

Cụ thể, chúng tôi chạy một MLP 2 lớp nhỏ trên đầu các biểu diễn đã học để khớp các nhãn nhị phân ngẫu nhiên. Trong tất cả các trường hợp, chúng tôi huấn luyện mô hình với các nhãn ngẫu nhiên trong 200 epoch với cùng các siêu tham số. Để khớp 10000 ví dụ được gắn nhãn ngẫu nhiên, chúng tôi đạt được độ chính xác: 92.08% (Cơ sở) và 97.00% (Huấn luyện Đối kháng PGD): cho thấy rằng huấn luyện đối kháng làm cho các biểu diễn ít nén hơn nhiều.

Manifold Mixup (Verma et al., 2019) đã chỉ ra học các đặc trưng nén hơn. Do đó, sử dụng Manifold Mixup với huấn luyện đối kháng có thể giảm thiểu tác động bất lợi của huấn luyện đối kháng. Sử dụng cùng thiết lập thí nghiệm như trên, chúng tôi đạt được độ chính xác: 64.17% (Manifold Mixup) và 71.00% (IAT sử dụng Manifold Mixup).

Các kết quả này cho thấy rằng huấn luyện đối kháng khiến các biểu diễn được học ít nén hơn, có thể là lý do cho độ chính xác kiểm tra chuẩn kém. Đồng thời, IAT với Manifold Mixup giảm đáng kể khả năng của mô hình để học các đặc trưng ít nén hơn, có thể cải thiện độ chính xác kiểm tra chuẩn.

Để cung cấp bằng chứng thêm cho sự khác biệt trong các đặc tính nén, chúng tôi huấn luyện các mô hình hoàn toàn kết nối 5 lớp trên MNIST và xem xét một lớp nút cổ chai 30 đơn vị ngay sau lớp ẩn đầu tiên. Sau đó chúng tôi thực hiện phân tích giá trị đơn trên các biểu diễn theo lớp và xem xét phổ của các giá trị đơn (Hình 1 và Bảng 1). Chúng tôi thấy rằng PGD tăng đáng kể số lượng giá trị đơn với các giá trị lớn tương đối so với mô hình cơ sở (FGSM ở đâu đó giữa cơ sở và PGD).

5. Thí nghiệm

5.1. Tính mạnh mẽ đối kháng

Mục tiêu của các thí nghiệm của chúng tôi là cung cấp hỗ trợ thực nghiệm cho hai khẳng định chính của chúng tôi: rằng huấn luyện đối kháng làm tổn hại hiệu suất trên dữ liệu không bị nhiễu loạn (điều này phù hợp với những gì đã được quan sát trước đây trong Madry et al., 2017; Tsipras et al., 2018; Zhang et al., 2019a) và để chỉ ra rằng sự khác biệt này có thể được giảm với phương pháp Huấn luyện Đối kháng Nội suy của chúng tôi. Cuối cùng, chúng tôi muốn chỉ ra rằng Huấn luyện Đối kháng Nội suy là mạnh mẽ đối kháng và không gặp vấn đề che khuất gradient (Athalye et al., 2018).

Trong các thí nghiệm của chúng tôi, chúng tôi luôn thực hiện huấn luyện đối kháng sử dụng cuộc tấn công PGD 7 bước nhưng chúng tôi đánh giá trên nhiều cuộc tấn công khác nhau: FGSM, PGD (với số bước và siêu tham số khác nhau), cuộc tấn công Carlini-Wagner (Carlini & Wagner, 2016), và AutoAttack (Croce & Hein, 2020).

Kiến trúc và Bộ dữ liệu: Chúng tôi tiến hành thí nghiệm trên các mạng cạnh tranh để chứng minh rằng Huấn luyện Đối kháng Nội suy có thể cải thiện hiệu suất tổng quát hóa mà không hy sinh tính mạnh mẽ đối kháng. Chúng tôi sử dụng hai kiến trúc: Thứ nhất, kiến trúc WideResNet được đề xuất trong He, Zhang, Ren, và Sun (2015), Zagoruyko và Komodakis (2016) và được sử dụng trong Madry et al. (2017) cho huấn luyện đối kháng. Thứ hai, kiến trúc PreActResnet18 là một biến thể của kiến trúc dư của He et al. (2015). Chúng tôi sử dụng bộ tối ưu hóa SGD với momentum trong các thí nghiệm của chúng tôi. Chúng tôi chạy các thí nghiệm trong 200 epoch với tốc độ học ban đầu là 0.1 và nó được ủ bằng hệ số 0.1 tại epoch 100 và 150. Chúng tôi sử dụng kích thước batch 64 cho tất cả các thí nghiệm.

Chúng tôi sử dụng ba bộ dữ liệu chuẩn (CIFAR10, CIFAR100 và SVHN), thường được sử dụng trong văn học tính mạnh mẽ đối kháng (Croce & Hein, 2020; Madry et al., 2017). Cả bộ dữ liệu CIFAR-10 và CIFAR-100 đều bao gồm 60000 hình ảnh màu mỗi hình kích thước 32×32, được chia giữa 50K hình huấn luyện và 10K hình kiểm tra. Bộ dữ liệu CIFAR-10 có mười lớp, bao gồm hình ảnh của xe hơi, ngựa, máy bay và hươu. Bộ dữ liệu CIFAR-100 có một trăm lớp được nhóm thành 20 siêu lớp như con người, cây, xe cộ, v.v. Bộ dữ liệu SVHN bao gồm 73257 mẫu huấn luyện và 26032 mẫu kiểm tra mỗi mẫu kích thước 32×32. Mỗi ví dụ là một hình ảnh cận cảnh của một số nhà (mười lớp là các chữ số từ 0-9).

Tiền xử lý Dữ liệu và Siêu tham số: Việc tăng cường dữ liệu và tiền xử lý hoàn toàn giống như trong Madry et al. (2017). Cụ thể, chúng tôi sử dụng cắt ngẫu nhiên và lật ngang cho CIFAR10 và CIFAR100. Đối với SVHN, chúng tôi sử dụng cắt ngẫu nhiên. Chúng tôi sử dụng chuẩn hóa từng hình ảnh cho tiền xử lý. Đối với huấn luyện đối kháng, chúng tôi tạo ra các ví dụ đối kháng sử dụng một đối thủ PGD sử dụng gradient descent dự án l∞ với 7 bước có kích thước 2, và ε₁=8. Đối với cuộc tấn công đối kháng, chúng tôi sử dụng một đối thủ FGSM với ε₁=8 và một đối thủ PGD với 7 bước và 20 bước có kích thước 2 và ε₁=8.

Trong các thí nghiệm Huấn luyện Đối kháng Nội suy, để tạo ra các ví dụ đối kháng, chúng tôi sử dụng PGD với cùng các siêu tham số như được mô tả ở trên. Để thực hiện nội suy, chúng tôi sử dụng Manifold Mixup với α=2.0 như được đề xuất trong Verma et al. (2019) hoặc Mixup với alpha=1.0 như được đề xuất trong Zhang et al. (2017). Đối với Manifold Mixup, chúng tôi thực hiện nội suy tại một lớp được chọn ngẫu nhiên từ lớp đầu vào, đầu ra của resblock đầu tiên hoặc đầu ra của resblock thứ hai, như được khuyến nghị trong Verma et al. (2019).

Kết quả: Kết quả cho các bộ dữ liệu CIFAR10, CIFAR100, SVHN được trình bày trong Bảng 2-3, 4-5, 6-7, tương ứng. Chúng tôi quan sát rằng IAT liên tục cải thiện lỗi kiểm tra chuẩn tương đối so với các mô hình chỉ sử dụng huấn luyện đối kháng, trong khi duy trì tính mạnh mẽ đối kháng ở cùng mức. Ví dụ, trong Bảng 2, chúng tôi quan sát rằng mô hình cơ sở (không có huấn luyện đối kháng) có lỗi kiểm tra chuẩn 4.43% trong khi huấn luyện đối kháng PGD tăng lỗi kiểm tra chuẩn lên 12.32%: một sự gia tăng tương đối 178% trong lỗi kiểm tra chuẩn. Với Huấn luyện Đối kháng Nội suy, lỗi kiểm tra chuẩn được giảm xuống 6.45%, một sự gia tăng tương đối chỉ 45% trong lỗi kiểm tra chuẩn so với cơ sở, trong khi mức độ tính mạnh mẽ đối kháng vẫn gần như không thay đổi, qua các loại cuộc tấn công đối kháng khác nhau. Chúng tôi cũng xem xét sử dụng kỹ thuật dừng sớm (Rice, Wong, & Kolter, 2020) để ngăn chặn overfitting mạnh mẽ. Chúng tôi thấy rằng dừng sớm giảm lỗi kiểm tra PGD (20 bước) của mô hình được huấn luyện với IAT trên CIFAR-10 từ 61.62% xuống 58.58%, mặc dù, với một sự gia tăng nhẹ của lỗi kiểm tra chuẩn từ 10.12% lên 10.62%. Do đó, dừng sớm có thể được xem xét bổ sung cho phương pháp của chúng tôi và có thể được sử dụng để cải thiện thêm tính mạnh mẽ đối kháng của các mô hình được huấn luyện IAT. Chúng tôi cũng chạy với cuộc tấn công Carlini-Wagner thách thức (Carlini & Wagner, 2016) và AutoAttack dựa trên ensemble Croce & Hein, 2020 trên CIFAR-10. Với cuộc tấn công Carlini-Wagner, mạng được huấn luyện IAT có tính mạnh mẽ được cải thiện liên tục, với lỗi kiểm tra 28.61% trên cuộc tấn công không nhắm mục tiêu và 29.53% trên cuộc tấn công có nhắm mục tiêu. Ngược lại, mô hình cơ sở có lỗi kiểm tra cao hơn 34.42% trên cuộc tấn công không nhắm mục tiêu và 33.37% trên cuộc tấn công có nhắm mục tiêu. Đối với phiên bản L-inf của AutoAttack, IAT hơi tăng lỗi kiểm tra từ 62.56% lên 66.35%. Đối với phiên bản L2 của cuộc tấn công, IAT cải thiện lỗi kiểm tra từ 43.90% xuống 41.57%. Chúng tôi cũng so sánh IAT với Huấn luyện Đối kháng dựa trên Phân tán Đặc trưng (FSAT) (Zhang & Wang, 2019) và TRADES (Zhang, Yu, Jiao, Xing, Ghaoui, & Jordan, 2019b). Chúng tôi thấy rằng FSAT và TRADES có lỗi kiểm tra chuẩn cao hơn lần lượt là 10.49% và 27.5%, so với IAT, có lỗi kiểm tra chuẩn 10.12%. Chúng tôi cũng so sánh tính mạnh mẽ của các mô hình được huấn luyện IAT, FSAT và TRADES đối với PGD (7 bước) và AutoAttack. Chúng tôi thấy rằng mô hình được huấn luyện IAT có tính mạnh mẽ thấp hơn đối với cuộc tấn công PGD (7 bước) so với FSAT và TRADES, với IAT có lỗi PGD (7 bước) là 55.43%, FSAT có lỗi PGD (7 bước) là 49.48%, và TRADES có lỗi PGD (7 bước) là 55.02%. Chống lại AutoAttack, IAT cải thiện tính mạnh mẽ so với FSAT, với IAT có lỗi kiểm tra mạnh mẽ 66.35% và FSAT có lỗi kiểm tra mạnh mẽ 67.34%. Mô hình được huấn luyện TRADES có lỗi kiểm tra mạnh mẽ 59.65% đối với AutoAttack. Nhìn chung, IAT cải thiện lỗi kiểm tra chuẩn trong khi vẫn đạt được tính mạnh mẽ so sánh với các phương pháp hiện đại, như FSAT và TRADES.

5.2. Các cuộc tấn công chuyển giao

Như một kiểm tra đúng đắn rằng Huấn luyện Đối kháng Nội suy không gặp vấn đề che khuất gradient (Athalye et al., 2018), chúng tôi thực hiện đánh giá cuộc tấn công chuyển giao trên bộ dữ liệu CIFAR-10 sử dụng kiến trúc PreActResNet18. Trong loại đánh giá này, mô hình được sử dụng để tạo ra các ví dụ đối kháng khác với mô hình được sử dụng để đánh giá cuộc tấn công. Vì các cuộc tấn công chuyển giao này không sử dụng các tham số của mô hình mục tiêu để tính toán ví dụ đối kháng, chúng được xem là các cuộc tấn công hộp đen. Trong đánh giá của chúng tôi (Bảng 8) chúng tôi thấy rằng các cuộc tấn công chuyển giao hộp đen luôn yếu hơn đáng kể so với các cuộc tấn công hộp trắng, do đó Huấn luyện Đối kháng Nội suy không gặp vấn đề che khuất gradient (Athalye et al., 2018). Ngoài ra, trong Bảng 9, chúng tôi quan sát rằng việc tăng ε₁ dẫn đến 100% thành công của cuộc tấn công, cung cấp bằng chứng bổ sung rằng Huấn luyện Đối kháng Nội suy không gặp vấn đề che khuất gradient (Athalye et al., 2018).

5.3. Thay đổi số lần lặp và ε₁ cho các cuộc tấn công lặp

Để nghiên cứu thêm về tính mạnh mẽ của Huấn luyện Đối kháng Nội suy, chúng tôi nghiên cứu tác động của việc thay đổi số lần lặp cuộc tấn công và phạm vi của cuộc tấn công đối kháng ε₁. Một số biện pháp phòng thủ đối kháng (Engstrom, Ilyas, & Athalye, 2018) đã được tìm thấy có tính dễ bị tổn thương tăng lên khi tiếp xúc với các cuộc tấn công có số lần lặp lớn. Chúng tôi nghiên cứu điều này (Bảng 10) và thấy rằng cả huấn luyện đối kháng và Huấn luyện Đối kháng Nội suy đều có tính mạnh mẽ chỉ giảm nhẹ với số bước tăng lên, với gần như không có sự khác biệt giữa cuộc tấn công 100 bước và cuộc tấn công 1000 bước. Ngoài ra chúng tôi thay đổi ε₁ để nghiên cứu liệu Huấn luyện Đối kháng Nội suy có dễ bị tổn thương hơn hay ít hơn đối với các cuộc tấn công với ε₁ khác với những gì mô hình được huấn luyện. Chúng tôi thấy rằng Huấn luyện Đối kháng Nội suy hơi mạnh mẽ hơn khi sử dụng ε₁ nhỏ hơn và hơi ít mạnh mẽ hơn khi sử dụng ε₁ lớn hơn (Bảng 9).

5.4. Phân tích trọng số của các thành phần mất mát

IAT giới thiệu một siêu tham số cho việc cân bằng trọng số của mất mát sạch và mất mát đối kháng, theo mặc định có thể được đặt thành trọng số đều của cả hai thành phần. Chúng tôi thấy rằng khi chúng tôi cân bằng mất mát sạch nhiều hơn, chúng tôi có độ chính xác kiểm tra sạch được cải thiện và khi chúng tôi cân bằng mất mát đối kháng nhiều hơn, chúng tôi có tính mạnh mẽ đối kháng được cải thiện. Các kết quả này được hiển thị trong Bảng 11.

6. Phân tích lý thuyết

Trong phần này, chúng tôi thiết lập các tính chất toán học của IAT với Mixup. Chúng tôi bắt đầu trong Phần 6.1 với ký hiệu bổ sung và sau đó phân tích tác động của IAT lên tính mạnh mẽ đối kháng trong Phần 6.2. Hơn nữa, chúng tôi thảo luận về tác động của IAT lên tổng quát hóa trong Phần 6.3 bằng cách chỉ ra cách ICT có thể giảm overfitting và dẫn đến các hành vi tổng quát hóa tốt hơn. Các chứng minh của tất cả các định lý và mệnh đề được trình bày trong Phụ lục B sử dụng một bổ đề chính được chứng minh trong Phụ lục A.

6.1. Ký hiệu

Để trình bày phân tích của chúng tôi một cách súc tích, chúng tôi giới thiệu ký hiệu bổ sung như sau. Mất mát mixup chuẩn Lc có thể được viết như

Lc = (1/n²) ∑ᵢ,ⱼ₌₁ⁿ E_λ∼Dλ ℓ(fθ(x̃ᵢ,ⱼ(λ)), ỹᵢ,ⱼ(λ)), (4)

trong đó x̃ᵢ,ⱼ(λ) = λxᵢ + (1-λ)xⱼ, ỹᵢ,ⱼ(λ) = λyᵢ + (1-λ)yⱼ, và λ∈[0,1]. Ở đây, Dλ đại diện cho phân phối Beta Beta(α,β) với một số siêu tham số α,β > 0. Tương tự, mất mát đối kháng-mixup La được sử dụng trong IAT có thể được định nghĩa bởi

La = (1/n²) ∑ᵢ,ⱼ₌₁ⁿ E_λ∼Dλ ℓ(fθ(x̌ᵢ,ⱼ(λ)), ỹᵢ,ⱼ(λ)), (5)

trong đó δ̂ᵢ = argmax_δᵢ:‖δᵢ‖ρ≤ε₁ ℓ(fθ(xᵢ+δᵢ),yᵢ), x̂ᵢ = xᵢ+δ̂ᵢ, và x̌ᵢ,ⱼ(λ) = λx̂ᵢ + (1-λ)x̂ⱼ. Sử dụng hai loại mất mát này, toàn bộ mất mát IAT được định nghĩa bởi

L = (Lc + La)/2. (6)

Trong phần này, chúng tôi tập trung vào họ hàm mất mát sau: ℓ(q,y) = h(q) - yq, cho một hàm h khả vi hai lần nào đó. Họ hàm mất mát ℓ này bao gồm nhiều mất mát thường được sử dụng, bao gồm mất mát logistic và mất mát cross-entropy.

Cho một tập F các hàm x ↦ f(x), độ phức tạp Rademacher (Bartlett & Mendelson, 2002; Mohri, Rostamizadeh, & Talwalkar, 2012) của tập ℓ∘F = {(x,y) ↦ ℓ(f(x;θ),y) : f∈F} có thể được định nghĩa bởi

Rn(ℓ∘F) := E_S,σ[sup_f∈F (1/n) ∑ᵢ₌₁ⁿ σᵢℓ(f(xᵢ),yᵢ)],

trong đó S = ((xᵢ,yᵢ))ⁿᵢ₌₁. Ở đây, σ₁,...,σn là các biến ngẫu nhiên độc lập đồng nhất lấy giá trị trong {-1,1}. Chúng tôi ký hiệu bằng D̃λ hỗn hợp đồng nhất của hai phân phối Beta, (α/(α+β))Beta(α+1,β) + (β/(α+β))Beta(β+1,α). Chúng tôi để Dx̂ là phân phối thực nghiệm của các mẫu huấn luyện bị nhiễu loạn (x̂₁,...,x̂n), và định nghĩa Dx là phân phối thực nghiệm của các mẫu huấn luyện (x₁,...,xn). Đặt cos(a,b) là độ tương tự cosine của hai vectơ a và b.

6.2. Tác động của IAT lên tính mạnh mẽ

Trong tiểu mục này, chúng tôi nghiên cứu cách thêm Mixup vào huấn luyện đối kháng ảnh hưởng đến tính mạnh mẽ của mô hình bằng cách phân tích mất mát đối kháng-mixup La được sử dụng trong IAT. Tiểu mục này tập trung vào mất mát cross-entropy nhị phân ℓ bằng cách đặt h(z) = log(1+eᶻ) và y∈{0,1}, trong khi phần tiếp theo xem xét một thiết lập tổng quát hơn. Chúng tôi định nghĩa một tập Θ của các vectơ tham số bởi

Θ = {θ∈Rᵈ : yᵢ(fθ(xᵢ+δ̂ᵢ)) + (yᵢ-1)(fθ(xᵢ+δ̂ᵢ)) ≥ 0 cho tất cả i=1,...,n}.

Lưu ý rằng tập Θ này chứa tập của tất cả các vectơ tham số với phân loại đúng của các điểm huấn luyện (trước Mixup) như
Θ ⊇ {θ∈Rᵈ : 1{fθ(xᵢ+δ̂ᵢ) ≥ 0} = yᵢ cho tất cả i=1,...,n}.

Do đó, điều kiện của θ∈Θ được thỏa mãn khi mạng nơ-ron sâu phân loại tất cả các nhãn một cách chính xác cho dữ liệu huấn luyện với nhiễu loạn trước Mixup. Vì lỗi huấn luyện (mặc dù không phải mất mát huấn luyện) trở thành không trong thời gian hữu hạn trong nhiều trường hợp thực tế, điều kiện của θ∈Θ được thỏa mãn trong thời gian hữu hạn trong nhiều trường hợp thực tế. Theo đó, chúng tôi nghiên cứu tác động của IAT lên tính mạnh mẽ trong chế độ của θ∈Θ.

Định lý 1 chỉ ra rằng mất mát đối kháng-mixup La là gần đúng một cận trên của mất mát đối kháng với các nhiễu loạn đối kháng của xᵢ ↦ xᵢ+δ̂ᵢ+δᵢᵐⁱˣ trong đó ‖δᵢ‖ρ≤ε₁ là nhiễu loạn đối kháng chuẩn và ‖δᵢ‖₂≤ε₁ᵐⁱˣ là nhiễu loạn bổ sung không chuẩn do IAT. Nói cách khác, IAT là gần đúng tối thiểu hóa cận trên của mất mát đối kháng với nhiễu loạn đối kháng bổ sung ‖δᵢ‖₂≤ε₁ᵐⁱˣ với bán kính phụ thuộc dữ liệu ε₁ᵐⁱˣ cho mỗi i∈{1,...,n}. Do đó, thêm Mixup vào huấn luyện đối kháng (tức là IAT) không làm giảm tác động của huấn luyện đối kháng gốc lên tính mạnh mẽ gần đúng (trong đó lỗi gần đúng theo thứ tự (1-λ̃)³ như thảo luận dưới đây). Điều này không tầm thường vì, không có Định lý 1, không chắc chắn liệu việc thêm Mixup có làm giảm tác động của huấn luyện đối kháng về mặt tính mạnh mẽ hay không. Hơn nữa, Định lý 1 chỉ ra rằng IAT cải thiện thêm tính mạnh mẽ tùy thuộc vào các giá trị của bán kính phụ thuộc dữ liệu ε₁ᵐⁱˣ khi so sánh với huấn luyện đối kháng chuẩn không có Mixup. Đây là những điều phù hợp với các quan sát thực nghiệm của chúng tôi.

Định lý 1. Đặt θ∈Θ là một điểm sao cho ∇fθ(xᵢ+δ̂ᵢ) và ∇²fθ(xᵢ+δ̂ᵢ) tồn tại cho tất cả i=1,...,n. Giả sử rằng fθ(xᵢ+δ̂ᵢ) = ∇fθ(xᵢ+δ̂ᵢ)ᵀ(xᵢ+δ̂ᵢ) và ∇²fθ(xᵢ+δ̂ᵢ) = 0 cho tất cả i∈{1,...,n}. Giả sử rằng E_r∼Dx̂[r] = 0 và ‖xᵢ+δ̂ᵢ‖₂ ≥ cx√d cho tất cả i∈{1,...,n}. Khi đó, tồn tại một cặp (φ,φ̄) sao cho lim_{z→0} φ(z), lim_{z→0} φ̄(z) = 0, và

La ≥ (1/n) ∑ᵢ₌₁ⁿ max_{‖δᵢᵐⁱˣ‖₂≤ε₁ᵐⁱˣ} ℓ(fθ(xᵢ+δ̂ᵢ+δᵢᵐⁱˣ),yᵢ) + E₁ + E₂,

trong đó ε₁ᵐⁱˣ := Rᵢcx E_λ[(1-λ)]/√d, Rᵢ := |cos(∇fθ(xᵢ+δ̂ᵢ),xᵢ+δ̂ᵢ)|,
E₁ := E_{λ̃∼D̃λ}[(1-λ̃)²φ(1-λ̃)], và E₂ := E_{λ̃∼D̃λ}[(1-λ̃)]²φ̄(E_{λ̃∼D̃λ}[(1-λ̃)]).

Giả định của fθ(z) = ∇fθ(z)ᵀz và ∇²fθ(z) = 0 trong Định lý 1 được thỏa mãn, ví dụ, bởi các mô hình tuyến tính cũng như mạng nơ-ron sâu với hàm kích hoạt ReLU và max-pooling. Trong Định lý 1, các thành phần lỗi gần đúng E₁ và E₂ theo thứ tự (1-λ̃)³ (vì lim_{z→0} φ(z), lim_{z→0} φ̄(z) = 0), và λ̃ có xu hướng gần một vì λ̃∼D̃λ trong đó D̃λ là hỗn hợp đồng nhất của hai phân phối Beta, (α/(α+β))Beta(α+1,β) + (β/(α+β))Beta(β+1,α), cho phân phối của Dλ = Beta(α,β) cho hệ số Mixup λ. Ví dụ, nếu thuật toán IAT sử dụng phân phối Beta Dλ = Beta(0.5,0.5) cho Mixup, thì chúng ta có D̃λ = Beta(1.5,0.5), mà λ̃∼D̃λ có xu hướng gần một như được minh họa trong Hình 2. Do đó, các thành phần lỗi gần đúng E₁ và E₂ có xu hướng gần không.

6.3. Tác động của IAT lên tổng quát hóa

Trong tiểu mục này, chúng tôi phân tích toán học tác động của IAT lên các tính chất tổng quát hóa. Chúng tôi bắt đầu trong Phần 6.3.1 với thiết lập tổng quát với h và fθ tùy ý, và chứng minh một cận tổng quát hóa cho mất mát IAT. Trong Phần 6.3.2, chúng tôi sau đó đưa ra các giả định về h và fθ và nghiên cứu các tác động chính quy hóa của IAT.

6.3.1. Các cận tổng quát hóa

Định lý sau trình bày một cận tổng quát hóa cho mất mát IAT (Lc+La)/2 — cận trên của sự khác biệt giữa lỗi kỳ vọng trên dữ liệu chưa thấy và mất mát IAT, E_{x,y}[ℓ(f(x),y)] - (Lc+La)/2:

Định lý 2. Đặt ρ≥1 là một số thực và F là một tập các ánh xạ x ↦ f(x). Giả sử rằng hàm |ℓ(q,y) - ℓ(q',y)| ≤ τ cho bất kỳ q,q'∈{f(x+δ) : f∈F, x∈X, ‖δ‖ρ≤ε₁} và y∈Y. Khi đó, với bất kỳ δ > 0, với xác suất ít nhất 1-δ trên một rút ngẫu nhiên i.i.d. của n mẫu i.i.d. ((xᵢ,yᵢ))ⁿᵢ₌₁, điều sau đây đúng: cho tất cả các ánh xạ f∈F, tồn tại một hàm φ:R→R sao cho

E_{x,y}[ℓ(f(x),y)] - (Lc+La)/2 (7)
≤ 2Rn(ℓ∘F) + 2τ√(ln(1/δ)/(2n)) - Q(f)/2
- E_X[∑³_{k=1} (Gk(X̂,Dx̂) + Gk(X,Dx))/2] - E₁,

trong đó lim_{q→0} φ(q) = 0, X := (x₁,...,xn), X̂ := (x̂₁,...,x̂n),
Q(f) := (1/n)E_S[∑ⁿᵢ₌₁(max_{δᵢ:‖δᵢ‖ρ≤ε₁} ℓ(f(xᵢ+δᵢ),yᵢ) - ℓ(f(xᵢ),yᵢ))] ≥ 0,

G₁(X̂,Dx̂) := (E_λ∼D̃λ[1-λ]/n) ∑ⁿᵢ₌₁ (h'(f(x̂ᵢ)) - yᵢ)∇f(x̂ᵢ)ᵀE_{r∼Dx̂}[r-x̂ᵢ],
G₂(X̂,Dx̂) := (E_λ∼D̃λ[(1-λ)²]/(2n)) ∑ⁿᵢ₌₁ h''(f(x̂ᵢ))∇f(x̂ᵢ)ᵀE_{r∼Dx̂}[(r-x̂ᵢ)(r-x̂ᵢ)ᵀ]∇f(x̂ᵢ),
G₃(X̂,Dx̂) := (E_λ∼D̃λ[(1-λ)²]/(2n)) ∑ⁿᵢ₌₁ (h'(f(x̂ᵢ)) - yᵢ)E_{r∼Dx̂}[(r-x̂ᵢ)∇²f(x̂ᵢ)(r-x̂ᵢ)ᵀ].

Để hiểu cận tổng quát hóa này hơn nữa, chúng tôi bây giờ so sánh nó với một cận tổng quát hóa cho IAT mà không sử dụng Mixup trên các thành phần Lc và La. IAT không có Mixup là huấn luyện đối kháng cùng với huấn luyện chuẩn, tối thiểu hóa mất mát của L' = (L'c + L'a)/2,

trong đó
L'c = (1/n) ∑ⁿᵢ₌₁ ℓ(fθ(xᵢ),yᵢ), và (8)
L'a = (1/n) ∑ⁿᵢ₌₁ ℓ(fθ(xᵢ+δ̂ᵢ),yᵢ). (9)

Định lý sau trình bày một cận tổng quát hóa cho IAT không có Mixup trên các thành phần Lc và La:

Định lý 3. Đặt ρ≥1 là một số thực và F là một tập các ánh xạ x ↦ f(x). Giả sử rằng hàm |ℓ(q,y) - ℓ(q',y)| ≤ τ cho bất kỳ q,q'∈{f(x+δ) : f∈F, x∈X, ‖δ‖ρ≤ε₁} và y∈Y. Khi đó, với bất kỳ δ > 0, với xác suất ít nhất 1-δ trên một rút ngẫu nhiên i.i.d. của n mẫu i.i.d. ((xᵢ,yᵢ))ⁿᵢ₌₁, điều sau đây đúng: cho tất cả các ánh xạ f∈F,

E_{x,y}[ℓ(f(x),y)] - (L'c + L'a)/2 ≤ 2Rn(ℓ∘F) + 2τ√(ln(1/δ)/(2n)) - Q(f)/2. (10)

Bằng cách so sánh Định lý 2 và 3, chúng ta có thể thấy rằng lợi ích của IAT với Mixup đến từ hai cơ chế về mặt tổng quát hóa. Cơ chế đầu tiên dựa trên thành phần của E_X[∑³_{k=1} (Gk(X̂,Dx̂) + Gk(X,Dx))/2] + E₁. Nếu thành phần này dương, thì IAT với Mixup có cận tổng quát hóa tốt hơn so với IAT không có Mixup (nếu chúng ta giả sử rằng thành phần độ phức tạp Rademacher Rn(ℓ∘F) giống nhau cho cả hai phương pháp). Cơ chế thứ hai dựa trên thành phần độ phức tạp mô hình Rn(ℓ∘F). Vì thành phần độ phức tạp mô hình bị ràng buộc bởi các chuẩn của trọng số đã huấn luyện (ví dụ, Bartlett, Foster, & Telgarsky, 2017), thành phần này khác nhau cho các sơ đồ huấn luyện khác nhau — IAT với Mixup và IAT không có Mixup. Theo đó, chúng tôi nghiên cứu các tác động chính quy hóa của IAT lên các chuẩn của trọng số trong tiểu mục tiếp theo.

6.3.2. Các tác động chính quy hóa

Các cận tổng quát hóa trong tiểu mục trước chứa thành phần độ phức tạp mô hình, được kiểm soát bởi các chuẩn của trọng số trong các nghiên cứu trước (ví dụ, Bartlett et al., 2017). Theo đó, chúng tôi bây giờ thảo luận về các tác động chính quy hóa của IAT lên các chuẩn của trọng số. Tiểu mục này xem xét các mô hình mà fθ(xᵢ+δ̂ᵢ) = ∇fθ(xᵢ+δ̂ᵢ)ᵀ(xᵢ+δ̂ᵢ) và ∇²fθ(xᵢ+δ̂ᵢ) = 0 cho i=1,...,n. Điều này được thỏa mãn bởi các mô hình tuyến tính cũng như mạng nơ-ron sâu với các hàm kích hoạt ReLU và max-pooling. Chúng tôi để y∈{0,1} và h(z) = log(1+eᶻ), làm cho hàm mất mát ℓ đại diện cho mất mát cross-entropy nhị phân. Định nghĩa g là hàm logic như g(z) = eᶻ/(1+eᶻ). Định nghĩa này ngụ ý rằng g(z)∈(0,1) cho z∈R.

Định lý sau chỉ ra rằng thành phần IAT có tác động chính quy hóa bổ sung lên ‖∇fθ(x̂ᵢ)‖₂ và ‖∇fθ(x̂ᵢ)‖₂ E_r[(r-x̂ᵢ)(r-x̂ᵢ)ᵀ]. Định lý này giải thích các tác động chính quy hóa bổ sung của thành phần IAT lên chuẩn của trọng số, vì ∇fθ(x̂ᵢ) = w cho các mô hình tuyến tính và ∇fθ(x̂ᵢ) = ‖W_H σ̇_H W_{H-1} σ̇_{H-1}...σ̇₁W₁‖ cho mạng nơ-ron sâu với ReLU và max-pooling.

Định lý 4. Giả sử rằng fθ(xᵢ+δ̂ᵢ) = ∇fθ(xᵢ+δ̂ᵢ)ᵀ(xᵢ+δ̂ᵢ) và ∇²fθ(xᵢ+δ̂ᵢ) = 0. Khi đó, tồn tại một hàm φ:R→R sao cho

La = (1/n) ∑ⁿᵢ₌₁ ℓ(fθ(x̂ᵢ),yᵢ) + C₁‖∇fθ(x̂ᵢ)‖₂ + C₂‖∇fθ(x̂ᵢ)‖₂ E_r[(r-x̂ᵢ)(r-x̂ᵢ)ᵀ] + E₁, (11)

trong đó lim_{q→0} φ(q) = 0 và

C₁ = (E_λ[(1-λ)]/n) ∑ⁿᵢ₌₁ (yᵢ - g(fθ(x̂ᵢ)))‖E_{r∼Dx̂}[r-x̂ᵢ]‖₂ cos(∇fθ(x̂ᵢ), E_{r∼Dx̂}[r-x̂ᵢ]),
C₂ = (E_λ[(1-λ)²]/(2n)) ∑ⁿᵢ₌₁ |g(fθ(x̂ᵢ))(1-g(fθ(x̂ᵢ)))|.

Trong Định lý 4, C₂ luôn dương nghiêm ngặt vì g(z)∈(0,1) cho tất cả z∈R. Trong khi C₁ có thể âm nói chung, mệnh đề sau chỉ ra rằng C₁ cũng sẽ không âm trong giai đoạn sau của huấn luyện IAT:

Mệnh đề 1. Nếu θ∈Θ', thì C₁≥0 trong đó Θ' = {θ∈Rᵈ : yᵢ(fθ(xᵢ+δᵢ(θ)) - ζᵢ) + (yᵢ-1)(fθ(xᵢ+δᵢ(θ)) - ζᵢ) ≥ 0 cho tất cả i=1,...,n}, và ζᵢ = ∇fθ(xᵢ+δᵢ(θ))ᵀE_{r∼Dx̂}[r].

Ở đây, chúng ta có rằng
Θ' ⊇ {θ∈Rᵈ : 1̂{fθ(xᵢ+δ̂ᵢ) - ζᵢ ≥ 0} = yᵢ cho tất cả i=1,...,n}.

Do đó, điều kiện của θ∈Θ' được thỏa mãn khi mô hình phân loại tất cả các nhãn một cách chính xác với lề ζᵢ cho các nhiễu loạn đối kháng. Vì lỗi huấn luyện (mặc dù không phải mất mát huấn luyện) trở thành không trong thời gian hữu hạn trong nhiều trường hợp thực tế và lề tăng lên thông qua bias ngầm của gradient descent sau đó Lyu và Li (2020), điều kiện của θ∈Θ' được thỏa mãn trong thời gian hữu hạn trong nhiều trường hợp thực tế. Định lý 4 và Mệnh đề 1 cùng nhau chỉ ra rằng IAT có thể giảm các chuẩn của trọng số khi so sánh với huấn luyện đối kháng.

Zhang, Deng, và Kawaguchi (2021) chỉ ra rằng mất mát mixup chuẩn Lc cũng có tác động chính quy hóa lên chuẩn của trọng số và do đó góp phần giảm độ phức tạp mô hình. Do đó, kết quả của chúng tôi cùng với nghiên cứu trước (Zhang et al., 2021) chỉ ra lợi ích của IAT về mặt giảm chuẩn của trọng số để kiểm soát độ phức tạp mô hình. Vì nghiên cứu gần đây chỉ xem xét Mixup chuẩn mà không có huấn luyện đối kháng, kết quả của chúng tôi bổ sung cho nghiên cứu gần đây để hiểu IAT.

Để xác thực dự đoán lý thuyết này, chúng tôi tính toán các chuẩn của trọng số cho một mạng hoàn toàn kết nối 6 lớp với 512 đơn vị ẩn được huấn luyện trên Fashion-MNIST và báo cáo kết quả trong Hình 3. Một mặt, huấn luyện đối kháng tăng các chuẩn Frobenius trên tất cả các lớp và tăng chuẩn phổ của đa số các lớp. Mặt khác, IAT tránh hoặc giảm thiểu những sự gia tăng này trong các chuẩn của trọng số. Điều này phù hợp với các dự đoán lý thuyết của chúng tôi và cho thấy rằng IAT học các bộ phân loại có độ phức tạp thấp hơn so với huấn luyện đối kháng thông thường.

Để hiểu thêm tại sao huấn luyện đối kháng có xu hướng tăng các chuẩn, xem xét trường hợp hồi quy tuyến tính:
L(θ) = (1/2)‖Xw - Y‖²_F.

Khi đó, chúng ta có
∇L(θ) = X^T(Xw - Y).

Do đó, mỗi bước của gradient descent (ngẫu nhiên) chỉ thêm một vectơ nào đó trong không gian cột của X^T vào w như
w_{t+1} = w_t + v_t trong đó v_t ∈ Col(X^T).

Ở đây, các nghiệm của hồi quy tuyến tính là bất kỳ w nào sao cho
w = X†Y + v_⊥ trong đó v_⊥ ∈ Null(X).

Do đó, gradient descent (ngẫu nhiên) không thêm bất kỳ phần tử không cần thiết nào vào w, ngầm tối thiểu hóa chuẩn của trọng số. Theo đó, nếu chúng ta khởi tạo w như w₀ ∈ Col(X^T), thì chúng ta đạt được nghiệm chuẩn tối thiểu ngầm thông qua gradient descent (ngẫu nhiên). Trong bối cảnh này, chúng ta có thể dễ dàng thấy rằng bằng cách tiến hành huấn luyện đối kháng, chúng ta thêm các vectơ v_⊥ ∈ Null(X), phá vỡ bias ngầm và tăng chuẩn của w. Tương tự, trong trường hợp mạng nơ-ron sâu, gradient descent (ngẫu nhiên) có bias ngầm hạn chế không gian tìm kiếm của w và do đó có xu hướng tối thiểu hóa chuẩn mà không có các phần tử không cần thiết (Lyu & Li, 2020; Moroshko, Gunasekar, Woodworth, Lee, Srebro, & Soudry, 2020; Woodworth et al., 2020). Do đó, tương tự như trường hợp các mô hình tuyến tính, huấn luyện đối kháng thêm các phần tử bổ sung thông qua nhiễu loạn và có xu hướng tăng chuẩn của trọng số. Kết quả của chúng tôi chỉ ra rằng chúng ta có thể tối thiểu hóa tác động này thông qua các tác động chính quy hóa bổ sung của IAT để giảm overfitting cho các hành vi tổng quát hóa tốt hơn.

7. Kết luận

Tính mạnh mẽ đối với các ví dụ đối kháng là thiết yếu để đảm bảo rằng các hệ thống học máy an toàn và đáng tin cậy. Tuy nhiên biện pháp phòng thủ hiệu quả nhất, huấn luyện đối kháng, có tác động làm tổn hại hiệu suất trên dữ liệu không bị nhiễu loạn. Điều này có ý nghĩa cả về lý thuyết và thực tế. Vì các nhiễu loạn đối kháng không thể nhận thức được (hoặc hầu như không thể nhận thức được) đối với con người và con người có thể tổng quát hóa cực kỳ tốt, thật đáng ngạc nhiên rằng huấn luyện đối kháng làm giảm khả năng của mô hình để hoạt động tốt trên dữ liệu kiểm tra không bị nhiễu loạn. Sự suy giảm trong tổng quát hóa này cực kỳ cấp bách đối với các nhà thực hành mà hệ thống của họ bị đe dọa bởi các cuộc tấn công đối kháng. Với các kỹ thuật hiện tại, những người muốn triển khai các hệ thống học máy cần phải xem xét một sự đánh đổi nghiêm trọng giữa hiệu suất trên dữ liệu không bị nhiễu loạn và tính mạnh mẽ đối với các ví dụ đối kháng, có thể có nghĩa là bảo mật và độ tin cậy sẽ gặp khó khăn trong các ứng dụng quan trọng. Công việc của chúng tôi đã giải quyết cả hai vấn đề này. Chúng tôi đề xuất giải quyết điều này bằng cách tăng cường huấn luyện đối kháng với huấn luyện dựa trên nội suy (Verma et al., 2019; Zhang et al., 2017). Chúng tôi thấy rằng điều này cải thiện đáng kể tổng quát hóa trên dữ liệu không bị nhiễu loạn trong khi bảo tồn tính mạnh mẽ đối kháng. Phân tích của chúng tôi chỉ ra tại sao và cách phương pháp đề xuất có thể cải thiện tổng quát hóa và bảo tồn tính mạnh mẽ đối kháng khi so sánh với huấn luyện đối kháng chuẩn.
