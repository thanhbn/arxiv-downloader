# 2210.04311.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/adversarial/2210.04311.pdf
# File size: 2477310 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pruning Adversarially Robust Neural Networks
without Adversarial Examples
Tong Jian1,y, Zifeng Wang1,y, Yanzhi Wang2, Jennifer Dy1, Stratis Ioannidis1
Department of Electrical and Computer Engineering
Northeastern University
1fjian, zifengwang, jdy, ioannidis g@ece.neu.edu
2yanz.wang@northeastern.edu
Abstract —Adversarial pruning compresses models while pre-
serving robustness. Current methods require access to adversarial
examples during pruning. This signiﬁcantly hampers training
efﬁciency. Moreover, as new adversarial attacks and training
methods develop at a rapid rate, adversarial pruning methods
need to be modiﬁed accordingly to keep up. In this work, we
propose a novel framework to prune a previously trained robust
neural network while maintaining adversarial robustness, without
further generating adversarial examples. We leverage concurrent
self-distillation and pruning to preserve knowledge in the original
model as well as regularizing the pruned model via the Hilbert-
Schmidt Information Bottleneck. We comprehensively evaluate
our proposed framework and show its superior performance in
terms of both adversarial robustness and efﬁciency when pruning
architectures trained on the MNIST, CIFAR-10, and CIFAR-100
datasets against ﬁve state-of-the-art attacks. Code is available at
https://github.com/neu-spiral/PwoA/.
Index Terms —Adversarial Robustness, Adversarial Pruning,
Self-distillation, HSIC Bottleneck
I. I NTRODUCTION
The vulnerability of deep neural networks (DNNs) to ad-
versarial attacks has been the subject of extensive research
recently [1]–[5]. Such attacks are intentionally crafted to
mislead DNNs towards incorrect predictions, e.g., by adding
delicately but visually imperceptible perturbations to original,
natural examples [6]. Adversarial robustness, i.e., the ability
of a trained model to maintain its predictive power despite
such attacks, is an important property for many safety-critical
applications [7]–[9]. The most common and effective way
to attain adversarial robustness is via adversarial training
[10]–[12], i.e., training a model over adversarially generated
examples. Adversarial training has shown reliable robustness
performance against improved attack techniques such as pro-
jected gradient descent (PGD) [3], the Carlini & Wagner attack
(CW) [4] and AutoAttack (AA) [5]. Nevertheless, adversarial
training is computationally expensive [3], [13], usually 3–
30[14] longer than natural training, precisely due to the
additional cost of generating adversarial examples.
As noted by Madry et al. [3], achieving adversarial robust-
ness requires a signiﬁcantly wider and larger architecture than
that for natural accuracy. The large network capacity required
by adversarial training may limit its deployment on resource-
constrained hardware or real-time applications. Weight prun-
yBoth authors contributed equally to this work.
adversarial examplenatural example
adversariallytrained modelResearcher AResearcher B
public releasepruned model(a) Motivation of our PwoAframework(b) Naïve Prune vs. PwoAnaturalexample
Train
PruneFig. 1: (a) A DNN publicly released by researcher A, trained adver-
sarially at a large computational expense, is pruned by Researcher B
and made executable on a resource-constrained device. Using PwoA,
pruning by B is efﬁcient, requiring only access to natural examples.
(b) Taking a pre-trained WRN34-10 pruned on CIFAR-100 as an
example, pruning an adversarially robust model in a na ¨ıve fashion,
without generating any adversarial examples, completely obliterates
robustness against AutoAttack [5] even under a 2pruning ratio.
In contrast, our proposed PwoA framework efﬁciently preserves
robustness for a broad range of pruning ratios, without any access
to adversarially generated examples. To achieve similar robustness,
SOTA adversarial pruning methods require 4–7more training
time (see Figure 3 in Section VI-C).
ing is a prominent compression technique to reduce model
size without notable accuracy degradation [15]–[21]. While
researchers have extensively explored weight pruning, only
a few recent works have studied it jointly with adversarial
robustness. Ye et al. [22], Gui et al. [23], and Sehwag et
al. [24] apply active defense techniques with pruning in their
research. However, these works require access to adversarial
examples during pruning. Pruning is itself a laborious process,
as effective pruning techniques simultaneously ﬁnetune an
existing, pre-trained network; incorporating adversarial exam-
ples to this process signiﬁcantly hampers training efﬁciency.
Moreover, adversarial pruning techniques tailored to speciﬁc
adversarial training methods need to be continually revised as
new methods develop apace.
In this paper, we study how take a dense, adversarially
robust DNN, that has already been trained over adversarial
examples, and prune it without any additional adversarialarXiv:2210.04311v1  [cs.LG]  9 Oct 2022

--- PAGE 2 ---
training . As a motivating example illustrated in Figure 1(a), a
DNN publicly released by researchers or a company, trained
adversarially at a large computational expense, could be sub-
sequently pruned by other researchers to be made executable
on a resource-constrained device, like an FPGA. Using our
method, the latter could be done efﬁciently, without access to
the computational resources required for adversarial pruning.
Restricting pruning to access only natural examples poses a
signiﬁcant challenge. As shown in Figure 1(b), na ¨ıvely pruning
a model without adversarial examples can be catastrophic,
obliterating all robustness against AutoAttack. In contrast, our
PwoA is notably robust under a broad range of pruning rates.
Overall, we make the following contributions:
1) We propose PwoA, an end-to-end framework for pruning
a pre-trained adversarially robust model without gener-
ating adversarial examples, by (a) preserving robustness
from the original model via self-distillation [25]–[27]
and (b) enhancing robustness from natural examples
via Hilbert-Schmidt independence criterion (HSIC) as
a regularizer [28], [29].
2) Our work is the ﬁrst to study how an adversarially pre-
trained model can be efﬁciently pruned without access
to adversarial examples. This is an important, novel
challenge: prior to our study, it was unclear whether this
was even possible. Our approach is generic, and is nei-
ther tailored nor restricted to speciﬁc pre-trained robust
models, architectures, or adversarial training methods.
3) We comprehensively evaluate PwoA on pre-trained ad-
versarially robust models publicly released by other
researchers. In particular, we prune ﬁve publicly avail-
able models that were pre-trained with state-of-the-art
(SOTA) adversarial methods on the MNIST, CIFAR-
10, and CIFAR-100 datasets. Compared to SOTA adver-
sarial pruning methods, PwoA can prune a large frac-
tion of weights while attaining comparable–or better–
adversarial robustness, at a 4 –7training speed up.
The remainder of this paper is structured as follows. We
review related work in Section II. In Section III, we discuss
standard adversarial robustness, knowledge distillation, and
HSIC. In Section V, we present our method. Section VI
includes our experiments; we conclude in Section VII.
II. R ELATED WORK
Adversarial Robustness. Popular adversarial attack methods
include projected gradient descent (PGD) [3], fast gradient
sign method (FGSM) [2], CW attack [4], and AutoAttack
(AA) [5]; see also [30] for a comprehensive review. Adver-
sarially robust models are typically obtained via adversarial
training [31], by augmenting the training set with adversarial
examples, generated by the aforementioned adversarial attacks.
Madry et al. [3] generate adversarial examples via PGD.
TRADES [11] and MART [12] extend adversarial training
by incorporating additional penalty terms. LBGAT [32] guide
adversarial training by a natural classiﬁer boundary to improve
robustness. However, generating adversarial examples is com-
putationally expensive and time consuming.Several recent works observe that information-bottleneck
penalties enhance robustness. Fischer [33] considers a con-
ditional entropy bottleneck (CEB), while Alemi et al. [34]
suggest a variational information bottleneck (VIB); both lead
to improved robustness properties. Ma et al. [28] and Wang
et al. [29] use a penalty based on the Hilbert Schmidt In-
dependence Criterion (HSIC), termed HSIC bottleneck as a
regularizer (HBaR). Wang et al. show that HBaR enhances
adversarial robustness even without generating adversarial
examples [29]. For this reason, we incorporate HBaR into our
uniﬁed robust pruning framework as a means of exploiting
adversarial robustness merely from natural examples during
the pruning process, without further adversarial training. We
are the ﬁrst to study HBaR under a pruning context; our abla-
tion study (Section VI-B) indicates HBaR indeed contributes
to enhancing robustness in our setting.
Adversarial Pruning. Weight pruning is one of the prominent
compression techniques to reduce model size with acceptable
accuracy degradation. While extensively explored for efﬁ-
ciency and compression purposes [15]–[20], only a few recent
works study pruning in the context of adversarial robustness.
Several works [35], [36] theoretically discuss the relationship
between adversarial robustness and pruning, but do not provide
any active defense techniques. Ye et al. [22] and Gui et al. [23]
propose AdvPrune to combine the alternating direction method
of multipliers (ADMM) pruning framework with adversarial
training. Lee et al. [37] propose APD to use knowledge distil-
lation for adversarial pruning optimized by a proximal gradient
method. Sehwag et al. [24] propose HYDRA, which uses a
robust training objective to learn a sparsity mask. However,
all these methods rely on adversarial training. HYDRA further
requires training additional sparsity masks, which hampers
training efﬁciency. In contrast, we distill from a pre-trained
adversarially robust model while pruning without generating
adversarial examples. Our compressed model can preserve
high adversarial robustness with considerable training speedup
compared to these methods, as we report in Section VI-C.
III. B ACKGROUND
We use the following standard notation throughout the
paper. In the standard k-ary classiﬁcation setting, we are given
a datasetD=f(xi;yi)gn
i=1, wherexi2RdX;yi2f0;1gk
are i.i.d. samples drawn from joint distribution PXY. Given
anL-layer neural network h:RdX!Rkparameterized
by weights:=flgL
l=12Rdl, wherelis the weight
corresponding to the l-th layer, for l= 1;:::;L , we deﬁne
the standard learning objective as follows:
L() =EXY[`(h(X);Y)]1
nnX
i=1`(h(xi);yi); (1)
where`:RkRk!Ris a loss function, e.g., cross-entropy.
A. Adversarial Robustness
We call a network adversarially robust if it maintains
high prediction accuracy against a constrained adversary that
perturbs input samples. Formally, prior to submitting an input

--- PAGE 3 ---
samplex2RdX, an adversary may perturb xby an arbitrary
2Br, whereBrRdXis the`1-ball of radius r, i.e.,
Br=B(0;r) =f2RdX:kk1rg: (2)
The adversarial robustness [3] of a model his measured by
the expected loss attained by such adversarial examples, i.e.,
~L() =EXY
max
2Br`(h(X+);Y)
1
nnX
i=1max
2Br`(h(xi+);yi):(3)
An adversarially robust neural network hcan be obtained
viaadversarial training , i.e., by minimizing the adversarial
robustness loss in (3) empirically over the training set D. In
practice, this amounts to stochastic gradient descent (SGD)
over adversarial examples xi+(see, e.g., [3]). In each epoch,
is generated on a per sample basis via an inner optimization
overBr, e.g., via projected gradient descent (PGD).
Adversarial pruning preserves robustness while pruning.
Current approaches combine adversarial training into their
pruning objective. In particular, AdvPrune [22] directly min-
imizes adversarial loss ~L()constrained by sparsity require-
ments. HYDRA [24] also uses ~L()to jointly learn a sparsity
mask along with l. Both are combined with and tailored to
speciﬁc adversarial training methods, and require considerable
training time. This motivates us to propose our PwoA frame-
work, described in Section V.
B. Knowledge Distillation
In knowledge distillation [25], [38], a student model learns
to mimic the output of a teacher. Consider a well-trained
teacher model T, and a student model hthat we wish to
train to match the teacher’s output. Let :Rk![0;1]kbe
the softmax function, i.e., (z)j=ezjP
j0ezj0,j= 1;:::;k . Let
T(x) =T(x)

andh
(x) =h(x)

(4)
be the softmax outputs of the two models weighed by temper-
ature parameter  >0[25]. Then, the knowledge distillation
penalty used to train is:
LKD()=(1 )L()+2EX[KL(h
(X);T(X))];(5)
whereLis the classiﬁcation loss of the tempered student
networkh
andKLis the Kullback–Leibler (KL) divergence.
Intuitively, the knowledge distillation loss LKDtreats the output
of the teacher as soft labels to train the student, so that the
student exhibits some inherent properties of the teacher, such
as adversarial robustness.
C. Hilbert-Schmidt Independence Criterion
The Hilbert-Schmidt Independence Criterion (HSIC) is
a statistical dependency measure introduced by Gretton et
al. [39]. HSIC is the Hilbert-Schmidt norm of the cross-
covariance operator between the distributions in Reproducing
Kernel Hilbert Space (RKHS). Similar to Mutual Information(MI), HSIC captures non-linear dependencies between random
variables. HSIC is deﬁned as:
HSIC(X;Y ) =EXYX0Y0[kX(X;X0)kY0(Y;Y0)]
+EXX0[kX(X;X0)]EYY0[kY(Y;Y0)]
 2EXY[EX0[kX(X;X0)]EY0[kY(Y;Y0)]];(6)
whereX0andY0are independent copies of XandY
respectively, and kXandkYare kernel functions. In practice,
we often approximate HSIC empirically. Given ni.i.d. samples
f(xi;yi)gn
i=1drawn from PXY, we estimate HSIC via:
\HSIC(X;Y ) = (n 1) 2tr (KXHKYH); (7)
whereKXandKYare kernel matrices with entries KXij=
kX(xi;xj)andKYij=kY(yi;yj), respectively, and H=
I 1
n11Tis a centering matrix.
IV. P ROBLEM FORMULATION
Given an adversarially robust model h, we wish to ef-
ﬁciently prune non-important weights from this pre-trained
model while preserving adversarial robustness of the ﬁnal
pruned model. We minimize the loss function subject to
constraints specifying sparsity requirements. More speciﬁcally,
the weight pruning problem can be formulated as:
Minimize:
L();
subject tol2Sl; l= 1;;L;(8)
whereL()is the loss function optimizing both the accuracy
and the robustness, and SlRdlis a weight sparsity
constraint set applied to layer l, deﬁned as
Sl=fljklk0lg; (9)
wherekk 0is the size ofl’s support (i.e., the number of non-
zero elements), and l2Nis a constant speciﬁed as sparsity
degree parameter.
V. M ETHODOLOGY
We now describe PwoA, our uniﬁed framework for pruning
a robust network without additional adversarial training.
A. Robustness-Preserving Pruning
Given an adversarially pre-trained robust model, we aim to
preserve its robustness while sparsifying it via weight pruning.
In particular, we leverage soft labels generated by the robust
model and directly incorporate them into our pruning objective
with only access to natural examples. Formally, we denote the
well pre-trained model by Tand its sparse counterpart by h.
The optimization objective is deﬁned as follows:
Min.:
LD() =2EX[KL(h
(X);T(X))];
subj. tol2Sl; l= 1;;L;(10)
whereis the temperature hyperparameter. Intuitively, our
distillation-based objective forces the sparse model hto
mimic the soft label produced by the original pre-trained
modelT, while the constraint enforces that the learnt weights
are subject to the desired sparsity. This way, we preserve

--- PAGE 4 ---
adversarial robustness via distilling knowledge from soft labels
efﬁciently, without regenerating adversarial examples. Depart-
ing from the original distillation loss in (5), we remove the
classiﬁcation loss where labels are used, as we observed that
it did not contribute to adversarial robustness (see Table V
in Section VI-B). Solving optimization problem (10) is not
straightforward; we describe how to deal with the combinato-
rial nature of the sparsity constraints in Section V-C.
B. Enhancing Robustness from Natural Examples
In addition to preserving adversarial robustness from the
pre-trained model, we can further enhance robustness directly
from natural examples. Inspired by the recent work that uses
information-bottleneck penalties, [28], [29], [33], [34], we
incorporate HSIC as a Regularizer (HBaR) into our robust
pruning framework. To the best of our knowledge, HBaR
has only been demonstrated effective under usual adversarial
learning scenarios; we are the ﬁrst to extend it to the context
of weight pruning. Formally, we denote by Zl2RdZl,
l2f1;:::;Lgthe output of the l-th layer ofhunder inputX
(i.e., thel-th latent representation). The HBaR learning penalty
[28], [29] is deﬁned as follows:
LH() =xLX
l=1HSIC(X;Zl) yLX
l=1HSIC(Y;Zl);(11)
wherex;y2R+are balancing hyperparameters.
Intuitively, since HSIC measures dependence between two
random variables, minimizing HSIC(X;Zl)corresponds to
removing redundant or noisy information from X. Hence,
this term also naturally reduces the inﬂuence of adversarial
attack, i.e. perturbation added on the input data. Meanwhile,
maximizing HSIC(Y;Zl)encourages this lack of sensitivity to
the input to happen while retaining the discriminative nature of
the classiﬁer, captured by the dependence to useful information
w.r.t. the output label Y. This intrinsic tradeoff is similar to the
so-called information-bottleneck [40], [41]. Wang et al. [29]
observe this tradeoff between penalties during training; we
also observe it during pruning (see Appendix C).
PwoA combines HBaR with self-distillation during weight
pruning. We formalize PwoA to solve the following problem:
Minimize:
LPwoA() =LD() +LH();
subject tol2Sl; l= 1;;L:(12)
C. Solving PwoA via ADMM
Problem (12) has combinatorial constraints due to sparsity.
Thus, it cannot be solved using stochastic gradient descent as
in the standard CNN training. To deal with this, we follow
the ADMM-based pruning strategy by Zhang et al. [18] and
Ren et al. [19]. We describe the complete procedure detail
in Appendix A. In short, ADMM is a primal-dual algorithm
designed for constrained optimization problems with decou-
pled objectives (e.g., problem (12)). Through the deﬁnition of
an augmented Lagrangian, the algorithm alternates between
two primal steps that can be solved efﬁciently and separately.
The ﬁrst subproblem optimizes objective LPwoA augmentedAlgorithm 1 PwoA Framework
Input: input samplesf(xi;yi)gn
i=1, a pre-trained robust neural
networkTwithLlayers, mini-batch size m, sparsity param-
eter, learning rate , proximal parameters flgL
l=1.
Output: parameter of classiﬁer 
whilehas not converged do
Sample a mini-batch of size mfrom input samples.
SGD step:
  r(LPwoA() +PL
l=1l
2kl 0
l+ulk2
F).
Projection step:
0
l Sl 
l+ul
;forl= 1;:::;L:
Dual variable update step:
u u+ 0
end
with a proximal penalty; this is an unconstrained optimization
solved by classic SGD. The second subproblem is solved by
performing Euclidean projections Sl()to the constraint sets
Sl; even though the latter are not convex, these projections
can be computed in polynomial time. The overall PwoA
framework is summarized in Algorithm 1.
VI. E XPERIMENTS
A. Experimental Setting
We conduct our experiments on three benchmark datasets,
MNIST, CIFAR-10, and CIFAR-100. To setup adversarially
robust pre-trained models for pruning, we consider ﬁve adver-
sarially trained models provided by open-source state-of-the-
art work, including Wang et al. [29], Zhang et al. [11], and
Cui et al. [32], summarized in Table I.
To understand the impact of each component of PwoA
to robustness, we examine combinations of the following
non-adversarial learning objectives for pruning: LCE,LH, and
LD. All of these objectives are optimized based on natural
examples. We also compare PwoA with three adversarially
pruning methods: APD [37], AdvPrune [22] and HYDRA [24].
Hyperparameters. We prune the pre-trained models using
SGD with initial learning rate 0.01, momentum 0.9 and weight
decay 10 4. We set the batch size to 128 for all methods.
For our PwoA, we set the number of pruning and ﬁne-tuning
epochs to 50 and 100, respectively. For SOTA methods Ad-
vPrune and HYDRA, we use code provided by authors along
with the optimal hyperparameters they suggest. Speciﬁcally,
for AdvPrune, we set pruning and ﬁne-tuning epochs to 50
and 100, respectively; for HYDRA, we set them to 20 and
100, respectively, and use TRADES as adversarial training
loss. We report all other tuning parameters in Appendix B.
Network Pruning Rate. Recall from Section IV that the
sparsity constraint sets fSlgL
l=1are deﬁned by Eq. (9) with
sparsity parameters l2Ndetermining the non-zero elements
per layer. We denote the pruning rate as the ratio of unpruned
size versus pruned size; i.e., for nlthe number of parameters
in layerl, the pruning rate at layer lcan be computed as
l=nl
l. We setlso that we get identical pruning rates per
layer, resulting in a uniform pruning rate across the network.

--- PAGE 5 ---
TABLE I: Summary of the pre-trained models used for MNIST, CIFAR-10 and CIFAR-100 datasets.
Dataset Architecture Training Method Natural FGSM PGD10PGD20CW AA
MNIST LeNet PGD [29] 98.66 96.02 97.53 96.44 95.10 91.57
CIFAR-10ResNet-18 TRADES [29] 84.10 58.97 53.76 52.92 51.00 49.43
WRN34-10 TRADES [11] 84.96 60.99 56.29 55.44 53.92 52.34
WRN34-10 LBGAT [32] 88.24 63.62 56.34 54.89 54.47 52.61
CIFAR-100 WRN34-10 LBGAT [32] 60.66 37.46 34.99 34.69 30.78 28.93
TABLE II: Prune LeNet (PGD) on MNIST. For all the non-
adversarial learning objectives, we report natural test accuracy (in
%) and adversarial robustness (in %) on FGSM, PGD, CW, and AA
attacked test examples under different pruning rates.
PRLCELDLHNatural FGSM PGD10PGD20CW AA
4X 99.18 35.73 0.07 0.00 0.00 0.00
X X 98.54 91.86 89.78 78.32 79.16 47.49
X 98.67 95.42 97.08 95.61 95.19 89.28
X X 98.66 95.89 97.35 96.16 96.15 90.00
8X 99.18 39.08 0.04 0.00 0.00 0.00
X X 98.63 88.70 88.89 70.67 71.42 40.71
X 98.66 94.15 96.94 95.98 94.74 86.48
X X 98.66 95.69 97.13 95.61 95.60 87.37
16X 98.96 79.09 0.06 0.00 0.00 0.00
X X 98.70 81.24 83.70 50.82 54.31 13.04
X 98.33 94.51 95.89 93.15 93.14 76.00
X X 98.59 95.03 96.34 94.43 94.48 77.21
TABLE III: Prune WRN34-10 (LBGAT) on CIFAR-10. For all
the non-adversarial, we report natural test accuracy (in %) and
adversarial robustness (in %) on FGSM, PGD, CW, and AA attacked
test examples under different pruning rates.
PRLCELDLHNatural FGSM PGD10PGD20CW AA
4X 93.59 48.47 2.47 0.74 0.21 0.00
X X 93.68 46.52 8.45 1.69 0.25 0.00
X 88.69 62.72 52.86 50.96 50.29 48.26
X X 88.51 63.44 53.54 51.51 50.89 49.03
8X 93.27 41.15 0.58 0.33 0.00 0.00
X X 93.81 40.08 2.95 1.04 0.28 0.00
X 88.40 61.93 50.76 48.13 48.07 44.87
X X 88.66 62.64 51.41 48.98 48.81 46.09
16X 92.87 20.95 0.00 0.00 0.00 0.00
X X 93.14 29.88 0.84 0.11 0.04 0.00
X 88.30 60.77 48.80 46.32 45.76 42.01
X X 88.51 61.52 49.68 47.19 47.01 43.33
Performance Metrics and Attacks. For all methods, we
evaluate the ﬁnal pruned model via the following metrics.
We ﬁrst measure (a) Natural accuracy (i.e., test accuracy over
natural examples). We then measure adversarial robustness via
test accuracy under (b) FGSM , the fast gradient sign attack
[2], (c) PGDm, the PGD attack with msteps used for the
internal PGD optimization [3], (d) CW (CW-loss within the
PGD framework) attack [4], and (e) AA, AutoAttack [5], which
is the strongest among all four attacks. All ﬁve metrics areTABLE IV: Prune WRN34-10 (LBGAT) on CIFAR-100. For all
the non-adversarial, we report natural test accuracy (in %) and
adversarial robustness (in %) on FGSM, PGD, CW, and AA attacked
test examples under different pruning rates.
PRLCELDLHNatural FGSM PGD10PGD20CW AA
4X 71.55 20.92 7.21 5.64 3.93 0.00
X X 71.83 23.45 7.57 5.95 4.07 0.00
X 60.91 36.21 32.69 31.87 27.74 25.52
X X 60.92 36.70 33.08 32.59 28.40 26.44
8X 71.34 15.28 3.52 2.65 1.37 0.00
X X 71.56 17.32 3.73 2.65 1.60 0.00
X 61.10 35.27 30.46 29.65 25.52 23.34
X X 61.44 35.61 31.19 30.45 26.32 24.20
16X 69.89 14.56 3.04 2.46 1.68 0.00
X X 70.54 16.88 3.56 2.72 1.62 0.00
X 62.34 34.65 28.48 27.19 23.30 20.11
X X 62.53 35.15 29.05 27.88 24.08 21.43
TABLE V: Effect of adding classiﬁcation loss. We compareLPwoA
(i.e.,LD+LHwhere= 1000 for CIFAR-100) with LPwoA+ceLCE
when pruning WRN34-10 (LBGAT) on CIFAR-100. Increasing atten-
tion onLCEimproves the natural accuracy while degrading adversarial
robustness signiﬁcantly.
PRLCELDLHce Natural FGSM PGD10PGD20CW AA
4X X 0 60.92 36.70 33.08 32.59 28.40 26.44
X X X 0.01 61.74 36.87 32.15 31.61 27.34 25.31
X X X 0.1 65.03 36.03 28.98 29.02 26.41 22.52
X X X 1 78.66 40.64 5.95 2.81 1.75 0.08
reported in percent (%) accuracy. Following prior adversarial
learning literature, we set step size to 0.01 and r= 0:3for
MNIST, and step size to 2=255andr= 8=255for CIFAR-10
and CIFAR-100, optimizing over `1-norm balls in all cases.
All attacks happen during the test phase and have full access
to model parameters. Since there is always a trade-off between
natural accuracy and adversarial robustness, we report the best
model when it achieves the lowest average loss among the
two, as suggested by Ye et al. [22] and Zhang et al. [11]. We
measure and report the overall training time over a Tesla V100
GPU with 32 GB memory and 5120 cores.
B. A Comprehensive Understanding of PwoA
Ablation Study and PwoA Robustness. We ﬁrst examine
the synergy between PwoA terms in the objective in Eq.
(12) and show how these terms preserve and even improve
robustness while pruning. We studied multiple combinations
ofLCE,LH, andLDin Tables II-IV. We report the natural test

--- PAGE 6 ---
4x 8x 16x
Pruning rate303540455055AA (%)Lite
PwoA
4x 8x 16x
Pruning rate051015202530AA (%)Lite
PwoA(a) CIFAR-10 (b) CIFAR-100
Fig. 2: Comparison between the pruned WRN34-10 and adversarially
trained WRN34-10-Lite from scratch on (a) CIFAR-10 and (b)
CIFAR-100 datasets. Each bar represents the robustness under AA
of the corresponding model vs. the pruning rate. The pruned model
outperforms its corresponding ‘Lite’ version in all cases, attaining a
considerably higher robustness with only access to natural examples.
accuracy and adversarial robustness under various attacks of
the pruned model under 3 pruning rates (4 , 8, and 16) on
MNIST, CIFAR-10, and CIFAR-100. For each result reported,
we explore hyperparameters ,x, andyas described in
Appendix B and report here the best performing values.
Overall, Tables II-IV suggest that our method PwoA
(namely,LD+LH) prunes a large fraction of weights while
attaining the best adversarial robustness for all three datasets .
In contrast, a model pruned by LCEalone (i.e., with no effort
to maintain robustness) catastrophically fails under adversarial
attacks on all the datasets. The reason is that when the dataset
is more complicated and/or pruning rate is high, LCEis forced
to maintain natural accuracy during pruning, making it deviate
from the adversarial robustness of the pre-trained model. In
contrast, concurrent self-distillation ( LD) and pruning is imper-
ative for preserving substantial robustness without generating
adversarial examples during pruning . We observe this for all
three datasets, taking AA under 4 pruning rate for example,
from 0:00% byLCEto89:28%,48:26%, and 25:52% byLD
on MNIST, CIFAR-10 and CIFAR-100, respectively.
We also observe that incorporatingLHwhile pruning is
beneﬁcial for maintaining high accuracy while improving
adversarial robustness against various attacks . By regulariz-
ingLCEwithLH, we observe a sharp adversarial robustness
advantage on MNIST, taking AA for example from 0:00%
byLCEto47:49%,40:71%, and 13:04% by incorporating
LHunder 4, 8, and 16pruning rate, respectively; by
regularizingLDwithLH, we again see that the regularization
improves adversarial robustness on all the cases, especially
w.r.t. the strongest attack (AA). We note that the robustness
improvement of incorporating LHwithLDis not caused by a
trade-off between accuracy and robustness: in fact, LD+LH
consistently improves both natural accuracy and robustness
under all pruning rates on all datasets. Motivated by the above
observations, we further analyze how the two terms in HBaR
deﬁned in Eq. (11) affect natural accuracy and robustness and
summarize these in Appendix C.
LCEDiminishes Robustness. Recall from Section V-A that weremove the classiﬁcation loss from the original distillation loss
to achieve robustness-preserving pruning. Table V empirically
shows that classiﬁcation loss (i.e., LCE) considerably dimin-
ishes robustness . Intuitively, PwoA distills robustness from
the pre-trained robust model rather than acquiring it from
natural examples. This is in contrast to observations made
with adversarial pruning methods, such as APD [37], where
the classiﬁcation loss increases robustness. This is because
APD prunes by optimizing the original distillation loss over
adversarial examples , so it may indeed beneﬁt from LCE.
Pruning Rate Effect. On Tables III-IV, we also observe
a slight natural accuracy increase during pruning. This is
because pruning reduces the complexity of the model, and
hence, to some extent, avoids overﬁtting. However, increasing
the pruning rate beyond a critical point can lead to sharp
drop in accuracy. This is expected, as reducing the model
capacity signiﬁcantly hampers its expressiveness and starts
to introduce bias in predictions. Not surprisingly, this critical
point occurs earlier in more complex datasets. We also see that
this saturation/performance drop happens earlier for adversar-
ial robustness when compared to natural accuracy: preserving
robustness is more challenging, especially without explicitly
incorporating adversarial training.
Comparison to Na ¨ıve Parsimony. We further demonstrate
that pruning while training is imperative for attaining high
robustness under a parsimonious model. To show this, we
construct a class of models that has fewer parameters than
the original WRN34-10, and explore the resulting robustness-
compression trade-off. We term the ﬁrst class of models as
‘WRN34-10-Lite’: these models have the same architecture
WRN34-10 but contain fewer ﬁlters in each convolutional
layer (resulting in fewer parameters in total). These WRN34-
10-Lite models are designed to have similar total number of
parameters as pruned models with pruning rates 4 , 8, and
16, respectively. We train these ‘Lite’ models for 100 epochs
on adversarial examples generated by PGD10. The pruned
model outperforms its corresponding ‘Lite’ version in all
cases, improving robustness under 16 against AA by 10.47%
and 7.35%, on CIFAR-10 and CIFAR-100, respectively.
C. Comparison to Adversarial Pruning (AP) Methods
Robustness with Partial Access to Adversarial Examples.
We ﬁrst compare PwoA with two state-of-the art AP baselines,
i.e., AdvPrune and HYDRA, in terms of adversarial robust-
ness and training efﬁciency on the CIFAR-10 and CIFAR-
100 datasets. Both AdvPrune and HYDRA require access to
adversarial examples. To make a fair comparison, we generate
adversarial examples progressively for all methods, including
PwoA: in Figure 3, we change the mix ratio , i.e., the fraction
of total natural examples replaced by adversarial examples
generated by PGD10. We plot AA robustness vs. training
time, under a 4pruning rate. We observe that, without
access to adversarial examples (mix ratio 0%), both competing
methods fail catastroﬁcally, exhibiting no robustness what-
soever. Moreover, to achieve the same robustness as PwoA,
they require between 4and7more training time; on

--- PAGE 7 ---
Mix Ratio (%)CIFAR-10CIFAR-100
(a) ResNet-18 (TRADES) (b) WRN34-10 (TRADES) (c) WRN34-10 (LBGAT) (d) WRN34-10 (LBGAT) 
Fig. 3: Robustness comparison with AdvPrune and HYDRA across different pre-trained models and datasets, under a varying mix ratio , i.e.,
fraction (in %) of natural examples replaced by adversarial examples during training. We plot AA robustness v.s. training time as we modify
the mix ratio; boxes indicate PwoA with 0%mix ratio (no adversarial examples). We observe that, competitors are not robust without
access to adversarial examples; to achieve PwoA’s robustness at 0% mix ratio, AdvPrune and HYDRA require 4–7more training time.
On CIFAR-100, they never meet the performance attained by PwoA. We also observe that PwoA improves by partial access to adversarial
examples; overall, it attains a much more favorable trade-off between robustness and training efﬁciency than the two competitors. In fact, in
all cases except (b), PwoA consistently outperforms competitors at 100% mix ratio, w.r.t. both robustness and training time.
TABLE VI: Prune WRN34-10 (LBGAT) on CIFAR-10 : Compari-
son of PwoA with SOTA methods w.r.t various attacks and training
time (TT, in h) under different pruning rates at 20% mix ratio.
PR Methods Natural FGSM PGD10PGD20CW AA TT
4AdvPrune 89.35 58.05 47.14 45.01 45.68 43.31 12.01
HYDRA 86.07 57.45 51.30 50.20 50.01 48.09 18.77
PwoA (ours) 88.10 62.96 55.40 53.72 53.30 51.07 17.06
8AdvPrune 89.31 57.91 47.18 45.22 45.45 43.20 12.44
HYDRA 86.50 57.89 51.28 50.20 50.15 48.09 18.89
PwoA (ours) 88.11 62.86 54.64 52.93 52.48 50.07 17.13
16AdvPrune 89.37 55.32 46.68 44.77 44.13 42.61 12.38
HYDRA 85.98 57.38 51.17 50.27 49.34 47.74 18.91
PwoA (ours) 88.10 62.04 53.38 51.35 51.03 48.44 17.11
CIFAR-100, they actually never meet the performance attained
by PwoA. We also observe that PwoA improves by partial
access to adversarial examples; overall, it attains a much more
favorable trade-off between robustness and training efﬁciency
than the two competitors. Interestingly, with the exception of
the case shown in Figure 3(b) (WRN34-10 over CIFAR-10),
PwoA consistently outperforms competitors at 100% mix ratio,
w.r.t. both robustness and training time.
Impact of Pre-training Method. We also observe that HY-
DRA performs well when pruning models pre-trained with
TRADES, but gets worse when dealing with model pre-trained
with LBGAT. This is because HYDRA prunes the model using
TRADES as adversarial loss, and is thus tailored to such
pre-training. When models are pre-trained via LBGAT, this
change of loss hampers performance. In contrast, PwoA can
successfully prune an arbitrary pre-trained model, irrespective
of the architecture or pre-training method.
Pruning Rate Impact. We further measure the natural accu-
racy and robustness of our PwoA and SOTA methods against
all ﬁve attacks under 4 , 8, and 16pruning rate. We report
these at 20% mix ratio, so that training times are roughly equalTABLE VII: Prune WRN34-10 (LBGAT) on CIFAR-100 : Compar-
ison of PwoA with SOTA methods w.r.t various attacks and training
time (TT, in h) under different pruning rates at 20% mix ratio.
PR Methods Natural FGSM PGD10PGD20CW AA TT
4AdvPrune 68.39 40.77 24.71 22.42 21.45 14.95 12.14
HYDRA 60.61 29.54 25.88 25.21 24.22 22.81 18.69
PwoA (ours) 60.93 36.92 33.62 33.30 29.10 27.31 17.03
8AdvPrune 68.33 40.73 24.34 22.03 20.97 12.73 12.31
HYDRA 61.04 29.90 25.55 25.04 24.11 22.36 18.73
PwoA (ours) 61.58 36.39 33.09 32.50 28.29 26.46 17.05
16AdvPrune 68.24 38.98 23.20 20.50 19.13 8.40 12.08
HYDRA 61.35 29.14 25.53 24.85 23.92 21.95 18.77
PwoA (ours) 61.84 35.78 32.24 31.34 27.31 25.28 17.09
across methods, in Table VI for CIFAR-10 and Table VII for
CIFAR-100. Overall, we can clearly see that PwoA consis-
tently outperforms other SOTA methods against all ﬁve attacks,
under similar (or lower) training time. Speciﬁcally, on CIFAR-
100, PwoA maintains high robustness against AA with only
1.62% drop (under 4 PR) from the pre-trained model by
LBGAT (see Table I), while the AA robustness achieved by
HYDRA and AdvPrune drop by 6.12% and 13.98%, respec-
tively. This again veriﬁes that, when pruning a robust model
pre-trained with different adversarial training methods, PwoA
is more stable in preserving robustness. Improvements are also
pronounced while increasing pruning rate: PwoA outperforms
HYDRA against AA by 4.50%, 4.10%, and 3.33% under 4 ,
8, and 16pruning rates, respectively. For completeness,
we also report performance at 0% mix ratio on CIFAR-100 in
Appendix D; in contrast to PwoA, competitors exhibit virtually
negligible robustness in this case.
Comparison with APD. Finally, we also compare to APD
[37], which is weaker than HYDRA and AdvPrune, but more
closely related to our PwoA: APD prunes by optimizing
KD over adversarial examples using a non-robust teacher .
Table VIII compares PwoA with APD on CIFAR-10 by

--- PAGE 8 ---
TABLE VIII: Comparison with APD. Comparison PwoA with APD
results reported in [37], for pruning ResNet-18 on CIFAR-10 under
4pruning rate. The authors report natural accuracy, robustness
under PGD10, and number of epochs. We estimate execution time
(T) per epoch and training time (TT, in h), by training KD alone
over adv. examples.
PRMethods Natural PGD10Epochs T/epoch TT
4APD 86.7345:6160147:55sy2:46hy
PwoA 86.07 49.61 150 45.19s 1.88h
*reported in [37].yestimated by KD over adv. examples.
ResNet-18 under 4 pruning rate (which is the largest pruning
rate reported in their paper). We observe that, while achieving
similar accuracy, PwoA outperforms APD w.r.t. both robust-
ness and training efﬁciency. This is expected, as distilling from
a non-robust teacher limits APD’s learning ability from adver-
sarial examples and generating adversarial examples hampers
training efﬁciency.
VII. C ONCLUSIONS AND FUTURE WORK
We proposed PwoA, a uniﬁed framework for pruning ad-
versarially robust networks without adversarial examples. Our
method leverages pre-trained adversarially robust models, pre-
serves adversarial robustness via self-distillation and enhances
it via the Hilbert-Schmidt independence criterion as a regular-
izer. Comprehensive experiments on MNIST, CIFAR-10, and
CIFAR-100 datasets demonstrate that PwoA prunes a large
fraction of weights while attaining comparable adversarial
robustness with up to 7 training speed up. Future directions
include extending PwoA framework to structured pruning and
weight quantization. Another interesting future direction is
to use distillation and novel penalties to prune a pre-trained
robust model even without access to natural examples.
VIII. A CKNOWLEDGEMENTS
The authors gratefully acknowledge support by the National
Science Foundation under grants CCF-1937500 and CNS-
2112471.
REFERENCES
[1] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in CVPR , 2016, pp.
2574–2582.
[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in ICLR , 2015.
[3] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” in ICLR , 2018.
[4] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
networks,” in IEEE Symposium on Security and Privacy , 2017, pp. 39–
57.
[5] F. Croce and M. Hein, “Reliable evaluation of adversarial robustness
with an ensemble of diverse parameter-free attacks,” in ICML , vol. 119,
2020, pp. 2206–2216.
[6] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” in ICLR , 2014.
[7] A. Chernikova, A. Oprea, C. Nita-Rotaru, and B. Kim, “Are self-
driving cars secure? evasion attacks against deep neural networks for
steering angle prediction,” in IEEE Symposium on Security and Privacy
Workshops , 2019, pp. 132–137.
[8] S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A. L. Beam, and I. S.
Kohane, “Adversarial attacks on medical machine learning,” Science ,
vol. 363, no. 6433, pp. 1287–1289, 2019.[9] S. Thys, W. Van Ranst, and T. Goedem ´e, “Fooling automated surveil-
lance cameras: adversarial patches to attack person detection,” in CVPR
Workshops , 2019.
[10] Z. Yan, Y . Guo, and C. Zhang, “Deep defense: Training dnns with
improved adversarial robustness,” in NeurIPS , 2018.
[11] H. Zhang, Y . Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan,
“Theoretically principled trade-off between robustness and accuracy,”
inICML , 2019, pp. 7472–7482.
[12] Y . Wang, D. Zou, J. Yi, J. Bailey, X. Ma, and Q. Gu, “Improving
adversarial robustness requires revisiting misclassiﬁed examples,” in
ICLR , 2019.
[13] C. Xie, Y . Wu, L. van der Maaten, A. L. Yuille, and K. He, “Feature
denoising for improving adversarial robustness,” CVPR , 2019.
[14] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer,
L. S. Davis, G. Taylor, and T. Goldstein, “Adversarial training for free!”
inNeurIPS , vol. 32, 2019.
[15] S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efﬁcient neural network,” in NeurIPS , 2015.
[16] X. Dong, S. Chen, and S. Pan, “Learning to prune deep neural networks
via layer-wise optimal brain surgeon,” in NeurIPS , 2017, pp. 4857–4867.
[17] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the
value of network pruning,” in ICLR , 2019.
[18] T. Zhang, S. Ye, K. Zhang, J. Tang, W. Wen, M. Fardad, and Y . Wang,
“A systematic dnn weight pruning framework using alternating direction
method of multipliers,” in ECCV , 2018, pp. 184–199.
[19] A. Ren, T. Zhang, S. Ye, J. Li, W. Xu, X. Qian, X. Lin, and Y . Wang,
“Admm-nn: An algorithm-hardware co-design framework of dnns using
alternating direction methods of multipliers,” in ASPLOS , 2019.
[20] T. Jian, Y . Gong, Z. Zhan, R. Shi, N. Soltani, Z. Wang, J. Dy, K. R.
Chowdhury, Y . Wang, and S. Ioannidis, “Radio frequency ﬁngerprinting
on the edge,” IEEE Transactions on Mobile Computing , 2021.
[21] Z. Wang, Z. Zhan, Y . Gong, G. Yuan, W. Niu, T. Jian, B. Ren,
S. Ioannidis, Y . Wang, and J. Dy, “Sparcl: Sparse continual learning
on the edge,” NeurIPS , 2022.
[22] S. Ye, K. Xu, S. Liu, H. Cheng, J.-H. Lambrechts, H. Zhang, A. Zhou,
K. Ma, Y . Wang, and X. Lin, “Adversarial robustness vs. model
compression, or both?” in ICCV , 2019.
[23] S. Gui, H. N. Wang, H. Yang, C. Yu, Z. Wang, and J. Liu, “Model com-
pression with adversarial robustness: A uniﬁed optimization framework,”
inNeurIPS , vol. 32, 2019.
[24] V . Sehwag, S. Wang, P. Mittal, and S. Jana, “HYDRA: Pruning adver-
sarially robust neural networks,” in NeurIPS , vol. 33, 2020.
[25] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” arXiv preprint arXiv:1503.02531 , 2015.
[26] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
inIEEE Symposium on security and privacy , 2016, pp. 582–597.
[27] M. Goldblum, L. Fowl, S. Feizi, and T. Goldstein, “Adversarially robust
distillation,” in AAAI , vol. 34, no. 04, 2020, pp. 3996–4003.
[28] W.-D. K. Ma, J. Lewis, and W. B. Kleijn, “The hsic bottleneck: Deep
learning without back-propagation,” in AAAI , 2020, pp. 5085–5092.
[29] Z. Wang, T. Jian, A. Masoomi, S. Ioannidis, and J. Dy, “Revisiting
hilbert-schmidt information bottleneck for adversarial robustness,” in
NeurIPS , 2021.
[30] S. H. Silva and P. Najaﬁrad, “Opportunities and challenges in deep learn-
ing adversarial robustness: A survey,” arXiv preprint arXiv:2007.00753 ,
2020.
[31] Y . Wang, X. Ma, J. Bailey, J. Yi, B. Zhou, and Q. Gu, “On the
convergence and robustness of adversarial training,” in ICML , vol. 97,
2019, pp. 6586–6595.
[32] J. Cui, S. Liu, L. Wang, and J. Jia, “Learnable boundary guided
adversarial training,” in ICCV , 2021.
[33] I. Fischer, “The conditional entropy bottleneck,” Entropy , vol. 22, no. 9,
p. 999, 2020.
[34] A. A. Alemi, I. Fischer, J. V . Dillon, and K. Murphy, “Deep variational
information bottleneck,” in ICLR , 2017.
[35] Y . Guo, C. Zhang, C. Zhang, and Y . Chen, “Sparse dnns with improved
adversarial robustness,” in NeurIPS , vol. 31, 2018.
[36] K. Y . Xiao, V . Tjeng, N. M. Shaﬁullah, and A. Madry, “Training for
faster adversarial robustness veriﬁcation via inducing relu stability.” in
ICLR , 2019.
[37] J. Lee and S. Lee, “Robust cnn compression framework for security-
sensitive embedded systems,” Applied Sciences , vol. 11, no. 3, 2021.

--- PAGE 9 ---
[38] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A
survey,” International Journal of Computer Vision , pp. 1–31, 2021.
[39] A. Gretton, O. Bousquet, A. Smola, and B. Sch ¨olkopf, “Measuring
statistical dependence with hilbert-schmidt norms,” in International
conference on algorithmic learning theory , 2005, pp. 63–77.
[40] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck
method,” arXiv preprint physics/0004057 , 2000.
[41] N. Tishby and N. Zaslavsky, “Deep learning and the information bot-
tleneck principle,” in 2015 IEEE Information Theory Workshop (ITW) .
IEEE, 2015, pp. 1–5.
[42] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method
of multipliers,” Foundations and Trends® in Machine learning , vol. 3,
no. 1, pp. 1–122, 2011.
[43] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning
Filters for Efﬁcient ConvNets,” in ICLR , 2017.
[44] W. Wen, C. Wu, Y . Wang, Y . Chen, and H. Li, “Learning structured
sparsity in deep neural networks,” in NeurIPS , 2016, pp. 2074–2082.
[45] X. Zhu, W. Zhou, and H. Li, “Improving deep neural network sparsity
through decorrelation regularization,” in IJCAI , 2018, pp. 3264–3270.
APPENDIX A
SOLVING PROBLEM (12) BYADMM
We follow [18]–[20] in how to solve problem (12 via
ADMM. We begin by rewriting problem (12) in the ADMM
form by introducing auxiliary variables 0
l:
Minimize:
LPwoA() +PL
l=1gl(0
l);
subject tol=0
l; l= 1;;L;(13)
wheregl()is the indicator of set Sl, deﬁned as:
g(0
l) =0 if0
l2Sl;
+1 otherwise.(14)
The augmented Lagrangian of problem (13) is [42]:
L(;0;u) =LPwoA() +PL
l=1gl(0
l)
+PL
l=1l(u>
l(l 0
l)) +PL
l=1l
2kl 0
lk2
2;(15)
wherelis a penalty value and ul2Rdlis a dual variable,
rescaled by l. The ADMM algorithm proceeds by repeating
the following iterative optimization process until convergence.
At thek-th iteration, the steps are given by
(k):= arg min
L(;0(k 1);u(k 1)) (16a)
0(k):= arg min
0L((k);0;u(k 1)) (16b)
u(k):=u(k 1)+(k) 0(k): (16c)
The problem (16a) is equivalent to:
min
LPwoA() +LADMM(); (17)
where
LADMM() =PL
l=1l
2kl 0(k 1)
l+u(k 1)
lk2
F: (18)
All two terms in (17) are quadratic and differentiable. Thus,
this subproblem can be solved by classic Stochastic Gradient
Descent (SGD). After solving problem (16a) at iteration k, we
proceed to solving problem (16b), which is equivalent to:
min
0PL
l=1g(0
l) +PL
l=1l
2k(k)
l 0
l+u(k 1)
lk2
F:(19)TABLE IX: Parameter Summary.
Stage Param.Dataset
MNIST CIFAR-10 CIFAR-100
OverallBatch size 128 128 128
Optimizer SGD SGD SGD
Scheduler cosine cosine cosine
(LD) 30 30 30
(LD) 10 10 1000
x(LH) 4e-4 2e-5 5e-7
y(LH) 1e-4 1e-4 2.5e-6
ADMM# epochs 50 50 50
Learning rate 0.0005 0.01 0.01
Fine-tuning# epochs 20 100 100
Learning rate 0.001 0.005 0.005
Asg()is the indicator function of the constraint set Sl,
problem (19) is equivalent to:
0(k)
l=Sl 
(k)
l+u(k 1)
l
; (20)
where Slis the Euclidean projection of (k)
l+u(k 1)
lonto
the setSl. The projection can be computed in polynomial time
by ﬁrst calculating (k)
l+u(k 1)
l, then keeping the largest
coefﬁcients, in absolute value, and setting the rest to zero.
The parameters produced by ADMM satisfy the constraints
fSlgL
l=1asymptotically. As a result, the ﬁne-tuning process
is typically required to improve the accuracy/robustness of
the pruned model with the training dataset and attain fea-
sibility [22], [43]–[45]. To ﬁne-tune the pruned model, we
can construct a binary mask strictly satisfying the sparsity
constraintsfSlgL
l=1and zero out weights that have been
masked during back propagation. Formally, a binary mask
is deﬁned as Ml2Sl\f0;1gdlfor each layer l. The
maskMlis constructed as follows for irregular pruning:
ﬁrst, we compute0
l=Sl(l);l2 f1;:::;Lg; then, we
set[Ml] =1, for all entries s.t. [l]6= 0. We then retrain
using gradient descent but constrained by masks fMlgL
l=1.
That is, during back propagation, we ﬁrst calculate the gradient
rlLPwoA(l), then apply the mask Mlto the gradient using
element-wise multiplication. Therefore, the weight update in
every step during the retraining process is
l:=l MlrlLPwoA(l); (21)
whereis the learning rate, and denotes element-wise
multiplication.
APPENDIX B
IMPLEMENTATION DETAILS
We report the parameter settings in Table IX.
ADMM Hyperparameters. In pruning stage, we run ADMM
every 3 iterations (Eq. (16)). In each iteration, step (16a) is
implemented by one epoch of SGD over the dataset, solving
Eq. (17) approximately. We set all i= 0:01initially; every
iteration of ADMM, we multiply them by a factor of 1.35,
until they reach 1. At the ﬁne-tuning stage, we retrain the
network under a pruned mask for several epochs.

--- PAGE 10 ---
KD and HBaR Hyperparameters. ForLD, we ﬁx= 30
in our experiments as we ﬁnd that further tuning it leads to
no performance gain. For LH, we follow original authors [29]
and apply Gaussian kernels for XandZand a linear kernel
forY. For Gaussian kernels, we set = 5p
d, wheredis the
dimension of the corresponding random variable.
PwoA Hyperparameters. Recall that ,xandyare
balancing hyper-parameters for LDandLH, respectively. We
ﬁrst describe how to set : ﬁrst, we compute the value of LD,
LHandLADMM, given by (10), (11) and (18) respectively, at the
end of the ﬁrst epoch. Then, we set so thatLADMM
LD= 10 ; we
empirically found that this ratio gives the best performance.
Then, given this , we setxandyas follows. We follow
Wang et al. [29] to determine the ratio between xandy:
they suggest that setting the ratio x:yas4 : 1 on MNIST,
and as 1 : 5 on CIFAR-10/100 provides better performance.
We adopt these ratios, and scale both xandy(maintaning
these ratios constant) so thatLD
LH= 10 ; our choice of this
ratio is determined empirically, by exploring different options.
Repetitions with Different Seeds. Note that initial weights
are ﬁxed in our setting : we start from the pre-trained model,
and repetition of experiments with different starting points
does not apply to our setting. The only source of randomness
comes from (a) the SGD data sampler across epochs (b) and
the adversarial example generation (in the mixed setting).
Since we span 150 epochs, both processes are sampled con-
siderably and our results are thus statistically signiﬁcant.
APPENDIX C
SYNERGY BETWEEN HBAR T ERMS
Figure 4 provides the learning dynamics on the HSIC plane
for all datasets under 4 pruning rate. The x-axis plots HSIC
between the last intermediate layer ZLand the input X,
while the y-axis plots HSIC between ZLand the output
Y. As discussed in Section V-B, minimizing HSIC(X;ZL)
corresponds to reducing the inﬂuence of adversarial attack,
while maximizing HSIC(Y;Zl)encourages the discriminative
nature of the classiﬁer. The performance of different schemes
can be clearly veriﬁed and demonstrated on the HSIC plain: as
shown in Figure 4, PwoA terminates with considerably lower
HSIC(X;ZL)thanLCE, indicating the stronger robustness
against attacks. Additionally, we observe the two optimization
phases, especially on MNIST, separated by the start of ﬁne-
tuning stage: the risky compression phase , where the top
priority of the neural network is to prune non-important
weights while maintain meaningful representation by increas-
ingHSIC(Y;ZL)regardless of the information redundancy
(HSIC(X;ZL)), and the robustness recovery phase , where the
neural network turns its focus onto inheriting robustness by
minimizing HSIC(X;ZL), while keeping highly label-related
information for natural accuracy.
APPENDIX D
PRUNING RATEIMPACT AT 0% M IXRATIO
We evaluate natural accuracy and the robustness of our
PwoA and SOTA methods against all ﬁve attacks under 4 ,
Fine-tune PowAEnding𝑳𝑪𝑬EndingEpochsPruning RobustnessLowHighAccuracyHighLow(a) MNIST by ResNet-18
Fine-tune EpochsPruning AccuracyHighLowRobustnessLowHighPowAEnding𝑳𝑪𝑬Ending
(b) CIFAR-10 by WRN34-10
AccuracyHighLowRobustnessLowHighPowAEnding𝑳𝑪𝑬EndingFine-tune EpochsPruning 
(c) CIFAR-100 by WRN34-10
Fig. 4: HSIC plane dynamics. The x-axis plots HSIC between the last
intermediate layer ZLand the input X, while the y-axis plots HSIC
betweenZLand the output Y. The color scale and arrows indicate
indicate dynamic direction w.r.t. training epochs. Each marker in
the ﬁgures represents a different setting: stars ,dots, and triangles
represent pre-trained, LCE, and PwoA, respectively.
TABLE X: Prune WRN34-10 (LBGAT) on CIFAR-100 : Compar-
ison of PwoA with SOTA methods w.r.t various attacks and training
time (TT, in h) under different pruning rates at 0% mix ratio.
PR Methods Natural FGSM PGD10PGD20CW AA TT
4AdvPrune 79.56 17.31 0.24 0.10 0.03 0.00 2.62
HYDRA 79.62 17.39 0.25 0.10 0.03 0.00 5.70
PwoA (ours) 60.92 36.70 33.08 32.59 28.4 26.44 9.33
8AdvPrune 79.38 16.98 0.18 0.08 0.02 0.00 2.64
HYDRA 79.44 17.21 0.21 0.10 0.02 0.00 5.54
PwoA (ours) 61.43 35.61 31.19 30.45 26.32 24.20 9.32
16AdvPrune 79.21 16.98 0.18 0.08 0.02 0.00 2.61
HYDRA 79.36 17.18 0.20 0.09 0.02 0.00 5.79
PwoA (ours) 62.53 35.15 29.05 27.88 24.08 21.43 9.18
8, and 16pruning rate as well as the overall training time,
and report these at 0% mix ratio in Table X for CIFAR-
100. We observe that, without access to adversarial examples
(mix ratio 0%), both competing methods fail catastrophically,
exhibiting no robustness whatsoever. Moreover, increasing the
pruning rate can lead to sharp drop in robustness, especially

--- PAGE 11 ---
with limited access to adversarial examples. Not surprisingly,
comparing to Table VII at 20% mix ratio, this drop occurs
much severer in method being more dependent on adversarial
learning objectives, e.g., under 8 against AA, HYDRA drops
from 22.26% (at 20% mix ratio) to 0.00% (at 0% mix ratio)
while PwoA drops from 26.46% to 24.20%; under 16 against
AA, HYDRA drops from 21.95% to 0.00% while PwoA drops
from 25.28% to 21.43%.
