# Về Tính Bền Vững Đối Kháng của Hỗn Hợp Chuyên Gia

Joan Puigcerver
Google Research

Rodolphe Jenatton
Google Research

Carlos Riquelme
Google Research

Pranjal Awasthi
Google Research

Srinadh Bhojanapalli
Google Research

## Tóm tắt

Tính bền vững đối kháng là một tính chất mong muốn quan trọng của mạng nơ-ron. Đã được chứng minh thực nghiệm rằng nó bị ảnh hưởng bởi kích thước của chúng, với các mạng lớn hơn thường bền vững hơn. Gần đây, Bubeck và Sellke [3] đã chứng minh một cận dưới về hằng số Lipschitz của các hàm khớp với dữ liệu huấn luyện theo số lượng tham số của chúng. Điều này đặt ra một câu hỏi mở thú vị, liệu các hàm có nhiều tham số hơn, nhưng không nhất thiết có chi phí tính toán cao hơn, có thể có tính bền vững tốt hơn không? Chúng tôi nghiên cứu câu hỏi này cho các mô hình Hỗn hợp Chuyên gia thưa thớt (MoE), cho phép mở rộng kích thước mô hình với chi phí tính toán gần như không đổi. Chúng tôi chứng minh lý thuyết rằng trong một số điều kiện nhất định về định tuyến và cấu trúc dữ liệu, MoE có thể có hằng số Lipschitz nhỏ hơn đáng kể so với các đối tác dày đặc của chúng. Tính bền vững của MoE có thể bị ảnh hưởng khi các chuyên gia có trọng số cao nhất cho một đầu vào thực hiện các hàm đủ khác biệt. Tiếp theo, chúng tôi đánh giá thực nghiệm tính bền vững của MoE trên ImageNet bằng các cuộc tấn công đối kháng và cho thấy chúng thực sự bền vững hơn các mô hình dày đặc có cùng chi phí tính toán. Chúng tôi đưa ra các quan sát quan trọng cho thấy tính bền vững của MoE đối với việc lựa chọn chuyên gia, làm nổi bật sự dư thừa của các chuyên gia trong các mô hình được huấn luyện trong thực tế.

## 1 Giới thiệu

Tính bền vững đối kháng đề cập đến tính bền vững dự đoán của một mô hình học máy đối với những thay đổi đối kháng nhưng có giới hạn đối với đầu vào. Các mạng nơ-ron được huấn luyện với các mục tiêu phân loại tiêu chuẩn đã được chứng minh là có tính bền vững đối kháng kém [25], một tính chất được cho là do sự tham số hóa quá mức của chúng. Ngược lại, trong thực tế, các mô hình lớn hơn, có nhiều tham số hơn và chi phí tính toán cao hơn, đã cho thấy tính bền vững tốt hơn [20, 26, 1, 12].

Trong một công trình gần đây, Bubeck và Sellke [3] đã nghiên cứu hiện tượng này từ góc độ lý thuyết, bằng cách phân tích mối quan hệ giữa kích thước mô hình và hằng số Lipschitz của nó - thước đo độ nhạy cảm của một hàm đối với những thay đổi trong đầu vào. Đặc biệt, các tác giả đã chứng minh rằng bất kỳ hàm nào với P tham số, ghi nhớ N điểm dữ liệu đầu vào trong D chiều, có hằng số Lipschitz ít nhất là O(√(ND/P)). Điều này cho thấy rằng, trên một tập dữ liệu nhất định, các mô hình lớn hơn có thể có tính bền vững tốt hơn (hằng số Lipschitz nhỏ hơn). Lưu ý rằng vì đây chỉ là cận dưới, nó không đảm bảo rằng các mô hình lớn hơn thực sự sẽ có hằng số Lipschitz nhỏ hơn. Thú vị là kết quả này không phụ thuộc vào các tính chất khác của hàm, chẳng hạn như chi phí tính toán hoặc kiến trúc cụ thể của nó.

Với kết quả trên, việc tự hỏi là tự nhiên: liệu có thể tăng kích thước mô hình mà không tăng chi phí tính toán và có được tính bền vững tốt hơn không? Một lớp mô hình ngày càng phổ biến như vậy là Hỗn hợp Chuyên gia (MoE) [23]. Các mô hình MoE có nhiều mô hình con chuyên gia và một hàm định tuyến chọn cho mỗi đầu vào một tập con nhỏ các chuyên gia và chỉ định tuyến đầu vào đến họ. Các mạng nơ-ron với các lớp MoE, như Switch Transformer [10] trong NLP và V-MoE [22] trong Thị giác Máy tính, đã được chứng minh đạt được hiệu suất vượt trội so với các đối tác dày đặc của chúng, bằng cách cho phép mở rộng kích thước mô hình mà không tăng chi phí tính toán. Trong bài báo này, chúng tôi nghiên cứu câu hỏi sau: các mô hình MoE có bền vững đối kháng hơn các mô hình dày đặc không?

Nói chung, các mô hình MoE thưa thớt không liên tục, và do đó thậm chí không mượt. Những thay đổi nhỏ đối với đầu vào có thể khiến bộ định tuyến chọn một chuyên gia khác cho một ví dụ nhất định. Thật không may, trong một số trường hợp, những chuyên gia này có thể rất khác nhau, dẫn đến những thay đổi lớn đối với đầu ra. Một yếu tố khác ảnh hưởng đến tính bền vững của các mô hình MoE là hình học của dữ liệu đầu vào và định tuyến của nó - cách dữ liệu được chia giữa các chuyên gia. Điều này quyết định dữ liệu nào một chuyên gia được huấn luyện và do đó tính bền vững của họ. Cuối cùng, đã quan sát thấy rằng - trừ khi một số tổn thất phụ trợ cũng được áp dụng để khuyến khích sự cân bằng - số lượng chuyên gia được sử dụng có xu hướng sụp đổ thành rất ít, và những cái còn lại chỉ bị bỏ qua [22, 10]. Với những vấn đề ổn định này, không rõ ràng liệu MoE, mặc dù có nhiều tham số hơn, có bền vững hơn các mô hình dày đặc hay không.

Để nghiên cứu lý thuyết tác động của cả hai yếu tố này, (1) ổn định bộ định tuyến và (2) định tuyến dữ liệu, đối với tính bền vững, chúng tôi xem xét các mô hình MoE mượt, nơi mỗi đầu ra chuyên gia được cân bằng theo xác suất định tuyến của họ. Đặc biệt, các mô hình MoE nơi tất cả các chuyên gia đều được áp dụng đồng thời cho mọi đầu vào. Chúng tôi phân tích các mô hình với định tuyến cố định chứng minh rằng MoE với các chuyên gia tuyến tính có thể đạt được tính bền vững tốt hơn nếu dữ liệu được tách biệt tốt và được định tuyến phù hợp. Trong trường hợp cực đoan, MoE với E chuyên gia có thể có hằng số Lipschitz nhỏ hơn một hệ số 1/√E so với các mô hình dày đặc tương đương. Chúng tôi cũng đặc trưng cho ảnh hưởng của sự khác biệt giữa các chuyên gia về mặt tính bền vững, cho thấy rằng MoE có thể có hằng số Lipschitz cao khi hai chuyên gia rất khác nhau đối với các đầu vào cân bằng các chuyên gia một cách tương tự và cao. Chúng tôi chỉ ra rằng cả hai yếu tố này, định tuyến dữ liệu và sự khác biệt giữa các chuyên gia liên quan, đặc trưng cho hằng số Lipschitz của các mô hình MoE.

Tiếp theo, chúng tôi đánh giá tính bền vững thực nghiệm bằng cách sử dụng các cuộc tấn công đối kháng [20]. Chúng tôi chứng minh thực nghiệm rằng trên tập dữ liệu ImageNet [6], MoE bền vững hơn các mô hình dày đặc có cùng chi phí tính toán. Thú vị là chúng tôi quan sát thấy rằng các cuộc tấn công đối kháng dẫn đến những thay đổi định tuyến đáng kể của các đầu vào, nhưng không dẫn đến tính bền vững thấp hơn các mô hình dày đặc. Điều này cho thấy rằng trong thực tế, MoE bền vững với việc lựa chọn chuyên gia ở một mức độ nào đó. Tiếp theo, chúng tôi thực hiện huấn luyện đối kháng tiêu chuẩn cho cả mô hình dày đặc và MoE và một lần nữa quan sát thấy rằng MoE bền vững hơn chống lại các cuộc tấn công đối kháng.

Các đóng góp chính của công trình hiện tại như sau:

1. Chúng tôi đề xuất một khung lý thuyết đơn giản để hiểu tính bền vững của các mô hình Hỗn hợp Chuyên gia (MoE). Chúng tôi cung cấp các điều kiện đủ tổng quát và không tầm thường mà theo đó MoE có thể chứng minh được bền vững hơn so với các đối tác dày đặc của chúng cho thiết lập chuyên gia tuyến tính.

2. Chúng tôi thực hiện các thí nghiệm rộng rãi để chứng minh rằng trong thực tế MoE thực sự có tính bền vững tốt hơn so với các mô hình dày đặc đối với các cuộc tấn công đối kháng bị giới hạn chuẩn. Chúng tôi cũng khám phá ra các tính chất hấp dẫn của các cuộc tấn công đối kháng đối với các mô hình MoE và cho thấy rằng ngay cả đối với các mô hình MoE bền vững, các cuộc tấn công rất thường xuyên thay đổi định tuyến của các điểm dữ liệu, do đó chỉ ra mức độ dư thừa cao trong các mô hình như vậy.

## 2 Kiến thức cơ bản

### 2.1 Hỗn hợp Chuyên gia

Hỗn hợp Chuyên gia kết hợp đầu ra của các (tiểu) mô hình, tức là các chuyên gia, thông qua tổng có trọng số của đầu ra của chúng [14, 15, 28, 9]. Hỗn hợp Chuyên gia Thưa thớt điều kiện trọng số của tổng trong các đầu vào và chỉ kích hoạt K (trong số E) chuyên gia, trong đó K thường là một số nguyên rất nhỏ so với E (như K = 1 hoặc K = 2) [23, 10]. Hình thức tính toán có điều kiện này cho phép dễ dàng tăng số lượng tham số trong mô hình (gần như) độc lập với chi phí tính toán của nó [21]. Phương pháp này gần đây đã được áp dụng để tăng đáng kể kích thước mô hình và chất lượng của các mô hình được sử dụng trong Xử lý Ngôn ngữ Tự nhiên [23, 18, 19, 10, 8] và các ứng dụng Thị giác Máy tính [22, 27].

Cụ thể hơn, chúng tôi định nghĩa các phiên bản thưa thớt và mượt (hoặc dày đặc) của hỗn hợp chuyên gia dưới đây. Trong khi các công trình thực nghiệm trước đây đôi khi đã chọn nhiều hơn một chuyên gia cho mỗi đầu vào (K > 1), để đơn giản chúng tôi giới hạn bản thân trong trường hợp chỉ một chuyên gia được chọn cho mỗi đầu vào (K = 1). Gần đây, các mô hình thực tế cũng có xu hướng theo thiết lập này để khớp với FLOP của các mô hình dày đặc [10].

**MoE Thưa thớt.** Trong các mô hình này, chỉ một chuyên gia được chọn cho một ví dụ x nhất định dựa trên xác suất định tuyến p_i(x) của nó. Các mô hình này không phải là hàm liên tục của đầu vào. Một thay đổi nhỏ trong đầu vào có thể dẫn đến những thay đổi lớn trong đầu ra, do những thay đổi trong các chuyên gia được chọn. Một lớp MoE thưa thớt được định nghĩa là:

f(x) = ∑_{i=1}^E 𝟙{p_i(x) ≥ p_j(x), ∀j ≠ i} f_i(x)     (1)

Ở đây f_i(x) là các hàm chuyên gia riêng lẻ. 𝟙 là hàm chỉ thị có giá trị 1 nếu điều kiện được thỏa mãn và 0 nếu ngược lại. Lưu ý rằng để phá vỡ sự ràng buộc trong định nghĩa trên, trong trường hợp nhiều chuyên gia có cùng xác suất tối đa, chúng tôi chỉ đơn giản là lấy mẫu đồng đều một trong các chuyên gia có xác suất tối đa. Trong thực tế, các mô hình MoE có thể có nhiều lớp thưa thớt kết hợp với các lớp dày đặc. Hơn nữa, nhiễu ngẫu nhiên đôi khi được thêm vào p_i [23, 22].

Chúng tôi đề cập đến MoE chỉ với 1 chuyên gia là các mô hình dày đặc. Trong các mô hình dày đặc, là tiêu chuẩn trong tài liệu mạng nơ-ron, cùng một hàm được áp dụng cho tất cả các ví dụ đầu vào. Trong MoE thưa thớt được giới thiệu ở trên, các phần khác nhau của mạng/chuyên gia được áp dụng cho mỗi ví dụ đầu vào tùy thuộc vào chuyên gia nào được chọn. Tính linh hoạt này cho phép chúng tôi huấn luyện các MoE thưa thớt lớn hơn, có cùng chi phí tính toán với các mô hình dày đặc, vì chỉ một phần của MoE được kích hoạt/chọn cho mỗi đầu vào, với độ chính xác tốt hơn so với các mô hình dày đặc [10, 22].

**MoE Mượt.** Để phân tích lý thuyết về tính bền vững, chúng tôi xem xét một mô hình hỗn hợp chuyên gia mượt, vì chúng là các hàm liên tục. Trong trường hợp này, đầu ra của mỗi chuyên gia f_i được cân bằng theo xác suất định tuyến của nó:

f(x) = ∑_{i=1}^E p_i(x)f_i(x)     (2)

Xác suất định tuyến p_i(x) thường được tính bởi một lớp định tuyến, ví dụ: p_i(x) = (σ(Sx))_i, trong đó σ là softmax và S ∈ ℝ^{E×D} là một biến có thể huấn luyện. Các lớp định tuyến tuyến tính như vậy là lựa chọn phổ biến cho các mô hình MoE trong thực tế [10].

Mặc dù các mô hình này gần đây đã được chứng minh đạt được hiệu suất tối tân cho các nhiệm vụ trong NLP [10] và thị giác [22], nhưng không có nhiều công trình phân tích tính bền vững của chúng. Gần đây, Allingham và cộng sự [2] đã nghiên cứu thực nghiệm tính bền vững của MoE đối với nhiễu loạn tự nhiên trong dữ liệu (ví dụ: đánh giá trên các phiên bản bị hỏng khác nhau của CIFAR10 [17] và ImageNet [6]), và cho thấy MoE bền vững hơn đối với những nhiễu loạn tự nhiên này so với các mô hình dày đặc tương ứng. Theo hiểu biết tốt nhất của chúng tôi, công trình của chúng tôi là công trình đầu tiên phân tích lý thuyết tính bền vững của MoE và khám phá thực nghiệm nó trong bối cảnh nhiễu loạn đối kháng.

**Tổn thất cân bằng tải** Để ngăn MoE khỏi sụp đổ và luôn chọn cùng một chuyên gia cho tất cả đầu vào, thông thường trong quá trình huấn luyện sẽ thêm một tổn thất cân bằng tải, khuyến khích tỷ lệ bằng nhau của các đầu vào được định tuyến đến các chuyên gia khác nhau. Chúng tôi định nghĩa và trình bày các tổn thất này chi tiết trong phụ lục E.

### 2.2 Tính bền vững đối kháng

Tính bền vững đối kháng mô hình hóa tính nhạy cảm của một hàm đối với nhiễu loạn đối kháng đối với đầu vào của nó [25, 20]. Cụ thể hơn, cho một hàm f, hàm tổn thất ℓ và một đầu vào x ∈ ℝ^D, chúng ta có thể viết tổn thất đối kháng phát sinh tại x với nhãn y như sau:

max_{z: ||z|| ≤ ε} ℓ(f(x + z), y)     (3)

Ở đây ε là bán kính tấn công trên mỗi đầu vào. Đối với ràng buộc chuẩn (||z||), các lựa chọn phổ biến trong thực tế là các chuẩn ℓ_∞ và ℓ_2 [20, 4]. Độ chính xác đối kháng là độ chính xác của mô hình trên dữ liệu với nhiễu loạn thỏa mãn phương trình trên, tức là khi hàm tổn thất ℓ là tổn thất phân loại 0/1.

Vì tối ưu hóa phương trình 3 cho mạng nơ-ron về mặt tính toán là khó khăn, các phương pháp phổ biến để tìm các ví dụ đối kháng sử dụng các phương pháp tìm kiếm cục bộ như gradient ascent. Phương pháp Gradient Signed nhanh (FGSM) có lẽ là đơn giản nhất, chỉ với một bước cập nhật gradient [11]. Đối với nhiễu loạn bị giới hạn ℓ_∞, cập nhật FGSM như sau:

x + z = x + ε · sgn(∇_x ℓ(f(x), y))     (4)

sgn(·) là hàm dấu. Phương pháp Projected Gradient Descent (PGD) là một cuộc tấn công mạnh hơn thực hiện nhiều (τ) bước gradient ascent để tìm nhiễu loạn [20]. Quy tắc cập nhật cho nhiễu loạn bị giới hạn ℓ_∞ như sau, bắt đầu với x_0 = x:

x_{t+1} = Π_C(x_t + α · sgn(∇_x ℓ(f(x), y))), ∀t ∈ [0, τ]     (5)

α được chọn là ε/τ. Ở đây C là tập ràng buộc {z : ||z||_∞ ≤ ε} và Π là toán tử chiếu.

Các công trình hiện có đã thiết lập rằng mạng nơ-ron nhạy cảm với các cuộc tấn công đối kháng dẫn đến sự sụt giảm đáng kể trong độ chính xác của chúng [25]. Thú vị là việc tăng kích thước của mạng nơ-ron, do đó tăng khả năng của chúng, dẫn đến cải thiện độ chính xác đối kháng, cho thấy rằng các mạng nơ-ron lớn hơn bền vững hơn [20, 26, 1, 12]. Trong một công trình gần đây, Bubeck và Sellke [3] đã nghiên cứu hiện tượng này về mặt lý thuyết. Họ đã chứng minh một cận dưới về hằng số Lipschitz của bất kỳ hàm nào khớp với dữ liệu huấn luyện mà tỷ lệ nghịch với 1/√P với số lượng tham số hàm P. Tuy nhiên, không rõ liệu các hàm có nhiều tham số hơn, nhưng không nhất thiết có chi phí tính toán cao hơn (ví dụ: MoE), có thể đạt được tính bền vững tốt hơn hay không. Trong bài báo này, chúng tôi phân tích tính bền vững đối kháng của MoE, cả về mặt lý thuyết và thực nghiệm.

**Ký hiệu.** Chúng tôi sử dụng các chữ cái in đậm nhỏ, ví dụ: x, để biểu thị vectơ và các chữ cái in đậm viết hoa để biểu thị ma trận, ví dụ: W. Vô hướng được biểu thị bằng các chữ cái thường. [N] biểu thị tập số nguyên từ 1 đến N. ||·|| biểu thị chuẩn ℓ_2 trừ khi được chỉ định khác.

## 3 Phân tích tính bền vững

Trong phần này, chúng tôi trình bày các kết quả chính của chúng tôi về tính bền vững của các mô hình hỗn hợp chuyên gia. Trước tiên, chúng tôi trình bày một cận về hằng số Lipschitz của MoE cho các hàm chuyên gia f_i tổng quát. Mặc dù đây là cận trên cho các hàm tổng quát, nó vẫn làm nổi bật hai thành phần chính ảnh hưởng đến tính bền vững của các mô hình MoE.

### 3.1 Ổn định bộ định tuyến

Trong phần này, chúng tôi tính hằng số Lipschitz của MoE mượt với định tuyến có thể học. Cho:

f(x) = ∑_{i=1}^E p_i(x)f_i(x) trong đó p_i(x) = exp(⟨s_i, x⟩) / ∑_{j=1}^E exp(⟨s_j, x⟩)     (6)

và {s_i}_{i∈[E]}, với mỗi s_i ∈ ℝ^D, là các biến có thể học quyết định định tuyến của một ví dụ đến các chuyên gia khác nhau. Sau đó chúng tôi chứng minh cận trên sau về hằng số Lipschitz của các mô hình MoE với định tuyến có thể học.

**Bổ đề 1.** Cho {f_i}_{i∈[E]} là các hàm mượt với hằng số Lipschitz {L_{f_i}}_{i∈[E]} và cho L_f là hằng số Lipschitz của f. Cho s̄(x) = ∑_j p_j(x)s_j. Khi đó f(x) trong phương trình 6 thỏa mãn:

||∇_x f(x)|| ≤ ∑_{i=1}^E p_i(x)L_{f_i} + ||∑_{i=1}^E p_i(x)f_i(x)(s_i - s̄(x))||

và do đó

L_f ≤ max_{i∈[E]} {L_{f_i}} + sup_x ||∑_{i=1}^E p_i(x)f_i(x)(s_i - s̄(x))||     (7)

Bổ đề trên giới hạn hằng số Lipschitz của các mô hình MoE với hai số hạng phụ thuộc vào (1) định tuyến dữ liệu và (2) ổn định bộ định tuyến. Số hạng đầu tiên ở trên là từ hằng số Lipschitz riêng lẻ của các chuyên gia. Điều này phụ thuộc vào cách dữ liệu được phân vùng hoặc định tuyến đến các chuyên gia khác nhau, mà chúng tôi sẽ thảo luận chi tiết hơn trong phần tiếp theo. Số hạng thứ hai phát sinh từ bộ định tuyến được sử dụng để tính xác suất cho các chuyên gia khác nhau. Người ta có thể tự hỏi liệu chúng ta có thể làm cho số hạng này lớn tùy ý bằng cách tăng chuẩn của x hay không. Điều này không phải như vậy vì việc tăng chuẩn của x thường dẫn đến sự sụp đổ của xác suất định tuyến thành một chuyên gia duy nhất. Điều này làm cho p_i(x)(s_i - s̄(x)) bằng không cho tất cả các chuyên gia i ∈ [E].

Để nghiên cứu điều trên một cách cụ thể hơn, chúng tôi xem xét trường hợp E = 2 chuyên gia. Trong thiết lập này, số hạng thứ hai ở trên giảm thành ||p_1(x)f_1(x)(s_1 - s̄(x)) + (1 - p_1(x))f_2(x)(s_2 - s̄(x))||. Bây giờ, trong trường hợp cực đoan, khi p_1(x) là 0 hoặc 1, số hạng trên sụp đổ thành 0 vì p_i(x)f_i(x)(s_i - s̄(x)) bằng 0 trong trường hợp đó. Điều này cũng được mong đợi vì đối với các ví dụ được định tuyến đến một chuyên gia với xác suất cao, yếu tố chi phối chính là hằng số Lipschitz chuyên gia riêng lẻ, vì những nhiễu loạn nhỏ không gây ra nhiều thay đổi đối với xác suất.

Một thiết lập thú vị hơn là khi p_1(x) là 1/2. Trong thiết lập này, những nhiễu loạn nhỏ đối với đầu vào có thể khiến mô hình thay đổi đáng kể trọng số giữa các chuyên gia, và số hạng trên giảm thành ||1/4(f_1(x) - f_2(x))(s_1 - s_2)||. Do đó, các mô hình MoE có thể bị ảnh hưởng bởi hằng số Lipschitz lớn nếu hai chuyên gia rất khác nhau đối với các điểm trên ranh giới, tức là nếu |f_1(x) - f_2(x)| ≫ 0. Thay thế, nếu f_1(x) ≈ f_2(x) đối với các điểm trên ranh giới (tức là p_1(x) ≈ 1/2) thì số hạng này nhỏ.

Chúng tôi sẽ thấy sau này trong các thí nghiệm rằng MoE được huấn luyện trong thực tế thường có ổn định bộ định tuyến tốt và những thay đổi đối với định tuyến không dẫn đến sự suy giảm nhiều về độ chính xác cuối cùng.

### 3.2 Định tuyến dữ liệu

Trong phần này, chúng tôi sẽ nghiên cứu tác động của định tuyến dữ liệu đối với hằng số Lipschitz của các chuyên gia riêng lẻ - số hạng đầu tiên ở vế phải của phương trình 7 (bổ đề 1). Đặc biệt, chúng tôi sẽ cố gắng giải quyết việc max_{i∈[E]} {L_{f_i}} có thể lớn như thế nào so với hằng số Lipschitz của một mô hình dày đặc được huấn luyện trên cùng dữ liệu. Hướng tới mục đích này, chúng tôi sẽ phân tích lý thuyết tính bền vững của các mô hình MoE với định tuyến cố định được xác định trước và các chuyên gia tuyến tính, dưới ánh sáng của cấu trúc dữ liệu.

#### 3.2.1 Thiết lập

Chúng tôi xem xét thiết lập định tuyến cố định, trong đó chúng tôi giả định rằng định tuyến của các ví dụ riêng lẻ đến các chuyên gia được xác định trước và cố định. Hơn nữa, chúng tôi xem xét các mô hình tuyến tính làm chuyên gia.

Cụ thể hơn, chúng tôi xem xét N điểm đầu vào có chiều D được xếp chồng trong ma trận X ∈ ℝ^{N×D}, cùng với vectơ mục tiêu tương ứng y ∈ ℝ^N. Trong trường hợp dày đặc (tức là không có chuyên gia), chúng tôi muốn tìm một mô hình tuyến tính w ∈ ℝ^D để giảm thiểu:

min_{w∈ℝ^D} ||Xw - y||^2     (8)

Nghiệm bình phương tối thiểu [13] cho bài toán này giảm thiểu tổn thất MSE là w* = (X^T X)^{-1} X^T y. Ở đây ^{-1} biểu thị nghịch đảo giả. Mặt khác, đối với hỗn hợp chuyên gia, cho tập dữ liệu được chia thành E tập con S_1, ..., S_E, với X_i, y_i biểu thị dữ liệu trong tập S_i. Bây giờ cho dữ liệu từ tập S_i được định tuyến đến chuyên gia i. Dưới đây chúng tôi viết mục tiêu cho mỗi chuyên gia:

min_{w_i∈ℝ^D} ||X_i w_i - y_i||^2 cho i ∈ [E]     (9)

Tương tự, cho w_i* = (X_i^T X_i)^{-1} X_i^T y_i là nghiệm tối ưu cho chuyên gia i giảm thiểu MSE.

#### 3.2.2 Phân tích

Trước khi giới thiệu phân tích của chúng tôi liên quan đến hằng số Lipschitz của các mô hình dày đặc và MoE, trước tiên chúng tôi trình bày hai ví dụ để hiểu rõ hơn về cách định tuyến ảnh hưởng đến hằng số Lipschitz. Đặc biệt, chúng tôi xem xét một thiết lập đơn giản với E = 2 chuyên gia. Cho X^T = [X_1^T; X_2^T] là dữ liệu được định tuyến đến hai chuyên gia. Nhớ lại rằng hằng số Lipschitz của hàm f(X) = Xw là ||w||.

**Trường hợp X_1 ⊥ X_2.** Trong thiết lập này, chúng ta có:

||w*|| = ||(X^T X)^{-1} X^T y|| = ||(X_1^T X_1)^{-1} X_1^T y_1 + (X_2^T X_2)^{-1} X_2^T y_2|| = ||w_1* + w_2*|| ≤ max(||w_1*||, ||w_2*||)

Đẳng thức thứ hai theo các bước sau - 1) X^T X = X_1^T X_1 + X_2^T X_2, 2) (X_1^T X_1 + X_2^T X_2)^{-1} y = (X_1^T X_1)^{-1} y + (X_2^T X_2)^{-1} y, và 3) (X_1^T X_1)^{-1} X_2^T = 0; trong đó các bước 2 và 3 tuân theo tính trực giao của X_1 và X_2 (xem [16]). Do đó, các chuyên gia có hằng số Lipschitz nhỏ hơn khi dữ liệu được định tuyến đến các chuyên gia khác nhau nằm trong các không gian con trực giao.

**Trường hợp X_1 = X_2 và y_1 = y_2.** Trong trường hợp này, chúng ta thấy rằng:

||w*|| = ||(X^T X)^{-1} X^T y|| = ||(X_1^T X_1 + X_2^T X_2)^{-1} X^T y||
= ||(X_1^T X_1 + X_2^T X_2)^{-1} (X_1^T y_1 + X_1^T y_1)|| = 0 ≤ min(||w_1*||, ||w_2*||)

Do đó, các chuyên gia có hằng số Lipschitz tệ hơn khi dữ liệu được định tuyến đến các chuyên gia khác nhau được căn chỉnh.

Hai ví dụ đơn giản này minh họa trong những điều kiện nào các chuyên gia có lợi thế so với một mô hình dày đặc duy nhất và khi nào thì không.

Bây giờ chúng tôi trình bày kết quả chính của chúng tôi. Để nắm bắt mối quan hệ này giữa hình học dữ liệu và định tuyến, chúng tôi giới thiệu các đại lượng sau. Cho {U_i}_{i∈[E]} là các ma trận chiếu lên các không gian con trực giao trong ℝ^D. Một cách để xây dựng chúng là bằng cách lấy các vectơ riêng của X^T X trước và gán chúng cho tập i với chiếu lớn nhất. Điều này đảm bảo rằng U_i trực giao với U_j với j ≠ i, và chúng có sự căn chỉnh lớn nhất với không gian con được bao bởi X_i.

Trước tiên, chúng tôi định nghĩa một đại lượng để nắm bắt mức độ U_i nắm bắt khoảng của tập con dữ liệu X_i.

**Định nghĩa 1 (Khoảng cách trong không gian con: ε_1).** ε_1 ≥ 0, ||U_i U_i^T (X^T X)^{-1} X^T y - (X_i^T X_i)^{-1} X_i^T y_i||_2 ≤ ε_1, ∀i ∈ [E].

Ở đây ||·||_2 cho một ma trận biểu thị chuẩn phổ. ε_1 nhỏ khi, ∀i, U_i nắm bắt X_i hoàn hảo.

Tiếp theo, chúng tôi định nghĩa khoảng cách chiếu giữa dữ liệu từ hai tập con khác nhau.

**Định nghĩa 2 (Khoảng cách giữa các không gian con: ε_2).** ε_2 ≥ 0 sao cho đối với bất kỳ z nào trong khoảng của X_j^T, chúng ta có ||(X_i^T X_i)^{-1} X_i^T y_i - z||_2 ≤ ε_2 ||z||, ∀i ≠ j.

Ở đây ε_2 nhỏ nếu dữ liệu trong các tập con khác nhau X_i nằm trong các không gian con trực giao.

**Định lý 1.** Cho w* là nghiệm cực tiểu của phương trình 8 và {w_i*}_{i∈[E]} là nghiệm cực tiểu của phương trình 9. Khi đó:

||w*||^2 ≥ ∑_{i=1}^E ([||w_i*|| - ε_1 ||X^T y||_2 - ε_2 ∑_{j≠i} ||X_j^T y_j||]_+)^2

trong đó [·]_+ biểu thị chiếu lên các số không âm.

Kết quả trên đưa ra cận dưới cho hằng số Lipschitz của mô hình dày đặc ||w*|| theo hằng số Lipschitz của các chuyên gia ||w_i*|| trong mô hình MoE. Chúng tôi trình bày chứng minh của định lý này trong phụ lục A.

Trong trường hợp ε_1 = ε_2 = 0, chúng ta có được cận ||w*|| ≥ √(∑_{i=1}^E ||w_i*||^2). Giả định một thiết lập cân bằng trong đó tất cả các chuyên gia có cùng tham số chuẩn, chúng ta có được tỷ lệ ||w*|| ≥ O(√E) max_{i∈[E]} {||w_i*||}. Do đó, trong thiết lập này, các chuyên gia có hằng số Lipschitz nhỏ hơn một hệ số 1/√E so với các mô hình dày đặc. Điều này xảy ra khi dữ liệu nằm trong E không gian con trực giao và dữ liệu từ mỗi không gian con được định tuyến đến cùng một chuyên gia. Điều này cho thấy rằng các mô hình MoE có thể có hằng số Lipschitz nhỏ hơn đáng kể so với các đối tác dày đặc của chúng, trong khi có cùng chi phí tính toán. Đối với MoE trong thực tế, dữ liệu được định tuyến đến các chuyên gia khác nhau thực sự hiển thị một số phân cụm của các đặc trưng (xem Hình 7 trong Riquelme và cộng sự [22]).

Khi dữ liệu trong các phân vùng khác nhau X_i trở nên căn chỉnh hơn, ε_1 và ε_2 tăng lên và giảm khoảng cách giữa hằng số Lipschitz của mô hình dày đặc và MoE. Điều này làm giảm khoảng cách trong hằng số Lipschitz của mô hình dày đặc và các chuyên gia. Trong trường hợp cực đoan, nếu tất cả các điểm dữ liệu đều giống nhau, thì ε_1 và ε_2 lớn, loại bỏ sự khác biệt này. Trong phụ lục D, chúng tôi đưa ra một kết quả bổ sung liên quan đến hằng số Lipschitz của các chuyên gia với mô hình dày đặc khi cả ε_1 và ε_2 có thể lớn.

**Kết nối với Bubeck và Sellke [3].** Bubeck và Sellke [3] đã chứng minh một cận dưới phổ quát về hằng số Lipschitz của một hàm cần thiết để δ-ghi nhớ N mẫu huấn luyện trong D chiều:

L_f ≥ Ω̃(√(ND/P))

Đối với mô hình tuyến tính được xem xét ở trên, điều này giảm thành Ω̃(√N) vì số lượng tham số là D. Vì MoE có E lần nhiều tham số hơn, chúng có cận dưới Ω̃(√(N/E)), tức là cận dưới về hằng số Lipschitz của MoE nhỏ hơn một hệ số √(1/E). Kết quả của chúng tôi cho thấy rằng cận dưới này thực sự có thể đạt được, và do đó chặt chẽ, khi dữ liệu nằm trong E không gian con trực giao và dữ liệu từ mỗi không gian con được định tuyến đến cùng một chuyên gia.

## 4 Thí nghiệm

### 4.1 Thiết lập

Chúng tôi so sánh tính bền vững của các mô hình Vision Transformer (ViT) [7] và Vision MoE (V-MoE) [22] chống lại các cuộc tấn công đối kháng. Đặc biệt, chúng tôi sử dụng các mô hình ViT-B/32 và V-MoE-B/32 trong tất cả các thí nghiệm. Các mô hình này có cùng kiến trúc cơ bản nhưng mô hình sau thay thế một trong hai lớp feedforward bằng Hỗn hợp Chuyên gia thưa thớt, chọn K = 2 trong số E = 32 chuyên gia feedforward được áp dụng trên mỗi token. Người ta có thể lập luận rằng bộ định tuyến trong mô hình V-MoE giới thiệu một chi phí phụ cần được tính đến. Do đó, chúng tôi đã huấn luyện một phiên bản lớn hơn của ViT-B/32 dày đặc (mà chúng tôi đặt tên là ViT-B++/32) đạt được hiệu suất dự đoán gần như giống mô hình V-MoE, nhưng có chi phí cao hơn. Chi phí đánh giá một hình ảnh trên các mô hình ViT dày đặc là 8.9 và 17.9 GFLOP và chi phí thời gian chạy tương ứng; và 12.4 GFLOP trên mô hình V-MoE. Phụ lục B chứa các chi tiết thí nghiệm bổ sung.

Chúng tôi tiền huấn luyện các mô hình của mình trên tập dữ liệu riêng tư JFT-300M [24] trong 7 epoch (517 859 bước với kích thước batch 4 096 hình ảnh), sử dụng độ phân giải hình ảnh 224×224 pixel và tăng cường dữ liệu tiêu chuẩn (crop inception và lật ngang). Vì JFT-300M là tập dữ liệu đa nhãn, chúng tôi giảm thiểu tổn thất sigmoid cross-entropy. V-MoE cũng thêm các tổn thất phụ trợ để khuyến khích tải cân bằng cho tất cả các chuyên gia; chúng tôi đã sử dụng cùng công thức như trong [22]. Trong cả hai trường hợp, chúng tôi sử dụng Adam (β_1 = 0.9, β_2 = 0.999), với tỷ lệ học đỉnh 8×10^{-4}, đạt được sau khi làm ấm tuyến tính 10^4 bước và sau đó giảm tuyến tính xuống giá trị cuối cùng 10^{-5}. Weight decay 0.1 được sử dụng trên tất cả các tham số. Đây là cùng giao thức tiền huấn luyện như được sử dụng trong [7, 22].

Sau khi tiền huấn luyện, các mô hình được tinh chỉnh trên ImageNet [6], ở độ phân giải 384×384 pixel và cùng tăng cường dữ liệu như trước, tổng cộng 10^4 bước, sử dụng kích thước batch 4 096 hình ảnh. SGD với Momentum (μ = 0.9) được sử dụng để tinh chỉnh, với tỷ lệ học đỉnh 0.03, đạt được sau khi làm ấm tuyến tính 500 bước, và theo sau bằng cosine decay đến giá trị cuối cùng 10^{-5}. Chuẩn của vectơ gradient được làm phẳng được cắt đến giá trị tối đa 10. Vì hình ảnh ImageNet có một nhãn duy nhất, chúng tôi giảm thiểu tổn thất softmax cross-entropy trong quá trình tinh chỉnh. Cùng giao thức tinh chỉnh đã được áp dụng trong [7, 22].

Chúng tôi đánh giá tính bền vững đối kháng của cả mô hình tiền huấn luyện và tinh chỉnh, bằng các cuộc tấn công đối kháng PGD [20]. Chúng tôi tối đa hóa tổn thất tương ứng (sigmoid hoặc softmax cross-entropy), thay đổi ràng buộc chuẩn ℓ_∞ trên hình ảnh đầu vào, tổng cộng τ = 40 bước.

### 4.2 Tính bền vững đối kháng của V-MoE

Trong quá trình tiền huấn luyện, mô hình ViT-B/32 đạt precision at 1 là 39.3%, và V-MoE-B/32 đạt precision-at-1 là 43.5% (ngược lại, false discovery rate at 1 là 60.7% và 56.5% tương ứng). Sau khi tinh chỉnh, lỗi phân loại đạt được bởi mỗi mô hình trên ImageNet là 19.3% và 17.8% tương ứng.

Hình 1 hiển thị trong các đường liền false discovery rate (trái) và classification error rate (phải) như một hàm của ràng buộc ℓ_∞. Mặc dù thực tế là mô hình V-MoE chứa một bộ định tuyến thực hiện các lựa chọn riêng biệt giữa các chuyên gia có điều kiện trên đầu vào, có thể dẫn đến điểm yếu nghiêm trọng chống lại kẻ tấn công đối kháng, chúng ta có thể quan sát thấy rằng nó theo cùng xu hướng với mô hình ViT dày đặc cơ bản. Nó có thể duy trì lỗi thấp hơn trong một phạm vi rộng các giá trị ℓ_∞. Một phiên bản lớn hơn của mô hình dày đặc khớp với chất lượng của V-MoE có chi phí cao hơn nhiều.

Ngoài ra, chúng tôi cũng tinh chỉnh các mô hình ViT cơ bản và V-MoE trên ImageNet bằng huấn luyện đối kháng PGD. Chúng tôi sử dụng cùng công thức như trên nhưng chúng tôi thực hiện cuộc tấn công PGD 10 bước trên hình ảnh đầu vào, với ℓ_∞ = 8/255 cố định, trước khi tính gradient của các tham số mô hình và cập nhật chúng. Lỗi phân loại trên tập dữ liệu ImageNet gốc đạt được bởi mỗi mô hình sau khi tinh chỉnh đối kháng là 51.7% và 49.8%, cho cả hai mô hình tương ứng. Hình 1 (phải) hiển thị trong các đường đứt nét lỗi phân loại khi các mô hình này được đánh giá chống lại kẻ tấn công đối kháng sử dụng các ràng buộc ℓ_∞ khác nhau. Khi ℓ_∞ tăng, lợi ích của tinh chỉnh đối kháng để bảo tồn độ chính xác được hiển thị trong cả hai trường hợp. Một lần nữa, cả hai mô hình báo cáo xu hướng tương tự, và mô hình V-MoE cho thấy tính bền vững tốt hơn cho một phạm vi rộng các giá trị ℓ_∞.

### 4.3 Tác động của các cuộc tấn công đối kháng lên các chuyên gia được chọn

Như được mô tả trong phần 3.1, trong vùng gần ranh giới quyết định của bộ định tuyến, nếu hai chuyên gia có đầu ra rất khác nhau, hằng số Lipschitz của mô hình MoE có thể cao hơn nhiều so với mô hình dày đặc tương tự. Nếu chỉ số của các chuyên gia được chọn thay đổi đáng kể, nhưng mô hình vẫn cho thấy độ chính xác khá cao dưới các cuộc tấn công đối kháng (so với mô hình dày đặc), điều này sẽ cho thấy rằng đầu ra của hai tập chuyên gia được chọn không khác nhau nhiều.

Hình 2 hiển thị tỷ lệ thay đổi trong bộ định tuyến như một hàm của ℓ_∞ được sử dụng trong cuộc tấn công đối kháng, trên các lớp khác nhau của mô hình V-MoE-B/32 có lớp MoE. Đối với mỗi token được xử lý bởi mô hình, chúng tôi tính intersection-over-union (IoU) của tập chuyên gia được chọn trước và sau cuộc tấn công đối kháng. Chúng tôi tính trung bình IoU trên tất cả các token được xử lý và định nghĩa tỷ lệ thay đổi định tuyến là phần bù của IoU trung bình.

Trong các đường đứt nét, chúng tôi báo cáo tỷ lệ thay đổi của mô hình V-MoE được tinh chỉnh bằng huấn luyện đối kháng PGD. Không chỉ mô hình có độ chính xác tốt hơn chống lại các cuộc tấn công đối kháng khi ℓ_∞ tăng, như được báo cáo trong hình 1, mà tỷ lệ thay đổi định tuyến cũng thường thấp hơn trên tất cả các lớp.

Mặc dù thực tế là một phần đáng kể các lựa chọn thay đổi trên mỗi lớp khi ℓ_∞ tăng, hình 1 cho thấy rằng mô hình V-MoE vẫn giữ được lợi thế chống lại các cuộc tấn công đối kháng so với mô hình ViT. Điều này cho thấy rằng trong các vùng gần ranh giới quyết định của bộ định tuyến, hai chuyên gia tương ứng có đầu ra tương tự, do đó ngăn chặn mô hình V-MoE đầy đủ khỏi việc kém bền vững hơn trong thực tế. Ngược lại, thực tế là mô hình V-MoE có chất lượng cơ bản tốt hơn mô hình ViT đối tác, cho thấy rằng các chuyên gia không tương đương đối với các vùng xa ranh giới quyết định, nếu không nó sẽ giảm thành mô hình dày đặc.

### 4.4 Tấn công các tổn thất phụ trợ của bộ định tuyến

Các mô hình MoE thưa thớt thường sử dụng các tổn thất phụ trợ để cân bằng tải giữa tất cả các chuyên gia. Đặc biệt, trong việc thực hiện V-MoE, nếu tải của các chuyên gia không cân bằng cao, các chuyên gia nhận được nhiều token hơn đáng kể so với trung bình có thể bỏ qua tất cả các token vượt quá khả năng của chuyên gia, có thể dẫn đến hiệu suất tệ hơn đáng kể. Câu hỏi là liệu kẻ tấn công đối kháng có thể khai thác tính chất này trong thực tế hay không. Hình 3 (trái) hiển thị false discovery rate trên JFT-300M, cho mô hình V-MoE khi các tổn thất phụ trợ của bộ định tuyến được tối đa hóa trong cuộc tấn công đối kháng, cùng với tổn thất cross-entropy tương ứng. Chúng tôi sử dụng cùng trọng số cho các tổn thất phụ trợ như trọng số được sử dụng để huấn luyện các mô hình. Hình cho thấy rằng tấn công tổn thất phụ trợ không mang lại lợi thế đáng kể nào cho kẻ tấn công. Kết quả tương tự về lỗi phân loại trên ImageNet (không hiển thị ở đây vì lợi ích của không gian).

### 4.5 Tăng kích thước mô hình bằng cách tăng tổng số chuyên gia

Trong phần 1, chúng tôi đã hỏi liệu, cho rằng cận dưới về hằng số Lipschitz được đưa ra bởi Bubeck và Sellke [3] là bất khả tri đối với chi phí tính toán của hàm, chúng ta có thể làm cho mô hình bền vững hơn bằng cách tăng kích thước mô hình mà không tăng tổng chi phí không? Ở đây chúng tôi đo lường việc tăng tổng số chuyên gia trong mô hình V-MoE giúp ích như thế nào chống lại các cuộc tấn công đối kháng. Lưu ý rằng việc tăng tổng số chuyên gia E không làm cho mô hình V-MoE đắt hơn. Hình 3 (giữa và phải) hiển thị false discovery rate trên JFT-300M và lỗi phân loại trên ImageNet cho số lượng chuyên gia tăng dần. Tất cả các mô hình V-MoE chọn K = 2 chuyên gia cho mỗi token.

Một mặt, việc tăng tổng số chuyên gia cải thiện tính bền vững trên JFT-300M lên đến E = 16 chuyên gia. Các đường cong cho E = 16 và E = 32 rất chồng chéo, do đó bất kỳ sự khác biệt nào rất có thể do nhiễu trong quá trình huấn luyện và tinh chỉnh. Mặt khác, khi các V-MoE được tinh chỉnh trên ImageNet, tất cả các mô hình với hơn hai chuyên gia đạt được độ chính xác gần như giống nhau dưới các cuộc tấn công đối kháng.

Điều này cho thấy rằng, mặc dù các kết quả được trình bày trong phần 3 cho thấy tính bền vững tốt hơn của MoE đòi hỏi một số giả định, các kết luận có giá trị ở một mức độ nào đó trong các tình huống thực tế. Việc tăng số lượng tham số bằng cách tăng số lượng chuyên gia là một cách hiệu quả để cải thiện tính bền vững của mô hình.

### 4.6 Tính bền vững chống lại các cuộc tấn công AutoPGD

Chúng tôi đã tiến hành các thí nghiệm bổ sung sử dụng cuộc tấn công đối kháng tinh vi hơn, AutoPGD [5], chọn kích thước bước để sử dụng trong mỗi cập nhật của cuộc tấn công. Chúng tôi cũng tăng số lượng bước được thực hiện trong cuộc tấn công, sử dụng τ = 100 bước. Hình 4a hiển thị kết quả đạt được bởi hai mô hình dày đặc (ViT-B/32 và ViT-B++/32) và một mô hình thưa thớt (V-MoE-B/32). Mặc dù AutoPGD hiệu quả hơn một chút như một cuộc tấn công đối kháng chống lại tất cả các phương pháp (ví dụ: với ℓ_∞ = 10^{-3} false discovery rate trên JFT-300M của ViT-B/32 là 0.744 sử dụng PGD và 0.757 sử dụng AutoPGD), xu hướng của tất cả các mô hình giống hệt như được thể hiện trong hình 1.

Hình 4b hiển thị tỷ lệ thay đổi định tuyến trong các lớp MoE khác nhau trong mô hình V-MoE-B/32. Hình này tương tự như hình 2 hiển thị kết quả cho cuộc tấn công PGD tiêu chuẩn. So với nó, AutoPGD có thể thay đổi phần lớn hơn các chuyên gia được chọn. Ví dụ: phần thay đổi tối đa trong Lớp 1 sử dụng PGD gần 0.2, trong khi AutoPGD tăng nó lên khoảng 0.5.

AutoPGD đưa ra cùng kết luận như các cuộc tấn công PGD: các mô hình sử dụng các lớp MoE thưa thớt cung cấp tính bền vững tốt hơn chống lại các cuộc tấn công đối kháng (mỗi GFLOP) so với các mô hình dày đặc, mặc dù thực tế là bản thân bộ định tuyến có thể khá nhạy cảm với các cuộc tấn công này.

## 5 Kết luận

Trong công trình này, chúng tôi đã phân tích tính bền vững đối kháng của MoE cho thấy lợi thế của chúng so với các mô hình dày đặc, với nhiều chuyên gia hơn dẫn đến tính bền vững tốt hơn, cả về mặt lý thuyết và thực nghiệm. Chúng tôi đã chỉ ra cách các tính chất của dữ liệu và định tuyến của nó đóng vai trò quan trọng trong việc học các MoE bền vững.

Mặc dù có một số bằng chứng rằng định tuyến được học bởi MoE trong thực tế hiển thị một số phân cụm của các đặc trưng trong một số lớp của mô hình (xem Hình 7 trong Riquelme và cộng sự [22]), hiện tại nó không được khuyến khích rõ ràng trong quá trình huấn luyện. Do đó, việc phát triển các chiến lược định tuyến thông minh hơn có tính đến hình học dữ liệu có thể là một hướng nghiên cứu thú vị trong tương lai. Hiện tại phân tích của chúng tôi bị giới hạn ở các mô hình tuyến tính, mở rộng điều này cho các mô hình tổng quát và suy ra sự phụ thuộc của định tuyến tối ưu vào chúng là một hướng nghiên cứu hứa hẹn khác.

Chúng tôi cũng đã chỉ ra rằng, đối với các đầu vào cân bằng hai chuyên gia một cách tương tự, nếu hai giá trị chuyên gia rất khác nhau, thì MoE có thể bị ảnh hưởng bởi hằng số Lipschitz cao hơn. Tuy nhiên, đối với các mô hình được huấn luyện trong thực tế, chúng tôi thấy dự đoán của chúng tương đối ổn định, mặc dù có những thay đổi đáng kể trong việc lựa chọn chuyên gia, làm nổi bật sự dư thừa tiềm năng của các chuyên gia đã học. Tuy nhiên, quá nhiều dư thừa, với tất cả các chuyên gia học các hàm tương tự là lãng phí khả năng và có thể ảnh hưởng đến hiệu suất mô hình. Do đó, đây là một vấn đề nghiên cứu thú vị để cân bằng tính bền vững và độ chính xác của MoE bằng cách kiểm soát sự dư thừa của các chuyên gia.

**Lời cảm ơn và Tiết lộ Tài trợ**

Tất cả các tác giả muốn cảm ơn Sven Gowal đã chia sẻ codebase tấn công AutoPGD của họ. Chúng tôi cũng cảm ơn Neil Houlsby, Basil Mustafa và André Susano Pinto đã cung cấp phản hồi sâu sắc trong khi làm việc trên dự án này.
