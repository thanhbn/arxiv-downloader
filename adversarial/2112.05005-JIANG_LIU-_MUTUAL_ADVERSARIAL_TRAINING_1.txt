# 2112.05005.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/adversarial/2112.05005.pdf
# File size: 558048 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JIANG LIU: MUTUAL ADVERSARIAL TRAINING 1
Mutual Adversarial Training:
Learning together is better than going alone.
Jiang Liu1
jiangliu@jhu.edu
Chun Pong Lau1
clau13@jhu.edu
Hossein Souri1
hsouri1@jhu.edu
Soheil Feizi2
sfeizi@cs.umd.edu
Rama Chellappa1
rchella4@jhu.edu1Johns Hopkins University,
Baltimore, Maryland, USA
2University of Maryland, College Park
College Park, Maryland, USA
Abstract
Recent studies have shown that robustness to adversarial attacks can be transferred
across networks. In other words, we can make a weak model more robust with the help
of a strong teacher model. We ask if instead of learning from a static teacher, can mod-
els “learn together” and “teach each other” to achieve better robustness? In this paper,
we study how interactions among models affect robustness via knowledge distillation.
We propose mutual adversarial training (MAT), in which multiple models are trained
together and share the knowledge of adversarial examples to achieve improved robust-
ness. MAT allows robust models to explore a larger space of adversarial samples, and
ﬁnd more robust feature spaces and decision boundaries. Through extensive experiments
on CIFAR-10 and CIFAR-100, we demonstrate that MAT can effectively improve model
robustness and outperform state-of-the-art methods under white-box attacks, bringing
8% accuracy gain to vanilla adversarial training (AT) under PGD-100 attacks. In ad-
dition, we show that MAT can also mitigate the robustness trade-off among different
perturbation types, bringing as much as 13.1% accuracy gain to AT baselines against the
union of l¥,l2andl1attacks. These results show the superiority of the proposed method
and demonstrate that collaborative learning is an effective strategy for designing robust
models.
1 Introduction
In recent years, we have witnessed the great success of deep neural networks (DNNs) in var-
ious ﬁelds of artiﬁcial intelligence including computer vision [20], speech recognition [15],
and robot control [26]. Despite their superior performance, DNNs are shown to be vulnera-
ble to adversarial attacks that add imperceptible manipulations to inputs [5, 6, 9, 12, 17, 21,
© 2021. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.arXiv:2112.05005v1  [cs.LG]  9 Dec 2021

--- PAGE 2 ---
2 JIANG LIU: MUTUAL ADVERSARIAL TRAINING
Table 1: Comparisons of KD-based defenses. "Robust" means the model is trained with
adversarial examples and "Natural" means the model is trained with natural examples.
Method Teacher Model Student Model Form of KD Multi-Perturbations
DD [32] Natural Natural Ofﬂine 7
ARD [11] Robust Robust Ofﬂine 7
ACT [1] Natural Robust Online 7
MAT (Ours) Robust Robust Online 3
22, 28, 31]. This poses a huge challenge in security critical applications such as autonomous
driving and medicine.
To enhance model robustness against adversarial attacks, many defense methods have
been proposed including empirical [18, 27, 28, 32, 37, 44, 46] and certiﬁable defenses [8, 23,
24, 25, 33, 38]. Adversarial training (AT) [28] is considered to be one of the most effective
algorithms for adversarial defenses. There have been many works for improving adversarial
training by using different loss functions, such as ALP [18], TRADES [46], and MART [44].
However, they only train one model without considering the synergy of a network cohort. In
addition, most defense methods focus on a single perturbation type, which can be vulnerable
to unseen perturbation types [29, 40]. For example, models adversarially trained on l¥-
bounded adversarial examples can be vulnerable to l1andl2-bounded attacks.
Knowledge distillation (KD) [16] is a well-known method for transferring knowledge
learned by one model to another. There are many forms of KD [43] including ofﬂine KD,
where the teacher models are pretrained and the students learns from static teachers[16],
andonline KD, where a group of student models learn from peers’ predictions [13, 39, 47].
Several techniques based on KD have been proposed for adversarial defenses [1, 7, 11, 32].
[11] demonstrates that robustness can be transferred among models through KD and [7]
shows that KD can help mitigate robust overﬁtting. Table 1 summarizes the differences
among KD-based defenses. We argue that current KD-based defenses are sub-optimal in
terms of improving adversarial robustness. Defensive distillation (DD) trains a natural model
with another natural model as the teacher, which cannot provide strong robustness as both the
teacher and student are not adversarially trained, and it is broken by [4]. Adversarially robust
distillation (ARD) [11] trains a robust model with another robust model as the teacher to
distill the robustness of a large network onto a smaller student, which relies on the existence
of strong teacher models, and the improvement of the student model is limited as the teacher
is ﬁxed. Adversarial concurrent training (ACT) [1] trains a natural and a robust model jointly
in an online KD manner to align the feature space of both. However, since natural models
and robust models learn fundamentally different features [42], aligning the feature space of
a robust model with a natural model may result in degraded robustness.
In this paper, we propose Mutual Adversarial Training (MAT) that allows models to
share their knowledge of adversarial robustness and teach each other to be more robust.
Unlike previous KD-based defenses, we train a group of robust models simultaneously, and
each network not only learns from ground-truth labels as in standard AT, but also the soft
labels from peer networks that encode peers’ knowledge for defending adversarial attacks to
achieve stronger robustness. The architecture of MAT is shown in Fig. 1.
MAT improves model robustness through many aspects: 1) MAT inherits the beneﬁts
of KD, such as improving generalization [10] and reducing robust overﬁtting [7]. 2) MAT
creates a positive feedback loop of increasing model robustness . Each network serves as
a robust teacher to provide semantic-aware and discriminative soft labels to its peers. By
learning from strong peers, a network becomes more robust, which in turn improves the ro-

--- PAGE 3 ---
JIANG LIU: MUTUAL ADVERSARIAL TRAINING 3
Figure 1: Mutual adversarial training (MAT) architecture. xis a clean image, xq1
advis the
adversarial image of network h1, and xq2
advis the adversarial image of network h2.
bustness of its peers. 3) MAT allows robust models to explore a larger space of adversarial
samples, and ﬁnd more robust feature spaces and decision boundaries jointly . The adversar-
ial examples of each model forms a subspace of adversarial inputs [41], and the predictions
of each model encode information about its decision boundary and adversarial subspace. In
MAT, each model not only learns from its own adversarial examples, but also receives infor-
mation about peers’ adversarial examples through the soft labels. In this way, each model
needs to consider a larger space of adversarial samples, and ﬁnd a feature space and deci-
sion boundary that not only work well on its own adversarial examples but also on peers’
adversarial examples, which encourages solutions that are more robust and generalizable.
To summarize, in this paper we propose a novel KD-based adversarial training algorithm
named MAT. MAT is a general framework for boosting adversarial robustness of any net-
work without the need for strong teacher models. We further extend MAT for defending
against multiple perturbation types (MAT-MP) by exploiting the transferability of adversar-
ial robustness, and propose several training strategies for training MAT-MP. Our extensive
experiments show that MAT brings signiﬁcant robustness improvements to AT baselines and
outperforms state-of-the-art methods for both single and multiple perturbations.
2 Mutual Adversarial Training
2.1 Our Framework
In this paper, we consider the K-class (K2)image classiﬁcation problem. We have image
and label tuples (x;y)drawn from an underlying data distribution D, where x2Rdis a
natural image, y=f1;;Kgis its corresponding class label. We formulate the proposed
mutual adversarial training algorithm with a cohort of two networks h1(;q1)andh2(;q2).
To train a robust h1, we minimize the classiﬁcation loss on the adversarial examples as
in adversarial training. The AT loss is then deﬁned as:
LAT1=LC(p1(xq1
adv);y); (1)

--- PAGE 4 ---
4 JIANG LIU: MUTUAL ADVERSARIAL TRAINING
where p1is the output probability of h1,LCis a classiﬁcation loss, and xq1
advis an adversarial
example crafted for h1,xq1
adv=x+argmaxd2SLC(p1(x+d);y). ForLC, we use the boosted
cross entropy loss as in MART [44], which uses a margin term to improve the decision
margin of the classiﬁer: LC(p(x);y) = log(p(x)y) log(1 max k6=yp(x)k).
To boost the robustness of h1, we use the peer network h2to provide its knowledge of
robustness to h1. Speciﬁcally, we use the KD loss to guide the prediction of h1:
LKD1=DKL(p2(x)jjp1(xq1
adv)); (2)
where DKL(jj)is the Kullback–Leibler (KL) divergence and p2is the output probability
ofh2. Note that we use the clean image xas the input to h2to generate the soft label p2
because adversarial examples can mislead h2to generate wrong predictions. The KD loss
aligns the feature spaces and decision boundaries of MAT models and allows them to ﬁnd
robust features and decision boundaries jointly. Similarly, we deﬁne LAT2andLKD2forh2:
LAT2=LC(p2(xq2
adv);y);LKD2=DKL(p1(x)jjp2(xq2
adv)); (3)
where xq2
advis an adversarial example crafted for h2,xq2
adv=x+argmaxd2SLC(p2(x+d);y).
The overall loss function for MAT is:
LMAT = (1 a)(LAT1+LAT2) +a(LKD1+LKD2); (4)
where ais a hyper-parameter that controls the trade-off between learning from ground-truth
labels and the peer network. The MAT algorithm for the case of two classiﬁers is summarized
in Alg. 1.
The extension of MAT to larger student cohorts is straightforward. Consider N(N2)
networks hn(;qn)(n=1;;N), the loss function of MAT becomes:
LMAT = (1 a)LAT+a
M 1LKD; (5)
whereLAT=åN
n=1LC(pn(xqn
adv);y), andLKD=åN
n=1åN
m6=nDKL(pm(x)jjpn(xqn
adv)).
2.2 Defense against Multiple Perturbations
In this section, we demonstrate how MAT can improve robustness to multiple perturbations.
Previous methods [29, 40] attempt to improve model robustness against multiple pertur-
bations by augmenting the training data. We take a different approach by transferring the
robustness of specialist models to a single model, which complements previous methods and
achieves better performance.
Given M(M2)different perturbation types, since it is difﬁcult for a single model to
be robust to all perturbations, we train an ensemble of M+1 networks including one gen-
eralist network h0(;q0)andMspecialist networks h1(;q1);h2(;q2);;hM(;qM). Each
specialist network is responsible for learning to defend against a speciﬁc perturbation and
learns from peer networks to boost its robustness. The loss for training specialist networks
hm(1mM)is deﬁned as:
Lm= (1 a)LC(pm(xqm
advm);y) +a
MM
å
n6=mDKL(pn(x)jjpm(xqm
advm)); (6)

--- PAGE 5 ---
JIANG LIU: MUTUAL ADVERSARIAL TRAINING 5
Algorithm 1 Mutual Adversarial Training (MAT)
Input: Training samplesXY , classiﬁer h1(;q1)andh2(;q2), attack model attack ,
learning rate t, weight a.
1:Randomly initialize q1,q2
2:forepoch = 1, . . . , N do
3: forminibatch (x;y)XY do
4: xq1
adv=attack (x;y;h1(;q1))
5: .Generate adversarial examples of h1
6: xq2
adv=attack (x;y;h2(;q2))
7: .Generate adversarial examples of h2
8:LMAT = (1 a)(LAT1+LAT2) +a(LKD1+LKD2) .Compute loss
9: q1 q1 tÑq1LMAT
10: q2 q2 tÑq2LMAT
11: .Update q1andq2with gradient descent
12: end for
13:end for
where xqm
advmis an adversarial example of hmgenerated by the m-th attack model.
The generalist network h0learns to defend against all perturbations with the help of
specialist networks. Speciﬁcally, when classifying adversarial examples generated by the
m-th attack model, the generalist compares its prediction with ground-truth labels, as well as
the predictions of the m-th specialist model hmthat specializes in this attack. In this way, the
generalist is able to consider the decision boundaries of different perturbations and ﬁnd an
optimal one that is robust to the union of multiple perturbations.
We propose three strategies for training the generalist h0:
1) MAT-A VG: we train h0with the average loss on all perturbations at each iteration.
The loss for training the generalist network h0in MAT-A VG is deﬁned as:
L0=1
MM
å
m=1h
(1 a)LC(p0(xq0
advm);y) +aDKL(pm(x)jjp0(xq0
advm))i
; (7)
where xq0
advmis an adversarial example of h0generated by the m-th attack model.
2) MAT-MAX: we train h0with the worse perturbation only. The loss for training the
generalist network h0in MAT-MAX is deﬁned as:
L0= (1 a)LC(p0(xq0
advk);y) +aDKL(pk(x)jjp0(xq0
advk)); (8)
where k =argmaxmLC(p(xq0
advm);y).
3) MAT-MSD: we train h0using MSD [29] adversarial examples. Since MSD incorpo-
rates multiple perturbation models into a single attack, h0learns from all the specialists at
each iteration. The loss for training the generalist network h0in MAT-MSD is deﬁned as:
L0=1
MM
å
m=1h
(1 a)LC(p0(xq0
MSD);y) +aDKL(pm(x)jjp0(xq0
MSD))i
; (9)
where xq0
MSDis the MSD adversarial example generated for h0.

--- PAGE 6 ---
6 JIANG LIU: MUTUAL ADVERSARIAL TRAINING
The total loss of Mutual Adversarial Training for Multiple Perturbations (MAT-MP) is
the sum of all networks:
LMAT-MP =M
å
m=0Lm: (10)
3 Experiments and Results
3.1 Implementation Details
We evaluate the robustness of the proposed methods on CIFAR-10 and CIFAR-100 [19]. All
images are normalized into [0, 1], and data augmentations are used during training includ-
ing random horizontal ﬂipping and 32 32 random cropping with 4-pixel padding. We use
ResNet-18 [14] and WideResNet-34-10 (WRN-34-10) [45] networks and train them with
SGD optimizers with a batch size of 128 for 120 epochs. For ResNet-18, we set the initial
learning rate to 0.01, momentum to 0.9 and weight decay to 3 :510 3. For WRN-34-10,
we set the initial learning rate to 0.1, momentum to 0.9 and weight decay to 7 10 4. The
learning rate drops by 0.1 at the 75-th, 90-th and 100-th epochs.
3.2 Defense against Single Perturbation
3.2.1 Settings
In this section, we evaluate the effectiveness of MAT against a single perturbation. We con-
sider the popular l¥attacks with e=8=255. During training, we use PGD [28] to generate
adversarial examples with step K=10 and step size h=0:007. We use a cohort of two
networks in MAT. We set a=0:6 for CIFAR-10 and a=0:45 for CIFAR-100.
3.2.2 Comparisons with state-of-the-art
We compare the performance of MAT trained models with state-of-the-art defenses including
MART [44] and TRADES [46], as well as vanilla AT [28]. We evaluate the model robustness
against FGSM [12], PGD [28], C&W [5] and Fog, Elastic and Gabor attacks [17]. For PGD,
we use step K=100 and step size h=0:003. For C&W attack [5], the maximum number of
iterations is set to 1,000 with a learning rate of 0.01. The evaluation results are summarized
in Table 2. The two models trained in MAT outperform baseline models under all attacks
considered, especially on the CIFAR-10 dataset. MAT improves model robustness without
signiﬁcant drops in clean accuracy. In fact, MAT models have higher accuracy in both clean
and adversarial data on CIFAR-100 dataset compared to TRADES and MART.
Interestingly, the two models trained in MAT demonstrate different robustness charac-
teristics: one model has slightly higher robust accuracy but lower clean accuracy, and the
other has slightly higher clean accuracy and lower robust accuracy. In addition, transfer at-
tacks from one model do not perform well on the other (Table 4). These results demonstrate
that although the two models are trained collaboratively, they indeed learn different robust
features and do not converge to one model.

--- PAGE 7 ---
JIANG LIU: MUTUAL ADVERSARIAL TRAINING 7
Table 2: Classiﬁcation accuracy (%) of different defense methods. MAT- h1and MAT- h2
are the two models trained in MAT. The best performance of each network architecture and
dataset is in bold and the second best is underlined .
Dataset Method Clean FGSM PGD C&W Fog Gabor ElasticResNet-18CIFAR-10AT [28] 84.21 63.95 49.52 49.47 40.59 69.48 51.30
TRADES [46] 81.48 62.60 52.26 49.92 38.48 68.08 51.61
MART [44] 83.07 65.81 53.47 50.03 41.80 70.01 51.74
MAT- h1 81.22 66.53 57.17 51.54 46.27 71.36 56.73
MAT- h2 81.91 66.45 56.30 51.12 46.01 71.19 57.11
CIFAR-100AT [28] 57.16 35.00 24.71 24.52 15.58 41.03 20.52
TRADES [46] 54.04 35.33 28.26 24.63 14.63 40.89 23.03
MART [44] 54.44 38.40 31.85 27.81 20.46 43.66 26.07
MAT- h1 56.06 39.25 31.99 28.32 22.15 44.90 28.37
MAT- h2 55.98 39.69 32.54 28.42 22.47 45.32 27.94WRN-34-10CIFAR-10AT [28] 86.50 59.06 50.72 51.88 42.88 71.21 49.94
TRADES [46] 84.92 60.87 55.58 54.36 45.73 72.39 52.52
MART [44] 83.62 61.61 56.49 53.28 43.34 72.06 53.03
MAT- h1 85.00 64.20 59.02 55.41 49.24 73.81 54.65
MAT- h2 84.96 64.23 58.86 55.65 49.56 74.12 55.17
CIFAR-100AT [28] 60.76 36.38 25.74 26.49 17.05 43.44 22.21
TRADES [46] 57.83 38.05 30.49 27.66 19.01 43.91 23.86
MART [44] 58.48 41.40 33.07 29.85 22.07 46.55 24.89
MAT- h1 62.28 42.57 33.63 31.35 24.97 48.71 27.19
MAT- h2 62.20 41.36 33.78 31.38 25.81 48.58 27.89
3.2.3 Comparisons with KD defenses
We compare the performance of MAT trained models with other KD defenses including
ARD [11] and ACT [1]. We evaluate the models using Kstep PGD attacks (PGD- K) with
step size h=0:003 and K=20, 100 and 1000 following [1]. For ARD, we use the same
network architecture for both the teacher model and student model, and report the perfor-
mance of the student models. The results are summarized in Table 3. MAT models achieve
signiﬁcantly higher robustness compared to ARD and ACT. ARD does not perform very well
compared to ACT and MAT, especially on the CIFAR-100 dataset.
Table 3: Classiﬁcation accuracy (%) of different knowledge distillation defense methods.
MAT- h1and MAT- h2are the two models trained in MAT. The best performance of each
network architecture and dataset is in bold and the second best is underlined .
Dataset MethodResNet-18 WRN-34-10
Clean PGD-20 PGD-100 PGD-1000 Clean PGD-20 PGD-100 PGD-1000
CIFAR-10AT [28] 84.21 51.66 49.50 49.40 86.50 53.04 50.72 50.65
ARD [11] 82.84 51.41 49.57 49.51 85.18 53.79 51.71 51.61
ACT [1] 84.33 55.83 53.73 53.62 87.10 54.77 50.65 50.51
MAT- h1 81.22 58.67 57.17 57.13 85.00 60.52 59.02 58.93
MAT- h2 81.76 58.01 56.30 56.28 84.96 60.68 58.86 58.84
CIFAR-100AT [28] 57.16 26.13 24.71 24.66 60.76 27.09 25.74 25.72
ARD [11] 47.72 25.29 24.57 24.54 45.89 25.16 23.97 23.93
ACT [1] 60.72 28.74 27.32 27.26 61.84 28.78 26.66 26.56
MAT- h1 56.06 32.68 31.99 31.98 62.28 34.74 33.64 33.63
MAT- h2 55.98 33.29 32.54 32.52 62.20 34.67 33.78 33.73

--- PAGE 8 ---
8 JIANG LIU: MUTUAL ADVERSARIAL TRAINING
Table 4: Classiﬁcation accuracy (%) of MAT models under PGD black-box attacks from
different source models. We use ResNet-18 networks and PGD attacks with step K=100
and step size h=0:003.
Dataset Model AT TRADES MART MAT- h1MAT- h2
CIFAR-10MAT- h162.74 62.50 63.10 57.16 63.49
MAT- h262.56 62.11 63.03 63.92 56.35
CIFAR-100MAT- h139.49 40.62 38.11 31.99 37.61
MAT- h239.96 41.41 38.51 38.36 32.41
3.2.4 Gradient Obfuscation
We further verify that the robustness of MAT does not come from gradient obfuscation [2].
This claim is supported by two observations: (1) One-step attacks, e.g., FGSM, perform
worse than iterative attacks, e.g., PGD (see Table 2); (2) white-box attacks have higher suc-
cess rates than black-box attacks (see Table 4).
3.2.5 Different Scenarios of MAT
In this section, we investigate how the interactions among the models affect the robustness
behavior in the MAT algorithm, where models exchange their knowledge via KD. We con-
sider four different KD scenarios in MAT: 1) MAT-rob-rob-online: h1andh2are both robust
models and are trained simultaneously using Eq.(5), which is the scenario considered in this
paper; 2) MAT-rob-rob-ofﬂine: h1andh2are both robust models, h1is a teacher model
trained by Eq.(1) and h2is a student model learning from h1using Eq.(5) with h1ﬁxed; 3)
MAT-nat-rob-online: h1is a natural model and h2is a robust model, and they are trained
simultaneously using Eq.(5) with xq1
advreplaced by x; 4) MAT-nat-rob-ofﬂine: h1is a natural
model and h2is a robust model, h1is a teacher model trained by LC, and h2is a student
model learning from h1using Eq.(5) with h1ﬁxed and xq1
advreplaced by x. Fig. 2 shows both
robust and clean accuracies corresponding to the four scenarios. We observe that 1) robust
models trained with natural models have high clean accuracy; 2) robust models trained with
robust models have high robust accuracy; 3) MAT-rob-rob-online achieves higher robust ac-
curacy than MAT-rob-rob-ofﬂine, which conﬁrms our intuition: by learning from peers, a
network becomes more robust, which in turn improves the robustness of its peers, while in
MAT-rob-rob-ofﬂine the teacher model is ﬁxed which limits the performance of the student.
We use MAT-rob-rob-online in this paper as it achieves the highest robust accuracy.
3.2.6 Effect of a
The hyper-parameter ain Eq. 5 controls the trade-off between learning from ground-truth
labels and peer network. In order to search for the optimal a, we randomly select 20% of
CIFAR-10 and CIFAR-100 training sets as the validation sets, and train ResNet-18 models
using different values of aon the other 80% of training data. Fig. 3 shows the effect of a
on CIFAR-10 and CIFAR-100 datasets. In general, the robustness of the models increases
with aas the models start to learn more from each other, and drops rapidly when abecomes
too high as the models learns too little from ground-truth labels. In addition, MAT is less
sensitive to the value of aon CIFAR-100 compared to CIFAR-10. We choose a=0:6 for
CIFAR-10 and a=0:45 for CIFAR-100 as they achieve the highest robust accuracy on the

--- PAGE 9 ---
JIANG LIU: MUTUAL ADVERSARIAL TRAINING 9
81.5 82.0 82.5 83.0 83.5 84.0 84.5
Clean Accuracy (%)4446485052545658Robust Accuracy (%)MAT-nat-rob-online (h2)
MAT-nat-rob-offline (h2)
MAT-rob-rob-online (h1)
MAT-rob-rob-online (h2)
MAT-rob-rob-offline (h1)
MAT-rob-rob-offline (h2)
(a)
56 57 58 59 60 61 62 63
Clean Accuracy (%)272829303132Robust Accuracy (%)MAT-nat-rob-online (h2)
MAT-nat-rob-offline (h2)
MAT-rob-rob-online (h1)
MAT-rob-rob-online (h2)
MAT-rob-rob-offline (h1)
MAT-rob-rob-offline (h2) (b)
Figure 2: Classiﬁcation accuracy of ResNet-18 models trained in different MAT scenarios.
Robust accuracy is the classiﬁcation accuracy under PGD-100 attack. (a) CIFAR-10 results;
(b) CIFAR-100 results.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
α52.553.053.554.054.555.055.556.056.5Robust Accuracy(%)
MAT-h1
MAT-h2
(a)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
α202224262830Robust Accuracy(%)
MAT-h1
MAT-h2 (b)
Figure 3: Performance of ResNet-18 models trained with different avalues. Robust accu-
racy is the classiﬁcation accuracy under PGD-20 attack on the validation set. (a) CIFAR-10
results; (b) CIFAR-100 results.
validation sets. After determining the values of a, we use the full training sets for training.
3.3 Defense against Multiple Perturbations
3.3.1 Settings
In this section, we evaluate the effectiveness of MAT-MP. We consider three perturbation
types: l¥,l2andl1attacks with e= (0:03;0:5;12). We use ResNet-18 [14] networks.
During training, we train three specialists using PGD l¥,l2, and l1attacks with step size
h= (0:003;0:05;0:05), number of iterations K= (40;50;50)respectively, and one general-
ist that aims to defend against all perturbation types by learning from the specialists.
During evaluation, a broad suite of both gradient and non-gradient based attacks are used
for each perturbation type. l¥attacks include FGSM, PGD, MIM [9]; l2attacks include
PGD, the Gaussian noise attack [34], the boundary attack [3], DeepFool [30], the pointwise
attack [36], DDN attack [35], and C&W attack [5]; l1attacks include PGD, the salt & pepper
attack [34], and the pointwise attack [36]. Each attack is run with 10 random starts. The
evaluation details can be found in [29].

--- PAGE 10 ---
10 JIANG LIU: MUTUAL ADVERSARIAL TRAINING
3.3.2 Results
We compare the robustness of our generalist models with models trained with PGD lpat-
tacks only (AT- lp), model trained with the worst PGD attack (AT-MAX) [29, 40], model
trained with all PGD attacks (AT-A VG) [29, 40], and model trained with MSD attacks (AT-
MSD) [29]. Baseline models are provided by the authors of [29]. The results are summarized
in Table 5. We report the average classiﬁcation accuracy under three perturbation types Ravg,
as well as the worst-case accuracy Rworst where we pick the strongest attack from all attacks
considered for each example. We can make several observations from Table 5:
1)MAT produces the strongest model against multiple perturbations. MAT-A VG model
achieves the best average accuracy against multiple perturbations, which reaches 58.7% av-
erage accuracy against l¥,l2, and l1attacks (individually 47.3%, 68.9%, 60.0%); MAT-MAX
model achieves the best performance against the union of attacks, which reaches 48.0% ac-
curacy against the union of l¥,l2, and l1attacks (individually 51.1%, 67.0%, 54.0%).
2)MAT improves model robustness as well as clean accuracy. Compared to their AT
counterparts, MAT trained models have higher clean accuracy and adversarial accuracy un-
der all perturbation types. This demonstrates the efﬁcacy of learning from the specialists
models. Improvement is most signiﬁcant when using "MAX" training strategy: MAT-MAX
achieves 3.5% higher clean accuracy, 8.7% higher Ravgand 13.1% higherRworst than AT-
MAX. This may be because simple generalization of adversarial training such as "MAX"
converges to sub-optimal solutions that are unable to balance the right trade-off between
multiple attacks [29, 40]. By learning from the soft labels provided by specialists models,
the generalist is aware of the optimal decision boundary of each perturbation type and ﬁnd a
better trade-off between multiple perturbations.
3) Among MAT models, MAT-MAX has the highest Rworst and MAT-A VG has the high-
estRavg, which is unsurprising since MAT-MAX model is trained on the worst PGD lpattack
and MAT-A VG model is trained on all PGD lpattacks. MAT-MSD achieves higher Rworst
than MAT-A VG, which indicates that it has better robustness against the union of multiple
perturbations, but it has slightly worse RavgandRworst than MAT-MAX.
In summary, our results suggest that by learning from specialists models, the generalist
is able to inherit their robustness against different perturbations and ﬁnd a decision boundary
that is more robust and more generalizable against multiple perturbations.
4 Conclusion
In this paper, we proposed a novel knowledge distillation based AT algorithm named MAT
that allows models to learn from each other during adversarial training. MAT is a general
framework and can be utilized to defend against multiple perturbations. We demonstrated
that the proposed method can effectively improve model robustness and outperform state-of-
the-art methods in CIFAR-10 and CIFAR-100 datasets. Our results demonstrate that collab-
orative learning is an effective strategy for training robust models.
Acknowledgement
This work was supported by the DARPA GARD Program HR001119S0026-GARD-FP-052.

--- PAGE 11 ---
JIANG LIU: MUTUAL ADVERSARIAL TRAINING 11
Table 5: Summary of classiﬁcation accuracy results on CIFAR-10. Ravgis the average accu-
racy of different perturbation types. Rworst is the worst-case accuracy. We show the accuracy
gain of MAT models over their AT counterparts in the brackets. The best performance in each
column is in bold and the second best is underlined .
Model Clean l¥attacks l2attacks l1attacks Ravg Rworst
AT-l¥[29] 83.3% 50.7% 57.3% 16.0% 41.3% 15.6%
AT-l2[29] 90.2% 28.3% 61.6% 46.6% 45.5% 27.5%
AT-l1[29] 73.3% 0.2% 0.0% 7.9% 2.7% 0.0%
AT-MAX [29] 81.0% 44.9% 61.7% 39.4% 48.7% 34.9%
MAT-MAX (Ours) 84.5% 51.1% 67.0% 54.0% 57.4% 48.0%
(+3.5%) (+6.2%) (+5.3%) (+14.6%) (+8.7%) (+13.1%)
AT-A VG [29] 84.6% 42.5% 65.0% 54.0% 53.8% 40.6%
MAT-A VG (Ours) 86.0% 47.3% 68.9% 60.0% 58.7% 45.9%
(+1.4%) (+4.8%) (+3.9%) (+6.0%) (+4.9%) (+5.3%)
AT-MSD [29] 81.1% 48.0% 64.3% 53.0% 55.1% 47.0%
MAT-MSD (Ours) 84.2% 48.8% 67.6% 55.1% 57.2% 47.7%
(+3.1%) (+0.8%) (+3.3%) (+2.1%) (+2.1%) (+0.7%)
References
[1] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Adversarial concurrent training:
Optimizing robustness and accuracy trade-off of deep neural networks. arXiv preprint
arXiv:2008.07015 , 2020.
[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false
sense of security: Circumventing defenses to adversarial examples. In International
Conference on Machine Learning , pages 274–283. PMLR, 2018.
[3] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial at-
tacks: Reliable attacks against black-box machine learning models. arXiv preprint
arXiv:1712.04248 , 2017.
[4] Nicholas Carlini and David Wagner. Defensive distillation is not robust to adversarial
examples. arXiv preprint arXiv:1607.04311 , 2016.
[5] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural net-
works. In 2017 IEEE Symposium on Security and Privacy (SP) , pages 39–57. IEEE,
2017.
[6] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. EAD:
Elastic-net attacks to deep neural networks via adversarial examples. arXiv preprint
arXiv:1709.04114 , 2017.
[7] Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust
overﬁtting may be mitigated by properly learned smoothening. In International Con-
ference on Learning Representations , 2021. URL https://openreview.net/
forum?id=qZzy5urZw9 .
[8] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via
randomized smoothing. In International Conference on Machine Learning , pages
1310–1320. PMLR, 2019.

--- PAGE 12 ---
12 JIANG LIU: MUTUAL ADVERSARIAL TRAINING
[9] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jian-
guo Li. Boosting adversarial attacks with momentum. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , pages 9185–9193, 2018.
[10] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima
Anandkumar. Born again neural networks. In International Conference on Machine
Learning , pages 1607–1616. PMLR, 2018.
[11] Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein. Adversarially robust
distillation. arXiv preprint arXiv:1905.09747 , 2019.
[12] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. arXiv preprint arXiv:1412.6572 , 2014.
[13] Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, and
Ping Luo. Online knowledge distillation via collaborative learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition , pages 11020–
11029, 2020.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 770–778, 2016.
[15] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V . Van-
houcke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep neural networks for acous-
tic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine , 29(6):82–97, 2012. doi: 10.1109/MSP.2012.2205597.
[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural
network. arXiv preprint arXiv:1503.02531 , 2015.
[17] Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt. Testing
robustness against unforeseen adversaries. arXiv preprint arXiv:1908.08016 , 2019.
[18] Harini Kannan, A. Kurakin, and Ian J. Goodfellow. Adversarial logit pairing. ArXiv ,
abs/1803.06373, 2018.
[19] Alex Krizhevsky. Learning multiple layers of features from tiny images. University of
Toronto , 05 2012.
[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with
deep convolutional neural networks. In Advances in Neural Information Processing
Systems , pages 1097–1105, 2012.
[21] Cassidy Laidlaw and Soheil Feizi. Functional adversarial attacks. arXiv preprint
arXiv:1906.00001 , 2019.
[22] Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness:
Defense against unseen threat models. arXiv preprint arXiv:2006.12655 , 2020.
[23] Alexander Levine and Soheil Feizi. (de) randomized smoothing for certiﬁable defense
against patch attacks. arXiv preprint arXiv:2002.10733 , 2020.

--- PAGE 13 ---
JIANG LIU: MUTUAL ADVERSARIAL TRAINING 13
[24] Alexander Levine and Soheil Feizi. Improved, deterministic smoothing for l1 certiﬁed
robustness. arXiv preprint arXiv:2103.10834 , 2021.
[25] Alexander Levine, Sahil Singla, and Soheil Feizi. Certiﬁably robust interpretation in
deep learning. arXiv preprint arXiv:1905.12105 , 2019.
[26] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learn-
ing hand-eye coordination for robotic grasping with deep learning and large-scale data
collection. The International Journal of Robotics Research , 37(4-5):421–436, 2018.
[27] Wei-An Lin, Chun Pong Lau, Alexander Levine, Rama Chellappa, and Soheil Feizi.
Dual Manifold Adversarial Robustness: Defense against Lp and non-Lp Adversarial
Attacks. In Advances in Neural Information Processing Systems , 2020.
[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv
preprint arXiv:1706.06083 , 2017.
[29] Pratyush Maini, Eric Wong, and Zico Kolter. Adversarial robustness against the union
of multiple perturbation models. In International Conference on Machine Learning ,
pages 6640–6650. PMLR, 2020.
[30] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a
simple and accurate method to fool deep neural networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , pages 2574–2582, 2016.
[31] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The
limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium
on Security and Privacy (EuroS P) , pages 372–387, 2016. doi: 10.1109/EuroSP.2016.
36.
[32] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Dis-
tillation as a defense to adversarial perturbations against deep neural networks. In 2016
IEEE Symposium on Security and Privacy (SP) , pages 582–597. IEEE, 2016.
[33] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against ad-
versarial examples. In International Conference on Learning Representations , 2018.
URL https://openreview.net/forum?id=Bys4ob-Rb .
[34] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python tool-
box to benchmark the robustness of machine learning models. arXiv preprint
arXiv:1707.04131 , 2017.
[35] Jérôme Rony, Luiz G Hafemann, Luiz S Oliveira, Ismail Ben Ayed, Robert Sabourin,
and Eric Granger. Decoupling direction and norm for efﬁcient gradient-based l2 adver-
sarial attacks and defenses. In Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition , pages 4322–4330, 2019.
[36] Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the ﬁrst
adversarially robust neural network model on MNIST. In International Conference
on Learning Representations , 2019. URL https://openreview.net/forum?
id=S1EHOsC9tX .

--- PAGE 14 ---
14 JIANG LIU: MUTUAL ADVERSARIAL TRAINING
[37] Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. arXiv preprint
arXiv:2105.11417 , 2021.
[38] Aman Sinha, Hongseok Namkoong, Riccardo V olpi, and John Duchi. Certifying
some distributional robustness with principled adversarial training. arXiv preprint
arXiv:1710.10571 , 2017.
[39] Guocong Song and Wei Chai. Collaborative learning for deep neural networks. In
Advances in Neural Information Processing Systems , pages 1832–1841, 2018.
[40] Florian Tramèr and Dan Boneh. Adversarial training and robustness for multiple per-
turbations. In Advances in Neural Information Processing Systems , pages 5866–5876,
2019.
[41] Florian Tramèr, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel.
The space of transferable adversarial examples. arXiv preprint arXiv:1704.03453 ,
2017.
[42] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Alek-
sander Madry. Robustness may be at odds with accuracy. In International Conference
on Learning Representations , 2019. URL https://openreview.net/forum?
id=SyxAb30cY7 .
[43] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for
visual intelligence: A review and new outlooks. arXiv preprint arXiv:2004.05937 ,
2020.
[44] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan
Gu. Improving adversarial robustness requires revisiting misclassiﬁed examples.
InInternational Conference on Learning Representations , 2019. URL https:
//openreview.net/forum?id=rklOg6EFwS .
[45] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Edwin R. Han-
cock Richard C. Wilson and William A. P. Smith, editors, Proceedings of the British
Machine Vision Conference (BMVC) , pages 87.1–87.12. BMV A Press, September
2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.87. URL https://dx.doi.org/
10.5244/C.30.87 .
[46] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and
Michael Jordan. Theoretically principled trade-off between robustness and accuracy.
InInternational Conference on Machine Learning , pages 7472–7482, 2019.
[47] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learn-
ing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4320–4328, 2018.
