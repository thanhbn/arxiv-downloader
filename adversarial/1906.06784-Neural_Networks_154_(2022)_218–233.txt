# 1906.06784.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/adversarial/1906.06784.pdf
# File size: 1068814 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Neural Networks 154 (2022) 218–233
Contents lists available at ScienceDirect
Neural Networks
journal homepage: www.elsevier.com/locate/neunet
Interpolated Adversarial Training: Achieving robust neural networks
without sacrificing too much accuracy
Alex Lamba,∗,1, Vikas Vermab,∗,1, Kenji Kawaguchic, Alexander Matyaskod, Savya Khoslae,
Juho Kannalab, Yoshua Bengioa
aMontreal Institute for Learning Algorithms (MILA), Canada
bAalto University, Finland
cHarvard University, USA
dNational University of Singapore, Singapore
eGoogle, India
article info
Article history:
Received 22 April 2021Received in revised form 10 April 2022Accepted 10 July 2022Available online 16 July 2022
Keywords:Adversarial robustnessMixupManifold MixupStandard test errorabstract
Adversarial robustness has become a central goal in deep learning, both in the theory and the
practice. However, successful methods to improve the adversarial robustness (such as adversarialtraining) greatly hurt generalization performance on the unperturbed data. This could have a majorimpact on how the adversarial robustness affects real world systems (i.e. many may opt to forego
robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial
Training, which employs recently proposed interpolation based training methods in the framework ofadversarial training. On CIFAR-10, adversarial training increases the standard test error ( when thereis no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retainthe adversarial robustness while achieving a standard test error of only 6.45%. With our technique,the relative increase in the standard error for the robust model is reduced from 178.1% to just45.5%. Moreover, we provide mathematical analysis of Interpolated Adversarial Training to confirm
its efficiencies and demonstrate its advantages in terms of robustness and generalization.
©2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license
(http://creativecommons.org/licenses/by/4.0/ ).
1. Introduction
Deep neural networks have been highly successful across a
variety of tasks. This success has driven applications in the areas
where reliability and security are critical, including face recog-
nition ( Sharif, Bhagavatula, Bauer, & Reiter ,2017 ), self-driving
cars ( Bojarski et al. ,2016 ), health care, and malware detec-
tion ( LeCun, Bengio, & Hinton ,2015 ). Security concerns emerge
when adversaries of the system stand to benefit from a sys-
tem performing poorly. Work on Adversarial examples (Szegedy,
Zaremba, Sutskever, Bruna, Erhan, Goodfellow, & Fergus ,2013 )
has shown that neural networks are vulnerable to the attacks per-turbing the data in imperceptible ways. Many defenses have been
proposed, but most of them rely on obfuscated gradients (Athalye,
Carlini, & Wagner ,2018 ) to give a false illusion of defense by
lowering the quality of the gradient signal, without actually
∗Corresponding authors.
E-mail addresses: lambalex@iro.umontreal.ca (A. Lamb),
vikas.verma@aalto.fi (V. Verma), kkawaguchi@fas.harvard.edu (K. Kawaguchi),
alex.m@nus.edu.sg (A. Matyasko), juho.kaanala@aalto.fi (J. Kannala),
yoshua.bengio@mila.quebec (Y. Bengio).
1Equal contribution.improving robustness ( Athalye et al. ,2018 ). Of these defenses,
only adversarial training ( Kurakin, Goodfellow, & Bengio ,2016b )
was still effective after addressing the problem of obfuscated
gradients.
However, adversarial training has a major disadvantage: it
drastically reduces the generalization performance of the net-
works on unperturbed data samples, especially for small net-
works. For example, Madry, Makelov, Schmidt, Tsipras, and Vladu
(2017 ) report that adding adversarial training to a specific model
increases the standard test error from 6.3% to 21.6% on CIFAR-10.
This phenomenon makes adversarial training difficult to use in
practice. If the tension between the performance and the security
turns out to be irreconcilable, then many systems would either
need to perform poorly or accept vulnerability, a situation leading
to great negative impact.
Our contribution: We propose to augment the adversarial
training with the interpolation based training, as a solution to the
above problem.
•We demonstrate that our approach substantially improves
standard test error while still achieving adversarial robust-
ness, using benchmark datasets (CIFAR10, CIFAR100 and
SHVN) and benchmark architectures (Wide-ResNet and
ResNet): Section 5.1.
https://doi.org/10.1016/j.neunet.2022.07.012
0893-6080/ ©2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).

--- PAGE 2 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
•We demonstrate that our approach does not suffer from ob-
fuscated gradient problem by performing black-box attacks
on the models trained with our approach: Section 5.2.
•We perform PGD attack of higher number of steps (up to
1000 steps) and higher value of maximum allowed pertur-bation/distortion epsilon , to demonstrate that the adversarial
robustness of our approach remains at the same level as that
of the adversarial training : Section 5.3.
•We demonstrate that the networks trained with our ap-
proach have lower complexity, hence resulting in improved
standard test error : Section 6.
•We mathematically analyze the benefit of the proposed
method in terms of robustness and generalization. For ro-bustness, we show that Interpolated Adversarial Trainingcorresponds to approximately minimizing an upper bound
of the adversarial loss with additional adversarial perturba-
tions. This explains why models obtained by the proposedmethod preserve the adversarial robustness and can some-times further improve the robustness when compared tostandard adversarial training. For generalization, we prove
a new generalization bound for Interpolated Adversarial
Training and analyze the benefits of the proposed method.
2. Related work
The trade-off between standard test error and adversarial
robustness has been studied in Madry et al. (2017 ),Raghunathan,
Xie, Yang, Duchi, and Liang (2019 ),Tsipras, Santurkar, Engstrom,
Turner, and Madry (2018 ) and Zhang, Yu, Jiao, Xing, Ghaoui, and
Jordan (2019a ). While Madry et al. (2017 ),Tsipras et al. (2018 )
and Zhang et al. (2019a ) empirically demonstrate this trade-
off,Tsipras et al. (2018 ) and Zhang et al. (2019a ) demonstrate this
trade-off theoretically as well on the constructed learning prob-lems. Furthermore, Raghunathan et al. (2019 ) study this trade-off
from the point-of-view of the statistical properties of the robust
objective (Ben-Tal, El Ghaoui, & Nemirovski ,2009 ) and the dy-
namics of optimizing a robust objective on a neural network, and
suggest that adversarial training requires more data to obtain alower standard test error. Our results on SVHN, CIFAR-10, andCIFAR-100 datasets (Section 5.1) also consistently show higher
standard test error with PGD adversarial training.
While Tsipras et al. (2018 ) presented data dependent proofs
showing that on certain artificially constructed distributions — itis impossible for a robust classifier to generalize as good as a non-robust classifier. How this relates to our results is an intriguingquestion. Our results suggest that the generalization gap between
adversarial training and non-robust models can be substantially
reduced through better algorithms, but it remains possible thatclosing this gap entirely on some datasets is impossible. An im-portant question for future work is how much this generalizationgap can be explained in terms of inherent data properties and
how much this gap can be addressed through better models.
Neural Architecture Search ( Zoph & Le ,2016 ) was used to find
architectures which achieve high robustness to PGD attacks aswell as better test error on the unperturbed data ( Cubuk, Zoph,
Schoenholz, & Le ,2018 ). This improved test error on the unper-
turbed data and a direct comparison to our method is in Table 2 .
However, the method of Cubuk et al. (2018 ) is computationally
very expensive as each experiment requires training thousandsof models to search for optimal architectures (9360 child modelseach trained for 10 epochs in Cubuk et al. ,2018
), whereas our
method involves no significant additional computation.
In our work we primarily concern ourselves with adversarial
training, but techniques in the research area of the provabledefenses have also shown a trade-off between robustness andgeneralization on unperturbed data. For example, the dual net-
work defense of Kolter and Wong (2017 ) reported 20.38% stan-
dard test error on SVHN for their provably robust convolutional
network (most non-robust models are well under 5% test error
on SVHN). Wong, Schmidt, Metzen, and Kolter (2018 ) reported
a best standard test accuracy of 29.23% using a convolutional
ResNet on CIFAR-10 (most non-robust ResNets have accuracy of
well over 90%). Our goal here is not to criticize this work, asdeveloping provable defenses is a challenging and important areaof work, but rather to show that this problem that we explore
with Interpolated Adversarial Training (on adversarial training type
defenses of Madry et al. ,2017 ) is just as severe with provable
defenses, and understanding if the insights developed here carryover to provable defenses, could be an interesting area for future
work.
Adversarially robust generalization: Another line of research
concerns adversarially robust generalization : the performance of
adversarially trained networks on adversarial test examples.
Schmidt, Santurkar, Tsipras, Talwar, and Madry (2018 ) observe
that a higher sample complexity is needed for better adversari-ally robust generalization. Yin, Ramchandran, and Bartlett (2018 )
demonstrate that adversarial training results in higher complexity
models and hence poorer adversarially robust generalization. Fur-
thermore, Schmidt et al. (2018 ) suggest that adversarially robust
generalization requires more data and Carmon, Raghunathan,
Schmidt, Liang, and Duchi (2019 ),Zhai et al. (2019 ) demonstrate
that unlabeled data can be used to improve adversarially robustgeneralization. In contrast to their work, in this work we focuson improving the generalization performance on unperturbed
samples (standard test error), while maintaining robustness on
unseen adversarial examples at the same level.
Interpolation based training techniques: Yet another line
of research ( Berthelot, Carlini, Goodfellow, Papernot, Oliver, &
Raffel ,2019 ;Jeong, Verma, Hyun, Kannala, & Kwak ,2021 ;Verma
et al. ,2019 ;Verma, Lamb, Kannala, Bengio, & Lopez-Paz ,2019 ;
Verma, Qu, Lamb, Bengio, Kannala, & Tang ,2019 ;Zhang, Cissé,
Dauphin, & Lopez-Paz ,2017 ) shows that simple interpolation
based training techniques are able to substantially decrease stan-
dard test error in fully-supervised and semi-supervised learn-ing paradigms. Along these lines, Zhang, Hsieh, and Tao
(2018 )
study the theoretical properties of interpolation based training
techniques such as Mixup ( Zhang et al. ,2017 )
3. Background
3.1. The empirical risk minimization framework
Let us consider a general classification task with an underlying
data distribution Dwhich consists of examples x∈Xand
corresponding labels y∈Y. The task is to learn a function f:
X→Ysuch that for a given x,foutputs corresponding y.I t
can be done by minimizing the risk E(x,y)∼D[L(x,y,θ)], where
L(θ,x,y) is a suitable loss function for instance the cross-entropy
loss and θ∈Rpis the set of parameters of function f. Since this
expectation cannot be computed, therefore a common approach
is to minimize the empirical risk 1 /N/summationtextN
i=1L(xi,yi,θ) taking into
account only a finite number of examples drawn from the datadistribution
D, namely ( x1,y1), .. . ..., (xN,yN).
3.2. Adversarial attacks and robustness
While the empirical risk minimization framework has been
very successful and often leads to excellent generalization onthe unperturbed test examples, it has the significant limitationthat it does not guarantee good performance on examples which
are carefully perturbed to fool the model ( Goodfellow, Shlens,
219

--- PAGE 3 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
& Szegedy ,2014 ;Szegedy et al. ,2013 ). That is, the empirical
risk minimization framework suffers from a lack of robustness toadversarial attacks.
Madry et al. (2017 ) proposed an optimization view of ad-
versarial robustness, in which the adversarial robustness of amodel is defined as a min–max problem. Using this view, the
parameters
θof a function fare learned by minimizing ρ(θ)a s
described in Eq. (1).Sdefines a region of points around each
example, which is typically selected so that it only contains
visually imperceptible perturbations.
min
θρ(θ),where ρ(θ)=E(x,y)∼D/bracketleftbigg
max
δ∈SL(θ,x+δ,y)/bracketrightbigg
(1)
Adversarial attacks can be broadly categorized into two cate-
gories: Single-step attacks and Multi-step attacks. We evaluated
the performance of our model as a defense against the most pop-ular and well-studied adversarial attack from each of these cate-gories. Firstly, we consider the Fast Gradient Sign Method ( Good-
fellow et al. ,2014 ) which is a single step and can still be effective
against many networks. Secondly, we consider the projected gra-dient descent attack ( Kurakin et al. ,2016b ) which is a multi-step
attack. It is slower than FGSM as it requires many iterations, buthas been shown to be a much stronger attack ( Madry et al. ,2017 ).
We briefly describe these two attacks as follows:
Fast Gradient Sign Method (FGSM) . The Fast Gradient Sign
Method ( Goodfellow et al. ,2014 ) produces
/lscript∞bounded adver-
saries by the following the sign of the gradient based pertur-
bation. This attack is cheap since it only relies on computing
the gradient once and is often an effective attack against deepnetworks ( Goodfellow et al. ,2014 ;Madry et al. ,2017 ), especially
when no adversarial defenses are employed.
/tildewidex=x+εsgn(∇xL(θ,x,y)). (2)
Projected Gradient Descent (PGD) . The projected gradient
descent attack ( Madry et al. ,2017 ), sometimes referred to as
FGSMk, is a multi-step extension of the FGSM attack characterized
as follows:
xt+1=Πx+S/parenleftbig
xt+αsgn(∇xL(θ,x,y))/parenrightbig
. (3)
initialized with x0as the clean input x.Sformalizes the manipu-
lative power of the adversary. Πrefers to the projection operator,
which in this context means projecting the adversarial example
back onto the region within an Sradius of the original data point,
after each step of size αin the adversarial attack.
3.3. Gradient obfuscation by adversarial defenses
Many approaches have been proposed as a defense against
adversarial attacks. A significant challenge with evaluating de-
fenses against adversarial attacks is that many attacks rely upona network’s gradient. The defense methods which reduce thequality of this gradient, either by making it flatter or noisier canlead to methods which lower the effectiveness of gradient-based
attacks, but which are not actually robust to adversarial exam-
ples ( Athalye, Engstrom, Ilyas, & Kwok ,2017 ;Papernot, McDaniel,
Sinha, & Wellman ,2016 ). This process, which has been referred
to as gradient masking or gradient obfuscation, must be analyzed
when studying the strength of an adversarial defense.
One method for examining the extent to which an adversarial
defense gives deceptively good results as a result of gradientobfuscation relies on the observation that black-box attacks are
a strict subset of white-box attacks, so white-box attacks should
always be at least as strong as black-box attacks. If a methodreports much better defense against white-box attacks than theblack-box attack, it suggests that the selected white-box attack
is underpowered as a result of the gradient obfuscation. Anothertest for gradient obfuscation is to run an iterative search, such
as projected gradient descent (PGD) with an unlimited range fora large number of iterations. If such an attack is not completelysuccessful, it indicates that the model’s gradients are not an
effective method for searching for adversarial images, and that
gradient obfuscation is occurring. We demonstrate successful re-sults with Interpolated Adversarial Training on these sanity checks
in Section 5.2. Still another test is to confirm that iterative attacks
with small step sizes always outperform single-step attacks with
larger step sizes (such as FGSM). If this is not the case, it maysuggest that the iterative attack becomes stuck in regions whereoptimization using gradients is poor due to gradient masking.In all of our experiments for Interpolated Adversarial Training ,
we found that the iterative PGD attacks with smaller step sizes
and more iterations were always stronger than the FGSM attacks(which take a single large step) against our models, as showninTables 2–7 .
3.4. Adversarial training
Adversarial training ( Goodfellow et al. ,2014 ) encompasses
crafting adversarial examples and using them during training
to increase robustness against unseen adversarial examples. To
scale adversarial training to large datasets and large models, oftenthe adversarial examples are crafted using the fast single stepmethods such as FGSM. However, adversarial training with fast
single step methods remains vulnerable to adversarial attacks
from a stronger multi-step attack such as PGD. Thus, in this work,we consider adversarial training with PGD.
4. Interpolated adversarial training
We propose Interpolated Adversarial Training (IAT), which trains
on interpolations of adversarial examples along with interpola-tions of unperturbed examples. We use the techniques of Mixup
(Zhang et al. ,2017 ) and Manifold Mixup ( Verma et al. ,2019 )
as ways of interpolating examples. Learning is performed in thefollowing four steps when training a network with Interpolated
Adversarial Training . In the first step, we compute the loss from an
unperturbed (non-adversarial) batch (with interpolations based
on either Mixup or Manifold Mixup). In the second step, we gen-erate a batch of adversarial examples using an adversarial attack(such as Projected Gradient Descent (PGD) ( Madry et al. ,2017 )o r
Fast Gradient Sign Method (FGSM) ( Goodfellow et al. ,2014 )). In
the third step, we train against these adversarial examples with
the original labels, with interpolations based on either Mixup orManifold Mixup. In the fourth step, we obtain the average ofthe loss from the unperturbed batch and the adversarial batch
and update the network parameters using this loss. Note that
following Kurakin, Goodfellow, and Bengio (2016a ),Tsipras et al.
(2018 ), we use both the unperturbed and adversarial samples
to train the model Interpolated Adversarial Training and we use it
in our baseline adversarial training models as well. The detailed
algorithm is described in Algorithm Block 1.
AsInterpolated Adversarial Training
combines adversarial train-
ing with either Mixup ( Zhang et al. ,2017 ) or Manifold Mixup
(Verma et al. ,2019 ), we summarize these supporting methods
in more detail. The Mixup method ( Zhang et al. ,2017 ) consists
of drawing a pair of samples from the dataset ( xi,yi)∼pDand
(xj,yj)∼pDand then taking a random linear interpolation in the
input space ˜x=λxi+(1−λ)xj. Thisλis sampled randomly on each
update (typically from a Beta distribution). Then the network fθ
is run forward on the interpolated input ˜xand trained using the
same linear interpolation of the losses L=λL(fθ(˜x),yi)+(1−
λ)L(fθ(˜x),yj). Here Lrefers to a loss function such as cross entropy.
220

--- PAGE 4 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Algorithm 1 The Interpolated Adversarial Training Algorithm
Require: fθ: Neural Network
Require: Mix: A way of combining examples (Mixup or Manifold Mixup )
Require: D: Data samples
Require: N: Total number of updates
Require: Loss: A function which runs the neural network with Mix applied
fork=1,..., Ndo
Sample ( xi,yi)∼D⊿Sample batch
Lc=Loss(fθ,Mix,xi,yi)⊿Compute loss on unperturbed data
using Mixup (or Manifold Mixup)
˜xi=attack (xi,yi)⊿Run attack (e.g. PGD as in Madry et al. ,2017 )
La=Loss(fθ,Mix,˜xi,yi)⊿Compute adversarial loss on adversarial
samples using Mixup (or Manifold Mixup)
L=(Lc+La)/2⊿Combined loss
g←∇ θL⊿Gradients of the combined Loss
θ←Step(θ,gθ)⊿Update parameters using gradients g(e.g. SGD )
end for
The Manifold Mixup method ( Verma et al. ,2019 ) is closely re-
lated to Mixup from a computational perspective, except that thelayer at which interpolation is performed, is selected randomlyon each training update.
Adversarial training consists of generating adversarial exam-
ples and training the model to give these points the originallabel. For generating these adversarial examples during training,we used the Projected Gradient Descent (PGD) attack, which isalso known as iterative FGSM. This attack consists of repeatedlyupdating an adversarial perturbation by moving in the directionof the sign of the gradient multiplied by some step size, whileprojecting back to an L
∞ball by clipping the perturbation to
maximum /epsilon1. Both /epsilon1, the step size to move on each iteration, and
the number of iterations are hyperparameters for the attack.
Why Interpolated Adversarial Training helps to improve the
standard test accuracy: We present two arguments for why
Interpolated Adversarial Training can improve standard test accu-
racy:
Increasing the training set size: Raghunathan et al. (2019 )
have shown that adversarial training could require more trainingsamples to attain a higher standard test accuracy. Mixup ( Zhang
et al. ,2017 ) and Manifold Mixup ( Verma et al. ,2019 ) can be seen
as the techniques that increase the effective size of the trainingset by creating novel training samples. Hence these techniquescan be useful in improving standard test accuracy.
Information compression: Shwartz-Ziv and Tishby (2017 )
and Tishby and Zaslavsky (2015 ) have shown a relationship be-
tween compression of information in the features learned by deepnetworks and generalization. This relates the degree to whichdeep networks compress the information in their hidden states tobounds on generalization, with a stronger bound when the deepnetworks have stronger compression.
To evaluate the effect of adversarial training on compression
of the information in the features, we performed an experimentwhere we take the representations learned after training, andstudy how well these frozen representations are able to success-fully predict fixed random labels. If the model compresses therepresentations well, then it will be harder to fit random labels.
In particular, we ran a small 2-layer MLP on top of the learned
representations to fit random binary labels. In all cases we trainedthe model with the random labels for 200 epochs with the samehyperparameters. For fitting 10000 randomly labeled examples,Table 1
Soft Rank (sum of singular values divided by largest singular value)of the representations (following first layer) from models trainedwith various methods. We report separately per MNIST class. FGSM
and PGD refer to models trained with adversarial training. We note
that FGSM slightly increases the numerical rank, but PGD (a muchstronger attack) often dramatically increases it.
Class Baseline Manifold mixup FGSM PGD
0 2.87 2.14 3.34 3.911 2.90 1.91 2.92 4.152 3.74 2.64 4.29 6.513 3.27 2.66 4.29 5.484 3.18 2.41 3.58 4.705 3.72 2.74 4.82 6.75
6 3.22 2.26 3.66 5.90
7 3.43 2.39 3.66 4.428 3.09 2.78 4.50 6.849 3.20 2.46 3.71 5.19
we achieved accuracy of: 92.08% (Baseline) and 97.00% (PGD
Adversarial Training): showing that adversarial training made the
representations much less compressed.
Manifold Mixup ( Verma et al. ,2019 ) has shown to learn more
compressed features. Hence, employing Manifold Mixup withthe adversarial training might mitigate the adverse effect of theadversarial training. Using the same experimental setup as above,
we achieved accuracy of : 64.17% (Manifold Mixup) and 71.00%
(IAT using Manifold Mixup).
These results suggest that adversarial training causes the
learned representations to be less compressed which may be thereason for poor standard test accuracy. At the same time, IAT withManifold Mixup significantly reduces the ability of the model to
learn less compressed features, which may potentially improve
standard test accuracy.
To provide further evidence for a difference in the compres-
sion characteristics, we trained 5-layer fully-connected models onMNIST and considered a bottleneck layer of 30 units directly fol-lowing the first hidden layer. We then performed singular value
decomposition on the per-class representations and looked at the
spectrum of singular values ( Fig. 1 and Table 1 ). We found that
PGD dramatically increased the number of singular values withlarge values relative to a baseline model (FGSM was somewherein-between baseline and PGD).
221

--- PAGE 5 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Fig. 1. Adversarial Training (especially with PGD) training makes representations have substantially more directions of significant variability (both w hen measured
in an absolute sense and when measured relative to the largest singular value).
5. Experiments
5.1. Adversarial robustness
The goal of our experiments is to provide empirical support
for our two major assertions: that adversarial training hurts per-formance on unperturbed data (which is consistent with whathas been previously observed in Madry et al. ,2017 ;Tsipras et al. ,
2018 ;Zhang et al. ,2019a ) and to show that this difference can
be reduced with our Interpolated Adversarial Training method. Fi-
nally, we want to show that Interpolated Adversarial Training is
adversarially robust and does not suffer from gradient obfusca-
tion ( Athalye et al. ,2018 ).
In our experiments we always perform adversarial training
using a 7-step PGD attack but we evaluate on a variety of attacks:FGSM, PGD (with a varying number of steps and hyperparame-ters), the Carlini–Wagner attack ( Carlini & Wagner ,2016 ), and the
AutoAttack ( Croce & Hein ,2020 ).
Architecture and Datasets : We conducted experiments on
competitive networks to demonstrate that Interpolated Adversar-
ial Training can improve generalization performance without sac-
rificing adversarial robustness. We used two architectures : First,the WideResNet architecture proposed in He, Zhang, Ren, and
Sun (2015 ),Zagoruyko and Komodakis (2016 ) and used in Madry
et al. (2017 ) for adversarial training
2. Second, the PreActResnet18
architecture which is a variant of the residual architecture of He
et al. (2015 ). We used SGD with momentum optimizer in our
experiments. We ran the experiments for 200 epochs with initiallearning rate is 0.1 and it is annealed by a factor of 0.1 at epoch
100 and 150. We use the batch-size of 64 for all the experiments.
We used three benchmark datasets (CIFAR10, CIFAR100 and
SVHN), which are commonly used in the adversarial robustness
2While Madry et al. (2017 ) use WRN32-10 architecture, we use the standard
WRN28-10 architecture, so our results are not directly comparable to theirresults.literature ( Croce & Hein ,2020 ;Madry et al. ,2017 ). Both CIFAR-
10 and CIFAR-100 datasets consist of 60000 color images each of
size 32 ×32, split between 50K training and 10K test images.
The CIFAR-10 dataset has ten classes, which include pictures of
cars, horses, airplanes and deer. The CIFAR-100 dataset has one
hundred classes grouped into 20 superclasses such as people,
trees, vehicles, etc. The SVHN dataset consists of 73257 training
samples and 26032 test samples each of size 32 ×32. Each
example is a close-up image of a house number (the ten classes
are the digits from 0–9).
Data Pre-Processing and Hyperparameters: The data aug-
mentation and pre-processing is exactly the same as in Madry
et al. (2017 ). Namely, we use random cropping and horizontal flip
for CIFAR10 and CIFAR100. For SVHN, we use random cropping.
We use the per-image standardization for pre-processing. For
adversarial training, we generated the adversarial examples using
a PGD adversary using a /lscript∞projected gradient descent with 7
steps of size 2, and /epsilon1=8. For the adversarial attack, we used an
FGSM adversary with /epsilon1=8 and a PGD adversary with 7 steps
and 20 steps of size 2 and /epsilon1=8.
In the Interpolated Adversarial Training experiments, for gen-
erating the adversarial examples, we used PGD with the same
hyper-parameters as described above. For performing interpola-
tion, we used either Manifold Mixup with α=2.0 as suggested
inVerma et al. (2019 ) or Mixup with alpha =1.0 as suggested
inZhang et al. (2017 ). For Manifold Mixup, we performed the
interpolation at a randomly chosen layer from the input layer, the
output of the first resblock or the output of the second resblock,
as recommended in Verma et al. (2019 ).
Results: The results for CIFAR10, CIFAR100, SVHN datasets are
presented in Tables 2 –3,4–5,6–7, respectively. We observe that
IAT consistently improves standard test error relative to models
using just adversarial training, while maintaining adversarial ro-
bustness at the same level. For example, in Table 2 , we observe
that the baseline model (no adversarial training) has standard
test error of 4.43% whereas PGD adversarial increase the standard
test error to 12.32%: a relative increase of 178% in standard test
222

--- PAGE 6 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Table 2
CIFAR10 results (error in %) to white-box attacks on WideResNet28-10 evaluated on the test data. The rows correspondto the training mechanism and columns correspond to adversarial attack methods. The upper part of the Table consists oftraining mechanisms that do not employ any explicit adversarial defense. The lower part of the Table consist of methodsthat employ adversarial training as a defense mechanism
a. For PGD, we used a /lscript∞projected gradient descent with size
α=2, and /epsilon1=8. For FGSM, we used /epsilon1=8. Our method of Interpolated Adversarial Training improves standard test error in
comparison to adversarial training (refer to the first column) and maintains the adversarial robustness on the same levelas that of adversarial training. The method of Cubuk et al. (2018 ) is close to our method in terms of standard test error
and adversarial robustness however it needs several orders of magnitude more computation (it trains 9360 models) for itsneural architecture search.
Training Adversary
No attack FGSM PGD (7 steps) PGD (20 steps)
Baseline ( Madry et al. ,2017 ) 4.80 67.3 95.9 96.5
Baseline 4.43 ±0.09 56.92 ±0.79 99.83 ±0.02 100.0 ±0.0
Mixup 3.25 ±0.11 32.63 ±0.88 92.75 ±0.61 99.27 ±0.03
Manifold Mixup 3.15 ±0.09 38.41 ±2.64 89.77 ±3.68 98.34 ±1.03
Neural Architecture Search ( Cubuk
et al. ,2018 )6.80 36.4 49.9 –
PGD (7 steps) ( Madry et al. ,2017 ) 12.70 43.90 50.00 54.20
PGD (7 steps) (our code) 12.32 ±0.14 41.87 ±0.04 50.97 ±0.15 54.87 ±0.16
Interpolated Adversarial Training(with Mixup)6.45
±0.52 33.83 ±0.86 49.88 ±0.55 54.89 ±1.37
Interpolated Adversarial Training
(Manifold Mixup)6.48±0.30 35.18 ±0.30 50.08 ±0.48 55.18 ±0.18
aSince the objective of this work is to demonstrate the effectiveness the Interpolated Adversarial Training over adversarial
training for improving the standard test error as well as maintaining the adversarial robustness to the same levels, wehighlight the best results in the lower part of the Table: the methods in the upper part of the Table have better standardtest error (‘‘No-attack’’ column), but their adversarial robustness is very poor against strong adversarial attacks (PGD, 7 stepsand 20 steps).
Table 3
CIFAR10 results (error in %) to white-box attacks on PreActResnet18. Rest of the details are same as Table 2 .
Training Adversary
No attack FGSM PGD (7 steps) PGD (20 steps)
Baseline 5.88 ±0.16 78.11 ±1.31 99.85 ±0.18 100.0 ±0.0
Mixup 4.42 ±0.03 38.32 ±0.76 97.48 ±0.15 99.88 ±0.02
Manifold Mixup 4.10 ±0.09 37.57 ±1.31 88.50 ±3.20 97.80 ±1.02
PGD (7 steps) 14.12 ±0.06 48.56 ±0.14 57.76 ±0.19 61.00 ±0.24
Interpolated Adversarial Training(with Mixup)10.12
±0.33 40.71 ±0.65 55.43 ±0.45 61.62 ±1.01
Interpolated Adversarial Training
(Manifold Mixup)10.30 ±0.15 42.48 ±0.29 55.78 ±0.67 61.80 ±0.51
Table 4
CIFAR100 results (error in %) to white-box attacks on WideResNet28-10 evaluated on the test data. The rows correspond tothe training mechanism and columns correspond to adversarial attack methods. For PGD, we used a
/lscript∞projected gradient
descent with size α=2, and /epsilon1=8. For FGSM, we used /epsilon1=8. Our method of Interpolated Adversarial Training improves
standard test error and adversarial robustness.
Training Adversary
No attack FGSM PGD (7 steps) PGD (20 steps)
Baseline 22.23 ±0.31 67.26 ±0.42 76.08 ±0.35 80.03 ±0.58
Mixup 19.37 ±0.09 61.97 ±0.31 88.12 ±1.77 93.77 ±1.64
Manifold Mixup 18.75 ±0.13 64.76 ±0.29 93.42 ±2.05 97.21 ±1.3
PGD (7 steps) 40.80 ±0.64 71.95 ±0.47 77.21 ±0.43 79.36 ±0.49
Interpolated Adversarial Training(with Mixup)33.43
±0.30 67.47 ±0.14 74.43 ±0.14 77.7 ±0.1
Interpolated Adversarial Training
(Manifold Mixup)33.27 ±0.55 67.93 ±0.48 76.21 ±0.61 79.80 ±0.59
error. With Interpolated Adversarial Training , the standard test
error is reduced to 6.45%, a relative increase of only 45% in stan-
dard test error as compared to the baseline, while the degree of
adversarial robustness remains approximately unchanged, acrossvaries type of adversarial attacks. We also considered using early
stopping technique ( Rice, Wong, & Kolter ,2020 ) to prevent robust
overfitting. We found that early stopping reduced a PGD (20
steps) test error of the model trained with IAT on CIFAR-10 from
61.62% to 58.58%, albeit, with a slight increase of the standard
test error from 10.12% to 10.62%. Thus, early stopping can be
considered complementary to our method and can be used tofurther improve the adversarial robustness of IAT trained models.We additionally ran with the challenging Carlini–Wagner ( Car-
lini & Wagner ,2016 ) attack and the ensemble-based AutoAt-
tack Croce & Hein ,2020 on CIFAR-10. With the Carlini–Wagner
attack, the IAT trained network had consistently improved ro-bustness, with test error of 28.61% on the un-targeted attack and
29.53% on the targeted attack. By contrast, the baseline model
had higher test errors of 34.42% on the un-targeted attack and
33.37% on the targeted attack. For the L-inf version of Auto-
Attack, IAT slightly increased the test error from 62.56% to 66.35%.
For the L2 version of the attack, IAT improved the test error from
43.90% to 41.57%. We also compared IAT with Feature Scattering-based Adversarial Training (FSAT) ( Zhang & Wang ,2019 ) and
223

--- PAGE 7 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Table 5
CIFAR100 results (error in %) to white-box attacks on PreActResNet18 evaluated on the test data. Rest of the detailsare same as Table 4 .
Training Adversary
No attack FGSM PGD (7 steps) PGD (20 steps)
Baseline 25.0 ±0.36 89.17 ±0.28 99.92 ±0.03 100.0 ±0.0
Mixup 23.4 ±0.27 75.26 ±0.55 99.39 ±0.03 99.94 ±0.01
Manifold Mixup 22.36 ±0.26 74.46 ±0.25 99.06 ±0.23 99.9 ±0.01
PGD (7 steps) 43.49 ±0.23 77.22 ±0.35 82.01 ±0.23 83.57 ±0.38
Interpolated Adversarial Training
(with Mixup)37.82 ±0.03 72.27 ±0.52 79.21 ±0.53 81.75 ±0.57
Interpolated Adversarial Training
(Manifold Mixup)39.48 ±0.49 72.52 ±0.55 79.12 ±0.43 81.17 ±0.49
Table 6
SVHN results (error in %) to white-box attacks on WideResNet28-10 using the 26032 test examples. The rowscorrespond to the training mechanism and columns correspond to adversarial attack methods. For PGD, we useda
/lscript∞projected gradient descent with step-size α=2, and /epsilon1=8. For FGSM, we used /epsilon1=8. Our method of
Interpolated Adversarial Training improves standard test error and adversarial robustness.
Training Adversary
No attack FGSM PGD (7 steps) PGD (20 steps)
Baseline 3.07 ±0.03 39.36 ±1.16 94.00 ±0.65 98.59 ±0.13
Mixup 2.59 ±0.08 26.93 ±1.96 90.18 ±3.43 98.78 ±0.79
Manifold Mixup 2.46 ±0.01 29.74 ±0.99 77.49 ±3.82 94.77 ±1.34
PGD (7 steps) 6.14 ±0.13 29.10 ±0.72 46.97 ±0.49 53.47 ±0.52
Interpolated Adversarial Training(with Mixup)3.47
±0.11 22.08 ±0.15 45.74 ±0.11 58.40 ±0.46
Interpolated Adversarial Training
(Manifold Mixup)3.38±0.22 22.30 ±1.07 42.61 ±0.40 52.79 ±0.22
Table 7
SVHN results (error in %) to white-box attacks on PreActResnet18. Rest of the details are same as Table 6.
Training Adversary
No attack FGSM PGD (7 steps) PGD (20 steps)
Baseline 3.47 ±0.09 50.73 ±0.22 96.37 ±0.12 98.61 ±0.06
Mixup 2.91 ±0.06 31.91 ±0.59 98.43 ±0.85 99.95 ±0.02
Manifold Mixup 2.66 ±0.02 29.86 ±3.60 72.47 ±1.82 94.00 ±0.96
PGD (7 steps) 5.27 ±0.13 26.78 ±0.62 47.00 ±0.22 54.40 ±0.42
Interpolated Adversarial Training
(with Mixup)3.63±0.05 23.57 ±0.64 47.69 ±0.22 54.62 ±0.18
Interpolated Adversarial Training
(Manifold Mixup)3.61±0.22 24.95 ±0.92 46.62 ±0.28 54.13 ±1.08
TRADES ( Zhang, Yu, Jiao, Xing, Ghaoui, & Jordan ,2019b ). We
found that FSAT and TRADES had higher standard test errors
of 10.49% and 27.5%, than IAT, which had a standard test error
of 10.12%. We also compared the robustness of IAT, FSAT andTRADES trained models to PGD (7 steps) and AutoAttack. Wefound that the IAT trained model had lower robustness to PGD
(7 steps) attack than FSAT and TRADES, with IAT having a PGD
(7 steps) error of 55.43%, FSAT having a PGD (7 steps) errorof 49.48%, and TRADES having a PGD (7 steps) error of 55.02%.Against AutoAttack, IAT improved robustness compared to FSAT,
with IAT having a robust test error of 66.35% and FSAT having
a robust test error of 67.34%. The TRADES trained model had arobust test error of 59.65% to AutoAttack. Overall, IAT improvedstandard test error while still achieving robustness comparable to
state-of-the-art methods, such as FSAT and TRADES.
5.2. Transfer attacks
As a sanity check that Interpolated Adversarial Training does
not suffer from gradient obfuscation ( Athalye et al. ,2018 ), we
performed a transfer attack evaluation on the CIFAR-10 dataset
using the PreActResNet18 architecture. In this type of evaluation,
the model which is used to generate the adversarial examples isdifferent from the model used to evaluate the attack. As thesetransfer attacks do not use the target model’s parameters tocompute the adversarial example, they are considered black-box
attacks. In our evaluation ( Table 8 ) we found that black-box trans-
fer were always substantially weaker than white-box attacks,hence Interpolated Adversarial Training does not suffer from gradi-
ent obfuscation ( Athalye et al. ,2018 ). Additionally, in Table 9 ,w e
observe that increasing
/epsilon1results in 100% success of attack, pro-
viding added evidence that Interpolated Adversarial Training does
not suffer from gradient obfuscation ( Athalye et al. ,2018 ).
5.3. Varying the number of iterations and /epsilon1for iterative attacks
To further study the robustness of Interpolated Adversarial
Training , we studied the effect of changing the number of attack
iterations and the range of the adversarial attack /epsilon1. Some ad-
versarial defenses ( Engstrom, Ilyas, & Athalye ,2018 ) have been
found to have increasing vulnerability when exposed to attackswith a large number of iterations. We studied this ( Table 10 ) and
found that both adversarial training and Interpolated Adversarial
Training have robustness which declines only slightly with an
increasing number of steps, with almost no difference betweenthe 100 step attack and the 1000 step attack. Additionally wevaried the
/epsilon1to study if Interpolated Adversarial Training was more
or less vulnerable to attacks with /epsilon1different from what the model
was trained on. We found that Interpolated Adversarial Training is
somewhat more robust when using smaller /epsilon1and slightly less
robust when using larger /epsilon1(Table 9 ).
224

--- PAGE 8 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Table 8
Transfer Attack evaluation of Interpolated Adversarial Training on CIFAR-10 reported in terms of error rate (%). Here
we consider three trained models, using normal adversarial training (Adv), IAT with mixup (IAT-M), and IAT withmanifold mixup (IAT-MM). On each experiment, we generate adversarial examples only using the model listed inthe column and then evaluate these adversarial examples on the target model listed in the row. Note that in all
of our experiments white box attacks (where the attacking model and target models are the same) led to stronger
attacks than black box attacks, which is the evidence that our approach does not suffer from gradient obfuscation(Athalye et al. ,2018 ).
/epsilon1 251 0
Target Attack
Adv. Train IAT M IAT MM Adv. Train IAT M IAT MM Adv. Train IAT M IAT MM
Adv. Train 28.54 21.11 21.87 43.68 28.10 29.21 74.66 44.39 48.14
IAT-M 17.14 25.57 18.07 25.02 45.03 28.85 48.74 78.49 51.35IAT-MM 18.57 18.74 25.71 26.84 26.7 43.23 50.43 48.11 77.05
Table 9Robustness on CIFAR-10 PreActResNet18 (Error %) with increasing
/epsilon1and a fixed number of iterations (20).
Interpolated Adversarial Training and adversarial training both have similar degradation in robustness with
increasing /epsilon1, but Interpolated Adversarial Training tends to be slightly better for smaller /epsilon1and adversarial
training is slightly better for larger /epsilon1.
Model Attack /epsilon1
1 2 1 01 52 02 55 0
Adversarial Training 21.44 28.54 74.66 92.43 98.53 99.77 100.0IAT (Mixup) 17.90 25.57 78.49 93.73 98.54 99.72 100.0IAT (Manifold Mixup) 18.24 25.71 77.05 93.31 98.67 99.85 100.0
Table 10Robustness on CIFAR-10 PreActResNet-18 (Error %) with fixed
/epsilon1=5 and a
variable number of iterations used for the adversarial attack.
Model Num. iterations
5 10 20 50 100 1000
Adversarial Training 42.35 43.44 43.68 43.76 43.80 43.83IAT (Mixup) 41.29 44.23 45.03 45.31 45.42 45.56IAT (Manifold Mixup) 40.74 42.72 43.23 43.43 43.51 43.60
Table 11Clean and Adversarial Test Errors on CIFAR-10 test data as a function ofweighting of clean and adversarial losses during IAT training. For PGD, we useda
/lscript∞projected gradient descent with 7 steps, step size α=2, and /epsilon1=8.
Weighting Model
Vanilla (No
attack)IAT (No
attack)Vanilla
(PGD)IAT
(PGD)
20% Lc, 80% La 15.78% 11.24% 57.92% 55.41%
40% Lc, 60% La 14.79% 10.63% 58.02% 55.44%
50% Lc, 50% La 14.12% 10.12% 57.76% 55.43%
60% Lc, 40% La 14.15% 9.89% 58.42% 56.48%
80% Lc, 20% La 12.2% 10.30% 58.56% 64.42%
5.4. Analysis of weighting of loss terms
IAT introduces a hyperparameter for the weighting of the clean
loss and the adversarial loss, which by default can be set to an
even weighting of both terms. We found that when we weightedthe clean loss more, we had improved clean test accuracy andwhen we weighted the adversarial loss more, we had improvedadversarial robustness. These results are shown in Table 11 .
6. Theoretical analysis
In this section, we establish mathematical properties of IAT
with Mixup. We begin in Section 6.1with additional notation
and then analyze the effect of IAT on adversarial robustness inSection 6.2. Moreover, we discuss the effects of IAT on generaliza-
tion in Section 6.3by showing how ICT can reduce overfitting and
lead to better generalization behaviors. The proofs of all theoremsand propositions are presented in Appendix B using a key lemma
proven in Appendix A .
6.1. Notation
In order to present our analysis succinctly, we introduce ad-
ditional notation as follows. The standard mixup loss
Lccan be
written as
Lc=1
n2n/summationdisplay
i,j=1Eλ∼Dλ/lscript(fθ(˜xi,j(λ)),˜yi,j(λ)), (4)
where ˜xi,j(λ)=λxi+(1−λ)xj,˜yi,j(λ)=λyi+(1−λ)yj, and
λ∈[0,1].Here, Dλrepresents the Beta distribution Beta(α,β )
with some hyper-parameters α,β > 0. Similarly, the adversarial-
mixup loss Laused in IAT can be defined by
La=1
n2n/summationdisplay
i,j=1Eλ∼Dλ/lscript(fθ(ˇxi,j(λ)),˜yi,j(λ)), (5)
where ˆδi=argmaxδi:/bardblδi/bardblρ≤/epsilon1/lscript(fθ(xi+δi),yi),ˆxi=xi+ˆδi,and
ˇxi,j(λ)=λˆxi+(1−λ)ˆxj.Using these two types of losses, the whole
IAT loss is defined by
L=Lc+La
2. (6)
In this section, we focus on the following family of loss functions:
/lscript(q,y)=h(q)−yq,for some twice differentiable function h. This
family of loss functions /lscriptincludes many commonly used losses,
including the logistic loss and the cross-entropy loss.
Given a set Fof functions x/mapsto→f(x), the Rademacher com-
plexity ( Bartlett & Mendelson ,2002 ;Mohri, Rostamizadeh, &
Talwalkar ,2012 ) of the set /lscript◦F={(x,y)/mapsto→/lscript(f(x;θ),y):f∈F}
can be defined by
Rn(/lscript◦F):=ES,σ/bracketleftBigg
sup
f∈F1
nn/summationdisplay
i=1σi/lscript(f(xi),yi)/bracketrightBigg
,
225

--- PAGE 9 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Fig. 2. The comparison of the distribution Dλof the Mixup coefficient λand the distribution ˜Dλof˜λin the error term E.
where S=((xi,yi))n
i=1. Here, σ1,...,σ nare independent uniform
random variables taking values in {−1,1}. We denote by ˜Dλthe
uniform mixture of two Beta distributions,α
α+βBeta(α+1,β)+
β
α+βBeta(β+1,α). We let Dˆxbe the empirical distribution of
the perturbed training samples ( ˆx1,...,ˆxn), and define Dxto be
the empirical distribution of the training samples ( x1,..., xn). Let
cos(a,b) be the cosine similarity of two vector aand b.
6.2. The effect of IAT on robustness
In this subsection, we study how adding Mixup to adversarial
training affects the robustness of the model by analyzing the
adversarial-mixup loss Laused in IAT. This subsection focuses on
the binary cross-entropy loss /lscriptby setting h(z)=log(1+ez) and
y∈{0,1}, whereas the next section considers a more general
setting. We define a set Θof parameter vectors by
Θ=/braceleftBig
θ∈Rd:yi(fθ(xi+ˆδi))+(yi−1)(fθ(xi+ˆδi))≥0
for all i=1,..., n/bracerightBig
.
Note that this set Θcontains the set of all parameter vectors with
correct classifications of training points (before Mixup) as
Θ⊇{θ∈Rd:1{fθ(xi+ˆδi)≥0}=yifor all i=1,..., n}.
Therefore, the condition of θ∈Θis satisfied when the deep
neural network classifies all labels correctly for the training datawith perturbations before Mixup. As the training error (although
not training loss) becomes zero in finite time in many practical
cases, the condition of
θ∈Θis satisfied in finite time in
many practical cases. Accordingly, we study the effect of IAT on
robustness in the regime of θ∈Θ.
Theorem 1 shows that the adversarial-mixup loss Lais ap-
proximately an upper bound of the adversarial loss with the
adversarial perturbations of xi/mapsto→xi+ˆδi+δmix
iwhere /bardblδi/bardblρ≤/epsilon1is
the standard adversarial perturbation and /bardblδi/bardbl2≤/epsilon1mix
iis the non-
standard additional perturbation due to IAT. In other words, IAT
is approximately minimizing the upper bound of the adversarial
loss with additional adversarial perturbation /bardblδi/bardbl2≤/epsilon1mix
iwith
data-dependent radius /epsilon1mix
ifor each i∈{1,..., n}. Therefore,
adding Mixup to adversarial training (i.e., IAT) does not decrease
the effect of the original adversarial training on the robustness
approximately (where the approximation error is in the order of(1
−˜λ)3as discussed below). This is non-trivial because, without
Theorem 1 , it is uncertain whether or not adding Mixup reducesthe effect of adversarial training in terms of the robustness. More-
over, Theorem 1 shows that IAT further improves the robustness
depending on the values of data-dependent radius /epsilon1mix
iwhen
compared to standard adversarial training without Mixup. Theseare consistent with our experimental observations.
Theorem 1. Let
θ∈Θbe a point such that ∇fθ(xi+ˆδi)and
∇2fθ(xi+ˆδi)exist for all i =1,..., n. Assume that f θ(xi+ˆδi)=
∇fθ(xi+ˆδi)/latticetop(xi+ˆδi)and∇2fθ(xi+ˆδi)=0for all i ∈{1,..., n}.
Suppose that Er∼Dˆx[r]= 0and/bardblxi+ˆδi/bardbl2≥cx√
d for all i ∈
{1,..., n}. Then, there exists a pair (ϕ,¯ϕ)such that lim z→0ϕ(z),
lim z→0¯ϕ(z)=0, and
La≥1
nn/summationdisplay
i=1max
/bardblδmix
i/bardbl2≤/epsilon1mix
i/lscript(fθ(xi+ˆδi+δmix
i),yi)+E1+E2,
where /epsilon1mix
i:=RicxEλ[(1−λ)]√
d, R i:= |cos(∇fθ(xi+ˆδi),xi+ˆδi)|,
E1:=E˜λ∼˜Dλ[(1−˜λ)2ϕ(1−˜λ)], and E 2:=E˜λ∼˜Dλ[(1−˜λ)]2¯ϕ
(E˜λ∼˜Dλ[(1−˜λ)]).
The assumption of fθ(z)=∇ fθ(z)/latticetopzand∇2fθ(z)=0i n
Theorem 1 is satisfied, for example, by linear models as well as
deep neural networks with ReLU activation function and max-
pooling. In Theorem 1 , the approximation error terms E1and E2
are in the order of (1 −˜λ)3(since lim z→0ϕ(z), lim z→0¯ϕ(z)=0),
and˜λtends to be close to one since ˜λ∼˜Dλwhere ˜Dλis the
uniform mixture of two Beta distributions,α
α+βBeta(α+1,β)+
β
α+βBeta(β+1,α), given the distribution of Dλ=Beta(α,β ) for
Mixup coefficient λ. For example, if the IAT algorithm uses the
Beta distribution Dλ=Beta(0.5,0.5) for Mixup, then we have
˜Dλ=Beta(1.5,0.5), for which ˜λ∼˜Dλtends to be close to one as
illustrated in Fig. 2 . Therefore, the approximation error terms E1
and E2tend to be close to zero.
6.3. The effect of IAT on generalization
In this subsection, we mathematically analyze the effect of IAT
on generalization properties. We start in Section 6.3.1 with th
general setting with arbitrary hand fθ, and prove a generalization
bound for IAT loss. In Section 6.3.2 , we then make assumptions
onhand fθand study the regularization effects of IAT.
6.3.1. Generalization bounds
The following theorem presents a generalization bound for the
IAT lossLc+La
2— the upper bound on the difference between the
226

--- PAGE 10 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
expected error on unseen data and the IAT loss, Ex,y[/lscript(f(x),y)]−
Lc+La
2:
Theorem 2. Letρ≥1be a real number and Fbe a set of maps
x/mapsto→f(x). Assume that the function |/lscript(q,y)−/lscript(q/prime,y)|≤τfor any
q,q/prime∈{f(x+δ):f∈F,x∈X,/bardblδ/bardblρ≤/epsilon1}and y∈Y. Then, for any
δ> 0, with probability at least 1−δover an i.i.d. draw of n i.i.d.
samples ((xi,yi))n
i=1, the following holds: for all maps f ∈F, there
exists a function ϕ:R→Rsuch that
Ex,y[/lscript(f(x),y)]−Lc+La
2(7)
≤2Rn(/lscript◦F)+2τ/radicalbigg
ln(1/δ)
2n−Q(f)
2
−EX/bracketleftBigg3/summationdisplay
k=1Gk(ˆX,Dˆx)+Gk(X,Dx)
2/bracketrightBigg
−E1,
where lim q→0ϕ(q)=0,X:=(x1,..., xn),ˆX:=(ˆx1,...,ˆxn),
Q(f):=1
nES/bracketleftBiggn/summationdisplay
i=1/parenleftbigg
max
δi:/bardblδi/bardblρ≤/epsilon1/lscript(f(xi+δi),yi)−/lscript(f(xi),yi)/parenrightbigg/bracketrightBigg
≥0,
G1(ˆX,Dˆx):=Eλ∼˜Dλ[1−λ]
nn/summationdisplay
i=1(h/prime(f(ˆxi))−yi)∇f(ˆxi)/latticetopEr∼Dˆx[r−ˆxi],
G2(ˆX,Dˆx):=Eλ∼˜Dλ[(1−λ)2]
2nn/summationdisplay
i=1h/prime/prime(f(ˆxi))∇f(ˆxi)/latticetopEr∼Dˆx
[(r−ˆxi)(r−ˆxi)/latticetop]∇f(ˆxi),
G3(ˆX,Dˆx):=Eλ∼˜Dλ[(1−λ)2]
2nn/summationdisplay
i=1(h/prime(f(ˆxi))−yi)Er∼Dˆx
[(r−ˆxi)∇2f(ˆxi)(r−ˆxi)/latticetop].
To understand this generalization bound further, we now com-
pare it with a generalization bound for IAT without using Mixup
on terms LcandLa. IAT without Mixup is the adversarial training
along with the standard training, which minimizes the loss of
L/prime=L/prime
c+L/prime
a
2,
where
L/prime
c=1
nn/summationdisplay
i=1/lscript(fθ(xi),yi),and (8)
L/prime
a=1
nn/summationdisplay
i=1/lscript(fθ(xi+ˆδi),yi). (9)
The following theorem presents a generalization bound for IAT
without Mixup on terms LcandLa:
Theorem 3. Letρ≥1be a real number and Fbe a set of maps
x/mapsto→f(x). Assume that the function |/lscript(q,y)−/lscript(q/prime,y)|≤τfor any
q,q/prime∈{f(x+δ):f∈F,x∈X,/bardblδ/bardblρ≤/epsilon1}and y ∈Y. Then, for
anyδ> 0, with probability at least 1−δover an i.i.d. draw of n
i.i.d. samples ((xi,yi))n
i=1, the following holds: for all maps f ∈F,
Ex,y[/lscript(f(x),y)]−L/prime
c+L/prime
a
2≤2Rn(/lscript◦F)+2τ/radicalbigg
ln(1/δ)
2n−Q(f)
2.(10)
By comparing Theorems 2 and 3, we can see that the benefit
of IAT with Mixup comes from the two mechanisms in terms
of generalization. The first mechanism is based on the term of
EX/bracketleftBig/summationtext3
k=1Gk(ˆX,Dˆx)+Gk(X,Dx)
2/bracketrightBig
+E1. If this term is positive, then IATwith Mixup has a better generalization bound than that of IAT
without Mixup (if we suppose that the Rademacher complexity
term Rn(/lscript◦F) is the same for both methods). The second
mechanism is based on the model complexity term Rn(/lscript◦F). As
the model complexity term is bounded by the norms of trainedweights (e.g., Bartlett, Foster, & Telgarsky ,2017 ), this term differs
for different training schemes — IAT with Mixup and IAT without
Mixup. Accordingly, we study the regularization effects of IAT onthe norms of weights in the next subsection.
6.3.2. Regularization effects
The generalization bounds in the previous subsection contain
the model complexity term, which are controlled by the norms
of the weights in the previous studies (e.g., Bartlett et al. ,2017 ).
Accordingly, we now discuss the regularization effects of IATon the norms of weights. This subsection considers the models
where f
θ(xi+ˆδi)=∇ fθ(xi+ˆδi)/latticetop(xi+ˆδi) and ∇2fθ(xi+ˆδi)=0
fori=1,..., n. This is satisfied by linear models as well as deep
neural networks with ReLU activation functions and max-pooling.We let y
∈{0,1}and h(z)=log(1+ez), which makes the loss
function /lscriptto represent the binary cross-entropy loss. Define gto
be the logic function as g(z)=ez
1+ez. This definition implies that
g(z)∈(0,1) for z∈R.
The following theorem shows that the IAT term has the ad-
ditional regularization effect on /bardbl∇fθ(ˆxi)/bardbl2and/bardbl∇fθ(ˆxi)/bardbl2
Er[(r−ˆxi)(r−ˆxi)/latticetop].
This theorem explains the additional regularization effects of the
IAT term on the norm of weights, since ∇fθ(ˆxi)=wfor linear
models and ∇fθ(ˆxi)=/bardbl WH˙σHWH−1˙σH−1...˙σ1W1/bardblfor deep
neural networks with ReLU and max-pooling.
Theorem 4. Assume that f θ(xi+ˆδi)=∇ fθ(xi+ˆδi)/latticetop(xi+ˆδi)and
∇2fθ(xi+ˆδi)=0. Then, there exists a function ϕ:R→Rsuch that
La=1
nn/summationdisplay
i=1/lscript(fθ(ˆxi),yi)+C1/bardbl∇fθ(ˆxi)/bardbl2+C2/bardbl∇fθ(ˆxi)/bardbl2
Er[(r−ˆxi)(r−ˆxi)/latticetop]+E1,
(11)
where lim q→0ϕ(q)=0and
C1=Eλ[(1−λ)]
nn/summationdisplay
i=1(yi−g(fθ(ˆxi)))/bardblEr∼Dˆx[r−ˆxi]/bardbl2
cos(∇fθ(ˆxi),Er∼Dˆx[r−ˆxi]),
C2=Eλ[(1−λ)2]
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|.
InTheorem 4 ,C2is always strictly positive since g(z)∈(0,1)
for all z∈R. While C1can be negative in general, the following
proposition shows that C1will be also non-negative in the later
phase of IAT training:
Proposition 1. Ifθ∈Θ/prime, then C 1≥0where Θ/prime={θ∈Rd:
yi(fθ(xi+δi(θ))−ζi)+(yi−1)(fθ(xi+δi(θ))−ζi)≥0for all i =
1,..., n},andζi=∇ fθ(xi+δi(θ))/latticetopEr∼Dˆx[r].
Here, we have that
Θ/prime⊇{θ∈Rd:ˆ1{fθ(xi+ˆδi)−ζi≥0}=yifor all i=1,..., n}.
Therefore, the condition of θ∈Θ/primeis satisfied when the model
classifies all labels correctly with margin ζifor adversarial pertur-
bations. As the training error (although not training loss) becomes
zero in finite time in many practical cases and margin increases
via implicit bias of gradient descent after that Lyu and Li (2020 ),
the condition of θ∈Θ/primeis satisfied in finite time in many prac-
tical cases. Theorem 4 and Proposition 1 together show that IAT
227

--- PAGE 11 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Fig. 3. We analyzed the Frobenius and spectral norms of the weight matrices on a 6-layer network. Generally Adversarial Training makes these norms larger, wh ereas
Interpolated Adversarial Training brings these norms closer to their values when doing normal training.
can reduce the norms of weights when compared to adversarial
training.
Zhang, Deng, and Kawaguchi (2021 ) showed that the stan-
dard mixup loss Lcalso has the regularization effect on the
norm of weights and thus contribute to reduce the model com-
plexity. Therefore, our result together with that of the previousstudy ( Zhang et al. ,2021 ) shows the benefit of IAT in terms of
reducing the norm of weights to control the model complexity.
As the recent study only considers the standard Mixup without
adversarial training, our result complements the recent study tounderstand IAT.
To validate this theoretical prediction, we computed the norms
of weights for a 6-layer fully-connected network with 512 hidden
units trained on Fashion-MNIST and report the results in Fig. 3 .O n
the one hand, adversarial training increased the Frobenius normsacross all the layers and increased the spectral norm of the ma-jority of the layers. On the other hand, IAT avoided or mitigated
these increases in the norms of weights. This is consistent with
our theoretical predictions and suggests that IAT learns lowercomplexity classifiers than normal adversarial training.
To further understand why adversarial training tends to in-
crease the norms, consider the case of linear regression:
L(
θ)=1
2/bardblXw−Y/bardbl2
F.
Then, we have
∇L(θ)=X/latticetop(Xw−Y).
Therefore, each step of (stochastic) gradient descent only adds
some vector in the column space of X/latticetoptowas
wt+1=wt+vtwhere vt∈Col(X/latticetop).
Here, the solutions of the linear regression are any wsuch that
w=X†Y+v⊥where v⊥∈Null( X).
Thus, (stochastic) gradient descent does not add any unnecessary
element to w, implicitly minimizing the norm of the weights.
Accordingly, if we initialize wasw0∈Col(X/latticetop), then we achieve
the minimum norm solution implicitly via (stochastic) gradient
descent.In this context, we can easily see that by conducting ad-
versarial training, we add vectors v⊥∈Null( X), breaking the
implicit bias and increasing the norm of w. Similarly, in the
case of deep neural networks, (stochastic) gradient descent has
the implicit bias that restricts the search space of wand hence
tend to minimize the norm without unnecessary elements ( Lyu
&L i ,2020 ;Moroshko, Gunasekar, Woodworth, Lee, Srebro, &
Soudry ,2020 ;Woodworth et al. ,2020 ). Thus, similarly to the
case of linear models, adversarial training adds extra elements
via the perturbation and tends to increase the norm of weights.
Our results show that we can minimize this effect via the addi-tional regularization effects of IAT to reduce overfitting for bettergeneralization behaviors.
7. Conclusion
Robustness to the adversarial examples is essential for en-
suring that machine learning systems are secure and reliable.
However the most effective defense, adversarial training, has theeffect of harming performance on the unperturbed data. This hasboth the theoretical and the practical significance. As adversarialperturbations are imperceptible (or barely perceptible) to hu-
mans and humans are able to generalize extremely well, it is
surprising that adversarial training reduces the model’s abilityto perform well on unperturbed test data. This degradation inthe generalization is critically urgent to the practitioners whose
systems are threatened by the adversarial attacks. With current
techniques those wishing to deploy machine learning systemsneed to consider a severe trade-off between performance on theunperturbed data and the robustness to the adversarial examples,which may mean that security and reliability will suffer in impor-
tant applications. Our work has addressed both of these issues.
We proposed to address this by augmenting adversarial trainingwith interpolation based training ( Verma et al. ,2019 ;Zhang et al. ,
2017 ). We found that this substantially improves generalization
on unperturbed data while preserving adversarial robustness.
Our analysis showed why and how the proposed method canimprove generalization and preserve adversarial robustness whencompared to standard adversarial training.
228

--- PAGE 12 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared
to influence the work reported in this paper.
Acknowledgments
The authors thank David Lopez-Paz for useful discussions
and feedback. We would also like to acknowledge ComputeCanada for providing computing resources used in this work.The research of Kenji Kawaguchi is partially supported by the
Center of Mathematical Sciences and Applications at Harvard
University, United States.
Appendix A. Lemma on adversarial-mixup loss
This appendix provides the key lemma, Lemma 1 , which is
used to prove theorems in Appendix B .
Lemma 1. For any f
θ, there exists a function ϕ:R→Rsuch that
La=1
nn/summationdisplay
i=1/lscript(fθ(ˆxi),yi)+3/summationdisplay
i=1Gi+Eλ∼˜Dλ[(1−λ)2ϕ(1−λ)],
where lim q→0ϕ(q)=0and
G1=Eλ∼˜Dλ[1−λ]
nn/summationdisplay
i=1(h/prime(fθ(ˆxi))−yi)∇fθ(ˆxi)/latticetopEr∼Dˆx[r−ˆxi],
G2=Eλ∼˜Dλ[(1−λ)2]
2nn/summationdisplay
i=1h/prime/prime(fθ(ˆxi))∇fθ(ˆxi)/latticetopEr∼Dˆx[(r−ˆxi)(r−ˆxi)/latticetop]∇fθ(ˆxi),
G3=Eλ∼˜Dλ[(1−λ)2]
2nn/summationdisplay
i=1(h/prime(fθ(ˆxi))−yi)Er∼Dˆx[(r−ˆxi)∇2fθ(ˆxi)(r−ˆxi)/latticetop].
Proof. Since/lscript(q,y)=h(q)−yq, we have that
1
nn/summationdisplay
i=1/lscript(fθ(ˆxi),yi)=1
nn/summationdisplay
i=1[h(fθ(ˆxi))−yifθ(ˆxi)],
and
La=1
n2Eλ∼Beta(α,β)n/summationdisplay
i,j=1[h(fθ(ˇxi,j(λ)))−(λyi+(1−λ)yj)fθ(ˇxi,j(λ))].
By expanding ( λyi+(1−λ)yj)fθ(ˇxi,j(λ)) and using h(fθ(ˇxi,j(λ)))=
λh(fθ(ˇxi,j(λ)))+(1−λ)h(fθ(ˇxi,j(λ))),
La=1
n2Eλ∼Beta(α,β)n/summationdisplay
i,j=1/braceleftBig
λ[h(fθ(ˇxi,j(λ)))−yifθ(ˇxi,j(λ))]
+(1−λ)[h(fθ(ˇxi,j(λ)))−yjfθ(ˇxi,j(λ))]/bracerightBig
.
Using the fact that EB∼Bern (λ)[B]=λ, we have
La=1
n2Eλ∼Beta(α,β)EB∼Bern (λ)n/summationdisplay
i,j=1/braceleftBig
B[h(fθ(ˇxi,j(λ)))−yifθ(ˇxi,j(λ))]
+(1−B)[h(fθ(ˇxi,j(λ)))−yjfθ(ˇxi,j(λ))]/bracerightBig
.
Since λ∼Beta(α,β ),B|λ∼Bern (λ), by conjugacy, we can
exchange them to have
B∼Bern (α
α+β),λ|B∼Beta(α+B,β+1−B).Thus,
La=1
n2n/summationdisplay
i,j=1/braceleftBigα
α+βEλ∼Beta(α+1,β)[h(fθ(ˇxi,j(λ)))−yifθ(ˇxi,j(λ))]
+β
α+βEλ∼Beta(α,β+1)[h(fθ(ˇxi,j(λ)))−yjfθ(ˇxi,j(λ))]/bracerightBig
.
Since 1 −Beta(α,β+1) and Beta(β+1,α) represent the same
distribution and ˇxij(1−λ)=ˇxji(λ), we have
/summationdisplay
i,jEλ∼Beta(α,β+1)[h(fθ(ˇxi,j(λ)))−yjfθ(ˇxi,j(λ))]
=/summationdisplay
i,jEλ∼Beta(β+1,α)[h(fθ(ˇxi,j(λ)))−yifθ(ˇxi,j(λ))].
By defining ˜Dλ=α
α+βBeta(α+1,β)+β
α+βBeta(β+1,α),
La=1
n2n/summationdisplay
i,j=1Eλ∼˜Dλ[h(fθ(ˇxi,j(λ)))−yifθ(ˇxi,j(λ))]
=1
n2n/summationdisplay
i,j=1Eλ∼˜Dλ/lscript(fθ(λˆxi+(1−λ)ˆxj),yi)
By defining Dˆxto be the empirical distribution induced by per-
turbed training samples ( ˆxj)n
j=1,
La=1
nn/summationdisplay
i=1Eλ∼˜DλEr∼Dˆx/lscript(fθ(λˆxi+(1−λ)r),yi)
Letˇxi=λˆxi+(1−λ)r,α=1−λ, andψi(α)=/lscript(fθ(ˇxi),yi). Then,
using the definition of the twice-differentiability of function ψi,
/lscript(fθ(ˇxi),yi)=ψi(α)=ψi(0)+ψ/prime
i(0)α+1
2ψ/prime/prime
i(0)α2+α2ϕi(α),
where lim z→0ϕi(z)=0. Therefore,
La=1
nEλ∼˜DλEr∼Dˆxn/summationdisplay
i=1[ψi(0)+ψ/prime
i(0)α+1
2ψ/prime/prime
i(0)α2]
+Eλ∼˜Dλ[(1−λ)2ϕ(1−λ)], (A.1)
where ϕ(α)=1
n/summationtextn
i=1ϕi(α). By linearity and chain rule,
ψ/prime
i(α)=h/prime(fθ(ˇxi))∂fθ(ˇxi)
∂ˇxi∂ˇxi
∂α−yi∂fθ(ˇxi)
∂ˇxi∂ˇxi
∂α
=h/prime(fθ(ˇxi))∂fθ(ˇxi)
∂ˇxi(r−ˆxi)−yi∂fθ(ˇxi)
∂ˇxi(r−ˆxi)
where we used∂ˇxi
∂α=(r−ˆxi). Moreover,
∂
∂α∂fθ(ˇxi)
∂ˇxi(r−ˆxi)=∂
∂α(r−ˆxi)/latticetop/bracketleftbigg∂fθ(ˇxi)
∂ˇxi/bracketrightbigg
/latticetop
=(r−ˆxi)/latticetop∇2fθ(ˇxi)∂ˇxi
∂α
=(r−ˆxi)/latticetop∇2fθ(ˇxi)(r−ˆxi).
Therefore,
ψ/prime/prime
i(α)=h/prime(fθ(ˇxi))(r−ˆxi)/latticetop∇2fθ(ˇxi)(r−ˆxi)
+h/prime/prime(fθ(ˇxi))[∂fθ(ˇxi)
∂ˇxi(r−ˆxi)]2−yi(r−ˆxi)/latticetop∇2fθ(ˇxi)(r−ˆxi).
By setting α=0,
ψ/prime
i(0)=h/prime(fθ(ˆxi))∇fθ(ˆxi)/latticetop(r−ˆxi)−yi∇fθ(ˆxi)/latticetop(r−ˆxi)
=(h/prime(fθ(ˆxi))−yi)∇fθ(ˆxi)/latticetop(r−ˆxi),
229

--- PAGE 13 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
and
ψ/prime/prime
i(0)=h/prime(fθ(ˆxi))(r−ˆxi)/latticetop∇2fθ(ˆxi)(r−ˆxi)
+h/prime/prime(fθ(ˆxi))[∇fθ(ˆxi)/latticetop(r−ˆxi)]2
−yi(r−ˆxi)/latticetop∇2fθ(ˆxi)(r−ˆxi)
=h/prime/prime(fθ(ˆxi))∇fθ(ˆxi)/latticetop(r−ˆxi)(r−ˆxi)/latticetop∇fθ(ˆxi)
+(h/prime(fθ(ˆxi))−yi)(r−ˆxi)/latticetop∇2fθ(ˆxi)(r−ˆxi).
By substituting these into (A.1) , we obtain the statement of this
lemma. /square
Appendix B. Proofs
Using Lemmas 1 proven in Appendix A , this appendix provides
the complete proofs of Theorems 1 ,2,3,4, and Proposition 1 .
Proof of Theorem 1 .Letˆxi=xi+ˆδi. From the assumption, we
have fθ(ˆxi)=∇ fθ(ˆxi)/latticetopˆxiand∇2fθ(ˆxi)=0. Since h(z)=log(1+ez),
we have h/prime(z)=ez
1+ez=g(z)≥0 and h/prime/prime(z)=ez
(1+ez)2=
g(z)(1−g(z))≥0. By substituting these into the equation of
Lemma 1 with Er∼Dˆx[r]=0,
La=1
nn/summationdisplay
i=1/lscript(fθ(ˆxi),yi)+G1+G2+E1, (B.1)
where
G1=Eλ[(1−λ)]
nn/summationdisplay
i=1(yi−g(fθ(ˆxi)))fθ(ˆxi)
G2=Eλ[(1−λ)2]
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|∇fθ(ˆxi)/latticetop
Er[(r−ˆxi)(r−ˆxi)/latticetop]∇fθ(ˆxi)
≥Eλ[(1−λ)]2
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|∇fθ(ˆxi)/latticetop
Er[(r−ˆxi)(r−ˆxi)/latticetop]∇fθ(ˆxi)
where we used E[z2]=E[z]2+Var( z)≥E[z]2and∇fθ(ˆxi)/latticetopEr[(r−
ˆxi)(r−ˆxi)/latticetop]∇fθ(ˆxi)≥0. Since Er[(r−ˆxi)(r−ˆxi)/latticetop]= Er[rr/latticetop−
rˆxi/latticetop−ˆxir/latticetop+ˆxiˆxi/latticetop]= Er[rr/latticetop]+ˆxiˆxi/latticetopwhere Er[rr/latticetop]is positive
semidefinite,
G2≥Eλ[(1−λ)]2
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|∇fθ(ˆxi)/latticetop
(Er[rr/latticetop]+ˆxiˆxi/latticetop)∇fθ(ˆxi).
≥Eλ[(1−λ)]2
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|(∇fθ(ˆxi)/latticetopˆxi)2
=Eλ[(1−λ)]2
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|/bardbl∇fθ(ˆxi)/bardbl2
2/bardblˆxi/bardbl2
2
(cos(∇fθ(ˆxi),ˆxi))2
≥1
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|/bardbl∇fθ(ˆxi)/bardbl2
2R2ic2
xEλ[(1−λ)]2d
Now we bound G1=Eλ[(1−λ)]
n/summationtextn
i=1(yi−g(fθ(ˆxi)))fθ(ˆxi) by using
θ∈Θ. Since θ∈Θ, we have yifθ(ˆxi)+(yi−1)fθ(ˆxi)≥0, which
implies that fθ(ˆxi)≥0i f yi=1 and fθ(ˆxi)≤0i f yi=0. Thus, if
yi=1,
(yi−g(fθ(ˆxi)))(fθ(ˆxi))=(1−g(fθ(ˆxi)))(fθ(ˆxi))≥0,since ( fθ(ˆxi))≥0 and (1 −g(fθ(ˆxi)))≥0 due to g(fθ(ˆxi))∈(0,1).
Ifyi=0,
(yi−g(fθ(ˆxi)))(fθ(ˆxi))=− g(fθ(ˆxi))(fθ(ˆxi))≥0,
since ( fθ(ˆxi))≤0 and −g(fθ(ˆxi))<0. Therefore, for all i=
1,..., n,
(yi−g(fθ(ˆxi)))(fθ(ˆxi))≥0,
which implies that, since Eλ[(1−λ)]≥0,
G1=Eλ[(1−λ)]
nn/summationdisplay
i=1|yi−g(fθ(ˆxi))||fθ(ˆxi)|
=Eλ[(1−λ)]
nn/summationdisplay
i=1|g(fθ(ˆxi))−yi|/bardbl∇fθ(ˆxi)/bardbl2/bardblˆxi/bardbl2|cos(∇fθ(ˆxi),ˆxi)|
≥1
nn/summationdisplay
i=1|g(fθ(ˆxi))−yi|/bardbl∇fθ(ˆxi)/bardbl2RicxEλ[(1−λ)]√
d
By substituting these lower bounds of G1and G2into (B.1) ,w e
obtain
La−1
nn/summationdisplay
i=1/lscript(fθ(ˆxi),yi) (B.2)
≥1
nn/summationdisplay
i=1|g(fθ(ˆxi))−yi|/bardbl∇fθ(ˆxi)/bardbl2/epsilon1mix
i
+1
2nn/summationdisplay
i=1|h/prime/prime(fθ(ˆxi))|/bardbl∇fθ(ˆxi)/bardbl2
2(/epsilon1mix
i)2+E1
On the other hand, for any z1,..., zn, there exist functions ϕ/prime
isuch
that lim z→0ϕ/prime
i(z)=0, and
1
nn/summationdisplay
i=1max
/bardblδi/bardbl2≤/epsilon1i/lscript(fθ(zi+δi),yi)−1
nn/summationdisplay
i=1/lscript(fθ(zi),yi)
≤1
nn/summationdisplay
i=1|g(fθ(zi))−yi|/bardbl∇fθ(zi)/bardbl2/epsilon1i
+1
2nn/summationdisplay
i=1|h/prime/prime(fθ(zi))|/bardbl∇fθ(zi)/bardbl2
2/epsilon12
i+1
nn/summationdisplay
i=1max
/bardblδi/bardbl2≤/epsilon1i/bardblδi/bardbl2
2ϕ/prime
i(δi)
≤1
nn/summationdisplay
i=1|g(fθ(zi))−yi|/bardbl∇fθ(zi)/bardbl2/epsilon1i
+1
2nn/summationdisplay
i=1|h/prime/prime(fθ(zi))|/bardbl∇fθ(zi)/bardbl2
2/epsilon12
i+1
nn/summationdisplay
i=1/epsilon12
iϕ/prime/prime
i(/epsilon1i) (B.3)
where ϕ/prime/prime
i(/epsilon1i)=max /bardblδi/bardbl2≤/epsilon1iϕ/prime
i(δi). Note that lim q→0ϕ/prime/prime
i(q)=0. By
combining (B.2) and (B.3) ,
La≥1
nn/summationdisplay
i=1max
/bardblδmix
i/bardbl2≤/epsilon1mix
i/lscript(fθ(xi+ˆδi+δmix
i),yi)
+Eλ[(1−λ)2ϕ(1−λ)]−1
nn/summationdisplay
i=1(/epsilon1mix
i)2ϕ/prime/prime(/epsilon1mix
i),
where /epsilon1mix
i=RicxEλ[(1−λ)]√
d. Define ¯ϕ(q)=−1
n/summationtextn
i=1(Ricx√
d)2
ϕ/prime/prime
i(Ricx√
dq). Note that lim q→0¯ϕ(q)=0. Then, since1
n/summationtextn
i=1(/epsilon1mix
i)2
ϕ/prime/prime(/epsilon1mix
i)=− E2,
La≥1
nn/summationdisplay
i=1max
/bardblδmix
i/bardbl2≤/epsilon1mix
i/lscript(fθ(xi+ˆδi+δmix
i),yi)+E1+E2.
where lim z→0¯ϕ(z)=0 and lim z→0ϕ(z)=0./square
230

--- PAGE 14 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Proof of Theorem 2 .LetS=((xi,yi))n
i=1and S/prime=((x/prime
i,y/prime
i))n
i=1.
Define
ϕ(S)=sup
f∈FEx,y[/lscript(f(x),y)]−Lc+La
2. (B.4)
To apply McDiarmid’s inequality to ϕ(S), we compute an upper
bound on |ϕ(S)−ϕ(S/prime)|where Sand S/primebe two test datasets
differing by exactly one point of an arbitrary index i0; i.e., Si=S/prime
i
for all i/negationslash=i0and Si0/negationslash=S/prime
i0. Then,
ϕ(S/prime)−ϕ(S)≤τ(2n−1)
n2≤2τ
n, (B.5)
where we use the fact that both Lcand Lahave n2terms and
2n−1 terms differ for Sand S/prime, each of which is bounded by the
constant τ. Similarly, ϕ(S)−ϕ(S/prime)≤2τ
n. Thus, by McDiarmid’s
inequality, for any δ> 0, with probability at least 1 −δ,
ϕ(S)≤ES[ϕ(S)]+2τ/radicalbigg
ln(1/δ)
2n. (B.6)
Moreover, by using Lemma 1 , there exist functions ϕ/primeandϕ/prime/primesuch
that
La=1
nn/summationdisplay
i=1/lscript(fθ(ˆxi),yi)+3/summationdisplay
i=1Gi+Eλ∼˜Dλ[(1−λ)2ϕ/prime(1−λ)],(B.7)
and
Lc=1
nn/summationdisplay
i=1/lscript(fθ(xi),yi)+3/summationdisplay
i=1Ri+Eλ∼˜Dλ[(1−λ)2ϕ/prime/prime(1−λ)],(B.8)
where lim q→0ϕ/prime(q)=0 and lim q→0ϕ/prime/prime(q)=0. Thus, by defining
Q(f)=1
nES/bracketleftBiggn/summationdisplay
i=1/parenleftbigg
max
δi:/bardblδi/bardblρ≤/epsilon1/lscript(f(xi+δi),yi)−/lscript(f(xi),yi)/parenrightbigg/bracketrightBigg
,
(B.9)
and
V(f)=ES/bracketleftBigg3/summationdisplay
i=1Gi+Ri
2/bracketrightBigg
−Eλ∼˜Dλ[(1−λ)2ϕ(1−λ)], (B.10)
we have that
ES[ϕ(S)] (B.11)
=ES/bracketleftBigg
sup
f∈FES/prime/bracketleftBigg
1
nn/summationdisplay
i=1/lscript(f(x/prime
i),y/prime
i)/bracketrightBigg
−Lc+La
2/bracketrightBigg
(B.12)
=ES/bracketleftBigg
sup
f∈FES/prime/bracketleftBigg
1
nn/summationdisplay
i=1/lscript(f(x/prime
i),y/prime
i)/bracketrightBigg
−1
nn/summationdisplay
i=1/lscript(f(xi),yi)/bracketrightBigg
−Q(f)
2−V(f)
(B.13)
≤ES,S/prime/bracketleftBigg
sup
f∈F1
nn/summationdisplay
i=1(/lscript(f(x/prime
i),y/prime
i)−/lscript(f(xi),yi)/bracketrightBigg
−Q(f)
2−V(f) (B.14)
≤Eξ,S,S/prime/bracketleftBigg
sup
f∈F1
nn/summationdisplay
i=1ξi(/lscript(f(x/prime
i),y/prime
i)−/lscript(f(xi),yi))/bracketrightBigg
−Q(f)
2−V(f)(B.15)
≤2Eξ,S/bracketleftBigg
sup
f∈F1
nn/summationdisplay
i=1ξi/lscript(f(xi),yi))/bracketrightBigg
−Q(f)
2−V(f) (B.16)
=2Rn(/lscript◦F)−Q(f)
2−V(f) (B.17)
where the second line follows the definitions of each term, the
third line uses ±1
n/summationtextn
i=1/lscript(f(xi),yi) inside the expectation and
the linearity of expectation, the fourth line uses the Jensen’sinequality and the convexity of the supremum, and the fifth line
follows that for each
ξi∈{ − 1,+1}, the distribution of eachtermξi(/lscript(f(x/prime
i),y/prime
i)−/lscript(f(xi),yi)) is the distribution of ( /lscript(f(x/prime
i),y/prime
i)−
/lscript(f(xi),yi)) since Sand S/primeare drawn iid with the same distribu-
tion. The sixth line uses the subadditivity of supremum.
Finally, by noticing that Q(f)≥0 from the definition of
Q(f)≥0 (since max δi:/bardblδi/bardblρ≤/epsilon1/lscript(f(xi+δi),yi)−/lscript(f(xi),yi)≥0)
and by combining Eqs. (B.6) and (B.17) , we have the desired
statement. /square
Proof of Theorem 3 .LetS=((xi,yi))n
i=1and S/prime=((x/prime
i,y/prime
i))n
i=1.
Define
ϕ(S)=sup
f∈FEx,y[/lscript(f(x),y)]−L/prime
c+L/prime
a
2. (B.18)
To apply McDiarmid’s inequality to ϕ(S), we compute an upper
bound on |ϕ(S)−ϕ(S/prime)|where Sand S/primebe two test datasets
differing by exactly one point of an arbitrary index i0; i.e., Si=S/prime
i
for all i/negationslash=i0and Si0/negationslash=S/prime
i0. Then,
ϕ(S/prime)−ϕ(S)≤2τ
n, (B.19)
since supf∈Fmaxδi0:/bardblδi0/bardblρ≤/epsilon1/lscript(f(xi0+δi0),yi0)−maxδi0:/bardblδi0/bardblρ≤/epsilon1/lscript(f(x/prime
i0+δi0),y/prime
i0)
2n≤
τ
n. Similarly, ϕ(S)−ϕ(S/prime)≤2τ
n. Thus, by McDiarmid’s inequality,
for any δ> 0, with probability at least 1 −δ,
ϕ(S)≤ES[ϕ(S)]+2τ/radicalbigg
ln(1/δ)
2n. (B.20)
Moreover, by defining
Q(f)=1
nES/bracketleftBiggn/summationdisplay
i=1/parenleftbigg
max
δi:/bardblδi/bardblρ≤/epsilon1/lscript(f(xi+δi),yi)−/lscript(f(xi),yi)/parenrightbigg/bracketrightBigg
,
(B.21)
we have that
ES[ϕ(S)] (B.22)
=ES/bracketleftBigg
sup
f∈FES/prime/bracketleftBigg
1
nn/summationdisplay
i=1/lscript(f(x/prime
i),y/prime
i)/bracketrightBigg
−L/prime
c+L/prime
a
2/bracketrightBigg
(B.23)
=ES/bracketleftBigg
sup
f∈FES/prime/bracketleftBigg
1
nn/summationdisplay
i=1/lscript(f(x/prime
i),y/prime
i)/bracketrightBigg
−1
nn/summationdisplay
i=1/lscript(f(xi),yi)/bracketrightBigg
−Q(f)
2
(B.24)
≤ES,S/prime/bracketleftBigg
sup
f∈F1
nn/summationdisplay
i=1(/lscript(f(x/prime
i),y/prime
i)−/lscript(f(xi),yi)/bracketrightBigg
−Q(f)
2(B.25)
≤Eξ,S,S/prime/bracketleftBigg
sup
f∈F1
nn/summationdisplay
i=1ξi(/lscript(f(x/prime
i),y/prime
i)−/lscript(f(xi),yi))/bracketrightBigg
−Q(f)
2(B.26)
≤2Eξ,S/bracketleftBigg
sup
f∈F1
nn/summationdisplay
i=1ξi/lscript(f(xi),yi))/bracketrightBigg
−Q(f)
2=2Rn(/lscript◦F)−Q(f)
2
(B.27)
where the second line follows the definitions of each term, the
third line uses ±1
n/summationtextn
i=1/lscript(f(xi),yi) inside the expectation and
the linearity of expectation, the fourth line uses the Jensen’s
inequality and the convexity of the supremum, and the fifth line
follows that for each ξi∈{ − 1,+1}, the distribution of each
termξi(/lscript(f(x/prime
i),y/prime
i)−/lscript(f(xi),yi)) is the distribution of ( /lscript(f(x/prime
i),y/prime
i)−
/lscript(f(xi),yi)) since Sand S/primeare drawn iid with the same distribu-
tion. The sixth line uses the subadditivity of supremum.
Finally, by noticing that Q(f)≥0 from the definition of
Q(f)≥0 (since max δi:/bardblδi/bardblρ≤/epsilon1/lscript(f(xi+δi),yi)−/lscript(f(xi),yi)≥0)
231

--- PAGE 15 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
and by combining Eqs. (B.20) and (B.27) , we have the desired
statement. /square
Proof of Theorem 4 .From the assumption, we have fθ(ˆxi)=
∇fθ(ˆxi)/latticetopˆxiand∇2fθ(ˆxi)=0. Since h(z)=log(1+ez), we have
h/prime(z)=ez
1+ez=g(z)≥0 and h/prime/prime(z)=ez
(1+ez)2=g(z)(1−g(z))≥0.
By substituting these into the equation of Lemma 1 ,
La=1
nn/summationdisplay
i=1/lscript(fθ(ˆxi),yi)+G1+G2+Eλ[(1−λ)2ϕ(1−λ)],(B.28)
where
G1=Eλ[(1−λ)]
nn/summationdisplay
i=1(yi−g(fθ(ˆxi)))/bardblEr∼Dˆx[r−ˆxi]/bardbl2
cos(∇fθ(ˆxi),Er∼Dˆx[r−ˆxi])/bardbl∇fθ(ˆxi)/bardbl2,
G2=Eλ[(1−λ)2]
2nn/summationdisplay
i=1|g(fθ(ˆxi))(1−g(fθ(ˆxi)))|/bardbl∇fθ(ˆxi)/bardbl2
Er[(r−ˆxi)(r−ˆxi)/latticetop]./square
Proof of Proposition 1 .Sinceθ∈Θ/prime, we have yi(fθ(ˆxi)−ζi)+
(yi−1)(fθ(ˆxi)−ζi)≥0, which implies that fθ(ˆxi)−ζi≥0i fyi=1
and fθ(ˆxi)−ζi≤0i fyi=0. Thus, if yi=1,
(yi−g(fθ(ˆxi)))(fθ(ˆxi)−ζi)=(1−g(fθ(ˆxi)))(fθ(ˆxi)−ζi)≥0,
since ( fθ(ˆxi)−ζi)≥0 and (1 −g(fθ(ˆxi)))≥0 due to g(fθ(ˆxi))∈
(0,1). If yi=0,
(yi−g(fθ(ˆxi)))(fθ(ˆxi)−ζi)=− g(fθ(ˆxi))(fθ(ˆxi)−ζi)≥0,
since ( fθ(ˆxi)−ζi)≤0 and −g(fθ(ˆxi))<0. Therefore, for all
i=1,..., n,
(yi−g(fθ(ˆxi)))(fθ(ˆxi)−ζi)≥0.
This implies that, since Eλ[(1−λ)]≥0 and fθ(ˆxi)=∇ fθ(ˆxi)/latticetopˆxi,
G1=Eλ[(1−λ)]
nn/summationdisplay
i=1(yi−g(fθ(ˆxi)))(fθ(ˆxi)−ζi)≥0.
Thus, we have that 0 ≤G1=C1/bardbl∇fθ(ˆxi)/bardbl2where /bardbl∇fθ(ˆxi)/bardbl2≥0,
which implies that C1≥0./square
References
Athalye, A., Carlini, N., & Wagner, D. (2018). Obfuscated gradients give a false
sense of security: Circumventing defenses to adversarial examples. arXive-prints arXiv:1802.00420 .
Athalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2017). Synthesizing robust
adversarial examples. CoRR, abs/1707.07397. URL http://arxiv.org/abs/1707.
07397 ,arXiv:1707.07397 .
Bartlett, P. L., Foster, D. J., & Telgarsky, M. (2017). Spectrally-normalized margin
bounds for neural networks. CoRR, abs/1706.08498. URL http://arxiv.org/abs/
1706.08498 ,arXiv:1706.08498 .
Bartlett, P. L., & Mendelson, S. (2002). Rademacher and Gaussian complexities:
Risk bounds and structural results. JMLR .
Ben-Tal, A., El Ghaoui, L., & Nemirovski, A. (2009). Princeton series in applied
mathematics ,Robust optimization . Princeton University Press.
Berthelot, D., Carlini, N., Goodfellow, I. J., Papernot, N., Oliver, A., & Raffel, C.
(2019). Mixmatch: A holistic approach to semi-supervised learning. CoRR,
abs/1905.02249. URL http://arxiv.org/abs/1905.02249 ,arXiv:1905.02249 .
Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., et
al. (2016). End to end learning for self-driving cars. arXiv preprint arXiv:
1604.07316 .
Carlini, N., & Wagner, D. A. (2016). Towards evaluating the robustness of neural
networks. CoRR, abs/1608.04644. URL http://arxiv.org/abs/1608.04644 ,arXiv:
1608.04644 .
Carmon, Y., Raghunathan, A., Schmidt, L., Liang, P., & Duchi, J. C. (2019).
Unlabeled data improves adversarial robustness. arXiv e-prints, arXiv:1905.
13736 .
Croce, F., & Hein, M. (2020). Reliable evaluation of adversarial robustness with
an ensemble of diverse parameter-free attacks. CoRR, abs/2003.01690. URL
http://arxiv.org/abs/2003.01690 ,arXiv:2003.01690 .Cubuk, E. D., Zoph, B., Schoenholz, S. S., & Le, Q. V. (2018). Intriguing properties
of adversarial examples. URL https://openreview.net/forum?id=rk6H0ZbRb .
Engstrom, L., Ilyas, A., & Athalye, A. (2018). Evaluating and understanding the
robustness of adversarial logit pairing. arXiv preprint arXiv:1807.10272 .
Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing
adversarial examples. arXiv e-prints arXiv:1412.6572 .
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep residual learning for im-
age recognition. CoRR, abs/1512.03385, URL http://arxiv.org/abs/1512.03385 ,
arXiv:1512.03385 .
Jeong, J., Verma, V., Hyun, M., Kannala, J., & Kwak, N. (2021). Interpolation-based
semi-supervised learning for object detection. In 2021 IEEE/CVF conference on
computer vision and pattern recognition .
Kolter, J. Z., & Wong, E. (2017). Provable defenses against adversarial examples
via the convex outer adversarial polytope. CoRR, abs/1711.00851. http://
arxiv.org/abs/1711.00851 ,arXiv:1711.00851 .
Kurakin, A., Goodfellow, I. J., & Bengio, S. (2016a). Adversarial examples in the
physical world. CoRR, abs/1607.02533. URL http://arxiv.org/abs/1607.02533 ,
arXiv:1607.02533 .
Kurakin, A., Goodfellow, I. J., & Bengio, S. (2016b). Adversarial machine learning
at scale. CoRR, abs/1611.01236. URL http://arxiv.org/abs/1611.01236 ,arXiv:
1611.01236 .
LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature ,521(7553), 436.
Lyu, K., & Li, J. (2020). Gradient descent maximizes the margin of homogeneous
neural networks. In International conference on learning representations .
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2017). Towards
deep learning models resistant to adversarial attacks. arXiv e-prints, arXiv:
1706.06083 .
Mohri, M., Rostamizadeh, A., & Talwalkar, A. (2012). Foundations of machine
learning . MIT Press.
Moroshko, E., Gunasekar, S., Woodworth, B., Lee, J. D., Srebro, N., & Soudry, D.
(2020). Implicit bias in deep linear classification: Initialization scale vstraining accuracy. arXiv preprint arXiv:2007.06738 .
Papernot, N., McDaniel, P. D., Sinha, A., & Wellman, M. P. (2016). Towards the
science of security and privacy in machine learning. CoRR, abs/1611.03814.
URL http://arxiv.org/abs/1611.03814 ,arXiv:1611.03814 .
Raghunathan, A., Xie, S. M., Yang, F., Duchi, J. C., & Liang, P. (2019). Adversarial
training can hurt generalization. arXiv e-prints, arXiv:1906.06032 .
Rice, L., Wong, E., & Kolter, Z. (2020). Overfitting in adversarially robust deep
learning. In H. D. III, & A. Singh (Eds.), Proceedings of machine learn-
ing research :Vol. 119 ,Proceedings of the 37th international conference on
machine learning (pp. 8093–8104). PMLR, URL http://proceedings.mlr.press/
v119/rice20a.html .
Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., & Madry, A. (2018). Adversarially
robust generalization requires more data. CoRR, abs/1804.11285. URL http:
//arxiv.org/abs/1804.11285 ,arXiv:1804.11285 .
Sharif, M., Bhagavatula, S., Bauer, L., & Reiter, M. K. (2017). Adversarial generative
nets: Neural network attacks on state-of-the-art face recognition. arXivpreprint arXiv:1801.00349 .
Shwartz-Ziv, R., & Tishby, N. (2017). Opening the black box of deep neural
networks via information. CoRR, abs/1703.00810. URL http://arxiv.org/abs/
1703.00810 ,arXiv:1703.00810 .
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., et
al. (2013). Intriguing properties of neural networks. arXiv e-prints, arXiv:
1312.6199 .
Tishby, N., & Zaslavsky, N. (2015). Deep learning and the information bottleneck
principle. CoRR, abs/1503.02406. URL http://arxiv.org/abs/1503.02406 ,arXiv:
1503.02406 .
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., & Madry, A. (2018). Robustness
may be at odds with accuracy. arXiv e-prints, arXiv:1805.12152 .
Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D., et
al. (2019). Manifold mixup: Better representations by interpolating hiddenstates. In K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of machine
learning research :Vol. 97 ,Proceedings of the 36th international conference on
machine learning (pp. 6438–6447). Long Beach, California, USA: PMLR, URL
http://proceedings.mlr.press/v97/verma19a.html .
Verma, V., Lamb, A., Kannala, J., Bengio, Y., & Lopez-Paz, D. (2019). Interpolation
consistency training for semi-supervised learning. arXiv e-prints, arXiv:1903.
03825 .
Verma, V., Qu, M., Lamb, A., Bengio, Y., Kannala, J., & Tang, J. (2019). Graph-
mix: Regularized training of graph neural networks for semi-supervisedlearning. CoRR, abs/1909.11715. URL http://arxiv.org/abs/1909.11715 ,
arXiv:
1909.11715 .
Wong, E., Schmidt, F., Metzen, J. H., & Kolter, J. Z. (2018). Scaling provable
adversarial defenses. CoRR, abs/1805.12514. URL http://arxiv.org/abs/1805.
12514 ,arXiv:1805.12514 .
Woodworth, B., Gunasekar, S., Lee, J. D., Moroshko, E., Savarese, P., Golan, I., et al.
(2020). Kernel and rich regimes in overparametrized models. arXiv preprintarXiv:2002.09277 .
232

--- PAGE 16 ---
A. Lamb, V. Verma, K. Kawaguchi et al. Neural Networks 154 (2022) 218–233
Yin, D., Ramchandran, K., & Bartlett, P. (2018). Rademacher complexity for
adversarially robust generalization. CoRR, abs/1810.11914. URL http://arxiv.
org/abs/1810.11914 ,arXiv:1810.11914 .
Zagoruyko, S., & Komodakis, N. (2016). Wide residual networks. CoRR,
abs/1605.07146. URL http://arxiv.org/abs/1605.07146 ,arXiv:1605.07146 .
Zhai, R., Cai, T., He, D., Dan, C., He, K., Hopcroft, J., et al. (2019). Adversarially
robust generalization just requires more unlabeled data. arXiv e-prints,arXiv:1906.00555 .
Zhang, H., Cissé, M., Dauphin, Y. N., & Lopez-Paz, D. (2017). Mixup: Beyond
empirical risk minimization. CoRR, abs/1710.09412. URL http://arxiv.org/abs/
1710.09412 ,arXiv:1710.09412 .
Zhang, L., Deng, Z., & Kawaguchi, K. (2021). How does mixup help with
robustness and generalization? In International conference on learning
representations (ICLR) .
Zhang, C., Hsieh, M., & Tao, D. (2018). Generalization bounds for vicinal risk
minimization principle. CoRR, abs/1811.04351. URL http://arxiv.org/abs/1811.
04351 ,arXiv:1811.04351 .Zhang, H., & Wang, J. (2019). Defense against adversarial attacks using feature
scattering-based adversarial training. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d’Alché Buc, E. Fox, & R. Garnett (Eds.), Vol. 32 ,Advances in neural in-
formation processing systems . Curran Associates, Inc., URL https://proceedings.
neurips.cc/paper/2019/file/d8700cbd38cc9f30cecb34f0c195b137-Paper.pdf .
Zhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L. E., & Jordan, M. (2019a). Theoretically
principled trade-off between robustness and accuracy. In K. Chaudhuri, &R. Salakhutdinov (Eds.), Proceedings of machine learning research :Vol. 97 ,
Proceedings of the 36th international conference on machine learning (pp.
7472–7482). Long Beach, California, USA: PMLR, URL http://proceedings.mlr.
press/v97/zhang19p.html .
Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., & Jordan, M. I. (2019b).
Theoretically principled trade-off between robustness and accuracy. CoRR,abs/1901.08573. URL http://arxiv.org/abs/1901.08573 ,arXiv:1901.08573 .
Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement
learning. CoRR, abs/1611.01578. URL http://arxiv.org/abs/1611.01578 ,arXiv:
1611.01578 .
233
