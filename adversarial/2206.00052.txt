# 2206.00052.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/adversarial/2206.00052.pdf
# File size: 1279911 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming
Language Models
Akshita Jha and Chandan K. Reddy
Department of Computer Science, Virginia Tech, Arlington V A - 22203.
akshitajha@vt.edu, reddy@cs.vt.edu
Abstract
Pre-trained programming language (PL) models (such as
CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the po-
tential to automate software engineering tasks involving code
understanding and code generation. However, these models
operate in the natural channel of code, i.e., they are primarily
concerned with the human understanding of the code. They
are not robust to changes in the input and thus, are poten-
tially susceptible to adversarial attacks in the natural chan-
nel. We propose, CodeAttack , a simple yet effective black-
box attack model that uses code structure to generate ef-
fective, efÔ¨Åcient, and imperceptible adversarial code samples
and demonstrates the vulnerabilities of the state-of-the-art PL
models to code-speciÔ¨Åc adversarial attacks. We evaluate the
transferability of CodeAttack on several code-code (transla-
tion and repair) and code-NL (summarization) tasks across
different programming languages. CodeAttack outperforms
state-of-the-art adversarial NLP attack models to achieve the
best overall drop in performance while being more efÔ¨Åcient,
imperceptible, consistent, and Ô¨Çuent. The code can be found
at https://github.com/reddy-lab-code-research/CodeAttack.
Introduction
There has been a recent surge in the development of gen-
eral purpose programming language (PL) models (Ahmad
et al. 2021; Feng et al. 2020; Guo et al. 2020; Tipirneni,
Zhu, and Reddy 2022; Wang et al. 2021). They can cap-
ture the relationship between natural language and source
code, and potentially automate software engineering devel-
opment tasks involving code understanding (clone detection,
defect detection) and code generation (code-code transla-
tion, code-code reÔ¨Ånement, code-NL summarization). How-
ever, the data-driven pre-training of the above models on
massive amounts of code data constraints them to primar-
ily operate in the ‚Äònatural channel‚Äô of code (Chakraborty
et al. 2022; Hindle et al. 2016; Zhang et al. 2022). This
‚Äònatural channel‚Äô focuses on conveying information to hu-
mans through code comments, meaningful variable names,
and function names (Casalnuovo et al. 2020). In such a sce-
nario, the robustness and vulnerabilities of the pre-trained
models need careful investigation. In this work, we lever-
age the code structure to generate adversarial samples in
Copyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: CodeAttack makes a small modiÔ¨Åcation to the in-
put code snippet (in red) which causes signiÔ¨Åcant changes
to the code summary obtained from the SOTA pre-trained
programming language models. Keywords are highlighted
in blue and comments in green.
the natural channel of code and demonstrate the vulnerabil-
ity of the state-of-the-art programming language models to
adversarial attacks.
Adversarial attacks are characterized by imperceptible
changes in the input that result in incorrect predictions from
a machine learning model. For pre-trained PL models oper-
ating in the natural channel, such attacks are important for
two primary reasons: (i) Exposing system vulnerabilities and
evaluating model robustness : A small change in the input
programming language (akin to a typo in the NL scenario)
can trigger the code summarization model to generate gib-
berish natural language code summary (Figure 1), and (ii)
Model interpretability : Adversarial samples can be used to
inspect the tokens pre-trained PL models attend to.
A successful adversarial attack in the natural channel for
code should have the following properties: (i) Minimal per-
turbations : Akin to spelling mistakes or synonym replace-
ment in NL that mislead neural models with imperceptible
changes, (ii) Code Consistency : Perturbed code is consistent
with the original input and follows the same coding style as
the original code, and (iii) Code Ô¨Çuency : Does not alter the
user-level code understanding of the original code. The cur-
rent natural language adversarial attack models fall short on
all three fronts. Hence, we propose CodeAttack ‚Äì a sim-
ple yet effective black-box attack model for generating ad-
versarial samples in the natural channel for any input code
snippet, irrespective of the programming language.
CodeAttack operates in a realistic scenario, where the ad-
versary does nothave access to model parameters but only toarXiv:2206.00052v3  [cs.CL]  18 Apr 2023

--- PAGE 2 ---
the test queries and the model prediction. CodeAttack uses
a pre-trained masked CodeBERT model (Feng et al. 2020)
as the adversarial code generator to generate imperceptible
and effective adversarial examples by leveraging the code
structure. Our primary contributions are as follows:
‚Ä¢ To the best of our knowledge, our work is the Ô¨Årst one
to detect the vulnerabilities of pre-trained programming
language models to adversarial attacks in the natural
channel of code. We propose a simple yet effective realis-
tic black-box attack method, CodeAttack, that generates
adversarial samples for a code snippet irrespective of the
input programming language.
‚Ä¢ We design a general purpose black-box attack method
for sequence-to-sequence PL models that is transferable
across different downstream tasks like code translation,
repair, and summarization. The input language agnos-
tic nature of our method also makes it extensible to
sequence-to-sequence tasks in other domains.
‚Ä¢ We demonstrate the effectiveness of CodeAttack over
existing NLP adversarial models through an extensive
empirical evaluation. CodeAttack outperforms the natu-
ral language baselines when considering both the attack
quality and its efÔ¨Åcacy.
Background and Related Work
Dual Channel of Source Code. Casalnuovo et al. (2020)
proposed a dual channel view of code: (i) formal, and (ii)
natural. The formal channel is precise and used for code ex-
ecution by compilers and interpreters. The natural language
channel, on the other hand, is for human comprehension and
is noisy. It relies on code comments, variable names, func-
tion names, etc., to ease human understanding. The state-of-
the-art PL models operate primarily in the natural channel
of code (Zhang et al. 2022) and therefore, we generate ad-
versarial samples by making use of this natural channel .
Adversarial Attacks in NLP. BERT-Attack (Li et al. 2020)
and BAE (Garg and Ramakrishnan 2020) use BERT for at-
tacking vulnerable words. TextFooler (Jin et al. 2020) and
PWWS (Ren et al. 2019) use synonyms and part-of-speech
(POS) tagging to replace important tokens. Deepwordbug
(Gao et al. 2018) and TextBugger (Li et al. 2019) use char-
acter insertion, deletion, and replacement strategy for at-
tacks whereas Hsieh et al. (2019) and Yang et al. (2020)
use a greedy search and replacement strategy. Alzantot et al.
(2018) use genetic algorithm and Ebrahimi et al. (2018), Pa-
pernot et al. (2016), and Pruthi, Dhingra, and Lipton (2019)
use model gradients for Ô¨Ånding subsititutes. None of these
methods have been designed speciÔ¨Åcally for programming
languages, which is more structured than natural language.
Adversarial Attacks for PL. Zhang et al. (2020) gen-
erate adversarial examples by renaming identiÔ¨Åers using
Metropolis-Hastings sampling (Metropolis et al. 1953).
Yang et al. (2022) improve on that by using greedy and ge-
netic algorithm. Yefet, Alon, and Yahav (2020) use gradi-
ent based exploration; whereas Applis, Panichella, and van
Deursen (2021) and (Henkel et al. 2022) propose metamor-
phic transformations for attacks. The above models focus onclassiÔ¨Åcation tasks like defect detection and clone detection.
Although some works do focus on adversarial examples for
code summarization (Henkel et al. 2022; Zhou et al. 2022),
they do not do so in the natural channel. They also do not
test the transferability to different tasks, PL models, and dif-
ferent programming languages. Our model, CodeAttack, as-
sumes black-box access to the state-of-the-art PL models for
generating adversarial attacks for code generation tasks like
code translation, code repair, and code summarization using
a constrained code-speciÔ¨Åc greedy algorithm to Ô¨Ånd mean-
ingful substitutes for vulnerable tokens, irrespective of the
input programming language.
CodeAttack
We describe the capabilities, knowledge, and the goal of the
proposed model, and provide details on how it detects vul-
nerabilities in the state-of-the-art pre-trained PL models.
Threat Model
Adversary‚Äôs Capabilities. The adversary is capable of
perturbing the test queries given as input to a pre-trained PL
model to generate adversarial samples. We follow the exist-
ing literature for generating natural language adversarial ex-
amples and allow for two types of perturbations for the input
code sequence in the natural channel: (i) character-level per-
turbations, and (ii) token-level perturbations. The adversary
is allowed to perturb only a certain number of tokens/char-
acters and must ensure a high similarity between the origi-
nal code and the perturbed code. Formally, for a given input
code sequenceX2X, whereXis the input space, a valid
adversarial code example XadvsatisÔ¨Åes the requirements:
X6=Xadv (1)
Xadv X +; s.t.jjjj< (2)
Sim(Xadv;X) (3)
whereis the maximum allowed perturbation; Sim ()is a
similarity function; and is the similarity threshold.
Adversary‚Äôs Knowledge. We assume standard black-box
access to realistically assess the vulnerabilities and robust-
ness of existing pre-trained PL models. The adversary does
nothave access to the model parameters, model architec-
ture, model gradients, training data, or the loss function.
It can only query the pre-trained PL model with input se-
quences and get their corresponding output probabilities.
This is more practical than a white-box scenario where the
attacker assumes access to all the above.
Adversary‚Äôs Goal. Given an input code sequence as
query, the adversary‚Äôs goal is to degrade the quality of the
generated output sequence through imperceptibly modifying
the query in the natural channel of code. The generated out-
put sequence can either be a code snippet (code translation,
code repair) or natural language text (code summarization).
Formally, given a pre-trained PL model F:X!Y, where
Xis the input space, and Yis the output space, the goal of

--- PAGE 3 ---
the adversary is to generate an adversarial sample Xadvfor
an input sequenceXs.t.
F(Xadv)6=F(X) (4)
Q(F(X)) Q(F(Xadv)) (5)
whereQ()measures the quality of the generated output and
is the speciÔ¨Åed drop in quality. This is in addition to the
constraints applied on Xadvearlier. We formulate our Ô¨Ånal
problem of generating adversarial samples as follows:
atk=argmax[Q(F(X)) Q(F(Xadv))] (6)
In the above objective function, Xadvis a minimally per-
turbed adversary subject to constraints on the perturbations
(Eqs.1-5). CodeAttack searches for a perturbation atkto
maximize the difference in the quality Q()of the output se-
quence generated from the original input code snippet Xand
that by the perturbed code snippet Xadv.
Attack Methodology
There are two primary steps: (i) Finding the most vulnerable
tokens, and (ii) Substituting these vulnerable tokens (subject
to code-speciÔ¨Åc constraints), to generate adversarial samples
in the natural channel of code.
Finding Vulnerable Tokens CodeBERT gives more at-
tention to keywords and identiÔ¨Åers while making predictions
(Zhang et al. 2022). We leverage this information and hy-
pothesize that certain input tokens contribute more towards
the Ô¨Ånal prediction than others. ‚ÄòAttacking‚Äô these highly in-
Ô¨Çuential or highly vulnerable tokens increases the probabil-
ity of altering the model predictions more signiÔ¨Åcantly as
opposed to attacking non-vulnerable tokens. Under a black-
box setting, the model gradients are unavailable and the ad-
versary only has access to the output logits of the pre-trained
PL model. We deÔ¨Åne ‚Äòvulnerable tokens‚Äô as tokens having a
high inÔ¨Çuence on the output logits of the model. Let Fbe an
encoder-decoder pre-trained PL model. The given input se-
quence is denoted by X= [x1;::;xi;:::;xm], wherefxigm
1
are the input tokens. The output is a sequence of vectors:
O=F(X) = [o1;:::;on];yt=argmax (ot); wherefotgn
1is
the output logit for the correct output token ytfor the time
stept. Without loss of generality, we can also assume the
output sequenceY=F(X) = [yi;:::;yl].Ycan either be a
sequence of code or natural language tokens.
To Ô¨Ånd the vulnerable input tokens, we re-
place a token xiwith [MASK] such that Xnxi=
[x1;:;xi 1;[MASK];xi+1;:;xm]and get its output logits.
The output vectors are now Onxi=F(Xnxi) = [o0
1;:::;o0
q]
wherefo0
tgq
1is the new output logit for the correct prediction
Y. The inÔ¨Çuence score for the token xiis as follows:
Ixi=nX
t=1ot qX
t=1o0
t (7)
We rank all the tokens according to their inÔ¨Çuence score Ixi
in descending order to Ô¨Ånd the most vulnerable tokens V.
We select the top- ktokens to limit the number of perturba-
tions and attack them iteratively either by replacing them or
by inserting/deleting a character around them.Token Class Description
Keywords Reserved word
IdentiÔ¨Åers Variable, Class Name, Method name
Operators Brackets ( fg,(),[]), Symbols (+,*,/,-,%,;,.)
Arguments Integer, Floating point, String, Character
Table 1: Token class and their description.
Substituting Vulnerable Tokens We adopt greedy search
using a masked programming language model, subject to
code-speciÔ¨Åc constraints, to Ô¨Ånd substitutes Sfor vulner-
able tokens Vsuch that they are minimally perturbed and
have the maximal probability of incorrect prediction.
Search Method. In a given input sequence, we mask a vul-
nerable token viand use the masked PL model to predict a
meaningful contextualized token in its place. We use the top-
kpredictions for each of the masked vulnerable tokens as
our initial search space. Let Mdenote a masked PL model.
Given an input sequence X= [x1;::;vi;::;xm], wherevi
is a vulnerable token, Muses WordPiece algorithm (Wu
et al. 2016) for tokenization that breaks uncommon words
into sub-words resulting in H= [h1;h2;::;hq]. We align
and mask all the corresponding sub-words for vi, and com-
bine the predictions to get the top- ksubstitutesS0=M(H)
for the vulnerable token vi. This initial search space S0con-
sists oflpossible substitutes for a vulnerable token vi. We
then Ô¨Ålter out substitute tokens to ensure minimal pertur-
bation, code consistency, and code Ô¨Çuency of the generated
adversarial samples, subject to code-speciÔ¨Åc constraints.
Code-SpeciÔ¨Åc Constraints. Since the tokens generated from
a masked PL model may not be meaningful individual code
tokens, we further use a CodeNet tokenizer (Puri et al. 2021)
to break a token into its corresponding code tokens. The
code tokens are tokenized into four primary code token
classes (Table 1). If siis the substitute for the vulnerable
tokenvias tokenized byM, andOp()denotes the oper-
ators present in any given token using CodeNet tokenizer,
we allow the substitute tokens to have an extra or a missing
operator (akin to typos in the natural channel of code).
jOp(vi)j 1jOp(si)jjOp(vi)j+ 1 (8)
LetC()denote the code token class (identiÔ¨Åers, keywords,
and arguments) of a token. We maintain the alignment be-
tween between viand the potential substitute sias follows.
C(vi) =C(si)andjC(vi)j=jC(si)j (9)
The above code constraints maintain the code Ô¨Çuency and
the code consistency of Xadvand signiÔ¨Åcantly reduce the
search space for Ô¨Ånding adversarial examples.
Substitutions. We allow two types of substitutions of vul-
nerable tokens to generate adversarial examples: (i) Oper-
ator (character) level substitution ‚Äì only an operator is in-
serted/replaced/deleted; and (ii) Token-level substitution . We
use the reduced search space Sand iteratively substitute, un-
til the adversary‚Äôs goal is met. We only allow replacing upto
p%of the vulnerable tokens/characters to limit the number

--- PAGE 4 ---
Algorithm 1: CodeAttack: Generating adversarial examples
for Code
Input: CodeX; Victim model F; Maximum perturbation ;
Similarity; Performance Drop 
Output: Adversarial Example Xadv
Initialize:Xadv X
//Find vulnerable tokens ‚ÄòV‚Äô
forxiinM(X)do
CalculateIxiacc. to Eq.(7)
end
V Rank (xi)based onIxi
//Find substitutes ‚ÄòS‚Äô
forviinVdo
S Filter (vi)subject to Eqs.(8), (9)
forsjinSdo
//Attack the victim model
Xadv= [x1;:::;xi 1;sj;:::;xm]
ifQ(F(X)) Q(F(Xadv))and
Sim(X;Xadv)andjjXadv  Xjj  
then
returnXadv//Success
end
end
//One perturbation
Xadv [x1;:::xi 1;sj;::xm]
end
return
of perturbations. We also maintain the cosine similarity be-
tween the input text Xand the adversarially perturbed text
Xadvabove a certain threshold (Equation 3). The complete
algorithm is given in Algorithm 1. CodeAttack maintains
minimal perturbation, code Ô¨Çuency, and code consistency
between the input and the adversarial code snippet.
Experiments
We study the following research questions:
‚Ä¢RQ1: How effective and transferable are the attacks gen-
erated using CodeAttack to different downstream tasks
and programming languages?
‚Ä¢RQ2: How is the quality of adversarial samples gener-
ated using CodeAttack?
‚Ä¢RQ3: Is CodeAttack effective when we limit the number
of allowed perturbations?
‚Ä¢RQ4: What is the impact of different components on the
performance of CodeAttack?
Downstream Tasks and Datasets We evaluate the trans-
ferability of CodeAttack across different sequence to se-
quence downstream tasks and in different programming lan-
guages: (i) Code Translation1involves translating between
C# and Java and vice-versa, (ii) Code Repair automatically
Ô¨Åxes bugs in Java functions. We use the ‚Äòsmall‚Äô dataset (Tu-
fano et al. 2019), (iii) Code Summarization involves gen-
erating natural language summary for a given code. We
1https://github.com/eclipse/jgit/, http://lucene.apache.org/,
http://poi.apache.org/, https://github.com/antlr/use Python, Java, and PHP from the CodeSearchNet dataset
(Husain et al. 2019). (See Appendix A for details).
Victim Models We pick a representative method from
different categories for our experiments: (i) CodeT5 : Pre-
trained encoder-decoder transformer-based PL model (Wang
et al. 2021), (ii) CodeBERT : Bimodal pre-trained PL model
(Feng et al. 2020), (iii) GraphCodeBERT : Pre-trained graph
PL model (Guo et al. 2020), (iv) RoBERTa : Pre-trained NL
model (Guo et al. 2020). (See Appendix A for details).
Baseline Models Since CodeAttack operates in the nat-
ural channel of code, we compare with two state-of-the-
art adversarial NLP baselines for a fair comparison: (i)
TextFooler: Uses synonyms, Part-Of-Speech checking, and
semantic similarity to generate adversarial text (Jin et al.
2020), (ii) BERT-Attack: Uses a pre-trained BERT masked
language model to generate adversarial text (Li et al. 2020).
Evaluation Metrics We evaluate the effectiveness and the
quality of the generated adversarial code.
Attack Effectiveness. We deÔ¨Åne the following metric.
‚Ä¢drop:We measure the drop in the downstream perfor-
mance before andafter the attack using CodeBLEU (Ren
et al. 2020) and BLEU (Papineni et al. 2002). We deÔ¨Åne
drop=Qbefore Qafter=Q(F(X);Y) Q(F(Xadv;Y)
whereQ=fCodeBLEU, BLEU g;Yis the ground truth
output;Fis the pre-trained victim PL model, Xadvis
the adversarial code sequence generated after perturbing
the original input source code X. CodeBLEU measures
the quality of the generated code snippet for code trans-
lation and code repair, and BLEU measures the quality
of the generated natural language code summary when
compared to the ground truth.
‚Ä¢Success %: Computes the % of successful attacks as
measured by drop. The higher the value, the more ef-
fective is the adversarial attack.
Attack Quality. The following metric measures the quality
of the generated adversarial code across three dimensions:
(i) efÔ¨Åciency, (ii) imperceptibility, and (iii) code consistency.
‚Ä¢# Queries: Under a black-box setting, the adversary can
query the victim model to check for changes in the output
logits. The lower the average number of queries required
per sample, the more efÔ¨Åcient is the adversary.
‚Ä¢# Perturbation: The number of tokens changed on an
average to generate an adversarial code. The lower the
value, the more imperceptible the attack will be.
‚Ä¢CodeBLEU q:Measures the consistency of the adversar-
ial code using CodeBLEU q=CodeBLEU (X;Xadv);
whereXadvis the adversarial code sequence generated
after perturbing the original input source code X. The
higher the CodeBLEU q, the more consistent the adver-
sarial code is with the original source code.
Implementation Details The model is implemented in
PyTorch. We use the publicly available pre-trained Code-
BERT (MLM) masked model as the adversarial code gen-
erator. We select the top 50 predictions for each vulner-
able token as the initial search space and allow attacking

--- PAGE 5 ---
Task Victim
ModelAttack
MethodAttack Effectiveness Attack Quality
Before After drop Success% #Queries #Perturb CodeBLEU q
Translate
(Code-
Code)CodeT5TextFooler
73.9968.08 5.91 28.29 94.95 2.90 63.19
BERT-Attack 63.01 10.98 75.83 163.5 5.28 62.52
CodeAttack 61.72 12.27 89.3 36.84 2.55 65.91
CodeBERTTextFooler
71.1660.45 10.71 49.2 73.91 1.74 66.61
BERT-Attack 58.80 12.36 70.1 290.1 5.88 52.14
CodeAttack 54.14 17.03 97.7 26.43 1.68 66.89
GraphCode-
BERTTextfooler
66.8046.51 20.29 38.70 83.17 1.82 63.62
BERT-Attack 36.54 30.26 94.33 175.8 6.73 52.07
CodeAttack 38.81 27.99 98 20.60 1.64 65.39
Repair
(Code-
Code)CodeT5Textfooler
61.1357.59 3.53 58.84 90.50 2.36 69.53
BERT-Attack 52.70 8.43 94.33 262.5 15.1 53.60
CodeAttack 53.21 7.92 99.36 30.68 2.11 69.03
CodeBERTTextfooler
61.3353.55 7.78 81.61 45.89 2.16 68.16
BERT-Attack 51.95 9.38 95.31 183.3 15.7 61.95
CodeAttack 52.02 9.31 99.39 25.98 1.64 68.05
GraphCode-
BERTTextfooler
62.1654.23 7.92 78.92 51.07 2.20 67.89
BERT-Attack 53.33 8.83 96.20 174.1 15.7 53.66
CodeAttack 51.97 10.19 99.52 24.67 1.67 66.16
Summarize
(Code-NL)CodeT5TextFooler
20.0614.96 5.70 64.6 410.15 6.38 53.91
BERT-Attack 11.96 8.70 78.4 1014.1 7.32 51.34
CodeAttack 11.06 9.59 82.8 314.87 10.1 52.67
CodeBERTTextfooler
19.7614.38 5.37 61.10 358.43 2.92 54.10
BERT-Attack 11.30 8.35 56.47 1912.6 15.8 46.24
CodeAttack 10.88 8.87 88.32 204.46 2.57 52.95
RoBERTaTextFooler
19.0614.06 4.99 62.60 356.68 2.80 54.11
BERT-Attack 11.34 7.71 60.46 1742.3 17.1 46.95
CodeAttack 10.98 8.08 87.51 183.22 2.62 53.03
Table 2: Results on translation (C#-Java), repair (Java-Java), and summarization (PHP) tasks. The performance is measured in
CodeBLEU for Code-Code tasks and in BLEU for Code-NL task. The best result is in boldface; the next best is underlined.
a maximum of 40% of code tokens. The cosine similarity
threshold between the original code and adversarially gen-
erated code is set to 0.5. As victim models, we use the
publicly available Ô¨Åne-tuned checkpoints for CodeT5 and
Ô¨Åne-tune CodeBERT, GraphCodeBERT, and RoBERTa on
the related downstream tasks. We use a batch-size of 256.
All experiments were conducted on a 48GiB RTX 8000
GPU. The source code for CodeAttack can be found at
https://github.com/reddy-lab-code-research/CodeAttack.
RQ1: Effectiveness of CodeAttack
We test the effectiveness and transferability of the generated
adversarial samples on three different sequence-to-sequence
tasks (Code Translation, Code Repair, and Code Summa-
rization). We generate adversarial code for four different
programming languages (C#, Java, Python, and PHP), and
attack four different pre-trained PL models (CodeT5, Graph-
CodeBERT, CodeBERT, and Roberta). The results for C#-
Java translation task and for the PHP code summarization
task are shown in Table 2. (See Appendix A for Java-C#
translation and Python and Java code summarization tasks).
CodeAttack has the highest success% compared to other ad-
versarial NLP baselines. CodeAttack also outperforms theadversarial baselines, BERT-Attack and TextFooler, in 6 out
of 9 cases ‚Äì the average drop using CodeAttack is around
20% for code translation and 10% for code repair tasks,
respectively. For code summarization, CodeAttack reduces
BLEU by almost 50% for all the victim models. As BERT-
Attack replaces tokens indiscriminately, its drop is higher
in some cases but its attack quality is the lowest.
RQ2: Quality of Attacks Using CodeAttack
Quantitative Analysis. Compared to the other adversarial
NLP models, CodeAttack is the most efÔ¨Åcient as it requires
the lowest number of queries for a successful attack (Ta-
ble 2). CodeAttack is also the least perceptible as the av-
erage number of perturbations required are 1-3 tokens in 8
out of 9 cases. The code consistency of adversarial samples,
as measured by CodeBLEU q, generated using CodeAttack
is comparable to TextFooler which has a very low success
rate. CodeAttack has the best overall performance.
Qualitative Analysis. Figure 2 presents qualitative exam-
ples of the generated adversarial code snippets from differ-
ent attack models. Although TextFooler has a slightly bet-
ter CodeBLEU qscore when compared to CodeAttack (as
seen from Table 2), it replaces keywords with closely re-

--- PAGE 6 ---
Figure 2: Qualitative examples of adversarial codes on C#-Java Code Translation task. (See Appendix A for more examples).
Figure 3: Syntactic correctness of adversarial code on C#,
Java, and Python demonstrating attack quality.
lated natural language words ( public!audiences ;
override!revoked ,void!cancelling ).
BERT-Attack has the lowest CodeBLEU qand substitutes to-
kens with seemingly random words. Both TextFooler and
BERT-Attack have not been designed for programming lan-
guages. CodeAttack generates more meaningful adversarial
code samples by replacing vulnerable tokens with variables
and operators which are imperceptible and consistent.
Syntactic correctness. Syntactic correctness of the gener-
ated adversarial code is a useful criteria for evaluating the
attack quality even though CodeAttack and other PL models
primarily operate in the natural channel of code, i.e., they are
concerned with code understanding for humans and not with
the execution or compilation of the code. The datasets de-
scribed earlier consist of code snippets and cannot be com-
piled. Therefore, we generate adversarial code for C#, Java,
and Python using TextFooler, BERT-Attack, and CodeAt-
tack and ask 3 human annotators, familiar with these lan-
guages to verify the syntax manually. We randomly sam-
ple 60 generated adversarial codes for all three program-
ming languages for evaluating each of the above methods.
CodeAttack has the highest average syntactic correctness for
C# (70%), Java (60%), and Python (76.19%) followed by
BERT-Attack and TextFooler (Figure 3), further highlight-
ing the need for a code-speciÔ¨Åc adversarial attack.
RQ3: Limiting Perturbations Using CodeAttack
We restrict the number of perturbations when attacking a
pre-trained PL model to a strict limit, and study the effective-
ness of CodeAttack. From Figure 4a, we observe that as the
perturbation % increases, the CodeBLEU afterfor CodeAt-tack decreases but remains constant for TextFooler and only
slightly decreases for BERT-Attack. We also observe that al-
though CodeBLEU qfor CodeAttack is the second best (Fig-
ure 4b), it has the highest attack success rate (Figure 4d) and
requires the lowest number of queries for a successful attack
(Figure 4c). This shows the efÔ¨Åciency of CodeAttack and the
need for code-speciÔ¨Åc adversarial attacks.
RQ4: Ablation Study
Importance of Vulnerable Tokens. We create a vari-
ant, CodeAttack RAND, which randomly samples tokens from
the input code for substitution. We deÔ¨Åne another vari-
ant, CodeAttack VUL, which Ô¨Ånds vulnerable tokens based on
logit information and attacks them, albeit without any con-
straints. As can be seen from Figure 5a, attacking random
tokens is not as effective as attacking vulnerable tokens. Us-
ing CodeAttack VULyields greater drop and requires fewer
number of queries when compared to CodeAttack RAND,
across all three models at similar CodeBLEU q(Figure 5b)
and success % (Figure 5d).
Importance of Code-SpeciÔ¨Åc Constraints. We Ô¨Ånd vul-
nerable tokens and apply two types of constraints: (i) Opera-
tor level constraint (CodeAttack OP), and (ii) Token level con-
straint (CodeAttack TOK). Only applying the operator level
constraint results in lower attack success% (Figure 5d) and
a lower drop (Figure 5a) but a much higher CodeBLEU q.
This is because we limit the changes only to operators re-
sulting in minimal changes. On applying both operator level
and token level constraints together, the drop and the at-
tack success% improve signiÔ¨Åcantly. (See Appendix A for
qualitative examples.)
Overall, the Ô¨Ånal model, CodeAttack, consists of
CodeAttack VUL, CodeAttack OP, and CodeAttack TOK, has the
best trade-off across drop, attack success %, CodeBLEU q,
and #Queries for all pre-trained PL victim models.
Human Evaluation. We sample 50 original and perturbed
Java and C# code samples and shufÔ¨Çe them to create a mix.
We ask 3 human annotators, familiar with the two program-
ming languages, to classify the codes as either original or
adversarial by evaluating the source codes in their natural
channel. On average, 72.1% of the given codes were clas-
siÔ¨Åed as original. We also ask them to read the given ad-
versarial codes and rate their code understanding on a scale
of 1 to 5; where 1 corresponds to ‚ÄòCode cannot be under-
stood at all‚Äô; and 5 corresponds to ‚ÄòCode is completely un-
derstandable‚Äô. The average code understanding for the ad-
versarial codes was 4.14. Additionally, we provide the an-
notators with pairs of adversarial and original codes and ask

--- PAGE 7 ---
(a) CodeBLEU after
 (b) CodeBLEU q
 (c) Average #Queries
 (d) Attack%
Figure 4: Varying the perturbation % to study attack effectiveness on CodeT5 for the code translation task (C#-Java).
(a) Performance Drop
 (b) CodeBLEU q
 (c) # Queries
 (d) Average Success Rate
Figure 5: Ablation Study for Code Translation (C#-Java): Performance of different components of CodeAttack with random
(RAND) and vulnerable tokens (VUL) and two code-speciÔ¨Åc constraints: (i) Operator level (OP), and (ii) Token level (TOK).
them to rate the code consistency between the two using a
scale between 0 to 1; where 0 corresponds to ‚ÄòNot at all con-
sistent with the original code‚Äô, and 1 corresponds to ‚ÄòEx-
tremely consistent with the original code‚Äô. On average, the
code consistency was 0.71.
Discussion
Humans ‚Äòsummarize‚Äô code by reading function calls, fo-
cusing on information denoting the intention of the code
(such as variable names) and skimming over structural infor-
mation (such as while andfor loops) (Rodeghero et al.
2014). Pre-trained PL models operate in a similar manner
and do not assign high attention weights to the grammar or
the code structure (Zhang et al. 2022). They treat software
code as natural language (Hindle et al. 2016) and do not fo-
cus on compilation or execution of the input source code
before processing them to generate an output (Zhang et al.
2022). Through extensive experimentation, we demonstrate
that this limitation of the state-of-the-art PL models can be
exploited to generate adversarial examples in the natural
channel of code and signiÔ¨Åcantly alter their performance.
We observe that it is easier to attack the code translation
task rather than code repair or code summarization tasks.
Since code repair aims to Ô¨Åx bugs in the given code snip-
pet, it is more challenging to attack but not impossible.
For code summarization, the BLEU score drops by almost
50%. For all three tasks, CodeT5 is comparatively more ro-
bust whereas GraphCodeBERT is the most susceptible to
attacks using CodeAttack. CodeT5 has been pre-trained on
the task of ‚ÄòMasked IdentiÔ¨Åer Prediction‚Äô or deobsfuction
(Lachaux et al. 2021) where changing the identiÔ¨Åer names
does not have an impact on the code semantics. This helps
the model avoid the attacks which involve changing the iden-tiÔ¨Åer names. GraphCodeBERT uses data Ô¨Çow graphs in their
pre-training which relies on predicting the relationship be-
tween the identiÔ¨Åers. Since CodeAttack modiÔ¨Åes the iden-
tiÔ¨Åers and perturbs the relationship between them, it proves
to be extremely effective on GraphCodeBERT. This results
in a more signiÔ¨Åcant drop on GraphCodeBERT compared
to other models for the code translation task. The adversar-
ial examples from CodeAttack, although effective, can be
avoided if the pre-trained PL models compile/execute the
code before processing it. This highlights the need to in-
corporate explicit code structure in the pre-training stage to
learn more robust program representations.
Conclusion
We introduce, CodeAttack, a black-box adversarial attack
model to detect vulnerabilities of the state-of-the-art pro-
gramming language models. It Ô¨Ånds the most vulnerable to-
kens in a given code snippet and uses a greedy search mech-
anism to identify contextualized substitutes subject to code-
speciÔ¨Åc constraints. Our model generates adversarial exam-
ples in the natural channel of code. We perform an extensive
empirical and human evaluation to demonstrate the trans-
ferability of CodeAttack on several code-code and code-NL
tasks across different programming languages. CodeAttack
outperforms the existing state-of-the-art adversarial NLP
models, in terms of its attack effectiveness, attack quality,
and syntactic correctness. The adversarial samples generated
using CodeAttack are efÔ¨Åcient, effective, imperceptible, Ô¨Çu-
ent, and code consistent. CodeAttack highlights the need for
code-speciÔ¨Åc adversarial attacks for pre-trained PL models
in the natural channel.

--- PAGE 8 ---
References
Ahmad, W.; Chakraborty, S.; Ray, B.; and Chang, K.-W.
2021. UniÔ¨Åed Pre-training for Program Understanding and
Generation. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies , 2655‚Äì
2668.
Alzantot, M.; Sharma, Y .; Elgohary, A.; Ho, B.-J.; Srivas-
tava, M.; and Chang, K.-W. 2018. Generating Natural Lan-
guage Adversarial Examples. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language
Processing , 2890‚Äì2896.
Applis, L.; Panichella, A.; and van Deursen, A. 2021. As-
sessing Robustness of ML-Based Program Analysis Tools
using Metamorphic Program Transformations. In 2021 36th
IEEE/ACM International Conference on Automated Soft-
ware Engineering (ASE) , 1377‚Äì1381. IEEE.
Casalnuovo, C.; Barr, E. T.; Dash, S. K.; Devanbu, P.; and
Morgan, E. 2020. A theory of dual channel constraints.
In2020 IEEE/ACM 42nd International Conference on Soft-
ware Engineering: New Ideas and Emerging Results (ICSE-
NIER) , 25‚Äì28. IEEE.
Chakraborty, S.; Ahmed, T.; Ding, Y .; Devanbu, P. T.; and
Ray, B. 2022. NatGen: generative pre-training by ‚Äúnaturaliz-
ing‚Äù source code. In Proceedings of the 30th ACM Joint Eu-
ropean Software Engineering Conference and Symposium
on the Foundations of Software Engineering , 18‚Äì30.
Ebrahimi, J.; Rao, A.; Lowd, D.; and Dou, D. 2018. Hot-
Flip: White-Box Adversarial Examples for Text ClassiÔ¨Åca-
tion. In Proceedings of the 56th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 2: Short Pa-
pers) , 31‚Äì36.
Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;
Shou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Lan-
guages. In Findings of the Association for Computational
Linguistics: EMNLP 2020 , 1536‚Äì1547.
Gao, J.; Lanchantin, J.; Soffa, M. L.; and Qi, Y . 2018. Black-
box generation of adversarial text sequences to evade deep
learning classiÔ¨Åers. In 2018 IEEE Security and Privacy
Workshops (SPW) , 50‚Äì56. IEEE.
Garg, S.; and Ramakrishnan, G. 2020. BAE: BERT-based
Adversarial Examples for Text ClassiÔ¨Åcation. In Proceed-
ings of the 2020 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP) , 6174‚Äì6181.
Guo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Shujie, L.;
Zhou, L.; Duan, N.; Svyatkovskiy, A.; Fu, S.; et al. 2020.
GraphCodeBERT: Pre-training Code Representations with
Data Flow. In International Conference on Learning Repre-
sentations .
Henkel, J.; Ramakrishnan, G.; Wang, Z.; Albarghouthi, A.;
Jha, S.; and Reps, T. 2022. Semantic Robustness of Models
of Source Code. In 2022 IEEE International Conference on
Software Analysis, Evolution and Reengineering (SANER) ,
526‚Äì537.Hindle, A.; Barr, E. T.; Gabel, M.; Su, Z.; and Devanbu, P.
2016. On the naturalness of software. Communications of
the ACM , 59(5): 122‚Äì131.
Hsieh, Y .-L.; Cheng, M.; Juan, D.-C.; Wei, W.; Hsu, W.-L.;
and Hsieh, C.-J. 2019. On the robustness of self-attentive
models. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics , 1520‚Äì1529.
Husain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and
Brockschmidt, M. 2019. Codesearchnet challenge: Eval-
uating the state of semantic code search. arXiv preprint
arXiv:1909.09436 .
Jin, D.; Jin, Z.; Zhou, J. T.; and Szolovits, P. 2020. Is bert
really robust? a strong baseline for natural language attack
on text classiÔ¨Åcation and entailment. In Proceedings of the
AAAI conference on artiÔ¨Åcial intelligence , volume 34, 8018‚Äì
8025.
Lachaux, M.-A.; Roziere, B.; Szafraniec, M.; and Lample,
G. 2021. DOBF: A Deobfuscation Pre-Training Objective
for Programming Languages. Advances in Neural Informa-
tion Processing Systems , 34.
Lai, G.; Xie, Q.; Liu, H.; Yang, Y .; and Hovy, E. 2017.
RACE: Large-scale ReAding Comprehension Dataset From
Examinations. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing , 785‚Äì
794.
Li, J.; Ji, S.; Du, T.; Li, B.; and Wang, T. 2019. TextBug-
ger: Generating Adversarial Text Against Real-world Appli-
cations. In 26th Annual Network and Distributed System
Security Symposium .
Li, L.; Ma, R.; Guo, Q.; Xue, X.; and Qiu, X. 2020. BERT-
ATTACK: Adversarial Attack Against BERT Using BERT.
InProceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP) , 6193‚Äì6202.
Liu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .
2019. Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Lu, S.; Guo, D.; Ren, S.; Huang, J.; Svyatkovskiy, A.;
Blanco, A.; Clement, C. B.; Drain, D.; Jiang, D.; Tang, D.;
Li, G.; Zhou, L.; Shou, L.; Zhou, L.; Tufano, M.; Gong,
M.; Zhou, M.; Duan, N.; Sundaresan, N.; Deng, S. K.; Fu,
S.; and Liu, S. 2021. CodeXGLUE: A Machine Learning
Benchmark Dataset for Code Understanding and Genera-
tion. CoRR , abs/2102.04664.
Metropolis, N.; Rosenbluth, A. W.; Rosenbluth, M. N.;
Teller, A. H.; and Teller, E. 1953. Equation of state calcula-
tions by fast computing machines. The journal of chemical
physics , 21(6): 1087‚Äì1092.
Papernot, N.; Faghri, F.; Carlini, N.; Goodfellow, I.; Fein-
man, R.; Kurakin, A.; Xie, C.; Sharma, Y .; Brown, T.;
Roy, A.; et al. 2016. Technical report on the clever-
hans v2. 1.0 adversarial examples library. arXiv preprint
arXiv:1610.00768 .
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
Bleu: a Method for Automatic Evaluation of Machine Trans-
lation. In Proceedings of the 40th Annual Meeting of the As-

--- PAGE 9 ---
sociation for Computational Linguistics , 311‚Äì318. Philadel-
phia, Pennsylvania, USA: Association for Computational
Linguistics.
Pruthi, D.; Dhingra, B.; and Lipton, Z. C. 2019. Combating
Adversarial Misspellings with Robust Word Recognition. In
Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , 5582‚Äì5591.
Puri, R.; Kung, D. S.; Janssen, G.; Zhang, W.; Domeni-
coni, G.; Zolotov, V .; Dolby, J.; Chen, J.; Choudhury, M.;
Decker, L.; et al. 2021. Project codenet: A large-scale ai for
code dataset for learning a diversity of coding tasks. arXiv
preprint arXiv:2105.12655 , 1035.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
SQuAD: 100,000+ Questions for Machine Comprehension
of Text. In Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing , 2383‚Äì2392.
Austin, Texas: Association for Computational Linguistics.
Ren, S.; Deng, Y .; He, K.; and Che, W. 2019. Generating
Natural Language Adversarial Examples through Probabil-
ity Weighted Word Saliency. In Proceedings of the 57th An-
nual Meeting of the Association for Computational Linguis-
tics, 1085‚Äì1097. Florence, Italy: Association for Computa-
tional Linguistics.
Ren, S.; Guo, D.; Lu, S.; Zhou, L.; Liu, S.; Tang, D.; Sun-
daresan, N.; Zhou, M.; Blanco, A.; and Ma, S. 2020. Code-
bleu: a method for automatic evaluation of code synthesis.
arXiv preprint arXiv:2009.10297 .
Rodeghero, P.; McMillan, C.; McBurney, P. W.; Bosch, N.;
and D‚ÄôMello, S. 2014. Improving automated source code
summarization via an eye-tracking study of programmers.
InProceedings of the 36th international conference on Soft-
ware engineering , 390‚Äì401.
Tipirneni, S.; Zhu, M.; and Reddy, C. K. 2022. StructCoder:
Structure-Aware Transformer for Code Generation. arXiv
preprint arXiv:2206.05239 .
Tufano, M.; Watson, C.; Bavota, G.; Penta, M. D.; White,
M.; and Poshyvanyk, D. 2019. An empirical study on learn-
ing bug-Ô¨Åxing patches in the wild via neural machine trans-
lation. ACM Transactions on Software Engineering and
Methodology (TOSEM) , 28(4): 1‚Äì29.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. 2018. GLUE: A Multi-Task Benchmark and
Analysis Platform for Natural Language Understanding. In
Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP , 353‚Äì
355.
Wang, Y .; Wang, W.; Joty, S.; and Hoi, S. C. 2021. CodeT5:
IdentiÔ¨Åer-aware UniÔ¨Åed Pre-trained Encoder-Decoder Mod-
els for Code Understanding and Generation. In Proceedings
of the 2021 Conference on Empirical Methods in Natural
Language Processing , 8696‚Äì8708.
Wu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;
Macherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;
et al. 2016. Google‚Äôs neural machine translation system:
Bridging the gap between human and machine translation.
arXiv preprint arXiv:1609.08144 .Yang, P.; Chen, J.; Hsieh, C.-J.; Wang, J.-L.; and Jordan,
M. I. 2020. Greedy Attack and Gumbel Attack: Generating
Adversarial Examples for Discrete Data. J. Mach. Learn.
Res., 21(43): 1‚Äì36.
Yang, Z.; Shi, J.; He, J.; and Lo, D. 2022. Natural Attack for
Pre-Trained Models of Code. In Proceedings of the 44th In-
ternational Conference on Software Engineering , ICSE ‚Äô22,
1482‚Äì1493. New York, NY , USA: Association for Comput-
ing Machinery. ISBN 9781450392211.
Yefet, N.; Alon, U.; and Yahav, E. 2020. Adversarial exam-
ples for models of code. Proceedings of the ACM on Pro-
gramming Languages , 4(OOPSLA): 1‚Äì30.
Zhang, H.; Li, Z.; Li, G.; Ma, L.; Liu, Y .; and Jin, Z.
2020. Generating adversarial examples for holding robust-
ness of source code processing models. In Proceedings of
the AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 34,
1169‚Äì1176.
Zhang, Z.; Zhang, H.; Shen, B.; and Gu, X. 2022. Diet code
is healthy: Simplifying programs for pre-trained models of
code. In Proceedings of the 30th ACM Joint European Soft-
ware Engineering Conference and Symposium on the Foun-
dations of Software Engineering , 1073‚Äì1084.
Zhou, Y .; Zhang, X.; Shen, J.; Han, T.; Chen, T.; and Gall,
H. 2022. Adversarial robustness of deep code comment gen-
eration. ACM Transactions on Software Engineering and
Methodology (TOSEM) , 31(4): 1‚Äì30.

--- PAGE 10 ---
A Appendix
Downstream Tasks and Datasets
We evaluate the transferability of CodeAttack across differ-
ent sequence to sequence downstream tasks and datasets in
different programming languages.
‚Ä¢Code Translation involves translating one programming
language to the other. The publicly available code trans-
lation datasets2consists of parallel functions between
Java and C#. There are a total of 11,800 paired functions,
out of which 1000 are used for testing. The average se-
quence length for Java functions is 38.51 tokens, and the
average length for C# functions is 46.16.
‚Ä¢Code Repair reÔ¨Ånes code by automatically Ô¨Åxing bugs.
The publicly available code repair dataset (Tufano et al.
2019) consists of buggy Java functions as source and
their corresponding Ô¨Åxed functions as target. We use the
small dataset with 46,680 train, 5,835 validation, and
5,835 test samples ( 50 tokens in each function).
‚Ä¢Code Summarization involves generating natural lan-
guage summary for a given code. We use the Code-
SearchNet dataset (Husain et al. 2019) which consists
of code and their corresponding summaries in natu-
ral language. We show the results of our model on
Python (252K/14K/15K), Java (165K/5K/11K), and PHP
(241K/13K/15K). The numbers in the bracket denote the
samples in train/development/test set, respectively.
Victim Models
We pick a representative method from different categories as
our victim models to evaluate the attack.
‚Ä¢CodeT5 (Wang et al. 2021): A uniÔ¨Åed pre-trained
encoder-decoder transformer-based PL model that lever-
ages code semantics by using an identiÔ¨Åer-aware pre-
training objective. This is the state-of-the-art on several
sub-tasks in the CodeXGlue benchmark (Lu et al. 2021).
‚Ä¢CodeBERT (Feng et al. 2020): A bimodal pre-trained
programming language model that performs code-code
and code-NL tasks.
‚Ä¢GraphCodeBert (Guo et al. 2020): Pre-trained graph
programming language model thatleverages code struc-
turethrough data Ô¨Çow graphs.
‚Ä¢RoBERTa (Liu et al. 2019): Pre-trained natural lan-
guage model with state-of-art results on GLUE (Wang
et al. 2018), RACE (Lai et al. 2017), and SQuAD (Ra-
jpurkar et al. 2016) datasets.
We use the publicly available Ô¨Åne-tuned checkpoints for
CodeT5 and Ô¨Åne-tune CodeBERT, GraphCodeBERT, and
RoBERTa on the related downstream tasks.
Baseline Models Since CodeAttack operates in the natu-
ral channel of code, we compare with two state-of-the-art
adversarial NLP baselines for a fair comparison:
2http://lucene.apache.org/, http://poi.apache.org/,
https://github.com/eclipse/jgit/, https://github.com/antlr/‚Ä¢TextFooler (Jin et al. 2020) : Uses a combination of syn-
onyms, Part-Of-Speech (POS) checking, and semantic
similarity to generate adversarial text.
‚Ä¢BERT-Attack (Li et al. 2020) : Uses a pre-trained BERT
masked language model to generate adversarial examples
satisfying a certain similarity threshold.
Results
Downstream Performance and Attack Quality We mea-
sure the CodeBLEU and CodeBLEU to evaluate the down-
stream performance for code-code tasks (code repair and
code translation). The programming languages used are C#-
Java and Java-C# for translation tasks; and Java for code
repair tasks (Table 3 and Table 4). We show the results
for code-NL task for code summarization in BLEU and
BLEU . We show the results for three programming lan-
guages: Python, Java, and PHP (Table 3 and Table 5). We
measure the quality of the attacks using the metric deÔ¨Åned
in . The results follow a similar pattern as that seen in Sec-
tions .
Ablation Study: Qualitative Analysis Table 6 shows the
adversarial examples generated using the variants described
in Section .

--- PAGE 11 ---
Task Victim
ModelAttack
MethodAttack Effectiveness Attack Quality
Before After drop Success% #Queries #Perturb CodeBLEU q
Translate
(C#-Java)CodeT5TextFooler
73.9968.08 5.91 28.29 94.95 2.90 63.19
BERT-Attack 63.01 10.98 75.83 163.5 5.28 62.52
CodeAttack 61.72 12.27 89.3 36.84 2.55 65.91
CodeBERTTextFooler
71.1660.45 10.71 49.2 73.91 1.74 66.61
BERT-Attack 58.80 12.36 70.1 290.1 5.88 52.14
CodeAttack 54.14 17.03 97.7 26.43 1.68 66.89
GraphCode-
BERTTextfooler
66.8046.51 20.29 38.70 83.17 1.82 63.62
BERT-Attack 36.54 30.26 94.33 175.8 6.73 52.07
CodeAttack 38.81 27.99 98 20.60 1.64 65.39
Translate
(Java-C#)CodeT5TextFooler
87.0379.83 7. 20 32.3 62.91 2.19 81.28
BERT-Attack 68.81 18.22 86.3 89.99 2.79 74.52
CodeAttack 66.97 20.06 94.8 19.85 2.03 75.21
CodeBERTTextFooler
83.4873.52 9.96 55.9 38.57 2.14 73.93
BERT-Attack 67.94 15.5 70.3 159.1 5.76 46.82
CodeAttack 66.98 16.5 91.1 24.42 1.66 76.77
GraphCode-
BERTTextfooler
82.4074.32 8.2 51.2 39.33 2.03 72.45
BERT-Attack 64.87 17.5 76.6 134.3 5.90 47.47
CodeAttack 58.88 23.5 90.8 23.22 1.64 77.33
Repair
(small)CodeT5Textfooler
61.1357.59 3.53 58.84 90.50 2.36 69.53
BERT-Attack 52.70 8.43 94.33 262.5 15.1 53.60
CodeAttack 53.21 7.92 99.36 30.68 2.11 69.03
CodeBERTTextfooler
61.3353.55 7.78 81.61 45.89 2.16 68.16
BERT-Attack 51.95 9.38 95.31 183.3 15.7 61.95
CodeAttack 52.02 9.31 99.39 25.98 1.64 68.05
GraphCode-
BERTTextfooler
62.1654.23 7.92 78.92 51.07 2.20 67.89
BERT-Attack 53.33 8.83 96.20 174.1 15.7 53.66
CodeAttack 51.97 10.19 99.52 24.67 1.67 66.16
Summarize
(PHP)CodeT5TextFooler
20.0614.96 5.70 64.6 410.15 6.38 53.91
BERT-Attack 11.96 8.70 78.4 1014.1 7.32 51.34
CodeAttack 11.06 9.59 82.8 314.87 10.1 52.67
CodeBERTTextfooler
19.7614.38 5.37 61.10 358.43 2.92 54.10
BERT-Attack 11.30 8.35 56.47 1912.6 15.8 46.24
CodeAttack 10.88 8.87 88.32 204.46 2.57 52.95
RoBERTaTextFooler
19.0614.06 4.99 62.60 356.68 2.80 54.11
BERT-Attack 11.34 7.71 60.46 1742.3 17.1 46.95
CodeAttack 10.98 8.08 87.51 183.22 2.62 53.03
Summarize
(Python)CodeT5TextFooler
20.3612.11 8.25 90.47 400.06 5.26 77.59
BERT-Attack 8.22 12.14 97.81 475.61 6.91 66.27
CodeAttack 7.97 12.39 98.50 174.05 5.10 69.17
CodeBERTTextFooler
26.1722.76 3.41 68.50 966.19 3.83 75.15
BERT-Attack 22.88 3.29 84.41 941.94 3.35 56.31
CodeAttack 18.69 7.48 86.63 560.68 3.23 59.11
RoBERTaTextfooler
17.0110.72 6.29 63.34 788.25 3.57 70.48
BERT-Attack 10.66 6.35 74.64 1358.8 4.07 51.74
CodeAttack 9.50 7.51 76.09 661.75 3.46 61.22
Summarize
(Java)CodeT5TextFooler
19.7714.06 5.71 67.80 291.82 3.76 90.82
BERT-Attack 11.94 7.83 77.37 811.97 17.4 45.71
CodeAttack 11.21 8.56 80.80 198.11 7.43 90.04
CodeBERTTextFooler
17.6516.44 1.21 42.4 400.78 4.07 90.29
BERT-Attack 15.49 2.16 46.51 1531.1 10.9 37.61
CodeAttack 14.69 2.96 73.70 340.99 3.27 59.37
RoBERTaTextfooler
16.4713.23 3.24 44.9 383.36 4.02 90.87
BERT-Attack 11.89 4.58 42.59 1582.2 9.21 37.86
CodeAttack 11.74 4.73 50.14 346.07 3.29 48.48
Table 3: Results on code translation, code repair, and code summarization tasks. The performance is measured in CodeBLEU
for Code-Code tasks and in BLEU for Code-NL (summarization) task. The best result is in boldface ; the next best is underlined.

--- PAGE 12 ---
Original Code TextFooler BERT-Attack CodeAttack
public string GetFullMessage()
{
...
if(msgB < 0){ return string.
Empty;}
...
return RawParseUtils.Decode(
enc, raw, msgB, raw.
Length);}citizenship string
GetFullMessage() {
...
if(msgB < 0){ return string.
Empty;}
...
return RawParseUtils.Decode(
enc, raw, msgB, raw.
Length);}loop string GetFullMessage() {
...
if(msgB < 0){ return string.
Empty;}
...
returnhereq.dir,(x) raw,
msgB, raw.Length);}public string GetFullMessage()
{
...
if(msgB=0){return string.
Empty;}
...
return RawParseUtils.Decode(
enc, raw, msgB, raw.
Length);}
CodeBLEU before : 77.09 drop : 18.84; CodeBLEU q: 95.11 drop : 15.09; CodeBLEU q: 57.46 drop : 21.04; CodeBLEU q: 88.65
public override void WriteByte(
byte b) {
if(outerInstance.upto ==
outerInstance.blockSize)
{... }}audiencesrevokedcanceling
WriteByte(byte b) {
if(outerInstance.upto ==
outerInstance.blockSize)
{.... }}public override void;.b) {
if(outerInstance.upto ==
outerInstance.blockSize)
{... }}public override void WriteByte(
bytes b) {
if(outerInstance.upto ==
outerInstance.blockSize)
{... }}
CodeBLEU before :100 drop :5.74; CodeBLEU q: 63.28 drop :27.26; CodeBLEU q:49.87 drop :20.04; CodeBLEU q: 91.69
Table 4: Qualitative examples of perturbed codes using TextFooler, BERT-Attack, and CodeAttack on Code Translation task.
Original TextFooler BERT-Attack CodeAttack
protected finalvoid
fastPathOrderedEmit(U
value, boolean delayError,
Disposable disposable) {
final Observer<? super V>
observer = downstream;
final ..... {
if(q.isEmpty()) {
accept(observer,
value);
if(leave(-1) == 0)
{
return ;
}
}else {
q.offer(value);
}
}else {
q.offer(value);
if(!enter()) {
return ;
}
}
QueueDrainHelper.drainLoop(
q, observer,
delayError, disposable
,this );
}protected finalinvalidate
fastPathOrderedEmit(U
value, boolean delayError,
Disposable disposable) {
finalizing Observer < ? super
V > observer =
downstream;
final .... {
if(q.isEmpty()) {
accept(observer, value);
if(leave(-1) == 0) {
return ;
}
}
yet {
q.offer(value);
}
}
annan {
q.offer(value);
than(!enter()) {
return ;
}
}
QueueDrainHelper.drainLoop(q,
observer, delayError,
disposable, this );
}(;period fastPathOrderedEmit(U
value,this)0c|/,)(
forepas Observer<? super
V > observer = downstream;
final .... {
if(q.isEmpty()) {
accept(),point0while
(leave...)]a
)]]returnspublic
.next,managerq
.offer,value
)Àò3009
thereforereturn
drawq.offer,
value)...)?
enter)public
}
QueueDrainHelper.drainLoop(q,
observer, ))10c))
,these?
}protected finalstatic
fastPathOrderedEmit( M
value, boolean ZdelayExc ,
Disposable diszyk) {
final Observer <? super V >
observer = downstream;
final .... {
if(q.isEmpty()) {
accept(observer, value);
if(leave(-1) == 0) {
continue;
}
}catch {
q.offer(value);
}
}else {
q.offer(value);
if(!exit() ) {
return ;
}
}
QueueDrainHelper.drainLoop(q,
observer, delayInfo ,
disposable, this );
}
Makes sure the fast-path emits
in orderThis method is used to avoid the
need for the fast path to emit a
value to the downstream.period 0jjjjjj dddddsssss
Table 5: Qualitative examples of adversarial codes and the generated summary using TextFooler, BERT-Attack, and CodeAttack
on Code Summarization task.

--- PAGE 13 ---
Original Code CodeAttack VUL CodeAttack VUL+OP CodeAttack VUL+OP+TOK
public void AddMultipleBlanks(
MulBlankRecord mbr) {
for (int j = 0; j < mbr.
NumColumns; j++) {
BlankRecord br = new
BlankRecord();
br.Column = j + mbr.
FirstColumn;
br.Row = mbr.Row;
br.XFIndex = (mbr.GetXFAt(j
));
InsertCell(br);
}
}((void AddMultipleBlanks(
MulBlankRecord mbr) {
for (int j?0; j < mbr.
NumColumns; j++) {
BlankRecord br = new
BlankRecord();
br.Column = j + mbr.
FirstColumn;
br.Row = mbr.Row;
br.XFIndex = (mbr.
GetXFAt(j));
InsertCell(br);
}
}public void AddMultipleBlanks(
MulBlankRecord mbr) {
for (int j>0; j < mbr.
NumColumns; j++) {
BlankRecord br = -new
BlankRecord();
br.Column = j + mbr.
FirstColumn;
br.Row = mbr.Row;
br.XFIndex >(mbr.GetXFAt(j
));
InsertCell(br);
}
}static void AddMultipleBlanks(
MulBlankRecord mbr) {
for (int j > 0;jj< mbr.
NumColumns; j++) {
BlankRecord br = new
BlankRecord();
br.Column = j + mbr.
FirstColumn;
br.Row = mbr.Row;
br.XFIndex = (mbr.GetXFAt(j
));
InsertCell(br);
}
}
CodeBLEU before : 76.3 drop : 7.21; CodeBLEU q: 43.85 drop : 5.85; CodeBLEU q: 69.61 drop : 12.96; CodeBLEU q: 59.29
public string GetFullMessage()
{
byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);
if(msgB < 0) {
return string.Empty;
}
Encoding enc = RawParseUtils.
ParseEncoding(raw);
return RawParseUtils.Decode(
enc, raw, msgB, raw.
Length);
}Àò0120public string
GetFullMessage() {
byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);
if(msgB < 0) {
return string.Empty;
}
Encoding enc = RawParseUtils.
ParseEncoding(raw);
return RawParseUtils.Decode(
enc,RAW.. , msgB, raw.
Length);
}public string GetFullMessage()
{
byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);
if(msgB=0) {
return string.Empty;
}
Encoding enc = RawParseUtils.
ParseEncoding(raw);
return RawParseUtils.Decode(
enc, raw, msgB, raw.
Length);
}static string GetFullMessage()
{
byte[] raw = buffer;
int msgB = RawParseUtils.
TagMessage(raw, 0);
if(msgB < 0 {
return string.Empty;
}
Encoding enc = RawParseUtils.
ParseEncoding(raw);
return RawParseUtils.Decode(
enc, raw, MsgB ,raw.
Length);
}
CodeBLEU before :77.09 drop : 10.42; CodeBLEU q: 64.19 drop : 21.93; CodeBLEU q: 87.25 drop : 22.8; CodeBLEU q: 71.30
Table 6: Qualitative examples for the ablation study on CodeAttack: Attack vulnerable tokens (VUL); with operator level
constraints (VUL+OP), and with token level (VUL+OP+TOK) contraints on code translation task.
