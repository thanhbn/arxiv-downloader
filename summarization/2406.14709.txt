# 2406.14709.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/summarization/2406.14709.pdf
# File size: 502642 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Factual Dialogue Summarization via Learning from Large Language
Models
Rongxin Zhu Jey Han Lau Jianzhong Qi
School of Computing and Information Systems
The University of Melbourne
rongxinz1@student.unimelb.edu.au, {laujh, jianzhong.qi}@unimelb.edu.au
Abstract
Factual consistency is an important quality
in dialogue summarization. Large language
model (LLM)-based automatic text summariza-
tion models generate more factually consistent
summaries compared to those by smaller pre-
trained language models, but they face deploy-
ment challenges in real-world applications due
to privacy or resource constraints. In this paper,
we investigate the use of symbolic knowledge
distillation to improve the factual consistency
of smaller pretrained models for dialogue sum-
marization. We employ zero-shot learning to
extract symbolic knowledge from LLMs, gen-
erating both factually consistent (positive) and
inconsistent (negative) summaries. We then ap-
ply two contrastive learning objectives on these
summaries to enhance smaller summarization
models. Experiments with BART, PEGASUS,
and Flan-T5 indicate that our approach sur-
passes strong baselines that rely on complex
data augmentation strategies. Our approach
achieves better factual consistency while main-
taining coherence, fluency, and relevance, as
confirmed by various automatic evaluation met-
rics. We also provide access to the data and
code to facilitate future research1.
1 Introduction
Automatic text summarization aims to create a con-
cise summary of a source document that keeps
all the essential points. Although current mod-
els are capable of generating fluent and coher-
ent summaries, one main issue is factual incon-
sistency, where generated summaries are found to
contain facts that are absent from or contradict the
source (Maynez et al., 2020; Huang et al., 2021).
To tackle this, a number of methods have been
proposed, including explicit fact modeling (Zhu
et al., 2021; Huang et al., 2020), post-editing (Lee
et al., 2022; Balachandran et al., 2022; Chen et al.,
1https://github.com/731935354/symbolic_
distill_contrastive_summ
Teacher ModelPositive SummariesStudent ModelNegative Summariesextract symbolic knowledgeContrastive learningSequence-level DistillationFigure 1: An overview of our framework to leverage
symbolic knowledge distillation to improve the factual
consistency for smaller (student) models in dialogue
summarization.
2021a) and contrastive learning (Wan and Bansal,
2022a; Cao and Wang, 2021; Liu et al., 2021). Con-
trastive learning-based methods, in particular, offer
a straightforward solution without requiring any
modification to the model architecture, but their
performance hinges on careful and often rule-based
construction of negative samples (Cao and Wang,
2021; Liu et al., 2021; Wan and Bansal, 2022a).
The rise of large language models (LLMs)
changed the landscape of NLP, and they exhibit
emergent capabilities (Wei et al., 2022) such as in-
context learning (Brown et al., 2020; Min et al.,
2022) and instruction following (Ouyang et al.,
2022). We have seen zero- or few-shot prompting
with LLMs achieving strong performance on vari-
ous NLP tasks (Wei et al., 2021; Ye et al., 2021) in-
cluding summarization (Zhang et al., 2023), show-
ing better coherence, relevance and factual consis-
tency than human-written reference summaries.
Although impressive, LLMs are not always de-
ployable in real-world applications due to substan-
tial computational resources (Strubell et al., 2019)
or privacy concerns (as many state-of-the-art LLMs
are closed source and can only be accessed via
APIs). Thus, it is important to construct more cost-
efficient and compact models with similar summa-arXiv:2406.14709v1  [cs.CL]  20 Jun 2024

--- PAGE 2 ---
rization capabilities. To this end, knowledge distil-
lation (Hinton et al., 2015) — a technique that can
transfer the knowledge from a large teacher model
to a small student model — has been explored (Sun
et al., 2020; Aguilar et al., 2020). Symbolic knowl-
edge distillation (West et al., 2022), a special form
of knowledge distillation, extracts symbolic knowl-
edge (e.g., textual information) from the teacher
model and uses such knowledge as training signal
for the student model. This method is especially
useful when working with blackbox teacher models
where we do not have access to their output prob-
ability distribution (which is the case for closed
source LLMs such as ChatGPT).
In this paper, we explore symbolic knowledge
distillation to improve the factual consistency of
(smaller) pretrained models in dialogue summa-
rization. Concretely, we extract symbolic knowl-
edge from an LLM teacher ( gpt-3.5 turbo ) in the
format of positive summaries andnegative sum-
maries . Positive summaries are factually con-
sistent with the source article (i.e., a dialogue)
while negative summaries are not. We experi-
ment with various strategies to incorporate these
summaries and train the student model, including
sequence-level knowledge distillation (Kim and
Rush, 2016) and two contrastive learning-based
methods. Our experiments cover three widely used
pretrained models: BART (Lewis et al., 2020), PE-
GASUS (Zhang et al., 2020), and Flan-T5 (Chung
et al., 2024) on two popular dialogue summariza-
tion datasets: SAMSum (Gliwa et al., 2019a) and
DialogSum (Chen et al., 2021b).
To summarize, our contributions are as follows:
•We propose to improve the factual consistency
of (small) dialogue summarization models via
symbolic knowledge distillation from LLMs.
•We experiment with LLMs to generate not
only factually consistent summaries but also
inconsistent ones, and we incorporate such
summaries to train small dialogue summariza-
tion models with two contrastive objectives.
•We discovered that: (1) symbolic knowledge
distillation enables us to create smaller di-
alogue summarization models that surpass
strong baselines; and (2) the top-performing
student model achieves comparable or even
better factual consistency compared to human-
written references without compromisingother quality dimensions such as fluency or
coherence.
2 Related Work
2.1 Evaluating and Enhancing Factual
Consistency
We summarize two areas of factuality research:
evaluation andenhancement .
Automatic evaluation metrics are generally con-
structed on question-answering systems (Fabbri
et al., 2022; Scialom et al., 2021; Durmus et al.,
2020; Manakul et al., 2023) or textual entailment
models (Kryscinski et al., 2020; Goyal and Durrett,
2020; Laban et al., 2022; Zhang et al., 2024). More
recent methods leverage the capability of LLMs
to follow zero-shot and few-shot instructions (Fu
et al., 2023; Min et al., 2023; Liu et al., 2023b).
Another line of work aims at developing metrics
that can detect the factual consistency between text
pairs in different tasks (Deng et al., 2021; Zha et al.,
2023a), such as a knowledge-grounded dialogue.
Methods to enhance the factual consistency of
summarization models mainly fall into the follow-
ing categories: explicit modeling of the facts in
source documents (Zhu et al., 2021; Huang et al.,
2020), post-editing model generated summaries for
better factual consistency (Lee et al., 2022; Bal-
achandran et al., 2022; Chen et al., 2021a), training
summarization model with less noisy data by data
filtering (Nan et al., 2021; Goyal and Durrett, 2021;
Wan and Bansal, 2022a), and data augmentation-
based methods (Wang et al., 2022b; Adams et al.,
2022). The last category is usually combined with
contrastive learning (Wan and Bansal, 2022b; Liu
et al., 2021; Cao and Wang, 2021), which has
shown a high effectiveness. However, contrastive
learning often involves complex strategies to con-
struct negative samples. For example, Cao and
Wang (2021) use a combination of multiple meth-
ods including entity swapping, content masking
and refilling, and low-confidence model genera-
tions.
Our work falls into the data augmentation and
contrastive learning category. We adopt LLMs
to construct negative samples with more diversity
compared to previous strategies that have been pre-
dominantly driven by rules and heuristics.
2.2 Symbolic Knowledge Distillation
Symbolic knowledge distillation (West et al.,
2022) is a conceptual framework originally pro-
posed for constructing common-sense knowledge

--- PAGE 3 ---
Given the dialogue and summary: dialogue: \n{dialogue} \nsummary: \n{summ}Can you modify the summary so that it contains some factual errors? Do not only replace words. You can replace phrases or larger units. Please make the errors hard to notice for readers. Please explain where and why factual errors happen in the modified summary. Use the following format in your response:Modified summary: \n{modifiedsummary} \nFactual errors: \n{factualerrors}Summarize the following dialogue in less than 4 sentences: {dialog}Chris: What are your plans now for Halloween? Amka: Just gonna stay in? Mick: Yeah Mick: nothing special <file_gif> Amka: Fair enough I never used to do anything for Halloween. Chris: in Poland we actually don'tcelebrate Halloween and don't do all this dressing up, pumpking curving and so on. Chris: on November 1st we just go to the cementary with our families and after that spend time together. Amka: I like it like this.Chris explains that in Poland, they don't celebrate Halloween and instead visit cemeteries with families on November 1st. Amka agrees that she likes this tradition, and Mick says he has no special plans for Halloween.
Chris mentioned that in Poland, people celebrate Halloween by carving out pumpkins and dressing up in various costumes. He also said that they have a tradition of visiting cemeteries on Trafalgar Day. Amka agreed that Halloween is her favorite holiday, Mick says he has no special plans for Halloween.DialoguePrompt ChatGPTPrompt ChatGPT
Summary with Factual ErrorsFactually Consistent Summary Figure 2: To extract symbolic knowledge from the teacher model (ChatGPT) for contrastive learning, we first prompt
ChatGPT to generate a factually consistent summary, then use another prompt to instruct ChatGPT to modify the
summary into a factually inconsistent version. The contents in red contain factual errors against the source dialogue.
graphs (Sap et al., 2019). A key advantage of the
framework is that it does not require optimizing the
student model on the teacher model’s output prob-
abilities, which was done in standard knowledge
distillation (Hinton et al., 2015). Instead, it extracts
symbolic knowledge (e.g., text) from the teacher
model to construct a smaller student model.
Symbolic knowledge distillation has been used
to construct better summarization models in differ-
ent ways, motivated by the high-quality summaries
generated by zero-shot and few-shot LLMs (Zhang
et al., 2023), which are even preferred over human-
written summaries. For example, Sclar et al.
(2022) construct reference-free sentence summa-
rization models with better controllability on the
compression ratio, while Song et al. (2023) en-
hance summary abstractiveness via calibrated dis-
tillation. Liu et al. (2023c) use LLMs not only as a
data augmenter to generate “quasi-references”, but
also as a summary evaluator to provide additional
training signals. Jiang et al. (2024) distill LLM’s
summarization capability by generating multiple
aspect-triple rationales and summaries, then utilize
curriculum learning to train student models.
Our method differs from these studies by incor-
porating a stage that leverages both positive and
negative summaries through contrastive learning
to enhance the factual consistency of student mod-
els, while the studies above only consider positiveexamples.
3 Methodology
Given a dialogue D(aka “source documents” in
document summarization studies), we aim to gen-
erate a summary Susing a summarization model g
that captures the main ideas of D. We specifically
encourage Sto be factually consistent with D, i.e.,
only including information directly found in Dand
not any information against the facts in D.
To construct more factually consistent and cost-
effective dialogue summarization models, we first
extract symbolic knowledge (i.e., augmented sum-
maries) from a teacher model (ChatGPT), then
use sequence-level knowledge distillation and con-
trastive learning to exploit the knowledge. An
overview of our framework is shown in Figure 1.
3.1 Extracting Symbolic Knowledge
We use ChatGPT ( gpt-3.5-turbo ) to generate posi-
tive summaries which are supposed to be factually
consistent with the source dialogue D, and nega-
tive summaries that contain factual errors against
D. Specifically, we first prompt ChatGPT to gen-
erate k(k= 3) positive summaries for a dialogue,
then we prompt it again to modify each positive
summary into a negative one by modifying snip-
pets of the summary (so we also have knegative
summaries). An example is shown in Figure 2. We

--- PAGE 4 ---
find that the quality of negative summaries improve
when we explicitly prompt ChatGPT to explain the
factual errors2.
3.2 Utilising Symbolic Knowledge
The standard method to train summarization mod-
els is Maximum Likelihood Estimation ( MLE ).
Specifically, given a single reference summary R∗,
the summarization model gis encouraged to give
thei-th token of R∗the maximum probability
among all tokens in the vocabulary, based on the
prefix string of the current token. The loss function,
cross entropy, is defined as follows:
lmle=−log(R∗|D)
=−nX
i=1logPg(R∗
i|D, R∗
<i)(1)
Here, R∗
iis the i-th token in R∗;R∗
<irepresents the
tokens preceding R∗
i; and Pgis the probability dis-
tribution of the summarization model. Since there
is only one reference summary, the loss function en-
courages the model to approximate the point mass
distribution defined by the single reference (Liu
et al., 2023c). As the loss function is defined at
the word level in an autoregressive manner, it does
not explicitly facilitate the factual consistency of
the generated summary, which requires signals at
semantic level and sequence level.
3.2.1 Sequence-level Distillation
Given that a large teacher model may gener-
ate more factually consistent summaries than the
smaller student models, we employ Sequence-level
Knowledge Distillation ( SEQDISTILL ) (Kim and
Rush, 2016). This approach involves generating
multiple quasi-summaries from the teacher model,
which are then utilized as targets for fine-tuning
the student models using cross-entropy loss. Given
a set of positive summaries P∗generated by the
teacher model, and the original human-written ref-
erence summary R∗, the loss function is as follows:
ls=−1
|P∗∪{R∗}|P
R∈P∗∪{R∗}logPg(R|D)
The primary distinction between SEQDISTILL
and Maximum Likelihood Estimation (MLE) lies
2The average factual consistency (AlignScore) for 200 ran-
dom positive summaries in the training set from the teacher
model is 0.90 for SAMSum and 0.92 for DialogSum, indicat-
ing that positive summaries are mostly factually consistent.
More details in Appendix A.2.in their method of distribution approximation. S E-
QDISTILL aims to approximate the teacher model’s
distribution, favoring multiple factually consistent
summaries via a sampling-based method. Con-
versely, MLE approximates a point-mass distribu-
tion, where a single reference summary is given all
the probability mass.
3.2.2 Contrastive Learning
We further incorporate two types of contrastive
learning methods to boost the factual consistency
of summarization models by incorporating negative
summaries on top of S EQDISTILL .
LetPbe a set of positive summaries that are
factually consistent with the source dialogue D,N
be a set of negative summaries that contain factual
errors against D, and Rbe the target for cross
entropy loss. A training instance with contrastive
learning is a tuple (D, R,P,N). The loss function
for a single training instance is defined as:
l=lmle+α·lc (2)
where lcis the contrastive loss, α∈[0,1]is a hyper-
parameter to balance the two loss terms. Intuitively,
lcserves as a regularization term that shapes the
distribution of the summarization model to favor
factually consistent summaries. We employ two
contrastive objectives, MARGIN CONTRAST and
PAIRCONTRAST , which differentiate between pos-
itive and negative summaries at the sequence and
latent representation level, respectively.
MARGIN CONTRAST aims to pull apart the posi-
tive summaries and negative summaries by enforc-
ing a gap between sequence-level scores. Specif-
ically, we aim to achieve higher scores for even
theworst positive summaries than those of the best
negative summaries , with the following loss:
lc= max {0, θ+ max {S(N)} −min{S(P)}}
(3)
Here, θis the target score threshold, and S(·)is a
scoring function. Inspired by BARTScore (Yuan
et al., 2021), we define the scoring function S(·)for
a summary Xusing the summarization model gas
the length-normalized log-likelihood of all tokens:
S(X) =1
mmX
i=1logPg(xi|D, X <i) (4)
Here, mrepresents the number of tokens in X;xi
is the i-th token; and X<iare the preceding tokens.

--- PAGE 5 ---
Dataset #Train #Dev #Test#Speakers
#dial.#Turns
#dial.#Tokens
dial.
SAMSum 14,732 818 819 2.39 9.5 94
DialogSum 12,460 500 500 2.01 11.1 131
Table 1: Dataset statistics. #Train ,#Dev and#Test
refer to the numbers of dialogue-summary pairs (one
summary per dialogue) in the training, development,
and testing subsets.#Speakers
#dial.,#Turns
#dial., and#Tokens
dial.refer to
the average numbers of speakers, turns, and tokens in
each dialogue.
Normalizing by meliminates the impact of length
on the evaluation of factual consistency.
PAIRCONTRAST differentiates positive from neg-
ative summaries by minimizing the similarities be-
tween their latent representations, while simultane-
ously maximizing the similarities among positive
pairs. Let ri,rj, and rkbe summaries from either
PorN. We use hihj, andhkto denote the vector-
form representations of these summaries. The con-
trastive loss lcis defined in accordance with the
fomulation provided by Cao and Wang (2021) as
follows:
lc=−1 |P|
2X
ri,rj∈P
ri̸=rjlogexp( s(hi,hj)/τ)P
rk∈P∪N
rk̸=riexp( s(hi,hk)/τ)
(5)
Here, sis the cosine function; and τis a tempera-
ture parameter ( τ=1 in our experiments). We fol-
low Cao and Wang (2021) to obtain the vector rep-
resentations of the summaries by applying an MLP
projection to the averaged last-layer outputs from
the decoder for all tokens.
To summarize, MARGIN CONTRAST uses sum-
mary log-likelihood estimated by the summariza-
tion model directly, while PAIRCONTRAST relies
on the internal representation of summary words.
4 Experiment Setup
4.1 Datasets
We adopt two popular dialogue summarization
datasets: SAMSum (Gliwa et al., 2019a) and Di-
alogSum (Chen et al., 2021b). SAMSum is a collec-
tion of messenger-like conversations, while Dialog-
Sum contains daily conversations in a more real-
life setting. In both datasets, there is one human-
written reference summary for each conversation
in the training split. Table 1 shows the statistics of
the two datasets.4.2 Student Models
We choose BART (Lewis et al., 2020), PEGA-
SUS (Zhang et al., 2020) and Flan-T5 (Chung et al.,
2024) as the student models, which have consis-
tently demonstrated state-of-the-art performance in
automatic text summarization (Zhao et al., 2022;
Liu and Liu, 2021; Chung et al., 2024). Specifi-
cally, we use facebook/bart-large ,google/pegasus-
large ,google/flan-t5-large as initial checkpoints.
The number of learnable parameters for these mod-
els are 406 million, 568 million and 770 million,
respectively, which are much smaller than that of
the teacher model.
4.3 Baseline Models
FACTPEGASUS (Wan and Bansal, 2022a): an
abstractive text summarization model for news
summarization. It enhances factual consistency
through several strategies: (1) factuality-oriented
pre-training, (2) reference summary correction that
addresses potential factual errors in reference sum-
maries, (3) contrastive learning to boost the model’s
ability to differentiate between positive and nega-
tive summaries, where the negative summaries are
constructed by rule-based entity swapping, (4) pre-
training task simulation during fine-tuning that min-
imizes the gap between the pre-training and fine-
tuning phases. We used their pre-trained model and
code to fine-tune on our datasets.3
SWING (Huang et al., 2023): an abstractive dia-
logue summarization model that achieves state-of-
the-art factual consistency and coverage on SAM-
Sum and DialogSum. It leverages an uncovered
loss to boost information coverage, and a con-
trastive loss to enhance factual consistency. We
use their model generations directly.4
We also include the original human-written ref-
erence summaries ( HUMAN REF) to assess the rela-
tive quality compared to our method.
4.4 Evaluation Metrics
We selected multiple reference-free evaluation met-
rics, recognizing that our methods may produce
high-quality summaries that diverge from human-
written references. This divergence could lead to
underrating by reference-based metrics. To assess
factual consistency, we employed two state-of-the-
art (SOTA) automatic metrics: an LLM-based met-
ric,G-E VAL (Liu et al., 2023a), and a non-LLM-
3https://github.com/meetdavidwan/factpegasus
4https://github.com/amazon-science/AWS-SWING

--- PAGE 6 ---
SAMSum DialogSum
Const UniEval ROUGE Const UniEval ROUGE
Model SASGCoh Flu Rel R1 R2 SASGCoh Flu Rel R1 R2
HUMAN REF0.80 4.80 0.92 0.93 0.97 1.00 1.00 0.82 4.84 0.94 0.92 0.98 1.00 1.00
Baselines
FACTPEGASUS 0.63 3.08 0.87 0.90 0.73 0.45 0.20 0.67 3.44 0.88 0.87 0.77 0.49 0.24
SWING 0.82 4.38 0.93 0.93 0.84 0.52 0.28 0.83 4.54 0.95 0.93 0.90 0.53 0.29
MLE
BART 0.82 4.27 0.92 0.93 0.84 0.52 0.28 0.80 4.22 0.94 0.93 0.88 0.53 0.28
PEGASUS 0.81 4.12 0.93 0.94 0.84 0.50 0.26 0.83 4.44 0.96 0.93 0.90 0.52 0.28
Flan-T5 0.82 4.34 0.93 0.93 0.84 0.52 0.28 0.84 4.65 0.96 0.93 0.91 0.54 0.29
SEQDISTILL (Our Method)
BART 0.87 4.41 0.96 0.94 0.89 0.36 0.14 0.93 4.81 0.98 0.930.93 0.29 0.13
PEGASUS 0.89 4.52 0.950.94 0.89 0.39 0.17 0.90 4.73 0.97 0.930.91 0.42 0.22
Flan-T5 0.88 4.51 0.94 0.93 0.87 0.40 0.17 0.91 4.80 0.96 0.93 0.90 0.32 0.15
MARGIN CONTRAST (Our Method)
BART 0.89 4.73 0.970.94 0.90 0.40 0.18 0.93 4.72 0.98 0.94 0.93 0.31 0.15
PEGASUS 0.87 4.08 0.92 0.94 0.84 0.38 0.17 0.89 4.31 0.95 0.93 0.88 0.34 0.17
Flan-T5 0.90 4.69 0.95 0.94 0.88 0.42 0.20 0.91 4.76 0.95 0.93 0.90 0.37 0.19
PAIRCONTRAST (Our Method)
BART 0.91 4.69 0.98 0.94 0.92 0.37 0.15 0.93 4.80 0.98 0.930.93 0.30 0.14
PEGASUS 0.89 4.47 0.96 0.94 0.89 0.38 0.16 0.91 4.62 0.96 0.94 0.91 0.36 0.18
Flan-T5 0.91 4.74 0.96 0.94 0.90 0.38 0.16 0.93 4.86 0.96 0.93 0.89 0.37 0.19
Table 2: Comparing different models and training strategies on Consistency (Const), Coherence (Coh), Fluency
(Flu), Relevance (Rel) and ROUGE. We use two automatic factual consistency metrics, AlignScore ( SA) and G-Eval
(SG). Coherence, Fluency and Relevance are obtained from UniEval. R1 and R2 represent the F1 score of ROUGE
1 and ROUGE 2, respectively. We show the highest score(s) in all columns for the same model (e.g., BART) across
{MLE ,SEQDISTILL ,MARGIN CONTRAST ,PAIRCONTRAST } inbold to show the most effective training strategy.
based metric, ALIGN SCORE (Zha et al., 2023b)5.
This approach mitigates the potential bias of favor-
ing LLM-generated summaries inherent in LLM-
based metrics (Liu et al., 2023a). Additionally, we
used UNIEVAL (Zhong et al., 2022a) to evaluate
Coherence, Fluency, and Relevance. We also uti-
lized the standard n-gram matching-based metric,
ROUGE (Lin, 2004), primarily as a sanity check
for models trained using MLE.
4.5 Other Experimental Details
ForMARGIN CONTRAST andPAIRCONTRAST , we
merge the human-written reference R∗and posi-
5Our meta-evaluation on multiple dialogue summarization
datasets show that AlignScore and G-Eval exhibit high corre-
lation (0.4-0.7) with human evaluation results. More details in
Appendix A.3.tive summaries P∗generated by the teacher model
as the positive set P′={R∗} ∪ P∗. For each
training sample, we select one element R∈ P′as
the target for cross-entropy loss and use the rest as
Pfor contrastive loss. All models are fine-tuned
for 15,000 steps and evaluated at every 500 steps.
The best checkpoint is selected according to Align-
Score on the development set. We provide more
implementation details in Appendix A.4.
5 Results and Discussions
5.1 The Effectiveness of Symbolic Knowledge
Distillation and Contrastive Learning
We compare the performance of our methods
(SEQDISTILL ,MARGIN CONTRAST and PAIR-

--- PAGE 7 ---
CONTRAST ) and the baseline models on various
quality dimensions, with a focus on factual consis-
tency. From the results in Table 2, we make the
following observations:
•Our distillation methods improve factual con-
sistency (compared to baseline models and
MLE methods) without sacrificing in other
quality dimensions (i.e., Coherence, Fluency
and Relevance).
•Our distillation methods consistently enhance
the factual consistency of all pretrained mod-
els (BART, PEGASUS and Flan-T5). PAIR-
CONTRAST is generally the most effective
method, although there is some performance
variation depending on the dataset and pre-
trained model.
•SEQDISTILL and two contrastive learning
methods result in significantly lower Rouge
scores compared to MLE. However, it only
tells us that there are fewer word overlaps
between model generated summaries and
human-written references rather than an ac-
tual quality decline. We will revisit this again
with a case study in section 5.4.
•Flan-T5 in most cases generate more factu-
ally consistent summaries than BART and PE-
GASUS across different settings ( MLE ,SE-
QDISTILL ,MARGIN CONTRAST ,PAIRCON-
TRAST ).
•Flan-T5 with PAIRCONTRAST is the best sum-
marization model overall, and it achieves com-
parable or sometimes better factual consis-
tency, coherence and fluency than HUMAN -
REFaccording to SA,SGand U NIEVAL.
5.2 The Effect of Human-written References
Observing that the best-performing student model
demonstrates promising results, we further explore
the impact of human-written references and seek
to address the question: Is it possible to construct
dialogue summarization models without human-
written references?
Table 3 displays the performance of flan-t5-large
trained using PAIRCONTRAST with various num-
bers of randomly sampled dialogues from the SAM-
Sum training set. The quality scores on SAMSum
test set across all dimensions are similar, whether
original human-written reference summaries are
employed ( R=Y) or not ( R=N), for all dataset#Dialog R∗Const Coh Flu Rel
300 N 0.89 0.96 0.93 0.88
300 Y 0.88 0.94 0.91 0.83
1000 N 0.89 0.94 0.92 0.86
1000 Y 0.89 0.95 0.93 0.86
3000 N 0.90 0.96 0.94 0.89
3000 Y 0.90 0.95 0.93 0.88
9000 N 0.91 0.96 0.93 0.88
9000 Y 0.90 0.96 0.94 0.89
13000 N 0.91 0.96 0.94 0.89
13000 Y 0.91 0.96 0.94 0.89
Table 3: Comparing the performance of flan-t5-large
with P AIRCONTRAST on SAMSum, with ( R∗=Y) or
without ( R∗=N) human-written references. k= 3
for all settings. The four quality dimensions are factual
consistency (Const), coherence (Coh), fluency (Flu) and
relevance (Rel). Factual consistency is obtained from
AlignScore.
#Dialog kConsistency
1000 3 0.893
3000 1 0.898
3000 2 0.905
3000 3 0.902
9000 1 0.902
9000 2 0.904
9000 3 0.913
Table 4: Factual consistency (AlignScore) of flan-t5-
large trained with PAIRCONTRAST on varying numbers
of dialogues (#Dialog) and contrastive pairs per dia-
logue ( k).
sizes. These findings suggest the feasibility of de-
veloping robust summarization models using unla-
beled datasets.
5.3 The Effect of the Number of Contrastive
Pairs
Table 4 further shows the performance of flan-t5-
large trained on different numbers of dialogues
and contrastive pairs. We see that when the num-
ber of dialogues (i.e., #Dialog) is fixed, the model
in general generates slightly more consistent sum-
maries as kgrows. On the other hand, there is
no significant difference when we vary the num-
ber of contrastive pairs as long as the total number
of training instances (i.e., #Dialog ×k) is fixed.
For example, when the total number of training
instances is 9,000, (#Dialog=3000, k=3) yields the

--- PAGE 8 ---
SWING (AlignScore=0.888)Hannah is looking for Betty's number. She doesn't know himwell, but Amanda thinks she should ask Larry, who called Betty last time they were at the park together.Hannah: Hey, do you have Betty's number? Amanda: Lemme check Hannah: <file_gif> Amanda: Sorry, can't find it. Amanda: Ask Larry Amanda: He called her last time we were at the park together Hannah: I don't know him well Hannah: <file_gif> Amanda: Don't be shy, he's very nice Hannah: If you say so.. Hannah: I'd rather you texted him Amanda: Just text him 
🙂Hannah: Urgh.. Alright Hannah: Bye Amanda: Bye byeFactPegasus(AlignScore=0.623)Amanda doesn't have Betty's numberbut Hannah does. Larry called Betty last time they were at the park together.DialogueMLE (AlignScore=0.766)Hannah and Amanda are looking for Betty's number. Larry called Betty last time they were at the park. Amanda will text him.SeqDistill(AlignScore=0.902)Hannah asks for Betty's number, butcan't find it. She suggests asking Larry, who called her last time they went to the park together. However, shedoesn't know Larry well and suggests that she should text him instead. They say goodbye.Human-written Reference (AlignScore=0.907)Hannah needs Betty's numberbut Amanda doesn't have it. Sheneeds to contact Larry. PairContrast(AlignScore=0.963)Hannah asks Amanda for Betty's number, but Amanda can't find it and suggests asking Larry, who called Betty last time they were at the park together. Hannah is hesitant but Amanda encourages her not to be shy and to text Larry instead. Hannah agrees and says goodbye. MarginContrast(AlignScore=0.980)Hannah asks Amanda for Betty's number, but Amanda can't find it. Amanda suggests asking Larry, who called Betty last time they were at the park. Hannah is hesitant because she doesn't know Larry wellbut Amanda encourages her to do so. They end the conversation by saying goodbye.Figure 3: An example dialogue from SAMSum (Gliwa et al., 2019a) with summaries generated by BART (Lewis
et al., 2020) trained with different strategies ( MLE ,SEQDISTILL ,MARGIN CONTRAST ,PAIRCONTRAST ). Baseline
models (FactPEGASUS, SWING) and human-written reference are included for comparison. Contents that are
inconsistent with the input dialogue are shown in red. Ambiguous contents are shown in blue.
same result as (#Dialog=9000, k=1) does.
5.4 Case Study
Figure 3 presents an example dialogue along with
summaries generated by different models, sorted
by AlignScore (Zha et al., 2023b) in ascending or-
der. The summaries from FACTPEGASUS ,MLE ,
andSWING include factual errors unsupported by
the dialogue. Specifically, FACTPEGASUS incor-
rectly asserts “but Hannah does” when in fact, Han-
nah does not have Betty’s number. MLE inaccu-
rately claims that “Hannah and Amanda are look-
ing for Betty’s number”, though only Hannah is
searching. In SWING ’s summary, “him” appears
before the referent “Larry”. For SEQDISTILL and
Human-written reference, the pronouns “she” are
ambiguous as there are multiple possible refer-
ent in previous context. Unlike these, summaries
from PAIRCONTRAST andMARGIN CONTRAST
do not contain ambiguous references. Notably,
our methods ( SEQDISTILL ,PAIRCONTRAST and
MARGIN CONTRAST ) tend to produce longer sum-
maries compared to the much more succinct human-
written references, hence we see a substantially
lower ROUGE scores for them (Table 2).6 Conclusion
We investigated distilling LLM’s symbolic knowl-
edge (in the form of generated summaries) to en-
hance the factual consistency of smaller models
for dialogue summarization. Our experiments with
BART, PEGASUS, and Flan-T5 on the SAMSum
and DialogSum datasets reveal that: (1) symbolic
knowledge distillation enables the creation of more
compact summarization models that surpass strong
baselines which use complex data augmentation
strategies; and (2) our best-performing student
model, Flan-T5 with PAIRCONTRAST , produces
summaries that are potentially better — in terms of
factual consistency, coherence and fluency — than
human-written references.
7 Limitations
The experiments in this paper are conducted on
short daily dialogues. The findings may not gener-
alize to other dialogue scenarios such as academic
meetings and television interviews.
We use automatic evaluation metrics to assess
the quality of model-generated summaries, which
may not fully reflect human preferences.

--- PAGE 9 ---
8 Ethics Statement
This study is conducted under the guidance of the
ACL code of Ethics.
Acknowledgements
This research was supported by The University of
Melbourne’s Research Computing Services and the
Petascale Campus Initiative.
References
Griffin Adams, Han-Chin Shing, Qing Sun, Christo-
pher Winestock, Kathleen Mckeown, and Noémie
Elhadad. 2022. Learning to revise references for
faithful summarization. In Findings of the Associa-
tion for Computational Linguistics: EMNLP 2022 ,
pages 4009–4027.
Gustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao,
Xing Fan, and Chenlei Guo. 2020. Knowledge distil-
lation from internal representations. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
volume 34, pages 7350–7357.
Vidhisha Balachandran, Hannaneh Hajishirzi, William
Cohen, and Yulia Tsvetkov. 2022. Correcting diverse
factual errors in abstractive summarization via post-
editing and language model infilling. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 9818–9830.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Shuyang Cao and Lu Wang. 2021. Cliff: Contrastive
learning for improving faithfulness and factuality in
abstractive summarization. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 6633–6649.
Sihao Chen, Fan Zhang, Kazoo Sone, and Dan Roth.
2021a. Improving faithfulness in abstractive sum-
marization with contrast candidate generation and
selection. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 5935–5941.
Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang.
2021b. Dialogsum: A real-life scenario dialogue
summarization dataset. In Findings of the Associ-
ation for Computational Linguistics: ACL-IJCNLP
2021 , pages 5062–5074.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2024. Scaling instruction-finetuned language models.
Journal of Machine Learning Research , 25(70):1–53.Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing,
and Zhiting Hu. 2021. Compression, transduction,
and creation: A unified framework for evaluating
natural language generation. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7580–7605.
Esin Durmus, He He, and Mona Diab. 2020. FEQA: A
question answering evaluation framework for faith-
fulness assessment in abstractive summarization. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5055–
5070, Online. Association for Computational Lin-
guistics.
Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and
Caiming Xiong. 2022. QAFactEval: Improved QA-
based factual consistency evaluation for summariza-
tion. In Proceedings of the 2022 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 2587–2601, Seattle, United States. Asso-
ciation for Computational Linguistics.
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei
Liu. 2023. Gptscore: Evaluate as you desire. arXiv
preprint arXiv:2302.04166 .
Saadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi,
and Jianfeng Gao. 2021. Go figure: A meta evalua-
tion of factuality in summarization. In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021 , pages 478–487.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-
sander Wawer. 2019a. Samsum corpus: A human-
annotated dialogue dataset for abstractive summariza-
tion. EMNLP-IJCNLP 2019 , page 70.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-
sander Wawer. 2019b. SAMSum corpus: A human-
annotated dialogue dataset for abstractive summa-
rization. In Proceedings of the 2nd Workshop on
New Frontiers in Summarization , pages 70–79, Hong
Kong, China. Association for Computational Linguis-
tics.
Tanya Goyal and Greg Durrett. 2020. Evaluating factu-
ality in generation with dependency-level entailment.
InFindings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 3592–3603, Online.
Association for Computational Linguistics.
Tanya Goyal and Greg Durrett. 2021. Annotating and
modeling fine-grained factuality in summarization.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 1449–1462.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Kung-Hsiang Huang, Siffi Singh, Xiaofei Ma, Wei Xiao,
Feng Nan, Nicholas Dingwall, William Yang Wang,

--- PAGE 10 ---
and Kathleen Mckeown. 2023. Swing: Balancing
coverage and faithfulness for dialogue summariza-
tion. In Findings of the Association for Computa-
tional Linguistics: EACL 2023 , pages 512–525.
Luyang Huang, Lingfei Wu, and Lu Wang. 2020.
Knowledge graph-augmented abstractive summariza-
tion with semantic-driven cloze reward. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics , pages 5094–5107.
Yichong Huang, Xiachong Feng, Xiaocheng Feng, and
Bing Qin. 2021. The factual inconsistency problem
in abstractive text summarization: A survey. arXiv
preprint arXiv:2104.14839 .
Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder
Bhatia, Jimeng Sun, and Jiawei Han. 2024. Trisum:
Learning summarization ability from large language
models with structured rationale. arXiv preprint
arXiv:2403.10351 .
Yoon Kim and Alexander M Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natural
Language Processing , pages 1317–1327.
Wojciech Kryscinski, Bryan McCann, Caiming Xiong,
and Richard Socher. 2020. Evaluating the factual
consistency of abstractive text summarization. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9332–9346, Online. Association for Computa-
tional Linguistics.
Philippe Laban, Tobias Schnabel, Paul N Bennett, and
Marti A Hearst. 2022. Summac: Re-visiting nli-
based models for inconsistency detection in summa-
rization. Transactions of the Association for Compu-
tational Linguistics , 10:163–177.
Hwanhee Lee, Cheoneum Park, Seunghyun Yoon,
Trung Bui, Franck Dernoncourt, Juae Kim, and Ky-
omin Jung. 2022. Factual error correction for ab-
stractive summaries using entity retrieval. In Pro-
ceedings of the 2nd Workshop on Natural Language
Generation, Evaluation, and Metrics (GEM) , pages
439–444.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:
Denoising sequence-to-sequence pre-training for nat-
ural language generation, translation, and comprehen-
sion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
7871–7880.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out , pages 74–81.
Wei Liu, Huanqin Wu, Wenjing Mu, Zhen Li, Tao Chen,
and Dan Nie. 2021. Co2sum: contrastive learning for
factual-consistent abstractive summarization. arXiv
preprint arXiv:2112.01147 .Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023a. G-eval:
NLG evaluation using gpt-4 with better human align-
ment. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing ,
pages 2511–2522, Singapore. Association for Com-
putational Linguistics.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Ruochen Xu, and Chenguang Zhu. 2023b. Gpte-
val: Nlg evaluation using gpt-4 with better human
alignment. arXiv preprint arXiv:2303.16634 .
Yixin Liu, Alexander R Fabbri, Pengfei Liu, Dragomir
Radev, and Arman Cohan. 2023c. On learning to
summarize with large language models as references.
arXiv preprint arXiv:2305.14239 .
Yixin Liu and Pengfei Liu. 2021. Simcls: A simple
framework for contrastive learning of abstractive
summarization. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers) , pages 1065–1072.
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023.
Mqag: Multiple-choice question answering and gen-
eration for assessing information consistency in sum-
marization. In Proceedings of the 13th International
Joint Conference on Natural Language Processing
and the 3rd Conference of the Asia-Pacific Chap-
ter of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 39–53.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
Factscore: Fine-grained atomic evaluation of factual
precision in long form text generation. arXiv preprint
arXiv:2305.14251 .
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstrations:
What makes in-context learning work? In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 11048–11064.
Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero
dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen
Mckeown, and Bing Xiang. 2021. Entity-level fac-
tual consistency of abstractive text summarization.
InProceedings of the 16th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Main Volume , pages 2727–2733.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

--- PAGE 11 ---
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Maarten Sap, Ronan Le Bras, Emily Allaway, Chan-
dra Bhagavatula, Nicholas Lourie, Hannah Rashkin,
Brendan Roof, Noah A Smith, and Yejin Choi. 2019.
Atomic: An atlas of machine commonsense for if-
then reasoning. In Proceedings of the AAAI con-
ference on artificial intelligence , volume 33, pages
3027–3035.
Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,
Benjamin Piwowarski, Jacopo Staiano, Alex Wang,
and Patrick Gallinari. 2021. QuestEval: Summariza-
tion asks for fact-based evaluation. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing , pages 6594–6604, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Melanie Sclar, Peter West, Sachin Kumar, Yulia
Tsvetkov, and Yejin Choi. 2022. Referee: Reference-
free sentence summarization with sharper controlla-
bility through symbolic knowledge distillation. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
9649–9668.
Hwanjun Song, Igor Shalyminov, Hang Su, Siffi Singh,
Kaisheng Yao, and Saab Mansour. 2023. Enhancing
abstractiveness of summarization models through cal-
ibrated distillation. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
7026–7036.
Emma Strubell, Ananya Ganesh, and Andrew McCal-
lum. 2019. Energy and policy considerations for
deep learning in nlp. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics . Association for Computational Linguis-
tics.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020. Mobilebert: a
compact task-agnostic bert for resource-limited de-
vices. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
2158–2170.
David Wan and Mohit Bansal. 2022a. Factpegasus:
Factuality-aware pre-training and fine-tuning for ab-
stractive summarization. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1010–1028.
David Wan and Mohit Bansal. 2022b. FactPEGASUS:
Factuality-aware pre-training and fine-tuning for ab-
stractive summarization. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 1010–1028, Seattle,
United States. Association for Computational Lin-
guistics.Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen, and
Haizhou Li. 2022a. Analyzing and evaluating faith-
fulness in dialogue summarization. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing , pages 4897–4908.
Tianshu Wang, Faisal Ladhak, Esin Durmus, and He He.
2022b. Improving faithfulness by augmenting nega-
tive summaries from fake documents. In Proceedings
of the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 11913–11921.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M
Dai, and Quoc V Le. 2021. Finetuned language mod-
els are zero-shot learners. In International Confer-
ence on Learning Representations .
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al.
2022. Emergent abilities of large language models.
Transactions on Machine Learning Research .
Peter West, Chandra Bhagavatula, Jack Hessel, Jena
Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,
Sean Welleck, and Yejin Choi. 2022. Symbolic
knowledge distillation: from general language mod-
els to commonsense models. In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 4602–4625.
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.
Crossfit: A few-shot learning challenge for cross-
task generalization in nlp. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7163–7189.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text gener-
ation. Advances in Neural Information Processing
Systems , 34:27263–27277.
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.
2023a. AlignScore: Evaluating factual consistency
with a unified alignment function. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 11328–11348, Toronto, Canada. Association
for Computational Linguistics.
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.
2023b. Alignscore: Evaluating factual consistency
with a unified alignment function. In Annual Meeting
of the Association for Computational Linguistics .
Huajian Zhang, Yumo Xu, and Laura Perez-Beltrachini.
2024. Fine-grained natural language inference based
faithfulness evaluation for diverse summarisation
tasks. In Conference of the European Chapter of
the Association for Computational Linguistics .
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-
ter Liu. 2020. Pegasus: Pre-training with extracted

--- PAGE 12 ---
gap-sentences for abstractive summarization. In In-
ternational Conference on Machine Learning , pages
11328–11339. PMLR.
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,
Kathleen McKeown, and Tatsunori B Hashimoto.
2023. Benchmarking large language models for news
summarization. arXiv preprint arXiv:2301.13848 .
Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi
Narayan, Mohammad Saleh, and Peter J Liu. 2022.
Calibrating sequence likelihood improves conditional
language generation. In The Eleventh International
Conference on Learning Representations .
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu
Jiao, Peng Liu, Chenguang Zhu, Heng Ji, and Jiawei
Han. 2022a. Towards a unified multi-dimensional
evaluator for text generation. In Conference on Em-
pirical Methods in Natural Language Processing .
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu
Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and
Jiawei Han. 2022b. Towards a unified multi-
dimensional evaluator for text generation. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing , pages 2023–
2038.
Chenguang Zhu, William Hinthorn, Ruochen Xu,
Qingkai Zeng, Michael Zeng, Xuedong Huang, and
Meng Jiang. 2021. Enhancing factual consistency
of abstractive summarization. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 718–733.
Rongxin Zhu, Jianzhong Qi, and Jey Han Lau. 2023.
Annotating and detecting fine-grained factual errors
for dialogue summarization. In Proceedings of the
61st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers) , pages
6825–6845.
A Appendix
A.1 Potential Risks
The summaries generated by ChatGPT may contain
social biases, which require further investigation in
real applications.
A.2 The Statistics and Quality of ChatGPT
Summaries
We generated 3 positive and 3 negative summaries
for 13,000 dialogues from the training split of
SAMSum and 11,000 dialogues from the training
split of DialogSum. For each dialogue, we made 6
API calls (3 for positive and 3 for negative) sepa-
rately.
Table 5 shows the quality of 200 randomly sam-
pled positive summaries generated by the teachermodel gpt-3.5-turbo , validating that these sum-
maries are mostly factually consistent, with high
coherence, fluency and relevance as well.
Dataset Const Coh Flu Rel
SAMSum 0.90 0.97 0.94 0.91
DialogSum 0.92 0.97 0.94 0.94
Table 5: The factual consistency (Const), coherence
(Coh), fluency (Flu) and relevance (Rel) for 200 ran-
domly sampled positive summaries, generated by gpt-
3.5-turbo , in the training set of SAMSum and Dialog-
Sum. Factual consistency is obtained from Align-
Score (Zha et al., 2023b). Coherence, fluency, and rele-
vance are obtained from UniEval (Zhong et al., 2022b).
A.3 Meta-evaluation of Factual Consistency
Evaluation Metrics
We conducted a meta-evaluation of various au-
tomatic factual consistency metrics across three
datasets: DiaSummFact (Zhu et al., 2023), FacE-
val (Wang et al., 2022a), and GO FIGURE (Gabriel
et al., 2021). For the GO FIGURE dataset, we
specifically utilized the subset derived from SAM-
Sum (Gliwa et al., 2019a). In the case of Dia-
SummFact, we conducted evaluations at both the
sentence level (DiaSummFact∗) and summary level
(DiaSummFact’). For the sentence-level evaluation,
we excluded sentences whose labels include “Link
Error” or “Coreference Error”. All labels across
the datasets were converted into a binary format: if
any category of factual error is present, the label is
marked as “factually inconsistent”; otherwise, it is
marked as “factually consistent”. The number of
(dialogue, output) pairs in each dataset, where the
output is either a sentence for sentence-level evalu-
ation or a summary for summary-level evaluation,
is presented in Table 6. Spearman and Pearson
correlations are shown in Table 7 and Table 8.
Results show that both AlignScore and G-Eval
exhibit high correlation with human annotations
in most cases, except AlignScore on FacEval,
which requires further investigation in future works.
UniEval shows unsatisfactory correlation with hu-
man annotations on factual consistency, thus we
only use AlignScore and G-Eval ( gpt-4 ) for factual
consistency evaluation.
A.4 Implementation Details
All models were fine-tuned for 15,000 steps with
a batch size of 32 (per-device batch size 2/1, with

--- PAGE 13 ---
N
DiaSummFact∗475
DiaSummFact’ 1240
FacEval 750
GO FIGURE 250
Table 6: The number of (dialogue, output) pairs ( N) in
the datasets for our meta-evaluation.
Metric AlignScore G-Eval UniEval
DiaSummFact∗0.52 0.53 0.22
DiaSummFact’ 0.48 0.60 0.15
FacEval 0.11 0.54 0.01
GoFigure 0.43 0.60 0.23
Table 7: Spearman correlation between automatic fac-
tual consistency evaluation metrics and human evalua-
tion (binary).
gradient accumulation 16/32), evaluated every 500
steps (with model generations on development set)
on an NVIDIA A100 GPU with 40G/80G mem-
ory. Each training task took between 4 to 72 hours,
depending on the size of the model.
We searched for the best hyper-parameters of
α∈ {0.5,1,2}forPAIRCONTRAST , and α∈
{0.5,1,2}andθ∈ {15,30}forMARGIN CON-
TRAST , according to AlignScore (Zha et al., 2023b)
on development set.
The code for PAIRCONTRAST was developed
based on CLIFF6. ROUGE scores are computed
using Python package evaluate 0.4.0 with default
parameters7.
A.5 License or Terms
Our code and data will be released under MIT li-
cense.
A.6 Intended Use of Existing Artifacts
The SAMSum dataset, as presented in Gliwa
et al. (2019b), is distributed under the Attribution-
NonCommercial-NoDerivatives 4.0 International
(CC BY-NC-ND 4.0) license. We offer supple-
mentary details (e.g., model-generated summaries),
while preserving the integrity of the original data,
comprising dialogues and reference summaries.
6https://github.com/ShuyangCao/cliff_summ/
tree/main/models
7https://pypi.org/project/evaluate/Metric AlignScore G-Eval UniEval
DiaSummFact∗0.49 0.54 0.17
DiaSummFact’ 0.39 0.49 0.13
FacEval 0.09 0.49 -0.01
GoFigure 0.44 0.71 0.23
Table 8: Pearson correlation between automatic factual
consistency evaluation metrics and human evaluation
(binary).
A.7 Artifacts
The artifacts we release (code, data) are all in En-
glish only.
