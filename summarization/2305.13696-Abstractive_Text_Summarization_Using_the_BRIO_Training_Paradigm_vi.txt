Tóm tắt văn bản trừu tượng sử dụng mô hình huấn luyện BRIO

Khang Nhut Lam
Đại học Cần Thơ, Việt Nam
lnkhang@ctu.edu.vnThieu Gia Doan
Đại học Cần Thơ, Việt Nam
dgthieu@cusc.ctu.edu.vn
Khang Thua Pham
Đại học Duy Tân, Việt Nam
phamthuakhang@dtu.edu.vnJugal Kalita
Đại học Colorado, Hoa Kỳ
jkalita@uccs.edu

Tóm tắt
Các câu tóm tắt được tạo ra bởi các mô hình tóm tắt trừu tượng có thể nhất quán và toàn diện, nhưng chúng thiếu khả năng kiểm soát và phụ thuộc nhiều vào các bản tóm tắt tham chiếu. Mô hình huấn luyện BRIO giả định một phân phối không xác định để giảm sự phụ thuộc của mô hình vào các bản tóm tắt tham chiếu, và cải thiện hiệu suất mô hình trong quá trình suy luận. Bài báo này trình bày một kỹ thuật đơn giản nhưng hiệu quả để cải thiện các bản tóm tắt trừu tượng bằng cách tinh chỉnh các mô hình ngôn ngữ được huấn luyện trước, và huấn luyện chúng với mô hình BRIO. Chúng tôi xây dựng một bộ dữ liệu tóm tắt văn bản cho tiếng Việt, được gọi là VieSum. Chúng tôi thực hiện các thí nghiệm với các mô hình tóm tắt trừu tượng được huấn luyện với mô hình BRIO trên các bộ dữ liệu CNNDM và VieSum. Kết quả cho thấy các mô hình, được huấn luyện trên phần cứng cơ bản, vượt trội so với tất cả các mô hình tóm tắt trừu tượng hiện có, đặc biệt là cho tiếng Việt.

1 Giới thiệu
Tóm tắt văn bản giảm kích thước của văn bản gốc trong khi vẫn bảo toàn nội dung chính của nó. Hai phương pháp chính để xây dựng bản tóm tắt là trích xuất và trừu tượng. Tóm tắt trích xuất trực tiếp lấy các câu hoặc từ ngữ truyền đạt các chủ đề chính của tài liệu gốc, và nối chúng lại với nhau. Tóm tắt trừu tượng khám phá nội dung chính của tài liệu và tạo ra các bản tóm tắt. Các bản tóm tắt trừu tượng thường tự nhiên và nhất quán hơn các bản tóm tắt trích xuất.

Hầu hết các mô hình tóm tắt trừu tượng tuân theo khung encoder-decoder. Các mô hình tóm tắt trừu tượng hiện có được huấn luyện bằng ước lượng khả năng cực đại và dựa vào các bản tóm tắt tham chiếu. Liu et al. (2022a) đề xuất mô hình huấn luyện BRIO để giải quyết sự phụ thuộc vào các bản tóm tắt tham chiếu bằng cách giả định phân phối không xác định của các bản tóm tắt ứng viên được tạo ra bởi hệ thống. Trong bài báo này, chúng tôi sử dụng mô hình huấn luyện BRIO cho các mô hình tóm tắt trừu tượng để xây dựng bản tóm tắt cho các tài liệu bằng tiếng Anh và tiếng Việt.

Chúng tôi đóng góp như sau:
• Chúng tôi thích ứng mô hình huấn luyện BRIO cho tóm tắt trừu tượng sử dụng các mô hình dựa trên BART và dựa trên T5 làm xương sống.
• Chúng tôi trình bày các vấn đề với mô hình BRIO.
• Chúng tôi nghiên cứu các mô hình tóm tắt trừu tượng sử dụng BARTpho-BRIO và ViT5-BRIO để đạt được kết quả cải thiện.
• Chúng tôi công khai phát hành bộ dữ liệu tóm tắt VieSum cho mục đích nghiên cứu.

Phần còn lại của bài báo được tổ chức như sau. Các công trình liên quan được trình bày trong Mục 2. Mục 3 giới thiệu một bộ dữ liệu lớn cho tóm tắt bằng tiếng Việt, có tên VieSum. Các thí nghiệm và thảo luận được trình bày trong Mục 4. Mục 5 kết luận bài báo.

2 Công trình liên quan
Mạng đối kháng sinh tương tự bảo toàn ngữ nghĩa (SSPGAN) của Sheng et al. (2022) sử dụng một bộ sinh dựa trên Transformer để tạo ra các bản tóm tắt. Một bộ phân biệt tương tự dựa trên Transformer nắm bắt tính nhất quán ngữ nghĩa giữa tài liệu nguồn và bản tóm tắt tương ứng. Trong quá trình huấn luyện đối kháng, bộ phân biệt tính toán phần thưởng cho mỗi từ được tạo ra. Trên bộ dữ liệu Gigaword, mô hình SSPGAN đạt được kết quả tốt hơn nhiều mô hình tóm tắt văn bản trừu tượng hiện có khác như bộ giải mã sinh hồi quy sâu (Li et al., 2017), các phương pháp actor-critic từ học tăng cường (Li et al., 2018), và Transformer (Vaswani et al., 2017).

Liu et al. (2022b) phát triển mô hình PageSum cho tóm tắt trừu tượng bằng cách kết hợp thiên vị địa phương trong cả encoder và decoder. Mỗi tài liệu được phân chia thành các trang không chồng lấp. Encoder, là một bộ tóm tắt trừu tượng, mã hóa từng trang và đưa ra dự đoán cục bộ. Decoder dự đoán đầu ra dựa trên sự kết hợp có trọng số của các dự đoán cục bộ. Các tác giả tinh chỉnh mô hình BART (Lewis et al., 2020) cho tóm tắt trừu tượng và nghiên cứu nhiều phương pháp tiếp cận địa phương, chẳng hạn như địa phương không gian, địa phương diễn ngôn, và địa phương tài liệu. PageSum vượt trội so với các mô hình tóm tắt trừu tượng như longformer encoder-decoder (Beltagy et al., 2020), encoder-decoder attention với head-wise positional strides (Huang et al., 2021), và BART với Hierarchical Attention Transformer (Rohde et al., 2021). Tuy nhiên, PageSum mất nhiều thời gian để huấn luyện, yêu cầu kích thước bộ nhớ lớn, và không thể nắm bắt các phụ thuộc khoảng cách xa.

Một số nghiên cứu sử dụng các mô hình được huấn luyện trước cho tóm tắt văn bản trừu tượng. Farahani et al. (2021) sử dụng mT5 (Xue et al., 2021) và sequence to sequence ParsBERT (Rothe et al., 2020) để xây dựng các bản tóm tắt trừu tượng cho văn bản tiếng Ba Tư. T5 (Raffel et al., 2020) và BERT (Devlin et al., 2018) cũng đã được sử dụng để xây dựng các bản tóm tắt trừu tượng (Garg et al., 2021). Kieuvongngam et al. (2020) tóm tắt các bài báo nghiên cứu y sinh COVID-19 sử dụng BERT và GPT-2 (Radford et al., 2019). Các đặc trưng của tài liệu được trích xuất và tích hợp vào một mô hình trừu tượng để cải thiện việc tạo bản tóm tắt. Nambiar et al. (2022) phát triển một mô hình encoder-decoder sử dụng attention, trong đó các đặc trưng POS được kết hợp vào các lớp word embedding để tăng cường các vector từ. Các thí nghiệm trên một bộ dữ liệu bằng tiếng Malayalam cho thấy rằng việc tích hợp mô hình attention và các đặc trưng POS tốt hơn các mô hình seq2seq và attention. Barna và Heickal (2021) thích ứng mạng pointer generator cho tóm tắt trừu tượng bằng cách kết hợp một lớp word embedding được huấn luyện trước để chuyển giao tính tương tự ngữ nghĩa và các đặc trưng chủ đề để có phạm vi chủ đề tốt hơn. Một nhược điểm của tóm tắt trừu tượng thông thường là việc bỏ sót các thực thể có tên. Để khắc phục, Berezin và Batura (2022) huấn luyện một mô hình nhận dạng thực thể có tên dựa trên ROBERTa để khám phá các thực thể có tên. Sau đó, mô hình ngôn ngữ BART masked named entity được huấn luyện để chú ý đến các thực thể có tên. Cuối cùng, BART được tinh chỉnh cho tóm tắt văn bản.

Hầu hết các nghiên cứu để xây dựng các bản tóm tắt trừu tượng bằng tiếng Việt sử dụng một khung encoder-decoder hoặc một mô hình được huấn luyện trước. Quoc et al. (2019) tích hợp vị trí câu và tần số thuật ngữ vào một mạng pointer generator với cơ chế coverage để thực hiện tóm tắt trừu tượng cho các tài liệu tiếng Việt. Lam et al. (2022) xây dựng các bản tóm tắt trừu tượng cho báo trực tuyến sử dụng RNN với attention, BiLSTM với copy generator, Transformer tiêu chuẩn, BERT, và các mô hình trừu tượng sequence-to-sequence sử dụng phương pháp bottom-up. Phan et al. (2022) thực hiện các thí nghiệm để tóm tắt các tài liệu tiếng Việt sử dụng các kiến trúc encoder-decoder dựa trên Transformer như Transformer, PhoBERT (Tran et al., 2022), và ViT5 (Phan et al., 2022).

3 Bộ dữ liệu VieSum
Chúng tôi xây dựng một bộ dữ liệu VieSum cho tiếng Việt bao gồm 1.627.415 tài liệu và các bản tóm tắt tương ứng của chúng, được nhóm thành 23 danh mục. Cụ thể, BeautifulSoup và Newspaper3k được sử dụng để thu thập và trích xuất các bài báo từ các tờ báo trực tuyến phổ biến bằng tiếng Việt như vnexpress.net, dantri.com.vn, danviet.vn, vietnamnet.vn, laodong.vn, và vov.vn. Các bản tóm tắt và tài liệu nội dung được coi là các bản tóm tắt tham chiếu và tài liệu, tương ứng.

4 Kết quả thí nghiệm
Chúng tôi thực hiện các thí nghiệm trong môi trường Google Colaboratory, NVIDIA Tesla T4 16GB. Chúng tôi sử dụng bộ dữ liệu CNNDM bằng tiếng Anh, và bộ dữ liệu VieSum của chúng tôi bằng tiếng Việt. Do hạn chế của phần cứng, chúng tôi thực hiện các thí nghiệm với 70.000 tài liệu được chọn ngẫu nhiên và các bản tóm tắt tham chiếu tương ứng từ VieSum. Mỗi bộ dữ liệu được chia thành 3 phần bao gồm 75% để huấn luyện, 8% để xác thực, và 17% để kiểm tra.

Trong bài báo này, các mô hình dựa trên BART 512-length và dựa trên T5 512-length được huấn luyện trước được sử dụng làm xương sống để tạo ra các bản tóm tắt trừu tượng. Các mô hình BART (Lewis et al., 2020) và T5 (Raffel et al., 2020) được huấn luyện trên bộ dữ liệu CNNDM, trong khi BARTpho (Tran et al., 2022) và ViT5 (Phan et al., 2022) được huấn luyện trên bộ dữ liệu VieSum. Tất cả các mô hình đều là các mô hình base. Để dễ so sánh, chúng tôi sử dụng các tham số giống như được đề xuất bởi các tác giả gốc.

4.1 Các mô hình trừu tượng tiêu chuẩn
Đầu tiên, chúng tôi thí nghiệm và đánh giá các phương pháp tóm tắt trừu tượng sử dụng các mô hình BART-base và T5-base tiêu chuẩn. Chúng tôi huấn luyện các mô hình sử dụng kích thước batch là 4, số epoch là 5, tốc độ học là 10^-5, bước khởi động là 20.000, và bộ tối ưu hóa Adam. Kết quả của các hệ thống tóm tắt trừu tượng sử dụng các mô hình xương sống tiêu chuẩn được trình bày trong Bảng 1.

4.2 Tinh chỉnh các mô hình trừu tượng
Để cải thiện chất lượng của các bản tóm tắt được tạo ra, chúng tôi tinh chỉnh các mô hình xương sống sử dụng Trainer được cung cấp bởi Hugging Face. Chúng tôi không tinh chỉnh mô hình BART vì nó đã được tinh chỉnh trên bộ dữ liệu CNN. Bảng 2 cho thấy điểm số ROUGE của các mô hình trừu tượng được tinh chỉnh.

4.3 Tinh chỉnh các mô hình trừu tượng và BRIO
Mô hình huấn luyện BRIO (Liu et al., 2022a) giúp các mô hình tóm tắt trừu tượng dự đoán các token chính xác hơn. Liu et al. (2022a) sử dụng BART làm mô hình xương sống. BRIO gán khối lượng xác suất cho các ứng viên tóm tắt đầu ra dựa trên chất lượng của chúng bằng cách sử dụng học tương phản. Mô hình trừu tượng hoạt động như một mô hình tạo sinh để tạo ra các ứng viên trừu tượng theo cách tự hồi quy, và một mô hình đánh giá để đánh giá các ứng viên bằng cách tính toán phân phối xác suất của chúng. Bộ sinh được huấn luyện sử dụng tổn thất MLE tiêu chuẩn, trong khi bộ đánh giá được huấn luyện sử dụng tổn thất tương phản (Hadsell et al., 2006).

Trong BRIO, một mô hình xương sống được sử dụng để tạo ra N bản tóm tắt trừu tượng, được gọi là candsum, cho mỗi tài liệu. Mỗi candsum được gán một điểm chất lượng bằng cách lấy điểm trung bình của các giá trị ROUGE-1, ROUGE-2, và ROUGE-L của nó. Cụ thể, Liu et al. (2022a) sử dụng mô hình BART 1024-length để tạo ra 16 candsum cho mỗi tài liệu. Tiếp theo, các tài liệu, bản tóm tắt tham chiếu, và các candsum tương ứng được sắp xếp theo điểm chất lượng giảm dần được sử dụng để huấn luyện mô hình tóm tắt trừu tượng sử dụng mô hình BRIO. Chúng tôi lưu ý rằng Liu et al. (2022a) sử dụng các mô hình tiêu chuẩn làm xương sống và huấn luyện chúng với mô hình BRIO.

Trong công trình của chúng tôi, các mô hình tóm tắt trừu tượng xương sống được tinh chỉnh, được trình bày trong mục trước, được sử dụng để tạo ra N=6 candsum cho mỗi tài liệu sử dụng diverse beam search (Vijayakumar et al., 2018) với num beam groups=6, diversity penalty=1.0, và num beams=4. Các mô hình tóm tắt trừu tượng được huấn luyện sử dụng tốc độ học 10^-3, và bộ tối ưu hóa Adafactor. Liu et al. (2022a) khẳng định rằng huấn luyện BRIO giúp các mô hình đạt hiệu suất tốt nhất trong vòng một epoch trên bộ dữ liệu CNNDM. Do đó, chúng tôi sử dụng một epoch để huấn luyện các mô hình tóm tắt được tinh chỉnh với mô hình BRIO. Kết quả của các hệ thống tóm tắt trừu tượng được huấn luyện với BRIO được trình bày trong Bảng 3.

4.4 Tinh chỉnh các mô hình trừu tượng và BRIO-Loop
Như được đề xuất bởi Liu et al. (2022a), chúng tôi thực hiện xử lý vòng lặp, sử dụng các candsum được tạo ra bởi các mô hình tóm tắt trừu tượng được huấn luyện với BRIO để huấn luyện các mô hình. Tuy nhiên, sau một số lần lặp của vòng lặp, điểm số ROUGE dường như thay đổi rất ít. Đặc biệt, BARTpho và ViT5 gần như đạt điểm số ROUGE cao nhất với 2 lần lặp. Bảng 4 trình bày điểm số ROUGE thu được sau khi lặp hai lần.

Kết quả thí nghiệm cho thấy rằng mô hình huấn luyện BRIO giúp cải thiện đáng kể các bản tóm tắt trừu tượng bằng cách giảm sự phụ thuộc của hệ thống vào các bản tóm tắt tham chiếu. Tuy nhiên, việc gán trọng số cho cả candsum và bản tóm tắt tham chiếu là cần thiết để giảm sự phụ thuộc vào các bản tóm tắt tham chiếu. Diverse beam search giúp có được các candsum đa dạng, nhưng có thể gây nhiễu trong không gian beam search vì mô hình có thể không tuân theo các bản tóm tắt tham chiếu. Ngoài ra, việc sử dụng thước đo ROUGE để đánh giá các mô hình tóm tắt trừu tượng được huấn luyện với mô hình BRIO có vẻ không công bằng vì các mô hình này có thể tạo ra các bản tóm tắt độc lập với các bản tóm tắt tham chiếu.

4.5 Thảo luận
Không dễ dàng để so sánh giữa các mô hình được huấn luyện trên phần cứng khác nhau và trên các bộ dữ liệu khác nhau. Chúng tôi cố gắng so sánh công trình của mình với các bài báo đã xuất bản trên các bộ dữ liệu tương tự.

Hiện tại, BRIO sử dụng mô hình BART 1024-length tiêu chuẩn làm xương sống, tạo ra 16 candsum, đạt kết quả SOTA trên bộ dữ liệu CNNDM với ROUGE-1 là 47,78 và ROUGE-L là 32,58 (Liu et al., 2022a). Ngoài ra, BART 1024-length-BRIO với 2 lần lặp đạt ROUGE-1 và ROUGE-L lần lượt là 48,01 và 44,67; cả hai đều tốt hơn BART 512-length-BRIO của chúng tôi, tạo ra 6 candsum cho mỗi tài liệu, sau 2 lần lặp: 46,55 cho ROUGE-1 và 43,00 cho ROUGE-L.

Tawmo et al. (2022) tinh chỉnh mô hình tóm tắt trừu tượng T5 và đánh giá trên bộ dữ liệu CNNDM. Mô hình T5 của họ đạt điểm số ROUGE-1 và ROUGE-L lần lượt là 40,79 và 34,80, thấp hơn điểm số của mô hình T5 tinh chỉnh của chúng tôi, và thấp hơn đáng kể so với điểm số của mô hình tốt nhất của chúng tôi, mô hình T5-BRIO-Loop: 45,24 cho ROUGE-1 và 41,80 cho ROUGE-L.

Đối với tóm tắt trừu tượng tiếng Việt, Quoc et al. (2019) sử dụng LSTM với các đặc trưng của vị trí câu và tần số thuật ngữ (LSTM+SP+TF) trên một bộ dữ liệu tiếng Việt được thu thập từ Baomoi. Điểm số ROUGE-1 và ROUGE-L tốt nhất của mô hình họ lần lượt là 31,89 và 29,97, thấp hơn đáng kể so với điểm số của mô hình BRIO-BART của chúng tôi.

Cả mô hình BARTpho và ViT5 được huấn luyện với mô hình BRIO đều vượt trội so với tất cả các mô hình được đề xuất bởi Lam et al. (2022) trên bộ dữ liệu CTUNLPSum, rất giống với bộ dữ liệu VieSum, bao gồm các mô hình sequence-to-sequence, copy generator network, sequence-to-sequence với phương pháp rewriter, và phương pháp bottom-up.

Tran et al. (2022) áp dụng một số mô hình cho tóm tắt trừu tượng trên bộ dữ liệu VNDS (Nguyen et al., 2019). Họ thực hiện các thí nghiệm trên 8 GPU A100 với mỗi cái 40GB. Mô hình của họ được huấn luyện trong 15 epoch trong khoảng 6 ngày. Mô hình tốt nhất của họ, BARTpho, đạt ROUGE-1 là 61,14, cao hơn một chút so với BARTpho-BRIO-Loop, và ROUGE-L là 40,15, thấp hơn so với BARTpho-BRIO-Loop. Ngoài ra, BARTpho-BRIO-Loop được huấn luyện trong một epoch trong khoảng 32 giờ sử dụng phần cứng cơ bản.

Phan et al. (2022) giới thiệu một Transformer text-to-text được huấn luyện trước cho tóm tắt trừu tượng tiếng Việt, được gọi là ViT5. Các tác giả khẳng định mô hình ViT5 là SOTA cho tóm tắt trừu tượng tiếng Việt. Mô hình tóm tắt trừu tượng ViT5 của họ đạt ROUGE-1 và ROUGE-L lần lượt là 61,85 và 41,70 trên bộ dữ liệu VNDS (Nguyen et al., 2019). Chúng tôi đã tiến hành các thí nghiệm trên VNDS và tìm thấy kết quả thú vị liên quan đến mô hình ViT5. Điểm số ROUGE của mô hình ViT5 được huấn luyện sử dụng mô hình thông thường về cơ bản giống hệt với điểm số ROUGE được cung cấp bởi Phan et al. (2022). Tuy nhiên, điểm số của mô hình ViT5 được huấn luyện sử dụng mô hình BRIO giảm xuống 59,37 và 41,6, tương ứng. Trên bộ dữ liệu VieSum, ViT5-base tiêu chuẩn đạt ROUGE-1 là 53,39 và ROUGE-L là 35,88; trong khi ViT5-BRIO-Loop có điểm số tốt hơn: ROUGE-1 là 60,90 và ROUGE-L là 44,36. Chúng tôi để dành việc khám phá và đánh giá thêm các kết quả không ổn định này cho công trình tương lai.

5 Kết luận
Chúng tôi đã nghiên cứu các mô hình tóm tắt trừu tượng được huấn luyện với mô hình BRIO. Các thí nghiệm cho thấy rằng chúng tôi có thể cải thiện các mô hình tóm tắt trừu tượng bằng cách tinh chỉnh các xương sống trước khi huấn luyện chúng với BRIO. Cụ thể, các mô hình tóm tắt được huấn luyện với BRIO vượt trội so với các mô hình tóm tắt khác bằng tiếng Việt. Chúng tôi cũng thảo luận các vấn đề với mô hình BRIO để khám phá thêm. Ngoài ra, chúng tôi đã xây dựng bộ dữ liệu VieSum cho tóm tắt bằng tiếng Việt. Đối với công trình tương lai, chúng tôi sẽ yêu cầu các tình nguyện viên đánh giá và cung cấp phản hồi về một tập con nhỏ của bộ dữ liệu VieSum.

Hạn chế
Trong khi nhiều nghiên cứu cho thấy rằng kiến trúc của các mô hình học sâu ảnh hưởng đáng kể đến kết quả, chúng tôi thực hiện các thí nghiệm với một số kiến trúc cơ bản do hạn chế phần cứng. Hơn nữa, chưa có một bộ dữ liệu tóm tắt chuẩn tiếng Việt nào vừa có kích thước lớn vừa có chất lượng cao. Các bộ dữ liệu tóm tắt hiện có được lấy từ các tạp chí trực tuyến, thường chứa các từ viết sai chính tả và lỗi ngữ pháp. Ngoài ra, các bản tóm tắt tham chiếu có thể không truyền đạt nội dung chính của các bài viết tương ứng. Do đó, việc lựa chọn và phát triển các mô hình tóm tắt hiệu quả cho tiếng Việt vẫn còn nhiều thách thức.

Tuyên bố đạo đức
Chúng tôi sử dụng một số công cụ phần mềm khác nhau trong các thí nghiệm của mình. Những công cụ này cũng như bộ dữ liệu tiếng Anh đều có sẵn công khai và chúng tôi không thấy có vấn đề đạo đức nào trong việc sử dụng chúng. Ngoài ra, chúng tôi tham chiếu rõ ràng các bài báo và nguồn khác cho các công cụ được sử dụng. Chúng tôi tự tạo ra bộ dữ liệu VieSum.

Công trình của bài báo này phụ thuộc vào việc sử dụng các phương pháp tiếp cận tóm tắt trừu tượng đã được xuất bản trước đó. Chúng tôi ghi nhận rõ ràng công lao của các tác giả của những phương pháp tiếp cận này bằng cách trích dẫn các nguồn gốc.

Bài báo này tập trung vào tóm tắt trừu tượng của các tài liệu dài. Có khả năng các bộ tóm tắt trừu tượng chất lượng cao bị sử dụng sai mục đích. Ví dụ, sinh viên khi được giao nhiệm vụ tóm tắt/đánh giá các bài báo/bài viết có thể sử dụng các bộ tóm tắt như vậy để tự động viết đánh giá và tuyên bố chúng là của riêng mình. Tuy nhiên, chúng tôi tin rằng các bộ tóm tắt trừu tượng cho tài liệu dài chưa đạt được mức độ tinh vi này vào thời điểm hiện tại.

Tài liệu tham khảo
[Các tài liệu tham khảo được giữ nguyên như trong bản gốc]
