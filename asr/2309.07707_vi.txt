# 2309.07707.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/asr/2309.07707.pdf
# Kích thước tệp: 705952 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
COLLD: CHƯNG CẤT TƯƠNG PHẢN TỪNG LỚP
ĐỂ NÉN BỘ MÃ HÓA TIẾNG NÓI ĐA NGÔN NGỮ ĐÃ ĐƯỢC TIỀN HUẤN LUYỆN
Heng-Jui Chang1,2,∗, Ning Dong2, Ruslan Mavlyutov2, Sravya Popuri2, Yu-An Chung2
1MIT CSAIL2Meta AI
hengjui@mit.edu, andyyuan@meta.com
TÓM TẮT
Các bộ mã hóa tiếng nói được tiền huấn luyện tự giám sát quy mô lớn vượt trội hơn
các phương pháp thông thường trong các tác vụ nhận dạng tiếng nói và dịch thuật.
Do chi phí cao của việc phát triển các mô hình lớn này, việc xây dựng bộ mã hóa mới
cho các tác vụ mới và triển khai chúng cho các ứng dụng trên thiết bị là không khả thi.
Các nghiên cứu trước đây đề xuất các phương pháp nén mô hình để giải quyết vấn đề này,
nhưng những công trình đó tập trung vào các mô hình nhỏ hơn và các tác vụ ít thực tế hơn.
Do đó, chúng tôi đề xuất Chưng cất Tương phản Từng lớp (CoLLD), một phương pháp
chưng cất kiến thức mới để nén các bộ mã hóa tiếng nói được tiền huấn luyện bằng cách
tận dụng dự đoán có che và học tương phản để huấn luyện các mô hình học sinh sao chép
hành vi của mô hình giáo viên lớn. CoLLD vượt trội hơn các phương pháp trước đây
và thu hẹp khoảng cách giữa các mô hình nhỏ và lớn trên các benchmark dịch thuật và
nhận dạng tiếng nói sang văn bản đa ngôn ngữ.
Từ khóa chỉ mục —Học tự giám sát, chưng cất kiến thức,
nén mô hình, dịch thuật tiếng nói đa ngôn ngữ

1. GIỚI THIỆU
Học tự giám sát (SSL) cho việc tiền huấn luyện bộ mã hóa tiếng nói mang lại lợi ích
cho các tác vụ xử lý tiếng nói khác nhau và vượt trội hơn các phương pháp thông thường [1].
Các phương pháp SSL tận dụng kho ngữ liệu tiếng nói không nhãn lớn để huấn luyện
các mạng nơ-ron sâu mã hóa các biểu diễn hữu ích và thành công trong các ứng dụng
như dịch thuật tiếng nói [2] và nhận dạng tiếng nói tự động (ASR) [3]. Tuy nhiên,
các bộ mã hóa tiếng nói mạnh mẽ thường có nhiều tham số, khiến việc xử lý tiếng nói
thời gian thực hoặc trên thiết bị trở nên kém khả thi.

Các nhà nghiên cứu đề xuất các kỹ thuật nén mô hình để giải quyết vấn đề của các bộ mã
hóa tiếng nói lớn. Các bộ mã hóa được tiền huấn luyện SSL đã nén có thể được áp dụng
cho các tác vụ hạ nguồn khác nhau. Các phương pháp này có thể được phân loại thành
chưng cất kiến thức (KD) và cắt tỉa tham số. Trong KD, một mô hình học sinh nhẹ
học dự đoán các biểu diễn ẩn để bắt chước hành vi của mô hình giáo viên lớn [4–10].
DistilHuBERT [4] dự đoán nhiều lớp ẩn trong một giáo viên HuBERT [11] bằng cách
sử dụng đầu ra của học sinh với các đầu dự đoán riêng biệt. FitHuBERT [5] và Ashihara
et al. [6] đề xuất KD từng lớp (L2L) sử dụng học sinh hẹp và sâu để chưng cất từng lớp
các biểu diễn ẩn của giáo viên. Trong cắt tỉa không có cấu trúc, các tham số có giá trị
nhỏ được đặt thành zero [12], trong khi cắt tỉa có cấu trúc loại bỏ các mô-đun con khỏi
mô hình [13–15] để giảm các tham số nhưng yêu cầu triển khai phức tạp. Các nghiên cứu
khác kết hợp các phương pháp trên [16] hoặc các kỹ thuật như bỏ qua lớp [17] và
lượng tử hóa bit thấp [18].

Mặc dù các phương pháp hiện có thành công trong nhiều tác vụ, hầu hết các công trình
tập trung vào việc nén các mô hình SSL nhỏ và đánh giá với các thiết lập bài toán không
thực tế. Những công trình đó nén một mô hình HuBERT Base [11]
∗Công việc được thực hiện trong thời gian thực tập tại Meta AI.

[THIS IS FIGURE: A graph showing encoder sizes vs. X-Eng speech-to-text translation BLEU scores, with various model sizes plotted]

Fig. 1 . Kích thước bộ mã hóa so với điểm BLEU dịch thuật tiếng nói sang văn bản X-Eng.
Mô hình được đề xuất là một mô hình XX-Large đã nén.

(95M tham số) thành các mô hình khoảng 20M đến 30M tham số và đánh giá với
Speech processing Universal PERformance Benchmark (SUPERB) [19, 20]. Các mô hình
đã nén này không phù hợp cho các tác vụ phức tạp yêu cầu tinh chỉnh do khả năng
mô hình nhỏ, hạn chế các kịch bản ứng dụng. Trong thiết lập này, hiệu quả của
các phương pháp này cho các mô hình và bài toán quy mô lớn vẫn chưa được khám phá.

Để thu hẹp khoảng cách giữa nghiên cứu học thuật và các vấn đề thực tế, chúng tôi
mở rộng tác vụ nén bộ mã hóa tiếng nói đến một bộ mã hóa tiếng nói được tiền huấn
luyện quy mô lớn (w2v-BERT 2.0 [2]) và áp dụng mô hình đã nén cho dịch thuật tiếng
nói sang văn bản đa ngôn ngữ (S2T). Bài toán này thách thức vì mô hình gốc lớn hơn
đáng kể (1B tham số), và mô hình đã nén được tinh chỉnh với một tác vụ phức tạp hơn
nhưng thực tế hơn. Theo các nghiên cứu trước đây, chúng tôi sử dụng dữ liệu không nhãn
để nén một mô hình giáo viên được tiền huấn luyện SSL vì thiết lập này cho phép
sử dụng linh hoạt và tránh tinh chỉnh các bộ mã hóa khổng lồ. Hơn nữa, bộ mã hóa
đã nén có 300M tham số, hiện là kích thước bộ mã hóa lớn nhất được sử dụng rộng rãi
trong cả sản xuất và học thuật [19].

Trong thiết lập bài toán mới này, chúng tôi đề xuất Chưng cất Tương phản Từng lớp
(CoLLD) bằng cách kết hợp L2L KD [6] và mục tiêu học dự đoán có che tương phản [21].
Đầu tiên, một số khung đầu vào của mô hình học sinh được che trong khi giáo viên
vẫn không bị che. Sau đó, mỗi khung lớp ẩn đã che của học sinh phân loại khung
lớp ẩn tương ứng của giáo viên từ một tập hợp các yếu tố gây nhiễu, nơi các yếu tố
gây nhiễu được lấy mẫu ngẫu nhiên từ các khung khác của các biểu diễn của giáo viên.
Sau khi chưng cất, chúng tôi đánh giá mô hình học sinh với các benchmark nội bộ
và công khai, bao gồm S2T và ASR đa ngôn ngữ. Như được thể hiện trong Hình 1 và
Mục 3, CoLLD vượt trội hơn các phương pháp chưng cất trước đây, thu hẹp khoảng cách
hiệu suất giữa các mô hình lớn (0.6B và 1.0B tham số) và vượt trội hơn các baseline
mạnh như XLS-R [22] và MMS [23].

--- TRANG 2 ---
[THIS IS FIGURE: A diagram showing the CoLLD framework with Teacher and Student models, including layers and contrastive loss]

Fig. 2 . Minh họa khung Chưng cất Tương phản Từng lớp (CoLLD) được đề xuất. (I) CoLLD
đưa cùng một đầu vào cho giáo viên đã đóng băng và mô hình học sinh có thể học, nơi các
khung đầu vào của học sinh được che một phần. Đối với mỗi lớp l của học sinh, các biểu
diễn đã che học phân loại khung giáo viên tương ứng trong lớp ˆl từ K khung gây nhiễu.
(II) Sau khi chưng cất, trọng số mô hình học sinh khởi tạo các mô hình hạ nguồn và được
tinh chỉnh với dữ liệu có nhãn để thực hiện các tác vụ như dịch thuật tiếng nói đa ngôn ngữ.

2. PHƯƠNG PHÁP
2.1. Tổng quan
Chúng tôi đề xuất khung Chưng cất Tương phản Từng lớp (CoLLD) như được thể hiện
trong Hình 2. Đầu tiên, các lớp của học sinh được huấn luyện để dự đoán các biểu diễn
lớp ẩn của giáo viên (Mục 2.2). Tiếp theo, chúng tôi kết hợp dự đoán có che để khuyến
khích mô hình học sinh học các biểu diễn tốt hơn (Mục 2.3). Cuối cùng, mục tiêu học
tương phản ngăn chặn mô hình khỏi bị sụp đổ. (Mục 2.4).

2.2. Chưng cất Từng lớp
Hơn nữa, như Ashihara et al. [6] đã chỉ ra, các mô hình học sinh sâu và hẹp nắm bắt
hành vi của giáo viên tốt hơn. Chúng tôi theo [5] và [6] bằng cách gán mỗi lớp học sinh
dự đoán một lớp ẩn của giáo viên. Ánh xạ lớp học sinh-giáo viên được thu được như sau.
Đặt LT và LS là số lớp của giáo viên và học sinh, với LT ≥ LS. Lớp thứ l của học sinh
học dự đoán lớp thứ ˆl của giáo viên, nơi

ˆl = round((l-1)LT-1)/(LS-1) + 1,                                                (1)

với l = 1, 2, ..., LS. Mỗi lớp học sinh được gán để dự đoán một lớp giáo viên duy nhất,
và các lớp được chọn được phân phối đều trên mô hình giáo viên. Quy tắc ánh xạ này
cho phép các kiến trúc học sinh linh hoạt cho các ứng dụng khác nhau.

Các công trình trước đây chưng cất đầu ra cuối cùng của mỗi lớp giáo viên [4,5].
Được truyền cảm hứng từ data2vec [24], chúng tôi để mô hình học sinh dự đoán các
đặc trưng mạng truyền thẳng (FFN) của mỗi lớp giáo viên để có mục tiêu học tốt hơn.
Cụ thể, học sinh học từ các đầu ra của FFN thứ hai của mỗi khối Conformer trong
giáo viên [25].

2.3. Dự đoán có Che
Các phương pháp KD trước đây thường giữ đầu vào của học sinh không bị che [4,5],
nhưng nhiều phương pháp SSL dựa vào mô hình hóa ngôn ngữ có che [11, 21, 24],
và các nghiên cứu đã chỉ ra kỹ thuật này hữu ích cho chưng cất kiến thức [7,9].
Do đó, chúng tôi chỉ che các khung đầu vào của học sinh và áp dụng chưng cất L2L
cho các khung đã che.

2.4. Mục tiêu Chưng cất Tương phản
Chúng tôi phát hiện rằng việc sử dụng mất mát L1 hoặc L2 cho KD đôi khi dẫn đến
các biểu diễn bị sụp đổ khi kết hợp dự đoán có che nếu các siêu tham số không được
điều chỉnh cẩn thận. Do đó, chúng tôi đề xuất mục tiêu học tương phản để giảm thiểu
vấn đề này [21, 26]. Đối với mỗi bước thời gian đã che t ∈ T trong một phát ngôn,
đầu ra lớp thứ l của học sinh z_t^l dự đoán biểu diễn lớp thứ ˆl của giáo viên h_t^ˆl.
Học sinh tối thiểu hóa khoảng cách giữa z_t^l và h_t^ˆl. Mất mát hồi quy L2 thông thường
được viết là

L_l = Σ_{t∈T} ||z_t^l - h_t^ˆl||_2^2,                                          (2)

trong khi mục tiêu chưng cất tương phản được đề xuất là

L_l = -Σ_{t∈T} log(exp(cos(z_t^l, h_t^ˆl)/τ) / Σ_{h'∈H_t^ˆl} exp(cos(z_t^l, h')/τ)),    (3)

nơi H_t^ˆl là một tập hợp bao gồm h_t^ˆl và K yếu tố gây nhiễu [26] được lấy mẫu
từ lớp thứ ˆl của giáo viên với các chỉ số cũng trong T. τ > 0 là một siêu tham số
và cos(·,·) biểu thị độ tương tự cosine. Với mục tiêu này, mô hình được mong đợi
tránh bị sụp đổ.

3. THÍ NGHIỆM
3.1. Thiết lập
3.1.1. Mô hình
Tất cả các thí nghiệm đều dựa trên w2v-BERT 2.0 [2], một loạt các bộ mã hóa tiếng nói
SSL được huấn luyện với học tương phản [21] và mô hình hóa ngôn ngữ có che [11].
Các kiến trúc Conformer [25] và chi phí tính toán thuận được liệt kê trong Bảng 2.
Kích thước kernel tích chập theo chiều sâu là 31. Mỗi mô hình nhận các đặc trưng
ngân hàng lọc 80 chiều làm đầu vào và giảm mẫu mỗi phát ngôn bằng cách nối
các khung liên tiếp để giảm tốc độ khung từ 100Hz xuống 50Hz. Ngoại trừ Large 40,
tất cả các mô hình w2v-BERT 2.0 đều được tiền huấn luyện từ đầu với một kho
ngữ liệu nội bộ chứa 4M giờ tiếng nói không nhãn, bao gồm 143+ ngôn ngữ. Trừ khi
có quy định khác, học sinh là các mô hình Large 40 hoặc Large 12 được khởi tạo
ngẫu nhiên chưng cất kiến thức từ giáo viên XX-Large.

--- TRANG 3 ---
[THIS IS TABLE: Table 1 showing BLEU scores for multilingual speech-to-text translation evaluated on CoVoST 2 and FLEURS 101 languages test set]

[THIS IS TABLE: Table 2 showing w2v-BERT 2.0 architectures with different dimensions, feed-forward net sizes, and attention heads]

3.1.2. Chưng cất Kiến thức
Chúng tôi triển khai các thí nghiệm với fairseq [32]. Chỉ 92k giờ dữ liệu âm thanh
trong kho ngữ liệu 4M giờ được sử dụng cho chưng cất vì KD yêu cầu ít cập nhật
hơn so với tiền huấn luyện, nơi lượng dữ liệu huấn luyện được sử dụng được tính
toán theo [33]. Chúng tôi đặt τ = 0.1 và K = 100 trong Phương trình 3. Các đặc trưng
đã giảm mẫu được che ngẫu nhiên với khoảng 10 khung và xác suất 0.065, dẫn đến
khoảng 49% khung bị che. Mỗi mô hình được huấn luyện với 200k cập nhật sử dụng
bộ tối ưu hóa Adam [34] với tốc độ học đỉnh 10^-4, β1 = 0.9, β2 = 0.98, ε = 10^-6,
và suy giảm trọng số 10^-2. Tốc độ học tăng tuyến tính trong 4k cập nhật đầu tiên
và giảm tuyến tính về 0 cho phần còn lại. Mỗi mô hình được nén trên 32 GPU NVIDIA
A100 80GB, với kích thước batch hiệu quả là 27.7 phút dữ liệu âm thanh trong mỗi
cập nhật. Học sinh Large 12 và Large 40 mất 2 và 4 ngày để chưng cất từ giáo viên
XX-Large. Mặc dù các tham số của mô hình 0.3B tương tự, thời gian chưng cất của
học sinh 40 lớp cao hơn vì hoạt động thuận của mỗi lớp ẩn không thể được song song hóa.
Một số phương pháp KD trước đây không được bao gồm để so sánh vì chúng yêu cầu
triển khai phức tạp và tìm kiếm siêu tham số.

3.1.3. Dịch thuật Tiếng nói Đa ngôn ngữ
Mô hình dịch thuật tiếng nói sang văn bản tiếng Anh (X-Eng S2T) bao gồm một bộ
mã hóa Conformer, một bộ điều chỉnh độ dài [35], và một mô hình dịch máy NLLB-200
1.3B tham số [36]. Dữ liệu tinh chỉnh bao gồm khoảng 60k giờ tiếng nói và văn bản
dịch thuật được ghép cặp bao gồm 88 hướng X-English. Bộ mã hóa Conformer được
tinh chỉnh hoàn toàn, nhưng chỉ chuẩn hóa lớp và tự chú ý cho NLLB. Tốc độ học
tăng tuyến tính đến 10^-4 trong 5k cập nhật đầu tiên (2 × 10^-4 cho XX-Large), và
sau đó theo lịch trình căn bậc hai nghịch đảo [37]. Tất cả mô hình được huấn luyện
với kích thước batch hiệu quả là 64 phút âm thanh và 150k cập nhật. Chúng tôi sử dụng
16 đến 64 GPU NVIDIA V100 32GB, tùy thuộc vào kích thước mô hình. Chúng tôi đánh
giá các mô hình S2T đã tinh chỉnh trên CoVoST 2 [27] và FLEURS [28] với kích thước
beam giải mã là 5.

3.2. Kết quả Tinh chỉnh
Mục này tiết lộ hiệu quả của CoLLD thông qua việc tinh chỉnh các mô hình w2v-BERT
2.0 trên X-Eng S2T. Như được thể hiện trong Bảng 1, chúng tôi cung cấp ba mô hình
w2v-BERT 2.0 được tiền huấn luyện từ đầu, nơi mô hình 0.3B được phục vụ làm
baseline. Các baseline loại bỏ lớp bảo toàn 30% lớp của mô hình XX-Large bằng
cách bảo toàn các lớp dưới cùng hoặc bỏ qua lớp đồng nhất theo Phương trình 1.

CoLLD Large 40 vượt trội hơn các baseline 0.3B ít nhất hai điểm BLEU trong hầu hết
các tập con, cho thấy rằng học sinh đã thành công trong việc thu nhận kiến thức từ
giáo viên XX-Large. Mặc dù CoLLD Large 40 không thể đạt được hiệu suất tương tự
như giáo viên 1.0B do khả năng mô hình, khoảng cách giữa các mô hình 0.3B và 0.6B
được giảm đáng kể. Đặc biệt trong FLEURS, CoLLD cung cấp điểm BLEU cao hơn
một chút trong hầu hết các tập con so với topline 0.6B. Do đó, CoLLD Large 40 có
thể so sánh với w2v-BERT 2.0 X-Large nhưng chỉ yêu cầu một nửa số tham số.

Chúng tôi cung cấp các nghiên cứu loại bỏ trong cùng bảng. Hiệu suất S2T tổng thể
bị giảm sút bởi việc thay thế từng thành phần được đề xuất trong CoLLD bằng các
phương pháp trước đây, cho thấy sự cần thiết của

--- TRANG 4 ---
[THIS IS TABLE: Table 3 showing SSL pre-trained models with 0.3B parameters on the 10-minute set of the ML-SUPERB benchmark]

thiết kế của CoLLD. Đầu tiên, kiến trúc học sinh nông và rộng (Large 12) giảm một
điểm BLEU trong hầu hết các tập thử nghiệm so với mô hình sâu hơn (Large 40),
củng cố với các nghiên cứu trước đây [5, 6]. Tuy nhiên, Large 12 vẫn vượt trội hơn
tất cả các baseline, và chi phí tinh chỉnh và suy luận của mô hình nông thấp hơn
so với mô hình sâu. Do đó, sự lựa chọn giữa mô hình nông và sâu phụ thuộc vào
kịch bản ứng dụng. Thứ hai, tối ưu hóa với mất mát L2 hoặc học từ đầu ra của
mỗi lớp giáo viên dẫn đến suy giảm 1 đến 2 điểm BLEU, cho thấy rằng các kỹ thuật
được đề xuất chưng cất các biểu diễn tốt hơn từ giáo viên. Thứ ba, thay thế dữ liệu
chưng cất bằng kho ngữ liệu tiếng Anh 1k giờ làm giảm điểm BLEU nhưng hoạt động
tốt hơn so với các baseline, ngụ ý rằng CoLLD vẫn hoạt động ngay cả khi tính đa
dạng của dữ liệu huấn luyện bị giảm. Hơn nữa, khởi tạo mô hình học sinh với một
số lớp giáo viên dẫn đến điểm số tệ hơn đáng kể, vì vậy khởi tạo mô hình là không
cần thiết. Lưu ý rằng chúng tôi không so sánh với DistilHuBERT vì các công trình
trước đây đã chỉ ra L2L KD có hiệu suất vượt trội [5, 6]. Các nghiên cứu loại bỏ
rõ ràng cho thấy tầm quan trọng của CoLLD được đề xuất.

Để đẩy giới hạn của CoLLD, chúng tôi xem xét chưng cất từ một giáo viên đã tinh
chỉnh S2T để so sánh. Trong phần cuối của Bảng 1, kết quả của một mô hình CoLLD
Large 40 được chưng cất từ một giáo viên XX-Large đã tinh chỉnh S2T được báo cáo.
Mô hình nén này cung cấp hiệu suất vượt trội so với topline 0.6B trong nhiều tập
con đánh giá, cho thấy rằng CoLLD có thể áp dụng cho cả mô hình w2v-BERT 2.0
được tiền huấn luyện và tinh chỉnh. Do đó, nếu có mô hình giáo viên được tinh chỉnh
với dữ liệu có nhãn, CoLLD tạo ra các mô hình nén tốt hơn. Tổng thể, CoLLD thành
công nén một w2v-BERT 2.0 XX-Large được tiền huấn luyện 70% trong khi duy trì
hiệu suất X-Eng S2T tốt.

3.3. SUPERB Đa ngôn ngữ
Mục này đánh giá CoLLD với Multilingual SUPERB (ML-SUPERB) [29], một benchmark
xử lý tiếng nói đa ngôn ngữ tiêu chuẩn, để cung cấp so sánh toàn diện hơn với
các mô hình SSL khác. ML-SUPERB bao gồm 143 ngôn ngữ và bốn tác vụ: ASR đơn
ngôn ngữ (Mono-ASR), ASR đa ngôn ngữ (Multi-ASR), nhận dạng ngôn ngữ (LID),
và Multi-ASR + LID. Chúng tôi sử dụng tập 10 phút của ML-SUPERB để cho thấy
hiệu suất của các mô hình được tiền huấn luyện trong thiết lập tài nguyên thấp.
Để so sánh công bằng, các mô hình được tiền huấn luyện và chưng cất được đóng
băng và phục vụ như các bộ trích xuất đặc trưng trong quá trình huấn luyện mô hình
hạ nguồn. Chúng tôi theo triển khai như trong ESPnet [38].

Như được thể hiện trong Bảng 3, w2v-BERT 2.0 cung cấp một baseline vững chắc
so với các công trình trước đây vì mô hình này được huấn luyện với dữ liệu nhiều
hơn đáng kể. Tiếp theo, CoLLD vượt trội hơn w2v-BERT 2.0 và các phương pháp
trước đây khác trong hầu hết các tác vụ ML-SUPERB và đạt được điểm SUPERB tổng
thể tốt nhất bằng cách chỉ sử dụng 92k giờ dữ liệu chưng cất. Kết quả một lần nữa
củng cố rằng CoLLD thành công chưng cất kiến thức từ giáo viên XX-Large.

3.4. Tác động của Cập nhật Chưng cất
Mục này điều tra tác động của dữ liệu cần thiết cho CoLLD bằng cách thay đổi tổng
số cập nhật chưng cất. Như được thể hiện trong Hình 3, CoLLD vượt trội hơn baseline
được tiền huấn luyện từ đầu 0.3B chỉ với 50k cập nhật chưng cất. Trong khi đó,
khi được huấn luyện với 200k cập nhật, CoLLD đạt hiệu suất tương tự như mô hình
topline 0.6B. Do đó, lượng dữ liệu chưng cất có tương quan cao với hiệu suất
hạ nguồn, và các mô hình được chưng cất cung cấp biểu diễn tốt hơn khi có thêm
dữ liệu và tài nguyên tính toán.

[THIS IS FIGURE: Figure 3 showing a graph of distillation updates vs. FLEURS-101 X-Eng BLEU scores]

4. KẾT LUẬN
Bài báo này đề xuất CoLLD, một phương pháp nén mô hình mới bằng cách kết hợp
chưng cất kiến thức từng lớp và học tương phản cho các bộ mã hóa tiếng nói đa
ngôn ngữ quy mô lớn. Chúng tôi chỉ ra rằng CoLLD vượt trội hơn các phương pháp
nén trước đây về nhận dạng tiếng nói đa ngôn ngữ và dịch thuật tiếng nói sang
văn bản bằng cách đánh giá các phương pháp được đề xuất trên các benchmark
nội bộ và công khai. Phương pháp này giảm kích thước mô hình của các bộ mã hóa
tiếng nói được tiền huấn luyện mạnh mẽ trong khi duy trì hiệu suất tốt sau khi
tinh chỉnh, cho phép các ứng dụng trên thiết bị và streaming.

--- TRANG 5 ---
5. TÀI LIỆU THAM KHẢO
[1] A. Mohamed et al., "Self-supervised speech representation
learning: A review," IEEE JSTSP, 2022.
[2] Seamless Communication et al., "Seamlessm4t—massively
multilingual & multimodal machine translation," arXiv, 2023.
[3] Y. Zhang et al., "Google usm: Scaling automatic speech recognition beyond 100 languages," arXiv, 2023.
[4] H.-J. Chang, S.-w. Yang, and H.-y. Lee, "DistilHuBERT:
Speech representation learning by layer-wise distillation of
hidden-unit bert," in ICASSP, 2022.
[5] Y. Lee, K. Jang, J. Goo, Y. Jung, and H. Kim, "Fithubert:
Going thinner and deeper for knowledge distillation of speech
self-supervised learning," Interspeech, 2022.
[6] T. Ashihara, T. Moriya, K. Matsuura, and T. Tanaka, "Deep
versus wide: An analysis of student architectures for taskagnostic knowledge distillation of self-supervised speech models," Interspeech, 2022.
[7] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang,
T. Ko, and H. Li, "Lighthubert: Lightweight and configurable
speech representation learning with once-for-all hidden-unit
bert," Interspeech, 2022.
[8] K.-P. Huang, T.-h. Feng, Y.-K. Fu, T.-Y. Hsu, P.-C. Yen, W.-C.
Tseng, K.-W. Chang, and H.-y. Lee, "Ensemble knowledge distillation of self-supervised speech models," in ICASSP, 2023.
[9] K. Jang, S. Kim, S.-Y. Yun, and H. Kim, "Recycle-and-distill:
Universal compression strategy for transformer-based speech
ssl models with attention map reusing and masking distillation," Interspeech, 2023.
[10] H. Wang, S. Wang, W.-Q. Zhang, and J. Bai, "Distilxlsr: A
light weight cross-lingual speech representation model," Interspeech, 2023.
[11] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, "Hubert: Self-supervised speech representation learning by masked prediction of hidden units,"
TASLP, vol. 29, 2021.
[12] C.-I. J. Lai, Y. Zhang, A. H. Liu, S. Chang, Y.-L. Liao, Y.-S.
Chuang, K. Qian, S. Khurana, D. Cox, and J. Glass, "PARP:
Prune, adjust and re-prune for self-supervised speech recognition," NeurIPS, 2021.
[13] Y. Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, "Structured pruning of self-supervised pre-trained models for speech
recognition and understanding," in ICASSP, 2023.
[14] H. Jiang, L. L. Zhang, Y. Li, Y. Wu, S. Cao, T. Cao, Y. Yang,
J. Li, M. Yang, and L. Qiu, "Accurate and structured pruning
for efficient automatic speech recognition," Interspeech, 2023.
[15] H. Wang, S. Wang, W.-Q. Zhang, H. Suo, and Y. Wan, "Taskagnostic structured pruning of speech representation models,"
Interspeech, 2023.
[16] Y. Peng, Y. Sudo, S. Muhammad, and S. Watanabe, "Dphubert: Joint distillation and pruning of self-supervised speech
models," Interspeech, 2023.
[17] Y. Peng, J. Lee, and S. Watanabe, "I3d: Transformer architectures with input-dependent dynamic depth for speech recognition," in ICASSP, 2023.
[18] C.-F. Yeh, W.-N. Hsu, P. Tomasello, and A. Mohamed, "Efficient speech representation learning with low-bit quantization,"
arXiv, 2022.
[19] S.-w. Yang et al., "SUPERB: Speech processing universal performance benchmark," in Interspeech, 2021.
[20] H.-S. Tsai et al., "SUPERB-SG: Enhanced speech processing
universal PERformance benchmark for semantic and generative capabilities," in ACL, 2022.
[21] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, "wav2vec
2.0: A framework for self-supervised learning of speech representations," in NeurIPS, 2020.
[22] A. Babu et al., "Xls-r: Self-supervised cross-lingual speech
representation learning at scale," Interspeech, 2022.
[23] V. Pratap et al., "Scaling speech technology to 1,000+ languages," arXiv, 2023.
[24] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli,
"data2vec: A general framework for self-supervised learning
in speech, vision and language," in ICML, 2022.
[25] A. Gulati et al., "Conformer: Convolution-augmented transformer for speech recognition," Interspeech, 2020.
[26] A. v. d. Oord, Y. Li, and O. Vinyals, "Representation learning
with contrastive predictive coding," arXiv, 2018.
[27] C. Wang, A. Wu, J. Gu, and J. Pino, "Covost 2 and massively
multilingual speech translation," in Interspeech, 2021.
[28] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod,
S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, "Fleurs:
Few-shot learning evaluation of universal representations of
speech," in SLT, 2023.
[29] J. Shi et al., "Ml-superb: Multilingual speech universal performance benchmark," Interspeech, 2023.
[30] T.-h. Feng et al., "Superb@ slt 2022: Challenge on generalization and efficiency of self-supervised speech representation
learning," in SLT, 2022.
[31] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and
M. Auli, "Unsupervised cross-lingual representation learning
for speech recognition," Interspeech, 2021.
[32] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng,
D. Grangier, and M. Auli, "fairseq: A fast, extensible toolkit
for sequence modeling," in NAACL-HLT, 2019.
[33] H.-J. Chang, A. H. Liu, and J. Glass, "Self-supervised Finetuning for Improved Content Representations by Speakerinvariant Clustering," in Interspeech, 2023.
[34] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," ICLR, 2015.
[35] J. Zhao, H. Yang, E. Shareghi, and G. Haffari, "M-adapter:
Modality adaptation for end-to-end speech-to-text translation,"
Interspeech, 2022.
[36] M. R. Costa-jussà et al., "No language left behind: Scaling
human-centered machine translation," arXiv, 2022.
[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all
you need," in NIPS, 2017.
[38] S. Watanabe et al., "Espnet: End-to-end speech processing
toolkit," Interspeech, 2018.

--- TRANG 6 ---
[Đây là hai biểu đồ hình thanh lớn hiển thị điểm BLEU chi tiết cho tất cả các ngôn ngữ trong tác vụ CoVoST 2 và FLEURS-101 X-Eng S2T, so sánh hiệu suất của các mô hình w2v-BERT 2.0 khác nhau]

Fig. 4. Điểm BLEU hoàn chỉnh trên tác vụ CoVoST 2 X-Eng S2T. w2vb2 biểu thị w2v-BERT 2.0.

Fig. 5. Điểm BLEU hoàn chỉnh trên tác vụ FLEURS-101 X-Eng S2T. w2vb2 biểu thị w2v-BERT 2.0. Các ngôn ngữ được gạch chân biểu thị các ngôn ngữ chưa thấy trong dữ liệu tinh chỉnh X-Eng.

[THIS IS TABLE: Table 4 showing layer mapping between student and teacher models]
Bảng 4. Ánh xạ lớp thứ l của học sinh đến lớp thứ ˆl của giáo viên cho
CoLLD được dẫn xuất từ Phương trình 1 khi chưng cất từ giáo viên 1B.

Kiến trúc    LS    LT    (l, ˆl)
Large 12     12    40    (1, 1), (2, 5), (3, 8), (4, 12),
                        (5, 15), (6, 19), (7, 22), (8, 26),
                        (9, 29), (10, 33), (11, 36), (12, 40)
Large 40     40    40    (1, 1), (2, 2), (3, 3), ..., (40, 40)

6. PHỤ LỤC
6.1. Chi tiết Chưng cất Kiến thức
Tại đây, chúng tôi cung cấp chi tiết về triển khai chưng cất kiến thức. Trong Bảng 4,
chúng tôi chỉ ra ánh xạ lớp học sinh-giáo viên trong các thí nghiệm chưng cất của
chúng tôi. Tiếp theo, mất mát hồi quy L2 cho một phát ngôn có thể được biểu diễn là

Lℓ2 = 1/(D·LS·|T|) ∑(l=1 to LS) ∑(t∈T) ||z_t^l - h_t^ˆl||_2^2,                (4)

nơi D là chiều của các biểu diễn z và h, và |T| là số bước thời gian bị che.
Đối với học tương phản, hàm mất mát là

LContrastive = -1/(LS·|T|) ∑(l=1 to LS) ∑(t∈T) log(exp(cos(z_t^l, h_t^ˆl)/τ) / ∑(h'∈H_t^ˆl) exp(cos(z_t^l, h')/τ)).   (5)

Cuối cùng, mất mát của tất cả các phát ngôn trong một mini-batch được tính trung
bình để có được hàm mất mát tổng cho tối ưu hóa.

6.2. Chi tiết Tinh chỉnh S2T
Mục này cung cấp chi tiết triển khai của tinh chỉnh X-Eng S2T. Một số siêu tham số
tinh chỉnh cho các kiến trúc mô hình khác nhau

[THIS IS TABLE: Table 5 showing fine-tuning hyperparameters]
Bảng 5. Siêu tham số tinh chỉnh X-Eng S2T cho các kiến trúc mô hình khác nhau.

Mô hình       Tốc độ      Kích thước Batch    Tích lũy      GPUs
             Học         Mỗi GPU             Gradient      
XX-Large     2×10^-4     30 giây             2             64
X-Large      1×10^-4     60 giây             2             32
Large 12     1×10^-4     60 giây             2             32
Large 40     1×10^-4     48 giây             2             40

được thể hiện trong Bảng 5. Đầu tiên, độ dài tối đa của một phát ngôn đầu vào là
30 giây, và số lượng token đầu ra tối đa là 113. Thứ hai, các khung đầu vào được
che ngẫu nhiên tương tự như quá trình chưng cất, nhưng với độ dài che là 5 và
xác suất che là 0.02. Tiếp theo, loại bỏ lớp với xác suất 0.1 được áp dụng cho
cả mô hình w2v-BERT 2.0 và NLLB. Hơn nữa, mô hình transformer NLLB được
tiền huấn luyện với các tác vụ dịch máy, nhận văn bản làm đầu vào, vì vậy chúng
tôi thêm một bộ điều chỉnh độ dài [35] sau bộ mã hóa tiếng nói để khớp độ dài
chuỗi giữa tiếng nói và văn bản. Bộ điều chỉnh bắt đầu với một lớp CNN 1-D
(kích thước kernel = stride = 8) và một đơn vị tuyến tính có cổng, theo sau là
một lớp bộ mã hóa Conformer đơn với kích thước kernel tích chập là 31. Sau bộ
điều chỉnh này, độ dài phát ngôn được giảm đi một hệ số tám để khớp với modalità văn bản.

6.3. Kết quả X-Eng S2T Hoàn chỉnh
Trong Hình 4 và 5, chúng tôi thể hiện điểm BLEU của một số mô hình của tất cả
các ngôn ngữ trong các tập đánh giá CoVoST 2 và FLEURS. Chi tiết về các ngôn
ngữ khác nhau trong tập dữ liệu tinh chỉnh có thể được tìm thấy trong Bảng 35
của [2]. Hầu hết các ngôn ngữ chưa thấy trong các tập thử nghiệm FLEURS có
điểm BLEU thấp. Tuy nhiên, một số ngôn ngữ chưa thấy như ast (Asturian) và
ltz (Luxembourgish) có điểm BLEU cao. Chúng tôi nghi ngờ các ngôn ngữ tài
nguyên cao trong cùng họ ngôn ngữ gây ra hiện tượng này.
