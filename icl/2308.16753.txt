# 2308.16753.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2308.16753.pdf
# File size: 3833097 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Context Aware Query Rewriting for Text Rankers using LLM
Abhijit Anand
aanand@l3s.de
L3S Research Institute
GermanyVenktesh V
v.Viswanathan-1@tudelft.nl
Delft University of Technology
Netherlands
Vinay Setty
vsetty@acm.org
University of Stavanger
NorwayAvishek Anand
Avishek.Anand@tudelft.nl
Delft University of Technology
Netherlands
ABSTRACT
Query rewriting refers to an established family of approaches that
are applied to underspecified and ambiguous queries to overcome
the vocabulary mismatch problem in document ranking. Queries are
typically rewritten during query processing time for better query
modelling for the downstream ranker. With the advent of large-
language models (LLMs), there have been initial investigations
into using generative approaches to generate pseudo documents to
tackle this inherent vocabulary gap. In this work, we analyze the
utility of LLMs for improved query rewriting for text ranking tasks.
We find that there are two inherent limitations of using LLMs
as query re-writers – concept drift when using only queries as
prompts and large inference costs during query processing. We
adopt a simple, yet surprisingly effective, approach called context
aware query rewriting ( CAR ) to leverage the benefits of LLMs for
query understanding. Firstly, we rewrite ambiguous training queries
by context-aware prompting of LLMs, where we use only relevant
documents as context. Unlike existing approaches, we use LLM-
based query rewriting only during the training phase. Eventually, a
ranker is fine-tuned on the rewritten queries instead of the original
queries during training. In our extensive experiments, we find that
fine-tuning a ranker using re-written queries offers a significant
improvement of up to 33% on the passage ranking task and up to
28% on the document ranking task when compared to the baseline
performance of using original queries.
CCS CONCEPTS
•Information systems →Retrieval models and ranking .
KEYWORDS
query rewriting, rank fusion, ranking performance
1 INTRODUCTION
The vocabulary mismatch between user queries and the documents
is a well known problem in the field of Information Retrieval (IR).
The user information needs usually represented in the form of
keyword queries can be ambiguous and may not be lexically similar
to the documents. The queries could be under-specified, rendering it
difficult to understand the user intent, which affects the downstream
retrieval performance. For instance, the query define sri could
refer to “sanskrit word sri” or “ Socially Responsible Investment”.
To disambiguate the users’ information need, many approaches
have been proposed to address the vocabulary gap.Classical approaches like pseudo-relevance feedback mechanism
[10,28] expand the original query with keywords from top-ranked
results to the original query. Alternate term-based approaches rep-
resent the queries in a continuous vector space , followed by a KNN-
search to expand query terms [ 18,51]. There exist other term-based
approaches that use seq2seq model [ 19] to generate query rewrites.
However, all the above approaches are term-based approaches that
improve retrieval but not necessary in document ranking.
Going beyond keywords, researchers have explored the possibil-
ity of natural language question generation for clarifying queries [ 35]
or rephrasing original queries using deep generative models for
alternative formulations of the original query [ 53]. More recently,
document expansion approaches have been adopted to improve
retrieval performance. Doc2Query [33] employs neural models
to predict queries relevant to documents and enhances document
representations. Especially with the advent of large language mod-
els or LLMs [ 27,40], recent approaches have started to investigate
the utility of LLMs to aid in enhancing query and document rep-
resentations [ 17,44]. LLMs are promising because encode world
knowledge [ 50] from pre-training can be employed to reformulate
the under-specified queries. In principle, the LLMs can be used to
expand/explain/specify concise and ambiguous queries, thereby
disambiguating the user information need. This bridges the gap
between the latent user intent in the original queries and the re-
triever’s representation of the user information need. The approach
query2doc [ 44], generates pseudo-documents by few-shot prompt-
ing LLMs and concatenates them with the original query to form a
new expanded query . However, there are two major limitations to
using LLMs for generating plausible query re-writes.
The problem of misaligned query and documents. Many queries
have multiple aspects, senses, and intent granularities. When using
LLM-based query expansions, the expanded query results in choos-
ing one aspect among the many possible aspects. This entails in a
problem that the chosen intent by the expansion might not align
with the ground-truth intent. This lack of alignment between the
actual intent and the inferred intent is problematic during training.
Specifically, such intent non-alignment results in the case where
the relevant document is forced to be matched with an erroneously
expanded query. For instance, in Table 1, we observe that for the
querydefine sri ,Query2Doc drifts away from the intent and
generates a query related to Stanford Research Institute instead of
denoting the Sanskrit word. In a similar manner, for the query HS
worms the generated output reflects multiple intents.arXiv:2308.16753v1  [cs.IR]  31 Aug 2023

--- PAGE 2 ---
Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand
Method Query Text
Original query define sri
Query2Doc SRI stands for "Stanford Research Institute," which is a nonprofit research organization that was founded in 1946 as
part of Stanford University. Over the years, SRI has conducted groundbreaking research in a wide range [. . . ]
Davinci-003 Sri is a Sanskrit word meaning “holy” or “auspicious. ” It is often used in Hinduism as an honorific title for a deity or
spiritual teacher. It is also used in Buddhist and Jain contexts, and is an important concept in South Asian culture.
ChatGPT Find the definition and meaning of the acronym SRI (Socially Responsible Investing).
CAR (Davinci-003 ) What is the meaning of Sanskrit word "Sri" and how is it used as a title of veneration?
CAR (ChatGPT ) What is the meaning of the Sanskrit word "Sri" and how is it used in Indian subcontinent?
CAR (Davinci-003 )(doc) What is the origin and meaning of the term "Sri"?
CAR (ChatGPT )(doc) What is the origin of the word "Sri" used in India and other South Asian countries?
Original query hs worms
Query2Doc HS Worms can refer to several different things depending on the context. If you are referring to HS Worms as an
educational institution, then it stands for Hochschule Worms which is a university of applied sciences or it can
refer to heartworms [. . . ]
Davinci-003 Find information about heartworms in dogs
ChatGPT Find information about the symptoms, causes, and treatment options for heartworm disease in dogs (also known as
HS worms).
CAR (Davinci-003 ) What are the courses offered by Hochschule Worms?
CAR (ChatGPT ) What are the programs offered by Hochschule Worms?
CAR (Davinci-003 )(doc) What is the country rank and world rank of HS Worms University?
CAR (ChatGPT )(doc) What is Hochschule Worms (HS Worms) and where is it located?
Original query ephesians 6:5-9
Query2Doc Ephesians 6:5-9 is a passage from the New Testament that speaks to the relationship between slaves and masters in
ancient times. It begins by instructing slaves to obey their earthly masters with respect [. . . ]
Davinci-003 Find information about Ephesians 6:5-9.
ChatGPT What is the meaning and interpretation of Ephesians 6:5-9 in the Bible?
CAR (Davinci-003 ) 7 With good will render service, as to the Lord, and not to men, 8 knowing that whatever good thing each one does,
this he will receive back from the Lord
CAR (ChatGPT ) How does the passage in Ephesians 6:5-9 instruct slaves to behave towards their masters?
CAR (Davinci-003 )(doc) too have a Master in heaven. What does Ephesians 6:5-9 teach about the relationship between slaves and masters?
CAR (ChatGPT )(doc) What is the meaning of Ephesians 6:5-9 in relation to submission and slavery?
Original query price nrz
Query2Doc Price NRZ stands for "Non-Return-to-Zero" and refers to a digital signal encoding technique used in telecommuni-
cations. In this technique, the voltage level of the signal remains constant during each bit interval[. . . ]
Davinci-003 Find information on the Pricing of Non-Recourse Z-Bonds.
ChatGPT Find information on the pricing strategy for non-return-to-zero (NRZ) encoding.
CAR (Davinci-003 ) What is the pricing of New Residential Investment Corp. common stock on Jan 30, 2017 7:16 PM EST?
CAR (ChatGPT ) What is the public offering price of New Residential Investment Corp.’s common stock?
CAR (Davinci-003 )(doc) announced today that it has entered into definitive agreements to acquire Shellpoint Partners, LLC ("Shellpoint"), a
leading mortgage servicer and originator.hat is the price of New Residential Investment Corp (NRZ) stock
CAR (ChatGPT )(doc) What is the current share price and financial information of New Residential Investment Corp (NRZ)?
Table 1: Comparing Original ambiguous queries with their rewrites using Query2Doc ,Davinci-003 ,ChatGPT approach with
in-context and context aware rewriter( CAR ) techniques. Approach with (doc) are rewrites using MS MARCO document corpus
and the rest from MS MARCO passage corpus.
The problem of efficient inference. Secondly, a serious limita-
tion (as acknowledged in [ 44]) is the computational overhead from
both the retrieval and re-ranking phases. Expanded query terms
in the re-written queries increase index lookups. More acutely,
query expansions use tokenwise auto-regressive decoding during
inference. However, current infrastructures just cannot support
efficient LLM-based inference during query processing. Therefore,to stay within sub-second ranking requirements without using pro-
hibitively expensive compute, the choice of a LLM during query
processing should be avoided.
Contextualized query rewriting. We make two simple yet im-
portant design decisions to overcome the problem of misalignment
and efficiency. We first generate query rewrites by additionally

--- PAGE 3 ---
Context Aware Query Rewriting for Text Rankers using LLM
providing the relevant document as context during training. Con-
sequently, the generated query rewrite is fully aligned with the
context improving training of text rankers. Secondly, and unlike
existing works, we fully avoid any query rewriting using LLMs dur-
ing inference . In other words, we assume that training a ranker
to match LLM-generated queries with relevant documents results
in learning a generalized ranking model.
Methodologically, we propose an LLM-based context-aware query
rewriting framework CAR , that replaces the traditional query rewriter
with a LLM for rewriting ambiguous queries. We employ context-
aware prompting to test the effectiveness of LLMs as a disambigua-
tion engine, using the ambiguous query and the relevant document
as a context in the prompts. We also include a context selection
mechanism to select relevant sections when a context is long and
spans multiple topics to handle topic drift . The outputs of our query
rewriter are used to fine-tune a ranking model to transfer the knowl-
edge of user information needs to the ranking model. At inference
time , the ranker equipped with knowledge of query disambiguation
mechanism improves the document ranking performance for sub-
sequent ambiguous queries without the rewriter component. The
proposed framework can be used with any off-the-shelf ranker. Dur-
ing inference, the ranking model fine-tuned on re-written queries
yields much better ranking performance in comparison to the orig-
inal queries. Our approach obviates the need for the LLM prompting
during inference because we consider efficiency and latency require-
ments during inference as a non-negotiable constraint . From Table 1
we observe that the proposed context-aware query reformulation
approach CAR , can generate concise rewrites that reflect the intent
when compared to Query2Doc . We also observe that it performs
better than pure in-context learning based approaches, as seen from
Table 1 and also through our empirical analysis of ranking results.
We perform extensive experiments using LLMs of different pa-
rameter scales and demonstrate that the proposed approach results
in better ranking performance. Specifically, we find that fine-tuning
a ranker using re-written queries offers a significant improvement
of up to 33% on the passage ranking task and up to 28% on the doc-
ument ranking task when compared to the baseline performance
of using original queries.
1.1 Research Questions
We address the following research questions:
RQ1 : Can we employ LLMs to generate fluent natural language
rewrites of original queries from ambiguous queries?
RQ2 : How effective is a ranker fine-tuned on rewritten queries for
downstream document ranking task?
Towards answering these research questions, we do extensive
experiments on trec web 2012 using LLM models (Section 4.2) for
evaluating quality of rewrites and on TREC-DL -19 and TREC-DL -20
passage and document dataset for re-ranking.
1.2 Contributions
In summary, here is a list of our contributions:
(1)We propose a Context Aware Rewriter ( CAR ) for query
reformulation, based on context-aware prompting of Large
Language Models (LLMs) which generate natural languagerewrites for ambiguous queries . The natural language rewrites
are used to fine-tune the ranker for encoding the ability to
disambiguate the user information need.
(2)We experiment with LLMs of different parameter scales and
with different pre-training objectives to demonstrate the
difference in quality of rewrites.
(3)We show that the ranking model fine-tuned on generated
rewrites delivers substantial performance improvement on
the ambiguous user queries at inference time.
2 RELATED WORK
In this section, we discuss two aspects of related work, namely
query expansion and generative capabilities of Large Language
Models. We further discuss how LLMs are leveraged in addressing
the vocabulary mismatch problem in information retrieval.
2.1 Query rewriting
One of the central problems in IR is bridging the lexical gap between
the user specified query and the documents. This can also be seen
as reconciling the difference between user intent and the intent
of the system ( machine intent ). The common approaches of query
reformulation involve query expansion, synonym substitution and
paraphrasing.
Query expansion approaches have been adopted to bridge this
gap. These approaches typically involve addition of terms to the
original query based on relevance feedback [ 22,37]. When user
relevance feedback is unavailable, pseudo-relevance feedback mech-
anism is applied [ 10,28]. Here, the top-ranked results to the original
query are used to expand the query with additional terms. However,
the performance of the rewritten query is severely limited by the
quality of the top-ranked results [ 7] and is rarely used in dense
retrieval [ 3]. Alternatively, researchers have proposed rephrasing
the original queries to tackle the “lexical chasm" problem. In the
work [ 58], the authors rephrase the user specified query iteratively
by mining similar phrases from WordNet. Alternatively, researchers
have also explored substituting terms in the input query with syn-
onymous terms based on user query logs [ 21]. However, these ap-
proaches are term based and expand queries based on static sources,
making it difficult to adapt to changes in the retrieval system.
The generative approaches in query rewriting involve gener-
ating paraphrases of the original queries. In the work [ 36], the
authors employ statistical approaches to rephrase terms of the
query and add the equivalent terms to the original query. however,
this could lead to ill-formed queries, as the term level paraphrasing
does not consider the surrounding context. It also heavily relies
on user feedback to select the best paraphrased version which is
cumbersome. More promising approaches in query expansion in-
volve paraphrasing queries using a generative model [ 53] at once
instead of phrasal level rewrites. For instance, the query “average
tesla cost" is rephrased to “what is the cost of the new tesla". Other
generative approaches use chain-of-thought and relevance feed-
back [ 20] to expand query terms or use pseudo relevance feedback
along with prompting or fine-tuning [ 45] for query reformulation.
However, this approach just provides alternative formulations of
query leveraging equivalent queries data from MS MARCO dataset

--- PAGE 4 ---
Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand
“deﬁne Sri”
Fine tuned rankerQuery   + Context Training timeWhat is the meaning of Sanskrit word "Sri" and how is it used as a title of veneration? Rewritten query   
LLM as  Query RewriterPerspectives on Large Language Models for Relevance JudgmentApril 2023, arXiv, Internetanswered with relevant text. An automatic question-answering Sys-tem is asked to answer these exam questions by using the contentof retrieved documents. The idea is that the more questions canbe answered correctly with the document, the better the searchsystem that retrieved the document is. Similar paradigms have beenused successfully in other labeling tasks as well [28, 31, 45]Query Performance Prediction.A related body of work concernsthe Query Performance Prediction (QPP), which is de￿ned as thetask of evaluating the performance of anIRsystem, in the absenceof human-made relevance judgements [15,42]. In this regard, ourproposal for automatic assessment of the documents using LargeLanguage Model (LLM) not only would provide bene￿ts to a numberof downstream tasks, such asQPP, but its e￿ectiveness has alreadybeen partially shown and is supported by￿ourishing literatureconcerning LLMs in the QPP domain [4, 5, 17, 27].2.3 Fully Automated Test CollectionsA further strategy to devise queries automatically is the Wikimarksapproach [29]. Wikimarks derives queries from the title and head-ing structure of Wikipedia articles, with passages below taken asrelevant. This approach has also been applied to aspect-based sum-marization [43], and text segmentation [6].Reconstruct Documents.Instead of hiring assessors, repositoriesof semi-structured (human-authored) articles can be used to derivewhat the human author considered relevant. To this end approachesuse anchor text [7], metadata of scienti￿c article sections [12],categories in the Open Directory Project [10], glosses in Freebase[25] or infoboxes [44, 51].Evaluation of Automatic Evaluation.A question is how well au-tomatic assessments would agree with manual assessments. To thisend, a study on the correlation of leaderboards on the TREC CARdata found a very high-rank correlation [30]. We repeat a similarstudy in the context of LLMs in Section 5.3 SPECTRUM OF HUMAN–MACHINECOLLABORATIONTo identify what contributions LLMs may provide to relevance judg-ments, we devise a human–machine collaboration spectrum. Thisspectrum outlines di￿erent levels of collaboration between humansand LLMs. At one end, humans make judgments manually, while atthe other end, LLMs replace humans completely. In between, LLMsassist humans with various degrees of interdependence. A sum-mary of our proposed four levels of human–machine collaborationis shown in Table 1. In the following, we discuss each level in detail.Human Judgment.On one extreme, humans do all judgmentsmanually and decide what is relevant without being in￿uencedby an LLM. In reality, of course, humans are still supported withbasic features of the judgment interface. Such features might bebased on heuristics, but should not require any form of automatictraining/feedback. For instance, humans may de￿ne “scan terms”to be highlighted in the text, they may limit viewing the pool ofdocuments that have already been judged, or they may order docu-ments so that similar documents are near each other. This end ofTable 1: Collaboration perspective: spectrum of possibilitiesfor collaborative
human –
machine task organization toproduce relevance judgments. The4indicates where on thespectrum each possibility falls.CollaborationIntegrationTask OrganizationHuman Judgment
4The human will do all judgments manuallywithout any kind of support.
4Humans have full control of judging but aresupported by text highlighting, documentclustering, etc.AI Assistance
4The human assessor judges anLLM-generated summary of the document.
4Balanced competence partitioning. Humansand LLMs focus on tasks they are good at.Human Veri￿cation
4Two LLMs each generate a judgment, and ahuman selects the better one.
4An LLM produces a judgment (and anexplanation) that a human can accept/reject.
·=4LLMs are considered crowdworkers, variedby speci￿c characteristics, and controlled bya human.Fully Automated
4Fully automatic assessment.the spectrum thus represents the status quo, where humans are, inthe end, the only reliable judges.AI Assistance.Advanced assistance can come in many forms,for example, an LLM may generate a summary of a to-be-judgeddocument so that the human assessor can more e￿ciently makea judgment based on this compressed representation. Another ap-proach could be to manually de￿ne information nuggets that arerelevant (e.g., exam questions [63]) and to then train an LLM toautomatically determine how many test nuggets are contained inthe retrieved results (e.g., via a QA system).This leads us to the￿rst research direction towards improvingthe human–machine collaboration:How to employ LLMs, as well asother AI tools, to aid human assessors in devising reliable judgmentswhile enhancing the e￿ciency of the process?What are tasks that canbe taken over by LLMs (e.g., document summarization or keyphraseextraction)?Human Veri￿cation.For each document to judge, a￿rst-passjudgment of an LLM is automatically produced as a suggestionalong with a generated rationale. We consider this to be ahuman-in-the-loopapproach: one or more LLMs provide their relevance349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406ICTIR ’23, July 23–27, 2023, Taipei, TaiwanAnon.407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464Figure 1: Overview of proposed frameworkmethods are domain-speci￿c, [45] propose a template-based weaksupervised learning approach for open domain IR.Document expansionapproaches have also been proposed totackle the vocabulary mismatch issue in IR. Doc2query [31] anddocTTTTquery [8], generates pseudo queries from document con-text using a seq2seq model and appends them to documents toenhance the ranking results. Works like InPars [5] and PromptAga-tor [12] focus on using large generative models to generate queriesfrom sampled documents to increase training data for dense re-trieval tasks. However, these approaches are prone to hallucinationand may drift away from the original intent of the query [17]. Inthis paper, we take a step in the direction of reformulating am-biguous queries by incorporating relevant context to transfer theknowledge of the disambiguation mechanism to a ranker for im-proved re-ranker performance. Our approach also departs from thetraditional approaches by employing a monolithic LLM for queryreformulation than multiple components that could lead to errorpropagation.2.2 Large Language ModelsRecent advances in generative modeling have led to large languagemodels that apply to a wide range of tasks [1,6,41]. These modelstrained in a self-supervised fashion demonstrate emergent capa-bilities where they can extrapolate to new tasks with few demon-stration samples [14,25,42]. These advances have also led to re-searchers rethinking parts of retrieval systems. Generative retrievalis one such emerging paradigm [4,22,27,37] where neural lan-guage models are used as indices for retrieval. The model employsconditional decoding approaches to generate document identi￿ersthat directly map to ground truth documents. This is accomplishedby training the language models on relevant data.More recently, researchers have tested the e￿ectiveness of LLMsfor document expansion [16,39] in a zero-shot and few-shot set-ting for dense retrieval. However, these approaches have seriouslimitations at inference time in terms of e￿ciency due to the au-toregressive decoding of LLMs. These approaches are also proneto hallucination due to the generation of long context for rank-ing. Researchers have also explored task-aware tuning of densePrompt:Being a ranking model your￿rst task is to do query expansion. Thismeans given a query and a document expand the query such that it isrelevant to the document. Expand and contextualize query as best asyou can in one or two short sentences.Query:de￿ne ‘sri’Document:Rate this de￿nition: Sri, also transliterated as Sree or Shrior Shree is a word of Sanskrit origin, used in the Indian subcontinentas polite form of address equivalent to the English Mr. in written andspoken language, or as a title of veneration for deities.Generated text from LLM:What is the meaning of the Sanskrit word ""Sri"" and how is it used inIndian subcontinent?Table 2: An example of context-aware prompting of LLM forquery reformulationretrievers [2] by appending queries with task-speci￿c instructionsas input to the dense encoder. The instructions equips the retrieverwith the ability to decipher users information needs. However, theinstructions require manual annotation and are limited to speci￿ctasks. In this paper, we make the ranking model aware of disam-biguation mechanism of under-speci￿ed queries by￿ne-tuning iton reformulated queries generated by a LLM.3 METHODIn this section, we describe the proposed Context Aware Rewriter(CAR) framework for ambiguous query reformulation. Figure 1shows the working of the proposed framework during the trainingand inference stages. During training, the framework is composed ofa query reformulation phase and a document ranking phase. In thequery reformulation phase, we employ a context-aware promptingof a LLM (Section 3.1). During inference, the ranker which was￿ne-tuned on disambiguated queries is directly employed to rankdocuments for new queries without rewriting them, as shown inFigure 1.3.1 Query RewriterAlgorithm 1 details the query reformulation process. Given anambiguous query@, our goal is to generate a rewrite@⇤that helpsdisambiguate the intent of the original query. We generate queryrewrites by conditioning the LLM ('⇢) through few-shot promptingon the relevant document for the query3+to avoid topic drift. Anexample of the few-shot prompting employed is shown in Table 2.@⇤='⇢(@,3+)We￿rst instruct the LLM (Table 2) to perform the task of queryreformulation in the context of the given document. We then pro-vide the ambiguous query (@) concatenated with the correspondingrelevant document as a part of the prompt. Thiscontext-awarepromptingof LLMs results in better rewrites without topic drifts, asthe LLM is conditioned on intents conveyed in the document. Wealso introduce a constraint on the maximum length of the outputsequence generated. This constraint coupled with grounding of the4465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522The Surprising E￿ectiveness of Training Time￿ery Rewriting using LLMs ICTIR ’23, July 23–27, 2023, Taipei, Taiwan523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580Algorithm 1:Query Rewriting, Fine-tuning and Ranking//Training PhaseInput:training batch⇡Output:training batch with rewritten queries⇡01⇡0 empty list2foreach(@,3+)in⇡do//create rewritten query3@⇤ '⇢(@,3+)4append(@⇤,3+)to⇡05Fine-tune ranker on⇡06end7return⇡0//Inference PhaseInput:test set⇡COutput:Ranked documents8foreach(@C,3)in⇡Cdo9ˆ~ (@C,3)10endgeneration on the relevant document context through promptingto prevent hallucination and topic drift.Note that the intent conveyed for the rewrite can be controlled byusing a di￿erent document context in the prompt. We experimentwith di￿erent prompting approaches and by varying the documentcontext. We discuss the di￿erent variations of few-shot promptingand the corresponding results in Section 4.2 and Section 5 respec-tively.3.1.1 Hyperparameters.We use a temperature of 0.5 to balanceexploration and deterministic generation. We set presence penaltyand frequency penalty to 0.6 and 0.8 respectively to minimize re-dundancy in generated reformulations of the original query.The resulting rewritten query is used as an input to￿ne-tunethe ranker. Note that we only rely on LLMs to rewrite ambiguousqueries for￿ne-tuning the ranker. During inference, the rankerdirectly ranks relevant documents for the new queries without theLLM based rewriting component.3.2 Ranking PhaseOur goal is to train a model forre-rankingdocuments on the queryrewrites for under-speci￿ed queries. Given a query-document pair(@⇤,3)as input, the ranking models output a relevance score. Here,@⇤refers to a rewritten (disambiguate) query. This score can thenbe used to rank documents based on their relevance to the givenquery.Formally, the training set comprises pairs@⇤8,38, where@⇤8is arewritten query using an LLM and38is a relevant or irrelevantdocument to the query. The aim is to￿ne-tune a ranker'thatpredicts a relevance scoreˆ~2[0; 1]given a reformulated query@⇤and a document3:':(@⇤,3)7!ˆ~(1)The￿ne-tuned ranking model (') can be employed to re-rank aset of documents obtained from a￿rst-stage lightweight frequency-based retriever. Recent studies have indicated that pre-trained lan-guage models which jointly model queries and documents tasks [11,29,35] have demonstrated signi￿cant performance on ranking tasks.In this work, we employ theBERT[13] model for ranking. The inputto the ranker is of format:[CLS]@[SEP]3[SEP].(2)We employ the pointwise loss to train the ranker. Let us assume ofa mini batch of#training examples{G8,~8}8=1,...,#. The rankingtask is cast as a binary classi￿cation problem, where each traininginstanceG8=(@⇤8,38)is a query-document pair and~820,1isa relevance label. The predicted score ofG8is denoted asˆ~8. Thecross-entropy loss function is de￿ned as follows:LPoint= 1##’8=1(~8·logˆ~8+(1 ~8)·log(1 ˆ~8))(3)Note that we only￿ne-tune the ranker on query reformulationsof ambiguous or under-speci￿ed queries. At inference time, theranker equipped with the knowledge of user information needs isdirectly deployed to rank relevant documents for new queries asshown inAlgorithm 1.4 EXPERIMENTAL SETUPIn this section, we describe the setup we used to answer the follow-ing research questions:RQ1: Can we employ LLMs to generate￿uent natural languagerewrites of original queries from ambiguous queries?RQ2: How e￿ective is a ranker￿ne-tuned on rewritten queries fordownstream document ranking task?Towards answering these research questions we employ thefollowing datasets, rankers and training settings4.1 DatasetsOur goal is to have natural language expansions of the query fordownstream ranking. Since there are no existing datasets explicitlytackling this problem, we curate queries and their descriptions fromseveral sources, as detailed below.4.1.1 Datasets for checking e￿ectiveness of the LLM-based re-writer.To tackleRQ1, we collect several datasets fromT￿￿￿web track[9]. Particularly, we collect topics and corresponding descriptionsfromT￿￿￿web tracks from 2009 to 2011 to be used as dataset for￿ne-tuning the generative model. We sourced the(topic, topicdescription)pairs to train a generative model. We obtain 1143samples for training, 126 samples for validation. Finally, we use103 samples of theT￿￿￿Web 2012 topics, subtopics, and theirdescriptions for testing the rewriter.4.1.2 Google People Also Asked (PAA) Data.Since the TREC top-ics/queries do not disambiguate the multiple intents a query couldhave, we propose to collect questions from the Googlepeople alsoask (PAA)section as external knowledge for each query. These textsfrom Google PAA could serve as expanded versions of the ambigu-ous query that convey di￿erent intents. For instance, for a topic403b, the proposed pipeline provides di￿erent questions like“Whatare the withdrawal limitations for a 403b retirementplan?”and“What is the difference between a 401k and403b?”. We augment the query topics in the dataset with the cor-responding Google PAA questions. We￿ne-tune the rewriters like5Modiﬁed query-doc pairsOriginal query-doc pairsContext-aware  query rewriting
Fine tuned rankerQuery  time465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522The Surprising E￿ectiveness of Training Time￿ery Rewriting using LLMs ICTIR ’23, July 23–27, 2023, Taipei, Taiwan523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580Algorithm 1:Query Rewriting, Fine-tuning and Ranking//Training PhaseInput:training batch⇡Output:training batch with rewritten queries⇡01⇡0 empty list2foreach(@,3+)in⇡do//create rewritten query3@⇤ '⇢(@,3+)4append(@⇤,3+)to⇡05Fine-tune ranker on⇡06end7return⇡0//Inference PhaseInput:test set⇡COutput:Ranked documents8foreach(@C,3)in⇡Cdo9ˆ~ (@C,3)10endgeneration on the relevant document context through promptingto prevent hallucination and topic drift.Note that the intent conveyed for the rewrite can be controlled byusing a di￿erent document context in the prompt. We experimentwith di￿erent prompting approaches and by varying the documentcontext. We discuss the di￿erent variations of few-shot promptingand the corresponding results in Section 4.2 and Section 5 respec-tively.3.1.1 Hyperparameters.We use a temperature of 0.5 to balanceexploration and deterministic generation. We set presence penaltyand frequency penalty to 0.6 and 0.8 respectively to minimize re-dundancy in generated reformulations of the original query.The resulting rewritten query is used as an input to￿ne-tunethe ranker. Note that we only rely on LLMs to rewrite ambiguousqueries for￿ne-tuning the ranker. During inference, the rankerdirectly ranks relevant documents for the new queries without theLLM based rewriting component.3.2 Ranking PhaseOur goal is to train a model forre-rankingdocuments on the queryrewrites for under-speci￿ed queries. Given a query-document pair(@⇤,3)as input, the ranking models output a relevance score. Here,@⇤refers to a rewritten (disambiguate) query. This score can thenbe used to rank documents based on their relevance to the givenquery.Formally, the training set comprises pairs@⇤8,38, where@⇤8is arewritten query using an LLM and38is a relevant or irrelevantdocument to the query. The aim is to￿ne-tune a ranker'thatpredicts a relevance scoreˆ~2[0; 1]given a reformulated query@⇤and a document3:':(@⇤,3)7!ˆ~(1)The￿ne-tuned ranking model (') can be employed to re-rank aset of documents obtained from a￿rst-stage lightweight frequency-based retriever. Recent studies have indicated that pre-trained lan-guage models which jointly model queries and documents tasks [11,29,35] have demonstrated signi￿cant performance on ranking tasks.In this work, we employ theBERT[13] model for ranking. The inputto the ranker is of format:[CLS]@[SEP]3[SEP].(2)We employ the pointwise loss to train the ranker. Let us assume ofa mini batch of#training examples{G8,~8}8=1,...,#. The rankingtask is cast as a binary classi￿cation problem, where each traininginstanceG8=(@⇤8,38)is a query-document pair and~820,1isa relevance label. The predicted score ofG8is denoted asˆ~8. Thecross-entropy loss function is de￿ned as follows:LPoint= 1##’8=1(~8·logˆ~8+(1 ~8)·log(1 ˆ~8))(3)Note that we only￿ne-tune the ranker on query reformulationsof ambiguous or under-speci￿ed queries. At inference time, theranker equipped with the knowledge of user information needs isdirectly deployed to rank relevant documents for new queries asshown inAlgorithm 1.4 EXPERIMENTAL SETUPIn this section, we describe the setup we used to answer the follow-ing research questions:RQ1: Can we employ LLMs to generate￿uent natural languagerewrites of original queries from ambiguous queries?RQ2: How e￿ective is a ranker￿ne-tuned on rewritten queries fordownstream document ranking task?Towards answering these research questions we employ thefollowing datasets, rankers and training settings4.1 DatasetsOur goal is to have natural language expansions of the query fordownstream ranking. Since there are no existing datasets explicitlytackling this problem, we curate queries and their descriptions fromseveral sources, as detailed below.4.1.1 Datasets for checking e￿ectiveness of the LLM-based re-writer.To tackleRQ1, we collect several datasets fromT￿￿￿web track[9]. Particularly, we collect topics and corresponding descriptionsfromT￿￿￿web tracks from 2009 to 2011 to be used as dataset for￿ne-tuning the generative model. We sourced the(topic, topicdescription)pairs to train a generative model. We obtain 1143samples for training, 126 samples for validation. Finally, we use103 samples of theT￿￿￿Web 2012 topics, subtopics, and theirdescriptions for testing the rewriter.4.1.2 Google People Also Asked (PAA) Data.Since the TREC top-ics/queries do not disambiguate the multiple intents a query couldhave, we propose to collect questions from the Googlepeople alsoask (PAA)section as external knowledge for each query. These textsfrom Google PAA could serve as expanded versions of the ambigu-ous query that convey di￿erent intents. For instance, for a topic403b, the proposed pipeline provides di￿erent questions like“Whatare the withdrawal limitations for a 403b retirementplan?”and“What is the difference between a 401k and403b?”. We augment the query topics in the dataset with the cor-responding Google PAA questions. We￿ne-tune the rewriters like5q“deﬁne cli”d1 > d2 > d3 ..Ranking 
Figure 1: Overview of the proposed CAR framework of contextual re-writing queries
[3]. They do not focus on disambiguation of user intent in the am-
biguous queries. They also do not consider performance prediction
on downstream retrieval and is also susceptible to exposure bias.
To enhance the performance on downstream retrieval, DRQR [ 46]
leverages deep reinforcement learning to train recurrent models
by incorporating query performance predictors as reward signals.
However, the authors mention that the proposed approach does
not improve downstream retrieval by a significant margin.
More recently, natural language question generation approaches
have been adopted for query reformulation. [ 35] introduced a model
to generate clarifying questions to compensate for the missing in-
formation. They used a reinforcement learning model to maximize
a utility function based on the value added by the potential response
to the question. [ 43], on the other hand, focused on identifying un-
clear posts in a community question answering setting that require
further clarification. Query rewriting approaches have been of huge
interest in the e-commerce domain and mostly require relevance
feedback from the user for personalization [ 24,26,59]. While these
methods are domain-specific, [ 52] propose a template-based weak
supervised learning approach for open domain IR.
Document expansion approaches have also been proposed to
tackle the vocabulary mismatch issue in IR. Doc2query [ 34] and
docTTTTquery [ 8], generates pseudo queries from document con-
text using a seq2seq model and appends them to documents to
enhance the ranking results. Works like InPars [ 5] and PromptAga-
tor [12] focus on using large generative models to generate queries
from sampled documents to increase training data for dense re-
trieval tasks. However, these approaches are prone to hallucination
and may drift away from the original intent of the query [ 17]. In this
paper, we take a step in the direction of rewriting ambiguous queries
by incorporating relevant context to transfer the knowledge of the
disambiguation mechanism to a ranker for improved re-ranker
performance. Our approach also departs from the traditional ap-
proaches by employing a monolithic LLM for query rewriting than
multiple components that could lead to error propagation.
2.2 Large Language Models
Recent advances in generative modeling have led to large language
models that apply to a wide range of tasks [ 1,6,47]. These modelstrained in a self-supervised fashion demonstrate emergent capa-
bilities where they can extrapolate to new tasks with few demon-
stration samples [ 14,27,48]. These advances have also led to re-
searchers rethinking parts of retrieval systems [ 57]. Generative
retrieval is one such emerging paradigm [ 4,23,30,42] where neu-
ral language models are used as indices for retrieval. The model
employs conditional decoding approaches to generate document
identifiers that directly map to ground truth documents. This is
accomplished by training the language models on relevant data.
More recently, researchers have tested the effectiveness of LLMs
for document expansion [ 16,44] in a zero-shot and few-shot set-
ting for dense retrieval. However, these approaches have serious
limitations at inference time in terms of efficiency due to the au-
toregressive decoding of LLMs. These approaches are also prone
to hallucination due to the generation of long context for ranking.
Query rewriting using LLMs has also found interest in QA tasks
with the new rewrite-retrieve-read paradigm [ 29]. In this work, the
authors fine-tune a small LLM for query rewriting by employing
reward signals from the reader. However, this approach is specific
to QA tasks and the retriever is a frozen model, relying on the reader
for fine-tuning the rewriter. The approach also requires the expen-
sive query rewriting mechanism during inference.Researchers have
also explored task-aware tuning of dense retrievers [ 2] by append-
ing queries with task-specific instructions as input to the dense
encoder. The instructions equip the retriever with the ability to de-
cipher user’s information needs. However, the instructions require
manual annotation and are limited to specific tasks. In this paper,
we make the ranking model aware of disambiguation mechanism
of under-specified queries by fine-tuning it on rewritten queries
generated by a LLM.
3 METHOD
In this section, we describe the proposed Context Aware Rewriter
(CAR ) framework for ambiguous query reformulation. Figure 1
shows the working of the proposed framework during the training
and inference stages. During training, the framework is composed of
a query reformulation phase and a document ranking phase. In the
query reformulation phase, we employ a context-aware prompting
of a LLM (Section 3.1). During inference, the ranker which was
fine-tuned on disambiguated queries is directly employed to rank

--- PAGE 5 ---
Context Aware Query Rewriting for Text Rankers using LLM
Prompt: Being a ranking model your ﬁrst task is to do query expansion. This means that a query and a document expand the query so that it is relevant to the document. Expand and contextualize the query as best as you can in one or two short sentences. Query: deﬁne ‘sri’ Document/Context:   Rate this deﬁnition: Sri, also transliterated as Sree or Shri or Shree is a word of Sanskrit origin, used in the Indian subcontinent as polite form of address equivalent to the English Mr. in written and spoken language, or as a title of veneration for deities. Generated text from LLM: What is the meaning of the Sanskrit word "Sri" and how is it used in Indian subcontinent?
Figure 2: An example of prompting LLM for query rewriting
using CAR .
documents for new queries without rewriting them, as shown in
Figure 1.
3.1 Query Rewriter
Algorithm 1 details the query reformulation process. Given an
ambiguous query 𝑞, our goal is to generate a rewrite 𝑞∗that helps
disambiguate the intent of the original query. We generate query
rewrites by conditioning the LLM ( 𝑅𝐸) through few-shot prompting
on the relevant document for the query 𝑑+to avoid topic drift. An
example of the few-shot prompting employed is shown in Figure 2.
𝑞∗=𝑅𝐸(𝑞,𝑑+)
We first instruct the LLM (Figure 2) to perform the task of query
reformulation in the context of the given document. We then pro-
vide an ambiguous query ( 𝑞) concatenated with the corresponding
relevant document as a part of the prompt. This context-aware
prompting of LLMs results in better rewrites without topic drifts, as
the LLM is conditioned on intents conveyed in the document. We
also introduce a constraint on the maximum length of the output
sequence generated. This constraint coupled with grounding of the
generation on the relevant document context through prompting
to prevent hallucination and topic drift.
Note that the intent conveyed for the rewrite can be controlled by
using a different document context in the prompt. We experiment
with different prompting approaches by varying the document con-
text. We discuss the different variations of few-shot prompting and
the corresponding results in Section 4.2 and Section 5 respectively.
3.1.1 Hyperparameters. We use a temperature of 0.5 to balance
exploration and deterministic generation. We set presence penalty
and frequency penalty to 0.6 and 0.8 respectively to minimize re-
dundancy in generated reformulations of the original query.
The resulting rewritten query is used as an input to fine-tune
the ranker. Note that we only rely on LLMs to rewrite ambiguous
queries for fine-tuning the ranker. During inference, the rankerAlgorithm 1: Query Rewriting, Fine-tuning and Ranking
// Training Phase
Input: training batch 𝐷
Output: training batch with rewritten queries 𝐷′
1𝐷′←empty list
2foreach(𝑞,𝑑+)in𝐷do
// create rewritten query
3𝑞∗←𝑅𝐸(𝑞,𝑑+)
4 append(𝑞∗,𝑑+)to𝐷′
5 Fine-tune ranker on 𝐷′
6end
7return𝐷′
// Inference Phase
Input: test set𝐷𝑡
Output: Ranked documents
8foreach(𝑞𝑡,𝑑)in𝐷𝑡do
9 ˆ𝑦←(𝑞𝑡,𝑑)
10end
directly ranks relevant documents for the new queries without the
LLM based rewriting component.
3.2 Ranking Phase
Our goal is to train a model for re-ranking documents on the query
rewrites for under-specified queries. Given a query-document pair
(𝑞∗,𝑑)as input, the ranking models output a relevance score. This
score can then be used to rank documents based on their relevance
to the given query.
Formally, the training set comprises pairs 𝑞∗
𝑖,𝑑𝑖, where𝑞∗
𝑖is a
rewritten (disambiguated) query using an LLM and 𝑑𝑖is a relevant
or irrelevant document to the query. The aim is to fine-tune a ranker
𝑅that predicts a relevance score ˆ𝑦∈[0; 1]given a reformulated
query𝑞∗and a document 𝑑:
𝑅:(𝑞∗,𝑑)↦→ ˆ𝑦 (1)
The fine-tuned ranking model ( 𝑅) can be employed to re-rank a
set of documents obtained from a first-stage lightweight frequency-
based retriever. Recent studies have indicated that pre-trained lan-
guage models which jointly model queries and documents tasks [ 11,
32,39] have demonstrated significant performance on ranking tasks.
In this work, we employ the BERT [13] model for ranking. The input
to the ranker is of format:
[CLS]𝑞[SEP]𝑑[SEP]. (2)
We employ the pointwise loss to train the ranker. Let us assume of
a mini batch of 𝑁training examples {𝑥𝑖,𝑦𝑖}𝑖=1,...,𝑁. The ranking
task is cast as a binary classification problem, where each training
instance𝑥𝑖=(𝑞∗
𝑖,𝑑𝑖)is a query-document pair and 𝑦𝑖∈0,1is
a relevance label. The predicted score of 𝑥𝑖is denoted as ˆ𝑦𝑖. The
cross-entropy loss function is defined as follows:
LPoint =−1
𝑁𝑁∑︁
𝑖=1(𝑦𝑖·logˆ𝑦𝑖+(1−𝑦𝑖)·log(1−ˆ𝑦𝑖)) (3)
Note that we only fine-tune the ranker on query reformulations
of ambiguous or under-specified queries. At inference time, the

--- PAGE 6 ---
Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand
ranker equipped with the knowledge of user information needs is
directly deployed to rank relevant documents for new queries as
shown in Algorithm 1 .
3.2.1 Passage selector for Documents. : The phenomenon of topic
drift has been identified in the context of utilizing the CAR (Conver-
sational Response Ranking) approach with documents. Topic drift
is characterized by the divergence of a rewritten query’s specificity
when applying the CAR approach to documents. This occurs due
to documents encompassing multiple topics, potentially relating
to one or more aspects of the query. To mitigate this occurrence
oftopic drift , we employ supervised passage selection techniques
(Attention ,Linear ) as proposed in [ 25]. These techniques aid in
selecting the most relevant passage from a document, aligning it
more closely with a given query.
•Linear Selection : This method involves the transforma-
tion of both the query and the passage into their average
embedding representations. These representations are then
processed through a singular feed-forward layer, followed
by a calculation of their similarity through dot product op-
eration. The sentence score, denoted as 𝑠𝑖 𝑗with respect to
the query𝑞, is computed as follows:
score Lin(𝑞,𝑠𝑖 𝑗)=⟨Enc(𝑞),Enc(𝑠𝑖 𝑗)⟩, (4)
where⟨·,·⟩is the dot product.
•Attention selection : The Attention-based selector op-
erates by deriving passage-level representations through
the utilization of the QA-LSTM model [ 41]. Initially, both
the query and the document undergo contextualization by
being subjected to shared bi-directional Long Short-Term
Memory (LSTM) processing of their token embeddings. Sub-
sequently, the query representation ˆ𝑞is derived through
the application of element-wise max-pooling over these
contextualized embeddings.
ˆ𝑞=Max-Pool(Bi-LSTM(𝑞)), (5)
𝑑LSTM=Bi-LSTM(𝑑). (6)
For each hidden representation 𝑑LSTM
𝑖, attention to the
query is computed as
𝑚𝑖=𝑊1ℎ1+𝑊2ˆ𝑞, (7)
ℎ𝑖=𝑑LSTM
𝑖exp(𝑊3tanh(𝑚𝑖)), (8)
where𝑊1,𝑊2and𝑊3are trainable parameters. For 𝑠𝑖 𝑗,
letℎ𝑖 𝑗denote the corresponding attention outputs. The
sentence representation is computed similarly to the query
representation, i.e.,
ˆ𝑠𝑖 𝑗=Max-Pool(ℎ𝑖 𝑗). (9)
The final score of a sentence is computed as the cosine sim-
ilarity of its representation and the query representation:
score Att(𝑞,𝑠𝑖 𝑗)=cos(ˆ𝑞,ˆ𝑠𝑖 𝑗). (10)
4 EXPERIMENTAL SETUP
In this section, we describe the setup we used to answer the follow-
ing research questions:RQ1 : Can we employ LLMs to generate fluent natural language
rewrites of ambiguous and under-specified queries?
RQ2 : How effective is a ranker fine-tuned on rewritten queries
using LLMs for downstream document ranking task?
Towards answering these research questions we employ the
following datasets, rankers and training settings
4.1 Datasets
Our goal is to have natural language expansions of the query for
downstream ranking. Since there are no existing datasets explicitly
tackling this problem, we curate queries and their descriptions from
several sources, as detailed below.
4.1.1 Datasets for checking effectiveness of the LLM-based re-writer.
To tackle RQ1 , we collect several datasets from Trec web track
[9]. Particularly, we collect topics and corresponding descriptions
from Trec web tracks from 2009 to 2011 to be used as dataset for
fine-tuning the generative model. We sourced the (topic, topic
description) pairs to train a generative model. We obtain 1143
samples for training, 126 samples for validation. Finally, we use
103 samples of the Trec Web 2012 topics, subtopics, and their
descriptions for testing the rewriter.
4.1.2 Google People Also Asked (PAA) Data. Since the TREC top-
ics/queries do not disambiguate the multiple intents a query could
have, we propose to collect questions from the Google people also
ask (PAA) section as external knowledge for each query. These texts
from Google PAA could serve as expanded versions of the ambigu-
ous query that convey different intents. For instance, for a topic
403b, the proposed pipeline provides different questions like “What
are the withdrawal limitations for a 403b retirement
plan?” and“What is the difference between a 401k and
403b?” . We augment the query topics in the dataset with the cor-
responding Google PAA questions. We fine-tune the rewriters like
BART andGPT-2 with topics concatenated with the corresponding
Google PAA question. However, on the test set (TREC web 2012)
we leverage only the topics as access to an external knowledge base
cannot be assumed during real-world deployments.
4.1.3 Datasets for Ranking Experiments. :
MS MARCO (Passage dataset) : We consider the passage dataset
from the TREC Deep Learning track (2019) [ 31]. We evaluate our
model on TREC-DL -19 and TREC-DL -20 passage dataset, each con-
taining 200 queries. For training, we select 1200 ambiguous/under-
specified queries from TREC-DL -19 passage training dataset, then
we rewrite these query using our query rewrite models (Subsection
4.2). The ambiguous queries are selected based on heuristics like
query length, acronyms and entities with varied semantics. Each
of these rewrites correspond to one dataset. So we end up with 13
rewrite training dataset and one baseline dataset (original queries).
MS MARCO (Document dataset) : Similar to above passage
dataset, we evaluate on TREC-DL -19 and TREC-DL -20 document
collection. For training we select 1200 ambiguous/under-specified
queries from TREC-DL -19 document training dataset and use ex-
actly the same method as above to create training datasets using
different approaches.

--- PAGE 7 ---
Context Aware Query Rewriting for Text Rankers using LLM
Model # parameters
text-ada-001 (Ada-001 ) 350M
text-babbage-001 (Babbage-001 ) 3B
text-curie-001 (Curie-001 ) 13B
text-davinci-002 (Davinci-002 ) 175B
text-davinci-003 (Davinci-003 ) 175B
gpt-3.5-turbo (ChatGPT ) 154B
Table 2: LLM (GPT3 models) of different parameter scales
4.2 Rewriter Models
We employ several generative models as the backbone for query
rewriting to compare and contrast the quality of query rewrites.
We employ two settings where the rewrites are smaller language
models fine-tuned on TREC web dataset for generating rewrites or
are LLMs which are prompted to generate rewrites.
GPT-2 .We fine-tune GPT-2 a transformer decoder based gener-
ative model with 117M parameters and 12 layers to generate topic
descriptions from topics on the TREC web dataset. We use a learn-
ing rate of 3.2e-5, weight decay of 0.01, batch size of 16 and train
the model for 6 epochs.
BART .We employ BART (base version) a denoising autoencoder
pre-trained using the objective for reconstructing the corrupted
text. As a result, BART is able to generate robust fluent natural
language queries from under-specified and ambiguous queries. We
fine-tune BART to generate topic descriptions from given input
topics on TREC web dataset. We use a learning rate of 2e-5, batch
size of 16 and train the model for 8 epochs
BART (topic+PAA) .We also propose a variation BART (topic+PAA)
where we concatenate the topics with external knowledge in the
form of related Google PAA questions (c.f. Section 4.1.2) for fine-
tuning the BART base model on the TREC web dataset collected.
However, when generating rewrites for new ambiguous queries in
the MS MARCO dataset, using the fine-tuned model, we provide
only topics as input. We propose this approach to test if augmen-
tation of Google PAA questions as context to topic names during
fine-tuning aid in disambiguation of the user information need
for smaller models. This variant also categorizes to context-aware
rewriting class of approaches CAR proposed in this work, where
Google PAA acts as context. We use similar hyperparameters for
BART as discussed. This is slightly different from the proposed
prompting based CAR approaches and is proposed as an alternative
to test the capabilities of smaller models.
Davinci-003 (prompting) .Since prompting LLMs like GPT-
3have proven to yield more stable outputs with less factual er-
rors, we employ two variants of prompts to Davinci-003 . We
feed the prompts “Generate short sentence expanding:” or
“Generate short sentence question:” followed by the topic
name to Davinci-003 . We call these two variants Davinci-003
(prompt1) and Davinci-003 (prompt2) respectively. We use a tem-
perature of 0.5, max token length of 35 to generate short natural
language rewrites of the original query. For frequency penalty and
presence penalty, we use values of 0.8 and 0.6 to avoid redundancy
in generated outputs.Davinci-003 + in-context examples .Since, plain prompting
might result in topic drifts of generated text and the model might
misinterpret the intended task, we also adopt in-context learning
[27]. In-context learning treats the LM as a black-box and instructs
the model of the task without gradient descent through examples.
We provide examples in form of
<query1,desc1>,<query2,desc2>...querytest,[insert]
and instruct the model to fill the description in the placeholder pro-
vided. We use the same hyperparameters as discussed for Davinci-
003(prompting).
Other GPT-3 models .We also test with models which are vari-
ants of GPT-3 of different parameter scales. These models include
Ada-001 ,Davinci-002 ,Curie-001 andBabbage-001 . The models
and the number of parameters are shown in Table 2. We employ in-
context learning by providing demonstration samples. The prompt
is as follows:
Generate short sentence as expansion for the given test query like the
following examples,
<input :query1,output :desc1>,...input :querytest,output :
Note that the prompt is a bit different from Davinci-003 as we
observed that smaller models need detailed instructions for better
generation capabilities. This maybe due to the difference in instruc-
tion fine-tuning approaches. We employ the same hyperparameters,
queries and their descriptions as discussed earlier.
ChatGPT + in-context examples .We also test the in-context
learning capabilities of gpt-3.5-turbo ( ChatGPT ) for query refor-
mulation. We employ the same prompt as Davinci-003 . We only
prepend the task instruction to set the role of the system. The task
description is :" You are a system that gives an expansion for queries,
expanding abbreviations and acronyms when applicable. Some exam-
ples are " followed by the demonstration samples as discussed earlier.
We employ the same values for hyperparameters as discussed for
other LLM prompting approaches.
Query2Doc : We also use the recently proposed document ex-
pansion approach, Query2Doc which generates pseudo documents
to aid in document ranking, as a baseline. We use the gpt-3.5-turbo
model with max tokens of 128 for generation and follow the original
hyperparameters used in the work [44] for reproducibility.
4.3 Ranking Models
For experiments on TREC-DL we use BERT -base [ 13], a pre-trained
contextual model based on the transformer architecture. In princi-
ple, one can use different transformer architectures but focus on
BERT as a representative model in our experiments. We use the
base version with 12 encoder layers, 12 attention heads and 768-
dimensional output representations. The input length is restricted
to a maximum of 512 tokens. We use cross-attention BERT archi-
tecture for ranking, sometimes also referred to as MonoBert . The
baseline model is trained on the original set of ambiguous queries
using a pointwise ranking loss objective. The improved ranking
models are also BERT base models but trained on re-written queries
using LLMs such as GPT-3 variants, BART ,ChatGPT , etc. In the
experiments, we refer to these improved ranking models after the

--- PAGE 8 ---
Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand
MethodBERTScoreRouge-L
P R F1
GPT2 (topic)†0.457 0.447 0.451 7.47
BART (topic)†0.792 0.791 0.792 28.89
Query2Doc 0.672 0.749 0.708 6.76
Ada-001 (in-context) 0.734 0.776 0.754 17.43
Babbage-001 (in-context) 0.804 0.802 0.802 32.16
Curie-001 (in-context) 0.812 0.800 0.805 34.25
Davinci-002 (in-context) 0.818 0.823 0.820 33.81
Davinci-003 (prompt1) 0.727 0.743 0.734 14.38
Davinci-003 (prompt2) 0.811 0.792 0.801 32.07
Davinci-003 (in-context) 0.817 0.818 0.817 34.68
ChatGPT 0.768 0.812 0.789 25.77
CAR
BART (topic+PAA)†0.777 0.783 0.779 25.85
Davinci-003 0.821 0.834 0.827 38.57
ChatGPT 0.825 0.847 0.836 40.51
Table 3: BERTScore and ROUGEL scores for different rewrites
on TREC web 2012. The best results for each dataset and each
model is in bold and second is underlined .†indicates that the
model has been fine-tuned with topic or topic + PAA data.
LLM query re-writer model that used to generate the training data
for them.
4.4 Metrics
To evaluate the quality of re-writes we use the TREC topic descrip-
tions as targets and compute ROUGE-L [ 38] scores and BERTScore [54–
56] which are commonly used automated evaluation metrics used in
text several generation tasks. While ROUGE-L is an n-gram overlap
metric, BERTScore correlates with human judgments by employ-
ing contextualized embeddings to compute word-level matches
between the reference sentence and the generated sentence [54].
The metrics serve as a proxy to measure the ability of the genera-
tive models to expand the query to disambiguate various intents to
aid in downstream retrieval tasks. They also measure if the queries
are plausible. To evaluate the ranking approach, we employ stan-
dard ranking metrics such as MRR and nDCG@10 and evaluate on
TREC-DL -19 and TREC-DL -20 test sets for passage and document
collection.
5 RESULTS
We begin by first answering if the re-writer component is able to
generate plausible natural language expansions of queries. Then
we conclude by analyzing the impact of the rewritten queries on
the document ranking performance.
5.1 Query Rewriter Evaluation
To answer RQ1 , we evaluate the proposed rewriting approaches us-
ing metrics like BERTScore and ROUGEL. The results are reported
in Table 3. We report the mean values of precision, recall and F1
scores across all the test samples from TREC web 2012 to report the
final scores. We observe that, CAR variants, specifically, Davinci-
003andChatGPT approaches with context-aware prompting havethe highest BERTScores and ROUGE-L scores. We attribute this
performance to the scale of the GPT-3 language model, instruc-
tion fine-tuning and the quality of context provided that enable
the rewriter to generate natural language queries. We observe that
CAR provides more fluent and relevant rewrites when compared
to vanilla prompt based methods. The in-context learning based
approaches also yield fluent rewrites. However, on analysis, we ob-
serve that they sometimes have topic drifts due to lack of relevant
context needed to disambiguate the queries. We also observe that
LLMs of smaller scales like Ada-001 ,Babbage-001 andCurie-001
generate rewrites which are factually inconsistent and drift away
from the relevant topic. This is evident from the downstream docu-
ment ranking performance(Table 4) and the qualitative analysis of
samples(Table 1). This demonstrates that CAR rewrites are consis-
tent, fluent and help improve downstream ranking performance.
Among the fine-tuned approaches in Table 3, BART (topic), BART
(topic + PAA) and GPT-2 (topic), we observe that BART (topic)
generates fluent query rewrites when compared to, GPT-2 (topic),
as evident from BERTScore and ROUGEL scores. After performing a
manual analysis of samples, and we observed that the query rewrites
generated by GPT-2 were not relevant due to hallucination and in
certain cases they were also grammatically incorrect. Therefore,
we omit the GPT-2 (topic) model for the rest of the experiments.
The CAR (BART (topic + PAA)) further improves the fluency of
generated queries, but still falls short compared to GPT-3 variants
andChatGPT approaches, even though they are not fine-tuned.
Since the metrics proposed above are not true indicators of qual-
ity, we assess their quality through their impact on downstream
ranking performance.
Insight 1 : LLMs can produce good rewrites for under-specified and
ambiguous queries. Among our methods CAR with context-aware
few-shot prompting has the best rewrites.
5.2 Ranking Evaluation
To answer RQ2 we first train rankers on data from MS MARCO
passage dataset (Section 4.3) with rewrites from different LLMs
(Section 4.2), and evaluate on TREC-DL -19 and TREC-DL -20 test
sets. Then we choose the best rewriter models and train document
ranking models on MS MARCO document dataset and evaluate on
TREC-DL -19 and TREC-DL -20. All results are shown in Table 4.
In case of passage dataset, CAR models perform better than their
counterparts with ChatGPT CAR showing improvements of 30%
and33% on nDCG@10 and 22% and25% on MRR over baseline
forTREC-DL -19 and TREC-DL -20 respectively. Apart from CAR ,
Davinci-003 in-context and ChatGPT in-context model outper-
form the baseline across test sets.
The poor performance of models such as Ada-001 ,Babbage-001 ,
etc are evident from our observation that they are not able to dis-
ambiguate the query, but rather just rewrite it. For example, the
query “define sri” is generally rewritten as “Definition of sri” while
Davinci-003 andChatGPT are able to convey the intent better
as shown in Table 1. Our CAR models perform better than non
CAR models, proving that localized context is important along with
global knowledge. We also observe that BART base model performs
the worst among all models but variation of BART , with context

--- PAGE 9 ---
Context Aware Query Rewriting for Text Rankers using LLM
TREC-DL Passage TREC-DL Document
TREC-DL -19 TREC-DL -20 TREC-DL -19 TREC-DL -20
Ranking Models RR nDCG 10 RR nDCG 10 RR nDCG 10 RR nDCG 10
Baseline
BERT Baseline 0.653 0.381 0.523 0.288 0.750 0.453 0.735 0.393
Query2Doc [44] 0.578 (▼11 .6%)#0.321 (▼15 .7%)0.608 (▲16 .2%)0.323 (▲12 .3%) 0.82 (▲9.7%) 0.467 (▲3.1%)∗0.749 (▲2%) 0.415 (▲5.6%)∗
Query expansion
BART (topic) 0.500 (▼23 .5%)∗0.254 (▼33 .3%)∗0.380 (▼27 .3%)∗0.202 (▼29 .9%)∗ - - - -
Ada-001 0.634 (▼2.9%)0.370 (▼2.9%)0.479 (▼8.4%)0.278 (▼3.6%) - - - -
Babbage-001 0.697 (▲6.7%)0.370 (▼3.0%)0.580 (▲10 .9%)0.327 (▲13 .5%) - - - -
Curie-001 0.563 (▼13 .9%)0.328 (▼13 .9%)0.498 (▼4.8%)0.248 (▼13 .8%) - - - -
Davinci-002 0.795 (▲21 .7%)∗0.391 (▲2.6%)0.507 (▼3.1%)0.284 (▼1.3%) - - - -
Davinci-003 (prompt1) 0.714 (▲9.4%)0.386 (▲1.3%)0.493 (▼5.7%)0.289 (▲0.4%) - - - -
Davinci-003 (prompt2) 0.717 (▲9.9%)0.397 (▲4.1%)0.574 (▲9.7%)0.338 (▲17 .3%) - - - -
Davinci-003 0.766 (▲17 .3%)#0.411 (▲7.9%)0.552 (▲5.6%)0.308 (▲7.1%) 0.818 (▲9.1%)0.490 (▲8.2%)#0.764 (▲4.0%)0.465 (▲18 .5%)#
ChatGPT 0.689 (▲5.5%)0.396 (▲4.0%)0.623 (▲19 .2%)#0.348 (▲20 .7%)# - - - -
CAR
BART (topic+PAA) 0.745 (▲14 .2%)0.391 (▲2.6%)0.614 (▲17 .4%)∗0.363 (▲26 .0%) 0.762 (▲1.6%)0.504 (▲11 .3%)#0.819 (▲11 .5%)0.485 (▲23 .4%)#
Davinci-003 0.798 (▲22 .2%)∗0.417 (▲9.5%)0.533 (▲2.0%)0.316 (▲9.9%) 0.774 (▲3.3%)0.507 (▲12 .0%)∗0.777 (▲5.8%)0.447 (▲13 .7%)∗
ChatGPT 0.794 (▲21 .6%)∗0.494 (▲29 .7%)∗0.653 (▲24 .8%)∗0.383 (▲33 .1%)∗0.769 (▲2.6%)0.509 (▲12 .3%)#0.7826 (▲6.5%)0.430 (▲9.4%)∗
Davinci-003 (Attention ) - - - - 0.81 (▲8.1%) 0.517 (▲14 .1%)∗0.785 (▲6.9%)0.459 (▲17%)#
Davinci-003 (Linear ) - - - - 0.782 (▲4.3%)0.497 (▲9.7%)∗0.848 (▲15 .5%)0.504 (▲28 .4%)∗
ChatGPT (Attention ) - - - - 0.759 (▲1.3%)0.465 (▲2.6%)0.773 (▲5.3%)0.417 (▲6.2%)∗
ChatGPT (Linear ) - - - - 0.778 (▲3.8%)0.517 (▲14 .1%)∗0.799 (▲8.8%)0.461 (▲17 .5%)∗
Table 4: Ranking performance of BERT model trained using query expansions from different query generative models on
TREC-DL ’19 and TREC-DL ’20. We show the relative improvement of our approaches against a baseline (fine-tuned on original
queries without query expansion) in parentheses. Statistically significant improvements at a level of 95%and 90%are indicated
by∗and #respectively [15]. The best results for each dataset and each model is in bold and second is underlined .
awareness BART (topic+PAA) does better than baseline model in all
evaluations. We posit that the external knowledge provided in the
form of Google PAA questions when fine-tuning BART (topic+PAA)
helps disambiguate the intent of the query. For instance, the query
"403b" in TREC web track 2012 is ambiguous without context. How-
ever, one of the corresponding Google PAA questions, "What are the
withdrawal limitations of a 430b retirement plan?" helps indicate
that the query refers to a retirement plan. We posit that the BART
(topic+PAA) approach encodes this knowledge through fine-tuning.
Hence, it has better document ranking performance compared to
other approaches. This proves that context matters when it comes
to rewriting ambiguous queries.
We also test our CAR approach on MS MARCO document collec-
tion. We consider the baselines and the best-performing approaches
on the passage dataset to test on MS MARCO document collection.
Table 4 shows the performance of different rankers trained using
ambiguous queries from MS MARCO document collection on TREC-
DL-19 and TREC-DL -20. In the case of TREC-DL -19ChatGPT CAR
methods perform better than in-context models and for TREC-DL -
20BART (topic+PAA) performs better. CAR models perform about
12% and23% better than baseline model on nDCG@10. In the case
of the passage dataset for a given query and a relevant passage, the
context is specific, whereas this is not true in the case of document
collection. There can be context drift in the case of a document,where a document can have generic contexts, which can result in
LLM generating queries of generic intents using the CAR approach.
In Table 1 we can see examples where in case of CAR rewrites from
different models are more specific compared to their document
counterparts for the same queries.
If we look at model performance using MS MARCO passage
collection (Table 4) and document collection (Table 4) combined,
we can see that CAR approach performs better overall. This is due
to the reason that the LLMs employed to expand ambiguous/ill
formed queries encode world knowledge, which when enriched
with relevant passage/document context helps ground the genera-
tion. The context serves as grounding, which prevents hallucination
and topic drift. In Table 1 we can see how CAR model rewrites are
more specific and factual compared to their non-CAR counterparts.
Insight 2 : The quality of context employed in few-shot prompting
of LLMs is crucial when expanding ambiguous/ill formed queries.
The proposed approach CAR , is able to form concise and relevant
query reformulations based on the given context.
How do we deal with context drift for long documents?
We previously discussed the phenomenon of topic drift within the
scope of document context. To mitigate this phenomenon, we em-
ploy techniques involving supervised selection techniques, namely
Attention andLinear selectors (Subsection 3.2.1), as proposed
in [25]. In both instances, the primary objective of the selector is

--- PAGE 10 ---
Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand
to identify and select the most pertinent passage based on a given
query.
In our methodology, given a query and a relevant document to
the query, we segment the document into passages, each comprising
approximately four sentences. Subsequently, we employ Attention
andLinear selectors to identify the passage that bears the highest
similarity to the given query within the document. This selected
passage, along with the original query, is used to rewrite the query
using LLM and then subsequently use it for model training.
The final four models detailed in Table 4 exemplify the appli-
cation of Attention andLinear selectors on Davinci-003 and
ChatGPT , resulting in improved performance as compared to their
CAR counterparts. Just comparing CAR models, we can clearly see
CAR ChatGPT (Linear ) does 2% and 8% better than CAR Chat-
GPT fornDCG 10onTREC-DL ’19 and TREC-DL ’20 respectively.
We cans see a similar trend in case of Davinci-003 andDavinci-
003(Attention ) models. Empirical observations demonstrate that
models employing the CAR selector exhibit a performance enhance-
ment ranging from 2% to 15% when compared to their CAR -only
counterparts. Hence, proving that if we reduce topic drift for docu-
ments we can improve query rewriting quality, in turn improving
downstream task performance. From Table 4 we can concur that for
query rewriting using documents CAR based models perform the
best, specifically ChatGPT (Linear ) and Davinci-003 (Attention )
are the two best performing models overall.
Insight 3 : Emphasizes that the utilization of Attention andLinear
selectors to mitigate document topic drift not only enhances the
quality of the rewritten query using the CAR methodology, but
also leads to an overall improvement in the ranking model’s per-
formance.
How does performance vary based on LLM of different pa-
rameter scales? : We experiment with models of different parame-
ter scales (Table 2). The results are shown in Table 4. We observe
that as we scale up the rewriter LLM from 350 million parameters to
175 billion or 154 billion parameters, the performance improves sig-
nificantly as observed during evaluation (Table 4). This is because
they are able to characterize the world knowledge better from the
instruction based training regime and the scale of the models [ 49].
We observe that smaller models like Ada-001 ,Babbage-001 and
Curie-001 generate factually inconsistent rewrites of the original
query and rewrites with significant topic drift from the original
query.
Insight 4 : We observe that when we scale up LLMs it results in bet-
ter quality rewrites as they encode more world knowledge. This is
evident from the significant improvement in ranking performance.
5.3 Qualitative Analysis
We analyze examples of ambiguous queries and their corresponding
rewrites obtained from the generative models. Some of these exam-
ples are in Table 1. We observe that the generated queries are close
to actual intents in the CAR (ChatGPT orDavinci-003 ) approach.
For instance, for the query hs worms , the corpus only contains doc-
uments pertaining to the educational institution. But it has multiple
contexts, such as the educational institution orheartworms . This
renders the query and underlying intent ambiguous without beingcontextualized by the documents in the corpus. However, among
the other rewriting approaches, CAR is able to decipher the correct
intent with fine granularity by also providing an expansion for the
query. The advantage of the proposed framework is more evident
from the fourth example urodeum function . This query is also of
ambiguous nature, as it could refer to any anatomy. However, we
observe that CAR with document context-aware prompting is able
to decipher the correct intent. We attribute this to the large scale
pre-training data and the ability of over-parameterized models to
serve as knowledge bases [49].
Insight 5 :CAR with context-aware prompting, is better at gener-
ating plausible rewrites with intents that are relevant to the search
domain and improve downstream ranking.
5.4 Limitations
In our current approach, we choose ambiguous queries based on
heuristics like the query length and specific types of queries like
acronyms, entities with multiple meanings based on context. Though
these heuristics reflect the nature of ambiguous queries, a more prin-
cipled approach would help identify and filter ambiguous queries.
6 CONCLUSION
In this work, we propose a framework for redefining query rewrit-
ing approaches using context-aware prompting of LLMs for improv-
ing document ranking. Our framework reformulates ambiguous
queries to interpretable natural language queries that disambiguate
user information needs. Our experiments demonstrate that gener-
ating plausible rewrites which disambiguate user intent is possible,
which further enhances downstream ranking performance. We
posit that the joint training of the rewriter and ranker in our frame-
work with feedback signals would yield better rewrites and ranking
performance. We also propose several challenges associated with
the development of the framework.

--- PAGE 11 ---
Context Aware Query Rewriting for Text Rankers using LLM
REFERENCES
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana
Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al .
2022. Flamingo: a visual language model for few-shot learning. arXiv preprint
arXiv:2204.14198 (2022).
[2] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian
Riedel, Hannaneh Hajishirzi, and Wen tau Yih. 2022. Task-aware Retrieval with
Instructions. (2022). arXiv:cs.CL/2211.09260
[3]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong
Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir
Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS
MARCO: A Human Generated MAchine Reading COmprehension Dataset. (2018).
arXiv:cs.CL/1611.09268
[4] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen tau Yih, Sebastian
Riedel, and Fabio Petroni. 2022. Autoregressive Search Engines: Generating
Substrings as Document Identifiers. (2022). arXiv:cs.CL/2204.10628
[5]Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.
InPars: Data Augmentation for Information Retrieval using Large Language
Models. arXiv preprint arXiv:2202.05144 (2022).
[6]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al .2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[7] Claudio Carpineto and Giovanni Romano. 2012. A Survey of Automatic Query
Expansion in Information Retrieval. ACM Comput. Surv. 44, 1, Article 1 (jan
2012), 50 pages. https://doi.org/10.1145/2071389.2071390
[8] David R. Cheriton. 2019. From doc2query to docTTTTTquery.
[9]Charles LA Clarke, Nick Craswell, and Ian Soboroff. [n. d.]. Overview of the
TREC 2009 Web Track. ([n. d.]).
[10] Bruce Croft, Donald Metzler, and Trevor Strohman. 2009. Search Engines: In-
formation Retrieval in Practice (1st ed.). Addison-Wesley Publishing Company,
USA.
[11] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with
Contextual Neural Language Modeling. In ACM SIGIR’19 . 985–988.
[12] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,
Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot
dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
CoRR abs/1810.04805 (2018). http://arxiv.org/abs/1810.04805
[14] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu
Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning.
(2023). arXiv:cs.CL/2301.00234
[15] Luke Gallagher. 2019. Pairwise t-test on TREC Run Files. https://github.com/
lgrz/pairwise-ttest/. (2019).
[16] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot
Dense Retrieval without Relevance Labels. (2022). https://doi.org/10.48550/
ARXIV.2212.10496
[17] Mitko Gospodinov, Sean MacAvaney, and Craig Macdonald. 2023. Doc2Query–:
When Less is More. (2023). arXiv:cs.IR/2301.03266
[18] Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, and
Narayan Bhamidipati. 2015. Context-and content-aware embeddings for query
rewriting in sponsored search. In Proceedings of the 38th international ACM SIGIR
conference on research and development in information retrieval . 383–392.
[19] Yunlong He, Jiliang Tang, Hua Ouyang, Changsung Kang, Dawei Yin, and Yi
Chang. 2016. Learning to rewrite queries. In Proceedings of the 25th ACM Inter-
national on Conference on Information and Knowledge Management . 1443–1452.
[20] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Ben-
dersky. 2023. Query Expansion by Prompting Large Language Models. arXiv
preprint arXiv:2305.03653 (2023).
[21] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating
Query Substitutions. In Proceedings of the 15th International Conference on World
Wide Web (WWW ’06) . Association for Computing Machinery, New York, NY,
USA, 387–396. https://doi.org/10.1145/1135777.1135835
[22] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models.
InProceedings of the 24th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’01) . Association for Computing
Machinery, New York, NY, USA, 120–127. https://doi.org/10.1145/383952.383972
[23] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative Multi-
hop Retrieval. (2022). arXiv:cs.IR/2204.13596
[24] Mu-Chu Lee, Bin Gao, and Ruofei Zhang. 2018. Rare Query Expansion Through
Generative Adversarial Networks in Search Advertising. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD ’18) . Association for Computing Machinery, New York, NY, USA,
500–508. https://doi.org/10.1145/3219819.3219850
[25] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2022. Extractive Ex-
planations for Interpretable Text Ranking. ACM Trans. Inf. Syst. (dec 2022).
https://doi.org/10.1145/3576924[26] Sen Li, Fuyu Lv, Taiwei Jin, Guiyang Li, Yukun Zheng, Tao Zhuang, Qingwen
Liu, Xiaoyi Zeng, James Kwok, and Qianli Ma. 2022. Query Rewriting in TaoBao
Search. In Proceedings of the 31st ACM International Conference on Information
and Knowledge Management (CIKM ’22) . Association for Computing Machinery,
New York, NY, USA, 3262–3271. https://doi.org/10.1145/3511808.3557068
[27] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and
Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In
Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on
Knowledge Extraction and Integration for Deep Learning Architectures . Association
for Computational Linguistics, Dublin, Ireland and Online, 100–114. https:
//doi.org/10.18653/v1/2022.deelio-1.10
[28] Yuanhua Lv and ChengXiang Zhai. 2009. A comparative study of methods for
estimating query language models with pseudo feedback. In Proceedings of the
18th ACM conference on Information and knowledge management . 1895–1898.
[29] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.
Query Rewriting for Retrieval-Augmented Large Language Models. (2023).
arXiv:cs.CL/2305.14283
[30] Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search.
ACM SIGIR Forum 55, 1 (jun 2021), 1–27. https://doi.org/10.1145/3476415.3476428
[31] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. In Proceedings of the Workshop on Cognitive
Computation: Integrating neural and symbolic approaches 2016 co-located with the
30th Annual Conference on Neural Information Processing Systems (NIPS 2016),
Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings) , Tarek Richard
Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne (Eds.), Vol. 1773.
CEUR-WS.org. http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf
[32] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.
CoRR abs/1901.04085 (2019). http://arxiv.org/abs/1901.04085
[33] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document
Expansion by Query Prediction. (2019). arXiv:cs.IR/1904.08375
[34] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document
Expansion by Query Prediction. (2019). https://doi.org/10.48550/ARXIV.1904.
08375
[35] Sudha Rao and Hal Daumé III. 2018. Learning to ask good questions: Ranking
clarification questions using neural expected value of perfect information. arXiv
preprint arXiv:1805.04655 (2018).
[36] Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and
Yi Liu. 2007. Statistical Machine Translation for Query Expansion in Answer
Retrieval. In Proceedings of the 45th Annual Meeting of the Association of Compu-
tational Linguistics . Association for Computational Linguistics, Prague, Czech
Republic, 464–471. https://aclanthology.org/P07-1059
[37] J. J. Rocchio. 1971. Relevance feedback in information retrieval. In The Smart
retrieval system - experiments in automatic document processing , G. Salton (Ed.).
Englewood Cliffs, NJ: Prentice-Hall, 313–323.
[38] Lin CY ROUGE. 2004. A package for automatic evaluation of summaries. In
Proceedings of Workshop on Text Summarization of ACL, Spain .
[39] Koustav Rudra and Avishek Anand. 2020. Distant supervision in BERT-based
adhoc document retrieval. In Proceedings of the 29th ACM International Conference
on Information and Knowledge Management . 2197–2200.
[40] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan
Boyd-Graber, and Lijuan Wang. 2022. Prompting GPT-3 To Be Reliable. (2022).
https://doi.org/10.48550/ARXIV.2210.09150
[41] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2016. Improved
Representation Learning for Question Answer Matching. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) . Association for Computational Linguistics, Berlin, Germany,
464–473. https://doi.org/10.18653/v1/P16-1044
[42] Yi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta,
Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al .2022. Transformer memory as a
differentiable search index. arXiv preprint arXiv:2202.06991 (2022).
[43] Jan Trienes and Krisztian Balog. 2019. Identifying unclear questions in com-
munity question answering websites. In Advances in Information Retrieval: 41st
European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14–18,
2019, Proceedings, Part I 41 . Springer, 276–289.
[44] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with
Large Language Models. (2023). arXiv:cs.IR/2303.07678
[45] Xiao Wang, Sean MacAvaney, Craig Macdonald, and Iadh Ounis. 2023. Generative
Query Reformulation for Effective Adhoc Search. arXiv preprint arXiv:2308.00415
(2023).
[46] Xiao Wang, Craig Macdonald, and Iadh Ounis. 2020. Deep Reinforced Query
Reformulation for Information Retrieval. (2020). arXiv:cs.IR/2007.07987
[47] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models
are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[48] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun
Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger lan-
guage models do in-context learning differently. (2023). arXiv:cs.CL/2303.03846

--- PAGE 12 ---
Abhijit Anand, Venktesh V, Vinay Setty, and Avishek Anand
[49] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu,
and Lijuan Wang. 2021. An Empirical Study of GPT-3 for Few-Shot Knowledge-
Based VQA. (2021). https://doi.org/10.48550/ARXIV.2109.05014
[50] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya
Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather
than Retrieve: Large Language Models are Strong Context Generators. (2023).
arXiv:cs.CL/2209.10063
[51] Hamed Zamani and W. Bruce Croft. 2017. Relevance-based Word Embedding.
InProceedings of the 40th International ACM SIGIR Conference on Research and
Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017 ,
Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries, and Ryen W.
White (Eds.). ACM, 505–514. https://doi.org/10.1145/3077136.3080831
[52] Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck.
2020. Generating clarifying questions for information retrieval. In Proceedings of
the web conference 2020 . 418–428.
[53] George Zerveas, Ruochen Zhang, Leila Kim, and Carsten Eickhoff. 2020. Brown
University at TREC Deep Learning 2019. (2020). arXiv:cs.IR/2009.04016[54] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.
2019. BERTScore: Evaluating Text Generation with BERT. (2019). https://doi.
org/10.48550/ARXIV.1904.09675
[55] Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. 2023. Code-
BERTScore: Evaluating Code Generation with Pretrained Models of Code. (2023).
https://doi.org/10.48550/ARXIV.2302.05527
[56] Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, and Wensheng Zhang. 2023. Lever-
aging Summary Guidance on Medical Report Summarization. (2023). https:
//doi.org/10.48550/ARXIV.2302.04001
[57] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong
Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for
Information Retrieval: A Survey. (2023). arXiv:cs.CL/2308.07107
[58] Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical Query Paraphrasing
for Document Retrieval. In COLING 2002: The 19th International Conference on
Computational Linguistics . https://aclanthology.org/C02-1161
[59] Simiao Zuo, Qingyu Yin, Haoming Jiang, Shaohui Xi, Bing Yin, Chao Zhang, and
Tuo Zhao. 2022. Context-Aware Query Rewriting for Improving Users’ Search
Experience on E-commerce Websites. (2022). arXiv:cs.IR/2209.07584
