# 2305.18507.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2305.18507.pdf
# File size: 2165781 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Code Prompting: a Neural Symbolic Method for
Complex Reasoning in Large Language Models
Yi Hu Haotong Yang Zhouchen Lin Muhan Zhang
Institute for Artificial Intelligence, Peking University
huyi2002@stu.pku.edu.cn
{haotongyang, zlin, muhan}@pku.edu.cn
Abstract
Large language models (LLMs) have scaled up to unlock a wide range of complex
reasoning tasks with the aid of various prompting methods. However, current
prompting methods generate natural language intermediate steps to help reason-
ing, which can cause imperfect task reduction and confusion. To mitigate such
limitations, we explore code prompting, a neural symbolic prompting method with
both zero-shot and few-shot versions which triggers code as intermediate steps. We
conduct experiments on 7 widely-used benchmarks involving symbolic reasoning
and arithmetic reasoning. Code prompting generally outperforms chain-of-thought
(CoT) prompting. To further understand the performance and limitations of code
prompting, we perform extensive ablation studies and error analyses, and identify
several exclusive advantages of using symbolic promptings compared to natural
language. We also consider the ensemble of code prompting and CoT prompting
to combine the strengths of both. Finally, we show through experiments how code
annotations and their locations affect code prompting.
1 Introduction
Recent years have seen huge revolutions in the field of Natural Language Processing (NLP) regarding
the shockingly fast development of large language models (LLMs) [Brown et al., 2020, OpenAI,
2023, Touvron et al., 2023, Zhang et al., 2022a, Thoppilan et al., 2022, Chowdhery et al., 2022,
Raffel et al., 2020]. The scaling law [Kaplan et al., 2020] suggests a promising way to enhance a
language model by scaling up the model size. According to Qin et al. [2023], OpenAI [2023], LLMs
have shown certain levels of the ability to solve complex reasoning tasks. Besides the size of LLMs,
“prompt engineering” is crucial to the reasoning ability. Prompting methods like chain-of-thought
prompting [Kojima et al., 2023, Wei et al., 2023] and least-to-most prompting [Zhou et al., 2023] lead
LLMs to generate rationales before the final answer and unlock a wide range of complex reasoning
tasks. However, the model performance is highly dependent on the prompt design. How to craft
simple, effective and general-purpose prompts still remains a hotly-debated open question.
A large number of works have proposed different prompting methods to facilitate LLM reasoning [Wei
et al., 2023, Zhang et al., 2022b, Kojima et al., 2023, Zhou et al., 2023, Fu et al., 2023, Khot et al.,
2023, Press et al., 2022]. These prompting methods induce LLMs to generate natural language
intermediate steps before the final answer and enhance the reasoning ability of LLMs to a great
extent. However, there still remain limitations. First, although these methods lead LLMs to solve the
problems step by step, they fail to plan the whole process of solution ahead, which is likely to result
in unreasonable task reduction. In other words, the “step” can be too big for the LLM to stride over.
For example, in Figure 1, we show a case where the LLM is supposed to concatenate the last letters
of the given words. Chain-of-thought prompting leads the LLM to first extract the last letter of each
word and then concatenate all the letters together in one step . However, the step may be too difficult
for the LLM with the number of letters increasing, resulting in wrong answers. Second, ambiguity
Preprint. Under review.arXiv:2305.18507v2  [cs.CL]  7 Oct 2023

--- PAGE 2 ---
Generate python code to concatenate the 
last letters of the given words.Here's the Python code to concatenate the last letters of 
the given words:
words = [ “apple”, “banana”, “cherry”, “date”, 
“elderberry ”]
result = “”
for word in words:
result += word[ -1]
print(result)
Generate python code to concatenate the 
last letters of the given words.
Here's the Python code to concatenate the 
last letters of the given words:
words = [ “apple”, “banana”, “cherry”, 
“date”, “elderberry ”]
for word in words:
result += word[ -1]
print(result)
Q: “weapon, reasonable, thin, loyalty ”
A:
Let’s think step by step. Print all the 
intermediate variables.words = [ “weapon”, “reasonable ”, “thin”, “loyalty”]
result = “”
for word in words:
last_letter = word[ -1]
result += last_letter
print(f “Word: {word}, Last Letter: {last_letter}, 
Result: {result} ”)
print(result)
Output:
```
Word: weapon, Last Letter: n, Result: n
Word: reasonable, Last Letter: e, Result: ne
Word: thin, Last Letter: n, Result: nen
Word: loyalty, Last Letter: y, Result: neny
```
Therefore, the answer is “neny”.Language 
Model
Language 
ModelCode Prompting
Stage 1
Stage 21. The last letter of “weapon” is “n” .
2. The last letter of “reasonable” is "e".
3. The last letter of “thin” is “n”.
4. The last letter of “loyalty” is “y”.
5. Concatenating these letters together, we get " ney".
Therefore, the answer is “ ney”.Q: Concatenate the last letters of the given 
words: “weapon, reasonable, thin, loyalty”.
A: Let’s think step by step.Language 
ModelChain -of-Thought Prompting
×
√Figure 1: The pipelines of zero-shot CoT prompting and zero-shot code prompting.
naturally lies in natural language. Rationales of natural language may also cause confusion, thus
misguiding the reasoning process.
To mitigate the limitations of the above methods, we consider combining neural methods with
symbolic ones, as symbolic methods have structured, unambiguous representations and computation
flows, which may make the reasoning process more accurate and alleviate the effect of ambiguity.
Specifically, we induce neural methods (LLMs) to first generate symbolic solvers (such as computa-
tion graphs) and let the symbolic solvers to output the final answers . Prior work of neural-symbolic
methods constructs intermediate steps by formal languages instead of natural language [Amini et al.,
2019, Chen et al., 2020a,b, Chiang and Chen, 2019, Roy et al., 2015, He-Yueya et al., 2023]. However,
they are often limited to a specific field and have trouble generalizing to broader reasoning tasks.
To overcome the difficulties, recent works [Gao et al., 2023, Chen et al., 2022, Cheng et al., 2023, Chen
et al., 2023] have tried to facilitate complex reasoning in LLMs with programs/code. As a general-
purpose tool, code is machine-executable, LLM-producible, and can define flexible computation
graphs. We follow the works and further explore code prompting, a neural-symbolic way to enhance
the reasoning ability of LLMs. As the corpus on which LLMs are pre-trained expands to contain code,
LLMs are naturally endowed with the ability to generate code following certain instructions [Chen
et al., 2021]. We expect to leverage the code generation ability of LLMs to promote reasoning. Our
code prompting is a two-stage method with the pipeline shown in Figure 1. To solve a specific
question, code prompting first guides LLMs to generate a piece of code. To fully exploit the reasoning
ability of pure LLMs, we explore two ways of leveraging code prompting. One is to instruct LLMs to
follow the generated code to complete the reasoning process and finally give the answer. The other is
to call an external compiler to execute the code.
We find out in experiments that code works as a mind map for the LLM and reduces the task into
sub-tasks represented by separate operations in the code. As the separate operations in code are often
easy for LLM, the task reduction greatly facilitates LLMs to solve the complex reasoning problems
step by step by themselves. Besides, code is a formal language without ambiguity, which eliminates
2

--- PAGE 3 ---
confusion caused by ambiguity in natural language and makes the reasoning process more robust.
Regarding the limitations of prior neural-symbolic methods, code is a formal language with the
potential to adapt to a wide range of tasks.
We evaluate code prompting on 7 datasets involving symbolic and arithmetic reasoning. We show that
for symbolic reasoning, zero-shot code prompting outperforms zero-shot chain-of-thought prompting
by at least 26.2% (71.0% →97.2%) in the last letter concatenation task and by 11.7% (75.3% →
87.0%) in average in the task of coin flip even without the help of external tools like an Python
interpreter. For arithmetic reasoning, zero-shot code prompting generally outperforms zero-shot
chain-of-thought prompting, and few-shot code prompting is highly competitive to the baselines.
We further dig into the failure cases of code prompting. Through error analysis, we identify several
limitations of the method in addition to some key aspects of its success. Moreover, we find out that
code prompting and chain-of-thought prompting can lead the LLM to think from different angles,
suggesting a combination of both methods, which achieves 87.95% accuracy on GSM8K.
2 Related Work
Prompt engineering. Recently, various prompting methods have emerged to boost LLM reason-
ing [Wei et al., 2023, Zhang et al., 2022b, Kojima et al., 2023, Zhou et al., 2023, Fu et al., 2023,
Khot et al., 2023, Press et al., 2022]. They have shown effectiveness in a wide range of reasoning
tasks. Classified based on whether specific exemplars are provided, prompting methods can be
divided into zero-shot prompting and few-shot prompting. For example, the widely-used technique
chain-of-thought prompting has a zero-shot version [Kojima et al., 2023] and a few-shot version [Wei
et al., 2023]. Few-shot prompting calls for manually constructed task-specific demonstrations, so it
is more costly. Zhang et al. [2022b] introduced a way to automatically construct few-shot demon-
strations based on zero-shot prompting to save the manual cost. In this paper, we introduce both
zero-shot and few-shot code prompting and show the competitive results of both versions regarding
symbolic and arithmetic reasoning. Existing prompting methods can also be divided into single-step
prompting and multi-step prompting methods. CoT prompting [Kojima et al., 2023, Wei et al.,
2023] is a classic single-step prompting method. A recent work [Zhou et al., 2023] has introduced
least-to-most prompting, which is a multi-step prompting method that first divides a question into
several sub-questions and then guides the LLM to solve each question sequentially. Least-to-most
prompting is limited to a few-shot version and requires careful hand-crafting of demonstrations which
vary significantly from task to task. Thus, it is less general than CoT and our code prompting.
Program-aided reasoning. Prior works have shown that programs generated by LLMs may facilitate
reasoning [Gao et al., 2023, Chen et al., 2022, Chowdhery et al., 2022]. A recent work demonstrates a
similar idea of using code to elicit LLM reasoning, which is called Program-aided Language Models
(PAL) [Gao et al., 2023]. We follow the work and dig deeper into code prompting: 1) PAL is a
few-shot prompting method, while we investigate both zero-shot and few-shot code prompting. We
show that zero-shot code prompting matches or even exceeds PAL in arithmetic reasoning tasks.
Besides, we improve the few-shot performance too. 2) We offer another option on the second stage
of final answer generation, namely directly asking LLM to generate the final answer according to the
code, instead of calling a Python interpreter as in Gao et al. [2023]. Our experiments on using LLM
in the second stage further prove that code prompting indeed assists reasoning in LLM even without
executing the code and that symbolic and arithmetic reasoning abilities are self-contained for LLMs.
3) We discuss some key limitations and insights of code prompting through error analysis. These
points help us understand the power of code prompting better and motivate us to combine CoT and
code prompting.
LLMs with external tools. Code prompting has an option to adopt a Python interpreter as an
external tool to assist the LLM to complete the tasks. The concept of augmenting LLMs with external
tools has drawn much attention [Khot et al., 2023, Cheng et al., 2023, Press et al., 2022]. A similar
work [Cheng et al., 2023] also uses programs to assist LLM reasoning. However, they focus on
generating SQL or SQL-like programs to deal with questions of reasoning with tables. Besides, a
recent work [Chen et al., 2023] introduces a method of teaching LLMs to debug their generated
program with the help of feedback from a code interpreter. In our work, we equip code prompting
with a similar technique. However, we consider reasoning tasks while Chen et al. [2023] focuses on
tasks of text-to-code or code-to-code generation.
3

--- PAGE 4 ---
Generate python code to answer the question.
Q: Marie ordered one chicken meal that costs $12, 5 packs of 
milk that costs $3 each, 4 apples that cost $1.50 each, and 
some boxes of pizza. Marie paid a total of $50. How many 
boxes of pizza did Marie order if each box costs $8.50?
Language 
Model
output: 2 chicken_meal =12
milk_packs =3*5
apple=1.5*4
total=chicken_meal +milk_packs +apple+8.5*x
# where x is the number of boxes of pizza
# we need to find the value of x
# total cost is $50
# so, we can write the equation as:
50=chicken_meal +milk_packs +apple+8.5*x
# solving for x
x=(50-chicken_meal -milk_packs -apple) /8.5
print("Marie ordered" , x, "boxes of pizza." )Bug report:
50 = chicken_meal + milk_packs + apple + 8.5 * x
^
SyntaxError: cannot assign to literal
Language 
Model
chicken_meal =12
milk_packs =3*5
apple=1.5*4
# initialize x to 0
x=0
total=chicken_meal +milk_packs +apple+8.5*x
# where x is the number of boxes of pizza
# we need to find the value of x
# total cost is $50
# so, we can write the equation as:
# 50 = chicken_meal + milk_packs + apple + 8.5 * x
# solving for x
x=(50-chicken_meal -milk_packs -apple) /8.5
print("Marie ordered" , int(x), "boxes of pizza." )√Python
InterpreterFigure 2: The pipeline of “self-debugging”.
3 Code Prompting
Code prompting guides LLMs to solve a complex problem in two stages:
1.Code Generation. In the first stage, the prompt asks the LLM to generate Python code to solve
the problem. We test on both zero-shot and few-shot prompting. The zero-shot prompt is simply
“Generate python code to {task description }.”. The few-shot prompt consists of exemplars of
questions followed by corresponding code.
2.Based-on-code Solution. The second stage is for solving the problem on the basis of the code
generated in the first stage. We compare two different methods. The first method is to still ask the
LLM to generate rationales and the solution to the problem according to the code. The second
method is to directly use an external Python interpreter to execute the code.
In Figure 1, we show the pipeline of zero-shot code prompting in the task of last letter concatenation
introduced by Wei et al. [2023], which is a commonly used benchmark for symbolic reasoning [Wei
et al., 2023, Kojima et al., 2023, Zhou et al., 2023, Zhang et al., 2022b]. In the first stage, we use
the zero-shot prompt to guide the LLM to output a piece of code for the task. To be more specific,
we prompt the LLM with the instruction “Generate python code to concatenate the last letters of
the given words.”. In the second stage, the LLM is given the code it generates in the first stage
and the specific question. Additionally, we add the sentence “Let’s think step by step. Print all the
intermediate variables.” to the prompt to further elicit the reasoning ability of the LLM.
As mentioned before, the first stage (code generation) has the option of few-shot prompting instead
of zero-shot prompting, and the second stage (solution) has the option of using an external code
interpreter instead of prompting the LLM. They are not shown in the figure.
Self-debugging If a python interpreter is used in the second stage, we can equip code prompting
with a “self-debugging” module. That is, when the interpreter returns errors (not often, the code is
runnable in most cases), we simply append the generated code (with bugs) and the bug report to
the original model input to prompt the LLM again. The pipeline of the “self-debugging” module is
shown in Figure 2. We show in experiments that state-of-the-art LLMs are capable of learning from
the bug report and fixing the code.
In the following experiments, we show the results of code-prompting for symbolic reasoning and
arithmetic reasoning. We conduct all the experiments on the model gpt-3.5-turbo (ChatGPT) and set
the temperature to 0 unless otherwise specified.
4

--- PAGE 5 ---
4 Symbolic Reasoning
4.1 Tasks
We consider two symbolic reasoning tasks introduced by Wei et al. [2023] and widely used in Wei
et al. [2023], Kojima et al. [2023], Zhou et al. [2023], Zhang et al. [2022b].
Last letter concatenation. The task asks LLMs to concatenate the last letters of given words. We
follow Zhou et al. [2023] to construct word lists by randomly selecting words from five thousand
words of the Wikipedia frequency list. We construct word lists of lengths 4, 8 and 12. For each length,
we test the prompting methods on 500 word lists, which form a test dataset of 1,500 samples.
Coin flip. The task requires LLMs to answer whether a coin is still heads up after several people
flipped or did not flip it. The number of people varies from 3 to 5. For each certain number of people,
we construct 500 questions, which form a test dataset of 1,500 samples.
4.2 Methods
We consider zero-shot standard prompting and zero-shot chain-of-thought prompting as baselines.
For our code prompting methods, we only consider zero-shot code prompting because the code for
each question is basically the same so that the exemplars in few-shot code prompting may leak the
answer. We also show the results of few-shot chain-of-thought prompting for reference.
Zero-shot Standard Prompting. Zero-shot standard prompting has the form of “Q:
{question }\nA:”. This prompting method reflects the symbolic reasoning ability of the LLM
without any prompt engineering.
Zero-shot Chain-of-Thought Prompting. Zero-shot chain of thought prompting (zero-shot CoT
prompting) is introduced by Kojima et al. [2023]. As shown in Figure 1, “Let’s think step by step.” is
appended to the question to elicit the LLM to generate chains of thought leading to the final answer.
Few-shot Chain-of-Thought Prompting. Few-shot chain of thought prompting (few-shot CoT
prompting) is introduced by Wei et al. [2023]. We provide 2 exemplars for the task of last letter
concatenation and 8 exemplars for the task of coin flip. See Appendix B for the demonstrations.
Zero-shot Code Prompting + LLM Self-contained. As is shown in Figure 1, our proposed method
prompts the LLM to first generate task-specific code and then follow the code to generate the final
answer. For zero-shot code prompting + LLM self-contained, we use the LLM itself to perform
reasoning in the second stage. For the task of last letter concatenation, the prompt of the first stage is
“Generate python code to concatenate the last letters of the given words.”; for the task of coin flip, the
first-stage prompt is “A coin is heads up, there are some people, each one flipped or didn’t flip the
coin. Generate python code to determine whether the coin is still heads up.” The prompt we use in
the second stage is “Q: {question }\nA: Let’s think step by step. Print all the intermediate variables.”
Zero-shot Code Prompting + Python Interpreter. The first stage of the method is the same as
zero-shot code prompting + LLM self-contained. In the second stage, we employ a Python interpreter
to execute the code to get the final answer directly in the second stage.
4.3 Results
As shown in the Table 1, even without external tools like Python interpreter, code prompting
outperforms the baseline prompting methods (here we mainly consider zero-shot CoT prompting)
by at least 26.2% in the task of last letter concatenation and by 11.67% on average in the task of
coin flip. Besides, the performance gain increases with the complexity of the questions, implying
that code prompting has better generalization ability in terms of symbolic reasoning. Specifically,
the performance gain grows from 26.2% to 72.8% in the task of last letter concatenation when the
length of word lists increases from 4 to 12, and in the task of coin flip, the performance gain rises
from 0.0% to 18.0% with flip times increasing from 3 to 5. Compared to few-shot CoT prompting,
zero-shot code prompting + Python interpreter generally performs better. Besides, it is noteworthy
that zero-shot code prompting + LLM self-contained achieves better performance in the task of last
letter concatenation without the help of Python interpreter. Even though few-shot CoT prompting
outperforms code prompting + LLM self-contained in the task of coin flip, it should be noticed that
few-shot methods are more costly due to the need for task-specific manual crafting of demonstrations.
5

--- PAGE 6 ---
Table 1: The accuracy (%) of zero-shot standard prompting, zero-shot CoT prompting, few-shot CoT
prompting and zero-shot code prompting on last letter concatenation and coin flip.
last letter concatenation coin flip
l=4 l=8 l=12 #flips=3 #flips=4 #flips=5
zero-shot standard 7.4 2.0 2.6 22.6 17.2 16.8
zero-shot CoT 71.0 27.8 2.8 86.2 71.8 67.8
few-shot CoT 94.6 69.6 39.8 99.8 99.8 99.0
zero-shot code + LLM self-contained 97.2 85.4 75.6 86.2 88.8 85.8
zero-shot code + Python interpreter 99.4 99.8 99.8 99.8 99.8 99.4
We summarize the possible reasons for the outstanding performance of code prompting as follows:
1.Abstraction and Simplification. Code prompting urges the LLM to generate code from the
question, which can be seen as a step of abstraction and simplification. For example, in both
tasks, code prompting endows the LLM the ability to extract the “loop” nature of the question and
leverages the nature explicitly in the code using “for” or “while” syntax in Python language. The
LLM might benefit from the explicit identification of loop and switch to its recursive mode.
2.Task Reduction. Code prompting transforms a question into a program. Each line of the code can
be seen as a sub-task. Sub-tasks that can be represented as a line of code are usually easy for the
LLM to complete, and thus the question transformation actually generates friendly instructions
for LLM reasoning. For example, as shown in Figure 1, in the task of last letter concatenation,
CoT prompting guides the LLM to find the last letter of each word first and then concatenate them
together. However, concatenating all letters in one step may be too difficult for the LLM when the
number of letters is large. Meanwhile, code prompting leads the LLM to store the current result in
astring and reduces the task to steps of extracting the last letter of a word and concatenating
it to the string sequentially. Code prompting triggers a more reasonable task reduction, as the
difficulty of each step is lower and does not grow with the number of words.
3.Disambiguation. The code generated by the LLM can be seen as a formalization of the question
that eliminates the ambiguity in the original natural language questions. In contrast, other
prompting methods like CoT may lead to confusion in the reasoning process. See Appendix C for
examples.
4.Explicit Template. Through case-by-case observation, we discover that code generated in the
first stage works as an explicit template, a mindmap for the LLM to conduct the reasoning process.
LLM follows the code to complete the task. In comparison, CoT prompting fails to provide such
an explicit template before reasoning, which proves in the experiments to be crucial.
5 Arithmetic Reasoning
5.1 Tasks
We consider five commonly used datasets for the task of arithmetic reasoning. (1) SinglEq [Koncel-
Kedziorski et al., 2015], (2) AddSub [Hosseini et al., 2014], (3) MultiArith [Roy and Roth, 2016], (4)
SV AMP [Patel et al., 2021], (5) GSM8K [Cobbe et al., 2021]. SingleEq, AddSub and MultiArith
are from the MAWPS (MAth Word ProblemS) benchmark introduced by Koncel-Kedziorski et al.
[2016]. Among the datasets, SingleEq and AddSub only take single-step calculation, while Multi-
Arith, GSM8K and SV AMP contain harder math problems that require multi-step reasoning. See
Appendix A.2 for more details of each dataset.
5.2 Methods
Zero-shot Chain-of-Thought Prompting. This method is the same as in §4.2.
Zero-shot Code Prompting. In the first stage, the prompt is “Generate python code to answer
the question. \nQ:{question }”. In the second stage, we use an external Python interpreter. We
additionally design several modules to augment code prompting, including self-debugging ,irrelevant
information (irr) and equation instruction (equ). The self-debugging is described in §3. For
irrelevant information, we simply add an instruction “There may be irrelevant information in the
6

--- PAGE 7 ---
Table 2: The accuracy (%) of zero-shot CoT prompting, zero-shot code prompting, few-shot CoT
prompting, PAL Gao et al. [2023] and few-shot code prompting on arithmetic reasoning.
SingleEq AddSub SV AMP MultiArith GSM8K
zero-shot methods
zero-shot CoT 93.11 86.08 78.20 96.67 81.58
zero-shot code (+self-debug) 97.64 91.65 (+irr) 79.40 96.67 78.92(+equ)
few-shot methods
few-shot CoT 95.47 90.63 77.10 98.50 82.11
PAL 97.64 89.11 79.50 97.00 79.45
few-shot code (+self-debug) 97.44 91.39 (+irr) 79.60 97.33 80.59
question. If you find it, ignore it.” to the zero-shot prompt. For equation instruction, we provide
an instruction on solving equations in Python using the package sympy . See Appendix D for more
details.
Few-shot Chain-of-Thought Prompting. We consider few-shot CoT prompting introduced by Wei
et al. [2023] as a baseline. The pipeline is shown in Appendix D. We use 3 of the exemplars introduced
by Wei et al. [2023] for all the math world problems. See Appendix D for the exemplars.
Program-aided Language Models. We also consider the recent work Program-aided Language
Models (PAL) introduced by Gao et al. [2023] as a baseline. PAL is a few-shot prompting method
that uses program as prompts. We run the method on gpt-3.5-turbo using the same 3 exemplars.
Few-shot Code Prompting. In the first stage, we use the same 3 questions as few-shot CoT prompting
to guide the LLM to generate code. We add annotations to raise the readability and further strengthen
the method. In the second stage, we use an external Python interpreter. We also add augmenting
modules, which are basically the same as in zero-shot code prompting. See Appendix D for more
details. We also experiment on using LLMs to execute the code in the second stage, the results are
shown in Appendix D.
5.3 Results
Table 2 presents the results. Note that few-shot code prompting in the table adds annotations at the
end of each line of the code. Zero-shot code prompting outperforms the baselines by 4.53%, 5.57%
and 1.20% in SingleEq, AddSub and SV AMP respectively. Nevertheless, zero-shot code prompting is
1.52% behind zero-shot CoT prompting in GSM8K. Few-shot code prompting generally outperforms
PAL with the aid of annotations, except for a gap of 0.20% in SingleEq. Besides, few-shot code
prompting outperforms few-shot CoT prompting by 1.97% in SingleEq, 0.76% in AddSub and
2.50% in SV AMP. However, code prompting is 1.17% and 1.52% behind few-shot CoT prompting in
MultiArith and GSM8K respectively. We will discuss the errors in §5.4.
It is noteworthy that zero-shot code prompting generates competitive performance to few-shot CoT
prompting and PAL. Compared with few-shot CoT prompting, zero-shot code prompting performs
better in SingleEq, AddSub and SV AMP by 2.17%, 1.02% and 2.30%. Compared with PAL, zero-shot
code prompting performs better in AddSub, and the performance gap in other datasets is no more
than 0.53%. This demonstrates that with proper augmentations, zero-shot code prompting is close to
few-shot performance.
5.4 Error analysis
AddSub. In AddSub, we find that the code generated by the LLM tend to contain irrelevant variables,
thus disturbing it. See Appendix E for examples. The observation suggests code generation by LLMs
is easy to be distracted by irrelevant information. This motivates the irr augmentation.
MultiArith. In MultiArith, the wrong answers from code prompting are mainly due to two reasons:
1) Code prompting is more sensitive to ambiguity in the question. See Appendix E for examples.
(The feature of code prompting is discussed in detail in §6). 2) The groundtruth for some questions is
wrong, while the predictions are correct. Examples are listed in Appendix E.
7

--- PAGE 8 ---
Table 4: Accuracy (%) of methods of few-shot code prompting w/ annotations or w/o annotations
added at different locations of each line of the code. “Self-debugging” is not used here.
method SingleEq AddSub MultiArith SV AMP GSM8K
few-shot code w/o ann 97.64 89.11 97.00 79.50 79.45
few-shot code w/ ann at the end 97.44 89.87 97.33 79.60 80.21
few-shot code w/ ann at the beginning 96.85 89.87 95.33 79.90 77.71
GSM8K. For the dataset GSM8K, we observe that the LLM struggles to solve equations in code. See
error examples in Appendix E and Appendix E. This motivates the “equation instruction”. Besides,
we find out through statistics that the error overlap of code prompting and CoT prompting is very
small, suggesting that the two prompting methods lead LLMs to think from different angles. (See
Appendix E for more detail.)
5.5 Ensemble of CoT and code prompting
method GSM8K
few-shot CoT 81.58
few-shot code 79.68
ensemble
(only CoT votes)87.49
ensemble
(only code votes)83.85
ensemble
(both methods vote)87.95
Table 3: Accuracy (%) of ensemble meth-
ods on GSM8K.Conceptually similar to Wang et al. [2023], we consider
the ensemble of CoT prompting and code prompting
based on voting. For each question, if both prompt-
ing methods generate the same answer, we accept the
answer as the final answer; otherwise, we set the tem-
perature to 0.7 and ask the LLM to generate nan-
swers following each prompting method. Then we
vote among the 2nanswers to give the final answer. We
also compare voting methods whose 2nanswers are
generated from the same prompting method. As shown
in Table 3, ensemble methods outperform the baselines
significantly. Further, the ensemble of two prompting
methods surpasses only code prompting votes or only
CoT prompting votes as we expected. See Appendix D
for experiment details.
5.6 Ablation study
We add annotations in the exemplars of few-shot code prompting. To explore whether and where we
should add the annotations, we conduct the following ablation experiments.
Do annotations matter? We compare the code prompting with and without annotations. As shown
in Table 4, adding annotations generally raises the accuracy. This demonstrates the benefits of adding
annotations for few-shot code prompting. Annotations written in natural language may assist the
LLM to understand the exemplars better and generate better code.
Does where we add annotations matter? We experiment on code prompting with annotations added
either at the beginning or at the end of each line of code. As shown in Table 4, in most cases, adding
annotations at the end is a better choice. We suspect this is due to humans’ habit of writing code. We
tend to add annotations specifically describing each line of code at the end of each line more often
than before the code, thus the training corpus contains more code samples with annotations at the end
of each line.
Do annotations help harder or easier questions? We experiment on GSM8K to find out where
the performance gain of adding annotations comes from. We consider the number of words in the
provided answer as a notion of difficulty. Figure 3 shows the performance gain of adding annotations
to questions with different difficulty. The performance gain on the easiest questions and hardest
questions is larger than that on the questions with medium difficulty.
Self-debugging. Figure 4 shows the accuracy of zero-shot and few-shot code prompting with
or without “self-debugging” on 5 arithmetic datasets. It is clear that the performance on harder
benchmarks is more likely to benefit from “self-debugging”. This is because more complex problems
are more likely to trigger bugs in code. Meanwhile, few bugs are observed in SingleEq and MultiArith.
8

--- PAGE 9 ---
Irrelevant information. Through error analysis of AddSub in §5.4, we show that model perfor-
mance may suffer from irrelevant information in questions. So we propose “irrelevant information”,
introduced in §5.2. Table 5 shows that despite the simplicity, it effectively improves the performance.
Note that code prompting methods in the table are all equipped with “self-debugging”.
0-20 20-40 40-60 60-80 >=80
rationale word num0.50.60.70.80.91.0accuracycode
code + ann
Figure 3: Accuracy of few-shot code
prompting w/ or w/o annotations on
questions with various difficulty.
SinglEq AddSub MultiArith SVAMP GSM8K0.700.750.800.850.900.951.00accuracyzero-shot code
zero-shot code w/ self-debug
few-shot code
few-shot code w/ self-debugFigure 4: Accuracy of zero-shot and few-shot code prompt-
ing w/ or w/o “self-debugging” on arithmetic datasets.
Equation instruction. As mentioned in §5.4, it is challenging for LLMs to solve equations in the
code. So we give an instruction on employing sympy to leverage Python to solve equations (see
Appendix D). Besides, we also experiment on directly asking the LLM to solve equations in the
annotations. Table 6 shows that the best choice is to teach the LLM to use certain Python packages to
solve equations. Note that code prompting methods in the table all have “self-debugging”.
method AddSub
zero-shot code 89.62
zero-shot code +irr 91.65
few-shot code 89.87
few-shot code +irr 91.39
Table 5: Accuracy (%) of code prompting w/
or w/o “irrelevant information” on AddSub.method GSM8K
zero-shot code 77.40
zero-shot code +equ(ann) 78.09
zero-shot code +equ(sympy) 78.92
Table 6: Accuracy (%) of code prompt-
ing w/ or w/o “equation instruction” on
GSM8K.
6 Discussion
code w/ amb code w/o amb CoT w/ amb CoT w/o amb0.02.55.07.510.012.515.0votesanswer 1
answer 2
answer 3
answer 4
code w/ amb code w/o amb CoT w/ amb CoT w/o amb0.02.55.07.510.012.515.0votes
code w/ amb code w/o amb CoT w/ amb CoT w/o amb0.02.55.07.510.012.515.0votes
code w/ amb code w/o amb CoT w/ amb CoT w/o amb0.02.55.07.510.012.515.0votes
code w/ amb code w/o amb CoT w/ amb CoT w/o amb0.02.55.07.510.012.515.0votes
Figure 5: Answer distribution on questions
w/ and w/o ambiguity.Disambiguation. Following the thought mentioned
in §5.4, we find out through experiments that code
prompting has the potential to discover ambiguity in
a question. Here we use 5 cases where the questions
present ambiguities and we can manually fix them. We
test few-shot code prompting and CoT prompting on
both questions with and without ambiguity. For each
question, we generate 15 answers by code prompting
and CoT prompting respectively with the temperature
of the LLM set to 0.7. Figure 5 shows the histogram
of answers. It is evident that ambiguity disturbs code
prompting, while CoT prompting is less sensitive to
ambiguity. We may leverage this feature to detect am-
biguity in questions.
Zero-shot methods vs. few-shot methods As
Large Language Models develop in size and human-
alignment, the gap between zero-shot and few-shot
prompting methods seems to narrow [Zhong et al.,
2023]. We have shown in the paper that zero-shot
code prompting is competitive with current few-shot
methods, including few-shot CoT [Wei et al., 2023] and
9

--- PAGE 10 ---
PAL [Gao et al., 2023]. However, the reasons behind
this are still left to be explored.
7 Conclusion
We introduce code prompting, a neural-symbolic prompting method with both zero-shot and few-shot
versions. We conduct experiments on 7 popular benchmarks involving both symbolic and arithmetic
reasoning and show that both zero-shot and few-shot code prompting enhance complex reasoning in
LLMs. Extensive experiments and analyses verify the effectiveness of our self-debugging, annotation,
equation instruction and irrelevant information techniques, and present many insights.
References
A. Amini, S. Gabriel, P. Lin, R. Koncel-Kedziorski, Y . Choi, and H. Hajishirzi. Mathqa: Towards
interpretable math word problem solving with operation-based formalisms, 2019.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,
J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models
are few-shot learners, 2020.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y . Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P.
Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol,
A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati,
K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba.
Evaluating large language models trained on code, 2021.
W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling
computation from reasoning for numerical reasoning tasks, 2022.
X. Chen, C. Liang, A. W. Yu, D. Song, and D. Zhou. Compositional generalization via neural-
symbolic stack machines, 2020a.
X. Chen, C. Liang, A. W. Yu, D. Zhou, D. Song, and Q. V . Le. Neural symbolic reader: Scalable
integration of distributed and symbolic representations for reading comprehension. In International
Conference on Learning Representations , 2020b. URL https://openreview.net/forum?id=
ryxjnREFwH .
X. Chen, M. Lin, N. Schärli, and D. Zhou. Teaching large language models to self-debug, 2023.
Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y . Hu, C. Xiong, D. Radev, M. Ostendorf, L. Zettlemoyer,
N. A. Smith, and T. Yu. Binding language models in symbolic languages, 2023.
T.-R. Chiang and Y .-N. Chen. Semantically-aligned equation generation for solving and reasoning
math word problems, 2019.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,
C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes,
Y . Tay, N. Shazeer, V . Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin,
M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski,
X. Garcia, V . Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph,
A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat,
A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz,
O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm:
Scaling language modeling with pathways, 2022.
K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,
R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021.
10

--- PAGE 11 ---
Y . Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot. Complexity-based prompting for multi-step
reasoning, 2023.
L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang, J. Callan, and G. Neubig. Pal: Program-aided
language models, 2023.
J. He-Yueya, G. Poesia, R. E. Wang, and N. D. Goodman. Solving math word problems by combining
language models with symbolic solvers, 2023.
M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman. Learning to solve arithmetic word
problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages 523–533, Doha, Qatar, Oct. 2014. Association
for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/
D14-1058 .
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,
and D. Amodei. Scaling laws for neural language models, 2020.
T. Khot, H. Trivedi, M. Finlayson, Y . Fu, K. Richardson, P. Clark, and A. Sabharwal. Decomposed
prompting: A modular approach for solving complex tasks, 2023.
T. Kojima, S. S. Gu, M. Reid, Y . Matsuo, and Y . Iwasawa. Large language models are zero-shot
reasoners, 2023.
R. Koncel-Kedziorski, H. Hajishirzi, A. Sabharwal, O. Etzioni, and S. D. Ang. Parsing algebraic
word problems into equations. Transactions of the Association for Computational Linguistics , 3:
585–597, 2015. doi: 10.1162/tacl_a_00160. URL https://aclanthology.org/Q15-1042 .
R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, and H. Hajishirzi. MAWPS: A math word
problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pages 1152–1157,
San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/
N16-1136. URL https://aclanthology.org/N16-1136 .
OpenAI. Gpt-4 technical report, 2023.
A. Patel, S. Bhattamishra, and N. Goyal. Are nlp models really able to solve simple math word
problems?, 2021.
O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis. Measuring and narrowing the
compositionality gap in language models, 2022.
C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang. Is chatgpt a general-purpose natural
language processing task solver?, 2023.
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.
Exploring the limits of transfer learning with a unified text-to-text transformer, 2020.
S. Roy and D. Roth. Solving general arithmetic word problems, 2016.
S. Roy, T. Vieira, and D. Roth. Reasoning about quantities in natural language. Transactions of
the Association for Computational Linguistics , 3:1–13, 2015. doi: 10.1162/tacl_a_00118. URL
https://aclanthology.org/Q15-1001 .
R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,
L. Baker, Y . Du, Y . Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y . Huang, M. Krikun,
D. Lepikhin, J. Qin, D. Chen, Y . Xu, Z. Chen, A. Roberts, M. Bosma, V . Zhao, Y . Zhou, C.-C.
Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern, M. R.
Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V . Prabhakaran, M. Diaz,
B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna,
M. Lamm, V . Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-Arcas, C. Cui,
M. Croak, E. Chi, and Q. Le. Lamda: Language models for dialog applications, 2022.
11

--- PAGE 12 ---
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient
foundation language models, 2023.
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-
consistency improves chain of thought reasoning in language models, 2023.
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-
thought prompting elicits reasoning in large language models, 2023.
S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin,
T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and
L. Zettlemoyer. Opt: Open pre-trained transformer language models, 2022a.
Z. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language
models, 2022b.
W. Zhong, R. Cui, Y . Guo, Y . Liang, S. Lu, Y . Wang, A. Saied, W. Chen, and N. Duan. Agieval: A
human-centric benchmark for evaluating foundation models, 2023.
D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le,
and E. Chi. Least-to-most prompting enables complex reasoning in large language models, 2023.
12

--- PAGE 13 ---
A Datasets
A.1 Symbolic reasoning
dataset #samples example problem
last letter concatenation 1,500500 (4 words) "fully, drug, gut, agreement"
500 (8 words) "urge, participate, strategic, rise,
win, through, now, within"
500 (12 words) "elementary, consensus, plaza, mes-
sage, telescope, accessible, another,
transport, bubble, bizarre, adviser,
cow"
coin flip 1,500500 (3 flips) A coin is heads up. Taylor doesn’t
flip the coin. Harmon doesn’t flip
the coin. Dejesus doesn’t flip the
coin. Is the coin still heads up?
500 (4 flips) A coin is heads up. Nichols flips the
coin. Mcbride flips the coin. Mathis
doesn’t flip the coin. Ruiz flips the
coin. Is the coin still heads up?
500 (5 flips) A coin is heads up. Weber flips
the coin. Contreras flips the coin.
Carter flips the coin. Moran flips
the coin. Colon doesn’t flip the coin.
Is the coin still heads up?
A.2 Arithmetic reasoning
dataset #samples example problem
SingleEq 508 Joan found 70 seashells on the beach. she gave Sam
some of her seashells. She has 27 seashell left. How
many seashells did she give to Sam ?
AddSub 395 There are 22 walnut trees currently in the park. Park
workers will plant walnut trees today. When the work-
ers are finished there will be 55 walnut trees in the park.
How many walnut trees did the workers plant today?
MultiArith 600 For Halloween Debby and her sister combined the
candy they received. Debby had 32 pieces of candy
while her sister had 42. If they ate 35 pieces the first
night, how many pieces do they have left?
SV AMP 1,000 Julia played tag with 18 kids on monday. She played
tag with 10 kids on tuesday. How many more kids did
she play with on monday than on tuesday ?
GSM8K 1,319 Janet’s ducks lay 16 eggs per day. She eats three for
breakfast every morning and bakes muffins for her
friends every day with four. She sells the remainder
at the farmers’ market daily for $2 per fresh duck egg.
How much in dollars does she make every day at the
farmers’ market?
13

--- PAGE 14 ---
B Experiment details for symbolic reasoning
B.1 Last letter concatenation
Here we show correct and incorrect examples for all the prompting methods we have mentioned in
the paper on the task of last letter concatenation.
Zero-shot standard
Table 7: Examples of correct and incorrect results by zero-shot standard prompting on last letter
concatenation.
Q: Concatenate the last letters of the given words: "imagine,admire,assume,equally"
A: e, e, e, y. Therefore, the answer is "eeey“ (correct)
Q: Concatenate the last letters of the given words: uh,employer,intelligent,straight"
A: hrtt. Therefore, the answer is "hrtt". (correct)
Q: Concatenate the last letters of the given words: "motion,instant,act,chapter"
A: n,t,t,r. Therefore, the answer is “nttr”. (correct)
Q: Concatenate the last letters of the given words: "fully,drug,gut,agreement"
A: lygdnt. Therefore, the answer is “lygdnt” (incorrect)
Q: Concatenate the last letters of the given words: "evil,secret,expansion,experience"
A: lnts. Therefore, the answer is lnts. (incorrect)
Q: Concatenate the last letters of the given words: "consist,wear,concede,considerable"
A: t-r-e-e-l-e. Therefore, the answer is "treele". (incorrect)
14

--- PAGE 15 ---
Zero-shot CoT
Table 8: Examples of correct and incorrect results by zero-shot CoT prompting on last letter concate-
nation.
Q: Concatenate the last letters of the given words: "fully,drug,gut,agreement"
A: Let’s think step by step.
1. The last letter of "fully" is "y".
2. The last letter of "drug" is "g".
3. The last letter of "gut" is "t".
4. The last letter of "agreement" is "t".
5. Concatenating these letters together, we get "ygtt".Therefore, the answer is "ygtt". (correct)
Q: Concatenate the last letters of the given words: "imagine,admire,assume,equally"
A: Let’s think step by step.1. The last letter of "imagine" is "e".
2. The last letter of "admire" is "e".
3. The last letter of "assume" is "e".
4. The last letter of "equally" is "y".
5. Concatenating these letters gives us "eeeY".Therefore, the answer is "eeeY". (correct)
Q: Concatenate the last letters of the given words: "consist,wear,concede,considerable"
A: Let’s think step by step.1. The last letter of "consist" is "t".
2. The last letter of "wear" is "r".
3. The last letter of "concede" is "e".
4. The last letter of "considerable" is "e".
5. Concatenating these letters together, we get "tree".Therefore, the answer is "tree". (correct)
Q: Concatenate the last letters of the given words: "evil,secret,expansion,experience"
A: Let’s think step by step.1. The last letter of "evil" is "l".
2. The last letter of "secret" is "t".
3. The last letter of "expansion" is "n".
4. The last letter of "experience" is "e".
5. Concatenating these letters together, we get "lten". Therefore, the answer is "lten". (incorrect)
Q: Concatenate the last letters of the given words: "weapon,reasonable,thin,loyalty"
A: Let’s think step by step.1. The last letter of "weapon" is "n".
2. The last letter of "reasonable" is "e".
3. The last letter of "thin" is "n".
4. The last letter of "loyalty" is "y".
5. Concatenating these letters together, we get "ney".Therefore, the answer is "ney". (incorrect)
Q: Concatenate the last letters of the given words: "motion,instant,act,chapter"
A: Let’s think step by step.1. The last letter of "motion" is "n".
2. The last letter of "instant" is "t".
3. The last letter of "act" is "t".
4. The last letter of "chapter" is "r".
5. Concatenating these letters together, we get "ntr".Therefore, the answer is "ntr". (incorrect)
15

--- PAGE 16 ---
Few-shot CoT
Table 9: Exemplars for the full prompt of few-shot CoT prompting on last letter concatenation. We
follow Zhou et al. [2023] to construct the exemplars.
Q: "think, machine"
A: The last letter of "think" is "k". The last letter of "machine" is "e". Concatenating "k", "e" leads
to "ke". So, "think, machine" outputs "ke".
Q: "learning, reasoning, generalization"
A: The last letter of "learning"is "g". The last letter of "reasoning" is "g". The last letter of
"generalization" is "n". Concatenating "g", "g", "n" leads to "ggn". So, "learning, reasoning,
generalization" outputs "ggn".
Table 10: Examples of correct and incorrect results by few-shot CoT prompting on last letter
concatenation.
Q: Concatenate the last letters of the given words: "fully,drug,gut,agreement"
A: The last letter of "fully" is "y". The last letter of "drug" is "g". The last letter of "gut" is
"t". The last letter of "agreement" is "t". Concatenating "y", "g", "t", "t" leads to "ygtt". So,
"fully,drug,gut,agreement" outputs "ygtt". (correct)
Q: Concatenate the last letters of the given words: "imagine,admire,assume,equally"
A: The last letter of "imagine" is "e". The last letter of "admire" is "e". The last letter of "assume"
is "e". The last letter of "equally" is "y". Concatenating "e", "e", "e", "y" leads to "eeey". So,
"imagine, admire, assume, equally" outputs "eeey". (correct)
Q: Concatenate the last letters of the given words: "evil,secret,expansion,experience"
A: The last letter of "evil" is "l". The last letter of "secret" is "t". The last letter of "expansion"
is "n". The last letter of "experience" is "e". Concatenating "l", "t", "n", "e" leads to "ltne". So,
"evil,secret,expansion,experience" outputs "ltne". (correct)
Q: "vacation,diverse,argument,speculation"
A: The last letter of "vacation" is "n". The last letter of "diverse" is "e". The last letter of
"argument" is "t". The last letter of "speculation" is "n". Concatenating "n", "e", "t", "n" leads to
"nent". So, "vacation, diverse, argument, speculation" outputs "nent". (incorrect)
Q: "Christianity,arm,invention,dog"
A: There is no common letter at the end of all four words, so there is no output for "Christianity,
arm, invention, dog". (incorrect)
Q: "German,establishment,accurate,initially"
A: The last letter of "German" is "n". The last letter of "establishment" is "t". The last letter of
"accurate" is "e". The last letter of "initially" is "y". Concatenating "n", "t", "e", "y" leads to "nty".
So, "German, establishment, accurate, initially" outputs "nty". (incorrect)
16

--- PAGE 17 ---
Zero-shot code
Table 11: Code generated in the first stage of code prompting for last letter concatenation.
Generate python code to concatenate the last letters of the given words.
Here’s the Python code to concatenate the last letters of the given words:
```
words = ["apple", "banana", "cherry", "date", "elderberry"]
result = ""
for word in words:
result += word[-1]
print(result)
```
17

--- PAGE 18 ---
Table 12: Examples of correct and incorrect results (in the second stage) by zero-shot code prompting
on last letter concatenation.
Q: "fully,drug,gut,agreement"
A:
Let’s think step by step. Print all the middle variables.
```
words = ["fully", "drug", "gut", "agreement"]
result = ""
for word in words:
last_letter = word[-1]
result += last_letter
print(f"Word: {word}, Last Letter: {last_letter}, Result: {result}")
print(result)
```
Output:
```
Word: fully, Last Letter: y, Result: y
Word: drug, Last Letter: g, Result: yg
Word: gut, Last Letter: t, Result: ygt
Word: agreement, Last Letter: t, Result: ygtt
ygtt
```
Therefore, the answer is "ygtt". (correct)
Q: "feed,characteristic,spray,anymore"
A:
Let’s think step by step. Print all the middle variables.
```
words = ["feed", "characteristic", "spray", "anymore"]
result = ""
for word in words:
last_letter = word[-1]
result += last_letter
print(f"Word: {word}, Last Letter: {last_letter}, Result: {result}")
print(result)
```
Output:
```
Word: feed, Last Letter: d, Result: d
Word: characteristic, Last Letter: c, Result: dc
Word: spray, Last Letter: y, Result: dcy
Word: anymore, Last Letter: e, Result: dbye
dbye
```
Therefore, the answer is dbye. (incorrect)
18

--- PAGE 19 ---
B.2 Coin flip
Here we show correct and incorrect examples for all the prompting methods we have mentioned in
the paper on the task of coin flip.
Zero-shot standard
Table 13: Examples of correct and incorrect results by zero-shot standard prompting on coin flip.
Q: A coin is heads up. Taylor doesn’t flip the coin. Harmon doesn’t flip the coin. Dejesus doesn’t
flip the coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Yes, the coin is still heads up. Therefore, the answer (Yes or No) is Yes. (correct)
Q: A coin is heads up. Tyler doesn’t flip the coin. Gates doesn’t flip the coin. Weber doesn’t flip
the coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Yes, the coin is still heads up. Therefore, the answer (Yes or No) is Yes. (correct)
Q: A coin is heads up. Kline doesn’t flip the coin. Underwood doesn’t flip the coin. Yu doesn’t
flip the coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Yes, the coin is still heads up. The fact that no one flipped the coin does not change its initial
state. Therefore, the answer (Yes or No) is Yes. (correct)
Q: A coin is heads up. Valencia doesn’t flip the coin. Ross flips the coin. Walter doesn’t flip the
coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Yes, the coin is still heads up. The actions of Valencia, Ross, and Walter do not affect the initial
state of the coin. Therefore, the answer (Yes or No) is Yes. (incorrect)
Q: A coin is heads up. Valentine flips the coin. Beck flips the coin. Molina doesn’t flip the coin. Is
the coin still heads up? Note that "flip" here means "reverse".
A: No, we don’t know the result of Valentine and Beck’s flips, so we can’t assume the coin is still
heads up. Therefore, the answer (Yes or No) is unknown. (incorrect)
Q: A coin is heads up. Stephens flips the coin. Cooper doesn’t flip the coin. Olson flips the coin.
Is the coin still heads up? Note that "flip" here means "reverse".
A: It depends on the outcome of Olson’s flip. If Olson flips the coin and it lands heads up, then the
coin is still heads up. If Olson flips the coin and it lands tails up, then the coin is no longer heads
up.Therefore, the answer (Yes or No) is uncertain. (incorrect)
19

--- PAGE 20 ---
Zero-shot CoT
Table 14: Examples of correct and incorrect results by zero-shot CoT prompting on coin flip.
Q: A coin is heads up. Taylor doesn’t flip the coin. Harmon doesn’t flip the coin. Dejesus doesn’t
flip the coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Let’s think step by step. Initially, the coin is heads up. None of the three people mentioned in
the question flip the coin, which means they don’t reverse it. Therefore, the coin remains in its
initial state, which is heads up. So, yes, the coin is still heads up. Therefore, the answer (Yes or
No) is Yes. (correct)
Q: A coin is heads up. Valencia doesn’t flip the coin. Ross flips the coin. Walter doesn’t flip the
coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Let’s think step by step.
1. The coin is heads up.
2. Valencia doesn’t flip the coin, so the coin remains heads up.
3. Ross flips the coin, which means the coin is now tails up.
4. Walter doesn’t flip the coin, so the coin remains tails up.
Therefore, the answer is no, the coin is not still heads up. It is now tails up. Therefore, the answer
(Yes or No) is No. (correct)
Q: A coin is heads up. Valentine flips the coin. Beck flips the coin. Molina doesn’t flip the coin. Is
the coin still heads up? Note that "flip" here means "reverse".
A: Let’s think step by step.1. The coin is heads up.
2. Valentine flips the coin, which means it is now tails up.
3. Beck flips the coin, which means it is now heads up again.
4. Molina doesn’t flip the coin, so it remains heads up.
Therefore, the coin is still heads up. Therefore, the answer (Yes or No) is Yes. (correct)
Q: A coin is heads up. Medrano doesn’t flip the coin. Cannon doesn’t flip the coin. Hutchinson
flips the coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Let’s think step by step.
1. The coin is heads up.
2. Medrano doesn’t flip the coin, so it remains heads up.
3. Cannon doesn’t flip the coin, so it still remains heads up.
4. Hutchinson flips the coin. We don’t know the result of the flip, so we can’t say for sure whether
the coin is still heads up or not.
Therefore, we can’t determine whether the coin is still heads up or not after Hutchinson flips it.
Therefore, the answer (Yes or No) is indeterminate. (incorrect)
Q: A coin is heads up. Farmer flips the coin. Chandler doesn’t flip the coin. Lozano flips the coin.
Is the coin still heads up? Note that "flip" here means "reverse".
A: Let’s think step by step.
1. The coin is heads up.
2. Farmer flips the coin, which means he reverses it. So now the coin is tails up.
3. Chandler doesn’t flip the coin, so it remains tails up.
4. Lozano flips the coin, which means he reverses it again. So now the coin is heads up again.
Therefore, the coin is not still heads up, but it has gone through a series of flips and is now heads
up again. Therefore, the answer (Yes or No) is No. (incorrect)
Q: A coin is heads up. Christian doesn’t flip the coin. Lewis doesn’t flip the coin. Becker flips the
coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: Let’s think step by step.
1. The coin is heads up.
2. Christian doesn’t flip the coin, so it remains heads up.
3. Lewis doesn’t flip the coin, so it still remains heads up.
4. Becker flips the coin. We don’t know the result of the flip, so we can’t say for sure whether the
coin is still heads up or not.
Therefore, we can’t determine whether the coin is still heads up or not without knowing the result
of Becker’s flip. Therefore, the answer (Yes or No) is unknown. (incorrect)
20

--- PAGE 21 ---
Few-shot CoT
Table 15: Exemplars for the full prompt of few-shot CoT prompting on coin flip. We follow Wei et al.
[2023] to construct the exemplars.
Q: A coin is heads up. Ka flips the coin. Sherrie flips the coin. Is the coin still heads up? Note that
"flip" here means "reverse".
A: The coin was flipped by Ka and Sherrie. So the coin was flipped 2 times, which is an even
number. The coin started heads up, so after an even number of flips, it will still be heads up. So
the answer is yes.
Q: A coin is heads up. Jamey flips the coin. Teressa flips the coin. Is the coin still heads up? Note
that "flip" here means "reverse".
A: The coin was flipped by Jamey and Teressa. So the coin was flipped 2 times, which is an even
number. The coin started heads up, so after an even number of flips, it will still be heads up. So
the answer is yes.
Q: A coin is heads up. Maybelle flips the coin. Shalonda does not flip the coin. Is the coin still
heads up? Note that "flip" here means "reverse".
A: The coin was flipped by Maybelle. So the coin was flipped 1 time, which is an odd number.
The coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.
Q: A coin is heads up. Millicent does not flip the coin. Conception flips the coin. Is the coin still
heads up? Note that "flip" here means "reverse".
A: The coin was flipped by Conception. So the coin was flipped 1 time, which is an odd number.
The coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.
Q: A coin is heads up. Sal flips the coin. Raymond does not flip the coin. Is the coin still heads
up? Note that "flip" here means "reverse".
A: The coin was flipped by Sal. So the coin was flipped 1 time, which is an odd number. The coin
started heads up, so after an odd number of flips, it will be tails up. So the answer is no.
Q: A coin is heads up. Conception flips the coin. Kristian does not flip the coin. Is the coin still
heads up? Note that "flip" here means "reverse".
A: The coin was flipped by Conception. So the coin was flipped 1 time, which is an odd number.
The coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.
Q: A coin is heads up. Inga does not flip the coin. Elanor does not flip the coin. Is the coin still
heads up? Note that "flip" here means "reverse".
A: The coin was flipped by no one. So the coin was flipped 0 times. The coin started heads up,
and it was not flipped, so it is still heads up. So the answer is yes.
Q: A coin is heads up. Ryan flips the coin. Shaunda flips the coin. Is the coin still heads up? Note
that "flip" here means "reverse".
A: The coin was flipped by Ryan and Shaunda. So the coin was flipped 2 times, which is an even
number. The coin started heads up, so after an even number of flips, it will still be heads up. So
the answer is yes.
21

--- PAGE 22 ---
Table 16: Examples of correct and incorrect results by few-shot CoT prompting on coin flip.
Q: A coin is heads up. Taylor doesn’t flip the coin. Harmon doesn’t flip the coin. Dejesus doesn’t
flip the coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: The coin was not flipped by anyone. So the coin was flipped 0 times. The coin started heads
up, and it was not flipped, so it is still heads up. So the answer is yes. (correct)
Q: A coin is heads up. Valencia doesn’t flip the coin. Ross flips the coin. Walter doesn’t flip the
coin. Is the coin still heads up? Note that "flip" here means "reverse".
A: The coin was flipped by Ross. So the coin was flipped 1 time, which is an odd number. The
coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.
(correct)
Q: A coin is heads up. Valentine flips the coin. Beck flips the coin. Molina doesn’t flip the coin. Is
the coin still heads up? Note that "flip" here means "reverse".
A: The coin was flipped by Valentine and Beck. So the coin was flipped 2 times, which is an
even number. The coin started heads up, so after an even number of flips, it will still be heads up.
Molina did not flip the coin, so it does not affect the outcome. So the answer is yes. (correct)
Q: A coin is heads up. Park flips the coin. Cooper doesn’t flip the coin. Wright doesn’t flip the
coin. Is the coin still heads up?
A: The coin was flipped by Park. So the coin was flipped 1 time, which is an odd number. The
coin started heads up, so after an odd number of flips, it will be tails up. However, since Cooper
and Wright did not flip the coin, we do not know if it was flipped again. So the answer is uncertain.
(incorrect)
Zero-shot code
Table 17: Code generated in the first stage of code prompting for coin flip.
A coin is heads up , there are some people, each one flipped or didn’t flip the coin. Generate
python code to determine whether the coin is still heads up.
Note that "flip" here means "reverse".
Assuming that the initial state of the coin is represented by a boolean variable called "heads_up"
and the list of people’s actions is represented by a list of boolean values called "flips", the following
Python code can be used to determine whether the coin is still heads up:
```
heads_up = True # initial state of the coin
flips = [True, False, True, True, False] # list of people’s actions
for flip in flips:
if flip:
heads_up = not heads_up
# if the person didn’t flip the coin, do nothing
if heads_up:
print("The coin is still heads up.")
else:
print("The coin is now tails up.")
```
22

--- PAGE 23 ---
Table 18: Examples of correct and incorrect results by zero-shot code prompting on coin flip.
Q: A coin is heads up. Taylor doesn’t flip the coin. Harmon doesn’t flip the coin. Dejesus doesn’t
flip the coin. Is the coin still heads up?
Let’s think step by step. Print all the middle variables.
A:Yes, the coin is still heads up.
To verify this, we can use the same Python code as before, but with a different list of flips:
```
heads_up = True # initial state of the coin
flips = [False, False, False] # list of people’s actions
for flip in flips:
if flip:
heads_up = not heads_up # reverse the state of the coin
# if the person didn’t flip the coin, do nothing
print(heads_up) # print the current state of the coin
if heads_up:
print("The coin is still heads up.")
else:
print("The coin is now tails up.")
```
When we run this code, we can see that the current state of the coin remains True (heads up)
throughout the loop, and the final message confirms that the coin is still heads up:
```
True
True
True
The coin is still heads up.
```
Therefore, the answer (Yes or No) is Yes. (correct)
Q: A coin is heads up. Valencia doesn’t flip the coin. Ross flips the coin. Walter doesn’t flip the
coin. Is the coin still heads up?
Let’s think step by step. Print all the middle variables.
A:Using the same code as before, we can modify the list of flips to match the scenario described
in the question:
```
heads_up = True # initial state of the coin
flips = [False, True, False] # list of people’s actions
for flip in flips:
if flip:
heads_up = not heads_up # reverse the state of the coin
# if the person didn’t flip the coin, do nothing
print(heads_up)
if heads_up:
print("The coin is still heads up.")
else:
print("The coin is now tails up.")
```
When we run this code, we get the following output:
```
True
False
True
The coin is still heads up.
```
This means that the coin is still heads up after Ross flips it, since the final state of the coin is still
heads up.Therefore, the answer (Yes or No) is Yes, the coin is still heads up. (incorrect)
23

--- PAGE 24 ---
C Error Analysis for symbolic reasoning
Here we show an example to illustrate how ambiguity disturbs CoT prompting.
Table 19: Example of ambiguity in CoT prompting. In the specific example, in the fourth step, the
LLM is confused by the meaning of “flip” and thus fail to give the correct answer.
Q: A coin is heads up. Christian doesn’t flip the coin. Lewis doesn’t flip the coin. Becker flips the
coin. Is the coin still heads up? Note that “flip” here means “reverse”.
A: Let’s think step by step.
1. The coin is heads up.
2. Christian doesn’t flip the coin, so it remains heads up.
3. Lewis doesn’t flip the coin, so it still remains heads up.
4. Becker flips the coin. We don’t know the result of the flip, so we can’t say for sure whether the
coin is still heads up or not.
Therefore, we can’t determine whether the coin is still heads up or not without knowing the result
of Becker’s flip. Therefore, the answer (Yes or No) is unknown.
D Experiment details for arithmetic reasoning
Here we show full prompts for zero-shot CoT prompting, zero-shot code prompting (+irr/+equ),
few-shot CoT, PAL, few-shot code prompting (+irr/+equ) and few-shot code prompting + LLM
self-contained. Furthermore, we show the results for few-shot code prompting + LLM self-contained.
Besides, we add system messages for all the prompting methods to align with Gao et al. [2023]. For
CoT prompting, we set the system message to “You will solve math problems.”; for PAL and code
prompting, we set the system message to “You will write python program to solve math problems.
You will only write code blocks.”.
Zero-shot CoT
Table 20: Zero-shot CoT prompt for math world problems.
Q: {question}
A: Let’s think step by step.
24

--- PAGE 25 ---
Zero-shot code
Table 21: Zero-shot code prompt (plain/+irr/+equ) for math world problems.
plain:
Generate python code to answer the question.
Note that code should follow the format ```code```.
Q: {question}
+irr:
Generate python code to answer the question.
Note that code should follow the format ```code```.
There may be irrelevant information in the question. If you find it, ignore it.
Q: {question}
+equ:
Generate python code to answer the question.
Note that code should follow the format ```code```.
If you need to solve an equation, here’s an instruction:
```python
# to solve an equation, you can use python package sympy
import sympy
# for example, to solve 2*x = 5
# First, declarify your variable, in this case, ’x’
x = sympy.symbols("x")
# Second, transform the equation so that the right hand side of the equation is zero.
# 2*x - 5 = 0
# Third, use ‘sympy.solve‘ to solve the equation
a = sympy.solve([2 * x - 5], [x])
# Print the output as a float. Note that ‘a‘ is a dict
print(float(a[x]))
```
Q: {question}
25

--- PAGE 26 ---
Few-shot CoT
A: The robe takes 2 bolts of blue 
fiber and half that much white fiber. 
So it takes 2 + 1 = 3 bolts in total. 
The answer is 3.Q: If there are 3 cars in the parking 
lot and 2 more cars arrive, how 
many cars are in the parking lot?
A: There are originally 3 cars. 2 
more cars arrive. 3 + 2 = 5. The 
answer is 5.
…(more exemplars hidden)
Q: A robe takes 2 bolts of blue fiber 
and half that much white fiber.  
How many bolts in total does it 
take?
Language ModelFew-shot Code Prompting
Q: If there are 3 cars in the parking 
lot and 2 more cars arrive, how 
many cars are in the parking lot?
C:
```
original_cars = 3 # There are 3 
cars in the parking lot
new_cars = 2 # 2 more cars arrive
total_cars = original_cars + 
new_cars # So the total number of 
cars in the parking lot is 
original_cars + new_cars
print( total_cars ) # print final result
```
…(more exemplars hidden)
Q: A robe takes 2 bolts of blue 
fiber and half that much white 
fiber.  How many bolts in total 
does it take?Language 
ModelPython
Interpreter3Few-shot Chain -of-Thought Prompting
blue_fiber_bolts =2# A robe takes 2 
bolts of blue fiber
white_fiber_bolts =blue_fiber_bolts /
2# Half as many white fiber bolts are 
needed as blue fiber bolts
total_bolts =blue_fiber_bolts +
white_fiber_bolts # Total number of 
bolts needed is the sum of blue and 
white fiber bolts
print(total_bolts ) # Print the final 
result
Figure 6: The pipelines of few-shot CoT prompting and few-shot code prompting are shown in the
figure.
Table 22: Few-shot CoT prompt for math world problems. We follow the code released by Gao et al.
[2023] to choose the exemplars.
Let’s think step by step to solve math problems. Here are three examples how to do it,
Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15
dollars left. 23 - 15 is 8. The answer is 8.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more.
How many golf balls did he have at the end of wednesday?
A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing
2 more, he had 35 - 2 = 33 golf balls. The answer is 33.
Q: There were nine computers in the server room. Five more computers were installed each day,
from monday to thursday. How many computers are now in the server room?
A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 *
4 = 20 computers were added. 9 + 20 is 29. The answer is 29.
How about this question?
Q: {question}
26

--- PAGE 27 ---
PAL
Table 23: PAL prompt for math world problems. We use the demonstrations from the code released
by Gao et al. [2023].
Let’s use python to solve math problems. Here are three examples how to do it,
Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
```
def solution():
"""Olivia has $23. She bought five bagels for $3 each. How much money does she have left?"""
money_initial = 23
bagels = 5
bagel_cost = 3
money_spent = bagels * bagel_cost
money_left = money_initial - money_spent
result = money_left
return result
```
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more.
How many golf balls did he have at the end of wednesday?
```
def solution():
"""Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more.
How many golf balls did he have at the end of wednesday?"""
golf_balls_initial = 58
golf_balls_lost_tuesday = 23
golf_balls_lost_wednesday = 2
golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday
result = golf_balls_left
return result
```
Q: There were nine computers in the server room. Five more computers were installed each day,
from monday to thursday. How many computers are now in the server room?
```
def solution():
"""There were nine computers in the server room. Five more computers were installed each day,
from monday to thursday. How many computers are now in the server room?"""
computers_initial = 9
computers_per_day = 5
num_days = 4 # 4 days between monday and thursday
computers_added = computers_per_day * num_days
computers_total = computers_initial + computers_added
result = computers_total
return result
```
How about this question?
Q: {question}
27

--- PAGE 28 ---
Few-shot code
Table 24: Few-shot code prompt for math world problems. Here we present the version of adding
annotations at the end of each line of the code.
Let’s use python to solve math problems. Here are three examples how to do it,
Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
```
def solution():
"""Olivia has $23. She bought five bagels for $3 each. How much money does she have left?"""
money_initial = 23 # Olivia has $23 initially
bagels = 5 # Olivia bought 5 bagels
bagel_cost = 3 # Each bagel cost $3
money_spent = bagels * bagel_cost # The total cost of 5 bagels is the product of the price of each
bagel and the number of bagels
money_left = money_initial - money_spent # Money left is the difference between initial money
and the total cost of 5 bagels
result = money_left
return result
```
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more.
How many golf balls did he have at the end of wednesday?
```
def solution():
"""Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more.
How many golf balls did he have at the end of wednesday?"""
golf_balls_initial = 58 # Michael had 58 golf balls initially
golf_balls_lost_tuesday = 23 # He lost 23 golf balls on Tuesday
golf_balls_lost_wednesday = 2 # He lost 2 more golf balls on Wednesday
golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday # Num-
ber of golf balls remaining is the difference between initial golf balls and the golf balls lost on
Tuesday and Wednesday
result = golf_balls_left
return result
```
Q: There were nine computers in the server room. Five more computers were installed each day,
from monday to thursday. How many computers are now in the server room?
```
def solution():
"""There were nine computers in the server room. Five more computers were installed each day,
from monday to thursday. How many computers are now in the server room?"""
computers_initial = 9 # There were 9 computers in the server room initially
computers_per_day = 5 # 5 more computers were installed each day from Monday to Thursday
num_days = 4 # 4 days between monday and thursday
computers_added = computers_per_day * num_days # Additional computers are the product of
the number of computers installed each day and the number of days
computers_total = computers_initial + computers_added # Total number of computers is the sum
of initial computers and the additional computers installed on 4 days
result = computers_total
return result
```
How about this question?
Q: {question}
28

--- PAGE 29 ---
Few-shot code + LLM self-contained
Table 25: Prompt of few-shot code + LLM self-contained for math world problems.
Let’s use python to solve math problems. Here are three examples how to do it,
Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
```
def solution():
"""Olivia has $23. She bought five bagels for $3 each. How much money does she have left?"""
money_initial = 23 # Olivia has $23 initially
bagels = 5 # Olivia bought 5 bagels
bagel_cost = 3 # Each bagel cost $3
money_spent = bagels * bagel_cost # The total cost of 5 bagels is the product of the price of each
bagel and the number of bagels
money_left = money_initial - money_spent # Money left is the difference between initial money
and the total cost of 5 bagels
result = money_left
return result
```
Let’s think step by step according to the code:
```
money_initial = 23
bagels = 5
bagel_cost = 3
money_left = money_initial - money_spent = 23 - 15 = 8
result = money_left = 8
```
So the answer is 8.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more.
How many golf balls did he have at the end of wednesday?
```
def solution():
"""Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more.
How many golf balls did he have at the end of wednesday?"""
golf_balls_initial = 58 # Michael had 58 golf balls initially
golf_balls_lost_tuesday = 23 # He lost 23 golf balls on Tuesday
golf_balls_lost_wednesday = 2 # He lost 2 more golf balls on Wednesday
golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday # Num-
ber of golf balls remaining is the difference between initial golf balls and the golf balls lost on
Tuesday and Wednesday
result = golf_balls_left
return result
```
Let’s think step by step according to the code:
```
golf_balls_initial = 58
golf_balls_lost_tuesday = 23
golf_balls_lost_wednesday = 2
golf_balls_left = golf_balls_initial - golf_balls_lost_tuesday - golf_balls_lost_wednesday = 58 -
23 - 2 = 33
result = golf_balls_left = 33
```
So the answer is 33.
29

--- PAGE 30 ---
Q: There were nine computers in the server room. Five more computers were installed each day,
from monday to thursday. How many computers are now in the server room?
```
def solution():
"""There were nine computers in the server room. Five more computers were installed each day,
from monday to thursday. How many computers are now in the server room?"""
computers_initial = 9 # There were 9
computers in the server room initially computers_per_day = 5 # 5 more computers were installed
each day from Monday to Thursday
num_days = 4 # 4 days between monday and thursday
computers_added = computers_per_day * num_days # Additional computers are the product of
the number of computers installed each day and the number of days
computers_total = computers_initial + computers_added # Total number of computers is the sum
of initial computers and the additional computers installed on 4 days
result = computers_total return result
```
Let’s think step by step according to the code:
```
computers_initial = 9
computers_per_day = 5
num_days = 4
computers_added = computers_per_day * num_days = 5 * 4 = 20
computers_total = computers_initial + computers_added = 9 + 20 = 29
result = computers_total = 29
```
So the answer is 29.
How about this question?
Q: {question}
The results of self-contained LLM reasoning after generating the code for arithmetic tasks are shown
in Table 26. While Gao et al. [2023] claims that “while current LLMs can be excellent at specifying
a high-level plan to solve a task—they are still incapable of executing them”, we show through
experiments that external tools like Python interpreter may not be necessary, and that symbolic and
arithmetic reasoning abilities are self-contained for LLMs.
Table 26: The accuracy (%) of few-shot code prompting (+ LLM self-contained) on arithmetic
reasoning datasets.
SingleEq AddSub SV AMP MultiArith GSM8K
few-shot code + LLM self-contained 97.44 90.89 79.80 96.83 77.62
E Error analysis for arithmetic reasoning
E.1 AddSub
As mentioned in §5.4, in AddSub, LLMs make errors mainly due to the distraction of irrelevant
variables. Here we show two examples of the LLMs distracted by irrelevant variables.
30

--- PAGE 31 ---
Table 27: Error examples for AddSub. In the left example, torn_card is an irrelevant vari-
able that should not appear in the equation. The correct answer should be remaining_cards
= initial_cards - torn_cards , which gives 562. In the right example, found_money is an ir-
relevant variable. The correct answer should be total_cost = snake_toy_cost + cage_cost ,
which gives 26.3.
Q: Joan had 695 Pokemon cards, and 6 were
torn. Sara bought 133 of Joan’s Pokemon cards.
How many Pokemon cards does Joan have now?Q: Dan spent $11.76 on a snake toy, and a cage
cost him $14.54. Dan also found a dollar bill
on the ground. What was the total cost of Dan’s
purchases?
# Joan had 695 Pokemon cards initially
initial_cards =695
# 6 of the cards were torn
torn_cards =6
# Sara bought 133 of Joan's Pokemon cards
cards_sold =133
# Total number of cards Joan has now is the difference 
between initial cards, torn cards, and cards sold
remaining_cards =initial_cards -torn_cards -cards_sold
# Print the final result
print(remaining_cards )
snake_toy_cost =11.76# The cost of the snake toy is 
$11.76
cage_cost =14.54# The cost of the cage is $14.54
found_money =1# Dan found $1 on the ground
total_cost =snake_toy_cost +cage_cost -found_money
# The total cost is the sum of the cost of the snake toy 
and the cage, minus the dollar bill Dan found
print(total_cost ) # Print the final result
# Joan had 695 Pokemon cards initially
initial_cards =695
# 6 of the cards were torn
torn_cards =6
# Sara bought 133 of Joan's Pokemon cards
cards_sold =133
# Total number of cards Joan has now is the difference 
between initial cards, torn cards, and cards sold
remaining_cards =initial_cards -torn_cards -cards_sold
# Print the final result
print(remaining_cards )
snake_toy_cost =11.76# The cost of the snake toy is 
$11.76
cage_cost =14.54# The cost of the cage is $14.54
found_money =1# Dan found $1 on the ground
total_cost =snake_toy_cost +cage_cost -found_money
# The total cost is the sum of the cost of the snake toy 
and the cage, minus the dollar bill Dan found
print(total_cost ) # Print the final result
E.2 MultiArith
We have mentioned in §5.4 that in MultiArith, the wrong answers from code prompting are mainly
due to two reasons: 1) Code prompting is more sensitive to ambiguity in the question, which may
leads to a wrong answer. 2) The groundtruth for some questions is wrong, while the predictions are
correct.
Here we show examples for both cases.
Table 28: An error example of few-shot code prompting for MultiArith. The question does not make
it clear how many apple a student want, but the ground truth suggests that we should take it as default
that each student wants 1apple. Code prompting makes the LLM more sensitive to such ambiguity.
Q: The school cafeteria ordered 43 red apples and 32 green apples for students lunches. But, if
only 2 students wanted fruit, how many extra did the cafeteria end up with?
red_apples =43# The cafeteria ordered 43 red apples
green_apples =32# The cafeteria ordered 32 green apples
total_apples =red_apples +green_apples # Total number of apples ordered is the sum of red and green apples
students_want_fruit =2# Only 2 students wanted fruit
apples_needed =students_want_fruit *2# Each student needs 2 apples
extra_apples =total_apples -apples_needed # Extra apples are the difference between total apples and the 
apples needed for the 2 students
print(extra_apples ) # Print the final result
Table 29: An error example of few-shot code prompting for MultiArith. The ground truth fails to
count Emily into the players, resulting in wrong answer (39−23)/2 = 8 .
Q: Emily’s team won their dodgeball game and scored 39 points total. If Emily scored 23 of the
points and everyone else scored 2 points each, how many players were on her team?
emily_points =23# Emily scored 23 points
other_players_points =2# Each other player scored 2 points
total_points =39# The team scored 39 points in total
num_other_players =(total_points -emily_points ) /other_players_points # Number of other players is the 
difference between total points and Emily's points, divided by the points each other player scored
num_players =num_other_players +1# Total number of players is the number of other players plus Emily
print(num_players ) # Print the final result
31

--- PAGE 32 ---
E.3 GSM8K
Error with equations. We have mentioned in §5.4 that the LLM struggles to solve equations in code.
When facing an equation, the LLM tends to solve it in the annotations or directly lists the equation in
the code, which may easily result in wrong answers or bugs. Here are two examples.
Table 30: An error example of code prompting for GSM8K.
Q: Ruby is 6 times older than Sam. In 9 years, Ruby will be 3 times as old as Sam. How old is
Sam now?
# Let's assume Sam's age is x
# According to the question, Ruby is 6 times older than Sam
# So, Ruby's age will be 6x
# In 9 years, Ruby will be 3 times as old as Sam
# So, (6x + 9) = 3(x + 9)
# Solving the above equation, we get x = 9
# Therefore, Sam's age now is 9 years old
sam_age =9
print("Sam's age now is" , sam_age, "years old." )
Table 31: An error example of code prompting for GSM8K.
Q: The three angles in a triangle add up to 180 degrees. One angle is twice the smallest angle, and
one angle is three times the smallest angle. What is the measure of the largest angle in the triangle,
in degrees?
smallest_angle =x
second_angle =2*x
third_angle =3*x
sum_of_angles =smallest_angle +second_angle +third_angle
largest_angle =third_angle
print (largest_angle )
Bug report:
NameError : name 'x' is not defined
Error distribution of CoT prompting and code prompting. Here we show the error distribution of
few-shot CoT prompting and few-shot code prompting(w/ annotations at the end). Since here we only
investigate into the error overlap of CoT prompting and code prompting, to eliminate the effects of
system messages and instructions, we remove all the system messages and unnecessary instructions
(They are added in the former experiments to align with the method PAL Gao et al. [2023]). Besides,
we use the original 8 exemplars from Wei et al. [2023]. In §5.5, we show the results of 8-shot CoT
prompting, 8-shot code prompting and the ensemble of them.
32

--- PAGE 33 ---
70.96% 8.72%
10.61% 9.70%few-shot CoT
right wrongfew-shot code
right wrongFigure 7: Error distribution of few-shot code prompting and few-shot CoT prompting regarding
dataset GSM8K.
33
