# 2401.02072.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2401.02072.pdf
# Kích thước tệp: 1349202 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ICE-GRT: Tăng cường Ngữ cảnh Hướng dẫn bằng Transformers dựa trên Học tăng cường Sinh tạo
Chen Zheng Ke Sun Da Tang Yukun Ma Yuyu Zhang
Chenguang Xi Xun Zhou
Nhóm AML, Bytedance Inc.
{chen.zheng1,ke.sun1,da.tang,mayukun,yuyu.zhang}@bytedance.com
{chenguang.xi,zhouxun}@bytedance.com
Tóm tắt
Sự xuất hiện của các Mô hình Ngôn ngữ Lớn (LLMs) như ChatGPT và LLaMA gặp phải những hạn chế trong các tác vụ chuyên môn, với những mô hình này thường thiếu chiều sâu và độ chính xác trong các lĩnh vực chuyên biệt, và thể hiện sự giảm sút trong khả năng tổng quát khi được tinh chỉnh, đặc biệt là khả năng phân tích trong các mô hình kích thước nhỏ. Để giải quyết những khoảng trống này, chúng tôi giới thiệu ICE-GRT, sử dụng Học tăng cường từ Phản hồi Con người (RLHF) dựa trên Tối ưu hóa Chính sách Gần kề (PPO), thể hiện khả năng đáng chú ý trong các tình huống trong miền mà không làm tổn hại đến hiệu suất tác vụ tổng quát. Việc khám phá ICE-GRT của chúng tôi làm nổi bật khả năng hiểu và lập luận của nó không chỉ để tạo ra các câu trả lời vững chắc mà còn cung cấp các phân tích chi tiết về lý do đằng sau câu trả lời. Khả năng này đánh dấu một tiến bộ đáng kể vượt xa phạm vi của các mô hình Tinh chỉnh có Giám sát. Thành công của ICE-GRT phụ thuộc vào một số yếu tố quan trọng, bao gồm Dữ liệu Phù hợp, Mở rộng Quy mô Phần thưởng, Kiểm soát KL, Chuẩn hóa Lợi thế, v.v. Mô hình ICE-GRT thể hiện hiệu suất tốt nhất trong các tác vụ chuyên môn và trên 12 tác vụ Ngôn ngữ tổng quát so với các LLM có kích thước tương đương và thậm chí lớn hơn, làm nổi bật tính hiệu quả của phương pháp của chúng tôi. Chúng tôi cung cấp một phân tích toàn diện về ICE-GRT, nhấn mạnh những tiến bộ đáng kể mà nó mang lại cho lĩnh vực LLM.

1 Giới thiệu
Sự ra đời của các Mô hình Ngôn ngữ Lớn (LLMs) như ChatGPT (Brown et al., 2020; OpenAI, 2023) và LLaMA (Touvron et al., 2023a,b) đã đánh dấu một cột mốc quan trọng trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP). Những mô hình này đã được công nhận rộng rãi nhờ khả năng hội thoại tổng quát vững chắc, cho phép các phản hồi trôi chảy và nhất quán trên một phạm vi chủ đề đa dạng. Tuy nhiên, có những hạn chế chính đối với những mô hình này. Thứ nhất, một hạn chế chính xuất hiện khi những mô hình này gặp phải các tác vụ chuyên môn (Zhao et al., 2023; Zhang et al., 2023a). Trong các tình huống đòi hỏi kiến thức kỹ thuật sâu hoặc chuyên môn chuyên biệt, những mô hình này thường không đạt yêu cầu, cung cấp các phản hồi thiếu chiều sâu và độ chính xác cần thiết. Thứ hai, các LLM Tinh chỉnh có Giám sát (SFT) có xu hướng thể hiện sự giảm sút trong khả năng tổng quát (Ling et al., 2023). Điều này trái ngược với những kỳ vọng dành cho các mô hình quy mô lớn, được cho là sẽ duy trì hoặc cải thiện hiệu suất của chúng trong một loạt các tác vụ (Pan et al., 2023a). Cuối cùng, các LLM kích thước nhỏ hơn hiện tại, chẳng hạn như 13 Tỷ, thể hiện khả năng hạn chế trong việc tiến hành phân tích chi tiết về các câu hỏi phức tạp, một năng lực kém hơn đáng kể so với khả năng của các mô hình như ChatGPT, có thể tham gia vào các cuộc thảo luận toàn diện và chi tiết hơn.

Để giải quyết những thách thức này, chúng tôi giới thiệu Tăng cường Ngữ cảnh Hướng dẫn bằng Transformers dựa trên Học tăng cường Sinh tạo (ICE-GRT), một LLM sáng tạo tận dụng các nguyên tắc của Học tăng cường từ Phản hồi Con người (RLHF) (Brown et al., 2020) dựa trên Tối ưu hóa Chính sách Gần kề (PPO) (Schulman et al., 2017). Trong khi đảm bảo rằng các khả năng tổng quát của Mô hình Ngôn ngữ Lớn (LLM) được duy trì, ICE-GRT thể hiện hiệu suất xuất sắc trong một số tình huống chuyên môn. Hơn nữa, ICE-GRT thể hiện khả năng cải thiện cho phân tích chi tiết, đặc biệt trong các tình huống phức tạp mà các LLM kích thước nhỏ không đạt yêu cầu.

Chúng tôi lấy một tác vụ chuyên môn về kiểm duyệt quảng cáo làm ví dụ. ICE-GRT không chỉ có thể xác định tính tuân thủ của quảng cáo mà còn xác định danh mục vi phạm cụ thể. Hơn nữa, nó tiến thêm một bước bằng cách phân tích chi tiết những yếu tố nào của quảng cáo có vấn đề và đưa ra các đề xuất sửa đổi mang tính xây dựng. Đây là một tiến bộ đáng chú ý so với cả các mô hình LLM được đào tạo trước và SFT (Chiang et al., 2023), vốn thường chỉ giới hạn trong việc xác định tính tuân thủ và danh mục vi phạm.

--- TRANG 2 ---
Khi phương pháp đào tạo của chúng tôi được áp dụng cho RLHF, chúng tôi quan sát thấy không chỉ những cải thiện đáng kể trong các tác vụ trong miền mà còn có sự tăng cường đáng ngạc nhiên trong các tác vụ tổng quát. Trong một phân tích so sánh với các mô hình có kích thước tham số tương đương và lớn hơn trên nhiều tác vụ tổng quát, mô hình ICE-GRT 13 tỷ tham số của chúng tôi liên tục đạt được hiệu suất tốt nhất trong 12 điểm chuẩn đánh giá LLM công khai nổi tiếng. Tính linh hoạt của ICE-GRT được làm rõ thêm thông qua việc xử lý hiệu quả các tác vụ chuyên môn khác nhau, không giới hạn nhưng bao gồm Tạo thơ, Chuyển đổi Văn bản sang Bảng, Đối thoại Đa vòng hấp dẫn, tạo ra các Phản hồi Đa ngôn ngữ chính xác, Tạo mã thành thạo, tạo Văn bản Quảng cáo phù hợp và Ghi nhãn Văn bản, v.v.

Việc khám phá mô hình ICE-GRT của chúng tôi đã phát hiện ra một số yếu tố quan trọng đối với thành công đào tạo của nó. Dữ liệu đào tạo của mô hình ICE-GRT, có nguồn gốc từ mô hình ICE-Instruct (SFT) và được làm phong phú bằng phản hồi con người với tiêu chí đánh giá nghiêm ngặt, cung cấp một tập dữ liệu đa dạng và toàn diện, cần thiết cho việc đào tạo vững chắc. Hơn nữa, việc mở rộng quy mô của mô hình phần thưởng là cần thiết để nắm bắt chính xác các tình huống phức tạp và phù hợp với sở thích con người trong RLHF. Ngoài ra, Kiểm soát KL là chìa khóa để điều chỉnh sự cân bằng giữa các mô hình, trong khi Chuẩn hóa Lợi thế cải thiện đáng kể tính ổn định học tập bằng cách điều chỉnh các ước lượng lợi thế. Bổ sung, chúng tôi phát hiện ra rằng việc sửa đổi Phạm vi Cắt và kiểm soát cẩn thận độ dài phản hồi tối đa trong quá trình lấy mẫu là quan trọng để tăng cường quá trình đào tạo. Những phát hiện này làm sâu sắc thêm hiểu biết của chúng tôi về các cơ chế RLHF và đóng vai trò quan trọng trong việc đào tạo hiệu quả mô hình ICE-GRT.

Hơn nữa, chúng tôi cung cấp một phân tích chi tiết về mô hình ICE-GRT, bao gồm cả khả năng tổng quát và trong miền. Thông qua việc khám phá này, chúng tôi nhằm mục đích đóng góp một góc nhìn và phương pháp mới cho lĩnh vực NLP, đặc biệt trong việc tăng cường chiều sâu và độ chính xác của việc xử lý các tác vụ chuyên môn bởi các mô hình ngôn ngữ lớn. Chúng tôi quan sát thấy rằng giai đoạn đào tạo trước tham gia vào "học tập kiến thức", nơi mô hình hấp thụ rộng rãi một phạm vi thông tin đa dạng, hình thành một cơ sở kiến thức nền tảng đáng kể. Tiếp theo, trong giai đoạn Tinh chỉnh có Giám sát, mô hình tham gia vào "khai thác kiến thức", nơi nó sử dụng kiến thức đã học để phản hồi các hướng dẫn cụ thể. Giai đoạn này rất quan trọng để mô hình chuyển từ tích lũy kiến thức thụ động sang ứng dụng kiến thức chủ động. Cuối cùng, giai đoạn RLHF tham gia vào "tăng cường kiến thức", nâng cao khả năng của mô hình để phù hợp với sở thích ngôn ngữ con người. Giai đoạn này xây dựng dựa trên kiến thức rộng lớn thu được trong giai đoạn đào tạo trước và khai thác kiến thức từ giai đoạn SFT, dẫn đến một mô hình không chỉ tái tạo kiến thức rộng lớn mà còn xuất sắc trong việc ứng dụng nó với sở thích lấy con người làm trung tâm. Quan trọng, giai đoạn này thể hiện một bước nhảy đáng kể trong khả năng xuất hiện của mô hình.

Trong cam kết thúc đẩy nghiên cứu hợp tác và sáng tạo, chúng tôi công khai ICE-GRT trên HuggingFace1. Sáng kiến nguồn mở này nhằm mục đích trao quyền cho các nhà nghiên cứu trên toàn cầu để điều tra thêm và mở rộng các phát hiện của chúng tôi với ICE-GRT. Bằng cách dân chủ hóa quyền truy cập vào mô hình tiên tiến này, chúng tôi hy vọng sẽ truyền cảm hứng và tạo điều kiện cho việc khám phá và tiến bộ trên toàn thế giới trong nghiên cứu mô hình ngôn ngữ. Bài báo này chỉ tiết lộ một phần nhỏ khả năng của ChatGPT, và việc chúng tôi chọn từ viết tắt "ICE" cho ICE-GRT là có mục đích. Nó đại diện cho khát vọng của chúng tôi để tăng tốc quá trình 'phá băng' trong nghiên cứu LLM, biểu tượng cho mong muốn truyền cảm hứng cho các nhà nghiên cứu khám phá và phát hiện tiềm năng to lớn của ICE-GRT trên một loạt các tác vụ và mở đường cho những khám phá và tiến bộ mới trong lĩnh vực này.

2 Các Nghiên cứu Liên quan
2.1 Tinh chỉnh Hướng dẫn cho LLM
Những tiến bộ gần đây trong phát triển Mô hình Ngôn ngữ Lớn (LLM) ngày càng tập trung vào tinh chỉnh hướng dẫn (Chiang et al., 2023), một kỹ thuật đang thu hút sự chú ý đáng kể đặc biệt trong lĩnh vực Hỏi đáp (QA) và các miền khác nhau (Zhao et al., 2023; Pan et al., 2023b; Qiu et al., 2020). Nghiên cứu chính trong lĩnh vực này bao gồm các công trình như ALPACA (Taori et al., 2023), Vicuna (Chiang et al., 2023), và (Zhang et al., 2023b), khám phá sự cân bằng giữa tính đa dạng và độ chính xác trong mô hình ngôn ngữ lớn. Hơn nữa, các nghiên cứu như (Sun et al., 2023) đi sâu vào các nguyên tắc của chiến lược QA hiệu quả, trong khi (Zhou et al., 2023) trình bày LIMA, một mô hình sáng tạo cho tương tác ngôn ngữ. Trong lĩnh vực giao diện hội thoại, những đóng góp đáng kể bao gồm việc phát triển OpenAssistant bởi (Köpf et al., 2023; Chiang et al., 2023).

2.2 Học tăng cường từ Phản hồi Con người (RLHF)
Cùng với sự phát triển của LLM, Học tăng cường từ Phản hồi Con người đã nổi lên như một phương pháp quan trọng để cải thiện LLM (Brown et al., 2020; Touvron et al., 2023b). RLHF liên quan đến việc đào tạo mô hình không chỉ trên các tập dữ liệu tĩnh mà còn kết hợp phản hồi con người để hướng dẫn quá trình học tập. Phương pháp này đã đặc biệt hữu ích trong việc phù hợp học tập kiến thức và khai thác với phản hồi con người. Ví dụ, các mô hình như InstructGPT của OpenAI đã sử dụng RLHF để điều chỉnh phản hồi dựa trên sở thích con người, dẫn đến các đầu ra chính xác hơn (Stiennon et al., 2020).

3 Mô hình
Trong phần này, chúng tôi giới thiệu ngắn gọn về mô hình SFT mà chúng tôi đã đào tạo, có tên ICE-Instruct, được thiết kế để cải thiện khả năng khai thác kiến thức chuyên môn của các LLM được đào tạo trước. Tiếp theo, chúng tôi sẽ mô tả chi tiết quá trình đào tạo mô hình phần thưởng, mà chúng tôi đã gọi là ICE-Reward. Cuối cùng, chúng tôi sẽ giới thiệu toàn diện toàn bộ quá trình đào tạo của ICE-GRT, bao gồm một số chiến lược đào tạo quan trọng.

3.1 ICE-Instruct
Mô hình ICE-Instruct được xây dựng dựa trên mô hình Vicuna (Chiang et al., 2023). Bằng cách kết hợp dữ liệu trong miền và dữ liệu đa mục đích trong quá trình tinh chỉnh, nó xuất sắc trong cả các tác vụ chuyên biệt và các tác vụ rộng hơn. Phương pháp này không chỉ duy trì khả năng ngôn ngữ rộng lớn mà còn tăng cường chuyên môn trong các miền cụ thể. Quan trọng, điều này thiết lập một nền tảng vững chắc cho các mô hình RLHF. Tất cả các mô hình actor và critic tiếp theo đều được khởi tạo bằng ICE-Instruct làm xương sống. Về bản chất, ICE-Instruct xác định khả năng ngưỡng dưới của ICE-GRT, đảm bảo một đường cơ sở mạnh mẽ và đáng tin cậy cho những tiến bộ tiếp theo. Để tối đa hóa khả năng áp dụng của mô hình trong các tương tác ngữ cảnh, chúng tôi đã chuyển đổi tất cả dữ liệu thu thập thành các cặp Câu hỏi-Trả lời. Mỗi điểm dữ liệu tuân theo định dạng prompt bắt đầu bằng "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### USER: <INPUT> ASSISTANT: <OUTPUT>", đảm bảo tính nhất quán và liên quan trong các ngữ cảnh.

3.2 ICE-Reward
Tạo Phản hồi và Lấy mẫu: Ban đầu, cho mỗi prompt trong tập dữ liệu đào tạo RLHF, chúng tôi tạo ra năm phản hồi. Những phản hồi này được tạo ra duy nhất bởi mô hình ICE-Instruct của chúng tôi. Bằng cách lấy mẫu từ phân phối đầu ra của mô hình, chúng tôi đảm bảo một phạm vi đa dạng của các câu trả lời được tạo ra, nắm bắt các khía cạnh khác nhau của các phản hồi tiềm năng.

Chú thích Con người và Xếp hạng: Các phản hồi được tạo ra sau đó được chú thích bởi con người. Các nhà chú thích xếp hạng những phản hồi này theo các tiêu chí được xác định trước chi tiết trong phần 4.3. Cụ thể, chúng tôi đã ghi nhãn 20.000 bộ xếp hạng, mỗi bộ chứa năm phản hồi. Từ các phản hồi được xếp hạng, chúng tôi trích xuất hai phản hồi hàng đầu và hai phản hồi thấp nhất cho mỗi prompt. Sau đó, chúng được ghép nối để tạo thành dữ liệu đào tạo. Các cặp bao gồm một phản hồi "tốt hơn" và một phản hồi "tệ hơn", như được xác định bởi chú thích con người. Chiến lược ghép nối này đóng vai trò quan trọng trong việc dạy mô hình những khác biệt giữa các phản hồi chất lượng cao và chất lượng thấp.

Đào tạo Mô hình Phần thưởng: Mục tiêu của việc đào tạo mô hình phần thưởng là phát triển một mô hình có khả năng phân biệt chính xác giữa các phản hồi chất lượng cao và thấp. Cho R(s, a) là hàm phần thưởng, trong đó s đại diện cho prompt đầu vào và a là phản hồi được tạo ra. Mục tiêu của chúng tôi là tối ưu hóa R sao cho nó phù hợp với các phán đoán của con người. Dữ liệu đào tạo bao gồm các cặp (ai, aj) trong đó ai là phản hồi được xếp hạng cao hơn so với aj cho cùng một prompt. Chúng tôi sử dụng hàm mất mát xếp hạng theo cặp, được định nghĩa là:
L(ai, aj) = max(0, margin - R(s, ai) + R(s, aj)).
Hàm mất mát này khuyến khích mô hình gán điểm số cao hơn cho ai so với aj.

--- TRANG 3 ---
Hình 1: Kiến trúc Mô hình ICE-GRT.

Do đó, mô hình phần thưởng được đào tạo học cách gán điểm số cao hơn cho các phản hồi liên quan và phù hợp với ngữ cảnh hơn, theo xếp hạng của con người. Mô hình này tạo thành phần quan trọng nhất của hệ thống của chúng tôi, đảm bảo các phản hồi chất lượng cao, nhận biết ngữ cảnh.

3.3 ICE-GRT
Trong phần này, chúng tôi cung cấp tổng quan toàn diện về từng thành phần liên quan đến ICE-GRT, tận dụng các nguyên tắc của RLHF (Brown et al., 2020) dựa trên PPO (Schulman et al., 2017), cùng với các công thức toán học tương ứng. Hình 1 hiển thị toàn bộ quá trình đào tạo.

Mô hình Actor: Mô hình Actor, được biểu diễn là πθact(a|s), ánh xạ các trạng thái s thành các hành động a. Nó chịu trách nhiệm tạo ra logits actor, là điểm số được gán cho mỗi hành động tiềm năng.

Mô hình Tham chiếu: Mô hình Tham chiếu, ký hiệu là πθref(a|s), đóng vai trò như một điểm chuẩn được đào tạo trước để đánh giá hành vi. Nó cung cấp đường cơ sở để so sánh các đầu ra của mô hình Actor trong suốt quá trình đào tạo.

Mô hình Phần thưởng: Mô hình Phần thưởng, được biểu diễn là R(s, a), gán điểm số phần thưởng dựa trên chất lượng của chuỗi được tạo ra, đánh giá cả hành động a và trạng thái s.

Mô hình Critic: Mô hình Critic, Vθcrt(s), ước lượng giá trị của việc ở trong một trạng thái cụ thể s, do đó tạo ra các giá trị critic hướng dẫn quá trình học tập.

3.3.1 Tính toán Ước lượng Lợi thế Tổng quát (GAE) trong ICE-GRT
Hàm lợi thế, A(s, a), đánh giá lợi ích tương đối của việc thực hiện một hành động cụ thể so với hành động trung bình trong một trạng thái cho trước. Công thức tính Lợi thế là:
A(s, a) = E(R(s, a) + γVθcrt(s') - Vθcrt(s)) (1)
trong đó γ đại diện cho hệ số chiết khấu, s' là trạng thái tiếp theo sau trạng thái hiện tại s, và Vθcrt(s) là hàm giá trị được ước lượng bởi mô hình Critic với trọng số θcrt.

Ước lượng Lợi thế Tổng quát (GAE), tăng cường ước lượng của hàm lợi thế trong RL (Schulman et al., 2015). GAE kết hợp các phương pháp trả về nhiều bước với các ước lượng hàm giá trị để giảm thiểu phương sai trong khi duy trì độ chệch hợp lý. Bản chất của GAE là việc sử dụng tổng có trọng số của các dư lượng Sai khác Thời gian (TD) n-bước:
δA_t = E(Rt+1(s, a) + γVt+1_θcrt(s') - Vt_θcrt(s)) (2)
Ở đây, δA_t đại diện cho dư lượng TD tại thời điểm t. Hơn nữa, hàm lợi thế GAE được tính như sau:
AGAE(s, a) = Σ∞_l=0(γλ)lδA_t+l, trong đó λ∈(0,1).

3.3.2 Học tập Mô hình Actor
Mô hình Actor được cập nhật bằng cách sử dụng mục tiêu Tối ưu hóa Chính sách Gần kề (Schulman et al., 2017), quá trình được tính như sau:
L(θact) = min{
    πθact(a|s)/πθold(a|s) * Aπθold_GAE(s, a),
    clip(πθact(a|s)/πθold(a|s), 1-ε, 1+ε) * Aπθold_GAE(s, a)
}, (3)
trong đó Aπθold_GAE(s, a) là hàm lợi thế được tính bằng chính sách cũ πθold, ε∈(0,1) là một siêu tham số. Thuật ngữ này đảm bảo rằng chính sách Actor đang phát triển không chỉ ổn định trong các cập nhật mà còn phù hợp hoặc phân kỳ như mong muốn từ mô hình cũ.

3.3.3 Tối ưu hóa Chính sách và Đào tạo
Trong giai đoạn cuối, thuật toán PPO tối ưu hóa chính sách của mô hình Actor dựa trên các lợi thế được tính, KL-divergence, và mô hình Actor được cập nhật. Chính sách được cập nhật lặp đi lặp lại để tối đa hóa phần thưởng kỳ vọng, với mục tiêu điều chỉnh hành vi của mô hình Actor gần hơn với các điểm chuẩn đã được thiết lập trong khi cũng đảm bảo học tập hiệu quả và hiệu suất.

3.3.4 Các Chiến lược Đào tạo Quan trọng
Dữ liệu Đào tạo ICE-GRT: Dữ liệu đào tạo của ICE-GRT có nguồn gốc từ mô hình ICE-Instruct và chú thích phản hồi con người cẩn thận. Dữ liệu này không chỉ là một tập hợp các phản hồi mà được thiết kế phức tạp để bao gồm một phạm vi rộng các tình huống. Mỗi prompt trong mô hình ICE-Instruct được phản hồi với một bộ câu trả lời đa dạng, được tạo ra bằng cách lấy mẫu từ phân phối đầu ra của mô hình. Phương pháp này đảm bảo một tập dữ liệu toàn diện và đa dạng, cần thiết cho việc đào tạo mô hình vững chắc. Các phản hồi được tinh chỉnh thêm thông qua quá trình chú thích con người tỉ mỉ, nơi các chuyên gia xếp hạng chúng dựa trên các tiêu chí được xác định trước. Phương pháp nghiêm ngặt này đảm bảo mô hình được đào tạo trên dữ liệu chất lượng cao, được con người xác minh, điều này rất quan trọng đối với khả năng hiểu và áp dụng thông tin phức tạp của mô hình. Thêm chi tiết và so sánh thực nghiệm được mô tả trong Phần 5.2.1.

Mở rộng Quy mô Phần thưởng: Trong ICE-GRT, việc mở rộng quy mô của mô hình phần thưởng là yếu tố quan trọng trong việc xác định tính hiệu quả và hiệu suất tổng thể của đào tạo. Một mô hình phần thưởng lớn hơn, ký hiệu là Rψ(s, a), trong đó ψ đại diện cho các tham số mô hình, có ý nghĩa đáng kể vì một số lý do. Thứ nhất, mô hình phần thưởng lớn hơn có thể nắm bắt tốt hơn các môi trường và hành động phức tạp, cần thiết trong RLHF nơi tín hiệu phần thưởng phải phản ánh chính xác sở thích con người và yêu cầu tác vụ chi tiết. Thứ hai, quy mô lớn hơn của kích thước phần thưởng hỗ trợ tổng quát hóa trên các prompt đa dạng. Điều này rất quan trọng cho hiệu suất nhất quán trong các tình huống khác nhau, đặc biệt trong ICE-GRT.

Kiểm soát KL là cơ chế quan trọng trong PPO, đặc biệt khi đào tạo với phản hồi con người. Một khía cạnh chính của Kiểm soát KL trong bối cảnh này là điều tiết sự phân kỳ giữa mô hình Actor và mô hình Tham chiếu. KL divergence giữa hai mô hình này được giám sát và kiểm soát để đảm bảo rằng quá trình phát triển chính sách tuân thủ chặt chẽ với phản hồi con người. Hơn nữa, đào tạo ICE-GRT bao gồm cơ chế cắt để tránh các cập nhật lớn, có thể làm mất ổn định trong hàm giá trị. Điều này đảm bảo rằng các thay đổi trong hàm giá trị là vừa phải và phản ánh chính xác các cải thiện thực sự như được đánh giá bởi Critic. Hơn nữa, như một biện pháp bổ sung, điều chỉnh Phần thưởng KL giúp giữ mô hình actor trên con đường mong muốn như được xác định bởi phản hồi con người. Điều này điều chỉnh các cập nhật mô hình actor gần hơn với sở thích con người.

Chuẩn hóa Lợi thế tăng cường tính ổn định và hiệu quả học tập trong RLHF dựa trên PPO. Nó điều chỉnh các ước lượng lợi thế, làm cho chúng nhất quán hơn và ít biến đổi hơn. Điều này đặc biệt có lợi trong RLHF, nơi phản hồi con người có thể gây ra các biến đổi không thể đoán trước. Việc chuẩn hóa lợi thế giúp mô hình tập trung vào các tín hiệu học tập có liên quan nhất, dẫn đến hội tụ nhanh hơn và ổn định hơn. Công thức cho Chuẩn hóa Lợi thế được hiển thị như sau:
Âπθ_t = (Aπθ_t - μAπθ)/σAπθ,
trong đó Âπθ_t đại diện cho lợi thế chuẩn hóa tại thời điểm t, Aπθ_t là lợi thế gốc tại thời điểm t, μAπθ là trung bình của lợi thế, σAπθ là độ lệch chuẩn của lợi thế.

4 Chi tiết Thực nghiệm
Quá trình đào tạo của chúng tôi sử dụng sức mạnh của 64 GPU A100, sử dụng chiến lược đa nút, đa GPU để tiến hành ICE-GRT. Các mô hình của chúng tôi được đào tạo và lưu trữ bằng định dạng độ chính xác bf16. Tốc độ học được chọn một cách tinh tế, với tốc độ học actor được đặt ở 5e-6 và tốc độ học critic ở 5e-7. Chúng tôi duy trì phạm vi cắt là 0,2. Hệ số chiết khấu γ được giữ không đổi ở 0,95, đảm bảo sự cân bằng tối ưu trong đào tạo của chúng tôi. Chúng tôi rất hào hứng thông báo việc phát hành sắp tới và mã nguồn mở mô hình ICE-GRT 13B của chúng tôi trên Hugging Face, được thiết kế đặc biệt cho mục đích nghiên cứu khoa học.

4.1 Thu thập Dữ liệu
Đối với kho dữ liệu đào tạo, chúng tôi đã tạo ra một hỗn hợp mới của các tập dữ liệu. Điều này bao gồm một lựa chọn từ các tài nguyên công khai có sẵn, bổ sung bằng dữ liệu trong miền. Chúng tôi đã loại bỏ tất cả thông tin nhạy cảm, bao gồm tên người dùng, địa chỉ email và thông tin cá nhân, để duy trì quyền riêng tư và bảo mật dữ liệu. Về bản chất, tập dữ liệu mà chúng tôi đã chuẩn bị cho mô hình phần thưởng và mô hình RLHF là đa dạng và đa diện, bao gồm nhiều miền. Nó bao gồm dữ liệu liên quan đến các tình huống hỏi đáp công khai và chuyên môn, cũng như các tác vụ liên quan đến điều chỉnh dữ liệu đa ngôn ngữ. Chúng tôi tạo ra 5 phản hồi khác biệt cho mỗi prompt trong bộ sưu tập dữ liệu của mình, sử dụng mô hình ICE-Instruct. Quá trình này bao gồm lấy mẫu từ phân phối đầu ra của mô hình, đảm bảo một phạm vi câu trả lời đa dạng. Để đào tạo tối ưu mô hình phần thưởng, các nhà ghi nhãn dữ liệu đã tiến hành ghi nhãn thủ công cẩn thận về xếp hạng cho 5 phản hồi khác biệt trên 20.000 prompt. Để tăng cường độ chính xác chú thích con người và giảm tính chủ quan giữa các nhà ghi nhãn, mỗi prompt được đánh giá độc lập bởi ba nhà ghi nhãn, thiết lập một quy trình xác minh và xác thực kỹ lưỡng và đáng tin cậy.

4.2 Đánh giá Tác vụ Tổng quát
Đánh giá ICE-GRT của chúng tôi sử dụng khung GPT-Fathom (Zheng et al., 2023) tập trung vào các tác vụ tổng quát công khai. Mục tiêu là đánh giá hiệu suất của ICE-GRT so với các mô hình hiện có và hiểu vị trí của nó trong bối cảnh LLM hiện tại. Chúng tôi sử dụng 12 điểm chuẩn, bao gồm nhiều danh mục khả năng như hiểu ngôn ngữ, lập luận, v.v. Những điểm chuẩn này được chọn cẩn thận để kiểm tra một phạm vi khả năng rộng, từ xử lý ngôn ngữ cơ bản đến các tác vụ giải quyết vấn đề và ra quyết định phức tạp. Trong đánh giá của chúng tôi, chúng tôi duy trì sự phù hợp với các cài đặt được sử dụng trong GPT-Fathom để đảm bảo so sánh công bằng và chính xác. Điều này bao gồm việc sử dụng các định dạng đầu vào, chỉ số đánh giá tương tự và các điều kiện môi trường.

4.3 Đánh giá dựa trên Chú thích Thủ công
Nghiên cứu của chúng tôi kết hợp tiêu chí đánh giá nghiêm ngặt, với sự nhấn mạnh đặc biệt vào chú thích thủ công để đánh giá khả năng của LLM, đặc biệt trong các ứng dụng khác nhau. Tiêu chí đánh giá phản hồi trong 8 danh mục cần thiết, sử dụng cơ chế chấm điểm ưu tiên các khía cạnh quan trọng nhất.

Rõ ràng: Phản hồi nên đơn giản và chính xác, đảm bảo dễ hiểu thông qua ngôn ngữ cụ thể, phù hợp.
Chính xác: Phản hồi được kỳ vọng phù hợp chặt chẽ với các sự kiện đã được xác minh, như được đánh giá bởi các nhà chú thích thủ công. Sự kiện thực tế có thể được xác nhận.
Toàn diện: Đánh giá việc bao gồm tất cả các khía cạnh của câu hỏi, cung cấp chi tiết toàn diện cho việc ra quyết định có thông tin.
An toàn: Tập trung vào việc đảm bảo không có dữ liệu cá nhân nào bị xử lý sai, với kiểm tra thủ công về quyền riêng tư dữ liệu.
Lịch sự: Phản hồi nên đúng đắn về mặt chính trị. ví dụ, bản dạng giới tính, nhóm dân tộc, v.v.
Thoải mái: Phản hồi phải duy trì giọng điệu lịch sự và tôn trọng, chứa từ vựng bao gồm và phản ánh tính đa dạng mọi lúc.
Súc tích: Nhấn mạnh tính ngắn gọn trong phản hồi, không làm tổn hại đến sự rõ ràng hoặc độ chính xác.
Ngữ cảnh: Phản hồi phải liên quan đến chủ đề và có liên quan đến câu hỏi.

Bảng 2 hiển thị trọng số và điểm của từng danh mục để đánh giá các tiêu chí này một cách chính xác, đảm bảo chất lượng và mức độ liên quan của phản hồi.

Đánh giá    Tích cực    Trung tính    Tiêu cực    Trọng số
Rõ ràng         5          2           0           6
Chính xác       5          2           0           6
Toàn diện       5          2           0           6
An toàn         5          2           0           3
Lịch sự         5          2           0           3
Thoải mái       5          2           0           3
Súc tích        5          2           0           1
Ngữ cảnh        5          2           0           1

Bảng 2: Tiêu chí Đánh giá dựa trên Chú thích Thủ công.

5 Kết quả và Phân tích
5.1 Kết quả
Điểm Benchmarks trên Tác vụ Tổng quát: Phân tích của chúng tôi tập trung vào hiệu suất của ICE-GRT 13B, so với các mô hình khác trong các danh mục dung lượng tương tự và cao hơn. Như được hiển thị trong Bảng 1, mô hình ICE-GRT 13B của chúng tôi thể hiện những cải thiện đáng kể so với LLaMa, Llama 2, Vicuna 13B và LLaMa 30B trong cả đào tạo trước và SFT trên các điểm chuẩn tổng quát khác nhau, như MMLU (Hendrycks et al., 2021), AGIEval (Zhong et al., 2023), BBH (Srivastava et al., 2022), ARC (Xu et al., 2023), HellaSWAG (Zellers et al., 2019), RACE (Lai et al., 2017), v.v. Nó thể hiện những tiến bộ đáng chú ý trong hiểu ngôn ngữ tổng quát và các tác vụ lập luận, cho thấy khả năng hiểu và lập luận được tăng cường. Đáng chú ý, mô hình ICE-GRT 13B đã thu hẹp đáng kể khoảng cách với mô hình đào tạo trước Llama 2 70B lớn hơn nhiều. So sánh này nhấn mạnh tính hiệu quả của ICE-GRT, bù đắp cho kích thước mô hình nhỏ hơn bằng khả năng tổng quát hóa nhiều hơn. Thành công của các mô hình ICE-GRT cho thấy rằng phương pháp học, có thể bao gồm các thành phần của phản hồi con người và điều chỉnh, đóng góp đáng kể vào khả năng hiểu và phản hồi các prompt phức tạp của mô hình, một yếu tố không chỉ phụ thuộc vào kích thước mô hình.

Điểm Chú thích Con người trên Tác vụ Trong Miền: Trong đánh giá trong miền được trình bày trong Bảng 3, ICE-GRT rõ ràng vượt trội hơn Llama2 SFT 13B và ICE-Instruct 13B trên một số khía cạnh quan trọng. Đáng chú ý, ICE-GRT đạt điểm số cao nhất về độ rõ ràng (98,1%), độ chính xác (97,0%), và tính toàn diện (92,9%), nhấn mạnh khả năng xuất sắc trong việc cung cấp các phản hồi chính xác, toàn diện và dễ hiểu. Trong khi nó đạt điểm thấp hơn một chút về an toàn và thoải mái so với các đối tác, nó vẫn duy trì tiêu chuẩn cao trong những lĩnh vực này. Điểm số tổng thể 95,5% cho ICE-GRT là minh chứng cho hiệu suất vượt trội của nó, vượt xa Llama2 SFT 13B (86,3%) và ICE-Instruct 13B (87,3%). Hiệu suất mạnh mẽ này trên nhiều chỉ số xác nhận các tuyên bố giới thiệu về khả năng của ICE-GRT, đặc biệt trong việc xử lý các tác vụ chuyên môn với mức độ sâu sắc và chính xác không thấy ở các mô hình hiện tại.

Llama2 sft    ICE-Instruct    ICE-GRT
Rõ ràng         95,9%           88,5%         98,1%
Chính xác       77,4%           84,44%        97,0%
Toàn diện       64,8%           71,11%        92,9%
An toàn         96,6%           100%          92,2%
Lịch sự         100%            95,9%         100%
Thoải mái       96,6%           98,1%         92,22%
Súc tích        95,1%           93,33%        91,8%
Ngữ cảnh        98,8%           94,0%         98,1%
Điểm tổng thể   86,3%           87,3%         95,5%

Bảng 3: Đánh giá điểm số được đánh giá bởi con người cho các Mô hình Ngôn ngữ Lớn trong miền.

5.2 Phân tích Chi tiết
5.2.1 Tầm quan trọng của Dữ liệu Đào tạo ICE-GRT
Trong việc đào tạo ICE-GRT, chúng tôi sử dụng hai tập dữ liệu khác biệt cho RLHF. Tập dữ liệu đầu tiên được tạo ra duy nhất bởi mô hình ICE-Instruct của chúng tôi. Đối với mỗi prompt, năm phản hồi đa dạng được tạo ra bằng cách lấy mẫu từ đầu ra mô hình. Những phản hồi này sau đó được chú thích bởi con người, nơi các nhà chú thích xếp hạng chúng theo tiêu chí được xác định trước. Tập dữ liệu thứ hai có nguồn gốc từ GPT-4-LLM (Peng et al., 2023). Nó bao gồm các phản hồi được xếp hạng từ GPT-4 và GPT-3.5, với các xếp hạng được tự động đánh giá bởi GPT-4. Những phát hiện của chúng tôi tiết lộ sự khác biệt hiệu suất đáng kể giữa các mô hình được đào tạo với những tập dữ liệu này, mặc dù chúng tôi thấy rằng các xu hướng điểm phần thưởng tương tự trong quá trình đào tạo ICE-GRT được hiển thị trong Hình 2a. Mô hình ICE-GRT, được đào tạo với tập dữ liệu chú thích con người của chúng tôi, thể hiện hiệu suất vượt trội trên các tác vụ tổng quát và tác vụ chuyên môn. Như được hiển thị trong Hình 2b, trên tác vụ Natural Question, mô hình ICE-GRT vượt trội hơn ICE-Instruct 4%. Khoảng cách này tăng lên khoảng 9,79% trên Web Questions và 17,17% trên điểm chuẩn LAMBADA. Tuy nhiên, khi chúng tôi sử dụng Tập dữ liệu GPT-4-LLM trên ICE-GRT, chúng tôi quan sát thấy rằng kết quả rất gần với ICE-Instruct, chỉ tăng 0,89% trong Natural Questions.

Một khía cạnh chính của thành công ICE-GRT là sự tập trung vào 'tăng cường kiến thức'. Quá trình này xây dựng dựa trên "khai thác kiến thức" trong ICE-Instruct, cho phép mô hình phù hợp tốt hơn với sở thích ngôn ngữ con người. Phương pháp này đảm bảo tính nhất quán và mức độ liên quan trong dữ liệu đào tạo, điều này rất quan trọng để mô hình có thể xây dựng và phát triển cấu trúc kiến thức hiện có một cách hiệu quả. Các nguồn dữ liệu bên ngoài, bất chấp tính đa dạng tiềm năng, không thể phù hợp hoàn hảo với cấu trúc kiến thức của mô hình. Việc sử dụng dữ liệu được tạo ra bởi ICE-Instruct đảm bảo sự tăng cường kiến thức tự nhiên và hiệu quả, như được quan sát trong ICE-GRT.

(a) So sánh điểm phần thưởng giữa các dữ liệu RLHF khác nhau.
(b) Hiệu suất benchmark giữa các mô hình khác nhau.

Hình 2: Ảnh hưởng của các dữ liệu đào tạo khác nhau.

5.2.2 ICE-GRT Mạnh mẽ trên Tác vụ Tổng quát
Mô hình ICE-GRT thể hiện sức mạnh đặc biệt trong các tác vụ dựa trên hiểu ngôn ngữ và lập luận. Ví dụ, như được hiển thị trong Hình 3a và Hình 3c, ICE-GRT 13B thể hiện khả năng vững chắc trong RACE, ARC, BBH, và GSM8K. Mô hình của chúng tôi đã đạt được hiệu suất tốt nhất trong những tác vụ này trong số các mô hình cùng kích thước. Hơn nữa, mô hình ICE-GRT 13B cho thấy hiệu suất tuyệt vời trong các lĩnh vực như "marketing" và "college-biology", như được mô tả trong Hình 3b. Đáng chú ý, mô hình ICE-GRT 13B vượt trội hơn Llama 2 70B trong một số điểm chuẩn AGIEval, đặc biệt trong các tác vụ yêu cầu hiểu ngôn ngữ và lập luận, như "gaokao-chinese" như được thấy trong Hình 3d. Hiệu suất vượt trội này bắt nguồn từ khả năng hiểu ngữ cảnh và sinh tạo được tăng cường kiến thức của ICE-GRT.

5.2.3 Tính hiệu quả của Chuẩn hóa Lợi thế
Việc tích hợp Chuẩn hóa Lợi thế và Mở rộng Quy mô Phần thưởng cải thiện đáng kể ICE-GRT. Những chiến lược này đóng góp vào việc cải thiện hiệu suất đào tạo và hiệu suất mô hình tốt hơn, thể hiện tầm quan trọng của chúng trong bối cảnh RLHF. Áp dụng Chuẩn hóa Lợi thế, ổn định học tập bằng cách chuẩn hóa các ước lượng lợi thế, dẫn đến cải thiện trong điểm chuẩn Natural Question so với đường cơ sở ICE-GRT. Như được hiển thị trong Hình 4, chiến lược này rất quan trọng để tăng cường độ nhạy cảm của mô hình với những tinh tế của phản hồi con người, dẫn đến kết quả học tập hiệu quả hơn.

5.3 Nghiên cứu Tình huống trên Tác vụ Chuyên môn
Chúng tôi cung cấp một phân tích so sánh các phản hồi được tạo ra bởi các mô hình khác nhau, cụ thể là ICE-Instruct 13B, 33B, và ICE-GRT 13B, tiết lộ các mức độ nhạy cảm và sáng tạo khác nhau trong việc giải quyết tuân thủ chính sách quảng cáo và viết lại để tuân thủ. Như được hiển thị trong Bảng 4, trong khi ICE-Instruct 13B có cách tiếp cận trực tiếp hơn và ít thận trọng hơn, ICE-Instruct 33B và ICE-GRT 13B thể hiện sự gia tăng tiến bộ trong nhận thức chính sách và tuân thủ sáng tạo.

ICE-GRT, đặc biệt, cho thấy sự hiểu biết toàn diện về quy định quảng cáo và tầm quan trọng của các tuyên bố được chứng minh, phản ánh khả năng tiên tiến trong giao tiếp tinh tế và có trách nhiệm. Trong trường hợp đầu tiên, ICE-GRT thể hiện độ nhạy cảm cao nhất đối với tuân thủ chính sách, làm nổi bật rủi ro vi phạm chính sách tuyên bố cường điệu, đặc biệt nếu sản phẩm được tiếp thị là "100% tự nhiên" mà không có bằng chứng đầy đủ. Nó nhấn mạnh nhu cầu quảng cáo dựa trên bằng chứng và tuân thủ quy định. Trong trường hợp thứ hai, ICE-GRT cung cấp bản viết lại chi tiết và thận trọng nhất, đảm bảo tuân thủ các chính sách quảng cáo. Nó tập trung vào các thành phần tự nhiên, không có hóa chất có hại, và tính phù hợp cho tất cả phụ nữ và độ tuổi, trong khi tránh các tuyên bố cường điệu.

Trong phần này, chúng tôi chỉ trình bày một phần nhỏ khả năng của mô hình, tập trung chủ yếu vào tác vụ trong miền của kiểm duyệt quảng cáo. Tuy nhiên, phạm vi của mô hình ICE-GRT mở rộng xa hơn chức năng đơn lẻ này. Trong các phụ lục, chúng tôi thể hiện sự thành thạo của nó trên vô số tác vụ chuyên môn. Những tác vụ này bao gồm, nhưng không giới hạn, Tạo thơ, Văn bản sang Bảng, Đối thoại Đa vòng (Phụ lục A), Tạo Phản hồi Hóa học (Phụ lục B), Tạo mã (Phụ lục C), Tạo Văn bản Quảng cáo, Ghi nhãn Văn bản (Phụ lục D), và Phản hồi Đa ngôn ngữ (Phụ lục E), v.v. Việc chọn từ viết tắt "ICE" cho ICE-GRT là có chủ ý. Nó đại diện cho khát vọng của chúng tôi để thúc đẩy một khoảnh khắc 'phá băng' trong nghiên cứu LLM. Điều này phản ánh hy vọng của chúng tôi khuyến khích các nhà nghiên cứu khám phá và nhận ra những khả năng rộng lớn của ICE-GRT trong một loạt các tác vụ. Chúng tôi nhằm mục đích mở đường cho những khám phá và tiến bộ mới trong lĩnh vực này, thể hiện rằng khả năng của mô hình của chúng tôi vừa rộng lớn và đa dạng cũng như sáng tạo.

6 Kết luận
Mô hình ICE-GRT đại diện cho một bước nhảy vọt đáng kể trong lĩnh vực LLM, đặc biệt trong việc tăng cường hiệu suất chuyên môn. Tận dụng các nguyên tắc của Học tăng cường từ Phản hồi Con người, ICE-GRT thể hiện khả năng đặc biệt trong cả các tác vụ tổng quát và trong miền, vượt trội hơn các mô hình tiêu chuẩn về độ chính xác và chiều sâu. Hơn nữa, mô hình của chúng tôi có khả năng mạnh mẽ để tạo ra các phân tích chi tiết về lý do đằng sau câu trả lời. Nghiên cứu của chúng tôi phát hiện ra một số khía cạnh của RLHF, cung cấp thông tin về các phương pháp đào tạo hiệu quả và làm nổi bật tầm quan trọng của các yếu tố như Dữ liệu Phù hợp, Mở rộng Quy mô Phần thưởng, Kiểm soát KL, v.v. Các giai đoạn đào tạo của ICE-GRT, bao gồm học tập kiến thức, khai thác và tăng cường, đóng góp vào khả năng tiên tiến trong việc phù hợp với sở thích con người. Chúng tôi hy vọng rằng ICE-GRT sẽ tăng tốc quá trình "phá băng" trong nghiên cứu LLM, khuyến khích khám phá thêm.

Lời cảm ơn
Chúng tôi đánh giá cao sâu sắc Youlong Cheng, Guokun Lai, Yingtong Bu, Zheng Zhang, Fan Qu vì sự giúp đỡ của họ ở giai đoạn đầu của dự án này. Hơn nữa, chúng tôi bày tỏ lòng biết ơn đối với Hanzhi Zhou, Yijie Zhu, Xuan Zou vì sự hỗ trợ kỹ thuật của họ để xây dựng các thành phần chính của cơ sở hạ tầng. Chúng tôi gửi lời cảm ơn đến Hang Wu, Ruoqi Zhang và Ruohong Zhang vì những thảo luận sâu sắc của họ đã đóng góp vào bài báo này. Hơn nữa, chúng tôi cảm ơn các nhà bình duyệt ẩn danh vì những đề xuất có giá trị của họ.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được duy trì nguyên như bản gốc]

--- TRANG 11 ---
A Tạo thơ, Văn bản sang Bảng, Đối thoại Đa vòng.
Phần phụ lục này trình bày khả năng đa diện của ICE-GRT khi điều hướng qua ba tác vụ khác biệt: Tạo thơ, Văn bản sang Bảng, và Đối thoại Đa vòng. Mỗi phần thể hiện một khía cạnh độc đáo của sự thành thạo của ICE-GRT. Trong Tạo thơ, chúng tôi khám phá khả năng sáng tạo để tạo ra nội dung biểu cảm. Tác vụ Văn bản sang Bảng thể hiện sự thành thạo trong cấu trúc dữ liệu và trình bày. Cuối cùng, phần Đối thoại Đa vòng thể hiện kỹ năng tham gia vào diễn ngôn dự đoán và phân tích. Những tác vụ này một cách tập thể nhấn mạnh khả năng mạnh mẽ của ICE-GRT, minh chứng cho tiềm năng của nó trong các ứng dụng ngôn ngữ học và phân tích đa dạng.

Câu hỏi    Phản hồi
Sáng tác một bài thơ để phân tích nền kinh tế Mỹ với thái độ tích cực.    Nền kinh tế Mỹ mạnh mẽ và sôi động,
Một ngọn hải đăng hy vọng và tiến bộ rực rỡ.
Từ bờ này đến bờ kia, doanh nghiệp thịnh vượng,
Sự đổi mới và thịnh vượng đang gia tăng.
Từ các gã khổng lồ công nghệ ở Thung lũng Silicon,
Đến những công ty khởi nghiệp nhỏ ở mọi thành phố,
Tinh thần doanh nghiệp Mỹ,
Thúc đẩy tăng trưởng và tạo ra vô số việc làm.
Từ Phố Wall đến Phố Main,
Các nhà đầu tư và người tiêu dùng,
Niềm tin vào tương lai cao,
Khi nền kinh tế tiếp tục mở rộng và phát triển.
Với thái độ tích cực và tinh thần làm được,
Nền kinh tế Mỹ sẽ tiếp tục phát triển và thịnh vượng,
Mang lại cơ hội và thịnh vượng,
Cho tất cả những ai chăm chỉ và phấn đấu thành công.

Bảng 5: Nghiên cứu Tình huống trong tác vụ Tạo thơ.

Câu hỏi    Phản hồi
Sau đó tạo một bảng để hiển thị các chỉ số kinh tế chính của người Mỹ, chẳng hạn như GPP, PPP, CPI, PPI, v.v.    Đây là bảng hiển thị một số chỉ số kinh tế chính của Hoa Kỳ:
[Bảng với các chỉ số kinh tế được hiển thị]
Lưu ý: Những giá trị này có thể thay đổi và có thể không phản ánh thông tin cập nhật nhất. Quan trọng là tham khảo các nguồn chính thức mới nhất để có dữ liệu kinh tế hiện tại nhất.

Bảng 6: Nghiên cứu Tình huống trong tác vụ Văn bản sang Bảng.

Câu hỏi    Phản hồi
Dự đoán nền kinh tế Mỹ năm tới dựa trên dữ liệu trên và đưa ra lý do của bạn.    Là một mô hình ngôn ngữ AI, tôi không thể dự đoán tương lai với sự chắc chắn, nhưng tôi có thể cung cấp một số thông tin dựa trên dữ liệu kinh tế có sẵn tính đến ngày cắt kiến thức của tôi (tháng 9 năm 2021). Hoa Kỳ đã trải qua sự phục hồi kinh tế mạnh mẽ vào năm 2021, với tăng trưởng GDP dự kiến khoảng 6% trong năm. Sự tăng trưởng này được thúc đẩy bởi sự kết hợp của các yếu tố, bao gồm chi tiêu tiêu dùng mạnh mẽ, tăng đầu tư kinh doanh, và sự phục hồi trong thương mại quốc tế. Chỉ số Giá Tiêu dùng (CPI) cũng tăng đều đặn trong suốt năm 2021, phản ánh áp lực lạm phát cao hơn trong nền kinh tế.

Nhìn về năm tới (2022), có một số yếu tố có thể tác động đến nền kinh tế Mỹ. Một yếu tố chính cần theo dõi sẽ là quỹ đạo của đại dịch COVID-19, vì đại dịch đang diễn ra tiếp tục đặt ra rủi ro cho tăng trưởng kinh tế và sự ổn định. Ngoài ra, Cục Dự trữ Liên bang Mỹ đã báo hiệu rằng họ có thể bắt đầu giảm dần việc mua tài sản trong những tháng tới, điều này có thể tác động đến thị trường tài chính và lãi suất.

Giả sử sự phục hồi kinh tế tiếp tục và việc bình thường hóa dần các chính sách tiền tệ và tài khóa, có thể nền kinh tế Mỹ có thể trải qua tăng trưởng khiêm tốn vào năm 2022. Tuy nhiên, cũng có những rủi ro và không chắc chắn có thể tác động đến triển vọng, bao gồm khả năng có những đợt dịch mới, rủi ro địa chính trị, và sự gián đoạn chuỗi cung ứng đang diễn ra. Nhìn chung, triển vọng cho nền kinh tế Mỹ năm 2022 sẽ phụ thuộc vào nhiều yếu tố, và quan trọng là các nhà hoạch định chính sách và doanh nghiệp phải duy trì sự cảnh giác và thích ứng trước những điều kiện kinh tế đang phát triển.

Bảng 7: Nghiên cứu Tình huống trong tác vụ Đối thoại Đa vòng.

--- TRANG 12 ---
B Tạo Phản hồi Hóa học.
Câu hỏi    Phản hồi
Hãy giúp tôi với các phương trình hóa học, chẳng hạn như các phương trình liên quan đến H2O.    Đây là một số phương trình hóa học thường được sử dụng liên quan đến H2O:
1. Đốt cháy khí hydro (H2):
2H2 + O2 -> 2H2O
2. Phân hủy nước (H2O) thành khí hydro (H2) và khí oxy (O2) bằng điện:
2H2O -> 2H2 + O2
3. Phản ứng của khí hydro (H2) và khí oxy (O2) để tạo thành nước (H2O):
2H2 + O2 -> 2H2O
Những phương trình này chỉ là một số ví dụ về cách H2O tham gia vào các phản ứng hóa học khác nhau. Có nhiều phương trình và phản ứng khác liên quan đến H2O, tùy thuộc vào bối cảnh và điều kiện cụ thể. Quan trọng là phải hiểu hóa học và vật lý cơ bản của những phương trình và phản ứng này để hiểu đầy đủ các tác động và ứng dụng của chúng. Ngoài ra, quan trọng là phải tuân theo các quy trình và thủ tục an toàn thích hợp khi xử lý hóa chất và thực hiện thí nghiệm, vì một số phản ứng có thể nguy hiểm hoặc thậm chí nguy hiểm nếu không được kiểm soát và giám sát đúng cách.

Bảng 8: Nghiên cứu Tình huống trong tác vụ Tạo Hóa học.

--- TRANG 13 ---
C Tạo mã.
Câu hỏi    Phản hồi
Viết một hàm để đếm số lần xuất hiện của mỗi từ, sau đó viết một hàm khác để sắp xếp danh sách này, nhưng bạn không thể sử dụng bất kỳ thư viện nào trong python.    Chắc chắn, đây là một hàm Python đếm số lần xuất hiện của mỗi từ trong một chuỗi cho trước và trả về một từ điển với từ làm khóa và số lần đếm của nó làm giá trị:
```python
def count_words(text):
    words = text.lower().split()
    word_counts = {}
    for word in words:
        if word in word_counts:
            word_counts[word] += 1
        else:
            word_counts[word] = 1
    return word_counts
```
Để sắp xếp từ điển đếm từ, bạn có thể sử dụng hàm sorted() cùng với hàm lambda so sánh các giá trị của các mục từ điển:
```python
def sort_word_counts(word_counts):
    sorted_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
    return sorted_counts
```
Đây là cách bạn có thể sử dụng những hàm này cùng nhau để đếm số lần xuất hiện của mỗi từ trong một chuỗi và sắp xếp số lần đếm từ:
```python
text = "The quick brown fox jumped over the lazy dog. The dog barked at the fox."
word_counts = count_words(text)
sorted_word_counts = sort_word_counts(word_counts)
print(sorted_word_counts)
```
Điều này sẽ xuất ra:
[('fox', 2), ('dog', 2), ('the', 2), ('quick', 1), ('brown', 1), ('jumped', 1), ('over', 1), ('lazy', 1), ('barked', 1)]

Bảng 9: Nghiên cứu Tình huống 1 trong tác vụ Tạo mã.

--- TRANG 14 ---
Câu hỏi    Phản hồi
Bằng cách sử dụng các từ trên làm kho dữ liệu, hãy thử triển khai mã word2vec dựa trên lý thuyết của nó    Chắc chắn, đây là một triển khai word2vec trong Python dựa trên lý thuyết được mô tả trong bài báo "Word2Vec: Negative Sampling for Fast and Accurate Learning of Word Vectors" bởi Mikolov et al. Triển khai này sử dụng thuật toán Skip-Gram để đào tạo mạng nơ-ron dự đoán các từ gần đó cho một từ mục tiêu, và ngược lại. Các vector từ kết quả có thể được sử dụng cho các tác vụ xử lý ngôn ngữ tự nhiên khác nhau, chẳng hạn như phân loại văn bản, dịch ngôn ngữ, và phân tích tình cảm.
[Mã Python được cung cấp cho triển khai word2vec]

Bảng 10: Nghiên cứu Tình huống 2 trong tác vụ Tạo mã.

--- TRANG 15 ---
D Tạo văn bản Quảng cáo, Ghi nhãn văn bản, Đối thoại Đa vòng.
[Các bảng 11, 12, 13 tiếp tục với các ví dụ về tạo văn bản quảng cáo, ghi nhãn văn bản, phản hồi đa ngôn ngữ và câu hỏi mẫu trong tác vụ kiểm duyệt quảng cáo]
