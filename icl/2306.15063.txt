# 2306.15063.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2306.15063.pdf
# File size: 1003116 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pretraining task diversity and the emergence of
non-Bayesian in-context learning for regression
Allan Raventós∗Mansheej Paul∗Feng Chen Surya Ganguli
Stanford University
{aravento, mansheej, fengc, sganguli}@stanford.edu
Abstract
Pretrained transformers exhibit the remarkable ability of in-context learning (ICL):
they can learn tasks from just a few examples provided in the prompt without updat-
ing any weights. This raises a foundational question: can ICL solve fundamentally
newtasks that are very different from those seen during pretraining? To probe this
question, we examine ICL’s performance on linear regression while varying the
diversity of tasks in the pretraining dataset. We empirically demonstrate a task
diversity threshold for the emergence of ICL. Below this threshold, the pretrained
transformer cannot solve unseen regression tasks, instead behaving like a Bayesian
estimator with the non-diverse pretraining task distribution as the prior. Beyond
this threshold, the transformer significantly outperforms this estimator; its behavior
aligns with that of ridge regression, corresponding to a Gaussian prior over all
tasks , including those not seen during pretraining. Thus, when pretrained on data
with task diversity greater than the threshold, transformers canoptimally solve
fundamentally new tasks in-context. Importantly, this capability hinges on it de-
viating from the Bayes optimal estimator with the pretraining distribution as the
prior. This study also explores the effect of regularization, model capacity and task
structure and underscores, in a concrete example, the critical role of task diversity,
alongside data and model scale, in the emergence of ICL.
1 Introduction
Pretrained transformers (PTs) can learn new tasks from just a few examples provided in the prompt
without taking any gradient steps on those examples [ 1]. This ability, called in-context learning
(ICL) , has unlocked the widspread use of language models by making it efficient to adapt general
purpose models to bespoke tasks without explicit training. Though remarkable, what makes ICL
mysterious, and potentially harmful [ 2], is that the learning algorithm implemented by the PT in its
forward pass is not built into its architecture or training process; instead it emerges from pretraining
on large-scale data with a next token prediction objective. This raises a foundational question: can
ICL really solve fundamentally newtasks that are very different from those seen during pretraining?
If so, what learning algorithm does ICL implement? To answer these questions, we need to better
understand how the different ingredients that go into pretraining influence this ability.
Towards this end, we explore how the diversity of tasks in the pretraining data affects the emergence
of ICL. Prior work [ 3] has proposed that ICL works by performing Bayesian inference. During
pretraining, transformers learn a prior over latent tasks represented in the pretraining data. When
prompted with examples at inference time, they "retrieve" relevant pretraining tasks and generate
subsequent tokens from the posterior distribution conditioned on the query and inferred tasks. This
suggests that ICL performance on a new task is influenced by its similarity to tasks implicitly learned
during pretraining. However, the distribution of tasks in our pretraining data, TPretrain , is usually a
∗Equal Contribution. Code released at https://github.com/mansheej/icl-task-diversity
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2306.15063v2  [cs.LG]  8 Nov 2023

--- PAGE 2 ---
limited and unrepresentative subsample of the ideal distribution of tasks, TTrue, that we want our
model to be capable of learning in-context. For instance, TTruecould be the set of all instructions we
want an A.I. assistant to follow. But, large-scale language modeling datasets [ 4,5] used to pretrain
these models contain very few examples correctly formatted for ICL. Instruction finetuning (IFT)
datasets [ 6–11] designed to ameliorate this are expensive to collect and thus contain tasks from just a
few domains. Under the Bayesian framework, this distribution mismatch would cause the Bayesian
estimator with a prior over the limited pretraining tasks, TPretrain , to perform suboptimally on tasks
that are very different from those seen during pretraining. This motivates our question: can a model
pretrained on a dataset with low task diversity nevertheless learn new, unseen tasks?
For general purpose language modeling, the size and complexity of TPretrain and the vague specification
ofTTruemake this question challenging to analyze. So, following recent work [ 12–14], we study
ICL for linear regression. Here, a task is a linear regression problem with a given latent regression
vector; the PT must predict the target for a new data point from examples of data-target pairs provided
in the prompt. Prior work [ 13] has shown that transformers that see an unlimited number of latent
regression vectors during pretraining learn to perform ridge regression with the Bayes optimal ridge
parameter. We instead consider the case where the pretraining task distribution, TPretrain , contains a
limited and finite set of latent regression vectors (see Section 2 for details). To evaluate its ability to
learn new tasks, the PT is tested on the ideal task distribution, TTrue, which is a Gaussian distribution
over alllatent regression vectors. Studying this setting has two advantages: first, we can directly
vary the task diversity in TPretrain by changing the number of unique latent regression vectors seen
during pretraining. Second, we can calculate the optimal estimator that minimizes the pretraining
loss—the Bayesian estimator with prior TPretrain —as well as the optimal estimator for all tasks—the
Bayesian estimator with prior TTrue. This allows us to interpret the behavior of the PT by comparing
its predictions to those of the optimal estimators under either task distribution. In our work, we vary
the pretraining task diversity and probe the PT’s ability to learn fundamentally new tasks in-context:
does it behave like the optimal estimator for TPretrain and perform suboptimally on tasks from TTrue, or
does it align with the optimal estimator for TTruewhich can solve new, unseen tasks?
Contributions. Our contributions are as follows:
•We find that a transformer pretrained on data with low task diversity behaves like the Bayesian
estimator with prior TPretrain ; it performs optimally on pretraining tasks but cannot learn new tasks
in-context. However, as pretraining task diversity increases, the PT deviates from this Bayesian
estimator, significantly outperforming it on new tasks, and at a large but still finite number of
pretraining tasks, the PT’s performance closely matches that of the optimal estimator on TTrue.
•We identify a task diversity threshold for the emergence of ICL. Below this threshold, increasing the
pretraining dataset size while keeping task diversity constant biases the PT towards the pretraining
task distribution. Conversely, beyond this threshold, increasing the dataset size without increasing
its task diversity improves the PT’s performance on new, unseen tasks. This suggests that the PT’s
behavior undergoes a sharp algorithmic phase transition in the limit of many examples per task,
aligning with the optimal estimators on TPretrain before the threshold and on TTrueafter it. We also
examine this transition from the perspective of learning dynamics.
•We empirically show that increasing the task dimension at fixed SNR increases the task diversity
threshold. However, the scaling of the PT’s error with dimension is vastly superior to that of
the optimal Bayesian estimator for TPretrain ; at a task diversity that is beyond the threshold at all
dimensions we consider, the PT remains near-optimal with increasing dimension, whereas the
optimal estimator for TPretrain grows progressively less similar to the optimal estimator for TTrue.
•We show that increasing weight decay significantly decreases the task diversity threshold while
increasing number of layers or embedding size increases the task diversity threshold. This elucidates
the effect of regularization and model capacity on the emergence of ICL.
Overall these contributions suggest that the emergence of in-context learning in pretrained transform-
ers cannot be fully explained by a theory of Bayesian inference on the pretraining distribution.
2 Problem setup
ICL of linear regression (schematic in Fig. 1). Each ICL taskcorresponds to a latent D-dimensional
regression vector, w∈RD. At inference time, the transformer takes as input a sequence of Kdata-
target pairs, (x1, y1, ...,xK, yK), which are the in-context examples corresponding to this single task
2

--- PAGE 3 ---
TransformerFigure 1: Schematic for ICL of linear regression. (Left) A task corresponds to a latent regression
vector, w(purple line). (x1, y1, ...,xK, yK)(black circles) is a sequence of in-context examples for
this task. (Right) The PT, fθ, takes this as input and generates Koutputs. The kth output, fθ(Sk), is
the prediction for the target of xkand depends only on the context Sk= (x1, y1, ...,xk−1, yk−1,xk).
w. Fork∈ {1, ..., K}, the data are drawn i.i.d. from a D-dimensional standard normal distribution,
xk∼ N(0,ID), and the targets are scalars given by yk=w⊺xk+εk. The εk’s are noise scalars
drawn i.i.d. from a normal distribution with mean 0 and variance σ2,εk∼ N(0, σ2). Letfθbe the
PT with parameters θ. Since we use a decoder-only transformer with a causal attention mask, for
eachk∈ {1, ..., K}, the transformer sees the context Sk= (x1, y1, ...,xk−1, yk−1,xk)and based
on this context, it makes a prediction, fθ(Sk), for the target of xk. Thus, in each forward pass, the PT
solves Klinear regression problems each with the same latent regression vector but an increasing
number of in-context examples.
Pretraining. The transformer is pretrained to minimize the next token prediction mean squared error
(MSE) on sequences of data and target pairs. The latent regression vector for each sequence is drawn
from the pretraining task distribution ,TPretrain . This distribution has limited diversity as it is the
uniform distribution over a finite set of Mtasks,TPretrain =U{w(1), ...,w(M)}. Each task in TPretrain
is drawn i.i.d from a D-dimensional standard normal distribution, w(i)∼ N(0,ID), i∈1, ..., M .
By increasing the number of tasks, M, inTPretrain , we can increase the diversity of the pretraining
data. Since the transformer makes a prediction for every data point in the sequence, its loss, LTPretrain,
is just the MSE for each prediction, averaged over the predictions in the sequence:
LTPretrain(θ) = E
w∼T Pretrain
x1,...,xK∼N (0,ID)
ε1,...,εK∼N (0,σ2)"
1
KKX
k=1(fθ(Sk)−yk)2#
. (1)
Evaluation. We evaluate the PT’s performance on tasks seen during pretraining by computing
LTPretrain using Eq. (1) but with new samples of data and noise. Since these are new instances of the
task with new in-context examples, this evaluation corresponds to the test error of the PT. For a PT
to successfully perform ICL of linear regression on new tasks , it must accurately predict the targets
from the in-context examples for any task drawn from an ideal task distribution ,TTrue, over all
latent regression vectors; in our case TTrue=N(0,ID). We evaluate the PT’s performance on new
tasks by computing LTTrue, which follows Eq. (1) but where the tasks are sampled from the ideal task
distribution: w∼ T Truein the expectation.
Comparing the PT to optimal estimators. An advantage of studying ICL of linear regression is that
we can calculate the ground truth optimal estimators that minimize the loss, LT, in Eq. (1) for both
task distributions, TPretrain andTTrue. The optimal estimator for the kth prediction, ˆyT
k, that minimizes
thekth term in the sum in LTis the Bayesian estimator with Tas the prior. This is given by the
posterior mean of ykconditioned on the context: ˆyT
k=ET,εk[yk|Sk], where the expectation is over
the task distribution, T, and the noise, εk(Appendix A.1).
For task distribution TPretrain =U{w(1), ...,w(M)}, the discrete minimum mean squared error
(dMMSE) estimator is optimal. It is given by ˆydMMSE
k = ˆwdMMSE
k⊺xkwhere ˆwdMMSE
1 =
3

--- PAGE 4 ---
0.10.20.30.4Pretrain
MSE/D
105
103
101
PT,dMMSE
104
103
102
PT,Ridge
202428212216220
# Pretraining T asks0.51.01.5True
MSE/D
Ridge
dMMSE
PT
22242628210
# Pretraining T asks105
103
101
PT,dMMSE
Batch Size
256
512
1024
212214216218220
# Pretraining T asks104
103
102
PT,Ridge
Batch Size
256
512
1024Figure 2: ICL emerges in PTs beyond a threshold pretraining task diversity. We show all results
on both tasks seen during pretraining ( top row ) and on new tasks ( bottom row ). The left column
compares the normalized loss of transformers pretrained with increasing task diversity to that of
dMMSE and Ridge. When the pretraining task diversity is small, the PT’s performance matches
that of dMMSE; it performs very well on tasks seen during pretraining but poorly on new tasks.
As the pretraining task diversity increases, both dMMSE and PT approach Ridge. However, the
PT approaches Ridge much faster, significantly outperforming dMMSE on new tasks (bottom left) .
In the middle and right columns , we compare the PT’s predictions to those of dMMSE and Ridge
respectively (Eq. (4)). We also increase the number of sequences per task at each level of task
diversity by increasing the batch size while keeping total training steps fixed. This reveals a task
diversity threshold between 214and215pretraining tasks at which there is a phase transition in the
behavior of the model. Below the threshold, increasing the dataset size leads to PTs with predictions
more aligned with dMMSE on TPretrain (top middle) . However, beyond this threshold (indicated by
gray shading), increasing the dataset size leads to PTs more aligned with Ridge on all tasks (right) .
1
MPM
i=1w(i)and for k∈ {2, ..., K}, (Appendix A.2)
ˆwdMMSE
k =MX
i=1exp
−1
2σ2Pk−1
j=1(yj−w(i)⊺xj)2
PM
l=1exp
−1
2σ2Pk−1
j=1(yj−w(l)⊺xj)2w(i). (2)
Intuitively, ˆwdMMSE
k is just a weighted sum of the pretraining w(i)s with weight governed by the
likelihood of observing targets {y1, ..., y k−1}conditioned on inputs {x1, ...,xk−1}and the task being
w(i). A PT that minimizes the pretraining loss LTPretrain will behave like this estimator.
For task distribution TTrue=N(0,ID), the Ridge regression estimator with the ridge parameter set
to the noise scale σ2is optimal: ˆyRidge
k=
ˆwRidge
k⊺
xk, where ˆwRidge
1=0and for k={2, ..., K},
ˆwRidge
k= 
X⊺X+σ2ID−1X⊺y, (3)
where X= (x⊺
1, ...,x⊺
k−1)∈R(k−1)×Dandy= (y1, ..., y k−1)(Appendix A.3). A PT that performs
optimally on new tasks will behave like this estimator. We can compare the behavior of the PT to that
of the optimal estimators by computing the mean square difference of the predictions under a given
task distribution T. We write this as
∆T
PT,Ridge/dMMSE = E
w∼T
x1,...,xK∼N (0,ID)
ε1,...,εK∼N (0,σ2)"
1
KDKX
k=1
fθ(Sk)−ˆyRidge/dMMSE
k2#
. (4)
3 Experiments and results
Unless specified otherwise, we study linear regression in D= 8 dimensions with up to K= 16
in-context examples and observation noise variance σ2= 0.25. We use either a base transformer
4

--- PAGE 5 ---
22242628210
# Pretraining T asks105
103
101
PT,dMMSE
Pretrain
22242628210
# Pretraining T asks105
103
101
PT,dMMSE
True
Steps
0.5M
1M
212214216218220
# Pretraining T asks104
103
102
PT,Ridge
Pretrain
212214216218220
# Pretraining T asks104
103
102
PT,Ridge
True
Figure 3: Increased pretraining steps reveals the same task diversity threshold for the emergence
of ICL. Columns 1 and 2 in this figure are similar to the middle column in Fig. 2 and columns 3 and 4
correspond to the right column in Fig. 2, except here we increase the number of sequences per task by
increasing the number of training steps while keeping batch size = 256. Both methods of increasing
dataset size—increasing batch size in Fig. 2 and increasing training steps in this figure—reveal a
transition in the behavior of the PT: beyond the task diversity threshold, ICL on new tasks emerges.
model with the GPT2 architecture [ 15] with 8 layers, 128-dimensional embeddings, and 2 attention
heads or a small model with 4 layers, 64-dimensional embeddings, and 2 attention heads. We train
with the Adam optimizer [ 16] and a one-cycle triangle learning rate schedule [ 17] with 50% warmup.
Thebase model is trained with batch size 256 for 500K training steps, though these hyperparameters
are varied in our experiments. We always sweep over a range of learning rates and choose the largest
learning rate at which training is stable. For further details see Appendix B.
To pretrain a randomly initialized transformer on data with task diversity M, we first construct the
pretraining task distribution, TPretrain , as described in Section 2. We then minimize the objective LTPretrain
in Eq. (1) using minibatch stochastic gradient descent. For each sequence in a minibatch, we sample
a single task wfromTPretrain , as well as newsamples of data, {xi}K
i=1, and noise, {εi}K
i=1, from their
respective continuous distributions, to form a sequence (x1,w⊺x1+ε1, . . . ,xK,w⊺xK+εK). If
we train for Nsteps at batch size B, the transformer will see a total of NB unique sequences and
roughlyNB
Munique sequences for each latent task in TPretrain . By increasing either BorNat fixed
M, we can increase the total size of the pretraining dataset (or number of sequences per task) while
keeping the dataset diversity —the number of unique ws inTPretrain —fixed.
3.1 Task diversity threshold for the emergence of in-context learning
For Fig. 2, we pretrain our base transformer on datasets with increasing task diversity (on the x-axis)
while keeping the total number of sequences seen during pretraining fixed ( B= 256 , N= 500 K).
We evaluate the PTs and both optimal estimators on tasks seen during pretraining drawn from TPretrain
(Fig. 2 top left) and on new tasks drawn from TTrue(Fig. 2 bottom left) and plot MSE normalized by
task dimension— LT/Dfrom Eq. (1)). Since dMMSE is optimal on tasks from TPretrain (as discussed
in Section 2), the green dMMSE markers denote the lowest possible loss the PT could achieve in
this setting. In fact, the pretraining objective LTPretrain explicitly encourages the PT to match dMMSE
performance. On the other hand, Ridge is optimal on tasks sampled from TTrue(Fig. 2 bottom left);
the blue markers denote the lowest possible MSE the PT could attain on newtasks.
Low task diversity phase: the PT is Bayesian with respect to the pretraining distribution and
cannot solve new tasks. At low pretraining task diversity— Mup to about 26—the PT’s MSE closely
tracks that of dMMSE on tasks sampled from TPretrain (Fig. 2 top left); the PT performs optimally
on tasks seen during pretraining. But it significantly underperforms on new tasks sampled from
TTrue, indicated by the gap in MSE between the PT and Ridge (Fig. 2, bottom left). In this regime, it
behaves like the Bayesian estimator with prior TPretrain .
High task diversity phase: the PT is non-Bayesian with respect to the pretraining task distribu-
tion and can solve new tasks. At higher task diversities—above 26pretraining tasks—the PT’s MSE
deviates from dMMSE and approaches Ridge under bothTPretrain andTTrue. Crucially, the PT starts
to significantly outperform dMMSE on unseen tasks sampled from TTrue(Fig. 2 bottom left) at the
expense of not fully minimizing its training objective, LTPretrain (gap between PT and dMMSE under
TPretrain , Fig. 2 top left). This suggests that, a PT trained on a finite but large number of pretraining
tasks can learn fundamentally new tasks in-context and this ability depends on it deviating from the
optimal Bayesian estimator on the pretraining task distribution.
5

--- PAGE 6 ---
0 250K 500K
Step00.020.040.060.08PT,Ridge on True
0 1M 2M
StepSmall PT Learning Curves: BS 512, 500K and 2M Steps
       #
Pretraining
    T asks
28
29
210
211
212
213
214
25210215
M1e41e51e6t*
t*M
(=0.47)
t*=2e6Scaling of Early Stopping TimeFigure 4: Learning dynamics of small PTs shows a transition at the task diversity threshold. We
plot∆TPretrain
PT,Ridge vs training steps for small PTs. For the same M, learning curves for short (500K steps,
left) or long (2M steps, center ) training durations are similar, and for M > M∗≈211.5learning
curves are similar to that of a model trained with infinite task diversity. Right: ForM≤210,t∗(the
training step at which ∆TPretrain
PT,Ridge is minimized) is well modeled by a scaling law t∗∝Mα. A linear fit
oflogt∗vslogM(dashed red line) gives α≈0.47. But for M > 210,∆TPretrain
PT,Ridge decreases through
training; t∗=2M, is larger than predicted by the scaling law. This sudden break in the scaling law
suggests a fundamental difference in the learning dynamics of models on either side of the threshold.
Finite size scaling of training data suggests an algorithmic phase transition as task-diversity
increases. The experiments in Fig. 2 (left column) suggest that, when tested on both task distributions
TPretrain andTTrue, the ICL algorithm implemented by a PT exhibits a smooth crossover in performance
from dMMSE to Ridge. We next examine how this transition changes as we increase the number of
sequences per task seen over pretraining, at fixed task diversity. One might reasonably expect that, if
the transformer sees more sequences per latent task in TPretrain , both its predictions and performance
should become more similar to those of dMMSE, and less similar to those of Ridge, at all values of
task diversity. Strikingly, this natural expectation is violated in a manner that facilitates ICL on TTrue.
At each number of tasks, we increase the number of sequences per task by increasing batch size
from 256 to 512 to 1024, while leaving the number of training steps fixed at 500K. We observe that
∆TPretrain
PT, dMMSE , which quantifies how different the PT and dMMSE estimator’s predictions are when
testing on tasks drawn from TPretrain , does in fact decrease for M≤210(Fig. 2 top center) as we
train on more sequences per task. Moreover, for each M∈ {210, ...,214}the PT’s predictions also
become less similar to those of Ridge, both on tasks from TPretrain (Fig. 2, top right) and TTrue(Fig. 2,
bottom right). Crucially , this movement in behavior of the PT towards dMMSE and away from
Ridge, at least on tasks drawn from TPretrain , holds only up to a threshold number of tasks between
214and215. Beyond this threshold, pretraining on more sequences per task at a fixed task diversity
actually makes the PT more like Ridge, in that both ∆TPretrain
PT,Ridge and∆TTrue
PT,Ridge decrease (Fig. 2, right
top and right bottom respectively). This means that, beyond a task diversity threshold, the PT can
not only optimally solve new tasks from TTrueby matching Ridge performance, but also the PT gets
better at doing so if trained on more sequences per task, despite the limited set of tasks experienced
in pretraining. Thus, in contrast to the natural expectation stated above, more sequences per task does
not promote overspecialization of the PT to the TPretrain at task diversities beyond the threshold.
Finally, the motion of the ICL algorithm implemented by PT towards (away) from Ridge above
(below) a task diversity threshold (Fig. 2, right top and bottom) indicates that as one increases the
number of sequences per task at fixed task diversity, the smooth cross over in performance of the
PT between dMMSE and Ridge, shown in Fig. 2, left top and bottom, will become sharper and
sharper in task diversity, ultimately exhibiting a sharp phase transition in the limit of infinite number
of sequences per task. Remarkably, this phase transition in the ICL algorithm implemented by the
PT appears at a moderate task diversity threshold below 215pretraining tasks; even though dMMSE
significantly underperforms relative to Ridge on TTrueat this task diversity, the PT nevertheless
remains unimpaired by this limited task diversity and can optimally solve new tasks.
Increased training time at fixed batch size further supports an algorithmic phase transition. To
confirm the above results, we also increase the number of sequences per task, at each task diversity, by
increasing the number of training steps Nfrom 500K to 1M while keeping batch size fixed at 256. We
observe that doubling N(change from pale blue to red in Fig. 3) and doubling B(change from pale
6

--- PAGE 7 ---
0.0 0.2 0.4 0.6 0.8 1.0
Interpolation 
0.20.30.40.50.6MSE/D
25 Pretraining Tasks
0.0 0.2 0.4 0.6 0.8 1.0
Interpolation 
210 Pretraining Tasks
0.0 0.2 0.4 0.6 0.8 1.0
Interpolation 
215 Pretraining Tasks
Ridge
dMMSE
PTFigure 5: Transformers pretrained with high, but not low, task diversity can learn new tasks
in-context. We compare the normalized loss of the PT to that of dMMSE and Ridge as we interpolate
between tasks in the pretraining dataset. Left: At25tasks, well below the task diversity threshold, the
PT performance matches that of the dMMSE estimator along interpolating paths, but under-performs
Ridge on newtasks at the center. Middle : At210tasks, the PT outperforms dMMSE on newtasks at
the center of the interpolation path, but is not yet as good as Ridge on new tasks. Right : AtM= 215
tasks, just above the task diversity threshold, the PT performs as well as Ridge even on new tasks at
the center. This demonstrates that, when pretrained on data with a finite but large number of unique
tasks, the PT, unlike the Bayes optimal estimator for TPretrain , can learn new tasks in-context.
blue to red in Fig. 2) have very similar effects on ∆T
PT,dMMSE and∆T
PT,Ridge , for both T=TTrueand
T=TPretrain . More importantly, the task diversity threshold, which we determined as the cross-over
point in ∆TTrue
PT,Ridge between batch sizes 256, 512, and 1024 at 500K training steps (Fig. 2 bottom
right) happens at the same number of tasks as the crossover point between 500K and 1M steps at
batch size 256 (Fig. 3, right). Given that our two approaches for training the baseline transformer on
more data yield the same task diversity threshold, and that doubling batch size leads to significantly
faster training times than doubling number of steps, from here onward we consider the task diversity
threshold to be cross-over point in ∆TTrue
PT,Ridge between batch sizes 256 and 512 when training for
500K steps. See Appendix D for more ablations of batch size and training steps that provide further
evidence for how the number of sequences seen by the transformer is the key factor determining the
similarity of its predictions to those of dMMSE and Ridge at each number of tasks.
Learning dynamics and a break in the scaling of early stopping time further supports an
algorithmic phase transition. To probe if the observed transition is merely an effect of under-fitting,
we study the learning dynamics of small PTs in the very large number of steps regime. First, in
Appendix E, we verify that the small PT also demonstrates an algorithmic phase transition but at
lower task diversity threshold between 211and212pretraining tasks. In Fig. 4 left, we visualize the
learning curves ( ∆TTrue
PT,Ridge vs training steps) of PTs trained for 500K steps at batch size 512 with
pretraining task diversities, M, below and above the task diversity threshold, M∗. For M < M∗,
∆TTrue
PT,Ridge decreases early in training until it reaches a minimum at time step t∗, and then increases
as the PT approaches dMMSE. We define t∗as the early stopping time for Ridge. For M > M∗,
∆TTrue
PT,Ridge decreases throughout training. To evaluate if, in the latter case, models are undertrained
andt∗is larger than the total training time, we extend training to 2M steps at batch size 512 ( 4×the
training time, see Appendix B). Fig. 4 center, shows these learning curves along with that of the model
trained with infinite task diversity; even in this long training regime, the task diversity threshold does
not change. For both short and long training durations, models trained with the same Mhave similar
qualitative behavior (whether distance to Ridge decreases then increases or monotonically decreases).
Additionally, learning curves of the models with M > M∗are very similar to the learning curves for
models trained on infinite pretraining task diversities and they achieve similar final accuracy (dahed
lines vs markers in Fig. 10), suggesting that these models are approaching the Ridge solution.
In Fig. 4 right, we study how t∗, scales with M. For most M < M∗,t∗obeys a simple scaling
behavior t∗∝Mα,α≈0.47. However, for M > 210, the distance to Ridge decreases monotonically
through training and t∗=2M steps. Despite the caveat that our experiments are necessarily in the
large but finite training step regime with a decayed learning rate schedule, this stark break in the
scaling behavior of t∗near the task diversity threshold suggests that the observed transition is not just
caused by under-fitting but an underlying difference in the learning dynamics.
7

--- PAGE 8 ---
212214216218220
# Pretraining tasks104
103
102
101
True
PT,Ridge
8 Dimensions
Batch Size
256
512
212214216218220
# Pretraining tasks104
103
102
101
16 Dimensions
212214216218220
# Pretraining tasks104
103
102
101
32 Dimensions
8 16 24 32
T ask Dimension0.00.10.20.30.40.5Model,Ridge
M=220
Model
dMMSE
PTFigure 6: The task diversity threshold increases with task dimension, and the PT’s ability
to solve new tasks scales significantly better than dMMSE’s. We vary the dimension of the
regression problem D(first three panels ) while leaving the signal-to-noise ratio fixed. The task
diversity threshold consistently increases with task dimension (gray shading denotes post threshold).
At220tasks ( right ), the PT’s predictions are similar to those of Ridge at all D(orange ∆TTrue
PT,Ridge ),
whereas dMMSE grows progressively less similar to Ridge (blue ∆TTrue
Ridge,dMMSE ).
The transition along interpolating paths. To obtain an additional description of the algorithmic
transition in the PT from dMMSE to Ridge, we compute the ICL performance of the PT, and compare
it to both dMMSE and Ridge, on a one parameter family of new tasks wαthat interpolate between
pairs of seen tasks wiandwjin the support of TPretrain . The interpolation path is given by
wα=1
2(∥wi∥2+∥wj∥2)αwi+ (1−α)wj
∥αwi+ (1−α)wj∥2forα∈[0,1]. (5)
Here we fix the norm of the interpolated vector wαto the average of the two endpoint norms to avoid
∥wα∥taking on very small values for α∼1
2. Fig. 5 shows the results of this analysis for 25(left,
low task diversity regime), 210(center, below task diversity threshold), and 215(right, just above
the task diversity threshold) tasks. At each value of α, MSE is averaged over a large number of task
pairs (wi,wj). Examination of the average performance at the center of many interpolation paths,
corresponding to fundamentally newtasks far from tasks seen during pretraining, clearly reveals a
transition in PT performance from dMMSE to Ridge, where new tasks can only be optimally learned
above, but not below, the task diversity threshold. In contrast, unlike the PT, dMMSE cannot solve
new tasks at any task diversity in the range considered.
The PT outperforms a smoothed dMMSE model. We have seen that at an intermediate task
diversity the PT significantly outperforms dMMSE on new tasks in TTrue. It is clear why dMMSE
performs poorly on new tasks in TTrueat low task diversity: its prior over tasks concentrates on M
unique tasks in TPretrain , while the prior over tasks in TTrueis Gaussian. A natural conjecture is that
the PT cannot memorize all Mtasks in TPretrain for large enough M. Therefore we also compare
PT performance to a smoothed dMMSE estimator in which the discrete point prior over Mtasks
seen in pretraining is replaced with a mixture of Misotropic Gaussians with the same centers but
with variance chosen to optimize performance on TTrue(see Appendix G for details). This smoothed
dMMSE outperforms dMMSE as it has a prior over tasks closer to the Gaussian TTrue. But remarkably,
the PT still outperforms the smoothed dMMSE even with optimal smoothing (Fig. 12). This indicates
the PT, even at moderate task diversity, implements a more sophisticated algorithm than a simple
smoothed dMMSE arising from the PT’s inability to resolve the Mpretraining tasks to high precision.
3.2 The PT exhibits superior scaling of task diversity threshold with dimension than dMMSE.
We next explore the dependence of the task diversity threshold on the regression problem dimension
D. We vary D∈ {8,16,32}while simultaneously scaling up maximal context length as K= 2D,
and increasing observation noise σ2to match the SNR to that of D= 8andσ2= 0.25. We also train
a larger model with 12 layers, 256-dimensional embeddings, and 4 attention heads that is sufficiently
expressive to match Ridge performance at D= 32 . Fig. 6, first 3 panels reveal that the task diversity
threshold of the PT increases moderately (approximately linearly) with task dimension (i.e. roughly
214,215, and 216atD= 8,16, and 32respectively). This linear scaling is remarkable considering
the volume of all possible tasks scales exponentially with dimension due to the concentration of the
Gaussian TTrueto a sphere for large D. Thus we expect dMMSE performance to scale much more
poorly with dimension Dsince the finite number of tasks in TPretrain would need to cover a substantial
portion of the sphere for dMMSE to approach Ridge. To test this hypothesis, for M= 220, which is
8

--- PAGE 9 ---
00.010.11
Weight Decay210211212213214215T ask Diversity Threshold
Base
SmallAblate Weight Decay
641282565121024
Embedding Dimension211212213214215T ask Diversity Threshold
Ablate Embedding Dimension
46810
# Layers211212213214T ask Diversity Threshold
Ablate # LayersFigure 7: Explicit regularization and model capacity affect the task diversity threshold. Increas-
ing explicit regularization, in the form of weight decay, lowers the task diversity threshold in base
PTs ( left). Increasing embedding dimension, has no effect on the threshold for base PTs, but does
increase the threshold for a small PT (middle ). Increasing depth, while holding other hyperparameters
fixed, increases the threshold for a small PT (right ). See Figure 13 for plots of ∆TTrue
dMMSE,Ridge vsM.
the largest task diversity we consider, we explore how the similarity of PT and dMMSE predictions to
Ridge on new tasks scales with D(Fig. 6, right panel). We see that ∆TTrue
dMMSE,Ridge grows significantly
as we increase D, while remarkably ∆TTrue
PT,Ridge is largely dimension independent. Overall this indicates
that the scaling of PT error with dimension is vastly superior than that of dMMSE; PT remains near
optimal and close to Ridge at all DforM= 220, while dMMSE departs from Ridge as Dincreases.
3.3 Effect of Regularization and model capacity on the task diversity threshold.
We study the dependence of the task diversity threshold on various hyperparameters. First, adding
explicit regularization in the form of weight decay (see Appendix B for details), and increasing its
value over three orders of magnitude, consistently lowers the threshold task diversity (Fig. 7, left).
Note however, the lower task diversity threshold also comes with worse performance (Figure 13, top).
This suggests that various forms of implicit regularization could help drive the algorithmic transition
in the PT without weight decay. We also explore the effect of model capacity on the task diversity
threshold by either increasing the embedding dimension of both small andbase PTs or increasing
the depth of small PTs. Fig. 7 center shows that increasing embedding dimension over a reasonable
range does not affect the task diversity threshold of base PT. However, for small PT, increasing either
the embedding dimension (Fig. 7 center) or depth (Fig. 7 right) increases the task diversity threshold.
Base PT has a much larger capacity then small PT and also has a larger threshold; we hypothesize that
small PT is still in a regime where the threshold is sensitive to capacity while base PT is not. Together,
these results suggest that model capacity plays an important role in the emergence of in-context
learning: increasing capacity (up to a point) leads to an increase in the task-diversity threshold.
4 Related work
The Bayesian framework for ICL introduced by Xie et al. [3], which motivates our work, hypothesizes
that PTs "locate" concepts learned during pretraining to solve ICL tasks. A series of empirical work
in language models [ 18–20] use this framework to select better in-context examples while Min et al.
[21] use it to study the robustness of latent task inference. Our work builds on this framework in the
linear regression setting and validates it at low task diversities. However, we find a regime—large but
finite number of pretraining tasks—in which the ability to learn new tasks in-context is an emergent
phenomenon that cannot be fully explained by Bayesian inference.
Prior work [ 12–14] has also shown that transformers can do linear regression in-context. However,
they pretrain with unlimited task diversity, sampling a completely new regression vector for each
sequence. In contrast, our work considers pretraining datasets with limited task diversity where ICL
on new tasks emerges even though the pretraining loss does not explicitly encode it. Another line of
work hypothesizes that ICL performs gradient descent in the activations of the forward pass, providing
explicit constructions for the weights of the PT to implement this for linear regression [ 13,14] or
exploring this hypothesis in language models [ 22]. However more experiments are required to test
the hypothesis that trained transformers actually match proposed constructions. Instead of studying
the explicit mechanism by which in-context learning is implemented, our work focuses on the impact
of the pretraining task diversity. Similar questions pertaining to the role of task diversification have
been explored in the meta-learning literature [23–25].
9

--- PAGE 10 ---
Kirsch et al. [26] also show the emergence of in-context learning with pretraining task diversity on a
toy classification task. By studying this question in the controlled setting of linear regression, we can
compare to the optimal estimators on TPretrain andTTrue. This allows us to establish that ICL at finite
task diversity emerges because the PT departs from the optimal estimator on the pretraining task
distribution, and is not just a consequence of the pretraining task distribution becoming similar to the
ideal task distribution. Among other important perspectives on ICL, Chan et al. [27] identify, in a toy
setting, several properties of the training distribution—burstiness and occurrence of rare classes—that
are necessary for the emergence of ICL. Wei et al. [28] study how ICL in large language models is
affected by semantic priors and input-label mappings, focusing on differences across model scale.
Olsson et al. [29] study inductions heads—circuits responsible for completing patterns by copying
tokens—as a mechanism for implementing ICL.
5 Discussion
Overall, we have extensively explored the impact of pretraining task diversity on the emergence of in-
context learning of fundamentally newtasks not seen during pretraining. We found several surprises
by working in the controlled setting of linear regression, where we could compare the performance
of the PT to Bayesian estimators that are optimal, either for the limited diversity pretraining task
distribution TPretrain (i.e. dMMSE), or for the diverse ideal task distribution TTrue(i.e. Ridge). These
comparisons reveal an algorithmic phase transition in the PT from the former to the latter at an
intermediate task diversity threshold; beyond this threshold, the PT solves fundamentally new tasks
not seen during pretraining. Strikingly, this task diversity threshold scales moderately with task
dimension, over the range of dimensions considered, despite the exponential growth in the volume
of all possible tasks with dimension. Indeed this PT scaling vastly outperforms that of dMMSE.
Overall, these results indicate that ICL of new tasks by PTs is an emergent phenomenon that cannot
be explained by Bayesian inference on limited diversity pretraining task distributions. Moreover, our
experiments suggest some form of implicit regularization in PTs allows them to break free of the
pretraining task distribution to solve new tasks, given a moderate pretraining task diversity.
Remarkably, beyond the task diversity threshold, PTs learn the optimal estimator for the underlying
generative model for pretraining tasks; this is the case for both Gaussian and Laplace priors over
tasks (see Figure 14 for experiments with Laplace prior). This is true even though solutions with
lower training loss exist; indeed when trained on more data at fixed diversity, PTs behave more like
Ridge at the expense of higher training loss. Our experiments in Fig. 4 suggest that this algorithmic
transition is due an underlying change in learning dynamics. We explore this hypothesis by probing
thelinear mode connectivity of the loss landscape [ 30,31]. In Fig. 11 we find that PTs trained with
large Minhabit the same loss basin as PTs trained with M=∞: the training loss barrier between
PTs trained with M≳213and PTs with M=∞is similar to two PTs trained with M=∞. In
contrast, there are large loss barriers between PTs trained with M < 213andM=∞. Additionally,
PTs trained with M=∞are closer in weight space to PTs trained with large Mthan those trained
with small M(see Appendix F). Overall, these experiments provide further evidence that PTs trained
with task diversities beyond the threshold find solutions similar to the optimal model for TTrue; we
leave further exploration of these loss landscapes to future work.
An intriguing question is how these observations carry over to language. A key mystery about
the efficacy of ICL in language tasks lies in how different the tasks learned in-context are from
the pretraining distribution of large language corpora. It is also less clear how to categorize the
contents of such corpora according to tasks and measure their resulting task diversity. Regardless,
our observation in linear regression that a moderate threshold in pretraining task diversity can enable
PTs to solve new tasks may imply that many language tasks that are quite different from the statistics
of large language corpora can still nevertheless be solved in-context.
Our results also suggest that the scale of data alone does not lead to good ICL performance. In fact,
below the task diversity threshold, increasing the size of the pretraining dataset without increasing
task diversity hurts ICL performance. It is necessary to increase both the diversity and size of the
dataset for ICL to emerge. Thus to improve ICL in language settings, our work motivates future
studies into uncovering the relevant notion of tasks in language modeling and approaches to increase
task diversity in language corpora. More generally, our empirical analysis of the impact of pretraining
task diversity on ICL motivates further theoretical studies. Such studies will be key to understanding
the mystery of why simple next token prediction during pretraining can lead to in-context learning of
so many apparently different tasks.
10

--- PAGE 11 ---
Acknowledgements
The authors would like to thank the Google TPU Research Cloud (TRC) whose generous support
made this project possible. SG thanks an NSF CAREER award and NTT Research for support.
References
[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in
Neural Information Processing Systems , volume 33, pages 1877–1901. Curran Associates,
Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .
[2]Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks
from learned optimization in advanced machine learning systems, 2021.
[3]Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080 , 2021.
[4]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. J. Mach. Learn. Res. , 21(1), jan 2020. ISSN 1532-4435.
[5]Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:
An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 ,
2020.
[6]Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In
International Conference on Learning Representations , 2022. URL https://openreview.
net/forum?id=gEZrGCozdqR .
[7]Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,
Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish
Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal
Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,
Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan,
Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multi-
task prompted training enables zero-shot task generalization. In International Conference on
Learning Representations , 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4 .
[8]Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan
Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,
Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir
Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh
Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A,
Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization
via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing , pages 5085–5109, Abu Dhabi, United
Arab Emirates, December 2022. Association for Computational Linguistics. URL https:
//aclanthology.org/2022.emnlp-main.340 .
11

--- PAGE 12 ---
[9]Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to
learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pages 2791–2809,
Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/
2022.naacl-main.201. URL https://aclanthology.org/2022.naacl-main.201 .
[10] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano,
Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
Advances in Neural Information Processing Systems , volume 35, pages 27730–27744. Curran
Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/
2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf .
[11] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,
Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for
effective instruction tuning. arXiv preprint arXiv:2301.13688 , 2023.
[12] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn
in-context? a case study of simple function classes. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems ,
2022. URL https://openreview.net/forum?id=flNZJ2eOet .
[13] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning
algorithm is in-context learning? investigations with linear models. In The Eleventh Inter-
national Conference on Learning Representations , 2023. URL https://openreview.net/
forum?id=0g0X4H8yN4I .
[14] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mord-
vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
descent, 2022.
[15] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR ,
abs/1412.6980, 2014.
[17] Leslie N. Smith and Nicholay Topin. Super-convergence: very fast training of neural networks
using large learning rates. In Defense + Commercial Sensing , 2017.
[18] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-
context learning. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies , pages 2655–2671,
Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/
2022.naacl-main.191. URL https://aclanthology.org/2022.naacl-main.191 .
[19] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
What makes good in-context examples for GPT-3? In Proceedings of Deep Learning In-
side Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for
Deep Learning Architectures , pages 100–114, Dublin, Ireland and Online, May 2022. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https:
//aclanthology.org/2022.deelio-1.10 .
[20] Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large
language models are implicitly topic models: Explaining and finding good demonstrations for
in-context learning, 2023.
[21] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?
InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing ,
pages 11048–11064, Abu Dhabi, United Arab Emirates, December 2022. Association for
Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.759 .
12

--- PAGE 13 ---
[22] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can
GPT learn in-context? language models implicitly perform gradient descent as meta-optimizers.
InICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models ,
2023. URL https://openreview.net/forum?id=fzbHRjAd8U .
[23] Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-
learning without memorization. 2020.
[24] Ramnath Kumar, Tristan Deleu, and Yoshua Bengio. The effect of diversity in meta-learning.
2022.
[25] Nilesh Tripuraneni, Chi Jin, and Michael I. Jordan. Provable meta-learning of linear representa-
tions, 2022.
[26] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-
context learning by meta-learning transformers, 2022.
[27] Stephanie C. Y . Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh,
Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive
emergent in-context learning in transformers, 2022.
[28] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao
Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning
differently, 2023.
[29] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,
Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson
Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer
Circuits Thread , 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-
heads/index.html.
[30] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear
mode connectivity and the lottery ticket hypothesis, 2020.
[31] Mansheej Paul, Feng Chen, Brett W. Larsen, Jonathan Frankle, Surya Ganguli, and
Gintare Karolina Dziugaite. Unmasking the lottery ticket hypothesis: What’s encoded in
a winning ticket’s mask? In The Eleventh International Conference on Learning Representa-
tions , 2023. URL https://openreview.net/forum?id=xSsW2Am-ukZ .
13

--- PAGE 14 ---
A Bayesian estimators
A.1 Bayesian MMSE estimator
The optimal estimator for the kth prediction, ˆyT
k, that minimizes the kth term in the sum in LT(in
Eq. (1)) is the Bayesian estimator with Tas the prior. This is given by the posterior mean of yk
conditioned on the context: ˆyT
k=ET,εk[yk|Sk], where the expectation is over the task distribution,
T, and the noise, εk. To derive this, notice that, using the law of total expectation, the kth term in the
lossLTis:
E
w∼T
x1,...,xk∼N (0,ID)
ε1,...,εk∼N (0,σ2)(fθ(Sk)−yk)2=E
SkE
w∼T
εk∼N (0,σ2)h
(fθ(Sk)−yk)2Ski
which implies that the optimal estimator is ˆyT
k=ET,εk[yk|Sk]. Adopting the notation X=
(x⊺
1, ...,x⊺
k−1)∈R(k−1)×Dandy= (y1, ..., y k−1), we can express the expectation explicitly:
E[yk|Sk] =Z
dwdykykp(w, yk|X,y,xk)
=Z
dwdykykp(yk|xk,w)p(w|X,y)
=Z
dw w⊺xkp(w|X,y)
≡ˆw⊺
kxk
where
ˆwk=E[w|Sk] =R
dwp(w)wQk−1
i=1p(yi|xi,w)R
dwp(w)Qk−1
i=1p(yi|xi,w)(6)
A.2 dMMSE estimator
For task distribution TPretrain =U{w(1), ...,w(M)}, we can directly get the discrete minimum mean
squared error (dMMSE) estimator by plugging the uniform discrete distribution into Eq. 6.
ˆwdMMSE
k =MX
i=1exp
−1
2σ2Pk−1
j=1(yj−w(i)⊺xj)2
PM
l=1exp
−1
2σ2Pk−1
j=1(yj−w(l)⊺xj)2w(i). (7)
A.3 Ridge estimator
For task distribution TTrue=N(0,ID), we can directly get the Ridge regression estimator from
Eq. 6:
ˆwRidge
k=R
wexp
−1
σ2(Xw−y)T(Xw−y)−wTw
dwR
exp
−1
σ2(Xw−y)T(Xw−y)−wTw
dw
=R
wexph
−1
σ2 
w−(XTX+σ2ID)−1XyT(XTX+σ2ID) 
w−(XTX+σ2ID)−1Xyi
dw
R
exph
−1
σ2(w−(XTX+σ2ID)−1Xy)T(XTX+σ2ID) (w−(XTX+σ2ID)−1Xy)i
dw
= 
X⊺X+σ2ID−1X⊺y (8)
where X= (x⊺
1, ...,x⊺
k−1)∈R(k−1)×Dandy= (y1, ..., y k−1).
B Experimental details
For most experiments, we study linear regression in D= 8dimensions with up to K= 16 in-context
examples and observation noise variance σ2= 0.25. Our base model is a transformer with the GPT2
architecture [ 15] with 8 layers, 128-dimensional embeddings, and 2 attention heads. We train with
14

--- PAGE 15 ---
the Adam optimizer [ 16] and a one-cycle triangle learning rate schedule [ 17] with 50% warmup. The
base model is trained with batch size 256 for 500K training steps, though these hyperparameters
are varied in our experiments. Specifically, for the experiments in Fig. 3 we sweep over 500K and
1M training steps to find the task diversity threshold as a function of training steps. For all other
experiments, we sweep batch size 256 and 512 and for the experiments in Fig. 2, we do an additional
batch size of 1024.
For the experiments in Fig. 6, we use a larger model with 12 layers, 256-dimensional embeddings, and
4 attention heads. This ensures that the model can learn to perform ridge regression in the D= 32
setting. We also train the models on D= 8,16,24,32with up to K= 2Din-context examples in
each case. The observation noise variance is scaled at each dimension to keep the signal to noise ratio
constant. So σ2= 0.5,0.707,0.866,1forD= 8,16,24,32respectively.
We always sweep over learning rates in {0.0001, 0.0003, 0.001} and choose the largest learning rate
at which training is stable. For almost all experiments, the learning rate is 0.001, the main exception
being the experiments in Fig. 6 where the learning rate is 0.0001.
For the 2M training step experiments in Fig. 4 and Fig. 10, the learning rate increases linearly up
to 0.001 at 250K steps (i.e. using the same warmup as in the 500K step experiments) and then
decreases linearly to 0 at 2M steps, consistent with our choice of decayed learning rate schedules. The
experiments in Appendix F are performed at batch size 512 and following the 500K step one-cycle
triangle learning rate schedule described above.
The implementation for this paper was done in JAX and all experiments were run on TPU v2-8
and v3-8 provided as part of the Google TPU Research Cloud program. Each training run takes
approximately 4 v2-8 TPU hours.
C Support Figure 2
Fig. 8 provides an additional visualization for Fig. 2 upper middle but on a linear scale on the y-axis.
In this figure it is clear that for all task diversities below the threshold—less that 214—increasing
the batch size aligns the PT with dMMSE, the optimal estimator for TPretrain . Thus below the task
diversity threshold, the optimal estimator does indeed approach the Bayesian estimator with prior
TPretrain when trained on more data and is thus unable to learn new tasks as discussed in Section 3.1.
22242628210212214
# Pretraining T asks0.000.020.040.060.08Pretrain
PT,dMMSE
Batch Size
256
512
1024
Figure 8: Another visualization for Fig. 2 upper middle but with the x-axis extending all the way
to the task diversity threshold and a linear scale on the y-axis to make it easy to differentiate the
different batch sizes at large number of pretraining tasks. See Appendix C for details.
D Dependence on number of sequences per task
In Section 3.1 we explore how the crossover of the PT from behaving like dMMSE to behaving like
Ridge as we increase pretraining task diversity depends on the number of sequences per task seen
during pretraining. In Fig. 2 middle and right column, we probe this by increasing the batch size to
increase the number of sequences per task seen during pretraining at each level of task diversity. To
ensure that that the phase transition and task diversity threshold we find is indeed a consequence of
increasing the number of sequences per task and not merely just an effect of increasing batch size, we
keep the batch size constant and increase the number of sequences per task in Fig. 3.
15

--- PAGE 16 ---
In Fig. 9 we provide additional evidence that the number of sequences per task and not batch size nor
number of training steps is the primary factor that drives the behavior of the final model. Specifically,
we show that the final behavior of the model is invariant to batch size or number of training steps if
the number of training sequences per task is kept constant, atleast within a reasonable range of batch
size and number of training steps. To do so, at each level of pretraining task diversity, we compare
our base model (batch size = 256, 500K training steps, red cicles) to a model trained with batch size
512 (light blue squares) and batch size 128 (purple triangles). For either variant of the batch size,
we adjust the number of training steps to keep the total number of pretraining sequences per task
constant: 250K training steps for batch size 512 and 1M training steps for batch size 128. In all three
cases, the behavior of the PT is identical as measured by ∆T
PT,dMMSE and∆T
PT,Ridge for both TPretrain
andTTrueacross all numbers of pretraining tasks.
22242628210
# Pretraining T asks105
103
101
PTM,dMMSE
Pretrain
22242628210
# Pretraining T asks105
103
101
PTM,dMMSE
Latent
Batch Size, Steps
512, 250K
256, 500K
128, 1M
212214216218220
# Pretraining T asks104
103
102
PTM,Ridge
Pretrain
212214216218220
# Pretraining T asks104
103
102
PTM,Ridge
Latent
Figure 9: Number of sequences per task and not batch size or number of training steps
affects final model behavior : This figure provides additional experiments to support the findings in
Section 3.1 and Figs. 2 and 3. Each panel corresponds to the same panel in Fig. 3. See Appendix D
for details.
E Small PT
We run our task diversity threshold finding experiment in a small PT with 64 dimensional embeddings,
4 layers, and 2 attention heads. The existence of a task diversity threshold reproduces in a Small
PT: the PTs transition from becoming less like Ridge to becoming more like Ridge when trained on
more data (see Fig. 10). We also note that the Small PT with a lower capacity has a lower threshold
(∼211.5compared to ∼214.5for the Base PT in Fig. 2) but also has lower overall performance
(compare y-axis range to Fig. 2 lower right).
210212214216218
M (# Pretraining Tasks)103
102
PT,Ridge on True
Small PT
Batch Size, Steps
256, 500K
512, 500K
512, 2M
M=
Figure 10: Existence of a task diversity threshold reproduces in small PT.Starting from batch
size 256 and 500K training steps, we train on 2x data (i.e. examples per task) by doubling the batch
size, and on 8x data by doubling batch size and quadrupling the number of training steps. We show
distance to Ridge on new tasks (as in Fig. 2 lower right), and also plot distance to Ridge for models
trained with infinite task diversity, where each training example samples a new task from the Gaussian
distribution, as dashed lines (colors indicating batch size and training time). Points after the threshold
(M≥212) are shaded gray. Importantly, the threshold value doesn’t change even if we go from 2x to
8x data suggesting that it isn’t an artifact of underfitting.
16

--- PAGE 17 ---
Figure 11: The training loss barrier between a PT trained with M≳213and a PT trained with
M=∞is small. At each M, including M=∞, we train four small PTs, each with a different
random seed for tasks. Each purple dot is an average over the 16 finite-infinite task model pairs at the
given value of M, and each light blue dot is an average over the 6 infinite-infinite task model pairs.
Error bars are standard deviations. See Appendix F for details on how loss barriers and distances
between models are computed.
F Examining the loss landscape along paths interpolating between finite and
infinite task models
For each finite number of tasks, M, as well as M=∞(where each training example samples a new
task from the Gaussian distribution), we train four small PTs, each with a different random seed for
tasks . This means that all randomness (including model initialization, data, and observation noise) is
shared across the four experiments, except for that governing the sampling of w’s used for training.
We then compare the finite Mmodels to the M=∞models, both in terms of the size of the training
loss barrier along a path interpolating between the models, as well as the distance in weight space
between the models.
We compute the training loss barrier between a finite Mmodel and an infinite task model under the
objective LT
Pretrain defined by the specific set of Mtasks the finite task model was trained on. We
follow the approach outlined in [ 31] where the barrier between two models is the loss at a model
whose weights are the average of the two models, minus the average of the losses of the two models.
Specifically, given two weight configurations w1andw2and a loss function L, the loss barrier is
calculated as
loss barrier =Lw1+w2
2
−L(w1) +L(w2)
2(9)
The distance in weight space, in turn, is simply the L2norm of the difference in model weights. For
eachMwe average barriers and distances across all finite-infinite task pairs of models. We treat the
analogous quantities computed between pairs of infinite task models as a baseline. See Fig. 11 and
the Discussion section in the main text.
G Smoothed dMMSE
212214216218220
# Pretraining T asks0.3450.3460.347MSE/D
True
Ridge
Smooth dMMSE
PT
Figure 12: The PT outperforms the Smoothed dMMSE estimator with optimal smoothing. See
Appendix G.
17

--- PAGE 18 ---
Here we consider the setting in which the discrete point prior over Mtasks seen in pretraining is
replaced with a mixture of Misotropic Gaussians with variance ϵ2. We call the estimator that does
posterior predictive inference under this prior the Smoothed dMMSE estimator (sMMSE). In the limit
ofϵ→0, the sMMSE estimator becomes the dMMSE estimator. On the other hand, if ϵ→ ∞ , the
prior will look like a Gaussian distribution but will have a variance much larger than the variance of
TTrue. In both edge cases, it won’t perform well on TTrue, and so there is an optimal ϵthat achieves the
best performance on TTrue. In practice, we perform a search by simulation to determine the optimal ϵ.
We observe in Fig. 12 that for 213tasks onward, the PT outperforms the optimally smoothed sMMSE
onTTrue.
To derive the sMMSE estimator, we want to calculate ˆwk=E[w|Sk]and apply Eq. (6).
p(w|Sk) =p(Sk|w)p(w)
p(Sk)
∝k−1Y
j=1p(yj|xj,w)MX
i=1exp
−1
2ϵ2
w−w(i)2
∝MX
i=1exp
−1
2σ2k−1X
j=1(w⊺xj−yj)2−1
2ϵ2(w−w(i))2

∝MX
i=1exp
−1
2w⊺1
σ2X⊺X+1
ϵ2I
w+1
ϵ2w(i)+1
σ2X⊺y⊺
w−1
2ϵ2w(i)2
We define,
A=1
σ2X⊺X+1
ϵ2I
Ji=1
ϵ2w(i)+1
σ2X⊺y
C=MX
i=1exp
−1
2ϵ2∥w(i)∥2Z
dwexp
−1
2w⊺Aw+J⊺
iw
=s
(2π)D
|A|MX
i=1J⊺
iA−1Jiexp
−1
2ϵ2∥w(i)∥2
so that,
p(w|Sk) =1
CMX
i=1exp
−1
2w⊺Aw+J⊺
iw−1
2ϵ2∥w(i)∥2
Plugging back to Eq. (6),
ˆwsMMSE
k =1
CMX
i=1exp
−1
2ϵ2∥w(i)∥2Z
dw wexp
−1
2w⊺Aw+J⊺
iw
=1
CMX
i=1s
(2π)D
|A|A−1Jiexp1
2J⊺
iA−1Ji−1
2ϵ2w(i)2
≡MX
i=1βiA−1Ji
where β=softmax (˜β)and˜βi≡1
2J⊺
iA−1Ji−∥w(i)∥2
2ϵ2.
H Supporting figures
18

--- PAGE 19 ---
210212214216218220
# Pretraining tasks101
102
103
104
True
PT,Ridge
Weight Decay: 0.0
Batch Size
256
512
210212214216218220
# Pretraining tasks
Weight Decay: 0.01
210212214216218220
# Pretraining tasks
Weight Decay: 0.1
210212214216218220
# Pretraining tasks
Weight Decay: 1.0
212214216218220
# Pretraining tasks104
103
102
101
True
PT,Ridge
128 Embedding
Batch Size
256
512
212214216218220
# Pretraining tasks
256 Embedding
212214216218220
# Pretraining tasks
512 Embedding
212214216218220
# Pretraining tasks
1024 EmbeddingFigure 13: Effect of weight decay and embedding dimension on the task diversity threshold
ofbase PT.Increasing explicit regularization, in the form of weight decay, consistently lowers the
task diversity threshold, which is the crossover point between ∆TTrue
PT,Ridge at the two batch sizes ( top,
gray shading denotes post threshold). However, note that more weight decay does make the PT’s
predictions lesssimilar to those of Ridge, as quantified by ∆TTrue
PT,Ridge at numbers of tasks beyond the
task diversity threshold (i.e. the flat region of both blue and red curves progressively move up with
increasing weight decay from left to right in the). Increasing embedding dimension, while scaling the
number of heads by the same factor, has no discernible effect on the threshold ( bottom ) for a base PT.
Figure 14: Task diversity threshold for tasks sampled from a Laplace prior . We run our task
diversity threshold finding experiment in a setting where each element of each task regression vector
is sampled from a Laplace prior instead of a Gaussian prior. We use the base model from our
paper. For the baseline, we approximate the Bayes-optimal model on new tasks by training for 500K
steps with batch size 512 on a pretraining distribution with infinite task diversity drawn from the
Laplace prior (black dashed line). The y-axis is the dimension normalized mean squared error on
new tasks drawn from the Laplace prior. In this setting, we see a task diversity threshold between
214and215pretraining tasks; beyond the threshold (gray shading), training on more data leads to
better performance on new, unseen tasks and ICL emerges. Thus, our findings for the task diversity
threshold reproduce when tasks are drawn from different priors.
19
