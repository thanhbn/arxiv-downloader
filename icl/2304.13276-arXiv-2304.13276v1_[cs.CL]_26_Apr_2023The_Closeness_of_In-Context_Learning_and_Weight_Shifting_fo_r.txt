# 2304.13276.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2304.13276.pdf
# File size: 239275 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2304.13276v1  [cs.CL]  26 Apr 2023The Closeness of In-Context Learning and Weight Shifting fo r
Softmax Regression
Shuai Li∗Zhao Song†Yu Xia‡Tong Yu§Tianyi Zhou¶
Abstract
Large language models (LLMs) are known for their exceptional per formance in natural lan-
guage processing, making them highly eﬀective in many human life-rela ted or even job-related
tasks. The attention mechanism in the Transformer architecture is a critical component of
LLMs, as it allows the model to selectively focus on speciﬁc input part s. The softmax unit,
which is a key part of the attention mechanism, normalizes the atten tion scores. Hence, the
performance of LLMs in various NLP tasks depends signiﬁcantly on t he crucial role played by
the attention mechanism with the softmax unit.
In-context learning, as one of the celebrated abilities of recent LL Ms, is an important concept
in querying LLMs such as ChatGPT. Without further parameter upd ates, Transformers can
learn to predict based on few in-context examples. However, the r eason why Transformers
becomes in-context learners is not well understood. Recently, se veral works [ ASA+22,GTLV22 ,
ONR+22] have studied the in-context learning from a mathematical perspe ctive based on a
linear regression formulation min x/bardblAx−b/bardbl2, which show Transformers’ capability of learning
linear functions in context.
In this work, we study the in-context learning based on a softmax r egression formulation
minx/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−b/bardbl2of Transformer’s attention mechanism. We show the upper
bounds of the data transformations induced by a single self-atten tion layer and by gradient-
descent on a ℓ2regression loss for softmax prediction function, which imply that wh en train-
ing self-attention-only Transformers for fundamental regress ion tasks, the models learned by
gradient-descent and Transformers show great similarity.
∗shuaili8@sjtu.edu.cn . Shanghai Jiao Tong University.
†zsong@adobe.com . Adobe Research.
‡xiayuu@umich.edu . University of Michigan.
§tyu@adobe.com . Adobe Research.
¶t8zhou@ucsd.edu . University of California San Diego.

--- PAGE 2 ---
1 Introduction
In recent years, there has been a signiﬁcant increase in rese arch and development in the ﬁeld
of Artiﬁcial Intelligence (AI), with large language models (LLMs) emerging as an eﬀective way
to tackle complex tasks. Transformers have achieved state- of-the-art results in various natural
language processing tasks, such as machine translation [ PCR19 ,GHG+20], language modeling,
question answering, and text generation [ LSX+22]. As a result, they have become the preferred
architecture for NLP. Based on that architecture, BERT [ DCLT18 ], GPT-3 [ BMR+20], PaLM
[CND+22], and OPT [ ZRG+22] were proposed. They have demonstrated remarkable learnin g and
reasoning capabilities and have proven to be more eﬃcient th an smaller models and traditional
techniques when processing natural language.
Additionally, LLMs can be ﬁne-tuned for multiple purposes w ithout requiring a new build from
scratch, making them a versatile tool for AI applications. A prime example of this is ChatGPT,
a chat software developed by OpenAI utilizing GPT-3’s poten tial to its fullest. Moreover, GPT-4
[Ope23 ] has the capability to handle intricate tasks that its prede cessors were unable to accomplish.
It has demonstrated a remarkable level of proﬁciency compar able to human performance in various
professional and academic benchmarks.
Transformers have a speciﬁc type of sequence-to-sequence n eural network architecture. They
utilize the attention mechanism [ VSP+17,RNS+18,DCLT18 ,BMR+20] that allows them to cap-
ture long-range dependencies and context from input data eﬀe ctively. The core of the attention
mechanism is the attention matrix which is comprised of rows and columns, corresponding to indi-
vidual words or “tokens”. The attention matrix represents t he relationships within the given text.
It measures the importance of each token in a sequence as it re lates to the desired output. During
the training process, the attention matrix is learned and op timized to improve the accuracy of the
model’s predictions. Through the attention mechanism, eac h input token is evaluated based on its
relevance to the desired output by assigning a token score. T his score is determined by a similarity
function that compares the current output state with input s tates.
Theoretically, the attention matrix is comprised of the que ry matrix Q∈Rn×d, the key matrix
K∈Rn×dand the value matrix V∈Rn×d. Following [ ZHDK23 ,AS23 ,BSZ23 ], the computation
of the normalized attention function is deﬁned as D−1exp(QK⊤)V. Following the transformer
literature, we apply exp to a matrix entry-wise way. Here D∈Rn×nis diagonal matrix that
deﬁned as D= diag(exp( QK⊤)1n). Intuitively, Ddenotes the softmax normalization matrix. A
more general computation formulation can be written as
D−1
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n×ndiagonal matrixexp(XQK⊤X⊤)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n×nX/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n×dV/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
d×d, D := diag(exp( XQK⊤X⊤)1n)
In the above setting, we treat Q,K,V∈Rd×das weights and Xis the input sentence data that
has length nand each word embedding size is d. In the remaining of the part, we will switch Xto
notation Aand useAto denote sentence.
Mathematically, the attention computation problem can be f ormulated as a regression problem
in the following sense
Deﬁnition 1.1. We consider the following problem
min
X∈Rd×d/bardblD−1exp(AXA⊤)−B/bardblF
whereA∈Rn×dcan be treated as a length- ndocument and each word has length- dembedding size.
HereD= diag(AXA⊤1n). For any given A∈Rn×dandB∈Rn×n, the goal is to ﬁnd some weight
Xto optimize the above objective function.
1

--- PAGE 3 ---
In contrast to the formulation in [ ZHDK23 ,AS23 ,BSZ23 ], the parameter Xin Deﬁnition 1.1
is equivalent to the QK⊤∈Rd×din the generalized version of [ ZHDK23 ,AS23 ,BSZ23 ] (e.g.
replacing Q∈Rn×dbyXQ whereX∈Rn×dandQ∈Rd×d. Similarly for KandV. In such
scenario, Xcan be viewed as a matrix representation of a length- nsentence.). A number of work
[ASA+22,GTLV22 ,ONR+22] study the in-context learning from mathematical perspect ive in a
much simpliﬁed setting than Deﬁnition 1.1, which is linear regression formulation (see following).
Deﬁnition 1.2. Given a matrix A∈Rn×dandb∈Rn, the goal is to solve
min
x/bardblAx−b/bardbl2
Several theoretical transformer work have studied either e xponential regression [ GMS23 ,LSZ23 ]
or softmax regression problem [ DLS23 ]. In this work, to take a more step forward to understand
the softmax unit in the attention scheme in LLMs. We consider the following softmax regression
and study the in-context learning phenomena based on it.
Deﬁnition 1.3 (Softmax Regression) .Given a A∈Rn×dand a vector b∈Rn, the goal is to solve
min
x∈Rd/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−b/bardbl2
We remark that the Deﬁnition 1.3is a formulation is between Deﬁnition 1.2and Deﬁnition 1.1.
1.1 Our Result
We state our major result as follows:
Theorem 1.4 (Bounded shift for Learning in-context, informal of combin ation of Theorem 8.1and
Theorem 8.2).If the following conditions hold
•LetA∈Rn×d.
•Letb∈Rn.
•/bardblA/bardbl ≤R.
•Let/bardblx/bardbl2≤R.
•/bardblA(xt+1−xt)/bardbl∞<0.01.
•/bardbl(At+1−At)x/bardbl∞<0.01.
•LetR≥4.
•LetM:=n1.5exp(10R2).
We consider the softmax regression (Deﬁnition 1.3) problem
min
x/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−b/bardbl2.
•Part 1. If we move the xttoxt+1, then we’re solving a new softmax regression problem with
min
x/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−/tildewideb/bardbl2
where
/bardbl/tildewideb−b/bardbl2≤M·/bardblxt+1−xt/bardbl2
2

--- PAGE 4 ---
•Part 2. If we move the AttoAt+1, then we’re solving a new softmax regression with
min
x/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−/hatwideb/bardbl2
where
/bardbl/hatwideb−b/bardbl2≤M·/bardblAt+1−At/bardbl
Recall that A∈Rn×ddenotes a length- ndocument and each word has the length- dembedding
size and xdenotes the simpliﬁed version of QK⊤. One-step gradient descent can be treated as an
update to the model’s weight x. Thus, part 1 of our result (Theorem 1.4) implies that the data
transformation of binduced by gradient-descent on the ℓ2regression loss is bounded by M·/bardblxt+1−
xt/bardbl2. Following [ ONR+22], to do in-context learning, a self-attention layer update can be treated
as an update to the tokenized document A. Thus, part 2 of our result (Theorem 1.4) implies that
the data transformation of binduced by a single self-attention layer is bounded by M·/bardblAt+1−At/bardbl.
We remark that the data transformation of binduced by 1) a single self-attention layer and by
2) gradient-descent on the ℓ2regression loss are both bounded. This bounded transformat ion for
thebimplies that when training self-attention-only Transform ers for fundamental regression tasks,
the models learned by gradient-descent and Transformers sh ow great similarity.
Roadmap. In Section 2, we introduce some related work. In Section 3, we give some preliminar-
ies. In Section 4, we compute the gradient of the loss function with softmax fu nction with respect
tox. Those functions include α(x)−1,α(x) andf(x). In Section 5, we show the Lipschitz for the
self-attention function with respect to x. In Section 6, we compute the gradient of the loss function
with softmax function with respect to A. In Section 7, we show the Lipschitz for the self-attention
function with respect to A. In Section 8, we give the main result of our paper.
2 Related Work
2.1 In-context Learning
[ASA+22] indicated that Transformer-based in-context learners ar e able to perform traditional
learning algorithms implicitly. This is achieved by encodi ng smaller models within their internal
activations. These smaller models are updated by the given c ontext. They theoretically investi-
gate the learning algorithms that Transformer decoders can implement. They demonstrate that
Transformers need only a limited number of layers and hidden units to implement various linear
regression algorithms. For d-dimensional regression problems, a O(d)-hidden-size Transformer can
perform a single step of gradient descent. They also demonst rate that the Transformer with O(d2)
hidden size is able to update a ridge regression problem. The study reveals that Transformers
theoretically have the ability to perform multiple linear r egression algorithms.
[GTLV22 ] concentrate on training Transformer to learn certain func tions, under in-context
conditions. The goal is to have a more comprehensive underst anding of in-context learning and
determine if Transformers can learn the majority of functio ns within a given class after training.
They found that in-context learning is possible even when th ere is a distribution shift between the
training and inference data or between in-context examples and query inputs. In addition, they ﬁnd
out that Transformers can learn more complex function class es such as sparse linear functions, two-
layer neural networks, and decision trees. These trained Tr ansformers have comparable performance
to task-speciﬁc learning algorithms.
3

--- PAGE 5 ---
[ONR+22] demonstrate and provide an explanation of the similarity b etween the training pro-
cess of the Transformers in in-context tasks and some meta-l earning formulations based on gradient
descent. During the process of training Transformers for au to-regressive tasks, the implementation
of in-context learning in the Transformer forward pass is ca rried out through gradient-based opti-
mization of an implicit auto-regressive inner loss that is c onstructed from the in-context data.
Formally speaking, they consider the following problem min x/bardblAx−b/bardbl2deﬁned in Deﬁnition
1.2. They ﬁrst show that doing one step of gradient descent carri es out data transformation as
follows:
/bardblA(x+δx)−b/bardbl2=/bardblAx−(b−δb)/bardbl2
=/bardblAx−/tildewideb/bardbl2
whereδxdenotes the one-step gradient descent on xandδbdenotes the corresponding data trans-
formation on b. They also show that a self-attention layer is in principle c apable of exploiting
statistics in the current training data samples. Concretel y, letQ,K,V∈Rd×ddenotes the weights
for the query matrix, key matrix, and value matrix respectiv ely. The linear self-attention layer
updates an input sample by doing the following data transfor mation:
/hatwidebj=bj+PVK⊤Qj
where/hatwidebdenotes the updated bandPdenotes the projection matrix such that a Transformer
step/hatwidebjon every jis identical to the gradient-induced dynamics /tildewidebj. This equivalence implies that
when training linear-self-attention-only Transformers f or fundamental regression tasks, the models
learned by GD and Transformers show great similarity.
[XRLM21 ] explores the occurrence of in-context learning during pre -training when documents
exhibit long-range coherence. The Language Model (LLM) dev elops the ability to generate coherent
next tokens by deducing a latent document-level concept. Du ring testing, in-context learning is
observed when the LLM deduces a shared latent concept betwee n examples in a prompt. Through
the research conducted, it has been demonstrated that in-co ntext learning happens even when there
is a distribution mismatch between prompts and pretraining data, especially in scenarios where the
pretraining distribution is a mixture of Hidden Markov Mode ls [BP66 ]. Theoretically, they show
that the error of the in-context predictor is optimal when a d istinguishability condition holds. In
cases where this condition does not hold, the expected error still reduces as the length of each
example increases. This ﬁnding highlights the importance o f both input and input-output mapping
contributes to in-context learning.
2.2 Transformer Theory
The advancements of Transformers have been noteworthy, how ever, their learning mechanisms
are not completely comprehensible yet. Although these mode ls have performed remarkably well
in structured and reasoning activities, our comprehension of their mathematical foundations lags
signiﬁcantly behind. Past research has indicated that the o utstanding performance of Transformer-
based models can be attributed to the information within the ir components, such as the multi-head
attention. Various studies [ TDP19 ,VB19 ,HL19 ,Bel22 ] have presented empirical proof that these
components carry a substantial amount of information, whic h can help in resolving diﬀerent probing
tasks.
Recent research has investigated the potential of Transfor mers through both theoretical and
experimental methods, including Turing completeness [ BPG20 ], function approximation [ YBR+20,
4

--- PAGE 6 ---
CDW+21], formal language representation [ BAG20 ,EGZ20 ,YPPN21 ], and abstract algebraic op-
eration learning [ ZBB+22]. Some of these studies have indicated that Transformers ma y act as uni-
versal approximators for sequence-to-sequence operation s and emulate Turing machines [ PMB19 ,
BPG20 ]. [LWD+23] demonstrate the existence of contextual sparsity in LLM, w hich can be ac-
curately predicted. They exploit the sparsity to speed up LL M inference without degrading the
performance from both a theoretical perspective and an empi rical perspective. [ DCL+21] proposed
the Pixelated Butterﬂy model that uses a simple ﬁxed sparsit y pattern to speed up the training of
Transformer. Other studies have focused on the expressiven ess of attention within Transformers
[DGV+18,VBC20 ,ZKV+20,EGKZ21 ,SZKS21 ,WCM21 ].
Furthermore, [ ZPGA23 ] has demonstrated that moderately sized masked language mo dels may
eﬀectively parse and recognize syntactic information that h elps in the partial reconstruction of
a parse tree. Inspired by the language grammar model studied by [ZPGA23 ], [DGS23 ] consider
the tensor cycle rank approximation problem. [ GMS23 ] consider the exponential regression in
neural tangent kernel over-parameterization setting. [ LSZ23 ] studied the computation of regularized
version of the exponential regression problem but they igno re the normalization factor. [ DLS23 ]
consider the softmax regression which considers the normal ization factor compared to exponential
regression problems [ GMS23 ,LSZ23 ]. The majority of LLMs can perform attention computations
in an approximate manner during the inference process, as lo ng as there are suﬃcient guarantees
of precision. This perspective has been studied by various r esearch, including [ CGRS19 ,KKL20 ,
WLK+20,DKOD20 ,KVPF20 ,CDW+21,CDL+22]. With this in mind, [ ZHDK23 ,AS23 ,BSZ23 ,
DMS23 ] have conducted a study on the computation of the attention m atrix from the hardness
perspective and developed faster algorithms.
3 Preliminary
In Section 3.1, we introduce the notations used in this paper. In Section 3.2, we give some facts
about the basic algebra. In Section 3.3, we propose the lower bound on /an}bracketle{texp(Ax),1n/an}bracketri}ht.
3.1 Notations
For a positive integer n, we use [ n] to denote {1,2,···,n}, for any positive integer n.
We useE[·] to denote expectation. We use Pr[ ·] to denote probability.
We use1nto denote the vector where all entries are one. We use 00to denote the vector where
all entries are zero. The identity matrix of size n×nis represented by Infor a positive integer n.
The symbol Rrefers to real numbers and R≥0represents non-negative real numbers.
For any vector x∈Rn, exp(x)∈Rndenotes a vector where the i-th entry is exp( xi) and/bardblx/bardbl2
represents its ℓ2norm, that is, /bardblx/bardbl2:= (/summationtextn
i=1x2
i)1/2. We use /bardblx/bardbl∞to denote max i∈[n]|xi|.
For any vector x∈Rnand vector y∈Rd, we use /an}bracketle{tx,y/an}bracketri}htto denote the inner product of vector x
andy.
The notation Biis used to indicate the i-th row of matrix B.
Ifaandbare two column vectors in Rn, thena◦bdenotes a column vector where ( a◦b)i=aibi.
For a square and full rank matrix B, we use B−1to denote the true inverse of B.
3.2 Basic Algebras
Fact 3.1. For vectors x,y∈Rn, we have
•/bardblx◦y/bardbl2≤ /bardblx/bardbl∞·/bardbly/bardbl2
5

--- PAGE 7 ---
•/bardblx/bardbl∞≤ /bardblx/bardbl2≤√n/bardblx/bardbl∞
•/bardblexp(x)/bardbl∞≤exp(/bardblx/bardbl2)
•For any /bardblx−y/bardbl∞≤0.01, we have /bardblexp(x)−exp(y)/bardbl2≤ /bardblexp(x)/bardbl2·2/bardblx−y/bardbl∞
Fact 3.2. For matrices X,Y, we have
•/bardblX⊤/bardbl=/bardblX/bardbl
•/bardblX/bardbl ≥ /bardblY/bardbl−/bardblX−Y/bardbl
•/bardblX+Y/bardbl ≤ /bardblX/bardbl+/bardblY/bardbl
•/bardblX·Y/bardbl ≤ /bardblX/bardbl·/bardblY/bardbl
•IfX/√recedesequalα·Y, then/bardblX/bardbl ≤α·/bardblY/bardbl
3.3 Lower bound on β
Lemma 3.3. If the following conditions holds
•/bardblA/bardbl ≤R
•/bardblx/bardbl2≤R
•Letβbe lower bound on /an}bracketle{texp(Ax),1n/an}bracketri}ht
Then we have
β≥exp(−R2)
Proof. We have
/an}bracketle{texp(Ax),1n/an}bracketri}ht=n/summationdisplay
i=1exp((Ax)i)
≥min
i∈[n]exp((Ax)i)
≥min
i∈[n]exp(−|(Ax)i|)
= exp(−max
i∈[n]|(Ax)i|)
= exp(−/bardblAx/bardbl∞)
≥exp(−/bardblAx/bardbl2)
≥exp(−R2)
the 1st step follows from simple algebra, the 2nd step comes f rom simple algebra, the 3rd step
follows from the fact that exp( x)≥exp(−|x|), the 4th step follows from the fact that exp( −x) is
monotonically decreasing, the 5th step comes from deﬁnitio n ofℓ∞norm, the 6th step follows from
Fact 3.1, the 7th step follows from the assumption on Aandx.
6

--- PAGE 8 ---
4 Softmax Function with respect to x
In Section 4.1, we give the deﬁnitions used in the computation. In Section 4.2, we compute the
gradient of the loss function with softmax function with res pect tox. Those functions includes
α(x)−1,α(x) andf(x).
4.1 Deﬁnitions
We deﬁne function softmax fas follows
Deﬁnition 4.1 (Function f, Deﬁnition 5.1 in [ DLS23 ]).Given a matrix A∈Rn×d. Let1ndenote
a length- nvector that all entries are ones. We deﬁne prediction functi onf:Rd→Rnas follows
f(x) :=/an}bracketle{texp(Ax),1n/an}bracketri}ht−1·exp(Ax).
Deﬁnition 4.2 (Loss function Lexp, Deﬁnition 5.3 in [ DLS23 ]).Given a matrix A∈Rn×dand a
vectorb∈Rn. We deﬁne loss function Lexp:Rd→Ras follows
Lexp(x) := 0.5·/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−b/bardbl2
2.
For convenient, we deﬁne two helpful notations αandc
Deﬁnition 4.3 (Normalized coeﬃcients, Deﬁnition 5.4 in [ DLS23 ]).We deﬁne α:Rd→Ras
follows
α(x) :=/an}bracketle{texp(Ax),1n/an}bracketri}ht.
Then, we can rewrite f(x)(see Deﬁnition 4.1) andLexp(x)(see Deﬁnition 4.2) as follows
•f(x) =α(x)−1·exp(Ax).
•Lexp(x) = 0.5·/bardblα(x)−1·exp(Ax)−b/bardbl2
2.
•Lexp(x) = 0.5·/bardblf(x)−b/bardbl2
2.
Deﬁnition 4.4 (Deﬁnition 5.5 in [ DLS23 ]).We deﬁne function c:Rd∈Rnas follows
c(x) :=f(x)−b.
Then we can rewrite Lexp(x)(see Deﬁnition 4.2) as follows
•Lexp(x) = 0.5·/bardblc(x)/bardbl2
2.
4.2 Gradient Computations
We state a lemma from previous work,
Lemma 4.5 (Gradient, Lemma 5.6 in [ DLS23 ]).If the following conditions hold
•Given matrix A∈Rn×dand a vector b∈Rn.
•Letα(x)be deﬁned in Deﬁnition 4.3.
•Letf(x)be deﬁned in Deﬁnition 4.1.
7

--- PAGE 9 ---
•Letc(x)be deﬁned in Deﬁnition 4.4.
•LetLexp(x)be deﬁned in Deﬁnition 4.2.
For each i∈[d], we have
•Part 1.
d exp(Ax)
dxi= exp(Ax)◦A∗,i
•Part 2.
d/an}bracketle{texp(Ax),1n/an}bracketri}ht
dxi=/an}bracketle{texp(Ax),A∗,i/an}bracketri}ht
•Part 3.
dα(x)−1
dxi=−α(x)−1·/an}bracketle{tf(x),A∗,i/an}bracketri}ht
•Part 4.
df(x)
dxi=dc(x)
dxi=−/an}bracketle{tf(x),A∗,i/an}bracketri}ht·f(x) +f(x)◦A∗,i
•Part 5.
dLexp(x)
dxi=A⊤
∗,i/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
1×n·/parenleftBig
f(x)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n×1/an}bracketle{tc(x),f(x)/an}bracketri}ht/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
scalar+ diag(f(x))/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n×nc(x)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
n×1/parenrightBig
5 Lipschitz with respect to x
In Section 5.1, we give the preliminary to compute the Lipschitz. In Sectio n5.2, we show the upper
bound of δb. In Section 5.3, we compute the Lipschitiz of function exp( Ax) with respect to x. In
Section 5.4, we compute the Lipschitiz of the function αwith respect to x. In Section 5.5, we
compute the Lipschitiz of function α−1with respect to x.
5.1 Preliminary
We can compute
dL
dx=g(x)
Letη >0 denote the learning rate.
We update
xt+1=xt+η·g(xt)
Deﬁnition 5.1. We deﬁne δb∈Rnto be the vector that satisﬁes the following conditions
/bardbl/an}bracketle{texp(Axt+1),1n/an}bracketri}ht−1exp(Axt+1)−b/bardbl2
2=/bardbl/an}bracketle{texp(Axt),1n/an}bracketri}ht−1exp(Axt)−b+δb/bardbl2
2
8

--- PAGE 10 ---
Let{−1,+1}ndenote a vector that each entry can be either −1 or +1. In the worst case, there
are 2npossible solutions, e.g.,
(/an}bracketle{texp(Axt+1),1n/an}bracketri}ht−1exp(Axt+1)−/an}bracketle{texp(Axt),1n/an}bracketri}ht−1exp(Axt))◦{−1,+1}n
The norm of all the choices are the same. Thus, it is suﬃcient t o only consider one solution as
follows.
Claim 5.2. We can write δbas follows
δb=/an}bracketle{texp(Axt+1),1n/an}bracketri}ht−1exp(Axt+1)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
f(xt+1)−/an}bracketle{texp(Axt),1n/an}bracketri}ht−1exp(Axt)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
f(xt).
Proof. The proof directly follows from Deﬁnition 5.1.
For convenience, we split δbinto two terms, and provide the following deﬁnitions
Deﬁnition 5.3. We deﬁne
δb,1:= (/an}bracketle{texp(Axt+1),1n/an}bracketri}ht−1−/an}bracketle{texp(Axt),1n/an}bracketri}ht−1)·exp(Axt+1)
δb,2:=/an}bracketle{texp(Axt),1n/an}bracketri}ht−1·(exp(Axt+1)−exp(Axt))
Thus, we have
Lemma 5.4. We have
•
δb=δb,1+δb,2
•We can rewrite δb,1as follows
δb,1= (α(xt+1)−1−α(xt)−1)·exp(Axt+1),
•We can rewrite δb,2as follows
δb,2=α(xt)−1·(exp(Axt+1)−exp(Axt)).
Proof. We have
δb=δb,1+δb,2
=α(xt+1)−1exp(Axt+1)−α(xt)−1exp(Axt+1)+
α(xt)−1exp(Axt+1)−α(xt)−1exp(Axt)
=α(xt+1)−1exp(Axt+1)−α(xt)−1exp(Axt)
=/an}bracketle{texp(Axt+1),1n/an}bracketri}ht−1exp(Axt+1)−/an}bracketle{texp(Axt),1n/an}bracketri}ht−1exp(Axt),
where the 1st step follows from the deﬁnitions of δb, the 2nd step follows from the deﬁnitions of δb,1
andδb,2, the 3rd step follows from simple algebra, the 4th step comes from the deﬁnition of α.
9

--- PAGE 11 ---
5.2 Upper Bounding δbwith respect to x
We can show that
Lemma 5.5. If the following conditions hold
•Letβ∈(0,1).
•Letδb,1∈Rnbe deﬁned as Deﬁnition 5.3.
•Letδb,2∈Rnbe deﬁned as Deﬁnition 5.3.
•Letδb=δb,1+δb,2.
•LetR≥4.
We have
•Part 1.
/bardblδb,1/bardbl2≤2β−2n1.5exp(2R2)·/bardblxt+1−xt/bardbl2
•Part 2.
/bardblδb,2/bardbl2≤2β−1√nRexp(R2)·/bardblxt+1−xt/bardbl2
•Part 3.
/bardblf(xt+1)−f(xt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δb/bardbl2≤4β−2n1.5Rexp(2R2)·/bardblxt+1−xt/bardbl2
Proof.Proof of Part 1. We have
/bardblδb,1/bardbl2≤ |α(xt+1)−1−α(xt)−1|·/bardblexp(Axt+1)/bardbl2
≤ |α(xt+1)−1−α(xt)−1|·√n·exp(R2)
≤β−2·|α(xt+1)−α(xt)|·√n·exp(R2)
≤β−2·√n·/bardblexp(Axt+1)−exp(Axt)/bardbl2·√n·exp(R2)
≤β−2·√n·2√nRexp(R2)/bardblxt+1−xt/bardbl2·√n·exp(R2)
= 2β−2n1.5Rexp(2R2)·/bardblxt+1−xt/bardbl2
where the ﬁrst step follows from deﬁnition, the second step f ollows from assumption on Aandx,
the third step follows Lemma 5.8, the forth step follows from Lemma 5.7, the ﬁfth step follows from
Lemma 5.6.
Proof of Part 2.
We have
/bardblδb,2/bardbl2≤ |α(xt+1)−1|·/bardblexp(Axt+1)−exp(Axt)/bardbl2
≤β−1·/bardblexp(Axt+1)−exp(Axt)/bardbl2
≤β−1·2√nRexp(2R2)·/bardblxt+1−xt/bardbl2
where the ﬁrst step follows from deﬁnition, the 2nd step come s from Lemma 5.6.
10

--- PAGE 12 ---
Proof of Part 3.
We have
/bardblδb/bardbl2=/bardblδb,1+δb,2/bardbl2
≤ /bardblδb,1/bardbl2+/bardblδb,2/bardbl2
≤2β−2n1.5Rexp(2R2)·/bardblxt+1−xt/bardbl2+ 2β−1n0.5Rexp(2R2)·/bardblxt+1−xt/bardbl2
≤2β−2n1.5Rexp(2R2)·/bardblxt+1−xt/bardbl2+ 2β−2n1.5Rexp(2R2)·/bardblxt+1−xt/bardbl2
≤4β−2n1.5Rexp(2R2)·/bardblxt+1−xt/bardbl2
where the 1st step follows from the deﬁnition of δb, the 2nd step follows from triangle inequality,
the 3rd step follows from the results in Part 1 and Part 2, the 4 th step follows from the fact that
n≥1 andβ−1≥1, the 5th step follows from simple algebra.
5.3 Lipschitz for function exp(Ax)with respect to x
Lemma 5.6. If the following conditions holds
•LetA∈Rn×d
•Let/bardblA(y−x)/bardbl∞<0.01
•Let/bardblA/bardbl ≤R
•Letx,ysatisfy that /bardblx/bardbl2≤Rand/bardbly/bardbl2≤R
Then we have
/bardblexp(Ax)−exp(Ay)/bardbl2≤2√nRexp(R2)·/bardblx−y/bardbl2.
Proof. We have
/bardblexp(Ax)−exp(Ay)/bardbl2≤ /bardblexp(Ax)/bardbl2·2/bardblA(x−y)/bardbl∞
≤√n·exp(/bardblAx/bardbl2)·2/bardblA(x−y)/bardbl∞
≤√nexp(R2)·2/bardblA(x−y)/bardbl2
≤√nexp(R2)·2/bardblA/bardbl·/bardblx−y/bardbl2
≤2√nRexp(R2)·/bardblx−y/bardbl2
where the 1st step follows from /bardblA(y−x)/bardbl∞<0.01 and Fact 3.1, the 2nd step comes from Fact 3.1,
the 3rd step follows from Fact 3.2, the 4th step follows from Fact 3.2, the last step follows from
/bardblA/bardbl ≤R.
5.4 Lipschitz for function α(x)with respect to x
We state a tool from previous work [ DLS23 ].
Lemma 5.7 (Lemma 7.2 in [ DLS23 ]).If the following conditions hold
•Letα(x)be deﬁned as Deﬁnition 4.3
Then we have
|α(x)−α(y)| ≤ /bardbl exp(Ax)−exp(Ay)/bardbl2·√n.
11

--- PAGE 13 ---
5.5 Lipschitz for function α(x)−1with respect to x
We state a tool from previous work [ DLS23 ].
Lemma 5.8 (Lemma 7.2 in [ DLS23 ]).If the following conditions hold
•Let/an}bracketle{texp(Ax),1n/an}bracketri}ht ≥β
•Let/an}bracketle{texp(Ay),1n/an}bracketri}ht ≥β
Then, we have
|α(x)−1−α(y)−1| ≤β−2·|α(x)−α(y)|.
6 Softmax Function with respect to A
In this section, we consider the function with respect to A. We deﬁne function softmax fas follows
Deﬁnition 6.1 (Function f, Reparameterized xbyAin Deﬁnition 4.1).Given a matrix A∈Rn×d.
Let1ndenote a length- nvector that all entries are ones. We deﬁne prediction functi onf:Rn×d→
Rnas follows
f(A) :=/an}bracketle{texp(Ax),1n/an}bracketri}ht−1·exp(Ax).
Similarly, we reparameterized xbyAfor our loss function L. We deﬁne loss function Las
follows
Deﬁnition 6.2 (Loss function Lexp, Reparameterized xbyAin Deﬁnition 4.2).Given a matrix
A∈Rn×dand a vector b∈Rn×d. We deﬁne loss function Lexp:Rn×d→Ras follows
Lexp(A) := 0.5·/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−b/bardbl2
2.
For convenience, we deﬁne two helpful notations αandcwith respect to Aas follows:
Deﬁnition 6.3 (Normalized coeﬃcients, Reparameterized xbyAin Deﬁnition 4.3).We deﬁne
α:Rn×d→Ras follows
α(A) :=/an}bracketle{texp(Ax),1n/an}bracketri}ht.
Then, we can rewrite f(A)(see Deﬁnition 6.1) andLexp(A)(see Deﬁnition 6.2) as follows
•f(A) =α(A)−1·exp(Ax).
•Lexp(A) = 0.5·/bardblα(A)−1·exp(Ax)−b/bardbl2
2.
•Lexp(A) = 0.5·/bardblf(A)−b/bardbl2
2.
Deﬁnition 6.4 (Reparameterized xbyAin Deﬁnition 4.4).We deﬁne function c:Rn×d∈Rnas
follows
c(A) :=f(A)−b.
Then we can rewrite Lexp(A)(see Deﬁnition 6.2) as follows
•Lexp(A) = 0.5·/bardblc(A)/bardbl2
2.
12

--- PAGE 14 ---
7 Lipschitz with respect to A
In Section 7.1, we give the preliminary to compute the Lipschitz. In Sectio n7.2, we show the upper
bound of δbwith respect to A. In Section 7.3, we compute the Lipschitiz of function exp( Ax) with
respect to A. In Section 7.4, we compute the Lipschitiz of the function αwith respect to A. In
Section 7.5, we compute the Lipschitiz of function α−1with respect to A.
7.1 Preliminary
We deﬁne δbas follows
Deﬁnition 7.1 (Reparameterized xbyAin Deﬁnition 5.1).We deﬁne δb∈Rnto be the vector
that satisﬁes the following conditions
/bardbl/an}bracketle{texp(At+1x),1n/an}bracketri}ht−1exp(At+1x)−b/bardbl2
2=/bardbl/an}bracketle{texp(Atx),1n/an}bracketri}ht−1exp(Atx)−b+δb/bardbl2
2
Claim 7.2 (Reparameterized xbyAin Deﬁnition 5.2).We can write δbas follows
δb=/an}bracketle{texp(At+1x),1n/an}bracketri}ht−1exp(At+1x)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
f(At+1)−/an}bracketle{texp(Atx),1n/an}bracketri}ht−1exp(Atx)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
f(At).
Proof. The proof directly follows from Deﬁnition 7.1.
For convenient, we split δbinto two terms, and provide the following deﬁnitions
Deﬁnition 7.3 (Reparameterized xbyAin Deﬁnition 5.3).We deﬁne
δb,1:= (/an}bracketle{texp(At+1x),1n/an}bracketri}ht−1−/an}bracketle{texp(Atx),1n/an}bracketri}ht−1)·exp(At+1x)
δb,2:=/an}bracketle{texp(Atx),1n/an}bracketri}ht−1·(exp(At+1x)−exp(Atx))
Thus, we have
Lemma 7.4 (Reparameterized xbyAin Lemma 5.4).We have
•We can rewrite δb∈Rnas follows
δb=δb,1+δb,2
•We can rewrite δb,1∈Rnas follows
δb,1= (α(At+1)−1−α(At)−1)·exp(At+1x),
•We can rewrite δb,2∈Rnas follows
δb,2=α(At)−1·(exp(At+1x)−exp(Atx)).
Proof. We have
δb=δb,1+δb,2
=α(At+1)−1exp(At+1x)−α(At)−1exp(At+1x)+
α(At)−1exp(At+1x)−α(At)−1exp(Atx)
=α(At+1)−1exp(At+1x)−α(At)−1exp(Atx)
=/an}bracketle{texp(At+1x),1n/an}bracketri}ht−1exp(At+1x)−/an}bracketle{texp(Atx),1n/an}bracketri}ht−1exp(Atx),
where the 1st step follows from the deﬁnitions of δb, the 2nd step follows from the deﬁnitions of δb,1
andδb,2, the 3rd step comes from simple algebra, the 4th step comes fr om the deﬁnition of α.
13

--- PAGE 15 ---
7.2 Upper Bounding δbwith respect to A
We can show that
Lemma 7.5 (Reparameterized xbyAin Lemma 5.5).If the following conditions hold
•Letβ∈(0,1).
•Letδb,1∈Rnbe deﬁned as Deﬁnition 7.3.
•Letδb,2∈Rnbe deﬁned as Deﬁnition 7.3.
•Letδb=δb,1+δb,2.
•LetR≥4.
We have
•Part 1.
/bardblδb,1/bardbl2≤2β−2n1.5exp(2R2)·/bardblAt+1−At/bardbl2
•Part 2.
/bardblδb,2/bardbl2≤2β−1√nRexp(R2)·/bardblAt+1−At/bardbl2
•Part 3.
/bardblf(At+1)−f(At)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
δb/bardbl2≤4β−2n1.5Rexp(2R2)·/bardblAt+1−At/bardbl2
Proof.Proof of Part 1. We have
/bardblδb,1/bardbl2≤ |α(At+1)−1−α(At)−1|·/bardblexp(At+1x)/bardbl2
≤ |α(At+1)−1−α(At)−1|·√n·exp(R2)
≤β−2·|α(At+1)−α(At)|·√n·exp(R2)
≤β−2·√n·/bardblexp(At+1x)−exp(Atx)/bardbl2·√n·exp(R2)
≤β−2·√n·2√nRexp(R2)/bardblAt+1−At/bardbl·√n·exp(R2)
= 2β−2n1.5Rexp(2R2)·/bardblAt+1−At/bardbl
where the ﬁrst step follows from deﬁnition, the second step f ollows from assumption on Aandx,
the third step follows Lemma 7.8, the forth step follows from Lemma 7.7, the ﬁfth step follows from
Lemma 7.6.
Proof of Part 2.
We have
/bardblδb,2/bardbl2≤ |α(At+1)−1|·/bardblexp(At+1x)−exp(Atx)/bardbl2
≤β−1·/bardblexp(At+1x)−exp(Atx)/bardbl2
≤β−1·2√nRexp(2R2)·/bardblAt+1−At/bardbl
Proof of Part 3.
14

--- PAGE 16 ---
We have
/bardblδb/bardbl2=/bardblδb,1+δb,2/bardbl2
≤ /bardblδb,1/bardbl2+/bardblδb,2/bardbl2
≤2β−2n1.5Rexp(2R2)·/bardblAt+1−At/bardbl+ 2β−1n0.5Rexp(2R2)·/bardblAt+1−At/bardbl
≤2β−2n1.5Rexp(2R2)·/bardblAt+1−At/bardbl+ 2β−2n1.5Rexp(2R2)·/bardblAt+1−At/bardbl
≤4β−2n1.5Rexp(2R2)·/bardblAt+1−At/bardbl
where the 1st step follows from the deﬁnition of δb, the 2nd step comes from triangle inequality,
the 3rd step comes from the results in Part 1 and Part 2, the 4th step follows from the fact that
n≥1 andβ−1≥1, the 5th step follows from simple algebra.
7.3 Lipschitz for function exp(Ax)with respect to A
Lemma 7.6 (Reparameterized xbyAin Lemma 5.6).If the following conditions holds
•LetA,B∈Rn×d
•Let/bardbl(A−B)x/bardbl∞<0.01
•Let/bardblA/bardbl ≤R
•Letxsatisfy that /bardblx/bardbl2≤R
Then we have
/bardblexp(Ax)−exp(Bx)/bardbl2≤2√nRexp(R2)·/bardblA−B/bardbl.
Proof. We have
/bardblexp(Ax)−exp(Bx)/bardbl2≤ /bardblexp(Ax)/bardbl2·2/bardbl(A−B)x/bardbl∞
≤√n·exp(/bardblAx/bardbl2)·2/bardbl(A−B)x/bardbl∞
≤√nexp(R2)·2/bardbl(A−B)x/bardbl2
≤√nexp(R2)·2/bardblA−B/bardbl·/bardblx/bardbl2
≤2√nRexp(R2)·/bardblA−B/bardbl
where the 1st step follows from /bardblA(y−x)/bardbl∞<0.01 and Fact 3.1, the 2nd step follows from Fact 3.1,
the 3rd step follows from Fact 3.2, the 4th step comes from Fact 3.2, the last step follows from
/bardblA/bardbl ≤R.
7.4 Lipschitz for function α(A)with respect to A
Lemma 7.7 (Reparameterized xbyAin Lemma 5.7).If the following conditions hold
•Letα(A)be deﬁned as Deﬁnition 6.3
Then we have
|α(A)−α(B)| ≤ /bardbl exp(Ax)−exp(Bx)/bardbl2·√n.
15

--- PAGE 17 ---
Proof. We have
|α(A)−α(B)|=|/an}bracketle{texp(Ax)−exp(Bx),1n/an}bracketri}ht|
≤ /bardblexp(Ax)−exp(Bx)/bardbl2·√n
where the 1st step comes from the deﬁnition of α(x), the 2nd step follows from Cauchy-Schwarz
inequality (Fact 3.1).
7.5 Lipschitz for function α(A)−1with respect to A
Lemma 7.8 (Reparameterized xbyAin Lemma 5.8).If the following conditions hold
•Let/an}bracketle{texp(Ax),1n/an}bracketri}ht ≥β
•Let/an}bracketle{texp(Bx),1n/an}bracketri}ht ≥β
Then, we have
|α(A)−1−α(B)−1| ≤β−2·|α(A)−α(B)|.
Proof. We can show that
|α(A)−1−α(B)−1|=α(A)−1α(B)−1·|α(A)−α(B)|
≤β−2·|α(A)−α(B)|
where the 1st step follows from simple algebra, the 2nd step f ollows from α(A)≥β,α(B)≥β.
8 Main Results
In Section 8.1, we show our upper bound result of δbwith respect to x. In Section 8.2, we show
our upper bound result of δbwith respect to A.
8.1 Shifting Weight Parameter x
Theorem 8.1 (Bounded shift for Learning in-context, informal of Theore m1.4).If the following
conditions hold
•LetA∈Rn×d
•/bardblA/bardbl ≤R
•/bardblA(xt+1−xt)/bardbl∞<0.01
•LetR≥4
•LetM:=n1.5exp(10R2).
We consider the softmax regression problem
min
x/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−b/bardbl2
If we move the xttoxt+1, then we’re solving a new softmax regression problem with
min
x/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−/tildewideb/bardbl2
where
/bardbl/tildewideb−b/bardbl2≤M·/bardblxt+1−xt/bardbl2
16

--- PAGE 18 ---
Proof. We have
/bardbl/tildewideb−b/bardbl2≤4β−2n1.5Rexp(2R2)·/bardblxt+1−xt/bardbl2
≤4n1.5Rexp(2R2) exp(2R2)·/bardblxt+1−xt/bardbl2
≤n1.5(4R) exp(4R2)·/bardblxt+1−xt/bardbl2
≤n1.5exp(6R2) exp(4R2)·/bardblxt+1−xt/bardbl2
≤n1.5exp(10R2)·/bardblxt+1−xt/bardbl2
≤M·/bardblxt+1−xt/bardbl2
where the 1st step follows from Lemma 5.5, the 2nd step comes from Lemma 3.3, the 3rd step
comes from simple algebra, the 4th step follows from simple a lgebra, the 5th step follows from
simple algebra and the 6th step follows from the deﬁnition of M.
8.2 Shifting Sentence Data A
Theorem 8.2 (Bounded shift for Learning in-context, informal of Theore m1.4).If the following
conditions hold
•LetA∈Rn×d
•/bardblA/bardbl ≤R
•/bardbl(At+1−At)x/bardbl∞<0.01
•LetR≥4
•LetM:=n1.5exp(10R2).
We consider the softmax regression problem
min
x/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−b/bardbl2
If we move the xttoxt+1, then we’re solving a new softmax regression problem with
min
x/bardbl/an}bracketle{texp(Ax),1n/an}bracketri}ht−1exp(Ax)−/tildewideb/bardbl2
where
/bardbl/tildewideb−b/bardbl2≤M·/bardblAt+1−At/bardbl.
Proof. We have
/bardbl/tildewideb−b/bardbl2≤4β−2n1.5Rexp(2R2)·/bardblAt+1−At/bardbl
≤4n1.5Rexp(2R2) exp(2R2)·/bardblAt+1−At/bardbl
≤n1.5(4R) exp(4R2)·/bardblAt+1−At/bardbl
≤n1.5exp(6R2) exp(4R2)·/bardblAt+1−At/bardbl
≤n1.5exp(10R2)·/bardblAt+1−At/bardbl
≤M·/bardblAt+1−At/bardbl
where the 1st step follows from Lemma 5.5, the 2nd step follows from Lemma 3.3, the 3rd step
follows from simple algebra, the 4th step comes from simple a lgebra, the 5th step comes from simple
algebra and the 6th step follows from the deﬁnition of M.
17

--- PAGE 19 ---
References
[AS23] Josh Alman and Zhao Song. Fast attention requires bou nded entries. arXiv preprint
arXiv:2302.13214 , 2023.
[ASA+22] Ekin Aky¨ urek, Dale Schuurmans, Jacob Andreas, Tengyu M a, and Denny Zhou. What
learning algorithm is in-context learning? investigation s with linear models. arXiv
preprint arXiv:2211.15661 , 2022.
[BAG20] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the Ability and Limitations of
Transformers to Recognize Formal Languages. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pages 7096–7116,
Online, November 2020. Association for Computational Ling uistics.
[Bel22] Yonatan Belinkov. Probing classiﬁers: Promises, s hortcomings, and advances. Compu-
tational Linguistics , 48(1):207–219, March 2022.
[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J ared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, et al.
Language models are few-shot learners. Advances in neural information processing
systems , 33:1877–1901, 2020.
[BP66] Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of
ﬁnite state markov chains. The annals of mathematical statistics , 37(6):1554–1563,
1966.
[BPG20] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of
transformers and its implications in sequence modeling. In Proceedings of the 24th Con-
ference on Computational Natural Language Learning , pages 455–475, Online, Novem-
ber 2020. Association for Computational Linguistics.
[BSZ23] Jan van den Brand, Zhao Song, and Tianyi Zhou. Algori thm and hardness for dynamic
attention maintenance in large language models. arXiv preprint arXiv:2304.02207 ,
2023.
[CDL+22] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao So ng, Atri Rudra, and
Christopher Re. Pixelated butterﬂy: Simple and eﬃcient spa rse training for neural
network models. In International Conference on Learning Representations (ICLR) ,
2022.
[CDW+21] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, a nd Christopher R´ e. Scat-
terbrain: Unifying sparse and low-rank attention. Advances in Neural Information
Processing Systems (NeurIPS) , 34:17413–17426, 2021.
[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sut skever. Generating long sequences
with sparse transformers. arXiv preprint arXiv:1904.10509 , 2019.
[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maar ten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, et al. Palm: Scaling language modeling with p athways. arXiv preprint
arXiv:2204.02311 , 2022.
18

--- PAGE 20 ---
[DCL+21] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao So ng, Atri Rudra, and
Christopher Re. Pixelated butterﬂy: Simple and eﬃcient spa rse training for neural
network models. arXiv preprint arXiv:2112.00029 , 2021.
[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kris tina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language u nderstanding. arXiv preprint
arXiv:1810.04805 , 2018.
[DGS23] Yichuan Deng, Yeqi Gao, and Zhao Song. Solving tenso r low cycle rank approximation.
arXiv preprint arXiv:2304.06594 , 2023.
[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob U szkoreit, and /suppress Lukasz Kaiser.
Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.
[DKOD20] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-
eﬃcient attention using asymmetric clustering. Advances in Neural Information Pro-
cessing Systems (NeurIPS) , 33:6476–6489, 2020.
[DLS23] Yichuan Deng, Zhihang Li, and Zhao Song. Attention s cheme inspired softmax regres-
sion.arXiv preprint arXiv:2304.10411 , 2023.
[DMS23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Ran domized and determinis-
tic attention sparsiﬁcation algorithms for over-paramete rized feature dimension. arxiv
preprint: arxiv 2304.03426 , 2023.
[EGKZ21] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and C yril Zhang. Inductive biases
and variable creation in self-attention mechanisms. arXiv preprint arXiv:2110.10090 ,
2021.
[EGZ20] Javid Ebrahimi, Dhruv Gelda, and Wei Zhang. How can s elf-attention networks recog-
nize Dyck-n languages? In Findings of the Association for Computational Linguistics:
EMNLP 2020 , pages 4301–4306, Online, November 2020. Association for C omputa-
tional Linguistics.
[GHG+20] Peng Gao, Chiori Hori, Shijie Geng, Takaaki Hori, and Jon athan Le Roux. Multi-pass
transformer for machine translation. arXiv preprint arXiv:2009.11382 , 2020.
[GMS23] Yeqi Gao, Sridhar Mahadevan, and Zhao Song. An over- parameterized exponential
regression. arXiv preprint arXiv:2303.16504 , 2023.
[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gr egory Valiant. What can trans-
formers learn in-context? a case study of simple function cl asses.arXiv preprint
arXiv:2208.01066 , 2022.
[HL19] John Hewitt and Percy Liang. Designing and interpret ing probes with control tasks.
InProceedings of the 2019 Conference on Empirical Methods in Na tural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 2733–2743, Hong Kong, China, November 2019. Associa tion
for Computational Linguistics.
[KKL20] Nikita Kitaev, /suppress Lukasz Kaiser, and Anselm Levskaya . Reformer: The eﬃcient trans-
former.arXiv preprint arXiv:2001.04451 , 2020.
19

--- PAGE 21 ---
[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap pas, and Fran¸ cois Fleuret. Trans-
formers are rnns: Fast autoregressive transformers with li near attention. In Interna-
tional Conference on Machine Learning , pages 5156–5165. PMLR, 2020.
[LSX+22] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, H oifung Poon, and Tie-
Yan Liu. Biogpt: generative pre-trained transformer for bi omedical text generation and
mining.Brieﬁngs in Bioinformatics , 23(6), 2022.
[LSZ23] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regu larized exp, cosh and sinh
regression problems. arXiv preprint, 2303.15725 , 2023.
[LWD+23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan , Zhao Song, Anshumali
Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, and B eidi Chen. Deja vu:
Contextual sparsity for eﬃcient llms at inference time. In Manuscript , 2023.
[ONR+22] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo , Jo˜ ao Sacramento, Alexan-
der Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Tra nsformers learn in-
context by gradient descent. arXiv preprint arXiv:2212.07677 , 2022.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
[PCR19] Gabriele Prato, Ella Charlaix, and Mehdi Rezagholi zadeh. Fully quantized transformer
for machine translation. arXiv preprint arXiv:1910.10485 , 2019.
[PMB19] Jorge P´ erez, Javier Marinkovi´ c, and Pablo Barcel ´ o. On the turing completeness of
modern neural network architectures. arXiv preprint arXiv:1901.03429 , 2019.
[RNS+18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Su tskever, et al. Improving
language understanding by generative pre-training. 2018.
[SZKS21] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob St einhardt. Approximating how
single head attention learns. arXiv preprint arXiv:2103.07601 , 2021.
[TDP19] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT re discovers the classical NLP
pipeline. In Proceedings of the 57th Annual Meeting of the Association fo r Computa-
tional Linguistics , pages 4593–4601, Florence, Italy, July 2019. Association for Compu-
tational Linguistics.
[VB19] Jesse Vig and Yonatan Belinkov. Analyzing the struct ure of attention in a transformer
language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages 63–76, Florence, Italy, August 2019.
Association for Computational Linguistics.
[VBC20] James Vuckovic, Aristide Baratin, and Remi Tachet d es Combes. A mathematical
theory of attention. arXiv preprint arXiv:2007.02876 , 2020.
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor eit, Llion Jones, Aidan N
Gomez, /suppress Lukasz Kaiser, and Illia Polosukhin. Attention is a ll you need. Advances in
neural information processing systems , 30, 2017.
[WCM21] Colin Wei, Yining Chen, and Tengyu Ma. Statisticall y meaningful approximation:
a case study on approximating turing machines with transfor mers.arXiv preprint
arXiv:2107.13163 , 2021.
20

--- PAGE 22 ---
[WLK+20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Ha o Ma. Linformer:
Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[XRLM21] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation
of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080 ,
2021.
[YBR+20] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, S ashank Reddi, and Sanjiv
Kumar. Are transformers universal approximators of sequen ce-to-sequence functions?
InInternational Conference on Learning Representations , 2020.
[YPPN21] Shunyu Yao, Binghui Peng, Christos Papadimitriou , and Karthik Narasimhan. Self-
attention networks can process bounded hierarchical langu ages. In Proceedings of the
59th Annual Meeting of the Association for Computational Ling uistics and the 11th In-
ternational Joint Conference on Natural Language Processin g (Volume 1: Long Papers) ,
pages 3770–3785, Online, August 2021. Association for Comp utational Linguistics.
[ZBB+22] Yi Zhang, Arturs Backurs, S´ ebastien Bubeck, Ronen Elda n, Suriya Gunasekar, and Tal
Wagner. Unveiling transformers with lego: a synthetic reas oning task, 2022.
[ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, and Amin Karb asi. Kdeformer: Accelerating
transformers via kernel density estimation. arXiv preprint arXiv:2302.02451 , 2023.
[ZKV+20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit , Seungyeon Kim, Sashank
Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive method s good for attention
models? Advances in Neural Information Processing Systems , 33:15383–15393, 2020.
[ZPGA23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanje ev Arora. Do transformers parse
while predicting the masked word? arXiv preprint arXiv:2303.08117 , 2023.
[ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe , Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al . Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068 , 2022.
21
