# 2204.02329.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2204.02329.pdf
# Kích thước tệp: 512146 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Các mô hình ngôn ngữ có thể học từ các giải thích trong ngữ cảnh không?
Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan
Kory Mathewson, Michael Henry Tessler, Antonia Creswell
James L. McClelland, Jane X. Wang, Felix Hill
DeepMind
London, UK
Tóm tắt
Các Mô hình Ngôn ngữ (LMs) có thể thực hiện các nhiệm vụ mới bằng cách thích ứng với một vài ví dụ trong ngữ cảnh. Đối với con người, các giải thích kết nối các ví dụ với các nguyên tắc nhiệm vụ có thể cải thiện việc học. Do đó, chúng tôi điều tra xem các giải thích về các ví dụ few-shot có thể giúp ích cho các LM không. Chúng tôi chú thích các câu hỏi từ 40 nhiệm vụ thách thức với các giải thích câu trả lời và các giải thích điều khiển phù hợp khác nhau. Chúng tôi đánh giá cách các loại giải thích, hướng dẫn và điều khiển khác nhau ảnh hưởng đến hiệu suất zero-shot và few-shot. Chúng tôi phân tích các kết quả này bằng các kỹ thuật mô hình hóa thống kê đa cấp có tính đến các phụ thuộc lồng nhau giữa các điều kiện, nhiệm vụ, prompt và mô hình. Chúng tôi thấy rằng các giải thích có thể cải thiện hiệu suất—ngay cả khi không có điều chỉnh. Hơn nữa, các giải thích được điều chỉnh thủ công cho hiệu suất trên một tập xác thực nhỏ mang lại lợi ích lớn hơn đáng kể, và xây dựng một prompt bằng cách chọn các ví dụ và giải thích cùng nhau cải thiện hiệu suất đáng kể so với việc chỉ chọn các ví dụ. Cuối cùng, ngay cả các giải thích chưa được điều chỉnh cũng vượt trội hơn các điều khiển được khớp cẩn thận, cho thấy rằng lợi ích là do liên kết giữa một ví dụ và giải thích của nó, chứ không phải do các đặc trưng cấp thấp hơn. Tuy nhiên, chỉ các mô hình lớn mới có lợi. Tóm lại, các giải thích có thể hỗ trợ việc học trong ngữ cảnh của các LM lớn trên các nhiệm vụ thách thức.

1 Giới thiệu
Một mô hình mới đã nổi lên trong xử lý ngôn ngữ tự nhiên: prompt few-shot của các Mô hình Ngôn ngữ (LMs). Các LM lớn có vẻ như thể hiện một số khả năng học trong ngữ cảnh, sao cho chúng có thể suy ra cách thực hiện một nhiệm vụ ngôn ngữ mới few-shot (Brown et al., 2020)—từ một vài ví dụ về các cặp đầu vào và đầu ra trong cửa sổ ngữ cảnh của mô hình, nhưng không có đào tạo. Ví dụ, những mô hình này có thể trả lời các câu hỏi kiến thức tổng quát hoặc thực hiện phép tính chính xác hơn với một vài ví dụ về các câu hỏi liên quan và câu trả lời đúng trong ngữ cảnh. Mặc dù không phải lúc nào cũng rõ ràng những gì đang được học hoặc suy ra từ những prompt này (ví dụ Min et al., 2022; Webson and Pavlick, 2021), việc prompt hóa là một lĩnh vực con đang phát triển (Liu et al., 2021).

Khả năng thích ứng với một nhiệm vụ mới từ một prompt few-shot có một số điểm tương đồng với sự linh hoạt mà con người có thể thích ứng với hướng dẫn hoặc ví dụ. Tuy nhiên, các giải thích cũng đóng vai trò trung tâm trong việc học của con người (Ahn et al., 1992)—các giải thích làm nổi bật các nguyên tắc nhiệm vụ cho phép chúng ta tổng quát hóa một cách rộng rãi (Lombrozo and Carey, 2006; Lombrozo, 2006). Ví dụ, một giải thích có thể mở rộng một câu trả lời ngắn gọn (ví dụ "false") bằng cách kết nối nó với quá trình suy luận rộng lớn hơn cần thiết để giải quyết vấn đề (ví dụ "những tuyên bố này mâu thuẫn với nhau; một số không thể vừa là số nguyên tố vừa chia hết cho 6, là số hợp"). Vì vậy, các giải thích có thể làm rõ nhiệm vụ dự định bằng cách minh họa các nguyên tắc liên kết câu hỏi với câu trả lời.

Do đó, chúng tôi điều tra xem các LM có thể cũng có lợi từ các giải thích khi học từ các ví dụ trong ngữ cảnh không. Các giải thích về câu trả lời có thể cải thiện hiệu suất nhiệm vụ few-shot không? Lưu ý rằng câu hỏi này không tập trung vào khả năng giải thích như một phương tiện để giúp người dùng hiểu một mô hình (ví dụ Danilevsky et al., 2020); thay vào đó, chúng tôi tập trung vào việc liệu các giải thích few-shot có thể giúp bản thân mô hình "hiểu" nhiệm vụ hay không, được đánh giá thông qua hiệu suất (cf. Santoro et al., 2021).

Câu hỏi này thú vị vì nhiều lý do. Về mặt thực tế, nó có tiềm năng cải thiện hiệu suất few-shot. Nhưng cơ bản hơn, câu trả lời chiếu sáng câu hỏi khoa học về loại khả năng học trong ngữ cảnh mà các LM thể hiện, đây là một chủ đề đang được tranh luận (ví dụ Min et al., 2022; Webson and Pavlick, 2021).

Một số phát hiện hội tụ cho thấy rằng các giải thích có thể cải thiện hiệu suất few-shot. Đầu tiên, việc prompt với hướng dẫn rõ ràng hoặc mô tả nhiệm vụ có thể là một cách hiệu quả để thích ứng các LM với một nhiệm vụ mới (ví dụ Liu et al., 2021). Hơn nữa, việc chia nhỏ các bước của quá trình suy luận cho các LM có thể cải thiện hiệu suất few-shot (Nye et al., 2021; Wei et al., 2022; Zelikman et al., 2022). Các giải thích về các ví dụ trong một prompt few-shot có thể cũng hỗ trợ việc suy ra nhiệm vụ chính xác, và do đó cải thiện hiệu suất nhiệm vụ hạ nguồn.

Các LM có thể có lợi từ các giải thích mà không cần thiết phải "hiểu" các giải thích theo cách giống con người (cf. Mitchell, 2021). Các giải thích phổ biến trong diễn ngôn của con người, và do đó các LM sẽ gặp các giải thích trong đào tạo mà được dự định (bởi những con người đã viết chúng) để làm rõ sự hiểu biết và cải thiện suy luận trong tương lai. Ví dụ, có nhiều đáp án thi trực tuyến chứa các câu hỏi, câu trả lời, và giải thích về những câu trả lời đó có thể giúp dự đoán câu trả lời cho các câu hỏi tiếp theo trong bài thi. Những mẫu dữ liệu này có thể đủ để học cách dự đoán câu trả lời cho các câu hỏi tiếp theo tốt hơn sau khi thấy các giải thích.

Chúng tôi không phải là những người đầu tiên khám phá các giải thích; với sự quan tâm ngày càng tăng đối với việc điều chỉnh prompt, đã có các công trình khác về cách thông tin phụ trợ trong ngữ cảnh có thể ảnh hưởng đến hiệu suất mô hình (ví dụ Wei et al., 2022; Reynolds and McDonell, 2021), cũng như một loạt các công trình rộng lớn khác ngoài prompt hóa về các chủ đề như đào tạo hoặc điều chỉnh với các giải thích (ví dụ Camburu et al., 2018; Ling et al., 2017; Hendricks et al., 2016; Narang et al., 2020; Lertvittayakumjorn and Toni, 2021). So với các công trình trước đây, chúng tôi tập trung đặc biệt vào tác dụng của các giải thích sau câu trả lời—trong khi ví dụ Wei et al. (2022) cung cấp các chuỗi suy luận trước câu trả lời. Các giải thích trước và sau câu trả lời có tác động khác nhau đến suy luận và đánh giá của mô hình, và mang lại những hiểu biết khoa học riêng biệt. Hơn nữa, chúng tôi đánh giá trên một tập nhiệm vụ thách thức và đa dạng rõ rệt, và bao gồm các so sánh và phân tích điều khiển cẩn thận. Xem Phần 4 để thảo luận thêm. Chúng tôi nhấn mạnh các đóng góp sau:

• Chúng tôi chú thích 40 nhiệm vụ ngôn ngữ đa dạng, thách thức với các giải thích về ví dụ, và phát hành các chú thích này.
• Chúng tôi đánh giá một số LM sau khi prompt với hoặc không có các ví dụ few-shot, giải thích, hướng dẫn, và điều kiện điều khiển.
• Các giải thích về ví dụ trong một prompt few-shot có thể cải thiện hiệu suất của các mô hình lớn; ngay cả khi không có điều chỉnh, chúng vượt trội hơn các điều kiện điều khiển được khớp.
• Các giải thích được điều chỉnh hoặc chọn lọc sử dụng một tập xác thực nhỏ có thể có tác dụng lớn hơn.
• Chúng tôi phân tích kết quả của chúng tôi bằng các mô hình thống kê phân cấp tôn trọng các phụ thuộc giữa các nhiệm vụ, mục, và yếu tố prompt. Chúng tôi nhấn mạnh giá trị rộng lớn hơn của các phương pháp này.

Hướng dẫn nhiệm vụ | Ví dụ few-shot #1 | Giải thích câu trả lời
4 ví dụ thêm + giải thích | Câu hỏi mục tiêu

Trả lời những câu hỏi này bằng cách xác định xem câu thứ hai có phải là một diễn giải thích hợp của câu đầu tiên, câu ẩn dụ, hay không.

Q: Đôi mắt của David như dao găm nhìn Paul khi Paul mời bạn gái mới của anh ấy khiêu vũ. <- -> David có hai dao găm khi Paul mời bạn gái mới của anh ấy khiêu vũ.
lựa chọn: True
lựa chọn: False
A: False
Giải thích: Đôi mắt của David không thực sự là dao găm, đó là một ẩn dụ được sử dụng để ám chỉ rằng David đang nhìn chăm chăm dữ dội vào Paul....

Q: Cả cuộc đời chúng ta bơi ngược dòng sóng hướng về ánh sáng xanh của hạnh phúc. <- -> Cả cuộc đời chúng ta cố gắng đạt đến hạnh phúc.
lựa chọn: True
lựa chọn: False
A:

Hình 1: Ví dụ prompt bao gồm hướng dẫn nhiệm vụ, các ví dụ few-shot với giải thích, và câu hỏi mục tiêu. Hướng dẫn nhiệm vụ phác thảo nhiệm vụ cần thực hiện, trước các ví dụ (hoặc một mình, trong prompt zero-shot). Giải thích câu trả lời có thể được thêm vào mỗi ví dụ trong prompt few-shot. Hiệu suất luôn được đánh giá trên câu hỏi mục tiêu. Bởi vì các giải thích được thêm vào các ví dụ trên một dòng riêng biệt sau câu trả lời, cùng một chỉ số đánh giá có thể được sử dụng cho câu hỏi mục tiêu bất kể có cung cấp giải thích hay không. (Ví dụ từ metaphor_boolean.)

2 Phương pháp
2.1 Tập dữ liệu được lấy mẫu từ BIG-Bench
BIG-bench collaboration (2021) đã crowdsource một tập các nhiệm vụ được dự định là thách thức đối với các LM, ngay cả những cái được đào tạo để mô hình hóa tất cả văn bản trên internet. Những nhiệm vụ này liên quan đến các loại suy luận, nền tảng, v.v. khác nhau có thể khó học từ văn bản một mình (cf. Rae et al., 2021). Cho các thí nghiệm của chúng tôi, chúng tôi đã chọn một tập con của 40 nhiệm vụ và nhiệm vụ con từ tập lớn hơn này bao trùm nhiều loại suy luận, kỹ năng và lĩnh vực. Những nhiệm vụ này thú vị cả vì sự đa dạng của chúng và việc lấy mẫu bán đối nghịch của chúng.

Ví dụ, một số nhiệm vụ chúng tôi xem xét bao gồm suy ra mục tiêu từ hành động, suy luận về các lập luận quy nạp toán học, suy luận về tính nhân quả, hoặc suy ra giả định đằng sau một phát ngôn. Như những ví dụ cụ thể: metaphor_boolean yêu cầu xác định xem một câu có diễn giải chính xác một câu khác, câu ẩn dụ hay không (Hình 1); trong khi penguins_in_a_table yêu cầu trả lời các câu hỏi suy luận về dữ liệu dạng bảng. Mỗi nhiệm vụ được chú thích với các thẻ, cho phép chúng tôi khám phá tác dụng của các giải thích trên các loại nhiệm vụ khác nhau. Xem Phụ lục A cho tất cả các nhiệm vụ được sử dụng và quy trình chọn lọc.

2.2 Các mô hình ngôn ngữ
Chúng tôi được cấp quyền truy cập đánh giá vào một tập các LM từ 1 tỷ đến 280 tỷ tham số (Rae et al., 2021). Những mô hình này là các LM Transformer chỉ decoder (Vaswani et al., 2017), sử dụng mã hóa vị trí tương đối và bộ tokenizer SentencePiece 32,000 token (Kudo and Richardson, 2018). Các mô hình khác nhau trong họ khác nhau về các đặc trưng kiến trúc như số lớp và kích thước embedding, nhưng quan trọng là đã được đào tạo trên cùng một lượng của cùng một tập dữ liệu, với các cửa sổ ngữ cảnh tương đương (2048 token). Xem bài báo gốc để biết chi tiết đào tạo đầy đủ. Việc đánh giá trên một tập mô hình tương đương trên nhiều quy mô cho phép chúng tôi kiểm tra xem tác dụng của các giải thích có phụ thuộc vào quy mô mô hình hay không, như nhiều hiệu ứng học trong ngữ cảnh (Brown et al., 2020; Ganguli et al., 2022). Cuối cùng, mô hình lớn nhất đạt được một số thành công trên nhiều nhiệm vụ BIG-Bench few-shot (Rae et al., 2021), nhưng còn xa mới hoàn hảo, có nghĩa là các giải thích có thể cải thiện hiệu suất của nó.

2.3 Chú thích các ví dụ với giải thích
Kết quả hiệu suất "Few-shot" có thể gây hiểu lầm nếu nhiều ví dụ hơn được sử dụng để điều chỉnh prompt (Perez et al., 2021). Để tránh các vấn đề tương tự, chúng tôi trước tiên khám phá lợi ích thô của các giải thích chưa được điều chỉnh. Để tạo ra các giải thích chưa được điều chỉnh, một tác giả duy nhất đã chú thích 15 ví dụ nhiệm vụ được chọn ngẫu nhiên (cặp câu hỏi/câu trả lời) từ mỗi tập dữ liệu với các giải thích "chuyên gia" sẽ giúp con người hiểu mối quan hệ giữa câu hỏi và câu trả lời. Việc đánh giá những giải thích chưa được điều chỉnh này sẽ đánh giá thấp lợi ích của các giải thích tối ưu, nhưng do đó cung cấp một ước lượng đại diện về lợi ích dự kiến từ việc thêm giải thích vào một nhiệm vụ mới, mà không có điều chỉnh. Sau đó chúng tôi khám phá những lợi ích tăng thêm của các giải thích được điều chỉnh cho hiệu suất trên một tập xác thực nhỏ, xem bên dưới.

Việc thêm giải thích vào một prompt làm thay đổi các đặc trưng prompt khác, chẳng hạn như tổng chiều dài và nội dung. Để xác định xem một trong những đặc trưng cấp thấp hơn này có thúc đẩy hiệu ứng của các giải thích hay không, chúng tôi tạo ra nhiều loại giải thích điều khiển khác nhau khớp với các khía cạnh khác nhau của ngữ nghĩa, nội dung cấp từ hoặc câu:

Giải thích bị xáo trộn: Để đảm bảo rằng lợi ích không phải do các đặc trưng cấp từ, chúng tôi so sánh với các giải thích có các từ bị xáo trộn.

Giải thích không thực sự: Để kiểm tra rằng đó là nội dung giải thích quan trọng, chúng tôi so sánh với một tuyên bố hợp lệ, liên quan, nhưng không giải thích.

Giải thích mục khác: Cuối cùng, chúng tôi đánh giá xem lợi ích có phải do mối quan hệ trực tiếp giữa giải thích và điều được giải thích hay không, thay vì một số đặc trưng khác của ngôn ngữ. Để làm điều này, chúng tôi lấy các ví dụ trong một prompt few-shot và hoán vị các giải thích, sao cho các giải thích không khớp với câu hỏi hoặc câu trả lời, nhưng tổng thể các câu chứa trong prompt là giống hệt nhau.

Một tác giả độc lập, mù điều kiện đã đánh giá chất lượng của một tập con các giải thích, được đánh giá cao hơn đáng kể so với trung tính hoặc giải thích không thực sự (Phụ lục B.3). Chúng tôi cũng chú thích hướng dẫn và không hướng dẫn điều khiển cho mỗi nhiệm vụ (xem Phụ lục B), để mở rộng tập các prompt mà chúng tôi đánh giá các giải thích, do đó làm cho kết quả của chúng tôi có khả năng tổng quát hóa cao hơn cho các thiết lập mới.

2.4 Prompt và đánh giá
Chúng tôi xây dựng một số prompt 0-shot và 5-shot cho mỗi nhiệm vụ (xem Hình 1) trong một tập con các kết hợp có thể của hướng dẫn nhiệm vụ (none, instruction, non-instruction) và giải thích về ví dụ (none, other item explanation, true non-explanation, scrambled explanation). Tất nhiên, không thể có giải thích trong prompt 0-shot, vì thiếu ví dụ, và chúng tôi bỏ qua các kết hợp của giải thích điều khiển với (không) hướng dẫn để giảm số lượng truy vấn. Chúng tôi tách các ví dụ prompt bằng một dòng trống.

Khi chúng tôi áp dụng giải thích cho một ví dụ, chúng tôi đặt chúng trên một dòng sau câu trả lời, có tiền tố "Explanation:" (Hình 1); điều này trái ngược với các công trình trước đây đã khám phá việc giải thích suy luận trước câu trả lời (ví dụ Nye et al., 2021; Wei et al., 2022). Một lợi ích của việc giải thích sau câu trả lời là việc đánh giá giống hệt nhau cho dù có cung cấp giải thích hay không; mô hình sẽ cố gắng tạo ra một câu trả lời trước giải thích. Ngược lại, các giải thích trước câu trả lời yêu cầu thay đổi đánh giá, và yêu cầu nhiều tính toán hơn tại thời điểm đánh giá cho các giải thích. Những cách tiếp cận này cũng mang lại những hàm ý khoa học khác nhau, xem Thảo luận.

Chúng tôi đánh giá hiệu suất mô hình trong mỗi điều kiện prompt trên tất cả các mục tập dữ liệu nhiệm vụ (ngoại trừ những mục có trong prompt). Chúng tôi giới hạn ở các nhiệm vụ trắc nghiệm, và đánh giá khả năng của mô hình đối với mỗi tùy chọn câu trả lời sau khi điều kiện hóa trên prompt và câu hỏi (Hình 1). Chúng tôi không chuẩn hóa khả năng theo chiều dài câu trả lời, nhưng chiều dài câu trả lời thường tương tự trong một câu hỏi, và trong một số thí nghiệm sơ bộ, việc chuẩn hóa như vậy không cải thiện hiệu suất. Chúng tôi chọn câu trả lời có khả năng cao nhất từ tập một cách tham lam, và tính điểm độ chính xác của mô hình theo điểm câu trả lời được xác định bởi nhiệm vụ (có thể cho phép nhiều câu trả lời đúng hoặc điểm một phần).

2.5 Điều chỉnh hoặc chọn lọc giải thích
Các giải thích chưa được điều chỉnh cho mô hình và nhiệm vụ chỉ cung cấp một cận dưới yếu về lợi ích tiềm năng của các giải thích. Ví dụ, các giải thích có thể được cải thiện bởi một chuyên gia có kiến thức lớn hơn về các nhiệm vụ hoặc mô hình. Thật vậy, con người có thể yêu cầu làm rõ một giải thích gây nhầm lẫn, nhưng các LM không có cơ hội hoặc khả năng đó. Để cung cấp một số hiểu biết về lợi ích tiềm năng của các giải thích tối ưu hơn, chúng tôi tiến hành hai thí nghiệm điều chỉnh.

Chọn lọc ví dụ và giải thích: Chúng tôi chọn lọc các ví dụ để xây dựng một prompt 5-shot, từ 15 ví dụ mà chúng tôi đã chú thích cho mỗi nhiệm vụ. Chúng tôi chọn các ví dụ một cách tham lam đã cho hiệu suất tốt nhất khi dự đoán câu trả lời đúng trên các ví dụ còn lại trong số 15 mục prompt. Tức là, chúng tôi tạo ra một prompt 1-shot đã cho hiệu suất tốt nhất trên 14 câu hỏi còn lại; sau đó tạo ra 14 prompt hai shot bằng cách thêm mỗi ví dụ còn lại vào prompt 1-shot này, v.v. Cách tiếp cận này hơi thiên vị so với một tập xác thực thực sự (việc thêm một ví dụ khó vào prompt tránh phải trả lời nó), nhưng sự thiên vị này sẽ chỉ khiến chúng tôi đánh giá thấp lợi ích tiềm năng. Như một điều khiển, chúng tôi so sánh với việc chọn lọc các ví dụ mà không bao gồm giải thích trong các ví dụ—tức là chúng tôi thực hiện cùng số lượng lựa chọn trên cùng tập ví dụ, đơn giản là bỏ qua các giải thích—hoặc chọn lọc các ví dụ chứa giải thích điều khiển. Thí nghiệm này đánh giá lợi ích của giải thích khi điều chỉnh prompt bằng cách chỉ chọn lọc ví dụ.

Giải thích được điều chỉnh thủ công: Đối với năm nhiệm vụ mà hiệu suất luôn thấp trong tất cả các điều kiện, chúng tôi bắt đầu với prompt giải thích chưa được điều chỉnh có hiệu suất cao nhất (không có hướng dẫn), và chỉnh sửa văn bản của các giải thích để cố gắng cải thiện hiệu suất trên tập xác thực (10 ví dụ từ các prompt khác). Quá trình này thường bao gồm việc chỉnh sửa tương đối nhỏ, ví dụ, các giải thích "giống ngôn ngữ" hoặc sư phạm hơn (xem Phụ lục B.6). Điều chỉnh thủ công cho phép chúng tôi ước lượng cận dưới chặt chẽ hơn về tác dụng có thể của các giải thích tối ưu hơn.

Lưu ý rằng trong mỗi trường hợp trên, việc điều chỉnh được thực hiện trên một tập xác thực nhỏ của các ví dụ khác mà chúng tôi đã chú thích, nhưng các prompt được điều chỉnh sau đó được thử nghiệm trên tập lớn hơn của các ví dụ nhiệm vụ còn lại.

2.6 Phân tích
Việc đánh giá các mô hình trong các điều kiện khác nhau tạo ra một tập dữ liệu kết quả phức tạp với các phụ thuộc phân cấp, lồng nhau. Các câu hỏi của một nhiệm vụ chia sẻ một số cấu trúc, nhưng các câu hỏi cụ thể có thể khó hơn; một prompt với các ví dụ và giải thích có thể được so sánh trực tiếp nhất với cùng một prompt không có giải thích; v.v. Để ước lượng chính xác hơn tác dụng của các thành phần khác nhau của prompt lên hiệu suất, chúng tôi chuyển sang các phương pháp mô hình hóa phân cấp/đa cấp (ví dụ Gelman and Hill, 2006), thường được sử dụng để xử lý các mẫu phụ thuộc lồng nhau tương tự trong các khoa học hành vi (ví dụ Baayen et al., 2008; Yarkoni, 2022).

Cụ thể, chúng tôi khớp các hồi quy logistic phân cấp tính đến cấu trúc lồng nhau và không đồng nhất nhiều lần của kết quả chúng tôi. Ví dụ, những mô hình này tính đến các phụ thuộc được giới thiệu bởi độ khó tổng thể của nhiệm vụ và độ khó đặc biệt của mỗi câu hỏi. Các mô hình cũng tính đến thực tế rằng các điều kiện prompt khác nhau chia sẻ nội dung—ví dụ, một tập giải thích được áp dụng cho một tập cụ thể các ví dụ few-shot. Cuối cùng, các mô hình cho phép khả năng rằng các ví dụ few-shot, giải thích, và hướng dẫn có tác dụng khác nhau lên hiệu suất trong các nhiệm vụ khác nhau. Các mô hình tính đến từng yếu tố bằng cách ước lượng các tham số cho mỗi hiệu ứng đặc biệt ở mỗi cấp của cấu trúc phân cấp—ví dụ, một tham số cho độ khó của mỗi nhiệm vụ, một tham số cho độ khó của mỗi câu hỏi lồng trong mỗi nhiệm vụ, và một tham số cho tác dụng của mỗi tập giải thích riêng biệt. Để biết thêm bối cảnh, xem Gelman and Hill (2006); để biết đặc tả mô hình đầy đủ, xem Phụ lục D.1.

Giải thích chưa được điều chỉnh và điều khiển | Giải thích được điều chỉnh hoặc chọn lọc | Hướng dẫn

●
●●
●

0.0 0.4 0.8 1.2

Ví dụ few-shot
Giải thích chưa được điều chỉnh
Giải thích mục khác
Giải thích không thực sự
Giải thích bị xáo trộn
Giải thích được điều chỉnh thủ công*
Giải thích được chọn lọc
Chọn lọc không có giải thích
Chọn lọc giải thích khác
Chọn lọc giải thích không thực sự
Hướng dẫn
Không hướng dẫn

Nội dung prompt được thêm vào | Cải thiện hiệu suất (kích thước hiệu ứng)

Hình 2: Lợi ích của các thành phần khác nhau của prompt đối với LM lớn nhất (280B), được ước lượng từ hồi quy logistic phân cấp. Mỗi điểm ước lượng đóng góp duy nhất được thêm vào của thành phần đó của prompt; ví dụ ước lượng tác dụng của giải thích chưa được điều chỉnh cung cấp lợi ích của việc thêm giải thích vào prompt few-shot, được tổng hợp qua các điều kiện hướng dẫn khác nhau. Các ví dụ few-shot cải thiện hiệu suất nhiệm vụ, và việc thêm giải thích (các điểm đậm) cải thiện hiệu suất hơn nữa. Ngay cả các giải thích chưa được điều chỉnh cũng giúp ích, và các giải thích được điều chỉnh hoặc chọn lọc có tác dụng đáng kể. Các điều kiện điều khiển trung tính hoặc có hại. Hướng dẫn cung cấp lợi ích nhỏ. (Thanh lỗi là khoảng tin cậy 95% của mô hình. Kích thước hiệu ứng được ước lượng là tỷ lệ log odds / 1.81, xem Chinn, 2000. *Giải thích chỉ được điều chỉnh thủ công trên năm nhiệm vụ thách thức, vì vậy CI lớn hơn.)

3 Kết quả
Chúng tôi trước tiên trình bày các ước lượng từ hồi quy logistic phân cấp dự đoán hiệu suất của mô hình lớn nhất (280 tỷ tham số). Trong Hình 2, chúng tôi hiển thị những ước lượng này về cải thiện kích thước hiệu ứng trong hiệu suất do việc thêm các loại nội dung khác nhau vào prompt. Mỗi điểm ước lượng tác dụng của thành phần prompt đó trong khi kiểm soát và tổng hợp qua các tác dụng của các thành phần khác của prompt. Việc thêm các ví dụ few-shot vào prompt cải thiện hiệu suất đáng kể so với prompt zero-shot. Giải thích chưa được điều chỉnh về câu trả lời cải thiện thêm hiệu suất, với khoảng 1/3 kích thước hiệu ứng của các ví dụ few-shot; giải thích được điều chỉnh có tác dụng lớn hơn. Hướng dẫn cải thiện hiệu suất một chút, nhưng có tác dụng nhỏ hơn so với giải thích.

Trong Hình 3, chúng tôi trình bày phân bố lợi ích từ các loại giải thích khác nhau—cải thiện trong log-score so với các điều kiện khớp không có giải thích. Giải thích chưa được điều chỉnh có tác dụng biến đổi, nhưng có lợi trung bình. Việc chọn lọc các ví dụ và giải thích cùng nhau cải thiện nhất quán hơn so với việc chỉ chọn lọc các ví dụ. Điều chỉnh thủ công giải thích mang lại lợi ích đáng kể hơn, ít nhất là trên năm nhiệm vụ mà chúng tôi đánh giá. Chúng tôi cũng thấy rằng tác dụng của giải thích chưa được điều chỉnh không thay đổi đáng kể trong prompt có hoặc không có hướng dẫn nhiệm vụ hoặc không hướng dẫn (Phụ lục D.2). Kết quả nhiệm vụ cá nhân có thể được tìm thấy trong Phụ lục D.7.

●●●
●●●
●●●

0 2 4 6

-0.25 0.00 0.25 0.50 0.75
Cải thiện trong log score từ việc thêm giải thích | Mật độ

●●●
●●●
●●●

Giải thích chưa được điều chỉnh
vs. không có giải thích
Chọn lọc có giải thích
vs. chọn lọc không có
Giải thích được điều chỉnh thủ công
vs. không có giải thích

Hình 3: Giải thích có thể có lợi ích đáng kể đối với mô hình lớn nhất—đặc biệt khi được điều chỉnh—nhưng tác dụng của chúng biến đổi. Biểu đồ này so sánh cải thiện mà giải thích gây ra so với prompt khớp không có giải thích trong ba điều kiện khác nhau. Màu tím chúng tôi hiển thị tác dụng của giải thích chưa được điều chỉnh, so với prompt few-shot không có giải thích. Màu xanh đậm chúng tôi hiển thị lợi ích của việc chọn lọc các ví dụ với giải thích so với việc chỉ chọn lọc các ví dụ; và màu xanh nhạt chúng tôi hiển thị lợi ích của giải thích được điều chỉnh thủ công so với prompt không có giải thích. Lưu ý chúng tôi chỉ điều chỉnh thủ công giải thích trên 5 nhiệm vụ, vì vậy có ít quan sát, có thể giải thích tính phân cực. (Điểm và đường ở trên là trung bình và khoảng tin cậy 95% bootstrap. Đường cong là mật độ làm mịn dựa trên các quan sát cá nhân, được vẽ dưới dạng điểm bên dưới.)

Chỉ các mô hình lớn mới có lợi từ giải thích chưa được điều chỉnh: Hình 4 hiển thị tóm tắt thô về tác dụng trung bình của giải thích chưa được điều chỉnh qua các quy mô mô hình. Giải thích chưa được điều chỉnh mang lại sự tăng nhẹ về độ chính xác trung bình từ mô hình lớn nhất so với prompt few-shot không có giải thích. Các mô hình nhỏ hơn không có lợi. Hồi quy phân cấp xác nhận rằng các mô hình lớn hơn có lợi nhiều hơn đáng kể từ giải thích (Phụ lục D.1.2).

●●●
●●●

40 50 60

1 7 280
Tham số mô hình (tỷ) | Độ chính xác trung bình (%)

●
●

5-shot + giải thích chưa được điều chỉnh
5-shot
Chọn lọc 5 shot + giải thích
Chọn lọc 5 shot
Chọn lọc 5 shot + giải thích khác
Chọn lọc 5 shot + giải thích không thực sự

Hình 4: Tác dụng của giải thích lên độ chính xác trung bình, qua các kích thước mô hình. Giải thích chưa được điều chỉnh dẫn đến sự tăng nhẹ về độ chính xác từ mô hình lớn nhất so với prompt few-shot không có giải thích. Việc chọn lọc các ví dụ few-shot với giải thích sử dụng tập xác thực nhỏ cải thiện đáng kể so với việc chỉ chọn lọc các ví dụ few-shot không có giải thích, hoặc với giải thích điều khiển. Lưu ý rằng những kết quả này tổng hợp qua tất cả 40 nhiệm vụ, có độ khó và số lượng câu trả lời có thể khác nhau. Xem Phụ lục D.7 cho biểu đồ từng nhiệm vụ. (Thanh lỗi là khoảng tin cậy 95% bootstrap. Điểm được dịch chuyển theo chiều ngang để tránh chồng chéo.)

Giải thích chưa được điều chỉnh vượt trội hơn các điều kiện điều khiển được khớp: Giải thích điều khiển và không hướng dẫn trung tính hoặc có hại đối với mô hình lớn nhất. Tức là, có vẻ như có lợi ích duy nhất đối với giải thích thực sự—ngay cả khi chưa được điều chỉnh—so với các điều kiện điều khiển. Chúng tôi mô tả tác dụng trung bình này của giải thích và điều khiển trong Phụ lục D.4. Giải thích thực sự vượt trội hơn ngay cả prompt tương tự với giải thích được áp dụng cho các ví dụ khác, cho thấy rằng lợi ích phụ thuộc vào mối quan hệ giữa ví dụ và giải thích thay vì các đặc trưng cấp thấp hơn. Hồi quy phân cấp xác nhận rằng giải thích chưa được điều chỉnh vượt trội đáng kể so với tất cả các điều kiện điều khiển đối với mô hình lớn nhất (Phụ lục D.1.3).

Giải thích có lợi duy nhất cho các loại nhiệm vụ cụ thể không? Như đã nói ở trên, các nhiệm vụ chúng tôi sử dụng được chú thích với từ khóa nội dung. Chúng tôi tạo ra 8 cụm từ khóa từ lẽ thường đến toán học, và khám phá tác dụng của giải thích trong mỗi cụm (Phụ lục D.5). Lợi ích của giải thích có vẻ khá nhất quán qua các cụm. Tuy nhiên, mỗi cụm tương đối nhỏ, vì vậy kết quả không nên được diễn giải quá mạnh.

4 Công trình liên quan
Các quan sát gần đây rằng các LM có thể thực hiện nhiệm vụ từ một vài ví dụ (Brown et al., 2020; Radford et al., 2019) đã kích thích thảo luận về điều gì làm nền tảng cho việc học đó—ví dụ, Xie et al. (2021) đề xuất rằng học trong ngữ cảnh là một hình thức suy luận Bayesian ngầm. Các nhà nghiên cứu khác đã đặt câu hỏi về việc liệu điều xảy ra có thực sự là "học" hay chỉ đơn giản là phục hồi các nhiệm vụ được trải nghiệm trước đó (Reynolds and McDonell, 2021). Ví dụ, (Min et al., 2022) quan sát rằng prompt với nhãn ngẫu nhiên chỉ làm suy giảm hiệu suất nhẹ trong các nhiệm vụ NLP thông thường như entailment. Trong thảo luận, chúng tôi mô tả đóng góp của kết quả chúng tôi đối với việc hiểu học trong ngữ cảnh của LM.

Hướng dẫn nhiệm vụ: Một loạt các công trình trước đây đã khám phá hướng dẫn nhiệm vụ như prompt zero-shot (Reynolds and McDonell, 2021; Liu et al., 2021), hoặc như một phần của prompt few-shot (ví dụ Mishra et al., 2021b). Mô hình có thể nhạy cảm với việc đóng khung nhiệm vụ trong những prompt này, và có thể có lợi từ việc phân tách rõ ràng các vấn đề thành nhiều bước (Mishra et al., 2021a). Le Scao and Rush (2021) ước lượng rằng prompt nhiệm vụ có thể đáng giá nhiều ví dụ, ít nhất khi đào tạo một bộ phân loại.

Giải thích và phân tách suy luận trong ngữ cảnh: Mishra et al. (2021b) khám phá prompt chứa mô tả nhiệm vụ và ví dụ với giải thích (cũng như các đặc trưng khác như hướng dẫn). Tuy nhiên, họ không ước lượng tác dụng độc lập của giải thích, cũng không so sánh với các điều kiện điều khiển. Marasovi´c et al. (2021) và Wiegreffe et al. (2021) sử dụng prompt few-shot để khuyến khích các LM giải thích câu trả lời của chúng, nhưng để tạo ra giải thích cho việc diễn giải của người dùng, thay vì để cải thiện hiệu suất nhiệm vụ. Shwartz et al. (2020) cho thấy rằng "tự nói"—lấy mẫu câu trả lời cho các câu hỏi hỗ trợ từ mô hình và thêm chúng vào ngữ cảnh—cải thiện việc trả lời câu hỏi.

Công trình gần đây khác đã cho thấy rằng mô hình có thể có lợi từ các ví dụ phân tách quá trình suy luận dẫn đến một câu trả lời (Wei et al., 2022), đặc biệt khi chúng được tăng cường với bảng nháp bên ngoài để lưu trữ các tính toán trung gian (Nye et al., 2021; Recchia, 2021). Những phân tách này có thể được coi là một loại giải thích cụ thể. Tuy nhiên, trái ngược với các công trình liên quan chặt chẽ như Wei et al. (2022), chúng tôi cung cấp giải thích sau câu trả lời trong prompt, thay vì cung cấp chuỗi suy luận trước câu trả lời. Chúng tôi vẫn quan sát thấy lợi ích từ những giải thích sau câu trả lời này (ngay cả trên một số nhiệm vụ như số học nơi việc tính toán các bước trung gian hữu ích). Đây là một phân biệt quan trọng có liên quan đến những lợi ích riêng biệt mà giải thích có thể cung cấp cho mô hình; xem thảo luận. Ngoài ra, chúng tôi đánh giá trên một tập nhiệm vụ thách thức rộng hơn so với nhiều công trình trước đây, bao gồm một tập điều khiển được khớp rộng hơn, và cung cấp phân tích thống kê sâu hơn.

Đào tạo các mô hình xử lý ngôn ngữ với hướng dẫn hoặc giải thích: Các công trình trước đây khác nhau đã khám phá đào tạo hoặc điều chỉnh các mô hình xử lý ngôn ngữ với hướng dẫn nhiệm vụ (ví dụ Raffel et al., 2020; Wei et al., 2021; Ouyang et al., 2022; Sanh et al., 2021), và với giải thích (Camburu et al., 2018; Rajani et al., 2019; Narang et al., 2020; Zhou et al., 2020; Hase and Bansal, 2021). Nhiều công trình tập trung vào điều chỉnh với giải thích đã sử dụng các phương pháp như làm nổi bật span hoặc từ thay vì giải thích ngôn ngữ tự nhiên, xem Lertvittayakumjorn and Toni (2021) để xem lại. Mặc dù công trình trước đây rộng lớn hơn này có liên quan đến ý tưởng chung rằng giải thích có thể hữu ích trong xử lý ngôn ngữ tự nhiên, nó không trực tiếp liên quan đến câu hỏi về cách giải thích trong ngữ cảnh ảnh hưởng đến các mô hình không được đào tạo với trọng tâm vào hướng dẫn nhiệm vụ hoặc giải thích. Tuy nhiên, các mô hình được đào tạo rõ ràng với giải thích hoặc hướng dẫn có thể có lợi nhiều hơn từ giải thích trong ngữ cảnh (cf. Wei et al., 2021).

Giải thích ngoài NLP: Đào tạo với giải thích (ngôn ngữ hoặc khác) cũng đã cho thấy lợi ích trong các lĩnh vực ngoài ngôn ngữ. Ví dụ, giải thích đã được sử dụng để đào tạo hoặc điều chỉnh các mô hình cho thị giác máy tính (Hendricks et al., 2016; Schramowski et al., 2020; Majumder et al., 2021; Mu et al., 2020), để giải quyết vấn đề toán học bằng quy nạp chương trình (Ling et al., 2017), hoặc suy luận quan hệ và học tăng cường (Andreas et al., 2018; Lampinen et al., 2021).

Quy mô và xuất hiện: Mặc dù loss của LM có thể thể hiện quy mô dự đoán được với kích thước mô hình (Kaplan et al., 2020), những thay đổi mượt mà này có thể dẫn đến sự khác biệt chất lượng trong các hành vi cụ thể (Ganguli et al., 2022)—"nhiều hơn là khác" (Anderson, 1972). Để đưa ra một vài ví dụ: Brown et al. (2020) quan sát thấy sự chuyển đổi mạnh mẽ trong hiệu suất trên các nhiệm vụ số học với quy mô tăng; và Wei et al. (2021) thấy rằng chỉ các LM lớn mới có thể tổng quát hóa từ điều chỉnh hướng dẫn để thực hiện các nhiệm vụ mới zero-shot. Phát hiện của chúng tôi rằng lợi ích của giải thích xuất hiện với quy mô mô hình phù hợp với những phát hiện trước đây này. Các mô hình lớn hơn trong tương lai (hoặc được cải thiện khác) có thể thể hiện lợi ích thậm chí còn lớn hơn.

5 Thảo luận
Giải thích có thể cải thiện việc học few-shot không? Có, nhưng lợi ích phụ thuộc vào quy mô mô hình và chất lượng giải thích. Đối với một mô hình lớn, ngay cả giải thích chưa được điều chỉnh cũng dẫn đến sự cải thiện hiệu suất khiêm tốn nhưng đáng kể—một lợi ích khoảng một phần ba lớn như việc ban đầu thêm các ví dụ few-shot, nhưng gấp đôi lớn như việc thêm hướng dẫn nhiệm vụ. Vì vậy, ngay cả khi không có điều chỉnh, giải thích có thể cải thiện hiệu suất few-shot của các LM lớn.

Hơn nữa, việc điều chỉnh giải thích cho nhiệm vụ (sử dụng một tập xác thực nhỏ) có thể tăng đáng kể lợi ích của chúng. Đầu tiên, việc chọn lọc các ví dụ với giải thích cho một prompt mang lại cải thiện hiệu suất lớn hơn so với việc chỉ chọn lọc các ví dụ. Thứ hai, việc điều chỉnh thủ công giải thích trên một tập ví dụ cố định có thể mang lại lợi ích lớn hơn, ngay cả trên các nhiệm vụ thách thức. Mặc dù hiệu suất vẫn còn xa mới hoàn hảo, những phát hiện này—đặc biệt trong bối cảnh của các công trình liên quan gần đây (Mishra et al., 2021b; Wei et al., 2022; Zelikman et al., 2022)—cho thấy rằng giải thích có thể cải thiện prompt few-shot.

Tại sao giải thích sau câu trả lời thú vị? Trái ngược với một số công trình liên quan (Nye et al., 2021; Wei et al., 2022), chúng tôi tập trung vào việc cung cấp giải thích sau câu trả lời, thay vì chuỗi suy luận trước câu trả lời. Mặc dù Wei et al. (2022) không quan sát thấy lợi ích của giải thích sau câu trả lời, kết quả của chúng tôi tích cực hơn, có thể do thực tế rằng nhiệm vụ của họ yêu cầu suy luận lặp lại nhiều hơn. Ngoài sự khác biệt này, các chế độ giải thích trước và sau câu trả lời cũng có những hàm ý khác nhau tại thời điểm kiểm tra, và những hàm ý khoa học khác nhau về suy luận trong ngữ cảnh của mô hình.

Khi kiểm tra, giải thích sau câu trả lời không ảnh hưởng đến pipeline đánh giá, bởi vì mô hình sẽ tạo ra câu trả lời trước khi tạo ra giải thích. Ngược lại, chuỗi suy luận trước câu trả lời yêu cầu hàm đánh giá phân tích câu trả lời từ đầu ra mô hình. Để có hiệu suất tối ưu, suy luận trước câu trả lời và giải thích sau câu trả lời có thể có lợi ích bổ sung, có thể kết hợp. Thật vậy, Zelikman et al. (2022) đã sử dụng "rationalization" gợi ý sau câu trả lời như một phần của quy trình đào tạo bootstrap của họ. Thường hữu ích khi chia vấn đề thành một loạt các bước, nhưng một số giải thích mang tính tổng thể và chỉ có ý nghĩa khi câu trả lời được biết.

Từ góc độ khoa học, hai cách tiếp cận này đại diện cho các cơ chế riêng biệt mà một mô hình có thể có lợi từ giải thích. Chuỗi suy luận trước câu trả lời có thể cho phép mô hình đưa ra quá trình suy luận và xử lý vấn đề từng bước trước khi trả lời. Ngược lại, giải thích sau câu trả lời chỉ có thể định hình các quá trình suy luận một cách trừu tượng, bằng cách thay đổi suy luận nhiệm vụ. Bởi vì mô hình có số bước xử lý bằng nhau giữa câu hỏi và câu trả lời qua tất cả các điều kiện prompt của chúng tôi, tác dụng của giải thích là do thay đổi trong cách mô hình xử lý câu hỏi và câu trả lời có thể. Lợi ích của giải thích do đó xuất phát từ các suy luận bậc cao khá phức tạp trong quá trình học trong ngữ cảnh, như chúng tôi thảo luận tiếp theo.

Kết quả của chúng tôi có hàm ý gì về khả năng học trong ngữ cảnh của LM? Quá trình mà các LM thích ứng với một prompt đang được tranh luận. Ví dụ, Min et al. (2022) đưa ra quan sát thú vị rằng GPT-3 có thể thực hiện gần như tốt trên các nhiệm vụ thông thường như entailment ngay cả với câu trả lời ngẫu nhiên trong prompt, và do đó đặt câu hỏi về việc liệu các mô hình có thực sự "học" trong ngữ cảnh hay không. Các so sánh giữa giải thích của chúng tôi và các điều khiển được khớp cho phép chúng tôi kiểm tra xem các đặc trưng bề ngoài của giải thích có thúc đẩy lợi ích của chúng hay không. Đặc biệt, prompt giải thích mục khác chứa cùng các câu như prompt giải thích thực sự; các giải thích chỉ đơn giản được ghép với các ví dụ khác nhau. Mô hình lớn nhất cho thấy lợi thế đáng kể đối với giải thích thực sự ngay cả so với điều khiển được khớp chặt chẽ này—thực tế, điều khiển có vẻ không tốt hơn so với không có giải thích. Điều này cho thấy rằng mô hình lớn nhất đang sử dụng mối quan hệ giữa các ví dụ và giải thích tương ứng của chúng khi xử lý câu trả lời cho câu hỏi mục tiêu, thay vì chỉ đơn giản dựa vào phân bố của các từ hoặc câu trong prompt. Vì vậy, các mô hình lớn nhất dường như thể hiện một số suy luận bậc cao khá phức tạp trong ngữ cảnh.

Mặc dù những quan sát này không loại trừ khả năng rằng mô hình chỉ đơn giản sử dụng các giải thích để nhớ lại một nhiệm vụ đã thấy trong đào tạo, thực tế rằng các nhiệm vụ BIG-bench collaboration (2021) chúng tôi sử dụng được lấy mẫu một cách đối nghịch để trở nên độc đáo và thách thức làm cho điều này ít có khả năng hơn. Một sự hòa giải có thể với những phát hiện trước đây là các mô hình "lười biếng" và dựa vào các đặc trưng cấp thấp khi chúng có thể, và chỉ sử dụng các mối quan hệ cấp cao hơn trong các thiết lập thách thức yêu cầu chúng (cf. Papadimitriou et al., 2022). Tuy nhiên, cần có điều tra thêm để giải quyết hoàn toàn những vấn đề này.

Giải thích liên quan đến hướng dẫn như thế nào? Mặc dù công trình trước đây đã thấy rằng hướng dẫn nhiệm vụ có thể hiệu quả hơn so với ví dụ (ví dụ Reynolds and McDonell, 2021), chúng tôi thấy rằng tác dụng của hướng dẫn nhiệm vụ nhỏ hơn so với các ví dụ few-shot hoặc giải thích. Điều này có thể do công trình trước đây điều chỉnh hướng dẫn rất nhiều cho hiệu suất—do đó làm cho các prompt có thể được tranh luận không phải "zero-shot" (cf. Perez et al., 2021)—hoặc có thể liên quan đến các nhiệm vụ mà chúng tôi đánh giá, được lấy mẫu một cách đối nghịch. Các nhiệm vụ thách thức có thể khó mô tả hơn, thay vì giải thích bằng ví dụ. Bất kể, giải thích cung cấp lợi ích tương tự qua các điều kiện hướng dẫn khác nhau; vì vậy hướng dẫn và giải thích có thể bổ sung.

Công trình của chúng tôi liên quan đến xử lý ngôn ngữ của con người như thế nào? Bởi vì chúng tôi được truyền cảm hứng bởi công trình nhận thức về việc con người sử dụng giải thích (ví dụ Lombrozo and Carey, 2006; Ahn et al., 1992), và với bằng chứng tích lũy rằng các LM dự đoán rất nhiều về xử lý ngôn ngữ thần kinh của con người (Schrimpf et al., 2021; Goldstein et al., 2022), tự nhiên là hỏi liệu có những hàm ý nhận thức nào của công trình chúng tôi hay không. Tuy nhiên, thực tế rằng cả LM và con người đều có lợi từ giải thích không có nghĩa là họ nhất thiết có lợi thông qua cùng một cơ chế. Thật vậy, lợi ích của giải thích mà chúng tôi quan sát nhỏ hơn so với những gì người ta có thể mong đợi đối với con người. Tuy nhiên, lưu ý rằng nhiều nhiệm vụ của chúng tôi, chẳng hạn như đánh giá các lập luận quy nạp toán học, sẽ thách thức đối với nhiều con người, ngay cả với hướng dẫn, ví dụ, và giải thích.

Sự khác biệt giữa phản ứng của con người và mô hình có thể xuất phát một phần từ trải nghiệm nghèo nàn của các LM, không trải nghiệm bối cảnh hoặc tình huống rộng lớn hơn mà ngôn ngữ đề cập đến (McClelland et al., 2020). Ngược lại, giải thích của con người về cơ bản mang tính thực dụng và giao tiếp (Van Fraassen, 1988; Cassens et al., 2021)—chúng được dự định để truyền đạt một ý nghĩa trong một cuộc đối thoại và bối cảnh cụ thể. Các quá trình suy luận mà con người tham gia để hiểu giải thích có thể được định hình bởi kinh nghiệm tương tác này. Vì vậy, các mô hình được đào tạo trong các thiết lập tương tác, phong phú hơn có thể có lợi nhiều hơn từ giải thích (Santoro et al., 2021). Tương hỗ, giải thích có thể cung cấp một thiết lập thú vị cho các điều tra trong tương lai về những điểm tương đồng và khác biệt giữa xử lý ngôn ngữ của con người và mô hình.

Lợi ích của phương pháp nghiên cứu của chúng tôi là gì? Khi các nhà nghiên cứu kiểm tra các hành vi ngày càng phức tạp được thể hiện bởi AI (Santoro et al., 2021), chúng tôi đề xuất rằng sẽ có lợi ích ngày càng tăng từ việc áp dụng các công cụ thực nghiệm và phân tích của các khoa học hành vi. Chúng tôi đặc biệt nhấn mạnh giá trị của hồi quy phân cấp (ví dụ Gelman and Hill, 2006) tính toán thống kê cho các phụ thuộc dữ liệu và các loại biến đổi quan sát khác nhau. Việc không mô hình hóa những yếu tố này dẫn đến các vấn đề như "ngụy biện kích thích-như-hiệu-ứng-cố-định" (Clark, 1973)—nhầm lẫn một hiệu ứng kích thích hoặc tập dữ liệu với một nguyên tắc tổng quát hơn. Vì vậy, các mô hình phân cấp thích hợp rất cần thiết cho nghiên cứu có thể tổng quát hóa (Yarkoni, 2022). Chúng tôi khuyến khích các nhà nghiên cứu khác áp dụng những công cụ phân tích này.

6 Kết luận
Việc bao gồm giải thích với các ví dụ trong prompt few-shot có thể cải thiện suy luận nhiệm vụ trong ngữ cảnh cho các mô hình ngôn ngữ. Giải thích được điều chỉnh sử dụng tập xác thực đặc biệt hiệu quả, nhưng ngay cả giải thích chưa được điều chỉnh cũng có tác dụng tích cực khiêm tốn và vượt trội hơn các điều kiện điều khiển được khớp cẩn thận. Tuy nhiên, trong các thí nghiệm của chúng tôi, khả năng này chỉ xuất hiện ở các mô hình lớn nhất (mặc dù có thể các mô hình nhỏ hơn có thể có lợi từ nhiều giải thích hơn, hoặc trên các nhiệm vụ đơn giản hơn). Những kết quả này có hàm ý cả cho kỹ thuật prompt, và cho hiểu biết khoa học về khả năng học trong ngữ cảnh của các mô hình ngôn ngữ lớn.

Giới hạn
Lợi ích và nhược điểm của các tập dữ liệu chúng tôi sử dụng là gì? Việc sử dụng các nhiệm vụ từ BIG-bench collaboration (2021) cho phép chúng tôi thu thập một tập các nhiệm vụ đa dạng, thách thức trong định dạng tương tự. Những nhiệm vụ này bao gồm các kỹ năng có thể không được đại diện tốt trong các tập dữ liệu tiêu chuẩn. Tuy nhiên, những nhiệm vụ đối nghịch được lấy mẫu đặc biệt này do đó có thể không đại diện cho các nhiệm vụ mà các LM thường được áp dụng. Sự khác biệt này có thể khuếch đại hoặc ngăn chặn các hiệu ứng của chúng tôi. Giải thích có thể đặc biệt hữu ích trong các nhiệm vụ thách thức nơi mối quan hệ giữa câu hỏi và câu trả lời bất thường. Thay vào đó, giải thích có thể hiệu quả hơn trong các thiết lập tiêu chuẩn hơn nơi chúng có thể liên quan trực tiếp hơn đến các mẫu suy luận thông thường. Tính rộng của các nhiệm vụ chúng tôi khám phá, cùng với các phân tích thống kê mô hình hóa rõ ràng sự khác biệt nhiệm vụ, giảm thiểu một phần những lo ngại này. Tuy nhiên, công trình trong tương lai nên khám phá lợi ích của giải thích ngoài BIG-Bench.

Chú thích giải thích: Một tác giả duy nhất của bài báo đã chú thích tập dữ liệu này, bằng cách đọc các vấn đề và giải pháp BIG-Bench tương ứng và sau đó cố gắng viết một giải thích sẽ giúp con người hiểu câu trả lời. Mặc dù quá trình này cho phép chú thích chuyên gia trên các nhiệm vụ nơi chúng có thể khó có được (ví dụ các nhiệm vụ yêu cầu hiểu quy nạp toán học), nó có những giới hạn tương ứng. Bởi vì quá trình tốn thời gian này, tập dữ liệu nhỏ và không bao gồm tất cả các nhiệm vụ BIG-Bench. Tiêu chí chọn lọc nhiệm vụ được mô tả chi tiết hơn trong văn bản chính. Hơn nữa, bởi vì một tác giả duy nhất đã chú thích các giải thích, chúng có thể thiên vị, theo nghĩa là chúng có thể bao gồm các mẫu giải thích nhất định (hoặc thậm chí lỗi) có thể có lợi hoặc không. Công trình trong tương lai nên khám phá những khả năng này.

Mô hình và đào tạo: Mặc dù tập trung vào một tập các mô hình ngôn ngữ với chế độ đào tạo được khớp cho phép chúng tôi đánh giá tác dụng quy mô dưới các điều kiện được kiểm soát, nó cũng có nghĩa là kết quả của chúng tôi có thể tổng quát hóa khác đối với các mô hình ngôn ngữ được đào tạo dưới các điều kiện khác nhau (ví dụ Brown et al., 2020; Hoffmann et al., 2022), và các mô hình được đào tạo hoặc điều chỉnh với hướng dẫn (Ouyang et al., 2022) có thể có lợi nhiều hơn từ giải thích. Thật vậy, việc điều chỉnh một mô hình để sử dụng giải thích trong prompt có thể sẽ cải thiện kết quả, và sẽ là một hướng thú vị cho công trình trong tương lai. Vì vậy, mặc dù chỉ mô hình lớn nhất mà chúng tôi đánh giá cho thấy lợi ích của giải thích (ít nhất là chưa được điều chỉnh), giải thích có thể hữu ích cho các mô hình nhỏ hơn được đào tạo hoặc điều chỉnh tốt hơn. Bởi vì chi phí tính toán của việc sử dụng các mô hình lớn, điều này có thể sẽ tăng khả năng tiếp cận của các cách tiếp cận như những gì chúng tôi khám phá. Hơn nữa, việc thêm giải thích làm dài prompt, và các mô hình ngôn ngữ hiện tại thường có độ dài ngữ cảnh cố định, giới hạn số lượng ví dụ có thể được giải thích. Vì vậy, giải thích có thể hữu ích hơn trong các mô hình trong tương lai có thể xử lý tốt hơn các ngữ cảnh dài hơn (Press et al., 2021; Rae et al., 2019).

Mức độ cải thiện được cung cấp bởi giải thích: Chúng tôi nhấn mạnh rằng mặc dù việc bao gồm giải thích câu trả lời dẫn đến sự cải thiện hiệu suất có ý nghĩa thống kê, nó không mang lại hiệu suất hoàn hảo hoặc hành vi hoàn toàn mạnh mẽ—thật vậy, không có điều chỉnh, lợi ích là khiêm tốn. Giải thích không phải là thuốc chữa bách bệnh. Tuy nhiên, cũng như các mô hình lớn hơn có lợi nhiều hơn từ giải thích, chúng tôi hy vọng rằng các thế hệ mô hình cải thiện trong tương lai sẽ có lợi thậm chí nhiều hơn; và giải thích có thể được kết hợp với các kỹ thuật khác để mang lại cải thiện thậm chí lớn hơn.

Hàm ý đạo đức
Vì công trình của chúng tôi chủ yếu là một điều tra khoa học về các yếu tố ảnh hưởng đến hiệu suất mô hình ngôn ngữ, chúng tôi không mong đợi nó có những tác động đạo đức trực tiếp. Tuy nhiên, với những lo ngại đạo đức xung quanh các mô hình ngôn ngữ lớn (ví dụ Weidinger et al., 2021), việc sử dụng giải thích có thể có một loạt tác dụng hạ nguồn. Ở mức bi quan nhất, giải thích có thể cải thiện hiệu suất của các mô hình ngôn ngữ, và do đó làm trầm trọng thêm bất kỳ tác động tiêu cực nào của chúng. Tuy nhiên, tích cực hơn, giải thích có thể cho phép dạy một mô hình những đặc trưng nào nó nên hoặc không nên tổng quát hóa từ một ví dụ—cũng như giải thích giúp con người (Ahn et al., 1992) và các tác nhân học tăng cường (Lampinen et al., 2021) suy ra những đặc trưng nào có thể tổng quát hóa. Cách tiếp cận này có thể cung cấp một tuyến đường mới để giảm thiểu sự thiên vị. Nó cũng có thể giúp khả năng diễn giải của người dùng, mặc dù với lưu ý rằng giải thích có thể không hoàn toàn đáng tin cậy, ít nhất là không có tinh chỉnh thêm (Marasovi´c et al., 2021; Wiegreffe et al., 2021).

Lời cảm ơn
Chúng tôi cảm ơn Dani Yogatama, Adhi Kuncoro, Neil Rabinowitz, và Raia Hadsell vì các bình luận và đề xuất hữu ích, cũng như đội ngũ đã đào tạo các mô hình ngôn ngữ.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo tiếng Anh được giữ nguyên do tính chất học thuật]
