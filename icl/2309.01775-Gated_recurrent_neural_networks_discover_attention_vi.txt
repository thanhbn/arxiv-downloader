# 2309.01775.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2309.01775.pdf
# Kích thước tệp: 2777409 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mạng nơ-ron tái phát có cổng khám phá cơ chế attention
Nicolas Zucchet* 1Seijin Kobayashi* 1Yassir Akram* 1Johannes von Oswald1Maxime Larcher1
Angelika Steger†1Jo˜ao Sacramento†1
Tóm tắt
Những phát triển kiến trúc gần đây đã cho phép mạng nơ-ron tái phát (RNN) đạt được và thậm chí vượt qua hiệu suất của Transformers trong một số tác vụ mô hình hóa chuỗi. Những RNN hiện đại này có một mẫu thiết kế nổi bật: các lớp tái phát tuyến tính được kết nối bằng các đường dẫn feedforward với cổng nhân. Ở đây, chúng tôi chỉ ra cách RNN được trang bị hai yếu tố thiết kế này có thể triển khai chính xác self-attention (tuyến tính). Bằng cách kỹ thuật ngược một tập hợp RNN được huấn luyện, chúng tôi thấy rằng gradient descent trong thực tế khám phá ra cấu trúc của chúng tôi. Cụ thể, chúng tôi kiểm tra RNN được huấn luyện để giải quyết các tác vụ học trong ngữ cảnh đơn giản và thấy rằng gradient descent truyền vào RNN của chúng tôi cùng một thuật toán học trong ngữ cảnh dựa trên attention. Các phát hiện của chúng tôi làm nổi bật tầm quan trọng của các tương tác nhân trong mạng nơ-ron và gợi ý rằng một số RNN có thể đang triển khai attention một cách bất ngờ ở bên trong.

1. Giới thiệu
Mạng nơ-ron dựa trên attention, đáng chú ý nhất là Transformers (Vaswani et al., 2017), đã nhanh chóng trở thành kiến trúc deep learning tối ưu, thay thế các mô hình truyền thống như perceptron đa lớp, mạng nơ-ron tích chập và mạng nơ-ron tái phát (RNN). Điều này đặc biệt đúng trong lĩnh vực mô hình hóa chuỗi, nơi RNN từng thống trị như bộ nhớ dài hạn-ngắn hạn (LSTM; Hochreiter & Schmidhuber, 1997) và đơn vị tái phát có cổng liên quan (GRU; Cho et al., 2014) đã chủ yếu được thay thế bởi Transformers.

Tuy nhiên, RNN vẫn được nghiên cứu tích cực vì nhiều lý do khác nhau, chẳng hạn như giá trị của chúng như các mô hình trong khoa học thần kinh (Dayan & Abbott, 2001), hoặc đơn giản là vì sự quan tâm thực sự đến các tính chất phong phú của chúng như một hệ thống động học và máy tính phi truyền thống (Jaeger et al., 2023). Có lẽ quan trọng nhất đối với các ứng dụng, RNN có thể thực hiện suy luận cho các chuỗi dài tùy ý với chi phí bộ nhớ không đổi, không giống như các mô hình dựa trên các lớp softmax-attention thông thường (Bahdanau et al., 2015). Nghiên cứu đang tiến hành này đã dẫn đến một làn sóng phát triển gần đây. Một mặt, các kiến trúc RNN tuyến tính sâu mới (Gu et al., 2022; Orvieto et al., 2023b) đã được chỉ ra là vượt trội đáng kể so với Transformers trong các tác vụ chuỗi dài thách thức (ví dụ, Tay et al., 2020) và trong một số tác vụ mô hình hóa ngôn ngữ (Gu & Dao, 2023). Mặt khác, nhiều mô hình attention tuyến tính hóa hiệu quả đã được phát triển, mà forward pass của chúng có thể được thực hiện theo cách giống RNN với chi phí bộ nhớ suy luận không đổi (Tsai et al., 2019; Katharopoulos et al., 2020; Choromanski et al., 2021; Schlag et al., 2021; Fu et al., 2023; Sun et al., 2023; Yang et al., 2023).

Chúng tôi trình bày một quan điểm thống nhất về hai hướng nghiên cứu dường như không liên quan này bằng cách cung cấp một tập hợp tham số mà dưới đó RNN có cổng trở nên tương đương với bất kỳ self-attention tuyến tính nào, mà không cần số lượng nơ-ron vô hạn hoặc viện dẫn lập luận tính phổ quát. Quan trọng, cấu trúc của chúng tôi sử dụng các phép nhân theo từng phần tử, được thể hiện rõ ràng dưới các dạng khác nhau trong các mô hình RNN tuyến tính sâu gần đây. Chuyển sang LSTM và GRU, cũng bao gồm các tương tác cổng nhân này, chúng tôi thấy khá bất ngờ rằng kết quả của chúng tôi chỉ mở rộng đến LSTM. Hơn nữa, cấu trúc LSTM mà chúng tôi cung cấp đòi hỏi một cấu hình rất cụ thể, gợi ý rằng thiên hướng quy nạp hướng tới các cấu hình tương thích với attention có thể yếu hơn đối với kiến trúc này so với RNN tuyến tính có cổng sâu.

Sau đó chúng tôi chứng minh rằng RNN tuyến tính với các tương tác nhân, nhưng không phải LSTM và GRU, có thể triển khai hiệu quả cấu trúc của chúng tôi một khi được huấn luyện, do đó hoạt động như các lớp attention. Hơn nữa, chúng tôi thấy rằng các RNN tuyến tính như vậy được huấn luyện để giải quyết các tác vụ hồi quy tuyến tính sẽ học được một thuật toán học trong ngữ cảnh dựa trên attention. Tình cờ, đã được chỉ ra rằng cùng một thuật toán thường được sử dụng bởi các lớp self-attention tuyến tính được huấn luyện trên lớp vấn đề này (von Oswald et al., 2023; Mahankali et al., 2023; Ahn et al., 2023; Zhang et al., 2023). Kết quả của chúng tôi do đó thách thức quan điểm chuẩn về RNN và các mô hình dựa trên attention như hai lớp mô hình loại trừ lẫn nhau và gợi ý rằng, thông qua học tập, RNN với các tương tác nhân có thể kết thúc bằng việc mã hóa các thuật toán dựa trên attention được ngụy trang trong trọng số của chúng.

2. Nền tảng
2.1. Self-attention tuyến tính
Chúng tôi nghiên cứu các lớp self-attention tuyến tính được che dấu theo nhân quả xử lý các chuỗi đầu vào (xt)t với xt∈Rd như sau:
yt=∑t′≤t(WVxt′)(WKxt′)⊤(WQxt) (1)

Trong phương trình trước, WV∈Rd×d là ma trận giá trị, WK∈Rd×d là ma trận khóa và WQ∈Rd×d là ma trận truy vấn. Chúng tôi sử dụng ma trận vuông trong suốt bài báo để đơn giản, nhưng phát hiện của chúng tôi mở rộng đến ma trận chữ nhật. Như thường được làm, chúng tôi gọi vt:=WVxt, kt:=WKxt và qt:=WQxt là các giá trị, khóa và truy vấn. Vector đầu ra yt có cùng chiều với đầu vào, tức là d. Các lớp self-attention tuyến tính như vậy có thể được hiểu là phiên bản tuyến tính hóa của cơ chế softmax attention (Bahdanau et al., 2015) được sử dụng trong Transformers (Vaswani et al., 2017). Tuy nhiên, chúng hoạt động trong một chế độ rất khác so với các lớp softmax, có bộ nhớ không bị giới hạn. Các lớp attention thường kết hợp các đầu attention khác nhau; chúng tôi tập trung vào một đầu duy nhất ở đây để đơn giản.

Trong một lớp self-attention tuyến tính, thông tin về quá khứ được lưu trữ trong ma trận trọng số hiệu quả Wfft:=∑t′vt′k⊤t′ sẽ sau này được sử dụng để xử lý truy vấn hiện tại qt thông qua yt=Wfftqt. Tại mỗi bước thời gian, Wfft được cập nhật thông qua quy tắc Wfft=Wfft−1+vtk⊤t, gợi nhớ đến học tập Hebbian (Schmidhuber, 1992; Schlag et al., 2021) và dẫn đến thời gian suy luận nhanh hơn (Katharopoulos et al., 2020; Choromanski et al., 2021; Shen et al., 2021; Peng et al., 2021) so với softmax self-attention.

2.2. Mạng nơ-ron tái phát có cổng
Trong bài báo này, chúng tôi tập trung phân tích vào một lớp đơn giản hóa của mạng nơ-ron tái phát tuyến tính đường chéo có cổng. Chúng triển khai cổng đầu vào gin và đầu ra gout song tuyến tính nhân một phép biến đổi tuyến tính Win/outx xt của đầu vào với một cổng tuyến tính Win/outm xt: gin/out(xt) = (Win/outm xt)⊙(Win/outx xt). Ở đây, ⊙ là tích theo từng phần tử. Lớp mạng có cổng mà chúng tôi xem xét thỏa mãn

ht+1=λ⊙ht+gin(xt), yt=Dgout(ht). (2)

Trong phương trình trước, λ là một vector thực, xt là đầu vào của lớp tái phát, ht là trạng thái ẩn, và D là một phép đọc tuyến tính. Lớp đơn giản hóa này làm cho việc kết nối với attention dễ dàng hơn trong khi sử dụng các cơ chế tính toán tương tự như các kiến trúc RNN có cổng tiêu chuẩn.

Lớp này có liên hệ chặt chẽ với các kiến trúc RNN tuyến tính sâu gần đây và chia sẻ hầu hết các cơ chế tính toán với chúng. Mặc dù sự tái phát tuyến tính đường chéo có thể được xem như một thiên hướng quy nạp rất mạnh, nhiều mô hình RNN tuyến tính sâu mạnh mẽ gần đây áp dụng thiên hướng tương tự (Gupta et al., 2022; Smith et al., 2023; Gu & Dao, 2023), và đã được chỉ ra là tạo thuận lợi cho học tập dựa trên gradient (Orvieto et al., 2023b; Zucchet et al., 2023b). Những kiến trúc đó thường sử dụng trạng thái ẩn có giá trị phức trong tái phát; chúng tôi chỉ sử dụng phần thực của nó ở đây. Một số công trình đó sử dụng GLU (Dauphin et al., 2017) sau mỗi lớp tái phát, với GLU(x) =σ(Wmxt)⊙Wxxtvới σ là hàm sigmoid. Cơ chế cổng mà chúng tôi xem xét có thể được diễn giải như một GLU tuyến tính hóa. Chúng ta có thể khôi phục (2) bằng cách xếp chồng hai lớp: GLU trong lớp đầu tiên hoạt động như cổng đầu vào của chúng tôi, và cái trong lớp thứ hai như cổng đầu ra. Hoặc, các kiến trúc như Mamba (Gu & Dao, 2023) sử dụng các ma trận phụ thuộc đầu vào như phép chiếu lên trạng thái ẩn thay vì cổng đầu vào. Nhân các ma trận như vậy với chính đầu vào do đó dẫn đến một cổng nhân. Cơ chế cổng đầu ra của nó hơi khác vì một trong các nhánh lấy đầu vào của lớp tái phát như đầu vào, thay vì trạng thái ẩn. Chúng tôi bao gồm một so sánh chi tiết hơn trong Phụ lục B. Trong phần còn lại của bài báo, chúng tôi sẽ sử dụng lớp LRU (Orvieto et al., 2023b) làm đại diện cho các kiến trúc RNN tuyến tính sâu vì tính đơn giản của nó.

LSTM có thể hoạt động trong chế độ của Phương trình 2, nhưng điều này đòi hỏi sự thích ứng nhiều hơn. Đầu tiên, quá trình tái phát là phi tuyến và bao gồm nhiều bước hơn những gì được nắm bắt trong (2). Thứ hai, cổng xảy ra ở các phần khác nhau của tính toán và phụ thuộc vào các biến bổ sung. Chúng tôi so sánh chi tiết hơn kiến trúc này và kiến trúc của Phương trình 2 trong Phụ lục B, chỉ ra rằng LSTM có thể triển khai (2) khi xếp chồng hai lớp lên nhau. Chúng tôi cũng chỉ ra rằng GRU không thể làm như vậy.

3. Cấu trúc lý thuyết
Như được nhấn mạnh trong phần trước, lớp RNN có cổng của chúng tôi và self-attention tuyến tính có những cách khác nhau để lưu trữ thông tin quá khứ và sử dụng nó để sửa đổi quá trình xử lý feedforward của đầu vào hiện tại. Trạng thái trước đó ht hoạt động thông qua một số hạng bias λ⊙ht được thêm vào đầu vào hiện tại gin(xt) trong RNN có cổng, trong khi trạng thái tái phát self-attention tuyến tính Wfft sửa đổi các trọng số của đường dẫn feedforward. Chúng tôi hòa giải hai quan điểm không khớp về tính toán nơ-ron này trong phần sau bằng cách chỉ ra rằng RNN có cổng có thể triển khai self-attention tuyến tính.

Trong phần này, chúng tôi chứng minh cách một lớp tái phát có cổng theo sau bởi một phép đọc tuyến tính như trong Phương trình 2 có thể triển khai bất kỳ lớp self-attention tuyến tính nào thông qua một chứng minh xây dựng. Đặc biệt, cấu trúc của chúng tôi chỉ yêu cầu một số lượng hữu hạn nơ-ron để khớp chính xác hàm mong muốn, do đó cung cấp kết quả tương đương mạnh hơn nhiều so với các định lý tính phổ quát tổng quát hơn của mạng tái phát tuyến tính (Boyd & Chua, 1985; Grigoryeva & Ortega, 2018; Orvieto et al., 2023a), tồn tại trong giới hạn của vô hạn nơ-ron tái phát.

3.1. Ý tưởng chính
Cấu trúc của chúng tôi bao gồm ba thành phần chính: Đầu tiên, cổng đầu vào gin có trách nhiệm tạo ra các tích theo từng phần tử giữa các khóa và giá trị, cũng như các truy vấn. Sau đó, các đơn vị tái phát liên kết với key-values tích lũy các đầu vào của chúng với λ= 1, trong khi những đơn vị nhận truy vấn như đầu vào trả về giá trị hiện tại của truy vấn, do đó λ= 0. Cuối cùng, cổng đầu ra gout và lớp đọc cuối cùng D chịu trách nhiệm nhân ma trận key-value được làm phẳng với vector truy vấn. Chúng tôi minh họa cấu trúc của chúng tôi và cung cấp một tập hợp trọng số mà tương đương chức năng tồn tại trong Hình 1. Quan trọng, key-values trong một lớp self-attention tuyến tính là tổng của các đa thức bậc hai của mỗi đầu vào trước đó. Cơ chế cổng đầu vào và các đơn vị bộ nhớ hoàn hảo (λ= 1) cần thiết để sao chép hành vi này trong một lớp tái phát có cổng. Tương tự, cổng đầu ra cần thiết để nhân key-values với các truy vấn.

3.2. Về số lượng nơ-ron cần thiết
Cấu trúc của Hình 1 yêu cầu d2+d nơ-ron ẩn để lưu trữ tất cả các mục của ma trận key-value d×d và của vector truy vấn kích thước d. Mặc dù cấu trúc này có thể được coi là trực quan nhất, nó không tối ưu về số lượng nơ-ron được sử dụng. Biết số lượng nơ-ron tối thiểu chính xác là cơ bản để hiểu giải pháp nào mà mạng học được. Do đó, chúng tôi chi tiết cách chúng tôi có thể làm cho cấu trúc của chúng tôi nhỏ gọn hơn trong phần sau.

Chúng tôi tận dụng hai hiểu biết: Đầu tiên, bất kỳ kết hợp nào của ma trận khóa và truy vấn mà (W⊤KWQ) được cố định dẫn đến cùng một hàm trong lớp self-attention tuyến tính. Do đó chúng ta có thể giả định rằng ma trận khóa và giá trị bằng nhau, vì lấy ma trận khóa bằng WV và thay đổi ma trận truy vấn thành W−⊤VW⊤KWQ không thay đổi hành vi của lớp attention. Thứ hai, khi ma trận khóa và giá trị bằng nhau, ma trận key-value là đối xứng và do đó chỉ yêu cầu d(d+ 1)/2 phần tử để được biểu diễn. Điều này ngụ ý rằng, khi ma trận giá trị khả nghịch, số lượng nơ-ron ẩn tối thiểu mà RNN có cổng của chúng tôi cần để lưu trữ key-values thực tế là d(d+ 1)/2 +d. Trong Phần 4, chúng tôi chỉ ra rằng RNN đã học tìm ra giải pháp này.

Hoặc, cũng có thể giảm kích thước cấu trúc khi các ma trận trọng số của lớp attention giáo viên có rank thấp. Trong trường hợp này, chúng ta vẫn có sự tỷ lệ bậc hai của số lượng nơ-ron tái phát cần thiết, nhưng lần này trong rank của các ma trận khác nhau thay vì toàn bộ chiều. Sự dẫn xuất chi tiết có thể được tìm thấy trong Phụ lục A.4.

Nhìn chung, cổng đầu ra yêu cầu O(d2) mục đầu vào và đầu ra để RNN có cổng khớp với một lớp self-attention tuyến tính. Do đó RNN yêu cầu O(d4) tham số tổng cộng, với nhiều thừa, đáng kể hơn so với 3d2 tham số của lớp self-attention tuyến tính. Chúng tôi lưu ý rằng thay đổi cổng đầu ra thành cổng bên là có thể, xem Phụ lục A.2, giảm số lượng tham số cần thiết xuống O(d3).

Với sự thừa tham số cao, không có gì ngạc nhiên khi tồn tại nhiều cấu hình tương đương trong RNN có cổng mà chúng tôi nghiên cứu. Ví dụ, cổng tuyến tính bất biến dưới hoán vị các hàng giữa hai ma trận của nó và dưới phép nhân-chia các hàng này bằng một hằng số. Nhân trái WQ trong cổng đầu vào bằng bất kỳ ma trận khả nghịch P nào, và sau đó đọc các nơ-ron ẩn với λ= 0 thông qua repeat(P−1, d), cũng không thay đổi đầu ra của mạng. Tồn tại một số bất biến khác, làm cho việc truy xuất trọng số chính xác gần như không thể. Những xem xét này sẽ có ích thực tế khi chúng tôi kỹ thuật ngược hàm được mã hóa bởi các mạng tái phát đã được huấn luyện trong Phần 4.1.

3.3. Hàm ý cho các lớp RNN hiện có
Chúng tôi kết thúc phần này bằng cách bình luận về việc liệu những hiểu biết tương tự có đúng cho các kiến trúc RNN có cổng thực tế hơn hay không. Kiến trúc LRU gần với (2) nhưng chỉ chứa cổng đầu ra thông qua một lớp GLU. Xếp chồng hai lớp LRU lên nhau cho phép cổng đầu ra của lớp đầu tiên hoạt động như cổng đầu vào cho lớp thứ hai và do đó triển khai cơ chế mà chúng tôi đã nêu bật trong các phần trước để bắt chước attention. Trực quan, việc thêm một GLU đầu vào sẽ thiên vị LRU hướng tới self-attention tuyến tính vì một lớp giờ đây đủ để triển khai nó. Sau này chúng tôi sẽ xác nhận rằng điều này thực sự cải thiện khả năng của LRU để bắt chước self-attention tuyến tính, cũng như tăng hiệu suất trên một số tác vụ. Khối Mamba có thiên hướng quy nạp mạnh hơn hướng tới attention do sự hiện diện của cổng bên truy vấn bộ nhớ được lưu trữ trong trạng thái tái phát. Thú vị, đã được tìm thấy rằng việc loại bỏ sự phụ thuộc đầu vào của ma trận chiếu lên trạng thái ẩn có hại cho hiệu suất (Gu & Dao, 2023). Điều này làm giảm thiên hướng quy nạp hướng tới self-attention tuyến tính, có thể giải thích một phần sự sụt giảm hiệu suất.

Như đã lưu ý trong Phần 2.2, LSTM và GRU xa hơn so với mô hình RNN có cổng đơn giản hóa của chúng tôi. Tuy nhiên, một lớp LSTM duy nhất có thể triển khai self-attention tuyến tính, nhưng các lớp GRU xếp chồng không thể. Chúng tôi tóm tắt ngắn gọn lập luận đằng sau những kết quả này. Lớp LSTM có cơ chế cổng đầu vào tinh vi cổng một trạng thái ô ứng viên dựa trên đầu vào hiện tại và trạng thái trước đó. Cổng và trạng thái ô ứng viên phụ thuộc, trong số những thứ khác, vào đầu vào hiện tại. Cơ chế này có thể đóng vai trò tương tự như gin và triển khai tích ngoài key-value. Sự tái phát của trạng thái ô có thể được đặt để tích hợp hoàn hảo key-values, bằng cách đặt cổng quên cho phù hợp. Cuối cùng, cổng đầu ra điều chỉnh trạng thái ô hiện tại, chứa các key-values tích lũy. Đặt cổng đầu ra để mã hóa truy vấn cho phép tính toán kết quả mong muốn. Chúng tôi lưu ý rằng cổng đầu ra khác với gout: nó nhân các phép biến đổi của trạng thái ô và đầu vào thay vì chỉ đầu vào. Tính chất này làm cho có thể triển khai attention trong một lớp, trong khi 2 lớp cần thiết cho mô hình RNN có cổng (2) của chúng tôi. Mặc dù lớp GRU lấy nhiều yếu tố tính toán từ LSTM, nó không thể triển khai attention vì nó không có cơ chế để tính toán nhân khóa và giá trị.

Chúng tôi tham khảo độc giả đến Phụ lục B để biết thêm chi tiết.

4. RNN có cổng học bắt chước attention
Bây giờ chúng tôi chứng minh rằng RNN có cổng học triển khai self-attention tuyến tính và hiểu cách chúng làm như vậy. Trong phần này, một RNN học sinh được giao nhiệm vụ tái tạo đầu ra của một lớp self-attention tuyến tính. Phụ lục C chứa mô tả chi tiết của tất cả các thí nghiệm được thực hiện trong phần này. Quan trọng, mỗi chuỗi chỉ được trình bày một lần cho mạng.

4.1. Nhận dạng giáo viên
Trong thí nghiệm đầu tiên, chúng tôi huấn luyện một RNN học sinh (|x|= 4, |h|= 100 và |y|= 4) để mô phỏng hành vi của một lớp self-attention tuyến tính với trọng số được lấy mẫu từ phân phối chuẩn và đầu vào xt được lấy mẫu i.i.d. từ phân phối chuẩn. Tổn thất huấn luyện thấp, được báo cáo trong Bảng 1, làm nổi bật rằng hành vi trong-phân-phối của học sinh phù hợp với giáo viên. Tuy nhiên, điều này không đủ để thiết lập rằng học sinh triển khai cùng một hàm với giáo viên.

Chiến lược chúng tôi áp dụng để chỉ ra tương đương chức năng như sau: Đầu tiên, chúng tôi quan sát rằng chỉ các nơ-ron bộ nhớ hoàn hảo (λ= 1) và các nơ-ron quên hoàn hảo (λ= 0) ảnh hưởng đến đầu ra của mạng. Ngoài ra, mỗi nhóm nơ-ron này nhận tất cả thông tin cần thiết để tái tạo tuyến tính tương ứng key-values và các truy vấn từ đầu vào (Bảng 1 cột Score KV và Score Q). Cuối cùng, chúng tôi chỉ ra rằng cổng đầu ra và ma trận giải mã nhân chính xác key-values tích lũy với các truy vấn hiện tại, dẫn đến nhận dạng đúng hàm self-attention của giáo viên, thậm chí ngoài phân phối huấn luyện (Bảng 1 Polynomial distance).

Sau quá trình học, một phần đáng kể của trọng số trong cổng đầu vào và đầu ra và phép đọc trở thành số không. Do đó chúng ta có thể cắt tỉa các nơ-ron với trọng số đầu vào hoặc đầu ra hoàn toàn là số không, do đó bảo tồn chức năng của mạng. Bằng cách làm như vậy, chúng ta có thể loại bỏ 86 trong số 100 nơ-ron ẩn và 87 trong số 100 nơ-ron trước-đọc. Sau khi đã hoán vị các hàng trong hai cơ chế cổng và sắp xếp lại các nơ-ron ẩn, chúng tôi vẽ các trọng số kết quả trên Hình 2.B.

Phù hợp với cấu trúc của chúng tôi, chỉ các nơ-ron tái phát với λ= 0 hoặc λ= 1 góp phần vào đầu ra của mạng. Các nơ-ron key-values nhận một đa thức bậc 2, vì gin là một dạng song tuyến tính, không có số hạng bậc 1 nào vì cột cuối cùng của Winm và Winx bằng không cho những đơn vị đó. Tương tự, các nơ-ron truy vấn nhận một đa thức bậc 1. Quá trình học khám phá rằng nó chỉ có thể sử dụng d(d+ 1)/2 = 10 nơ-ron để lưu trữ key-values, tương tự như cấu trúc tối ưu của chúng tôi. Chúng tôi chỉ ra trong Bảng 1 rằng có thể tái tạo tuyến tính key-values từ 10 nơ-ron đó một cách hoàn hảo, cũng như các truy vấn từ 4 nơ-ron truy vấn. Bằng cách kết hợp thông tin này với thực tế rằng các λ là số không và một, chúng tôi suy ra rằng key-values tích lũy ∑t′≤tvt′k⊤t′ có thể thu được tuyến tính từ các nơ-ron key-values ẩn, và các truy vấn tức thời qt từ các nơ-ron truy vấn.

Ngoài ra, cổng đầu ra kết hợp với phép đọc tuyến tính có thể nhân key-values với các truy vấn. Vì chúng tôi đã xác nhận rằng xử lý thời gian tích lũy đúng key-values, trọng tâm của chúng tôi chuyển sang chứng minh rằng xử lý tức thời của RNN có cổng khớp với lớp attention trên toàn bộ miền đầu vào. Cho rằng cả hai kiến trúc chỉ sử dụng các kết hợp tuyến tính và nhân, xử lý tức thời của chúng có thể được biểu diễn như một đa thức của đầu vào. Của self-attention tuyến tính, (WVx)(WKx)⊤(WQx), tương ứng với một đa thức bậc 3, trong khi của RNN có cổng, gout(gin(x)), tương ứng với một bậc 4. Bằng cách so sánh hai đa thức này, chúng ta có thể so sánh các hàm của chúng ngoài miền huấn luyện. Đối với mỗi một trong bốn đầu ra mạng, chúng tôi tính toán các hệ số của các số hạng bậc 4 hoặc thấp hơn của các đa thức tương ứng và lưu trữ thông tin này vào một vector. Sau đó chúng tôi tính khoảng cách Euclidean chuẩn hóa giữa các vector hệ số này của lớp self-attention tuyến tính và RNN có cổng, và báo cáo trung bình trên tất cả 4 đơn vị đầu ra trong Bảng 1. Bằng chứng được trình bày cho đến nay cho phép chúng tôi kết luận rằng mạng học sinh đã nhận dạng đúng hàm của giáo viên.

Mặc dù phần lớn các trọng số được mô tả trong Hình 2.A tuân thủ cấu trúc khối đặc trưng của cấu trúc của chúng tôi, ba hàng cuối cùng trong các ma trận cổng đầu ra lệch khỏi xu hướng này. Như được chỉ ra trong Hình 2.B, ba hàng này có thể được kết hợp thành một hàng duy nhất khớp với cấu trúc mong muốn. Thêm chi tiết về thao tác này có thể được tìm thấy trong Phụ lục C.2.

4.2. Nhận dạng đòi hỏi tham số hóa quá mức nhẹ
Thí nghiệm trước đây chỉ ra rằng chỉ một vài nơ-ron trong một mạng có 100 nơ-ron ẩn cần thiết để sao chép hành vi của một lớp self-attention có kích thước đầu vào là d. Do đó chúng tôi tự hỏi liệu nhận dạng có còn có thể khi giảm số lượng nơ-ron ẩn và trước-cổng-đầu-ra mà học sinh có. Chúng tôi quan sát rằng tham số hóa quá mức nhẹ, khoảng gấp đôi số nơ-ron so với số lượng nơ-ron thực sự cần thiết, cần thiết để đạt được nhận dạng. Chúng tôi báo cáo kết quả trong Hình 3.A.

4.3. Tính phi tuyến làm cho nhận dạng khó khăn hơn
Bây giờ chúng tôi rời khỏi lớp RNN có cổng đơn giản hóa của chúng tôi và tìm hiểu cách phát hiện của chúng tôi áp dụng cho LSTM, GRU và LRU. Chúng tôi sử dụng kiến trúc sau cho ba lớp đó: một lớp nhúng tuyến tính chiếu đầu vào lên biểu diễn tiềm ẩn, sau đó chúng tôi lặp lại lớp tái phát một hoặc hai lần, và cuối cùng áp dụng một phép đọc tuyến tính. Mặc dù những lớp đó thường được kết hợp với chuẩn hóa lớp, dropout, hoặc kết nối bỏ qua trong các thí nghiệm deep learning hiện đại, chúng tôi không bao gồm bất kỳ cái nào trong số đó ở đây để ở gần nhất có thể với đặc tả của giáo viên. Trong một lớp LRU, chiều đầu vào/đầu ra khác với số lượng nơ-ron khác nhau; chúng tôi ở đây đặt tất cả những chiều đó về cùng một giá trị để so sánh công bằng với LSTM và GRU.

Chúng tôi so sánh những phương pháp này với hiệu suất của RNN có cổng đơn giản hóa của chúng tôi, với cả kết nối tái phát tuyến tính đường chéo (như trong Phương trình 2) và dày đặc.

Chúng tôi báo cáo kết quả trong Hình 4.A cho đầu vào chiều d= 6. Mặc dù kết nối đường chéo cung cấp một thiên hướng quy nạp hữu ích để học cách bắt chước self-attention tuyến tính, nó không hoàn toàn cần thiết vì thay đổi kết nối tái phát thành dày đặc không ảnh hưởng đáng kể đến hiệu suất. Về mặt lý thuyết có thể nhận dạng giáo viên với một lớp LSTM. Tuy nhiên, gradient descent không tìm thấy giải pháp như vậy và hiệu suất của LSTM gần với GRU không thể triển khai attention. Được thúc đẩy bởi cấu trúc của Phần 3, chúng tôi sửa đổi nhẹ kiến trúc LRU (LRU+) và thêm một cổng đầu vào phi tuyến vào cổng đầu ra đã tồn tại. Chúng tôi thấy rằng sửa đổi này cải thiện đáng kể khả năng của một lớp LRU để bắt chước attention. Phụ lục C chứa các thí nghiệm so sánh rộng rãi các kiến trúc LRU khác nhau, cũng như các so sánh có tính đến số lượng tham số của các kiến trúc khác nhau. Ngoài ra, chúng tôi cung cấp kết quả xác nhận rằng các tương tác nhân là cơ bản để bắt chước attention: thay thế cổng bằng một MLP 1-lớp ẩn với cùng số lượng tham số làm giảm đáng kể hiệu suất.

5. Học trong ngữ cảnh dựa trên attention xuất hiện trong RNN đã huấn luyện
Phần trước chỉ ra rằng RNN có cổng học sao chép một giáo viên self-attention tuyến tính cho trước. Bây giờ chúng tôi chứng minh rằng chúng có thể tìm thấy cùng một giải pháp như self-attention tuyến tính khi cả hai đều được học. Để làm điều đó, chúng tôi nghiên cứu một tác vụ hồi quy trong ngữ cảnh trong đó mạng được hiển thị một vài cặp đầu vào-đầu ra và sau đó phải dự đoán giá trị đầu ra tương ứng với một đầu vào chưa thấy. Self-attention tuyến tính là một thiên hướng quy nạp đặc biệt có lợi để giải quyết tác vụ này. Khi ánh xạ đầu vào-đầu ra là tuyến tính, (von Oswald et al., 2023) đã chỉ ra rằng self-attention tuyến tính triển khai một bước của gradient descent.

5.1. Hồi quy tuyến tính trong ngữ cảnh
Hồi quy tuyến tính bao gồm việc ước tính các tham số W∗∈Rdy×dx của một mô hình tuyến tính y=W∗x từ một tập hợp quan sát {(xt, yt)}Tt=1 thỏa mãn yt=W∗xt. Mục tiêu bao gồm việc tìm một tham số Ŵ giảm thiểu tổn thất lỗi bình phương L(W) =1/(2T)∑Tt=1∥yt−Wxt∥2. Cho một ước tính ban đầu của tham số W0, một bước gradient descent trên L với tốc độ học Tη cho ra thay đổi trọng số

∆W0=η∑Tt=1(yt−W0xt)x⊤t. (3)

Trong phiên bản trong ngữ cảnh của tác vụ, các quan sát (xt, yt)1≤t≤T được cung cấp lần lượt cho mạng, và sau đó, tại thời điểm T+ 1, mạng được truy vấn với (xT+1,0) và đầu ra của nó được hồi quy so với yT+1. Dưới thiết lập này, von Oswald et al. (2023) chỉ ra rằng nếu tất cả các số hạng bias bằng không, một lớp self-attention tuyến tính học triển khai một bước gradient descent bắt đầu từ W0= 0 và dự đoán thông qua

ŷT+1= (W0+ ∆W0)xT+1=η∑Tt=1ytx⊤txT+1.(4)

Trong phần sau, chúng tôi chỉ ra rằng RNN có cổng cũng học triển khai cùng một thuật toán và tận dụng cấu trúc thưa thớt của các ma trận attention khác nhau tương ứng với gradient descent để học một biểu diễn nén hơn so với cấu trúc xây dựng.

5.2. RNN có cổng học triển khai gradient descent
Bây giờ chúng tôi huấn luyện RNN có cổng như trong Phương trình 2 để giải quyết tác vụ hồi quy tuyến tính trong ngữ cảnh, xem Phụ lục D.1 để biết thêm chi tiết. Chúng tôi đặt số lượng quan sát thành T= 12 và đặt chiều đầu vào và đầu ra thành 3 sao cho d= 6. Một khi đã học, RNN triển khai một bước gradient descent với tốc độ học tối ưu, cũng là giải pháp tối ưu mà một lớp self-attention tuyến tính có thể tìm thấy (Mahankali et al., 2023). Một số bằng chứng ủng hộ tuyên bố này: tổn thất huấn luyện của RNN sau huấn luyện (0.0945) gần bằng với một bước gradient descent tối ưu (0.0947) và RNN đã huấn luyện triển khai cùng một hàm tức thời, như phân tích đa thức của Bảng 2 tiết lộ.

Trọng số self-attention tuyến tính triển khai gradient descent có một cấu trúc rank thấp rất cụ thể (von Oswald et al., 2023). Để kiểm tra liệu mạng đã học cấu trúc nén tương ứng của chúng tôi, chúng tôi thay đổi kích thước RNN có cổng và báo cáo trong Hình 3.C sự khác biệt giữa tổn thất huấn luyện cuối cùng và tổn thất thu được sau một bước gradient descent tối ưu. Chúng tôi quan sát một chuyển đổi tương tự từ cao đến thấp như trong thí nghiệm giáo viên-học sinh, lần này xảy ra xung quanh số lượng nơ-ron tái phát được quy định bởi cấu trúc rank thấp của chúng tôi. Do đó RNN có cổng học một biểu diễn nén hơn so với cái naively bắt chước self-attention. Kết quả này cung cấp một số hy vọng liên quan đến việc mở rộng O(d4) kém cơ bản của cấu trúc chúng tôi: trong các tình huống đòi hỏi một cơ chế attention với ma trận (WV, WK, WQ) rank thấp, RNN có cổng có thể triển khai attention với ít nơ-ron hơn nhiều. Một hiểu biết chính xác về mức độ nén có thể trong các tình huống thực tế đòi hỏi nghiên cứu thêm.

Trong Phụ lục D.3, chúng tôi cung cấp một tập hợp kết quả bổ sung tập trung vào gọi lại liên kết, một tác vụ trong ngữ cảnh nơi mục tiêu là ghi nhớ (và sau đó truy xuất) các liên kết giữa các cặp đầu vào được trình bày theo chuỗi (Fu et al., 2023). Điều này có thể được xem như một trường hợp đơn giản của phân loại trong ngữ cảnh, không yêu cầu khái quát hóa. Như đối với hồi quy tuyến tính, chúng tôi thấy rằng RNN có cổng đã huấn luyện khám phá một thuật toán tương tự như được sử dụng bởi self-attention tuyến tính.

5.3. RNN có cổng phi tuyến là những người học trong ngữ cảnh tốt hơn so với một bước gradient descent
Cuối cùng, như một câu hỏi phụ, chúng tôi so sánh khả năng học trong ngữ cảnh của các kiến trúc RNN có cổng phi tuyến là LSTM, GRU và LRU. Mặc dù không phải là trọng tâm chính của bài báo chúng tôi, điều này cho phép chúng tôi đặt kết quả trước đây của chúng tôi vào quan điểm. Đặc biệt, chúng tôi quan tâm đến việc hiểu liệu sự tương tự với attention có tương quan với hiệu suất học trong ngữ cảnh hay không, vì attention đã được giả thuyết là một cơ chế chính cho học trong ngữ cảnh (Olsson et al., 2022; Garg et al., 2022; von Oswald et al., 2023). Chúng tôi báo cáo kết quả so sánh trong Hình 4.B, đo lường tổn thất trên trọng số W∗ được rút từ phân phối với phương sai gấp đôi so với phân phối được sử dụng để huấn luyện mô hình.

Nhìn chung, chúng tôi thấy rằng tính phi tuyến giúp ích rất nhiều và cho phép các kiến trúc RNN có cổng phi tuyến vượt trội hơn một bước gradient descent khi được cung cấp đủ tham số, gợi ý rằng chúng triển khai một cơ chế tinh vi hơn. Đáng ngạc nhiên, mặc dù GRU là kiến trúc xa nhất so với attention, nó hoạt động tốt nhất trong tác vụ. Trong số các lớp LRU khác nhau mà chúng tôi so sánh, chúng tôi thấy một tương quan cao giữa khả năng học trong ngữ cảnh và sự gần gũi với attention, xem Hình 6 trong Phụ lục. Đặc biệt, chúng tôi quan sát một cải thiện hiệu suất to lớn từ kiến trúc LRU vanilla đến những kiến trúc bổ sung bao gồm cổng đầu vào để khớp cấu trúc của chúng tôi gần hơn. Một lần nữa, thay thế GLU bằng MLP dẫn đến một sự giảm lớn về hiệu suất.

6. Thảo luận
Nghiên cứu của chúng tôi tiết lộ mối quan hệ khái niệm gần gũi hơn giữa RNN và các kiến trúc dựa trên attention so với thường được giả định. Chúng tôi chứng minh rằng RNN có cổng có thể triển khai self-attention tuyến tính về mặt lý thuyết và thực tế, làm cầu nối khoảng cách giữa hai kiến trúc này. Hơn nữa, mặc dù Transformers đã được chỉ ra là những người học trong ngữ cảnh mạnh mẽ (Brown et al., 2020; Chan et al., 2022), chúng tôi thấy rằng RNN xuất sắc trong các tác vụ học trong ngữ cảnh đồ chơi và hiệu suất này một phần không tương quan với thiên hướng quy nạp kiến trúc hướng tới attention. Điều này làm nổi bật nhu cầu nghiên cứu thêm về sự khác biệt giữa RNN và Transformers trong các thiết lập được kiểm soát, như cũng được ủng hộ bởi (Garg et al., 2022).

Kết quả của chúng tôi một phần phục vụ như một kết quả tiêu cực: triển khai attention là có thể nhưng đòi hỏi bình phương số lượng tham số mà attention có. Chúng tôi đã chỉ ra rằng RNN có cổng có thể tận dụng nén có thể, nhưng hiểu liệu các cơ chế attention thế giới thực nằm trong chế độ này vẫn là một câu hỏi mở. Tuy nhiên, công trình của chúng tôi có liên quan thực tế hiện tại vì nó cung cấp một khung có thể hướng dẫn các phát triển thuật toán trong tương lai, như chúng tôi ví dụ hóa trong Phụ lục B.5. Việc làm cầu nối khoảng cách giữa sức mạnh tính toán của Transformers và hiệu quả suy luận của RNN là một lĩnh vực nghiên cứu phát triển mạnh (Fournier et al., 2023), và liên kết mà chúng tôi tạo ra tạo thuận lợi cho việc nội suy giữa hai lớp mô hình đó.

Cuối cùng, công trình của chúng tôi mang ý nghĩa vượt ra ngoài deep learning. Được truyền cảm hứng bởi bằng chứng từ khoa học thần kinh hỗ trợ sự tồn tại của tính dẻo synap ở các thang thời gian khác nhau, công trình trước đây (Schmidhuber, 1992; Ba et al., 2016; Miconi et al., 2018) đã thêm một quy tắc học Hebbian nhanh, tương tự như self-attention tuyến tính, vào tính dẻo synap chậm với RNN. Chúng tôi chỉ ra rằng, ở một mức độ nào đó, cơ chế này đã tồn tại trong động lực học nơ-ron, với điều kiện là phản ứng của các nơ-ron có thể được khuếch đại hoặc tắt nhân theo cách phụ thuộc đầu vào. Kết quả của chúng tôi do đó gợi ý rằng các mạch nơ-ron tái phát với hằng số thời gian tích hợp dài, chẳng hạn như những mạch được tìm thấy trong vỏ não trước trán, có thể đang học và giữ các liên kết giữa các đầu vào quá khứ trong bộ nhớ làm việc. Những mạch này sẽ hiệu quả mã hóa các trọng số liên kết trong hoạt động nơ-ron của chúng, không phải trong các kết nối synap thực tế, như sẽ là trường hợp đối với các mạng bộ nhớ liên kết cổ điển (Steinbuch, 1961; Willshaw et al., 1969; Kohonen, 1972). Thú vị, một số cơ chế đơn nơ-ron và mức mạch đã được xác định thực nghiệm có thể hỗ trợ phép nhân cần thiết trong mạng nơ-ron sinh học (Silver, 2010). Chúng tôi suy đoán rằng các cơ chế nhân như vậy có thể tham gia vào việc triển khai các tính toán giống self-attention trong mạch sinh học.

Lời cảm ơn
Các tác giả cảm ơn Asier Mujika và Razvan Pascanu cho những thảo luận vô giá. Nghiên cứu này được hỗ trợ bởi một khoản tài trợ Ambizione (PZ00P3 186027) từ Quỹ Khoa học Quốc gia Thụy Sĩ và một Khoản tài trợ Nghiên cứu ETH (ETH-23 21-1).
