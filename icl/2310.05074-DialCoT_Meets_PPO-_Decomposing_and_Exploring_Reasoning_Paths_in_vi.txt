# 2310.05074.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2310.05074.pdf
# Kích thước tệp: 5624509 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
DialCoT Kết hợp PPO: Phân tích và Khám phá Đường dẫn Lý luận trong
Các Mô hình Ngôn ngữ Nhỏ hơn
Chengcheng Han♢∗Xiaowei Du♠Che Zhang♡
Yixin Lian♠Xiang Li♢Ming Gao♢♣†Baoyuan Wang♠†
♢Trường Khoa học và Kỹ thuật Dữ liệu, Đại học Sư phạm Đông Trung Quốc
♠Xiaobing.AI
♡Trường Phần mềm & Vi điện tử, Đại học Bắc Kinh
♣KLATASDS-MOE tại Trường Thống kê, Đại học Sư phạm Đông Trung Quốc
chengchenghan@stu.ecnu.edu.cn
{duxiaowei,lianyixin,wangbaoyuan}@xiaobing.ai
mmt@stu.pku.edu.cn
{xiangli,mgao}@dase.ecnu.edu.cn
Tóm tắt
Phương pháp nhắc Chain-of-Thought (CoT) đã được chứng minh là hiệu quả trong việc nâng cao khả năng lý luận của các Mô hình Ngôn ngữ Lớn (LLM) với ít nhất 100 tỷ tham số. Tuy nhiên, nó không hiệu quả hoặc thậm chí có hại khi được áp dụng cho các nhiệm vụ lý luận trong các Mô hình Ngôn ngữ Nhỏ hơn (SLM) với ít hơn 10 tỷ tham số. Để giải quyết hạn chế này, chúng tôi giới thiệu Dialogue-guided Chain-of-Thought (DialCoT) sử dụng định dạng đối thoại để tạo ra các bước lý luận trung gian, hướng dẫn mô hình đến câu trả lời cuối cùng. Ngoài ra, chúng tôi tối ưu hóa việc lựa chọn đường dẫn lý luận của mô hình bằng thuật toán Proximal Policy Optimization (PPO), nâng cao thêm khả năng lý luận của nó. Phương pháp của chúng tôi mang lại một số lợi thế so với các phương pháp trước đây. Thứ nhất, chúng tôi biến đổi quá trình giải quyết các câu hỏi lý luận phức tạp bằng cách chia nhỏ chúng thành một chuỗi các câu hỏi phụ đơn giản hơn, giảm đáng kể độ khó của nhiệm vụ và làm cho nó phù hợp hơn với SLM. Thứ hai, chúng tôi tối ưu hóa việc lựa chọn đường dẫn lý luận của mô hình thông qua thuật toán PPO. Chúng tôi thực hiện các thí nghiệm toàn diện trên bốn bộ dữ liệu lý luận số học, chứng minh rằng phương pháp của chúng tôi đạt được cải thiện hiệu suất đáng kể so với các đối thủ cạnh tranh hiện đại.1

1 Giới thiệu
Với sự ra đời của phương pháp nhắc Chain-of-Thought (CoT) (Wei et al., 2022), khuyến khích các Mô hình Ngôn ngữ Lớn (LLM) tạo ra một chuỗi các bước trung gian để giúp có được câu trả lời cuối cùng, khả năng lý luận của LLM đã có sự cải thiện đáng kể. Tuy nhiên, kết quả sơ bộ của Wei et al. (2022) đã chứng minh rằng phương pháp nhắc CoT chỉ thể hiện mức tăng hiệu suất đáng kể trên các LLM (≥100B), chẳng hạn như LaMDA-137B (Thoppilan et al., 2022), GPT-3 175B (Brown et al., 2020) và PaLM-540B (Chowdhery et al., 2022). Nhưng nó không hiệu quả, hoặc thậm chí có hại, đối với hiệu suất trên các nhiệm vụ lý luận trong các Mô hình Ngôn ngữ Nhỏ hơn (≤10B). Hiện tượng này được giải thích bởi Wei et al. (2022), người cho rằng những khả năng như hiểu ngữ nghĩa và ánh xạ ký hiệu chỉ biểu hiện ở quy mô lớn hơn.

--- TRANG 2 ---
Yêu cầu tính toán khổng lồ và chi phí suy luận của các LLM khiến chúng không khả thi cho việc triển khai rộng rãi. Có sự quan tâm cấp bách của cộng đồng trong việc tìm hiểu cách nâng cao thêm khả năng lý luận trong các Mô hình Ngôn ngữ Nhỏ hơn (SLM).

Các nghiên cứu gần đây (Magister et al., 2022; Ho et al., 2022; Fu et al., 2023) đã cố gắng nâng cao hiệu suất của SLM trên các nhiệm vụ lý luận bằng cách tinh chỉnh chúng với dữ liệu huấn luyện được tạo ra từ LLM có chứa các bước lý luận trung gian. Tuy nhiên, kết quả vẫn chưa tối ưu. Để nâng cao thêm khả năng lý luận của SLM, chúng tôi đề xuất Dialogue-guided Chain-of-Thought (DialCoT), nhằm mục đích tạo ra dần dần các bước lý luận trung gian theo định dạng đối thoại, thay vì tạo ra tất cả các bước lý luận trung gian cùng một lúc (như thể hiện trong Hình 1). Cụ thể, chúng tôi gán cho mô hình hai vai trò: Decomposer và Solver. Decomposer có nhiệm vụ chia nhỏ câu hỏi ban đầu thành một chuỗi các câu hỏi phụ. Solver giải quyết tuần tự từng câu hỏi phụ được trình bày bởi Decomposer, từ đó có được câu trả lời cho câu hỏi ban đầu. Chúng sử dụng các hướng dẫn khác nhau trong khi chia sẻ cùng các tham số mô hình.

Chúng tôi đề xuất ba hình thức khác nhau của DialCoT: 1) DialCoT-A (All at once), trong đó Decomposer tạo ra tất cả các câu hỏi phụ cùng một lúc và Solver đồng thời cung cấp tất cả các câu trả lời. 2) DialCoT-M (Mixed), trong đó Decomposer tạo ra tất cả các câu hỏi phụ cùng một lúc nhưng Solver tuần tự đưa ra các câu trả lời của các câu hỏi phụ được tạo ra bởi Decomposer. 3) DialCoT-S (Step by step), trong đó cả Decomposer và Solver đều hoạt động tuần tự để tạo ra các câu hỏi phụ và các câu trả lời tương ứng của chúng. Chúng tôi cung cấp một so sánh chi tiết về hiệu suất của ba hình thức khác nhau của DialCoT trong Mục 4.4. Hơn nữa, dựa trên DialCoT-S, chúng tôi thiết kế DialCoT-S-PPO, tận dụng thuật toán Proximal Policy Optimization để lựa chọn đường dẫn lý luận tối ưu, từ đó nâng cao thêm hiệu suất của nó trong các nhiệm vụ lý luận. So với các phương pháp trước đây (Ho et al., 2022; Fu et al., 2023), phương pháp của chúng tôi có hai lợi thế chính:

1. Chúng tôi biến đổi quá trình giải quyết một câu hỏi lý luận phức tạp thành việc phân tích câu hỏi và giải quyết một chuỗi các câu hỏi phụ đơn giản hơn, điều này giảm độ khó của nhiệm vụ và phù hợp hơn với SLM.

2. Bằng cách chia nhỏ các bước lý luận trung gian thành các câu hỏi phụ và câu trả lời theo định dạng đối thoại, chúng tôi có thể sử dụng học tăng cường hiệu quả hơn để chọn đường dẫn lý luận tối ưu từ nhiều tùy chọn khác nhau.

Để xác nhận hiệu quả của phương pháp của chúng tôi, chúng tôi tinh chỉnh Flan-T5 (Chung et al., 2022) sử dụng 7000 ví dụ huấn luyện từ bộ dữ liệu GSM8K (Cobbe et al., 2021) bao gồm các câu hỏi và câu trả lời trung gian. Kết quả vượt qua phương pháp mới nhất, SpecialFT (Fu et al., 2023), 6.2%. Đáng chú ý, lượng dữ liệu huấn luyện chúng tôi sử dụng chỉ bằng 1/20 so với SpecialFT. Ngoài ra, để xác minh khả năng tổng quát hóa của mô hình trên các nhiệm vụ ngoài phân phối, chúng tôi cũng thử nghiệm trên các bộ dữ liệu Multi-Arith (Roy and Roth, 2015), ASDiv (Miao et al., 2020) và SVAMP (Patel et al., 2021). Phương pháp của chúng tôi đạt được hiệu suất hiện đại so với các baseline khác.

2 Nghiên cứu liên quan
2.1 Phương pháp nhắc Chain-of-Thought

Chain-of-Thought (CoT), nâng cao đáng kể khả năng lý luận của các mô hình ngôn ngữ lớn, ban đầu được tiên phong bởi Wei et al. (2022). Phương pháp này tập trung vào việc tăng cường các ví dụ few-shot với các bước lý luận chi tiết, từ đó cải thiện hiệu suất một cách rõ rệt trên các nhiệm vụ lý luận. Các nghiên cứu tiếp theo, được truyền cảm hứng bởi Wei et al. (2022), đã tinh chỉnh thêm phương pháp CoT, chẳng hạn như Self-Consistency (Wang et al., 2022), Least-to-Most prompting (Zhou et al., 2022b), Dynamic Least-to-Most prompting (Drozdov et al., 2022), Self-Training (Huang et al., 2022), Verifier (Li et al., 2022) và Tree of Thought (Yao et al., 2023). Các phương pháp nêu trên chủ yếu tập trung vào việc cải thiện định dạng cụ thể của phương pháp nhắc CoT để kích thích tốt hơn khả năng lý luận của LLM (≥100B). Tuy nhiên, chúng không được thiết kế riêng để tăng cường khả năng lý luận của SLM (≤10B). Chúng tôi đề xuất một phương pháp mới được thiết kế đặc biệt để nâng cao hiệu suất của SLM trên các nhiệm vụ lý luận.

2.2 Nâng cao Lý luận trong SLM
Chung et al. (2022) quan sát rằng việc huấn luyện SLM với dữ liệu bao gồm các bước lý luận trung gian có thể cải thiện khả năng lý luận của SLM. Cả Magister et al. (2022) và Ho et al. (2022) đều nâng cao khả năng lý luận của SLM bằng cách

--- TRANG 3 ---
tinh chỉnh chúng với dữ liệu huấn luyện, bao gồm các bước lý luận trung gian được tạo ra bởi LLM. STaR (Zelikman et al., 2022) cho phép mô hình tự cải thiện thông qua các lý lẽ được tạo ra bởi chính nó. SpecialFT (Fu et al., 2023) sử dụng LLM làm mô hình giáo viên và sử dụng khớp phân phối trong chưng cất kiến thức để chuyển giao khả năng lý luận từ LLM sang SLM. Orca (Mukherjee et al., 2023) học cách bắt chước quá trình lý luận của LLM từ các tín hiệu phong phú được tạo ra bởi LLM, bao gồm các dấu vết giải thích, quá trình suy nghĩ từng bước và các hướng dẫn phức tạp khác.

Khác biệt, DialCoT biến đổi việc giải quyết các câu hỏi lý luận phức tạp thành việc phân tích câu hỏi và giải quyết một chuỗi các câu hỏi đơn giản hơn, giảm đáng kể độ khó của nhiệm vụ. Hơn nữa, chúng tôi kết hợp thuật toán PPO để cho phép mô hình chọn đường dẫn lý luận tối ưu giữa nhiều tùy chọn, từ đó nâng cao thêm hiệu suất trong các nhiệm vụ lý luận. Đáng chú ý, phương pháp của chúng tôi không yêu cầu tạo ra một lượng lớn dữ liệu huấn luyện với các bước lý luận trung gian thông qua LLM. Ví dụ, chỉ bằng cách tinh chỉnh đơn giản với 7.000 ví dụ từ bộ dữ liệu GSM8K, chúng tôi có thể đạt được sự nâng cao đáng kể trong hiệu suất SLM trên các nhiệm vụ lý luận.

2.3 Phân tích Câu hỏi
Phân tích câu hỏi là rất quan trọng để hiểu và giải quyết các câu hỏi phức tạp. Nghiên cứu trước đây (Kalyanpur et al., 2012) sử dụng các quy tắc phân tích dựa trên các đặc điểm từ vựng-cú pháp để hỗ trợ phân tích câu hỏi. HSP (Zhang et al., 2019) đề xuất một phương pháp phân tích ngữ nghĩa phân cấp dựa trên mô hình sequence-to-sequence, kết hợp một bộ phân tích câu hỏi và một bộ trích xuất thông tin. Patel et al. (2022) thiết kế một phương pháp phân tích câu hỏi có con người tham gia để cải thiện hiệu suất mô hình. Least-to-Most prompting (Zhou et al., 2022b) cải thiện định dạng của CoT, nâng cao khả năng lý luận của LLM bằng cách phân tích các vấn đề. Self-Ask (Press et al., 2022) tự hỏi các câu hỏi tiếp theo một cách rõ ràng trước khi trả lời câu hỏi ban đầu để thực hiện các nhiệm vụ lý luận kết hợp. DecomT5 (Zhou et al., 2022a) phát triển các mô hình dựa trên phân tích mạnh mẽ sử dụng giám sát từ xa từ các văn bản có thể so sánh. Decomposition Distillation (Shridhar et al., 2022) học một phân tích ngữ nghĩa của câu hỏi ban đầu thành một chuỗi các câu hỏi phụ và sử dụng nó để huấn luyện hai mô hình được chỉ định cho phân tích và giải quyết câu hỏi. So với các phương pháp nêu trên, chúng tôi không chỉ phân tích câu hỏi mà còn cho phép mô hình chọn đường dẫn lý luận tối ưu thông qua các phương pháp học tăng cường, từ đó nâng cao thêm khả năng của mô hình trong việc giải quyết các câu hỏi phức tạp.

3 Dialogue-guided Chain-of-Thought
Chúng tôi đề xuất Dialogue-guided Chain-of-Thought (DialCoT), nhằm mục đích phân tích các câu hỏi phức tạp thành các câu hỏi phụ theo định dạng đối thoại và dần dần hướng dẫn mô hình để có được câu trả lời cuối cùng. Cụ thể, chúng tôi giới thiệu hai vai trò cho mô hình, đó là Decomposer và Solver, những người tham gia vào tương tác dựa trên đối thoại. Decomposer chịu trách nhiệm chia nhỏ câu hỏi ban đầu thành một chuỗi các câu hỏi phụ đơn giản hơn, trong khi Solver tuần tự trả lời các câu hỏi phụ này. Chúng tôi thiết kế các hướng dẫn riêng biệt cho Decomposer và Solver, sau đó thực hiện điều chỉnh hướng dẫn (Wei et al., 2021) trên SLM. Chúng tôi đầu tiên giới thiệu ba hình thức khác nhau của DialCoT. Tiếp theo, chúng tôi mô tả cách chúng tôi kết hợp thuật toán Proximal Policy Optimization (PPO) vào DialCoT để cho phép mô hình lựa chọn đường dẫn lý luận tối ưu và nâng cao thêm khả năng lý luận của nó.

3.1 Ba Hình thức của DialCoT
Chúng tôi đề xuất ba hình thức đối thoại khác nhau của DialCoT, đó là DialCoT-A, DialCoT-M và DialCoT-S. Cụ thể, DialCoT-A nhằm mục đích hướng dẫn SLM trong lý luận thông qua số lượt đối thoại tối thiểu. DialCoT-M tinh chỉnh Solver dựa trên DialCoT-A, giảm thêm độ phức tạp của nhiệm vụ. DialCoT-S phân tích tối đa các bước lý luận trung gian, cho phép nó tham chiếu các câu hỏi phụ trước đó và câu trả lời của chúng khi đề xuất các câu hỏi phụ mới. Hình 2 và Hình 3(1) trình bày khung tổng thể của chúng.2 Tiếp theo, chúng tôi sẽ giới thiệu từng hình thức này một cách riêng biệt.

DialCoT-A (All at once). Chúng tôi đầu tiên thiết lập một hướng dẫn cho Decomposer, cho phép nó tạo ra tất cả các câu hỏi phụ trong một bước duy nhất. Tiếp theo, chúng tôi kết hợp các câu hỏi phụ được tạo ra vào văn bản ban đầu và thiết kế một hướng dẫn mới cho Solver, cho phép Solver trả lời tất cả các câu hỏi phụ đồng thời. Hình 2(1) hiển thị

2Cấu trúc prompt cụ thể có thể được tìm thấy trong Bảng 3 của Phụ lục A.

--- TRANG 4 ---
Hình 2: (1) DialCoT-A, trong đó Decomposer tạo ra tất cả các câu hỏi phụ cùng một lúc, và Solver phản hồi tất cả các câu hỏi phụ trong một bước duy nhất. (2) DialCoT-M, trong đó Decomposer giống như trong DialCoT-A, trong khi Solver giải quyết một câu hỏi phụ tại một bước duy nhất, với phản hồi được kết hợp vào văn bản ban đầu để hỗ trợ giải quyết các câu hỏi phụ tiếp theo.

các hướng dẫn chúng tôi thiết kế và các ví dụ về input/output khi mô hình hoạt động như một Decomposer và Solver. DialCoT-A có động lực tương tự như Orca (Mukherjee et al., 2023), cả hai đều phấn đấu cải thiện hiệu suất lý luận của mô hình bằng cách cung cấp các đường dẫn lý luận rõ ràng. Orca thể hiện đường dẫn lý luận thông qua các bước giải quyết vấn đề, trong khi phương pháp của chúng tôi thể hiện đường dẫn lý luận thông qua một chuỗi các câu hỏi phụ.

DialCoT-M (Mixed). Sau khi thu được một chuỗi các câu hỏi phụ thông qua cùng một Decomposer được sử dụng trong DialCoT-A, chúng tôi tuần tự thay thế câu hỏi cuối cùng trong văn bản ban đầu bằng các câu hỏi phụ này, cho phép Solver giải quyết từng câu hỏi phụ một cách riêng biệt. Phản hồi của Solver từ mỗi câu hỏi phụ được thêm vào văn bản ban đầu, cung cấp hỗ trợ ngữ cảnh cho việc giải quyết các câu hỏi tiếp theo. Hình 2(2) trình bày một ví dụ về DialCoT-M giải quyết một bài toán từ toán học. So với DialCoT-A, DialCoT-M giảm thiểu độ phức tạp của nhiệm vụ cho Solver bằng cách giải quyết một câu hỏi đơn giản và đơn lẻ trong mỗi bước.

DialCoT-S (Step by Step). Chúng tôi thiết kế các hướng dẫn mới để chỉ đạo Decomposer chỉ tạo ra một câu hỏi phụ duy nhất tại một bước và Solver để giải quyết câu hỏi phụ đó. Các phản hồi được đặt trước bằng các định danh vai trò như "Decomposer: " và "Solver: ". Lịch sử của cuộc đối thoại được thêm vào sau văn bản ban đầu, hỗ trợ mô hình trong việc trả lời các câu hỏi tiếp theo và rút ra câu trả lời cuối cùng. Hình 3(1) hiển thị khung tổng thể của DialCoT-S. So với hai hình thức trước đây của DialCoT, DialCoT-S có thể tham chiếu các câu hỏi phụ trước đó và câu trả lời của chúng khi tạo ra các câu hỏi phụ mới. Hơn nữa, DialCoT-S giống hơn với định dạng đối thoại nhiều lượt truyền thống. Do đó, nó có thể kích thích hiệu quả hơn khả năng đối thoại nhiều lượt của mô hình để cải thiện hiệu suất của mô hình trên các nhiệm vụ lý luận.

3.2 DialCoT-S-PPO
DialCoT-S-PPO nhằm mục đích cho phép mô hình lựa chọn đường dẫn lý luận tối ưu bằng cách kết hợp DialCoT-S với thuật toán PPO, nâng cao thêm hiệu suất của mô hình trên các nhiệm vụ lý luận. Hình 3(2) trình bày một ví dụ về DialCoT-S-PPO giải quyết một bài toán từ toán học. DialCoT-S-PPO chọn các câu hỏi hoặc câu trả lời trung gian tối ưu từ nhiều đầu ra của mô hình, do đó tạo thành một đường dẫn lý luận thông qua một chuỗi các lựa chọn.

Cụ thể, chúng tôi đầu tiên cần thu thập một số dữ liệu được tạo thành từ các trạng thái S, hành động A và phần thưởng R để huấn luyện mạng chính sách πθ. S đại diện cho không gian các trạng thái của môi trường, là đầu vào của mạng chính sách. Gọi st∈S là một trạng thái

--- TRANG 5 ---
Hình 3: (1) DialCoT-S, trong đó Decomposer trình bày một câu hỏi phụ tại một bước và Solver trả lời nó. Thông tin đối thoại quá khứ của họ được chèn vào <Lịch sử Đối thoại> để hỗ trợ tạo ra câu trả lời cho câu hỏi cuối cùng. (2) Một ví dụ về DialCoT-S-PPO giải quyết một bài toán từ toán học. Mạng chính sách được sử dụng để chọn một phản hồi từ mỗi bước của SLM, cuối cùng tạo thành một đường dẫn lý luận tối ưu và đạt đến câu trả lời cuối cùng.

tại thời điểm t được định nghĩa là
st = [h1;h2;...;hk], (1)
[h1,h2, ...,hk] = LMϕ(X), (2)
trong đó X biểu thị văn bản đầu vào được xây dựng thông qua DialCoT-S, LMϕ(·) đại diện cho SLM sau khi điều chỉnh hướng dẫn và h biểu thị trạng thái ẩn cuối cùng của phản hồi của mô hình. Chúng tôi sử dụng tìm kiếm chùm để tạo ra các phản hồi top-k với xác suất cao nhất làm ứng viên và st là sự nối của các trạng thái ẩn cuối cùng của các ứng viên này.

A = [0,1, ..., k] đại diện cho không gian hành động. Tại mỗi thời điểm t, chúng tôi nhập st vào πθ để có được xác suất p của các hành động:
p = πθ(st). (3)

Dựa trên xác suất p, πθ chọn một hành động at∈A, đại diện cho việc chọn ứng viên thứ at. Trong giai đoạn khám phá, πθ có được at thông qua lấy mẫu. Trong giai đoạn suy luận, πθ chọn at với xác suất cao nhất. Khi mô hình trả lời đúng một câu hỏi phụ, nó nhận được phần thưởng rm∈[0,1]. rm là một siêu tham số đại diện cho mức độ mà mô hình tập trung vào các bước trung gian. Khi mô hình trả lời đúng câu hỏi cuối cùng, nó nhận được phần thưởng rf = 1. Trong tất cả các trường hợp khác, phần thưởng là 0. Chúng tôi cập nhật mạng chính sách πθ thông qua hàm mục tiêu sau:

J(θ) = Eπ[min(πθ(at|st)/πθold(at|st) * At, clip(πθ(at|st)/πθold(at|st), 1−ε, 1+ε) * At)], (4)

trong đó πθ(·) đại diện cho mạng chính sách huấn luyện và πθold biểu thị mạng chính sách đã tương tác với môi trường để thu thập dữ liệu. Thông tin thêm về clip(·) và At có thể được tìm thấy trong Schulman et al. (2017). Sau khi cập nhật các tham số của πθ, tham số mới của πθ được truyền tới πθold. Chúng tôi sau đó lặp lại việc thu thập dữ liệu và cập nhật πθ cho đến khi hoàn thành huấn luyện.

4 Thí nghiệm
4.1 Bộ dữ liệu
Chúng tôi xem xét bốn bộ dữ liệu bài toán từ toán học sau: GSM8K (Cobbe et al., 2021), Multi-Arith (Roy and Roth, 2015), ASDiv (Miao et al., 2020) và SVAMP (Patel et al., 2021). Bộ dữ liệu GSM8K chứa 7.000 mẫu huấn luyện với các câu hỏi và câu trả lời trung gian. Khác với các nghiên cứu trước đây (Magister et al., 2022; Ho et al., 2022; Fu et al., 2023), chúng tôi chỉ tinh chỉnh

--- TRANG 6 ---
mô hình của chúng tôi sử dụng 7.000 mẫu này, loại bỏ nhu cầu tạo ra dữ liệu huấn luyện bổ sung với các bước lý luận trung gian thông qua LLM. Ngoài việc đánh giá phương pháp của chúng tôi trên bộ test GSM8K, chúng tôi đánh giá hiệu suất ngoài phân phối của mô hình trên ba bộ dữ liệu khác. Tất cả các bộ dữ liệu đều bao gồm các vấn đề lý luận số học ở cấp tiểu học, khác nhau bởi các thực thể mà chúng kết hợp. Hình thức tổng quát hóa ngoài phân phối này thường được phân loại là tổng quát hóa kết hợp cấp từ vựng (Liu et al., 2021). Theo SpecialFT (Fu et al., 2023), đối với mỗi bộ dữ liệu, chúng tôi sử dụng 500 mẫu làm bộ validation, sử dụng các mẫu còn lại (800 cho GSM8K, 400 cho MultiArith, 18K cho ASDiv, 500 cho SVAMP) làm bộ test.

4.2 Baseline
Trong các thí nghiệm của chúng tôi, chúng tôi so sánh phương pháp của chúng tôi với một số baseline cạnh tranh có thể được nhóm thành hai loại: (1) các mô hình ngôn ngữ lớn chung: code-davinci-002 (Chen et al., 2021) có thể có kích thước 175B hoặc hơn, LaMDA-137B (Thoppilan et al., 2022), PaLM-60B (Chowdhery et al., 2022) và UL2-20B (Tay et al., 2022), mỗi mô hình đều thể hiện khả năng lý luận mạnh mẽ trong phương pháp nhắc Chain-of-Thought. (2) các nghiên cứu đồng thời nâng cao khả năng lý luận của SLM: CoT-FT (Wei et al., 2021) trực tiếp sử dụng 7000 mẫu huấn luyện CoT từ bộ dữ liệu GSM8K để thực hiện điều chỉnh hướng dẫn, đây là một phương pháp thuần túy để nâng cao khả năng lý luận của SLM. DecomDistill (Shridhar et al., 2022) là một phương pháp dựa trên phân tích, học một phân tích ngữ nghĩa của vấn đề ban đầu thành một chuỗi các vấn đề phụ thông qua LLM. Cả Magister et al. (2022) và Ho et al. (2022) đều tinh chỉnh SLM bằng cách tạo ra dữ liệu huấn luyện với các bước lý luận trung gian thông qua LLM. SpecialFT (Fu et al., 2023) sử dụng LLM làm mô hình giáo viên và sử dụng khớp phân phối trong chưng cất kiến thức để chuyển giao khả năng lý luận từ LLM sang SLM. Cần lưu ý rằng SpecialFT sử dụng 130K mẫu huấn luyện với các bước lý luận trung gian được tạo ra bởi LLM, gần như gấp hai mươi lần kích thước bộ huấn luyện của chúng tôi.

4.3 Triển khai
Chúng tôi xem xét việc sử dụng FlanT5-XL (3B)/XXL (11B) làm xương sống của mô hình. Decomposer và Solver sử dụng các hướng dẫn khác nhau (như thể hiện trong Hình 2) nhưng chia sẻ cùng các tham số mô hình.

[Bảng 1: Phạm vi tìm kiếm cho các siêu tham số của thuật toán proximal policy optimization. Chúng tôi tô đậm các cài đặt tốt nhất.]

Theo Chung et al. (2022), chúng tôi tinh chỉnh mô hình trong 50 epoch với batch size 4096 và learning rate 5e−4. Đối với thuật toán PPO, chúng tôi sử dụng ba lớp feed-forward làm mạng chính sách và đặt số đơn vị ẩn là 1024. Hơn nữa, chúng tôi sử dụng tìm kiếm lưới để tìm các siêu tham số tốt nhất. Chi tiết tìm kiếm lưới được hiển thị trong Bảng 1. Kết quả là, chúng tôi đặt learning rate là 3e−4 và batch size là 4096 cho mạng chính sách. Chúng tôi cũng đặt ε là 0.2, k là 3 và rm là 0.3. Trong giai đoạn tối ưu hóa mạng chính sách, chúng tôi đóng băng các tham số xương sống. Tất cả kết quả baseline trừ CoT-FT (Wei et al., 2021) được ghi lại trong SpecialFT (Fu et al., 2023). Đối với CoT-FT và phương pháp của chúng tôi, chúng tôi giữ thiết lập thí nghiệm nhất quán với các baseline khác. Chúng tôi chạy tất cả các thí nghiệm trên tám GPU NVIDIA Tesla A100.

4.4 Kết quả
Bảng 2 hiển thị hiệu suất của các phương pháp khác nhau trên bốn bộ dữ liệu lý luận số học. Đầu tiên, chúng tôi thảo luận về kết quả cho DialCoT-A, DialCoT-M và DialCoT-S. Sau đó, chúng tôi so sánh phương pháp của chúng tôi với các baseline khác để chứng minh tính ưu việt của nó. Cuối cùng, chúng tôi xác thực hiệu quả của việc kết hợp thuật toán PPO dựa trên DialCoT-S thông qua một nghiên cứu ablation.

Thảo luận về Ba Biến thể DialCoT. So với CoT-FT, cả ba hình thức của DialCoT đều vượt trội trên bốn nhiệm vụ lý luận, chứng minh hiệu quả của chúng trong việc nâng cao khả năng lý luận của SLM. Cụ thể hơn, DialCoT-M hoạt động tốt hơn DialCoT-A. Điều này cho thấy SLM thiếu khả năng phân tích một vấn đề lý luận và trả lời nó cùng một lúc. DialCoT-M chỉ giải quyết một câu hỏi phụ tại một bước duy nhất, điều này giảm độ khó của nhiệm vụ và làm cho nó phù hợp hơn với SLM. DialCoT-S, so với DialCoT-M, cho thấy mức tăng hiệu suất lớn hơn, có thể được quy cho hai yếu tố: (1)

--- TRANG 7 ---
[Bảng 2: Độ chính xác (%) của các phương pháp khác nhau trên bốn nhiệm vụ lý luận. †chỉ ra rằng phương pháp sử dụng dữ liệu huấn luyện bổ sung với các bước lý luận trung gian được tạo ra thông qua LLM, trong đó SpecialFT sử dụng gần như gấp 20 lần dữ liệu huấn luyện so với phương pháp của chúng tôi. Chúng tôi tô đậm kết quả tốt nhất trên SLM (~10B).]

DialCoT-S có được thêm thông tin trung gian trước khi tạo ra các câu hỏi phụ. (2) DialCoT-S kích thích hiệu quả hơn khả năng đối thoại nhiều lượt của mô hình để thúc đẩy hiệu suất lý luận của nó.3

So sánh giữa DialCoT và Baseline. Từ Bảng 2, chúng tôi quan sát thấy rằng DialCoT-S-PPO đạt kết quả hiện đại trên SLM. Cụ thể, khi sử dụng FlanT5-XXL làm xương sống, DialCoT-S-PPO cải thiện hiệu suất trung bình trên bốn bộ dữ liệu 6.2% so với SpecialFT. Đáng chú ý, dữ liệu huấn luyện chúng tôi sử dụng chỉ bằng 1/20 so với SpecialFT, điều này rõ ràng chứng minh rằng phương pháp của chúng tôi rất hiệu quả trong việc cải thiện khả năng lý luận của SLM. Mặt khác, khi so sánh với LLM, tất cả các biến thể của DialCoT (tức là DialCoT-A/M/S/PPO) đều vượt trội hơn LaMDA-137B trung bình trên bốn bộ dữ liệu, mặc dù các tham số của phương pháp chúng tôi chỉ bằng 1/12 so với LaMDA. Điều này tiếp tục chứng thực tính ưu việt của phương pháp chúng tôi. Mặc dù vẫn có một khoảng cách đáng chú ý khi so sánh với code-davinci-002, kết quả thí nghiệm của chúng tôi chứng minh rằng có tiềm năng cho SLM đạt được khả năng lý luận cấp độ LLM thông qua các phương pháp tinh chỉnh phù hợp.4

Ablation. Nghiên cứu ablation được thực hiện để chứng minh hiệu quả của việc kết hợp thuật toán PPO dựa trên DialCoT-S. So với DialCoT-S với FlanT5-XXL, DialCoT-S-PPO đạt được sự cải thiện gần 2%, xác nhận hiệu quả của việc sử dụng thuật toán PPO để lựa chọn đường dẫn lý luận tối ưu. Ngoài ra, chúng tôi quan sát thấy rằng khi sử dụng FlanT5-XL làm xương sống, mức tăng hiệu suất do thuật toán PPO mang lại là 1.4%, thấp hơn hiệu suất trên FlanT5-XXL. Điều này có thể được quy cho tính đa dạng thấp hơn trong các phản hồi đa dạng được tạo ra bởi mô hình nhỏ hơn.

4.5 Phân tích
Kích thước Mô hình khác nhau. Chúng tôi mở rộng phương pháp của chúng tôi cho các xương sống nhỏ hơn, bao gồm FlanT5-Base (250M) và FlanT5-Large (760M), trên các bộ dữ liệu GSM8K và MultiArith. Kết quả thí nghiệm của chúng tôi được minh họa trong Hình 4. So với FlanT5 ban đầu, phương pháp của chúng tôi cải thiện hiệu suất của mô hình trên các nhiệm vụ lý luận trên các kích thước mô hình khác nhau, khẳng định hiệu quả của phương pháp chúng tôi cho các kích thước mô hình khác nhau. Đáng chú ý, chúng tôi quan sát thấy rằng

3Một cuộc thảo luận chi tiết hơn về ba biến thể DialCoT có thể được tìm thấy trong Phụ lục D.
4So sánh thí nghiệm chi tiết giữa DialCoT và SelfAsk có thể được tìm thấy trong Phụ lục A.

--- TRANG 8 ---
Hình 4: Kết quả dưới các kích thước mô hình khác nhau trên các bộ dữ liệu GSM8K và MultiArith. FlanT5 chỉ kết quả sử dụng Few-shot CoT (Wei et al., 2022) trên xương sống (Chung et al., 2022). Các phương pháp của chúng tôi đạt được cải thiện hiệu suất trên tất cả các kích thước mô hình.

phương pháp của chúng tôi mang lại mức tăng hiệu suất lớn hơn trên các kích thước mô hình lớn hơn, tương tự như kết quả của Chung et al. (2022). Điều này có thể được quy cho những khả năng mạnh mẽ hơn mà các mô hình lớn hơn có được trong quá trình tiền huấn luyện, làm cho chúng dễ dàng được kích thích hơn.

Kiến trúc Mô hình khác nhau. Để đánh giá khả năng tổng quát hóa của DialCoT trên các LM với kiến trúc khác nhau, ngoài LM encoder-decoder (ví dụ: FlanT5), chúng tôi thực hiện thí nghiệm sử dụng LM chỉ decoder (ví dụ: LLaMA-7B (Touvron et al., 2023)) làm xương sống của phương pháp chúng tôi trên các bộ dữ liệu GSM8K và MultiArith. Kết quả được minh họa trong Hình 5. Như có thể thấy từ hình, tất cả các phương pháp của chúng tôi đều đạt được mức tăng hiệu suất đáng kể so với CoT-FT, đặc biệt là DialCoT-S và DialCoT-S-PPO. Điều này chứng minh rằng phương pháp của chúng tôi có thể áp dụng cho SLM với các kiến trúc khác nhau, không chỉ hiệu quả trên các LM encoder-decoder. Hơn nữa, chúng tôi quan sát thấy rằng DialCoT-A hoạt động tốt hơn DialCoT-M trên bộ dữ liệu MultiArith, khác với kết quả dựa trên FlanT5 (như thể hiện trong Hình 4). Điều này cho thấy rằng hình thức phù hợp nhất của DialCoT có thể khác nhau đối với các SLM khác nhau, có thể liên quan đến kiến trúc mô hình và corpus tiền huấn luyện.

Hình 5: Kết quả sử dụng LLaMA-7B (Touvron et al., 2023) làm xương sống trên các bộ dữ liệu GSM8K và MultiArith. Phương pháp của chúng tôi đạt được mức tăng hiệu suất đáng kể trên các LM chỉ decoder.

Hình 6: Ảnh hưởng của siêu tham số k đến hiệu suất mô hình trên các bộ dữ liệu GSM8K và MultiArith.

Ảnh hưởng của Siêu tham số k. Chúng tôi đặt mô hình xuất ra các phản hồi top-k với xác suất cao nhất trong mỗi bước đối thoại thông qua tìm kiếm chùm. Nói cách khác, k đại diện cho kích thước của không gian hành động, chỉ ra rằng chúng tôi có thể lựa chọn phản hồi tối ưu từ k phản hồi khác nhau trong mỗi

--- TRANG 9 ---
bước. Hình 6 minh họa ảnh hưởng của k đến hiệu suất mô hình trên các bộ dữ liệu GSM8K và MultiArith. Cụ thể, mô hình có thể đạt hiệu suất tối ưu khi k được đặt là 3 hoặc 4. Khi k quá nhỏ, số lượng đường dẫn lý luận chúng ta có thể chọn quá hạn chế, ngăn chúng ta đạt được hiệu suất tối ưu. Ngược lại, khi k quá lớn, không gian các đường dẫn lý luận có sẵn quá lớn, có thể gây ra nhiễu và khiến mô hình khó học cách chọn đường dẫn lý luận tối ưu.

5 Kết luận
Trong bài báo này, chúng tôi đã khám phá các chiến lược để thúc đẩy khả năng lý luận của SLM và đề xuất DialCoT, nhằm mục đích tạo ra các bước lý luận trung gian theo định dạng đối thoại dẫn đến câu trả lời cuối cùng. Cụ thể, chúng tôi đã thiết kế hai vai trò cho mô hình, đó là Decomposer và Solver. Decomposer chịu trách nhiệm chia nhỏ câu hỏi thành nhiều câu hỏi phụ, trong khi Solver có nhiệm vụ giải quyết các câu hỏi phụ. Chúng tham gia vào đối thoại để đạt đến câu trả lời cuối cùng. Chúng tôi đã giới thiệu ba định dạng đối thoại khác nhau: DialCoT-A (All at once), DialCoT-M (Mixed) và DialCoT-S (Step by step). Hơn nữa, chúng tôi đã kết hợp thuật toán PPO vào DialCoT-S để cho phép mô hình chọn đường dẫn lý luận tối ưu giữa nhiều tùy chọn, từ đó cải thiện thêm hiệu suất của nó trên các nhiệm vụ lý luận. Chúng tôi đã thực hiện các thí nghiệm rộng rãi trên bốn bộ dữ liệu lý luận số học và kết quả thí nghiệm chứng minh hiệu quả của phương pháp chúng tôi. Nghiên cứu trong tương lai chủ yếu liên quan đến việc mở rộng phương pháp của chúng tôi sang các loại nhiệm vụ lý luận khác, chẳng hạn như lý luận thông thường và lý luận ký hiệu. Ngoài ra, chúng tôi sẽ khám phá các phương pháp phân tích khác hoặc các phương pháp học tăng cường khác để tối ưu hóa đường dẫn lý luận của SLM.

Hạn chế
Chúng tôi thực hiện thí nghiệm trên bốn nhiệm vụ lý luận số học, chứng minh hiệu quả của DialCoT. Tuy nhiên, vì mô hình phần thưởng của chúng tôi được thiết kế đặc biệt cho lý luận số học, những sửa đổi là cần thiết để áp dụng phương pháp của chúng tôi cho lý luận thông thường hoặc lý luận ký hiệu. Điều này đưa ra một hạn chế đối với khả năng áp dụng rộng rãi hơn của phương pháp chúng tôi. Chúng tôi có kế hoạch mở rộng DialCoT cho một phạm vi rộng hơn các nhiệm vụ lý luận trong tương lai. Mặt khác, DialCoT tập trung đặc biệt vào việc nâng cao khả năng lý luận của SLM. Do hạn chế về tài nguyên tính toán, chúng tôi không thực hiện thí nghiệm trên các mô hình ngôn ngữ quy mô lớn hơn (≥20B), do đó khả năng áp dụng của phương pháp chúng tôi cho LLM vẫn chưa được xác định. Chúng tôi sẽ tiếp tục khám phá hiệu suất của DialCoT trên các mô hình ngôn ngữ quy mô lớn hơn trong nghiên cứu tương lai.

Tuyên bố Đạo đức
Phương pháp đề xuất không có rủi ro tiềm ẩn rõ ràng nào. Tất cả các hiện vật khoa học được sử dụng/tạo ra đều được trích dẫn/cấp phép đúng cách, và việc sử dụng phù hợp với mục đích dự định của chúng. Ngoài ra, chúng tôi mở mã nguồn và các siêu tham số của mình để hỗ trợ tái tạo trong tương lai mà không cần chi phí năng lượng lặp lại.

Lời cảm ơn
Công trình này đã được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Nghị định số U1911203, Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Nghị định số 62377012 và Quỹ Nghiên cứu Cơ bản cho các Trường Đại học Trung ương theo số hiệu YBNLTS2023-015.

[Các tài liệu tham khảo được liệt kê từ trang 9-12]

--- TRANG 12 ---
A So sánh giữa DialCoT và SelfAsk
Chúng tôi thêm thảo luận chi tiết hơn về so sánh với SelfAsk. Self-Ask (Press et al., 2022) tự hỏi các câu hỏi tiếp theo một cách rõ ràng trước khi trả lời câu hỏi ban đầu để thực hiện các nhiệm vụ lý luận kết hợp. Vui lòng tham khảo Bảng 3 để so sánh giữa SelfAsk và các phương pháp của chúng tôi.

Từ bảng, chúng ta thấy rằng: (1) SelfAsk được thiết kế cho phương pháp học trong ngữ cảnh mà không cần tinh chỉnh, trong khi DialCoT là một phương pháp dựa trên tinh chỉnh. (2) Ngay cả khi tinh chỉnh có thể được áp dụng cho SelfAsk về mặt lý thuyết, cách định dạng tinh chỉnh vẫn chưa được khám phá. Chúng tôi cung cấp một cách mới để tận dụng hai nhiệm vụ tùy chỉnh (phân tích vấn đề và giải quyết vấn đề) thông qua tinh chỉnh các hướng dẫn định hướng nhiệm vụ trên cùng một mô hình (SLM). Do đó, chúng tôi tin rằng mức tăng hiệu suất chủ yếu đến từ cách phân tích và giải quyết các câu hỏi phụ thông qua mô hình được tinh chỉnh với các hướng dẫn tùy chỉnh. Các hướng dẫn bổ sung tự nhiên đi kèm với giải pháp được đề xuất. Chúng sẽ hướng dẫn mô hình đóng vai trò chuyên dụng trong quá trình tinh chỉnh hướng dẫn. Tuy nhiên, chúng tôi không nghĩ rằng chúng là những yếu tố chính.

Hơn nữa, chúng tôi thực hiện các thí nghiệm bổ sung (SelfAsk) trên bộ dữ liệu GSM8K sử dụng Flan-T5-XXL. Kết quả được minh họa trong Bảng 4. Từ bảng, chúng ta có thể rút ra những kết luận sau:

Thứ nhất, trong môi trường tinh chỉnh, SelfAsk cải thiện so với Standard CoT 5.5% với tinh chỉnh tất cả cùng một lúc và 13.2% với tinh chỉnh tuần tự. Chúng tôi tin rằng những cải thiện này xuất phát từ phân tích vấn đề. Hơn nữa, SelfAsk với tinh chỉnh tuần tự cải thiện 7.7% so với SelfAsk với tinh chỉnh tất cả cùng một lúc. Điều này một lần nữa xác nhận kết luận trước đây của chúng tôi rằng phân tích vấn đề tuần tự hiệu quả hơn phân tích chúng tất cả cùng một lúc. Hơn nữa, DialCoT-S cải thiện 5.9% so với SelfAsk với tinh chỉnh tuần tự. Chúng tôi quy cải thiện bổ sung này cho tinh chỉnh với các hướng dẫn khác nhau được thiết kế cho các nhiệm vụ cụ thể. So với SelfAsk với tinh chỉnh tuần tự, DialCoT-S có các hướng dẫn rõ ràng và độc lập hơn cho cả phân tích vấn đề và giải quyết vấn đề.

Thứ hai, so với các phương pháp dựa trên tinh chỉnh, các phương pháp không có tinh chỉnh hoạt động kém, chỉ ra rằng tinh chỉnh là rất quan trọng đối với SLM. Đồng thời, chúng tôi phát hiện rằng cả SelfAsk và DialCoT-S đều trải qua sự sụt giảm hiệu suất trong môi trường này so với Standard CoT. Điều này có thể do Flan-T5 được huấn luyện với một số dữ liệu huấn luyện có định dạng standard-CoT hoặc do khả năng tuân theo hướng dẫn yếu hơn của SLM, nơi các hướng dẫn phức tạp tăng độ khó của nhiệm vụ.

Đối với cấu trúc encoder-decoder, tức là T5, sự khác biệt giữa SelfAsk-A và SelfAsk-S là đáng kể, do thực tế chú ý hai chiều trong đầu vào. Đối với cấu trúc chỉ decoder, sự khác biệt thực sự rất tinh tế. Bảng 6 hiển thị kết quả của các thí nghiệm bổ sung (SelfAsk) trên GSM8K sử dụng LLaMA-7B. SelfAsk-A xuất ra tất cả các câu hỏi trung gian, câu trả lời và từ kết nối giữa chúng như "Follow up" và "Intermediate answer", trong khi tinh chỉnh tuần tự tập trung vào việc xuất ra câu hỏi hoặc câu trả lời trung gian, và không bao gồm loss của từ kết nối, khuyến khích mô hình tập trung nhiều hơn vào phần quan trọng nhất của việc học. Chúng tôi suy đoán rằng sự tinh tế này mang lại sự cải thiện.

[Các bảng và phụ lục tiếp theo được dịch tương tự...]
