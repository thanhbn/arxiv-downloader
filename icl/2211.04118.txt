# 2211.04118.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2211.04118.pdf
# File size: 1156279 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CONSPROMPT: EXPLOITING CONTRASTIVE SAMPLES FOR FEW-SHOT PROMPT
LEARNING
Jinta Weng†Yifan Deng†Donghao Li†Hao You†Yue Hu†⋆Heyan Huang††⋆
†Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
†School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
††Beijing Institute of Technology, School of Computer Science and Technology, Beijing, China
††Southeast Academy of Information Technology, Putian, China
ABSTRACT
Prompt has become an effective linguistic tool for utilizing
pre-trained language models. However, in few-shot sce-
narios, subtle changes of prompt’s design always make the
result widely different, and the prompt learning methods
are also easy to overfit the limited samples. To alleviate
this, we explore utilizing suitable contrastive samples and
multi-degree contrastive learning methods to improve the ro-
bustness of prompt’s representation. Therefore, the proposed
Consprompt combined with prompt encoding network, con-
trastive sampling modules, and contrastive scoring modules,
is introduced to realize differential contrastive learning. Our
results exhibit the state-of-the-art performance in different
few-shot settings, and the ablation experiments also certify
the effectiveness of utilizing multi-degree contrastive learning
in prompt-based fine-tuning process.
Index Terms —Prompt learning, Pre-trained language
model, contrastive learning, few-shot learning
1. INTRODUCTION
With the exponential growth of Large language models(LLMs)
and prompting strategies, pre-trained language models now
could serve as a huge knowledge base or superior linguistic
tool for few-shot learning [1], like ChatGPT and LLAMA.
To activate the existing knowledge embedded in PLMs, re-
searches are mainly concentrating on different prompting
strategies and prompt learning methods. [2], and zhong et al.
put forward a OptiPrompt model for factual probing based on
soft-prompt embedding [3]. In terms of training strategies,
prompt learning could classify into the fine-tuning of prefix-
prompt and fine-tuning of independent prompt encoder, such
as P-tuning[4], Prefix-tuning[5], Auto-prompt learning[6],
and Prompt-tuning[7].
However, subtle changes of prompt’s design always make
the result widely different, and the prompt learning process is
also easy to over-fit the limited samples. Therefore, utilizing
different cognitive learning abilities and prompting strategies
to activate LLMs in few-shot settings is still a considerablequestion [8, 4, 7]. In addition to the prompting strategies, hu-
man can also use different learning rules to realize knowledge
activation based on the few samples[9], like contrastive learn-
ing of different prompt and different samples. Motivated by
this, we exploit how to use fewer samples and multiple con-
trastive learning methods in prompt-based fine-tuning mod-
els.
As shown in Fig. 1, it reveals the token and sentence-
level attention mechanism of transformer-based model, which
shows the effectiveness of LLMs can be enhanced by more
accurate token-level attention and contrastive learning meth-
ods. Therefore, we utilize the prompt-level and the batch-
level contrastive information of tokens to strengthen few-shot
prompt-based learning task, and two negative sampling strate-
gies and SBERT [10] model are also used to facilitate the
contrastive prompt learning model – ConsPrompt. Our re-
sult shows its effectiveness of multiple contrastive learning in
using prompt-based fine-tuning works. And the robustness
of prompt-based learning can empower with suitable negative
sampling strategies. The main contribution is:
• We proposed a novel contrastive prompt model Con-
sPrompt for increasing the prompt’s distinguishing
ability on prompt-level and batch-level samples.
• Two sampling strategies and the SimCLR contrastive
learning method, are introduced to alleviate the widespread
overfitting phenomenons in prompt learning process.
• The results demonstrate the state-of-the-art perfor-
mance and show its k-shot robustness on five represen-
tative few-shot tasks.
2. MODEL
As depicted in fig. 2, the ConsPrompt are constituted by
prompt-based Encoding Network, contrastive sampling mod-
ule, and contrastive learning method ConsPrompt . In prompt
encoding network (§2.1), the input would be encoded to aarXiv:2211.04118v3  [cs.CL]  12 Mar 2024

--- PAGE 2 ---
Fig. 1 : The word and sentence distribution in attention-based
model. We plotted contrastive learning process with the yel-
low row, and it reveals negative samples could contribute to
the LLM’s fine-tuning process and text representation step by
step.
prompting input, and then predict the mapping tokens by
prompt-based fine-tuning method. The core component of
ConsPrompt is utilizing two contrast-aware prompt learning
modules, which containing the prompt-level and the batch-
level contrastive learning strategies. Both learning strategies
containing a specif contrastive sampling module (§2.2) and
learning encoder (§2.3. In contrastive sampling modules, the
prompting input would extract negative and positive samples
by similarity-based or label-based sampling methods, while
these samples are subsequently used to construct the support-
ing set for multiple contrastive learning (§2.3). Finally, we
use a joint loss to fine-tune the prompt encoding network and
two contrastive learning encoders.
2.1. Prompt Encoding Network
We take prompt-based fine-tuning model [11] as the baseline
of our prompt encoding Network. In this method, a tem-
plate Tconsisting of textual tokens and mask token would
be defined first. Consequently, the original input xiwould
then be transformed to xi. For example, if the template is
adding It is [MASK ]after origin input, the prompting input
xiwould be:
xi=T(xi) =xi.It is [MASK ] (1)
, where the hidden value of [MASK] token is used to generate
the word distribution over PLMs vocabulary consequently.
Next, a label mapping is defined to map each label to
specif token. The detail formulation of label mapping is de-
fined by:
F(y) :yt→vt, yt∈Y, vt∈V (2)
,where vtis one of tokens in PLMs vocabulary, and sthe label
index. And the final prediction would formulate in following
equation:p(yt|xi) =p(vt|h[MASK ])⇒p(vt|W·h[MASK ])
⇒p(vt|hvt) = lnexp(hvt)
P|Y|
k=1exp(hvk)(3)
where his the hidden representation of last layer in specif
token, the Wis a vocabulary projector with the size of |h×V|
,vused to represent each token of the label mapping.
The fine-tuning loss of prompt encoding network is:
LCE=−1
NX
i|Y|X
t=1yilogp(yt|xi) (4)
where iis the index of the training pair (xi, yi)and —Y— is
the total number of labels.
2.2. Contrastive Sampling Module
The essence of contrastive learning is to learn the differences
between limited positive and negative samples. Therefore, a
suitable sampling strategies for selecting positive and nega-
tive samples is essential to develop contrastive learning.
In the sentence representation of contrastive learning,
different prompts(similar as the context knowledge) always
cause different sentence representation, and different compar-
ative objects also result in differential comparative loss. We
thus use two sampling strategy to construct the support set Sq:
prompt-level sampling and batch-level sampling module. In
prompt-level sampling, we use different prompt template to
construct different querying inputs for constructing the sup-
porting set. In batch-level sampling, we use different samples
of current batch to construct the support set.
We then calculate the cosine similarity between the repre-
sentation of current instance qand each supporting set Sqto
choose the positive and negative samples. The detailed for-
mulation is:
MaxRank (sim(Msbert(q), Msbert(Sq))) (5)
where n is the number of supporting set Sqused to gener-
ate contrastive samples, and qis the query sample. The final
support set would be ordered by the max similarity.
After generating the similarity-based order in Sq, the rep-
resentation of the positive samples and negative samples still
use the same PLM of prompt encoding network.
sn
q=PLM (Sn
q) (6)
where sn
qis the PLM’s representation of Sn
q.
What is more, we only use one positive sample select-
ing by the highest similarity of support set, while negative
samples are selected by the lower similarity among in other
samples with different labels.

--- PAGE 3 ---
Fig. 2 : The model of ConsPrompt integrating prompt encording network and contrastive sampling module and scoring module
2.3. Contrastive Scoring Module
Contrastive scoring modules are core component using to
learn the representation of sample distinction. After above-
mentioned sampling operation, we use same PLM of prompt
encoding network to representing each samples.
SBC,PC (i, j) =−logexp(si,j/τ)
PNneg
k1[k̸=i]exp(si,k/τ)(7)
where kis the index of negative sample set, τis a controllable
temperature parameter, S BCand S PCis the batch-level and
prompt-level supporting sets.
We utilize the SimCLR [12] to compute two comparative
losses, the S BCand the S PCof prompt-level and batch-level
sampling module. Owing to the difference negative support
set of the comparative objects, we also switch their position
inS(i, j)to calculate another inverted loss. The final loss of
contrastive learning modules are thus defined as:
LBC=1
NP
N[SBC(i, j) +SBC(j, i)]
LPC=1
NP
N[SBC(i, j) +SPC(j, i)](8)
where N is the samples number of prompt-level or batch-level
supporting sets. Considering the loss of contrastive ability
and prompt mechanism, the final loss is formulated as:
L=LCE+t∗LBC+a∗LPC (9)
where tandais the hyper-parameter to determine the ratio of
comparative learning loss.
3. EXPERIMENT
In this section, we introduce our using datasets, experimental
setup, the main results of our experiments and its analyses.3.1. Few-shot Experiment setups
We use RoBERTa-large as our pre-training language model.
Our experiment are developed in NVIDIA V100 32GB (also
could run in 1080ti with low batch size). In the training pro-
cess, we develop many experiments on different batch sizes
bs=4,8,16 , learning rate lr=1e-5,2e-5,5e-5 .
We evaluate our proposed model ConsPrompt on few-
shot glue tasks, including question-answering task (TREC
dataset), emotion classification tasks (the sst-2 and sst-5
datasets), and text entailment tasks(QNLI and SNLI datasets).
To satisfy the few-shot learning setting, we pick five different
K-shot sub-datasets and each sub-dataset is constructed by
K=16 training pairs on each type of labels[11]. Also, we use
the mean score and variance of the prediction result on differ-
ent subsets instead of the highest result to control the fairness.
In the setting of contrastive learning, we select all instances
except query instance q to combine the initial supporting set.
In order to decree the calculation within initial supporting
samples, we set the filtering ratio of the support set to 0.5. We
setτ= 0.07 to realize the smoothing of loss, and the ratio t
andaof contrastive learning are 0.5. Our source is available
at https://github.com/Nagin-Kim/cosprompt.
3.2. Baselines
We compare with the Majority method (select the major-
ity class as prediction), fine-tuning method[15], prompt-
based zero-shot learning, GPT3-in-context-learning[16],
prompt learning modules(prefix-tuning, p-tuning), LMBFF
model[11]), and two contrastive prompt framework, the CP-
tuning([13], only certificate in binary classification tasks)
and [14](label-based sampling method). Based on different
sampling strategies, our ConsPrompt is divide into two label-
based and sim-based ConsPrompt. In ConsPrompt(-sim), the
negative and positive samples are directly sampled based on
the similarity between the support sets and query instance,
while ConsPrompt(-label) are label-based sampling strategy.

--- PAGE 4 ---
Baselines TREC (acc) SNIL (acc) QNLI (acc) SST-5 (acc) SST-2 (acc)
Majority 18.8 33.8 49.5 50.9 23.1
prompt-based zero-shot learning 32.0 49.5 50.8 35.0 83.6
GPT3-in-context-learning 26.2(2.4) 47.1(0.6) 53.8(0.4) 30.6(0.9) 84.8
fine-tuning 27.2(1.4) 48.4(4.8) 60.2(6.5) 43.9(2.0) 81.4(3.8)
Prefix-tuning 36.0(1.1) 33.5(3.9) 54.5(2.2) 46.1(1.3) 88.1(2.3)
P-tuning 40.2(1.3) 37.5(1.6) 57.6(3.9) 32.1(3.1) 90.1(1.2)
LMBFF 84.8(5.1) 77.1(3.9) 64.5(4.2) 46.1(1.3) 92.1(1.1)
CP-Tuning – – 69.22 – 93.35
Jian et al., 2022 83.3 (1.5) 69.9 (2.4) 66.4 (3.5) 49.5 (1.1) 90.6 (0.1)
ConsPrompt(-sim) 87.5 (2.2) 77.3 (3.6) 72.0 (3.0) 47.2 (3.1) 95.0(2.8)
ConsPrompt(-label) 86.8 (2.8) 76.2 (3.8) 71.9 (2.7) 47.5 (2.4) 93.1 (1.4)
Table 1 : The main result of The ConsPrompt. We select the baselines from prompt learning modules(prefix-tuning, p-tuning),
LMBFF model[11]), and two contrastive prompt framework, the CP-tuning([13] and the Jiang’s [14] work.
t,aAverage Variance Median
(acc.) (+std) (acc.)
0.1 75.5 2.7 76.0
0.5 75.7 2.4 76.0
1.0 75.5 3.2 76.1
20 77.3 3.6 78.9
Ensemble 77.0 2.9 77.7
Table 2 : The comparative experiment using different ratios of
comparative scoring module. We choose ConsPrompt(-sim)
as the baseline and change the t and a on 0.1, 0.5, 1.0 and 20.
Tasks Fine-tuning Our Sim-based Our Label-based
K-shot Acc Acc(+std) Mean. Acc (+std) Mean.
8 16.5 47.1 (1.6) 47.3 44.5 (2.1) 44.4
16 19.8 47.2 (3.1) 46.9 47.5 (2.4) 46.5
32 22.4 48.8 (1.6) 48.9 48.3 (1.3) 48.6
64 28.2 51.1 (1.0) 50.9 50.8 (0.9) 50.6
128 35.9 51.8 (2.0) 52.4 51.5 (1.1) 51.4
160 42.1 51.8 (1.3) 51.9 51.7 (0.8) 51.8
Table 3 : The result using different K sampling strategies. K
is the samples of per classes in current datasets.
3.3. Main Result
The main result of ConsPrompt is depicted in Tab. 1.
Comparing with existing baselines, the ConsPrompt model
achieves the state-of-the-art results. It reveals that the prompt
learning process indeed increases the model’s discrimination
on negative and positive samples. What is more, the effective-
ness of ConsPrompt(-sim) is better than ConsPrompt(-label)
in TREC, SNLI and SST tasks. Since the sampling samples
from ConsPrompt(-sim) use a similarity-first viewpoint on
generating negative set from SBERT, it seems that we need
to integrate more information about the fine-grained seman-
tic distinction for few-shot prompt learning, instead of the
coarse label distinction. In addition, the variances result of
the ConsPrompt are generally lower than other baselines,
which shows its greater robustness to some extent.
3.4. Analysis
We explore different temperatures and K-shot settings to cer-
tificate the Consprompt’s necessity and effectiveness.3.4.1. Different Ratio of Contrastive Loss.
In order to explore the effectiveness of the contrastive learn-
ing module, we choose SNLI task and set different ratio of t
andato control the loss from contrastive scoring module. As
the result shown in Tab. 2, with the increase of value tin com-
parative learning module, the proposed ConsPrompt is able to
receive more gain from the contrastive learning module, since
it does not make any big gains while setting the lower t values,
and higher t value can empower the original prompt encoding
network. In addition, the integration result of different t set-
tings reveals the necessity of comparative learning module.
3.4.2. The K-shot Robustness of ConsPrompt.
We also consider the influence of different sampling numbers
in few-shot datasets. We set the value K to 8,16,32,64,128,160
on SST-5 tasks. As the result shown in Tab. 3, the higher K-
shot setting can bring more effectiveness on the accuracy and
robustness. The ConsPrompt (Sim-based) are effective on
higher K-setting experiments, while label-based ConsPrompt
is effective in 16-shot settings. It reveals the similarity-based
sampling strategy is more efficient than label-based strategy.
Also, with the increase of K, the benefit from the training
corpus is decreasing, which shows the consprompt are more
suggestive for few-shot experiments.
4. CONCLUSION
Combining prompt learning and unsupervised contrastive
learning, we propose an efficient prompt model ConsPrompt
for few-shot scenario. The ConsPrompt integrating prompt-
ing encoding network, Contrastive sampling module and
contrastive scoring module, is able to realize multiple learn-
ing and alleviate the over-fit problem in prompt design .The
effectiveness of ConsPrompt on few-shot learning tasks (only
16 samples per class) shows state-of-the-art performances
and more sample-degree robustness. Our future work will
focus on improving the controllability of comparison sample
selection.

--- PAGE 5 ---
5. ACKNOWLEDGMENT
This work is supported by the National Natural Science Foun-
dation of China (Grant No.U21B2009) and the Science Foun-
dation of (Grant No.E110151101, E250471101).
6. REFERENCES
[1] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al., “Language models
are unsupervised multitask learners,” OpenAI blog , vol.
1, no. 8, pp. 9, 2019.
[2] Andrea Madotto, Zhaojiang Lin, Genta Indra Winata,
and Pascale Fung, “Few-shot bot: Prompt-based learn-
ing for dialogue systems,” CoRR , vol. abs/2110.08118,
2021.
[3] Zexuan Zhong, Dan Friedman, and Danqi Chen, “Fac-
tual probing is [mask]: Learning vs. learning to recall,”
2021.
[4] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yu-
jie Qian, Zhilin Yang, and Jie Tang, “Gpt understands,
too,” arXiv preprint arXiv:2103.10385 , 2021.
[5] Xiang Lisa Li and Percy Liang, “Prefix-tuning: Opti-
mizing continuous prompts for generation,” in Proceed-
ings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Vol-
ume 1: Long Papers) , Online, Aug. 2021, pp. 4582–
4597, Association for Computational Linguistics.
[6] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric
Wallace, and Sameer Singh, “Autoprompt: Eliciting
knowledge from language models with automatically
generated prompts,” arXiv preprint arXiv:2010.15980 ,
2020.
[7] Timo Schick and Hinrich Sch ¨utze, “Exploiting cloze-
questions for few-shot text classification and natural lan-
guage inference,” in Proceedings of the 16th Conference
of the European Chapter of the Association for Compu-
tational Linguistics: Main Volume , Online, Apr. 2021,
pp. 255–269, Association for Computational Linguis-
tics.
[8] Adam Roberts, Colin Raffel, and Noam Shazeer, “How
much knowledge can you pack into the parameters of a
language model?,” in Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP) , Online, Nov. 2020, pp. 5418–5426,
Association for Computational Linguistics.[9] Yangyan Xu, Fangfang Yuan, Cong Cao, Majing Su,
Yuhai Lu, and Yanbing Liu, “A contrastive self-
distillation bert with kernel alignment-based inference,”
inInternational Conference on Computational Science .
Springer, 2023, pp. 553–565.
[10] Nils Reimers and Iryna Gurevych, “Sentence-bert: Sen-
tence embeddings using siamese bert-networks,” in Pro-
ceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Process-
ing (EMNLP-IJCNLP) , 2019, pp. 3982–3992.
[11] Tianyu Gao, Adam Fisch, and Danqi Chen, “Making
pre-trained language models better few-shot learners,”
arXiv preprint arXiv:2012.15723 , 2020.
[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and
Geoffrey Hinton, “A simple framework for contrastive
learning of visual representations,” in International con-
ference on machine learning . PMLR, 2020, pp. 1597–
1607.
[13] Ziyun Xu, Chengyu Wang, Minghui Qiu, Fuli Luo,
Runxin Xu, Songfang Huang, and Jun Huang, “Mak-
ing pre-trained language models end-to-end few-shot
learners with contrastive prompt tuning,” arXiv preprint
arXiv:2204.00166 , 2022.
[14] Yiren Jian, Chongyang Gao, and Soroush V osoughi,
“Contrastive learning for prompt-based few-shot lan-
guage learners,” in Proceedings of the 2022 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies , Seattle, United States, July 2022, pp. 5577–
5587, Association for Computational Linguistics.
[15] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov, “Roberta: A ro-
bustly optimized bert pretraining approach,” ArXiv , vol.
abs/1907.11692, 2019.
[16] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen
Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei, “Language models are few-shot
learners,” ArXiv , vol. abs/2005.14165, 2020.
