# Viết lại truy vấn nhận biết ngữ cảnh cho bộ xếp hạng văn bản sử dụng LLM

Abhijit Anand
aanand@l3s.de
Viện Nghiên cứu L3S
Đức

Venktesh V
v.Viswanathan-1@tudelft.nl
Đại học Công nghệ Delft
Hà Lan

Vinay Setty
vsetty@acm.org
Đại học Stavanger
Na Uy

Avishek Anand
Avishek.Anand@tudelft.nl
Đại học Công nghệ Delft
Hà Lan

TÓM TẮT
Viết lại truy vấn đề cập đến một họ phương pháp đã được thiết lập được áp dụng cho các truy vấn không được chỉ định đầy đủ và mơ hồ để khắc phục vấn đề không khớp từ vựng trong xếp hạng tài liệu. Các truy vấn thường được viết lại trong thời gian xử lý truy vấn để mô hình hóa truy vấn tốt hơn cho bộ xếp hạng phía sau. Với sự ra đời của các mô hình ngôn ngữ lớn (LLM), đã có những nghiên cứu ban đầu về việc sử dụng các phương pháp sinh để tạo ra các tài liệu giả để giải quyết khoảng cách từ vựng cố hữu này. Trong công trình này, chúng tôi phân tích tính hữu ích của LLM để cải thiện việc viết lại truy vấn cho các nhiệm vụ xếp hạng văn bản. Chúng tôi thấy rằng có hai hạn chế cố hữu khi sử dụng LLM làm bộ viết lại truy vấn - trôi dạt khái niệm khi chỉ sử dụng truy vấn làm lời nhắc và chi phí suy luận lớn trong quá trình xử lý truy vấn. Chúng tôi áp dụng một phương pháp đơn giản nhưng hiệu quả đáng ngạc nhiên được gọi là viết lại truy vấn nhận biết ngữ cảnh (CAR) để tận dụng lợi ích của LLM trong hiểu truy vấn. Trước tiên, chúng tôi viết lại các truy vấn mơ hồ trong huấn luyện bằng cách nhắc LLM có nhận biết ngữ cảnh, nơi chúng tôi chỉ sử dụng các tài liệu liên quan làm ngữ cảnh. Không giống như các phương pháp hiện có, chúng tôi chỉ sử dụng việc viết lại truy vấn dựa trên LLM trong giai đoạn huấn luyện. Cuối cùng, một bộ xếp hạng được tinh chỉnh trên các truy vấn đã viết lại thay vì các truy vấn gốc trong quá trình huấn luyện. Trong các thí nghiệm mở rộng của chúng tôi, chúng tôi thấy rằng việc tinh chỉnh bộ xếp hạng sử dụng các truy vấn đã viết lại mang lại cải thiện đáng kể lên đến 33% trên nhiệm vụ xếp hạng đoạn văn và lên đến 28% trên nhiệm vụ xếp hạng tài liệu khi so sánh với hiệu suất cơ sở sử dụng truy vấn gốc.

KHÁI NIỆM CCS
• Hệ thống thông tin → Mô hình truy xuất và xếp hạng.

TỪ KHÓA
viết lại truy vấn, hợp nhất xếp hạng, hiệu suất xếp hạng

1 GIỚI THIỆU
Sự không khớp từ vựng giữa truy vấn của người dùng và tài liệu là một vấn đề nổi tiếng trong lĩnh vực Truy xuất Thông tin (IR). Nhu cầu thông tin của người dùng thường được đại diện dưới dạng truy vấn từ khóa có thể mơ hồ và có thể không tương tự về mặt từ vựng với tài liệu. Các truy vấn có thể không được chỉ định đầy đủ, khiến việc hiểu ý định người dùng trở nên khó khăn, điều này ảnh hưởng đến hiệu suất truy xuất phía sau. Ví dụ, truy vấn "define sri" có thể đề cập đến "từ tiếng Sanskrit sri" hoặc "Đầu tư Có trách nhiệm Xã hội". Để phân biệt nhu cầu thông tin của người dùng, nhiều phương pháp đã được đề xuất để giải quyết khoảng cách từ vựng.

Các phương pháp cổ điển như cơ chế phản hồi liên quan giả [10,28] mở rộng truy vấn gốc bằng từ khóa từ các kết quả xếp hạng cao nhất cho truy vấn gốc. Các phương pháp dựa trên thuật ngữ thay thế đại diện cho truy vấn trong không gian vector liên tục, sau đó là tìm kiếm KNN để mở rộng thuật ngữ truy vấn [18,51]. Tồn tại các phương pháp dựa trên thuật ngữ khác sử dụng mô hình seq2seq [19] để tạo ra các bản viết lại truy vấn. Tuy nhiên, tất cả các phương pháp trên đều là phương pháp dựa trên thuật ngữ cải thiện truy xuất nhưng không nhất thiết trong xếp hạng tài liệu.

Vượt ra ngoài từ khóa, các nhà nghiên cứu đã khám phá khả năng tạo câu hỏi ngôn ngữ tự nhiên để làm rõ truy vấn [35] hoặc diễn đạt lại truy vấn gốc sử dụng mô hình sinh sâu cho các công thức thay thế của truy vấn gốc [53]. Gần đây hơn, các phương pháp mở rộng tài liệu đã được áp dụng để cải thiện hiệu suất truy xuất. Doc2Query [33] sử dụng mô hình neural để dự đoán truy vấn liên quan đến tài liệu và tăng cường biểu diễn tài liệu. Đặc biệt với sự ra đời của các mô hình ngôn ngữ lớn hoặc LLM [27,40], các phương pháp gần đây đã bắt đầu điều tra tính hữu ích của LLM để hỗ trợ tăng cường biểu diễn truy vấn và tài liệu [17,44]. LLM có triển vọng vì mã hóa kiến thức thế giới [50] từ huấn luyện trước có thể được sử dụng để tái công thức hóa các truy vấn không được chỉ định đầy đủ. Về nguyên tắc, LLM có thể được sử dụng để mở rộng/giải thích/chỉ định các truy vấn súc tích và mơ hồ, từ đó phân biệt nhu cầu thông tin của người dùng. Điều này thu hẹp khoảng cách giữa ý định người dùng tiềm ẩn trong truy vấn gốc và biểu diễn nhu cầu thông tin của người dùng bởi bộ truy xuất. Phương pháp query2doc [44] tạo ra các tài liệu giả bằng cách nhắc LLM few-shot và nối chúng với truy vấn gốc để tạo thành một truy vấn mở rộng mới. Tuy nhiên, có hai hạn chế chính khi sử dụng LLM để tạo ra các bản viết lại truy vấn hợp lý.

Vấn đề truy vấn và tài liệu không phù hợp. Nhiều truy vấn có nhiều khía cạnh, nghĩa và độ chi tiết ý định. Khi sử dụng mở rộng truy vấn dựa trên LLM, truy vấn mở rộng dẫn đến việc chọn một khía cạnh trong số nhiều khía cạnh có thể. Điều này dẫn đến vấn đề là ý định được chọn bởi việc mở rộng có thể không phù hợp với ý định thực tế. Sự thiếu phù hợp giữa ý định thực tế và ý định được suy ra này là có vấn đề trong quá trình huấn luyện. Cụ thể, sự không phù hợp ý định như vậy dẫn đến trường hợp tài liệu liên quan bị buộc phải khớp với một truy vấn được mở rộng sai lầm. Ví dụ, trong Bảng 1, chúng tôi quan sát thấy rằng đối với truy vấn "define sri", Query2Doc trôi dạt khỏi ý định và tạo ra một truy vấn liên quan đến Viện Nghiên cứu Stanford thay vì biểu thị từ tiếng Sanskrit. Tương tự, đối với truy vấn "HS worms", đầu ra được tạo phản ánh nhiều ý định.

Vấn đề suy luận hiệu quả. Thứ hai, một hạn chế nghiêm trọng (như được thừa nhận trong [44]) là chi phí tính toán từ cả giai đoạn truy xuất và xếp hạng lại. Các thuật ngữ truy vấn mở rộng trong các truy vấn đã viết lại tăng tra cứu chỉ mục. Nghiêm trọng hơn, việc mở rộng truy vấn sử dụng giải mã tự hồi quy theo từ trong quá trình suy luận. Tuy nhiên, cơ sở hạ tầng hiện tại chỉ không thể hỗ trợ suy luận dựa trên LLM hiệu quả trong quá trình xử lý truy vấn. Do đó, để duy trì trong yêu cầu xếp hạng dưới một giây mà không sử dụng tính toán đắt đỏ một cách cấm đoán, việc lựa chọn LLM trong quá trình xử lý truy vấn nên được tránh.

Viết lại truy vấn theo ngữ cảnh. Chúng tôi đưa ra hai quyết định thiết kế đơn giản nhưng quan trọng để khắc phục vấn đề không phù hợp và hiệu quả. Chúng tôi trước tiên tạo ra các bản viết lại truy vấn bằng cách bổ sung cung cấp tài liệu liên quan làm ngữ cảnh trong quá trình huấn luyện. Do đó, bản viết lại truy vấn được tạo ra hoàn toàn phù hợp với ngữ cảnh cải thiện việc huấn luyện bộ xếp hạng văn bản. Thứ hai, và không giống như các công trình hiện có, chúng tôi hoàn toàn tránh bất kỳ việc viết lại truy vấn nào sử dụng LLM trong quá trình suy luận. Nói cách khác, chúng tôi giả định rằng việc huấn luyện bộ xếp hạng để khớp các truy vấn được tạo bởi LLM với tài liệu liên quan dẫn đến việc học một mô hình xếp hạng tổng quát.

Về mặt phương pháp, chúng tôi đề xuất một khung viết lại truy vấn nhận biết ngữ cảnh dựa trên LLM có tên CAR, thay thế bộ viết lại truy vấn truyền thống bằng LLM để viết lại các truy vấn mơ hồ. Chúng tôi sử dụng nhắc nhở nhận biết ngữ cảnh để kiểm tra hiệu quả của LLM như một công cụ phân biệt, sử dụng truy vấn mơ hồ và tài liệu liên quan làm ngữ cảnh trong các lời nhắc. Chúng tôi cũng bao gồm một cơ chế lựa chọn ngữ cảnh để chọn các phần liên quan khi ngữ cảnh dài và trải qua nhiều chủ đề để xử lý trôi dạt chủ đề. Đầu ra của bộ viết lại truy vấn của chúng tôi được sử dụng để tinh chỉnh mô hình xếp hạng để chuyển giao kiến thức về nhu cầu thông tin của người dùng cho mô hình xếp hạng. Tại thời điểm suy luận, bộ xếp hạng được trang bị kiến thức về cơ chế phân biệt truy vấn cải thiện hiệu suất xếp hạng tài liệu cho các truy vấn mơ hồ tiếp theo mà không cần thành phần viết lại. Khung được đề xuất có thể được sử dụng với bất kỳ bộ xếp hạng có sẵn nào. Trong quá trình suy luận, mô hình xếp hạng được tinh chỉnh trên các truy vấn đã viết lại mang lại hiệu suất xếp hạng tốt hơn nhiều so với các truy vấn gốc. Phương pháp của chúng tôi loại bỏ nhu cầu nhắc LLM trong quá trình suy luận vì chúng tôi coi yêu cầu hiệu quả và độ trễ trong quá trình suy luận là một ràng buộc không thể thương lượng. Từ Bảng 1, chúng tôi quan sát thấy rằng phương pháp tái công thức hóa truy vấn nhận biết ngữ cảnh được đề xuất CAR có thể tạo ra các bản viết lại súc tích phản ánh ý định khi so sánh với Query2Doc. Chúng tôi cũng quan sát thấy rằng nó hoạt động tốt hơn các phương pháp học trong ngữ cảnh thuần túy, như được thấy từ Bảng 1 và cũng thông qua phân tích thực nghiệm của chúng tôi về kết quả xếp hạng.

Chúng tôi thực hiện các thí nghiệm mở rộng sử dụng LLM có quy mô tham số khác nhau và chứng minh rằng phương pháp được đề xuất dẫn đến hiệu suất xếp hạng tốt hơn. Cụ thể, chúng tôi thấy rằng việc tinh chỉnh bộ xếp hạng sử dụng các truy vấn đã viết lại mang lại cải thiện đáng kể lên đến 33% trên nhiệm vụ xếp hạng đoạn văn và lên đến 28% trên nhiệm vụ xếp hạng tài liệu khi so sánh với hiệu suất cơ sở sử dụng truy vấn gốc.

1.1 Câu hỏi nghiên cứu
Chúng tôi giải quyết các câu hỏi nghiên cứu sau:
RQ1: Chúng ta có thể sử dụng LLM để tạo ra các bản viết lại ngôn ngữ tự nhiên lưu loát của các truy vấn gốc từ các truy vấn mơ hồ không?
RQ2: Hiệu quả của bộ xếp hạng được tinh chỉnh trên các truy vấn đã viết lại cho nhiệm vụ xếp hạng tài liệu phía sau như thế nào?

Hướng tới việc trả lời các câu hỏi nghiên cứu này, chúng tôi thực hiện các thí nghiệm mở rộng trên TREC web 2012 sử dụng các mô hình LLM (Mục 4.2) để đánh giá chất lượng viết lại và trên tập dữ liệu đoạn văn và tài liệu TREC-DL-19 và TREC-DL-20 cho việc xếp hạng lại.

1.2 Đóng góp
Tóm lại, đây là danh sách các đóng góp của chúng tôi:
(1) Chúng tôi đề xuất một Bộ Viết lại Nhận biết Ngữ cảnh (CAR) cho tái công thức hóa truy vấn, dựa trên nhắc nhở nhận biết ngữ cảnh của Mô hình Ngôn ngữ Lớn (LLM) tạo ra các bản viết lại ngôn ngữ tự nhiên cho các truy vấn mơ hồ. Các bản viết lại ngôn ngữ tự nhiên được sử dụng để tinh chỉnh bộ xếp hạng để mã hóa khả năng phân biệt nhu cầu thông tin của người dùng.
(2) Chúng tôi thí nghiệm với LLM có quy mô tham số khác nhau và với các mục tiêu huấn luyện trước khác nhau để chứng minh sự khác biệt về chất lượng viết lại.
(3) Chúng tôi cho thấy rằng mô hình xếp hạng được tinh chỉnh trên các bản viết lại được tạo ra mang lại cải thiện hiệu suất đáng kể trên các truy vấn mơ hồ của người dùng tại thời điểm suy luận.

2 CÔNG TRÌNH LIÊN QUAN
Trong phần này, chúng tôi thảo luận về hai khía cạnh của công trình liên quan, cụ thể là mở rộng truy vấn và khả năng sinh của Mô hình Ngôn ngữ Lớn. Chúng tôi thảo luận thêm về cách LLM được tận dụng trong việc giải quyết vấn đề không khớp từ vựng trong truy xuất thông tin.

2.1 Viết lại truy vấn
Một trong những vấn đề trung tâm trong IR là thu hẹp khoảng cách từ vựng giữa truy vấn do người dùng chỉ định và tài liệu. Điều này cũng có thể được xem như việc hòa giải sự khác biệt giữa ý định người dùng và ý định của hệ thống (ý định máy). Các phương pháp chung của tái công thức hóa truy vấn bao gồm mở rộng truy vấn, thay thế từ đồng nghĩa và diễn đạt lại.

Các phương pháp mở rộng truy vấn đã được áp dụng để thu hẹp khoảng cách này. Các phương pháp này thường bao gồm việc thêm các thuật ngữ vào truy vấn gốc dựa trên phản hồi liên quan [22,37]. Khi phản hồi liên quan của người dùng không có sẵn, cơ chế phản hồi liên quan giả được áp dụng [10,28]. Ở đây, các kết quả xếp hạng cao nhất cho truy vấn gốc được sử dụng để mở rộng truy vấn với các thuật ngữ bổ sung. Tuy nhiên, hiệu suất của truy vấn đã viết lại bị hạn chế nghiêm trọng bởi chất lượng của các kết quả xếp hạng cao nhất [7] và hiếm khi được sử dụng trong truy xuất dày đặc [3]. Thay vào đó, các nhà nghiên cứu đã đề xuất diễn đạt lại các truy vấn gốc để giải quyết vấn đề "vực thẳm từ vựng". Trong công trình [58], các tác giả diễn đạt lại truy vấn do người dùng chỉ định một cách lặp đi lặp lại bằng cách khai thác các cụm từ tương tự từ WordNet. Thay vào đó, các nhà nghiên cứu cũng đã khám phá việc thay thế các thuật ngữ trong truy vấn đầu vào bằng các thuật ngữ đồng nghĩa dựa trên nhật ký truy vấn người dùng [21]. Tuy nhiên, các phương pháp này dựa trên thuật ngữ và mở rộng truy vấn dựa trên các nguồn tĩnh, khiến việc thích ứng với các thay đổi trong hệ thống truy xuất trở nên khó khăn.

Các phương pháp sinh trong việc viết lại truy vấn bao gồm việc tạo ra các cách diễn đạt của truy vấn gốc. Trong công trình [36], các tác giả sử dụng các phương pháp thống kê để diễn đạt lại các thuật ngữ của truy vấn và thêm các thuật ngữ tương đương vào truy vấn gốc. Tuy nhiên, điều này có thể dẫn đến các truy vấn có dạng kém, vì việc diễn đạt lại ở mức thuật ngữ không xem xét ngữ cảnh xung quanh. Nó cũng phụ thuộc nhiều vào phản hồi của người dùng để chọn phiên bản được diễn đạt lại tốt nhất, điều này rất cồng kềnh. Các phương pháp mở rộng truy vấn hứa hẹn hơn bao gồm việc diễn đạt lại truy vấn sử dụng mô hình sinh [53] cùng một lúc thay vì viết lại ở mức cụm từ. Ví dụ, truy vấn "average tesla cost" được diễn đạt lại thành "what is the cost of the new tesla". Các phương pháp sinh khác sử dụng chuỗi suy nghĩ và phản hồi liên quan [20] để mở rộng các thuật ngữ truy vấn hoặc sử dụng phản hồi liên quan giả cùng với nhắc nhở hoặc tinh chỉnh [45] cho tái công thức hóa truy vấn. Tuy nhiên, phương pháp này chỉ cung cấp các công thức thay thế của truy vấn tận dụng dữ liệu truy vấn tương đương từ tập dữ liệu MS MARCO và không tập trung vào phân biệt ý định người dùng trong các truy vấn mơ hồ. Chúng cũng không xem xét dự đoán hiệu suất trên truy xuất phía sau và cũng dễ bị thiên lệch tiếp xúc.

Để tăng cường hiệu suất trên truy xuất phía sau, DRQR [46] tận dụng học tăng cường sâu để huấn luyện các mô hình tái phát bằng cách kết hợp các bộ dự đoán hiệu suất truy vấn làm tín hiệu phần thưởng. Tuy nhiên, các tác giả đề cập rằng phương pháp được đề xuất không cải thiện truy xuất phía sau một cách đáng kể.

Gần đây hơn, các phương pháp tạo câu hỏi ngôn ngữ tự nhiên đã được áp dụng cho tái công thức hóa truy vấn. [35] đã giới thiệu một mô hình để tạo ra các câu hỏi làm rõ để bù đắp cho thông tin bị thiếu. Họ sử dụng mô hình học tăng cường để tối đa hóa hàm tiện ích dựa trên giá trị được thêm vào bởi phản hồi tiềm năng cho câu hỏi. [43], mặt khác, tập trung vào việc xác định các bài đăng không rõ ràng trong bối cảnh trả lời câu hỏi cộng đồng cần làm rõ thêm. Các phương pháp viết lại truy vấn đã thu hút sự quan tâm lớn trong lĩnh vực thương mại điện tử và chủ yếu yêu cầu phản hồi liên quan từ người dùng để cá nhân hóa [24,26,59]. Trong khi các phương pháp này có tính đặc thù theo lĩnh vực, [52] đề xuất một phương pháp học có giám sát yếu dựa trên mẫu cho IR lĩnh vực mở.

Các phương pháp mở rộng tài liệu cũng đã được đề xuất để giải quyết vấn đề không khớp từ vựng trong IR. Doc2query [34] và docTTTTquery [8] tạo ra các truy vấn giả từ ngữ cảnh tài liệu sử dụng mô hình seq2seq và nối chúng với tài liệu để tăng cường kết quả xếp hạng. Các công trình như InPars [5] và PromptAgator [12] tập trung vào việc sử dụng các mô hình sinh lớn để tạo ra truy vấn từ các tài liệu được lấy mẫu để tăng dữ liệu huấn luyện cho các nhiệm vụ truy xuất dày đặc. Tuy nhiên, các phương pháp này dễ bị ảo giác và có thể trôi dạt khỏi ý định gốc của truy vấn [17]. Trong bài báo này, chúng tôi tiến một bước trong hướng viết lại các truy vấn mơ hồ bằng cách kết hợp ngữ cảnh liên quan để chuyển giao kiến thức về cơ chế phân biệt cho bộ xếp hạng để cải thiện hiệu suất bộ xếp hạng lại. Phương pháp của chúng tôi cũng khác biệt với các phương pháp truyền thống bằng cách sử dụng LLM nguyên khối cho tái công thức hóa truy vấn hơn là nhiều thành phần có thể dẫn đến lan truyền lỗi.

2.2 Mô hình Ngôn ngữ Lớn
Những tiến bộ gần đây trong mô hình hóa sinh đã dẫn đến các mô hình ngôn ngữ lớn áp dụng cho nhiều nhiệm vụ [1,6,47]. Các mô hình này được huấn luyện theo cách tự giám sát thể hiện các khả năng nổi lên nơi chúng có thể ngoại suy sang các nhiệm vụ mới với ít mẫu minh họa [14,27,48]. Những tiến bộ này cũng đã khiến các nhà nghiên cứu suy nghĩ lại các phần của hệ thống truy xuất. Truy xuất sinh là một mô hình mới nổi như vậy [4,23,30,42] nơi các mô hình ngôn ngữ neural được sử dụng làm chỉ mục cho truy xuất. Mô hình sử dụng các phương pháp giải mã có điều kiện để tạo ra các định danh tài liệu ánh xạ trực tiếp đến tài liệu sự thật cơ bản. Điều này được thực hiện bằng cách huấn luyện các mô hình ngôn ngữ trên dữ liệu liên quan.

Gần đây hơn, các nhà nghiên cứu đã kiểm tra hiệu quả của LLM cho mở rộng tài liệu [16,44] trong bối cảnh zero-shot và few-shot cho truy xuất dày đặc. Tuy nhiên, các phương pháp này có những hạn chế nghiêm trọng tại thời điểm suy luận về hiệu quả do việc giải mã tự hồi quy của LLM. Các phương pháp này cũng dễ bị ảo giác do việc tạo ra ngữ cảnh dài cho xếp hạng. Viết lại truy vấn sử dụng LLM cũng đã tìm thấy sự quan tâm trong các nhiệm vụ QA với mô hình viết lại-truy xuất-đọc mới [29]. Trong công trình này, các tác giả tinh chỉnh một LLM nhỏ để viết lại truy vấn bằng cách sử dụng tín hiệu phần thưởng từ bộ đọc. Tuy nhiên, phương pháp này đặc thù cho các nhiệm vụ QA và bộ truy xuất là một mô hình đóng băng, dựa vào bộ đọc để tinh chỉnh bộ viết lại. Phương pháp này cũng yêu cầu cơ chế viết lại truy vấn đắt đỏ trong quá trình suy luận.

Các nhà nghiên cứu cũng đã khám phá việc điều chỉnh nhận thức nhiệm vụ của các bộ truy xuất dày đặc [2] bằng cách nối truy vấn với hướng dẫn đặc thù nhiệm vụ làm đầu vào cho bộ mã hóa dày đặc. Các hướng dẫn trang bị cho bộ truy xuất khả năng giải mã nhu cầu thông tin của người dùng. Tuy nhiên, các hướng dẫn yêu cầu chú thích thủ công và bị giới hạn cho các nhiệm vụ cụ thể. Trong bài báo này, chúng tôi làm cho mô hình xếp hạng nhận thức về cơ chế phân biệt của các truy vấn không được chỉ định đầy đủ bằng cách tinh chỉnh nó trên các truy vấn được tái công thức hóa được tạo ra bởi LLM.

3 PHƯƠNG PHÁP
Trong phần này, chúng tôi mô tả khung Bộ Viết lại Nhận biết Ngữ cảnh (CAR) được đề xuất cho tái công thức hóa truy vấn mơ hồ. Hình 1 cho thấy hoạt động của khung được đề xuất trong các giai đoạn huấn luyện và suy luận. Trong quá trình huấn luyện, khung bao gồm một giai đoạn tái công thức hóa truy vấn và một giai đoạn xếp hạng tài liệu. Trong giai đoạn tái công thức hóa truy vấn, chúng tôi sử dụng nhắc nhở nhận biết ngữ cảnh của LLM (Mục 3.1). Trong quá trình suy luận, bộ xếp hạng được tinh chỉnh trên các truy vấn được phân biệt được sử dụng trực tiếp để xếp hạng tài liệu cho các truy vấn mới mà không viết lại chúng, như được hiển thị trong Hình 1.

3.1 Bộ Viết lại Truy vấn
Thuật toán 1 chi tiết quy trình tái công thức hóa truy vấn. Cho một truy vấn mơ hồ q, mục tiêu của chúng tôi là tạo ra một bản viết lại q* giúp phân biệt ý định của truy vấn gốc. Chúng tôi tạo ra các bản viết lại truy vấn bằng cách điều kiện hóa LLM (RE) thông qua nhắc nhở few-shot trên tài liệu liên quan cho truy vấn d+ để tránh trôi dạt chủ đề. Một ví dụ về nhắc nhở few-shot được sử dụng được hiển thị trong Hình 2.

q* = RE(q, d+)

Chúng tôi trước tiên hướng dẫn LLM (Hình 2) thực hiện nhiệm vụ tái công thức hóa truy vấn trong ngữ cảnh của tài liệu đã cho. Sau đó chúng tôi cung cấp một truy vấn mơ hồ (q) được nối với tài liệu liên quan tương ứng như một phần của lời nhắc. Nhắc nhở nhận biết ngữ cảnh này của LLM dẫn đến các bản viết lại tốt hơn mà không có trôi dạt chủ đề, vì LLM được điều kiện hóa trên các ý định được truyền đạt trong tài liệu. Chúng tôi cũng đưa ra một ràng buộc về độ dài tối đa của chuỗi đầu ra được tạo ra. Ràng buộc này kết hợp với việc định căn của quá trình tạo ra trên ngữ cảnh tài liệu liên quan thông qua nhắc nhở để ngăn chặn ảo giác và trôi dạt chủ đề.

Lưu ý rằng ý định được truyền đạt cho việc viết lại có thể được kiểm soát bằng cách sử dụng một ngữ cảnh tài liệu khác trong lời nhắc. Chúng tôi thí nghiệm với các phương pháp nhắc nhở khác nhau bằng cách thay đổi ngữ cảnh tài liệu. Chúng tôi thảo luận về các biến thể khác nhau của nhắc nhở few-shot và các kết quả tương ứng trong Mục 4.2 và Mục 5 tương ứng.

3.1.1 Siêu tham số. Chúng tôi sử dụng nhiệt độ 0.5 để cân bằng khám phá và tạo ra xác định. Chúng tôi đặt phạt hiện diện và phạt tần suất lần lượt là 0.6 và 0.8 để giảm thiểu sự dư thừa trong các tái công thức hóa được tạo ra của truy vấn gốc.

Truy vấn đã viết lại kết quả được sử dụng làm đầu vào để tinh chỉnh bộ xếp hạng. Lưu ý rằng chúng tôi chỉ dựa vào LLM để viết lại các truy vấn mơ hồ để tinh chỉnh bộ xếp hạng. Trong quá trình suy luận, bộ xếp hạng xếp hạng trực tiếp các tài liệu liên quan cho các truy vấn mới mà không có thành phần viết lại dựa trên LLM.

3.2 Giai đoạn Xếp hạng
Mục tiêu của chúng tôi là huấn luyện một mô hình để xếp hạng lại tài liệu trên các bản viết lại truy vấn cho các truy vấn không được chỉ định đầy đủ. Cho một cặp truy vấn-tài liệu (q*, d) làm đầu vào, các mô hình xếp hạng đưa ra một điểm liên quan. Điểm này sau đó có thể được sử dụng để xếp hạng tài liệu dựa trên mức độ liên quan của chúng đến truy vấn đã cho.

Chính thức, tập huấn luyện bao gồm các cặp q*i, di, trong đó q*i là một truy vấn được viết lại (được phân biệt) sử dụng LLM và di là một tài liệu liên quan hoặc không liên quan đến truy vấn. Mục tiêu là tinh chỉnh một bộ xếp hạng R dự đoán điểm liên quan ŷ ∈ [0; 1] cho một truy vấn được tái công thức hóa q* và một tài liệu d:

R: (q*, d) ↦ ŷ (1)

Mô hình xếp hạng được tinh chỉnh (R) có thể được sử dụng để xếp hạng lại một tập hợp tài liệu thu được từ một bộ truy xuất tần suất nhẹ giai đoạn đầu. Các nghiên cứu gần đây đã chỉ ra rằng các mô hình ngôn ngữ được huấn luyện trước mô hình hóa chung các nhiệm vụ truy vấn và tài liệu [11,32,39] đã thể hiện hiệu suất đáng kể trên các nhiệm vụ xếp hạng. Trong công trình này, chúng tôi sử dụng mô hình BERT [13] để xếp hạng. Đầu vào cho bộ xếp hạng có định dạng:

[CLS]q[SEP]d[SEP]. (2)

Chúng tôi sử dụng mất mát pointwise để huấn luyện bộ xếp hạng. Giả sử của một mini batch gồm N ví dụ huấn luyện {xi, yi}i=1,...,N. Nhiệm vụ xếp hạng được đúc thành một bài toán phân loại nhị phân, trong đó mỗi thực thể huấn luyện xi = (q*i, di) là một cặp truy vấn-tài liệu và yi ∈ {0, 1} là nhãn liên quan. Điểm dự đoán của xi được ký hiệu là ŷi. Hàm mất mát entropy chéo được định nghĩa như sau:

LPoint = -1/N ∑(i=1 to N) (yi · log ŷi + (1 - yi) · log(1 - ŷi)) (3)

Lưu ý rằng chúng tôi chỉ tinh chỉnh bộ xếp hạng trên các tái công thức hóa truy vấn của các truy vấn mơ hồ hoặc không được chỉ định đầy đủ. Tại thời điểm suy luận, bộ xếp hạng được trang bị kiến thức về nhu cầu thông tin của người dùng được triển khai trực tiếp để xếp hạng các tài liệu liên quan cho các truy vấn mới như được hiển thị trong Thuật toán 1.

3.2.1 Bộ chọn đoạn văn cho Tài liệu. Hiện tượng trôi dạt chủ đề đã được xác định trong bối cảnh sử dụng phương pháp CAR (Xếp hạng Phản hồi Hội thoại) với tài liệu. Trôi dạt chủ đề được đặc trưng bởi sự phân kỳ của tính đặc thù của truy vấn được viết lại khi áp dụng phương pháp CAR cho tài liệu. Điều này xảy ra do tài liệu bao gồm nhiều chủ đề, có thể liên quan đến một hoặc nhiều khía cạnh của truy vấn. Để giảm thiểu sự xuất hiện của trôi dạt chủ đề, chúng tôi sử dụng các kỹ thuật lựa chọn đoạn văn có giám sát (Attention, Linear) như được đề xuất trong [25]. Các kỹ thuật này hỗ trợ trong việc chọn đoạn văn liên quan nhất từ một tài liệu, sắp xếp nó gần hơn với một truy vấn đã cho.

• Lựa chọn Tuyến tính: Phương pháp này liên quan đến việc chuyển đổi cả truy vấn và đoạn văn thành các biểu diễn nhúng trung bình của chúng. Các biểu diễn này sau đó được xử lý thông qua một lớp feed-forward duy nhất, tiếp theo là tính toán độ tương tự của chúng thông qua phép toán tích vô hướng. Điểm câu, được ký hiệu là sij đối với truy vấn q, được tính như sau:

scoreLin(q, sij) = ⟨Enc(q), Enc(sij)⟩, (4)

trong đó ⟨·, ·⟩ là tích vô hướng.

• Lựa chọn Attention: Bộ chọn dựa trên Attention hoạt động bằng cách rút ra các biểu diễn mức đoạn văn thông qua việc sử dụng mô hình QA-LSTM [41]. Ban đầu, cả truy vấn và tài liệu đều được ngữ cảnh hóa bằng cách được xử lý qua LSTM hai chiều chung của các nhúng token của chúng. Tiếp theo, biểu diễn truy vấn q̂ được rút ra thông qua việc áp dụng max-pooling theo từng phần tử trên các nhúng được ngữ cảnh hóa này.

q̂ = Max-Pool(Bi-LSTM(q)), (5)
dLSTM = Bi-LSTM(d). (6)

Đối với mỗi biểu diễn ẩn dLSTM i, sự chú ý đến truy vấn được tính như sau:

mi = W1h1 + W2q̂, (7)
hi = dLSTM i exp(W3tanh(mi)), (8)

trong đó W1, W2 và W3 là các tham số có thể huấn luyện. Đối với sij, hãy để hij biểu thị các đầu ra attention tương ứng. Biểu diễn câu được tính tương tự như biểu diễn truy vấn, tức là:

ŝij = Max-Pool(hij). (9)

Điểm cuối cùng của một câu được tính là độ tương tự cosine của biểu diễn của nó và biểu diễn truy vấn:

scoreAtt(q, sij) = cos(q̂, ŝij). (10)

4 THIẾT LẬP THÍ NGHIỆM
Trong phần này, chúng tôi mô tả thiết lập mà chúng tôi đã sử dụng để trả lời các câu hỏi nghiên cứu sau:
RQ1: Chúng ta có thể sử dụng LLM để tạo ra các bản viết lại ngôn ngữ tự nhiên lưu loát của các truy vấn mơ hồ và không được chỉ định đầy đủ không?
RQ2: Hiệu quả của bộ xếp hạng được tinh chỉnh trên các truy vấn được viết lại sử dụng LLM cho nhiệm vụ xếp hạng tài liệu phía sau như thế nào?

Hướng tới việc trả lời các câu hỏi nghiên cứu này, chúng tôi sử dụng các tập dữ liệu, bộ xếp hạng và thiết lập huấn luyện sau:

4.1 Tập dữ liệu
Mục tiêu của chúng tôi là có các mở rộng ngôn ngữ tự nhiên của truy vấn cho việc xếp hạng phía sau. Vì không có tập dữ liệu hiện có giải quyết rõ ràng vấn đề này, chúng tôi tuyển chọn các truy vấn và mô tả của chúng từ một số nguồn, như được chi tiết dưới đây.

4.1.1 Tập dữ liệu để kiểm tra hiệu quả của bộ viết lại dựa trên LLM. Để giải quyết RQ1, chúng tôi thu thập một số tập dữ liệu từ track web Trec [9]. Cụ thể, chúng tôi thu thập các chủ đề và mô tả tương ứng từ các track web Trec từ 2009 đến 2011 để sử dụng làm tập dữ liệu cho việc tinh chỉnh mô hình sinh. Chúng tôi tìm nguồn các cặp (chủ đề, mô tả chủ đề) để huấn luyện mô hình sinh. Chúng tôi có được 1143 mẫu để huấn luyện, 126 mẫu để xác thực. Cuối cùng, chúng tôi sử dụng 103 mẫu của các chủ đề, chủ đề phụ và mô tả của chúng từ Trec Web 2012 để kiểm tra bộ viết lại.

4.1.2 Dữ liệu Google People Also Asked (PAA). Vì các chủ đề/truy vấn TREC không phân biệt nhiều ý định mà một truy vấn có thể có, chúng tôi đề xuất thu thập các câu hỏi từ phần Google people also ask (PAA) như kiến thức bên ngoài cho mỗi truy vấn. Các văn bản từ Google PAA có thể phục vụ như các phiên bản mở rộng của truy vấn mơ hồ truyền đạt các ý định khác nhau. Ví dụ, đối với chủ đề 403b, pipeline được đề xuất cung cấp các câu hỏi khác nhau như "What are the withdrawal limitations for a 403b retirement plan?" và "What is the difference between a 401k and 403b?". Chúng tôi bổ sung các chủ đề truy vấn trong tập dữ liệu với các câu hỏi Google PAA tương ứng. Chúng tôi tinh chỉnh các bộ viết lại như BART và GPT-2 với các chủ đề được nối với câu hỏi Google PAA tương ứng. Tuy nhiên, trên tập kiểm tra (TREC web 2012), chúng tôi chỉ tận dụng các chủ đề vì không thể giả định quyền truy cập vào cơ sở tri thức bên ngoài trong các triển khai thực tế.

4.1.3 Tập dữ liệu cho Thí nghiệm Xếp hạng:
MS MARCO (Tập dữ liệu đoạn văn): Chúng tôi xem xét tập dữ liệu đoạn văn từ track TREC Deep Learning (2019) [31]. Chúng tôi đánh giá mô hình của chúng tôi trên tập dữ liệu đoạn văn TREC-DL-19 và TREC-DL-20, mỗi tập chứa 200 truy vấn. Để huấn luyện, chúng tôi chọn 1200 truy vấn mơ hồ/không được chỉ định đầy đủ từ tập dữ liệu huấn luyện đoạn văn TREC-DL-19, sau đó chúng tôi viết lại các truy vấn này sử dụng các mô hình viết lại truy vấn của chúng tôi (Mục 4.2). Các truy vấn mơ hồ được chọn dựa trên các phỏng đoán như độ dài truy vấn, từ viết tắt và thực thể có ngữ nghĩa đa dạng. Mỗi bản viết lại này tương ứng với một tập dữ liệu. Vì vậy chúng tôi có 13 tập dữ liệu huấn luyện viết lại và một tập dữ liệu cơ sở (truy vấn gốc).

MS MARCO (Tập dữ liệu tài liệu): Tương tự như tập dữ liệu đoạn văn trên, chúng tôi đánh giá trên bộ sưu tập tài liệu TREC-DL-19 và TREC-DL-20. Để huấn luyện, chúng tôi chọn 1200 truy vấn mơ hồ/không được chỉ định đầy đủ từ tập dữ liệu huấn luyện tài liệu TREC-DL-19 và sử dụng chính xác cùng phương pháp như trên để tạo ra các tập dữ liệu huấn luyện sử dụng các phương pháp khác nhau.

4.2 Mô hình Viết lại
Chúng tôi sử dụng một số mô hình sinh làm xương sống cho việc viết lại truy vấn để so sánh và đối chiếu chất lượng của các bản viết lại truy vấn. Chúng tôi sử dụng hai thiết lập trong đó các bản viết lại là các mô hình ngôn ngữ nhỏ hơn được tinh chỉnh trên tập dữ liệu web TREC để tạo ra các bản viết lại hoặc là các LLM được nhắc để tạo ra các bản viết lại.

GPT-2. Chúng tôi tinh chỉnh GPT-2, một mô hình sinh dựa trên bộ giải mã transformer với 117M tham số và 12 lớp để tạo ra mô tả chủ đề từ các chủ đề trên tập dữ liệu web TREC. Chúng tôi sử dụng tốc độ học 3.2e-5, giảm trọng số 0.01, kích thước batch 16 và huấn luyện mô hình trong 6 epoch.

BART. Chúng tôi sử dụng BART (phiên bản cơ sở), một bộ tự mã hóa khử nhiễu được huấn luyện trước sử dụng mục tiêu để tái tạo văn bản bị hỏng. Kết quả là, BART có thể tạo ra các truy vấn ngôn ngữ tự nhiên mạnh mẽ và lưu loát từ các truy vấn không được chỉ định đầy đủ và mơ hồ. Chúng tôi tinh chỉnh BART để tạo ra mô tả chủ đề từ các chủ đề đầu vào đã cho trên tập dữ liệu web TREC. Chúng tôi sử dụng tốc độ học 2e-5, kích thước batch 16 và huấn luyện mô hình trong 8 epoch.

BART (topic+PAA). Chúng tôi cũng đề xuất một biến thể BART (topic+PAA) nơi chúng tôi nối các chủ đề với kiến thức bên ngoài dưới dạng các câu hỏi Google PAA liên quan (tham khảo Mục 4.1.2) để tinh chỉnh mô hình BART base trên tập dữ liệu web TREC được thu thập. Tuy nhiên, khi tạo ra các bản viết lại cho các truy vấn mơ hồ mới trong tập dữ liệu MS MARCO, sử dụng mô hình đã tinh chỉnh, chúng tôi chỉ cung cấp các chủ đề làm đầu vào. Chúng tôi đề xuất phương pháp này để kiểm tra xem việc bổ sung các câu hỏi Google PAA làm ngữ cảnh cho tên chủ đề trong quá trình tinh chỉnh có hỗ trợ phân biệt nhu cầu thông tin của người dùng cho các mô hình nhỏ hơn hay không. Biến thể này cũng được phân loại vào lớp phương pháp viết lại nhận biết ngữ cảnh CAR được đề xuất trong công trình này, nơi Google PAA đóng vai trò như ngữ cảnh. Chúng tôi sử dụng các siêu tham số tương tự cho BART như đã thảo luận. Điều này hơi khác so với các phương pháp CAR dựa trên nhắc nhở được đề xuất và được đề xuất như một thay thế để kiểm tra khả năng của các mô hình nhỏ hơn.

Davinci-003 (nhắc nhở). Vì việc nhắc nhở các LLM như GPT-3 đã chứng minh mang lại các đầu ra ổn định hơn với ít lỗi thực tế hơn, chúng tôi sử dụng hai biến thể lời nhắc cho Davinci-003. Chúng tôi cung cấp các lời nhắc "Generate short sentence expanding:" hoặc "Generate short sentence question:" theo sau bởi tên chủ đề cho Davinci-003. Chúng tôi gọi hai biến thể này là Davinci-003 (prompt1) và Davinci-003 (prompt2) tương ứng. Chúng tôi sử dụng nhiệt độ 0.5, độ dài token tối đa 35 để tạo ra các bản viết lại ngôn ngữ tự nhiên ngắn của truy vấn gốc. Đối với phạt tần suất và phạt hiện diện, chúng tôi sử dụng giá trị 0.8 và 0.6 để tránh sự dư thừa trong các đầu ra được tạo ra.

Davinci-003 + ví dụ trong ngữ cảnh. Vì việc nhắc nhở thuần túy có thể dẫn đến trôi dạt chủ đề của văn bản được tạo ra và mô hình có thể hiểu sai nhiệm vụ dự định, chúng tôi cũng áp dụng học trong ngữ cảnh [27]. Học trong ngữ cảnh coi LM như một hộp đen và hướng dẫn mô hình về nhiệm vụ mà không cần gradient descent thông qua các ví dụ. Chúng tôi cung cấp các ví dụ dưới dạng <query1,desc1>,<query2,desc2>...querytest,[insert] và hướng dẫn mô hình điền mô tả vào chỗ trống được cung cấp. Chúng tôi sử dụng các siêu tham số tương tự như đã thảo luận cho Davinci-003(prompting).

Các mô hình GPT-3 khác. Chúng tôi cũng kiểm tra với các mô hình là biến thể của GPT-3 có quy mô tham số khác nhau. Các mô hình này bao gồm Ada-001, Davinci-002, Curie-001 và Babbage-001. Các mô hình và số lượng tham số được hiển thị trong Bảng 2. Chúng tôi sử dụng học trong ngữ cảnh bằng cách cung cấp các mẫu minh họa. Lời nhắc như sau:

Generate short sentence as expansion for the given test query like the following examples,
<input:query1,output:desc1>,...input:querytest,output:

Lưu ý rằng lời nhắc hơi khác so với Davinci-003 vì chúng tôi quan sát thấy rằng các mô hình nhỏ hơn cần hướng dẫn chi tiết để có khả năng sinh tốt hơn. Điều này có thể do sự khác biệt trong các phương pháp tinh chỉnh hướng dẫn. Chúng tôi sử dụng các siêu tham số tương tự, truy vấn và mô tả của chúng như đã thảo luận trước đó.

ChatGPT + ví dụ trong ngữ cảnh. Chúng tôi cũng kiểm tra khả năng học trong ngữ cảnh của gpt-3.5-turbo (ChatGPT) cho tái công thức hóa truy vấn. Chúng tôi sử dụng lời nhắc tương tự như Davinci-003. Chúng tôi chỉ thêm vào trước hướng dẫn nhiệm vụ để thiết lập vai trò của hệ thống. Mô tả nhiệm vụ là: "You are a system that gives an expansion for queries, expanding abbreviations and acronyms when applicable. Some examples are " theo sau bởi các mẫu minh họa như đã thảo luận trước đó. Chúng tôi sử dụng các giá trị tương tự cho các siêu tham số như đã thảo luận cho các phương pháp nhắc nhở LLM khác.

Query2Doc: Chúng tôi cũng sử dụng phương pháp mở rộng tài liệu được đề xuất gần đây, Query2Doc, tạo ra các tài liệu giả để hỗ trợ xếp hạng tài liệu, làm cơ sở. Chúng tôi sử dụng mô hình gpt-3.5-turbo với token tối đa 128 để tạo ra và tuân theo các siêu tham số gốc được sử dụng trong công trình [44] để tái tạo.

4.3 Mô hình Xếp hạng
Đối với các thí nghiệm trên TREC-DL, chúng tôi sử dụng BERT-base [13], một mô hình ngữ cảnh được huấn luyện trước dựa trên kiến trúc transformer. Về nguyên tắc, người ta có thể sử dụng các kiến trúc transformer khác nhưng tập trung vào BERT như một mô hình đại diện trong các thí nghiệm của chúng tôi. Chúng tôi sử dụng phiên bản cơ sở với 12 lớp mã hóa, 12 đầu attention và biểu diễn đầu ra 768 chiều. Độ dài đầu vào bị hạn chế tối đa 512 token. Chúng tôi sử dụng kiến trúc BERT chú ý chéo để xếp hạng, đôi khi còn được gọi là MonoBert. Mô hình cơ sở được huấn luyện trên tập truy vấn mơ hồ gốc sử dụng mục tiêu mất mát xếp hạng pointwise. Các mô hình xếp hạng được cải thiện cũng là các mô hình BERT base nhưng được huấn luyện trên các truy vấn đã viết lại sử dụng các LLM như biến thể GPT-3, BART, ChatGPT, v.v. Trong các thí nghiệm, chúng tôi đề cập đến các mô hình xếp hạng được cải thiện này sau mô hình bộ viết lại truy vấn LLM đã được sử dụng để tạo ra dữ liệu huấn luyện cho chúng.

4.4 Thước đo
Để đánh giá chất lượng của các bản viết lại, chúng tôi sử dụng các mô tả chủ đề TREC làm mục tiêu và tính toán điểm ROUGE-L [38] và BERTScore [54–56], các thước đo đánh giá tự động thường được sử dụng trong một số nhiệm vụ tạo văn bản. Trong khi ROUGE-L là thước đo chồng chéo n-gram, BERTScore tương quan với đánh giá của con người bằng cách sử dụng các nhúng được ngữ cảnh hóa để tính toán các khớp ở mức từ giữa câu tham chiếu và câu được tạo ra [54]. Các thước đo này phục vụ như một proxy để đo lường khả năng của các mô hình sinh mở rộng truy vấn để phân biệt các ý định khác nhau nhằm hỗ trợ trong các nhiệm vụ truy xuất phía sau. Chúng cũng đo lường xem các truy vấn có hợp lý hay không. Để đánh giá phương pháp xếp hạng, chúng tôi sử dụng các thước đo xếp hạng tiêu chuẩn như MRR và nDCG@10 và đánh giá trên các tập kiểm tra TREC-DL-19 và TREC-DL-20 cho bộ sưu tập đoạn văn và tài liệu.

5 KẾT QUẢ
Chúng tôi bắt đầu bằng cách trả lời đầu tiên xem liệu thành phần bộ viết lại có thể tạo ra các mở rộng ngôn ngữ tự nhiên hợp lý của truy vấn hay không. Sau đó chúng tôi kết luận bằng cách phân tích tác động của các truy vấn đã viết lại đối với hiệu suất xếp hạng tài liệu.

5.1 Đánh giá Bộ Viết lại Truy vấn
Để trả lời RQ1, chúng tôi đánh giá các phương pháp viết lại được đề xuất sử dụng các thước đo như BERTScore và ROUGEL. Kết quả được báo cáo trong Bảng 3. Chúng tôi báo cáo các giá trị trung bình của điểm precision, recall và F1 trên tất cả các mẫu kiểm tra từ TREC web 2012 để báo cáo điểm cuối cùng. Chúng tôi quan sát thấy rằng các biến thể CAR, cụ thể là các phương pháp Davinci-003 và ChatGPT với nhắc nhở nhận biết ngữ cảnh có BERTScore và điểm ROUGE-L cao nhất. Chúng tôi quy hiệu suất này cho quy mô của mô hình ngôn ngữ GPT-3, tinh chỉnh hướng dẫn và chất lượng của ngữ cảnh được cung cấp cho phép bộ viết lại tạo ra các truy vấn ngôn ngữ tự nhiên. Chúng tôi quan sát thấy rằng CAR cung cấp các bản viết lại lưu loát và liên quan hơn khi so sánh với các phương pháp dựa trên nhắc nhở vanilla. Các phương pháp học trong ngữ cảnh cũng mang lại các bản viết lại lưu loát. Tuy nhiên, trong phân tích, chúng tôi quan sát thấy rằng đôi khi chúng có trôi dạt chủ đề do thiếu ngữ cảnh liên quan cần thiết để phân biệt các truy vấn. Chúng tôi cũng quan sát thấy rằng các LLM có quy mô nhỏ hơn như Ada-001, Babbage-001 và Curie-001 tạo ra các bản viết lại không nhất quán về mặt thực tế và trôi dạt khỏi chủ đề liên quan. Điều này rõ ràng từ hiệu suất xếp hạng tài liệu phía sau (Bảng 4) và phân tích định tính của các mẫu (Bảng 1). Điều này chứng minh rằng các bản viết lại CAR nhất quán, lưu loát và giúp cải thiện hiệu suất xếp hạng phía sau.

Trong số các phương pháp được tinh chỉnh trong Bảng 3, BART (topic), BART (topic + PAA) và GPT-2 (topic), chúng tôi quan sát thấy rằng BART (topic) tạo ra các bản viết lại truy vấn lưu loát khi so sánh với GPT-2 (topic), như rõ ràng từ điểm BERTScore và ROUGEL. Sau khi thực hiện phân tích thủ công các mẫu, chúng tôi quan sát thấy rằng các bản viết lại truy vấn được tạo ra bởi GPT-2 không liên quan do ảo giác và trong một số trường hợp chúng cũng không chính xác về mặt ngữ pháp. Do đó, chúng tôi bỏ qua mô hình GPT-2 (topic) cho phần còn lại của các thí nghiệm.

CAR (BART (topic + PAA)) cải thiện thêm tính lưu loát của các truy vấn được tạo ra, nhưng vẫn không bằng các phương pháp biến thể GPT-3 và ChatGPT, mặc dù chúng không được tinh chỉnh.

Vì các thước đo được đề xuất ở trên không phải là chỉ báo thực sự về chất lượng, chúng tôi đánh giá chất lượng của chúng thông qua tác động của chúng đối với hiệu suất xếp hạng phía sau.

Insight 1: LLM có thể tạo ra các bản viết lại tốt cho các truy vấn không được chỉ định đầy đủ và mơ hồ. Trong số các phương pháp của chúng tôi, CAR với nhắc nhở few-shot nhận biết ngữ cảnh có các bản viết lại tốt nhất.

5.2 Đánh giá Xếp hạng
Để trả lời RQ2, chúng tôi đầu tiên huấn luyện các bộ xếp hạng trên dữ liệu từ tập dữ liệu đoạn văn MS MARCO (Mục 4.3) với các bản viết lại từ các LLM khác nhau (Mục 4.2), và đánh giá trên các tập kiểm tra TREC-DL-19 và TREC-DL-20. Sau đó chúng tôi chọn các mô hình bộ viết lại tốt nhất và huấn luyện các mô hình xếp hạng tài liệu trên tập dữ liệu tài liệu MS MARCO và đánh giá trên TREC-DL-19 và TREC-DL-20. Tất cả kết quả được hiển thị trong Bảng 4.

Trong trường hợp tập dữ liệu đoạn văn, các mô hình CAR hoạt động tốt hơn các đối tác của chúng với ChatGPT CAR cho thấy cải thiện 30% và 33% trên nDCG@10 và 22% và 25% trên MRR so với cơ sở cho TREC-DL-19 và TREC-DL-20 tương ứng. Ngoài CAR, mô hình Davinci-003 in-context và ChatGPT in-context vượt trội hơn cơ sở trên các tập kiểm tra.

Hiệu suất kém của các mô hình như Ada-001, Babbage-001, v.v. rõ ràng từ quan sát của chúng tôi rằng chúng không thể phân biệt truy vấn, mà chỉ viết lại nó. Ví dụ, truy vấn "define sri" thường được viết lại thành "Definition of sri" trong khi Davinci-003 và ChatGPT có thể truyền đạt ý định tốt hơn như được hiển thị trong Bảng 1. Các mô hình CAR của chúng tôi hoạt động tốt hơn các mô hình không CAR, chứng minh rằng ngữ cảnh cục bộ quan trọng cùng với kiến thức toàn cầu. Chúng tôi cũng quan sát thấy rằng mô hình BART base hoạt động tệ nhất trong số tất cả các mô hình nhưng biến thể của BART, với ngữ cảnh nhận biết BART (topic+PAA) hoạt động tốt hơn mô hình cơ sở trong tất cả các đánh giá. Chúng tôi cho rằng kiến thức bên ngoài được cung cấp dưới dạng câu hỏi Google PAA khi tinh chỉnh BART (topic+PAA) giúp phân biệt ý định của truy vấn. Ví dụ, truy vấn "403b" trong TREC web track 2012 mơ hồ mà không có ngữ cảnh. Tuy nhiên, một trong những câu hỏi Google PAA tương ứng, "What are the withdrawal limitations of a 430b retirement plan?" giúp chỉ ra rằng truy vấn đề cập đến một kế hoạch nghỉ hưu. Chúng tôi cho rằng phương pháp BART (topic+PAA) mã hóa kiến thức này thông qua tinh chỉnh. Do đó, nó có hiệu suất xếp hạng tài liệu tốt hơn so với các phương pháp khác. Điều này chứng minh rằng ngữ cảnh quan trọng khi viết lại các truy vấn mơ hồ.

Chúng tôi cũng kiểm tra phương pháp CAR của chúng tôi trên bộ sưu tập tài liệu MS MARCO. Chúng tôi xem xét các cơ sở và các phương pháp hoạt động tốt nhất trên tập dữ liệu đoạn văn để kiểm tra trên bộ sưu tập tài liệu MS MARCO. Bảng 4 cho thấy hiệu suất của các bộ xếp hạng khác nhau được huấn luyện sử dụng các truy vấn mơ hồ từ bộ sưu tập tài liệu MS MARCO trên TREC-DL-19 và TREC-DL-20. Trong trường hợp TREC-DL-19, các phương pháp ChatGPT CAR hoạt động tốt hơn các mô hình in-context và đối với TREC-DL-20, BART (topic+PAA) hoạt động tốt hơn. Các mô hình CAR hoạt động khoảng 12% và 23% tốt hơn mô hình cơ sở trên nDCG@10. Trong trường hợp tập dữ liệu đoạn văn, đối với một truy vấn đã cho và một đoạn văn liên quan, ngữ cảnh cụ thể, trong khi điều này không đúng trong trường hợp bộ sưu tập tài liệu. Có thể có trôi dạt ngữ cảnh trong trường hợp tài liệu, nơi một tài liệu có thể có ngữ cảnh tổng quát, có thể dẫn đến LLM tạo ra các truy vấn có ý định tổng quát sử dụng phương pháp CAR. Trong Bảng 1, chúng ta có thể thấy các ví dụ trong đó các bản viết lại CAR từ các mô hình khác nhau cụ thể hơn so với các đối tác tài liệu của chúng cho cùng các truy vấn.

Nếu chúng ta nhìn vào hiệu suất mô hình sử dụng bộ sưu tập đoạn văn MS MARCO (Bảng 4) và bộ sưu tập tài liệu (Bảng 4) kết hợp, chúng ta có thể thấy rằng phương pháp CAR hoạt động tốt hơn tổng thể. Điều này là do các LLM được sử dụng để mở rộng các truy vấn mơ hồ/không được định hình tốt mã hóa kiến thức thế giới, khi được làm phong phú với ngữ cảnh đoạn văn/tài liệu liên quan giúp định căn việc tạo ra. Ngữ cảnh phục vụ như định căn, ngăn chặn ảo giác và trôi dạt chủ đề. Trong Bảng 1, chúng ta có thể thấy cách các bản viết lại mô hình CAR cụ thể và thực tế hơn so với các đối tác không CAR của chúng.

Insight 2: Chất lượng của ngữ cảnh được sử dụng trong nhắc nhở few-shot của LLM rất quan trọng khi mở rộng các truy vấn mơ hồ/không được định hình tốt. Phương pháp được đề xuất CAR có thể tạo ra các tái công thức hóa truy vấn súc tích và liên quan dựa trên ngữ cảnh đã cho.

Chúng ta giải quyết trôi dạt ngữ cảnh cho tài liệu dài như thế nào?
Chúng tôi đã thảo luận trước đó về hiện tượng trôi dạt chủ đề trong phạm vi ngữ cảnh tài liệu. Để giảm thiểu hiện tượng này, chúng tôi sử dụng các kỹ thuật liên quan đến các kỹ thuật lựa chọn có giám sát, cụ thể là các bộ chọn Attention và Linear (Mục 3.2.1), như được đề xuất trong [25]. Trong cả hai trường hợp, mục tiêu chính của bộ chọn là xác định và chọn đoạn văn liên quan nhất dựa trên một truy vấn đã cho.

Trong phương pháp của chúng tôi, cho một truy vấn và một tài liệu liên quan đến truy vấn, chúng tôi phân đoạn tài liệu thành các đoạn văn, mỗi đoạn bao gồm khoảng bốn câu. Tiếp theo, chúng tôi sử dụng các bộ chọn Attention và Linear để xác định đoạn văn có độ tương tự cao nhất với truy vấn đã cho trong tài liệu. Đoạn văn được chọn này, cùng với truy vấn gốc, được sử dụng để viết lại truy vấn sử dụng LLM và sau đó sử dụng nó để huấn luyện mô hình.

Bốn mô hình cuối cùng được chi tiết trong Bảng 4 minh họa việc áp dụng các bộ chọn Attention và Linear trên Davinci-003 và ChatGPT, dẫn đến hiệu suất được cải thiện so với các đối tác CAR của chúng. Chỉ so sánh các mô hình CAR, chúng ta có thể thấy rõ ràng CAR ChatGPT (Linear) hoạt động tốt hơn 2% và 8% so với CAR ChatGPT cho nDCG 10 trên TREC-DL '19 và TREC-DL '20 tương ứng. Chúng ta có thể thấy xu hướng tương tự trong trường hợp các mô hình Davinci-003 và Davinci-003(Attention). Các quan sát thực nghiệm chứng minh rằng các mô hình sử dụng bộ chọn CAR thể hiện cải thiện hiệu suất từ 2% đến 15% khi so sánh với các đối tác chỉ CAR của chúng. Do đó, chứng minh rằng nếu chúng ta giảm trôi dạt chủ đề cho tài liệu, chúng ta có thể cải thiện chất lượng viết lại truy vấn, từ đó cải thiện hiệu suất nhiệm vụ phía sau. Từ Bảng 4, chúng ta có thể kết luận rằng đối với việc viết lại truy vấn sử dụng tài liệu, các mô hình dựa trên CAR hoạt động tốt nhất, cụ thể là ChatGPT (Linear) và Davinci-003 (Attention) là hai mô hình hoạt động tốt nhất tổng thể.

Insight 3: Nhấn mạnh rằng việc sử dụng các bộ chọn Attention và Linear để giảm thiểu trôi dạt chủ đề tài liệu không chỉ tăng cường chất lượng của truy vấn được viết lại sử dụng phương pháp CAR, mà còn dẫn đến cải thiện tổng thể trong hiệu suất của mô hình xếp hạng.

Hiệu suất thay đổi như thế nào dựa trên LLM có quy mô tham số khác nhau?: Chúng tôi thí nghiệm với các mô hình có quy mô tham số khác nhau (Bảng 2). Kết quả được hiển thị trong Bảng 4. Chúng tôi quan sát thấy rằng khi chúng tôi mở rộng quy mô LLM bộ viết lại từ 350 triệu tham số lên 175 tỷ hoặc 154 tỷ tham số, hiệu suất cải thiện đáng kể như quan sát được trong quá trình đánh giá (Bảng 4). Điều này là do chúng có thể đặc trưng hóa kiến thức thế giới tốt hơn từ chế độ huấn luyện dựa trên hướng dẫn và quy mô của các mô hình [49]. Chúng tôi quan sát thấy rằng các mô hình nhỏ hơn như Ada-001, Babbage-001 và Curie-001 tạo ra các bản viết lại không nhất quán về mặt thực tế của truy vấn gốc và các bản viết lại với trôi dạt chủ đề đáng kể từ truy vấn gốc.

Insight 4: Chúng tôi quan sát thấy rằng khi chúng tôi mở rộng quy mô LLM, nó dẫn đến các bản viết lại chất lượng tốt hơn vì chúng mã hóa nhiều kiến thức thế giới hơn. Điều này rõ ràng từ cải thiện đáng kể trong hiệu suất xếp hạng.

5.3 Phân tích Định tính
Chúng tôi phân tích các ví dụ về truy vấn mơ hồ và các bản viết lại tương ứng của chúng thu được từ các mô hình sinh. Một số ví dụ này có trong Bảng 1. Chúng tôi quan sát thấy rằng các truy vấn được tạo ra gần với các ý định thực tế trong phương pháp CAR (ChatGPT hoặc Davinci-003). Ví dụ, đối với truy vấn "hs worms", tập dữ liệu chỉ chứa các tài liệu liên quan đến tổ chức giáo dục. Nhưng nó có nhiều ngữ cảnh, như tổ chức giáo dục hoặc giun tim. Điều này làm cho truy vấn và ý định cơ bản trở nên mơ hồ mà không được ngữ cảnh hóa bởi các tài liệu trong tập dữ liệu. Tuy nhiên, trong số các phương pháp viết lại khác, CAR có thể giải mã ý định chính xác với độ chi tiết tốt bằng cách cũng cung cấp một mở rộng cho truy vấn. Lợi thế của khung được đề xuất rõ ràng hơn từ ví dụ thứ tư "urodeum function". Truy vấn này cũng có tính chất mơ hồ, vì nó có thể đề cập đến bất kỳ giải phẫu nào. Tuy nhiên, chúng tôi quan sát thấy rằng CAR với nhắc nhở nhận biết ngữ cảnh tài liệu có thể giải mã ý định chính xác. Chúng tôi quy điều này cho dữ liệu huấn luyện trước quy mô lớn và khả năng của các mô hình có tham số quá mức để phục vụ như cơ sở tri thức [49].

Insight 5: CAR với nhắc nhở nhận biết ngữ cảnh tốt hơn trong việc tạo ra các bản viết lại hợp lý với các ý định liên quan đến lĩnh vực tìm kiếm và cải thiện xếp hạng phía sau.

5.4 Hạn chế
Trong phương pháp hiện tại của chúng tôi, chúng tôi chọn các truy vấn mơ hồ dựa trên các phỏng đoán như độ dài truy vấn và các loại truy vấn cụ thể như từ viết tắt, thực thể có nhiều nghĩa dựa trên ngữ cảnh. Mặc dù các phỏng đoán này phản ánh bản chất của các truy vấn mơ hồ, một phương pháp có nguyên tắc hơn sẽ giúp xác định và lọc các truy vấn mơ hồ.

6 KẾT LUẬN
Trong công trình này, chúng tôi đề xuất một khung để định nghĩa lại các phương pháp viết lại truy vấn sử dụng nhắc nhở nhận biết ngữ cảnh của LLM để cải thiện xếp hạng tài liệu. Khung của chúng tôi tái công thức hóa các truy vấn mơ hồ thành các truy vấn ngôn ngữ tự nhiên có thể diễn giải phân biệt nhu cầu thông tin của người dùng. Các thí nghiệm của chúng tôi chứng minh rằng việc tạo ra các bản viết lại hợp lý phân biệt ý định người dùng là có thể, điều này tiếp tục tăng cường hiệu suất xếp hạng phía sau. Chúng tôi cho rằng việc huấn luyện chung của bộ viết lại và bộ xếp hạng trong khung của chúng tôi với tín hiệu phản hồi sẽ mang lại các bản viết lại và hiệu suất xếp hạng tốt hơn. Chúng tôi cũng đề xuất một số thách thức liên quan đến việc phát triển khung.

TÀI LIỆU THAM KHẢO
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198 (2022).
[2] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen tau Yih. 2022. Task-aware Retrieval with Instructions. (2022). arXiv:cs.CL/2211.09260
[3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. (2018). arXiv:cs.CL/1611.09268
[4] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen tau Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive Search Engines: Generating Substrings as Document Identifiers. (2022). arXiv:cs.CL/2204.10628
[5] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Data Augmentation for Information Retrieval using Large Language Models. arXiv preprint arXiv:2202.05144 (2022).
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
[7] Claudio Carpineto and Giovanni Romano. 2012. A Survey of Automatic Query Expansion in Information Retrieval. ACM Comput. Surv. 44, 1, Article 1 (jan 2012), 50 pages. https://doi.org/10.1145/2071389.2071390
[8] David R. Cheriton. 2019. From doc2query to docTTTTTquery.
[9] Charles LA Clarke, Nick Craswell, and Ian Soboroff. [n. d.]. Overview of the TREC 2009 Web Track. ([n. d.]).
[10] Bruce Croft, Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st ed.). Addison-Wesley Publishing Company, USA.
[11] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In ACM SIGIR'19. 985–988.
[12] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 (2022).
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR abs/1810.04805 (2018). http://arxiv.org/abs/1810.04805
[14] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning. (2023). arXiv:cs.CL/2301.00234
[15] Luke Gallagher. 2019. Pairwise t-test on TREC Run Files. https://github.com/lgrz/pairwise-ttest/. (2019).
[16] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot Dense Retrieval without Relevance Labels. (2022). https://doi.org/10.48550/ARXIV.2212.10496
[17] Mitko Gospodinov, Sean MacAvaney, and Craig Macdonald. 2023. Doc2Query–: When Less is More. (2023). arXiv:cs.IR/2301.03266
[18] Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, and Narayan Bhamidipati. 2015. Context-and content-aware embeddings for query rewriting in sponsored search. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. 383–392.
[19] Yunlong He, Jiliang Tang, Hua Ouyang, Changsung Kang, Dawei Yin, and Yi Chang. 2016. Learning to rewrite queries. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. 1443–1452.
[20] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query Expansion by Prompting Large Language Models. arXiv preprint arXiv:2305.03653 (2023).
[21] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating Query Substitutions. In Proceedings of the 15th International Conference on World Wide Web (WWW '06). Association for Computing Machinery, New York, NY, USA, 387–396. https://doi.org/10.1145/1135777.1135835
[22] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '01). Association for Computing Machinery, New York, NY, USA, 120–127. https://doi.org/10.1145/383952.383972
[23] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative Multi-hop Retrieval. (2022). arXiv:cs.IR/2204.13596
[24] Mu-Chu Lee, Bin Gao, and Ruofei Zhang. 2018. Rare Query Expansion Through Generative Adversarial Networks in Search Advertising. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '18). Association for Computing Machinery, New York, NY, USA, 500–508. https://doi.org/10.1145/3219819.3219850
[25] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2022. Extractive Explanations for Interpretable Text Ranking. ACM Trans. Inf. Syst. (dec 2022). https://doi.org/10.1145/3576924
[26] Sen Li, Fuyu Lv, Taiwei Jin, Guiyang Li, Yukun Zheng, Tao Zhuang, Qingwen Liu, Xiaoyi Zeng, James Kwok, and Qianli Ma. 2022. Query Rewriting in TaoBao Search. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management (CIKM '22). Association for Computing Machinery, New York, NY, USA, 3262–3271. https://doi.org/10.1145/3511808.3557068
[27] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Association for Computational Linguistics, Dublin, Ireland and Online, 100–114. https://doi.org/10.18653/v1/2022.deelio-1.10
[28] Yuanhua Lv and ChengXiang Zhai. 2009. A comparative study of methods for estimating query language models with pseudo feedback. In Proceedings of the 18th ACM conference on Information and knowledge management. 1895–1898.
[29] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting for Retrieval-Augmented Large Language Models. (2023). arXiv:cs.CL/2305.14283
[30] Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search. ACM SIGIR Forum 55, 1 (jun 2021), 1–27. https://doi.org/10.1145/3476415.3476428
[31] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings), Tarek Richard Besold, Antoine Bordes, Artur S. d'Avila Garcez, and Greg Wayne (Eds.), Vol. 1773. CEUR-WS.org. http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf
[32] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. CoRR abs/1901.04085 (2019). http://arxiv.org/abs/1901.04085
[33] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document Expansion by Query Prediction. (2019). arXiv:cs.IR/1904.08375
[34] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document Expansion by Query Prediction. (2019). https://doi.org/10.48550/ARXIV.1904.08375
[35] Sudha Rao and Hal Daumé III. 2018. Learning to ask good questions: Ranking clarification questions using neural expected value of perfect information. arXiv preprint arXiv:1805.04655 (2018).
[36] Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical Machine Translation for Query Expansion in Answer Retrieval. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics, Prague, Czech Republic, 464–471. https://aclanthology.org/P07-1059
[37] J. J. Rocchio. 1971. Relevance feedback in information retrieval. In The Smart retrieval system - experiments in automatic document processing, G. Salton (Ed.). Englewood Cliffs, NJ: Prentice-Hall, 313–323.
[38] Lin CY ROUGE. 2004. A package for automatic evaluation of summaries. In Proceedings of Workshop on Text Summarization of ACL, Spain.
[39] Koustav Rudra and Avishek Anand. 2020. Distant supervision in BERT-based adhoc document retrieval. In Proceedings of the 29th ACM International Conference on Information and Knowledge Management. 2197–2200.
[40] Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, and Lijuan Wang. 2022. Prompting GPT-3 To Be Reliable. (2022). https://doi.org/10.48550/ARXIV.2210.09150
[41] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2016. Improved Representation Learning for Question Answer Matching. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, 464–473. https://doi.org/10.18653/v1/P16-1044
[42] Yi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. 2022. Transformer memory as a differentiable search index. arXiv preprint arXiv:2202.06991 (2022).
[43] Jan Trienes and Krisztian Balog. 2019. Identifying unclear questions in community question answering websites. In Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14–18, 2019, Proceedings, Part I 41. Springer, 276–289.
[44] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. (2023). arXiv:cs.IR/2303.07678
[45] Xiao Wang, Sean MacAvaney, Craig Macdonald, and Iadh Ounis. 2023. Generative Query Reformulation for Effective Adhoc Search. arXiv preprint arXiv:2308.00415 (2023).
[46] Xiao Wang, Craig Macdonald, and Iadh Ounis. 2020. Deep Reinforced Query Reformulation for Information Retrieval. (2020). arXiv:cs.IR/2007.07987
[47] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[48] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. (2023). arXiv:cs.CL/2303.03846
[49] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2021. An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA. (2021). https://doi.org/10.48550/ARXIV.2109.05014
[50] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than Retrieve: Large Language Models are Strong Context Generators. (2023). arXiv:cs.CL/2209.10063
[51] Hamed Zamani and W. Bruce Croft. 2017. Relevance-based Word Embedding. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017, Noriko Kando, Tetsuya Sakai, Hideo Joho, Hang Li, Arjen P. de Vries, and Ryen W. White (Eds.). ACM, 505–514. https://doi.org/10.1145/3077136.3080831
[52] Hamed Zamani, Susan Dumais, Nick Craswell, Paul Bennett, and Gord Lueck. 2020. Generating clarifying questions for information retrieval. In Proceedings of the web conference 2020. 418–428.
[53] George Zerveas, Ruochen Zhang, Leila Kim, and Carsten Eickhoff. 2020. Brown University at TREC Deep Learning 2019. (2020). arXiv:cs.IR/2009.04016
[54] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating Text Generation with BERT. (2019). https://doi.org/10.48550/ARXIV.1904.09675
[55] Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. 2023. CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code. (2023). https://doi.org/10.48550/ARXIV.2302.05527
[56] Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, and Wensheng Zhang. 2023. Leveraging Summary Guidance on Medical Report Summarization. (2023). https://doi.org/10.48550/ARXIV.2302.04001
[57] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for Information Retrieval: A Survey. (2023). arXiv:cs.CL/2308.07107
[58] Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical Query Paraphrasing for Document Retrieval. In COLING 2002: The 19th International Conference on Computational Linguistics. https://aclanthology.org/C02-1161
[59] Simiao Zuo, Qingyu Yin, Haoming Jiang, Shaohui Xi, Bing Yin, Chao Zhang, and Tuo Zhao. 2022. Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites. (2022). arXiv:cs.IR/2209.07584
