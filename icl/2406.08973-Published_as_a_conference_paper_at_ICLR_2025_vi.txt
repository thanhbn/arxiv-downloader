# 2406.08973.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2406.08973.pdf
# Kích thước tệp: 7427590 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
XLAND-100B: MỘT BỘ DỮ LIỆU ĐA NHIỆM VỤ QUY MÔ LỚN
CHO HỌC TĂNG CƯỜNG TRONG NGỮ CẢNH

Alexander Nikulin∗†
AIRI, MIPTIlya Zisman∗
AIRI, SkoltechAlexey Zemtsov∗
NUST MISIS, T-TechVladislav Kurenkov
AIRI, Innopolis University

TÓM TẮT
Tiếp theo thành công của mô hình học trong ngữ cảnh trong các mô hình ngôn ngữ và thị giác máy tính quy mô lớn, lĩnh vực mới nổi gần đây của học tăng cường trong ngữ cảnh đang trải qua sự phát triển nhanh chóng. Tuy nhiên, sự phát triển của nó đã bị cản trở bởi việc thiếu các bài kiểm tra thử thách, do tất cả các thí nghiệm đều được thực hiện trong môi trường đơn giản và trên các bộ dữ liệu quy mô nhỏ. Chúng tôi giới thiệu XLand-100B, một bộ dữ liệu quy mô lớn cho học tăng cường trong ngữ cảnh dựa trên môi trường XLand-MiniGrid, như một bước đầu tiên để giảm bớt vấn đề này. Nó chứa lịch sử học tập hoàn chỉnh cho gần 30.000 nhiệm vụ khác nhau, bao gồm 100B chuyển đổi và 2.5B tập. Phải mất 50.000 giờ GPU để thu thập bộ dữ liệu, điều này vượt quá khả năng tiếp cận của hầu hết các phòng thí nghiệm học thuật. Cùng với bộ dữ liệu, chúng tôi cung cấp các tiện ích để tái tạo hoặc mở rộng nó thêm nữa. Chúng tôi cũng đánh giá các đường cơ sở RL trong ngữ cảnh phổ biến và chỉ ra rằng chúng gặp khó khăn trong việc tổng quát hóa cho các nhiệm vụ mới và đa dạng. Với nỗ lực đáng kể này, chúng tôi nhằm mục đích dân chủ hóa nghiên cứu trong lĩnh vực học tăng cường trong ngữ cảnh đang phát triển nhanh chóng và cung cấp nền tảng vững chắc cho việc mở rộng quy mô hơn nữa.

1 GIỚI THIỆU
Học trong ngữ cảnh, tức là khả năng học các nhiệm vụ mới hoàn toàn dựa trên các ví dụ được đưa ra trong ngữ cảnh trong quá trình suy luận và không cần cập nhật trọng số nào, ban đầu được cho là một đặc tính nổi lên của các mô hình ngôn ngữ lớn, như GPT-3 (Brown et al., 2020). Tuy nhiên, người ta nhanh chóng phát hiện ra rằng các transformer nhỏ cũng có khả năng học trong ngữ cảnh (Kirsch et al., 2022; Von Oswald et al., 2023), và thậm chí nhiều mô hình không phải transformer cũng có khả năng như vậy (Bhattamishra et al., 2023; Akyürek et al., 2024; Park et al., 2024; Grazzi et al., 2024; Vladymyrov et al., 2024; Tong & Pehlevan, 2024). Quan trọng hơn, được thúc đẩy bởi các đặc tính của dữ liệu, thay vì kiến trúc (Chan et al., 2022; Gu et al., 2023), học trong ngữ cảnh không chỉ dành riêng cho mô hình hóa ngôn ngữ và đã được tìm thấy trong các lĩnh vực khác, ví dụ như sinh ảnh (Bai et al., 2023; Najdenkoska et al., 2023; Doveh et al., 2024; Tian et al., 2024).

Tuy nhiên, mặc dù việc áp dụng nhanh chóng kiến trúc transformer trong học tăng cường (RL) sau khi phát hành Decision Transformer (DT) (Chen et al., 2021; Hu et al., 2022; Agarwal et al., 2023; Li et al., 2023), các mô hình có khả năng học trong ngữ cảnh chỉ xuất hiện gần đây. Sự chậm trễ này được gây ra bởi một số lý do. Thứ nhất, để chuyển từ học trong trọng số sang học trong ngữ cảnh, một mô hình phải được huấn luyện trên hàng chục nghìn nhiệm vụ duy nhất (Kirsch et al., 2022). Thật không may, ngay cả các bộ dữ liệu RL lớn nhất hiện tại cũng chỉ chứa hàng trăm nhiệm vụ (Padalkar et al., 2023). Thứ hai, cần phải xác định cách đúng đắn để cung cấp ngữ cảnh cho transformer và phát triển pipeline thu thập dữ liệu, điều này đối với nhiều phương pháp (Laskin et al., 2022; Lee et al., 2023; Shi et al., 2024) khác với những gì thường có sẵn trong các bộ dữ liệu hiện có (xem Phần 3).

Do thiếu các bộ dữ liệu phù hợp và chi phí cao để thu thập dữ liệu trong các môi trường hiện có, làn sóng nghiên cứu RL trong ngữ cảnh gần đây (Laskin et al., 2022; Lee et al., 2023; Norman & Clune, 2023; Kirsch et al., 2023; Sinii et al., 2023; Zisman et al., 2023) đã sử dụng các môi trường với phân phối nhiệm vụ rất đơn giản, nơi có thể thu thập các bộ dữ liệu với hàng trăm nhiệm vụ. Mặc dù các bài kiểm tra này có thể chi trả được, chúng không phù hợp để so sánh các phương pháp ở quy mô lớn trên các nhiệm vụ

∗Đóng góp bằng nhau.
†Liên hệ: nikulin@airi.net. Công việc được thực hiện bởi dunnolab.ai, bắt đầu tại T-Tech.

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
Bảng 1: So sánh với các bộ dữ liệu RL khác.

Dữ liệu # nhiệm vụ # chuyển đổi # tập Kích thước Mã nguồn mở Hỗ trợ ICRL
XLand-100B (của chúng tôi) 28.876 100B 2.5B 320GB ✓ ✓
JAT 157 323M N/A 1TB ✓ ✗
GATO 596 1.5T 63M N/A ✗ ✗
Open X-Embodiment 527 N/A 2.4M 9TB ✓ ✗
AlphaStar Unplugged 1 21B 2.8M N/A ✗ ?
NetHack 1 3.5B 110K 97 GB ✗ ✗
D4RL 11 N/A 40 M 4.8GB ✓ ✗
V-D4RL 4 2.4M N/A 17GB ✓ ✗
D5RL 50 N/A 2500 N/A ✗ ✗
RL Unplugged 90 80M N/A 54.1TB ✓ ✗
Procgen 16 37M N/A 500GB ✓ ✗

có tính đa dạng và độ khó cao, điều này rất quan trọng cho các ứng dụng thực tế. Vì điều này, sự phát triển của RL trong ngữ cảnh hiện tại bị cản trở bởi những yếu tố này. Chúng tôi tin rằng việc giải quyết những rào cản này là rất quan trọng, xét đến vai trò thiết yếu của học trong ngữ cảnh trong con đường đến các mô hình nền tảng và các tác nhân tổng quát thực sự (Team et al., 2021; 2023; Kirsch et al., 2023; Lu et al., 2024; Liu et al., 2024).

Chúng tôi phát hành XLand-100B, một bộ dữ liệu quy mô lớn cho RL trong ngữ cảnh dựa trên môi trường XLand-MiniGrid (Nikulin et al., 2023). Nó chứa lịch sử học tập hoàn chỉnh cho gần 30.000 nhiệm vụ khác nhau, bao gồm 100B chuyển đổi và 2.5B tập. Phải mất 50.000 giờ GPU để thu thập bộ dữ liệu, điều này vượt quá khả năng tiếp cận của hầu hết các phòng thí nghiệm học thuật. Trái ngược với hầu hết các bộ dữ liệu hiện có cho RL, bộ dữ liệu của chúng tôi tương thích với các phương pháp RL học trong ngữ cảnh được sử dụng rộng rãi nhất (xem Phần 3). Với nỗ lực đáng kể này, chúng tôi nhằm mục đích dân chủ hóa nghiên cứu trong lĩnh vực RL trong ngữ cảnh đang phát triển nhanh chóng và cung cấp nền tảng vững chắc cho việc mở rộng quy mô hơn nữa.

Cùng với bộ dữ liệu chính, chúng tôi cung cấp một phiên bản nhỏ hơn và đơn giản hơn để thử nghiệm nhanh hơn, cũng như các tiện ích để tái tạo hoặc mở rộng các bộ dữ liệu thêm nữa. Chúng tôi mô tả cẩn thận toàn bộ quy trình thu thập dữ liệu (xem Phần 4), cung cấp tất cả các chi tiết cần thiết về thuật toán được sử dụng để thu thập, lọc và gắn nhãn lại với hành động chuyên gia (xem Phần 4.2). Chúng tôi phân tích bộ dữ liệu kết quả để đảm bảo rằng chúng tôi đã đáp ứng tất cả các yêu cầu cho RL trong ngữ cảnh (xem Phần 4.3). Ngoài ra, chúng tôi đã tiến hành các thí nghiệm sơ bộ với các đường cơ sở thông thường trên các bộ dữ liệu đã thu thập, cho thấy vẫn cần nhiều nghiên cứu để cải thiện khả năng thích ứng trong ngữ cảnh trên các nhiệm vụ phức tạp (xem Phần 5).

2 KIẾN THỨC NỀN TẢNG

2.1 HỌC TĂNG CƯỜNG TRONG NGỮ CẢNH

Nhiều phương pháp cho RL trong ngữ cảnh đã ra đời, mỗi phương pháp cung cấp một cách khác nhau để huấn luyện và tổ chức ngữ cảnh (Laskin et al., 2022; Lee et al., 2023; Mirchandani et al., 2023; Liu & Abbeel, 2023; Raparthy et al., 2023; Shi et al., 2024). Chúng tôi tập trung vào Algorithm Distillation (AD) (Laskin et al., 2022) và Decision-Pretrained Transformer (DPT) (Lee et al., 2023), mà chúng tôi chọn làm phương pháp chính cho công việc của mình do tính đơn giản và tính tổng quát của chúng.

Algorithm Distillation. AD (Laskin et al., 2022) là một trong những phương pháp đầu tiên chỉ ra rằng học trong ngữ cảnh có thể thực hiện được trong RL, và nắm bắt được chi tiết của nhiều phương pháp gần đây khác (Mirchandani et al., 2023; Liu & Abbeel, 2023; Shi et al., 2024) trong khi vẫn rất đơn giản. Nó huấn luyện một transformer, hoặc bất kỳ mô hình chuỗi nào khác, để dự đoán hành động tiếp theo một cách tự hồi quy dựa trên lịch sử của các tương tác trước đó, tức là quan sát, hành động và phần thưởng. Để chuyển từ học trong trọng số sang học trong ngữ cảnh, điều quan trọng là ngữ cảnh phải chứa nhiều tập được sắp xếp theo thứ tự tăng dần của lợi nhuận, điều này khác với cách thực hiện trong các phương pháp giống DT (Chen et al., 2021; Janner et al., 2021; Lee et al., 2022).

Decision-Pretrained Transformer. DPT là một cách tiếp cận thay thế được lấy cảm hứng từ xấp xỉ suy luận Bayesian (Müller et al., 2021). Không giống như AD, nó huấn luyện một transformer để dự đoán hành động tối ưu cho một trạng thái truy vấn dựa trên một ngữ cảnh ngẫu nhiên, cụ thể cho nhiệm vụ. Nghĩa là, ngữ cảnh không cần phải được sắp xếp, mà chỉ cần chứa các chuyển đổi thuộc cùng một nhiệm vụ. Do đó, DPT yêu cầu truy cập vào các hành động tối ưu, nhưng không yêu cầu bộ dữ liệu về lịch sử học tập.

Ngoài ra, các phân tích lý thuyết của các phương pháp AD và DPT (Lin et al., 2023; Wang et al., 2024) cho thấy chúng có thể thực hiện các thuật toán RL trực tuyến gần tối ưu như Lin-UCB, Thompson sampling hoặc thậm chí các phương pháp temporal difference (TD) chỉ trong quá trình lan truyền thuận.

2.2 XLAND-MINIGRID

Hình 1: Hình ảnh hóa một môi trường XLand-MiniGrid chung. Bố cục lưới phải được chọn trước, trong khi vị trí của các đối tượng được ngẫu nhiên hóa mỗi lần khởi tạo lại. Đối với bộ dữ liệu, chúng tôi sử dụng bố cục đơn giản hơn với chỉ một phòng, xem Phụ lục M.

Bắt đầu từ công trình đột phá của Wang et al. (2016); Duan et al. (2016); Finn et al. (2017) về meta-RL, phần lớn công trình tiếp theo (Zintgraf et al., 2019; Melo, 2022; Grigsby et al., 2023; Lu et al., 2024; Shala et al., 2024; Beck et al., 2024) đã tập trung vào các môi trường có phân phối nhiệm vụ rất đơn giản, hoặc có phân phối nhiệm vụ khó rất nhỏ và hạn chế. Điều này là do, để tổng quát hóa trong meta-RL, việc huấn luyện cần được thực hiện trên nhiều nhiệm vụ khác nhau, làm tăng đáng kể chi phí và thời gian cần thiết cho thí nghiệm. Gần đây, Nikulin et al. (2023) đã phát hành XLand-MiniGrid, một môi trường được tăng tốc GPU và các bài kiểm tra triệu nhiệm vụ đã giảm đáng kể rào cản gia nhập cho nghiên cứu meta-RL. Chúng tôi sẽ mô tả ngắn gọn nó ở đây.

Môi trường. XLand-MiniGrid là một phiên bản viết lại hoàn toàn của MiniGrid (Chevalier-Boisvert et al., 2023) trong JAX (Bradbury et al., 2018), tích hợp khái niệm về quy tắc và mục tiêu từ XLand (Team et al., 2023). Tận dụng JAX, nó có thể chạy trên bộ tăng tốc GPU hoặc TPU với tốc độ hàng triệu bước mỗi giây. Về cốt lõi, nó là một môi trường thế giới lưới hướng mục tiêu với động lực học cơ bản đơn giản, khả năng quan sát một phần và phần thưởng thưa. Không gian hành động rất đơn giản, chủ yếu bao gồm di chuyển và tương tác với các đối tượng trò chơi, như mở cửa hoặc nhặt và đặt vật phẩm. Quan sát là các "hình ảnh" biểu tượng mã hóa môi trường xung quanh tác nhân dưới dạng ID ô và màu sắc. Quy tắc là các hàm có thể thay đổi trạng thái của môi trường dựa trên một số điều kiện, ví dụ như khi hai đối tượng cụ thể được đặt gần nhau, cả hai sẽ biến mất và một đối tượng mới được đặt. Mục tiêu tương tự, ngoại trừ chúng chỉ xác thực một số điều kiện được xác định trước và không thay đổi gì. Kết hợp các quy tắc và mục tiêu khác nhau với nhau, chúng ta có thể tạo ra các nhiệm vụ mới với các hàm phần thưởng và động lực học khác nhau. Để mô tả chi tiết hơn, chúng tôi tham khảo Nikulin et al. (2023).

Bài kiểm tra. Cùng với bản thân môi trường, Nikulin et al. (2023) đã phát hành một công cụ để tạo ra một số lượng lớn các nhiệm vụ duy nhất theo thủ tục với các mức độ khó khác nhau. Mỗi nhiệm vụ được biểu diễn bằng một cây nhị phân, trong đó gốc là mục tiêu cần đạt được và các nút còn lại xác định các quy tắc của môi trường được kích hoạt theo trình tự lặp lại. Để chuẩn hóa so sánh, bốn bài kiểm tra được lấy mẫu trước với độ đa dạng tăng dần được cung cấp: trivial, small, medium, high, mỗi bài có một triệu nhiệm vụ duy nhất. Đối với công việc này, chúng tôi chọn medium làm điểm giữa giữa bài kiểm tra high chưa được giải quyết và bài kiểm tra small ít thách thức hơn. Chúng tôi cũng sử dụng trivial cho phiên bản bộ dữ liệu nhỏ hơn và đơn giản hơn (xem Phần 4).

3 MẢNH GHÉP THIẾU CHO RL TRONG NGỮ CẢNH

Để huấn luyện thành công một tác nhân trong ngữ cảnh, dữ liệu huấn luyện phải đáp ứng một số tiêu chí nhất định. Để bắt đầu, dữ liệu phải được tạo thành từ lịch sử học tập thực tế (Laskin et al., 2022) hoặc các xấp xỉ của chúng (Zisman et al., 2023), chứa đủ các giai đoạn khám phá và khai thác của việc học. Học chỉ với các quỹ đạo chuyên gia sẽ không đủ để khả năng trong ngữ cảnh xuất hiện, vì một tác nhân cần biết lịch sử cải thiện chính sách (Laskin et al., 2022; Kirsch et al., 2023). Một cách tiếp cận khác là học từ các hành động tối ưu như được đề xuất bởi Lee et al. (2023), nhưng không rõ làm thế nào để truy cập các chính sách tối ưu để có được chúng. Bên cạnh đó, dữ liệu cần chứa hàng nghìn nhiệm vụ khác nhau để học từ đó (Kirsch et al., 2022). Nghĩa là, đối với một nhiệm vụ đơn giản tìm hai hình vuông trên lưới 9×9, một tác nhân cần thấy khoảng 2000 tổ hợp mục tiêu khác nhau để bắt đầu thích ứng với các

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
Bảng 2: Thống kê mô tả của các bộ dữ liệu XLand.

Bộ dữ liệu XLand-Trivial-20B XLand-100B
Tập 868.805.556 2.500.152.898
Chuyển đổi 19.496.960.000 112.598.843.392
Độ dài lịch sử 60.928 121.856
Số nhiệm vụ 10.000 28.876
Số quy tắc tối đa 0 9
Hình dạng quan sát (5, 5) (5, 5)
Số hành động 6 6
Lợi nhuận cuối trung bình 0,915 0,894
Lợi nhuận cuối trung vị 0,948 0,925
Chuyển đổi tập trung vị 22,45 57,75
Kích thước đĩa (nén) 60 GB 326 GB

vị trí chưa thấy (Laskin et al., 2022). Vì dữ liệu như vậy chưa bao giờ được thu thập và đưa vào một bộ dữ liệu duy nhất, tất cả các nhà thực hành RL trong ngữ cảnh hiện tại đều buộc phải tự tạo dữ liệu, điều này không thể tránh khỏi làm tăng thêm sự phức tạp trong việc tái tạo các phương pháp.

Bên cạnh đó, việc thu thập hàng nghìn tập khác nhau trong ngữ cảnh đòi hỏi một cam kết đáng kể, vì huấn luyện nhiều tác nhân RL tốn kém về thời gian và tài nguyên. Đối với vấn đề đó, dữ liệu được sử dụng trong nghiên cứu hiện tại được thu thập trong các môi trường rất đơn giản với mục tiêu trực tiếp, như đến một mục tiêu cụ thể trên bản đồ (Laskin et al., 2022; Lee et al., 2023; Zisman et al., 2023) hoặc để áp dụng lực lên các bộ truyền động để đi bộ một robot (Kirsch et al., 2023). Điều này làm chậm đáng kể tốc độ nghiên cứu RL trong ngữ cảnh, vì không chỉ khó kiểm tra khả năng áp dụng của các phương pháp được đề xuất, mà còn chưa khả thi để xác định các quy luật mở rộng trong những môi trường này.

Để cung cấp một bức tranh hoàn chỉnh cho người đọc, chúng tôi thảo luận ngắn gọn về các bộ dữ liệu hiện có và nêu bật lý do tại sao chúng không phù hợp để huấn luyện các tác nhân RL trong ngữ cảnh. Để đơn giản, chúng tôi phân chúng thành hai nhóm: các bộ dữ liệu cổ điển được thiết kế cho offline-RL và các bộ dữ liệu được thu thập cho học có giám sát quy mô lớn. Lưu ý rằng việc phân loại này có bản chất mờ nhạt và chỉ phục vụ để hiểu rõ hơn về cấu trúc hiện tại trong dữ liệu RL.

Bộ dữ liệu Offline RL. Các bộ dữ liệu trong danh mục này có thể được coi là cổ điển, vì một số trong số chúng đã tồn tại hơn bốn năm (Fu et al., 2020). Chúng ban đầu được đề xuất cho offline RL, chứa các nhiệm vụ đơn giản với cấu trúc phẳng, ví dụ như thực hiện di chuyển với các robot khác nhau (Lu et al., 2023) hoặc tìm đường trong mê cung. Một số trong số chúng cũng chứa dữ liệu từ các robot thao tác (Fu et al., 2020; Rafailov et al., 2023), hoặc thậm chí các khung hình Atari (Gulcehre et al., 2021). Các bộ dữ liệu khác thu thập dữ liệu cho các môi trường phức tạp hơn, như NetHack Learning Environment (Hambro et al., 2023; Kurenkov et al., 2024) hoặc ProcGen (Cobbe et al., 2019; Mediratta et al., 2024). Tuy nhiên, các bộ dữ liệu nêu trên cung cấp <100 nhiệm vụ khác nhau với chính sách cố định (ngoại trừ các bộ dữ liệu replay, có phạm vi bao phủ hạn chế của các chính sách khác nhau). Hạn chế này làm cho khó khăn cho RL trong ngữ cảnh xuất hiện từ dữ liệu như vậy. Để khắc phục tính bất lợi này, chúng tôi đã thu thập gần 30.000 nhiệm vụ với cấu trúc bộ quy tắc sâu, đây là một vấn đề thách thức để giải quyết.

Huấn luyện trước có giám sát quy mô lớn. Tiến bộ gần đây trong các tác nhân tổng quát, có thể giải quyết nhiều môi trường, đã được thực hiện nhờ các bộ dữ liệu lớn. Bộ dữ liệu GATO (Reed et al., 2022), tuy nhiên không được phát hành công khai, bao gồm 1,5 nghìn tỷ chuyển đổi cùng với 596 nhiệm vụ, làm cho nó trở thành một trong những bộ dữ liệu lớn nhất trong RL. Bộ dữ liệu tương tự mã nguồn mở, bộ dữ liệu JAT (Gallouédec et al., 2024), nhỏ hơn với 157 nhiệm vụ và 300 triệu chuyển đổi, nhưng nó cung cấp hiệu suất tương đương trên hầu hết các bài kiểm tra. Cả hai bộ dữ liệu đều chứa các minh chứng RL chuyên gia từ BabyAI (Hui et al., 2020), trò chơi Atari (Bellemare et al., 2013), Meta-World (Yu et al., 2020) và nhiều hơn nữa.

Một bộ dữ liệu lớn khác, Open X-Embodiment (Padalkar et al., 2023), là sự kết hợp của hơn 60 bộ dữ liệu từ các phòng thí nghiệm nghiên cứu robot khác nhau. Nó bao gồm 527 nhiệm vụ khác nhau trong robot với các minh chứng chủ yếu từ các chuyên gia con người. Mặc dù có số lượng lớn chuyển đổi trong các bộ dữ liệu này, chúng không chứa lịch sử học tập với các chính sách cải thiện, làm cho việc áp dụng chúng cho RL trong ngữ cảnh khá thách thức. Ngược lại, bộ dữ liệu XLand-100B của chúng tôi bao gồm 100 tỷ chuyển đổi từ lịch sử học tập của các tác nhân RL, làm cho khả năng trong ngữ cảnh có thể xuất hiện.

Bộ dữ liệu duy nhất có thể áp dụng được để sử dụng cho RL trong ngữ cảnh là AlphaStar Unplugged (Mathieu et al., 2023). Mặc dù các tác giả ban đầu không có kế hoạch thu thập một bộ dữ liệu phù hợp, dữ liệu có thể được sắp xếp theo MMR của người chơi (tương tự như xếp hạng Elo). Việc sắp xếp này có thể được coi là sự cải thiện chính sách ổn định, do đó cho phép khả năng RL trong ngữ cảnh. Để biết thêm chi tiết về các bộ dữ liệu, tham khảo Bảng 1.

4 BỘ DỮ LIỆU XLAND-100B

Chúng tôi trình bày XLand-100B, một bộ dữ liệu lớn cho RL trong ngữ cảnh, và phiên bản nhỏ hơn và đơn giản hơn XLand-Trivial-20B để thử nghiệm nhanh hơn. Cùng nhau chúng chứa khoảng 3,5B tập, 130B chuyển đổi và 40.000 nhiệm vụ duy nhất (xem Bảng 2 để biết thống kê chi tiết). Các bộ dữ liệu được lưu trữ trên bucket S3 công cộng và sẽ có sẵn miễn phí cho mọi người dưới giấy phép CC BY-SA 4.0. Tiếp theo, chúng tôi mô tả định dạng dữ liệu, thu thập và đánh giá.

4.1 ĐỊNH DẠNG DỮ LIỆU

Định dạng lưu trữ. Chúng tôi chọn lưu trữ các bộ dữ liệu ở định dạng tệp HDF5 dựa trên tính phổ biến và tiện lợi của nó. Nó cho phép làm việc với lượng lớn dữ liệu có cấu trúc mà không cần tải vào bộ nhớ, lưu trữ siêu dữ liệu tùy ý, và tùy chỉnh nén và kích thước chunk để tối đa hóa thông lượng lấy mẫu. Chúng tôi sử dụng nén gzip với cường độ nén mặc định là 6, điều này đã giảm kích thước bộ dữ liệu từ gần 5TB+ xuống chỉ ~600GB cho bộ dữ liệu chính của chúng tôi. Sử dụng một mẹo nhỏ được mô tả sau, chúng tôi đã có thể giảm kích thước thêm nữa xuống chỉ còn 326 GB (xem Bảng 2). Tuy nhiên, việc sử dụng nén một cách ngây thơ có thể làm tăng đáng kể thời gian lấy mẫu batch và làm chậm thời gian huấn luyện tổng thể. Chúng tôi đã điều chỉnh kích thước chunk bộ nhớ cache HDF5 đặc biệt để tối đa hóa thông lượng lấy mẫu cho độ dài chuỗi lớn. Sau khi điều chỉnh, chúng tôi đạt được tốc độ tăng gấp bốn lần so với nén ngây thơ, và chỉ chậm hơn hai lần so với không nén. Xét rằng chúng tôi đã giảm kích thước bộ dữ liệu đi 15 lần, chúng tôi thấy đây là một sự đánh đổi tốt, làm tăng khả năng chi trả tổng thể của bộ dữ liệu. Xem Phụ lục C để biết các bài kiểm tra thông lượng.

Định dạng dữ liệu và siêu dữ liệu. Chúng tôi thu thập lịch sử học tập hoàn chỉnh, tức là đối với mỗi lịch sử chúng tôi lưu trữ tất cả quan sát, hành động, phần thưởng và dones gặp phải trong quá trình huấn luyện tác nhân trong các nhóm HDF5 riêng biệt với ID duy nhất cho mỗi lịch sử (xem Phụ lục B để biết thêm chi tiết). Để tương thích với các phương pháp giống DPT (Lee et al., 2023), chúng tôi cũng lưu trữ hành động chuyên gia cho mỗi chuyển đổi (xem Phần 4.2). Không giống như các định dạng phổ biến như RLDS và Minari, chúng tôi lưu trữ các chuyển đổi như một mảng tuần tự cho mỗi lịch sử cho mỗi phương thức. Lý do ở đây là dưới nén, việc lấy mẫu các lát cắt của các tập dài theo trình tự sẽ rẻ hơn nhiều so với lấy mẫu trên các nhóm khác nhau.

Chúng tôi cũng lưu trữ quan sát một cách hiệu quả để giảm thêm kích thước bộ dữ liệu. Thay vì lưu trữ hai kênh cho ô và màu sắc, chúng tôi ánh xạ các chỉ số của chúng vào tích Cartesian của màu sắc và ô, giảm một nửa kích thước lưu trữ. Chúng có thể được giải mã dễ dàng trong quá trình lấy mẫu mà không có bất kỳ chi phí nào với hàm divmod. Ngoài ra, đối với mỗi lịch sử chúng tôi lưu trữ ID môi trường XLand-MiniGrid, ID bài kiểm tra và ID bộ quy tắc, có thể được sử dụng sau để lọc bộ dữ liệu, ví dụ dựa trên độ phức tạp của các nhiệm vụ, chia thành train và test hoặc thiết lập môi trường để đánh giá.

4.2 THU THẬP DỮ LIỆU

Ở mức cao, việc thu thập dữ liệu được tổ chức thành ba giai đoạn, cụ thể là huấn luyện trước đa nhiệm vụ có điều kiện nhiệm vụ; tinh chỉnh một nhiệm vụ để thu thập lịch sử học tập; và cuối cùng là hậu xử lý và lọc. Mặc dù chúng tôi đã sử dụng các triển khai được tối ưu hóa GPU cao của thuật toán RL cơ bản và môi trường, vẫn mất 50.000 giờ GPU để thu thập bộ dữ liệu đầy đủ. Việc thu thập một bộ dữ liệu có kích thước này cho bất kỳ môi trường nào khác phù hợp cho RL trong ngữ cảnh, ví dụ như Meta-World (Yu et al., 2020), sẽ mất nhiều thời gian hơn, điều này khó có thể khả thi đối với hầu hết các nhà thực hành (Nikulin et al.,

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

0 12,5B 25B
Chuyển đổi0.10.20.30.4Lợi nhuận
Hình 2: Lợi nhuận đánh giá cho việc huấn luyện trước đa nhiệm vụ có điều kiện mục tiêu với PPO lặp lại trên 65k nhiệm vụ. Tác nhân được huấn luyện trước đã được sử dụng tiếp theo như điểm bắt đầu cho việc tinh chỉnh một nhiệm vụ trong quá trình thu thập bộ dữ liệu.

0 0,5B 1B
Chuyển đổi0.00.51.0Lợi nhuậnTừ đầu
Được huấn luyện trước

Hình 3: Các đường cong đánh giá một nhiệm vụ trên 36 nhiệm vụ khó cho các chính sách được huấn luyện từ đầu hoặc tinh chỉnh từ các checkpoint đa nhiệm vụ được huấn luyện trước. Xem Phụ lục D để biết các đường cong trên nhiệm vụ ở mọi mức độ khó.

0123456789
Số quy tắc02k4k6kBan đầu
Đã lọc

Hình 4: Phân phối các nhiệm vụ theo độ khó được lấy mẫu ban đầu và trong bộ dữ liệu kết quả. Để đảm bảo chất lượng, chúng tôi đã lọc các nhiệm vụ có lợi nhuận cuối dưới 0,3 hoặc dữ liệu bị hỏng do một số lỗi trong quá trình huấn luyện.

2023). Tiếp theo, chúng tôi mô tả quá trình thu thập, bao gồm việc lựa chọn thuật toán RL cơ bản và tất cả các bước tiếp theo. Chúng tôi cung cấp các siêu tham số chính xác cho mỗi giai đoạn trong Phụ lục O.

Thuật toán cơ bản. Đối với các bộ dữ liệu của chúng tôi, chúng tôi chọn PPO (Schulman et al., 2017), do khả năng mở rộng cao và tính tương thích với các môi trường song song quy mô lớn. Chúng tôi đã chuyển đổi việc triển khai từ PPO lặp lại được cung cấp bởi (Nikulin et al., 2023), tùy chỉnh nó để đáp ứng nhu cầu của chúng tôi. Chúng tôi đã thêm callback để lưu các chuyển đổi trong quá trình huấn luyện và mở rộng kiến trúc tác nhân để lấy mã hóa bộ quy tắc như một điều kiện tùy chọn cho việc huấn luyện trước. Chúng tôi sử dụng GRU (Cho et al., 2014) cho bộ nhớ, vì nó cho thấy hiệu suất hài lòng trong các thí nghiệm sơ bộ. Vì thuật toán cơ bản được triển khai trong JAX (Bradbury et al., 2018), chúng tôi có thể biên dịch just-in-time toàn bộ vòng lặp huấn luyện, đạt được 1M bước mỗi giây trong quá trình huấn luyện trên một GPU với độ chính xác hỗn hợp được bật. Vì PPO không phải là thuật toán hiệu quả mẫu nhất, vẫn cần thiết phải huấn luyện nó trên hàng tỷ chuyển đổi. May mắn thay, vì chúng tôi huấn luyện nó trên hàng nghìn môi trường song song, lịch sử học tập cho mỗi môi trường cụ thể khá ngắn, tức là khoảng 120k chuyển đổi.

Huấn luyện trước. Đối với bộ dữ liệu XLand-100B chính của chúng tôi, chúng tôi lấy mẫu đồng đều các nhiệm vụ từ bài kiểm tra medium-1m từ XLand-MiniGrid. Nó chứa các nhiệm vụ có độ khó khác nhau, từ không đến chín quy tắc. Thật không may, trên nhiều nhiệm vụ khó, thuật toán cơ bản của chúng tôi không thể hội tụ trong ngân sách thời gian được phân bổ cho một lần chạy huấn luyện duy nhất. Trên các nhiệm vụ khó nhất, thậm chí không thể có được phần thưởng khác không, do thách thức khám phá mà các nhiệm vụ như vậy đặt ra. Để tăng tốc hội tụ và khám phá trên các nhiệm vụ khó hơn, chúng tôi huấn luyện trước một tác nhân theo cách đa nhiệm vụ có điều kiện nhiệm vụ và cũng sử dụng bố cục lưới đơn giản hơn với chỉ một phòng (xem Phụ lục M). Chúng tôi tiết lộ đặc tả bộ quy tắc, thường được ẩn khỏi tác nhân, và mã hóa mục tiêu và quy tắc qua embedding, nối các mã hóa kết quả và truyền nó như một đầu vào bổ sung cho tác nhân. Sau đó, chúng tôi huấn luyện tác nhân trên 65k nhiệm vụ đồng thời trong 25B chuyển đổi. Như Hình 2 cho thấy, một tác nhân như vậy học cách tổng quát zero-shot trên các nhiệm vụ mới khá tốt, mặc dù chúng tôi không nhằm mục đích đẩy nó đến giới hạn, vì tổng quát hóa zero-shot không tạo ra lịch sử học tập mượt mà trong quá trình tinh chỉnh. Chúng tôi bỏ qua giai đoạn này cho bộ dữ liệu XLand-Trivial-20B do tính đơn giản của các nhiệm vụ trong bài kiểm tra trivial-1m.

Tinh chỉnh. Đây là giai đoạn quan trọng trong quá trình thu thập dữ liệu, trong đó chúng tôi tinh chỉnh một tác nhân được huấn luyện trước trong khi ghi lại các chuyển đổi gặp phải vào bộ dữ liệu. Chúng tôi tinh chỉnh tác nhân sử dụng 8192 môi trường song song cho 1B chuyển đổi trên 30k nhiệm vụ được lấy mẫu đồng đều từ bài kiểm tra medium-1m. Chúng tôi che giấu mã hóa điều kiện nhiệm vụ để ngăn chặn tổng quát hóa zero-shot. Chúng tôi ghi lại chuyển đổi chỉ từ 32 môi trường song song đầu tiên. Bằng cách này, chúng tôi có thể giữ kích thước bộ dữ liệu có thể quản lý được, vẫn để lại khả năng nghiên cứu quy luật mở rộng và tổng quát hóa một cách có kiểm soát. Ví dụ, chúng tôi có thể huấn luyện AD (Laskin et al., 2022) trên tất cả 30k nhiệm vụ sử dụng một lịch sử cho mỗi nhiệm vụ hoặc trên ~900 nhiệm vụ sử dụng tất cả lịch sử cho mỗi nhiệm vụ để cân bằng số lượng token huấn luyện. Đối với bộ dữ liệu XLand-Trivial-20B, thay vì tinh chỉnh, chúng tôi huấn luyện tác nhân từ đầu trên 10k nhiệm vụ được lấy mẫu đồng đều từ bài kiểm tra trivial-1m, giữ tất cả các siêu tham số khác giống nhau. Trong Hình 3, chúng tôi cho thấy hiệu ứng của việc tinh chỉnh trên các nhiệm vụ khó (với hơn bảy quy tắc) so với huấn luyện từ đầu. Có thể thấy rằng chúng tôi có thể cho thấy hiệu suất mạnh mẽ ngay cả trên các nhiệm vụ khó nhất, làm tăng tính đa dạng và phạm vi bao phủ của bộ dữ liệu kết quả. Để biết cùng kết quả trên nhiệm vụ ở mọi mức độ khó, xem Phụ lục D.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

0,0 0,5 1,0
Số tập chuẩn hóa0.000.250.500.751.00Lợi nhuận
3 quy tắc
5 quy tắc7 quy tắc
9 quy tắc

Hình 5: Lịch sử học tập cho bộ dữ liệu XLand-100B được phân tách theo số quy tắc. Để rõ ràng về mặt hình ảnh, chúng tôi chỉ hiển thị một mẫu số quy tắc có thể và chuẩn hóa số tập, vì chúng có thể khác nhau đáng kể.

Hậu xử lý. Sau khi tinh chỉnh, cần thiết phải gắn nhãn thêm các chuyển đổi với hành động chuyên gia để hỗ trợ các phương pháp giống DPT (Lee et al., 2023). Để làm điều này, chúng tôi đi qua toàn bộ lịch sử học tập với chính sách cuối cùng, bắt đầu từ trạng thái ẩn ban đầu cho RNN. Chúng tôi đánh giá tính hợp lệ của lược đồ gắn nhãn như vậy sau đó trong Phần 4.3. Cuối cùng, tất cả lịch sử học tập riêng lẻ từ các nhiệm vụ khác nhau được kết hợp thành một bộ dữ liệu lớn. Để đảm bảo chất lượng, chúng tôi lọc bỏ bất kỳ nhiệm vụ nào có lợi nhuận cuối dưới 0,3 như một lịch sử học tập không đại diện. Có một số lỗi, như GPU bị lỗi, điều này không thể tránh khỏi trong quá trình huấn luyện quy mô lớn. Vì vậy, bất kỳ lần chạy nào có dữ liệu bị hỏng cũng được lọc bỏ. Tổng cộng, chúng tôi đã lọc bỏ khoảng 1k nhiệm vụ, để lại gần 29k nhiệm vụ trong bộ dữ liệu cuối cùng. Chúng tôi cung cấp thống kê chi tiết cho mỗi bộ dữ liệu trong Bảng 2 và phân phối cuối cùng của các nhiệm vụ theo số quy tắc trong Hình 4.

4.3 ĐÁNH GIÁ DỮ LIỆU

Trong phần này, chúng tôi xác thực rằng bộ dữ liệu XLand-100B kết quả thực sự đáp ứng hai yêu cầu quan trọng nhất cho RL trong ngữ cảnh, cụ thể là nó chứa lịch sử học tập với mô hình cải thiện chính sách riêng biệt và có hành động chuyên gia cho mỗi chuyển đổi (xem Phần 3 để thảo luận). Chúng tôi cung cấp kết quả tương tự cho XLand-Trivial-20B trong Phụ lục E.

0k 25k 50k 75k 100k 125k
Chuyển đổi0.50.60.70.80.91.0Thỏa thuận hành động

Hình 6: Thỏa thuận giữa các hành động được dự đoán bởi chuyên gia và các hành động thực tế trong lịch sử học tập. Chúng tôi sử dụng chính sách PPO cuối cùng như một chuyên gia để gắn nhãn hành động. Như có thể thấy, thỏa thuận đang tăng lên về phía cuối lịch sử học tập.

Lịch sử cải thiện. Trong Hình 5, chúng tôi cho thấy lợi nhuận trung bình từ lịch sử học tập được phân tách theo số quy tắc. Để thể hiện tốt hơn tốc độ học tập trên cùng một thang đo, chúng tôi đã chuẩn hóa trục x cho mỗi mức độ khó, vì số tập có thể khác nhau rất lớn (vì phải mất nhiều thời gian hơn để giải quyết các nhiệm vụ phức tạp). Có thể thấy rằng bộ dữ liệu cung cấp toàn bộ phạm vi tốc độ học tập, từ rất nhanh trên các vấn đề dễ đến chậm hơn nhiều trên các vấn đề khó nhất, điều này có thể quan trọng đối với các phương pháp dựa trên AD (Zisman et al., 2023; Shi et al., 2024). Để biết lịch sử học tập được tính trung bình trên toàn bộ bộ dữ liệu, xem Phụ lục E.

Gắn nhãn lại hành động chuyên gia. Trái ngược với AD, dự đoán hành động tiếp theo từ chính quỹ đạo, các phương pháp giống DPT yêu cầu truy cập vào hành động tối ưu trên mỗi chuyển đổi để dự đoán. Tuy nhiên, đối với hầu hết các vấn đề không tầm thường hoặc thực tế, việc có được hành động tối ưu thực sự với số lượng lớn khó có thể thực hiện được. Gần đây, Lin et al. (2023) đã giới thiệu lược đồ DPT xấp xỉ, trong đó hành động chuyên gia được ước tính từ toàn bộ lịch sử bằng một thuật toán nào đó. Chúng tôi đã triển khai lược đồ này vì thiếu các lựa chọn thay thế rõ ràng. Tuy nhiên, chúng tôi phải đảm bảo rằng việc gắn nhãn như vậy là phù hợp trong trường hợp của chúng tôi và chuyên gia ở cuối dự đoán các hành động gần với những gì chính sách thực sự đã làm gần cuối quá trình huấn luyện. Điều này không rõ ràng, vì trong quá trình gắn nhãn, nó có thể phân kỳ vào các trạng thái ẩn ngoài phân phối cho RNN. Trong Hình 6, chúng tôi cho thấy trên XLand-100B, sự thỏa thuận giữa các hành động được dự đoán bởi chuyên gia và các hành động thực tế tăng lên gần cuối lịch sử học tập, có nghĩa là chuyên gia không phân kỳ trong quá trình gắn nhãn lại.

5 THÍ NGHIỆM

Trong phần này, chúng tôi điều tra xem các bộ dữ liệu của chúng tôi có thể tạo ra khả năng RL trong ngữ cảnh hay không. Ngoài ra, chúng tôi chứng minh các thuật toán trong ngữ cảnh hiện tại hoạt động tốt như thế nào trên các độ phức tạp nhiệm vụ khác nhau và nêu bật các hạn chế hiện tại của chúng. Chúng tôi lấy AD (Laskin et al., 2022) và DPT (Lee et al., 2023) cho các thí nghiệm của mình, các chi tiết triển khai chính xác có trong Phụ lục F và Phụ lục G. Cả hai phương pháp đều được huấn luyện trên XLand-Trivial-20B và XLand-100B với độ dài ngữ cảnh {512,1024,2048,4096} và {1024,2048,4096} tương ứng. Chúng tôi không bao gồm độ dài ngữ cảnh 512 cho

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

0100 200 300 400 500
Tập0.20.40.60.81.0Lợi nhuận# quy tắc =1
Dữ liệu
Mô hình

0100 200 300 400 500
Tập0.000.250.500.751.00Lợi nhuận# quy tắc =3
Dữ liệu
Mô hình

0100 200 300 400 500
Tập0.000.250.500.751.00Lợi nhuận# quy tắc =6
Dữ liệu
Mô hình

0100 200 300 400 500
Tập0.2
0.00.20.40.60.8Lợi nhuận# quy tắc =9
Dữ liệu
Mô hình

Hình 7: So sánh lịch sử học tập trong bộ dữ liệu -100B với hiệu suất AD trên cùng các nhiệm vụ huấn luyện. AD có thể giải quyết các nhiệm vụ đơn giản, tuy nhiên hiệu suất của nó giảm khi các bộ quy tắc trở nên sâu hơn. Độ dài ngữ cảnh của mô hình là 1024. Các tham số đánh giá, ngoại trừ nhiệm vụ huấn luyện, giống như trong Hình 8.

bộ dữ liệu -100B vì chúng tôi coi nó quá ngắn, xét đến độ dài tập trung vị tăng gần 3 lần giữa hai bộ dữ liệu. Để đánh giá, chúng tôi chạy ba mô hình trên 1024 nhiệm vụ chưa thấy trong 500 tập.

AD. Algorithm Distillation cho thấy sự xuất hiện của khả năng trong ngữ cảnh trong quá trình huấn luyện trên cả hai bộ dữ liệu. Hình 8 chứng minh hiệu suất của phương pháp cho các độ dài ngữ cảnh khác nhau. Trên bộ dữ liệu -Trivial-20B, nó có thể cho thấy sự cải thiện chính sách ổn định từ khoảng 0,28 đến 0,4 trong quá trình đánh giá. Đối với -100B, hiệu suất tương tự, nhưng tốc độ cải thiện nhanh hơn. Chúng tôi giả thuyết rằng điều này xảy ra do phạm vi bao phủ dữ liệu rộng hơn, vì tác nhân nhìn thấy các nhiệm vụ phức tạp hơn và có thể học nhanh hơn từ chúng.

Hình 8: Hiệu suất AD trên các bộ dữ liệu của chúng tôi cho các độ dài chuỗi khác nhau. Cả hai bộ dữ liệu đều dẫn đến sự xuất hiện của khả năng trong ngữ cảnh. Chúng tôi báo cáo lợi nhuận trung bình trên 1024 nhiệm vụ chưa thấy qua 3 seed.

Để kiểm tra thêm hiệu suất trên bộ dữ liệu -100B, chúng tôi đánh giá AD trên các nhiệm vụ huấn luyện từ bộ dữ liệu và phân tách hiệu suất dựa trên độ phức tạp của các nhiệm vụ. Độ phức tạp được xác định bởi số quy tắc một tác nhân cần kích hoạt trước khi hoàn thành thành công. Như được thể hiện trong Hình 7, AD có thể chứng minh khả năng trong ngữ cảnh trên các nhiệm vụ đơn giản, nhưng nó gặp khó khăn với những nhiệm vụ phức tạp hơn. Chúng tôi suy đoán rằng kết quả của chúng tôi chưa phải là cuối cùng, vì AD vẫn chưa hoàn toàn có khả năng học cách giải quyết các nhiệm vụ huấn luyện. Chúng tôi tin rằng cần có nghiên cứu thêm để khám phá các kiến trúc mới và hiệu quả mẫu hơn có khả năng giải quyết các bộ quy tắc phức tạp hơn của bộ dữ liệu chúng tôi.

DPT. Decision-Pretrained Transformer (Lee et al., 2023) là một phương pháp khác thể hiện khả năng RL trong ngữ cảnh. Tuy nhiên, trong các thí nghiệm của chúng tôi, chúng tôi không thể huấn luyện nó để những khả năng này xuất hiện. Hình 18 trong Phụ lục K chứng minh sự thiếu hiệu suất ngay cả trên các nhiệm vụ tầm thường đơn giản nhất. Chúng tôi tin rằng điều này có liên quan chặt chẽ đến việc DPT không thể lý luận trong môi trường POMDP. Để điều tra chi tiết, chúng tôi tham khảo người đọc đến Phụ lục H.

6 HẠN CHẾ VÀ CÔNG VIỆC TƯƠNG LAI

Có một số hạn chế đối với công việc của chúng tôi, một số trong đó chúng tôi hy vọng sẽ giải quyết trong các phiên bản tương lai. Mặc dù có kích thước và tính đa dạng của các bộ dữ liệu được cung cấp về mặt nhiệm vụ, chúng tôi không cung cấp tính đa dạng về mặt lĩnh vực, vì tất cả các nhiệm vụ đều chia sẻ cùng không gian quan sát và hành động. Ngoài ra, các nhiệm vụ cũng chia sẻ cấu trúc tiềm ẩn tổng thể, tức là nó luôn là một dạng cây nhị phân. Điều này có thể được giải quyết với các trình tạo benchmark đa dạng hơn trong thư viện XLand-MiniGrid (Nikulin et al., 2023). Tất cả lịch sử học tập được thu thập trên các lưới chỉ có một phòng, điều này có thể hạn chế việc chuyển giao sang các bố cục khó hơn với nhiều phòng chứa cửa. Cuối cùng, hiệu ứng của việc tinh chỉnh từ các checkpoint được huấn luyện trước chưa được khám phá đầy đủ và có thể làm tổn hại hiệu suất, vì có nhiều lịch sử học tập bắt đầu từ phần thưởng cao. Chúng tôi hy vọng sẽ cải thiện đường cơ sở RL để thu thập dữ liệu nhằm tránh nhu cầu huấn luyện trước đa nhiệm vụ trong tương lai.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

TUYÊN BỐ ĐẠO ĐỨC

Tuân thủ pháp luật và cấp phép. Bộ dữ liệu sẽ được phát hành dưới giấy phép CC BY-SA 4.0, và mã đi kèm sẽ có sẵn dưới giấy phép Apache 2.0. Chúng tôi đã đảm bảo rằng việc phát hành dữ liệu và mã của chúng tôi tuân thủ tất cả các hướng dẫn pháp lý và đạo đức liên quan.

Quyền riêng tư và bảo mật dữ liệu. Bộ dữ liệu không bao gồm bất kỳ đối tượng con người nào hoặc thông tin nhận dạng cá nhân. Tất cả dữ liệu được tạo ra bởi các tác nhân nhân tạo tương tác trong môi trường mô phỏng. Do đó, không có mối quan ngại về quyền riêng tư hoặc bảo mật liên quan đến bộ dữ liệu.

Lạm dụng tiềm ẩn và ứng dụng có hại. Bộ dữ liệu và các phương pháp nhằm thúc đẩy nghiên cứu trong học tăng cường trong ngữ cảnh. Chúng tôi không dự đoán bất kỳ rủi ro trực tiếp nào về lạm dụng hoặc ứng dụng có hại phát sinh từ công việc của chúng tôi. Tuy nhiên, chúng tôi khuyến khích người dùng áp dụng bộ dữ liệu một cách có trách nhiệm và tuân thủ các tiêu chuẩn đạo đức trong nghiên cứu của họ.

Tác động môi trường. Việc thu thập bộ dữ liệu đòi hỏi tài nguyên tính toán đáng kể, tổng cộng khoảng 50.000 giờ GPU. Chúng tôi thừa nhận dấu chân môi trường liên quan đến các tính toán quy mô lớn. Bằng cách phát hành công khai bộ dữ liệu này, chúng tôi nhằm mục đích giảm thiểu các nỗ lực thu thập dữ liệu dư thừa của các nhà nghiên cứu khác, có thể giảm tác động môi trường tổng thể trong lĩnh vực này.

TUYÊN BỐ TÁI TẠO

Để đảm bảo khả năng tái tạo, chúng tôi chia sẻ các chi tiết quan trọng trong toàn bộ bài báo và phụ lục. Chúng tôi giải thích cách chúng tôi huấn luyện trước, tinh chỉnh các mô hình để thu thập dữ liệu và bước hậu xử lý trong Phần 4.2, cũng như thảo luận về chi tiết bộ dữ liệu (kích thước, tỷ lệ nén, v.v.) trong Phụ lục B và Phụ lục C. Đối với phần thí nghiệm, chúng tôi thảo luận về chi tiết triển khai đường cơ sở trong Phụ lục F và Phụ lục G. Chúng tôi cung cấp các siêu tham số để thu thập bộ dữ liệu và cho các đường cơ sở trong Phụ lục O. Chúng tôi cũng phát hành codebase với các công cụ để tạo và mở rộng bộ dữ liệu trong repository sau: xland-minigrid-datasets.

TÀI LIỆU THAM KHẢO

Pranav Agarwal, Aamer Abdul Rahman, Pierre-Luc St-Charles, Simon JD Prince, và Samira Ebrahimi Kahou. Transformers in reinforcement learning: a survey. arXiv preprint arXiv:2307.05979, 2023.

Ekin Akyürek, Bailin Wang, Yoon Kim, và Jacob Andreas. In-context language learning: Arhitectures and algorithms. arXiv preprint arXiv:2401.12973, 2024.

Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, và Alexei A Efros. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023.

Jacob Beck, Risto Vuorio, Zheng Xiong, và Shimon Whiteson. Recurrent hypernetworks are surprisingly strong in meta-rl. Advances in Neural Information Processing Systems, 36, 2024.

M. G. Bellemare, Y. Naddaf, J. Veness, và M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279, jun 2013.

Satwik Bhattamishra, Arkil Patel, Phil Blunsom, và Varun Kanade. Understanding in-context learning in transformers and llms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016, 2023.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, và Felix Hill. Data distributional properties drive emergent in-context learning in transformers. Advances in Neural Information Processing Systems, 35:18878–18891, 2022.

Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, và Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021.

Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, và Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, và Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.

Karl Cobbe, Christopher Hesse, Jacob Hilton, và John Schulman. Leveraging procedural generation to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019.

Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024.

Sivan Doveh, Shaked Perek, M Jehanzeb Mirza, Amit Alfassy, Assaf Arbelle, Shimon Ullman, và Leonid Karlinsky. Towards multimodal in-context learning for vision & language models. arXiv preprint arXiv:2403.12736, 2024.

Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, và Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.

Chelsea Finn, Pieter Abbeel, và Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126–1135. PMLR, 2017.

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, và Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020.

Quentin Gallouédec, Edward Beeching, Clément Romac, và Emmanuel Dellandréa. Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent. arXiv preprint arXiv:2402.09844, 2024. URL https://arxiv.org/abs/2402.09844.

Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, và Frank Hutter. Is mamba capable of in-context learning? arXiv preprint arXiv:2402.03170, 2024.

Jake Grigsby, Linxi Fan, và Yuke Zhu. Amago: Scalable in-context reinforcement learning for adaptive agents. arXiv preprint arXiv:2310.09971, 2023.

Yuxian Gu, Li Dong, Furu Wei, và Minlie Huang. Pre-training to learn in context. arXiv preprint arXiv:2305.09137, 2023.

Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Ofir Nachum, George Tucker, Nicolas Heess, Nando de Freitas. Rl unplugged: A suite of benchmarks for offline reinforcement learning, 2021.

Eric Hambro, Roberta Raileanu, Danielle Rothermel, Vegard Mella, Tim Rocktäschel, Heinrich Küttler, và Naila Murray. Dungeons and data: A large-scale nethack dataset, 2023.

Shengchao Hu, Li Shen, Ya Zhang, Yixin Chen, và Dacheng Tao. On transforming reinforcement learning by transformer: The development trajectory. arXiv preprint arXiv:2212.14164, 2022.

David Yu-Tung Hui, Maxime Chevalier-Boisvert, Dzmitry Bahdanau, và Yoshua Bengio. Babyai 1.1, 2020.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Michael Janner, Qiyang Li, và Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. Advances in neural information processing systems, 34:1273–1286, 2021.

Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, và Luke Metz. General-purpose in-context learning by meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.

Louis Kirsch, James Harrison, C. Freeman, Jascha Sohl-Dickstein, và Jürgen Schmidhuber. Towards general-purpose in-context learning agents. In NeurIPS 2023 Workshop on Generalization in Planning, 2023. URL https://openreview.net/forum?id=eDZJTdUsfe.

Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov, và Sergey Kolesnikov. Katakomba: Tools and benchmarks for data-driven nethack. Advances in Neural Information Processing Systems, 36, 2024.

Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, và cộng sự. In-context reinforcement learning with algorithm distillation. arXiv preprint arXiv:2210.14215, 2022.

Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, và Emma Brunskill. Supervised pretraining can learn in-context reinforcement learning. arXiv preprint arXiv:2306.14892, 2023.

Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, và cộng sự. Multi-game decision transformers. Advances in Neural Information Processing Systems, 35:27921–27936, 2022.

Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, và Deheng Ye. A survey on transformers in reinforcement learning. arXiv preprint arXiv:2301.03044, 2023.

Licong Lin, Yu Bai, và Song Mei. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. arXiv preprint arXiv:2310.08566, 2023.

Hao Liu và Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience. In International Conference on Machine Learning, pp. 21362–21374. PMLR, 2023.

Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, và Junge Zhang. Position: Foundation agents as the paradigm shift for decision making. arXiv preprint arXiv:2405.17009, 2024.

Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, và Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.

Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A. Osborne, và Yee Whye Teh. Challenges and opportunities in offline reinforcement learning from visual observations, 2023.

Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, và Oriol Vinyals. Alphastar unplugged: Large-scale offline reinforcement learning, 2023.

Ishita Mediratta, Qingfei You, Minqi Jiang, và Roberta Raileanu. The generalization gap in offline reinforcement learning, 2024.

Luckeciano C Melo. Transformers are meta-reinforcement learners. In International Conference on Machine Learning, pp. 15340–15359. PMLR, 2022.

Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, và Andy Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, và Frank Hutter. Transformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021.

Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, và Filip Radenovic. Context diffusion: In-context aware image generation. arXiv preprint arXiv:2312.03584, 2023.

Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, Artem Agarkov, Viacheslav Sinii, và Sergey Kolesnikov. Xland-minigrid: Scalable meta-reinforcement learning environments in jax. arXiv preprint arXiv:2312.12044, 2023.

Ben Norman và Jeff Clune. First-explore, then exploit: Meta-learning intelligent exploration. arXiv preprint arXiv:2307.02276, 2023.

Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, và cộng sự. Open x-embodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023.

Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, và Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.

Ofir Press, Noah A. Smith, và Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022.

Rafael Rafailov, Kyle Beltran Hatch, Anikait Singh, Aviral Kumar, Laura Smith, Ilya Kostrikov, Philippe Hansen-Estruch, Victor Kolev, Philip J Ball, Jiajun Wu, và cộng sự. D5rl: Diverse datasets for data-driven deep reinforcement learning. 2023.

Sharath Chandra Raparthy, Eric Hambro, Robert Kirk, Mikael Henaff, và Roberta Raileanu. Generalization to new sequential decision making tasks with in-context learning. arXiv preprint arXiv:2312.03801, 2023.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505–3506, 2020.

Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, và Nando de Freitas. A generalist agent, 2022.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, và Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Gresa Shala, André Biedenkapp, và Josif Grabocka. Hierarchical transformers are efficient meta-reinforcement learners. arXiv preprint arXiv:2402.06402, 2024.

Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby, Linxi Fan, và Yuke Zhu. Cross-episodic curriculum for transformer agents. Advances in Neural Information Processing Systems, 36, 2024.

Viacheslav Sinii, Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, và Sergey Kolesnikov. In-context reinforcement learning for variable action spaces. arXiv preprint arXiv:2312.13327, 2023.

Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, và cộng sự. Human-timescale adaptation in an open-ended task space. arXiv preprint arXiv:2301.07608, 2023.

Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, và cộng sự. Open-ended learning leads to generally capable agents. arXiv preprint arXiv:2107.12808, 2021.

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, và Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024.

William L Tong và Cengiz Pehlevan. Mlps learn in-context. arXiv preprint arXiv:2405.15618, 2024.

Max Vladymyrov, Johannes von Oswald, Mark Sandler, và Rong Ge. Linear transformers are versatile in-context learners. arXiv preprint arXiv:2402.14180, 2024.

Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, và Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 35151–35174. PMLR, 2023.

Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, và Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.

Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, và Shangtong Zhang. Transformers learn temporal difference methods for in-context reinforcement learning. arXiv preprint arXiv:2405.13861, 2024.

Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, và Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 1094–1100. PMLR, 2020.

Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, và Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. arXiv preprint arXiv:1910.08348, 2019.

Ilya Zisman, Vladislav Kurenkov, Alexander Nikulin, Viacheslav Sinii, và Sergey Kolesnikov. Emergence of in-context reinforcement learning from noise distillation. arXiv preprint arXiv:2312.12275, 2023.

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

A TẢI XUỐNG CÁC BỘ DỮ LIỆU

Cả hai bộ dữ liệu XLand-100B và XLand-Trivial-20B đều được lưu trữ trên bucket S3 công cộng và có sẵn miễn phí cho mọi người dưới Giấy phép CC BY-SA 4.0. Chúng tôi khuyên nên bắt đầu với bộ dữ liệu Trivial để debug do kích thước nhỏ hơn và thời gian tải xuống nhanh hơn. Các bộ dữ liệu có thể được tải xuống bằng tiện ích curl (hoặc bất kỳ tiện ích tương tự nào khác).

# XLand-Trivial-20B, kích thước khoảng 60GB
curl -L -o xland-trivial-20b.hdf5 https://tinyurl.com/trivial-10k

# XLand-100B, kích thước khoảng 325GB
curl -L -o xland-100b.hdf5 https://tinyurl.com/medium-30k

B CÓ GÌ TRONG BỘ DỮ LIỆU?

Cả hai bộ dữ liệu -Trivial và -100B đều là các tệp HDF5 có cùng cấu trúc. Bộ dữ liệu được nhóm theo cách sau:

data["{key_id}/{entity_name}"][learning_history_id]

trong đó key_id là số thứ tự của một nhiệm vụ trong bộ dữ liệu, learning_history_id là số lịch sử học tập từ 0 đến 32 và entity_name là một trong các tên được đề cập trong Bảng 3.

LƯU Ý! Đừng nhầm lẫn key_id với ID nhiệm vụ, phải được truy cập thông qua data["{key_id}"].attrs["ruleset-id"]

Bảng 3: Mô tả dữ liệu
Tên Loại Hình dạng Mô tả
states np.uint8 (5, 5) st, màu sắc và ô từ POV của tác nhân
actions np.uint8 scalar at, từ 0 đến NUM_ACTIONS
rewards np.float16 scalar rt, phần thưởng tác nhân nhận được tại timestep t
dones np.bool scalar dt, cờ tập kết thúc hoặc bị cắt ngắn
expert_actions np.uint8 scalar giống như at nhưng từ chính sách tạo ra

C ĐIỀU CHỈNH KÍCH THƯỚC CHUNK NÉN

Bảng 4: Thông lượng với PyTorch dataloader với các cài đặt kích thước chunk nén HDF5 khác nhau. Chúng tôi sử dụng độ dài chuỗi 2048, kích thước batch 64 và 8 worker. Chunking được áp dụng dọc theo chiều lịch sử học tập.

Nén Kích thước chunk Thông lượng
None None 1.549.619
gzip None 173.895
gzip 256 423.513
gzip 512 549.397
gzip 1024 666.152
gzip 2048 768.706
gzip 4096 749.851
gzip 8192 737.646

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

D CÁC HÌNH ẢNH BỔ SUNG VỀ THU THẬP DỮ LIỆU

0 0,5B 1B
Chuyển đổi0.00.51.0Lợi nhuận
Từ đầu
Được huấn luyện trước

Hình 9: Các đường cong đánh giá một nhiệm vụ trên 256 nhiệm vụ cho các chính sách được huấn luyện từ đầu hoặc tinh chỉnh từ các checkpoint đa nhiệm vụ được huấn luyện trước.

012345678
Số quy tắc0.00.20.40.60.81.0Lợi nhuậnhuấn luyện trước
từ đầu

Hình 10: Lợi nhuận cuối theo số quy tắc trên 256 nhiệm vụ cho các chính sách được huấn luyện từ đầu hoặc tinh chỉnh từ các checkpoint đa nhiệm vụ được huấn luyện trước.

0123456789
Số quy tắc0.00.51.0Lợi nhuận

Hình 11: Lợi nhuận cuối theo số quy tắc trong bộ dữ liệu XLand-100B cuối cùng sau hậu xử lý.

E CÁC HÌNH ẢNH BỔ SUNG VỀ ĐÁNH GIÁ DỮ LIỆU

0k 2k 4k 6k 8k
Tập0.00.51.0Lợi nhuậnXLand-Trivial-20B

Hình 12: Lợi nhuận được tính trung bình trên tất cả lịch sử học tập trong bộ dữ liệu XLand-Trivial-20B cuối cùng.

0k 3k 6k 9k12k 15k
Tập0.40.60.81.0Lợi nhuậnXLand-100B

Hình 13: Lợi nhuận được tính trung bình trên tất cả lịch sử học tập trong bộ dữ liệu XLand-100B cuối cùng.

0k 10k 20k 30k 40k 50k 60k
Chuyển đổi0.20.40.60.8Thỏa thuận hành độngXLand-Trivial-20B

Hình 14: Thỏa thuận giữa các hành động được dự đoán bởi chuyên gia và các hành động thực tế trong lịch sử học tập. Chúng tôi sử dụng chính sách PPO cuối cùng như một chuyên gia để gắn nhãn hành động.

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

F TRIỂN KHAI AD

Bảng 5: Thời gian huấn luyện và đánh giá.
độ dài chuỗi 1024 2048 4096
huấn luyện 15 giờ 10 giờ 11,5 giờ
đánh giá 20 phút 30 phút 50 phút

Chúng tôi triển khai AD theo bài báo gốc của Laskin et al. (2022). Để tối ưu hóa tốc độ huấn luyện và suy luận, chúng tôi sử dụng FlashAttention-2 (Dao, 2024) với KV-caching và embedding vị trí ALiBi (Press et al., 2022). Chúng tôi cũng nối quan sát, hành động và phần thưởng dọc theo các chiều embedding, vì điều này làm giảm kích thước ngữ cảnh đi ba lần. Chúng tôi cũng sử dụng DeepSpeed (Rasley et al., 2020) để cho phép huấn luyện phân tán. Tổng số tham số của mô hình chúng tôi là 25M.

Thời gian huấn luyện xấp xỉ cho một epoch trên bộ dữ liệu -100B và đánh giá trên 1024 nhiệm vụ trên 8 GPU H100 được hiển thị trong Bảng 5. Các tính toán được thực hiện trên cluster nội bộ.

Các siêu tham số được sao chép từ (Laskin et al., 2022) ngoại trừ kích thước mạng, nó đã được mở rộng lên 25M. Các siêu tham số chính xác có thể được tìm thấy trong Bảng 8.

Chúng tôi cũng hiển thị nhật ký huấn luyện cho cả hai bộ dữ liệu. Trivial: wandb; medium: wandb

G TRIỂN KHAI DPT

Việc triển khai của chúng tôi dựa trên triển khai gốc (Lee et al., 2023). So với triển khai AD (Laskin et al., 2022), trong giai đoạn huấn luyện, ngữ cảnh mô hình được tạo ra với các chuyển đổi bộ dữ liệu không tương quan để tăng tính đa dạng dữ liệu và độ bền của mô hình: cho một quan sát truy vấn và một hành động chuyên gia tương ứng cho nó, một bộ dữ liệu trong ngữ cảnh được cung cấp bởi các tương tác ngẫu nhiên trong cùng một bộ quy tắc. Trong giai đoạn đánh giá, bộ đệm ngữ cảnh đa tập chỉ bao gồm các tập trước đó và cập nhật sau khi tập hiện tại kết thúc. Trực giác đằng sau cách tiếp cận này là chính sách được quan sát trong bất kỳ tập nào được cho trước là cố định, vì vậy việc phân tích chính sách này dễ dàng hơn nhiều so với một chính sách thay đổi động trong khi nó đang thực thi.

Cả AD và DPT đều chia sẻ cùng một bộ mã hóa quan sát và khối transformer, ngoại trừ không có mã hóa vị trí trong DPT, như được nêu trong (Lee et al., 2023).

Việc huấn luyện bao gồm 3 epoch do hạn chế về tính toán và thời gian vì 1 epoch kéo dài khoảng 12 giờ, trong khi đánh giá trên 1024 bộ quy tắc trong 500 tập có thể mất từ 5 đến 21 giờ, tùy thuộc vào độ dài chuỗi của mô hình. Tất cả thí nghiệm chạy trên 8 GPU A100. Các tính toán được thực hiện trên cluster nội bộ.

Các siêu tham số được sao chép từ (Lee et al., 2023) ngoại trừ kích thước mạng, nó đã được làm lên 25M, và độ dài chuỗi, nó đã được tăng lên do độ phức tạp của các nhiệm vụ. Các siêu tham số chính xác có thể được tìm thấy trong Bảng 7.

Chúng tôi cũng hiển thị nhật ký huấn luyện DPT. Trivial: wandb; medium: wandb

H VỀ HẠN CHẾ DPT TRONG POMDP

Chúng tôi bổ sung chứng minh sự bất lực của DPT trong việc học trong ngữ cảnh trong Partially Observable MDP (POMDP) trên ví dụ về môi trường Dark Key-To-Door đồ chơi (Laskin et al., 2022). Tác nhân được yêu cầu tìm một chìa khóa vô hình và sau đó mở một cánh cửa vô hình. Phần thưởng là 1 được đưa ra khi tác nhân lần đầu tiên đến chìa khóa và sau đó đến cánh cửa. Lưu ý rằng cánh cửa không thể đến được cho đến khi tìm thấy chìa khóa. Bằng cách này, Key-To-Door có thể được coi là một POMDP. Tuy nhiên, môi trường có thể được tái cấu trúc như một MDP bằng cách cung cấp thêm chỉ báo boolean về việc đạt được chìa khóa ngoài vị trí của tác nhân. Bằng cách này, các thuật toán chỉ hoạt động với MDP có thể giải quyết môi trường này.

Dựa trên thực tế này, chúng tôi học hai Q-table khác nhau cho cả hai môi trường: có và không có chỉ báo chìa khóa. Lịch sử học tập của thuật toán Q-Learning được lưu trữ cùng với các hành động tối ưu được tính toán thông qua oracle.

Để rõ ràng, chúng tôi gọi việc huấn luyện và đánh giá DPT là Markovian khi chỉ báo "đã đạt được chìa khóa" được cung cấp cho mỗi trạng thái. Chúng tôi đã huấn luyện DPT trên Key-To-Door cho 150.000 cập nhật trong các thiết lập Markovian và không Markovian để cho thấy sự khác biệt về hiệu suất. Như có thể thấy trong Hình 15, trong trường hợp Markovian, mô hình hội tụ về lợi nhuận tối ưu, tìm được cả chìa khóa và cánh cửa. Trong trường hợp sau, mô hình đạt được phần thưởng plateau là 1 có nghĩa là nó tìm thấy chìa khóa. Như chúng tôi quan sát thực nghiệm, không có chỉ báo, DPT chỉ đạt được lợi nhuận dưới tối ưu. Chúng tôi tin rằng điều này xảy ra do DPT không có khả năng lý luận xem chìa khóa đã được tìm thấy hay chưa từ ngữ cảnh ngẫu nhiên. Không có kiến thức này, tác nhân không thể biết liệu nó nên tìm kiếm chìa khóa hay cánh cửa.

Chúng tôi cũng hiển thị nhật ký cho Thí nghiệm Key-To-Door: Markovian: wandb; không Markovian: wandb

Bảng 6: Siêu tham số Q-Learning
Siêu tham số Giá trị
Số mục tiêu huấn luyện 2424
Số lịch sử 5000
Số cập nhật 50.000
Tốc độ học 3e-4

0 50 100 150 200 250
Tập0.00.51.01.52.0Lợi nhuận

0 50 100 150 200 250
Tập0.00.51.01.52.0Lợi nhuận

0 50 100 150 200 250
Tập0.00.51.01.52.0Lợi nhuận

0 50 100 150 200 250
Tập0.00.51.01.52.0Lợi nhuậnlợi nhuận tối ưu: 2.00

Hình 15: Hiệu suất của DPT trên môi trường Key-To-Door. Từ trái sang phải: hai biểu đồ đầu tiên chỉ ra lợi nhuận trên 20 mục tiêu huấn luyện và 50 mục tiêu kiểm tra, tương ứng, cho Key-To-Door như MDP bằng cách cung cấp chỉ báo bổ sung về việc đạt được chìa khóa cho mô hình. Hai biểu đồ thứ hai chỉ ra lợi nhuận với mô hình không có quyền truy cập vào thực tế đạt được chìa khóa. Kết quả được tính trung bình trên bốn seed.

I CHI TIẾT ĐÁNH GIÁ TRONG NGỮ CẢNH

Quá trình đánh giá giống với học trong trọng số tiêu chuẩn, với sự khác biệt chính là bản thân việc học xảy ra trong quá trình đánh giá. Khi tương tác với môi trường, tác nhân điền ngữ cảnh của Transformer với các quan sát, hành động và phần thưởng mới nhất. Khi bắt đầu đánh giá, ngữ cảnh trống. Lưu ý rằng ngữ cảnh của tác nhân là xuyên tập, điều này làm cho tác nhân có thể truy cập các chuyển đổi trong các tập trước đó. Chúng tôi báo cáo phần thưởng tích lũy mà tác nhân đạt được ở cuối mỗi tập. Chúng tôi chạy đánh giá cho mỗi mô hình trong 500 tập, báo cáo lợi nhuận trung bình trên 1024 nhiệm vụ chưa thấy với độ lệch chuẩn trên 3 seed. Chúng tôi khẳng định rằng trong ngữ cảnh đã xuất hiện khi lợi nhuận trung bình tăng lên đến một mức nào đó. Điều này có nghĩa là tác nhân cải thiện chính sách của mình từ tập này sang tập khác, học cách giải quyết một nhiệm vụ.

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

J CÁC HÌNH ẢNH BỔ SUNG VỀ HIỆU SUẤT AD

0 1 2 3 4 5 6 7 8 9
Số quy tắc0.00.20.40.60.81.0Lợi nhuậndữ liệu
mô hình

Hình 16: Hiệu suất AD trên các độ phức tạp khác nhau của bộ quy tắc. AD được đánh giá trên 1024 nhiệm vụ huấn luyện từ -100B. Độ dài chuỗi là 1024.

0 100 200 300 400 500
Tập0.40.60.81.0Lợi nhuận# quy tắc =0
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.20.40.60.81.0Lợi nhuận# quy tắc =1
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# quy tắc =2
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# quy tắc =3
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# quy tắc =4
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# quy tắc =5
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# quy tắc =6
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.2
0.00.20.40.60.81.0Lợi nhuận# quy tắc =7
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# quy tắc =8
Dữ liệu
Mô hình

0 100 200 300 400 500
Tập0.2
0.00.20.40.60.8Lợi nhuận# quy tắc =9
Dữ liệu
Mô hình

Hình 17: Hiệu suất AD trên các độ phức tạp nhiệm vụ khác nhau, biến thể đầy đủ của Hình 7.

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

K CÁC HÌNH ẢNH BỔ SUNG VỀ HIỆU SUẤT DPT

0100 200 300 400 500
Tập0.00.10.20.30.40.50.6Lợi nhuậnXLand-Trivial-20B
512
1024
2048
4096

0100 200 300 400 500
Tập0.00.10.20.30.40.50.6Lợi nhuậnXLand-100B
1024
2048
4096

Hình 18: Hiệu suất DPT trên cả hai bộ dữ liệu. Các tham số đánh giá giống như trong Hình 8.

0 1 2 3 4 5 6 7 8 9
Số quy tắc0.00.20.40.60.81.0Lợi nhuậndữ liệu
mô hình

Hình 19: Hiệu suất DPT trên các độ phức tạp khác nhau của bộ quy tắc. DPT được đánh giá trên 1024 nhiệm vụ huấn luyện từ -100B. Độ dài chuỗi là 1024.

0100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# mục tiêu =0
mô hình
dữ liệu

0100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# mục tiêu =1
mô hình
dữ liệu

0100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# mục tiêu =2
mô hình
dữ liệu

0100 200 300 400 500
Tập0.00.20.40.60.81.0Lợi nhuận# mục tiêu =8
mô hình
dữ liệu

Hình 20: So sánh lịch sử học tập trong bộ dữ liệu -100B với hiệu suất DPT trên cùng các nhiệm vụ huấn luyện. DPT không thể giải quyết các nhiệm vụ đơn giản và không có quan sát nào về việc học trong ngữ cảnh xuất hiện, hiệu suất mô hình cũng giảm khi các bộ quy tắc trở nên khó hơn. Độ dài ngữ cảnh của mô hình là 1024. Các tham số đánh giá, ngoại trừ nhiệm vụ huấn luyện, giống như trong Hình 8.

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

L HÌNH ẢNH HÓA BỘ QUY TẮC

AgentNearRight( )
AgentNearLeft( )⇒
NearRight( , )⇒
AgentNearLeft( )⇒
Cây nhiệm vụ
Ví dụ bố cục bắt đầuQuy tắc phân tâm
Bộ quy tắc 22

NearDown( , )
NearRight( , )⇒
Near( , )⇒
NearRight( , )⇒
AgentNearUp( )⇒
AgentNearRight( )⇒
AgentHold( )⇒
Cây nhiệm vụ
Ví dụ bố cục bắt đầuQuy tắc phân tâm
Bộ quy tắc 25

NEAR( , )
NearUp( , )⇒
NearLeft( , )⇒
NearRight( , )⇒
NearLeft( , )⇒
NEAR( , )⇒
AgentHold( )⇒
AgentNearLeft( )⇒
AgentNear( )⇒
AgentNear( )⇒
Cây nhiệm vụ
Ví dụ bố cục bắt đầuQuy tắc phân tâm
Bộ quy tắc 60

Hình 21: Bộ quy tắc với độ khó tăng dần (tức là với 3, 5, và 9 quy tắc tương ứng) được lấy mẫu từ bài kiểm tra medium-1m. Chúng có thể được truy cập theo ID thông qua gói XLand-MiniGrid.

M BỐ CỤC LƯỚI SỬ DỤNG CHO THU THẬP BỘ DỮ LIỆU

Hình 22: Bố cục được sử dụng cho XLand-Trivial-20B. Tương ứng với môi trường XLand-MiniGrid-R1-9x9 từ bộ XLand-MiniGrid. Các đối tượng trò chơi phụ thuộc vào nhiệm vụ.

Hình 23: Bố cục được sử dụng cho XLand-100B. Tương ứng với môi trường XLand-MiniGrid-R1-13x13 từ bộ XLand-MiniGrid. Các đối tượng trò chơi phụ thuộc vào nhiệm vụ.

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

N THÍ NGHIỆM BỔ SUNG VỀ SỐ LƯỢNG NHIỆM VỤ

Chúng tôi chạy thí nghiệm bổ sung với số lượng nhiệm vụ khác nhau để cho thấy nó có thể ảnh hưởng đến hiệu suất như thế nào. Thí nghiệm được thực hiện với độ dài chuỗi là 2048. Để làm cho số lượng cập nhật gradient gần hơn với các thí nghiệm gốc, chúng tôi tăng dần số epoch cho mỗi tập con nhỏ hơn. Lưu ý rằng lợi nhuận tăng từ 100 mục tiêu đến 3k mục tiêu, cho thấy tầm quan trọng của việc mở rộng số lượng mục tiêu trong bộ dữ liệu. Chạy AD cho một epoch duy nhất trên bộ dữ liệu mục tiêu đầy đủ rõ ràng là không đủ, tuy nhiên, việc huấn luyện quy mô lớn quá tốn kém về mặt tính toán và được để lại cho nghiên cứu tương lai.

Hình 24: Huấn luyện AD cho số lượng mục tiêu khác nhau.

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

O SIÊU THAM SỐ

Bảng 7: Siêu tham số DPT
(a) Siêu tham số DPT cho bộ dữ liệu
Siêu tham số Giá trị
Chiều embedding 64
Số lớp 8
Số head 8
Chiều feedforward 256
Vị trí layernorm Pre Norm
Tỷ lệ dropout embedding 0,1
Kích thước batch [512, 256, 128]
Độ dài chuỗi [1024, 2048, 4096]
Optimizer Adam
Betas (0,9, 0,99)
Tốc độ học 1e-3
Lịch trình tốc độ học Cosine Decay
Tỷ lệ warmup 0,05
Số tham số 25M

(b) Siêu tham số DPT Key-to-Door
Siêu tham số Giá trị
Chiều embedding 64
Số lớp 4
Số head 4
Chiều feedforward 64
Vị trí layernorm Pre Norm
Dropout residual 0,5
Độ dài chuỗi [50, 100, 250, 350, 500]
Kích thước batch 128
Optimizer Adam
Betas (0,9, 0,99)
Tốc độ học 1e-3
Làm mịn nhãn 0,3
Lịch trình tốc độ học Cosine Decay
Tỷ lệ warmup 0,05
Số tham số 200K

Bảng 8: Siêu tham số AD
Siêu tham số Giá trị
Chiều embedding 64
Số lớp 8
Số head 8
Chiều feedforward 512
Vị trí layernorm Pre-norm
Dropout embedding 0,1
Kích thước batch [256, 128, 64]
Độ dài chuỗi [1024, 2048, 4096]
Optimizer Adam
Betas (0,9, 0,99)
Tốc độ học 1e-3
Lịch trình tốc độ học CosineLR
Bước warmup 500
Số tham số 25M

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 9: Siêu tham số PPO được sử dụng trong huấn luyện trước đa nhiệm vụ từ Phần 4.2.
Siêu tham số Giá trị
env_id XLand-MiniGrid-R1-13x13
benchmark_id medium-1m
use_bf16 True
pretrain_multitask True
context_emb_dim 16
context_hidden_dim 64
context_dropout 0,0
obs_emb_dim 16
action_emb_dim 16
rnn_hidden_dim 1024
rnn_num_layers 1
head_hidden_dim 256
num_envs 65536
num_steps 256
update_epochs 1
num_minibatches 64
total_timesteps 25.000.000.000
optimizer Adam
decay_lr True
lr 0,0005
clip_eps 0,2
gamma 0,995
gae_lambda 0,999
ent_coef 0,001
vf_coef 0,5
max_grad_norm 0,5
eval_episodes 256
eval_seed 42
train_seed 42

--- TRANG 21 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 10: Siêu tham số PPO được sử dụng trong tinh chỉnh một nhiệm vụ từ Phần 4.2.
Siêu tham số Giá trị
env_id XLand-MiniGrid-R1-13x13
benchmark_id medium-1m
use_bf16 True
pretrain_multitask False
context_emb_dim 16
context_hidden_dim 64
context_dropout 0,0
obs_emb_dim 16
action_emb_dim 16
rnn_hidden_dim 1024
rnn_num_layers 1
head_hidden_dim 256
num_envs 8192
num_steps 256
update_epochs 1
num_minibatches 8
total_timesteps 1.000.000.000
optimizer Adam
decay_lr True
lr 0,0005
clip_eps 0,2
gamma 0,995
gae_lambda 0,999
ent_coef 0,001
vf_coef 0,5
max_grad_norm 0,5
eval_episodes 256
eval_seed 42
train_seed 42
