# 2310.10508.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2310.10508.pdf
# File size: 520610 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Prompt Engineering or Fine-Tuning:
An Empirical Assessment of LLMs for Code
Jiho Shin∗, Clark Tang, Tahmineh Mohati†, Maleknaz Nayebi∗, Song Wang∗, Hadi Hemmati∗†
∗Lassonde School of Engineering, York University, Toronto, Canada
†Schulich School of Engineering, University of Calgary, Calgary, Canada
{jihoshin, mnayebi, wangsong, hemmati }@yorku.ca, clark.f.tang@gmail.com, tahmineh.mohati1@ucalgary.ca
Abstract —The rapid advancements in large language models
(LLMs) have greatly expanded the potential for automated
code-related tasks. Two primary methodologies are used in this
domain: prompt engineering and fine-tuning. Prompt engineering
involves applying different strategies to query LLMs, like Chat-
GPT , while fine-tuning further adapts pre-trained models, such
asCodeBERT , by training them on task-specific data. Despite
the growth in the area, there remains a lack of comprehensive
comparative analysis between the approaches for code models.
In this paper, we evaluate GPT-4 using three prompt engineer-
ing strategies—basic prompting, in-context learning, and task-
specific prompting—and compare it against 17 fine-tuned models
across three code-related tasks: code summarization, generation,
and translation. Our results indicate that GPT-4 with prompt
engineering does not consistently outperform fine-tuned models.
For instance, in code generation, GPT-4 is outperformed by fine-
tuned models by 28.3% points on the MBPP dataset. It also
shows mixed results for code translation tasks.
Additionally, a user study was conducted involving 27 graduate
students and 10 industry practitioners. The study revealed
that GPT-4 with conversational prompts, incorporating human
feedback during interaction, significantly improved performance
compared to automated prompting. Participants often provided
explicit instructions or added context during these interactions.
These findings suggest that GPT-4 with conversational prompt-
ing holds significant promise for automated code-related tasks,
whereas fully automated prompt engineering without human
involvement still requires further investigation.
Index Terms —Prompt engineering, Fine-tuning, LLM4SE,
Empirical study, Survey.
I. I NTRODUCTION
Language models have gained significant interest, leading
to numerous studies adapting their use for automated code-
related tasks. Although originally developed for natural lan-
guage, Large Language Models (LLMs) demonstrate potential
in automated code-related tasks. Previous literature mainly
focuses on pre-training on large corpora of Source Code (SC)
and Natural Language (NL) and fine-tuning with task-specific
datasets for various tasks. State-of-the-art LLMs, however, are
trained using massive unsupervised corpora and use prompts
for task querying. The large training corpus and numerous
parameters enable LLMs to perform well on automated code-
related tasks with simple prompts.
Prompting offers advantages over fine-tuning: it does not
require labelled datasets, which are costly to acquire. Further-
more, running a prompt is less resource-intensive than fine-
tuning an LLM. Despite its potential, prompt engineering isstill in its early stages and lacks systematic studies on its
performance compared to fine-tuned models in code tasks.
This paper presents a quantitative and qualitative investi-
gation of OpenAI’s ChatGPT , specifically the latest version
(GPT-4 ), which has shown notable improvements over its pre-
decessor, GPT-3.5 . We focus on three automated code tasks:
code summarization (SC-to-NL), code generation (NL-to-SC),
and code translation (SC-to-SC). These tasks are chosen for
their commonality among developers and prevalence in the
literature. We employ three automated prompting techniques
(basic, in-context learning, and task-specific prompts) and
compare them with 17 fine-tuned language models. Addition-
ally, we surveyed 27 graduate students and 10 industry prac-
titioners to gather qualitative insights through conversational
prompts. We address the following research questions:
RQ1: How does GPT-4 with automated prompting compare
to fine-tuned models in performance? We quantitatively
assess GPT-4 using three prompting strategies: (a) basic
prompt, (b) in-context learning, and (c) task-specific prompt,
comparing results with fine-tuned language models.
RQ2: How do participants perceive the usefulness of GPT-4
using a basic prompt? We ask participants to assess the basic
prompting strategy qualitatively to understand its usefulness.
RQ3: How do participants refine their prompts when
interacting with GPT-4 ?We investigate how participants
refine prompts to achieve better results by analyzing their
interaction logs and summarizing prompt evolution patterns.
RQ4: What is the impact of different prompt evolution
patterns? We investigate the impact of prompt evolution
patterns derived from participant interactions.
Our quantitative analysis suggests that prompt-engineered
GPT-4 does not consistently outperform fine-tuned LLMs in
all tasks. For code summarization, GPT-4 with task-specific
prompting outperforms the top fine-tuned model by 8.33%
points in the BLEU score. In code generation, GPT-4 outper-
forms fine-tuned models by 8.59% points on the HumanEval
dataset. However, it is outperformed by fine-tuned models
by 28.3% points on the MBPP dataset. In code translation,
GPT-4 and fine-tuned models also show mixed results. Anal-
ysis of participant interactions reveals significant performance
improvements when using conversational prompts compared
to automated strategies, with improvements of 15.8% points,
18.3% points, and 16.1% points for code summarization, gen-
eration, and translation, respectively. Participants frequentlyarXiv:2310.10508v2  [cs.SE]  19 Feb 2025

--- PAGE 2 ---
requested improvements, added context, or provided specific
instructions to enhance GPT-4 ’s output. Our results indicate
thatGPT-4 with conversational prompting holds potential for
automated code-related tasks, but fully automated prompt
engineering requires further development.
This paper contributes the following:
•The first empirical study comparing automated prompting
strategies on GPT-4 with fine-tuned LLMs for three
automated code-related tasks.
•A user study with 27 graduate students and ten industry
practitioners exploring the evolution of conversational
prompts in automated code-related tasks.
•Identification of gaps between conversational and auto-
mated prompts, and suggestions for leveraging LLMs in
automated code-related tasks.
•Release of all study artifacts to facilitate replication and
extension by other researchers1.
II. B ACKGROUND AND RELATED WORK
A. Fine-Tuning Language Models for Code
The field of automating code-related tasks has increasingly
adopted language models (LMs) [1]–[5]. These methods offer
significant advantages over traditional approaches such as
domain-specific models, probabilistic grammars, and simple
neural LMs. In recent years, LMs have been applied to various
ASE applications, including code completion [6], code search
[7], code generation [8], [9], test case generation [10], [11],
test oracle generation [12], code summarization [13], code
translation [14], and automated program repair [15].
Pre-trained code models learn general-purpose code repre-
sentations capturing lexical, syntactic, semantic, and structural
information. Fine-tuning adapts these models to specific tasks
using task-specific data, allowing them to outperform existing
baselines. Numerous studies have leveraged pre-training on
source code and natural language corpora, followed by fine-
tuning for downstream tasks, e.g., code search, code summa-
rization, code generation, etc. [16]–[20].
B. Prompt Engineering in Software Engineering
Prompt engineering is an alternative to fine-tuning that
adapts pre-trained LMs without requiring a supervised dataset.
Instead, it uses prompts to treat different tasks as generation
problems. These models, termed Large Language Models
(LLMs), have larger corpora and more parameters. The advent
of LLMs and prompt engineering has significantly improved
task performance [21]–[27]. Studies have explored LLMs
and prompt engineering to tackle code tasks with various
prompting strategies, such as basic prompting, in-context
learning, task-specific prompting, chain-of-thought prompting,
auto-prompting, and soft prompting [6], [28]–[38].
Gao et al. [29] investigated three key factors in in-context
learning for code tasks: selection, order, and number of exam-
ples. They found that both similarity and diversity in example
selection were crucial for performance and stability. Li et al.
1https://github.com/shinjh0849/gpt4 asetasks[39] studied ChatGPT ’s ability to find failure-inducing test
cases, showing that performance improved drastically with
correct guidance. Feng et al. [30] introduced AdbGPT , an
LLM-based approach for reproducing bugs using few-shot
learning and chain-of-thought reasoning. Kabir et al. [40]
analyzed ChatGPT ’s responses to Stack Overflow questions,
finding over half of the answers incorrect and 77% verbose.
Geng et al. [31] adopted in-context learning for code sum-
marization, using customized strategies like selection and re-
ranking to enhance performance.
Wang et al. [41] compared prompt-tuning and fine-tuning
on three code tasks (defect prediction, code summarization,
and code translation) using two pre-trained models, CodeBERT
andCodeT5 . In contrast, we explored the effectiveness of GPT-
4using three prompting techniques (basic, in-context learning,
and task-specific prompting) against 16 fine-tuned LLMs. The
main difference between the studies lies in model size: they
assessed models with 220M and 125M parameters, whereas
we evaluated GPT-4 with 1.76T parameters. We also included
a qualitative analysis surveying academia and industry partic-
ipants to explore the impact of different prompting strategies.
Despite growing interest in prompt engineering, compar-
isons between fine-tuned and prompt-engineered LLMs in
ASE tasks remain limited. Fine-tuning modifies pre-trained
LM parameters to optimize a task, while prompt engineering
crafts natural language queries to obtain the desired output
without parameter changes. Fine-tuning can achieve better
performance but requires more data and resources [41], [42].
Prompt engineering leverages LLMs’ versatility but may face
issues like inconsistency and insufficient domain knowledge
optimization [42]–[44]. To address this gap, we conducted the
first quantitative and qualitative comparison of fine-tuning and
prompt engineering methods.
III. E MPIRICAL STUDY SETUP
A. Downstream Tasks on Code Automation
We select three typical automated code-related software
engineering tasks for the experiments, i.e., Code summariza-
tion: [45]–[47]The model generates a short natural language
summary from a source code snippet (SC-to-NL), Code gener-
ation: [48]–[50]The model generates the corresponding code
snippet from a natural language description (NL-to-SC), and
Code translation: [51]–[53] The model translates a source
code snippet into another programming language (SC-to-SC).
These tasks assess GPT-4 ’s ability to generate various
modalities (SC and NL). The programming languages differ
per task: code summarization involves Ruby ,JavaScript ,
Go,Python ,Java , and PHP; code generation targets
Python ; and code translation involves Java andC#, cov-
ering both translation directions. The languages are chosen
based on the benchmarks assessed, which are discussed in the
following section.
B. Dataset and Data Process
We use well-known benchmarks for each examined task.
The rationale for using these benchmarks is their commonality,

--- PAGE 3 ---
TABLE I: Stats of dataset used in quantitative analysis
Tasks Dataset Language # of test instance
CodSum CSN [54]Python 14,918
PHP 14,014
Go 8,122
Java 10,955
JavaScript 3,291
Ruby 1,261
CodTran CT [55]C# 1,000
Java 1,000
CodGenHumanEval [56] Python 164
MBPP [57] Python 500
enabling comparison with other studies without retraining
models, and allowing assessment of generalization across
multiple languages. For code summarization and translation,
we use CodeXGLUE (CSN and CT) [55]. For code generation,
we use HumanEval [56] and MBPP [57]. Dataset statistics are
in Table I. Since we do not train a new model, we show only
the number of instances in the test sets used for evaluation.
Since GPT-4 ’s training data is not public, it is uncertain
if these benchmarks are included in its training. To reduce
potential information leakage, we collected sample test sets
from GitHub repositories created after Sept. 2021 ( GPT-
4’s end of training) for qualitative analysis. The criteria for
selecting repositories included: created after Oct. 2021, written
inJava , actively maintained, not a fork, and with English
comments. We selected three top-rated projects in different
domains (library, algorithm, web) and picked two examples per
task, totalling 18 sets of problems. The average token length
is 14 for NL descriptions and 38 for SC snippets. The selected
examples cover different domains, include multi-modalities
and vary in difficulty.
We followed common pre- and post-processing for each
task. For code summarization, we removed special characters
(except commas and periods) and tokenized them. For code
generation, we formatted generated Python code to match
the datasets. For code translation, we tokenized the generated
code using ctok inPython , as evaluation metrics are sensitive
to tokenization. GPT-4 sometimes generated non-code text,
requiring post-processing to keep only code. To automate GPT-
4prompting, we used the OpenAI API2, specifying the GPT-4
model. We set the temperature parameter to zero for better
determinism [43]. Metrics were calculated using scripts from
the benchmark dataset’s repository.
C. Baselines
Table II lists the fine-tuned baseline models compared with
GPT-4 . These are the best-performing fine-tuned models re-
ported in each benchmark’s leaderboard. Note that some base-
lines reported on the leaderboard were not publicly available,
so we only included publicly available models in the table.
2https://platform.openai.com/TABLE II: Baseline Fine-tuned models in three code tasks.
Tasks Baseline Pre-trained Model Param Fine-Tuned
Code
SummarizationPolyglotCodeBERT [20] CodeBERT [16] 125MCodeSearchNet
(CSN)CoTexT [58] T5-base [59] 220M
ProphetNet-X [60] ProphetNet-Code [60] 300M
Code
generationPanGu-Coder2 [61] StarCoder [62] 15BHumanEval
(HE)WizardCoder [63] StarCoder [62] 15B
XCoder [64] LLaMA3-8B-Base [65] 8B
CODE-T-ITER [66] cushman-001
davinci-001&002 [56]12B
175BHE & MBPP
CODE-T [66]
MBPP StarCoder [62] StarCoderBase [62] 15.5B
StarCoder2 [67] StarCoder2-15B [67] 15B
Code
translationStructCoder [68] CodeT5 based [69] 224M CodeTrans
(CT) PLBART [19] PLBART [19] 140M
Quantitative analysis
Collection
Benchmark
Qualitative analysis
New data
from GitHub
Perform
survey
Categorize &
analyzeManual task
solving
Performance
Evaluation
Generation
w/o prompts
Comparison 
w/ baslines
Fig. 1: Overall workflow of our study.
The leaderboard can be found from each dataset: CodeXGLUE
(CSN & CT)3, HumanEval4, and MBPP5.
D. Evaluation Metrics
We use the same metrics as the benchmarks to evaluate
GPT-4 . For code generation, we use pass@k[56], defined as
the probability that one of the top k-generated samples passes
the unit tests (with k set to 1). For code summarization, we
useBLEU [70], which assesses similarity to the ground truth
using n-gram precision. For code translation, we use BLEU ,
ACC [71], and CodeBLEU [72], which combines n-gram
precision, keyword matching, AST matching, and dataflow
matching.
IV. M ETHODOLOGY AND PROTOCOLS
In this section, we discuss the details of the methodology
and the protocol designs for our quantitative and qualitative
study. Figure 1 shows the overall workflow of this study.
A. Quantitative Study of GPT-4 with Baselines (RQ1)
To quantitatively assess the performance of GPT-4 for each
ASE task, we use widely known and used benchmark datasets:
CodeXGLUE, HumanEval, and MBPP (details in Section
III-B). For the baseline on fine-tuned models, we report the
scores from the leaderboard of each benchmark and compare
them with the evaluation metrics calculated from the results
ofGPT-4 . To generate the results from GPT-4 , we use three
different kinds of prompting strategies:
1)Basic prompting [21], [32]: we directly query GPT-4
with the input (code or description) and ask to generate
solutions in the form of the desired output.
3https://microsoft.github.io/CodeXGLUE/
4https://paperswithcode.com/sota/code-generation-on-humaneval
5https://paperswithcode.com/sota/code-generation-on-mbpp

--- PAGE 4 ---
2)In-context prompting [31], [33]: together with the basic
prompt, we give a set of input/output examples to GPT-
4. This idea is very similar to few-shot learning in the
context of fine-tuned language models.
3)Task specific engineered prompting [34], [35], [73]:
together with the basic prompt, we design additional
prompts to guide GPT-4 in generating better results for
each task.
We have selected three prompting strategies covering a range
of prompts from simple to more advanced. We use a small
subset (between 10 and 50, depending on the task) to evaluate
how a crafted prompt results from the actual data (using the
BLEU metric). This phase is done to avoid obvious mistakes
and not to optimize the prompt for each task. Note that
finding the optimal prompting strategy is not the goal of
the paper. Although the selections of prompts do not cover
all possibilities, our goal is to provide an initial study on
automated prompting techniques in three different levels of
complexity. We select basic prompting as a baseline or the
least complex, in-context learning as one of the best and most
generic prompting approaches (at the time of conducting this
study) and task-specific prompting as a specialized ad-hoc
prompt that potentially outperforms in-context learning. We
make the basic prompt as simple and generic as possible
to compare how adding different strategies shows/improves
different results. Similarly, for in-context learning, we want to
keep the prompt as close to the basic prompt as possible and
only add different contexts so we can compare how giving
examples improves the basic prompts.
1) Basic Prompting: The basic prompts used for each task
are shown in Listing 1. They are the simplest prompts that
consist of the input code/description and the description of
the expected output.
### code summarization task
f"Given a {lang} code snippet surrounded in ???,
generate one line of semantic focused and
abstract summarization ???{code}???"
### code generation task
f"Generate Python source code for a function, given
a natural language prompt (surrounded by ???)
describing the function’s purpose. Output only the
Python source code on exactly one line. ???{nl}???"
### code translation task
f"Given that the code surrounded by ??? is written
in C#, output only the corresponding Java code
condensed on one line ???{code}???"
Listing 1: Basic prompt used to query.
2) In-Context Prompting: For in-context prompting, we
add input/output examples on the basic prompt as additional
contexts. In-context prompting is one of the advanced types
of automated prompts that help LLM learn the additional
context. We select three input-output examples for the context.
A very high number of examples will consume more tokens
that might not fit into the allowed context length and also be
more expensive. To select the context examples, we followthe existing studies [29], [74] that used BM25 similarity-
based selection [75]. We calculate the similarity of each test
instance’s input to each training instance’s input and select
the top three similar instances for the context examples. The
training set here refers to the benchmark datasets, i.e., the
dataset in Table. I. Although we do not use them for training,
we use them to select the context examples. It was shown
to outperform the random selection or the fixed selection for
in-context learning [29], [74].
3) Task-Specific Engineered Prompts: We created task-
specific prompts using simple heuristics to address the chal-
lenges we faced with basic and in-context prompting strate-
gies. We manually analyzed a subset of outputs to identify
the root causes of issues. We discuss the challenges and how
we addressed them with task-specific prompts in the following
paragraphs.
For code summarization, we tested GPT-4 ’s initial responses
using ten samples and found that the BLEU score decreased
due to verbose explanations. To improve the score, we in-
structed GPT-4 to limit the output to 15 tokens, the average
token length of the training set’s outputs. We also found cases
where GPT-4 tries to summarize code for each line of code at
the syntax level. To mitigate this, we added “generate one line
of semantic focused and abstract summary of the code” to the
prompt. Also, due to the creativity and immense dictionary of
GPT-4 , it generates synonyms that are not used in the input.
To mitigate this problem, we added instructions to guide the
model in using naturalized identifiers to form the summary.
For code generation, we found that GPT-4 had a hard time
inferring the steps that were missing in the input (descrip-
tion of code). To mitigate this, we tried to guide GPT-4 to
consider the steps required to solve the described problem
carefully. First, we selected five potential prompts. Two of
them used in-context prompting, one used chain-of-thought
[36], and the final two prompts combined in-context and chain-
of-thought. For the prompts with in-context prompting, we
randomly selected three samples from the sanitized train set
for the MBPP dataset and the test set for the HumanEval
dataset. For the samples, we selected 50 examples out of
120 from the sanitized train set for the MBPP dataset and 50
randomly selected examples out of 164 from the test set for the
HumanEval dataset. Based on the Pass@1 scores, we chose
the best-performing prompt. From the preliminary evaluation,
we found that using only the chain-of-thought resulted in the
best performance.
For code translation, we selected ten samples to check
the initial results. We observed that GPT-4 fails to generate
syntactic keywords that were prevalent in the ground truth,
e.g.,virtual andoverride . However, missing syntactic
keywords were easily mitigated by giving more context to
the different languages, i.e., input-output examples. Therefore,
we decided to modify the in-context prompt to overcome its
weaknesses. Five code pair examples were chosen as opposed
to three for the in-context prompt, with the rationale being that
a wider variety of lengths and keywords would provide better
exposure to the “quirks” of the dataset that the first in-context

--- PAGE 5 ---
prompt missed (e.g., C#code referring to Java libraries such
asjava.nio.ByteBuffer ). Listing 2 shows the resulting
task-specific prompt.
### code summarization task
f"Generate one line of semantic focused and abstract
summary of the code surrounded by ???. Compose
the summarization by naturalizing the identifier
of variables and function names in the code as
keywords. The summarization should be very
concise, with an approximate limitation of
around 15 tokens in length."
### code generation task
f"Generate Python source code for a function, given
a natural language prompt (surrounded by ???)
describing the function’s purpose. Carefully
consider the steps required to fulfill the
function’s purpose stated in the natural
language prompt. Output only the Python source
code on exactly one line. ???{nl}???"
### code translation task
f"Given the five examples of translating Java code
to C# code (both surrounded by ???), translate
the 5 Java code samples (surrounded by ???) to
C# code and output each C# code on exactly one
line. Do not include any extra text such as ’C#
code sample’, and do not include surrounding ???
either. ???{5_i_o_examples}???"
Listing 2: Task-specific prompt used to query.
B. Qualitative Analysis of Participants’ Perception and Inter-
action with GPT-4 (RQ2 and RQ3)
To answer RQ2 and RQ3, we performed a user study by
surveying participants from both academia and industry. We
first discuss the protocols for sampling and conducting this
empirical evaluation.
Survey Participants: We conducted a user study with 27
graduate students (recruited via convenience sampling [76]
from an EECS mailing list; 31 respondents, 27 selected for
availability) and 10 industry professionals (recruited from 26
invited across Canadian, U.S., and Asian tech companies; 12
respondents, 10 selected).
Participants’ demographics are shown in Figure 2. From
Academic participants, the distributions are 77.8% M.Sc.,
18.5% Ph.D., 1 Postdoc. From Industry participants, the distri-
butions are 4 senior software engineers, 3 software engineers,
1 associate ML developer, 1 lead engineer, and 1 MLOps lead.
For LLM familiarity, 5.4% never used, 37.8% application-
level users, 29.7% users with blog/research exposure, 8.1%
implemented models, 16.2% active researchers, 2.7% no prior
exposure.
Study Design: The survey consists of three primary phases,
i.e., the background and demographic survey, user observation,
and post-experimentation survey.
Background and Demographic: We ask for participants’
demographic information, i.e., years of experience in pro-
gramming outside of school, degree of familiarity with LLMs,
current position, degree of familiarity with the target language
(Java andC#), and the level of technical writing skills in
English. We asked six close-ended questions overall.
0%25%50%75%100%
Student Java Industry Java Student C# Industry C#Advanced Intermediate Beginner(a) Participant programming language proficiency.
0%25%50%75%100%
Student experience Industry experience Student writing Industry writingNative
Excellent
Very Good
Good
Fair
6<
4-6
1-3
<1
(b) Programming experience and writing proficiency.
Fig. 2: Demographics from the survey.
User Observation Study: We assign two out of three tasks
to each participant, considering their familiarity with pro-
gramming languages. Since participants should have some
knowledge about C#for the code translation task, we first
asked if they knew C#and only gave them the code translation
task. The rest of the tasks were assigned randomly. For each
task, we gave one sample from each of 3 projects. The partic-
ipants worked on the task in a conversation with ChatGPT-4 .
ChatGPT-4 is a chat-interfaced framework that runs on GPT-4
6. We keep the settings of ChatGPT-4 to default. We ensured
a balanced allocation of task samples to participants randomly
(with an equal number of participants per project and task).
We ask participants to start with their basic prompt and then to
proceed with conversations to improve the result based on their
first evaluation until they consider the solution satisfactory
or have spent 5 minutes per task sample. In this setup, we
have not provided any example prompts to avoid any bias and
to get participants’ fresh perspectives. Overall, we evaluated
three tasks over six samples per task. Each sample has been
assigned and solved with an average of 12 times by different
participants [77]. The survey was designed to take no more
than 60 minutes for each participant.
6https://chat.openai.com/

--- PAGE 6 ---
Post-Experimentation Survey: After the observation study,
we posed four closed-ended questions to the participants. We
inquire whether they received an incorrect response from the
model, the degree to which they found GPT-4 useful for the
assigned task, their perception of response quality on the first
and last attempt, and their overall assessment. Additionally,
we included four open-ended questions to assess the utility of
theGPT-4 model. Strengths and weaknesses of the first and
last response for both tasks.
For RQ2, we report the responses from the four close-ended
questions explained in the post-experimentation survey. We
show the percentage of the evaluation responses from all the
participants. The evaluation of each survey question can be
answered to one out of the five following selections: 1) poor,
2) fair, 3) good, 4) very good, and 5) excellent. We consider
the evaluation to be satisfactory if they answer either good,
very good, or excellent.
For RQ3, we investigate the chat logs of the participants
and manually categorize the conversational prompts. We first
extracted the conversational prompts as a list of abstract
instructions for the qualitative analysis. We then used an
Open Coding Approach, where two authors of this paper,
who were not involved in the extraction process, performed
sessions of card sorting for our qualitative data analysis [78].
This analysis design was chosen to make the categorization
and extraction as independent from each other as possible.
After that, a third author acted as a moderator to resolve
the disagreements. Finally, all other authors are involved in
manually reviewing the forms completed by the three authors
and resolving any disagreements that occurred during the
process to ensure consensus among reviewers. Note that we
also provide the category of conversational prompt and the
raw chat logs for each participant in our replication package
to provide transparent results of our work. After categorizing
the types of conversational prompts, we count and rank the
occurrences for each task to show the overall trends among
the participants.
C. Analysis of Users’ Conversational Prompts (RQ4)
For RQ4, we investigate each prompt created by the par-
ticipants and the response by GPT-4 . Since GPT-4 generates
verbose explanations for its response, we manually extract the
core part of the answer for each task. For code generation
andtranslation tasks , we excerpt the code snippets from the
generated response. For code summarization tasks , we excerpt
the texts that summarize the given code snippet. The responses
are written in a specific code editor format, allowing us to
extract them from the logs manually. After we extract the
answers, we rank them based on the BLEU score. For each
category of conversational prompt, we calculate the mean re-
ciprocal rank (MRR) to find which type of prompt constitutes
the most to a higher rank. Lastly, we report the average BLEU
score of the conversational prompts, the highest BLEU score
achieved by a participant, and the generation from the two
automated prompt strategies we used in our quantitative study,
i.e., basic prompts and tasks-specific prompts, to compare theTABLE III: Result of code summarization. Bold denotes the
highest score. The asterisk (*) denotes that the source was not
public. GPT-4 + B/IC/TS denotes basic, in-context, and task-
specific prompts, respectively.
BLEU
Model Ruby JS Go Py Java PHP Avg
StarCoder-LoRA* 17.21 18.15 21.61 23.13 22.61 28.76 21.91
DistillCodeT5* 15.75 16.42 20.21 20.59 20.51 26.58 20.01
PolyglotCodeBERT [20] 14.75 15.80 18.77 18.71 20.11 26.23 19.06
CoTexT [58] 14.02 14.96 18.86 19.73 19.06 24.68 18.55
ProphetNet-X [60] 14.37 16.60 18.43 17.87 19.39 24.57 18.54
GPT-4 + B 19.93 19.96 29.70 27.19 24.11 18.63 23.25
GPT-4 + IC 19.60 20.08 28.98 19.90 24.45 19.24 22.04
GPT-4 + TS 25.48 27.23 35.34 29.61 33.08 30.67 30.24
TABLE IV: Result of code generation task.
HumanEval MBPP
Model Pass@1 Model Pass@1
CODE-T (davinci-002) [66] 65.80 CODE-T (davinci-002) [66] 67.70
CODE-T-Iter (davinci-002) [66] 65.20 StarCoder2 [67] 66.20
PanGu-Coder2 [61] 61.64 CODE-T (davinci-001) [66] 61.90
WizardCoder [63] 57.30 CODE-T (cushman-001) [66] 55.40
XCoder [64] 57.30 StarCoder [62] 52.70
GPT-4 + B 69.51 GPT-4 + B 39.60
GPT-4 + IC 64.63 GPT-4 + IC 40.60
GPT-4 + TS 74.39 GPT-4 + TS 39.40
performance of conversational prompts. We did not include the
in-context prompt as a comparison baseline for this experiment
as we do not have a separate training set to select similar
context examples (as mentioned in Section IV-A2).
All the survey questions, survey responses, the raw chat logs
from the participants, and analyzed prompt categories by the
authors are available in our replication package.
V. E XPERIMENTAL RESULTS
A. RQ1: Automated Prompts vs. Fine-Tuned Models
Table III shows the results of fine-tuned baselines and
GPT-4 on the code summarization task. The basic prompt-
ing improved the 1st ranked baseline by 1.34% points on
average. However, this pattern wasn’t consistent across all
programming languages where for PHP, we see a 10.13%
points decrease. We observe the biggest improvement from
Gowith 8.09% points improvement. The in-context prompting
improved the 1st ranked baseline by 0.13% points on average.
However, the inconsistent pattern was similar to the basic
prompt where a decrease was shown in PHP with a 9.52%
points and the highest improvement from Gowith a 7.37%
points improvement. When comparing in-context prompting
to basic prompting, it didn’t improve the basic prompting,
showing a 1.21% points decrease on average. The lowest
decrease was found in Python with 7.29% points. The
highest improvement was found in PHP with 0.61% points.
The task-specific prompt had the most noticeable improve-
ment compared to the 1st ranked baseline with 8.33% points
improvement on average. When comparing task-specific to the
basic prompt, it improved by 6.99% points overall.
Table IV shows the results for the code generation tasks.
The basic prompting was able to improve the 1st ranked
baseline with an increase of 3.71% points for HumanEval.
However, for MBPP, it decreased by 28.10% points. The

--- PAGE 7 ---
TABLE V: Result of code translation. Bold denotes the highest
score. The asterisk (*) denotes that the source was not public.
GPT-4 + B/IC/TS denotes basic, in-context, and task-specific
prompts, respectively.
Java to C# C# to Java
Model BLEU ACC CB BLEU ACC CB
StructCoder [68] 85.02 66.60 88.42 80.66 67.70 86.03
PLNMT-sys0* 83.37 64.60 87.38 80.91 66.80 85.87
PLBART [19] 83.02 64.60 87.92 78.35 65.00 85.27
CodePALM* 83.26 65.50 86.37 78.94 65.20 83.74
ContraBERT G* 80.78 59.90 84.98 76.24 60.50 81.69
GPT-4 + B 55.33 5.50 68.34 63.08 20.20 73.57
GPT-4 + IC 80.72 34.10 86.69 84.12 50.20 87.44
GPT-4 + TS 80.55 34.30 86.87 81.29 49.20 85.33
in-context prompting also couldn’t outperform the baselines
(1.17% points decrease for HumanEval and 27.64% points
decrease for MBPP). When comparing in-context prompting
to basic prompting, there is a decrease of 4.88% points in
HumanEval and a small increase of 1.00% points in MBPP.
The task-specific prompting improved from the baseline, with
an 8.59% points increase in HumanEval. However, for MBPP,
it decreased by 28.30% points. When comparing task-specific
to basic prompting, it didn’t show a significant improvement,
showing a 4.88% points improvement in HumanEval and a
decrease of 0.20% points in MBPP.
Table V shows the results for code translation. The basic
prompting couldn’t outperform the 1st ranked baseline with a
decrease of 29.69% points in BLEU, 61.10% points in ACC,
and 20.08% points in CodeBLEU for translating Java-C# .
Similar patterns showed when translating C#-Java with a
decrease of 17.83% points in BLEU, 47.50% points in ACC,
and 12.46% points in CodeBLEU. The low ACC scores when
translating Java-C# are mainly caused by the failure to
predict the frequently used syntactic keywords (e.g., virtual
andoverride ). For example, the keyword override ap-
peared in 216, and virtual appeared in 431 out of the 1,000
test samples (totaling 647, as no sample used both keywords).
Conversely, GPT-4 generated the keyword override in only
48 results and never generated virtual . We also observed
that it failed to infer code elements that are missing from
the input, e.g., parameters in API calls that are not in the
source language but are in the target language. Since ACC
requires exact matching, a single token could prevent an
exact match, severely impacting the overall ACC score. We
observed another type of failure which was caused by the LLM
hallucination [79]–[82] where the model generates content that
is irrelevant, made-up, or inconsistent from the input data.
For example, the model would generate APIs that are not
used in the source language or even APIs that don’t exist.
ForC#-Java , we did not find any noticeable frequently
omitted syntactic keyword, which could explain the relatively
higher ACC score. A drastic improvement was achieved by in-
context and task-specific prompting, especially for generating
the missing syntactic keywords. Overall, GPT-4 with in-
context prompting showed mixed performance, i.e., it couldn’t
outperform the baselines in translating Java-C# while therewas an improvement in C#-Java . When comparing in-
context prompting and basic prompting, it showed a drastic
improvement in both Java-C# andC#-Java . We observed
that task-specific prompting outperforms basic prompting.
However, it could not outperform in-context learning, as we
found in our initial subset when designing the prompt.
Overall, GPT-4 with different automated prompting strate-
gies could not outperform the fine-tuned models significantly.
For code summarization, a reason for this can be due to the
verbose and creative generation of GPT-4 . Since the evaluation
metrics heavily rely on textual similarity, diverse and verbose
generation will significantly impact the metric scores. When
we tried to mitigate this with our task-specific prompt, the
result improved by a big margin, outperforming the fine-tuned
baselines. For code generation, the code generated by the
GPT-4 seems plausible and executable. However, they fail to
pass the tests, e.g., off-by-one error, using imprecise logical
operators (choosing between and andoroperator), or missing
to generate intermediate steps that are not mentioned in the
NL description. A possible reason is that since it is not fine-
tuned, it is harder to generate code that requires project-/data-
specific knowledge, which could be leveraged to generate code
with correct functionality. For code translation, the model
failed to generate the same code (low ACC) for both target
languages. The possible reasons can be: 1) a much larger
and diverse corpus is used for training, resulting in a more
creative generation, and 2) generating something irrelevant to
the input, i.e., LLM hallucination. Failing to generate similar
(low BLEU) C#code compared to Java code can be due
to the difference in data size used for training GPT-4 , since
Java is more commonly used than C#in GitHub7.
Answer to RQ1 : Automated prompting with GPT-4 did
not significantly outperform fine-tuned baselines, except
for code summarization, indicating the need for more so-
phisticated prompting strategies to fully leverage GPT-4 ’s
capabilities.
B. RQ2: Participants’ Perception of GPT-4
Figure 3 shows how the participants qualitatively assessed
the responses of GPT-4 . The first bar shows participants’
prior perception of GPT-4 ’s general usefulness. More than
83.8% of the participants were satisfied ( >=Good) with GPT-
4. Some of the participants (5.4%) have never used GPT-4
before. The second bar shows if they encountered any incorrect
responses on any steps during the study. We could see that
almost half (48.6%) of the participants did not encounter any
incorrect responses during the study. The third bar shows their
evaluation of the responses from the first attempt of prompts,
and the last bar shows their evaluation at the end of the
experiment. The evaluations for initial responses were already
considerably good with more than 83.8% satisfactory score
7https://gitnux.org/github-languages-statistics/

--- PAGE 8 ---
0%25%50%75%100%
Usefulness Incorrect First attempt End of experimentNo
Yes
N/A
Excellent
Very Good
Good
FairFig. 3: RQ2: Qualitative results.
(>=Good). After applying the conversational prompt, the re-
sponses received higher evaluations with a 94.6% satisfactory
score ( >=Good), an increase of 10.8% points.
We can see that their evaluations change positively from
the initial responses to the end of the experiment (excellent
+24.3%), indicating that their conversational prompt helped
GPT-4 in generating better responses. This result is also
consistent with previous studies that multiple conversational
prompting improves the results of LLMs [83]–[85].
Answer to RQ2 : Our qualitative analysis shows that par-
ticipants were generally satisfied with GPT-4 ’s responses.
Satisfaction increased from 83.8% to 94.6% after using
conversational prompts, a 10.8% points improvement, in-
dicating the benefit of conversational prompts.
C. RQ3: Trend in Forming Conversational Prompts
From analyzing the chat logs, we identified nine types
of conversational prompts: (1) Request improvements with
certain keywords, (2) Provide more context, (3) Add specific
instructions, (4) Point mistakes then request fixes, (5) Ask
questions to guide the correct way, (6) Request verification,
(7) Request more examples, (8) Request more detailed de-
scription, and (9) Request another or a different version of
generation.
Table VI shows the categorized conversational prompts
and their numbers used for each task. We can see that the
trends tend to differ per task. For code summarization, we
can see that requesting certain improvements with different
keywords was the most common. The keywords for requesting
improvement are e.g., “more natural”, “abstract”, “semantic-
focused”, “human-readable”, etc. We suspect they requested
these improvements due to GPT-4 ’s verbose generations,
which generate line-by-line explanations of the code’s syntax.
Code summarization focuses on automating the technical doc-
umentation process as it is a nontrivial task [86]. Explaining a
method’s syntax line-by-line would defeat the whole purpose
of the task as developers would not read all the syntax
explanations. To get the implicit summary of the method’sfunctionality, we observe that most of the participants request
improvements in abstracting and focusing on the semantics.
Previous literature also points to the verbosity of LLMs’
explanations of code [40], [87]. In addition, some participants
requested more descriptions for core logic, where syntax-level
explanations are lacking, and added specific instructions to
move the line-level explanations to the top of the method to
resemble technical documentation, e.g., JavaDoc. Since the
output of the code summarization task is a natural language
description of code, participants also request the model to
generate descriptions with a certain style or format.
Adding specific instructions was found to be the most
common for code generation tasks. The instructions have a
variety of ranges, e.g., “remove the main method”, “remove
the import statements”, “use a certain type for the variables/-
parameters”, “remove external packages”, etc. Close runner-
ups were “requesting improvements” and “asking questions to
guide the model”. The keywords used for requesting improve-
ment in code generation are e.g., “good format”, “readable”,
“better quality”, “most succinct working”, “clear”, etc. An
example for asking questions to the model would be, e.g.,
“Is the following code snippet syntactically correct?”, “Is
the generated code executable?”, “Is this the most efficient
version of code?”, etc. These questions aren’t the type to
ask to perform a specific instruction but to question the
reasoning path of the model to re-think their decision and
then make changes if they were incorrect. We suspect these
conversational prompts are made as GPT-4 fails to infer some
missing specifications that aren’t explained in the abstract text
description of code. To fill these gaps, participants would
1) add specific instructions, 2) ask for improvement of code
quality, or 3) ask questions for verification or to guide the
model into a different reasoning path. Generally, our findings
are aligned with previous literature [88] in that developers tend
to add specific instructions for refactoring (i.e., add/remove
certain code elements) or request improvement code quality
from LLMs. “Asking questions for verification to guide the
model to correct reasoning paths” is a new pattern we found
in this study. We also observed that participants requested to
remove irrelevant generations (e.g., important statements, main
method, external packages) due to LLM hallucination.
For the code translation task, the most common conver-
sational prompt was to give certain instructions to perform.
These instructions include, e.g., “using a certain type”, “using
a certain logic”, “modifying the code to support certain
data types”, “removing import statements”, “handling certain
parameters”, etc. The runner-up asked certain questions to
verify or double-check if the generated code met the specific
syntax of the target language. Generally, the results were
similar to code generation as their output is both source code.
Other commonly used conversational prompts include “adding
more context information”, “requesting more details or de-
scriptions”, “pointing out mistakes then requesting fixes”, etc.
A previous study found that the most prevalent translation bug
was data-related, i.e., data types, parsing input data, and output
formatting issues [52]. We can observe that the participants try

--- PAGE 9 ---
TABLE VI: Trend: the number of different prompt categories
participants used for each task.
Prompts ComGen Codgen CodTrans
Request improvements 12 7 5
Request more description 7 - -
Add specific instructions 7 8 9
Ask questions to find correct way 6 7 7
Add more context 5 6 3
Request examples 3 1 1
Request verification 2 1 1
Point mistake then request fix 2 5 4
Request another generation 1 - 2
to address these problems by adding specific instructions like
“use a certain type”, “modify to support certain data type”,
or “handle certain parameters”. We also observe that similar
to the code generation task, they tried to remove irrelevant
generations due to LLM hallucination. Previous studies have
also reported that more than one-third of translation bugs are
due to syntactic and semantic differences between languages.
This finding is also in line with our previous findings in
Section IV-A3 that they fail to generate certain keywords
specific to the target language. We observe that participants
tried to mitigate this by adding specific instructions or asking
questions for target language-specific syntax.
Answer to RQ3: Different trends were observed in conver-
sational prompts per task. “Requesting improvements with
general keywords” was most common for code summariza-
tion, while “adding specific instructions” was most frequent
for code generation and translation. Participants used con-
versational prompts to address known LLM limitations, such
as verbosity, missing information, and hallucination.
D. RQ4: Efficacy of Conversational Prompts
RQ4 aims to investigate which category of conversation
prompts contributes to better results. Table VII shows the
top five ranks of conversational prompts that contribute the
most to better results. “Requesting improvement” was shown
to be the most effective conversational prompt category for
code summarization. “Adding more context information” was
shown to improve code generation the most. For code transla-
tion, “asking questions to find the correct reasoning path” was
shown to have the best performance. We can observe that the
ranking of trends and the ranks that best improve the responses
are very similar but not the same. For example, the first rank
in trend and improvement for code summarization were both
requesting certain improvements. For code generation, adding
specific instruction was first ranked on the trend ranking but
appeared second in the improvement ranking. Adding more
context was found to be the first for improvement but found to
be the third most common in the trends. For code translation,
asking questions to find the correct path was the first rank in
improvement but appears on the second in the trend ranking.
Adding specific instructions was the most common prompt
but was found to be the third most effective in improvement.From the observation, we can state that the conversational
prompt that requested certain improvements was most effective
in mitigating verbose and syntax-focused code summariza-
tion. For example, “please make summaries more natural
and abstract”, “improve this by generating semantic-focused
and abstract summaries”, etc. The prompt resulted in GPT-4
generate that are closer to the ground truth, which are short
phrases that summarize the functionality of the method. For
code generation, adding more context was most effective to
mitigate the limitation of GPT-4 in inferring the missing steps
that the abstract text description encapsulates. For example,
they would add the class information, e.g., class signature,
description of what the class does, and other public method
signatures within the class. The model was able to infer
the missing steps by the project- or class-specific knowledge
provided by the additional context. For code translation, asking
questions to find the correct way was the most effective in
generating the correct syntax of different languages and data
types. For example, “what about the @Parameter annotation?”,
“The JSON.parseArray function needs to know the generic
type T to know the format of the return value, how should
I modify the code?”, etc. As the examples show, they would
ask questions to guide the model in resolving the data issue.
The similarity of the two ranks can be interpreted as partici-
pants having a good knowledge of how to guide the models for
better results, even though the ground truth wasn’t provided.
It is interesting to find that “requesting improvement” actually
helps generate better responses even though these prompts do
not necessarily provide specific knowledge on how to improve.
We also report the average BLEU score from the responses
of conversational prompts in Table VIII. We can see that
the average BLEU score of all conversational prompts is
comparable to the two automated prompts, i.e., basic and
task-specific prompts. However, only the code translation task
outperforms the better-automated prompt, i.e., task-specific
prompt, by a mere 0.27% points. The reason is due to the
large variance caused by the different types of conversational
prompts. The conversational prompt that produces the highest
BLEU score outperforms the automated prompts by a big
margin, 15.8% points for code summarization, 18.3% points
for code generation, and 16.1% points for code translation
tasks.
Answer to RQ4: Overall, conversational prompts can im-
prove code tasks. “Requesting improvement” worked best
for code summarization, “adding context” for code gen-
eration, and “asking questions” for code translation. The
rank of trends and improvements showed similar patterns,
indicating participants effectively guided the models. The
best conversational prompts significantly outperformed au-
tomated prompts, improving by 15.8% points, 18.3% points,
and 16.1% points for code summarization, generation, and
translation, respectively.

--- PAGE 10 ---
TABLE VII: Improvement: Top-5 prompt categories that ef-
fectively improve the basic prompts.
Rank Code summarization Code generation Code translation
1 Request improvements. Add more context. Ask questions.
2 Add more context. Add instructions. Point mistake then fix.
3 Ask questions. Request improvements. Add instructions.
4 Add instructions. Ask questions. Request improvements.
5 Request verification. Point mistake then fix. Add more context.
TABLE VIII: Result of conversational prompts (conv.) vs
automated prompts. Bold indicates the highest score.
BLEU
Task Average conv. Highest conv. Basic Task-specific
CodSum 36.22 55.75 35.76 39.95
CodGen 53.15 69.16 57.33 50.85
CodTran 67.43 83.26 52.10 67.16
E. Discussion
In this section, we discuss the main findings of this study
in terms of prompt engineering and summarize them as
recommendations for future work. Then, we discuss the trade-
offs between prompt engineering and fine-tuning.
As the quantitative results indicate, even though GPT-4 is a
larger model than its baselines, it does not always yield better
results. Even typical prompt engineering strategies, such as
in-context learning, did not show significant improvement. On
the other hand, the qualitative user study revealed that the key
potential of GPT-4 is on conversation-based prompting, which
includes human feedback in the loop.
There are two takeaway messages from our results: (a) To
leverage LLMs to their best, a sequence of back-and-forth
queries with the model may be needed. Iterative processes such
as search-based optimization or reinforcement learning-based
agents might be useful to “engineer” prompts for ASE tasks
with full automation. (b) Human feedback still plays a crucial
role in optimizing the prompts, as shown by conversational
prompting. Thus, to remove humans from the loop completely,
future studies are needed to analyze developer-written prompts
more carefully and extract patterns that can be implemented
as rules, fitness functions, rewards, and policies.
To summarize our findings, we conclude that, like most
engineering problems, the choice between prompt engineering
and fine-tuning LLMs is a trade-off. The following are some
of the main factors that play a role in the trade-off:
•Performance: The most explicit aspect is the perfor-
mance of the models. From the results, neither approach
always dominates the other. However, conversational
prompt engineering has shown to have great potential.
•Cost: Although fine-tuning is usually considered more
expensive than prompt engineering, depending on the
subscription models of the commercial LLMs, inference
fees can quickly become a huge burden. For example,
running GPT-4 for this study costs around 1,500 USD.
Therefore, developers who have a large number of in-
stances for inference should consider using a cheaper
model or even fine-tuning a smaller in-house model.•Ease of Use: Prompt engineering has a significant benefit
from this aspect, especially when using natural language-
based interfaces. It becomes more significant as we see
participants with little to no background in LLMs can per-
form ASE tasks with ChatGPT-4 . In contrast, they would
not be able to perform with fine-tuned models without
a certain knowledge of deployment, configuration, and
developing fine-tuned models.
•Control: Finally, another aspect of the trade-off is the
level of control on the internal configurations and outputs.
For example, with GPT-4 , although you can set the tem-
perature to zero, you still get non-deterministic answers,
which makes the verification of the models more difficult
and thus less interesting to some users.
VI. T HREATS TO VALIDITY
Internal Validity: A key threat is whether the designed
prompts truly reflect the intended strategy or if better ap-
proaches could exist. For example, poor results of GPT-4 with
in-context learning might be due to confounding factors, such
as suboptimal sample choices or vocabulary. To address this,
we reported the exact prompts used and conducted a qualitative
survey where participants created their own prompts, providing
a broader view of prompt engineering effectiveness. Another
potential threat to validity arises from not explicitly examining
biases such as task order and participant learning effects,
though we mitigated order-related risks by randomizing task
sequences across participants. The implications of participant
segregation, while a compelling avenue for future analysis,
were beyond the scope of this work due to brevity and will
be explored in future research.
Construct Validity: We used common metrics, such as
BLEU scores, as in the original benchmarks. However, these
metrics may not fully capture what they aim to measure.
In the qualitative study, participants’ evaluations of GPT-
4’s responses might be subjective. To mitigate this, we also
performed a manual analysis by three co-authors. Investigating
better human-aligned metrics remains an open area in the field.
Conclusion Validity: The non-deterministic behavior of
LLMs [43] means results may not be reproducible. We set
the temperature parameter to zero to increase determinism
but acknowledge this does not eliminate the issue entirely.
We provide generated responses from the quantitative study
and raw chat logs from the survey to address this. A further
limitation lies in the absence of statistical tests for quantitative
comparisons, as fine-tuned model baseline distributions were
unavailable (benchmark scores were sourced directly from
leaderboards), and the qualitative analysis’s limited sample
size (6 samples per task) precluded meaningful statistical
testing. While these constraints temper the robustness of our
conclusions, we intend to address them in future work through
expanded datasets and formal statistical evaluations.
External Validity: We used recent state-of-the-art fine-
tuned models for each task, but new baselines could alter our
observations. Although we used widely accepted benchmark
datasets, they do not represent all domains. For the survey, we

--- PAGE 11 ---
used 18 samples to limit participant time, acknowledging that
this limits generalizability. We chose a 1-hour survey duration
to recruit more participants, balancing practical constraints like
participant availability and budget. To mitigate this threat, we
considered diverse domains and difficulty levels in the exam-
ples. We also included three different project types (library,
algorithm, and web) to cover different domains. While in-
context learning was one of the best automated prompting
strategies at the start of our study, newer methods like Retrieval
Augmented Generation (RAG) [89] and Self-Reflection [90]
could affect our findings, warranting further study.
VII. C ONCLUSION
In this paper, we evaluated GPT-4 using basic, in-context
learning, and task-specific prompts and compared them with
fine-tuned language models. We also conducted a user study
with 27 academic and 10 industry participants to assess the
quality of GPT-4 ’s responses and examine how participants
designed conversational prompts. Our quantitative results show
that GPT-4 does not significantly outperform the baselines.
Qualitative results indicate that participants often request im-
provements, add context, or give specific instructions, which
helps GPT-4 generate better responses. Our study suggests that
GPT-4 has great potential for code tasks but requires careful
human verification and interpretation.
REFERENCES
[1] S. Wang, T. Liu, and L. Tan, “Automatically learning semantic features
for defect prediction,” in Proceedings of the 38th International Confer-
ence on Software Engineering , 2016, pp. 297–308.
[2] M. White, M. Tufano, C. Vendome, and D. Poshyvanyk, “Deep learning
code fragments for code clone detection,” in Proceedings of the 31st
IEEE/ACM international conference on automated software engineering ,
2016, pp. 87–98.
[3] P. Yin and G. Neubig, “Tranx: A transition-based neural abstract syntax
parser for semantic parsing and code generation,” in Proceedings of
the Conference on Empirical Methods in Natural Language Processing
(Demo Track) , 2018.
[4] H. Liu, J. Jin, Z. Xu, Y . Zou, Y . Bu, and L. Zhang, “Deep learning based
code smell detection,” IEEE transactions on Software Engineering ,
vol. 47, no. 9, pp. 1811–1837, 2019.
[5] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning to rep-
resent programs with graphs,” in International Conference on Learning
Representations , 2018.
[6] Y . Wei, C. S. Xia, and L. Zhang, “Copiloting the copilots: Fusing
large language models with completion engines for automated program
repair,” in Proceedings of the 31st ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering , 2023.
[7] X. Gu, H. Zhang, and S. Kim, “Deep code search,” in Proceedings of
the 40th International Conference on Software Engineering , 2018, pp.
933–944.
[8] J. Shin and J. Nam, “A survey of automatic code generation from natural
language,” Journal of Information Processing Systems , vol. 17, no. 3,
pp. 537–555, 2021.
[9] J. Shin, M. Wei, J. Wang, L. Shi, and S. Wang, “The good, the bad, and
the missing: Neural code generation for machine learning tasks,” ACM
Transactions on Software Engineering and Methodology , vol. 33, no. 2,
pp. 1–24, 2023.
[10] J. Shin, S. Hashtroudi, H. Hemmati, and S. Wang, “Domain adaptation
for code model-based unit test case generation,” in Proceedings of the
33rd ACM SIGSOFT International Symposium on Software Testing and
Analysis , 2024, pp. 1211–1222.
[11] J. Shin, R. Aleithan, H. Hemmati, and S. Wang, “Retrieval-augmented
test generation: How far are we?” arXiv preprint arXiv:2409.12682 ,
2024.[12] J. Shin, H. Hemmati, M. Wei, and S. Wang, “Assessing evaluation
metrics for neural test oracle generation,” IEEE Transactions on Software
Engineering , 2024.
[13] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment genera-
tion,” in Proceedings of the 26th conference on program comprehension ,
2018, pp. 200–210.
[14] W. Sun, C. Fang, Y . Chen, G. Tao, T. Han, and Q. Zhang, “Code search
based on context-aware code translation,” in Proceedings of the 44th
International Conference on Software Engineering , 2022, pp. 388–400.
[15] Y . Li, S. Wang, and T. N. Nguyen, “Dear: A novel deep learning-based
approach for automated program repair,” in Proceedings of the 44th
International Conference on Software Engineering , 2022, pp. 511–523.
[16] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al. , “Codebert: A pre-trained model for programming
and natural languages,” in Findings of the Association for Computational
Linguistics: EMNLP 2020 , 2020, pp. 1536–1547.
[17] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, “Learning and
evaluating contextual embedding of source code,” in International
conference on machine learning . PMLR, 2020, pp. 5110–5121.
[18] D. Guo, S. Lu, N. Duan, Y . Wang, M. Zhou, and J. Yin, “Unixcoder:
Unified cross-modal pre-training for code representation,” in Proceed-
ings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , 2022, pp. 7212–7225.
[19] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Unified pre-
training for program understanding and generation,” in Proceedings of
the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , 2021,
pp. 2655–2668.
[20] T. Ahmed and P. Devanbu, “Multilingual training for software engineer-
ing,” in Proceedings of the 44th International Conference on Software
Engineering , 2022, pp. 1443–1455.
[21] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,
A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M.
Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,
S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
I. Sutskever, and D. Amodei, “Language models are few-shot learners,”
2020. [Online]. Available: https://arxiv.org/abs/2005.14165
[22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton,
F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,
J. Leike, and R. Lowe, “Training language models to follow instructions
with human feedback,” 2022.
[23] OpenAI, “Gpt-4 technical report,” 2023.
[24] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient
foundation language models,” 2023.
[25] H. T. et al., “Llama 2: Open foundation and fine-tuned chat models,”
2023.
[26] A. C. et al., “Palm: Scaling language modeling with pathways,” 2022.
[27] R. A. et al., “Palm 2 technical report,” 2023.
[28] J. Y . Khan and G. Uddin, “Automatic code documentation generation
using gpt-3,” in Proceedings of the 37th IEEE/ACM International
Conference on Automated Software Engineering , 2022, pp. 1–6.
[29] S. Gao, X.-C. Wen, C. Gao, W. Wang, and M. R. Lyu, “Construct-
ing effective in-context demonstration for code intelligence tasks: An
empirical study,” in Proceedings of the 38th IEEE/ACM International
Conference on Automated Software Engineering , 2023.
[30] S. Feng and C. Chen, “Prompting is all your need: Automated android
bug replay with large language models,” in 2024 IEEE/ACM 46th
International Conference on Software Engineering (ICSE) , 2023.
[31] M. Geng, S. Wang, D. Dong, H. Wang, G. Li, Z. Jin, X. Mao, and
X. Liao, “Large language models are few-shot summarizers: Multi-intent
comment generation via in-context learning,” in 2024 IEEE/ACM 46th
International Conference on Software Engineering (ICSE) , 2024.
[32] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-
train, prompt, and predict: A systematic survey of prompting methods
in natural language processing,” ACM Computing Surveys , vol. 55, no. 9,
pp. 1–35, 2023.
[33] J. Liu, D. Shen, Y . Zhang, B. Dolan, L. Carin, and W. Chen, “What
makes good in-context examples for gpt-3?” in Deep Learning Inside
Out: 3rd Workshop on Knowledge Extraction and Integration for Deep

--- PAGE 12 ---
Learning Architectures, DeeLIO 2022 . Association for Computational
Linguistics (ACL), 2022, pp. 100–114.
[34] Y . He, S. Zheng, Y . Tay, J. Gupta, Y . Du, V . Aribandi, Z. Zhao,
Y . Li, Z. Chen, D. Metzler et al. , “Hyperprompt: Prompt-based task-
conditioning of transformers,” in International Conference on Machine
Learning . PMLR, 2022, pp. 8678–8690.
[35] S. Carta, A. Giuliani, L. Piano, A. S. Podda, L. Pompianu, and
S. G. Tiddia, “Iterative zero-shot llm prompting for knowledge graph
construction,” 2023.
[36] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,
D. Zhou et al. , “Chain-of-thought prompting elicits reasoning in large
language models,” Advances in Neural Information Processing Systems ,
vol. 35, pp. 24 824–24 837, 2022.
[37] T. Shin, Y . Razeghi, R. L. Logan IV , E. Wallace, and S. Singh, “Auto-
prompt: Eliciting knowledge from language models with automatically
generated prompts,” in Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , 2020, pp. 4222–
4235.
[38] K. Hambardzumyan, H. Khachatrian, and J. May, “Warp: Word-level
adversarial reprogramming,” in Proceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Processing (Volume 1:
Long Papers) , 2021, pp. 4921–4933.
[39] T.-O. Li, W. Zong, Y . Wang, H. Tian, Y . Wang, S.-C. Cheung, and
J. Kramer, “Nuances are the key: Unlocking chatgpt to find failure-
inducing tests with differential prompting,” in Proceedings of the 38th
IEEE/ACM International Conference on Automated Software Engineer-
ing, 2023.
[40] S. Kabir, D. N. Udo-Imeh, B. Kou, and T. Zhang, “Is stack overflow
obsolete? an empirical study of the characteristics of chatgpt answers to
stack overflow questions,” 2023.
[41] C. Wang, Y . Yang, C. Gao, Y . Peng, H. Zhang, and M. R. Lyu, “No
more fine-tuning? an experimental evaluation of prompt tuning in code
intelligence,” in Proceedings of the 30th ACM Joint European Software
Engineering Conference and Symposium on the Foundations of Software
Engineering , 2022, pp. 382–394.
[42] F. Trad and A. Chehab, “Prompt engineering or fine-tuning? a case study
on phishing detection with large language models,” Machine Learning
and Knowledge Extraction , vol. 6, no. 1, pp. 367–384, 2024.
[43] S. Ouyang, J. M. Zhang, M. Harman, and M. Wang, “Llm is like a
box of chocolates: the non-determinism of chatgpt in code generation,”
2023.
[44] H. Li, Y . Hao, Y . Zhai, and Z. Qian, “Assisting static analysis with large
language models: A chatgpt experiment,” in Proceedings of the 31st
ACM Joint European Software Engineering Conference and Symposium
on the Foundations of Software Engineering , 2023, pp. 2107–2111.
[45] T. Ahmed and P. Devanbu, “Few-shot training llms for project-specific
code-summarization,” in Proceedings of the 37th IEEE/ACM Interna-
tional Conference on Automated Software Engineering , 2022, pp. 1–5.
[46] T. Ahmed, K. S. Pai, P. Devanbu, and E. Barr, “Automatic semantic
augmentation of language model prompts (for code summarization),”
inProceedings of the IEEE/ACM 46th International Conference on
Software Engineering , 2024, pp. 1–13.
[47] E. Shi, Y . Wang, L. Du, J. Chen, S. Han, H. Zhang, D. Zhang, and
H. Sun, “On the evaluation of neural code summarization,” in Proceed-
ings of the 44th international conference on software engineering , 2022,
pp. 1597–1608.
[48] F. Mu, L. Shi, S. Wang, Z. Yu, B. Zhang, C. Wang, S. Liu, and Q. Wang,
“Clarifygpt: A framework for enhancing llm-based code generation
via requirements clarification,” Proceedings of the ACM on Software
Engineering , vol. 1, no. FSE, pp. 2332–2354, 2024.
[49] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al. , “Competition-
level code generation with alphacode,” Science , vol. 378, no. 6624, pp.
1092–1097, 2022.
[50] Y . Dong, X. Jiang, Z. Jin, and G. Li, “Self-collaboration code generation
via chatgpt,” ACM Transactions on Software Engineering and Method-
ology , vol. 33, no. 7, pp. 1–38, 2024.
[51] Y . Luo, R. Yu, F. Zhang, L. Liang, and Y . Xiong, “Bridging gaps in llm
code translation: Reducing errors with call graphs and bridged debug-
gers,” in Proceedings of the 39th IEEE/ACM International Conference
on Automated Software Engineering , 2024, pp. 2448–2449.
[52] R. Pan, A. R. Ibrahimzada, R. Krishna, D. Sankar, L. P. Wassi,
M. Merler, B. Sobolev, R. Pavuluri, S. Sinha, and R. Jabbarvand, “Lost intranslation: A study of bugs introduced by large language models while
translating code,” in 2024 IEEE/ACM 46th International Conference on
Software Engineering (ICSE) , 2024.
[53] Z. Yang, F. Liu, Z. Yu, J. W. Keung, J. Li, S. Liu, Y . Hong, X. Ma,
Z. Jin, and G. Li, “Exploring and unleashing the power of large language
models in automated code translation,” Proceedings of the ACM on
Software Engineering , vol. 1, no. FSE, pp. 1585–1608, 2024.
[54] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code search,”
2019.
[55] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou,
L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K.
Deng, S. Fu, and S. Liu, “Codexglue: A machine learning benchmark
dataset for code understanding and generation,” 2021.
[56] C. et al., “Evaluating large language models trained on code,” 2021.
[57] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton, “Program synthesis
with large language models,” 2021.
[58] L. Phan, H. Tran, D. Le, H. Nguyen, J. Annibal, A. Peltekian, and
Y . Ye, “Cotext: Multi-task learning with code-text transformer,” 2021.
[Online]. Available: http://dx.doi.org/10.18653/v1/2021.nlp4prog-1.5
[59] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning
with a unified text-to-text transformer,” Journal of machine learning
research , vol. 21, no. 140, pp. 1–67, 2020.
[60] W. Qi, Y . Gong, Y . Yan, C. Xu, B. Yao, B. Zhou, B. Cheng,
D. Jiang, J. Chen, R. Zhang, H. Li, and N. Duan, “ProphetNet-X:
Large-scale pre-training models for English, Chinese, multi-lingual,
dialog, and code generation,” in Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing:
System Demonstrations , Online, Aug. 2021, pp. 232–239. [Online].
Available: https://aclanthology.org/2021.acl-demo.28
[61] B. Shen, J. Zhang, T. Chen, D. Zan, B. Geng, A. Fu, M. Zeng, A. Yu,
J. Ji, J. Zhao et al. , “Pangu-coder2: Boosting large language models for
code with ranking feedback,” arXiv preprint arXiv:2307.14936 , 2023.
[62] R. Li, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone,
C. Akiki, L. Jia, J. Chim, Q. Liu et al. , “Starcoder: may the source
be with you!” Transactions on Machine Learning Research .
[63] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,
and D. Jiang, “Wizardcoder: Empowering code large language models
with evol-instruct,” in The Twelfth International Conference on Learning
Representations .
[64] Y . Wang, K. He, D. Fu, Z. Gongque, H. Xu, Y . Chen, Z. Wang,
Y . Fu, G. Dong, M. Diao et al. , “How do your code llms perform?
empowering code instruction tuning with high-quality data,” arXiv
preprint arXiv:2409.03810 , 2024.
[65] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,
A. Mathur, A. Schelten, A. Yang, A. Fan et al. , “The llama 3 herd of
models,” arXiv preprint arXiv:2407.21783 , 2024.
[66] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and W. Chen,
“Codet: Code generation with generated tests,” 2022.
[67] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi,
A. Tang, D. Pykhtar, J. Liu, Y . Wei et al. , “Starcoder 2 and the stack
v2: The next generation,” arXiv preprint arXiv:2402.19173 , 2024.
[68] S. Tipirneni, M. Zhu, and C. K. Reddy, “Structcoder: Structure-aware
transformer for code generation,” 2023.
[69] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware
unified pre-trained encoder-decoder models for code understanding
and generation,” in Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , 2021, pp. 8696–8708.
[70] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of the 40th
annual meeting of the Association for Computational Linguistics , 2002,
pp. 311–318.
[71] S. V . Stehman, “Selecting and interpreting measures of thematic classi-
fication accuracy,” Remote sensing of Environment , vol. 62, no. 1, pp.
77–89, 1997.
[72] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan,
M. Zhou, A. Blanco, and S. Ma, “Codebleu: a method for automatic
evaluation of code synthesis,” CoRR , vol. abs/2009.10297, 2020.
[73] C. Liu, X. Bao, H. Zhang, N. Zhang, H. Hu, X. Zhang, and M. Yan,
“Improving chatgpt prompt for code generation,” 2023.

--- PAGE 13 ---
[74] Z. Yuan, J. Liu, Q. Zi, M. Liu, X. Peng, and Y . Lou, “Evaluating
instruction-tuned large language models on code comprehension and
generation,” 2023.
[75] S. Robertson, H. Zaragoza et al. , “The probabilistic relevance frame-
work: Bm25 and beyond,” Foundations and Trends® in Information
Retrieval , vol. 3, no. 4, pp. 333–389, 2009.
[76] B. Kitchenham and S. L. Pfleeger, “Principles of survey research: part 5:
populations and samples,” ACM SIGSOFT Software Engineering Notes ,
vol. 27, no. 5, pp. 17–20, 2002.
[77] G. Guest, A. Bunce, and L. Johnson, “How many interviews are enough?
an experiment with data saturation and variability,” Field methods ,
vol. 18, no. 1, pp. 59–82, 2006.
[78] M. B. Miles and A. M. Huberman, Qualitative data analysis: An
expanded sourcebook . sage, 1994.
[79] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,
I. Harper, A. Marginean, S. Sengupta, and E. Wang, “Automated unit
test improvement using large language models at meta,” 2024.
[80] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin,
and X. Hu, “Harnessing the power of llms in practice: A survey on
chatgpt and beyond,” ACM Transactions on Knowledge Discovery from
Data , 2023.
[81] Y . Chen, Q. Fu, Y . Yuan, Z. Wen, G. Fan, D. Liu, D. Zhang, Z. Li,
and Y . Xiao, “Hallucination detection: Robustly discerning reliable
answers in large language models,” in Proceedings of the 32nd ACM
International Conference on Information and Knowledge Management ,
2023, pp. 245–255.
[82] N. M. Guerreiro, D. M. Alves, J. Waldendorf, B. Haddow, A. Birch,
P. Colombo, and A. F. Martins, “Hallucinations in large multilingual
translation models,” Transactions of the Association for ComputationalLinguistics , vol. 11, pp. 1500–1517, 2023.
[83] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and
C. J. Cai, “Promptchainer: Chaining large language model prompts
through visual programming,” in CHI Conference on Human Factors
in Computing Systems Extended Abstracts , 2022, pp. 1–10.
[84] T. Wu, M. Terry, and C. J. Cai, “Ai chains: Transparent and controllable
human-ai interaction by chaining large language model prompts,” in
Proceedings of the 2022 CHI conference on human factors in computing
systems , 2022, pp. 1–22.
[85] D. Trautmann, “Large language model prompt chaining for long legal
document classification,” SwissText’23: The 8th edition of the Swiss
Text Analytics Conference – Generative AI & LLM, June 12–14, 2023,
Neuch ˆatel, Switzerland , 2023.
[86] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment generation
with hybrid lexical and syntactical information,” Empirical Software
Engineering , vol. 25, pp. 2179–2217, 2020.
[87] S. MacNeil, A. Tran, A. Hellas, J. Kim, S. Sarsa, P. Denny, S. Bernstein,
and J. Leinonen, “Experiences from using code explanations generated
by large language models in a web software development e-book,”
inProceedings of the 54th ACM Technical Symposium on Computer
Science Education V . 1 , 2023, pp. 931–937.
[88] E. A. AlOmar, A. Venkatakrishnan, M. W. Mkaouer, C. D. Newman, and
A. Ouni, “How to refactor this code? an exploratory study on developer-
chatgpt refactoring conversations,” 2024.
[89] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language
models in retrieval-augmented generation,” 2024.
[90] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Re-
flexion: Language agents with verbal reinforcement learning,” Advances
in Neural Information Processing Systems , vol. 36, 2024.
