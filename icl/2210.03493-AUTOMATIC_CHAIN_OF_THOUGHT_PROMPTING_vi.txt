# 2210.03493.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2210.03493.pdf
# Kích thước tệp: 834336 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
THÚC ĐẨY CHUỖI SUY NGHĨ TỰ ĐỘNG
TRONG CÁC MÔ HÌNH NGÔN NGỮ LỚN

Zhuosheng Zhangy, Aston Zhangz, Mu Liz, Alex Smolaz
yĐại học Giao thông Thượng Hải, zDịch vụ Web Amazon

TÓM TẮT
Các mô hình ngôn ngữ lớn (LLM) có thể thực hiện suy luận phức tạp bằng cách tạo ra các bước suy luận trung gian. Việc cung cấp các bước này cho các minh chứng thúc đẩy được gọi là thúc đẩy chuỗi suy nghĩ (CoT). Thúc đẩy CoT có hai mô hình chính. Một mô hình tận dụng một lời nhắc đơn giản như "Hãy suy nghĩ từng bước" để tạo điều kiện cho việc suy nghĩ từng bước trước khi trả lời câu hỏi. Mô hình khác sử dụng một số minh chứng thủ công từng cái một, mỗi minh chứng bao gồm một câu hỏi và một chuỗi suy luận dẫn đến câu trả lời. Hiệu suất vượt trội của mô hình thứ hai phụ thuộc vào việc tự tạo ra các minh chứng cụ thể cho từng nhiệm vụ một cách thủ công. Chúng tôi chỉ ra rằng những nỗ lực thủ công như vậy có thể được loại bỏ bằng cách tận dụng LLM với lời nhắc "Hãy suy nghĩ từng bước" để tạo ra các chuỗi suy luận cho các minh chứng từng cái một, tức là hãy suy nghĩ không chỉ từng bước mà còn từng cái một. Tuy nhiên, những chuỗi được tạo ra này thường có lỗi sai. Để giảm thiểu tác động của những lỗi sai đó, chúng tôi thấy rằng tính đa dạng rất quan trọng cho việc tự động xây dựng minh chứng. Chúng tôi đề xuất phương pháp thúc đẩy CoT tự động: Auto-CoT. Phương pháp này lấy mẫu các câu hỏi có tính đa dạng và tạo ra các chuỗi suy luận để xây dựng minh chứng. Trên mười nhiệm vụ suy luận chuẩn công khai với GPT-3, Auto-CoT luôn phù hợp hoặc vượt trội so với hiệu suất của mô hình CoT yêu cầu thiết kế thủ công các minh chứng. Mã nguồn có sẵn tại https://github.com/amazon-research/auto-cot

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) [Brown et al., 2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022] đã thực hiện một cách ấn tượng trên các nhiệm vụ suy luận phức tạp bằng cách phân tách các vấn đề nhiều bước thành các bước trung gian trước khi đưa ra câu trả lời. Quá trình suy luận này được kích hoạt bởi một kỹ thuật rất gần đây: thúc đẩy chuỗi suy nghĩ (CoT) [Wei et al., 2022a].

Thúc đẩy CoT có thể được phân loại thành hai mô hình chính. Một mô hình thêm một lời nhắc đơn lẻ như "Hãy suy nghĩ từng bước" sau câu hỏi kiểm tra để tạo điều kiện cho các chuỗi suy luận trong LLM [Kojima et al., 2022]. Vì mô hình thúc đẩy này bất khả tri về nhiệm vụ và không cần các minh chứng đầu vào-đầu ra, nó được gọi là Zero-Shot-CoT (bên trái của Hình 1). Với Zero-Shot-CoT, LLM đã chỉ ra là những người suy luận zero-shot khá tốt. Mô hình khác là thúc đẩy few-shot với các minh chứng suy luận thủ công từng cái một [Wei et al., 2022a]. Mỗi minh chứng có một câu hỏi và một chuỗi suy luận. Một chuỗi suy luận được tạo thành từ một lý lẽ (một chuỗi các bước suy luận trung gian) và một câu trả lời mong đợi. Với tất cả các minh chứng được thiết kế thủ công, mô hình này được gọi là Manual-CoT (bên phải của Hình 1).

Trong thực tế, Manual-CoT đã có được hiệu suất mạnh hơn so với Zero-Shot-CoT [Wei et al., 2022a, Kojima et al., 2022]. Tuy nhiên, hiệu suất vượt trội này phụ thuộc vào việc soạn thảo thủ công các minh chứng hiệu quả. Cụ thể, việc soạn thảo thủ công liên quan đến những nỗ lực không tầm thường trong việc thiết kế cả câu hỏi và chuỗi suy luận của chúng cho các minh chứng. Hơn nữa, nỗ lực con người cho việc thiết kế minh chứng cụ thể cho từng nhiệm vụ thậm chí còn nhiều hơn: các nhiệm vụ khác nhau, như số học [Roy and Roth, 2015] và suy luận thông thường [Talmor et al., 2019], đòi hỏi những cách minh chứng khác nhau.

Để loại bỏ những thiết kế thủ công như vậy, chúng tôi ủng hộ một mô hình Auto-CoT khác để tự động xây dựng minh chứng với các câu hỏi và chuỗi suy luận. Cụ thể, Auto-CoT tận dụng LLM với lời nhắc "Hãy suy nghĩ từng bước" để tạo ra các chuỗi suy luận cho các minh chứng từng cái một, tức là hãy suy nghĩ không chỉ từng bước mà còn từng cái một.

Công việc được thực hiện trong thời gian thực tập tại Amazon Web Services. Liên hệ với Zhuosheng Zhang <zhangzs@sjtu.edu.cn> và Aston Zhang <astonz@amazon.com>

--- TRANG 2 ---
A: Hãy suy nghĩ từng bước. Có 64 chó con. 28 trong số chúng đã được bán. Điều này để lại 36 chó con. Mỗi lồng có 4 chó con, vậy chúng ta cần 9 lồng. Vì vậy, câu trả lời (số Ả Rập) là

Câu hỏi kiểm tra
Lý lẽ được tạo ra
Minh chứng thủ công từng cái một

Q: Một cửa hàng thú cưng có 64 chó con. Trong một ngày họ bán 28 con và đặt phần còn lại vào các lồng với 4 con trong mỗi lồng. Họ đã sử dụng bao nhiêu lồng?

Q: Có 15 cây trong rừng nhỏ. Công nhân rừng sẽ trồng cây trong rừng nhỏ hôm nay. Sau khi họ hoàn thành, sẽ có 21 cây. Công nhân rừng đã trồng bao nhiêu cây hôm nay?

A: Có 15 cây ban đầu. Sau đó có 21 cây sau khi một số cây được trồng thêm. Vậy phải có 21-15=6. Câu trả lời là 6.

...

Tạo lý lẽ

9.

Trích xuất câu trả lời từ LLM

Cửa hàng thú cưng có 64 chó con. Họ bán 28 con. Vậy họ còn lại 64-28=36 chó con. Họ đặt chúng vào các lồng với 4 con trong mỗi lồng. Vậy họ đã sử dụng 36/4=9 lồng. Câu trả lời là 9.

Q: Một cửa hàng thú cưng có 64 chó con. Trong một ngày họ bán 28 con và đặt phần còn lại vào các lồng với 4 con trong mỗi lồng. Họ đã sử dụng bao nhiêu lồng?

A:

(a) Zero-Shot-CoT    (b) Manual-CoT

Câu hỏi
Câu trả lời
Lý lẽ

LLM
LLM

Q: Một cửa hàng thú cưng có 64 chó con. Trong một ngày họ bán 28 con và đặt phần còn lại vào các lồng với 4 con trong mỗi lồng. Họ đã sử dụng bao nhiêu lồng?

A: Hãy suy nghĩ từng bước.

Hình 1: Zero-Shot-CoT [Kojima et al., 2022] (sử dụng lời nhắc "Hãy suy nghĩ từng bước") và Manual-CoT [Wei et al., 2022a] (sử dụng các minh chứng được thiết kế thủ công từng cái một) với các đầu vào và đầu ra ví dụ của một LLM.

Tuy nhiên, chúng tôi thấy rằng thách thức này không thể được giải quyết hiệu quả bằng các giải pháp đơn giản. Ví dụ, cho một câu hỏi kiểm tra của một tập dữ liệu, việc truy xuất các câu hỏi tương tự về mặt ngữ nghĩa và gọi Zero-Shot-CoT để tạo ra các chuỗi suy luận sẽ thất bại. Mặc dù LLM là những người suy luận zero-shot khá tốt, nhưng chúng không hoàn hảo: Zero-Shot-CoT vẫn có thể mắc lỗi trong các chuỗi suy luận.

Để giảm thiểu tác động của các lỗi sai chuỗi suy luận từ Zero-Shot-CoT, phân tích của chúng tôi cho thấy rằng tính đa dạng của các câu hỏi minh chứng là chìa khóa. Dựa trên hiểu biết này, chúng tôi đề xuất một phương pháp Auto-CoT để tự động xây dựng minh chứng. Auto-CoT bao gồm hai bước chính. Đầu tiên, phân chia các câu hỏi của một tập dữ liệu đã cho thành một vài cụm. Thứ hai, chọn một câu hỏi đại diện từ mỗi cụm và tạo ra chuỗi suy luận của nó bằng cách sử dụng Zero-Shot-CoT với các phương pháp heuristic đơn giản.

Chúng tôi đánh giá Auto-CoT trên mười nhiệm vụ suy luận chuẩn bao gồm: (i) suy luận số học (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021], AQUA-RAT [Ling et al., 2017], SVAMP [Patel et al., 2021]); (ii) suy luận thông thường (CSQA [Talmor et al., 2019], StrategyQA [Geva et al., 2021]); (iii) suy luận ký hiệu (Last Letter Concatenation, Coin Flip) [Wei et al., 2022a]. Kết quả thực nghiệm cho thấy rằng với GPT-3, Auto-CoT luôn phù hợp hoặc vượt trội so với hiệu suất của Manual-CoT yêu cầu thiết kế thủ công. Điều này chỉ ra rằng LLM có thể thực hiện suy luận CoT bằng cách tự động xây dựng minh chứng.

2 Nghiên cứu liên quan
Phần này xem xét hai hướng nghiên cứu tạo thành cơ sở cho công việc này: thúc đẩy chuỗi suy nghĩ (CoT) cho suy luận nhiều bước và học trong ngữ cảnh để thúc đẩy LLM học từ các minh chứng.

2.1 Thúc đẩy chuỗi suy nghĩ
Thúc đẩy CoT là một kỹ thuật không có gradient để thúc đẩy LLM tạo ra các bước suy luận trung gian dẫn đến câu trả lời cuối cùng. Wei et al. [2022a] đã nghiên cứu chính thức chủ đề thúc đẩy CoT trong các mô hình ngôn ngữ. Kỹ thuật này kích hoạt LLM để tạo ra một chuỗi các bước suy luận trung gian mạch lạc dẫn đến câu trả lời cuối cùng cho một câu hỏi. Các nghiên cứu đã chỉ ra rằng LLM có thể thực hiện suy luận CoT với thúc đẩy zero-shot (Zero-Shot-CoT) [Kojima et al., 2022] hoặc các minh chứng few-shot được viết thủ công (Manual-CoT) [Wei et al., 2022a].

Zero-Shot-CoT. Kojima et al. [2022] đã chỉ ra rằng LLM là những người suy luận zero-shot khá tốt mà các lý lẽ được tạo ra đã phản ánh suy luận CoT. Phát hiện này truyền cảm hứng cho công việc của chúng tôi để tận dụng các lý lẽ tự tạo cho các minh chứng. Việc tạo ra các lý lẽ bởi LLM đã được chỉ ra là thực tế trong một công việc gần đây [Zelikman et al., 2022]. Trong

--- TRANG 3 ---
công việc của họ, một LLM được thúc đẩy để tạo ra các lý lẽ và những lý lẽ dẫn đến câu trả lời đúng được chọn. Việc lựa chọn đòi hỏi một tập dữ liệu huấn luyện với các câu hỏi có câu trả lời được chú thích. Ngược lại, công việc của chúng tôi xem xét một tình huống thách thức hơn khi chỉ có một tập các câu hỏi kiểm tra được đưa ra (không có tập dữ liệu huấn luyện), theo các nghiên cứu thúc đẩy CoT của Wei et al. [2022a] và Kojima et al. [2022].

Manual-CoT. Manual-CoT đạt được hiệu suất mạnh hơn bằng cách kích hoạt khả năng suy luận CoT với các minh chứng thủ công hiệu quả. Các minh chứng cho quá trình suy luận được thiết kế thủ công. Tuy nhiên, nỗ lực con người trong việc thiết kế cả câu hỏi và chuỗi suy luận của chúng là không tầm thường. Thay vì giải quyết hạn chế này, các nghiên cứu gần đây chủ yếu tập trung vào việc tạo ra các minh chứng phức tạp hơn bằng tay hoặc tận dụng các phương pháp giống như ensemble. Một xu hướng là phân tách vấn đề. Trong thúc đẩy từ ít nhất đến nhiều nhất [Zhou et al., 2022], các vấn đề phức tạp được giảm xuống thành các vấn đề phụ, và sau đó các vấn đề phụ được giải quyết tuần tự. Xu hướng khác là bỏ phiếu trên nhiều đường suy luận cho một câu hỏi kiểm tra. Wang et al. [2022a] đã giới thiệu một chiến lược giải mã tự nhất quán để lấy mẫu nhiều đầu ra của LLM và sau đó lấy đa số trên các câu trả lời cuối cùng. Wang et al. [2022b] và Li et al. [2022] đã giới thiệu tính ngẫu nhiên trong không gian đầu vào để tạo ra các đầu ra đa dạng hơn cho việc bỏ phiếu. Họ sử dụng các minh chứng được thiết kế thủ công làm tập hạt giống và tạo ra các lý lẽ bổ sung: bỏ một câu hỏi từ tập hạt giống và sử dụng các minh chứng còn lại để tạo ra các lý lẽ cho câu hỏi này bằng LLM. Không giống như các hướng nghiên cứu nêu trên dựa vào các minh chứng được thiết kế thủ công, công việc của chúng tôi dự định loại bỏ các thiết kế thủ công với hiệu suất cạnh tranh.

2.2 Học trong ngữ cảnh
Thúc đẩy CoT liên quan chặt chẽ đến học trong ngữ cảnh (ICL) [Radford et al., 2019, Brown et al., 2020]. ICL cho phép LLM thực hiện một nhiệm vụ đích bằng cách đưa vào một vài ví dụ được thúc đẩy như một phần của đầu vào. Không có cập nhật gradient, ICL cho phép một mô hình duy nhất thực hiện các nhiệm vụ khác nhau một cách phổ quát. Có nhiều hướng nghiên cứu khác nhau để cải thiện hiệu suất của ICL: (i) truy xuất các minh chứng liên quan đến trường hợp kiểm tra nơi thực hành phổ biến là động tức truy xuất các ví dụ huấn luyện liên quan cho một đầu vào kiểm tra đã cho [Rubin et al., 2022, Su et al., 2022]; (ii) tăng cường với thông tin chi tiết, chẳng hạn như kết hợp hướng dẫn nhiệm vụ [Mishra et al., 2022, Wei et al., 2022b, Sanh et al., 2022]; (iii) thao tác xác suất đầu ra của LLM thay vì tính toán trực tiếp khả năng của các nhãn đích [Holtzman et al., 2021, Zhao et al., 2021, Min et al., 2022a].

Mặc dù ICL thành công, các nghiên cứu [Liu et al., 2022a, Lu et al., 2022] đã chỉ ra rằng sức mạnh của ICL có thể thay đổi rộng rãi tùy thuộc vào việc lựa chọn các minh chứng trong ngữ cảnh [Liu et al., 2022b]. Chi tiết, việc định dạng lời nhắc, chẳng hạn như từ ngữ hoặc thứ tự của các minh chứng, có thể dẫn đến biến động hiệu suất [Webson and Pavlick, 2022, Zhao et al., 2021]. Một công việc gần đây [Min et al., 2022b] thậm chí đã đặt câu hỏi về sự cần thiết của ánh xạ đầu vào-đầu ra cơ bản: việc sử dụng các nhãn không chính xác trong các ví dụ chỉ làm giảm hiệu suất một cách nhỏ. Tuy nhiên, phân tích hiện tại về ICL chủ yếu dựa trên các tập dữ liệu phân loại tiêu chuẩn và đa lựa chọn chỉ có ánh xạ <đầu vào→đầu ra> đơn giản. Chúng tôi khám phá rằng những phát hiện đó có thể không áp dụng cho tình huống thúc đẩy CoT với các ánh xạ <đầu vào→lý lẽ→đầu ra> phức tạp hơn. Ví dụ, lỗi sai trong ánh xạ <đầu vào→lý lẽ> hoặc ánh xạ <lý lẽ→đầu ra> sẽ dẫn đến sự sụt giảm hiệu suất đáng kể (Phụ lục A.1).

3 Thách thức của Auto-CoT
Như vừa thảo luận, hiệu suất của ICL phụ thuộc vào các minh chứng được tạo ra bằng tay. Như được báo cáo trong Manual-CoT [Wei et al., 2022a], việc sử dụng các minh chứng được viết bởi các người chú thích khác nhau mang lại độ chênh lệch độ chính xác lên đến 28.2% trong một nhiệm vụ suy luận ký hiệu, trong khi việc thay đổi thứ tự của các minh chứng dẫn đến ít hơn 2% thay đổi trong hầu hết các nhiệm vụ. Điều này cho thấy rằng thách thức chính của Auto-CoT nằm ở việc tự động xây dựng minh chứng với các câu hỏi tốt và chuỗi suy luận của chúng.

Nhớ lại rằng Manual-CoT tạo ra bằng tay một vài (ví dụ, 8) câu hỏi trong các minh chứng. Với các phương pháp truy xuất dựa trên độ tương tự được áp dụng rộng rãi để thúc đẩy LLM [Rubin et al., 2022, Su et al., 2022], một giải pháp ứng cử viên đầy hứa hẹn là lấy mẫu các câu hỏi minh chứng bằng cách sử dụng truy xuất dựa trên độ tương tự. Chúng tôi theo giả định thách thức hơn trong các nghiên cứu CoT [Wei et al., 2022a, Kojima et al., 2022] rằng chỉ có một tập các câu hỏi kiểm tra được đưa ra (không có tập dữ liệu huấn luyện). Theo Liu et al. [2022a], chúng tôi sử dụng Sentence-BERT [Reimers and Gurevych, 2019] để mã hóa câu hỏi. Đối với mỗi câu hỏi qtest trong một tập dữ liệu kiểm tra, chúng tôi lấy mẫu các câu hỏi minh chứng qdemo_i (i = 1, ..., k) từ phần còn lại của các câu hỏi. Chúng tôi thiết kế một phương pháp Retrieval-Q-CoT để truy xuất top-k (ví dụ, k = 8) câu hỏi tương tự dựa trên độ tương tự cosine.

Để so sánh với phương pháp dựa trên độ tương tự này, chúng tôi cũng kiểm tra một phương pháp dựa trên tính đa dạng tương đối hơn: Random-Q-CoT, nó lấy mẫu ngẫu nhiên k câu hỏi kiểm tra khác cho mỗi câu hỏi kiểm tra.

Cả Retrieval-Q-CoT và Random-Q-CoT đều gọi Zero-Shot-CoT [Kojima et al., 2022] để tạo ra chuỗi suy luận cdemo_i (lý lẽ và câu trả lời) cho mỗi câu hỏi được lấy mẫu qdemo_i, vì LLM là những người suy luận zero-shot khá tốt [Kojima et al., 2022]. Chúng tôi sử dụng GPT-3 [Brown et al., 2020] với 175B tham số (text-davinci-002) cho LLM trừ khi có quy định khác.

--- TRANG 4 ---
Ở mức cao, cả Retrieval-Q-CoT và Random-Q-CoT đều lấy phép nối của các cặp qdemo_i, cdemo_i (i = 1, ..., k) và qtest làm đầu vào để dự đoán chuỗi suy luận cho qtest, chứa câu trả lời ở cuối (như bên phải của Hình 1).

Bảng 1: Độ chính xác (%) của các phương pháp lấy mẫu khác nhau. Ký hiệu y chỉ ra việc sử dụng các tập huấn luyện với các chuỗi suy luận được chú thích.

Phương pháp | MultiArith | GSM8K | AQuA
Zero-Shot-CoT | 78.7 | 40.7 | 33.5
Manual-CoT | 91.7 | 46.9 | 35.8y
Random-Q-CoT | 86.2 | 47.6y | 36.2y
Retrieval-Q-CoT | 82.8 | 48.0y | 39.7y

Đáng ngạc nhiên, Retrieval-Q-CoT hoạt động kém hơn Random-Q-CoT trên tập dữ liệu số học MultiArith [Roy and Roth, 2015] (Bảng 1). Lưu ý rằng các phương pháp truy xuất ban đầu được đề xuất trong các nhiệm vụ với các nhãn được chú thích [Rubin et al., 2022, Su et al., 2022], tuy nhiên, việc gọi Zero-Shot-CoT không đảm bảo các chuỗi suy luận hoàn toàn chính xác. Vì vậy, chúng tôi giả thuyết rằng hiệu suất kém của Retrieval-Q-CoT được gây ra bởi các chuỗi suy luận không chính xác từ Zero-Shot-CoT. Để kiểm tra giả thuyết này, chúng tôi thử nghiệm với Retrieval-Q-CoT trên hai tập dữ liệu khác GSM8K [Cobbe et al., 2021] và AQuA [Ling et al., 2017] có các tập huấn luyện với các chuỗi suy luận được chú thích. Kết quả được hiển thị với y trong Bảng 1. Dưới thiết lập với các chuỗi suy luận được chú thích, Retrieval-Q-CoT thậm chí vượt trội hơn Manual-CoT. Kết quả chỉ ra rằng Retrieval-Q-CoT hiệu quả khi có chú thích của con người.

Mặc dù chú thích của con người hữu ích, nhưng những nỗ lực thủ công như vậy là không tầm thường. Tuy nhiên, việc tự động tạo ra các chuỗi suy luận qua Zero-Shot-CoT hoạt động kém hơn Manual-CoT, đặc biệt khi thách thức của việc lấy mẫu câu hỏi không được giải quyết. Để thiết kế Auto-CoT hiệu quả hơn, chúng ta cần hiểu rõ hơn về thách thức của nó.

3.1 Retrieval-Q-CoT thất bại do bị sai lệch bởi độ tương tự
Vì Retrieval-Q-CoT sử dụng một vài minh chứng thúc đẩy giống như trong Manual-CoT, Retrieval-Q-CoT được mong đợi hoạt động cạnh tranh tốt. Tuy nhiên, các chuỗi suy luận (cả lý lẽ và câu trả lời) trong Retrieval-Q-CoT được tạo ra bởi Zero-Shot-CoT: chúng có thể có lỗi sai dẫn đến câu trả lời sai. Chúng ta hãy đơn giản gọi các minh chứng có câu trả lời sai là minh chứng sai. Trực quan, sau khi các câu hỏi tương tự với một câu hỏi kiểm tra được truy xuất, các minh chứng sai do Zero-Shot-CoT gây ra có thể sai lệch cùng LLM để suy luận tương tự với câu trả lời sai (ví dụ, sao chép lỗi sai) cho câu hỏi kiểm tra. Chúng tôi gọi hiện tượng này là sai lệch bởi độ tương tự. Chúng tôi sẽ điều tra xem sai lệch bởi độ tương tự có góp phần vào hiệu suất kém của Retrieval-Q-CoT hay không.

Retrieval-Q-CoT Random-Q-CoT
20 30 40 50
Tỷ lệ (%)

Hình 2: Tỷ lệ không giải quyết được.

Để bắt đầu, chúng tôi gọi Zero-Shot-CoT trên tất cả 600 câu hỏi từ tập dữ liệu MultiArith. Trong số đó, chúng tôi thu thập 128 câu hỏi (ký hiệu là Q) nơi Zero-Shot-CoT tạo ra câu trả lời sai (tỷ lệ lỗi: 21.3% = 128/600). Như chúng tôi đã đề cập, với các minh chứng bổ sung, Retrieval-Q-CoT và Random-Q-CoT được mong đợi hoạt động cạnh tranh hơn so với Zero-Shot-CoT. Trong số Q nơi Zero-Shot-CoT thất bại, chúng tôi gọi những câu hỏi mà Retrieval-Q-CoT hoặc Random-Q-CoT vẫn thất bại là các câu hỏi không giải quyết được của chúng. Chúng tôi chia số câu hỏi không giải quyết được cho 128 (số câu hỏi trong Q) để tính tỷ lệ không giải quyết được. Tỷ lệ không giải quyết được cao hơn có nghĩa là một phương pháp có nhiều khả năng vẫn mắc lỗi sai giống như Zero-Shot-CoT.

Hình 2 cho thấy rằng tỷ lệ không giải quyết được của Retrieval-Q-CoT (46.9%) cao hơn nhiều so với Random-Q-CoT (25.8%). Điều này chỉ ra rằng với các câu hỏi tương tự được lấy mẫu cho các câu hỏi kiểm tra, Retrieval-Q-CoT bị ảnh hưởng tiêu cực bởi sai lệch bởi độ tương tự.

Để chỉ ra rằng các câu hỏi không giải quyết được của Retrieval-Q-CoT có xu hướng tương tự, chúng tôi trình bày một nghiên cứu trường hợp trong Bảng 2. Ở phần bên trái, các câu hỏi minh chứng được truy xuất tương tự với câu hỏi kiểm tra và hỏi "mất bao lâu để anh ta nấu phần còn lại?". Các chuỗi suy luận được tạo ra bởi Zero-Shot-CoT đưa ra các câu trả lời về "tổng số" thay vì "phần còn lại". Theo các minh chứng, Retrieval-Q-CoT cũng thất bại bằng cách hiểu sai ý nghĩa của "phần còn lại". Ngược lại, Random-Q-CoT hiểu "phần còn lại" tốt hơn mà không mắc lỗi tương tự trong các minh chứng, nhờ vào các minh chứng tương đối đa dạng hơn (ngẫu nhiên).

3.2 Lỗi sai thường rơi vào cùng một cụm
Được thúc đẩy bởi các quan sát trong Bảng 2, chúng tôi sử dụng k-means để phân chia tất cả 600 câu hỏi kiểm tra thành k = 8 cụm, nơi mỗi cụm chứa các câu hỏi tương tự. Với các cụm này và các chuỗi suy luận được tạo ra bởi Zero-Shot-CoT

2 Chúng tôi sử dụng Sentence-BERT [Reimers and Gurevych, 2019] để mã hóa câu hỏi và áp dụng k-means để phân cụm.

--- TRANG 5 ---
Bảng 2: Ví dụ về Retrieval-Q-CoT và Random-Q-CoT để xây dựng minh chứng và dự đoán. Phần trên chứa các minh chứng được xây dựng tự động, và phần dưới chứa các câu hỏi kiểm tra cùng với các chuỗi suy luận được dự đoán.

Retrieval-Q-CoT | Random-Q-CoT
Q: Một đầu bếp cần nấu 9 củ khoai tây. Anh ta đã nấu 7 củ. Nếu mỗi củ khoai tây mất 3 phút để nấu, mất bao lâu để anh ta nấu phần còn lại? | Q: Wendy đã tải lên 79 hình ảnh lên Facebook. Cô ấy đã đặt 44 hình vào một album và đặt phần còn lại vào 5 album khác nhau. Có bao nhiêu hình ảnh trong mỗi album?

A: Hãy suy nghĩ từng bước. Đầu bếp đã nấu 7 củ khoai tây. Điều đó có nghĩa là anh ta đã mất 7 * 3 phút để nấu 7 củ khoai tây đó. Điều đó có nghĩa là anh ta sẽ mất 3 phút nữa để nấu mỗi củ khoai tây còn lại. Điều đó có nghĩa là anh ta sẽ mất tổng cộng 7 * 3 + 2 * 3 = 21 phút để nấu tất cả 9 củ khoai tây. Câu trả lời là 21. 7 | A: Hãy suy nghĩ từng bước. Wendy đã tải lên 79 hình ảnh lên Facebook. Cô ấy đã đặt 44 hình vào một album. Điều đó có nghĩa là cô ấy đã đặt phần còn lại vào 5 album khác nhau. Điều đó có nghĩa là mỗi album sẽ có (79-44)/5 = 15 hình trong mỗi album. Câu trả lời là 15. X

Q: Một đầu bếp cần nấu 13 củ khoai tây. Anh ta đã nấu 5 củ. Nếu mỗi củ khoai tây mất 6 phút để nấu, mất bao lâu để anh ta nấu phần còn lại? | Q: Tại một nhà hàng, mỗi bữa ăn của người lớn có giá 5 đô la và trẻ em ăn miễn phí. Nếu một nhóm 15 người đến và 8 người là trẻ em, nhóm này sẽ phải trả bao nhiêu tiền để ăn?

A: Hãy suy nghĩ từng bước. Đầu bếp đã nấu 5 củ khoai tây. Điều đó có nghĩa là anh ta đã mất 30 phút cho đến nay. Có 13 củ khoai tây tổng cộng. 5 củ đã được nấu. Điều đó có nghĩa là còn 8 củ để nấu. Mỗi củ khoai tây mất 6 phút để nấu. Điều đó có nghĩa là đầu bếp sẽ mất 48 phút để nấu 8 củ khoai tây còn lại. Tổng cộng, đầu bếp sẽ mất 78 phút để nấu tất cả 13 củ khoai tây. Câu trả lời là 78. 7 | A: Hãy suy nghĩ từng bước. Có 8 trẻ em, vậy có nghĩa là có 7 người lớn. Mỗi bữa ăn của người lớn có giá 5 đô la, vậy có nghĩa là 7 người lớn sẽ có giá 5*7=35 đô la. Bây giờ chúng ta chỉ cần cộng hai số lại với nhau. 35+8=43. Vậy nhóm này sẽ phải trả 43 đô la để ăn. Câu trả lời là 43. 7

Q: Một đầu bếp cần nấu 15 củ khoai tây. Anh ta đã nấu 8 củ. Nếu mỗi củ khoai tây mất 9 phút để nấu, mất bao lâu để anh ta nấu phần còn lại? | Q: Một đầu bếp cần nấu 15 củ khoai tây. Anh ta đã nấu 8 củ. Nếu mỗi củ khoai tây mất 9 phút để nấu, mất bao lâu để anh ta nấu phần còn lại?

A: Hãy suy nghĩ từng bước. Đầu bếp đã nấu 8 củ khoai tây. Điều đó có nghĩa là anh ta đã mất 8 * 9 phút để nấu 8 củ khoai tây đó. Điều đó có nghĩa là anh ta sẽ mất 9 phút để nấu mỗi củ khoai tây còn lại. Điều đó có nghĩa là anh ta sẽ mất tổng cộng 8 * 9 + 7 * 9 = 135 phút để nấu tất cả 15 củ khoai tây. Câu trả lời là 135. 7 | A: Hãy suy nghĩ từng bước. Đầu bếp cần nấu 15 củ khoai tây. Anh ta đã nấu 8 củ. Điều đó có nghĩa là anh ta phải nấu 15-8=7 củ khoai tây nữa. Mỗi củ khoai tây mất 9 phút để nấu. Điều đó có nghĩa là anh ta sẽ mất 9*7=63 phút để nấu phần còn lại của khoai tây. Câu trả lời là 63. X

(trong Phần 3.1), bây giờ chúng tôi tò mò liệu các cụm nhất định có chứa các câu hỏi mà Zero-Shot-CoT thường xuyên thất bại hay không. Vì vậy, chúng tôi tính toán tỷ lệ lỗi (câu hỏi với câu trả lời Zero-Shot-CoT sai / tổng số câu hỏi) cho mỗi cụm.

1 2 3 4 5 6 7 8
0 20 40 60
Tỷ lệ lỗi (%)

Hình 3: Các cụm câu hỏi tương tự.

Như được hiển thị trong Hình 3, tồn tại một cụm (Cụm 2) với các lỗi Zero-Shot-CoT thường xuyên (52.3%). Hiện tượng này có thể chung chung vì Zero-Shot-CoT có thể thiếu một số kỹ năng để giải quyết một số vấn đề phổ biến trong các nhiệm vụ đích. Để tiện mô tả, chúng ta hãy gọi cụm có tỷ lệ lỗi cao nhất là cụm lỗi thường xuyên (ví dụ, Cụm 2 trong Hình 3). Vì vậy, bản chất không hoàn hảo của các chuỗi suy luận được tạo ra theo cách zero-shot đặt ra rủi ro truy xuất nhiều câu hỏi tương tự bên trong cụm lỗi thường xuyên bằng cách sử dụng các phương pháp dựa trên độ tương tự. Đối với câu hỏi kiểm tra trong cụm lỗi thường xuyên, Retrieval-Q-CoT dễ dàng xây dựng minh chứng với nhiều lỗi sai tương tự. Kết quả là, Retrieval-Q-CoT thường mắc lỗi tương tự như Zero-Shot-CoT, được nhắc lại bằng tỷ lệ không giải quyết được cao hơn trong Hình 2.

3.3 Tính đa dạng có thể giảm thiểu sai lệch bởi độ tương tự
Phân tích cho đến nay một cách thuyết phục cho thấy rằng LLM vẫn không phải là những người suy luận zero-shot hoàn hảo; vì vậy, chúng tôi nhằm giảm thiểu tác động của các lỗi Zero-Shot-CoT của chúng, đặc biệt là để giảm thiểu sai lệch bởi độ tương tự trong thiết kế Auto-CoT.

Như chúng tôi sẽ chỉ ra sau (Phần 5.5), việc trình bày một phần nhỏ lỗi sai (ví dụ, 1 hoặc 2 minh chứng sai trong số 8) sẽ không làm hại hiệu suất suy luận tổng thể cho các câu hỏi kiểm tra. Giả sử rằng các câu hỏi của tất cả các minh chứng sai rơi vào cùng cụm lỗi thường xuyên; sau đó việc lấy mẫu một câu hỏi từ mỗi cụm khác nhau sẽ dẫn đến cơ hội cao hơn 7/8 = 87.5% để xây dựng tất cả 8 minh chứng chính xác. Vì các cụm khác nhau phản ánh ngữ nghĩa đa dạng của các câu hỏi, phương pháp lấy mẫu dựa trên phân cụm này có thể được coi là dựa trên tính đa dạng, tương phản mạnh mẽ với Retrieval-Q-CoT dựa trên độ tương tự. Một mặt, việc lấy mẫu câu hỏi với tính đa dạng có thể giảm thiểu tác động của sai lệch bởi độ tương tự (Phần 3.1). Mặt khác, nếu chúng ta coi mỗi minh chứng như một loại kỹ năng, các minh chứng đa dạng dường như bao phủ nhiều kỹ năng thay thế hơn để giải quyết các câu hỏi đích: ngay cả khi vẫn tồn tại một phần nhỏ (ví dụ, 1/8) lỗi sai trong các minh chứng, hiệu suất sẽ không bị ảnh hưởng tiêu cực (sẽ được hiển thị trong Hình 6).

Tuy nhiên, phương pháp lấy mẫu dựa trên phân cụm vẫn có thể xây dựng một phần nhỏ minh chứng sai, chẳng hạn như từ các câu hỏi trong cụm lỗi thường xuyên. Như chúng tôi sẽ chỉ ra sau, một số minh chứng sai này có thể được loại bỏ bằng phương pháp heuristic. Ví dụ, các minh chứng sai thường đi kèm với các câu hỏi dài và lý lẽ dài. Việc sử dụng các phương pháp heuristic đơn giản và chung chung, chẳng hạn như chỉ xem xét các câu hỏi ngắn hơn với lý lẽ ngắn hơn, tiếp tục giúp giảm thiểu tác động của khả năng Zero-Shot-CoT không hoàn hảo (Phụ lục C.2).

4 Auto-CoT: Thúc đẩy chuỗi suy nghĩ tự động
Dựa trên các quan sát và cân nhắc trong Phần 3, chúng tôi đề xuất phương pháp Auto-CoT để xây dựng minh chứng với các câu hỏi và chuỗi suy luận tự động. Auto-CoT bao gồm hai giai đoạn chính: (i) phân cụm câu hỏi: phân chia các câu hỏi của một tập dữ liệu đã cho thành một vài cụm; (ii) lấy mẫu minh chứng: chọn một câu hỏi đại diện từ mỗi cụm và tạo ra chuỗi suy luận của nó bằng cách sử dụng Zero-Shot-CoT với các phương pháp heuristic đơn giản. Quy trình tổng thể được minh họa trong Hình 4.

[Hình 4: Tổng quan về phương pháp Auto-CoT]

4.1 Phân cụm câu hỏi
Vì phân cụm dựa trên tính đa dạng có thể giảm thiểu sai lệch bởi độ tương tự (Phần 3.3), chúng tôi thực hiện phân tích cụm cho một tập câu hỏi Q đã cho. Đầu tiên, chúng tôi tính toán biểu diễn vector cho mỗi câu hỏi trong Q bằng Sentence-BERT [Reimers and Gurevych, 2019]. Các vector ngữ cảnh được tính trung bình để tạo thành biểu diễn câu hỏi có kích thước cố định. Sau đó, các biểu diễn câu hỏi được xử lý bằng thuật toán phân cụm k-means để tạo ra k cụm câu hỏi. Đối với các câu hỏi trong mỗi cụm i, sắp xếp chúng thành danh sách q(i) = [q(i)_1, q(i)_2, ...] theo thứ tự tăng dần của khoảng cách đến tâm cụm i. Giai đoạn phân cụm câu hỏi này được tóm tắt trong Thuật toán 1.

4.2 Lấy mẫu minh chứng
Trong giai đoạn thứ hai, chúng ta cần tạo ra các chuỗi suy luận cho những câu hỏi được lấy mẫu đó và sau đó lấy mẫu các minh chứng thỏa mãn tiêu chí lựa chọn của chúng ta.

--- TRANG 6 ---
Cụ thể hơn, chúng tôi xây dựng một minh chứng d(i) (phép nối của một câu hỏi, một lý lẽ và một câu trả lời) cho mỗi cụm i (i = 1, ..., k). Đối với cụm i, chúng tôi lặp qua các câu hỏi trong danh sách được sắp xếp q(i) = [q(i)_1, q(i)_2, ...] (thu được từ Thuật toán 1) cho đến khi thỏa mãn tiêu chí lựa chọn của chúng tôi. Nói cách khác, một câu hỏi gần với tâm cụm i hơn được xem xét trước. Giả sử câu hỏi gần thứ j q(i)_j đang được xem xét. Một đầu vào được thúc đẩy được hình thành như: [Q:q(i)_j. A:[P]], trong đó [P] là một lời nhắc đơn "Hãy suy nghĩ từng bước". Đầu vào được hình thành này được đưa vào LLM sử dụng Zero-Shot-CoT [Kojima et al., 2022] để xuất ra chuỗi suy luận bao gồm lý lẽ r(i)_j và câu trả lời được trích xuất a(i)_j. Sau đó, một minh chứng ứng cử viên d(i)_j cho cụm thứ i được xây dựng bằng cách nối câu hỏi, lý lẽ và câu trả lời: [Q:q(i)_j;A:r(i)_j a(i)_j].

Tương tự như tiêu chí của các minh chứng tạo bằng tay trong Wei et al. [2022a], tiêu chí lựa chọn của chúng tôi tuân theo các phương pháp heuristic đơn giản để khuyến khích lấy mẫu các câu hỏi và lý lẽ đơn giản hơn: đặt minh chứng được chọn d(i) như d(i)_j nếu nó có câu hỏi q(i)_j với không quá 60 token và lý lẽ r(i)_j với không quá 5 bước suy luận.

Thuật toán 1 Phân cụm
Yêu cầu: Một tập câu hỏi Q và số minh chứng k
Đảm bảo: Các câu hỏi được sắp xếp q(i) = [q(i)_1, q(i)_2, ...] cho mỗi cụm i (i = 1, ..., k)
1: thủ tục CLUSTER(Q, k)
2: foreach câu hỏi q trong Q do
3: Mã hóa q bằng Sentence-BERT
4: Phân cụm tất cả các biểu diễn câu hỏi được mã hóa thành k cụm
5: foreach cụm i = 1, ..., k do
6: Sắp xếp các câu hỏi q(i) = [q(i)_1, q(i)_2, ...] theo thứ tự tăng dần của khoảng cách đến tâm cụm
7: return q(i) (i = 1, ..., k)

Thuật toán 2 Xây dựng
Yêu cầu: Các câu hỏi được sắp xếp q(i) = [q(i)_1, q(i)_2, ...] cho mỗi cụm i (i = 1, ..., k), danh sách minh chứng trống d
Đảm bảo: Danh sách minh chứng d = [d(1), ..., d(k)]
1: thủ tục CONSTRUCT(q(i), ..., q(k))
2: foreach cụm i = 1, ..., k do
3: foreach câu hỏi q(i)_j trong q(i) do
4: Tạo ra lý lẽ r(i)_j và câu trả lời a(i)_j cho q(i)_j bằng Zero-Shot-CoT
5: if q(i)_j, r(i)_j thỏa mãn tiêu chí lựa chọn then
6: Thêm d(i) = [Q:q(i)_j;A:r(i)_j a(i)_j] vào d
7: break
8: return d

Như được tóm tắt trong Thuật toán 2, sau khi lấy mẫu minh chứng cho tất cả k cụm, sẽ có k minh chứng được xây dựng [d(1), ..., d(k)]. Các minh chứng được xây dựng được sử dụng để tăng cường một câu hỏi kiểm tra qtest cho học trong ngữ cảnh. Cụ thể, đầu vào là phép nối của tất cả các minh chứng [d(1), ..., d(k)] theo sau bởi [Q: qtest. A: [P]]. Đầu vào này được đưa vào LLM để có được chuỗi suy luận với câu trả lời ở cuối cho qtest (bên phải của Hình 4).

5 Thí nghiệm
Chúng tôi mô tả ngắn gọn thiết lập thí nghiệm và trình bày kết quả thí nghiệm chính. Có thể tìm thấy chi tiết và kết quả thí nghiệm bổ sung trong các phụ lục.

5.1 Thiết lập thí nghiệm
Nhiệm vụ và tập dữ liệu. Phương pháp của chúng tôi được đánh giá trên mười tập dữ liệu chuẩn từ ba loại nhiệm vụ suy luận: (i) suy luận số học (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021], AddSub [Hosseini et al., 2014], AQUA-RAT [Ling et al., 2017], SingleEq [Koncel-Kedziorski et al., 2015], SVAMP [Patel et al., 2021]); (ii) suy luận thông thường (CSQA [Talmor et al., 2019], StrategyQA [Geva et al., 2021]); (iii) suy luận ký hiệu (Last Letter Concatenation, Coin Flip) [Wei et al., 2022a].

Triển khai. Chúng tôi sử dụng GPT-3 công khai [Brown et al., 2020] phiên bản text-davinci-002 với 175B tham số cho LLM [Ouyang et al., 2022] trừ khi có quy định khác. Chúng tôi chọn LLM này vì nó có hiệu suất suy luận CoT mạnh nhất trong số các LLM công khai, như được báo cáo trong Kojima et al. [2022] và Wei et al. [2022a]. Chúng tôi cũng đánh giá mô hình Codex [Chen et al., 2021] (code-davinci-002) làm LLM. Theo Wei et al. [2022a], số minh chứng k là 8 trừ AQuA và Letter (4), CSQA (7), và StrategyQA (6).

4 Bởi vì Zero-Shot-CoT thường sử dụng "nn" để tách các bước suy luận, quy tắc có thể được triển khai dễ dàng bằng cách đếm các token "nn" trong các lý lẽ được tạo ra.

--- TRANG 7 ---
Bảng 3: Độ chính xác trên mười tập dữ liệu từ ba loại nhiệm vụ suy luận.

Mô hình | Số học | Thông thường | Ký hiệu
--- | --- | --- | ---
 | MultiArith | GSM8K | AddSub | AQuA | SingleEq | SVAMP | CSQA | Strategy | Letter | Coin
Zero-Shot | 22.7 | 12.5 | 77.0 | 22.4 | 78.7 | 58.8 | 72.6 | 54.3 | 0.2 | 53.8
Zero-Shot-CoT | 78.7 | 40.7 | 74.7 | 33.5 | 78.7 | 63.7 | 64.6 | 54.8 | 57.6 | 91.4
Few-Shot | 33.8 | 15.6 | 83.3 | 24.8 | 82.7 | 65.7 | 79.5 | 65.9 | 0.2 | 57.2
Manual-CoT | 91.7 | 46.9 | 81.3 | 35.8 | 86.6 | 68.9 | 73.5 | 65.4 | 59.0 | 97.2
Auto-CoT | 92.0 | 47.9 | 84.8 | 36.5 | 87.0 | 69.5 | 74.4 | 65.4 | 59.7 | 99.9

Đường cơ sở. Chúng tôi so sánh phương pháp của chúng tôi với bốn phương pháp đường cơ sở: Zero-Shot [Kojima et al., 2022], Zero-Shot-CoT [Kojima et al., 2022], Few-Shot [Wei et al., 2022a], và Manual-CoT [Wei et al., 2022a]. Zero-Shot-CoT và Manual-CoT được minh họa trong Hình 1. Đường cơ sở Zero-Shot nối một câu hỏi kiểm tra với lời nhắc "Câu trả lời là" làm đầu vào LLM. Đường cơ sở Few-Shot có cùng đầu vào LLM như Manual-CoT ngoại trừ việc loại bỏ các lý lẽ từ tất cả các minh chứng.

5.2 Hiệu suất cạnh tranh của Auto-CoT trên mười tập dữ liệu

Bảng 4: Độ chính xác sử dụng LLM Codex.

Phương pháp | MultiArith | GSM8K | AddSub
--- | --- | --- | ---
Zero-Shot-CoT | 64.8 | 31.8 | 65.6
Manual-CoT | 96.8 | 59.4 | 84.6
Auto-CoT | 93.2 | 62.8 | 91.9

Bảng 3 so sánh độ chính xác trên mười tập dữ liệu từ ba loại nhiệm vụ suy luận. Kết quả Zero-Shot và Zero-Shot-CoT được lấy từ Kojima et al. [2022], kết quả Few-Shot và Manual-CoT được lấy từ Wei et al. [2022a], và kết quả Auto-CoT được tính trung bình trên ba lần chạy ngẫu nhiên. Nhìn chung, Auto-CoT luôn phù hợp hoặc vượt trội so với hiệu suất của mô hình CoT yêu cầu thiết kế thủ công các minh chứng. Do chi phí của thiết kế thủ công, Manual-CoT có thể thiết kế cùng các minh chứng cho nhiều tập dữ liệu (ví dụ, 5/6 của các tập dữ liệu số học). Ngược lại, Auto-CoT linh hoạt và thích ứng với nhiệm vụ hơn: mọi tập dữ liệu đều có các minh chứng riêng được xây dựng tự động.

5.3 Trực quan hóa phân cụm câu hỏi
Hình 5 trực quan hóa phân cụm câu hỏi (với phép chiếu PCA) trong mười tập dữ liệu. Minh họa chỉ ra rằng tồn tại các mẫu chung chung, nơi các mẫu khác nhau có thể được đặc trưng bởi các câu hỏi từ các cụm khác nhau. Chúng tôi trình bày các minh chứng được xây dựng của Auto-CoT trong Phụ lục D.

[Hình 5: Phân cụm câu hỏi trên mười tập dữ liệu của các nhiệm vụ suy luận. Các ngôi sao biểu thị tâm cụm.]

--- TRANG 8 ---
5.4 Hiệu quả chung khi sử dụng LLM Codex
Để đánh giá hiệu quả chung của Auto-CoT sử dụng các LLM khác nhau, ở đây chúng tôi thay đổi LLM thành mô hình Codex [Chen et al., 2021]. Như trong Bảng 4, LLM Codex dẫn đến cải thiện hiệu suất cho Manual-CoT khi so sánh với Bảng 3 sử dụng LLM GPT-3 (text-davinci-002). Tuy nhiên, khi sử dụng LLM Codex, hiệu suất tổng thể của Auto-CoT vẫn cạnh tranh so với Manual-CoT, cung cấp bằng chứng thực nghiệm bổ sung cho hiệu quả của Auto-CoT.

5.5 Tác động của minh chứng sai
Nhớ lại các thảo luận của chúng tôi trong Phần 3.3 rằng có thể có minh chứng sai (có câu trả lời sai). Để xem liệu tính đa dạng có giảm thiểu tác động này hay không, chúng tôi thiết kế một đường cơ sở In-Cluster Sampling xây dựng minh chứng bằng cách lấy mẫu ngẫu nhiên các câu hỏi từ cùng cụm chứa một câu hỏi kiểm tra. Hình 6 so sánh độ chính xác với số lượng minh chứng sai khác nhau trên MultiArith. So với In-Cluster Sampling, Auto-CoT (sử dụng phân cụm dựa trên tính đa dạng) ít bị ảnh hưởng bởi minh chứng sai: hiệu suất của nó vẫn không suy giảm đáng kể ngay cả khi được trình bày với 50% minh chứng sai.

12.5% 25.0% 37.5% 50.0%
80 85 90 95 100
Tỷ lệ phần trăm minh chứng sai
Độ chính xác (%)
In-Cluster Sampling Auto-CoT

Hình 6: Tác động của minh chứng sai.

5.6 Thiết lập streaming thách thức hơn
Các nghiên cứu CoT thường giả định rằng một tập dữ liệu đầy đủ với các câu hỏi kiểm tra được đưa ra [Wei et al., 2022a, Kojima et al., 2022]. Dựa trên tập dữ liệu đã cho, Auto-CoT lấy mẫu các câu hỏi để xây dựng các minh chứng. Tuy nhiên, bây giờ chúng tôi xem xét một thiết lập streaming thách thức hơn nơi một batch nhỏ các câu hỏi kiểm tra (giả sử m câu hỏi) đến một lần như trong các luồng dữ liệu.

1 2 3 4 5 6 7 8 9 10
60 70 80 90 100
Batch
Độ chính xác (%)
Zero-Shot-CoT Manual-CoT Auto-CoT*

Hình 7: Bootstrapping cho thiết lập streaming.

Để giải quyết thách thức này, chúng tôi mở rộng Auto-CoT thành một phiên bản bootstrapping Auto-CoT*: (i) Khởi tạo một tập trống M0; (ii) Khi batch 1 của các câu hỏi q(1)_1, ..., q(1)_m đến, gọi Zero-Shot-CoT (không phân cụm do m nhỏ) cho mỗi q(1)_i để có được chuỗi suy luận c(1)_i của nó. Thêm các cặp câu hỏi-chuỗi (q(1)_1, c(1)_1), ..., (q(1)_m, c(1)_m) vào M0 và gọi tập mới là M1; (iii) Khi batch b (b > 1) của các câu hỏi q(b)_1, ..., q(b)_m đến, xây dựng minh chứng với các câu hỏi và chuỗi suy luận hiện có trong Mb-1 (như Auto-CoT) và sử dụng các minh chứng cho suy luận trong ngữ cảnh cho mỗi q(b)_i. Thêm các cặp câu hỏi-chuỗi (q(b)_1, c(b)_1), ..., (q(b)_m, c(b)_m) vào Mb-1 và gọi tập mới là Mb.

Hình 7 so sánh độ chính xác trên MultiArith tại mỗi batch (m = 30) trong thiết lập streaming này (phiên bản mở rộng: Hình 11 trong Phụ lục). Như mong đợi, đối với batch 1, Auto-CoT* và Zero-Shot-CoT có được độ chính xác bằng nhau. Từ batch 2, Auto-CoT* hoạt động tương đương với Manual-CoT. Kết quả này chỉ ra rằng phương pháp của chúng tôi vẫn hiệu quả trong thiết lập streaming thách thức hơn.

6 Kết luận
LLM đã thể hiện khả năng suy luận với thúc đẩy CoT. Hiệu suất vượt trội của Manual-CoT phụ thuộc vào việc tạo ra các minh chứng bằng tay. Để loại bỏ những thiết kế thủ công như vậy, chúng tôi đã đề xuất Auto-CoT để tự động xây dựng minh chứng. Nó lấy mẫu các câu hỏi với tính đa dạng và tạo ra các chuỗi suy luận để xây dựng minh chứng. Kết quả thí nghiệm trên mười tập dữ liệu suy luận chuẩn công khai cho thấy rằng với GPT-3, Auto-CoT luôn phù hợp hoặc vượt trội so với hiệu suất của mô hình CoT yêu cầu thiết kế thủ công các minh chứng.

--- TRANG 9 ---
Tài liệu tham khảo
[Các tài liệu tham khảo được liệt kê từ trang 9-25, bao gồm các nghiên cứu về mô hình ngôn ngữ lớn, thúc đẩy chuỗi suy nghĩ, học trong ngữ cảnh, và các phương pháp liên quan khác]
