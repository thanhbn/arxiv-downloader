# 2304.07575.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2304.07575.pdf
# Kích thước file: 893939 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Điều gì tạo nên các minh chứng trong ngữ cảnh tốt cho
các tác vụ trí tuệ mã với LLM?
Shuzheng Gao1†, Xin-Cheng Wen1, Cuiyun Gao1∗, Wenxuan Wang2, Hongyu Zhang3, Michael R. Lyu2
1Trường Khoa học và Công nghệ Máy tính, Viện Công nghệ Harbin, Thâm Quyến, Trung Quốc
2Khoa Khoa học và Kỹ thuật Máy tính, Đại học Trung văn Hồng Kông, Trung Quốc
3Trường Kỹ thuật Dữ liệu lớn và Phần mềm, Đại học Trùng Khánh, Trung Quốc
szgao98@gmail.com, xiamenwxc@foxmail.com, gaocuiyun@hit.edu.cn, hyzhang@cqu.edu.cn, {wxwang,lyu }@cse.cuhk.edu.hk
Tóm tắt —Các mô hình được đào tạo trước trên mã nguồn đã trở nên phổ biến rộng rãi trong nhiều tác vụ trí tuệ mã. Gần đây, với việc mở rộng quy mô mô hình và kho dữ liệu, các mô hình ngôn ngữ lớn đã thể hiện khả năng học trong ngữ cảnh (ICL). ICL sử dụng hướng dẫn tác vụ và một vài ví dụ làm minh chứng, sau đó nhập các minh chứng vào mô hình ngôn ngữ để thực hiện dự đoán. Mô hình học tập mới này không cần đào tạo và đã cho thấy hiệu suất ấn tượng trong nhiều tác vụ xử lý ngôn ngữ tự nhiên và trí tuệ mã. Tuy nhiên, hiệu suất của ICL phụ thuộc rất nhiều vào chất lượng minh chứng, ví dụ như các ví dụ được chọn. Điều quan trọng là phải điều tra có hệ thống cách xây dựng minh chứng tốt cho các tác vụ liên quan đến mã.
Trong bài báo này, chúng tôi khám phá thực nghiệm tác động của ba yếu tố chính đến hiệu suất ICL trong các tác vụ trí tuệ mã: việc lựa chọn, thứ tự và số lượng ví dụ minh chứng. Chúng tôi tiến hành thí nghiệm rộng rãi trên ba tác vụ trí tuệ mã bao gồm tóm tắt mã, sửa lỗi và tổng hợp chương trình. Kết quả thực nghiệm của chúng tôi cho thấy rằng cả ba yếu tố trên đều tác động mạnh đến hiệu suất ICL trong các tác vụ trí tuệ mã. Ngoài ra, chúng tôi tóm tắt các phát hiện và đưa ra các đề xuất thực tiễn về cách xây dựng minh chứng hiệu quả, xem xét ba góc độ này. Chúng tôi cũng chỉ ra rằng minh chứng được thiết kế cẩn thận dựa trên các phát hiện của chúng tôi có thể dẫn đến cải thiện đáng kể so với các phương pháp xây dựng minh chứng được sử dụng rộng rãi, ví dụ như cải thiện BLEU-4, EM và EM ít nhất 9.90%, 175.96% và 50.81% tương ứng trên tóm tắt mã, sửa lỗi và tổng hợp chương trình.
I. GIỚI THIỆU
Gần đây, đã có sự quan tâm ngày càng tăng đến nghiên cứu trí tuệ mã, nhằm giảm bớt gánh nặng cho các nhà phát triển phần mềm và nâng cao năng suất lập trình [1], [2]. Với kho dữ liệu mã nguồn mở quy mô lớn và tiến bộ của các kỹ thuật học sâu, nhiều mô hình mã nguồn thần kinh khác nhau đã được phát triển và đạt được hiệu suất tốt nhất trên nhiều tác vụ trí tuệ mã bao gồm tóm tắt mã [3], sửa lỗi [4] và tổng hợp chương trình [5].
Trong những năm gần đây, sự ra đời của các kỹ thuật đào tạo trước đã thúc đẩy đáng kể tiến bộ trong lĩnh vực này. Ví dụ, CodeBERT [6], một mô hình dựa trên BERT được đào tạo trước trên dữ liệu ngôn ngữ tự nhiên và lập trình, đã thể hiện hiệu suất đầy hứa hẹn trong nhiều tác vụ trí tuệ mã [4], [7]. Các mô hình mã được đào tạo trước tiếp theo khác như PLBART [8] và CodeT5 [9] cải thiện thêm nhiều so với CodeBERT. Tuy nhiên, kích thước và dữ liệu đào tạo của các mô hình trên bị hạn chế, có thể cản trở các mô hình đạt được tiềm năng của chúng [10]. Trong những năm này, chúng ta đã chứng kiến sự tăng trưởng bùng nổ về kích thước của các mô hình được đào tạo trước. Nhiều mô hình ngôn ngữ lớn cấp tỷ tham số khác nhau đã được đề xuất như GPT-3 [11] và PALM-E [12]. Ví dụ, kích thước của mô hình được đào tạo trước PALM-E [12] (562B) năm 2023 gấp hơn hai nghìn lần so với mô hình lớn nhất BERT [13] (223M) năm 2018.
Khi kích thước của mô hình ngôn ngữ và dữ liệu đào tạo tiếp tục tăng, các mô hình ngôn ngữ lớn (LLM) thể hiện nhiều khả năng nổi bật khác nhau. Một khả năng như vậy là học trong ngữ cảnh (ICL) [11], [14], cho phép mô hình học từ chỉ một vài ví dụ trong một ngữ cảnh cụ thể. Như được thể hiện trong Hình 1, ICL sử dụng minh chứng bao gồm hướng dẫn tác vụ và một vài ví dụ để mô tả tác vụ, sau đó được nối với câu hỏi truy vấn để tạo thành đầu vào cho mô hình ngôn ngữ thực hiện dự đoán. Sự khác biệt quan trọng nhất giữa ICL và các phương pháp điều chỉnh truyền thống như tinh chỉnh [6] là nó không cần đào tạo và không cần cập nhật tham số. Mô hình đào tạo này cho phép ICL được sử dụng trực tiếp trên bất kỳ LLM nào và giảm đáng kể chi phí đào tạo để điều chỉnh mô hình cho các tác vụ mới [11]. Các nghiên cứu gần đây cho thấy ICL đã đạt được kết quả ấn tượng trong nhiều lĩnh vực khác nhau, bao gồm suy luận logic [15], hệ thống đối thoại [16] và sửa chữa chương trình [17]–[19], và thậm chí có thể vượt trội hơn các phương pháp tinh chỉnh có giám sát được đào tạo trên dữ liệu cụ thể theo tác vụ lớn.

Tham số Đông lánh
Mô hình ngôn ngữ lớn
Dự đoán:
kiểm tra xem đường dẫn đã cho có phải là file hay không.
...Tạo bình luận cho mã này
[input] @ Override public long ...
[output] đọc giá trị int của ...
Dữ liệu kiểm tra
Tạo bình luận cho mã này
[input] public static boolean isFile...
[output]
Tạo bình luận cho mã này
[input] static boolean contains ...
[output] trả về true nếu không ...
Minh chứng
Prompt:
Hình 1: Một ví dụ về học trong ngữ cảnh trên tác vụ tóm tắt mã.

--- TRANG 2 ---
các lĩnh vực, bao gồm suy luận logic [15], hệ thống đối thoại [16] và sửa chữa chương trình [17]–[19], và thậm chí có thể vượt trội hơn các phương pháp tinh chỉnh có giám sát được đào tạo trên dữ liệu cụ thể theo tác vụ lớn.
Mặc dù ICL đã được chứng minh là hữu ích trong các tác vụ trí tuệ mã, hiệu suất của ICL được biết là phụ thuộc mạnh vào chất lượng minh chứng [20], [21]. Các nghiên cứu hiện tại [17], [19] chủ yếu xây dựng minh chứng bằng cách chọn và sắp xếp ngẫu nhiên các ví dụ minh chứng. Theo hiểu biết của chúng tôi, vẫn thiếu một cuộc điều tra sâu về ICL cho các tác vụ trí tuệ mã. Xem xét hiệu suất ấn tượng của ICL, việc hiểu tác động của thiết kế minh chứng và điều tra các thách thức của việc áp dụng ICL cho các tác vụ trí tuệ mã là xứng đáng. Trong công việc này, chúng tôi phân tích có hệ thống cách các phương pháp xây dựng minh chứng khác nhau ảnh hưởng đến hiệu suất ICL trên các tác vụ trí tuệ mã, nhằm trả lời câu hỏi sau: Điều gì tạo nên các minh chứng trong ngữ cảnh tốt cho các tác vụ trí tuệ mã với LLM? Bằng cách phân tích không gian thiết kế của minh chứng trong ngữ cảnh, nghiên cứu của chúng tôi chủ yếu tập trung vào ba khía cạnh của minh chứng trong ngữ cảnh, bao gồm việc lựa chọn, thứ tự và số lượng ví dụ minh chứng. Chúng tôi tiến hành một nghiên cứu thực nghiệm về ba tác vụ trí tuệ mã phổ biến bao gồm tóm tắt mã, sửa lỗi và tổng hợp chương trình. Cụ thể, chúng tôi chủ yếu điều tra bốn câu hỏi nghiên cứu (RQ) sau:
1) Loại phương pháp lựa chọn nào hữu ích cho ICL trong các tác vụ trí tuệ mã?
2) Các ví dụ minh chứng nên được sắp xếp như thế nào cho ICL trong các tác vụ trí tuệ mã?
3) Số lượng ví dụ minh chứng trong một prompt tác động như thế nào đến hiệu suất ICL trong các tác vụ trí tuệ mã?
4) Tính khái quát hóa của các phát hiện của chúng tôi như thế nào?
Để trả lời RQ đầu tiên, chúng tôi so sánh một loạt các phương pháp lựa chọn minh chứng như lựa chọn ngẫu nhiên, lựa chọn dựa trên độ tương tự và lựa chọn dựa trên tính đa dạng. Chúng tôi cũng thử nghiệm với các phương pháp truy xuất khác nhau trong lựa chọn dựa trên độ tương tự để tìm ra phương pháp truy xuất nào hữu ích hơn cho ICL trong các tác vụ trí tuệ mã. Để trả lời RQ thứ hai, chúng tôi so sánh sắp xếp ngẫu nhiên với hai phương pháp sắp xếp khác bao gồm độ tương tự và độ tương tự ngược, hướng tới khám phá tác động của các phương pháp sắp xếp khác nhau. Để trả lời RQ3, chúng tôi thay đổi số lượng ví dụ minh chứng trong prompt và điều tra xem hiệu suất ICL có tăng với sự gia tăng số lượng ví dụ minh chứng hay không. Để trả lời RQ cuối cùng, chúng tôi thử nghiệm trên các LLM khác nhau và xác nhận các phát hiện mà chúng tôi đạt được trong các RQ trên.
Các phát hiện chính. Dựa trên các thí nghiệm rộng rãi, nghiên cứu của chúng tôi tiết lộ một số phát hiện chính:
1) Cả độ tương tự và tính đa dạng trong lựa chọn minh chứng đều là những yếu tố quan trọng cho ICL trong các tác vụ trí tuệ mã. Chúng không chỉ nâng cao hiệu suất tổng thể mà còn dẫn đến dự đoán ổn định hơn.
2) Thứ tự của các ví dụ minh chứng có tác động lớn đến hiệu suất ICL. Trong hầu hết các trường hợp, đặt các mẫu tương tự ở cuối prompt đạt được kết quả tốt hơn.
3) Tăng số lượng ví dụ minh chứng có thể có lợi cho ICL, với điều kiện các ví dụ không bị cắt bớt do giới hạn độ dài đầu vào của LLM. Cần chú ý cẩn thận đến vấn đề này, vì độ dài của mã thường dài hơn ngôn ngữ tự nhiên.
Chúng tôi cũng chỉ ra rằng minh chứng được thiết kế cẩn thận dựa trên các phát hiện đạt được có thể dẫn đến cải thiện đáng kể so với các phương pháp xây dựng minh chứng được sử dụng rộng rãi [17], [19], [22], ví dụ như cải thiện BLEU-4, EM và EM ít nhất 9.90%, 175.96% và 50.81% tương ứng trên tóm tắt mã, sửa lỗi và tổng hợp chương trình.
Đóng góp. Tóm lại, các đóng góp chính của công việc này như sau:
1) Theo hiểu biết của chúng tôi, bài báo này đại diện cho nghiên cứu có hệ thống đầu tiên về cách xây dựng minh chứng hiệu quả cho các tác vụ trí tuệ mã.
2) Việc khám phá toàn diện về thiết kế minh chứng của chúng tôi làm nổi bật một loạt phát hiện để cải thiện hiệu suất ICL trong các tác vụ trí tuệ mã.
3) Chúng tôi thảo luận về ý nghĩa của các phát hiện đối với các nhà nghiên cứu và nhà phát triển cũng như công việc tương lai cho các tác vụ trí tuệ mã trong thời đại mô hình ngôn ngữ lớn.

II. KIẾN THỨC NỀN TẢNG
A. Mô hình ngôn ngữ lớn
LLM đã trở thành một phần phổ biến của Xử lý ngôn ngữ tự nhiên (NLP) do hiệu suất xuất sắc [11], [23]. Các mô hình này thường tuân theo kiến trúc Transformer [24] và được đào tạo trên kho dữ liệu quy mô lớn bằng các mục tiêu tự giám sát như mô hình ngôn ngữ che mặt [13]. Kích thước của LLM đã tăng đáng kể trong vài năm qua. Ví dụ, các tham số của LLM gần đây như GPT-3 [11] và PALM-E [12] là hơn một trăm tỷ. Ngoài LLM cho mục đích chung, cũng có LLM với tham số cấp tỷ được đào tạo trên kho dữ liệu mã, như AlphaCode [25] và Codex [2]. Codex của OpenAI là một mô hình mã được đào tạo trước lớn có khả năng cung cấp sức mạnh cho Copilot. AlphaCode [25] là một mô hình lớn 41 tỷ tham số được đào tạo để tạo mã trong các cuộc thi lập trình như Codeforces. Gần đây, các LLM như ChatGPT [26] và GPT-4 [23] cũng đã thể hiện hiệu suất ấn tượng trong nhiều tác vụ trí tuệ mã.
Ngoài việc đề xuất LLM mới, cách tận dụng chúng một cách hiệu quả cũng đã trở thành một chủ đề nghiên cứu quan trọng. Một phương pháp phổ biến là tinh chỉnh mô hình và cập nhật các tham số của nó trên tập dữ liệu downstream [13]. Gần đây, tinh chỉnh dựa trên prompt đã được đề xuất, nhằm chuyển đổi mục tiêu đào tạo của các tác vụ downstream thành dạng tương tự như giai đoạn đào tạo trước [27], [28]. Xét đến chi phí điều chỉnh toàn bộ mô hình, nhiều phương pháp Điều chỉnh hiệu quả tham số khác nhau đã được đề xuất, như Adapter [29], Lora [30] và prefix tuning [31]. Các phương pháp này giữ hầu hết các tham số trong mô hình đông lạnh và chỉ điều chỉnh một phần nhỏ trong số chúng.

--- TRANG 3 ---
1. Lựa chọn minh chứng

Demo 3 Demo 2 Demo 1

Demo K... ... Tập đào tạo 

Demo 3 Demo 1

Demo N... Tập lựa chọn

Demo 3 Demo N Demo 1 ...

Demo 3 Demo N Demo 1 ...

Demo 3 Demo N Demo 1 ...

Mẫu kiểm tra

Mẫu kiểm tra

Mẫu kiểm tra

...

Thứ tự 1:

Thứ tự 2:

Thứ tự N!:

2. Sắp xếp minh chứng

3. Số lượng ví dụ minh chứng

Hình 2: Minh họa không gian thiết kế của minh chứng trong ngữ cảnh.

B. Học trong ngữ cảnh
Điều chỉnh một mô hình được đào tạo trước lớn có thể tốn kém và không thực tế đối với các nhà nghiên cứu, đặc biệt là khi dữ liệu tinh chỉnh có sẵn hạn chế cho một số tác vụ nhất định. ICL cung cấp một lựa chọn thay thế mới sử dụng mô hình ngôn ngữ để thực hiện các tác vụ downstream mà không cần cập nhật tham số [11], [14]. Nó tận dụng minh chứng trong prompt để giúp mô hình học ánh xạ đầu vào-đầu ra của tác vụ. Mô hình mới này đã đạt được kết quả ấn tượng trong nhiều tác vụ khác nhau như suy luận logic và sửa chữa chương trình [15], [17], [19].
Cụ thể, như được thể hiện trong Hình 1, ICL sử dụng N ví dụ minh chứng {(x1, y1),(x2, y2), ..., (xN, yN)} và tiếp tục tái cấu trúc chúng thành các ví dụ được tái cấu trúc {(x′₁, y′₁),(x′₂, y′₂), ..., (x′ₙ, y′ₙ)} bằng hướng dẫn ngôn ngữ tự nhiên và mẫu prompt, trong đó xi,yi,x′i,y′i là đầu vào, đầu ra, đầu vào được tái cấu trúc và đầu ra được tái cấu trúc, tương ứng. Thông thường, giá trị của N tương đối nhỏ, tức là ít hơn 50 mẫu, nhỏ hơn đáng kể so với kích thước của tập đào tạo trong các phương pháp tinh chỉnh trước đây [6], [9]. Cài đặt này được gọi là học trong ngữ cảnh few-shot. Đặc biệt, khi giá trị của N bằng không, nó được gọi là cài đặt học trong ngữ cảnh zero-shot. Sau đó, ICL nối các ví dụ minh chứng được tái cấu trúc d₁ đến dₙ một cách đen thẳng thành minh chứng D=x′₁∥y′₁∥x′₂∥y′₂∥...∥x′ₙ∥y′ₙ, và tiếp tục thêm mẫu kiểm tra vào cuối để xây dựng prompt đầu vào P=D∥x′test, trong đó ∥ biểu thị phép nối đen thẳng. Prompt này cuối cùng được đưa vào mô hình ngôn ngữ để dự đoán nhãn ytest cho mẫu kiểm tra.
Các nghiên cứu trước đây trong NLP đã chỉ ra rằng hiệu suất của ICL phụ thuộc mạnh vào chất lượng minh chứng. Ví dụ, Liu et al. [20] cho thấy rằng việc chọn các ví dụ minh chứng với độ tương tự cao hơn hoặc tăng số lượng ví dụ minh chứng có thể cải thiện hiệu suất ICL. Kết quả trong [21] cho thấy rằng thứ tự của các ví dụ minh chứng cũng có tác động lớn đến kết quả. Theo các nghiên cứu trước đây, chúng tôi tóm tắt ba yếu tố chính cần xem xét khi thiết kế minh chứng cho ICL: việc lựa chọn, sắp xếp và số lượng ví dụ minh chứng, như được thể hiện trong Hình 2.
Chúng tôi muốn làm rõ thêm rằng có hai loại minh chứng trong ICL: minh chứng cấp tác vụ và minh chứng cấp instance [32], [33]. Minh chứng cấp tác vụ sử dụng cùng các ví dụ minh chứng cho tất cả mẫu kiểm tra và không xem xét sự khác biệt của từng mẫu kiểm tra, trong khi minh chứng cấp instance chọn các ví dụ minh chứng khác nhau cho các mẫu kiểm tra khác nhau. Mặc dù minh chứng cấp instance thường hoạt động tốt hơn minh chứng cấp tác vụ, nó đòi hỏi một tập đào tạo có nhãn trước để truy xuất. Minh chứng cấp tác vụ linh hoạt hơn vì nó có thể được sử dụng trong các tình huống mà rất ít dữ liệu được gán nhãn, hoặc không có dữ liệu có nhãn nào bằng cách chọn vài dữ liệu đại diện để con người gán nhãn [33]. Trong bài báo này, chúng tôi điều tra cả phương pháp xây dựng minh chứng cấp tác vụ và cấp instance cho các tác vụ trí tuệ mã.

III. ĐÁNH GIÁ THỰC NGHIỆM
A. Câu hỏi nghiên cứu
Chúng tôi thiết kế thí nghiệm để điều tra tác động của việc lựa chọn, sắp xếp và số lượng minh chứng đối với ICL cho các tác vụ trí tuệ mã. Nghiên cứu của chúng tôi nhằm trả lời các câu hỏi sau:
RQ1: Loại phương pháp lựa chọn nào hữu ích cho ICL trong các tác vụ trí tuệ mã?
RQ2: Các ví dụ minh chứng nên được sắp xếp như thế nào cho ICL trong các tác vụ trí tuệ mã?
RQ3: Số lượng ví dụ minh chứng trong một prompt tác động như thế nào đến hiệu suất ICL trong các tác vụ trí tuệ mã?
RQ4: Tính khái quát hóa của các phát hiện của chúng tôi như thế nào?
Trong RQ1, chúng tôi nhằm xác minh xem việc chọn các ví dụ minh chứng tương tự và đa dạng có hữu ích hay không. Bên cạnh đó, chúng tôi cũng so sánh các phương pháp truy xuất khác nhau để phân tích tác động của các phương pháp đo lường độ tương tự khác nhau cho ICL. RQ2 nhằm điều tra ảnh hưởng của các phương pháp sắp xếp bằng cách so sánh sắp xếp ngẫu nhiên với sắp xếp dựa trên độ tương tự. Trong RQ3, chúng tôi muốn khám phá xem việc tăng số lượng ví dụ có thể mang lại hiệu suất tốt hơn cho ICL hay không. Trong RQ4, chúng tôi đánh giá xem các phát hiện đạt được trong RQ1-RQ3 có áp dụng được cho các LLM khác nhau để xác minh tính khái quát hóa của các phát hiện hay không.

B. Các tác vụ đánh giá
Chúng tôi tiến hành thí nghiệm trên ba tác vụ trí tuệ mã phổ biến: tóm tắt mã, sửa lỗi và tổng hợp chương trình.
1) Tóm tắt mã: Tóm tắt mã, còn được gọi là tạo bình luận mã, nhằm tự động tạo ra các bình luận hữu ích cho đoạn mã cho sẵn [7]. Các nghiên cứu gần đây chủ yếu hình thức hóa nó như một tác vụ dịch máy thần kinh chuỗi-đến-chuỗi (NMT) và liên quan đến các kỹ thuật được đào tạo trước để đạt hiệu suất tốt hơn [9], [34].

--- TRANG 4 ---
BẢNG I: Thống kê của các tập dữ liệu benchmark.
Tác vụ Tập dữ liệu Đào tạo Phát triển Kiểm tra
Tóm tắt mã CSN-Java 164,923 5,183 10,955
TLC 69,708 8,714 6,489
Sửa lỗi B2Fsmall 46,628 5,828 5,831
B2Fmedium 53,324 6,542 6,538
Tổng hợp chương trình CoNaLa 2,389 - 500

Tập dữ liệu. Để đánh giá hiệu suất tóm tắt mã, chúng tôi sử dụng hai tập dữ liệu được sử dụng rộng rãi: CodeSearchNet (CSN) [35] và TLCodeSum (TLC) [7]. CSN là tập dữ liệu mã nguồn quy mô lớn được khai thác từ các kho lưu trữ GitHub mã nguồn mở. Nó chứa dữ liệu tóm tắt mã trong sáu ngôn ngữ lập trình, tức là Java, Go, JavaScript, PHP, Python và Ruby. Tập dữ liệu được chia thành các tập đào tạo, xác thực và kiểm tra theo tỷ lệ 8:1:1. Trong nghiên cứu này, xét đến giới hạn thời gian và tài nguyên của chúng tôi, chúng tôi sử dụng phần Java của tập dữ liệu CSN được lọc trong CodeBERT [6], chứa 181,061 mẫu qua các tập đào tạo, xác thực và kiểm tra để đánh giá. TLC có 87,136 cặp mã-bình luận được thu thập từ 9,732 dự án Java mã nguồn mở trên GitHub với ít nhất 20 sao. Các đoạn mã đều ở cấp độ phương thức và các bình luận của các phương thức Java tương ứng được coi là tóm tắt mã. Tỷ lệ tập đào tạo, xác thực và kiểm tra cũng là 8:1:1. Như báo cáo trong công việc trước đây, có dữ liệu trùng lặp trong tập đào tạo và kiểm tra. Do đó, chúng tôi theo công việc trước đây [36] và loại bỏ dữ liệu trùng lặp, cuối cùng nhận được tập kiểm tra với 6,489 mẫu.

Chỉ số. Chúng tôi sử dụng ba chỉ số được áp dụng rộng rãi để đánh giá tóm tắt mã: BLEU-4 [37], ROUGE-L [38] và METEOR [39] để đánh giá. Các chỉ số này đánh giá độ tương tự giữa tóm tắt được tạo ra và tóm tắt ground-truth và được sử dụng rộng rãi trong tóm tắt mã [3], [36], [40].

2) Sửa lỗi: Sửa lỗi là tác vụ tự động sửa lỗi trong đoạn mã cho sẵn. Nó giúp các nhà phát triển phần mềm tìm và sửa lỗi phần mềm [4], [41].

Tập dữ liệu. Tập dữ liệu cho sửa lỗi là B2F được thu thập bởi Tufano et al. [4] từ các commit sửa lỗi trong GitHub. Chúng tôi sử dụng phiên bản đa mô hình được đề xuất trong MODIT [42] cho thí nghiệm vì nó chứa cả thay đổi mã và hướng dẫn sửa lỗi. Mô hình được cung cấp cả mã lỗi và hướng dẫn sửa lỗi bằng ngôn ngữ tự nhiên để dự đoán mã đã sửa. Chúng tôi theo cài đặt ban đầu của họ để chia tập dữ liệu thành hai phần B2Fmedium và B2Fsmall dựa trên độ dài của token mã (độ dài mã của B2Fmedium nằm giữa 50 và 100 token và của B2Fsmall dưới 50 token).

Chỉ số. Chúng tôi theo công việc trước đây [43] và sử dụng Exact Match (EM) và BLEU-4 cho cả hai tập dữ liệu.

3) Tổng hợp chương trình: Tổng hợp chương trình là tác vụ tạo mã nguồn dựa trên mô tả ngôn ngữ tự nhiên đã cho. Nó cung cấp hỗ trợ thực tế cho các nhà phát triển và nâng cao năng suất của họ [2].

Tập dữ liệu. Cho tổng hợp chương trình, chúng tôi sử dụng tập dữ liệu CoNaLa [44] để đánh giá. Tập dữ liệu này bao gồm 2,889 cặp ⟨intent, code⟩ được khai thác từ Stack Overflow trong Python. Chúng tôi trực tiếp

BẢNG II: Mẫu prompt cho từng tác vụ. Ở đây văn bản dưới dạng {#xxx} sẽ được điền vào các đầu vào thực tế từ tập dữ liệu.
Tác vụ Mẫu
Tóm tắt mã Tạo bình luận (tóm tắt) cho mã này
[input] {#code}[output] {#comment}
Sửa lỗi Sửa lỗi theo hướng dẫn [input]
{#buggy code}<s>{#instruction}[output] {#fixed code}
Tổng hợp chương trình Tạo mã dựa trên yêu cầu
[input] {#requirement}[output] {#code}

sử dụng phân vùng gốc của tập dữ liệu, bao gồm 2,389 mẫu để đào tạo và 500 mẫu để kiểm tra.

Chỉ số. Chúng tôi theo công việc trước đây [43] và đánh giá hiệu suất tổng hợp chương trình với bốn chỉ số bao gồm Exact Match (EM), CodeBLEU (CB), Syntax Match (SM) và Dataflow Match (DM). EM đo lường xem mã được tạo ra bởi mô hình có giống với mã mục tiêu hay không. CB [45] là phiên bản sửa đổi của BLEU được thiết kế đặc biệt cho mã, tận dụng thông tin cú pháp và ngữ nghĩa như Cây cú pháp trừu tượng (AST) và luồng dữ liệu để đo lường độ tương tự của hai đoạn mã. SM và DM là hai thành phần tính toán tỷ lệ cây con khớp và các cạnh luồng dữ liệu, tương ứng.

C. Triển khai
Chúng tôi sử dụng API OpenAI Codex (code-davinci-002) [2] trong bài báo của chúng tôi cho tất cả các thí nghiệm trong ba RQ đầu tiên. Trong RQ4, chúng tôi tiếp tục sử dụng API của GPT-3.5 (text-davinci-003) [11] và ChatGPT (gpt-3.5-turbo) [26] cho thí nghiệm. Về siêu tham số của API, theo công việc trước đây [46], [47], chúng tôi đặt nhiệt độ thành 0 để nhận đầu ra tất định. Phạt tần suất và phạt hiện diện cũng được đặt thành 0. Giới hạn độ dài đầu vào của Codex, GPT-3.5 và ChatGPT lần lượt là 8,001, 4,096 và 4,097 token. Do đó chúng tôi cắt bỏ mã đầu vào của mỗi ví dụ minh chứng thành 8001/(N+1), 4096/(N+1) và 4097/(N+1) token, tương ứng, trong đó N đại diện cho số lượng ví dụ minh chứng. Theo kinh nghiệm, cần khoảng 6 giờ để đánh giá 1,000 ví dụ cho Codex. Để tránh chi phí thời gian quá mức, chúng tôi lấy mẫu ngẫu nhiên một tập kiểm tra nhỏ (2,000 mẫu) cho mỗi tập dữ liệu có hơn 2,000 mẫu kiểm tra. Chúng tôi sử dụng bốn ví dụ trong minh chứng trong RQ1 và RQ2, và tiếp tục thảo luận về tác động của số lượng ví dụ minh chứng trong RQ3. Các mẫu được sử dụng trong nghiên cứu này được thể hiện trong Bảng II. Chúng tôi cũng hiển thị một số ví dụ trong kho lưu trữ GitHub của chúng tôi¹. Chúng tôi tiến hành tất cả thí nghiệm trên một máy chủ có 2 GPU Nvidia RTX 3090. Các GPU được sử dụng trong quá trình truy xuất dày đặc.

IV. KẾT QUẢ THỰC NGHIỆM
A. RQ1: Lựa chọn minh chứng
1) Thiết kế thực nghiệm: Chúng tôi đầu tiên khám phá tác động của các phương pháp lựa chọn minh chứng đối với ICL cho các tác vụ liên quan đến mã. Để cung cấp một nghiên cứu toàn diện, chúng tôi áp dụng khác nhau

¹https://github.com/gszsectan/ICL/tree/master/prompts

--- TRANG 5 ---
BẢNG III: Kết quả thực nghiệm của các phương pháp lựa chọn minh chứng khác nhau trên Tóm tắt mã. "Avg" và "CV" biểu thị kết quả trung bình và Hệ số biến thiên trên ba thứ tự khác nhau, tương ứng.

Phương pháp     Tóm tắt mã
                CSN                                 TLC
                BLEU-4  ROUGE-L  METEOR             BLEU-4  ROUGE-L  METEOR
                Avg CV  Avg CV   Avg CV             Avg CV  Avg CV   Avg CV

Minh chứng cấp tác vụ
Random          19.64 1.44  35.46 1.88  15.30 1.54    17.29 0.71  34.28 0.61  12.48 0.67
KmeansRND       20.71 0.82  38.03 0.44  16.34 0.83    17.91 1.19  35.69 1.60  13.48 0.91

Minh chứng cấp instance
BM-25           22.35 0.46  38.31 0.56  17.01 0.78    36.96 0.84  51.42 0.79  24.22 0.99
SBERT           22.27 0.23  38.39 0.42  16.91 0.22    36.42 0.61  50.47 0.40  23.86 0.68
UniXcoder       22.11 0.61  38.23 0.53  16.81 0.23    36.77 0.52  51.11 0.29  24.08 0.79
CoCoSoDa        21.92 0.46  37.85 0.22  16.78 0.24    36.91 0.69  50.69 0.53  24.08 0.39
Oracle (BM-25)  27.69 0.43  46.17 0.14  20.26 0.22    43.16 0.15  59.17 0.09  28.09 0.16

BẢNG IV: Kết quả thực nghiệm của các phương pháp lựa chọn minh chứng khác nhau trên Sửa lỗi.

Phương pháp     Sửa lỗi
                B2Fmedium                       B2Fsmall
                BLEU-4      EM                  BLEU-4      EM
                Avg CV      Avg CV              Avg CV      Avg CV

Minh chứng cấp tác vụ
Random          86.96 0.16  7.26 16.18         71.18 0.56  9.95 6.33
KmeansRND       86.91 0.17  9.03 5.45          72.89 1.36  10.37 3.86

Minh chứng cấp instance
BM-25           88.05 0.09  21.85 1.78         77.54 0.13  30.45 0.96
SBERT           87.98 0.06  19.00 2.88         76.26 0.16  26.15 0.87
UniXcoder       87.87 0.09  19.14 2.00         77.52 0.07  29.93 0.51
CoCoSoDa        87.73 0.07  19.23 0.74         76.45 0.07  27.40 1.04

các phương pháp lựa chọn minh chứng cho ba tác vụ trí tuệ mã.

Đối với minh chứng cấp tác vụ, chúng tôi cần chọn một nhóm ví dụ minh chứng cho toàn bộ tập kiểm tra, như minh họa trong Phần II-B. Để khám phá ảnh hưởng của các ví dụ minh chứng trong ngữ cảnh khác nhau đến hiệu suất ICL, chúng tôi chọn ngẫu nhiên ba nhóm ví dụ minh chứng từ tập đào tạo, và đánh giá hiệu suất của chúng trên các tác vụ khác nhau, được ký hiệu là Random. Bên cạnh đó, chúng tôi tiếp tục điều tra xem việc cải thiện tính đa dạng của các ví dụ minh chứng có lợi cho ICL hay không. Chúng tôi chọn các ví dụ minh chứng bằng cách đầu tiên chia tất cả mẫu thành N cluster và sau đó chọn ngẫu nhiên một mẫu từ mỗi cluster, được gọi là KmeansRND. Cụ thể, chúng tôi sử dụng UniXcoder [48] cho việc vector hóa và sử dụng thuật toán K-means++ [49] cho clustering, trong đó K được đặt thành N đại diện cho số lượng ví dụ minh chứng. Tương tự như Random, chúng tôi cũng điều tra hiệu suất của các nhóm ví dụ khác nhau cho KmeansRND và tiến hành quá trình lựa chọn ba lần, dẫn đến ba nhóm ví dụ minh chứng.

Đối với minh chứng cấp instance, chúng tôi cần chọn ví dụ cho từng mẫu kiểm tra, như minh họa trong Phần II-B. Theo [20], chúng tôi hình thức hóa quá trình lựa chọn như một vấn đề truy xuất và so sánh hiệu suất của các phương pháp dựa trên truy xuất khác nhau bao gồm:

1) BM-25: BM-25 là phương pháp truy xuất thưa thớt cổ điển trong lĩnh vực truy xuất thông tin. Nó cũng đã được sử dụng rộng rãi trong nhiều mô hình trí tuệ mã [50], [51].

2) SBERT: SBERT [52] là phương pháp mô hình hóa câu phổ biến và đã được sử dụng rộng rãi trong truy xuất văn bản [52], [53]. Cụ thể, trong bài báo này, chúng tôi sử dụng phiên bản được đào tạo thêm trên tập dữ liệu liên quan đến mã để có được biểu diễn mã và văn bản [54].

3) UniXcoder: UniXcoder [48] là mô hình được đào tạo trước đa phương thức chéo thống nhất được đào tạo trước với ba tác vụ mô hình hóa chuỗi và hai tác vụ dựa trên học tương phản. Nó cho thấy hiệu suất đầy hứa hẹn trong tìm kiếm mã-đến-mã zero-shot.

4) CoCoSoDa: CoCoSoDa [55] là mô hình tìm kiếm mã tiên tiến sử dụng học tương phản cho việc học biểu diễn mã và văn bản.

Đối với BM-25, chúng tôi triển khai với gói gensim [56] bằng cách truy xuất mẫu với độ tương tự cao nhất từ tập đào tạo. Đối với các phương pháp truy xuất dày đặc, chúng tôi trực tiếp sử dụng các mô hình được đào tạo trước này trong các gói tái tạo được phát hành bởi các tác giả mà không cần điều chỉnh thêm. Dựa trên biểu diễn mã/văn bản được xuất ra bởi các mô hình được đào tạo trước, chúng tôi chọn các mẫu đào tạo có độ tương tự cosine cao nhất với mẫu kiểm tra. Chúng tôi cũng theo công việc trước đây [32] và tạo ra một phương pháp được gọi là Oracle, chọn ví dụ minh chứng bằng cách tính toán độ tương tự giữa đầu ra của mẫu kiểm tra và đầu ra của tất cả mẫu tập đào tạo. Phương pháp Oracle thường được coi là giới hạn trên của hiệu suất, xét rằng đầu ra của mẫu kiểm tra không có sẵn trong thực tế. Quá trình truy xuất trong Oracle được triển khai bằng BM-25, vì BM-25 cho thấy hiệu suất tốt nhất so với các phương pháp truy xuất dày đặc khác như được thể hiện trong Bảng III-V.

Để tránh ảnh hưởng của các thứ tự khác nhau của các ví dụ minh chứng, chúng tôi chạy mỗi thí nghiệm ba lần với các thứ tự khác nhau và báo cáo kết quả trung bình trên mỗi chỉ số. Bên cạnh đó, chúng tôi tiếp tục đánh giá độ nhạy cảm của mỗi phương pháp đối với các thứ tự khác nhau bằng Hệ số biến thiên (CV) [57]. CV được tính bằng σ/μ, trong đó σ là độ lệch chuẩn và μ là trung bình. CV thấp hơn cho thấy biến thiên dữ liệu nhỏ hơn. Nó tính đến độ lớn của dữ liệu và đã được sử dụng rộng rãi để đo lường sự phân tán dữ liệu trong nhiều lĩnh vực như kinh tế và kỹ thuật phần mềm [57], [58].

2) Phân tích: Chúng tôi trình bày kết quả thực nghiệm trong Bảng III-V. Đối với mỗi chỉ số, chúng tôi báo cáo kết quả trung bình trên

--- TRANG 6 ---
BẢNG V: Kết quả thực nghiệm của các phương pháp lựa chọn minh chứng khác nhau trên Tổng hợp chương trình.

Phương pháp     Tổng hợp chương trình
                CB      SM      DM      EM
                Avg CV  Avg CV  Avg CV  Avg CV

Minh chứng cấp tác vụ
Random          28.36 1.30  44.37 0.83  39.70 1.33  16.00 1.60
KmeansRND       28.03 1.47  44.41 0.54  37.31 1.54  17.03 1.06

Minh chứng cấp instance
BM-25           30.37 0.91  46.22 0.84  40.75 1.06  18.53 0.50
SBERT           29.08 0.70  44.91 0.31  39.81 3.01  16.13 2.54
UniXcoder       28.96 0.50  43.93 0.67  37.96 1.12  16.00 3.53
CoCoSoDa        29.42 0.82  44.62 0.70  40.91 1.12  16.30 0.86

ba thứ tự ngẫu nhiên và CV đo lường độ nhạy cảm của chúng đối với các thứ tự khác nhau. Trong Hình 3, chúng tôi hiển thị phân phối kết quả với các nhóm ví dụ khác nhau cho Random và KmeansRND.

Tính đa dạng của ví dụ có lợi cho minh chứng cấp tác vụ. Như có thể thấy trong Bảng III-V và Hình 3, bằng cách so sánh kết quả trên Random và KmeansRND, chúng tôi có thể thấy rằng trong hầu hết các trường hợp, việc cải thiện tính đa dạng của minh chứng cấp tác vụ không chỉ có thể cải thiện hiệu suất trung bình của ICL mà còn giảm biến động mang lại bởi các nhóm ví dụ khác nhau. Ví dụ, như được thể hiện trong Bảng III, so sánh kết quả tóm tắt mã trên CSN, các cải thiện trung bình của KmeansRND so với Random là 5.45%, 7.25% và 6.80% tương ứng với BLEU-4, ROUGE-L và METEOR. Bên cạnh đó, chúng tôi cũng có thể thấy rằng hiệu suất của các ví dụ minh chứng trong ngữ cảnh khác nhau của Random thay đổi rất nhiều, và việc cải thiện tính đa dạng của các ví dụ được chọn có thể giảm biến thiên này một cách chung. Ví dụ, như được thể hiện trong Hình 3 (a), khoảng cách giữa điểm BLEU-4 tốt nhất và tệ nhất của Random là khoảng 2.5 trong khi của KmeansRND chỉ khoảng 0.6. Điều này cho thấy rằng việc cải thiện tính đa dạng của các ví dụ minh chứng được chọn có lợi cho việc xây dựng minh chứng cấp tác vụ.

Phát hiện 1: Tính đa dạng của ví dụ hữu ích cho việc lựa chọn minh chứng của ICL. Nó có thể giúp cải thiện hiệu suất tổng thể và dẫn đến dự đoán ổn định hơn đối với các nhóm ví dụ khác nhau.

BM-25 là phương pháp đơn giản và hiệu quả cho minh chứng cấp instance. Bằng cách so sánh kết quả của các phương pháp minh chứng cấp instance khác nhau, chúng tôi có thể thấy rằng phương pháp BM-25 đơn giản có thể đạt được hiệu suất tương đương hoặc thậm chí tốt hơn so với các phương pháp truy xuất dày đặc khác trong việc lựa chọn minh chứng trong ICL. Ví dụ, EM trung bình của BM-25 trên Tổng hợp chương trình là 18.53, vượt trội hơn ba phương pháp truy xuất dày đặc mạnh SBERT, UniXcoder và CoCoSoDa lần lượt 14.88%, 15.81% và 13.68%. Kết quả này cho thấy rằng BM-25 hoạt động như một phương pháp baseline hiệu quả và có thể được xem xét trong các nghiên cứu tương lai về lựa chọn minh chứng cho các tác vụ trí tuệ mã.

Random KmeansRND
18.5 19.0 19.5 20.0 20.5 21.0
BLEU-4
(a) CSN.

Random KmeansRND
16.8 17.0 17.2 17.4 17.6 17.8 18.0 18.2
(b) TLC.

Random KmeansRND
3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0
Exact Match(%)
(c) B2Fmedium.

Random KmeansRND
9.0 10.0 11.0 12.0 13.0
(d) B2Fsmall.

Random KmeansRND
15.5 16.0 16.5 17.0 17.5
(e) CoNaLa.

Hình 3: So sánh phân phối hiệu suất của Random và KmeansRND đối với các nhóm ví dụ khác nhau trên ba tác vụ.

Phát hiện 2: Các phương pháp truy xuất cho lựa chọn minh chứng có thể tác động đến hiệu suất ICL, trong đó BM-25 là phương pháp đơn giản và hiệu quả.

Minh chứng cấp instance vượt trội hơn minh chứng cấp tác vụ rất nhiều. Như được thể hiện trong Bảng III-V, chúng tôi có thể thấy rằng minh chứng cấp instance có thể đạt hiệu suất tốt hơn nhiều trong tất cả các tác vụ. Cụ thể, các phương pháp lựa chọn cấp instance cải thiện kết quả exact match của minh chứng cấp tác vụ tốt nhất ít nhất 141.97% và 193.64% tương ứng trên B2Fmedium và B2Fsmall. Các kết quả này cho thấy rằng việc chọn các ví dụ minh chứng tương tự cụ thể cho từng mẫu kiểm tra có thể mang lại lợi ích rất lớn cho ICL trong các tác vụ trí tuệ mã.

Minh chứng cấp tác vụ nhạy cảm hơn với thứ tự so với minh chứng cấp instance. Bằng cách so sánh CV của minh chứng cấp tác vụ và minh chứng cấp instance, chúng tôi có thể thấy rằng hiệu suất của minh chứng cấp instance thường ổn định hơn so với minh chứng cấp tác vụ đối với các thứ tự ví dụ khác nhau. Cụ thể, như được thể hiện trong Bảng IV, CV của BLEU-4 của minh chứng cấp tác vụ KmeansRND đối với thứ tự là 0.17 và 1.36 trên hai tập dữ liệu sửa lỗi, lớn hơn nhiều so với các phương pháp minh chứng cấp instance (ví dụ, 0.09 và 0.13 cho BM-25, tương ứng). Điều này cho thấy rằng việc chọn ví dụ theo độ tương tự bền vững hơn đối với các thay đổi trong thứ tự minh chứng và chúng ta nên cẩn thận sắp xếp thứ tự của các ví dụ minh chứng khi sử dụng minh chứng cấp tác vụ.

Phát hiện 3: So với minh chứng cấp tác vụ, minh chứng cấp instance có thể đạt hiệu suất tốt hơn nhiều và thường bền vững hơn đối với các thay đổi trong thứ tự minh chứng.

Ngoài những điều trên, chúng tôi cũng có thể quan sát trong Bảng III rằng phương pháp lựa chọn minh chứng tốt nhất BM-25 vẫn có khoảng cách lớn với Oracle. Điều này cho thấy rằng các phương pháp truy xuất này có thể thất bại trong việc chọn các ví dụ tương tự về ngữ nghĩa và tồn tại không gian lớn để cải thiện thêm liên quan đến phương pháp lựa chọn minh chứng cho các tác vụ trí tuệ mã.

B. RQ2: Thứ tự minh chứng
1) Thiết lập thực nghiệm: Trong RQ1, chúng tôi đã thấy rằng thứ tự của các ví dụ minh chứng tác động đến hiệu suất ICL trên các tác vụ trí tuệ mã, đặc biệt là trên minh chứng cấp tác vụ. Do đó, trong phần này, chúng tôi khám phá cách sắp xếp tốt hơn các ví dụ minh chứng trong ICL. Được truyền cảm hứng từ phát hiện rằng minh chứng cấp tác vụ nhạy cảm hơn với thứ tự ví dụ so với minh chứng cấp instance, chúng tôi cho rằng thứ tự độ tương tự giữa mỗi ví dụ minh chứng và mẫu kiểm tra đóng vai trò quan trọng trong ICL.

Để xác minh điều này, trong RQ này, chúng tôi so sánh thứ tự ngẫu nhiên với hai phương pháp sắp xếp cơ bản, tức là Similarity và Reverse Similarity. Trong phương pháp Similarity, chúng tôi so sánh độ tương tự của mỗi ví dụ với mẫu kiểm tra và ví dụ có độ tương tự cao hơn sẽ được đặt gần mẫu kiểm tra hơn. Ngược lại, đối với phương pháp Reverse Similarity, các ví dụ minh chứng sẽ được đặt theo thứ tự giảm dần theo độ tương tự của chúng với mẫu kiểm tra. Chúng tôi thử nghiệm với ba phương pháp lựa chọn minh chứng ở đây. Như minh họa trong RQ1, vì việc sắp xếp thứ tự quan trọng đối với minh chứng cấp tác vụ, chúng tôi sử dụng cả Random và KmeansRND cho thí nghiệm. Đối với minh chứng cấp instance, chúng tôi tiến hành thí nghiệm trên BM-25, vì nó cho thấy hiệu suất tốt nhất trong tất cả các phương pháp lựa chọn minh chứng cấp instance.

2) Phân tích: Từ kết quả trong Bảng VI, chúng tôi có thể thấy rằng việc đặt các ví dụ minh chứng theo thứ tự tăng dần dựa trên độ tương tự của chúng với mẫu kiểm tra thường hoạt động tốt hơn so với thứ tự ngược lại. Cụ thể, Similarity liên tục vượt trội hơn Reverse Similarity trên tóm tắt mã và sửa lỗi ít nhất 0.45% và 0.21% tương ứng với BLEU-4 và EM. Bằng cách so sánh thêm tất cả kết quả cùng nhau, chúng tôi có thể quan sát rằng similarity đạt hiệu suất tốt nhất trong hầu hết các trường hợp. Cụ thể, nó đạt hiệu suất tốt nhất trong 62.96% (17/27) chỉ số và tác vụ. Tuy nhiên, chúng tôi cũng có thể quan sát rằng có một số trường hợp mà cả Similarity và Reverse Similarity đều hoạt động tệ hơn so với kết quả trung bình của việc sử dụng thứ tự ngẫu nhiên, cho thấy rằng các phương pháp sắp xếp minh chứng phức tạp hơn có thể được khám phá bởi công việc tương lai.

Phát hiện 4: Các thứ tự khác nhau của các ví dụ minh chứng có thể tác động đến hiệu suất ICL. Sắp xếp các ví dụ minh chứng dựa trên độ tương tự của chúng với mẫu kiểm tra theo thứ tự tăng dần có thể đạt kết quả tương đối tốt hơn trong hầu hết các trường hợp.

C. RQ3: Số lượng ví dụ minh chứng
1) Thiết lập thực nghiệm: Trong phần này, chúng tôi điều tra xem việc tăng số lượng ví dụ có cải thiện hiệu suất ICL trên các tác vụ trí tuệ mã hay không. Chúng tôi thay đổi số lượng ví dụ minh chứng từ 1 đến 64. Chúng tôi sử dụng BM-25 và Similarity làm phương pháp lựa chọn minh chứng và sắp xếp minh chứng, tương ứng, dựa trên các phát hiện trên.

2) Phân tích: Như được thể hiện trong Hình 4, chúng tôi có thể thấy rằng hiệu suất ICL trên tất cả các tác vụ tăng lên với số lượng ví dụ minh chứng ban đầu. Tuy nhiên, khi số lượng ví dụ trên 16, kết quả trên các tác vụ khác nhau cho thấy các xu hướng khác nhau. Ví dụ, đối với sửa lỗi, hiệu suất đạt đỉnh khi số lượng ví dụ minh chứng là 32 và chịu sự sụt giảm đáng kể khi tiếp tục tăng số lượng lên 64. Đối với tổng hợp chương trình, hiệu suất tiếp tục tăng và có xu hướng ổn định khi số lượng vượt quá 32. Chúng tôi tin rằng các xu hướng khác nhau được gây ra bởi vấn đề cắt cụt [59], [60]. Như minh họa trong Phần III-C, khi tăng số lượng ví dụ, độ dài của toàn bộ minh chứng sẽ tăng và các ví dụ có thể bị cắt bớt để tránh vượt quá giới hạn độ dài của LLM. Cụ thể, đối với tập dữ liệu B2Fsmall, tất cả các ví dụ đều hoàn chỉnh không bị cắt bớt khi số lượng ví dụ dưới 32. Tuy nhiên, khi số lượng trở thành 32, 2.33% ví dụ minh chứng bị cắt bớt. Khi

--- TRANG 7 ---
BẢNG VI: Kết quả thực nghiệm của các phương pháp sắp xếp minh chứng khác nhau.

Phương pháp    Tóm tắt mã (CSN)           Sửa lỗi (B2Fsmall)        Tổng hợp chương trình (CoNaLa)
               BLEU-4 ROUGE-L METEOR     BLEU-4 EM                  CB     SM     DM     EM

Random
Random         20.46  36.71   16.17      72.40  9.52               27.72  44.46  37.53  15.53
Similarity     21.04  37.86   16.26      72.02  9.93               28.47  44.87  37.79  16.00
Reverse Similarity 19.78 33.71 15.64     71.44  9.02               27.62  44.48  37.96  15.20

KmeansRND
Random         20.67  37.64   15.97      72.29  8.60               26.64  42.97  37.24  16.87
Similarity     20.69  37.62   16.05      72.90  10.15              27.20  42.97  36.93  16.40
Reverse Similarity 20.55 37.43 16.20     72.05  9.78               27.09  43.74  37.19  16.60

BM-25
Random         22.35  38.31   17.01      77.54  30.45              30.37  46.22  40.75  18.53
Similarity     22.23  38.12   17.01      77.76  30.95              30.83  46.41  41.33  17.60
Reverse Similarity 22.13 38.26 16.91     77.60  29.80              30.01  45.72  39.60  18.20

tiếp tục tăng số lượng lên 64, vấn đề cắt cụt xảy ra trên hơn 80% ví dụ và 44.32% ký tự trong những ví dụ đó bị loại bỏ, dẫn đến suy giảm hiệu suất nghiêm trọng. Vì độ dài của mẫu trong tập dữ liệu CSN và B2Fsmall lớn hơn nhiều so với tập dữ liệu CoNaLa, tức là 557, 492, 101 ký tự trên mỗi mẫu tương ứng cho CSN, B2Fsmall và CoNaLa, vấn đề cắt cụt không xuất hiện trong tổng hợp chương trình ngay cả khi số lượng tăng lên 64. Do đó, việc cân bằng số lượng ví dụ và vấn đề cắt cụt tiếp theo là quan trọng đối với ICL.

Vì mã thường dài hơn nhiều so với ngôn ngữ tự nhiên [35], vấn đề cắt cụt dễ xuất hiện hơn trong các tác vụ trí tuệ mã. Bên cạnh đó, nhiều ví dụ hơn cũng sẽ dẫn đến chi phí lớn hơn khi sử dụng API bên ngoài và thời gian suy luận [61]. Số lượng ví dụ nhỏ hơn có thể phù hợp hơn cho các tác vụ trí tuệ mã. Từ kết quả (Hình 4), chúng tôi cũng có thể thấy rằng hiệu suất với bốn ví dụ minh chứng đủ tốt, đạt 96.48%, 97.80% và 94.80% hiệu suất tốt nhất trên ba tác vụ tương ứng với EM, BLEU-4 và CodeBLEU. Do đó, xét đến sự cân bằng trên, việc sử dụng bốn ví dụ trong minh chứng là lựa chọn tốt cho các tác vụ trí tuệ mã.

Phát hiện 5: Nhiều ví dụ minh chứng hơn trong prompt không phải lúc nào cũng dẫn đến hiệu suất tốt hơn khi xét đến vấn đề cắt cụt. Để tiết kiệm chi phí, khuyến nghị sử dụng bốn ví dụ trong minh chứng.

D. RQ4: Khái quát hóa các phát hiện
1) Thiết lập thực nghiệm: Trong phần này, chúng tôi đánh giá tính khái quát hóa của các phát hiện trên các LLM khác nhau. Ngoài Codex, chúng tôi thử nghiệm trên hai LLM khác bao gồm GPT-3.5 [11] và ChatGPT [26]. Để xác thực phát hiện 1-4, chúng tôi thử nghiệm với các kết hợp sau của phương pháp lựa chọn và sắp xếp minh chứng: Random+Random, KmeansRND+Random, UniXcoder+Random, BM-25+Random và BM-25+Similarity. Đối với phát hiện 5 trong RQ3, chúng tôi sử dụng BM-25+Similarity làm phương pháp lựa chọn và sắp xếp và thay đổi số lượng ví dụ minh chứng từ 1 đến 128 để xác thực xem việc cắt cụt có dẫn đến suy giảm hiệu suất hay không. Do giới hạn chi phí, chúng tôi chọn tác vụ tổng hợp chương trình để đánh giá.

Chúng tôi cũng đo lường mức độ cải thiện mà các phát hiện của chúng tôi có thể mang lại bằng cách so sánh hiệu suất ICL với minh chứng được thiết kế cẩn thận, ICL với phương pháp xây dựng minh chứng được sử dụng rộng rãi [17], [19], [22] và ICL zero-shot. Trong minh chứng được thiết kế cẩn thận, chúng tôi sử dụng BM-25 và Similarity làm phương pháp lựa chọn và sắp xếp minh chứng và sử dụng bốn ví dụ minh chứng; trong khi đối với phương pháp xây dựng minh chứng baseline được sử dụng rộng rãi, chúng tôi sử dụng cài đặt trong công việc trước đây [17], [19], [22] và chọn ngẫu nhiên hai ví dụ minh chứng từ tập đào tạo với thứ tự ngẫu nhiên. Đối với ICL zero-shot, như minh họa trong phần II-B, không có ví dụ minh chứng nào được sử dụng và mô hình dự đoán chỉ dựa trên hướng dẫn.

2) Phân tích: Chúng tôi trình bày kết quả trung bình và CV của GPT-3.5 và ChatGPT trong Bảng VII. Trong Hình 5 và Hình 6, chúng tôi trình bày phân phối hiệu suất của các nhóm ví dụ khác nhau và tác động của số lượng ví dụ trên hai LLM này, tương ứng. Sự so sánh của các minh chứng khác nhau được thể hiện trong Bảng VIII. Do giới hạn không gian, chúng tôi chỉ trình bày hiệu suất trên EM và CB và kết quả trên các chỉ số khác có thể được tìm thấy trong gói tái tạo của chúng tôi. Từ những kết quả này, chúng tôi có thể quan sát rằng các phát hiện của chúng tôi cũng có thể được áp dụng cho GPT-3.5 và ChatGPT.

Như được thể hiện trong Bảng VII và Hình 6, chúng tôi có thể quan sát rằng KmeansRND+Random không chỉ vượt trội hơn Random+Random về kết quả trung bình, mà còn có phân phối dự đoán ổn định hơn đối với các nhóm ví dụ khác nhau. Lấy GPT-3.5 làm ví dụ, KmeansRND+Random cải thiện Random+Random 6.24% và 10.39% tương ứng với CB và EM. Điều này cho thấy rằng tính đa dạng cũng có lợi cho việc xây dựng minh chứng của hai mô hình này (phát hiện 1). Tương tự, bằng cách so sánh BM-25+Random và UniXcoder+Random, chúng tôi cũng có thể thấy rằng BM-25 có thể đạt hiệu suất tương tự và thậm chí vượt trội hơn UniXcoder trên GPT-3.5 lần lượt 2.50% và 4.88% tương ứng với CB và EM. Điều này cho thấy rằng BM-25 cũng là phương pháp lựa chọn minh chứng đơn giản và hiệu quả trong hai mô hình này (phát hiện 2). Bên cạnh đó, trên GPT-3.5 và ChatGPT, minh chứng cấp instance cũng liên tục vượt trội hơn minh chứng cấp tác vụ và đạt CV thấp hơn đối với các thứ tự khác nhau nói chung. Điều này cho thấy rằng việc chọn ví dụ minh chứng theo độ tương tự cũng có lợi cho hai LLM này (phát hiện 3). Đối với tác động của thứ tự ví dụ, chúng tôi cũng có thể thấy rằng BM-25+Similarity liên tục cải thiện BM-25+Random trên tất cả chỉ số và LLM, ví dụ như cải thiện EM trung bình 5.56% và 5.42% tương ứng trên GPT-3.5 và ChatGPT (phát hiện 4). Đối với tác động của số lượng, chúng tôi có thể quan sát các xu hướng tương tự trên GPT-3.5 và ChatGPT trong Hình 6, EM đầu tiên tăng với số lượng ví dụ minh chứng. Khi số lượng tiếp tục tăng lên 128, 25.05% ví dụ gặp phải vấn đề cắt cụt, dẫn đến suy giảm đột ngột (phát hiện 5).

Bảng VIII hiển thị sự so sánh của các minh chứng khác nhau. Chúng tôi cũng có thể quan sát rằng hiệu suất của ICL zero-shot rất kém trên tất cả các tác vụ, điều này cho thấy tầm quan trọng của việc sử dụng các ví dụ minh chứng để hướng dẫn LLM hiểu tác vụ. Bên cạnh đó, bằng cách so sánh hiệu suất của minh chứng được thiết kế cẩn thận với minh chứng baseline, chúng tôi có thể thấy rằng ChatGPT với minh chứng được thiết kế cẩn thận vượt trội hơn minh chứng baseline ít nhất 10.59%, 294.57% và 51.06% tương ứng trên tóm tắt mã, sửa lỗi và tổng hợp chương trình với BLEU-4, EM và EM. Kết quả cho thấy tầm quan trọng của việc xây dựng minh chứng tốt, và tính khái quát hóa của các phát hiện.

--- TRANG 8 ---
20 21 22 23
1 2 4 8 16 32 64
Số lượng ví dụ
BLEU-4 CSN
(a) Tóm tắt mã.

25 27 29 31 33
1 2 4 8 16 32 64
Số lượng ví dụ
Exact Match (%)
B2Fsmall
(b) Sửa lỗi.

10 13 16 19 22
1 2 4 8 16 32 64
Số lượng ví dụ
CoNaLa
Exact Match (%)
(c) Tổng hợp chương trình.

Hình 4: Kết quả thực nghiệm của ICL với số lượng ví dụ minh chứng khác nhau.

BẢNG VII: Thí nghiệm khái quát hóa các phát hiện trên GPT3.5 và ChatGPT.

Phương pháp    CB       EM
               Avg CV   Avg CV
Lựa chọn Thứ tự

GPT-3.5
Random   Random      26.60 3.01  12.32 4.73
KmeansRND Random     28.26 1.93  13.60 1.65
UniXcoder Random     30.06 0.53  13.73 1.13
BM-25    Random      30.81 1.05  14.40 1.81
BM-25    Similarity  30.69 0.00  15.20 0.00

ChatGPT
Random   Random      28.17 1.98  11.88 4.24
KmeansRND Random     28.25 2.31  12.92 1.78
UniXcoder Random     29.33 1.85  14.32 2.87
BM-25    Random      28.95 5.75  13.47 1.82
BM-25    Similarity  30.03 0.00  14.20 0.00

Random KmeansRND
11.0 12.0 13.0 14.0 15.0
Exact Match(%)
(a) GPT-3.5.

Random KmeansRND
11.0 11.5 12.0 12.5 13.0 13.5
(b) ChatGPT.

Hình 5: So sánh phân phối hiệu suất của Random và KmeansRND đối với các nhóm ví dụ khác nhau trên GPT-3.5 và ChatGPT.

1 1 1 3 1 5 1 7
1 2 4 8 16 32 64 128
Số lượng ví dụ
Exact Match (%)
(a) GPT-3.5.

3 6 9 12 15
1 2 4 8 16 32 64 128
Exact Match (%)
Số lượng ví dụ
(b) ChatGPT.

Hình 6: Kết quả thực nghiệm của số lượng ví dụ minh chứng khác nhau trên GPT-3.5 và ChatGPT.

V. THẢO LUẬN
A. Ý nghĩa của các phát hiện
Trong phần này, chúng tôi thảo luận về ý nghĩa của công việc chúng tôi đối với các nhà nghiên cứu và nhà phát triển.

Các nhà nghiên cứu: Nghiên cứu của chúng tôi chứng minh rằng hiệu suất học trong ngữ cảnh few-shot phụ thuộc rất nhiều vào thiết kế minh chứng. Với các minh chứng được xây dựng tốt, ICL có thể đạt hiệu suất tốt hơn nhiều. Kết quả thực nghiệm của chúng tôi cũng cho thấy các hướng nghiên cứu tiềm năng trong thời đại LLM và ICL cho cộng đồng trí tuệ mã. Cụ thể:

--- TRANG 9 ---
BẢNG VIII: So sánh các phương pháp xây dựng minh chứng khác nhau trên ba LLM.

Phương pháp    Tóm tắt mã (CSN)        Sửa lỗi (B2Fsmall)    Tổng hợp chương trình (CoNaLa)
               BLEU-4 ROUGE-L METEOR  BLEU-4 EM            CB    SM    DM    EM

Codex
Zero-shot      1.82   4.27    4.19    34.65  1.43         8.71  9.26  23.81 0.20
Minh chứng baseline  17.37  32.04   13.43   69.07  9.70   27.54 44.56 37.07 14.20
Minh chứng được thiết kế cẩn thận  22.73  39.52   17.35   77.54  32.25  32.07 48.03 42.88 21.40

GPT-3.5
Zero-shot      6.34   15.05   14.08   2.81   0.15         0.06  0.26  0.00  0.20
Minh chứng baseline  14.55  21.53   13.81   62.87  9.15   26.36 36.94 41.67 10.00
Minh chứng được thiết kế cẩn thận  15.99  26.78   16.70   71.70  25.25  30.69 43.95 44.78 15.20

ChatGPT
Zero-shot      3.63   11.40   13.16   2.32   0.05         25.70 37.64 54.44 3.40
Minh chứng baseline  10.76  20.02   14.83   41.57  4.60   27.62 41.83 46.85 9.40
Minh chứng được thiết kế cẩn thận  11.90  23.31   16.93   53.92  18.15  30.03 45.04 44.26 14.20

• Như được thể hiện trong kết quả RQ1, các mô hình truy xuất mã tiên tiến hiện tại vẫn có khoảng cách lớn với Oracle, cho thấy rằng các mô hình này thất bại trong việc chọn các ví dụ có độ tương tự ngữ nghĩa cao nhất. Do đó, các mô hình biểu diễn mã hiệu quả cho tìm kiếm mã-đến-mã zero-shot đáng được nghiên cứu. Bên cạnh đó, việc thiết kế chiến lược lựa chọn ví dụ dựa trên kiến thức tiên nghiệm của từng tác vụ hoặc thuộc tính của mã nguồn cũng là hướng thú vị đáng khám phá.

• Đặt các ví dụ tương tự ở cuối tất cả các ví dụ dẫn đến hiệu suất tương đối tốt hơn so với đặt ngẫu nhiên và đặt ngược lại. Tuy nhiên, sự cải thiện như vậy không nhất quán. Do đó, cách tự động thiết kế phương pháp sắp xếp tốt hơn cho các tác vụ trí tuệ mã cần được điều tra thêm.

• Khác với văn bản ngôn ngữ tự nhiên, độ dài của đoạn mã thường dài hơn nhiều. Điều này giới hạn số lượng ví dụ trong prompt và có thể mang lại chi phí tính toán và thời gian lớn cho LLM. Do đó, việc kết hợp các kỹ thuật cắt và giảm chương trình vào ICL để giảm chi phí đáng được điều tra.

Các nhà phát triển: Học trong ngữ cảnh là một mô hình cho phép học từ một vài ví dụ trong prompt mà không cần cập nhật tham số. Phương pháp mới này cũng đã thu hút cộng đồng language-model-as-a-service. Các phát hiện của chúng tôi cho thấy rằng việc lựa chọn, thứ tự và số lượng ví dụ minh chứng có tác động đáng kể đến hiệu suất ICL cho các tác vụ trí tuệ mã. Dựa trên các phát hiện của chúng tôi, chúng tôi kết luận các cái nhìn và lời khuyên sau cho các nhà phát triển sử dụng LLM trong công việc của họ:

• Bao gồm các ví dụ minh chứng trong prompt, giúp mô hình hiểu tác vụ và hướng dẫn định dạng đầu ra.

• Sử dụng phương pháp truy xuất để chọn ví dụ minh chứng khi có tập đào tạo được gán nhãn. Đối với các phương pháp truy xuất, hãy xem xét sử dụng BM-25 vì nó là phương pháp đơn giản nhưng hiệu quả.

• Cải thiện tính đa dạng của các ví dụ minh chứng cấp tác vụ với clustering để có được dự đoán chính xác và ổn định hơn.

• Khi sắp xếp thứ tự các ví dụ minh chứng, đặt các mẫu tương tự ở cuối danh sách là lựa chọn tốt trong hầu hết các trường hợp.

• Sử dụng càng nhiều ví dụ minh chứng càng tốt, nhưng cần chú ý đến giới hạn độ dài tối đa để tránh vấn đề cắt cụt. Để tiết kiệm chi phí, cũng khuyến nghị sử dụng bốn ví dụ trong minh chứng.

B. Mối đe dọa đến tính hợp lệ
Chúng tôi xác định ba mối đe dọa chính đến tính hợp lệ của nghiên cứu:

1) Rò rỉ dữ liệu tiềm ẩn. Trong bài báo này, chúng tôi tiến hành thí nghiệm bằng cách sử dụng API của OpenAI Codex, GPT-3.5 và ChatGPT. Tuy nhiên, vì chúng là các mô hình nguồn đóng, các tham số và tập đào tạo của chúng không có sẵn công khai, điều này gây lo ngại về rò rỉ dữ liệu tiềm ẩn. Cụ thể, có khả năng mô hình đã được đào tạo trên tập kiểm tra và chỉ ghi nhớ kết quả thay vì dự đoán chúng. Tuy nhiên, chúng tôi có thể quan sát từ các thí nghiệm của mình rằng hiệu suất của mô hình trong cài đặt zero-shot là thảm khốc, cho thấy xác suất thấp của việc ghi nhớ trực tiếp tập dữ liệu. Hơn nữa, tất cả các thí nghiệm trong bài báo của chúng tôi đều được tiến hành bằng cách sử dụng các mô hình này và chúng tôi sử dụng cải thiện hiệu suất tương đối để đo lường hiệu quả của các chiến lược xây dựng minh chứng khác nhau. Do đó, các phát hiện của bài báo chúng tôi vẫn có sức thuyết phục.

2) Lựa chọn tác vụ. Trong nghiên cứu này, chúng tôi điều tra việc xây dựng minh chứng trên ba tác vụ đại diện bao gồm tóm tắt mã, sửa lỗi và tổng hợp chương trình. Các tác vụ này bao gồm các loại khác nhau như Code→Text, Code+Text→Code và Text→Code. Do đó, chúng tôi tin rằng phát hiện của bài báo chúng tôi có thể khái quát hóa cho một loạt rộng các tác vụ trí tuệ mã. Trong tương lai, chúng tôi dự định tiến hành thí nghiệm trên các loại tác vụ khác như tác vụ Code→Class (ví dụ, phát hiện lỗ hổng) và tác vụ Code→Code (ví dụ, dịch mã).

3) Lựa chọn mô hình. Trong bài báo này, chúng tôi chọn ba LLM cho thí nghiệm. Tuy nhiên, có các LLM khác có sẵn, như CodeGen [62] và CodeGeeX [63]. Trong tương lai, chúng tôi dự định tiến hành thí nghiệm trên một loạt LLM rộng hơn để xác minh tính khái quát hóa của các phát hiện.

4) Lựa chọn ngôn ngữ. Đối với mỗi tác vụ, chúng tôi chọn một tập dữ liệu phổ biến để đánh giá. Tập dữ liệu của ba tác vụ chỉ chứa hai ngôn ngữ lập trình, tức là Java và Python. Trong tương lai, chúng tôi sẽ xác thực hiệu quả của các phương pháp xây dựng minh chứng trong các ngôn ngữ khác.

VI. CÔNG VIỆC LIÊN QUAN
A. Mô hình được đào tạo trước của mã
Gần đây, với sự phát triển của các kỹ thuật được đào tạo trước, các mô hình được đào tạo trước của mã đã được sử dụng rộng rãi và đạt hiệu suất tốt nhất trên nhiều tác vụ kỹ thuật phần mềm khác nhau. Một mô hình như vậy là CodeBERT [6], là mô hình được đào tạo trước chỉ encoder trên sáu ngôn ngữ lập trình với hai tác vụ tự giám sát. Mô hình khác, CodeT5 [9] là mô hình được đào tạo trước encoder-decoder tuân theo cùng kiến trúc như T5. CodeGPT [64] là mô hình chỉ decoder được đào tạo trước trên tập dữ liệu ngôn ngữ lập trình và có cùng kiến trúc như GPT-2. PLBART [8] sử dụng đào tạo trước chuỗi-đến-chuỗi khử nhiễu cho cả mục đích hiểu và tạo chương trình. UniXCoder [48] liên quan đến học tương phản đa phương thức và mục tiêu tạo chéo phương thức để học biểu diễn của các đoạn mã.

Ngoài các mô hình được đào tạo trước nhỏ hơn này trong giới học thuật, nhiều mô hình mã được đào tạo trước với kích thước lớn hơn nhiều đã được đề xuất trong công nghiệp trong những năm gần đây. Codex [2] là mô hình mã được đào tạo trước lớn được đề xuất bởi OpenAI hỗ trợ dịch vụ Copilot. Ngoài Codex, các mô hình được phát hành gần đây bởi OpenAI, như ChatGPT [26] và GPT-4 [23], cũng được đào tạo trước trên dữ liệu mã nguồn

--- TRANG 10 ---
và thể hiện khả năng lập trình ấn tượng. AlphaCode [25] được đào tạo để tạo mã cho các cuộc thi lập trình như Codeforces, sử dụng dữ liệu 715G và 41B tham số. CodeGen [62] là mô hình được đào tạo trước lớn cho tổng hợp chương trình đa lượt với hơn 16B tham số, trong khi CodeGeeX [63] là mô hình tạo mã đa ngôn ngữ nguồn mở được đề xuất gần đây với 13B tham số.

B. Học trong ngữ cảnh
Các mô hình ngôn ngữ lớn đã cách mạng hóa xử lý ngôn ngữ tự nhiên (NLP) trong những năm gần đây. Dựa trên dữ liệu đào tạo trước lớn và kích thước mô hình, LLM cho thấy các khả năng nổi bật ấn tượng chưa được quan sát trong các mô hình nhỏ [10]. Brown et al. [11] đầu tiên cho thấy rằng GPT-3 có khả năng học từ một vài ví dụ trong ngữ cảnh mà không cần cập nhật tham số. Liu et al. [20] đầu tiên khám phá việc chọn các neighbor gần nhất làm ví dụ trong ngữ cảnh. Gần đây, Levy et al. [65] đề xuất cải thiện tính đa dạng của các ví dụ trong ngữ cảnh và đạt hiệu suất tốt hơn trên các tác vụ khái quát hóa tổng hợp NLP. Lu et al. [21] thấy rằng thứ tự của các ví dụ trong ngữ cảnh có tác động lớn đến hiệu suất và đề xuất hai phương pháp LocalE và GlobalE dựa trên entropy. Gần đây, một loạt công việc [15], [66] tập trung vào các tác vụ suy luận phức tạp và đề xuất prompt chuỗi suy nghĩ bằng cách hướng dẫn mô hình xuất ra đường dẫn suy luận của nó.

Ngoài NLP, đã có sự quan tâm ngày càng tăng trong việc áp dụng học trong ngữ cảnh cho các tác vụ trí tuệ mã [17], [19], [22], [47], [67], [68]. Ví dụ, Xia et al. [17] đánh giá hiệu quả của LLM trong sửa chữa chương trình. Nashid et al. [47] đề xuất sử dụng BM-25 để truy xuất các ví dụ tương tự và xây dựng minh chứng cho tạo assert và sửa chữa chương trình. Tuy nhiên, các công việc này chủ yếu tập trung vào việc đánh giá LLM trên một hoặc hai tác vụ và không thảo luận sâu về việc xây dựng minh chứng trong ngữ cảnh. Ngược lại, công việc của chúng tôi nhằm tiến hành nghiên cứu có hệ thống về việc thiết kế minh chứng tốt hơn cho ICL trong các tác vụ trí tuệ mã.

VII. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Trong bài báo này, chúng tôi điều tra thực nghiệm tác động của các phương pháp lựa chọn minh chứng khác nhau, các phương pháp sắp xếp minh chứng khác nhau và số lượng ví dụ minh chứng đối với hiệu suất học trong ngữ cảnh cho các tác vụ trí tuệ mã. Nghiên cứu của chúng tôi chứng minh rằng minh chứng được thiết kế cẩn thận cho ICL vượt trội hơn các minh chứng đơn giản rất nhiều. Chúng tôi tóm tắt các phát hiện và cung cấp các đề xuất để giúp các nhà nghiên cứu và nhà phát triển xây dựng minh chứng tốt hơn cho các tác vụ trí tuệ mã. Trong tương lai, chúng tôi sẽ khám phá thêm các khía cạnh khác của mã nguồn về hiệu suất học trong ngữ cảnh như chất lượng của mã và tính tự nhiên của mã. Ngoài ra, chúng tôi cũng sẽ tiếp tục xác minh các phát hiện của mình trên các mô hình ngôn ngữ lớn khác. Mã nguồn và kết quả thực nghiệm đầy đủ của chúng tôi có sẵn tại https://github.com/shuzhenggao/ICL4code.

TÀI LIỆU THAM KHẢO
[1] Z. Zeng, H. Tan, H. Zhang, J. Li, Y. Zhang, và L. Zhang, "An extensive study on pre-trained models for program understanding and generation," trong ISSTA '22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Event, South Korea, July 18 - 22, 2022. ACM, 2022, pp. 39–51.
[2] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., "Evaluating large language models trained on code," CoRR, vol. abs/2107.03374, 2021.
[3] W. U. Ahmad, S. Chakraborty, B. Ray, và K. Chang, "A transformer-based approach for source code summarization," trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. Association for Computational Linguistics, 2020, pp. 4998–5007.
[4] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, và D. Poshyvanyk, "On learning meaningful code changes via neural machine translation," trong Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019. IEEE / ACM, 2019, pp. 25–36.
[5] P. Yin và G. Neubig, "A syntactic neural model for general-purpose code generation," trong Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers. Association for Computational Linguistics, 2017, pp. 440–450.
[6] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, và M. Zhou, "Codebert: A pre-trained model for programming and natural languages," trong Findings of the Association for Computational Linguistics: EMNLP 2020, ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 1536–1547.
[7] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, và Z. Jin, "Summarizing source code with transferred API knowledge," trong Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden. ijcai.org, 2018, pp. 2269–2275.
[8] W. U. Ahmad, S. Chakraborty, B. Ray, và K. Chang, "Unified pre-training for program understanding and generation," trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. Association for Computational Linguistics, 2021, pp. 2655–2668.
[9] Y. Wang, W. Wang, S. R. Joty, và S. C. H. Hoi, "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation," trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021. Association for Computational Linguistics, 2021, pp. 8696–8708.
[10] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., "Emergent abilities of large language models," Trans. Mach. Learn. Res., vol. 2022, 2022.
[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[12] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., "Palm-e: An embodied multimodal language model," CoRR, vol. abs/2303.03378, 2023.
[13] J. Devlin, M. Chang, K. Lee, và K. Toutanova, "BERT: pre-training of deep bidirectional transformers for language understanding," trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 2019, pp. 4171–4186.

--- TRANG 11 ---
[14] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, và Z. Sui, "A survey for in-context learning," arXiv preprint arXiv:2301.00234, 2022.
[15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, và D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," trong NeurIPS, 2022.
[16] Y. Hu, C. Lee, T. Xie, T. Yu, N. A. Smith, và M. Ostendorf, "In-context learning for few-shot dialogue state tracking," trong Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022. Association for Computational Linguistics, 2022, pp. 2627–2643.
[17] C. S. Xia, Y. Wei, và L. Zhang, "Automated program repair in the era of large pre-trained language models," trong 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2023, pp. 1482–1494.
[18] Y. Peng, S. Gao, C. Gao, Y. Huo, và M. R. Lyu, "Domain knowledge matters: Improving prompts with fix templates for repairing python type errors," CoRR, vol. abs/2306.01394, 2023.
[19] J. A. Prenner, H. Babii, và R. Robbes, "Can openai's codex fix bugs?: An evaluation on quixbugs," trong 3rd IEEE/ACM International Workshop on Automated Program Repair, APR@ICSE 2022, Pittsburgh, PA, USA, May 19, 2022. IEEE, 2022, pp. 69–75.
[20] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, và W. Chen, "What makes good in-context examples for gpt-3?" trong Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022. Association for Computational Linguistics, 2022, pp. 100–114.
[21] Y. Lu, M. Bartolo, A. Moore, S. Riedel, và P. Stenetorp, "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity," trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Association for Computational Linguistics, 2022, pp. 8086–8098.
[22] J. Y. Khan và G. Uddin, "Automatic code documentation generation using GPT-3," trong 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022. ACM, 2022, pp. 174:1–174:6.
[23] OpenAI, "GPT-4 technical report," CoRR, vol. abs/2303.08774, 2023.
[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, và I. Polosukhin, "Attention is all you need," trong Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, 2017, pp. 5998–6008.
[25] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., Science, vol. 378, no. 6624, pp. 1092–1097, 2022.
[26] ChatGPT, "Chatgpt," https://chat.openai.com/, 2022.
[27] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, và G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," ACM Comput. Surv., vol. 55, no. 9, pp. 195:1–195:35, 2023.
[28] C. Wang, Y. Yang, C. Gao, Y. Peng, H. Zhang, và M. R. Lyu, "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence," trong Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022. ACM, 2022, pp. 382–394.
[29] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan, và S. Gelly, "Parameter-efficient transfer learning for NLP," trong Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, vol. 97. PMLR, 2019, pp. 2790–2799.
[30] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, và W. Chen, "Lora: Low-rank adaptation of large language models," trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
[31] X. L. Li và P. Liang, "Prefix-tuning: Optimizing continuous prompts for generation," trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021. Association for Computational Linguistics, 2021, pp. 4582–4597.
[32] O. Rubin, J. Herzig, và J. Berant, "Learning to retrieve prompts for in-context learning," trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022. Association for Computational Linguistics, 2022, pp. 2655–2671.
[33] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin, R. Zhang, M. Ostendorf, L. Zettlemoyer, N. A. Smith, và T. Yu, "Selective annotation makes language models better few-shot learners," 2023.

--- TRANG 12 ---
[34] S. Gao, H. Zhang, C. Gao, và C. Wang, "Keeping pace with ever-increasing data: Towards continual learning of code intelligence models," trong 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2023, pp. 30–42.
[35] H. Husain, H. Wu, T. Gazit, M. Allamanis, và M. Brockschmidt, "Codesearchnet challenge: Evaluating the state of semantic code search," CoRR, vol. abs/1909.09436, 2019.
[36] E. Shi, Y. Wang, L. Du, J. Chen, S. Han, H. Zhang, D. Zhang, và H. Sun, "On the evaluation of neural code summarization," trong 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2022, pp. 1597–1608.
[37] K. Papineni, S. Roukos, T. Ward, và W. Zhu, "Bleu: a method for automatic evaluation of machine translation," trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA. ACL, 2002, pp. 311–318.
[38] C.-Y. Lin, "ROUGE: A package for automatic evaluation of summaries," trong Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74–81.
[39] S. Banerjee và A. Lavie, "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments," trong Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005. Association for Computational Linguistics, 2005, pp. 65–72.
[40] S. Gao, C. Gao, Y. He, J. Zeng, L. Nie, X. Xia, và M. R. Lyu, "Code structure-guided transformer for source code summarization," ACM Trans. Softw. Eng. Methodol., vol. 32, no. 1, pp. 23:1–23:32, 2023.
[41] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, và D. Poshyvanyk, "An empirical study on learning bug-fixing patches in the wild via neural machine translation," ACM Trans. Softw. Eng. Methodol., vol. 28, no. 4, pp. 19:1–19:29, 2019.
[42] S. Chakraborty và B. Ray, "On multi-modal learning of editing source code," trong 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November 15-19, 2021. IEEE, 2021, pp. 443–455.
[43] S. Chakraborty, T. Ahmed, Y. Ding, P. T. Devanbu, và B. Ray, "Natgen: generative pre-training by "naturalizing" source code," trong Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022. ACM, 2022, pp. 18–30.
[44] P. Yin, B. Deng, E. Chen, B. Vasilescu, và G. Neubig, "Learning to mine aligned code and natural language pairs from stack overflow," trong Proceedings of the 15th International Conference on Mining Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018. ACM, 2018, pp. 476–486.
[45] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan, M. Zhou, A. Blanco, và S. Ma, "Codebleu: a method for automatic evaluation of code synthesis," CoRR, vol. abs/2009.10297, 2020.
[46] Z. Cheng, T. Xie, P. Shi, C. Li, R. Nadkarni, Y. Hu, C. Xiong, D. Radev, M. Ostendorf, L. Zettlemoyer, N. A. Smith, và T. Yu, "Binding language models in symbolic languages," 2023.
[47] N. Nashid, M. Sintaha, và A. Mesbah, "Retrieval-based prompt selection for code-related few-shot learning," trong 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), 2023, pp. 2450–2462.
[48] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, và J. Yin, "Unixcoder: Unified cross-modal pre-training for code representation," trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022. Association for Computational Linguistics, 2022, pp. 7212–7225.
[49] D. Arthur và S. Vassilvitskii, "k-means++: the advantages of careful seeding," trong Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007. SIAM, 2007, pp. 1027–1035.
[50] J. Zhang, X. Wang, H. Zhang, H. Sun, và X. Liu, "Retrieval-based neural source code summarization," trong ICSE '20: 42nd International Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020. ACM, 2020, pp. 1385–1397.
[51] B. Wei, Y. Li, G. Li, X. Xia, và Z. Jin, "Retrieve and refine: Exemplar-based neural comment generation," trong 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020. IEEE, 2020, pp. 349–360.
[52] N. Reimers và I. Gurevych, "Sentence-bert: Sentence embeddings using siamese bert-networks," trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. Association for Computational Linguistics, 2019, pp. 3980–3990.
[53] Y. Luan, J. Eisenstein, K. Toutanova, và M. Collins, "Sparse, dense, and attentional representations for text retrieval," Trans. Assoc. Comput. Linguistics, vol. 9, pp. 329–345, 2021.
[54] Sentence-transformers, "st-codesearch-distilroberta," https://huggingface.co/flax-sentence-embeddings/st-codesearch-distilroberta-base, 2021.
[55] E. Shi, Y. Wang, W. Gu, L. Du, H. Zhang, S. Han, D. Zhang, và H. Sun, "Cocosoda: Effective contrastive learning for code search," pp. 2198–2210, 2023.
[56] Gensim, "Gensim package," https://github.com/RaRe-Technologies/gensim, 2010.
[57] C. E. Brown, Coefficient of Variation. Springer Berlin Heidelberg, 1998, pp. 155–157.
[58] M. Wei, N. S. Harzevili, Y. Huang, J. Wang, và S. Wang, "CLEAR: contrastive learning for API recommendation," trong 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022. ACM, 2022, pp. 376–387.
[59] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, và R. Salakhutdinov, "Transformer-xl: Attentive language models beyond a fixed-length context," trong Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers. Association for Computational Linguistics, 2019, pp. 2978–2988.
[60] A. Bulatov, Y. Kuratov, và M. Burtsev, "Recurrent memory transformer," trong Advances in Neural Information Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave, và K. Cho, Eds., 2022.
[61] OpenAI-pricing, "Openai-pricing," https://openai.com/pricing, 2022.
[62] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, và C. Xiong, "Codegen: An open large language model for code with multi-turn program synthesis," arXiv preprint arXiv:2203.13474, 2022.
[63] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li et al., "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x," CoRR, vol. abs/2303.17568, 2023.
[64] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., "Codexglue: A machine learning benchmark dataset for code understanding and generation," trong Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, J. Vanschoren và S. Yeung, Eds., 2021.
[65] I. Levy, B. Bogin, và J. Berant, "Diverse demonstrations improve in-context compositional generalization," trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 2023, pp. 1401–1422.
[66] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, và Y. Iwasawa, "Large language models are zero-shot reasoners," trong NeurIPS, 2022.
[67] T. Ahmed và P. T. Devanbu, "Few-shot training llms for project-specific code-summarization," trong 37th IEEE/ACM International Conference on Automated Software Engineering, ASE 2022, Rochester, MI, USA, October 10-14, 2022. ACM, 2022, pp. 177:1–177:5.
[68] G. Poesia, A. Polozov, V. Le, A. Tiwari, G. Soares, C. Meek, và S. Gulwani, "Synchromesh: Reliable code generation from pre-trained language models," trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
