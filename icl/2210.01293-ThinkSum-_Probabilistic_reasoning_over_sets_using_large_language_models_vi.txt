# ThinkSum: Suy luận xác suất trên tập hợp sử dụng các mô hình ngôn ngữ lớn

Batu Ozturkler
Đại học Stanford
Stanford, California, USA
ozt@stanford.edu

Nikolay Malkin
Mila, Université de Montréal
Montréal, Québec, Canada
nikolay.malkin@mila.quebec

Zhen Wang
Đại học bang Ohio
Columbus, Ohio, USA
wang.9215@osu.edu

Nebojsa Jojic
Microsoft Research
Redmond, Washington, USA
jojic@microsoft.com

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLM) có khả năng đáng kể trong việc suy luận tương tự ở mức cao: tái tạo các mẫu trong văn bản tuyến tính xuất hiện trong dữ liệu huấn luyện của chúng (đánh giá zero-shot) hoặc trong ngữ cảnh được cung cấp (học in-context few-shot). Tuy nhiên, các nghiên cứu gần đây cho thấy rằng ngay cả những LLM tiên tiến hơn cũng thất bại trong các tình huống đòi hỏi suy luận trên nhiều đối tượng hoặc sự thật và đưa ra các chuỗi suy diễn logic. Chúng tôi đề xuất một paradigm suy luận xác suất hai giai đoạn, ThinkSum, suy luận trên các tập hợp đối tượng hoặc sự thật theo cách có cấu trúc. Trong giai đoạn đầu tiên (Think - truy xuất các liên kết), một LLM được truy vấn song song trên một tập hợp các cụm từ được trích xuất từ prompt hoặc một lời gọi mô hình phụ trợ. Trong giai đoạn thứ hai (Sum - suy luận xác suất hoặc lý luận), các kết quả của những truy vấn này được tổng hợp để đưa ra dự đoán cuối cùng. Chúng tôi chứng minh các khả năng và ưu điểm của ThinkSum trên bộ công cụ đánh giá LLM BIG-bench, đạt được cải thiện so với hiện trạng tốt nhất sử dụng các mô hình họ GPT trên mười ba nhiệm vụ khó, thường với các biến thể mô hình nhỏ hơn nhiều. Chúng tôi cũng so sánh và đối chiếu ThinkSum với các sửa đổi được đề xuất khác cho việc prompting trực tiếp các LLM, chẳng hạn như các biến thể của chain-of-thought prompting. Kết quả của chúng tôi cho thấy rằng vì suy luận xác suất trong ThinkSum được thực hiện bên ngoài các lời gọi đến LLM, ThinkSum ít nhạy cảm hơn với thiết kế prompt, mang lại các dự đoán có thể giải thích được hơn, và có thể được kết hợp linh hoạt với các mô hình biến tiềm ẩn để trích xuất kiến thức có cấu trúc từ các LLM. Nhìn chung, paradigm được đề xuất của chúng tôi đại diện cho một cách tiếp cận đầy hứa hẹn để nâng cao khả năng suy luận của các LLM.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM; Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) có thể nhớ lại một loạt rộng các sự thật cơ bản, nhận dạng và bắt chước các dạng khác nhau trong ngôn ngữ, và ngoại suy hiệu quả các phép tương tự trong cấu trúc và ý nghĩa. Những khả năng này cho phép các LLM vượt trội trong các nhiệm vụ zero-shot và few-shot được xây dựng như việc sinh ra hoặc lựa chọn một phần hoàn thành có khả năng cao cho một prompt. Việc xây dựng này đòi hỏi các LLM thực hiện tư duy liên kết nhanh, trong đó mỗi token văn bản trong chuỗi tạo nên câu trả lời được sinh ra hoặc được chấm điểm trong một lần thông qua mô hình và, ngoài ra, không có thông tin trung gian nào được tạo ra hoặc giữ lại. Tư duy nhanh này được thực hiện bởi việc nén thông tin được lặp lại theo nhiều cách khác nhau trong các tập dữ liệu huấn luyện lớn, trong các trọng số của LLM.

Tuy nhiên, ngày càng rõ ràng rằng khi cần suy luận, hoặc tư duy chậm, các chế độ thất bại của LLM được tiết lộ. Trong cách sử dụng của chúng tôi, suy luận đề cập đến việc thao tác tuần tự các khái niệm có thể được biểu đạt bằng ngôn ngữ. Các nhiệm vụ đòi hỏi truy xuất lặp đi lặp lại kiến thức ít khi được nêu, tính không chắc chắn trên nhiều đối tượng hoặc sự thật, hoặc nhiều bước suy diễn là khó khăn ngay cả đối với những LLM tiên tiến nhất (Suzgun et al., 2022). Trong một bộ đánh giá được thiết kế gần đây, BIG-bench (Srivastava et al., 2022), một số nhiệm vụ mà khoảng cách giữa hiệu suất máy và con người là lớn bao gồm các chuỗi suy luận với các phản thực lồng nhau (LOGICAL DEDUCTION), các khái niệm được giới thiệu thông qua các định nghĩa (CONCEPTUAL COMBINATIONS), v.v. (xem Hình B.1). Đây là những nhiệm vụ mà cảm giác trực quan của người giải về '(không) nhất quán' là không đủ để tạo ra câu trả lời đúng, và một chuỗi các suy nghĩ, cùng với việc sử dụng các kết quả trung gian, có thể cần thiết để đi đến giải pháp, đặc biệt khi bộ nhớ làm việc không đủ.

Chúng tôi cho thấy một số nhiệm vụ trong BIG-bench có thể được giải quyết bằng một cơ chế hai thành phần, mà chúng tôi đặt tên là ThinkSum¹:

¹ThinkSum được đặt tên theo tương tự với các thuật toán khác
