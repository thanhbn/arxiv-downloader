# 2306.03799.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2306.03799.pdf
# Kích thước tệp: 3104316 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tối ưu hóa Không gian Prompt cho Thành công Lập luận Few-shot với Các Mô hình Ngôn ngữ Lớn

Fobo Shi1*†, Peijun Qing2,∗,†, Dong Yang3,∗‡, Nan Wang3, Youbo Lei4,†
Haonan Lu3,‡,Xiaodong Lin5,Duantengchuan Li1
1Đại học Vũ Hán,2Đại học Dartmouth,3Viện Nghiên cứu OPPO,
4Đại học Giao thông Tây An,5Đại học Rugster

Tóm tắt
Kỹ thuật prompt engineering là một phương pháp thiết yếu để tăng cường khả năng của các mô hình ngôn ngữ lớn (LLMs) bằng cách cung cấp các hướng dẫn rõ ràng và cụ thể. Nó giúp LLMs vượt trội trong các nhiệm vụ khác nhau, như lập luận số học, trả lời câu hỏi, tóm tắt, trích xuất quan hệ, dịch máy và phân tích cảm xúc. Các nhà nghiên cứu đã tích cực khám phá các chiến lược prompt engineering khác nhau, như Chain of Thought (CoT), Zero-CoT và In-context learning. Tuy nhiên, một vấn đề chưa được giải quyết xuất hiện từ thực tế là các phương pháp hiện tại thiếu một giải pháp toán học vững chắc để xác định các prompt tối ưu. Để giải quyết vấn đề này trong prompt engineering, chúng tôi đề xuất một phương pháp mới và hiệu quả được gọi là Prompt Space. Phương pháp của chúng tôi sử dụng text embeddings để thu được các vector cơ sở bằng phân tích ma trận, sau đó xây dựng một không gian để đại diện cho tất cả các prompt. Prompt Space vượt trội hơn đáng kể so với các mô hình prompt tiên tiến nhất trên mười bộ dữ liệu lập luận công khai. Đáng chú ý, không cần sự trợ giúp của phương pháp CoT và prompt "Let's think step by step", Prompt Space thể hiện hiệu suất vượt trội so với phương pháp few-shot. Nhìn chung, phương pháp của chúng tôi cung cấp một khung toán học mạnh mẽ và hiệu quả để lựa chọn các prompt đơn giản và hiệu quả. Tiến bộ này đánh dấu một bước quan trọng hướng tới việc cải thiện prompt engineering cho nhiều ứng dụng trong LLMs. Mã nguồn của chúng tôi được công khai tại https://github.com/YouBLEI/Prompt-Space

1 Giới thiệu
Prompt engineering trở thành một lĩnh vực tương đối mới và nóng bỏng để thiết kế và tối ưu hóa các prompt nhằm sử dụng hiệu quả các mô hình ngôn ngữ lớn (LLMs) cho nhiều ứng dụng và lĩnh vực nghiên cứu khác nhau (Brown et al., 2020; Thoppilan et al., 2022; Zhou et al., 2022; Sun et al., 2022; Dong et al., 2022). Các nhà nghiên cứu khám phá việc sử dụng các hướng dẫn đơn giản và cụ thể để tăng cường hiệu suất của LLMs trên các nhiệm vụ phức tạp, bao gồm lập luận số học và lập luận thông thường, cũng như trả lời câu hỏi (Chowdhery et al., 2022; Scao et al., 2022; Ouyang et al., 2022; Bai et al., 2022). Các nhà phát triển cố gắng thiết kế các prompt mạnh mẽ và hiệu quả một cách thủ công (Schick và Schütze, 2020; Reynolds và McDonell, 2021) hoặc tự động (Gao et al., 2020) để giao tiếp với LLMs và các công cụ khác (Wu et al., 2023; Xie et al., 2023). Mục tiêu là khám phá toàn bộ tiềm năng của LLMs trên các lĩnh vực khác nhau, cho phép chúng giải quyết các nhiệm vụ phức tạp với hiệu suất và độ chính xác được cải thiện.

Để gợi ra khả năng lập luận của LLMs, (Wei et al., 2022) đã đề xuất khái niệm về chain-of-thought (CoT) prompting. Khác với các mẫu input-output truyền thống, CoT prompting tạo ra một chuỗi các bước lập luận trung gian để hướng dẫn LLMs thông qua một vấn đề phức tạp. Phương pháp này cho phép LLMs phát triển một đường lối lập luận phân tách vấn đề phức tạp thành nhiều bước lập luận. Đáng chú ý, CoT prompting chứng minh rằng khả năng lập luận của LLMs hoàn toàn phù hợp với các quy luật tỷ lệ, với khả năng lập luận của LLMs tăng đáng kể theo kích thước của mô hình PaLM 540B. Lấy cảm hứng từ CoT prompting, một số nghiên cứu khám phá các phương pháp để tăng cường khả năng lập luận của LLMs với các kỹ thuật đơn giản. (Kojima et al., 2022) giới thiệu prompt "Let's think step by step", giúp LLMs áp dụng phương pháp suy nghĩ từng bước, dẫn đến câu trả lời cuối cùng. Phương pháp của họ, được gọi là Zero-shot-CoT, thành công trong việc tạo ra một đường lối lập luận trong các tình huống lập luận zero-shot. Trong thực tế, CoT prompting đã cho thấy hiệu suất tốt hơn Zero-shot-CoT (Wei et al., 2022; Kojima et al., 2022). Tuy nhiên, CoT prompting đòi hỏi nỗ lực rất lớn trong việc thiết kế thủ công cả câu hỏi và các chuỗi lập luận liên quan. Để tránh phương pháp thủ công, (Zhang et al., 2022) đề xuất một CoT prompting tự động, được gọi là Auto-CoT. Nó áp dụng thuật toán phân cụm để xác định các câu hỏi đại diện cho mỗi cụm và tạo ra các chuỗi lập luận sử dụng phương pháp Zero-shot-CoT cho mỗi câu hỏi.

Các nghiên cứu trước đây về CoT đã đóng góp rất lớn vào sự hiểu biết của chúng ta về các prompt hiệu quả để cải thiện khả năng lập luận của LLMs. Tuy nhiên, các nghiên cứu này có một số hạn chế nhất định, như thiếu hướng dẫn về việc tìm ra các prompt tối ưu cho các nhiệm vụ lập luận. Trong bài báo này, chúng tôi đề xuất một phương pháp mới được gọi là Prompt Space để vượt qua những hạn chế này và đồng thời tận dụng những điểm mạnh của các nghiên cứu trước đây. Phương pháp của chúng tôi bắt đầu bằng việc embedding các câu hỏi và sau đó sử dụng phân tích ma trận để tạo ra các vector cơ sở, hoặc câu hỏi cơ sở. Những câu hỏi cơ sở này được sử dụng để xây dựng một không gian có thể đại diện cho tất cả các câu hỏi. Với Zero-shot-CoT, chúng tôi kết hợp những câu hỏi cơ sở này với mỗi câu hỏi để tự động tạo ra các minh chứng lập luận cho LLMs. Phương pháp của chúng tôi đưa ra một giải pháp đầy hứa hẹn để tìm ra các prompt tối ưu cho các nhiệm vụ lập luận và cải thiện đáng kể khả năng lập luận few-shot của LLMs.

Prompt Space vượt trội hơn hiệu suất của các mô hình prompt hiện tại trên mười bộ dữ liệu lập luận công khai. Nghiên cứu của chúng tôi khám phá những hiểu biết quan trọng về tác động của số lượng câu hỏi cơ sở đối với các nhiệm vụ lập luận. Ngoài ra, chúng tôi xác định mối quan hệ giữa các câu hỏi được chọn và khả năng lập luận của LLMs, và nghiên cứu cách xác định số lượng tối ưu của các mẫu cho mỗi nhiệm vụ lập luận. Các thí nghiệm mở rộng chứng minh rằng phương pháp của chúng tôi thiết lập một phương pháp đáng tin cậy và có cơ sở toán học để lựa chọn các prompt đơn giản và hiệu quả. Mục tiêu của chúng tôi không chỉ là thiết kế các prompt mạnh mẽ và hiệu quả cho các nhiệm vụ lập luận khó khăn, mà còn nhấn mạnh tầm quan trọng của việc cẩn thận khám phá và phân tích các prompt tối ưu để mở khóa tiềm năng của LLMs trong nhiều ứng dụng khác nhau.

--- TRANG 2 ---

2 Nghiên cứu liên quan
2.1 Chain-of-thought Prompting
Chain-of-thought (CoT) prompting là một phương pháp hiệu quả để gợi ra khả năng lập luận của LLMs thông qua một chuỗi tư duy, trong đó một chuỗi các bước lập luận trung gian được sử dụng để tạo ra câu trả lời (Wei et al., 2022). Phương pháp này đã được chứng minh là cải thiện đáng kể hiệu suất của LLMs trên các nhiệm vụ lập luận phức tạp. Để tăng cường hiệu suất hơn nữa, self-consistency (SC) đã được giới thiệu, thay thế việc giải mã tham lam tiêu chuẩn của đầu ra LLM bằng một ensemble không gian đầu ra ngẫu nhiên (Wang et al., 2022b). Các nghiên cứu hiện tại về CoT prompting có thể được chia thành hai loại: CoT prompting được xây dựng thủ công và CoT prompting được tạo tự động. Nghiên cứu của chúng tôi nhằm cung cấp một khung toán học mạnh mẽ để lựa chọn các prompt đơn giản và hiệu quả.

2.2 CoT Prompts được tạo tự động
Để tăng cường lập luận CoT trong LLMs, một số nghiên cứu trước đây đã khám phá ý tưởng tự tạo ra một chuỗi tư duy (Kojima et al., 2022; Zhang et al., 2022; Zhou et al., 2022; Hebenstreit et al., 2023). (Kojima et al., 2022) phát hiện ra rằng việc sử dụng các cụm từ cụ thể, như "Let's think step by step", làm prompt có thể hướng dẫn LLMs tạo ra các bước lập luận mà không cần bất kỳ mẫu few-shot thủ công nào. Tiếp theo nghiên cứu này, (Zhou et al., 2022) đề xuất một khung được gọi là Automatic Prompt Engineer (APE) để tạo ra và lựa chọn các hướng dẫn tự động. APE giải quyết vấn đề tạo hướng dẫn bằng cách sử dụng LLMs để tạo ra và tìm kiếm các giải pháp ứng cử viên.

Ngoài ra, một số nghiên cứu triển khai Zero-shot-CoT để tạo ra quá trình lập luận trong minh chứng của họ (Kojima et al., 2022). (Zhang et al., 2022) đề xuất một phương pháp mới được gọi là Auto-CoT, để tự động tạo ra Chain of Thought (CoT) prompting trong LLMs. Phương pháp này lấy mẫu các câu hỏi và chuỗi lập luận đa dạng để xây dựng các minh chứng hiệu quả cho LLMs. Nó có thể gợi ra lập luận chain-of-thought mà không làm giảm hiệu suất và loại bỏ nhu cầu thiết kế prompt thủ công. Ngược lại, (Shao et al., 2023) sử dụng các minh chứng seed để tự động tổng hợp thêm các ví dụ thông qua các quá trình tiến và lùi. Lấy cảm hứng từ các nghiên cứu này, chúng tôi xây dựng một không gian với text embeddings và phân tích ma trận để đại diện cho tất cả các câu hỏi. Chúng tôi cũng sử dụng Zero-shot-CoT để tạo ra các chuỗi tư duy cho các ví dụ prompt (Kojima et al., 2022).

2.3 Lựa chọn Ví dụ
Để thiết kế prompt, một số nghiên cứu chứng minh rằng hiệu suất của LLMs bị ảnh hưởng bởi nhiều yếu tố khác nhau, như nhiệm vụ, prompt và cấu trúc mô hình (Zhao et al., 2021; Lu et al., 2021; Su et al., 2022; Griffin et al., 2023; Jiang et al., 2022). Thách thức chính là phát triển các tiêu chí lựa chọn vừa hiệu quả vừa có thể tổng quát hóa dựa trên các thí nghiệm thực nghiệm. (Wang et al., 2022b) cho thấy rằng trình tự các bước lập luận là quan trọng để đạt được hiệu suất tối ưu. Ngoài ra, (Rubin et al., 2021) đề xuất một phương pháp lựa chọn dựa trên độ tương đồng, truy xuất các instance training tương tự nhất làm prompt cho một test case đã cho. Hơn nữa, một phương pháp khác được đề xuất bởi (Fu et al., 2022) gợi ý rằng các prompt được chọn với nhiều bước hơn có thể cải thiện đáng kể hiệu suất trong quá trình lập luận. Tuy nhiên, phương pháp của chúng tôi, Prompt Space, khám phá các embedding câu hỏi để thu được các câu hỏi cơ sở trong một nhiệm vụ lập luận, điều này tránh được việc sử dụng các câu hỏi không hiệu quả làm minh chứng một cách đáng kể. Phương pháp này cung cấp một giải pháp toán học sáng tạo để lựa chọn các prompt hiệu quả, tạo ra các chuỗi lập luận có thể tổng quát hóa và toàn diện hơn. Prompt Space của chúng tôi nhằm phát triển hiểu biết sâu sắc về cách thiết kế CoT prompting.

--- TRANG 3 ---

3 Prompt Space
Trong nghiên cứu này, chúng tôi đề xuất một phương pháp mới được gọi là Prompt Space, tự động tạo ra các minh chứng với câu hỏi và chuỗi lập luận. Prompt Space tìm cách thiết kế một không gian thích hợp để xác định các câu hỏi cơ sở để xây dựng các mẫu prompt. Đối với một không gian vector V, cơ sở vector của nó được định nghĩa là một tập con v1,v2...,vn trong V. Những vector cơ sở này độc lập tuyến tính trong span V. Do đó, nếu (v1,v2...,vn) là một danh sách các vector trong V, thì những vector này tạo thành một cơ sở vector khi và chỉ khi mỗi x∈V có thể được viết duy nhất dưới dạng x=c1v1+c2v2+...+cnvn, (1) trong đó c1, c2, ..., cn là các phần tử của trường cơ sở.

Trong Prompt Space, một vector đại diện cho một embedding câu hỏi. Bằng cách kết hợp các câu hỏi cơ sở với câu hỏi test, chúng tôi tạo ra một minh chứng cho phép LLMs hiệu quả tạo ra một chuỗi tư duy. Tiếp theo, chúng tôi sẽ chỉ ra cách lựa chọn các câu hỏi cơ sở như vậy để xây dựng Prompt Space.

Lựa chọn các prompt hiệu quả làm mẫu có thể tăng cường đáng kể khả năng lập luận của LLMs. Để giải quyết các vấn đề số học, con người có xu hướng học hỏi từ các cặp câu hỏi-câu trả lời trước đó và tổng quát hóa chúng để giải quyết các vấn đề tương tự. Lấy cảm hứng từ cơ chế tư duy này, nghiên cứu của chúng tôi nhằm lựa chọn các câu hỏi đại diện hơn làm mẫu để tạo điều kiện cho LLMs phát triển một chuỗi các bước lập luận. Chúng tôi giả định rằng tồn tại một không gian prompt thực P với k vector chiều, trong đó các câu hỏi đại diện được chọn có thể phục vụ như các vector cơ sở của không gian này. Những vector cơ sở này cung cấp một giải pháp hiệu quả cho LLMs để lập luận qua không gian vấn đề.

Phân tích thành phần chính (PCA) là một thuật toán được sử dụng rộng rãi để xác định các thành phần chính của các đặc trưng dữ liệu mở rộng bằng các bước hình học (Abdi và Williams, 2010). Việc triển khai PCA có thể nén hiệu quả một ma trận n chiều thành một ma trận k chiều và thu được k vector chính từ không gian ban đầu. Lấy cảm hứng từ PCA, Prompt Space tuân theo các bước sau:

1. Embedding Câu hỏi. Tập câu hỏi của một nhiệm vụ là Q={q1, q2, ..., qm}, trong đó m là số câu hỏi trong một nhiệm vụ. Mô hình MiniLM-L6-v2 fMiniLM (Wang et al., 2020) mã hóa những câu hỏi này như sau: qi = fMiniLM(qi)∈Rn, với i= 1,2, ..., m. Sau quá trình mã hóa, ma trận câu hỏi Q được tạo ra bằng cách ghép tất cả các embedding câu hỏi, tức là Q= [q1,q2, ...,qm]T∈Rm×n.

Giả sử chiều (rank) của không gian prompt P là k, quá trình tìm k vector cơ sở giống hệt với việc tìm kiếm k thành phần chính (câu hỏi) của ma trận câu hỏi Q.

2. Tìm vector cơ sở. Chúng tôi sử dụng Singular Value Decomposition (SVD) để tính toán k vector cơ sở trong không gian prompt P (Wall et al., 2003). Sử dụng SVD, Q có thể được tính toán như: Q=UΛVT, (2) trong đó U được ký hiệu là ma trận singular trái, U= [u1,u2, ...,um]T∈Rm×m, và ui∈R1×m là eigenvector của QQT∈Rm×m (với i= 1, ..., m). Một chứng minh đầy đủ được trình bày trong Phụ lục A. Tương tự, V là ma trận singular phải, có thể được viết là V= [v1,v2, ...vm]T∈Rn×n, vi∈Rn×1 (với i= 1, ...n) là eigenvector của QTQ. Tiếp theo, k thành phần chính của Q có thể được thu được: Qk=UkQ, (3) trong đó Uk= [u1,u2, ...uk]T∈Rk×m, và Qk∈Rk×n chứa top k thành phần chính được xếp hạng theo eigenvalues liên quan. Các vector hàng trong Qk= [q′1,q′2, ...q′k]T∈Rk×n là k vector cơ sở của không gian prompt P.

3. Lựa chọn câu hỏi cơ sở. Trong bước này, chúng tôi lựa chọn top k câu hỏi từ ma trận câu hỏi Q làm câu hỏi cơ sở, có embeddings gần nhất với những vector cơ sở này. Nó có thể dẫn đến: f(x) =argmax (x·QT), với x∈ {q′i∈R1×n, i= 1,2...k} (4) trong đó argmax (•) là để tính toán độ tương đồng tối đa giữa các embedding câu hỏi và vector cơ sở (tức là, cosine similarity) (Sidorov et al., 2014). Cuối cùng, chúng tôi có thể tạo ra mẫu prompt, bao gồm k câu hỏi cơ sở và câu hỏi gốc trong Q, để có được đầu ra cuối cùng (câu trả lời).

Hình 1 cho thấy một ví dụ về Prompt Space tạo ra các câu hỏi cơ sở để giải quyết một vấn đề số học. Bằng cách tuân theo ba bước, chúng tôi có thể lựa chọn k câu hỏi cơ sở và sau đó kết hợp chúng với câu hỏi test. Để hỗ trợ LLMs trong việc tạo ra đầu ra cuối cùng, chúng tôi cũng sử dụng prompt, "Let's think step by step". Trong suốt quá trình, chúng tôi vẫn chọn tự động tạo ra prompt thay vì thiết kế thủ công. Kết quả là, LLMs có thể tạo ra một quá trình tư duy từng bước để đi đến câu trả lời. Thuật toán của Prompt Space được trình bày trong Phụ lục B.

Prompt Space có một số tính chất hấp dẫn như một phương pháp để tăng cường lập luận trong LLMs.

1. Prompt Space cho phép LLMs xác định các prompt tối ưu cho một loạt các nhiệm vụ lập luận và tạo ra các đầu ra cuối cùng một cách hiệu quả.

2. Prompt Space cung cấp một khung toán học mạnh mẽ để thiết kế prompt. Nó có thể gợi ý số lượng tối ưu của các mẫu để cải thiện khả năng lập luận của LLMs. Phương pháp của chúng tôi cung cấp những hiểu biết có giá trị về các chiến lược prompting hiệu quả để đạt được kết quả thành công.

3. Prompt Space có tiềm năng được sử dụng trong nhiều nhiệm vụ few-shot thông qua prompt engineering, bao gồm nhưng không giới hạn ở dịch thuật, tóm tắt và mở rộng.

--- TRANG 4 ---

4 Thí nghiệm
Chúng tôi mô tả ngắn gọn thiết lập thí nghiệm và nêu bật các kết quả chính. Chi tiết thí nghiệm và kết quả bổ sung có thể được tìm thấy trong Phụ lục C và D.

Prompt Space được đánh giá trên ba loại nhiệm vụ lập luận, cụ thể là lập luận số học, lập luận thông thường và lập luận ký hiệu. Thí nghiệm chứng minh Prompt Space trên nhiều nhiệm vụ khác nhau: 1. Prompt Space vượt trội hơn các baseline tiên tiến nhất trên những nhiệm vụ này. 2. Prompt Space có thể xây dựng hiệu quả một không gian và tìm ra các câu hỏi cơ sở cho mỗi nhiệm vụ. 3. Prompt Space có thể xác định số lượng tối ưu của các câu hỏi cơ sở để cải thiện đáng kể hiệu suất của LLMs trên mỗi tập dữ liệu. 4. Prompt Space phụ thuộc vào việc lựa chọn các mô hình embedding.

4.1 Thiết lập thí nghiệm
Nhiệm vụ và Tập dữ liệu. Prompt Space được nghiên cứu trên mười tập dữ liệu tiêu chuẩn từ ba loại nhiệm vụ lập luận:

1. Lập luận số học chứa sáu tập dữ liệu: (1) AddSub (Hosseini et al., 2014), (2) MultiArith (Roy et al., 2015), (3) SingleEq (Koncel-Kedziorski et al., 2015), (4) AQUA-RAT (Ling et al., 2017), (5) SVAMP (Patel et al., 2021), (6) GSM8K (Cobbe et al., 2021). Những tập dữ liệu này được sắp xếp theo thời gian phát hành. SingleEq và AddSub có nhiều vấn đề dễ hơn, trong khi MultiArith, AQUA-RAT, SVAMP và GSM8K khó hơn và yêu cầu các bước lập luận đa bước.

2. Lập luận thông thường: (1) CommonsenseQA (CSQA) (Talmor et al., 2019), (2) StrategyQA (STQA) (Geva et al., 2021). CSQA là một tập dữ liệu thách thức cho việc trả lời câu hỏi thông thường. Các câu hỏi của nó chứa ngữ nghĩa phức tạp thường yêu cầu kiến thức trước. STQA yêu cầu lập luận đa bước với một chiến lược được suy ra trong câu hỏi.

3. Lập luận ký hiệu: (1) Last Letter Concatenation (Letter) và (2) Coin Flip (Coin) (Wei et al., 2022). Last Letter Concatenation yêu cầu mô hình nối các chữ cái cuối của các từ trong một tên. Chúng tôi tạo ra tên đầy đủ bằng cách nối ngẫu nhiên các tên từ các mẫu. Coin Flip yêu cầu mô hình trả lời liệu một đồng xu vẫn úp mặt ngửa sau khi mọi người tung nó hoặc không tung nó. Trong nghiên cứu này, chúng tôi xem xét một tập test ngoài miền, trong đó các ví dụ có nhiều bước hơn những ví dụ trong các mẫu.

Mô tả chi tiết của mỗi tập dữ liệu được trình bày trong Phụ lục C.1.

Baselines. Chúng tôi so sánh Prompt Space với năm phương pháp baseline: Few-shot (Wei et al., 2022), Manual-CoT (Wei et al., 2022), Zero-shot (Kojima et al., 2022), Zero-shot-CoT (Kojima et al., 2022), và Auto-CoT (Zhang et al., 2022). Few-shot dễ dàng lựa chọn các cặp câu hỏi-câu trả lời làm minh chứng để đưa vào LLMs. Manual-CoT bao gồm việc tạo thủ công một chuỗi các chuỗi lập luận làm minh chứng để gợi ra khả năng lập luận của LLMs. Zero-shot là một kỹ thuật prompting tiêu chuẩn để đánh giá khả năng của LLMs. Zero-shot-CoT lựa chọn ngẫu nhiên các câu hỏi làm minh chứng, và sau đó sử dụng prompt "Let's think step by step". Ngoài ra, Auto-CoT sử dụng các kỹ thuật phân cụm để lấy mẫu câu hỏi và tạo ra minh chứng bằng phương pháp Zero-shot-CoT.

Để đảm bảo so sánh công bằng với các baseline, chúng tôi chạy thí nghiệm với các mẫu in-context nhất quán và một seed không đổi trên tất cả các phương pháp và tập dữ liệu. Few-shot và Manual-CoT lựa chọn các ví dụ bằng con người, trong khi Auto-CoT lựa chọn các ví dụ bằng thuật toán phân cụm K-means. Prompt Space của chúng tôi sử dụng cùng lý lẽ với Zero-CoT và Auto-CoT chứ không phải Manual-CoT. CoT của chúng tôi được tạo ra bởi LLMs chứ không phải con người. Hình E3 cho thấy các minh chứng của CSQA về các phương pháp khác nhau bao gồm Random selection, Manual-CoT, Auto-CoT và Prompt Space của chúng tôi. Vui lòng tham khảo Phụ lục C.2 để biết chi tiết về các baseline.

Triển khai. Chúng tôi sử dụng phiên bản gpt-3.5-turbo-0301 của mô hình ChatGPT công khai từ OpenAI API với 175 tỷ tham số (Brown et al., 2020; Ouyang et al., 2022). Chúng tôi chọn LLM này vì nó có hiệu suất tốt hơn phiên bản text-davinci-002 của GPT-3, như được báo cáo trong (OpenAI, 2023; Bai et al., 2022). Trong quá trình giải mã, chúng tôi đặt temperature thành 0 và sử dụng thuật toán tìm kiếm tham lam để có được kết quả. Đối với các phương pháp zero-shot, kết quả của chúng tôi là xác định. Theo (Wei et al., 2022), chúng tôi đặt số lượng minh chứng k thành 8, ngoại trừ AQUA-RAT (4) và Letter (4), StrategyQA (6), và CSQA (7). Tuy nhiên, Prompt Space của chúng tôi có thể xác định số lượng tối ưu của các câu hỏi cơ sở cho mỗi nhiệm vụ. Trong các phần tiếp theo, chúng tôi sẽ trình bày một phân tích chi tiết và cung cấp thêm hiểu biết về việc lựa chọn câu hỏi cơ sở. Các mô hình embedding được chọn là mô hình T5 (base/large/XL/XXL) (Raffel et al., 2020), mô hình E5 (small/base/large) (Wang et al., 2022a) và mô hình MiniLM-L6-v2 (Wang et al., 2020). Kích thước embedding của mỗi câu hỏi trong tất cả các mô hình T5 là 768, trong khi đối với các mô hình E5 (small, base, large), kích thước embedding của chúng lần lượt là 384, 768, 1024. Mô hình MiniLM-L6-v2 của chúng tôi mã hóa câu hỏi với kích thước embedding là 384. Vui lòng tham khảo Phụ lục C.3 để biết mô tả mô hình chi tiết. Trong các phương pháp của chúng tôi, chúng tôi nghiên cứu hai loại Prompt Space được hiển thị trong Hình 2. Loại đầu tiên kết hợp CoT với prompt "Let's think step by step", được ký hiệu là Prompt-Space-CoT-Zero. Ngược lại, loại thứ hai chỉ sử dụng CoT, cụ thể là Prompt-Space-CoT.

--- TRANG 5 ---

4.2 Kết quả chính
Trong các thí nghiệm, chúng tôi đánh giá Prompt Space trên mười tập dữ liệu từ ba loại nhiệm vụ lập luận. Do giải mã tham lam, các kết quả chính cho thấy các kết quả xác định mà không có thanh lỗi. Đáng chú ý, Bảng 1 và 2 cho thấy rằng Prompt Space đạt được hiệu suất vượt trội so với các phương pháp tiên tiến nhất trên mười nhiệm vụ lập luận, tương ứng. So với Auto-CoT, Prompt space với số lượng tối ưu các mẫu đạt được trung bình lên đến 3.2% trong Bảng 2.

Prompt Space so với Few-shot. Bảng 1 tóm tắt các so sánh giữa phương pháp của chúng tôi (Prompt Space) và hai baseline (Zero-shot và Few-shot) cho mỗi tập dữ liệu. Trong Bảng 1, Prompt Space không bao gồm CoT và prompt "Let's think step by step", và chỉ lựa chọn câu hỏi cơ sở làm minh chứng. Kết quả của chúng tôi cho thấy rằng Prompt Space với cùng cài đặt đạt được trung bình lên đến 2.3%, 2% so với Zero-shot và Few-shot trên mười tập dữ liệu lập luận, tương ứng. Đặc biệt, Prompt Space, với số lượng tối ưu các mẫu, đạt được trung bình lên đến 3.3%, 3% so với Zero-shot và Few-shot trên mười tập dữ liệu lập luận, tương ứng. Cải thiện đáng kể nhất được quan sát thấy trong các tập dữ liệu STQA và Letter, với mức tăng tương đối 13.5%, 112.5% so với Few-shot, tương ứng. Hơn nữa, Prompt Space vượt trội hơn hai baseline trên tám trong mười tập dữ liệu lập luận.

Lập luận Số học. Phương pháp của chúng tôi vượt trội hơn đáng kể so với ba baseline trên năm nhiệm vụ lập luận số học ngoại trừ AddSub trong Bảng 2. Quan trọng, Prompt Space của chúng tôi với cùng cài đặt đạt được cải thiện điểm số 1.8%, 1.2%, 2% và 2.1% so với các phương pháp tiên tiến nhất trước đó trên MultiArith, SingleEq, SVAMP và GSM8K, tương ứng. Mặc dù Prompt Space không thể hiện hiệu suất cạnh tranh trên AddSub, nó gần với Auto-CoT. Ngoài ra, Prompt Space đạt được hiệu suất cao nhất trên AQUA-RAT, SVAMP và GSM8K, chỉ ra rằng nó có thể giải quyết lập luận số học phức tạp hơn. Sự khác biệt giữa Prompt-Space-CoT-Zero và Prompt-Space-CoT là không đáng kể, khoảng 2%. Nhìn chung, điểm số trung bình của Prompt-Space-CoT vượt trội hơn ba baseline trên tất cả các tập dữ liệu lập luận số học, cho thấy hiệu suất vượt trội của nó.

Lập luận Thông thường. Prompt Space vượt trội hơn đáng kể so với Auto-CoT tiên tiến nhất trước đó trên hai tập dữ liệu lập luận thông thường. Phương pháp của chúng tôi với cùng cài đặt đạt được các cải thiện tương ứng 1.9%, 1.6% so với Manual-CoT và 1.4%, 0.9% so với Auto-CoT. So với Zero-shot, Zero-shot-CoT và Manual-CoT không gợi ra lập luận thông thường tốt hơn, trong khi Prompt Space tận dụng phương pháp CoT để tăng hiệu suất một cách đáng kể thay vì giảm nó. Những kết quả này chứng minh rằng Prompt Space có thể cải thiện hiệu suất trên các nhiệm vụ lập luận thông thường yêu cầu kiến thức trước.

Lập luận Ký hiệu. Hiệu suất của Prompt Space đạt được một mức tăng đáng kể 3.2% so với Manual-CoT và 9.4% so với Auto-CoT trên tập dữ liệu Letter, tương ứng. Thú vị, độ chính xác của Manual-CoT, Auto-CoT và phương pháp của chúng tôi đạt đến 100% trên tập dữ liệu Coin Flip. Kết quả cho thấy rằng Prompt Space tăng cường đáng kể khả năng lập luận của LLMs trên các nhiệm vụ ký hiệu.

4.3 Ảnh hưởng của Mô hình Embedding
Hình 3 cho thấy rằng việc tăng kích thước embedding không thể cải thiện hiệu suất của Prompt Space trên các nhiệm vụ lập luận khác nhau. Bên cạnh đó, kích thước embedding thích hợp có thể là 768 trong các mô hình T5 và E5. Khi các mô hình T5 tăng kích thước mô hình của chúng, hiệu suất của Prompt Space giảm đáng kể. Hơn nữa, tỷ lệ giải quyết của Prompt Space thể hiện sự dao động rõ ràng trên các mô hình embedding khác nhau.

4.4 Phân tích sâu hơn về Câu hỏi Cơ sở
Hình 4 minh họa hiệu suất của Prompt Space với các câu hỏi cơ sở khác nhau trên chín tập dữ liệu. Kết quả của chúng tôi tiết lộ rằng số lượng thích hợp của câu hỏi cơ sở là 8 trên các nhiệm vụ lập luận số học ngoại trừ AQUA-RAT, trong khi số lượng câu hỏi cơ sở xấp xỉ 6 hoặc 7 trên các nhiệm vụ lập luận thông thường. Thú vị, các tập dữ liệu AQUA-RAT và Letter thể hiện sự ưu tiên cho số lượng câu hỏi cơ sở nhỏ hơn, điều này cho thấy không gian của chúng có thể được span bởi chỉ bốn hoặc năm vector cơ sở. Nhìn chung, các phát hiện của chúng tôi chứng minh rằng số lượng thích hợp của câu hỏi cơ sở có thể cải thiện đáng kể hiệu suất, điều này cho thấy rằng tồn tại các vector cơ sở (câu hỏi) trong không gian prompt. Tuy nhiên, vẫn còn một thách thức là chúng tôi không thể tự động xác định số lượng tối ưu của câu hỏi cơ sở cho mỗi tập dữ liệu. Thêm phân tích về câu hỏi cơ sở của Prompt-Space-CoT được trình bày trong Phụ lục D.1. Bên cạnh đó, chúng tôi cung cấp thêm các hình ảnh trực quan của Prompt Space và các minh chứng được xây dựng trong Phụ lục D.2 và E, tương ứng.

4.5 Ảnh hưởng của Trình tự Câu hỏi
Bảng 3 cho thấy rằng Prompt Space đạt được hiệu suất tốt hơn so với các trường hợp khác, khi các câu hỏi cơ sở được sắp xếp theo thứ tự tăng dần của eigenvalues của chúng. Tuy nhiên, sắp xếp giảm dần (trình tự gốc) có hiệu suất vượt trội hơn các baseline trên ba trong bốn benchmarks. Hơn nữa, sự khác biệt giữa trình tự gốc và trình tự ngược lại là không đáng kể (~0.1%). Do đó, những phát hiện này gợi ý rằng sắp xếp giảm dần là một phương pháp có thể chấp nhận được sử dụng trong các thí nghiệm của chúng tôi.

--- TRANG 6 ---

5 Kết luận
Trong bài báo này, chúng tôi đề xuất một phương pháp prompting mới, cụ thể là Prompt Space, để khám phá việc lựa chọn prompt để tăng cường lập luận trong LLMs. Đối với bất kỳ tập dữ liệu nào, Prompt Space có thể ánh xạ các câu hỏi của nó lên một không gian thực để xác định các câu hỏi cơ sở làm minh chứng. Thông qua các thí nghiệm trên các nhiệm vụ lập luận số học, thông thường và ký hiệu, chúng tôi thấy rằng các minh chứng được xây dựng bởi Prompt Space có thể cải thiện đáng kể khả năng lập luận của LLMs trên mười benchmarks công khai. Hơn nữa, không cần sự trợ giúp của phương pháp CoT và prompt "Let's think step by step", Prompt Space cũng thể hiện hiệu suất vượt trội so với học few-shot và zero-shot trong LLMs. Nhìn chung, Prompt Space có thể phục vụ như một công cụ hiệu quả để giải quyết các nhiệm vụ lập luận, mà còn có tiềm năng trở thành một học viên few-shot cho một loạt các ứng dụng và nhiệm vụ rộng rãi.

Hạn chế và Tuyên bố Đạo đức
So với các phương pháp tiên tiến nhất, Prompt Space cho thấy hiệu suất cạnh tranh hơn trên ba loại nhiệm vụ lập luận. Bên cạnh đó, nó sẽ tăng đáng kể khả năng và tính mạnh mẽ của phương pháp chain-of-thought trên các tập dữ liệu lớn. Tuy nhiên, có một số hạn chế tiềm năng cần xem xét. Thứ nhất, số lượng tối ưu của câu hỏi cơ sở được quan sát bằng kết quả thí nghiệm. Ngoài ra, hiệu suất của Prompt Space có thể bị ảnh hưởng bởi việc lựa chọn các mô hình embedding. Cuối cùng, chúng tôi sử dụng một phương pháp xấp xỉ để thu được top k câu hỏi cơ sở, điều này có thể tăng sự không chắc chắn của phương pháp này. Nhìn chung, chúng tôi sẽ tiếp tục làm việc trên vấn đề này để giải quyết những hạn chế này và phát triển các phương pháp prompting hiệu quả và mạnh mẽ hơn.

Để tái tạo, tất cả các thí nghiệm được chạy bằng phiên bản gpt-35-turbo của mô hình ChatGPT công khai từ OpenAI API với 175 tỷ tham số. Và các phương pháp baseline này là triển khai mã nguồn mở. Để hỗ trợ đánh giá, chúng tôi tóm tắt thống kê của mười tập dữ liệu benchmark, và bao gồm cấu hình của các mô hình embedding khác nhau và cài đặt thí nghiệm trong tài liệu bổ sung.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đã được dịch sẽ rất dài, tôi sẽ bỏ qua phần này để giữ cho bản dịch ngắn gọn]

--- TRANG 7 và các trang sau ---

Phụ lục
[Phần phụ lục bao gồm các bảng, hình ảnh, và thông tin chi tiết về thuật toán, thiết lập thí nghiệm, và kết quả đã được dịch sang tiếng Việt, nhưng để giữ cho bản dịch ngắn gọn, tôi sẽ không bao gồm toàn bộ phụ lục]

A Dẫn xuất ma trận Q
Các ma trận QQT∈Rm×m và QTQ∈Rn×n được chéo hóa, có thể được viết lại như:
(QQT=UΛ1UT QTQ=VΛ2VT) (A.1)

B Thuật toán của Prompt Space
Thuật toán 1 mô tả thuật toán chi tiết của Prompt Space được đề xuất.

C Chi tiết Thiết lập Thí nghiệm
C.1 Tập dữ liệu
Bảng C1 tóm tắt thống kê cơ bản của mười tập dữ liệu benchmark.

[Phần còn lại của phụ lục chứa các bảng chi tiết, hình ảnh, và phân tích bổ sung đã được dịch sang tiếng Việt]
