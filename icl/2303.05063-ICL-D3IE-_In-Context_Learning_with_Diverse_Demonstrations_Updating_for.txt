# 2303.05063.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2303.05063.pdf
# File size: 4092406 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for
Document Information Extraction
Jiabang He1, Lei Wang2*, Yi Hu1, Ning Liu3, Hui Liu4, Xing Xu1‚Ä†, Heng Tao Shen1
1Center for Future Media & School of Computer Science and Engineering,
University of Electronic Science and Technology of China
2Singapore Management University ,3Beijing Forestry University
4Beijing Rongda Technology Co., Ltd.
Abstract
Large language models (LLMs), such as GPT-3 and Chat-
GPT, have demonstrated remarkable results in various natu-
ral language processing (NLP) tasks with in-context learn-
ing, which involves inference based on a few demonstration
examples. Despite their successes in NLP tasks, no inves-
tigation has been conducted to assess the ability of LLMs
to perform document information extraction (DIE) using
in-context learning. Applying LLMs to DIE poses two chal-
lenges: the modality and task gap. To this end, we propose
a simple but effective in-context learning framework called
ICL-D3IE , which enables LLMs to perform DIE with dif-
ferent types of demonstration examples. Specifically, we
extract the most difficult and distinct segments from hard
training documents as hard demonstrations for benefiting
all test instances. We design demonstrations describing
relationships that enable LLMs to understand positional
relationships. We introduce formatting demonstrations for
easy answer extraction. Additionally, the framework im-
proves diverse demonstrations by updating them iteratively.
Our experiments on three widely used benchmark datasets
demonstrate that the ICL-D3IE framework enables Davinci-
003/ChatGPT to achieve superior performance when com-
pared to previous pre-trained methods fine-tuned with full
training in both the in-distribution (ID) setting and in the
out-of-distribution (OOD) setting. Code is available at
https://github.com/MAEHCM/ICL-D3IE .
1. Introduction
The task of visually rich document understanding
(VRDU), which involves extracting information from
VRDs [ 2,19], requires models that can handle various types
of documents, such as voice, receipts, forms, emails, and
*Corresponding author. lei.wang.2019@phdcs.smu.edu.sg
‚Ä†Corresponding author. xing.xu@uestc.edu.cn
(a) Pretrained Document Understanding Models
(b) Large Language Models
OCRContext ùüè:{text:"TAX 5.4",Box :[11,12,32,44]}... Q:What
are the labels for these texts ? A:SUB_TOTAL.TAX_PRICE ,‚Ä¶
Context ùíè:{text:"J.S PR",Box :[13 525 469 555]}... Q:What
are the labels for these texts? A:MENU.NM ,...Context :{text
:"1X",Box:[11 
13 10 40]} ‚Ä¶
Q:What are 
the labels for 
these texts?+
...A:MENU.CNT,‚Ä¶
In-context example ùüè In-context example ùíè Test example
Test exampleB-MENU.CNT,‚Ä¶ OCR LaytoutLMv3
/LayoutLMv2
‚Ä¶GPT-3/ChatGPTFigure 1: Two approaches for solving the DIE task: (a)
previous pre-trained document understanding models [ 15,
42] fine-tuned with full training examples, and (b) in-context
learning over LLMs with a few examples.
advertisements, and various types of information, includ-
ing rich visuals, large amounts of text, and complex doc-
ument layouts [ 28,18,26]. Recently, fine-tuning based
on pre-trained visual document understanding models has
yielded impressive results in extracting information from
VRDs [ 41,13,22,23,15,21], suggesting that the use of
large-scale, unlabeled training documents in pre-training
document understanding models can benefit information
extraction from VRDs. As shown in Figure 13 (a), a pre-
trained model such as LayoutLMv3 [ 15] can predict labels
for entities in a test VRD.
Large language models (LLMs), such as GPT-3 [ 1],
OPT [ 45], and PaLM [ 5], develop quickly and have shown
1arXiv:2303.05063v4  [cs.CL]  21 Aug 2023

--- PAGE 2 ---
remarkable results in various natural language processing
(NLP) tasks. As LLMs grow in model parameters and train-
ing corpus size, they reveal emergent abilities that allow
them to learn to reason from just a few demonstration exam-
ples within a given context [ 38]. This paradigm of learning
is referred to as in-context learning (ICL) [ 8]. Recently,
approaches [ 43,14] have been proposed to explore how to
use LLMs to solve vision-language (VL) tasks. However,
to date, there has been no investigation into the ability of
LLMs to handle VRD understanding tasks, such as docu-
ment information extraction (DIE). Similar to VQA [ 12],
Two main challenges arise when applying LLMs to DIE:
the modality gap and the task gap, as LLMs cannot directly
process images and may lack training on layout information
in VRDs.
To address these challenges, one popular strategy in using
LLMs for the VQA task is to use demonstration QA pairs
and convert their corresponding images into image descrip-
tions through image caption models [ 43,14]. Subsequently,
the demonstration QA pairs and image descriptions are com-
bined as a prompt for the LLM to answer a test question.
Figure 13 (b) shows this straightforward strategy to apply
LLMs to the DIE task. It first utilizes Optical Character
Recognition (OCR) tools to convert images of demonstra-
tion documents from the training data into textual contents
and corresponding entity bounding boxes. The converted
demonstrations with entity labels are then combined as a
prompt for LLMs to predict labels for entities in a test doc-
ument. However, this strategy may perform poorly, as it
ignores positional relationships among textual contents and
is sensitive to examples selected for demonstrations.
In this paper, we propose ICL-D3IE , a simple and ef-
fective in-context learning framework for LLMs to perform
the DIE task with various types of demonstration examples
within a given context. Our method constructs different types
of demonstrations based on three criteria: (1) the demon-
strations should benefit all test documents rather than just
a subset of them, (2) layout information must be included,
and (3) the demonstrations should predict labels in an easily
extractable format. To construct hard demonstrations for
the first criterion, we select challenging segments from the
training documents that are difficult for LLMs to accurately
predict entities. To construct layout-aware demonstrations
for the second criterion, we use a prompt question to di-
rect LLMs to describe positional relationships between tex-
tual content boxes in selected regions. To create formatting
demonstrations for the third criterion, we randomly choose
training segments to guide LLMs to predict labels in a de-
sired format for easy extraction. Furthermore, the framework
iteratively enhances diverse demonstrations by updating hard
demonstrations through in-context learning with previous
diverse demonstrations.
Experiments conducted on three widely used bench-mark datasets (FUNSD [ 18], CORD [ 28], and SROIE [ 16]),
demonstrate that ICL-D3IE allows LLMs to achieve DIE
performance that is superior or comparable to previous pre-
trained methods fine-tuned with full training samples when
tested in the in-distribution (ID) setting. For example, ICL-
D3IE with GPT-3 (97.88%) outperforms LayoutLMv3 base
(96.89%) on SROIE. Moreover, in the out-of-distribution
(OOD) setting, ICL-D3IE performs much better than previ-
ous pre-trained methods on all datasets, achieving superior
performance. Together, these remarkable results encourage
new ways to leverage LLMs for solving VRD-related tasks.
2. Related Work
Visually Rich Document Understanding (VRDU). The
research topic of VRDU has been a challenging area of
research for many years, with numerous named entity recog-
nition (NER) methods proposed based on neural networks,
such as recurrent neural networks [ 20]. However, most of
these methods only identify key information in plain text, ne-
glecting the visual and layout information present in the doc-
ument. To address this issue, convolutional and graph neural
networks have been introduced to model layout and semantic
information [ 46,24]. Recently, multimodal self-supervised
pre-training and fine-tuning have proven effective in visually
rich documents by modeling visual, layout, and textual infor-
mation [ 40,35,11,36,15]. Huang et al. [ 15] were inspired
by the Vision Transformer (ViT) [ 10] to use patch-level
embeddings to learn visual features in LayoutLMv3. DIE
involves automatically extracting information from VRDs.
The objective is to identify valuable information in these
complex documents and organize it in a format that can be
easily analyzed and used. The process of extracting infor-
mation from VRDs requires two essential steps: (1) text
detection and recognition in document images, and (2) entity
labeling of the recognized text. The first step falls under the
area of research known as optical character recognition. This
study focuses on the second step and mainly discusses how
to leverage GPT-3 to accurately label entities in recognized
text.
In-Context Learning. LLMs like GPT-3 [ 1], OPT [ 45],
and PaLM [ 5] demonstrate emergent abilities as model and
corpus sizes increase [ 38]. These abilities are learned from
demonstrations containing a few examples in the context,
which is known as in-context learning [ 8]. To enable rea-
soning in LLMs, [ 39] propose Chain-of-Thought (CoT)
prompting, which adds multiple reasoning steps to the input
question. CoT prompting is a simple and effective few-
shot prompting strategy that improves LLMs‚Äô performance
on complex reasoning tasks. Several works [ 34,32,31]
have since aimed to improve CoT prompting in different
aspects, such as prompt format [ 4], prompt selection [ 25],
prompt ensemble [ 37], and problem decomposition [ 47].
While LLMs were originally developed for NLP tasks, re-
2

--- PAGE 3 ---
Figure 2: A detailed illustration of ICL-D3IE framework, including obtaining nearest neighbor documents for test samples
from the training dataset, constructing iteratively updated diverse demonstrations, and performing inference.
cent studies[ 43,3,44] have shown that LLMs with in-context
learning have few-shot or zero-shot abilities for multimodal
problems, including visual question answering tasks. Fur-
thermore, Frozen [ 33] demonstrates promising few-shot per-
formance using pre-trained models for vision-and-language
tasks. However, to our knowledge, our work is the first to
explore the use of LLMs with in-context learning for infor-
mation extraction from VRDs. You can refer to [ 9] for more
related works on in-context learning
3. Our ICL-D3IE Method
3.1. Preliminary of In-Context Learning
In-context learning enables LLMs to quickly adapt to
solve downstream tasks using just a few examples during
inference [ 1], requiring no training. In contrast, fine-tuning
LLMs necessitates training on as many samples as feasible,
resulting in redundant computation and time expenses. This
section describes how to formulate in-context learning for
solving the DIE task.
A data sample consists of a document image Iand its
corresponding entity labels Y={y1, y2, ..., y L}, where L
is the number of entities in the document. To obtain textual
contents and their corresponding boxes, we process the doc-
ument image Iusing an OCR tool. We denote the set of
textual contents as T={t1, t2, ..., t L}, where tlis a seg-
ment of words, and denote the set of their corresponding
boxes as B={b1, b2, ..., b L}, where blis the coordinates
pl
1, pl
2, pl
3, pl
4
‚ààZ4of the box bl. Note that the ordering of
Tis crucial because GPT-3 is sensitive to the permutation of
words. We follow the approach of XYLayoutLM [ 11] and
use the XYCut algorithm to determine the ordering of textual
regions. The DIE task (This paper considers the task of entity
labeling in VRDs) involves generating labels Yfor the given
entities Tin the document image Iby maximizing the condi-
tional probability as follows: p(Y|T) =1
LPL
lp(yl|tl).
While previous state-of-the-art studies [ 40,11] typicallyfine-tune pre-trained models to downstream tasks, this paper
proposes using LLMs with in-context learning to solve the
DIE task. Specifically, we define the probability of generat-
ing the target entity labels Yfor a given document image I
and in-context string Cusing a LLM Plmas follows:
p(Y|I, C) =LX
l=1Plm(V(yl)|C,T(I)). (1)
Here, T(¬∑)denotes a set of operations used to con-
vert the original document image into a text format as
GPT-3 desire, Cis the in-context examples obtained
by concatenating kinput-output demonstration examples
{(T(I1), Y1),(T(I2), Y2), . . . , (T(Ik), Yk)}, andVis an
operation for mapping an entity label ylto natural language
words that can be understood by GPT-3.
3.2. Overview Framework of ICL-D3IE
We present ICL-D3IE, a novel in-context learning frame-
work for tackling the DIE task, that enables GPT-3 to predict
entity labels in a test document based on different types of
demonstrations. Constructing demonstrations is designed to
satisfy three criteria: (i) the demonstrations should benefit all
test documents, not just a subset, (ii) they should include lay-
out information, which is essential for solving VRD-related
tasks., and (iii) they should predict entity labels in an easily
extracted and evaluated format.
The proposed ICL-D3IE framework involves four key
steps as shown in Figure 2. Firstly, the framework se-
lectsntraining documents most similar to the ntest doc-
uments. Secondly, ICL-D3IE constructs diverse demon-
strations based on the selected similar training documents.
These demonstrations include initial hard demonstrations for
criterion (i), layout-aware demonstrations for criterion (ii),
and formatting demonstrations for criterion (iii). Thirdly,
the framework iteratively updates the diverse demonstrations
by improving the hard demonstrations through in-context
3

--- PAGE 4 ---
There are four labels for selection, "question", "answer", "header", and "other".(a) Label mapping
(c)Layout -Aware Demonstrations
"0595 -529-1510 -0" is located on the below of "KOOL", so it can be labeled as "answer".Q1:{text: "KOOL ",Box :[54 41 100 54], entity:question }{text: "0595 -529-1510 -0",Box:[58 
55 193 72], entity:answer }‚Ä¶, Please describe the positional relationship of these texts?
A1:"KOOL" is located in the upper left corner, so it can be labeled as "question".Context:{ text:"SOLVENT",Box :[485 300 526 317], entity:header }{text:"‚òë",Box:[538 303 
552 317], entity:answer }‚Ä¶
Q3:{text:"TO:",Box:[102 345 129 359]}‚Ä¶.., What are the labels for these texts?
A3:{text:"TO:",Box:[102 345 129 359], entity:question }‚Ä¶..{text:"TO:",Box :[102 345 129 359]}
‚Ä¶Test Sample
Input TextTraining  Sample
(b)Hard Demonstrations
(d)Formatting Demonstrations
Q2:{text:"FULL",Box:[298 262 330 275 ]}, What are the labels for these texts?
A2:{text:"FULL",Box:[298 262 330 275], entity:question }{text:"KOOL",Box :[54 41 100 
54],entity:question }‚Ä¶Layout -Aware Text SegmentsHard Text Segments
{text:"SOLVENT",Box :[485 300 
526 317], entity:header }‚Ä¶
{text:"FULL",Box :[298 262 330 
275], entity:question }Formatting Text Segments
(a)&Q1
&A1
(a)&(b)
&(c)&
(d)&Q3
A3Large Language ModelFigure 3: Example of the input and output of in-context learning with diverse demonstrations. The text highlighted in blue
is not processed by LLMs, while the text highlighted in red is fed into LLMs. The green-highlighted text represents the
output of LLMs. The text in red represents the prediction made by the LLM. The final prompt comprises label mapping, hard
demonstrations, layout-aware demonstrations, formatting demonstrations, and a question prompt of ‚ÄúWhat are the labels for
these texts?‚Äù.
learning with previous diverse demonstrations. Lastly, ICL-
D3IE performs inference using in-context learning with the
updated diverse demonstrations.
3.3. Nearest Neighbor Document Selection
To facilitate effective in-context learning, the proposed
ICL-D3IE selects ntraining documents that are most similar
to the ntest documents. This process involves several steps.
Firstly, we leverage OCR tools to convert mtraining and n
test document images into plain text with corresponding box
information. Subsequently, the plain text is fed into Sentence-
BERT [ 30] to obtain document representations, and cosine
similarity scores are calculated to identify the most similar
training document for each test document. Finally, we can
identify ntraining documents that are the closest match to
thentest documents, which we refer to as nearest neighbor
documents Innd
1, Innd
2, . . . , Innd
n.
3.4. Diverse Demonstrations Construction
Once we have obtained nnearest neighbor documents
from the training dataset, we construct diverse demonstra-
tions for effective in-context learning. The standard approach
to constructing in-context demonstrations involves designing
a template for the target task to convert data examples into
texts that LLMs can process. Unlike standard in-context
learning that relies solely on task-specific demonstrations,
ICL-D3IE constructs diverse demonstrations for each test
instance: hard demonstrations that highlight challenging as-
pects of a task, layout-aware demonstrations that describe
the positional relationship between textual contents, and
formatting demonstrations that provide output formattingexamples.
Initial Hard Demonstrations. The first criterion for se-
lecting hard demonstrations is that they should highlight the
most challenging aspects of the DIE task to benefit all test
documents. The process of obtaining initial hard demon-
strations involves several steps. First, we use a zero-shot
prompting technique, which involves using a prompt such
as ‚ÄúWhat are the labels for these texts?‚Äù pt0to ask GPT-3
to predict labels for entities in Innd
i. Next, we calculate
entity-level F1 scores based on the predicted labels and the
corresponding ground truth labels. We then identify the text
segment thard with the lowest F1 scores from the nearest
neighbor documents. An initial hard demonstration can be
formulated as:
Chard,0= CONCAT( thard, bhard, pt0, yhard),(2)
where bhard andyhard are the box coordinate and answer of
the text segment thard, respectively.
Layout-Aware Demonstrations. The second criterion
necessitates the inclusion of layout information in the in-
context demonstrations, which is crucial for completing the
DIE task. To acquire demonstrations mindful of layout,
We randomly select adjacent hard segments obtained in the
construction of Chard,0to create a region Rlfor positional
relationship description. We use a prompt ‚ÄúPlease describe
the positional relationship of these texts‚Äù ptlto guide GPT-3
to generate a description Àúylof the positional relationship
between text segments in Rl. A layout-aware demonstration
can be formulated as:
Cl= CONCAT( Rl, Bl, ptl,Àúyl), (3)
4

--- PAGE 5 ---
where Blare the box coordinates for text segments of the
selected region Rl.
Formatting Demonstrations. The third criterion expects
to provide examples to guide GPT-3 to format the output for
the DIE task. To achieve this, we first randomly select a text
segment tffrom the nearest neighbor documents. Then, a
formatting demonstration Cfconsist of a text segment tf, its
corresponding box coordinate bf, the formatting prompt pt0,
and the ground truth answer yf, denoted as Cf:
Cf= CONCAT( tf, bf, pt0, yf). (4)
Label Mapping. The objective of label mapping is to
translate unnatural word labels to an answer space where
GPT-3 can effectively function as a predictive model. To
achieve this, we gather text descriptions of the original labels
from the provided datasets, such as ‚Äútotal. cash price‚Äù rep-
resenting ‚Äúthe amount paid in cash.‚Äù Then, we include the
original labels ( Y‚Ä≤) and their corresponding descriptions ( Y)
in the context before various demonstrations to prompt GPT-
3 to solve the test sample. Label Mapping for prompting can
be formulated as:
Cm= CONCAT( Y‚Ä≤, Y). (5)
3.5. Diverse Demonstrations Updating
To further highlight the most challenging aspects of the
DIE task, ICL-D3IE iteratively updates its diverse demon-
strations by improving hard demonstrations through in-
context learning with previous diverse demonstrations. Ini-
tial diverse demonstrations with initial hard demonstrations
Chard,0are used to perform inference for all nearest neighbor
documents Innd
1, Innd
2, . . . , Innd
n. Entity-level F1 scores are
computed for all entities, and the text segment with the low-
est F1 score is appended to the initial hard demonstrations
to obtain new hard demonstrations Chard,1. This process
is iterated ktimes to obtain final updated hard demonstra-
tionsChard,k, which are used to construct the final diverse
demonstrations.
3.6. Inference
After diverse demonstrations updating, the obtained di-
verse and comprehensive demonstrations can be used to
direct GPT-3 to perform the test, which is formulated as
follows:
p(Y|I, C) =1
LLX
l=1Plm(V(yl)|Cm, Chard,k, Cl, Cf,T(I)).
(6)
Finally, ICL-D3IE extracts the corresponding answers from
the generated predictions and then converts them into a suit-
able format for evaluation.4. Experiment
4.1. Experimental Setup
Datasets. We experiment on three widely used DIE
datasets. Here is a brief introduction to these datasets: The
FUNSD dataset [ 17] is a noisy scanned form understanding
dataset. It comprises 199 documents with varying layouts
and 9,707 semantic entity annotations in total. In our study,
we focus on the semantic entity labeling task, which involves
assigning labels such as ‚Äúquestion,‚Äù ‚Äúanswer,‚Äù ‚Äúheader,‚Äù or
‚Äúother‚Äù to each semantic entity. The training set comprises
149 samples, and the test set comprises 50 samples. The
CORD dataset [ 29] is a consolidated receipt understanding
dataset that includes 800 receipts for training, 100 receipts
for validation, and 100 receipts for testing. The labels in
this dataset have a hierarchy, comprising 30 semantic la-
bels under four categories. However, the labels are more
complex than those in the FUNSD dataset and require label
mapping. The SROIE dataset [ 16] is another receipt under-
standing dataset, comprising 973 receipts categorized into
four classes. The dataset includes 626 training images and
347 test images. The labels in this dataset are ‚Äúcompany,‚Äù
‚Äúdate,‚Äù ‚Äúaddress,‚Äù and ‚Äútotal.‚Äù
Baselines. We compare ICL-D3IE with three types of
baselines. The first type includes strong pre-trained models
fine-tuned with full training samples, while the second type
includes those fine-tuned with only a few samples. The third
type includes standard in-context learning, where one of its
demonstrations includes one document‚Äôs textual contents,
the corresponding box coordinates, the prompt question pt0,
and the corresponding ground truth answers.
For the text modality-based pre-trained baseline, we com-
pare our method to BERT [ 6]. For the text and layout modal-
ities based on pre-trained baselines, we employ LiLT [ 35]
and BROS [ 13]. LiLT uses a language-independent layout
transformer that decouples text and layout modalities. BROS
is a pre-trained key information extraction model that en-
codes relative layout information. Furthermore, we also con-
sider pre-trained baselines that utilize text, layout, and image
modalities, including LayoutLM [ 40], XYLayoutLM [ 11],
LayoutLMv2 [ 42], and LayoutLMv3 [ 15]. LayoutLM uses
two objectives to learn language representation during pre-
training and incorporates image information during the fine-
tuning phase. XYLayoutLM employs a preprocessing algo-
rithm called Augmented XY Cut to generate proper reading
orders. LayoutLMv2 uses CNN to encode document im-
ages and utilizes image information during the pre-training
stage. Lastly, LayoutLMv3 can model patch-level document
information.
Implementation Details. In our experiments, we use the
public GPT-3 text-davinci-003 (175B) and ChatGPT
gpt-3.5-turbo with the API1as the backbone language
1https://platform.openai.com/docs/models/gpt-3-5
5

--- PAGE 6 ---
Dataset FUNSD CORD SROIE
Setting ModelID OOD Average ID OOD Average ID OOD Average
F1‚Üë F1‚Üë F1‚Üë F1‚Üë F1‚Üë F1‚Üë F1‚Üë F1‚Üë F1‚Üë
Full-TrainingBERT BASE [7] 60.26 51.02 55.64 89.68 55.68 72.68 90.99 72.36 81.68
LiLT BASE [35] 88.41 64.29 76.35 96.07 73.32 84.70 94.68 74.29 84.49
BROS BASE [13] 83.05 68.72 75.89 95.73 71.24 83.49 95.48 75.51 85.50
XYLayoutLMBASE [11] 83.35 61.24 72.30 94.45 69.12 81.79 95.74 75.91 85.83
LayoutLMBASE [41] 79.27 54.38 66.83 91.06 70.13 80.60 94.38 76.24 85.31
LayoutLMv2BASE [42] 82.76 59.66 71.21 94.95 76.39 85.67 96.25 78.57 87.41
LayoutLMv3BASE [15] 90.29 73.24 81.77 96.56 75.23 85.90 96.89 78.34 87.62
Few-ShotBERT BASE [7] 38.76 19.68 29.22 38.88 15.31 27.10 38.76 20.56 59.32
LiLT BASE [35] 54.88 25.32 40.10 69.12 29.94 49.53 84.03 61.25 72.64
BROS BASE [13] 59.46 27.49 43.48 72.78 36.34 54.56 76.78 57.28 67.03
XYLayoutLMBASE [11] 65.44 30.56 48.00 69.16 32.19 50.68 75.66 56.23 65.95
LayoutLMBASE [41] 32.49 17.66 25.08 40.19 23.62 31.91 76.79 55.44 66.12
LayoutLMv2BASE [42] 71.42 49.12 60.27 65.71 29.43 47.57 81.81 59.56 70.69
LayoutLMv3BASE [15] 70.67 48.33 59.50 70.13 32.88 51.51 79.13 56.08 67.61
Standard ICL (ChatGPT) 72.76 69.32 71.04 68.34 65.68 67.01 82.11 80.31 81.21
Standard ICL (Davinci-003) 71.52 67.31 69.42 67.96 64.28 66.12 79.34 76.12 77.73
ICL-D3IE (ChatGPT) 83.66 79.05 81.36 87.13 70.69 78.91 92.63 86.31 89.47
ICL-D3IE (Davinci-003) 90.32 88.71 89.52 94.12 91.23 92.68 97.88 93.76 95.82
Table 1: Results of comparing ICL-D3IE with Standard ICL and existing pre-trained VDU models fine-tuned with full training
samples and a few samples on three benchmark datasets in ID and OOD settings.
models due to their popularity and accessibility. To ensure
consistent output, we set the temperature parameter to 0. For
evaluation, we employ the same metrics as in LayoutLMv3
and reported entity-level F1 for all methods. For our ICL-
D3IE method, we use 4 hard demonstrations, 4 layout-ware
demonstrations, and 4 formatting demonstrations. For the
fine-tuning-based baselines, we adopt the hyper-parameters
reported in their original papers. Note that our demonstra-
tions may be segments, and we use document examples that
include segments used in our method to fine-tune few-shot
baseline models to ensure a fair comparison. To demonstrate
the generalization ability of in-context learning over LLMs,
we generate out-of-distribution (OOD) test data for three
benchmark datasets using TextAttack [ 27]. The original test
data for these datasets are referred to as in-distribution (ID)
test data. Specifically, we replace original words with words
that are nearly identical in appearance yet different in mean-
ing and delete certain characters in words, such as ‚Äúname‚Äù
‚àí‚Üí‚Äúnme,‚Äù to generate OOD samples.
4.2. Main Results
Table 1 presents the performance comparison of ICL-
D3IE with existing full-training and few-shot baseline meth-
ods on both in-domain (ID) and out-of-domain (OOD) set-
tings. We can first observe that on the ID setting, ICL-D3IE
(Davinci-003) achieves a new state-of-the-art on FUNSD
and SROIE datasets with only a few data examples andwithout any training. It achieves 90.32% on FUNSD and
97.88% on SROIE, beating all other VDU achieving SOTA.
On the SROIE dataset, ICL-D3IE (Davinci-003) reaches
within 3% of the state-of-the-art performance, which is com-
parable to pre-trained VDU models that are fine-tuned with
full training samples. On the other hand, ICL-D3IE has
large performance gains for DIE in the few-shot setting. For
instance, in CORD, average performance more than doubled
for the VDU in the few-shot setting. Meanwhile, compared
to other full-training baselines, ICL-D3IE has greater ro-
bustness to OCR errors in document content on the OOD
settings, resulting in significantly better performance.
Moreover, we can see that ICL-D3IE outperforms Stan-
dard ICL on three datasets, with ICL-D3IE (Davinci-
003) showing an 18.8 F1 score improvement over
Standard ICL (Davinci-003) on FUNSD. We experi-
ment with GPT-3 ( text-davinci-003 ) and ChatGPT
(gpt-3.5-turbo ) to investigate the applicability of ICL-
D3IE with different backbone language models and find that
ICL-D3IE substantially improves the performance of Chat-
GPT compared with Standard ICL. However, ChatGPT gen-
eration‚Äôs flexibility makes answer extraction harder, result-
ing in slightly worse performance for ICL-D3IE (ChatGPT)
compared to ICL-D3IE (Davinci-003). These promising re-
sults demonstrate the effectiveness of ICL-D3IE for the DIE
task and its versatility across different backbone language
models.
6

--- PAGE 7 ---
FUNSD CORD SROIE
F1‚Üë F1‚Üë F1‚Üë
ICL-D3IE 90.32 94.12 97.88
w/o HD 78.20 87.13 89.13
w/o LD 87.25 84.13 96.83
w/o LM 89.63 87.94 97.19
w/o FD 88.73 93.07 90.58
Table 2: The effect of different components in ICL-D3IE.
HD means Hard Demonstrations. LD means Layout-Aware
Demonstrations. LM means Label Mapping. FD means
Formatting Demonstrations.
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000001b/uni00000014/uni0000001b/uni00000017/uni0000001b/uni0000001a/uni0000001c/uni00000013/uni0000001c/uni00000016/uni0000001c/uni00000019/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni0000002b/uni00000027
/uni0000002f/uni00000027
/uni00000029/uni00000027
(a)
/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000001b/uni00000017/uni0000001b/uni0000001a/uni0000001c/uni00000013/uni0000001c/uni00000016/uni0000001c/uni00000019/uni0000001c/uni0000001c/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000029/uni00000038/uni00000031/uni00000036/uni00000027
/uni00000026/uni00000032/uni00000035/uni00000027
/uni00000036/uni00000035/uni00000032/uni0000002c/uni00000028 (b)
Figure 4: Further analysis on (a) the effect of the number of
different demonstrations on CORD and (b) the effect of the
number of hard demonstrations updating.
Overall, our ICL-D3IE method shows consistent supe-
riority over other methods across all datasets and settings
except for the ID setting on CORD, suggesting its potential
to effectively solve VRD-related tasks using LLMs. These
remarkable results not only highlight the effectiveness of
ICL-D3IE but also inspire the development of novel meth-
ods with LLMs that require less manual effort.
4.3. Further Analysis
In this section, we conduct a detailed analysis of ICL-
D3IE and its components.
Effect of Different Components in Diverse Demonstra-
tions. ICL-D3IE‚Äôs demonstrations consist of four compo-
nents: hard demonstrations, layout-aware demonstrations,
formatting demonstrations, and label mapping. In this sec-
tion, we evaluate the impact of each component by remov-
ing one at a time and measuring the effect on ICL-D3IE
(Davinci-003) performance.
As shown in Table 2, removing any components drops
DIE performance. Removing hard demonstrations has the
most significant impact, indicating the effectiveness of it-
eratively updated hard demonstrations in benefiting all test
samples. Removing layout-aware demonstrations leads to a
drop of around 10 F1 score on CORD but little on SROIE
since CORD labels require more layout information than
SROIE. Removing label mapping results in a significant
drop in CORD due to its unnatural labels. ICL-D3IE‚Äôs per-formance without label mapping suggests formatting demon-
strations contribute to easier and better answer extraction.
Notably, ICL-D3IE (Davinci-003) outperforms Standard ICL
(Davinci-003) (Table 1), even with one component removed.
Overall, these results highlight the effectiveness of each
component in ICL-D3IE‚Äôs in-context demonstrations.
Effect of the Number of each Type of Demonstrations.
In Table 1, we set the number of different types of demon-
strations in ICL-D3IE to 4. However, varying the number of
each type of demonstration in the in-context diverse demon-
strations may result in varying performance outcomes. To
investigate this, we vary the number of a specific type of
demonstration from 0 to 4 while keeping the number of other
types of demonstrations constant at 4.
We present the F1 score of ICL-D3IE (Davinci-003) on
CORD in Figure 4a. We can observe that the number of
demonstrations of each type influences the performance of
ICL-D3IE. Besides, performance improves as the number of
any demonstration increases. Interestingly, we observe sig-
nificant changes in performance when varying the number of
hard and layout-aware demonstrations, suggesting that hard
demonstrations are beneficial for solving all test samples and
that the DIE task on CORD requires a substantial amount of
layout information to solve.
Effect of the Number of Hard Demonstrations Updat-
ing. This study aims to investigate the impact of the number
of Hard Demonstrations Updating on three different datasets.
As highlighted in Figure 4b, initial hard demonstrations can
help ICL-D3IE work very well, and hard demonstrations
after 20 iterations can achieve better performance. These
findings demonstrate that incorporating feedback from chal-
lenging aspects, as identified through predictions on training
data, to the prompt for LLMs is a useful strategy that can
benefit solving all test samples. Additionally, updating Hard
Demonstrations through in-context learning with previous
diverse demonstrations can enhance the performance of ICL-
D3IE (Davinci-003).
Effect of Ordering of Diverse Demonstrations . This
study investigates the impact of the different ordering of
demonstrations on ICL-DI3E (Davinci-003) performance.
Specifically, we change the ordering of hard and layout-ware
demonstrations and evaluate two different orderings: M-
H-L-F (label mapping, hard demonstrations, layout-aware
demonstrations, and formatting demonstrations) and M-L-H-
F (label mapping, layout-aware demonstrations, hard demon-
strations, and formatting demonstrations).
Figure 5a presents a comparison of the performance of
these two orderings. In our case, M-H-L-F consistently
outperforms M-L-H-F across all three datasets. It suggests
that in-context learning is highly sensitive to the ordering
of demonstrations and that finding the optimal ordering for
in-context learning is critical. Our study highlights the im-
portance of optimizing the ordering of demonstrations for
7

--- PAGE 8 ---
/uni00000029/uni00000038/uni00000031/uni00000036/uni00000027 /uni00000026/uni00000032/uni00000035/uni00000027 /uni00000036/uni00000035/uni00000032/uni0000002c/uni00000028
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000056/uni00000048/uni00000057/uni0000001a/uni00000018/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni0000001c/uni00000018/uni00000014/uni00000013/uni00000013/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000030/uni00000010/uni0000002b/uni00000010/uni0000002f/uni00000010/uni00000029
/uni00000030/uni00000010/uni0000002f/uni00000010/uni0000002b/uni00000010/uni00000029(a)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000047/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037
/uni0000002f/uni00000044/uni0000005c/uni00000052/uni00000058/uni00000057/uni0000002f/uni00000030/uni00000059/uni00000016 (b)
Figure 5: Further analysis on (a) the performance effect
of arranging demonstrations in a different order and (b)
the performance comparison of increasing the number of
demonstrations on ICL-D3IE (Davinci-003/ChatGPT) and
LayoutLMv3 on CORD.
in-context learning, and this will be a focus of our future
research.
Effect of the Number of Demonstration Examples . To
further evaluate the performance of ICL-D3IE in compar-
ison to pre-trained VRDU models fine-tuned with a few
demonstrations, we varied the number of demonstrations
for ICL-D3IE (Davinci-003), ICL-D3IE (ChatGPT), and
LayoutLMv3 from 1 to 12. Figure 5b demonstrates that the
performances of all three methods improve as the number
of demonstrations increases on CORD. Notably, ICL-D3IE
(Davinci-003) and ICL-D3IE (ChatGPT) consistently out-
perform LayoutLMv3 by a large margin across all numbers
of demonstrations. These results suggest that our proposed
in-context diverse demonstrations approach is effective and
outperforms pre-trained VRDU models fine-tuned with a
few demonstrations.
Case Study . Figure 6a presents two examples of ask-
ing positional relationship descriptions with Standard ICL
(Davinci-003) and ICL-D3IE (Davinci-003) during the test
phase. Our results illustrate that Standard ICL, without
layout-ware demonstrations, cannot accurately describe the
positional relationships between textual contents in a docu-
ment, while ICL-D3IE can do so effectively. In Figure 6b,
we observe that Standard ICL predicts the entities in the
blue box as ‚Äúheader,‚Äù while ICL-D3IE predicts the entities
as ‚Äúquestion.‚ÄùThese findings highlight the importance of ap-
plying diverse demonstrations such as hard and layout-aware
demonstrations in DIE tasks.
5. Conclusion
In this paper, we proposed ICL-D3IE, an in-context learn-
ing framework that addresses the challenges of applying
LLMs to DIE tasks, specifically the modality and task gap.
We extracted challenging segments from hard training docu-
ments to benefit all test instances, designed demonstrations
that describe positional relationships to enable LLMs to
understand the layout of documents, and introduced for-
matting demonstrations to facilitate easy answer extraction.
Question: {text:"AUTHORIZED COST:",Box :[55 262 194 279], entity:question }‚Ä¶..{ text:"JAN",Box :[195 568 
227 583], entity:answer },Please describe the positional relationship of these texts?
Answer :
‚Ä¶‚Ä¶
"CARRYOVER TO 1988 --0" is located on the 
right of "AUTHORIZED COST:".
"RELEASED TO ACCTG" is located below of 
"AUTHORIZED COST:".
"PAID OUT OF 1987 BUDGET" is located below 
of "RELEASED TO ACCTG".
"CUMULATIVE" is located on the right of "PAID 
OUT OF 1987 BUDGET".
"JAN" is located below of "CUMULATIVE" ‚Ä¶‚Ä¶Answer:
......
"CARRYOVER TO 1988 --0" is located on the 
right of "12, 500 (10 -15-87)". 
"RELEASED TO ACCTG" is located below of 
"AUTHORIZED COST:". 
"PAID OUT OF 1987 BUDGET" is located on the 
right of "RELEASED TO ACCTG".
"CUMULATIVE" is located on the right of "PAID 
OUT OF 1987 BUDGET". 
"JAN" is located below of "PAID OUT OF 1987 
BUDGET" ‚Ä¶‚Ä¶.
(1)Standard ICL (2) ICL-D3IE
(a)
Question: {text:"file",Box :[100 42 148 99]}{ text:"PRODUCT",Box :[104 116 157 130 ]}
{text:"Triumph",Box :[199 116 249 127]}‚Ä¶..{ text:"PROJECT TITLE",Box :[323 99 380 124]},
What are the labels for these texts?
Answer : 
{text:"file",Box :[100 42 148 99], entity:other }
{text:"PRODUCT",Box :[104 116 157 
130], entity:header }
{text:"Triumph",Box :[199 116 249 
127], entity:answer }
‚Ä¶‚Ä¶
{text:"PROJECT TITLE",Box :[323 99 380 
124], entity:header }
{text:"Triumph Disaster Check Study",Box :[399 
111 594 126], entity:answer }Answer: 
{text:"file",Box :[100 42 148 99], entity:other }
{text:"PRODUCT",Box :[104 116 157 
130], entity:question }
{text:"Triumph",Box :[199 116 249 
127], entity:answer }
‚Ä¶‚Ä¶
{text:"PROJECT TITLE",Box :[323 99 380 
124], entity:question }
{text:"Triumph Disaster Check Study",Box :[399 
111 594 126], entity:answer }
‚ùå ‚úî
(1)Standard ICL Test (2) ICL -D3IE Test
(b)
Figure 6: Case study on comparison of (a) positional relation-
ship description and (b) predictions generated by Standard
ICL (Davinci-003) and ICL-D3IE (Davinci-003).
The framework also improves diverse demonstrations itera-
tively and uses label mapping to convert unnatural words to
words that GPT can process. Our evaluation of three DIE
datasets shows that ICL-D3IE consistently outperforms other
methods, except for the ID setting on CORD. These results
highlight the potential of in-context learning frameworks for
VRD understanding tasks based on LLMs, and we hope to
inspire future research in this area.
6. Acknowledgments
This work was supported in part by National Natural
Science Foundation of China under Grants (No. 62222203
and 61976049).
References
[1]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan,
8

--- PAGE 9 ---
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey
Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-
shot learners. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Pro-
cessing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual , 2020.
[2]Haoyu Cao, Xin Li, Jiefeng Ma, Deqiang Jiang, Antai Guo,
Yiqing Hu, Hao Liu, Yinsong Liu, and Bo Ren. Query-driven
generative network for document information extraction in
the wild. In Proceedings of the 30th ACM International
Conference on Multimedia , pages 4261‚Äì4271, 2022.
[3]Rui CAO, Roy Ka-Wei LEE, Wen-Haw CHONG, and Jing
JIANG. Prompting for multimodal hateful meme classifica-
tion. 2022.
[4]Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W
Cohen. Program of thoughts prompting: Disentangling com-
putation from reasoning for numerical reasoning tasks. arXiv
preprint arXiv:2211.12588 , 2022.
[5]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,
et al. Palm: Scaling language modeling with pathways. ArXiv
preprint , abs/2204.02311, 2022.
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of Deep Bidirectional Trans-
formers for Language Understanding. In NAACL-HLT , pages
4171‚Äì4186, 2019.
[7]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional transform-
ers for language understanding. In NAACL , 2019.
[8]Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu,
Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A sur-
vey for in-context learning. arXiv preprint arXiv:2301.00234 ,
2022.
[9]Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu,
Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A
survey on in-context learning. 2022.
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021.
[11] Zhangxuan Gu, Changhua Meng, Ke Wang, Jun Lan,
Weiqiang Wang, Ming Gu, and Liqing Zhang. Xylayoutlm:
Towards layout-aware multimodal networks for visually-rich
document understanding. In CVPR , 2022.
[12] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Boyang Li, Dacheng Tao, and Steven CH Hoi. From
images to textual prompts: Zero-shot vqa with frozen large
language models. arXiv preprint arXiv:2212.10846 , 2022.
[13] Teakgyu Hong, DongHyun Kim, Mingi Ji, Wonseok Hwang,
Daehyun Nam, and Sungrae Park. Bros: A pre-trained lan-
guage model focusing on text and layout for better key infor-
mation extraction from documents. In AAAI , 2022.[14] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A
Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware
image captioning. arXiv preprint arXiv:2211.09699 , 2022.
[15] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu
Wei. Layoutlmv3: Pre-training for document ai with unified
text and image masking. arXiv preprint arXiv:2204.08387 ,
2022.
[16] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis
Karatzas, Shijian Lu, and C. V . Jawahar. ICDAR2019 Com-
petition on Scanned Receipt OCR and Information Extraction.
InICDAR , pages 1516‚Äì1520, 2019.
[17] Guillaume Jaume, H. K. Ekenel, and J. Thiran. FUNSD:
A Dataset for Form Understanding in Noisy Scanned Docu-
ments. ICDARW , 2:1‚Äì6, 2019.
[18] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe
Thiran. Funsd: A dataset for form understanding in noisy
scanned documents. In ICDARW , 2019.
[19] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon
Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sang-
doo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free
document understanding transformer. In European Confer-
ence on Computer Vision (ECCV) , 2022.
[20] Guillaume Lample, Miguel Ballesteros, Sandeep Subrama-
nian, Kazuya Kawakami, and Chris Dyer. Neural architec-
tures for named entity recognition. In NAACL HLT , 2016.
[21] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang,
and Furu Wei. Dit: Self-supervised pre-training for document
image transformer. arXiv preprint arXiv:2203.02378 , 2022.
[22] Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I Morariu, Han-
dong Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu.
Selfdoc: Self-supervised document representation learning.
InCVPR , 2021.
[23] Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan
Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, and Errui
Ding. Structext: Structured text understanding with multi-
modal transformers. In ACM Multimedia , 2021.
[24] Xiaojing Liu, Feiyu Gao, Qiong Zhang, and Huasha
Zhao. Graph convolution for multimodal information ex-
traction from visually rich documents. arXiv preprint
arXiv:1903.11279 , 2019.
[25] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-
Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin
Kalyan. Dynamic prompt learning via policy gradient for
semi-structured mathematical reasoning. arXiv preprint
arXiv:2209.14610 , 2022.
[26] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
Docvqa: A dataset for vqa on document images. In WACV ,
2021.
[27] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di
Jin, and Yanjun Qi. Textattack: A framework for adversarial
attacks, data augmentation, and adversarial training in nlp. In
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations ,
pages 119‚Äì126, 2020.
[28] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jae-
heung Surh, Minjoon Seo, and Hwalsuk Lee. Cord: A con-
solidated receipt dataset for post-ocr parsing. In Document
9

--- PAGE 10 ---
Intelligence Workshop at Neural Information Processing Sys-
tems, 2019.
[29] Seunghyun Park, S. Shin, Bado Lee, Jihyo Lee, Jaeheung
Surh, Minjoon Seo, and Hwal-Suk Lee. CORD: A Consoli-
dated Receipt Dataset for Post-OCR Parsing. In Workshop on
Document Intelligence at NeurIPS 2019 , 2019.
[30] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence
embeddings using siamese bert-networks. arXiv preprint
arXiv:1908.10084 , 2019.
[31] Omar Shaikh, Hongxin Zhang, William Held, Michael Bern-
stein, and Diyi Yang. On second thought, let‚Äôs not think
step by step! bias and toxicity in zero-shot reasoning. arXiv
preprint arXiv:2212.08061 , 2022.
[32] Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Sebastian
Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdh-
ery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging
big-bench tasks and whether chain-of-thought can solve them.
ArXiv preprint , abs/2210.09261, 2022.
[33] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. arXiv preprint
arXiv:2106.13884 , 2021.
[34] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You
Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding
chain-of-thought prompting: An empirical study of what
matters. arXiv preprint arXiv:2212.10001 , 2022.
[35] Jiapeng Wang, Lianwen Jin, and Kai Ding. Lilt: A simple
yet effective language-independent layout transformer for
structured document understanding. In ACL, 2022.
[36] Lei Wang, Jiabang He, Xing Xu, Ning Liu, and Hui Liu.
Alignment-enriched tuning for patch-level pre-trained docu-
ment image models. arXiv preprint arXiv:2211.14777 , 2022.
[37] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed
Chi, and Denny Zhou. Self-consistency improves chain
of thought reasoning in language models. arXiv preprint
arXiv:2203.11171 , 2022.
[38] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-
ret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori
Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and
William Fedus. Emergent abilities of large language models.
CoRR , abs/2206.07682, 2022.
[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompt-
ing elicits reasoning in large language models. In Thirty-
sixth Conference on Neural Information Processing Systems
(NeurIPS 2022) , 2022.
[40] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu
Wei, and Ming Zhou. In LayoutLM: Pre-training of Text
and Layout for Document Image Understanding , pages 1192‚Äì
1200, 2020.
[41] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei,
and Ming Zhou. Layoutlm: Pre-training of text and layout
for document image understanding. In KDD , 2020.
[42] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei,
Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanx-
iang Che, Min Zhang, and Lidong Zhou. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding.
InACL, 2021.
[43] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,
Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study
of gpt-3 for few-shot knowledge-based vqa. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 36,
pages 3081‚Äì3089, 2022.
[44] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choro-
manski, Federico Tombari, Aveek Purohit, Michael Ryoo,
Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. So-
cratic models: Composing zero-shot multimodal reasoning
with language. arXiv preprint arXiv:2204.00598 , 2022.
[45] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona T.
Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle
Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
OPT: open pre-trained transformer language models. CoRR ,
abs/2205.01068, 2022.
[46] Xiaohui Zhao, Zhuo Wu, and Xiaoguang Wang. CUTIE:
Learning to Understand Documents with Convolutional
Universal Text Information Extractor. arXiv preprint
arXiv:1903.12363 , 2019.
[47] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan
Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet,
Quoc Le, and Ed Chi. Least-to-most prompting enables
complex reasoning in large language models. arXiv preprint
arXiv:2205.10625 , 2022.
10

--- PAGE 11 ---
A. Additional Analysis of ICL-D3IE
Due to the limitations of space, we provide more exper-
imental analysis of our proposed ICL-D3IE in this supple-
ment material. Specifically, the main content includes:
‚Ä¢More experimental results and discussion of ICL-D3IE
performance on two benchmark datasets.
‚Ä¢More visual examples obtained by our ICL-D3IE are
provided for easy reference.
For more implementation details, please kindly re-
fer to our code at https://github.com/MAEHCM/
ICL-D3IE .
A.1. Effect of the Number of Different Demonstra-
tions
We present the F1 score of ICL-D3IE (GPT-3 [ 1]) on
FUNSD [ 17] in Figure 7a and on SROIE [ 16] in Figure 7b,
respectively. We observed that the performance of ICL-D3IE
is influenced by the number of demonstrations of each type.
Moreover, the performance of ICL-D3IE is significantly
improved as the number of any demonstration increases.
Notably, we observe that changes in the number of hard
demonstrations (HD) and layout-aware demonstrations (LD)
on FUNSD result in significant changes in performance.
Conversely, on the SROIE dataset, changes in the number of
hard demonstrations and format demonstrations (FD) have a
greater impact on performance. It is worth menioning that
the DIE task on FUNSD requires a substantial amount of lay-
out and difficult text information to solve, whereas SROIE
requires a more favorable extraction format for optimal per-
formance.
A.2. Effect of Increasing the Number of Demonstra-
tions
We conduct experiments where we varied the number
of demonstrations for ICL-D3IE (GPT-3), ICL-D3IE (Chat-
GPT), and LayoutLMv3 from 1 to 12. The results of our ex-
periments are shown in Figure 8, which clearly demonstrates
that the performances of all three methods improve as the
number of demonstrations increases on FUNSD and SROIE.
ICL-D3IE (GPT-3) and ICL-D3IE (ChatGPT) consistently
outperform LayoutLMv3 by a significant margin across all
numbers of demonstrations on two datasets. These findings
suggest that our proposed in-context diverse demonstrations
approach is highly effective and outperforms pre-trained
VRDU models that have been fine-tuned with only a few
demonstrations.
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000001a/uni00000019/uni0000001b/uni00000013/uni0000001b/uni00000017/uni0000001b/uni0000001b/uni0000001c/uni00000015/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni0000002b/uni00000027
/uni0000002f/uni00000027
/uni00000029/uni00000027(a)
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000001b/uni0000001a/uni0000001c/uni00000013/uni0000001c/uni00000016/uni0000001c/uni00000019/uni0000001c/uni0000001c/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni0000002b/uni00000027
/uni0000002f/uni00000027
/uni00000029/uni00000027 (b)
Figure 7: The effect of the number of different demonstra-
tions on (a) FUNSD and on (b) SROIE.
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000047/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037
/uni0000002f/uni00000044/uni0000005c/uni00000052/uni00000058/uni00000057/uni0000002f/uni00000030/uni00000059/uni00000016
(a)
/uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000047/uni00000048/uni00000050/uni00000052/uni00000051/uni00000056/uni00000057/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000029/uni00000014/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016
/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037
/uni0000002f/uni00000044/uni0000005c/uni00000052/uni00000058/uni00000057/uni0000002f/uni00000030/uni00000059/uni00000016 (b)
Figure 8: The performance comparison of increasing the
number of demonstrations on (a) FUNSD and on (b) SROIE.
11

--- PAGE 12 ---
otherquestionanswer
question
(a) (b)Figure 9: Visualization of two cases on FUNSD, which are predicted by ICL-D3IE with text-davinci-003 (90.32 F1).
Blue :Header -label, Green :Answer , Red: Question ,Purple :Other . The entities contained within the red box are predicted
inaccurately.
Table 3: Examples of prediction by GPT-3 and ChatGPT on CORD.
GPT-3:
{text:"Vanilla SLC",Box:[282 443 394 463],entity:MENU.SUB_ETC}{text:"1",Box:[261 387 275 401],entity:MENU.CNT}{text:"FL-Xmas
30 Off",Box:[284 384 434 408],entity:MENU.NM}{text:"68,180",Box:[520 390 587 407],entity:MENU.PRICE}{text:"1",Box:[261 405
275 421],entity:MENU.SUB_CNT}{text:"PAKET SLICES",Box:[310 405 460 425],entity:MENU.SUB_NM}{text:"0",Box:[574 410
587 425],entity:MENU.SUB_PRICE}{text:"1",Box:[261 425 275 441],entity:MENU.SUB_CNT}{text:" FL Cake - French",Box:[305
425 488 444],entity:MENU.SUB_NM}{text:"0",Box:[574 430 587 444],entity:MENU.SUB_PRICE}{text:"1",Box:[261 466 273 479],en-
tity:MENU.SUB_CNT}... ‚úó
ChatGPT:
{text:"Vanilla SLC",Box:[282 443 394 463],entity:MENU.SUB_NM}{text:"1",Box:[261 387 275 401],entity:MENU.SUB_CNT}{text:"FL-
Xmas 30 Off",Box:[284 384 434 408],entity:MENU.NM}{text:"68,180",Box:[520 390 587 407],entity:MENU.PRICE}{text:"1",Box:[261
405 275 421],entity:MENU.SUB_CNT}{text:"PAKET SLICES",Box:[310 405 460 425],entity:MENU.SUB_NM}{text:"0",Box:[574 410
587 425],entity:MENU.SUB_PRICE}{text:"1",Box:[261 425 275 441],entity:MENU.SUB_CNT}{text:" FL Cake - French",Box:[305
425 488 444],entity:MENU.SUB_NM}{text:"0",Box:[574 430 587 444],entity:MENU.SUB_PRICE}{text:"1",Box:[261 466 273 479],en-
tity:MENU.SUB_CNT}... ‚úó
Ground Truth:
{text:"Vanilla SLC",Box:[282 443 394 463],entity:MENU.SUB_NM}{text:"1",Box:[261 387 275 401],entity:MENU.CNT}{text:"FLXmas
30 Off",Box:[284 384 434 408],entity:MENU.NM}{text:"68,180",Box:[520 390 587 407],entity:MENU.PRICE}{text:"1",Box:[261 405
275 421],entity:MENU.SUB_CNT}{text:"PAKET SLICES",Box:[310 405 460 425],entity:MENU.SUB_NM}{text:"0",Box:[574 410
587 425],entity:MENU.SUB_PRICE}{text:"1",Box:[261 425 275 441],entity:MENU.SUB_CNT}{text:" FL Cake - French",Box:[305
425 488 444],entity:MENU.SUB_NM}{text:"0",Box:[574 430 587 444],entity:MENU.SUB_PRICE}{text:"1",Box:[261 466 273 479],en-
tity:MENU.SUB_CNT}... ‚úì
12

--- PAGE 13 ---
headerother
header
header
(c) (d)Figure 10: Visualization of two cases on FUNSD, which are predicted by ICL-D3IE with text-davinci-003 (90.32 F1).
Blue :Header -label, Green :Answer , Red: Question ,Purple :Other .
(a) GPT-3 (b) ChatGPT (c) GT
Figure 11: Visualization of three cases on CORD, which are predicted by GPT-3 (Column (a)), ChatGPT (Column (b)), and
Ground Truth(Column (c)). ICL-D3IE with text-davinci-003 (94.12 F1) versus ICL-D3IE with ChatGPT (87.13 F1)
on CORD test-set.
13

--- PAGE 14 ---
(a) GPT-3 (b) ChatGPT (c) GT
Figure 12: Visualization of three cases on CORD, which are predicted by GPT-3 (Column (a)), ChaTGPT (Column (b)), Ground
Truth(Column (c)). Column(a), Column(b) and Column(c) corresponds to Table4. ICL-D3IE with text-davinci-003
(94.12 F1) versus ICL-D3IE with ChatGPT (87.13 F1) on CORD test-set
Table 4: Examples of prediction by GPT-3 and ChatGPT on CORD.
GPT-3:
{text:"1",Box:[277 427 294 442],entity:MENU.CNT}{text:"[MD] SOFT MORNING ROLL",Box:[305 427 579 444],en-
tity:MENU.NM}{text:"18,000",Box:[649 427 729 443],entity:MENU.PRICE}{text:"2",Box:[275 445 296 460],en-
tity:MENU.CNT}{text:"REAL HONEY PANCAKE BREAD",Box:[305 445 603 461],entity:MENU.NM}{text:"20,000",Box:[647
444 730 460],entity:MENU.PRICE}{text:"4",Box:[277 463 296 478],entity:MENU.CNT}{text:"CUSTARD BRIOCHE",Box:[305 462
494 478],entity:MENU.NM}{text:"36,000",Box:[648 461 729 478],entity:MENU.PRICE}{text:"TOTAL 74,000",Box:[304 495 733
526],entity:TOTAL.TOTAL_PRICE}{text:"CASH 100,000",Box:[305 524 732 540],entity:TOTAL.CASHPRICE}... ‚úì
ChatGPT:
{text:"1",Box:[277 427 294 442],entity:MENU.CNT}{text:"[MD] SOFT MORNING ROLL",Box:[305 427 579 444],en-
tity:MENU.NM}{text:"18,000",Box:[649 427 729 443],entity:MENU.PRICE}{text:"2",Box:[275 445 296 460],en-
tity:MENU.CNT}{text:"REAL HONEY PANCAKE BREAD",Box:[305 445 603 461],entity:MENU.NM}{text:"20,000",Box:[647
444 730 460],entity:MENU.PRICE}{text:"4",Box:[277 463 296 478],entity:MENU.CNT}{text:"CUSTARD BRIOCHE",Box:[305 462
494 478],entity:MENU.NM}{text:"36,000",Box:[648 461 729 478],entity:MENU.PRICE}{text:"TOTAL 74,000",Box:[304 495 733
526],entity:SUB_TOTAL.SUBTOTAL_PRICE}{text:"CASH 100,000",Box:[305 524 732 540],entity:TOTAL.CASHPRICE}... ‚úó
Ground Truth:
{text:"1",Box:[277 427 294 442],entity:MENU.CNT}{text:"[MD] SOFT MORNING ROLL",Box:[305 427 579 444],en-
tity:MENU.NM}{text:"18,000",Box:[649 427 729 443],entity:MENU.PRICE}{text:"2",Box:[275 445 296 460],en-
tity:MENU.CNT}{text:"REAL HONEY PANCAKE BREAD",Box:[305 445 603 461],entity:MENU.NM}{text:"20,000",Box:[647
444 730 460],entity:MENU.PRICE}{text:"4",Box:[277 463 296 478],entity:MENU.CNT}{text:"CUSTARD BRIOCHE",Box:[305 462
494 478],entity:MENU.NM}{text:"36,000",Box:[648 461 729 478],entity:MENU.PRICE}{text:"TOTAL 74,000",Box:[304 495 733
526],entity:TOTAL.TOTAL_PRICE}{text:"CASH 100,000",Box:[305 524 732 540],entity:TOTAL.CASHPRICE}... ‚úì
14

--- PAGE 15 ---
(a) (b) (c)
Figure 13: Visualization of three cases on CORD, which are predicted by ICL-D3IE with text-davinci-003 (94.12 F1).
Column(a) corresponds to Table 5 Q(A) , Column(b) corresponds to Table 5 Q(B) and Column(b) corresponds to Table 6 Q(C) .
Table 5: Examples of prediction by ICL-D3IE on CORD. The part with green background is generated by LLM.
Q(A): {text:"1",Box:[262 481 276 506]}{text:"CIRENG PANDAWA",Box:[291 479 557 502]}{text:"26,818",Box:[719 478 818
504]}{text:"Subtotal 26,818",Box:[451 537 821 564]}{text:"Pb1 2,681",Box:[512 568 819 596]}{text:"Total 29,500",Box:[476 605
820 633]}{text:"CASH 29,500",Box:[469 632 822 663]}{text:"Total Item 1",Box:[418 665 824 690]}{text:"Total Qty 1",Box:[435 697 825
725]},What are the labels for these texts?
A: {text:"1",Box:[262 481 276 506],en tity:MENU.CNT}{text:"CIRENG PAN DAWA",Box:[291 479 557 502],en -
tity:MENU.NM}{text:"26,818",Box:[719 478 818 504],en tity:MENU.PRICE}{text:"Subtotal 26,818",Box:[451 537 821 564],en -
tity:SUB_TOTAL.SUBTOTAL_PRICE}{text:"Pb1 2,681",Box:[512 568 819 596],en tity:SUB_TOTAL.TAX_PRICE}{text:"Total
29,500",Box:[476 605 820 633],en tity:TOTAL.TOTAL_PRICE}{text:"CASH 29,500",Box:[469 632 822 663],en -
tity:TOTAL.CASHPRICE}{text:"Total Item 1",Box:[418 665824690],en tity:TOTAL.MENUTYPE_CNT}{text:"Total Qty1",Box:[435
697825725],en tity:TOTAL.MENUQTY_CNT.} ‚úì
Q(B): {text:"1",Box:[292 615 300 632]}{text:"BLACK PAPPER MEATBALL PAS",Box:[311 597 575 629]}{text:"79,000",Box:[601
591 668 606]}{text:"1",Box:[292 636 300 655]}{text:"TRUFFLE CREAM",Box:[311 629 454 650]}{text:"77,000",Box:[605 614 674
627]}{text:"1",Box:[291 660 299 676]}{text:"EARL GREY MILK TEA",Box:[310 645 509 675]}{text:"59,000",Box:[610 636 680
648]}{text:"SUBTTL 215,000",Box:[311 658 685 697]}{text:"Service Charge 6% 12,900",Box:[312 684 693 725]}{text:"TAX 10%
22,790",Box:[311 706 696 752]}{text:"TOTAL 250,690",Box:[310 763 704 803]},What are the labels for these texts?
A: {text:"1",Box:[292 615 300 632],en tity:MENU.CNT}{text:"BLACK PAPPER MEAT BALL PAS",Box:[311 597 575
629],en tity:MENU.NM}{text:"79,000",Box:[601 591 668 606],en tity:MENU.PRICE}{text:"1",Box:[292 636 300 655],en -
tity:MENU.CNT}{text:"TRUFFLE CREAM",Box:[311 629 454 650],en tity:MENU.NM}{text:"77,000",Box:[605 614 674
627],en tity:MENU.PRICE}{text:"1",Box:[291 660 299 676],en tity:MENU.CNT}{text:"EARL GREY MILK TEA",Box:[310 645
509 675],en tity:MENU.NM}{text:"59,000",Box:[610 636 680 648],en tity:MENU.PRICE}{text:"SUBTTL 215,000",Box:[311
658 685 697],en tity:SUB_TOTAL.SUBTOTAL_PRICE}{text:"Service Charge 6% 12,900",Box:[312 684 693 725],en -
tity:SUB_TOTAL.SERVICE_PRICE}{text:"TAX 10% 22,790",Box:[311 706696752],en tity:SUB_TOTAL.TAX_PRICE}{text:"TOTAL
250,690",Box:[310 763704803],en tity:TOTAL.TOTAL_PRICE}. ‚úì
15

--- PAGE 16 ---
Table 6: Examples of prediction by ICL-D3IE on CORD. The part with green background is generated by LLM.
Q(C): {text:"2",Box:[150 416 173 439]}{text:"[MD] SOFT STEAMED CHEESEC",Box:[189 415 629 442]}{text:"20,000",Box:[678 419
789 445]}{text:"2",Box:[150 445 173 469]}{text:"[MD] SOFT STEAMED CHOCOCA",Box:[189 444 629 472]}{text:"20,000",Box:[675
447 787 473]}{text:"TOTAL 40,000",Box:[180 499 784 550]}{text:"CASH 100,000",Box:[180 546 784 572]}{text:"CHANGE
60,000",Box:[173 574 798 625]},What are the labels for these texts?
A: {text:"2",Box:[150 416 173 439],en tity:MENU.CNT}{text:"[MD] SOFT STEAMED CHEESEC",Box:[189 415 629
442],en tity:MENU.NM}{text:"20,000",Box:[678 419 789 445],en tity:MENU.PRICE}{text:"2",Box:[150 445 173 469],en -
tity:MENU.CNT}{text:"[MD] SOFT STEAMED CHOCOCA",Box:[189 444629472],en tity:MENU.NM}{text:"20,000",Box:[675 447787
473],en tity:MENU.PRICE}{text:"TOTAL 40,000",Box:[180 499784550],en tity:TOTAL.TOTAL_PRICE}{text:"CASH 100,000",Box:[180
546784572],en tity:TOTAL.CASHPRICE}{text:"CHANGE 60,000",Box:[173 574798625],en tity:TOTAL.CHANGEPRICE}. ‚úì
(a) (b)
Figure 14: Visualization of two layout-aware demonstrations on FUNSD, which are generated by ICL-D3IE. Column (a)
corresponds to Table7 and Column (b) corresponds to Table8. Blue :Header -label, Green :Answer , Red: Question ,Purple :
Other .
16

--- PAGE 17 ---
Table 7: Examples of layout-aware demonstrations produced by ICL-D3IE on FUNSD. The part with green background is
generated by LLM.
Q: {text:"TO:",Box:[84 53 112 67],entity:question}{text:"R. B. SPELL",Box:[147 50 228 68],en-
tity:answer}{text:"SUBMISSION DATE:",Box:[532 52 670 67],entity:header}{text:"FROM:",Box:[85 85 134
102],entity:question}{text:"S. J. Farnham",Box:[147 85 239 102],entity:answer}{text:"OCT 3",Box:[535 85 581
103],entity:question}{text:"DEC 26",Box:[651 85 704 103],entity:question}{text:"SUBJECT:",Box:[87 122 157
139],entity:question}{text:"HARLEY DA VIDSON 100‚ÄôS CIGARETTES PROGRESS REPORT",Box:[161 122 571
139],entity:answer}{text:"OCT 31",Box:[531 105 581 122],entity:question}{text:"JAN 23, 1995",Box:[652 105 705
136],entity:question}{text:"X",Box:[725 105 740 119],entity:answer}{text:"DISTRIBUTION:",Box:[87 180 193
197],entity:question}{text:"EFFECTIVENESS OF SELL (REPORT ON OCT 3 ONLY).",Box:[197 182 596 198],en-
tity:answer}{text:"DIRECT ACCOUNT AND CHAIN VOIDS (USE X TO INDICATE A VOID).",Box:[85 272 546
292],entity:header}{text:"ACCOUNT",Box:[236 321 303 332],entity:question}{text:"NO. STORES",Box:[511 307
570 331],entity:question}{text:"100‚ÄôS",Box:[623 318 657 333],entity:question}{text:"LTS. 100‚ÄôS",Box:[669 306 706
332],entity:question}{text:"NONE",Box:[246 339 289 353],entity:answer}{text:"% OF DISTRIBUTION ACHIEVED IN
RETAIL OUTLETS:",Box:[87 541 447 559],entity:question}..., Please describe the positional relationship of these text?
A:"TO:" is located in the upper left corner with a Box of [84 53 112 67], so it can be labeled as "question".
"R. B. SPELL" is located on the right of "TO:" with a Box of [147 50 228 68], so it can be labeled as "answer".
"SUBMISSION DATE:" is located at the top right corner with a Box of [532 52 670 67], so it can be labeled as "header".
"FROM:" is located below "TO:" with a Box of [85 85 134 102], so it can be labeled as "question".
"S. J. Farnham" is located on the right of "FROM:" with a Box of [147 85 239 102], so it can be labeled as "answer".
"OCT 3" is located below "SUBMISSION DATE:" with a Box of [535 85 581 103], so it can be labeled as "question".
"DEC 26"islocated ontheright of"OCT 3"with aBox of[651 85704103], soitcanbelabeled as"ques tion".
"SUB JECT:" islocated below"FROM:" with aBox of[87122157139], soitcanbelabeled as"ques tion".
"HARLEY DA VID SON 100‚ÄôS CIGARETTES PROGRESS REPORT" islocated ontheright of"SUB JECT:" with aBox of
[161 122571139], soitcanbelabeled as"answer".
"OCT 31"islocated below"DEC 26"with aBox of[531 105581122], soitcanbelabeled as"ques tion".
"JAN 23,1995" islocated ontheright of"OCT 31"with aBox of[652 105705136], soitcanbelabeled as"ques tion".
"X"islocated ontheright of"JAN 23,1995" with aBox of[725 105740119], soitcanbelabeled as"answer".
"DIS TRIBUTION:" islocated below"SUB JECT:" with aBox of[87180193197], soitcanbelabeled as"ques tion".
"EFFECTIVE NESS OFSELL (REPORT ONOCT 3ONLY)." islocated ontheright of"DIS TRIBUTION:" with aBox of
[197 182596198], soitcanbelabeled as"answer".
"DIRECT ACCOUNT AND CHAIN VOIDS (USE XTOINDICATE AVOID)." islocated below"DIS TRIBUTION:" with a
Box of[85272546292], soitcanbelabeled as"header".
"ACCOUNT" islocated below"DIRECT ACCOUNT AND CHAIN VOIDS (USE XTOINDICATE AVOID)." with aBox
of[236 321303332], soitcanbelabeled as"ques tion".
"NO. STORES" islocated ontheright of"ACCOUNT" with aBox of[511 307570331], soitcanbelabeled as"ques tion".
"100‚ÄôS" islocated ontheright of"NO. STORES" with aBox of[623 318657333], soitcanbelabeled as"ques tion".
"LTS. 100‚ÄôS" islocated ontheright of"100‚ÄôS" with aBox of[669 306706332], soitcanbelabeled as"ques tion".
"NONE" islocated below"ACCOUNT" with aBox of[246 339289353], soitcanbelabeled as"answer".
"%OFDISTRIBUTION ACHIEVED INRETAIL OUT LETS:" islocated below"DIRECT ACCOUNT AND CHAIN VOIDS
(USE XTOINDICATE AVOID)." with aBox of[87541447559], soitcanbelabeled as"ques tion".
...
17

--- PAGE 18 ---
Table 8: Examples of layout-aware demonstrations produced by ICL-D3IE on FUNSD. The part with green background is
generated by LLM.
Q:{text:"KOOL",Box:[54 41 100 54],entity:question}{text:"0595- 529- 1510 -0",Box:[58 55 193 72],entity:answer}{text:"PROJECT
NO.",Box:[430 35 527 55],entity:question}{text:"1987- 144- K",Box:[558 31 653 51],entity:other}{text:"DESCRIPTION:",Box:[54
107 155 126],entity:question}{text:"KMDP ANALYSIS UTILIZING BLACK SWITCHING STUDY DATA",Box:[197 100
625 125],entity:answer}{text:"MR PERSONNEL:",Box:[55 179 165 196],entity:question}{text:"C. B. Pugh",Box:[200 177 278
195],entity:answer}{text:"SUPPLIER:",Box:[54 224 132 238],entity:question}{text:"Market Facts",Box:[200 218 308 235],en-
tity:answer}{text:"AUTHORIZED COST:",Box:[55 262 194 279],entity:question}{text:"12, 500 (10- 15- 87)",Box:[198 258 372 280],en-
tity:answer}{text:"CARRYOVER TO 1988 ‚Äì 0",Box:[460 275 659 296],entity:other}{text:"RELEASED TO ACCTG",Box:[75 517 151
551],entity:question}{text:"PAID OUT OF 1987 BUDGET",Box:[292 509 389 548],entity:question}{text:"CUMULATIVE",Box:[553
527 637 545],entity:question}{text:"JAN",Box:[195 568 227 583],entity:answer}{text:"466024372",Box:[548 884 671 906],en-
tity:other}{text:"PRODUCED FROM B&W WEB SITE",Box:[5 950 228 970],entity:other}{text:"466024372",Box:[678 950 744 968],en-
tity:other}, Please describe the positional relationship of these text?
A:"KOOL" has a Box of [54 41 100 54] and is located in the upper left corner, so it can be labeled as "question".
"0595- 529- 1510 -0" has a Box of [58 55 193 72] and is located on the below of "KOOL", so it can be labeled as "answer".
"PROJECT NO." has a Box of [430 35 527 55] and is located at the top right corner, so it can be labeled as "question".
"1987- 144- K" has a Box of [558 31 653 51] and is located on the right of "PROJECT NO.", so it can be labeled as "other".
"DESCRIP TION:" hasaBox of[54107155126] andislocated belowof"KOOL" question, soitcanbelabeled as"ques tion".
"KMDP ANAL YSISUTILIZING BLACK SWITCH ING STUDY DATA" hasaBox of[197 100625125] andislocated ontheright of
"DESCRIP TION:" question, soitcanbelabeled as"answer".
"MR PERSON NEL:" hasaBox of[55179165196] andislocated belowof"DESCRIP TION:" question, soitcanbelabeled as"ques tion".
...
(a) GPT-3 (b) ChatGPT (c) GT
Figure 15: Visualization of three cases on SROIE, which are predicted by GPT-3 (Column (a)), ChaTGPT (Column (b)) and
Ground Truth (Column (c)). ICL-D3IE with text-davinci-003 (97.88 F1) versus ICL-D3IE with ChatGPT (92.63 F1).
Blue :Company -label, Green :Address , Red: Total ,Purple :Date .
18

--- PAGE 19 ---
Table 9: Predictions produced by GPT-3 and ChatGPT on SROIE. The visualization results are shown in Figure15.
GPT-3:
{"KEDAI PAPAN YEW CHUAN"}.
{"LOT 276 JALAN BANTING"}{"43800 DENGKIL, SELANGOR."}.
{"283.55"}.
{"12/03/2018"}.
‚úì
ChatGPT:
{"KEDAI PAPAN YEW CHUAN"}.
{"LOT 276 JALAN BANTING"}{"43800 DENGKIL, SELANGOR."}{"TEL : 03-87686092"}{"FAX : 03-87686092"}.
{"283.55"}.
{"12/03/2018"}.
‚úó
Ground Truth:
{"KEDAI PAPAN YEW CHUAN"}.
{"LOT 276 JALAN BANTING"}{"43800 DENGKIL, SELANGOR."}.
{"283.55"}.
{"12/03/2018"}.
‚úì
(a) (b) (c)
Figure 16: Visualization of three cases on SROIE, which are predicted by GPT-3. Blue :Company -label, Green :Address ,
Red: Total ,Purple :Date .
19

--- PAGE 20 ---
Table 10: Predictions produced by GPT-3 on SROIE. The visualization results are shown in Figure16.
GPT-3:
{"STAR GROCER SDN BHD (1171881-W)"}.
{"NO 4, DESA PANDAN, OFF KAMPONG PANDAN,"}{"55100 KUALA LUMPUR"}.
{"66.15"}.
{"25/03/18"}.
‚úì
GPT-3:
{"SYARIKAT PERNIAGAAN GIN KEE"}.
{"NO 290. JALAN AIR PANAS."}{"SETAPAK."}{"53200, KUALA LUMPUR."}.
{"36.04"}.
{"01/02/2018"}.
‚úì
GPT-3:
{"GUARDIAN HEALTH AND BEAUTY SDN BHD"}.
{"LOT B-005-006, BASEMENT LEVEL 1"}{"THE STARLING MALL"}{"JALAN SS21/60, DAMANSARA UTAMA."}.
{"108.21"}.
{"16/08/17"}.
‚úì
20

--- PAGE 21 ---
Table 11: An Example of ICL-D3IE Prompt for FUNSD. Blue :Hard Demonstrations ,Green :Formatting Demonstrations ,
Red: Layout-Aware Demonstrations ,Purple :Label Mapping .
PROMPT FOR FUNSD NER
There arefour labelsforselection, "ques tion", "answer", "header", and"other".
sample1
"Attendence" :other
"140" :other
"QUAN TITY REQUIRED" :question
"CUS TOMER SHIP PING NUM BER" :question
"DATE FOR WARDED TOPRO MOTION SERVICES:" :question
"REQFORM" :other
"01/17/97" :other
sample2
"DIS TRIBUTION" :header
"DIRECT ACCOUNTS PRO JECTED TOACCEPT WHICH REFUSED ONE ORMORE
PACK ING" :other
"PACK ING" :question
"BASIC" :question
"G.P.C" :question
"$":other
sample3"MAR KET ING RESEARCH PROJECT APPROV AL" :header
"Mar ketingResearch" :question
"Group Prod uct" :question
...
Context: {text:"Name ofAccount",Box:[369 481459497],en tity:question}{text:"Ind/ LorV olume",Box:[494 472537503],en tity:question}
{text:"Number ofStores",Box:[563 477619505],en tity:question}{text:"Meijer Gas",Box:[68 510123524],en tity:answer}{text:"245/
19",Box:[208 509245521],en tity:answer}.
...
Q:{text:"KOOL",Box:[54 4110054],en tity:question}{text:"0595 -529-1510 -0",Box:[58 5519372],en tity:answer}{text:"PROJECT
NO.",Box:[430 35527 55],en tity:question}{text:"1987 -144-K",Box:[558 31653 51],en tity:other}{text:"DESCRIPTION:",Box:[54
107 155 126],en tity:question}{text:"KMDP ANAL YSIS UTILIZING BLACK SWITCH ING STUDY DATA",Box:[197 100
625 125],en tity:answer}{text:"MR PERSON NEL:",Box:[55 179 165 196],en tity:question}{text:"C. B.Pugh",Box:[200 177 278
195],en tity:answer}{text:"SUPPLIER:",Box:[54 224 132 238],en tity:question}{text:"Market Facts",Box:[200 218 308 235],en -
tity:answer}{text:"AUTHORIZED COST:",Box:[55 262194279],en tity:question}{text:"12, 500(10-15-87)",Box:[198 258372280],en -
tity:answer}{text:"CARRYOVER TO1988 ‚Äì0",Box:[460 275659296],en tity:other}{text:"RELEASED TOACCTG",Box:[75 517151
551],en tity:question}{text:"PAID OUT OF1987 BUD GET",Box:[292 509389548],en tity:question}{text:"CUMULATIVE",Box:[553
527 637 545],en tity:question}{text:"JAN",Box:[195 568 227 583],en tity:answer}{text:"466024372",Box:[548 884 671 906],en -
tity:other}{text:"PRODUCED FROM B&W WEB SITE",Box:[5 950228970],en tity:other}{text:"466024372",Box:[678 950744968],en -
tity:other}, Please describe thepositional relationship ofthese text?
A:"KOOL" hasaBox of[544110054]andislocated intheupperleftcorner,soitcanbelabeled as"ques tion".
"0595 -529-1510 -0"hasaBox of[585519372]andislocated onthebelowof"KOOL" question, soitcanbelabeled as"answer".
"PROJECT NO." hasaBox of[430 3552755]andislocated atthetopright corner,soitcanbelabeled as"ques tion".
"1987 -144-K"hasaBox of[558 3165351]andislocated ontheright of"PROJECT NO." question, soitcanbelabeled as"other".
"DESCRIP TION:" hasaBox of[54107155126] andislocated belowthe"KOOL" question, soitcanbelabeled as"ques tion".
...
Q:{text:"ACUTE TOX ICITY INMICE",Box:[295 5651279]}{text:"COMPOUND",Box:[84 109136119]},What arethelabelsforthese
texts?
A:{text:"ACUTE TOX ICITY INMICE",Box:[295 5651279],en tity:header}{text:"COMPOUND",Box:[84 109136119],en tity:question}.
Q:
21

--- PAGE 22 ---
Table 12: An Example of ICL-D3IE Prompt for CORD. Green :Formatting Demonstrations , Red: Layout-Aware
Demonstrations ,Purple :Label Mapping .
PROMPT FOR CORD NER
MENU.NM :name ofmenu
TOTAL.TOTAL_PRICE :totalprice
SUB_TOTAL.SERVICE_PRICE :service charge
TOTAL.CASHPRICE :amount ofprice paid incash
TOTAL.CHANGEPRICE :amount ofchange incash
TOTAL.MENUTYPE_CNT :totalcount oftype ofmenu
TOTAL.MENUQTY_CNT :totalcount ofquan tity
TOTAL.TOTAL_ETC :"Coupon 100,000"
TOTAL.CREDITCARDPRICE :amount ofprice paid incredit/debit card
MENU.NUM :identificationofmenu
MENU.DISCOUNTPRICE :discounted price ofmenu
MENU.SUB_ETC :"Gula Murni 100%"
VOID_MENU.NM :"SOP AYM BNG"
SUB_TOTAL.OTHERSVC_PRICE :"BIAYA TAM BAHAN 27,300"
SUB_TOTAL.SUBTOTAL_PRICE :subto talprice
MENU.V ATYN :"Sales included PB1"
MENU.ETC :"TMBHN CUP"...
Context: {text:"NASI PUTIH",Box:[137 446366475],en tity:MENU.NM}{text:"13,000",Box:[730 446871476],en tity:MENU.PRICE}
{text:"AYAM GRBUMBU",Box:[135 511438539],en tity:MENU.NM}{text:"44,000",Box:[724 512869542],en tity:MENU.PRICE}
{text:"2X",Box:[134 479184506],en tity:MENU.CNT}{text:"22,000",Box:[500 480638508],en tity:MENU.UNITPRICE}{text:"NESTLE
330ML",Box:[138 573433601],en tity:MENU.NM}{text:"16,000",Box:[725 576865605],en tity:MENU.PRICE}
...
Context: {text:"BASO TAHU BIHUN",Box:[49 649361680],en tity:MENU.NM}{text:"1",Box:[449 646472671],en tity:MENU.CNT}
{text:"43.636",Box:[484 645613668],en tity:MENU.UNITPRICE}{text:"43.636",Box:[701 640828666],en tity:MENU.PRICE}
{text:"TOTAL 43.636",Box:[45 677840717],en tity:SUB_TOTAL.SUBTOTAL_PRICE}{text:"TAX 10.00 4,364",Box:[43 756853800],en -
tity:SUB_TOTAL.TAX_PRICE}{text:"GRAND TOTAL 48.000",Box:[40 796854838],en tity:TOTAL.TOTAL_PRICE}{text:"TUNAI
50.000",Box:[45 838862877],en tity:TOTAL.CASHPRICE}{text:"KEMBALI 2.000",Box:[48 879859913],en tity:TOTAL.CHANGE -
PRICE}...
Q:What isdifference between MENU.UNITPRICE andMENU.PRICE?
A:There aretwotexts "43.636" onthesame line. Thetextontheleftislabeled asMENU.UNITPRICE andthetextontheright islabeled as
MENU.PRICE.
Context: {text:"Rp",Box:[608 584667610],en tity:SUB_TOTAL.SUBTOTAL_PRICE}{text:"Rp",Box:[606 673667703],en tity:TOTAL.
TOTAL_PRICE}{text:"Subtotal 12.000",Box:[81 584831610],en tity:SUB_TOTAL.SUBTOTAL_PRICE}{text:"Total 13.200",Box:[77 670
829700],en tity:TOTAL.TOTAL_PRICE}.
Q:which labelselectionfortext"Rp" with [608 584667610]?
A:SUB_TOTAL.SUBTOTAL_PRICE,the text"Rp" with box[608 584667610] islocated onthesame lineandobscured bythetext
‚ÄôSubto tal12.000‚Äô with box[81584831610].
...
Q:{text:"Berry Many -Low (P)",Box:[320 418519444]},What arethelabelsforthese texts?
A:{text:"Berry Many -Low (P)",Box:[320 418519444],en tity:MENU.NM}.
Q:{text:"Kupon 1",Box:[135 548250576]}{text:"Total Item: 2",Box:[0 621506666]},What arethelabelsforthese texts?
A:{text:"Kupon 1",Box:[135 548250576],en tity:MENU.NM}{text:"Total Item: 2",Box:[0 621506666],en tity:TOTAL.MENUQTY_CNT}.
Q:
22

--- PAGE 23 ---
Table 13: An Example of ICL-D3IE Prompt for SROIE. Green :Formatting Demonstrations , Red: Layout-Aware
Demonstrations ,Purple :Label Mapping .
PROMPT FOR SROIE NER
There arefour labelsforselection, "com pany", "address", "date", and"total".
Context: "99SPEED MART S/B" :company
"LOT P.T.2811, JALAN ANGSA, TAMAN BERKE LEY 41150 KLANG, SELAN GOR 1243 -JLN PUDU ULU" :address
"AEON CO. (M)SDN BHD" :company
"3RD FLR, AEON TAMAN MALURI SCJLN JEJAKA, TAMAN MALURI CHERAS, 55100 KUALA LUMPUR" :address
"BENS INDEPEN DENT GRO CER SDN. BHD" :company
"LOT 6,JALAN BATAI, PLAZA BATAI, DAMANSARA HEIGHTS 50490, KUALA LUMPUR" :address
...
Context: {text:"TOTAL SALES INCL GST",Box:[1598 2761 2305 2855],en tity:other}{text:"38.59",Box:[2833 2763 3008 2848],en -
tity:other}{text:"TOTAL AFTER ADJ INCL GST",Box:[1595 2876 2445 2957],en tity:other}{text:"38.60",Box:[2827 2871 3012 2950],en -
tity:total}.
Q:Why "38.60" labeled astotal?
A:Because "38.60" isontheright of"TOTAL AFTER ADJ INCL GST".
Context: {text:"TOTAL INCLU SIVE GST:",Box:[84 716352741],en tity:other}{text:"29.00",Box:[562 718629743],en tity:total}.
Q:Why "29.00" labeled astotal?
A:Because "29.00" isontheright of"TOTAL INCLU SIVE GST:".
...
Context: {text:"107",Box:[347 216454293],en tity:other}{text:"GERBANG ALAF RESTAU RANTS SDN BHD",Box:[75 338649380],en -
tity:company}{text:"(65351 -M)",Box:[272 385437429],en tity:other}{text:"GOLDEN ARCHES RESTAU RANTS SDN BHD",Box:[64
469 662 511],en tity:other}{text:"FORMERLY KNOWN AS",Box:[203 431 515 469],en tity:other}{text:"LEVEL 6,BAN GUNAN
TH, DAMANSARA UPTOWN3",Box:[24 560 727 602],en tity:address}{text:"LICENSEE OFMCDON ALD‚ÄôS",Box:[161 512 568
560],en tity:other}{text:"NO.3, JALAN SS21/39, 47400 PETAL ING JAYA",Box:[26 604723648],en tity:address}{text:"(GST IDNO:
000504664064)",Box:[143 693594737],en tity:other}{text:"SETANGOR",Box:[286 649441693],en tity:address}{text:"MCDONALD‚ÄôS
SHELL MAHKOTA CHERAS DT(#36",Box:[18 739725783],en tity:other}{text:"TEL NO. 03-9010 -9849",Box:[189 784568828],en -
tity:other}{text:"QTY ITEM",Box:[20 1088 1771126],en tity:other}{text:"ORD #07-REG #19-10/03/2018",Box:[22 1048 5261090],en -
tity:date}{text:"17:24:07",Box:[536 1046 685 1088],en tity:other}{text:"TOTAL",Box:[625 1098 712 1130],en tity:other}{text:"1 M
COKE",Box:[60 1140 213 1174],en tity:other}{text:"3.50",Box:[641 1140 716 1176],en tity:other}{text:"1 MCCHICKEN",Box:[56
1181 2571215],en tity:other}{text:"5.00",Box:[639 1179 7121223],en tity:other}{text:"TAKEOUT TOTAL (INCL GST)",Box:[20 1263
457 1311],en tity:other}{text:"TOTAL ROUNDED",Box:[24 1308 260 1350],en tity:other}{text:"8.50",Box:[639 1271 716 1311],en -
tity:total}{text:"8.50",Box:[635 1314 7161356],en tity:total}{text:"CASH TEN DERED",Box:[26 1358 2721394],en tity:other}
Q:Returntextlabeled ascompany, originaladdress, total,anddate?
A:{"GER BANG ALAF RESTAU RANTS SDN BHD"}.
{"LEVEL 6,BAN GUNAN TH,DAMANSARA UPTOWN3"}{"NO.3, JALAN SS21/39, 47400 PETAL ING JAYA"}{"SETANGOR"}.
{"8.50"}{"8.50"}.
{"ORD #07-REG #19-10/03/2018"}.
Q:Returnonecompany anditsoriginaladdress, onetotal,andonedate?
A:{"GER BANG ALAF RESTAU RANTS SDN BHD"}.
{"LEVEL 6,BAN GUNAN TH,DAMANSARA UPTOWN3"}{"NO.3, JALAN SS21/39, 47400 PETAL ING JAYA"}{"SETANGOR"}.
{"8.50"}.
{"ORD #07-REG #19-10/03/2018"}.
Q:
23
