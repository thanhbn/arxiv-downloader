# 2305.18507.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2305.18507.pdf
# Kích thước tệp: 2165781 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Code Prompting: một Phương pháp Thần kinh Ký hiệu cho
Lý luận Phức tạp trong các Mô hình Ngôn ngữ Lớn
Yi Hu Haotong Yang Zhouchen Lin Muhan Zhang
Viện Trí tuệ Nhân tạo, Đại học Bắc Kinh
huyi2002@stu.pku.edu.cn
{haotongyang, zlin, muhan}@pku.edu.cn
Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) đã mở rộng quy mô để mở khóa một loạt rộng các
nhiệm vụ lý luận phức tạp với sự hỗ trợ của các phương pháp prompting khác nhau. Tuy nhiên,
các phương pháp prompting hiện tại tạo ra các bước trung gian bằng ngôn ngữ tự nhiên để giúp
lý luận, điều này có thể gây ra việc rút gọn nhiệm vụ không hoàn hảo và nhầm lẫn. Để giảm thiểu
những hạn chế này, chúng tôi khám phá code prompting, một phương pháp prompting thần kinh
ký hiệu với cả phiên bản zero-shot và few-shot, kích hoạt code làm các bước trung gian. Chúng
tôi tiến hành thí nghiệm trên 7 benchmark được sử dụng rộng rãi liên quan đến lý luận ký hiệu
và lý luận số học. Code prompting nhìn chung vượt trội hơn chain-of-thought (CoT) prompting.
Để hiểu rõ hơn về hiệu suất và hạn chế của code prompting, chúng tôi thực hiện các nghiên cứu
ablation và phân tích lỗi mở rộng, và xác định một số ưu điểm độc quyền của việc sử dụng
symbolic promptings so với ngôn ngữ tự nhiên. Chúng tôi cũng xem xét sự kết hợp của code
prompting và CoT prompting để kết hợp điểm mạnh của cả hai. Cuối cùng, chúng tôi chỉ ra
thông qua thí nghiệm cách các chú thích code và vị trí của chúng ảnh hưởng đến code prompting.

1 Giới thiệu
Những năm gần đây đã chứng kiến những cuộc cách mạng lớn trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) về sự phát triển nhanh chóng đến mức gây sốc của các mô hình ngôn ngữ lớn (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023, Zhang et al., 2022a, Thoppilan et al., 2022, Chowdhery et al., 2022, Raffel et al., 2020]. Định luật mở rộng [Kaplan et al., 2020] đề xuất một cách đầy hứa hẹn để nâng cao mô hình ngôn ngữ bằng cách mở rộng quy mô kích thước mô hình. Theo Qin et al. [2023], OpenAI [2023], LLMs đã thể hiện một số mức độ khả năng giải quyết các nhiệm vụ lý luận phức tạp. Bên cạnh kích thước của LLMs, "prompt engineering" là quan trọng đối với khả năng lý luận. Các phương pháp prompting như chain-of-thought prompting [Kojima et al., 2023, Wei et al., 2023] và least-to-most prompting [Zhou et al., 2023] dẫn LLMs tạo ra các lý do trước câu trả lời cuối cùng và mở khóa một loạt rộng các nhiệm vụ lý luận phức tạp. Tuy nhiên, hiệu suất mô hình phụ thuộc rất nhiều vào thiết kế prompt. Cách tạo ra các prompts đơn giản, hiệu quả và đa mục đích vẫn là một câu hỏi mở được tranh luận sôi nổi.

Một số lượng lớn các công trình đã đề xuất các phương pháp prompting khác nhau để tạo điều kiện cho việc lý luận của LLM [Wei et al., 2023, Zhang et al., 2022b, Kojima et al., 2023, Zhou et al., 2023, Fu et al., 2023, Khot et al., 2023, Press et al., 2022]. Các phương pháp prompting này khuyến khích LLMs tạo ra các bước trung gian bằng ngôn ngữ tự nhiên trước câu trả lời cuối cùng và nâng cao khả năng lý luận của LLMs một cách đáng kể. Tuy nhiên, vẫn còn những hạn chế. Đầu tiên, mặc dù các phương pháp này dẫn LLMs giải quyết các vấn đề từng bước, chúng không thể lập kế hoạch toàn bộ quá trình giải pháp trước, điều này có thể dẫn đến việc rút gọn nhiệm vụ không hợp lý. Nói cách khác, "bước" có thể quá lớn để LLM vượt qua. Ví dụ, trong Hình 1, chúng tôi cho thấy một trường hợp mà LLM được cho là nối các chữ cái cuối cùng của các từ đã cho. Chain-of-thought prompting dẫn LLM đầu tiên trích xuất chữ cái cuối cùng của mỗi từ và sau đó nối tất cả các chữ cái lại với nhau trong một bước. Tuy nhiên, bước này có thể quá khó khăn cho LLM khi số lượng chữ cái tăng lên, dẫn đến câu trả lời sai. Thứ hai, sự mơ hồ

Preprint. Đang được xem xét. arXiv:2305.18507v2 [cs.CL] 7 Oct 2023

--- TRANG 2 ---
Tạo mã python để nối các chữ cái cuối cùng của các từ đã cho. Đây là mã Python để nối các chữ cái cuối cùng của các từ đã cho:
words = ["apple", "banana", "cherry", "date", "elderberry"]
result = ""
for word in words:
result += word[-1]
print(result)

Tạo mã python để nối các chữ cái cuối cùng của các từ đã cho.
Đây là mã Python để nối các chữ cái cuối cùng của các từ đã cho:
words = ["apple", "banana", "cherry", "date", "elderberry"]
for word in words:
result += word[-1]
print(result)

Q: "weapon, reasonable, thin, loyalty"
A:
Hãy suy nghĩ từng bước. In tất cả các biến trung gian.

words = ["weapon", "reasonable", "thin", "loyalty"]
result = ""
for word in words:
last_letter = word[-1]
result += last_letter
print(f"Word: {word}, Last Letter: {last_letter}, Result: {result}")
print(result)

Output:
```
Word: weapon, Last Letter: n, Result: n
Word: reasonable, Last Letter: e, Result: ne
Word: thin, Last Letter: n, Result: nen
Word: loyalty, Last Letter: y, Result: neny
```
Do đó, câu trả lời là "neny".

Mô hình Ngôn ngữ
Mô hình Ngôn ngữ
Code Prompting
Giai đoạn 1
Giai đoạn 2

1. Chữ cái cuối cùng của "weapon" là "n".
2. Chữ cái cuối cùng của "reasonable" là "e".
3. Chữ cái cuối cùng của "thin" là "n".
4. Chữ cái cuối cùng của "loyalty" là "y".
5. Nối các chữ cái này lại với nhau, chúng ta có "ney".
Do đó, câu trả lời là "ney".

Q: Nối các chữ cái cuối cùng của các từ đã cho: "weapon, reasonable, thin, loyalty".
A: Hãy suy nghĩ từng bước.

Mô hình Ngôn ngữ
Chain-of-Thought Prompting
×
√

Hình 1: Các quy trình của zero-shot CoT prompting và zero-shot code prompting.

tự nhiên nằm trong ngôn ngữ tự nhiên. Các lý do của ngôn ngữ tự nhiên cũng có thể gây nhầm lẫn, do đó làm sai lệch quá trình lý luận.

Để giảm thiểu những hạn chế của các phương pháp trên, chúng tôi xem xét việc kết hợp các phương pháp thần kinh với các phương pháp ký hiệu, vì các phương pháp ký hiệu có các biểu diễn và luồng tính toán có cấu trúc, không mơ hồ, điều này có thể làm cho quá trình lý luận chính xác hơn và giảm thiểu ảnh hưởng của sự mơ hồ. Cụ thể, chúng tôi khuyến khích các phương pháp thần kinh (LLMs) đầu tiên tạo ra các bộ giải ký hiệu (như đồ thị tính toán) và để các bộ giải ký hiệu xuất ra các câu trả lời cuối cùng. Công trình trước đây về các phương pháp thần kinh-ký hiệu xây dựng các bước trung gian bằng các ngôn ngữ hình thức thay vì ngôn ngữ tự nhiên [Amini et al., 2019, Chen et al., 2020a,b, Chiang and Chen, 2019, Roy et al., 2015, He-Yueya et al., 2023]. Tuy nhiên, chúng thường bị giới hạn trong một lĩnh vực cụ thể và gặp khó khăn trong việc tổng quát hóa cho các nhiệm vụ lý luận rộng hơn.

Để vượt qua những khó khăn, các công trình gần đây [Gao et al., 2023, Chen et al., 2022, Cheng et al., 2023, Chen et al., 2023] đã cố gắng tạo điều kiện cho lý luận phức tạp trong LLMs bằng các chương trình/mã. Như một công cụ đa mục đích, mã có thể thực thi được bằng máy, có thể được LLM tạo ra, và có thể định nghĩa các đồ thị tính toán linh hoạt. Chúng tôi theo các công trình này và khám phá thêm code prompting, một cách thần kinh-ký hiệu để nâng cao khả năng lý luận của LLMs. Vì kho ngữ liệu mà LLMs được huấn luyện trước mở rộng để chứa mã, LLMs được tự nhiên ban cho khả năng tạo mã theo các hướng dẫn nhất định [Chen et al., 2021]. Chúng tôi hy vọng tận dụng khả năng tạo mã của LLMs để thúc đẩy lý luận. Code prompting của chúng tôi là một phương pháp hai giai đoạn với quy trình được hiển thị trong Hình 1. Để giải quyết một câu hỏi cụ thể, code prompting đầu tiên hướng dẫn LLMs tạo ra một đoạn mã. Để khai thác đầy đủ khả năng lý luận của LLMs thuần túy, chúng tôi khám phá hai cách tận dụng code prompting. Một là hướng dẫn LLMs theo mã được tạo ra để hoàn thành quá trình lý luận và cuối cùng đưa ra câu trả lời. Cách khác là gọi một trình biên dịch bên ngoài để thực thi mã.

Chúng tôi phát hiện trong thí nghiệm rằng mã hoạt động như một sơ đồ tư duy cho LLM và giảm nhiệm vụ thành các nhiệm vụ con được đại diện bởi các phép toán riêng biệt trong mã. Vì các phép toán riêng biệt trong mã thường dễ dàng cho LLM, việc giảm nhiệm vụ tạo điều kiện lớn cho LLMs giải quyết các vấn đề lý luận phức tạp từng bước một cách tự chủ. Bên cạnh đó, mã là một ngôn ngữ hình thức không có sự mơ hồ, điều này loại bỏ

--- TRANG 3 ---
nhầm lẫn gây ra bởi sự mơ hồ trong ngôn ngữ tự nhiên và làm cho quá trình lý luận mạnh mẽ hơn. Về những hạn chế của các phương pháp thần kinh-ký hiệu trước đây, mã là một ngôn ngữ hình thức có tiềm năng thích ứng với một loạt rộng các nhiệm vụ.

Chúng tôi đánh giá code prompting trên 7 tập dữ liệu liên quan đến lý luận ký hiệu và số học. Chúng tôi cho thấy rằng đối với lý luận ký hiệu, zero-shot code prompting vượt trội hơn zero-shot chain-of-thought prompting ít nhất 26.2% (71.0% →97.2%) trong nhiệm vụ nối chữ cái cuối cùng và 11.7% (75.3% → 87.0%) trung bình trong nhiệm vụ tung đồng xu ngay cả khi không có sự hỗ trợ của các công cụ bên ngoài như trình thông dịch Python. Đối với lý luận số học, zero-shot code prompting nhìn chung vượt trội hơn zero-shot chain-of-thought prompting, và few-shot code prompting có tính cạnh tranh cao với các baseline.

Chúng tôi tìm hiểu sâu hơn về các trường hợp thất bại của code prompting. Thông qua phân tích lỗi, chúng tôi xác định một số hạn chế của phương pháp cùng với một số khía cạnh chính của thành công của nó. Hơn nữa, chúng tôi phát hiện rằng code prompting và chain-of-thought prompting có thể dẫn LLM suy nghĩ từ các góc độ khác nhau, đề xuất sự kết hợp của cả hai phương pháp, đạt được 87.95% độ chính xác trên GSM8K.

2 Công trình Liên quan
Kỹ thuật prompt. Gần đây, các phương pháp prompting khác nhau đã xuất hiện để thúc đẩy lý luận của LLM [Wei et al., 2023, Zhang et al., 2022b, Kojima et al., 2023, Zhou et al., 2023, Fu et al., 2023, Khot et al., 2023, Press et al., 2022]. Chúng đã thể hiện hiệu quả trong một loạt rộng các nhiệm vụ lý luận. Được phân loại dựa trên việc có cung cấp các mẫu cụ thể hay không, các phương pháp prompting có thể được chia thành zero-shot prompting và few-shot prompting. Ví dụ, kỹ thuật được sử dụng rộng rãi chain-of-thought prompting có phiên bản zero-shot [Kojima et al., 2023] và phiên bản few-shot [Wei et al., 2023]. Few-shot prompting yêu cầu các minh họa cụ thể cho nhiệm vụ được xây dựng thủ công, vì vậy nó tốn kém hơn. Zhang et al. [2022b] giới thiệu một cách để tự động xây dựng các minh họa few-shot dựa trên zero-shot prompting để tiết kiệm chi phí thủ công. Trong bài báo này, chúng tôi giới thiệu cả zero-shot và few-shot code prompting và cho thấy kết quả cạnh tranh của cả hai phiên bản liên quan đến lý luận ký hiệu và số học. Các phương pháp prompting hiện có cũng có thể được chia thành các phương pháp prompting đơn bước và đa bước. CoT prompting [Kojima et al., 2023, Wei et al., 2023] là một phương pháp prompting đơn bước cổ điển. Một công trình gần đây [Zhou et al., 2023] đã giới thiệu least-to-most prompting, đây là một phương pháp prompting đa bước đầu tiên chia một câu hỏi thành nhiều câu hỏi con và sau đó hướng dẫn LLM giải quyết từng câu hỏi theo trình tự. Least-to-most prompting bị giới hạn ở phiên bản few-shot và yêu cầu việc tạo thủ công cẩn thận các minh họa thay đổi đáng kể từ nhiệm vụ này sang nhiệm vụ khác. Do đó, nó ít tổng quát hơn CoT và code prompting của chúng tôi.

Lý luận được hỗ trợ bởi chương trình. Các công trình trước đây đã cho thấy rằng các chương trình được tạo ra bởi LLMs có thể tạo điều kiện cho lý luận [Gao et al., 2023, Chen et al., 2022, Chowdhery et al., 2022]. Một công trình gần đây chứng minh một ý tưởng tương tự về việc sử dụng mã để gợi ra lý luận của LLM, được gọi là Program-aided Language Models (PAL) [Gao et al., 2023]. Chúng tôi theo công trình này và đào sâu hơn vào code prompting: 1) PAL là một phương pháp prompting few-shot, trong khi chúng tôi điều tra cả zero-shot và few-shot code prompting. Chúng tôi cho thấy rằng zero-shot code prompting phù hợp hoặc thậm chí vượt qua PAL trong các nhiệm vụ lý luận số học. Bên cạnh đó, chúng tôi cũng cải thiện hiệu suất few-shot. 2) Chúng tôi cung cấp một lựa chọn khác về giai đoạn thứ hai của việc tạo ra câu trả lời cuối cùng, cụ thể là trực tiếp yêu cầu LLM tạo ra câu trả lời cuối cùng theo mã, thay vì gọi trình thông dịch Python như trong Gao et al. [2023]. Các thí nghiệm của chúng tôi về việc sử dụng LLM trong giai đoạn thứ hai chứng minh thêm rằng code prompting thực sự hỗ trợ lý luận trong LLM ngay cả khi không thực thi mã và rằng khả năng lý luận ký hiệu và số học là tự chứa đối với LLMs. 3) Chúng tôi thảo luận về một số hạn chế và hiểu biết quan trọng của code prompting thông qua phân tích lỗi. Những điểm này giúp chúng tôi hiểu rõ hơn về sức mạnh của code prompting và thúc đẩy chúng tôi kết hợp CoT và code prompting.

LLMs với các công cụ bên ngoài. Code prompting có tùy chọn sử dụng trình thông dịch Python làm công cụ bên ngoài để hỗ trợ LLM hoàn thành các nhiệm vụ. Khái niệm về việc tăng cường LLMs bằng các công cụ bên ngoài đã thu hút nhiều sự chú ý [Khot et al., 2023, Cheng et al., 2023, Press et al., 2022]. Một công trình tương tự [Cheng et al., 2023] cũng sử dụng các chương trình để hỗ trợ lý luận của LLM. Tuy nhiên, họ tập trung vào việc tạo ra các chương trình SQL hoặc giống SQL để xử lý các câu hỏi về lý luận với bảng. Bên cạnh đó, một công trình gần đây [Chen et al., 2023] giới thiệu một phương pháp dạy LLMs gỡ lỗi chương trình được tạo ra của chúng với sự hỗ trợ của phản hồi từ trình thông dịch mã. Trong công trình của chúng tôi, chúng tôi trang bị code prompting với một kỹ thuật tương tự. Tuy nhiên, chúng tôi xem xét các nhiệm vụ lý luận trong khi Chen et al. [2023] tập trung vào các nhiệm vụ tạo text-to-code hoặc code-to-code.

--- TRANG 4 ---
Tạo mã python để trả lời câu hỏi.
Q: Marie đã đặt một bữa ăn gà có giá $12, 5 gói sữa có giá $3 mỗi gói, 4 quả táo có giá $1.50 mỗi quả, và một số hộp pizza. Marie đã trả tổng cộng $50. Marie đã đặt bao nhiêu hộp pizza nếu mỗi hộp có giá $8.50?

Mô hình Ngôn ngữ

output: 2
chicken_meal = 12
milk_packs = 3*5
apple = 1.5*4
total = chicken_meal + milk_packs + apple + 8.5*x
# where x is the number of boxes of pizza
# we need to find the value of x
# total cost is $50
# so, we can write the equation as:
50 = chicken_meal + milk_packs + apple + 8.5*x
# solving for x
x = (50 - chicken_meal - milk_packs - apple) / 8.5
print("Marie ordered", x, "boxes of pizza.")

Bug report:
50 = chicken_meal + milk_packs + apple + 8.5 * x
^
SyntaxError: cannot assign to literal

Mô hình Ngôn ngữ

chicken_meal = 12
milk_packs = 3*5
apple = 1.5*4
# initialize x to 0
x = 0
total = chicken_meal + milk_packs + apple + 8.5*x
# where x is the number of boxes of pizza
# we need to find the value of x
# total cost is $50
# so, we can write the equation as:
# 50 = chicken_meal + milk_packs + apple + 8.5 * x
# solving for x
x = (50 - chicken_meal - milk_packs - apple) / 8.5
print("Marie ordered", int(x), "boxes of pizza.")

√

Trình Thông dịch Python

Hình 2: Quy trình của "self-debugging".

3 Code Prompting
Code prompting hướng dẫn LLMs giải quyết một vấn đề phức tạp trong hai giai đoạn:

1. Tạo Mã. Trong giai đoạn đầu tiên, prompt yêu cầu LLM tạo mã Python để giải quyết vấn đề. Chúng tôi kiểm tra cả zero-shot và few-shot prompting. Zero-shot prompt đơn giản là "Generate python code to {task description}.". Few-shot prompt bao gồm các mẫu về các câu hỏi theo sau bởi mã tương ứng.

2. Giải pháp Dựa trên Mã. Giai đoạn thứ hai để giải quyết vấn đề dựa trên mã được tạo ra trong giai đoạn đầu tiên. Chúng tôi so sánh hai phương pháp khác nhau. Phương pháp đầu tiên là vẫn yêu cầu LLM tạo ra lý do và giải pháp cho vấn đề theo mã. Phương pháp thứ hai là sử dụng trực tiếp trình thông dịch Python bên ngoài để thực thi mã.

Trong Hình 1, chúng tôi cho thấy quy trình của zero-shot code prompting trong nhiệm vụ nối chữ cái cuối cùng được giới thiệu bởi Wei et al. [2023], đây là một benchmark thường được sử dụng cho lý luận ký hiệu [Wei et al., 2023, Kojima et al., 2023, Zhou et al., 2023, Zhang et al., 2022b]. Trong giai đoạn đầu tiên, chúng tôi sử dụng zero-shot prompt để hướng dẫn LLM xuất ra một đoạn mã cho nhiệm vụ. Cụ thể hơn, chúng tôi prompt LLM với hướng dẫn "Generate python code to concatenate the last letters of the given words.". Trong giai đoạn thứ hai, LLM được cung cấp mã mà nó tạo ra trong giai đoạn đầu tiên và câu hỏi cụ thể. Ngoài ra, chúng tôi thêm câu "Let's think step by step. Print all the intermediate variables." vào prompt để gợi ra thêm khả năng lý luận của LLM.

Như đã đề cập trước đây, giai đoạn đầu tiên (tạo mã) có tùy chọn few-shot prompting thay vì zero-shot prompting, và giai đoạn thứ hai (giải pháp) có tùy chọn sử dụng trình thông dịch mã bên ngoài thay vì prompting LLM. Chúng không được hiển thị trong hình.

Self-debugging Nếu trình thông dịch python được sử dụng trong giai đoạn thứ hai, chúng tôi có thể trang bị code prompting với mô-đun "self-debugging". Nghĩa là, khi trình thông dịch trả về lỗi (không thường xuyên, mã có thể chạy trong hầu hết các trường hợp), chúng tôi chỉ cần thêm mã được tạo ra (có lỗi) và báo cáo lỗi vào đầu vào mô hình gốc để prompt LLM lần nữa. Quy trình của mô-đun "self-debugging" được hiển thị trong Hình 2. Chúng tôi cho thấy trong thí nghiệm rằng các LLMs tiên tiến có khả năng học từ báo cáo lỗi và sửa mã.

Trong các thí nghiệm sau, chúng tôi cho thấy kết quả của code-prompting cho lý luận ký hiệu và lý luận số học. Chúng tôi tiến hành tất cả các thí nghiệm trên mô hình gpt-3.5-turbo (ChatGPT) và đặt nhiệt độ bằng 0 trừ khi có quy định khác.

--- TRANG 5 ---
4 Lý luận Ký hiệu

4.1 Nhiệm vụ
Chúng tôi xem xét hai nhiệm vụ lý luận ký hiệu được giới thiệu bởi Wei et al. [2023] và được sử dụng rộng rãi trong Wei et al. [2023], Kojima et al. [2023], Zhou et al. [2023], Zhang et al. [2022b].

Nối chữ cái cuối cùng. Nhiệm vụ yêu cầu LLMs nối các chữ cái cuối cùng của các từ đã cho. Chúng tôi theo Zhou et al. [2023] để xây dựng danh sách từ bằng cách chọn ngẫu nhiên các từ từ năm nghìn từ của danh sách tần suất Wikipedia. Chúng tôi xây dựng danh sách từ có độ dài 4, 8 và 12. Đối với mỗi độ dài, chúng tôi kiểm tra các phương pháp prompting trên 500 danh sách từ, tạo thành một tập dữ liệu kiểm tra gồm 1,500 mẫu.

Tung đồng xu. Nhiệm vụ yêu cầu LLMs trả lời liệu một đồng xu có còn ngửa lên sau khi một số người đã tung hoặc không tung nó. Số lượng người thay đổi từ 3 đến 5. Đối với mỗi số người nhất định, chúng tôi xây dựng 500 câu hỏi, tạo thành một tập dữ liệu kiểm tra gồm 1,500 mẫu.

4.2 Phương pháp
Chúng tôi xem xét zero-shot standard prompting và zero-shot chain-of-thought prompting làm baseline. Đối với các phương pháp code prompting của chúng tôi, chúng tôi chỉ xem xét zero-shot code prompting vì mã cho mỗi câu hỏi về cơ bản giống nhau nên các mẫu trong few-shot code prompting có thể làm lộ câu trả lời. Chúng tôi cũng hiển thị kết quả của few-shot chain-of-thought prompting để tham khảo.

Zero-shot Standard Prompting. Zero-shot standard prompting có dạng "Q: {question}\nA:". Phương pháp prompting này phản ánh khả năng lý luận ký hiệu của LLM mà không có bất kỳ kỹ thuật prompt nào.

Zero-shot Chain-of-Thought Prompting. Zero-shot chain of thought prompting (zero-shot CoT prompting) được giới thiệu bởi Kojima et al. [2023]. Như được hiển thị trong Hình 1, "Let's think step by step." được thêm vào câu hỏi để gợi ra LLM tạo ra các chuỗi suy nghĩ dẫn đến câu trả lời cuối cùng.

Few-shot Chain-of-Thought Prompting. Few-shot chain of thought prompting (few-shot CoT prompting) được giới thiệu bởi Wei et al. [2023]. Chúng tôi cung cấp 2 mẫu cho nhiệm vụ nối chữ cái cuối cùng và 8 mẫu cho nhiệm vụ tung đồng xu. Xem Phụ lục B để biết các minh họa.

Zero-shot Code Prompting + LLM Self-contained. Như được hiển thị trong Hình 1, phương pháp được đề xuất của chúng tôi prompt LLM đầu tiên tạo ra mã cụ thể cho nhiệm vụ và sau đó theo mã để tạo ra câu trả lời cuối cùng. Đối với zero-shot code prompting + LLM self-contained, chúng tôi sử dụng chính LLM để thực hiện lý luận trong giai đoạn thứ hai. Đối với nhiệm vụ nối chữ cái cuối cùng, prompt của giai đoạn đầu tiên là "Generate python code to concatenate the last letters of the given words."; đối với nhiệm vụ tung đồng xu, prompt giai đoạn đầu tiên là "A coin is heads up, there are some people, each one flipped or didn't flip the coin. Generate python code to determine whether the coin is still heads up." Prompt chúng tôi sử dụng trong giai đoạn thứ hai là "Q: {question}\nA: Let's think step by step. Print all the intermediate variables."

Zero-shot Code Prompting + Python Interpreter. Giai đoạn đầu tiên của phương pháp giống như zero-shot code prompting + LLM self-contained. Trong giai đoạn thứ hai, chúng tôi sử dụng trình thông dịch Python để thực thi mã để có được câu trả lời cuối cùng trực tiếp trong giai đoạn thứ hai.

4.3 Kết quả
Như được hiển thị trong Bảng 1, ngay cả khi không có các công cụ bên ngoài như trình thông dịch Python, code prompting vượt trội hơn các phương pháp prompting baseline (ở đây chúng tôi chủ yếu xem xét zero-shot CoT prompting) ít nhất 26.2% trong nhiệm vụ nối chữ cái cuối cùng và 11.67% trung bình trong nhiệm vụ tung đồng xu. Bên cạnh đó, mức tăng hiệu suất tăng theo độ phức tạp của câu hỏi, ngụ ý rằng code prompting có khả năng tổng quát hóa tốt hơn về lý luận ký hiệu. Cụ thể, mức tăng hiệu suất tăng từ 26.2% lên 72.8% trong nhiệm vụ nối chữ cái cuối cùng khi độ dài danh sách từ tăng từ 4 lên 12, và trong nhiệm vụ tung đồng xu, mức tăng hiệu suất tăng từ 0.0% lên 18.0% khi số lần tung tăng từ 3 lên 5. So với few-shot CoT prompting, zero-shot code prompting + Python interpreter nhìn chung hoạt động tốt hơn. Bên cạnh đó, đáng chú ý là zero-shot code prompting + LLM self-contained đạt được hiệu suất tốt hơn trong nhiệm vụ nối chữ cái cuối cùng mà không cần sự hỗ trợ của trình thông dịch Python. Mặc dù few-shot CoT prompting vượt trội hơn code prompting + LLM self-contained trong nhiệm vụ tung đồng xu, cần lưu ý rằng các phương pháp few-shot tốn kém hơn do cần tạo thủ công các minh họa cụ thể cho nhiệm vụ.

--- TRANG 6 ---
Bảng 1: Độ chính xác (%) của zero-shot standard prompting, zero-shot CoT prompting, few-shot CoT prompting và zero-shot code prompting trên nối chữ cái cuối cùng và tung đồng xu.

nối chữ cái cuối cùng | tung đồng xu
l=4 | l=8 | l=12 | #flips=3 | #flips=4 | #flips=5
zero-shot standard | 7.4 | 2.0 | 2.6 | 22.6 | 17.2 | 16.8
zero-shot CoT | 71.0 | 27.8 | 2.8 | 86.2 | 71.8 | 67.8
few-shot CoT | 94.6 | 69.6 | 39.8 | 99.8 | 99.8 | 99.0
zero-shot code + LLM self-contained | 97.2 | 85.4 | 75.6 | 86.2 | 88.8 | 85.8
zero-shot code + Python interpreter | 99.4 | 99.8 | 99.8 | 99.8 | 99.8 | 99.4

Chúng tôi tóm tắt các lý do có thể cho hiệu suất xuất sắc của code prompting như sau:

1. Trừu tượng hóa và Đơn giản hóa. Code prompting thúc đẩy LLM tạo ra mã từ câu hỏi, điều này có thể được xem như một bước trừu tượng hóa và đơn giản hóa. Ví dụ, trong cả hai nhiệm vụ, code prompting ban cho LLM khả năng trích xuất bản chất "vòng lặp" của câu hỏi và tận dụng bản chất này một cách rõ ràng trong mã sử dụng cú pháp "for" hoặc "while" trong ngôn ngữ Python. LLM có thể hưởng lợi từ việc xác định rõ ràng vòng lặp và chuyển sang chế độ đệ quy của nó.

2. Giảm Nhiệm vụ. Code prompting biến đổi một câu hỏi thành một chương trình. Mỗi dòng mã có thể được xem như một nhiệm vụ con. Các nhiệm vụ con có thể được đại diện như một dòng mã thường dễ dàng để LLM hoàn thành, và do đó việc biến đổi câu hỏi thực sự tạo ra các hướng dẫn thân thiện cho lý luận của LLM. Ví dụ, như được hiển thị trong Hình 1, trong nhiệm vụ nối chữ cái cuối cùng, CoT prompting hướng dẫn LLM tìm chữ cái cuối cùng của mỗi từ trước và sau đó nối chúng lại với nhau. Tuy nhiên, nối tất cả các chữ cái trong một bước có thể quá khó khăn cho LLM khi số lượng chữ cái lớn. Trong khi đó, code prompting dẫn LLM lưu trữ kết quả hiện tại trong một chuỗi và giảm nhiệm vụ thành các bước trích xuất chữ cái cuối cùng của một từ và nối nó vào chuỗi một cách tuần tự. Code prompting kích hoạt việc giảm nhiệm vụ hợp lý hơn, vì độ khó của mỗi bước thấp hơn và không tăng theo số lượng từ.

3. Loại bỏ Sự mơ hồ. Mã được tạo ra bởi LLM có thể được xem như một sự hình thức hóa của câu hỏi loại bỏ sự mơ hồ trong câu hỏi ngôn ngữ tự nhiên gốc. Ngược lại, các phương pháp prompting khác như CoT có thể dẫn đến nhầm lẫn trong quá trình lý luận. Xem Phụ lục C để biết các ví dụ.

4. Mẫu Rõ ràng. Thông qua quan sát từng trường hợp, chúng tôi phát hiện rằng mã được tạo ra trong giai đoạn đầu tiên hoạt động như một mẫu rõ ràng, một sơ đồ tư duy cho LLM để tiến hành quá trình lý luận. LLM theo mã để hoàn thành nhiệm vụ. Để so sánh, CoT prompting không thể cung cấp một mẫu rõ ràng như vậy trước khi lý luận, điều này được chứng minh trong các thí nghiệm là quan trọng.

5 Lý luận Số học

5.1 Nhiệm vụ
Chúng tôi xem xét năm tập dữ liệu được sử dụng phổ biến cho nhiệm vụ lý luận số học. (1) SingleEq [Koncel-Kedziorski et al., 2015], (2) AddSub [Hosseini et al., 2014], (3) MultiArith [Roy and Roth, 2016], (4) SVAMP [Patel et al., 2021], (5) GSM8K [Cobbe et al., 2021]. SingleEq, AddSub và MultiArith là từ benchmark MAWPS (MAth Word ProblemS) được giới thiệu bởi Koncel-Kedziorski et al. [2016]. Trong số các tập dữ liệu, SingleEq và AddSub chỉ cần tính toán một bước, trong khi MultiArith, GSM8K và SVAMP chứa các bài toán khó hơn yêu cầu lý luận đa bước. Xem Phụ lục A.2 để biết chi tiết hơn về từng tập dữ liệu.

5.2 Phương pháp
Zero-shot Chain-of-Thought Prompting. Phương pháp này giống như trong §4.2.

Zero-shot Code Prompting. Trong giai đoạn đầu tiên, prompt là "Generate python code to answer the question. \nQ:{question}". Trong giai đoạn thứ hai, chúng tôi sử dụng trình thông dịch Python bên ngoài. Chúng tôi thiết kế thêm một số mô-đun để tăng cường code prompting, bao gồm self-debugging, irrelevant information (irr) và equation instruction (equ). Self-debugging được mô tả trong §3. Đối với irrelevant information, chúng tôi chỉ cần thêm một hướng dẫn "There may be irrelevant information in the

--- TRANG 7 ---
Bảng 2: Độ chính xác (%) của zero-shot CoT prompting, zero-shot code prompting, few-shot CoT prompting, PAL Gao et al. [2023] và few-shot code prompting trên lý luận số học.

SingleEq | AddSub | SVAMP | MultiArith | GSM8K

zero-shot methods
zero-shot CoT | 93.11 | 86.08 | 78.20 | 96.67 | 81.58
zero-shot code (+self-debug) | 97.64 | 91.65 (+irr) | 79.40 | 96.67 | 78.92(+equ)

few-shot methods
few-shot CoT | 95.47 | 90.63 | 77.10 | 98.50 | 82.11
PAL | 97.64 | 89.11 | 79.50 | 97.00 | 79.45
few-shot code (+self-debug) | 97.44 | 91.39 (+irr) | 79.60 | 97.33 | 80.59

question. If you find it, ignore it." vào zero-shot prompt. Đối với equation instruction, chúng tôi cung cấp một hướng dẫn về giải phương trình trong Python sử dụng gói sympy. Xem Phụ lục D để biết chi tiết hơn.

Few-shot Chain-of-Thought Prompting. Chúng tôi xem xét few-shot CoT prompting được giới thiệu bởi Wei et al. [2023] làm baseline. Quy trình được hiển thị trong Phụ lục D. Chúng tôi sử dụng 3 mẫu được giới thiệu bởi Wei et al. [2023] cho tất cả các bài toán từ toán học. Xem Phụ lục D để biết các mẫu.

Program-aided Language Models. Chúng tôi cũng xem xét công trình gần đây Program-aided Language Models (PAL) được giới thiệu bởi Gao et al. [2023] làm baseline. PAL là một phương pháp prompting few-shot sử dụng chương trình như prompts. Chúng tôi chạy phương pháp trên gpt-3.5-turbo sử dụng cùng 3 mẫu.

Few-shot Code Prompting. Trong giai đoạn đầu tiên, chúng tôi sử dụng cùng 3 câu hỏi như few-shot CoT prompting để hướng dẫn LLM tạo ra mã. Chúng tôi thêm chú thích để tăng tính dễ đọc và tăng cường thêm phương pháp. Trong giai đoạn thứ hai, chúng tôi sử dụng trình thông dịch Python bên ngoài. Chúng tôi cũng thêm các mô-đun tăng cường, về cơ bản giống như trong zero-shot code prompting. Xem Phụ lục D để biết chi tiết hơn. Chúng tôi cũng thí nghiệm việc sử dụng LLMs để thực thi mã trong giai đoạn thứ hai, kết quả được hiển thị trong Phụ lục D.

5.3 Kết quả
Bảng 2 trình bày kết quả. Lưu ý rằng few-shot code prompting trong bảng thêm chú thích ở cuối mỗi dòng mã. Zero-shot code prompting vượt trội hơn các baseline lần lượt 4.53%, 5.57% và 1.20% trong SingleEq, AddSub và SVAMP. Tuy nhiên, zero-shot code prompting kém hơn zero-shot CoT prompting 1.52% trong GSM8K. Few-shot code prompting nhìn chung vượt trội hơn PAL với sự hỗ trợ của chú thích, ngoại trừ khoảng cách 0.20% trong SingleEq. Bên cạnh đó, few-shot code prompting vượt trội hơn few-shot CoT prompting lần lượt 1.97% trong SingleEq, 0.76% trong AddSub và 2.50% trong SVAMP. Tuy nhiên, code prompting kém hơn few-shot CoT prompting lần lượt 1.17% và 1.52% trong MultiArith và GSM8K. Chúng tôi sẽ thảo luận về các lỗi trong §5.4.

Đáng chú ý là zero-shot code prompting tạo ra hiệu suất cạnh tranh với few-shot CoT prompting và PAL. So với few-shot CoT prompting, zero-shot code prompting hoạt động tốt hơn trong SingleEq, AddSub và SVAMP lần lượt 2.17%, 1.02% và 2.30%. So với PAL, zero-shot code prompting hoạt động tốt hơn trong AddSub, và khoảng cách hiệu suất trong các tập dữ liệu khác không quá 0.53%. Điều này chứng minh rằng với các tăng cường phù hợp, zero-shot code prompting gần với hiệu suất few-shot.

5.4 Phân tích lỗi
AddSub. Trong AddSub, chúng tôi phát hiện rằng mã được tạo ra bởi LLM có xu hướng chứa các biến không liên quan, do đó làm xáo trộn nó. Xem Phụ lục E để biết các ví dụ. Quan sát này cho thấy việc tạo mã bởi LLMs dễ bị phân tâm bởi thông tin không liên quan. Điều này thúc đẩy việc tăng cường irr.

MultiArith. Trong MultiArith, các câu trả lời sai từ code prompting chủ yếu do hai lý do: 1) Code prompting nhạy cảm hơn với sự mơ hồ trong câu hỏi. Xem Phụ lục E để biết các ví dụ. (Đặc điểm của code prompting được thảo luận chi tiết trong §6). 2) Đáp án đúng cho một số câu hỏi là sai, trong khi dự đoán là đúng. Các ví dụ được liệt kê trong Phụ lục E.

--- TRANG 8 ---
Bảng 4: Độ chính xác (%) của các phương pháp few-shot code prompting có chú thích hoặc không có chú thích được thêm vào các vị trí khác nhau của mỗi dòng mã. "Self-debugging" không được sử dụng ở đây.

method | SingleEq | AddSub | MultiArith | SVAMP | GSM8K
few-shot code w/o ann | 97.64 | 89.11 | 97.00 | 79.50 | 79.45
few-shot code w/ ann at the end | 97.44 | 89.87 | 97.33 | 79.60 | 80.21
few-shot code w/ ann at the beginning | 96.85 | 89.87 | 95.33 | 79.90 | 77.71

GSM8K. Đối với tập dữ liệu GSM8K, chúng tôi quan sát rằng LLM gặp khó khăn trong việc giải phương trình trong mã. Xem các ví dụ lỗi trong Phụ lục E và Phụ lục E. Điều này thúc đẩy "equation instruction". Bên cạnh đó, chúng tôi phát hiện thông qua thống kê rằng sự chồng chéo lỗi của code prompting và CoT prompting rất nhỏ, cho thấy hai phương pháp prompting dẫn LLMs suy nghĩ từ các góc độ khác nhau. (Xem Phụ lục E để biết chi tiết hơn.)

5.5 Kết hợp CoT và code prompting

Bảng 3: Độ chính xác (%) của các phương pháp kết hợp trên GSM8K.

method | GSM8K
few-shot CoT | 81.58
few-shot code | 79.68
ensemble (only CoT votes) | 87.49
ensemble (only code votes) | 83.85
ensemble (both methods vote) | 87.95

Về mặt khái niệm tương tự như Wang et al. [2023], chúng tôi xem xét việc kết hợp CoT prompting và code prompting dựa trên bỏ phiếu. Đối với mỗi câu hỏi, nếu cả hai phương pháp prompting tạo ra cùng một câu trả lời, chúng tôi chấp nhận câu trả lời đó làm câu trả lời cuối cùng; nếu không, chúng tôi đặt nhiệt độ thành 0.7 và yêu cầu LLM tạo ra n câu trả lời theo từng phương pháp prompting. Sau đó chúng tôi bỏ phiếu trong số 2n câu trả lời để đưa ra câu trả lời cuối cùng. Chúng tôi cũng so sánh các phương pháp bỏ phiếu có 2n câu trả lời được tạo ra từ cùng một phương pháp prompting. Như được hiển thị trong Bảng 3, các phương pháp kết hợp vượt trội hơn các baseline một cách đáng kể. Hơn nữa, việc kết hợp hai phương pháp prompting vượt qua chỉ code prompting votes hoặc chỉ CoT prompting votes như chúng tôi mong đợi. Xem Phụ lục D để biết chi tiết thí nghiệm.

5.6 Nghiên cứu Ablation
Chúng tôi thêm chú thích trong các mẫu của few-shot code prompting. Để khám phá xem có nên và ở đâu chúng tôi nên thêm chú thích, chúng tôi tiến hành các thí nghiệm ablation sau đây.

Chú thích có quan trọng không? Chúng tôi so sánh code prompting có và không có chú thích. Như được hiển thị trong Bảng 4, việc thêm chú thích nhìn chung làm tăng độ chính xác. Điều này chứng minh lợi ích của việc thêm chú thích cho few-shot code prompting. Chú thích được viết bằng ngôn ngữ tự nhiên có thể hỗ trợ LLM hiểu các mẫu tốt hơn và tạo ra mã tốt hơn.

Vị trí thêm chú thích có quan trọng không? Chúng tôi thí nghiệm về code prompting với chú thích được thêm vào đầu hoặc cuối mỗi dòng mã. Như được hiển thị trong Bảng 4, trong hầu hết các trường hợp, việc thêm chú thích ở cuối là lựa chọn tốt hơn. Chúng tôi nghi ngờ điều này do thói quen viết mã của con người. Chúng ta có xu hướng thêm chú thích mô tả cụ thể từng dòng mã ở cuối mỗi dòng thường xuyên hơn so với trước mã, do đó kho ngữ liệu huấn luyện chứa nhiều mẫu mã hơn với chú thích ở cuối mỗi dòng.

Chú thích giúp ích cho các câu hỏi khó hơn hay dễ hơn? Chúng tôi thí nghiệm trên GSM8K để tìm hiểu mức tăng hiệu suất của việc thêm chú thích đến từ đâu. Chúng tôi xem xét số từ trong câu trả lời được cung cấp như một khái niệm về độ khó. Hình 3 cho thấy mức tăng hiệu suất của việc thêm chú thích vào các câu hỏi có độ khó khác nhau. Mức tăng hiệu suất trên các câu hỏi dễ nhất và khó nhất lớn hơn so với các câu hỏi có độ khó trung bình.

Self-debugging. Hình 4 cho thấy độ chính xác của zero-shot và few-shot code prompting có hoặc không có "self-debugging" trên 5 tập dữ liệu số học. Rõ ràng là hiệu suất trên các benchmark khó hơn có nhiều khả năng hưởng lợi từ "self-debugging". Điều này là do các vấn đề phức tạp hơn có nhiều khả năng kích hoạt lỗi trong mã. Trong khi đó, ít lỗi được quan sát trong SingleEq và MultiArith.

--- TRANG 9 ---
Thông tin không liên quan. Thông qua phân tích lỗi của AddSub trong §5.4, chúng tôi cho thấy rằng hiệu suất mô hình có thể bị ảnh hưởng bởi thông tin không liên quan trong câu hỏi. Vì vậy chúng tôi đề xuất "irrelevant information", được giới thiệu trong §5.2. Bảng 5 cho thấy mặc dù đơn giản, nó hiệu quả cải thiện hiệu suất. Lưu ý rằng các phương pháp code prompting trong bảng đều được trang bị "self-debugging".

[Hình 3: Độ chính xác của few-shot code prompting có/không có chú thích trên các câu hỏi có độ khó khác nhau.]

[Hình 4: Độ chính xác của zero-shot và few-shot code prompting có/không có "self-debugging" trên các tập dữ liệu số học.]

Hướng dẫn phương trình. Như đã đề cập trong §5.4, việc LLMs giải phương trình trong mã là một thách thức. Vì vậy chúng tôi đưa ra một hướng dẫn về việc sử dụng sympy để tận dụng Python giải phương trình (xem Phụ lục D). Bên cạnh đó, chúng tôi cũng thí nghiệm việc yêu cầu LLM trực tiếp giải phương trình trong chú thích. Bảng 6 cho thấy lựa chọn tốt nhất là dạy LLM sử dụng các gói Python nhất định để giải phương trình. Lưu ý rằng các phương pháp code prompting trong bảng đều có "self-debugging".

Bảng 5: Độ chính xác (%) của code prompting có hoặc không có "irrelevant information" trên AddSub.

method | AddSub
zero-shot code | 89.62
zero-shot code +irr | 91.65
few-shot code | 89.87
few-shot code +irr | 91.39

Bảng 6: Độ chính xác (%) của code prompting có hoặc không có "equation instruction" trên GSM8K.

method | GSM8K
zero-shot code | 77.40
zero-shot code +equ(ann) | 78.09
zero-shot code +equ(sympy) | 78.92

6 Thảo luận

[Hình 5: Phân phối câu trả lời trên các câu hỏi có và không có sự mơ hồ.]

Loại bỏ sự mơ hồ. Theo suy nghĩ được đề cập trong §5.4, chúng tôi phát hiện thông qua thí nghiệm rằng code prompting có tiềm năng phát hiện sự mơ hồ trong một câu hỏi. Ở đây chúng tôi sử dụng 5 trường hợp mà các câu hỏi có sự mơ hồ và chúng tôi có thể sửa chữa thủ công. Chúng tôi kiểm tra few-shot code prompting và CoT prompting trên cả câu hỏi có và không có sự mơ hồ. Đối với mỗi câu hỏi, chúng tôi tạo ra 15 câu trả lời bằng code prompting và CoT prompting tương ứng với nhiệt độ của LLM được đặt thành 0.7. Hình 5 hiển thị biểu đồ của các câu trả lời. Rõ ràng rằng sự mơ hồ làm xáo trộn code prompting, trong khi CoT prompting ít nhạy cảm hơn với sự mơ hồ. Chúng tôi có thể tận dụng tính năng này để phát hiện sự mơ hồ trong câu hỏi.

Phương pháp Zero-shot so với phương pháp few-shot Khi các Mô hình Ngôn ngữ Lớn phát triển về kích thước và căn chỉnh con người, khoảng cách giữa các phương pháp prompting zero-shot và few-shot dường như thu hẹp [Zhong et al., 2023]. Chúng tôi đã chỉ ra trong bài báo rằng zero-shot code prompting có tính cạnh tranh với các phương pháp few-shot hiện tại, bao gồm few-shot CoT [Wei et al., 2023] và

--- TRANG 10 ---
PAL [Gao et al., 2023]. Tuy nhiên, những lý do đằng sau điều này vẫn chưa được khám phá.

7 Kết luận
Chúng tôi giới thiệu code prompting, một phương pháp prompting thần kinh-ký hiệu với cả phiên bản zero-shot và few-shot. Chúng tôi tiến hành thí nghiệm trên 7 benchmark phổ biến liên quan đến cả lý luận ký hiệu và số học và cho thấy rằng cả zero-shot và few-shot code prompting đều nâng cao lý luận phức tạp trong LLMs. Các thí nghiệm và phân tích mở rộng xác minh tính hiệu quả của các kỹ thuật self-debugging, annotation, equation instruction và irrelevant information của chúng tôi, và trình bày nhiều hiểu biết.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài được dịch tương ứng...]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 12 ---
[Tiếp tục danh sách tài liệu tham khảo...]

A Tập dữ liệu
A.1 Lý luận ký hiệu
[Bảng mô tả tập dữ liệu cho lý luận ký hiệu...]

A.2 Lý luận số học
[Bảng mô tả tập dữ liệu cho lý luận số học...]

--- TRANG 13 ---
[Tiếp tục phụ lục với các ví dụ chi tiết...]

--- TRANG 14 ---
[Các ví dụ về Zero-shot standard prompting...]

--- TRANG 15 ---
[Các ví dụ về Zero-shot CoT prompting...]

--- TRANG 16 ---
[Các ví dụ về Few-shot CoT prompting...]

--- TRANG 17 ---
[Mã được tạo ra trong giai đoạn đầu tiên...]

--- TRANG 18 ---
[Các ví dụ kết quả của zero-shot code prompting...]

--- TRANG 19 ---
[Các ví dụ về coin flip...]

--- TRANG 20 ---
[Tiếp tục các ví dụ...]

--- TRANG 21 ---
[Các ví dụ về Few-shot CoT prompting cho coin flip...]

--- TRANG 22 ---
[Mã được tạo ra cho coin flip...]

--- TRANG 23 ---
[Các ví dụ kết quả của zero-shot code prompting cho coin flip...]

--- TRANG 24 ---
[Phân tích lỗi cho lý luận ký hiệu...]

--- TRANG 25 ---
[Chi tiết thí nghiệm cho lý luận số học...]

--- TRANG 26 ---
[Quy trình của few-shot prompting...]

--- TRANG 27 ---
[Các prompt PAL...]

--- TRANG 28 ---
[Các prompt few-shot code...]

--- TRANG 29 ---
[Few-shot code + LLM self-contained...]

--- TRANG 30 ---
[Tiếp tục các prompt...]

--- TRANG 31 ---
[Phân tích lỗi cho lý luận số học...]

--- TRANG 32 ---
[Các ví dụ lỗi cho MultiArith...]

--- TRANG 33 ---
[Phân phối lỗi và biểu đồ kết thúc...]
