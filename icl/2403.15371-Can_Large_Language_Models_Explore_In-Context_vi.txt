# 2403.15371.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2403.15371.pdf
# Kích thước tệp: 1692632 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Các Mô Hình Ngôn Ngữ Lớn Có Thể Khám Phá Trong Ngữ Cảnh Không?
Akshay Krishnamurthy∗1, Keegan Harris†2, Dylan J. Foster1,
Cyril Zhang1, và Aleksandrs Slivkins∗1
1Microsoft Research
2Carnegie Mellon University
Phiên bản đầu tiên: Tháng 3 năm 2024
Phiên bản này: Tháng 10 năm 2024‡
Tóm tắt
Chúng tôi điều tra mức độ mà các Mô Hình Ngôn Ngữ Lớn (LLM) đương đại có thể tham gia vào khám phá, một khả năng cốt lõi trong học tăng cường và ra quyết định. Chúng tôi tập trung vào hiệu suất bản địa của các LLM hiện có, không có can thiệp huấn luyện. Chúng tôi triển khai LLM như các tác nhân trong môi trường tên cướp nhiều tay đơn giản, chỉ định mô tả môi trường và lịch sử tương tác hoàn toàn trong ngữ cảnh, tức là trong lời nhắc LLM. Chúng tôi thử nghiệm với Gpt-3.5, Gpt-4, và Llama2, sử dụng nhiều thiết kế lời nhắc khác nhau, và phát hiện rằng các mô hình không tham gia vào khám phá một cách mạnh mẽ mà không có can thiệp đáng kể: i) Qua tất cả các thí nghiệm của chúng tôi, chỉ một cấu hình dẫn đến hành vi khám phá thỏa đáng: Gpt-4 với suy luận chuỗi tư duy và lịch sử tương tác được tóm tắt bên ngoài, được trình bày dưới dạng thống kê đầy đủ; ii) Tất cả các cấu hình khác không dẫn đến hành vi khám phá mạnh mẽ, bao gồm cả những cấu hình có suy luận chuỗi tư duy nhưng lịch sử chưa được tóm tắt. Mặc dù những phát hiện này có thể được diễn giải tích cực, chúng gợi ý rằng việc tóm tắt bên ngoài—có thể không khả thi trong các môi trường phức tạp hơn—là quan trọng để có được hành vi mong muốn từ các tác nhân LLM. Chúng tôi kết luận rằng các can thiệp thuật toán không tầm thường, chẳng hạn như tinh chỉnh hoặc tuyển chọn tập dữ liệu, có thể cần thiết để trao quyền cho các tác nhân ra quyết định dựa trên LLM trong các môi trường phức tạp.

1 Giới thiệu
Học trong ngữ cảnh là một khả năng nổi bật quan trọng của các Mô Hình Ngôn Ngữ Lớn (LLM) cho phép sử dụng LLM được huấn luyện trước để giải quyết vấn đề bằng cách chỉ định mô tả vấn đề và dữ liệu liên quan hoàn toàn trong ngữ cảnh, tức là trong lời nhắc LLM, mà không cần cập nhật các tham số LLM (Brown et al., 2020). Ví dụ, có thể nhắc LLM với các vector hiệp biến số và mục tiêu vô hướng và sau đó thu được dự đoán kiểu hồi quy từ mô hình bằng cách bao gồm các vector hiệp biến mới trong lời nhắc (Garg et al., 2022). Có lẽ đáng ngạc nhiên, LLM không được huấn luyện rõ ràng cho hành vi này; thay vào đó, các thuật toán cơ bản được sử dụng cho học trong ngữ cảnh được trích xuất từ kho ngữ liệu huấn luyện và nổi lên ở quy mô.

Kể từ khi được khám phá trong mô hình Gpt-3 (Brown et al., 2020), học trong ngữ cảnh đã trở thành chủ đề của một nhóm nghiên cứu ngày càng phát triển. Các công trình này bao gồm các điều tra lý thuyết về các cơ chế cơ bản (ví dụ, Xie et al., 2021; Akyürek et al., 2022), các thăm dò thực nghiệm (ví dụ, Garg et al.,

∗Tác giả liên hệ: {akshaykr,slivkins}@microsoft.com.
†Một số kết quả được thu thập khi tác giả là thực tập sinh tại Microsoft Research.
‡Được chấp nhận xuất bản tại Advances in Neural Information Processing Systems, NeurIPS, 2024.

1arXiv:2403.15371v3 [cs.LG] 28 Oct 2024

--- TRANG 2 ---
2022; Kirsch et al., 2022), và các công trình tận dụng học trong ngữ cảnh trong các ứng dụng (ví dụ, Xu et al., 2022; Som et al., 2023; Edwards et al., 2023). Văn học này chủ yếu nghiên cứu học trong ngữ cảnh cho dự đoán hoặc các nhiệm vụ học có giám sát, và trong khi tiến bộ lý thuyết còn ở giai đoạn sơ khai, hiểu biết của chúng ta về cách sử dụng học có giám sát trong ngữ cảnh (ICSL) trong thực tế đang nhanh chóng hình thành.

Mặc dù học có giám sát là một khả năng quan trọng, nhiều ứng dụng đòi hỏi việc sử dụng các mô hình ML cho ra quyết định hạ lưu. Do đó, học tăng cường trong ngữ cảnh (ICRL) và ra quyết định tuần tự là biên giới tự nhiên tiếp theo. LLM đã được sử dụng như các tác nhân ra quyết định trong các ứng dụng từ thiết kế thí nghiệm trong khoa học tự nhiên (Lee et al., 2023b) đến chơi game (Shinn et al., 2023; Wang et al., 2023), nhưng hiểu biết của chúng ta—về mặt lý thuyết và vận hành—về ICRL kém phát triển hơn nhiều so với ICSL. Cho đến nay, chúng ta thiếu hiểu biết có hệ thống về việc liệu LLM có thể được coi là các tác nhân ra quyết định có mục đích chung hay không.

Các tác nhân ra quyết định phải sở hữu ba khả năng cốt lõi: tổng quát hóa (cần thiết cho học có giám sát), khám phá (đưa ra quyết định có thể không tối ưu trong ngắn hạn để thu thập thêm thông tin) và lập kế hoạch (để tính đến hậu quả dài hạn của quyết định). Trong bài báo này, chúng tôi tập trung vào khám phá, khả năng có chủ đích thu thập thông tin để đánh giá các lựa chọn thay thế và giảm bất định. Một loạt bài báo gần đây (Laskin et al., 2022; Lee et al., 2023a; Raparthy et al., 2023) chứng minh hành vi học tăng cường trong ngữ cảnh (bao gồm khám phá) trong các mô hình transformer khi chúng được huấn luyện rõ ràng để tạo ra hành vi này bằng cách sử dụng dữ liệu từ các tác nhân học tăng cường hoặc minh chứng chuyên gia về các nhiệm vụ liên quan. Việc huấn luyện như vậy có xu hướng tốn kém, tốn thời gian và có thể đặc thù cho nhiệm vụ. Cụ thể, những phát hiện này không làm sáng tỏ việc liệu hành vi khám phá có biểu hiện trong LLM có mục đích chung được thu thập qua phương pháp huấn luyện tiêu chuẩn hay không, điều này gợi ý câu hỏi cơ bản sau:

Các LLM đương đại có thể hiện khả năng khám phá trong ngữ cảnh không?

Đóng góp. Chúng tôi điều tra câu hỏi này bằng cách triển khai LLM như các tác nhân trong các vấn đề học tăng cường tổng hợp đơn giản, cụ thể là tên cướp nhiều tay (MAB) (Slivkins, 2019; Lattimore và Szepesván, 2020), chỉ định mô tả môi trường và lịch sử tương tác hoàn toàn trong lời nhắc LLM. Tên cướp nhiều tay là một loại vấn đề RL cổ điển và được nghiên cứu kỹ lưỡng, cô lập sự đánh đổi giữa khám phá và khai thác, tức là đưa ra quyết định tốt nhất dựa trên dữ liệu có sẵn. Chúng cũng là một khối xây dựng cơ bản hướng tới ra quyết định tuần tự tổng quát; khả năng giải quyết MAB là điều kiện tiên quyết cho các nhiệm vụ học tăng cường thách thức hơn. Tính đơn giản, tính trung tâm của chúng đối với RL, và tập trung vào khám phá so với khai thác làm cho MAB trở thành lựa chọn tự nhiên để nghiên cứu có hệ thống khả năng khám phá trong ngữ cảnh của LLM.

Chúng tôi đánh giá hành vi khám phá trong ngữ cảnh của Gpt-3.5 (Brown et al., 2020), Gpt-4 (OpenAI, 2023), và Llama2 (Touvron et al., 2023) trong môi trường MAB, sử dụng nhiều thiết kế lời nhắc khác nhau. Trong các thí nghiệm của chúng tôi, chúng tôi phát hiện rằng chỉ một cấu hình duy nhất (tức là một thiết kế lời nhắc và cặp LLM) dẫn đến hành vi khám phá thỏa đáng. Tất cả các cấu hình khác thể hiện thất bại khám phá, không hội tụ về quyết định tốt nhất (cánh tay) với xác suất đáng kể. Chúng tôi phát hiện rằng điều này thường xảy ra do thất bại hậu tố, trong đó LLM không chọn cánh tay tốt nhất thậm chí một lần sau một số vòng ban đầu (tức là trong một số "hậu tố thời gian"). Tình huống này được phản ánh trong Hình 1(a): cụ thể, Gpt-4 với thiết kế lời nhắc cơ bản của chúng tôi gặp thất bại hậu tố trong >60% các bản sao. Một chế độ thất bại thay thế mà chúng tôi xác định là khi LLM hành xử "đồng nhất", chọn tất cả các cánh tay gần như đều nhau và không thu hẹp về những cánh tay tốt hơn.

Cấu hình duy nhất thành công trong các thí nghiệm của chúng tôi bao gồm sự kết hợp của Gpt-4 và lời nhắc "nâng cao" mà (a) cung cấp gợi ý khuyến khích khám phá, (b) tóm tắt bên ngoài lịch sử tương tác thành trung bình mỗi cánh tay, và (c) yêu cầu LLM sử dụng suy luận chuỗi tư duy không-shot (Wei et al., 2022; Kojima et al., 2022). Cấu hình này được hiển thị trong Hình 1(b). Có thể diễn giải phát hiện này một cách tích cực: LLM tiên tiến nhất có sở hữu khả năng khám phá mạnh mẽ, miễn là lời nhắc được thiết kế cẩn thận để kích thích hành vi này. Mặt khác, chúng tôi phát hiện rằng cùng cấu hình không có tóm tắt bên ngoài thì thất bại, điều này dẫn đến diễn giải tiêu cực: LLM có thể thất bại trong việc khám phá trong môi trường phức tạp hơn, nơi việc tóm tắt bên ngoài lịch sử là một vấn đề thiết kế thuật toán không tầm thường.¹

Chúng tôi kết luận rằng trong khi thế hệ LLM hiện tại có lẽ có thể khám phá trong môi trường RL đơn giản với kỹ thuật lời nhắc thích hợp, các can thiệp huấn luyện—theo tinh thần của Lee et al. (2023a); Raparthy et al. (2023)—có thể cần thiết để trao cho LLM khả năng khám phá tinh vi hơn được yêu cầu cho các môi trường phức tạp hơn.

Phương pháp luận. Một thách thức kỹ thuật cơ bản trong việc đánh giá khả năng và hạn chế của LLM là phải tìm kiếm không gian kết hợp lớn của các thiết kế lời nhắc trong khi thu được kết quả có ý nghĩa thống kê, tất cả trong khi đáp ứng các ràng buộc tài chính và tính toán liên quan đến LLM. Việc đánh giá học bandit trong ngữ cảnh thậm chí còn thách thức hơn vì (a) tính ngẫu nhiên trong môi trường đòi hỏi mức độ sao chép cao để có ý nghĩa thống kê và (b) độ phức tạp mẫu của học/khám phá đòi hỏi rằng thậm chí một thí nghiệm đơn lẻ cũng cần hàng trăm hoặc hàng nghìn truy vấn LLM để thu được kích thước hiệu ứng có ý nghĩa (tức là phân tách giữa các phương pháp thành công và thất bại). Để giải quyết những vấn đề này, đóng góp kỹ thuật cốt lõi của chúng tôi là xác định các thống kê thay thế như chẩn đoán cho thất bại khám phá dài hạn. Các thống kê thay thế mà chúng tôi xem xét đặc trưng cho thất bại khám phá dài hạn, nhưng có thể được đo lường ở quy mô vừa phải với ít bản sao và chân trời học tập ngắn, ngay cả khi thước đo hiệu suất tiêu chuẩn (cụ thể là phần thưởng) quá nhiễu để hữu ích.

2 Thiết lập thí nghiệm
Tên cướp nhiều tay (MAB). Chúng tôi xem xét một biến thể tên cướp nhiều tay cơ bản, tên cướp Bernoulli ngẫu nhiên. Có K hành động có thể (cánh tay), được đánh chỉ số là [K] := {1, . . . , K}. Mỗi cánh tay a được liên kết với phần thưởng trung bình μa ∈ [0,1], là không biết. Một tác nhân tương tác với môi trường trong T bước thời gian, trong đó ở mỗi bước thời gian t ∈ [T] tác nhân chọn một cánh tay at ∈ [K] và nhận phần thưởng rt ∈ {0,1} được rút độc lập từ phân phối Bernoulli với trung bình μat. Do đó, thể hiện MAB được xác định bởi phần thưởng trung bình (μa : a ∈ [K]) và chân trời thời gian T. Mục tiêu là tối đa hóa tổng phần thưởng, tương ứng với việc xác định cánh tay tốt nhất: một cánh tay có phần thưởng trung bình cao nhất. Một đặc điểm chính của thiết lập MAB là phần thưởng cho các cánh tay không được tác nhân chọn sẽ không được tiết lộ, do đó khám phá là cần thiết để xác định cánh tay tốt nhất.

Chúng tôi tập trung vào các thể hiện MAB trong đó cánh tay tốt nhất có phần thưởng trung bình μ⋆ = 0.5 + Δ/2 với tham số Δ > 0, trong khi tất cả các cánh tay khác có phần thưởng trung bình μ = 0.5 − Δ/2 (vậy, Δ = μ⋆ − μ là khoảng cách giữa cánh tay tốt nhất và cánh tay tốt thứ hai). Thể hiện chính mà chúng tôi xem xét có K = 5 cánh tay và khoảng cách Δ = 0.2. Chúng tôi gọi đây là thể hiện khó, vì chúng tôi cũng xem xét thể hiện dễ với K = 4 và Δ = 0.5.²

Lời nhắc. Chúng tôi sử dụng LLM để hoạt động như các tác nhân ra quyết định tương tác với các thể hiện MAB bằng cách nhắc chúng với mô tả vấn đề MAB (bao gồm chân trời thời gian T) và lịch sử tương tác cho đến thời điểm hiện tại. Thiết kế lời nhắc của chúng tôi cho phép một số lựa chọn độc lập. Đầu tiên là "kịch bản", cung cấp nền tảng cho vấn đề ra quyết định, định vị LLM hoặc a) như một tác nhân chọn nút để nhấn, hoặc b) như một công cụ khuyến nghị hiển thị quảng cáo cho người dùng. Thứ hai, chúng tôi chỉ định "khung" là a) gợi ý rõ ràng về

¹Ví dụ, nếu có nhiều cánh tay, hoặc nếu chúng ta xem xét tên cướp ngữ cảnh với nhiều ngữ cảnh, thì chúng ta có thể chỉ chơi mỗi cánh tay (cặp ngữ cảnh-cánh tay) một vài lần, vì vậy việc tính trung bình phần thưởng riêng biệt cho mỗi—như chúng ta làm trong các thí nghiệm—không cung cấp nhiều tóm tắt. (Xem Mục 5 để thảo luận thêm.)

²Khoảng cách Δ lớn hơn làm cho việc phân biệt các cánh tay dễ dàng hơn, trong khi K nhỏ hơn có nghĩa là có ít lựa chọn thay thế hơn để khám phá.

3

--- TRANG 3 ---
0 50 100 150 200 250 300 350 400 450
Lượt chơi cánh tay tốt nhất trong vòng [0,500]0.00.20.40.60.81.0Tỷ lệ bản saoTS
UCBTham lam
GPT4
0 50 100 150 200 250 300 350 400
Bước thời gian (t)0.00.20.40.60.8Tần suất Thất bại Hậu tố @ tTS
UCBTham lam
GPT4
0 100 200 300 400 500
Bước thời gian (t)0.400.450.500.550.60Phần thưởng trung bình theo thời gianTS
UCB
Tham lamGPT4
TỐI ƯU
0 25 50 75 100 125 150 175
Lượt chơi cánh tay tốt nhất trong vòng [0,200]0.00.20.40.60.81.0Tỷ lệ bản saoTS
UCBTham lam
GPT4-CoT
0 20 40 60 80 100 120 140 160
Bước thời gian (t)0.00.20.40.60.8Tần suất Thất bại Hậu tố @ tTS
UCBTham lam
GPT4-CoT
0 25 50 75 100 125 150 175 200
Bước thời gian (t)0.400.450.500.550.60Phần thưởng trung bình theo thời gianTS
UCB
Tham lamGPT4-CoT
TỐI ƯU
Nút, 5-cánh tay, Delta=0.2Hình 1: Các thí nghiệm đại diện: Hai cấu hình lời nhắc cho Gpt-4 trên vấn đề tên cướp 5 cánh tay, chứng minh thất bại khám phá (trên) và thành công (dưới). Các đường cơ sở là hai thuật toán bandit tiêu chuẩn với đảm bảo hiệu suất, Upper Confidence Bound (UCB) và Thompson Sampling (TS), cũng như thuật toán Tham lam, luôn chọn cánh tay có phần thưởng trung bình tốt nhất cho đến nay và được biết là hoạt động kém. Các hình ảnh là: (Trái) biểu đồ trên các bản sao về số lần cánh tay tốt nhất được chọn, (Giữa) cho mỗi t, chúng tôi vẽ tần suất thất bại hậu tố, tỷ lệ bản sao mà cánh tay tốt nhất không bao giờ được chọn sau bước thời gian t, và (Phải) phần thưởng trung bình tích lũy theo thời gian, trung bình trên các bản sao.

(a) Hàng trên. Gpt-4 với thiết kế lời nhắc cơ bản của chúng tôi với nhiệt độ không. Thí nghiệm chạy trong T = 500 vòng, và được sao chép N = 20 lần, thay đổi tính ngẫu nhiên môi trường. Cấu hình này thể hiện hành vi hai cực cao: một phần lớn (>60%) các bản sao chọn cánh tay tốt nhất chỉ một số ít lần và thể hiện thất bại hậu tố, tương tự như Tham lam, và rất khác với UCB và TS. Điều này gợi ý về thất bại dài hạn trong khám phá và, thực sự, cấu hình này hoạt động kém đáng kể về mặt phần thưởng.

(b) Hàng dưới. Gpt-4 với khung gợi ý, lịch sử tóm tắt, và chuỗi tư duy với nhiệt độ không. Thí nghiệm chạy trong T = 200 vòng và được sao chép N = 40 lần. Cấu hình này thể hiện phân phối một cực về lượt chơi cánh tay tốt nhất, rất ít thất bại hậu tố, và phần thưởng có thể so sánh với TS.

nhu cầu cân bằng khám phá và khai thác, hoặc b) trung lập. Thứ ba, lịch sử có thể được trình bày dưới dạng a) danh sách thô theo vòng, hoặc nó có thể b) được tóm tắt thông qua số lượt chơi và phần thưởng trung bình của mỗi cánh tay. Thứ tư, câu trả lời cuối cùng được yêu cầu có thể là a) một cánh tay đơn, hoặc b) một phân phối trên các cánh tay. Cuối cùng, chúng tôi a) yêu cầu chỉ câu trả lời, hoặc b) cũng cho phép LLM cung cấp giải thích "chuỗi tư duy" (CoT). Tổng cộng, những lựa chọn này dẫn đến 2⁵ = 32 thiết kế lời nhắc, được minh họa trong Hình 2. Thêm chi tiết về thiết kế lời nhắc, bao gồm các ví dụ, được cung cấp trong Phụ lục A.

Thiết kế lời nhắc cơ bản nhất từ các tùy chọn trên sử dụng kịch bản nút, khung trung lập, và lịch sử thô, và yêu cầu LLM trả về chỉ một cánh tay không có CoT. Mỗi trong năm sửa đổi có thể đối với lời nhắc này đều có thể giúp LLM, và các thí nghiệm của chúng tôi đánh giá điều này. Ví dụ, cả kịch bản quảng cáo và khung gợi ý đều có thể giúp gọi ra

4

--- TRANG 4 ---
kiến thức của LLM về thuật toán bandit (vì thuật toán bandit thường được sử dụng trong khuyến nghị nội dung). Tóm tắt lịch sử có thể giúp nếu LLM không thể tóm tắt lịch sử một cách đáng tin cậy (có lẽ do lỗi số học³) và/hoặc không hoàn toàn nhận ra rằng nó nên làm như vậy. Trả về phân phối có thể giúp nếu LLM có thể xác định phân phối tốt, nhưng thất bại trong việc lấy mẫu chính xác từ nó. Cuối cùng, chuỗi tư duy được biết là giúp ích trong nhiều tình huống LLM khác nhau (Wei et al., 2022; Malach, 2023), ngay cả khi được sử dụng theo cách zero-shot (Kojima et al., 2022) như chúng tôi làm ở đây.

Hình 2: Thiết kế lời nhắc; xem Hình 11 để có cái nhìn chi tiết hơn. Một lời nhắc được tạo ra bằng cách đi qua biểu đồ từ trên xuống dưới.

Lời nhắc được trình bày cho mỗi LLM bằng cách sử dụng cả thông điệp hệ thống và người dùng (được hiển thị bởi cả ba API LLM). Thông điệp hệ thống trình bày thông tin về kịch bản và khung và nhắc LLM về việc có sử dụng CoT hay không và có trả về phân phối hay không (và như thế nào). Thông điệp người dùng trình bày lịch sử và nhắc nhở LLM về cách định dạng phản hồi của nó. Chỉ riêng với Gpt-4, chúng tôi phát hiện rằng việc nhắc LLM sử dụng CoT trong lời nhắc hệ thống không đáng tin cậy tạo ra đầu ra CoT, vì vậy—chỉ với Gpt-4—chúng tôi cũng xem xét thiết kế lời nhắc CoT tăng cường thêm nhắc nhở LLM sử dụng CoT ở cuối lời nhắc người dùng. Xem Phụ lục A để biết ví dụ.

Cấu hình LLM. Chúng tôi thử nghiệm với ba LLM: Gpt-3.5, Gpt-4, và Llama2.⁴ Ngoài các biến thể lời nhắc trên, chúng tôi cũng xem xét hai lựa chọn cho tham số nhiệt độ, 0 và 1. Nhiệt độ 0 buộc LLM phải xác định và do đó cô lập hành vi khám phá "có chủ đích" của bản thân LLM. Nhiệt độ 1 cung cấp nguồn ngẫu nhiên bên ngoài trong phản hồi LLM, có thể hoặc không thể dẫn đến việc ngẫu nhiên hóa giữa các cánh tay. Cho phép LLM trả về phân phối thay vì cánh tay đơn cũng cung cấp tính ngẫu nhiên bên ngoài (khi chúng tôi lấy mẫu từ phân phối được trả về); để cô lập các nguồn ngẫu nhiên, chúng tôi không xem xét nhiệt độ 1 với thiết kế lời nhắc "trả về phân phối".

Chúng tôi gọi tuple (thiết kế lời nhắc, nhiệt độ) là cấu hình LLM. Chúng tôi xác định mỗi cấu hình với "mã" 5 chữ cái L₁L₂L₃L₄L₅, với chữ cái Lᵢ biểu thị các lựa chọn:

• L₁: 'B' hoặc 'A' cho, tương ứng, kịch bản nút hoặc quảng cáo;
• L₂: 'N' hoặc 'S' cho, tương ứng, khung trung lập hoặc gợi ý;
• L₃: 'R' hoặc 'S' cho, tương ứng, lịch sử thô hoặc tóm tắt;
• L₄: 'C' hoặc 'eC' hoặc 'N' cho, tương ứng, chuỗi tư duy, CoT tăng cường, hoặc không CoT.
• L₅: '0', '1' hoặc 'D' cho, tương ứng, nhiệt độ và trả về phân phối (với nhiệt độ 0).

Chúng tôi gọi "BNRN0" là cấu hình cơ bản từ đây trở đi. Hầu hết các thí nghiệm của chúng tôi xem xét kịch bản "nút", và chúng tôi sử dụng kịch bản "quảng cáo" chủ yếu như một kiểm tra tính mạnh mẽ. Với Gpt-3.5 và Llama2, chúng tôi không xem xét CoT tăng cường vì nó không cần thiết để tạo ra đầu ra CoT đáng tin cậy; do đó, chúng tôi có 48 cấu hình tổng cộng cho hai LLM này. Với Gpt-4, chúng tôi

³Ví dụ, LLM đôi khi thất bại trong số học cơ bản (Gao et al., 2023; Liu et al., 2024), mặc dù điều này có thể sẽ cải thiện trong tương lai gần thông qua huấn luyện tốt hơn và/hoặc tích hợp các công cụ giống như máy tính.

⁴Cụ thể: GPT-3.5-Turbo-0613 (phát hành 13/06/2023), GPT-4-0613 (phát hành 13/06/2023), và Llama2-13B-chat được lượng tử hóa xuống 4-bit (Dettmers và Zettlemoyer, 2023).

5

--- TRANG 5 ---
chủ yếu sử dụng CoT tăng cường, nhưng đã thử nghiệm với một số thiết kế lời nhắc CoT tiêu chuẩn; do đó, có 72 cấu hình tổng cộng cho Gpt-4.

Đường cơ sở. Với đường cơ sở, chúng tôi xem xét hai thuật toán MAB tiêu chuẩn, UCB (Auer et al., 2002a) và Thompson Sampling (TS) (Thompson, 1933), được tối ưu theo nghĩa lý thuyết nhất định và cũng khá hiệu quả trong thực tế. Chúng tôi cũng xem xét thuật toán Tham lam, không khám phá và được biết là thất bại.⁵ Mặc dù tất cả ba đường cơ sở đều có tham số có thể điều chỉnh, chúng tôi không thực hiện điều chỉnh tham số nào (xem Mục 4.1 để mô tả chi tiết về mỗi thuật toán với các thiết lập tham số). Ngoài những đường cơ sở này, một số thí nghiệm của chúng tôi bao gồm thuật toán ε-Tham lam⁶ với nhiều lựa chọn ε khác nhau để chứng minh định lượng sự đánh đổi giữa khám phá và khai thác. Chúng tôi chạy 1000 bản sao cho mỗi đường cơ sở và mỗi thể hiện MAB (với phần thưởng được thực hiện độc lập qua các bản sao).

Quy mô thí nghiệm. Tập hợp thí nghiệm chính của chúng tôi có chân trời thời gian T = 100. Để tính đến tính ngẫu nhiên trong phần thưởng (và có thể trong LLM, qua nhiệt độ) chúng tôi chạy N ∈ {10,20} bản sao cho mỗi cấu hình LLM và mỗi thể hiện bandit, với phần thưởng được tạo ra độc lập qua các bản sao. Như một kiểm tra tính mạnh mẽ, chúng tôi chạy một thí nghiệm duy nhất trên Gpt-4 với cấu hình cơ bản cho T = 500 vòng (với N = 20), và thu được kết luận nhất quán/mạnh mẽ hơn, được mô tả trong Hình 1(a).

Chi tiết hơn, với Gpt-3.5 chúng tôi sử dụng N = 20 bản sao qua tất cả 48 cấu hình lời nhắc, dẫn đến ≈200K truy vấn tổng cộng. Gpt-4 đắt hơn một bậc độ lớn, chậm hơn đáng kể về thông lượng, và chịu tắc nghẽn không thể đoán trước. Như vậy, chúng tôi chỉ sử dụng N = 10 bản sao qua 10 cấu hình lời nhắc đại diện.⁷ Để kiểm tra tính mạnh mẽ bổ sung, chúng tôi chạy bốn cấu hình Gpt-4 với T = 200, hai cho N = 20 bản sao và hai cho N = 40 bản sao. Tổng cộng, điều này dẫn đến ≈50K truy vấn được gửi đến Gpt-4. Llama2 về cơ bản miễn phí từ quan điểm của chúng tôi (vì nó được lưu trữ cục bộ), nhưng hiệu suất của nó liên tục kém; chúng tôi giới hạn các thí nghiệm của mình ở thể hiện MAB khó, 32 cấu hình, và N = 10 bản sao.

Chúng tôi nhấn mạnh rằng các thí nghiệm bandit với LLM khá tốn kém về tiền và thời gian. Chúng cần N·T truy vấn LLM cho mỗi cấu hình LLM và mỗi thể hiện MAB được thử nghiệm. Cả N và T đều phải tương đối lớn để có được kết quả có ý nghĩa thống kê: N điều chỉnh mức độ ý nghĩa và phải lớn để vượt qua tính ngẫu nhiên trong thực hiện phần thưởng, trong khi T điều chỉnh kích thước hiệu ứng và phải lớn để các thuật toán tốt có đủ thời gian để xác định cánh tay tối ưu. Cả hai vấn đề đều rõ ràng hơn trong các thể hiện MAB khó hơn (nhiều cánh tay K và/hoặc khoảng cách nhỏ Δ), nhưng thất bại khám phá cũng có xu hướng ít thường xuyên hơn trong các thể hiện MAB (rất) dễ.⁸ Hơn nữa, chúng tôi cần bao phủ không gian của các thiết kế lời nhắc có thể, về cơ bản là vô hạn lớn, để đảm bảo rằng các phát hiện của chúng tôi không overfitting một thiết kế cụ thể. Do đó, lý tưởng là chúng tôi sẽ lấy N, T, số thể hiện MAB, và số lời nhắc khá lớn, nhưng làm như vậy là không khả thi trong thực tế.⁹ Thay vào đó, chúng tôi sử dụng khoảng cách vừa phải nhỏ Δ = 0.2, lựa chọn vừa phải lớn cho N ∈ {10,20} và T = 100, và không gian thiết kế lời nhắc như mô tả ở trên.

Như chúng ta sẽ thấy dưới đây, những lựa chọn này (cụ thể, N ∈ {10,20} và T = 100 và Δ = 0.2) không cung cấp đủ sức mạnh thống kê để phân biệt giữa các phương pháp thành công và không thành công dựa

⁵Trong mỗi vòng, Tham lam chọn cánh tay có phần thưởng trung bình lớn nhất cho đến nay. Thuật toán được khởi tạo với một mẫu của mỗi cánh tay. Nó thất bại ở chỗ với xác suất không đổi, nó không bao giờ chọn cánh tay tốt nhất sau khởi tạo.

⁶ε-Tham lam là thuật toán MAB tiêu chuẩn mà trong mỗi vòng chọn cánh tay ngẫu nhiên đều với xác suất đã cho ε, và khai thác (tức là bắt chước Tham lam) trong trường hợp khác.

⁷Chính xác, N = 10 cho kịch bản nút, và N = 3 cho kiểm tra tính mạnh mẽ với kịch bản quảng cáo.

⁸Ví dụ, Tham lam luôn thành công khi khoảng cách là Δ = 1, tức là không có nhiễu, và thành công tầm thường với xác suất ít nhất (1 + Δ)²/4 khi các mẫu ban đầu đánh giá 1 cho cánh tay tốt và 0 cho cánh tay xấu.

⁹Lời nhắc lịch sử thô và đầu ra chuỗi tư duy đặc biệt đắt đỏ, vì các API LLM tính phí theo token.

6

--- TRANG 6 ---
chỉ trên phần thưởng tích lũy. Thay vì tiếp tục tăng quy mô thí nghiệm, điều không khả thi trong thực tế, chúng tôi dựa vào các thống kê thay thế có thể được phát hiện ở quy mô vừa phải của chúng tôi, và có tính gợi ý cao về thất bại khám phá dài hạn/bền vững. Các kiểm tra tính mạnh mẽ của chúng tôi với T và N lớn hơn, cũng như các phát hiện định tính mà chúng tôi báo cáo dưới đây cung cấp bằng chứng hỗ trợ cho phương pháp luận này.

3 Kết quả thí nghiệm
Trong mục này, chúng tôi trình bày các phát hiện thí nghiệm của mình, bắt đầu với tóm tắt trong Mục 3.1. Trong Mục 3.2 chúng tôi điều tra chi tiết các cấu hình LLM thất bại, và trong Mục 3.3 chúng tôi tập trung vào cấu hình LLM thành công duy nhất mà các thí nghiệm của chúng tôi xác định. Cuối cùng, trong Mục 3.4 chúng tôi cố gắng chẩn đoán các nguyên nhân cơ bản của thất bại khám phá.

3.1 Tổng quan
Chúng tôi phát hiện rằng tất cả trừ một trong các cấu hình LLM mà chúng tôi xem xét đều thể hiện thất bại khám phá, không hội tụ về cánh tay tốt nhất với xác suất đáng kể. Điều này xảy ra hoặc do thất bại hậu tố, trong đó LLM không bao giờ chọn cánh tay tốt nhất sau một số ít vòng ban đầu, hoặc (trong một số ít cấu hình) do thất bại giống đồng nhất, trong đó LLM chọn tất cả các cánh tay với tỷ lệ gần như đồng nhất, không loại bỏ các cánh tay hoạt động kém. Ngoại lệ duy nhất là Gpt-4 với cấu hình BSSeC0, tức là với kịch bản nút, khung gợi ý, lịch sử tóm tắt, CoT tăng cường, và nhiệt độ 0.

Chúng tôi tóm tắt các phát hiện chính trong Hình 3 và Hình 4. Hình 3 tóm tắt tập hợp thí nghiệm chính (mà chúng tôi nhớ lại xem xét thể hiện MAB khó), hình dung mỗi cấu hình LLM với một điểm duy nhất trên biểu đồ phân tán trong đó các trục tương ứng với hai thống kê thay thế, SuffFailFreq và MinFrac, đại diện cho cường độ của hai chế độ thất bại (SuffFailFreq

7

--- TRANG 7 ---
đo lường thất bại hậu tố, và K·MinFrac đo lường thất bại giống đồng nhất); những thống kê này được mô tả chi tiết trong phần tiếp theo. Hình 4 hiển thị SuffFailFreq, MinFrac, GreedyFrac (đo lường mức độ tương tự của một phương pháp với Tham lam), và các thống kê tóm tắt bổ sung cho mỗi cấu hình Gpt-4 trong tập hợp thí nghiệm chính. Những thống kê này tiết lộ rằng tất cả các cấu hình LLM, ngoại trừ Gpt-4-BSSeC0 (ngôi sao xanh trong Hình 3), hành xử khác biệt cơ bản so với các thuật toán cơ sở UCB và TS, và chúng tôi phát hiện rằng những khác biệt này dẫn đến sự sụt giảm hiệu suất lớn, bền vững. Ngược lại, chúng tôi phát hiện rằng Gpt-4-BSSeC0 khám phá thành công và (do đó) hội tụ về cánh tay tốt nhất.

3.2 Xác định thất bại
Bây giờ chúng tôi đưa ra tổng quan chính xác về các thất bại khám phá được minh họa trong Hình 3 và Hình 4, và cung cấp kết quả và hình ảnh bổ sung minh họa thất bại chi tiết hơn. Chúng tôi tập trung vào Gpt-4, vì chúng tôi phát hiện rằng Gpt-3.5 và Llama2 hoạt động kém hơn (và thường kém hơn nhiều) trong tất cả các thí nghiệm; kết quả chi tiết cho Gpt-3.5 và Llama2 được bao gồm trong Phụ lục B để hoàn thiện. Chúng tôi bắt đầu với nền tảng chi tiết về các thống kê thay thế, SuffFailFreq và MinFrac, được sử dụng để định lượng thất bại trong Hình 3 và 4 và tiếp theo, cung cấp bằng chứng rằng thất bại khám phá—được định lượng bởi những thống kê này—dẫn đến sự sụt giảm hiệu suất bền vững.

Thất bại hậu tố. Hầu hết các cấu hình LLM mà chúng tôi xem xét thể hiện hành vi hai cực cao, theo đó một phần lớn các bản sao chọn cánh tay tốt nhất rất hiếm, và một vài bản sao hội tụ về cánh tay tốt nhất cực kỳ nhanh. Phù hợp với hành vi hai cực này, chúng tôi quan sát tỷ lệ cao thất bại hậu tố, trong đó cánh tay tốt nhất không được chọn thậm chí một lần sau một số ít vòng ban đầu (tức là trong một số "hậu tố thời gian"). Thất bại hậu tố gợi ý về thất bại dài hạn trong khám phá không thể được cải thiện bằng cách chạy thuật toán lâu hơn, bởi vì, không chơi cánh tay tối ưu, người ta không thể thu thập thông tin để học rằng nó thực sự tối ưu. Những hành vi như vậy về mặt định tính tương tự với những hành vi của Tham lam và về mặt định tính rất khác với những hành vi của UCB và Thompson Sampling.

Thống kê thay thế của chúng tôi để đo lường thất bại hậu tố được định nghĩa như sau: Đối với một bản sao thí nghiệm R và vòng t, cho SuffFail(t, R) là biến nhị phân bằng 1 nếu cánh tay tốt nhất không bao giờ được chọn trong các vòng [t, T]. Sau đó cho SuffFailFreq(t) := mean({SuffFail(t, R) : bản sao R}). Thất bại hậu tố biểu hiện trong hầu hết các thí nghiệm của chúng tôi ở T = 100. Trong biểu đồ phân tán trong Hình 3, trục X vẽ SuffFailFreq(T/2) cho mỗi cấu hình LLM, và chúng tôi phát hiện rằng tất cả trừ năm cấu hình có SuffFailFreq(T/2) ≥ 15%. Nhớ lại định nghĩa của thất bại hậu tố, điều này có nghĩa là ≥15% thời gian, những cấu hình này không kéo cánh tay tốt nhất thậm chí một lần trong nửa cuối các vòng.

Có thể có được cái nhìn chi tiết hơn về thất bại hậu tố và hành vi hai cực bằng cách tập trung vào từng cấu hình LLM riêng lẻ. Chúng tôi hình dung điều này cho cấu hình cơ bản (Gpt-4-BNRN0) trong Hình 1 (trên) cho T = 500, và trong Hình 5 cho Gpt-4 (BNRN0 và BNRN1) ở T = 100. Trong những cái nhìn chi tiết này, các bảng điều khiển giữa vẽ SuffFailFreq(t) ở mỗi thời điểm t cho các cấu hình LLM đã cho, cũng như UCB, TS, và Tham lam. Chúng tôi phát hiện rằng những cấu hình LLM này có tỷ lệ thất bại hậu tố cao hơn nhiều so với cả UCB và TS. Hành vi hai cực được hình dung trong bảng điều khiển trái của mỗi biểu đồ, trong đó đối với mỗi cấu hình, một phần lớn các bản sao hiếm khi kéo cánh tay tốt nhất, trong khi phần còn lại hầu như luôn kéo cánh tay tốt nhất. Do hành vi hai cực này (đặc biệt là vì một phần không đổi các bản sao tình cờ hầu như luôn kéo cánh tay tốt nhất), thất bại hậu tố không được phản ánh đầy đủ trong các biểu đồ tổng phần thưởng trong các bảng điều khiển phải của Hình 5, vì chân trời thời gian T = 100 không đủ lớn. Tuy nhiên, như đã đề cập, thất bại hậu tố gợi ý về thất bại không thể phục hồi trong khám phá dẫn đến sự khác biệt rõ rệt trong phần thưởng cho T lớn hơn. Đây chính xác là những gì chúng tôi tìm thấy ở T = 500 trong Hình 1, điều gợi ý rằng thất bại hậu tố thực sự dẫn đến hiệu suất dài hạn kém.

Thất bại giống đồng nhất. Quay lại bảng điều khiển trái của Hình 3, chúng ta thấy rằng ba cấu hình Gpt-4 tránh được thất bại hậu tố. Hai trong số những cấu hình này thể hiện một loại thất bại khác, trong đó LLM chọn cánh tay theo tỷ lệ gần như bằng nhau trong toàn bộ T vòng và không khai thác thông tin thu được để tập trung vào các cánh tay tốt hơn. Chúng tôi gọi đây là thất bại giống đồng nhất.

Thống kê thay thế của chúng tôi để đo lường những thất bại như vậy được định nghĩa như sau: Đối với một bản sao thí nghiệm cụ thể R và vòng t, cho fa(t, R) là tỷ lệ các vòng trong [1, t] mà cánh tay a đã cho được chọn, MinFrac(t, R) := minaafa(t, R), và MinFrac(t) := mean({MinFrac(t, R) : bản sao R}). Vì MinFrac(t) ≤ 1/K, ∀t ∈ [T], chúng tôi luôn vẽ K·MinFrac(t), để quy mô lại phạm vi thành [0, 1]. MinFrac(t) lớn hơn tương ứng với việc chọn cánh tay đồng nhất hơn ở thời điểm t. Khi MinFrac(t) của LLM không giảm theo thời gian và duy trì lớn hơn đáng kể so với các đường cơ sở (đặc biệt là khi t tiến đến chân trời thời gian T), chúng tôi coi đó là dấu hiệu của thất bại giống đồng nhất. Trục Y của Hình 3 ghi lại K·MinFrac(T) cho mỗi cấu hình, trong đó chúng ta thấy rằng trong ba cấu hình Gpt-4 tránh được thất bại hậu tố, hai cấu hình có MinFrac(T) rất cao so với UCB và TS (cấu hình thứ ba là Gpt-4-BSSeC0, thành công). Hai cấu hình này là Gpt-4-BNRND và Gpt-4-BSSCD, cả hai đều sử dụng định dạng đầu ra phân phối. Chúng tôi cung cấp cái nhìn chi tiết hơn về Gpt-4-BNRND (cũng như Gpt-4-BNSND, cũng thể hiện thất bại giống đồng nhất, nhưng chỉ khác với Gpt-4-BNRND trong việc sử dụng lịch sử tóm tắt) trong Hình 6, xem xét chân trời dài hơn và nhiều bản sao hơn (T = 200 và N = 20). Bảng điều khiển giữa tiết lộ rằng K·MinFrac(t) không giảm theo thời gian cho những cấu hình LLM này, trong khi nó làm như vậy cho các đường cơ sở. Hành vi này dẫn đến không có thất bại hậu tố, nhưng dẫn đến phần thưởng thấp hơn nhiều so với các đường cơ sở. Cụ thể, chúng tôi thu được sự tách biệt rõ ràng trong tổng phần thưởng, cho thấy rằng thất bại giống đồng nhất thực sự dẫn đến hiệu suất dài hạn kém.

Tính tổng quát của các thất bại. Để tóm tắt, Hình 3 cho thấy rằng tất cả các cấu hình LLM ngoại trừ Gpt-4-BSSeC0 đều thể hiện thất bại hậu tố hoặc thất bại đồng nhất cho thể hiện MAB khó và kịch bản nút. Các biểu đồ phân tán cho ba thí nghiệm khác (tức là kịch bản quảng cáo và/hoặc thể hiện MAB dễ) về mặt định tính tương tự và được hoãn lại Phụ lục B.

Cùng dữ liệu, nhưng với các phân bổ cho các cấu hình LLM cụ thể, được trình bày cho tất cả các cấu hình Gpt-4 trong Hình 4; các bảng tương tự cho các LLM khác và các thiết lập thí nghiệm được đưa ra trong Phụ lục B. Vì không có tính hướng dẫn để trình bày các biểu đồ chi tiết như Hình 5 cho mỗi cấu hình LLM, Hình 4 tóm tắt hiệu suất của mỗi cấu hình chỉ với một vài thống kê. Chúng tôi bao gồm:

• SuffFailFreq(T/2) và MinFrac(T), được định nghĩa ở trên.
• MedianReward: trung vị quy mô lại (trên các bản sao) của tổng phần thưởng trung bình theo thời gian.¹⁰
• GreedyFrac: tỷ lệ các vòng tham lam, trung bình trên các bản sao. Vòng tham lam là vòng mà cánh tay có phần thưởng trung bình lớn nhất được chọn. Đây là một cách để định lượng mức độ mà một cấu hình hành xử giống Tham lam.

Bây giờ chúng tôi tóm tắt các phát hiện thêm từ các biểu đồ phân tán (Hình 3 và 12) và các bảng tóm tắt (Hình 13 đến 19). Đầu tiên, Gpt-4 hoạt động tốt hơn nhiều so với Gpt-3.5, và Llama2 hoạt động kém hơn nhiều (cụ thể, tần suất thất bại hậu tố cho Llama2 dao động từ của Tham lam đến lớn hơn nhiều). Thứ hai, chúng tôi quan sát rằng tất cả LLM đều nhạy cảm với những thay đổi nhỏ trong thiết kế lời nhắc. Tuy nhiên, các sửa đổi khác nhau mà chúng tôi xem xét dường như tương tác với nhau, và khó xác định sửa đổi riêng lẻ nào cải thiện hiệu suất và sửa đổi nào làm suy giảm nó.

3.3 Điều tra thành công
Trên thể hiện MAB khó, cấu hình duy nhất trong các thí nghiệm của chúng tôi tránh được cả thất bại hậu tố và thất bại giống đồng nhất là Gpt-4 với thiết kế lời nhắc BSSeC0. Như có thể thấy từ Hình 4, ở

¹⁰ Chính xác hơn, cho Φ(R) là tổng phần thưởng trung bình theo thời gian cho một bản sao R đã cho. Sau đó E[Φ(R)] dao động trong khoảng [1/2 − Δ/2, 1/2 + Δ/2]. Chúng tôi quy mô lại Φ(R), bằng cách dịch chuyển và nhân, để E[Φ(R)] dao động trong [0, 1].

10

--- TRANG 8 ---
T = 100, cấu hình này không có thất bại hậu tố, giá trị K·MinFrac chỉ lớn hơn TS một chút, và phần thưởng có thể so sánh với TS. Những thống kê này gợi ý rằng cấu hình này thành công, và trong mục này chúng tôi trình bày bằng chứng thêm hỗ trợ tuyên bố này.

Để làm như vậy, chúng tôi chạy Gpt-4-BSSeC0 trên thể hiện MAB khó với T = 200 và N = 40 để thu được kết quả có ý nghĩa thống kê hơn. Chúng tôi cũng xem xét Gpt-4-BSReC0, thay đổi lịch sử tóm tắt thành lịch sử thô, như một ablation. Hình 7 cung cấp tóm tắt kết quả từ thí nghiệm này, trong khi Hình 1(b) cung cấp cái nhìn chi tiết về cấu hình BSSeC0. Các hình tiết lộ rằng BSSeC0 tiếp tục tránh thất bại hậu tố và hoạt động tương đối tốt về mặt phần thưởng cho T lớn hơn. Mặt khác, chúng ta thấy rằng BSReC0 thể hiện một phần không tầm thường của thất bại hậu tố, chứng minh rằng ablation này dẫn đến hành vi khác biệt cơ bản.

Chúng tôi cũng cung cấp hai hình dung bổ sung cung cấp một số bằng chứng định tính hướng tới

11

--- TRANG 9 ---
thành công của BSSeC0, cũng như thất bại của các cấu hình khác. Những hình ảnh này được trình bày trong Hình 8 và Hình 9. Trong Hình 8, chúng tôi hình dung cánh tay được chọn ở mỗi bước thời gian cho nhiều bản sao khác nhau của một số phương pháp khác nhau (LLM và đường cơ sở). Cụ thể, Hình 8 hiển thị bốn bản sao cho cấu hình cơ bản (BNRN0) và hai cấu hình với CoT tăng cường (BSReC0 và BSSeC0), cũng như một bản sao của mỗi thuật toán đường cơ sở. Chúng ta thấy rằng cấu hình cơ bản BNRN0 có xu hướng cam kết với một cánh tay duy nhất trong nhiều vòng, một hành vi tương tự như của Tham lam và rất khác với cả UCB và TS. BSReC0 cũng cam kết trong thời gian dài, nhưng ít hơn so với cấu hình cơ bản. Ngược lại, BSSeC0 chuyển đổi cánh tay thường xuyên hơn nhiều, và về mặt định tính dường như tương tự hơn nhiều với TS.

Trong Hình 9, chúng tôi vẽ tỷ lệ các vòng trong [0, t] mà cánh tay tối ưu được kéo như một hàm của t cho từng bản sao riêng lẻ. BSReC0 về mặt trực quan tương tự như UCB, ngoại trừ một phần không tầm thường các lần chạy thể hiện thất bại hậu tố (các đường cong hội tụ về 0 trên biểu đồ). Trong khi đó, BSSeC0 về mặt trực quan tương tự như TS, với hầu như tất cả các bản sao từ từ hội tụ về 1. Những hình dung này, cùng với các thống kê tóm tắt, gợi ý rằng BSSeC0 hành xử tương tự nhất với TS, điều này tiếp tục gợi ý rằng nó sẽ hội tụ thành công về cánh tay tối ưu với chân trời thời gian đủ dài.

3.4 Nguyên nhân gốc rễ
Các phát hiện thí nghiệm của chúng tôi ở trên làm sáng tỏ cách các tác nhân ra quyết định dựa trên LLM hành xử, nhưng cũng đáng để hiểu tại sao chúng hành xử theo cách đó (và đặc biệt, tại sao chúng thất bại). Câu hỏi này khá thách thức để trả lời một cách quyết định, nhưng hai giả thuyết tự nhiên là các cấu hình mà chúng tôi xem xét (ngoài Gpt-4-BSSeC0) hoặc a) quá tham lam, hoặc b) quá giống đồng nhất. Trong mục này, chúng tôi mô tả cách các thí nghiệm của chúng tôi cung cấp một số hiểu biết sâu sắc về giả thuyết này.

Đầu tiên, tập trung vào Gpt-4, các thí nghiệm của chúng tôi tiết lộ hành vi khác biệt định tính giữa các thể hiện dễ và khó (Hình 13(a) và Hình 13(c)). Thực sự, thể hiện dễ dường như dễ dàng hơn nhiều; hầu hết các cấu hình Gpt-4 tránh thất bại hậu tố và tích lũy phần thưởng lớn trên thể hiện này, và thống kê GreedyFrac cung cấp giải thích tiềm năng về lý do tại sao. Trên thể hiện dễ, hầu hết các cấu hình Gpt-4 có giá trị GreedyFrac rất cao, do đó chúng hành xử tương tự như Tham lam, hoạt động khá tốt (mặc dù Tham lam có thể chứng minh thất bại với xác suất không đổi nhỏ và, theo thực nghiệm, có nhiều thất bại hậu tố trên thể hiện này).¹¹ Một giả thuyết hợp lý từ điều này là Gpt-4 hoạt động khá tốt trong các thiết lập nhiễu thấp, chính xác là khi Tham lam cũng hoạt động tốt.

Một giả thuyết mạnh hơn sẽ là hầu hết các cấu hình Gpt-4 (ngoại trừ có lẽ những cấu hình sử dụng CoT tăng cường) hành xử như Tham lam trên tất cả các thể hiện, nhưng giả thuyết này được vô hiệu hóa bởi các thống kê GreedyFrac cho các thí nghiệm của chúng tôi trên thể hiện khó. Trên thể hiện khó, có vẻ như hầu hết các cấu hình Gpt-4 đang làm điều gì đó không tầm thường (mặc dù có lỗi); hành vi của chúng không hoàn toàn giống Tham lam cũng không giống như đồng nhất-ngẫu nhiên.

Hướng tới hiểu biết chi tiết hơn, chúng tôi chạy một tập hợp các thí nghiệm thứ cấp quy mô nhỏ tập trung vào các quyết định mỗi vòng của các tác nhân LLM. Các thí nghiệm tập trung vào một vòng t duy nhất trong vấn đề bandit. Mỗi thí nghiệm xem xét một "nguồn dữ liệu" cụ thể (một phân phối của lịch sử bandit), lấy mẫu N = 50 lịch sử bandit có độ dài t từ phân phối này, và trình bày chúng cho các tác nhân (LLM và đường cơ sở) và yêu cầu chúng xuất ra cánh tay hoặc phân phối trên các cánh tay. Chúng tôi theo dõi hai thống kê cho mỗi tác nhân: GreedyFrac và LeastFrac, tỷ lệ bản sao mà tác nhân đã chọn, tương ứng, cánh tay tốt nhất theo thực nghiệm cho đến nay và cánh tay ít được chọn nhất cho đến nay. Chúng tôi thay đổi nguồn dữ liệu, tức là thuật toán tạo ra lịch sử. Cụ thể, chúng tôi xem xét lịch sử được tạo ra bằng cách lấy mẫu ngẫu nhiên đều (Unif) và bằng cách chạy các đường cơ sở UCB và TS của chúng tôi trong t vòng.

Kết quả được tóm tắt trong Hình 10. Thật không may, chúng tôi phát hiện rằng hiệu suất mỗi vòng của cả LLM và đường cơ sở rất nhạy cảm với nguồn dữ liệu cụ thể. Ví dụ, thống kê MinFrac của UCB có thể thay đổi từ cao tới 0.46 trên lịch sử được tạo ra ngẫu nhiên đều đến thấp tới 0.09 trên lịch sử được tạo ra bởi chính UCB. Có vẻ hợp lý để kết luận rằng BNSN0 quá tham lam trong khi BSRN0 quá đồng nhất, nhưng các thống kê cho hai cấu hình LLM khác (BNRN0 và BNRC0)—cả hai đều thất bại trong các thí nghiệm dọc của chúng tôi—rơi vào phạm vi hợp lý được cung cấp bởi các đường cơ sở. Do đó, chúng tôi phát hiện rằng việc đánh giá liệu các tác nhân LLM có quá tham lam hay quá giống đồng nhất dựa trên các quyết định mỗi vòng là thách thức, mặc dù những tác nhân này hành xử khá khác biệt so với các đường cơ sở trong các thí nghiệm dọc.

4 Công trình liên quan
Bài báo này thuộc về một nhóm công trình gần đây nhằm hiểu khả năng của LLM, tức là chúng có thể và không thể làm gì tốt, và tại sao. Các khả năng đã nhận được sự chú ý đáng kể, nhưng ngoại biên với bài báo hiện tại, bao gồm trí tuệ tổng quát (Bubeck et al., 2023; Binz và Schulz, 2023), suy luận nhân quả (Kıcıman et al., 2023; Yiu et al., 2023) và toán học (Cobbe et al., 2021; Lu et al., 2023), lập kế hoạch (Valmeekam et al., 2023; Momennejad et al., 2023; Brooks et al., 2023), và tính kết hợp (Yu et al., 2023).

Chi tiết hơn, công trình của chúng tôi đóng góp cho văn học rộng hơn về khả năng của học trong ngữ cảnh. Các nghiên cứu trước đây về học trong ngữ cảnh bao gồm lý thuyết (Xie et al., 2021; Akyürek et al., 2022; Zhang et al., 2023b; Abernethy et al., 2023; Zhang et al., 2023a; Han et al., 2023a; Cheng et al., 2023; Ahn et al., 2023; Wies et al., 2023; Fu et al., 2023; Wu et al., 2023; Huang et al., 2023; Hendel et al., 2023; Li et al., 2023; Von Oswald et al., 2023; Bai et al., 2023; Hahn và Goyal, 2023; Jeon et al., 2024) và thực nghiệm (Garg et al., 2022; Kirsch et al., 2022; Ahuja et al., 2023; Han et al., 2023b; Raventós et al., 2023; Weber et al., 2023; Bhattamishra et al., 2023; Guo et al., 2023; Shen et al., 2023; Akyürek et al., 2024) điều tra, mặc dù như đã đề cập trong

¹¹ Thực sự, trong Hình 13(c) chúng ta thấy rằng hầu hết các cấu hình Gpt-4 có GreedyFrac rất cao nhưng không có thất bại hậu tố. Rõ ràng, ngay cả một lượng khám phá rất nhỏ cũng đủ cho các thể hiện dễ (và tạo ra sự khác biệt lớn, so với Tham lam). Tuy nhiên, điều này không nên được hiểu là bằng chứng cho hành vi khám phá tổng quát và mạnh mẽ hơn cần thiết cho các thể hiện bandit khó hơn.

13

--- TRANG 10 ---
phần tiếp theo, phần lớn công trình này liên quan đến học có giám sát trong ngữ cảnh; học tăng cường trong ngữ cảnh nhận được ít sự chú ý hơn nhiều. Tập hợp nhỏ các công trình thực nghiệm nghiên cứu RL trong ngữ cảnh (Laskin et al., 2022; Lee et al., 2023a; Raparthy et al., 2023; Xu et al., 2022) tập trung vào các mô hình được huấn luyện từ đầu sử dụng dữ liệu quỹ đạo được thu thập từ tác nhân khác (hoặc thuật toán RL hoặc chuyên gia); về mặt lý thuyết, Lee et al. (2023a) và sau đó Lin et al. (2023) biện minh cho phương pháp này với quan điểm meta-học tăng cường Bayesian (Simchowitz et al., 2021), và cho thấy rằng transformer được huấn luyện trước có thể thực hiện các chiến lược khám phá cổ điển như Thompson sampling và upper confidence bounds (UCB). Tuy nhiên, những công trình này yêu cầu can thiệp vào giai đoạn huấn luyện trước của mô hình ngôn ngữ, và không nghiên cứu liệu LLM hiện có có thể hiện khả năng khám phá dưới điều kiện huấn luyện tiêu chuẩn hay không.

Có lẽ gần nhất với bài báo hiện tại, Coda-Forno et al. (2023) đánh giá hiệu suất của học trong ngữ cảnh với Gpt-3.5 trên nhiệm vụ tên cướp hai tay và nhiệm vụ meta-học liên quan. Như với nghiên cứu của chúng tôi, họ phát hiện rằng Gpt-3.5 hoạt động tương tự (thực sự, hơi kém hơn) so với Tham lam; tuy nhiên, họ không xem xét chân trời thời gian đủ dài để phân biệt Tham lam với các đường cơ sở thành công như UCB.

Song song, có một dòng công trình phát triển nhanh chóng áp dụng LLM vào các ứng dụng ra quyết định trong thế giới thực. Ngoài các công trình đã đề cập trước đó (Shinn et al., 2023; Wang et al., 2023; Lee et al., 2023b), xem xét các ứng dụng cho gaming, lập trình, và y học, điểm nổi bật bao gồm Park et al. (2023), người giới thiệu các tác nhân tạo sinh mô phỏng hành vi con người trong môi trường thế giới mở, Ahn et al. (2022); Xu et al. (2023), người phát triển robot có khả năng LLM.

Công trình đồng thời. Hai công trình liên quan đồng thời (Wu et al., 2024; Park et al., 2024) cũng nghiên cứu hiệu suất LLM trong ngữ cảnh trong các nhiệm vụ bandit. Wu et al. (2024) xem xét một loạt nhiệm vụ nhằm đặc trưng "tác nhân thông minh" với tên cướp hai tay như một nhiệm vụ quan tâm cụ thể. Các thí nghiệm bandit của họ khác biệt trong một số khía cạnh chính: Họ xem xét thể hiện MAB rất dễ (với 2 cánh tay và khoảng cách Δ = 0.6, dễ hơn nhiều so với cả hai thể hiện của chúng tôi), tập trung vào thiết kế lời nhắc đơn (tương tự như lời nhắc cơ bản của chúng tôi), và so sánh với người chơi con người thay vì các đường cơ sở thuật toán. Những khác biệt này dẫn đến các phát hiện thí nghiệm rất khác nhau. Cụ thể, họ phát hiện rằng Gpt-4 hoạt động tốt trên thể hiện MAB đơn giản của họ, hội tụ rất nhanh về cánh tay tốt nhất, trong khi chúng tôi phát hiện rằng Gpt-4 với lời nhắc tương tự thất bại trên thể hiện MAB khó hơn. Tuy nhiên, phát hiện của họ phù hợp với của chúng tôi, vì chúng tôi cũng phát hiện rằng một số cấu hình của Gpt-4 hoạt động tốt trên thể hiện MAB dễ. Như chúng tôi thảo luận trong Mục 3.4, thể hiện này quá đơn giản để cung cấp bằng chứng thuyết phục cho hành vi khám phá có nguyên tắc.

Park et al. (2024) chủ yếu tập trung vào học trực tuyến thông tin đầy đủ và các thiết lập trò chơi lặp lại nhưng cũng đánh giá LLM trong các thiết lập bandit. Các thí nghiệm của họ khác với chúng tôi trong hai cách quan trọng. Đầu tiên, mặc dù một số giao thức tạo dữ liệu của họ có tính chất ngẫu nhiên, họ chủ yếu quan tâm đến các thiết lập đối kháng. Do đó họ so sánh với các đường cơ sở bandit đối kháng và trình bày lịch sử cho LLM thông qua tổn thất trọng số quan trọng (Auer et al., 2002b). Thứ hai, họ chủ yếu xem xét chân trời thời gian ngắn hơn (T = 25 cho bandit và lên đến T = 50 cho thông tin đầy đủ). Trong phiên bản cập nhật của bài báo của họ (được công bố trên arXiv vào mùa thu 2024), họ cũng bao gồm các thí nghiệm chân trời dài hơn của các thiết lập ban đầu của họ, nơi họ phát hiện rằng LLM tiếp tục hoạt động tốt, cũng như các thí nghiệm với thể hiện MAB khó của chúng tôi với chân trời T = 100, nơi họ đánh giá thất bại đồng nhất và hậu tố. Thú vị, họ phát hiện rằng cả Gpt-4 và Gpt-4o đều thành công (với phần thưởng cao, không thất bại hậu tố, và MinFrac thấp) khi sử dụng lời nhắc mặc định của họ yêu cầu đầu ra phân phối, chuỗi tư duy, và trình bày lịch sử thông qua trọng số quan trọng. Họ tiếp tục phát hiện rằng việc loại bỏ trọng số quan trọng dẫn đến thất bại, cụ thể, MinFrac cao hơn cho Gpt-4 và thất bại hậu tố cho Gpt-4o. Những phát hiện này có lẽ phù hợp với của chúng tôi: cả hai kết quả đều nói lên rằng việc tiền xử lý lịch sử (hoặc thông qua tóm tắt hoặc thông qua trọng số quan trọng) là quan trọng để kích thích hành vi khám phá từ LLM.

Công trình đồng thời khác bao gồm Schubert et al. (2024); Hayes et al. (2024); Coda-Forno et al. (2024) người sử dụng bandit trong ngữ cảnh và các nhiệm vụ khác để nghiên cứu liệu LLM có thể hiện hành vi giống con người (đặc biệt, thành kiến) trong các nhiệm vụ ra quyết định hay không.

Chúng tôi cũng giới thiệu cho độc giả quan tâm một khảo sát gần đây về các phương pháp sử dụng LLM trong các thiết lập học tăng cường (Cao et al., 2024).

Công trình tiếp theo. Monea et al. (2024) và Nie et al. (2024) tiếp tục kết quả của chúng tôi với một số phát hiện thí nghiệm mới. Cả hai công trình đều xem xét bandit ngữ cảnh (và Nie et al. (2024) cũng xem xét MAB vanilla), và phát hiện rằng LLM thất bại trong việc khám phá mà không có can thiệp không tầm thường. Theo nghĩa này, những công trình này củng cố các phát hiện chính của chúng tôi. Hơn nữa, cả hai công trình đều đề xuất các can thiệp cải thiện khám phá LLM. Cụ thể, Monea et al. (2024) đề xuất can thiệp không cần huấn luyện theo đó lịch sử tương tác được lấy mẫu phụ đồng nhất trước khi được bao gồm trong lời nhắc LLM, trong khi Nie et al. (2024) xem xét lời nhắc few-shot và tinh chỉnh với minh chứng tối ưu. Những can thiệp này cải thiện hiệu suất, nhưng vẫn không cạnh tranh với các đường cơ sở thuật toán tiêu chuẩn.

4.1 Nền tảng thêm về tên cướp nhiều tay
Ở đây, chúng tôi cung cấp nền tảng bổ sung về vấn đề tên cướp nhiều tay, và về các thuật toán đường cơ sở được sử dụng trong bài báo này. Thảo luận sâu hơn có thể được tìm thấy trong Bubeck và Cesa-Bianchi (2012); Slivkins (2019); Lattimore và Szepesván (2020).

Thuật toán UCB (Auer et al., 2002a) khám phá bằng cách gán cho mỗi cánh tay a một chỉ số, được định nghĩa là phần thưởng trung bình từ cánh tay cho đến nay cộng với bonus dạng √(C/na), trong đó C = Θ(log T) và na là số mẫu từ cánh tay cho đến nay. Trong mỗi vòng, nó chọn cánh tay có chỉ số lớn nhất. Bonus thực hiện nguyên tắc lạc quan dưới bất định. Chúng tôi sử dụng một phiên bản của UCB đặt C = 1 (một heuristic), đã được quan sát có hiệu suất thực nghiệm thuận lợi (ví dụ, Slivkins et al., 2013; Ho et al., 2016).

Thompson Sampling (Thompson, 1933; Russo et al., 2018, cho khảo sát) tiến hành như thể phần thưởng trung bình của các cánh tay ban đầu được rút từ một số tiên nghiệm Bayesian. Trong mỗi vòng, nó tính toán hậu nghiệm Bayesian cho trước lịch sử cho đến nay, rút một mẫu từ hậu nghiệm, và chọn cánh tay có phần thưởng trung bình lớn nhất theo mẫu này (tức là giả sử mẫu là sự thật cơ bản). Trong thiết lập của chúng tôi, tiên nghiệm về cơ bản là một tham số cho thuật toán. Chúng tôi chọn tiên nghiệm rút phần thưởng trung bình của mỗi cánh tay độc lập và ngẫu nhiên đều từ khoảng [0, 1]. Đây là một lựa chọn tiêu chuẩn, đạt được ranh giới hối tiếc gần tối ưu, cũng như hiệu suất thực nghiệm tốt (Kaufmann et al., 2012; Agrawal và Goyal, 2012, 2017). Mỗi cánh tay được cập nhật độc lập như một tiên nghiệm liên hợp Beta-Bernoulli. Việc tối ưu hóa thêm UCB và Thompson Sampling không cần thiết cho bài báo này, vì chúng đã hoạt động khá tốt trong các thí nghiệm của chúng tôi.

Đảm bảo có thể chứng minh cho thuật toán bandit thường được biểu đạt qua hối tiếc: sự khác biệt trong tổng phần thưởng kỳ vọng của cánh tay tốt nhất và thuật toán. Cả hai đường cơ sở đều đạt được hối tiếc O(√KT log T), gần tối ưu minimax như một hàm của T và K. Chúng cũng đạt được tỷ lệ hối tiếc gần tối ưu theo thể hiện, tỷ lệ như O(K/Δ log T) cho các thể hiện chúng tôi xem xét.

Thuật toán ε-Tham lam (Chú thích 6) về cơ bản không hiệu quả ở chỗ nó không điều hướng khám phá một cách thích ứng hướng tới các cánh tay hoạt động tốt hơn. Theo đó, tỷ lệ hối tiếc của nó tỷ lệ như T^(2/3) (với thiết lập tối ưu của ε ~ T^(-1/3)). Cố định ε như vậy, hối tiếc không cải thiện cho các thể hiện dễ hơn.

Thuật toán Tham lam (Chú thích 5) không khám phá gì cả, gây ra thất bại hậu tố. Điều này rõ ràng khi thuật toán được khởi tạo với một mẫu duy nhất (n = 1) của mỗi cánh tay: thất bại hậu tố xảy ra khi cánh tay tốt trả về 0, và một trong các cánh tay khác trả về 1. Tuy nhiên, thất bại hậu tố không phải là tạo tác của n nhỏ: chúng có thể xảy ra với bất kỳ n nào, với xác suất tỷ lệ như Ω(1/√n) (Banihashem et al., 2023).

5 Thảo luận và câu hỏi mở
Điều tra của chúng tôi gợi ý rằng các LLM đương đại không tham gia khám phá mạnh mẽ được yêu cầu cho các vấn đề học tăng cường thống kê và ra quyết định rất cơ bản, ít nhất là không có can thiệp thêm. Trong những gì tiếp theo, chúng tôi xác định một số bước tiếp theo để đánh giá thêm giả thuyết này và tìm kiếm các can thiệp để giảm thiểu hành vi này.

Can thiệp cơ bản và nhu cầu tiến bộ phương pháp luận. Trước các kết quả tiêu cực của chúng tôi, các can thiệp rõ ràng nhất mà người ta có thể xem xét bao gồm:

1. Thử nghiệm với các lời nhắc khác. Như với nhiều thiết lập khác (Sclar et al., 2023), có thể những thay đổi nhỏ đối với mẫu lời nhắc của chúng tôi có thể cải thiện hiệu suất. Tuy nhiên, sự nhạy cảm với thiết kế lời nhắc đã đáng lo ngại.

2. Thử nghiệm với lời nhắc few-shot, trong đó lời nhắc chứa các ví dụ về hành vi khám phá, sử dụng các ví dụ như vậy để tinh chỉnh LLM, hoặc tuyển chọn kho ngữ liệu huấn luyện để bao gồm các ví dụ về hành vi khám phá.

3. Huấn luyện LLM để sử dụng các công cụ hỗ trợ, chẳng hạn như máy tính cho số học cơ bản hoặc "bộ ngẫu nhiên" để lấy mẫu chính xác từ phân phối.

Mặc dù những bước này khá tự nhiên, chi phí, quyền truy cập vào mô hình, và tính toán tạo ra các rào cản đáng kể cho nghiên cứu thêm, đặc biệt là do nhu cầu sử dụng chân trời dài T và nhiều bản sao N để có được kết quả có ý nghĩa thống kê. Vì lý do này, chúng tôi tin rằng những tiến bộ phương pháp luận và/hoặc thống kê thêm để cho phép chẩn đoán và hiểu biết về hành vi tác nhân LLM hiệu quả về chi phí (ví dụ, các thống kê thay thế của chúng tôi) là cần thiết.

Ý nghĩa cho các vấn đề ra quyết định phức tạp. Tập trung của chúng tôi vào các vấn đề tên cướp nhiều tay đơn giản cung cấp thiết lập thí nghiệm sạch sẽ và có thể kiểm soát để nghiên cứu hành vi khám phá của LLM và các can thiệp thuật toán tiềm năng. Thất bại khám phá ở đây gợi ý rằng những thất bại tương tự cũng sẽ xảy ra trong các thiết lập RL và ra quyết định phức tạp hơn. Mặt khác, cần thận trọng trong việc phát triển các biện pháp giảm thiểu, vì các giải pháp thành công cho thiết lập MAB có thể không khái quát hóa đến các thiết lập phức tạp hơn. Ví dụ, trong khi Gpt-4 với lịch sử tương tác tóm tắt và CoT tăng cường dường như khám phá thành công trong thiết lập MAB của chúng tôi, không rõ làm thế nào để tóm tắt bên ngoài lịch sử trong các thiết lập có quan sát phức tạp, cao chiều như bandit ngữ cảnh (xem Chú thích 1). Thực sự, ngay cả đối với bandit ngữ cảnh tuyến tính, phương pháp có thể không áp dụng được mà không có can thiệp thuật toán đáng kể (chẳng hạn như, ví dụ, hồi quy tuyến tính được tính toán bên ngoài và bao gồm trong lời nhắc) và nhiều lựa chọn mô hình và thuật toán rõ ràng liên quan đến những can thiệp như vậy. Chúng tôi tin rằng điều tra sâu hơn về các can thiệp thuật toán là cần thiết để hiểu mức độ mà LLM có thể hoạt động như các tác nhân ra quyết định.

16
