# Đa dạng tác vụ tiền huấn luyện và sự xuất hiện của học trong ngữ cảnh phi-Bayesian cho hồi quy

Allan Raventós∗Mansheej Paul∗Feng Chen Surya Ganguli
Đại học Stanford
{aravento, mansheej, fengc, sganguli}@stanford.edu

## Tóm tắt
Các transformer đã được tiền huấn luyện thể hiện khả năng đáng chú ý của học trong ngữ cảnh (ICL): chúng có thể học các tác vụ chỉ từ vài ví dụ được cung cấp trong câu nhắc mà không cần cập nhật bất kỳ trọng số nào. Điều này đặt ra một câu hỏi cơ bản: liệu ICL có thể giải quyết các tác vụ hoàn toàn mới rất khác biệt so với những tác vụ đã thấy trong quá trình tiền huấn luyện không? Để khám phá câu hỏi này, chúng tôi kiểm tra hiệu suất của ICL trên hồi quy tuyến tính trong khi thay đổi tính đa dạng của các tác vụ trong tập dữ liệu tiền huấn luyện. Chúng tôi chứng minh thực nghiệm một ngưỡng đa dạng tác vụ cho sự xuất hiện của ICL. Dưới ngưỡng này, transformer đã được tiền huấn luyện không thể giải quyết các tác vụ hồi quy chưa thấy, thay vào đó hoạt động như một ước lượng Bayesian với phân phối tác vụ tiền huấn luyện không đa dạng làm prior. Vượt qua ngưỡng này, transformer vượt trội đáng kể so với ước lượng này; hành vi của nó phù hợp với hồi quy ridge, tương ứng với prior Gaussian trên tất cả các tác vụ, bao gồm cả những tác vụ không thấy trong quá trình tiền huấn luyện. Do đó, khi được tiền huấn luyện trên dữ liệu với tính đa dạng tác vụ lớn hơn ngưỡng, các transformer có thể giải quyết tối ưu các tác vụ hoàn toàn mới trong ngữ cảnh. Quan trọng là, khả năng này phụ thuộc vào việc nó lệch khỏi ước lượng tối ưu Bayes với phân phối tiền huấn luyện làm prior. Nghiên cứu này cũng khám phá ảnh hưởng của chính quy hóa, dung lượng mô hình và cấu trúc tác vụ và nhấn mạnh, trong một ví dụ cụ thể, vai trò quan trọng của tính đa dạng tác vụ, bên cạnh quy mô dữ liệu và mô hình, trong sự xuất hiện của ICL.

## 1 Giới thiệu

Các transformer đã được tiền huấn luyện (PT) có thể học các tác vụ mới chỉ từ vài ví dụ được cung cấp trong câu nhắc mà không cần thực hiện bất kỳ bước gradient nào trên những ví dụ đó [1]. Khả năng này, được gọi là học trong ngữ cảnh (ICL), đã mở khóa việc sử dụng rộng rãi các mô hình ngôn ngữ bằng cách làm cho việc thích ứng các mô hình đa mục đích với các tác vụ tùy chỉnh trở nên hiệu quả mà không cần huấn luyện rõ ràng. Mặc dù đáng chú ý, điều làm cho ICL bí ẩn và có thể có hại [2] là thuật toán học được thực hiện bởi PT trong lượt truyền tiến của nó không được tích hợp vào kiến trúc hay quy trình huấn luyện; thay vào đó, nó xuất hiện từ tiền huấn luyện trên dữ liệu quy mô lớn với mục tiêu dự đoán token tiếp theo. Điều này đặt ra một câu hỏi cơ bản: liệu ICL có thể thực sự giải quyết các tác vụ hoàn toàn mới rất khác biệt so với những tác vụ đã thấy trong quá trình tiền huấn luyện không? Nếu có, thuật toán học nào mà ICL thực hiện? Để trả lời những câu hỏi này, chúng ta cần hiểu rõ hơn về cách các thành phần khác nhau đi vào tiền huấn luyện ảnh hưởng đến khả năng này.

Hướng tới mục tiêu này, chúng tôi khám phá cách tính đa dạng của các tác vụ trong dữ liệu tiền huấn luyện ảnh hưởng đến sự xuất hiện của ICL. Nghiên cứu trước đây [3] đã đề xuất rằng ICL hoạt động bằng cách thực hiện suy luận Bayesian. Trong quá trình tiền huấn luyện, các transformer học một prior trên các tác vụ tiềm ẩn được biểu diễn trong dữ liệu tiền huấn luyện. Khi được nhắc với các ví dụ tại thời điểm suy luận, chúng "truy xuất" các tác vụ tiền huấn luyện có liên quan và tạo ra các token tiếp theo từ phân phối posterior có điều kiện trên truy vấn và các tác vụ được suy luận. Điều này cho thấy hiệu suất ICL trên một tác vụ mới bị ảnh hưởng bởi sự tương đồng của nó với các tác vụ được học ngầm trong quá trình tiền huấn luyện. Tuy nhiên, phân phối của các tác vụ trong dữ liệu tiền huấn luyện của chúng ta, TPretrain, thường là một mẫu con giới hạn và không đại diện của phân phối tác vụ lý tưởng, TTrue, mà chúng ta muốn mô hình của mình có khả năng học trong ngữ cảnh. Ví dụ, TTrue có thể là tập hợp tất cả các hướng dẫn mà chúng ta muốn một trợ lý A.I. tuân theo. Nhưng, các tập dữ liệu mô hình hóa ngôn ngữ quy mô lớn [4,5] được sử dụng để tiền huấn luyện những mô hình này chứa rất ít ví dụ được định dạng chính xác cho ICL. Các tập dữ liệu tinh chỉnh hướng dẫn (IFT) [6–11] được thiết kế để cải thiện điều này thì tốn kém để thu thập và do đó chỉ chứa các tác vụ từ một vài domain. Trong khuôn khổ Bayesian, sự không khớp phân phối này sẽ làm cho ước lượng Bayesian với prior trên các tác vụ tiền huấn luyện giới hạn, TPretrain, hoạt động không tối ưu trên các tác vụ rất khác biệt so với những tác vụ đã thấy trong quá trình tiền huấn luyện. Điều này thúc đẩy câu hỏi của chúng tôi: liệu một mô hình được tiền huấn luyện trên tập dữ liệu với tính đa dạng tác vụ thấp có thể học các tác vụ mới, chưa thấy không?

Đối với mô hình hóa ngôn ngữ đa mục đích, kích thước và độ phức tạp của TPretrain và đặc tả mơ hồ của TTrue làm cho câu hỏi này khó phân tích. Vì vậy, theo nghiên cứu gần đây [12–14], chúng tôi nghiên cứu ICL cho hồi quy tuyến tính. Ở đây, một tác vụ là một bài toán hồi quy tuyến tính với một vector hồi quy tiềm ẩn cho trước; PT phải dự đoán đích cho một điểm dữ liệu mới từ các ví dụ của các cặp dữ liệu-đích được cung cấp trong câu nhắc. Nghiên cứu trước đây [13] đã chỉ ra rằng các transformer thấy số lượng không giới hạn các vector hồi quy tiềm ẩn trong quá trình tiền huấn luyện sẽ học thực hiện hồi quy ridge với tham số ridge tối ưu Bayes. Thay vào đó, chúng tôi xem xét trường hợp mà phân phối tác vụ tiền huấn luyện, TPretrain, chứa một tập hợp giới hạn và hữu hạn các vector hồi quy tiềm ẩn (xem Mục 2 để biết chi tiết). Để đánh giá khả năng học các tác vụ mới, PT được kiểm tra trên phân phối tác vụ lý tưởng, TTrue, là một phân phối Gaussian trên tất cả các vector hồi quy tiềm ẩn. Nghiên cứu bối cảnh này có hai lợi thế: thứ nhất, chúng ta có thể thay đổi trực tiếp tính đa dạng tác vụ trong TPretrain bằng cách thay đổi số lượng vector hồi quy tiềm ẩn duy nhất được thấy trong quá trình tiền huấn luyện. Thứ hai, chúng ta có thể tính toán ước lượng tối ưu giảm thiểu loss tiền huấn luyện—ước lượng Bayesian với prior TPretrain—cũng như ước lượng tối ưu cho tất cả các tác vụ—ước lượng Bayesian với prior TTrue. Điều này cho phép chúng ta diễn giải hành vi của PT bằng cách so sánh dự đoán của nó với những ước lượng tối ưu dưới một trong hai phân phối tác vụ. Trong nghiên cứu của chúng tôi, chúng tôi thay đổi tính đa dạng tác vụ tiền huấn luyện và khám phá khả năng của PT học các tác vụ hoàn toàn mới trong ngữ cảnh: liệu nó có hoạt động như ước lượng tối ưu cho TPretrain và hoạt động không tối ưu trên các tác vụ từ TTrue, hay nó có phù hợp với ước lượng tối ưu cho TTrue có thể giải quyết các tác vụ mới, chưa thấy?

**Đóng góp.** Đóng góp của chúng tôi như sau:

• Chúng tôi thấy rằng một transformer được tiền huấn luyện trên dữ liệu với tính đa dạng tác vụ thấp hoạt động như ước lượng Bayesian với prior TPretrain; nó hoạt động tối ưu trên các tác vụ tiền huấn luyện nhưng không thể học các tác vụ mới trong ngữ cảnh. Tuy nhiên, khi tính đa dạng tác vụ tiền huấn luyện tăng, PT lệch khỏi ước lượng Bayesian này, vượt trội đáng kể so với nó trên các tác vụ mới, và ở một số lượng lớn nhưng vẫn hữu hạn các tác vụ tiền huấn luyện, hiệu suất của PT khớp chặt chẽ với ước lượng tối ưu trên TTrue.

• Chúng tôi xác định một ngưỡng đa dạng tác vụ cho sự xuất hiện của ICL. Dưới ngưỡng này, việc tăng kích thước tập dữ liệu tiền huấn luyện trong khi giữ tính đa dạng tác vụ không đổi làm cho PT thiên về phân phối tác vụ tiền huấn luyện. Ngược lại, vượt qua ngưỡng này, việc tăng kích thước tập dữ liệu mà không tăng tính đa dạng tác vụ của nó cải thiện hiệu suất của PT trên các tác vụ mới, chưa thấy. Điều này cho thấy hành vi của PT trải qua một chuyển đổi pha thuật toán sắc nhọn trong giới hạn nhiều ví dụ mỗi tác vụ, phù hợp với các ước lượng tối ưu trên TPretrain trước ngưỡng và trên TTrue sau ngưỡng. Chúng tôi cũng kiểm tra chuyển đổi này từ góc độ động học học tập.

• Chúng tôi chứng minh thực nghiệm rằng việc tăng chiều tác vụ ở SNR cố định làm tăng ngưỡng đa dạng tác vụ. Tuy nhiên, sự scaling của lỗi PT với chiều vượt trội hơn nhiều so với ước lượng Bayesian tối ưu cho TPretrain; ở tính đa dạng tác vụ vượt qua ngưỡng ở tất cả các chiều chúng tôi xem xét, PT vẫn gần tối ưu với chiều tăng, trong khi ước lượng tối ưu cho TPretrain trở nên ít giống với ước lượng tối ưu cho TTrue.

• Chúng tôi chỉ ra rằng việc tăng weight decay giảm đáng kể ngưỡng đa dạng tác vụ trong khi tăng số lớp hoặc kích thước embedding tăng ngưỡng đa dạng tác vụ. Điều này làm sáng tỏ tác động của chính quy hóa và dung lượng mô hình lên sự xuất hiện của ICL.

Nhìn chung, những đóng góp này cho thấy sự xuất hiện của học trong ngữ cảnh trong các transformer được tiền huấn luyện không thể được giải thích hoàn toàn bằng lý thuyết suy luận Bayesian trên phân phối tiền huấn luyện.

## 2 Thiết lập vấn đề

**ICL của hồi quy tuyến tính (sơ đồ trong Hình 1).** Mỗi tác vụ ICL tương ứng với một vector hồi quy D chiều tiềm ẩn, w∈RD. Tại thời điểm suy luận, transformer nhận đầu vào là một chuỗi K cặp dữ liệu-đích, (x1, y1, ..., xK, yK), là các ví dụ trong ngữ cảnh tương ứng với tác vụ đơn này w. Với k∈{1, ..., K}, dữ liệu được rút i.i.d. từ phân phối chuẩn D chiều, xk∼N(0,ID), và các đích là các scalar được cho bởi yk=w⊺xk+εk. Các εk là các scalar nhiễu được rút i.i.d. từ phân phối chuẩn với mean 0 và variance σ2, εk∼N(0,σ2). Gọi fθ là PT với tham số θ. Vì chúng tôi sử dụng transformer chỉ decoder với causal attention mask, với mỗi k∈{1, ..., K}, transformer thấy ngữ cảnh Sk=(x1, y1, ..., xk−1, yk−1, xk) và dựa trên ngữ cảnh này, nó đưa ra dự đoán, fθ(Sk), cho đích của xk. Do đó, trong mỗi lượt truyền tiến, PT giải quyết K bài toán hồi quy tuyến tính mỗi bài với cùng vector hồi quy tiềm ẩn nhưng với số lượng ví dụ trong ngữ cảnh tăng dần.

**Tiền huấn luyện.** Transformer được tiền huấn luyện để giảm thiểu lỗi bình phương trung bình (MSE) dự đoán token tiếp theo trên các chuỗi của các cặp dữ liệu và đích. Vector hồi quy tiềm ẩn cho mỗi chuỗi được rút từ phân phối tác vụ tiền huấn luyện, TPretrain. Phân phối này có tính đa dạng giới hạn vì nó là phân phối đều trên một tập hợp hữu hạn M tác vụ, TPretrain=U{w(1), ..., w(M)}. Mỗi tác vụ trong TPretrain được rút i.i.d từ phân phối chuẩn D chiều, w(i)∼N(0,ID), i∈1, ..., M. Bằng cách tăng số lượng tác vụ, M, trong TPretrain, chúng ta có thể tăng tính đa dạng của dữ liệu tiền huấn luyện. Vì transformer đưa ra dự đoán cho mỗi điểm dữ liệu trong chuỗi, loss của nó, LTPretrain, chỉ là MSE cho mỗi dự đoán, được tính trung bình trên các dự đoán trong chuỗi:

LTPretrain(θ) = E[w∼TPretrain, x1,...,xK∼N(0,ID), ε1,...,εK∼N(0,σ2)][1/K ∑k=1K (fθ(Sk)−yk)2]. (1)

**Đánh giá.** Chúng tôi đánh giá hiệu suất của PT trên các tác vụ được thấy trong quá trình tiền huấn luyện bằng cách tính LTPretrain sử dụng Eq. (1) nhưng với các mẫu mới của dữ liệu và nhiễu. Vì đây là các instance mới của tác vụ với các ví dụ trong ngữ cảnh mới, đánh giá này tương ứng với test error của PT. Để PT thực hiện thành công ICL của hồi quy tuyến tính trên các tác vụ mới, nó phải dự đoán chính xác các đích từ các ví dụ trong ngữ cảnh cho bất kỳ tác vụ nào được rút từ phân phối tác vụ lý tưởng, TTrue, trên tất cả các vector hồi quy tiềm ẩn; trong trường hợp của chúng tôi TTrue=N(0,ID). Chúng tôi đánh giá hiệu suất của PT trên các tác vụ mới bằng cách tính LTTrue, tuân theo Eq. (1) nhưng trong đó các tác vụ được lấy mẫu từ phân phối tác vụ lý tưởng: w∼TTrue trong expectation.

**So sánh PT với các ước lượng tối ưu.** Một lợi thế của việc nghiên cứu ICL của hồi quy tuyến tính là chúng ta có thể tính toán các ước lượng tối ưu ground truth giảm thiểu loss, LT, trong Eq. (1) cho cả hai phân phối tác vụ, TPretrain và TTrue. Ước lượng tối ưu cho dự đoán thứ k, ŷTk, giảm thiểu số hạng thứ k trong tổng trong LT là ước lượng Bayesian với T làm prior. Điều này được cho bởi posterior mean của yk có điều kiện trên ngữ cảnh: ŷTk=ET,εk[yk|Sk], trong đó expectation là trên phân phối tác vụ, T, và nhiễu, εk (Phụ lục A.1).

Với phân phối tác vụ TPretrain=U{w(1), ..., w(M)}, ước lượng minimum mean squared error rời rạc (dMMSE) là tối ưu. Nó được cho bởi ŷdMMSEk=ŵdMMSEk⊺xk trong đó ŵdMMSE1=1/M∑i=1M w(i) và với k∈{2, ..., K}, (Phụ lục A.2)

ŵdMMSEk = ∑i=1M [exp(-1/2σ2 ∑j=1k-1 (yj−w(i)⊺xj)2) / ∑l=1M exp(-1/2σ2 ∑j=1k-1 (yj−w(l)⊺xj)2)] w(i). (2)

Trực quan, ŵdMMSEk chỉ là tổng có trọng số của các w(i) tiền huấn luyện với trọng số được điều chỉnh bởi likelihood của việc quan sát đích {y1, ..., yk−1} có điều kiện trên đầu vào {x1, ..., xk−1} và tác vụ là w(i). Một PT giảm thiểu loss tiền huấn luyện LTPretrain sẽ hoạt động như ước lượng này.

Với phân phối tác vụ TTrue=N(0,ID), ước lượng hồi quy Ridge với tham số ridge được đặt thành scale nhiễu σ2 là tối ưu: ŷRidgek=ŵRidgek⊺xk, trong đó ŵRidge1=0 và với k={2, ..., K},

ŵRidgek = (X⊺X+σ2ID)−1X⊺y, (3)

trong đó X=(x1⊺, ..., xk−1⊺)∈R(k−1)×D và y=(y1, ..., yk−1) (Phụ lục A.3). Một PT hoạt động tối ưu trên các tác vụ mới sẽ hoạt động như ước lượng này. Chúng ta có thể so sánh hành vi của PT với các ước lượng tối ưu bằng cách tính mean square difference của các dự đoán dưới phân phối tác vụ T cho trước. Chúng tôi viết điều này như

ΔTuPT,Ridge/dMMSE = E[w∼T, x1,...,xK∼N(0,ID), ε1,...,εK∼N(0,σ2)][1/KD ∑k=1K (fθ(Sk)−ŷRidge/dMMSEk)2]. (4)

## 3 Thí nghiệm và kết quả

Trừ khi được chỉ định khác, chúng tôi nghiên cứu hồi quy tuyến tính trong D=8 chiều với tối đa K=16 ví dụ trong ngữ cảnh và variance nhiễu quan sát σ2=0.25. Chúng tôi sử dụng một transformer cơ bản với kiến trúc GPT2 [15] với 8 lớp, embedding 128 chiều, và 2 attention head hoặc một mô hình nhỏ với 4 lớp, embedding 64 chiều, và 2 attention head. Chúng tôi huấn luyện với optimizer Adam [16] và lịch learning rate tam giác một chu kỳ [17] với 50% warmup. Mô hình cơ bản được huấn luyện với batch size 256 trong 500K bước huấn luyện, mặc dù các siêu tham số này được thay đổi trong thí nghiệm của chúng tôi. Chúng tôi luôn quét qua một dải learning rate và chọn learning rate lớn nhất mà tại đó huấn luyện ổn định. Để biết thêm chi tiết xem Phụ lục B.

Để tiền huấn luyện một transformer được khởi tạo ngẫu nhiên trên dữ liệu với tính đa dạng tác vụ M, đầu tiên chúng tôi xây dựng phân phối tác vụ tiền huấn luyện, TPretrain, như mô tả trong Mục 2. Sau đó chúng tôi giảm thiểu mục tiêu LTPretrain trong Eq. (1) sử dụng minibatch stochastic gradient descent. Với mỗi chuỗi trong một minibatch, chúng tôi lấy mẫu một tác vụ đơn w từ TPretrain, cũng như các mẫu mới của dữ liệu, {xi}Ki=1, và nhiễu, {εi}Ki=1, từ các phân phối liên tục tương ứng, để tạo thành một chuỗi (x1,w⊺x1+ε1, ..., xK,w⊺xK+εK). Nếu chúng tôi huấn luyện trong N bước ở batch size B, transformer sẽ thấy tổng cộng NB chuỗi duy nhất và khoảng NB/M chuỗi duy nhất cho mỗi tác vụ tiềm ẩn trong TPretrain. Bằng cách tăng B hoặc N ở M cố định, chúng ta có thể tăng tổng kích thước của tập dữ liệu tiền huấn luyện (hoặc số chuỗi mỗi tác vụ) trong khi giữ tính đa dạng tập dữ liệu—số lượng w duy nhất trong TPretrain—cố định.

### 3.1 Ngưỡng đa dạng tác vụ cho sự xuất hiện của học trong ngữ cảnh

Đối với Hình 2, chúng tôi tiền huấn luyện transformer cơ bản của chúng tôi trên các tập dữ liệu với tính đa dạng tác vụ tăng dần (trên trục x) trong khi giữ tổng số chuỗi được thấy trong quá trình tiền huấn luyện cố định (B=256, N=500K). Chúng tôi đánh giá các PT và cả hai ước lượng tối ưu trên các tác vụ được thấy trong quá trình tiền huấn luyện được rút từ TPretrain (Hình 2 hàng trên) và trên các tác vụ mới được rút từ TTrue (Hình 2 hàng dưới) và vẽ MSE được chuẩn hóa theo chiều tác vụ—LT/D từ Eq. (1)). Vì dMMSE là tối ưu trên các tác vụ từ TPretrain (như đã thảo luận trong Mục 2), các điểm đánh dấu dMMSE màu xanh lá cây biểu thị loss thấp nhất có thể mà PT có thể đạt được trong thiết lập này. Thực tế, mục tiêu tiền huấn luyện LTPretrain khuyến khích rõ ràng PT khớp với hiệu suất dMMSE. Mặt khác, Ridge là tối ưu trên các tác vụ được lấy mẫu từ TTrue (Hình 2 dưới trái); các điểm đánh dấu màu xanh dương biểu thị MSE thấp nhất có thể mà PT có thể đạt được trên các tác vụ mới.

**Pha đa dạng tác vụ thấp: PT là Bayesian đối với phân phối tiền huấn luyện và không thể giải quyết các tác vụ mới.** Ở tính đa dạng tác vụ tiền huấn luyện thấp—M lên đến khoảng 26—MSE của PT theo sát dMMSE trên các tác vụ được lấy mẫu từ TPretrain (Hình 2 trên trái); PT hoạt động tối ưu trên các tác vụ được thấy trong quá trình tiền huấn luyện. Nhưng nó hoạt động kém đáng kể trên các tác vụ mới được lấy mẫu từ TTrue, được chỉ ra bởi khoảng cách MSE giữa PT và Ridge (Hình 2, dưới trái). Trong chế độ này, nó hoạt động như ước lượng Bayesian với prior TPretrain.

**Pha đa dạng tác vụ cao: PT là phi-Bayesian đối với phân phối tác vụ tiền huấn luyện và có thể giải quyết các tác vụ mới.** Ở tính đa dạng tác vụ cao hơn—trên 26 tác vụ tiền huấn luyện—MSE của PT lệch khỏi dMMSE và tiến gần đến Ridge dưới cả TPretrain và TTrue. Quan trọng là, PT bắt đầu vượt trội đáng kể so với dMMSE trên các tác vụ chưa thấy được lấy mẫu từ TTrue (Hình 2 dưới trái) với chi phí không giảm thiểu hoàn toàn mục tiêu huấn luyện của nó, LTPretrain (khoảng cách giữa PT và dMMSE dưới TPretrain, Hình 2 trên trái). Điều này cho thấy rằng, một PT được huấn luyện trên số lượng hữu hạn nhưng lớn các tác vụ tiền huấn luyện có thể học các tác vụ hoàn toàn mới trong ngữ cảnh và khả năng này phụ thuộc vào việc nó lệch khỏi ước lượng Bayesian tối ưu trên phân phối tác vụ tiền huấn luyện.

**Finite size scaling của dữ liệu huấn luyện cho thấy một chuyển đổi pha thuật toán khi tính đa dạng tác vụ tăng.** Các thí nghiệm trong Hình 2 (cột trái) cho thấy rằng, khi được kiểm tra trên cả hai phân phối tác vụ TPretrain và TTrue, thuật toán ICL được thực hiện bởi PT thể hiện một crossover mượt mà trong hiệu suất từ dMMSE sang Ridge. Tiếp theo chúng tôi kiểm tra cách chuyển đổi này thay đổi khi chúng ta tăng số chuỗi mỗi tác vụ được thấy trong quá trình tiền huấn luyện, ở tính đa dạng tác vụ cố định. Người ta có thể hợp lý mong đợi rằng, nếu transformer thấy nhiều chuỗi hơn mỗi tác vụ tiềm ẩn trong TPretrain, cả dự đoán và hiệu suất của nó sẽ trở nên giống dMMSE hơn, và ít giống Ridge hơn, ở tất cả các giá trị của tính đa dạng tác vụ. Đáng ngạc nhiên, kỳ vọng tự nhiên này bị vi phạm theo cách thúc đẩy ICL trên TTrue.

Ở mỗi số lượng tác vụ, chúng tôi tăng số chuỗi mỗi tác vụ bằng cách tăng batch size từ 256 lên 512 lên 1024, trong khi để số bước huấn luyện cố định ở 500K. Chúng tôi quan sát rằng ΔTPretrain PT,dMMSE, định lượng mức độ khác biệt giữa dự đoán của PT và ước lượng dMMSE khi kiểm tra trên các tác vụ được rút từ TPretrain, thực sự giảm với M≤210 (Hình 2 trên giữa) khi chúng tôi huấn luyện trên nhiều chuỗi hơn mỗi tác vụ. Hơn nữa, với mỗi M∈{210, ..., 214} dự đoán của PT cũng trở nên ít giống Ridge hơn, cả trên các tác vụ từ TPretrain (Hình 2, trên phải) và TTrue (Hình 2, dưới phải). Quan trọng là, chuyển động này trong hành vi của PT hướng về dMMSE và ra khỏi Ridge, ít nhất trên các tác vụ được rút từ TPretrain, chỉ giữ đến một ngưỡng số lượng tác vụ giữa 214 và 215. Vượt qua ngưỡng này, tiền huấn luyện trên nhiều chuỗi hơn mỗi tác vụ ở tính đa dạng tác vụ cố định thực sự làm cho PT giống Ridge hơn, theo nghĩa là cả ΔTPretrain PT,Ridge và ΔTTrue PT,Ridge đều giảm (Hình 2, phải trên và phải dưới tương ứng). Điều này có nghĩa là, vượt qua ngưỡng đa dạng tác vụ, PT không chỉ có thể giải quyết tối ưu các tác vụ mới từ TTrue bằng cách khớp hiệu suất Ridge, mà PT còn trở nên tốt hơn trong việc làm điều đó nếu được huấn luyện trên nhiều chuỗi hơn mỗi tác vụ, bất chấp tập hợp giới hạn các tác vụ được trải nghiệm trong tiền huấn luyện. Do đó, trái với kỳ vọng tự nhiên đã nêu ở trên, nhiều chuỗi hơn mỗi tác vụ không thúc đẩy sự chuyên biệt hóa quá mức của PT với TPretrain ở tính đa dạng tác vụ vượt qua ngưỡng.

Cuối cùng, chuyển động của thuật toán ICL được thực hiện bởi PT hướng về (ra khỏi) Ridge trên (dưới) ngưỡng đa dạng tác vụ (Hình 2, phải trên và dưới) chỉ ra rằng khi người ta tăng số chuỗi mỗi tác vụ ở tính đa dạng tác vụ cố định, crossover mượt mà trong hiệu suất của PT giữa dMMSE và Ridge, được hiển thị trong Hình 2, trái trên và dưới, sẽ trở nên sắc nét hơn và sắc nét hơn trong tính đa dạng tác vụ, cuối cùng thể hiện một chuyển đổi pha sắc nhọn trong giới hạn số chuỗi vô hạn mỗi tác vụ. Đáng chú ý, chuyển đổi pha này trong thuật toán ICL được thực hiện bởi PT xuất hiện ở ngưỡng đa dạng tác vụ vừa phải dưới 215 tác vụ tiền huấn luyện; mặc dù dMMSE hoạt động kém đáng kể so với Ridge trên TTrue ở tính đa dạng tác vụ này, PT vẫn không bị cản trở bởi tính đa dạng tác vụ giới hạn này và có thể giải quyết tối ưu các tác vụ mới.

**Tăng thời gian huấn luyện ở batch size cố định hỗ trợ thêm cho chuyển đổi pha thuật toán.** Để xác nhận các kết quả trên, chúng tôi cũng tăng số chuỗi mỗi tác vụ, ở mỗi tính đa dạng tác vụ, bằng cách tăng số bước huấn luyện N từ 500K lên 1M trong khi giữ batch size cố định ở 256. Chúng tôi quan sát rằng việc tăng gấp đôi N (thay đổi từ màu xanh nhạt sang đỏ trong Hình 3) và tăng gấp đôi B (thay đổi từ màu xanh nhạt sang đỏ trong Hình 2) có tác động rất giống nhau lên ΔT PT,dMMSE và ΔT PT,Ridge, cho cả T=TTrue và T=TPretrain. Quan trọng hơn, ngưỡng đa dạng tác vụ, mà chúng tôi xác định là điểm crossover trong ΔTTrue PT,Ridge giữa các batch size 256, 512, và 1024 ở 500K bước huấn luyện (Hình 2 dưới phải) xảy ra ở cùng số lượng tác vụ với điểm crossover giữa 500K và 1M bước ở batch size 256 (Hình 3, phải). Cho rằng hai cách tiếp cận của chúng tôi để huấn luyện transformer cơ bản trên nhiều dữ liệu hơn mang lại cùng ngưỡng đa dạng tác vụ, và việc tăng gấp đôi batch size dẫn đến thời gian huấn luyện nhanh hơn đáng kể so với tăng gấp đôi số bước, từ đây trở đi chúng tôi xem ngưỡng đa dạng tác vụ là điểm crossover trong ΔTTrue PT,Ridge giữa các batch size 256 và 512 khi huấn luyện trong 500K bước. Xem Phụ lục D để biết thêm ablation của batch size và bước huấn luyện cung cấp bằng chứng thêm về cách số chuỗi được thấy bởi transformer là yếu tố chính xác định sự tương đồng của dự đoán của nó với dMMSE và Ridge ở mỗi số lượng tác vụ.

**Động học học tập và sự gián đoạn trong scaling của thời gian early stopping hỗ trợ thêm cho chuyển đổi pha thuật toán.** Để khám phá xem chuyển đổi quan sát được chỉ là tác động của under-fitting, chúng tôi nghiên cứu động học học tập của các PT nhỏ trong chế độ số bước rất lớn. Đầu tiên, trong Phụ lục E, chúng tôi xác minh rằng PT nhỏ cũng thể hiện chuyển đổi pha thuật toán nhưng ở ngưỡng đa dạng tác vụ thấp hơn giữa 211 và 212 tác vụ tiền huấn luyện. Trong Hình 4 trái, chúng tôi hiển thị các đường cong học tập (ΔTTrue PT,Ridge vs bước huấn luyện) của các PT được huấn luyện trong 500K bước ở batch size 512 với tính đa dạng tác vụ tiền huấn luyện, M, dưới và trên ngưỡng đa dạng tác vụ, M∗. Với M < M∗, ΔTTrue PT,Ridge giảm sớm trong quá trình huấn luyện cho đến khi nó đạt minimum tại bước thời gian t∗, và sau đó tăng khi PT tiến gần đến dMMSE. Chúng tôi định nghĩa t∗ là thời gian early stopping cho Ridge. Với M > M∗, ΔTTrue PT,Ridge giảm suốt quá trình huấn luyện. Để đánh giá xem, trong trường hợp sau, các mô hình có bị under-trained và t∗ lớn hơn tổng thời gian huấn luyện không, chúng tôi mở rộng huấn luyện đến 2M bước ở batch size 512 (4× thời gian huấn luyện, xem Phụ lục B). Hình 4 giữa, hiển thị các đường cong học tập này cùng với mô hình được huấn luyện với tính đa dạng tác vụ vô hạn; ngay cả trong chế độ huấn luyện dài này, ngưỡng đa dạng tác vụ không thay đổi. Với cả thời gian huấn luyện ngắn và dài, các mô hình được huấn luyện với cùng M có hành vi định tính tương tự (việc khoảng cách đến Ridge giảm rồi tăng hay giảm đơn điệu). Ngoài ra, đường cong học tập của các mô hình với M > M∗ rất giống với đường cong học tập của các mô hình được huấn luyện trên tính đa dạng tác vụ tiền huấn luyện vô hạn và chúng đạt được độ chính xác cuối cùng tương tự (đường đứt nét vs điểm đánh dấu trong Hình 10), cho thấy những mô hình này đang tiến gần đến nghiệm Ridge.

Trong Hình 4 phải, chúng tôi nghiên cứu cách t∗ scale với M. Với hầu hết M < M∗, t∗ tuân theo hành vi scaling đơn giản t∗∝Mα, α≈0.47. Tuy nhiên, với M > 210, khoảng cách đến Ridge giảm đơn điệu suốt quá trình huấn luyện và t∗=2M bước. Bất chấp lưu ý rằng các thí nghiệm của chúng tôi nhất thiết ở chế độ bước huấn luyện lớn nhưng hữu hạn với lịch learning rate suy giảm, sự gián đoạn rõ ràng này trong hành vi scaling của t∗ gần ngưỡng đa dạng tác vụ cho thấy chuyển đổi quan sát được không chỉ do under-fitting mà còn do sự khác biệt cơ bản trong động học học tập.

**Chuyển đổi dọc theo đường nội suy.** Để có thêm mô tả về chuyển đổi thuật toán trong PT từ dMMSE sang Ridge, chúng tôi tính hiệu suất ICL của PT, và so sánh nó với cả dMMSE và Ridge, trên họ một tham số của các tác vụ mới wα nội suy giữa các cặp tác vụ đã thấy wi và wj trong support của TPretrain. Đường nội suy được cho bởi

wα = [1/2(∥wi∥2+∥wj∥2)] * [αwi + (1−α)wj] / ∥αwi + (1−α)wj∥2 với α∈[0,1]. (5)

Ở đây chúng tôi cố định norm của vector nội suy wα thành trung bình của hai norm điểm cuối để tránh ∥wα∥ nhận giá trị rất nhỏ cho α∼1/2. Hình 5 hiển thị kết quả của phân tích này cho 25 (trái, chế độ đa dạng tác vụ thấp), 210 (giữa, dưới ngưỡng đa dạng tác vụ), và 215 (phải, vừa trên ngưỡng đa dạng tác vụ) tác vụ. Ở mỗi giá trị α, MSE được tính trung bình trên số lượng lớn các cặp tác vụ (wi,wj). Kiểm tra hiệu suất trung bình tại trung tâm của nhiều đường nội suy, tương ứng với các tác vụ hoàn toàn mới xa cách các tác vụ đã thấy trong quá trình tiền huấn luyện, rõ ràng tiết lộ một chuyển đổi trong hiệu suất PT từ dMMSE sang Ridge, nơi các tác vụ mới chỉ có thể được học tối ưu trên, chứ không phải dưới, ngưỡng đa dạng tác vụ. Ngược lại, không giống PT, dMMSE không thể giải quyết các tác vụ mới ở bất kỳ tính đa dạng tác vụ nào trong phạm vi xem xét.

**PT vượt trội so với mô hình dMMSE được làm mịn.** Chúng tôi đã thấy rằng ở tính đa dạng tác vụ trung gian PT vượt trội đáng kể so với dMMSE trên các tác vụ mới trong TTrue. Rõ ràng tại sao dMMSE hoạt động kém trên các tác vụ mới trong TTrue ở tính đa dạng tác vụ thấp: prior của nó trên các tác vụ tập trung vào M tác vụ duy nhất trong TPretrain, trong khi prior trên các tác vụ trong TTrue là Gaussian. Một phỏng đoán tự nhiên là PT không thể ghi nhớ tất cả M tác vụ trong TPretrain với M đủ lớn. Do đó chúng tôi cũng so sánh hiệu suất PT với ước lượng dMMSE được làm mịn trong đó prior điểm rời rạc trên M tác vụ đã thấy trong tiền huấn luyện được thay thế bằng hỗn hợp M Gaussian đẳng hướng với cùng tâm nhưng với variance được chọn để tối ưu hiệu suất trên TTrue (xem Phụ lục G để biết chi tiết). dMMSE được làm mịn này vượt trội so với dMMSE vì nó có prior trên các tác vụ gần hơn với TTrue Gaussian. Nhưng đáng chú ý, PT vẫn vượt trội so với dMMSE được làm mịn ngay cả với làm mịn tối ưu (Hình 12). Điều này chỉ ra PT, ngay cả ở tính đa dạng tác vụ vừa phải, thực hiện thuật toán tinh vi hơn so với dMMSE được làm mịn đơn giản phát sinh từ khả năng không thể phân giải M tác vụ tiền huấn luyện với độ chính xác cao của PT.

### 3.2 PT thể hiện scaling vượt trội của ngưỡng đa dạng tác vụ với chiều so với dMMSE.

Tiếp theo chúng tôi khám phá sự phụ thuộc của ngưỡng đa dạng tác vụ vào chiều của bài toán hồi quy D. Chúng tôi thay đổi D∈{8,16,32} trong khi đồng thời scale up độ dài ngữ cảnh tối đa thành K=2D, và tăng nhiễu quan sát σ2 để khớp SNR với D=8 và σ2=0.25. Chúng tôi cũng huấn luyện một mô hình lớn hơn với 12 lớp, embedding 256 chiều, và 4 attention head đủ biểu cảm để khớp hiệu suất Ridge ở D=32. Hình 6, 3 panel đầu tiên tiết lộ rằng ngưỡng đa dạng tác vụ của PT tăng vừa phải (gần như tuyến tính) với chiều tác vụ (tức là khoảng 214, 215, và 216 ở D=8, 16, và 32 tương ứng). Scaling tuyến tính này đáng chú ý xem xét thể tích của tất cả các tác vụ có thể scale theo cấp số nhân với chiều do sự tập trung của TTrue Gaussian vào một sphere với D lớn. Do đó chúng tôi mong đợi hiệu suất dMMSE scale kém hơn nhiều với chiều D vì số lượng hữu hạn các tác vụ trong TPretrain sẽ cần bao phủ phần đáng kể của sphere để dMMSE tiến gần đến Ridge. Để kiểm tra giả thuyết này, với M=220, là tính đa dạng tác vụ lớn nhất chúng tôi xem xét, chúng tôi khám phá cách sự tương đồng của dự đoán PT và dMMSE với Ridge trên các tác vụ mới scale với D (Hình 6, panel phải). Chúng tôi thấy rằng ΔTTrue dMMSE,Ridge tăng đáng kể khi chúng tôi tăng D, trong khi đáng chú ý ΔTTrue PT,Ridge phần lớn độc lập với chiều. Nhìn chung điều này chỉ ra rằng scaling của lỗi PT với chiều vượt trội hơn rất nhiều so với dMMSE; PT vẫn gần tối ưu và gần Ridge ở tất cả D với M=220, trong khi dMMSE rời xa Ridge khi D tăng.

### 3.3 Tác động của chính quy hóa và dung lượng mô hình lên ngưỡng đa dạng tác vụ.

Chúng tôi nghiên cứu sự phụ thuộc của ngưỡng đa dạng tác vụ vào các siêu tham số khác nhau. Đầu tiên, việc thêm chính quy hóa rõ ràng dưới dạng weight decay (xem Phụ lục B để biết chi tiết), và tăng giá trị của nó qua ba bậc độ lớn, làm giảm nhất quán ngưỡng đa dạng tác vụ (Hình 7, trái). Tuy nhiên lưu ý, ngưỡng đa dạng tác vụ thấp hơn cũng đi kèm với hiệu suất tệ hơn (Hình 13, trên). Điều này cho thấy các dạng chính quy hóa ngầm khác nhau có thể giúp thúc đẩy chuyển đổi thuật toán trong PT mà không có weight decay. Chúng tôi cũng khám phá tác động của dung lượng mô hình lên ngưỡng đa dạng tác vụ bằng cách tăng chiều embedding của cả PT nhỏ và cơ bản hoặc tăng độ sâu của PT nhỏ. Hình 7 giữa hiển thị rằng tăng chiều embedding trong phạm vi hợp lý không ảnh hưởng đến ngưỡng đa dạng tác vụ của PT cơ bản. Tuy nhiên, với PT nhỏ, tăng chiều embedding (Hình 7 giữa) hoặc độ sâu (Hình 7 phải) đều tăng ngưỡng đa dạng tác vụ. PT cơ bản có dung lượng lớn hơn nhiều so với PT nhỏ và cũng có ngưỡng lớn hơn; chúng tôi giả thuyết rằng PT nhỏ vẫn ở chế độ mà ngưỡng nhạy cảm với dung lượng trong khi PT cơ bản thì không. Cùng nhau, những kết quả này cho thấy dung lượng mô hình đóng vai trò quan trọng trong sự xuất hiện của học trong ngữ cảnh: tăng dung lượng (đến một điểm) dẫn đến tăng ngưỡng đa dạng tác vụ.

## 4 Nghiên cứu liên quan

Khuôn khổ Bayesian cho ICL được giới thiệu bởi Xie et al. [3], thúc đẩy nghiên cứu của chúng tôi, giả thuyết rằng các PT "định vị" các khái niệm được học trong quá trình tiền huấn luyện để giải quyết các tác vụ ICL. Một loạt nghiên cứu thực nghiệm trong các mô hình ngôn ngữ [18–20] sử dụng khuôn khổ này để chọn các ví dụ trong ngữ cảnh tốt hơn trong khi Min et al. [21] sử dụng nó để nghiên cứu tính robust của suy luận tác vụ tiềm ẩn. Nghiên cứu của chúng tôi xây dựng trên khuôn khổ này trong thiết lập hồi quy tuyến tính và xác thực nó ở tính đa dạng tác vụ thấp. Tuy nhiên, chúng tôi tìm thấy một chế độ—số lượng lớn nhưng hữu hạn các tác vụ tiền huấn luyện—trong đó khả năng học các tác vụ mới trong ngữ cảnh là một hiện tượng xuất hiện không thể được giải thích hoàn toàn bằng suy luận Bayesian.

Nghiên cứu trước đây [12–14] cũng đã chỉ ra rằng các transformer có thể thực hiện hồi quy tuyến tính trong ngữ cảnh. Tuy nhiên, họ tiền huấn luyện với tính đa dạng tác vụ không giới hạn, lấy mẫu một vector hồi quy hoàn toàn mới cho mỗi chuỗi. Ngược lại, nghiên cứu của chúng tôi xem xét các tập dữ liệu tiền huấn luyện với tính đa dạng tác vụ giới hạn nơi ICL trên các tác vụ mới xuất hiện mặc dù loss tiền huấn luyện không mã hóa rõ ràng điều này. Một hướng nghiên cứu khác giả thuyết rằng ICL thực hiện gradient descent trong các activation của lượt truyền tiến, cung cấp các cấu trúc rõ ràng cho các trọng số của PT để thực hiện điều này cho hồi quy tuyến tính [13,14] hoặc khám phá giả thuyết này trong các mô hình ngôn ngữ [22]. Tuy nhiên cần thêm thí nghiệm để kiểm tra giả thuyết rằng các transformer được huấn luyện thực sự khớp với các cấu trúc được đề xuất. Thay vì nghiên cứu cơ chế rõ ràng mà học trong ngữ cảnh được thực hiện, nghiên cứu của chúng tôi tập trung vào tác động của tính đa dạng tác vụ tiền huấn luyện. Các câu hỏi tương tự liên quan đến vai trò của đa dạng hóa tác vụ đã được khám phá trong văn học meta-learning [23–25].

Kirsch et al. [26] cũng chỉ ra sự xuất hiện của học trong ngữ cảnh với tính đa dạng tác vụ tiền huấn luyện trên một tác vụ phân loại đồ chơi. Bằng cách nghiên cứu câu hỏi này trong thiết lập có kiểm soát của hồi quy tuyến tính, chúng tôi có thể so sánh với các ước lượng tối ưu trên TPretrain và TTrue. Điều này cho phép chúng tôi thiết lập rằng ICL ở tính đa dạng tác vụ hữu hạn xuất hiện bởi vì PT lệch khỏi ước lượng tối ưu trên phân phối tác vụ tiền huấn luyện, và không chỉ là hậu quả của phân phối tác vụ tiền huấn luyện trở nên tương tự với phân phối tác vụ lý tưởng. Trong số các góc nhìn quan trọng khác về ICL, Chan et al. [27] xác định, trong thiết lập đồ chơi, một số tính chất của phân phối huấn luyện—burstiness và sự xuất hiện của các lớp hiếm—cần thiết cho sự xuất hiện của ICL. Wei et al. [28] nghiên cứu cách ICL trong các mô hình ngôn ngữ lớn bị ảnh hưởng bởi semantic prior và ánh xạ input-label, tập trung vào sự khác biệt qua quy mô mô hình. Olsson et al. [29] nghiên cứu induction head—các mạch chịu trách nhiệm hoàn thành pattern bằng cách sao chép token—như một cơ chế để thực hiện ICL.

## 5 Thảo luận

Nhìn chung, chúng tôi đã khám phá rộng rãi tác động của tính đa dạng tác vụ tiền huấn luyện lên sự xuất hiện của học trong ngữ cảnh của các tác vụ hoàn toàn mới không thấy trong quá trình tiền huấn luyện. Chúng tôi tìm thấy một số bất ngờ bằng cách làm việc trong thiết lập có kiểm soát của hồi quy tuyến tính, nơi chúng tôi có thể so sánh hiệu suất của PT với các ước lượng Bayesian tối ưu, hoặc cho phân phối tác vụ tiền huấn luyện đa dạng giới hạn TPretrain (tức là dMMSE), hoặc cho phân phối tác vụ lý tưởng đa dạng TTrue (tức là Ridge). Những so sánh này tiết lộ một chuyển đổi pha thuật toán trong PT từ cái trước sang cái sau ở ngưỡng đa dạng tác vụ trung gian; vượt qua ngưỡng này, PT giải quyết các tác vụ hoàn toàn mới không thấy trong quá trình tiền huấn luyện. Đáng chú ý, ngưỡng đa dạng tác vụ này scale vừa phải với chiều tác vụ, trong phạm vi các chiều được xem xét, bất chấp sự tăng trưởng theo cấp số nhân trong thể tích của tất cả các tác vụ có thể với chiều. Thực sự scaling PT này vượt trội hơn rất nhiều so với dMMSE.

Nhìn chung, những kết quả này chỉ ra rằng ICL của các tác vụ mới bởi các PT là một hiện tượng xuất hiện không thể được giải thích bằng suy luận Bayesian trên các phân phối tác vụ tiền huấn luyện đa dạng giới hạn. Hơn nữa, các thí nghiệm của chúng tôi cho thấy một dạng chính quy hóa ngầm nào đó trong các PT cho phép chúng thoát khỏi phân phối tác vụ tiền huấn luyện để giải quyết các tác vụ mới, với tính đa dạng tác vụ tiền huấn luyện vừa phải.

Đáng chú ý, vượt qua ngưỡng đa dạng tác vụ, các PT học ước lượng tối ưu cho mô hình sinh cơ bản cho các tác vụ tiền huấn luyện; điều này đúng cho cả prior Gaussian và Laplace trên các tác vụ (xem Hình 14 cho thí nghiệm với prior Laplace). Điều này đúng ngay cả khi tồn tại các nghiệm với training loss thấp hơn; thực sự khi được huấn luyện trên nhiều dữ liệu hơn ở tính đa dạng cố định, các PT hoạt động giống Ridge hơn với chi phí training loss cao hơn. Các thí nghiệm của chúng tôi trong Hình 4 cho thấy chuyển đổi thuật toán này do một thay đổi cơ bản trong động học học tập. Chúng tôi khám phá giả thuyết này bằng cách khám phá tính kết nối tuyến tính mode của loss landscape [30,31]. Trong Hình 11 chúng tôi tìm thấy rằng các PT được huấn luyện với M lớn cư trú trong cùng loss basin với các PT được huấn luyện với M=∞: rào cản training loss giữa các PT được huấn luyện với M≳213 và các PT với M=∞ tương tự như hai PT được huấn luyện với M=∞. Ngược lại, có các rào cản loss lớn giữa các PT được huấn luyện với M<213 và M=∞. Ngoài ra, các PT được huấn luyện với M=∞ gần hơn trong không gian trọng số với các PT được huấn luyện với M lớn hơn so với những PT được huấn luyện với M nhỏ (xem Phụ lục F). Nhìn chung, những thí nghiệm này cung cấp bằng chứng thêm rằng các PT được huấn luyện với tính đa dạng tác vụ vượt qua ngưỡng tìm thấy các nghiệm tương tự với mô hình tối ưu cho TTrue; chúng tôi để lại khám phá thêm những loss landscape này cho nghiên cứu tương lai.

Một câu hỏi hấp dẫn là những quan sát này chuyển đổi như thế nào sang ngôn ngữ. Một bí ẩn chính về hiệu quả của ICL trong các tác vụ ngôn ngữ nằm ở việc các tác vụ được học trong ngữ cảnh khác biệt như thế nào so với phân phối tiền huấn luyện của các corpus ngôn ngữ lớn. Cũng ít rõ ràng hơn về cách phân loại nội dung của các corpus như vậy theo các tác vụ và đo lường tính đa dạng tác vụ kết quả của chúng. Bất kể, quan sát của chúng tôi trong hồi quy tuyến tính rằng một ngưỡng vừa phải trong tính đa dạng tác vụ tiền huấn luyện có thể cho phép các PT giải quyết các tác vụ mới có thể ngụ ý rằng nhiều tác vụ ngôn ngữ khá khác biệt so với thống kê của các corpus ngôn ngữ lớn vẫn có thể được giải quyết trong ngữ cảnh.

Kết quả của chúng tôi cũng cho thấy quy mô dữ liệu đơn thuần không dẫn đến hiệu suất ICL tốt. Thực tế, dưới ngưỡng đa dạng tác vụ, việc tăng kích thước tập dữ liệu tiền huấn luyện mà không tăng tính đa dạng tác vụ làm tổn hại hiệu suất ICL. Cần thiết phải tăng cả tính đa dạng và kích thước của tập dữ liệu để ICL xuất hiện. Do đó để cải thiện ICL trong thiết lập ngôn ngữ, nghiên cứu của chúng tôi thúc đẩy các nghiên cứu tương lai về khám phá khái niệm tác vụ liên quan trong mô hình hóa ngôn ngữ và các cách tiếp cận để tăng tính đa dạng tác vụ trong các corpus ngôn ngữ. Tổng quát hơn, phân tích thực nghiệm của chúng tôi về tác động của tính đa dạng tác vụ tiền huấn luyện lên ICL thúc đẩy các nghiên cứu lý thuyết thêm. Các nghiên cứu như vậy sẽ là chìa khóa để hiểu bí ẩn tại sao dự đoán token tiếp theo đơn giản trong quá trình tiền huấn luyện có thể dẫn đến học trong ngữ cảnh của rất nhiều tác vụ khác nhau một cách rõ ràng.

## Lời cảm ơn

Các tác giả muốn cảm ơn Google TPU Research Cloud (TRC) với sự hỗ trợ hào phóng đã làm cho dự án này có thể thực hiện được. SG cảm ơn giải thưởng NSF CAREER và NTT Research cho sự hỗ trợ.
