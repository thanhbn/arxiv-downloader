# 2306.03799.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2306.03799.pdf
# File size: 3104316 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Prompt Space Optimizing Few-shot Reasoning Success with Large
Language Models
Fobo Shi1*†, Peijun Qing2,∗,†, Dong Yang3,∗‡, Nan Wang3, Youbo Lei4,†
Haonan Lu3,‡,Xiaodong Lin5,Duantengchuan Li1
1Wuhan University,2Dartmouth College,3OPPO Research Institute,
4Xi’an Jiaotong University,5Rugster Unversity
Abstract
Prompt engineering is an essential technique
for enhancing the abilities of large language
models (LLMs) by providing explicit and spe-
cific instructions. It enables LLMs to excel
in various tasks, such as arithmetic reasoning,
question answering, summarization, relation
extraction, machine translation, and sentiment
analysis. Researchers have been actively ex-
ploring different prompt engineering strategies,
such as Chain of Thought (CoT), Zero-CoT,
and In-context learning. However, an unre-
solved problem arises from the fact that cur-
rent approaches lack a solid mathematical so-
lution for determining optimal prompts. To
address this issue in prompt engineering, we
propose a new and effective approach called
Prompt Space. Our methodology utilizes text
embeddings to obtain basis vectors by matrix
decomposition, and then constructs a space for
representing all prompts. Prompt Space sig-
nificantly outperforms state-of-the-art prompt
paradigms on ten public reasoning benchmarks.
Notably, without the help of the CoT method
and the prompt "Let’s think step by step",
Prompt Space shows superior performance over
the few-shot method. Overall, our approach
provides a robust and effective mathematical
framework for selecting simple and effective
prompts. This advancement marks a significant
step towards improving prompt engineering for
a wide variety of applications in LLMs. Our
code is publicly available at https://github.
com/YouBLEI/Prompt-Space
1 Introduction
Prompt engineering becomes a relatively new
and hot discipline for designing and optimizing
prompts to effectively use large language models
*Equal contribution.
†The work was done during their internship in OPPO
Research Institute.
‡Corresponding author (dongyang3-c@my.cityu.edu.hk);
luhaonan@oppo.com.(LLMs) for a wide variety of applications and re-
search domains (Brown et al., 2020; Thoppilan
et al., 2022; Zhou et al., 2022; Sun et al., 2022;
Dong et al., 2022). Researchers explore the use
of simple and specific instructions to enhance the
performance of LLMs on complex tasks, including
arithmetic and commonsense reasoning, as well
as question answering (Chowdhery et al., 2022;
Scao et al., 2022; Ouyang et al., 2022; Bai et al.,
2022). Developers strive to design robust and effec-
tive prompts either manually (Schick and Schütze,
2020; Reynolds and McDonell, 2021) or automati-
cally (Gao et al., 2020) that interface with LLMs
and other tools (Wu et al., 2023; Xie et al., 2023).
The goal is to uncover the full potential of LLMs
across various domains, enabling them to tackle
complex tasks with improved performance and ac-
curacy.
To elicit the reasoning ability of LLMs, (Wei
et al., 2022) has proposed the concept of the chain-
of-thought (CoT) prompting. Unlike traditional
input-output exemplars, the CoT prompting cre-
ates a series of intermediate reasoning steps that
guide LLMs through a complex problem. This ap-
proach enables LLMs to develop a reasoning path
that decomposes the complex problem into multi-
ple reasoning steps. Notably, the CoT prompting
demonstrates that the reasoning ability of LLMs
perfectly matches the scaling laws, with the reason-
ing ability of LLMs significantly increasing with
the size of the PaLM 540B model. Inspired by
the CoT prompting, several works explore methods
to enhance LLMs’ reasoning abilities with sim-
ple techniques. (Kojima et al., 2022) introduces
the "Let’s think step by step" prompt, which helps
LLMs adopt a step-by-step thinking approach, lead-
ing to the final answer. Their approach, known as
Zero-shot-CoT, successfully generates a reason-
ing path in zero-shot reasoning scenarios. In prac-
tice, the CoT prompting has showed better per-
formance than Zero-shot-CoT (Wei et al., 2022;arXiv:2306.03799v2  [cs.CL]  28 Mar 2024

--- PAGE 2 ---
Kojima et al., 2022). However, the CoT prompting
involves huge efforts in the manual design of both
questions and related reasoning chains. To avoid
the manual method, (Zhang et al., 2022) proposes
an automatic CoT prompting, called Auto-CoT. It
applies a clustering algorithm to identify represen-
tative questions for each cluster and generates rea-
soning chains using the Zero-shot-CoT method for
each question.
The previous works on CoT have greatly con-
tributed to our understanding of effective prompts
for improving the reasoning ability of LLMs. How-
ever, these works have certain limitations, such as
the lack of guidance on finding optimal prompts
for reasoning tasks. In this paper, we propose
a novel approach called Prompt Space that over-
comes these limitations and simultaneously lever-
ages the strengths of previous works. Our approach
starts by embedding questions and then utilizes ma-
trix decomposition to yield basis vectors, or basis
questions. These basis questions are used to con-
struct a space that can represent all questions. With
Zero-shot-CoT, we combine these basis questions
with every question to automatically generate rea-
soning demonstrations for LLMs. Our approach
offers a promising solution to find optimal prompts
on reasoning tasks and significantly improves the
few-shot reasoning of LLMs.
Prompt Space surpasses the performance of cur-
rent prompt paradigms on ten public reasoning
benchmarks. Our work uncovers critical insights
into the impact of the number of basis questions on
reasoning tasks. Additionally, we identify the rela-
tionship between the selected questions and the rea-
soning ability of LLMs, and investigate how to de-
termine the optimal number of exemplars for each
reasoning task. Extensive experiments demonstrate
that our approach establishes a reliable and mathe-
matical methodology for selecting simple and ef-
fective prompts. Our goal is to not only design
the robust and effective prompts for challenging
reasoning tasks, but also highlight the significance
of carefully exploring and analyzing the optimal
prompts for unlocking the potential of LLMs in a
wide variety of applications.
2 Related Work
2.1 Chain-of-thought Prompting
Chain-of-thought (CoT) prompting is an effective
method to elicit the reasoning ability of LLMs
through a chain of thought, where a series of inter-mediate reasoning steps are used to generate the
answer (Wei et al., 2022). This approach has been
shown to significantly improve the performance
of LLMs on complex reasoning tasks. To further
enhance their performance, self-consistency (SC)
has been introduced, which replaces the standard
greedy decoding of the LLM output with a stochas-
tic output space ensemble (Wang et al., 2022b). Ex-
isting studies on the CoT prompting can be broadly
divided into two categories: manually constructed
the CoT prompting and automatically generated
the CoT prompting. Our work aims at providing a
robust and mathematical framework for selecting
simple and effective prompts.
2.2 Automatically Generated CoT Prompts
For enhancing CoT reasoning in LLMs, several
previous works have explored the idea of self-
generating a chain of thought (Kojima et al., 2022;
Zhang et al., 2022; Zhou et al., 2022; Hebenstreit
et al., 2023). (Kojima et al., 2022) finds that us-
ing specific phrases, like "Let’s think step by step",
as a prompt can guide LLMs to generate reason-
ing steps without any few-shot hand-crafting ex-
emplars. Following this work, (Zhou et al., 2022)
proposes a framework called Automatic Prompt En-
gineer (APE) for generating and selecting instruc-
tions automatically. APE addresses the instruction
generation problem by using LLMs to generate and
search for candidate solutions.
Additionally, some studies implement Zero-shot-
CoT to generate the reasoning process in their
demonstration (Kojima et al., 2022). (Zhang et al.,
2022) proposes a novel method called Auto-CoT,
for automatically creating the Chain of Thought
(CoT) prompting in LLMs. This method sam-
ples diverse questions and reasoning chains to con-
struct effective demonstrations for LLMs. It can
elicit chain-of-thought reasoning without impair-
ing performance and eliminating the need for hand-
crafting prompts. In contrast, (Shao et al., 2023)
utilizes seed demonstrations to automatically syn-
thesize more examples through forward and back-
ward processes. Inspired by these works, we con-
struct a space with text embeddings and the matrix
decomposition to represent all questions. We also
utilize Zero-shot-CoT to generate chains of thought
for prompt examples (Kojima et al., 2022).
2.3 Example Selection
For designing prompts, certain studies demonstrate
that the performance of LLMs is influenced by var-

--- PAGE 3 ---
SVD
𝑄𝑚×𝑛𝑈𝑚×𝑚 𝛬𝑚×𝑛 𝑉𝑛×𝑛
(𝑈𝑘)𝑘×𝑚(𝑄𝑘)𝑘×𝑛𝑄𝑛×𝑚𝑇
Basis Vector 𝑞𝑖′Calculate the 
similarity 
between𝒒𝒊′and 𝒒𝒋
A question embedding 𝑞𝑗Select 𝒌principal 
components
𝑄𝑚×𝑛argmax
Indices of 
approximate 
basis vectors:
{𝐼1,𝐼2,…𝐼𝑘}
1≤𝐼𝑗≤𝑚Question[1]:  “Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?”
…
Question[m]: “Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many 
lollipops did Jason give to Denny?”
Demonstration Construction
Question[ 𝐼1],  A: Let’s think step by step. ….. 1
Question[ 𝐼𝑘],  A: Let’s think step by step. ….. k●
● Finding basis questions and their answers
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Let's think step by step. Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 
42 = 74. After eating 35, they had 74 -35 = 39.
…
Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
A: Let’s think step by step. 
Same prompt exemplars are adhered to each question
Test questionLLM In-Context Reasoning
A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 -15 dollars left. 23 -15 
is 8.Step 1: Embedding Questions.
Step 2: Finding basis vectors.
Step 3: Selecting basis questions.Figure 1: The schematic of Prompt Space. Prompt Space consists of three steps, including embedding questions,
finding basis vectors, and selecting basis questions. First, all questions in a reasoning dataset are encoded into the
question matrix Qm×n. Second, the kbasis vectors in the question matrix Qm×nare calculated by SVD and PCA.
Finally, the top kquestions are selected to form the question space. The prompt exemplar is made up of the selected
kquestions and the test question. LLMs could follow the prompt exemplar and then develop a chain of reasoning
steps to get the final answer.
ious factors, such as tasks, prompts, and model
structures (Zhao et al., 2021; Lu et al., 2021; Su
et al., 2022; Griffin et al., 2023; Jiang et al., 2022).
The main challenge is to develop selection criteria
that are both effective and generalizable based on
empirical experiments. (Wang et al., 2022b) shows
that the sequence of reasoning steps is critical for
achieving optimal performance. Additionally, (Ru-
bin et al., 2021) proposes a similarity-based se-lection method, which retrieves the most similar
training instances as a prompt for a given test case.
Furthermore, another approach proposed by (Fu
et al., 2022) suggests the selected prompts with
more steps can significantly improve performance
in the reasoning process. However, our method,
Prompt Space, explores question embeddings to
obtain basis questions in a reasoning task, which
dramatically avoids using ineffective questions as

--- PAGE 4 ---
a demonstration. This approach provides an inno-
vative mathematical solution for selecting effective
prompts, yielding more generalizable and compre-
hensive reasoning chains. Our Prompt Space aims
to develop a deep understanding of how to design
the CoT prompting.
3 Prompt Space
In this work, we propose a novel method called
Prompt Space, which automatically creates demon-
strations with questions and reasoning chains.
Prompt Space seeks to design an appropriate space
for identifying the basis questions for building
prompt exemplars. For a vector space V, its vec-
tor basis is defined as a subset v1,v2...,vninV.
These basis vectors are linearly independent in span
V. Consequently, if ( v1,v2...,vn) is a list of vec-
tors in V, then these vectors form a vector basis if
and only if every x∈Vcan be uniquely written as
x=c1v1+c2v2+...+cnvn, (1)
where c1, c2, ..., c nare elements of the base field.
In Prompt Space, one vector represents a ques-
tion embedding. By combining the basis questions
with the test question, we create a demonstration
that enables LLMs to effectively generate a chain
of thought. Next, we will show how to select such
basis questions for constructing Prompt Space.
Selecting effective prompts as an exemplar can
significantly enhance the reasoning abilities of
LLMs. To solve arithmetic problems, humans tend
to learn from previous question-answer pairs and
generalize them to solve similar problems. Inspired
by this thought mechanism, our work aims to select
more representative questions as an exemplar to fa-
cilitate LLMs in developing a chain of reasoning
steps. We assume that there exists a real prompt
space Pwithkdimensional vectors, where the se-
lected representative questions can serve as basis
vectors of this space. These basis vectors provide
an effective solution for LLMs to reason through
the problem space.
Principal component analysis (PCA) is a widely
used algorithm for identifying the key components
of extensive data features by geometric steps (Abdi
and Williams, 2010). The implementation of PCA
can efficiently compress a n-dimension matrix into
ak-dimension matrix and obtain kprincipal vectors
from the original space. Inspired by PCA, Prompt
Space follows the below steps:
1.Embedding Questions. The question set of a
task is Q={q1, q2, ..., q m}, where mis thenumber of questions in a task. The MiniLM-
L6-v2 model fMiniLM (Wang et al., 2020)
encodes these questions as follows: qi=
fMiniLM (qi)∈Rn, fori= 1,2, ..., m . Af-
ter the encoding process, the question matrix
Qis created by putting together all question
embeddings, i.e., Q= [q1,q2, ...,qm]T∈
Rm×n.
Assuming the dimension (rank) of the prompt
space Pisk, the process of finding kbasis
vectors is identical to searching for the kprin-
cipal components (questions) of the question
matrix Q.
2.Finding basis vectors. We use Singular Value
Decomposition (SVD) to calculate kbasis vec-
tors in the prompt space P(Wall et al., 2003).
Using SVD, Qcan be calculated as:
Q=UΛVT, (2)
where Uis denoted as a left singular ma-
trix,U= [u1,u2, ...,um]T∈Rm×m, and
ui∈R1×mis the eigenvector of QQT∈
Rm×m(fori= 1, ..., m ). A complete proof
is shown in Appendix A. Similarly, Vis the
right singular matrix, which can be written
asV= [v1,v2, ...vm]T∈Rn×n,vi∈Rn×1
(fori= 1, ...n) is the eigenvector of QTQ.
Next, the kprincipal components of Qcan be
obtained:
Qk=UkQ, (3)
where Uk= [u1,u2, ...uk]T∈Rk×m, and
Qk∈Rk×ncontains the top kprincipal com-
ponents ranked by related eigenvalues. The
row vectors in Qk= [q′
1,q′
2, ...q′
k]T∈Rk×n
arekbasis vectors of the prompt space P.
3.Selecting basis questions. In this step, we
select the top kquestions from the question
matrix Qas basis questions, whose embed-
dings are closest to these basis vectors. It can
result in:
f(x) =argmax (x·QT),
for x∈ {q′
i∈R1×n, i= 1,2...k}(4)
where argmax (•)is to calculate the maxi-
mum similarities between question embed-
dings and basis vectors (i.e, cosine similarity)
(Sidorov et al., 2014). Finally, we can gen-
erate the prompt exemplar, including kbasis
questions and the original question in Q, to
get the final output (answer).

--- PAGE 5 ---
Figure 1 shows that an example of Prompt Space
produces basis questions to solve an arithmetic
problem. By following the three steps, we could
select kbasis questions and then combine them
with the test question. To assist LLMs in generating
the final output, we also use the prompt, "Let’s
think step by step". Throughout the process, we
still opt to automatically create the prompt rather
than manually design it. As a result, LLMs can
generate a step-by-step thought process for arriving
at the answer. The algorithm of Prompt Space is
shown in Appendix B.
Prompt Space has several attractive properties as
an approach for enhancing reasoning in LLMs.
1.Prompt Space enables LLMs to identify op-
timal prompts for a range of reasoning tasks
and efficiently generate final outputs.
2.Prompt Space provides a robust mathemati-
cal framework for designing the prompt. It
can suggest the optimal number of exemplars
to improve the reasoning abilities of LLMs.
Our method provides valuable insights into
effective prompting strategies for achieving
successful outcomes.
3.Prompt Space has the potential to be utilized
in a variety of few-shot tasks through prompt
engineering, including but not limited to trans-
lation, summarization, and expansion.
4 Experiments
We briefly describe the experimental setup and
highlight the main results. Additional experimental
details and results can be found in Appendices C
and D.
Prompt Space is evaluated on three categories of
reasoning tasks, namely arithmetic reasoning, com-
monsense reasoning, and symbolic reasoning. The
experiment demonstrates Prompt Space on various
tasks: 1. Prompt Space outperforms the state-of-
the-art baselines on these tasks. 2. Prompt Space
can efficiently construct a space and find its basis
questions for each task. 3. Prompt Space can de-
termine the optimal number of basis questions for
significantly improving the performance of LLMs
on each dataset. 4. Prompt Space relies on the
selection of embedding models.
4.1 Experimental setup
Tasks and Datasets. Prompt Space is studied on
ten standard datasets from three categories of rea-
soning tasks:1. Arithmetic reasoning contains six datasets:
(1)AddSub (Hosseini et al., 2014), (2) MultiArith
(Roy et al., 2015), (3) SingleEq (Koncel-Kedziorski
et al., 2015), (4) AQUA-RAT (Ling et al., 2017), (5)
SV AMP (Patel et al., 2021), (6) GSM8K (Cobbe
et al., 2021). These datasets are sorted by release
time. SingleEq and AddSub have plenty of easier
problems, while MultiArith, AQUA-RAT, SV AMP,
and GSM8K are more difficult and require multi-
step reasoning steps.
2. Commonsense reasoning: (1) Common-
senseQA (CSQA) (Talmor et al., 2019), (2) Strate-
gyQA (STQA) (Geva et al., 2021). CSQA is a chal-
lenging dataset for commonsense question answer-
ing. Its questions contain complex semantics that
often requires prior knowledge. STQA requires
multi-step reasoning with an inferred strategy in
the question.
3. Symbolic reasoning: (1) Last Letter Concate-
nation (Letter) and (2) Coin Flip (Coin) (Wei et al.,
2022). Last Letter Concatenation asks the model
to concatenate the last letters of words in a name.
We generate full names by randomly concatenating
names from samples. Coin Flip requires the model
to answer whether a coin is still heads up after peo-
ple either flip it or do not flip it. In this work, we
consider an out-of-domain test set, where examples
have more steps than those in exemplars.
The detailed description of each dataset is shown
in Appendix C.1.
Baselines. We compare our Prompt Space with
five baseline methods: Few-shot (Wei et al., 2022),
Manual-CoT (Wei et al., 2022), Zero-shot (Kojima
et al., 2022), Zero-shot-CoT (Kojima et al., 2022),
and Auto-CoT (Zhang et al., 2022). Few-shot eas-
ily selects question-answer pairs as a demonstra-
tion for feeding to LLMs. Manual-CoT involves
manually creating a series of reasoning chains as
a demonstration to elicit the reasoning ability of
LLMs. Zero-shot is a standard prompting tech-
nique for evaluating the abilities of LLMs. Zero-
shot-CoT randomly selects questions as demonstra-
tions, and then uses the prompt "Let’s think step
by step". Additionally, Auto-CoT utilizes cluster-
ing techniques to sample questions and generate
demonstrations with the Zero-shot-CoT method.
To ensure fair comparisons with the baselines, we
run experiments with consistent in-context exem-
plars and a constant seed across all methods and
datasets. Few-shot and Manual-CoT select the ex-
amples by human, while Auto-CoT select the ex-
amples by the K-means clustering algorithm. Our

--- PAGE 6 ---
Q: A marketing company … Calculate the amount of money Antonella earned if she sold goods worth $2500.A: Let's think step by step. Antonella earned … The answer is 4.…Q: Kyle bought last year's best-selling book for $19.50. This is with a 25% ... What was the original price of the book?A: Let's think step by step. The original price of the book was $19.50 with a 25% discount … The answer is 26.Q: Andy works in the pro shop of a tennis resort … 22 minutes to string with polyester string, and 18 minutes for hybrid sets? A: Let’s think step by step. LLMIn-Context ReasoningA: Andy needs to string a total of 12 racquets. 3 of them will take 15 minutes each,  … The answer is 227.Q: A marketing company … Calculate the amount of money Antonella earned if she sold goods worth $2500.A: Antonella earned … The answer is 4.…Q: Kyle bought last year's best-selling book for $19.50. This is with a 25% ... What was the original price of the book?A: The original price of the book was $19.50 with a 25% discount … The answer is 26.Q: Andy works in the pro shop of a tennis resort … 22 minutes to string with polyester string, and 18 minutes for hybrid sets? A:LLMIn-Context ReasoningA: To string 3 racquets with synthetic gut. it will take Andy 3 x 15 = 45 minutes… The answer is 227.Prompt-Space-CoT-ZeroPrompt-Space-CoT𝑘basis questions 𝑘basis questions Test questionTest question(a)(b)Figure 2: Prompt-Space-CoT-Zero (including the prompt, “Let’s think step by step” prompt) and Prompt-Space-CoT
(not including the prompt, “Let’s think step by step”) with an input-output exemplar of an LLM.
Prompt Space uses the same rationales with Zero-
CoT and Auto-CoT not Manual-CoT. Our CoT is
generated by LLMs not humans. Figure E3 shows
demonstrations of CSQA on difference methods
including Random selection, Manual-CoT, Auto-
CoT and Our Prompt Space. Please refer to Ap-
pendix C.2 for detailed baselines.
Implementation. We use the gpt-3.5-turbo-0301
version of the public ChatGPT model from the
OpenAI API with 175 billion parameters (Brown
et al., 2020; Ouyang et al., 2022). We select this
LLM because it has better performance than the
text-davinci-002 version of GPT-3, as reported in
(OpenAI, 2023; Bai et al., 2022). In the decod-
ing process, we set the temperature to 0 and use a
greedy searching algorithm to obtain results. For
zero-shot approaches, our results are deterministic.
Following (Wei et al., 2022), we set the number
of demonstrations kto 8, except for AQUA-RAT
(4) and Letter (4), StrategyQA (6), and CSQA (7).
However, Our Prompt Space can determine the opti-
mal number of basis questions for each task. In the
following sections, we will present a detailed anal-
ysis and provide further insights into the selection
of basis questions. The selected embedding models
are T5 models (base/large/XL/XXL) (Raffel et al.,
2020), E5 models (small/base/large) (Wang et al.,
2022a) and MiniLM-L6-v2 model (Wang et al.,
2020). The embedding size of each question in all
T5 models is 768, while for E5 models (small, base,
large), their embedding sizes are 384, 768, 1024
respectively. Our MiniLM-L6-v2 model encodes
questions with an embedding size of 384. Please
refer to Appendix C.3 for detailed model descrip-
tions. In our approaches, we investigate two types
of Prompt Space shown in Fig. 2. The first type
combines CoT with the “Let’s think step by step”prompt, denoted as Prompt-Space-CoT-Zero. In
contrast, the second type only uses CoT, namely
Prompt-Space-CoT.
4.2 Main Results
In the experiments, we evaluate Prompt Space on
ten datasets from three categories of reasoning
tasks. Due to the greedy decoding, the main re-
sults show deterministic results without error bars.
Notably, Table 1 and 2 show that Prompt Space
achieves superior performance over the state-of-
the-art methods on ten reasoning tasks, respectively.
Compared to Auto-CoT, Prompt space with the op-
timal number of exemplars achieves up to average
3.2% in Table 2.
Prompt Space vs Few-shot. Table 1 summa-
rizes comparisons between our approach (Prompt
Space) and two baselines (Zero-shot and Few-shot)
for each dataset. In Table 1, Prompt Space doesn’t
include CoT and the “Let’s think step by step”
prompt, and just selects basis questions as a demon-
stration. Our results show that Prompt Space with
the same settings achieves up to average 2.3%,
2% over Zero-shot and Few-shot on ten reasoning
datasets, respectively. Especially, Prompt Space,
with the optimal number of exemplars, achieves up
to average 3.3%, 3% over Zero-shot and Few-shot
on ten reasoning datasets, respectively. The most
significant improvement is observed in the STQA
and Letter datasets, with a relative increase of
13.5%, 112.5% over Few-shot, respectively. More-
over, Prompt Space outperforms two baselines on
eight out of ten reasoning datasets.
Arithmetic Reasoning. Our approach substan-
tially outperforms the three baselines on five arith-
metic reasoning tasks except for AddSub in Tab. 2.
Importantly, our Prompt Space with the same set-

--- PAGE 7 ---
Table 1: Accuracy (%) comparison of Prompt Space with two baselines on ten reasoning datasets. Two baselines
are Zero-shot and Few-shot, respectively. Ten benchmark datasets contain three categories, including arithmetic
reasoning, commonsense reasoning, and symbolic reasoning. The last column shows average scores. See Appendix C
for a detailed setup.
ModelArithmetic Commonsense SymbolicAvg
AddSub MultiArith SingleEq AQUA-RAT SV AMP GSM8K CSQA STQA Letter Coin
Zero-shot 87.6 80.0 87.4 27.6 74.0 22.9 73.6 61.3 0.8 22.8 53.8
Few-shot 85.8 82.7 89.4 89.489.4 30.7 76.1 76.176.1 24.0 79.3 54.0 1.6 57.0 58.1
Prompt Space w/o CoT-Zero 89.4 89.489.4 83.7 83.783.7 88.8 32.7 32.732.7 75.2 25.2 25.225.2 79.1 61.3 61.361.3 3.4 3.43.4 62.0 62.062.0 60.1 60.160.1
Prompt Space w/o CoT-Zero (best) 89.9(10) 89.9(10)89.9(10) 86.3 (9) 86.3 (9)86.3 (9) 88.8 (8) 32.7 (3) 32.7 (3)32.7 (3) 75.6 (6) 25.9 (6) 25.9 (6)25.9 (6) 80.0 (8) 80.0 (8)80.0 (8) 62.8 (10) 62.8 (10)62.8 (10) 5.2 (9) 5.2 (9)5.2 (9) 63.8 (6) 63.8 (6)63.8 (6) 61.1 61.161.1
tings achieves score gains of 1.8%, 1.2%, 2% and
2.1% over the previous state-of-the-art methods on
MultiArith, SingleEq, SV AMP, and GSM8K, re-
spectively. Although Prompt Space doesn’t show
competitive performance on AddSub, it is close
to Auto-CoT. Additionally, Promp Space achieves
the highest performance on AQUA-RAT, SV AMP,
and GSM8K, indicating that it can solve more com-
plex arithmetic reasoning. The difference between
Prompt-Space-CoT-Zero and Prompt-Space-CoT
is trivial, approximately 2%. Overall, the average
score of Prompt-Space-CoT surpasses that of the
three baselines on all arithmetic reasoning datasets,
indicating its superior performance.
Commonsense Reasoning. Prompt Space
significantly outperforms the prior state-of-the-
art Auto-CoT over two commonsense reasoning
datasets. Our approach with the same settings
achieves respective improvements of 1.9%, 1.6%
over Mannual-CoT and 1.4%, 0.9% over Auto-
CoT. Compared to Zero-shot, Zero-shot-CoT and
Manual-CoT don’t elicit better commonsense rea-
soning, while Prompt Space leverages the CoT
method to dramatically increase performance in-
stead of decreasing it. These results demon-
strate that Prompt Space can improve performance
on commonsense reasoning tasks requiring prior
knowledge.
Symbolic Reasoning. The performance of
Prompt Space achieves a significant increase of
3.2% over Mannual-CoT and 9.4%, over Auto-CoT
on the Letter dataset, respectively. Interestingly, the
accuracy of Mannual-CoT, Auto-CoT, and our ap-
proach reaches to 100% on the Coin Flip dataset.
The result indicates that Prompt Space dramatically
enhances the reasoning abilities of LLMs on sym-
bolic tasks.
4.3 Effect of Embedding Models
Figure 3 shows that the increase of embedding size
cannot improve the performance of Prompt Space
on various reasoning tasks. Besides, the appro-
priate embedding size could be 768 in T5 and E5models. As T5 models increase their model size,
the performance of Prompt Space decreases signifi-
cantly. Moreover, the solving rate of Prompt Space
exhibits clear fluctuations on different embedding
models.
4.4 Further Analysis of Basis Questions
Figue 4 illustrates the performance of Prompt
Space with different basis questions on nine
datasets. Our results reveal that the appropriate
number of basis questions is 8 on arithmetic reason-
ing tasks except for AQUA-RAT, while that of basis
questions is approximately 6 or 7 on commonsense
reasoning tasks. Interestingly, the AQUA-RAT and
Letter datasets exhibit a preference of a smaller
number of basis questions, which indicates their
space could be spanned by just four or five basis
vectors. Overall, our findings demonstrate that the
appropriate number of basis questions can signifi-
cantly improve performance, which indicates that
there exist basis vectors (questions) in the prompt
space. However, there remains a challenge that we
cannot automatically determine the optimal num-
ber of basis questions for each dataset. More anal-
ysis about basis questions of Prompt-Space-CoT
is shown in Appendix D.1. Besides, we provide
more visualizations of Prompt Space and the con-
structed demonstrations in Appendix D.2 and E,
respectively.
4.5 Effect of Question Sequence
Table 3 shows that Prompt Space achieves better
performance than other cases, when the basis ques-
tions are sorted in ascending order of their eigen-
values. However, the descending sort (original se-
quence) has superior performance over baselines
on three out of four benchmarks. Furthermore, the
difference between the original sequence and the
reverse sequence is trivial ( ∼0.1%). Thus, these
findings suggest that the descending sort is a ac-
ceptable approach used in our experiments.

--- PAGE 8 ---
Table 2: Accuracy (%) comparison of Prompt Space with four state-of-the-art methods on ten reasoning datasets.
These datasets contain three categories, such as arithmetic reasoning, commonsense reasoning, and symbolic
reasoning. The last column records average scores. See Appendix C for a detailed setup.
ModelArithmetic Commonsense SymbolicAvg
AddSub MultiArith SingleEq AQUA-RAT SV AMP GSM8K CSQA STQA Letter Coin
Zero-shot-CoT 82.5 96.0 90.4 38.2 76.5 57.1 72.0 57.6 71.0 64.4 69.9
Manual-CoT 86.8 97.0 90.9 45.3 80.2 75.8 72.2 61.7 78.8 100 100100 78.9
Auto-CoT 88.5 88.588.5 96.0 90.2 46.5 46.546.5 78.2 74.1 72.7 62.4 72.6 100 100100 78.1
Prompt-Space-CoT-Zero 87.3 98.0 89.2 36.2 82.2 82.282.2 72.4 71.1 63.3 63.363.3 82.0 82.082.0 100 100100 78.3
Prompt-Space-CoT 87.9 98.8 98.898.8 92.1 92.192.1 40.6 81.0 77.9 77.977.9 74.1 74.174.1 62.5 79.6 100 100100 79.5 79.579.5
Prompt-Space-CoT (best) 87.9 (8) 98.8 98.898.8(8) 92.5 92.592.5(4) 48.8 48.848.8(7) 82.6 82.682.6(10) 77.9 77.977.9(8) 77.9 77.977.9(8) 64.4 64.464.4(9) 84.4 84.484.4(1) 100 100100(3) 81.3 81.381.3
GSM8K SV AMP CSQA LetterT5 base (768) 
T5 XL (768)
E5 small (384)
E5 large (1024)
Ours (MiniLLM:384)E5 base (768)T5 XXL (768)T5 large (768)
60708090
Figure 3: Accuracy (%) of Prompt-Space-CoT-Zero with different embedding models on three types of reasoning
tasks: arithmetic reasoning (GSM8K, SV AMP), commonsense reasoning (CSQA), and symbolic reasoning (Letter).
The embedding models are T5 (base/large/XL/XXL), E5 (small/base/large), and MiniLM-L6-v2 (ours), respectively.
4 6 8 10406080100MultiArith
GSM8K
SingleEq
AddSub
SVAMP
AQUA -RAT
Number of 𝒌basis questionsAccuracy (%)(a)
3 8 10556575CSQA
STQA
4 5 6 7 9Accuracy (%)(b)
2 6 8Letter
3 4 5 7Accuracy (%)(c)
557080
60
Number of 𝒌basis questions Number of 𝒌basis questions
Figure 4: Accuracy (%) of Prompt-Space-CoT-Zero with various numbers of basis questions on nine reasoning
datasets.
Table 3: Accuracy (%) of Prompt-Space-CoT-Zero with different question sequences on three types of reasoning
tasks: arithmetic reasoning (GSM8K, SV AMP), commonsense reasoning (CSQA), and symbolic reasoning (Letter).
Our model ranks the sequence of basis questions by their eigenvalue scores from largest to smallest. Three different
sequences are evaluated: (a) original sequence, (b) reversed sequence, (c) random sequence.
SequenceArithmetic Commonsense SymbolicAvg
SV AMP GSM8K CSQA Letter
Random sequence 81.4 70.5 73.1 81.40 76.6
Reversed sequence 82.1 70.9 74.8 74.874.8 82.0 82.082.0 77.4 77.477.4
Original sequence (ours) 82.2 82.282.2 72.4 72.472.4 72.5 82.0 82.082.0 77.3
5 Conclusion
In this paper, we propose a novel prompting
method, namely Prompt Space, to explore the selec-
tion of prompts for enhancing reasoning in LLMs.
For any dataset, Prompt Space can map its ques-
tions onto a real space for determining basis ques-
tions as a demonstration. Through experiments on
arithmetic, commonsense, and symbolic reasoningtasks, we find that the demonstrations constructed
by Prompt Space can significantly improve the rea-
soning abilities of LLMs on ten public benchmarks.
Furthermore, without the help of the CoT method
and the "Let’s think step by step" prompt , Prompt
Space also exhibits superior performance than few-
shot and zero-shot learning in LLMs. Overall,
Prompt Space could serve as an efficient tool for

--- PAGE 9 ---
solving reasoning tasks, but also has the potential
to be a few-shot learner for a wide range of appli-
cations and tasks.
Limitations and Ethics Statement
Compared to state-of-the-art methods, Prompt
Space shows more competitive performance on
three categories of reasoning tasks. Besides, it will
significantly increases the capability and robust-
ness of the chain-of-thought method on massive
datasets. However, there are some potential limita-
tions to consider. First, the optimal number of basis
questions is observed by experimental results. Ad-
ditionally, the performance of Prompt Space could
be influenced by the selections of embedding mod-
els. Finally, we use an approximating method to
obtain top kbasis questions, which could increase
the uncertainty of this method. Overall, we will
continue to work on this problem to address these
limitations and develop more effective and robust
prompting methods.
For reproducibility, all experiments are run by
gpt-35-turbo version of the public ChatGPT model
from the OpenAI API with 175 billion parameters.
And these baseline methods are open-sourced im-
plementation. To aid reviewing, we summarize
the statistic of ten benchmark datasets, and include
configures of different embedding models and ex-
perimental settings in the supplementary materials.
References
Hervé Abdi and Lynne J Williams. 2010. Principal
component analysis. Wiley interdisciplinary reviews:
computational statistics , 2(4):433–459.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-
ong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and
Zhifang Sui. 2022. A survey for in-context learning.
arXiv preprint arXiv:2301.00234 .
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,
and Tushar Khot. 2022. Complexity-based prompt-
ing for multi-step reasoning. arXiv preprint
arXiv:2210.00720 .
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and
Tushar Khot. 2023. Complexity-based prompting for
multi-step reasoning. In The Eleventh International
Conference on Learning Representations .
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723 .
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics , 9:346–
361.
Lewis D Griffin, Bennett Kleinberg, Maximilian Mozes,
Kimberly T Mai, Maria Vau, Matthew Caldwell, and
Augustine Marvor-Parker. 2023. Susceptibility to
influence of large language models. arXiv preprint
arXiv:2303.06074 .
Konstantin Hebenstreit, Robert Praas, Louis P Kiesewet-
ter, and Matthias Samwald. 2023. An automati-
cally discovered chain-of-thought prompt general-
izes to novel models and datasets. arXiv preprint
arXiv:2305.02897 .
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren
Etzioni, and Nate Kushman. 2014. Learning to solve
arithmetic word problems with verb categorization.
InEMNLP , pages 523–533.
Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra
Molina, Aaron Donsbach, Michael Terry, and Carrie J
Cai. 2022. Promptmaker: Prompt-based prototyping
with large language models. In CHI Conference on
Human Factors in Computing Systems Extended Ab-
stracts , pages 1–8.
Zhanming Jie and Wei Lu. 2023. Leveraging training
data in few-shot prompting for numerical reasoning.
InFindings of the Association for Computational
Linguistics: ACL 2023 .
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. arXiv preprint
arXiv:2205.11916 .

--- PAGE 10 ---
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
Sabharwal, Oren Etzioni, and Siena Dumas Ang.
2015. Parsing algebraic word problems into equa-
tions. Transactions of the Association for Computa-
tional Linguistics , 3:585–597.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. arXiv preprint arXiv:1705.04146 .
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,
and Pontus Stenetorp. 2021. Fantastically ordered
prompts and where to find them: Overcoming
few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786 .
OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are nlp models really able to solve
simple math word problems? arXiv preprint
arXiv:2103.07191 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Laria Reynolds and Kyle McDonell. 2021. Prompt pro-
gramming for large language models: Beyond the
few-shot paradigm. In Extended Abstracts of the
2021 CHI Conference on Human Factors in Comput-
ing Systems , pages 1–7.
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reason-
ing about quantities in natural language. Transac-
tions of the Association for Computational Linguis-
tics, 3:1–13.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
2021. Learning to retrieve prompts for in-context
learning. arXiv preprint arXiv:2112.08633 .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Timo Schick and Hinrich Schütze. 2020. It’s not just
size that matters: Small language models are also
few-shot learners. arXiv preprint arXiv:2009.07118 .Zhihong Shao, Yeyun Gong, Yelong Shen, Min-
lie Huang, Nan Duan, and Weizhu Chen. 2023.
Synthetic prompting: Generating chain-of-thought
demonstrations for large language models. arXiv
preprint arXiv:2302.00618 .
Grigori Sidorov, Alexander Gelbukh, Helena Gómez-
Adorno, and David Pinto. 2014. Soft similarity and
soft cosine measure: Similarity of features in vector
space model. Computación y Sistemas , 18(3):491–
504.
Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,
Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,
Luke Zettlemoyer, Noah A Smith, et al. 2022. Selec-
tive annotation makes language models better few-
shot learners. arXiv preprint arXiv:2209.01975 .
Tian-Xiang Sun, Xiang-Yang Liu, Xi-Peng Qiu, and
Xuan-Jing Huang. 2022. Paradigm shift in natural
language processing. Machine Intelligence Research ,
19(3):169–183.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4149–4158, Minneapolis, Minnesota. Association for
Computational Linguistics.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Language models for dialog applica-
tions. arXiv preprint arXiv:2201.08239 .
Michael E Wall, Andreas Rechtsteiner, and Luis M
Rocha. 2003. Singular value decomposition and prin-
cipal component analysis. A practical approach to
microarray data analysis , pages 91–109.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
and Furu Wei. 2022a. Text embeddings by weakly-
supervised contrastive pre-training. arXiv preprint
arXiv:2212.03533 .
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. Advances in Neural In-
formation Processing Systems , 33:5776–5788.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022b. Self-consistency
improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .

--- PAGE 11 ---
Chenfei Wu, Shengming Yin, Weizhen Qi, Xi-
aodong Wang, Zecheng Tang, and Nan Duan.
2023. Visual chatgpt: Talking, drawing and edit-
ing with visual foundation models. arXiv preprint
arXiv:2303.04671 .
Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen,
Haonan Lu, Dong Yang, Fobo Shi, and Xiaodong
Lin. 2023. Edit everything: A text-guided gen-
erative system for images editing. arXiv preprint
arXiv:2304.14006 .
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2022. Automatic chain of thought prompt-
ing in large language models. arXiv preprint
arXiv:2210.03493 .
Fang Zhao. 2022. Auto-correction dans un analyseur
neuronal par transitions : un comportement factice
? (self-correction in a transition-based neural parser
: a spurious behaviour ?). In Actes de la 29e Con-
férence sur le Traitement Automatique des Langues
Naturelles. Volume 2 : 24e Rencontres Etudiants
Chercheurs en Informatique pour le TAL (RECITAL) ,
pages 20–32, Avignon, France. ATALA.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In In-
ternational Conference on Machine Learning , pages
12697–12706. PMLR.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,
Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy
Ba. 2022. Large language models are human-level
prompt engineers. arXiv preprint arXiv:2211.01910 .

--- PAGE 12 ---
Appendix
The following is the supplementary Appendix for
the paper. All the references are made in context of
the main paper.
A Derivation of the matrix Q
The matrices QQT∈Rm×mandQTQ∈Rn×n
are diagonalized, which can be rewritten as:
(
QQT=UΛ1UT
QTQ=VΛ2VT(A.1)
B Algorithm of Prompt Space
Algorithm 1 describes the detailed algorithm of the
proposed Prompt Space.
C Details of Experimental Setup
C.1 Datasets
Table C1 summarizes the basic statistics of ten
benchmark datasets.
C.2 Demonstrations of Baselines
Figure C1 shows the illustrations of five baselines,
including Zero-shot, Few-shot, Manual-CoT, Zero-
shot-CoT, and Auto-CoT. It is clear to see the dif-
ference between five baselines in Fig. C1.
C.3 Models
Table C2 shows the configurations of different em-
bedding models.
D Complementary Experimental Results
D.1 Effect of Basis Questions in
Prompt-Space-CoT
We additionally evaluated Prompt Space w/o CoT-
Zero and Prompt-Space-CoT on ten tasks as the
number of basis questions is increased from 0 to
10. The result shows that an appropriate number
of basis questions is 8 for half of the arithmetic
reasoning tasks (MultiArith, GSM8K, AddSub),
while it varies in other arithmetic tasks. For com-
monsense reasoning tasks, the optimal numbers of
basis questions are 6 and 9 on CSQA and STQA,
respectively. Importantly, we only use one number
of basis questions to achieve best performance on
the Letter dataset, and that of basis questions is
also small (3) on the Coin dataset. These findingsAlgorithm 1 The detailed algorithm of the pro-
posed Prompt Space.
Input: Pre-trained text embedding models E, a
question set Q={qi}m
i=1, and a large lan-
guage model LLM
Patameter: A conditional variable CoT-Zero for
determining two cases: Prompt-Space-CoT-
Zero and Prompt-Space-CoT (default CoT-
Zero=True ), and the initial number of basis
questions is k
Output: Answers {ai}m
i=1fromLLM
1:Embed each question qiwithEto yield vectors
qi. Then combine all question embeddings as
a matrix Q= [q1,q2, ...,qm]T∈Rm×n.
2:Factorize the matrix Qwith SVD: Q=UΛVT,
where Q∈Rm×n.
3:Findkprincipal components of Q, i.e., Qk=
UkQ,where Uk= [u1,u2, ...uk]T∈Rk×m,
andQk∈Rk×n.
4:Calculate the similarity between basis vec-
tors and question embeddings, and then ob-
tain the indices of the most similar problems,
i.e.,argmax (Qk, QT) = [I1, I2, ..., I k]T, for
1<=Ij<=m.
5:Construct a demonstration with the following
format:
qI1, A: Let’s think step by step. ...
qI2, A: Let’s think step by step. ...
...
qIk, A: Let’s think step by step. ...
6:foreach question i= 1, ..., m do
7: ifCoT-Zero then
8: Combine the demonstration from STEP 5
with the current test question qi:
... (demonstration in STEP 5)
qi
A: Let’s think step by step.
9: else
10: Combine the demonstration from STEP 5
with the current test question qi:
... (demonstration in STEP 5)
qi
A:
11: end if
12: Generate the output answer aiwith the
prompt obtained from STEP 7 in the lan-
guage model LLM
13:end for

--- PAGE 13 ---
Table C1: Statistics of ten benchmark datasets.
Dataset Answer format # of Samples Average words # of basis questions License
AddSub Number 395 31.5 8 Unspecified
MultiArith Number 600 31.8 8 Unspecified
SingleEq Number 508 27.4 8 No license
AQUA-RAT Multiple choices 254 51.9 4 Apache-2.0
SV AMP Number 1000 31.8 8 MIT license
GSM8K Number 1319 46.9 8 MIT license
CommonsenseQA (CSQA) Multiple choices 1221 27.8 7 Unspecified
StrategyQA (STQA) Yes or no 2290 9.6 6 Apache-2.0
Last Letter (Letter) Free format 500 15.0 4 -
Coin Flip (Coin) Yes or no 500 37.0 8 -
Table C2: Configurations of different embedding models.
ModelConfigurations
# of Layers Hidden size # of Parameters
MiniLM-L6-v2 6 384 22M
E5-small 12 384 33M
E5-base 12 768 110M
E5-large 24 1024 330M
Sentence-t5-base 12 768 110M
Sentence-t5-large 24 768 336M
Sentence-t5-xl 24 768 1242M
Sentence-t5-xxl 24 768 4866M
indicate that our Prompt-Space-CoT needs a few
basis questions to get the best performance on sym-
bolic reasoning tasks, which dramatically reduces
the cost of exemplar constructions in LLMs. For
a fair comparison, we don’t show the best results
with optimal basis questions, while showing the
results with the same number of exemplars in Ta-
ble 2. Overall, these results further demonstrate
that the existence of Prompt Space is significant
for improving the reasoning abilities of LLMs, and
reducing the cost of exemplar constructions.
D.2 Visualization of Prompt Space
Figs. D2 visualizes Prompt Space via PCA projec-
tion on ten different datasets, namely AddSub, Mul-
tiArith, SingleEq, SV AMP, AQUA-RAT, GSM8K,
CSQA, STQA, Letter, and Coin. The visualiza-
tion demonstrates that there exists a real space in-
cluding basis vectors (questions) on each dataset.
The space on each dataset may be represented by
different basis questions. From Figs. D2, basis
questions are located on three base planes in the
three-dimensional space.D.3 Comparisons among Complex CoT,
Dynamic Program Prompting, and
Prompt Space.
Table D5 shows the comparisons between baselines
and Prompt Space on MultiArith, SV AMP, and
GSM8K datasets.
E Demonstrations of Each Datasets
This section shows the specific demonstrations of
each datasets.

--- PAGE 14 ---
Q: Bianca and her mom were picking carrots from their garden. Bianca picked 26 and her mother picked 15. 
If only 16 of the carrots were good, how many bad carrots did they have?
A: Let's think step by step , Bianca and her mom picked a total of 26 + 15 = 41 carrots. If only 16 of them 
were good, that means they had 41 -16 = 25 bad carrots. The answer is 25.
Q: For Halloween Megan received 11 pieces of candy from neighbors and 5 pieces from her older sister. If she 
only ate 8 pieces a day, how long would the candy last her?
A:
----------------------------------------------------------------------------------------------------------------------------- --------
(Output) Megan received a total of 16 pieces of candy. If she eats 8 pieces a day, the candy will last her for 2 
days. To find out, we divide the total number of pieces by the number of pieces she eats per day: 16 / 8 = 3. 
The answer is 3 days. 
Q: In fourth grade there were 4 students at the start of the year. During the year 3 students left and 42 new 
students came to school. How many students were in fourth grade at the end?
A:At the start of the year, there were 4 students. Then, 3 students left. So there were 4 -3 = 1 student left. 
After that, 42 new students came to school. So the total number of students in fourth grade at the end of the 
year is 1 + 42 = 43. The answer is 43.
Q: For Halloween Megan received 11 pieces of candy from neighbors and 5 pieces from her older sister. If she 
only ate 8 pieces a day, how long would the candy last her?
A:
----------------------------------------------------------------------------------------------------------------------------- --------
(Output) Megan received 11 pieces of candy from neighbors and 5 pieces from her sister. That means she has a 
total of 16 pieces of candy. If she eats 8 pieces a day, the candy will last her for 2 days . The answer is 2. Q: Bianca and her mom were picking carrots from their garden. Bianca picked 26 and her mother picked 15. If 
only 16 of the carrots were good, how many bad carrots did they have?
A:The answer is 25
Q: For Halloween Megan received 11 pieces of candy from neighbors and 5 pieces from her older sister. If she 
only ate 8 pieces a day, how long would the candy last her?
A:
----------------------------------------------------------------------------------------------------------------------------- --------
(Output) The answer is 3. 
Q: For Halloween Megan received 11 pieces of candy from neighbors and 5 pieces from her older sister. If she 
only ate 8 pieces a day, how long would the candy last her?
A: The answer is
----------------------------------------------------------------------------------------------------------------------------- ---------
(Output) 6.
Q: Bianca and her mom were picking carrots from their garden. Bianca picked 26 and her mother picked 15. 
If only 16 of the carrots were good, how many bad carrots did they have?
A: Bianca picked 26 carrots and her mother picked 15, so they picked a total of 26 + 15 = 41 carrots. If only 
16 of the carrots were good, that means they had 41 -16 = 25 bad carrots. The answer is 25.
Q: For Halloween Megan received 11 pieces of candy from neighbors and 5 pieces from her older sister. If she 
only ate 8 pieces a day, how long would the candy last her?
A:
----------------------------------------------------------------------------------------------------------------------------- --------
(Output) Megan received 11 pieces of candy from neighbors and 5 pieces from her sister. That means she has 
a total of 16 pieces of candy. If she eats 8 pieces a day, the candy will last her for 2 days . The answer is 3. 
Q: For Halloween Megan received 11 pieces of candy from neighbors and 5 pieces from her older sister. If she 
only ate 8 pieces a day, how long would the candy last her?
A: Let’s think step by step.
----------------------------------------------------------------------------------------------------------------------------- ---------
(Output) Megan received a total of 16 pieces of candy. If she eats 8 pieces a day, the candy will last her for 2 
days. To find out, we divide the total number of pieces by the number of pieces she eats per day: 16 / 8 = 3. 
The answer is 3 days.
Hand -made question -rationale pair 
Using clustering to select questions
Generate rationale by LLM
Select more appropriate question with 
Our method
Generate rationale by LLM
(f) Prompt Space(e) Auto -CoT(d) Zero -shot CoT(c) Few -shot CoT(b) Few -shot(a) Zero -shotFigure C1: Demonstrations of five baselines, including Zero-shot, Few-shot, Few-shot-CoT (Manual-CoT), Zero-
shot-CoT, and Auto-CoT.

--- PAGE 15 ---
Table D3: Accuracy (%) comparison of Prompt Space w/o CoT-Zero (gpt-turbo-0125) with different amounts of
basis questions on ten reasoning datasets. Ten benchmark datasets contain three categories, including arithmetic
reasoning, commonsense reasoning, and symbolic reasoning.
# of BasisArithmetic Commonsense SymbolicAvg
AddSub MultiArith SingleEq AQUA-RAT SV AMP GSM8K CSQA STQA Letter Coin
gpt-3.5-turbo-0301
1 87.1 69.7 84.1 26.0 70.8 19.0 77.4 46.0 3.2 15.4 49.9
2 87.1 82.5 86.8 29.5 70.4 24.8 77.7 45.1 2.8 57.2 56.4
3 89.1 84.3 88.4 32.3 74.2 23.2 78.2 52.8 4.2 41.8 56.9
4 89.4 83.0 88.8 31.9 74.5 25.7 78.7 55.5 3.4 30.6 56.1
5 89.1 83.8 88.4 30.7 75.4 24.9 78.7 56.2 4.4 51.8 58.3
6 89.6 83.7 88.2 31.9 75.6 25.9 79.0 61.3 4.2 63.8 60.3
7 88.6 84.5 88.6 29.9 75.6 25.4 79.1 60.3 4.0 59.0 59.5
8 88.4 83.7 88.8 30.7 75.5 25.2 80.0 62.0 3.8 61.2 59.8
9 89.9 86.3 88.4 30.3 74.7 25.8 79.9 61.4 5.2 60.2 60.2
10 89.6 85.8 88.2 29.9 75.0 25.5 79.0 62.8 4.2 60.6 60.1
Best results 89.9 86.3 88.8 32.3 75.6 25.9 80.0 62.8 5.2 63.8 61.1
gpt-3.5-turbo-0125
1 81.4 86.7 88.5 53.7 80.8 73.4 72.1 44.4 53.1 95.3 72.9
2 84.4 95.2 92.5 53.3 80.3 76.8 73.7 55.9 68.3 100.0 78.0
3 84.3 94.8 93.4 54.7 79.8 77.4 74.4 58.1 75.3 93.9 78.6
4 83.7 95.1 92.1 55.1 81.4 78.4 71.8 60.9 74.5 100.0 79.3
5 83.5 95.2 93.5 54.7 81.8 78.4 71.5 60.1 76.6 100.0 79.5
6 83.8 94.7 94.2 55.6 82.5 77.5 72.3 60.7 74.9 97.0 79.3
7 83.5 94.0 94.2 54.5 82.5 78.2 72.5 61.8 73.7 97.0 79.2
8 84.1 94.4 93.5 52.0 82.5 77.6 72.6 60.9 76.9 97.8 79.2
9 84.0 95.6 93.2 53.7 83.4 76.9 72.1 62.2 76.4 92.0 78.9
10 84.1 95.1 94.0 50.1 83.6 76.8 71.7 62.9 76.9 78.7 77.4
Best results 84.4 95.6 94.2 55.6 83.6 78.4 74.4 62.9 76.9 100.0 80.6

--- PAGE 16 ---
Table D4: Accuracy (%) comparison of Prompt-Space-CoT with different amounts of basis questions on ten
reasoning datasets. Ten benchmark datasets contain three categories, including arithmetic reasoning, commonsense
reasoning, and symbolic reasoning.
# of BasisArithmetic Commonsense SymbolicAvg
AddSub MultiArith SingleEq AQUA-RAT SV AMP GSM8K CSQA STQA Letter Coin
gpt-3.5-turbo-0301
1 39.7 58.8 41.7 15.0 34.3 24.7 72.2 44.6 84.4 61.8 47.7
2 80.0 94.3 91.9 32.3 35.6 69.3 74.4 59.7 73.0 88.0 69.9
3 85.1 96.2 92.3 36.2 77.1 71.1 74.8 61.7 76.0 100.0 77.1
4 86.6 96.8 92.5 40.6 80.4 75.7 74.0 61.9 74.2 99.6 78.2
5 86.1 96.8 92.5 43.3 81.7 76.1 72.5 61.6 75.6 100.0 78.6
6 86.6 97.3 91.7 39.8 82.5 75.6 75.6 62.5 74.4 97.8 78.4
7 86.1 98.0 91.9 48.8 82.1 76.6 74.1 63.6 74.0 84.2 78.0
8 87.9 98.8 92.1 47.6 81.0 77.9 74.1 62.5 79.6 100.0 80.2
9 87.8 97.5 92.1 45.3 82.1 76.6 74.3 64.4 76.2 99.4 79.6
10 87.6 97.8 91.5 45.3 82.6 76.9 74.4 64.4 77.6 99.0 79.7
Best results 87.9 98.8 92.5 48.8 82.6 77.9 75.6 64.4 84.4 100.0 81.3
gpt-3.5-turbo-0125
1 84.4 90.1 90.7 53.5 81.3 73.7 74.4 36.1 70.3 96.5 75.1
2 85.8 97.0 92.6 56.3 80.5 79.8 76.3 62.5 67.6 99.0 79.7
3 85.7 96.3 92.5 52.2 79.6 79.8 76.9 64.8 75.1 100.0 80.3
4 85.1 95.6 91.7 53.8 80.3 80.7 74.6 66.1 73.3 100.0 80.1
5 86.3 95.6 91.9 57.2 81.4 79.9 74.4 63.3 74.3 99.9 80.4
6 85.7 95.2 92.7 55.9 82.2 79.6 75.6 61.7 73.3 99.9 80.2
7 86.8 95.4 92.5 56.0 80.5 79.5 72.5 67.9 72.1 99.9 80.3
8 87.5 95.2 91.4 54.6 82.2 80.2 72.9 65.6 74.9 99.9 80.4
9 87.7 95.9 92.8 50.7 83.1 79.6 74.2 66.8 75.2 99.7 80.6
10 87.3 96.2 93.1 53.8 82.8 79.1 72.0 68.0 76.1 100.0 80.8
Best results 87.7 97.0 93.1 57.2 83.1 80.7 76.9 68.0 76.1 100.0 82.0
Table D5: Enhanced comparisons between baselines and Prompt Space across MultiArith, Svamp, and GSM8K
datasets.
DatasetsModelMultiArith SV AMP GSM8K
text-davinci-002
Complex CoT (Fu et al., 2023) 94.2 - 55.0
code-davinci-002
Manual-CoT (Wei et al., 2022) 91.7 68.9 46.9
Auto-CoT (Zhao, 2022) 92.0 69.5 47.9
Complex CoT 95.8 - 66.6
Dynamic Program Prompting (Jie and Lu, 2023) - 80.3 76.6
gpt-3.5-turbo-0301
Manual-CoT 97.0 80.2 75.8
Auto-CoT 96.0 78.2 74.1
Complex CoT(Random Sampling) - - 77.1
Complex CoT(Dialog In-Context Learning) - - 74.9
Complex CoT(Greedy Decoding) - - 78.9
Prompt-Space-CoT 98.8 82.6 77.9

--- PAGE 17 ---
(b)
 (c)
 (d)
 (a)Basis vectors Question embeddings
MultiArith (8) SingleEq (8) AQUA -RAT (4) AddSu b(8)
(j)
(e)
 (f)
Coin (8)SV AMP (8) GSM8K (8)
(g)
(h)
 (i)CSQA (7)
STQA (6) Letter (4)Figure D2: Visualizations of basis questions of Prompt Space on ten benchmark datasets.
Q: Take the last letters of each words in "Amy Ella Amilcar Roman" and concatenate them.
A: Let's think step by step. The last letter of "Amy" is "y".The last letter of "Ella" is "a".The last letter of "Amilcar" is 
"r".The last letter of "Roman" is "n".So, we have "y" + "a" + "r" + "n" = "yarn". Therefore, the answer is "yarn".
Q: Take the last letters of each words in "Lara Greg Ethan Terrence" and concatenate them.
A: Let's think step by step. The last letter of "Lara" is "a".The last letter of "Greg" is "g".The last letter of "Ethan" is 
"n".The last letter of "Terrence" is "e". Therefore, the answer is "agne".
Q: Take the last letters of each words in "Luisa Tatiana Drake Hillary" and concatenate them.
A: Let's think step by step. The last letter of "Luisa" is "a".The last letter of "Tatiana" is "a".The last letter of "Drake" 
is "e".The last letter of "Hillary" is "y". Therefore, the answer is "aaye".
Q: Take the last letters of each words in "Gus Brock Ava Jenna" and concatenate them.
A: Let's think step by step. The last letter of "Gus" is "s".The last letter of "Brock" is "k".The last letter of "Ava" is 
"a".The last letter of "Jenna" is "a". Therefore, the answer is "skaa".
Q: Take the last letters of each words in "Tim Candace Cecil Misael" and concatenate them.
A: Let's think step by step. The last letter of "Tim" is "m". The last letter of "Candace" is "e". The last letter of 
"Cecil" is "l". The last letter of "Misael" is "l". So the final answer would be "mel".
Q: Take the last letters of each words in "Alina Alessandra Amina Bianca" and concatenate them.
A: Let's think step by step. The last letter of "Alina" is "a". The last letter of "Alessandra" is "a". The last letter of 
"Amina" is "a". The last letter of "Bianca" is "a". So the final answer is "aaaa".
Q: Take the last letters of each words in "Felipe Heidi Nino Bradley" and concatenate them.
A: Let's think step by step. The last letter of "Felipe" is "e". The last letter of "Heidi" is "i". The last letter of "Nino" 
is "o". The last letter of "Bradley" is "y". So, the final answer would be "eiofy".
Q: Take the last letters of each words in "Lacey Nora Debra Ashleigh" and concatenate them.
A: Let's think step by step. The last letter of "Lacey" is "y". The last letter of "Nora" is "a". The last letter of 
"Debra" is "a". The last letter of "Ashleigh" is "h". So the final answer is "yahah". Q: Take the last letters of each words in "Ever Gio Elia Ramesh" and concatenate them.
A: Let’s think step by step. The last letter of "Ever" is "r". The last letter of "Gio" is "o". The lastletter of "Elia" is 
"a". The last letter of "Ramesh" is "h". So, the final answer is "roah".
Q:  Take the last letters of each words in "Marian Joanne Darrin Rohit" and concatenate them.
A: Let’s think step by step. The last letter of "Marian" is "n". The last letter of "Joanne" is "e". Thelast letter of 
"Darrin" is "n". The last letter of "Rohit" is "t". So the final answer would be "nent".
Q: Take the last letters of each words in "Craig Dillon Troy Griselda" and concatenate them.
A: Let’s think step by step. The last letter of the first word is "g". The last letter of the second wordis "y". The 
last letter of the third word is "a". The last letter of the fourth word is "a". Therefore,the final answer is "gyaa".
Q: Take the last letters of each words in "Liliana Quincy Bart Makayla" and concatenate them.
A: Let’s think step by step. The last letter of "Liliana" is "a". The last letter of "Quincy" is "y".The last letter of 
"Bart" is "t". The last letter of "Makayla" is "a". So the final answer is "ayta".Auto-CoT Prompt SpaceRandom SelectionPrompt Demonstrations on Last Letter
Manual-CoT
Q: Take the last letters of the words in "Elon Musk" and concatenate them.
A: The last letter of “Elon” is “n”. The last letter of “Musk” is “k”. Concatenating them is “nk”. The answer is nk.
Q: Take the last letters of the words in "Larry Page" and concatenate them.
A: The last letter of "Larry" is "y". The last letter of "Page" is "e". Concatenating them is "ye". The answer is ye.
Q: Take the last letters of the words in "Sergey Brin" and concatenate them.
A:  The last letter of “Sergey” is “y”. The last letter of “Brin” is “n”. Concatenating them is “yn”.  The answer isyn.
Q: Take the last letters of the words in "Bill Gates" and concatenate them.
A: The last letter of "Bill" is "l". The last letter of "Gates" is "s". Concatenating them is "ls". The answer is ls.
Figure E3: The demonstrations of Last Letter on difference methods, including Random selection, Manual-CoT,
Auto-CoT, and Our Prompt Space.

--- PAGE 18 ---
Table E6: Basis questions of the AddSub dataset
Q:As part of a lesson on earthquakes , a science class is researching the movement of a
nearby fault line . The fault line moved 1.25 inches during the past year and 5.25 inches
the year before . How far did the fault line move in all ?
A:Let’s think step by step. Thefault linemoved 1.25 inches duringthepast year and
5.25 inches theyear before. That means itmoved atotalof6.5inches intwoyears.
Q:There are 5 oak trees currently in the park . Park workers will plant 4 oak trees today .
How many oak trees will the park have when the workers are finished ?
A:Let’s think step by step. There are5oaktrees currently inthepark. Park work erswill
plant 4oaktrees today. This means thatwhen thework ersarefinished, there willbea
totalof9oaktrees inthepark.
Q:There is 0.16666666666666666 cup of oil in Scarlett ’s measuring cup . If Scarlett
adds 0.6666666666666666 cup more , how much oil will be in the measuring cup ?
A:Let’s think step by step. There iscurrently 0.16666666666666666 cups ofoilin
themeasuringcup. IfScarlettadds 0.6666666666666666 more cups, thatmeans she
isadding (0.6666666666666666 +0.16666666666666666) cups ofoil,which equals
0.8333333333333334 cups ofoil.
Q:Fred picked 36 limes , Alyssa picked 32 limes , and Nancy picked 35 limes and 18
pears , at the farm . How many limes were picked in total ?
A:Let’s think step by step. First, weneed tofindouthow many limes were picked in
total.Wecandothisbyadding upthenumberoflimes thateach personpicked. 36+32+
35=103So,103limes were picked intotal.
Q:As part of a lesson on earthquakes , a science class is researching the movement of a
nearby fault line . The fault line moved 1.25 inches during the past year and 5.25 inches
the year before . How far did the fault line move in all ?
A:Let’s think step by step. Thefault linemoved 1.25 inches duringthepast year and
5.25 inches theyear before. That means itmoved atotalof6.5inches intwoyears.
Q:There are 41 short trees and 44 tall trees currently in the park . Park workers will
plant 57 short trees today . How many short trees will the park have when the workers
are finished ?
A:Let’s think step by step. There are41short trees and44talltrees currently inthe
park. Park work erswillplant 57short trees today.This means thatwhen thework ersare
finished, there willbe41+57=98short trees inthepark.
Q:Alyssa picked 17 plums and Jason picked 10 plums . Melanie picked 35 pears . How
many plums were picked in all ?
A:Let’s think step by step. Alyssa picked 17plums. Jasonpicked 10plums. That means
that, intotal,27plums were picked.
Q:Greg and Sharon own neighboring cornfields . Greg harvested 0.4 acre of corn on
Monday and Sharon harvested 0.1 acre . How many more acres did Greg harvest than
Sharon ?
A:Let’s think step by step. Greg harvested 0.4acre ofcorn onMon day. That means
Sharon harvested 0.1acre ofcorn onMonday.That means Greg harvested 0.3more acres
ofcorn than Sharon.

--- PAGE 19 ---
Table E7: Basis questions of the MultiArith dataset
Q:A group of 3 friends went into a restaurant. The chef already had 6 chicken wings
cooked but cooked 3 more for the group. If they each got the same amount how many
would each person get?
A:Let’s think step by step. There areatotalof9chicken wings. Each personwould get3
chicken wings.
Q:In a video game, each enemy defeated gives you 9 points. If a level has 11 enemies
total and you destroy all but 3 of them, how many points would you earn?
A:Let’s think step by step. Each enemy defeated gives you9points. Ifalevel has11
enemies totalandyoudestroy allbut3ofthem, youwould have destroyed 8enemies. 8
times 9equals 72points.
Q:Wendy uploaded 45 pictures to Facebook. She put 27 pics into one album and put the
rest into 9 different albums. How many pictures were in each album?
A:Let’s think step by step. First, weknow thatWendy uploaded 45pictures intotal.
Second, weknow thatWendy put27pictures intoonealbum. That means thatWendy
puttheremain ing18pictures into9differentalbums. That means thateach album would
have 2pictures.
Q:Wendy was playing a video game where she scores 5 points for each treasure she finds.
If she found 4 treasures on the first level and 3 on the second, what would her score be?
A:Let’s think step by step. Onthefirstlevel, Wendy scores 5points foreach treasure, so
shescores 20points intotal.Onthesecondlevel, Wendy scores 5points foreach treasure,
soshescores 15points intotal.TofindWendy’s totalscore, weneed toaddthepoints
shescored oneach level together. Sowetake 20+15andweget35.
Q:Faye was playing a video game where she scores 7 points for each treasure she finds.
If she found 2 treasures on the first level and 6 on the second, what would her score be?
A:Let’s think step by step. Onthefirstlevel, Faye found 2treasures. Soherscore forthe
firstlevel is7*2=14. Onthesecondlevel, Faye found 6treasures. Soherscore forthe
secondlevel is7*6=42. Tofindhertotalscore, wejustneed toaddupherscores foreach
level. Sohertotalscore is14+42=56.
Q:A waiter had 9 tables he was waiting on, with 4 women and 3 men at each table. How
many customers total did the waiter have?
A:Let’s think step by step. There are9tables. Each tablehas4women and3men. That
means there are36women and27men intotal.There fore, thewaiter had63customers in
total.
Q:While shopping for music online, Nancy bought 3 country albums and 5 pop albums.
Each album came with a lyric sheet and had 8 songs. How many songs did Nancy buy
total?
A:Let’s think step by step. Nancy bought 3coun tryalbums. Each album has8songs. So
shebought 24songs from thecoun tryalbums. Nancy bought 5popalbums. Each album
has8songs. Soshebought 40songs from thepopalbums. Nancy bought atotalof24+
40=64songs.
Q:In a video game, each enemy defeated gives you 9 points. If a level has 11 enemies
total and you destroy all but 3 of them, how many points would you earn?
A:Let’s think step by step. Each enemy defeated gives you9points. Ifalevel has11
enemies totalandyoudestroy allbut3ofthem, youwould have destroyed 8enemies. 8
times 9equals 72points.

--- PAGE 20 ---
Table E8: Basis questions of the SingleEq dataset
Q:Mary, Sam, Keith, and Alyssa each have 6 baseball cards. How many baseball cards
do they have in all ?
A:Let’s think step by step. There arefour people,soweneed toaddupalloftheir
baseballcards. Mary has6baseballcards, Sam has6baseballcards, Keith has6baseball
cards, andAlyssa has6baseballcards. 6+6+6+6=24So,they have 24baseballcards
inall.
Q:As part of a lesson on earthquakes, a science class is researching the movement of a
nearby fault line. The fault line moved 1.25 inches during the past year and 5.25 inches
the year before. How far did the fault line move in all?
A:Let’s think step by step. Thefault linemoved 1.25 inches duringthepast year. The
fault linemoved 5.25 inches theyear before. Tofindthetotalmove ment ofthefault line,
weneed toadd1.25 +5.25. Thefault linemoved 6.5inches inall.
Q:It rained 0.2 inches on Saturday and 0.4 inches on Sunday. How much did it rain on
Saturday and Sunday combined?
A:Let’s think step by step. Thefirstthing weneed todoisaddthetwonumbers together.
This will give usthetotalamount ofrain thatfellover thetwodays. 0.2+0.4=0.6
There fore, itrained atotalof0.6inches onSaturdayandSundaycombined.
Q:Sandy has 8 dozen books. Fred has 5 times more books than Sandy. How many books
does Fred have ?
A:Let’s think step by step. Sandy has8dozen books. This is96books. Fred has5times
more books than Sandy. This means thatFred has5times 96books, or480books.
Q:As part of a lesson on earthquakes, a science class is researching the movement of a
nearby fault line. The fault line moved 1.25 inches during the past year and 5.25 inches
the year before. How far did the fault line move in all?
A:Let’s think step by step. Thefault linemoved 1.25 inches duringthepast year. The
fault linemoved 5.25 inches theyear before. Tofindthetotalmove ment ofthefault line,
weneed toadd1.25 +5.25. Thefault linemoved 6.5inches inall.
Q:A DVD book holds 126 DVDs. There are 81 DVDs already in the book. How many
more DVDs can be put in the book?
A:Let’s think step by step. There arealready 81DVDs inthebook. That means there are
45DVDs thatcanstillfitinthebook.
Q:Sally bought 4 dozen eggs from the grocery store to bake some cakes. How many
eggs did Sally buy ?
A:Let’s think step by step. 1dozen =12eggs 4dozen =4x12=48eggs SoSally bought
48eggs from thegrocery store.
Q:Bert runs 2 miles every day. How many miles will Bert run in 3 weeks?
A:Let’s think step by step. Inoneweek, Bert willrun14miles. Intwoweeks, Bert will
run28miles. Inthree weeks, Bert willrun42miles.

--- PAGE 21 ---
Table E9: Basis questions of the AQUA-RAT dataset
Q:30 is subtracted from a number, it is reduced to its one third. What is the value of 50
A:Let’s think step by step. First, weknow thatif30issubtracted from anumber,itis
reduced toonethird ofitsoriginalvalue. Next, wewant tofind50% ofthatnumber.To
dothis, wecantake onethird ofthenumberandmultiplyitby2.
Q:If Jill needed to buy 10 bottles of soda for a party in which 8 people attended, how
many bottles of soda will she need to buy for a party in which 12 people are attending?
Answer Choices: (A) 6 (B) 8 (C) 10 (D) 12 (E) 14
A:Let’s think step by step. For8people,Jillneeds 10bottlesofsoda. For12people,Jill
needs ?bottlesofsoda. Since Jillneeds 2.5bottlesofsoda perperson,shewillneed 2.5*
12=30bottlesofsoda for12people.Theanswer is(E).
Q:A computer routine was developed to generate two numbers (x,y) the first being a
random number between 0 and 100 inclusive, and the second being less than or equal to
the square root of the first. Each of the following pair satisfies the routine except Answer
Choices: (A) (99,10) (B) (85,9) (C) (50,7) (D) (1,1) (E) (1,0)
A:Let’s think step by step. Thefirstnumber,x,isarandom numberbetween 0and100
inclusive. Thesecondnumber,y,islessthan orequal tothesquare rootofthefirst. That
means thatycannever begreater than 10.There fore, (99,10) does notsatisfytheroutine.
Q:If a man rows at the rate of 4 kmph in still water and his rate against the current is
2 kmph, then the man’s rate along the current is: Answer Choices: (A) 15 kmph (B) 6
kmph (C) 12 kmph (D) 14 kmph (E) 6 kmph
A:Let’s think step by step. Theman’s rateinstillwateris4kmph. Theman’s rateagainst
thecurrentis2kmph. This means thattheman’s ratealong thecurrentmust befaster
than hisrateinstillwater,butslower than hisrateagainst thecurrent. Theanswer must
bebetween 4kmph and2kmph. Theanswer must be6kmph.

--- PAGE 22 ---
Table E10: Basis questions of the SV AMP dataset
Q:During summer break 61619 kids from Lawrence county stayed home and the rest
went to camp. The total number of kids in Lawrence county is 91676. About how many
kids in Lawrence county went to camp?
A:Let’s think step by step. There are91676 kids inLawrence county. 61619 kids stayed
home. That means thattherest, 91676 -61619, went tocamp. Soabout 30057 kids in
Lawrence county went tocamp.
Q:During summer break 61619 kids from Lawrence county stayed home and the rest
went to camp. The total number of kids in Lawrence county is 91676. About how many
kids in Lawrence county went to camp?
A:Let’s think step by step. There are91676 kids inLawrence county. 61619 kids stayed
home. That means thattherest, 91676 -61619, went tocamp. Soabout 30057 kids in
Lawrence county went tocamp.
Q:Lewis earns $ 21 every week during the 216 weeks of harvest. If he has to pay $ 702
tax How much money will have at the end of the harvest season?
A:Let’s think step by step. First, let’s calculatehow much money heearns intotal.He
earns $21perweek, andthere are216weeks intheharvestseason.Soheearns $21*216
=$4536 intotal.Then, weneed tocalculatehow much taxheneeds topay. Heneeds to
pay$702 intax.Sothefinalanswer is$4536 -$702 =$3834.
Q:The grasshopper, the frog and the mouse had a jumping contest. The grasshopper
jumped 25 inches. The frog jumped 18 inches farther than the grasshopper and the mouse
jumped 2 inches farther than the frog. How far did the mouse jump?
A:Let’s think step by step. The grasshop perjumped 25inches. The frog jumped 18
inches fartherthan thegrasshop per.This means thatthefrog jumped atotalof(25+18)
=43inches. Themouse jumped 2inches fartherthan thefrog. This means thatthemouse
jumped atotalof(43+2)=45inches. There fore, themouse jumped 45inches.
Q:Winter is almost here and most animals are migrating to warmer countries. There
are 3 bird families living near the mountain. 26 new bird families came to live near the
mountain from the arctic while 2 bird families flew away further south for winter. How
many bird families were left near the mountain?
A:Let’s think step by step. There are3bird familieslivingnear themoun tain. 26new
bird familiescame tolivenear themoun tainfrom thearctic.2bird familiesflew away
furthersouth forwinter.3+26-2=27birdfamilieswere leftnear themoun tain.
Q:Danny has 12 bottle caps in his collection. He found 53 bottle caps at the park. How
many bottle caps does he have now?
A:Let’s think step by step. Danny has12bottlecaps inhiscollection. Hefound 53bottle
caps atthepark. How many bottlecaps does hehave now? Hewould have 12+53=65
bottlecaps now.
Q:The grasshopper and the frog had a jumping contest. The grasshopper jumped 13
inches. The grasshopper jumped 2 inches farther than the grasshopper. How far did the
frog jump?
A:Let’s think step by step. Thegrasshop perjumped 13inches. Thegrasshop perjumped
2inches fartherthan thegrasshop per.Sothefrog jumped 15inches.
Q:Lewis earns $ 21 every week during the 216 weeks of harvest. If he has to pay $ 702
tax How much money will have at the end of the harvest season?
A:Let’s think step by step. First, let’s calculatehow much money heearns intotal.He
earns $21perweek, andthere are216weeks intheharvestseason.Soheearns $21*216
=$4536 intotal.Then, weneed tocalculatehow much taxheneeds topay. Heneeds to
pay$702 intax.Sothefinalanswer is$4536 -$702 =$3834.

--- PAGE 23 ---
Table E11: Basis questions of the GSM8K dataset
Q:A marketing company pays its employees on a commission-based salary system. If
you sell goods worth $1000, you earn a 30% commission. Sales over $1000 get you an
additional 10% commission. Calculate the amount of money Antonella earned if she sold
goods worth $2500.
A:Let’s think step by step. Antonella earned a30% commission onthefirst$1000, so
sheearned $300. Ontheremain ing$1500, sheearned 10% commission. Sosheearned
anadditional $150. Intotal,sheearned $450.
Q:John buys 2 pairs of shoes for each of his 3 children. They cost $60 each. How much
did he pay?
A:Let’s think step by step. John buys 2pairs ofshoes foreach ofhis3children. That
means hebuys 6pairs ofshoes intotal.Each pairofshoes costs $60. That means thetotal
costis6x$60=$360.
Q:A marketing company pays its employees on a commission-based salary system. If
you sell goods worth $1000, you earn a 30% commission. Sales over $1000 get you an
additional 10% commission. Calculate the amount of money Antonella earned if she sold
goods worth $2500.
A:Let’s think step by step. Antonella earned a30% commission onthefirst$1000, so
sheearned $300. Ontheremain ing$1500, sheearned 10% commission. Sosheearned
anadditional $150. Intotal,sheearned $450.
Q:Daisy bought a bag of potatoes that weighed 5 pounds. She also bought a bag of sweet
potatoes that weighed 2 times as much as the potatoes and a bag of carrots that weighed 3
pounds fewer than the sweet potatoes. How many pounds of carrots did Daisy buy?
A:Let’s think step by step. Thebagofpotatoes weighed 5pounds. Thebagofsweet
potatoes weighed 2times asmuch asthepotatoes. Sothesweet potatoes weighed 10
pounds. The bagofcarrotsweighed 3pounds fewer than thesweet potatoes. Sothe
carrotsweighed 7pounds.
Q:A marketing company pays its employees on a commission-based salary system. If
you sell goods worth $1000, you earn a 30% commission. Sales over $1000 get you an
additional 10% commission. Calculate the amount of money Antonella earned if she sold
goods worth $2500.
A:Let’s think step by step. Antonella earned a30% commission onthefirst$1000, so
sheearned $300. Ontheremain ing$1500, sheearned 10% commission. Sosheearned
anadditional $150. Intotal,sheearned $450.
Q:After scoring 14 points, Erin now has three times more points than Sara, who scored
8. How many points did Erin have before?
A:Let’s think step by step. Erin now has3times more points than Sara, who scored 8.
That means thatifwetake away 8points from Erin, shewill stillhave 3times more
points than Sara. That means thatErin had24points before.
Q:Dijana and Anis live near a lake, and every weekend they go out rowing into the lake.
On a Sunday morning, both went out rowing, and Dijana rowed for 50 miles the whole
day. Anis rowed 1/5 times more miles than Dijana. Calculate the total distance the two of
them rowed on that day.
A:Let’s think step by step. Dijana rowed 50miles, soAnis rowed 1/5more than that,
which is10miles. Sothetotaldistance they rowed is60miles.
Q:Kyle bought last year’s best-selling book for $19.50. This is with a 25% discount
from the original price. What was the original price of the book?
A:Let’s think step by step. Theoriginalprice ofthebook was$19.50 with a25% discount.
This means thattheoriginalprice was$26. Theoriginalprice was$26.

--- PAGE 24 ---
Table E12: Basis questions of the CSQA dataset
Q:Where can someone get a new saw? Answer Choices: (A) hardware store (B) toolbox
(C) logging camp (D) tool kit (E) auger
A:Let’s think step by step. Where would yougotobuyanew saw? Ahardware store
would beagood place tostart.
Q:The man was going fishing instead of work, what is he seeking? Answer Choices: (A)
food (B) relaxation (C) killing (D) missing morning cartoons (E) boredom
A:Let’s think step by step. Theman isgoingfishinginstead ofwork. This means thatthe
man would rather gofishingthan work. Theman isseekingrelaxation.
Q:What do animals do when an enemy is approaching? Answer Choices: (A) feel
pleasure (B) procreate (C) pass water (D) listen to each other (E) sing
A:Let’s think step by step. When ananimalnotices anenemy approach ing, itsfirst
instinct isusuallytoflee. Iffleeingisnotanoption, theanimalmay trytohide. Ifhiding
isnotanoption, theanimalmay trytodefend itself.
Q:I want to buy a gong, where should I look for one? Answer Choices: (A) orchestra
(B) church (C) chinese temple (D) chinatown (E) music store
A:Let’s think step by step. Agong isapercussion instrument, sowecaneliminate (A)
and(B).Itisalsolikely thatagong would beused inaChinese temple,so(C)and(D)
aregood choices. How ever, thebestanswer isprobably(E),since amusicstore would
have awider selectionofgongs tochoose from.
Q:I want to buy a gong, where should I look for one? Answer Choices: (A) orchestra
(B) church (C) chinese temple (D) chinatown (E) music store
A:Let’s think step by step. Agong isapercussion instrument, sowecaneliminate (A)
and(B).Itisalsolikely thatagong would beused inaChinese temple,so(C)and(D)
aregood choices. How ever, thebestanswer isprobably(E),since amusicstore would
have awider selectionofgongs tochoose from.
Q:What is it called when a person with mental illness is able to lead a relatively normal
life? Answer Choices: (A) managed (B) effectively treated (C) recur (D) cause delusion
(E) illusion
A:Let’s think step by step. Thefirstthing todoistoidentifythekeywords inthequestion.
Inthiscase, thekeywords are"men talillness," "relatively normallife," and"able to
lead." Next, weneed tothink about what these keywords mean. "Men talillness" refers to
aconditionthataffects aperson’s thoughts, emotions, andbehavior."Relatively normal
life" means thatthepersonisable tofunctioninsociety,even though they may have some
symp toms oftheir mentalillness. "Able tolead" means thatthepersonisable totake care
ofthem selves andliveindependently.
Q:What is the feeling of one having fun? Answer Choices: (A) smiling (B) pleasure (C)
hurt (D) injuries (E) laughter
A:Let’s think step by step. Thefeelingofonehavingfunisgenerallyassociated with
positiveemotions, such ashappiness, laugh ter,andpleasure. There fore, wecaneliminate
choices (C)and(D). Next, wecanlook attheremain ingchoices andtrytodetermine
which oneisthebestfit.Both choices (A)and(E)could work, but(E)isamore specific
answer, soitisprobablythecorrectanswer.

--- PAGE 25 ---
Table E13: Basis questions of the STQA dataset
Q:Is Bern located east of Paris?
A:Let’s think step by step. First, weneed toknow thecoordinates ofParis andBern.
AccordingtoGoogle Maps, Paris islocated at48.856614, 2.3522219000000177 andBern
islocated at46.947922, 7.4446080000000255. Next, weneed tocompare thelongitude
ofParis (2.3522219000000177) with thelongitude ofBern (7.4446080000000255). Since
thelongitude ofBern isgreater than thelongitude ofParis, Bern islocated eastofParis.
Q:If he were poor, would Christopher Reeve have lived?
A:Let’s think step by step. Ifhewere poor, hewould nothave hadthemoney togetthe
bestmedicalcare possible.Ifhedidnothave thebestmedicalcare possible,hisinjuries
would nothave been treated aswell asthey were. Ifhisinjuries were nottreated aswell
asthey were, hewould nothave lived aslong ashedid.So,inshort, ifhewere poor, he
would nothave lived aslong ashedid.
Q:Would it be unusual to see frost in September in Texas?
A:Let’s think step by step. First, what isthedefinitionoffrost? Frost isdefined asathin
layer oficethatforms onsurfaces when thetemperatureoutside isbelowfreez ing.In
Texas, theaveragetemperatureinSeptem berisaround 80degrees Fahren heit. So,no,it
would notbeunusual toseefrost inSeptem berinTexas.
Q:Can an American black bear swallow a sun bear whole?
A:Let’s think step by step. Theaverageadult Amer icanblack bear weighs between 250
and550pounds. Theaverageadult sunbear weighs between 60and80pounds. Soan
Amer icanblack bear could theoretically swallowasunbear whole.
Q:Is CAS number 8009-03-8 harmful for a rash?
A:Let’s think step by step. CAS number8009 -03-8istheCAS numberforShea Butter.
Shea Butterisanaturalfatthatisextracted from thenutoftheAfrican shea tree. Itis
solid atroom temperaturebutmelts when itcomes intocontactwith theskin. There is
noevidence thatShea Butterisharm fulforarash. Infact, itisoftenused asanatural
remedyforskin conditions such aseczema andpsoriasis.
Q:If Martin Luther did one theses a day would he run out in half a year?
A:Let’s think step by step. There are365days inayear. IfMartinLuther didonetheses
aday, hewould do365thesesinayear. There fore, hewould notrunoutinhalfayear.

--- PAGE 26 ---
Table E14: Basis questions of the Letter dataset
Q:Take the last letters of each words in "Ever Gio Elia Ramesh" and concatenate them.
A:Let’s think step by step. Thelastletterof"Ever" is"r".Thelastletterof"Gio" is"o".
Thelastletterof"Elia" is"a".Thelastletterof"Ramesh" is"h".So,thefinalanswer is
"roah".
Q:Take the last letters of each words in "Marian Joanne Darrin Rohit" and concatenate
them.
A:Let’s think step by step. Thelastletterof"Mar ian"is"n".Thelastletterof"Joanne" is
"e".Thelastletterof"Dar rin"is"n".Thelastletterof"Rohit"is"t".Sothefinalanswer
would be"nent".
Q:Take the last letters of each words in "Craig Dillon Troy Griselda" and concatenate
them.
A:Let’s think step by step. Thelastletterofthefirstword is"g".Thelastletterofthe
secondword is"y".Thelastletterofthethird word is"a".Thelastletterofthefourth
word is"a".There fore, thefinalanswer is"gyaa".
Q:Take the last letters of each words in "Liliana Quincy Bart Makayla" and concatenate
them.
A:Let’s think step by step. Thelastletterof"Liliana" is"a".Thelastletterof"Quincy" is
"y".Thelastletterof"Bart" is"t".Thelastletterof"Makayla" is"a".Sothefinalanswer
is"ayta".

--- PAGE 27 ---
Table E15: Basis questions of the Coin Dataset
Q:A coin is heads up. Miguel Angel flips the coin. Saul does not flip the coin. Brady
does not flip the coin. Darryl flips the coin. Is the coin still heads up? Note that "flip"
here means "reverse".
A:Let’s think step by step. Miguel flips thecoin, soit’snow tails up.Saul doesn’t flipthe
coin, soitremains tails up.Brady doesn’t flipthecoin, soitremains tails up.Darrylflips
thecoin, soit’snow heads upagain.
Q:A coin is heads up. Jacky flips the coin. Socorro flips the coin. Mark flips the coin.
Wanda does not flip the coin. Is the coin still heads up? Note that "flip" here means
"reverse".
A:Let’s think step by step. Jacky flips thecoin, soit’snow tails up.Socorro flips thecoin,
soit’snow heads upagain. Mark flips thecoin, soit’snow tails upagain. Wanda does not
flipthecoin, soitremains tails up.
Q:A coin is heads up. Claudia flips the coin. Cole does not flip the coin. Matthew does
not flip the coin. Juan Pablo does not flip the coin. Is the coin still heads up? Note that
"flip" here means "reverse".
A:Let’s think step by step. Clau diaflips thecoin, soitisnow tails up.Cole does notflip
thecoin, soitremains tails up.Matthew does notflipthecoin, soitremains tails up.Juan
Pablo does notflipthecoin, soitremains tails up.Sotheanswer isno,thecoin isnot
heads up.
Q:A coin is heads up. Teresa flips the coin. Reid does not flip the coin. Karin does not
flip the coin. Gracie does not flip the coin. Is the coin still heads up? Note that "flip" here
means "reverse".
A:Let’s think step by step. IfTeresa flips thecoin, then itisnotheads up.IfReid flips
thecoin, then itisnotheads up.IfKarin flips thecoin, then itisnotheads up.IfGracie
flips thecoin, then itisnotheads up.So,thecoin isnotheads up.
Q:A coin is heads up. Miguel Angel flips the coin. Saul does not flip the coin. Brady
does not flip the coin. Darryl flips the coin. Is the coin still heads up? Note that "flip"
here means "reverse".
A:Let’s think step by step. Miguel flips thecoin, soit’snow tails up.Saul doesn’t flipthe
coin, soitremains tails up.Brady doesn’t flipthecoin, soitremains tails up.Darrylflips
thecoin, soit’snow heads upagain.
Q:A coin is heads up. Aj does not flip the coin. Jd flips the coin. Maddie does not flip
the coin. Francisca does not flip the coin. Is the coin still heads up? Note that "flip" here
means "reverse".
A:Let’s think step by step. Ajdoes notflipthecoin, sothecoin remains heads up.Jdflips
thecoin, sothecoin isnow tails up.Maddiedoes notflipthecoin, sothecoin remains
tails up.Francisca does notflipthecoin, sothecoin remains tails up.So,thefinalanswer
isthatthecoin istails up.
Q:A coin is heads up. Tomas does not flip the coin. Nic flips the coin. Zoila flips the
coin. Calvin does not flip the coin. Is the coin still heads up? Note that "flip" here means
"reverse".
A:Let’s think step by step. Tomas does notflipthecoin, sothecoin remains heads up.
Nicflips thecoin, sothecoin isnow tails up.Zoila flips thecoin, sothecoin isnow heads
upagain. Calvin does notflipthecoin, sothecoin remains heads up.
Q:A coin is heads up. Tere does not flip the coin. Niko flips the coin. Keith does not flip
the coin. Conner does not flip the coin. Is the coin still heads up? Note that "flip" here
means "reverse".
A:Let’s think step by step. Tere does notflipthecoin, sothecoin remains heads up.Niko
flips thecoin, sothecoin isnow tails up.Keith does notflipthecoin, sothecoin remains
tails up.Connerdoes notflipthecoin, sothecoin remains tails up.So,thefinalanswer is
thatthecoin istails up.
