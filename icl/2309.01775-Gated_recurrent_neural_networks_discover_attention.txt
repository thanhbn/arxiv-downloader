# 2309.01775.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2309.01775.pdf
# File size: 2777409 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Gated recurrent neural networks discover attention
Nicolas Zucchet* 1Seijin Kobayashi* 1Yassir Akram* 1Johannes von Oswald1Maxime Larcher1
Angelika Steger†1Jo˜ao Sacramento†1
Abstract
Recent architectural developments have enabled
recurrent neural networks (RNNs) to reach and
even surpass the performance of Transformers on
certain sequence modeling tasks. These modern
RNNs feature a prominent design pattern: lin-
ear recurrent layers interconnected by feedfor-
ward paths with multiplicative gating. Here, we
show how RNNs equipped with these two de-
sign elements can exactly implement (linear) self-
attention. By reverse-engineering a set of trained
RNNs, we find that gradient descent in practice
discovers our construction. In particular, we ex-
amine RNNs trained to solve simple in-context
learning tasks and find that gradient descent in-
stills in our RNNs the same attention-based in-
context learning algorithm. Our findings highlight
the importance of multiplicative interactions in
neural networks and suggest that certain RNNs
might be unexpectedly implementing attention
under the hood.
1. Introduction
Attention-based neural networks, most notably Transform-
ers (Vaswani et al., 2017), have rapidly become the state-
of-the-art deep learning architecture, replacing traditional
models such as multi-layer perceptrons, convolutional neu-
ral networks, and recurrent neural networks (RNNs). This is
particularly true in the realm of sequence modeling, where
once-dominating RNNs such as the long short-term memory
(LSTM; Hochreiter & Schmidhuber, 1997) model and the
related gated recurrent unit (GRU; Cho et al., 2014) have
been mostly replaced by Transformers.
Nevertheless, RNNs remain actively researched for vari-
ous reasons, such as their value as models in neuroscience
(Dayan & Abbott, 2001), or simply out of genuine interest
in their rich properties as a dynamical system and uncon-
ventional computer (Jaeger et al., 2023). Perhaps most
*Equal contribution†Shared senior autorship1Department of
Computer Science, ETH Z ¨urich. Correspondence to: <nzucchet,
seijink, yakram, voswaldj, larcherm, asteger, rjoao@ethz.ch >.importantly for applications, RNNs are able to perform in-
ference for arbitrarily long sequences at a constant memory
cost, unlike models based on conventional softmax-attention
layers (Bahdanau et al., 2015). This ongoing research has
led to a wave of recent developments. On the one hand,
new deep linear RNN architectures (Gu et al., 2022; Orvieto
et al., 2023b) have been shown to significantly outperform
Transformers on challenging long-sequence tasks (e.g., Tay
et al., 2020) and on some language modelling tasks (Gu &
Dao, 2023). On the other hand, many efficient linearized
attention models have been developed, whose forward pass
can be executed in an RNN-like fashion at a constant infer-
ence memory cost (Tsai et al., 2019; Katharopoulos et al.,
2020; Choromanski et al., 2021; Schlag et al., 2021; Fu
et al., 2023; Sun et al., 2023; Yang et al., 2023).
We present a unifying perspective on these two seemingly
unrelated lines of work by providing a set of parameters
under which gated RNNs become equivalent to any lin-
earized self-attention, without requiring infinite number of
neurons or invoking a universality argument. Crucially,
our construction makes use of elementwise multiplications,
which are ostensibly featured in different forms in recent
deep linear RNN models. Turning to LSTMs and GRUs,
which also include these multiplicative gating interactions,
we find somewhat surprisingly that our results extend only
to LSTMs. Moreover, the LSTM construction we provide
requires a very specific configuration, which hints that the
inductive bias towards attention-compatible configurations
might be weaker for this architecture than for deep gated
linear RNNs.
We then demonstrate that linear RNNs with multiplicative
interactions, but not LSTMs and GRUs, can effectively
implement our construction once trained, thus behaving as
attention layers. Moreover, we find that such linear RNNs
trained to solve linear regression tasks acquire an attention-
based in-context learning algorithm. Incidentally, it has
been shown that the very same algorithm is typically used
by linear self-attention layers trained on this problem class
(von Oswald et al., 2023; Mahankali et al., 2023; Ahn et al.,
2023; Zhang et al., 2023). Our results thus challenge the
standard view of RNNs and attention-based models as two
mutually exclusive model classes and suggest that, through
learning, RNNs with multiplicative interactions may end
1arXiv:2309.01775v2  [cs.LG]  7 Feb 2024

--- PAGE 2 ---
Gated recurrent neural networks discover attention
up encoding attention-based algorithms disguised in their
weights.
2. Background
2.1. Linear self-attention
We study causally-masked linear self-attention layers that
process input sequences (xt)twithxt∈Rdas follows:
yt=
X
t′≤t(WVxt′)(WKxt′)⊤
(WQxt) (1)
In the previous equation, WV∈Rd×dis the value matrix,
WK∈Rd×dthe key matrix and WQ∈Rd×dthe query
matrix. We use square matrices throughout the paper for
simplicity, but our findings extend to rectangular ones. As
usually done, we call vt:=WVxt,kt:=WKxtandqt:=
WQxtthe values, keys and queries. The output vector ythas
the same dimension as the input, that is d. Such linear self-
attention layers can be understood as a linearized version
of the softmax attention mechanism (Bahdanau et al., 2015)
in use within Transformers (Vaswani et al., 2017). Yet,
they operate in a very different regime than softmax layers,
which have unbounded memory. Attention layers commonly
combine different attention heads; we focus on a single one
here for simplicity.
In a linear self-attention layer, information about the past is
stored in an effective weight matrix Wff
t:=P
t′vt′k⊤
t′that
will later be used to process the current query qtthrough
yt=Wff
tqt. At every timestep, Wff
tis updated through
the rule Wff
t=Wff
t−1+vtk⊤
t, which is reminiscent of
Hebbian learning (Schmidhuber, 1992; Schlag et al., 2021)
and leads to faster inference time (Katharopoulos et al.,
2020; Choromanski et al., 2021; Shen et al., 2021; Peng
et al., 2021) than softmax self-attention.
2.2. Gated recurrent neural networks
In this paper, we focus our analysis on a simplified class
of gated diagonal linear recurrent neural networks. They
implement bilinear input ginand output gating goutthat
multiplies a linear transformation Win/out
x xtof the input
with a linear gate Win/out
m xt:gin/out(xt) = (Win/out
m xt)⊙
(Win/out
x xt). Here, ⊙is the elementwise product. The class
of gated networks we consider satisfies
ht+1=λ⊙ht+gin(xt), yt=Dgout(ht). (2)
In the previous equation, λis a real vector, xtis the input
to the recurrent layer, htthe hidden state, and Da linear
readout. This simplified class makes connecting to attention
easier while employing similar computational mechanisms
as standard gated RNNs architectures.This class is tightly linked to recent deep linear RNN archi-
tectures and shares most of its computational mechanisms
with them. While linear diagonal recurrence might be seen
as a very strong inductive bias, many of the recent power-
ful deep linear RNN models adopt a similar bias (Gupta
et al., 2022; Smith et al., 2023; Gu & Dao, 2023), and it has
been shown to facilitate gradient-based learning (Orvieto
et al., 2023b; Zucchet et al., 2023b). Those architectures
often use complex-valued hidden states in the recurrence;
we only use its real part here. Some of those works employ a
GLU (Dauphin et al., 2017) after each recurrent layer, with
GLU( x) =σ(Wmxt)⊙Wxxtwithσthe sigmoid function.
The gating mechanism we consider can thus be interpreted
as a linearized GLU. We can recover (2)by stacking two
layers: the GLU in the first layer acts as our input gating,
and the one in the second as output gating. Alternatively,
architectures like Mamba (Gu & Dao, 2023) uses input-
dependent matrices as projection to the hidden state instead
of the input gating. Multiplying such matrices with the in-
put itself thus results in a multiplicative gating. Its output
gating mechanism is slightly different as one of the branch
takes the input of the recurrent layer as input, instead of
the hidden state. We include a more detailed comparison in
Appendix B. In the rest of the paper, we will use the LRU
layer (Orvieto et al., 2023b) as the representative of the deep
linear RNN architectures because of its simplicity.
LSTMs can operate in the regime of Equation 2, but this
requires more adaptation. First, the recurrent processing is
nonlinear and involves more steps than are captured in (2).
Second, gating occurs in different parts of the computation
and depends on additional variables. We compare in more
details this architecture and the one of Equation 2 in Ap-
pendix B, showing that LSTMs can implement (2)when
stacking two layers on top of each other. We additionally
show that GRUs cannot do so.
3. Theoretical construction
As highlighted in the previous section, our class of gated
RNNs and linear self-attention have different ways of stor-
ing past information and using it to modify the feedforward
processing of the current input. The previous state htacts
through a bias term λ⊙htthat is added to the current input
gin(xt)in gated RNNs, whereas the linear self-attention
recurrent state Wff
tmodifies the weights of the feedforward
pathway. We reconcile these two mismatched views of
neural computation in the following by showing that gated
RNNs can implement linear self-attention.
In this section, we demonstrate how a gated recurrent layer
followed by a linear readout as in Equation 2 can implement
any linear self-attention layer through a constructive proof.
In particular, our construction only requires a finite number
of neurons to exactly match the desired function, therefore
2

--- PAGE 3 ---
Gated recurrent neural networks discover attention
1. Input gating
Compute the outer product corresponding to
current key-values and the current query
2. Recurrent neurons
Accumulate key-values over time (          )
and store current query (          ) 
3. Output gating and readout 
matrix (key-values) - vector (query)
multiplication 
 sum matrix
Figure 1. An example of a diagonal linear gated recurrent neural network that implements the same function as a linear self-attention layer
with parameters (WV, WK, WQ)and input dimension d, as described in Section 3. Inputs are processed from top to the bottom. We do
not use biases so we append 1 to the input vector xtto be able to send queries to the recurrent neurons. We use repeat( A, n)to denote
that the matrix Ais repeated ntimes on the row axis and WV,iis the i-th row of the WVmatrix. The bars within the matrices separate the
different kinds of inputs/outputs. Digits in matrices denote column vectors appropriately sized. The readout matrix Dappropriately sums
the elementwise products between key-values and queries computed after the output gating gout. Exact matrix values can be found in
Appendix A.1.
providing a much stronger equivalence result than more
general universality of linear recurrent networks theorems
(Boyd & Chua, 1985; Grigoryeva & Ortega, 2018; Orvieto
et al., 2023a), which hold in the limit of infinitely many
recurrent neurons.
3.1. Key ideas
Our construction comprises three main components: First,
the input gating ginis responsible for generating the elemen-
twise products between the keys and values, as well as the
queries. Then, recurrent units associated with key-values
accumulate their inputs with λ= 1, whereas those receiving
queries as inputs return the current value of the query, hence
λ= 0. Lastly, the output gating goutand the final readout
layer Dare in charge of multiplying the flattened key-value
matrix with the query vector. We illustrate our construction
and provide a set of weights for which the functional equiva-
lence holds in Figure 1. Crucially, the key-values in a linear
self-attention layer are the sum of degree two polynomials
of each previous input. Input gating mechanism and perfect
memory units ( λ= 1) are needed to replicate this behavior
within a gated recurrent layer. Similarly, output gating is
required to multiply key-values with the queries.
3.2. On the number of neurons needed
The construction of Figure 1 requires d2+dhidden neu-
rons to store all the entries of the d×dkey-value matrix
and of the query vector of size d. While this constructionis arguably the most intuitive, it is not optimal in terms
of number of neurons used. Knowing the exact minimal
number of neurons is fundamental for understanding which
solution the network learns. Therefore, we detail how we
can make our construction more compact in the following.
We leverage two insights: First, any combination of key and
query matrices for which (W⊤
KWQ)is fixed leads to the
same function in the linear self-attention layer. We can thus
assume that the key and value matrices are equal, as taking
the key matrix to be equal to WVand changing the query
matrix to be W−⊤
VW⊤
KWQdoes not change the behavior of
the attention layer. Second, when the key and value matrices
are equal, the key-value matrix is symmetric and, therefore,
only requires d(d+ 1)/2elements to be represented. This
implies that, when the value matrix is invertible, the min-
imal number of hidden neurons our gated RNN needs to
store key-values is in fact d(d+ 1)/2 +d. In Section 4, we
show that learned RNNs find this solution.
Alternatively, it is also possible to reduce the construction
size when the weight matrices of the teacher attention layer
are of low rank. In this case, we still have a quadratic
scaling of the required numbers of recurrent neurons, but
this time in the rank of the different matrices instead of the
entire dimension. The detailed derivation can be found in
Appendix A.4.
Overall, the output gating requires O(d2)input and output
entries for the gated RNN to match a linear self-attention
layer. The RNN thus requires O(d4)parameters in total,
3

--- PAGE 4 ---
Gated recurrent neural networks discover attention
with a lot of redundancy, significantly more than the 3d2
parameters of the linear self-attention layer. We note that
changing the output gating to a side one is possible, c.f.
Appendix A.2, reducing the number of required parameters
toO(d3).
Given the high parameter redundancy, it comes as no sur-
prise that numerous equivalent configurations exist within
the gated RNN we study. For instance, linear gating is in-
variant under permutations of rows between its two matrices
and under multiplication-division of these two rows by a
constant. Left-multiplying WQin the input gating by any
invertible matrix P, and subsequently reading out the hid-
den neurons with λ= 0through repeat( P−1, d), also does
not alter the network’s output. Several other invariances ex-
ist, making exact weight retrieval nearly impossible. These
considerations will be of practical use when we will reverse
engineer the function encoded by trained recurrent networks
in Section 4.1.
3.3. Implications for existing classes of RNNs
We conclude this section by commenting on whether similar
insights hold for more realistic gated RNNs architectures.
The LRU architecture is close to (2)but only contains output
gating through a GLU layer. Stacking two LRU layers on
top of each other enables the output gating of the first layer
to act as the input gating for the second layer and, therefore,
implement the mechanism we highlighted in the previous
sections to mimick attention. Intuitively, adding an input
GLU would bias the LRU towards linear self-attention as
one layer would now enough to implement it. We will later
confirm that this indeed improves the LRU ability to mim-
ick linear self-attention, as well as boost its performance on
certain tasks. The Mamba block has a stronger inductive
bias towards attention due to the presence of a side gating
querying the memory stored in the recurrent state. Interest-
ingly, it has been found that removing the input dependence
of the matrix projecting to the hidden state is detrimental to
performance (Gu & Dao, 2023). This decreases the induc-
tive bias towards linear self-attention, which might partly
explain the performance drop.
As noted in Section 2.2, LSTMs and GRUs are further away
from our simplified gated RNN model. However, one single
LSTM layer can implement linear self-attention, but stacked
GRU layers cannot. Let us briefly summarize the argument
behind these results. The LSTM layer has a sophisticated in-
put gating mechanism that gates a candidate cell state based
on the current input and previous state. The gate and the can-
didate cell state depend, among other things, on the current
input. This mechanism can thus play a similar role to gin
and implement the key-value outer product. The recurrence
of the cell state can be set to perfectly integrate key-values,
by setting the forgetting gate accordingly. Finally, the out-put gate modulates the current cell state, which contains the
accumulated key-values. Setting the output gate to encode
the query enables computing the desired result. We note
that the output gating differs from gout: it multiplies trans-
formations of the cell state and the input instead of the input
only. This property makes it possible to implement attention
within one layers, where as two layers are required for our
gated RNN model (2). While the GRU layer takes many
of the computational elements from the LSTM, it cannot
implement attention as it has no mechanism to compute
multiply keys and values.
We refer the reader to Appendix B for more details.
4. Gated RNNs learn to mimic attention
We now demonstrate that gated RNNs learn to implement
linear self-attention and comprehend how they do so. In this
section, a student RNN is tasked to reproduce the output of
a linear self-attention layer. Appendix C contains detailed
descriptions of all experiments performed in this section.
Importantly, each sequence is only presented once to the
network.
4.1. Teacher identification
In our first experiment, we train a student RNN ( |x|= 4,
|h|= 100 and|y|= 4) to emulate the behavior of a linear
self-attention layer with weights sampled from a normal
distribution and inputs xtsampled i.i.d. from a normal
distribution. The low training loss, reported in Table 1, high-
lights that the student’s in-distribution behavior aligns with
the teacher’s. However, this is insufficient to establish that
the student implements the same function as the teacher.
The strategy we adopt to show functional equivalence is as
follows: First, we observe that only perfect memory neurons
(λ= 1) and perfect forget neurons ( λ= 0) influence the net-
work output. Additionally, each of these groups of neurons
receives all the information needed to linearly reconstruct
resp. the key-values and the queries from the input (Table 1
Score KV and Score Q columns). Finally, we show that
the output gating and the decoder matrix accurately multi-
ply accumulated key-values with current queries, leading to
proper identification of the teacher self-attention function,
even outside the training distribution (Table 1 Polynomial
distance).
After the learning process, a significant part of the weights
in the input and output gating and the readout becomes zeros.
We can thus prune neurons with input or output weights that
are entirely zeros, thereby preserving the network’s function.
By doing so, we can remove 86out of the 100hidden neu-
rons and 87out of the 100pre-readout neurons. After having
permuted rows in the two gating mechanisms and reordered
hidden neurons, we plot the resulting weights on Figure 2.B.
4

--- PAGE 5 ---
Gated recurrent neural networks discover attention
Figure 2. In our teacher-student experiment of Section 4.1 ( d= 4), the structure of the weights of the RNN after learning matches the one
of our compact construction, c.f. Section 3. (A)Summary of the post-processing we apply to the trained network weights. The number of
recurrent neurons is denoted n, and the number of neurons after the output gating is denoted m.(B)Only recurrent neurons with perfect
memory ( λ= 1, dark blue) or no memory at all ( λ= 0, light grey) influence the output, consistently with the theory. The block structure
of the different weight matrices almost perfectly match the one of our construction, c.f. Figure 1 (C)The last three output neurons of
the output gating are functionally equivalent to a single neuron whose input weights match the structure of the rest of the output gating
weights. This can be achieved by representing each such neuron as an outer product (left part) which will later be combined by the readout
matrix D. The combined kernels are rank 1 and proportional to each other. They can thus be expressed as the same outer product (right
part). In all the matrices displayed here, zero entries are shown in light grey, blue denotes positive entries, and red negative ones.
Consistently with our construction, only recurrent neurons
withλ= 0 orλ= 1 contribute to the network’s output.
The key-values neurons receive a polynomial of degree 2,
asginis a bilinear form, without any term of degree 1as
the last column of Win
mandWin
xis equal to zero for those
units. Similarly, the query neurons receive a polynomial of
degree 1. The learning process discovers that it can only use
d(d+ 1)/2 = 10 neurons to store key-values, similar to our
optimal construction. We show in Table 1 that it is possible
to linearly reconstruct the key-values from those 10neurons
perfectly, as well as the queries from the 4query neurons.
By combining this information with the fact that the λs are
zeros and ones, we deduce that the cumulative key-valuesP
t′≤tvt′k⊤
t′can be obtained linearly from the key-values’
hidden neurons, and the instantaneous queries qtfrom the
query neurons.
Additionally, the output gating combined with the linear
readout can multiply the key-values with the queries. Since
we have already confirmed that the temporal processing cor-
rectly accumulates key-values, our focus shifts to proving
that the instantaneous processing of the gated RNN matches
the one of the attention layer across the entire input domain.
Given that both architectures solely employ linear combi-
nations and multiplications, their instantaneous processing
can be expressed as a polynomial of their input. The one ofLoss Score KV Score Q Polynomial distance
4.97×10−84.52×10−82.06×10−103.73×10−4
Table 1. Gated RNNs implement the same function as a linear self-
attention layer in our teacher-student experiment (Section 4.1).
The KV and Q scores are equal to one minus the R2score of
the linear regression that predicts key-values and queries from
resp. the perfect memory neurons (those whose λ= 1) and perfect
forget neurons ( λ= 0). The polynomial distance is the L2 distance
between the coefficients of the degree-4 polynomial that describes
the instantaneous processing of the (optimal) linear self-attention
layer and the trained RNN.
linear self-attention, (WVx)(WKx)⊤(WQx), corresponds
to a polynomial of degree 3, whereas the one of the gated
RNN, gout(gin(x)), corresponds to one of degree 4. By
comparing these two polynomials, we can compare their
functions beyond the training domain. For every one of the
four network outputs, we compute the coefficients of terms
of degree 4or lower of their respective polynomials and
store this information into a vector. We then calculate the
normalized Euclidean distance between these coefficient
vectors of the linear self-attention layer and the gated RNN,
and report the average over all 4 output units in Table 1. The
evidence presented so far enables us to conclude that the
student network has correctly identified the function of the
5

--- PAGE 6 ---
Gated recurrent neural networks discover attention
A B C
Figure 3. Gated RNNs learn compressed representations when possible. In the teacher-student experiment of Section 4 (A, B) , the gated
RNN identifies the teacher function under mild overparametrization. When the attention layer weights are low rank (B)the RNN learns a
more compressed representation than what it would do when they are full rank (A).(C)In the linear regression task of Section 5, the
gated RNN behaves similarly to the optimal linear attention layer for that task, as the difference between their losses (delta loss) goes to 0.
Moreover, the RNN discovers the same low-rank structure as this attention layer.
teacher.
While the majority of the weights depicted in Figure 2.A
conform to the block structure characteristic of our construc-
tion, the final three rows within the output gating matrices
deviate from this trend. As shown in Figure 2.B, these three
rows can be combined into a single row matching the de-
sired structure. More details about this manipulation can be
found in Appendix C.2.
4.2. Identification requires mild overparametrization
The previous experiment shows that only a few neurons in a
network of 100hidden neurons are needed to replicate the
behavior of a self-attention layer whose input size is d. We
therefore wonder if identification remains possible when
decreasing the number of hidden and pre-output gating neu-
rons the student has. We observe that mild overparametriza-
tion, around twice as many neurons as the actual number
of neurons required, is needed to reach identification. We
report the results in Figure 3.A.
4.3. Nonlinearity makes identification harder
We now move away from our simplified class of gated RNNs
and seek to understand how our findings apply to LSTMs,
GRUs, and LRUs. We use the following architecture for
those three layers: a linear embedding layer projects the
input to a latent representation, we then repeat the recurrent
layer once or twice, and finally apply a linear readout. While
those layers are often combined with layer normalization,
dropout, or skip connections in modern deep learning ex-
periments, we do not include any of those here to stay as
close as possible to the teacher’s specifications. In an LRU
layer, the input/output dimension differs from the number
of different neurons; we here set all those dimensions to the
same value for a fair comparison with LSTMs and GRUs.
We compare these methods to the performance of our sim-
plified gated RNNs, with both diagonal (as in Equation 2)and dense linear recurrent connectivity.
We report the results in Figure 4.A for inputs of dimension
d= 6. While diagonal connectivity provides a useful in-
ductive bias to learn how to mimic linear self-attention, it is
not absolutely needed as changing the recurrence connec-
tivity to be dense does not significantly affect performance.
It is theoretically possible to identify the teacher with one
LSTM layer. However, gradient descent does not find such
a solution and the performance of LSTMs is close to that
of GRUs that cannot implement attention. Motivated by
the construction of Section 3, we slightly modify the LRU
architecture (LRU+) and add a nonlinear input gating to
the already existing output gating. We find that this mod-
ification significantly improves the ability of a LRU layer
to mimic attention. Appendix C contains experiments that
extensively compare different LRU architectures, as well as
comparisons that take into account the number of parame-
ters of the different architectures. Additionally, we provide
results confirming that multiplicative interactions are fun-
damental for mimicking attention: replacing gating with a
1-hidden layer MLP with the same number of parameters
significantly deteriorates performance.
5.Attention-based in-context learning emerges
in trained RNNs
The previous section shows that gated RNNs learn to repli-
cate a given linear self-attention teacher. We now demon-
strate that they can find the same solution as linear self-
attention when both are learned. To that end, we study an in-
context regression task in which the network is shown a few
input-output pairs and later has to predict the output value
corresponding to an unseen input. Linear self-attention is
a particularly beneficial inductive bias for solving this task.
When the input-output mapping is linear, (von Oswald et al.,
2023) have shown that linear self-attention implement one
step of gradient descent.
6

--- PAGE 7 ---
Gated recurrent neural networks discover attention
A B
Figure 4. Comparison of the test loss obtained by different gated recurrent networks architectures in (A)the teacher-student task of
Section 4 and (B)the in-context linear regression task of Section 5. The construction baseline corresponds to the gated RNN of Eq. 2, with
diagonal or dense connectivity. We use the default implementation of LSTMs and GRUs, and slightly modify the LRU architecture to
reflect our construction better. Non-linearity improves the in-context learning performance but deteriorates the ability to mimic attention.
5.1. In-context linear regression
Linear regression consists in estimating the parameters
W∗∈Rdy×dxof a linear model y=W∗xfrom a set of
observations {(xt, yt)}T
t=1that satisfy yt=W∗xt. The ob-
jective consists in finding a parameter ˆWwhich minimizes
the squared error loss L(W) =1
2TPT
t=1∥yt−Wxt∥2.
Given an initial estimate of the parameter W0, one step
of gradient descent on Lwith learning rate Tηyields the
weight change
∆W0=ηTX
t=1(yt−W0xt)x⊤
t. (3)
In the in-context version of the task, the observations
(xt, yt)1≤t≤Tare provided one after the other to the net-
work, and later, at time T+ 1, the network is queried with
(xT+1,0)and its output regressed against yT+1. Under this
setting, von Oswald et al. (2023) showed that if all bias
terms are zero, a linear self-attention layer learns to imple-
ment one step of gradient descent starting from W0= 0and
predict through
ˆyT+1= (W0+ ∆W0)xT+1=ηTX
t=1ytx⊤
txT+1.(4)
In the following, we show that gated RNNs also learn to
implement the same algorithm and leverage the sparse struc-
ture of the different attention matrices corresponding to
gradient descent to learn a more compressed representation
than the construction one.
5.2. Gated RNNs learn to implement gradient descent
We now train gated RNNs as in Equation 2 to solve the in-
context linear regression task, see Appendix D.1 for more
details. We set the number of observations to T= 12 and
set the input and output dimensions to 3so that d= 6. Once
learned, the RNN implements one step of gradient descentTerm RNN GD
x2
1y1 6.81×10−2±8.52×10−56.76×10−2
x2
2y1 6.82×10−2±6.40×10−56.76×10−2
x2
3y1 6.82×10−2±5.56×10−56.76×10−2
residual 1.35×10−3±1.97×10−40
Table 2. Gated RNNs implement gradient descent in the in-context
linear regression task of Section 5. Here, the input (resp. out-
put) at time tis denoted as xt= (xt,1, xt,2, xt,3)⊤(resp. yt=
(yt,1, yt,2, yt,3)). The instantaneous function for each output neu-
ron can implement a polynomial of degree 4 in these terms. The
table shows the coefficients of the polynomial implemented by
the first output neuron of a trained RNN on the in-context linear
regression task. Interestingly, the only terms without negligible co-
efficients (averaged over 4 seeds) are (x1)2y1,(x3)2y1,(x3)2y1.
The polynomial is virtually identical to that of one optimal step
of gradient descent. The optimal GD learning rate is obtained
analytically ( η∗= (T+dx−1/5)−1), c.f. Appendix D.2. The
residual norm measures the norm of the polynomial coefficients,
excluding the ones appearing in the table.
with optimal learning rate, which is also the optimal solution
one layer of linear self-attention can find (Mahankali et al.,
2023). Several pieces of evidence back up this claim: the
training loss of RNN after training ( 0.0945 ) is almost equal
to the one of an optimal step of gradient descent ( 0.0947 )
and the trained RNN implements the same instantaneous
function, as the polynomial analysis of Table 2 reveals.
Linear self-attention weights implementing gradient descent
have a very specific low-rank structure (von Oswald et al.,
2023). To test whether the network learned our correspond-
ing compressed construction, we vary the gated RNN size
and report in Figure 3.C the difference between the final
training loss and the loss obtained after one optimal gra-
dient descent step. We observe a similar transition from
high to low low than in the teacher-student experiment, this
time happening around the number of recurrent neurons
7

--- PAGE 8 ---
Gated recurrent neural networks discover attention
prescribed by our low-rank construction. Gated RNNs thus
learn a more compressed representation than the one naively
mimicking self-attention. This result provides some hope re-
garding the poor O(d4)scaling underlying our construction:
in situations that require an attention mechanism with low-
rank(WV, WK, WQ)matrices, gated RNNs can implement
attention with far fewer neurons. A precise understanding
of how much compression is possible in practical scenarios
requires further investigation.
In Appendix D.3, we provide an additional set of results
focusing on associative recall, an in-context task where
the goal is to memorize (and then retrieve) associations
between pairs of inputs presented in sequence (Fu et al.,
2023). This may be viewed as a simple instance of in-
context classification, which does not require generalization.
As for linear regression, we find that trained gated RNNs
discover an algorithm similar to the one employed by linear
self-attention.
5.3. Nonlinear gated RNNs are better in-context
learners than one step gradient descent
Finally, as a side question, we compare the ability to learn
in context of the nonlinear gated RNN architectures that are
LSTMs, GRUs and LRUs. Although not the main focus
of our paper, this allows us to put our previous results in
perspective. In particular, we are interested in understand-
ing if similarity with attention correlates with in-context
learning performance, as attention has been hypothesized to
be a key mechanism for in-context learning (Olsson et al.,
2022; Garg et al., 2022; von Oswald et al., 2023). We report
our comparison results in Figure 4.B, measuring the loss
on weights W∗drawn from a distribution with double the
variance of the one used to train the model.
Overall, we find that nonlinearity greatly helps and enables
nonlinear gated RNN architectures to outperform one gra-
dient descent step when given enough parameters, suggest-
ing that they implement a more sophisticated mechanism.
Surprisingly, while the GRU is the architecture that is the
furthest away from attention, it performs the best in the task.
Within the different LRU layers we compare, we find a high
correlation between in-context learning abilities and close-
ness to attention, c.f. Figure 6 in the Appendix. In particular,
we observe a massive performance improvement from the
vanilla LRU architecture to the ones additionally including
input gating to match our construction more closely. Once
again, replacing the GLU by a MLP leads to a great decrease
in performance.
6. Discussion
Our study reveals a closer conceptual relationship between
RNNs and attention-based architectures than commonlyassumed. We demonstrate that gated RNNs can theoretically
and practically implement linear self-attention, bridging
the gap between these two architectures. Moreover, while
Transformers have been shown to be powerful in-context
learners (Brown et al., 2020; Chan et al., 2022), we find
that RNNs excel in toy in-context learning tasks and that
this performance is partly uncorrelated with the architecture
inductive bias toward attention. This highlights the need for
further investigations on the differences between RNNs and
Transformers in controlled settings, as also advocated by
(Garg et al., 2022).
Our results partly serve as a negative result: implementation
of attention is possible but requires squaring the number
of parameters attention has. We have shown that gated
RNNs can leverage possible compression, but understand-
ing whether real-world attention mechanisms lie in this
regime remains an open question. Yet, our work is of cur-
rent practical relevance as it provides a framework that can
guide future algorithmic developments, as we exemplify
in Appendix B.5. Bridging the gap between Transformers’
computational power and RNNs’ inference efficiency is a
thriving research area (Fournier et al., 2023), and the link
we made facilitates interpolation between those two model
classes.
Finally, our work carries implications beyond deep learning.
Inspired by evidence from neuroscience supporting the exis-
tence of synaptic plasticity at different timescales, previous
work (Schmidhuber, 1992; Ba et al., 2016; Miconi et al.,
2018) added a fast Hebbian learning rule, akin to linear
self-attention, to slow synaptic plasticity with RNNs. We
show that, to some extent, this mechanism already exists
within the neural dynamics, provided that the response of
neurons can be multiplicatively amplified or shut-off in an
input-dependent manner. Our results therefore suggest that
recurrent neural circuits with long integration time constants,
such as those found in the prefrontal cortex, might be learn-
ing and holding associations between past inputs in working
memory. These circuits would effectively encode associa-
tive weights in their neural activity, not in actual synaptic
connections, as would be the case for classical associative
memory networks (Steinbuch, 1961; Willshaw et al., 1969;
Kohonen, 1972). Interestingly, several single-neuron and
circuit-level mechanisms have been experimentally iden-
tified which could support the required multiplication op-
eration in biological neural networks (Silver, 2010). We
speculate that such multiplicative mechanisms could be in-
volved in implementing self-attention-like computations in
biological circuitry.
Acknowledgements
The authors thank Asier Mujika and Razvan Pascanu for
invaluable discussions. This study was supported by an
8

--- PAGE 9 ---
Gated recurrent neural networks discover attention
Ambizione grant (PZ00P3 186027) from the Swiss National
Science Foundation and an ETH Research Grant (ETH-23
21-1).References
Ahn, K., Cheng, X., Daneshmand, H., and Sra, S.
Transformers learn to implement preconditioned gra-
dient descent for in-context learning. arXiv preprint
arXiv:2306.00297 , 2023.
Ba, J., Hinton, G. E., Mnih, V ., Leibo, J. Z., and Ionescu,
C. Using fast weights to attend to the recent past. In
Advances in neural information processing systems , 2016.
Bahdanau, D., Cho, K., and Bengio, Y . Neural machine
translation by jointly learning to align and translate. In
International Conference on Learning Representations ,
2015.
Boyd, S. and Chua, L. Fading memory and the problem
of approximating nonlinear operators with V olterra se-
ries. IEEE Transactions on Circuits and Systems , 32(11),
1985.
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,
Wanderman-Milne, S., and Zhang, Q. JAX: composable
transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax .
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., and others. Language models are few-shot
learners. In Advances in neural information processing
systems , 2020.
Chan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A.,
Richemond, P., McClelland, J., and Hill, F. Data distri-
butional properties drive emergent in-context learning in
transformers. In Advances in Neural Information Pro-
cessing Systems , 2022.
Cho, K., van Merrienboer, B., Bahdanau, D., and Bengio, Y .
On the properties of neural machine translation: encoder-
decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statisti-
cal Translation , 2014.
Choromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,
Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin,
A., Kaiser, L., Belanger, D., Colwell, L., and Weller, A.
Rethinking attention with Performers. In International
Conference on Learning Representations , 2021.
Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan-
guage modeling with gated convolutional networks. In
International Conference on Machine Learning , 2017.
Dayan, P. and Abbott, L. F. Theoretical neuroscience: com-
putational and mathematical modeling of neural systems .
MIT Press, 2001.
9

--- PAGE 10 ---
Gated recurrent neural networks discover attention
Fournier, Q., Caron, G. M., and Aloise, D. A practical sur-
vey on faster and lighter transformers. ACM Computing
Surveys , 55(14s), 2023.
Fu, D. Y ., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A.,
and R ´e, C. Hungry Hungry Hippos: Towards Language
Modeling with State Space Models. In International
Conference on Learning Representations , 2023.
Garg, S., Tsipras, D., Liang, P. S., and Valiant, G. What
can transformers learn in-context? a case study of simple
function classes. In Advances in Neural Information
Processing Systems , 2022.
Grigoryeva, L. and Ortega, J.-P. Universal discrete-time
reservoir computers with stochastic inputs and linear
readouts using non-homogeneous state-affine systems.
Journal of Machine Learning Research , 19, 2018.
Gu, A. and Dao, T. Mamba: Linear-time sequence modeling
with selective state spaces, 2023.
Gu, A., Goel, K., and R ´e, C. Efficiently modeling long
sequences with structured state spaces. In International
Conference on Learning Representations , 2022.
Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are
as effective as structured states spaces. In Advances in
Neural Information Processing Systems , 2022.
Harris, C. R., Millman, K. J., Walt, S. J. v. d., Gommers, R.,
Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg,
S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., Kerkwijk,
M. H. v., Brett, M., Haldane, A., R ´ıo, J. F. d., Wiebe, M.,
Peterson, P., G ´erard-Marchant, P., Sheppard, K., Reddy,
T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant,
T. E. Array programming with NumPy. Nature , 585
(7825), 2020.
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre,
B., Steiner, A., and Zee, M. v. Flax: A neural network
library and ecosystem for JAX, 2023. URL http://
github.com/google/flax .
Hochreiter, S. and Schmidhuber, J. Long short-term memory.
Neural Computation , 9(8), 1997.
Hunter, J. D. Matplotlib: A 2D graphics environment. Com-
puting in Science & Engineering , 9(3), 2007.
Jaeger, H., Noheda, B., and Van Der Wiel, W. G. Toward
a formal theory for computing machines made out of
whatever physics offers. Nature Communications , 14(1),
2023.
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are RNNs: fast autoregressive Transformers
with linear attention. In International Conference on
Machine Learning , 2020.Kohonen, T. Correlation matrix memories. IEEE Transac-
tions on Computers , 100(4):353–359, 1972.
Loshchilov, I. and Hutter, F. Decoupled weight decay reg-
ularization. In International Conference on Learning
Representations , 2019.
Mahankali, A., Hashimoto, T. B., and Ma, T. One step of
gradient descent is provably the optimal in-context learner
with one layer of linear self-attention. arXiv preprint
arXiv:2307.03576 , 2023.
Martinelli, F., Simsek, B., Brea, J., and Gerstner, W. Expand-
and-cluster: exact parameter recovery of neural networks.
arXiv preprint arXiv:2304.12794 , 2023.
Miconi, T., Clune, J., and Stanley, K. O. Differentiable
plasticity: training plastic neural networks with back-
propagation. In International Conference on Machine
Learning , 2018.
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma,
N., Henighan, T., Mann, B., Askell, A., Bai, Y ., Chen,
A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds,
Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J.,
Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J.,
Kaplan, J., McCandlish, S., and Olah, C. In-context learn-
ing and induction heads. Transformer Circuits Thread ,
2022.
Orvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith,
S. L. On the universality of linear recurrences followed
by nonlinear projections. In ICML 2023: 1st Workshop
on High-dimensional Learning Dynamics , 2023a.
Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre,
C., Pascanu, R., and De, S. Resurrecting recurrent neural
networks for long sequences. In International Conference
on Machine Learning , 2023b.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V ., and others. Scikit-learn: Ma-
chine learning in Python. Journal of machine Learning
research , 12, 2011.
Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,
S., Cao, H., Cheng, X., Chung, M., Grella, M., GV , K. K.,
He, X., Hou, H., Kazienko, P., Kocon, J., Kong, J., Kop-
tyra, B., Lau, H., Mantri, K. S. I., Mom, F., Saito, A.,
Tang, X., Wang, B., Wind, J. S., Wozniak, S., Zhang,
R., Zhang, Z., Zhao, Q., Zhou, P., Zhu, J., and Zhu, R.-J.
RWKV: Reinventing RNNs for the transformer era. arXiv
preprint arXiv:2305.13048 , 2023.
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,
N. A., and Kong, L. Random feature attention. In Inter-
national Conference on Learning Representations , 2021.
10

--- PAGE 11 ---
Gated recurrent neural networks discover attention
Schlag, I., Irie, K., and Schmidhuber, J. Linear Transformers
are secretly fast weight programmers. In International
Conference on Machine Learning , 2021.
Schmidhuber, J. Learning to control fast-weight memories:
an alternative to dynamic recurrent networks. Neural
Computation , 4(1), 1992.
Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient
attention: attention with linear complexities. In Proceed-
ings of the IEEE/CVF Winter Conference on Applications
of Computer Vision (WACV) , 2021.
Silver, R. A. Neuronal arithmetic. Nature Reviews Neuro-
science , 11(7), 2010.
Smith, J. T., Warrington, A., and Linderman, S. W. Simpli-
fied state space layers for sequence modeling. In Interna-
tional Conference on Learning Representations , 2023.
Steinbuch, K. Die lernmatrix. Kybernetik , 1:36–45, 1961.
Sun, Y ., Dong, L., Huang, S., Ma, S., Xia, Y ., Xue, J.,
Wang, J., and Wei, F. Retentive network: A successor to
transformer for large language models, 2023.
Tay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham,
P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long
range arena: A benchmark for efficient transformers.
arXiv preprint arXiv:2011.04006 , 2020.
Tsai, Y .-H. H., Bai, S., Yamada, M., Morency, L.-P., and
Salakhutdinov, R. Transformer dissection: a unified un-
derstanding of transformer’s attention via the lens of ker-
nel. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Pro-
cessing , 2019.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-
tion is all you need. In Advances in Neural Information
Processing Systems , 2017.
von Oswald, J., Niklasson, E., Randazzo, E., Sacramento,
J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov,
M. Transformers learn in-context by gradient descent. In
International Conference on Machine Learning , 2023.
Willshaw, D. J., Buneman, O. P., and Longuet-Higgins, H. C.
Non-holographic associative memory. Nature , 222(5197):
960–962, 1969.
Yang, S., Wang, B., Shen, Y ., Panda, R., and Kim, Y . Gated
linear attention Transformers with hardware-efficient
training, 2023.Zhang, R., Frei, S., and Bartlett, P. L. Trained trans-
formers learn linear models in-context. arXiv preprint
arXiv:2306.09927 , 2023.
Zucchet, N., Meier, R., and Schug, S. Minimal LRU, 2023a.
URLhttps://github.com/NicolasZucchet/
minimal-LRU .
Zucchet, N., Meier, R., Schug, S., Mujika, A., and Sacra-
mento, J. Online learning of long-range dependencies.
InAdvances in Neural Information Processing Systems ,
2023b.
11

--- PAGE 12 ---
Gated recurrent neural networks discover attention
A. Additional details about the construction
In Section 3 and Figure 1, we have shortly described our
construction. We here provide additional details, as well
as refine it to settings in which we assume additional struc-
ture on the key, query and values matrices. We recall the
mathematical definition of the gated RNN we consider:
ht+1=λ⊙ht+gin(xt) (5)
yt=Dgout(ht) (6)
gin(x) = (Win
mx)⊙(Win
xx) (7)
gout(x) = (Wout
mx)⊙(Wout
xx). (8)
A.1. Explicit values of the matrices of the vanilla
construction
Here, we detail the values the matrices in Figure 1 take to
mimic a linear self-attention layer with key, query and value
matrices WK,WQandWV. The key-values are stored in
the first d2recurrent neurons and the queries in the last d
ones (indices d2+ 1tod2+d).
Input gating. Win
xandWin
mare matrices of size (d2+
d)×(d+ 1). The matrix Win
xboth computes the values and
the queries:
(Win
x)i,j=

(WV)i/d,jifj≤dandi≤d2
(WQ)i−d2,jifj≤dandi > d2
0 otherwise(9)
and the matrix Win
mthe keys:
(Win
m)i,j=

(WK)imodd,j ifj≤dandi≤d2
1 ifj=d+ 1andi > d2
0 otherwise
(10)
where /denotes integer division and mod the modulo op-
eration. As a consequence, the input received by the i-th
recurrent neuron is (WVx)i/d(WKx)imoddwhen i≤d2,
and(WQx)i−d2when i > d2.
Recurrent neurons. λis a vector of size d2+dwith
λi=1ifi≤d2
0otherwise.(11)
The memory neurons, the first d2for which λ= 1, perfectly
integrate all the key-values pairs.
Output gating. Wout
xandWout
mare matrices of size d2×
(d2+d)withWout
xselecting the desired key-value element
(Wout
x)i,j=1ifj≤dandi=j
0otherwise(12)
sum matrixFigure 5. Construction for gated RNNs with side gating, as de-
scribed in Section A.2
andWout
mthe query element
(Wout
m)i,j=1ifj > d2andi=jmodd
0otherwise(13)
After the d2output neurons of the output gating thus
contains all the P
t′(WVxt′)(WKxt′)⊤
i,j(WQxt)jel-
ements, and it only remains to sum them.
Readout. The goal of the readout matrix D, which has
sized×d2, is to sum the key-values query products. It is
equal to
Di,j=1ifi=j/d
0otherwise(14)
The output iof the gated RNN will thus beP
j P
t′(WVxt′)(WKxt′)⊤
i,j(WQxt)j, which is
equals to  P
t′(WVxt′)(WKxt′)⊤
(WQxt)
i, the
desired output.
A.2. Alternative construction with side gating
With input and output gating, one has to waste some of the
recurrent neurons to instantaneously pass through the query
values. We chose this architecture because it is arguably
simple and more common, but it is possible to give a RNN
with a stronger inductive bias towards linear self-attention
by replacing the output gating with a side gating, that is
yt=Dgside(x, ht),with gside(x, h) = (Wsidex)⊙h.
(15)
Interestingly, this kind of side gating is featured in the re-
cently proposed Mamba layer and indirectly in LSTMs, as
we shall discuss in further detail in Section B. We detail how
to adapt our construction to the side gating and provide a
visual depiction of it in Figure A.2. Crucially, this construc-
tion only requires O(d3)parameters instead of the O(d4)
of the previous one.
12

--- PAGE 13 ---
Gated recurrent neural networks discover attention
Input gating and recurrent neurons. The construction
remains the same as the previous one, except that we get
rid of the constant term in the input and the last drecurrent
neurons.
Side gating. The side gating matrix Wsideis of size
Rd2×dhas to copy queries dtimes and put them in front of
the corresponding key-value entry, that is
Wside
i,j= (WQ)imodd,j (16)
Readout matrix. It remains the same as before.
A.3. Reducing construction size with invertible WV/
WK
In Section 3.2, we have argued that it is possible to reduce
the number of recurrent neurons to d(d+ 1)/2 +dwhen
WQis invertible. We use two insights.
Invariances of the linear self-attention layer. The first
thing we can remark is that modifying WQandWKdoes
not change the output of the layer as long as W⊤
KWQis kept
constant. This is because
 X
t′(WVxt′)(WKxt′)⊤!
(WQxt)
=WV X
t′xt′x⊤
t′!
W⊤
KWQxt(17)
It follows that a linear self-attention layer with weights
(WK, WQ, WV)behaves similarly to one with weights
(WV, W−⊤
VW⊤
KWQ, WV), as
W⊤
VW−⊤
VW⊤
KWQ=WKWQ. (18)
Note that a similar argument holds if WKis invertible.
Symmetry of the key-values. In the paragraph above, we
have justified why we can consider the key and query values
to be equal. In this case, the key-values matrix becomes
symmetric. Knowing the elements contained in the upper
triangular part is thus enough to know the entire matrix.
We can thus ignore recurrent neurons corresponding to the
lower triangular part. Note that similar insights apply to the
side gating construction.
A.4. Reducing construction size with low-rank teacher
Intuitively, when the teacher attention layer is of low rank,
it is not necessary to represent all the elements of the key-
values matrices if we can change the basis considered. We
formalize this argument in the following. To that extent, weintroduce the SVD decomposition of the value and query-
key matrices:
WV=UVΣVV⊤
V (19)
W⊤
KWQ=UKQΣKQV⊤
KQ. (20)
withΣdiagonal matrices with as many non-zero elements
as the rank of the matrix, and UandVorthogonal matrices.
The output of the attention layer can thus be written as
UV X
t′(ΣVVVxt′)(ΣKQUKQxt′)⊤!
VKQxt.(21)
With this decomposition, only the first rank( WV)rows
andrank( W⊤
KWQ)columns of the key-values matrix are
not 0, that is we can reduce the number of recurrent neu-
rons in our construction to rank( W⊤
KWQ) rank( WV). Re-
garding the queries, only the first rank( W⊤
KWQ)coordi-
nates will be considered. In total, we thus need at most
rank( W⊤
KWQ)(rank( WV) + 1) neurons to replicate the
teacher. As in the previous section, similar insights applies
to the side gating construction.
To confirm that gated RNNs learn this solution, we per-
formed a similar analysis to the one we did in Figure 3.A,
this time with low-rank teacher. To that extent, we take
d= 12 and restrict the rank of the key, query and value
matrices to be 6. We do so by randomly sampling WK,WQ
andWVand removing 12−6 = 6 singular values. Given
the random sampling, rank( W⊤
KWQ) = 6 almost surely.
We observe the stereotypical transition when the number of
hidden neurons match rank( W⊤
KWQ)(rank( WV) + 1) =
6×7 = 42 , as plotted in Figure 3.B.
B. Gated RNNs and linear self-attention
In this section, we compare our simplified gated RNN
model, linear self-attention, and nonlinear gated RNN mod-
els (LSTMs, GRUs, LRUs and Mamba). We recall that the
key ingredients of our simplified gated RNNs defined as
ht+1=λ⊙ht+gin(xt), yt=Dgout(ht), (22)
are the diagonal linear recurrence and the input and output
gating. The input gating serves as a way to generate the
key-values of linear self-attention, which will then be accu-
mulated in the hidden recurrent units and combined with
queries within the output gating.
Table 3 summarizes how many layers of LRUs, Mamba,
LSTMs and GRUs are needed to exactly implement our
simplified class of gated RNNs and linear self-attention. We
provide more details below.
B.1. LRU
An LRU layer (Orvieto et al., 2023b) consists of a recur-
rent state htand some instantaneous post-processing. Its
13

--- PAGE 14 ---
Gated recurrent neural networks discover attention
Simplified
gated RNNLinear self-
attention
LRU 2 2
LRU In-Out 1 1
LRU In-Out (MLP) – –
Mamba 2 1
LSTM 2 1
GRU – –
Table 3. Number of layers needed for different RNN layers to
exactly implement our simplified class and linear self-attention.
recurrent state is updated as
ht+1=λ⊙ht+γ⊙(Bxt+1) (23)
and its output ytis computed with
˜yt+1= Re[ Cht] +Dxt+1 (24)
yt+1=σ(Wm˜yt+1)⊙(Wx˜yt+1). (25)
In the equations above, ht+1,BandCare complex-valued,
Redenotes the real part of a complex number, and σis
the sigmoid function. The transformation nonlinear trans-
formation between yt+1and˜yt+1is called a gated linear
unit (GLU) and was introduced in (Dauphin et al., 2017).
Additionally, λandγare parametrized exponentially:
λ= exp( −exp(νlog) +iexp(θlog)) and γ= exp( γlog).
(26)
The LRU layer detailed above comprises two central com-
putational mechanisms: a linear recurrence coupled with a
GLU serving as nonlinear output gating. The recurrence is
here complex-valued, but we only need the real part of it for
our purposes. Assuming that the sigmoid can be linearized,
our class of gated RNNs can be implemented using two lay-
ers by letting the output gating of the first layer serve as input
gating. We are now left with linearizing the sigmoid. To
achieve this, we double the number of output neurons of the
GLU and require small weights in Wm, that can for example,
be compensated by large weights in Wm. Under this regime,
we have σ(Wmx)⊙(Wxx)≈(1/2 +Wmx)⊙(Wxx).
Half of the neurons require identical weights as the target
linear gating (up to a proportional factor), half should have
Wm= 0 and the same Wxas target linear gating. The
1/2Wxxterm that comes from the second half of the neu-
rons can be subtracted from the first half of the neurons
in a subsequent linear transformation, thereby yielding the
desired result.
In our experiments, we consider two additional variations of
the LRU layer that can implement our class of gated RNNs
and/or linear self-attention using only one layer. The LRU
In+Out variation has an additional nonlinear input gatingmechanism compared to the original version (LRU Out) that
modifies the input before the recurrent part of the layer. The
LRU In+Out (MLP) replaces the GLU in the LRU In-Out
variation by a 1-hidden layer MLP, keeping the number of
parameters fixed. The LRU In-Out variation can implement
both linear self-attention and our class of gated RNNs in
one layer, whereas LRU In-Out (MLP) cannot, as it does
not have any multiplicative interactions.
B.2. Mamba
A (simplified) Mamba layer is defined as
˜xt=Winput(xt) (27)
¯At= exp(∆(˜ xt)A(˜xt)) (28)
¯Bt= ∆(˜ xt)B(˜xt) (29)
ht+1=¯At+1ht+¯Bt+1˜xt+1 (30)
yt=C(˜xt+1)ht+1⊙σ(Wside(xt)) (31)
where ∆,A,B,C,WinputandWsideare linear transfor-
mations that produce resp. a scalar, matrix, matrix, matrix,
vector and vector of appropriate size. For simplicity, we
have ignored the convolutional layer after Winputin˜x, the
fact that each coordinate of ˜xhas its own independent recur-
rent layer and the specific parametrizations of the different
parameters.
Here, the recurrence is linear with input-dependence, and
thus more general than the one we are focusing on in this
paper. It is easy to set it to what our construction requires.
However, finding an input/output gating in this architecture
is more tricky. The main insight is to look at
(B(x)x)i=X
jB(x)ijxj (32)
=B(x)iixi+X
j̸=iB(x)ijxj (33)
and realize that it can implement a gating mechanism in
which one of the branch is the identity. If it is preceded
by a liner layer, such as Winputit can thus behave as the
kind of gating we are focusing on in this paper. The input-
dependent Bthus provides an input gating. The side gating
we studied in Appendix A.2 can be implemented through
the side modulation, by linearizing the sigmoid, or indirectly
through C. This implies that one single Mamba layer can
emulate a linear self-attention layer. However, there is no
mechanism to implement an output gating, so 2 layers are
needed to mimick our simplified class of gated RNNs.
B.3. LSTM
An LSTM cell (Hochreiter & Schmidhuber, 1997) has two
recurrent states: the hidden state htand the cell state ct.
14

--- PAGE 15 ---
Gated recurrent neural networks discover attention
They are updated as follows.
ft+1=σ(Ufxt+1+Vfht+bf) (34)
˜ct+1= tanh( Ucxt+1+Vcht+bc) (35)
gt+1=σ(Ugxt+1+Vght+bg) (36)
ct+1=ft+1⊙ct+gt+1⊙˜ct+1 (37)
ot+1=σ(Uoxt+1+Voht+bo) (38)
ht+1=ot+1⊙tanh( ct+1). (39)
Here, ftis the cell state forget gate, ˜ctthe cell state update
candidate, gtthe cell state update candidate gate, otthe out-
put gate, and σthe sigmoid function applied elementwise.
First, we show that one single LSTM layer can implement
linear self-attention, by using gt+1⊙˜ct+1as a way to com-
pute key-values and cto aggregate them, ft+1and use ot+1
for the query. We provide the corresponding weights in the
table below, ignoring all the nonlinearities except σin the f
computation. Note that, compared to our simplified gated
RNN class, we do not need to include neurons that forget
their last state ( λ= 0) here as the output gate directly pro-
vides the query to the output. Finally, linearizing the tanh
function requires small Ucweights that can later be com-
pensated by large decoder weights, and ways to linearize
the sigmoid were discussed in the previous section.
Implementing a gated RNN as in Equation 2 can be done
by using two layers: in the first layer gt+1⊙˜ct+1serves
as input gating, ft+1corresponds to λ, and, in the second
layer, gt+1⊙˜ct+1serves as output gating. Table 4 provides
one set of such weights. This ignores the linearization trick
for the tanh in˜cand the sigmoid in gt+1.
B.4. GRU
A GRU cell (Cho et al., 2014) has a hidden state ht, updated
through
rt+1=σ(Urxt+1+Vrht+br) (40)
˜ht+1= tanh( Uhxt+1+Vh(rt+1⊙ht) +bh) (41)
zt+1=σ(Uzxt+1+Vzht+bz) (42)
ht+1= (1−zt+1)⊙ht+zt+1⊙˜ht+1 (43)
where rtis the reset gate, ztis the update gate, ˜htthe update
candidate, and σis the sigmoid function.
Here, stacking multiple GRUs on top of each other does not
enable the implementation of any network from our class
of gated RNNs nor linear self-attention layers. One layer
can implement diagonal linear recurrence by linearizing the
tanh , having zt+1= 1andrt+1=λ. However, implement-
ing a gating mechanism of the form g(x) = (Wmx⊙Wxx)
is not possible1: we would need to use zt+1to implement
1When the tanh is replaced by Id, it is possible to achieve soone branch of the gating and ˜ht+1the other but, given that
zt+1̸= 0, the previous hidden state htinfluence the result.
B.5. Can linear self-attention implement gated
recurrent networks?
Throughout the paper, we mainly focus on understand-
ing whether diagonal gated RNNs implement linear self-
attention. In this section, we ask the opposite question: can
linear self-attention layers can implement gated recurrent
networks. The answer is that attention layers as we defined
in Section 2.1 cannot, because it can only perfectly integrate
inputs or send the current one (thus λ= 0orλ= 1). How-
ever, adding a mechanism akin to weight decay bridges the
gap. In particular, we will describe how the output ytof
a such a linear self-attention layer can satisfy a recurrence
relationship of the form yt+1=λ⊙yt+xt. To do so, we
consider the following attention layer:
vt=WVxt+bV (44)
kt=WKxt+bK (45)
qt=WQxt+bQ (46)
yt= tX
t′=1Γt−t′⊙(vt′k⊤
t′)!
qt (47)
where Γt−t′is a matrix of size d×din which all entries of
thei-th row have value (1−γi)t−t′. Such a layer is featured
in recent work, e.g. (Sun et al., 2023) or (Yang et al., 2023).
Theγterm can be interpreted as a weight decay: if we note
Wff
t:= tX
t′=1Γt′−t⊙(WVxt′)(WKxt′)⊤!
, (48)
we have
Wff
t+1=Wff
t+(WVxt+1+bV)(WKxt+1+bK)⊤−Γ1Wff
t.
(49)
Now, we set the value, key and query matrices and biases to
WV= Id, bV= 0, WK= 0, bK= 1, WQ= 0, bQ= 1/d
and1−γ=λ. This way, we have
yt+1=1
dWff
t+11 (50)
=1
d 
Γ1⊙Wff
t+xt+11⊤
1 (51)
= 
Γ1⊙Wff
t
1 +xt+1 (52)
=λ⊙yt+xt+1 (53)
In the last line, we use the structure of Γ1and the value of
γ. Biases terms are crucial to make this link: without them
Wff
twould be a polynomial with only degree 2 coefficients
by having ht≪˜ht+1and correcting for the exponential growth
in the next layer.
15

--- PAGE 16 ---
Gated recurrent neural networks discover attention
Layer 1
U V b
f 0 0 + ∞
˜c˜WK0 0
g˜WV0 0
o˜WQ0 0Layer 1 Layer 2
U V b U V b
f 0 0 σ−1(λ) 0 0 −∞
c Win
m 0 0 Wout
m 0 0
g Win
x 0 0 Wout
x 0 0
o 0 0 + ∞ 0 0 + ∞
Table 4. LSTM weight configuration that matches a linear self-attention layer (left) and a gated RNN as in Equation 2 (right). This
presumes that the activation functions in ˜c,gandoare linear. We use ˜Wto denote the value, key and query matrices transformed in a
similar way to what we did in Figure 1.
and the equivalence would not be possible. The gating
mechanism within networks described in Equation 2 can
also be implemented by forgetting ( 1−γ= 0) and having
the key-value taking care of the multiplication.
This analysis reveals the importance of weight decay to im-
plement recurrent neural network like computations with a
wide range of timescales. Adding complex-valued weight
decay to linear self-attention layers makes them closer to
state-of-the-art recurrent neural networks architecture (Orvi-
eto et al., 2023b; Smith et al., 2023) for capturing long-range
dependencies. Therefore, such a modification might boost
the performance of attention layers on benchmarks testing
these properties, such as the Long Range Arena (Tay et al.,
2020). Interestingly, this view can partly explain the great
empirical performance of the RWKV (Peng et al., 2023),
which features a similar mechanism to weight decay. Over-
all, the analysis we conducted in this section examplify how
the connection between RNNs and attention layers we made
in this paper can be used to guide development of future
architectures.
C. Teacher-student
C.1. Experimental details
For all experiments in Section 4, we train the student for al-
most one million training iterations on sequences of length
32 and a batch size of 64 (50000 training examples per
epoch, 1000 epochs). We use the AdamW (Loshchilov &
Hutter, 2019) optimizer with a cosine annealing learning
rate scheduler. The initial learning rate is set at 10−3, sched-
uled to anneal down to 10−6by the end of training and a
weight decay of 10−4is applied to all parameters except the
recurrent ones λin the experiment of Section 4.1. To ensure
that the hidden states do not explode, we ensure that λstays
within [0,1]by employing the exponential parametrization
described in Appendix B.1 (we only keep the νpart as λ
takes real values here).
In Figure 6, we add more results to the architecture compar-
ison we did in Figure 4. In particular, we compare the three
different types of LRU we mentioned in Appendix B.1, andobserve that adding an input GLU improves LRUs ability to
mimic linear self-attention within one layer, but also with
several layers.
C.2. Compression of the learned output gating weights
In Figure 2, we show that the gating weight matrices have a
structure that is close to the one of our construction, except
for three different rows (11, 12, and 13). We claim they
can be reduced to a single row; we now provide details
justifying it.
Therefore, our objective is to demonstrate that these three
rows are functionally equivalent to a single row with the
expected structure and to gain insights into the invariances
inherent to the gating mechanism we study in this paper
along the way. The initial step toward achieving this entails
examining the influence of these three rows on the i-th
coordinate of the network’s output:
13X
j=11Di,jgout(h)j=13X
j=11Di,j(Wout
m,jx)(Wout
x,jx) (54)
=x⊤
13X
j=11Di,jWout
m,jWout
x,j⊤
x.
(55)
This contribution can be interpreted as a quadratic form
whose kernel is a weighted sum of rank-1 kernels defined
by the rows of the output gating matrices. In Figure 2.C, we
plot the obtained kernel for one of the output components.
Crucially, the resulting kernel for the four output units are all
proportional to one another and is of rank-1. We can thus re-
duce the three neurons (11, 12 and 13) to one. Furthermore,
the two vectors whose outer product yields the resulting
kernel now mirror the construction’s structure. One of these
two vectors exclusively accesses query neurons while the
other reads key-value neurons, as seen in Figure 2.C. As
usually occurs with this kind of manipulation (Martinelli
et al., 2023), merging the neurons slightly increases the loss,
but original loss levels can be recovered after fine-tuning.
16

--- PAGE 17 ---
Gated recurrent neural networks discover attention
Figure 6. Extensive comparison between the different architectures. Compared to Figure 4, we consider different versions of the LRU
here, plot the loss as the function of the number of parameters, and include both training and validation losses. Those two losses are
almost (up to some sampling noise) for the teacher-student task but are different for the in-context linear regression task because we
change the W∗distribution in the validation set.
17

--- PAGE 18 ---
Gated recurrent neural networks discover attention
D. In-context linear regression
D.1. Experimental details
In the in-context linear regression experiment, each se-
quence is a task characterized by a unique W∗. The weight
matrix W∗entries are sampled i.i.d. from a normal dis-
tribution N(0,1
3). Each element of the sequence is of the
form (xt, W∗xt). The entries of the inputs (xt)T+1
t=1are
sampled i.i.d. from the uniform distribution U(−√
3,√
3).
During the validation phase, we draw tasks from a different
distribution, W∗
ij∼ N(0,2
3)to highlight the generalization
abilities of the learned models. We train the model with
the same optimization scheme described in Appendix C.1,
except that we use a smaller number of training iterations,
totaling 300,000. By default, we use gated RNNs with 80
hidden neurons.
D.2. Optimal learning rate for one-step gradient descent
LetX∈Rdx×n, W∈Rdy×dxrandom variables such that
all entries of Xare sampled i.i.d. from a centered uniform
distribution with variance σ2
x, and those of Wi.i.d. from
some centered distribution with finite variance σ2
W. We set
Y=WX . Let x∈Rdya column vector, whose entries
are sampled from the same distribution as those of X, and
y=Wx.
The goal of this section is to analytically derive the optimal
learning rate for the in-context linear regression task, that is
to find ηwhich minimizes
L(η) =1
2EX,W,Y,x,yh
∥y−ˆW(η, X, Y )x∥2i
(56)
where ˆW(X, Y)is the result of one gradient descent step
starting from 0with learning rate ηon the loss W7→1
2∥Y−
WX∥2. The calculation is presented in a more general
form in (Mahankali et al., 2023). We include it here as
we additionally provide a simple formula for exact optimal
learning rate value.
Plugging in the analytical expressions for yandˆW, we get
L(η) =1
2EX,W,Y,x,y
∥y−ηY X⊤x∥2
(57)
=1
2EX,W,x
∥Wx−ηWXX⊤x∥2
(58)
=1
2EX,W,x
∥W(I−ηXX⊤)x∥2
(59)
We want to minimize L, i.e. look for η∗that satisfies∂ηL(η∗) = 0 . We have
∂ηL(η) =EX,W,xh 
W(I−ηXX⊤)x⊤WXX⊤xi
(60)
= TrEX,W,x
(I−ηXX⊤)W⊤WXX⊤xx⊤
(61)
=σ2
xTrEX,W
(I−ηXX⊤)W⊤WXX⊤
(62)
=σ2
xTrEX,W
XX⊤(I−ηXX⊤)W⊤W
(63)
=σ2
xσ2
WTrEX
XX⊤(I−ηXX⊤)
(64)
In the first equation, we use that E[a⊤b] = Tr E[ba⊤].
Third and fifth ones make use of Ex[xx⊤] =σ2
xIdand
EW[WW⊤] =σ2
WId. Having ∂ηL(η∗) = 0 is then equiva-
lent to
η⋆:=TrEX[XX⊤]
TrEX[XX⊤XX⊤]. (65)
This result shows that only the distribution of the learn-
ing data matters. Let us compute this quantity. We
haveEX[XX⊤] =nσ2
xIdso we are left with computing
Ex[XX⊤XX⊤]. Using that entries of Xare i.i.d., we get
TrEX[XX⊤XX⊤] (66)
=dxEX
X
i X
txi,tx1,t!2
 (67)
=dxEX
 X
tx2
1,t!2
 (68)
+dx(dx−1)EX
 X
tx1,tx2,t!2
 (69)
=dxEX
X
tx4
1,t+X
t̸=t′x2
1,tx2
1,t′
 (70)
+dx(dx−1)EX"X
tx2
2,tx2
1,t#
(71)
=9
5ndxσ4
x+n(n−1)dxσ4
x+n(dx−1)σ4
x (72)
=ndxσ4
x
n+dx−1
5
(73)
because the fourth moment of a centered uniform distribu-
tion is9
5σ4
x. Putting everything together, we finally have
η∗=1
σ2x(n+dx−1
5). (74)
18

--- PAGE 19 ---
Gated recurrent neural networks discover attention
D.3. Associative recall
As a complement to in-context linear regression, we con-
sider a simple in-context classification task studied by Fu
et al. (2023), where the network has to remember associa-
tions between paired inputs. As for in-context regression,
the network is presented with a sequence of tokens of the
form(xt, yt)1≤t≤T, followed by a token containing a query
input and a null placeholder (xT+1,0). In this task, xT+1
corresponds exactly to one of the previously seen xt, and the
goal is to complete the placeholder with the corresponding
yt.
To make the task solvable by a single layer of
linear attention, we present the following sequence:
([x1, y1],[y1, x2],[x2, y2]. . . ,[xT, yT],[xT+1,0]), where x
(resp y) have been transformed to a 2T-sized one-hot en-
coding of [1, T](resp. [T+ 1,2T]), resulting in a input
dimension of 2T. Each xand each yonly appear once. We
use a cross entropy loss, using the desired yas target, and
T= 8in our experiments.
Solving the task with linear self-attention. Given that we
provide non-repeating one hot encoded inputs, we can see
that a linear self-attention layer that uses xas key and query,
andyas value will solve the task. That is, its output is
yT+1=
X
t≤Tytx⊤
t
xT+1. (75)
Input-output gating. We first trained a gated RNN on this
task and observe that the solution it finds differs from the
linear self-attention layer, and requires way less recurrent
neurons. To each y, it associates a recurrent neuron with
λ= 1in which it will store a value vxcorresponding to x
when that yappears. That is, if the pair (x, y)appears, the
recurrent neuron associated to yreceives vxas input, and
the other receive no input. Addtionally, the RNN uses one
neuron with λ= 0 containing the value vxassociated to
the current x. The output gating then computes the nega-
tive squared difference between the current value and the
stored ones, so that neural activity after gating is equal to
(−(vxT+1−vx(y))2)ywhere x(y)is the xthat was associ-
ated to yin the sequence. The index of the smallest one,
equal to 0, gives the desired output after taking the argmax.
We note that such a solution is possible as each xandy
appear only once in the sequence, as this is a classification
class and as inputs are one-hot encoded.
Side gating. Then, we use the RNN with side gating of
Section A.2, with parameters Win
x,Win
m,λandWside, and
check whether it implements the same function as the RNN
with input-output gating. It does not, and we detail the
solution it finds in the following. We apply the same post
processing of the weights as we did in Section 4, and findthat only recurrent neurons with λ= 1are remaining. Con-
sistently with the linear self-attention layer that optimally
solves this task, one of the input gating matrix, Win
xon
reads out from the xpart of the input, and the other one,
Win
mfrom y. Additionally, the side gating matrix is equal
to the Win
xmatrix, in a similar way that the query matrix is
equal the key one in the linear self-attention layer. Finally,
theDmatrix is the transpose of the value-like part of matrix
Win
m. Based on those observations, we can rewrite
Win
x=Wside= [A|0] (76)
Win
m= [0|B] (77)
D=B⊤(78)
Asλ= 1, we have
hT=X
t≤Tgin([xt, yt]) =X
t≤T(Byt)⊙(Axt) (79)
and
yT+1=hT+1⊙(WxT+1) (80)
=B⊤X
t≤T(Byt)⊙(Axt)⊙(AxT+1) (81)
=X
t≤TM(xt, yt)xT+1 (82)
In the last equation, we remarked that yT+1is a linear func-
tion of xT+1so that we can write it as a matrix, and this
matrix a sum of matrices that depend linearly on xtandyt.
We can now compare the behavior of this solution, with the
solution found by linear self-attention, by looking in more
detail into the Mmatrices. We first observe that Mand
(x, y)7→yx⊤are bilinear so that it is enough to study their
behavior on the canonical basis (ui)i. We plot those differ-
ent matrices on Figure 7. We observe that each component
is of rank 1 similarly to the self-attention layer solution,
with a peak on the component (i, j)as expected. However,
there is an additional negative peak, of same amplitude as
the positive one, that does not affect the prediction as we are
dealing with one-hot encoded inputs and outputs and a clas-
sification task. One putative reason to explain explain these
observations is that, as the patterns are one-hot encoded,
it is possible to represent them in logTneurons, without
affecting classification performance. This would require
less neurons in the output gating as what the link with atten-
tion would, and can be compensated with the kind of binary
patterns we observe. Alternatively, binary patterns do not
cover all directions from the input space so identification
might become more difficult.
E. Software
We run our experiments using the Jax (Bradbury et al., 2018)
Python framework, using the Flax (Heek et al., 2023) library
19

--- PAGE 20 ---
Gated recurrent neural networks discover attention
y=u1x=u1
x=u2
x=u3
x=u4
x=u5
x=u6
x=u7
x=u8
y=u2
 y=u3
 y=u4
 y=u5
 y=u6
 y=u7
 y=u8
Figure 7. Values taken by the M(x, y)when xandyare equal to the canonical basis. The obtained matrices are all of rank 1.
20

--- PAGE 21 ---
Gated recurrent neural networks discover attention
for neural networks. We base our code base on the Minimal-
LRU (Zucchet et al., 2023a) repository. Data analysis and
visualization were done using Numpy (Harris et al., 2020),
Scikit-learn (Pedregosa et al., 2011) and Matplotlib (Hunter,
2007).
21
