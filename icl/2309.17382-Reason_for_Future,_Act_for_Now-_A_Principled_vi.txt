# 2309.17382.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2309.17382.pdf
# Kích thước tệp: 4080694 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Lý luận cho Tương lai, Hành động cho Hiện tại: Một Khung
Nguyên lý cho Các Tác nhân LLM Tự động với Hiệu quả
Mẫu Có thể Chứng minh
Zhihan Liu∗†Hao Hu∗‡Shenao Zhang∗†
Hongyi Guo†Shuqi Ke§Boyi Liu†Zhaoran Wang†
2 tháng 7, 2024
Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) thể hiện khả năng lý luận ấn tượng,
nhưng việc chuyển đổi lý luận thành hành động trong thế giới thực vẫn là thách thức. Cụ
thể, không rõ làm thế nào để hoàn thành một nhiệm vụ cho trước một cách có thể chứng minh trong
số lượng tương tác tối thiểu với môi trường bên ngoài, ví dụ, thông qua một
cơ chế lý luận nội bộ. Để đạt được điều này, chúng tôi đề xuất khung đầu tiên
với đảm bảo hối tiếc có thể chứng minh để điều phối lý luận và hành động, mà chúng tôi
gọi là "lý luận cho tương lai, hành động cho hiện tại" (RAFA). Cụ thể, chúng tôi thiết kế một mẫu
nhắc cho lý luận học từ bộ đệm bộ nhớ và lập kế hoạch một quỹ đạo tương lai
trên một chân trời dài ("lý luận cho tương lai"). Tại mỗi bước, tác nhân LLM
thực hiện hành động ban đầu của quỹ đạo đã lập kế hoạch ("hành động cho hiện tại"), lưu trữ
phản hồi thu thập được trong bộ đệm bộ nhớ, và gọi lại thói quen lý luận
để lập lại kế hoạch quỹ đạo tương lai từ trạng thái mới. Ý tưởng chính là
coi lý luận trong LLM như học và lập kế hoạch trong các quá trình quyết định Markov
thích ứng Bayesian (MDP). Tương ứng, chúng tôi nhắc LLM với bộ đệm
bộ nhớ để ước tính môi trường chưa biết (học) và tạo ra một
quỹ đạo tối ưu cho nhiều bước tương lai để tối đa hóa một hàm giá trị
(lập kế hoạch). Các thói quen học và lập kế hoạch được thực hiện theo cách "trong
ngữ cảnh" để mô phỏng cập nhật actor-critic cho MDP. Phân tích lý thuyết của chúng tôi
thiết lập một hối tiếc √T, trong khi xác nhận thực nghiệm của chúng tôi thể hiện
hiệu suất thực nghiệm vượt trội. Ở đây, T biểu thị số lượng tương tác trực tuyến.
Trang dự án: https://agentification.github.io/RAFA .

∗Đóng góp ngang nhau.
†Đại học Northwestern. {zhihanliu2027,shenaozhang2028,hongyiguo2025,boyiliu2018 }
@u.northwestern.edu,zhaoranwang@gmail.com
‡Đại học Thanh Hoa. huh22@mails.tsinghua.edu.cn
§Đại học Trung văn Hồng Kông. shuqike@link.cuhk.edu.cn
1arXiv:2309.17382v3  [cs.AI]  24 tháng 6 2024

--- TRANG 2 ---
Mục lục
1 Giới thiệu 4
1.1 Tài liệu tham khảo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2 Ký hiệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2 Nối cầu LLM và RL 8
3 Thuật toán 11
4 Thí nghiệm 15
4.1 Trò chơi 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2 ALFWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.3 BlocksWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.4 Tic-Tac-Toe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5 Phân tích Lý thuyết 19
5.1 Tính tối ưu của Lập kế hoạch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5.2 LLM với Căn chỉnh Hậu nghiệm Thực hiện Trung bình Mô hình Bayesian (BMA) 20
5.3 Ràng buộc Hối tiếc của RAFA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5.4 RAFA với Chiến lược Khám phá Hiệu quả . . . . . . . . . . . . . . . . . . . . . . . 23
5.4.1 Thưởng Lạc quan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
5.4.2 Lấy mẫu Hậu nghiệm . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6 Kết luận 27
A Ký hiệu 35
B Thêm Thuật toán 36
C Chứng minh Chính 38
C.1 Chứng minh Mệnh đề 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
C.2 LLM với Căn chỉnh Hậu nghiệm Thực hiện BMA . . . . . . . . . . . . . . . . . 38
C.3 Tính chất Co lại của Phương sai Hậu nghiệm . . . . . . . . . . . . . . . . . . 39
C.4 Chứng minh Định lý 5.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
C.5 Chứng minh Định lý 5.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
C.6 Chứng minh Định lý 5.10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
C.7 Nới lỏng Giả định 5.3 cho Định lý 5.7 . . . . . . . . . . . . . . . . . . . . . . . 53
D Chứng minh Thiếu trong Phụ lục C 57
D.1 Chứng minh Bổ đề C.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
E Trường hợp Đặc biệt Tuyến tính 60

--- TRANG 3 ---
F Thêm Thí nghiệm 63
F.1 Trò chơi 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
F.2 ALFWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
F.3 BlocksWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
F.4 Tic-Tac-Toe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
G Lời nhắc 69
G.1 Trò chơi 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
G.2 ALFWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
G.3 Blocksworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
G.4 Tic-Tac-Toe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
3

--- TRANG 4 ---
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) thể hiện khả năng lý luận đáng chú ý, điều này mở ra một
hướng mới cho các tác nhân tương tác với thế giới thực một cách tự động. Tuy nhiên, việc biến lý luận
thành hành động vẫn là thách thức. Cụ thể, mặc dù LLM được trang bị kiến thức tiên nghiệm
thu được thông qua tiền huấn luyện, bản chất của nó là không trạng thái và không được định vị trong thế giới
thực, điều này làm cho hành động kết quả không tối ưu. Để bắc cầu khoảng trống lý luận-hành động, chúng
tôi nhằm thiết kế một cơ chế lý luận nội bộ trên nền tảng LLM, tối ưu hóa hành động
một cách lặp đi lặp lại bằng cách kết hợp phản hồi từ môi trường bên ngoài. Cụ thể, chúng tôi tập trung
vào hiệu quả mẫu của các tác nhân LLM tự động trong các nhiệm vụ ra quyết định tương tác,
điều này đóng vai trò quan trọng trong việc áp dụng thực tế của chúng, đặc biệt khi các tương tác tốn kém và
rủi ro. Mục tiêu chính của chúng tôi là cho phép các tác nhân hoàn thành một nhiệm vụ cho trước một cách đảm bảo
thông qua lý luận trong số lượng tương tác tối thiểu với môi trường bên ngoài.

Học tăng cường (RL) là một paradigm được nghiên cứu kỹ lưỡng để cải thiện hành động bằng cách thu
thập phản hồi. Tuy nhiên, để điều chỉnh các kỹ thuật RL hiện có cho các tác nhân LLM tự động,
chúng ta thiếu một ánh xạ nghiêm ngặt giữa RL và LLM, dẫn đến các sự khác biệt khái niệm
khác nhau. Ví dụ, RL hoạt động trong một hệ thống số, nơi phần thưởng và chuyển tiếp
được định nghĩa bởi vô hướng và xác suất. Trong khi đó, đầu vào và đầu ra của LLM
được mô tả bởi token trong một hệ thống ngôn ngữ học. Là một ví dụ khác, LLM được huấn luyện trên một
corpus có mục đích chung và giữ cố định trong suốt quá trình tương tác. Ngược lại, RL
huấn luyện actor và critic thông qua cập nhật tham số trên phản hồi thu thập được một cách lặp đi lặp lại. Vì vậy, có
vẻ không phù hợp khi coi LLM như actor hoặc critic dưới khung RL, mặc dù
tất cả chúng đều được tham số hóa bởi mạng nơ-ron sâu. Hơn nữa, vẫn không rõ lý luận với LLM có nghĩa là gì dưới khung RL, ví dụ, đầu vào và đầu ra của một thói quen lý luận là gì và lý luận nên được phối hợp với hành động như thế nào. Những sự khác biệt khái niệm như vậy ngăn cản chúng ta thiết lập một khung nguyên lý ngoài việc mượn khái niệm "thử và sai" từ RL một cách đơn giản và khiến việc thiết lập đảm bảo lý thuyết trở nên khó khăn.

Để giải quyết những sự khác biệt khái niệm như vậy, chúng tôi chính thức hóa lý luận và hành động với LLM
dưới khung quá trình quyết định Markov thích ứng Bayesian (MDP), nơi biến ẩn
quan tâm là môi trường chưa biết. Điểm khởi đầu là coi toàn bộ lịch sử
của trạng thái (của môi trường bên ngoài), hành động, phần thưởng, và tóm tắt ngôn ngữ của chúng trong
bộ đệm bộ nhớ như trạng thái thông tin của MDP thích ứng Bayesian. Trong suốt quá trình
tương tác, trạng thái thông tin tích lũy một tập hợp phản hồi ngày càng tăng từ
môi trường bên ngoài, được ánh xạ thành một hành động tối ưu hóa tại mỗi bước bởi một cơ
chế lý luận nội bộ. Như được chi tiết dưới đây, chúng tôi xây dựng thói quen lý luận thông qua
hai thói quen con chính, cụ thể là học và lập kế hoạch, được thực hiện bởi LLM với
các lời nhắc được thiết kế đặc biệt. (a) Thói quen con học tạo ra ước tính của môi trường bên ngoài
cho bộ đệm bộ nhớ, nơi LLM được nhắc để suy ra chuyển tiếp
và mô hình phần thưởng (mô hình) hoặc/và hàm giá trị (critic). (b) Thói quen con lập kế hoạch
tạo ra một chính sách tối ưu (actor) hoặc quỹ đạo cho nhiều bước tương lai, tối đa hóa
hàm giá trị (đến một sai số nhất định). Tùy thuộc vào cấu hình cụ thể của
4

--- TRANG 5 ---
̂P,̂r,̂VpriorposteriorQuy tắc Bayes
Môi trườngatLearninĝst+1Planningtt+1t+2t+3s0,a0,s1,r0stk−1,atk−1,stk,rtk−1⋯̂Vt+3st,at,st+1,rt⋯stk,atk,stk+1,rtk̂st+2at+1at+2thêmTác nhânstdepth-3 tree-searchbreadth-2
tree-search"hành động cho hiện tại""lý luận
cho tương lai"st+1t+4
𝙸𝚏−𝚂𝚠𝚒𝚝𝚌𝚑bỏ𝚈𝚘𝚞𝚛𝚊𝚝𝚝𝚎𝚖𝚙𝚝𝚎𝚍𝚊𝚗𝚜𝚠𝚎𝚛𝚜𝚊𝚗𝚍𝚏𝚎𝚎𝚍𝚋𝚊𝚌𝚔𝚜𝚊𝚛𝚎{𝚎𝚡𝚊𝚖𝚙𝚕𝚎𝚜}:rt←r(st,at)st+1∼P(⋅|st,at)phản hồi mới thu thậpbộ đệm bộ nhớHình 1: Minh
họa khung RAFA
("lý luận cho tương
lai, hành động cho
hiện tại").

không gian trạng thái và hành động (liên tục so với rời rạc) và mô hình chuyển tiếp và phần thưởng (ngẫu nhiên so với xác định), thói quen con lập kế hoạch mô phỏng thuật toán lặp giá trị, thuật toán bắn ngẫu nhiên, hoặc thuật toán tìm kiếm cây Monte-Carlo.

Mặc dù LLM giữ cố định trong suốt quá trình tương tác, chúng ta có thể giảm bất định ước tính của chúng bằng cách nhắc tập hợp phản hồi ngày càng tăng từ môi trường bên ngoài như ngữ cảnh, điều này được xác minh cả về mặt lý thuyết và thực nghiệm trong bài báo này. Từ quan điểm của MDP thích ứng Bayesian, LLM có thể được coi như một số hàm của hậu nghiệm của môi trường (ví dụ, trung bình mô hình Bayesian (Wasserman, 2000)), do đó bất định ước tính giảm với thông tin tăng thông qua tương tác. Đối với một số nhiệm vụ, chúng tôi chứng minh rằng LLM có thể đưa ra dự đoán chính xác hơn khi được nhắc với nhiều dữ liệu hơn như ngữ cảnh. Do đó, LLM có thể đóng vai trò tương tự như bộ ước tính mô hình trong thiết kế thuật toán RL trực tuyến cho tương tác. Chúng tôi cải thiện độ chính xác của LLM bằng cách đơn giản thêm phản hồi mới vào bộ đệm bộ nhớ như ngữ cảnh, thay vì thực hiện cập nhật tham số rõ ràng (như gradient descent) trên mạng nơ-ron sâu như trong các phương pháp RL hiện có.

Chúng tôi kết luận đóng góp của chúng tôi trong bài báo này từ ba quan điểm. (a) Chúng tôi thiết lập tương ứng LLM-RL và thiết kế một khung nguyên lý RAFA để điều phối lý luận và hành động của LLM. (b) Xác nhận thực nghiệm của chúng tôi cho thấy RAFA vượt trội hơn các khung hiện có khác nhau trong các nhiệm vụ ra quyết định tương tác, bao gồm ALFWorld, BlocksWorld, Trò chơi 24, và một benchmark mới dựa trên Tic-Tac-Toe. (c) Phân tích lý thuyết của chúng tôi chứng minh rằng RAFA đạt được hối tiếc √T, giải thích tại sao RAFA thể hiện hiệu suất thực nghiệm mạnh mẽ. Ở đây, T biểu thị số lượng tương tác trực tuyến. Chúng tôi cũng cung cấp hai biến thể hiệu quả có thể chứng minh của RAFA để thực hiện khám phá hiệu quả cho các nhiệm vụ phức tạp hơn.

1.1 Tài liệu tham khảo
Lý luận với LLM. Chúng tôi xây dựng trên một dòng công việc gần đây phát triển các sơ đồ nhắc khác nhau để cải thiện hiệu suất lý luận của LLM. "Chuỗi suy nghĩ" ("CoT") (Wei et al., 2022) phân tách một vấn đề thách thức thành nhiều giai đoạn lý luận và hướng dẫn LLM giải quyết từng giai đoạn một. Như các tổng quát hóa, "cây suy nghĩ" (Yao et al., 2023a), "đồ thị suy nghĩ" (Yao et al., 2023b), "thuật toán của suy nghĩ" (Sel et al., 2023), và "lý luận tích lũy" (Zhang et al., 2023a) cung cấp các sơ đồ tìm kiếm đồ thị khác nhau để hướng dẫn LLM. Xem thêm
5

--- TRANG 6 ---
Wang et al. (2022a); Creswell et al. (2022); Creswell và Shanahan (2022); Guo et al. (2024); Zhang et al. (2024). Ngoài ra, "lý luận thông qua lập kế hoạch" ("RAP") (Hao et al., 2023) mô phỏng thuật toán tìm kiếm cây Monte-Carlo (MCTS) để giảm độ phức tạp tìm kiếm. Pouplin et al. (2024) cải thiện quá trình lý luận LLM với MCTS và công thức hóa quá trình lý luận như một MDP. Sun et al. (2023a) sử dụng RL nghịch đảo ngoại tuyến để tối ưu hóa các lời nhắc cho các bài toán số học. Đối với các tác nhân LLM hiện thân, Huang et al. (2022a) đề xuất phân tách một nhiệm vụ phức tạp thành nhiều bước có thể thực thi. Hầu hết trong số chúng tập trung vào các nhiệm vụ lý luận chung, ví dụ, giải một câu đố toán học hoặc logic, nơi LLM tạo ra một dấu vết chi tiết (quỹ đạo) của các luận cứ thông qua một cơ chế nội bộ để đạt được một câu trả lời cuối cùng. Ở đây, LLM đóng vai trò tương tự như thói quen con lập kế hoạch trong RAFA. Ngược lại, chúng tôi tập trung vào các nhiệm vụ ra quyết định tương tác, nơi các tác nhân LLM tự động thu thập phản hồi từ môi trường bên ngoài để tối ưu hóa hành động một cách lặp đi lặp lại. Cụ thể, chúng tôi nhằm hoàn thành một nhiệm vụ cho trước trong số lượng tương tác tối thiểu với môi trường bên ngoài. Để đạt được điều này, việc vận hành ba mô-đun xen kẽ, cụ thể là học, lập kế hoạch, và hành động, trong một vòng lặp khép kín là thiết yếu. Mặc dù có thể kết hợp các sơ đồ tìm kiếm đồ thị hoặc MCTS hiện có như thói quen con lập kế hoạch để tạo quỹ đạo, đóng góp cốt lõi của chúng tôi là một khung nguyên lý thực thi một tập con được chọn của quỹ đạo đã lập kế hoạch để thu thập phản hồi ("hành động cho hiện tại") và lập lại kế hoạch một quỹ đạo cải thiện từ trạng thái mới bằng cách học từ phản hồi ("lý luận cho tương lai"). Từ quan điểm RL, các sơ đồ tìm kiếm đồ thị hoặc MCTS hiện có tương tự như một phương pháp vòng lặp mở, ví dụ, lập kế hoạch chuyển động hoặc tối ưu hóa quỹ đạo (Betts, 1998), không liên quan đến tương tác với môi trường bên ngoài. Để tích hợp chúng vào một cách tiếp cận vòng lặp khép kín, ví dụ, điều khiển dự đoán mô hình (Rawlings, 2000), người ta phải chỉ định cách hành động cho quỹ đạo đã lập kế hoạch và khi nào gọi lại thói quen lý luận (học và lập kế hoạch), đây là kỹ thuật chính của RAFA. Một dòng công việc gần đây khác giải quyết các nhiệm vụ phức tạp hơn bằng cách cho phép LLM truy cập các mô-đun bổ sung khác nhau, ví dụ, công cụ, chương trình, và các thuật toán học khác (Ahn et al., 2022; Shen et al., 2023; Lu et al., 2023; Liu et al., 2023a; Cai et al., 2023), hoặc bằng cách tinh chỉnh LLM trên phản hồi (Zelikman et al., 2022; Li et al., 2022; Paul et al., 2023; Sun, 2023).

Hành động (và Lý luận) với LLM. Chúng tôi xây dựng trên một dòng công việc gần đây phát triển các khung vòng lặp khép kín khác nhau để tương tác với môi trường bên ngoài. "Độc thoại nội tâm" (Huang et al., 2022b) và "ReAct" (Yao et al., 2022) kết hợp lý luận và hành động để tinh chỉnh lẫn nhau lần đầu tiên. So với đó, RAFA cung cấp một lịch trình cụ thể để điều phối lý luận và hành động (như đã thảo luận ở trên). Như các tổng quát hóa, "Reflexion" (Shinn et al., 2023) cho phép các tác nhân LLM tự động sửa đổi hành động hiện tại của một quỹ đạo được tạo trước bằng cách học từ phản hồi, đặc biệt khi chúng mắc lỗi. Xem thêm Kim et al. (2023). Tuy nhiên, việc thực hiện sửa đổi cục bộ đối với quỹ đạo được tạo trước là cận thị vì nó không xem xét hậu quả dài hạn của hành động. Kết quả là, chính sách thu được có thể bị mắc kẹt bởi một tối ưu cục bộ. Từ quan điểm RL, "Reflexion" (Shinn et al., 2023) là một phiên bản đơn giản hóa quá mức của RAFA, nơi thói quen con lập kế hoạch sửa đổi hành động hiện tại để tối đa hóa hàm phần thưởng ("lý luận cho hiện tại") thay vì lập kế hoạch nhiều
6

--- TRANG 7 ---
Cơ chế Vòng lặp Khép kín Không Cập nhật Tham số Đảm bảo Lý thuyết
RAFA ✓ ✓
RL Sâu Dựa trên Mô hình ✗ ✓
Điều khiển Dự đoán Mô hình ✗ ✓
Lấy mẫu Thompson ✗ ✓
"React", "Reflexion", và "Adaplanner" ✓ ✗
Bảng 1: So sánh giữa RAFA và các cơ chế khác.

bước tương lai để tối đa hóa hàm giá trị ("lý luận cho tương lai"), đo lường phần thưởng tích lũy tương lai dự kiến. Để khắc phục vấn đề này, "AdaPlanner" (Sun et al., 2023b) tái tạo toàn bộ quỹ đạo tại mỗi bước, mang lại cải thiện toàn cục. Xem thêm Wang et al. (2023b). Tuy nhiên, thói quen lý luận của "AdaPlanner" yêu cầu một tập hợp chương trình được chế tác thủ công để từ chối các quỹ đạo ứng viên không tối ưu. Không có kiến thức lĩnh vực của nhiệm vụ hiện tại, quỹ đạo được tái tạo không nhất thiết phải tối ưu, tức là, tối đa hóa hàm giá trị (đến một sai số nhất định). Ngược lại, thói quen lý luận của RAFA được thiết kế theo cách tiếp cận nguyên lý trong RL. Cụ thể, thói quen con học suy ra mô hình chuyển tiếp và phần thưởng (mô hình) hoặc/và hàm giá trị (critic), trong khi thói quen con lập kế hoạch mô phỏng thuật toán lặp giá trị, thuật toán bắn ngẫu nhiên, hoặc thuật toán MCTS, không có thuật toán nào sử dụng bất kỳ kiến thức lĩnh vực nào. RAFA cũng đạt được đảm bảo hiệu quả mẫu có thể chứng minh lần đầu tiên và vượt trội hơn những khung hiện có về mặt thực nghiệm.

Mô hình Ngôn ngữ Lớn (LLM) và Học Trong Ngữ cảnh (ICL). LLM (Radford et al., 2019; Brown et al., 2020; Hoffmann et al., 2022; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023) thể hiện khả năng lý luận đáng chú ý. Một khía cạnh quan trọng của lý luận là khả năng ICL (Liang et al., 2022; Razeghi et al., 2022; Shin et al., 2022; Olsson et al., 2022; Akyürek et al., 2022; Kirsch et al., 2022; Garg et al., 2022; Von Oswald et al., 2023; Li et al., 2023; Abernethy et al., 2023), cho phép LLM giải quyết một loạt rộng các nhiệm vụ chỉ với một vài ví dụ trong ngữ cảnh thay vì tinh chỉnh tham số trên một tập dữ liệu cụ thể. Chúng tôi tập trung vào khai thác khả năng ICL của LLM để tối ưu hóa hành động trong thế giới thực, điều này quan trọng đối với các tác nhân LLM tự động. Cụ thể, chúng tôi xây dựng trên một dòng công việc gần đây (Xie et al., 2021; Zhang et al., 2022, 2023b; Wang et al., 2023a; Wies et al., 2023; Jiang, 2023; Lee et al., 2023) quy khả năng ICL cho suy luận Bayesian ẩn, tức là, một cơ chế ẩn cho phép LLM suy ra một khái niệm ẩn từ những ví dụ trong ngữ cảnh, được xác minh cả về mặt lý thuyết và thực nghiệm. Trong RAFA, khái niệm ẩn là mô hình chuyển tiếp và phần thưởng (mô hình) của môi trường chưa biết hoặc/và hàm giá trị (critic), được suy ra từ bộ đệm bộ nhớ trong thói quen con học. Khẳng định 2.1 cũng có thể được coi là kết quả của khả năng ICL.

Học Tăng cường (RL) dưới Khung Bayesian. Chúng tôi xây dựng trên một dòng công việc gần đây về chân trời vô hạn (Abbasi-Yadkori và Szepesváti, 2015; Dong et al., 2019;
7

--- TRANG 8 ---
Wei et al., 2020; Zhou et al., 2021a,b; Chen et al., 2022; Chua et al., 2018; Hafner et al., 2019; Sekar et al., 2020) và cài đặt Bayesian (Strens, 2000; Osband et al., 2013; Russo và Van Roy, 2014a,b, 2016; Lu và Van Roy, 2019) của RL, bao gồm RL sâu dựa trên mô hình (Janner et al., 2019; Liu et al., 2023b; Wang et al., 2022b; Liu et al., 2024), điều khiển dự đoán mô hình (Morari và Lee, 1999), và lấy mẫu Thompson (Russo và Van Roy, 2014b). Cài đặt chân trời vô hạn cho phép RAFA tương tác với môi trường bên ngoài liên tục mà không đặt lại về trạng thái ban đầu, trong khi cài đặt Bayesian cho phép chúng ta kết nối RAFA với BMA và thiết lập đảm bảo lý thuyết. RL hoạt động trong một hệ thống số, nơi phần thưởng và chuyển tiếp được định nghĩa bởi vô hướng và xác suất, và huấn luyện actor và critic trên phản hồi thu thập được một cách lặp đi lặp lại. Chúng tôi tập trung vào mô phỏng cập nhật actor-mô hình hoặc actor-critic trong RL thông qua một cơ chế lý luận nội bộ trên nền tảng LLM, cho phép dữ liệu và hành động là token trong một hệ thống ngôn ngữ học trong khi bỏ qua việc cập nhật tham số rõ ràng trong RL dựa trên mô hình (Chua et al., 2018; Hafner et al., 2019; Sekar et al., 2020; Liu et al., 2022b; Zhong et al., 2022; Zheng et al., 2022; Liu et al., 2022a). Cụ thể, các thói quen con học và lập kế hoạch của RAFA mô phỏng cập nhật hậu nghiệm và các thuật toán lập kế hoạch khác nhau trong RL. Hơn nữa, RAFA điều phối lý luận (học và lập kế hoạch) và hành động theo cách tiếp cận nguyên lý trong RL, tức là, (tái) lập kế hoạch một quỹ đạo tương lai trên một chân trời dài ("lý luận cho tương lai") tại trạng thái mới và thực hiện hành động ban đầu của quỹ đạo đã lập kế hoạch ("hành động cho hiện tại"). Kết quả là, RAFA kế thừa đảm bảo hiệu quả mẫu có thể chứng minh từ RL. Chúng tôi tóm tắt so sánh giữa RAFA và các cơ chế vòng lặp khép kín khác trong Bảng 1.

1.2 Ký hiệu
Chúng tôi cung cấp một bảng ký hiệu trong Phụ lục A.

2 Nối cầu LLM và RL
Giao thức Tương tác. Chúng tôi sử dụng các quá trình quyết định Markov thích ứng Bayesian (MDP) (Ghavamzadeh et al., 2015) để mô hình hóa cách các tác nhân LLM tự động tương tác với môi trường bên ngoài. Chúng tôi xem xét một MDP chân trời vô hạn M= (S,A, P, r, ρ, γ, P0), trong đó S là không gian trạng thái, A là không gian hành động, P:S × A 7→ ∆(S) là nhân chuyển tiếp, r:S × A 7→ R là hàm phần thưởng, ρ là phân phối ban đầu của trạng thái, γ∈(0,1) là hệ số giảm giá, và P0 là phân phối tiên nghiệm của nhân chuyển tiếp và hàm phần thưởng. Ở đây, P cho xác suất phân phối của trạng thái tiếp theo cho trạng thái hiện tại và hành động, trong khi r được giả định là xác định mà không mất tính tổng quát. Để đơn giản ký hiệu, chúng tôi tham số hóa P và r bởi một tham số chung θ∈Θ và ký hiệu chúng là Pθ và rθ. Ở đầu tương tác, tham số sinh dữ liệu θ⋆ được lấy mẫu từ tiên nghiệm P0. Tại bước thứ t trong quá trình tương tác, tác nhân LLM nhận một trạng thái st∈ S, thực hiện một hành động at∈ A theo chính sách hiện tại πt:S 7→ A, và nhận một phần thưởng rt=rθ⋆(st, at). Tiếp theo, môi trường bên ngoài chuyển tiếp đến trạng thái tiếp theo st+1∼Pθ⋆(·|st, at), trong khi tác nhân LLM tính toán chính sách cập nhật πt+1 thông qua một cơ chế lý luận nội bộ (như đã thảo luận dưới đây). Lưu ý rằng S và A được biểu diễn bởi token trong một hệ thống ngôn ngữ học. Ở đây, π∈Π được giả định là xác định mà không mất tính tổng quát, trong đó Π là tập hợp khả thi của các chính sách.
8

--- TRANG 9 ---
Hàm Giá trị. Đối với một chính sách π và một tham số θ của mô hình chuyển tiếp và phần thưởng, chúng tôi định nghĩa các hàm giá trị trạng thái và giá trị hành động như

Vπ_θ(s) =E[h∞∑_t=0γ^t rθ(st, at)|s0=s],

Qπ_θ(s, a) =E[h∞∑_t=0γ^t rθ(st, at)|s0=s, a0=a], (2.1)

trong đó E được lấy đối với at=π(st) và st+1∼Pθ(·|st, at) cho tất cả t≥0. Nói cách khác, Vπ_θ (và Qπ_θ) cho phần thưởng tích lũy tương lai dự kiến từ trạng thái hiện tại s (và hành động a).

Chúng tôi định nghĩa chính sách tối ưu π⋆_θ đối với một tham số θ cho trước như π⋆_θ= argmaxaA Q⋆_θ, trong đó Q⋆_θ là điểm cố định của phương trình tối ưu Bellman sau,

Q⋆_θ(s, a) = (BθV⋆_θ) (s, a),
V⋆_θ(s) = max_a∈A Q⋆_θ(s, a), (2.2)

trong đó Q⋆_θ và V⋆_θ là các giải pháp điểm cố định. Ở đây, chúng tôi định nghĩa (BθV)(s, a) =rθ(s, a) +γ·(Pθ⋆V)(s, a) và (PθV)(s, a) =Es′∼Pθ(·|s,a)[V(s′)] cho bất kỳ hàm giá trị V nào. Xem Sutton và Barto (2018) cho các đảm bảo tồn tại và duy nhất cho Q⋆_θ, V⋆_θ, và π⋆_θ.

Hậu nghiệm, Entropy, và Tăng Thông tin. Bởi quy tắc Bayes, hậu nghiệm của θ⋆ cho bất kỳ tập dữ liệu trong ngữ cảnh D nào là

Ppost(θ|D)∝P0(θ)L(D|θ), (2.3)

trong đó chúng tôi ký hiệu bởi L(D|θ) khả năng của D có điều kiện trên θ. Chúng tôi định nghĩa biến ngẫu nhiên ξ(s,a) như cặp của trạng thái tiếp theo và phần thưởng hiện tại (s′, r) cho cặp trạng thái-hành động truy vấn (s, a). Cho bất kỳ tập dữ liệu trong ngữ cảnh D và cặp trạng thái-hành động truy vấn (s, a) nào, hậu nghiệm của ξ(s,a) có thể được chỉ định là

Ppost(ξ(s,a)|D, s, a) =Eθ∼Ppost(·|D)[Pθ(s′|s, a)·1(r=rθ(s, a))], (2.4)

trong đó chúng tôi sử dụng quy tắc Bayes và thực tế rằng cặp trạng thái-hành động truy vấn (s, a) là độc lập có điều kiện của θ⋆ cho D. Để đặc trưng bất định của θ⋆ có điều kiện trên D, chúng tôi định nghĩa entropy hậu nghiệm H(θ|D) là

H(θ|D) =Eθ∼Ppost(·|D)[-log ppost(θ|D)], (2.5)

9

--- TRANG 10 ---
trong đó ppost là hàm khối lượng xác suất (hoặc mật độ) của Ppost. Entropy hậu nghiệm cao H(θ|D) có nghĩa là bất định cao của θ⋆, điều này cho thấy rằng khó khăn cho tác nhân đưa ra dự đoán chính xác cho D. Chúng tôi cũng định nghĩa tăng thông tin I(θ;ξ|D) là H(θ|D)− H(θ|D, ξ), đặc trưng cho lượng thông tin mà ξ(s,a) mang lại để giảm bất định của θ⋆ có điều kiện trên D.

Hiệu quả Mẫu. Như thước đo hiệu suất, chúng tôi định nghĩa hối tiếc Bayesian sau T bước tương tác,

R(T) =E[∑T−1_t=0 Vπ⋆_θ⋆(st)−Vπt_θ⋆(st)], (2.6)

trong đó π⋆=π⋆_θ⋆, E được lấy đối với phân phối tiên nghiệm P0 của θ⋆, kết quả ngẫu nhiên của st, và cập nhật lặp của πt, liên quan đến trạng thái, hành động, và phần thưởng cho đến bước thứ t, tức là, toàn bộ lịch sử Dt={(si, ai, si+1, ri)}t−1_i=0. Chúng tôi nhằm thiết kế một tác nhân hiệu quả mẫu thỏa mãn R(T) =o(T), tức là, hối tiếc Bayesian là dưới tuyến tính trong tổng số tương tác T.

Ý nghĩa của Lý luận và Vai trò của LLM. Để nối cầu cơ chế LLM với thuật toán RL trực tuyến, chúng tôi khẳng định rằng LLM có thể đóng vai trò tương tự như bộ ước tính mô hình trong thiết kế thuật toán RL trực tuyến cho tương tác, đây là một khía cạnh của khả năng Học Trong Ngữ cảnh (ICL) của LLM.

Khẳng định 2.1. LLM cung cấp ước tính chính xác hơn cho môi trường với nhiều phản hồi hơn từ tương tác trực tuyến.

Trong Mệnh đề 5.4 trong Phần 5, chúng tôi chứng minh rằng LLM với căn chỉnh hậu nghiệm thực hiện trung bình mô hình Bayesian (BMA). Kết quả lý thuyết này hỗ trợ Khẳng định 2.1, vì bất định ước tính của BMA giảm cho nhiều phản hồi hơn từ tương tác với môi trường (Wasserman, 2000). Chúng tôi cũng cung cấp bằng chứng thực nghiệm trên ba nhiệm vụ cho Khẳng định 2.1 như sau.

(a) Information Bandit. Mục tiêu của thí nghiệm bandit 100 cánh tay của chúng tôi là tìm cánh tay có phần thưởng cao nhất. Có một cánh tay thông tin có phần thưởng là chỉ số của cánh tay tốt nhất. Chúng tôi nhắc LLM (gpt-4) kéo cánh tay bằng cách cung cấp cho nó dữ liệu lịch sử của một số trường hợp bandit chia sẻ cùng một cánh tay thông tin. Có thể quan sát từ hình bên phải rằng LLM có thể học cánh tay tốt nhất chỉ với 6 ví dụ, và do đó là một bộ ước tính phần thưởng hiệu quả.

(b) Học Khái niệm. Chúng tôi đánh giá LLM (Llama 2-7B) trong ba nhiệm vụ (Todd et al., 2023) với các khái niệm ẩn: (1) Antonym: Tạo từ có nghĩa trái ngược cho một từ đầu vào; (2) Country-Capital: Tạo thành phố thủ đô của một quốc gia cho trước; và (3) Present-Past: Tạo biến thể quá khứ của động từ cho một động từ ở thì hiện tại. Chúng tôi quan sát rằng với nhiều ví dụ trong ngữ cảnh hơn được cung cấp cho LLM, độ chính xác của trường hợp kiểm tra tăng một cách đơn điệu, cho thấy rằng các khái niệm ẩn của nhiệm vụ được học.
10

--- TRANG 11 ---
2 6 10 14 18
Số Ví dụ0.00.20.40.60.81.0Độ chính xác
bandit(a) Information bandit.

0 1 5 10
Số Ví dụ0.00.20.40.60.81.0Độ chính xác
antonym
country-capital
present-past (b) Học khái niệm.

0 60 120 180 240 300
Số Token10−510−410−310−210−1100CNLL x·sin(x)
x
x2
exp(x2/2)·sin(x)
sin(x) (c) Dự đoán giá trị hàm.

Hình 2: Bằng chứng thực nghiệm cho Khẳng định 2.1 trên các nhiệm vụ khác nhau. LLM thể hiện khả năng dự đoán được cải thiện khi số lượng mẫu trong ngữ cảnh tăng.

(c) Dự đoán Giá trị Hàm. Mục tiêu của thí nghiệm này là để LLM (gpt-3) dự đoán các giá trị của một hàm trên các điểm dữ liệu chưa thấy cho các giá trị trên các điểm với khoảng cách cố định. Theo Gruver et al. (2023), chúng tôi báo cáo khả năng log âm tích lũy t-interval CNLL = −∑t_i log P(vi|prompti−1), trong đó vi là giá trị của hàm tại điểm dữ liệu i. Có thể quan sát rằng LLM là những nhà dự báo chuỗi thời gian tốt.

Dưới Khẳng định 2.1, chúng tôi thiết lập tương ứng giữa LLM và RL bằng cách sử dụng LLM như bộ ước tính mô hình trong thuật toán RL, điều này mở ra cánh cửa để tạo ra một thuật toán thực tế kết hợp điểm mạnh của cả LLM và RL. LLM xuất sắc trong độ chính xác với phản hồi tối thiểu, điều này cải thiện hiệu quả mẫu. LLM cũng có thể tinh chỉnh ước tính bằng cách sử dụng phản hồi mới như lời nhắc, điều này tránh được cập nhật tham số rõ ràng. Thuật toán RL hưởng lợi từ tương tác trực tuyến để cải thiện ước tính và chính sách và có đảm bảo lý thuyết với thuật toán lập kế hoạch tối ưu như lặp giá trị. Tương ứng LLM-RL này truyền cảm hứng cho chúng tôi giới thiệu một khung mới trong phần tiếp theo, nhằm điều phối lý luận (học và lập kế hoạch) và hành động của LLM.

3 Thuật toán
Kiến trúc của RAFA. Bằng cách tận dụng tương ứng LLM-RL trong Phần 2, chúng tôi cung cấp một khung nguyên lý để điều phối lý luận và hành động, cụ thể là "lý luận cho tương lai, hành động cho hiện tại" (RAFA), trong Thuật toán 1 và 2. Tại bước thứ t của Thuật toán 1, tác nhân LLM gọi thói quen lý luận, học từ bộ đệm bộ nhớ và lập kế hoạch một quỹ đạo tương lai trên một chân trời dài ("lý luận cho tương lai" trong Dòng 6), thực hiện hành động ban đầu của quỹ đạo đã lập kế hoạch ("hành động cho hiện tại" trong Dòng 7), và lưu trữ phản hồi thu thập được (trạng thái, hành động, và phần thưởng) trong bộ đệm bộ nhớ (Dòng 8). Khi chuyển tiếp trạng thái của môi trường bên ngoài, tác nhân LLM gọi lại thói quen lý luận để lập lại kế hoạch một quỹ đạo tương lai khác từ trạng thái mới (Dòng 6 sau Dòng 9). Để đảm bảo tính ổn định của học và lập kế hoạch, chúng tôi áp đặt điều kiện chuyển đổi (Dòng 10) để quyết định có nên kết hợp phần lịch sử mới nhất, tức là, hiệu tập Dt− D tk, vào trạng thái thông tin, được sử dụng trong thói quen lý luận như ngữ cảnh. Nói cách khác, thói quen lý luận sử dụng cùng một
11

--- TRANG 12 ---
Thuật toán 1 Lý luận cho tương lai, hành động cho hiện tại (RAFA): Phiên bản LLM.
1:đầu vào : Một người học-lập kế hoạch LLM LLM-LR-PL, nhằm tạo ra một quỹ đạo tối ưu cho một trạng thái ban đầu và trả về hành động ban đầu (ví dụ, Thuật toán 2), và một điều kiện chuyển đổi If-Switch.
2:khởi tạo : Lấy mẫu trạng thái ban đầu s0∼ρ, đặt t= 0, và khởi tạo bộ đệm bộ nhớ D0=∅.
3:cho k= 0,1, . . . ,thực hiện
4: Đặt tk←t.
5:lặp lại
6: Học và lập kế hoạch cho bộ nhớ Dtk để có hành động at←LLM-LR-PL (Dtk, st). ("lý luận cho tương lai")
7: Thực hiện hành động at để nhận phần thưởng rt và trạng thái st+1 từ môi trường. ("hành động cho hiện tại")
8: Cập nhật bộ nhớ Dt+1← D t∪ {(st, at, st+1, rt)}.
9: Đặt t←t+ 1.
10: cho đến khi If-Switch (Dt) là True. (điều kiện chuyển đổi được thỏa mãn)
11:kết thúc cho

Mô hình LLM Elite LLM
MôitrườngLý luận
Hành độngĐầu vào: 2 5 8 112*5
2+511-52*5=10
(còn lại: 8 10 11)
2+5=7
(còn lại: 7 8 11)11-5=6
(còn lại: 2 6 8)
11-5
8/6
2*2
8-2
MôiTrường2*211-5=6
(còn lại: 2 6 8)
11-5=6
(còn lại: 2 6 8)
8/6
8-68/28/6=1.33
(còn lại: 2 1.33)
8-6=2
(còn lại: 2 2)8/2=4
(còn lại: 4 6)
6-4
4*6
4+6
Có khả năngCritic LLM
Không thể
Không thể
Tôi đã sử dụng 2 lặp lại. 2*2=4:
Không thểCó khả năng
Không thể
Không thể
8/6=1.33
(còn lại: 2 1.33)
2*2=4
(còn lại: 4 6)
8-2=6
(còn lại: 6 6)
6/4=2
(còn lại: 2)
4*6=24
(còn lại: 24)
4+6=10
(còn lại: 10)
8/24-6=24
(còn lại: 24)
4*6
8/2=4
(còn lại: 4 6)
MôiTrườngMôiTrường
Lý luậnHành động

Hình 3: RAFA cho Trò chơi 24. Các hành động được đề xuất (đường nét đứt) và được chọn (xanh lá). Các ảo giác rằng cùng một số có thể được sử dụng lại được giảm thiểu thông qua tương tác.

lịch sử Dtk cho tất cả tk≤t < t k+1 cho đến khi chuyển đổi thứ (k+ 1) tại bước (tk+1−1), điều này đảm bảo rằng phân phối hậu nghiệm và hành động tối ưu hóa hoặc chính sách tương ứng được cập nhật một cách bảo thủ. Chúng tôi chỉ định điều kiện chuyển đổi trong Phần 4 và 5.

"Lý luận cho Tương lai" (Dòng 6 trong Thuật toán 1 và Dòng 3-11 trong Thuật toán 2).
Như được chi tiết dưới đây, thói quen lý luận sáng tác các thói quen con học và lập kế hoạch để ánh xạ toàn bộ lịch sử Dtk (cho đến bước tk) thành một hành động tối ưu hóa at. Lưu ý rằng thói quen lý luận không tương tác với môi trường bên ngoài trong suốt các thói quen con học và lập kế hoạch.

•Thói quen con học (Dòng 3-4 trong Thuật toán 2) ánh xạ bộ đệm bộ nhớ Dtk thành một nhân chuyển tiếp (Model) và một hàm giá trị (Critic), được sử dụng trong thói quen con lập kế hoạch. Trong thực tế, chúng tôi nhắc LLM để tạo ra ước tính của môi trường bên ngoài dựa trên bộ đệm bộ nhớ. Ở đây, ước tính được thực hiện bởi hai LLM: Model và Critic, ước tính các đối tác thực tế của chúng liên quan đến tham số sinh dữ liệu. Dưới Khẳng định 2.1, thói quen con học mang lại các phiên bản chính xác hơn của Model và Critic khi chúng tôi nhắc chúng với tập hợp phản hồi ngày càng tăng từ môi trường bên ngoài. Kết quả là, thói quen con lập kế hoạch có thể sử dụng chúng để đánh giá kết quả dài hạn của hành động với độ chính xác cao hơn. Tùy thuộc vào việc chúng ta mô phỏng cách tiếp cận dựa trên mô hình hay không có mô hình của RL, chúng ta có thể chọn mô phỏng Model hoặc Critic riêng lẻ. So với thói quen con học trong RL, chúng tôi thay thế xấp xỉ hàm tham số hóa (thường là mạng nơ-ron sâu) bằng LLM và sử dụng cách "trong ngữ cảnh" để cập nhật LLM, điều này loại bỏ nhu cầu cập nhật tham số rõ ràng. Bởi vì LLM được tiền huấn luyện và trải qua tinh chỉnh có giám sát, chúng cung cấp ước tính tốt hơn nhiều so với việc học từ đầu, dẫn đến cải thiện hiệu quả mẫu cho tương tác trực tuyến.

Thuật toán 2 Người học-lập kế hoạch LLM (LLM-LR-PL): Một ví dụ tìm kiếm cây. (trường hợp xác định)
1:đầu vào : Bộ đệm bộ nhớ D, trạng thái ban đầu s, độ rộng tìm kiếm B, và độ sâu tìm kiếm U.
2:khởi tạo : Khởi tạo mảng trạng thái S0← {s} và mảng hành động A0←∅.
——————————————— (thói quen con học) ——————————————–
3:Đặt Model như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để tạo trạng thái tiếp theo.
4:Đặt Critic như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để ước tính hàm giá trị.
——————————————— (thói quen con lập kế hoạch) ——————————————–
5:Đặt Elite như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để tạo nhiều hành động ứng viên.
6:cho u= 0, . . . , U thực hiện
7: Đối với mỗi trạng thái hiện tại trong Su, gọi Elite để tạo B hành động ứng viên và lưu trữ chúng trong Au.
8: Đối với mỗi hành động ứng viên trong Au, gọi Model để tạo trạng thái tiếp theo và lưu trữ nó trong Su+1.
9:kết thúc cho
10:Đối với tất cả các rollout kết quả trong S0× A 0× ··· × S U× A U, gọi Critic để đánh giá phần thưởng tích lũy tương lai dự kiến và chọn rollout tốt nhất (s†0, a†0, . . . , s†U, a†U), trong đó s†0=s.
11:đầu ra : Hành động ban đầu a†0 của rollout được chọn.

•Thói quen con lập kế hoạch (Dòng 5-11 trong Thuật toán 2) ánh xạ Model và Critic thành một quỹ đạo tương lai (s†0, a†0, . . . , s†U, a†U), trong đó s†0 là trạng thái hiện tại st và a†0 được thực thi trong môi trường bên ngoài như hành động hiện tại at trong giai đoạn hành động. Trực quan, chúng tôi nhắc LLM để tạo ra một chính sách tối ưu (actor) cho nhiều bước tương lai, tối đa hóa hàm giá trị (Critic). Từ quan điểm RL (Phần 2 và 5), thói quen con lập kế hoạch giải gần đúng phương trình Bellman (Sutton và Barto, 2018), nơi chúng ta giải chính sách tối ưu (hoặc hành động tương ứng) cho nhân chuyển tiếp và hàm phần thưởng ước tính (hoặc critic) bởi LLM. Như hai thể hiện LLM từ thói quen con học, Model và Critic thực hiện nhân chuyển tiếp ước tính và hàm giá trị ước tính. Do đó, chúng ta có thể mô phỏng một số lượng quỹ đạo cho trước với Model, đánh giá chúng với Critic, và thu được một chính sách cải thiện, điều này đạt được bằng các lời nhắc được thiết kế đặc biệt thay vì một thuật toán số. Bằng cách tối đa hóa phần thưởng tích lũy tương lai dự kiến (thay vì phần thưởng tức thì), thói quen con lập kế hoạch trả về một hành động tối ưu hóa cải thiện kết quả dài hạn. Có hai nguồn lỗi ảnh hưởng đến thói quen con lập kế hoạch, cụ thể là bất định hậu nghiệm, được kế thừa từ Model và Critic do kích thước hữu hạn của Dtk, và tính không tối ưu của lập kế hoạch, được gây ra bởi khả năng tính toán hạn chế, ví dụ, độ rộng và độ sâu có giới hạn của tìm kiếm cây (Dòng 6-9 trong Thuật toán 2). Tùy thuộc vào cấu hình cụ thể của không gian trạng thái và hành động (liên tục so với rời rạc) và mô hình chuyển tiếp và phần thưởng (ngẫu nhiên so với xác định), chúng ta có thể chọn mô phỏng thuật toán lặp giá trị, thuật toán bắn ngẫu nhiên, hoặc thuật toán tìm kiếm cây Monte-Carlo. Tất cả chúng đều cho phép RAFA đạt được đảm bảo hiệu quả mẫu có thể chứng minh miễn là chúng thỏa mãn một yêu cầu cụ thể về tính tối ưu (Định nghĩa 5.1). Để minh họa, chúng tôi mô phỏng thuật toán tìm kiếm cây và trì hoãn biến thể ngẫu nhiên của nó đến Phụ lục B.
13

--- TRANG 14 ---
"Hành động cho Hiện tại" (Dòng 7-10 trong Thuật toán 1). Tại trạng thái hiện tại st, tác nhân LLM thực thi hành động tối ưu hóa at trong môi trường bên ngoài, được thu được từ thói quen lý luận. Cụ thể, chúng tôi thực hiện hành động ban đầu a†0 của quỹ đạo đã lập kế hoạch (s†0, a†0, . . . , s†U, a†U), trong đó s†0=st và a†0=at, và loại bỏ tập con còn lại. Tại trạng thái tiếp theo st+1, tác nhân LLM lập lại kế hoạch một quỹ đạo tương lai khác (s†0, a†0, . . . , s†U, a†U) với s†0=st+1 và a†0=at+1. Nói cách khác, giai đoạn hành động theo một tập con ngắn hạn của kế hoạch dài hạn, được tái tạo tại mỗi trạng thái mới. Tác nhân LLM lưu trữ phản hồi thu thập được (st, at, rt, st+1) trong bộ đệm bộ nhớ Dt và truy vấn điều kiện chuyển đổi If-Switch để quyết định khi nào cập nhật bộ đệm bộ nhớ Dtk, được sử dụng trong thói quen lý luận như ngữ cảnh cho học và lập kế hoạch. Trực quan, chúng tôi kết hợp phần lịch sử mới nhất Dt−D tk để cải thiện chính sách hiện tại chỉ khi nó mang thông tin mới đáng kể, ví dụ, khi tác nhân LLM thua lần đầu tiên sau một chuỗi thắng.

Mô hình LLM Elite LLM
MôitrườngLý luận
Hành độngĐầu vào: 2 5 8 112*5
2+511-52*5=10
(còn lại: 8 10 11)
2+5=7
(còn lại: 7 8 11)11-5=6
(còn lại: 2 6 8)
11-5
8/6
2*2
8-2
MôiTrường2*211-5=6
(còn lại: 2 6 8)
11-5=6
(còn lại: 2 6 8)
8/6
8-68/28/6=1.33
(còn lại: 2 1.33)
8-6=2
(còn lại: 2 2)8/2=4
(còn lại: 4 6)
6-4
4*6
4+6
Có khả năngCritic LLM
Không thể
Không thể
Tôi đã sử dụng 2 lặp lại. 2*2=4:
Không thểCó khả năng
Không thể
Không thể
8/6=1.33
(còn lại: 2 1.33)
2*2=4
(còn lại: 4 6)
8-2=6
(còn lại: 6 6)
6/4=2
(còn lại: 2)
4*6=24
(còn lại: 24)
4+6=10
(còn lại: 10)
8/24-6=24
(còn lại: 24)
4*6
8/2=4
(còn lại: 4 6)
MôiTrườngMôiTrường
Lý luậnHành động

Hình 4: RAFA cho Trò chơi 24. Các hành động được đề xuất (đường nét đứt) và được chọn (xanh lá). Các ảo giác rằng cùng một số có thể được sử dụng lại được giảm thiểu thông qua tương tác.
14

--- TRANG 15 ---
4 Thí nghiệm
Chúng tôi đánh giá RAFA trong một số benchmark dựa trên văn bản, ví dụ, Trò chơi 24, ALFWorld, BlocksWorld, và Tic-Tac-Toe. Các thiết lập chi tiết, kết quả, và nghiên cứu loại bỏ được cung cấp trong Phụ lục F, trong khi các lời nhắc chi tiết được tìm thấy trong Phụ lục G. Chúng tôi phát hành tất cả mã trên trang: https://agentification.github.io/RAFA.

4.1 Trò chơi 24
Trò chơi 24 (Yao et al., 2023a) là một câu đố toán học để thu được 24 từ bốn số tự nhiên thông qua các phép toán số học cơ bản. Trạng thái là công thức hiện tại (có thể chưa hoàn thành) và hành động là công thức tiếp theo (hoặc phần được sửa đổi).

Thiết lập. Chúng tôi mô phỏng thuật toán tìm kiếm cây để lập kế hoạch (B∈ {1,2}). Tại bước thứ t, RAFA học từ bộ đệm bộ nhớ và chuyển sang một chính sách mới khi nhận được phần thưởng bất ngờ, đây là điều kiện chuyển đổi. Sau bước thứ t, RAFA tiêu hóa phản hồi thu thập được và tạo ra một tóm tắt ngôn ngữ, được lưu vào bộ đệm bộ nhớ để tránh những lỗi tương tự trước đó.

RAFA (B= 1) RAFA (B= 2) ToT(B= 1) ToT(B= 2) Reflexion
gpt-4 89% 93% 73% 81% 21%
gpt-3.5 29% 46% 10% 17% 16%
Bảng 2: Kết quả Trò chơi 24.

Kết quả. RAFA đạt được hiệu suất SOTA như được hiển thị trong Bảng 2. RAFA đạt được hiệu quả mẫu vượt trội bằng cách giảm thiểu ảo giác và tránh các thử nghiệm bất cẩn (Hình 4 và 5).

3 6 9 12 15 18
Bước020406080100Tỷ lệ Thành công (%)Trò chơi 24, gpt-4
RAFA (b= 1)
RAFA (b= 2)
Reflexion
ToT(b= 1,20 thử nghiệm)
ToT(b= 2,20 thử nghiệm)
3 6 9 12 15 18
Bước01020304050Tỷ lệ Thành công (%)Trò chơi 24, gpt-3.5-turbo
RAFA (b= 1)
RAFA (b= 2)
Reflexion
ToT(b= 1,20 thử nghiệm)
ToT(b= 2,20 thử nghiệm)

Hình 5: Hiệu quả mẫu trên Trò chơi 24.
15

--- TRANG 16 ---
Pick Clean Heat Cool Examine PickTwo Total
BUTLER 46.00 39.00 74.00 100.00 22.00 24.00 37.00
ReAct 66.67 41.94 91.03 80.95 55.56 35.29 61.94
AdaPlanner 100.00 96.77 95.65 100.00 100.00 47.06 91.79
Reflexion 100.00 90.32 82.61 90.48 100.00 94.12 92.54
RAFA 100.00 96.77 100.00 100.00 100.00 100.00 99.25
Bảng 3: Kết quả ALFWorld (tỷ lệ thành công %).

4.2 ALFWorld
Elite LLMLý luận
Model LLMBạn thấy một nĩa 1
và một cốc 1.
Bạn thấy một đĩa cd 1
và một cốc 1.
Lấy trứng 1
Lấy cốc 1
từ
ngăn kéo 1Đi đến
ngăn kéo 1

Bạn thấy một bánh mì
1 và một trứng 1.
Đi đến
ngăn kéo 1Đi đến
tủ 1
Đi đến
mặt bàn
1
Critic LLM
Bạn thấy một
đĩa cd 1.
Trứng 1 được
nhặt lên.
Cốc 1 được
nhặt lên.Mục tiêu: làm nóng một trứng và 
đặt nó lên bàn ăn
Bạn thấy một
tủ 1, một
mặt bàn 1, một
bàn ăn 1,
và một ngăn kéo 1.
Hành động
Môi trường
...Đi đến
mặt bàn 1

Hình 6: Minh họa RAFA trong môi trường ALFWorld.

ALFWorld (Shridhar et al., 2020) là một môi trường tương tác cho mô phỏng tác nhân hiện thân, bao gồm 134 nhiệm vụ gia đình trong sáu danh mục tổng thể (Bảng 3). Chúng tôi sử dụng gpt-3 (text-davinci-003).

0 2 4 6 8 10
Tập6472808896Tỷ lệ Thành công (%)ALFWorld, gpt-3 cho RAFA
RAFA
Reflexion
AdaPlanner
ReAct

Hình 7: Hiệu quả mẫu trên ALFWorld.Thiết lập. Chúng tôi mô phỏng thuật toán tìm kiếm cây để lập kế hoạch (B= 2). RAFA gọi Critic để đánh giá phần hoàn thành của mục tiêu mong muốn và chuyển sang một chính sách mới sau 20 lần thất bại liên tiếp.

Kết quả. RAFA vượt trội hơn các khung hiện có khác nhau (Hình 7). Hiệu suất tốt hơn của AdaPlanner ở tập ban đầu được quy cho một tập hợp chương trình được chế tác thủ công để từ chối các quỹ đạo ứng viên không tối ưu, điều này thách thức để xây dựng mà không có kiến thức lĩnh vực của một nhiệm vụ cụ thể. Một ví dụ như vậy là danh mục PickTwo.
16

--- TRANG 17 ---
4.3 BlocksWorld
BlocksWorld (Hao et al., 2023) chứa các nhiệm vụ để sắp xếp các khối thành các cấu hình cụ thể.

Thiết lập. Chúng tôi sử dụng mô hình Vicuna (Zheng et al., 2023) và mô phỏng thuật toán MCTS để lập kế hoạch.

Gỡ khối vàng
khỏi khối xanh
s1
Model LLM Critic LLM
Nhặt lên
khối cam

Gỡ khối tím
khỏi khối đỏ
Elite LLM
Lý luận
Mục tiêu: khối xanh ở trên
khối cam  
Hành động
Môi trường
...Gỡ khối vàng
khỏi khối xanh

Hình 8: RAFA cho BlocksWorld.

Kết quả. RAFA đạt được tỷ lệ thành công vượt trội trên nhiều phiên bản Vicuna (Hình 9). So sánh với CoT và RAP thể hiện cách thói quen con học cải thiện tính tối ưu của lập kế hoạch.

0 15 30 45 60
Bước020406080100Tỷ lệ Thành công (%)BlocksWorld (4-bước), Vicuna-13B
RAFA
RAP(thử nghiệm=10)
RAP(thử nghiệm=20)
0 15 30 45 60
Bước020406080100Tỷ lệ Thành công (%)BlocksWorld (4-bước), Vicuna-33B
RAFA
CoT(gpt-4)
CoT(LLaMA-33B)
RAP(thử nghiệm=10)
RAP(thử nghiệm=20)
0 15 30 45 60
Bước020406080100Tỷ lệ Thành công (%)BlocksWorld (6-bước), Vicuna-13B
RAFA
CoT(gpt-4)
RAP(thử nghiệm=10)
RAP(thử nghiệm=20)
CoT(LLaMA-33B)
0 15 30 45 60
Bước020406080100Tỷ lệ Thành công (%)BlocksWorld (6-bước), Vicuna-33B
RAFA
CoT(gpt-4)
CoT(LLaMA-33B)

Hình 9: Hiệu quả mẫu trên BlocksWorld (4 và 6 là số bước tối thiểu để giải quyết một nhiệm vụ cụ thể). CoT được nhắc bởi bốn ví dụ trong ngữ cảnh.
17

--- TRANG 18 ---
4.4 Tic-Tac-Toe

0 2 4 6 8 10
Tập−0.9−0.6−0.30.00.30.60.9ĐiểmTic-Tac-Toe (Người chơi O), gpt-4
RAFA (B= 3)
RAFA (B= 4)

Hình 10: Hiệu quả mẫu trên Tic-Tac-Toe (0 có nghĩa là hòa).Tic-Tac-Toe (Beck, 2008) là một trò chơi cạnh tranh nơi các bên X và O thay phiên nhau đặt dấu. RAFA gọi Model để mô phỏng chuyển tiếp và động lực đối thủ (Hình 18).

Thiết lập. Chúng tôi sử dụng gpt-4 và mô phỏng thuật toán tìm kiếm cây để lập kế hoạch (B∈ {3,4}). RAFA chuyển sang một chính sách mới khi (a) trạng thái dự đoán khác với trạng thái quan sát được, (2) hành động dự đoán của đối thủ khác với hành động quan sát được, hoặc (3) Critic đưa ra dự đoán sai về trạng thái trò chơi. Ở đây, X có lợi thế bất đối xứng (thắng chắc chắn nếu chơi đúng cách).

Kết quả. RAFA (chơi O) ngang bằng và đánh bại gpt-4 cho T= 5 và T= 7 (Bảng 4), mặc dù O định mệnh thua. Nghiên cứu loại bỏ (B= 3 so với B= 4) minh họa cách tính không tối ưu của lập kế hoạch ảnh hưởng đến hiệu quả mẫu (Hình 10).

Elite LLM
L ý  l u ậ n
Động lực
Model LLMCritic LLM Chưa
xong
Đối thủ
Model LLM
O | 2 | 3
-----------
4 | X | X
-----------
O | 8 | 9O | 2 | 3
-----------
O | X | X
-----------
X | 8 | 9O | 2 | 3
-----------
O | X | X
-----------
7 | 8 | 9
O | 2 | 3
-----------
X | X | X
-----------
O | 8 | 9ThấtbạiO | O | 3
-----------
4 | X | X
-----------
7 | 8 | 9O | O | 3
-----------
X | X | X
-----------
7 | 8 | 9
ThấtbạiH à n h  đ ộ n gO | 2 | 3
-----------
4 | X | X
-----------
7 | 8 | 9
Môi trường
...Hành động đối thủ

Hình 11: RAFA (chơi O) cho Tic-Tac-Toe.

X O gpt-4 RAFA (T=1) RAFA (T=5) RAFA (T=7)
gpt-4 90%, 0%, 10% 90%, 0%, 10% 50%, 0%, 50% 0%, 0%, 100%
Bảng 4: Kết quả Tic-Tac-Toe. Chúng tôi đặt B= 4 và báo cáo tỷ lệ thắng của X, tỷ lệ hòa, và tỷ lệ thắng của O.
18

--- TRANG 19 ---
5 Phân tích Lý thuyết
Trong phần này, chúng tôi cung cấp các kết quả lý thuyết trong bài báo này. Trong Phần 5.1, chúng tôi đặc trưng yêu cầu cho thói quen con lập kế hoạch trong RAFA và hiển thị thuật toán lặp giá trị với chân trời cắt ngắn có thể là một ví dụ của người lập kế hoạch mong muốn. Trong Phần 5.2, chúng tôi hiển thị rằng LLM với căn chỉnh hậu nghiệm thực hiện BMA, hỗ trợ Khẳng định 2.1 về mặt lý thuyết. Chúng tôi trình bày phân tích hối tiếc cho RAFA trong Phần 5.3 để giải thích hiệu suất thực nghiệm vượt trội của nó, nơi chúng tôi cung cấp các giả định cần thiết và ràng buộc hối tiếc của RAFA. Trong Phần 5.4, chúng tôi hiển thị rằng RAFA có thể được sửa đổi để khuyến khích khám phá hiệu quả cho các nhiệm vụ phức tạp hơn sao cho RAFA vẫn hiệu quả mẫu mà không có giả định tập trung trong Phần 5.3.

5.1 Tính tối ưu của Lập kế hoạch
Để đặc trưng yêu cầu cho thói quen con lập kế hoạch trong RAFA (Thuật toán 1), chúng tôi định nghĩa người lập kế hoạch ϵ-tối ưu như sau.

Định nghĩa 5.1 (Người lập kế hoạch ϵ-Tối ưu). Ký hiệu {V|V là một hàm giá trị} như V. Một thuật toán lập kế hoạch PLϵ:P × R 7→ Π× V là một người lập kế hoạch ϵ-tối ưu nếu PLϵ(P, r) = (π, V), trong đó |Q(s, a)−r(s, a)−γ·(PV)(s, a)| ≤ϵ và V(s) = max aQ(s, a) =Q(s, π(s)) cho tất cả (s, a)∈ S × A.

Nói cách khác, một người lập kế hoạch ϵ-tối ưu với một mô hình (nhân chuyển tiếp và hàm phần thưởng) có thể tạo ra một chính sách để tối đa hóa gần đúng hàm giá trị dài hạn tương ứng thay vì phần thưởng cận thị với giới hạn lỗi gần đúng ϵ. Như một thể hiện của người lập kế hoạch thỏa mãn Định nghĩa 5.1, chúng tôi trình bày thuật toán lặp giá trị (Thuật toán 3) với chân trời cắt ngắn U, tức là, độ dài hữu hạn của cửa sổ nhìn trước như người lập kế hoạch ϵ-tối ưu trong Thuật toán 4. Mệnh đề sau đảm bảo rằng Thuật toán 3 thỏa mãn Định nghĩa 5.1.

Mệnh đề 5.2. Thuật toán 3 là một người lập kế hoạch ϵ-tối ưu miễn là chúng ta đặt U≥1+⌈logγ(ϵ/L)⌉ và bất kỳ hàm giá trị nào được giới hạn bởi L≥0.

Chứng minh. Xem Phụ lục C.1 cho chứng minh chi tiết.

Thuật toán 3 Người lập kế hoạch ϵ-tối ưu: Thuật toán lặp giá trị với chân trời cắt ngắn.
1:đầu vào : Mô hình (P, r) và chân trời cắt ngắn U.
2:khởi tạo : Đặt hàm giá trị V(U)_θ(·)←0.
3:cho u=U−1, . . . , 1 thực hiện
4: Đặt hàm giá trị V(u)(·)←max a∈AQ(u)(·, a), trong đó Q(u)(·,·)←r(·,·) + γ(PV(u+1))(·,·).
5:kết thúc cho
6:đầu ra : Chính sách tham lam π(·) = arg max a∈AQ(1)(·, a) và hàm giá trị V(1).
19

--- TRANG 20 ---
Thay vào đó, chúng ta có thể chọn mô phỏng thuật toán tìm kiếm cây, thuật toán bắn ngẫu nhiên, hoặc thuật toán tìm kiếm cây Monte-Carlo. Trong ví dụ tìm kiếm cây (Dòng 5-11 trong Thuật toán 2), ϵ giảm khi độ rộng tìm kiếm B và độ sâu U tăng. Lưu ý rằng, miễn là chúng ta mô phỏng một người lập kế hoạch ϵ-tối ưu, chúng ta có thể thiết lập đảm bảo hiệu quả mẫu có thể chứng minh.

5.2 LLM với Căn chỉnh Hậu nghiệm Thực hiện Trung bình Mô hình Bayesian (BMA)

Trong phần sau, chúng tôi phân tích Khẳng định 2.1 từ quan điểm lý thuyết. Đối với LLM được sử dụng trong RAFA, chúng tôi ký hiệu PLLM(ξ(s,a)|D, s, a) như thước đo xác suất của cặp trạng thái-phần thưởng dự đoán cho cặp trạng thái-hành động truy vấn và bộ đệm bộ nhớ D như tập dữ liệu trong ngữ cảnh. Được gây ra bởi PLLM, chúng tôi cũng ký hiệu PLLM(D) và rLLM(D) như nhân chuyển tiếp ước tính và hàm phần thưởng bởi LLM, tương ứng.

Để đơn giản hóa phân tích, chúng tôi giả định rằng tất cả LLM có căn chỉnh hậu nghiệm trong các nhiệm vụ mà chúng tôi nghiên cứu, tức là, phân phối hậu nghiệm của chúng về phần thưởng và trạng thái tiếp theo cho cặp trạng thái-hành động hiện tại và bất kỳ tập dữ liệu trong ngữ cảnh nào khớp với hậu nghiệm trong các nhiệm vụ này. Chúng tôi công thức hóa giả định này như sau.

Giả định 5.3 (Căn chỉnh Hậu nghiệm). Chúng tôi giả định rằng LLM được căn chỉnh với hậu nghiệm của trạng thái và phần thưởng trong MDP cơ bản, được công thức hóa như

PLLM[ξ(s,a)|D, s, a] = Ppost[ξ(s,a)|D, s, a],

cho bất kỳ tập dữ liệu trong ngữ cảnh D={(si, ai, ri, s′i)}I_i=0 với kích thước I, cặp trạng thái-hành động truy vấn (s, a), phần thưởng r, và trạng thái s′. Ở đây, hậu nghiệm Ppost được định nghĩa trong (2.4).

Chúng tôi nhận xét rằng căn chỉnh hậu nghiệm trong Giả định 5.3 đến từ khả năng trong ngữ cảnh của LLM, được nghiên cứu rộng rãi trong Lee et al. (2023); Wies et al. (2024); Xie et al. (2021). Chúng tôi cũng nhận xét rằng Giả định 5.3 không có nghĩa là LLM có thể đưa ra quyết định tối ưu một cách tầm thường tại mỗi bước trong MDP cơ bản: (1) Mặc dù các phân phối hậu nghiệm của trạng thái và phần thưởng được căn chỉnh, LLM vẫn cần được hướng dẫn để tối đa hóa giá trị dài hạn (thông qua lập kế hoạch rõ ràng) thay vì phần thưởng cận thị. (2) LLM vẫn yêu cầu tương tác trực tuyến để mở rộng tập dữ liệu trong ngữ cảnh D sao cho bất định dự đoán của chúng có thể được giảm từ bất định tiên nghiệm. Trong Phụ lục C.7, chúng tôi cũng thảo luận cách nới lỏng Giả định 5.3 để chứa một thuật ngữ lỗi bổ sung trong ràng buộc hối tiếc được suy ra bởi phân tích của chúng tôi, nơi chúng tôi giả định rằng LLM là bộ ước tính khả năng tối đa (MLE) trên tập dữ liệu tiền huấn luyện với phủ sóng đều. Dựa trên Giả định 5.3, chúng tôi chứng minh rằng LLM với căn chỉnh hậu nghiệm thực hiện BMA trong ước tính mô hình trong mệnh đề sau.

Mệnh đề 5.4 (LLM với Căn chỉnh Hậu nghiệm Thực hiện BMA). Dưới Giả định 5.3, các dự đoán LLM thỏa mãn

rLLM(D)(s, a) +γ·(PLLM(D)V)(s, a) =Eθ∼Ppost(·|D)[(BθV)(s, a)]
20

--- TRANG 21 ---
cho bất kỳ tập dữ liệu D={(si, ai, ri, s′i)}I_i=0 với kích thước I, hàm giá trị V, và cặp trạng thái-hành động truy vấn (s, a)∈ S ×A. Ở đây, Ppost(θ|D) là hậu nghiệm của θ⋆ cho D trong MDP cơ bản.

Chứng minh Mệnh đề 5.4. Xem chứng minh chi tiết trong Phụ lục C.2.

Chứng minh của Mệnh đề 5.4 có thể được tìm thấy trong Phụ lục C.2. Một số biến thể của Mệnh đề 5.4 có thể được tìm thấy trong các tài liệu khác nhau (Lee et al., 2023; Zhang et al., 2022, 2023b). Cụ thể, Zhang et al. (2022) thiết lập sự tương đương lý thuyết giữa BMA và kiến trúc attention lý tưởng và phân tích tỷ lệ lỗi tổng quát hóa của LLM. Bởi Mệnh đề 5.4, LLM có thể cung cấp ước tính chắc chắn và chính xác hơn cho mô hình sinh dữ liệu với nhiều phản hồi thu thập được hơn, vì bất định trong hậu nghiệm giảm với nhiều dữ liệu hơn. Vì vậy, Mệnh đề 5.4 hỗ trợ Khẳng định 2.1 về mặt lý thuyết.

5.3 Ràng buộc Hối tiếc của RAFA

Thuật toán 4 Lý luận cho tương lai, hành động cho hiện tại (RAFA): Phiên bản lý thuyết.
1:đầu vào : Một người lập kế hoạch ϵ-tối ưu PLϵ, trả về một chính sách ϵ-tối ưu tối đa hóa hàm giá trị đến độ chính xác ϵ (Định nghĩa 5.1), và LLM với căn chỉnh hậu nghiệm.
2:khởi tạo : Lấy mẫu trạng thái ban đầu s0∼ρ, đặt t= 0, và khởi tạo bộ đệm bộ nhớ D0=∅.
3:cho k= 0,1, . . . , thực hiện
4: Đặt tk←t.
5:lặp lại
6: Lập kế hoạch trước với người lập kế hoạch ϵ-tối ưu và LLM (πt, Vt)←PLϵ(PLLM(Dtk), rLLM(Dtk)). ("lý luận cho tương lai")
7: Thực hiện hành động at=πt(st) để nhận phần thưởng rt và trạng thái st+1 từ môi trường. ("hành động cho hiện tại")
8: Cập nhật bộ đệm bộ nhớ Dt+1← D t∪ {(st, at, st+1, rt)}.
9: Đặt t←t+ 1.
10: cho đến khi Htk−Ht>log 2, trong đó Ht ký hiệu entropy hậu nghiệm của θ⋆ có điều kiện trên Dt. (điều kiện chuyển đổi được thỏa mãn)
11:kết thúc cho

Để phân tích RAFA về mặt lý thuyết, chúng tôi đề xuất phiên bản lý thuyết của RAFA trong Thuật toán 4, nơi chúng tôi thực hiện điều kiện chuyển đổi của RAFA trong Dòng 10 bằng cách đo giảm entropy hậu nghiệm. Tại bước thứ t và lần chuyển đổi thứ k, Thuật toán 6 chỉ thực hiện chuyển đổi thứ (k+1) khi giảm entropy hậu nghiệm Htk−Ht lớn hơn log 2. Trong Dòng 6 của Thuật toán 4, chúng tôi mô tả thói quen con lập kế hoạch trong RAFA (Thuật toán 1) bằng một ϵ-planner PLϵ (được định nghĩa trong Định nghĩa 5.1). Chúng tôi chỉ định điều kiện kết thúc cho Thuật toán 4. Gọi (K−1) là tổng số chuyển đổi cho đến khi t đạt (T−1). Gọi tK=T. Tại bước (T−1), Thuật toán 4 thực thi aT−1=πT−1(sT−1), trong đó chúng ta có πT−1=PLϵ(PLLM(DtK−1), rLLM(DtK−1)). Khi nhận rT−1 và sT từ môi trường bên ngoài, Thuật toán 4 cập nhật DT={(st, at, st+1, rt)}T−1_t=0 và kết thúc. Vì tác nhân trong Thuật toán 4 thực thi cùng một chính sách cho đến khi thực hiện chuyển đổi, chúng ta có πt=πtk cho bất kỳ tk≤k < t k+1. Chúng tôi ký hiệu bởi πk=πtk cho sự đơn giản ký hiệu. Tiếp theo, chúng tôi áp đặt một giả định tính quy luật về cấu trúc của MDP để đo độ khó học. Nhớ lại rằng chúng tôi định nghĩa entropy hậu nghiệm Ht trong (2.5), tăng thông tin I(θ;ξ|D), và ξ(s,a) như cặp của trạng thái tiếp theo và phần thưởng hiện tại (s′, r) cho cặp trạng thái-hành động truy vấn (s, a) trong Phần 2. Định nghĩa Ht như entropy hậu nghiệm H(θ|Dt) cho tập dữ liệu Dt={(si, ai, ri, si+1)}t−1_i=0.

Giả định 5.5 (Tính Quy luật MDP). Chúng tôi giả định rằng tồn tại một hệ số η >0 sao cho, nếu Ht1−Ht2≤log 2, thì điều đó xảy ra

I(θ;ξ(s,a)|Dt1)≤4η·I(θ;ξ(s,a)|Dt2)

cho bất kỳ hàm giá trị V, t1< t2 và (s, a)∈ S × A cho trước.

Giả định 5.5 là một giả định tính quy luật về MDP và là nội tại đối với thiết kế tác nhân. Trong Phụ lục E, chúng tôi chứng minh rằng MDP nhân tuyến tính Bayesian d-chiều (được định nghĩa trong Định nghĩa E.1), thỏa mãn Giả định 5.5 với hệ số η=d/log(1 + d). Trực quan, Giả định 5.5 hạn chế sự gia tăng của tăng thông tin cho một bit (log 2) giảm entropy hậu nghiệm.

Tương tự như các công việc lý thuyết khác về RL sâu (Lazaric et al., 2010; Fan et al., 2020; Zhang et al., 2020), chúng tôi giới thiệu hệ số tập trung κ để giới hạn sự dịch chuyển phân phối giữa chính sách hiện tại và chính sách tối ưu. Để đơn giản hóa thảo luận, chúng tôi định nghĩa thước đo viếng thăm γ-giảm giá tối ưu ν⋆ bắt đầu từ trạng thái s như

ν⋆(s′|s) =1/(1−γ)·∑∞_τ=0γτ·P[sτ=s′|s0=s, si+1∼Pθ⋆(·|si, π⋆(si)) cho bất kỳ 0 ≤i < τ], (5.1)

cho bất kỳ trạng thái s, s′∈ S. Ở đây, ν⋆(·|s) mô tả thước đo xác suất trung bình giảm giá của trạng thái mà chính sách tối ưu π⋆ viếng thăm bắt đầu từ trạng thái s trong MDP cơ bản. Bây giờ, chúng tôi sẵn sàng cung cấp tuyên bố đầy đủ của hệ số tập trung như sau.

Giả định 5.6 (Tập trung). Đối với RAFA (Thuật toán 4), chúng tôi giả định rằng tồn tại một hằng số κ <∞ sao cho

E[∑K−1_k=0 Eπk[∑tk+1−1_t=tk Eθ⋆∼Ptk[Es∼ν⋆(·|st)[(Bk−Bθ⋆)Vt]2(s, π⋆(s))] / [(Bk−Bθ⋆)Vt]2(st, πk(st))|Dtk]]]

được giới hạn bởi κ2·T, trong đó chúng tôi định nghĩa (BkV)(s, a) =rLLM(Dtk)(s, a) +γ·(PLLM(Dtk)V)(s, a) và ký hiệu bởi Ptk hậu nghiệm của θ⋆ cho Dtk.
22

--- TRANG 23 ---
Trực quan, κ đo độ khó để tổng quát hóa lỗi dự đoán thấp (Bk−Bθ⋆)Vt trên quỹ đạo hiện tại được gây ra bởi πk đến quỹ đạo tối ưu được gây ra bởi π⋆ trong MDP cơ bản. Chúng tôi nhận xét rằng chúng ta có thể bỏ sự phụ thuộc của hệ số tập trung κ (Giả định 5.6) nếu chúng ta sửa đổi RAFA để khuyến khích khám phá hiệu quả trong MDP. Chúng tôi sẽ thảo luận các biến thể của RAFA với các chiến lược khám phá hiệu quả trong Phần 5.4.

Trong định lý sau, chúng tôi đưa ra ràng buộc của hối tiếc Bayesian của RAFA (Thuật toán 4) như sau.

Định lý 5.7. Dưới Giả định 5.3, 5.5, và 5.6, hối tiếc Bayesian của RAFA (Thuật toán 4) thỏa mãn

R(T) =O[(κ+ 1)L·√E[H0−HT]/(1−γ)·√T+ϵ/(1−γ)·T+L·E[H0−HT]/(1−γ)],

trong đó κ là hệ số tập trung được định nghĩa trong Giả định 5.6, Ht là entropy hậu nghiệm của θ⋆ cho lịch sử Dt={(si, ai, ri, si+1)}t−1_i=0, và L là ràng buộc của |r+V(s)| cho bất kỳ phần thưởng r, trạng thái s, và hàm giá trị V.

Chứng minh Định lý 5.7. Xem chứng minh chi tiết trong Phụ lục C.4.

Định lý 5.10 thiết lập hối tiếc √T của RAFA (Thuật toán 4) cho lựa chọn phù hợp của tính không tối ưu lập kế hoạch ϵ, ví dụ, ϵ=O(1/√T), điều này cho thấy rằng RAFA hiệu quả mẫu và giải thích hiệu suất thực nghiệm mạnh mẽ của nó trong Phần 4. Ở đây, thuật ngữ đầu tiên trong ràng buộc trên trong Định lý 5.10 là thuật ngữ hàng đầu và liên quan đến một số yếu tố nhân, cụ thể là chân trời hiệu quả 1/(1−γ), ràng buộc giá trị L, và giảm entropy hậu nghiệm tích lũy H0−HT trong suốt T bước, điều này phổ biến trong tài liệu RL (Abbasi-Yadkori và Szepesváti, 2015; Osband et al., 2013; Russo và Van Roy, 2014a,b, 2016; Lu và Van Roy, 2019). Cụ thể, H0 nhấn mạnh kiến thức tiên nghiệm thu được thông qua tiền huấn luyện, vì H0 định lượng bất định tiên nghiệm của LLM trước khi kết hợp bất kỳ phản hồi thu thập nào. Do đó, H0−HT nhấn mạnh giảm bất định đạt được bằng lý luận và hành động, vì HT định lượng bất định hậu nghiệm của LLM sau khi kết hợp phản hồi thu thập được. Trong Phụ lục E, chúng tôi chứng minh rằng H0−HT=O(d·logT) và ràng buộc xác suất 1−δ trên hàm giá trị L=O(√d·log(dT/δ)) cho MDP nhân tuyến tính Bayesian d-chiều, điều này ngụ ý R(T) =Õ((1−γ)−1(κ+ 1)·√d3T) với xác suất ít nhất 1−δ. Ở đây Õ ẩn yếu tố logarithmic.

5.4 RAFA với Chiến lược Khám phá Hiệu quả

Để bỏ sự phụ thuộc của Giả định 5.6 (Tập trung) và giải quyết các nhiệm vụ phức tạp hơn, chúng tôi cung cấp hai biến thể của RAFA (Thuật toán 4): (1) RAFA với thưởng lạc quan (Thuật toán 5) và (2) RAFA với lấy mẫu hậu nghiệm (Thuật toán 6). Chúng tôi cũng chứng minh ràng buộc của hối tiếc Bayesian của mỗi biến thể, điều này thể hiện hiệu quả của các chiến lược khám phá hiệu quả này mà không có Giả định 5.6 (Tập trung).
23

--- TRANG 24 ---
Thuật toán 5 Lý luận cho tương lai, hành động cho hiện tại (RAFA): Phiên bản lý thuyết với thưởng lạc quan.
1:đầu vào : Một người lập kế hoạch ϵ-tối ưu PLϵ, trả về một chính sách ϵ-tối ưu tối đa hóa hàm giá trị đến độ chính xác ϵ (Định nghĩa 5.1), và LLM với căn chỉnh hậu nghiệm.
2:khởi tạo : Lấy mẫu trạng thái ban đầu s0∼ρ, đặt t= 0, và khởi tạo bộ đệm bộ nhớ D0=∅.
3:cho k= 0,1, . . . , thực hiện
4: Đặt tk←t.
5:lặp lại
6: Thiết kế thưởng lạc quan Γ k(s, a) =√2L·√I(θ;ξ(s,a)|Dtk) cho tất cả (s, a)∈ S × A.
7: Lập kế hoạch trước với người lập kế hoạch ϵ-tối ưu và LLM (πt, Vt)←PLϵ(PLLM(Dtk), rLLM(Dtk)+ Γ k). ("lý luận cho tương lai")
8: Thực hiện hành động at=πt(st) để nhận phần thưởng rt và trạng thái st+1 từ môi trường. ("hành động cho hiện tại")
9: Cập nhật bộ đệm bộ nhớ Dt+1← D t∪ {(st, at, st+1, rt)}.
10: Đặt t←t+ 1.
11: cho đến khi Htk−Ht>log 2, trong đó Ht ký hiệu entropy hậu nghiệm của θ⋆ có điều kiện trên Dt. (điều kiện chuyển đổi được thỏa mãn)
12:kết thúc cho

5.4.1 Thưởng Lạc quan

Chúng tôi kết hợp nguyên tắc Lạc quan Trước Bất định (OFU) (Cai et al., 2020; Zhou et al., 2021b; Jin et al., 2020; Liu et al., 2022b; Wang et al., 2023c) để khuyến khích khám phá hiệu quả bằng cách thêm thưởng lạc quan vào hàm phần thưởng trong thói quen con lập kế hoạch của RAFA. Chúng tôi thiết kế thưởng lạc quan bằng tăng thông tin và thực hiện một biến thể của RAFA trong Thuật toán 5. Cụ thể, thưởng Γ k(s, a) có dạng sau

Γk(s, a) =√2L·√I(θ;ξ(s,a)|Dtk) (5.2)

cho bất kỳ (s, a)∈ S × A và k < K. Trong Dòng 7 của Thuật toán 5, chúng ta tạo chính sách πt bằng PLϵ(PLLM(Dtk), rLLM(Dtk)+ Γ k) cho bất kỳ tk≤t < t k+1. Trực quan, thưởng cao hơn tại cặp trạng thái-hành động với tăng thông tin cao hơn, điều này khuyến khích tác nhân khám phá những trạng thái ít được viếng thăm (với tăng thông tin cao hơn). Trong định lý sau, chúng tôi chứng minh ràng buộc hối tiếc của RAFA với thưởng lạc quan (Thuật toán 5).

Định lý 5.8. Dưới Giả định 5.3 và 5.5, hối tiếc Bayesian của RAFA với thưởng lạc quan (Thuật toán 5) thỏa mãn

R(T) =O[L·√E[H0−HT]/(1−γ)·√T+ϵ/(1−γ)·T+L·E[H0−HT]/(1−γ)],

trong đó tất cả các biến có cùng định nghĩa trong Định lý 5.7.

Chứng minh Định lý 5.8. Xem chứng minh chi tiết trong Phụ lục C.5.

So với Định lý 5.7, ràng buộc hối tiếc trong Định lý 5.8 không phụ thuộc vào hệ số tập trung κ, điều này thể hiện hiệu quả của thưởng lạc quan trong Thuật toán 5. Trong Phụ lục E, chúng tôi chứng minh rằng H0−HT=O(d·logT) và ràng buộc xác suất 1−δ trên hàm giá trị L=O(√d·log(dT/δ)) cho MDP nhân tuyến tính Bayesian d-chiều, điều này ngụ ý R(T) =Õ((1−γ)−1·√d3T) với xác suất ít nhất 1−δ. Ở đây Õ ẩn yếu tố logarithmic.
24

--- TRANG 25 ---
5.4.2 Lấy mẫu Hậu nghiệm

Thuật toán 6 Lý luận cho tương lai, hành động cho hiện tại (RAFA): Phiên bản lý thuyết với lấy mẫu hậu nghiệm.
1:đầu vào : Một người lập kế hoạch ϵ-tối ưu PLϵ, trả về một chính sách ϵ-tối ưu tối đa hóa hàm giá trị đến độ chính xác ϵ (Định nghĩa 5.1), và LLM thỏa mãn Giả định 5.9.
2:khởi tạo : Lấy mẫu trạng thái ban đầu s0∼ρ, đặt t= 0, và khởi tạo bộ đệm bộ nhớ D0=∅.
3:cho k= 0,1, . . . , thực hiện
4: Đặt tk←t.
5:lặp lại
6: Lập kế hoạch trước với người lập kế hoạch ϵ-tối ưu và cơ chế lấy mẫu hậu nghiệm của LLM (được định nghĩa trong Giả định 5.9) (πt, Vt)←PLϵ(PLLM+PS (Dtk), rLLM+PS (Dtk)). ("lý luận cho tương lai")
7: Thực hiện hành động at=πt(st) để nhận phần thưởng rt và trạng thái st+1 từ môi trường. ("hành động cho hiện tại")
8: Cập nhật bộ đệm bộ nhớ Dt+1← D t∪ {(st, at, st+1, rt)}.
9: Đặt t←t+ 1.
10: cho đến khi Htk−Ht>log 2, trong đó Ht ký hiệu entropy hậu nghiệm của θ⋆ có điều kiện trên Dt. (điều kiện chuyển đổi được thỏa mãn)
11:kết thúc cho

Như một phương pháp khác cho khám phá hiệu quả, chúng tôi giả định rằng tồn tại một cơ chế triển khai lấy mẫu hậu nghiệm và chúng tôi sử dụng cơ chế này để khuyến khích khám phá cho RAFA.

Giả định 5.9 (LLM với Cơ chế Lấy mẫu Hậu nghiệm). Chúng tôi giả định rằng tồn tại một cơ chế LLM+PS ánh xạ bộ đệm bộ nhớ D thành nhân chuyển tiếp và hàm phần thưởng, sao cho (rLLM+PS (D)(s, a) +γ·(PLLM+PS (D)V)(s, a))|D và (Bθ⋆V(s, a))|D phân phối giống hệt nhau độc lập cho bất kỳ (s, a)∈ S × A, tập dữ liệu trong ngữ cảnh D, và hàm giá trị V. Ở đây, θ⋆ là tham số sinh dữ liệu.

Chúng tôi nhận xét rằng phương pháp bootstrap (Efron, 1982) có thể xấp xỉ cơ chế lấy mẫu hậu nghiệm thỏa mãn Giả định 5.9. Được sử dụng rộng rãi trong thống kê ứng dụng (Davison và Hinkley, 1997) và thiết kế thuật toán RL (Osband et al., 2016; Hao et al., 2019), phương pháp bootstrap lấy tập dữ liệu D và bộ ước tính hàm g làm đầu vào. Tùy thuộc vào cấu hình bootstrap, chúng ta tạo tập dữ liệu bootstrap D̃ từ D bằng lấy mẫu đều có hoàn lại (Efron, 1982) hoặc lấy mẫu có trọng số có hoàn lại (Newton và Raftery, 1994). Xem LLM như bộ ước tính hàm g và bộ đệm bộ nhớ D như tập dữ liệu D, chúng ta có thể sử dụng phương pháp bootstrap này để xấp xỉ cơ chế LLM+PS được giới thiệu trong Giả định 5.9. Từ tài liệu thống kê (Bickel và Freedman, 1981; Singh, 1981; Newton và Raftery, 1994), chúng ta cũng biết rằng phân phối bootstrap khôi phục phân phối hậu nghiệm một cách tiệm cận.

Dựa trên cơ chế thỏa mãn Giả định 5.9, chúng tôi đề xuất một biến thể của RAFA trong Thuật toán 6, nơi chúng ta sử dụng cơ chế LLM+PS như bộ ước tính mô hình trong thói quen con học của RAFA. Trong Dòng 7 của Thuật toán 5, chúng ta tạo chính sách πt bằng PLϵ(PLLM+PS (Dtk), rLLM+PS (Dtk)). Trong phần sau, chúng tôi đưa ra giải thích đơn giản về cách cơ chế này giúp tác nhân khám phá một cách hiệu quả. Bởi quy tắc Bayes, chúng ta có p(θ|D)∝L(D|θ)P0(θ), trong đó L(D|θ) là khả năng của D cho θ và P0 là tiên nghiệm của θ⋆. Lấy logarithm, chúng ta có log(p(θ|D)) = c+ log(P0(θ)) + log(L(D|θ)) cho một hằng số c nào đó. Do đó, bất định của hậu nghiệm cao hơn (p(θ|D) gần 0 hơn) tại các trạng thái ít được viếng thăm (khả năng của các trạng thái này gần 0 hơn). Giả sử chúng ta lấy mẫu bộ ước tính mô hình từ hậu nghiệm. Trong trường hợp đó, tác nhân có nhiều động cơ hơn để khám phá các trạng thái ít được viếng thăm, điều này giải thích tại sao cơ chế LLM+PS khuyến khích khám phá hiệu quả.

Trong định lý sau, chúng tôi chứng minh ràng buộc hối tiếc của RAFA với lấy mẫu hậu nghiệm (Thuật toán 6).

Định lý 5.10 (Hối tiếc Bayesian). Dưới Giả định 5.5 và 5.9, hối tiếc Bayesian của RAFA với lấy mẫu hậu nghiệm (Thuật toán 6) thỏa mãn

R(T) =O[L·√E[H0−HT]/(1−γ)·√T+ϵ/(1−γ)·T+L·E[H0−HT]/(1−γ)],

trong đó tất cả các biến có cùng định nghĩa trong Định lý 5.7.

Chứng minh Định lý 5.10. Xem chứng minh chi tiết trong Phụ lục C.6.

So với Định lý 5.7, ràng buộc hối tiếc trong Định lý 5.10 không phụ thuộc vào hệ số tập trung κ, điều này thể hiện hiệu quả của cơ chế lấy mẫu hậu nghiệm trong Thuật toán 6. Trong Phụ lục E, chúng tôi chứng minh rằng H0−HT=O(d·logT) và ràng buộc xác suất 1−δ trên hàm giá trị L=O(√d·log(dT/δ)) cho MDP nhân tuyến tính Bayesian d-chiều, điều này ngụ ý R(T) =Õ((1−γ)−1·√d3T) với xác suất ít nhất 1−δ. Ở đây Õ ẩn yếu tố logarithmic.
26

--- TRANG 27 ---
6 Kết luận
Trong bài báo này, chúng tôi thiết lập tương ứng LLM-RL và đề xuất một khung nguyên lý RAFA để điều phối lý luận và hành động, đạt được đảm bảo hiệu quả mẫu có thể chứng minh trong các tác nhân LLM tự động lần đầu tiên. Chúng tôi chứng minh ràng buộc hối tiếc √T của RAFA để nhấn mạnh sự phối hợp giữa kiến thức tiên nghiệm từ tiền huấn luyện và quá trình lặp của lý luận và hành động. Hiệu suất thực nghiệm xuất sắc của RAFA nhấn mạnh tiềm năng của nó cho ra quyết định tự động và thích ứng trong các nhiệm vụ phức tạp khác nhau, điều mà chúng tôi để lại cho công việc tương lai.

Lời cảm ơn
Zhaoran Wang ghi nhận Quỹ Khoa học Quốc gia (Giải thưởng 2048075, 2008827, 2015568, 1934931), Viện Simons (Lý thuyết Học Tăng cường), Amazon, J.P. Morgan, và Two Sigma cho sự hỗ trợ của họ.

Tài liệu tham khảo
Abbasi-Yadkori, Y., Pál, D. và Szepesváti, C. (2011). Thuật toán cải tiến cho bandit ngẫu nhiên tuyến tính. Trong Tiến bộ trong Xử lý Thông tin Nơ-ron.

Abbasi-Yadkori, Y. và Szepesváti, C. (2015). Điều khiển tối ưu Bayesian của các hệ thống được tham số hóa mượt mà. Trong Bất định trong Trí tuệ Nhân tạo.

Abernethy, J., Agarwal, A., Marinov, T. V. và Warmuth, M. K. (2023). Một cơ chế cho học trong ngữ cảnh hiệu quả mẫu cho các nhiệm vụ truy xuất thưa thớt. arXiv preprint arXiv:2305.17040.

Agarwal, A., Kakade, S., Krishnamurthy, A. và Sun, W. (2020). Flambe: Độ phức tạp cấu trúc và học biểu diễn của MDP hạng thấp. Tiến bộ trong hệ thống xử lý thông tin nơ-ron, 3320095–20107.

Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K. et al. (2022). Làm như tôi có thể, không phải như tôi nói: Định vị ngôn ngữ trong khả năng robot. arXiv preprint arXiv:2204.01691.

Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. và Zhou, D. (2022). Thuật toán học nào trong học trong ngữ cảnh? Điều tra với các mô hình tuyến tính. arXiv preprint arXiv:2211.15661.

Beck, J. (2008). Trò chơi tổ hợp: Lý thuyết Tic-Tac-Toe.

Betts, J. T. (1998). Khảo sát các phương pháp số cho tối ưu hóa quỹ đạo. Tạp chí Hướng dẫn, Điều khiển, và Động lực.
27

--- TRANG 28 ---
Bickel, P. J. và Freedman, D. A. (1981). Một số lý thuyết tiệm cận cho bootstrap. The annals of statistics, 91196–1217.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. et al. (2020). Các mô hình ngôn ngữ là những người học few-shot. Tiến bộ trong hệ thống xử lý thông tin nơ-ron, 331877–1901.

Cai, Q., Yang, Z., Jin, C. và Wang, Z. (2020). Khám phá hiệu quả có thể chứng minh trong tối ưu hóa chính sách. Trong Hội nghị Quốc tế về Học Máy.

Cai, T., Wang, X., Ma, T., Chen, X. và Zhou, D. (2023). Các mô hình ngôn ngữ lớn như những nhà chế tạo công cụ. arXiv preprint arXiv:2305.17126.

Chen, Y., He, J. và Gu, Q. (2022). Về độ phức tạp mẫu của việc học MDP nhân tuyến tính chân trời vô hạn giảm giá. Trong Hội nghị Quốc tế về Học Máy.

Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S. et al. (2022). PaLM: Mở rộng mô hình ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311.

Chua, K., Calandra, R., McAllister, R. và Levine, S. (2018). Học tăng cường sâu trong một số lần thử sử dụng các mô hình động lực xác suất. Tiến bộ trong hệ thống xử lý thông tin nơ-ron, 31.

Creswell, A. và Shanahan, M. (2022). Lý luận trung thành sử dụng các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2208.14271.

Creswell, A., Shanahan, M. và Higgins, I. (2022). Selection-inference: Khai thác các mô hình ngôn ngữ lớn cho lý luận logic có thể diễn giải. arXiv preprint arXiv:2205.09712.

Davison, A. C. và Hinkley, D. V. (1997). Các phương pháp Bootstrap và ứng dụng của chúng. 1, Cambridge university press.

Dong, K., Wang, Y., Chen, X. và Wang, L. (2019). Q-learning với khám phá UCB hiệu quả mẫu cho MDP chân trời vô hạn. arXiv preprint arXiv:1901.09311.

Efron, B. (1982). The jackknife, the bootstrap và các kế hoạch lấy mẫu lại khác. SIAM.

Fan, J., Wang, Z., Xie, Y. và Yang, Z. (2020). Phân tích lý thuyết của deep q-learning. Trong Học cho động lực và điều khiển. PMLR.

Garg, S., Tsipras, D., Liang, P. S. và Valiant, G. (2022). Transformer có thể học gì trong ngữ cảnh? Một nghiên cứu trường hợp của các lớp hàm đơn giản. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron.

Ghavamzadeh, M., Mannor, S., Pineau, J., Tamar, A. et al. (2015). Học tăng cường Bayesian: Một khảo sát. Foundations and Trends® in Machine Learning, 8359–483.

28

--- TRANG 29 ---
Ghosh, M. (2021). Ràng buộc đuôi mũ cho các biến ngẫu nhiên chi-bình phương. Journal of Statistical Theory and Practice, 1535.

Gruver, N., Finzi, M., Qiu, S. và Wilson, A. G. (2023). Các mô hình ngôn ngữ lớn là những người dự báo chuỗi thời gian zero-shot. arXiv preprint arXiv:2310.07820.

Guo, H., Liu, Z., Zhang, Y. và Wang, Z. (2024). Các mô hình ngôn ngữ lớn có thể chơi trò chơi không? một nghiên cứu trường hợp của cách tiếp cận tự chơi. arXiv preprint arXiv:2403.05632.

Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H. và Davidson, J. (2019). Học động lực ẩn cho lập kế hoạch từ pixel. Trong Hội nghị quốc tế về học máy. PMLR.

Hao, B., Abbasi Yadkori, Y., Wen, Z. và Cheng, G. (2019). Bootstrapping ràng buộc tin cậy trên. Tiến bộ trong hệ thống xử lý thông tin nơ-ron, 32.

Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z. và Hu, Z. (2023). Lý luận với mô hình ngôn ngữ là lập kế hoạch với mô hình thế giới. arXiv preprint arXiv:2305.14992.

Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A. et al. (2022). Huấn luyện các mô hình ngôn ngữ tối ưu tính toán. arXiv preprint arXiv:2203.15556.

Huang, W., Abbeel, P., Pathak, D. và Mordatch, I. (2022a). Các mô hình ngôn ngữ như những người lập kế hoạch zero-shot: Trích xuất kiến thức có thể hành động cho các tác nhân hiện thân. Trong Hội nghị Quốc tế về Học Máy.

Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y. et al. (2022b). Độc thoại nội tâm: Lý luận hiện thân thông qua lập kế hoạch với các mô hình ngôn ngữ. arXiv preprint arXiv:2207.05608.

Janner, M., Fu, J., Zhang, M. và Levine, S. (2019). Khi nào tin tưởng mô hình của bạn: Tối ưu hóa chính sách dựa trên mô hình. Tiến bộ trong hệ thống xử lý thông tin nơ-ron, 32.

Jiang, H. (2023). Lý thuyết không gian ẩn cho khả năng nổi lên trong các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2304.09960.

Jin, C., Yang, Z., Wang, Z. và Jordan, M. I. (2020). Học tăng cường hiệu quả có thể chứng minh với xấp xỉ hàm tuyến tính. Trong Hội nghị về Lý thuyết Học. PMLR.

Kim, G., Baldi, P. và McAleer, S. (2023). Các mô hình ngôn ngữ có thể giải quyết các nhiệm vụ máy tính. arXiv preprint arXiv:2303.17491.

Kirsch, L., Harrison, J., Sohl-Dickstein, J. và Metz, L. (2022). Học trong ngữ cảnh mục đích chung bằng meta-learning transformer. arXiv preprint arXiv:2212.04458.

29

--- TRANG 30 ---
Lazaric, A., Ghavamzadeh, M. và Munos, R. (2010). Phân tích thuật toán lặp chính sách dựa trên phân loại. Trong ICML-27th International Conference on Machine Learning. Omnipress.

Lee, J. N., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum, O. và Brunskill, E. (2023). Tiền huấn luyện có giám sát có thể học học tăng cường trong ngữ cảnh. arXiv preprint arXiv:2306.14892.

Li, B. Z., Nye, M. và Andreas, J. (2022). Mô hình ngôn ngữ với các tình huống ẩn. arXiv preprint arXiv:2212.10012.

Li, Y., Ildiz, M. E., Papailiopoulos, D. và Oymak, S. (2023). Transformer như thuật toán: Tổng quát hóa và lựa chọn mô hình ẩn trong học trong ngữ cảnh. arXiv preprint arXiv:2301.07067.

Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A. et al. (2022). Đánh giá toàn diện các mô hình ngôn ngữ. arXiv preprint arXiv:2211.09110.

Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J. và Stone, P. (2023a). LLM+P: Trao quyền cho các mô hình ngôn ngữ lớn với thành thạo lập kế hoạch tối ưu. arXiv preprint arXiv:2304.11477.

Liu, Z., Lu, M., Wang, Z., Jordan, M. và Yang, Z. (2022a). Tối đa hóa phúc lợi trong cân bằng cạnh tranh: Học tăng cường cho nền kinh tế trao đổi Markov. Trong Hội nghị Quốc tế về Học Máy. PMLR.

Liu, Z., Lu, M., Xiong, W., Zhong, H., Hu, H., Zhang, S., Zheng, S., Yang, Z. và Wang, Z. (2023b). Tối đa hóa để khám phá: Một hàm mục tiêu hợp nhất ước tính, lập kế hoạch, và khám phá. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron, tập. 36.

Liu, Z., Lu, M., Xiong, W., Zhong, H., Hu, H., Zhang, S., Zheng, S., Yang, Z. và Wang, Z. (2024). Tối đa hóa để khám phá: Một hàm mục tiêu hợp nhất ước tính, lập kế hoạch, và khám phá. Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron, 36.

Liu, Z., Zhang, Y., Fu, Z., Yang, Z. và Wang, Z. (2022b). Học từ minh họa: Bắt chước chính sách đối nghịch hiệu quả có thể chứng minh với xấp xỉ hàm tuyến tính. Trong Hội nghị quốc tế về học máy. PMLR.

Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y. N., Zhu, S.-C. và Gao, J. (2023). Chameleon: Lý luận có thể kết hợp plug-and-play với các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2304.09842.

Lu, X. và Van Roy, B. (2019). Ràng buộc tin cậy lý thuyết thông tin cho học tăng cường. Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron.

30

--- TRANG 31 ---
Morari, M. và Lee, J. H. (1999). Điều khiển dự đoán mô hình: quá khứ, hiện tại và tương lai. Computers & chemical engineering, 23667–682.

Newton, M. A. và Raftery, A. E. (1994). Suy luận Bayesian gần đúng với bootstrap khả năng có trọng số. Journal of the Royal Statistical Society Series B: Statistical Methodology, 563–26.

Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A. et al. (2022). Học trong ngữ cảnh và đầu quy nạp. arXiv preprint arXiv:2209.11895.

OpenAI (2023). Báo cáo kỹ thuật GPT-4.

Osband, I., Blundell, C., Pritzel, A. và Van Roy, B. (2016). Khám phá sâu thông qua bootstrapped dqn. Tiến bộ trong hệ thống xử lý thông tin nơ-ron, 29.

Osband, I., Russo, D. và Van Roy, B. (2013). Học tăng cường (Hiệu quả hơn) thông qua lấy mẫu hậu nghiệm. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron.

Paul, D., Ismayilzada, M., Peyrard, M., Borges, B., Bosselut, A., West, R. và Faltings, B. (2023). REFINER: Phản hồi lý luận trên các biểu diễn trung gian. arXiv preprint arXiv:2304.01904.

Pouplin, T., Sun, H., Holt, S. và Van der Schaar, M. (2024). Quá trình suy nghĩ tăng cường truy xuất như ra quyết định tuần tự. arXiv preprint arXiv:2402.07812.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. và Sutskever, I. (2019). Các mô hình ngôn ngữ là những người học đa nhiệm không giám sát.

Rawlings, J. B. (2000). Tổng quan hướng dẫn điều khiển dự đoán mô hình. IEEE Control Systems Magazine.

Razeghi, Y., Logan IV, R. L., Gardner, M. và Singh, S. (2022). Tác động của tần suất thuật ngữ tiền huấn luyện đến lý luận few-shot. arXiv preprint arXiv:2202.07206.

Russo, D. và Van Roy, B. (2014a). Học tối ưu hóa thông qua lấy mẫu hướng dẫn thông tin. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron.

Russo, D. và Van Roy, B. (2014b). Học tối ưu hóa thông qua lấy mẫu hậu nghiệm. Mathematics of Operations Research.

Russo, D. và Van Roy, B. (2016). Phân tích lý thuyết thông tin của lấy mẫu Thompson. Journal of Machine Learning Research.

Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D. và Pathak, D. (2020). Lập kế hoạch khám phá thông qua các mô hình thế giới tự giám sát. Trong Hội nghị Quốc tế về Học Máy. PMLR.

31

--- TRANG 32 ---
Sel, B., Al-Tawaha, A., Khattar, V., Wang, L., Jia, R. và Jin, M. (2023). Thuật toán của suy nghĩ: Tăng cường khám phá ý tưởng trong các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2308.10379.

Shen, Y., Song, K., Tan, X., Li, D., Lu, W. và Zhuang, Y. (2023). HuggingGPT: Giải quyết các nhiệm vụ AI với ChatGPT và bạn bè của nó trong HuggingFace. arXiv preprint arXiv:2303.17580.

Shin, S., Lee, S.-W., Ahn, H., Kim, S., Kim, H., Kim, B., Cho, K., Lee, G., Park, W., Ha, J.-W. et al. (2022). Về ảnh hưởng của corpora tiền huấn luyện đến học trong ngữ cảnh bởi một mô hình ngôn ngữ quy mô lớn. arXiv preprint arXiv:2204.13509.

Shinn, N., Cassano, F., Labash, B., Gopinath, A., Narasimhan, K. và Yao, S. (2023). Reflexion: Tác nhân ngôn ngữ với học tăng cường bằng lời. arXiv preprint arXiv:2303.11366.

Shridhar, M., Yuan, X., Côté, M.-A., Bisk, Y., Trischler, A. và Hausknecht, M. (2020). ALFworld: Căn chỉnh văn bản và môi trường hiện thân cho học tương tác. arXiv preprint arXiv:2010.03768.

Singh, K. (1981). Về độ chính xác tiệm cận của bootstrap của Efron. The Annals of Statistics 1187–1195.

Strens, M. (2000). Khung Bayesian cho học tăng cường. Trong Hội nghị Quốc tế về Học Máy.

Sun, H. (2023). Học tăng cường trong kỷ nguyên của LLM: Cái gì thiết yếu? cái gì cần thiết? một quan điểm RL về RLHF, prompting, và hơn thế nữa.

Sun, H., Hüyük, A. và van der Schaar, M. (2023a). Đánh giá và tối ưu hóa prompt phụ thuộc truy vấn với RL nghịch đảo ngoại tuyến. Trong Hội nghị Quốc tế thứ Mười hai về Biểu diễn Học.

Sun, H., Zhuang, Y., Kong, L., Dai, B. và Zhang, C. (2023b). AdaPlanner: Lập kế hoạch thích ứng từ phản hồi với các mô hình ngôn ngữ. arXiv preprint arXiv:2305.16653.

Sutton, R. S. và Barto, A. G. (2018). Học tăng cường: Một giới thiệu.

Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C. và Bau, D. (2023). Vector hàm trong các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2310.15213.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F. et al. (2023). LLaMa: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. arXiv preprint arXiv:2302.13971.

Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A. và Vladymyrov, M. (2023). Transformer học trong ngữ cảnh bằng gradient descent. Trong Hội nghị Quốc tế về Học Máy.

32

--- TRANG 33 ---
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A. và Zhou, D. (2022a). Tự nhất quán cải thiện lý luận chuỗi suy nghĩ trong các mô hình ngôn ngữ. arXiv preprint arXiv:2203.11171.

Wang, X., Zhu, W. và Wang, W. Y. (2023a). Các mô hình ngôn ngữ lớn là các mô hình chủ đề ẩn: Giải thích và tìm kiếm các minh chứng tốt cho học trong ngữ cảnh. arXiv preprint arXiv:2301.11916.

Wang, Z., Cai, S., Liu, A., Ma, X. và Liang, Y. (2023b). Mô tả, giải thích, lập kế hoạch và chọn: Lập kế hoạch tương tác với các mô hình ngôn ngữ lớn cho phép các tác nhân đa nhiệm thế giới mở. arXiv preprint arXiv:2302.01560.

Wang, Z., Pan, T., Zhou, Q. và Wang, J. (2023c). Khám phá hiệu quả trong học tăng cường hạn chế tài nguyên. Proceedings of the AAAI Conference on Artificial Intelligence, 3710279–10287.

Wang, Z., Wang, J., Zhou, Q., Li, B. và Li, H. (2022b). Học tăng cường hiệu quả mẫu thông qua actor-critic dựa trên mô hình bảo thủ. Trong Proceedings of the AAAI Conference on Artificial Intelligence, tập. 36.

Wasserman, L. (2000). Lựa chọn mô hình Bayesian và trung bình mô hình. Journal of mathematical psychology, 4492–107.

Wei, C.-Y., Jahromi, M. J., Luo, H., Sharma, H. và Jain, R. (2020). Học tăng cường không có mô hình trong các quá trình quyết định Markov phần thưởng trung bình chân trời vô hạn. Trong Hội nghị Quốc tế về Học Máy.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V. và Zhou, D. (2022). Prompting chuỗi suy nghĩ gợi lý luận trong các mô hình ngôn ngữ lớn. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron.

Wies, N., Levine, Y. và Shashua, A. (2023). Khả năng học của học trong ngữ cảnh. arXiv preprint arXiv:2303.07895.

Wies, N., Levine, Y. và Shashua, A. (2024). Khả năng học của học trong ngữ cảnh. Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron, 36.

Xie, S. M., Raghunathan, A., Liang, P. và Ma, T. (2021). Giải thích học trong ngữ cảnh như suy luận Bayesian ẩn. arXiv preprint arXiv:2111.02080.

Yang, L. và Wang, M. (2019). Q-learning tham số tối ưu mẫu sử dụng các đặc trưng cộng tuyến tính. Trong Hội nghị Quốc tế về Học Máy.

Yang, L. và Wang, M. (2020). Học tăng cường trong không gian đặc trưng: Matrix bandit, kernel, và ràng buộc hối tiếc. Trong Hội nghị Quốc tế về Học Máy.

33

--- TRANG 34 ---
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y. và Narasimhan, K. (2023a). Cây suy nghĩ: Giải quyết vấn đề có chủ ý với các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2305.10601.

Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. và Cao, Y. (2022). ReAct: Phối hợp lý luận và hành động trong các mô hình ngôn ngữ. arXiv preprint arXiv:2210.03629.

Yao, Y., Li, Z. và Zhao, H. (2023b). Ngoài chuỗi suy nghĩ, lý luận đồ thị suy nghĩ hiệu quả trong các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2305.16582.

Zelikman, E., Wu, Y., Mu, J. và Goodman, N. (2022). STaR: Bootstrapping lý luận với lý luận. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Nơ-ron.

Zhang, S., Zheng, S., Ke, S., Liu, Z., Jin, W., Yuan, J., Yang, Y., Yang, H. và Wang, Z. (2024). LLM có thể hướng dẫn RL như thế nào? một cách tiếp cận dựa trên giá trị. arXiv preprint arXiv:2402.16181.

Zhang, Y., Cai, Q., Yang, Z. và Wang, Z. (2020). Học bắt chước đối nghịch tạo sinh với tham số hóa mạng nơ-ron: Tính tối ưu toàn cục và tỷ lệ hội tụ. Trong Hội nghị Quốc tế về Học Máy. PMLR.

Zhang, Y., Liu, B., Cai, Q., Wang, L. và Wang, Z. (2022). Phân tích attention thông qua lăng kính của tính trao đổi và các mô hình biến ẩn. arXiv preprint arXiv:2212.14852.

Zhang, Y., Yang, J., Yuan, Y. và Yao, A. C.-C. (2023a). Lý luận tích lũy với các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2308.04371.

Zhang, Y., Zhang, F., Yang, Z. và Wang, Z. (2023b). Học trong ngữ cảnh học gì và như thế nào? Trung bình mô hình Bayesian, tham số hóa, và tổng quát hóa. arXiv preprint arXiv:2305.19420.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. et al. (2023). Đánh giá LLM-as-a-judge với MT-bench và chatbot arena. arXiv preprint arXiv:2306.05685.

Zheng, S., Wang, L., Qiu, S., Fu, Z., Yang, Z., Szepesvari, C. và Wang, Z. (2022). Khám phá lạc quan với các đặc trưng đã học giải quyết có thể chứng minh các quá trình quyết định Markov với động lực nơ-ron. Trong Hội nghị Quốc tế thứ Mười một về Biểu diễn Học.

Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z. và Zhang, T. (2022). Gec: Một khung thống nhất cho ra quyết định tương tác trong MDP, POMDP, và hơn thế nữa. arXiv preprint arXiv:2211.01962.

Zhou, D., Gu, Q. và Szepesváti, C. (2021a). Học tăng cường gần như tối ưu minimax cho các quá trình quyết định Markov hỗn hợp tuyến tính. Trong Hội nghị về Lý thuyết Học.

Zhou, D., He, J. và Gu, Q. (2021b). Học tăng cường hiệu quả có thể chứng minh cho MDP giảm giá với ánh xạ đặc trưng. Trong Hội nghị Quốc tế về Học Máy.

34

--- TRANG 35 ---
A Ký hiệu
Chúng tôi cung cấp một bảng ký hiệu ở đây.

Ký hiệu Giải thích
ξ(s,a) cặp của trạng thái tiếp theo và phần thưởng hiện tại (s′, r), cho cặp trạng thái-hành động truy vấn (s, a)
PLLM(ξ(s,a)|D, s, a) thước đo xác suất của LLM dự đoán ξ(s,a) cho bộ đệm bộ nhớ D được nhắc như ngữ cảnh
rLLM(D) bộ ước tính phần thưởng LLM với bộ đệm bộ nhớ D được nhắc như ngữ cảnh
PLLM(D) bộ ước tính nhân chuyển tiếp LLM với bộ đệm bộ nhớ D được nhắc như ngữ cảnh
Dt lịch sử tại bước thứ t, bao gồm {(si, ai, ri, si+1)}t−1_i=0
P0(θ) tiên nghiệm của θ⋆
Ppost(θ|D) hậu nghiệm của θ⋆ có điều kiện trên D
Pt(θ) viết tắt của Ppost(θ|Dt)
H(θ|D) entropy hậu nghiệm của hậu nghiệm của θ có điều kiện trên D
I(θ;ξ|D) tăng thông tin của ξ, được định nghĩa bởi H(θ|D)−H(θ, ξ|D)
Ht viết tắt của H(θ|Dt)
tk bước thời gian khi RAFA chuyển đổi chính sách lần thứ k
πk viết tắt của πtk
Bθ toán tử Bellman được gây ra bởi θ
Bk toán tử Bellman được gây ra bởi LLM(Dtk)
dTV(p∥q) biến phân tổng (TV) giữa hai thước đo xác suất p và q
dKL(p∥q) divergence Kullback–Leibler giữa hai thước đo xác suất p và q
Vx∼p[f(x)] phương sai của f(X), trong đó X tuân theo phân phối p
(PV)(s, a) Es′∼P(·|s,a)[V(s′)]
L ràng buộc của |r+V(s)| cho bất kỳ r∈ R, s∈ S, và giá trị V
ν⋆(·|s) thước đo viếng thăm γ-giảm giá tối ưu bắt đầu từ trạng thái s
N tập hợp các số tự nhiên
1(x=y) chỉ báo với giá trị 1 nếu x bằng y và giá trị 0 nếu ngược lại
E kỳ vọng
V phương sai
35

--- TRANG 36 ---
B Thêm Thuật toán
Tùy thuộc vào cấu hình cụ thể của không gian trạng thái và hành động (liên tục so với rời rạc) và mô hình chuyển tiếp và phần thưởng (ngẫu nhiên so với xác định), chúng ta có thể chọn mô phỏng thuật toán tìm kiếm cây, thuật toán lặp giá trị, thuật toán bắn ngẫu nhiên, hoặc thuật toán MCTS. Tất cả chúng đều cho phép RAFA đạt được đảm bảo hiệu quả mẫu có thể chứng minh miễn là chúng thỏa mãn một yêu cầu cụ thể về tính tối ưu (Định nghĩa 5.1). Để minh họa, chúng tôi mô phỏng thuật toán beam-search (một phiên bản nâng cao của thuật toán tìm kiếm cây) trong Thuật toán 7 và thuật toán MCTS trong Thuật toán 8. Cho thảo luận lý thuyết, chúng tôi trình bày thuật toán lặp giá trị trong Thuật toán 3.

Thuật toán 7 Người học-lập kế hoạch LLM (LLM-LR-PL): Một ví dụ beam-search (cho trường hợp xác định).
1:đầu vào : Bộ đệm bộ nhớ D, trạng thái ban đầu s, độ rộng đề xuất L, độ rộng tìm kiếm B, và độ sâu tìm kiếm U.
2:khởi tạo : Khởi tạo mảng trạng thái S0← {s} và mảng hành động A0←∅.
————————————— (thói quen con học) —————————————–
3:Đặt Model như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để tạo trạng thái tiếp theo.
4:Đặt Critic như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để ước tính hàm giá trị.
————————————— (thói quen con lập kế hoạch) —————————————–
5:Đặt Elite như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để tạo nhiều hành động ứng viên.
6:cho u= 0, . . . , U thực hiện
7: Đối với mỗi trạng thái hiện tại su trong Su, gọi Elite để tạo L hành động ứng viên.
8: Đối với mỗi hành động ứng viên a(ℓ)_u, gọi Model để tạo trạng thái tiếp theo s(ℓ)_u+1 và phần thưởng nhận được r(ℓ)_u.
9: Đối với mỗi bộ ba kết quả (su, a(ℓ)_u, s(ℓ)_u+1, r(ℓ)_u), gọi Critic để đánh giá phần thưởng tích lũy tương lai dự kiến Q̂(su, a(ℓ)_u)←r(ℓ)_u+γV̂(s(ℓ)_u+1), trong đó V̂ được cho bởi Critic.
10: Chọn B bộ ba tốt nhất (su, a(ℓ)_u, s(ℓ)_u+1) với giá trị Q̂(su, a(ℓ)_u) cao nhất và ghi chúng vào Su× A u× S u+1.
11:kết thúc cho
12:Cho B rollout được bảo tồn trong S0× A 0× ··· × S U× A U× S U+1, gọi Critic để đánh giá phần thưởng tích lũy tương lai dự kiến ∑U_u=0γur(b)_u+γU+1V̂(s(b)_U+1) và chọn rollout tốt nhất (s†0, a†0, . . . , s†U, a†U, s†U+1), trong đó V̂ được cho bởi Critic và s†0=s.
13:đầu ra : Hành động ban đầu a†0 của rollout được chọn.
36

--- TRANG 37 ---
Thuật toán 8 Người học-lập kế hoạch LLM (LLM-PL) cho RAFA: Một ví dụ tìm kiếm cây Monte-Carlo (cho trường hợp ngẫu nhiên).
1:đầu vào : Bộ đệm bộ nhớ D, trạng thái ban đầu s, độ rộng đề xuất L,L′, và ngân sách mở rộng E.
2:khởi tạo : Khởi tạo nút gốc n←s và hàm con c(·)←∅.
————————————— (thói quen con học) —————————————–
3:Đặt Model như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để tạo trạng thái tiếp theo.
4:Đặt Critic như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để ước tính hàm giá trị.
————————————— (thói quen con lập kế hoạch) —————————————–
5:Đặt Elite như một thể hiện LLM được nhắc sử dụng D như ngữ cảnh để tạo nhiều hành động ứng viên.
6:cho e= 0, . . . , E thực hiện
7: Đặt se←n.
8:trong khi se không phải là nút lá, tức là, c(se)̸=∅, thực hiện
9: Gọi Critic để đánh giá phần thưởng tích lũy tương lai dự kiến và chọn nút con ae trong c(se) với giá trị Q̂(se, ae) cao nhất.
10: Đặt se như một nút con trong c(ae).
11: kết thúc while
12: Đối với trạng thái hiện tại se, gọi Elite để tạo L hành động ứng viên.
13: Ghi mỗi hành động ứng viên a(ℓ)_e vào c(se), tức là, c(se)← {a(ℓ)_e}L_ℓ=1.
14: Đối với mỗi hành động ứng viên a(ℓ)_e, gọi Model để lấy mẫu L′ trạng thái tiếp theo.
15: Ghi mỗi trạng thái tiếp theo s(ℓ,ℓ′)_e vào c(a(ℓ)_e), tức là, c(a(ℓ)_e)← {s(ℓ,ℓ′)_e}L′_ℓ′=1.
16: Đối với mỗi trạng thái được tạo s(ℓ,ℓ′)_e, gọi Critic để đánh giá phần thưởng tích lũy tương lai dự kiến và cập nhật giá trị ước tính V̂ cho tất cả các nút tổ tiên. (Tùy chọn)
17:kết thúc cho
18:Đặt s†0←n và i←0.
19:trong khi s†i không phải là nút lá, tức là, c(s†i)̸=∅, thực hiện
20: Gọi Critic để đánh giá phần thưởng tích lũy tương lai dự kiến và chọn nút con a†i+1 trong c(s†i) với giá trị Q̂(s†i, a†i) cao nhất.
21: Đặt s†i+1 như một nút con trong c(a†i) và i←i+ 1.
22:kết thúc while
23:đầu ra : Hành động ban đầu a†0 của rollout được chọn (s†0, a†0, . . . , s†i, a†i).
37

--- TRANG 38 ---
C Chứng minh Chính
C.1 Chứng minh Mệnh đề 5.2
Chứng minh Mệnh đề 5.2. Bây giờ chúng tôi chứng minh rằng thuật toán lặp giá trị với chân trời cắt ngắn U (Thuật toán 3) thỏa mãn định nghĩa của người lập kế hoạch ϵ-tối ưu (Định nghĩa 5.1), trong đó U phụ thuộc vào ϵ. Để đơn giản ký hiệu, chúng tôi ký hiệu max s∈S và max a∈A như max s và max a.

Gọi
ϵ†= max s,a |Q(1)(s, a)−r(s, a)−γ(PV(1))(s, a)|. (C.1)

Lưu ý rằng phân tích hội tụ của thuật toán lặp giá trị trong Sutton và Barto (2018) cho
max s,a |Q(1)(s, a)−Q(2)(s, a)|≤γU−2 max s,a |Q(U−1)(s, a)−Q(U)(s, a)|,
điều này ngụ ý
max s,a |Q(1)(s, a)−Q(2)(s, a)|≤γU−2L. (C.2)

Chúng ta có
ϵ†= max s,a |Q(1)_θ(s, a)−r(s, a)−γ(PV(2))(s, a)|
+γEs′∼P(·|s,a)[|V(1)(s′)−V(2)(s′)|]
=γ·max s,a Es′∼P(·|s,a)[|V(1)(s′)−V(2)(s′)|]
=γ·max s,a Es′∼P(·|s,a)[|max a Q(1)(s′, a)−max a Q(2)(s′, a)|]
≤γ·max s,a Es′∼P(·|s,a)[max a |Q(1)(s′, a)−Q(2)(s′, a)|]
≤γU−1L, (C.3)

trong đó đẳng thức đầu tiên và thứ ba dựa trên Thuật toán 3, bất đẳng thức thứ hai cuối sử dụng tính chất co của toán tử maximum, và bất đẳng thức cuối sử dụng (C.2). Để cho ϵ†< ϵ, đủ để đặt U≥1 +⌈logγ(ϵ/L)⌉. Lưu ý rằng chính sách π được trả về bởi Thuật toán 3 thỏa mãn π(s) = argmaxaQ(1)(s, a). Vì vậy, chúng tôi chứng minh Mệnh đề 5.2.

C.2 LLM với Căn chỉnh Hậu nghiệm Thực hiện BMA
Chứng minh Mệnh đề 5.4. Nhớ lại rằng PLLM(D) và rLLM(D) là nhân chuyển tiếp ước tính và hàm phần thưởng được gây ra bởi PLLM thỏa mãn Giả định 5.3. Đối với bất kỳ cặp trạng thái-hành động truy vấn (s, a) và tập dữ liệu trong ngữ cảnh D, điều đó xảy ra

(PLLM(D)V)(s, a) =∫S V(s′)PLLM(D)(ds′|s, a)
=∫S V(s′)∫Θ Pθ(ds′|s, a)Ppost(dθ|D)
=∫Θ Ppost(dθ|D)∫S Pθ(ds′|s, a)V(s′)
=Eθ∼Ppost(·|D)[(PθV)(s, a)], (C.4)

trong đó đẳng thức thứ hai sử dụng Giả định 5.3 (Căn chỉnh Hậu nghiệm), đẳng thức thứ ba sử dụng định lý Fubin, và đẳng thức cuối sử dụng (2.4). Đối với bất kỳ cặp trạng thái-hành động truy vấn (s, a) và tập dữ liệu trong ngữ cảnh D, điều đó xảy ra

rLLM(D)(s, a) =EPLLM[r|D, s, a]
=EPpost[r|D, s, a]
=Eθ∼Ppost(·|D)[rθ(s, a)], (C.5)

trong đó đẳng thức thứ hai sử dụng Giả định 5.3 (Căn chỉnh Hậu nghiệm) và đẳng thức cuối sử dụng (2.4). Bởi tính tuyến tính của kỳ vọng, chúng tôi kết hợp (C.4) và (C.5) để có

rLLM(D)(s, a) +γ·(PLLM(D)V)(s, a) =Eθ∼Ppost(·|D)[rθ(s, a) +γ·(PθV)(s, a)]
=Eθ∼Ppost(·|D)[(BθV)(s, a)],

trong đó đẳng thức cuối sử dụng định nghĩa của Bθ. Vì vậy, chúng tôi hoàn thành chứng minh Mệnh đề 5.4.

C.3 Tính chất Co lại của Phương sai Hậu nghiệm
Mệnh đề C.1 (Tính chất Co lại của Phương sai Hậu nghiệm). Dưới Giả định 5.5, phương sai hậu nghiệm trong Thuật toán 4, 5, và 6 thỏa mãn hai tính chất sau:
(i) Vθ∼Ptk[(BθVt)(s, a)|Dtk] ≤2L2·I(θ;ξ(s,a)|Dtk)
(ii) E[∑K−1_k=0 Eπk[∑tk+1−1_t=tk Vθ∼Ptk[(BθVt)(st, at)|Dtk]]] ≤8ηL2·E[H0−HT],

trong đó chúng tôi ký hiệu ràng buộc trên của tổng của bất kỳ hàm giá trị và phần thưởng bởi một hằng số dương L, tức là, |r+V(s)| ≤L cho bất kỳ phần thưởng r, trạng thái s, và hàm giá trị ước tính V.

Chứng minh Mệnh đề C.1. Chúng tôi bắt đầu với chứng minh tính chất đầu tiên trong Mệnh đề C.1. Nhớ lại định nghĩa rằng ξ(s,a) ký hiệu các biến ngẫu nhiên (s′, r) trong MDP cơ bản cho trạng thái hiện tại s và hành động a. Định nghĩa rằng gt(ξ(s,a)) = (r+Vt(s′))/(2L). Vì tổng của bất kỳ phần thưởng và hàm giá trị nào được giới hạn bởi L, chúng ta biết rằng |gt| ≤ 1/2. cho bất kỳ tk≤t < t k+1, chúng ta có

2L·E[gt(ξ(s,a))|θ,Dtk] = (BθVt)(s, a)
2L·E[gt(ξ(s,a))|Dtk] =Eθ∼Pt[(BθVt)(s, a)], (C.6)

cho bất kỳ trạng thái truy vấn s và hành động a. Bởi dạng biến phân của khoảng cách biến phân tổng (TV) dTV, chúng ta có

d2_TV(P(ξ(s,a)|θ,Dt)∥P(ξ(s,a)|Dt)) = sup g:|g|≤1/2 |E[g(ξ(s,a))|θ,Dtk]−E[g(ξ(s,a))|Dtk]|2
≥ |E[gt(ξ(s,a))|θ,Dtk]−E[gt(ξ(s,a))|Dtk]|2
=1/(4L2)·|(BθVt)(s, a)−Eθ∼Pt[(BθVt)(s, a)]|2,(C.7)

trong đó đẳng thức cuối là kết quả của (C.6). Bằng cách lấy kỳ vọng đối với θ∼Pt trên (C.7), chúng ta có

Vθ∼Ptk[(BθVt)(s, a)|Dtk]
=Eθ∼Pt[|(BθVt)(s, a)−Eθ∼Pt[(BθVt)(s, a)]|2]
≤4L2·Eθ∼Pt[d2_TV(P(ξ(s,a)|θ,Dtk)∥P(ξ(s,a)|Dtk))]
≤2L2·Eθ∼Pt[dKL(P(ξ(s,a)|θ,Dtk)∥P(ξ(s,a)|Dtk))]
= 2L2·[H(ξ(s,a)|Dtk)−H(ξ(s,a)|θ,Dtk)]
= 2L2·I(ξ(s,a);θ|Dtk)
= 2L2·I(θ;ξ(s,a)|Dtk), (C.8)

trong đó đẳng thức đầu tiên sử dụng định nghĩa của phương sai, bất đẳng thức đầu tiên sử dụng (C.7) bằng cách lấy kỳ vọng đối với θ∼Pt, và bất đẳng thức thứ hai sử dụng bất đẳng thức Pinsker. Ở đây, đẳng thức thứ hai sử dụng định nghĩa của entropy và đẳng thức thứ hai cuối sử dụng định nghĩa của tăng thông tin. Ở đây, đẳng thức cuối sử dụng thực tế rằng I(X;Y) =I(Y;X) cho bất kỳ hai biến ngẫu nhiên X và Y. Vì vậy, chúng tôi hoàn thành chứng minh tính chất đầu tiên trong Mệnh đề C.1.

Tiếp theo, chúng tôi chứng minh tính chất thứ hai trong Mệnh đề C.1. Bởi thực tế rằng at=πk(st) cho bất kỳ tk≤t < t k+1, chúng ta có

E[∑K−1_k=0 Eπk[∑tk+1−1_t=tk Vθ∼Ptk[(BθVt)(st, at)|Dtk]]]
=E[∑K−1_k=0 Eπk[∑tk+1−1_t=tk Vθ∼Ptk[(BθVt)(st, πk(st))|Dtk]]]
≤2L2·E[∑K−1_k=0 Eπk[∑tk+1−1_t=tk I(θ;ξ(st,πk(st))|Dtk)]],

trong đó bất đẳng thức gọi (C.8). Dưới Giả định 5.5 và cùng điều kiện chuyển đổi trong Thuật toán 4, 5, và 6, chúng ta có

E[∑K−1_k=0 Eπk[∑tk+1−1_t=tk Vθ∼Ptk[(BθVt)(st, at)|Dtk]]]
≤8ηL2·E[∑K−1_k=0 Eπk[∑tk+1−1_t=tk I(θ;ξ(st,πk(st))|Dt)]]
≤8ηL2(H0−HT),

trong đó bất đẳng thức cuối sử dụng quy tắc chuỗi của tăng thông tin. Vì vậy, chúng tôi hoàn thành chứng minh tính chất thứ hai trong Mệnh đề C.1.

40
