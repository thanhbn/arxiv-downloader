# 2304.13276.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2304.13276.pdf
# Kích thước tệp: 239275 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2304.13276v1 [cs.CL] 26 Apr 2023 Sự Gần Gũi Giữa Học Trong Ngữ Cảnh và Dịch Chuyển Trọng Số cho Hồi Quy Softmax
Shuai Li∗Zhao Song†Yu Xia‡Tong Yu§Tianyi Zhou¶

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) được biết đến với hiệu suất vượt trội trong xử lý ngôn ngữ tự nhiên, khiến chúng có hiệu quả cao trong nhiều nhiệm vụ liên quan đến đời sống con người hoặc thậm chí là công việc. Cơ chế attention trong kiến trúc Transformer là một thành phần quan trọng của LLM, vì nó cho phép mô hình tập trung có chọn lọc vào các phần đầu vào cụ thể. Đơn vị softmax, là một phần chính của cơ chế attention, chuẩn hóa các điểm số attention. Do đó, hiệu suất của LLM trong các nhiệm vụ NLP khác nhau phụ thuộc đáng kể vào vai trò quan trọng của cơ chế attention với đơn vị softmax.

Học trong ngữ cảnh, là một trong những khả năng được ca ngợi của các LLM gần đây, là một khái niệm quan trọng trong việc truy vấn LLM như ChatGPT. Không cần cập nhật tham số thêm, Transformer có thể học dự đoán dựa trên một vài ví dụ trong ngữ cảnh. Tuy nhiên, lý do tại sao Transformer trở thành những người học trong ngữ cảnh chưa được hiểu rõ. Gần đây, một số nghiên cứu [ASA+22, GTLV22, ONR+22] đã nghiên cứu việc học trong ngữ cảnh từ góc độ toán học dựa trên công thức hồi quy tuyến tính min_x ||Ax-b||^2, cho thấy khả năng của Transformer trong việc học các hàm tuyến tính trong ngữ cảnh.

Trong nghiên cứu này, chúng tôi nghiên cứu việc học trong ngữ cảnh dựa trên công thức hồi quy softmax min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-b||^2 của cơ chế attention của Transformer. Chúng tôi chứng minh các cận trên của các phép biến đổi dữ liệu được tạo ra bởi một lớp self-attention đơn và bởi gradient descent trên mất mát hồi quy ℓ2 cho hàm dự đoán softmax, điều này ngụ ý rằng khi huấn luyện Transformer chỉ có self-attention cho các nhiệm vụ hồi quy cơ bản, các mô hình được học bởi gradient descent và Transformer thể hiện sự tương đồng lớn.

∗shuaili8@sjtu.edu.cn . Shanghai Jiao Tong University.
†zsong@adobe.com . Adobe Research.
‡xiayuu@umich.edu . University of Michigan.
§tyu@adobe.com . Adobe Research.
¶t8zhou@ucsd.edu . University of California San Diego.

--- TRANG 2 ---
1 Giới thiệu
Trong những năm gần đây, đã có sự gia tăng đáng kể trong nghiên cứu và phát triển trong lĩnh vực Trí tuệ Nhân tạo (AI), với các mô hình ngôn ngữ lớn (LLM) nổi lên như một cách hiệu quả để giải quyết các nhiệm vụ phức tạp. Transformer đã đạt được kết quả tiên tiến trong nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên, chẳng hạn như dịch máy [PCR19, GHG+20], mô hình hóa ngôn ngữ, trả lời câu hỏi và tạo văn bản [LSX+22]. Kết quả là, chúng đã trở thành kiến trúc được ưa chuộng cho NLP. Dựa trên kiến trúc đó, BERT [DCLT18], GPT-3 [BMR+20], PaLM [CND+22] và OPT [ZRG+22] đã được đề xuất. Chúng đã thể hiện khả năng học tập và lý luận đáng chú ý và đã được chứng minh là hiệu quả hơn so với các mô hình nhỏ hơn và các kỹ thuật truyền thống khi xử lý ngôn ngữ tự nhiên.

Thêm vào đó, LLM có thể được tinh chỉnh cho nhiều mục đích mà không cần xây dựng lại từ đầu, khiến chúng trở thành một công cụ linh hoạt cho các ứng dụng AI. Một ví dụ điển hình là ChatGPT, một phần mềm chat được phát triển bởi OpenAI sử dụng tối đa tiềm năng của GPT-3. Hơn nữa, GPT-4 [Ope23] có khả năng xử lý các nhiệm vụ phức tạp mà các phiên bản trước đó không thể hoàn thành. Nó đã thể hiện một mức độ thành thạo đáng chú ý có thể so sánh với hiệu suất của con người trong các tiêu chuẩn chuyên nghiệp và học thuật khác nhau.

Transformer có một loại kiến trúc mạng neural sequence-to-sequence cụ thể. Chúng sử dụng cơ chế attention [VSP+17, RNS+18, DCLT18, BMR+20] cho phép chúng nắm bắt hiệu quả các phụ thuộc tầm xa và ngữ cảnh từ dữ liệu đầu vào. Cốt lõi của cơ chế attention là ma trận attention bao gồm các hàng và cột, tương ứng với các từ hoặc "token" riêng lẻ. Ma trận attention biểu diễn các mối quan hệ trong văn bản đã cho. Nó đo lường tầm quan trọng của mỗi token trong một chuỗi khi nó liên quan đến đầu ra mong muốn. Trong quá trình huấn luyện, ma trận attention được học và tối ưu hóa để cải thiện độ chính xác của các dự đoán của mô hình. Thông qua cơ chế attention, mỗi token đầu vào được đánh giá dựa trên mức độ liên quan của nó đến đầu ra mong muốn bằng cách gán một điểm số token. Điểm số này được xác định bởi một hàm tương tự so sánh trạng thái đầu ra hiện tại với các trạng thái đầu vào.

Về lý thuyết, ma trận attention bao gồm ma trận query Q∈R^{n×d}, ma trận key K∈R^{n×d} và ma trận value V∈R^{n×d}. Theo [ZHDK23, AS23, BSZ23], việc tính toán hàm attention chuẩn hóa được định nghĩa là D^{-1}exp(QK^T)V. Theo tài liệu transformer, chúng tôi áp dụng exp cho ma trận theo cách từng phần tử. Ở đây D∈R^{n×n} là ma trận đường chéo được định nghĩa là D = diag(exp(QK^T)1_n). Trực quan, D biểu thị ma trận chuẩn hóa softmax. Một công thức tính toán tổng quát hơn có thể được viết là

D^{-1}exp(XQK^TX^T)V, D := diag(exp(XQK^TX^T)1_n)

Trong cài đặt trên, chúng tôi xem Q, K, V∈R^{d×d} là trọng số và X là dữ liệu câu đầu vào có độ dài n và mỗi kích thước nhúng từ là d. Trong phần còn lại, chúng tôi sẽ chuyển X sang ký hiệu A và sử dụng A để biểu thị câu.

Về mặt toán học, bài toán tính toán attention có thể được hình thức hóa như một bài toán hồi quy theo nghĩa sau:

Định nghĩa 1.1. Chúng tôi xem xét bài toán sau
min_{X∈R^{d×d}} ||D^{-1}exp(AXA^T)-B||_F
trong đó A∈R^{n×d} có thể được coi là một tài liệu có độ dài n và mỗi từ có kích thước nhúng d. Ở đây D = diag(AXA^T1_n). Với bất kỳ A∈R^{n×d} và B∈R^{n×n} cho trước, mục tiêu là tìm một số trọng số X để tối ưu hóa hàm mục tiêu trên.

--- TRANG 3 ---
Trái ngược với công thức trong [ZHDK23, AS23, BSZ23], tham số X trong Định nghĩa 1.1 tương đương với QK^T∈R^{d×d} trong phiên bản tổng quát của [ZHDK23, AS23, BSZ23] (ví dụ thay thế Q∈R^{n×d} bằng XQ trong đó X∈R^{n×d} và Q∈R^{d×d}. Tương tự cho K và V. Trong trường hợp như vậy, X có thể được xem là biểu diễn ma trận của một câu có độ dài n.). Một số nghiên cứu [ASA+22, GTLV22, ONR+22] nghiên cứu học trong ngữ cảnh từ góc độ toán học trong một cài đặt đơn giản hơn nhiều so với Định nghĩa 1.1, đó là công thức hồi quy tuyến tính (xem sau).

Định nghĩa 1.2. Cho một ma trận A∈R^{n×d} và b∈R^n, mục tiêu là giải
min_x ||Ax-b||^2

Một số nghiên cứu lý thuyết về transformer đã nghiên cứu hoặc hồi quy mũ [GMS23, LSZ23] hoặc bài toán hồi quy softmax [DLS23]. Trong nghiên cứu này, để tiến thêm một bước trong việc hiểu đơn vị softmax trong sơ đồ attention trong LLM. Chúng tôi xem xét hồi quy softmax sau và nghiên cứu hiện tượng học trong ngữ cảnh dựa trên nó.

Định nghĩa 1.3 (Hồi quy Softmax). Cho A∈R^{n×d} và một vector b∈R^n, mục tiêu là giải
min_{x∈R^d} ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-b||^2

Chúng tôi lưu ý rằng Định nghĩa 1.3 là một công thức nằm giữa Định nghĩa 1.2 và Định nghĩa 1.1.

1.1 Kết quả của chúng tôi
Chúng tôi nêu kết quả chính như sau:

Định lý 1.4 (Dịch chuyển có giới hạn cho Học trong ngữ cảnh, không chính thức của sự kết hợp Định lý 8.1 và Định lý 8.2). Nếu các điều kiện sau được thỏa mãn
• Cho A∈R^{n×d}.
• Cho b∈R^n.
• ||A|| ≤ R.
• Cho ||x||_2 ≤ R.
• ||A(x_{t+1}-x_t)||_∞ < 0.01.
• ||(A_{t+1}-A_t)x||_∞ < 0.01.
• Cho R ≥ 4.
• Cho M := n^{1.5}exp(10R^2).

Chúng tôi xem xét bài toán hồi quy softmax (Định nghĩa 1.3)
min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-b||^2.

• Phần 1. Nếu chúng tôi di chuyển x_t đến x_{t+1}, thì chúng tôi đang giải một bài toán hồi quy softmax mới với
min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-\tilde{b}||^2
trong đó
||\tilde{b}-b||^2 ≤ M·||x_{t+1}-x_t||^2

--- TRANG 4 ---
• Phần 2. Nếu chúng tôi di chuyển A_t đến A_{t+1}, thì chúng tôi đang giải một bài toán hồi quy softmax mới với
min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-\hat{b}||^2
trong đó
||\hat{b}-b||^2 ≤ M·||A_{t+1}-A_t||

Nhớ lại rằng A∈R^{n×d} biểu thị một tài liệu có độ dài n và mỗi từ có kích thước nhúng d và x biểu thị phiên bản đơn giản hóa của QK^T. Gradient descent một bước có thể được coi là một cập nhật cho trọng số x của mô hình. Do đó, phần 1 của kết quả chúng tôi (Định lý 1.4) ngụ ý rằng phép biến đổi dữ liệu của b được tạo ra bởi gradient descent trên mất mát hồi quy ℓ2 được giới hạn bởi M·||x_{t+1}-x_t||^2. Theo [ONR+22], để thực hiện học trong ngữ cảnh, việc cập nhật lớp self-attention có thể được coi là một cập nhật cho tài liệu được token hóa A. Do đó, phần 2 của kết quả chúng tôi (Định lý 1.4) ngụ ý rằng phép biến đổi dữ liệu của b được tạo ra bởi một lớp self-attention đơn được giới hạn bởi M·||A_{t+1}-A_t||.

Chúng tôi lưu ý rằng phép biến đổi dữ liệu của b được tạo ra bởi 1) một lớp self-attention đơn và bởi 2) gradient descent trên mất mát hồi quy ℓ2 đều được giới hạn. Phép biến đổi có giới hạn này đối với b ngụ ý rằng khi huấn luyện Transformer chỉ có self-attention cho các nhiệm vụ hồi quy cơ bản, các mô hình được học bởi gradient descent và Transformer thể hiện sự tương đồng lớn.

Lộ trình. Trong Mục 2, chúng tôi giới thiệu một số nghiên cứu liên quan. Trong Mục 3, chúng tôi đưa ra một số kiến thức sơ bộ. Trong Mục 4, chúng tôi tính toán gradient của hàm mất mát với hàm softmax đối với x. Những hàm đó bao gồm α(x)^{-1}, α(x) và f(x). Trong Mục 5, chúng tôi chứng minh Lipschitz cho hàm self-attention đối với x. Trong Mục 6, chúng tôi tính toán gradient của hàm mất mát với hàm softmax đối với A. Trong Mục 7, chúng tôi chứng minh Lipschitz cho hàm self-attention đối với A. Trong Mục 8, chúng tôi đưa ra kết quả chính của bài báo.

2 Nghiên cứu liên quan

2.1 Học trong ngữ cảnh
[ASA+22] chỉ ra rằng những người học trong ngữ cảnh dựa trên Transformer có thể thực hiện các thuật toán học tập truyền thống một cách ngầm định. Điều này được thực hiện bằng cách mã hóa các mô hình nhỏ hơn trong các kích hoạt nội bộ của chúng. Những mô hình nhỏ hơn này được cập nhật bởi ngữ cảnh đã cho. Họ nghiên cứu lý thuyết về các thuật toán học tập mà bộ giải mã Transformer có thể thực hiện. Họ chứng minh rằng Transformer chỉ cần một số lượng hạn chế các lớp và đơn vị ẩn để thực hiện các thuật toán hồi quy tuyến tính khác nhau. Đối với các bài toán hồi quy d chiều, một Transformer có kích thước ẩn O(d) có thể thực hiện một bước gradient descent. Họ cũng chứng minh rằng Transformer có kích thước ẩn O(d^2) có thể cập nhật một bài toán hồi quy ridge. Nghiên cứu cho thấy rằng Transformer về mặt lý thuyết có khả năng thực hiện nhiều thuật toán hồi quy tuyến tính.

[GTLV22] tập trung vào huấn luyện Transformer để học các hàm nhất định, trong điều kiện trong ngữ cảnh. Mục tiêu là có hiểu biết toàn diện hơn về học trong ngữ cảnh và xác định xem Transformer có thể học phần lớn các hàm trong một lớp đã cho sau khi huấn luyện không. Họ phát hiện rằng học trong ngữ cảnh là khả thi ngay cả khi có sự dịch chuyển phân phối giữa dữ liệu huấn luyện và suy luận hoặc giữa các ví dụ trong ngữ cảnh và đầu vào truy vấn. Ngoài ra, họ phát hiện rằng Transformer có thể học các lớp hàm phức tạp hơn như hàm tuyến tính thưa thớt, mạng neural hai lớp và cây quyết định. Những Transformer được huấn luyện này có hiệu suất tương đương với các thuật toán học tập cụ thể cho từng nhiệm vụ.

--- TRANG 5 ---
[ONR+22] chứng minh và cung cấp lời giải thích về sự tương đồng giữa quá trình huấn luyện của Transformer trong các nhiệm vụ trong ngữ cảnh và một số công thức meta-learning dựa trên gradient descent. Trong quá trình huấn luyện Transformer cho các nhiệm vụ auto-regressive, việc thực hiện học trong ngữ cảnh trong lượt truyền tiến của Transformer được thực hiện thông qua tối ưu hóa dựa trên gradient của một mất mát auto-regressive nội bộ ngầm được xây dựng từ dữ liệu trong ngữ cảnh. Nói một cách chính thức, họ xem xét bài toán sau min_x ||Ax-b||^2 được định nghĩa trong Định nghĩa 1.2. Họ đầu tiên chứng minh rằng thực hiện một bước gradient descent thực hiện phép biến đổi dữ liệu như sau:

||A(x+δx)-b||^2 = ||Ax-(b-δb)||^2
= ||Ax-\tilde{b}||^2

trong đó δx biểu thị gradient descent một bước trên x và δb biểu thị phép biến đổi dữ liệu tương ứng trên b. Họ cũng chứng minh rằng một lớp self-attention về nguyên tắc có khả năng khai thác thống kê trong các mẫu dữ liệu huấn luyện hiện tại. Cụ thể, gọi Q, K, V∈R^{d×d} biểu thị trọng số cho ma trận query, ma trận key và ma trận value tương ứng. Lớp self-attention tuyến tính cập nhật một mẫu đầu vào bằng cách thực hiện phép biến đổi dữ liệu sau:

\hat{b}_j = b_j + PVK^TQ_j

trong đó \hat{b} biểu thị b được cập nhật và P biểu thị ma trận chiếu sao cho một bước Transformer \hat{b}_j trên mỗi j giống hệt với động lực được tạo ra bởi gradient \tilde{b}_j. Sự tương đương này ngụ ý rằng khi huấn luyện Transformer chỉ có linear-self-attention cho các nhiệm vụ hồi quy cơ bản, các mô hình được học bởi GD và Transformer thể hiện sự tương đồng lớn.

[XRLM21] khám phá sự xuất hiện của học trong ngữ cảnh trong quá trình pre-training khi các tài liệu thể hiện tính mạch lạc tầm xa. Mô hình Ngôn ngữ (LLM) phát triển khả năng tạo ra các token tiếp theo mạch lạc bằng cách suy luận một khái niệm tiềm ẩn cấp tài liệu. Trong quá trình kiểm tra, học trong ngữ cảnh được quan sát thấy khi LLM suy luận một khái niệm tiềm ẩn chung giữa các ví dụ trong một prompt. Thông qua nghiên cứu được thực hiện, đã được chứng minh rằng học trong ngữ cảnh xảy ra ngay cả khi có sự không khớp phân phối giữa prompt và dữ liệu pretraining, đặc biệt trong các tình huống mà phân phối pretraining là một hỗn hợp của các Mô hình Markov Ẩn [BP66]. Về mặt lý thuyết, họ chứng minh rằng lỗi của bộ dự đoán trong ngữ cảnh là tối ưu khi một điều kiện phân biệt được thỏa mãn. Trong các trường hợp mà điều kiện này không được thỏa mãn, lỗi kỳ vọng vẫn giảm khi độ dài của mỗi ví dụ tăng. Phát hiện này làm nổi bật tầm quan trọng của cả đầu vào và ánh xạ đầu vào-đầu ra đóng góp vào học trong ngữ cảnh.

2.2 Lý thuyết Transformer
Những tiến bộ của Transformer đã đáng chú ý, tuy nhiên, các cơ chế học tập của chúng chưa được hiểu hoàn toàn. Mặc dù những mô hình này đã hoạt động xuất sắc trong các hoạt động có cấu trúc và lý luận, sự hiểu biết của chúng tôi về nền tảng toán học của chúng vẫn tụt hậu đáng kể. Nghiên cứu trong quá khứ đã chỉ ra rằng hiệu suất xuất sắc của các mô hình dựa trên Transformer có thể được quy cho thông tin trong các thành phần của chúng, chẳng hạn như multi-head attention. Nhiều nghiên cứu khác nhau [TDP19, VB19, HL19, Bel22] đã trình bày bằng chứng thực nghiệm rằng những thành phần này mang một lượng thông tin đáng kể, có thể giúp giải quyết các nhiệm vụ probing khác nhau.

Nghiên cứu gần đây đã điều tra tiềm năng của Transformer thông qua cả phương pháp lý thuyết và thực nghiệm, bao gồm tính đầy đủ Turing [BPG20], xấp xỉ hàm [YBR+20, CDW+21], biểu diễn ngôn ngữ hình thức [BAG20, EGZ20, YPPN21], và học tập thao tác đại số trừu tượng [ZBB+22]. Một số nghiên cứu này đã chỉ ra rằng Transformer có thể hoạt động như những bộ xấp xỉ phổ quát cho các phép toán sequence-to-sequence và mô phỏng máy Turing [PMB19, BPG20]. [LWD+23] chứng minh sự tồn tại của độ thưa thớt ngữ cảnh trong LLM, có thể được dự đoán chính xác. Họ khai thác độ thưa thớt để tăng tốc suy luận LLM mà không làm giảm hiệu suất từ cả góc độ lý thuyết và thực nghiệm. [DCL+21] đề xuất mô hình Pixelated Butterfly sử dụng một mẫu độ thưa thớt cố định đơn giản để tăng tốc huấn luyện Transformer. Các nghiên cứu khác tập trung vào tính biểu đạt của attention trong Transformer [DGV+18, VBC20, ZKV+20, EGKZ21, SZKS21, WCM21].

Hơn nữa, [ZPGA23] đã chứng minh rằng các mô hình ngôn ngữ được che đậy có kích thước vừa phải có thể phân tích và nhận dạng hiệu quả thông tin cú pháp giúp tái tạo một phần cây phân tích. Được truyền cảm hứng bởi mô hình ngữ pháp ngôn ngữ được nghiên cứu bởi [ZPGA23], [DGS23] xem xét bài toán xấp xỉ hạng chu kỳ tensor. [GMS23] xem xét hồi quy mũ trong cài đặt over-parameterization neural tangent kernel. [LSZ23] nghiên cứu việc tính toán phiên bản chính quy hóa của bài toán hồi quy mũ nhưng họ bỏ qua yếu tố chuẩn hóa. [DLS23] xem xét hồi quy softmax có xét đến yếu tố chuẩn hóa so với các bài toán hồi quy mũ [GMS23, LSZ23]. Phần lớn LLM có thể thực hiện các tính toán attention một cách gần đúng trong quá trình suy luận, miễn là có đủ đảm bảo về độ chính xác. Góc độ này đã được nghiên cứu bởi nhiều nghiên cứu, bao gồm [CGRS19, KKL20, WLK+20, DKOD20, KVPF20, CDW+21, CDL+22]. Với điều này trong tâm trí, [ZHDK23, AS23, BSZ23, DMS23] đã tiến hành nghiên cứu về việc tính toán ma trận attention từ góc độ độ khó và phát triển các thuật toán nhanh hơn.

3 Kiến thức sơ bộ

Trong Mục 3.1, chúng tôi giới thiệu các ký hiệu được sử dụng trong bài báo này. Trong Mục 3.2, chúng tôi đưa ra một số sự kiện về đại số cơ bản. Trong Mục 3.3, chúng tôi đề xuất cận dưới trên ⟨exp(Ax),1_n⟩.

3.1 Ký hiệu
Với một số nguyên dương n, chúng tôi sử dụng [n] để biểu thị {1,2,···,n}, cho bất kỳ số nguyên dương n nào.
Chúng tôi sử dụng E[·] để biểu thị kỳ vọng. Chúng tôi sử dụng Pr[·] để biểu thị xác suất.
Chúng tôi sử dụng 1_n để biểu thị vector mà tất cả các phần tử đều bằng một. Chúng tôi sử dụng 0 để biểu thị vector mà tất cả các phần tử đều bằng không. Ma trận đơn vị kích thước n×n được biểu thị bởi I_n cho một số nguyên dương n.
Ký hiệu R đề cập đến số thực và R_{≥0} biểu thị số thực không âm.
Với bất kỳ vector x∈R^n, exp(x)∈R^n biểu thị một vector mà phần tử thứ i là exp(x_i) và ||x||_2 biểu thị chuẩn ℓ2 của nó, tức là ||x||_2 := (∑_{i=1}^n x_i^2)^{1/2}. Chúng tôi sử dụng ||x||_∞ để biểu thị max_{i∈[n]} |x_i|.
Với bất kỳ vector x∈R^n và vector y∈R^d, chúng tôi sử dụng ⟨x,y⟩ để biểu thị tích vô hướng của vector x và y.
Ký hiệu B_i được sử dụng để chỉ hàng thứ i của ma trận B.
Nếu a và b là hai vector cột trong R^n, thì a∘b biểu thị một vector cột mà (a∘b)_i = a_i b_i.
Với một ma trận vuông và đầy hạng B, chúng tôi sử dụng B^{-1} để biểu thị nghịch đảo thực của B.

3.2 Đại số cơ bản
Sự kiện 3.1. Với các vector x,y∈R^n, chúng ta có
• ||x∘y||_2 ≤ ||x||_∞ · ||y||_2
• ||x||_∞ ≤ ||x||_2 ≤ √n ||x||_∞
• ||exp(x)||_∞ ≤ exp(||x||_2)
• Với bất kỳ ||x-y||_∞ ≤ 0.01, chúng ta có ||exp(x)-exp(y)||_2 ≤ ||exp(x)||_2 · 2||x-y||_∞

Sự kiện 3.2. Với các ma trận X,Y, chúng ta có
• ||X^T|| = ||X||
• ||X|| ≥ ||Y|| - ||X-Y||
• ||X+Y|| ≤ ||X|| + ||Y||
• ||X·Y|| ≤ ||X|| · ||Y||
• Nếu X ⪯ α·Y, thì ||X|| ≤ α·||Y||

3.3 Cận dưới trên β
Bổ đề 3.3. Nếu các điều kiện sau được thỏa mãn
• ||A|| ≤ R
• ||x||_2 ≤ R
• Cho β là cận dưới trên ⟨exp(Ax),1_n⟩

Thì chúng ta có
β ≥ exp(-R^2)

Chứng minh. Chúng ta có
⟨exp(Ax),1_n⟩ = ∑_{i=1}^n exp((Ax)_i)
≥ min_{i∈[n]} exp((Ax)_i)
≥ min_{i∈[n]} exp(-|(Ax)_i|)
= exp(-max_{i∈[n]} |(Ax)_i|)
= exp(-||Ax||_∞)
≥ exp(-||Ax||_2)
≥ exp(-R^2)

bước thứ 1 theo đại số đơn giản, bước thứ 2 đến từ đại số đơn giản, bước thứ 3 theo từ sự kiện rằng exp(x) ≥ exp(-|x|), bước thứ 4 theo từ sự kiện rằng exp(-x) giảm đơn điệu, bước thứ 5 đến từ định nghĩa chuẩn ℓ_∞, bước thứ 6 theo từ Sự kiện 3.1, bước thứ 7 theo từ giả thiết về A và x.

--- TRANG 6 ---
4 Hàm Softmax đối với x

Trong Mục 4.1, chúng tôi đưa ra các định nghĩa được sử dụng trong tính toán. Trong Mục 4.2, chúng tôi tính toán gradient của hàm mất mát với hàm softmax đối với x. Những hàm đó bao gồm α(x)^{-1}, α(x) và f(x).

4.1 Định nghĩa
Chúng tôi định nghĩa hàm softmax f như sau

Định nghĩa 4.1 (Hàm f, Định nghĩa 5.1 trong [DLS23]). Cho một ma trận A∈R^{n×d}. Cho 1_n biểu thị một vector có độ dài n mà tất cả các phần tử đều bằng một. Chúng tôi định nghĩa hàm dự đoán f: R^d → R^n như sau
f(x) := ⟨exp(Ax),1_n⟩^{-1} · exp(Ax).

Định nghĩa 4.2 (Hàm mất mát L_{exp}, Định nghĩa 5.3 trong [DLS23]). Cho một ma trận A∈R^{n×d} và một vector b∈R^n. Chúng tôi định nghĩa hàm mất mát L_{exp}: R^d → R như sau
L_{exp}(x) := 0.5 · ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-b||_2^2.

Để thuận tiện, chúng tôi định nghĩa hai ký hiệu hữu ích α và c

Định nghĩa 4.3 (Hệ số chuẩn hóa, Định nghĩa 5.4 trong [DLS23]). Chúng tôi định nghĩa α: R^d → R như sau
α(x) := ⟨exp(Ax),1_n⟩.

Sau đó, chúng tôi có thể viết lại f(x) (xem Định nghĩa 4.1) và L_{exp}(x) (xem Định nghĩa 4.2) như sau
• f(x) = α(x)^{-1} · exp(Ax).
• L_{exp}(x) = 0.5 · ||α(x)^{-1} · exp(Ax)-b||_2^2.
• L_{exp}(x) = 0.5 · ||f(x)-b||_2^2.

Định nghĩa 4.4 (Định nghĩa 5.5 trong [DLS23]). Chúng tôi định nghĩa hàm c: R^d ∈ R^n như sau
c(x) := f(x) - b.

Sau đó chúng tôi có thể viết lại L_{exp}(x) (xem Định nghĩa 4.2) như sau
• L_{exp}(x) = 0.5 · ||c(x)||_2^2.

4.2 Tính toán Gradient
Chúng tôi nêu một bổ đề từ nghiên cứu trước đây,

Bổ đề 4.5 (Gradient, Bổ đề 5.6 trong [DLS23]). Nếu các điều kiện sau được thỏa mãn
• Cho ma trận A∈R^{n×d} và một vector b∈R^n.
• Cho α(x) được định nghĩa trong Định nghĩa 4.3.
• Cho f(x) được định nghĩa trong Định nghĩa 4.1.
• Cho c(x) được định nghĩa trong Định nghĩa 4.4.
• Cho L_{exp}(x) được định nghĩa trong Định nghĩa 4.2.

Với mỗi i∈[d], chúng ta có
• Phần 1.
d exp(Ax)/dx_i = exp(Ax) ∘ A_{*,i}

• Phần 2.
d⟨exp(Ax),1_n⟩/dx_i = ⟨exp(Ax),A_{*,i}⟩

• Phần 3.
dα(x)^{-1}/dx_i = -α(x)^{-1} · ⟨f(x),A_{*,i}⟩

• Phần 4.
df(x)/dx_i = dc(x)/dx_i = -⟨f(x),A_{*,i}⟩ · f(x) + f(x) ∘ A_{*,i}

• Phần 5.
dL_{exp}(x)/dx_i = A_{*,i}^T · (f(x)⟨c(x),f(x)⟩ + diag(f(x))c(x))

5 Lipschitz đối với x

Trong Mục 5.1, chúng tôi đưa ra kiến thức sơ bộ để tính toán Lipschitz. Trong Mục 5.2, chúng tôi chỉ ra cận trên của δb. Trong Mục 5.3, chúng tôi tính toán Lipschitz của hàm exp(Ax) đối với x. Trong Mục 5.4, chúng tôi tính toán Lipschitz của hàm α đối với x. Trong Mục 5.5, chúng tôi tính toán Lipschitz của hàm α^{-1} đối với x.

5.1 Kiến thức sơ bộ
Chúng tôi có thể tính toán
dL/dx = g(x)

Cho η > 0 biểu thị learning rate.
Chúng tôi cập nhật
x_{t+1} = x_t + η · g(x_t)

Định nghĩa 5.1. Chúng tôi định nghĩa δb∈R^n là vector thỏa mãn các điều kiện sau
||⟨exp(Ax_{t+1}),1_n⟩^{-1}exp(Ax_{t+1})-b||_2^2 = ||⟨exp(Ax_t),1_n⟩^{-1}exp(Ax_t)-b+δb||_2^2

Cho {-1,+1}^n biểu thị một vector mà mỗi phần tử có thể là -1 hoặc +1. Trong trường hợp xấu nhất, có 2^n giải pháp khả thi, ví dụ,
(⟨exp(Ax_{t+1}),1_n⟩^{-1}exp(Ax_{t+1}) - ⟨exp(Ax_t),1_n⟩^{-1}exp(Ax_t)) ∘ {-1,+1}^n

Chuẩn của tất cả các lựa chọn là như nhau. Do đó, việc chỉ xem xét một giải pháp là đủ như sau.

Nhận xét 5.2. Chúng tôi có thể viết δb như sau
δb = ⟨exp(Ax_{t+1}),1_n⟩^{-1}exp(Ax_{t+1}) - ⟨exp(Ax_t),1_n⟩^{-1}exp(Ax_t).

Chứng minh. Chứng minh trực tiếp từ Định nghĩa 5.1.

Để thuận tiện, chúng tôi chia δb thành hai thành phần, và cung cấp các định nghĩa sau

Định nghĩa 5.3. Chúng tôi định nghĩa
δb,1 := (⟨exp(Ax_{t+1}),1_n⟩^{-1} - ⟨exp(Ax_t),1_n⟩^{-1}) · exp(Ax_{t+1})
δb,2 := ⟨exp(Ax_t),1_n⟩^{-1} · (exp(Ax_{t+1}) - exp(Ax_t))

Do đó, chúng ta có

Bổ đề 5.4. Chúng ta có
• δb = δb,1 + δb,2
• Chúng tôi có thể viết lại δb,1 như sau
δb,1 = (α(x_{t+1})^{-1} - α(x_t)^{-1}) · exp(Ax_{t+1}),
• Chúng tôi có thể viết lại δb,2 như sau
δb,2 = α(x_t)^{-1} · (exp(Ax_{t+1}) - exp(Ax_t)).

Chứng minh. Chúng ta có
δb = δb,1 + δb,2
= α(x_{t+1})^{-1}exp(Ax_{t+1}) - α(x_t)^{-1}exp(Ax_{t+1}) +
α(x_t)^{-1}exp(Ax_{t+1}) - α(x_t)^{-1}exp(Ax_t)
= α(x_{t+1})^{-1}exp(Ax_{t+1}) - α(x_t)^{-1}exp(Ax_t)
= ⟨exp(Ax_{t+1}),1_n⟩^{-1}exp(Ax_{t+1}) - ⟨exp(Ax_t),1_n⟩^{-1}exp(Ax_t),

trong đó bước thứ 1 theo từ các định nghĩa của δb, bước thứ 2 theo từ các định nghĩa của δb,1 và δb,2, bước thứ 3 theo từ đại số đơn giản, bước thứ 4 đến từ định nghĩa của α.

--- TRANG 7 ---
5.2 Cận trên δb đối với x
Chúng tôi có thể chỉ ra rằng

Bổ đề 5.5. Nếu các điều kiện sau được thỏa mãn
• Cho β∈(0,1).
• Cho δb,1∈R^n được định nghĩa như Định nghĩa 5.3.
• Cho δb,2∈R^n được định nghĩa như Định nghĩa 5.3.
• Cho δb = δb,1 + δb,2.
• Cho R ≥ 4.

Chúng ta có
• Phần 1.
||δb,1||_2 ≤ 2β^{-2}n^{1.5}exp(2R^2) · ||x_{t+1}-x_t||_2

• Phần 2.
||δb,2||_2 ≤ 2β^{-1}√n R exp(R^2) · ||x_{t+1}-x_t||_2

• Phần 3.
||f(x_{t+1})-f(x_t)||_2 ≤ 4β^{-2}n^{1.5}R exp(2R^2) · ||x_{t+1}-x_t||_2

Chứng minh. Chứng minh Phần 1. Chúng ta có
||δb,1||_2 ≤ |α(x_{t+1})^{-1} - α(x_t)^{-1}| · ||exp(Ax_{t+1})||_2
≤ |α(x_{t+1})^{-1} - α(x_t)^{-1}| · √n · exp(R^2)
≤ β^{-2} · |α(x_{t+1}) - α(x_t)| · √n · exp(R^2)
≤ β^{-2} · √n · ||exp(Ax_{t+1}) - exp(Ax_t)||_2 · √n · exp(R^2)
≤ β^{-2} · √n · 2√nR exp(R^2)||x_{t+1}-x_t||_2 · √n · exp(R^2)
= 2β^{-2}n^{1.5}R exp(2R^2) · ||x_{t+1}-x_t||_2

trong đó bước đầu tiên theo từ định nghĩa, bước thứ hai theo từ giả thiết về A và x, bước thứ ba theo Bổ đề 5.8, bước thứ tư theo từ Bổ đề 5.7, bước thứ năm theo từ Bổ đề 5.6.

Chứng minh Phần 2.
Chúng ta có
||δb,2||_2 ≤ |α(x_{t+1})^{-1}| · ||exp(Ax_{t+1}) - exp(Ax_t)||_2
≤ β^{-1} · ||exp(Ax_{t+1}) - exp(Ax_t)||_2
≤ β^{-1} · 2√nR exp(2R^2) · ||x_{t+1}-x_t||_2

trong đó bước đầu tiên theo từ định nghĩa, bước thứ 2 đến từ Bổ đề 5.6.

Chứng minh Phần 3.
Chúng ta có
||δb||_2 = ||δb,1 + δb,2||_2
≤ ||δb,1||_2 + ||δb,2||_2
≤ 2β^{-2}n^{1.5}R exp(2R^2) · ||x_{t+1}-x_t||_2 + 2β^{-1}n^{0.5}R exp(2R^2) · ||x_{t+1}-x_t||_2
≤ 2β^{-2}n^{1.5}R exp(2R^2) · ||x_{t+1}-x_t||_2 + 2β^{-2}n^{1.5}R exp(2R^2) · ||x_{t+1}-x_t||_2
≤ 4β^{-2}n^{1.5}R exp(2R^2) · ||x_{t+1}-x_t||_2

trong đó bước thứ 1 theo từ định nghĩa của δb, bước thứ 2 theo từ bất đẳng thức tam giác, bước thứ 3 theo từ kết quả trong Phần 1 và Phần 2, bước thứ 4 theo từ sự kiện rằng n ≥ 1 và β^{-1} ≥ 1, bước thứ 5 theo từ đại số đơn giản.

5.3 Lipschitz cho hàm exp(Ax) đối với x
Bổ đề 5.6. Nếu các điều kiện sau được thỏa mãn
• Cho A∈R^{n×d}
• Cho ||A(y-x)||_∞ < 0.01
• Cho ||A|| ≤ R
• Cho x,y thỏa mãn ||x||_2 ≤ R và ||y||_2 ≤ R

Thì chúng ta có
||exp(Ax) - exp(Ay)||_2 ≤ 2√nR exp(R^2) · ||x-y||_2.

Chứng minh. Chúng ta có
||exp(Ax) - exp(Ay)||_2 ≤ ||exp(Ax)||_2 · 2||A(x-y)||_∞
≤ √n · exp(||Ax||_2) · 2||A(x-y)||_∞
≤ √n exp(R^2) · 2||A(x-y)||_2
≤ √n exp(R^2) · 2||A|| · ||x-y||_2
≤ 2√nR exp(R^2) · ||x-y||_2

trong đó bước thứ 1 theo từ ||A(y-x)||_∞ < 0.01 và Sự kiện 3.1, bước thứ 2 đến từ Sự kiện 3.1, bước thứ 3 theo từ Sự kiện 3.2, bước thứ 4 theo từ Sự kiện 3.2, bước cuối theo từ ||A|| ≤ R.

5.4 Lipschitz cho hàm α(x) đối với x
Chúng tôi nêu một công cụ từ nghiên cứu trước đây [DLS23].

Bổ đề 5.7 (Bổ đề 7.2 trong [DLS23]). Nếu các điều kiện sau được thỏa mãn
• Cho α(x) được định nghĩa như Định nghĩa 4.3

Thì chúng ta có
|α(x) - α(y)| ≤ ||exp(Ax) - exp(Ay)||_2 · √n.

5.5 Lipschitz cho hàm α(x)^{-1} đối với x
Chúng tôi nêu một công cụ từ nghiên cứu trước đây [DLS23].

Bổ đề 5.8 (Bổ đề 7.2 trong [DLS23]). Nếu các điều kiện sau được thỏa mãn
• Cho ⟨exp(Ax),1_n⟩ ≥ β
• Cho ⟨exp(Ay),1_n⟩ ≥ β

Thì chúng ta có
|α(x)^{-1} - α(y)^{-1}| ≤ β^{-2} · |α(x) - α(y)|.

6 Hàm Softmax đối với A

Trong mục này, chúng tôi xem xét hàm đối với A. Chúng tôi định nghĩa hàm softmax f như sau

Định nghĩa 6.1 (Hàm f, Tham số hóa lại x bằng A trong Định nghĩa 4.1). Cho một ma trận A∈R^{n×d}. Cho 1_n biểu thị một vector có độ dài n mà tất cả các phần tử đều bằng một. Chúng tôi định nghĩa hàm dự đoán f: R^{n×d} → R^n như sau
f(A) := ⟨exp(Ax),1_n⟩^{-1} · exp(Ax).

Tương tự, chúng tôi tham số hóa lại x bằng A cho hàm mất mát L của chúng tôi. Chúng tôi định nghĩa hàm mất mát L như sau

Định nghĩa 6.2 (Hàm mất mát L_{exp}, Tham số hóa lại x bằng A trong Định nghĩa 4.2). Cho một ma trận A∈R^{n×d} và một vector b∈R^{n×d}. Chúng tôi định nghĩa hàm mất mát L_{exp}: R^{n×d} → R như sau
L_{exp}(A) := 0.5 · ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-b||_2^2.

Để thuận tiện, chúng tôi định nghĩa hai ký hiệu hữu ích α và c đối với A như sau:

Định nghĩa 6.3 (Hệ số chuẩn hóa, Tham số hóa lại x bằng A trong Định nghĩa 4.3). Chúng tôi định nghĩa α: R^{n×d} → R như sau
α(A) := ⟨exp(Ax),1_n⟩.

Sau đó, chúng tôi có thể viết lại f(A) (xem Định nghĩa 6.1) và L_{exp}(A) (xem Định nghĩa 6.2) như sau
• f(A) = α(A)^{-1} · exp(Ax).
• L_{exp}(A) = 0.5 · ||α(A)^{-1} · exp(Ax)-b||_2^2.
• L_{exp}(A) = 0.5 · ||f(A)-b||_2^2.

Định nghĩa 6.4 (Tham số hóa lại x bằng A trong Định nghĩa 4.4). Chúng tôi định nghĩa hàm c: R^{n×d} ∈ R^n như sau
c(A) := f(A) - b.

Sau đó chúng tôi có thể viết lại L_{exp}(A) (xem Định nghĩa 6.2) như sau
• L_{exp}(A) = 0.5 · ||c(A)||_2^2.

7 Lipschitz đối với A

Trong Mục 7.1, chúng tôi đưa ra kiến thức sơ bộ để tính toán Lipschitz. Trong Mục 7.2, chúng tôi chỉ ra cận trên của δb đối với A. Trong Mục 7.3, chúng tôi tính toán Lipschitz của hàm exp(Ax) đối với A. Trong Mục 7.4, chúng tôi tính toán Lipschitz của hàm α đối với A. Trong Mục 7.5, chúng tôi tính toán Lipschitz của hàm α^{-1} đối với A.

7.1 Kiến thức sơ bộ
Chúng tôi định nghĩa δb như sau

Định nghĩa 7.1 (Tham số hóa lại x bằng A trong Định nghĩa 5.1). Chúng tôi định nghĩa δb∈R^n là vector thỏa mãn các điều kiện sau
||⟨exp(A_{t+1}x),1_n⟩^{-1}exp(A_{t+1}x)-b||_2^2 = ||⟨exp(A_tx),1_n⟩^{-1}exp(A_tx)-b+δb||_2^2

Nhận xét 7.2 (Tham số hóa lại x bằng A trong Định nghĩa 5.2). Chúng tôi có thể viết δb như sau
δb = ⟨exp(A_{t+1}x),1_n⟩^{-1}exp(A_{t+1}x) - ⟨exp(A_tx),1_n⟩^{-1}exp(A_tx).

Chứng minh. Chứng minh trực tiếp từ Định nghĩa 7.1.

Để thuận tiện, chúng tôi chia δb thành hai thành phần, và cung cấp các định nghĩa sau

Định nghĩa 7.3 (Tham số hóa lại x bằng A trong Định nghĩa 5.3). Chúng tôi định nghĩa
δb,1 := (⟨exp(A_{t+1}x),1_n⟩^{-1} - ⟨exp(A_tx),1_n⟩^{-1}) · exp(A_{t+1}x)
δb,2 := ⟨exp(A_tx),1_n⟩^{-1} · (exp(A_{t+1}x) - exp(A_tx))

Do đó, chúng ta có

Bổ đề 7.4 (Tham số hóa lại x bằng A trong Bổ đề 5.4). Chúng ta có
• Chúng tôi có thể viết lại δb∈R^n như sau
δb = δb,1 + δb,2
• Chúng tôi có thể viết lại δb,1∈R^n như sau
δb,1 = (α(A_{t+1})^{-1} - α(A_t)^{-1}) · exp(A_{t+1}x),
• Chúng tôi có thể viết lại δb,2∈R^n như sau
δb,2 = α(A_t)^{-1} · (exp(A_{t+1}x) - exp(A_tx)).

Chứng minh. Chúng ta có
δb = δb,1 + δb,2
= α(A_{t+1})^{-1}exp(A_{t+1}x) - α(A_t)^{-1}exp(A_{t+1}x) +
α(A_t)^{-1}exp(A_{t+1}x) - α(A_t)^{-1}exp(A_tx)
= α(A_{t+1})^{-1}exp(A_{t+1}x) - α(A_t)^{-1}exp(A_tx)
= ⟨exp(A_{t+1}x),1_n⟩^{-1}exp(A_{t+1}x) - ⟨exp(A_tx),1_n⟩^{-1}exp(A_tx),

trong đó bước thứ 1 theo từ các định nghĩa của δb, bước thứ 2 theo từ các định nghĩa của δb,1 và δb,2, bước thứ 3 đến từ đại số đơn giản, bước thứ 4 đến từ định nghĩa của α.

--- TRANG 8 ---
7.2 Cận trên δb đối với A
Chúng tôi có thể chỉ ra rằng

Bổ đề 7.5 (Tham số hóa lại x bằng A trong Bổ đề 5.5). Nếu các điều kiện sau được thỏa mãn
• Cho β∈(0,1).
• Cho δb,1∈R^n được định nghĩa như Định nghĩa 7.3.
• Cho δb,2∈R^n được định nghĩa như Định nghĩa 7.3.
• Cho δb = δb,1 + δb,2.
• Cho R ≥ 4.

Chúng ta có
• Phần 1.
||δb,1||_2 ≤ 2β^{-2}n^{1.5}exp(2R^2) · ||A_{t+1}-A_t||_2

• Phần 2.
||δb,2||_2 ≤ 2β^{-1}√nR exp(R^2) · ||A_{t+1}-A_t||_2

• Phần 3.
||f(A_{t+1})-f(A_t)||_2 ≤ 4β^{-2}n^{1.5}R exp(2R^2) · ||A_{t+1}-A_t||_2

Chứng minh. Chứng minh Phần 1. Chúng ta có
||δb,1||_2 ≤ |α(A_{t+1})^{-1} - α(A_t)^{-1}| · ||exp(A_{t+1}x)||_2
≤ |α(A_{t+1})^{-1} - α(A_t)^{-1}| · √n · exp(R^2)
≤ β^{-2} · |α(A_{t+1}) - α(A_t)| · √n · exp(R^2)
≤ β^{-2} · √n · ||exp(A_{t+1}x) - exp(A_tx)||_2 · √n · exp(R^2)
≤ β^{-2} · √n · 2√nR exp(R^2)||A_{t+1}-A_t|| · √n · exp(R^2)
= 2β^{-2}n^{1.5}R exp(2R^2) · ||A_{t+1}-A_t||

trong đó bước đầu tiên theo từ định nghĩa, bước thứ hai theo từ giả thiết về A và x, bước thứ ba theo Bổ đề 7.8, bước thứ tư theo từ Bổ đề 7.7, bước thứ năm theo từ Bổ đề 7.6.

Chứng minh Phần 2.
Chúng ta có
||δb,2||_2 ≤ |α(A_{t+1})^{-1}| · ||exp(A_{t+1}x) - exp(A_tx)||_2
≤ β^{-1} · ||exp(A_{t+1}x) - exp(A_tx)||_2
≤ β^{-1} · 2√nR exp(2R^2) · ||A_{t+1}-A_t||

Chứng minh Phần 3.
Chúng ta có
||δb||_2 = ||δb,1 + δb,2||_2
≤ ||δb,1||_2 + ||δb,2||_2
≤ 2β^{-2}n^{1.5}R exp(2R^2) · ||A_{t+1}-A_t|| + 2β^{-1}n^{0.5}R exp(2R^2) · ||A_{t+1}-A_t||
≤ 2β^{-2}n^{1.5}R exp(2R^2) · ||A_{t+1}-A_t|| + 2β^{-2}n^{1.5}R exp(2R^2) · ||A_{t+1}-A_t||
≤ 4β^{-2}n^{1.5}R exp(2R^2) · ||A_{t+1}-A_t||

trong đó bước thứ 1 theo từ định nghĩa của δb, bước thứ 2 đến từ bất đẳng thức tam giác, bước thứ 3 đến từ kết quả trong Phần 1 và Phần 2, bước thứ 4 theo từ sự kiện rằng n ≥ 1 và β^{-1} ≥ 1, bước thứ 5 theo từ đại số đơn giản.

7.3 Lipschitz cho hàm exp(Ax) đối với A
Bổ đề 7.6 (Tham số hóa lại x bằng A trong Bổ đề 5.6). Nếu các điều kiện sau được thỏa mãn
• Cho A,B∈R^{n×d}
• Cho ||(A-B)x||_∞ < 0.01
• Cho ||A|| ≤ R
• Cho x thỏa mãn ||x||_2 ≤ R

Thì chúng ta có
||exp(Ax) - exp(Bx)||_2 ≤ 2√nR exp(R^2) · ||A-B||.

Chứng minh. Chúng ta có
||exp(Ax) - exp(Bx)||_2 ≤ ||exp(Ax)||_2 · 2||(A-B)x||_∞
≤ √n · exp(||Ax||_2) · 2||(A-B)x||_∞
≤ √n exp(R^2) · 2||(A-B)x||_2
≤ √n exp(R^2) · 2||A-B|| · ||x||_2
≤ 2√nR exp(R^2) · ||A-B||

trong đó bước thứ 1 theo từ ||A(y-x)||_∞ < 0.01 và Sự kiện 3.1, bước thứ 2 theo từ Sự kiện 3.1, bước thứ 3 theo từ Sự kiện 3.2, bước thứ 4 đến từ Sự kiện 3.2, bước cuối theo từ ||A|| ≤ R.

7.4 Lipschitz cho hàm α(A) đối với A
Bổ đề 7.7 (Tham số hóa lại x bằng A trong Bổ đề 5.7). Nếu các điều kiện sau được thỏa mãn
• Cho α(A) được định nghĩa như Định nghĩa 6.3

Thì chúng ta có
|α(A) - α(B)| ≤ ||exp(Ax) - exp(Bx)||_2 · √n.

Chứng minh. Chúng ta có
|α(A) - α(B)| = |⟨exp(Ax) - exp(Bx),1_n⟩|
≤ ||exp(Ax) - exp(Bx)||_2 · √n

trong đó bước thứ 1 đến từ định nghĩa của α(x), bước thứ 2 theo từ bất đẳng thức Cauchy-Schwarz (Sự kiện 3.1).

7.5 Lipschitz cho hàm α(A)^{-1} đối với A
Bổ đề 7.8 (Tham số hóa lại x bằng A trong Bổ đề 5.8). Nếu các điều kiện sau được thỏa mãn
• Cho ⟨exp(Ax),1_n⟩ ≥ β
• Cho ⟨exp(Bx),1_n⟩ ≥ β

Thì chúng ta có
|α(A)^{-1} - α(B)^{-1}| ≤ β^{-2} · |α(A) - α(B)|.

Chứng minh. Chúng tôi có thể chỉ ra rằng
|α(A)^{-1} - α(B)^{-1}| = α(A)^{-1}α(B)^{-1} · |α(A) - α(B)|
≤ β^{-2} · |α(A) - α(B)|

trong đó bước thứ 1 theo từ đại số đơn giản, bước thứ 2 theo từ α(A) ≥ β, α(B) ≥ β.

8 Kết quả chính

Trong Mục 8.1, chúng tôi chỉ ra kết quả cận trên của δb đối với x. Trong Mục 8.2, chúng tôi chỉ ra kết quả cận trên của δb đối với A.

8.1 Dịch chuyển tham số trọng số x
Định lý 8.1 (Dịch chuyển có giới hạn cho Học trong ngữ cảnh, không chính thức của Định lý 1.4). Nếu các điều kiện sau được thỏa mãn
• Cho A∈R^{n×d}
• ||A|| ≤ R
• ||A(x_{t+1}-x_t)||_∞ < 0.01
• Cho R ≥ 4
• Cho M := n^{1.5}exp(10R^2).

Chúng tôi xem xét bài toán hồi quy softmax
min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-b||^2

Nếu chúng tôi di chuyển x_t đến x_{t+1}, thì chúng tôi đang giải một bài toán hồi quy softmax mới với
min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-\tilde{b}||^2

trong đó
||\tilde{b}-b||^2 ≤ M · ||x_{t+1}-x_t||^2

Chứng minh. Chúng ta có
||\tilde{b}-b||^2 ≤ 4β^{-2}n^{1.5}R exp(2R^2) · ||x_{t+1}-x_t||^2
≤ 4n^{1.5}R exp(2R^2) exp(2R^2) · ||x_{t+1}-x_t||^2
≤ n^{1.5}(4R) exp(4R^2) · ||x_{t+1}-x_t||^2
≤ n^{1.5}exp(6R^2) exp(4R^2) · ||x_{t+1}-x_t||^2
≤ n^{1.5}exp(10R^2) · ||x_{t+1}-x_t||^2
≤ M · ||x_{t+1}-x_t||^2

trong đó bước thứ 1 theo từ Bổ đề 5.5, bước thứ 2 đến từ Bổ đề 3.3, bước thứ 3 đến từ đại số đơn giản, bước thứ 4 theo từ đại số đơn giản, bước thứ 5 theo từ đại số đơn giản và bước thứ 6 theo từ định nghĩa của M.

8.2 Dịch chuyển dữ liệu câu A
Định lý 8.2 (Dịch chuyển có giới hạn cho Học trong ngữ cảnh, không chính thức của Định lý 1.4). Nếu các điều kiện sau được thỏa mãn
• Cho A∈R^{n×d}
• ||A|| ≤ R
• ||(A_{t+1}-A_t)x||_∞ < 0.01
• Cho R ≥ 4
• Cho M := n^{1.5}exp(10R^2).

Chúng tôi xem xét bài toán hồi quy softmax
min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-b||^2

Nếu chúng tôi di chuyển x_t đến x_{t+1}, thì chúng tôi đang giải một bài toán hồi quy softmax mới với
min_x ||⟨exp(Ax),1_n⟩^{-1}exp(Ax)-\tilde{b}||^2

trong đó
||\tilde{b}-b||^2 ≤ M · ||A_{t+1}-A_t||.

Chứng minh. Chúng ta có
||\tilde{b}-b||^2 ≤ 4β^{-2}n^{1.5}R exp(2R^2) · ||A_{t+1}-A_t||
≤ 4n^{1.5}R exp(2R^2) exp(2R^2) · ||A_{t+1}-A_t||
≤ n^{1.5}(4R) exp(4R^2) · ||A_{t+1}-A_t||
≤ n^{1.5}exp(6R^2) exp(4R^2) · ||A_{t+1}-A_t||
≤ n^{1.5}exp(10R^2) · ||A_{t+1}-A_t||
≤ M · ||A_{t+1}-A_t||

trong đó bước thứ 1 theo từ Bổ đề 5.5, bước thứ 2 theo từ Bổ đề 3.3, bước thứ 3 theo từ đại số đơn giản, bước thứ 4 đến từ đại số đơn giản, bước thứ 5 đến từ đại số đơn giản và bước thứ 6 theo từ định nghĩa của M.

Tài liệu tham khảo
[AS23] Josh Alman và Zhao Song. Fast attention requires bounded entries. arXiv preprint arXiv:2302.13214, 2023.
[ASA+22] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, và Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.
[BAG20] Satwik Bhattamishra, Kabir Ahuja, và Navin Goyal. On the Ability and Limitations of Transformers to Recognize Formal Languages. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 7096–7116, Online, tháng 11 năm 2020. Association for Computational Linguistics.
[Bel22] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207–219, tháng 3 năm 2022.
[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và các cộng sự. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[BP66] Leonard E Baum và Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554–1563, 1966.
[BPG20] Satwik Bhattamishra, Arkil Patel, và Navin Goyal. On the computational power of transformers and its implications in sequence modeling. Trong Proceedings of the 24th Conference on Computational Natural Language Learning, trang 455–475, Online, tháng 11 năm 2020. Association for Computational Linguistics.
[BSZ23] Jan van den Brand, Zhao Song, và Tianyi Zhou. Algorithm and hardness for dynamic attention maintenance in large language models. arXiv preprint arXiv:2304.02207, 2023.
[CDL+22] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, và Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. Trong International Conference on Learning Representations (ICLR), 2022.
[CDW+21] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, và Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems (NeurIPS), 34:17413–17426, 2021.
[CGRS19] Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.
[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, và các cộng sự. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[DCL+21] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, và Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021.
[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[DGS23] Yichuan Deng, Yeqi Gao, và Zhao Song. Solving tensor low cycle rank approximation. arXiv preprint arXiv:2304.06594, 2023.
[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, và Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.
[DKOD20] Giannis Daras, Nikita Kitaev, Augustus Odena, và Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. Advances in Neural Information Processing Systems (NeurIPS), 33:6476–6489, 2020.
[DLS23] Yichuan Deng, Zhihang Li, và Zhao Song. Attention scheme inspired softmax regression. arXiv preprint arXiv:2304.10411, 2023.
[DMS23] Yichuan Deng, Sridhar Mahadevan, và Zhao Song. Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension. arxiv preprint: arxiv 2304.03426, 2023.
[EGKZ21] Benjamin L Edelman, Surbhi Goel, Sham Kakade, và Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. arXiv preprint arXiv:2110.10090, 2021.
[EGZ20] Javid Ebrahimi, Dhruv Gelda, và Wei Zhang. How can self-attention networks recognize Dyck-n languages? Trong Findings of the Association for Computational Linguistics: EMNLP 2020, trang 4301–4306, Online, tháng 11 năm 2020. Association for Computational Linguistics.
[GHG+20] Peng Gao, Chiori Hori, Shijie Geng, Takaaki Hori, và Jonathan Le Roux. Multi-pass transformer for machine translation. arXiv preprint arXiv:2009.11382, 2020.
[GMS23] Yeqi Gao, Sridhar Mahadevan, và Zhao Song. An over-parameterized exponential regression. arXiv preprint arXiv:2303.16504, 2023.
[GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, và Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. arXiv preprint arXiv:2208.01066, 2022.
[HL19] John Hewitt và Percy Liang. Designing and interpreting probes with control tasks. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), trang 2733–2743, Hong Kong, China, tháng 11 năm 2019. Association for Computational Linguistics.
[KKL20] Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.
[KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. Trong International Conference on Machine Learning, trang 5156–5165. PMLR, 2020.
[LSX+22] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, và Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6), 2022.
[LSZ23] Zhihang Li, Zhao Song, và Tianyi Zhou. Solving regularized exp, cosh and sinh regression problems. arXiv preprint, 2303.15725, 2023.
[LWD+23] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, và Beidi Chen. Deja vu: Contextual sparsity for efficient llms at inference time. Trong Manuscript, 2023.
[ONR+22] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, và Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint arXiv:2212.07677, 2022.
[Ope23] OpenAI. Gpt-4 technical report, 2023.
[PCR19] Gabriele Prato, Ella Charlaix, và Mehdi Rezagholizadeh. Fully quantized transformer for machine translation. arXiv preprint arXiv:1910.10485, 2019.
[PMB19] Jorge Pérez, Javier Marinković, và Pablo Barceló. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019.
[RNS+18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, và các cộng sự. Improving language understanding by generative pre-training. 2018.
[SZKS21] Charlie Snell, Ruiqi Zhong, Dan Klein, và Jacob Steinhardt. Approximating how single head attention learns. arXiv preprint arXiv:2103.07601, 2021.
[TDP19] Ian Tenney, Dipanjan Das, và Ellie Pavlick. BERT rediscovers the classical NLP pipeline. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 4593–4601, Florence, Italy, tháng 7 năm 2019. Association for Computational Linguistics.
[VB19] Jesse Vig và Yonatan Belinkov. Analyzing the structure of attention in a transformer language model. Trong Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, trang 63–76, Florence, Italy, tháng 8 năm 2019. Association for Computational Linguistics.
[VBC20] James Vuckovic, Aristide Baratin, và Remi Tachet des Combes. A mathematical theory of attention. arXiv preprint arXiv:2007.02876, 2020.
[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[WCM21] Colin Wei, Yining Chen, và Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. arXiv preprint arXiv:2107.13163, 2021.
[WLK+20] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.
[XRLM21] Sang Michael Xie, Aditi Raghunathan, Percy Liang, và Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
[YBR+20] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, và Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? Trong International Conference on Learning Representations, 2020.
[YPPN21] Shunyu Yao, Binghui Peng, Christos Papadimitriou, và Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 3770–3785, Online, tháng 8 năm 2021. Association for Computational Linguistics.
[ZBB+22] Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, và Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task, 2022.
[ZHDK23] Amir Zandieh, Insu Han, Majid Daliri, và Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. arXiv preprint arXiv:2302.02451, 2023.
[ZKV+20] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, và Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33:15383–15393, 2020.
[ZPGA23] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, và Sanjeev Arora. Do transformers parse while predicting the masked word? arXiv preprint arXiv:2303.08117, 2023.
[ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, và các cộng sự. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.
