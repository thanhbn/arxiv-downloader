# 2401.02072.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/icl/2401.02072.pdf
# File size: 1349202 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ICE-GRT: Instruction Context Enhancement by Generative Reinforcement
based Transformers
Chen Zheng Ke Sun Da Tang Yukun Ma Yuyu Zhang
Chenguang Xi Xun Zhou
AML Group, Bytedance Inc.
{chen.zheng1,ke.sun1,da.tang,mayukun,yuyu.zhang}@bytedance.com
{chenguang.xi,zhouxun}@bytedance.com
Abstract
The emergence of Large Language Models
(LLMs) such as ChatGPT and LLaMA en-
counter limitations in domain-specific tasks,
with these models often lacking depth and ac-
curacy in specialized areas, and exhibiting a de-
crease in general capabilities when fine-tuned,
particularly analysis ability in small sized mod-
els. To address these gaps, we introduce ICE-
GRT, utilizing Reinforcement Learning from
Human Feedback (RLHF) grounded in Proxi-
mal Policy Optimization (PPO), demonstrating
remarkable ability in in-domain scenarios with-
out compromising general task performance.
Our exploration of ICE-GRT highlights its un-
derstanding and reasoning ability to not only
generate robust answers but also to provide de-
tailed analyses of the reasons behind the an-
swer. This capability marks a significant pro-
gression beyond the scope of Supervised Fine-
Tuning models. The success of ICE-GRT is
dependent on several crucial factors, including
Appropriate Data, Reward Size Scaling, KL-
Control, Advantage Normalization, etc. The
ICE-GRT model exhibits state-of-the-art per-
formance in domain-specific tasks and across
12 general Language tasks against equivalent
size and even larger size LLMs, highlighting
the effectiveness of our approach. We provide a
comprehensive analysis of the ICE-GRT, under-
scoring the significant advancements it brings
to the field of LLM.
1 Introduction
The advent of Large Language Models (LLMs)
like ChatGPT (Brown et al., 2020; OpenAI, 2023)
and LLaMA (Touvron et al., 2023a,b) has marked
a significant milestone in the field of Natural
Language Processing (NLP). These models have
gained widespread recognition for their robust gen-
eral conversational abilities, enabling fluid and co-
herent responses across a diverse range of topics.
However, there are key limitations to these models.Firstly, a key limitation surfaces when these mod-
els encounter domain-specific tasks (Zhao et al.,
2023; Zhang et al., 2023a). In scenarios that de-
mand deep technical knowledge or specialized ex-
pertise, these models often fall short, providing
responses that lack necessary depth and accuracy.
Secondly, Supervised Fine Tune (SFT) LLMs tend
to exhibit a decrease in general capabilities (Ling
et al., 2023). This is contrary to the expectations
held for large-scale models, which are presumed
to either maintain or improve their performance in
a wide array of tasks (Pan et al., 2023a). Lastly,
the current smaller-sized LLMs, such as 13Billion,
demonstrate a limited ability to conduct detailed
analysis on complex questions, a competency that
is significantly inferior compared to the capabilities
of models like ChatGPT, which can engage in more
comprehensive and detailed discussions.
Addressing these challenges, we introduce the
Instruction Context Enhancement by Generative
Reinforcement based Transformers (ICE-GRT),
an innovative LLM that leverages the principles
of Reinforcement Learning from Human Feed-
back (RLHF) (Brown et al., 2020) based on Prox-
imal Policy Optimization (PPO) (Schulman et al.,
2017). While ensuring that the general capabilities
of the Large Language Model (LLM) are main-
tained, ICE-GRT exhibits exceptional performance
in several domain-specific scenarios. Furthermore,
ICE-GRT demonstrates an improved ability for de-
tailed analysis, particularly in complex scenarios
where smaller-sized LLMs fall short.
We take one domain-specific task of ad moder-
ation as an example. ICE-GRT can not only de-
termine the compliance of advertisements but also
identify the specific category of violation. More-
over, it goes a step further by detailed analyzing
which elements of the ad are problematic and of-
fers constructive modification suggestions. This
is a notable advancement over both pretrained and
SFT (Chiang et al., 2023) LLM models, which arearXiv:2401.02072v1  [cs.CL]  4 Jan 2024

--- PAGE 2 ---
typically limited to identifying compliance and vi-
olation categories.
When our training methodology was applied to
RLHF, we observed not just significant improve-
ments in in-domain tasks but also a surprising en-
hancement in general tasks. In a comparative anal-
ysis against models of equivalent and larger pa-
rameter size across many general tasks, our ICE-
GRT model with 13 billion parameters consistently
achieved state-of-the-art performance in 12well-
known public LLM evaluation benchmarks. ICE-
GRT’s versatility is further illuminated through
its effective handling of various domain-specific
tasks, not limited to but including Poem Genera-
tion, Text-to-Table conversions, engaging Multiple
Round Dialogue, generating accurate Multi-lingual
Responses, proficient Code Generation, creating
tailored Ads Text and Labeling Text, etc.
Our exploration of the ICE-GRT model has un-
covered several factors critical to its training suc-
cess. The ICE-GRT model’s training data, sourced
from our ICE-Instruct (SFT) model and enriched
with human feedback with strict evaluation criteria,
offers a diverse and comprehensive dataset, essen-
tial for its robust training. Moreover, the scaling
of the reward model is essential for accurately cap-
turing complex scenarios and aligning with human
preferences in RLHF. Additionlly, KL-Control is
key to regulating the balance between the models,
while Advantage Normalization significantly im-
proves learning stability by adjusting advantage
estimates. Additionally, we discovered that modi-
fying the Clipping Range and carefully controlling
the maximum response length during sampling are
vital for enhancing the training process. These
findings deepen our understanding of RLHF mech-
anisms and are instrumental in effectively training
the ICE-GRT model.
Moreover, we provide a detailed analysis of
the ICE-GRT model, encompassing both general
and in-domain capabilities. Through this explo-
ration, we aim to contribute a novel perspective and
methodology to the field of NLP, particularly in en-
hancing the depth and accuracy of domain-specific
task handling by large language models. We ob-
serve that the pretrain phase engages in “knowledge
learning”, where the model extensively absorbs a
diverse range of information, forming a substan-
tial foundational knowledge base. Subsequently,
in the Supervised Fine-Tuning stage, the model
engages in “knowledge mining”, where it utilizes
the learned knowledge in response to specific in-structions. This stage is crucial for the model to
transition from passive knowledge accumulation to
active knowledge application. Finally, the RLHF
phase engages in “knowledge enhancement”, en-
hancing the model’s ability to align with human
language preferences. This stage builds upon the
vast knowledge gained in the pretrain phase and the
knowledge mining from the SFT stage, leading to
a model that not only reconstruct extensive knowl-
edge but also excels in applying it with human-
centric preference. Importantly, this phase show-
cases a significant leap in the model’s emergence
capabilities.
In our commitment to fostering collaborative
research and innovation, we make ICE-GRT
publicly available on HuggingFace1. This
open-source initiative is aimed at empowering re-
searchers globally to further investigate and expand
upon our findings with ICE-GRT. By democratiz-
ing access to this advanced model, we hope to
inspire and facilitate worldwide exploration and
progress in language model research. This paper
unveils just a fraction of ChatGPT’s capabilities,
and our choice of the acronym "ICE" for ICE-GRT
is purposeful. It represents our aspiration to accel-
erate the ’ice-breaking’ process in LLM research,
symbolizing our desire to inspire researchers to ex-
plore and uncover the vast potential of ICE-GRT
across an array of tasks and paving the way for new
discoveries and advancements in the field.
2 Related Works
2.1 Instruction-Tuning for LLM
Recent advancements in Large Language Model
(LLM) development have increasingly focused on
instruction-tuning (Chiang et al., 2023), a tech-
nique that is gaining significant traction particu-
larly within the realms of Question Answering
(QA) and different domains (Zhao et al., 2023;
Pan et al., 2023b; Qiu et al., 2020). Key re-
search in this area includes works such as AL-
PACA (Taori et al., 2023), Vicuna (Chiang et al.,
2023), and (Zhang et al., 2023b), which explores
the balance between diveristy and accuracy in large
language model. Furthermore, studies like (Sun
et al., 2023) delve into principles of effective QA
strategies, while (Zhou et al., 2023) present LIMA,
an innovative model for language interaction. In
the sphere of conversational interfaces, significant
1Our ICE-GRT is available at https://huggingface.co/
zhengchenphd/ICE-GRT .

--- PAGE 3 ---
Figure 1: ICE-GRT Model Architecture.
contributions include the development of OpenAs-
sistant by (Köpf et al., 2023; Chiang et al., 2023).
2.2 Reinforcement Learning from Human
Feedback (RLHF)
Alongside the development of LLMs, Reinforce-
ment Learning from Human Feedback has emerged
as an important approach to improve LLMs (Brown
et al., 2020; Touvron et al., 2023b). RLHF involves
training models not just on static datasets but also
incorporating human feedback to guide the learn-
ing process. This method has been particularly
useful in aligning knowledge learning and mining
with human feedback. For instance, models like
OpenAI’s InstructGPT have utilized RLHF to tailor
responses based on human preferences, leading to
more accurate outputs (Stiennon et al., 2020).
3 Model
In this section, we briefly introduce a SFT model
we have trained, named ICE-Instruct, designed to
improve the domain-specific knowledge mining ca-
pabilities of pre-trained LLMs. Following this, we
will give a detailed description of our process for
training the reward model, which we have termed
ICE-Reward. Finally, we will comprehensively
introduce the entire training process of ICE-GRT,
including some important training strategies.
3.1 ICE-Instruct
The ICE-Instruct model built upon the Vicuna
model (Chiang et al., 2023). By blending in-
domain and general-purpose data during fine-
tuning, it excels in both specialized tasks and
broader tasks. This approach not only maintains
its vast linguistic capacities but also enhances its
expertise in specific domains. Importantly, this setsa solid foundation for RLHF models. All subse-
quent actor and critic models are initialized using
ICE-Instruct as backbone. In essence, ICE-Instruct
determines the lower-bound capabilities of ICE-
GRT, ensuring a strong and reliable baseline for
further advancements. To maximize the model’s
applicability in contextual interactions, we have
converted all collected data into Question-Answer
pairs. Each data point adheres to a prompt for-
mat that begins with “Below is an instruction that
describes a task. Write a response that appropri-
ately completes the request. ### USER: <INPUT>
ASSISTANT: <OUTPUT> ” , ensuring consistency
and relevance in contexts.
3.2 ICE-Reward
Response Generation and Sampling: Initially,
for each prompt in the RLHF training dataset,
we generate five responses. These responses are
uniquely produced by our ICE-Instruct model. By
sampling from the model’s output distribution, we
ensure a diverse range of generated answers , cap-
turing various aspects of potential responses.
Human Annotation and Ranking: The gener-
ated responses are then subjected to human annota-
tion. Annotators rank these responses according to
predefined criteria detailed in section 4.3. Specif-
ically, we labeled 20,000 sets of rankings, each
set containing five responses. From the ranked re-
sponses, we extract the top two and the bottom two
responses for each prompt. These are then paired
to form training data. The pairs consist of a “better”
response and a “worse” response, as determined
by the human annotation. This pairing strategy is
instrumental in teaching the model the differences
between high-quality and low-quality responses.
Training Reward Model: The objective of train-
ing reward model is to develop a model capable
of accurately differentiating between high and low-
quality responses. Let R(s, a)be the reward func-
tion, where srepresents the input prompt and a
the generated response. Our goal is to optimize
Rso that it aligns with human judgments. The
training data consists of pairs (ai, aj)where aiis
a higher-ranked response compared to ajfor the
same prompt. We use a pairwise ranking loss func-
tion, defined as:
L(ai, aj) = max(0 ,margin −R(s, ai)+R(s, aj)).
This loss function encourages the model to assign
a higher score to aithanaj.

--- PAGE 4 ---
The trained reward model, therefore, learns to
assign higher scores to more relevant and contextu-
ally appropriate responses, as per human rankings.
This model forms a most critical part of our system,
ensuring high-quality, context-aware responses.
3.3 ICE-GRT
In this section, we provide a comprehensive
overview of each component involved in ICE-GRT,
leverages the principles of RLHF (Brown et al.,
2020) based on PPO (Schulman et al., 2017), along
with their respective mathematical formulations.
Figure 1 shows the whole training process.
Actor Model: The Actor model, represented as
πθact(a|s), maps states sto actions a. It is respon-
sible for generating actor logits, which are scores
assigned to each potential action.
Reference Model: The Reference model, denoted
asπθref(a|s), serves as a pre-trained benchmark for
evaluating behavior. It provides a baseline against
which the Actor model’s outputs are compared
throughout the training process.
Reward Model: The Reward model, expressed
asR(s, a), assigns a reward score based on the
quality of the generated sequence, evaluating both
the action aand the state s.
Critic Model: The Critic model, Vθcrt(s), estimates
the value of being in a specific state s, thereby pro-
ducing critic values that guide the learning process.
3.3.1 Generalized Advantage Estimation
(GAE) Calculation in ICE-GRT
The advantage function, A(s, a), assesses the rela-
tive benefit of executing a specific action in contrast
to the average action in a given state. The formula
for calculating the Advantage is:
A(s, a) =E(R(s, a) +γVθcrt(s′)−Vθcrt(s))(1)
where γrepresents the discount factor, s′is the
subsequent state following the current state s, and
Vθcrt(s)is the value function estimated by the Critic
model with weights θcrt.
Generalized Advantage Estimation (GAE), en-
hances the estimation of the advantage function in
RL (Schulman et al., 2015). GAE blends multi-
step return methods with value function estimates
to mitigate variance while preserving a reasonable
bias. The essence of GAE is the employment of a
weighted sum of n-step Temporal Difference (TD)
residuals:
δA
t=E(Rt+1(s, a) +γVt+1
θcrt(s′)−Vt
θcrt(s))(2)Here, δA
trepresents the TD residual at time t. Fur-
ther, the GAE advantage function is calcuated as:
AGAE(s, a) =P∞
l=0(γλ)lδA
t+l, where λ∈(0,1).
3.3.2 Actor Model Learning
The Actor Model is updated using the Proximal Pol-
icy Optimization objective (Schulman et al., 2017),
the process is calculated as follows:
L(θact) = min 
πθact(a|s)
πθold(a|s)Aπθold
GAE(s, a),
clipπθact(a|s)
πθold(a|s),1−ε,1 +ε
Aπθold
GAE(s, a)!
,
(3)
where Aπθold
GAE(s, a)is the advantage function calcu-
lated using the old policy πθold,ε∈(0,1)is a hy-
perparameter. This term ensures that the evolving
Actor policy remains not only stable in its updates
but also aligned or divergent as desired from the
old model.
3.3.3 Policy Optimization and Training
In the final stage, the PPO algorithm optimizes
the Actor model’s policy based on the calculated
advantages, the KL-divergence, and the updated
Actor model. The policy is iteratively updated to
maximize the expected rewards, with the aim of
aligning the Actor model’s behavior more closely
with established benchmarks while also ensuring
effective and efficient learning.
3.3.4 Important Training Strategies
ICE-GRT Training Data : Our ICE-GRT’s train-
ing data originates from ICE-Instruct model and
careful human feedback annotation. This data is
not just a collection of responses but is intricately
designed to encompass a wide range of scenarios.
Each prompt within the ICE-Instruct model is re-
sponded to with a set of diverse answers, gener-
ated by sampling from the model’s output distri-
bution. This method ensures a comprehensive and
varied dataset, essential for robust model training.
The responses are further refined through a metic-
ulous human annotation process, where experts
rank them based on predefined criteria. This rig-
orous approach ensures the model is trained on
high-quality, human-verified data, which is crucial
for the model’s ability to understand and apply com-
plex information. More details and experimental
comparsions are described in Section 5.2.1.
Reward size Scaling: In ICE-GRT, the scaling of
the reward model is a critical factor in determining

--- PAGE 5 ---
the overall effectiveness and efficiency of training.
A larger reward model, denoted as Rψ(s, a), where
ψrepresents the model parameters, is significant
for several reasons. Firstly, larger reward model
can better capture complex environments and ac-
tions, essential in RLHF where the reward signal
must accurately reflect human preferences and de-
tailed task requirements. Secondly, larger scale
of reward size aids in generalizing across diverse
prompts. This is vital for consistent performance
in various scenarios, especially in ICE-GRT.
KL-Control is a crucial mechanism in PPO, es-
pecially when training with human feedback. A
key aspect of KL-Control in this context is the reg-
ulation of divergence between the Actor and the
Reference models. The KL divergence between
these two models is monitored and controlled to
ensure that the policy evolution adheres closely to
the human feedback. Moreover, ICE-GRT training
includes a clipping mechanism to avoid large, po-
tentially destabilizing updates in the value function.
This ensures that changes in the value function are
moderate and accurately reflect real improvements
as assessed by the Critic. Furthermore, as an addi-
tional measure, KL Reward adjustment helps keep
the actor model on the desired path as defined by
human feedback. This aligns actor model updates
more closely with human preferences.
Advantage Normalization enhances learning sta-
bility and efficiency in PPO-based RLHF. It ad-
justs the advantage estimates, making them more
consistent and less variable. This is particularly
beneficial in RLHF, where human feedback can in-
troduce unpredictable variations. Normalizing the
advantage helps the model to focus on the most rel-
evant learning signals, leading to faster and more
stable convergence. The formula for Advantage
Normalization is shown as follows:
ˆAπθ
t=Aπθ
t−µAπθ
σAπθ,
where ˆAπθ
trepresents the normalized advantage at
timet,Aπθ
tis the original advantage at time t,µAπθ
is the mean of the advantage, σAπθis the standard
deviation of the advantage.
4 Experimental Details
Our training process utilized the power of 64A100
GPUs, employing a multi-node, multi-GPU strat-
egy to conduct ICE-GRT. Our models were trained
and stored using the bf 16precision format. Thelearning rates were finely selected, with the actor
learning rate set at 5e−6and the critic learning
rate at 5e−7. We maintained a clipping range
of0.2. The discount factor γwas kept constant
at0.95, ensuring optimal balance in our training.
We are excited to announce the upcoming release
and open-sourcing of our ICE-GRT 13B model on
Hugging Face, specifically tailored for scientific
research purposes.
4.1 Data Collection
For our training corpus, we have crafted a novel
mix of datasets. This includes a selection from
publicly available resources, complemented by in-
domain data. We have removed all the sensitive
information, including usernames, email addresses,
and personal details, to uphold the data privacy and
security. In essence, the dataset we have prepared
for reward model and RLHF model is diverse and
multi-faceted, covering a range of domains. It in-
cludes data relevant to public and domain-specific
question-answering scenarios, as well as tasks in-
volving multilingual data alignment. We generated
5distinct responses for every prompt in our data
collection, utilizing our ICE-Instruct model. This
process involves sampling from the model’s output
distribution, which guarantees a varied spectrum
of answers. To optimally train our reward model,
the data labelers carefully conducted manual label-
ing of the rankings for the 5distinct responses on
20,000 prompts. To enhance the human-annotation
accuracy and reduce subjectivity among labelers,
each prompt was independently evaluated by three
labelers, establishing a thorough and reliable vali-
dation processverification process.
4.2 General Task Evaluation
Our evaluation of ICE-GRT using the GPT-Fathom
framework (Zheng et al., 2023) focused on public
general tasks. The objective was to benchmark ICE-
GRT’s performance against existing models and to
understand its position in the landscape of current
LLMs. We employed 12 benchmarks, which span
across various capability categories such as lan-
guage understanding, reasoning, etc. These bench-
marks were carefully chosen to test a wide range of
abilities, from basic language processing to com-
plex problem-solving and decision-making tasks.
In our evaluation, we maintained alignment with
the settings used in GPT-Fathom to ensure a fair
and accurate comparison. This involved employ-
ing similar input formats, evaluation metrics, and

--- PAGE 6 ---
Model MMLU AGIEval BBH AGIEval-ZH ARC-E ARC-C HellaSWAG Winogrande RACE-M RACE-H GSM8K Math
5-shot few-shot 3-shot few-shot 1-shot 1-shot 1-shot 1-shot 1-shot 1-shot 8-shot 4-shot
LLaMA 7B 24.66% 20.05% 33.48% 23.68% 30.01% 26.71% 24.58% 50.36% 26.74% 29.19% 13.80% 0.36%
Llama2 7B 40.91% 25.97% 38.21% 26.21% 62.37% 48.46% 25.39% 50.36% 45.75% 39.54% 17.51% 0.08%
Vicuna 7B 38.49% 22.71% 37.26% 27.00% 69.74% 46.33% 17.37% 49.80% 50.21% 46.83% 21.68% 0.96%
ICE-Instruct 7B 26.30% 15.95% 39.00% 31.14% 67.63% 45.31% 3.10% 36.07% 53.55% 52.09% 35.48% 0.82%
LLaMA 13B 38.42% 26.78% 38.28% 25.51% 67.63% 49.23% 28.90% 47.51% 52.23% 48.51% 18.42% 0.42%
Llama2 13B 49.57% 34.85% 45.89% 32.93% 76.52% 55.63% 37.17% 52.17% 57.73% 55.09% 28.66% 0.44%
Vicuna 13B 35.84% 28.68% 39.27% 30.33% 60.23% 40.96% 0.03% 5.84% 59.19% 60.69% 24.56% 0.66%
ICE-Instruct 13B 50.08% 24.51% 48.09% 34.15% 85.19% 66.89% 19.30% 47.99% 72.14% 56.52% 47.08% 1.02%
ICE-GRT 13B 55.33% 34.92% 49.78% 34.23% 87.58% 70.99% 39.37% 53.04% 75.91% 71.64% 51.48% 0.92%
LLaMA 30B 50.38% 34.87% 49.70% 30.68% 82.41% 60.67% 31.31% 51.30% 65.18% 64.18% 35.10% 0.58%
Llama2-70B 64.72% 43.99% 65.22% 39.52% 93.43% 79.61% 68.45% 69.69% 87.60% 85.13% 56.56% 3.72%
Table 1: Evaluating Benchmark Performance of Large Language Models in General Language Tasks.
environmental conditions.
4.3 Manual Annotation-Based Evaluation
Our study incorporates a rigorous evaluation crite-
ria, with a special emphasis on manual annotation
for assessing the capabilities of LLMs, particularly
in different applications. The criteria evaluates re-
sponses in 8essential categories, utilizing a scoring
mechanism that prioritizes the most crucial aspects.
Clarity : Responses should be straightforward and
precise, ensuring easy comprehension through spe-
cific, appropriate language.
Accuracy : The responses are expected to align
closely with verified facts, as assessed by manual
annotators. Actual fact can be validated.
Completeness : Evaluated for covering all aspects
of the inquiry, providing comprehensive details for
informed decision-making.
Safety : Focuses on ensuring no personal data is
mishandled, with manual checks for data privacy.
Courtesy : Responses should be politically correct.
e.g., gender identity, ethnic groups, etc.
Comfortableness : Responses must maintain a po-
lite and respectful tone, containing inclusive vocab-
ulary and reflect diversity at all times..
Conciseness : Emphasizes brevity in responses,
without compromising on clarity or accuracy.
Context : Response must be related to the topic and
relevant to the question.
Table 2 shows the weight and score of each cate-
gories to evaluate these criteria accurately, ensuring
responses quality and relevance.
Evaluation Positive Neutral Negative Weights
Clarity 5 2 0 6
Accuracy 5 2 0 6
Completeness 5 2 0 6
Safety 5 2 0 3
Courtesy 5 2 0 3
Comfortableness 5 2 0 3
Conciseness 5 2 0 1
Context 5 2 0 1
Table 2: Manual Annotation-Based Evaluation Criteria.5 Results and Analysis
5.1 Results
Benckmarks Scores on General Tasks: Our anal-
ysis focuses on the performance of ICE-GRT 13B,
as compared to other models in similar and higher
capacity categories. As is shown in Table 1, our
ICE-GRT 13B model demonstrates significant im-
provements over the LLaMa, Llama 2, Vicuna
13B and LLaMa 30B in both its pretrained and
SFT across various general benchmarks, such as
MMLU (Hendrycks et al., 2021), AGIEval (Zhong
et al., 2023), BBH (Srivastava et al., 2022),
ARC (Xu et al., 2023), HellaSWAG (Zellers et al.,
2019), RACE (Lai et al., 2017), etc. It shows re-
markable advancements in general language un-
derstanding and reasoning tasks, indicating en-
hanced comprehension and reasoning capabilities.
Remarkably, the ICE-GRT 13B model has signif-
icantly narrowed the gap with the much larger
Llama 2 70B pretrain model. This comparison un-
derscores the effectiveness of the ICE-GRT, com-
pensating for smaller model size with more gener-
alization capabilities. The success of the ICE-GRT
models suggests that the methodology, which likely
includes components of human feedback and align-
ment, contributes significantly to the models’ abil-
ity to understand and respond to complex prompts,
a factor that is not solely dependent on model size.
Human-Annotated Scores on In-Domain Task:
In the in-domain evaluation presented in Table
3, ICE-GRT distinctly outperforms Llama2 SFT
13B and ICE-Instruct 13B across several critical
dimensions. Notably, ICE-GRT achieves the high-
est scores in clarity ( 98.1%), accuracy (97.0%),
and completeness ( 92.9%), underscoring its excep-
tional ability to deliver precise, comprehensive, and
understandable responses. While it scores slightly
lower in safety and comfort compared to its coun-
terparts, it still maintains a high standard in these
areas. The overall score of 95.5%for ICE-GRT is a
testament to its superior performance, significantly

--- PAGE 7 ---
surpassing Llama2 SFT 13B (86.3%) and ICE-
Instruct 13B (87.3%). This robust performance
across multiple metrics confirms the introductory
claims about ICE-GRT’s capabilities, particularly
in handling domain-specific tasks with a level of
depth and precision not seen in current models.
Llama2 sft ICE-Instruct ICE-GRT
Clarity 95.9% 88.5% 98.1%
Accuracy 77.4% 84.44% 97.0%
Completeness 64.8% 71.11% 92.9%
Safety 96.6% 100% 92.2%
Courtesy 100% 95.9% 100%
Comfortable 96.6% 98.1% 92.22%
Conciseness 95.1% 93.33% 91.8%
Context 98.8% 94.0% 98.1%
Overall Score 86.3% 87.3% 95.5%
Table 3: Evaluating human-assessed scores for in-
domain Large Language Models.
5.2 Detailed Analysis
5.2.1 The importance of ICE-GRT Training
Data
In the training of the ICE-GRT, we employed two
distinct datasets for RLHF. The first dataset was
uniquely produced by our ICE-Instruct model. For
each prompt, five diverse responses were generated
by sampling from the model outputs. These re-
sponses were then subjected to human annotation,
where annotators ranked them according to prede-
fined criteria. The second dataset originated from
the GPT-4-LLM (Peng et al., 2023). It included
ranked responses from GPT-4 and GPT-3.5, with
the rankings automatically assessed by GPT-4.
Our findings reveal a significant performance dis-
parity between models trained with these datasets,
although we found that the reward score trends
were similar during the ICE-GRT training shown
in Figure 2a. The ICE-GRT model, trained with
our human-annotated dataset, demonstrated supe-
rior performance across general tasks and domain-
specific tasks. As shown in Figure 2b, on the Nat-
ural Question task, the ICE-GRT model outper-
formed ICE-Instruct by 4%. This gap increased to
approximately 9.79% on the Web Questions and
17.17% on the LAMBADA benchmark. However,
when we employed the GPT-4-LLM Dataset on
ICE-GRT, we observe that the results were very
close to those of ICE-Instruct, with only a 0.89%
increase in the Natural Questions.
A key aspect of ICE-GRT’s success is its fo-
cus on ‘knowledge enhancement”. This process
builds upon the “knowledge mining” during the
ICE-Instruct, enabling the model to better alignwith human language preferences. This approach
guarantees consistency and relevance in training
data, which is crucial for the model to effectively
build upon and evolve its existing knowledge. Ex-
ternal data sources, despite their potential diversity,
could not perfectly align with the model’s knowl-
edge structure. The use of data generated by ICE-
Instruct ensures a natural and effective enhance-
ment of knowledge, as observed in ICE-GRT.
(a) Reward score comparsion
between different RLHF data.
(b) Benchmark performance
between different models.
Figure 2: The influence of different training data.
5.2.2 Powerful ICE-GRT on General Task
ICE-GRT model exhibits exceptional strength in
tasks that are grounded in language understanding
and reasoning. For instance, as shown in Figure 3a
and Figure 3c, ICE-GRT 13B demonstrates a robust
ability in RACE, ARC, BBH, and GSM8K. Our
model has achieved state-of-the-art performance in
these tasks among models of the same size.
Moreover, the ICE-GRT 13B model shows great
performance in areas such as "marketing" and
"college-biology", as depicted in Figure 3b. Re-
markably, the ICE-GRT 13B model surpasses the
Llama 2 70B in several AGIEval benchmarks, most
notably in tasks requiring language understanding
and reasoning, such as "gaokao-chinese" as seen
Figure 3d. This outperformance stems from ICE-
GRT knowledge enhanced contextual understand-
ing and generative capabilities.
5.2.3 The effectiveness of Advantage Norm
The integration of Advantage Normalization and
Reward Size Scaling significantly enhances ICE-
GRT. These strategies contribute to improved
training efficiency and better model performance,
demonstrating their importance in the context of
RLHF. Applying Advantage Normalization, which
stabilizes learning by normalizing advantage es-
timates, led to improvement in Natural Question
benchmark over ICE-GRT baseline. As shown in
Figure 4,this strategy is crucial for enhancing the
model’s sensitivity to the subtleties of human feed-

--- PAGE 8 ---
Question ICE-Insturct 13B ICE-Insturct 33B ICE-GRT 13B
"this is a 100% natural prod-
uct for all females and all
ages" what ad policy does the
above text violate?None, just make sure you’re
not lying about it.The advertisement text may vi-
olate exaggerated claims pol-
icy. Overstates the product’s
effectiveness by claiming it is
suitable for all females and all
ages.The above text may violate exaggerated claims policy if the product
is marketed as "100% natural" without providing sufficient evidence
or scientific backing for the claim . Making an exaggerated or unsup-
ported claim about a product’s naturalness could mislead consumers
and potentially violate advertising regulations. To avoid violating this
policy, advertisers should provide clear evidence to support any
claims about a product’s naturalness or provide a more nuanced
description of its ingredients and production methods. Additionally,
advertisers should ensure that their claims are not overstated or mislead-
ing, and that they are consistent with any applicable laws and regulations
governing product labeling and marketing.
Rewrite "this is a 100% natu-
ral product for all females and
all ages" to an ad without any
violationsThis product is suitable for all
women and all age groups.Experience the pure and nat-
ural goodness of this product,
suitable for all ages and gen-
ders.Introducing our natural product designed for all females and ages. Our
carefully crafted formulation is free of harmful chemicals and artifi-
cial additives, ensuring that you can use it with confidence. Whether
you’re looking for a daily skincare routine or specialized care for a spe-
cific concern, our product is a safe and effective choice for all skin types.
Experience the benefits of natural ingredients and nourishing care
with our product. Suitable for all ages and skin types, our product is a
safe and gentle choice for daily skincare.
Table 4: Comparative Case Study in the Domain-Specific Moderation Task.
(a) Language understanding
and Reasoning Tasks.
(b) MMLU Task Score Com-
parsion.
(c) BBH Task Performance.
 (d) AGIEval-ZH Task.
Figure 3: Score Comparsions between different LLMs.
back, leading to more effective learning outcomes.
5.3 Case Study on Domain-Specific Task
We provide a comparative analysis of the responses
generated by different models, specifically ICE-
Instruct 13B,33B, and ICE-GRT 13B, revealing
varying levels of sensitivity and creativity in ad-
dressing advertising policy adherence and rewrit-
ing for compliance. As is shown in Table 4, while
ICE-Instruct 13B takes a more direct and less cau-
tious approach, ICE-Instruct 33B and ICE-GRT
13B demonstrate a progressive increase in policy
awareness and creative compliance.
ICE-GRT, in particular, shows a comprehensive
understanding of advertising regulations and the im-
portance of substantiated claims, reflecting its ad-
vanced capability in nuanced and responsible com-
munication. In the first case, ICE-GRT displayed
the highest sensitivity to policy adherence, high-
lighting the risk of violating exaggerated claims
Figure 4: Comparative Analysis of ICE-GRT and ICE-
GRT Advantage Normalization on the Natural Ques-
tion (NQ) Benchmark. The x-axis represents different
epochs, while the y-axis shows the NQ scores.
policy, especially if the product is marketed as
"100% natural" without adequate evidence. It em-
phasizes the need for evidence-based advertising
and compliance with regulations. In the second
case, ICE-GRT Provided the most detailed and cau-
tious rewrite, ensuring compliance with advertising
policies. It focuses on natural ingredients, absence
of harmful chemicals, and suitability for all females
and ages, while avoiding exaggerated claims.
In this section, we have showcased only a small
fraction of our model’s capabilities, focusing pri-
marily on the in-domain task of ad moderation.
However, the scope of our model, ICE-GRT, ex-
tends far beyond this singular function. Within the
appendices, we demonstrate its proficiency across
a myriad of domain-specific tasks. These include,
but are not limited to, Poem Generation, Text-to-
Table, Multiple Round Dialogue (Appendix A),
Chemistry Response Generation (Appendix B),
Code Generation (Appendix C), Ads Text Gen-
eration, Labeling Text (Appendix D), and Multi-
lingual Response (Appendix E), etc. The choice
of the acronym "ICE" for ICE-GRT is deliber-
ate. It represents our aspiration to catalyze an

--- PAGE 9 ---
’ice-breaking’ moment in the research of LLMs.
This reflects our hope to encourage researchers to
explore and realize the broad possibilities of ICE-
GRT in a range of tasks. We aim to pave the way
for novel discoveries and advancements in the field,
demonstrating that the capabilities of our model
are as extensive and varied as they are innovative.
6 Conclusion
ICE-GRT model represents a significant leap for-
ward in the realm of LLMs, particularly in enhanc-
ing domain-specific performance. Leveraging the
principles of Reinforcement Learning from Human
Feedback, ICE-GRT demonstrates exceptional ca-
pabilities in both general and in-domain tasks, out-
performing standard models in accuracy and depth.
Moreover, our model have strong ability to gen-
erate detailed analyses of the reasons behind the
answer. Our research uncovers several aspects of
RLHF, providing insights into effective training
methodologies and highlighting the importance of
factors like Appropriate Data, Reward Size Scaling,
KL-Control, etc. ICE-GRT’s training phases, in-
cluding knowledge learning, mining, and enhance-
ment, contribute to its advanced abilities in aligning
with human preferences. We hope that ICE-GRT
will accelerate the “ice-breaking” process in LLM
research, encouraging further exploration.
Acknowledgements
We deeply appreciate Youlong Cheng, Guokun Lai,
Yingtong Bu, Zheng Zhang, Fan Qu for their help at
the early stage of this project. Moreover, we convey
appreciation to Hanzhi Zhou, Yijie Zhu, Xuan Zou
for their engineering support to build key compo-
nents of the infrastructure. We extend our gratitude
to Hang Wu, Ruoqi Zhang and Ruohong Zhang for
their insightful discussions that contributed to this
paper. Furthermore, we thank anonymous review-
ers for their valuable suggestions.
References
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, T. J. Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.Language models are few-shot learners. ArXiv ,
abs/2005.14165.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2021. Measuring massive multitask language
understanding. Proceedings of the International Con-
ference on Learning Representations (ICLR) .
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte,
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stan-
ley, Richárd Nagyfi, et al. 2023. Openassistant
conversations–democratizing large language model
alignment. arXiv preprint arXiv:2304.07327 .
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and
Eduard H. Hovy. 2017. Race: Large-scale reading
comprehension dataset from examinations. ArXiv ,
abs/1704.04683.
Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan
Deng, Can Zheng, Junxiang Wang, Tanmoy Chowd-
hury, Yun-Qing Li, Hejie Cui, Xuchao Zhang, Tian
yu Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang,
Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris
White, Quanquan Gu, Jian Pei, Carl Yang, and Liang
Zhao. 2023. Domain specialization as the key to
make large language models disruptive: A compre-
hensive survey.
OpenAI. 2023. Gpt-4 technical report. ArXiv ,
abs/2303.08774.
Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che,
and Libo Qin. 2023a. A preliminary evaluation of
chatgpt for zero-shot dialogue understanding. ArXiv ,
abs/2304.04256.
Wenbo Pan, Qiguang Chen, Xiao Xu, Wanxiang Che,
and Libo Qin. 2023b. A preliminary evaluation of
chatgpt for zero-shot dialogue understanding. arXiv
preprint arXiv:2304.04256 .
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277 .
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,
Ning Dai, and Xuanjing Huang. 2020. Pre-trained
models for natural language processing: A survey.
Science China Technological Sciences , 63(10):1872–
1897.
John Schulman, Philipp Moritz, Sergey Levine,
Michael I. Jordan, and P. Abbeel. 2015. High-
dimensional continuous control using generalized
advantage estimation. CoRR , abs/1506.02438.

--- PAGE 10 ---
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615 .
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. Advances
in Neural Information Processing Systems , 33:3008–
3021.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin
Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. 2023. Principle-driven self-
alignment of language models from scratch with
minimal human supervision. arXiv preprint
arXiv:2305.03047 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. ArXiv ,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-
tian Cantón Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony S. Hartshorn, Saghar Hos-
seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V .
Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, R. Subramanian, Xia Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin
Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang, Aure-
lien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. 2023b. Llama 2: Open foundation
and fine-tuned chat models. ArXiv , abs/2307.09288.Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott
Sanner, and Elias Boutros Khalil. 2023. Llms and the
abstraction and reasoning corpus: Successes, failures,
and the importance of object-based representations.
ArXiv , abs/2305.18354.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a ma-
chine really finish your sentence? In Annual Meeting
of the Association for Computational Linguistics .
Zheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun
Ma, Yingtong Bu, Xun Zhou, and Liang Zhao. 2023a.
Balancing specialized and general skills in llms: The
impact of modern tuning and data strategy. arXiv
preprint arXiv:2310.04945 .
Zheng Zhang, Chen Zheng, Da Tang, Ke Sun, Yukun
Ma, Yingtong Bu, Xun Zhou, and Liang Zhao. 2023b.
Balancing specialized and general skills in llms: The
impact of modern tuning and data strategy. ArXiv ,
abs/2310.04945.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang
Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu,
Jianyun Nie, and Ji rong Wen. 2023. A survey of
large language models. ArXiv , abs/2303.18223.
Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi,
Pengyang Gao, Xun Zhou, and Kevin Chen-Chuan
Chang. 2023. Gpt-fathom: Benchmarking large lan-
guage models to decipher the evolutionary path to-
wards gpt-4 and beyond. ArXiv , abs/2309.16583.
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,
Shuai Lu, Yanlin Wang, Amin Saied Sanosi Saied,
Weizhu Chen, and Nan Duan. 2023. Agieval: A
human-centric benchmark for evaluating foundation
models. ArXiv , abs/2304.06364.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
arXiv preprint arXiv:2305.11206 .

--- PAGE 11 ---
A Poem Generation, Text-to-Table, Multiple Round Dialogue.
This appendix section presents the multifaceted capabilities of the ICE-GRT, as it navigates through three
distinct tasks: Poem Generation, Text-to-Table, and Multiple Round Dialogue. Each section showcases a
unique aspect of the ICE-GRT prowess. In Poem Generation, we explore its creative capacity to craft
expressive content. The Text-to-Table task demonstrates its proficiency in data structuring and presentation.
Finally, the Multiple Round Dialogue section exhibits its skill in engaging in predictive and analytical
discourse. These tasks collectively underscore strong abilities of the ICE-GRT, exemplifying its potential
in diverse linguistic and analytical applications.
Questions Responses
Compose a poem to analyze American econ-
omy with a positive attitude.The American economy is strong and vibrant,
A shining beacon of hope and progress.
From coast to coast, businesses thriving,
Innovation and prosperity on the rise.
From the tech giants in Silicon Valley,
To the small startups in every town,
The American spirit of enterprise,
Driving growth and creating jobs galore.
From Wall Street to Main Street,
Investors and consumers alike,
Confidence in the future is high,
As the economy continues to expand and thrive.
With a positive attitude and a can-do spirit,
The American economy will continue to grow and prosper,
Bringing opportunity and prosperity,
To all who work hard and strive for success.
Table 5: Case Study in Poem Generation task.
Questions Responses
Then Generate a table to show Americans main
economic metrics, such as GPP, PPP, CPI, PPI,
etcHere is a table showing some of the main economic metrics for the United States:
Note: These values are subject to change and may not reflect the most up-to-date information. It’s important to consult
the latest official sources for the most current economic data.
Table 6: Case Study in Text-to-Table Task.
Questions Responses
Predict American economy next year based on
the data above and give your reasons.As an AI language model, I cannot predict the future with certainty, but I can provide some insights based on the
economic data available as of my knowledge cutoff date (September 2021). The United States has experienced a strong
economic recovery in 2021, with GDP growth projected to be around 6% for the year. This growth has been driven
by a combination of factors, including strong consumer spending, increased business investment, and a rebound in
international trade. The Consumer Price Index (CPI) has also risen steadily throughout 2021, reflecting higher inflationary
pressures in the economy.
Looking ahead to the next year (2022), there are several factors that may impact the U.S. economy. One key factor to
watch will be the trajectory of the COVID-19 pandemic, as the ongoing pandemic continues to pose risks to economic
growth and stability. Additionally, the U.S. Federal Reserve has signaled that it may begin tapering its asset purchases in
the coming months, which could impact financial markets and interest rates.
Assuming a continued economic recovery and a gradual normalization of monetary and fiscal policies, it is possible that
the U.S. economy could experience modest growth in 2022. However, there are also risks and uncertainties that could
impact the outlook, including the potential for new waves of the pandemic, geopolitical risks, and ongoing supply chain
disruptions. Overall, the outlook for the U.S. economy in 2022 will depend on a variety of factors, and it is important for
policymakers and businesses to remain vigilant and adaptable in the face of evolving economic conditions.
Table 7: Case Study in Multiple Round Dialogue task.

--- PAGE 12 ---
B Chemistry Response Generation.
Questions Responses
Help me with the chemistry equations, such as
H2O related equations.Here are some commonly used chemical equations related to H2O:
1. Combustion of hydrogen gas (H2):
2H2 + O2 -> 2H2O
2. Decomposition of water (H2O) into hydrogen gas (H2) and oxygen gas (O2) using electricity:
2H2O -> 2H2 + O2
3. Reaction of hydrogen gas (H2) and oxygen gas (O2) to form water (H2O):
2H2 + O2 -> 2H2O
These equations are just a few examples of how H2O is involved in various chemical reactions. There are many other
equations and reactions that involve H2O, depending on the specific context and conditions. It’s important to understand
the underlying chemistry and physics of these equations and reactions in order to fully comprehend their implications and
applications. Additionally, it’s important to follow proper safety protocols and procedures when handling chemicals and
performing experiments, as some reactions can be hazardous or even dangerous if not properly controlled and monitored.
Table 8: Case Study in Chemistry Generation Task.
C Code Generation.
Questions Responses
Write a function to count the
appearing times of each word,
then write another function to
sort this list, but you cannot use
any libs in python.Sure, here’s a Python function that counts the occurrences of each word in a
given string and returns a dictionary with the word as the key and its count as
the value:
d e f count_words ( t e x t ) :
words = t e x t . lower ( ) . s p l i t ( )
word_counts = {}
f o r word i n words :
i f word i n word_counts :
word_counts [ word ] += 1
e l s e :
word_counts [ word ] = 1
r e t u r n word_counts
To sort the dictionary of word counts, you can use the sorted() function along
with a lambda function that compares the values of the dictionary items:
d e f s o r t _ w o r d _ c o u n t s ( word_counts ) :
s o r t e d _ c o u n t s = s o r t e d ( word_counts . i t e m s ( ) ,
key=lambda x : x [ 1 ] , r e v e r s e =True )
r e t u r n s o r t e d _ c o u n t s
Here’s how you can use these functions together to count the occurrences of each
word in a string and sort the word counts:
t e x t = " The q u i c k brown fox jumped ov e r t h e l a z y dog
. The dog b a r k e d a t t h e fox . "
word_counts = count_words ( t e x t )
s o r t e d _ w o r d _ c o u n t s = s o r t _ w o r d _ c o u n t s ( word_counts )
p r i n t ( s o r t e d _ w o r d _ c o u n t s )
This will output:
[ ( ’ fox ’ , 2) , ( ’ dog ’ , 2) , ( ’ the ’ , 2) , ( ’ quick ’ , 1) ,
( ’ brown ’ , 1) , ( ’ jumped ’ , 1) , ( ’ over ’ , 1) , ( ’ lazy
’ , 1) , ( ’ barked ’ , 1) ]
Table 9: Case Study 1 in Code Generation Task.

--- PAGE 13 ---
Questions Responses
By using the words above as cor-
pus, try to implement word2vec
code based on its theorySure, here’s an implementation of word2vec in Python based on the theory
described in the paper "Word2Vec: Negative Sampling for Fast and Accurate
Learning of Word Vectors" by Mikolov et al. This implementation uses the Skip-
Gram algorithm to train a neural network to predict nearby words given a target
word, and vice versa. The resulting word vectors can be used for various natural
language processing tasks, such as text classification, language translation, and
sentiment analysis.
Here’s the implementation:
i m p o r t numpy as np
i m p o r t gensim
from gensim . models i m p o r t CoherenceModel
from gensim . models . coherencemodel i m p o r t
CoherenceModel as Coherence
from gensim . models . word2vec i m p o r t Word2Vec
from gensim . c o r p o r a i m p o r t D i c t i o n a r y
from gensim . models . ldamodel i m p o r t LdaModel
# S e t up t h e c o r p u s and d i c t i o n a r y
t e x t = " The q u i c k brown fox jumped ov e r t h e l a z y dog
. The dog b a r k e d a t t h e fox . " . lower ( )
t o k e n s = [ word f o r word i n t e x t . s p l i t ( ) ]
d i c t i o n a r y = D i c t i o n a r y ( t o k e n s )
# T r a i n t h e word v e c t o r s u s i n g t h e Skip −Gram
a l g o r i t h m
model = Word2Vec ( [ t e x t ] , min_count =1 , s i z e =100 ,
window =5 , workers =4)
# E v a l u a t e t h e word embeddings u s i n g c o h e r e n c e
a n a l y s i s
model_coherence = Coherence ( model . wv , model . wv ,
d i c t i o n a r y = d i c t i o n a r y , c o h e r e n c e = ’ c_v ’ ,
coherence_mode = ’ v e c t o r ’ )
c o h e r e n c e _ s c o r e = coherence_model . g e t _ c o h e r e n c e ( )
p r i n t ( f " Coherence s c o r e : { c o h e r e n c e _ s c o r e } " )
# T r a i n a LDA model on t h e word embeddings t o
i d e n t i f y t o p i c s i n t h e c o r p u s
lda_model = LdaModel ( c o r p u s =[ model . wv ] , id2word =
d i c t i o n a r y , num_topics =5 , r a n d o m _ s t a t e =42)
t o p i c s = lda_model . p r i n t _ t o p i c _ w o r d s ( num_words =5)
p r i n t ( t o p i c s )
This implementation uses the gensim library, which provides an easy-to-use
interface for training word embeddings and other natural language processing
models. The Word2Vec class is used to train the word vectors using the Skip-
Gram algorithm, while the CoherenceModel class is used to evaluate the word
embeddings using coherence analysis. Finally, the LdaModel class is used to
train a LDA model on the word embeddings to identify topics in the corpus. The
resulting topics can provide insights into the main themes and topics present in
the corpus.
Table 10: Case Study 2 in Code Generation Task.

--- PAGE 14 ---
D Ads text Generation, Label text, Multiple Round Dialogue.
Questions Responses
I want to post decoration ads,
please generate some candidate
for me to promote my devices
and materials.Sure, here are some sample ad copy ideas for decoration products:
1. "Transform your home into a work of art with our high-quality decorative
items. From wall art and sculptures to accent pieces and lighting, we’ve got
everything you need to make your space truly unique."
2. "Unleash your creativity and elevate your home decor with our wide selection
of decorative materials. Choose from a variety of textures, colors, and patterns
to create a personalized look that’s all your own."
3. "Looking for the perfect finishing touch for your home decor? Look no further
than our selection of decorative accents. From trendy tapestries and macrame
wall hangings to stylish vases and planters, we’ve got everything you need to
add the finishing touch to your space."
4. "Create a warm and inviting atmosphere in your home with our selection
of decorative lighting. From chic pendant lights and table lamps to statement
chandeliers and wall sconces, we have the perfect lighting solution for every
room in your home."
5. "Make your home a haven with our collection of decorative cushions and
throws. Choose from a variety of patterns and colors to add a pop of personality
to your sofa, armchair, or bed, or layer multiple cushions for a cozy and inviting
look."
I hope these ad copy ideas help inspire your decoration ads!
Transform the ad texts above
into semantically negativeSure, here are some sample ad copy ideas for decoration products with a negative
spin:
1. "Don’t settle for mediocre home decor - upgrade to our high-quality items and
still end up with a bland, uninspired look."
2. "Looking for a way to make your home decor even more generic and unorigi-
nal? Our selection of decorative materials is just what you need!"
3. "Want to make your home decor look even more outdated and unstylish? Our
selection of decorative accents will do the trick!"
4. "Create an even duller and more uninviting atmosphere in your home with our
selection of decorative lighting. Choose from a variety of boring and unflattering
options to make your home even less welcoming."
5. "Transform your home into a cold, impersonal space with our collection of
decorative cushions and throws. Choose from a variety of lifeless and uninspired
patterns and colors to make your sofa, armchair, or bed even less comfortable
and inviting."
Note: It’s generally not recommended to use negative or sarcastic language in
advertising as it may turn off potential customers and harm the brand’s reputation.
It’s usually more effective to use positive and aspirational language that appeals
to the customer’s desires and values.
Assume you are a labeler, please
label each texts above into "Neg-
ative", "Positive", "Neutral"Sure, here are the labels for each of the ad copy ideas:
1. Negative
2. Negative
3. Negative
4. Negative
5. Positive
Note: The labels are subjective and based on the overall tone and language
used in the ad copy ideas. The same ad copy idea could potentially be labeled
differently by different labelers depending on their personal interpretation and
perspective.
We have one more requirement,
when labeling the texts, please
also add probabilities of the la-
bel.Sure, here are the labels and probabilities for each of the ad copy ideas: 1.
Negative (95%)
2. Negative (80%)
3. Negative (85%)
4. Negative (70%)
5. Positive (60%)
Note: The probabilities are based on my analysis of the overall tone and language
used in the ad copy ideas, and are intended as an estimate rather than a definitive
measurement. The same ad copy idea could potentially have different probabili-
ties assigned by different labelers depending on their personal interpretation and
perspective. Additionally, the probabilities are subject to variation based on the
specific context and audience being targeted, as well as any changes or updates
made to the ad copy ideas over time.
Table 11: Case Study in Ads related text Generation and Labeling Task.

--- PAGE 15 ---
E Multi-lingual Response
Questions Responses
我很喜欢周杰伦的《简单
爱》这首歌曲。可以帮我分
析一下这首歌吗？当然可以。周杰伦的《简单爱》这首歌是一首抒情的流行歌曲，以简单
的旋律和易于记忆的歌词著称。以下是对这首歌的一些分析：
1.歌词：歌曲的歌词简单易懂，表达了简单的爱情，没有太多复杂的情
感和抽象的比喻。歌词中的“简单爱”是指一种真实、纯粹的爱情，没
有太多的承诺和纷乱的感情，而是一种简单而又深刻的感情体验。
2.旋律：这首歌的旋律非常易于记忆，很快就能让人唱出来。旋律简单
而优美，具有一种轻快的节奏感，让人感到快乐和舒适。
3.编曲：这首歌的编曲非常简单，主要由钢琴、吉他和弦乐组成。钢琴
和吉他的和声营造了一种温馨的氛围，弦乐则增加了歌曲的感染力和气
氛。
4.表现手法：这首歌的表现手法非常简单，主要是周杰伦的嗓音以及他
的情感投入。周杰伦在演唱这首歌时，通过自然而然的呼吸、声调的变
化和情感的表达，让人感到他真正地体验到了简单的爱情，并且让人产
生共鸣。
综上所述，《简单爱》这首歌是一首非常易于记忆、简单易懂、温馨感
人的流行歌曲，通过周杰伦的演唱和简单的编曲，传递出了一种简单而
真实的爱情感受。
describe the panda in Chinese熊猫（xiong mao ）是一种来自中国的熊科动物，也被称为大熊猫。它们
有黑白相间的柔软皮毛和圆圆的脸部，是世界上最受欢迎的动物之一。
熊猫主要生活在中国的山区，以竹子为食，是濒危物种之一。它们是中
国文化的重要象征，也被广泛认为是保护自然环境的代表。
Table 12: Case Study in Multi-lingual Response Task.

--- PAGE 16 ---
F Sampled Questions in In-domain task.
No. Questions
1 What is an auction ad?
2 What is pre-audit and smart fix in ads platform?
3 What is machine moderation
4 What is human moderation in advertising
5 Will the company use machine learning models to moderate ad groups?
6 Explain Advertiser Experience (AdEx)
7 Generate a list of advertising metrics on platform
8 How to advertise on platform
9 How to create an ad
10 How can I create an ad group
11 Can I add a link to my account in an ad?
12 Can I edit a bid?
13 Where can I see my account’s transaction?
14 Why were my ad groups rejected?
15 Why did you reject my ads?
16 How do I fix an ad that was disapproved in different locations?
17 What ad policy does the following text violate? Lonely and thirsty for a man
18What ad policy does the following text violate? This Streamline Makes Your Cheeky Hidden Hips Look
Like a Juicy Peach
19What ad policy does the following text violate? Red Boost is a powerful new formula for boosting male
sexual health, weight loss, muscle building.
20 Generate 10 suggestions to post non-political content ads on platform
21 show me the details of prohibited industry policy in the united States
22Classify the following comment into labels [non-delivery, description doesn’t match item, data theft,
body harm, unreasonable expense]: "Didn’t get it!"
23Classify the following text into these ad policies [political content policy, sexual hint policy, no violated
policy]. "How are you"
24 List some examples concerning adult products, which is prohibited
25 List some examples concerning sexual hint, which is restricted
26 List some reasons why sexual hint is not allowed on platform
27 Generate 10 reasons why sexual hint is not allowed on platform
28"This is a 100% natural product for all females and all ages" what ad policy does the above text violate?
(A) no violations (B) exaggerated (C) political content (D) absolute terms (E) none of the above
29 "This is a 100% natural product for all females and all ages" what ad policy does the above text violate?
30Explain the reasons why the following text violates exaggerated description ad policy. "This is a 100%
natural product for all females and all ages"
31 Rewrite "This is a 100% natural product for all females and all ages" to an ad without any violations
32 Rewrite "Number 1 product in the world" to the text that doesn’t violate exaggerated description policy
33 Generate a list of 10 common reasons why ad groups are rejected
34 How to lose weight without effort, write more than 1000 words
Table 13: Sampled Questions in Ad moderation Task.
