# 2305.08360.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2305.08360.pdf
# Kích thước file: 394103 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Cải thiện Prompt ChatGPT cho Sinh mã
Chao Liu1, Xuanlin Bao1, Hongyu Zhang1, Neng Zhang2, Haibo Hu1, Xiaohong Zhang1, Meng Yan1
1Trường Kỹ thuật Dữ liệu lớn và Phần mềm, Đại học Trùng Khánh, Trung Quốc
fliu.chao, baoxuanlin, hyzhang, haibo.hu, xhongz, mengy g@cqu.edu.cn
2Trường Kỹ thuật Phần mềm, Đại học Trung Sơn, Trung Quốc
zhangn279@mail.sysu.edu.cn
Tóm tắt —Sinh mã tự động có thể là một kỹ thuật mạnh mẽ
cho phát triển phần mềm, giảm đáng kể nỗ lực và thời gian
cần thiết để tạo mã mới bằng cách sinh tự động dựa trên
yêu cầu. Gần đây, mô hình ngôn ngữ ChatGPT của OpenAI
đã nổi lên như một công cụ mạnh mẽ để tạo phản hồi giống
con người cho nhiều loại đầu vào văn bản (tức là prompt),
bao gồm cả những prompt liên quan đến sinh mã. Tuy nhiên,
hiệu quả của ChatGPT trong sinh mã chưa được hiểu rõ,
và hiệu suất sinh có thể bị ảnh hưởng mạnh bởi việc chọn
prompt. Để trả lời những câu hỏi này, chúng tôi đã tiến hành
thí nghiệm sử dụng bộ dữ liệu CodeXGlue để đánh giá khả
năng của ChatGPT cho hai nhiệm vụ sinh mã, bao gồm sinh
văn bản thành mã và sinh mã thành mã. Chúng tôi đã thiết kế
prompt bằng cách tận dụng chiến lược chuỗi tư duy với tối
ưu hóa nhiều bước. Kết quả cho thấy bằng cách thiết kế cẩn
thận prompt để hướng dẫn ChatGPT, hiệu suất sinh có thể
được cải thiện đáng kể. Chúng tôi cũng phân tích các yếu tố
ảnh hưởng đến thiết kế prompt và đưa ra những hiểu biết có
thể hướng dẫn nghiên cứu tương lai.
Từ khóa chỉ mục —ChatGPT, sinh mã, kỹ thuật prompt

I. GIỚI THIỆU
Sinh mã là một kỹ thuật nhằm tự động tạo mã dựa trên
yêu cầu của nhà phát triển [1], [2]. Nó có thể giảm nỗ lực
mã hóa lặp đi lặp lại và cải thiện năng suất phát triển phần
mềm [3], [4]. Những yêu cầu này có thể được biểu đạt bằng
mô tả ngôn ngữ tự nhiên (NL), cho phép nhà phát triển chỉ
định nhu cầu của họ một cách trực quan. Ví dụ, một nhà
phát triển có thể yêu cầu công cụ sinh mã "chuyển đổi một
biến nguyên n thành chuỗi trong Java", và công cụ sẽ tạo
một ví dụ mã phù hợp như: "String s = Integer.toString(n)".
Quá trình này được gọi là sinh Văn bản thành Mã (T2C)
[5], [6]. Một loại sinh mã khác là sinh Mã thành Mã (C2C),
dịch một đoạn mã hiện có từ ngôn ngữ lập trình này sang
ngôn ngữ lập trình khác [5], [7]. Ví dụ, mã C# "String s =
n.ToString()" có thể được dịch sang mã Java ở trên. Sinh
C2C có thể hữu ích khi chuyển mã hiện có sang ngôn ngữ
lập trình mới [7].

Các mô hình ngôn ngữ lớn (LLM) đã nổi lên như một
công cụ mạnh mẽ cho các nhiệm vụ xử lý ngôn ngữ tự
nhiên (NLP), như phân tích cảm xúc [8], [9] và dịch ngôn
ngữ [10], nhờ khả năng được tiền huấn luyện trên lượng
dữ liệu văn bản không giám sát khổng lồ và tinh chỉnh
trên bộ dữ liệu cụ thể theo miền. Mô hình "tiền huấn luyện,
tinh chỉnh" này đã được áp dụng cho các nhiệm vụ kỹ thuật
phần mềm (SE), như sinh mã, với kết quả đầy hứa hẹn.
Ví dụ, Feng et al. [11] đã phát triển CodeBERT, một LLM
có kiến trúc tương tự BERT [12] và được tiền huấn luyện
trên sáu ngôn ngữ lập trình [12]. CodeBERT có thể được
sử dụng cho các nhiệm vụ SE khác nhau, như tìm kiếm và
tóm tắt mã, với hiệu suất tốt. Một mô hình đáng chú ý khác
là CodeGPT được phát triển bởi Lu et al. [5]. CodeGPT
được tiền huấn luyện trên bộ dữ liệu Python và Java sử
dụng kiến trúc GPT-2 [13], và tinh chỉnh cho nhiều nhiệm
vụ SE khác nhau, như sinh mã và dịch mã.

Gần đây, OpenAI đã giới thiệu ChatGPT, một LLM cách
mạng dựa trên kiến trúc GPT-3.5 [14], có thể làm việc
trên nhiều nhiệm vụ khác nhau bao gồm sinh mã [15].
Khác với các LLM hiện có, ChatGPT có thể tạo phản hồi
giống con người thông qua học tăng cường [16] dựa trên
đầu vào văn bản của người dùng (tức là prompt). Nhờ hiệu
quả trên các nhiệm vụ khác nhau, ChatGPT đã thu hút
100 triệu người dùng hoạt động trên toàn thế giới chỉ trong
vòng hai tháng sau khi phát hành ban đầu [17]. Tuy nhiên,
hiệu suất của ChatGPT phụ thuộc cao vào chất lượng prompt
được sử dụng. Thiết kế prompt tốt hơn, được gọi là kỹ thuật
prompt [18], đang được nghiên cứu tích cực.

Trong bài báo này, chúng tôi nghiên cứu hiệu suất sinh
mã của ChatGPT với các phương pháp kỹ thuật prompt
khác nhau.

Chúng tôi đã tiến hành đánh giá khả năng sinh mã của
ChatGPT sử dụng bộ dữ liệu CodeXGlue [5] được sử dụng
rộng rãi cho cả nhiệm vụ sinh T2C và C2C. Ban đầu,
chúng tôi sử dụng prompt cơ bản cho các nhiệm vụ: "viết
một phương thức Java mà" + mô tả NL cho nhiệm vụ T2C,
và "dịch mã C# thành mã Java:" + mã cho nhiệm vụ C2C.
Kết quả thí nghiệm cho thấy những prompt này (ChatGPT-task)
đạt điểm CodeBLEU lần lượt là 22,76 và 39,37, trong đó
CodeBLEU là một chỉ số đánh giá tổng thể được sử dụng
rộng rãi [19]. Để cải thiện hiệu suất sinh, chúng tôi đã tận
dụng chiến lược chuỗi tư duy với xây dựng thủ công [20]
để tăng cường prompt cho các nhiệm vụ khác nhau. Phương
pháp này thực hiện tối ưu hóa nhiều bước dựa trên phản
hồi từ ChatGPT. Kết quả thí nghiệm cho thấy: 1) thêm
yêu cầu cụ thể hơn vào prompt đã cải thiện CodeBLEU
của ChatGPT-task lần lượt 73,58% và 3,45% cho hai
nhiệm vụ; 2) yêu cầu trực tiếp ChatGPT tạo mã ngắn
gọn trong prompt (ví dụ: "viết một phương thức Java
ngắn gọn mà" + NL) dẫn đến cải thiện thêm CodeBLEU
cho nhiệm vụ T2C, đạt 50,18; 3) chia sẻ phiên ChatGPT
cho một số lần thử nghiệm prompt cũng tăng CodeBLEU
của nhiệm vụ C2C lên 48,80; và 4) tính ngẫu nhiên sinh
của ChatGPT có ít ảnh hưởng đến hiệu suất sinh do các
hướng dẫn cụ thể trong prompt.

Hơn nữa, chúng tôi so sánh hiệu suất với các LLM tinh
chỉnh tiên tiến và phân tích độ chính xác và chất lượng
của mã được sinh.

Tóm lại, những đóng góp chính của bài báo này như sau:
- Đánh giá ChatGPT trên bộ dữ liệu CodeXGlue được sử
dụng rộng rãi cho hai nhiệm vụ sinh mã.
- Đề xuất phương pháp thiết kế và tối ưu hóa prompt để
hướng dẫn ChatGPT tạo mã tốt hơn với kỹ thuật prompt.
- Công bố gói nhân bản1 để khám phá tương lai trong
cộng đồng nghiên cứu này.

II. KIẾN THỨC NỀN TẢNG VÀ CÔNG TRÌNH LIÊN QUAN

A. Mô hình Ngôn ngữ Lớn cho Sinh mã
Nhiều mô hình ngôn ngữ (LM) đã được đề xuất, được
tiền huấn luyện với một mục tiêu đặc biệt (ví dụ: mô hình
hóa ngôn ngữ có mặt nạ [21]) và áp dụng cho các nhiệm
vụ downstream bằng tinh chỉnh. Nói chung, có ba loại
LM: 1) LM có mặt nạ, một mô hình được huấn luyện để
dự đoán từ bị che trong câu dựa trên ngữ cảnh xung quanh,
như BERT [12] và RoBERTa [22]. 2) Bộ mã hóa-giải mã,
một mô hình hoạt động cho các tác vụ câu sang câu như
dịch và tóm tắt, trong đó bộ mã hóa mã hóa đầu vào thành
vector có độ dài cố định và bộ giải mã tạo đầu ra từ vector
đã mã hóa, như T5 [13], BART [23], và MASS [24]. 3)
LM từ trái sang phải, một mô hình được huấn luyện để
dự đoán từ tiếp theo trong câu dựa trên các từ trước đó,
như GPT [25], GPT-2 [13], và GPT-3 [26]. Đối với các
LM này, Transformer [27] được sử dụng làm mô hình
cơ sở vì các lớp self-attention của nó có thể xử lý hiệu
quả đầu vào với bộ nhớ dài hạn và thích nghi hiệu quả
với các nhiệm vụ downstream khác nhau [28].

Các nhà nghiên cứu đã đề xuất nhiều mô hình dựa trên
LM có thể được sử dụng cho các nhiệm vụ sinh mã. Các
đại diện là: 1) Dựa trên BERT. CodeBERT [11] huấn
luyện một mô hình giống BERT với sáu ngôn ngữ lập
trình. GraphCodeBERT [4] là một mô hình cải tiến xem
xét cấu trúc vốn có của mã thay vì văn bản thuần túy như
CodeBERT. UniXcoder [29] giải quyết khó khăn trong việc
học cấu trúc mã bằng cách chuyển đổi mã thành chuỗi
nhưng giữ lại thông tin cấu trúc. ContraBERT [30] tận
dụng học tương phản [31] để cải thiện độ bền của CodeBERT
và GraphCodeBERT. 2) Dựa trên T5. Mastropaolo et al.
[32] cho thấy tinh chỉnh T5 có thể hoạt động trên các
nhiệm vụ SE. CodeT5 [33] là một mô hình T5 nhận thức
định danh có thể phân biệt token mã nào là định danh
và khôi phục chúng khi bị che. 3) Dựa trên BART. PLBART
[34] được xây dựng trên BART được tiền huấn luyện với
bộ sưu tập phong phú các hàm Java và Python và văn
bản NL liên quan thông qua tự động mã hóa khử nhiễu.
CommitBART [7] tiền huấn luyện BART sử dụng dữ liệu
thu thập từ các commit GitHub. 4) Dựa trên GPT. GPT-C
[35] là một biến thể của GPT-2 được tiền huấn luyện trên
bộ dữ liệu mã nguồn đa ngôn ngữ không giám sát lớn.
CodeGPT [5] tiền huấn luyện GPT-2 trên corpus Python
và Java từ CodeSearchNet [36]. CodeGen [6] trình bày
một họ kiến trúc tương tự GPT-3 được thiết kế cho tổng
hợp chương trình đa lượt. CodeX [37] tinh chỉnh GPT-3
trên mã có sẵn công khai từ GitHub, phiên bản sản xuất
riêng biệt của nó cung cấp năng lượng cho GitHub Copilot [38].

B. ChatGPT và Kỹ thuật Prompt
ChatGPT là một LM được phát triển bởi OpenAI và được
thiết kế cho các nhiệm vụ hội thoại (ví dụ: hỏi đáp và
sinh mã) [14]. ChatGPT được xây dựng trên loạt GPT-3.5
với 175 tỷ tham số và được tối ưu hóa bằng cách sử dụng
học tăng cường từ phản hồi con người [16]. Nó có thể tạo
phản hồi giống con người cho prompt văn bản của người
dùng dựa trên hiểu biết ngữ cảnh và lịch sử hội thoại.
Bên cạnh đó, OpenAI đang cải thiện ChatGPT bằng cách
tiếp tục tối ưu hóa GPT-4 [39].

Khi LM (ví dụ: ChatGPT [14]) với số lượng lớn tham
số (>100 triệu) xuất hiện với khả năng sinh văn bản tiên
tiến, kỹ thuật prompt (PE) trở thành một mô hình mới
cho NLP [28]. Mục tiêu của PE là thiết kế prompt phù
hợp cho mô hình tiền huấn luyện và thực hiện dự đoán
như mong đợi với hiệu suất tốt, dẫn đến mô hình "tiền
huấn luyện, prompt, dự đoán". Cụ thể, PE tạo prompt
x0=fprompt (x)∈X cho đầu vào văn bản x (ví dụ: "viết
mã Java để chuyển đổi int thành string") mô tả nhiệm
vụ downstream (ví dụ: sinh mã). Với prompt x', LLM
thực hiện dự đoán y=fLLM (x0)∈Y. Hai nhiệm vụ PE
cơ bản là: 1) Kỹ thuật Mẫu Prompt, thiết kế mẫu x0
phù hợp cho đầu vào LM (ví dụ: "viết mã Java cho [x]",
trong đó "[x]" là biến cho mô tả NL), vì hiệu suất dự
đoán LM y nhạy cảm với (các) câu được thiết kế trong
mẫu. 2) Kỹ thuật Câu trả lời Prompt, nhằm thiết kế
không gian câu trả lời Z trong prompt để câu trả lời
y tốt hơn có thể được tạo từ phạm vi hạn chế y∈Z (ví
dụ: "Mã nào tốt hơn? A hay B"). Các nhiệm vụ PE tiên
tiến hơn có ý định thao tác đa prompt, như tăng cường
prompt [40], kết hợp, v.v. [28]

Prompt (x') có thể được tạo theo bốn cách (fprompt)
[28], [41]: 1) Xây dựng Thủ công, phù hợp cho prompt
dựa trên mẫu và prompt few-shot trong đó prompt không
phức tạp [40], [42]. 2) Sinh LM, tận dụng LM để tạo
prompt tùy chỉnh (x') cho mỗi đầu vào văn bản (x),
có thể bù đắp cho những thiếu sót của xây dựng thủ
công [43]. 3) Prompt dựa trên Truy xuất, dựa vào tài
nguyên bên ngoài được chú thích tốt (ví dụ: Wikipedia)
để giảm thiểu vấn đề không ổn định của sinh [44]. 4)
Học Prompt, xây dựng mô hình có giám sát để tự động
cập nhật prompt theo sinh của LM và ground-truth liên
quan [28]. Trong nghiên cứu này, chúng tôi tận dụng
xây dựng thủ công để khám phá khả năng hướng dẫn
ChatGPT cho các nhiệm vụ sinh mã, điều tra các yếu
tố có ảnh hưởng trong thiết kế prompt, và cung cấp
cho các nhà nghiên cứu những hiểu biết cho các công
trình tương lai.

III. ĐỐI TƯỢNG NGHIÊN CỨU
Để áp dụng ChatGPT cho sinh mã, phần này trình bày
các đối tượng nghiên cứu. Cụ thể, Phần III-A mô tả hai
nhiệm vụ sinh mã được điều tra. Phần III-B trình bày
bộ dữ liệu được sử dụng và Phần III-C liệt kê các chỉ
số đánh giá.

--- TRANG 2 ---
A. Nhiệm vụ Sinh mã
Sinh mã là quá trình tự động tạo mã theo đặc tả yêu
cầu. Sinh mã có thể tiết kiệm thời gian và nỗ lực cho
nhà phát triển bằng cách tự động hóa các nhiệm vụ lập
trình lặp đi lặp lại. Đặc tả có thể được biểu đạt theo
các cách khác nhau. Trong nghiên cứu này, chúng tôi
điều tra hai nhiệm vụ đại diện: 1) Sinh Văn bản thành
Mã (T2C). Nó lấy mô tả văn bản được viết bằng ngôn
ngữ tự nhiên (NL) làm đặc tả chức năng. Một mô hình
sinh mã tạo mã (ví dụ: Java) theo mô tả văn bản. 2)
Sinh Mã thành Mã (C2C). Nó lấy một đoạn mã (ví dụ:
C#) làm đầu vào và một mô hình NL tạo mã được viết
bằng ngôn ngữ lập trình khác (ví dụ: Java) với cùng
chức năng. Nhiệm vụ này cũng được gọi là dịch mã
do sự tương tự với dịch ngôn ngữ. Trong hai phần tiếp
theo, chúng tôi sẽ giới thiệu các mô hình liên quan.

B. Bộ dữ liệu
Ở đây chúng tôi trình bày các bộ dữ liệu để thử nghiệm
hai loại nhiệm vụ sinh mã được điều tra.

Bộ dữ liệu T2C. Chúng tôi chọn bộ dữ liệu CONCODE
được sử dụng rộng rãi [45], được thu thập trong CodeXGLUE
[5]. Bộ dữ liệu này thu thập khoảng 33k repository Java
từ GitHub, bao gồm 100k dữ liệu huấn luyện, 2k dữ liệu
valid, và 2k dữ liệu test. Mỗi instance trong dữ liệu
là một tuple gồm ba phần tử: 1) Đoạn mã, đây là ground-truth
cho sinh mã; 2) Mô tả Ngôn ngữ Tự nhiên, được trích
xuất từ Javadoc của đoạn mã cho đầu vào sinh; 3) Môi
trường Mã, mô tả file lớp (tức là ngữ cảnh lập trình)
nơi đoạn mã hoạt động, bao gồm tên lớp, đường dẫn
lớp, biến thành viên, và chữ ký của hàm thành viên.

Bộ dữ liệu C2C. Chúng tôi sử dụng bộ dữ liệu C2C từ
CodeXGLUE [5], một bộ dữ liệu benchmark phổ biến
cho hiểu và sinh mã. Bộ dữ liệu C2C thu thập dữ liệu
từ một số repository mã nguồn mở, bao gồm Lucene,
POI, JGit, và Antlr. Tổng cộng, nó chứa 10k dữ liệu
huấn luyện, 0.5k dữ liệu valid, và 1k dữ liệu test. Mỗi
instance trong dữ liệu chứa một cặp đoạn mã được
viết bằng C# và Java, chia sẻ cùng chức năng nhưng
được triển khai bằng các ngôn ngữ lập trình khác nhau.
Trong nghiên cứu này, chúng tôi coi đoạn mã Java
là mục tiêu sinh của bộ dữ liệu T2C, và sử dụng đoạn
mã C# làm đầu vào.

C. Chỉ số Đánh giá
Để phân tích hiệu quả của sinh mã, chúng tôi đo hiệu
suất với các chỉ số được sử dụng rộng rãi cho nhiệm
vụ sinh mã [5], bao gồm BLEU [46] và CodeBLEU [19].
Chi tiết được mô tả như sau. Theo Lu et al. [5], chúng
tôi sử dụng CodeBLEU làm chỉ số đánh giá tổng thể.

BLEU, một chỉ số phổ biến để đo độ chính xác sinh
cho các đoạn mã với độ dài khác nhau [5], [46]. Cụ thể,
BLEU =BP⋅e(log P₁+...+log Pₙ)/n trong đó BP là giá trị
phạt ngắn, bằng 1 nếu mã được sinh dài hơn ground-truth.
Ngược lại, nó bằng tỷ lệ giữa độ dài của hai mã. Pᵢ
là chỉ số cho sự chồng chéo giữa các túi i-gram xuất
hiện trong mã được sinh và ground-truth.

CodeBLEU, một biến thể của BLEU, cũng xem xét tính
đúng đắn của cú pháp và luồng dữ liệu ngữ nghĩa của
sinh mã. Nó tương tự BLEU, nhưng tính điểm precision
dựa trên một tập hợp token mã, thay vì n-gram ngôn
ngữ tự nhiên. Nói chung, CodeBLEU là trung bình có
trọng số của từ vựng, cây cú pháp trừu tượng, và khớp
luồng dữ liệu giữa mã được sinh và ground-truth [19].

IV. PHƯƠNG PHÁP LUẬN
Phần này mô tả kỹ thuật prompt cho hai nhiệm vụ sinh
mã. Cụ thể, Phần IV-A mô tả phương pháp chung cho
thiết kế prompt. Phần IV-B và IV-C trình bày chi tiết
thiết kế prompt cụ thể cho hai nhiệm vụ. Cuối cùng,
Phần IV-D trình bày các câu hỏi nghiên cứu (RQ) được
điều tra trong nghiên cứu này.

A. Phương pháp Thiết kế Prompt
Hiệu suất của ChatGPT thường nhạy cảm với thiết kế
prompt [28]. Để tăng cường prompt, Wei et al. [42]
chỉ ra rằng prompt Chuỗi Tư duy (CoT) là chiến lược
then chốt, cho phép LLM giải quyết vấn đề bằng cách
hướng dẫn chúng tạo ra chuỗi các bước trung gian trước
khi đưa ra câu trả lời cuối cùng. Do tính hiệu quả của
nó, chiến lược CoT được nghiên cứu và áp dụng rộng
rãi [20], [47].

Nói chung, để hướng dẫn ChatGPT cho các nhiệm vụ
sinh mã, chúng tôi thiết kế prompt với chiến lược CoT
trong hai bước: 1) Mô tả Prompt, trước tiên chúng tôi
phân tích yêu cầu của nhiệm vụ sinh mã, và thiết kế
prompt cơ bản một cách tự nhiên. Sau đó, chúng tôi
cung cấp prompt cơ bản cho ChatGPT và hỏi "làm
thế nào để cải thiện prompt?", và tiếp tục cải thiện
prompt theo đề xuất của ChatGPT. 2) Tối ưu hóa Nhiều
Bước, chúng tôi thử nghiệm prompt trong bước đầu
tiên trên một số mẫu từ dữ liệu huấn luyện của bộ
dữ liệu liên quan, phân tích hiệu suất sinh với ground-truth,
và tiếp tục tối ưu hóa kết quả sinh bằng cách cung
cấp ChatGPT với một loạt prompt mới.

Dựa trên kiến thức về quy trình thiết kế prompt, chúng
tôi tạo ra một số prompt cơ sở và đánh giá chúng trên
dữ liệu thử nghiệm, có thể tìm thấy trong Phần V-A.
Hình 1 minh họa tổng quan về thiết kế và xác minh
prompt. Trong quá trình thiết kế và thử nghiệm prompt,
chúng tôi làm việc với ChatGPT bằng cách gọi API
của nó [48] với cài đặt mặc định (ví dụ: sử dụng mô
hình GPT-3.5-Turbo). Bảng II cho thấy hiệu suất sinh
của ChatGPT sử dụng các kết hợp khác nhau của prompt
được thiết kế trong Bảng I. Hai phần tiếp theo trình
bày chi tiết cách chúng tôi thiết kế prompt cho hai
nhiệm vụ sinh mã, trong đó các prompt được thảo luận
được liệt kê trong Bảng I.

B. Thiết kế Prompt cho Sinh Văn bản thành Mã
Mô tả Prompt. Như được mô tả bởi Lu et al. [5], nhiệm
vụ sinh T2C lấy mô tả NL làm đầu vào văn bản (ví dụ:
"chuyển đổi int thành String") và mong đợi một sinh
chính xác của phương thức mã Java, khớp với ý định
của mô tả. Theo mô tả nhiệm vụ, chúng tôi tự nhiên
trình bày prompt nhiệm vụ cơ bản: "viết một phương
thức Java mà + #{NL}" (Bảng I-P1). Để đánh giá hiệu
quả của prompt, chúng tôi lấy mẫu ngẫu nhiên 100
instance từ dữ liệu huấn luyện và yêu cầu ChatGPT
sinh mã với prompt đã cho. Chúng tôi có độ chính xác
sinh thấp là BLEU=5.29 và CodeBLEU=22.76.

Tối ưu hóa Nhiều Bước. Với mẫu prompt này (Bảng I-P1),
chúng tôi hỏi ChatGPT: "làm thế nào để cải thiện prompt:
viết một phương thức Java chuyển đổi int thành string".
ChatGPT nói rằng bằng cách cung cấp chi tiết cụ thể
hơn về hành vi phương thức, ngữ cảnh lập trình, và
ví dụ đầu vào/đầu ra, chúng ta có thể tạo prompt rõ
ràng và nhiều thông tin hơn giúp hướng dẫn sinh một
phương thức Java được thiết kế tốt. Chúng tôi nhận
thấy rằng môi trường lập trình được cung cấp trong
bộ dữ liệu như được mô tả trong Phần III-B có thể
được sử dụng làm thông tin ngữ cảnh bổ sung. Để đạt
được mục tiêu, chúng tôi thêm prompt ngữ cảnh trước
prompt nhiệm vụ: "nhớ bạn có một lớp Java tên + '#{CN}',
biến thành viên + '#{MV}', hàm thành viên + '#{MF}'"
(Bảng I-P2). Trong prompt, cloze #... sẽ được điền
bằng thông tin tương ứng được cung cấp trong bộ
dữ liệu. Lưu ý rằng chúng tôi nói với ChatGPT nhớ
lớp vì nó sẽ sinh toàn bộ lớp nếu chúng tôi không
hướng dẫn ChatGPT với hướng dẫn rõ ràng. Bằng cách
thêm prompt ngữ cảnh, độ chính xác của các mẫu có
thể được cải thiện với BLEU=10.42 và CodeBLEU=25.05.

Sau khi phân tích ground-truth, chúng tôi quan sát rằng
ground-truth đã được tiền xử lý trong bốn khía cạnh:
1) tất cả comment, throw, và modifier phương thức
đều bị xóa; 2) tên phương thức được đổi thành "function";
3) tất cả argument được đổi tên thành "arg0", "arg1",
v.v.; 4) tất cả biến cục bộ được đổi tên thành "loc0",
"loc1", v.v. Theo những quan sát này, chúng tôi thêm
prompt xử lý với một loạt hướng dẫn sau prompt nhiệm
vụ: "xóa comment; xóa summary; xóa throw; xóa modifier
hàm; đổi tên phương thức thành "function"; đổi tên
argument thành "arg0", "arg1"...; đổi tên biến cục bộ
thành "loc0", "loc1"..." (Bảng I-P3). Lưu ý rằng một
số summary được sinh cho phần của đoạn mã không
thể bị xóa bởi prompt "xóa comment" mà cần "xóa summary";
chúng tôi sử dụng dấu ba chấm "..." trong prompt thay
vì "v.v.", vì ChatGPT không thể thực hiện hành động
đổi tên với lệnh "v.v.". Đánh giá cho thấy cải thiện
thêm với BLEU=13.11 và CodeBLEU=36.00.

Bằng cách so sánh mã được sinh với ground-truth, chúng
tôi nhận thấy ChatGPT có thể sinh mã với các API và
cài đặt xử lý ngoại lệ khác nhau. Tự nhiên là yêu cầu
ChatGPT sinh lại mã theo phản hồi của nó và yêu cầu
cụ thể của người dùng. Để trích xuất yêu cầu của API
và xử lý ngoại lệ, chúng tôi nhập ChatGPT với prompt
"liệt kê các phương thức được sử dụng chỉ với tên trong
các phương thức Java sau và không giải thích: #{Code}"
và "mã có chứa xử lý ngoại lệ không? + #{Code}" cho
ground-truth. Sau đó, chúng tôi viết script để phân tích
phản hồi cho yêu cầu của API (tức là danh sách tên)
và xử lý ngoại lệ (tức là đúng hoặc sai). Với hai yêu
cầu này, chúng tôi thay thế prompt nhiệm vụ bằng prompt
hành vi: "viết một phương thức Java #{that calls ...}
with[out] exception handling to #{NL}" (Bảng I-P5).
Lưu ý rằng nếu danh sách API trống, chúng tôi xóa
"#{that calls ...}", ngược lại chúng tôi thay thế "..."
bằng danh sách tên. Đối với "with[out]", chúng tôi xác
định nó là "with" hay "without" theo nhu cầu thực tế.
Chúng tôi thấy rằng xem xét yêu cầu API, độ chính xác
sinh có thể được tăng cường (BLEU=22.14 và CodeBLEU=44.18).
Đồng thời, sử dụng toàn bộ prompt hành vi (tức là API
+ xử lý ngoại lệ), hiệu suất có thể được tăng cường
thêm với BLEU=27.48 và CodeBLEU=46.78.

C. Thiết kế Prompt cho Sinh Mã thành Mã
Vì sinh C2C có quy trình thiết kế prompt tương tự sinh
T2C, chúng tôi chủ yếu chỉ ra những khác biệt chính
trong phần này.

Mô tả Prompt. Theo mô tả nhiệm vụ trong Phần III-B,
nhiệm vụ sinh C2C của chúng tôi nhằm sinh một phương
thức mã Java theo hàm mã C# đã cho. Dựa trên yêu
cầu nhiệm vụ, chúng tôi hình thành prompt nhiệm vụ:
"dịch mã C# thành mã Java: #{Code}" (Bảng I-P1).
Đối với 100 mẫu được chọn ngẫu nhiên từ dữ liệu huấn
luyện, hiệu suất sinh là BLEU=9.76 và CodeBLEU=39.37.

Tối ưu hóa Nhiều Bước. So với sinh T2C, chúng tôi có
thể thấy nhiều khác biệt trong nhiệm vụ sinh C2C: bộ
dữ liệu C2C không liên quan đến lớp liên quan; ChatGPT
không sinh comment và throw cho mã; ground-truth
không được tiền xử lý cho tên phương thức, modifier,
tên argument, và tên biến cục bộ. Tuy nhiên, ChatGPT
sẽ sinh annotation theo mã C# nhưng ground-truth đã
xóa tất cả annotation. Do đó, chúng tôi thêm prompt
xử lý đơn giản vào prompt nhiệm vụ: "không cung cấp
annotation" (Bảng I-P3). Hơn nữa, chúng tôi thấy rằng
ChatGPT có khả năng hiểu cú pháp markdown trong
prompt. Vì vậy, trong prompt nhiệm vụ, chúng tôi thay
đổi #{Code} thành "'''#{Code}'''" làm prompt nhiệm vụ
cập nhật (Bảng I-P4). Thử nghiệm trên các mẫu với
prompt xử lý, độ chính xác sinh được cải thiện trên
CodeBLEU (45.28) nhưng không trên BLEU (8.55).
Sau khi cập nhật định dạng mã trong prompt nhiệm vụ,
chúng tôi đạt được tăng cường thêm với BLEU=15.44
và CodeBLEU=45.00.

Giống như sinh T2C, chúng tôi trích xuất yêu cầu sử
dụng API và xử lý ngoại lệ từ mã ground-truth. Tiếp
theo, chúng tôi thêm thông tin này vào prompt nhiệm
vụ làm prompt hành vi: "dịch mã C# thành mã Java:
'''#{Code}''' #{that calls ...} with[out] exception handling"
(Bảng I-P5). Kết quả thí nghiệm trên các mẫu cho thấy
bằng cách thêm yêu cầu sử dụng API (BLEU=13.37 và
CodeBLEU=46.17), độ chính xác sinh sẽ được cải thiện
nhẹ về CodeBLEU. Hơn nữa, sử dụng toàn bộ prompt
hành vi cũng cho thấy giảm BLEU (8.90) và tăng nhẹ
CodeBLEU (46.88). Chúng tôi quan sát rằng ChatGPT
có thể hiểu ngữ cảnh dịch và sinh kết quả tốt. Nhưng
thêm yêu cầu nhiều hơn có thể mang lại sự không chắc
chắn cho sinh. Do đó, prompt hành vi này có thể có
tác động tiêu cực đến nhiệm vụ sinh C2C.

D. Câu hỏi Nghiên cứu
Trong nghiên cứu này, chúng tôi đề xuất một phương
pháp để hướng dẫn ChatGPT cho hai nhiệm vụ sinh
mã. Để xác minh hiệu quả của phương pháp và phân
tích các yếu tố có ảnh hưởng liên quan, nghiên cứu
này điều tra các RQ sau:

RQ1: Prompt được thiết kế hiệu quả như thế nào với
ChatGPT?
Như được mô tả trong Phần IV-B và IV-B, chúng tôi
tận dụng chiến lược CoT [42] để tăng cường thủ công
prompt cho hai nhiệm vụ sinh mã với tối ưu hóa nhiều
bước. RQ đầu tiên có ý định đánh giá hiệu quả của
prompt được thiết kế trên các bộ dữ liệu thử nghiệm
tương ứng, và xác minh tính hợp lệ của phương pháp
thiết kế của chúng tôi.

RQ2: Yêu cầu ngắn gọn ảnh hưởng đến ChatGPT như
thế nào?
Trong thiết kế prompt, chúng tôi quan sát rằng ChatGPT
thường sinh mã chi tiết, phức tạp hơn nhiều so với
ground truth. Vì vậy, một mục tiêu của tối ưu hóa
nhiều bước là hướng dẫn ChatGPT sinh mã ngắn gọn
với một loạt prompt. Đáng để điều tra liệu hiệu suất
sinh có thể được cải thiện thêm bằng cách yêu cầu
trực tiếp ChatGPT sinh ngắn gọn hay không.

RQ3: Cài đặt phiên ảnh hưởng đến ChatGPT như thế nào?
Khi giao tiếp với ChatGPT, chúng tôi bắt đầu một phiên
riêng lẻ cho mỗi prompt. Đồng thời, được biết rõ rằng
ChatGPT có thể học ngữ cảnh phiên và sinh phản hồi
tốt hơn từ ngữ cảnh [42], [49]. Do đó, RQ này có ý
định trả lời liệu ChatGPT có thể sinh mã tốt hơn bằng
cách nhập phiên với một số prompt hay không.

RQ4: Tính ngẫu nhiên sinh ảnh hưởng đến ChatGPT như
thế nào?
Được biết rằng ChatGPT có thể sinh mã với sự khác
biệt nhỏ mỗi lần cho cùng một prompt [50], [51]. Để
điều tra tính ngẫu nhiên ảnh hưởng đến hiệu suất sinh
như thế nào, chúng tôi chạy lại ChatGPT được hướng
dẫn nhiều lần và phân tích tính ổn định của hiệu suất
sinh.

V. KẾT QUẢ
Phần này trình bày cài đặt thí nghiệm và kết quả cho
bốn RQ được mô tả trong Phần IV-D.

A. Hiệu quả của Prompt được Thiết kế (RQ1)
Mục tiêu. Để đánh giá hiệu quả của prompt được thiết
kế, chúng tôi có ý định trình bày một số baseline, thử
nghiệm chúng trên dữ liệu thử nghiệm của các nhiệm
vụ sinh mã tương ứng, và phân tích hiệu quả về độ
chính xác dự đoán.

Phương pháp. Nghiên cứu này trình bày ba baseline
có thể sinh mã bằng cách sử dụng ChatGPT với ba
mức độ prompt: 1) ChatGPT-task, chúng tôi sử dụng
prompt nhiệm vụ trong Bảng I(P1) làm đầu vào của
ChatGPT, vì chúng có thể đại diện cho điều kiện chat
thông thường với yêu cầu sinh mã trực tiếp. 2) ChatGPT-detail,
chúng tôi hợp nhất prompt của nhiệm vụ, ngữ cảnh,
xử lý, và nhiệm vụ cập nhật cho hai nhiệm vụ trong
Bảng I(P1-P4). Lưu ý rằng sinh T2C không có prompt
nhiệm vụ cập nhật (P4) trong khi sinh C2C không chứa
prompt ngữ cảnh (P2). 3) ChatGPT-behaviour, dựa trên
baseline ChatGPT-detail, chúng tôi tiếp tục cập nhật
prompt nhiệm vụ với prompt hành vi (Bảng I-P5) để
cung cấp hướng dẫn thêm về sinh mã. Mặc dù Phần
IV-C cho thấy P5 có tác động tiêu cực đến nhiệm vụ
sinh C2C cho dữ liệu mẫu, chúng tôi vẫn thiết lập
baseline này cho sinh C2C để xác nhận thêm quan sát
trước đó. Dữ liệu thử nghiệm và chỉ số đánh giá đã
được trình bày chi tiết trong Phần III.

Kết quả. Trong nhiệm vụ sinh T2C, Bảng III cho thấy
ChatGPT-task đạt độ chính xác sinh BLEU=5.63 và
CodeBLEU=28.05. Đối với ChatGPT-detail với một
số prompt mở rộng, hiệu suất sinh của nó là BLEU=14.09
và CodeBLEU=39.90, vượt trội ChatGPT-task lần lượt
140.27% và 42.25% về BLEU và CodeBLEU. Ngoài ra,
baseline cuối cùng ChatGPT-behaviour đạt hiệu suất
tốt hơn cho sinh T2C với BLEU=21.59 và CodeBLEU=48.69.
Chúng ta có thể thấy rằng ChatGPT-behaviour cải thiện
hiệu suất của ChatGPT-task lần lượt 283.48% và 73.58%
về BLEU và CodeBLEU. Những kết quả này cho thấy
prompt được thiết kế của chúng tôi kết hợp với phương
pháp thiết kế có thể cải thiện đáng kể sinh T2C cho
ChatGPT.

Về nhiệm vụ sinh C2C, Bảng III cho thấy ChatGPT-tasks
có thể đạt hiệu suất tốt hơn (BLEU=10.61 và CodeBLEU=46.12)
so với nhiệm vụ sinh T2C. Hơn nữa, ChatGPT-detail
cho thấy độ chính xác sinh tốt hơn với BLEU=15.79
và CodeBLEU=47.71, vượt trội ChatGPT lần lượt 48.82%
và 3.45%. Tuy nhiên, chúng ta có thể thấy rằng ChatGPT-behaviour
cho thấy hiệu suất kém hơn với BLEU=9.47 và CodeBLEU=47.38,
tăng ChatGPT-task lần lượt -10.74% và 2.32%. Do đó,
ChatGPT-detail cho thấy hiệu suất tốt hơn ChatGPT-behaviour.
Những kết quả này ngụ ý rằng đối với nhiệm vụ C2C,
prompt được thiết kế cũng có thể cải thiện hiệu suất
sinh cho ChatGPT.

Trả lời RQ1: Phương pháp thiết kế prompt của chúng
tôi có thể cải thiện đáng kể hiệu suất của các nhiệm
vụ sinh T2C và C2C bằng cách hướng dẫn ChatGPT
với prompt tốt hơn.

B. Tác động của Yêu cầu Ngắn gọn (RQ2)
Mục tiêu. Trong thiết kế prompt của chúng tôi, chúng
tôi hướng dẫn ChatGPT xóa các thành phần mã không
liên quan. Tuy nhiên, chúng tôi quan sát rằng mã được
sinh vẫn phức tạp hơn ground-truth. Không có thêm
thông tin, chúng tôi không thể thiết kế prompt tốt hơn.
Do đó, RQ này có ý định điều tra liệu chúng tôi có
thể thay đổi prompt nhiệm vụ bằng cách yêu cầu trực
tiếp ChatGPT sinh mã ngắn gọn hơn, thay vì thêm
thủ công các hướng dẫn cụ thể.

Phương pháp. RQ1 cho thấy ChatGPT-behaviour và
ChatGPT-detail là baseline tốt nhất cho các nhiệm vụ
sinh T2C và C2C. Để thêm yêu cầu ngắn gọn vào prompt
của chúng, chúng tôi tìm thấy một cách khả thi và
đơn giản để thêm từ "ngắn gọn" trước mục tiêu sinh.
Cụ thể, đối với sinh T2C, prompt hành vi của ChatGPT-behaviour
được thay đổi thành "viết một phương thức Java ngắn
gọn..." (Bảng I-P5). Tương tự, đối với nhiệm vụ sinh
C2C, prompt nhiệm vụ của ChatGPT-detail được cập
nhật: "dịch mã C# thành mã Java ngắn gọn..." (Bảng
I-P4). Để đánh giá tác động của yêu cầu ngắn gọn,
chúng tôi thử nghiệm prompt đã sửa đổi trên dữ liệu
thử nghiệm.

Kết quả. Đối với nhiệm vụ sinh T2C, Bảng IV cho thấy
yêu cầu ngắn gọn là hữu ích (ChatGPT-behaviour-C)
với BLEU=26.86 và CodeBLEU=50.18. So với baseline
tốt nhất trong RQ1 (ChatGPT-behaviour), hai chỉ số
đánh giá này được tăng cường thêm lần lượt 24.41%
và 3.06%. Đồng thời, chúng tôi nhận thấy rằng bằng
cách thêm yêu cầu ngắn gọn vào nhiệm vụ sinh C2C,
hiệu suất sinh của ChatGPT-detail-C cho thấy giảm
nhẹ về CodeBLEU (46.62) so với ChatGPT-detail (baseline
tốt nhất trong RQ1), mặc dù điểm BLUE được cải thiện
6.08%. Những kết quả này cho thấy yêu cầu ngắn gọn
hữu ích cho sinh T2C nhưng không cho sinh C2C.

Trả lời RQ2: Thêm yêu cầu ngắn gọn vào prompt có
thể cải thiện thêm hiệu suất của sinh T2C, nhưng cho
thấy tác động tiêu cực nhỏ trên sinh C2C.

C. Tác động của Cài đặt Phiên (RQ3)
Mục tiêu. Theo mặc định, chúng tôi mở một phiên riêng
lẻ cho mỗi prompt và giao tiếp với ChatGPT. Ngược
lại, giao tiếp có thể hoạt động với phiên liên tục tạo
phản hồi cho một số prompt. Theo cách này, ChatGPT
có thể học từ ngữ cảnh và có thể sinh phản hồi tốt
hơn cho các nhiệm vụ sinh mã. Vì vậy, RQ này nhằm
phân tích tác động của cài đặt phiên.

Phương pháp. Trong quá trình giao tiếp với ChatGPT,
chúng tôi sử dụng một phiên để sinh mã cho một số
prompt. Khi số lượng đạt giới hạn tối đa, prompt tiếp
theo sẽ không nhận được phản hồi. Sau đó, chúng tôi
bắt đầu phiên mới khác. Theo cách này, chúng tôi có
thể đảm bảo rằng mỗi phiên được sử dụng đầy đủ và
ChatGPT có thể hiểu ngữ cảnh tốt hơn. Chúng tôi biết
rằng lượng thông tin ngữ cảnh sẽ thấp hơn cho các
prompt trước đó trong phiên. Đáng để điều tra cài
đặt phiên ảnh hưởng đến hiệu suất sinh mã như thế nào.

Kết quả. Như được hiển thị trong Bảng V, sử dụng
phiên liên tục (ChatGPT-behaviour-CS) không cho
thấy cải thiện tổng thể cho sinh T2C với BLEU=29.29
và CodeBLEU=49.74. So với baseline tốt nhất trong
RQ2 (ChatGPT-behaviour-C), điểm BLEU được cải thiện
9.05% nhưng CodeBLEU giảm 0.88%. Mặt khác, phiên
liên tục có thể trình bày cải thiện cho nhiệm vụ sinh
C2C (ChatGPT-detail-S) với BLEU=16.82 và CodeBLEU=48.80.
So với baseline tốt nhất ChatGPT-detail trong RQ2,
các chỉ số đánh giá được cải thiện thêm lần lượt 6.52%
và 2.28%. Những kết quả thí nghiệm này cho thấy phiên
liên tục có lợi cho nhiệm vụ sinh C2C nhưng không
có cải thiện cho nhiệm vụ sinh T2C. Do đó, phiên riêng
lẻ phù hợp hơn cho sinh T2C với prompt được thiết kế.

Trả lời RQ3: Phiên liên tục hữu ích cho nhiệm vụ sinh
T2C trong khi phiên riêng lẻ phù hợp cho nhiệm vụ
sinh C2C.

D. Tác động của Tính ngẫu nhiên Sinh (RQ4)
Mục tiêu. Thông thường, ChatGPT sinh phản hồi với
sự khác biệt nhỏ để cân bằng độ chính xác và sáng
tạo. Vì vậy, tính ngẫu nhiên sinh có thể ảnh hưởng
đến hiệu suất sinh mã. RQ này điều tra tính ngẫu nhiên
ảnh hưởng đến hiệu quả của prompt được thiết kế như
thế nào.

Phương pháp. Để đạt được mục tiêu, chúng tôi chạy
các baseline tốt nhất (tức là ChatGPT-behaviour-C và
ChatGPT-detail-S) trong RQ3 năm lần. Chúng tôi tính
trung bình (AVG) và độ lệch chuẩn (STD) của hiệu
suất của các kết quả sinh nhiều lần này. Dựa trên
những phép đo này, chúng tôi phân tích tính ổn định
sinh và tác động của tính ngẫu nhiên.

Kết quả. Từ Bảng VI, chúng ta có thể quan sát rằng
năm vòng sinh T2C cho thấy hiệu suất ổn định, trong
đó BLEU dao động từ 26.86 đến 27.02 với trung bình=26.93
và STD=0.08; CodeBLEU dao động từ 50.07 đến 50.20
với trung bình=50.16 và STD=0.05. Đồng thời, nhiều
lần chạy sinh C2C cũng cho thấy độ chính xác dự đoán
ổn định, trong đó BLEU dao động từ 16.82 đến 17.34
với trung bình=17.18 và STD=0.21; CodeBLEU dao động
từ 48.80 đến 49.17 với trung bình=48.86 và STD=0.33.
Những kết quả này cho thấy thử nghiệm ChatGPT với
prompt được thiết kế của chúng tôi sẽ sinh phản hồi
ổn định. Chúng tôi quan sát rằng lý do chính là các
hướng dẫn trong prompt cụ thể nên tính ngẫu nhiên
sinh bị hạn chế. Vì chúng tôi quan sát rằng tác động
của tính ngẫu nhiên là không đáng kể, chúng tôi không
mở rộng RQ này với nhiều vòng thí nghiệm hơn.

Trả lời RQ4: Tính ngẫu nhiên sinh cho thấy ít tác động
đến các nhiệm vụ sinh mã do các hướng dẫn cụ thể
được mô tả trong prompt được thiết kế.

VI. THẢO LUẬN
Phần này cung cấp một số phân tích định tính về prompt
được thiết kế. Cụ thể, Phần VI-A so sánh hiệu suất
sinh mã của chúng tôi với các mô hình tinh chỉnh hiện
có. Phần VI-B và VI-C phân tích tính đúng đắn và chất
lượng của mã được sinh bởi ChatGPT với prompt được
thiết kế của chúng tôi.

--- TRANG 3 ---
A. So sánh với Mô hình Tinh chỉnh
Mục tiêu. Phần V cho thấy prompt được thiết kế có
thể hướng dẫn ChatGPT sinh mã tốt hơn đáng kể. Kết
quả này ngụ ý hiệu quả của mô hình "tiền huấn luyện,
prompt, dự đoán" cho LLM như được mô tả trong Phần
II-B. Tuy nhiên, nhiều LLM dựa trên mô hình "tiền huấn
luyện, tinh chỉnh" đã được áp dụng thành công trong
các nhiệm vụ sinh mã như được minh họa trong Phần
II-A. Do đó, chúng tôi muốn so sánh hiệu suất của hai
mô hình này. Cụ thể, chúng tôi điều tra phương pháp
của chúng tôi hiệu quả như thế nào so với các LLM
tinh chỉnh hiện có trên các nhiệm vụ sinh mã.

Phương pháp. Trong nghiên cứu này, chúng tôi thử
nghiệm phương pháp của chúng tôi trên bộ dữ liệu
CodeXGlue được sử dụng rộng rãi [5], cung cấp một
số mô hình benchmark. Chúng tôi sử dụng kết quả thí
nghiệm được báo cáo bởi Lu et al. [5] để so sánh.
Hơn nữa, chúng tôi xem xét tất cả các công trình liên
quan được trình bày trong Phần II-A. Chúng tôi thấy
rằng Wang et al. [33] cũng thử nghiệm mô hình đề xuất
CodeT5 và mô hình baseline PLBART [34] trên CodeXGlue,
vì vậy chúng tôi bao gồm những mô hình này trong
so sánh. Chúng tôi loại trừ các báo cáo khác trong
công trình liên quan vì chúng không sử dụng bộ dữ
liệu CodeXGlue hoặc không báo cáo chỉ số CodeBLEU
theo [5].

Kết quả. Bảng VII minh họa các LLM tinh chỉnh được
bao gồm cho hai nhiệm vụ sinh mã, mô tả và tham
chiếu của LLM liên quan, và điểm BLEU và CodeBLEU
được báo cáo. Chúng tôi cũng đặt cài đặt prompt tốt
nhất của chúng tôi (ChatGPT-best) trong bảng. Chúng
ta có thể thấy rằng ChatGPT-best cho thấy hiệu suất
tốt nhất trên các nhiệm vụ sinh T2C về CodeBLEU.
Kết quả này ngụ ý rằng hướng dẫn ChatGPT với prompt
được thiết kế của chúng tôi vượt trội các LLM tinh
chỉnh tiên tiến khác. Tuy nhiên, đối với nhiệm vụ sinh
C2C, ChatGPT-best đạt thứ hạng thứ năm, chỉ vượt
trội PBSMT [54]. Hiệu suất kém hơn này trên nhiệm
vụ C2C có thể do thông tin ngữ cảnh hạn chế, vì vậy
chúng tôi không thể mở rộng prompt được thiết kế
với hướng dẫn cụ thể hơn như được hiển thị trong
Phần V-A. Hơn nữa, những kết quả này có thể chứng
minh khả năng tiềm năng của mô hình "tiền huấn luyện,
prompt, dự đoán", vì hiệu suất có thể được cải thiện
thêm bằng cách cung cấp prompt với hướng dẫn cụ
thể hơn hoặc tinh chỉnh ChatGPT với dữ liệu huấn
luyện liên quan.

Phát hiện 1: Hướng dẫn ChatGPT với prompt được thiết
kế của chúng tôi vượt trội các LLM tinh chỉnh tiên tiến
cho sinh T2C, nhưng nó cho thấy thứ hạng kém hơn
trên sinh C2C do thông tin ngữ cảnh hạn chế được biểu
đạt trong prompt.

B. Tính đúng đắn của Sinh mã
Mục tiêu. CodeBLEU (hoặc BLEU) là một chỉ số hiệu
quả và được sử dụng rộng rãi cho đánh giá tự động.
Nhưng một mã được sinh với điểm cao có thể không
đúng. Do đó, chúng tôi muốn điều tra tính đúng đắn
của mã được sinh từ cài đặt prompt tốt nhất của chúng
tôi.

Phương pháp. Chúng tôi chọn ngẫu nhiên 100 mẫu từ
mã được sinh của mỗi nhiệm vụ sinh mã (T2C và C2C).
Tính đúng đắn được đo bằng tương đương chức năng
giữa mã được sinh và ground-truth. Tính liên quan được
bỏ phiếu bởi ba tác giả (thứ nhất, thứ hai, và thứ tư),
trong đó tính liên quan được xác định khi số phiếu
bầu lớn hơn hoặc bằng hai.

Kết quả. Bảng VIII cho thấy trong số 100 mẫu, nhiệm
vụ T2C chỉ sinh 31 mã tương đương với ground-truth
trong hành vi chức năng. Chúng tôi quan sát rằng
CodeBLEU cao hơn chỉ chỉ ra khớp từ vựng đúng, khớp
cú pháp, và khớp luồng dữ liệu, nhưng không nhất
thiết gợi ý tương đương chức năng. Đồng thời, mô tả
NL cho sinh T2C thường không cụ thể, và ChatGPT
có thể hiểu sai yêu cầu. Một ví dụ đơn giản là đối
với mã ground-truth "String function(){return namespaceURI;}"
có nghĩa là trả về biến namespaceURI, nhưng mô tả
NL được cung cấp trong bộ dữ liệu là "Lấy namespace
WS-ReliableMessaging để sử dụng cho mã hóa và giải
mã thông điệp". Do đó, đối với prompt của sinh T2C,
mô tả NL có thể cần được tinh chỉnh theo một số cách.

Đối với nhiệm vụ sinh C2C, chúng tôi thấy 59 đoạn
mã được sinh tương đương chức năng với ground-truth
của chúng, tốt hơn nhiều so với nhiệm vụ T2C. Chúng
tôi nhận thấy rằng dịch từ mã C# cung cấp nhiều ngữ
cảnh hữu ích để ChatGPT có thể sinh mã Java tương
ứng từng dòng. Tuy nhiên, sinh T2C chỉ hoạt động cho
mã với API được sử dụng thông thường.

Phát hiện 2: ChatGPT với prompt tốt nhất cho thấy
tính đúng đắn tốt hơn trên nhiệm vụ C2C so với nhiệm
vụ T2C, vì mô tả NL không nghiêm ngặt.

C. Chất lượng của Sinh mã
Mục tiêu. Chất lượng là một đặc điểm quan trọng của
một mã được sinh tốt, bất chấp tính đúng đắn của
sinh mã. Trong nghiên cứu này, chúng tôi cũng điều
tra chất lượng của mã được sinh bởi ChatGPT với
prompt được thiết kế của chúng tôi. Đồng thời, chúng
tôi cũng kiểm tra chất lượng của mã ground-truth để
so sánh chất lượng của nó với mã được sinh.

Phương pháp. Để đo chất lượng sinh mã, chúng tôi
sử dụng SonarQube (phiên bản 9.8) [55], một nền
tảng mã nguồn mở được sử dụng rộng rãi để phân tích
và theo dõi chất lượng mã [56], [57]. Nó có thể kiểm
tra ba loại vấn đề chất lượng (bug, lỗ hổng, và code
smell) trong năm mức độ nghiêm trọng (tức là blocker,
critical, major, minor, và info) [55]. Trong nghiên cứu
này, chúng tôi đo chất lượng sinh mã bằng cách đếm
số lượng mã chứa vấn đề critical hoặc blocker. Điều
này vì hai mức độ nghiêm trọng này có tác động mạnh
thường được nhà phát triển xem xét, trong đó mức
blocker có khả năng cao hơn mức critical.

Kết quả. Bảng IX cho thấy đối với sinh T2C, mã được
sinh bởi ChatGPT-best cho thấy chất lượng tốt hơn
nhẹ so với ground-truth. Cụ thể, SonarQube tìm thấy
một bug critical nhắc nhở chúng tôi đảm bảo biến
cục bộ khác không trước khi thực hiện phép chia. Và
29 vấn đề khác thuộc về code smell critical cần chú
ý thêm. Ngược lại, ground-truth liên quan chứa 35
code smell. Đối với sinh C2C, mã được sinh có 62
code smell, cao hơn nhiều so với số lượng ground-truth
liên quan (17 code smell). Trong hai nhiệm vụ này,
mã được sinh không có bug hoặc lỗ hổng nghiêm trọng.

Chúng tôi thấy rằng các code smell được xác định chủ
yếu cung cấp sáu loại đề xuất: 1) định nghĩa hằng số
thay vì nhân bản String; 2) thay thế cuộc gọi "replaceAll()"
bằng "replace()"; 3) giảm độ phức tạp nhận thức của
mã; 4) thêm trường hợp mặc định vào switch, không
override phương thức Object.finalize(); 5) đổi tên
phương thức để tránh hiểu nhầm; và 6) sử dụng copy
constructor hoặc copy factory thay vì triển khai "clone".
Chúng tôi tin rằng đáng để giải quyết bug và code
smell được phát hiện bởi SonarQube, mặc dù số lượng
không lớn so với tổng số của bộ dữ liệu. Do đó, ChatGPT
có thể không thể đảm bảo chất lượng của mã được
sinh, cần điều tra thêm.

Phát hiện 3: Mã được sinh bởi ChatGPT không chứa
bug hoặc lỗ hổng nghiêm trọng trên các bộ dữ liệu
thí nghiệm nhưng chúng chứa nhiều code smell, nên
được nhà phát triển giải quyết.

VII. HÀM Ý
Dựa trên kết quả thí nghiệm và thảo luận của chúng
tôi, phần này cung cấp hàm ý cho nhà phát triển và
nhà nghiên cứu.

Mẹo Sử dụng Prompt ChatGPT cho Nhà phát triển.
Kết quả thí nghiệm của chúng tôi cho thấy hướng dẫn
ChatGPT với prompt được chế tác cẩn thận có thể cải
thiện đáng kể hiệu suất sinh mã. Để tận dụng tối đa
khả năng của ChatGPT, nhà phát triển nên cung cấp
prompt với ngữ cảnh lập trình phong phú, bao gồm
lớp liên quan, biến thành viên, và hàm. Ngoài ra, ChatGPT
có thể hiểu mã, vì vậy nhà phát triển có thể hướng
dẫn nó tiền xử lý mã, như xóa tóm tắt và thay đổi
tên biến. Bao quanh đoạn mã trong "'''" cũng có thể
giúp ChatGTP hiểu các khối mã tốt hơn với cú pháp
markdown. Để sinh mã với API mong đợi hoặc dưới
dạng ngắn gọn, nhà phát triển có thể yêu cầu trực tiếp
ChatGPT. Chat với ChatGPT trong một phiên liên tục
cũng có thể có lợi vì nó có thể hiểu ngữ cảnh và tinh
chỉnh phản hồi của mình. Với tính năng chuỗi tư duy,
nhà phát triển có thể tối ưu hóa prompt từng bước
bằng cách xem xét phản hồi của ChatGPT. Tuy nhiên,
nhà phát triển nên cẩn thận về chất lượng của mã được
sinh, ngay cả khi sinh cấp độ hàm xuất hiện không
có bug hoặc lỗ hổng nghiêm trọng trong thí nghiệm
của chúng tôi.

Hướng Nghiên cứu Tương lai Tiềm năng cho Nhà nghiên
cứu. Nghiên cứu này chứng minh rằng cung cấp ChatGPT
với prompt được cải thiện có thể tăng cường hiệu suất
sinh mã, vượt trội so với các LLM tinh chỉnh tiên tiến.
Các nghiên cứu tương lai có thể điều tra phương pháp
tự động thiết kế và tối ưu hóa prompt cho các nhiệm
vụ sinh mã; thiết kế chỉ số đánh giá tốt hơn để tự
động đánh giá tính đúng đắn của mã được sinh; và
tiếp tục đánh giá và cải thiện chất lượng sinh mã.
Nghiên cứu của chúng tôi cũng làm nổi bật tiềm năng
của các LLM như ChatGPT, sử dụng mô hình "tiền huấn
luyện, prompt, dự đoán" và kỹ thuật prompt, để tác
động đến lĩnh vực nghiên cứu SE.

VIII. ĐE DỌA ĐỘ HỢP LỆ
Có một số mối đe dọa tiềm năng ảnh hưởng đến độ
hợp lệ của kết quả thí nghiệm và kết luận của chúng tôi.

Nhiệm vụ và Bộ dữ liệu Hạn chế. Nghiên cứu này đề
xuất một phương pháp cho thiết kế prompt trên hai
nhiệm vụ sinh mã. Phương pháp có thể không phù hợp
cho các nhiệm vụ sinh khác, như hoàn thành mã [58]
và sinh test case [59]. Đối với các nhiệm vụ sinh T2C
và C2C được điều tra, chúng tôi chỉ chọn một bộ dữ
liệu CodeXGlue [5] làm đối tượng nghiên cứu, mặc
dù nó là bộ dữ liệu được sử dụng rộng rãi. Trên nhiệm
vụ sinh C2C, chúng tôi thử nghiệm sinh từ C# sang
Java. Không chắc chắn liệu kết quả thí nghiệm và
phát hiện của chúng tôi có thể được mở rộng sang các
ngôn ngữ khác (ví dụ: Python và Go) và sinh ngược
(tức là từ Java sang C#). Trong tương lai gần, chúng
tôi dự định điều tra prompt được thiết kế của chúng
tôi trên nhiều ngôn ngữ lập trình hơn, bộ dữ liệu khác,
và nhiều loại nhiệm vụ sinh mã hơn.

Thiết kế Prompt Thủ công. Giống như các nghiên cứu
kỹ thuật prompt khác với xây dựng thủ công, thiết
kế prompt và tối ưu hóa nhiều bước được thực hiện
theo hiểu biết và quan sát của con người. Kiến thức
của người thiết kế có thể ảnh hưởng đến hiệu suất
của prompt được sử dụng. Hơn nữa, lựa chọn thiết
kế và kết hợp của chúng tôi dựa trên 100 mẫu được
chọn ngẫu nhiên, và kích thước và tính ngẫu nhiên
của việc lấy mẫu có thể mang lại các lựa chọn khác
nhau. Tuy nhiên, nghiên cứu này nhằm khám phá tính
khả thi của thiết kế prompt và điều tra một số yếu
tố có ảnh hưởng liên quan. Kết quả thí nghiệm của
chúng tôi chứng minh hiệu quả của prompt được thiết
kế. Trong tương lai, chúng tôi muốn điều tra kỹ thuật
prompt với các phương pháp xây dựng tự động [28], [41].

Thử nghiệm Hạn chế. Trong nghiên cứu của chúng tôi,
chúng tôi tiến hành một số lượng hạn chế thử nghiệm
như được trình bày trong Phần V, như kết hợp lựa
chọn và kết hợp prompt, yêu cầu ngắn gọn và cài đặt
phiên, và nhiều lần chạy cho tính ngẫu nhiên sinh.
Chúng tôi cho rằng nhiều thử nghiệm hơn có thể giúp
chúng tôi cải thiện thêm prompt được thiết kế và tăng
cường kết luận và phát hiện của chúng tôi. Tuy nhiên,
gọi API ChatGPT trên toàn bộ bộ dữ liệu đòi hỏi chi
phí cao. Do đó, chúng tôi chỉ thử nghiệm một số lượng
hạn chế lựa chọn có khả năng cải thiện hiệu suất sinh,
để chúng tôi có thể chứng minh tầm quan trọng của
thiết kế prompt cho ChatGPT.

Đánh giá Con người. Để đánh giá tính đúng đắn của
mã được sinh, chúng tôi chọn ngẫu nhiên 100 mẫu
để phân tích tính liên quan giữa mã được sinh và
ground-truth. Tính liên quan được xác định bởi đánh
giá con người. Để giảm thiểu tác động của thiên lệch
chủ quan, các quyết định dựa trên bỏ phiếu của ba
tác giả. Mặc dù số lượng mẫu hạn chế, phân tích tính
đúng đắn của những mẫu ngẫu nhiên này cung cấp
một số hiểu biết hữu ích cho các nghiên cứu tiếp theo.

IX. KẾT LUẬN
Trong bài báo này, chúng tôi đã thiết kế và cải thiện
prompt để hướng dẫn ChatGPT trên hai nhiệm vụ sinh
mã, bao gồm sinh văn bản thành mã và sinh mã thành
mã. Kết quả thí nghiệm của chúng tôi cho thấy hiệu
quả của prompt khi yêu cầu ChatGPT sinh mã trên
bộ dữ liệu CodeXGlue được sử dụng rộng rãi. Hơn nữa,
chúng tôi điều tra các yếu tố có ảnh hưởng để thiết
kế prompt cho các nhiệm vụ sinh mã. Bên cạnh đó,
chúng tôi so sánh hiệu suất của prompt tốt nhất với
các LLM tinh chỉnh tiên tiến, và đánh giá tính đúng
đắn và chất lượng của mã được sinh bởi ChatGPT.
Dựa trên phát hiện của chúng tôi, chúng tôi trình bày
các hướng nghiên cứu tương lai tiềm năng.

--- TRANG 4 ---
[Phần tài liệu tham khảo từ trang 4 trở đi được giữ nguyên như trong bản gốc]
