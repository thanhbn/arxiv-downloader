# 2111.04130.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/icl/2111.04130.pdf
# Kích thước file: 2326143 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn:
Một Framework Đơn Giản và Hiệu Quả
Xingcheng Yao* 1Yanan Zheng* 2Xiaocong Yang3 4Zhilin Yang1 5 4

Tóm tắt
Các mô hình ngôn ngữ được tiền huấn luyện đã trở thành phương pháp tiêu chuẩn cho nhiều tác vụ NLP do hiệu suất mạnh mẽ, nhưng chúng rất tốn kém để huấn luyện. Chúng tôi đề xuất một framework học tập đơn giản và hiệu quả TLM không dựa vào tiền huấn luyện quy mô lớn1. Với một số dữ liệu tác vụ được gán nhãn và một kho ngữ liệu chung lớn, TLM sử dụng dữ liệu tác vụ làm truy vấn để truy xuất một tập con nhỏ từ kho ngữ liệu chung và tối ưu hóa đồng thời mục tiêu tác vụ và mục tiêu mô hình hóa ngôn ngữ từ đầu. Trên tám tập dữ liệu phân loại trong bốn lĩnh vực, TLM đạt được kết quả tốt hơn hoặc tương tự với các mô hình ngôn ngữ được tiền huấn luyện (ví dụ: RoBERTa-Large) trong khi giảm FLOPs huấn luyện đi hai bậc độ lớn. Với độ chính xác và hiệu quả cao, chúng tôi hy vọng TLM sẽ góp phần dân chủ hóa NLP và thúc đẩy sự phát triển của nó2.

1. Giới thiệu
Các mô hình ngôn ngữ được tiền huấn luyện (PLM) đã thu hút nhiều sự chú ý từ cộng đồng xử lý ngôn ngữ tự nhiên (NLP). Các mạng nơ-ron dựa trên kiến trúc Transformer (Vaswani et al., 2017) được huấn luyện trên các kho ngữ liệu chung lớn cho các tác vụ mô hình hóa ngôn ngữ tự giám sát như masked language modeling (Devlin et al., 2019; Liu et al.,

*Đóng góp ngang nhau1Institute for Interdisciplinary Information Sciences, Tsinghua University2Department of Computer Science and Technology, Tsinghua University3School of Economics and Management, Tsinghua University4Recurrent AI, Inc5Shanghai Qi Zhi Institute. Liên hệ: Zhilin Yang <zhiliny@tsinghua.edu.cn >.

Kỷ yếu Hội nghị Quốc tế lần thứ 39 về Học máy, Baltimore, Maryland, USA, PMLR 162, 2022. Bản quyền 2022 thuộc về (các) tác giả.

1Theo nghĩa rộng nhất, tiền huấn luyện có nghĩa là huấn luyện trên một số mục tiêu trước khi tối ưu hóa các tác vụ mục tiêu. Ngược lại, trong suốt bài báo này, chúng tôi sử dụng "tiền huấn luyện" để chỉ việc huấn luyện mô hình ngôn ngữ không phụ thuộc vào tác vụ trên một kho ngữ liệu chung lớn, như BERT (Devlin et al., 2019).

2Mã nguồn, điểm kiểm tra mô hình và tập dữ liệu của chúng tôi được công khai tại: https://github.com/yaoxingcheng/TLM

[BIỂU ĐỒ: Hiển thị hiệu suất trung bình trên tám tác vụ so với FLOPs tương đối w.r.t. RoBERTa-Large (Liu et al., 2019). TLM vượt trội hơn một chút so với RoBERTa-Large trong khi giảm FLOPs đi hai bậc độ lớn.]

2019; Raffel et al., 2019), mô hình hóa ngôn ngữ tự hồi quy (Radford et al., 2018; Brown et al., 2020), mô hình hóa ngôn ngữ hoán vị (Yang et al., 2019), v.v., và sau đó được tinh chỉnh trên một lượng nhỏ dữ liệu được gán nhãn cho các tác vụ downstream. Framework tiền huấn luyện-tinh chỉnh này đã cải thiện đáng kể hiệu suất của nhiều tác vụ NLP.

Tuy nhiên, mặc dù được coi là hiệu quả, tiền huấn luyện quy mô lớn thường rất tốn kém về mặt tính toán. Ví dụ, RoBERTa-Large (Liu et al., 2019), một PLM được sử dụng rộng rãi, tiêu tốn chi phí tính toán 4.36×1021 FLOPs3. Các PLM lớn hơn như GPT-3 (Brown et al., 2020) tiêu tốn gấp 50 lần FLOPs để huấn luyện so với RoBERTa-Large. Chi phí đắt đỏ của tiền huấn luyện quy mô lớn ngăn cản nhiều nhóm nghiên cứu có ngân sách hạn chế khỏi việc tiền huấn luyện các mô hình ngôn ngữ tùy chỉnh, khám phá các kiến trúc mạng nơ-ron mới, hoặc cải thiện các hàm mất mát tiền huấn luyện. Ngược lại, một số lượng lớn nhà nghiên cứu NLP phải dùng đến việc cải thiện các thuật toán tinh chỉnh, mà hiệu suất của chúng phần lớn bị giới hạn bởi quy trình tiền huấn luyện. Điều này tạo ra một rào cản cao cho nghiên cứu NLP và có thể không lý tưởng cho sự phát triển lâu dài của lĩnh vực này.

3Nó được tiền huấn luyện với 1.000 GPU V100 mỗi GPU có bộ nhớ 32GB trong khoảng một ngày.

--- TRANG 2 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

Mặc dù đã có những nỗ lực dành cho việc nghiên cứu và cải thiện hiệu quả của tiền huấn luyện mô hình ngôn ngữ (Clark et al., 2020; So et al., 2021; Tay et al., 2021; Chen et al., 2021), hầu hết chúng tập trung vào thiết kế các tác vụ tự giám sát hiệu quả về mẫu hoặc khám phá các kiến trúc Transformer hiệu quả phù hợp cho tiền huấn luyện. Các cải tiến của chúng bị hạn chế, với việc giảm chi phí tính toán (theo FLOPs) ít hơn một bậc độ lớn. Một hướng nghiên cứu khác nhắm vào việc giảm kích thước của PLM bằng cách sử dụng chưng cất (Sanh et al., 2019; Jiao et al., 2020) để cải thiện hiệu quả suy luận, nhưng các phương pháp này dựa vào việc tiền huấn luyện một PLM lớn trước khi chưng cất. Hơn nữa, các mô hình được chưng cất thường không hoạt động tốt bằng một số PLM tốt nhất không được chưng cất như RoBERTa-Large (Sanh et al., 2019; Jiao et al., 2020).

Công trình này khám phá các phương án thay thế cho mô hình tiền huấn luyện-tinh chỉnh tiêu chuẩn, nhằm cải thiện hiệu quả mạnh mẽ hơn mà không làm giảm hiệu suất. Chúng tôi đề xuất một framework đơn giản, hiệu quả, không cần tiền huấn luyện, gọi là Task-driven Language Modeling (TLM). Với một kho ngữ liệu chung lớn và một số dữ liệu tác vụ được gán nhãn, TLM trực tiếp huấn luyện mô hình từ đầu mà không dựa vào PLM. TLM được thúc đẩy bởi hai ý tưởng chính. Thứ nhất, con người làm chủ một tác vụ bằng cách chỉ sử dụng một phần nhỏ kiến thức thế giới (ví dụ: học sinh chỉ cần xem lại vài chương, trong tổng số tất cả sách trên thế giới, để ôn thi). Chúng tôi giả thuyết rằng có nhiều dư thừa trong kho ngữ liệu lớn cho một tác vụ cụ thể. Thứ hai, huấn luyện trên dữ liệu được gán nhãn có giám sát hiệu quả hơn nhiều về dữ liệu cho hiệu suất downstream so với tối ưu hóa mục tiêu mô hình hóa ngôn ngữ trên dữ liệu không có nhãn. Dựa trên những động lực này, TLM sử dụng dữ liệu tác vụ làm truy vấn để truy xuất một tập con nhỏ từ kho ngữ liệu chung. Tiếp theo là tối ưu hóa đồng thời mục tiêu tác vụ có giám sát và mục tiêu mô hình hóa ngôn ngữ sử dụng cả dữ liệu được truy xuất và dữ liệu tác vụ.

Chúng tôi đánh giá TLM trên tám tác vụ khác nhau bao gồm các lĩnh vực tin tức, đánh giá, khoa học máy tính và khoa học y sinh, theo thiết lập của Gururangan et al. (2020). TLM đạt được kết quả tốt hơn hoặc tương tự với BERT (Devlin et al., 2019) và RoBERTa (Liu et al., 2019) trong khi giảm FLOPs huấn luyện đi hai bậc độ lớn4.

2. Công trình liên quan

Mô hình Ngôn ngữ Được Tiền Huấn Luyện Các mô hình ngôn ngữ được tiền huấn luyện đã trở thành giải pháp thực tế cho nhiều tác vụ NLP (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2019; Brown et al., 2020; Yang et al., 2019). Những mô hình này thường được tiền huấn luyện trên một kho ngữ liệu quy mô lớn theo cách tự giám sát để học biểu diễn có ngữ cảnh của token trong ngôn ngữ tự nhiên, và sau đó được tinh chỉnh với dữ liệu được gán nhãn cho các tác vụ cụ thể. BERT (Devlin et al., 2019), một trong những PLM phổ biến nhất, được tiền huấn luyện trên kho ngữ liệu tiếng Anh 16GB sử dụng mục tiêu masked language modeling (tức là dự đoán các token bị che ngẫu nhiên). RoBERTa (Liu et al., 2019) kế thừa mục tiêu huấn luyện của BERT, nhưng được tiền huấn luyện trên kho ngữ liệu lớn hơn gồm 160GB văn bản tiếng Anh với kích thước batch lớn hơn và masking token động. Trong công trình này, chúng tôi lấy cả BERT và RoBERTa làm baseline chính.

Tiền Huấn Luyện Hiệu Quả cho NLP Có một hướng nghiên cứu dành riêng cho việc cải thiện hiệu quả của tiền huấn luyện mô hình ngôn ngữ. You et al. (2020) và Shoeybi et al. (2019) đã sử dụng song song dữ liệu và mô hình trên các thiết bị tính toán khác nhau để tăng tốc quy trình tiền huấn luyện. Tuy nhiên, tăng tốc thông qua song song hóa không thực sự giảm chi phí tính toán theo FLOPs để huấn luyện mô hình ở quy mô lớn. Chen et al. (2021) và So et al. (2021) đã cố gắng xác định các kiến trúc mạng nơ-ron hiệu quả cho tiền huấn luyện mô hình ngôn ngữ, dựa trên giả thuyết lottery ticket và tìm kiếm kiến trúc mạng nơ-ron. Những sửa đổi kiến trúc như vậy có thể mang lại 50%-70% giảm chi phí tính toán. Clark et al. (2020) và He et al. (2021) đã tích hợp các cơ chế được thiết kế thủ công vào tiền huấn luyện mô hình ngôn ngữ, như huấn luyện đối kháng và biểu diễn tách biệt của nội dung và vị trí, mang lại khoảng 50%-75% giảm chi phí tính toán. Gu et al. (2020) đề xuất sử dụng tiền huấn luyện được hướng dẫn bởi tác vụ với masking chọn lọc, giảm chi phí tính toán khoảng 50%. Trong công trình này, trực giao với các nghiên cứu nêu trên, chúng tôi điều tra cải thiện hiệu quả bằng cách giảm dư thừa dữ liệu huấn luyện. Phương pháp của chúng tôi cũng mang lại những cải tiến mạnh mẽ hơn.

Suy Luận Hiệu Quả của Mô hình Được Tiền Huấn Luyện Một hướng nghiên cứu khác nhằm cải thiện hiệu quả suy luận của PLM. Một số công trình cải thiện hiệu quả suy luận bằng cách chưng cất PLM lớn thành các mô hình kích thước nhỏ và sử dụng các mô hình được chưng cất để suy luận, như DistilBERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2020), MobileBERT (Sun et al., 2020), FastBERT (Liu et al., 2020), BORT (de Wynter & Perry, 2020), và BERT-of-Theseus (Xu et al., 2020). Các công trình khác tăng tốc suy luận bằng cách lượng tử hóa PLM với biểu diễn độ chính xác thấp trong suy luận, như Q8-BERT (Zafrir et al., 2019), Q-BERT (Shen et al., 2020), và I-BERT (Kim et al., 2021). Một loại công trình khác, như (Michel et al., 2019; Wang et al., 2020; Gordon et al., 2020), áp dụng pruning bằng cách loại bỏ các phần của PLM để làm cho chúng nhỏ hơn và nhanh hơn. Tuy nhiên, các phương pháp này dựa vào PLM lớn, và hiệu suất sau chưng cất, pruning, hoặc lượng tử hóa thường giảm ở một mức độ nhất định so với một số PLM tốt nhất (ví dụ: RoBERTa-Large). Ngược lại, phương pháp của chúng tôi không dựa vào tiền huấn luyện quy mô lớn và đạt được hiệu suất tốt hơn hoặc ít nhất là tương đương.

--- TRANG 3 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[BIỂU ĐỒ: So sánh giữa phương pháp tiền huấn luyện-tinh chỉnh truyền thống và framework TLM được đề xuất của chúng tôi]

Dữ liệu tác vụ
Tôi thích bộ phim Jaws, đạo diễn bởi Steven Spielberg năm 1975.

Kho ngữ liệu chung
Truy vấn → Truy xuất
Giới thiệu về Spielberg...
Mô tả về Jaws, 1975...
Tôi thích các bộ phim của Lucas...

Lựa chọn Dữ liệu
encoder → Học Tập Kết Hợp
Mục tiêu Tác vụ + Mục tiêu LM

Phương pháp truyền thống:
Kho ngữ liệu chung → encoder → Mục tiêu LM
Dữ liệu tác vụ → encoder → Mục tiêu Tác vụ

Phương pháp của chúng tôi:
Một tập con nhỏ của kho ngữ liệu

Thích Ứng Lĩnh Vực và Tác Vụ cho Mô hình Được Tiền Huấn Luyện
Tinh chỉnh thích ứng lĩnh vực là một phương pháp tinh chỉnh mô hình được tiền huấn luyện trên dữ liệu trong lĩnh vực sử dụng mục tiêu mô hình hóa ngôn ngữ. Nó đã được chứng minh là hiệu quả cho thích ứng lĩnh vực và tác vụ (Zhang et al., 2019; Gururangan et al., 2020; Li et al., 2020; Lee et al., 2020). Có một số khác biệt quan trọng giữa tinh chỉnh thích ứng lĩnh vực và TLM. Thứ nhất, TLM là một phương pháp tổng quát để cải thiện hiệu quả huấn luyện mà không sử dụng bất kỳ dữ liệu lĩnh vực bổ sung nào. Nó chỉ sử dụng kho ngữ liệu chung như trong BERT và RoBERTa. Ngược lại, tinh chỉnh thích ứng lĩnh vực sử dụng dữ liệu lĩnh vực để cải thiện thích ứng lĩnh vực. Thứ hai, trong khi các công trình trước đây về tinh chỉnh thích ứng lĩnh vực được xây dựng trên mô hình được tiền huấn luyện trên kho ngữ liệu chung, TLM học từ đầu mà không có tiền huấn luyện quy mô lớn để tiết kiệm đáng kể chi phí tính toán.

Đồng Huấn Luyện cho Học Bán Giám Sát và Học Tích Cực Dựa trên Mật Độ Dữ Liệu Thêm vào đó, chúng tôi quan sát hai kỹ thuật liên quan đến TLM. Chúng là Đồng Huấn Luyện (CT) (Qiao et al., 2018; Yang et al., 2021) và Học Tích Cực Dựa trên Mật Độ Dữ Liệu (DAL) (Zhu et al., 2010; Wang et al., 2017) tương ứng. Cả CT và TLM đều sử dụng dữ liệu không có nhãn để hỗ trợ học tập trên một tác vụ nhất định. Sự khác biệt giữa TLM và CT là 2 mặt: Thứ nhất, CT yêu cầu huấn luyện các mô hình riêng biệt từ nhiều góc nhìn của dữ liệu không có nhãn, trong khi TLM chỉ huấn luyện một mô hình duy nhất thông qua các tác vụ tiền văn bản như MLM. Thứ hai, TLM tính đến quy trình lựa chọn dữ liệu không có nhãn, điều này ít được thảo luận trong CT. TLM và DAL có cùng hương vị tìm kiếm các thể hiện đại diện trong một tập dữ liệu không có nhãn. Tuy nhiên, DAL đưa ra giả định rằng mọi mẫu không có nhãn đều có thể được gán nhãn hiệu quả bằng định nghĩa của tác vụ, điều này không được yêu cầu bởi TLM. Ngoài ra, DAL cố gắng tìm các thể hiện quan trọng một cách lặp đi lặp lại từ toàn bộ tập dữ liệu không có nhãn, trong khi TLM chỉ cố gắng tìm các thể hiện liên quan theo cách một lần duy nhất đối với dữ liệu có nhãn, làm cho TLM hiệu quả hơn các thuật toán DAL cổ điển.

3. Phương pháp

3.1. TLM: Mô hình Hóa Ngôn Ngữ Hướng Tác Vụ

Đó là một hiện tượng thú vị rằng con người có thể nhanh chóng làm chủ một tác vụ nhất định với thời gian và nỗ lực hạn chế bằng cách chỉ tập trung vào những mảnh kiến thức liên quan. Ví dụ, khi học sinh ôn thi cấp tốc, họ xem lại một vài chương thay vì đọc qua tất cả sách trên thế giới. Theo quan sát này, chúng tôi phỏng đoán rằng một trong những khía cạnh chính của việc học một tác vụ là nhanh chóng và chính xác xác định thông tin liên quan đến tác vụ. Vì mục đích này, chúng tôi phát triển TLM trước tiên tự động truy xuất dữ liệu huấn luyện liên quan từ một kho ngữ liệu chung và sau đó học trên dữ liệu được truy xuất và dữ liệu tác vụ kết hợp.

Chính thức, với kho ngữ liệu chung D={di}i trong đó di là một tài liệu, và dữ liệu tác vụ có nhãn T={(xi,yi)}i trong đó xi là văn bản và yi∈Y là nhãn5, mục tiêu của chúng tôi là huấn luyện một mô hình f để ước lượng xác suất có điều kiện cho phân loại f(x) = p̂(y|x).

TLM gồm hai bước như được hiển thị trong Hình 2.

1. Truy xuất dữ liệu từ kho ngữ liệu chung sử dụng dữ liệu tác vụ làm truy vấn.
2. Huấn luyện mô hình từ đầu bằng cách tối ưu hóa đồng thời mục tiêu tác vụ và mục tiêu mô hình hóa ngôn ngữ trên dữ liệu được truy xuất và dữ liệu tác vụ.

Truy Xuất Từ Kho Ngữ Liệu Chung Với mỗi ví dụ trong dữ liệu tác vụ xi∈T, chúng tôi truy xuất một tập tài liệu

5Mặc dù việc mở rộng framework của chúng tôi cho các tác vụ sinh ra là đơn giản, chúng tôi tập trung vào các tác vụ phân loại trong công trình này.

--- TRANG 4 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

Si={d̃i,1,d̃i,2,...} từ kho ngữ liệu chung D đã cho. Tập Si đại diện cho top-K tài liệu tương tự nhất với xi trong D. Dữ liệu được truy xuất cho tất cả ví dụ xi được kết hợp S=⋃iSi. Dữ liệu được truy xuất S là một tập con nhỏ của kho ngữ liệu chung D.

Chúng tôi sử dụng BM25 (Robertson & Zaragoza, 2009) để truy xuất do tính hiệu quả của nó. Trong khi sử dụng các retriever dày đặc dựa trên embedding (Karpukhin et al., 2020) có thể dẫn đến kết quả truy xuất tốt hơn, chúng tôi không xem xét các phương pháp này để giữ cho phương pháp của chúng tôi càng đơn giản càng tốt. Hơn nữa, các retriever dày đặc dựa vào tiền huấn luyện, có thể mang lại chi phí tính toán bổ sung. Việc khám phá đạt được sự cân bằng tốt hơn giữa hiệu quả và hiệu suất truy xuất được để lại cho nghiên cứu tương lai. Hơn nữa, đối với các tác vụ có văn bản cực dài (ví dụ: Helpfulness (McAuley et al., 2015)), chúng tôi thấy hiệu quả hơn khi trích xuất từ khóa (ví dụ: sử dụng thuật toán RAKE (Rose et al., 2010)) để tạo truy vấn cho truy xuất thay vì sử dụng toàn bộ chuỗi đầu vào. Chúng tôi gọi dữ liệu được truy xuất S là dữ liệu ngoại vi và dữ liệu tác vụ T là dữ liệu nội bộ.

Lưu ý rằng phương pháp truy xuất dữ liệu của chúng tôi là bất khả tri về tác vụ—nó chỉ phụ thuộc vào văn bản x mà không phụ thuộc vào y. Hơn nữa, quy trình truy xuất không giả định tính khả dụng của dữ liệu cụ thể theo lĩnh vực. Nó hoạt động trên kho ngữ liệu chung và có cùng đầu vào như mô hình tiền huấn luyện-tinh chỉnh.

Huấn Luyện Kết Hợp Với cả dữ liệu nội bộ và ngoại vi, chúng tôi huấn luyện mô hình ngôn ngữ f từ đầu. Để Lmlm(x) là mất mát masked language modeling như trong BERT (Devlin et al., 2019), và để Ltask(f(x),y) là hàm mất mát tác vụ (ví dụ: cross entropy cho phân loại). TLM tối ưu hóa hàm mất mát sau:

λ1Ex∈S[Lmlm(x)] + Ex,y∈T[λ2Lmlm(x) + Ltask(f(x),y)]

trong đó λ1 và λ2 là các siêu tham số. Kiến trúc mạng mà chúng tôi sử dụng giống hệt BERT, nơi chúng tôi sử dụng đầu CLS cho phân loại và đầu LM cho masked language modeling. TLM cũng có thể được mở rộng cho các kiến trúc khác cho các tác vụ không phải phân loại. Triển khai của chúng tôi bao gồm quy trình huấn luyện hai giai đoạn. Trong giai đoạn đầu, chúng tôi xen kẽ một batch dữ liệu nội bộ với λ1 batch dữ liệu ngoại vi cho mini-batch stochastic gradient descent, trong đó λ1 được đặt là một số nguyên. Trong giai đoạn thứ hai, chúng tôi đặt cả λ1 và λ2 bằng không để chỉ tinh chỉnh mô hình trên dữ liệu nội bộ với mục tiêu tác vụ.

3.2. So Sánh Giữa TLM và PLM

Cả TLM và tiền huấn luyện-tinh chỉnh đều có hai giai đoạn. Thực tế, giai đoạn thứ hai của TLM bằng với giai đoạn tinh chỉnh truyền thống. Sự khác biệt chính giữa giai đoạn đầu của TLM và tiền huấn luyện (PLM) được thể hiện trong Bảng 1. Không giống như PLM học càng nhiều kiến thức bất khả tri về tác vụ càng tốt với chi phí cực kỳ cao, TLM học kiến thức liên quan đến tác vụ cho mỗi tác vụ với chi phí rất thấp.

Bảng 1. So sánh giữa TLM và PLM. Ở đây chúng tôi cung cấp so sánh định tính, trong khi so sánh định lượng về kích thước dữ liệu huấn luyện, FLOPs và số lượng tham số có trong Bảng 2.

TLM | PLM
Hàm Mất Mát | Ltask và Lmlm | Lmlm
Dữ Liệu Huấn Luyện | Một tập con nhỏ của D và dữ liệu tác vụ T | Toàn bộ D
Chi Phí Tính Toán | 8 GPU 42 giờ | 1.000 GPU một ngày
Tính Tổng Quát | Hướng Tác Vụ | Bất Khả Tri Tác Vụ

Với sự khác biệt trên giữa TLM và PLM, chúng tôi sẽ thảo luận chi tiết về ưu và nhược điểm của TLM.

Dân Chủ Hóa NLP Trong mô hình tiền huấn luyện-tinh chỉnh, hiệu suất tinh chỉnh phần lớn bị giới hạn bởi mô hình được tiền huấn luyện. Tuy nhiên, do các ràng buộc về tài nguyên tính toán, phần lớn nhà nghiên cứu NLP không thể đủ khả năng huấn luyện mô hình ngôn ngữ quy mô lớn và phải dùng đến việc nghiên cứu các thuật toán tinh chỉnh. Vì chỉ một phần nhỏ nhà nghiên cứu đang làm việc trên các kiến trúc, hàm mất mát và các lựa chọn thiết kế khác của PLM, có nguy cơ sự phát triển của lĩnh vực có thể đang chậm lại. Mặt khác, TLM hiệu quả và có hiệu suất cao. Kết quả là, TLM có tiềm năng dân chủ hóa NLP và thúc đẩy sự phát triển của nó bằng cách cho phép hầu hết các nhà nghiên cứu tự do khám phá các kiến trúc, hàm mất mát, thuật toán và các lựa chọn thiết kế khác trong lân cận của giải pháp tiên tiến.

Hiệu Quả TLM cải thiện so với PLM về FLOPs cho từng tác vụ. Trong nhiều trường hợp khi chỉ có một vài tác vụ mục tiêu, TLM được ưa chuộng. Ví dụ, một nhà nghiên cứu có thể quan tâm đến việc giải quyết bốn tập dữ liệu suy luận văn bản, hoặc một nhóm công nghiệp có thể muốn cải thiện hệ thống gợi ý có thể được xem như một tác vụ. Tuy nhiên, nếu mục tiêu là giải quyết 1.000 tác vụ cùng lúc (ví dụ: xây dựng nền tảng NLP để phục vụ nhiều đơn vị kinh doanh trong một công ty), PLM vẫn có thể được ưa chuộng.

Tính Linh Hoạt Vì TLM hướng tác vụ, có mức độ linh hoạt lớn hơn. Các nhà nghiên cứu có thể sử dụng các chiến lược tùy chỉnh cho tokenization, độ dài chuỗi, biểu diễn dữ liệu, điều chỉnh siêu tham số, v.v., có thể cải thiện hiệu suất và/hoặc hiệu quả.

Tính Tổng Quát PLM học biểu diễn chung bất khả tri về tác vụ và có thể được sử dụng cho học few-shot và zero-shot (Brown et al., 2020). Ngược lại, TLM đánh đổi tính tổng quát để có hiệu quả bằng cách chỉ học biểu diễn cụ thể cho tác vụ. Cách cải thiện thêm TLM về việc học biểu diễn tổng quát hơn đặt ra thách thức cho nghiên cứu tương lai. Chúng tôi tin rằng học đa tác vụ có thể giảm bớt vấn đề này với các quan sát gần đây (Wei et al., 2021; Zhong et al., 2021), đặc biệt là cho khái quát hóa zero-shot trong lĩnh vực. Cũng có thể kết hợp tiền huấn luyện với TLM, ví dụ: sử dụng PLM nhỏ với TLM để khớp với PLM lớn hơn, để đạt được sự cân bằng tốt hơn giữa tính tổng quát và hiệu quả.

4. Thử nghiệm

4.1. Thiết lập

Tập Dữ Liệu Theo (Gururangan et al., 2020), chúng tôi tiến hành thử nghiệm trên tám tác vụ qua bốn lĩnh vực, bao gồm khoa học y sinh, khoa học máy tính, tin tức và đánh giá (hai tác vụ trong mỗi lĩnh vực). Các tác vụ có thể được phân loại thành tác vụ tài nguyên cao và tài nguyên thấp. Tác vụ tài nguyên cao có hơn 5K dữ liệu tác vụ, bao gồm AGNews (Zhang et al., 2015), IMDB (Maas et al., 2011), RCT (Dernoncourt & Lee, 2017), và Helpfulness (McAuley et al., 2015), trong khi tác vụ tài nguyên thấp bao gồm ChemProt (Kringelum et al., 2016), ACL-ARC (Jurgens et al., 2018), SciERC (Luan et al., 2018), và HyperPartisan (Kiesel et al., 2019). Đối với kho ngữ liệu huấn luyện chung, chúng tôi thu thập hai kho ngữ liệu tương ứng khớp với kho ngữ liệu huấn luyện gốc của BERT và RoBERTa. Chúng tôi đặt tên chúng tương ứng là Corpus-BERT (CBERT) và Corpus-RoBERTa (CRoBERTa). Kích thước của CRoBERTa lớn gấp 10 lần CBERT.

Baseline Thử nghiệm của chúng tôi tập trung vào so sánh với PLM chung. Chúng tôi tinh chỉnh cả BERT (Devlin et al., 2019) và RoBERTa (Liu et al., 2019) ở quy mô cơ sở và lớn làm baseline. Mặc dù TLM là phương pháp tổng quát không sử dụng dữ liệu trong lĩnh vực bổ sung, nó thậm chí còn hoạt động gần với các phương pháp tinh chỉnh thích ứng lĩnh vực (Gururangan et al., 2020) (xem Phụ lục A để so sánh chi tiết).

Chiến Lược Đánh Giá Chúng tôi báo cáo hiệu suất trung bình qua ba seed ngẫu nhiên, cùng với độ lệch chuẩn. Chúng tôi theo Beltagy et al. (2019) và Gururangan et al. (2020) để báo cáo micro-F1 test cho ChemProt và RCT, và macro-F1 cho các tập dữ liệu còn lại.

Để so sánh công bằng, chúng tôi đánh giá TLM ở các quy mô huấn luyện khác nhau. Quy mô huấn luyện được xác định bởi ba yếu tố, bao gồm số lượng tham số, kích thước kho ngữ liệu chung và số lượng token huấn luyện tổng. Số lượng token huấn luyện tổng được tính bằng tích của bước huấn luyện, kích thước batch và độ dài chuỗi. Chúng tôi báo cáo TLM ở ba quy mô huấn luyện như được hiển thị trong Bảng B.1, cụ thể là quy mô nhỏ, trung bình và lớn. Mỗi quy mô của TLM được so sánh tương ứng với các baseline PLM với chi phí tính toán tăng dần.

Chi Tiết Huấn Luyện Đối với mỗi thử nghiệm của TLM, trong khi cố định các siêu tham số quy mô huấn luyện (tức là bước huấn luyện, kích thước batch và độ dài chuỗi), chúng tôi thực hiện tìm kiếm lưới trên λ1 và λ2. Chúng tôi liệt kê các siêu tham số được sử dụng trong Bảng B.1 trong Phụ lục.

4.2. Kết Quả Chính

Bảng 2 hiển thị kết quả chính so sánh TLM ở ba quy mô khác nhau và các baseline PLM tương ứng. Kết luận là TLM có thể đạt được kết quả tốt hơn hoặc tương đương với baseline với việc giảm đáng kể FLOPs và kích thước dữ liệu huấn luyện. Cụ thể, ở quy mô nhỏ, TLM đạt được kết quả tương đương với BERT-Large với trung bình 1/33 FLOPs và 1/16 kho ngữ liệu huấn luyện. Ở quy mô trung bình và lớn, TLM cải thiện hiệu suất lần lượt 0.59 và 0.24 điểm trung bình, trong khi giảm đáng kể cả FLOPs và kích thước dữ liệu huấn luyện hai bậc độ lớn trở lên. Những kết quả này xác nhận rằng TLM có độ chính xác cao và hiệu quả hơn nhiều so với PLM. Hơn nữa, TLM có được nhiều lợi thế hơn về hiệu quả ở quy mô lớn hơn. Điều này cho thấy rằng PLM quy mô lớn hơn có thể đã được huấn luyện để lưu trữ nhiều kiến thức chung hơn không hữu ích cho một tác vụ cụ thể.

4.3. Nghiên Cứu Loại Bỏ

4.3.1. TRUY XUẤT DỮ LIỆU

Bảng 3 hiển thị so sánh giữa các phương pháp truy xuất khác nhau (tức là BM25 và truy xuất ngẫu nhiên) và các kích thước khác nhau của kho ngữ liệu chung. Chúng tôi thấy rằng với cùng một kho ngữ liệu chung, kết quả của BM25 vượt trội đáng kể so với truy xuất ngẫu nhiên với biên độ lớn trên tất cả các tác vụ, cho thấy việc sử dụng dữ liệu liên quan đến tác vụ cho huấn luyện kết hợp là quan trọng để đạt hiệu suất tốt nhất. Cụ thể, BM25 cho thấy lợi thế gần 1 điểm so với truy xuất ngẫu nhiên trên các tác vụ tài nguyên cao như IMDB, và lợi thế đáng kể hơn trên các tác vụ tài nguyên thấp như SciERC và ChemProt khoảng 3-4 điểm. Điều này phù hợp với trực giác của chúng tôi rằng các tác vụ tài nguyên thấp dựa nhiều hơn vào dữ liệu ngoại vi.

Bằng cách so sánh kết quả của CBERT và CRoBERTa với BM25, chúng tôi quan sát thấy việc tăng kích thước kho ngữ liệu chung cải thiện hiệu suất (lần lượt 0.5, 1.34 và 1.35 điểm trên IMDB, SciREC và ChemProt). Các cải tiến khi sử dụng dữ liệu nhiều gấp 10 lần tương tự với những cải tiến được quan sát trong PLM (Yang et al., 2019; Liu et al., 2019). Điều này cho thấy rằng mặc dù TLM chỉ sử dụng một lượng nhỏ dữ liệu, nó có thể mở rộng khi có sẵn kho ngữ liệu chung lớn hơn trong khi duy trì hiệu quả. Mặt khác, các cải tiến khi sử dụng kho ngữ liệu lớn hơn giảm dần với truy xuất ngẫu nhiên, cho thấy rằng truy xuất ngẫu nhiên, như một phương pháp bất khả tri về tác vụ, không rất nhạy cảm với kích thước kho ngữ liệu chung.

Truy xuất dữ liệu chọn top-K tài liệu tương tự nhất từ

--- TRANG 5 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Bảng 2. Kết quả đánh giá cho TLM ở ba quy mô huấn luyện khác nhau. Đối với mỗi tác vụ, chúng tôi báo cáo điểm F1 trung bình qua ba seed ngẫu nhiên với độ lệch chuẩn làm chỉ số dưới. Chúng tôi cũng liệt kê số lượng tham số, tổng chi phí tính toán huấn luyện (FLOPs), và kích thước kho ngữ liệu huấn luyện để so sánh.]

Mô hình | #Param | FLOPs¹ | Dữ liệu² | AGNews | Hyp. | Help. | IMDB | ACL. | SciERC | Chem. | RCT | Avg.
BERT-Base³ | 109M | 2.79E19 | 16GB | 93.50₀.15 | 91.93₁.74 | 69.11₀.17 | 93.77₀.22 | 69.45₂.90 | 80.98₁.07 | 81.94₀.38 | 87.00₀.06 | 83.46
BERT-Large³ | 355M | 9.07E19 | 16GB | 93.51₀.40 | 91.62₀.69 | 69.39₁.14 | 94.76₀.09 | 69.13₂.93 | 81.37₁.35 | 83.64₀.41 | 87.13₀.09 | 83.82
TLM (quy mô nhỏ) | 109M | 2.74E18 | 0.91GB | 93.74₀.20 | 93.53₁.61 | 70.54₀.39 | 93.08₀.17 | 69.84₃.69 | 80.51₁.53 | 81.99₀.42 | 86.99₀.03 | 83.78

RoBERTa-Base³ | 125M | 1.54E21 | 160GB | 94.02₀.15 | 93.53₁.61 | 70.45₀.24 | 95.43₀.16 | 68.34₇.27 | 81.35₀.63 | 82.60₀.53 | 87.23₀.09 | 84.12
TLM (quy mô trung bình) | 109M | 8.30E18 | 1.21GB | 93.96₀.18 | 94.05₀.96 | 70.90₀.73 | 93.97₀.10 | 72.37₂.11 | 81.88₁.92 | 83.24₀.36 | 87.28₀.10 | 84.71

RoBERTa-Large³ | 355M | 4.36E21 | 160GB | 94.30₀.23 | 95.16₀.00 | 70.73₀.62 | 96.20₀.19 | 72.80₀.62 | 82.62₀.68 | 84.62₀.50 | 87.53₀.13 | 85.50
TLM (quy mô lớn) | 355M | 7.59E19 | 3.64GB | 94.34₀.12 | 95.16₀.00 | 72.49₀.33 | 95.77₀.24 | 72.19₁.72 | 83.29₀.95 | 85.12₀.85 | 87.50₀.12 | 85.74

¹Tổng chi phí tính toán huấn luyện (FLOPs) được tính bằng (6×Total Training Tokens×Parameter Size) như trong (Brown et al., 2020). Đối với TLM, FLOPs được báo cáo như kết quả trung bình qua tám tác vụ.
²Kích thước dữ liệu được chọn từ kho ngữ liệu chung thực sự được sử dụng trong huấn luyện. Đối với TLM, nó được báo cáo bằng cách lấy trung bình qua tám tác vụ.
³BERT-Base và BERT-Large được tiền huấn luyện bởi (Devlin et al., 2019) và RoBERTa-Base và RoBERTa-Large được tiền huấn luyện bởi (Liu et al., 2019). Chúng tôi tinh chỉnh chúng để có được kết quả trên tám tác vụ.

[Bảng 3. Kết quả trên tập phát triển sử dụng các phương pháp truy xuất khác nhau và các kho ngữ liệu chung khác nhau trên mỗi tác vụ. Chúng tôi so sánh hai phương pháp truy xuất dữ liệu: truy xuất ngẫu nhiên và thuật toán BM25. Chúng tôi so sánh hai kho ngữ liệu chung nguồn: kho ngữ liệu được sử dụng trong BERT (CBERT) và kho ngữ liệu được sử dụng trong RoBERTa (CRoBERTa). Kích thước của CRoBERTa lớn gấp 10 lần CBERT.]

| | IMDB | SciERC | ChemProt
Ngẫu nhiên với CBERT | 93.65₀.09 | 83.80₀.62 | 80.65₀.48
Ngẫu nhiên với CRoBERTa | 94.04₀.22 | 83.10₁.54 | 80.73₀.46
BM25 với CBERT | 94.40₀.09 | 86.07₀.48 | 83.64₀.26
BM25 với CRoBERTa | 94.90₀.06 | 87.41₀.36 | 84.99₀.72

kho ngữ liệu chung. Bảng 4 hiển thị kết quả của các giá trị K khác nhau. Chúng tôi quan sát thấy rằng các tác vụ tài nguyên cao như AGNews chỉ cần giá trị K nhỏ, trong khi các tác vụ tài nguyên thấp như SciREC và ChemProt yêu cầu K lớn để đạt được hiệu suất tốt nhất. Quan sát này phù hợp với phân tích trên rằng các tác vụ tài nguyên thấp dựa nhiều hơn vào dữ liệu ngoại vi để cải thiện từ huấn luyện kết hợp.

4.3.2. TRỌNG SỐ MÔ HÌNH HÓA NGÔN NGỮ λ1 VÀ λ2

Các siêu tham số λ1 và λ2 là trọng số cho mất mát LM trên dữ liệu ngoại vi và nội bộ tương ứng. Chúng tôi tiến hành phân tích độ nhạy cảm với λ1 và λ2. Kết quả được hiển thị trong Bảng 5 và Bảng 6.

Đối với λ1, chúng tôi thấy rằng các tác vụ tài nguyên cao như Helpfulness

[Bảng 4. Kết quả trên tập phát triển với các giá trị K khác nhau. Giá trị K là số lượng tài liệu được truy xuất cho mỗi ví dụ tác vụ. AGNews là tác vụ tài nguyên cao, trong khi SciREC và ChemProt là tác vụ tài nguyên thấp. Ở đây chúng tôi sử dụng λ2 = 20 cho tất cả các tác vụ. Khi có dữ liệu ngoại vi, chúng tôi sử dụng λ1 = 4 cho AGNews và λ1 = 1000 cho SciERC và ChemProt.]

| | AGNews | SciERC | ChemProt
Chỉ Dữ Liệu Tác Vụ | 93.41₀.10 | 51.23₁.13 | 55.05₀.18
Top-50 | 94.51₀.15 | 77.61₁.75 | 77.21₀.47
Top-500 | 94.32₀.05 | 82.39₀.55 | 81.44₀.50
Top-5000 | 94.42₀.10 | 86.07₀.48 | 83.64₀.26

hoạt động tốt hơn với λ1 nhỏ hơn (tức là Helpfulness đạt tốt nhất khi λ1 = 1) trong khi các tác vụ tài nguyên thấp như SciERC và ChemProt đạt tốt nhất khi λ1 lớn (tức là cả hai tác vụ sử dụng λ1 = 999). Điều này phù hợp với kết luận trong Phần 4.3.1 rằng các tác vụ tài nguyên thấp dựa nhiều hơn vào dữ liệu ngoại vi. Thêm vào đó, việc loại bỏ dữ liệu tác vụ và chỉ sử dụng dữ liệu ngoại vi để huấn luyện (tức là λ1 = #CBERT), nó hoạt động tệ hơn so với khi kết hợp dữ liệu tác vụ, chứng minh tính không thể thiếu của dữ liệu tác vụ nhỏ.

Kết quả trong Bảng 6 cho thấy rằng mô hình hóa ngôn ngữ trên dữ liệu nội bộ là cần thiết: kết quả tốt hơn đều đạt được khi λ2 khác không. Dựa trên quan sát của chúng tôi, hiệu suất cạnh tranh có thể đạt được khi λ2 được đặt ở giá trị phù hợp từ 20 đến 1000.

--- TRANG 6 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Hình 3. Trực quan hóa attention của TLM và baseline tiền huấn luyện-tinh chỉnh, với "[CLS] crystallographic comparison with the structurally related. [SEP]" từ ChemProt làm đầu vào. Các đầu vị trí (Voita et al., 2019) được làm nổi bật trong hộp đỏ và các đầu dọc (Kovaleva et al., 2019) được che trong màu xám.]

[Bảng 5. Kết quả trên tập phát triển với các trọng số khác nhau trên dữ liệu ngoại vi (tức là λ1). Chúng tôi gán các giá trị khác nhau cho λ1 cho giai đoạn đầu, và báo cáo hiệu suất cuối cùng sau học tập kết hợp hai giai đoạn. "Chỉ ngoại vi" có nghĩa là chỉ sử dụng dữ liệu ngoại vi để huấn luyện (tức là λ1 = 1). Helpfulness là tác vụ tài nguyên cao, và những tác vụ khác là tài nguyên thấp. Đối với tất cả các tác vụ, chúng tôi cố định λ2 = 20.]

| | Helpfulness | SciERC | ChemProt
λ1=1 | 71.02₀.51 | 80.72₃.32 | 73.27₀.30
λ1=3 | 70.41₀.52 | 80.01₀.72 | 79.43₁.03
λ1=99 | 69.56₀.23 | 84.95₀.57 | 83.30₀.30
λ1=999 | 69.35₀.72 | 86.07₀.48 | 83.64₀.26
Chỉ ngoại vi | 69.76₀.50 | 85.66₁.58 | 82.50₀.27

[Bảng 6. Kết quả trên tập phát triển với các trọng số mô hình hóa ngôn ngữ khác nhau trên dữ liệu nội bộ (tức là λ2). Ở đây chúng tôi đặt λ1 = 1000 cho SciERC và ChemProt, và λ1 = 4 cho RCT]

| | RCT | SciERC | ChemProt
λ2=0 | 85.75₀.11 | 83.31₀.88 | 83.41₀.33
λ2=20 | 88.08₀.02 | 86.07₀.48 | 83.64₀.26
λ2=100 | 88.16₀.15 | 85.48₁.01 | 83.77₀.77
λ2=1000 | 88.02₀.04 | 85.29₁.86 | 83.63₀.90

4.3.3. GIAI ĐOẠN THỨ HAI CỦA HUẤN LUYỆN

TLM chứa hai giai đoạn huấn luyện—đầu tiên huấn luyện trên cả ba số hạng kết hợp và sau đó tinh chỉnh chỉ sử dụng mục tiêu tác vụ. Để xác nhận tính hiệu quả của giai đoạn thứ hai của TLM, chúng tôi so sánh hiệu suất của huấn luyện hai giai đoạn với việc chỉ sử dụng giai đoạn một. Kết quả được hiển thị trong Bảng 7. Chúng tôi thấy rằng việc loại bỏ giai đoạn thứ hai làm tổn hại hiệu suất cuối cùng một cách nhất quán, chứng minh tính không thể thiếu của nó. Đặc biệt, giai đoạn thứ hai có ảnh hưởng nhiều hơn đến các tác vụ tài nguyên thấp (với mức giảm lớn 19.37 điểm trên ACL-ARC và 14.34 điểm trên ChemProt) so với các tác vụ tài nguyên cao (với mức giảm hiệu suất 0.53 điểm trên AGNews và 2.17 điểm trên IMDB).

[Bảng 7. Kết quả trên tập phát triển của huấn luyện hai giai đoạn và huấn luyện một giai đoạn (loại bỏ giai đoạn 2).]

| | AGNews | IMDB | ChemProt | ACL-ARC
hai giai đoạn | 94.51 | 94.40 | 83.64 | 76.37
không có giai đoạn-2 | 93.98# | 92.23# | 69.30# | 57.00#

[Bảng 8. Kết quả thêm mất mát MLM trên dữ liệu tác vụ vào PLM. Kết quả dựa trên RoBERTa-base.]

Mô hình | AGNews | Hyp. | Help. | IMDB | ACL. | SciERC | Chem. | RCT | Avg.
PLM | 94.02 | 93.53 | 70.45 | 95.43 | 68.34 | 81.35 | 82.60 | 87.23 | 84.12
PLM+MLM | 93.83 | 93.50 | 71.12 | 95.54 | 70.94 | 80.90 | 82.53 | 87.09 | 84.43
TLM | 93.96 | 94.05 | 70.90 | 93.97 | 72.37 | 81.88 | 83.24 | 87.28 | 84.71

4.3.4. MẤT MÁT MLM TRÊN DỮ LIỆU TÁC VỤ

Trong giai đoạn huấn luyện đầu tiên, TLM sử dụng mất mát ngôn ngữ có mặt nạ trên dữ liệu tác vụ. Để kiểm tra liệu thủ thuật này có đạt được cải tiến chính hay không, chúng tôi so sánh kết quả trên PLM, PLM với mất mát MLM bổ sung trên dữ liệu tác vụ (PLM+MLM) và TLM. Kết quả trong Bảng 8 cho thấy rằng việc thêm mất mát MLM trên dữ liệu tác vụ vào PLM chỉ có cải tiến nhỏ và không ảnh hưởng đến kết luận chính của bài báo. Thêm vào đó, kết quả trong Bảng 3 và Bảng 4 cho thấy rằng việc truy xuất dữ liệu liên quan phù hợp cũng cần thiết cho hiệu suất của TLM.

4.4. Phân tích

4.4.1. TRỰC QUAN HÓA TRỌNG SỐ ATTENTION

Chúng tôi cũng nghiên cứu sự khác biệt giữa hành vi mô hình của TLM và tiền huấn luyện-tinh chỉnh bằng cách trực quan hóa trọng số attention của chúng. Voita et al. (2019) thấy rằng một loại đầu cụ thể, được gọi là "đầu vị trí" trong đó ít nhất 90% trọng số attention tối đa được gán cho các token lân cận, có đóng góp quan trọng cho dự đoán cuối cùng của mô hình. Một loại đầu khác mà chúng tôi quan tâm là những đầu trong đó hầu hết trọng số attention tối đa được gán cho [CLS], [SEP] hoặc token dấu chấm ("."), có thể mã hóa ít thông tin ngữ nghĩa hoặc cú pháp hơn (Kovaleva et al., 2019). Trong thử nghiệm của chúng tôi, nếu hơn 90% trọng số tối đa được gán cho [CLS], [SEP] hoặc token dấu chấm, chúng tôi phân loại đầu này là "đầu dọc".

Kết quả trong Hình 3 cho thấy rằng trên tác vụ ChemProt, nhiều đầu vị trí hơn và ít đầu dọc hơn được quan sát trong TLM so với PLM. Chúng tôi cũng quan sát các mẫu tương tự qua các tác vụ khác nhau (xem Phụ lục C). Những hiện tượng này gợi ý rằng TLM học các mẫu attention khác nhau (có thể nhiều thông tin hơn) so với PLM.

4.4.2. NGHIÊN CỨU TRƯỜNG HỢP CỦA DỮ LIỆU ĐƯỢC TRUY XUẤT

Chúng tôi đã hiển thị một số trường hợp dữ liệu được truy xuất trong Bảng 9. TLM truy xuất dữ liệu liên quan từ kho ngữ liệu chung sử dụng BM25 (Robertson & Zaragoza, 2009). Vì BM25 dựa trên đặc trưng thưa thớt, nó tập trung nhiều hơn vào sự tương tự từ vựng thay vì sự tương tự ngữ nghĩa. Điều này có thể đặc biệt có lợi cho các lĩnh vực chuyên nghiệp, ví dụ: SciERC cho khoa học máy tính và ChemProt cho khoa học y sinh), vì có một số lượng lớn danh từ riêng trong các lĩnh vực này. Đối với các lĩnh vực khác, có vẻ như BM25 cũng hoạt động hợp lý để truy xuất các tài liệu liên quan.

4.5. Kết Quả Trên Nhiều Tập Dữ Liệu Hơn

Cho đến nay chúng tôi đã theo thiết lập của Gururangan et al. (2020) và áp dụng các tập dữ liệu trong đó. Trong phần này, chúng tôi bổ sung thử nghiệm với benchmark GLUE (Wang et al., 2018) theo thiết lập của BERT (Devlin et al., 2019) để kiểm tra hiệu suất của TLM trên tập hợp tác vụ đa dạng hơn bao gồm hiểu ngôn ngữ tự nhiên. Chúng tôi theo thiết lập quy mô nhỏ trong Phần 4.2 về kích thước mô hình, dữ liệu và FLOPs. Kết quả trong Bảng 10 cho thấy rằng với các lợi thế về hiệu quả, hiệu suất trung bình của TLM tương đương với BERT qua 8 tác vụ, phù hợp với những phát hiện trước đây của chúng tôi và chứng minh tính hiệu quả của TLM.

5. Kết luận

Trong bài báo này, chúng tôi đã đề xuất một framework đơn giản, hiệu quả, không cần tiền huấn luyện, TLM. Ý tưởng cốt lõi là chỉ sử dụng một tập con nhỏ, liên quan đến tác vụ của kho ngữ liệu chung để huấn luyện mô hình ngôn ngữ. Thử nghiệm của chúng tôi cho thấy TLM đạt được kết quả tương tự hoặc thậm chí tốt hơn PLM, với việc giảm FLOPs huấn luyện hai bậc độ lớn. TLM mở ra khả năng giảm sự phụ thuộc nặng nề vào PLM quy mô lớn và huấn luyện mô hình từ đầu một cách hiệu quả, trong khi không làm tổn hại hiệu suất tổng thể. Chúng tôi hy vọng TLM sẽ góp phần dân chủ hóa NLP và

--- TRANG 7 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Bảng 9. Ví dụ về dữ liệu được truy xuất. Sự trùng lặp giữa truy vấn và dữ liệu được truy xuất được làm nổi bật bằng màu xanh in nghiêng.]

Tác vụ | Dữ liệu Tác vụ làm Truy vấn | Dữ liệu Chung Được Truy xuất
---|---|---
Hyp. | "Một hiệp hội sinh viên Đảng Cộng hòa tại Đại học Bang San Diego (SDSU) đang đối mặt với phản ứng dữ dội vì đã gửi thư yêu cầu sinh viên Hồi giáo lên án các cuộc tấn công khủng bố tuần trước tại Barcelona. ..." | Ví dụ 1: "...Các đội bóng nước liên trường SDSU Aztecs, bơi lội và lặn có trụ sở tại Aztec Aquaplex..." Ví dụ 2: Daily Aztec là tờ báo sinh viên độc lập, phi lợi nhuận phục vụ Đại học Bang San Diego (SDSU) và khu vực College Area xung quanh ở San Diego, California...
Help. | Chất lượng kém. Vỏ bị vỡ sau khi thả xuống sàn gạch. ... | Ví dụ 1: ...một thuật toán hợp tác sẽ có thể đề xuất nó, chất lượng của những đề xuất đó sẽ kém... Ví dụ 2: ...Những cuốn sách có chất lượng kém sẽ nhanh chóng ngừng bán...
ChemProt | FCEO đã ức chế đáng kể nitric oxide (NO) và prostaglandin E2 (PGE2) bằng cách ngăn chặn biểu hiện protein của inducible nitric oxide synthase (iNOS) và cyclooxygenase (COX)-2, tương ứng. | Ví dụ 1: ...Chúng điều chỉnh sự phát triển của tinh trùng bằng cách kiểm soát phân chia tế bào và khả năng sống sót. Các yếu tố miễn dịch khác được tìm thấy trong tinh hoàn bao gồm enzyme inducible nitric oxide synthase (iNOS)... Ví dụ 2: Những hợp chất này đã được chứng minh "in vivo" để giảm hai protein trung gian viêm, cyclooxygenase-2 (COX-2) và inducible nitric oxide synthase (iNOS)...
SciERC | Các kỹ thuật xử lý chuỗi hình ảnh được sử dụng để nghiên cứu các quá trình trao đổi, tăng trưởng và vận chuyển và để giải quyết các câu hỏi chính trong vật lý môi trường và sinh học. | Ví dụ 1: ...Các lực thúc đẩy trong xử lý tín hiệu cho song song dữ liệu là mã hóa video, xử lý hình ảnh và đồ họa, truyền thông không dây để kể tên một vài... Ví dụ 2: Chúng có ứng dụng trong nhiều lĩnh vực như sinh học, hóa học, sinh thái học, khoa học thần kinh, vật lý, xử lý hình ảnh,...

[Bảng 10. Kết quả đánh giá trên benchmark GLUE. Kích thước mô hình, dữ liệu và FLOPs tương tự với Bảng 2.]

Phương pháp | CoLA | RTE | STS-B | MRPC | QQP | SST-2 | QNLI | MNLI | Avg.
---|---|---|---|---|---|---|---|---|---
BERT-Base | 59.3 | 68.2 | 89.8/89.4 | 86.0/90.5 | 91.1/88.1 | 92.5 | 91.8 | 84.5/84.5 | 82.97
TLM (quy mô nhỏ) | 59.8 | 67.1 | 89.0/88.7 | 86.8/90.4 | 91.1/88.1 | 92.2 | 91.0 | 83.3/83.9 | 82.60

thúc đẩy sự phát triển của nó bằng cách cho phép hầu hết các nhà nghiên cứu tự do khám phá các kiến trúc, hàm mất mát, thuật toán và các lựa chọn thiết kế khác trong lân cận của giải pháp tiên tiến.

Như đã thảo luận trong Phần 3.2, có một số hướng tiềm năng cho nghiên cứu tương lai. Sẽ thú vị khi nghiên cứu cách sử dụng TLM để khớp với hiệu suất của PLM quy mô lớn hơn. Hơn nữa, việc mở rộng và cải thiện thêm TLM cho học few-shot và zero-shot là vấn đề quan trọng.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo được dịch giữ nguyên định dạng và nội dung gốc]

--- TRANG 8 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 9 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 10 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

A. So sánh với Thích ứng Lĩnh vực

Công trình của chúng tôi khác với thích ứng lĩnh vực như Gururangan et al. (2020). Trong khi thích ứng lĩnh vực nhằm giải quyết cách thích ứng hiệu quả LM được tiền huấn luyện vào một tác vụ cụ thể theo lĩnh vực với đủ dữ liệu lĩnh vực, công trình này nhắm vào việc cung cấp phương pháp đủ tổng quát để giải quyết bất kỳ tác vụ nào mà không cần dữ liệu lĩnh vực. Tuy nhiên, chúng tôi vẫn so sánh TLM với (Gururangan et al., 2020) như Bảng A.2 hiển thị. Chúng tôi hy vọng tìm hiểu rằng, dưới điều kiện khắc nghiệt nhưng thực tế rằng không có dữ liệu lĩnh vực nào có thể truy cập, liệu framework TLM được đề xuất của chúng tôi có thể vẫn khớp hoặc thậm chí vượt trội hơn các phương pháp thích ứng lĩnh vực truyền thống với mô hình ngôn ngữ được tiền huấn luyện lớn cũng như dữ liệu lĩnh vực.

Từ kết quả trong Bảng A.2, chúng tôi có những quan sát:

1. Chúng tôi tái tạo kết quả RoBERTa-Base sử dụng các siêu tham số được báo cáo bởi Gururangan et al. (2020) cũng như các siêu tham số riêng của chúng tôi. Kết quả cho thấy rằng kết quả baseline RoBERTa-Base bị đánh giá thấp trong bài báo với khoảng cách khoảng 3 điểm. Chúng tôi liệt kê các siêu tham số cho việc tinh chỉnh RoBERTa trong Bảng A.1.

2. Chúng tôi cũng tái tạo kết quả DAPT+TAPT sử dụng các siêu tham số riêng của chúng tôi. Kết quả cho thấy rằng DAPT+TAPT với siêu tham số mới cũng hoạt động tốt hơn một chút so với nó được báo cáo bởi Gururangan et al. (2020).

3. Từ góc độ tổng chi phí tính toán (FLOPs), DAPT+TAPT tiêu tốn FLOPs tương đương với TLM (quy mô lớn), và TLM (quy mô lớn) đạt kết quả tương đương với DAPT+TAPT (tức là 85.70 vs 85.57). Tuy nhiên, từ góc độ sử dụng dữ liệu, DAPT+TAPT sử dụng lượng lớn dữ liệu lĩnh vực, lượng dữ liệu cho mỗi lĩnh vực gần bằng lượng kho ngữ liệu huấn luyện tổng của BERT. TLM không dựa vào nó.

[Bảng A.1. So sánh giữa các siêu tham số để tinh chỉnh từ triển khai của chúng tôi và từ Gururangan et al. (2020).]

[Bảng A.2. Kết quả so sánh của TLM và Gururangan et al. (2020).]

--- TRANG 13 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Bảng B.1. Siêu tham số chi tiết cho TLM ở các quy mô khác nhau cho mỗi tác vụ.]

--- TRANG 14 ---
NLP Từ Đầu Mà Không Cần Tiền Huấn Luyện Quy Mô Lớn: Một Framework Đơn Giản và Hiệu Quả

[Hình C.1. tác vụ: RCT; đầu vào: "[CLS] twenty-eight individuals from outpatient physiotherapy departments were randomized. [SEP]"]

[Hình C.2. tác vụ: SciERC; đầu vào: "[CLS] multi-view constraints associated with groups of patches are combined. [SEP]"]

B. Thiết lập Thử nghiệm Chi tiết

Bảng B.1 liệt kê các siêu tham số chi tiết cho TLM ở giai đoạn 1 của các quy mô khác nhau cho mỗi tác vụ. Ở quy mô nhỏ và trung bình, đối với các tác vụ có ít hơn 5K ví dụ huấn luyện (HyperPartisan, ChemProt, SciERC, ACL-ARC), chúng tôi đặt K = 5000; đối với các tác vụ có hơn 100K ví dụ huấn luyện (RCT, AGNews, Helpfulness), chúng tôi đặt K = 50, đối với các tác vụ còn lại (IMDB), chúng tôi đặt K = 500. Ở quy mô lớn, K được nhân đôi cho mỗi tác vụ. Ở mỗi quy mô trên mỗi tác vụ, chúng tôi tiến hành tìm kiếm lưới cho λ1 ∈ {1,3,7,19,99,499,999,1999} và λ2 ∈ {20,100,1000}, và điều chỉnh bước huấn luyện, kích thước batch và độ dài chuỗi để tối thiểu hóa chi phí huấn luyện trong khi duy trì hiệu suất cạnh tranh. Chúng tôi quan sát rằng đối với hầu hết tất cả các tác vụ, quy mô huấn luyện càng lớn, càng dựa nhiều vào dữ liệu ngoại vi, được chỉ ra bởi xu hướng tăng của λ1 và λ2 khi tổng token huấn luyện tăng lên.

C. Trực quan hóa attention trên các tác vụ khác

Bên cạnh ChemProt (Hình 3), chúng tôi cũng thử nghiệm trên RCT (Hình C.1) và SciERC (Hình C.2) để có trực quan hóa attention. Chúng tôi thấy TLM một cách nhất quán chứa nhiều đầu vị trí hơn (trong hộp đỏ) và ít đầu dọc hơn (trong mặt nạ xám). Những kết quả này tiết lộ rằng mẫu được đề cập ở trên thường giữ cho TLM.
