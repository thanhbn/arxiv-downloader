# Huấn luyện few-shot LLM cho tóm tắt mã nguồn theo dự án cụ thể

Toufique Ahmed
Đại học California, Davis
Davis, California, USA
tfahmed@ucdavis.edu

Premkumar Devanbu
Đại học California, Davis
Davis, California, USA
ptdevanbu@ucdavis.edu

## TÓM TẮT

Các mô hình ngôn ngữ lớn (LLM) như GPT-3 và Codex đã đạt được hiệu suất tốt nhất trên nhiều tác vụ ngôn ngữ tự nhiên và cho thấy tiềm năng lớn cho mã nguồn. Một khía cạnh đặc biệt thú vị của LLM là khả năng học few-shot và zero-shot: chúng có thể học thực hiện một tác vụ với rất ít ví dụ. Few-shot có những ưu điểm đặc biệt trong kỹ thuật phần mềm, nơi có nhiều hiện tượng cụ thể theo dự án. Các nhà phát triển giới thiệu tên định danh rất địa phương, API, thuật ngữ, mẫu mã hóa, v.v. để phù hợp với nhu cầu của từng dự án. Những hiện tượng ngôn ngữ địa phương này phù hợp với các khái niệm miền, thuật ngữ thông tục, thuật toán và dữ liệu phù hợp với từng miền và dự án, và giúp các nhà phát triển khác đọc mã. Những hiện tượng này cũng có thể cung cấp các manh mối hữu ích cho các mô hình học máy. Tuy nhiên, dữ liệu cụ thể theo dự án có thể khá hạn chế, đặc biệt là trong giai đoạn đầu lịch sử của một dự án; do đó khả năng học few-shot của LLM mang lại một lựa chọn rất hấp dẫn. Trong bài báo này, chúng tôi điều tra việc sử dụng huấn luyện few-shot với mô hình GPT (Generative Pre-trained Transformer) Codex rất lớn, và tìm thấy bằng chứng cho thấy rằng có thể vượt qua đáng kể các mô hình tốt nhất hiện tại cho tóm tắt mã nguồn, tận dụng huấn luyện cụ thể theo dự án.

## TỪ KHÓA
học sâu, tóm tắt mã nguồn, mô hình ngôn ngữ lớn

## 1. GIỚI THIỆU

Các mô hình ngôn ngữ lớn (LLM) được xem là một tiến bộ cách mạng trong xử lý ngôn ngữ tự nhiên. Các mô hình như GPT-3 [4], có hơn 150 tỷ tham số, được huấn luyện bằng chế độ dự đoán token tiếp theo đơn giản, tự hồi quy trên corpus khổng lồ. Codex [5], ví dụ, là một mô hình tương tự 12 tỷ tham số được huấn luyện trên mã nguồn. Trong khi những mô hình như vậy chắc chắn thực hiện rất tốt thực sự ở tác vụ dự đoán (ví dụ, cho hoàn thành mã), chúng cũng khá tốt ở các tác vụ khác, như tạo mã từ docstring, và ngược lại, sau khi tinh chỉnh phù hợp [5].

Một trong những khía cạnh thú vị nhất của LLM là huấn luyện zero, one- hoặc few-shot. Trong hướng nghiên cứu này, LLM không chịu sự tinh chỉnh thông thường (như phổ biến nhất với BERT, T5, RoBERTA, v.v. [6,16,18]) sử dụng một số lượng đáng kể các ví dụ huấn luyện trên tác vụ (thường trong khoảng 100-100,000 ví dụ); thay vào đó nó được cung cấp một tiền tố, bao gồm chỉ một số ít cặp đầu vào-đầu ra, và sau đó được nhắc với một đầu vào truy vấn (không có đầu ra). Trong chế độ này (hiệu quả mẫu cao), LLM được biết là thực hiện một cách đáng ngạc nhiên tốt. Đáng chú ý nhất, huấn luyện few-shot không yêu cầu bất kỳ điều chỉnh trọng số nào. Thay vào đó, LLM tận dụng thông tin trong phần đầu của prompt để điều kiện hóa chính nó để thực hiện tác vụ được phản ánh trong một vài ví dụ. Điều này hoạt động vì khả năng khổng lồ (hàng tỷ tham số!) của mô hình cho phép nó điều kiện hóa hành vi tạo sinh của nó trên prompt đã cho theo những cách cực kỳ đa dạng, tinh tế & linh hoạt. Một ví dụ prompt huấn luyện two-shot, cho tác vụ dịch Anh-Đức, có thể là, ví dụ:

Câu "how are you?" bằng tiếng Đức
là "wie geht es?". Câu "See
you later!" bằng tiếng Đức là "Bis Bald!".
Câu "How much is that apple?"
bằng tiếng Đức là <submit>

Nếu được nhắc với điều này, khi nhấn nút submit, GPT3 phản hồi "Wie viel kostet diese Apfel?", đây là một bản dịch tốt¹. Tương tự, LLM được biết là có khả năng học few-shot trên một loạt các tác vụ, bao gồm hỏi đáp, suy luận ngôn ngữ tự nhiên, tóm tắt, v.v. Cần lưu ý rằng học few-shot thực sự rất thách thức, và năng khiếu của LLM để học thực hiện các tác vụ khác nhau trong chế độ này khá phi thường². Thú vị, học few-shot có một tầm quan trọng đặc biệt và thú vị đối với kỹ thuật phần mềm: để xử lý các hiện tượng ngôn ngữ cụ thể theo dự án.

Mỗi dự án phần mềm được thiết kế để đáp ứng nhu cầu trong một miền kinh doanh hoặc kỹ thuật cụ thể; trong mỗi miền, có những quy ước quy định các khái niệm mã hóa, thuật ngữ thông tục và thành ngữ cụ thể. Các ứng dụng khoa học, ứng dụng kinh doanh, ứng dụng miền chính phủ, tất cả đều đi kèm với thuật ngữ và khái niệm chuyên biệt. Những quy ước này (và từ vựng liên quan) hầu như luôn được áp dụng trực tiếp vào các ứng dụng phần mềm trong miền, và được sử dụng trong tất cả các hiện vật văn bản liên quan đến dự án: tài liệu, báo cáo lỗi, định danh, v.v. Ngoài ra, có những thuật toán và cấu trúc dữ liệu cụ thể cho các dự án và miền, và những điều này sẽ được phản ánh trong các mẫu mã hóa mà các nhà phát triển trong dự án đó sẽ nhận ra. Hầu hết các kỹ sư có kinh nghiệm trong một miền nhất định đều rất nhận thức về điều này: các dự án khác nhau tận dụng các khái niệm cụ thể theo miền khác nhau, và những điều này được phản ánh trong đặt tên định danh, gọi API và các mẫu mã hóa. Nhưng liệu chúng ta có thể khai thác điều này trong các ứng dụng học máy trong kỹ thuật phần mềm?

Đã được biết rõ ngay từ đầu rằng mô hình hóa ngôn ngữ cho mã phải xử lý các hiện tượng cụ thể theo dự án [11,12,23]. Điểm cản trở ở đây, tuy nhiên, là dữ liệu cụ thể theo dự án, đặc biệt là trong giai đoạn đầu lịch sử của một dự án, có thể khá hạn chế về khối lượng; các mô hình học sâu cũ hơn, yêu cầu O(10⁴) hoặc thậm chí O(10⁵) mẫu cụ thể cho một dự án hoặc miền để học các đặc trưng địa phương. Ngay cả các mô hình nền tảng kiểu BERT cũng yêu cầu nhiều ví dụ huấn luyện. Những ví dụ như vậy có thể khó tìm trên cơ sở cụ thể theo dự án, ngay cả trong giai đoạn đầu lịch sử của một dự án. Ngay cả khi có đủ ví dụ, việc huấn luyện lại một mô hình lớn cho mỗi dự án mới có thể cồng kềnh, mà cũng cần thiết (nhờ vào vấn đề "quên lãng thảm khốc" [8]).

Khả năng học few-shot của các mô hình ngôn ngữ rất lớn mang lại một giải pháp thay thế. Những mô hình này có thể làm với chỉ một số ít ví dụ huấn luyện; hơn nữa việc huấn luyện lại không thực sự cồng kềnh, người ta có thể chỉ cần thay đổi prompt. Ngoài ra, yêu cầu huấn luyện rất hạn chế cho thấy rằng chúng ta có thể (trong tương lai) địa phương hóa thậm chí chỉ một tệp, hoặc thậm chí chỉ một phương thức. Do đó chúng tôi tin rằng thiết lập few-shot có tiềm năng to lớn để hữu ích trong các thiết lập cụ thể theo dự án trong kỹ thuật phần mềm.

Trong bài báo này, chúng tôi chủ yếu tập trung vào tổng hợp bình luận. Ứng dụng này có lợi thế là vừa khá hữu ích, vừa được nghiên cứu kỹ. Đã có khá nhiều công trình điều tra các loại mô hình khác nhau: RNN, Transformer, Mô hình nền tảng, v.v., và có những điểm chuẩn tốt có sẵn. Do đó chúng tôi sử dụng vấn đề này như một thử nghiệm để điều tra các câu hỏi sau.

(1) Liệu khả năng học few-shot của các mô hình ngôn ngữ lớn có mở rộng đến tác vụ tóm tắt mã nguồn?
(2) Liệu khả năng học few-shot này có thể được mở rộng đến học cùng dự án trên cùng tác vụ này?
(3) Hiệu suất của LLM trong hai thiết lập trên so sánh như thế nào với các mô hình tốt nhất hiện tại?

## 2. BỐI CẢNH VÀ NGHIÊN CỨU LIÊN QUAN

Các nhà phát triển dành khoảng 59% thời gian của họ để hiểu hoặc tìm hiểu công việc của người khác hoặc công việc trước đây của chính họ [25]. Các bình luận chất lượng tốt có thể mang lại lợi ích cho các nhà phát triển bằng cách đóng góp vào cả quá trình phát triển và bảo trì [21]. Đáng ngạc nhiên, các bình luận không phù hợp và lỗi thời rất phổ biến trong các dự án SE. Ngoài việc viết bình luận mới, tóm tắt mã nguồn tự động có thể giúp cập nhật các bình luận không phù hợp và lỗi thời. Điều này đã thúc đẩy nghiên cứu về các công cụ tóm tắt mã nguồn tự động.

Tóm tắt mã nguồn có sự tương đồng mạnh mẽ với Dịch máy thần kinh (NMT) (ví dụ, dịch tiếng Anh sang tiếng Đức). Được truyền cảm hứng bởi NMT, các nhà nghiên cứu học máy trong miền SE đã áp dụng khung encoder-decoder thần kinh cho các tác vụ tóm tắt mã nguồn. Các công trình sớm nhất sử dụng mô hình RNN [22], và công trình mới nhất dựa trên mô hình nền tảng [3], tất cả đều tận dụng mô hình encoder-decoder. Tuy nhiên, với sự ra đời của LLM được tham số hóa rất cao (với > 150 tỷ tham số), gợi ý một con đường khác xa các mô hình encoder-decoder, hướng tới việc sử dụng các mô hình chỉ decoder (như Codex) cho một tác vụ như tóm tắt mã nguồn.

Các mô hình ngôn ngữ lớn (bao gồm Codex) đã được áp dụng cho tác vụ tóm tắt mã nguồn (đôi khi được gọi là "tạo Docstring"). Fried và cộng sự [9] giới thiệu một mô hình ngôn ngữ lớn, InCoder, và thử huấn luyện zero-shot trên tập dữ liệu CodeXGLUE Python. Họ đạt được kết quả ấn tượng; nhưng các mô hình được tinh chỉnh như CodeT5 [24], CodeBERT [7], và PLBART [1] vẫn có thể vượt trội hơn thiết lập zero-shot. Chen và cộng sự [5] tinh chỉnh Codex trên tác vụ tóm tắt mã nguồn và đề xuất một mô hình mới Codex-D. Tuy nhiên, họ đã sử dụng một tập dữ liệu đánh giá con người rất nhỏ cho Codex-D và không sử dụng BLEU-4, được khuyến nghị bởi điểm chuẩn CodeXGLUE. Công trình này không làm rõ hoàn toàn hiệu suất của Codex-D so với các mô hình được tiền huấn luyện khác. Không có công trình nào trong số trên báo cáo hiệu suất của huấn luyện few-shot hoặc điều tra tính hiệu quả của huấn luyện few-shot cùng dự án, như chúng tôi làm dưới đây.

## 3. PHƯƠNG PHÁP

Chúng tôi trình bày cách tiếp cận của chúng tôi để tóm tắt mã nguồn trong phần này. Chúng tôi cũng thảo luận về tập dữ liệu được sử dụng để đánh giá, và giải thích các lựa chọn thiết kế của chúng tôi. Hình 1 trình bày cách tiếp cận dựa trên few-shot đơn giản của chúng tôi để tạo ra tóm tắt mã nguồn sử dụng mô hình Codex. Có bốn bước chính như sau. Trong phần tiếp theo, chúng tôi giả định fi, si đề cập đến một cặp hàm (mã nguồn) thứ i được lập chỉ mục, tóm tắt (văn bản ngôn ngữ tự nhiên) thứ i

(1) Chúng tôi thêm vào đầu n hàm (cross-project/ same-project), mỗi hàm được theo sau bởi một bình luận, theo sau bởi hàm mục tiêu mà mô hình sẽ tạo bình luận. Do đó prompt được cấu trúc như f₁, s₁, f₂, s₂, ... fn, sn, fq trong đó các cặp fi, si cho i ≤ n tạo thành các ví dụ huấn luyện "few shot", và fq đề cập đến hàm "truy vấn" mà mô hình sẽ tạo tóm tắt sq. Mỗi bình luận có một ký hiệu bắt đầu và kết thúc (tức là, ⟨s⟩ & ⟨s⟩). Chúng tôi hoàn thiện đầu vào bằng cách thêm một ký hiệu bắt đầu bình luận (<s>) ở cuối hàm mục tiêu.

(2) Sau đó, chúng tôi gửi prompt đến mô hình Codex.

(3) Chúng tôi nhận đầu ra phản hồi từ mô hình. Đầu ra có thể chứa văn bản bổ sung sau bình luận vì chúng tôi phải cố định độ dài đầu ra trước khi xử lý đầu vào.

(4) Cuối cùng, chúng tôi chuẩn bị bình luận mục tiêu bằng cách sử dụng ký hiệu kết thúc bình luận (</s>).

**Tập dữ liệu** Chúng tôi sử dụng điểm chuẩn tóm tắt mã nguồn CodeXGLUE [17]. Cần lưu ý rằng tập dữ liệu này không liên quan đến mô hình Codex. CodeXGLUE ban đầu được chuyển đổi từ tập dữ liệu CodeSearchNet [13]. Nó là đa ngôn ngữ, với dữ liệu từ sáu ngôn ngữ khác nhau (tức là, Ruby, JavaScript, Java, Go, PHP, Python). Khá nhiều bài báo sử dụng các mô hình nền tảng [1,2,7,10,24] đã được đánh giá trên tập dữ liệu này cho tác vụ tóm tắt mã nguồn; vì vậy nó tạo thành một điểm chuẩn tốt. Tuy nhiên, chúng tôi không thể đánh giá toàn bộ tập dữ liệu vì chúng tôi chỉ có quyền truy cập hạn chế (20 yêu cầu/phút) vào phiên bản beta riêng của Mô hình Codex; tại trường đại học của chúng tôi, chúng tôi không có tài nguyên để sao chép một mô hình lớn như vậy. Tuy nhiên, chúng tôi có thể cố gắng thu thập bằng chứng liên quan đến câu hỏi nghiên cứu của chúng tôi; chúng tôi đã chọn ngẫu nhiên chỉ 1000 ví dụ từ tập thử nghiệm của tất cả sáu ngôn ngữ. Để so sánh đúng cách với các mô hình nền tảng khác, chúng tôi cũng tìm ra hiệu suất của những mô hình đó trên cùng bộ sưu tập mẫu. Chúng tôi đã chọn ngẫu nhiên mười mẫu từ tập huấn luyện cho huấn luyện few-shot với Codex. Lưu ý rằng CodeXGLUE là một tập dữ liệu được khử trùng lặp đúng cách và sử dụng phân chia cross-project cho tập huấn luyện, thử nghiệm và dev [20].

Chúng tôi cũng đánh giá mô hình Codex trên huấn luyện few-shot cùng dự án. Chúng tôi trước đây đã chỉ ra rằng hiệu suất của các mô hình học sâu phụ thuộc vào các định danh cho tác vụ tóm tắt mã nguồn [2]. Từ vựng của một dự án rất địa phương, và các hàm từ cùng dự án có khả năng chia sẻ cùng bộ định danh [11,23]. Chúng tôi đã chọn bốn dự án Python và bốn dự án Java từ tập thử nghiệm của CodeXGLUE. Để có sự so sánh công bằng với các mô hình nền tảng trước đây, chúng tôi phải hạn chế vào tập thử nghiệm của CodeXGLUE. Sau khi chọn các dự án, chúng tôi truy xuất ngày tạo cho mỗi mẫu bằng cách sử dụng "git blame –ignore rev". Chúng tôi sắp xếp các hàm theo ngày tạo và đảm bảo rằng chỉ dữ liệu lịch sử được sử dụng cho huấn luyện few-shot để ngăn chặn rò rỉ dữ liệu từ các mẫu tương lai.

**Lựa chọn số lượng mẫu few-shot** Chúng tôi sử dụng "code-davinci-002", mô hình lớn nhất trong chuỗi Codex; nó có thể chứa các prompt lên đến 4000 token. Quyền truy cập của chúng tôi vào phiên bản beta riêng của mô hình cho phép few-shotting (tinh chỉnh với điều chỉnh trọng số trên mô hình thần kinh thực tế chưa khả thi, và nằm ngoài phạm vi của bài báo này). Do đó, huấn luyện few-shot của chúng tôi bị giới hạn bởi 4000 token. Chúng tôi phát hiện rằng chúng tôi có thể an toàn chứa 10-15 chuỗi trong prompt và yêu cầu mô hình tạo bình luận cho chúng tôi. Chúng tôi đã thử 5, 10, và 15 mẫu cho huấn luyện few-shot cho 1000 mẫu thử nghiệm từ tập dữ liệu tóm tắt mã nguồn CodeXGLUE Java và đạt được 19.76, 21.88, và 21.46 BLEU-4, tương ứng. Chúng tôi sử dụng 10-shot cho phần còn lại của công trình này, vì nó cần ít thời gian hơn ngoài việc cho hiệu suất tốt nhất. Ngoài ra, lưu ý rằng việc sử dụng quá nhiều dữ liệu cho few-shot hoặc tinh chỉnh có thể gây ra quên lãng thảm khốc trong mô hình [14]. Chúng tôi cũng thảo luận về hiệu suất cho huấn luyện zero-shot và one-shot trong Phần 4.4.

**Lựa chọn thiết kế** Một số tham số cần được cố định để có đầu ra từ Codex. Temperature là một trong những tham số quan trọng. Temperature cao hơn cho phép mô hình chấp nhận rủi ro nhiều hơn. Theo khuyến nghị của tài liệu OpenAI, chúng tôi đặt temperature thành 0 vì chúng tôi nhắm đến các câu trả lời được định nghĩa rõ³. Chúng tôi cũng đặt giá trị mặc định 1.0 làm Top_p và 50 làm số lượng max_token. Phần lớn các tóm tắt có ít hơn 50 token. Tuy nhiên, mô hình vẫn tiếp tục tạo token ngay cả sau khi hoàn thành tóm tắt. Chúng tôi cắt tóm tắt bằng cách sử dụng ký hiệu kết thúc bình luận (</s>). Lưu ý rằng một số tham số khác có thể được thay đổi để tạo ra các tóm tắt sáng tạo hơn. Chúng tôi không thể khám phá hoàn toàn việc điều chỉnh siêu tham số do giới hạn truy cập API.

## 4. KẾT QUẢ

Chúng tôi trình bày dữ liệu hiệu suất của chúng tôi minh họa việc huấn luyện few-shot cross-project và same-project với mô hình LLM Codex. Kết quả của chúng tôi cho thấy rằng a) Hiệu suất của Codex khá ấn tượng, trong một số trường hợp vượt trội đáng kể so với các baseline; b) Codex (chỉ với một vài ví dụ từ cùng dự án) trong một số trường hợp có thể đi xa hơn nữa.

### 4.1 Few-shot cross-project

Như đã đề cập trước đó, CodeXGLUE là một tập dữ liệu cross-project. Để chỉ ra tính hiệu quả của huấn luyện few-shot, chúng tôi đã chọn ngẫu nhiên 10 mẫu từ tập huấn luyện CodeXGLUE cho mỗi ngôn ngữ. Chúng tôi thêm vào đầu 10 mẫu này vào một mẫu đã chọn (truy vấn), từ tập thử nghiệm, và yêu cầu mô hình hoàn thành prompt kết quả. Theo các công trình trước đây, chúng tôi sử dụng BLEU-4 mượt [15] làm thang đo đánh giá. Chúng tôi so sánh cách tiếp cận của chúng tôi với CodeBERT, GraphCodeBERT, CodeT5, và các phiên bản PolyGlot của mô hình CodeBERT và GraphCodeBERT. Bảng 1 cho thấy rằng Codex, được few-shotted cho tóm tắt mã nguồn, có thể vượt trội hơn các mô hình cạnh tranh. Chúng tôi quan sát thấy cải thiện BLEU-4 hơn +2 cho JavaScript và Go. Roy và cộng sự chỉ ra rằng cải thiện BLEU-4 hơn +2 điểm là proxy hợp lý cho sở thích có thể nhận thấy bằng con người [19]. Kết quả này cho thấy rằng LLM như Codex thực sự hiệu quả về mẫu. Tất cả các baseline được tinh chỉnh với 24K-251K cho mỗi ngôn ngữ, trong khi LLM vượt trội hơn tất cả chúng chỉ với 10 mẫu!

**Quan sát 1.** Với 10 mẫu, Codex vượt trội hơn tất cả các mô hình nền tảng được tinh chỉnh CodeT5, CodeBERT, GraphCodeBERT, Polyglot CodeBERT, và PolyGlotGraphCodeBERT trong tất cả sáu ngôn ngữ lập trình, mặc dù các mô hình được tinh chỉnh được huấn luyện với hàng nghìn dữ liệu.

### 4.2 Few-shot same-project

Giả thuyết của chúng tôi là few-shotting cùng dự án sẽ cho thấy lợi ích, vì các dự án có xu hướng theo một phong cách mã hóa và tài liệu đặc biệt. Dữ liệu của chúng tôi (phần trước) cho thấy rằng few-shot cross-project có thể vượt qua các mô hình được tiền huấn luyện trước đây với biên độ đáng kể chỉ với 10 mẫu. Chúng tôi sẽ thay thế 10 mẫu huấn luyện few-shot cross-project đó bằng 10 mẫu từ cùng dự án, (tôn trọng thứ tự chuỗi thời gian, để tránh rò rỉ giữa các ví dụ huấn luyện và thử nghiệm) và quan sát hiệu suất. Chúng tôi tin rằng ngay cả với một vài mẫu, mô hình Codex sẽ có thể tạo ra cải thiện đáng kể cho đầu ra. Bảng 2 cho thấy rằng chúng tôi vượt trội hơn tất cả các mô hình, ngay cả mô hình Codex với dữ liệu cross-project cho tất cả các dự án đang xem xét. Hiệu suất tăng từ 21.65 BLEU-4 lên 24.37 BLEU-4 (cải thiện 12.56%) cho các mô hình Codex, điều này thể hiện tính hiệu quả của huấn luyện few-shot.

**Quan sát 2.** Huấn luyện few-shot cùng dự án cải thiện hiệu suất của mô hình Codex cho tất cả 8 dự án.

### 4.3 Kiểm tra tính có ý nghĩa thống kê của các cải thiện

Chúng tôi thực hiện kiểm tra Wilcoxon-rank theo cặp một phía để xem tác động của huấn luyện few-shot trong một mô hình ngôn ngữ lớn. Chúng tôi so sánh mô hình CodeT5 với Codex trong thiết lập huấn luyện few-shot cross-project vì CodeT5 là mô hình hiệu suất tốt nhất trong số các mô hình được tiền huấn luyện. Chúng tôi so sánh đầu ra codex cross-project và same-project trong thiết lập same-project vì chúng tôi quan tâm đến việc huấn luyện few-shot có thể cải thiện hiệu suất của mô hình bao nhiều. Cho thiết lập cross-project, chúng tôi quan sát thấy cải thiện 1%-15% cho tất cả sáu ngôn ngữ lập trình (xem Bảng 1). Chúng tôi cũng tìm thấy cải thiện có ý nghĩa thống kê đáng kể cho bốn ngôn ngữ. Mặc dù chúng tôi không tìm thấy bất kỳ cải thiện có ý nghĩa nào cho Python và PHP, huấn luyện few-shot Codex vẫn vượt trội hơn các mô hình được tiền huấn luyện được tinh chỉnh truyền thống với 10 mẫu. Chúng tôi tìm thấy cải thiện có ý nghĩa thống kê cho 2 dự án (Bảng 2) so với Codex cross-project cho huấn luyện same-project mặc dù chúng tôi cải thiện cho tất cả 8 dự án (cải thiện 2% đến 46%). Tuy nhiên, cho cả hai thiết lập, chúng tôi quan sát thấy cải thiện có ý nghĩa thống kê tổng thể.

**Quan sát 3.** Mặc dù chúng tôi không quan sát thấy kết quả có ý nghĩa thống kê cho tất cả ngôn ngữ lập trình và tất cả dự án, chúng tôi quan sát thấy cải thiện có ý nghĩa thống kê tổng thể.

### 4.4 Huấn luyện zero-shot và one-shot

Các thuật ngữ như huấn luyện zero-shot và one-shot đang trở nên phổ biến với các mô hình ngôn ngữ lớn. Tuy nhiên, dữ liệu của chúng tôi cho thấy rằng zero-shot không hoạt động tốt cho các tác vụ như tóm tắt mã nguồn. Mô hình Codex hoạt động từ trái sang phải và chỉ dự đoán các token tương lai. Với huấn luyện zero-shot, mô hình kém khả năng hơn ở các tác vụ mà nó không được huấn luyện để làm. Ví dụ, thường thì docstring xuất hiện trước mã, và Codex được huấn luyện trên dữ liệu GitHub. Vì vậy, mô hình có thể có khả năng tạo mã khi được nhắc với docstring, ngay cả khi không nhìn thấy bất kỳ ví dụ nào. Đây không phải là trường hợp đối với tóm tắt mã nguồn, có thứ tự mặc định ngược lại. Ở đây, đầu vào cho mô hình là mã, và docstring là đầu ra. Chúng tôi cần một vài mẫu để dạy Codex tạo docstring sau mã. Tuy nhiên, chúng tôi đã thử cả huấn luyện zero-shot và one-shot với Codex và chỉ đạt được 2.96 và 6.22 BLEU-4 trung bình; chúng tôi bỏ qua chi tiết do hiệu suất tồi tệ một cách thuyết phục.

**Quan sát 4.** Huấn luyện zero-shot và one-shot trong Codex không hoạt động cho tác vụ tóm tắt mã nguồn.

## 5. NGUY CƠ

Tóm tắt mã nguồn sử dụng Codex gây ra ít nguy cơ an toàn & bảo mật trực tiếp hơn các vấn đề khác như tạo mã. Docstring hoặc bình luận không bao giờ được thực thi như một phần của chương trình; tuy nhiên, chúng có thể dẫn đến vấn đề nếu chúng làm sai lệch các lập trình viên.

Có nguy cơ rằng dữ liệu thử nghiệm của chúng tôi đã được CodeX nhìn thấy trong quá trình tiền huấn luyện quy mô rất lớn của nó; LLM được tiền huấn luyện trên các tập dữ liệu khổng lồ. Tập dữ liệu huấn luyện không có sẵn cho chúng tôi tại thời điểm đó, và vì vậy chúng tôi không thể tính đến nguy cơ này. Tuy nhiên, có một vài quan sát cung cấp bằng chứng gợi ý rằng mô hình không chỉ ghi nhớ trước đó dữ liệu thử nghiệm của chúng tôi: đầu tiên, hiệu suất của nó trong thiết lập zero- hoặc one-shot trong hầu hết các trường hợp khá tệ hại. Thứ hai, hiệu suất thực sự cải thiện một cách mượt mà, như mong đợi, trong hầu hết các trường hợp lên đến khoảng 10 mẫu huấn luyện được nhúng trong prompt. Điều này cho thấy rằng khả năng tạo sinh được điều kiện hóa của mô hình cải thiện với nhiều mẫu huấn luyện hơn; prior mà mô hình tính toán nội bộ và sử dụng để điều kiện hóa việc tạo bình luận của nó (p(comments|code)) đang cải thiện dần dần với nhiều mẫu huấn luyện hơn, cho thấy rằng nó thực sự đang khái quát hóa từ few-shot, thay vì chỉ nôn ra một ví dụ mà nó đã thấy trước đây.

## 6. KẾT LUẬN

Các mô hình ngôn ngữ lớn đang trở nên phổ biến và thậm chí còn lớn hơn mỗi vài tháng. Trong bài báo này, chúng tôi điều tra tính hiệu quả của huấn luyện few-shot cho tác vụ tóm tắt mã nguồn và thấy rằng nó có thể vượt trội đáng kể hơn một mô hình được tinh chỉnh được huấn luyện với hàng nghìn mẫu chỉ với mười mẫu. Hiệu quả mẫu này cũng mở ra cánh cửa cho việc sử dụng các mẫu cùng dự án, được biết là chia sẻ từ vựng và các tính chất nội bộ quan trọng khác của dự án. Chúng tôi quan sát thấy tác động của huấn luyện few-shot cùng dự án và thấy rằng một codex few-shot trong thiết lập cùng dự án hoạt động tốt hơn so với cross-project, và cải thiện tổng thể có ý nghĩa thống kê. Áp dụng dữ liệu cùng dự án rất hứa hẹn và khả thi vì mười mẫu cho một tác vụ như tóm tắt có thể được tạo ra trong vài giờ của quá trình phát triển. Chúng tôi tin rằng huấn luyện few-shot cùng dự án với các mô hình LLM có thể mang lại lợi ích cho các tác vụ SE khác cũng. Cuối cùng, tập dữ liệu tóm tắt mã nguồn được cung cấp ẩn danh tại https://doi.org/10.5281/zenodo.6592064.

Công trình này được hỗ trợ bởi NSF CISE MEDIUM 2107592, và NSIF CISE LARGE 1414172. Ahmed cũng được hỗ trợ bởi College of Engineering Dean's Distinguished Fellowship tại UC Davis.

## TÀI LIỆU THAM KHẢO

[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 2655–2668.

[2] Toufique Ahmed và Premkumar Devanbu. 2022. Multilingual training for software engineering. Trong Proceedings of the 44th International Conference on Software Engineering. 1443–1455.

[3] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the Opportunities and Risks of Foundation Models. arXiv preprint arXiv:2108.07258 (2021).

[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.

[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[7] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 1536–1547.

[8] Robert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3, 4 (1999), 128–135.

[9] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, và Mike Lewis. 2022. InCoder: A Generative Model for Code Infilling and Synthesis. arXiv preprint arXiv:2204.05999 (2022).

[10] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. Trong International Conference on Learning Representations.

[11] Vincent J Hellendoorn và Premkumar Devanbu. 2017. Are deep neural networks the best choice for modeling source code?. Trong Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. 763–773.

[12] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, và Premkumar Devanbu. 2012. On the naturalness of software. Trong 2012 34th International Conference on Software Engineering (ICSE).

[13] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

[14] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114, 13 (2017), 3521–3526.

[15] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Trong Text summarization branches out. 74–81.

[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).

[17] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, và Shujie Liu. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. CoRR abs/2102.04664 (2021).

[18] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019).

[19] Devjeet Roy, Sarah Fakhoury, và Venera Arnaoudova. 2021. Reassessing automatic evaluation metrics for code summarization tasks. Trong Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1105–1116.

[20] Ensheng Shia, Yanlin Wangb, Lun Dub, Junjie Chenc, Shi Hanb, Hongyu Zhangd, Dongmei Zhangb, và Hongbin Suna. 2022. On the Evaluation of Neural Code Summarization. ICSE.

[21] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, và K Vijay-Shanker. 2010. Towards automatically generating summary comments for java methods. Trong Proceedings of the IEEE/ACM international conference on Automated software engineering. 43–52.

[22] Ilya Sutskever, Oriol Vinyals, và Quoc V Le. 2014. Sequence to sequence learning with neural networks. Trong Advances in neural information processing systems. 3104–3112.

[23] Zhaopeng Tu, Zhendong Su, và Premkumar Devanbu. 2014. On the localness of software. Trong Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. 269–280.

[24] Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 8696–8708.

[25] Xin Xia, Lingfeng Bao, David Lo, Zhenchang Xing, Ahmed E Hassan, và Shanping Li. 2017. Measuring program comprehension: A large-scale field study with professionals. IEEE Transactions on Software Engineering 44, 10 (2017), 951–976.
