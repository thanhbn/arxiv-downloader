Chính thức, với một yêu cầu trong nhóm ứng viên, chúng tôi sử dụng chính các LLM để ước tính xem mỗi ứng viên khác trong nhóm có lợi để tạo ra ground truth của yêu cầu hay không, và gán nhãn mỗi ứng viên dựa trên mức độ có lợi của nó. Theo các nghiên cứu trước đây [19,31], bài báo này sử dụng tập huấn luyện làm nhóm ứng viên. Mục tiêu của quy trình này có thể được công thức hóa như sau:

D = {ei, Spi, Sni}Ni=1    (3)

trong đó ei = {xi, yi} là ví dụ thứ i trong tập huấn luyện. Spi và Sni là tập ví dụ tích cực và tiêu cực của ei, tương ứng. N là số lượng ví dụ trong tập huấn luyện R.

Mô hình hóa trên toàn bộ không gian của tập huấn luyện là bậc hai và do đó cấm đoán. Để giảm thiểu hạn chế này, chúng tôi giới thiệu một quy trình hai giai đoạn để ước tính các ví dụ trong tập huấn luyện. Trong giai đoạn đầu tiên, đối với mỗi ví dụ ei, chúng tôi sử dụng một bộ truy xuất có sẵn để chọn một tập hợp ví dụ Si = {xiq, yiq}tq=1 từ R trong đó t ≪ N. Trong bài báo này, chúng tôi áp dụng BM25 [42] để xây dựng Si, đo lường khớp n-gram văn bản giữa xi và xiq. Theo điểm số BM25, top-r ví dụ có điểm số BM25 cao hơn tạo thành tập hợp Si. Trong giai đoạn thứ hai, chúng tôi tận dụng chính các LLM để ước tính mỗi ví dụ trong Si. Cụ thể, chúng tôi đưa mỗi ví dụ {xiq, yiq} trong Si và xi vào LLM, và thu được xác suất dự đoán của chương trình ground-truth yi. Để định lượng phản hồi xác suất của LLM, chúng tôi thiết kế một thước đo M được định nghĩa như sau:

M = 1/|yi| × PLLM{yi|xiq, yiq, xi}    (4)

PLLM{yi|xiq, yiq, xi} = Σu=1|yi| log{p{ti,u|xiq, yiq, xi, ti,<u}}    (5)

trong đó yi = t1, ..., tb và tu là token thứ u trong yi. Thước đo có thể phản ánh mức độ hữu ích của ví dụ {xiq, yiq} đối với việc tạo ra chương trình mục tiêu yi, đại diện cho sở thích của LLM.

Sau đó chúng tôi xếp hạng tất cả các ví dụ trong tập Si theo điểm số thước đo M từ cao xuống thấp. Top-z ví dụ có điểm số cao hơn được đưa vào tập ví dụ tích cực Spi, trong khi đó, bottom-v ví dụ tạo thành tập ví dụ tiêu cực Sni vì các ví dụ tốt nên có lợi cho LLM để tạo ra chương trình đúng. Chúng tôi áp dụng quy trình hai giai đoạn này cho toàn bộ tập huấn luyện và cuối cùng thu được dữ liệu được gán nhãn D.

Trong dữ liệu được gán nhãn của chúng tôi, các ví dụ hữu ích hơn được gán nhãn là ví dụ tích cực và các ví dụ có điểm số thước đo thấp hơn được coi là ví dụ tiêu cực, điều này có thể phản ánh sở thích của LLM đối với các ví dụ ứng viên với một yêu cầu cụ thể.

3.2 Huấn luyện Bộ Truy xuất Neural
Như được mô tả trong Phần 3.1, dữ liệu được gán nhãn có thể phản ánh sở thích của LLM. Trong phần này, chúng tôi sử dụng dữ liệu được gán nhãn D để huấn luyện một bộ truy xuất neural, nhằm mục đích căn chỉnh với sở thích của LLM. Sau khi được huấn luyện, với một yêu cầu kiểm tra, bộ truy xuất có thể chọn một tập hợp các ví dụ ứng viên làm lời nhắc có lợi cho LLM để tạo ra chương trình đúng.

Để tối ưu hóa bộ truy xuất, chúng tôi áp dụng một mục tiêu học tương phản. Mục tiêu nhắm đến việc kéo một yêu cầu kiểm tra với các ví dụ huấn luyện hữu ích lại gần nhau và đẩy yêu cầu đã cho với các ví dụ hạn chế kiến thức ra xa. Chính thức, đối với một ví dụ ei trong dữ liệu được gán nhãn D, chúng tôi chọn ngẫu nhiên một mục epi từ Spi làm ví dụ tích cực tương ứng. Trong khi đó, chúng tôi chọn ngẫu nhiên một ví dụ huấn luyện eni từ tập Sni và chọn h ví dụ từ tập H (ví dụ, H = R \ Si) làm tập ví dụ tiêu cực Ni. Tiếp theo, chúng tôi áp dụng GraphCodeBERT [21] để mã hóa các yêu cầu của chúng và thu được các biểu diễn tương ứng. Sau đó, chúng tôi mô hình hóa các mối quan hệ của các yêu cầu này với mất mát tương phản. Theo SimCLR [15], mục tiêu học L được công thức hóa như sau:

L = -log(es{EiCLS, Ei,pCLS}/τ / Σ Ei,nCLS∈Ni es{EiCLS, Ei,nCLS}/τ)    (6)

trong đó τ là tham số nhiệt độ. ECLS có nghĩa là toàn bộ biểu diễn của một yêu cầu. s(·) đại diện cho sự tương đồng cosine của hai vector.

Dựa trên mục tiêu, bộ truy xuất của chúng tôi học sở thích của LLM, điều này sẽ hiệu quả để chọn các ví dụ hữu ích từ tập huấn luyện với một yêu cầu kiểm tra, dẫn đến việc thúc đẩy LLM tạo ra nhiều chương trình đúng hơn.

3.3 Suy luận
Trong quá trình suy luận, thay vì sử dụng các phương pháp heuristic, chúng tôi áp dụng LAIL để chọn các ví dụ hạn chế từ tập huấn luyện, có thể truy xuất các ví dụ với điểm số thước đo cao với một yêu cầu kiểm tra và do đó hữu ích cho LLM trong tạo sinh mã. Cụ thể, chúng tôi đầu tiên đưa các yêu cầu trong tập huấn luyện R vào bộ truy xuất được huấn luyện của chúng tôi tương ứng và thu được các vector biểu diễn {EiCLS}Ni=1 của chúng. Với một yêu cầu kiểm tra xt, chúng tôi thu được biểu diễn EtCLS của nó bằng bộ truy xuất. Tiếp theo, chúng tôi khớp EtCLS và EiCLS và do đó dẫn đến N cặp biểu diễn {{EtCLS, EiCLS}}Ni=1. Chúng tôi tính toán sự tương đồng cosine {ci}Ni=1 của tất cả các cặp và xếp hạng các ví dụ ứng viên theo điểm số tương đồng từ cao xuống thấp. Sau đó, top-r ví dụ được chọn cho ICL. Chúng tôi nối top-r ví dụ làm lời nhắc {e1, ..., ei, ..., er} trong đó điểm số tương đồng của chúng giảm dần (ví dụ, c1 < ci < cr). Chúng tôi đưa lời nhắc và yêu cầu kiểm tra vào LLM và làm cho LLM tạo ra chương trình với lấy mẫu nuclear [22] như được mô tả trong Công thức 2.

Lưu ý rằng LAIL chỉ cần mã hóa các ví dụ huấn luyện một lần. Với một yêu cầu kiểm tra, LAIL chỉ tính toán sự tương đồng cosine giữa nó và tất cả các ứng viên. Do đó, hiệu quả của phương pháp chúng tôi có thể chấp nhận được so với các phương pháp heuristic khác.

4 THIẾT KẾ NGHIÊN CỨU
Để điều tra hiệu quả của LAIL, chúng tôi thực hiện một nghiên cứu quy mô lớn để trả lời ba câu hỏi nghiên cứu. Trong phần này, chúng tôi mô tả chi tiết về nghiên cứu của mình, bao gồm bộ dữ liệu, thước đo đánh giá, baseline, LLM cơ sở và chi tiết thí nghiệm.

4.1 Câu hỏi Nghiên cứu
Nghiên cứu của chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau.

RQ1: LAIL hoạt động như thế nào so với các baseline tiên tiến nhất? RQ này nhằm xác minh rằng LAIL có thể tạo ra nhiều chương trình đúng hơn so với các baseline tiên tiến nhất (SOTA). Chúng tôi áp dụng ba LLM để đánh giá phương pháp của mình. Chúng tôi so sánh LAIL với 7 baseline trong ba bộ dữ liệu tạo sinh mã và sử dụng các bài kiểm tra đơn vị để kiểm tra tính đúng đắn của các chương trình được tạo ra.

RQ2: Các nhà phát triển có thích các chương trình được tạo ra bởi LAIL không? Mục tiêu cuối cùng của một mô hình tạo sinh mã là hỗ trợ các nhà phát triển viết chương trình. Trong RQ này, chúng tôi thuê 10 nhà phát triển để đánh giá thủ công các chương trình được tạo ra bởi LAIL và các baseline của chúng tôi. Chúng tôi đánh giá các chương trình này trong ba khía cạnh, bao gồm tính đúng đắn, chất lượng mã và khả năng bảo trì.

RQ3: Lựa chọn thiết kế nào tốt hơn để ước tính ví dụ? Trong phần 3.1, chúng tôi tận dụng xác suất dự đoán của các chương trình ground-truth để ước tính các ví dụ ứng viên. Trong RQ này, chúng tôi khám phá các lựa chọn thiết kế khác để đo lường các ví dụ bằng chính các LLM và so sánh chúng với thiết kế của chúng tôi.

4.2 Bộ dữ liệu
Chúng tôi tiến hành thí nghiệm trên ba bộ dữ liệu tạo sinh mã, bao gồm MBJP [8] bằng Java, MBPP [9] bằng Python và MBCPP [8] bằng C++. Thống kê của các bộ dữ liệu này được đưa ra trong Bảng 1.

MBPP [9] chứa 974 bài toán lập trình Python được xây dựng bằng cách huy động đông đảo. Mỗi ví dụ bao gồm một mô tả ngắn gọn, một hàm tự chứa duy nhất giải quyết vấn đề được chỉ định và ba trường hợp kiểm tra để đánh giá tính đúng đắn của các chương trình được tạo ra. Các bài toán từ các thao tác số đơn giản hoặc các nhiệm vụ yêu cầu sử dụng cơ bản các hàm thư viện tiêu chuẩn đến các nhiệm vụ đòi hỏi kiến thức bên ngoài không tầm thường.

MBJP [8] và MBCPP [8] được rút ra từ MBPP [9]. MBJP và MBCPP chứa lần lượt 966 và 848 bài toán lập trình huy động đông đảo bằng Java và C++. Mỗi bài toán bao gồm một mô tả, một hàm riêng lẻ và ba trường hợp kiểm tra. Những bài toán này bao gồm các nguyên tắc cơ bản lập trình, chức năng thư viện tiêu chuẩn, v.v.

Chúng tôi theo các nghiên cứu trước đây [8,9] để chia ba bộ dữ liệu thành tập huấn luyện, tập hợp lệ và tập kiểm tra, tương ứng. Chúng tôi đo lường hiệu suất của các phương pháp ICL khác nhau trên tập kiểm tra.

4.3 Thước đo Đánh giá
Theo các nghiên cứu tạo sinh mã trước đây [8,14], chúng tôi tận dụng Pass@k để đánh giá các phương pháp của mình. Pass@k đánh giá tính đúng đắn chức năng của các chương trình được tạo ra bằng cách thực thi các trường hợp kiểm tra. Chính xác, với một yêu cầu kiểm tra, chúng tôi tạo ra k chương trình sử dụng chiến lược lấy mẫu. Nếu bất kỳ chương trình nào trong k chương trình được tạo ra vượt qua tất cả các trường hợp kiểm tra, chúng tôi nghĩ rằng yêu cầu đã được giải quyết. Cuối cùng, tỷ lệ phần trăm của các yêu cầu được giải quyết trong tất cả các yêu cầu kiểm tra được coi là Pass@k. Trong bài báo này, chúng tôi đặt k thành 1, 3 và 5.

Chúng tôi nhận thấy rằng các nghiên cứu trước đây [20,21] cũng sử dụng một số thước đo dựa trên khớp như BLEU [37] và CodeBLEU [41]. Những thước đo này tập trung vào sự tương đồng văn bản hoặc sự tương đồng cú pháp giữa các chương trình tham chiếu và các chương trình được tạo ra. Tuy nhiên, công việc hiện tại [24] đã chỉ ra rằng các thước đo dựa trên khớp không thể đo lường hiệu quả chức năng của các chương trình. Trong bài báo này, chúng tôi theo các nghiên cứu gần đây [24, 29, 38] sử dụng các thước đo dựa trên thực thi (ví dụ Pass@k).

4.4 Baseline
Có một vài nghiên cứu để điều tra việc lựa chọn ví dụ trong ngữ cảnh cho tạo sinh mã như học zero-shot [11], lựa chọn ngẫu nhiên [14] và AceCoder [31], trong đó AceCoder [31] là baseline tiên tiến nhất (SOTA).

Học Zero-Shot [11] đưa trực tiếp một yêu cầu vào LLM mà không có bất kỳ ví dụ nào làm lời nhắc. LLM tạo ra chương trình cho yêu cầu đã cho.

Random [14] chọn ngẫu nhiên một vài ví dụ từ tập huấn luyện và xây dựng một lời nhắc. Sau đó, LLM dự đoán mã nguồn dựa trên lời nhắc.

AceCoder [31] sử dụng BM25 [42] để tính toán sự tương đồng văn bản giữa một yêu cầu kiểm tra và các yêu cầu của ứng viên, và truy xuất một tập hợp các ví dụ có sự tương đồng cao. Trong suy luận, nó đầu tiên tạo ra các trường hợp kiểm tra và sau đó dự đoán các chương trình. Để so sánh công bằng, chúng tôi sử dụng cùng một pipeline với LAIL và tất cả các baseline.

Để đánh giá rộng rãi hiệu quả của LAIL, chúng tôi cũng chuyển một số phương pháp ICL tiên tiến trong xử lý ngôn ngữ tự nhiên sang mã nguồn.

TOP-k-SBERT [35] tận dụng Sentence-BERT [40], một bộ mã hóa câu đại diện, để mã hóa tất cả các yêu cầu trong tập huấn luyện. Với một yêu cầu kiểm tra, chúng tôi đầu tiên mã hóa nó và tính toán sự tương đồng ngữ nghĩa giữa nó và các yêu cầu huấn luyện. Tiếp theo, chúng tôi chọn k ví dụ tương tự nhất.

TOP-k-GraphCodeBERT [21] là một biến thể của TOP-k-SBERT. Nó áp dụng GraphCodeBERT để mã hóa các yêu cầu và truy xuất một vài ví dụ từ tập huấn luyện dựa trên sự tương đồng ngữ nghĩa.

TOP-k-VOTE [43] là một phương pháp dựa trên đồ thị [43], để bỏ phiếu cho các ví dụ. Nó đầu tiên mã hóa mỗi ví dụ bằng GraphCodeBERT và mỗi ví dụ là một đỉnh trong đồ thị. Mỗi đỉnh kết nối với k đỉnh gần nhất dựa trên sự tương đồng ngữ nghĩa của chúng. Cuối cùng, nó coi k ví dụ gần nhất là một lời nhắc.

Uncertainty-Target [17] giả định rằng các ví dụ có độ bất định cao hơn có tác động lớn hơn đến LLM. Nó định nghĩa độ bất định là độ phức tạp khi LLM tạo ra ground truth. Phương pháp này tính toán độ bất định của mỗi ứng viên và chọn k mục có độ phức tạp cao làm lời nhắc.

4.5 Mô hình Ngôn ngữ Lớn Cơ sở
Bài báo này tập trung vào tạo sinh mã với LLM. Do đó, chúng tôi chọn ba LLM được đề xuất gần đây cho tạo sinh mã làm mô hình cơ sở. Chi tiết của các mô hình cơ sở được hiển thị như sau.

ChatGPT [1] là mô hình ngôn ngữ tiên tiến nhất cho tạo sinh mã. Nó được huấn luyện trên một lượng lớn dữ liệu văn bản ngôn ngữ tự nhiên và lập trình. Sau đó, ChatGPT được huấn luyện liên tục với học tăng cường và học cách căn chỉnh với hướng dẫn của con người. Chúng tôi tận dụng API của OpenAI để truy cập ChatGPT (tức là gpt-3.5-turbo).

GPT-3.5 [2] là một mô hình ngôn ngữ lớn mạnh mẽ và được huấn luyện trên kho ngữ liệu lớn không được gán nhãn. Trong bài báo này, chúng tôi sử dụng API của OpenAI để truy cập phiên bản mới nhất với 175 tỷ tham số (tức là text-davinci-003).

CodeGen [47] là một họ mô hình ngôn ngữ cho tạo sinh mã. CodeGen được huấn luyện với kho ngữ liệu mã 635GB và dữ liệu văn bản tiếng Anh 1159GB. Trong bài báo này, chúng tôi tận dụng phiên bản lớn nhất với 16 tỷ tham số (tức là CodeGen-Multi-16B).

4.6 Chi tiết Triển khai
Ước tính và Gán nhãn Ví dụ. Chúng tôi sử dụng giải mã tham lam để tạo ra chương trình và thu thập các xác suất dự đoán của các chương trình ground-truth. Theo các công việc tạo sinh trước đây [31], chúng tôi đặt độ dài tạo sinh tối đa là 500 token. Số lượng ví dụ trong tập Si là 50 để đảm bảo hiệu quả. Lưu ý rằng kích thước của Si càng lớn thì hiệu suất của LLM càng tốt và chúng tôi để kích thước khác trong công việc tương lai của mình. Chúng tôi đặt z và v lần lượt là 5 và phân tích tác động của chúng đối với hiệu suất trong Phần 6.3. Do đó, tập tích cực Spi chứa 5 ví dụ có điểm số thước đo cao hơn và tập tiêu cực Sni có 5 mục có điểm số thấp hơn.

Huấn luyện Bộ Truy xuất Neural. Trong quy trình này, chúng tôi đặt số lượng của tập ví dụ tiêu cực Ni là 64. Nói cách khác, h được đặt thành 63. Đối với mỗi epoch, chúng tôi chọn ngẫu nhiên 63 ví dụ từ tập H. Chúng tôi cũng cố gắng đặt kích thước của Ni thành 32 và 128 trong Phần 6.3. Tỷ lệ học là 5e-5 và kích thước lô là 32. Chúng tôi huấn luyện bộ truy xuất trong khoảng 1 giờ trên 4 NVIDIA A100.

Suy luận. Chúng tôi coi một mô hình ngôn ngữ lớn như một bộ tạo sinh hộp đen và lấy mẫu các chương trình từ nó. Đầu vào của LLM chỉ chứa một lời nhắc và một yêu cầu kiểm tra mà không có bất kỳ hướng dẫn ngôn ngữ tự nhiên nào. Trong quá trình lấy mẫu, chúng tôi sử dụng lấy mẫu nuclear [22] để giải mã, trong đó nhiệt độ là 0.8 và top-p là 0.95. Độ dài tạo sinh tối đa là 500 token. Đối với mỗi yêu cầu kiểm tra, chúng tôi tạo ra 5 chương trình. Để so sánh công bằng, chúng tôi đặt các tham số tương tự để tạo ra chương trình cho tất cả các baseline và LAIL của chúng tôi.

5 KẾT QUẢ VÀ PHÂN TÍCH
RQ1: LAIL hoạt động như thế nào so với các baseline tiên tiến nhất?
Trong RQ1, chúng tôi áp dụng LAIL và các baseline cho hai LLM và đo lường tính đúng đắn của các chương trình được tạo ra.

Thiết lập. Chúng tôi so sánh LAIL và các baseline (Phần 4.4) trên ba bộ dữ liệu tạo sinh mã (Phần 4.2). Thước đo đánh giá là Pass@k như được mô tả trong Phần 4.3. Đối với thước đo, điểm số cao hơn cho thấy hiệu suất tốt hơn của các phương pháp.

Kết quả. Bảng 2 và Bảng 3 báo cáo Pass@k (k ∈ [1, 3, 5]) của các phương pháp khác nhau trên CodeGen và GPT-3.5, tương ứng. Số in đậm có nghĩa là cải thiện có ý nghĩa so với các baseline, và các phần trăm màu đỏ đại diện cho những cải thiện tương đối so với baseline tiên tiến nhất (SOTA).

Phân tích. (1) LAIL đạt được hiệu suất tốt nhất trong tất cả các phương pháp với những cải thiện đáng kể. Trong tất cả các bộ dữ liệu, LAIL tạo ra nhiều chương trình đúng hơn so với các baseline. So với baseline SOTA, LAIL vượt trội lần lượt 11.58%, 6.89% và 5.07% trên CodeGen, và đạt được 4.38%, 2.85% và 2.74% cải thiện trên GPT-3.5 ở Pass@1. Lưu ý rằng Pass@1 là một thước đo rất nghiêm ngặt và khác biệt để được cải thiện. Những cải thiện đáng kể chứng minh tính ưu việt của LAIL trong tạo sinh mã. (2) Việc chọn các ví dụ phù hợp là quan trọng đối với hiệu suất của ICL. So với lựa chọn ngẫu nhiên, LAIL đạt được những cải thiện tuyệt đối lần lượt 8.31%, 30.60% và 32.86% ở Pass@1 trên GPT-3.5. AceCoder và các phương pháp heuristic khác tiếp tục cải thiện hiệu suất tạo sinh mã bằng cách chọn các ví dụ tương tự về mặt văn bản hoặc ngữ nghĩa. LAIL khai thác chính các LLM để ước tính và huấn luyện một bộ truy xuất neural để căn chỉnh với sở thích của LLM, sau đó đạt được kết quả tốt nhất trong tất cả các phương pháp. Điều đó chứng minh tầm quan trọng của các ví dụ trong ICL và xác minh tính hợp lý của việc sử dụng chính các LLM để gán nhãn ví dụ. (3) LAIL có hiệu quả trong các LLM có kích thước khác nhau và các ngôn ngữ lập trình khác nhau. Như được mô tả ở trên, phương pháp của chúng tôi trên CodeGen và GPT-3.5 đều đạt được hiệu suất tốt nhất trong tất cả các phương pháp. Bảng 2 và Bảng 3 cũng cho thấy rằng LAIL có thể tạo ra nhiều chương trình đúng hơn trên tất cả các bộ dữ liệu bao gồm MBPP (Python), MBJP (Java) và MBCPP (C++). Điều này tiết lộ LAIL được tổng quát hóa và có thể được áp dụng cho các LLM và ngôn ngữ khác nhau.

Trả lời cho RQ1: LAIL đạt được kết quả tốt nhất trong tất cả các baseline. Trong ba bộ dữ liệu, LAIL đạt được những cải thiện 11.58%, 6.89% và 5.07% trên CodeGen, và đạt được 4.38%, 2.85% và 2.74% cải thiện trên GPT-3.5 ở Pass@1. Những cải thiện đáng kể chứng minh phương pháp của chúng tôi có thể căn chỉnh với sở thích của LLM. Do đó, nó có thể chọn các ví dụ phù hợp cho ICL và tạo ra nhiều chương trình đúng hơn.

RQ2: Các nhà phát triển có thích các chương trình được tạo ra bởi LAIL không?
Mục tiêu của một phương pháp tạo sinh mã là hỗ trợ các nhà phát triển viết chương trình. Do đó, một chương trình tốt không chỉ thỏa mãn yêu cầu mà còn dễ đọc và duy trì. Trong câu hỏi này, chúng tôi xác minh thủ công các chương trình được tạo ra trong ba khía cạnh.

Thiết lập. Theo công việc trước đây [29], chúng tôi đo lường thủ công các chương trình được tạo ra bởi các phương pháp khác nhau trong ba khía cạnh (ví dụ: tính đúng đắn, chất lượng mã, khả năng bảo trì). Tính đúng đắn đo lường xem một chương trình có thỏa mãn yêu cầu đã cho hay không, chất lượng mã xác minh xem một chương trình có chứa mùi mã xấu hay không, và khả năng bảo trì đo lường xem việc triển khai có được tiêu chuẩn hóa và dễ đọc hay không. Đối với mỗi khía cạnh, điểm số là một số nguyên và dao động từ 0 đến 2. Điểm số càng cao, mã càng tốt.

Cụ thể, chúng tôi chọn ngẫu nhiên 50 mẫu kiểm tra từ MBPP, MBJP và MBCPP, tương ứng. Sau đó, chúng tôi sử dụng LAIL và các baseline để tạo ra chương trình cho các ví dụ này. Cuối cùng, chúng tôi thu được 400 (50×8) chương trình để đánh giá con người. 400 chương trình được chia ngẫu nhiên thành 5 nhóm. Chúng tôi thuê 10 nhà phát triển để xác minh các chương trình này. Những nhà phát triển này là sinh viên khoa học máy tính hoặc thực hành công nghiệp và viết chương trình ít nhất 3 năm. Mỗi nhóm được đo lường bởi hai nhà phát triển và điểm số cuối cùng là trung bình của điểm số của hai nhà phát triển.

Kết quả. Kết quả của đánh giá con người được hiển thị trong Bảng 4. Các phần trăm trong ngoặc đơn là những cải thiện tương đối so với baseline SOTA.

Phân tích. (1) Các nhà phát triển thích các chương trình được tạo ra bởi LAIL hơn tất cả các baseline. LAIL vượt trội đáng kể so với tất cả các baseline trong ba khía cạnh, điều này chứng minh hiệu quả của phương pháp chúng tôi. (2) Ngoài việc thỏa mãn yêu cầu, các chương trình được cung cấp bởi phương pháp của chúng tôi dễ đọc hơn và có ít mùi xấu hơn. Đặc biệt, LAIL của chúng tôi vượt trội hơn phương pháp SOTA 5.811% trong chất lượng mã và 7.027% trong khả năng bảo trì. (3) Một số phương pháp heuristic chọn cùng các ví dụ làm lời nhắc cho tất cả các yêu cầu kiểm tra không tối ưu cho ICL. Như được hiển thị trong Hình 4, Uncertainty-Target và Vote-K hoạt động tương đương với phương pháp ngẫu nhiên, điều này cho thấy rằng việc áp dụng cùng một lời nhắc cho tất cả dữ liệu kiểm tra không phải là một lựa chọn tốt, và các yêu cầu kiểm tra khác nhau nên sử dụng lời nhắc phù hợp của chúng.

Trả lời cho RQ2: Đánh giá con người cho thấy LAIL vượt trội đáng kể so với các baseline SOTA trong cả ba khía cạnh. Các chương trình được tạo ra bởi LAIL có thể thỏa mãn yêu cầu tốt hơn, dễ đọc hơn và có ít mùi mã xấu. Nghĩa là, các nhà phát triển thích các chương trình được tạo ra bởi LAIL trong tất cả các phương pháp.

RQ3: Lựa chọn thiết kế nào tốt hơn để ước tính ví dụ?
Trong bài báo này, chúng tôi áp dụng xác suất dự đoán của ground truth để ước tính ví dụ. Chúng tôi điều tra thêm hai lựa chọn hợp lý để đo lường ứng viên bằng chính các LLM.

Thiết lập. Chúng tôi thiết kế hai phương pháp hợp lý bao gồm các định dạng Match-BLEU và Match-CodeBLEU. Định dạng Match-BLEU sử dụng điểm số BLEU của chương trình được tạo ra để đo lường ví dụ. Phương pháp Match-CodeBLEU áp dụng điểm số CodeBLEU của chương trình được dự đoán để gán nhãn ví dụ. Trong RQ này, chúng tôi chọn một LLM đại diện (tức là GPT-3.5) làm mô hình cơ sở và đánh giá hiệu suất của chúng trên ba bộ dữ liệu.

Kết quả. Kết quả của các điểm số phản hồi khác nhau được đại diện trong Bảng 5. Để so sánh, chúng tôi cũng trình bày kết quả của baseline SOTA hiện tại trong Bảng 5.

Phân tích. (1) Phương pháp dựa trên xác suất của chúng tôi tốt hơn Match-BLEU và Match-CodeBLEU. Trong tất cả các bộ dữ liệu, LAIL của chúng tôi vượt trội so với chúng lần lượt 4.05%, 5.86% và 3.77% trên ba bộ dữ liệu ở Pass@1. Chúng tôi lập luận rằng xác suất dự đoán của ground truth phản ánh chính xác sự chắc chắn của LLM đối với các chương trình đúng. Các phương pháp dựa trên BLEU và CodeBLEU chỉ phản ánh độ chính xác theo nghĩa đen của dự đoán của LLM, nhưng không thể cung cấp mức độ chắc chắn của LLM khi dự đoán chương trình. (2) Phương pháp Match-CodeBLEU hiệu quả hơn phương pháp Match-BLEU. Match-CodeBLEU vượt trội so với Match-BLEU trên tất cả các bộ dữ liệu. Lý do có thể là CodeBLEU đo lường sự khớp n-gram, sự khớp ngữ nghĩa và sự khớp cú pháp giữa các chương trình giả thuyết và các token mã tham chiếu, điều này tốt hơn để đánh giá các chương trình được tạo ra so với BLEU.

Trả lời cho RQ3: Phương pháp dựa trên xác suất tốt hơn các định dạng Match-CodeBLEU và Match-BLEU. Nó có thể phản ánh hiệu quả sở thích của LLM đối với các ví dụ ứng viên với một yêu cầu kiểm tra.

6 THẢO LUẬN
6.1 Khả năng Chuyển giao
Chúng tôi khám phá xem bộ truy xuất của chúng tôi dựa trên phản hồi của một LLM và bộ dữ liệu cụ thể có thể được chuyển giao cho các LLM khác hoặc bộ dữ liệu tạo sinh mã mà không cần điều chỉnh thêm hay không. Đây là một câu hỏi nghiên cứu quan trọng vì bộ truy xuất cho mỗi LLM và bộ dữ liệu cần được huấn luyện trong các ứng dụng thực tế.

6.1.1 Chuyển giao giữa các LLM. Chúng tôi xem xét việc chuyển giao bộ truy xuất dựa trên phản hồi của một LLM sang LLM khác. Cụ thể, chúng tôi sử dụng LLM nguồn (ví dụ: CodeGen hoặc GPT-3.5) để ước tính ví dụ để huấn luyện bộ truy xuất và sau đó áp dụng bộ truy xuất cho LLM mục tiêu khác (ví dụ: ChatGPT) trong việc tạo ra chương trình. Bảng 6 cho thấy hiệu suất của ChatGPT trong ba bộ dữ liệu. Chúng tôi ngạc nhiên khi thấy rằng bộ truy xuất của chúng tôi dựa trên CodeGen và GPT-3.5 có thể mang lại những cải thiện rõ ràng cho ChatGPT. Đặc biệt, về Pass@1, ChatGPT đạt được 2.56% cải thiện từ phản hồi của CodeGen và 4.31% nâng cao từ phản hồi của GPT-3.5 so với baseline SOTA. Những hiện tượng này chứng minh rằng phương pháp của chúng tôi có khả năng chuyển giao đáng hài lòng giữa các LLM khác nhau. Lưu ý rằng ChatGPT không thể cung cấp xác suất dự đoán của ground truth trong thực tế, do đó LAIL là một phương pháp khá có ý nghĩa, đặc biệt đối với các LLM có tham số không có sẵn. Ngoài ra, hiệu suất của ChatGPT từ phản hồi của GPT-3.5 cao hơn so với phản hồi từ CodeGen. Lý do có thể là GPT-3.5 và ChatGPT có khả năng tương đương trong tạo sinh mã, do đó sở thích của chúng tương tự và GPT-3.5 có thể cung cấp các ví dụ phù hợp hơn làm lời nhắc.

Để xác minh khả năng chuyển giao giữa các LLM có kích thước khác nhau, chúng tôi đánh giá thêm hiệu suất của CodeGen dựa trên phản hồi của GPT-3.5 và kết quả của GPT-3.5 từ phản hồi của CodeGen, trong đó kích thước tham số của CodeGen nhỏ hơn nhiều so với GPT-3.5. Như được hiển thị trong Bảng 6, so với phương pháp ngẫu nhiên, bộ truy xuất học từ CodeGen đạt được những cải thiện tương đối lần lượt 8.25%, 12.09% và 4.32% đối với GPT-3.5 ở Pass@1, trong khi bộ truy xuất của chúng tôi dưới phản hồi của GPT-3.5 mang lại những cải thiện lần lượt 22.30%, 14.47% và 16.94% cho CodeGen. Điều này cho thấy phương pháp của chúng tôi có khả năng chuyển giao đáng hài lòng giữa các LLM và có thể mang lại cải thiện cho các LLM mục tiêu.

6.1.2 Chuyển giao giữa các bộ dữ liệu. Xem xét rằng các tính năng tổng hợp của ngôn ngữ tự nhiên là tổng quát, bộ truy xuất được huấn luyện trên một bộ dữ liệu có thể áp dụng cho các bộ dữ liệu khác và khai thác kiến thức tương tự trong các bộ dữ liệu khác nhau. Trong phần này, chúng tôi điều tra thêm xem bộ truy xuất được huấn luyện trên một bộ dữ liệu có thể chuyển giao cho các bộ dữ liệu khác hay không. Chúng tôi chuyển giao bộ truy xuất giữa ba bộ dữ liệu (ví dụ: MBJP, MBPP và MBCPP) và Hình 3 chứng minh kết quả chuyển giao trên Pass@1. Chúng tôi thấy rằng hầu hết các bộ truy xuất có thể chuyển giao thành công sang các bộ dữ liệu khác và mang lại cải thiện so với baseline SOTA của chúng. Cụ thể, bộ truy xuất được huấn luyện trên MBJP (MBCPP) đạt được những cải thiện tuyệt đối 1.28% (1.07%) khi nó di chuyển sang MBCPP (MBJP). Trong khi đó, bộ truy xuất được huấn luyện trên MBCPP khó chuyển giao sang MBPP và bộ truy xuất dựa trên MBPP gặp khó khăn với hiệu suất tạo sinh trên MBPP. Lý do có thể là Java và C++ là các ngôn ngữ lập trình hướng đối tượng, và cú pháp và hình thái mã của chúng tương tự. Khám phá một bộ truy xuất phù hợp cho nhiều bộ dữ liệu là một câu hỏi nghiên cứu thách thức nhưng có ý nghĩa, và chúng tôi để chủ đề này làm công việc tương lai.

6.2 Tác động của Số lượng Ví dụ Trong Ngữ cảnh
Hầu hết các LLM được huấn luyện với độ dài đầu vào hạn chế, điều này hạn chế số lượng ví dụ trong lời nhắc. Các nghiên cứu trước đây [19,35] cũng thấy rằng LLM bị ảnh hưởng bởi số lượng ví dụ trong ngữ cảnh. Ở đây chúng tôi khám phá tác động của số lượng ví dụ trong lời nhắc đối với các baseline và phương pháp của chúng tôi. Hình 4 báo cáo cách hiệu suất của LAIL và GraphCodeBERT thay đổi theo số lượng ví dụ trong ngữ cảnh khác nhau trong bộ dữ liệu MBPP. Lưu ý rằng GraphCodeBERT là baseline tốt nhất trong bộ dữ liệu này, do đó chúng tôi chọn nó để so sánh với phương pháp của chúng tôi. Chúng tôi quan sát thấy rằng hiệu suất của GraphCodeBERT và phương pháp của chúng tôi tăng đơn điệu một cách thô với sự gia tăng của số lượng ví dụ trong ngữ cảnh. Ngoài ra, LAIL của chúng tôi luôn vượt trội so với GraphCodeBERT trong tất cả các trường hợp, điều này chứng minh thêm tính ưu việt của phương pháp chúng tôi ngay cả với số lượng ví dụ trong ngữ cảnh khác nhau.

6.3 Tác động của Tham số Học Tương phản
Số lượng ví dụ tiêu cực Ni (được gọi là Ni để thuận tiện trong Hình 7) là một yếu tố quan trọng trong học tương phản để huấn luyện bộ truy xuất neural. Chúng tôi điều tra cách yếu tố này ảnh hưởng đến hiệu suất của phương pháp chúng tôi. Như được hiển thị trong Hình 7, với sự gia tăng của số lượng ví dụ tiêu cực, LAIL đạt được những cải thiện nhất quán và có thể tạo ra nhiều chương trình đúng hơn. Ngoài ra, như được mô tả trong 3.2, chúng tôi chọn ngẫu nhiên một ví dụ eni từ Sni vào Ni. Chúng tôi cũng khám phá tác động của số lượng ví dụ được chọn từ Sni (được gọi là τne). Chúng ta có thể thấy rằng càng nhiều ví dụ tiêu cực khó, phương pháp của chúng tôi hoạt động càng tệ. Chúng tôi lập luận rằng các ví dụ của Sni có điểm số BM25 cao trong tất cả các ví dụ ứng viên và do đó Sni có thể chứa một số ví dụ tiêu cực sai. Việc chọn nhiều ví dụ hơn từ Sni sẽ ảnh hưởng đến việc học bộ truy xuất để chọn các ví dụ trong ngữ cảnh phù hợp. Giữa hai yếu tố này, số lượng ví dụ được chọn từ Sni có ảnh hưởng lớn hơn đến hiệu suất của phương pháp chúng tôi.

6.4 Các Mối Đe dọa đối với Tính Hợp lệ
Có hai mối đe dọa chính đối với tính hợp lệ của công việc chúng tôi.

Khả năng tổng quát hóa của các kết quả thí nghiệm. Đối với các bộ dữ liệu, chúng tôi theo các nghiên cứu trước đây [8,9,14] và tận dụng ba bộ dữ liệu tạo sinh mã đại diện. Ba bộ dữ liệu bao gồm các ngôn ngữ lập trình khác nhau (ví dụ: Java, Python và C++) và đến từ các cộng đồng phần mềm thực tế. Để xác minh tính ưu việt của LAIL, chúng tôi xem xét bảy phương pháp ICL hiện tại trong cả nhiệm vụ tạo sinh mã và nhiều nhiệm vụ ngôn ngữ tự nhiên duy trì. Ngoài ra, để đánh giá hiệu quả phương pháp của chúng tôi, chúng tôi chọn một loạt các mô hình ngôn ngữ lớn được huấn luyện trước tiên tiến (CodeGen [47], GPT-3.5 và ChatGPT) làm mô hình cơ sở trong ba năm qua. Chúng tôi áp dụng phương pháp và baseline cho các mô hình cơ sở và đánh giá hiệu suất của chúng trong tạo sinh mã. Đối với thước đo, theo các công việc hiện tại [13,47], chúng tôi chọn thước đo Pass@k được sử dụng rộng rãi để đánh giá tất cả các phương pháp. Đây là một thước đo dựa trên thực thi sử dụng các trường hợp kiểm tra để kiểm tra tính đúng đắn của các chương trình được tạo ra. Để đảm bảo công bằng, chúng tôi thực hiện mỗi phương pháp ba lần và báo cáo kết quả thí nghiệm trung bình.

Việc triển khai các mô hình và lời nhắc. Được biết rộng rãi rằng các mô hình mạng neural sâu nhạy cảm với các chi tiết triển khai. Trong bài báo này, chúng tôi cần thực hiện tất cả các baseline và phương pháp của chúng tôi trên ba mô hình cơ sở. Đối với các baseline, chúng tôi áp dụng mã nguồn và tham số được xuất bản bởi các bài báo gốc của chúng [21,40,47]. Đối với LLM (ví dụ: GPT-3.5 và ChatGPT), các siêu tham số (ví dụ: nhiệt độ) của lấy mẫu sẽ ảnh hưởng đến đầu ra của chúng. Trong các thí nghiệm, chúng tôi theo các nghiên cứu trước đây [29,47] để đặt siêu tham số cho tất cả các phương pháp. Ngoài ra, hiệu suất của LLM phụ thuộc rất nhiều vào lời nhắc, bao gồm hướng dẫn và số lượng ví dụ. Để giảm thiểu mối đe dọa này, chúng tôi tận dụng cùng số lượng ví dụ cho tất cả các phương pháp và xây dựng lời nhắc trực tiếp mà không có bất kỳ hướng dẫn ngôn ngữ tự nhiên nào. Ngoài ra, đáng chú ý rằng nhóm ứng viên để lựa chọn ví dụ cũng ảnh hưởng đến hiệu suất ICL. Một nghiên cứu quy mô lớn trên 13.2 triệu tệp mã thực tế cho thấy tỷ lệ mã được tái sử dụng lên đến 80% [36]. Do đó, chúng tôi sử dụng tập huấn luyện của mỗi bộ dữ liệu làm tập ứng viên cho tất cả các phương pháp trong bài báo này và tin rằng tập huấn luyện có thể cung cấp các ví dụ hỗ trợ cho các yêu cầu kiểm tra. Chúng tôi không điều chỉnh lời nhắc và siêu tham số thực nghiệm mà đặt chúng theo kinh nghiệm. Do đó, có thể có không gian để điều chỉnh cài đặt siêu tham số của phương pháp chúng tôi để có thêm cải thiện.

7 CÔNG VIỆC LIÊN QUAN
7.1 Mô hình Ngôn ngữ Được Huấn luyện Trước
Tạo sinh mã nhằm tự động tạo ra chương trình với các yêu cầu ngôn ngữ tự nhiên đã cho. Các mô hình ngôn ngữ được huấn luyện trước đã đạt được kết quả tiên tiến nhất trong tạo sinh mã. Chúng được huấn luyện trước trên kho ngữ liệu mã không được gán nhãn quy mô lớn và sau đó được chuyển giao cho tạo sinh mã. Các mô hình được huấn luyện trước hiện tại có thể được chia thành các mô hình encoder-decoder và các mô hình chỉ decoder.

Các mô hình Encoder-decoder bao gồm một encoder và một decoder. Một encoder nhận một yêu cầu làm đầu vào, và một decoder đưa ra một chương trình. Nhiều kiến trúc encoder-decoder phổ biến trong xử lý ngôn ngữ tự nhiên đã được áp dụng cho mã nguồn, thường sử dụng các mục tiêu huấn luyện trước dựa trên denoising để huấn luyện trước, như dự đoán chuỗi được che dấu, gắn thẻ định danh và dự đoán định danh được che dấu. Ví dụ, CodeT5 [44] là biến thể của mô hình T5 [39] để hỗ trợ chương trình. [5] được điều chỉnh từ mô hình BART [28], được huấn luyện trước trên một số chương trình. Ngoài ra, một số nghiên cứu [12] xem xét thêm tính năng tự nhiên hóa của mã và huấn luyện các mô hình để tạo ra các chương trình tự nhiên dựa trên các chương trình có nhiễu.

Các mô hình chỉ Decoder chứa các mạng decoder, được huấn luyện trước với mục tiêu dự đoán token tiếp theo. Được truyền cảm hứng từ thành công của chuỗi GPT [38] trong xử lý ngôn ngữ tự nhiên, các nhà nghiên cứu cố gắng điều chỉnh các mô hình tương tự cho mã nguồn. CodeGPT [34] được huấn luyện trước trên kho ngữ liệu CodeSearchNet [23] với cùng cài đặt và cấu trúc như GPT-2. CodeX được fine-tuned liên tục trên GPT-3 [11] trên kho ngữ liệu mã từ GitHub. CodeX thành thạo hơn một tá ngôn ngữ (ví dụ: JavaScript và Python) và hỗ trợ một ứng dụng thương mại (ví dụ: Copilot). Vì các tham số của nó không có sẵn, nhiều nhà nghiên cứu cố gắng tái tạo chúng và mang lại CodeParrot [3], GPT-CC [4], PyCodeGPT [45] và CodeGen [47]. CodeGeeX [46] được huấn luyện trước trên kho ngữ liệu mã lớn và có thể tạo ra hơn 20 ngôn ngữ lập trình. Gần đây, ChatGPT được đề xuất bởi OpenAI và đạt được hiệu suất ấn tượng trong tạo sinh mã. Trong bài báo này, chúng tôi chọn ba mạng đại diện bao gồm CodeGen [47], GPT-3.5 [2] và ChatGPT [1] làm mô hình cơ sở của chúng tôi.

7.2 Học Trong Ngữ cảnh
Học trong ngữ cảnh (ICL) là một phương pháp mới nổi để sử dụng các mô hình ngôn ngữ lớn (LLM). Bằng cách cung cấp các ví dụ hạn chế làm lời nhắc, ICL trao quyền cho LLM học một nhiệm vụ downstream cụ thể. ICL [11] được đề xuất lần đầu tiên trong xử lý ngôn ngữ tự nhiên và đạt được hiệu suất ấn tượng trong nhiều nhiệm vụ [19,27] như tạo sinh văn bản và phân loại cảm tình.

Được truyền cảm hứng từ thành công của ICL trong xử lý ngôn ngữ tự nhiên, các nhà nghiên cứu cố gắng điều chỉnh ICL cho mã nguồn [10,14,16]. Họ thiết kế các hướng dẫn cụ thể cho nhiệm vụ với một tập hợp các ví dụ để nhắc LLM và cải thiện hiệu suất trong nhiều nhiệm vụ (ví dụ: tạo sinh mã [16] và sửa chữa mã [25]). Sự phổ biến của ICL cũng giới thiệu sự bất ổn trong hiệu suất: với các tập hợp ví dụ khác nhau làm lời nhắc, hiệu suất của LLM thường dao động từ ngẫu nhiên đến gần tiên tiến nhất [33]. Một số nghiên cứu [16,31] nỗ lực để giảm thiểu vấn đề này. Các nhà nghiên cứu [16] chọn ngẫu nhiên các ví dụ hạn chế cho ICL và xác minh kết quả trong tạo sinh mã. AceCoder [31] sử dụng BM25 để chọn các ví dụ khớp n-gram và tạo ra nhiều chương trình đúng hơn. Tuy nhiên, các phương pháp này dựa trên các tính năng từ vựng, yêu cầu các nhà phát triển thiết kế heuristic và dẫn đến hiệu suất không tối ưu. So với các phương pháp hiện tại, LAIL là một phương pháp dựa trên học để chọn các ví dụ trong ngữ cảnh. Thay vì xem xét sự tương đồng văn bản, nó áp dụng chính các LLM để đo lường các ví dụ ứng viên và sử dụng học tương phản để học sở thích của LLM trong tạo sinh mã.

8 KẾT LUẬN
Do tính bất ổn của ICL, có nhu cầu ngày càng tăng về lựa chọn ví dụ trong ngữ cảnh trong tạo sinh mã. Bài báo này đề xuất một phương pháp ICL dựa trên học mới LAIL. Chúng tôi khai thác chính các LLM để ước tính các ví dụ bằng cách xem xét các xác suất tạo sinh của các chương trình ground-truth với một yêu cầu, và gán nhãn các ví dụ này dựa trên phản hồi của LLM. Sau đó chúng tôi giới thiệu một mục tiêu học tương phản để huấn luyện một bộ truy xuất, dẫn đến việc có được sở thích của LLM. Kết quả thí nghiệm trên ba bộ dữ liệu tạo sinh mã chứng minh rằng phương pháp đề xuất của chúng tôi đạt được hiệu suất xuất sắc. LAIL cũng cho thấy khả năng chuyển giao đáng hài lòng giữa các LLM và bộ dữ liệu, cho thấy rằng đây là một phương pháp hiệu quả và thực tế để tạo sinh mã. Trong tương lai, chúng tôi sẽ áp dụng phương pháp của mình cho các LLM và bộ dữ liệu khác.

TÀI LIỆU THAM KHẢO
[1] Có sẵn. https://openai.com/chatgpt.
[2] Có sẵn. https://platform.openai.com/docs/models/gpt-3-5.
[3] Có sẵn. https://huggingface.co/codeparrot/codeparrot.
[4] Có sẵn. https://github.com/CodedotAl/gpt-code-clippy.
[5] WU Ahmad, S Chakraborty, B Ray, and KW Chang. 2021. Unified pre-training for program understanding and generation.. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
[6] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 1–37.
[7] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton. 2018. A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv. 51, 4 (2018), 81:1–81:37. https://doi.org/10.1145/3212695
[8] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual Evaluation of Code Generation Models. arXiv preprint arXiv:2210.14868 (2022).
[9] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).
[10] Patrick Bareiß, Beatriz Souza, Marcelo d'Amorim, and Michael Pradel. 2022. Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code. arXiv preprint arXiv:2206.01335 (2022).
[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
[12] Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar T Devanbu, and Baishakhi Ray. 2022. Natgen: generative pre-training by "naturalizing" source code. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 18–30.
[13] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. Codet: Code generation with generated tests. arXiv preprint arXiv:2207.10397 (2022).
[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).
[15] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597–1607.
[16] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128 (2023).
[17] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. 2019. Selection via proxy: Efficient data selection for deep learning. arXiv preprint arXiv:1906.11829 (2019).
[18] Li Dong and Mirella Lapata. 2016. Language to Logical Form with Neural Attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 33–43.
[19] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234 (2022).
[20] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).
[21] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020).
[22] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019).
[23] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).
[24] Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnación, Shuvendu Lahiri, Madanlal Musuvathi, and Jianfeng Gao. 2022. Fault-aware neural code rankers. Advances in Neural Information Processing Systems 35 (2022), 13419–13432.
[25] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of code language models on automated program repair. arXiv preprint arXiv:2302.05020 (2023).
[26] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. 2023. Self-planning code generation with large language model. arXiv preprint arXiv:2303.06689 (2023).
[27] Itay Levy, Ben Bogin, and Jonathan Berant. 2022. Diverse demonstrations improve in-context compositional generalization. arXiv preprint arXiv:2212.06800 (2022).
[28] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461 (2019).
[29] Jia Li, Yongmin Li, Ge Li, and Zhi Jin. 2023. Structured Chain-of-Thought Prompting for Code Generation. CoRR (2023). https://arxiv.org/abs/2305.06599
[30] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. 2023. SkCoder: A Sketch-based Approach for Automatic Code Generation. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2124–2135. https://doi.org/10.1109/ICSE48619.2023.00179
[31] Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2023. AceCoder: Utilizing Existing Code to Enhance Code Generation. CoRR (2023). https://arxiv.org/abs/2303.17780v3
[32] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092–1097.
[33] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? arXiv preprint arXiv:2101.06804 (2021).
[34] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).
[35] Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Y Zhao. 2023. Dr. ICL: Demonstration-Retrieved In-context Learning. arXiv preprint arXiv:2305.14128 (2023).
[36] Audris Mockus. 2007. Large-scale code reuse in open source software. In First International Workshop on Emerging Trends in FLOSS Research and Development (FLOSS'07: ICSE Workshops 2007). IEEE, 7–7.
[37] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318.
[38] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).
[39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 140 (2020), 1–67.
[40] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).
[41] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020).
[42] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333–389.
[43] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975 (2022).
[44] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).
[45] Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. 2022. CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation. arXiv preprint arXiv:2206.06888 (2022).
[46] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568 (2023).
[47] Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng, and Mingwen Wang. 2022. CodeGen-Test: An Automatic Code Generation Model Integrating Program Test Information. arXiv preprint arXiv:2202.07612 (2022).
