# 2306.05087.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/hyperparameters/2306.05087.pdf
# Kích thước tệp: 3940265 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
PANDA LM: MỘT BẢNG ĐÁNH GIÁ TỰ ĐỘNG CHO TỐI ƯU HÓA ĐIỀU CHỈNH HƯỚNG DẪN LLM
Yidong Wang1,2∗, Zhuohao Yu1∗,
Wenjin Yao1, Zhengran Zeng1, Linyi Yang2, Cunxiang Wang2,
Hao Chen3, Chaoya Jiang1, Rui Xie1, Jindong Wang3, Xing Xie3,
Wei Ye1†, Shikun Zhang1†, Yue Zhang2†
1Đại học Bắc Kinh 2Đại học Westlake 3Microsoft Research Asia
TÓM TẮT
Điều chỉnh hướng dẫn cho các mô hình ngôn ngữ lớn (LLM) vẫn là một nhiệm vụ đầy thách thức, do tính phức tạp của việc lựa chọn siêu tham số và khó khăn trong việc đánh giá các mô hình được điều chỉnh. Để xác định các siêu tham số tối ưu, một bảng đánh giá tự động, mạnh mẽ và đáng tin cậy là cần thiết. Tuy nhiên, việc thiết lập một bảng đánh giá như vậy không phải là nhiệm vụ đơn giản do những thách thức liên quan đến độ chính xác đánh giá và bảo vệ quyền riêng tư. Để đáp ứng những thách thức này, chúng tôi giới thiệu một mô hình ngôn ngữ lớn thẩm phán, có tên là PandaLM, được huấn luyện để phân biệt mô hình vượt trội nhất trong số một số LLM. Trọng tâm của PandaLM mở rộng ra ngoài tính đúng đắn khách quan của phản hồi, đây là trọng tâm chính của các tập dữ liệu đánh giá truyền thống. Nó giải quyết các yếu tố chủ quan quan trọng như tính súc tích tương đối, sự rõ ràng, tuân thủ hướng dẫn, tính toàn diện và tính trang trọng. Để đảm bảo độ tin cậy của PandaLM, chúng tôi thu thập một tập dữ liệu kiểm tra được con người chú thích đa dạng, trong đó tất cả các ngữ cảnh đều được con người tạo ra và các nhãn được điều chỉnh theo sở thích của con người. Trong các đánh giá sử dụng tập dữ liệu kiểm tra được thu thập của chúng tôi, các phát hiện của chúng tôi cho thấy rằng PandaLM-7B cung cấp hiệu suất có thể so sánh với cả GPT-3.5 và GPT-4. Đáng ấn tượng là PandaLM-70B vượt trội hơn hiệu suất của chúng. PandaLM cho phép đánh giá LLM công bằng hơn nhưng với chi phí thấp hơn, được chứng minh bằng những cải tiến đáng kể đạt được bởi các mô hình được điều chỉnh thông qua PandaLM so với các mô hình tương ứng được huấn luyện với các siêu tham số mặc định của Alpaca. Ngoài ra, PandaLM không phụ thuộc vào các đánh giá dựa trên API, do đó tránh được rò rỉ dữ liệu tiềm ẩn.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã thu hút sự chú ý ngày càng tăng trong lĩnh vực trí tuệ nhân tạo (OpenAI, 2023; Google, 2023; Zeng et al., 2022a; Brown et al., 2020; Chowdhery et al., 2022; Anil et al., 2023; Zhang et al., 2023), với nhiều ứng dụng từ trả lời câu hỏi (Hirschman & Gaizauskas, 2001; Kwiatkowski et al., 2019; Wang et al., 2021), dịch máy (Vaswani et al., 2017; Stahlberg, 2020) đến tạo nội dung (Biswas, 2023; Adams & Chuah, 2022). Dự án Alpaca (Taori et al., 2023) đã là một nỗ lực tiên phong trong việc điều chỉnh hướng dẫn của LLaMA (Touvron et al., 2023), đặt tiền lệ cho việc điều chỉnh hướng dẫn LLM, tiếp theo là Vicunna (Chiang et al., 2023). Nghiên cứu tiếp theo (Diao et al., 2023; Ji et al., 2023; Chaudhary, 2023) thường áp dụng các siêu tham số của Alpaca như một tiêu chuẩn để huấn luyện LLM của họ. Với sự cần thiết của việc điều chỉnh hướng dẫn cho các mô hình được huấn luyện trước này để hiểu và tuân theo hiệu quả các hướng dẫn ngôn ngữ tự nhiên (Wang et al., 2022c; Taori et al., 2023; Peng et al., 2023), việc tối ưu hóa các siêu tham số điều chỉnh của chúng là rất quan trọng để đạt được hiệu suất đỉnh cao. Các yếu tố quan trọng như lựa chọn bộ tối ưu hóa, tốc độ học, số lượng epoch huấn luyện, và chất lượng cũng như kích thước của dữ liệu huấn luyện có ảnh hưởng đáng kể đến hiệu suất của mô hình (Liaw et al., 2018; Tan & Le, 2019). Tuy nhiên, vẫn còn một khoảng trống nghiên cứu trong lĩnh vực tối ưu hóa siêu tham số được thiết kế đặc biệt cho việc điều chỉnh hướng dẫn LLM. Để giải quyết vấn đề này,
∗Đóng góp ngang nhau. Yidong đã thực hiện công việc này trong thời gian thực tập tại Đại học Westlake.
†Liên hệ wye@pku.edu.cn; zhangsk@pku.edu.cn; zhangyue@westlake.edu.cn.
1arXiv:2306.05087v2 [cs.CL] 24 tháng 5 năm 2024

--- TRANG 2 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
chúng tôi nhắm đến việc xây dựng một phương pháp đánh giá tự động, đáng tin cậy và mạnh mẽ, có thể được tích hợp vào bất kỳ LLM mã nguồn mở nào và được sử dụng làm cơ sở phán quyết cho việc tối ưu hóa siêu tham số.

Việc phát triển một phương pháp đánh giá như vậy đặt ra những thách thức riêng (Guo et al., 2023; Chang et al., 2023), bao gồm đảm bảo độ tin cậy đánh giá và bảo vệ quyền riêng tư. Trong bối cảnh của bài báo này, khi chúng tôi đề cập đến "quyền riêng tư", chúng tôi chủ yếu ám chỉ các nguyên tắc được khắc sâu trong học liên bang (Zhang et al., 2021a) cho phép huấn luyện mô hình trên nhiều thiết bị hoặc máy chủ trong khi giữ dữ liệu được bản địa hóa, do đó cung cấp một mức độ quyền riêng tư. Các phương pháp hiện tại thường liên quan đến việc sử dụng công việc cộng đồng hoặc sử dụng API, có thể tốn kém và mất thời gian. Bên cạnh đó, các phương pháp này đối mặt với thách thức về tính nhất quán và khả năng tái tạo. Điều này chủ yếu do thiếu minh bạch về nhật ký thay đổi mô hình ngôn ngữ và tính chủ quan cố có của chú thích con người. Lưu ý rằng việc sử dụng các đánh giá dựa trên API mang rủi ro về chi phí cao tiềm ẩn liên quan đến việc giải quyết rò rỉ dữ liệu. Mặc dù các LLM mã nguồn mở có thể là những người đánh giá thay thế, chúng không được thiết kế đặc biệt để đánh giá, do đó khiến việc triển khai chúng trực tiếp như những người đánh giá trở nên khó khăn.

Mặt khác, các nhãn của các phương pháp đánh giá trước đây (Zheng et al., 2023; Gao et al., 2021; Wang et al., 2023b;c) đơn giản chỉ là câu trả lời xác định và không xem xét sự phức tạp của ngôn ngữ trong thực tế. Các chỉ số đánh giá của các quy trình này thường là độ chính xác và điểm F1, mà không xem xét các chỉ số đánh giá chủ quan mà các mô hình ngôn ngữ sinh tự hồi quy nên chú ý, do đó không phản ánh tiềm năng của các mô hình như vậy để tạo ra văn bản liên quan đến ngữ cảnh. Các chỉ số đánh giá chủ quan thích hợp có thể là tính súc tích tương đối, sự rõ ràng, tuân thủ hướng dẫn, tính toàn diện, tính trang trọng và liên quan đến ngữ cảnh.

Để giải quyết những thách thức này, chúng tôi giới thiệu một mô hình ngôn ngữ thẩm phán, nhằm đánh giá mô hình ngôn ngữ tự động và có thể tái tạo (PandaLM). Được điều chỉnh từ LLaMA, PandaLM được sử dụng để phân biệt mô hình vượt trội nhất trong số các ứng viên khác nhau, mỗi cái được tinh chỉnh với các siêu tham số khác nhau, và cũng có khả năng cung cấp lý do đằng sau lựa chọn của nó dựa trên phản hồi tham chiếu cho ngữ cảnh. PandaLM vượt qua những hạn chế của các phương pháp đánh giá truyền thống và tập trung vào các khía cạnh chủ quan hơn, như tính súc tích tương đối, sự rõ ràng, tính toàn diện, tính trang trọng và tuân thủ hướng dẫn. Hơn nữa, độ mạnh mẽ của PandaLM được tăng cường bởi khả năng xác định và sửa chữa các vấn đề như lỗi logic, lặp lại không cần thiết, sai sót ngữ pháp và không liên quan đến ngữ cảnh. Bằng cách xem xét những khía cạnh đa dạng này, chúng tôi tận dụng khả năng của PandaLM để phân biệt mô hình vượt trội nhất trong số các ứng viên trên tập xác thực và sau đó cung cấp thông tin chi tiết để tạo điều kiện thuận lợi cho việc tối ưu hóa siêu tham số của điều chỉnh hướng dẫn.

Trong thực tế, chúng tôi tạo ra các phản hồi ghép đôi từ một tập hợp đa dạng các mô hình nền tảng có kích thước tương tự bao gồm LLaMA-7B (Touvron et al., 2023), Bloom-7B (Scao et al., 2022), Cerebras-GPT-6.7B (Dey et al., 2023), OPT-7B (Zhang et al., 2022a), và Pythia-6.9B (Biderman et al., 2023). Mỗi mô hình này được tinh chỉnh bằng cách sử dụng cùng dữ liệu và siêu tham số như Alpaca (Taori et al., 2023). Các phản hồi ghép đôi từ các LLM được điều chỉnh này tạo thành đầu vào của dữ liệu huấn luyện cho PandaLM. Cách tiếp cận đơn giản nhất để tạo ra mục tiêu tương ứng của dữ liệu huấn luyện là thông qua chú thích con người, nhưng phương pháp này có thể tốn kém và mất thời gian (Wang et al., 2023h). Và việc thiếu dữ liệu huấn luyện được chú thích đầy đủ luôn là một vấn đề đáng kể trong thời đại học sâu Ouali et al. (2020); Wang et al. (2023e); Zhang et al. (2021b); Chen et al. (2023); Wang et al. (2022a). Xem xét rằng GPT-3.5 có thể cung cấp đánh giá đáng tin cậy ở một mức độ nào đó, để giảm chi phí, chúng tôi theo dõi self-instruct (Wang et al., 2022c), đây là một phương pháp tận dụng kiến thức có sẵn trong các mô hình ngôn ngữ lớn để tạo ra chú thích hoặc đầu ra thông qua các hướng dẫn tự tạo. để chưng cất dữ liệu từ GPT-3.5 và áp dụng các chiến lược lọc dữ liệu heuristic để giảm thiểu nhiễu. Cụ thể, chúng tôi lọc bỏ các đánh giá không hợp lệ từ gpt-3.5-turbo với các quy tắc thủ công. Để giải quyết thiên vị vị trí, chúng tôi cũng lọc bỏ các mẫu không nhất quán khi hoán đổi thứ tự của các phản hồi trong prompt. Mặc dù sử dụng dữ liệu được chưng cất từ GPT-3.5, việc loại bỏ tích cực nhiễu nâng cao chất lượng của dữ liệu huấn luyện, thúc đẩy quá trình huấn luyện hiệu quả và mạnh mẽ hơn cho PandaLM.

Để đảm bảo độ tin cậy của PandaLM, chúng tôi phát triển một tập dữ liệu kiểm tra phù hợp với sở thích con người và bao gồm một loạt các nhiệm vụ và ngữ cảnh. Các hướng dẫn và đầu vào của dữ liệu kiểm tra được lấy mẫu từ tập dữ liệu đánh giá con người của self-instruct (Wang et al., 2022c), với các phản hồi được tạo ra bởi các LLM khác nhau và mỗi nhãn được cung cấp độc lập bởi ba người đánh giá con người khác nhau. Các mẫu có sự khác biệt đáng kể được loại trừ để đảm bảo Thỏa thuận giữa các người chú thích (IAA) của mỗi người chú thích vẫn lớn hơn 0.85. Như được minh họa trong Bảng 2, PandaLM-7B thể hiện hiệu suất mạnh mẽ và cạnh tranh. Đáng chú ý là hiệu quả của PandaLM-70B thậm chí còn rõ rệt hơn,

--- TRANG 3 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
llama bloom cerebras opt pythia0102030405060Số lượngThắng
Thua
(a) Kết quả so sánh của GPT-3.5.
llama bloom cerebras opt pythia0102030405060Số lượngThắng
Thua (b) Kết quả so sánh của GPT-4.
llama bloom cerebras opt pythia020406080Số lượngThắng
Thua (c) Kết quả so sánh của Con người.
Hình 1: Các mô hình được đánh giá và so sánh bằng cách sử dụng cả GPT-3.5, GPT-4 và người chú thích con người. Số lượng 'Thắng' đại diện cho số lượng phản hồi mà các mô hình được tinh chỉnh với siêu tham số tối ưu được PandaLM chọn vượt trội hơn các mô hình sử dụng siêu tham số của Alpaca. Ngược lại, số lượng 'Thua' đại diện cho số lượng phản hồi mà các mô hình sử dụng siêu tham số của Alpaca tạo ra phản hồi vượt trội so với những mô hình được tinh chỉnh với siêu tham số tối ưu được xác định bởi PandaLM. Lưu ý rằng tập kiểm tra tổng thể bao gồm 170 trường hợp, và các tình huống 'Hòa' không được xem xét trong minh họa này.

vượt quá các chỉ số hiệu suất của GPT-4. Sự cải tiến này chủ yếu có thể được quy cho các chiến lược giảm thiểu nhiễu hiệu quả được sử dụng trong giai đoạn huấn luyện.

Hơn nữa, như được minh họa trong Hình 1, việc áp dụng các siêu tham số tối ưu được PandaLM chọn bao gồm lựa chọn bộ tối ưu hóa, tốc độ học, số lượng epoch huấn luyện và bộ lập lịch tốc độ học mang lại những cải tiến đáng chú ý. Khi được đánh giá bằng GPT-4 với một bộ gồm 170 hướng dẫn, một nhóm năm mô hình ngôn ngữ mở, được điều chỉnh với siêu tham số tối ưu được PandaLM chọn, đạt được trung bình 47.0 phản hồi vượt trội và 26.2 phản hồi kém hơn, vượt trội hơn những mô hình được huấn luyện bằng siêu tham số của Alpaca. Lưu ý rằng dữ liệu huấn luyện vẫn giữ nguyên để thực hiện so sánh công bằng. Hơn nữa, khi các LLM này được đánh giá bởi các chuyên gia con người, sử dụng cùng bộ 170 hướng dẫn, chúng thể hiện trung bình 79.8 phản hồi vượt trội và 25.2 phản hồi kém hơn, một lần nữa vượt trội hơn hiệu suất của các mô hình được huấn luyện với siêu tham số của Alpaca. Kết quả thực nghiệm nhấn mạnh hiệu quả của PandaLM trong việc xác định siêu tham số tối ưu để chọn ra những LLM tốt nhất. Ngoài ra, khi các LLM được tinh chỉnh được đánh giá bằng lm-eval (Gao et al., 2021), một khung thống nhất để kiểm tra LLM trên nhiều nhiệm vụ đánh giá truyền thống khác nhau, kết quả càng củng cố tính ưu việt của các LLM được tối ưu hóa bởi PandaLM.

Tóm lại, công việc của chúng tôi đem lại ba đóng góp chính:
• Chúng tôi giới thiệu PandaLM, một mô hình ngôn ngữ thẩm phán bảo vệ quyền riêng tư để đánh giá và tối ưu hóa siêu tham số cho các LLM.
• Chúng tôi tạo ra một tập dữ liệu đáng tin cậy được chú thích bởi con người, cần thiết để xác thực hiệu suất của PandaLM và nghiên cứu tiếp theo.
• Chúng tôi sử dụng PandaLM để tối ưu hóa siêu tham số của một loạt các LLM mã nguồn mở. So với những LLM được điều chỉnh sử dụng siêu tham số được xác định bởi Alpaca, việc điều chỉnh mô hình với siêu tham số được PandaLM chọn mang lại những cải tiến hiệu suất đáng kể.

2 NGHIÊN CỨU LIÊN QUAN
Phần này đánh giá tài liệu liên quan về chủ đề tối ưu hóa siêu tham số và đánh giá các mô hình ngôn ngữ.

Tối ưu hóa siêu tham số Tầm quan trọng của tối ưu hóa siêu tham số trong học máy (Yu & Zhu, 2020; Falkner et al., 2018; Li et al., 2017; Xu et al., 2023; Wang et al., 2023a; Wu et al., 2019), đặc biệt trong bối cảnh tinh chỉnh các mô hình ngôn ngữ học sâu như BERT (Kenton & Toutanova, 2019) và GPT (Radford et al.), không thể bỏ qua. Đối với các mô hình này, việc lựa chọn siêu tham số như tốc độ học, kích thước batch, hoặc số lượng epoch huấn luyện có thể ảnh hưởng đáng kể đến hiệu suất của chúng (Godbole et al., 2023; Sun et al., 2019; Tunstall et al., 2022). Quá trình lựa chọn này trở nên quan trọng hơn nữa khi tinh chỉnh các mô hình này cho các nhiệm vụ cụ thể theo miền,

--- TRANG 4 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
Mô hình nền tảng LLaMA
Mô hình được điều chỉnh hướng dẫn
Đánh giá
Dựa trên API Rò rỉ dữ liệu Giới hạn truy cập Không thể tái tạo
Con người Mất thời gian Đắt đỏ Không nhất quán
PandaLM Có thể tái tạo Mã nguồn mở An toàn & Hiệu quả
Vicuna Alpaca
BELLE
BLOOM
Chi phí huấn luyện: Ctrain
Chi phí đánh giá: Ceval
Lần lặp thứ nhất của quy trình điều chỉnh hướng dẫn
Lần lặp thứ hai của quy trình điều chỉnh hướng dẫn
...
Lần lặp thứ N của quy trình điều chỉnh hướng dẫn

Hình 2: Quy trình điều chỉnh hướng dẫn LLM.

trong đó tập hợp siêu tham số tối ưu có thể khác nhau đáng kể giữa các miền khác nhau (Dodge et al., 2020; Sun et al., 2019).

Đánh giá mô hình ngôn ngữ Đánh giá chính xác các mô hình ngôn ngữ là rất quan trọng trong việc xác định siêu tham số tối ưu, do đó cải thiện hiệu suất tổng thể của mô hình (Sun et al., 2019; Godbole et al., 2023). Các chỉ số khách quan thông thường như perplexity (Mallio et al., 2023) và accuracy (Xu et al., 2020; Wang et al.; Yang et al., 2022a; Zhong et al., 2023) trên các nhiệm vụ downstream (Gao et al., 2021) cung cấp những thông tin chi tiết có giá trị, nhưng chúng có thể không hướng dẫn hiệu quả việc lựa chọn siêu tham số để nâng cao LLM (Rogers et al., 2021) vì việc đánh giá LLM đòi hỏi các chỉ số chủ quan khác.

Các mô hình ngôn ngữ tiên tiến như GPT-4 (OpenAI, 2023) và Bard (Google, 2023) kết hợp đánh giá con người như một phần của phương pháp kiểm tra LLM, nhắm đến việc phù hợp tốt hơn với phán đoán của con người (Wang et al., 2023h). Mặc dù các phương pháp đánh giá dựa trên con người cung cấp thông tin chi tiết đáng kể về hiệu suất của mô hình, chúng tốn kém và tốn nhiều lao động, khiến chúng ít khả thi hơn cho các quy trình tối ưu hóa siêu tham số lặp đi lặp lại. Những tiến bộ gần đây trong NLP đã mang lại các chỉ số dựa trên mô hình như BERTScore (Zhang et al., 2019) và MAUVE (Pillutla et al., 2021). Trong khi các chỉ số này cung cấp thông tin chi tiết có giá trị, có những lĩnh vực quan trọng mà chúng có thể không phù hợp hoàn hảo với mục tiêu của việc đánh giá phản hồi. Thứ nhất, BERTScore và MAUVE được thiết kế để đo lường sự tương tự giữa nội dung được tạo ra và văn bản tham chiếu. Tuy nhiên, chúng không được thiết kế cố hữu để phân biệt phản hồi nào trong số nhiều phản hồi là vượt trội hơn. Một phản hồi gần với tham chiếu được viết bởi con người không nhất thiết có nghĩa là nó tuân thủ tốt hơn các hướng dẫn đã cho hoặc thỏa mãn một ngữ cảnh cụ thể. Thứ hai, trong khi các chỉ số này cho ra điểm số đại diện cho sự tương tự nội dung, chúng không phải lúc nào cũng trực quan với người dùng. Việc diễn giải những điểm số này và dịch chúng thành phản hồi có thể thực hiện được có thể là một thách thức. Ngược lại, PandaLM cung cấp một cách tiếp cận đơn giản hơn. Nó được thiết kế riêng để đưa ra trực tiếp kết quả đánh giá một cách có thể diễn giải được, làm cho quá trình phản hồi trở nên minh bạch và dễ hiểu với con người. Tóm lại, trong khi các chỉ số như BERTScore và MAUVE cung cấp thông tin chi tiết có giá trị về sự tương tự nội dung, có một nhu cầu cấp thiết cho các công cụ đánh giá chuyên biệt như PandaLM. Các công cụ không chỉ phân biệt chất lượng phản hồi mà còn làm điều đó một cách thân thiện với người dùng, có thể hiểu được bởi con người.

Phân tích định tính chủ quan về đầu ra của mô hình, chẳng hạn như khả năng xử lý các hướng dẫn mơ hồ và cung cấp phản hồi phù hợp với ngữ cảnh, ngày càng được công nhận là một chỉ số có giá trị để đánh giá mô hình (Zheng et al., 2023). Việc tối ưu hóa siêu tham số với những xem xét đối với các biện pháp định tính này có thể dẫn đến các mô hình hoạt động mạnh mẽ hơn trong các tình huống thực tế đa dạng. Phân tích định tính trước đây có thể đạt được thông qua người đánh giá con người hoặc thông qua API của các mô hình ngôn ngữ tiên tiến, khác với động cơ của chúng tôi.

3 PHƯƠNG PHÁP LUẬN

Như được thể hiện trong Hình 2, quá trình điều chỉnh hướng dẫn bắt đầu với một mô hình nền tảng, sau đó được tinh chỉnh bằng các hướng dẫn. Hiệu suất của mỗi mô hình được điều chỉnh được đánh giá để xác định đầu ra tốt nhất. Điều này liên quan đến việc khám phá nhiều mô hình, mỗi mô hình được điều chỉnh với các siêu tham số khác nhau, để xác định mô hình tối ưu. Để tạo điều kiện thuận lợi cho quy trình này, một hệ thống đánh giá mô hình ngôn ngữ đáng tin cậy và tự động là cần thiết. Để giải quyết điều này, chúng tôi giới thiệu PandaLM - một LLM thẩm phán được thiết kế đặc biệt để đánh giá hiệu suất của các LLM được tinh chỉnh với các tham số khác nhau. Mục tiêu của chúng tôi là xác định mô hình vượt trội từ một nhóm ứng viên một cách chính xác.

--- TRANG 5 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
đúng súc tích rõ ràng giải thích chính xác chi tiết theo hướng dẫn độc đáo cụ thể toàn diện trang trọng lỗi logic lặp lại thiếu cụ thể lỗi ngữ pháp ngữ cảnh050001000015000200002500030000Số lượng28776
23939
17749
16113
14189
10528
9504
7444 7063
60255436
31252599 2218 2182 2062

Hình 3: Top 16 từ được sử dụng trong lý do đánh giá PandaLM-7B từ 80k đầu ra đánh giá được lấy mẫu ngẫu nhiên. Một ví dụ về lý do đánh giá và đầu ra đánh giá có thể được tìm thấy trong Hình 5. Các từ dừng đã được lọc.

3.1 THU THẬP VÀ TIỀN XỬ LÝ DỮ LIỆU HUẤN LUYỆN

Việc thu thập dữ liệu huấn luyện nhằm tạo ra một tập dữ liệu phong phú cho phép mô hình đánh giá các phản hồi khác nhau trong một ngữ cảnh nhất định và tạo ra lý do đánh giá cũng như phản hồi tham chiếu sử dụng cùng ngữ cảnh. Như được thể hiện trong Phụ lục A, mỗi thể hiện dữ liệu huấn luyện bao gồm một bộ đầu vào (instruction, input, response1, response2) và một bộ đầu ra (evaluation result, evaluation reason, reference response). Các hướng dẫn và đầu vào trong bộ đầu vào được lấy mẫu từ tập dữ liệu Alpaca 52K (Taori et al., 2023). Các cặp phản hồi được tạo ra bởi nhiều mô hình được điều chỉnh hướng dẫn: LLaMA-7B (Touvron et al., 2023), Bloom-7B (Scao et al., 2022), Cerebras-GPT-6.7B (Dey et al., 2023), OPT-7B (Zhang et al., 2022a), và Pythia-6.9B (Biderman et al., 2023). Các mô hình này được chọn do kích thước tương đương và tính có sẵn công khai của trọng số mô hình. Mỗi mô hình được tinh chỉnh bằng cách sử dụng cùng dữ liệu hướng dẫn và siêu tham số theo Alpaca (Taori et al., 2023).

Bộ đầu ra tương ứng bao gồm kết quả đánh giá, giải thích ngắn gọn cho đánh giá, và phản hồi tham chiếu. Kết quả đánh giá sẽ là '1' hoặc '2', cho biết phản hồi 1 hoặc phản hồi 2 tốt hơn, và 'Tie' cho biết hai phản hồi có chất lượng tương tự. Prompt huấn luyện của PandaLM được thể hiện tại Phụ lục A. Vì không thể lấy hàng triệu bộ đầu ra từ người chú thích con người, và với việc GPT-3.5 có khả năng đánh giá LLM ở một mức độ nào đó, chúng tôi theo dõi self-instruct (Wang et al., 2022c) để tạo ra bộ đầu ra sử dụng GPT-3.5. Như được minh họa trong Hình 3, chúng tôi thiết kế prompt cẩn thận để hướng dẫn việc tạo ra dữ liệu huấn luyện cho PandaLM. Mục tiêu là đảm bảo PandaLM không chỉ ưu tiên tính đúng đắn khách quan của phản hồi mà còn nhấn mạnh các khía cạnh chủ quan quan trọng như tính súc tích tương đối, sự rõ ràng, tính toàn diện, tính trang trọng và tuân thủ hướng dẫn. Bên cạnh đó, chúng tôi khuyến khích PandaLM xác định và sửa chữa các vấn đề như lỗi logic, lặp lại không cần thiết, sai sót ngữ pháp và thiếu liên quan đến ngữ cảnh. Sau đó, một chiến lược lọc dữ liệu heuristic được áp dụng để loại bỏ dữ liệu nhiễu. Cụ thể, để giải quyết thiên vị cố hữu được quan sát trong GPT-3.5 liên quan đến thứ tự của các phản hồi đầu vào ngay cả với các prompt được thiết kế cẩn thận, các mẫu từ tập dữ liệu huấn luyện được loại bỏ nếu kết quả đánh giá của chúng mâu thuẫn khi thứ tự của các phản hồi đầu vào được hoán đổi. Cuối cùng chúng tôi thu được một tập dữ liệu được lọc chứa 300K mẫu.

3.2 HUẤN LUYỆN PANDALM

Trong phần này, chúng tôi cung cấp chi tiết về quy trình huấn luyện cho PandaLM. Xương sống của PandaLM là mô hình LLaMA, vì nó thể hiện hiệu suất mạnh mẽ trên nhiều nhiệm vụ NLP phức tạp (Beeching et al., 2023).

Trong giai đoạn tinh chỉnh của PandaLM, chúng tôi sử dụng hàm mất mát cross-entropy tiêu chuẩn nhắm vào việc dự đoán token tiếp theo. Mô hình hoạt động theo mô hình sequence-to-sequence mà không cần một đầu phân loại riêng biệt. Chúng tôi huấn luyện PandaLM với thư viện DeepSpeed (Rasley et al., 2020), và Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020; Ren et al., 2021) Stage 2, trên 8 GPU NVIDIA A100-SXM4-80GB. Chúng tôi sử dụng tùy chọn độ chính xác tính toán bfloat16 (BF16) để tối ưu hóa thêm tốc độ và hiệu quả của mô hình. Về các siêu tham số huấn luyện, chúng tôi áp dụng bộ tối ưu hóa AdamW (Loshchilov & Hutter, 2017) với tốc độ học 2e-5 và bộ lập lịch tốc độ học cosine. Mô hình được huấn luyện trong 2 epoch. Quá trình huấn luyện sử dụng tỷ lệ khởi động 0.03 để

--- TRANG 6 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
LLaMA
BLOOM40
OPT41
Cerebras_GPT58 Pythia33
6
21
1421
(a) DAG của GPT-3.5.
LLaMA
BLOOM43
OPT44
Cerebras_GPT60 Pythia35
31
1712
24(b) DAG của GPT-4.
LLaMA
BLOOM44
OPT47
Cerebras_GPT56 Pythia31
8
29
1621
26(c) DAG của con người.
LLaMA
BLOOM17
OPT26
Cerebras_GPT50 Pythia7
6
32
1122(d) DAG của PandaLM.

Hình 4: Trực quan hóa so sánh hiệu suất mô hình. Các mô hình được điều chỉnh hướng dẫn sử dụng cùng dữ liệu huấn luyện và siêu tham số. Một cạnh có hướng từ nút A đến B cho biết mô hình A vượt trội đáng kể so với B, trong khi một cạnh không hướng đứt nét cho biết hai mô hình có hiệu suất tương tự. Số liên quan đến cạnh có hướng (A, B) đại diện cho sự khác biệt giữa số lần thắng và thua của mô hình A so với mô hình B. Việc không có số trên cạnh không hướng đứt nét cho biết sự khác biệt giữa số lần thắng và thua cho các mô hình nhỏ hơn 5. Chúng tôi hoán đổi thứ tự của hai phản hồi để thực hiện suy luận hai lần trên mỗi dữ liệu. Các kết quả đánh giá mâu thuẫn sau đó được sửa đổi thành 'Tie'.

tránh gradient lớn ở đầu quá trình huấn luyện. Chúng tôi sử dụng kích thước batch là 2 trên mỗi GPU với tất cả đầu vào được cắt ngắn đến tối đa 1024 token và sử dụng chiến lược tích lũy gradient với 8 bước.

4 ĐÁNH GIÁ ĐỘ TIN CẬY CỦA PANDALM

Để đảm bảo độ tin cậy của PandaLM, chúng tôi tạo ra một tập dữ liệu kiểm tra được gắn nhãn bởi con người và được thiết kế để phù hợp với sở thích của con người đối với các phản hồi. Mỗi thể hiện của tập dữ liệu kiểm tra này bao gồm một hướng dẫn và đầu vào, và hai phản hồi được tạo ra bởi các LLM được điều chỉnh hướng dẫn khác nhau. Các phản hồi ghép đôi được cung cấp bởi LLaMA-7B, Bloom-7B, Cerebras-GPT-6.7B, OPT-7B, và Pythia-6.9B, tất cả đều được điều chỉnh hướng dẫn bằng cách sử dụng cùng dữ liệu hướng dẫn và siêu tham số theo Alpaca (Taori et al., 2023). Dữ liệu kiểm tra được lấy mẫu từ tập dữ liệu đánh giá con người đa dạng của self-instruct (Wang et al., 2022c), bao gồm dữ liệu từ Grammarly, Wikipedia, National Geographic và gần một trăm ứng dụng hoặc trang web. Các đầu vào và nhãn hoàn toàn do con người tạo ra và bao gồm một loạt các nhiệm vụ và nội dung. Ba người đánh giá con người khác nhau độc lập chú thích các nhãn chỉ ra phản hồi được ưa thích. Các mẫu có sự khác biệt đáng kể được loại trừ để đảm bảo Thỏa thuận giữa các người chú thích (IAA) của mỗi người chú thích vẫn lớn hơn 0.85. Điều này là bởi vì các mẫu như vậy đòi hỏi kiến thức bổ sung hoặc thông tin khó có được, khiến chúng trở nên thách thức cho con người đánh giá. Tập dữ liệu kiểm tra được lọc chứa 1K mẫu, trong khi tập dữ liệu gốc chưa được lọc có 2.5K mẫu.

Để duy trì công việc cộng đồng chất lượng cao, chúng tôi liên quan đến ba chuyên gia để chú thích cùng một điểm dữ liệu đồng thời trong quá trình chú thích. Không có mối quan hệ trước đây giữa các chuyên gia và tác giả. Các chuyên gia được thuê từ một công ty chú thích. Những chuyên gia này nhận được đào tạo chuyên biệt vượt ra ngoài việc đánh giá tính đúng đắn của phản hồi, cho phép họ nhấn mạnh các khía cạnh quan trọng khác như tính súc tích tương đối, sự rõ ràng, tính toàn diện, tính trang trọng và tuân thủ hướng dẫn. Hơn nữa, chúng tôi hướng dẫn những người chú thích này trong việc xác định và giải quyết các vấn đề như lỗi logic, lặp lại không cần thiết, sai sót ngữ pháp và thiếu liên quan đến ngữ cảnh. Tất cả đánh giá của con người được thu thập nhất quán trong cùng một phiên. Để đảm bảo sự rõ ràng và nhất quán, chúng tôi cung cấp hướng dẫn toàn diện cho mỗi người chú thích. Sau giai đoạn thử nghiệm của chú thích dữ liệu, chúng tôi loại bỏ một số dữ liệu được gắn nhãn chất lượng thấp. IAA cuối cùng giữa ba người chú thích, được đo bằng Cohen's Kappa (Cohen, 1960), đạt điểm số trung bình lần lượt là 0.85, 0.86, và 0.88, cho thấy mức độ tin cậy tương đối cao cho tập dữ liệu kiểm tra của chúng tôi. Để tinh chỉnh đánh giá hiệu suất của mô hình so với người đánh giá con người, chúng tôi có thể sử dụng thỏa thuận giữa các người chú thích (IAA) 0.85 làm điểm chuẩn. Nếu mô hình của chúng tôi vượt quá điều này, nó cho thấy hiệu suất mạnh mẽ. Tuy nhiên, việc đặt một mục tiêu thực tế hơi cao hơn IAA con người này, khoảng 0.90, cung cấp một mục tiêu đầy thách thức nhưng có thể đạt được. Phân phối của dữ liệu kiểm tra bao gồm 105 trường hợp hòa, 422 trường hợp Phản hồi 1 thắng, và 472 trường hợp Phản hồi 2 dẫn đầu. Lưu ý rằng tập dữ liệu được tạo ra bởi con người không có

--- TRANG 7 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
Bảng 1: Phân tích so sánh kết quả đánh giá từ nhiều mô hình chú thích khác nhau. Bộ ba trong bảng có nghĩa là (#thắng,#thua,#hòa). Cụ thể, (72,28,11) ở dòng đầu tiên của bảng cho biết rằng LLaMA-7B vượt trội hơn Bloom-7B trong 72 phản hồi, kém hơn trong 28, và chất lượng tương đương trong 11 phản hồi. Cột 'Judged By' đại diện cho các phương pháp đánh giá phản hồi khác nhau. 'Human' cho biết con người đánh giá kết quả, và 'PandaLM' cho biết mô hình PandaLM được đề xuất của chúng tôi đánh giá kết quả.

Judged By Base Model LLaMA-7B Bloom-7B Cerebras-6.7B OPT-7B Pythia-6.9B
Human LLaMA-7B / (72,28,11) (80,24,6) (71,24,11) (58,27,9)
Bloom-7B (28,72,11) / (59,30,11) (43,35,11) (47,49,11)
Cerebras-6.7B (24,80,6) (30,59,11) / (33,49,9) (27,53,11)
OPT-7B (24,71,11) (35,43,11) (49,33,9) / (32,53,15)
Pythia-6.9B (27,58,9) (49,47,11) (53,27,11) (53,32,15) /
GPT-3.5 LLaMA-7B / (59,19,33) (71,13,26) (58,17,31) (49,16,29)
Bloom-7B (19,59,33) / (40,19,41) (36,30,23) (33,34,40)
Cerebras-6.7B (13,71,26) (19,40,41) / (24,38,29) (22,43,26)
OPT-7B (17,58,31) (30,36,23) (38,24,29) / (30,30,40)
Pythia-6.9B (16,49,29) (34,33,40) (43,22,26) (30,30,40) /
GPT-4 LLaMA-7B / (58,15,38) (69,9,32) (58,14,34) (52,17,25)
Bloom-7B (15,58,38) / (47,16,37) (35,31,23) (32,33,42)
Cerebras-6.7B (9,69,32) (16,47,37) / (23,40,28) (17,41,33)
OPT-7B (14,58,34) (31,35,23) (40,23,28) / (25,37,38)
Pythia-6.9B (17,52,25) (33,32,42) (41,17,33) (37,25,38) /
PandaLM-7B LLaMA-7B / (46,29,36) (68,18,24) (52,26,28) (35,28,31)
Bloom-7B (29,46,36) / (50,18,32) (36,30,23) (36,31,40)
Cerebras-6.7B (18,68,24) (18,50,32) / (28,39,24) (24,46,21)
OPT-7B (26,52,28) (30,36,23) (39,28,24) / (30,32,38)
Pythia-6.9B (28,35,31) (31,36,40) (46,24,21) (32,30,38) /

Bảng 2: So sánh giữa kết quả Chú thích Con người và kết quả đánh giá Mô hình Thẩm phán.
Judged Model Accuracy Precision Recall F1
GPT-3.5 0.6296 0.6195 0.6359 0.5820
GPT-4 0.6647 0.6620 0.6815 0.6180
PandaLM-7B 0.5926 0.5728 0.5923 0.5456
PandaLM-70B-LoRA 0.6186 0.7757 0.6186 0.6654
PandaLM-70B 0.6687 0.7402 0.6687 0.6923

thông tin nhận dạng cá nhân hoặc nội dung xúc phạm, và tất cả người chú thích nhận được phí lao động dư thừa.

Sau khi thu được tập dữ liệu kiểm tra được gắn nhãn bởi con người, chúng tôi có thể đánh giá và so sánh hiệu suất đánh giá của GPT-3.5, GPT-4, và PandaLM. Một quan sát thú vị từ Bảng 1 là biểu đồ thứ tự một phần tương tự được chia sẻ giữa GPT-3.5, GPT-4, PandaLM-7B, và con người. Hơn nữa, Hình 4 minh họa các thứ tự có hướng của tính ưu việt mô hình (nếu mô hình A vượt trội hơn mô hình B, một cạnh có hướng từ A đến B được vẽ; nếu mô hình A và mô hình B hoạt động tương tự, một đường đứt nét từ A đến B được vẽ), và cung cấp một đại diện trực quan của hiệu quả mô hình so sánh. Kết quả thực nghiệm cho thấy sự tương tự trong sở thích của GPT-3.5, GPT-4, PandaLM-7B, và con người. Lưu ý rằng đối với PandaLM, GPT-3.5, và GPT-4, chúng tôi hoán đổi thứ tự phản hồi đầu vào và suy luận hai lần để có được đầu ra đánh giá cuối cùng. Các kết quả đánh giá mâu thuẫn được sửa đổi thành 'Tie'.

Như được thể hiện trong Bảng 2, chúng tôi tiến hành phân tích thống kê so sánh độ chính xác, độ chính xác, độ nhạy, và điểm F1 của GPT-3.5, GPT-4, và PandaLM so với chú thích con người. Hiệu suất của PandaLM-70B thậm chí còn vượt trội hơn GPT-4. Kết quả cho thấy hiệu quả của việc loại bỏ nhiễu trong dữ liệu huấn luyện và việc lựa chọn kiến trúc mô hình nền tảng và hướng dẫn.

Để chứng minh tính mạnh mẽ và khả năng thích ứng của PandaLM qua các sự thay đổi phân phối, chúng tôi cũng tập trung đánh giá của mình vào các lĩnh vực riêng biệt, với sự nhấn mạnh đặc biệt vào các miền pháp lý (LSAT) và sinh học (PubMedQA và BioASQ). Phần giới thiệu về các tập dữ liệu được sử dụng có thể được tìm thấy tại Phụ lục D. Lưu ý rằng để tạo ra phản hồi, chúng tôi sử dụng các mô hình ngôn ngữ mã nguồn mở như Vicuna và Alpaca. Do giới hạn thời gian, GPT-4 được áp dụng để tạo ra câu trả lời tiêu chuẩn vàng (thắng/hòa/thua của phản hồi) thay vì người chú thích con người. Kết quả được minh họa trong Bảng 3 nhấn mạnh khả năng của PandaLM không chỉ trong các ngữ cảnh chung mà còn trong các miền cụ thể như luật (thông qua LSAT) và sinh học (thông qua PubMedQA và BioASQ). Vì chúng tôi đang giải quyết một nhiệm vụ phân loại ba danh mục (thắng/thua/hòa), nơi một phỏng đoán ngẫu nhiên sẽ dẫn đến khoảng 33% trong điểm precision, recall, và F1. Kết quả của PandaLM-7B rõ ràng cao hơn mức này. Để xác thực thêm PandaLM-7B, chúng tôi đã tiến hành đánh giá con người với 30 mẫu trên đánh giá BioASQ. Đánh giá con người cho thấy rằng cả PandaLM-7B và GPT-4 đều có xu hướng ưa thích Vicuna hơn Alpaca, cho thấy một xu hướng nhất quán trong đánh giá của chúng. Sự cải thiện đáng kể về hiệu suất khi mô hình mở rộng quy mô khẳng định lại tiềm năng của PandaLM qua các ứng dụng đa dạng, nhấn mạnh thêm độ tin cậy của nó giữa các phân phối nội dung khác nhau.

Chúng tôi tiếp tục nghiên cứu hiệu suất của PandaLM bằng cách đối chiếu hiệu quả của nó khi chỉ được huấn luyện trên so sánh số (thắng/hòa/thua) — tương tự như một mô hình phần thưởng truyền thống — với cách tiếp cận toàn diện

--- TRANG 8 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
Bảng 3: Đánh giá hiệu suất của PandaLM qua các miền đa dạng. Bảng thể hiện độ chính xác, độ chính xác, độ nhạy, và điểm F1 đạt được bởi PandaLM của hai kích thước khác nhau (được tinh chỉnh từ LLaMA-7B và LLaMA2-70B) trên ba tập dữ liệu riêng biệt: LSAT, PubMedQA, và BioASQ. Các tập dữ liệu này đại diện cho các miền pháp lý và sinh học, được chọn để thể hiện tính mạnh mẽ và khả năng thích ứng của PandaLM với các sự thay đổi phân phối khác nhau. Đáng chú ý là GPT-4 được sử dụng để tạo ra câu trả lời tiêu chuẩn vàng thay vì chú thích con người.

Accuracy Precision Recall F1 Score
LSAT (PandaLM-7B) 0.4717 0.7289 0.4717 0.5345
LSAT (PandaLM-70B) 0.6604 0.7625 0.6604 0.6654
PubMedQA (PandaLM-7B) 0.6154 0.8736 0.6154 0.6972
PubMedQA (PandaLM-70B) 0.7692 0.7811 0.7692 0.7663
BioASQ (PandaLM-7B) 0.5152 0.7831 0.5152 0.5602
BioASQ (PandaLM-70B) 0.7727 0.8076 0.7727 0.7798

Bảng 4: So sánh hiệu suất PandaLM có và không có lý do và tham chiếu.
Accuracy Precision Recall F1 Score
PandaLM-7B chỉ với thắng/hòa/thua 0.4725 0.4505 0.4725 0.3152
PandaLM-7B 0.5926 0.5728 0.5923 0.5456

Bảng 5: Đánh giá hiệu quả của siêu tham số được PandaLM chọn và siêu tham số của Alpaca. Bộ ba trong bảng có nghĩa là (#thắng,#thua,#hòa). Cụ thể, (45,26,99) ở dòng đầu tiên của bảng cho biết rằng LLaMA-7B được điều chỉnh siêu tham số bởi PandaLM vượt trội hơn phiên bản Alpaca trong 45 phản hồi, kém hơn trong 26, và chất lượng tương đương trong 99 trường hợp. Cột 'Judged By' đại diện cho các phương pháp đánh giá phản hồi khác nhau.

Judge Model LLaMA-7B Bloom-7B Cerebras-6.7B OPT-7B Pythia-6.9B
GPT-3.5 (45,26,99) (48,24,98) (58,21,91) (48,34,88) (59,20,91)
GPT-4 (40,17,113) (44,34,92) (60,20,90) (39,30,101) (52,30,88)
Human (82,21,67) (79,23,68) (88,25,57) (68,26,76) (82,31,57)

của PandaLM tiêu chuẩn kết hợp lý do đánh giá và phản hồi tham chiếu. Như được thể hiện trong Bảng 4, rõ ràng là lý do đánh giá và phản hồi tham chiếu hỗ trợ đáng kể LLM trong việc hiểu các nhiệm vụ đánh giá. Lưu ý rằng trong Phụ lục G, kết quả thể hiện rõ ràng rằng một mô hình nhỏ hơn, khi được điều chỉnh chính xác, có khả năng vượt trội hơn một mô hình lớn hơn, chưa được điều chỉnh trong các chỉ số đánh giá. Phát hiện này nhấn mạnh tác động đáng kể của việc điều chỉnh có mục tiêu đối với hiệu suất mô hình trong các tình huống đánh giá. Chúng tôi cũng cung cấp phân tích về PandaLM qua các sự thay đổi mô hình trong Phụ lục K.

Ngoài ra, ngoài các chỉ số hiệu suất, PandaLM giới thiệu những lợi thế độc đáo không có trong các mô hình như GPT-3.5 và GPT-4. Nó cung cấp khả năng mã nguồn mở, cho phép tái tạo, và bảo vệ quyền riêng tư dữ liệu. Hơn nữa, nó cung cấp truy cập không giới hạn, loại bỏ mọi hạn chế có thể cản trở đánh giá toàn diện và ứng dụng.

5 SỬ DỤNG PANDALM ĐỂ ĐIỀU CHỈNH HƯỚNG DẪN LLM

Để làm nổi bật hiệu quả của việc sử dụng PandaLM để điều chỉnh hướng dẫn LLM, chúng tôi so sánh hiệu suất của các mô hình được điều chỉnh với siêu tham số tối ưu được PandaLM chọn so với những mô hình được điều chỉnh với tham số của Alpaca bằng cách sử dụng GPT-3.5, GPT-4, và chuyên gia con người. Đáng chú ý là PandaLM-7B được sử dụng cho so sánh này do cân nhắc về tài nguyên tính toán. Với hiệu quả đã được chứng minh của PandaLM-7B, có một kỳ vọng có cơ sở rằng hiệu suất của PandaLM-70B sẽ thể hiện cải tiến thêm. So sánh này đánh giá nhiều LLM được điều chỉnh: LLaMA-7B, Bloom-7B, Cerebras-GPT-6.7B, OPT-7B, và Pythia-6.9B. Đánh giá được thực hiện trên một tập xác thực bao gồm 170 hướng dẫn và đầu vào riêng biệt thu được từ tập kiểm tra 1K của chúng tôi được giới thiệu trong Phần 4. Quy trình điều chỉnh của Alpaca liên quan đến huấn luyện trong ba epoch với các checkpoint của lần lặp cuối cùng được sử dụng. Nó sử dụng bộ tối ưu hóa AdamW (Loshchilov & Hutter, 2017) với tốc độ học 2e-5 và bộ lập lịch tốc độ học cosine. Chúng tôi thực hiện một phạm vi siêu tham số rộng hơn để điều chỉnh LLM bằng PandaLM-7B. Cụ thể, chúng tôi khám phá các checkpoint từ mỗi epoch (từ epoch 1 đến epoch 5), bốn tốc độ học khác nhau (2e-6, 1e-5, 2e-5, 2e-4), hai loại bộ tối ưu hóa (SGD (Goodfellow et al., 2016) và AdamW), và hai bộ lập lịch tốc độ học (cosine và linear). Tổng cộng, điều này tạo ra một không gian cấu hình gồm 80 khả năng khác nhau cho mỗi mô hình.

--- TRANG 9 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024
Chúng tôi tìm kiếm siêu tham số tối ưu trong số 80 cấu hình. Chúng được chia thành bốn khối, mỗi khối chứa 20 cấu hình. Các so sánh tuần tự xác định cấu hình tốt nhất trong mỗi khối. Các cấu hình hàng đầu từ mỗi khối sau đó được so sánh để xác định cấu hình tốt nhất tổng thể. Chúng tôi lặp lại mỗi so sánh hai lần để đảm bảo độ mạnh mẽ và thực hiện tổng cộng 800 so sánh. Các kết quả đánh giá mâu thuẫn được sửa đổi thành 'Tie'. Những thông tin chính từ quá trình điều chỉnh của chúng tôi bao gồm: Bloom-7B hoạt động tốt nhất với SGD, tốc độ học 2e-5, và lịch cosine trong 5 epoch. Cerebras-GPT-6.7B cũng ưa thích SGD với cùng tốc độ học nhưng với lịch linear. LLaMA-7B thích AdamW, tốc độ học 1e-5, và lịch linear trong 4 epoch. OPT-6.7B đạt kết quả tốt nhất với AdamW, tốc độ học 2e-5, và bộ lập lịch linear trong 5 epoch. Pythia-6.9B thích SGD, tốc độ học 1e-5, lịch cosine, và 5 epoch. Điều này làm nổi bật tầm quan trọng của việc điều chỉnh siêu tham số tùy chỉnh cho các mô hình khác nhau để đạt được hiệu suất đỉnh cao.

Chúng tôi cũng cung cấp phân tích về kích thước dữ liệu, chất lượng và LoRA trong Phụ lục E và Phụ lục F.

Như được minh họa trong Bảng 5, đối với GPT-3.5, GPT-4, và con người, tất cả các mô hình cơ sở đều đạt được hiệu suất vượt trội khi được điều chỉnh với siêu tham số được PandaLM chọn so với siêu tham số của Alpaca. Lưu ý rằng quy trình chuyển đổi thứ tự phản hồi đầu vào, như được áp dụng cho PandaLM, cũng được thực hiện cho GPT-3.5 và GPT-4 để có được kết quả đánh giá mạnh mẽ hơn. Kết quả này không chỉ hỗ trợ tuyên bố rằng PandaLM-7B có thể nâng cao hiệu suất của mô hình mà còn làm nổi bật tiềm năng của nó để cải thiện thêm các mô hình ngôn ngữ lớn khác nhau. Bên cạnh đó, như được thể hiện trong Phụ lục B, dựa trên đánh giá của PandaLM, mô hình thể hiện hiệu suất vượt trội là LLaMA-PandaLM. Lưu ý rằng các đặc tính của mô hình nền tảng cơ sở có thể là một yếu tố quan trọng trong hiệu suất, như được chứng minh bởi các mô hình LLaMA đảm bảo hai vị trí hàng đầu. Mô hình xếp hạng được quan sát phù hợp chặt chẽ với xếp hạng mô hình cơ sở được trình bày trong Hình 4. Chúng tôi cũng cung cấp phân tích tối ưu hóa siêu tham số trong Phụ lục J.

Hơn nữa, Bảng 6 trong Phụ lục C so sánh các LLM được tinh chỉnh trên các nhiệm vụ truyền thống khác nhau với lm-eval (Gao et al., 2021). Thú vị là, trong khi hầu hết các mô hình ngôn ngữ thể hiện hiệu suất tăng cường với việc tinh chỉnh PandaLM, Cerebras lại trải qua sự suy giảm. Điều này nhấn mạnh giá trị của đánh giá chủ quan (thắng/hòa/thua của phản hồi), vì đánh giá từ con người, GPT-4, và GPT-3.5 đều cho thấy hiệu suất vượt trội cho Cerebras với PandaLM.

6 HẠN CHỂ

Mặc dù kết quả của nghiên cứu này rất khuyến khích, chúng tôi thảo luận về một số hạn chế ở đây. Thứ nhất, phạm vi siêu tham số được chọn được sử dụng trong công việc này dựa trên thực hành phổ biến và tài liệu trước đây, và do đó có thể không bao gồm các siêu tham số tối ưu tuyệt đối. Trong khi việc mở rộng phạm vi tìm kiếm chắc chắn sẽ tăng chi phí tính toán. Mặc dù dữ liệu cốt lõi, có nguồn gốc từ GPT-3.5, có thể không hoàn toàn tương hưởng với sở thích con người, điều cần thiết là nhận ra rằng hiệu quả của LLM không chỉ phụ thuộc vào dữ liệu huấn luyện mà còn vào kiến trúc mô hình nền tảng và hướng dẫn. Hiện tại, sự nhấn mạnh của chúng tôi chủ yếu vào đánh giá dựa trên kết quả, điều này thực sự tốn nhiều tài nguyên. Tuy nhiên, việc tích hợp dự đoán hành vi (ví dụ, sử dụng phân tích hợp lý Lu et al. (2022); Wang et al. (2023d; 2020); Yang et al. (2021; 2023a)) vào một khung đánh giá có thể cung cấp hiểu biết toàn diện hơn về hiệu suất LLM. Ví dụ, phân tích và đánh giá các đầu ra văn bản mở rộng của một LLM chưa được điều chỉnh có thể giúp dự đoán cách một phiên bản được điều chỉnh có thể hoạt động trong các tình huống khác nhau. Cách tiếp cận này có thể cung cấp một cách hiệu quả và sâu sắc hơn để cân bằng các đánh giá kết quả tốn nhiều tài nguyên. Bên cạnh đó, chúng tôi chỉ nghiên cứu về huấn luyện có giám sát của LLM, nhưng dữ liệu thực tế có thể bị mất cân bằng hoặc không được gắn nhãn, do đó huấn luyện bán giám sát Wang et al. (2023e); Chen et al. (2023); Wang et al. (2022b), nhiễu Zhang et al. (2022b); Chen et al. (2024) và mất cân bằng Yang et al. (2022b); Wang et al. (2023f;g) cũng là hướng tương lai của chúng tôi.

7 KẾT LUẬN

Trong việc khám phá tối ưu hóa siêu tham số, chúng tôi áp dụng PandaLM: một mô hình thẩm phán tự động và đáng tin cậy cho việc điều chỉnh LLM. Các phát hiện của chúng tôi cho thấy rằng việc sử dụng PandaLM là khả thi và liên tục tạo ra các mô hình có hiệu suất vượt trội so với những mô hình được điều chỉnh với các tham số mặc định của Alpaca. Chúng tôi cam kết tiếp tục nâng cao PandaLM bằng cách mở rộng khả năng của nó để hỗ trợ các mô hình lớn hơn và phân tích các đặc tính nội tại của nó, từ đó phát triển các phiên bản thẩm phán ngày càng mạnh mẽ hơn trong tương lai.

--- TRANG 10 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

LỜI CẢM ƠN

Chúng tôi xin cảm ơn các nhà đánh giá ẩn danh vì những nhận xét và đề xuất sâu sắc của họ để giúp cải thiện bài báo. Xuất bản này bắt nguồn từ nghiên cứu được thực hiện với sự hỗ trợ tài chính của Chương trình R&D Pioneer và "Leading Goose" của Chiết Giang dưới Số hiệu Grant 2022SDXHDX0003 và Chương trình Chủ chốt của Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc dưới Số hiệu Grant 62336006.

TÀI LIỆU THAM KHẢO

Donnie Adams và Kee-Man Chuah. Artificial intelligence-based tools in research writing: Current trends and future potentials. Artificial Intelligence in Higher Education, pp. 169–184, 2022.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

Edward Beeching, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, và Thomas Wolf. Open llm leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023.

Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.

Som Biswas. Chatgpt and the future of medical writing, 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.

Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https://github.com/sahil280114/codealpaca, 2023.

Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, và Marios Savvides. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. 2023.

Hao Chen, Jindong Wang, Lei Feng, Xiang Li, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, và Bhiksha Raj. A general framework for learning from weak supervision. arXiv preprint arXiv:2402.01922, 2024.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Jacob Cohen. Kappa: Coefficient of concordance. Educ Psych Measurement, 20(37):37–46, 1960.

--- TRANG 11 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208, 2023.

Shizhe Diao, Rui Pan, Hanze Dong, KaShun Shum, Jipeng Zhang, Wei Xiong, và Tong Zhang. Lmflow: An extensible toolkit for finetuning and inference of large foundation models. https://optimalscale.github.io/LMFlow/, 2023.

Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, và Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv preprint arXiv:2002.06305, 2020.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, và Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.

Stefan Falkner, Aaron Klein, và Frank Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. In International Conference on Machine Learning, pp. 1437–1446. PMLR, 2018.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.

Varun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue, và Zachary Nado. Deep learning tuning playbook, 2023. URL http://github.com/google-research/tuning_playbook. Version 1.0.

Ian Goodfellow, Yoshua Bengio, Aaron Courville, và Yoshua Bengio. Deep learning, volume 1. MIT Press, 2016.

Google. Bard. 2023.

Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. Evaluating large language models: A comprehensive survey. arXiv preprint arXiv:2310.19736, 2023.

Lynette Hirschman và Robert Gaizauskas. Natural language question answering: the view from here. natural language engineering, 7(4):275–300, 2001.

Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.

Yunjie Ji, Yong Deng, Yiping Peng Yan Gong, Qiang Niu, Lei Zhang, Baochang Ma, và Xiangang Li. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. arXiv preprint arXiv:2303.14742, 2023.

Jacob Devlin Ming-Wei Chang Kenton và Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171–4186, 2019.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.

Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, và Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765–6816, 2017.

Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E Gonzalez, và Ion Stoica. Tune: A research platform for distributed model selection and training. arXiv preprint arXiv:1807.05118, 2018.

--- TRANG 12 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

Jinghui Lu, Linyi Yang, Brian Mac Namee, và Yue Zhang. A rationale-centric framework for human-in-the-loop machine learning. arXiv preprint arXiv:2203.12918, 2022.

Carlo A Mallio, Andrea C Sertorio, Caterina Bernetti, và Bruno Beomonte Zobel. Large language models for structured reporting in radiology: performance of gpt-4, chatgpt-3.5, perplexity and bing. La radiologia medica, pp. 1–5, 2023.

OpenAI. Gpt-4 technical report. 2023.

Yassine Ouali, Céline Hudelot, và Myriam Tami. An overview of deep semi-supervised learning. arXiv preprint arXiv:2006.05278, 2020.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, và Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:4816–4828, 2021.

Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1–16. IEEE, 2020.

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505–3506, 2020.

Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, và Yuxiong He. Zero-offload: Democratizing billion-scale model training. In USENIX Annual Technical Conference, pp. 551–564, 2021.

Anna Rogers, Olga Kovaleva, và Anna Rumshisky. A primer in bertology: What we know about how bert works. Transactions of the Association for Computational Linguistics, 8:842–866, 2021.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Iliĉ, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

Anastasia Shimorina và Anya Belz. The human evaluation datasheet: A template for recording details of human evaluation experiments in NLP. In Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval), pp. 54–75, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.humeval-1.6. URL https://aclanthology.org/2022.humeval-1.6.

Felix Stahlberg. Neural machine translation: A review. Journal of Artificial Intelligence Research, 69:343–418, 2020.

Chi Sun, Xipeng Qiu, Yige Xu, và Xuanjing Huang. How to fine-tune bert for text classification? In Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18–20, 2019, Proceedings 18, pp. 194–206. Springer, 2019.

Mingxing Tan và Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 6105–6114. PMLR, 2019.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

--- TRANG 13 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Lewis Tunstall, Leandro V on Werra, và Thomas Wolf. Natural language processing with transformers. " O'Reilly Media, Inc.", 2022.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.

Chi Wang, Susan Xueqing Liu, và Ahmed H Awadallah. Cost-effective hyperparameter optimization for large language model generation inference. arXiv preprint arXiv:2303.04673, 2023a.

Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, và Tian Gao. Does it make sense? and why? a pilot study for sense making and explanation, 2020.

Cunxiang Wang, Pai Liu, và Yue Zhang. Can generative pre-trained language models serve as knowledge bases for closed-book qa?, 2021.

Cunxiang Wang, Sirui Cheng, Qipeng Guo, Yuanhao Yue, Bowen Ding, Zhikun Xu, Yidong Wang, Xiangkun Hu, Zheng Zhang, và Yue Zhang. Evaluating open-QA evaluation. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023b. URL https://openreview.net/forum?id=UErNpveP6R.

Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, và Yue Zhang. Survey on factuality in large language models: Knowledge, retrieval and domain-specificity, 2023c.

Cunxiang Wang, Haofei Yu, và Yue Zhang. RFiD: Towards rational fusion-in-decoder for open-domain question answering. In Anna Rogers, Jordan Boyd-Graber, và Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 2473–2481, Toronto, Canada, July 2023d. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.155. URL https://aclanthology.org/2023.findings-acl.155.

Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, Heli Qi, Zhen Wu, Yu-Feng Li, Satoshi Nakamura, Wei Ye, Marios Savvides, Bhiksha Raj, Takahiro Shinozaki, Bernt Schiele, Jindong Wang, Xing Xie, và Yue Zhang. Usb: A unified semi-supervised learning benchmark for classification. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022a. doi: 10.48550/ARXIV.2208.07204. URL https://arxiv.org/abs/2208.07204.

Yidong Wang, Hao Wu, Ao Liu, Wenxin Hou, Zhen Wu, Jindong Wang, Takahiro Shinozaki, Manabu Okumura, và Yue Zhang. Exploiting unlabeled data for target-oriented opinion words extraction. arXiv preprint arXiv:2208.08280, 2022b.

Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, , Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, và Xing Xie. Freematch: Self-adaptive thresholding for semi-supervised learning. 2023e.

Yidong Wang, Zhuohao Yu, Jindong Wang, Qiang Heng, Hao Chen, Wei Ye, Rui Xie, Xing Xie, và Shikun Zhang. Exploring vision-language models for imbalanced learning. International Journal of Computer Vision, 2023f.

Yidong Wang, Bowen Zhang, Wenxin Hou, Zhen Wu, Jindong Wang, và Takahiro Shinozaki. Margin calibration for long-tailed visual recognition. In Asian Conference on Machine Learning, pp. 1101–1116. PMLR, 2023g.

--- TRANG 14 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Yiming Wang, Zhuosheng Zhang, và Rui Wang. Element-aware summarization with large language models: Expert-aligned evaluation and chain-of-thought method. arXiv preprint arXiv:2305.13412, 2023h.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, và Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022c.

Jia Wu, Xiu-Yun Chen, Hao Zhang, Li-Dong Xiong, Hang Lei, và Si-Hao Deng. Hyperparameter optimization for machine learning models based on bayesian optimization. Journal of Electronic Science and Technology, 17(1):26–40, 2019.

Canwen Xu, Daya Guo, Nan Duan, và Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023.

Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. Clue: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 4762–4772, 2020.

Linyi Yang, Jiazheng Li, Pádraig Cunningham, Yue Zhang, Barry Smyth, và Ruihai Dong. Exploring the efficacy of automatically generated counterfactuals for sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 306–316, 2021.

Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, và Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. arXiv preprint arXiv:2211.08073, 2022a.

Linyi Yang, Yaoxian Song, Xuan Ren, Chenyang Lyu, Yidong Wang, Jingming Zhuo, Lingqiao Liu, Jindong Wang, Jennifer Foster, và Yue Zhang. Out-of-distribution generalization in natural language processing: Past, present, and future. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 4533–4559, 2023a.

Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, et al. Supervised knowledge makes large language models better in-context learners. arXiv preprint arXiv:2312.15918, 2023b.

Lu Yang, He Jiang, Qing Song, và Jun Guo. A survey on long-tailed visual recognition. International Journal of Computer Vision, 130(7):1837–1872, 2022b.

Tong Yu và Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020.

Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, và Shikun Zhang. Kieval: A knowledge-grounded interactive evaluation framework for large language models. arXiv preprint arXiv:2402.15043, 2024.

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022a.

Zhengran Zeng, Hanzhuo Tan, Haotian Zhang, Jing Li, Yuqun Zhang, và Lingming Zhang. An extensive study on pre-trained models for program understanding and generation. In Sukyoung Ryu và Yannis Smaragdakis (eds.), ISSTA '22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Event, South Korea, July 18 - 22, 2022, pp. 39–51. ACM, 2022b. doi: 10.1145/3533767.3534390. URL https://doi.org/10.1145/3533767.3534390.

Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, và Yuan Gao. A survey on federated learning. Knowledge-Based Systems, 216:106775, 2021a.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022a.

--- TRANG 15 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.

Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, và Pengjun Xie. Crowdsourcing learning as domain adaptation: A case study on named entity recognition. In Chengqing Zong, Fei Xia, Wenjie Li, và Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5558–5570, Online, August 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.432. URL https://aclanthology.org/2021.acl-long.432.

Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, Xiaobin Wang, và Min Zhang. Identifying Chinese opinion expressions with extremely-noisy crowdsourcing annotations. In Smaranda Muresan, Preslav Nakov, và Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2801–2813, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.200. URL https://aclanthology.org/2022.acl-long.200.

Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, và Min Zhang. Language models are universal embedders. arXiv preprint arXiv:2310.08232, 2023.

Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph Gonzalez, E., và Ion Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings. GitHub repository, 2023.

Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, và Dacheng Tao. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. arXiv preprint arXiv:2302.10198, 2023.

--- TRANG 16 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

"inputs": {
  "instruction": "Tìm một ví dụ về loại dữ liệu được cho",
  "input": "Dữ liệu định tính",
  "response1": "Một ví dụ về dữ liệu định tính là phản hồi của khách hàng.",
  "response2": "Một ví dụ về dữ liệu định tính là đánh giá của khách hàng."
}
"outputs": {
  "evaluation_result": "Hòa",
  "evaluation_reason": "Cả hai phản hồi đều đúng và cung cấp các ví dụ tương tự về dữ liệu định tính.",
  "reference_response": "Một ví dụ về dữ liệu định tính là bản ghi phỏng vấn."
}

Hình 5: Một ví dụ dữ liệu huấn luyện cho PandaLM.

Dưới đây là hai phản hồi cho một nhiệm vụ nhất định. Nhiệm vụ được xác định bởi Hướng dẫn với một Đầu vào cung cấp ngữ cảnh thêm. Hãy đánh giá các phản hồi và tạo ra một câu trả lời tham chiếu cho nhiệm vụ.
### Hướng dẫn:{instruction}
### Đầu vào:{input}
### Phản hồi 1:{response 1, được tạo ra bởi một mô hình ứng viên}
### Phản hồi 2:{response 2, được tạo ra bởi một mô hình ứng viên khác}
### Đánh giá:{evaluation result}{evaluation reason}
### Tham chiếu:{a reference response for the instruction}

Hình 6: Prompt để huấn luyện PandaLM.

A CHI TIẾT PROMPT HUẤN LUYỆN
Chúng tôi giới thiệu prompt chi tiết của việc huấn luyện PandaLM trong Hình 6.

B BIỂU ĐỒ ACYCLIC CÓ HƯỚNG MÔ TẢ XẾP HẠNG HỖN HỢP CỦA CÁC MÔ HÌNH ĐƯỢC HUẤN LUYỆN SỬ DỤNG CẢ SIÊU THAM SỐ CỦA ALPACA VÀ PANDALM.

Một biểu đồ acyclic có hướng (DAG) được trình bày trong Hình 7, minh họa xếp hạng tương đối của các mô hình khác nhau được tinh chỉnh với các bộ siêu tham số khác nhau. Đáng chú ý là xếp hạng này khác với những xếp hạng trong Hình 4, do sự khác biệt trong dữ liệu kiểm tra: dữ liệu kiểm tra cho 7 là một tập con được lấy mẫu từ dữ liệu được sử dụng trong Hình 4 được chọn một cách có chủ ý để đảm bảo Thỏa thuận giữa các người chú thích (IAA) cao. Một mô hình có thể nhận biết được xuất hiện từ các xếp hạng: các mô hình được tinh chỉnh sử dụng siêu tham số của PandaLM liên tục vượt trội hơn các mô hình tương ứng được tinh chỉnh với Alpaca. Mô hình được xếp hạng cao nhất là PandaLM-LLaMA, tiếp theo là Alpaca-LLaMA, PandaLM-Bloom, PandaLM-Pythia, PandaLM-OPT, PandaLM-Cerebras-GPT, Alpaca-OPT, Alpaca-Bloom, Alpaca-Pythia, và Alpaca-Cerebras-GPT, theo thứ tự hiệu suất giảm dần. Sự đối chiếu này làm nổi bật hiệu quả của việc lựa chọn siêu tham số của PandaLM trong việc cải thiện hiệu suất mô hình, vì các mô hình được tối ưu hóa với PandaLM luôn xếp hạng cao hơn những mô hình sử dụng siêu tham số của Alpaca trong xếp hạng hỗn hợp. Những phát hiện này nhấn mạnh tiềm năng của PandaLM như một công cụ mạnh mẽ trong việc nâng cao hiệu suất của các mô hình ngôn ngữ lớn, hỗ trợ thêm cho khẳng định về hiệu quả của nó.

--- TRANG 17 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Alpaca@llama-7b
PandaLM@opt-6.7b
Alpaca@bloom-7b
PandaLM@Cerebras-GPT-6.7B
Alpaca@Cerebras-GPT-6.7B
Alpaca@opt-6.7b
Alpaca@pythia-6.9b
PandaLM@bloom-7b
PandaLM@pythia-6.9b
PandaLM@llama-7b

Hình 7: Biểu đồ Acyclic có hướng mô tả xếp hạng hỗn hợp của các mô hình được huấn luyện sử dụng cả siêu tham số của Alpaca và PandaLM. Các mô hình được xếp hạng từ mạnh nhất đến yếu nhất theo thứ tự sau: PandaLM-LLaMA, Alpaca-LLaMA, PandaLM-Bloom, PandaLM-Pythia, PandaLM-OPT, PandaLM-Cerebras-GPT, Alpaca-OPT, Alpaca-Bloom, Alpaca-Pythia, Alpaca-Cerebras-GPT.

Bảng 6: So sánh trên một số nhiệm vụ downstream sử dụng lm-eval(Gao et al., 2021) giữa các mô hình nền tảng được tinh chỉnh trên siêu tham số của Alpaca, và các mô hình nền tảng được tinh chỉnh với PandaLM. Lưu ý rằng nhiệm vụ MMLU bao gồm 57 nhiệm vụ phụ, có nghĩa là việc cung cấp độ lệch chuẩn toàn diện ở đây là không khả thi.

ARC-Challenge-acc_norm(25-shot) Hellaswag-acc_norm(10-shot) MMLU-average-acc(5-shot) TruthfulQA-mc2(0-shot) Trung bình
llama-7b gốc 0.4923±0.0146 0.7583±0.0043 0.3306 0.3703±0.0141 0.4879
llama-7b với PandaLM 0.5162±0.0146 0.7764±0.0042 0.3396 0.3801±0.0145 0.5031
opt-6.7b gốc 0.3805±0.0142 0.6535±0.0047 0.2476 0.3587±0.0139 0.4101
opt-6.7b với PandaLM 0.3771±0.0142 0.6540±0.0047 0.2502 0.3609±0.0142 0.4106
pythia-6.9b gốc 0.3848±0.0142 0.6093±0.0049 0.2490 0.4187±0.0148 0.4155
pythia-6.9b với PandaLM 0.4130±0.0144 0.6337±0.0048 0.2581 0.3972±0.0144 0.4255
bloom-7b gốc 0.3985±0.0143 0.6086±0.0049 0.2635 0.3975±0.0148 0.4170
bloom-7b với PandaLM 0.3951±0.0143 0.6084±0.0049 0.2520 0.3997±0.0149 0.4138
Cerebras-GPT-6.7B gốc 0.3524±0.0140 0.5613±0.0050 0.2584 0.3624±0.0140 0.3836
Cerebras-GPT-6.7B với PandaLM 0.3558±0.0140 0.5550±0.0050 0.2452 0.3448±0.0141 0.3752

C SO SÁNH GIỮA CÁC MÔ HÌNH GỐC VÀ CÁC MÔ HÌNH ĐƯỢC ĐIỀU CHỈNH SỬ DỤNG PANDALM TRÊN CÁC NHIỆM VỤ TRUYỀN THỐNG

Chúng tôi so sánh các LLM được tinh chỉnh trên các nhiệm vụ truyền thống khác nhau với lm-eval (Gao et al., 2021). Mặc dù đa số các mô hình ngôn ngữ thể hiện hiệu suất cải thiện sau khi tinh chỉnh với PandaLM, Cerebras lại trải qua sự suy giảm. Điều này làm nổi bật tầm quan trọng của các đánh giá chủ quan tinh tế (thắng/hòa/thua của phản hồi). Đánh giá con người, cũng như các đánh giá từ GPT-4 và GPT-3.5, đều đồng thuận trong việc chỉ ra hiệu suất tốt hơn từ Cerebras khi kết hợp với PandaLM. Điều này cũng được xác nhận trong (Yu et al., 2024).

Như được thể hiện trong Bảng 7, kết quả đánh giá của các mô hình ngôn ngữ cho thấy perplexity thấp hơn, cho thấy khả năng dự đoán tốt hơn trong pretraining hoặc các nhiệm vụ khác, không phải lúc nào cũng có nghĩa là hiệu suất tổng thể tốt hơn của các mô hình được điều chỉnh hướng dẫn. Ví dụ, LLaMA-PandaLM có perplexity cao hơn LLaMA-Alpaca nhưng vượt trội hơn nó trong cả so sánh theo cặp (PandaLM, GPT, Human) và các nhiệm vụ truyền thống

--- TRANG 18 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Bảng 7: Phân tích về perplexity và các chỉ số đánh giá khác. Lưu ý rằng chúng tôi báo cáo tỷ lệ thắng trên 170 mẫu của PandaLM, GPT, và Human.

Model Perplexity (↓) PandaLM-7B (↑) PandaLM-70B (↑) GPT-3.5 (↑) GPT-4 (↑) Human (↑) lm-eval avg. score (↑)
LLaMA-Alpaca 2.75 15.88% 22.94% 15.29% 10.00% 12.35% 0.4879
LLaMA-PandaLM 2.81 19.41% 35.88% 26.47% 23.53% 48.24% 0.5031

(lm-eval). Điều này cho thấy rằng trong khi perplexity không khả thi cho các mô hình được điều chỉnh hướng dẫn nơi perplexity thấp hơn có thể có nghĩa là overfitting và khả năng tổng quát hóa kém hơn.

D GIỚI THIỆU TẬP DỮ LIỆU PHÁP LÝ/Y SINH

Cụ thể, chúng tôi đánh giá khả năng thành thạo của PandaLM bằng cách sử dụng tập dữ liệu LSAT (Law School Admission Test), phục vụ như một bộ câu hỏi thi tuyển sinh cho các trường luật của Mỹ. Tập dữ liệu này kết hợp 1,009 câu hỏi, được chia thành ba tập con: AR, LR, và RC. Trong lĩnh vực y sinh, chúng tôi sử dụng tập dữ liệu PubMedQA—một kho lưu trữ khổng lồ cho dữ liệu QA truy xuất y sinh, tự hào với 1k chú thích chuyên gia, 61.2k mục không được gắn nhãn, và 211.3k thể hiện QA được tạo ra bởi con người khổng lồ. Để đánh giá của chúng tôi, chúng tôi dựa vào phần được gắn nhãn (PubMedQA-l) chứa 1k thể hiện. Mỗi thể hiện bao gồm một câu hỏi, ngữ cảnh, và nhãn. Ngoài ra, chúng tôi khai thác tập dữ liệu BioASQ, cụ thể là tận dụng tập dữ liệu task b từ thách thức thứ 11 của nó. Tập dữ liệu này nổi tiếng với khả năng lập chỉ mục ngữ nghĩa y sinh và trả lời câu hỏi (QA). Từ đó, chúng tôi sử dụng 1k mẫu cho đánh giá của mình. Chúng tôi sẽ kiểm tra tập dữ liệu code/math Cobbe et al. (2021); Zeng et al. (2022b) trong công việc tương lai.

E PHÂN TÍCH KÍCH THƯỚC VÀ CHẤT LƯỢNG DỮ LIỆU TRONG ĐIỀU CHỈNH HƯỚNG DẪN

Chúng tôi tiến hành một nghiên cứu khử loại trừ để điều tra tác động của kích thước dữ liệu huấn luyện (lên đến 1,344,000) đối với hiệu suất của mô hình, với siêu tham số tối ưu. Quan trọng là, một mối quan hệ tồn tại giữa kích thước và chất lượng của dữ liệu huấn luyện. Vì vậy, chúng tôi tập trung vào nghiên cứu khử loại trừ về kích thước dữ liệu ở đây, nhưng việc tiến hành một thí nghiệm tương tự về chất lượng dữ liệu là khả thi. Chúng tôi rút ra kết quả từ PandaLM-7B. Mục tiêu là phân biệt cần bao nhiêu dữ liệu huấn luyện để đạt được hiệu suất đỉnh của mỗi mô hình. Bảng 8 cho thấy lượng dữ liệu huấn luyện tối ưu khác nhau giữa các mô hình. Nhiều dữ liệu huấn luyện hơn thường nâng cao hiệu suất mô hình. Tuy nhiên, một điểm tối ưu tồn tại cho mỗi mô hình, vượt quá đó dữ liệu thêm không cải thiện hiệu suất. Ví dụ, mô hình OPT đạt đỉnh ở 992,000 điểm dữ liệu, cho thấy dữ liệu bổ sung không nâng cao hiệu suất của mô hình.

Bảng 8: Kích thước dữ liệu huấn luyện tối ưu cho mỗi mô hình.
Model Bloom Cerebras-GPT LLaMA OPT Pythia
Kích thước dữ liệu huấn luyện tối ưu 1,216,000 1,344,000 11,520,000 992,000 1,344,000

F PHÂN TÍCH LORA TRONG ĐIỀU CHỈNH HƯỚNG DẪN

Chúng tôi tiếp tục nhằm đánh giá hiệu quả của Low-Rank Adaptation (LoRA) (Hu et al.) so với tinh chỉnh đầy đủ qua các mô hình khác nhau, sử dụng siêu tham số tối ưu. Kết quả cũng được thu từ PandaLM-7B. Phân tích của chúng tôi tìm cách cung cấp hiểu biết so sánh về các phương pháp điều chỉnh này. Như được thể hiện trong Bảng 9, kết quả cho mô hình Bloom cho thấy một lợi thế rõ ràng cho tinh chỉnh đầy đủ, chiến thắng trên LoRA trong 66 trường hợp so với 35 của LoRA. Đáng chú ý là chúng hòa trong 69 trường hợp. Trong trường hợp mô hình Cerebras, tinh chỉnh đầy đủ lại một lần nữa chứng minh tính vượt trội, dẫn đầu trong 59 trường hợp so với 40 của LoRA, mặc dù hòa 71 lần. Xu hướng tinh chỉnh đầy đủ vượt trội nhất quán trong mô hình LLaMA. Trong số 170 trường hợp, tinh chỉnh đầy đủ dẫn đến hiệu suất tốt hơn trong 48 trường hợp, trong khi LoRA xuất hiện chiến thắng chỉ trong 28 trường hợp. Đa số kết quả được hòa, lên tới 94 trường hợp. Trong mô hình OPT, tinh chỉnh đầy đủ một lần nữa thể hiện lợi thế với 64 trường hợp hiệu suất vượt trội so với 33 của LoRA, trong khi ghi nhận hòa trong 73 trường hợp. Cuối cùng, đối với mô hình Pythia, tinh chỉnh đầy đủ dẫn đầu cuộc đua với 71 trường hợp hiệu suất tốt hơn so với 21 của LoRA, và hòa xảy ra trong 78 trường hợp. Những kết quả này nhấn mạnh rằng tinh chỉnh đầy đủ nói chung mang lại kết quả thuận lợi hơn so với việc sử dụng LoRA, mặc dù kết quả có thể thay đổi tùy thuộc vào mô hình. Mặc dù số lượng hòa đáng kể, tinh chỉnh đầy đủ giữ ưu thế trong hầu hết các mô hình, do đó làm nổi bật hiệu quả của nó. Điều này cho thấy rằng trong khi LoRA có thể cung cấp kết quả có thể so sánh trong một số trường hợp, một chiến lược tinh chỉnh đầy đủ thường chứng minh là phương pháp có lợi hơn trong việc nâng cao hiệu suất mô hình.

Bảng 9: So sánh LoRA và Tinh chỉnh đầy đủ.
Model LoRA Thắng Tinh chỉnh đầy đủ Thắng Hòa
Bloom 35 66 69
Cerebras-GPT 40 59 71
LLaMA 28 48 94
OPT 33 64 73
Pythia 21 71 78

G TẬN DỤNG CÁC MÔ HÌNH ĐƯỢC HUẤN LUYỆN TRƯỚC VÀ CÁC MÔ HÌNH ĐƯỢC ĐIỀU CHỈNH HƯỚNG DẪN KHÁC ĐỂ ĐÁNH GIÁ

Sử dụng LLM để đánh giá phản hồi mà không cần huấn luyện bổ sung là một hướng tự nhiên cho nhiệm vụ. Tuy nhiên, việc thực hiện các tiêu chí đánh giá thông qua các phương pháp zero-shot hoặc few-shot là thách thức đối với LLM do sự cần thiết cho độ dài ngữ cảnh mở rộng.

Chúng tôi đã thực hiện các thí nghiệm sử dụng đánh giá zero-shot và few-shot (in-context learning Dong et al. (2022); Yang et al. (2023b)) với LLaMA. Các quan sát của chúng tôi chỉ ra rằng LLaMA chưa được điều chỉnh gặp khó khăn với việc tuân thủ các yêu cầu định dạng được chỉ định bởi người dùng. Do đó, các thí nghiệm của chúng tôi tập trung vào việc tính toán và so sánh log-likelihood của việc tạo ra các phần tiếp theo (ví dụ, xác định xem "Response 1 is better," "Response 2 is better," hay nếu cả hai phản hồi có chất lượng tương tự) từ cùng ngữ cảnh. Chúng tôi coi lựa chọn với log-likelihood cao nhất là kết quả dự đoán. Chúng tôi cũng xen kẽ thứ tự phản hồi trong các thí nghiệm để giảm thiên vị vị trí. Hơn nữa, chúng tôi đã thực hiện các thí nghiệm với Vicuna, một phiên bản được tinh chỉnh của LLaMA. Các thí nghiệm chứng minh rằng khả năng đánh giá của các mô hình được điều chỉnh hướng dẫn có tiềm năng đáng kể để nâng cao.

Kết quả trong Bảng 10 làm nổi bật tầm quan trọng của việc điều chỉnh có mục tiêu để đánh giá, một mô hình nhỏ được điều chỉnh chính xác vượt trội hơn một mô hình lớn hơn trong các tình huống zero và few-shot.

H NÂNG CAO PANDALM VỚI GIÁM SÁT ĐƯỢC TINH CHỈNH.

Trong mục tiêu giám sát của chúng tôi, chúng tôi kết hợp không chỉ kết quả so sánh của các phản hồi mà còn cả một giải thích ngắn gọn và phản hồi tham chiếu. Phương pháp này tăng cường sự hiểu biết của PandaLM về các tiêu chí đánh giá.

Bảng 10: Nghiên cứu khử loại trừ việc sử dụng trực tiếp các mô hình được huấn luyện trước và các mô hình được điều chỉnh hướng dẫn để đánh giá.

Model Accuracy Precision Recall F1 score
LLaMA-7B 0-shot (log-likelihood) 12.11 70.23 34.52 8.77
LLaMA-30B 0-shot (log-likelihood) 31.43 56.48 43.12 32.83
LLaMA-7B 5-shot (log-likelihood) 24.82 46.99 39.79 25.43
LLaMA-30B 5-shot (log-likelihood) 42.24 61.99 51.76 42.93
Vicuna-7B (log-likelihood) 15.92 57.53 34.90 14.90
Vicuna-13B (log-likelihood) 35.24 57.45 43.65 36.29
PandaLM-7B 59.26 57.28 59.23 54.56
PandaLM-7B (log-likelihood) 59.26 59.70 63.07 55.78

--- TRANG 19 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

Bảng 11: Nghiên cứu khử loại trừ mục tiêu giám sát.
Model Accuracy Precision Recall F1
PandaLM-7B (chỉ với nhãn đánh giá) 0.4725 0.4505 0.4725 0.3152
PandaLM-7B 0.5926 0.5728 0.5923 0.5456

Để đánh giá thực nghiệm tầm quan trọng của giải thích này, một thí nghiệm đã được thực hiện. Ở đây, giải thích và tham chiếu được bỏ qua trong quá trình huấn luyện, và chỉ các kết quả phân loại (0/1/2 hoặc Tie/Win/Lose) được giữ lại trong tập dữ liệu để huấn luyện một phiên bản mới của PandaLM. Kết quả, như được mô tả trong Bảng 11, cho thấy rằng khi không có giải thích, PandaLM gặp khó khăn trong việc xác định chính xác phản hồi được ưa thích.

I BẢNG DỮ LIỆU ĐÁNH GIÁ CON NGƯỜI

Chúng tôi sử dụng những người chú thích con người từ một công ty cộng đồng và trả cho họ một cách công bằng. Cụ thể, chúng tôi trả cho người chú thích của mình 50 đô la mỗi giờ, cao hơn mức thu nhập địa phương trung bình. Chúng tôi đã điền vào Google Sheet được cung cấp trong (Shimorina & Belz, 2022).

J PHÂN TÍCH TỐI ƯU HÓA SIÊU THAM SỐ

Trong quá trình tìm kiếm siêu tham số của chúng tôi, chúng tôi khám phá một phạm vi tốc độ học, epoch, bộ tối ưu hóa, và bộ lập lịch. Các tốc độ học được kiểm tra thay đổi từ 2e-6 đến 2e-4, với các checkpoint mô hình được lưu ở cuối mỗi epoch. Hiệu suất được đánh giá nghiêm ngặt thông qua so sánh theo cặp giữa các checkpoint, đếm các vòng thắng cho mỗi mô hình, như chi tiết trong Hình 8.

Phân tích của chúng tôi, như được mô tả trong Hình 8a, cho thấy xu hướng hướng tới tốc độ học 2e-5, mặc dù sở thích này không rõ ràng đồng nhất qua tất cả các mô hình. Hình 8b thể hiện sự biến thiên trong số epoch tối ưu, với xu hướng cho thấy rằng hiệu suất đỉnh thường xảy ra quanh epoch thứ tư hoặc thứ năm. Bằng chứng này chỉ ra sự tương tác phức tạp của siêu tham số với hiệu suất mô hình, được ảnh hưởng thêm bởi phân phối dữ liệu, bộ tối ưu hóa, và lựa chọn bộ lập lịch.

Các phát hiện từ quá trình tối ưu hóa siêu tham số của chúng tôi làm nổi bật rằng không có cài đặt tối ưu phổ quát cho các mô hình và thiết lập huấn luyện khác nhau. Trong khi một mô hình xuất hiện cho thấy rằng tốc độ học quanh 2e-5 và số epoch gần 4 có thể có lợi trong một số trường hợp, những kết quả này không kết luận. Điều này củng cố sự cần thiết cho các tìm kiếm siêu tham số cụ thể cho các mô hình khác nhau, như được chứng minh trong các trực quan hóa của chúng tôi. Một cách tiếp cận được điều chỉnh cho tối ưu hóa siêu tham số là cần thiết, vì nó cho phép hiểu biết tinh tế hơn về hiệu suất mô hình qua các tình huống khác nhau.

Bên cạnh đó, chúng tôi đã thực hiện một chiến lược dừng sớm sử dụng Pandalm. Chúng tôi tập trung cụ thể vào LLaMA. Các thí nghiệm của chúng tôi cho thấy rằng trong một số trường hợp, hiệu suất của mô hình ở epoch 3 kém hơn so với epoch 2. Tuy nhiên, các epoch tiếp theo thể hiện cải thiện hiệu suất. Điều này chỉ ra rằng dừng sớm có thể không phù hợp cho việc tinh chỉnh mô hình lớn, vì nó có thể dừng huấn luyện quá sớm trước khi đạt được hiệu suất tối ưu.

Bảng 12: Phân tích khả năng đánh giá của PandaLM trên các mô hình chưa thấy.
Model Comparison PandaLM Human Metrics (P, R, F1)
llama1-7b vs llama2-7b (23,61,16) (23,70,7) (0.7061, 0.7100, 0.6932)
llama1-13b vs llama2-13b (18,73,9) (20,68,12) (0.7032, 0.6800, 0.6899)
llama1-65b vs llama2-70b (20,66,14) (34,56,10) (0.7269, 0.6600, 0.6808)

K PHÂN TÍCH THAY ĐỔI MÔ HÌNH

Trong Bảng 12, chúng tôi cung cấp so sánh chi tiết về hiệu suất của PandaLM so với điểm chuẩn con người và trong bối cảnh các phiên bản khác nhau của các mô hình LLaMA được điều chỉnh hướng dẫn. Lưu ý rằng llama1-13b, llama1-65b và llama2 cho thấy sự thay đổi mô hình. Kết quả chứng minh rằng PandaLM phù hợp

--- TRANG 20 ---
Được xuất bản như bài báo hội thảo tại ICLR 2024

2e-6 1e-5 2e-5 2e-4203040506070 Model
llama-7b
pythia-6.9b
opt-6.7b
Cerebras-GPT-6.7B
bloom-7b
Learning Rate# Win Rounds
(a) Kết quả so sánh của GPT-4.
1 2 3 4 515202530354045505560
Model
llama-7b
pythia-6.9b
opt-6.7b
Cerebras-GPT-6.7B
bloom-7b
Epoch# Win Rounds (b) Kết quả so sánh của Human.

Hình 8: Phân tích tối ưu hóa siêu tham số trong PandaLM. Hình minh họa hiệu suất qua các tốc độ học khác nhau và sự biến thiên trong hiệu suất mô hình qua các epoch.

chặt chẽ với con người, liên tục thể hiện sở thích cho mô hình LLama-2. Sự phù hợp này phù hợp với kỳ vọng, vì LLama-2 được hưởng lợi từ nhiều dữ liệu huấn luyện trước hơn. Những phát hiện như vậy làm nổi bật tầm quan trọng của việc huấn luyện trước rộng rãi trong việc phát triển các mô hình ngôn ngữ có kỹ năng hơn trong việc hiểu và phản hồi đúng đắn với các hướng dẫn khác nhau.
