# Các proxy quy mô nhỏ cho bất ổn đào tạo Transformer quy mô lớn
Mitchell Wortsman Peter J. Liu Lechao Xiao Katie Everett
Alex Alemi Ben Adlam John D. Co-Reyes Izzeddin Gur Abhishek Kumar
Roman Novak Jeffrey Pennington Jascha Sohl-dickstein Kelvin Xu
Jaehoon Lee*Justin Gilmer*Simon Kornblith*
Google DeepMind
Tóm tắt
Các nhóm đã đào tạo các mô hình lớn dựa trên Transformer đã báo cáo sự bất ổn đào tạo ở quy mô lớn mà không xuất hiện khi đào tạo với cùng các siêu tham số ở quy mô nhỏ hơn. Mặc dù các nguyên nhân của những bất ổn như vậy có ý nghĩa khoa học, số lượng tài nguyên cần thiết để tái tạo chúng đã khiến việc điều tra trở nên khó khăn. Trong công trình này, chúng tôi tìm cách tái tạo và nghiên cứu tính ổn định và bất ổn đào tạo ở quy mô nhỏ hơn. Đầu tiên, chúng tôi tập trung vào hai nguồn bất ổn đào tạo được mô tả trong công trình trước đó: sự tăng trưởng của logits trong các lớp attention (Dehghani et al., 2023) và sự phân kỳ của output logits khỏi log probabilities (Chowdhery et al., 2022). Bằng cách đo lường mối quan hệ giữa learning rate và loss qua các quy mô, chúng tôi cho thấy những bất ổn này cũng xuất hiện trong các mô hình nhỏ khi đào tạo ở learning rates cao, và các biện pháp giảm thiểu đã được sử dụng trước đây ở quy mô lớn cũng hiệu quả trong chế độ này. Điều này thúc đẩy chúng tôi điều tra mức độ mà các can thiệp optimizer và mô hình đã biết khác ảnh hưởng đến độ nhạy của loss cuối cùng với những thay đổi trong learning rate. Để làm điều này, chúng tôi nghiên cứu các phương pháp như warm-up, weight decay, và µParam (Yang et al., 2022), và kết hợp các kỹ thuật để đào tạo các mô hình nhỏ đạt được losses tương tự qua nhiều bậc độ lớn của biến thiên learning rate. Cuối cùng, để kết thúc thăm dò của chúng tôi, chúng tôi nghiên cứu hai trường hợp mà bất ổn có thể được dự đoán trước khi chúng xuất hiện bằng cách xem xét hành vi scaling của model activation và gradient norms.

1 Giới thiệu
Việc mở rộng quy mô transformers đã dẫn đến tiến bộ đáng kể từ các mô hình chat đến tạo ảnh. Tuy nhiên, không phải mọi lần chạy đào tạo đều thành công. Khi đào tạo các Transformers lớn, các nhà nghiên cứu đã báo cáo sự bất ổn làm chậm hoặc làm mất ổn định quá trình học [6,11,53,35,8]. Khi các tài nguyên cần thiết cho các lần chạy lớn tiếp tục tăng, việc xem xét các cách mà đào tạo Transformer có thể thất bại là quan trọng.

Trong báo cáo này chúng tôi tái tạo, nghiên cứu, và dự đoán bất ổn đào tạo trong các mô hình Transformer. Chúng tôi thấy rằng việc đo lường mối quan hệ giữa learning rate và loss qua các quy mô là một công cụ hữu ích để xác định bất ổn (ví dụ, Hình 1). Do đó, chúng tôi giới thiệu độ nhạy learning rate (LR), phục vụ như một thống kê tóm tắt hữu ích cho các đường cong learning rate vs. loss. Độ nhạy LR đo lường độ lệch khỏi hiệu suất tối ưu khi thay đổi LR qua nhiều bậc độ lớn.

Chúng tôi cho thấy rằng hai nguồn bất ổn, đã được mô tả trước đây ở quy mô lớn, có thể được tái tạo trong các Transformers nhỏ.¹ Điều này cho phép nghiên cứu chúng mà không cần truy cập vào các nhóm tài nguyên lớn. Cụ thể, chúng tôi xem xét sự tăng trưởng của logits trong các lớp attention [11,16,51] và sự phân kỳ của output logits khỏi log probabilities [6]. Như thể hiện rõ từ các đường cong learning rate vs. loss và bằng cách kiểm tra các đặc điểm mô hình, cả hai bất ổn đều xuất hiện ở learning rates cao trong các mô hình nhỏ. Hơn nữa, các can thiệp đã được sử dụng trước đây ở quy mô lớn cũng thành công trong chế độ này (ví dụ, Hình 1). Những can thiệp này—qk-layernorm [11]² và z-loss regularization [6]—giảm độ nhạy LR và cho phép đào tạo thành công qua ba bậc độ lớn biến thiên LR.

Những quan sát này đặt ra câu hỏi về cách các can thiệp optimizer và mô hình đã biết khác ảnh hưởng đến hình dạng của các đường cong learning rate vs. loss qua các quy mô. Do đó, chúng tôi nghiên cứu hiệu ứng của các kỹ thuật như warm-up, weight decay, và µParam [50] trong bối cảnh này. Khi sử dụng qk-layernorm và z-loss regularization, những kỹ thuật khác này thường có ít tác động đến phạm vi learning rates mà các mô hình có thể được đào tạo ổn định, nhưng ảnh hưởng đến độ nhạy với learning rate trong phạm vi này. Phù hợp với công trình trước đó, chúng tôi thấy rằng warm-up dài hơn giảm độ nhạy learning rate, cũng như việc scaling độc lập của learning rate và weight decay được khuyến nghị bởi Loshchilov và Hutter [33]. Một phát hiện thú vị là scaling depth tăng độ nhạy LR với tốc độ nhanh hơn so với scaling width.

Phần còn lại của điều tra chúng tôi tập trung vào hành vi scaling cho các đặc điểm mô hình như activation và gradient norms. Sử dụng bất ổn tăng trưởng attention logit làm ví dụ, chúng tôi cho thấy rằng có thể dự đoán một bất ổn trước khi nó xuất hiện. Điều này trái ngược với các công trình trước đó về scaling chủ yếu tập trung vào các xu hướng scaling liên quan đến loss [27,22]. Chúng tôi kết thúc bằng cách sử dụng hành vi scaling của các đặc điểm mô hình để tìm kiếm các bất ổn hiện không được ghi chép rõ ràng. Điều tra của chúng tôi cho thấy gradient norms giảm theo cả scale và learning rate, sao cho siêu tham số epsilon AdamW [33] mặc định quá lớn. Điều này gây ra các updates quá nhỏ. Chúng tôi kết nối hiện tượng này và bất ổn tăng trưởng attention logit với sự tăng trưởng parameter norm [34, 29].

Nhìn chung, chúng tôi tin rằng công trình của chúng tôi mang đến các cơ hội khoa học mới để nghiên cứu tính ổn định đào tạo mà không cần truy cập vào các nhóm tài nguyên lớn.

2 Phương pháp thực nghiệm
Phần này chi tiết thiết lập thực nghiệm của chúng tôi (Phần 2.1) và các công cụ hữu ích được sử dụng bởi phân tích của chúng tôi: (i) đo lường mối quan hệ giữa learning rate và loss qua các quy mô (Phần 2.2) và (ii) xem xét các xu hướng scaling cho các đặc điểm mô hình (Phần 2.3).

2.1 Thiết lập thực nghiệm
Chúng tôi đào tạo các mô hình Transformer nhỏ [45] với thiết lập thực nghiệm tương tự như GPT-2 [38] được triển khai trong Flax [20]: các mô hình chỉ là decoder [31] và được đào tạo với auto-regressive loss (tham khảo Phần A để biết thêm chi tiết hạ tầng). Trong khi chúng tôi thao tác thực nghiệm nhiều siêu tham số sau đây, phần này cung cấp các giá trị mặc định của chúng, mà chúng tôi sử dụng trừ khi được chỉ định khác.

Theo mặc định, chúng tôi sử dụng AdamW [33] với β₁ = 0.9, β₂ = 0.95, ε = 1e-8, và gradient clipping ở global norm 1. Warm-up mặc định là 5e3 steps, và số steps tổng mặc định là 1e5. Chúng tôi sử dụng lịch trình tuyến tính cho warm-up và lịch trình cosine-decay [32] cho phần còn lại, với learning rate tối thiểu 1e-5. Chúng tôi sử dụng weight decay độc lập là 1e-4 và auxiliary z-loss [6] với hệ số 1e-4. Các phần 3.2.2 và 3.1.2 tương ứng cung cấp thông tin bổ sung và ablations về decoupled weight decay và z-loss.

Chúng tôi sử dụng Transformers pre-normalization [38] với qk-layernorm [11] (xem Phần 3.1.1 để biết thông tin). Chúng tôi không sử dụng bất kỳ biases nào theo Chowdhery et al. [6], và layernorm [1] ε vẫn ở giá trị mặc định trong Flax [20] là 1e-6. Chúng tôi cùng scale up embedding size, depth, và số heads khi scaling parameters. Chúng tôi không sử dụng weight tying của first và last layer [37], và khi báo cáo số parameters chúng tôi loại trừ embedding và head (như trong Kaplan et al. [27]). Chúng tôi sử dụng rotary positional embeddings [43], và cho training data chúng tôi sử dụng C4 [39]. Để d tham chiếu đến model dimension (tức là, embedding size), thành phần feed-forward của Transformer là một MLP với hidden dimension 4d và gelu [21] activations. Như trong Vaswani et al. [45] chúng tôi sử dụng factor 1/√d scaling trong self-attention. Embedding initialization là mặc định trong Flax, được phân phối normally với standard deviation 1/√d. Phần còn lại của weights được khởi tạo với truncated normal distribution với inverse root fan-in standard deviation [18]. Batch size mặc định là 256, trong đó mỗi batch element có sequence length 512 tokens. Sequences được packed để không cần padding. Cuối cùng, chúng tôi sử dụng vocabulary từ Raffel et al. [40] có size 32101 và sử dụng SentencePiece [28] tokenizer. Chúng tôi đào tạo trên TPUs [26] trong bfloat16 precision sử dụng Flax [20] và JAX [4].

2.2 Đường cong LR vs. loss và độ nhạy learning rate
Để điều tra cách bất ổn mô hình xuất hiện theo scale, việc vẽ mối quan hệ giữa learning rate (LR) và loss cho các mô hình có kích thước khác nhau là hữu ích. Ví dụ, một bất ổn thường được đặc trưng bởi sự bùng nổ trong loss ở learning rates cao. Các đường cong LR vs. loss có thể tiết lộ cách learning rate không ổn định thấp nhất thay đổi như một hàm của kích thước mô hình.

Để tóm tắt các đường cong LR vs. loss, chúng tôi sử dụng độ nhạy LR. Độ nhạy LR đo lường độ lệch trong validation loss cuối cùng khỏi tối ưu khi quét LR qua ba bậc độ lớn. Nếu một mô hình thất bại trong việc đào tạo ở learning rates cao, thì độ nhạy LR sẽ cao. Có những trường hợp mà các đường cong LR vs. loss và độ nhạy LR không còn có ý nghĩa, ví dụ nếu một can thiệp thay đổi ý nghĩa của learning rate—xem Phụ lục B cho thảo luận chi tiết.

Để θ = A(η) biểu thị weights θ của mô hình thu được khi đào tạo với learning rate η, và để ℓ(θ) biểu thị validation loss khi sử dụng weights θ. Cho một phạm vi learning rate [a, b], để ℓ* biểu thị loss thu được với learning rate tốt nhất, tức là, ℓ* = min_{η∈[a,b]} ℓ(A(η)). Hơn nữa, để ℓ₀ biểu thị loss tại initialization. Thì độ nhạy LR được định nghĩa là E_{η∈[a,b]}[min(ℓ(A(η)), ℓ₀) - ℓ*].

Trừ khi được đề cập khác, chúng tôi sử dụng phạm vi learning rate 3e-4 đến 3e-1 với AdamW [33] để đo độ nhạy LR, trong đó LR tham chiếu đến giá trị tối đa trong lịch trình cosine decay với warm-up [32]. Chúng tôi xem xét LRs trong {3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1} khi tính minimum và expectation.

2.3 Xu hướng scaling cho các đặc điểm mô hình
Để nghiên cứu bất ổn, chúng tôi cũng thấy hữu ích khi xem xét xu hướng scaling cho các đặc điểm mô hình như gradient hoặc activation norms. Phương pháp này hữu ích để dự đoán bất ổn và trái ngược với công trình trước đó về scaling, chủ yếu tập trung vào xu hướng liên quan model scale và loss [27, 22].

3 Kết quả
Phần này trình bày kết quả của chúng tôi về tính ổn định đào tạo cho các Transformers nhỏ. Được trang bị độ nhạy LR (Phần 2.2), chúng tôi nghiên cứu hai bất ổn đã biết và sự giảm thiểu tương ứng của chúng ở quy mô nhỏ (Phần 3.1). Điều này đặt ra câu hỏi về cách các can thiệp mô hình và optimizer khác ảnh hưởng đến độ nhạy của loss cuối cùng với learning rate, mà chúng tôi điều tra trong Phần 3.2. Cuối cùng, chúng tôi xem xét liệu bất ổn có thể được dự đoán đáng tin cậy trước khi chúng xuất hiện: Phần 3.3 dự đoán khi bất ổn tăng trưởng logit có thể gây ra divergence trong một mô hình lớn hơn, trong khi Phần 3.4 nhằm tìm các vấn đề khác có thể xảy ra khi scaling up với các siêu tham số mặc định của chúng tôi.

3.1 Tái tạo hai bất ổn đã biết ở quy mô nhỏ
Ở đây, chúng tôi xem xét hai bất ổn đã được mô tả trước đây ở quy mô lớn: sự tăng trưởng của logits trong các lớp attention [11,16,51] và sự phân kỳ của output logits khỏi log probabilities [6]. Bằng cách xem xét các đường cong LR vs. loss, chúng tôi cho thấy những bất ổn này có thể được tái tạo trong các mô hình nhỏ bằng cách sử dụng learning rates cao và các biện pháp giảm thiểu được sử dụng ở quy mô lớn hiệu quả trong chế độ này.

3.1.1 Tăng trưởng attention logit
Các nhà nghiên cứu đã ghi chép trước đây rằng đào tạo Transformer thất bại khi attention logits trở nên lớn [11,51]. Trong Dehghani et al. [11], vấn đề này xuất hiện khi đào tạo một mô hình ViT [14] với 22 tỷ parameters.

Trong lớp self-attention của một Transformer [45], queries q_i và keys k_i được kết hợp để tính attention logits z_{ij} = ⟨q_i, k_j⟩/√d_h, trong đó d_h là head dimension. Tiếp theo, attention logits được truyền qua softmax để tạo attention weights, được sử dụng để kết hợp values v_i. Dehghani et al. [11] quan sát rằng attention logits z trở nên lớn, mà họ gọi là attention logit growth. Kết quả là, attention weights sụp đổ thành one-hot vectors, được Zhai et al. [51] gọi là attention entropy collapse. Để giải quyết vấn đề này, Dehghani et al. [11] đề xuất qk-layernorm, áp dụng LayerNorm [1] cho queries và keys trước khi tính attention logits.

Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng các mô hình không cần phải lớn để thể hiện bất ổn liên quan đến attention logit growth. Như được hiển thị trong Hình 1, learning rate tối đa mà các mô hình nhỏ có thể được đào tạo tăng khi sử dụng qk-layernorm. Không có qk-layernorm, learning rate mà các mô hình diverge trở nên nhỏ hơn với kích thước mô hình tăng. Ngược lại, các mô hình có qk-layernorm thể hiện độ nhạy LR thấp hơn đáng kể và đào tạo đến loss thấp ở learning rates cao. Như một điểm nổi bật, qk-layernorm cho phép đào tạo một mô hình với 1.2B parameters ở learning rate 0.3. Cả với và không có qk-layernorm, độ nhạy LR tăng theo scale.

Hình 2 hiển thị loss và max attention logit cho hai quy mô mô hình khác nhau ba bậc độ lớn. Trong cả hai trường hợp, loss diverges không có qk-layernorm. Kết quả của chúng tôi trong Hình Phụ lục E.1 gợi ý rằng attention logit growth là do sự tăng trưởng trong queries và keys, không phải do sự gia tăng trong alignment của chúng. Thay vào đó, chúng tôi đưa ra giả thuyết rằng bất ổn này có thể do sự phụ thuộc bậc hai của attention logits vào parameter norms.

3.1.2 Phân kỳ output logit
Một bất ổn khác được báo cáo bởi các nhà nghiên cứu đào tạo các mô hình lớn là sự phân kỳ trong output logits khỏi log probabilities [6]. Giống như trước đó, chúng tôi tái tạo bất ổn này với các mô hình nhỏ ở learning rates lớn, và biện pháp giảm thiểu được đề xuất làm giảm vấn đề. Nhìn chung, Hình 3 tóm tắt hiệu ứng.

Để y biểu thị output logits của mô hình, được sử dụng để tính class probabilities p_i qua softmax p_i = e^{y_i}/Z trong đó Z = Σ_j e^{y_j}. Bất ổn này xảy ra khi logits diverge và trở nên rất âm, như được minh họa trong Hình 4 cho một mô hình 2.4M parameter ở learning rate 0.1. Trái ngược với bất ổn attention logit growth, sự phân kỳ này xảy ra vào cuối đào tạo. Biện pháp giảm thiểu được đề xuất bởi Chowdhery et al. [6] là khuyến khích log Z duy trì gần bằng không. Họ thêm một auxiliary loss log²Z, được gọi là z-loss, với hệ số 1e-4.

Như được minh họa trong Hình 3 và 4, chúng tôi thấy rằng bất ổn liên quan đến output logit divergence xảy ra trong các mô hình không có weight decay bất kể quy mô, và z-loss giải quyết bất ổn này. Weight decay cũng giảm thiểu bất ổn này cho các mô hình lớn hơn mà chúng tôi kiểm tra.

3.2 Đo lường hiệu ứng của các can thiệp đã biết khác
Phần trước đã sử dụng mối quan hệ giữa learning rate và loss như một công cụ hữu ích để xem xét hai bất ổn đã biết và sự giảm thiểu của chúng. Điều này đặt ra câu hỏi về cách các can thiệp mô hình và optimizer đã biết khác ảnh hưởng đến hình dạng của các đường cong LR vs. loss qua các quy mô. Cụ thể, độ nhạy LR có thể giúp xác định các vấn đề hoặc giải pháp bổ sung khi scaling không?

3.2.1 Warm-up
Như được minh họa bởi Hình 5, một giai đoạn warm-up dài hơn giảm độ nhạy LR. Điều này rõ ràng nhất đối với các mô hình lớn hơn, không ổn định ở LR 3e-1 không có warm-up dài. Số steps tổng được cố định ở 1e5 trong thí nghiệm này, và tất cả các mô hình sử dụng qk-layernorm. Tầm quan trọng của warm-up đối với tính ổn định đã được nổi bật trước đây [17,42,30], mặc dù những công trình này không đo lường hành vi scaling.

3.2.2 Weight decay độc lập
Parameterizing weight decay độc lập với learning rate giảm độ nhạy LR, như được minh họa trong Hình 6. Trong khi điều này được khuyến nghị bởi Loshchilov và Hutter [33], nó không phải là thực tiễn phổ biến trong các triển khai AdamW mặc định của PyTorch [36] hoặc Optax [2]. Chúng tôi giải thích sự khác biệt dưới đây.

Cho parameters θ, để Δ = v/(√u + ε) biểu thị AdamW update không có learning rate hoặc weight decay. Cho weight decay coefficient λ, max learning rate η, và schedule s_t ∈ [0,1], Loshchilov và Hutter [33] khuyến nghị update θ ← θ - s_t(ηΔ - λθ), mà chúng tôi gọi là independent decay. Mặt khác, triển khai mặc định trong PyTorch hoặc Optax áp dụng update θ ← θ - s_tη(Δ - λθ), tức là, η bây giờ scale cả hai terms.

Khi báo cáo độ nhạy LR không có independent decay trong Hình 6, chúng tôi báo cáo độ nhạy LR tối thiểu trên các phạm vi [1e-4, 1e-1] và [3e-4, 3e-1] vì cái trước đôi khi được centered tốt hơn trên minimum. Thiết lập mặc định trong paper này là sử dụng independent decay. Khi sử dụng independent decay chúng tôi đặt λ = 1e-4, và không có independent decay chúng tôi đặt λ = 0.1. Một sweep về các giá trị weight decay được tiến hành trong Hình E.10.

3.2.3 Scaling width vs. depth
Chúng tôi đã nhất quán quan sát rằng việc tăng số parameters tăng độ nhạy LR. Bây giờ chúng tôi xem xét phần nào của scaling chịu trách nhiệm nhiều nhất. Kết quả của chúng tôi, được minh họa bởi Hình 7, chỉ ra rằng scaling depth tăng độ nhạy LR với tốc độ nhanh hơn so với scaling width. Tuy nhiên, ở quy mô lớn nhất mà chúng tôi kiểm tra, scaling depth độc lập tạo ra một mô hình với validation loss thấp hơn. Một so sánh validation loss giữa width scaling, depth scaling, và joint scaling có trong Hình Phụ lục E.3. Thực tiễn tiêu chuẩn của joint scaling hoạt động tốt nhất ở quy mô lớn nhất và cũng có dự đoán scaling đáng tin cậy hơn khi extrapolating.

Khi scaling depth, chúng tôi sử dụng d = 512, và khi scaling width, chúng tôi sử dụng 6 layers. Số heads được scaled tỷ lệ với width, để head dimension vẫn giữ nguyên.

Hình E.2 lặp lại thí nghiệm này không có qk-layernorm, thấy rằng bất ổn attention logit growth xảy ra thường xuyên hơn ở scale bất kể width hay depth được scaled.

3.2.4 µParam
Yang và Hu [49] giới thiệu phương pháp µParam để parameterizing một neural network. Như một sản phẩm, LR tối ưu vẫn nhất quán khi scaling model width [50]. Phần này kiểm tra hiệu ứng của µParam trên độ nhạy LR, và xem xét liệu µParam có giảm bớt nhu cầu cho qk-layernorm [11] không.

Như được minh họa bởi Hình 8, µParam thực sự thành công trong việc ổn định LR tối ưu ở scale mà chúng tôi kiểm tra. Tuy nhiên, µParam không cải thiện loss hoặc giảm độ nhạy LR trong các thí nghiệm của chúng tôi. Hình Phụ lục E.4 lặp lại thí nghiệm này không có qk-layernorm. Kết quả của chúng tôi cho thấy µParam không giảm bớt nhu cầu cho can thiệp này ở learning rates cao. Chúng tôi lưu ý rằng từ góc độ thực tiễn, việc giảm độ nhạy LR không quan trọng nếu LR tối ưu không thay đổi.

Chúng tôi gọi variant của µParam mà chúng tôi sử dụng trong những thí nghiệm này là µParam (simple) vì nó chỉ duy trì tính năng cốt lõi của µParam. Chúng tôi thêm các tính năng bổ sung từ Yang et al. [50] trong Hình Phụ lục E.5 không có cải thiện đo được ở quy mô lớn nhất mà chúng tôi kiểm tra. Cho µParam (simple) chúng tôi thực hiện các thay đổi sau từ baseline tiêu chuẩn của chúng tôi: scale LR cho linear layers bởi base-fan-in/fan-in. Cho µParam (full) có ba thay đổi bổ sung: (i) khởi tạo head với standard deviation √(base-fan-in/fan-in); (ii) thay đổi 1/√d_h scaling factor trong attention layers thành 1/d_h trong đó d_h là head dimension; và (iii) khởi tạo query projection weights với zeros. Cho base-fan-in chúng tôi sử dụng các giá trị fan-in cho mô hình nhỏ nhất mà chúng tôi kiểm tra, có width 256.

Chúng tôi bình luận ngắn gọn về các thay đổi (ii) và (iii) đã đề cập. Đầu tiên, chúng tôi ablate về thay đổi (ii) riêng biệt trong Hình Phụ lục E.6. Trong khi can thiệp này giảm loss một chút ở quy mô nhỏ nhất mà chúng tôi kiểm tra, điều ngược lại đúng cho quy mô lớn nhất mà chúng tôi kiểm tra. Ngoài ra, việc loại bỏ căn bậc hai khỏi scaling factor trong attention layers không giảm bớt nhu cầu cho qk-layernorm. Cuối cùng, về thay đổi (iii), chúng tôi lưu ý rằng trong các thí nghiệm sơ bộ thay đổi này không có hiệu ứng đáng chú ý.

3.2.5 Các can thiệp bổ sung
Phần này tái tạo các plots trước đó với các can thiệp bổ sung hoặc thay đổi siêu tham số. Các hình tương ứng được hiển thị trong phụ lục.

• Thay đổi số training steps từ 1e5 thành 5e4 hoặc 2e5 không thay đổi đáng kể độ nhạy LR (Hình Phụ lục E.7).

• Chúng tôi thử áp dụng qk-layernorm qua toàn bộ model dimension thay vì individually per-head với shared parameters. Như được minh họa trong Hình Phụ lục E.8, cái sau hoạt động tốt hơn. Chúng tôi sử dụng per-head qk-layernorm làm mặc định trong tất cả các thí nghiệm khác.

• Tăng batch size từ 256 lên 512 hoặc 1024 không thay đổi đáng kể độ nhạy LR (Hình Phụ lục E.9, mỗi batch element chứa 512 tokens). Khi tăng batch size chúng tôi giảm số training steps để lượng data seen không đổi. Chúng tôi tin rằng một hiệu ứng tương tự sẽ được quan sát nếu thay vào đó chúng tôi giữ số steps không đổi vì việc thay đổi số steps không có tác động đến độ nhạy LR ở batch size 256 (Hình Phụ lục E.7).

• Hiệu ứng của việc thay đổi weight decay từ 1e-4 được minh họa trong Hình E.10. Tăng decay dường như hơi dịch chuyển LR tối ưu sang phải.

• Chúng tôi thấy rằng bất ổn logit growth không phải do softmax trong self-attention layer, vì nó vẫn xảy ra với một variant pointwise của attention (Hình Phụ lục E.11).

3.3 Dự đoán bất ổn attention logit growth từ hành vi scaling của các đặc điểm mô hình
Một câu hỏi trung tâm khi nghiên cứu bất ổn là liệu chúng có thể được dự đoán không. Bây giờ chúng tôi xem xét liệu có thể dự đoán logit growth instability trước khi nó xảy ra không. Chúng tôi theo dõi attention logit maximums qua các quy mô mô hình và fit một đường cong với data. Chúng tôi sử dụng điều này để dự đoán rằng một mô hình 4.8B parameter sẽ không ổn định ở LR 1e-2 không có qk-layernorm và chạy một thí nghiệm để xác nhận dự đoán này.

Hình 9 plots số parameters vs. max attention logit ở các giá trị learning rate khác nhau.³ Ở mỗi learning rate, chúng tôi fit một quadratic để dự đoán cách max attention logit sẽ thay đổi với model scale. Chúng tôi đầu tiên nhận thấy rằng tất cả các điểm có attention logits trên 1e4 đều diverged. Hơn nữa, quadratic fit dự đoán rằng cho LR 1e-2 quy mô mô hình tiếp theo cũng sẽ vượt qua giá trị đó. Dựa trên dự đoán này, chúng tôi đào tạo một mô hình 4.8B parameter mới ở LR 1e-2. Mô hình này diverged như dự đoán. Không chỉ chúng tôi dự đoán divergence, mà fit của chúng tôi extrapolates chặt chẽ để dự đoán giá trị của max attention logit.

Một câu hỏi không được giải quyết bởi phân tích của chúng tôi cho đến nay là liệu chúng tôi có thể dự đoán rằng bất ổn nảy sinh khi max attention logit vượt quá 1e4 mà không thao tác learning rate và model size. Chúng tôi thực hiện các bước đầu tiên hướng tới một câu trả lời bằng cách transplanting các giá trị khác nhau của max attention logit vào một network nhỏ với 10M parameters. Cho các constants κ khác nhau chúng tôi truyền queries và keys qua g(z) = √κ · z/√(E_i[z_i²]) trước khi tính attention logits. Kết quả được minh họa trong Hình 10. Loss xấu đi khoảng κ = 1e3, và đến κ = 1e4 loss vượt quá của một zero-layer bigram model gồm Transformer mà chúng tôi sử dụng không có bất kỳ self-attention hoặc MLP layers nào.

3.4 Tìm kiếm bất ổn mới qua xu hướng scaling của các đặc điểm mô hình
Phần này xem xét liệu hành vi scaling của các đặc điểm mô hình có thể được sử dụng để dự đoán các vấn đề mới với các thiết lập mô hình và siêu tham số mặc định không.

Trong Hình 11 chúng tôi xem xét xu hướng scaling cho gradient root mean square RMS(g) = √(E_i[g_i²]). Hình này báo cáo RMS cho lớp đầu tiên của MLP, mặc dù chúng tôi quan sát xu hướng tương tự cho các lớp khác (Hình Phụ lục E.12).

Khi các mô hình trở nên lớn hơn, giá trị mà grad RMS tiếp cận là nguyên nhân gây quan ngại. Ở quy mô và learning rate lớn nhất mà chúng tôi kiểm tra, grad RMS là khoảng siêu tham số ε AdamW mặc định. Nhớ rằng unscaled AdamW update là Δ = v/(√u + ε), trong đó v và u tương ứng là first và second gradient moment EMA. Nếu grad RMS cùng bậc với ε, thì Δ sẽ giảm magnitude như được minh họa bởi Hình 13, và parameters sẽ không nhận được learning signals như dự định.

Một giảm thiểu rõ ràng cho vấn đề này là đơn giản lower siêu tham số ε AdamW từ mặc định 1e-8. Chúng tôi tiến hành thí nghiệm này cho một mô hình 4.8B parameter ở LR 0.3 và trình bày kết quả trong Hình 12. Giảm ε xuống 1e-15 cải thiện loss và giảm thiểu sự sụp đổ trong grad RMS. Chúng tôi tin rằng cải thiện này sẽ chỉ tăng ở scale. Mặt khác, tăng ε lên 1e-6 dẫn đến bất ổn (được hiển thị trong Hình E.15).

Hình 13 mở rộng kết quả này bằng cách minh họa grad và update RMS trong suốt đào tạo ở quy mô và learning rate lớn nhất mà chúng tôi kiểm tra. Khi grad RMS đạt ε, update RMS trở nên nhỏ. Hình E.13 trình bày data từ một thí nghiệm tương tự ở nhiều scales và LRs khác nhau, chứng minh rằng vấn đề này rõ ràng nhất đối với các mô hình và LRs lớn hơn mà chúng tôi kiểm tra.

Mặc dù chúng tôi xác định bất ổn ở trên bằng cách đo lường empirically hành vi scaling của gradients, một giải thích mechanistic tồn tại. Đối với các networks lớn hơn và learning rates, Transformer output RMS entering final layernorm có thể tăng trưởng. Vì layernorm gradients được scaled bởi nghịch đảo của input RMS của chúng, gradient nhận được bởi Transformer sẽ shrink. Tham khảo Phụ lục C để thảo luận chi tiết hơn.

4 Công trình liên quan
Paper này chủ yếu tập trung vào hiệu ứng của các can thiệp và bất ổn đã biết, và vì vậy công trình liên quan chủ yếu được thảo luận khi có liên quan. Điều này bao gồm bất ổn attention growth được quan sát bởi Dehghani et al. [11], Zhai et al. [51], và vấn đề final logit divergence gặp phải bởi Chowdhery et al. [6], Thilak et al. [44]. Tuy nhiên, chúng tôi highlight các phương pháp thực nghiệm tương tự trong công trình trước đó. Ví dụ, Yang et al. [50] cũng đo lường mối quan hệ giữa LR và loss qua các scales, nhưng focus của họ là centering optimum (xem Phần 3.2.4). Ngoài ra, Zhai et al. [51] elicit bất ổn trong base models bằng cách doubling learning rate, và Dettmers et al. [12] đo lường sự hiện diện của outlier features như một hàm của scale.

Cũng có những bất ổn quan trọng và chủ đề liên quan mà chúng tôi chưa thảo luận trực tiếp cho đến nay. Ví dụ, chúng tôi chủ yếu tập trung vào bất ổn dẫn đến divergence chậm, và bây giờ chúng tôi tóm tắt nghiên cứu về fast loss spikes. Bất ổn này được đặc trưng bởi sự gia tăng nhanh trong loss thường cuối cùng phục hồi.

Edge of Stability và fast spikes
Hiểu biết thông thường về gradient descent dự đoán rằng bất ổn loss chỉ xảy ra khi learning rate vượt quá 2/λ_max(H), trong đó H là Hessian. Tuy nhiên các điều tra gần đây về dynamics đào tạo neural network batch lớn đã tiết lộ một bức tranh phức tạp hơn qua edge of stability (EoS) [7]. Khi đào tạo neural networks với large batch SGD, loss curvature liên tục tiến hóa qua tương tác của hai quá trình: progressive sharpening và self stabilization. Progressive sharpening là quan sát empirical rằng khi LR < 2/λ_max(H), curvature dần dần tăng cho đến khi stability threshold bị vi phạm. Khi learning rate trở nên quá lớn so với curvature, fast loss spikes xảy ra và parameters oscillate vào một vùng với λ_max(H) nhỏ hơn nơi stable training và progressive sharpening tiếp tục. Quá trình sau nơi instability dẫn đến λ_max(H) nhỏ hơn là self-stabilization, một theoretical model của nó được đưa ra trong Damian et al. [9]. Gradually shrinking λ_max(H) qua self stabilization được chỉ ra là cơ chế chính đằng sau thành công của learning rate warmup trong Gilmer et al. [17], người nghiên cứu chặt chẽ các kết nối giữa curvature, initialization, architecture và max trainable learning rates.

Cohen et al. [8] phân tích thêm edge of stability của dynamics với adaptive optimizers, cho thấy progressive sharpening tương tác với cả quá trình self-stabilization và adaptive optimizer state. Tương tác này dẫn đến preconditioned sharpness λ_max(P^{-1}H) oscillating xung quanh một optimizer specific threshold (38/LR trong trường hợp Adam với β₁ = 0.9). Adaptive EoS (AEoS) cũng có thể dẫn đến periodic loss spikes khi progressive sharpening đẩy preconditioned sharpness lên trên stability threshold, tuy nhiên optimizer hyperparameters đóng một vai trò. Cụ thể, khi LR > 38/λ_max(P^{-1}H), hai cơ chế bây giờ đang hoạt động để giải quyết step size quá lớn—hoặc H có thể shrink hoặc P^{-1} có thể shrink (hoặc cả hai). Cohen et al. [8] thấy rằng khi β₂ lớn, H có xu hướng shrink và fast loss spikes dẫn đến trong quá trình, giống với quá trình self stabilization quan sát với gradient descent. Tuy nhiên khi β₂ nhỏ, P^{-1} có xu hướng shrink, không có loss spikes nào được quan sát, và λ_max(H) có xu hướng dần dần tăng trong suốt đào tạo.

Đáng chú ý rằng quá trình adaptive edge of stability (và vai trò của β₂) được nghiên cứu trong Cohen et al. [8] cung cấp một hiểu biết hoàn chỉnh hơn cho loss spikes được nghiên cứu trong một tập các tài liệu [42,6,35,47,52,5]. Ví dụ, Shazeer và Stern [42] lập luận rằng trong đào tạo Transformers với adaptive optimizers, optimizer update có thể trở nên quá lớn dẫn đến loss spike theo sau bởi recovery. Điều này đôi khi được quy cho adaptive optimizer state trở nên "stale", phù hợp với quan sát rằng giảm β₂ giải quyết loss spikes [42,47,52]. Điều này có lẽ cùng quan sát như Cohen et al. [8] rằng giảm β₂ cho phép P^{-1} thay đổi nhanh hơn để điều chỉnh với quá trình progressive sharpening. AEoS cũng cung cấp một giải thích cho periodic loss spikes được quan sát khi đào tạo large transformer models [35].

Parameter-free methods và more parameterizations. Trong khi công trình của chúng tôi đã nghiên cứu sensitivity với learning rate, cũng có nghiên cứu nhằm loại bỏ nhu cầu chỉ định learning rate [24,10]. Dựa trên phân tích của họ, Ivgi et al. [24] đặt step size cho iteration t thành maximum distance từ initialization chia cho root sum của historical gradient squares. Hơn nữa, trong khi công trình của chúng tôi điều tra µParam, có các parameterizations bổ sung mà sẽ thú vị để khám phá LR vs. loss [13, 48, 3, 25].

5 Kết luận
Khi compute cần thiết để đào tạo các mô hình lớn nhất tiếp tục tăng, việc hiểu liệu đào tạo có ổn định hay không trở nên ngày càng quan trọng. Paper này đã chỉ ra rằng những insight hữu ích về stability có thể được tìm thấy khi nghiên cứu các Transformers nhỏ. Chúng tôi hy vọng rằng điều này mở ra các cơ hội mới cho nghiên cứu có tác động mà mang lại lợi ích cho các lần chạy lớn mà không cần truy cập vào các nhóm tài nguyên lớn.

Lời cảm ơn
Chúng tôi cảm ơn George Dahl cho những bình luận và gợi ý kỹ lưỡng, và Hugo Larochelle và Rif A. Saurous cho thảo luận hữu ích. Ngoài ra, chúng tôi cảm ơn các thành viên của nhóm Google DeepMind PAGI cho sự hỗ trợ của họ cho nỗ lực này, Noah Fiedel, Noah Constant, Aaron Parisi, Alex Rizkowsky, Avi Singh, Azade Nova, Bernd Bohnet, Daniel Freeman, Gamaleldin Elsayed, Hanie Sedghi, Isabelle Simpson, James Harrison, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Max Bileschi, Merrie Morris, Rosanne Liu, Yundi Qian, Sharad Vikram, Tris Warkentin.
