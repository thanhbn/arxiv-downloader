# 2110.08232.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/hebbian/2110.08232.pdf
# Kích thước tệp: 3058739 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bắn Cùng Nhau, Nối Cùng Nhau:
Một Phương Pháp Cắt Tỉa Động với Dự Đoán Mặt Nạ Tự Giám Sát
Sara Elkerdawy1Mostafa Elhoushi2Hong Zhang1
Nilanjan Ray1
1Đại học Alberta,2Phòng thí nghiệm Trình biên dịch Dị hình Toronto, Huawei
{elkerdaw, hzhang, nray1 }@ualberta.ca
Tóm tắt
Cắt tỉa mô hình động là một hướng nghiên cứu gần đây cho phép suy luận một mạng con khác nhau cho mỗi mẫu đầu vào trong quá trình triển khai. Tuy nhiên, các phương pháp động hiện tại dựa vào việc học một cổng kênh liên tục thông qua regularization bằng cách tạo ra loss thưa thớt. Công thức này đưa ra sự phức tạp trong việc cân bằng các loss khác nhau (ví dụ: loss nhiệm vụ, loss regularization). Ngoài ra, các phương pháp dựa trên regularization thiếu lựa chọn siêu tham số đánh đổi minh bạch để thực hiện ngân sách tính toán. Đóng góp của chúng tôi gồm hai phần: 1) tách biệt loss nhiệm vụ và cắt tỉa. 2) Lựa chọn siêu tham số đơn giản cho phép ước tính giảm FLOPs trước khi huấn luyện. Lấy cảm hứng từ lý thuyết Hebbian trong Khoa học Thần kinh: "các tế bào thần kinh bắn cùng nhau sẽ nối cùng nhau", chúng tôi đề xuất dự đoán một mặt nạ để xử lý k bộ lọc trong một lớp dựa trên kích hoạt của lớp trước đó. Chúng tôi đặt vấn đề như một bài toán phân loại nhị phân tự giám sát. Mỗi mô-đun dự đoán mặt nạ được huấn luyện để dự đoán liệu log-likelihood cho mỗi bộ lọc trong lớp hiện tại có thuộc top-k bộ lọc được kích hoạt hay không. Giá trị k được ước tính động cho mỗi đầu vào dựa trên một tiêu chí mới sử dụng khối lượng của heatmap. Chúng tôi thực hiện thí nghiệm trên nhiều kiến trúc mạng neural, như VGG, ResNet và MobileNet trên các tập dữ liệu CIFAR và ImageNet. Trên CIFAR, chúng tôi đạt độ chính xác tương tự các phương pháp SOTA với việc giảm FLOPs cao hơn 15% và 24%. Tương tự trên ImageNet, chúng tôi đạt được sự sụt giảm độ chính xác thấp hơn với cải thiện lên đến 13% về giảm FLOPs. Mã nguồn có tại https://github.com/selkerdawy/FTWT

1. Giới thiệu
Mạng Nơ-ron Tích chập (CNN) đã có sự phát triển chưa từng có trong thập kỷ qua, đại diện cho công nghệ tiên tiến nhất trong nhiều lĩnh vực. Tuy nhiên, CNN yêu cầu tiêu thụ tính toán và bộ nhớ lớn, điều này hạn chế việc triển khai trên các nền tảng edge và nhúng.

[Hình 1. Giảm FLOPs so với sự sụt giảm độ chính xác từ baseline cho các mô hình động và tĩnh khác nhau trên ResNet34 ImageNet.]

Có nhiều tiến bộ trong nghiên cứu nén mô hình bao gồm các mô hình nhẹ được thiết kế thủ công [16, 17], độ chính xác bit thấp [24, 51], tìm kiếm kiến trúc [4, 42], và cắt tỉa mô hình [9,22,31,36]. Hầu hết các kỹ thuật nén đều bất khả tri với dữ liệu đầu vào và tối ưu hóa cho một mô hình hiệu quả tĩnh. Những nỗ lực gần đây trong tài liệu cắt tỉa đề xuất giữ backbone cho mô hình baseline như một tổng thể và thực hiện suy luận sử dụng các mạng con khác nhau có điều kiện trên đầu vào. Điều này được biết đến như cắt tỉa động, nơi các tuyến đường khác nhau được kích hoạt dựa trên đầu vào, cho phép mức độ tự do cao hơn và tính linh hoạt hơn so với cắt tỉa tĩnh.

Các phương pháp cắt tỉa động hiện tại thường đưa ra một thuật ngữ regularization để tạo ra độ thưa thớt trên một tham số liên tục cho cổng kênh/che mặt [6, 10, 44]. Những phương pháp khác áp dụng gradient chính sách được giới thiệu trong học tăng cường [46] để học các tuyến đường khác nhau. Những phương pháp này yêu cầu điều chỉnh cẩn thận trong huấn luyện để giải quyết các vấn đề như tính ổn định huấn luyện với schedule annealing [44], xử lý huấn luyện thiên vị [18], hoặc tỷ lệ cắt tỉa được xác định trước cho mỗi lớp [10, 27]. Ngoài ra, như đã lưu ý trong [6], loss thưa thớt bổ sung làm giảm loss nhiệm vụ

--- TRANG 2 ---
[Hình 2. Kích hoạt tối đa trong tất cả các đặc trưng ở lớp tích chập cuối cùng và một lớp giữa trong mobilenetv1 CIFAR-10. Mỗi hàng trong một biểu đồ con đại diện cho một mẫu đầu vào. Các mẫu thuộc cùng một lớp kích hoạt cùng nhóm bộ lọc. Được hình dung tốt hơn khi có màu.]

vì khó cân bằng loss nhiệm vụ và loss cắt tỉa, đặc biệt dưới tỷ lệ cắt tỉa cao như thể hiện trong Hình 1. Hơn nữa, việc giảm FLOPs của những phương pháp động này phụ thuộc vào siêu tham số độ thưa thớt mục tiêu được đặt trước. Việc lựa chọn siêu tham số này thiếu mối quan hệ minh bạch giữa siêu tham số độ thưa thớt và FLOPs đạt được; do đó, cản trở việc huấn luyện hiệu quả thực tế với nhiều lần lặp thử và sai để đạt được mức giảm FLOPs mục tiêu.

Trong bài báo này, chúng tôi giải quyết những vấn đề này bằng cách hình thành vấn đề như một nhiệm vụ phân loại nhị phân tự giám sát. Chúng tôi tạo ra mặt nạ nhị phân của lớp hiện tại (nối dây) dựa trên kích hoạt (bắn) của lớp trước đó. Chúng tôi lấy cảm hứng từ lý thuyết Hebbian [33] trong Khoa học Thần kinh với một khúc mắc rằng chúng tôi thực thi mối quan hệ nối-bắn này thay vì nghiên cứu về tính nhân quả như trong lý thuyết. Hình 2 vẽ biểu đồ phản hồi tối đa cho mỗi bộ lọc (trục x) của lớp tích chập cuối cùng và một lớp giữa của MobileNet-V1 cho các mẫu đầu vào ngẫu nhiên (trục y) được nhóm theo lớp của chúng. Biểu đồ cho thấy các mẫu thuộc cùng một lớp có xu hướng kích hoạt cùng một tổ hợp bộ lọc và do đó chúng tôi chỉ cần xử lý một số ít bộ lọc. Đáng lưu ý rằng số lượng cụm thay đổi theo lớp. Tương tự như các phương pháp cắt tỉa động khác, chúng tôi học một head quyết định cho cổng kênh. Tuy nhiên, chúng tôi học cổng bằng cách sử dụng loss entropy chéo nhị phân cho mỗi kênh. Mỗi lớp dự đoán các bộ lọc có khả năng được kích hoạt cao nhất khi được cho các kích hoạt đầu vào của lớp. Chúng tôi tạo ra mặt nạ nhị phân ground truth cho mỗi lớp dựa trên khối lượng của heatmap cho mỗi mẫu. Công thức này cung cấp lợi thế trong hai khía cạnh. Đầu tiên, loss cổng kênh tuân thủ ngầm định và thích ứng với trạng thái của backbone, điều này ổn định huấn luyện so với trường hợp với regularization thưa thớt hoặc huấn luyện dựa trên RL. Thứ hai, việc giảm FLOPs có thể được ước tính trước khi huấn luyện, vì mặt nạ mục tiêu được kiểm soát bởi mặt nạ ground truth được tạo ra, điều này cho một ước tính về việc giảm. Điều này đơn giản hóa việc lựa chọn siêu tham số kiểm soát tỷ lệ cắt tỉa. Các đóng góp chính được tóm tắt như sau:

• Một công thức loss mới với việc tạo ra mặt nạ ground truth tự giám sát thân thiện với stochastic gradient descent (SGD) mà không cần các thủ thuật weighting gradient.

• Chúng tôi đề xuất một chữ ký động mới dựa trên khối lượng heatmap mà không cần tỷ lệ cắt tỉa được xác định trước cho mỗi lớp.

• Lựa chọn siêu tham số đơn giản cho phép ước tính giảm FLOPs trước khi huấn luyện. Điều này đơn giản hóa việc thực hiện mục tiêu ngân sách trước với không gian tìm kiếm siêu tham số có giới hạn.

2. Công trình Liên quan
Cắt tỉa Tĩnh. Cắt tỉa tĩnh loại bỏ trọng số trong giai đoạn huấn luyện offline và áp dụng cùng một mô hình nén cho tất cả các mẫu. Cắt tỉa không có cấu trúc [9, 11, 32, 40] nhắm mục tiêu loại bỏ các trọng số riêng lẻ có đóng góp tối thiểu. Hạn chế của cắt tỉa trọng số không có cấu trúc là cần phần cứng và thư viện chuyên dụng [40] để đạt được tăng tốc từ việc nén. Cắt tỉa có cấu trúc đang trở thành một giải pháp thực tế hơn, nơi các bộ lọc hoặc khối được xếp hạng và cắt tỉa dựa trên một tiêu chí [7,13,15,29,34,36]. Các phương pháp cắt tỉa bộ lọc trước đây [23,34] yêu cầu tính toán

--- TRANG 3 ---
[Hình 3. Pipeline đề xuất cho huấn luyện định tuyến động cho một lớp. Đối với lớp l, head dự đoán fl_p(Il;Wpl) nhận đầu vào Il, áp dụng global max pooling (GMP), chuẩn hóa với Softmax, sau đó đưa vào tích chập 1x1 để tạo ra logits Pl cho mặt nạ nhị phân Ml. Loss Binary Cross Entropy (BCEWithLogits) phạt việc dự đoán mặt nạ dựa trên top-k thu được từ bản đồ đặc trưng không bị cắt tỉa Ol.]

phân tích độ nhạy theo lớp để tạo ra chữ ký mô hình (tức là số lượng bộ lọc cho mỗi lớp) trước khi cắt tỉa. Phân tích độ nhạy tốn kém về mặt tính toán, đặc biệt khi các mô hình trở nên sâu hơn. Các phương pháp gần đây [30, 36, 45] học một thước đo quan trọng toàn cục. Molchanov et al. [36] đề xuất một phép gần đúng Taylor trên trọng số của mạng, nơi gradient và norm của bộ lọc được sử dụng để gần đúng điểm số quan trọng toàn cục của nó. Liu et al. [30] và Wen et al. [45] giới thiệu một loss thưa thớt ngoài loss nhiệm vụ như một regularization sau đó cắt tỉa các bộ lọc có tiêu chí nhỏ hơn một ngưỡng.

Cắt tỉa Động. Trái ngược với triển khai một-mô-hình-phù-hợp-tất-cả như trong cắt tỉa tĩnh, cắt tỉa động xử lý các tuyến đường khác nhau cho mỗi mẫu đầu vào. Tương tự như cắt tỉa tĩnh, các phương pháp có thể áp dụng độ chi tiết khác nhau để cắt tỉa. Mạng cổng kênh (CGNet) [18] là một phương pháp chi tiết nhỏ bỏ qua các vị trí không trong bản đồ đặc trưng. Một tập con của các kênh đầu vào được xử lý bởi mỗi kernel. Cổng quyết định được học thông qua regularization với một hàm không khả vi gần đúng phức tạp. Họ áp dụng tích chập nhóm và thao tác xáo trộn để cân bằng tần suất cập nhật bộ lọc từ các nhóm đặc trưng khác nhau. Gần nhất với công trình của chúng tôi, chúng tôi tập trung vào các phương pháp cắt tỉa bộ lọc động. Trong Runtime Neural Pruning (RNP) [27], một đơn vị quyết định được mô hình hóa như một lớp tái phát toàn cục, tạo ra các hành động rời rạc tương ứng với bốn nhóm lựa chọn kênh được đặt trước. Việc lựa chọn nhóm được huấn luyện với học tăng cường. Tương tự, trong BlockDrop [47], một mạng chính sách được huấn luyện để bỏ qua các khối trong mạng tồn dư thay vì chỉ các kênh.

D2NN [28] định nghĩa một tập hợp biến thể các nhánh có điều kiện trong DNN, và sử dụng Q-learning để huấn luyện các chính sách phân nhánh. Những phương pháp này huấn luyện các hàm chính sách của chúng bằng học tăng cường, có thể là một nhiệm vụ tối ưu hóa tốn kém không tầm thường cùng với backbone CNN. Phương pháp Feature Boosting and Suppression (FBS) [10] tạo ra sự nổi bật kênh liên tục và sử dụng tỷ lệ cắt tỉa được xác định trước cho mỗi lớp để có được một cổng nhị phân rời rạc. LCS [44] đề xuất có được một hành động rời rạc từ N nhóm kênh đã học được lấy mẫu từ phân phối Gumbel. Họ áp dụng nhiệt độ annealing để ổn định huấn luyện và giới thiệu sự đa dạng trong các tuyến đường đã học bằng thủ thuật chuẩn hóa gradient. Những phương pháp dựa trên regularization hiện tại này yêu cầu điều chỉnh cẩn thận bổ sung để ổn định huấn luyện được gây ra bởi gradient chính sách hoặc để thực thi việc học các tuyến đường đa dạng để không hội tụ về một cắt tỉa tĩnh. Do đó, chúng tôi đề xuất hình thành việc lựa chọn kênh như một phân loại nhị phân tự giám sát, trong đó các tuyến đường có thể giải thích có thể được nghiên cứu và đơn giản huấn luyện với SGD thông thường.

3. Phương pháp học
Trong phần này, trước tiên chúng tôi giải thích cơ chế cho cổng động. Sau đó, chúng tôi thảo luận về cách chúng tôi thiết kế các head quyết định và loss có giám sát

--- TRANG 4 ---
3.1. Cổng Kênh
Gọi Il, Wl là các đặc trưng đầu vào, và trọng số của một lớp tích chập l, tương ứng, trong đó Il∈Rcl−1×wl×hl, Wl∈Rcl×cl−1×kl×kl, và cl là số lượng bộ lọc trong lớp l. Một khối CNN điển hình gồm một phép toán tích chập (*), chuẩn hóa batch (BN), và một hàm kích hoạt (f) như ReLU được sử dụng phổ biến. Không mất tính tổng quát, chúng tôi bỏ qua thuật ngữ bias vì có sự bao gồm BN, do đó, bản đồ đặc trưng đầu ra Ol có thể được viết như Ol=f(BN(Il∗Wl)). Chúng tôi dự đoán một mặt nạ nhị phân Ml∈Rcl biểu thị các bản đồ đặc trưng đầu ra Ol được kích hoạt cao từ bản đồ kích hoạt đầu vào Il bằng cách áp dụng một head quyết định fl_p với các tham số có thể học Wl_p. Đầu ra đã được che mặt Il+1 sau đó được biểu diễn như Il+1=Ol⊙Binarize(fl_p(Il;Wp)). Hàm Binarize(.) là round(Sigmoid(.)) để chuyển đổi logits thành mặt nạ nhị phân. Việc dự đoán các bản đồ đặc trưng đầu ra được kích hoạt cao cho phép xử lý các bộ lọc f trong đó Ml_f=1 trong thời gian suy luận và bỏ qua phần còn lại. Head quyết định của chúng tôi có chi phí cl−1×cl FLOPs cho mỗi lớp l, điều này có thể bỏ qua.

3.2. Cổng Nhị phân Tự Giám sát
Phương pháp đề xuất của chúng tôi như thể hiện trong Hình 3 học định tuyến động này theo cách tự giám sát bằng cách chèn một head dự đoán sau mỗi khối tích chập để dự đoán k bộ lọc được kích hoạt cao của lớp tiếp theo. Giá trị k được tính toán tự động cho mỗi đầu vào dựa trên khối lượng của heatmap.

Hàm loss. Mặt nạ nhị phân ground truth của các đặc trưng được kích hoạt cao có thể đạt được bằng cách sắp xếp norm của các đặc trưng. Mục tiêu huấn luyện tổng thể là:

min_{W,Wp} Ltotal = Lent(fn(x;W),yk) + Lpred({fl_p(Il;Wpl),gl}L)    (1)

trong đó fn là backbone của mô hình baseline, Lent là loss entropy chéo nhiệm vụ, Lpred là tổng loss dự đoán cho tất cả các lớp l∈1...L. Cụ thể, chúng tôi định nghĩa Lpred như sau:

Lpred({Pl,gl}L) = Σ_{l=1}^L Σ_{f=1}^{Fl} BCEWithLogits(Pl_f,gl_f)    (2)

trong đó Pl là đầu ra của head quyết định fl_p(Il;Wpl), gl là mặt nạ ground truth được tạo ra dựa trên top-k đầu ra được kích hoạt cao Ol, BCEWithLogits là một Sigmoid theo sau bởi loss entropy chéo nhị phân BCE(p,g)=−(glog(p)+(1−g)log(1−p)).

Số lượng k được kích hoạt. Chúng tôi tự động tính toán k bằng cách giữ một tỷ lệ phần trăm r không đổi của khối lượng heatmap. Đối với mỗi kênh i=1,...,cl, chúng tôi giữ phản hồi tối đa bằng cách áp dụng global maximum pooling (GMP) (GMP(Ol_i)). Đối với mỗi ví dụ đầu vào, k là số lượng bộ lọc được giữ sao cho khối lượng tích lũy của kích hoạt chuẩn hóa được sắp xếp đạt r%. Thuật toán tạo ground truth được thể hiện trong Thuật toán 1. Chúng tôi sử dụng cùng r cho tất cả các lớp, tuy nhiên, mỗi mẫu sẽ có tỷ lệ cắt tỉa khác nhau cho mỗi lớp dựa trên kích hoạt của nó. Vì ground truth nhị phân mục tiêu được tạo ra từ các kích hoạt của các bộ lọc không bị che mặt, việc giảm FLOPs có thể được ước tính một cách lỏng lẻo trước khi huấn luyện để điều chỉnh r cho phù hợp. Lợi thế này thêm vào tính thực tế của phương pháp của chúng tôi, không dựa vào điều chỉnh siêu tham số gián tiếp để đạt được mục tiêu ngân sách trong việc giảm FLOPs. Cũng đáng lưu ý rằng r=1 là một trường hợp đặc biệt cho biết head quyết định sẽ dự đoán các đặc trưng hoàn toàn bị vô hiệu hóa (ví dụ: phản hồi tối đa bằng không). Điều này cho phép duy trì độ chính xác của baseline với head quyết định được huấn luyện lý tưởng cho các backbone có độ thưa thớt cao.

Thuật toán 1 Tạo ground truth mặt nạ nhị phân
Đầu vào: I1...IL, r
Đầu ra: g ground truth nhị phân với 0 là cắt tỉa
1: gt ← ones(L, cl)
2: for l ← 1 to L do
3:   acts ← GMP((Ol))
4:   normalized ← acts/Σacts
5:   sorted, idx ← SORT(normalized, "descend")
6:   cumulative ← CUMSUM(sorted)
7:   prune_idx ← WHERE(cumulative > r)
8:   gt[l][prune_idx] ← 0
9: end for

3.3. Thiết kế Head Dự đoán
Thiết kế head dự đoán nên được mô hình hóa một cách đơn giản để giảm chi phí phụ trội so với mạng baseline. Trong lượt truyền xuôi, chúng tôi áp dụng GMP làm giảm bản đồ đặc trưng Il cho mỗi lớp thành El∈Rcl−1×1×1. Tiếp theo, chúng tôi áp dụng tích chập 1x1 trên embedding được làm phẳng El để tạo ra logits của mặt nạ. Chúng tôi thí nghiệm với hai chế độ huấn luyện: 1) tách biệt, và 2) liên kết. Trong cả hai chế độ, chúng tôi huấn luyện trọng số backbone và của các head quyết định song song. Sự khác biệt là liệu chúng tôi thực hiện huấn luyện hoàn toàn khả vi (liên kết) hay dừng gradient từ các head để lan truyền ngược về backbone và ngược lại (tách biệt). Trong huấn luyện liên kết, head quyết định hoàn toàn khả vi trừ phần nhị phân hóa. Tương tự như các công trình trước đây [8, 35, 50], chúng tôi sử dụng straight-through estimator (STE) để bỏ qua hàm không khả vi. Một vấn đề cần xem xét là sự can thiệp của các loss với nhiều loss ở độ sâu khác nhau trong mạng như đã chỉ ra trong [19]. Sự can thiệp của loss nhấn mạnh rằng các bản đồ đặc trưng có thể bị thiên vị hướng tới

--- TRANG 5 ---
[Bảng 1. Kết quả trên CIFAR-10. FLOPs red. chỉ ra sự giảm FLOPs tính theo phần trăm. r trong phương pháp của chúng tôi biểu thị tỷ lệ siêu tham số trong Thuật toán 1. x trong FTWT x chỉ ra huấn luyện liên kết (J) hoặc tách biệt (D).]

đạt được độ chính xác cao cho nhiệm vụ cục bộ hơn là toàn bộ kiến trúc. Không giống như các phương pháp khác dựa vào điều chỉnh huấn luyện cẩn thận để quản lý gradient từ các loss khác nhau, chúng tôi huấn luyện các head cùng với backbone song song nhưng hợp tác vì các mặt nạ được tạo ra từ trạng thái hiện tại của mô hình. Các mặt nạ nhị phân ground truth được điều chỉnh một cách rõ ràng bởi các trọng số backbone đã cập nhật, do đó, tuân thủ ngầm định tốc độ học của backbone.

4. Thí nghiệm và Phân tích
Chúng tôi đánh giá phương pháp của mình trên các tập dữ liệu CIFAR [20] và ImageNet [5] trên nhiều kiến trúc như VGG [41], ResNet [12] và MobileNet [16]. Trong tất cả các kiến trúc, các mặt nạ ground truth được tạo ra sau mỗi khối conv-BN-ReLU. Đối với các mô hình baseline CIFAR, chúng tôi huấn luyện trong 200 epoch sử dụng batch-size 128 với bộ tối ưu SGD. Tốc độ học ban đầu (lr) 0.1 được chia cho 10 tại các epoch 80, 120, và 150. Chúng tôi sử dụng momentum 0.9 với weight decay 5−4. Đối với ImageNet, chúng tôi sử dụng các mô hình được huấn luyện trước trong PyTorch [39] như baseline. Trọng số của các head quyết định được huấn luyện với 0.1 như lr ban đầu và cùng lịch trình lr như backbone. Chúng tôi sử dụng một máy 4 V100-GPU trong các thí nghiệm của mình.

4.1. Thí nghiệm trên CIFAR
Chúng tôi theo các thiết lập huấn luyện tương tự được sử dụng trong baseline cho huấn luyện động, nhưng chúng tôi huấn luyện tất cả các mô hình với tốc độ học ban đầu 1e−2. Chúng tôi báo cáo độ chính xác trung bình trên ba thí nghiệm lặp lại và giảm FLOPs trên CIFAR-10 trên nhiều kiến trúc trong Bảng 1. Phương pháp của chúng tôi (FTWT) đạt được giảm FLOPs cao hơn với độ chính xác top1 tương tự so với các phương pháp cắt tỉa tĩnh và động. Chúng tôi đạt lên đến 66% giảm FLOPs trên VGG-16 và ResNet-56, cao hơn các phương pháp cắt tỉa bộ lọc động RNP [27], FBS [10], LCS [44] lên đến 15%. Huấn luyện liên kết hoạt động tốt như huấn luyện tách biệt trên các ngưỡng r cao. Tuy nhiên, độ chính xác giảm so với huấn luyện tách biệt trên các ngưỡng thấp hơn. Điều này do sự gia tăng xung đột giữa các loss có thể thấy trên giảm FLOPs ≈73% trên VGG. Chúng tôi tiếp tục đạt 73% giảm FLOPs trên VGG với chỉ 0.63% sụt giảm độ chính xác. Hơn nữa, FTWT vượt trội hơn các biến thể nhỏ hơn của MobileNet về độ chính xác 3.42% với giảm FLOPs cao hơn.

Chúng tôi hình dung số lượng tổ hợp duy nhất của các bộ lọc (cụm) được kích hoạt trên toàn bộ tập dữ liệu D và tỷ lệ cắt tỉa cho mỗi lớp trong Hình 4. Có nghĩa là, mỗi

--- TRANG 6 ---
[Hình 4. Phân phối MobileNetV1 CIFAR10]

mẫu i tạo ra một mặt nạ nhị phân mi,j cho mỗi lớp j, các cụm duy nhất cho mỗi lớp là set(m0,j, ..., mi,j, ...m|D|,j). Trong LCS và RNP, một số cụm cố định được đặt trước như một siêu tham số cho tất cả các lớp, chúng tôi thể hiện trong Hình 4a rằng các lớp khác nhau về số lượng cụm đa dạng. Phương pháp của chúng tôi điều chỉnh số lượng cụm khác nhau cho mỗi lớp một cách tự động do cơ chế tạo mặt nạ tự giám sát. Để dễ hình dung, trục y được thể hiện ở thang logarit. Các lớp đầu có sự đa dạng nhỏ trong nhóm bộ lọc được kích hoạt, do đó, hoạt động tương tự như cắt tỉa tĩnh. Điều này hợp lý vì các lớp đầu phát hiện các đặc trưng cấp thấp và có sự phụ thuộc ít hơn vào đầu vào. Mặt khác, số lượng cụm tăng khi chúng ta đi sâu hơn vào mạng. Đáng lưu ý rằng những cụm khác nhau này có độ chi tiết nhỏ, có nghĩa là các cụm có thể chỉ khác nhau một bộ lọc. Chúng tôi cũng tính toán phần trăm bộ lọc lõi được chia sẻ giữa tất cả các cụm cho mỗi lớp. Chúng tôi thấy rằng phạm vi phần trăm bộ lọc lõi so với tổng số bộ lọc thay đổi từ 0.4 đến 1.0. Điều này cho cái nhìn sâu sắc về lý do tại sao các phương pháp cắt tỉa tĩnh dẫn đến sự sụt giảm lớn về độ chính xác với cắt tỉa lớn. Vì tỷ lệ cắt tỉa có thể đạt được bị giới hạn bởi số lượng bộ lọc lõi và việc cắt tỉa thêm sẽ hạn chế khả năng của mô hình. Một câu hỏi nghiên cứu tương lai thú vị sẽ là liệu chúng ta có thể xác định khả năng nén của một mô hình dựa trên khái niệm tỷ lệ bộ lọc lõi. Cuối cùng, Hình 4b thể hiện tỷ lệ cắt tỉa cho mỗi lớp, như mong đợi, các lớp sau được cắt tỉa mạnh hơn các lớp đầu

vì các lớp trở nên rộng hơn và có thể nén được hơn. Chúng ta nhận thấy cắt tỉa mạnh đạt 85% ở các lớp giữa với một chuỗi lớp có 512 bộ lọc.

4.2. Thí nghiệm trên ImageNet
Đối với ImageNet, chúng tôi huấn luyện trong 90 epoch với tốc độ học ban đầu 10−2 giảm mỗi 30 epoch với 0.1. Thí nghiệm trên ImageNet được thực hiện với chế độ huấn luyện tách biệt. Bảng 3 thể hiện sự sụt giảm độ chính xác từ baseline cho mỗi phương pháp để tính đến sự khác biệt huấn luyện do tăng cường. Kết quả cho thấy phương pháp của chúng tôi đạt được sự sụt giảm độ chính xác nhỏ hơn với giảm FLOPs cao hơn so với các phương pháp SOTA khác. Chúng tôi đạt sự giảm độ chính xác tương tự như LCCL với 13% giảm FLOPs cao hơn trên ResNet34. Mặt khác, trên giảm FLOPs tương tự (≈25), chúng tôi có sự sụt giảm tối thiểu về độ chính xác (≈0.05%). Mặc dù chúng tôi đạt độ chính xác tương tự trên ResNet18 với FBS, phương pháp sau yêu cầu số lượng bộ lọc được xác định trước để giữ cho mỗi lớp. Mặt khác, phương pháp của chúng tôi gán động tỷ lệ cắt tỉa cho mỗi lớp, điều này cho thấy hiệu quả của khối lượng heatmap như một tiêu chí. Chúng tôi cũng so sánh với các biến thể nhỏ hơn của kiến trúc như ResNet18 và MobileNet-75. Chúng tôi vượt trội hơn ResNet18 và MobileNet-75 ≈2% về độ chính xác trên ngân sách tính toán tương tự.

[Bảng 2. Kết quả trên ImageNet. Độ chính xác baseline cho mỗi phương pháp được báo cáo cùng với độ chính xác của mô hình đã cắt tỉa và thay đổi độ chính xác từ baseline. FLOPs red. đại diện cho giảm FLOPs tính theo phần trăm. Delta âm cho thấy tăng độ chính xác từ baseline. r trong phương pháp của chúng tôi biểu thị tỷ lệ siêu tham số trong Thuật toán 1.]

4.3. Nghiên cứu Khử bỏ
4.3.1 Không chắc chắn dưới Dataset Shift
Trong phần này, chúng tôi đo lường độ nhạy của định tuyến của chúng tôi với dataset shift. Các metric dưới dataset shift hiếm khi được kiểm tra trong tài liệu cắt tỉa mô hình. Chúng tôi tin vào tầm quan trọng của nó vì độ phức tạp suy luận tăng lên và do đó muốn bắt đầu báo cáo những so sánh như vậy. Chúng tôi tiến hành thí nghiệm cho VGG16-bn CIFAR-10 với tỷ lệ cắt tỉa cao 73% cho tất cả các mô hình đã cắt tỉa. Lấy cảm hứng từ [37], chúng tôi báo cáo điểm Brier [3] dưới các loại nhiễu khác nhau như làm mờ Gaussian Bảng 3a và nhiễu cộng Bảng 3b với mô hình dày đặc baseline như tham chiếu. Như có thể thấy, phương pháp của chúng tôi có khả năng chịu đựng tốt hơn so với cắt tỉa Taylor tĩnh với điểm brier thấp hơn. Chúng tôi cũng so sánh với cắt tỉa đồng nhất tĩnh, chúng tôi đạt điểm Brier tương tự (đôi khi thấp hơn một chút). Điều này cho thấy khả năng phục hồi của mô hình của chúng tôi với data shift ngay cả khi so sánh với quyết định cắt tỉa tĩnh không phụ thuộc vào dữ liệu. Cuối cùng, như mong đợi, mô hình dày đặc có khả năng chịu đựng nhiễu nhất. Tuy nhiên, phương pháp của chúng tôi vẫn cho thấy chất lượng khá tốt nhìn chung. Chúng tôi quy chất lượng ổn định phân phối này cho softmax trong head. Softmax hoạt động như một bộ chuẩn hóa giảm độ nhạy với dataset shift. Chúng tôi so sánh phương pháp của mình có và không có chuẩn hóa softmax trong head quyết định để xác minh giả thuyết này. Bảng 3c thể hiện điểm Brier với nhiễu cộng và làm mờ cho so sánh này. Như có thể thấy, thực sự, bộ chuẩn hóa ổn định đầu ra mặt nạ quyết định đặc biệt trong trường hợp làm mờ.

[Bảng 3. Thí nghiệm dataset shift: Các số biểu thị điểm Brier trên CIFAR-10 VGG16]

4.3.2 Chữ ký Động và Định tuyến Động
Chúng tôi điều tra việc tách biệt hiệu ứng từ chữ ký động (tức là tỷ lệ cắt tỉa cho mỗi lớp) cho mỗi mẫu từ định tuyến động (tức là nhóm bộ lọc được kích hoạt). Chúng tôi khám phá hiệu quả của định tuyến động với một chữ ký được xác định trước cho tất cả các đầu vào. Trong những thí nghiệm này, chữ ký được xác định trước sử dụng tiêu chí Taylor được đề xuất trong [36] như một nghiên cứu trường hợp. Như trong thiết lập trước đây, chúng tôi chọn k đặc trưng được kích hoạt cao nhất trong đó k được định nghĩa bởi chữ ký trong khi các mẫu khác nhau về k bộ lọc nào được chọn. Bảng 6 thể hiện kết quả của định tuyến động dưới các tỷ lệ cắt tỉa khác nhau. Như có thể thấy, định tuyến động hoạt động tốt hơn suy luận tĩnh đặc biệt ở tỷ lệ cắt tỉa cao lên đến 4%. Thiết lập huấn luyện tương tự như cho CIFAR được giải thích trong bài báo, tuy nhiên, chúng tôi huấn luyện các mô hình ImageNet trong 30 epoch sử dụng thiết lập finetune thay vì thiết lập huấn luyện với 90 epoch như được phân biệt trong [31] để tăng tốc thí nghiệm.

4.3.3 Lựa chọn Siêu tham số r
Siêu tham số r (tức là tỷ lệ khối lượng) được chọn dựa trên một đánh giá đơn giản trước khi huấn luyện. Chúng tôi ước tính FLOPs trước khi huấn luyện bằng cách áp dụng các mặt nạ ground truth trên mô hình dày đặc đông lạnh được huấn luyện trước trên tập huấn luyện (lượt truyền một lần). Sau đó, chúng tôi có một ước tính trước khi huấn luyện được bắt đầu về việc giảm FLOPs mong đợi dưới các giá trị r khác nhau. Điều này đơn giản hóa việc lựa chọn siêu tham số để đạt được giảm FLOPs mục tiêu. Mặt khác, siêu tham số regularization thưa thớt thường được tinh chỉnh với một quá trình xác thực chéo và yêu cầu thử và sai của nhiều huấn luyện đầy đủ để đạt được ngân sách mục tiêu. Không có mối quan hệ trực tiếp giữa trọng số regularization và việc giảm FLOPs cuối cùng đạt được một cách rõ ràng trước khi huấn luyện. Phương pháp của chúng tôi đơn giản hóa việc lựa chọn và làm cho nó trở thành một lựa chọn thực tế hơn khi một ngân sách mục tiêu được cho trước. Bảng 4 thể hiện FLOPs ước tính trước khi huấn luyện sử dụng các ngưỡng khác nhau, r, và việc giảm FLOPs thực tế đạt được sau khi huấn luyện. Sự khác biệt trong việc giảm là do sự không chính xác của các head quyết định. Tuy nhiên, FLOPs ước tính cho một phép gần đúng tốt cho FLOPs cuối cùng đạt được và do đó giảm tìm kiếm siêu tham số.

[Bảng 4. FLOPs ước tính trước khi huấn luyện dưới các ngưỡng khác nhau (được chỉ ra trong ngoặc đơn) so với FLOPs đạt được sau khi huấn luyện.]

4.4. Tăng tốc Lý thuyết so với Thực tế
Đối với tất cả các phương pháp nén, bao gồm cắt tỉa tĩnh và động, thường có khoảng cách rộng giữa việc giảm FLOPs và tăng tốc thực tế do các yếu tố khác như độ trễ I/O và thư viện BLAS. Tăng tốc phụ thuộc vào phần cứng và backend như thể hiện trong các công trình trước đây [1, 7, 49]. Chúng tôi kiểm tra tốc độ thực tế trên PyTorch [39] sử dụng backend MKL trên CPU AMD sử dụng một luồng duy nhất như thể hiện trong Bảng 5.

[Bảng 5. Tăng tốc thực tế so với lý thuyết trên ImageNet trên CPU AMD Ryzen Threadripper 2970WX với batch size 1.]

Hạn chế. Tăng tốc thực tế của chúng tôi ít hơn việc giảm FLOPs và điều này được quy cho hai yếu tố: 1) Chi phí truyền dữ liệu từ việc cắt ma trận trọng số dày đặc dựa

--- TRANG 8 ---
trên dự đoán mặt nạ, có thể được giảm thiểu bằng các backend có suy luận thưa thớt in-place hiệu quả. 2) Tăng tốc phụ thuộc vào chữ ký của mô hình và thông số kỹ thuật của phần cứng. Cắt tỉa từ các lớp sau xử lý độ phân giải đầu vào nhỏ hơn có thể không đạt được tăng tốc nhiều như cắt tỉa từ các lớp đầu. Tối ưu hóa nhận thức ràng buộc sử dụng Alternating Direction Method of Multipliers (ADMM) [2] như được đề xuất trong [48] có thể được tích hợp thêm với phương pháp của chúng tôi để tối ưu hóa trên latency thay vì FLOPs.

[Bảng 6. So sánh độ chính xác của định tuyến động với chữ ký được xác định trước và đối tác của nó với suy luận tĩnh.]

5. Kết luận
Trong bài báo này, chúng tôi đề xuất một công thức mới cho cắt tỉa mô hình động. Tương tự như các phương pháp cắt tỉa động khác, chúng tôi trang bị một head quyết định rẻ cho lớp tích chập gốc. Tuy nhiên, chúng tôi đề xuất huấn luyện các head quyết định trong một mô hình tự giám sát. Head này dự đoán các bộ lọc có khả năng được kích hoạt cao nhất khi được cho kích hoạt đầu vào của lớp. Các mặt nạ được huấn luyện sử dụng loss entropy chéo nhị phân tách biệt khỏi loss nhiệm vụ để loại bỏ sự can thiệp của các loss. Chúng tôi tạo ra ground truth mặt nạ dựa trên một tiêu chí mới sử dụng khối lượng heatmap cho mỗi mẫu đầu vào. Trong các thí nghiệm của chúng tôi, chúng tôi đã thể hiện kết quả trên nhiều kiến trúc khác nhau trên các tập dữ liệu CIFAR và ImageNet, và phương pháp của chúng tôi vượt trội hơn các phương pháp cắt tỉa động và tĩnh khác dưới việc giảm FLOPs tương tự.

--- TRANG 9 ---
Lời cảm ơn
Chúng tôi cảm ơn các người đánh giá vì phản hồi quý giá của họ. Chúng tôi cũng muốn cảm ơn Compute Canada vì các siêu máy tính của họ để tiến hành các thí nghiệm của chúng tôi.

Tài liệu tham khảo
[1] Simone Bianco, Remi Cadene, Luigi Celona, và Paolo Napoletano. Benchmark analysis of representative deep neural network architectures. IEEE Access, 6:64270–64277, 2018. 7

[2] Stephen Boyd, Neal Parikh, và Eric Chu. Distributed optimization and statistical learning via the alternating direction method of multipliers. Now Publishers Inc, 2011. 8

[3] Glenn W Brier và cộng sự. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1–3, 1950. 7

[4] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019. 1

[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5

[6] Xuanyi Dong, Junshi Huang, Yi Yang, và Shuicheng Yan. More is less: A more complicated network with less inference complexity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5840–5848, 2017. 1, 6

[7] Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong Zhang, và Nilanjan Ray. To filter prune, or to layer prune, that is the question. In Proceedings of the Asian Conference on Computer Vision, 2020. 2, 7

[8] Sara Elkerdawy, Hong Zhang, và Nilanjan Ray. Lightweight monocular depth estimation model by joint end-to-end filter pruning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 4290–4294. IEEE, 2019. 4

[9] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. 1, 2

[10] Xitong Gao, Yiren Zhao, Łukasz Dudziak, Robert Mullins, và Cheng-zhong Xu. Dynamic channel pruning: Feature boosting and suppression. arXiv preprint arXiv:1810.05331, 2018. 1, 3, 5, 6

[11] Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems (NIPS), pages 1135–1143, 2015. 2

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5, 6

[13] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, và Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. 2, 5, 6

[14] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4340–4349, 2019. 6

[15] Yihui He, Xiangyu Zhang, và Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 1389–1397, 2017. 2, 5

[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, và Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 1, 5, 6

[17] Jie Hu, Li Shen, và Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132–7141, 2018. 1

[18] Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, và G Edward Suh. Channel gating neural networks. arXiv preprint arXiv:1805.12549, 2018. 1, 3

[19] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, và Kilian Q Weinberger. Multi-scale dense networks for resource efficient image classification. arXiv preprint arXiv:1703.09844, 2017. 4

[20] Alex Krizhevsky, Geoffrey Hinton, và cộng sự. Learning multiple layers of features from tiny images. 2009. 5

[21] Bailin Li, Bowen Wu, Jiang Su, và Guangrun Wang. Eagleeye: Fast sub-net evaluation for efficient neural network pruning. In European conference on computer vision, pages 639–654. Springer, 2020. 11

[22] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016. 1, 5

[23] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. ICLR, 2017. 2

[24] Yuhang Li, Xin Dong, và Wei Wang. Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. 1

[25] Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, và Daniela Rus. Lost in pruning: The effects of pruning neural networks beyond test accuracy. arXiv preprint arXiv:2103.03014, 2021. 12

[26] Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, và Daniela Rus. Provable filter pruning for efficient neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. 6

[27] Ji Lin, Yongming Rao, Jiwen Lu, và Jie Zhou. Runtime neural pruning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 2178–2188, 2017. 1, 3, 5

[28] Lanlan Liu và Jia Deng. Dynamic deep neural networks: Optimizing accuracy-efficiency trade-offs by selective execution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. 3

--- TRANG 10 ---
[29] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE International Conference on Computer Vision, pages 2736–2744, 2017. 2

[30] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE ICCV, pages 2736–2744, 2017. 3

[31] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. 1, 7

[32] Christos Louizos, Max Welling, và Diederik P Kingma. Learning sparse neural networks through l0 regularization. arXiv preprint arXiv:1712.01312, 2017. 2

[33] Siegrid Lowel và Wolf Singer. Selection of intrinsic horizontal connections in the visual cortex by correlated neuronal activity. Science, 255(5041):209–212, 1992. 2

[34] Jian-Hao Luo, Jianxin Wu, và Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pages 5058–5066, 2017. 2, 5

[35] Arun Mallya, Dillon Davis, và Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67–82, 2018. 4

[36] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, và Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11264–11272, 2019. 1, 2, 3, 5, 6, 7

[37] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, và Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019. 7

[38] Michela Paganini. Prune responsibly. arXiv preprint arXiv:2009.09936, 2020. 12

[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, và cộng sự. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. 5, 7

[40] Sayeh Sharify, Alberto Delmas Lascorz, Mostafa Mahmoud, Milos Nikolic, Kevin Siu, Dylan Malone Stuart, Zissis Poulos, và Andreas Moshovos. Laconic deep learning inference acceleration. In 2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA), pages 304–317. IEEE, 2019. 2

[41] Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio và Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 5

[42] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, và Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820–2828, 2019. 1

[43] Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chungjing Xu, Chao Xu, và Chang Xu. Scop: Scientific control for reliable neural network pruning. Advances in Neural Information Processing Systems, 33:10936–10947, 2020. 11

[44] Yulong Wang, Xiaolu Zhang, Xiaolin Hu, Bo Zhang, và Hang Su. Dynamic network pruning with interpretable layer-wise channel selection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6299–6306, 2020. 1, 3, 5

[45] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, và Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pages 2074–2082, 2016. 3

[46] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992. 1

[47] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S Davis, Kristen Grauman, và Rogerio Feris. Blockdrop: Dynamic inference paths in residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8817–8826, 2018. 3

[48] Haichuan Yang, Yuhao Zhu, và Ji Liu. Ecc: Platform-independent energy-constrained deep neural network compression via a bilinear regression model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11206–11215, 2019. 8

[49] Tien-Ju Yang, Yu-Hsin Chen, và Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5687–5695, 2017. 7

[50] Xucheng Ye, Pengcheng Dai, Junyu Luo, Xin Guo, Yingjie Qi, Jianlei Yang, và Yiran Chen. Accelerating cnn training by pruning activation gradients. In European Conference on Computer Vision, pages 322–338. Springer, 2020. 4

[51] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, và Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 365–382, 2018. 1

[52] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, và Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. Advances in neural information processing systems, 31, 2018. 11

--- TRANG 11 ---
Phụ lục
1. Thông tin Tính toán
Bảng 1 thể hiện số giờ tính toán cho việc huấn luyện các mô hình được báo cáo trong bài báo của chúng tôi sử dụng máy 4 V100 GPU.

[Bảng 1. Số giờ tính toán trên máy 4 V100 GPU]

2. CIFAR
2.1. MobileNet
Chúng tôi so sánh phương pháp của mình trên MobileNetv1/v2 dưới tỷ lệ cắt tỉa khác nhau với các phương pháp cắt tỉa khác như EagleEye [21], SCOP [43] và DCP [52]. Lưu ý rằng EagleEye báo cáo kết quả tốt nhất trong hai mô hình ứng viên khác nhau về chữ ký, do đó gấp đôi thời gian huấn luyện. Chúng tôi vượt trội hơn SOTA 37% giảm FLOPs cao hơn với độ chính xác tương tự như thể hiện trong 1.

[Hình 1. MobileNetV1/V2 trên CIFAR10.]

2.2. Hình dung Bộ lọc Lõi
Chúng tôi thể hiện hình dung số lượng bộ lọc lõi cho mỗi lớp như được giải thích trong bài báo chính. Bộ lọc lõi cho mỗi lớp chỉ ra các bộ lọc được kích hoạt trong tất cả các tuyến đường khác nhau. Sự đa dạng cao nhất ở các lớp giữa, điều này giải thích sự sụt giảm độ chính xác trong cắt tỉa đồng nhất tĩnh (Bảng 1 bài báo chính). MobileNet 50 tĩnh dẫn đến sự sụt giảm độ chính xác 3.31% so với phương pháp động của chúng tôi, cải thiện baseline với sự tăng nhẹ về độ chính xác 0.17%.

[Hình 2. Số lượng bộ lọc lõi cho mỗi lớp trong MobileNet.]

2.3. Thanh Lỗi
Bảng 2 thể hiện chi tiết số liệu của các thí nghiệm CIFAR với giá trị trung bình và độ lệch chuẩn trên 3 lần chạy.

2.4. Hình dung Heatmap
Chúng tôi hình dung heatmap của một lớp được cắt tỉa mạnh so với mô hình baseline. Hình 3 thể hiện so sánh giữa heatmap từ baseline với tất cả bộ lọc được kích hoạt và heatmap của các bộ lọc được chọn động. Như có thể thấy, cắt tỉa động gần đúng baseline với sự chú ý cao trên các đối tượng nền trước. Điều này cho thấy ngay cả với tỷ lệ cắt tỉa 70% trong lớp đó, chúng tôi có thể gần đúng hành vi của mô hình gốc.

3. ImageNet
3.1. Liên kết so với Tách biệt
Vì việc huấn luyện các mô hình ImageNet tốn kém, chúng tôi thể hiện một số thí nghiệm để so sánh kết quả với các chế độ huấn luyện liên kết và tách biệt trong Bảng 3. Tương tự như kết quả trên CIFAR, huấn luyện tách biệt vượt trội hơn huấn luyện liên kết dưới việc giảm FLOPs tương tự.

[Bảng 3. Huấn luyện liên kết so với tách biệt trên ResNet34 ImageNet]

Tác động Rộng lớn
Cắt tỉa Mạng Nơ-ron có tiềm năng tăng hiệu quả triển khai về mặt năng lượng và thời gian phản hồi. Tuy nhiên, việc có được những mô hình đã cắt tỉa này vẫn chưa được tối ưu hóa cho tiêu thụ tính toán tổng thể tốt hơn và thân thiện với môi trường hơn. Hơn nữa, cắt tỉa yêu cầu hiểu biết cẩn thận về các kịch bản triển khai như

--- TRANG 12 ---
[Hình 3. Hình dung heatmap của các mẫu đầu vào ngẫu nhiên từ CIFAR cho lớp thứ 10 trong MobileNetV1. Mỗi bộ ba đại diện cho hình ảnh đầu vào, heatmap baseline, heatmap đã cắt tỉa. Giảm FLOPs trong lớp là ≈70%, nhưng heatmap đã cắt tỉa gần đúng cao heatmap với các bộ lọc được kích hoạt đầy đủ.]

[Bảng 2. Giá trị trung bình và độ lệch chuẩn trên các lần chạy khác nhau trên CIFAR.]

việc đặt câu hỏi về khái quát hóa ngoài phân phối [25] hoặc thay đổi hành vi của mạng theo cách không công bằng [38]. Chúng tôi đã thể hiện kết quả về dataset shift để giải quyết phần đầu tiên. Chúng tôi không điều tra tính công bằng của dự đoán mô hình vì cả hai tập dữ liệu (tức là CIFAR và ImageNet) đều cân bằng. Mặc dù, chúng tôi cắt tỉa dựa trên khối lượng của heatmap như nhau cho tất cả các mẫu. Chúng tôi đưa ra giả thuyết rằng đặc điểm này có thể mang lại cho phương pháp của chúng tôi một lợi thế so với tỷ lệ cắt tỉa cố định có thể làm tổn hại một số mẫu đầu vào so với một số mẫu khác.
