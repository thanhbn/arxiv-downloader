# 2402.01613.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/tokenizer/2402.01613.pdf
# File size: 310981 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2402.01613v2  [cs.CL]  3 Feb 2025Published in Transactions on Machine Learning Research (02 /2025)
Nomic Embed: Training a Reproducible Long Context Text
Embedder
Zach Nussbaum zach@nomic.ai
Nomic AI
John X. Morris jack@nomic.ai, jxm3@cornell.edu
Nomic AI, Cornell University
Brandon Duderstadt brandon@nomic.ai
Nomic AI
Andriy Mulyar andriy@nomic.ai
Nomic AI
Reviewed on OpenReview: https: // openreview. net/ forum? id= IPmzyQSiQE
Abstract
This technical report describes the training of nomic-embe d-text-v1, the ﬁrst fully repro-
ducible, open-source, open-weights, open-data, 8192 cont ext length English text embed-
ding model that outperforms both OpenAI Ada-002 and OpenAI t ext-embedding-3-small
on the short-context MTEB benchmark and the long context LoC o benchmark. We release
the training code and model weights under an Apache 2.0 licen se. In contrast with other
open-source models, we release the full curated training da ta and code that allows for full
replication of nomic-embed-text-v1. You can ﬁnd code and da ta to replicate the model at
https://github.com/nomic-ai/contrastors.
1 Introduction
Text embeddings are an integral component of modern NLP appl ications powering retrieval-augmented-
generation (RAG) for LLMs and semantic search (Lewis et al., 2021a; Izacard et al., 2022b; Ram et al.,
2023). These embeddings encode semantic information about sentences as low-dimensional vectors that are
used in downstream applications, such as clustering for dat a visualization, classiﬁcation, and information
retrieval.
The majority of the top open-source models on the MTEB benchm ark (Muennighoﬀ et al., 2023) are limited
to context lengths of 512, such as E5 (Wang et al., 2022), GTE ( Li et al., 2023), and BGE (Xiao et al.,
2023). This short context length reduces model utility in do mains where the overall document semantics
are not localized to sentences or paragraphs. Most top embed ding models with a context length longer
than 2048 are closed-source, such as Voyage-lite-01-instr uct (Voyage, 2023) and text-embedding-ada-002
(Neelakantan et al., 2022).
As of October 2024, the top-performing open-source long con text embedding models are jina-embedding-
v2-base-en (Günther et al., 2024) and E5-Mistral-7b-instr uct (Wang et al., 2023b). Unfortunately, jina-
embedding-v2-base does not surpass OpenAI’s text-embeddi ng-ada-002 (Neelakantan et al., 2022) (see Table
1). Further, E5-Mistral (Wang et al., 2023b) is not feasible to use in many engineering applications due to
the large inference requirements of a 7 billion parameter tr ansformer, and does not perform well beyond 4096
tokens.
In this paper, we present an end-to-end training pipeline fo r a state of the art long context text embedding
model at only 137 million parameters. nomic-embed-text-v1 outperforms OpenAI text-embedding-ada and
1

--- PAGE 2 ---
Published in Transactions on Machine Learning Research (02 /2025)
50 55 60 65 70 75 80 85MTEB
LoCo
JinaLC60.99
52.7
55.2562.26
82.4
58.260.39
85.45
51.962.39
85.53
54.16Nomic Embed
Jina Base V2
text-embedding-3-small
text-embedding-ada
Figure 1: Benchmarking Text Embedding Model. Aggregate performance of nomic-embed-text-v1,
OpenAI text-embedding-ada, OpenAI text-embedding-3-sma ll and jina-embedding-base-v2 on both short
and long context benchmarks. nomic embed is the only fully au ditable long context model that exceeds
OpenAI text-embedding-ada and OpenAI text-embedding-3-s mall on MTEB and LoCo. nomic embed per-
forms similarly or outperforms Jina Base V2 on all tasks. X-a xis units vary per benchmark suite.
text-embedding-3-small performance on short context (MTE B) and long context benchmarks (LoCo) (Table
1).
Further, we are the ﬁrst to release all training artifacts ne eded to train a high-performing text embedding
model. We release the model weights, training code, and trai ning data to enable end-to-end auditability and
replication of the model
2 Related Work
Text embedding models have historically been trained with s equence lengths less than or equal to 512
tokens. Recently, Günther et al. (2024) trained a long conte xt text embedding model, jina-embeddings-
base-v2, but underperforms closed source text embedding mo dels like text-embedding-ada-002 on both the
MTEB benchmark as well as the Jina Long Context Benchmark. Ad ditionally, jina-embeddings-base-v2
underperforms other open-weight short-context text embed ding models like E5 (Wang et al., 2022), GTE
(Li et al., 2023), and BGE (Xiao et al., 2023).
Further, there is a lack of transparency into the training pi peline for high performing open-weight text
embedding models. Many of these released models omit key det ails such as data source, data curation
techniques, and training code. Wang et al. (2022) outlined t heir data ﬁltering procedure for E5 which
includes ﬁrst training a model over a large noisy dataset and then using the resulting model to ﬁlter low
quality text pairs. However, they do not release details on t he model used for consistency ﬁltering, how it was
trained, or what data was used. They additionally do not rele ase any training code or data for the released
embedding model. Similarly, Li et al. (2023) and Günther et a l. (2024) did not detail the data sources for
contrastive pretraining, omit details on how data ﬁltering and mining was approached, and did not release
training code.
Additionally, few details have been released on how closed s ource text embedding models are trained
like Voyage-lite-01-instruct (Voyage, 2023) and OpenAI’s text-embedding-ada-002 and text-embedding-3
(Neelakantan et al., 2022).
2

--- PAGE 3 ---
Published in Transactions on Machine Learning Research (02 /2025)
3 Background
State-of-the-art text embedding models are generally trai ned in three stages: masked language modeling
(Devlin et al., 2019), weakly-supervised contrastive pret raining, and contrastive ﬁnetuning (Wang et al.,
2022; Li et al., 2023; Günther et al., 2023; 2024). Tradition ally, ﬁnetuning involved leveraging labeled
datasets such as MSMarco and SNLI (Bowman et al., 2015) to gen erate paired training data for the con-
trastive signal. Examples include SBERT (Reimers & Gurevyc h, 2019), SimCSE (Gao et al., 2022), and
SGPT (Muennighoﬀ, 2022). Recent models such as E5 (Wang et al ., 2022), GTE (Li et al., 2023), BGE
(Xiao et al., 2023), InstructOR (Su et al., 2023a), and Jina ( Günther et al., 2023; 2024) utilize a multi-
stage regime in which a pretrained transformer is ﬁrst contr astively trained using a large corpus of weakly
paired data (e.g. Quora, Reddit Comments) and then addition ally ﬁnetuned on small, higher quality labeled
datasets such as MSMarco. The two-stage paradigm signiﬁcan tly improves model quality as weakly paired
data is available in much greater quantity.
Table 1: nomic-embed-text-v1 is the only open-source long- context model to outperform closed source models
like text-embedding-ada-002 and text-embedd-3-small on t he short-context MTEB benchmark and the long
context LoCo benchmark.
Model Params Seq MTEB LoCo Jina LC Weights Code Data
nomic-embed-text-v1 137M 8192 62.39 85.53 54.16 Yes Yes Yes
nomic-embed-text-v1-ablated 137M 8192 61.36 86.89 53.53 Yes Yes Yes
jina-embeddings-base-v2-en 137M 8192 60.39 85.45 51.90 Yes No No
text-embedding-ada-002 N/A 8192 60.99 52.70 55.25 No No No
text-embedding-3-small N/A 8192 62.26 82.4 58.21 No No No
E5-Mistral-7b-instruct 7B 4096 66.6 87.8 N/A Yes No No
text-embedding-3-large N/A 8192 64.59 79.4 58.69 No No No
3.1 Masked Language Modeling
Masked language modeling masks a percentage of inputs and tr ains a bidirectional transformer to predict
the masked tokens (Devlin et al., 2019). The original BERT mo del was additional trained with a binary
auxiliary Next-Sentence Prediction (NSP) task. Liu et al. ( 2019) released RoBERTa in which they attained
better performance by training on more data and for longer. T hey additionally removed the NSP task as it
didn’t show any performance improvements. More recently, P ortes et al. (2023) introduced MosaicBERT, an
eﬃcient and high performing BERT training recipe by increas ing the masking rate, incorporating FlashAt-
tention (Dao et al., 2022), and other training optimization s.
3.2 Weakly-supervised Contrastive Pretraining
Weakly-supervised contrastive pretraining aims to teach a model to distinguish the most similar documents
from other irrelevant documents. To do so, we employ the Info NCE contrastive loss (van den Oord et al.,
2019). For a given batch B= (q0, d0),(q1, d1), ...,(qn, dn), we minimize the loss function:
LC=−1
n/summationdisplay
iloges(qi,di)/τ
es(qi,di)/τ+/summationtextn
j/negationslash=ies(qi,dj)/τ(1)
where s(q, d) is the (learned) score between query qand document d. We set sto cosine similarity for all
of our experiments. Contrary to other approaches, we adopt a unidirectional contrastive loss from query to
document. Other approaches like Günther et al. (2023) use a b idirectional contrastive loss by including the
contrastive loss from document to query as well.
3

--- PAGE 4 ---
Published in Transactions on Machine Learning Research (02 /2025)
3.3 Contrastive Finetuning
The last stage of training aims to boost performance by utili zing human-labeled datasets. Several papers
including Ni et al. (2021a;b); Wang et al. (2022); Li et al. (2 023) have shown that ﬁnetuning on these datasets
leads to improvements in downstream performance, especial ly for QA and web-search retrieval tasks. We
adapt Equation 1 to include hard negative documents in each b atch:
LC=−1
n/summationdisplay
iloges(qi,di)/τ
es(qi,di)/τ+/summationtextn
j/negationslash=ies(qi,dj)/τ+/summationtextH
m=1es(qi,dhn(1,m))/τ(2)
Here, we modify the partition function of the contrastive lo ss to include Hhard negative documents dhn(1, m)
which are documents specially chosen to be close to dbut not true positive documents of q.
3.4 Rotary Positional Embeddings
Rotary Positional Embeddings (RoPE) are an alternative pos itional encoding introduced in (Su et al., 2023b)
that encode relative positional information through rotat ions within the attention-layers.
Following the notation in (Su et al., 2023b), setting θ= 10 ,000 is standard. We set θ= 1,000 for our
experiments but found little to no performance degradation .
3.5 RoPE Context Length Extrapolation
However, a limitation with RoPE is in scaling to sequence len gths longer than seen during training. We
discuss two methods: position interpolation and frequency -based scaling.
3.5.1 Position Interpolation
Chen et al. (2023) and kaiokendev (2023) independently prop osed extrapolating RoPE based models by
interpolating the position indices to be within the origina l training sequence length. Following the notation
in (Su et al., 2023b), given a pretrained model with context l ength L, the position embedding function fW
is modiﬁed as:
fW(xm, m, θ d) =fW(xm,mL
L′, θd) (3)
where L′> Lis the target extended context length. This approach, while simple, requires ﬁne-tuning on a
smaller dataset to achieve stable performance at longer con texts.
3.5.2 Frequency-based Scaling
NTK-Aware Scaling bloc97 (2023) ﬁrst proposed scaling the high frequencies mo re and low frequencies
less by changing the base θin order to be "NTK-aware" as it was shown in Tancik et al. (202 0) that neural
networks struggle to represent high frequencies well. The b ase is scaled by the ratio of the longer sequence
length and the trained sequence length:
b‘ =b∗s|D|
|D|−2 (4)
where s=L‘
L.
Dynamic NTK Scaling Dynamic NTK scaling (emozilla, 2023; Peng et al., 2023) impr oves upon NTK-
aware by introducing a hyperparameter αto Equation 4.
b‘ =b∗((α∗s)−(α−1))|D|
|D|−2 (5)
This maintains the original position embeddings for sequen ces within the pretrained context length ( lcurrent ≤
L) and gradually scales the embeddings as sequences grow long er, preventing abrupt performance degrada-
tion. Additionally, this approach can be used without any ﬁn etuning as shown in (Peng et al., 2023).
4

--- PAGE 5 ---
Published in Transactions on Machine Learning Research (02 /2025)
4 Methods
4.1 Masked Language Modeling
4.1.1 Data
Following Devlin et al. (2019), we use BooksCorpus (Zhu et al ., 2015) and a Wikipedia dump from 2023 to
train a long context BERT model, hereinafter called nomic-b ert-2048. Each document from BooksCorpus
and Wikipedia is tokenized using the bert-base-uncased tok enizer from Devlin et al. (2019) and packed across
documents to chunks of 2048 tokens. If a document is shorter t han 2048 tokens, we append another document
until it ﬁts 2048 tokens. If a document is greater than 2048 to kens, we split it across multiple documents.
nomic-bert-2048 follows a similar training pipeline for ma sked language modeling as Portes et al. (2023). We
omit next sentence prediction similarly to Liu et al. (2019) and Portes et al. (2023) as it was shown to not
improve performance and simpliﬁes the training recipe.
4.1.2 Training Modiﬁcations
To train a long sequence length and eﬃcient BERT, we adapt the BERT architecture. We make the following
architecture changes to BERT base (Devlin et al., 2019):
• Substituting absolute positional embeddings for rotary p ositional embeddings (Su et al., 2023b)
• Using SwiGLU activation instead of GeLU (Shazeer, 2020)
• Using Flash Attention (Dao et al., 2022)
• Setting Dropout to 0 (Geiping & Goldstein, 2022)
• Vocab size as a multiple of 64 (Portes et al., 2023; Shoeybi e t al., 2020)
resulting in a 137 million parameter encoder.
We train all stages with a max sequence length of 2048 and empl oy Dynamic NTK interpolation at inference
to scale to 8192 sequence length (Peng et al., 2023; emozilla , 2023). Additionally, we opt for SwiGLU versus
GeGLU like proposed in Portes et al. (2023) as runtime is roug hly 25% faster for SwiGLU using the Flash
Attention repository1.
We use a 30% masking rate instead of 15% following Portes et al . (2023) and we remove the Next Sentence
Prediction task to simplify the training recipe (Liu et al., 2019; Portes et al., 2023). We use the AdamW
optimizer (Loshchilov & Hutter, 2019) with a max learning ra te of 5e-4 with β1= 0.9 β2= 0.98. We employ
a linear warmup of 6% of the total training steps and a linear d ecay to 0. We use a global batch size of
4096 with gradient accumulation over 8 batches. We utilize D eepSpeed (Rajbhandari et al., 2020) stage
2 to ﬁt larger batches into memory. Additionally, we use bﬂoa t16 mixed precision and fp32 for gradient
accumulation dtype. We disable gradient clipping (Liu et al ., 2019) and set weight decay to 1e-5. We call
our ﬁnal model nomic-bert-2048 and also release its weights .
4.2 Weakly-Supervised Contrastive Pretraining
4.2.1 Data
Similar to Wang et al. (2022); Li et al. (2023); Xiao et al. (20 23); Ni et al. (2022), we use large collections of
publicly available data to form contrastive pairs. These da tasets span various objectives and domains, from
web retrieval to clustering of scientiﬁc articles. In total , we curated 470 million pairs across 29 datasets2.
Consistency Filtering : Since many of these datasets may contain noisy examples, we employ consistency
ﬁltering to remove the potential false positives in the data set (Günther et al., 2023; Wang et al., 2022).
1https://github.com/Dao-AILab/flash-attention/tree/m ain
2https://huggingface.co/datasets/sentence-transforme rs/embedding-training-data
5

--- PAGE 6 ---
Published in Transactions on Machine Learning Research (02 /2025)
Consistency ﬁltering uses a pretrained model to ﬁlter out po tential noisy examples in an eﬀort to improve
data quality and subsequently model quality. Additionally , reducing the total number of examples needed
to train a high quality text embedding model can reduce the ov erall cost to train the text embedding model.
For each pair, described as ( query ,document ), we embed the queries and documents separately. We sample
1 million points from the dataset and for each query, we ﬁnd th e top-k (in this case 2) neighbors using cosine
similarity. If document is not in the top-k neighbors, we discard the example.
Günther et al. (2023) uses all-MiniLM-L6-v23, a 22 million parameter sentence embedding model for con-
sistency ﬁltering. However, we found that it regularly disc arded retrieval pairs that were true positives but
had low lexical overlap. We instead utilized gte-base4(Li et al., 2023), a 109 million parameter model for
consistency ﬁltering. After ﬁltering, we end up with ∼235 million pairs. The full dataset distribution can
be seen in Appendix B.
We additionally explored consistency ﬁltering using a cosi ne similarity threshold instead of the method
described above. For a given pair, if the cosine similarity w as greater or equal to the threshold, we kept the
pair and otherwise discarded. However, we abandoned this ap proach in favor of top-k consistency ﬁltering
as we found through manual inspection the threshold consist ency ﬁltering discarded high quality retrieval
pairs that had low cosine similarity. We additionally notic ed lower retrieval scores in models trained using
a threshold for consistency ﬁltering versus using top-k con sistency ﬁltering.
Curating Long Context Text Pairs : As the majority of these datasets are composed of sequences
shorter than 2048 tokens we additionally curate long contex t datasets to allow for the learning of long-range
dependencies. We use Wikipedia titles paired with the corre sponding body and S2ORC (Lo et al., 2020)
abstracts and full paper text from a single paper.
Table 2: nomic-bert-2048 performs similarly to other short and long-context encoders when evaluated on
the GLUE benchmark.
Model Seq Bsz Steps Cola SST2 MRPC STSB QQP MNLI QNLI RTE Avg
MosaicBERT 128 4k 178k 0.59 0.94 0.89 0.90 0.92 0.86 0.91 0.83 0.85
JinaBERTBase 512 4k 100k 0.51 0.95 0.88 0.90 0.81 0.86 0.92 0. 79 0.83
RobertaBase 512 8k 500k 0.64 0.95 0.90 0.91 0.92 0.88 0.93 0.7 9 0.86
MosaicBERT 2k 4k 70k 0.54 0.93 0.87 0.90 0.92 0.86 0.92 0.82 0. 85
nomic-bert-2048 2k 4k 100k 0.50 0.93 0.88 0.90 0.92 0.86 0.92 0.82 0.84
You can access the training data of nomic-embed-text-v1 by v isiting the code repository . You can explore a
5M sample of our contrastive training pairs at https://atla s.nomic.ai/map/nomic-text-embed-v1-5m-sample.
4.2.2 Training Modiﬁcations
We initialize the model for weakly-supervised contrastive training with the weights of nomic-bert-2048. We
use a global batch size of 16,384. We use AdamW with a learning rate of 2e-4, β1= 0.9,β2= 0.999, and
weight decay of 0.01. Gradient clipping is set to 1.0. We use a linear warmup schedule of 700 steps and an
inverse square root decay schedule.
We sample one data source and ﬁll each batch with only data fro m that source to discourage the model
learning source-speciﬁc shortcuts. We train with a max sequ ence length of 2048 for 1 full epoch over the
weakly-supervised contrastive data. Full details on data c omposition can be found in Appendix B.
Due to GPU memory constraints, we employ GradCache (Luyu Gao & Callan, 2021) as well as mixed pre-
cision training (Micikevicius et al., 2018).
3all-MiniLM-L6-v2 model https://huggingface.co/sentence-transformers/all-Mi niLM-L6-v2 )
4gte-base model (https://huggingface.co/thenlper/gte-base )
6

--- PAGE 7 ---
Published in Transactions on Machine Learning Research (02 /2025)
Finally, we use task speciﬁc preﬁxes to break the symmetry of the biencoder as in Wang et al. (2022).
Without preﬁxes, the model receives conﬂicting reward sign al. Consider the case of determining which
document is closest to the query "What is the capital of Franc e?":
1. “What is the name of the capital city of France?
2. “Paris is the capital of France."
A semantic similarity task would consider the ﬁrst closest, while a question answering task would consider
the second closest. Preﬁxes enable the model to distinguish between the behaviors speciﬁed by each of these
tasks.
We use the following task-speciﬁc preﬁxes:
•search_query
•search_document
•classification
•clustering
inspired by Reimers et al. (2023). We ﬁrst break preﬁxes into two categories: symmetric, where the query
and document have a similar structure, and asymmetric, wher e the query is usually a single sentence and the
document can be many sentences (Su et al., 2023a). The ﬁrst tw o preﬁxes are used for retrieval tasks: where
search_query is used for the question and search_document is used for the response. classification
is used for STS-related tasks like rephrasals. clustering is used for tasks where the objective is to group
semantically similar texts close together, like Arxiv titl e-abstract pairs. For symmetric tasks, the same preﬁx
is appended to both the query and document.
4.2.3 Supervised Contrastive ﬁnetuning
4.2.4 Data
Supervised ﬁne tuning is performed on MSMarco (Bajaj et al., 2018; Wang et al., 2023a), NQ
(Karpukhin et al., 2020; Gao & Callan, 2021), NLI (Gao et al., 2022), HotpotQA (Yang et al., 2018),
FEVER (Thorne et al., 2018), portions of MEDI (Su et al., 2023 a), WikiAnswers (Fader et al., 2014), and
Reddit5.
For the datasets MSMarco, NQ, NLI, FEVER, and HotpotQA, we tr ain over the released training sets from
the BEIR benchmark (Thakur et al., 2021). For the retrieval d atasets (MSMarco, NQ, HotpotQA, and
Fever), we mine negatives, if not already mined, using gte-b ase (Li et al., 2023). For each ( q, d) pair, we
ﬁnd the top 20 documents among the corpus most similar to the q uery q, excluding d, and use these as
hard negatives. For other non-retrieval datasets, we rando mly sample negatives among the corpus in place
of mining hard negatives as we found that mining did not impro ve performance.
Although the BEIR component of MTEB was originally intended as a zero shot benchmark, several open
source models, such as those in Xiao et al. (2023); Li et al. (2 023); Wang et al. (2023b), report training on
train splits of BEIR benchmark datasets such as FEVER and Hot potQA. We report results for nomic-embed-
text-v1-ablated trained without FEVER, HotpotQA, and MEDI .
Similarly to the weakly supervised contrastive stage, we sa mple a dataset and ﬁll a batch with all points
from that chosen dataset. In total, we train on 1.6 million da tapoints. The full dataset distribution can be
seen in Table 3.
5https://github.com/PolyAI-LDN/conversational-datase ts/tree/master/reddit
7

--- PAGE 8 ---
Published in Transactions on Machine Learning Research (02 /2025)
Table 3: Supervised ﬁnetuning dataset distribution.
Dataset Number of Samples
MSMarco 484,864
NLI 275,200
Reddit 199,680
Medi Supernli 177,408
Hotpot 169,728
Fever 139,776
Medi Stackexchange 100,352
NQ 69,888
Medi Flickr 50,944
Medi Wiki 24,832
4.2.5 Training Modiﬁcations
We train for one epoch using seven hard negatives per pair and a batch size of 256. We employ a learning
rate of 2e-5, β1= 0.9,β2= 0.999, and weight decay of 0.01. Gradient clipping is set to 1.0 . We use a
linear warmup schedule of 400 steps and a linear cooldown to 0 and train with preﬁxes as described above.
We found that increasing the number of negatives above 7 does not signiﬁcantly improve performance. We
also found that training for multiple epochs hurts performa nce. Instead of choosing the ﬁrst N negatives,
we randomly sampled the mined negatives. We found this to imp rove performance as some of the mined
negatives introduced false negatives.
5 Results
5.1 nomic-bert-2048 GLUE Results
We ﬁrst evaluate nomic-bert-2048 on the GLUE benchmark (Wan g et al., 2019) to verify that our adapted
BERT architecture performs similarly or better compared to similar encoders. The GLUE benchmark
consists of 9 tasks, but we evaluate on 8 similar to Liu et al. ( 2019). We follow the evaluation methodology
presented in Liu et al. (2019). Roberta numbers are taken fro m Table 8 in (Liu et al., 2019). MosaicBert
numbers are taken from Table S1 in Portes et al. (2023) except for the 2048 model which we evaluated in the
same manner as nomic-bert-2048. JinaBertBase Glue Test num bers reported in Table 2 from (Günther et al.,
2024).
For each task, we train for 10 epochs with batch sizes 16, 32 an d learning rate 1e-5, 2e-5, 3e-5 with a linear
warmup of 6% across 5 seeds. The median score per task at the en d of the 10 epochs is presented in Table
2. Note we report accuracy for MRPC and QQP and Pearson for STS B6. Similar to Liu et al. (2019), we
initialize from an MNLI checkpoint for RTE, STSB, and MRPC.
Across all tasks, nomic-bert-2048 scores similarly to Mosa icBERT (Portes et al., 2023) except on Cola.
MosaicBERT is trained with more gradient updates on C4 (Raﬀe l et al., 2019). However, nomic-bert-2048
was trained with a longer sequence length and in eﬀect has see n more tokens during pretraining. The
diﬀerence in results could be due to a few reasons. First the t raining corpus could lead to better results on
Cola as nomic-bert-2048 trains on Wikipedia and Bookscorpu s while MosaicBERT trains on C4 which tends
to skew towards shorter sequence lengths. Additionally, no mic-bert-2048 utilizes RoPE while MosaicBERT
uses ALiBi for long-context extrapolation.
JinaBERT also trains a similar model to MosaicBERT using ALi BI for long-context extrapolation and C4 as
its training corpus but sets the max sequence length to 512. I t performs slightly worse on average to nomic-
bert-2048 and MosaicBERT. Even though it was trained simila rly to MosaicBERT, JinaBERT performs
worse on Cola, RTE, and QQP. These ﬁndings are similar when co mpared to nomic-bert-2048 except Cola
where JinaBERT outperforms nomic-bert-2048.
6https://github.com/facebookresearch/fairseq/issues/ 1561#issuecomment-571729519
8

--- PAGE 9 ---
Published in Transactions on Machine Learning Research (02 /2025)
Table 4: MTEB benchmark results (Muennighoﬀ et al., 2023). n omic-embed-text-v1 outperforms all simi-
larly sized models on short-context tasks except BGE-Base.
Category → Params. Cls. Clust. PairCls.Rerank Retr. STS Summ. Avg
Number of datasets → 12 11 3 4 15 10 1 56
Unsupervised Models
Glove (Pennington et al., 2014) 0.3B 57.3 27.7 70.9 43.3 21.6 61.9 28.9 42.0
SimCSE (Gao et al., 2022) 110M 62.5 29.0 70.3 46.5 20.3 74.3 31 .2 45.5
nomic-embed-text-v1 unsup 137M 71.2 42.5 83.7 55.0 48.0 80.8 30.7 59.9
Supervised Models
SimCSE bert-sup (Gao et al., 2022) 110M 67.3 33.4 73.7 47.5 21.8 79.1 23.3 48.7
Contriever (Izacard et al., 2022a) 110M 66.7 41.1 82.5 53.1 4 1.9 76.5 30.4 56.0
E5base(Wang et al., 2022) 110M 75.2 44.2 86.0 56.6 50.6 82.1 30.2 61. 6
GTE base(Li et al., 2023) 110M 73.0 46.2 84.6 58.6 51.1 82.3 31.2 62.4
BGE base(Xiao et al., 2023) 110M 75.5 45.8 86.6 58.9 53.3 82.4 31.1 63. 6
Jina v2(Günther et al., 2024) 137M 73.5 41.7 85.4 57.0 47.9 80.7 31.6 60.4
nomic-embed-text-v1-ablated 137M 73.6 43.7 84.6 53.3 51.4 80.2 31.3 61.4
nomic-embed-text-v1 137M 74.1 43.9 85.2 55.7 52.8 82.1 30.1 62.4
E5large-v2 (Wang et al., 2022) 335M 75.2 44.5 86.0 56.6 50.6 82.1 30.2 62. 3
GTE large (Li et al., 2023) 335M 73.3 46.8 85.0 59.1 52.2 83.4 31.7 63.1
BGE large (Xiao et al., 2023) 335M 76.0 46.1 87.1 60.0 54.3 83.1 31.6 64. 2
GTR xxl(Ni et al., 2021a) 4.8B 67.4 42.4 86.1 56.7 48.5 78.4 30.6 59.0
Sentence-T5 xxl(Ni et al., 2021b) 4.8B 73.4 43.7 85.1 56.4 42.2 82.6 30.1 59.5
text-embedding-ada-002 NA 70.9 45.9 84.9 56.3 49.3 81.0 30.8 61.0
text-embedding-3-small NA 73.2 46.7 85.0 56.7 51.1 81.6 31.1 62.3
text-embedding-3-large NA 75.5 49.0 85.7 59.2 55.4 81.7 29.9 64.6
E5mistral (Wang et al., 2023b) 7B 78.5 50.3 88.3 60.2 56.9 84.6 31.4 66.6
5.2 Text Embedding Benchmark Results
To evaluate nomic-embed-text-v1 eﬀectiveness as a text enc oder, we evaluate it on MTEB
(Muennighoﬀ et al., 2023), Jina’s Long Context Benchmark (G ünther et al., 2024), and LoCo
(Saad-Falcon et al., 2024).
5.2.1 MTEB Results
MTEB is a general text embedding benchmark released by Muenn ighoﬀ et al. (2023). It measures text
embedding performance across tasks Classiﬁcation, Cluste ring, Pair Classiﬁcation, Reranking, Retrieval,
STS, and Summarization.
During evaluation, we add the classification preﬁx to both the query and document for the Classiﬁcation,
Pair Classiﬁcation, STS, and Summarization tasks. We add th eclustering preﬁx to both the query and
document for the Clustering task. And we add the search_query preﬁx to the query and search_document
preﬁx to the document for the Retrieval task. We truncate all texts to 512 tokens. Additionally, we found
better performance by not L2 normalizing the embeddings for the Classiﬁcation task, similar to the evaluation
code released by Wang et al. (2022). For all other tasks, we L2 normalize the embeddings.
The performance of nomic-embed-text-v1 and nomic-embed-t ext-v1-ablated are broken down by task in Table
4. Compared to similarly sized open-source text embedding m odels, nomic-embed-text-v1 outperforms all
models except BGE-base (Xiao et al., 2023). Additionally, n omic-embed-text-v1 outperforms larger open-
source text embedding models like E5 Large v2 (Wang et al., 20 22), GTR XXL (Ni et al., 2021a), and
Sentence T5 XXL (Ni et al., 2021b).
9

--- PAGE 10 ---
Published in Transactions on Machine Learning Research (02 /2025)
Table 5: Jina Long Context benchmark results. nomic-embed- text-v1 outperforms jina-embeddings-v2-base
and performs similarly to text-embeddings-ada-002.
Model Seq NarrativeQA WikiCities SciFact BigPatent Avg
jina-embeddings-base-v2 128 19.6 79.9 62.1 14.4 44.0
nomic-embed-text-v1-ablated 128 20.8 86.8 65.2 17.5 47.6
nomic-embed-text-v1 128 20.1 90.0 65.4 18.5 48.5
text-embedding-ada-002 128 25.4 84.9 68.8 16.6 48.9
text-embedding-3-small 128 29.5 87.5 68.8 15.0 50.2
text-embedding-3-large 128 45.6 87.9 74.8 16.5 56.2
jina-embeddings-base-v2 512 21.3 79.3 66.7 21.9 47.3
nomic-embed-text-v1-ablated 512 25.7 81.9 71.5 23.7 50.7
nomic-embed-text-v1 512 23.9 88.7 70.5 25.3 52.1
text-embedding-ada-002 512 25.5 84.8 72.6 23.0 51.5
text-embedding-3-small 512 32.2 89.0 73.2 23.6 54.5
text-embedding-3-large 512 48.1 89.9 77.6 23.6 59.6
jina-embeddings-base-v2 8191 39.4 75.7 69.4 23.1 51.9
nomic-embed-text-v1-ablated 8191 44.0 77.4 69.1 23.6 53.5
nomic-embed-text-v1 8191 37.8 84.3 70.2 24.5 54.2
text-embedding-ada-002 8191 41.1 84.7 72.7 22.5 55.3
text-embedding-3-small 8191 47.1 89.9 73.3 22.5 58.3
text-embedding-3-large 8191 51.6 86.2 77.7 19.3 58.7
Table 6: LoCo benchmark results (Saad-Falcon et al., 2024). nomic-embed-text-v1 is the best-performing
100M parameter class unsupervised model. nomic-embed-tex t-v1 is competitive with the top-performing
models in both the 7B parameter class and with models trained in a supervised setting speciﬁcally for the
LoCo benchmark.
Model Seq Param. Tau
Scr.Tau
Gov.Tau
QMS.QASP.
Tit.
Art.QASP.
Abs.
Art.Avg
M2-Bert (Saad-Falcon et al., 2024) 2048 80M 81.8 94.7 58.5 87 .3 95.5 83.6
Jina base-v2 (Günther et al., 2024) 2048 137M 87.2 97.7 35.1 95.3 99.7 83.0
nomic-embed-text-v1-ablated 2048 137M 83.1 97.3 49.4 97.4 99.9 85.4
nomic-embed-text-v1 2048 137M 86.1 96.9 47.8 96.1 99.7 85.3
nomic-embed-text-v1 4096 137M 89.0 97.4 45.7 95.8 99.9 85.6
nomic-embed-text-v1-ablated 4096 137M 89.1 97.6 49.6 97.5 99.9 86.7
E5mistral (Wang et al., 2023b) 4096 7B 95.9 98.3 46.8 98.4 99.8 87.8
M2-Bert (Saad-Falcon et al., 2024) 8192 80M 94.7 96.5 64.1 86 .8 97.5 87.9
Jina base-v2 (Günther et al., 2023) 8192 137M 93.3 98.6 40.8 95.1 99.3 85.5
nomic-embed-text-v1-ablated 8192 137M 92.5 97.8 47.6 96.5 99.9 86.9
nomic-embed-text-v1 8192 137M 90.9 97.8 44.2 94.9 99.9 85.5
text-embedding-ada-002 8192 N/A 37.3 44.3 7.30 85.1 89.7 52 .7
text-embedding-3-small 8192 N/A 92.2 97.7 27.4 95.9 98.9 82 .4
text-embedding-3-large 8192 N/A 88.0 93.6 25.5 93.2 96.8 79 .4
Compared to closed-source models, nomic-embed-text-v1 ou tperforms text-embedding-ada-002 and text-
embedding-3-small on average and notably the Retrieval tas k. nomic-embed-text-v1 is the only open-source
long-context text embedding model to outperform text-embe dding-ada-002 and text-embedding-3-small on
MTEB.
10

--- PAGE 11 ---
Published in Transactions on Machine Learning Research (02 /2025)
nomic-embed-text-v1-ablated unsurprisingly performs wo rse than nomic-embed-text-v1 and other open-
source text embedding models that ﬁnetune on the training se ts of BEIR like BGE-base and GTE-base.
However, nomic-embed-text-v1-ablated still outperforms text-embedding-ada-002 and jina-embeddings-base-
v2 and is competitive with E5-base v2. jina-embeddings-bas e-v2 is trained on similar datasets to nomic-
embed-text-v1-ablated yet nomic-embed-text-v1-ablated outperfoms jina-embeddings-base-v2 on MTEB.
5.2.2 Long Context Results
However, as noted in Günther et al. (2024), MTEB has very few d atasets that include long sequences. To
evaluate nomic-embed-text-v1’s performance on longer seq uences, we consider two additional benchmarks:
the Jina Long Context Dataset (Günther et al., 2024) as well a s the LoCo benchmark from Saad-Falcon et al.
(2024). Although nomic-embed-text-v1 was trained with a ma x sequence length of 2048, we are able to use
length extrapolation techniques proposed in emozilla (202 3); Peng et al. (2023).
For texts longer than 2048, the max sequence length nomic-em bed-text-v1 was trained on, we employ Dy-
namic NTK Interpolation as described in Equation 5. We set αto 2.
5.2.3 JinaAI Long Context Benchmark
The Jina Long Context Benchmark (Günther et al., 2024) evalu ates on 4 datasets across Retrieval and
Clustering; namely, NarrativeQA (Günther et al., 2024), Wi kiCites7, SciFact (Wadden et al., 2020), and
BigPatent8(Sharma et al., 2019). Similar to Günther et al. (2024), we re port the V-scores and NDCG@10
for the clustering and retrieval datasets respectively. We evaluate all models at sequence length 128, 512,
and 8191. For nomic-embed-text-v1 and nomic-embed-text-v 1-ablated on NarrativeQARetrieval and Scifact,
we use the search_query andsearch_document preﬁxes for the query and document respectively. For
BigPatentClustering and WikiCities, we use the clustering preﬁx for both the query and document. Results
are presented in Table 5.
Numbers for text-embedding-ada-002 and jina-embeddings-base-v2 are taken from (Günther et al.,
2024).
Across all context lengths, nomic-embed-text-v1 outperfo rms jina-embeddings-v2-base. When evaluated on
shorter sequence lengths, nomic-embed-text-v1 performs s imilarly to text-embedding-ada-002 but is outper-
formed at 8k context. Additionally, nomic-embed-text-v1- ablated outperforms jina-embeddings-v2-base, but
underperforms nomic-embed-text-v1.
However, nomic-embed-text-v1 underperforms text-embedd ing-ada-002, text-embedding-3-small, and text-
embedding-3-large. Without any information on training da ta or architecture for the closed-source models,
it’s unclear why the gap exists. It is also surprising to see t ext-embedding-3-large performance decrease
as sequence length increases from 512 to 8191 while performa nce increases for text-embedding-3-small and
text-embedding-002-ada.
Similar to results in Günther et al. (2023), we see lower perf ormance in WikiCities across models as sequence
length increases suggesting the task may not be a good measur e of long context embedding performance.
5.2.4 LoCo Benchmark
The LoCo Benchmark consists of 5 retrieval datasets: 3 datas ets from Shaham et al. (2022) and 2
from Dasigi et al. (2021). Similar to the other retrieval eva luations, we use the search_query and
search_document preﬁxes for the query and document respectively. We evaluat e nomic-embed-text-v1
and jina-embeddings-base-v2 at sequence length 2048, 4096 , and 8192. We additionally include results from
Saad-Falcon et al. (2024) as well even though the model was ﬁn etuned on training sets of these datasets.
Results are presented in Table 6. We include the QASPER Abstr act Articles dataset for completeness, but
would like to highlight that many models seem to oversaturat e the benchmark and may not be representative
of long-context performance.
7https://huggingface.co/datasets/jinaai/cities_wiki_ clustering
8https://huggingface.co/datasets/jinaai/big-patent-c lustering
11

--- PAGE 12 ---
Published in Transactions on Machine Learning Research (02 /2025)
At 2048 sequence length, nomic-embed-text-v1 and nomic-em bed-text-v1-ablated outperform jina-
embeddings-v2-base. At a 4096 sequence length nomic-embed -text-v1 and nomic-embed-text-v1-ablated
is able to perform similarly to E5 Mistral, a model ≈70x bigger on all tasks except Tau Scrolls.
Both nomic-embed-text-v1 variants outperform text-embed ding-ada-002 and text-embedding-3-small and
perform similarly to jina-embeddings-v2-base at 8192 sequ ence length. Interestingly, nomic-embed-text-v1-
ablated outperforms nomic-embed-text-v1 and jina-embedd ing-base-v2 suggesting that the BEIR training
data may be orthogonal to the LoCo tasks.
6 Conclusion
We release the ﬁrst fully open-source long context text embe dding model that surpasses OpenAI’s text-
embedding-Ada-002 and text-embedding-003-small perform ance on both sort and long context benchmarks.
We release the model weights and training code under a permis sible license as well as the recipe, including
data, to reproduce the model. As of this writing, nomic-embe d has garnered over 14 million downloads
on the Hugging Face model hub, underscoring the widespread d emand for performant open source model
recipes.
References
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfen g Gao, Xiaodong Liu, Rangan Majumder,
Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,
and Tong Wang. Ms marco: A human generated machine reading co mprehension dataset, 2018.
bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have ex tended (8k+) con-
text size without any ﬁne-tuning and minimal perplexity deg radation., 2023. URL
https://www.reddit.com/r/LocalLLaMA/comments/14lz7j 5/ntkaware_scaled_rope_allows_llama_models_to_have/ .
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Chri stopher D. Manning. A large annotated
corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing (EMNLP) . Association for Computational Linguistics, 2015.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong T ian. Extending context window of large
language models via positional interpolation, 2023. URL https://arxiv.org/abs/2306.15595 .
William Coster and David Kauchak. Simple English Wikipedia : A new text simpliﬁcation task. In Dekang
Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologi es, pp. 665–669, Portland, Oregon, USA, June
2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-2117 .
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christo pher Ré. Flashattention: Fast and memory-
eﬃcient exact attention with io-awareness, 2022.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smi th, and Matt Gardner. A dataset of
information-seeking questions and answers anchored in res earch papers. 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Tout anova. Bert: Pre-training of deep bidirectional
transformers for language understanding, 2019.
emozilla. Dynamically scaled rope further increases perfo rmance of long context llama with zero ﬁne-tuning,
2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgp r/dynamically_scaled_rope_further_increase 
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. Open Que stion Answering Over Curated and Extracted
Knowledge Bases. In KDD , 2014.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Ja son Weston, and Michael Auli. ELI5: long
form question answering. In Anna Korhonen, David R. Traum, a nd Lluís Màrquez (eds.), Proceedings of
the 57th Conference of the Association for Computational Li nguistics, ACL 2019, Florence, Italy, July 28-
12

--- PAGE 13 ---
Published in Transactions on Machine Learning Research (02 /2025)
August 2, 2019, Volume 1: Long Papers , pp. 3558–3567. Association for Computational Linguistic s, 2019.
doi: 10.18653/v1/p19-1346. URL https://doi.org/10.18653/v1/p19-1346 .
Katja Filippova and Yasemin Altun. Overcoming the lack of pa rallel data in sentence compression.
In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Li vescu, and Steven Bethard (eds.),
Proceedings of the 2013 Conference on Empirical Methods in N atural Language Processing , pp. 1481–
1491, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL
https://aclanthology.org/D13-1155 .
Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org .
Luyu Gao and Jamie Callan. Condenser: a pre-training archit ecture for dense retrieval, 2021.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple co ntrastive learning of sentence embeddings,
2022.
Jonas Geiping and Tom Goldstein. Cramming: Training a langu age model on a single gpu in one day, 2022.
Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha R ayasam, and Zachary C Lipton. Amazonqa:
A review-based question answering task, 2019.
Michael Günther, Louis Milliken, Jonathan Geuter, Georgio s Mastrapas, Bo Wang, and Han Xiao. Jina
embeddings: A novel set of high-performance sentence embed ding models, 2023.
Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abd essalem, Tanguy Abel, Mohammad Kalim
Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo W ang, Maximilian Werk, Nan Wang, and
Han Xiao. Jina embeddings 2: 8192-token general-purpose te xt embeddings for long documents, 2024.
Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Be la Gipp. news-please: A generic news crawler
and extractor. In Proceedings of the 15th International Symposium of Informa tion Science , pp. 218–223,
March 2017. doi: 10.5281/zenodo.4120316.
Christopher Hidey and Kathy McKeown. Identifying causal re lations using parallel Wikipedia articles. In Ka-
trin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of the Association fo r Compu-
tational Linguistics (Volume 1: Long Papers) , pp. 1424–1433, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi: 10.18653/v1/P16-113 5. URL https://aclanthology.org/P16-1135 .
Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allam anis, and Marc Brockschmidt. CodeSearchNet
challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 , 2019.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastia n Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. Unsupervised dense information retrieval w ith contrastive learning, 2022a.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hossei ni, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,
Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented
language models, 2022b.
kaiokendev. Things I’m learning while training superhot., 2023. URL
https://kaiokendev.github.io/til#extending-context- to-8k .
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen tau Yih. Dense passage retrieval for open-domain questi on answering, 2020.
Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Ha nnaneh Hajishirzi, and Chris Callison-Burch.
Gooaq: Open question answering with diverse answer types, 2 021.
Mahnaz Koupaee and William Yang Wang. Wikihow: A large scale text summarization dataset, 2018.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petro ni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastia n Riedel, and Douwe Kiela. Retrieval-
augmented generation for knowledge-intensive nlp tasks, 2 021a.
13

--- PAGE 14 ---
Published in Transactions on Machine Learning Research (02 /2025)
Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini , Heinrich Küttler, Aleksandra Piktus, Pontus
Stenetorp, and Sebastian Riedel. Paq: 65 million probably- asked questions and what you can do with
them, 2021b.
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xi e, and Meishan Zhang. Towards general
text embeddings with multi-stage contrastive learning, 20 23.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly opt imized bert pretraining approach, 2019.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Dan S. Weld. S2orc: The semantic scholar
open research corpus, 2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay re gularization, 2019.
Jiawei Han Luyu Gao, Yunyi Zhang and Jamie Callan. Scaling de ep contrastive learning batch size under
memory limited setup. In Proceedings of the 6th Workshop on Representation Learning for NLP , 2021.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venka tesh, and Hao Wu. Mixed precision training,
2018.
Niklas Muennighoﬀ. Sgpt: Gpt sentence embeddings for seman tic search, 2022.
Niklas Muennighoﬀ, Nouamane Tazi, Loïc Magne, and Nils Reim ers. Mteb: Massive text embedding bench-
mark, 2023.
Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse M ichael Han, Jerry Tworek, Qiming
Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power,
Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, Dav id Schnurr, Felipe Petroski Such, Kenny
Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joa nne Jang, Peter Welinder, and Lilian
Weng. Text and code embeddings by contrastive pre-training , 2022.
Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying reco mmendations using distantly-labeled reviews
and ﬁne-grained aspects. In Kentaro Inui, Jing Jiang, Vince nt Ng, and Xiaojun Wan (eds.), Proceed-
ings of the 2019 Conference on Empirical Methods in Natural L anguage Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing (EM NLP-IJCNLP) , pp. 188–197, Hong Kong,
China, November 2019. Association for Computational Lingu istics. doi: 10.18653/v1/D19-1018. URL
https://aclanthology.org/D19-1018 .
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Áb rego, Ji Ma, Vincent Y. Zhao, Yi Luan,
Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual en coders are generalizable retrievers, 2021a.
Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, K eith B. Hall, Daniel Cer, and Yinfei Yang.
Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models, 2021b.
Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, K eith Hall, Daniel Cer, and Yinfei Yang.
Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Smaranda Muresan,
Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics :
ACL 2022 , pp. 1864–1874, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:
10.18653/v1/2022.ﬁndings-acl.146. URL https://aclanthology.org/2022.findings-acl.146 .
Bowen Peng, Jeﬀrey Quesnelle, Honglu Fan, and Enrico Shippo le. Yarn: Eﬃcient context window extension
of large language models, 2023.
Jeﬀrey Pennington, Richard Socher, and Christopher Mannin g. GloVe: Global vectors for word repre-
sentation. In Alessandro Moschitti, Bo Pang, and Walter Dae lemans (eds.), Proceedings of the 2014
Conference on Empirical Methods in Natural Language Proces sing (EMNLP) , pp. 1532–1543, Doha,
Qatar, October 2014. Association for Computational Lingui stics. doi: 10.3115/v1/D14-1162. URL
https://aclanthology.org/D14-1162 .
14

--- PAGE 15 ---
Published in Transactions on Machine Learning Research (02 /2025)
Jacob Portes, Alex Trott, Sam Havens, Daniel King, Abhinav V enigalla, Moin Nadeem, Nikhil Sardana,
Daya Khudia, and Jonathan Frankle. Mosaicbert: A bidirecti onal encoder optimized for fast pretraining,
2023.
Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sha ran Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of transfer lea rning with a uniﬁed text-to-text transformer.
arXiv e-prints , 2019.
Samyam Rajbhandari, Jeﬀ Rasley, Olatunji Ruwase, and Yuxio ng He. Zero: Memory optimizations toward
training trillion parameter models, 2020.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Perc y Liang. SQuAD: 100,000+ Questions for
Machine Comprehension of Text. arXiv e-prints , art. arXiv:1606.05250, 2016.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Sh ashua, Kevin Leyton-Brown, and Yoav
Shoham. In-context retrieval-augmented language models, 2023.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence e mbeddings using siamese bert-networks, 2019.
Nils Reimers, Elliot Choi, Amr Kayid, Alekhya Nandula, Mano j Govindassamy, and Abdullah Elkady.
Introducing embed v3, Nov 2023. URL https://txt.cohere.com/introducing-embed-v3/ .
Jon Saad-Falcon, Dan Fu, and Simran Arora. Long-context ret rieval models with monarch mixer, Jan 2024.
URL https://hazyresearch.stanford.edu/blog/2024-01-11-m 2-bert-retrieval .
Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th Annual Meeting of the Association fo r Computational Lin-
guistics (Volume 1: Long Papers) , pp. 1073–1083, Vancouver, Canada, July 2017. Association for Compu-
tational Linguistics. doi: 10.18653/v1/P17-1099. URL https://www.aclweb.org/anthology/P17-1099 .
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standard ized CompaRison over long language
sequences. In Proceedings of the 2022 Conference on Empirical Methods in N atural Language Processing ,
pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational
Linguistics. URL https://aclanthology.org/2022.emnlp-main.823 .
Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale da taset for abstractive and coherent
summarization. CoRR , abs/1906.03741, 2019. URL http://arxiv.org/abs/1906.03741 .
Noam Shazeer. Glu variants improve transformer, 2020.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGr esley, Jared Casper, and Bryan Catanzaro.
Megatron-lm: Training multi-billion parameter language m odels using model parallelism, 2020.
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.
Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-ﬁnetuned text embeddings,
2023a.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding, 2023b.
Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. F ourier features let networks learn high
frequency functions in low dimensional domains, 2020. URL https://arxiv.org/abs/2006.10739 .
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Sriv astava, and Iryna Gurevych. Beir: A het-
erogenous benchmark for zero-shot evaluation of informati on retrieval models, 2021.
James Thorne, Andreas Vlachos, Christos Christodoulopoul os, and Arpit Mittal. FEVER: a large-scale
dataset for fact extraction and VERiﬁcation. In NAACL-HLT , 2018.
15

--- PAGE 16 ---
Published in Transactions on Machine Learning Research (02 /2025)
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representat ion learning with contrastive predictive
coding, 2019.
Voyage. Excited to announce voyage embeddings!, Nov 2023. U RL
https://blog.voyageai.com/2023/10/29/voyage-embeddi ngs/.
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madelein e van Zuylen, Arman Cohan, and Han-
naneh Hajishirzi. Fact or ﬁction: Verifying scientiﬁc clai ms. In Bonnie Webber, Trevor Cohn, Yulan
He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in N atural Language
Processing (EMNLP) , pp. 7534–7550, Online, November 2020. Association for Com putational Linguistics.
doi: 10.18653/v1/2020.emnlp-main.609. URL https://aclanthology.org/2020.emnlp-main.609 .
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Ome r Levy, and Samuel R. Bowman. GLUE: A
multi-task benchmark and analysis platform for natural lan guage understanding. 2019. In the Proceedings
of ICLR.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Y ang, Daxin Jiang, Rangan Majumder, and
Furu Wei. Text embeddings by weakly-supervised contrastiv e pre-training, 2022.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Y ang, Daxin Jiang, Rangan Majumder, and
Furu Wei. Simlm: Pre-training with representation bottlen eck for dense passage retrieval, 2023a.
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Ma jumder, and Furu Wei. Improving text
embeddings with large language models, 2023b.
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennigho ﬀ. C-pack: Packaged resources to advance
general chinese embedding, 2023.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, Willia m W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. HotpotQA: A dataset for diverse, ex plainable multi-hop question answering. In
Conference on Empirical Methods in Natural Language Proces sing (EMNLP) , 2018.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level co nvolutional networks for text classiﬁcation,
2016.
Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov , Raquel Urtasun, Antonio Torralba, and
Sanja Fidler. Aligning books and movies: Towards story-lik e visual explanations by watching movies and
reading books, 2015.
A Training Resources
Full training of nomic-embed-text-v1 can be conducted in a s ingle week on one 8xH100 node. Masked
language modeling of nomic-bert-2048 takes roughly 4 days. Contrastive pretraining lasts 3 and a half days.
Contrastive ﬁnetuning takes one hour. We encourage the read er to initialize from our nomic-bert-2048 or
Unsupervised Constrastive checkpoints, released under th e same license as nomic-embed-text-v1.
B Pretraining Dataset Distribution
Weakly-supervised contrastive pretraining datasets are d etailed in Table 7. This is the number of datapoints
per source after consistency ﬁltering.
16

--- PAGE 17 ---
Published in Transactions on Machine Learning Research (02 /2025)
Table 7: Weakly Unsupervised Dataset Distribution
Dataset Datapoints % Dataset
Reddita64,978,944 0.28
PAQ (Lewis et al., 2021b) 52,953,088 0.23
Amazon Reviews (Ni et al., 2019) 38,682,624 0.16
S2ORC Title Abstract (Lo et al., 2020) 35438592 0.15
WikiAnswers (Fader et al., 2014) 9,912,320 0.04
S2ORC Citation Titles (Lo et al., 2020) 7,585,792 0.03
S2ORC Abstract Citation (Lo et al., 2020) 7,503,872 0.03
S2ORC Abstract Body (Lo et al., 2020) 6,389,760 0.03
Wikipedia Title Body (Foundation) 6,078,464 0.03
Gooaq (Khashabi et al., 2021) 1,245,184 0.01
Codesearch (Husain et al., 2019) 835,584 <.01
AGNews (Zhang et al., 2016) 409,600 <.01
CCNews (Hamborg et al., 2017) 344,064 <.01
NPRb344,064 <.01
CNN (See et al., 2017) 278,528 <.01
Yahoo Title-Answerc262,144 <.01
AmazonQA (Gupta et al., 2019) 212,992 <.01
Yahoo Title-Questiond196,608 <.01
Sentence Compression (Filippova & Altun, 2013) 163,840 <.01
YahooQAe131,072 <.01
ELI5 (Fan et al., 2019) 98,304 <.01
Altlex (Hidey & McKeown, 2016) 98,304 <.01
Wikihow (Koupaee & Wang, 2018) 81,920 <.01
SimpleWiki (Coster & Kauchak, 2011) 81,920 <.01
StackExchange Duplicate Questionsf65,536 <.01
StackExchange Title Bodyg65,536 <.01
StackExchange Body Bodyh65,536 <.01
Quora Duplicate Questionsi32,768 <.01
SQuAD (Rajpurkar et al., 2016) 16,384 <.01
Total 234,553,344 1
ahttps://huggingface.co/datasets/sentence-transforme rs/reddit-title-body
bhttps://files.pushshift.io/news/
chttps://www.kaggle.com/soumikrakshit/yahoo-answers- dataset
dhttps://www.kaggle.com/soumikrakshit/yahoo-answers- dataset
ehttps://www.kaggle.com/soumikrakshit/yahoo-answers- dataset
fhttps://data.stackexchange.com/apple/query/fork/145 6963
ghttps://data.stackexchange.com/apple/query/fork/145 6963
hhttps://data.stackexchange.com/apple/query/fork/145 6963
ihttps://quoradata.quora.com/First-Quora-Dataset-Rel ease-Question-Pairs
17
