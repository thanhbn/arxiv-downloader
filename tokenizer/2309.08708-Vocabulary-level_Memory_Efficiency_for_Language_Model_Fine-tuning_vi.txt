# 2309.08708.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/2309.08708.pdf
# Kích thước file: 440112 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Hiệu quả bộ nhớ cấp từ vựng cho việc tinh chỉnh mô hình ngôn ngữ
Miles Williams và Nikolaos Aletras
Đại học Sheffield
{mwilliams15, n.aletras}@sheffield.ac.uk
Tóm tắt
Dung lượng bộ nhớ lớn của việc tinh chỉnh mô hình ngôn ngữ (LM) tạo ra thách thức cho cả các nhà nghiên cứu và thực hành. Các LM sử dụng ma trận nhúng để đại diện cho từ vựng rộng lớn, tạo thành một tỷ lệ đáng kể các tham số mô hình. Trong khi các nghiên cứu trước về tinh chỉnh hiệu quả bộ nhớ đã tập trung vào việc giảm thiểu số lượng tham số có thể huấn luyện, việc giảm dung lượng bộ nhớ của ma trận nhúng vẫn chưa được khám phá. Chúng tôi đầu tiên chứng minh rằng một tỷ lệ đáng kể từ vựng vẫn không được sử dụng trong quá trình tinh chỉnh. Sau đó chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả tận dụng phát hiện này để giảm thiểu việc sử dụng bộ nhớ. Chúng tôi cho thấy phương pháp của chúng tôi mang lại sự giảm đáng kể trong việc sử dụng bộ nhớ trên nhiều mô hình và nhiệm vụ khác nhau. Đáng chú ý, phương pháp của chúng tôi không ảnh hưởng đến hiệu suất nhiệm vụ downstream, đồng thời cho phép sử dụng tài nguyên tính toán hiệu quả hơn.¹

1 Giới thiệu
Các mô hình ngôn ngữ (LM) (Chung et al., 2022; Touvron et al., 2023; Warner et al., 2024) tạo thành nền tảng của xử lý ngôn ngữ tự nhiên (NLP) đương đại, tuy nhiên chúng đòi hỏi tài nguyên tính toán lớn để huấn luyện (Kaplan et al., 2020; Hoffmann et al., 2022). Điều này trái ngược với việc dân chủ hóa NLP, làm gia tăng bất bình đẳng kinh tế và cản trở tính bao trùm (Schwartz et al., 2020; Weidinger et al., 2022). Do đó, có sự tập trung ngày càng tăng vào việc phát triển các phương pháp hiệu quả cho việc huấn luyện và tinh chỉnh LM (Treviso et al., 2023; Lialin et al., 2023).

Dung lượng bộ nhớ của các LM là một thách thức lớn cho việc ứng dụng chúng. Việc lưu trữ các tham số mô hình đòi hỏi lượng bộ nhớ lớn, hạn chế kích thước và kiến trúc của mô hình (Paleyes et al., 2022). Vấn đề này đặc biệt nổi bật trong quá trình huấn luyện vì gradient và trạng thái optimizer cũng phải được giữ lại (Kingma và Ba, 2017). Điều này có thể gây vấn đề khi sử dụng phần cứng tiêu dùng hoặc đối mặt với ngân sách học thuật (Izsak et al., 2021; Ciosici và Derczynski, 2022).

Các LM thông thường sử dụng từ vựng cố định để tạo ra các biểu diễn vector từ văn bản, được gọi là nhúng từ. Mỗi phần tử của từ vựng có một nhúng từ tương ứng, cùng nhau tạo thành ma trận nhúng trong LM. Kích thước của ma trận nhúng tỷ lệ với cả kích thước từ vựng và chiều nhúng, chiếm một tỷ lệ đáng kể các tham số mô hình (Bảng 5, Phụ lục A). Tỷ lệ này thường còn lớn hơn đối với các LM đa ngôn ngữ, có lợi từ từ vựng lớn hơn (Conneau et al., 2020; Liang et al., 2023). Tuy nhiên, chúng tôi giả thuyết rằng một tỷ lệ đáng kể từ vựng LM vẫn không được sử dụng trong quá trình tinh chỉnh trên nhiều nhiệm vụ downstream.

Trong bài báo này, chúng tôi đầu tiên chứng minh rằng giả thuyết của chúng tôi đúng cho nhiều nhiệm vụ downstream khác nhau, với chỉ một tập con nhỏ từ vựng được sử dụng. Sau đó chúng tôi đề xuất một phương pháp để giảm việc sử dụng bộ nhớ trong quá trình tinh chỉnh bằng cách loại trừ các nhúng không sử dụng. Cuối cùng, chúng tôi chứng minh thực nghiệm việc tiết kiệm bộ nhớ từ phương pháp của chúng tôi trên nhiều mô hình và nhiệm vụ. Đáng chú ý, phương pháp của chúng tôi không ảnh hưởng đến hiệu suất nhiệm vụ downstream và trực giao với nhiều kỹ thuật hiệu quả bộ nhớ LM hiện có.

--- TRANG 2 ---
2 Công trình liên quan
Tokenization. Các LM Transformer (Vaswani et al., 2017) thường áp dụng tokenization subword (Schuster và Nakajima, 2012; Sennrich et al., 2016) để mã hóa văn bản sử dụng từ vựng hữu hạn. Việc sử dụng từ vựng subword lớn cho phép cải thiện hiệu suất nhiệm vụ (Gallé, 2019), hiệu quả suy luận (Tay et al., 2022), và hiệu suất đa ngôn ngữ (Liang et al., 2023). Ngược lại, tokenization cấp ký tự hoặc byte có thể được sử dụng (Clark et al., 2022; Xue et al., 2022), giảm kích thước ma trận nhúng với chi phí tăng độ dài chuỗi.

Giảm tham số nhúng. Để giảm kích thước ma trận nhúng, các LM có thể được huấn luyện với phân tích nhân tử nhúng (Sun et al., 2020; Lan et al., 2020), mặc dù với hiệu suất nhiệm vụ thấp hơn một chút. Ngoài ra, các nhúng có thể được tạo từ hàm hash (Sankar et al., 2021; Xue và Aletras, 2022; Cohn et al., 2023), mặc dù điều này có thể làm hại hiệu suất do ánh xạ nhiều-đến-một từ token đến nhúng.

Cắt tỉa từ vựng đa ngôn ngữ. Công trình gần nhất với của chúng tôi là Abdaoui et al. (2020), tạo ra các LM đa ngôn ngữ nhỏ hơn bằng cách giảm vĩnh viễn số lượng ngôn ngữ được hỗ trợ. Điều này có thể làm hại hiệu suất vì từ vựng bị loại bỏ có thể sau này cần thiết cho nhiệm vụ downstream. Hơn nữa, việc chọn từ vựng nào để loại bỏ đòi hỏi xử lý tốn kém tính toán của một corpus lớn. Ushio et al. (2023) tiếp tục kiểm tra ảnh hưởng hiệu suất của việc loại bỏ vĩnh viễn từ vựng LM trước hoặc sau tinh chỉnh. Tuy nhiên, các hạn chế cơ bản tương tự vẫn tồn tại.

Tinh chỉnh hiệu quả tham số. Các phương pháp PEFT, như adapter (Houlsby et al., 2019), soft prompt (Lester et al., 2021; Li và Liang, 2021), ladder side-tuning (Sung et al., 2022), và low-rank adaptation (Hu et al., 2022), hiệu quả thích ứng các LM bằng cách tinh chỉnh chỉ một số lượng nhỏ tham số. Tuy nhiên, các phương pháp này vẫn yêu cầu tất cả tham số LM được giữ trong bộ nhớ accelerator.

Offloading. Để giảm thiểu việc sử dụng bộ nhớ accelerator (ví dụ GPU), các tham số LM có thể được giữ trong bộ nhớ riêng biệt (ví dụ CPU) cho đến khi cần thiết (Pudipeddi et al., 2020; Ren et al., 2021). Tuy nhiên, phương pháp này làm tăng đáng kể độ trễ suy luận.

Nén mô hình. Trong Phụ lục B, chúng tôi thảo luận về nhiều phương pháp nén LM trực giao, như lượng tử hóa, pruning, và distillation.

[Hình 2: Xu hướng sử dụng từ vựng cho các dataset trong GLUE khi sử dụng từ vựng từ GPT-2.]

[Bảng 1: Năm ví dụ về token từ từ vựng GPT-2 không xuất hiện trong English Wikipedia.]

3 Phân tích sử dụng từ vựng
Để đánh giá thực nghiệm mức độ sử dụng từ vựng trong quá trình tinh chỉnh, chúng tôi đầu tiên kiểm tra benchmark GLUE phổ biến (Wang et al., 2019). Điều này bao gồm một loạt các nhiệm vụ đa dạng về cả kích thước và miền (Phụ lục C). Để tokenization, chúng tôi sử dụng từ vựng subword từ GPT-2, sau này được áp dụng bởi các mô hình bao gồm RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), GPT-3 (Brown et al., 2020), và OPT (Zhang et al., 2022).

Hình 2 minh họa mối quan hệ giữa token duy nhất và tổng token trong mỗi dataset GLUE. Đáng chú ý, sáu trong chín dataset không sử dụng hơn một nửa từ vựng. Hơn nữa, dataset nhỏ nhất, WNLI, sử dụng ít hơn 4%. Thú vị, chúng tôi quan sát thấy các dataset GLUE tuân theo xu hướng giống Định luật Heaps (Heaps, 1978). Điều này nói rằng khi kích thước corpus tăng, có lợi ích giảm dần trong từ vựng mới. Tuy nhiên, việc sử dụng từ vựng subword hữu hạn có nghĩa là xu hướng tiệm cận đến kích thước từ vựng.

Riêng biệt, việc xây dựng thống kê từ vựng subword có thể phản ánh các bất thường trong dữ liệu huấn luyện của chúng, tạo ra các token có thể không bao giờ được sử dụng. Để kiểm tra mức độ của vấn đề, chúng tôi xác định các token như vậy bằng cách đánh giá dump đã xử lý của English Wikipedia, bao gồm hơn 20GB văn bản. Kỳ lạ, chúng tôi xác định gần 200 token bất thường mà không có một lần xuất hiện nào (xem Bảng 1).²

--- TRANG 3 ---
4 Thích ứng ma trận nhúng một phần
Phân tích thực nghiệm của chúng tôi (Phần 3) gợi ý rằng nhiều dataset tinh chỉnh chỉ sử dụng một phần nhỏ từ vựng LM. Chúng tôi tận dụng hiểu biết này để đề xuất Thích ứng ma trận nhúng một phần (PEMA), một phương pháp đạt được tiết kiệm bộ nhớ đáng kể bằng cách chọn chỉ tập con tối thiểu của nhúng từ cần thiết cho tinh chỉnh. Đáng chú ý, điều này không ảnh hưởng đến hiệu suất nhiệm vụ, vì các nhúng từ không sử dụng không được cập nhật trong quá trình lan truyền ngược.

Kiến thức cơ bản. Gọi mỗi token trong từ vựng {w1, ..., wk} được ký hiệu bằng một số nguyên duy nhất i sao cho V={i∈ℕ|i≤k}. Ma trận nhúng E∈ℝ|V|×d sau đó được sử dụng để chiếu mỗi token đến một vector d-chiều tương ứng.

Trước tinh chỉnh. Giả sử chúng ta có dataset tinh chỉnh D∈Vm×n với m là số lượng ví dụ và n là độ dài của mỗi ví dụ. Chúng tôi tính toán từ vựng một phần V'⊆V chỉ bao gồm các token trong D. Vì các phần tử của V' không nhất thiết là số nguyên liên tiếp, chúng tôi định nghĩa một ánh xạ tùy ý f:V'→{i∈ℕ|i≤|V'|}. Sau đó chúng tôi xây dựng ma trận nhúng một phần E'∈ℝ|V'|×d với các mục E'[:,f(i)]=E[:,i] cho tất cả i∈V'. Tức là, E' chỉ giữ lại các vector nhúng tương ứng với token trong V'. Để thích ứng D cho từ vựng một phần V', chúng tôi tạo dataset trung gian D' với mỗi mục D'[i,j]=f(D[i,j]). Cuối cùng, chúng tôi sử dụng D' và E' thay cho D và E.

Sau tinh chỉnh. Sau tinh chỉnh, ma trận nhúng một phần E' của chúng tôi giữ các nhúng mới học được cho từ vựng một phần. Tuy nhiên, chúng tôi không muốn chỉ giữ từ vựng một phần, vì điều này sẽ hạn chế việc sử dụng mô hình trong tương lai (tức là các nhiệm vụ với từ vựng khác). Do đó, chúng tôi kết hợp các nhúng mới học được vào ma trận nhúng gốc (được lưu trữ trên đĩa). Chính thức hơn, chúng tôi cập nhật E sao cho E[:,f⁻¹(i)]=E'[:,i] cho tất cả i∈V'. Điều này đảm bảo rằng mô hình vẫn giống hệt về cấu trúc, với các nhúng cho từ vựng hoàn chính.

5 Thiết lập thí nghiệm
Dataset. Để cung cấp lựa chọn công bằng các dataset, chúng tôi tuân theo tài liệu PEFT hiện có (Houlsby et al., 2019; Hu et al., 2022; Sung et al., 2022; Zhang et al., 2023) và tập trung đánh giá của chúng tôi trên benchmark GLUE phổ biến. Chúng tôi bổ sung sử dụng XNLI (Conneau et al., 2018) để đánh giá hiệu suất của phương pháp chúng tôi với dữ liệu đa ngôn ngữ. Các nguồn dữ liệu hoàn chỉnh và chi tiết thực hiện được liệt kê trong Phụ lục C và Phụ lục D, tương ứng.

Mô hình. Tương tự, chúng tôi chọn nhiều mô hình phổ biến được sử dụng trong công trình hiện có. Tuy nhiên, chúng tôi nhấn mạnh việc có nhiều từ vựng khác nhau (Bảng 5, Phụ lục A). Đối với các mô hình đơn ngôn ngữ, chúng tôi sử dụng BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), và DeBERTaV3 (He et al., 2023). Đối với các mô hình đa ngôn ngữ, chúng tôi sử dụng mBERT (Devlin et al., 2019), XLM-RoBERTa (Conneau et al., 2020), và XLM-V (Liang et al., 2023). Để đánh giá hiệu suất của các mô hình distilled, chúng tôi cũng sử dụng các counterpart distilled có sẵn: DistilBERT, DistilRoBERTa, và DistilmBERT (Sanh et al., 2020a). Để so sánh công bằng giữa các mô hình, chúng tôi nhất quán chọn kích thước base (dmodel=768).

Các metric hiệu quả bộ nhớ. Tuân theo quy ước trong tài liệu PEFT (Houlsby et al., 2019; Hu et al., 2022; Ben Zaken et al., 2022), chúng tôi báo cáo hiệu quả bộ nhớ về các tham số mô hình. Điều này có lợi vì tránh các yếu tố gây nhiễu như độ chính xác trọng số, lựa chọn optimizer, thực hiện phần mềm, và kích thước batch.

6 Kết quả
Từ vựng lớn hơn thấy tiết kiệm bộ nhớ nhiều hơn. Bảng 2 trình bày sự giảm tham số cho mỗi mô hình trên benchmark GLUE. Theo kỳ vọng của chúng tôi từ Phần 3, chúng tôi thường quan sát thấy khi kích thước từ vựng tăng (Bảng 5, Phụ lục A), tiềm năng tiết kiệm bộ nhớ cũng tăng. Ví dụ, sự giảm trung bình tham số nhúng 47.3% đạt được cho BERT, 52.1% cho RoBERTa, và 72.4% cho DeBERTaV3.

Tiết kiệm bộ nhớ khác nhau giữa các dataset. Phù hợp với kỳ vọng của chúng tôi từ Phần 3, tiết kiệm bộ nhớ khác nhau đáng kể giữa các dataset. Đối với BERT, ma trận nhúng có thể được giảm 94.3% cho dataset nhỏ nhất (WNLI), nhưng chỉ 11.5% cho lớn nhất (QQP). Chúng tôi chứng minh rằng hiệu suất nhiệm vụ downstream vẫn nhất quán trên các mô hình và dataset trong Phụ lục E.

Các mô hình distilled có lợi đáng kể. Xem xét các mô hình distilled, chúng tôi quan sát thấy tất cả đều đạt được sự giảm giống hệt trong tham số nhúng so với counterpart gốc của chúng. Điều này vì chúng sử dụng cùng từ vựng và kích thước nhúng (Sanh et al., 2020a). Tuy nhiên, chúng cung cấp tiết kiệm tổng thể đáng kể, vì có ít tham số được phân bổ cho các lớp transformer.

--- TRANG 4 ---
[Bảng 2: Sự giảm tham số nhúng và mô hình (%) cho mỗi mô hình trên benchmark GLUE.]

[Bảng 3: Sự giảm tham số mô hình (triệu) cho mỗi kích thước DeBERTaV3 trên benchmark GLUE.]

Tiết kiệm bộ nhớ tỷ lệ với kích thước mô hình. Bảng 3 trình bày sự giảm tham số mô hình cho mỗi mô hình từ họ DeBERTaV3. Chúng tôi quan sát thấy sự giảm này tiếp tục tăng với kích thước mô hình. Trung bình, kích thước cực nhỏ được giảm 35.6M tham số, trong khi kích thước lớn được giảm 95.0M tham số. Mặc dù cùng từ vựng kích thước cố định được chia sẻ trên các mô hình, chiều nhúng tiếp tục tăng (Bảng 6, Phụ lục A), cung cấp tiết kiệm bộ nhớ thêm. Ngoại lệ là kích thước nhỏ và base, nơi sự khác biệt duy nhất là số lượng lớp.

Các mô hình đa ngôn ngữ đạt được tiết kiệm cực độ. Không ngạc nhiên, các mô hình đa ngôn ngữ thể hiện tiết kiệm bộ nhớ cực độ trên benchmark GLUE đơn ngôn ngữ. Trung bình, sự giảm tham số mô hình 44.2% đạt được cho mBERT, 64.3% cho XLM-RoBERTa, và 85.7% cho XLM-V. Bảng 4 trình bày sự giảm tham số cho các mô hình đa ngôn ngữ khi tinh chỉnh trên các tập con khác nhau của XNLI. Ngay cả khi tinh chỉnh trên tất cả mười lăm ngôn ngữ, các mô hình này vẫn thể hiện tiết kiệm bộ nhớ đáng kể từ 23.0% đến 58.4%.

[Bảng 4: Sự giảm tham số trên các tập con khác nhau của XNLI, ngoài tất cả mười lăm ngôn ngữ.]

7 Kết luận
Trong bài báo này, chúng tôi xác định rằng nhiều dataset tinh chỉnh không sử dụng phần lớn từ vựng LM. Sau đó chúng tôi đề xuất Thích ứng ma trận nhúng một phần (PEMA), một phương pháp đơn giản nhưng hiệu quả để giảm thiểu việc sử dụng bộ nhớ LM trong quá trình tinh chỉnh, trực giao với nhiều phương pháp hiện có. Cuối cùng, chúng tôi chứng minh thực nghiệm rằng phương pháp của chúng tôi cung cấp tiết kiệm bộ nhớ đáng kể trên nhiều nhiệm vụ và mô hình phổ biến, mà không ảnh hưởng đến hiệu suất. Là công việc tương lai, chúng tôi quan tâm đến việc thích ứng phương pháp của chúng tôi cho ma trận nhúng đầu ra để cung cấp tiết kiệm bộ nhớ thêm.

--- TRANG 5 ---
Hạn chế
Xử lý dataset tinh chỉnh để đánh giá việc sử dụng từ vựng phát sinh chi phí thời gian chạy. Tuy nhiên, chúng tôi quan sát thấy chi phí này không đáng kể. Chúng tôi cung cấp phân tích chi tiết về vấn đề này trong Phụ lục F.

Cân nhắc đạo đức
Phương pháp của chúng tôi cải thiện hiệu quả bộ nhớ của tinh chỉnh LM, do đó tạo điều kiện cho việc sử dụng phần cứng ít mạnh hơn. Mặc dù chúng tôi hy vọng rằng điều này có thể giảm tác động môi trường của tinh chỉnh LM, chúng tôi thừa nhận rằng nó có thể được sử dụng để hỗ trợ tinh chỉnh các LM thậm chí lớn hơn. Chúng tôi cũng nhận ra bản chất sử dụng kép của các LM và thừa nhận rằng các nỗ lực cải thiện hiệu quả, bao gồm của chúng tôi, có thể hạ thấp rào cản tham gia cho việc sử dụng sai (Weidinger et al., 2022).

Lời cảm ơn
Chúng tôi chân thành biết ơn Nafise Sadat Moosavi, Huiyin Xue, Atsuki Yamaguchi, và các reviewer ẩn danh vì phản hồi vô giá của họ. MW được hỗ trợ bởi Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications được tài trợ bởi UK Research and Innovation grant EP/S023062/1.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ với tất cả các trích dẫn được duy trì như trong bản gốc]

--- TRANG 6 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 7 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 8 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 10 ---
A Kích thước từ vựng mô hình ngôn ngữ
Bảng 5 trình bày kích thước từ vựng (|V|) cho các mô hình được sử dụng trong thí nghiệm của chúng tôi, được xác định bởi Hugging Face Hub. Chúng tôi cũng báo cáo số lượng tham số nhúng (Nemb), số lượng tham số mô hình (N), và tỷ lệ tổng thể của tham số nhúng (Nemb/N). Các metric này cũng được trình bày trong Bảng 6 cho mỗi kích thước DeBERTa, ngoài các siêu tham số mô hình.

B Nén mô hình ngôn ngữ
Bổ sung cho thảo luận về công trình liên quan (Phần 2), chúng tôi thêm thảo luận về mối quan hệ với nhiều phương pháp nén LM phổ biến. Chúng tôi nhấn mạnh rằng các phương pháp này trực giao với phương pháp đề xuất của chúng tôi.

Chưng cất kiến thức. Chưng cất kiến thức (Hinton et al., 2015) nhằm đạt được hiệu suất tương đương bằng cách huấn luyện mô hình nhỏ hơn sử dụng dự đoán từ mô hình lớn hơn. Phương pháp này đã được áp dụng thành công cho các LM (Sanh et al., 2020a; Sun et al., 2020). Nó cũng có thể được sử dụng để huấn luyện các mô hình với từ vựng nhỏ hơn so với bản gốc (Zhao et al., 2021; Singh và Lefever, 2022).

Pruning. Pruning mạng neural (LeCun et al., 1989) tìm cách loại bỏ các trọng số dư thừa trong khi bảo tồn hiệu suất. Các phương pháp hiện có tập trung vào pruning các trọng số tuyến tính và attention trong các LM (Sanh et al., 2020b; Kurtic et al., 2022; Frantar và Alistarh, 2023). Tuy nhiên, pruning ma trận nhúng được tránh rộng rãi, vì nó có thể làm hại đáng kể hiệu suất (Kurtic et al., 2024).

Lượng tử hóa. Mục đích của lượng tử hóa là đại diện cho các trọng số mạng neural sử dụng độ chính xác thấp hơn, do đó giảm chi phí tính toán. Các nỗ lực lượng tử hóa LM gần đây thường tập trung vào lượng tử hóa các lớp tuyến tính (Dettmers et al., 2022; Yao et al., 2022; Frantar et al., 2023). Ma trận nhúng cũng có thể được lượng tử hóa (Zafrir et al., 2019; Bondarenko et al., 2021), mặc dù Shen et al. (2020) thấy rằng nó nhạy cảm hơn với lượng tử hóa.

C Dataset
Trong tất cả các trường hợp, chúng tôi sử dụng phiên bản công khai có sẵn của mỗi dataset từ Hugging Face (Lhoest et al., 2021). Benchmark GLUE bao gồm nhiều nhiệm vụ đa dạng, bao gồm chấp nhận ngôn ngữ học (CoLA, Warstadt et al. 2019), phân tích cảm xúc (SST-2, Socher et al. 2013), diễn giải/độ tương tự câu (MRPC, Dolan và Brockett 2005; STS-B, Cer et al. 2017; QQP, Iyer et al. 2017), và suy luận ngôn ngữ tự nhiên (RTE, Dagan et al. 2006; WNLI, Levesque et al. 2012; QNLI, Rajpurkar et al. 2016; MNLI, Williams et al. 2018). Số lượng ví dụ cho mỗi phần trong mỗi dataset được liệt kê trong Bảng 7. Dataset XNLI (Conneau et al., 2018) mở rộng MNLI đến 15 ngôn ngữ: Ả Rập, Bulgaria, Trung Quốc, Anh, Pháp, Đức, Hy Lạp, Hindi, Nga, Tây Ban Nha, Swahili, Thái, Thổ Nhĩ Kỳ, Việt Nam, và Urdu.

D Thực hiện & Phần cứng
Chúng tôi thực hiện các thí nghiệm của mình sử dụng PyTorch (Paszke et al., 2019), Hugging Face Transformers (Wolf et al., 2020) và Hugging Face Datasets (Lhoest et al., 2021). Vì hiệu suất nhiệm vụ downstream không liên quan đến nghiên cứu này, chúng tôi không thực hiện điều chỉnh siêu tham số. Thay vào đó, chúng tôi tuân theo rộng rãi các siêu tham số từ Devlin et al. (2019), được liệt kê trong Bảng 8.

Chúng tôi tinh chỉnh tất cả các mô hình sử dụng một GPU NVIDIA Tesla V100 (SXM2 32GB) và CPU Intel Xeon Gold 6138. Để nhất quán, mỗi loại mô hình được đánh giá trên cùng phần cứng vật lý.

E Tinh chỉnh trên GLUE
Bảng 10 trình bày hiệu suất nhiệm vụ cho mỗi mô hình trên benchmark GLUE. Chúng tôi quan sát thấy hiệu suất phần lớn giống hệt, mặc dù có những biến động thỉnh thoảng nơi PEMA thực hiện tốt hơn hoặc tệ hơn một phần so với baseline. Cuối cùng, chúng tôi lưu ý rằng XLM-RoBERTa và XLM-V đều thể hiện hiệu suất rất thấp trên CoLA, mặc dù vấn đề này cũng đã được quan sát trong các nghiên cứu khác, ví dụ Zhou et al. (2023).

F Tác động thời gian chạy
Bảng 9 trình bày thời lượng trung bình và độ lệch chuẩn của việc áp dụng PEMA cho RoBERTa và quá trình tinh chỉnh tiếp theo. Nó cũng cho thấy tỷ lệ thời gian được dành để áp dụng PEMA so với tinh chỉnh. Chúng tôi quan sát thấy đối với năm trong chín dataset trong GLUE, việc áp dụng PEMA mất ít hơn nửa giây. Đối với tám trong chín dataset, việc áp dụng PEMA mất ít hơn 1% thời lượng tinh chỉnh. Chúng tôi lưu ý rằng thời gian để áp dụng PEMA tương quan với kích thước dataset tinh chỉnh (Hình 2). Tổng thể, chúng tôi lưu ý rằng thời gian để áp dụng PEMA thường là phần nhỏ so với thời lượng tinh chỉnh, mặc dù chúng tôi không nỗ lực tối ưu hóa thực hiện của mình. Là hướng dẫn cho các nỗ lực tối ưu hóa tương lai, chúng tôi lưu ý rằng các hoạt động xử lý dataset trong PEMA có thể song song hóa dễ dàng.

--- TRANG 11 ---
[Bảng 5: Kích thước từ vựng và phân bổ tham số cho mỗi mô hình được sử dụng trong thí nghiệm của chúng tôi.]

[Bảng 6: Họ mô hình DeBERTaV3.]

[Bảng 7: Số lượng ví dụ cho mỗi phần trong mỗi dataset GLUE.]

[Bảng 8: Các siêu tham số được sử dụng cho mỗi tập thí nghiệm.]

[Bảng 9: Thời lượng trung bình (giây) và độ lệch chuẩn trên năm lần chạy của việc áp dụng PEMA cho RoBERTa và tinh chỉnh trên các dataset GLUE.]

--- TRANG 12 ---
[Bảng 10: Kết quả trên tập validation cho mỗi nhiệm vụ từ GLUE. Chúng tôi trình bày hiệu suất trung bình trên năm seed khác nhau, kèm theo mean và độ lệch chuẩn tổng thể. Chúng tôi báo cáo tương quan Matthews cho CoLA, F1 cho QQP, tương quan Spearman cho STS-B, và độ chính xác cho các nhiệm vụ còn lại.]
