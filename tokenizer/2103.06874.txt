# 2103.06874.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/tokenizer/2103.06874.pdf
# File size: 483440 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CANINE : Pre-training an EfÔ¨Åcient Tokenization-Free Encoder
for Language Representation
Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting
Google Research
{jhclark,dhgarrette,iuliaturc,jwieting}@google.com
Abstract
Pipelined NLP systems have largely been
superseded by end-to-end neural model-
ing, yet nearly all commonly-used models
still require an explicit tokenization step.
While recent tokenization approaches based
on data-derived subword lexicons are less
brittle than manually engineered tokenizers,
these techniques are not equally suited to all
languages, and the use of any Ô¨Åxed vocab-
ulary may limit a model‚Äôs ability to adapt.
In this paper, we present C ANINE , a neural
encoder that operates directly on character
sequences‚Äîwithout explicit tokenization or
vocabulary‚Äîand a pre-training strategy that
operates either directly on characters or op-
tionally uses subwords as a soft inductive
bias. To use its Ô¨Åner-grained input ef-
fectively and efÔ¨Åciently, C ANINE combines
downsampling, which reduces the input se-
quence length, with a deep transformer
stack, which encodes context. C ANINE out-
performs a comparable mB ERT model by
5.7 F1 on T YDIQA, a challenging mul-
tilingual benchmark, despite having fewer
model parameters.
1 Introduction
End-to-end neural models have generally replaced
the traditional NLP pipeline, and with it, the
error cascades and feature engineering common
to such systems, preferring instead to let the
model automatically induce its own sophisticated
representations. Tokenization, however, is one
of few holdovers from that era, with nearly all
commonly-used models today requiring an ex-
plicit preprocessing stage to segment a raw text
CANINE :Character Architecture with No tokenization
InNeural Encoders.
Code and checkpoints are available on GitHub at
http://caninemodel.page.link/code .
Published in Transactions of the Association for Compu-
tational Linguistics (TACL) , 2022.string into a sequence of discrete model inputs.
Broadly speaking, tokenizers are generally either
carefully constructed systems of language-speciÔ¨Åc
rules, which are costly, requiring both manual
feature engineering and linguistic expertise, or
data-driven algorithms such as Byte Pair Encod-
ing (Sennrich et al., 2016), WordPiece (Wu et al.,
2016), or SentencePiece (Kudo and Richardson,
2018) that split strings based on frequencies in a
corpus, which are less brittle and easier to scale,
but are ultimately too simplistic to properly handle
the wide range of linguistic phenomena that can‚Äôt
be captured by mere string-splitting (¬ß2.1).
The degree of sophistication required to accu-
rately capture the full breadth of linguistic phe-
nomena, along with the infeasibility of writing
such rules by hand across all languages and do-
mains, suggests that explicit tokenization itself is
problematic. In contrast, an end-to-end model
that operates directly on raw text strings would
avoid these issues, instead learning to compose in-
dividual characters into its own arbitrarily com-
plex features, with potential beneÔ¨Åts for both ac-
curacy and ease of use. While this change is con-
ceptually very simple‚Äîone could replace the sub-
word vocabulary in a model like B ERT (Devlin
et al., 2019) with a vocabulary made solely of indi-
vidual characters‚Äîdoing so leads to two immedi-
ate problems. First, the computational complexity
of a transformer (Vaswani et al., 2017), the main
components in B ERTas well as other models such
as GPT (Radford et al., 2019; Brown et al., 2020)
and T5 (Raffel et al., 2020), grows quadratically
with the length of the input. Since standard sub-
word models have roughly four characters per sub-
word on average, the 4x increase in input sequence
length would result is a signiÔ¨Åcantly slower model.
Second, simply switching to a character vocabu-
lary yields empirically poor results (¬ß4.2).
In order to enable tokenization-free model-
ing that overcomes these obstacles, we presentarXiv:2103.06874v4  [cs.CL]  18 May 2022

--- PAGE 2 ---
CANINE . C ANINE is a large language encoder
with a deep transformer stack at its core. Inputs to
the model are sequences of Unicode characters.1
To represent the full space of Unicode characters2
without a vocabulary, we employ a hashing strat-
egy. To avoid the slowdown from increasing the
sequence length, C ANINE uses strided convolu-
tions to downsample input sequences to a shorter
length before the deep transformer stack.
Like B ERT, we pre-train C ANINE on the
Masked Language Model (MLM) and Next Sen-
tence Prediction (NSP) tasks. For the MLM task,
CANINE offers two options:
1. A fully character-level loss that autoregres-
sively predicts characters in masked spans.
2. A vocabulary-based loss that predicts the
identities of masked subword tokens. Crit-
ically, this tokenization is used only for the
pre-training loss; tokens are never input to
the encoder, and the tokenizer and subword
vocabulary can be safely discarded after pre-
training. This effectively converts the hard
constraint of token boundaries found in other
models into a soft inductive bias in C ANINE .
In this article, we contribute:
the Ô¨Årst pre-trained tokenization-free deep
encoder;
an efÔ¨Åcient model architecture that directly
encodes long sequences of characters with
speed comparable to vanilla B ERT; and
a model that performs no tokenization on the
input, avoiding the lossy information bottle-
neck associated with most pre-processing.
2 Motivation
2.1 Linguistic pitfalls of tokenization
Subword tokenizers are the de-facto standard in
modern NLP (Devlin et al., 2019; Raffel et al.,
2020; Brown et al., 2020). These algorithms are
limited to only simple word-splitting operations.
While this is perhaps a reasonable approach for
a language with impoverished morphology such
as English, it is much less appropriate in the face
of phenomena like agglutinative morphology, non-
1We consider splitting on Unicode characters to be
tokenization-free because it depends only on the (determinis-
tic) process deÔ¨Åned by the Unicode standard, and not on any
models, hand-crafted rules, or other linguistic knowledge.
2Unicode deÔ¨Ånes 1,114,112 total codepoints , of which
only 143,698 are assigned to characters as of Unicode 13.0.
This covers 154 scripts and over 900 languages.I.J¬ª k-t-b ‚Äúwrite‚Äù (root form)
I.J¬ª kataba ‚Äúhe wrote‚Äù
I.J¬ª kattaba ‚Äúhe made (someone) write‚Äù
I.J¬ª @iktataba ‚Äúhe signed up‚Äù
Table 1: Non-concatenative morphology in Arabic.4
When conjugating, letters are interleaved within the
root. The root is therefore not separable from its in-
Ô¨Çection via any contiguous split.
concatenative morphology, consonant mutation,
vowel harmony, etc.
Even in high-resource languages, subword
models still tend to struggle on challenging do-
mains, such as informal text, which often includes
typos, spelling variation,5transliteration, or emoji
(O‚ÄôConnor et al., 2010). B ERT, which uses Word-
Piece tokenization, is sensitive to corruptions of
the input, both natural typos (Sun et al., 2020)
and adversarial manipulations (Pruthi et al., 2019),
with some of the loss attributable to corrupted
strings no longer being covered by the vocabulary.
Seemingly safe heuristics used by these algo-
rithms, such as splitting on whitespace and punc-
tuation, are problematic when applied to lan-
guages that do not use spaces between words
(Thai, Chinese) or use punctuation as letters
(Hawaiian,6Twi7). While SentencePiece does of-
fer the option to skip whitespace splitting, it is not
typically used due to poor empirical performance.
Fixed vocabulary methods can also force mod-
elers to choose between difÔ¨Åcult preprocessing
tradeoffs: should one keep accents, casing, etc.
and avoid destructive preprocessing?‚ÄîOr keep
such orthographic information and risk important
words dropping out of the frequency-based vocab-
ulary altogether due to the presence of multiple
variants of otherwise-similar words? For instance,
mB ERTinitially removed all diacritics, thus drop-
ping tense information in Spanish8and conÔ¨Çating
many unrelated words in Vietnamese.9
4From en.wikipedia.org/wiki/Arabic_verbs
5e.g. Spanish speakers may drop accents when typing.
6Hawaiian uses an apostrophe to indicate a glottal stop.
7Informal Twi uses a right paren ) to represent the letter O.
8Spanish past tense uses an accented Ô¨Ånal vowel.
9Vietnamese uses diacritics to indicate tones‚Äîoften the
only difference among several unrelated content words.

--- PAGE 3 ---
Finally, using a Ô¨Åxed vocabulary during pre-
training also creates complications for down-
stream tasks, which are subsequently tied to the
same tokenizer and vocabulary used for pre-
training, even if it is not well-suited for the target
domain and/or end-task. Boukkouri et al. (2020)
showed that B ERT‚Äôs Wikipedia+BooksCorpus
WordPiece vocabulary results in excessive seg-
mentation when Ô¨Åne-tuning on medical data, di-
minishing the beneÔ¨Åt of pre-training as a strategy.
2.2 Enabling better generalization
Much as Tenney et al. (2019) showed that large en-
coders learn elements of the classic NLP pipeline,
it seems natural to let the model discover tokeniza-
tion as well. With this in mind, we seek an ap-
proach that can better generalize beyond the or-
thographic forms encountered during pre-training.
In terms of scientiÔ¨Åc inquiry, we would like to
know whether we can build models that learn how
tocompose words where appropriate, and memo-
rizethem where memorization is needed. Large
frequency-derived vocabularies partially mitigate
this problem by simply memorizing more, but lan-
guage inherently requires aspects of both memo-
rization and composition. By building a model
that directly engages with these issues within the
small scale of word composition, we hope to en-
able future work studying these problems at larger
scales such as phrasal constructions.
Practically, generalization is hindered for vo-
cabulary elements that are slight orthographic
variations, where one is very infrequent. Hypo-
thetically, a model may estimate a very good em-
bedding for a common vocabulary element kitten ,
but a poor embedding for the less frequent element
kittens since the model has no a priori knowledge
that they are related. Embeddings that are rarely
touched during pre-training will not be updated
much beyond their random initializations.
2.3 Reducing engineering effort
Mature tokenizers often include years of hand-
engineered rules around special cases such as
email addresses, URLs, and handling unknown
words;10even fairly minimal modern tokenizers
include initial word-splitting heuristics followed
by a speciÔ¨Åc algorithm and vocabulary for further
breaking these tokens into subwords.
10For example, should a subword containing an unknown
character be a separate token, or should the unknown charac-
ter be separated as its own token?Modern pre-trained models also have many re-
quirements throughout their lifecycle: Between
the time a model is pre-trained, Ô¨Åne-tuned, and
served‚Äîpotentially months or years apart‚Äîits
weights and model implementation may be con-
verted to be compatible with another toolkit, its
Ô¨Åne-tuning data may be tokenized in a different
way, and the natural distribution of words may be
quite different. All of these things introduce ample
opportunities for mismatches to arise between to-
kenization and the vocabulary from pre-training.
Yet this same pre-training paradigm presents an
advantage for character models: access to a far
more (unsupervised) data to learn word compo-
sition from characters; without transfer learning,
this has historically been impractical for many
tasks having little supervised data.
3 C ANINE
CANINE consists of three primary components:
(1) a vocabulary-free technique for embedding
text; (2) a character-level model that is efÔ¨Åcient
by means of downsampling and upsampling; and
(3) an effective means of performing masked lan-
guage modeling on a character-level model.
3.1 Model
CANINE is designed to be a minimally modiÔ¨Åed
variant of the deep transformer stack found in
modern encoders such as GPT, (m)B ERT, XLM,
and XLM-R such that its architecture is easily
adoptable by other models in this family. The sim-
plest implementation of such a character model
would be to feed characters at each position in
place of subwords. However, this approach would
result in far more sequence positions given the
same input text, leading to linearly more com-
pute in feed forward layers and quadratically more
compute in self-attention layers.
The overall form of the C ANINE model is the
composition of a downsampling function D OWN ,
a primary encoder E NCODE , and an upsampling
function U P;11given an input sequence of char-
acter embeddings e2Rndwith length nand
dimensionality d:
Yseq UP(ENCODE (DOWN(e)))
11Enveloping the attention stack between downsampling
and upsampling layers is similar to the Funnel-Transformer
(Dai et al., 2020), which operates on WordPiece. However,
many of its design choices (e.g., average pooling, their resid-
ual structure) did not work well in C ANINE .

--- PAGE 4 ---
Ô∏∏Codepoint
IntegersHash
EmbeddingDeep Transformer StackPosition 0 Used as [CLS] representation for classiÔ¨ÅcationFinal Character
Representation
for Sequence TasksCharacter
EmbeddingsSingle
Local
TransformerDownsample‚Ä®(Strided
Convolution)Contextualized
CharactersConcatenated
Convolved
RepresentationsSingle
Transformerxehinithdownh'downyseqyclshupUpsamplingFigure 1: C ANINE neural architecture.
where Yseq2Rndis the Ô¨Ånal representation for
sequence prediction tasks. Similarly, for classiÔ¨Å-
cation tasks, the model simply uses the zeroth ele-
ment of the primary encoder:
ycls [ENCODE (DOWN(e))]0
Preprocessing Like existing models, the input
to C ANINE must ultimately be represented as a se-
quence of integers, but because the nature of char-
acters is well-deÔ¨Åned and standardized by Uni-
code, preprocessing code that would typically be
hundreds or thousands of lines can be replaced by
a very simple procedure: just iterate over the char-
acters in the input string, and return their code-
point integer values (e.g., a single line of code12in
Python). Furthermore, because codepoint values
are part of the Unicode Standard, they are docu-
mented publicly, already supported by program-
ming languages, and will not change over time,
unlike arbitrary vocabulary-based IDs.
Character hash embeddings CANINE uses
hashing (Svenstrup et al., 2017) to support em-
bedding the full space of Unicode codepoints with
a relatively small number of parameters, but to
reduce the chance that different codepoints will
share exactly the same representation, we deÔ¨Åne
a generalization of the standard hashing approach
in which we apply multiple hash functions to each
codepoint and concatenate the representations as-
sociated with the various hash values.
More formally, given a single codepoint13xi2
N, we apply Khash functionsHk:N!N, and
look up each hashing result in its own embedding
matrix14Ek2RBd0, yielding Kembeddings of
sized0=d=K, which are then concatenated into a
12Python preprocessing: [ord(c) for c in text]
13Conceptually, a codepoint is a character; however, a
Unicode codepoint is deÔ¨Åned precisely and unambiguously.
14CANINE uses learned embeddings, not random embed-
ding as in other hash embeddings (Kaliamoorthi et al., 2019).single representation of size d:
ei KM
kLOOKUP (Hk(xi)%B;Ek)
wheredenotes vector concatenation. We refer
to these as the character embeddings e2Rnd.
In our experiments, we use d= 768 ,K= 8, and
B= 16 k.15
While each individual hash function is subject
to hash collisions,16the overall effect is minimal
since each function only accounts for a small por-
tion of the codepoint‚Äôs overall embedding, and it
is highly improbable that the other hash functions
will produce the same collisions.
Because the model always supports all code-
points, it is possible to learn representations dur-
ing Ô¨Åne-tuning for characters (and, by extension,
words, scripts, etc.) that were never seen during
pre-training, while still making use of what pre-
training learned about word composition and sen-
tence structure.
Optional vocabulary-free n-grams We can
also redeÔ¨Åne the embeddings eiabove to include
character n-grams, again without a Ô¨Åxed vocab-
ulary, such that each n-gram order contributes
equally to a summed embedding:17
eN
i KM
kNX
jLOOKUP 
H0
k(xi:::j)%B;Ej;k
H0
k(xi:::j) =(
Hk(xi) ifi=j
H0
k 
xi+H0
k(x(i+1):::j)
o/w
15The memory footprint of these hash embeddings is
equivalent to a vocabulary embedding with 16k items.
16This is nota probing/chaining hash table, but rather as
anapproximate map , where we expect and tolerate collisions,
similar to a Bloom Map (Talbot and Talbot, 2008).
17We use B= 15 k and N= 4for our n-grams.

--- PAGE 5 ---
This formulation still admits tokenization-free
modeling, but provides the model with an induc-
tive bias that favors slightly more memorization
via a compute-cheap means of adding parameters.
Notably, it also allows the model‚Äôs input signature
to remain a simple sequence of codepoints.
Downsampling To make C ANINE efÔ¨Åcient, we
use a multi-part downsampling strategy. First, we
encode characters using a single-layer block-wise
local attention transformer. This model performs
self-attention only within each block of a pre-
deÔ¨Åned size,18saving the quadratic cost of atten-
tion while leveraging the linguistic intuition that
word composition‚Äîi.e., the kind of composition
relevant in the lowest layers of the model (Tenney
et al., 2019)‚Äîtends to happen at a very local level.
Next, we use a strided convolution to reduce the
number of sequence positions to be similar to that
of a word piece model.19Given character embed-
dings e2Rndwith a sequence length of nchar-
acters and dimensionality d, we use a convolution
with a stride of rto downsample the sequence:
hinit LOCAL TRANSFORMER 1(e)
hdown STRIDED CONV(hinit; r)
We refer to this output as the downsampled po-
sitions :hdown2Rmdwhere m=n=ris the
number of downsampled positions. In our exper-
iments, we use r= 4 andn= 2048 such that
m= 512 , giving C ANINE ‚Äôs primary encoder‚Äîthe
transformer stack‚Äîthe same length as in mB ERT.
Deep transformer stack After downsampling,
CANINE applies a deep transformer stack with
Llayers to the resulting downsampled positions.
This is the same as the core of B ERT and deriva-
tive models, and remains the core of C ANINE in
that it accounts for the vast majority of its compute
and parameters, though we note that this middle
portion of the model could easily be replaced with
any other sequence-to-sequence model including
those with better compute performance such as
Performer (Choromanski et al., 2021), Big Bird
(Zaheer et al., 2020), RFA (Peng et al., 2021),
ETC (Ainslie et al., 2020), etc. This portion of the
model yields a new downsampled representation
18We use blocks of 128 characters in our experiments.
19In our experiments, we found a downsampling rate of 4X
to result in high quality with a speed comparable to B ERT.h0
down2Rmd:
h0
down TRANSFORMER L(hdown)
ycls= [h0
down]0
We used L= 12 to match mB ERT.
Upsampling While the above architecture is
sufÔ¨Åcient for classiÔ¨Åcation tasks, sequence predic-
tion tasks require that the model expose an output
layer with the same sequence length as the input
(i.e., characters are the model‚Äôs input and output
‚ÄúAPI‚Äù for tasks like tagging and span prediction).
We reconstruct a character-wise output repre-
sentation by Ô¨Årst concatenating the output of the
original character transformer (above) with the
downsampled representation produced by the deep
transformer stack. (Note that since each down-
sampled position is associated with exactly rchar-
acters for a downsampling rate of r, each posi-
tion of downsampled representation is replicated
rtimes before concatenation.) More formally,
hup CONV 
hinith0
down; w
yseq TRANSFORMER 1 
hup
whereindicates vector concatenation of the rep-
resentations (i.e. not sequences) such that C ONV
projects from Rn2dback to Rndacross a win-
dow of wcharacters.20Applying a Ô¨Ånal trans-
former layer (standard, not local) yields a Ô¨Ånal se-
quence representation yseq2Rnd.
Residual connections While the initial charac-
ter encoder (before downsampling) and Ô¨Ånal char-
acter encoder (after upsampling) both represent
character positions , they conceptually have very
different purposes in the network. Intuitively, we
think of the initial character encoder as composing
characters to create a more word-like representa-
tion, while the Ô¨Ånal character encoder is extract-
ing the in-context representation that‚Äôs relevant for
predicting the ‚Äúmeaning‚Äù of the content at each
position; C ANINE must be able to deal with addi-
tional ambiguity during upsampling since a single
downsampled position may span more than one
conceptual word. Because of the different roles
of these induced features, we do notuse residual
connections from hinittohup.
20We use w= 4in our experiments.

--- PAGE 6 ---
[CLS]Thebigcatsat.[SEP]‚Ä¶To
Model
Inputs[CLS]Theig[SEP]‚Ä¶‚ìç Not selected for
    MLM prediction
   (85% of spans)Identity
Transform
(10% of predictions)bTo
Prediction
Targetsigbsat.ShuÔ¨Ñe
Prediction Order1234567Mask
(90% of predictions)[M][M][M][M]catFigure 2: C ANINE -C pre-training data preparation (¬ß3.2.1). Character-wise predictions are made by an auto-
regressive transformer layer that predicts then reveals one character at a time, in a shufÔ¨Çed order.
3.2 Pre-training
Recent pre-trained models ranging from B ERT to
T5 have largely used variations on a masked lan-
guage model (MLM) task (also known as span
corruption ) as an unsupervised pre-training loss
function‚Äîa means of generating synthetic ex-
amples that are not from any realistic task, yet
prepare a model to learn realistic tasks in fu-
ture phases of training (i.e. Ô¨Åne-tuning). The
CANINE pre-training procedure retains the MLM
task, and offers two distinct strategies for comput-
ing the MLM loss‚Äîautoregressive character pre-
diction vs. subword prediction‚Äîboth of which
yield a fully tokenization-free model following
pre-training. In our experiments, we use only one
of these losses at a time.
3.2.1 Autoregressive Character Loss
Span-wise masking CANINE -C is an autore-
gressive character loss that masks character spans
within each sequence. These spans are chosen
based on whitespace boundaries. No punctuation
splitting nor other heuristics are used. All char-
acters within the masked span are replaced by a
special mask codepoint in the input.21No random
subword replacement is performed as there is no
subword vocabulary.22
Span prediction CANINE -C auto-regressively
predicts the masked characters. The order of the
masked positions is shufÔ¨Çed such that masked con-
text is not necessarily revealed left-to-right, but
rather a single character at a time. The pre-training
data preparation is shown in Figure 2. Masked in-
puts are fed to the model as x. The output of the
CANINE model yseqand the embeddings egof the
gold characters g(i.e. the character positions se-
21We use codepoints in Unicode‚Äôs Private Use Area block
such that the input remains a valid Unicode string.
22Though we expect that future work on vocabulary-free
random replacement may improve quality.lected for MLM prediction) are concatenated and
then fed through a small feed-forward neural net-
work to project back to the original dimensional-
ityd; these are Ô¨Ånally shufÔ¨Çed and used by a sin-
gle layer auto-regressive transformer with a left-
to-right self-attention mask:23
^y TRANSFORMER AUTOREG 
egyseq
This representation ^yis then used to predict each
character. To avoid wasting time on a large out-
put weight matrix and softmax, the gold target
classes tare bucketed codepoint IDs such that
ti=gi%B. This is similar to the strategy used
in the character hash embedder (¬ß3.1). The oc-
cassional collisions among characters is less prob-
lematic due (a) the fact that this is an encoder-only
model and (b) that the embeddings must still retain
contextual information in order to correctly pre-
dict characters. Because we‚Äôre only predicting a
relatively small subsequence of the input (15% in
our experiments), the cost of this layer is small.
3.2.2 Subword Loss
We also experiment with C ANINE -S, a subword-
based loss function, to demonstrate how a token-
aware pre-training loss can still be paired with
a tokenization-free model such that the tokenizer
and vocabulary are discarded after pre-training.
Span-wise masking Like mB ERT‚Äôs MLM
setup, each span in C ANINE -S corresponds to a
single subword. As with the autoregressive loss,
all characters within the masked span are replaced
with a special ‚Äúmask‚Äù codepoint. Random
replacements of subwords are chosen from the
vocabulary of same-length subwords such that
the length of the character sequence remains un-
changed; more formally, given a subword selected
23The left-to-right self-attention masking is with regard to
theshufÔ¨Çed sequence.

--- PAGE 7 ---
for random replacement xand a vocabulary of
subwords V,x‚Äôs replacement will be drawn from
the subset of v2Vwhere L EN(v) =LEN(x).
Span prediction Within each masked character
span, C ANINE -S randomly selects a character po-
sition where the model will make a prediction; the
model predicts the identity of the masked subword
via softmax. The associated subword embeddings
are discarded after pre-training.
3.2.3 Targeted Upsampling
By design, each Ô¨Ånal character representation (af-
ter upsampling) is a function of the output of the
initial character encoder (before downsampling)
and the output of the deep transformer stack‚Äî
there are no inter-position dependencies across the
upsampled sequence. This depends on the up-
sampler using position-wise feed-forward projec-
tions and a single transformer layer. During pre-
training, we leverage this design to improve speed
by only performing upsampling on the sequence
positions that will be used by the MLM task p.
More formally, we use the following equivalent24
form of the U Pfunction during pre-training:
h
up GATHER 
p;hup
y
seq TRANSFORMER 1 
Q=h
up;KV=hup
3.2.4 Modularity
Unlike previous models, C ANINE removes both
the vocabulary and tokenization algorithm as fos-
silized parts of the Ô¨Ånal model that must be repli-
cated during Ô¨Åne-tuning and prediction. Regard-
less of which pre-training loss is chosen (char-
acters or subwords), the use of these compo-
nents in C ANINE is limited to a detail of the pre-
training procedure‚Äîan inductive bias of the loss
function‚Äîthat is then discarded. The Ô¨Åne-tuning
and prediction phases of the model lifecycle never
have any knowledge of what vocabulary or to-
kenization algorithm (if any) were used in pre-
training. This allows the model to natively pro-
24This highly-effective targeted upsampling optimization
is the primary reason that C ANINE uses a full Transformer
layer for the Ô¨Ånal full-length character sequence rather than
a local transformer. Because a block-wise local transformer
assumes uniform position-wise locality over attention blocks,
it is not trivial to combine these two optimizations; the lo-
cal self-attention mask would no longer be a simple block
diagonal. However, this Ô¨Ånal upsampling layer is discarded
for classiÔ¨Åcation tasks and so does not contribute any cost.
Hence, while it is possible to combine local attention and tar-
geted upsampling, this is left as future work.cess untokenized data, or even process data that
has been pre-processed by different tokenizers, a
situation that would otherwise introduce a signiÔ¨Å-
cant skew between training phases.
4 Experiments
4.1.1 Information-Seeking QA Data
TYDIQA: Primary Tasks TYDIQA is a
dataset of information-seeking questions in 11 ty-
pologically diverse languages (Clark et al., 2020).
Questions are written before answers, leading to
less lexical and morphological overlap between
questions and answers, which are drawn from
Wikipedia. We evaluate on the primary tasks.25
Passage Selection Task (S ELECT P) Given a
list of the passages in a Wikipedia article, return
either the index of the passage that answers the
question, or return NULL if the article contains no
acceptable answer.
Minimal Answer Span Task (M INSPAN)
Given a full Wikipedia article, return the start
and end byte indices of the minimal span that
completely answers the question. Alternatively,
a system may indicate that the article does not
contain an answer, or return YES orNOfor yes/no
type questions.
4.1.2 Named Entity Recognition Data
We also consider the task of named entity recogni-
tion (NER), which requires the model to identify
which spans of a sentence correspond to entities
and label the entity type. In all of our experiments,
we framed the task as sequence labeling, predict-
ing BIO-encoded span labels.
CoNLL NER We use Spanish and Dutch
data from the CoNLL 2002 NER task (Tjong
Kim Sang, 2002) and English and German from
the CoNLL 2003 NER task (Tjong Kim Sang and
De Meulder, 2003), all from the newswire domain.
MasakhaNER To widen the scope of our exper-
iments beyond Europoean languages, we also in-
clude MasakhaNER (Adelani et al., 2021), which
includes ten African languages (Amharic, Hausa,
Igbo, Kinyarwanda, Luganda, Luo, Nigerian Pid-
gin, Swahili, Wolof, and Yor√πb√°) with human an-
notations on local news text.
25As opposed to the simpliÔ¨Åed T YDIQA-G OLDP task,
which is part of the X TREME meta-benchmark.

--- PAGE 8 ---
Examples T YDIQA T YDIQA
Model Input MLM rLength / sec Params S ELECT P M INSPAN
mB ERT(public) Subwords Subwords ‚Äì 512 ‚Äì 179M 63.1 50.5
mB ERT(ours) Subwords Subwords ‚Äì 512 9000 179M 63.2 51.3
Chars Single Chars 1 2048 925 127M 59.5 (-3.7) 43.7 (-7.5)
Chars Subwords 1 2048 900 127M 63.8 (+0.6) 50.2 (-1.0)
CANINE -S Chars Subwords 4 2048 6400 127M 66.0 (+2.8) 52.5 (+1.2)
CANINE -C Chars Autoreg. Chars 4 2048 6050 127M 65.7 (+2.5) 53.0 (+1.7)
CANINE -C + n-grams Chars Autoreg. Chars 4 2048 5600 167M 68.1 (+4.9) 57.0 (+5.7)
Table 2: Direct comparison between mB ERT(rows 1‚Äì2) and C ANINE (rows 5‚Äì7) on T YDIQA. Public mB ERT
results are taken from the T YDIQA paper. Rows 3 and 4 show simple baselines that yield inefÔ¨Åcient / low-quality
performance. Despite operating on 4x more sequence positions, C ANINE remains comparable to mB ERTin terms
of speed. Pre-training example/sec are shown for our reported hardware (see Setup, ¬ß4.1). rrepresents the ratio for
downsampling. Parameters are calculated at Ô¨Åne-tuning time. All results are averaged over 3 Ô¨Åne-tuning replicas.
TYDIQA scores are F1 scores, macro-averaged across languages. Deltas from our mB ERT(the most comparable
baseline) are shown in parentheses.
4.1.3 Model ConÔ¨Åguration
Direct comparison with mB ERT In order to
determine which pre-training architecture pro-
duces better quality downstream predictions, we
compare C ANINE to mB ERT, which we re-
implemented and re-trained in order to hold as
many variables as possible constant. Note that we
intentionally do notcompare against public pre-
trained checkpoints that use different pre-training
corpora since (a) this would be a major confound-
ing variable and (b) most publicly available pre-
trained models are simply instantiations of B ERT,
including XLM-R26and X-STILTS.27
Setup We pre-train on the multilingual
Wikipedia data of mB ERT, which includes
104 languages. Similarly, we reuse mB ERT‚Äôs
exponential smoothing technique to weight the
languages within the pre-training samples. We
train for 124k steps with batch size 4096 (2.5
passes over the data) using the LAMB optimizer
(You et al., 2020) with a linearly decayed learning
rate of 0.018 where 2.5% of the steps are used
for warm-up. We use a sequence length of 512
for mB ERT, and 2048 for C ANINE , which results
in 512 downsampled positions in its core deep
transformer stack. We pre-train on 64 Cloud
TPUs v328for approximately one day (see results
for precise timings). For both mB ERT and
CANINE -S (C ANINE with the subword loss), we
26XLM-R instantiates B ERTwith a larger pre-training cor-
pus, larger model size, and larger vocabulary size.
27X-STILTS performs English Ô¨Åne-tuning on an existing
XLM-R checkpoint. (Phang et al., 2020)
28v3 TPUs have 16 GiB memory / core (128 GiB total).select 15% of subwords for the MLM loss and
predict up to 80 output positions; 80% of these
are masked in the input, 10% are randomly re-
placed, and 10% are unmodiÔ¨Åed. For C ANINE -C
(CANINE with the autoregressive character loss),
we select 15% of contiguous spans for the MLM
loss and predict up to 320 output characters,
and no random replacement is performed. For
TYDIQA, we use a maximum answer length of
100 characters, which is approximately the 99th
percentile answer length. Sequences longer than
the maximum sequence length are zero-padded,
following BERT.29
4.2 T YDIQA Results
Our main result is shown in Table 2. C ANINE -S
(CANINE with the subword loss) improves over
mB ERTin the T YDIQA S ELECT P task by 2.8 F1,
while using about 30% fewer parameters. Sim-
ilarly, C ANINE -C (C ANINE with the autoregres-
sive character loss), improves over mB ERTby 2.5
F1. Adding vocab-free character n-grams leads
to even further gains over mB ERT (+3.8 F1) and
even more on the M INSPAN task (+6.9 F1). A
language-wise breakdown is provided in Table 7
in the appendix.
29Each pre-training uses approximately 24 hours on 64
TPUs (1.5k TPU-hours), so the 18 pre-trainings in Tables
2/3/4 required about 28k TPU-hours. The 18 TyDi QA ex-
periments in these tables, each take about 1 hour on 16 TPUs,
each with 3 replicas (48 TPU-hours), about 1k TPU-hours to-
tal. The 3 NER experiments in Table 5 each took 3 hours on
4 TPUs with 3 replicas each (36 TPU-hours), 108 TPU-hours
total. Thus replicating the experiments in this paper would
take approximately 29k TPU-hours.

--- PAGE 9 ---
Question Passage Answer
Chelsea ina milikiwa na nani? Kwa kawaida Chelsea huvaa jezi ya blu, kaptula blu na soksi nyeupe. Nembo ya klabu ime-
badilishwa mara nyingi kulingana na wakati na kuboresha muonekano wa klabu. Nembo ya
sasa inaonesha picha ya simba akiwa amebeba mkuki. Tangu Julai 2003, Chelsea imekuwa
ikimilikiwa na Bilionea wa Kirusi, Roman Abramovich.
Who owns Chelsea? Chelsea usually wear blue jerseys, blue shorts and white socks. The club logo has been
changed many times over time and improved the club‚Äôs appearance. The current emblem
shows a picture of a lion carrying a spear. Since July 2003, Chelsea has been owned by
Russian billionaire Roman Abramovich.
Kampuni isambazayo umeme nchini Kenya
inaitwaje?Kenya Power and Lighting (KPLC) ni kampuni inayohusika na maambukizi ya umeme na
usambazaji wa umeme nchini Kenya.
What is the name of the company that dis-
tributes electricity in Kenya?Kenya Power and Lighting (KPLC) is a company responsible for electricity transmission
and distribution in Kenya.
Table 3: Kiswahili examples in which C ANINE improved over mB ERT in the T YDIQA S ELECT P task. On
examining the mB ERT‚Äôs subword tokenization, we observe that the segmentations do not align well, putting more
pressure on the model to combine them and more opportunities for some embeddings to be poorly estimated.
Top: The model must match a key word in the question milikiwa (own) to a morphological variant in the an-
swer iki-milikiwa (to be owned ). mB ERT‚Äôs WordPiece segmentation produces milik -iwa andiki -mi -iki -wa for
these, respectively. Bottom: The model must match i-sambaza-yo (distributes ) in the question with u-sambaza-ji
(distribution ). mB ERT‚Äôs WordPiece segmentation produces isam -ba -za -yo andusa -mba -zaj -i .
We also present results from some ablation
models as additional baselines in rows 3-4 of Ta-
ble 2. First, for row 3, we simply replace B ERT‚Äôs
subword vocabulary with a pure character vocabu-
lary, which makes characters both the input gran-
ularity and the unit of masking and prediction for
the MLM task, and observe that not only is the
model 10X slower than subword-based B ERT, but
the quality also suffers greatly. Then, for row 4,
we modify that model to use subwords for mask-
ing and MLM predictions, while keeping charac-
ters as the input granularity, and we see a substan-
tial quality improvement, though pre-training re-
mains extremely slow. Finally, by comparing to
the full C ANINE model in row 5, we can see that
adding the downsampling strategy improves speed
by 700%, and also leads to an additional small
bump in quality. We speculate that this additional
quality gain comes from giving the model a better
inductive bias toward more word-like units within
the deep transformer stack.
Analysis CANINE fares particularly well on
morphologically rich languages such as Kiswahili.
Table 3 shows examples where C ANINE out-
performs mB ERT on the T YDIQA S ELECT P
task. In particular, we observe examples where
Kiswahili‚Äôs rich morphology does not hinder the
matching process for C ANINE .
30These ablations were carried out during initial model de-
velopment, hence comparisons to a non-Ô¨Ånal model.4.3 Ablations
In Table 6, we consider minor modiÔ¨Åcations to the
Ô¨Ånal C ANINE architecture, and evaluate the effect
of each on the downstream quality of the model.30
Attending directly to h0
down Instead of attend-
ing to the character-wise sequence hup, we attend
to the downsampled sequence:
y+
seq=TRANSFORMER 1 
Q=hup;KV=h0
down
While this change reduces the overall FLOPS of
the model due to the reduced attention computa-
tion, it does not have a major effect on pre-training
throughput. However, it does substantially de-
grade quality.
Number of hash buckets We reduce the num-
ber of hash buckets ( B) from 16k to 8k, meaning
more (partial) collisions in embedding lookups.
This signiÔ¨Åcantly hinders the M INSPAN task.
Character vocab We switch from our hash-
based no-vocabulary strategy to using a normal
character vocabulary (which we derive from the
pre-training corpus). We observe that this under-
performs the hashing approach. We speculate that
this might be due to skew between the pre-training
corpus and the Ô¨Ånal downstream task since not all
codepoints can be included in the vocabulary.
Input character dimension We reduced the
embedding size of the initial character encoder
(i.e. the embedding size of hinitande‚Äînot hup
noryseq) and observe that quality falls off rapidly.

--- PAGE 10 ---
Model S ELECT P M INSPAN
CANINE -C 65.7 53.0
No concatenation 17.2 35.6
+Final-to-initial resid. 17.3 35.9
+Final-to-downsampled resid. 62.0 50.2
Table 4: Ablations for residuals and feature concate-
nation on T YDIQA. Rows are cumulative (each row
contains all changes from the previous).
No initial transformer We remove the local
transformer from hinitand similarly observed a
marked reduction in quality.
Increased downsampling While more aggres-
sive downsampling (a factor of 5X or 6X, rather
than 4X) brings substantial speed gains, the
passage-level quality degrades substantially and
the minimal span predictions suffer even more.
No position-limited MLM When we do not use
the trick of applying the Ô¨Ånal character trans-
former ( yseq) only to the positions that will be
computed by the MLM task, we observe a large
reduction in speed. Since this model is theoreti-
cally equivalent in terms of operations, we show
only the speed for exposition.
We also performed ablations aimed at explor-
ing the effect of feature concatenation and residu-
als; results are in Table 4. Not concatenating the
downsampled representation with the initial char-
acter representation when computing hupcauses
the model to become unstable (row 2); adding a
residual from hupback to hinitdoes not help (row
3). However, additionally inserting a residual from
hupback to h0
down does stabilize the model (row 4)
though it does not recover the original quality.
4.4 NER Results
Named entity recognition is a task in which mem-
orization is often a very effective strategy. For ex-
ample, if a model has London in its vocabulary
and sees it with the label LOCATION during train-
ing, then it simply has to retrieve this memorized
association when it sees the token London at test
time. Therefore, evaluating on NER is helpful for
understanding the ways in which different models
emphasize memorization vs. generalization.
As shown in Table 5, C ANINE -C performs
signiÔ¨Åcantly worse than mB ERT on NER, likely
due to mB ERT‚Äôs memorization-friendly vocabu-
lary. However, when (tokenization-free) n-gram
features are added to C ANINE -C, performance re-
bounds, showing that it is possible to cheaplyModel CoNLL MasakhaNER
mB ERT(ours) 87.8 72.4
CANINE -C 74.0 (-13.8) 65.5 (-6.9)
CANINE -C + n-grams 86.7 (-1.1) 76.8 (+4.3)
Table 5: F1 scores on NER tasks.
boost a model‚Äôs memorization ability while re-
maining fully tokenization-free.
A full language-wise breakdown is provided in
the appendix (Table 8). It‚Äôs worth noting that part
of the performance difference on MasakhaNER is
due to mB ERT producing no usable outputs for
Amharic. The mB ERT pre-training data does not
contain Amharic (or any Amharic-script text), so
it has no vocabulary entries to Amharic‚Äôs script
(meaning that mB ERT sees only sequences of
[UNK] on Amharic inputs). However, since
CANINE always supports the full Unicode space,
it is able to achieve 50 F1 even though it, too, had
never seen Amharic text during pre-training. We
take this as validation of C ANINE ‚Äôs vocabulary-
free approach. It may also be evidence that
CANINE exhibits cross-script transfer abilities
analogous to those in mB ERT(Pires et al., 2019).
Error analysis CANINE -C tends not to label
rarer lexical items that mB ERT appears to have
memorized. For example, with C ANINE -C,
JCPenney (a relatively rare lexical item) is not rec-
ognized as an entity. C ANINE -C also tends to sep-
arate long entities; for example, ‚Äú State Street Bank
and Trust Company ‚Äù is labeled as two separate
spans: ‚Äú State Street Bank ‚Äù and ‚Äú Trust Company ‚Äù;
and the location TAMPA BAY is recognized only
asTAMPA . However, adding n-grams features ap-
pears to mostly resolve this issue.
5 Related Work
5.1 Improvements to subword tokenization
Further improvements to standard subword tok-
enization like Byte Pair Encoding (BPE) (Sennrich
et al., 2016), WordPiece (Wu et al., 2016), and
SentencePiece (Kudo and Richardson, 2018) have
been proposed. Subword regularization (Kudo,
2018) and BPE-dropout (Provilkov et al., 2020)
recognize that deterministic segmentation during
training limits the ability to leverage morphol-
ogy and word composition; instead, they sam-
ple at random one of the multiple tokenizations
of the training input, made possible by the in-
herent ambiguity of subword vocabularies. Wang

--- PAGE 11 ---
Examples T YDIQA T YDIQA
Condition / sec S ELECT P M INSPAN
Attend to h0
down(instead of hup) 6400 64.5 52.2
8k codepoint hash buckets (instead of 16k) 6400 64.1 (-0.4) 50.5 (-1.7)
Character vocab (no hashing) 6400 64.6 (+/-) 51.2 (-1.0)
Input character dim 384 (instead of 768) 6600 62.9 (-1.2) 49.3 (-1.2)
Input character dim 192 (instead of 768) 6400 61.7 (-2.4) 47.3 (-3.2)
No initial character transformer 6700 63.2 (-1.4) 48.3 (-2.9)
Downsample by a factor of 5 (instead of 4) 7000 62.9 (-1.7) 49.2 (-2.0)
Downsample by a factor of 6 (instead of 4) 9200 62.7 (-1.9) 47.6 (-3.6)
Don‚Äôt limit Ô¨Ånal character transformer to MLM positions 5200 ‚Äî ‚Äî
CANINE -S 6400 66.0 52.5
Table 6: Ablation experiments on the C ANINE model with T YDIQA F1 scores. Deltas are shown in parentheses
with regard to the top-most experiment, which serves as the baseline conÔ¨Åguration for all experiments in this table.
Each result is averaged over 3 Ô¨Åne-tuning and evaluation replicas.
et al. (2021) recently expanded on this paradigm
to enforce consistency of predictions over differ-
ent segmentations. Unigram LM (Kudo, 2018),
which builds its vocabulary top-down, was shown
to align with morphology better than BPE on pre-
trained encoders (Bostrom and Durrett, 2020).
Others have built hybrid models that use mul-
tiple granularities, combining characters with to-
kens (Luong and Manning, 2016) or different sub-
word vocabularies (Zhang and Li, 2021).
5.2 Character-level models
Following the larger NLP trend, character-level
n-gram models (Huang et al., 2013; Wieting
et al., 2016; Bojanowski et al., 2017) have mostly
been replaced by neural networks. While gener-
ally lagging behind their word-level counterparts,
character-level features are important for mor-
phologically rich languages, particularly in low-
resource settings (Garrette and Baldridge, 2013).
For language modeling Character language
models (CLMs) have used vanilla RNN architec-
tures to produce distributions over sequences of
characters in a purely tokenization-free manner
(Sutskever et al., 2011; Graves, 2013; Hwang and
Sung, 2017; Radford et al.). Hierarchical RNNs
modeled the assumption that language operates
on increasing layers of abstraction: Chung et al.
(2017) jointly trained a sub-module to segment the
character-level input into larger spans at each layer
of a stacked LSTM.
Due to the consistent lag in performance behind
their word-level counterparts, attention shifted
from pure CLMs towards merely character-aware
models, still reliant on traditional tokenization.Some hybrid models processed the input at char-
acter level, but predicted words from a closed vo-
cabulary (Kim et al., 2016; Gerz et al., 2018).
Others reintroduced explicit tokenization on the
input side, and either generated bursts of char-
acter sequences that formed an open vocabulary
(Kawakami et al., 2017) or used a character-only
generator as a fallback when the main closed-
vocabulary word generator produced a rare or un-
known token (Matthews et al., 2019; Mielke and
Eisner, 2019). Especially after the popularization
of the inherently ambiguous subword vocabularies
like BPE, several studies moved beyond a single
input segmentation and marginalized over all pos-
sible segmentations (van Merri√´nboer et al., 2017;
Buckman and Neubig, 2018; Grave et al., 2019).
Coming full circle, Kawakami et al. (2019) in-
duced a lexicon without any explicit supervision,
reverting back to pure CLMs. In a revitalized ef-
fort to bring them on-par with coarser granulari-
ties, researchers leveraged external resources such
as grounding in vision (Kawakami et al., 2019) or
multi-task learning together with supervised mor-
phology tasks (Blevins and Zettlemoyer, 2019).
After the transformer (Vaswani et al., 2017) re-
placed RNNs as the dominant architecture in NLP,
character-level models followed. Al-Rfou et al.
(2019) showed that byte-level vanilla Transform-
ers signiÔ¨Åcantly underperform their word-level
counterparts. A similar Ô¨Ånding was reported by
Radford et al. (2019). Although the gap has been
reduced (Choe et al., 2019), subword transformers
remain the status quo for pure language modeling.
For speciÔ¨Åc tasks In parallel with LM efforts,
the neural machine translation (NMT) commu-

--- PAGE 12 ---
nity sought to solve its open-vocabulary problem
via character-level modeling. Luong and Man-
ning (2016) proposed a hybrid model that operated
mainly at the word level, but consulted a character-
level LSTM for unknown words; this was a prac-
tical compromise, as their character-only model
took 3 months to train. Lee et al. (2017) en-
abled pure character NMT by shortening the in-
put length via convolutional, pooling, and high-
way layers. Notably, their many-to-English model
outperformed its subword counterpart and most
bilingual baselines, with a 35% increase in train-
ing time (on a single GPU) compared to a baseline
BPE-to-char model. C ANINE has a similar moti-
vation, but operates in the context of pre-trained
transformers; training is 7x faster compared to a
char-to-char baseline (on TPU v3), and has a 28%
increase in training time over mB ERT(Table 2).
Character information has been leveraged for
many other end tasks as well, including: text clas-
siÔ¨Åcation (Zhang et al., 2015; Zhang and LeCun,
2017), part-of-speech tagging and NER (Gillick
et al., 2016; Akbik et al., 2018; Pinter et al., 2019),
named entity detection (Yu et al., 2018), depen-
dency parsing (Vania et al., 2018), and machine
reading comprehension (Hewlett et al., 2018).
Character information proved particularly useful
for low-resource languages (Xie et al., 2018), phe-
nomena such as code-switching and translitera-
tion (Ball and Garrette, 2018), and rich morphol-
ogy (Vania and Lopez, 2017), previously receiv-
ing special modeling including adaptor grammars
(Botha and Blunsom, 2013).
For transfer learning Token-based models
have also been augmented with character-level
information in the context of transfer learn-
ing, where encoders trained with unsupervised
objectives are repurposed to solve downstream
tasks. Pinter et al. (2017) addressed the out-of-
vocabulary problem of static pre-trained word em-
beddings by training a model to map the surface of
a word to its pre-trained representation, and used it
on unknown words. ELMo (Peters et al., 2018), a
bidirectional LSTM model, applied character con-
volutions to its whitespace-separated input tokens.
CharacterB ERT (Boukkouri et al., 2020) ported
this technique to B ERT, augmenting its existing
WordPiece-tokenized input. Consistent with pre-
vious observations that feeding characters into a
transformer stack comes with a huge computa-
tional cost while not improving over tokenization-based approaches (Al-Rfou et al., 2019), a B ERT
model Ô¨Åne-tuned for semantic parsing achieved
gains only when characters complemented sub-
words (van Noord et al., 2020).
5.3 Multilingual models
Multilingual NLP has been dominated by deep
pre-trained multilingual models whose subword
vocabularies are shared across languages. Such
models borrow their architectures from monolin-
gual predecessors and apply joint training in 100+
languages, either with unsupervised LM losses:
mB ERT, mT5 (Xue et al., 2021), or with additional
translation losses: XLM (Lample and Conneau,
2019), XLM-R (Conneau et al., 2020). Chung
et al. (2020) extended this by forming language
clusters with per-cluster vocabularies. To ac-
commodate languages unseen during pre-training,
Wang et al. (2020) extended the vocabulary and
continued pre-training.
6 Conclusion
In this article, we described C ANINE , which
is, to our knowledge, the Ô¨Årst pre-trained deep
encoder for language understanding that uses a
tokenization-free, vocabulary-free model, while
surpassing the quality of models built on top of
heuristic tokenizers. C ANINE eliminates many en-
gineering pitfalls for practitioners and opens up
new research directions for the community.
Acknowledgements
The authors wish to thank Noah Constant, Rami
Al-Rfou, Kristina Toutanova, Kenton Lee, Ming-
Wei Chang, and Tim Dozat for their feedback on
this work. We would also like to thank Martin
Njoroge and Nanjala Misiko for their consulta-
tions on the Kiswahili examples, Diana Akrong
for consulting on Twi orthography, and Waleed
Ammar for consulting on Arabic morphology.

--- PAGE 13 ---
References
David Ifeoluwa Adelani, Jade Abbott, Gra-
ham Neubig, Daniel D‚Äôsouza, Julia Kreutzer,
Constantine Lignos, Chester Palen-Michel,
Happy Buzaaba, Shruti Rijhwani, Sebastian
Ruder, Stephen Mayhew, Israel Abebe Az-
ime, Shamsuddeen H. Muhammad, Chris Chi-
nenye Emezue, Joyce Nakatumba-Nabende,
Perez Ogayo, Aremu Anuoluwapo, Cather-
ine Gitau, Derguene Mbaye, Jesujoba Alabi,
Seid Muhie Yimam, Tajuddeen Rabiu Gwad-
abe, Ignatius Ezeani, Rubungo Andre Niy-
ongabo, Jonathan Mukiibi, Verrah Otiende,
Iroro Orife, Davis David, Samba Ngom,
Tosin Adewumi, Paul Rayson, Mofetoluwa
Adeyemi, Gerald Muriuki, Emmanuel Anebi,
Chiamaka Chukwuneke, Nkiruka Odu, Eric Pe-
ter Wairagala, Samuel Oyerinde, Clemencia
Siro, Tobius Saul Bateesa, Temilola Oloyede,
Yvonne Wambui, Victor Akinode, Debo-
rah Nabagereka, Maurice Katusiime, Ayodele
Awokoya, Mouhamadane MBOUP, Dibora Ge-
breyohannes, Henok Tilaye, Kelechi Nwaike,
Degaga Wolde, Abdoulaye Faye, Blessing
Sibanda, Orevaoghene Ahia, Bonaventure F. P.
Dossou, Kelechi Ogueji, Thierno Ibrahima
DIOP, Abdoulaye Diallo, Adewale Akinfaderin,
Tendai Marengereke, and Salomey Osei. 2021.
MasakhaNER: Named Entity Recognition for
African Languages. TACL .
Joshua Ainslie, Santiago Ontanon, Chris Alberti,
Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang,
and Li Yang. 2020. ETC: Encoding Long and
Structured Inputs in Transformers. In Proc. of
EMNLP .
Alan Akbik, Duncan Blythe, and Roland V ollgraf.
2018. Contextual String Embeddings for Se-
quence Labeling. In Proc. of COLING .
Rami Al-Rfou, Dokook Choe, Noah Constant,
Mandy Guo, and Llion Jones. 2019. Character-
level language modeling with deeper self-
attention. In Proc. of AAAI .
Kelsey Ball and Dan Garrette. 2018. Part-of-
Speech Tagging for Code-Switched, Transliter-
ated Texts without Explicit Language IdentiÔ¨Å-
cation. In Proc. of EMNLP .Terra Blevins and Luke Zettlemoyer. 2019. Better
character language modeling through morphol-
ogy. In Proc. of ACL .
Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2017. Enriching word vec-
tors with subword information. TACL .
Kaj Bostrom and Greg Durrett. 2020. Byte pair
encoding is suboptimal for language model pre-
training. In Findings of the Association for
Computational Linguistics: EMNLP .
Jan A. Botha and Phil Blunsom. 2013. Adaptor
Grammars for learning non-concatenative mor-
phology. In Proc. of EMNLP .
Hicham El Boukkouri, Olivier Ferret, Thomas
Lavergne, Hiroshi Noji, Pierre Zweigenbaum,
and Junichi Tsujii. 2020. CharacterBERT:
Reconciling ELMo and BERT for Word-Level
Open-V ocabulary Representations From Char-
acters. In Proc. of COLING .
Tom Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter,
Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCan-
dlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot
learners. In Proc. of NeurIPS .
Jacob Buckman and Graham Neubig. 2018. Neu-
ral lattice language models. TACL .
Dokook Choe, Rami Al-Rfou, Mandy Guo, Heey-
oung Lee, and Noah Constant. 2019. Bridging
the Gap for Tokenizer-Free Language Models.
arXiv preprint arXiv:1908.10322 .
Krzysztof Choromanski, Valerii Likhosherstov,
David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Davis,
Afroz Mohiuddin, Lukasz Kaiser, David Be-
langer, Lucy Colwell, and Adrian Weller. 2021.
Rethinking Attention with Performers. In Proc.
of ICLR .

--- PAGE 14 ---
Hyung Won Chung, Dan Garrette, Kiat Chuan
Tan, and Jason Riesa. 2020. Improving Mul-
tilingual Models with Language-Clustered V o-
cabularies. In Proc. of EMNLP .
Junyoung Chung, Sungjin Ahn, and Yoshua Ben-
gio. 2017. Hierarchical Multiscale Recurrent
Neural Networks. In Proc. of ICLR .
Jonathan H. Clark, Eunsol Choi, Michael Collins,
Dan Garrette, Tom Kwiatkowski, Vitaly Niko-
laev, and Jennimaria Palomaki. 2020. TyDi QA:
A benchmark for Information-Seeking Ques-
tion Answering in Typologically Diverse Lan-
guages. TACL .
Alexis Conneau, Kartikay Khandelwal, Naman
Goyal, Vishrav Chaudhary, Guillaume Wen-
zek, Francisco Guzm√°n, Edouard Grave, Myle
Ott, Luke Zettlemoyer, and Veselin Stoyanov.
2020. Unsupervised Cross-lingual Representa-
tion Learning at Scale. In Proc. of ACL .
Zihang Dai, Guokun Lai, Yiming Yang, and
Quoc V . Le. 2020. Funnel-Transformer: Fil-
tering out Sequential Redundancy for EfÔ¨Åcient
Language Processing. In Proc. of NeurIPS .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of Deep Bidirectional Transformers for Lan-
guage Understanding. In Proc. of NAACL .
Dan Garrette and Jason Baldridge. 2013. Learn-
ing a part-of-speech tagger from two hours of
annotation. In Proc. of NAACL .
Daniela Gerz, Ivan Vuli ¬¥c, Edoardo Ponti, Jason
Naradowsky, Roi Reichart, and Anna Korho-
nen. 2018. Language Modeling for Morpholog-
ically Rich Languages: Character-Aware Mod-
eling for Word-Level Prediction. TACL .
Dan Gillick, Cliff Brunk, Oriol Vinyals, and
Amarnag Subramanya. 2016. Multilingual
Language Processing From Bytes. In Proc. of
NAACL .
Edouard Grave, Sainbayar Sukhbaatar, Piotr Bo-
janowski, and Armand Joulin. 2019. Training
hybrid language models by marginalizing over
segmentations. In Proc. of ACL .
Alex Graves. 2013. Generating sequences with
recurrent neural networks. arXiv preprint
arXiv:1308.0850 .Daniel Hewlett, Alexandre Lacoste, Llion Jones,
Illia Polosukhin, Andrew Fandrianto, Jay Han,
Matthew Kelcey, and David Berthelot. 2018.
Byte-Level Machine Reading across Morpho-
logically Varied Languages. In Proc. of AAAI .
Po-Sen Huang, Xiaodong He, Jianfeng Gao,
Li Deng, Alex Acero, and Larry Heck. 2013.
Learning deep structured semantic models for
web search using clickthrough data. In Proc. of
the ACM International Conference on Informa-
tion and Knowledge Management (CIKM) .
Kyuyeon Hwang and Wonyong Sung. 2017.
Character-Level Language Modeling with Hier-
archical Recurrent Neural Networks. In Proc.
of ICASSP .
Prabhu Kaliamoorthi, Sujith Ravi, and Zornitsa
Kozareva. 2019. PRADO: Projection attention
networks for document classiÔ¨Åcation on-device.
InProc. of EMNLP .
Kazuya Kawakami, Chris Dyer, and Phil Blun-
som. 2017. Learning to create and reuse words
in open-vocabulary neural language modeling.
InProc. of ACL .
Kazuya Kawakami, Chris Dyer, and Phil Blun-
som. 2019. Learning to Discover, Ground and
Use Words with Segmental Neural Language
Models. In Proc. of ACL .
Yoon Kim, Yacine Jernite, David Sontag, and
Alexander M Rush. 2016. Character-Aware
Neural Language Models. In Proc. of AAAI .
Taku Kudo. 2018. Subword Regularization: Im-
proving Neural Network Translation Models
with Multiple Subword Candidates. In Proc. of
ACL.
Taku Kudo and John Richardson. 2018. Senten-
cePiece: A simple and language independent
subword tokenizer and detokenizer for Neural
Text Processing. In Proc. of EMNLP: System
Demonstrations .
Guillaume Lample and Alexis Conneau. 2019.
Cross-lingual Language Model Pretraining. In
Proc. of NeurIPS .
Jason Lee, Eth Z√ºrich, Kyunghyun Cho, and
Thomas Hofmann. 2017. Fully Character-Level
Neural Machine Translation without Explicit
Segmentation. TACL .

--- PAGE 15 ---
Minh-Thang Luong and Christopher D. Manning.
2016. Achieving open vocabulary neural ma-
chine translation with hybrid word-character
models. In Proc. of ACL .
Austin Matthews, Graham Neubig, and Chris
Dyer. 2019. Using Morphological Knowledge
in Open-V ocabulary Neural Language Models.
InProc. of NAACL .
Sebastian J. Mielke and Jason Eisner. 2019. Spell
once, summon anywhere: A two-level open-
vocabulary language model. In Proc. of AAAI .
Rik van Noord, Antonio Toral, and Johan Bos.
2020. Character-level Representations Improve
DRS-based Semantic Parsing Even in the Age
of BERT. In Proc. of EMNLP .
Brendan O‚ÄôConnor, Michel Krieger, and David
Ahn. 2010. TweetMotif: Exploratory Search
and Topic Summarization for Twitter Introduc-
tion and Description. In Proc. of the Interna-
tional AAAI Conference on Web and Social Me-
dia.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz, Noah Smith, and Lingpeng Kong.
2021. Random feature attention. In Proc. of
ICLR .
Matthew E. Peters, Mark Neumann, Mohit Iyyer,
Matt Gardner, Christopher Clark, Kenton Lee,
and Luke Zettlemoyer. 2018. Deep contextual-
ized word representations. In Proc. of NAACL .
Jason Phang, Iacer Calixto, Phu Mon Htut,
Yada Pruksachatkun, Haokun Liu, Clara Va-
nia, Katharina Kann, and Samuel R Bowman.
2020. English Intermediate-Task Training Im-
proves Zero-Shot Cross-Lingual Transfer Too.
InProc. of AACL .
Yuval Pinter, Robert Guthrie, and Jacob Eisen-
stein. 2017. Mimicking word embeddings us-
ing subword RNNs. In Proc. of EMNLP .
Yuval Pinter, Marc Marone, and Jacob Eisen-
stein. 2019. Character Eyes: Seeing Language
through Character-Level Taggers. In Proc. of
BlackboxNLP .
Telmo Pires, Eva Schlinger, and Dan Garrette.
2019. How multilingual is Multilingual BERT?
InProc. of ACL .Ivan Provilkov, Dmitrii Emelianenko, and Elena
V oita. 2020. BPE-Dropout: Simple and Effec-
tive Subword Regularization. In Proc. of ACL .
Danish Pruthi, Bhuwan Dhingra, and Zachary C.
Lipton. 2019. Combating adversarial mis-
spellings with robust word recognition. In Proc.
of ACL .
Alec Radford, Rafal Jozefowicz, and Ilya
Sutskever. Learning to Generate Reviews
and Discovering Sentiment. arXiv preprint
arXiv:1704.01444 .
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Lan-
guage models are unsupervised multitask learn-
ers.
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.
2020. Exploring the limits of transfer learning
with a uniÔ¨Åed text-to-text transformer. JMLR .
Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016. Neural Machine Translation of
Rare Words with Subword Units. In Proc. of
ACL.
Lichao Sun, Kazuma Hashimoto, Wenpeng Yin,
Akari Asai, Jia Li, Philip Yu, and Caiming
Xiong. 2020. Adv-BERT: BERT is not ro-
bust on misspellings! Generating nature ad-
versarial samples on BERT. arXiv preprint
arXiv:2003.04985 .
Ilya Sutskever, James Martens, and Geoffrey E
Hinton. 2011. Generating text with recurrent
neural networks. In Proc. of ICML .
Dan Svenstrup, Jonas Meinertz Hansen, and Ole
Winther. 2017. Hash Embeddings for EfÔ¨Åcient
Word Representations. In Proc. of NeurIPS .
David Talbot and John Talbot. 2008. Bloom maps.
InProc. of the Workshop on Analytic Algorith-
mics and Combinatorics (ANALCO) .
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
BERT rediscovers the classical NLP pipeline.
InProc. of ACL .
Erik F. Tjong Kim Sang. 2002. Introduction
to the CoNLL-2002 shared task: Language-
independent named entity recognition. In Proc.
of CoNLL .

--- PAGE 16 ---
Erik F. Tjong Kim Sang and Fien De Meul-
der. 2003. Introduction to the CoNLL-2003
shared task: Language-independent named en-
tity recognition. In Proc. of NAACL .
Bart van Merri√´nboer, Amartya Sanyal,
H. Larochelle, and Yoshua Bengio. 2017.
Multiscale sequence modeling with a learned
dictionary. arXiv preprint arXiv:1707.00762 .
Clara Vania, Andreas Grivas, and Adam Lopez.
2018. What do character-level models learn
about morphology? The case of dependency
parsing. In Proc. of EMNLP .
Clara Vania and Adam Lopez. 2017. From Char-
acters to Words to in Between: Do We Capture
Morphology? In Proc. of ACL .
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. In Proc. of NeurIPS .
Xinyi Wang, Sebastian Ruder, and Graham Neu-
big. 2021. Multi-view subword regularization.
InProc. of NAACL .
Zihan Wang, Karthikeyan K, Stephen Mayhew,
and Dan Roth. 2020. Extending multilingual
BERT to low-resource languages. In Findings
of EMNLP .
John Wieting, Mohit Bansal, Kevin Gimpel, and
Karen Livescu. 2016. Charagram: Embedding
words and sentences via character n-grams. In
Proc. of EMNLP .
Yonghui Wu, Mike Schuster, Zhifeng Chen,
Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao,
Klaus Macherey, Jeff Klingner, Apurva Shah,
Melvin Johnson, Xiaobing Liu, ≈Åukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo,
Hideto Kazawa, Keith Stevens, George Kurian,
Nishant Patil, Wei Wang, Cliff Young, Ja-
son Smith, Jason Riesa, Alex Rudnick, Oriol
Vinyals, Greg Corrado, Macduff Hughes, and
Jeffrey Dean. 2016. Google‚Äôs Neural Ma-
chine Translation System: Bridging the Gap be-
tween Human and Machine Translation. arXiv
preprint arXiv:1609.08144 .
Jiateng Xie, Zhilin Yang, Graham Neubig,
Noah A. Smith, and Jaime Carbonell. 2018.Neural cross-lingual named entity recognition
with minimal resources. In Proc. of EMNLP .
Linting Xue, Noah Constant, Adam Roberts,
Mihir Kale, Rami Al-Rfou, Aditya Siddhant,
Aditya Barua, and Colin Raffel. 2021. mT5: A
massively multilingual pre-trained text-to-text
transformer. In Proc. of NAACL .
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu,
Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-
Jui Hsieh. 2020. Large Batch Optimization for
Deep Learning: Training BERT in 76 minutes.
InProc. of ICLR .
Xiaodong Yu, Stephen Mayhew, Mark Sammons,
and Dan Roth. 2018. On the strength of char-
acter language models for multilingual named
entity recognition. In Proc. of EMNLP .
Manzil Zaheer, Guru Guruganesh, Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago
Ontanon, Philip Pham, Anirudh Ravula, Qifan
Wang, Li Yang, and Amr Ahmed. 2020. Big
Bird: Transformers for Longer Sequences. In
Proc. of NeurIPS .
Xiang Zhang and Yann LeCun. 2017. Which En-
coding is the Best for Text ClassiÔ¨Åcation in Chi-
nese, English, Japanese and Korean? arXiv
preprint arXiv:1708.02657v2 .
Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015. Character-level convolutional networks
for text classiÔ¨Åcation. In Proc. of NeurIPS .
Xinsong Zhang and Hang Li. 2021. AMBERT:
A Pre-trained Language Model with Multi-
Grained Tokenization. In Findings of ACL .

--- PAGE 17 ---
A Appendix
Language mB ERT CANINE -S C ANINE -C C ANINE -C
+ n-grams
SELECT P
(English) 62.2 58.6 (-3.6) 61.6 (-0.6) 64.6 (+2.4)
Arabic 82.3 82.8 (+0.5) 82.5 (+0.2) 84.3 (+2.0)
Bengali 58.5 61.8 (+3.3) 62.5 (+4.0) 66.0 (+7.5)
Finnish 60.4 62.2 (+1.8) 63.6 (+3.2) 66.7 (+6.3)
Indonesian 61.3 63.5 (+2.2) 64.2 (+2.9) 65.9 (+4.6)
Japanese 46.2 51.7 (+5.5) 49.7 (+3.5) 51.2 (+5.0)
Korean 60.2 60.3 (+0.1) 59.7 (-0.5) 60.6 (+0.4)
Russian 62.2 64.6 (+2.4) 65.6 (+3.4) 68.5 (+6.3)
Swahili 58.8 67.8 (+9.0) 67.0 (+8.2) 67.2 (+8.4)
Telugu 81.0 82.5 (+1.5) 81.1 (+0.1) 84.6 (+3.6)
Thai 61.1 62.8 (+1.7) 61.2 (+0.1) 65.8 (+4.7)
Macro Avg 63.2 66.0 (+2.8) 65.7 (+2.5) 68.1 (+4.9)
MINSPAN
(English) 46.0 46.3 (+0.3) 49.0 (+3.0) 51.8 (+5.8)
Arabic 70.7 66.9 (-3.8) 65.6 (-5.1) 73.0 (+2.3)
Bengali 47.3 46.7 (-0.6) 52.5 (+5.2) 57.1 (+9.8)
Finnish 51.1 53.0 (+1.9) 53.8 (+2.7) 57.1 (+6.0)
Indonesian 52.2 53.6 (+1.4) 54.4 (+2.2) 56.8 (+4.6)
Japanese 36.1 40.3 (+4.2) 40.7 (+4.6) 42.0 (+5.9)
Korean 36.8 35.7 (-1.1) 36.5 (-0.3) 39.9 (+3.1)
Russian 45.6 46.7 (+1.1) 47.2 (+1.6) 51.5 (+5.9)
Swahili 49.4 59.0 (+9.6) 57.6 (+8.2) 59.2 (+9.8)
Telugu 75.6 75.2 (-0.4) 74.2 (-1.4) 79.7 (+4.1)
Thai 48.4 47.9 (-0.5) 47.1 (-1.3) 54.2 (+5.8)
Macro Avg 51.3 52.5 (+1.2) 53.0 (+1.7) 57.0 (+5.7)
Table 7: Language-wise breakdown for T YDIQA primary tasks. English is parenthesized because it is not
included in the overall score calculation for T YDIQA.

--- PAGE 18 ---
Language mB ERT CANINE -C C ANINE -C
+ n-grams
CONLL
Dutch 90.2 74.7 (-15.5) 88.5 (-1.7)
English 91.1 79.8 (-11.3) 89.8 (-1.3)
German 82.5 64.1 (-18.4) 82.1 (-0.4)
Spanish 87.6 77.4 (-10.2) 86.5 (-1.1)
Macro Avg 87.8 74.0 (-13.8) 86.7 (-1.1)
MASAKHA NER
Amharic 0.0 44.6 (+44.6) 50.0 (+50.0)
Hausa 89.3 76.1 (-13.2) 88.0 (-1.3)
Igbo 84.6 75.6 (-9.0) 85.0 (+0.4)
Kinyarwanda 73.9 58.3 (-15.6) 72.8 (-1.1)
Luganda 80.2 69.4 (-10.8) 79.6 (-0.6)
Luo 75.8 63.4 (-12.4) 74.2 (-1.6)
Nigerian Pidgin 89.8 66.6 (-23.2) 88.7 (-1.1)
Swahili 87.1 72.7 (-14.4) 83.7 (-3.4)
Wolof 64.9 60.7 (-4.2) 66.5 (+1.6)
Yor√πb√° 78.7 67.9 (-10.8) 79.1 (+0.4)
Macro Avg 72.4 65.5 (-6.9) 76.8 (+4.3)
Table 8: Language-wise breakdown for Named Entity Recognition for the CoNLL and MasakhaNER datasets
(labeled F1). mB ERT obtains a score of zero on Amharic due to having no vocabulary entries in the Amharic
script.
