# 2306.07764.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/tokenizer/2306.07764.pdf
# File size: 748267 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Tokenization with Factorized Subword Encoding
David Samuel and Lilja Øvrelid
University of Oslo, Language Technology Group
Abstract
In recent years, language models have become
increasingly larger and more complex. How-
ever, the input representations for these mod-
els continue to rely on simple and greedy sub-
word tokenization methods. In this paper, we
propose a novel tokenization method that fac-
torizes subwords onto discrete triplets using a
VQ-V AE model. The effectiveness of the pro-
posed tokenization method, referred to as the
FACTORIZER , is evaluated on language mod-
eling and morpho-syntactic tasks for 7 diverse
languages. Results indicate that this method
is more appropriate and robust for morpholog-
ical tasks than the commonly used byte-pair
encoding (BPE) tokenization algorithm.
1 Introduction
A typical subword tokenizer consists of a vocabu-
lary of 10 000s of subwords, each of them mapped
onto a single and independent index. Instead, we
propose a method that learns to project subwords
onto triplets of 256indices:
melon →[30,255,209].
This mapping is learned by a vector-quantized vari-
ational auto-encoder (VQ-V AE; van den Oord et al.,
2017) from a large word-frequency list, resulting
in a projection where orthographically different
words use different indices and similar words share
similar indices:
melons →[261,255,209],
water →[96,235,109].
Modeling this projection with a VQ-V AE also au-
tomatically gives the estimated probability of every
subword given an index triplet. Maximizing the
joint probability allows for an optimal subword tok-
enization of words not contained in the vocabulary.
water |melon →[208,235,109],[45,255,209].In this paper, we present FACTORIZER , a subword
encoding method that serves as a drop-in replace-
ment for any subword tokenizer used in modern
NLP pipelines. We release the source code, trained
models and ready-to-use tokenizers online.1Our
approach demonstrates the following advantages:
1.Improved performance. We evaluate the
performance of our FACTORIZER on masked
language models of seven linguistically di-
verse languages: Arabic, Chinese, Czech, En-
glish, Norwegian, Scottish Gaelic and Turkish.
These models are evaluated on part-of-speech
tagging, dependency parsing, and lemmatiza-
tion tasks and demonstrate a substantial im-
provement in performance.
2.Increased robustness. Additionally, we in-
vestigate the robustness of our FACTORIZER to
random noise during inference as well as the
robustness to data scarcity during pretraining.
We measure performance with increasing lev-
els of noise and data scarcity and demonstrate
that our FACTORIZER improves robustness to
these factors.
3.More effective use of parameters. Tradi-
tional subword tokenizers require large vo-
cabularies to cover most word forms, which
results in a substantial portion of learnable
parameters being consumed by the subword
embedding layer. For example, the embed-
ding layer of BERT baseuses more than 20%
of its parameters, totaling over 23 million. In
contrast, the memory footprint of our FAC-
TORIZER embedding is substantially lower as
it only requires about 0.6 million parameters.2
The remaining parameters can be then allo-
cated more effectively in self-attention layers.
1https://github.com/ltgoslo/factorizer
2It uses embedding matrices for 3×256indices, where
each embedding vector has length 768, as in BERT base.arXiv:2306.07764v1  [cs.CL]  13 Jun 2023

--- PAGE 2 ---
2 Background: VQ-V AE
In this paper, we propose a novel tokenizer that
utilizes a vector-quantized variational auto-encoder
(VQ-V AE; van den Oord et al., 2017) as its central
component. VQ-V AE is a powerful technique for
learning discrete latent variables that can encode
words and reconstruct them back to their original
form. This process can be broken down into three
main steps, as illustrated in Figure 1:
1.The encoder maps the data samples (sub-
words) xto continuous latent vectors e(x).
2.The codebook is made up of Kcodebook
vectors zk, k∈1. . . K . Each latent vector
e(x)isquantized to its nearest codebook vec-
torzk, effectively mapping each input sample
xonto a discrete variable k. The encoder
and codebook downsample and compress the
information in x, serving as an information
bottleneck (Tishby et al., 1999).
3.The decoder reconstructs the symbols zback
into the original input distribution, modeling
the distribution p(x|z)asd(z).
Gradient backpropagation. The model is opti-
mized jointly with the backpropagation algorithm.
However, special attention must be given to the
codebook quantization qas this operation is not
differentiable:
q(e(x)) =zk,where
k= argmin
i∥e(x)−zi∥2(1)
During the forward phase, the quantized codebook
vectors are simply passed to the decoder. However,
during the backward phase, the gradient of the loss
function ∇dLis passed directly from the decoder
to the encoder. This technique, known as straight-
through gradient estimation , is possible because
the output of the encoder and the input of the de-
coder share the same latent space. The output e(x)
is sufficiently similar to the decoder input z, such
that the gradient carries information about how to
change e(x)to lower the reconstruction loss.
Loss terms. In addition to the standard recon-
struction loss Lr=logp(x|z), which mea-
sures the auto-encoding performance, the VQ-V AE
model incorporates two auxiliary loss terms that
align the encoder outputs e(x)with the nearest
codebook vectors. Specifically, the codebook loss
vector 
quantizationgradien t
encoder 
networkdecoder 
network[ 0.12, -3.14,  0.32, ...]
[-1.56,  0.23, -0.67, ...]
[ 0.02, -3.14,  1.43, ...]
[ 0.53,  0.04, -1.02, ...]
[-0.23, -0.45, -0.14, ...]
gradien t 
  shortcut  codebookFigure 1: The main components of a VQ-V AE model.
The latent vector e(x)is quantized into the 2ndcode-
book vector z2and then the decoder tries to reconstruct
the original input as d(z2). We use the codebook size
K= 5in this illustrative example, i.e. all vectors are
clustered into five possible values.
is defined as Lq=∥sg(e(x))−zk∥2and the com-
mitment loss asLe=∥e(x)−sg(zk)∥2, where ‘sg’
is the stop gradient operation and zkis the code-
book vector defined in Equation (1). The overall
loss of a VQ-V AE model is computed as:
L=Lr+Lq+βLe,
where βis a hyperparameter that balances the focus
between reconstruction and codebook alignment.
Codebook EMA. An alternative approach to up-
dating the codebook, proposed by van den Oord
et al. (2017), is to update it as an exponential mov-
ing average (EMA) of the latent variables e(x)–
instead of incorporating the codebook loss Lq. This
update consists of updating two variables: the code-
book usage counts ckand the codebook vectors zk,
where λis the EMA decay hyperparameter:
ck←λck+ (1−λ)X
i1[q(e(xi)) =zk]
zk←λzk+(1−λ)
ckX
ie(xi) 1[q(e(xi)) =zk]
This approach leads to more stable training (Kaiser
et al., 2018) and it allows us to mitigate the code-
book collapse , which occurs when the usage count
of a vector drops to zero and the vector is then never
updated (Kaiser et al., 2018). Thus, whenever a
codebook vector zihas usage cilower than cmin, it
is reassigned to a random latent vector zi←e(xj)
and the usage count is reset ci←1, similar to
Williams et al. (2020) or Dhariwal et al. (2020).

--- PAGE 3 ---
3 F ACTORIZER
We utilize the VQ-V AE architecture to train a
model capable of mapping discrete triplet rep-
resentations zto subword strings w(and vice
versa). Furthermore, we employ the VQ-V AE de-
coder to estimate the probabilities of the subword
strings p(w|z). After the model is trained, we
infer its vocabulary, consisting of a set of tuples
⟨wi,zi,logp(wi|zi)⟩. Finally, we use this vocab-
ulary to perform optimal (in regards to the log-
probabilities) subword tokenization.
3.1 VQ-V AE subword encoding
Training data. The auto-encoder is trained on a
word-frequency list obtained by word-tokenizing
a large text corpus. Let us denote the frequency of
word wbyfw. Note that while this data representa-
tion simplifies the model by discarding any contex-
tual information, it still enables proper estimation
ofp(wi|zi)by following the word frequencies fw.
Word representation. In this study, words are
represented as sequences of UTF-8 bytes. To en-
sure proper handling of word boundaries, the word
sequences start with a special “beginning-of-word”
token and end with an “end-of-word” token; both
tokens are depicted as the symbol ‘ ’ in this text.
Data sampling. In theory, the correct way of sam-
pling the training data is to directly follow the fre-
quencies fw. However, in practice, the distribution
of words in a natural language follows Zipf’s law,
resulting in a skewed distribution that collapses the
training process. To address this issue, we sample
the data according to a more balanced distribution
psample (w)∝fw
log(fw+ 1).
To compensate for this alteration and accurately
model the true word distribution, we incorporate
the denominator into the loss function by weighting
it as follows:
L=X
wlog(fw+ 1)L(w).
Subword splitting. Up to this point, we have
only discussed word encoding. To also encode
subwords, we randomly split some of the sampled
words into subwords. Specifically, as the more
frequent words should be split less frequently, we
keep the word as-is with the probability
pnot-split (w) =log(fw+ 1)
maxxlog(fx+ 1).Factorized codebooks. To capture fine-grained
information about the characters inside words, we
employ separate codebooks, each within a unique
latent space. In addition, to further reduce the issue
of codebook collapse, each codebook consists only
ofK= 256 codebook vectors.3
Backbone architecture. The auto-encoder
model is based on an encoder-decoder transformer
architecture (Vaswani et al., 2017) with three
quantization bottlenecks. Specifically, we first
pad the byte-tokens wwith special R,G,Bprior
tokens, then embed and encode each sequence
with a transformer encoder. The contextualized
embedding vectors for the first three tokens serve
as the encoding e(w). These three vectors are
then quantized and prepended to the subword
byte-tokens w, which are finally input into the
autoregressive transformer decoder d(z,w),
modeling p(w|z).
Training details. All VQ-V AE models in this
study utilize a transformer with 6 layers and 256
hidden size for both the encoder and decoder. The
models are trained for 50 000 steps with batch size
of 4 096 and optimized with adaptive sharpness-
aware minimization (ASAM; Kwon et al., 2021)
to ensure better generalization to unseen words,
with AdamW (Loshchilov and Hutter, 2019) as
the underlying optimizer. To further improve gen-
eralization, we calculate the exponential moving
average of the parameters (with a decay of 0.999)
and use this average for inference. For more details
about the transformer architecture and hyperpa-
rameters, please refer to Samuel et al. (2023) and
Appendix D.
Vocabulary construction. The VQ-V AE is not
used directly for tokenization as that would greatly
slow it down. Instead, we infer its static vocabulary
by iterating through all 2563instances of the RGB
codebooks. The subword vocabulary is decoded as
W=
argmax
wp(w|z)z∈[256]3, p(z)>0
,
where the prior distribution p(z)is calculated by
counting the usage of all codebook triplets through-
out training. Finally, the full vocabulary Vis a set
of tuples ⟨wi,zi,logp(wi|zi)⟩, where wi∈ W
andzi= argmaxzp(wi|z).
3Note that this is reminiscent of the standard 24-bit RGB
color encoding, where every codebook triplet can be viewed
as an RGB color. FACTORIZER essentially projects subwords
into a color space, see Appendix A for more details.

--- PAGE 4 ---
_t o k e n i z a t i o n_1.313.54
1.00
1.11
1.31
1.872.11
1.00
1.311.001.07
1.321.01 1.02 1.00 1.01
1.011.24
1.001.02
1.081.031.01Figure 2: Diagram showing an illustrative subset of the state space when searching for the optimal split of word
“tokenization” . Every subword wis associated with a weighted directed edge in the state graph of value score( w);
the optimal tokenization is then equivalent to finding the shortest path between the leftmost and the rightmost node.
3.2 Subword tokenizer
Optimal split search. After inferring the vocab-
ularyV, we can search for the optimal tokenization
of a word xinto subwords w1,w2, . . .wk:
tokenize (x) = argmin
w1w2...wk=xkX
i=1score (wi),
where for each subword wifrom the vocabulary
triplets ⟨wi,zi,logp(wi|zi)⟩ ∈ V , its score is
score (wi) =−logp(wi|zi) +αsplit.
The parameter αsplitallows for a smooth change of
the amount of splits per word, as shown in Figure 3.
We use αsplit= 0.1for all experiments in this work.
The tokenize function is implemented via the
shortest path search in a tokenization graph, as il-
lustrated in Figure 2. Specifically, the search uses
Dijkstra’s algorithm (Dijkstra, 1959) and the for-
ward edges from each node are efficiently iterated
with the prefix search in a directed acyclic word
graph (DAWG; Blumer et al., 1985) of all subwords
fromV. A simplified pseudocode of the search al-
gorithm is given below, the optimal tokenization is
then obtained by reverse iteration of the returned
prev dictionary.
def search( word ,dawg_trie ):
Q.append( word ) priority queue of suffixes
cost [word ] = 0.0 cost returns +inf by default
prev [word ] = none pointers to predecesors
while Q is not empty:
suffix = suffix from Qwith minimal cost
remove suffix from Q
if suffix is empty: break
for every prefix ofsuffix indawg_trie :
new_cost =cost [suffix ] + score( prefix )
new_suffix =suffix [len( prefix ):]
if new_cost <cost [new_suffix ]:
Q.append( new_suffix )
cost [new_suffix ] = new_cost
prev [new_suffix ] = min_suffix
return prev
0.001 0.01 0.1 1.0 10.0
αsplit1.001.051.101.151.20splits / wordBPE
FactorizerFigure 3: The influence of the splitting parameter αsplit
on the amount of splits per word, as calculated on the
en-ewt dataset from Universal Dependencies. The
BPE tokenizer has vocabulary size of 32 768.
Sampling. Our formulation of the tokenizer al-
lows for a straightforward modification for sam-
pling different subword splits. This is similar to
BPE dropout (Provilkov et al., 2020), a powerful
regularization technique. The sampling works by
modifying the score function as follows, making
sure that all scores are non-negative so that the
correctness of the search method holds:
score (wi) =−logp(wi|zi) +αsplit+|wi|exp(ϵ)
ϵ∼ N(0, σ2
sample )
We set the parameter σsample to 0.02 in all sampling
experiments.
3.3 Application
Embedding layer. The three FACTORIZER sub-
word indices are embedded with separate embed-
ding layers, summed and transformed with a GeLU
non-linearity. In the end, we get a single embed-
ding vector for each subword, which makes it a
simple drop-in replacement for standard embed-
ding layers.
Averaging. The random subword sampling can
be utilized for more robust inference by sampling
multiple tokenizations of each data instance and
averaging the predictions for each of them.

--- PAGE 5 ---
4 Experiments
The effectiveness of FACTORIZER is experimen-
tally verified on a typologically diverse set of lan-
guages. We train masked language models on each
language and then evaluate it on morpho-syntactic
tasks from the Universal Dependencies (UD) tree-
banks (Nivre et al., 2016). Additionally, we demon-
strate that the FACTORIZER -based language models
exhibit greater robustness to noise and superior per-
formance in low-resource settings. We present four
sets of experiments in this section:
1.In order to provide some initial observations,
we evaluate simple parsing models that do not
rely on any pretrained embeddings.
2.Then, we ablate different tokenizer configu-
rations with English language modeling and
UD finetuning.
3.As the main experiment, we pretrain and fine-
tune language models on 7 typologically di-
verse languages.
4. Lastly, controlled experiments on English ex-
amine the robustness of our method to noise
and data scarcity.
Languages. In total, our method is evaluated
on 7 different languages to investigate its perfor-
mance on different morphological typologies. For
the most part, we follow Vania and Lopez (2017)
in their selection of languages according to the
traditional categories of morphological systems.
The text corpus for each language is drawn from
the respective part of the mC4 corpora (Xue et al.,
2021), unless specified otherwise. Note that we use
the same corpus for training FACTORIZER models
(where we extract a word-frequency list for every
language) as for training language models. The
languages chosen for evaluation are:
•Arabic – The first distinct language type used
for evaluation are introflexive languages. We
decided to use Arabic as a representative of
introflexive languages because it is arguably
the most wide-spread language in this class.
Arabic also tests how a tokenizer can handle
a non-Latin script.
•Chinese – A purely analytical language; eval-
uation on Chinese tests performance on a com-
pletely different morphological typology than
the other 6 languages.•Czech – An Indo-European language which
exhibits fusional features and a rich system of
morphology , typical for all Slavic languages.
•English – In principle, also a fusional lan-
guage, however, one of the most analytical
languages in this class. The English corpus
is made of the publicly available replications
of BookCorpus and OpenWebText (because
of the unnecessarily large size of the English
part of mC4 for our purposes).45
•Norwegian – Another Germanic language, ty-
pologically close to English, but has different
morphological properties, in particular highly
productive use of morphological compound-
ing, which makes it a fitting choice for evalua-
tion of tokenizers.
•Scottish Gaelic – Also a fusional language.
In order to evaluate performance on low-
resource languages, this language, which has
about 57 000 fluent speakers, was chosen as it
is the smallest language from mC4 with a full
train-dev-test split in Universal Dependencies.
•Turkish – As an agglutinative language, Turk-
ish is on the other end of the ‘morphemes-per-
word spectrum’ than an analytical language
like Chinese.
BPE tokenizer baseline. We compare the per-
formance of FACTORIZER with the byte-pair en-
coding compression algorithm (BPE; Gage, 1994;
Sennrich et al., 2016) as the most commonly used
subword tokenizer. Following recent language
modeling improvements (Radford et al., 2019), we
use BPE directly on UTF-8 bytes instead of uni-
code characters – similarly to FACTORIZER , which
also utilizes UTF-8 bytes as the atomic unit. The
training itself uses the open implementation from
tokenizers library6and utilizes the same text
corpora as the corresponding FACTORIZER models.
The choice of BPE also allows for a comparison
between FACTORIZER sampling and BPE dropout
(Provilkov et al., 2020).
4.1 Experiment 1: UD parser ‘from scratch’
Before training large language models and evalu-
ating the tokenizers in a more resource-intensive
4https://the-eye.eu/public/AI/pile_
preliminary_components/books1.tar.gz
5https://openwebtext2.readthedocs.io
6https://github.com/huggingface/
tokenizers

--- PAGE 6 ---
1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4
splits / word82838485LAS
1K2K4K
8K
16K
32K0.10.01
0.001
0.0001
0.00001
0.000001Factorizer
BPEFigure 4: The LAS scores on the development split of
en-ewt . We evaluate multiple ‘from scratch’ models
trained on top of tokenizers with different amounts of
subword splits-per-word. FACTORIZER can change this
value by altering the αsplitparameter, while BPE influ-
ences this value indirectly by altering its vocabulary
size. The lightly-colored regions illustrate the standard
deviation of 5 runs with different random seeds.
setting, we motivate our proposed method on a sim-
ple model that does not utilize any pretrained word
embeddings.
UD parser architecture. We base this first ex-
periment on UDPipe 2, a publicly available model
for PoS tagging, lemmatization and dependency
parsing by Straka et al. (2019). We generally fol-
low the original architecture, only replacing its
word embeddings – UDPipe 2 utilizes a combi-
nation of pretrained contextualized embeddings,
static embeddings and character embeddings – in-
stead, we use only randomly initialized subword
embeddings and learn them from scratch; the final
contextualized representations for the subwords
are average-pooled for each word-token to obtain
word-level representations. Otherwise we use the
same joint modeling approach based on bidirec-
tional LSTM layers (Hochreiter and Schmidhuber,
1997) followed by individual classification heads
forUPOS ,XPOS andUFeats tagging, a classi-
fication head for relative lemma rules and a final
head for dependency parsing (Dozat and Manning,
2017).
Results. The amount of splits per word has a
noticeable impact on the performance, we therefore
include this dimension in the evaluation, visualized
in Figure 4. This shows that FACTORIZER is clearly
a Pareto-optimal tokenizer in this comparison. We
conjecture that this is caused by its ability to learn
a usable embedding for less frequent, or even out-
of-vocabulary subwords, as illustrated in Figure 5.
0 20 40 60 80 100
tokens sorted by frequency (percentiles)1101001 00010 000frequencyFactorizer — factor 1
Factorizer — factor 2
Factorizer — factor 3
BPEFigure 5: The number of occurrences of each subword
index in en-ewt . The BPE subwords clearly follow
the Zipf’s law, with 90% of them appearing less than
10 times. On the other hand, the factorized indices are
more evenly distributed, which makes it easier to learn
their embeddings with less data.
4.2 Experiment 2: Ablation study with
English language modeling
In order to evaluate the performance of different
BPE and FACTORIZER configurations, we conduct
a comparative study on English.
Language model pretraining. In practice, the
capabilities of a UD parser can be massively im-
proved by utilizing contextualized embeddings
from large language models that have been trained
on vast amounts of data. In light of this, the main
focus of this experimental section is to evaluate the
impact of the FACTORIZER on language models
and their downstream performance.
We follow the training budget and parameters
of the original BERT base(Devlin et al., 2019) for
pretraining the language models, the BPE-based
model also uses BERT’s vocabulary size of 32K.
But unlike BERT base, we establish our models on
the more efficient LTG-BERT architecture (Samuel
et al., 2023), which enhances the models with some
recent improvements, such as NormFormer layer
normalization (Shleifer and Ott, 2022), disentan-
gled attention with relative positional encoding (He
et al., 2021) or span masking (Joshi et al., 2020).
In order to reduce training time, pretraining is par-
alellized over 128 GPUs, the total batch size is
increased to 8 192 and the amount of steps is re-
duced to 31 250, matching the training budget of
BERT base. Please refer Samuel et al. (2023) for
details on the architecture and Appendix D for the
set of all hyperparameters.
In order to make the comparison between both
tokenizers fair, we approximately match the num-

--- PAGE 7 ---
ber of trainable parameters in all language models.
FACTORIZER requires relatively small embedding
layers, so we move its ‘parameter budget’ to 3 ad-
ditional transformer layers – then the BPE-based
models use 110.8M parameters while the FACTOR -
IZER -based models use 108.1M parameters in to-
tal. The effect of this choice is evaluated in Ap-
pendix B.
UD finetuning. We employ the same model as
in Section 4.1, only replacing the LSTM layers
with a convex combination of hidden layers from
a pretrained language model, similarly to UDify
(Kondratyuk and Straka, 2019). Then we finetune
the full model on a UD treebank. The detailed
hyperparameter choice is given in the Appendix D.
Results. The comparison is presented in Table 1.
On average, FACTORIZER outperforms BPE, espe-
cially in lemmatization. The results show that both
sampling and averaging techniques (discussed in
Section 3) improve performance, with the FACTOR -
IZER particularly benefiting from these features.
4.3 Experiment 3: Multi-lingual evaluation.
Finally, we train 7 BPE-based and FACTORIZER -
based language models on the selected languages
and finetune them on the respective UD treebanks.
Results. The results, displayed in Table 2, in-
dicate that FACTORIZER clearly achieves better
performance than BPE on average. Furthermore,
theFACTORIZER is consistently more effective on
lemmatization (with statistical significance), sug-
gesting that the factorized subwords are indeed able
to carry information about the characters inside the
subword-units.
Interestingly, FACTORIZER does not always per-
form better on Czech and Turkish, even though we
hypothesized that these languages should benefit
from a better character information given their rich
morphology. Neither BPE or FACTORIZER is an
overall better choice for these languages.
While the results for dependency parsing in iso-
lation (as measured by UAS andLAS) are more
mixed, the parsing results which further take into
account morphology ( MLAS ) and lemmatization
(BLEX ) largely favor the factorized subword en-
coding. FACTORIZER outperforms BPE (with sta-
tistical significance) in 6 out of 7 languages on
BLEX scores.Model AllTags Lemmas LAS
BPE 96.05±0.0698.03±0.0392.06±0.08
BPE + sampling 96.21±0.0498.25±0.0192.22±0.05
BPE + sampling + averaging 96.24±0.0498.29±0.0492.22±0.12
FACTORIZER 96.06±0.0598.01±0.0192.25±0.13
FACTORIZER + sampling 96.29±0.0898.40±0.0292.29±0.07
FACTORIZER + sampling + averaging 96.34±0.0598.48±0.0392.39±0.14
Table 1: Comparison of BPE and FACTORIZER configu-
rations. The metrics are measured on the development
split of en-ewt . We show the mean and standard devi-
ation statistics over 5 random seeds, the best results are
displayed in bold. Full results are shown in Table 3.
4.4 Experiment 4: Robustness
Robustness to noise. Being able to maintain per-
formance when dealing with unnormalized and
noisy text, is an important quality of any NLP tool
deployed in real-world scenarios. In order to evalu-
ate how FACTORIZER deals with unexpected noise,
we finetune a UD parser on clean data using the
en-ewt corpus and evaluate it on modified de-
velopment sets with increasing levels of character
noise. The noise is introduced by perturbing every
character with a set probability pnoiseby uniformly
choosing from (1) deleting the character, (2) chang-
ing its letter case, or (3) repeating the character 1–3
times.
Figure 6 shows the relation between the noise
level pnoiseand the expected performance. When
comparing FACTORIZER with BPE, it is appar-
ent that FACTORIZER is more robust to increased
noise on tagging and dependency parsing as the
performance drops more slowly with increased
noise. The accuracy drops equally on lemmatiza-
tion, which may be attributed to its formulation as
prediction of relative lemma rules (as in UDPipe 2
by Straka et al. (2019)).
Robustness to resource scarcity. The promising
results achieved on the very low-resource language
of Scottish Gaelic have motivated a more thorough
examination of the relationship between the size
of a pretraining corpus and the downstream perfor-
mance. In order to investigate this relationship, we
pretrain multiple English language models using
random fractions of the full English corpus while
maintaining a constant number of training steps.
The performance of the finetuned models is eval-
uated and plotted in Figure 7. The results demon-
strate that our proposed method is more robust to
resource scarcity. All language models are able to

--- PAGE 8 ---
Model UPOS XPOS UFeats Lemmas UAS LAS MLAS BLEX
Arabic ( ar-padt )
UDPipe 2 97.02 94.38 94.53 95.31 88.11 83.49 74.57 76.13
Stanza 62.06 54.60 68.34 49.20 42.72 39.75 39.07 38.75
BPE 97.48±0.0295.84±0.0395.97±0.0395.14±0.0490.84±0.0786.50±0.0779.03±0.1579.31±0.11
FACTORIZER 97.48±0.0395.90±0.0396.03±0.0595.91±0.0491.24±0.0887.05±0.0979.70±0.1180.80±0.11
Chinese ( zh-gsd )
UDPipe 2 96.21 96.08 99.40 99.99 87.15 83.96 78.41 82.59
Stanza 95.35 95.10 99.13 99.99 81.06 77.38 72.04 76.37
BPE 97.14±0.1096.95±0.1099.60±0.0199.99±0.0088.32±0.1685.42±0.2080.23±0.2384.33±0.17
FACTORIZER 97.00±0.0796.79±0.0899.66±0.0399.99±0.0089.34±0.1386.39±0.2181.21±0.1685.17±0.17
Czech ( cs-pdt )
UDPipe 2 99.45 98.47 98.40 99.28 95.63 94.23 90.88 92.50
Stanza 98.88 95.82 92.64 98.63 93.03 91.09 80.19 87.94
BPE 99.44±0.0198.54±0.0298.50±0.0299.26±0.0195.63±0.0494.15±0.0590.96±0.0592.42±0.04
FACTORIZER 99.39±0.0198.53±0.0398.51±0.0399.30±0.0295.54±0.0494.10±0.0590.95±0.0592.42±0.06
English ( en-ewt )
UDPipe 2 97.35 97.06 97.52 98.07 92.62 90.56 84.02 85.98
Stanza 96.87 96.67 97.08 97.82 90.39 87.94 80.44 82.57
BPE 97.82±0.0497.65±0.0397.90±0.0497.99±0.0593.46±0.0991.63±0.1085.95±0.1487.21±0.08
FACTORIZER 97.90±0.0797.72±0.0698.03±0.0298.29±0.0393.63±0.1291.80±0.1586.27±0.2287.90±0.17
Norwegian ( no-bokmaal )
UDPipe 2 98.61 — 97.68 98.82 94.40 92.91 87.59 89.43
Stanza 98.43 — 97.71 98.39 93.39 91.64 85.60 87.10
BPE 98.94±0.02— 98.21±0.0698.82±0.0295.42±0.0794.13±0.0989.68±0.1790.86±0.10
FACTORIZER 98.90±0.04— 98.37±0.0399.19±0.0195.35±0.0994.05±0.0889.85±0.1191.43±0.08
Scottish Gaelic ( gd-arcosg )
UDPipe 2 96.62 92.24 94.02 97.59 87.33 81.65 69.25 75.23
Stanza 75.23 70.87 73.41 75.59 66.97 61.71 48.75 56.79
BPE 97.66±0.0694.96±0.0795.82±0.1297.60±0.0489.01±0.0885.63±0.1575.81±0.1879.14±0.18
FACTORIZER 97.78±0.0595.78±0.0896.67±0.0998.09±0.0488.84±0.1685.30±0.1577.00±0.2279.85±0.13
Turkish ( tr-kenet )
UDPipe 2 93.72 — 92.06 93.33 84.07 71.29 61.92 64.89
Stanza 92.98 — 90.85 93.31 89.00 76.94 63.11 67.02
BPE 94.02±0.10— 92.22±0.0492.60±0.1086.35±0.0774.60±0.2165.17±0.3067.71±0.25
FACTORIZER 93.94±0.09— 92.22±0.1192.95±0.0886.18±0.0474.49±0.1365.26±0.1468.03±0.05
Macro average
UDPipe 2 97.00 95.89 96.23 97.48 89.90 85.44 78.10 79.53
Stanza 88.54 82.67 88.45 87.56 79.51 75.20 67.03 70.93
BPE 97.50±0.0696.79±0.0696.89±0.0697.34±0.0591.29±0.0987.44±0.1480.98±0.1983.00±0.15
FACTORIZER 97.48±0.0696.94±0.0697.07±0.0697.67±0.0491.45±0.1087.60±0.1381.46±0.1683.65±0.12
Table 2: The results of finetuning our language models on treebanks from the Universal Dependencies project – as
compared to two well-performing publicly available parsers, UDPipe 2 (Straka et al., 2019) and Stanza (Qi et al.,
2020). Every model is evaluated on the test split of the respective treebank, we use the official evaluation script
from the CoNLL 2018 shared task (Zeman et al., 2018, https://universaldependencies.org/conll18/
evaluation.html )and report the mean and standard deviation over 5 runs with different random seeds, using the
gold tokenization. The best results for each language are displayed in bold. In order to compare models, we use the
Almost Stochastic Order test (ASO; Del Barrio et al., 2018; Dror et al., 2019) implemented by Ulmer et al. (2022).
We compare the BPE and FACTORIZER models based on five random seeds using ASO with a confidence level
ofα= 0.05(before adjusting for comparison of 7 different languages using the Bonferroni correction). Almost
stochastic dominance ( ϵmin< τwithτ= 0.2) is indicated by underlines.

--- PAGE 9 ---
0.0 0.25 1.0 4.0
noise percentage91929394959697
AllTags
BPE
Factorizer
0.0 0.25 1.0 4.0
noise percentage889092949698
Lemmas
BPE
Factorizer
0.0 0.25 1.0 4.0
noise percentage888990919293
LAS
BPE
FactorizerFigure 6: Robustness to noise. We train a single English
parser on clean data and evaluate it on modified devel-
opment sets with increasing amount of character noise.
The performance of the FACTORIZER -based parser is
deteriorating slower than the BPE-based model.
maintain performance with 1/64of the full corpus
and after this point, the FACTORIZER -based models
are less sensitive to reducing the corpus size even
more.
5 Related work
The neural era in NLP has brought about a change
in how sentences are typically tokenized into
atomic units. Tokenization in current NLP typi-
cally involves the segmentation of the sentence into
subwords, smaller units than the traditional word
tokens of the pre-neural era. The question of what
these subword units should consist of has received
a fair amount of attention in previous research. Lin-
guistically motivated or rule-based approaches are
found (Porter, 1980), however the majority of this
work is based on unsupervised morphological seg-
mentation. While this field has a long-standing his-
tory (Mielke et al., 2021), subwords have in recent
years been based on BPE (Gage, 1994; Sennrich
et al., 2016) and variations such as WordPiece (Wu
et al., 2016) or SentencePiece (Kudo and Richard-
son, 2018).
There have been quite a few studies examin-
ing the influence of different morphological sys-
tems on language modeling performance (Vania
and Lopez, 2017; Gerz et al., 2018; Bostrom and
Durrett, 2020). In a highly multilingual setting, ex-
amining 92 languages, Park et al. (2021) study the
influence of morphology on language modeling dif-
ficulty, contrasting BPE with the Morfessor system
(Creutz and Lagus, 2007) and a rule-based mor-
phological segmenter. There has also been some
work comparing BPE with alternative tokenizers
for downstream applications, mostly within ma-
chine translation and largely with negative results
(Ataman and Federico, 2018; Domingo et al., 2018;
1
11
41
161
641
2561
1024
corpus subset94.595.095.596.096.5
AllTags
BPE
Factorizer
1
11
41
161
641
2561
1024
corpus subset97.497.697.898.098.298.498.6
Lemmas
BPE
Factorizer
1
11
41
161
641
2561
1024
corpus subset8990919293
LAS
BPE
FactorizerFigure 7: Robustness to data scarcity during pretrain-
ing. We pretrain language models on random fractions
of the English corpus and then measure their perfor-
mance when finetuned on en-ewt . We can see that
FACTORIZER -based language models are more robust
to pretraining in low-resource settings.
Machá ˇcek et al., 2018). In this work we instead
examine several morpho-syntactic tasks, finding
clear improvements over BPE.
The recent pixel-based encoder of language
(PIXEL; Rust et al., 2023) reframes language mod-
eling as a visual recognition task, employing a
transformer-based encoder-decoder trained to re-
construct the pixels in masked image patches, and
disposing of the vocabulary embedding layer com-
pletely. The method shows strong results for un-
seen scripts, however, more modest performance
for languages with Latin script, such as English.
As in our work, they find morpho-syntactic tasks to
benefit the most and more semantic tasks to show
more mixed results.
6 Conclusion
We proposed a novel tokenization method where
every subword token is factorized into triplets of
indices. The main benefit of this factorization is
that the subwords-units maintain some character-
level information without increasing the length of
tokenized sequences. In practice, FACTORIZER
even slightly decreases the number of subword
splits (Figure 3), while noticeably improving the
performance of large language models on morpho-
syntactic tasks, especially on lemmatization (Ta-
ble 2). Further experiments demonstrated increased
robustness to noise and to data scarcity (Figure 6
and 7).
We hope that this work clearly demonstrated
the potential of utilizing factorized subwords for
language modeling and future work will improve
their performance even further.

--- PAGE 10 ---
Limitations
Size. A limiting factor for some low-resource ap-
plications might be the size of a saved FACTORIZER
file. We only have to store the subword vocabulary,
which however takes substantially more space than
BPE as it needs to be stored as DAWG trie to keep
the tokenization speed similar to BPE. For exam-
ple, the saved English FACTORIZER takes about
115MB of space while the English BPE with 32K
subwords takes just about 1MB of space. We be-
lieve that the space requirements are negligable
compared to the size of large language models, but
they can a limiting factor in some edge cases.
The parameter-efficiency of FACTORIZER fol-
lows the basic nature of factorized representations:
a sequence of 3 bytes can represent more than 16M
values ( 2563). That’s how we can embed millions
of subwords with a negligible parameter count. On
the other hand, when we store a vocabulary with
millions of subwords on disc, it necessarily requires
more space than a BPE vocabulary with 10 000s of
subwords.
GLUE performance. While our main interest
and focus has been on morpho-syntactic down-
stream tasks, it is reasonable to ask what is the
performance of FACTORIZER -based language mod-
els on other tasks, such as natural language under-
standing. We utilize the pretrained English lan-
guage models and finetune them on 8 GLUE tasks
(Wang et al., 2018). The results in Appendix C
indicate that in this setting, our method is compara-
ble to BPE but not better on average, even though
both approaches stay within a standard deviation
from each other. We hope that these results can be
improved in future work.
Acknowledgements
We would like to thank Petter Mæhlum, Andrey
Kutuzov and Erik Velldal for providing very useful
feedback on this work.
The efforts described in the current paper were
jointly funded by the HPLT project (High Perfor-
mance Language Technologies; coordinated by
Charles University).
The computations were performed on resources
provided through Sigma2 – the national research
infrastructure provider for High-Performance Com-
puting and large-scale data storage in Norway.References
Duygu Ataman and Marcello Federico. 2018. An
evaluation of two vocabulary reduction methods
for neural machine translation. In Proceedings of
the13th Conference oftheAssociation forMachine
Translation intheAmericas (V olume 1:Research
Track) , pages 97–110, Boston, MA. Association for
Machine Translation in the Americas. 9
A. Blumer, J. Blumer, D. Haussler, A. Ehrenfeucht, M.T.
Chen, and J. Seiferas. 1985. The smallest automa-
tion recognizing the subwords of a text. Theoretical
Computer Science , 40:31–55. Eleventh International
Colloquium on Automata, Languages and Program-
ming. 4
Kaj Bostrom and Greg Durrett. 2020. Byte pair encod-
ing is suboptimal for language model pretraining.
InFindings oftheAssociation forComputational
Linguistics: EMNLP 2020 , pages 4617–4624, On-
line. Association for Computational Linguistics. 9
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Trans. Speech Lang. Process. , 4(1).
9
Eustasio Del Barrio, Juan A Cuesta-Albertos, and Car-
los Matrán. 2018. An optimal transportation ap-
proach for assessing almost stochastic order. In The
Mathematics oftheUncertain, pages 33–44. 8
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings ofthe2019 Conference
oftheNorth American Chapter oftheAssociation
forComputational Linguistics: Human Language
Technologies, V olume 1(Long andShort Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Asso-
ciation for Computational Linguistics. 6
Prafulla Dhariwal, Heewoo Jun, Christine Payne,
Jong Wook Kim, Alec Radford, and Ilya Sutskever.
2020. Jukebox: A generative model for music. arXiv
preprint arXiv:2005.00341. 2
Edsger W Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische mathematik ,
1(1):269–271. 4
Miguel Domingo, Mercedes Garcıa-Martınez, Alexan-
dre Helle, Francisco Casacuberta, and Manuel Her-
ranz. 2018. How much does tokenization affect neu-
ral machine translation? 9
Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In International Conference onLearning
Representations. 6
Rotem Dror, Segev Shlomov, and Roi Reichart. 2019.
Deep dominance - how to properly compare deep neu-
ral models. In Proceedings ofthe57th Conference
oftheAssociation forComputational Linguistics,

--- PAGE 11 ---
ACL 2019, Florence, Italy, July 28-August 2,2019,
V olume 1:Long Papers , pages 2773–2785. Associa-
tion for Computational Linguistics. 8
Philip Gage. 1994. A new algorithm for data compres-
sion. CUsers J., 12(2):23–38. 5, 9
Daniela Gerz, Ivan Vuli ´c, Edoardo Maria Ponti, Roi
Reichart, and Anna Korhonen. 2018. On the relation
between linguistic typology and (limitations of) mul-
tilingual language modeling. In Proceedings ofthe
2018 Conference onEmpirical Methods inNatural
Language Processing , pages 316–327, Brussels, Bel-
gium. Association for Computational Linguistics. 9
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2021. DeBERTa: Decoding-Enhanced
BERT with disentangled attention. In International
Conference onLearning Representations. 6
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation , 9(8):1735–
1780. 6
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,
Luke Zettlemoyer, and Omer Levy. 2020. Span-
BERT: Improving pre-training by representing and
predicting spans. Transactions oftheAssociation for
Computational Linguistics, 8:64–77. 6
Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish
Vaswani, Niki Parmar, Jakob Uszkoreit, and
Noam Shazeer. 2018. Fast decoding in se-
quence models using discrete latent vari-
ables. In Proceedings ofthe35th International
Conference onMachine Learning , volume 80 of
Proceedings of Machine Learning Research , pages
2390–2399. PMLR. 2
Dan Kondratyuk and Milan Straka. 2019. 75 languages,
1 model: Parsing Universal Dependencies univer-
sally. In Proceedings ofthe2019 Conference on
Empirical Methods inNatural Language Processing
and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP) ,
pages 2779–2795, Hong Kong, China. Association
for Computational Linguistics. 7
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings ofthe2018 Conference onEmpirical
Methods inNatural Language Processing: System
Demonstrations , pages 66–71, Brussels, Belgium.
Association for Computational Linguistics. 9
Jungmin Kwon, Jeongseop Kim, Hyunseo Park,
and In Kwon Choi. 2021. ASAM: Adap-
tive sharpness-aware minimization for scale-
invariant learning of deep neural networks.
In Proceedings of the 38th International
Conference onMachine Learning , volume 139
of Proceedings of Machine Learning Research ,
pages 5905–5914. PMLR. 3Ilya Loshchilov and Frank Hutter. 2019. Decou-
pled weight decay regularization. In International
Conference onLearning Representations. 3
Dominik Machá ˇcek, Jonáš Vidra, and Ond ˇrej Bo-
jar. 2018. Morphological and language-agnostic
word segmentation for NMT. In Text, Speech, and
Dialogue , pages 277–284, Cham. Springer Interna-
tional Publishing. 9
Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky,
Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja,
Chenglei Si, Wilson Y Lee, Benoît Sagot, et al. 2021.
Between words and characters: A brief history of
open-vocabulary modeling and tokenization in NLP.
arXiv preprint arXiv:2112.10508. 9
Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Haji ˇc, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal Dependencies v1: A multilingual
treebank collection. In Proceedings oftheTenth
International Conference onLanguage Resources
andEvaluation (LREC’16) , pages 1659–1666, Por-
torož, Slovenia. European Language Resources As-
sociation (ELRA). 5
Hyunji Hayley Park, Katherine J. Zhang, Coleman Ha-
ley, Kenneth Steimel, Han Liu, and Lane Schwartz.
2021. Morphology matters: A multilingual language
modeling analysis. Transactions oftheAssociation
forComputational Linguistics, 9:261–276. 9
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch:
An imperative style, high-performance deep learning
library. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors,
Advances inNeural Information Processing Systems
32, pages 8024–8035. Curran Associates, Inc. 15
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137. 9
Ivan Provilkov, Dmitrii Emelianenko, and Elena
V oita. 2020. BPE-dropout: Simple and effec-
tive subword regularization. In Proceedings of
the58th Annual Meeting oftheAssociation for
Computational Linguistics , pages 1882–1892, On-
line. Association for Computational Linguistics. 4,
5
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and
Christopher D. Manning. 2020. Stanza: A Python
natural language processing toolkit for many hu-
man languages. In Proceedings ofthe58th Annual
Meeting ofthe Association for Computational
Linguistics: System Demonstrations. 8

--- PAGE 12 ---
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. 5
Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Eliz-
abeth Salesky, Miryam de Lhoneux, and Desmond
Elliott. 2023. Language modelling with pixels. In
TheEleventh International Conference onLearning
Representations. 9
David Samuel, Andrey Kutuzov, Lilja Øvrelid, and
Erik Velldal. 2023. Trained on 100 million
words and still in shape: BERT meets British
National Corpus. In Findings oftheAssociation
forComputational Linguistics: EACL 2023 , pages
1954–1974, Dubrovnik, Croatia. Association for
Computational Linguistics. 3, 6
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings ofthe54th Annual
Meeting ofthe Association for Computational
Linguistics (V olume 1:Long Papers) , pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics. 5, 9
Sam Shleifer and Myle Ott. 2022. Normformer: Im-
proved transformer pretraining with extra normaliza-
tion. 6
Milan Straka, Jana Straková, and Jan Hajic. 2019. UD-
Pipe at SIGMORPHON 2019: Contextualized em-
beddings, regularization with morphological cate-
gories, corpora merging. In Proceedings ofthe16th
Workshop onComputational Research inPhonetics,
Phonology, and Morphology , pages 95–103, Flo-
rence, Italy. Association for Computational Linguis-
tics. 6, 7, 8
Naftali Tishby, Fernando C. Pereira, and William Bialek.
1999. The information bottleneck method. In
Proc. ofthe37-th Annual Allerton Conference on
Communication, Control andComputing , pages 368–
377. 2
Dennis Ulmer, Christian Hardmeier, and Jes Frellsen.
2022. deep-significance-easy and meaningful statisti-
cal significance testing in the age of neural networks.
arXiv preprint arXiv:2204.06815. 8
Aaron van den Oord, Oriol Vinyals, and koray
kavukcuoglu. 2017. Neural discrete representa-
tion learning. In Advances inNeural Information
Processing Systems , volume 30. Curran Associates,
Inc. 1, 2
Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphol-
ogy? In Proceedings ofthe55th Annual Meeting
oftheAssociation forComputational Linguistics
(V olume 1:Long Papers) , pages 2016–2027, Van-
couver, Canada. Association for Computational Lin-
guistics. 5, 9Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. In Advances inNeural Information
Processing Systems , volume 30. Curran Associates,
Inc. 3
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings ofthe
2018 EMNLP Workshop BlackboxNLP: Analyzing
andInterpreting Neural Networks forNLP, pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics. 10
Will Williams, Sam Ringer, Tom Ash, David MacLeod,
Jamie Dougherty, and John Hughes. 2020. Hierarchi-
cal quantized autoencoders. In Advances inNeural
Information Processing Systems , volume 33, pages
4524–4535. Curran Associates, Inc. 2
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,
Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner, Apurva Shah, Melvin Johnson, Xiaobing
Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,
Taku Kudo, Hideto Kazawa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason
Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
Greg Corrado, Macduff Hughes, and Jeffrey Dean.
2016. Google’s neural machine translation system:
Bridging the gap between human and machine trans-
lation. 9
Linting Xue, Noah Constant, Adam Roberts, Mi-
hir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. 2021. mT5: A mas-
sively multilingual pre-trained text-to-text trans-
former. In Proceedings ofthe2021 Conference
oftheNorth American Chapter oftheAssociation
forComputational Linguistics: Human Language
Technologies , pages 483–498, Online. Association
for Computational Linguistics. 5
Daniel Zeman, Jan Haji ˇc, Martin Popel, Martin Potthast,
Milan Straka, Filip Ginter, Joakim Nivre, and Slav
Petrov. 2018. CoNLL 2018 shared task: Multilingual
parsing from raw text to Universal Dependencies.
InProceedings oftheCoNLL 2018 Shared Task:
Multilingual Parsing from Raw Text toUniversal
Dependencies , pages 1–21, Brussels, Belgium. Asso-
ciation for Computational Linguistics. 8

--- PAGE 13 ---
A Color interpretation
Our subword encoding method factorizes every subword-unit into 3×256values, which is reminiscent of
the standard 24-bit RGB color encoding. Thus every subword can be directly visualized as one pixel with
an RGB color. While this interpretation does not have any practical application, it might help to explain
our method from a different angle. Note that this analogy is not perfect – our method views all indices as
completely independent and thus the factorized ‘R channel’ with index 101is as close to 102as to 237.
We illustrate this interpretation in Figure 8, where we show the factorized subwords of the first verse
and chorus of the song Suzanne by Leonard Cohen:
Suzanne takes you down to her place near the river
You can hear the boats go by, you can spend the night beside her
And you know that she’s half-crazy but that’s why you want to be there
And she feeds you tea and oranges that come all the way from China
And just when you mean to tell her that you have no love to give her
Then she gets you on her wavelength
And she lets the river answer that you’ve always been her lover
And you want to travel with her, and you want to travel blind
And you know that she will trust you
For you’ve touched her perfect body with your mind
Figure 8: The first verse and chorus of the song Suzanne by Leonard Cohen, encoded with FACTORIZER . Every
subword can be interpreted as one 24-bit RGB pixel because we factorize every subword into 3×256values. Notice
how the colors of ‘river’ [96,42,127],‘lover‘ [96,42,104] and‘love’ [88,42,104] appear similar because these
(sub)words share two out of three indices.

--- PAGE 14 ---
B Full English ablation results
The full results of the comparative study from Section 4.2 (using the full set of UD metrics) are given
in Table 3. This table also shows what happens if we keep a constant number of layers between the
FACTORIZER and BPE-based language models – as opposed to fixing the parameter count.
Model # params UPOS XPOS UFeats Lemmas UAS LAS MLAS BLEX
BPE 110.8M 97.74±0.0297.55±0.0397.72±0.0498.03±0.0393.79±0.1092.06±0.0886.22±0.1187.94±0.09
BPE + sampling 110.8M 97.88±0.0297.71±0.0597.82±0.0598.25±0.0193.94±0.0792.22±0.0586.44±0.0488.35±0.06
BPE + sampling + averaging 110.8M 97.88±0.0497.72±0.0297.81±0.0598.29±0.0493.89±0.1392.22±0.1286.58±0.0988.47±0.09
FACTORIZER 108.1M 97.76±0.0497.58±0.0497.74±0.0398.01±0.0194.04±0.1392.25±0.1386.26±0.1488.02±0.20
FACTORIZER + sampling 108.1M 97.94±0.0297.75±0.0697.84±0.0698.40±0.0293.96±0.0692.29±0.0786.62±0.1188.71±0.12
FACTORIZER + sampling + averaging 108.1M 97.97±0.0297.77±0.0397.87±0.0298.48±0.0394.06±0.1392.39±0.1486.87±0.1988.92±0.19
FACTORIZER (12L) 86.8M 97.75±0.0297.54±0.0397.74±0.0398.01±0.0593.72±0.0891.93±0.0885.99±0.1987.69±0.20
FACTORIZER (12L) + sampling 86.8M 97.80±0.0397.68±0.0497.72±0.0798.24±0.0393.45±0.1191.66±0.0885.46±0.0987.61±0.07
FACTORIZER (12L) + sampling + averaging 86.8M 97.88±0.0497.70±0.0497.76±0.0498.31±0.0393.75±0.0692.00±0.0986.06±0.2088.19±0.10
Table 3: Full comparison of BPE and FACTORIZER configurations. ‘ FACTORIZER (12L)’ denote the configuration
without any additional transformer layers. The metrics are measured on the development split of en-ewt . We
show the mean and standard deviation statistics over 5 random seeds, the best results are displayed in bold.
C GLUE results
As noted in the Limitations section, our method is comparable to BPE but not better on average, when
evaluated on this NLU benchmark suite. The detailed results are given in Table 4.
Task MetricBPE BPE BPE F ACTORIZER FACTORIZER FACTORIZER
+ sampling + sampling + sampling + sampling
+ averaging + averaging
CoLA MCC 59.56±1.3359.93±1.7460.25±1.7162.02±1.8260.71±1.2960.42±2.28
MNLI matched acc. 87.08±0.2986.03±0.1586.18±0.0786.35±0.1685.60±0.0785.86±0.16
mismatched acc. 86.48±0.2086.46±0.2286.49±0.1586.26±0.1185.61±0.1585.77±0.23
MRPC accuracy 89.61±0.4588.77±0.7288.48±0.7186.96±1.1886.96±0.4087.79±1.32
F1 92.60±0.4592.00±0.7291.79±0.7190.60±1.1890.74±0.4091.27±1.32
QNLI accuracy 91.98±0.2291.93±0.1291.93±0.1092.01±0.2091.14±0.2291.58±0.36
QPP accuracy 91.59±0.0591.62±0.1191.63±0.1091.75±0.1491.69±0.0591.72±0.07
F1 88.78±0.0888.78±0.1688.86±0.1389.04±0.1888.92±0.0888.94±0.09
RTE accuracy 73.14±2.4273.94±1.0975.31±1.1666.35±1.4168.88±1.2371.48±1.37
SST-2 accuracy 93.60±0.0593.62±0.5193.76±0.4893.83±0.4193.07±0.2893.30±0.43
STS-B Pearson corr. 89.21±0.3489.26±0.4289.67±0.4689.32±0.2588.71±0.1088.84±0.19
Spearman corr. 89.23±0.2789.14±0.3389.52±0.3989.06±0.2088.48±0.0988.61±0.20
Average 84.45±1.0084.43±0.8184.70±0.8183.61±0.9383.40±0.6683.90±1.07
Table 4: Detailed development GLUE results for the English language models. We show the mean and standard
deviation statistics over 5 runs with different random seeds.

--- PAGE 15 ---
D Hyperparameters
All hyperparameters are listed below: VQ-V AE hyperparameters in Table 5, pretraining hyperparameters
in Table 6 and finetuning hyperparameters in Table 7. Note that the the full PyTorch (Paszke et al., 2019)
source code can be found in https://github.com/ltgoslo/factorizer .
The training was performed on 128 AMD MI250X GPUs (distributed over 16 compute nodes) and took
approximately 8 hours per model in a mixed precision mode.
Hyperparameter Value
Codebook size 256
Number of codebooks 3
Number of layers 6
Hidden size 256
FF intermediate size 683
FF activation function GEGLU
Attention heads 4
Attention head size 64
Dropout 0.1
Attention dropout 0.1
Training steps 50 000
Batch size 4 096
Warmup steps 500
Initial learning rate 0.001
Final learning rate 0.0001
Learning rate decay cosine
Weight decay 0.01
Layer norm ϵ 1e-5
Optimizer ASAM + AdamW
ASAM ρ 0.2
AdamW ϵ 1e-6
AdamW β1 0.9
AdamW β2 0.98
VQ-V AE β 0.5
Codebook EMA λ 0.96
Weight EMA λ 0.999
Table 5: VQ-V AE hyperparameters.

--- PAGE 16 ---
Hyperparameter Value
Number of layers 12 or 15
Hidden size 768
FF intermediate size 2 048
V ocabulary size 32 768 or 3×256
FF activation function GEGLU
Attention heads 12
Attention head size 64
Dropout 0.1
Attention dropout 0.1
Training steps 31 250
Batch size 32 768 (90% steps) / 8 192 (10% steps)
Sequence length 128 (90% steps) / 512 (10% steps)
Tokens per step 4 194 304
Warmup steps 500 (1.6% steps)
Initial learning rate 0.01
Final learning rate 0.001
Learning rate decay cosine
Weight decay 0.1
Layer norm ϵ 1e-5
Optimizer LAMB
LAMB ϵ 1e-6
LAMB β1 0.9
LAMB β2 0.98
Gradient clipping 2.0
FACTORIZER αsplit(if applicable) 0.1
FACTORIZER σsample (if applicable) 0.0 or 0.02
BPE dropout rate (if applicable) 0.0 or 0.1
Table 6: Language model pretraining hyperparameters.

--- PAGE 17 ---
Hyperparameter Value
Hidden size 768
Dropout 0.2
Attention dropout 0.2
Word dropout 0.15
Label smoothing 0.1
Epochs 20
Batch size 32
Warmup steps 250
Initial learning rate 0.001
Final learning rate 0.0001
Learning rate decay cosine
Weight decay 0.001
Layer norm ϵ 1e-5
Optimizer AdamW
AdamW ϵ 1e-6
AdamW β1 0.9
AdamW β2 0.98
Gradient clipping 10.0
FACTORIZER αsplit(if applicable) 0.1
FACTORIZER σsample (if applicable) 0.0 or 0.02
BPE dropout rate (if applicable) 0.0 or 0.1
Samples for averaging (if applicable) 128
Table 7: Hyperparameters for finetuning language models on UD treebanks.
