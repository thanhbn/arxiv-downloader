# Biểu diễn cấp từ từ byte cho mô hình hóa ngôn ngữ

Chu-Tak Lee
Trường Khoa học Máy tính
Đại học Fudan
lizd20@fudan.edu.cn

Qipeng Guo
Amazon AWS AI
Amazon
gqipeng@amazon.com

Xipeng Qiu
Trường Khoa học Máy tính
Đại học Fudan
xpqiu@fudan.edu.cn

## Tóm tắt

Các mô hình ngôn ngữ hiện đại chủ yếu nhận đầu vào là các từ con, một thiết kế cân bằng sự đánh đổi giữa kích thước từ vựng, số tham số và hiệu suất. Tuy nhiên, token hóa từ con vẫn có những nhược điểm như không mạnh mẽ trước nhiễu và khó tổng quát hóa sang ngôn ngữ mới. Ngoài ra, xu hướng hiện tại của việc mở rộng quy mô mô hình cho thấy các mô hình lớn hơn yêu cầu embedding lớn hơn nhưng điều đó làm cho việc song song hóa trở nên khó khăn. Nghiên cứu trước đây về phân loại hình ảnh chứng minh rằng việc chia đầu vào thô thành một chuỗi các khối là một thiên hướng quy nạp mạnh mẽ, không phụ thuộc vào mô hình. Dựa trên quan sát này, chúng tôi tái xem xét phương pháp nhận biết ký tự hiện có nhận đầu vào ở cấp ký tự nhưng thực hiện mô hình hóa và dự đoán chuỗi ở cấp từ. Chúng tôi cải tiến phương pháp này bằng cách giới thiệu một mạng cross-attention xây dựng biểu diễn cấp từ trực tiếp từ byte, và một dự đoán cấp từ con dựa trên trạng thái ẩn cấp từ để tránh yêu cầu thời gian và không gian của dự đoán cấp từ. Với hai cải tiến này kết hợp, chúng tôi có một mô hình không token với embedding đầu vào nhỏ gọn cho các tác vụ downstream. Chúng tôi đặt tên phương pháp của mình là Byte2Word và thực hiện đánh giá trên mô hình hóa ngôn ngữ và phân loại văn bản. Các thí nghiệm cho thấy Byte2Word ngang bằng với baseline từ con mạnh mẽ BERT nhưng chỉ chiếm 10% kích thước embedding. Chúng tôi tiếp tục kiểm tra phương pháp của mình trên nhiễu tổng hợp và chuyển giao đa ngôn ngữ và thấy nó cạnh tranh với các phương pháp baseline trong cả hai thiết lập.

## 1 Giới thiệu

Các mô hình ngôn ngữ là nền tảng của xử lý ngôn ngữ tự nhiên hiện đại. Từ Word2Vec (Mikolov et al., 2013) đến BERT (Devlin et al., 2018) và GPT-2 (Radford et al., 2019), một thành phần chính để làm cho việc học biểu diễn ngôn ngữ tổng quát thành công là chuyển từ đầu vào cấp từ (Mikolov et al., 2013; Pennington et al., 2014) sang đầu vào cấp từ con (Vaswani et al., 2017). Token hóa từ con giảm kích thước từ vựng, do đó giảm kích thước embedding đầu vào và đầu ra, làm cho việc huấn luyện trên một kích thước khổng lồ văn bản thu thập từ Internet trở nên khả thi về mặt tính toán. Tuy nhiên, sự phụ thuộc vào token hóa cấp từ con hạn chế khả năng của các hệ thống NLP hiện tại. Bởi vì cách thức hoạt động của các thuật toán token hóa từ con, các mô hình ngôn ngữ từ con không mạnh mẽ trước sự biến thiên nhiễu của đầu vào. Lỗi chính tả, chính tả biến đổi, viết hoa và thay đổi hình thái đều có thể khiến biểu diễn token của một từ hoặc cụm từ thay đổi hoàn toàn. Hơn nữa, gần đây Luật Mở rộng (Kaplan et al., 2020) tiết lộ mạng lớn hơn yêu cầu embedding lớn hơn, GPT-3 (Brown et al., 2020), Bloom (Scao et al., 2022) và XLM-R (Conneau et al., 2019) tất cả chúng đều có kích thước từ điển khổng lồ là 250k. Nhưng lớp embedding lớn hơn làm cho song song hóa mô hình khó khăn vì nó cố gắng chia mô hình thành các khối có kích thước bằng nhau.

ByT5 (Xue et al., 2021) cố gắng giảm thiểu những vấn đề này bằng cách chuyển sang đầu vào cấp byte. Do độ phức tạp bậc hai của Transformer, các mô hình cấp byte cần nhiều overhead hơn để xử lý cùng một cụm từ đầu vào (ví dụ, MNLI thường yêu cầu độ dài chuỗi 128 cho từ con, đối với cấp byte là >1k), trong khi vẫn kém hơn về hiệu suất. Charformer (Tay et al., 2021) đề xuất sử dụng token hóa có thể học được, có tham số. Tuy nhiên phương pháp này không nhận biết ranh giới từ nên để rút ngắn độ dài chuỗi, downsampling chỉ đơn giản là thu nhỏ chuỗi sau token hóa. Và phương pháp này dự đoán từ bị che bằng giải mã tự hồi quy, điều này thêm sự không chắc chắn khi lấy mẫu và làm cho chúng khó huấn luyện hơn.

Những tiến bộ gần đây (Dosovitskiy et al., 2020; Trockman và Kolter, 2022) về phân loại hình ảnh cho thấy rằng việc phân đoạn hình ảnh thành các patch làm tăng hiệu suất của cả mô hình transformer và convolution. Hiện tượng này có thể chỉ ra rằng phân đoạn và ranh giới token có thể hoạt động như thông tin quan trọng để học biểu diễn và thúc đẩy chúng tôi tái xem xét các phương pháp nhận biết ký tự được giới thiệu trước đây (Kim et al., 2015; Jozefowicz et al., 2016; El Boukkouri et al., 2020).

Các phương pháp nhận biết ký tự xử lý ký tự thông qua mạng convolution 1D với một bộ kernel có độ rộng khác nhau để nắm bắt thông tin của các độ chi tiết đa dạng. Sau convolution, các trạng thái ẩn cấp từ được xây dựng bằng cách nối các tín hiệu đầu ra được max-pooled theo chiều dài. Chúng tôi hiện đại hóa quá trình này bằng phương pháp cross-attention tổng quát hơn nhiều. Các phương pháp nhận biết ký tự dự đoán từ, vì vậy cần một từ vựng cấp từ có kích thước hợp lý để dự đoán nhằm giảm ngân sách tính toán. (Jozefowicz et al., 2016) đề xuất một phương pháp học contrastive tạo ra embedding đầu ra cấp từ một cách tức thời. Ngoài ra, chúng ta có thể giải mã các từ bị che theo kiểu tự hồi quy. Tuy nhiên, chúng tôi không áp dụng những phương pháp này với tâm lý về tính ổn định huấn luyện và ngân sách tính toán. Chúng tôi chứng minh rằng một cách đơn giản nhưng hiệu quả để huấn luyện mô hình cấp từ mà không có từ vựng cấp từ là tái sử dụng từ vựng cấp từ con bằng cách dự đoán từ con được đưa ra các trạng thái ẩn cấp từ bị che từ lớp encoder cuối cùng. Thấy rằng từ vựng cấp từ con chỉ cần thiết khi pretraining và embedding đầu ra cấp từ con sẽ được loại bỏ để finetuning, những gì chúng tôi có được cuối cùng là một mô hình ngôn ngữ không token xử lý câu bằng byte và với embedding đầu vào nhỏ gọn, sẵn sàng cho học chuyển giao trên các tác vụ downstream. Phương pháp của chúng tôi được đặt tên là Byte2Word và được minh họa trong Hình 1. Thông qua các thí nghiệm về mô hình hóa ngôn ngữ có che và phân loại văn bản downstream, chúng tôi cho thấy Byte2Word ngang bằng với BERT trên benchmark GLUE (Wang et al., 2018), chứng minh rằng việc học biểu diễn cấp từ từ byte có thể giữ được hiệu suất cấp BERT trong khi thu nhỏ kích thước embedding 10 lần. Để cho thấy phương pháp của chúng tôi mạnh mẽ trước nhiễu tổng hợp, chúng tôi tiêm bốn loại nhiễu vào văn bản của MNLI và thấy rằng phương pháp của chúng tôi cạnh tranh với các mô hình cấp byte trong khi sử dụng ít tính toán hơn nhiều. Bên cạnh đó, chúng tôi thấy rằng phương pháp của chúng tôi có khả năng chuyển giao đa ngôn ngữ tốt hơn so với các phương pháp từ con.

## 2 Nghiên cứu liên quan

**Mô hình hóa ngôn ngữ đa ngôn ngữ** XLM (Lample và Conneau, 2019) cho thấy việc xử lý nhiều ngôn ngữ với từ vựng chung được huấn luyện thông qua Byte Pair Encoding (Sennrich et al., 2015) cải thiện sự đối chỉnh của không gian embedding. Tuy nhiên, điều này dẫn đến tăng 3 lần kích thước từ vựng so với BERT. XLM-R (Conneau et al., 2019) nhằm mở rộng thiết lập này với nhiều tham số và ngôn ngữ hơn, dẫn đến một bảng tra cứu có kích thước 250k và một lớp embedding chiếm hơn 47% tổng số tham số mô hình. RemBERT (Chung et al., 2020) trình bày các embedding tách rời có thể cho phép mô hình có nhiều tính linh hoạt hơn. Theo nghĩa này, họ đã cân bằng lại embedding đầu vào và đầu ra của mBERT (Devlin et al., 2018) và đạt được hiệu suất tốt hơn trên mô hình hóa ngôn ngữ đa ngôn ngữ so với XLM-R, mặc dù sử dụng ít token được huấn luyện hơn nhiều.

**Mô hình hóa ngôn ngữ cấp từ** Trong khi hầu hết các mô hình ngôn ngữ dựa trên transformer được xây dựng trên token hóa từ con, các mô hình ngôn ngữ transformer cấp từ không hoàn toàn không khả thi. WordBERT (Feng et al., 2022) là BERT cấp từ đầu tiên đạt hiệu suất tốt hơn trên kiểm tra cloze, gán nhãn chuỗi và trả lời câu hỏi so với BERT. WordBERT sử dụng negative sampling để huấn luyện thành công một encoder transformer hai chiều với kích thước từ vựng 1 triệu.

**Mô hình hóa ngôn ngữ cấp byte hoặc cấp ký tự** Đối với các tác vụ chỉ tiếng Anh, token hóa cấp byte và cấp ký tự là tương đương vì mỗi ký tự tiếng Anh chỉ chiếm một byte nếu bạn bỏ qua các ký tự không ASCII. Nhưng để kết hợp các ngôn ngữ khác, các phương pháp cấp ký tự cần mở rộng từ vựng của chúng. Các phương pháp cấp byte không có vấn đề này, nhưng các ngôn ngữ sử dụng một số script khác cần nhiều hơn một byte để biểu diễn một ký tự. Ví dụ, tiếng Hy Lạp cần khoảng 2 byte và các ngôn ngữ Đông Á cần khoảng 3 byte. Điều này dẫn đến độ dài chuỗi còn dài hơn nữa. Mặc dù có những hạn chế này, ByT5 (Xue et al., 2021) cho thấy một transformer seq2seq với độ sâu encoder và decoder được cân bằng lại cộng với nhiều huấn luyện hơn có thể đạt hiệu suất cạnh tranh trên nhiều tác vụ đa dạng. CANNIE là một mô hình cấp ký tự có từ vựng 1 triệu ký tự nên nó sử dụng hash embedding (Tito Svenstrup et al., 2017) để kiềm chế lượng tham số lớn. Charformer (Tay et al., 2021) đề xuất một tokenizer có thể học được dựa trên attention xây dựng embedding từ con. Nhưng vì Charformer không có khái niệm về ranh giới từ, nó chỉ có thể giảm độ dài đầu vào bằng cách downsampling một cách ngây thơ.

**Phương pháp nhận biết ký tự** Với một chuỗi ký tự của một từ đơn, nghiên cứu trước đây (Kim et al., 2015; El Boukkouri et al., 2020) sử dụng mạng CNN 1D với nhiều kernel để nắm bắt thông tin của các độ chi tiết khác nhau. Thông tin được trích xuất bởi CNN sau đó được pooled và nối để xây dựng embedding từ cuối cùng.

**Tăng cường mô hình cấp từ con với thông tin cấp ký tự** CharBERT (Ma et al., 2020) có phương pháp kênh kép kết hợp thông tin từ con và thông tin cấp ký tự với nhau và huấn luyện khử nhiễu cấp ký tự buộc mô hình tái tạo chính tả đúng của một từ. Char2Subword (Aguilar et al., 2021) sửa đổi Transformer cấp từ con với một module học tái tạo embedding từ con BERT từ đầu vào cấp ký tự. Các thí nghiệm cho thấy phương pháp này có hiệu suất vượt trội trên dữ liệu có chuyển đổi mã. Cả hai phương pháp này đều được xây dựng trên một mô hình đã được pretrain.

## 3 Byte2Word

Mục tiêu thiết kế Byte2Word của chúng tôi là lấy một phương pháp nhận biết ký tự hiện có và thực hiện một bộ cải tiến để làm cho nó không token nhưng vẫn có hiệu suất ngang bằng so với các mô hình cấp từ con. Những yêu cầu này nảy sinh hai thách thức 1) Làm thế nào để xây dựng biểu diễn cấp từ từ byte? 2) Làm thế nào để dự đoán từ bị che mà không có từ điển cấp từ? Chúng tôi giải quyết những thách thức này trong các phần sau.

### 3.1 Độ chi tiết của Token

Token hóa cấp byte và cấp từ là hai cực đối khi nói đến việc biểu diễn văn bản thành chỉ số của từ vựng. Phương pháp đơn giản nhất, token hóa cấp ký tự là để từ vựng V là bảng chữ cái. Phương pháp này khó thực hành trên văn bản đa ngôn ngữ vì tổng kích thước bảng chữ cái của nhiều ngôn ngữ có thể lên đến một triệu. Một cách để giải quyết vấn đề này là chuyển sang đầu vào cấp byte không token vì chỉ có 256 byte, có thể biểu diễn văn bản ở mức độ chi tiết nhỏ nhất. Tuy nhiên, điều này có xu hướng tạo ra các chuỗi rất dài với thông tin thưa thớt. Token hóa cấp từ nằm ở đầu kia của phổ. Token hóa cấp từ có xu hướng yêu cầu từ vựng rất lớn để bao phủ một lượng văn bản đa dạng. Corpus BERT được huấn luyện chứa hơn 10 triệu từ duy nhất. Điều này dẫn đến một lớp embedding lớn và tính toán softmax mạnh mẽ khi dự đoán. Các phương pháp được thiết kế riêng như negative sampling được sử dụng trong (Feng et al., 2022) để huấn luyện mô hình cấp từ thành công. Transformer chủ yếu sử dụng token hóa từ con ngay khi nó được giới thiệu lần đầu. Một cách dễ dàng để giải thích token hóa từ con là nó sử dụng từ vựng chứa một tập hợp các đoạn từ xuất hiện thường xuyên như 'ing' và 'ly'. Tuy nhiên, (Gowda và May, 2020) và (Xu et al., 2020) cho thấy rằng các siêu tham số của phương pháp từ con, đặc biệt là kích thước từ điển, ảnh hưởng đến hiệu suất cuối cùng. Do đó, một số mô hình (Brown et al., 2020) cho phép một lượng dư thừa nhất định cho token hóa, dẫn đến các độ chi tiết token khác nhau trong từ vựng của nó và học gián tiếp.

Bài báo này chọn kết hợp các phương pháp cấp byte và cấp từ như được đề xuất trong (Kim et al., 2015). Tuy nhiên, để thực sự đạt được không token, chúng tôi không muốn duy trì từ vựng cấp từ, do đó chúng tôi cần đưa ra một số quy tắc hoặc phương pháp để chia câu văn bản thành từ để cung cấp byte và ranh giới từ cho mô hình. Để chia từ trong câu, ranh giới của một từ được định nghĩa rõ ràng trong tiếng Anh. Chúng tôi chia cụm từ thành từ đơn giản bằng khoảng trắng, dấu câu, camel case và snake case sau khi tuân theo thủ tục tiền xử lý được sử dụng trong (Devlin et al., 2018). Tuy nhiên, đây không phải là lựa chọn tối ưu cho một số corpus không phải tiếng Anh, chúng tôi để vấn đề này cho công việc tương lai.

### 3.2 Down-sampling

Sau khi chia câu thành từ, để xây dựng embedding cấp từ từ byte cho một trong những từ này, chúng tôi sử dụng cơ chế cross attention được đề xuất trong (Vaswani et al., 2017), đây là một cơ chế pooling hiệu quả và tổng quát hơn so với convolution nhận biết ký tự. Cho một câu và để từ thứ i của câu đó chứa L byte, chúng tôi sử dụng các embedding cấp byte tương ứng (eB_i1; eB_i2; ...; eB_iL) từ bảng tra cứu embedding cấp byte E ∈ R^(256×d) để xây dựng embedding từ eW_i với kích thước embedding d. Chúng tôi đóng gói các embedding cấp byte này lại với nhau như ma trận key Ki và ma trận value Vi cho cross-attention. Cụ thể, chúng tôi có:

Ki = Concat(eB_i1; eB_i2; ...; eB_iL)Wk  (1)
Vi = Concat(eB_i1; eB_i2; ...; eB_iL)Wv  (2)
eW_i = softmax(QiKi^T/√d)Vi  (3)

Ở đây, Qi ∈ R^(1×d) biểu thị query từ có thể học được thứ i cho vị trí i và Wk ∈ R^(d×d); Wv ∈ R^(d×d) là các ma trận cho phép chiếu key và value. Chúng tôi thấy rằng việc có nhiều query từ cho các vị trí khác nhau giúp hội tụ. Sau khi pooling bằng cross-attention, các embedding cấp từ được xử lý bởi lớp feed-forward theo vị trí như thủ tục tiêu chuẩn. Embedding cấp từ eW_i của chúng tôi sau đó được cộng bằng kết nối dư và các loại embedding khác. Theo sau bởi layer normalization (Ba et al., 2016) và phép chiếu tuyến tính, chúng tôi có được các trạng thái ẩn cấp từ cuối cùng HW_i cho encoder của chúng tôi. Chiến thuật chiếu tuyến tính này giống với phương pháp tham số hóa embedding đã được phân tích được giới thiệu trong (Lan et al., 2019) và các embedding được cân bằng lại trong (Chung et al., 2020).

HW_i = LN((FFN(eW_i) + eW_i + epos_i + etype_i))Wproj  (4)

trong đó epos là embedding vị trí, etype là embedding loại token, Wproj ∈ R^(d×d_encoder) chiếu embedding cấp từ để khớp với kích thước ẩn của encoder backbone.

### 3.3 Up-sampling và Dự đoán

Mặc dù với phương pháp trên, nó đủ cho mô hình hóa ngôn ngữ. Tuy nhiên, dự đoán ở cấp từ vẫn tốn kém về mặt tính toán do kích thước lớn của bảng tra cứu. (Feng et al., 2022) trình bày một công thức để huấn luyện BERT cấp từ bằng cách sử dụng negative sampling. Nhưng phương pháp như vậy không trực tiếp để thực hiện trong thiết lập của chúng tôi vì chúng tôi thậm chí không có từ vựng cấp từ. (Jozefowicz et al., 2016) đề xuất phương pháp dựa trên học contrastive tạo động embedding đầu ra cho chuỗi embedding cấp ký tự của từ đích. Nhưng học contrastive có thể dễ dàng dẫn đến sụp đổ mô hình và yêu cầu chăm sóc thêm để huấn luyện. Có một cách khác để huấn luyện mô hình hoàn toàn độc lập với từ vựng, (Jozefowicz et al., 2016; Xue et al., 2021; Tay et al., 2021) đề xuất dự đoán một từ bằng cách giải mã tự hồi quy một chuỗi ký tự cho biểu diễn cấp từ. Tuy nhiên, do ràng buộc về hiệu quả, giải mã cấp ký tự của (Jozefowicz et al., 2016) dựa trên mô hình cấp từ đã được pretrain sau đó đóng băng. Chúng tôi cố gắng tránh những thiết kế phức tạp này và thay vào đó chọn dự đoán ở cấp từ con. (Chung et al., 2020) cho thấy rằng việc tách kích thước ẩn của embedding đầu vào và đầu ra có thể tăng cường hiệu suất. Chúng tôi tin rằng có thể thực hiện thêm một bước nữa và tách độ chi tiết văn bản đầu vào và đầu ra. Bằng cách dự đoán từ con, chúng ta có thể tiết kiệm rất nhiều công sức và giữ cho mô hình của chúng ta hoạt động tương tự như các mô hình che toàn bộ từ và làm cho nó có thể so sánh với baseline BERT nếu chúng ta tái sử dụng từ vựng từ con của nó. Bên cạnh đó, phương pháp này vẫn không token khi học chuyển giao trên các tác vụ không tạo sinh. Các đặc trưng cấp từ con có thể được xây dựng bằng up-sampling từ các trạng thái ẩn cấp từ. Để giữ mọi thứ đơn giản, chúng tôi sử dụng phương pháp up-sampling vị trí. Giả sử chúng ta có biểu diễn Hi của từ thứ i, từ đó chứa nhiều từ con và chúng ta muốn dự đoán từ con thứ j, chúng ta có:

Hij = Hi + Pj  (5)

Trong đó Pj là query vị trí cho từ con thứ j của một từ. Chúng tôi đưa Hij vào đầu dự đoán cho dự đoán cấp từ con cuối cùng. Chúng tôi thấy rằng việc đơn giản cộng các query vị trí vào biểu diễn cấp từ hoạt động tốt hơn so với up-sampling attention phức tạp hơn (Lee et al., 2018).

## 4 Thí nghiệm

### 4.1 Thiết lập Mô hình và Pretraining

Mặc dù Byte2Word có ý nghĩa hơn trên dữ liệu đa ngôn ngữ, chúng tôi đánh giá phương pháp của mình bằng tiếng Anh do ràng buộc ngân sách. Chúng tôi tuân theo (Izsak et al., 2021) và pretrain mô hình của chúng tôi trên English Wikipedia và BookCorpus (Zhu et al., 2015). Corpus này bao gồm 20GB văn bản thô và 10 triệu từ duy nhất, điều này sẽ đủ để kiểm tra xem phương pháp của chúng tôi có thể học được sự đa dạng của từ hay không. Chúng tôi cung cấp một số thông tin thống kê về corpus trong Bảng 2. Chúng tôi tuân theo chiến lược che trong (Devlin et al., 2018) nhưng sửa đổi nhẹ định dạng đầu vào để sử dụng các giá trị byte cụ thể cho các token đặc biệt. Cụ thể chúng tôi thay thế "[PAD]" bằng HEX0, "[UNK]" bằng HEX1, "[CLS]" bằng HEX2, "[SEP]" bằng HEX3 và "[MASK]" bằng HEX4. Ví dụ, thay vì "[CLS] a [MASK] sentence example. [SEP]", Byte2Word nhận đầu vào là "\x00 a \x03 sentence example. \x02". Bằng cách này mô hình của chúng tôi học mỗi token đặc biệt từ một chỉ số duy nhất, tiết kiệm rắc rối thêm. Cũng để kiểm soát việc sử dụng bộ nhớ, chúng tôi giới hạn số lượng byte tối đa mà một từ có thể chứa là 128. Tương tự như BERT Large, chúng tôi áp dụng encoder transformer 24 lớp với 16 đầu attention và 1024 kích thước ẩn cho backbone mô hình của chúng tôi. Để giảm thiểu overhead tính toán, chúng tôi giới hạn kích thước lớp embedding Byte2Word d là 192. Bằng cách này, mô hình của chúng tôi tiêu thụ ít hơn 0.2% FLOPS thêm mỗi lần suy luận so với BERT Large. Chúng tôi cũng thấy rằng việc tăng độ rộng của embedding byte2word làm chậm sự hội tụ trong nghiên cứu ablation trong 5.4. Để đảm bảo hiệu quả pretraining, chúng tôi tuân theo (Liu et al., 2019; Izsak et al., 2021) và loại bỏ dự đoán câu tiếp theo. Chúng tôi xây dựng codebase trên (Izsak et al., 2021) và huấn luyện mô hình Byte2Word của chúng tôi trong 230k bước trên 8x NVIDIA T4 16 GB trong khoảng một tháng.

### 4.2 Đánh giá và Phân tích Downstream

Chúng tôi kiểm tra hiệu suất của mô hình trên benchmark GLUE (Wang et al., 2018), bộ đánh giá tiêu chuẩn về hiểu ngôn ngữ cho mô hình ngôn ngữ đã được pretrain, và so sánh kết quả của chúng tôi với baseline cấp từ con BERT Large. Trong quá trình finetuning, thay vì thực hiện tìm kiếm lưới trên các tập siêu tham số, chúng tôi sử dụng một tập siêu tham số cố định trên tất cả các tác vụ cho mỗi mô hình. Bảng 1 cho thấy kết quả tập kiểm tra của benchmark GLUE. Mô hình Byte2Word của chúng tôi hoạt động ngang bằng với BERT Large trên MNLI, QNLI và QQP, vượt trội hơn trên SST-2, MRPC và STS-B. Tuy nhiên, mô hình của chúng tôi có kết quả thấp hơn trên COLA và RTE. Nhìn chung, điều này tương đương với sự khác biệt <0.5% trong điểm trung bình, chứng minh rằng phương pháp của chúng tôi có thể đạt hiệu suất ngang bằng với mô hình cấp từ con, trong khi không token cho học chuyển giao và có thể chấp nhận và thích ứng với từ mới.

## 5 Phân tích

### 5.1 Học với Nhiễu Tổng hợp

Để khám phá khả năng xử lý đầu vào nhiễu của Byte2Word, tương tự như ByT5 (Xue et al., 2021), chúng tôi kiểm tra phương pháp của mình trên việc học nhiễu tổng hợp được tiêm trong quá trình học chuyển giao. Chúng tôi thí nghiệm bốn sơ đồ nhiễu tổng hợp được liệt kê dưới đây:

• **Random drop**: 10% ký tự sẽ bị loại bỏ trong một câu
• **Repetition**: 20% ký tự sẽ được lặp lại 1-3 lần (rút ra đồng đều)
• **Uppercase**: Tất cả ký tự sẽ được chuyển thành chữ hoa
• **Random case**: Tất cả ký tự sẽ được chuyển thành chữ hoa hoặc chữ thường một cách ngẫu nhiên. Mẫu này mô phỏng Alternating caps tồn tại trên Internet

Những loại nhiễu này được thêm vào dữ liệu huấn luyện và đánh giá. Chúng tôi so sánh phương pháp của chúng tôi với mô hình cấp byte vanilla ByT5 và mô hình cấp từ con phân biệt chữ hoa thường BERT Cased trên MNLI. Đối với ByT5, chúng tôi áp dụng phương pháp được giới thiệu trong EncT5 (Liu et al., 2021) để finetune encoder ByT5 trên MNLI nhưng giữ ngân sách huấn luyện gần với mô hình baseline của chúng tôi. Bảng 3 cho thấy hiệu suất kiểm tra của phương pháp cấp byte, cấp từ con và phương pháp lai Byte2word của chúng tôi trên MNLI. Byte2Word có hiệu suất vượt trội trên tất cả các loại nhiễu so với mô hình cấp từ con. Trên random dropping, ByT5 Encoder có ít mất mát hiệu suất hơn, nhưng lưu ý rằng ByT5 được huấn luyện trên mC4 (Xue et al., 2020), một tập dữ liệu đa ngôn ngữ lớn hơn nhiều được thu thập từ Internet chứa văn bản nhiễu. Trên Random case, không có gì ngạc nhiên khi thấy phương pháp của chúng tôi hoạt động tệ, do chiến lược chia camel case trong đầu vào. Kết quả này cũng cho thấy rằng ranh giới từ là quan trọng để học biểu diễn ngôn ngữ. Cho kết quả tiêm nhiễu random case sau phân đoạn từ làm suy giảm hiệu suất tối thiểu trong Bảng 3, chúng tôi giả định hiệu suất sẽ tốt hơn nhiều nếu chúng tôi đưa ra ít giả định hơn về tiền xử lý dữ liệu. Cũng thú vị khi thấy rằng mặc dù mô hình Byte2Word được pretrain sử dụng từ vựng cấp từ con không phân biệt chữ hoa thường, thiết lập này không cản trở việc finetuning trên văn bản nhiễu. Phương pháp Byte2word của chúng tôi có thể học mẫu nhiễu một cách tức thời và có sự suy giảm ít nhất trên các loại nhiễu khác nhau.

### 5.2 Chuyển giao Đa ngôn ngữ

Không giống như (Xue et al., 2021), chúng tôi đánh giá khả năng chuyển giao đa ngôn ngữ của phương pháp bằng cách finetuning mô hình chỉ tiếng Anh đã được pretrain của chúng tôi trên các tập dữ liệu downstream không phải tiếng Anh. Vì mô hình của chúng tôi không được pretrain trên corpus đa ngôn ngữ, nó phải sử dụng các từ đồng nguồn gốc hoặc ngữ pháp chung để tạo điều kiện học. Chúng tôi kiểm tra mô hình của mình trên XNLI (Conneau et al., 2018) về cơ bản là MNLI được dịch sang nhiều ngôn ngữ. Kết quả của chúng tôi trên một tập con của XNLI được hiển thị trong Bảng 4. Mặc dù hiệu suất của phương pháp chúng tôi vẫn kém hơn các mô hình ngôn ngữ đa ngôn ngữ, nó có khả năng tốt hơn khi thích ứng với ngôn ngữ mới so với phương pháp từ con. Kết quả này thúc đẩy chúng tôi áp dụng phương pháp của mình vào mô hình hóa ngôn ngữ đa ngôn ngữ trong công việc tương lai.

### 5.3 Độ tương tự Cosine giữa các Biểu diễn Từ

Chúng tôi phân tích mô hình Byte2Word đã học của chúng tôi bằng cách tính độ tương tự cosine của các biểu diễn cấp từ đã học. So sánh với không gian embedding thưa thớt của BERT (Hình 3), chúng tôi suy đoán các biểu diễn được học bởi phương pháp Byte2Word của chúng tôi dường như chứa ít ý nghĩa ngữ nghĩa hơn, vì khoảng cách cosine giữa "live" và "liver" quá gần. Tất nhiên, các trạng thái ẩn thường không thưa thớt như các mục của bảng embedding. Dựa trên những phát hiện này, chúng tôi sử dụng các cặp từ từ (Kirov et al., 2018) và tính toán tương quan Spearman giữa độ tương tự cosine của các biểu diễn đã học và các loại khoảng cách chỉnh sửa khác nhau của những cặp này. Kết quả trong Bảng 5. Chúng ta có thể thấy rằng các biểu diễn của chúng tôi có tương quan mạnh mẽ với khoảng cách jaro-winkler, trong đó tính đến các ký tự khớp và hoán vị. Chúng tôi tin rằng một lớp cross-attention duy nhất với kích thước ẩn hạn chế đóng vai trò như một nút thắt cổ chai thông tin làm cho các biểu diễn chỉ chứa trừu tượng nông cấp ký tự và tiết kiệm việc học cấp cao cho các lớp encoder tiếp theo. Ngoài ra, thú vị khi chỉ ra rằng mặc dù không có thông tin chữ hoa thường tồn tại trong tín hiệu giám sát trong quá trình pretraining, Byte2Word vẫn có thể phân biệt chữ hoa và chữ thường trong văn bản.

### 5.4 Tốc độ Hội tụ của các Kích thước Embedding khác nhau

Để đảm bảo rằng lựa chọn kích thước embedding nhỏ d cho mô hình Byte2Word của chúng tôi không làm giảm hiệu suất, chúng tôi thực hiện nghiên cứu ablation về tốc độ hội tụ của các kích thước embedding khác nhau. Chúng tôi sử dụng thiết lập trước đó trong 4.1 để pretrain các mô hình Byte2Word với nhiều kích thước embedding 96, 192, 512 và 1024 nhưng giới hạn tổng bước huấn luyện là 23k, khoảng 10% ngân sách pretraining của chúng tôi. Kết quả được hiển thị trong Hình 2. Chúng ta có thể thấy rằng mặc dù tất cả các mô hình đạt perplexity tương tự sau một lượng tính toán nhất định, 192 có tốc độ hội tụ nhanh nhất. Với ý định giới hạn kích thước ẩn của module Byte2Word càng nhỏ càng tốt, việc chọn 192 dường như là một lựa chọn tuyệt vời cân bằng tính toán và hiệu suất.

### 5.5 So sánh với phương pháp CNN 1D nhận biết ký tự

Để thể hiện phương pháp của chúng tôi vượt trội hơn các phương pháp dựa trên convolution 1D trước đây tham khảo các ký tự của token để tạo ra biểu diễn của một từ đơn, chúng tôi pretrain mô hình Byte2Word và CharacterBERT sử dụng 10% ngân sách pretraining của chúng tôi trong 4.1 và đánh giá hiệu suất của chúng trên MNLI, tác vụ đại diện nhất của benchmark GLUE. Tương tự như (Aguilar et al., 2021), chúng tôi sử dụng từ vựng của BERT như từ vựng cấp từ giả để giảm thiểu sự khác biệt kiến trúc so với baseline cấp từ con. Kết quả trong Bảng 6 cho thấy phương pháp pooling dựa trên attention của Byte2Word hoạt động tốt hơn pooling CNN 1D.

### 5.6 Tham số & Hiệu quả

Bảng tra cứu từ con của BERT Large có kích thước 30522 và đóng góp khoảng 31M tham số, gần 10% tổng kích thước mô hình. Ngược lại, phương pháp Byte2word của chúng tôi có bảng tra cứu kích thước nhỏ và một pooling cross-attention tiếp theo, bao gồm khoảng 0.4M tham số. Mặc dù nó có gấp 2 lần tham số so với phương pháp embedding nhận biết ký tự CNN 1D, so với BERT thì không đáng kể. Tuy nhiên, tra cứu embedding mặc dù kích thước của nó không yêu cầu tính toán, nhưng phương pháp Byte2Word của chúng tôi có tính toán thêm với trần 25 MFLOPS mỗi lần suy luận nếu chúng ta đặt kích thước ẩn cấp byte là 192 và câu đầu vào là 512 từ.

## 6 Hạn chế & Công việc Tương lai

Mặc dù khớp hiệu suất BERT trong nhiều trường hợp, Byte2Word hoạt động kém hơn một chút trên CoLA và RTE của GLUE Benchmark. Hai tác vụ đó đều có kích thước nhỏ. Chúng tôi suy đoán miền văn bản của những tác vụ này không được bao phủ bởi corpus pretrain của chúng tôi. Trong công việc tương lai, sẽ quan trọng để pretraining Byte2Word trên corpus với nhiều nhiễu và ngôn ngữ hơn, như OpenWebText và mC4. Ngoài ra sẽ thú vị khi kiểm tra Byte2Word trên dịch nhiều-thành-một với embedding Byte2Word của chúng tôi trên encoder nhưng decoder cụ thể cho ngôn ngữ.

## 7 Kết luận

Trong công việc này, chúng tôi trình bày Byte2Word, một phương pháp nhận biết ký tự cải tiến không token, nhẹ và ít tốn kém tính toán hơn. Về chất lượng tác vụ downstream, Byte2Word ngang bằng với BERT dựa vào từ vựng WordPiece. Về xử lý đầu vào nhiễu, phương pháp của chúng tôi vượt trội hơn nhiều so với các mô hình cấp từ con và cạnh tranh với các mô hình cấp byte vanilla. Đồng thời, hiệu quả tính toán của phương pháp chúng tôi ngang bằng với các mô hình cấp từ con và cao hơn nhiều so với các phương pháp cấp byte vanilla. Về chuyển giao sang ngôn ngữ khác, Byte2Word cho thấy hiệu suất tốt hơn baseline từ con BERT. Những kết quả này cho thấy phương pháp lai của chúng tôi có thể là sự kết hợp đúng của cả hai thế giới và có thể mở đường cho các mô hình NLP tương lai hiệu quả trong việc xử lý văn bản đa dạng.
