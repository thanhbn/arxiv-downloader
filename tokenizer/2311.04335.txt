# 2311.04335.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/tokenizer/2311.04335.pdf
# File size: 576800 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sub-Sentence Encoder:
Contrastive Learning of Propositional Semantic Representations
Sihao Chen♣∗Hongming Zhang♢Tong Chen♡Ben Zhou♣Wenhao Yu♢
Dian Yu♢Baolin Peng♢Hongwei Wang♢Dan Roth♣Dong Yu♢
♣University of Pennsylvania♡University of Washington♢Tencent AI Lab
sihaoc@cis.upenn.edu
Abstract
We introduce sub-sentence encoder , a
contrastively-learned contextual embedding
model for fine-grained semantic representation
of text. In contrast to the standard practice with
sentence embeddings, where the meaning of an
entire sequence of text is encoded into a fixed-
length vector, the sub-sentence encoder learns
to produce distinct contextual embeddings
corresponding to different atomic propositions ,
i.e. atomic units of meaning expressed within
a text sequence. The sub-sentence embed-
dings are contrastively learned to recognize
(inferred) semantic equivalence between
propositions across different text sequences.
Our experiments show the effectiveness of
sub-sentence encoders in applications, such
as retrieving supporting facts for fine-grained
text attribution or recognizing the conditional
semantic similarity between texts. In practice,
we demonstrate that sub-sentence encoders
keep the same level of inference cost and space
complexity compared to sentence encoders.
/githubhttps://github.com/schen149/
sub-sentence-encoder
1 Introduction
Sentence embeddings are a class of techniques that
represent text semantics as dense vector embed-
ding(s) (Conneau et al., 2017; Cer et al., 2018;
Reimers and Gurevych, 2019). Sentence embed-
dings are widely used in zero-shot or transfer learn-
ing settings on information retrieval and text clas-
sification tasks (Karpukhin et al., 2020; Gao et al.,
2021). With sentence embeddings, the common
practice is to encode the entire text sequence as
a fixed-length vector, where the semantic relation
with other text sequences is typically modeled by a
similarity function (Bromley et al., 1993).
While sentence embeddings provide unified and
compact semantic representations of text, it is dif-
∗Work was done during internship at Tencent AI Lab,
Bellevue.
The Gothic novel  Dracula is  
written by Bram Stoker .Similarity: Low
Dracula  – a 19th-century  
Gothic  novel , featuring … Dracula is  a novel by Bram 
Stoker,  published in 1897 .
Similarity: High Figure 1: Given an atomic proposition in a sentence
(represented by a highlighted subset of tokens), the sub-
sentence encoder produces a contextual embedding for
the meaning of the proposition. The cosine similarity
between the sub-sentence embeddings captures the (in-
ferred) semantic similarity between the propositions.
ficult to query for the varying granularity of se-
mantics from fixed dimensional sentence embed-
dings. For example, consider the two sentences
at the bottom of Figure 1 about the novel Drac-
ula. While the two sentences as a whole convey
different meanings, at the level of atomic propo-
sitions , i.e., atomic pieces of meaning conveyed
in each sentence, it becomes obvious that the two
sentences in part share similar meanings, e.g., both
sentences agree on Dracula is a novel andis pub-
lished in the 19th century .
Efficiently encoding and indexing text on a more
granular level potentially has a profound impact
on applications like long-form text evaluation (Am-
playo et al., 2022), attribution (Rashkin et al., 2023)
or factuality estimation (Min et al., 2023). With
long-form generated text, multiple propositions in
the same text might have different truthfulness val-
ues. The prerequisite for verifying or attributing
such long-form text involves (1) representing the
text on a more granular level of atomic propositions
and (2) being able to retrieve evidence for different
propositions within a text sequence. (Chen et al.,
2023a; Kamoi et al., 2023).arXiv:2311.04335v1  [cs.CL]  7 Nov 2023

--- PAGE 2 ---
Proposition #2Architecture 
Transformer Encoder 
Dracula   is    a   novel   by   Bram   Stoker Sentence  
Tokens Proposition #11 0 0 0 1 1 1
1 1 1 1 0 0 0Selective Mean Pooling MLPAtomic 
Proposition #1
embedding Atomic 
Proposition #2
embedding Sub-Sentence 
Encoder Dracula  is a novel  
by Bram  Stoker .Dracula  is a novel  
by Bram Stoker. 
Dracula , a 19th century  novel 
featuring Count Dracula. 
Dracula , a 19th  century  novel 
featuring Count Dracula. 
Dracula , a 19th century novel 
featuring  Count  Dracula .
Atomic Propositions of Sentence #1: 
Atomic Propositions of Sentence #2: 
…Positive Instance 
Negative Instance Learning Objective …Figure 2: Overview of the sub-sentence encoder architecture and learning objective: The model takes a sentence
and its propositions (represented as binary token masks) as input and outputs an embedding for each proposition.
Given a minibatch of sentences, the model learns to identify pairs of propositions that express the same meaning.
All others (including other propositions within the same sentence) are taken as negative examples (§3).
Motivated by such, we introduce sub-sentence
encoder , a contrastively-learned contextual embed-
ding model for representing sub-sentence-level se-
mantics. As shown in Figure 2, the sub-sentence
encoder takes one or more propositions within a
text sequence as input. It outputs an embedding that
represents the meaning of the proposition. Each
proposition takes the format of a binary token mask
sequence over the text, which denotes the tokens
included in each proposition (Chen et al., 2023b).
We train the sub-sentence encoder model to rec-
ognize the semantic equivalence between pairs of
atomic propositions via in-batch supervised con-
trastive learning (Khosla et al., 2020). We sample
and create training examples from a large corpus
of unlabeled sentence pair data with proposition
extraction and NLI models (§3.3).
We evaluate sub-sentence encoders on two types
of downstream tasks that involve semantic rep-
resentation on the sub-sentence level. First, we
demonstrate that sub-sentence encoders can be
used for fine-grained retrieval, e.g., for text attri-
bution, where a model is expected to retrieve sup-
porting evidence for different parts of a sentence.
Second, we show that sub-sentence encoders can
be used to infer the conditional semantic similarity
between a pair of text (Deshpande et al., 2023).
We discuss the design choices and practical chal-
lenges in applying sub-sentence encoders in large-
scale indexing of a retrieval corpus on the propo-
sition level. As encoding an entire corpus on the
proposition level might result in a prohibitively
large, we reduce the output dimension of the sub-
sentence encoder model during training (Wanget al., 2023). We show that this simple yet effective
trick results in 12 ×to 16×compression in index
size with minimal performance drop.
The main contributions of the paper are: (1)
We propose the sub-sentence encoder, a contextual
embedding method for fine-grained text semantics;
(2) We introduce an automatic process for creating
training data for sub-sentence encoders; (3) We
evaluate the utility of sub-sentence encoders in the
downstream applications of atomic fact retrieval
and conditional semantic textual similarity.
2 Preliminaries
2.1 Motivation: Text Attribution
Our design of the sub-sentence encoder is largely
motivated by the downstream application of text
attribution (Rashkin et al., 2023), i.e., identify
supporting information from known sources to at-
tribute model-generated text. With the widespread
adoption of text generation models, evaluating and
attributing generated text has become an emerg-
ing research topic in need (Gao et al., 2023a,b;
Liu et al., 2023; Malaviya et al., 2023). A key
challenge in such tasks lies in the granularity of
attributed information, i.e., one piece of generated
text usually makes more than one claim, each of
which might have different veracity. For instance,
as Figure 1 shows, there could exist multiple claims
even within one generated sentence in the form of
propositions. Each claim or proposition needs to be
contextualized (Choi et al., 2021) and individually
verified against potentially different information
sources (Kamoi et al., 2023; Min et al., 2023). This
2

--- PAGE 3 ---
process inevitably requires an efficient model repre-
senting the semantics of different sentence parts in
context, which describes the key design principle
for the sub-sentence encoder.
2.2 Limitations of Sentence Embeddings
From the perspective of downstream applications
such as text attribution, our study addresses the
following two shortcomings of current sentence
encoder models.
Granularity. Although sentence embeddings
usually capture the meaning of the entire text se-
quence as a fixed-length embedding (Morris et al.,
2023), it is difficult in practice to query sentence
embeddings for semantic information or structure
on a more granular level (Rudinger et al., 2017; Qin
and Van Durme, 2023; Wang and Yu, 2023). The
format would offer limited expressivity when mod-
eling tasks such as document retrieval, especially
when the task conceptually involves identifying
document parts that respond to the query. For such
reason, previous studies have found empirical suc-
cess with phrase retrieval or late-interaction models,
which support more granular and expressive repre-
sentations of the retrieval corpus (Seo et al., 2019;
Khattab and Zaharia, 2020; Lee et al., 2021a,b).
Contextualization. A typical assumption for sen-
tence encoder models and training/evaluation task
setup is that the sentence is encoded independently
without context. This becomes a limiting factor
in scenarios where similarities and discrepancies
between text pairs depend on the context they ap-
pear in (Chen et al., 2019; Schuster et al., 2022;
Milbauer et al., 2023a,b; Deshpande et al., 2023).
3 Sub-Sentence Encoder
We study a new type of architecture and learning
objective – sub-sentence encoders . Contrary to
sentence encoders, sub-sentence encoders are de-
signed to produce contextual embeddings for each
atomic proposition in a sentence.
3.1 Architecture
The sub-sentence encoder architecture is instan-
tiated similarly to transformer-based sentence bi-
encoders (Reimers and Gurevych, 2019), as shown
in Figure 2. The key difference is the sub-sentence
encoder takes ksets of binary token masks as ex-
tra inputs, which indicate the kpropositions of a
sentence that it should produce embeddings for.The input sentence is first forwarded through a
transformer encoder, which can be initialized from
any pre-trained encoder model. Then, for each
of the ktoken masks, the token embeddings with
mask values of 1are mean pooled and forwarded
through a projection MLP layer. The model outputs
kfixed length embedding corresponding to the k
input propositions.
Note that the ktoken masks are only applied dur-
ing pooling, and the encoder still gets full attention
to the entire sentence. This allows the proposition
embeddings to have the contextual information of
the entire input sentence/paragraph, potentially al-
leviating the need for decontextualizing the propo-
sitions (Choi et al., 2021). In addition, since there is
no cross-attention between the proposition embed-
dings, each proposition is encoded independently
of others, and its representation is inherently invari-
ant to the input ordering of the propositions.
Compared to sentence encoders, the sub-
sentence encoder adds a small amount of parame-
ters with the MLP layer on top. As the sentence can
be forwarded only once, the extra inference cost
of encoding multiple propositions in a sentence is
minimal in practice, as we discuss in §4.
3.2 Contrastive Learning
With two propositions from different sentences, the
goal is to make them have similar embedding rep-
resentations if they express similar meanings, and
have dissimilar representations otherwise. Within
a minibatch of Npropositions from Msentences.
Letvi=∈Rdbe the encoded representation of
theithproposition in the batch. Let I={1..N}
denote the index of all propositions. We formulate
the learning objective as minimizing the in-batch
supervised contrastive loss L(Khosla et al., 2020):
L=X
i∈I−1
|P(i)|X
p∈P(i)logexp(vi·vp/τ)P
j∈I\{i}exp(vi·vj/τ)
where P(i)is the set of indices of all positive
propositions to the ithproposition within the mini-
batch, and |P(i)|denotes its cardinality. τcon-
trols the softmax temperature. The learning objec-
tive encourages the model to produce embeddings
with higher cosine similarity with positive pairs
of propositions, while all other propositions in the
same batch are considered negatives. Note that if
all propositions from the same sentence are pack-
aged in the same minibatch, under the assumption
that they cannot be positive examples of each other,
3

--- PAGE 4 ---
the learning objective would inherently encourage
the model to assign different representations for
different parts of a sentence.
The supervised contrastive loss is a generalized
form of other commonly used loss functions for bi-
or dual-encoder training, e.g., N-pairs loss (Sohn,
2016) or in-batch softmax (Karpukhin et al., 2020).
We opt for this formulation mostly due to its ability
to generalize to an arbitrary number of positive
examples in the same batch. In our case, this is
important, as each proposition may have zero or
more positive instances in the same minibatch.
3.3 Sampling Proposition Pairs for Training
Here, we describe how we automatically sample
positive proposition pairs from a collection of un-
labeled sentence pairs as training data for the sub-
sentence encoder. We start from a collection of
2.5M sentence pairs from topically related news
articles (Zhou et al., 2022). This data is collected
from RealNews (Zellers et al., 2019) to find paral-
lel sentence pairs that generally describe the same
event with slightly different angles and focuses.
These instances serve as great starting points for
our need since we want to find proposition pairs
with both similarities and differences.
Step 1: Segment Sentences ⇒Propositions.
Given an unlabeled sentence pair, we first parse
each sentence into propositions in natural language
forms. First, we prompt GPT-3.5-turbo to gen-
erate propositions for 1%of all sentence pairs
as the seed set of training data. We find that
GPT-3.5-turbo with few shot in-context demon-
strations gives reasonable performance on the task,
which echos with the observations from Min et al.
(2023); Kamoi et al. (2023).
Next, we finetune T5-large (Raffel et al., 2020)
on the seed training set and use the model to gen-
erate propositions for the rest of the dataset. We
include more details about the prompt and training
process in Appendix A.
Step 2: Identify Positive Pairs with NLI mod-
els. Given the two sets of propositions (in natu-
ral language form) in each sentence pair, we infer
and label the positive proposition pairs with an off-
the-shelf NLI model (Nie et al., 2020)1. We for-
ward each pair of propositions across two sentences
through the NLI model two times, with flipped or-
ders between hypothesis and premise. We label a
1https://huggingface.co/ynie/
roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli
Grand Theft Auto V , 
usually abbreviated 
to GTA V, is a video  
game developed by  
Rockstar North .Grand Theft Auto V  
(GTA V) is a video game  
in the action genre  
released by Rockstar 
Games in 2013. Grand Theft Auto V is an action-adventure  
video game  developed by Rockstar North .
…
Corpus of Atomic Facts 
From Documents Example: A sentence with  two atomic facts 
Attribution for Fact #1 Attribution for Fact #2 Retrieval 
System Figure 3: In the Atomic Fact Retrieval task (Chen et al.,
2023b), given an atomic proposition in the query, a sys-
tem is expected to retrieve the set of supporting atomic
propositions from the corpus. The dataset features 8.8k
query propositions and 45kcandidate evidence proposi-
tions from 1.5kdocuments.
proposition pair positive if the NLI model classifies
their relation as entailment in both directions. We
only keep sentence pairs with at least one pair of
positive propositions. This leaves us with 240k
sentence pairs, with 3.32propositions per sentence
and1.21positive propositions on average.
Step 3: Convert Propositions ⇒Token Masks.
We convert the propositions in natural language
form to the token mask format used for sub-
sentence encoder input by aligning the tokens in
each proposition to the sentence. We use NLTK
(Bird et al., 2009) to lemmatize each token in a
proposition and its sentence and construct an affin-
ity matrix between the two, where tokens with iden-
tical lemmas are assigned a similarity score of 1.
To break ties between multiple token matches, we
apply a 2D-convolution filter on the affinity ma-
trix, which adds a small score offset for other to-
ken matches in a context window of three tokens.
We find the optimal matches between the proposi-
tion and sentence with max bipartite matching on
the affinity matrix with the Hungarian algorithm
(Kuhn, 1955). We include a more detailed process
description in Appendix A.
4 Experimental Setup
4.1 Model Configurations
We initialize the transformer encoder layers with
pre-trained weights from three types of sentence
encoders: SimCSE (Gao et al., 2021), Sentence-
T5 (Ni et al., 2022a), and GTR (Ni et al., 2022b).
4

--- PAGE 5 ---
System Param. Size PRECISION @1 R ECALL @5 R ECALL @10 R ECALL @20
MiniLM-L6-v2 23M 18.36 37.28 44.87 51.62
DistilRoberta 82M 16.59 33.65 40.82 46.79
SimCSE (unsupervised) 110M 8.90 45.13 69.47 84.29
SimCSE (supervised) 110M 16.53 57.83 77.28 87.89
GTR base 110M 21.90 52.50 65.54 75.69
ST5 base 110M 26.16 57.65 69.00 78.58
SUBENCODER (SimCSE) 110M (+0.5M) 41.64 71.48 78.22 83.34
SUBENCODER (ST5 base) 110M (+0.5M) 40.97 72.15 79.30 84.33
SUBENCODER (GTR base)110M (+0.5M) 40.77 72.90 80.45 85.81
Table 1: Zero-shot evaluation results on the Atomic Fact Retrieval task in P ROPSEGM ENT(Chen et al., 2023b).
With Sentence-T5 and GTR, we experiment with
the base, large, and xl-sized variants of the models.
For the MLP layer, we keep the output dimension
the same as the transformer encoder. We discuss
the impact of varying output dimensions in §5.4.
We finetune the sub-sentence encoder with dif-
ferent variants of backbone sentence encoders on
the240ksentence pairs with at least one pair of pos-
itive propositions. We denote the resulting model
asSUBENCODER . We include the details for our
distributed training setup and hyperparameters in
Appendix B.
4.2 Evaluation
To assess the utility of the sub-sentence encoders,
we evaluate our model on two types of downstream
tasks in zero-shot settings.
4.2.1 Atomic Fact Retrieval for Fine-Grained
Text Attribution
We first evaluate the sub-sentence encoders in re-
trieving fine-grained attributions (Rashkin et al.,
2023) for text. We conduct the evaluations with
thePROPSEGM ENTdataset (Chen et al., 2023b).
An overview of the task setup is shown in Figure 3.
Given an atomic proposition in the sentence, a sys-
tem is expected to identify and retrieve support-
ing evidence from a corpus of ˜45khuman-labeled
atomic propositions from 1.5kNews or Wikipedia
documents in total. The task setup emulates the
setting where each part of a sentence might have
different veracity, and so each atomic proposition in
a sentence might be attributed to different support-
ing evidence from different source documents. On
average, each query proposition has 1.13ground
truth supporting propositions.
Metrics. Given a system’s output rankings of the
45kcandidate evidence propositions with respect
to a query proposition, we measure the precision@ 1
plus recall@ {5,10,20}of the ranking against thehuman-annotated ground truth set of evidence
propositions.
Baselines. We compare pre-trained sentence en-
coders as baselines. We first evaluate variants of un-
supervised and supervised SimCSE, Sentence-T5,
and GTR on similar model parameter sizes. In addi-
tion, we compare two popular compact models, i.e.,
all-MiniLM-L6-v2 andall-distilroberta-v1
from sentence-transformers (Reimers and
Gurevych, 2019). We discuss the setup for sen-
tence encoders for the tasks in Appendix C.
Results. Table 1 summarizes our evaluation re-
sults. We observe that the SUBENCODER with dif-
ferent backbone encoders generally improve over
their sentence encoder counterparts. We see the
most visible improvements of SUBENCODER in
terms of Precision@1 and Recall@5, while the
performance gap becomes smaller in terms of Re-
call@10 and 20. This suggests that sub-sentence
contrastive learning gives the model better capa-
bilities at recognizing the nuanced semantic differ-
ences between propositions appearing in the same
context.
Across different variants of SUBENCODER with
different backbone sentence encoders, we ob-
serve similar performance levels overall, with the
GTR basevariant having a slight edge. In Table 1,
we mostly compare models with the same back-
bone encoder size and configurations. Our SUBEN-
CODER only introduces 0.5% extra parameters with
the MLP layer on top. We include a more com-
prehensive analysis of model size, efficiency, and
performance trade-off in §5.
4.2.2 Conditional Semantic Text Similarity
To assess SUBENCODER ’s ability to produce
contextual representations for fine-grained sub-
sentence level semantics of text, we conduct exper-
iments on the Condition Semantic Text Similarity
5

--- PAGE 6 ---
Type Sentence 1 Sentence 2 Condition Label Pred.
Correct. A group of people go sledding on a snowy
hill, and a dog chases one as he slides.A person, dressed in black, skipping
down a snow covered road and playing
with a black dog.The physical activ-
ity.4 3.44
Mistake: fails to find a
good set of words.A man being thrown into the air while
being trampled by a bull.The cowboy holds on to the bull who is
desperately tryingtothrow himoff.The person’s eleva-
tion.4 1
Mistake: correct set of
words; failed inference.Aman wear ingawhite tank topanda
white hard hat is holding two pieces of
pipe at a construc tionsite.Aconstruc tion worker in a lime-green
safety vest and orange hard hat is looking
closely at something held in his hands.The occupation of
the man.5 2.47
Table 2: Example outputs and typical mistakes of SUBENCODER on C-STS. The set of words identified by gpt-3.5
is highlighted yellow. For display purposes here, the model predicted cosine similarity is normalized to match the
human labels’ scale of 1 - 5, where 1 =Least similar, and 5 =Most similar
Model Setting Spearman r↑
Roberta base 0-shot (No. Cond.) -0.43∗
SimCSE base 0-shot (No. Cond.) 1.66∗
FlanT5 large0-shot -3.0∗
2-shot 11.7∗
GPT-3.50-shot 14.1
2-shot 15.4
GPT-40-shot 36.9
2-shot 40.7
GPT-3.5
+ SUBENCODER
(0-shot)(SimCSE base) 27.5
(GTR base) 31.9
(ST5 base) 33.0
GPT-4
+ SUBENCODER
(0-shot)(SimCSE base) 34.5
(GTR base) 36.9
(ST5 base) 37.2
Table 3: Spearman correlation coefficient ( ×100) of
model predictions evaluated in zero- or few-shot settings
on the Conditional Semantic Textual Similarity (C-STS)
task. * denotes results from Deshpande et al. (2023).
(C-STS) task (Deshpande et al., 2023). Compared
to STS (Agirre et al., 2012), C-STS introduces a
condition notion of similarity between text pairs,
where an additional natural language condition is
provided along with the text pair as input. A system
is expected to output a similarity score between the
pair from the perspective of the given condition.
Table 2 shows some examples of the task.
Method. Given the condition, we first prompt an
LLM to identify a set of words in each sentence that
best corresponds to the condition. Here, the LLM
only sees one sentence at a time, so the condition
words in each sentence are identified independently.
We use the sub-sentence encoder to encode the
set of words in the context of each sentence as
the conditional representation. We take the cosine
similarity between two encoded sets of words from
the text pair as their conditional similarity.Metrics. We compare the Spearman correlation
coefficient between predicted similarity from a sys-
tem against human ratings.
Baselines. We compare to a list of zero- and few-
shot baselines provided by (Deshpande et al., 2023).
This includes two bi-encoder models, Roberta base
and SimCSE basethat do not make use of the condi-
tion as input, as well as zero- and few-shot prompt-
ing results with FlanT5 large,GPT-3.5-turbo and
GPT-4 , where each LLM provided detailed instruc-
tions of the task, and is prompted to generate a
similarity score from 1 to 5 given the text pair and
condition with/without in-context demonstrations.
Results. Table 3 shows the evaluation results. By
having SUBENCODER comparing the contextual
similarity between the set of words selected by
gpt-3.5-turbo , we see an improvement in zero-
shot setting from Spearman’s r= 14.1→33.0,
compared to directly prompt LLM to output the
similarity. However, the performance gap when
using gpt-4 becomes much smaller ( r= 36.9→
37.2). This is reasonable considering the fact that
direct gpt-4 prompting demonstrates on-par per-
formance with supervised systems on C-STS, as
reported in Deshpande et al. (2023). We show ex-
amples of typical mistakes made by our model in
Table 2. We observe that our method typically
fails when (1) the LLM fails to identify a good
set of condition words, or when no such corre-
sponding words explicitly exist in the sentence, or
(2)SUBENCODER fails to correctly infer the sim-
ilarity between the two sets of condition words.
For instance, with the third example in Table 2,
the inference is particularly challenging for the
model, considering the relation between two sen-
tences is modeled only via a cosine similarity with
no learned parameters.
6

--- PAGE 7 ---
68707274Recall@5
69.6269.39 69.3770.7871.4372.15Batch Size vs. Performance 
 (Param. Size = 110M)
68707274
70.7872.9373.74Model Size vs. Performance 
 (Batch Size = 64)
816 32 64128 256
Batch size38404244Precision@1
37.5437.9138.2440.6241.44
40.97
110M 330M 3B
Parameter Size38404244
40.6242.6143.17Figure 4: The effect of varying batch size and model
parameter size on the atomic fact retrieval performance,
tested with the Sentence-T5 variant of S UBENCODER .
5 Analysis and Discussions
5.1 Scaling S UBENCODER
In Figure 4, we show an analysis of the effect of
scaling model sizes and batch sizes during training.
For our analysis, we use the Sentence-T5 variant
ofSUBENCODER , and evaluate the performance
on the atomic fact retrieval task.
Scaling Batch Size. As our contrastive learn-
ing objective leverages in-batch negative sampling,
scaling up the batch sizes during training could
bring performance gains. To illustrate this, we
initialize SUBENCODER with Sentence-T5 base en-
coder parameters and finetune with a varying batch
size of {8,16,32,64,128,256}. We observe that
increasing the batch size generally increases perfor-
mance, which suggests that batch size scaling could
yield better model generalizability. We observe a
significant performance gain when increasing batch
size from 32→64while seeing diminishing gains
with further increase. This echoes the empirical
findings with in-batch contrastive learning in gen-
eral (Khosla et al., 2020). The phenomena can
be attributed to the model-predicted labels in our
training dataset, which can be noisy.
Scaling Model Size. We initialize the encoder
with different sizes of Sentence-T5 from 110M to
3B parameters and finetune with a fixed batch size
of 64. We observe that starting from a larger pre-
trained encoder brings better performance. We see
a bigger gain when increasing the model size from
110M to 330M, while the gain becomes smallerModelSentence-Level Document-Level
P@1 R@5 P@1 R@5
GTR base 49.35 77.01 51.93 81.97
Sentence-T5 base 50.59 79.37 45.27 77.10
SUBENCODER (GTR) 42.94 82.27 45.04 90.13
SUBENCODER (ST5) 43.49 81.44 45.93 89.19
Table 4: Sentence and document retrieval performance
of the atomic fact retrieval task. We evaluate GTR base
and Sentence-T5 basevariants of S UBENCODER .
when we increase from 330M to 3B.
5.2 Using S UBENCODER for Sentence or
Document Retrieval
In §4, we compare SUBENCODER ’s performance
on the atomic fact retrieval task against the base-
line sentence encoders. In reality, a more likely
application scenario is when the system is expected
to retrieve supporting evidence on the sentence
or document level. To evaluate this, we cast the
atomic fact retrieval task as a sentence or document
retrieval task, where given a query proposition, a
system is expected to retrieve the set of sentences
or documents that contain the target proposition(s).
From the intuition that finer-grained retrieval,
e.g., with propositions, entails the more coarse
sentence- or document-level retrieval, we follow
Lee et al. (2021b) and use a simple strategy with
SUBENCODER for sentence- and document-level
retrieval. Given each query, we retrieve a slightly
larger number of propositions. From the set of
sentences and documents where the propositions
belong, we use the highest score among the set of
propositions as the score for each sentence or doc-
ument. The top kunique sentences or documents
are then returned as results.
Table 4 shows the the evaluation result. Com-
pared to GTR baseand Sentence-T5 base, which are
trained for document-level and sentence-level re-
trieval, respectively, we observe a similar level of
performance with retrieving by propositions with
SUBENCODER . Overall, we see lower top-1 accu-
racy compared to the baselines. This is possibly
due to the more complex nature of the proposition
retrieval task. However, we generally see an im-
provement in terms of recall @ 5. The findings
indicate the potential of using SUBENCODER for
multi-vector retrieval across different granularities.
7

--- PAGE 8 ---
Model Dim. Precision@1 Recall@5
SUBENCODER
(ST5-Large)1024 42.61 72.93
64 42.10 (-0.51) 70.17 (-2.76)
SUBENCODER
(ST5-Base)768 40.97 72.15
64 40.45 (-0.52) 71.62 (-0.53)
Table 5: The performance difference on the atomic
fact retrieval task with vs. without reducing the output
dimensionality of S UBENCODER .
5.3 Robustness to Input Formats/Boundaries
Although SUBENCODER is fine-tuned with data
formatted as propositions specifically, we observe
from the C-STS evaluations that the model gen-
eralizes to not necessarily proposition-shaped in-
puts, as shown in Table 2. In downstream appli-
cations, we would expect the model to general-
ize to input token masks with imperfect bound-
aries, e.g., propositions generated by a model in-
stead of labeled by humans. Alongside the C-
STS evaluation results, which indirectly support
our hypothesis, we conduct a simple evaluation
with the atomic fact retrieval task. Instead of
human-annotated queries, we use queries gener-
ated by gpt-3.5-turbo . The evaluation results
of the proposition segmentation performance of
gpt-3.5-turbo and the distilled T5-Large model
can be found in Appendix A. We observe that, with
a fuzzing token-level Jaccard-similarity-based met-
ric, most propositions extracted by the model can
be aligned with human-labeled propositions. When
we test the atomic fact retrieval performance on
the set of model-generated propositions that can be
fuzzy-matched with the human-annotated ones, we
only see a small drop in performance, e.g., with the
GTR-base variant of SUBENCODER , precision@1
drops from 41.21. →39.56, recall@5 drops from
73.14→72.23. We hypothesize that the robustness
of proposition boundaries partly comes from how
we train the model. As the labels between proposi-
tion pairs are generated independent of the model-
generated fuzzy proposition boundaries, the model
potentially learns to adapt to imperfect proposition
boundaries, similar to the intuition behind unsuper-
vised SimCSE training (Gao et al., 2021).
5.4 Offline Indexing and Compression
With the promising performance gain from SUBEN-
CODER in the atomic fact retrieval task, we dis-
cuss and assess the possibility of applying SUBEN-
CODER for fine-grained retrieval on larger-scale
corpora, which involves offline indexing andIndex Num. Entries Dim. Index Size
Propositions 270M 64 62GB
dpr-100w 21M 768 61GB
Table 6: The resulting index size with proposition-level
indexing with compressed dimension, compared to a
DPR index of 100-word blocks (Karpukhin et al., 2020).
caching the encoded corpora. In our case, the in-
dexing happens on the level of atomic propositions,
where we need to store one embedding for every
atomic proposition in the corpora. Compared to
document-level indexing, indexing on the proposi-
tion level would result in a prohibitively large index
size. In previous works (Lee et al., 2021b), this is
commonly addressed with techniques such as prod-
uct quantization (Jegou et al., 2010) to compress
index size or approximate nearest neighbor search
(Malkov and Yashunin, 2018) for faster inference.
Orthogonal to the two techniques above, we
study a simpler yet effective compression strat-
egy by reducing the output dimension of SUBEN-
CODER . In the context of sentence encoders, Wang
et al. (2023) discover that reducing the output di-
mensionality during training generally incurs mini-
mal downstream performance loss. Following this
idea, we finetune the Sentence T5 base and large
variants of SUBENCODER with a bottlenecked out-
put dimension of 64 instead of the original output
dimensions of 1024 and 768, respectively. Table 5
shows the performance comparison when evaluated
on the atomic fact retrieval task. Overall, we ob-
serve a very small performance drop while gaining
12×to 16×reduction in output embedding size.
To demonstrate the implication of this in prac-
tice, we use the Sentence T5 large variant of
SUBENCODER to encode and index an English
Wikipedia dump from 2021/10/13, as used by
Bohnet et al. (2022). We segment all sentences
in Wikipedia into propositions with the T5-large
model (§3.3). This results in ˜270M propositions
from 5.3M Wikipedia pages. Table 6 shows the
resulting index. The resulting size of 62GB is
close to a prebuilt (uncompressed) dense passage
retrieval (DPR) index on the level of 100-word
blocks (Karpukhin et al., 2020). We see that de-
creasing the output dimension of the embeddings
helps in reducing the cached index size. It is worth
noting that compared to the document-level index,
we still expect the query speed of the index to in-
crease slightly due to the increase in the number of
entries. However, in practice, we observe a reason-
8

--- PAGE 9 ---
able overall time and space complexity involved in
offline indexing and online similarity querying on
the proposition-level.
6 Conclusion
We introduce sub-sentence encoders, a contrastive
learning framework for learning contextual embed-
dings for semantic units on the sub-sentence level.
Beyond the use cases covered in the paper, the
sub-sentence encoder architecture could potentially
serve as the backbone for any cross-document infor-
mation linking tasks in context, and the learning ob-
jectives could potentially apply to a broader range
of tasks with various granularity of information,
e.g., linking sentences or spans within different
documents (Ma et al., 2023). We hope that the find-
ings in this paper will facilitate further exploration
along these directions.
Limitations
This work mostly serves as exploratory work to val-
idate the idea behind sub-sentence encoder archi-
tecture and learning objectives. We acknowledge
the limited scale of our experiments, specifically
in terms of the languages supported by the model .
In our experiments, we explore the idea of sub-
sentence encoder with English text only. However,
the techniques described in the paper for sampling
training data and training the sub-sentence encoder
can be applied to other languages as well. We leave
exploration on multilingual sub-sentence encoder
for future work.
Acknowledgements
The authors would like to thank Alex Fabrikant,
Jianmo Ni, and Tal Schuster for the discussions
leading to the development of this idea. The authors
thank Xinran Zhao, Kaixin Ma, Vivek Gupta, and
Xiaodong Yu for valuable feedback on the project
and the paper presentation.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics – Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval 2012) , pages 385–
393, Montréal, Canada. Association for Computa-
tional Linguistics.Reinald Kim Amplayo, Peter J Liu, Yao Zhao, and
Shashi Narayan. 2022. Smart: Sentences as basic
units for text evaluation. In The Eleventh Interna-
tional Conference on Learning Representations .
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python: analyzing text
with the natural language toolkit . " O’Reilly Media,
Inc.".
Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,
Daniel Andor, Livio Baldini Soares, Jacob Eisenstein,
Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al.
2022. Attributed question answering: Evaluation and
modeling for attributed large language models. arXiv
preprint arXiv:2212.08037 .
Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
Säckinger, and Roopak Shah. 1993. Signature verifi-
cation using a" siamese" time delay neural network.
Advances in neural information processing systems ,
6.
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St. John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
Brian Strope, and Ray Kurzweil. 2018. Universal
sentence encoder for English. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations ,
pages 169–174, Brussels, Belgium. Association for
Computational Linguistics.
Jifan Chen, Grace Kim, Aniruddh Sriram, Greg Durrett,
and Eunsol Choi. 2023a. Complex claim verification
with evidence retrieved in the wild. arXiv preprint
arXiv:2305.11859 .
Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan
Roth, and Tal Schuster. 2023b. PropSegmEnt: A
large-scale corpus for proposition-level segmentation
and entailment recognition. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 8874–8893, Toronto, Canada. Association for
Computational Linguistics.
Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris
Callison-Burch, and Dan Roth. 2019. Seeing things
from a different angle:discovering diverse perspec-
tives about claims. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers) , pages 542–557, Minneapolis, Minnesota. As-
sociation for Computational Linguistics.
Eunsol Choi, Jennimaria Palomaki, Matthew Lamm,
Tom Kwiatkowski, Dipanjan Das, and Michael
Collins. 2021. Decontextualization: Making sen-
tences stand-alone. Transactions of the Association
for Computational Linguistics , 9:447–461.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
9

--- PAGE 10 ---
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing , pages 670–680, Copen-
hagen, Denmark. Association for Computational Lin-
guistics.
Ameet Deshpande, Carlos E Jimenez, Howard Chen,
Vishvak Murahari, Victoria Graf, Tanmay Rajpuro-
hit, Ashwin Kalyan, Danqi Chen, and Karthik
Narasimhan. 2023. CSTS: Conditional Semantic
Textual Similarity. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing .
William Falcon and The PyTorch Lightning team. 2019.
PyTorch Lightning.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent
Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
Kelvin Guu. 2023a. RARR: Researching and revis-
ing what language models say, using language mod-
els. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers) , pages 16477–16508, Toronto, Canada.
Association for Computational Linguistics.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.
2023b. Enabling large language models to generate
text with citations. arXiv preprint arXiv:2305.14627 .
Herve Jegou, Matthijs Douze, and Cordelia Schmid.
2010. Product quantization for nearest neighbor
search. IEEE transactions on pattern analysis and
machine intelligence , 33(1):117–128.
Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and
Greg Durrett. 2023. Wice: Real-world entailment for
claims in wikipedia. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing .
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6769–6781,
Online. Association for Computational Linguistics.
Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval , pages 39–
48.Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron
Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-
pervised Contrastive Learning. Advances in neural
information processing systems , 33:18661–18673.
Harold W Kuhn. 1955. The Hungarian method for
the assignment problem. Naval research logistics
quarterly , 2(1-2):83–97.
Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky-
omin Jung. 2021a. Learning to select question-
relevant relations for visual question answering. In
Proceedings of the Third Workshop on Multimodal
Artificial Intelligence , pages 87–96, Mexico City,
Mexico. Association for Computational Linguistics.
Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b.
Phrase retrieval learns passage retrieval, too. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 3661–
3672, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Nelson F Liu, Tianyi Zhang, and Percy Liang. 2023.
Evaluating verifiability in generative search engines.
arXiv preprint arXiv:2304.09848 .
Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric
Nyberg, and Jianfeng Gao. 2023. Chain-of-skills:
A configurable model for open-domain question an-
swering. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 1599–1618, Toronto,
Canada. Association for Computational Linguistics.
Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth
Sieber, Mark Yatskar, and Dan Roth. 2023. Ex-
pertqa: Expert-curated questions and attributed an-
swers. arXiv preprint arXiv:2309.07852 .
Yu A Malkov and Dmitry A Yashunin. 2018. Efficient
and robust approximate nearest neighbor search us-
ing hierarchical navigable small world graphs. IEEE
transactions on pattern analysis and machine intelli-
gence , 42(4):824–836.
Jeremiah Milbauer, Ziqi Ding, Zhijin Wu, and Tong-
shuang Wu. 2023a. From nuisance to news sense:
Augmenting the news with cross-document evidence
and context. arXiv preprint arXiv:2310.04592 .
Jeremiah Milbauer, Annie Louis, Mohammad Javad
Hosseini, Alex Fabrikant, Donald Metzler, and Tal
Schuster. 2023b. LAIT: Efficient multi-segment en-
coding in transformers with layer-adjustable interac-
tion. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 10251–10269, Toronto,
Canada. Association for Computational Linguistics.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
FActScore: Fine-grained atomic evaluation of factual
10

--- PAGE 11 ---
precision in long form text generation. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing .
John X Morris, V olodymyr Kuleshov, Vitaly Shmatikov,
and Alexander M Rush. 2023. Text embeddings
reveal (almost) as much as text. arXiv preprint
arXiv:2310.06816 .
Jianmo Ni, Gustavo Hernandez Abrego, Noah Con-
stant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang.
2022a. Sentence-t5: Scalable sentence encoders
from pre-trained text-to-text models. In Findings of
the Association for Computational Linguistics: ACL
2022 , pages 1864–1874, Dublin, Ireland. Association
for Computational Linguistics.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-
nandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith
Hall, Ming-Wei Chang, and Yinfei Yang. 2022b.
Large dual encoders are generalizable retrievers. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
9844–9855, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,
Jason Weston, and Douwe Kiela. 2020. Adversarial
NLI: A new benchmark for natural language under-
standing. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 4885–4901, Online. Association for Computa-
tional Linguistics.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style,
high-performance deep learning library. Advances in
neural information processing systems , 32.
Guanghui Qin and Benjamin Van Durme. 2023. Nugget:
Neural agglomerative embeddings of text. In Pro-
ceedings of the 40th International Conference on
Machine Learning , ICML’23. JMLR.org.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1–67.
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm,
Lora Aroyo, Michael Collins, Dipanjan Das, Slav
Petrov, Gaurav Singh Tomar, Iulia Turc, and David
Reitter. 2023. Measuring attribution in natural lan-
guage generation models. Computational Linguistics ,
pages 1–64.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Rachel Rudinger, Kevin Duh, and Benjamin Van Durme.
2017. Skip-prop: Representing sentences with one
vector per proposition. In IWCS 2017 — 12th Inter-
national Conference on Computational Semantics —
Short papers .
Tal Schuster, Sihao Chen, Senaka Buthpitiya, Alex
Fabrikant, and Donald Metzler. 2022. Stretching
sentence-pair nli models to reason over long docu-
ments and clusters. In Findings of the Association
for Computational Linguistics: EMNLP 2022 , pages
394–412.
Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur
Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.
Real-time open-domain question answering with
dense-sparse phrase index. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics , pages 4430–4441, Florence, Italy.
Association for Computational Linguistics.
Kihyuk Sohn. 2016. Improved deep metric learning
with multi-class n-pair loss objective. Advances in
neural information processing systems , 29.
Hongwei Wang and Dong Yu. 2023. Going beyond
sentence embeddings: A token-level matching algo-
rithm for calculating semantic textual similarity. In
Proceedings of the 61st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 563–570.
Hongwei Wang, Hongming Zhang, and Dong Yu.
2023. On the dimensionality of sentence embeddings.
arXiv preprint arXiv:2310.15285 .
Rowan Zellers, Ari Holtzman, Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. 2019. Defending against neural fake
news. NeurIPS .
Ben Zhou, Kyle Richardson, Xiaodong Yu, and Dan
Roth. 2022. Learning to decompose: Hypothetical
question decomposition based on comparable texts.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
2223–2235, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
11

--- PAGE 12 ---
A Proposition Segmentation
In this section, we provide details on the few-shot
prompt and distilled T5-large model we use for seg-
menting sentences into propositions. We provide
evaluations the two methods against PROPSEG-
MENT(Chen et al., 2023b).
A.1 Prompt for Proposition Segmentation
We use the following prompt with gpt-3.5-turbo
to generate the initial set of seed training data
for segmenting sentences into propositions. We
provide one example from PROPSEGM ENTfor
in-context learning demonstration. We process
˜23,000sentence pairs with the prompt, which gen-
erates a total of 44,970 sentences with propositions,
after filtering out malformed and empty genera-
tions.
Prompt for sentence ⇒propositions
Given the following sentence, tell me what claims
they are making. Please split the sentence as much as
possible, but do not include information not in the
sentence.
Sentence : The Andy Warhol Museum in his
hometown, Pittsburgh, Pennsylvania, contains an
extensive permanent collection of art.
Claims :
1. The Andy Warhol Museum is in Pittsburgh.
2. Andy Warhol’s hometown is in Pittsburgh.
3. Pittsburgh is in Pennsylvania.
4.The Andy Warhol Museum contains an exten-
sive permanent collection of art.
Sentence : (input sentence )
Claims :
A.2 Training detail of T5 for proposition
segmentation
We finetune a T5-large (Raffel et al., 2020) model
on a seed set of training data generated via
GPT-3.5-turbo . We use an AdamW optimizer
with a constant learning rate of 1e−4, with a batch
size of 128. We train the model for 3 epochs on 8x
Nvidia A6000s, which takes 2 hours to finish.
A.3 Converting propositions from natural
language to token masks
Given a proposition of a sentence in the natural
language form, we convert and align it to a sub-
set of tokens from the original sentence with thefollowing steps. We first tokenize and lemmatize
each of the tokens in the proposition using NLTK
(Bird et al., 2009). Next, we construct an affin-
ity matrix between the set of lemmatized tokens
from the proposition and the sentence. With the
matrix, we assign tokens with identical lemmas
are assigned a similarity score of 1. To break ties
between multiple token matches, we apply a 2D-
convolution filter on the affinity matrix, which adds
a small score offset for other token matches in a
context window of three tokens. With the affinity
matrix, we find the optimal alignment between the
proposition and sentence tokens with max bipartite
matching on the affinity matrix with the Hungarian
algorithm (Kuhn, 1955).
A.4 Proposition Segmentation Evaluation on
PROPSEGM ENT
To evaluate the quality of propositions extracted via
our pipeline, we evaluate the proposition segmenta-
tion performance on PROPSEGM ENT. The results
are shown in Table 7. For details of the Jaccard
similarity based evaluation metrics for proposition
segmentation, please refer to Chen et al. (2023b).
B Training and Hyperparameters
We implement the sub-sentence encoder archi-
tecture with pytorch (Paszke et al., 2019) and
pytorch-lightning (Falcon and The PyTorch
Lightning team, 2019). All of our sub-encoder
model variants are trained on 8 ×Nvidia A6000
GPUs with 48GB VRAM.
Distributed Training Since we adopt in-batch
contrastive loss, we scale up the number of nega-
tive examples by increasing the batch size with dis-
tributed training across GPUs. We distribute train-
ing processes across GPU nodes via Distributed
Data Parallel (DDP). Specifically, given a mini-
batch of Ngpu×Msentences, each GPU gets M
sentences, which gets forwarded through model
parameters on the GPU. Next, we gather and copy
all the encoded propositions along with gradients
to each of the GPUs, so that each GPU has the full
minibatch for loss computation. Each GPU process
backpropagates the loss independently on its copy
of the model parameters.
Hyperparameters For all experiments, we use
the temperature parameter τ= 0.01for the super-
vised contrastive loss, with AdamW optimizer. For
Sentence-T5 and GTR variants of the model, we
12

--- PAGE 13 ---
ModelJaccard θ= 0.8 Jaccard θ= 0.5
Precision Recall F1 Precision Recall F1
Systems used in this paper
GPT-3.5-turbo 35.79 31.65 33.60 71.52 63.87 67.48
T5-Large (w/ GPT3.5 training data) 35.91 31.70 33.68 70.27 63.39 66.65
Systems fine-tuned on PROPSEGM ENT(Chen et al., 2023b)
BERT-Large 34.97 33 .42 34 .17 67 .42 64 .17 65 .75
T5-Large 55.95 55 .05 55 .50 78 .03 76 .74 77 .38
Table 7: Sentence segmentation performance of systems used in this paper when evaluated in zero-shot settings
onPROPSEGM ENT. We include the performance of models trained on PROPSEGM ENTreported by Chen et al.
(2023b) as a reference.
use learning rate of 1 e−4. For SimCSE, we use
learning rate of 5e−5. We train the models for 10
epochs, and a linear decay is applied at the end of
each epoch, which decreases the learning to 0 after
10 epochs. We select the best checkpoint based on
validation loss after each epoch.
C Evaluation Setup
C.1 Representing Atomic Propostion with
Sentence Encoder
With the atomic fact retrieval evaluation on
PROPSEGM ENT, since the ground truth query and
target propositions are both represented in the for-
mat of token masks, we experiment with a few
different strategies of formatting the input for sen-
tence encoders. Specifically, with respect to the
input sentence and the token masks denoting the
proposition, here are the different strategies in con-
sideration.
1.Mask pooling only. Encoder has full atten-
tion, apply proposition mask during pooling.
Note that this is the same method we use for
the sub-sentence encoder.
2.Full mask. Apply proposition mask as atten-
tion mask during both encoding and pooling.
3.Token subset only. Take the subset of tokens
and discard the rest. Feed only the subset
of tokens as a sequence to the encoder and
pooling layer.
When tested on a small validation set for the atomic
fact retrieval task, we generally observe that mask
pooling only yields the best result across most
models, except for the two compact models, i.e.
MiniLM-L6-v2 and DistilRoberta. On the two com-
pact models, we see full mask outperforming the
mask pooling only strategy by a small margin. Weobserve that with the third strategy token subset
only, the validation performance trails behind the
other two across all models.
13
