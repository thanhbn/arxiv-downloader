# 2407.08818.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/tokenizer/2407.08818.pdf
# File size: 1532941 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MAGNET
 : ImprovingtheMultilingualFairness
of Language Models
with Adaptive Gradient-Based Tokenization
Orevaoghene Ahia1Sachin Kumar2,3Hila Gonen1Valentin Hofmann2
Tomasz Limisiewicz4Yulia Tsvetkov1Noah A. Smith1,2
1University of Washington2Allen Institute for AI3The Ohio State University
4Charles University
oahia@cs.washington.edu
Abstract
In multilingual settings, non-Latin scripts and low-resource languages are usually
disadvantaged in terms of language models‚Äô utility, efficiency, and cost. Specifi-
cally, previous studies have reported multiple modeling biases that the current to-
kenizationalgorithmsintroducetonon-Latinscriptlanguages,themainonebeing
over-segmentation. In this work, we propose MAGNET ‚Äîmultilingual adaptive
gradient-basedtokenization‚Äîto reduce over-segmentation via adaptive gradient-
basedsubwordtokenization. MAGNET learnstopredictsegmentboundariesbetween
bytetokensinasequenceviasub-moduleswithinthemodel,whichactasinternal
boundary predictors (tokenizers). Previous gradient-based tokenization methods
aimedforuniformcompressionacrosssequencesbyintegratingasingleboundary
predictor during training and optimizing it end-to-end through stochastic reparame-
terization alongside the next token prediction objective. However, this approach still
resultsinover-segmentationfornon-Latinscriptlanguagesinmultilingualsettings.
In contrast, MAGNET offers a customizable architecture where byte-level sequences
are routed through language-script-specific predictors, each optimized for its respec-
tive language script. This modularity enforces equitable segmentation granularity
across different language scripts compared to previous methods. Through extensive
experiments,wedemonstratethatinadditiontoreducingsegmentationdisparities,
MAGNET alsoenables fasterlanguage modellingandimprovesdownstreamutility.
1 Introduction
Despitetheproliferationofgenerativelanguagemodels(LMs)inEnglish,theirnon-Englishcounterparts
arefarfrombeingwidelyadopted. WhilemultilingualLMsofferseveraladvantagessuchasresource
efficiency and cross-lingual generalization, the performance disparities across languages remain a
significant challenge. Previous work has largely attributed these disparities to training data imbalances
acrosslanguages[ 43,34,29,24]. Recentwork,however,highlightsthat tokenization ‚Äîthewayinput
textissegmented‚Äîcanconsiderablydegradenotonlymodelperformancebutalsotrainingandinference
costsonaccountofoverlyfragmentingcertainlanguagesandscripts[ 3,33]. Subwordsegmentation
algorithmsusedtobuildLMtokenizers[ 28,39,22,40]typicallysegmentthetrainingcorpusrelying
onfrequencystatisticsalone. Duetodataimbalances,theyobtainhighcompressioninhigh-resource
languages, while majority of languages are over-fragmented. This issue disproportionately affects
non-Latinscriptscoveringlanguagesspokenbybillionsofpeople,whicharenotonlylessfrequentin
such corpora, but can require up to 4more bytes to represent the same information.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).

--- PAGE 2 ---
Monolingual SequenceTransformer  BlockBoundary predictorTransformer BlockMAGNETPrior WorkENGLISH:AerosmithhavecancelledtheirremainingconcertsontheirtourBPEAerosmithhavecancelledtheirremainingconcertsontheirtourDTPAerosmithhavecancelledtheirremainingconcertsontheirtourMAGNETAerosmithhavecancelledtheirremainingconcertsontheirtourRUSSIAN:–ì—Ä—É–ø–ø–∞Aerosmith–æ—Ç–º–µ–Ω–∏–ª–∞–æ—Å—Ç–∞–≤—à–∏–µ—Å—è–∫–æ–Ω—Ü–µ—Ä—Ç—ã–≤—Å–≤–æ–µ–º—Ç—É—Ä–Ω–µBPE–ì7—Ä—É–ø–ø–∞Aerosmith–æ—Ç–º–µ–Ω–∏–ª–∞–æ—Å—Ç–∞–≤—à–∏–µ—Å—è–∫–æ–Ω—Ü–µ—Ä—Ç—ã–≤—Å–≤–æ–µ–º—Ç—É—Ä—Ç–Ω–µ—ãDTP–ì—Ä—É–ø–ø–∞ Aerosmith–æ—Ç–º–µ–Ω–∏–ª–∞–æ—Å—Ç–∞–≤—à–∏–µ—Å—è–∫–æ–Ω—Ü–µ—Ä—Ç—ã–≤—Å–≤–æ–µ–º—Ç—É—Ä–Ω–µMAGNET–ì—Ä—É–ø–ø–∞Aerosmith–æ—Ç–º–µ–Ω–∏–ª–∞–æ—Å—Ç–∞–≤—à–∏–µ—Å—è–∫–æ–Ω—Ü–µ—Ä—Ç—ã–≤ —Å–≤–æ–µ–º—Ç—É—Ä–Ω–µTELUGU:‡∞è‡∞∞‡±ã‡∞∏‡∞ø‡∞Æ‡±ç‡∞§‡±ç‡∞µ‡∞æ‡∞∞‡∞ø‡∞™‡∞∞‡∞Ø‡±ç‡∞ü‡∞®‡∞≤‡±ã‡∞Æ‡∞ø‡∞ó‡∞ø‡∞≤‡∞ø‡∞®‡∞∏‡∞Ç‡∞ó‡±Ä‡∞§‡∞ï‡∞ö‡±á‡∞∞‡±Ä‡∞≤‡∞®‡±Å‡∞∞‡∞¶‡±Å‡∞¶‡±ç‡∞ö‡±á‡∞∏‡∞ø‡∞Ç‡∞¶‡∞øBPE‡∞è‡∞∞‡±ã‡∞∏‡±ç‡∞Æ‡∞ø‡∞§‡±ç‡∞µ‡∞æ‡∞∞‡∞™‡∞∞‡±ç‡∞Ø‡∞ü‡∞®‡∞≤‡∞≤‡±ã‡∞Æ‡∞ø‡∞ó‡∞ø‡∞≤‡∞ø‡∞®‡∞∏‡∞Ç‡∞ó‡±Ä‡∞§‡∞ï‡∞ö‡±á‡∞∞‡±Ä‡∞≤‡∞®‡±Å‡∞∞‡∞¶‡±ç‡∞¶‡±Å‡∞ö‡±á‡∞∏‡∞ø‡∞Ç‡∞¶‡∞øDTP‡∞è‡∞∞‡±ã‡∞∏‡±ç‡∞Æ‡∞ø‡∞§‡±ç‡∞µ‡∞æ‡∞∞‡∞ø‡∞™‡∞∞‡±ç‡∞Ø‡∞ü‡∞®‡∞≤‡±ã‡∞Æ‡∞ø‡∞ó‡∞ø‡∞≤‡∞ø‡∞∏‡∞Ç‡∞ó‡±Ä‡∞§‡∞ï‡∞ö‡±á‡∞∞‡±Ä‡∞≤‡∞®‡±Å‡∞∞‡∞¶‡∞¶‡±ç‡±Å‡∞≤‡∞ö‡±á‡∞∏‡∞ø‡∞Ç‡∞¶‡∞øMAGNET‡∞è‡∞∞‡±ã‡∞∏‡±ç‡∞Æ‡∞ø‡∞§‡±ç‡∞µ‡∞æ‡∞∞‡∞ø‡∞™‡∞∞‡∞Ø‡±ç‡∞ü‡∞®‡∞≤‡±ã‡∞Æ‡∞ø‡∞ó‡∞ø‡∞≤‡∞ø‡∞®‡∞∏‡∞Ç‡∞ó‡±Ä‡∞§‡∞ï‡∞ö‡±á‡∞∞‡±Ä‡∞≤‡∞®‡±Å‡∞∞‡∞¶‡±Å‡∞¶‡±ç‡∞ö‡±á‡∞∏‡∞ø‡∞Ç‡∞¶‡∞øTransformer BlockTransformer BlockTransformer  BlockLatin predictorCyrl predictorIndic predictorTransformer BlockLatin sequenceCyrl sequenceIndic sequenceFigure 1: MAGNET routes byte-level sequences via language-script specific boundary predictors.
These predictors infer boundaries leading to equitable segmentation across languages. Prior work infers
boundaries with a single predictor across languages and leads to over-segmentation.
To address these challenges, prior work has instead proposed to build tokenizer-free models by directly
trainingoncharacterorbytesequences[ 45,4]. Operatingonsmallerorfiner-grainedsegmentsleads
to significantly higher compute- and memory-intense modeling, caused by much longer sequences.
To alleviate this issue, recent work introduced a tokenization ‚Äúlayer‚Äù within the model architecture
by pooling fixed or dynamic length contiguous character representations into smaller sets of patch
representations [ 30,9,41,31,14,15], resulting in models optimized with a sequence ‚Äúcompression
rate‚Äù inmind. While these can improve efficiency, theyare mostlysuited for character-level modeling
for scripts whose characters can be mapped to single Unicode codepoint. Due to the extensive number
of codepoints in Unicode, character-based vocabularies can be extremely large for multilingual models.
Sincemanyofthosecharactersmayneverappearduringtraining,‚Äúout-of-vocabulary‚Äùissuessimilar
to those experienced with subword models may arise if we only included codepoints in the training
data [28].1When extending these methods to training byte-level multilingual models, we observe that
thedisparitiesinfragmentationratesacrosslanguagespersist. Forinstance,theEnglishtext‚ÄúFellow
wrestlers also paid tribute to Luna.‚Äù and its Telugu equivalent, ‚Äú ‡∞§‡±ã‡∞ü‡∞ø ‡∞Æ ‡∞≤ÔøΩ‡∞Ø‡±Å ‡∞¶ÔøΩ‡∞ï‡∞æ‡∞∞‡±Å‡∞≤‡±Å ‡∞ï‡±Ç‡∞°‡∞æ ‡∞≤‡±Ç‡∞®‡∞æ‡∞ï‡±Å
‡∞®‡∞ø‡∞µ‡∞æ‡∞≥‡±Å‡∞≤‡±Å ‡∞Ö ‡∞∞‡∞øÔøΩ‡∞Ç ‡∞ö‡∞æ‡∞∞‡±Å.‚Äù, contain 43 and 148 UTF-8 bytes, respectively. A fixed compression ratio for both
languages will result in the Telugu text getting over fragmented with requiring close to 3 more tokens
than English.
In this work, we propose MAGNET (multilingual adaptivegradient-basedtokenization) to reduce this
disparityintokenizer-freemultilingualLMs. Ourgoalistoobtainend-to-endmultilinguallanguage
modelingwithgradient-basedsubwordtokenizationthatresultsinhighandsimilarsequencecompression
acrosslanguages withvaryingscripts. Weleverage hourglasstransformers[30, 31] to efficientlyroute
byte-level sequences through language-script specific internal boundary predictors trained to infer word
boundariesbetweenbytetokensinasequence. Theseboundarypredictorsaretrainedend-to-endthrough
stochasticreparameterisation[ 20,27]. Theinferredboundariesarethenusedtopoolrepresentations
of contiguous tokens in the same segment, after which the pooled (narrow) representation is passed
into the rest of the transformer block. Unlike previous gradient-based tokenization approaches that
apply the same compression rate to all languages in the pretraining data by incorporating a single
boundarypredictorwithintheirmodelarchitectures, MAGNET employsmodularity. Itincorporates
multiplelanguage-scriptspecificpredictorstoachieveequitablesegmentationgranularityacrossdifferent
language scripts. We test the effectiveness of MAGNET on equitable fragmentation, model efficiency,
anddownstreamtaskperformanceacrossninetypologicallydifferentlanguages,comparingtobyte-level
models without compression and existing gradient-based tokenization models [ 31]. Our extensive
experiments demonstrate that our approach MAGNET results in more equitable tokenization when
comparedtosubword-tokenizers,byte-leveltokenizersandpreviousgradient-basedtokenizationmodels.
This in turn leads to faster modelling and competitive performance across downstream-tasks.2
1Anotherdisparitywithcharacter-leveltokenizersisthatChinese-Japanese-Koreanscriptsusea highnumber
of Unicode codepoints.
2Code and data are publicly available at https://github.com/orevaahia/
magnet-tokenization
2

--- PAGE 3 ---
2 MAGNET: Multilingual Adaptive GradieNt-basEd Tokenization
Our goal is to build a multilingual byte-level language model with equitable segmentation across
languages. We propose to achieve this by dedicating a separate module within the model for each
writing script, to serve as an internal tokenizer for languages that use that script. Our proposed model,
calledMAGNET , builds on hourglass transformers [30,31], an architecture that was introduced to
efficiently handle long sequences in tokenizer-free models. We make several simple but important
modifications to this architecture in order to obtain equitable segmentation across languages, while
maintaining a high quality of multilingual modeling. In what follows, we explain the main concepts
ofhourglasstransformers,andthenintroducethemodificationswemaketoaccommodateequitable
multilingual modeling.
2.1 Background: HourglassTransformers
The hourglass transformer [ 31,30] is a hierarchical architecture for efficiently handling long sequences.
The architecture has three main components, each consisting of one or more transformer layers: A
tokenizationsubmodule whichtakesas inputa bytesequenceandoutputsa segmentation, a language
modelingsubmodule thattakesasinputthepredictedsegmentsortokensandisthentrainedtoperform
next token prediction, and an upsampling module that takes as input the hidden representations of the
segmentationsandconvertsthembacktoabytesequenceonwhichatypicallanguagemodelingloss
canbeapplied. Consideringthismodelasablackbox,itstillperformsbyte-levellanguagemodeling,
however, it requires significantly less compute thanks to the tokenization submodule.
Gradient-based Tokenization This submodule performs two steps. First, the given input sequence
x1; : : : ; x N(whereeach xtisabyteinourcase)isencodedusingasmalltransformernetwork(with
causal attention) to produce a sequence of hidden vectors hT
1; : : : ; hT
N. Next, a boundary predictor
takes as input each htand predicts a scalar valuebetween 0and1, indicating the probability of position
tto be the end of a segment. It isimplementedas
^bt=p(bt= 1) = (MLP (ht)); (1)
where MLP indicates a multi-layer perceptron and is the sigmoid function. To convert the soft
probabilities to hard segment predictions, a Bernoulli distribution is sampled from, defined by ^bt. Since
the sampling operation will make the process non-differentiable, hard Gumbel-sigmoid is used, a
stochastic reparameterization of the Bernoulli distribution, following Nawrot et al. [31]:
bt=sigmoid"
log^btu
(1 ^bt)(1 u)1
#
; u Uniform (0;1) (2)
where is a hyper-parameter. Since this module is differentiable, the segmentations are learned during
training of the full model.3This module is referred to as ‚Äúgradient-based tokenization.‚Äù
Language Modeling Given a sequence of segment boundaries bt2 f0;1gfrom the boundary
predictor, this submodule first pools the hidden states belonging to the same segment by averaging
them to form a sequence of representations hP
1; : : : ; hP
k.4Lett1; : : : ; t kindicate the positions at
which a boundary is sampled, i.e., for any contiguous pair tj; tj+1, the sequence xtj+1: : : x tj+1
forms a ‚Äútoken‚Äù ending at position tj+1.5The input representation of this ‚Äútoken‚Äù is defined as
hPj=1
tj+1 tjPtj+1
t=tj+1hTt. These representations are then passed through the middle block
of transformer layers (with causal attention) to obtain another sequence of hidden representations
hM
1; : : : ; hM
k. From theperspective ofa subword-tokenizatonbased languagemodel, thismodule is
equivalent to the transformer blocks without the input and output embedding layers.
Upsampling Thismoduleconverts hM
ltoprobabilitiesoverabytevocabulary. Thisinvolves,first,
upsamplingtheoutputofthemiddleblocktotheoriginalresolutionbyduplicationfollowedbyskip
3Nawrot et al. [31]also explore learning the segmentations using supervision from predefined word or subword
boundaries. However,itisnota viablesolutionforalllanguagesanddoesnotresolvetheunfairnessissues.
4P,M, andTdenote representations in the middle transformer block, after pooling and at the token level.
5Thefirsttokenisdefinedas x0: : : x t1andthe last token as xtk+1: : : x N.
3

--- PAGE 4 ---
connection: hU
t=hMl
t k+1
km+hT
t.6These vectors are further passed through a small transformer
networkfollowedbyanunembeddinglayerandasoftmaxtogetaprobabilitydistributionoverwhich
language modeling loss (cross entropy) can be computed. To prevent the boundary predictor from
collapsingandtriviallypredictingeachposition tasaboundary,Nawrotetal. [31]proposeaddinga
regularizer to the LM objective:  logBinomial (;l; k)where,
Binomial (;N; k ) =N
k
k(1 )N k;and k=X
Nbt: (3)
Here 2[0;1]is a hyperparameter, kdefines the number of predicted segments or tokens. Intuitively,
this loss nudges the model to find a kclose to lwhich is the mode of the Binomial distribution. In
other words, allows to control the compression rate of the input sequence to approximately1
.
Setting it as 1will lead to every position being predicted as boundary whereas setting it to 0will cause
no boundaries to be predicted.
2.2 Adaptive Gradient-Based Tokenization via Multiple Boundary Predictors
To encode the same information, different languages requiredifferent number of bytes, owing to their
differentinherentefficiencies[ 3,25,33]aswellasrestrictionsimposedbyUnicodemappings,where
non-Latin languages (e.g., Indian languages) may require up to 4 bytes per character.78In multilingual
models, setting the same compression rate (via ) for all languages will lead to text in some languages
getting segmented into much longer sequences,9see Equation (3). This disparity contributes to higher
computeandmemorycostsforsuchlanguagesaswellaspoorermodelperformancefordownstream
tasks[3]. Thisissueparallelssubwordtokenizerswherelanguageswithhighersegmentationratesget
disadvantagedduetolongercontext-lengthrequirementstoperformthesametaskandpoorerin-context
learningperformancesincethesamecontextlengthfitsfewertraininginstancesthanotherlanguages
leading to unfairness [5, 3, 25, 33].
To make tokenization more equitable, we propose MAGNET , which efficiently learns to segment
sequences across languages and language scripts with similar granularity. As part of creating equitable
segmentation, we aim to efficiently maximize sequence compression, without having a negative impact
on downstream performance across languages.
Introducing multiple gradient-based tokenizers To achieve this, we propose a modification to the
model architecture that enables the processing of multiple language scripts. Each script has its own
boundarypredictortrainedwithdistinctBinomialpriors determinedbasedonthescripts‚ÄôUnicode
encodingandalsotailoredtoadesiredcompressionrate. Thisallowsustoachievesimilarfragmentation
rates across languages, due to variations in compression. The input sequence is tagged with its script10
and we infer the segmentation by routing it through the appropriate boundary predictor. The remainder
of the model architecture remains the same.
Determining forequitabletokenizaton Weusethebinomialpriors foreachboundarypredictor
to control the rates of the resulting segmentations for the different scripts. Since we want to impose
equitable lengthsacross languages,wesetthe different according to the followingprocess. First,we
choosean anchor language Lforeachscript in our trainingcorpusanddefine a quantity byte-to-word
ratio Rfor thisscript as follows. Let X=fx1; : : : ;xDgbe a sample of text sequences inlanguage L
fromourtrainingcorpuswith jxijdenotingthebyte-lengthand countwords(xi)thenumberofwords11
in sequence xi. Wedefinetheaveragebyte-to-wordratio RoverXas:
R=1
DPD
i=1jxij
countwords(xi)(4)
6Thehiddenvectorsareshiftedbyoneinorderto performnexttokenprediction.
7https://en.wikipedia.org/wiki/UTF-8
8Modelingcharactersdirectlymayalleviatethisissue;however,character-levelmultilingualmodelscanexplode
vocabulary sizes.
9As is the case generally in multilingual modeling, also without boundary predictors.
10Determiningthescriptofgiventextsequenceistrivial,weassumeeverysequencecontainsa singlescript.
11Forthepurposesofthiswork,wordsaredefinedbywhitespaceboundaries.
4

--- PAGE 5 ---
Wethensettheprior Sforthecorrespondingscript Stobe 1/R. Our finaltrainingobjectiveovera
single instance xis as follows:
NX
i=1 logp(xijx<i) X
SI(script (x) =S)logBinomial (S;N; k )
where Iistheindicatorfunctionand script ()isafunctionassigningawritingscripttoasequenceof
bytesx, such assignment can be easily obtained based on codepoint definitions in Unicode.
3 Experimental Setup
3.1 Language Modeling
Pretraining Data We pretrain all models on nine languages (English, Spanish, French, Russian,
Ukranian,Belarusian,Telugu,BengaliandHindi)dividedintothreegroups,writtenwithdistinctscripts:
Latin,Cyrillic,andIndic(Brahmic)scripts. Ourchoiceofselectionisbasedonthelinguisticdiversityof
these languages and the availability of data for downstream evaluation. Our pretraining data is obtained
from the OSCAR dataset [32]. WepresentthestatisticsforeachlanguageinAppendixC.
Baselines We compare MAGNET against Dynamic Token Pooling [ 31], which infers boundaries
with a fixed binomial prior for every sequence, irrespective of the language script. This model is
referred to as DTPin the rest of the paper. DTPhas a single boundary predictor; we train two versions
of this baseline with the binomial prior as 0.2 and 0.1 respectively yielding 5 and 10 compression
respectively. We also compare against a byte-level decoder language model. To ensure fair model
comparisons, this model has a similar architecture as DTP, but without any sequence compression.
Table 1: Binomial prior choice for each MAGNET
andDTPmodel configuration. These combinations
ofbinomialpriorsdeterminethecompressionrate
ofsequencesperlanguagescript. While DTPuses
fixedpriorsforalllanguages, MAGNET isdynamic
and script-specific.
Binomial Prior
Configuration Latin Cyrillic Indic
DTP 5 0.2 0.2 0.2
DTP 10 0.1 0.1 0.1
MAGNET (1;2;4)1 0.5 0.25
MAGNET (3;6;12)0.33 0.17 0.083
MAGNET (5;10;13)0.2 0.10 0.076
MAGNET (5;10;15)0.2 0.10 0.066
MAGNET (5;10;20)0.2 0.10 0.05MAGNET configurations We compute the
byte-word-ratios‚Äô, choosing English, Russian and
Teluguasanchorlanguagesforeachthelanguage
script, based on initial explorations. The FLO-
RES [16] dataset is used for this purpose and
the resulting ratios are approximately 5 , 10,
and 20 for English, Russian, and Telugu, re-
spectively. Based on these ratios, we train five
MAGNET modelswithdifferentbinomialprior
combinations maintaining the ratio but adjusting
themultipliers. First,tooptimizeforword-level
boundary segmentation, we use the original byte-
to-word ratio configuration, i.e., 5compression
forLatin, 10compressionfor Cyrillic and 20
compression for Indic languages within the same
model. The second configuration; (1;2;4)is
the averagebytes-to-characterratioforEnglish,
Russian and Telugu. Hence, using this configura-
tion optimizes for fair byte-level-modelling with character-level granularity. The third configuration
(3;6;12)is based on a hypothesis that it would lead to fair subword-segmentation boundaries. Fi-
nally,sinceweapplyaveryhighcompressiononIndiclanguages,weempiricallytesttwoadditional
configurations (5;10;13)and(5;10;15)with a reduced compression rate for Indic languages.
Subword Tokenizer Training To compare the segmentation derived from MAGNET to traditional
subword tokenizers, we create byte-level byte pair encoding (BPE) vocabularies containing 50K, 100K
and250Ksubwordunitsonourpretrainingdata. Weemploy -samplingtotrainthetokenizers,typically
used to improve representation of low-resource languages [ 10]. That is, we sample documents for each
language accordingto a multinomial distributionwithprobabilities fqigi=1:::N, where: qi=p
iPN
j=1p
j
with pi=niPN
k=1nk. Thisincreasestokensforlow-resourcelanguagesandhasbeenshowntoreduce
bias towards high-resource ones. Weconsider = 0:5;0:3consistent with [10, 12].
5

--- PAGE 6 ---
DownstreamDatasets Todemonstratetheeffectivenessof MAGNET ,weevaluatebyfinetuning
our trained models on several question answering and classification tasks. Specifically, we evaluate
onXQuAD(questionanswering)[ 6],XNLI(naturallanguageinference)[ 11],PAWS-X (paraphrase
detection) [ 46] from XTREME [ 19], and SIB 200 (the topic classification) [ 2]. We provide a detailed
language coverage across all tasks in Table 5. In addition, to test how adaptation capabilities of
MAGNET , we evaluate on dialectal tasks, specifically ILI [ 48] the Indo-Aryan Language Identification
(ILI) shared task and HaSCoSVa-2022 [ 7], hatespeech detection on Spanish dialects. We provide
finetuning details in Appendix D.
3.2 Analyzing segmentation across models
The objective of this analysis is to compare segmentation granularity across different approaches. That
is,wemeasurewhetherthesameamountofinformationisconveyedthroughsimilartokencountsacross
variouslanguages. Followingpreviouswork[ 33,3],weconductthisanalysiswiththeparallelcorpus
FLORES-200 [16] focusing on the nine languages in our pretraining data.
For byte-level models, segmentation is done by converting each sentence to raw UTF-8 bytes and
computing the average number of bytes per sentence. With the subword tokenizer, each sentence is
segmented using the tokenizer we trained in 3.1, and the average number of resulting tokens computed
acrossallsentences. Asforgradient-basedmethodslike DTPandourMAGNET ,wefeedeachsentence
intothemodelandretrieveasequenceofboundarypredictionsfromtheboundarypredictorlayers. The
count of positive predictions determines the number of tokens per sentence.
4 Results
The goal of MAGNET is to learn equitable segmentation during training while maintaining high quality
downstream performance. Ideally, we expect that MAGNET results in higher compression for the
non-Latinscriptlanguages,hencebalancingsegmentationgranularityacrossalllanguages. This,inturn,
should improve modeling efficiency by reducing computational costs at training and inference time.
4.1MAGNET results in equitable segmentation across language scripts.
en es fr uk ru be hi bn te
Language020406080100 Average number of tokensTokenizer
DTP 5x
MAGNET(5, 10, 13) x
MAGNET(5, 10, 20) x
BPE 50k
BPE 100k
BPE 250kalpha 0.5 BPE 250k
alpha 0.5 BPE 100k
alpha 0.5 BPE 50k
alpha 0.3 BPE 250k
alpha 0.3 BPE 100k
alpha 0.3 BPE 50k
Figure 2: Average number of tokens after seg-
menting the FLORES dataset. Subword tok-
enizersand DTPresultinover-segmentationin
non-Latin script languages, while MAGNET
closes the gap.Weanalyzethesegmentationgranularity,contrasting
our method with byte-level, subword tokenization, and
DTPasdescribedin¬ß3.2. OurresultsinFigure2show
thatMAGNET models produce similar segmentation
ratesforalllanguages. Theimprovementisparticularly
noticeable in non-Latin script languages that are most
susceptible to over-segmentation with the baselines.
First, we compare byte-level segmentation to MAG-
NETwith the (1;2;4)segmentation configuration.
As described in ¬ß3.1, Indic languages have approxi-
mately four byte code-points to one character, Cyrillic
languages have approximately two, and many Latin
languageshaveaone-to-onemapping. Therefore,we
expectthattraining MAGNET withthe (1;2;4)con-
figurationwillresultinequitablebyte-levelmodeling
across all of these languages. Appendix Figure 7a
shows that MAGNET (1;2;4)resultsina 3drop
intheaveragenumberoftokensfortheIndiclanguages,
andcloseto 2dropforCyrillic,whiletheLatin lan-
guages are not affected. Next, we compare segmen-
tation between DTP 5,MAGNET at(5;10;13),
MAGNET at(5;10;20)andsubword-tokenizersatvocabularysizes50k,100kand250kwithand
without alpha sampling (see ¬ß3.1). We find that MAGNET models result in the most equitable segmen-
tation across all languages. In fact, we measure a drop close to 5 in the average number of tokens for
the Indic languages compared to DTPand the subword tokenizers. Notably, we also see that a subword
tokenizer with a large vocabulary size is required to achieve a lower segmentation rate on Cyrillic
6

--- PAGE 7 ---
languages, whereas for Indic languages, even with a large vocabulary and sampling, we observe
a pronounced disparity. This contrasts with findings from previous work that sampling alleviates
tokenizationdisparities[ 10]. Overall,theseresultssuggestthat MAGNET learnsequitablesegmentation
acrosslanguageswithdiversescripts,whilefixedsegmentationmodelslike DTPandbyte-basedsubword
tokenizers are sub-optimal and very likely to result in over-segmentation.
4.2MAGNET maintains performance on downstream tasks.
en fr es ru hi bn teAvg
Byte-Level
DTP 5x
DTP 10x
MAGNET(1,2,4) x
MAGNET(3, 6, 12) x
MAGNET(5, 10, 20) xModel74.72 70.89 71.09 67.13 62.74 67.00 67.19 68.68
72.91 70.19 70.77 66.73 62.89 66.87 66.78 68.16
71.92 68.05 68.98 67.06 62.52 66.16 66.47 67.31
74.44 70.63 71.10 67.47 64.01 67.03 66.52 68.74
74.42 70.83 71.55 67.65 61.80 66.33 65.84 68.35
73.57 70.82 71.70 67.02 62.08 65.77 64.73 67.96
Figure 3: Language-specific accuracy on the XNLI
taskacrossbyte-level,andall DTPandMAGNET
models. Results are mostly competitive between
byte-leveland MAGNET modelsonLatinlanguages
and Russian.Our goal is to enforce equitable segmentation
while maintaining model performance across
tasks. In Table2, wepresentresultsforthebest-
performing MAGNET model compared to DTP
andbyte-levelmodels(weprovidecomparisonsto
allMAGNET models in ¬ß5.1). Overall, We find
thatMAGNET models perform better than DTP
but are competitive with the byte-level models
while being considerably faster and requiring less
compute. MAGNET (3;6;12)performs best
on PAWS-X and SIB, while (1;2;4)performs
best on XNLI and XQUAD.
We report language-specific results on the XNLI
dataset in Figure 3. We see better results with
theMAGNET models even in some cases where
themodelsincomparisonoptimizeforasimilar
segmentation. For instance, on Spanish, MAG-
NETat(5;10;15)outperforms DTPat5. On Indic languages, MAGNET models are generally
competitivewiththe DTPmodels. Weprovidemoreanalysisonthetrade-offsbetweendownstream
performanceandsegmentationin¬ß5.1. Resultsondialectaltasksarereportedin¬ß2Weseecompetitive
resultsacrossallmodelsonallthetasks,suggestingthatthereisalsononegativeimpactas aresultof
adapting the models to their respective dialects.
Table 2: The average performance (accuracy) on downstream tasks on all languages across different
models. Wepresentresultsforthebest-performing MAGNET model: ( (3;6;12)forPAWS-Xand
SIB),( (1;2;4)forXQUADandXNLI).Boldindicatesthebestoverallperformance.Table6provides
more detailed language-specific results.
Model XNLI PAWS-X SIB XQUAD
Byte-level 68.68 82.18 71.05 44.62
DTP 5 68.16 81.29 69.17 43.31
DTP 10 67.31 75.99 67.83 35.83
MAGNET 68.74 85.41 71.43 44.61
(a) In-language tasks.Hascova ILI
87.38 89.24
86.87 89.17
87.62 88.72
87.25 89.27
(b) Dialectal tasks.
4.3MAGNET results in more efficient models.
Comparing inference times across all models, we expect that models which optimize for fixed compres-
sion likeDTPwould be only efficient for Latin languages because of their lower byte-to-word ratio.
Hence, we anticipate that our routing strategy with the MAGNET models would result in an efficiency
gainfornon-Latinscriptlanguages. InFigure4,weplottheinferencetimeperlanguageinXQUAD,
relative to the inference time of the byte-level models. We show that MAGNET has a shorter inference
time than the byte-level models, comparable to DTPfor English and Russian and slightly lower for
Hindi andRussian. If we assume the optimal compression rates forEnglish and Russian to be 5and
10,respectively,usinga DTPmodelwithafixedcompressionrateforbothlanguagesrequirestraining
twoseparatemonolingualmodelstoobtaintheidealcompressionrateforeach. However,traininga
7

--- PAGE 8 ---
singleMAGNET (5;10;20)modelthatdynamicallyachieves5 compressionrateforEnglishand
10compression rate for Russian results in a lower inference time for both.
en hi ru0.00.51.0Inference time (relative)0.38x 0.38x 0.38x0.34x 0.35x 0.34x0.74x
0.30x0.42x
0.29x 0.29x 0.29x0.38x
0.33x 0.33xByte-level
Fixed compression (5x)
Fixed compression (10x)
MAGNET(1,2,4) x
MAGNET(3, 6, 12) x
MAGNET(5, 10, 20) x
Figure 4: Inference time per language in XQUAD, relative to the byte-level model. MAGNET ‚Äôs
inference time is shorter than the byte-level model and comparable to DTPfor most of the languages.
5 Analysis and Discussion
5.1 Trade-off between downstream performance and equitable segmentation
Compression rateAverage task performance304050607080
1 6 11 16Latin Cyrillic Indic
Figure5: Averagetaskperformancevscompression
trade off across language scripts.Previous studies have reported correlations be-
tween compression and model performance [ 14].
We empirically investigate these tradeoffs by
comparingdownstreamperformanceacrossdif-
ferentMAGNET configurations defined in Ta-
ble 1. In the results reported in Table 3, we
find that the configurations that perform best are
MAGNET (1;2;4)andMAGNET (3;6;12).
These configurations are equivalent to fair byte-
level and subword modelling across all languages.
We also report the average task performance per
script at various compression rates in Figure 5 .
Herewearenotlookingtocompareperformance
acrosslanguagescripts,butrathertoassessperfor-
manceacrossdifferentcompressionrateswithin
each language script. Our results show that there
islittletonodropinperformanceforLatinand
Cyrillic languages as compression increases moderately. However, for Indic languages, we see an
average of 5% drop in performance as compression increases.
Table 3: Results(accuracy)fromablationsacrossall MAGNET configurations.
Model XNLI PAWSX SIB XQUAD
MAGNET (1;2;4)68.74 83.30 71.02 44.61
MAGNET (3;6;12)68.35 85.41 71.43 43.72
MAGNET (5;10;20)67.50 80.44 71.13 41.06
MAGNET (5;10;13)66.28 79.85 70.64 41.92
MAGNET (5;10;15)67.96 84.79 68.79 42.81
(a)In-languagetasksHascova ILI
86.75 88.62
87.13 88.57
87.25 89.27
87.88 88.35
88.37 88.67
(b) Dialectal tasks
5.2 What is the granularity of segmentation across different compression rates?
In ¬ß3.1, we highlight that the binomial prior is essential for determining the granularity of the seg-
mentation derived from the boundary predictor. To intrinsically validate that MAGNET indeed learns
8

--- PAGE 9 ---
segmentationsofsimilarlengthsacrossdifferentlanguages,wemanuallyanalyzeexamplesfromtheSIB
corpus,comparingthemwith DTPat5and10 . AsshowninTable4 DTPat5producesword-level
segmentation for all Latin languages while producing subword-level segmentation for Cyrillic and Indic
languages. At10 ,weseeword-levelsegmentationforCyrlliclanguages,phrase-levelsegmentationfor
Latinlanguagesandamixofsubwordandword-levelsegmentationonIndiclanguages. Toachieveword-
level segmentation for all languages, DTPrequires training three separate models. However MAGNET
alleviates this requirement by producing a similar segmentation granularity across all the languages. For
comparison,thesegmentationgranularityoftheBPEtokenizerishighlysub-optimalforIndiclanguages
as shown inAppendix Table4. WhiletheBPE tokenizerproducesword-level segmentationsforLatin
and Cyrillic languages, it produces character-level segmentation for Indic languages. MAGNET , on the
other hand, finds a good balance of segmentation granularity across languages.
Table4: SegmentationofEnglish,Spanish,Hindi,Russian,UkranianandTeluguexamplesfromtheSIB
corpus. Whileothermodelsproducedifferentsegmentationgranularityacrosslanguages, MAGNET
consistently produces similar segmentation granularity across languages.
Lang Text DTP5x DTP10x MAGNET (5;10;20) BPE 100k
en This will allow play-
ers to control actions
and movements in video
games by moving the de-
vice through the air.This || will || allow || players ||
to||con||trol||action||s||and
|| movemen || ts || in || video ||
games || by || moving || the ||
device || through || the || air.This will || allow players || to
control || actions || and move-
ments || in video || games || by
moving||thedevice||through
|| the air.This || will || allow || players ||
to || con || trol || actions || and
|| movemen || ts || in || video ||
games || by || moving || the ||
device || through || the || air.This||will||allow||players||to
||control||actions||and||move-
ments||in||video||games||by||
moving||the||device||through
|| the || air ||.
es Esto permitir√° a los ju-
gadorescontrolarlasac-
ciones y los movimien-
tos en los videojuegos,
atrav√©sdelmovimiento
del dispositivo por el
aire.Es || to || permitir || √° || a los ||
jugadores||con||trolar||las||
accion||es||y||los||movimien-
tos || en || los || videojuegos, ||
a || trav√©s || del || movimiento
|| del || dispositivo || por || el ||
aire.Esto permitir√° || a los || ju-
gadores || controlar || las ac-
ciones||ylos||movimientos||
enlos||videojuegos||,atrav√©s
|| del movimiento || del disposi-
tivo || por el aire.Es || to || permitir || √° || a || los ||
jugadores||con||trolar||las||
acciones||y||los||movimien||
tos || en || los || videojuegos, || a
||trav√©s||de||lmovimien||to
|| de || l dispositivo || por || e || l
aire.Esto|| permitir√°|| a|| los|| ju-
gadores|| controlar|| las|| ac-
ciones|| y|| los|| movimientos||
en|| los|| videojuegos||,|| a||
trav√©s||del||movimiento||del||
dispositivo|| por|| el|| aire||.
te ‡∞™‡∞∞‡∞ø‡∞ï‡∞∞‡∞æ ‡∞®‡∞øÔøΩ ‡∞ó‡∞æ‡∞≤‡±ãÔøΩ ‡∞ï‡∞¶‡∞ø‡∞≤‡∞ø‡∞Ç‡∞ö-
‡∞°‡∞Ç ÔøΩÔøΩ ‡∞∞‡∞æ ‡∞µ‡±Ä‡∞°‡∞ø‡∞Ø‡±ã ‡∞ó‡±á ÔøΩÔøΩ ‡∞≤‡±ã
‡∞Ø‡∞æ‡∞ï‡±ç‡∞∑ ‡∞®‡±ÅÔøΩ ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞ï‡∞¶‡∞≤‡∞ø‡∞ï-
‡∞≤‡∞®‡±Å ‡∞®‡∞ø ‡∞Ø‡∞Ç ÔøΩÔøΩ‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞á‡∞¶‡∞ø
‡∞Ü‡∞ü‡∞ó‡∞æ ‡∞≥ÔøΩ‡∞ï‡±Å ‡∞µ‡±Ä‡∞≤‡±Å ‡∞ï ‡∞≤‡∞øÔøΩÔøΩÔøΩ‡∞Ç ‡∞¶‡∞ø‡∞™ || ‡∞∞ || ‡∞ø || ‡∞ï‡∞∞ || ‡∞æ || ‡∞®ÔøΩ || ‡∞ø || ‡∞ó || ‡∞æ || ‡∞≤ÔøΩ|| ‡±ã
|| ‡∞ï‡∞¶ || ‡∞ø || ‡∞≤‡∞ø || ‡∞Ç‡∞ö || ‡∞°‡∞Ç ||‡∞¶ÔøΩ || ‡∞æ || ‡∞∞ || ‡∞æ
|| ‡∞µ‡±Ä || ‡∞° || ‡∞ø || ‡∞Ø‡±ã || ‡∞ó || ‡±á‡∞Æ || ‡±ç‡∞∏ || ‡±ç || ‡∞≤‡±ã ||
‡∞Ø‡∞æ || ‡∞ï‡±ç‡∞∑ || ‡∞®ÔøΩ|| ‡±Å || ‡∞Æ‡∞∞ || ‡∞ø || ‡∞Ø‡±Å || ‡∞ï‡∞¶ ||
‡∞≤ || ‡∞ø || ‡∞ï‡∞≤ || ‡∞® || ‡±Å || ‡∞®‡∞ø || ‡∞Ø‡∞Ç ‡∞§ || ‡±ç‡∞∞ || ‡∞ø ||
‡∞Ç‡∞ö || ‡∞°‡∞æ || ‡∞®‡∞ø || ‡∞ï‡∞ø || ‡∞á‡∞¶ || ‡∞ø || ‡∞Ü‡∞ü || ‡∞ó || ‡∞æ ||
‡∞≥ÔøΩ|| ‡∞ï‡±Å || ‡∞µ‡±Ä || ‡∞≤ || ‡±Å || ‡∞ï‡∞≤ || ‡±ç‡∞™ || ‡∞ø || ‡∞∏ || ‡±ç‡∞§
|| ‡±Å‡∞Ç || ‡∞¶ || ‡∞ø.‡∞™‡∞∞ || ‡∞ø‡∞ï‡∞∞‡∞æ‡∞® || ‡±ç‡∞® || ‡∞ø ‡∞ó || ‡∞æ ‡∞≤ÔøΩ|| ‡±ã ‡∞ï‡∞¶ || ‡∞ø ‡∞≤‡∞ø‡∞Ç-
‡∞ö‡∞° || ‡∞Ç ‡∞¶ || ‡±ç‡∞µ‡∞æ‡∞∞‡∞æ || ‡∞µ || ‡±Ä‡∞°‡∞ø‡∞Ø || ‡±ã ‡∞ó ||
‡±áÔøΩÔøΩ ‡∞≤ || ‡±ã ‡∞Ø || ‡∞æ‡∞ï‡±ç‡∞∑‡∞® || ‡±ç‡∞≤ || ‡±Å ‡∞Æ‡∞∞ || ‡∞ø‡∞Ø‡±Å
‡∞ï‡∞¶‡∞≤‡∞ø‡∞ï‡∞≤‡∞® || ‡±Å ‡∞® || ‡∞ø ‡∞Ø‡∞Ç ‡∞§ || ‡±ç‡∞∞ || ‡∞ø‡∞Ç‡∞ö‡∞°
|| ‡∞æ‡∞®‡∞ø‡∞ï || ‡∞ø || ‡∞á‡∞¶‡∞ø || ‡∞Ü‡∞ü‡∞ó‡∞æ‡∞≥ || ‡±ç‡∞≤‡∞ï || ‡±Å ‡∞µ ||
‡±Ä‡∞≤‡±Å ‡∞ï‡∞≤ || ‡±ç‡∞™‡∞ø ‡∞∏ÔøΩ|| ‡±Å‡∞Ç‡∞¶ || ‡∞ø‡∞™‡∞∞‡∞ø‡∞ï‡∞∞‡∞æ‡∞® || ‡±ç‡∞®‡∞ø ‡∞ó‡∞æ‡∞≤ || ‡±ç‡∞≤‡±ã ‡∞ï‡∞¶ || ‡∞ø‡∞≤ || ‡∞ø‡∞Ç-
‡∞ö‡∞° || ‡∞Ç ‡∞¶ || ‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞µ‡±Ä‡∞°‡∞ø‡∞Ø‡±ã ‡∞ó‡±á‡∞Æ || ‡±ç ÔøΩ ‡∞≤
|| ‡±ã ‡∞Ø‡∞æ‡∞ï‡±ç‡∞∑‡∞® || ‡±ç‡∞≤‡±Å ‡∞Æ‡∞∞ || ‡∞ø‡∞Ø‡±Å ‡∞ï‡∞¶‡∞≤‡∞ø‡∞ï-
‡∞≤‡∞® || ‡±Å ‡∞®‡∞ø ‡∞Ø‡∞Ç ‡∞§ || ‡±ç ‡∞∞‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï || ‡∞ø ‡∞á‡∞¶‡∞ø
‡∞Ü‡∞ü‡∞ó‡∞æ‡∞≥ || ‡±ç‡∞≤‡∞ï‡±Å ‡∞µ‡±Ä‡∞≤ || ‡±Å ‡∞ï‡∞≤ || ‡±ç‡∞™‡∞ø‡∞∏ ||
‡±çÔøΩ‡∞Ç ‡∞¶ || ‡∞ø‡∞™‡∞∞ ||‡∞ø ||‡∞ï‡∞∞ ||‡∞æ ||‡∞® ||‡±ç ||‡∞® ||‡∞ø || ‡∞ó ||‡∞æ ||‡∞≤ ||‡±ç ||‡∞≤
||‡±ã || ‡∞ï‡∞¶ ||‡∞ø ||‡∞≤ ||‡∞ø‡∞Ç ||‡∞ö‡∞° ||‡∞Ç || ‡∞¶ ||‡±ç ||‡∞µ ||‡∞æ
||‡∞∞ ||‡∞æ || ‡∞µ ||‡±Ä ||‡∞° ||‡∞ø ||‡∞Ø ||‡±ã || ‡∞ó ||‡±á ||‡∞Æ ||‡±ç ||‡∞∏
||‡±ç ||‡∞≤ ||‡±ã || || ‡∞Ø ||‡∞æ ||‡∞ï ||‡±ç ||‡∞∑‡∞® ||‡±ç ||‡∞≤ ||‡±Å ||
‡∞Æ‡∞∞ ||‡∞ø ||‡∞Ø ||‡±Å || ‡∞ï‡∞¶‡∞≤ ||‡∞ø ||‡∞ï‡∞≤‡∞® ||‡±Å || ‡∞® ||‡∞ø
||‡∞Ø ||‡∞Ç ||‡∞§ ||‡±ç ||‡∞∞ ||‡∞ø‡∞Ç ||‡∞ö‡∞° ||‡∞æ ||‡∞® ||‡∞ø ||‡∞ï ||‡∞ø ||
‡∞á‡∞¶ ||‡∞ø || || ‡∞Ü‡∞ü‡∞ó ||‡∞æ ||‡∞≥ ||‡±ç ||‡∞≤‡∞ï ||‡±Å || ‡∞µ ||‡±Ä ||‡∞≤
||‡±Å || ‡∞ï‡∞≤ ||‡±ç ||‡∞™ ||‡∞ø ||‡∞∏ ||‡±ç ||‡∞§ ||‡±Å‡∞Ç ||‡∞¶ ||‡∞ø
hi ‡§Ø‡§πÔøΩ‡§°‡§µ‡§æ‡§á‡§∏ ‡§ï‡•ã ‡§π‡§µ‡§æ ‡§Æ ‡•á‡§Ç
‡§Æ‡•Ç‡§µ ‡§ï‡§∞‡§ï‡•á ÔøΩ‡§ñ‡§≤‡§æ‡§ø‡•ú‡§Ø‡•ã‡§Ç ‡§ï‡•ã
‡§è‡§ï‡•ç‡§∂‡§® ‡§î‡§∞ ‡§Æ‡•Ç‡§µ‡§Æ ‡•á‡§Ç‡§ü ‡§ï‡§Ç‡§ü‡•ç‡§∞‡•ã‡§≤
‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ ÔøΩ‡§§‡§¶‡•á‡§ó‡§æ.‡§Ø || ‡§π ||ÔøΩ‡§°|| ‡§µ‡§æ || ‡§á‡§∏ || ‡§ï‡•ã ||
‡§π‡§µ ||‚óå‡§æ|| ‡§Æ‡•á ||‚óå‡§Ç|| ‡§Æ‡•Ç || ‡§µ ||
‡§ï‡§∞ || ‡§ï‡•á || ÔøΩ‡§ñ|| ‡§≤‡§æ ||‡§ø‡•ú|| ‡§Ø‡•ã
||‚óå‡§Ç|| ‡§ï‡•ã || ‡§è‡§ï || ‚óå‡•ç‡§∂ || ‡§® ||
‡§î‡§∞ || ‡§Æ‡•Ç || ‡§µ || ‡§Æ‡•á || ‚óå‡§Ç‡§ü || ‡§ï‡§Ç||
‡§ü ||‚óå‡•ç‡§∞||‚óå‡•ã|| ‡§≤ || ‡§ï‡§∞ || ‡§® ||
‚óå‡•á|| ‡§ï‡•Ä || ‡§Ö || ‡§® || ‚óå‡•Å|| ‡§Æ‡§§ ||
‡§ø‚óå|| ‡§¶‡•á|| ‡§ó || ‡§æ.‡§Ø || ‡§πÔøΩ‡§°‡§µ‡§æ‡§á‡§∏ ‡§ï || ‚óå‡•ã‡§π || ‡§µ‡§æ
‡§Æ ||‚óå‡•á‡§Ç‡§Æ ||‚óå‡•Ç‡§µ ‡§ï || ‡§∞‡§ï‡•á ‡§ñ
||‡§ø‚óå‡§≤‡§æ‡§ø‡•ú‡§Ø‡•ã‡§Ç ‡§ï ||‚óå‡•ã‡§è || ‡§ï ||
‚óå‡•ç‡§∂‡§® ‡§î‡§∞ ‡§Æ || ‚óå‡•Ç‡§µ‡§Æ ||‚óå‡•á‡§Ç‡§ü ‡§ï
||‚óå‡§Ç‡§ü‡•ç‡§∞||‚óå‡•ã‡§≤ ‡§ï || ‡§∞‡§®‡•á ‡§ï || ‚óå‡•Ä
|| ‡§Ö‡§® ||‚óå‡•Å‡§ÆÔøΩ‡§§‡§¶ ||‚óå‡•á‡§ó‡§æ.‡§Ø‡§πÔøΩ‡§°‡§µ‡§æ‡§á‡§∏ ‡§ï || ‚óå‡•ã‡§π‡§µ‡§æ ‡§Æ ||
‚óå‡•á‡§Ç‡§Æ ||‚óå‡•Ç‡§µ ‡§ï‡§∞‡§ï‡•áÔøΩ‡§ñ‡§≤‡§æ‡§ø‡•ú‡§Ø || ‡•ã‡§Ç
‡§ï ||‚óå‡•ã‡§è‡§ï‡•ç‡§∂‡§® ‡§î‡§∞ ‡§Æ || ‚óå‡•Ç‡§µ‡§Æ‡•á‡§Ç‡§ü
‡§ï‡§Ç‡§ü ||‚óå‡•ç‡§∞‡•ã‡§≤ ‡§ï‡§∞‡§® || ‚óå‡•á‡§ï‡•Ä ‡§Ö‡§® ||
‚óå‡•Å‡§ÆÔøΩ‡§§‡§¶ ||‚óå‡•á‡§ó‡§æ.‡§Ø‡§π || ‡§° || ‡§ø‚óå||‡§µ ||‚óå‡§æ||‡§á‡§∏ ||
‡§ï ||‚óå‡•ã|| ‡§π‡§µ ||‚óå‡§æ|| ‡§Æ ||‚óå‡•á‡§Ç||
‡§Æ ||‚óå‡•Ç||‡§µ || ‡§ï‡§∞‡§ï ||‡•á || ‡§ñ || ‡§ø‚óå
||‡§≤ ||‚óå‡§æ||‡•ú ||‡§ø‚óå||‡§Ø ||‚óå‡•ã‡§Ç|| ‡§ï
||‚óå‡•ã|| ‡§è‡§ï ||‚óå‡•ç||‡§∂‡§® || ‡§î‡§∞ || ‡§Æ
||‚óå‡•Ç||‡§µ‡§Æ ||‚óå‡•á‡§Ç||‡§ü || ‡§ï || ‚óå‡§Ç||‡§ü
||‚óå‡•ç||‡§∞ ||‚óå‡•ã||‡§≤ || ‡§ï‡§∞‡§® ||‡•á || ‡§ï
||‚óå‡•Ä|| ‡§Ö‡§® ||‚óå‡•Å||‡§Æ‡§§ ||‡§ø‚óå|| ‡§¶
||‚óå‡•á||‡§ó ||‚óå‡§æ
ru –ü–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º –¥–≤–∏-
–∂–µ–Ω–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
–≤ –≤–æ–∑–¥—É—Ö–µ –∏–≥—Ä–æ–∫–∏
—Å–º–æ–≥—É—Ç —É–ø—Ä–∞–≤–ª—è—Ç—å
–¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏
–¥–≤–∏–∂–µ–Ω–∏—è–º–∏ –≤ –≤–∏-
–¥–µ–æ–∏–≥—Ä–∞—Ö.–ü||–æ||—Å—Ä–µ||–¥—Å—Ç–≤||–æ||–º
|| –¥–≤–∏ || –∂–µ–Ω–∏ || —è || —É—Å—Ç—Ä–æ
|| –π—Å—Ç–≤ || –∞ || –≤ || –≤–æ || –∑–¥—É-
—Ö–µ || –∏ || –≥—Ä–æ || –∫–∏ || —Å–º–æ
|| –≥—É—Ç || —É–ø—Ä–∞ || –≤–ª—è || —Ç—å ||
–¥–µ || –π—Å—Ç–≤ || –∏ || —è–º–∏ || –∏
|| –¥–≤–∏ || –∂–µ–Ω–∏ || —è–º–∏ || –≤ ||
–≤–∏ || –¥–µ–æ || –∏ || –≥—Ä–∞ || —Ö.–ü–æ—Å—Ä–µ–¥ || —Å—Ç–≤–æ || –º || –¥–≤–∏-
–∂–µ–Ω || –∏—è || —É—Å—Ç—Ä–æ–π ||
—Å—Ç–≤–∞ || –≤ –≤–æ–∑–¥—É—Ö–µ || –∏–≥—Ä–æ-
–∫–∏ || —Å–º–æ–≥—É—Ç || —É–ø—Ä–∞–≤–ª—è—Ç—å
|| –¥–µ–π—Å—Ç–≤–∏ || —è–º–∏ || –∏ –¥–≤–∏-
–∂–µ–Ω || –∏—è–º–∏ || –≤ –≤–∏–¥–µ–æ–∏–≥-
—Ä–∞ || —Ö–ü–æ || —Å—Ä–µ–¥—Å—Ç–≤–æ–º || –¥–≤–∏-
–∂–µ || –Ω–∏—è || —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
|| –≤ –≤–æ–∑–¥—É—Ö–µ || –∏ || –≥—Ä–æ–∫–∏
|| —Å–º–æ–≥—É—Ç || —É–ø—Ä–∞–≤–ª—è—Ç—å ||
–¥–µ–π—Å—Ç–≤||–∏||—è–º–∏||–∏–¥–≤–∏-
–∂–µ || –Ω–∏—è–º–∏ || –≤ –≤–∏–¥–µ–æ–∏ ||
–≥—Ä–∞—Ö–ü–æ—Å || —Ä–µ–¥ || —Å—Ç–≤–æ–º || –¥–≤–∏-
–∂–µ–Ω–∏—è||—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞||–≤||
–≤–æ–∑–¥—É—Ö–µ || –∏–≥—Ä–æ–∫–∏ || —Å–º–æ-
–≥—É—Ç || —É–ø—Ä–∞–≤–ª—è—Ç—å || –¥–µ–π-
—Å—Ç–≤–∏—è || –º–∏ || –∏ || –¥–≤–∏–∂ ||
–µ–Ω–∏—è–º–∏ || –≤|| –≤–∏–¥–µ–æ || –∏–≥
|| —Ä–∞—Ö ||.
uk –¶–µ –∑–∞–±–µ–∑–ø–µ—á–∏—Ç—å –≥—Ä–∞–≤-
—Ü—è–º –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥
–¥—ñ—è–º–∏ —Ç–∞ —Ä—É—Ö–∞–º–∏ —É
–≤—ñ–¥–µ–æ—ñ–≥—Ä–∞—Ö, —Ä—É—Ö–∞—é—á–∏
–ø—Ä–∏—Å—Ç—Ä—ñ–π —É –ø–æ–≤—ñ—Ç—Ä—ñ.–ü–æ–¥–æ—Ä||–æ–∂–Ω||–∞–º—ñ||—Å—Ü–µ
|| –º–æ–∂–Ω || –∞ –∑—Ä || —É—á–Ω–æ –ø–æ
||—î–¥–Ω–∞||—Ç–∏–∑–ø—Ä||–æ–≥—É–ª—è–Ω-
–∫–æ—é –æ–∑–µ || —Ä–æ–º || —É —á–æ–≤–Ω—ñ.–¶–µ || –∑–∞–±–µ–∑–ø–µ—á–∏—Ç—å || –≥—Ä–∞–≤-
—Ü—è–º || –∫–æ–Ω—Ç—Ä–æ || –ª—å || –Ω–∞–¥
–¥—ñ—è–º–∏ || —Ç–∞ || —Ä—É—Ö–∞–º–∏ || —É
–≤—ñ–¥–µ–æ—ñ–≥—Ä–∞ || —Ö || , —Ä—É—Ö–∞—é—á–∏
|| –ø—Ä–∏—Å—Ç—Ä—ñ–π || —É –ø–æ–≤—ñ—Ç—Ä—ñ.–¶–µ || –∑–∞–±–µ–∑–ø–µ—á–∏—Ç—å || –≥—Ä–∞–≤-
—Ü—è–º||–∫–æ–Ω—Ç—Ä–æ–ª—å||–Ω–∞–¥–¥—ñ—è-
–º–∏||—Ç–∞—Ä—É—Ö–∞–º–∏||—É–≤—ñ–¥–µ–æ—ñ-
–≥—Ä–∞—Ö, || —Ä—É—Ö–∞—é || —á–∏ || –ø—Ä–∏-
—Å—Ç—Ä—ñ–π || —É –ø–æ–≤—ñ—Ç—Ä—ñ.–¶–µ || –∑–∞–±–µ–∑–ø–µ—á ||–∏—Ç—å ||
–≥—Ä–∞–≤ ||—Ü—è–º || –∫–æ–Ω—Ç—Ä–æ–ª—å ||
–Ω–∞–¥ || –¥—ñ—è–º–∏ || —Ç–∞ || —Ä—É—Ö
||–∞–º–∏||—É||–≤—ñ–¥–µ–æ||—ñ–≥||—Ä–∞—Ö
||, || —Ä—É—Ö ||–∞—é—á–∏ || –ø—Ä–∏-
—Å—Ç—Ä—ñ–π || —É || –ø–æ–≤—ñ—Ç—Ä—ñ ||.
5.3 Does segmentation significantly change after finetuning?
We investigate the effects of fine-tuning on the resulting segmentation boundaries across various down-
streamtasks. Essentially,weinspecthowtheboundarypredictionchangesafterfine-tuningforeach
downstream task. We find that there are no differences in segmentation before and after fine-tuning
despite updating the parameters of the boundary predictors. While there are a few instances where the
segmentationofthefine-tunedmodelisdifferentthanthatofthepretrainedmodel,thereisnoclear
evidence of the segmentation changing drastically after fine-tuning. In Table 7 in the Appendix, we
presenttwoexamplesfromtheSIB datasetwherethereisaslightchangeinsegmentation. Wefoundno
indication that any observed changes contribute to or hurt task performance.
9

--- PAGE 10 ---
6 Related Work
Overcoming segmentation disparities in subword tokenization In multilingual settings, subword
tokenizers have proven to be prone to over-segmentation, due to the data-driven nature of the BPE
algorithm [ 39]. Previous work [ 13,12,44] has attempted to address data imbalance issues in subword
tokenizersbyover-samplinglow-resourcelanguages. Ourworkshowsthatthisonlyalleviatesthebiason
certainscriptsanddoesn‚Äôtsolvetheproblem. Otherstudies[ 1,36,17]havealsoshownthattokenization
intransformersremainsbiasedinfavorofhigh-resourcelanguages. Wangetal. [42]enforcemodels
to use smaller subword units in high-resource languages to make segmentation fairer. Some works
[23,8]suggesttrainingmultilingualtokenizersonlanguageclusterstomitigatesegmentationdisparities,
however, this leads to expanded vocabularies. Despite these attempts, it is evident that the training
objectives of subword tokenizers do not effectively align with those of language modeling.
Tokenizer-free language models Language modelling over bytes [ 45,4] and pixels [ 37,38,26]
has become desirable, as it removes complicated preprocessing pipelines in modelling. Xue et al. [45]
introduced ByT5, a tokenizer-free variant of T5 [ 35] that processes text at the byte level. However
byte-level encoding over-fragments non-Latin script languages resulting in overly long sequences. Since
byte or character sequences usually result in longer sequences, previous work [ 30,9,41,47,31,15] on
tokenizer-free LMs has introduced novel model architectures to mitigate the computational overhead of
processingrawcharacterorbytetextdirectly. Thesemethods[ 9,41,47,30]endupsegmentingraw
sequences into fixed/dynamic-size patches, which is not suitable for modelling over non-Latin scripts.
7 Conclusion
In this work, we introduce MAGNET , a gradient-based tokenization method to learn equitable seg-
mentationacrosslanguagesscriptsinbyte-levelmultilingualmodels. MAGNET dynamicallyroutes
byte-level sequences through language-script-specific internal boundary predictors trained to infer word
boundariesthroughstochasticreparameterisation. Weshowthat MAGNET enablesustolearntoken
representationswiththesamegranularityacrosslanguagescomparedtovanillabyte-levelmodelsand
previous gradient-based tokenization approaches. Our analysis demonstrates that while there are indeed
downstream performance trade-offs as a result of MAGNET inducing high compression on non-Latin
script languages, we are still able to maintain downstream performance quality. Overall, our results
hold promise for future research on equitable segmentation and text processing more generally.
Acknowledgments
We would like to thank the UW NLP community for valuable discussions of this work. We are grateful
toFarhanSamirandAlisaLiufordiscussionsonexperimentsandanalysis. Thisworkwassupported
in part by NSF IIS 2113530. We also gratefully acknowledge support from the National Science
Foundation under CAREER Grant No. IIS2142739, NSF grants No. IIS2125201 and IIS2203097, and
gift funding from Google, MSR, and OpenAI.
10

--- PAGE 11 ---
References
[1]J. √Åcs. Exploring bert‚Äôs vocabulary. Blog Post, 2019.
[2]D. Adelani, H. Liu, X. Shen, N. Vassilyev, J. Alabi, Y. Mao, H. Gao, and E.-S. Lee. SIB-
200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages
and dialects. In Y. Graham and M. Purver, editors, Proceedings of the 18th Conference of the
European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages226‚Äì245, St. Julian‚Äôs,Malta,Mar. 2024. Associationfor ComputationalLinguistics. URL
https://aclanthology.org/2024.eacl-long.14 .
[3]O. Ahia, S. Kumar, H. Gonen, J. Kasai, D. Mortensen, N. Smith, and Y. Tsvetkov. Do all
languages cost the same? tokenization in the era of commercial language models. In H. Bouamor,
J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural
LanguageProcessing ,pages9904‚Äì9923,Singapore,Dec.2023.AssociationforComputational
Linguistics. doi: 10.18653/v1/2023.emnlp-main.614. URL https://aclanthology.
org/2023.emnlp-main.614 .
[4]R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones. Character-level language modeling
with deeper self-attention. In AAAI Conference on Artificial Intelligence , 2018. URL https:
//api.semanticscholar.org/CorpusID:52004855 .
[5]C. Arnett, P. D. Riviere, T. A. Chang, and S. Trott. Different tokenization schemes lead to
comparable performance in spanish number agreement. ArXiv, abs/2403.13754, 2024. URL
https://api.semanticscholar.org/CorpusID:268536952 .
[6]M. Artetxe, S. Ruder, and D. Yogatama. On the cross-lingual transferability of monolingual
representations. CoRR, abs/1910.11856, 2019.
[7]G.Castillo-l√≥pez,A. Riabi,andD. Seddah. Analyzingzero-shottransferscenariosacrossSpan-
ish variants for hate speech detection. In Y. Scherrer, T. Jauhiainen, N. Ljube≈°iƒá, P. Nakov,
J. Tiedemann, and M. Zampieri, editors, Tenth Workshop on NLP for Similar Languages,
Varieties and Dialects (VarDial 2023) , pages 1‚Äì13, Dubrovnik, Croatia, May 2023. Asso-
ciation for Computational Linguistics. doi: 10.18653/v1/2023.vardial-1.1. URL https:
//aclanthology.org/2023.vardial-1.1 .
[8]H.W.Chung,D.Garrette,K.C.Tan,andJ.Riesa. Improvingmultilingualmodelswithlanguage-
clustered vocabularies. In B. Webber, T. Cohn, Y. He, and Y. Liu, editors, Proceedings of
the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages
4536‚Äì4546, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/
v1/2020.emnlp-main.367. URL https://aclanthology.org/2020.emnlp-main.
367.
[9]J.H.Clark,D.Garrette,I.Turc,andJ.Wieting. Canine: Pre-traininganefficienttokenization-free
encoderforlanguagerepresentation. TransactionsoftheAssociationforComputationalLinguistics ,
10:73‚Äì91,2022. doi: 10.1162/tacl_a_00448. URL https://aclanthology.org/2022.
tacl-1.5 .
[10]A. CONNEAU and G. Lample. Cross-lingual language model pretraining. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, editors, Ad-
vances in Neural Information Processing Systems , volume 32. Curran Associates, Inc., 2019.
URLhttps://proceedings.neurips.cc/paper_files/paper/2019/file/
c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf .
[11]A.Conneau,R.Rinott,G.Lample,A.Williams,S.R.Bowman,H.Schwenk,andV.Stoyanov.
Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on
EmpiricalMethodsinNaturalLanguageProcessing .AssociationforComputationalLinguistics,
2018.
[12]A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm√°n, E. Grave, M. Ott,
L. Zettlemoyer, and V. Stoyanov. Unsupervised cross-lingual representation learning at scale.
In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics , pages 8440‚Äì8451, Online, July 2020.
11

--- PAGE 12 ---
Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https:
//aclanthology.org/2020.acl-main.747 .
[13]J.Devlin,M.-W.Chang,K.Lee, andK.Toutanova. BERT:Pre-trainingofdeepbidirectional
transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors,
Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages
4171‚Äì4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:
10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .
[14]W. Fleshman and B. V. Durme. Toucan: Token-aware character level language modeling. ArXiv,
abs/2311.08620, 2023. URL https://api.semanticscholar.org/CorpusID:
265213263 .
[15]N. Godey, R. Castagn√©, √â. de la Clergerie, and B. Sagot. MANTa: Efficient gradient-based
tokenization for end-to-end robust language modeling. In Y. Goldberg, Z. Kozareva, and
Y. Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022 ,
pages 2859‚Äì2870, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.findings-emnlp.207. URL https://aclanthology.
org/2022.findings-emnlp.207 .
[16]N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato,
F. Guzm√°n, and A. Fan. The Flores-101 evaluation benchmark for low-resource and multilingual
machinetranslation. TransactionsoftheAssociationforComputationalLinguistics ,10:522‚Äì538,
2022. doi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.tacl-1.
30.
[17]J.Hayase,A.Liu,Y.Choi,S.Oh,andN.A.Smith.Datamixtureinference: Whatdobpetokenizers
reveal about their training data?, 2024. URL https://arxiv.org/abs/2407.16607 .
[18]D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus), 2023.
[19]J.Hu,S.Ruder,A.Siddhant,G.Neubig,O.Firat,andM.Johnson. Xtreme: Amassivelymultilin-
gualmulti-taskbenchmarkforevaluatingcross-lingualgeneralization. CoRR,abs/2003.11080,
2020.
[20]E.Jang,S.Gu,andB.Poole. Categoricalreparameterizationwithgumbel-softmax. In International
ConferenceonLearningRepresentations ,2017. URL https://openreview.net/forum?
id=rkE3y85ee .
[21]D.P.KingmaandJ. Ba. Adam: A methodforstochasticoptimization. CoRR,abs/1412.6980,
2014. URL https://api.semanticscholar.org/CorpusID:6628106 .
[22]T. Kudo. Subword regularization: Improving neural network translation models with multiple
subwordcandidates. InI.GurevychandY.Miyao,editors, Proceedingsofthe56thAnnualMeeting
oftheAssociationforComputationalLinguistics(Volume1: LongPapers) ,pages66‚Äì75,Melbourne,
Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1007.
URLhttps://aclanthology.org/P18-1007 .
[23]D.Liang,H.Gonen,Y.Mao,R.Hou,N.Goyal,M.Ghazvininejad,L.Zettlemoyer,andM.Khabsa.
XLM-V: Overcoming the vocabulary bottleneck in multilingual masked language models. In
H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical
Methods in NaturalLanguageProcessing , pages 13142‚Äì13152, Singapore, Dec. 2023. Association
for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.813. URL https://
aclanthology.org/2023.emnlp-main.813 .
[24]T.Limisiewicz,D.Malkin,andG.Stanovsky. Youcanhaveyourdataandbalanceittoo: Towards
balancedandefficientmultilingualmodels. InL.Beinborn,K.Goswami,S.Muradoƒülu,A.Sorokin,
R.Kumar,A. Shcherbakov,E.M.Ponti,R.Cotterell,andE.Vylomova,editors, Proceedingsof
the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP , pages
1‚Äì11, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/
v1/2023.sigtyp-1.1. URL https://aclanthology.org/2023.sigtyp-1.1 .
12

--- PAGE 13 ---
[25]T.Limisiewicz,T.Blevins,H.Gonen,O.Ahia,andL.Zettlemoyer. Myte: Morphology-driven
byte encoding forbetter and fairer multilingual language modeling. ArXiv, abs/2403.10691, 2024.
URLhttps://api.semanticscholar.org/CorpusID:268512851 .
[26]J.Lotz,E.Salesky,P.Rust,andD.Elliott. Textrenderingstrategiesforpixellanguagemodels.
InH.Bouamor,J.Pino,andK.Bali,editors, Proceedingsofthe2023ConferenceonEmpirical
Methods in NaturalLanguageProcessing , pages 10155‚Äì10172, Singapore, Dec. 2023. Association
for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.628. URL https://
aclanthology.org/2023.emnlp-main.628 .
[27]C. J. Maddison,A. Mnih,andY. W. Teh. The concrete distribution: A continuous relaxationof
discreterandomvariables. In InternationalConferenceonLearning Representations ,2017. URL
https://openreview.net/forum?id=S1jE5L5gl .
[28]S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gall√©, A. Raja, C. Si, W. Y. Lee,
B. Sagot, and S. Tan. Between words and characters: A brief history of open-vocabulary
modeling and tokenization in nlp. ArXiv, abs/2112.10508, 2021. URL https://api.
semanticscholar.org/CorpusID:245335281 .
[29]B. Muller, A. Anastasopoulos, B. Sagot, and D. Seddah. When being unseen from mBERT
is just the beginning: Handling new languages with multilingual language models. In
K.Toutanova,A.Rumshisky,L.Zettlemoyer,D.Hakkani-Tur,I.Beltagy,S.Bethard,R.Cotterell,
T. Chakraborty, and Y. Zhou, editors, Proceedings of the 2021 Conference of the North American
ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies ,pages
448‚Äì462, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
naacl-main.38. URL https://aclanthology.org/2021.naacl-main.38 .
[30]P.Nawrot,S.Tworkowski,M.Tyrolski,L.Kaiser,Y.Wu,C.Szegedy,andH.Michalewski. Hier-
archical transformers are more efficient language models. In M. Carpuat, M.-C. de Marneffe, and
I. V. Meza Ruiz, editors, Findings of the Association for Computational Linguistics: NAACL 2022 ,
pages1559‚Äì1571,Seattle,UnitedStates,July2022.AssociationforComputationalLinguistics.
doi: 10.18653/v1/2022.findings-naacl.117. URL https://aclanthology.org/2022.
findings-naacl.117 .
[31]P. Nawrot, J. Chorowski, A. Lancucki, and E. M. Ponti. Efficient transformers with dynamic
tokenpooling. InA. Rogers,J.Boyd-Graber, andN.Okazaki, editors, Proceedingsofthe61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages
6403‚Äì6417, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.
18653/v1/2023.acl-long.353. URL https://aclanthology.org/2023.acl-long.
353.
[32]P.J.OrtizSu‚Äôarez,B.Sagot,andL.Romary. Asynchronouspipelinesforprocessinghugecorpora
on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in
the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 ‚Äì 16,
Mannheim,2019.Leibniz-Institutf‚ÄùurDeutscheSprache. doi: 10.14618/ids-pub-9021. URL
http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215 .
[33]A. Petrov, E. La Malfa, P. H. S. Torr, and A. Bibi. Language model tokenizers introduce
unfairness between languages. In Advances in Neural Information Processing Systems , 2023. URL
https://arxiv.org/abs/2305.15425 .
[34]J. Pfeiffer, I. Vuliƒá, I. Gurevych, and S. Ruder. UNKs everywhere: Adapting multilingual
language models to new scripts. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih,
editors,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-
cessing, pages 10186‚Äì10203, Online and Punta Cana, Dominican Republic, Nov. 2021. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.800. URL
https://aclanthology.org/2021.emnlp-main.800 .
[35]C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and
P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J.
Mach. Learn. Res. , 21:140:1‚Äì140:67, 2019. URL https://api.semanticscholar.
org/CorpusID:204838007 .
13

--- PAGE 14 ---
[36]P. Rust, J. Pfeiffer, I. Vuliƒá, S. Ruder, and I. Gurevych. How good is your tokenizer? on the
monolingual performance of multilingual language models. In C. Zong, F. Xia, W. Li, and
R. Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational
Linguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume
1: LongPapers) ,pages3118‚Äì3135,Online,Aug.2021.AssociationforComputationalLinguis-
tics. doi: 10.18653/v1/2021.acl-long.243. URL https://aclanthology.org/2021.
acl-long.243 .
[37]P.Rust,J.F.Lotz,E.Bugliarello,E.Salesky,M.deLhoneux,andD.Elliott. Languagemodelling
withpixels. In TheEleventhInternationalConferenceonLearningRepresentations ,2023. URL
https://openreview.net/forum?id=FkSp8VW8RjH .
[38]E.Salesky,N.Verma,P.Koehn,andM.Post. Multilingualpixelrepresentationsfortranslationand
effectivecross-lingualtransfer.InH.Bouamor,J.Pino,andK.Bali,editors, Proceedingsofthe2023
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing ,pages13845‚Äì13861,Singapore,
Dec. 2023. Associationfor ComputationalLinguistics. doi: 10.18653/v1/2023.emnlp-main.854.
URLhttps://aclanthology.org/2023.emnlp-main.854 .
[39]R.Sennrich,B.Haddow,andA.Birch.Neuralmachinetranslationofrarewordswithsubwordunits.
In K. Erk and N. A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers) , pages 1715‚Äì1725, Berlin, Germany,
Aug. 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL
https://aclanthology.org/P16-1162 .
[40]X. Song, A. Salcianu, Y. Song, D. Dopson, and D. Zhou. Fast WordPiece tokenization. In M.-F.
Moens,X.Huang,L.Specia,andS.W.-t.Yih,editors, Proceedingsofthe2021Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing ,pages2089‚Äì2103,OnlineandPuntaCana,
Dominican Republic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/
v1/2021.emnlp-main.160. URL https://aclanthology.org/2021.emnlp-main.
160.
[41]Y. Tay, V. Q. Tran, S. Ruder, J. Gupta, H. W. Chung, D. Bahri, Z. Qin, S. Baumgartner,
C. Yu, and D. Metzler. Charformer: Fast character transformers via gradient-based subword
tokenization. In International Conference on Learning Representations , 2022. URL https:
//openreview.net/forum?id=JtBRnrlOEFN .
[42]X. Wang, S. Ruder, and G. Neubig. Multi-view subword regularization. In K. Toutanova,
A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell,
T. Chakraborty, and Y. Zhou, editors, Proceedings of the 2021 Conference of the North American
ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies ,pages
473‚Äì482, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
naacl-main.40. URL https://aclanthology.org/2021.naacl-main.40 .
[43]S. Wu and M. Dredze. Are all languages created equal in multilingual BERT? In S. Gella,
J. Welbl, M. Rei, F. Petroni, P. Lewis, E. Strubell, M. Seo, and H. Hajishirzi, editors, Proceedings
of the 5th Workshop on Representation Learning for NLP , pages 120‚Äì130, Online, July 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.repl4nlp-1.16. URL https:
//aclanthology.org/2020.repl4nlp-1.16 .
[44]L.Xue,N.Constant,A.Roberts,M.Kale,R.Al-Rfou,A.Siddhant,A.Barua,andC.Raffel. mT5:
Amassivelymultilingualpre-trainedtext-to-texttransformer. InK.Toutanova,A.Rumshisky,
L.Zettlemoyer,D.Hakkani-Tur,I.Beltagy,S.Bethard,R.Cotterell,T.Chakraborty,andY.Zhou,
editors,Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pages 483‚Äì498, Online, June
2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.naacl-main.41. URL
https://aclanthology.org/2021.naacl-main.41 .
[45]L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts, and C. Raffel.
ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the
Association for Computational Linguistics , 10:291‚Äì306, 2022. doi: 10.1162/tacl_a_00461. URL
https://aclanthology.org/2022.tacl-1.17 .
14

--- PAGE 15 ---
[46]Y. Yang,Y. Zhang,C. Tar,andJ. Baldridge. PAWS-X:A Cross-lingualAdversarialDatasetfor
Paraphrase Identification. In Proc. of EMNLP , 2019.
[47]L. YU, D. Simig, C. Flaherty, A. Aghajanyan, L. Zettlemoyer, and M. Lewis. MEGABYTE:
Predicting million-byte sequences with multiscale transformers. In Thirty-seventh Conference on
Neural Information Processing Systems , 2023. URL https://openreview.net/forum?
id=JTmO2V9Xpz .
[48]M. Zampieri, P. Nakov, N. Ljube≈°iƒá, J. Tiedemann, S. Malmasi, and A. Ali, editors. Proceedings
of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018) ,
Santa Fe, New Mexico, USA, Aug. 2018. Association for Computational Linguistics. URL
https://aclanthology.org/W18-3900 .
15

--- PAGE 16 ---
Appendix
A Limitations
Theprimarylimitationofourworkistherestrictedresourcesavailablefortheextensiveexperimentswe
carried out. This constrained the number of languages included in our pretraining data, the size of the
pretraining data itself, and the size of the models. Nonetheless, we hypothesize that our current results
willholdtruewhenreplicatedwithlargermodels,as MAGNET islikelytoprovideevengreaterbenefits
when integrated into large models. We leave this to future work to explore further. Another limitation
is the performance-compression trade-offs associated with MAGNET , as some languages are sensitive
to high compression rates. However we note that this is not universal across all tasks. In fact, we argue
thatMAGNET offers users the flexibility to optimize for their desired benefits. Finally, MAGNET also
inheritssomelimitationsfrompreviousgradient-basedtokenizationmethods[ 31,30,41]andthevast
majorityofsegmentation methods. This approachmaynotbe suitableforSemiticlanguages, where
morphemes are discontinuous and vowels are interspersed between consonant roots for inflection or
sometimesomitted. Computingbyte-to-wordratioswouldbeharderinCJK(Chinese,Japanese,and
Korean) languages, as they do not mark word boundaries with space. However we note that our choice
oflanguageswasnotinfluencedbyspaceseparation,butratherbylinguisticdiversityandtheavailability
ofdatafordownstreamevaluation. MAGNET isveryflexibleandapplicableto alllanguages thatcan
beexpressedwithUTF-8,weusebyte-word-ratioasasimpleproxytotrainourboundarypredictors
tolearnequitabletokenization. Wenotethatbyte-word-ratioisnotacompulsoryproxyandforsuch
languages other proxies can be used
B Broader Impacts Statement
In this work, we contribute to promoting equitable segmentation in multilingual language models
acrossvariouslanguagescripts. Ourapproachholdspromiseforenhancingtheutilityandefficiency
of multilingual language models, particularlybenefiting low-resourced and non-Latin script languages
spokenbybillionsworldwide. WeacknowledgelimitationsofourworkinAppendixA,andstrongly
adviseagainstunintendedusageofthemodels. Wewillreleaseourcodeandmodelstofacilitatefurther
research in this direction.
C Dataset Statistics
C.1 Pretraining data
Our pre-training data is obtained from the OSCAR dataset [32]. Due to resource constraints, we only
pretrain our models on a subset of this dataset. The distribution of tokens across languages in displayed
in Figure 6.
C.2 Downstream data
Table 5: Downstreamlanguageandtaskcoverage
Dataset Task Languages
XNLI Naturallanguageinference en, fr, es, ru, hi, bn , te
XQUAD Questionanswering en,es,ru, hi
PAWS-X Adversarial paraphrase identification en, fr, es
SIB-200 Topic classification en, fr, es, ru, uk, be, bn, te, hi
D Technical Details
DataPreprocessing Forallourdatasets,wepreprocessalltexttorawUTF-8bytes. Inthe MAGNET
models, we add a unique script identifier to the front of every sequence that guides the models to route
the sequence to the respective script boundary predictor.
16

--- PAGE 17 ---
LanguageSize (Bytes)
2.30E+82.35E+82.40E+82.45E+82.50E+8
en fr es be ru uk bn te hiFigure 6: Languagestatisticsinthepretrainingdata.
D.1 Model Hyperparameters
Forallourexperiments,weuse14-layerhourglasstransformerswith2layersinthefirstblock,10layers
in the second block and 2 layers in the final block. For every transformer layer, the hidden dimension is
768,theintermediatefeed-forwarddimensionis3072. Eachself-attentionlayerconsistsof12heads.
We use a post-norm architecture, GELU activation function [ 18] in feedforward layers and the relative
attentionparametrisationfromTransformerXL.Thisbringsourmodel‚Äôssizeto 126Mparameters.
The boundary predictor is a 2-layer MLP that takes in a hidden state as input and outputs a scalar
prediction ateach time step. We use the Adam optimizer [ 21] with( 1,2) and parameters as (0.9,
0.98) and 1e-6, respectively. We use a learning rate of 5e-5, a warmup ratio of 0.1 and 38,000 training
steps,abatchsizeof512distributedacross4A40GPUs. Eachbatchconsistsofexamplesconcatenated
up to the maximum sequence length of 512.
D.2 Finetuning
We finetune the pretrainedmodels byappending a linear classification layer and update all parameters
during training. The boundary predictors‚Äô parameters are also updated to ensure that the predicted
segmentations are well adaptedto each task. We finetune for5 epochs with a batchsizeof 32 and the
same learning rate and warm-up ratio that we used for pretraining. We report accuracy averaged over 2
runs with different random seeds.
E Supplementary Results
E.1 Equitable Segmentation at Byte-Level Granularity
In Figure 7, we present plots comparing segmentation granularity between (1.) byte-level segmentation
andMAGNET (1;2;4). (2)Between DTP 5,MAGNET at(5;10;13),MAGNET at(5;10;20)
and subword-tokenizers at vocabulary sizes 50k, 100k and 250k with and without alpha sampling (see
section 3.1)
E.2 Downstream Tasks
We present language-level results (accuracy) across all tasks and models in Table 6
17

--- PAGE 18 ---
Table 6: Language-levelresultsacrossalltasks
Model en fr es ru hi bn te Avg
Byte-Level 74.72 70.89 71.77 67.89 62.89 67.93 66.90 69.12
DTP 5 73.18 70.19 69.78 66.96 61.79 67.11 66.12 66.74
DTP 10 71.92 68.05 68.13 67.00 61.26 65.79 65.49 66.38
MAGNET (1;2;4)x74.44 70.63 71.99 67.48 64.73 67.44 66.88 69.17
MAGNET (3;6;12)x74.41 70.83 70.78 67.66 61.57 65.32 65.14 67.53
MAGNET (5;10;20)x73.71 70.23 71.15 66.35 61.72 65.12 65.01 67.13
MAGNET (5;10;13)x73.97 70.38 70.58 66.85 62.42 66.11 65.46 67.97
MAGNET (5;10;15)x73.57 70.82 71.36 67.02 61.89 65.43 64.96 67.86
(a) XNLI
Model en fr es Avg
Byte-Level 85.70 80.45 80.40 82.18
DTP 5 84.13 80.20 79.55 81.29
DTP 10 76.13 76.10 75.75 75.99
MAGNET (1;2;4)87.63 81.73 80.55 83.30
MAGNET (3;6;12)87.60 83.98 84.65 85.41
MAGNET (5;10;20)82.05 78.83 80.45 80.44
MAGNET (5;10;13)81.40 78.28 79.10 79.59
MAGNET (5;10;15)87.78 83.05 83.55 84.79
(b) PAWSXen es ru hi Avg
55.61 53.225 40.49 29.18 44.62
51.40 49.85 41.62 30.39 43.31
40.68 37.75 38.09 26.79 35.82
55.94 51.78 41.75 28.98 44.61
55.32 52.46 41.29 25.82 43.72
53.12 50.29 39.61 21.23 41.06
53.13 50.16 38.31 26.07 41.92
53.14 51.88 39.82 26.41 42.81
(c) XQUAD
Model en es fr ru be uk bn te hi Avg
Byte-Level 76.96 72.06 71.57 75.98 71.07 72.55 63.00 68.63 67.65 71.05
DTP 5 73.28 73.03 69.60 70.34 73.77 70.83 62.01 66.18 63.48 69.17
DTP 10 73.28 68.38 68.14 67.89 70.83 67.89 63.24 66.67 64.21 67.83
MAGNET (1;2;4)75.25 73.04 72.55 75.00 70.09 70.59 63.24 66.43 73.04 71.02
MAGNET (3;6;12)79.17 77.94 75.69 75.00 75.74 73.29 56.62 59.31 56.13 69.87
MAGNET (5;10;20)80.15 78.92 77.70 75.98 76.47 74.02 49.75 53.90 51.96 68.76
MAGNET (5;10;13)79.17 79.66 73.78 71.57 73.78 71.33 59.07 65.68 61.76 70.64
MAGNET (5;10;15)80.64 78.92 74.80 72.30 76.23 72.31 56.37 52.94 54.66 68.79
(d) SIB-200
18

--- PAGE 19 ---
en es fr uk ru be hi bn te
Language050100150200250300350 Average number of tokensTokenizer
byte level
MAGNET(1, 2, 4) x(a)MAGNET compared to byte-level model
en es fr uk ru be hi bn te
Language020406080100 Average number of tokensTokenizer
DTP 5x
MAGNET(5, 10, 13) x
MAGNET(5, 10, 20) x
BPE 50k
BPE 100k
BPE 250kalpha 0.5 BPE 250k
alpha 0.5 BPE 100k
alpha 0.5 BPE 50k
alpha 0.3 BPE 250k
alpha 0.3 BPE 100k
alpha 0.3 BPE 50k (b)MAGNET compared to DTPand BPE
Figure 7: Average number of tokens after segmenting the FLORES dataset. Evidently subword
tokenizers and DTPresult in over-segmentation in non-Latin script languages, while MAGNET closes
the gap.
Table7: EnglishandRussianinstancesfromtheSIBdatasetshowingslightchangesinsegmentation
before and after fine-tuning. Segmentation for these examples was performed using the MAGNET (5x,
10x, 20x).
Lang Text FinetuningSegmentation PretrainingSegmentation
enLastmonth a presidential
commission recommended the
prior CEP‚Äôs resignation as part
of a package of measures to
move the country towards new
elections.Lastkmonth kakpresiden ktialkcom-
missionkrecommen kdedkthekpriork
CEP‚Äôs kresignkaktionkaskpartkofk
akpackage kofkmeasures ktokmove k
thekcountry ktowards knewkekleck
tions.Lastkmonkthkakpresiden ktialkcomm
kissionkrecommended kthekprior k
CEP‚Äôs kresignkationkaskpartkofkak
package kofkmeasures ktokmove kthe
kcountry ktowards knewkeklections.
ru–í –ø—Ä–æ—à–ª–æ–º –º–µ—Å—è—Ü–µ –ø—Ä–µ–∑–∏-
–¥–µ–Ω—Ç—Å–∫–∞—è –∫–æ–º–∏—Å—Å–∏—è —Ä–µ–∫–æ-
–º–µ–Ω–¥–æ–≤–∞–ª–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É
–í—Ä–µ–º–µ–Ω–Ω–æ–º—É –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å-
–Ω–æ–º—É —Å–æ–≤–µ—Ç—É —É–π—Ç–∏ –≤ –æ—Ç-
—Å—Ç–∞–≤–∫—É –≤ –∫–∞—á–µ—Å—Ç–≤–µ —á–∞—Å—Ç–∏
–ø–∞–∫–µ—Ç–∞–º–µ—Ä–¥–ª—è–¥–≤–∏–∂–µ–Ω–∏—è
—Å—Ç—Ä–∞–Ω—ã–∫–Ω–æ–≤—ã–º–≤—ã–±–æ—Ä–∞–º.–í –ø—Ä–æ k—à–ª–æ–º k–º–µ—Å—è—Ü–µ k–ø—Ä–µ–∑–∏–¥–µ k
–Ω—Ç—Åk–∫–∞—èk–∫–æ–º–∏k—Å—Å–∏k—èk—Ä–µ–∫–æ-
–º–µk–Ω–¥–æ–≤–∞–ª–∞ k–ø—Ä–µ–¥—ã–¥ k—Ék—â–µ–º—É k
–í—Ä–µ–º–µ k–Ω–Ω–æ–º—É k–∏–∑–±–∏—Ä–∞—Ç–µ–ª—å k–Ω–æ–º—É
k—Å–æ–≤–µ—Ç—É k—É–π—Ç–∏ k–≤ –æ—Ç—Å—Ç–∞–≤–∫—É k–≤ –∫–∞-
—á–µ—Å—Ç–≤–µ k—á–∞—Å—Ç–∏ k–ø–∞–∫–µ—Ç–∞ k–º–µ—Äk–¥–ª—è
–¥–≤–∏–∂–µ k–Ω–∏—èk—Å—Ç—Ä–∞–Ω—ã k–∫ –Ω–æ–≤—ã–º k
–≤—ã–±–æ—Ä–∞–º–í –ø—Ä–æ k—à–ª–æ–º k–º–µ—Å—è—Ü–µ k–ø—Ä–µ–∑–∏–¥–µ k
–Ω—Ç—Åk–∫–∞—èk–∫–æ–º–∏—Å—Å–∏—èk—Ä–µ–∫–æ–º–µ k–Ω–¥–æ-
–≤–∞–ª–∞ k–ø—Ä–µ–¥—ã–¥—É k—â–µ–º—É k–í—Ä–µ–º–µ k
–Ω–Ω–æ–º—É k–∏–∑–±–∏—Ä–∞—Ç–µ–ª—å k–Ω–æ–º—É k—Å–æ–≤–µ-
—Ç—Ék—É–π—Ç–∏ k–≤ –æ—Ç—Å—Ç–∞–≤–∫—É k–≤ –∫–∞—á–µ—Å—Ç–≤–µ k
—á–∞—Å—Ç–∏ k–ø–∞–∫–µ—Ç–∞ k–º–µ—Äk–¥–ª—è –¥–≤–∏–∂–µ k
–Ω–∏—èk—Å—Ç—Ä–∞–Ω—ã k–∫ –Ω–æ–≤—ã–º k–≤—ã–±–æ—Ä–∞–º.
19
