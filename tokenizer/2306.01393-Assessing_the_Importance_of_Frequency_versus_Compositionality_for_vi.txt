# 2306.01393.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/2306.01393.pdf
# Kích thước tệp: 273887 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đánh giá Tầm quan trọng của Tần suất so với Tính tổng hợp đối với
Tokenization dựa trên Subword trong NMT
Benoist Wolleb1,Romain Silvestri1,Giorgos Vernikos1,2,
Ljiljana Dolamic3vàAndrei Popescu-Belis1,2
1HEIG-VD / HES-SO2EPFL3Armasuisse W+T
Yverdon-les-Bains Lausanne Thun
Thụy Sĩ Thụy Sĩ Thụy Sĩ
name.surname@heig-vd.ch ljiljana.dolamic@armasuisse.ch
Tóm tắt
Tokenization subword là tiêu chuẩn de facto
cho tokenization trong các mô hình ngôn ngữ
thần kinh và hệ thống dịch máy. Ba ưu điểm
thường được trích dẫn để ủng hộ subword:
mã hóa ngắn hơn cho các token thường xuyên,
tính tổng hợp của subword, và khả năng xử lý
các từ chưa biết. Vì tầm quan trọng tương đối
của chúng vẫn chưa hoàn toàn rõ ràng, chúng
tôi đề xuất một phương pháp tokenization cho
phép chúng tôi tách biệt tần suất (ưu điểm đầu
tiên) khỏi tính tổng hợp. Phương pháp này sử
dụng mã hóa Huffman để tokenize các từ theo
thứ tự tần suất, sử dụng một lượng ký hiệu cố
định. Các thí nghiệm với CS-DE, EN-FR và
EN-DE NMT cho thấy chỉ riêng tần suất đã
chiếm 90%-95% điểm số đạt được bởi BPE,
do đó tính tổng hợp có tầm quan trọng ít hơn
so với suy nghĩ trước đây.
1 Giới thiệu
Tokenization thành subword đã trở thành một
tiêu chuẩn không bị thách thức được sử dụng trong
hầu như tất cả các hệ thống NMT và mô hình ngôn
ngữ. Kể từ đề xuất của Sennrich et al. (2016) sử
dụng Byte-Pair Encoding (BPE) (Gage, 1994) để
tạo từ vựng subword, theo sau bởi việc sử dụng mô
hình ngôn ngữ unigram và việc triển khai Sentence-
Piece (Kudo, 2018), không có mô hình thay thế
nào khác đã vượt qua. Trong khi subword đã được
chứng minh thực nghiệm vượt trội hơn tokenization
ở cấp độ ký tự và từ (Sennrich et al., 2016; Wu et al.,
2016; Denkowski and Neubig, 2017), các yếu tố
© 2023 Các tác giả. Bài viết này được cấp phép theo giấy phép Creative
Commons 4.0, không có tác phẩm phái sinh, ghi công, CC-
BY-ND.góp phần vào thành công của chúng vẫn chưa được
hiểu đầy đủ. Một số nghiên cứu đã điều tra hiệu
suất của subword liên quan đến nén (Gall ´e, 2019;
Libovick ´y et al., 2022), gợi ý rằng nén tốt hơn có
thể liên quan đến hiệu suất cải thiện. Tuy nhiên, các
yếu tố khác như tính tổng hợp vẫn chưa được khám
phá kỹ lưỡng.
Trong bài báo này, chúng tôi sử dụng một thuật
toán thay thế để tạo từ vựng subword, chỉ giữ lại
một trong những đặc điểm đã được viện dẫn để giải
thích hiệu quả của BPE, cụ thể là thực tế rằng các
từ thường xuyên được mã hóa thành các subword
duy nhất, trong khi những từ ít thường xuyên hơn
được mã hóa bằng nhiều subword, có thể lên đến
cấp độ ký tự. Thuật toán dựa trên mã hóa Huffman
(Huffman, 1952), một phương pháp nén văn bản
khác với phương pháp được BPE sử dụng. Thuật
toán khác với BPE ở hai khía cạnh chính: trong khi
một số subword BPE truyền đạt thuộc tính ngôn
ngữ tổng hợp (ví dụ: ý nghĩa hoặc hình thái học),
mã hóa Huffman về cơ bản là phi tổng hợp và không
thể tokenize các từ không được thấy trong quá trình
huấn luyện. Khi sử dụng mã hóa Huffman để tokenize
dữ liệu cho MT dựa trên Transformer, chúng tôi đạt
được điểm số trong phạm vi 10-12% so với những
điểm số thu được bằng BPE khi đo bằng BLEU và
trong phạm vi 4-8% khi đo bằng COMET, cho kích
thước từ vựng 32k ký hiệu. Điều này chứng minh
rằng yếu tố chính chiếm thành công của BPE là tần
suất từ, chứ không phải tính tổng hợp subword. Các
đóng góp chính của chúng tôi là:
1. Chúng tôi cho thấy cách xây dựng từ vựng sub-
word cho tokenization sử dụng mã hóa Huffman.
2. Chúng tôi nghiên cứu tác động của phương pháp
này đối với NMT bằng cách thay đổi một loạt
tham số, đặc biệt là kích thước từ vựng.arXiv:2306.01393v3  [cs.CL]  12 Jan 2024

--- TRANG 2 ---
3. Quan sát rằng các điểm số thu được bằng mã
hóa Huffman gần với những điểm số thu được
bằng BPE, và lập luận rằng phương pháp trước
chỉ giữ lại khía cạnh tần suất của BPE, chúng
tôi kết luận rằng tần suất là lý do chính cho
hiệu quả của BPE.
2 Hiện trạng nghệ thuật
2.1 Tokenization Subword
Việc xử lý các từ ngoài từ vựng (OOV) - không
được thấy trong quá trình huấn luyện - đã là một
vấn đề lặp đi lặp lại trong MT và NLP cùng các
lĩnh vực khác. Các kích thước trên giới hạn chấp
nhận được của các lớp đầu vào/đầu ra trong mạng
thần kinh thường là 104-105 ký hiệu, thấp hơn
nhiều bậc độ lớn so với số lượng kiểu từ xuất hiện
trong một ngôn ngữ nhất định, khi xem xét các từ
ghép, tên riêng, số hoặc ngày tháng (chưa nói gì
đến các ngôn ngữ phong phú về mặt hình thái học).
Các phương pháp đầu tiên để dịch các từ OOV bao
gồm cơ chế sao chép và từ điển (Luong et al., 2015).
Ngoài ra, Jean et al. (2015) đã sử dụng các phép
xấp xỉ để tăng kích thước từ vựng hiệu quả mà
không tăng đáng kể số lượng tham số của các mô
hình.
Việc sử dụng các đoạn từ như các ký hiệu tương
ứng với các đơn vị đầu vào/đầu ra được giới thiệu
bởi Schuster và Nakajima (2012) cho nhận dạng
giọng nói tiếng Nhật và Hàn Quốc nhưng chỉ thu
hút sự chú ý lớn trong NMT với bài báo mang tính
bước ngoặt của Sennrich et al. (2016), đã chứng
minh được những cải thiện đáng kể trong nhiệm vụ
dịch thuật WMT 2015 (Bojar et al., 2015). Sennrich
et al. đã sử dụng một kỹ thuật bắt nguồn từ nén văn
bản, cụ thể là Byte-Pair Encoding (BPE) (Gage,
1994), để tạo ra một từ vựng có kích thước cố định
gồm các từ, đoạn từ (còn gọi là subword) và ký tự,
mà họ sử dụng để tokenize văn bản nguồn và đích
trong NMT. Từ vựng này được xây dựng bằng cách
dần dần hợp nhất các bigram ký hiệu thường xuyên
nhất, bắt đầu ở cấp độ ký tự, cho đến khi đạt được
kích thước từ vựng mong muốn. Với các biến thể
được mô tả dưới đây, phương pháp này đã trở thành
tiêu chuẩn de facto cho NMT và các mô hình ngôn
ngữ thần kinh.
Một trong những hệ thống NMT trực tuyến quy
mô lớn đầu tiên, được phát hành bởi Google, đã sử
dụng WordPiece (Wu et al., 2016), một phương pháp
tương tự với BPE trong đó việc lựa chọn các ký hiệu
được thêm vào từ vựng dựa trên xác suất trong dữ
liệu huấn luyện thay vì tần suất. Một kỹ thuật thay
thế cho phân đoạn subword là UnigramLM, được
giới thiệu bởi Kudo et al. (2018). Với phương pháp
này, một từ vựng ban đầu được điền với một số
lượng lớn ký hiệu và dần dần được giảm theo log-
likelihood của dữ liệu được tính bởi mô hình ngôn
ngữ unigram. Hơn nữa, UnigramLM giúp điều
chỉnh NMT vì nó cho phép nhiều tokenization của
cùng một văn bản, bằng cách thay đổi các subword
mà các lần xuất hiện khác nhau của cùng một kiểu
từ được phân đoạn. Một kỹ thuật điều chỉnh tương
tự đã được giới thiệu trong BPE, bằng cách ngẫu
nhiên loại bỏ một số phần tử của từ vựng để thay
đổi tokenisation của mỗi từ (Provilkov et al., 2020).
Những phương pháp này được triển khai trong thư
viện SentencePiece được sử dụng rộng rãi (Kudo
và Richardson, 2018)1.
Vì BPE và UnigramLM được sử dụng trong hầu
như tất cả các hệ thống NMT, các phương pháp thay
thế cho tokenization giải quyết cùng những vấn đề
đã hiếm khi được khám phá. Các mô hình NMT dựa
trên ký tự đã được nghiên cứu từ những ngày đầu
của NMT (Luong và Manning, 2016; Lee et al.,
2017; Cherry et al., 2018; Gupta et al., 2019), nhưng
phương pháp cấp độ ký tự đã nhường chỗ cho to-
kenization subword (Libovick ´y et al., 2022). Điều
này có thể do hiệu suất dưới tối ưu của tokenization
ký tự khi so sánh với subword và chi phí tính toán
tăng lên liên quan đến chuỗi token dài hơn trong
NMT.
Tính tổng hợp của BPE được chuyển giao sang
kịch bản đa ngôn ngữ nơi các ngôn ngữ có chữ viết
tương tự chia sẻ subword, được gọi là neo liên ngôn
ngữ. Trong khi các nhà nghiên cứu đã đề xuất cải
thiện số lượng neo thông qua phiên âm (Amrhein
và Sennrich, 2020), hoặc bằng cách sử dụng tương
đồng ngữ nghĩa (Vernikos và Popescu-Belis, 2021)
hoặc chồng chéo từ vựng (Patil et al., 2022), đã có
tương đối ít nghiên cứu trong việc tách biệt các hiệu
ứng của tính tổng hợp và tần suất.
Việc khám phá sớm mã hóa Huffman bởi Chitnis
và DeNero (2015) là một giải pháp sớm cho vấn
đề dịch từ hiếm, trước khi giới thiệu subword. Trong
khi kết quả của họ có triển vọng cho MT dựa trên
RNN so với tokenization cấp độ từ, phương pháp
của họ sau đó đã bị vượt qua bởi subword. Mặc dù
thuật toán của chúng tôi chia sẻ cơ sở lý thuyết giống
với họ, với một số khác biệt về triển khai, phạm vi
công việc của chúng tôi khác: chúng tôi không sử
dụng mã hóa Huffman để tạo ra thuật toán phân
đoạn tốt hơn, mà thay vào đó như một công cụ để
phân tích mối quan hệ giữa tính tổng hợp và mã
hóa các token thường xuyên.
1https://github.com/google/sentencepiece

--- TRANG 3 ---
gorithm, mà thay vào đó như một công cụ để phân
tích mối quan hệ giữa tính tổng hợp và mã hóa các
token thường xuyên.
2.2 Giải thích Hiệu quả của BPE
Một số ưu điểm của tokenization subword đã được
đưa ra, mặc dù các đóng góp cá nhân của chúng
đối với cải thiện hiệu suất NMT vẫn chưa được
nghiên cứu một cách có hệ thống. Những ưu điểm
này có thể được tóm tắt như sau:
Tần suất: các từ thường xuyên nhất tương ứng
với các token duy nhất (tức là các ký hiệu hoặc
chỉ số được sử dụng cho các lớp đầu vào/đầu
ra của NMT) trong khi những từ ít thường
xuyên hơn được phân giải thành hai hoặc nhiều
subword (sau đó được dịch như một chuỗi).
Tính tổng hợp: không giống như các sơ đồ nén
khác chuyển đổi từ thành một hoặc nhiều ký
hiệu, BPE tạo ra các ký hiệu là đoạn từ, do đó
cho phép tổng quát hóa khi dịch các từ chưa
thấy bằng cách kết hợp các bản dịch của các
subword tạo thành chúng.
Từ chưa biết: vì các ký tự riêng lẻ là một phần
của từ vựng token, bất kỳ từ nào trong dữ liệu
kiểm tra đều có thể được tokenize, trong trường
hợp xấu nhất thành các ký tự tạo thành nó.
Chỉ có các từ với ký tự không được thấy trong
dữ liệu huấn luyện không thể được biểu diễn.
Tính tổng hợp của BPE thường được trình bày
như công đức chính của nó, mặc dù không phải
không có cảnh báo. Sennrich et al. (2016) tuyên
bố rằng BPE "dựa trên trực giác rằng các lớp từ
khác nhau có thể dịch được thông qua các đơn vị
nhỏ hơn từ" và dựa trên sự tương tự với một dịch
giả người có thể dịch một số từ "ngay cả khi chúng
mới lạ với anh ta hoặc cô ta, dựa trên bản dịch của
các đơn vị subword đã biết như morpheme hoặc
phoneme." Chỉ ra sự khác biệt với mã hóa Huffman,
các tác giả nói rằng "chuỗi ký hiệu của họ vẫn có
thể được hiểu như các đơn vị subword" mà "mạng
có thể tổng quát hóa để dịch và tạo ra các từ mới."
Về mặt định lượng, Sennrich et al. (2016) phát
hiện rằng trong số "100 token hiếm (không nằm
trong 50.000 kiểu thường xuyên nhất) trong dữ
liệu huấn luyện tiếng Đức, phần lớn các token có
khả năng dịch được từ tiếng Anh thông qua các
đơn vị nhỏ hơn," đặc biệt là 21 từ ghép mà họ quan
sát được.
Tuy nhiên, không hoàn toàn rõ ràng liệu sub-
word có thực sự tương ứng với phần có ý nghĩa
của từ, như morpheme hoặc thành phần của từ
ghép. Sennrich et al. (2016) thừa nhận rằng "không
phải mọi phân đoạn chúng tôi tạo ra đều minh bạch"
và họ "không mong đợi lợi ích hiệu suất từ các
phân đoạn mờ đục," tức là các phân đoạn trong
đó các đơn vị không có ý nghĩa độc lập. Ví dụ,
Sennrich et al. cho thấy rằng BPE dẫn đến điểm
số BLEU gần như giống nhau với một mã hóa giữ
50.000 từ thường xuyên nhất như các ký hiệu duy
nhất, và mã hóa tất cả những từ khác bằng bigram
ký tự như các ký hiệu. Thách thức thực sự là cho
một mạng thần kinh học bản dịch đúng của một
chuỗi hai hoặc nhiều subword vô nghĩa. Tuy nhiên,
miễn là các ký tự được bao gồm trong từ vựng,
BPE có thể tokenize bất kỳ từ nào, do đó hiệu quả
giải quyết vấn đề từ chưa biết - một công đức được
công nhận rộng rãi.
Lý do chính khác cho hiệu quả của BPE là vai
trò trung tâm mà tần suất token đóng trong việc
xây dựng từ vựng, do đó trong việc quyết định khi
nào phân đoạn một từ hay không. BPE sử dụng ít
ký hiệu hơn để mã hóa các từ thường xuyên hơn
so với những từ ít thường xuyên hơn, và một phần
đáng kể của từ vựng BPE thực sự được tạo thành
từ toàn bộ từ (xem Hình 2 trong Phần 6 dưới đây).
Điều này có nghĩa là một tỷ lệ lớn các token trong
dữ liệu được mã hóa như các ký hiệu riêng lẻ, và
chỉ một tỷ lệ nhỏ hơn được phân đoạn thành sub-
word. Ví dụ, Kudo (2018) thừa nhận rằng "một ưu
điểm của BPE .. là nó có thể cân bằng hiệu quả
kích thước từ vựng .. và số lượng token cần thiết để
mã hóa một câu", bởi vì khi áp dụng BPE "các từ
thông thường vẫn là các ký hiệu duy nhất." Nói
cách khác, BPE hiệu quả vì nó "giữ nguyên các từ
thường xuyên nhất trong khi chia các từ hiếm thành
nhiều token" (Provilkov et al., 2020).
3 Tokenization Subword dựa trên Mã hóa
Huffman
Bây giờ chúng tôi giới thiệu một phương pháp
tokenization subword thay thế tách biệt tính tổng
hợp khỏi tần suất, và chỉ triển khai khía cạnh thứ
hai. Phương pháp này sẽ cho phép chúng tôi hiểu
khía cạnh nào trong số này có tác động lớn nhất
đến hiệu suất của NMT. Giống như BPE ban đầu
được lấy cảm hứng từ một thuật toán nén văn bản,
chúng tôi chuyển đổi ở đây văn bản đầu vào và đầu
ra thành chuỗi ký hiệu bằng cách sử dụng một sự
thích ứng của thuật toán nén dựa trên tần suất của
Huffman (1952).

--- TRANG 4 ---
3.1 Tổng quan
Để sử dụng mã hóa Huffman, tất cả các câu nguồn
và đích được xử lý như sau:
1. Tokenize mỗi câu thành từ bằng tokenizer
Moses (Koehn et al., 2007), và áp dụng true-
casing cho các từ.2
2. Đối với mỗi ngôn ngữ, đếm số lần xuất hiện
của mỗi từ, sắp xếp chúng theo thứ tự giảm
dần, và xây dựng cây Huffman với n ký hiệu
bằng thuật toán được đưa ra dưới đây.
3. Lưu ánh xạ 'từ' ↔'mã' kết quả từ cây, cho
mỗi ngôn ngữ, trong đó các mã được tạo thành
từ một hoặc nhiều trong số n ký hiệu được
phép.3
4. Mã hóa các câu huấn luyện và kiểm tra, thay
thế mỗi token bằng đối tác ký hiệu của nó.
Tách các token bằng ký hiệu Unicode cho
khoảng trắng (code point 0x2420).
5. Chia tất cả các mã Huffman thành ký hiệu và
tách chúng bằng khoảng trắng. Điều này cho
phép sử dụng NMT trực tiếp trên các tệp văn
bản kết quả, xử lý mỗi ký hiệu như một token
riêng lẻ, tương tự như bất kỳ đầu vào/đầu ra
được tokenize. Kích thước từ vựng do đó là
số lượng ký hiệu được sử dụng để xây dựng
cây Huffman cộng với dấu phân cách.
6. Huấn luyện NMT sử dụng dữ liệu song song
được mã hóa.
7. Mã hóa dữ liệu kiểm tra. Nếu các từ chưa thấy
trong quá trình huấn luyện xuất hiện ở phía
nguồn, đánh dấu chúng bằng ký hiệu "chưa
biết" đặc biệt.
8. Dịch dữ liệu kiểm tra được mã hóa bằng NMT
thành đầu ra được mã hóa.
9. Detokenize đầu ra NMT bằng cách nối các
ký hiệu không được tách bởi ký hiệu phân
cách 0x2420. Sau đó, giải mã các ký hiệu bằng
ánh xạ 'từ' ↔'mã'. Các chuỗi ký hiệu chưa
được thấy trong thời gian huấn luyện, và do
đó không có mặt trong ánh xạ, bị bỏ qua.
10. Chấm điểm văn bản được dịch bằng cách so
sánh nó với bản dịch tham chiếu sử dụng
BLEU, ChrF và COMET (xem Phần 4).
2Xem www2.statmt.org/moses/?n=Moses.Baseline.
3Về mặt kỹ thuật, trong triển khai của chúng tôi, các ký hiệu
được lấy từ phạm vi Unicode của CJK Unicode Ideographs
(Unicode Consortium, 2022, Ch. 18) trong đó gần 100.000
code point được định nghĩa, bắt đầu từ code point 0x4E00.
Điều này cung cấp một biểu diễn văn bản có thể hiển thị của
các ký hiệu, không có mã điều khiển có thể bị diễn giải sai
bởi hệ thống NMT.3.2 Xây dựng Cây Huffman
Cây Huffman có thể được xây dựng theo nhiều
cách, dẫn đến các mẫu mất cân bằng độ sâu khác
nhau, có thể được tối ưu hóa tùy thuộc vào tần suất
tương đối của các mục cần mã hóa. Đối với tất cả
các mẫu, các mục thường xuyên được đặt cao hơn
trong cây, để chúng được mã hóa với ít ký hiệu hơn.
Chúng tôi thích ứng phương pháp như sau, gần
nhất, mặc dù không giống hệt với biến thể "Repeat-
Symbol" của Chitnis và DeNero (2015), với ngoại
lệ chính là chúng tôi mã hóa tất cả các token.
Chúng tôi sử dụng cây Huffman n-ary, là những
cây không cân bằng trong đó các token cần mã hóa
xuất hiện trên các lá, và các đường dẫn dẫn đến
chúng tạo thành các biểu diễn được mã hóa của
chúng, tức là các chuỗi ký hiệu trên các nhánh.
Điều này được minh họa trong Hình 1 cho một cây
tam phân với ba ký hiệu, mã hóa tám kiểu từ dựa
trên tần suất của chúng trong một ví dụ đồ chơi.
Dữ liệu: Tần suất từ F:{(wi,fi),. . .},
Hàng đợi ưu tiên H:{(node i,score i),
. . .}được sắp xếp theo điểm số tăng dần,
Số lượng ký hiệu: n
Kết quả: Cây Huffman
foreach (wi,fi)inFdo
Tạo node iwith key wiand score fi;
Thêm node itoH;
end
while length( H)>1do
L←danh sách trống các node;
S←0;
fori←0tondo
ifH=tập hợp trống then
break;
else
Lấy (node i,score i) từ H;
Thêm ( node i,score i) vàoL;
Thêm score ivàoS;
end
end
Tạo node mới N= ('None', S);
foreach nodeinLdo
Thêm node vào con của N;
end
Đẩy NtoH;
end
Thuật toán 1: Xây dựng cây Huffman.

--- TRANG 5 ---
on1CCA
hill1CCB
sky1CCC
. 1BA
house2BB
blue2BC
, 2CA
is3CB
the4AAB
C A
blueB A C
, house is .B
B A C
on hill skytheCHình 1: Cây Huffman tam phân minh họa phương pháp của chúng tôi. Cây được xây dựng với Thuật toán 1 từ tần suất từ, được hiển thị như
chỉ số trong ánh xạ (phải), dựa trên văn bản sau: "the house is on the hill, the house is blue, the sky is blue."
Số lượng ký hiệu mã hóa ntương ứng với kích
thước từ vựng của hệ thống NMT (số lượng đơn vị
đầu vào hoặc chỉ số). Mỗi node có tối đa ncon,
mỗi con được gắn nhãn với một ký hiệu. Mỗi kiểu
từ xuất hiện ở phía nguồn của dữ liệu huấn luyện
(sau đó, tương ứng, ở phía đích) được đặt trên một
lá của cây, và các ký hiệu trên đường dẫn dẫn đến
nó cung cấp biểu diễn của nó với từ vựng ký hiệu
mới. Ví dụ, nếu 'the' ở lá xuất phát từ nhánh thứ
10 của gốc, nó sẽ được mã hóa với ký hiệu #10,
trong khi nếu 'control' có thể được đạt đến thông
qua nhánh thứ 123 sau đó nhánh thứ 54, nó sẽ được
mã hóa với hai ký hiệu, #123#54. Dù giá trị của
n>=2 là gì, một cây Huffman có thể mã hóa một
số lượng từ lớn tùy ý, nhưng cây trở nên sâu hơn
khi ngiảm.
Chúng tôi sử dụng một triển khai mã nguồn mở
của một thuật toán xây dựng cây Huffman.4Chúng
tôi đã sửa đổi mã để làm cho nó có thể áp dụng cho
từ thay vì ký tự, và để tạo ra một cây n-ary thay vì
cây nhị phân, dẫn đến Thuật toán 1 ở trên. Cấu
trúc dữ liệu chính là một hàng đợi ưu tiên với các
node và điểm số, luôn được sắp xếp theo điểm số
tăng dần, và được khởi tạo với các kiểu từ và tần
suất của chúng từ dữ liệu huấn luyện.
Khi Thuật toán 1 được chạy, mỗi node của cây
kết quả có tối đa ncon, do đó chúng ta có thể liên
kết các ký hiệu với mỗi node, đệ quy thực hiện
cùng thao tác cho bất kỳ node nào với nhãn 'None'
(tức là không phải lá, có nhãn từ). Ở cuối phân bổ
này, mọi node đều có một mã ký hiệu duy nhất,
là sự nối tiếp của các ký hiệu từ các nhánh dẫn
đến nó. Các lá gần gốc hơn có mã ngắn hơn các
lá sâu hơn. Thư viện của chúng tôi5hỗ trợ văn bản
đầu vào lớn, tạo ánh xạ giữa từ và ký hiệu, và cho
phép mã hóa và giải mã văn bản.
4Có sẵn tại github.com/bhrigu123/huffman-coding và được
giải thích trong một bài blog (Srivastava, 2017).
5Có sẵn tại github.com/heig-iict-ida/huffman-tokenizer.3.3 Thuộc tính của Phương pháp của chúng tôi
Trước NMT, các mã được tạo ra bởi thuật toán
Huffman được phân đoạn thành các ký hiệu tạo
thành chúng. Do đó, kích thước từ vựng là n, số
lượng ký hiệu. Trong cây Huffman, các từ thường
xuyên nhất sẽ xuất hiện như các lá gần gốc. Do đó,
trong ánh xạ kết quả, các từ thường xuyên nhất sẽ
được biểu diễn với một ký hiệu duy nhất , và những
từ ít thường xuyên hơn sẽ sử dụng nhiều ký hiệu
hơn, điều này được coi là một trong những ưu điểm
của BPE (xem Phần 2.2).
Tuy nhiên, không giống như BPE, chúng tôi
không phân đoạn từ thành subword, do đó chúng
tôi không tính đến tính tổng hợp của subword ,
theo nghĩa là các từ bắt đầu với tiền tố tương tự
không được mã hóa thành mã Huffman bắt đầu với
các ký hiệu tương tự. Ví dụ, tính tổng hợp của BPE
có nghĩa là nếu 'restor', 'ing' và 'ation' là subword,
thì hệ thống NMT có thể sử dụng kiến thức về bản
dịch của 'restoring' để dịch 'restoration' (giả sử
chúng được tokenize thành 'restor' + 'ing' và 'restor'
+ 'ation') vì cả hai từ đều chia sẻ một tiền tố chung,
có ý nghĩa. Nhưng nếu hai mã Huffman chia sẻ
cùng tiền tố, như '#10#32' và '#10#76#25', kiến
thức về bản dịch của '#10' không thể được tái sử
dụng từ từ này sang từ khác, vì các từ gốc không
liên quan. Đây là lý do tại sao nghiên cứu của chúng
tôi định lượng tiện ích của tần suất một mình, bằng
cách tách nó khỏi tính tổng hợp.
Ngoài ra, vì cây Huffman được xây dựng trên
các từ trong dữ liệu huấn luyện, nó không thể mã
hóa các từ chưa biết trong dữ liệu kiểm tra, một
hiệu ứng sẽ được định lượng dưới đây.

--- TRANG 6 ---
4 Dữ liệu và Hệ thống
Chúng tôi thí nghiệm với một số cặp ngôn ngữ có
tiếng Séc, Đức, Anh và Pháp. Dữ liệu huấn luyện
và kiểm tra chủ yếu đến từ WMT 2014 (Bojar et
al., 2014) và WMT 2019 (Barrault et al., 2019) và
cũng bao gồm dữ liệu JW300 (Agi ´c và Vuli ´c,
2019). Dữ liệu tiếng Séc-Đức được hiển thị trong
Bảng 1, dữ liệu tiếng Anh-Đức trong Bảng 2 và
dữ liệu tiếng Anh-Pháp trong Bảng 3. Chúng tôi
lấy mẫu ngẫu nhiên từ mỗi subcorpus 0.1-0.2%
câu để làm dữ liệu kiểm tra. Phân chia đặc biệt này
được cung cấp với thư viện của chúng tôi, để có
thể tái tạo.
Tập dữ liệu Số lượng dòng
News Commentary v14 172,995
Europarl v9 556,182
JW300 1,052,338
Newstest 2019 1,997
Tổng cộng 1,783,512
Huấn luyện / Kiểm tra 1,780,068 / 3,444
Bảng 1: Dữ liệu song song tiếng Séc-Đức (dòng không trống).
Tập dữ liệu Số lượng dòng
Common Crawl 2,399,123
Europarl v7 1,911,843
News Commentary v11 241,094
Tổng cộng 4,552,060
Huấn luyện / Kiểm tra 4,547,445 / 4,615
Bảng 2: Dữ liệu song song tiếng Anh-Đức (dòng không trống).
Tập dữ liệu Số lượng dòng
Common Crawl 3,244,152
Europarl v7 2,005,688
Tổng cộng 5,249,840
Huấn luyện / Kiểm tra 5,245,392 / 4,448
Bảng 3: Dữ liệu song song tiếng Anh-Pháp (dòng không trống).
Chúng tôi sử dụng các mô hình NMT Transformer
(Vaswani et al., 2017) từ thư viện OpenNMT-py
(Klein et al., 2017) phiên bản 2.3.0.6Chúng tôi
huấn luyện các mô hình trong 150.000 bước, mất
khoảng một ngày trên hai CPU NVIDIA GeForce
RTX 2080 Ti với bộ nhớ 11 GB. Các siêu tham số
của các mô hình, thường là mặc định, được đưa ra
trong Phụ lục A. Chúng tôi đánh giá chất lượng
dịch thuật bằng điểm số BLEU (Papineni et al.,
2002), điểm số ChrF (Popovi ´c, 2015) như được
triển khai bởi SacreBLEU (Post, 2018)7và điểm số
COMET (Rei et al., 2020). Chúng tôi tính điểm số
BLEU thu được bởi mỗi checkpoint (mỗi 10.000
bước) trên tập kiểm tra, và chọn checkpoint có điểm
cao nhất, trên đó chúng tôi đo ChrF và COMET
cũng như.
6github.com/OpenNMT/OpenNMT-py
7Chữ ký điểm số BLEU: nrefs:1|case:mixed|eff:no|
tok:13a|smooth:exp|version:2.2.1
5 NMT Sử dụng Mã hóa Huffman: Kết quả
Trong phần này, chúng tôi cho thấy rằng mã hóa
Huffman là một phương pháp tokenization khả thi,
chúng tôi nghiên cứu tác động của số lượng ký hiệu
có sẵn đối với chất lượng dịch thuật, và so sánh
phương pháp với một baseline hoàn toàn dựa trên
tần suất.
Trước tiên chúng tôi điều tra cách chất lượng
dịch thuật thay đổi theo kích thước từ vựng, đây
là siêu tham số chính của phương pháp. Nếu có
nhiều ký hiệu, thì nhiều từ thường xuyên sẽ được
mã hóa với một ký hiệu duy nhất. Ngược lại, ít ký
hiệu hơn dẫn đến hầu hết các từ được mã hóa với
hai hoặc nhiều ký hiệu. Hình 2 dưới đây minh họa
thuộc tính này cho mã Huffman so với BPE.
Các điểm số thu được với mã hóa Huffman trên
CS-DE NMT với các số lượng ký hiệu khác nhau,
được hiển thị trong năm dòng đầu tiên của Bảng 5,
chứng minh rằng phương pháp này hoạt động và
nó được hưởng lợi từ số lượng ký hiệu tăng lên.
Khi số lượng ký hiệu có sẵn rất thấp, hiệu ứng đối
với tokenization gần hơn với dịch thuật dựa trên
ký tự, ngoại trừ một số từ thường xuyên vẫn được
mã hóa trên một ký hiệu với Huffman, trong khi
hầu như tất cả các từ đều chứa hai ký tự trở lên.
Không được hiển thị trong bảng, điểm số BLEU
với 1.000 ký hiệu là 19.6, rất gần với điểm số
BLEU của Transformer dựa trên ký tự sử dụng từ
vựng 485 ký tự, là 19.4. Tuy nhiên, điểm số tốt
nhất của chúng tôi được tìm thấy cho kích thước
từ vựng cao hơn, tương tự như những kích thước
được sử dụng với BPE (như được thảo luận trong
Phần 6 dưới đây), có nghĩa là về mặt khái niệm
chúng tôi gần hơn với subword hơn là mô hình
dựa trên ký tự.
Chúng tôi nghiên cứu ảnh hưởng của một số siêu
tham số đối với điểm số CS-DE BLEU khi sử dụng
1.000 ký hiệu cho mã hóa Huffman. Như được hiển
thị trong Bảng 4, kích thước embedding nhỏ hơn
(từ 512 xuống 64) dẫn đến điểm số BLEU thấp hơn
đáng kể. Tăng số lượng lớp Transformer từ 8 lên
20 dường như tăng điểm số, điều này phù hợp với
các phát hiện của Gupta et al. (2019) cho NMT
dựa trên ký tự. Tuy nhiên, hiệu ứng không mạnh,
và vì chi phí huấn luyện tăng đáng kể, chúng tôi
tiếp tục sử dụng Transformer 8 lớp khi so sánh với
BPE. Cuối cùng, chúng tôi lưu ý rằng số lượng
attention head (8, 16, hoặc 32) hầu như không có
ảnh hưởng đến điểm số.
Kích thước Emb. Lớp Head BLEU
512 8 8 19.6
256 - - 17.6
128 - - 14.4
64 - - 9.7
512 12 8 19.6
- 16 - 21.4
- 20 - 21.4
512 8 16 19.3
- - 32 19.4
Bảng 4: Điểm số BLEU với mã hóa Huffman 1000 ký hiệu
khi thay đổi kích thước embedding của Transformer, số
lượng lớp, và số lượng attention head ('-' có nghĩa là "giống
như trên").
Chúng tôi cũng so sánh mã hóa Huffman với một
baseline đơn giản giữ các token thường xuyên nhất
như ký hiệu trong văn bản huấn luyện nguồn và
đích, tất cả các tham số khác đều bằng nhau. Giữ
16k token dẫn đến điểm số BLEU 17.0 (so với 22.3
cho Huffman với 16k ký hiệu) và giữ 32k token
dẫn đến 19.1 điểm BLEU (so với 23.1 cho Huffman
với 32k ký hiệu). Vì các điểm số mới chỉ thấp hơn
4-5 điểm, chúng tôi kết luận rằng tập trung vào các
token thường xuyên nhất bảo tồn một số hiệu quả,
đặc biệt là khi từ vựng phát triển, nhưng bị hạn chế
bởi thực tế là tất cả các token khác đều bị bỏ qua.

--- TRANG 7 ---
1000 2000 4000 8000 16000 3200005000000100000001500000020000000250000003000000035000000
1
2
3
4
2000 4000 8000 16000 320000500000010000000150000002000000025000000
1
2
3
4
5
6
7
8
9
10Hình 2: Biểu đồ tần suất số lượng token từ dữ liệu CS được phân đoạn thành 1, 2, hoặc nhiều ký hiệu hơn, cho mã hóa Huffman (trái) so với BPE (phải). Sáu kích thước từ vựng khác nhau được hiển thị cho mã hóa Huffman (từ 1k đến 32k ký hiệu) và năm cho BPE (từ 2k đến 32k merge). Trong khi mã hóa Huffman sử dụng tối đa 4 ký hiệu mỗi token, BPE có thể sử dụng tối đa 10 subword.
6 Mã hóa Huffman so với BPE
Để so sánh BPE với các mô hình Huffman, chúng
tôi tokenize phía nguồn và đích cùng nhau cho mỗi
cặp sử dụng BPE từ bộ công cụ SentencePiece (xem
chú thích 1) với số lượng merge tăng dần: 2k, 4k,
8k, 16k và 32k.
Trước tiên chúng tôi so sánh các từ vựng kết quả
từ BPE với những từ vựng từ mã hóa Huffman về
số lượng ký hiệu mỗi token. Biểu đồ tần suất số
lượng token có tương ứng 1, 2, 3, và tới 10 ký hiệu
/ subword được hiển thị trong Hình 2, cạnh nhau
cho mã hóa Huffman (trái) và cho BPE (phải), cho
mỗi kích thước từ vựng. Khi kích thước từ vựng
(hoặc số lượng ký hiệu) phát triển, kết quả token-
ization trở nên tương tự hơn giữa hai phương pháp,
với hơn 3/4 token được giữ như ký hiệu duy nhất.
Trong khi hai là số lượng ký hiệu tối đa mỗi token
cho mã hóa Huffman, theo cấu trúc, chúng ta thấy
rằng đối với BPE một số token được phân đoạn
thành 3 hoặc nhiều subword hơn (tới 10, mặc dù
số lượng của chúng quá nhỏ để được thấy trong
hình). Những quan sát này hỗ trợ tuyên bố của
chúng tôi rằng mã hóa Huffman nắm bắt thông tin
liên quan đến tần suất tương tự như BPE, trong khi
theo thiết kế nó không nắm bắt tính tổng hợp.
Chuyển sang điểm số NMT, Bảng 5 so sánh
những điểm số của các mô hình dựa trên BPE với
đối tác Huffman của chúng cho ba cặp ngôn ngữ và
ba chỉ số. Chúng tôi quan sát rằng tăng số lượng
merge BPE có tác động tích cực nhưng khá hạn
chế trong thiết lập này, với cải thiện chỉ 2 điểm
BLEU giữa các mô hình 2k và 32k tốt nhất. Trên
tất cả các cặp ngôn ngữ, điểm số Huffman và BPE
trở nên tương tự hơn khi số lượng ký hiệu tăng,
như được hiển thị trong cột '%' cho biết tỷ lệ giữa
điểm số Huffman và BPE (với một ngoại lệ, EN-DE
với 32k ký hiệu). Vượt quá 8k ký hiệu, phương
pháp của chúng tôi thu được giữa 86.1% và 91.4%
điểm số BLEU của BPE cho tất cả các cặp ngôn
ngữ, và thậm chí các phần cao hơn cho ChrF (giữa
90.6% và 95.1%) và COMET (giữa 92.2% và 96.0%).
Tuy nhiên, các mô hình BPE luôn vượt trội hơn
đối tác Huffman tương đương 2-3 điểm BLEU cho
tất cả các cặp ngôn ngữ.

--- TRANG 8 ---
Cặp Nb. của BLEU ChrF COMET
ngôn ngữ ký hiệu Huffman BPE % Huffman BPE % Huffman BPE %
CS-DE 2k 20.3 24.4 83.2 46.6 52.6 88.6 0.758 0.829 91.4
4k 20.9 24.8 84.3 47.2 53.2 88.7 0.762 0.833 91.4
8k 21.6 25.1 86.1 48.4 53.4 90.6 0.780 0.834 93.6
16k 22.3 24.8 89.9 49.3 53.3 92.5 0.791 0.830 95.2
32k 23.1 26.4 87.5 50.2 54.5 92.1 0.804 0.837 96.0
EN-DE 8k 19.5 22.4 87.1 46.4 49.7 93.4 0.709 0.769 92.2
16k 20.3 22.2 91.4 46.6 49.3 94.5 0.718 0.768 93.5
32k 19.8 22.5 88.0 46.9 49.5 94.7 0.712 0.772 92.2
EN-FR 8k 27.1 31.2 86.9 51.1 55.3 92.4 0.728 0.783 93.0
16k 27.6 30.9 89.3 51.8 55 94.2 0.739 0.781 94.6
32k 27.9 30.9 90.3 52.2 54.9 95.1 0.746 0.784 95.1
Bảng 5: Chất lượng dịch thuật đạt được bởi các mô hình Huffman và BPE với số lượng ký hiệu tăng dần.
Chúng tôi quy những khác biệt này cho thực tế
rằng mã hóa Huffman chỉ dựa vào tần suất để chọn
số lượng subword mỗi token, và không được hưởng
lợi từ tính tổng hợp. Chúng tôi diễn giải kết quả
như một định lượng tầm quan trọng của tần suất so
với tính tổng hợp trong tokenization subword, với
một phần lớn hiệu suất cuối cùng đến từ tần suất
và sự khác biệt còn lại (giữa 4 và 14 điểm phần
trăm tùy thuộc vào chỉ số) đối với tính tổng hợp
và khả năng xử lý các từ chưa biết. Một lý do khác
cho sự khác biệt còn lại là thực tế rằng từ vựng
BPE được xây dựng cùng nhau trên dữ liệu nguồn
và đích, không giống như phương pháp của chúng
tôi.
Cuối cùng, các từ chưa biết cũng có khả năng
hạn chế hiệu suất của mã hóa Huffman, mặc dù
số lượng của chúng rất nhỏ trong dữ liệu kiểm tra.
Có 0.55% token chưa biết trong nguồn CS cho CS-
DE, 0.46% trong nguồn EN cho EN-DE, và không
có trong nguồn EN cho EN-FR. Thú vị là, ở phía
giải mã, phần lớn các kết hợp ký hiệu được tạo ra
bởi các mô hình NMT của chúng tôi tương ứng với
các lá thực tế của cây Huffman: tỷ lệ phần trăm
các kết hợp ký hiệu chưa biết trong tổng số token
đầu ra tương ứng là 0.07%, 0.04% và 0.02% cho
CS-DE, EN-DE và EN-FR. Các kết hợp như vậy
không thể được giải mã và do đó bị bỏ qua.
7 Kết luận
Trong bài báo này, chúng tôi đã trình bày một
phương pháp gốc cho tokenization văn bản, khai
thác thuộc tính nén văn bản của cây Huffman, và
do đó tính đến tần suất của subword, nhưng không
dựa vào tính tổng hợp của chúng. Chúng tôi đã
đóng khung những khái niệm này và, dựa trên việc
so sánh các điểm số thu được với mã hóa Huffman
với những điểm số thu được với BPE, chúng tôi đã
bảo vệ tuyên bố rằng hầu hết các cải thiện được
mang lại bởi BPE là do sự xem xét thích hợp của
tần suất subword, và tương đối ít hơn nhiều đối với
tính tổng hợp. Những kết quả này có xu hướng hạ
thấp tầm quan trọng của tính tổng hợp, thường
được đề cập như một ưu điểm của BPE, và góp
phần vào việc hiểu hiệu quả đáng chú ý của phương
pháp này.
Chúng tôi giả thuyết rằng các phương pháp nén
văn bản có thể cung cấp cảm hứng, trong tương lai,
cho các phương pháp tokenization thậm chí hiệu
quả hơn, cho rằng hiện trạng trong nén đã có
tiến bộ đáng kể kể từ BPE. Đặc biệt, Prediction
by Partial Matching dường như là một ứng viên
đầy hứa hẹn, nhưng đang chờ một giải pháp có
nguyên tắc để liên quan token với ký hiệu mã hóa.
Lời cảm ơn
Chúng tôi biết ơn sự hỗ trợ nhận được từ Arma-
suisse (dự án UNISUB: Unsupervised NMT with
Innovative Multilingual Subword Models) và từ
Quỹ Khoa học Quốc gia Thụy Sĩ (dự án DOMAT:
On-demand Knowledge for Document-level Ma-
chine Translation, n. 175693).

--- TRANG 9 ---
Chúng tôi cảm ơn bốn người đánh giá EAMT ẩn
danh vì những gợi ý của họ, và ông Bhrigu Sriva-
stava vì đã chia sẻ việc triển khai mã hóa Huffman
của ông.
Tài liệu tham khảo
[Agi ´c và Vuli ´c2019] Agi ´c,ˇZeljko và Ivan Vuli ´c.
2019. JW300: Một corpus song song có phạm vi
rộng cho các ngôn ngữ ít tài nguyên. Trong Kỷ
yếu Hội nghị thường niên lần thứ 57 của Hiệp hội
Ngôn ngữ học Tính toán , trang 3204-3210.
[Amrhein và Sennrich2020] Amrhein, Chantal và
Rico Sennrich. 2020. Về La-tinh hóa cho chuyển
giao mô hình giữa các chữ viết trong dịch máy thần
kinh. Trong Findings của Hiệp hội Ngôn ngữ học
Tính toán: EMNLP 2020 , trang 2461-2469.
[Barrault et al.2019] Barrault, Lo ¨ıc, Ond ˇrej Bojar,
Marta R. Costa-juss `a, Christian Federmann, Mark
Fishel, Yvette Graham, Barry Haddow, Matthias
Huck, Philipp Koehn, Shervin Malmasi, Christof
Monz, Mathias M ¨uller, Santanu Pal, Matt Post,
và Marcos Zampieri. 2019. Findings của Hội nghị
Dịch máy 2019 (WMT19). Trong Kỷ yếu Hội
nghị Dịch máy lần thứ tư , trang 1-61.
[Bojar et al.2014] Bojar, Ond ˇrej, Christian Buck, Chris-
tian Federmann, Barry Haddow, Philipp Koehn, Jo-
hannes Leveling, Christof Monz, Pavel Pecina, Matt
Post, Herve Saint-Amand, Radu Soricut, Lucia Spe-
cia, và Ale ˇs Tamchyna. 2014. Findings của Hội
thảo Dịch máy Thống kê 2014. Trong Kỷ yếu Hội
thảo Dịch máy Thống kê lần thứ chín , trang 12-58.
[Bojar et al.2015] Bojar, Ond ˇrej, Rajen Chatterjee,
Christian Federmann, Barry Haddow, Matthias
Huck, Chris Hokamp, Philipp Koehn, Varvara Lo-
gacheva, Christof Monz, Matteo Negri, Matt Post,
Carolina Scarton, Lucia Specia, và Marco Turchi.
2015. Findings của Hội thảo Dịch máy Thống kê
2015. Trong Kỷ yếu Hội thảo Dịch máy Thống kê
lần thứ mười , trang 1-46.
[Cherry et al.2018] Cherry, Colin, George Foster, Ankur
Bapna, Orhan Firat, và Wolfgang Macherey. 2018.
Xem xét lại dịch máy thần kinh dựa trên ký tự với
khả năng và nén. Trong Kỷ yếu Hội nghị 2018 về
Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ
Tự nhiên , trang 4295-4305.
[Chitnis và DeNero2015] Chitnis, Rohan và John
DeNero. 2015. Mã hóa từ có độ dài thay đổi cho
các mô hình dịch thần kinh. Trong Kỷ yếu Hội
nghị 2015 về Phương pháp Thực nghiệm trong Xử
lý Ngôn ngữ Tự nhiên , trang 2088-2093.
[Denkowski và Neubig2017] Denkowski, Michael và
Graham Neubig. 2017. Baseline mạnh hơn cho
kết quả đáng tin cậy trong dịch máy thần kinh.
Trong Kỷ yếu Hội thảo đầu tiên về Dịch máy Thần
kinh , trang 18-27.
[Gage1994] Gage, Philip. 1994. Một thuật toán mới
cho nén dữ liệu. Lưu trữ tạp chí C Users , 12:23-38.
[Gall ´e2019] Gall ´e, Matthias. 2019. Điều tra hiệu
quả của BPE: Sức mạnh của các chuỗi ngắn hơn.
Trong Kỷ yếu Hội nghị 2019 về Phương pháp Thực
nghiệm trong Xử lý Ngôn ngữ Tự nhiên và Hội
nghị Quốc tế lần thứ 9 về Xử lý Ngôn ngữ Tự nhiên
(EMNLP-IJCNLP) , trang 1375-1381.
[Gupta et al.2019] Gupta, Rohit, Laurent Besacier,
Marc Dymetman, và Matthias Gall ´e. 2019.
NMT dựa trên ký tự với Transformer.
[Huffman1952] Huffman, David A. 1952. Một phương
pháp xây dựng mã có độ dư thừa tối thiểu. Kỷ yếu
IRE , 40(9):1098-1101.
[Jean et al.2015] Jean, S ´ebastien, Kyunghyun Cho,
Roland Memisevic, và Yoshua Bengio. 2015. Về
việc sử dụng từ vựng đích rất lớn cho dịch máy
thần kinh. Trong Kỷ yếu Hội nghị thường niên lần
thứ 53 của Hiệp hội Ngôn ngữ học Tính toán ,
trang 1-10.
[Klein et al.2017] Klein, Guillaume, Yoon Kim, Yun-
tian Deng, Jean Senellart, và Alexander Rush.
2017. OpenNMT: Bộ công cụ nguồn mở cho dịch
máy thần kinh. Trong Kỷ yếu ACL 2017, Demon-
strations Hệ thống , trang 67-72.
[Koehn et al.2007] Koehn, Philipp, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade
Shen, Christine Moran, Richard Zens, Chris Dyer,
Ondˇrej Bojar, Alexandra Constantin, và Evan
Herbst. 2007. Moses: Bộ công cụ nguồn mở cho
dịch máy thống kê. Trong Kỷ yếu Hội nghị thường
niên lần thứ 45 của Hiệp hội Ngôn ngữ học Tính
toán, Demo và Poster Sessions , trang 177-180.
[Kudo và Richardson2018] Kudo, Taku và John
Richardson. 2018. SentencePiece: Một subword
tokenizer và detokenizer đơn giản và độc lập ngôn
ngữ cho xử lý văn bản thần kinh. Trong Kỷ yếu
Hội nghị 2018 về Phương pháp Thực nghiệm trong
Xử lý Ngôn ngữ Tự nhiên: System Demonstrations
, trang 66-71.
[Kudo2018] Kudo, Taku. 2018. Điều chỉnh subword:
Cải thiện các mô hình dịch mạng thần kinh với
nhiều ứng viên subword. Trong Kỷ yếu Hội nghị
thường niên lần thứ 56 của Hiệp hội Ngôn ngữ
học Tính toán , trang 66-75.
[Lee et al.2017] Lee, Jason, Kyunghyun Cho, và
Thomas Hofmann. 2017. Dịch máy thần kinh hoàn
toàn dựa trên ký tự mà không có phân đoạn rõ
ràng. Transactions của Hiệp hội Ngôn ngữ học
Tính toán , 5:365-378.

--- TRANG 10 ---
[Libovick ´y et al.2022] Libovick ´y, Jind ˇrich, Helmut
Schmid, và Alexander Fraser. 2022. Tại sao mọi
người không sử dụng dịch máy cấp độ ký tự?
Trong Findings của Hiệp hội Ngôn ngữ học Tính
toán: ACL 2022 , trang 2470-2485.
[Luong và Manning2016] Luong, Minh-Thang và
Christopher D. Manning. 2016. Đạt được dịch máy
thần kinh từ vựng mở với các mô hình từ-ký tự
lai. Trong Kỷ yếu Hội nghị thường niên lần thứ 54
của Hiệp hội Ngôn ngữ học Tính toán , trang 1054-
1063.
[Luong et al.2015] Luong, Thang, Ilya Sutskever, Quoc
Le, Oriol Vinyals, và Wojciech Zaremba. 2015.
Giải quyết vấn đề từ hiếm trong dịch máy thần
kinh. Trong Kỷ yếu Hội nghị thường niên lần thứ
53 của Hiệp hội Ngôn ngữ học Tính toán , trang
11-19.
[Papineni et al.2002] Papineni, Kishore, Salim Roukos,
Todd Ward, và Wei-Jing Zhu. 2002. BLEU: một
phương pháp đánh giá tự động dịch máy. Trong
Kỷ yếu Hội nghị thường niên lần thứ 40 của Hiệp
hội Ngôn ngữ học Tính toán , trang 311-318.
[Patil et al.2022] Patil, Vaidehi, Partha Talukdar, và
Sunita Sarawagi. 2022. Tạo từ vựng dựa trên
chồng chéo cải thiện chuyển giao liên ngôn ngữ
giữa các ngôn ngữ liên quan. Trong Kỷ yếu Hội
nghị thường niên lần thứ 60 của Hiệp hội Ngôn
ngữ học Tính toán , trang 219-233.
[Popovi ´c2015] Popovi ´c, Maja. 2015. chrF: điểm số
F n-gram ký tự cho đánh giá MT tự động. Trong
Kỷ yếu Hội thảo Dịch máy Thống kê lần thứ mười
, trang 392-395.
[Post2018] Post, Matt. 2018. Một lời kêu gọi rõ ràng
trong báo cáo điểm số BLEU. Trong Kỷ yếu Hội
nghị Dịch máy lần thứ ba: Research Papers, trang
186-191.
[Provilkov et al.2020] Provilkov, Ivan, Dmitrii Emelia-
nenko, và Elena V oita. 2020. BPE-dropout: Điều
chỉnh subword đơn giản và hiệu quả. Trong Kỷ
yếu Hội nghị thường niên lần thứ 58 của Hiệp hội
Ngôn ngữ học Tính toán , trang 1882-1892.
[Rei et al.2020] Rei, Ricardo, Craig Stewart, Ana C Far-
inha, và Alon Lavie. 2020. COMET: Một khung
thần kinh cho đánh giá MT. Trong Kỷ yếu Hội
nghị 2020 về Phương pháp Thực nghiệm trong Xử
lý Ngôn ngữ Tự nhiên (EMNLP) , trang 2685-2702.
[Schuster và Nakajima2012] Schuster, Mike và
Kaisuke Nakajima. 2012. Tìm kiếm giọng nói
Nhật Bản và Hàn Quốc. Trong Hội nghị Quốc tế
IEEE 2012 về Âm thanh, Giọng nói và Xử lý Tín
hiệu (ICASSP) , trang 5149-5152.
[Sennrich et al.2016] Sennrich, Rico, Barry Haddow,
và Alexandra Birch. 2016. Dịch máy thần kinh của
các từ hiếm với các đơn vị subword. Trong Kỷ yếu
Hội nghị thường niên lần thứ 54 của Hiệp hội
Ngôn ngữ học Tính toán (Tập 1: Bài báo Dài) ,
trang 1715-1725.
[Srivastava2017] Srivastava, Bhrigu. 2017. Triển khai
Python mã hóa Huffman. Blog Cá nhân .
[Unicode Consortium2022] Unicode Consortium, The.
2022. Tiêu chuẩn Unicode, Phiên bản 15.0.0 . The
Unicode Consortium.
[Vaswani et al.2017] Vaswani, Ashish, Noam Shazeer,
Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N
Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017.
Attention is all you need. Trong Guyon, I., U. V on
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, và R. Garnett, editors, Tiến bộ trong
Xử lý Thông tin Thần kinh , tập 30.
[Vernikos và Popescu-Belis2021] Vernikos, Giorgos
và Andrei Popescu-Belis. 2021. Ánh xạ subword
và neo liên ngôn ngữ. Trong Findings của Hiệp
hội Ngôn ngữ học Tính toán: EMNLP 2021 , trang
2633-2647.
[Wu et al.2016] Wu, Yonghui, Mike Schuster, Z. Chen,
Quoc V . Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao,
Klaus Macherey, Jeff Klingner, Apurva Shah,
Melvin Johnson, Xiaobing Liu, Lukasz Kaiser,
Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason R. Smith, Ja-
son Riesa, Alex Rudnick, Oriol Vinyals, Gregory S.
Corrado, Macduff Hughes, và Jeffrey Dean. 2016.
Hệ thống dịch máy thần kinh của Google: Bắc
cầu khoảng cách giữa dịch thuật của con người
và máy móc. ArXiv , abs/1609.08144.
Phụ lục A. Tham số của OpenNMT-py
Các siêu tham số chúng tôi sử dụng cho các thí
nghiệm với OpenNMT-py như sau:
• Số lượng lớp: 8
• Số lượng head: 8
• Kích thước embedding: 512
• Kích thước feed-forward Transformer: 2048
• Kích thước batch: 2.000 token
• Optimizer: Adam
• Hệ số learning rate: 2.0
• Bước warmup: 8.000
• Tỷ lệ dropout: 0.1
