# 1911.12385.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/1911.12385.pdf
# Kích thước tệp: 5192248 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020
DEFINE: BIỂU DIỄN TOKEN ĐẦU VÀO ĐƯỢC PHÂN TÍCH NHÂN TỬ SÂU
CHO MÔ HÌNH HÓA CHUỖI THẦN KINH
Sachin Mehta1, Rik Koncel-Kedziorski1, Mohammad Rastegari1, và Hannaneh Hajishirzi1,2
1Đại học Washington2Viện Allen về AI
TÓM TẮT
Đối với các mô hình chuỗi có từ vựng lớn, phần lớn các tham số mạng nằm ở các lớp đầu vào và đầu ra. Trong công trình này, chúng tôi mô tả một phương pháp mới, DeFINE, để học biểu diễn token sâu một cách hiệu quả. Kiến trúc của chúng tôi sử dụng cấu trúc phân cấp với các kết nối bỏ qua mới cho phép sử dụng các lớp đầu vào và đầu ra có chiều thấp, giảm tổng số tham số và thời gian huấn luyện trong khi vẫn mang lại hiệu suất tương tự hoặc tốt hơn so với các phương pháp hiện có. DeFINE có thể được tích hợp dễ dàng vào các mô hình chuỗi mới hoặc hiện có. So với các phương pháp hiện đại bao gồm biểu diễn đầu vào thích ứng, kỹ thuật này dẫn đến giảm 6% đến 20% độ bối rối. Trên WikiText-103, DeFINE giảm tổng số tham số của Transformer-XL xuống một nửa với tác động tối thiểu đến hiệu suất. Trên Penn Treebank, DeFINE cải thiện AWD-LSTM 4 điểm với việc giảm 17% tham số, đạt được hiệu suất tương đương với các phương pháp hiện đại với ít tham số hơn. Đối với dịch máy, DeFINE cải thiện hiệu quả của mô hình Transformer khoảng 1:4 lần trong khi vẫn mang lại hiệu suất tương tự.

1 GIỚI THIỆU
Các mô hình thần kinh cho các tác vụ NLP, như mô hình hóa ngôn ngữ và dịch máy, yêu cầu từ vựng lớn để có tính tổng quát (Chelba et al., 2013; Bahdanau et al., 2015; Luong et al., 2015; Merity et al., 2017). Những mô hình này thường sử dụng kiến trúc tương tự: các token (ví dụ: từ, từ con, hoặc ký tự), được biểu diễn dưới dạng vector một chiều, được ánh xạ tới không gian liên tục dày đặc; sau đó chúng được xử lý bởi một mô hình ngữ cảnh; cuối cùng, các biểu diễn được ngữ cảnh hóa được ánh xạ trở lại thành vector có kích thước từ vựng để tính toán xác suất token tiếp theo. Một ví dụ về mô hình hóa ngôn ngữ được hiển thị trong Hình 1a. Việc ánh xạ trong bước đầu và cuối thường sử dụng bảng tra cứu được học chung, được gọi là lớp nhúng, nhận mỗi token trong từ vựng thành một vector m chiều cố định. Một nhược điểm của cách tiếp cận này là số lượng tham số trong lớp nhúng tăng theo kích thước từ vựng tăng, hạn chế chúng ta ở các giá trị m nhỏ trên từ vựng lớn. Các nhà nghiên cứu đã tìm cách cải thiện hiệu quả của lớp nhúng bằng cách gán cho các token có tần suất thấp hơn các vector có chiều nhỏ hơn, tuy nhiên, việc giảm tham số đáng kể lại phải trả giá bằng hiệu suất (Morin & Bengio, 2005; Grave et al., 2017a; Baevski & Auli, 2019). Trong tất cả các cách tiếp cận này, nhúng token được xấp xỉ bằng hàm tuyến tính từ token sang vector.

Trong công trình này, chúng tôi giới thiệu Deep Factorized INput token Embeddings (DeFINE) cho mô hình hóa chuỗi thần kinh. DeFINE xấp xỉ hàm nhúng token phức tạp với ít tham số hơn nhiều so với các phương pháp tiêu chuẩn. DeFINE cho phép ánh xạ đầu vào và đầu ra có chiều thấp hơn trong các mô hình chuỗi, giảm gánh nặng tính toán của chúng mà không làm giảm hiệu suất. Các biểu diễn được tạo ra bởi DeFINE mạnh mẽ hơn so với các kỹ thuật phân tích nhân tử khác và thậm chí cả các lớp nhúng tiêu chuẩn. Để thực hiện điều này, DeFINE tận dụng biến đổi nhóm phân cấp (HGT) học biểu diễn sâu một cách hiệu quả và hiệu quả. HGT kết nối các tập con khác nhau của đầu vào bằng các kết nối thưa và dày đặc. Để cải thiện luồng thông tin, DeFINE giới thiệu kết nối bỏ qua mới thiết lập liên kết trực tiếp với lớp đầu vào ở mọi cấp độ của hệ thống phân cấp, cho phép gradient chảy trở lại trực tiếp tới đầu vào qua nhiều đường dẫn. DeFINE thay thế các lớp nhúng tiêu chuẩn, để lại phần còn lại của mô hình không bị ảnh hưởng, và

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020
Lớp 
NhúngTransformer-XLLớp 
Phân loại
/g1875/g3047/g1875/g3047/g2878/g2869
ℝ/g3040/g3400/g2869ℝ/g3040/g3400/g2869
DeFINE
Lớp 
NhúngTransformer-XLLớp 
Phân loại
/g1875/g3047/g1875/g3047/g2878/g2869
ℝ/g3041/g3400/g2869
ℝ/g3040/g3400/g2869
ℝ/g3041/g3400/g2869/g1865 ≫ /g1866Chia sẻ
Trọng số
Chia sẻ
Trọng số
Đầu vàoMô hình
Ngữ cảnhĐầu ra
(a) Transformer-XL không có và có DeFINE
1.92
020406080100120
DeFINE Mô hình
ngữ cảnhĐầu vào-Đầu ra
(chia sẻ)Tham số 
(triệu)Transformer-XL
Transformer-XL +
DeFINE (b) Phân bố tham số trên WikiText-103
Hình 1: Với DeFINE, Transformer-XL học biểu diễn đầu vào (nhúng) và đầu ra (phân loại) trong không gian n chiều thấp thay vì không gian m chiều cao, do đó giảm đáng kể tham số trong khi có tác động tối thiểu đến hiệu suất.

do đó nó có thể được sử dụng với nhiều loại kiến trúc mô hình hóa chuỗi và loại token, bao gồm từ và từ con. Hình 1 cho thấy cách chúng tôi tích hợp DeFINE với Transformer-XL (Dai et al., 2019), một mô hình ngôn ngữ dựa trên Transformer hiện đại, và việc giảm tổng số tham số kết quả.

Các thí nghiệm của chúng tôi cho thấy cả mô hình chuỗi dựa trên LSTM và Transformer đều được hưởng lợi từ việc sử dụng DeFINE. Hơn nữa, các thí nghiệm của chúng tôi với mô hình hóa ngôn ngữ cấp từ và các tác vụ dịch máy cấp từ con cho thấy DeFINE có thể được sử dụng với các loại token khác nhau. Trên tập dữ liệu Wikitext-103, mô hình ngôn ngữ dựa trên LSTM với DeFINE cung cấp cải thiện 9 điểm so với mô hình dung lượng đầy đủ trong khi sử dụng một nửa số tham số. Khi kết hợp với biểu diễn đầu vào thích ứng (Baevski & Auli, 2019) và đầu ra (Grave et al., 2017a), DeFINE cải thiện hiệu suất khoảng 3 điểm trên các mô hình ngôn ngữ dựa trên LSTM (xem Bảng 1a) và dựa trên Transformer-XL (xem Bảng 2) với việc tăng tối thiểu tham số huấn luyện. Thời gian tính toán khi suy luận không bị ảnh hưởng.¹ Tích hợp DeFINE vào mô hình ngôn ngữ AWD-LSTM phổ biến (Merity et al., 2018b) mà không cần tinh chỉnh dẫn đến độ bối rối kiểm tra 54.2 trên tập dữ liệu Penn Treebank, vượt trội so với cả mô hình AWD-LSTM gốc và được tinh chỉnh cũng như Transformer-XL và MoS (Yang et al., 2018). Đối với dịch máy, DeFINE cải thiện hiệu quả của mô hình Transformer (Vaswani et al., 2017) 26% trong khi duy trì chất lượng dịch. Chúng tôi cung cấp các thí nghiệm thực chất chi tiết tác động của các quyết định kiến trúc và chứng minh hiệu quả của DeFINE trên các mô hình có dung lượng khác nhau.

2 CÔNG TRÌNH LIÊN QUAN
Nhiều tác vụ mô hình hóa chuỗi, bao gồm mô hình hóa ngôn ngữ và dịch máy, có từ vựng lớn. Kết quả là, phần lớn tham số của mô hình nằm ở các lớp đầu vào (hoặc nhúng) và đầu ra (hoặc phân loại). Để giảm tải tính toán từ các lớp này, Press & Wolf (2017) và Inan et al. (2017) giới thiệu cơ chế hiệu quả gọi là chia sẻ trọng số cho phép học biểu diễn đầu vào và đầu ra cùng nhau trong khi giảm đáng kể số lượng tham số mạng. Để tiếp tục giảm tải tính toán từ các lớp này, các phương pháp dựa trên phân tích nhân tử, như nhúng chiếu (Dai et al., 2019), nhúng nhóm (Chen et al., 2018; Grave et al., 2017a; Goodman, 2001; Mnih & Hinton, 2009; Morin & Bengio, 2005), và nhúng mỏng (Li et al., 2018), đã được đề xuất. Nhúng chiếu xấp xỉ ma trận nhúng lớn bằng hai ma trận nhỏ hơn trong khi nhúng nhóm phân cụm token đầu vào theo tần suất và gán dung lượng khác nhau cho các cụm khác nhau bằng các phương pháp nhúng chiếu. Chúng tôi lưu ý rằng nhúng chiếu là trường hợp đặc biệt của nhúng nhóm khi số cụm là một. Phương pháp đầu vào thích ứng của Baevski & Auli (2019) tổng quát hóa các phương pháp nhúng chiếu và nhóm và đề xuất phương pháp phân tích nhân tử cho phép huấn luyện đầu-cuối nhanh hơn, hiệu quả bộ nhớ trong khi cung cấp lợi ích tương tự hoặc tốt hơn so với các phương pháp sau huấn luyện hiện có yêu cầu ma trận nhúng được huấn luyện trước (Chen et al., 2018). Khác với nhúng chiếu và nhóm, Li et al. (2018) mở rộng biến đổi nhóm (Kuchaiev & Ginsburg, 2017; Mehta et al., 2018) với thuật toán xáo trộn của Fisher & Yates (1943) để phân tích nhân tử các lớp này. Các kỹ thuật khác như học mã hóa (Shu & Nakayama, 2017; Chen et al., 2016; Acharya et al., 2019) và lượng tử hóa (Rastegari et al., 2016; Hubara et al., 2017) có thể được sử dụng để cải thiện thêm hiệu quả, đặc biệt về yêu cầu lưu trữ. DeFINE trực giao với các phương pháp này; kết quả thực nghiệm của chúng tôi trong Phần 4 cho thấy hiệu suất được cải thiện so với các phương pháp này riêng lẻ.

Những tiến bộ gần đây trong mô hình hóa chuỗi, như Transformer và RNN đa lớp, chứng minh sức mạnh của kiến trúc sâu trong NLP (Jozefowicz et al., 2016; Vaswani et al., 2017; Merity et al., 2018a). Nhưng trong khi sự chú ý đáng kể đã được dành cho việc mô hình hóa tương tác giữa các token với kiến trúc sâu (ví dụ: ELMo (Peters et al., 2018) và BERT (Devlin et al., 2019)), biểu diễn token không phụ thuộc ngữ cảnh thường được mô hình hóa chỉ với thống kê kho ngữ liệu (Pennington et al., 2014) hoặc một biến đổi tuyến tính duy nhất (Mikolov et al., 2013; McCann et al., 2017). Mô hình cấp ký tự (Kim et al., 2016) cũng tạo ra biểu diễn sâu của từ như một tích chập trên ký tự, tuy nhiên những mô hình này thường yêu cầu dung lượng nhiều hơn để mang lại hiệu suất tương đương với mô hình cấp từ hoặc từ con (Baevski & Auli, 2019). Tuy nhiên, DeFINE có thể được sử dụng để học biểu diễn sâu của nhiều loại token, bao gồm từ, ký tự, hoặc từ con (mã hóa cặp byte) (Sennrich et al., 2015).

3 DEFINE
Nhúng token thường được coi như hàm đơn giản của vector một chiều thành không gian liên tục dày đặc. Do đó, lớp nhúng có thể được coi như mạng rộng, nông bao gồm một biến đổi tuyến tính duy nhất. Về cốt lõi, hàm mà mạng này xấp xỉ (gọi là f) nhận một token từ dạng chính tả của nó thành biểu diễn của những thuộc tính hình thái cú pháp và ngữ nghĩa có liên quan để mô hình hóa số lượng tùy ý các ngữ cảnh mà token có thể xuất hiện. Hầu hết nghiên cứu NLP giả định rằng lớp nhúng đơn giản có thể xấp xỉ đủ hàm f khó giải. Chúng tôi giả thuyết rằng, do độ phức tạp của f, mạng nông sẽ yêu cầu dung lượng đặc biệt để học xấp xỉ tốt. Ràng buộc về thời gian và dữ liệu cấm học mạng nông có dung lượng cao như vậy. Chúng tôi đề xuất, dựa trên kết quả lý thuyết gần đây của Liang & Srikant (2017),² rằng mạng sâu hơn có thể xấp xỉ f với ít tham số hơn đáng kể so với mạng nông. Tính hợp lệ của giả định này được chứng minh bằng kết quả thực nghiệm của chúng tôi trong Phần 4.

Trong công trình này, chúng tôi giới thiệu DeFINE, một cách hiệu quả để học biểu diễn token sâu trong không gian chiều cao với tối thiểu tham số bổ sung. Phương pháp của chúng tôi dựa trên nguyên lý Ánh xạ-Mở rộng-Giảm (MER), được mô tả trong Phần 3.1, đầu tiên ánh xạ token đầu vào thành vector nhúng chiều thấp, sau đó biến đổi nó thành không gian chiều cao bằng biến đổi nhóm phân cấp hiệu quả tính toán (HGT, Phần 3.2), được phác thảo trong Hình 2c. Vector kết quả sau đó được biến đổi thành không gian chiều thấp. Trong quá trình các biến đổi này, chúng tôi sử dụng mẫu kết nối mới thiết lập liên kết trực tiếp giữa các lớp đầu vào và đầu ra (Hình 3), thúc đẩy tái sử dụng đặc trưng và cải thiện luồng gradient (Phần 3.3). Lớp đầu ra của DeFINE sau đó có thể được sử dụng thay cho nhúng truyền thống như đầu vào cho các tác vụ mô hình hóa chuỗi. Chúng tôi chi tiết các khía cạnh khác nhau của kiến trúc bên dưới.

3.1 NGUYÊN LÝ ÁNH XẠ-MỞ RỘNG-GIẢM (MER)
Bước đầu tiên trong MER, Ánh xạ, tương tự như các mô hình chuỗi tiêu chuẩn. Mỗi token đầu vào trong từ vựng V được ánh xạ thành vector có chiều cố định ei ∈ Rn×1. Tuy nhiên, trong trường hợp của chúng tôi, giá trị n nhỏ (ví dụ 64 hoặc 128, so với chiều thông thường là 400 trở lên). Bước tiếp theo, Mở rộng,

² Liang & Srikant (2017) chứng minh rằng, đối với một lớp hàm lớn, số lượng neuron cần thiết bởi mạng nông để xấp xỉ một hàm lớn hơn theo cấp số nhân so với số lượng neuron tương ứng cần thiết bởi mạng sâu. Chúng tôi giả định rằng f thuộc lớp hàm này.

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

/g1875/g3047Mở rộng Ánh xạ
(a) LT
/g1875/g3047 (b) GLT
/g1875/g3047 (c)HGTLớp #tham số
LTN1∑P
l=0nlkl
GLTN1∑P
l=0nlkl=g
HGTN1∑P
l=0nlkl=gl
(d)
Hình 2: Học biểu diễn token bằng các lớp biến đổi khác nhau với N= 3. (a) Biến đổi Tuyến tính (b) Biến đổi tuyến tính nhóm (GLT) (c) HGT (xem văn bản để biết chi tiết). Ở đây, N là tổng số lớp, nl và kl là chiều đầu vào và đầu ra của lớp thứ l, gl là số nhóm trong lớp thứ l, và g là số nhóm cố định trong biến đổi tuyến tính nhóm.

nhận ei làm đầu vào và áp dụng biến đổi nhóm phân cấp (HGT) để tạo ra vector có chiều rất cao ê ei ∈ Rk×1, trong đó k >> n. Khác với chồng các lớp kết nối đầy đủ, HGT học biểu diễn sâu hiệu quả từ các tập con khác nhau của đầu vào bằng các kết nối thưa và dày đặc. Bước cuối, Giảm, chiếu vector ê ei thành không gian chiều thấp hơn để tạo ra vector nhúng cuối cùng eo ∈ Rm×1 cho token đầu vào đã cho. Chiều của eo có thể được khớp với các mô hình biểu diễn ngữ cảnh, như LSTM hoặc Transformer, cho phép DeFINE phục vụ như lớp đầu vào cho các mô hình này.

3.2 BIẾN ĐỔI NHÓM PHÂN CẤP (HGT)
Chúng tôi giới thiệu biến đổi nhóm phân cấp (HGT), được phác thảo trong Hình 2c, để học biểu diễn token sâu hiệu quả. HGT bao gồm chồng N lớp. Ở mỗi lớp, HGT sử dụng số lượng nhóm khác nhau cho phép nó học biểu diễn từ các tập con khác nhau của đầu vào. HGT bắt đầu với gmax nhóm ở lớp đầu tiên và sau đó liên tiếp giảm số lượng nhóm theo hệ số 2 ở mỗi cấp độ. Cơ chế nhóm phân cấp này làm thưa các kết nối trong các lớp kết nối đầy đủ (hoặc tuyến tính) và cho phép chúng ta học biểu diễn hiệu quả với ít tham số hơn. Tương tự như chồng các lớp kết nối đầy đủ, lớp thứ N trong HGT có quyền truy cập vào mọi phần tử đầu vào của lớp đầu tiên thông qua nhiều đường dẫn, do đó cho phép nó học biểu diễn hiệu quả. Biến đổi tuyến tính nhóm (GLT), ban đầu được giới thiệu để cải thiện hiệu quả của LSTM, cũng làm thưa các kết nối trong các lớp kết nối đầy đủ và giảm đáng kể chi phí tính toán (Kuchaiev & Ginsburg, 2017; Mehta et al., 2018). Tuy nhiên, nếu chúng ta chồng nhiều lớp GLT, đầu ra của một nhóm nhất định chỉ được lấy từ một phần nhỏ của đầu vào, do đó học biểu diễn yếu. Cơ chế nhóm phân cấp trong HGT cho phép lớp thứ N lấy dữ liệu đầu vào từ nhiều đường dẫn, cho phép HGT học biểu diễn mạnh hơn. So sánh các biến đổi khác nhau được đưa ra trong Hình 2. Chúng ta có thể thấy rằng HGT vừa hiệu quả vừa có quyền truy cập tốt hơn vào đầu vào. Lưu ý rằng biến đổi tuyến tính và tuyến tính nhóm là trường hợp đặc biệt của HGT khi gl = 1 và gl = g (cố định), tương ứng.

Để biến đổi ei ∈ Rn×1 thành ê ei ∈ Rk×1, HGT đầu tiên lấy mẫu không gian giữa n và k một cách tuyến tính để xây dựng N lớp trung gian có chiều tăng dần. Do đó, vector đầu ra được tạo ra bởi lớp thứ l+ 1 sẽ có chiều cao hơn lớp thứ l. Giả sử rằng các chiều vector được phân cách tuyến tính chia hết cho gmax, chúng ta biến đổi ei thành ê ei như sau:
ê eli = FG(ei; Wl; gl); l = 1
FG(ê el-1i; Wl; gl); 1< l ≤ N (1)
trong đó gl = max(⌊gmax/2l-1⌋; 1), Wl là trọng số được học ở lớp thứ l, và FG là hàm biến đổi nhóm được định nghĩa trong Mehta et al. (2018). Biến đổi nhóm chia đầu vào thành g nhóm, mỗi nhóm được xử lý độc lập bằng biến đổi tuyến tính. Đầu ra của các nhóm này sau đó được nối lại để tạo ra đầu ra cuối cùng. Xem Phần A.1 để biết chi tiết.

3.3 ĐƠN VỊ DEFINE
Đơn vị DeFINE bao gồm các biến đổi HGT được thiết kế bằng nguyên lý MER. Mặc dù các lớp HGT là xấp xỉ hiệu quả cho các lớp kết nối đầy đủ tốn kém tính toán, chúng có thể cản trở huấn luyện khi độ sâu N của đơn vị DeFINE tăng. Kết nối dư (He et al., 2016) đã chứng minh rất hiệu quả trong việc giảm thiểu vấn đề này, tuy nhiên, các kết nối như vậy khó thực hiện trong HGT vì chiều đầu vào và đầu ra của mỗi lớp khác nhau.

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Chia(/g1859/g2870=2)Trộn
/g1875/g3047Lớp - 2
/g894/g1859/g2870=2)
Lớp - 1
/g894/g1859/g2869=4)
Hình 3: Đơn vị DeFINE với N= 2 sử dụng HGT để học biểu diễn token đầu vào hiệu quả và kết nối trực tiếp với đầu vào để tối đa hóa luồng thông tin.

Để tối đa hóa luồng thông tin và hỗ trợ huấn luyện với các đơn vị DeFINE sâu hơn, chúng tôi giới thiệu kết nối bỏ qua mới đơn giản thiết lập liên kết trực tiếp giữa bất kỳ lớp nào trong HGT với đầu vào ei. Hình 3 trực quan hóa đơn vị DeFINE với độ sâu hai (N=2). Để cho phép các kết nối thưa trong HGT có quyền truy cập vào đầu vào ei và đầu ra của lớp trước (ê el-1i), chúng tôi chia đầu vào và đầu ra thành gl nhóm bằng lớp chia. Các vector đầu vào và đầu ra được chia sau đó được trộn sao cho phần đầu tiên của đầu vào và phần đầu tiên của đầu ra lớp thứ l-1 được đặt cùng nhau làm đầu vào cho biến đổi nhóm đầu tiên trong lớp thứ l và tiếp tục cho đến khi gl đầu vào đã được xây dựng. Vector kết quả sau đó được đưa vào lớp thứ l. Cơ chế này thúc đẩy tái sử dụng đặc trưng đầu vào hiệu quả. Ngoài ra, nó thiết lập liên kết trực tiếp với đầu vào ei, cho phép gradient chảy trở lại đầu vào qua nhiều đường dẫn và dẫn đến hiệu suất được cải thiện.

3.4 DEFINE CHO MÔ HÌNH HÓA CHUỖI
Đơn vị DeFINE có thể được tích hợp dễ dàng với bất kỳ mô hình chuỗi mới hoặc hiện có nào. Các mô hình chuỗi thường bao gồm chồng lớp đầu vào (nhúng hoặc lớp đầu vào thích ứng), mô hình ngữ cảnh (ví dụ: LSTM hoặc Transformer), và lớp phân loại (kết nối đầy đủ hoặc softmax thích ứng). Vì DeFINE học biểu diễn token sâu, chúng ta có thể dễ dàng chồng nó ngay sau đầu vào. Một ví dụ được hiển thị trong Hình 1, nơi DeFINE được tích hợp với Transformer-XL, một mô hình ngôn ngữ hiện đại. DeFINE cho phép sử dụng chiều tương đối thấp hơn trong lớp đầu vào, do đó giảm tham số mạng.

Các biểu diễn token đầu vào, ei, ê ei, và eo, mà mô hình thần kinh học cho mỗi token độc lập với các token khác. Điều này cho phép chúng ta tạo bảng tra cứu độc lập khác (sau khi huấn luyện mô hình) lưu trữ ánh xạ giữa token đầu vào và đầu ra của đơn vị DeFINE (eo), dẫn đến cơ chế cho phép bỏ qua các tính toán của đơn vị DeFINE tại thời điểm suy luận.

4 KẾT QUẢ THỰC NGHIỆM
Chúng tôi chứng minh hiệu suất của DeFINE trên hai tác vụ mô hình hóa chuỗi: mô hình hóa ngôn ngữ (Phần 4.1) và dịch máy (Phần 4.2). Chúng tôi so sánh hiệu suất của DeFINE với các phương pháp phân tích nhân tử và nén hiện có trong Phần 4.3. Chúng tôi cũng cung cấp các ablation trong Phần 4.4 để cho thấy hiệu quả của các quyết định thiết kế. Trong suốt phần này, chúng tôi sử dụng

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Cấu hình Phân bố Tham số (triệu) Huấn luyện Độ bối rối
Hàng Đầu vào-Đầu ra Độ sâu của Chiều của DeFINE Ngữ cảnh Đầu vào-Đầu ra Tổng Thời gian Val Test
# Lớp DeFINE (N) ei(n) mô hình (chia sẻ) (ms/batch)
R1? Tiêu chuẩn - 256 0.00 23.36 68.81 92.17 1150 43.24 44.12
R2 Thích ứng - 256 0.00 23.36 9.25 32.61 297 43.49 44.87
R3 Thích ứng + DeFINE 3 256 0.41 23.36 9.25 33.02 298 39.99 41.17
R4 Thích ứng + DeFINE 7 384 1.83 24.73 13.90 40.46 364 36.95 38.01
R5 Thích ứng + DeFINE 11 512 3.89 26.24 18.55 48.69 459 34.94 35.94
(a) Mô hình ngôn ngữ dựa trên LSTM (của chúng tôi) trên WT103.? Cho thí nghiệm này, chúng tôi sử dụng hai GPU.

Mô hình #Tham số Độ bối rối
(triệu) (Test)
Grave et al. (2017b)-LSTM - 48.7
Grave et al. (2017b)-LSTM + Neural Cache - 40.8
Merity et al. (2018a) - QRNN 151 M 33.0
LSTM + DeFINE (Chúng tôi) 48.69 M 35.94
(b) So sánh với các công trình hiện có trên WT-103

Mô hình #Tham số Độ bối rối
(triệu) Val Test
AWD-LSTM (Merity et al., 2018b) 24 M 61.2 58.8
AWD-LSTM + Tinh chỉnh 24 M 58.8 56.5
AWD-LSTM-MoS (Yang et al., 2018) 22 M 58.1 56.0
AWD-LSTM-MoS + Tinh chỉnh 22 M 56.5 54.4
Transformer-XL (Dai et al., 2019) 24 M - 54.5
AWD-LSTM + DeFINE (Chúng tôi) 20 M 56.5 54.2
(c) So sánh với các công trình hiện có trên tập dữ liệu PTB

Bảng 1: Hiệu suất của mô hình ngôn ngữ dựa trên RNN trên tập dữ liệu WT-103 và PTB. Trong (a), tiêu chuẩn đề cập đến lớp nhúng và phân loại tiêu chuẩn (tuyến tính) trong khi thích ứng đề cập đến đầu vào thích ứng và softmax thích ứng cho lớp đầu vào và đầu ra, tương ứng.

ký hiệu sau: n, k, và m là chiều của ei, ê ei, và eo tương ứng, và N đại diện cho độ sâu của DeFINE.

4.1 MÔ HÌNH HÓA NGÔN NGỮ
Trong phần này, chúng tôi nghiên cứu hiệu suất của các mô hình với mô hình ngôn ngữ dựa trên LSTM và Transformer trên hai tập dữ liệu: WikiText-103 (Merity et al., 2017) và Penn Treebank (Marcus et al., 1994). Trên cả hai tập dữ liệu, chúng tôi cho thấy DeFINE hiệu quả về tham số và cải thiện hiệu suất của các mô hình ngôn ngữ hiện có.

4.1.1 WIKITEXT-103 (WT-103)
Dữ liệu và mô hình: Tập dữ liệu WikiText-103 (Merity et al., 2017) bao gồm 103M/217K/245K token cho tập huấn luyện, xác thực và kiểm tra tương ứng và có kích thước từ vựng khoảng 260K. Tập dữ liệu này bao gồm các bài viết Wikipedia và giữ lại dấu câu, số và chữ hoa. Để đánh giá hiệu quả của DeFINE, chúng tôi nghiên cứu hai loại mô hình ngữ cảnh khác nhau: LSTM và Transformer (Transformer-XL (Dai et al., 2019)). Chúng tôi đo hiệu suất của các mô hình này về độ bối rối, một chỉ số tiêu chuẩn cho mô hình hóa ngôn ngữ. Giá trị độ bối rối thấp hơn cho thấy hiệu suất tốt hơn. Theo các công trình gần đây, bao gồm Merity et al. (2018a), Baevski & Auli (2019), và Dai et al. (2019), chúng tôi sử dụng đầu vào thích ứng làm hàm ánh xạ trong DeFINE và softmax thích ứng cho phân loại với trọng số chia sẻ. Xem A.3 để biết thêm chi tiết.

Kết quả của mô hình ngôn ngữ dựa trên LSTM: Bảng 1 tóm tắt kết quả của mô hình ngôn ngữ dựa trên LSTM. Mặc dù các phương pháp đầu vào thích ứng (Baevski & Auli, 2019) và đầu ra (Grave et al., 2017a) hiệu quả và giảm số lượng tham số đáng kể, phương pháp của chúng tôi tiếp tục cải thiện hiệu suất khoảng 3 điểm trong khi học chỉ 1.25% (hoặc 0.4 triệu) tham số thêm. Điều quan trọng cần lưu ý là độ phức tạp tính toán của các mô hình trong R2 và R3 giống nhau vì phương pháp của chúng tôi cho phép lưu trữ đầu ra của DeFINE để sử dụng khi suy luận (xem Phần 3.4).

Khi chúng tôi tăng độ sâu của DeFINE từ 3 đến 11 lớp (Bảng 1b)³, hiệu suất cải thiện thêm 6 điểm, mang lại hiệu suất cạnh tranh với các phương pháp dựa trên RNN hiện có với ít tham số hơn (ví dụ: 1/3 số tham số so với Merity et al. (2018a)). Hiệu suất của mô hình chúng tôi tốt hơn các phương pháp hiện có như Dauphin et al. (2017) và Bai et al. (2018).

Kết quả của mô hình dựa trên Transformer: Bảng 2 so sánh hiệu suất của Transformer-XL, một mô hình dựa trên Transformer hiện đại, có và không có DeFINE. Bảng 2a cho thấy phương pháp của chúng tôi có thể đạt được hiệu suất tương tự như Dai et al. (2019) trong khi học ít hơn 10M tham số. Điều thú vị cần lưu ý là DeFINE cho phép chúng ta giảm gánh nặng tính toán từ các lớp đầu vào và đầu ra một lượng lớn với tác động tối thiểu đến hiệu suất. Với DeFINE, hiệu suất của Transformer-XL chỉ giảm khoảng 2 điểm trong khi số lượng tham số được giảm 50%. Đối với việc giảm tương tự về số lượng tham số, hiệu suất của Transformer-XL gốc giảm 5 điểm, cho thấy phương pháp đề xuất để học biểu diễn cấp từ hiệu quả. Bảng 2b nổi bật thực tế rằng Transformer-XL với DeFINE có thể đạt được độ bối rối tương đương với Transformer-XL tiêu chuẩn với nhúng chiếu trong khi sử dụng ít tham số hơn đáng kể.

³ Chúng tôi tăng tỷ lệ chiều đầu vào và đầu ra để tăng đồng đều độ phức tạp mạng.

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Mô hình Đầu vào-Đầu ra Chiều Phân bố Tham số (triệu) Huấn luyện Độ bối rối
của ei DeFINE Ngữ cảnh Đầu vào-Đầu ra Tổng Thời gian Val Test
Lớp (n) mô hình (chia sẻ) (ms/batch)
Transformer-XL? Tiêu chuẩn 410 0.00 41.07 110.04 151.11 894 - 24.03
Transformer-XL Tiêu chuẩn 384 0.00 36.25 103.08 139.33 855 26.10 27.06
Transformer-XL DeFINE 384 1.92 36.25 103.08 141.25 860 23.59 24.17
Transformer-XL Chiếu 256 0.00 36.25 69.20 105.45 714 27.18 28.09
Transformer-XL DeFINE 256 1.92 36.25 69.20 107.37 721 24.81 25.72
Transformer-XL Chiếu 128 0.00 36.25 34.73 70.98 600 28.06 29.16
Transformer-XL DeFINE 128 1.92 36.25 34.73 72.90 606 25.43 26.33
Transformer-XL Chiếu 64 0.00 36.25 17.50 53.75 550 32.94 33.74
Transformer-XL DeFINE 64 1.92 36.25 17.50 55.67 553 28.03 29.10
(a) So sánh nhóm theo lớp ánh xạ ei.

Mô hình Tham số (triệu) Độ bối rối
Transformer-XL (Tiêu chuẩn) 139.33 M 27.06
Transformer-XL (DeFINE) 72.90 M 26.33
Transformer-XL (Chiếu) 70.98 M 29.16
Transformer-XL (DeFINE) 55.67 M 29.10
(b) So sánh nhóm theo độ bối rối tương tự.

Bảng 2: Hiệu suất Transformer-XL trên tập dữ liệu Wikitext-103. Chúng tôi sử dụng DeFINE với N= 3, k= 4096, và m= 384. Đối với các mô hình không có DeFINE, chúng tôi sử dụng nhúng chiếu (Dai et al., 2019) chiếu tuyến tính vector ei thành chiều m= 384. Ngoại trừ hàng được đánh dấu với ? sử dụng chiều mô hình bên trong 2100, tất cả các hàng khác sử dụng chiều mô hình bên trong 1920. Số tốt nhất trong mỗi nhóm trong Bảng 2a được tô đỏ trong khi số tốt nhất tổng thể được đánh dấu đậm. Bảng 2a cho thấy việc thêm DeFINE cải thiện đáng kể kết quả với chi phí thấp; Bảng 2b cho thấy việc giảm tham số bằng DeFINE cho hiệu suất tương tự.

4.1.2 PENN TREEBANK (PTB)
Dữ liệu và mô hình: Tập dữ liệu Penn Treebank (Marcus et al., 1994) chứa khoảng 929K/74K/82K token trong tập huấn luyện, xác thực và kiểm tra tương ứng. Nó có kích thước từ vựng khoảng 10K. Theo các công trình gần đây, chúng tôi sử dụng phiên bản đã xử lý được cung cấp bởi Mikolov et al. (2010). Để đánh giá hiệu quả của mô hình, chúng tôi so sánh với AWD-LSTM (Merity et al., 2018b). Mô hình của chúng tôi thay thế lớp nhúng trong AWD-LSTM bằng đơn vị DeFINE với các thiết lập sau: n= 128, k= 1024, N= 7, và m= 400. Chúng tôi sử dụng cùng siêu tham số và phiên bản PyTorch như AWD-LSTM gốc.

Kết quả: Kết quả được tóm tắt trong Bảng 1c. Phương pháp đề xuất cải thiện hiệu suất của AWD-LSTM 4 điểm trong khi đồng thời giảm số lượng tham số 4 triệu. Không cần tinh chỉnh, AWD-LSTM + DeFINE đạt được hiệu suất tương đương với các phương pháp hiện đại, bao gồm Transformer-XL, với ít tham số hơn.

4.2 DỊCH MÁY
Dữ liệu và mô hình: Chúng tôi sử dụng tập dữ liệu WMT 2014 English-German (EN-DE) (Luong et al., 2015) để huấn luyện. Theo Vaswani et al. (2017), chúng tôi mã hóa câu bằng mã hóa cặp byte (Britz et al., 2017) và sử dụng newstest2014 và newstest2017 làm tập xác thực và kiểm tra, tương ứng. Chúng tôi tích hợp DeFINE với mô hình Transformer hiện đại (Vaswani et al., 2017) với các tham số sau: n= 128, k= 1024, m= 512, và N= 3. Chúng tôi sử dụng triển khai trong OpenNMT-py (Klein et al., 2017) để huấn luyện và đánh giá với các siêu tham số được khuyến nghị.

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Mô hình Checkpoint Tham số BLEU (EN-DE)
Trung bình? (triệu) newstest2014 newstest2017
Transformer (Vaswani et al., 2017) ✓ - 27.30 -
Transformer + SRU (Lei et al., 2018) ✓ 90 M 27.1 28.30
Transformer (OpenNMT impl.) (Klein et al., 2017) ✓ 92 M 26.89 28.09
Transformer ✗ 92 M 25.01 25.81
Transformer + DeFINE ✗ 68 M 27.01 28.25

Bảng 3: Kết quả của mô hình dựa trên Transformer (có và không có DeFINE) trên tác vụ dịch máy thần kinh. DeFINE đạt được hiệu suất tương tự với trung bình checkpoint, nhưng với ít tham số hơn.

Mô hình Tác vụ Lớp Đầu vào-Đầu ra Tham số Hiệu suất
Chuỗi (triệu)
LSTM Mô hình hóa Ngôn ngữ Tiêu chuẩn 92 M 44.12
(Bảng 1a) Thích ứng 33 M 44.87
DeFINE 33 M 41.17
AWD-LSTM Mô hình hóa Ngôn ngữ Tiêu chuẩn 24 M 58.8
(Bảng 1c) DeFINE 20 M 54.2
Transformer-XL Mô hình hóa Ngôn ngữ Tiêu chuẩn 139 M 27.06
(Bảng 2) Chiếu 71 M 29.16
DeFINE 73 M 26.33
Transformer Dịch Máy Tiêu chuẩn 92 M 25.81
(Bảng 3) DeFINE 68 M 28.25

Bảng 4: So sánh hiệu suất của các mô hình chuỗi khác nhau với các phương pháp phân tích nhân tử khác nhau. Phương pháp phân tích nhân tử chiếu và thích ứng đề cập đến các phương pháp trong Dai et al. (2019) và Baevski & Auli (2019), tương ứng. Đối với mô hình hóa ngôn ngữ, hiệu suất được đo bằng độ bối rối; đối với dịch máy, BLEU được sử dụng.

Kết quả: Bảng 3 tóm tắt kết quả. DeFINE cải thiện hiệu suất của mô hình Transformer không có trung bình checkpoint 2% trong khi đồng thời giảm tổng số tham số 26%, cho thấy DeFINE hiệu quả.

4.3 SO SÁNH VỚI CÁC PHƯƠNG PHÁP KHÁC NHAU
Phương pháp dựa trên phân tích nhân tử: Bảng 4 so sánh hiệu suất của các phương pháp phân tích nhân tử khác nhau cho các mô hình chuỗi khác nhau. Với DeFINE, hiệu suất và hiệu quả của các mô hình chuỗi cải thiện trên các tác vụ khác nhau. Điều này có thể là do đầu ra của DeFINE xấp xỉ gần hơn mẫu tương quan của lớp nhúng tiêu chuẩn so với các nhúng khác (xem Hình 4 và Phụ lục B). Hơn nữa, chúng ta thấy rằng các tương quan mạnh giữa các chiều trong lớp ánh xạ của DeFINE được giảm trong quá trình các lớp mở rộng (xem Hình 8, 9, và 10 trong Phụ lục). Hình 11 trong Phụ lục cho thấy các nhóm trong lớp mở rộng của DeFINE không tương quan, cho thấy các ma trận này đang học biểu diễn khác nhau của đầu vào.

Tác động của các phương pháp dựa trên nén: Các phương pháp dựa trên nén cho phép rời rạc hóa hiệu quả các vector nhúng liên tục 32-bit độ chính xác đầy đủ, do đó giảm dấu chân bộ nhớ của lớp đầu vào. Với DeFINE, chúng ta cũng học vector nhúng liên tục 32-bit điểm nổi độ chính xác đầy đủ (tương tự như Baevski & Auli (2019) và Dai et al. (2019)). Do đó, các phương pháp dựa trên nén, như (Shu & Nakayama, 2017), có thể được áp dụng cho các mô hình chuỗi với DeFINE và các phương pháp phân tích nhân tử khác. Bảng 5 cho thấy nhúng DeFINE có thể được nén tương tự như nhúng tiêu chuẩn mà không mất hiệu suất.

4.4 NGHIÊN CỨU ABLATION TRÊN TẬP DỮ LIỆU WIKITEXT-103
Trong phần này, chúng tôi cung cấp phân tích về các lựa chọn thiết kế bằng mô hình ngôn ngữ dựa trên LSTM. Trong các ablation, chúng tôi chọn mô hình ngôn ngữ dựa trên LSTM thay vì Transformer vì chúng ít

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

(a) Tiêu chuẩn (không phân tích nhân tử)
(b) Chiếu (Dai et al., 2019)
(c) DeFINE (Chúng tôi)
Hình 4: Bản đồ tương quan (m×m) của các lớp nhúng khác nhau được sử dụng trong Transformer-XL với n= 128 và m= 384 trên WikiText-103. DeFINE có thể xấp xỉ ma trận nhúng tiêu chuẩn hiệu quả. Nhiều trực quan hóa hơn được bao gồm trong Phụ lục B.

Chiều của Lớp Đầu vào-Đầu ra Nén Kích thước Bảng tra cứu Độ bối rối Thời gian Suy luận
ei(n) Được sử dụng? (MB) (ms/batch)
384 Tiêu chuẩn Không 411 27.06 202
384 Tiêu chuẩn Có 21 27.36 201
128 Chiếu Không 127 29.16 129
128 Chiếu Có 21 29.82 129
128 DeFINE Không 127 26.33 131
128 DeFINE Có 21 26.03 130

Bảng 5: Hiệu suất của Transformer-XL với các phương pháp phân tích nhân tử khác nhau, có và không có phương pháp nén của Shu & Nakayama (2017). Đối với nén, chúng tôi sử dụng mã hóa 32 x 16 được mô tả trong Shu & Nakayama (2017).

nhạy cảm với siêu tham số và có thể được huấn luyện trên một GPU. Chúng tôi sử dụng cùng siêu tham số để huấn luyện như được mô tả trong Phần 4.1.1, cụ thể là N= 7, n= 384, k= 1024, và m= 384.

Tác động của các biến đổi khác nhau: Bảng 6 tóm tắt kết quả của chúng tôi. HGT hiệu quả như biến đổi tuyến tính trong khi học ít hơn hai triệu tham số. So với biến đổi tuyến tính nhóm (GLT), HGT cải thiện độ bối rối khoảng 5 điểm trong khi học số lượng tham số tương tự. Hơn nữa, khi chúng ta thiết lập kết nối trực tiếp với đầu vào (xem Phần 3.2 để biết chi tiết), hiệu suất tiếp tục cải thiện 2.9 điểm với tác động tối thiểu đến số lượng tham số, cho thấy DeFINE học biểu diễn tốt.

Tác động của tăng tỷ lệ độ sâu (N) và chiều rộng (k): Bảng 7 tóm tắt kết quả của các thí nghiệm tăng tỷ lệ. Đối với cùng giá trị k, hiệu suất của mô hình ngôn ngữ cải thiện với việc tăng độ sâu N. Tuy nhiên, khi chúng ta tăng chiều rộng k cho giá trị độ sâu N cố định, hiệu suất không cải thiện. Điều này có thể là do khi chúng ta tăng kích thước k, nhiều neuron hơn nhận đầu vào từ cùng tập con chiều và do đó học nhiều tham số dư thừa.

DeFINE với các kết nối khác nhau: Bảng 8a chứng minh tác động của kết nối dư trong DeFINE. Để hỗ trợ kết nối dư bên trong DeFINE, chúng tôi cố định chiều của mỗi lớp ê eli trong DeFINE thành k/2 thay vì phân bố tuyến tính từ n đến k. Chúng ta có thể thấy rõ ràng rằng kết nối bỏ qua được đề xuất hiệu quả hơn.

Tác động của thao tác giảm trong MER: Trong chiến lược MER (Phần 3.1), chúng tôi chiếu vector chiều cao thành không gian chiều thấp trước khi đưa nó vào mô hình ngữ cảnh, như LSTM. Chúng tôi thấy thực nghiệm rằng hiệu suất có và không có bước giảm này tương tự, tuy nhiên, mô hình không có bước giảm học nhiều tham số hơn (Bảng 8b).

5 KẾT LUẬN
DeFINE sử dụng mạng sâu, phân cấp, thưa với kết nối bỏ qua mới để học nhúng token tốt hơn một cách hiệu quả. Các mô hình chuỗi với DeFINE (ví dụ: Transformer và LSTM) hoạt động tương đương hoặc tốt hơn với các phương pháp hiện đại với ít tham số hơn. Các thí nghiệm của chúng tôi cho thấy

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Lớp #Tham số Độ bối rối
(triệu) Val Test
Tuyến tính 42.86 39.89 41.19
GLT 39.69 44.28 45.63
GLT + Xáo trộn 39.69 44.08 45.25
HGT 40.73 39.79 40.92
(a) Các biến đổi khác nhau (xem Hình 2)

Lớp #Tham số Độ bối rối
(triệu) Val Test
HGT 40.73 39.79 40.92
DeFINE (không có trộn) 40.89 37.84 38.91
DeFINE 40.89 36.95 38.01
(b) HGT so với DeFINE

Bảng 6: So sánh giữa các biến đổi khác nhau trên tập dữ liệu WikiText-103.

Độ sâu của Chiều của #Tham số Độ bối rối
DeFINE (N) ei(n) eo(m) êei(k) (triệu) Val Test
3 256 256 1024 33.02 39.99 41.17
1536 33.15 40.08 41.25
2048 33.29 40.23 41.37
7 384 384 1024 40.73 36.95 38.01
1536 41.86 36.85 37.81
2048 43.19 36.95 37.84
11 512 512 1024 49.55 34.94 35.94
1536 52.02 35.25 35.98
2048 55.02 35.00 35.92
(a) Độ sâu (N) so với chiều rộng (k)

30405060708090
1 3 5 7 9 11 13 15 17 19 Độ bối rối
Epoch N=3
N=7
N=11 (b) Độ bối rối xác thực so với epoch

Bảng 7: Tác động của tăng tỷ lệ độ sâu và chiều rộng trên WT-103.

Tham số Độ bối rối
(triệu) val Test
DeFINE + kết nối dư 41.63 38.96 40.03
DeFINE 40.89 36.95 38.01
(a)

Tham số Độ bối rối
(triệu) val Test
MER 40.89 36.95 38.01
- Giảm 43.91 37.19 38.34
(b)

Bảng 8: Các thiết lập khác nhau trên WT-103: (a) Tác động của các kết nối bỏ qua khác nhau. Xem Hình 5b và Hình 5c trong Phần A.2 cho sơ đồ cấp khối. (b) Tác động của thao tác giảm trong MER (Phần 3.1).

rằng các quyết định kiến trúc được đề xuất mỗi cái đều góp phần vào hiệu quả của đơn vị DeFINE. Chúng tôi tin rằng các mô hình chuỗi thần kinh với DeFINE có thể được cải thiện thêm với tìm kiếm siêu tham số mở rộng, tương tự như Melis et al. (2018). Trong công việc tương lai, chúng tôi sẽ áp dụng DeFINE cho các tác vụ mô hình hóa chuỗi khác. Ví dụ, chúng tôi tin rằng các kiến trúc mô hình ngôn ngữ được huấn luyện trước như ELMo và BERT có thể được hưởng lợi từ việc tích hợp DeFINE để cải thiện hiệu quả và hiệu suất. Một hướng khác là sử dụng các thành phần của DeFINE - cụ thể là MER, HGT, và các lớp trộn - trong các quy trình tìm kiếm kiến trúc thần kinh. Chúng tôi đã cho thấy tiềm năng của các thành phần này ở đây, nhưng tìm kiếm kiến trúc kỹ lưỡng có thể khám phá các cấu hình tối ưu hơn trong không gian tìm kiếm lớn được định nghĩa bởi các tham số độ sâu, nhóm và kết nối.

6 LỜI CẢM ƠN
Nghiên cứu này được hỗ trợ bởi ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF (IIS-1616112, IIS1252835), Giải thưởng Nhà điều tra Xuất sắc Allen, Samsung GRO và quà tặng từ Allen Institute for AI, Google, và Amazon. Các tác giả cũng muốn cảm ơn các thành viên của UW-NLP và H2Lab tại Đại học Washington vì phản hồi và nhận xét có giá trị của họ.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

TÀI LIỆU THAM KHẢO
Anish Acharya, Rahul Goel, Angeliki Metallinou, và Inderjit Dhillon. Nén nhúng trực tuyến cho phân loại văn bản sử dụng phân tích nhân tử ma trận hạng thấp. Trong Proceedings of the AAAI Conference on Artificial Intelligence, tập 33, trang 6196-6203, 2019.

Alexei Baevski và Michael Auli. Biểu diễn đầu vào thích ứng cho mô hình hóa ngôn ngữ thần kinh. Trong International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=ByxZX20qFQ.

Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. Dịch máy thần kinh bằng cách học cùng lúc để căn chỉnh và dịch. Trong International Conference on Learning Representations, 2015.

Shaojie Bai, J. Zico Kolter, và Vladlen Koltun. Đánh giá thực nghiệm các mạng tích chập và hồi quy chung cho mô hình hóa chuỗi. arXiv:1803.01271, 2018.

Denny Britz, Anna Goldie, Minh-Thang Luong, và Quoc Le. Khám phá khổng lồ các kiến trúc dịch máy thần kinh. arXiv preprint arXiv:1703.03906, 2017.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, và Tony Robinson. Benchmark một tỷ từ để đo tiến bộ trong mô hình hóa ngôn ngữ thống kê. arXiv preprint arXiv:1312.3005, 2013.

Patrick Chen, Si Si, Yang Li, Ciprian Chelba, và Cho-Jui Hsieh. Groupreduce: Xấp xỉ hạng thấp theo khối để thu hẹp mô hình ngôn ngữ thần kinh. Trong Advances in Neural Information Processing Systems, trang 10988-10998, 2018.

Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, và Zhi Jin. Nén mô hình ngôn ngữ thần kinh bằng biểu diễn từ thưa. arXiv preprint arXiv:1610.03950, 2016.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, và Ruslan Salakhutdinov. Transformer-XL: Mô hình ngôn ngữ chú ý vượt qua ngữ cảnh độ dài cố định. Trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.

Yann N Dauphin, Angela Fan, Michael Auli, và David Grangier. Mô hình hóa ngôn ngữ với mạng tích chập có cổng. Trong Proceedings of the 34th International Conference on Machine Learning-Volume 70, trang 933-941. JMLR. org, 2017.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Huấn luyện trước các transformer hai chiều sâu để hiểu ngôn ngữ. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.

Ronald A Fisher và Frank Yates. Bảng thống kê cho nghiên cứu sinh học, nông nghiệp và y tế. Oliver and Boyd Ltd, London, 1943.

J. Goodman. Lớp cho huấn luyện entropy cực đại nhanh. Trong 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221), 2001.

Édouard Grave, Armand Joulin, Moustapha Cissé, David Grangier Facebook AI Research, và Hervé Jégou. Xấp xỉ softmax hiệu quả cho gpu. Trong Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, 2017a.

Edouard Grave, Armand Joulin, và Nicolas Usunier. Cải thiện mô hình ngôn ngữ thần kinh với bộ nhớ đệm liên tục. Trong International Conference on Learning Representations, 2017b.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học dư sâu để nhận dạng hình ảnh. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770-778, 2016.

Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, và Yoshua Bengio. Mạng thần kinh lượng tử: Huấn luyện mạng thần kinh với trọng số và kích hoạt độ chính xác thấp. The Journal of Machine Learning Research, 18(1):6869-6898, 2017.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Hakan Inan, Khashayar Khosravi, và Richard Socher. Ràng buộc vector từ và bộ phân loại từ: Khung mất mát cho mô hình hóa ngôn ngữ. Trong International Conference on Learning Representations, 2017.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, và Yonghui Wu. Khám phá giới hạn của mô hình hóa ngôn ngữ. arXiv preprint arXiv:1602.02410, 2016.

Yoon Kim, Yacine Jernite, David Sontag, và Alexander M Rush. Mô hình ngôn ngữ thần kinh nhận biết ký tự. Trong Thirtieth AAAI Conference on Artificial Intelligence, 2016.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, và Alexander Rush. OpenNMT: Bộ công cụ mã nguồn mở cho dịch máy thần kinh. Trong Proceedings of ACL 2017, System Demonstrations, 2017.

Oleksii Kuchaiev và Boris Ginsburg. Thủ thuật phân tích nhân tử cho mạng lstm. Trong International Conference on Learning Representations Workshops, 2017.

Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, và Yoav Artzi. Đơn vị hồi quy đơn giản cho độ song song hóa cao của sự hồi quy. Trong Empirical Methods in Natural Language Processing (EMNLP), 2018.

Zhongliang Li, Raymond Kulhanek, Shaojun Wang, Yunxin Zhao, và Shuang Wu. Lớp nhúng mỏng cho mô hình ngôn ngữ thần kinh hồi quy. Trong Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

Shiyu Liang và Rayadurgam Srikant. Tại sao mạng thần kinh sâu cho xấp xỉ hàm? ICLR, 2017.

Minh-Thang Luong, Hieu Pham, và Christopher D. Manning. Các cách tiếp cận hiệu quả cho dịch máy thần kinh dựa trên chú ý. Trong Empirical Methods in Natural Language Processing (EMNLP), September 2015.

Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, và Britta Schasberger. Penn treebank: Chú thích cấu trúc đối số vị từ. Trong Proceedings of the Workshop on Human Language Technology, HLT '94, 1994.

Bryan McCann, James Bradbury, Caiming Xiong, và Richard Socher. Học trong dịch: Vector từ được ngữ cảnh hóa. Trong Advances in Neural Information Processing Systems, trang 6294-6305, 2017.

Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, và Hannaneh Hajishirzi. Đơn vị hồi quy kim tự tháp cho mô hình hóa ngôn ngữ. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018.

Gábor Melis, Chris Dyer, và Phil Blunsom. Về hiện trạng của đánh giá trong mô hình ngôn ngữ thần kinh. Trong International Conference on Learning Representations, 2018.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Mô hình hỗn hợp con trỏ sentinel. Trong International Conference on Learning Representations, 2017.

Stephen Merity, Nitish Shirish Keskar, và Richard Socher. Phân tích mô hình hóa ngôn ngữ thần kinh ở nhiều quy mô. arXiv preprint arXiv:1803.08240, 2018a.

Stephen Merity, Nitish Shirish Keskar, và Richard Socher. Điều hòa và tối ưu hóa mô hình ngôn ngữ LSTM. Trong International Conference on Learning Representations, 2018b.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černocký, và Sanjeev Khudanpur. Mô hình ngôn ngữ dựa trên mạng thần kinh hồi quy. Trong Eleventh annual conference of the international speech communication association, 2010.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, và Jeff Dean. Biểu diễn phân tán của từ và cụm từ và tính chất kết hợp của chúng. Trong Advances in neural information processing systems, trang 3111-3119, 2013.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Andriy Mnih và Geoffrey E Hinton. Mô hình ngôn ngữ phân tán phân cấp có thể mở rộng. Trong Advances in neural information processing systems, trang 1081-1088, 2009.

Frederic Morin và Yoshua Bengio. Mô hình ngôn ngữ thần kinh xác suất phân cấp. Trong Aistats, tập 5, trang 246-252. Citeseer, 2005.

Jeffrey Pennington, Richard Socher, và Christopher D. Manning. Glove: Vector toàn cầu cho biểu diễn từ. Trong Empirical Methods in Natural Language Processing (EMNLP), trang 1532-1543, 2014. URL http://www.aclweb.org/anthology/D14-1162.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, và Luke Zettlemoyer. Biểu diễn từ được ngữ cảnh hóa sâu. Trong Proc. of NAACL, 2018.

Ofir Press và Lior Wolf. Sử dụng nhúng đầu ra để cải thiện mô hình ngôn ngữ. Trong Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 2017.

Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, và Ali Farhadi. Xnor-net: Phân loại imagenet sử dụng mạng tích chập nhị phân. Trong European Conference on Computer Vision, trang 525-542. Springer, 2016.

Rico Sennrich, Barry Haddow, và Alexandra Birch. Dịch máy thần kinh của từ hiếm với đơn vị từ con. arXiv preprint arXiv:1508.07909, 2015.

Raphael Shu và Hideki Nakayama. Nén nhúng từ qua học mã kết hợp sâu. arXiv preprint arXiv:1711.01068, 2017.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Advances in Neural Information Processing Systems 30, trang 5998-6008. 2017.

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, và William W. Cohen. Phá vỡ nút cổ chai softmax: Mô hình ngôn ngữ RNN hạng cao. Trong International Conference on Learning Representations, 2018.

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

A PHỤ LỤC
A.1 HÀM BIẾN ĐỔI FG
Để tạo ra đầu ra y ∈ Rm×1 từ đầu vào x ∈ Rn×1 và ma trận trọng số W ∈ Rn/g×m/g, FG đầu tiên chia đầu vào x thành g nhóm và sau đó nối các phần được chia để tạo ra x̂ ∈ Rg×n/g. x̂ sau đó được nhân với ma trận trọng số W để tạo ra ŷ = x̂W ∈ Rg×m/g. Vector kết quả ŷ sau đó được làm phẳng để tạo ra y. Khi g= 1, chúng ta có được biến đổi tuyến tính.

A.2 SƠ ĐỒ CẤP KHỐI CỦA CÁC KẾT NỐI BỎ QUA KHÁC NHAU TRONG DEFINE
Sơ đồ cấp khối của các biến thể khác nhau của DeFINE được đưa ra trong Hình 5. Hình 5a chồng lớp biến đổi FG (Eq. 1) và giống với HGT trong Hình 2c. Hình 5b thêm kết nối dư vào Hình 5a. Hình 5c giống với Hình 3 trong khi Hình 5d giống với Hình 5c, nhưng không có chức năng chia và trộn.

Lớp 
BiếnđổiLớp 
BiếnđổiLớp 
Biếnđổi
/g2187/g2191/g2187/g2197
(a) HGT
Lớp 
BiếnđổiLớp 
BiếnđổiLớp 
Biếnđổi
/g2187/g2191/g2187/g2197
Cộng (b) HGT+ Dư
Lớp 
BiếnđổiLớp 
BiếnđổiLớp 
Biếnđổi
/g2187/g2191/g2187/g2197
Chia và Trộn
Chia và Trộn (c) DeFINE
Lớp 
BiếnđổiLớp 
BiếnđổiLớp 
Biếnđổi
/g2187/g2191/g2187/g2197
Nối
Nối (d) DeFINE (không có trộn)

Hình 5: Các cách khác nhau để chồng lớp biến đổi FG (Sec. A.1) để học biểu diễn token sâu.

A.3 SIÊU THAM SỐ CHO HUẤN LUYỆN MÔ HÌNH NGÔN NGỮ
Để huấn luyện mô hình ngôn ngữ dựa trên LSTM, chúng tôi sử dụng một GPU NVIDIA GTX 1080 Ti với bộ nhớ GPU 11 GB trong khi để huấn luyện Transformer-XL, chúng tôi sử dụng bốn GPU GeForce RTX 2080 Ti, mỗi cái có bộ nhớ GPU 11 GB (như được khuyến nghị bởi tác giả). Theo các công trình gần đây, bao gồm Merity et al. (2018a) và Baevski & Auli (2019), chúng tôi sử dụng đầu vào thích ứng làm hàm ánh xạ trong DeFINE và softmax thích ứng cho phân loại cho các thí nghiệm với mô hình chuỗi dựa trên RNN. Chúng tôi cũng chia sẻ trọng số giữa đầu vào thích ứng và đầu ra. Đối với Transformer-XL Dai et al. (2019), chúng tôi sử dụng nhúng chiếu (như được thực hiện bởi tác giả). Chúng tôi huấn luyện mô hình bằng PyTorch (v1.2). Đối với mô hình ngôn ngữ dựa trên LSTM, chúng tôi sử dụng siêu tham số tương tự như Merity et al. (2018a) được tóm tắt trong Phần 9.

A.4 HIỆU SUẤT CỦA TRANSFORMER-XL TRÊN WIKITEXT-103
Hình 6 vẽ độ bối rối xác thực của Transformer-XL trên WikiText-103 như một hàm của các bước huấn luyện. Chúng ta có thể thấy rằng DeFINE cho phép Transformer-XL mang lại hiệu suất tương tự với ít tham số hơn.

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

WikiText-103
# GPU 1
Decay trọng số 0
Optimizer SGD
LR 20
Độ dài BPTT 140
Kích thước batch 60
Epochs 20
Giảm LR (hệ số, bước) 10, [15]
Chiều ẩn LSTM 1024
# Lớp LSTM 4
Chiều max của êei(k) 1024
Dropout Giống như Merity et al. (2018a)

Bảng 9: Siêu tham số cho huấn luyện mô hình ngôn ngữ dựa trên LSTM cấp từ trên WikiText-103. Các thiết lập này tương tự như Merity et al. (2018a).

Hình 6: Hiệu suất Transformer-XL trên tập dữ liệu Wikitext-103 với DeFINE.

B TRỰC QUAN HÓA BẢN ĐỒ TƯƠNG QUAN CHO TRANSFORMER-XL TRÊN WIKITEXT-103
Tính toán bản đồ tương quan: Giả sử chúng ta có bảng tra cứu tùy ý E ∈ RV×m ánh xạ mỗi token trong từ vựng V tới không gian vector m chiều. Chúng ta tính bản đồ tương quan M như: M = ETE ∈ Rm×m.⁴ Nếu bản đồ tương quan là đồng nhất, thì nó cho thấy rằng m chiều trong E độc lập. Để mã hóa biểu diễn ngữ cảnh tốt hơn giữa các token bằng mô hình ngữ cảnh như LSTM và Transformer, các chiều nhúng nên độc lập.

DeFINE có thể xấp xỉ lớp nhúng tiêu chuẩn không? Hình 7 trực quan hóa bản đồ tương quan của nhúng được học bằng lớp nhúng tiêu chuẩn (hàng trên), nhúng chiếu (Acharya et al., 2019; Dai et al., 2019) (hàng giữa), và nhúng DeFINE (hàng dưới) tại các giá trị khác nhau của n, trong đó n là chiều của lớp ánh xạ trong DeFINE. So với nhúng chiếu, DeFINE có thể xấp xỉ lớp nhúng tiêu chuẩn hiệu quả và hiệu quả (xem Bảng 2 để so sánh hiệu quả và hiệu suất).

Hơn nữa, chúng tôi cung cấp so sánh theo lớp cho DeFINE tại các giá trị khác nhau của n trong Hình 8, 9, và 10. Lớp ánh xạ trong DeFINE ở trong không gian chiều thấp và có tương quan. Khi chúng ta học biểu diễn sâu hơn bằng DeFINE, những tương quan này được giảm và chúng ta có được ma trận tương quan tương tự như lớp nhúng tiêu chuẩn. Điều này cho thấy DeFINE hiệu quả trong việc xấp xỉ lớp nhúng tiêu chuẩn. Quan trọng là, các nhóm tại các lớp mở rộng khác nhau trong DeFINE độc lập (xem Hình 11), cho thấy các ma trận này đang học biểu diễn khác nhau của đầu vào.

⁴ Bản đồ tương quan được chuẩn hóa giữa 0 và 1.

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Nhúng tiêu chuẩn
Nhúng chiếu
n= 64 n= 128 n= 256
DeFINE
n= 64 n= 128 n= 256

Hình 7: Xấp xỉ ma trận nhúng tiêu chuẩn bằng nhúng chiếu và DeFINE cho Transformer-XL tại các giá trị khác nhau của n.

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Lớp ánh xạ trong DeFINE
Lớp mở rộng 1 trong DeFINE
Lớp mở rộng 2 trong DeFINE
Lớp giảm trong DeFINE

Hình 8: Trực quan hóa theo lớp của bản đồ tương quan của nhúng DeFINE khi n= 64.

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Lớp ánh xạ trong DeFINE
Lớp mở rộng 1 trong DeFINE
Lớp mở rộng 2 trong DeFINE
Lớp giảm trong DeFINE

Hình 9: Trực quan hóa theo lớp của bản đồ tương quan của nhúng DeFINE khi n= 128

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại ICLR 2020

Lớp ánh xạ trong DeFINE
Lớp mở rộng 1 trong DeFINE
Lớp mở rộng 2 trong DeFINE
Lớp giảm trong DeFINE

Hình 10: Trực quan hóa theo lớp của bản đồ tương quan của nhúng DeFINE khi n= 256

Lớp mở rộng 1 trong DeFINE với 4 nhóm
n= 64 n= 128 n= 256
Lớp mở rộng 2 trong DeFINE với 2 nhóm
n= 64 n= 128 n= 256

Hình 11: Các nhóm trong lớp mở rộng của DeFINE trực giao, cho thấy các ma trận này đang học biểu diễn khác nhau của đầu vào.
