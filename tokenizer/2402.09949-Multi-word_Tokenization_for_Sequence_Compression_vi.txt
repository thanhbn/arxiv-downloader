# 2402.09949.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/2402.09949.pdf
# Kích thước tệp: 407392 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tokenization Đa-từ cho Nén Chuỗi
Leonidas Gee
Đại học Sussex, Vương quốc Anh
jg717@sussex.ac.ukLeonardo Rigutini
expert.ai, Siena, Italy
lrigutini@expert.ai
Marco Ernandes
expert.ai, Siena, Italy
mernandes@expert.aiAndrea Zugarini
expert.ai, Siena, Italy
azugarini@expert.ai
Tóm tắt
Các Mô hình Ngôn ngữ Lớn đã chứng minh
thành công cao trong việc mô hình hóa nhiều
loại tác vụ khác nhau. Tuy nhiên, điều này
đi kèm với chi phí tính toán cao làm cản trở
việc áp dụng rộng rãi trong công nghiệp. Trong
bài báo này, chúng tôi trình bày MWT: một
Tokenizer Đa-từ vượt ra ngoài ranh giới từ
bằng cách biểu diễn các biểu thức đa-từ phổ
biến như các token đơn. MWT tạo ra một
tokenization nhỏ gọn và hiệu quả hơn mang
lại hai lợi ích: (1) Tăng hiệu suất do phạm
vi bao phủ dữ liệu đầu vào lớn hơn với ngân
sách độ dài chuỗi cố định; (2) Suy luận nhanh
hơn và nhẹ hơn do khả năng giảm độ dài chuỗi
với sự sụt giảm hiệu suất không đáng kể.
Kết quả của chúng tôi cho thấy MWT mạnh
mẽ hơn trên các độ dài chuỗi ngắn hơn, do
đó cho phép tăng tốc đáng kể thông qua việc
cắt ngắn chuỗi sớm.

1 Giới thiệu
Lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) đã
chứng kiến những đột phá lớn với sự ra đời của
các Mô hình Ngôn ngữ Lớn (LLM) (Vaswani et al.,
2017; Devlin et al., 2018; Touvron et al., 2023;
OpenAI, 2023). Mặc dù thành công, các LLM
như ChatGPT (OpenAI, 2023; Brown et al., 2020)
có hàng trăm tỷ tham số đòi hỏi chi phí tính toán
khổng lồ theo thiết kế.

Các phương pháp nén mô hình truyền thống như
Chưng cất Kiến thức (Hinton et al., 2015), Cắt
tỉa (Michel et al., 2019; Zhu and Gupta, 2017), và
Lượng tử hóa (Shen et al., 2020; Gupta et al., 2015)
đã tập trung vào việc tạo ra các mô hình nhẹ hơn
bằng cách thu nhỏ kích thước kiến trúc hoặc giảm
số lượng FLOP.

Gần đây, các LLM đã được chứng minh tạo ra
hiệu suất ấn tượng trên các đầu vào được thiết kế
cẩn thận để chứa tất cả thông tin cần thiết cho
một hướng dẫn cụ thể. Do đó, có xu hướng ngày
càng tăng trong việc thiết kế các prompt dài hơn
và dài hơn đã dẫn đến tăng đáng kể chi phí tính
toán. Để giải quyết điều này, sự quan tâm đã tăng
lên trong việc nén các chuỗi đầu vào từ tokenizer
(Gee et al., 2022; Mu et al., 2023; Petrov et al.,
2023). Thật vậy, nhiều nghiên cứu đã chỉ ra tầm
quan trọng của tokenization trong việc xác định
độ dài của một chuỗi trong các lĩnh vực chuyên
biệt (Gee et al., 2022) hoặc trên các ngôn ngữ
thiểu số (Petrov et al., 2023).

Trong bài báo này, chúng tôi đề xuất một phương
pháp để giảm chi phí tính toán của LLM bằng cách
nén các đầu vào văn bản sử dụng Tokenizer Đa-từ
(MWT). Để đạt được điều này, chúng tôi làm phong
phú từ vựng của tokenizer với các biểu thức đa-từ
được xác định thống kê. Bằng cách mã hóa các
n-gram phổ biến với các token đơn, các chuỗi được
tạo ra vừa ngắn hơn vừa chứa nhiều thông tin hơn,
do đó cho phép tăng tốc đáng kể thông qua việc
cắt ngắn chuỗi sớm. Ngoài ra, MWT được chứng
minh tương thích với các phương pháp nén truyền
thống đã đề cập. Về mặt thực nghiệm, chúng tôi
đánh giá MWT trên ba bộ dữ liệu phân loại văn
bản. Chúng tôi cho thấy cách tiếp cận của chúng
tôi vẫn hoạt động tốt khi kết hợp với các mô hình
chưng cất (Sanh et al., 2019) và các kỹ thuật nén
chuỗi khác (Gee et al., 2022). Mã cho bài báo của
chúng tôi được công khai¹.

Phần còn lại của bài báo được tổ chức như sau.
Đầu tiên, chúng tôi xem xét các công trình liên quan
trong Phần 2. Sau đó, chúng tôi mô tả cách tiếp
cận của chúng tôi trong Phần 3 và trình bày các
thí nghiệm trong Phần 4. Cuối cùng, chúng tôi
rút ra kết luận trong Phần 5.

2 Các Công trình Liên quan
Hầu hết nghiên cứu nén mô hình thuộc một trong
các danh mục sau: Chưng cất Kiến thức (Hinton
et al., 2015; Sanh et al., 2019; Jiao et al., 2020;
Wang et al., 2020; Sun et al., 2020), Cắt tỉa (Zhu
and Gupta, 2017; Michel et al., 2019), và Lượng
tử hóa (Shen et al., 2020). Họ các phương pháp
này có phần bổ sung cho nhau và

¹https://github.com/LeonidasY/
fast-vocabulary-transfer/tree/emnlp2023arXiv:2402.09949v2  [cs.CL]  4 Apr 2024

--- TRANG 2 ---
Đầu vào: an energizable member is operably coupled to the outer sleeve .
Tgen:an, en, ##er, ##gi, ##zable, member, is, opera, ##bly, coupled, to, the, outer,
sleeve, .
T1000
gen:an, en, ##er, ##gi, ##zable, member_is , opera, ##bly, coupled_to ,the_outer , sleeve,
.
T100:an, energizable , member, is, operably , coupled, to, the, outer, sleeve, .
T1000
100:an, energizable ,member_is ,operably ,coupled_to ,the_outer , sleeve, .

Hình 1: Tokenization sử dụng tokenizer chung Tgen và tokenizer thích ứng T100. T1000
gen và T1000
100 được mở rộng với
top-1000 bigram. Các token thu được bằng domain-adaptation hoặc MWT được tô sáng màu cam và xanh tương ứng.
MWT được chứng minh là bổ sung cao cho các tokenizer hiện có trong nén chuỗi.

có thể được áp dụng riêng lẻ hoặc cùng nhau. Mỗi
cách tiếp cận thay đổi kích thước của mô hình để
có được một kiến trúc hiệu quả hơn. Khác biệt,
các công trình khác như FlashAttention (Dao et al.,
2022) tìm cách tối ưu hóa việc triển khai của mô
hình. Cụ thể, các LLM được tăng tốc bằng cách
giảm số lần truy cập bộ nhớ cho cơ chế self-attention.

Nén Chuỗi. Một hướng mới nổi để giảm chi phí
của LLM liên quan đến việc thiết kế các chuỗi đầu
vào ngắn hơn. Các kỹ thuật prompting như Mu et al.
(2023) nén các prompt dài lặp lại thành các gist token.
Các công trình khác nhấn mạnh vai trò của tokenization
trong nén chuỗi. Trong Petrov et al. (2023), các tác
giả cho thấy tokenizer của hầu hết các LLM thiên
về tiếng Anh hơn các ngôn ngữ khác. Đối với các
ngôn ngữ thiểu số, cùng một câu được dịch có thể
bao gồm các đầu vào dài hơn tới 15 lần. Tương tự,
Gee et al. (2022) điều tra hiệu quả tokenization của
các tokenizer đa năng trong các lĩnh vực dọc như
y học và luật. Họ đề xuất một kỹ thuật transfer learning
thích ứng từ vựng của LLM với các lĩnh vực ngôn
ngữ cụ thể. Hiệu ứng của từ vựng chuyên dụng là
tokenization hiệu quả hơn giảm số lượng sub-word
token trong một chuỗi.

Trong công trình này, chúng tôi đẩy hiệu ứng này
xa hơn, vượt ra ngoài ranh giới từ bằng cách giới
thiệu Biểu thức Đa-từ (MWE) dưới dạng n-gram
vào tokenizer như được hiển thị trong Hình 1. Trực
giác cơ bản đằng sau điều này là tokenization nhỏ
gọn hơn có thể tiết kiệm tính toán bằng cách cho
phép mô hình xử lý các chuỗi ngắn hơn mà không
mất đáng kể thông tin. Việc sử dụng MWE không
mới với một số công trình (Lample et al., 2018;
Otani et al., 2020; Kumar and Thawani, 2022) giới
thiệu các cụm từ hoặc n-gram để cải thiện chất
lượng dịch máy. Trong Kumar and Thawani (2022),
các tác giả tổng quát hóa BPE (Sennrich et al.,
2016) cho các token đa-từ. Tuy nhiên, theo hiểu
biết tốt nhất của chúng tôi, chúng tôi là những
người đầu tiên điều tra MWE trong bối cảnh nén
chuỗi.

3 Tokenizer Đa-từ
Tokenization là một bước cần thiết trong việc cung
cấp dữ liệu văn bản cho LLM. Thông thường, các
tokenizer chia một văn bản thành một chuỗi các
ký hiệu có thể là từ hoàn chỉnh hoặc chỉ là các
phần con. Để làm điều này, từ vựng trước tiên
được xây dựng bằng cách học thống kê các token
phổ biến nhất từ một corpus đa năng lớn (Sennrich
et al., 2016; Schuster and Nakajima, 2012; Kudo
and Richardson, 2018). Tokenizer kết quả sau đó
có thể được sử dụng để phân đoạn một văn bản
đầu vào bằng cách tham lam tìm kiếm giải pháp
với số lượng token ít nhất. Dựa trên điều này,
chúng tôi tiêm vào tokenizer các ký hiệu mới được
hình thành bởi n-gram của từ. Chúng tôi làm điều
này bằng cách đầu tiên chọn các n-gram phổ biến
nhất để đưa vào từ vựng của nó. Sau đó, chúng
tôi đặt một bước merge n-gram trong pipeline
tokenization như được phác thảo trong Hình 2.
Các n-gram được thêm vào sẽ được coi như các
token đơn trong pipeline tokenization tiếp theo.

Lựa chọn N-gram. Để tối đa hóa việc giảm chuỗi,
chúng tôi ước tính thống kê top-K n-gram phổ
biến nhất trong một corpus huấn luyện tham chiếu.
Mặc dù cách tiếp cận là tham lam, do đó không
tối ưu, nó vẫn hiệu quả mang lại nén đáng kể
trong khi cực kỳ nhanh và dễ tính toán. Chính
thức hơn, cho một corpus D và N>=2, chúng tôi
tính toán tất cả các n-gram có thể gn trong D,
trong đó n = 2, . . . , N. Sau đó, chúng tôi đếm tần
suất f(gn) của chúng, cho tất cả gn trong D. K
n-gram phổ biến nhất GK được đưa vào từ vựng
V ← V union GK của tokenizer T.

--- TRANG 3 ---
Hình 2: Phác thảo pipeline Tokenizer Đa-từ. Đầu tiên, các n-gram được học thống kê từ tập huấn luyện.
Sau đó, top-K n-gram được thêm vào từ vựng của tokenizer. N-gram được merge từ trái sang phải trong
một chuỗi sau pre-tokenization.

energizable +
+ member_isen
##er
##zable
member
istreatedPre-trained LM
embeddingsAdapted Tokenizer
embeddings
##gi

Hình 3: Fast Vocabulary Transfer. Các embedding pre-trained của các token hiện có được kết hợp để tạo thành
các embedding của từ vựng thích ứng mới.

Fast Vocabulary Transfer. Do từ vựng của tokenizer
đã thay đổi, các ký hiệu mới được thêm GK phải
được đưa vào ma trận embedding của mô hình ngôn
ngữ. Để tránh huấn luyện lại toàn bộ mô hình từ
đầu vốn rất tốn tài nguyên, hoặc khởi tạo ngẫu
nhiên các token mới vốn sẽ hoạt động kém, chúng
tôi sử dụng Fast Vocabulary Transfer (FVT) thay
thế (Gee et al., 2022).

FVT là một kỹ thuật transfer learning gán embedding
cho các token mới bằng cách kết hợp các phần tử
hiện có của ma trận embedding như được hiển thị
trong Hình 3. Sau khi khởi tạo các embedding đa-từ
với FVT, chúng tôi thấy có lợi khi điều chỉnh mô
hình với Masked-Language Modeling (MLM) như
đã thực hiện bởi Gee et al. (2022). Chúng tôi tin
rằng điều này hữu ích vì nó giúp mô hình điều
chỉnh thêm các embedding của các token mới.

4 Thí nghiệm
Cho một số lượng token cố định, một chuỗi đầu
vào nhỏ gọn hơn bảo tồn lượng thông tin lớn hơn.
Điều này có thể được sử dụng để đạt được hiệu
suất tốt hơn với lợi ích hạn chế về tăng tốc, hoặc
ngược lại, tức là làm cho mô hình nhanh hơn với
sự sụt giảm hiệu suất không đáng kể. Các thí
nghiệm nhằm phân tích cách hai khía cạnh này
tương tác với nhau. Chúng tôi tập trung vào phân
loại văn bản vì đây là vấn đề quan tâm đặc biệt
cho nhiều ứng dụng hướng công nghiệp.

4.1 Thiết lập Thí nghiệm
Các thí nghiệm của chúng tôi được thực hiện trên
các phiên bản có phân biệt chữ hoa thường của
BERT base (Devlin et al., 2018) và DistilBERT
base (Sanh et al., 2019). Ngoài ra, chúng tôi xem
xét một tokenizer thích ứng với kích thước từ vựng
bằng với tokenizer chung từ mô hình pre-trained
như đã thực hiện bởi Gee et al. (2022). Chúng tôi
gọi các tokenizer chung và thích ứng là Tgen và
T100 tương ứng. Cả hai tokenizer đều được mở
rộng với top-K n-gram là 1000, 2500, và 5000.
Tổng thể, chúng tôi so sánh tám tokenizer khác
nhau được ký hiệu như: Tgen, T1000
gen, T2500
gen, T5000
gen
và T100, T1000
100, T2500
100, T5000
100.

Chi tiết Triển khai. Chúng tôi huấn luyện mỗi mô
hình với 5 khởi tạo ngẫu nhiên khác nhau. Macro-F1
và tăng tốc suy luận được đo như các chỉ số. Trung
bình của tất cả 5 khởi tạo được lấy làm giá trị cuối
cùng của mỗi chỉ số. Các đo lường tăng tốc suy
luận được thực hiện trên GPU V100-PCIE với 16GB
RAM chuyên dụng.

Theo Gee et al. (2022), chúng tôi đầu tiên áp dụng
một epoch MLM sử dụng bộ dữ liệu in-domain.
Tiếp theo, mô hình được fine-tune trong 10 epoch
với early stopping trên tác vụ downstream. Chúng
tôi đặt learning rate ban đầu là 3·10⁻⁵ cho cả MLM
và downstream fine-tuning, trong khi batch size
được đặt

--- TRANG 4 ---
Dataset TgenT1000
gen T2500
gen T5000
gen T100T1000
100 T2500
100 T5000
100
ADE 31 26 25 23 21 18 17 16
LEDGAR 155 118 107 98 131 97 90 84
PATENT 134 110 105 100 118 94 90 86

Bảng 1: Độ dài chuỗi trung bình từ tokenization. Các tokenizer chung Tgen và thích ứng T100 được mở rộng
với top-K khác nhau là 1000, 2500, và 5000.

là 8 và 32 cho MLM và downstream fine-tuning
tương ứng.

Lựa chọn N. Một hyperparameter quan trọng là N,
tức là số từ tối đa tạo thành một n-gram. Trong
các thí nghiệm của chúng tôi, N được đặt là 2 vì
chúng tôi tin rằng chỉ sử dụng bigram cung cấp
tính chất tổng quát hóa tốt hơn. Tăng giá trị N có
thể dẫn đến việc chuyên biệt hóa quá mức của
n-gram có thể overfit trên các corpus văn bản nhỏ.

4.2 Bộ dữ liệu
Để xác định hiệu quả của MWT, chúng tôi chọn
3 tác vụ phân loại văn bản khác nhau từ các lĩnh
vực ngôn ngữ đa dạng, cụ thể là y học (ADE),
pháp lý (LEDGAR), và công nghệ (PATENT).

ADE. Một bộ dữ liệu phân loại câu xác định liệu
một câu có liên quan đến Adverse Drug Event
(ADE) hay không (Gurulingappa et al., 2012).
Các câu được đặc trưng bởi sự hiện diện của
thuật ngữ y học về thuốc và tác dụng phụ của
chúng. Chúng tôi sử dụng cùng phân chia train,
validation, và test như trong Gee et al. (2022).

LEDGAR. Một bộ dữ liệu phân loại tài liệu về
các hợp đồng thu được từ hồ sơ nộp của Ủy ban
Chứng khoán và Giao dịch Hoa Kỳ (SEC) (Tuggener
et al., 2020). Tác vụ là xác định chủ đề chính của
điều khoản hợp đồng từ một tập 100 nhãn loại
trừ lẫn nhau. Bộ dữ liệu cũng là một phần của
LexGLUE (Chalkidis et al., 2022), một benchmark
cho hiểu biết ngôn ngữ pháp lý.

PATENT. Một bộ dữ liệu phân loại tài liệu² về
các đơn xin cấp bằng sáng chế Hoa Kỳ được nộp
theo mã Phân loại Bằng sáng chế Hợp tác (CPC)
(Sharma et al., 2019). Một bản tóm tắt trừu tượng
do con người viết được cung cấp cho mỗi đơn xin
cấp bằng sáng chế. Tác vụ là xác định danh mục
mà một đơn xin cấp bằng sáng chế thuộc về từ
9 lớp không cân bằng.

²https://huggingface.co/datasets/ccdv/
patent-classification

4.3 Kết quả
Phân tích Sơ bộ. Trước khi đo lường hiệu ứng
của MWT trên LLM, chúng tôi phân tích cách độ
dài chuỗi trung bình thay đổi cho mỗi bộ dữ liệu
tùy thuộc vào tokenizer. Từ Bảng 1, tăng top-K
n-gram phổ biến nhất tự nhiên mang lại nén lớn
hơn. Tuy nhiên, ngay cả 1000 bigram cũng đủ để
đạt được giảm khoảng 20%. Khi các đa-từ được
kết hợp với tokenizer thích ứng T100, các hiệu
ứng thu hẹp chuỗi chung xuất hiện bổ sung cao,
đạt được tỷ lệ nén gần 50% trong ADE. Trong
thực tế, giảm 50% có nghĩa là trung bình chúng
ta có thể lưu trữ cùng lượng văn bản trong một
nửa độ dài chuỗi. Do đó, về nguyên tắc chúng
ta có thể giảm độ dài chuỗi tối đa của LLM theo
hệ số 2.

Tokenization Đa-từ. Như một đánh giá đầu tiên,
chúng tôi đánh giá macro-F1 và tăng tốc suy luận
đạt được bởi các mô hình BERT fine-tuned với
tokenizer đa-từ: T1000
gen, T2500
gen, T5000
gen. BERT
pre-trained với tokenizer chung Tgen được coi
là mô hình tham chiếu. Từ Bảng 2, MWT được
chứng minh cải thiện hiệu suất tham chiếu hoặc
gây ra sự suy giảm tương đối không đáng kể. Đồng
thời, nén chuỗi từ MWT mang lại tăng tốc tự nhiên
mà tùy thuộc vào bộ dữ liệu khác nhau từ khoảng
x1.1 đến x1.4.

MWT và Domain Adaptation. Ngoài ra, chúng
tôi điều tra việc áp dụng MWT với các tokenizer
thích ứng với bộ dữ liệu: T1000
100, T2500
100, T5000
100.
Ngoại trừ PATENT, hầu hết các mô hình được
chứng minh đạt được tăng tốc suy luận đáng kể
lên tới x1.8 với suy giảm tối thiểu về hiệu suất
từ Bảng 2. Chúng tôi giả thuyết rằng điều này
là do thực tế là lĩnh vực ngôn ngữ của PATENT
không chuyên biệt như ADE và LEDGAR, điều
này làm giảm lợi ích của việc sử dụng tokenizer
thích ứng.

--- TRANG 5 ---
20 40 60 80 100 120
Độ dài Chuỗi Tối đa75.077.580.082.585.087.590.092.5Macro-F1
~x1.8 ~x2.4 ~x4.4ADE
100 200 300 400 500
Độ dài Chuỗi Tối đa787980818283Macro-F1
~x2.1 ~x4.4 ~x9.4LEDGAR
50 100 150 200 250
Độ dài Chuỗi Tối đa575859606162Macro-F1
~x2.0 ~x4.2 ~x8.6PATENT
gen
1000
gen
2500
gen
5000
gen
100
1000
100
2500
100
5000
100
 tăng tốc tối đa

Hình 4: Biểu đồ macro-F1 theo độ dài chuỗi tối đa. Các tokenizer chung Tgen và thích ứng T100 được biểu diễn
bằng các đường liền và đứt tương ứng. MWT được chứng minh mạnh mẽ hơn trên các độ dài chuỗi ngắn hơn,
do đó cho phép tăng tốc lớn thông qua cắt ngắn chuỗi sớm.

MethodADE LEDGAR PATENT
∆F1 Speedup ∆F1 Speedup ∆F1 Speedup
Tgen 90.74±0.84 1.00 82.12 ±0.33 1.00 61.44±0.38 1.00
T1000
gen -0.09±0.70 1.32 0.54±0.24 1.14 -0.42 ±0.54 1.11
T2500
gen 0.37±0.54 1.38 0.05 ±0.44 1.23 -0.07 ±0.46 1.16
T5000
gen 0.29±0.68 1.43 -0.05 ±0.41 1.33 -0.46 ±0.69 1.19
T100 0.24±0.67 1.51 0.00 ±0.41 1.10 -1.27 ±0.39 1.06
T1000
100 -0.86±1.21 1.71 0.32 ±0.58 1.36 -0.78 ±0.62 1.24
T2500
100 -0.88±0.72 1.78 -0.19 ±0.57 1.47 -1.04 ±0.42 1.30
T5000
100 -0.51±0.65 1.79 0.02±0.58 1.57 -1.66±0.44 1.34

Bảng 2: Giá trị tuyệt đối của BERT fine-tuned trên tác vụ downstream sử dụng độ dài chuỗi 128, 512 và 256
cho ADE, LEDGAR và PATENT tương ứng. Tgen được hiển thị ở hàng đầu tiên, trong khi các giá trị tương đối
so với Tgen được hiển thị ở các hàng tiếp theo.

MWT và Truncation. Dựa trên phân tích sơ bộ,
chúng tôi phân tích cách cắt ngắn chuỗi với các
độ dài tối đa khác nhau ảnh hưởng đến cả hiệu
suất và tăng tốc suy luận. Giảm độ dài chuỗi tối
đa có tác động kép đến tăng tốc suy luận với một
lượng tài nguyên cố định. Đầu tiên, độ trễ tăng
tuyến tính so với độ dài chuỗi. Thứ hai, giảm độ
dài chuỗi giải phóng tài nguyên GPU có thể được
sử dụng để tăng kích thước batch. Chúng tôi xem
xét 4 độ dài chuỗi tối đa cho mỗi bộ dữ liệu bằng
cách liên tục chia đôi độ dài chuỗi tối đa ban đầu,
tức là {128,64,32,16} cho ADE, {256,128,64,32}
cho LEDGAR, và {512,256,128,64} cho PATENT.

Từ Hình 4, chúng ta có thể thấy hiệu suất của
Tgen giảm nhanh hơn MWT khi truncation tăng
(độ dài chuỗi tối đa giảm). Trong truncation cực
đoan 8 lần, hiệu suất của Tgen giảm mạnh cho
cả ADE và LEDGAR. Tuy nhiên, MWT được chứng
minh mạnh mẽ hơn đối với truncation, do đó sự
suy giảm hiệu suất của chúng mượt mà hơn và
không có sự sụp đổ đột ngột. Trong cả ADE và
LEDGAR, truncation 4 lần dẫn đến hiệu suất gần
như giống hệt hoặc tốt hơn, trong khi mang lại
tăng tốc suy luận đáng kể ~x2.4 và ~x4.4 tương
ứng. Nếu có thể chấp nhận sự suy giảm hiệu suất
nhất định, tăng tốc suy luận có thể được tối đa
hóa, đạt tới ~x9.4 trong LEDGAR.

MWT và Distillation. Ngoài ra, chúng tôi điều
tra sự tương tác giữa nén chuỗi và chưng cất kiến
thức trong Bảng 3. Để làm điều này, chúng tôi
sử dụng mô hình DistilBERT với MWT. Để đơn
giản, chúng tôi giới hạn phân tích của mình với
LEDGAR và một tokenizer đa-từ duy nhất T2500
gen
trên các độ dài chuỗi tối đa khác nhau. Từ bảng,
MWT của chúng tôi được chứng minh giữ lại hầu
hết hiệu suất với một phần tư độ dài chuỗi và tăng
tốc suy luận ~x8.8. Ngay cả với truncation chuỗi
cực đoan chỉ còn 64 token, chúng tôi vẫn có thể
đạt được tăng tốc suy luận ~x18.1 với chỉ 2.7%
giảm hiệu suất tương đối.

Model Length ∆F1 Speedup
Tgen 512 82.12 1.00
Distil. + Tgen 512 -0.78 2.43
Distil. + T2500
gen 128 -0.32 8.81
Distil. + T2500
gen 64 -2.70 18.13

Bảng 3: Kết quả macro-F1 và tăng tốc suy luận trên
LEDGAR với DistilBERT. MWT được chứng minh
tương thích cao với các mô hình chưng cất.

5 Kết luận
Trong công trình này, chúng tôi đề xuất một cách
tiếp cận nén chuỗi giảm các đầu vào văn bản bằng
cách khai thác việc sử dụng các biểu thức đa-từ
được rút ra từ tập huấn luyện theo tần suất top-K
của chúng. Chúng tôi tiến hành điều tra trên 3 bộ
dữ liệu khác nhau bằng cách đánh giá mỗi mô
hình kết hợp với các phương pháp nén khác (Gee
et al., 2022; Sanh et al., 2019). Cách tiếp cận của
chúng tôi được chứng minh mạnh mẽ đối với các
độ dài chuỗi ngắn hơn, do đó mang lại giảm hơn
x4 chi phí tính toán với sự sụt giảm hiệu suất không
đáng kể. Trong tương lai, chúng tôi dự kiến mở
rộng phân tích của mình sang các mô hình ngôn
ngữ và tác vụ khác như sinh ngôn ngữ trong phạm
vi nén chuỗi.

6 Hạn chế
Như đã chứng minh trong bài báo, MWT hoạt động
tốt trên các vấn đề phân loại văn bản. Mặc dù
không tiến hành thí nghiệm trên các tác vụ sinh,
không có hạn chế nào trong việc mở rộng MWT
cho chúng. Khác biệt, việc áp dụng MWT cho các
vấn đề phân loại token có thể gặp thách thức. Cụ
thể, khi merge nhiều từ lại với nhau, không rõ
cách gắn nhãn cho các token hợp nhất đó.

Lời cảm ơn
Công trình này được hỗ trợ bởi dự án IBRIDAI,
một dự án được tài trợ bởi Chương trình Hoạt động
Khu vực "FESR 2014-2020" của Emilia Romagna
(Italy), nghị quyết của Hội đồng Khu vực số
863/2021.

--- TRANG 6 ---
Tài liệu tham khảo
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877-1901.

Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael
Bommarito, Ion Androutsopoulos, Daniel Katz, and
Nikolaos Aletras. 2022. LexGLUE: A benchmark
dataset for legal language understanding in English.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 4310-4330, Dublin, Ireland.
Association for Computational Linguistics.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems ,
35:16344-16359.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. arXiv preprint arXiv:1810.04805 .

Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and
Paolo Torroni. 2022. Fast vocabulary transfer for
language model compression. In Proceedings of the
2022 Conference on Empirical Methods in Natural
Language Processing: Industry Track , pages 409-
416, Abu Dhabi, UAE. Association for Computa-
tional Linguistics.

Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan,
and Pritish Narayanan. 2015. Deep learning with lim-
ited numerical precision. In International conference
on machine learning , pages 1737-1746. PMLR.

Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius, and
Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drug-
related adverse effects from medical case reports.
Journal of Biomedical Informatics , 45(5):885 - 892.
Text Mining and Natural Language Processing in
Pharmacogenomics.

Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2(7).

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.
TinyBERT: Distilling BERT for natural language un-
derstanding. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 4163-
4174.

Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing.
arXiv preprint arXiv:1808.06226 .

--- TRANG 7 ---
Dipesh Kumar and Avijit Thawani. 2022. Bpe beyond
word boundary: How not to use multi word expres-
sions in neural machine translation. In Proceedings
of the Third Workshop on Insights from Negative Re-
sults in NLP , pages 172-179.

Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic
Denoyer, and Marc'Aurelio Ranzato. 2018. Phrase-
based & neural unsupervised machine translation.
arXiv preprint arXiv:1804.07755 .

Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? Advances
in neural information processing systems , 32.

Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.
Learning to compress prompts with gist tokens.
arXiv preprint arXiv:2304.08467 .

OpenAI. 2023. Gpt-4 technical report.

Naoki Otani, Satoru Ozaki, Xingyuan Zhao, Yucen
Li, Micaelah St Johns, and Lori Levin. 2020. Pre-
tokenization of multi-word expressions in cross-
lingual word embeddings. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 4451-4464,
Online. Association for Computational Linguistics.

Aleksandar Petrov, Emanuele La Malfa, Philip HS
Torr, and Adel Bibi. 2023. Language model tokeniz-
ers introduce unfairness between languages. arXiv
preprint arXiv:2305.15425 .

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter.

Mike Schuster and Kaisuke Nakajima. 2012. Japanese
and korean voice search. In 2012 IEEE international
conference on acoustics, speech and signal process-
ing (ICASSP) , pages 5149-5152. IEEE.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1715-1725.

Eva Sharma, Chen Li, and Lu Wang. 2019. BIG-
PATENT: A large-scale dataset for abstractive and
coherent summarization. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 2204-2213, Florence, Italy. Asso-
ciation for Computational Linguistics.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, and Kurt
Keutzer. 2020. Q-BERT: Hessian based ultra low
precision quantization of BERT. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 34, pages 8815-8821.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020. MobileBERT:
a compact task-agnostic BERT for resource-limited
devices. arXiv preprint arXiv:2004.02984 .

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .

Don Tuggener, Pius von Däniken, Thomas Peetz, and
Mark Cieliebak. 2020. Ledgar: A large-scale multi-
label corpus for text classification of legal provisions
in contracts. In Proceedings of the 12th Language
Resources and Evaluation Conference , pages 1235-
1241.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. Advances in Neural In-
formation Processing Systems , 33:5776-5788.

Michael Zhu and Suyog Gupta. 2017. To prune, or not
to prune: exploring the efficacy of pruning for model
compression. arXiv preprint arXiv:1710.01878 .

--- TRANG 8 ---
A Chi tiết Bổ sung
A.1 Kết quả
Chúng tôi lập bảng kết quả hoàn chỉnh cho BERT và
DistilBERT trên ADE, LEDGAR, và PATENT trong
Bảng 4 và 5 tương ứng. Các giá trị trong mỗi bảng
được tính trung bình trên 5 seed.

--- TRANG 9 ---
ModelĐộ dài Chuỗi Tối đa
128 64 32 16
Tgen 90.74±0.84 91.22 ±0.74 87.78 ±0.74 76.04 ±2.09
T1000
gen 90.66±0.70 90.62 ±0.41 88.62 ±0.41 80.26 ±0.91
T2500
gen 91.08±0.54 90.76 ±0.87 89.06 ±0.87 80.76 ±0.93
T5000
gen 91.00±0.68 91.28 ±0.62 89.28 ±0.62 79.92 ±1.42
T100 90.96±0.67 90.82 ±0.71 89.32 ±0.71 82.82 ±0.85
T1000
100 89.96±1.21 90.38 ±0.48 89.00 ±0.48 85.18 ±1.11
T2500
100 89.94±0.72 90.56 ±0.61 89.54 ±0.61 85.78 ±0.72
T5000
100 90.28±0.65 90.38 ±0.75 90.70 ±0.75 84.94 ±0.45
(a) ADE

ModelĐộ dài Chuỗi Tối đa
512 256 128 64
Tgen 82.12±0.33 81.94 ±0.36 81.46 ±0.39 79.62 ±0.56
T1000
gen 82.56±0.24 82.52 ±0.35 82.12 ±0.40 80.54 ±0.37
T2500
gen 82.16±0.44 82.24 ±0.40 81.92 ±0.54 80.80 ±0.57
T5000
gen 82.08±0.41 82.02 ±0.20 81.66 ±0.19 80.70 ±0.16
T100 82.12±0.41 82.34 ±0.21 81.68 ±0.43 79.74 ±0.66
T1000
100 82.38±0.58 82.30 ±0.68 81.80 ±0.34 80.84 ±0.23
T2500
100 81.96±0.57 81.78 ±0.60 82.06 ±0.35 80.72 ±0.57
T5000
100 82.14±0.58 82.32 ±0.35 81.92 ±0.31 80.92 ±0.71
(b) LEDGAR

ModelĐộ dài Chuỗi Tối đa
256 128 64 32
Tgen 61.44±0.38 61.28 ±0.37 60.46 ±0.24 58.60 ±0.60
T1000
gen 61.18±0.54 61.28 ±0.36 60.40 ±0.45 59.46 ±0.50
T2500
gen 61.40±0.46 61.40 ±0.69 61.22 ±0.68 59.26 ±0.42
T5000
gen 61.16±0.69 61.08 ±0.49 60.40 ±0.71 59.14 ±0.44
T100 60.66±0.39 60.62 ±1.04 59.52 ±0.63 58.44 ±0.63
T1000
100 60.96±0.62 60.16 ±0.68 59.48 ±0.25 58.76 ±0.63
T2500
100 60.80±0.42 60.36 ±1.02 59.98 ±1.15 58.78 ±0.58
T5000
100 60.42±0.44 59.80 ±0.73 59.54 ±0.46 58.24 ±1.76
(c) PATENT

Bảng 4: Hiệu suất mô hình BERT tính trung bình trên 5 seed.

--- TRANG 10 ---
ModelĐộ dài Chuỗi Tối đa
128 64 32 16
Distil. + Tgen 90.66±0.69 91.66 ±0.43 87.56 ±1.64 74.78 ±1.50
Distil. + T1000
gen 90.18±0.89 90.44 ±0.73 88.16 ±0.81 78.74 ±0.88
Distil. + T2500
gen 91.08±0.28 90.64 ±0.53 88.30 ±0.96 79.24 ±1.37
Distil. + T5000
gen 89.60±0.92 90.22 ±1.11 88.06 ±0.79 79.52 ±1.16
Distil. + T100 90.52±0.48 89.76 ±0.84 88.54 ±1.01 81.16 ±0.91
Distil. + T1000
100 88.26±0.86 89.10 ±0.44 88.52 ±0.68 82.84 ±0.35
Distil. + T2500
100 88.58±1.20 89.10 ±1.18 89.32 ±1.01 83.38 ±0.62
Distil. + T5000
100 87.68±0.92 87.94 ±1.22 87.88 ±0.55 82.84 ±0.77
(a) ADE

ModelĐộ dài Chuỗi Tối đa
512 256 128 64
Distil. + Tgen 81.48±0.52 81.12 ±0.50 81.18 ±0.31 79.22 ±0.29
Distil. + T1000
gen 82.02±0.83 82.30 ±0.31 81.56 ±0.44 80.20 ±0.41
Distil. + T2500
gen 81.74±0.23 81.36 ±0.25 81.86 ±0.18 79.90 ±1.01
Distil. + T5000
gen 81.38±0.52 81.62 ±0.29 81.60 ±0.29 80.34 ±0.28
Distil. + T100 81.42±0.70 81.60 ±0.12 81.50 ±0.48 80.02 ±0.54
Distil. + T1000
100 81.42±0.59 80.90 ±0.68 81.98 ±0.18 80.62 ±0.47
Distil. + T2500
100 81.80±0.17 81.36 ±0.30 82.06 ±0.27 80.46 ±0.38
Distil. + T5000
100 81.58±0.57 81.34 ±0.42 81.92 ±0.18 80.82 ±0.43
(b) LEDGAR

ModelĐộ dài Chuỗi Tối đa
256 128 64 32
Distil. + Tgen 60.88±0.61 60.98 ±0.67 59.88 ±0.57 57.72 ±0.71
Distil. + T1000
gen 60.58±0.31 59.92 ±0.63 59.94 ±0.94 58.36 ±0.62
Distil. + T2500
gen 59.96±0.75 59.94 ±0.43 59.90 ±0.65 58.16 ±0.61
Distil. + T5000
gen 59.86±0.61 60.10 ±0.88 59.26 ±0.53 58.46 ±0.52
Distil. + T100 59.58±0.77 59.22 ±0.59 58.10 ±0.70 57.22 ±0.59
Distil. + T1000
100 59.52±0.49 59.88 ±0.54 58.72 ±0.47 57.42 ±0.72
Distil. + T2500
100 59.04±0.32 58.82 ±0.95 57.58 ±0.53 56.76 ±0.47
Distil. + T5000
100 59.82±0.57 58.74 ±0.40 58.76 ±0.59 57.30 ±1.01
(c) PATENT

Bảng 5: Hiệu suất mô hình DistilBERT tính trung bình trên 5 seed.
