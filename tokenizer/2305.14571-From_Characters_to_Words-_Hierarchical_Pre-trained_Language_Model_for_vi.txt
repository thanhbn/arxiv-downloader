# 2305.14571.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/2305.14571.pdf
# Kích thước tệp: 504090 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Từ Ký tự đến Từ: Mô hình Ngôn ngữ Phân cấp Được Huấn luyện Trước cho
Hiểu biết Ngôn ngữ Từ vựng Mở
Li Sun1†, Florian Luisier2, Kayhan Batmanghelich1, Dinei Florencio2, Cha Zhang2
1Đại học Boston2Microsoft
1{lisun,batman}@bu.edu ,2{flluisie,dinei,chazhang}@microsoft.com
Tóm tắt
Các mô hình hiện đại hàng đầu cho hiểu biết ngôn ngữ tự nhiên yêu cầu một bước tiền xử lý để chuyển đổi văn bản thô thành các token rời rạc. Quá trình này được gọi là tokenization dựa vào một từ vựng được xây dựng sẵn gồm các từ hoặc hình vị từ phụ. Từ vựng cố định này hạn chế tính bền vững của mô hình đối với lỗi chính tả và khả năng thích ứng với các lĩnh vực mới. Trong công trình này, chúng tôi giới thiệu một mô hình ngôn ngữ từ vựng mở mới áp dụng phương pháp phân cấp hai cấp độ: một ở cấp độ từ và một khác ở cấp độ chuỗi. Cụ thể, chúng tôi thiết kế một module nội từ sử dụng kiến trúc Transformer nông để học biểu diễn từ từ các ký tự của chúng, và một module Transformer liên từ sâu để ngữ cảnh hóa mỗi biểu diễn từ bằng cách chú ý đến toàn bộ chuỗi từ. Do đó mô hình của chúng tôi trực tiếp hoạt động trên các chuỗi ký tự với nhận thức rõ ràng về ranh giới từ, nhưng không có từ vựng từ phụ hoặc cấp từ thiên vị. Các thí nghiệm trên nhiều tác vụ xuôi dòng cho thấy phương pháp của chúng tôi vượt trội hơn các baseline mạnh. Chúng tôi cũng chứng minh rằng mô hình phân cấp của chúng tôi bền vững với sự hỏng hóc văn bản và chuyển đổi lĩnh vực.

1 Giới thiệu
Các mô hình ngôn ngữ được huấn luyện trước với Transformer đã đạt được những đột phá trong nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) (Devlin et al., 2019; Liu et al., 2019). Một trong những lợi thế chính của Transformer so với các pipeline NLP kỹ thuật đặc trưng truyền thống là Transformer cho phép huấn luyện đầu cuối từ lượng dữ liệu khổng lồ để tự động học biểu diễn ngôn ngữ tối ưu (Mikolov et al., 2013b). Tuy nhiên, hầu hết các mô hình ngôn ngữ gần đây vẫn yêu cầu một giai đoạn tiền xử lý riêng biệt được gọi là tokenization. Tokenization là một quá trình chia các phần văn bản thô thành một danh sách các token rời rạc từ một từ vựng cố định. Từ vựng được định nghĩa trước này vẫn là một nút thắt cổ chai quan trọng ngăn cản việc huấn luyện đầu cuối thực sự của các mô hình ngôn ngữ (Tay et al., 2021; Islam et al., 2022).

†Công việc được thực hiện trong thời gian thực tập tại Microsoft.
Mã giả kiểu PyTorch có sẵn trong Phụ lục A.3.

Từ BERT Token Lỗi chính tả
changable (changeable) ch, ##anga, ##ble (change, ##able)
outragous (outrageous) out, ##rag, ##ous (outrage, ##ous)
Chuyển đổi lĩnh vực
reimbursement re, ##im, ##bur, ##se, ##ment
invoice in, ##vo, ##ice

Bảng 1: Ví dụ về tokenization từ phụ của các từ bị sai chính tả (với chính tả đúng trong ngoặc đơn) và các từ từ corpus chuyên biệt lĩnh vực (ví dụ: tài liệu kinh doanh). Tokenizer từ phụ được huấn luyện trước có xu hướng phân đoạn quá mức các từ từ hai danh mục này, dẫn đến các token ít ý nghĩa hơn.

Dựa trên độ chi tiết của các đơn vị token cơ bản, các phương pháp tokenization có thể được chia thành ba danh mục: dựa trên ký tự, dựa trên từ phụ và dựa trên từ. Một tokenizer dựa trên từ phân đoạn câu thành các khối từ nhỏ hơn. Do sự phức tạp của ngôn ngữ và giới hạn bộ nhớ, một từ vựng dựa trên từ không thể đại diện cho tất cả các từ có thể. Do đó tokenization cấp từ thường gặp phải vấn đề từ ngoài từ vựng. Một tokenizer dựa trên ký tự đơn giản chia văn bản thành một chuỗi các ký tự của nó. Nó linh hoạt để mã hóa các từ tùy ý, nhưng tokenization cấp ký tự tạo ra các chuỗi dài, điều này không mong muốn vì chi phí tính toán của Transformer tăng theo bậc hai với độ dài chuỗi. Để tạo ra sự cân bằng tốt giữa độ phức tạp về thời gian và không gian, hầu hết các mô hình ngôn ngữ được huấn luyện trước hiện đại do đó áp dụng tokenization từ phụ. Các tokenizer từ phụ dựa trên dữ liệu (Kudo and Richardson, 2018; Schuster and Nakajima, 2012; Kudo, 2018) thường được huấn luyện trước trên một corpus văn bản tổng quát để học một từ vựng từ phụ dựa trên tần suất của các mảnh từ.

Mặc dù phổ biến, các tokenizer từ phụ hạn chế tính bền vững và khả năng tổng quát hóa của các mô hình ngôn ngữ được xây dựng dựa trên chúng. Thứ nhất, các tokenizer từ phụ nhạy cảm với các nhiễu loạn văn bản nhỏ (Xue et al., 2022). Trong khi con người vẫn có thể hiểu văn bản với những lỗi chính tả tinh tế và biến thể viết hoa (Rawlinson, 2007; Davis, 2003), những nhiễu loạn này có thể thay đổi đáng kể kết quả tokenization, có khả năng dẫn đến biểu diễn văn bản dưới tối ưu. Thứ hai, từ vựng từ phụ được xây dựng sẵn và vẫn được đóng băng trong quá trình huấn luyện trước mô hình ngôn ngữ và tinh chỉnh chuyên biệt tác vụ. Do đó, khi thích ứng một mô hình ngôn ngữ được huấn luyện trước vào một ngữ cảnh ngôn ngữ mới (ví dụ: văn bản y sinh và tài liệu kinh doanh), tokenizer dễ bị phân mảnh quá mức các mảnh từ phụ (Yasunaga et al., 2022; Islam et al., 2022), như được minh họa trong Bảng 1. Trong khi vấn đề này có thể được khắc phục một phần bằng việc huấn luyện trước chuyên biệt tác vụ thêm hoặc bằng cách thu thập thêm dữ liệu tinh chỉnh, việc giảm thiểu như vậy sẽ tốn kém và không phải lúc nào cũng khả thi.

Chúng tôi nhằm mang lại điều tốt nhất của cả mô hình dựa trên ký tự và dựa trên từ để giải quyết các thách thức được thảo luận ở trên. Để đạt được điều này, chúng tôi đề xuất một mô hình ngôn ngữ được huấn luyện trước mới với kiến trúc phân cấp hai cấp độ. Ở cấp độ từ, chúng tôi chia chuỗi văn bản theo ký tự, và giới thiệu một module nội từ sử dụng Transformer để học một biểu diễn cho mỗi từ trong chuỗi từ các embedding của các ký tự tương ứng của chúng. Ở cấp độ chuỗi, chúng tôi giới thiệu một module liên từ ngữ cảnh hóa embedding cho mỗi từ trong chuỗi văn bản. Phương pháp của chúng tôi không yêu cầu từ vựng từ phụ hoặc cấp từ rõ ràng, và do đó có thể được coi là một phương pháp từ vựng mở (Mielke et al., 2021). Bằng cách hạn chế phạm vi chú ý đến các ký tự trong cùng một từ thay vì toàn bộ chuỗi trong module nội từ, mô hình của chúng tôi vẫn hiệu quả về mặt tính toán.

Để xác thực mô hình của chúng tôi, chúng tôi so sánh toàn diện phương pháp của chúng tôi với các phương pháp baseline khác nhau, bao gồm mô hình dựa trên từ phụ phổ biến nhất BERT (Devlin et al., 2019), một số mô hình dựa trên ký tự hiện đại (Clark et al., 2022a; Boukkouri et al., 2020), và một mô hình hybrid ký tự/từ phụ (Ma et al., 2020). Ngoài benchmarking tiêu chuẩn, chúng tôi cũng kiểm tra tính bền vững của các mô hình khác nhau theo hai cách: bằng cách đưa noise chính tả vào tập xác thực và bằng cách kiểm tra trên các tác vụ chéo lĩnh vực.

Các đóng góp của chúng tôi có thể được tóm tắt như sau:

• Chúng tôi giới thiệu một mô hình ngôn ngữ được huấn luyện trước từ vựng mở mới với kiến trúc phân cấp hai cấp độ. Phương pháp của chúng tôi không dựa vào từ vựng từ hoặc từ phụ được định nghĩa trước.

• Chúng tôi đề xuất một phương pháp tổng hợp thích ứng và có thể học mới để tóm tắt các đặc trưng cấp ký tự thành các biểu diễn cấp từ. Một nghiên cứu ablation làm nổi bật hiệu quả của nó.

• Chúng tôi cho thấy phương pháp của chúng tôi vượt trội hơn các baseline mạnh trên nhiều bộ dữ liệu benchmarking, trong khi hiệu quả về mặt tính toán.

• Chúng tôi thực hiện các thí nghiệm định lượng và một nghiên cứu trường hợp để cho thấy mô hình của chúng tôi bền vững với sự hỏng hóc văn bản và chuyển đổi lĩnh vực.

2 Công trình Liên quan

2.1 Mô hình Cấp từ
Các phương pháp embedding từ bao gồm Word2vec (Mikolov et al., 2013a) và GloVe (Pennington et al., 2014) đã dẫn đến nhiều đột phá NLP ban đầu. Các phương pháp này học các biểu diễn không gian vector của từ từ các corpus không được gán nhãn quy mô lớn, và mã hóa các mối quan hệ và ý nghĩa ngữ nghĩa (Goldberg and Levy, 2014). Để tổng quát hóa cho các từ hiếm, Bhatia et al. (2016) đề xuất sử dụng LSTM để học embedding từ từ cả cấu trúc hình thái và phân phối từ. Trong khi các phương pháp ban đầu chỉ học một biểu diễn từ độc lập ngữ cảnh, ELMo (Peters et al., 2018) đề xuất sử dụng một mô hình ngôn ngữ hai chiều sâu để học các biểu diễn từ được ngữ cảnh hóa. Trong các nghiên cứu gần đây hơn, Transformer-XL (Dai et al., 2019) đã tăng cường kiến trúc Transformer với một cơ chế tái phát để học embedding từ được ngữ cảnh hóa thông qua mô hình hóa ngôn ngữ. Mặc dù có tiến bộ gần đây, các mô hình cấp từ vẫn đối mặt với thách thức ngoài từ vựng đối với văn bản nhiễu và các dạng từ không chuẩn (Eisenstein, 2013).

2.2 Mô hình Cấp ký tự
Các mô hình ngôn ngữ cấp ký tự xuất hiện trong những năm đầu nhờ sự đơn giản và khả năng giải quyết tốt hơn các từ ngoài từ vựng so với các mô hình cấp từ (Elman, 1990; Graves, 2013; Kalchbrenner et al., 2016). Trong khi các phương pháp dựa trên từ phụ trở nên phổ biến trong mô hình hóa ngôn ngữ do hiệu suất vượt trội của chúng, các nghiên cứu gần đây (Choe et al., 2019; Xue et al., 2022) cho thấy các mô hình cấp ký tự/byte có thể sánh ngang hiệu suất với đối tác từ phụ của chúng khi được cung cấp đủ khả năng tham số. Ngoài ra, các mô hình cấp ký tự đã được chứng minh là bền vững hơn với sự hỏng hóc văn bản (Tay et al., 2021), các cuộc tấn công đối nghịch, và chuyển đổi lĩnh vực (Aguilar et al., 2020).

Các mô hình cấp ký tự cũng cho thấy kết quả đầy hứa hẹn trong các thiết lập đa ngôn ngữ. Trong khi các tokenizer từ phụ hoặc từ yêu cầu một từ vựng khổng lồ để bao phủ đầy đủ các ngôn ngữ khác nhau, từ vựng đa ngôn ngữ dựa trên ký tự có thể vẫn toàn diện và nhỏ. Lớp embedding văn bản không chiếm hết phần lớn ngân sách tham số của mô hình như trong mô hình BERT Base đa ngôn ngữ chẳng hạn (lên đến 52%). Nhiều tham số hơn sau đó có thể được dành cho các lớp Transformer trong các phương pháp dựa trên ký tự. Các mô hình cấp ký tự cũng đã được chứng minh hoạt động tốt hơn trên các ngôn ngữ ít tài nguyên (Islam et al., 2022).

Một nhược điểm quan trọng của các mô hình cấp ký tự là chúng thường yêu cầu nhiều tính toán hơn các mô hình cấp từ phụ và từ. Điều này là do tokenization cấp ký tự tạo ra các chuỗi token dài hơn so với các phương pháp dựa trên từ phụ hoặc từ, và các yêu cầu tính toán và bộ nhớ của cơ chế self-attention tăng theo bậc hai với độ dài chuỗi. Để giải quyết thách thức này, CANINE (Clark et al., 2022b) tận dụng convolution có bước để downsample chuỗi ký tự, trong khi Charformer (Tay et al., 2021) sử dụng average pooling. Mặc dù các phương pháp này cải thiện hiệu quả tính toán của các mô hình cấp ký tự, chúng yêu cầu tỷ lệ downsampling tĩnh được định nghĩa trước. Hoạt động downsampling như vậy thường phá vỡ ranh giới của các đơn vị ngôn ngữ cơ bản, bao gồm hình vị và từ.

2.3 Mô hình Hybrid
Các mô hình cấp ký tự thuần túy không trích xuất rõ ràng các biểu diễn từ hoặc hình vị từ phụ, điều này có thể ảnh hưởng tiêu cực đến hiệu suất của chúng trên các tác vụ xuôi dòng cấp từ, bao gồm nhận dạng thực thể có tên và trả lời câu hỏi trích xuất. Để giải quyết vấn đề này, đã có những nỗ lực kết hợp các phương pháp cấp ký tự và cấp từ/từ phụ để xây dựng các mô hình hybrid. Các công trình này đề xuất sử dụng thông tin từ chính tả ký tự để thông báo biểu diễn từ. Ví dụ, Flair (Akbik et al., 2018) đề xuất sử dụng các trạng thái nội bộ của một mô hình ngôn ngữ ký tự được huấn luyện trước để tạo ra các embedding cấp từ. CharBERT (Ma et al., 2020) kết hợp các token từ phụ và token ký tự và hợp nhất các biểu diễn không đồng nhất của chúng. CharacterBERT (Boukkouri et al., 2020) sử dụng CNN để học các biểu diễn cấp từ từ các embedding của các ký tự của chúng, nhưng vẫn yêu cầu một từ vựng cấp từ để huấn luyện trước. Char2Subword (Aguilar et al., 2020) đề xuất một phương pháp tương tự, trong đó các embedding ký tự được sử dụng để bắt chước biểu diễn được huấn luyện trước của các token từ phụ với bộ mã hóa Transformer.

3 Phương pháp
Hầu hết các mô hình bộ mã hóa Transformer cấp ký tự đều dưới tối ưu vì hai lý do: (1) Self-attention dày đặc trên chuỗi ký tự dài tốn kém về mặt tính toán; (2) Chúng không tận dụng ranh giới từ, đây là một thiên kiến quy nạp quan trọng trong ngôn ngữ học. Để vượt qua những thách thức này, chúng tôi đề xuất phân tách bộ mã hóa Transformer cấp ký tự dày đặc thành hai phần: bộ mã hóa Transformer nội từ và bộ mã hóa Transformer liên từ. Mô hình ngôn ngữ phân cấp (HLM) của chúng tôi áp dụng cấu trúc đồng hồ cát (Nawrot et al., 2022) và chứa ba thành phần chính: (1) một module nội từ học các embedding từ từ các ký tự của chúng; (2) một module liên từ ngữ cảnh hóa các biểu diễn từ bằng cách chú ý đến tất cả các từ trong chuỗi đầu vào; (3) một đầu dự đoán nội từ cho việc huấn luyện trước cấp ký tự. Kiến trúc tổng thể của mô hình chúng tôi được hiển thị trong Hình 1. Trong các phần tiếp theo, chúng tôi thảo luận từng thành phần riêng biệt.

3.1 Module Nội từ
Chúng tôi nhằm học các biểu diễn cấp từ từ các embedding của các ký tự của chúng. Một phương pháp lý tưởng nên có thể xử lý các từ có độ dài tùy ý, chú ý đến mọi ký tự thay vì một cửa sổ cục bộ, và vẫn hiệu quả về mặt tính toán. Do đó, chúng tôi chọn một bộ mã hóa Transformer nông (4 lớp trong các thí nghiệm của chúng tôi) để học các embedding ký tự được ngữ cảnh hóa, thay vì CNN hoặc LSTM được sử dụng bởi các phương pháp trước đây (Boukkouri et al., 2020; Peters et al., 2018). Average hoặc max pooling (Boukkouri et al., 2020; Clark et al., 2022b) thường được sử dụng để tổng hợp các embedding ký tự được ngữ cảnh hóa và do đó giảm độ dài chuỗi. Tuy nhiên, pooling đơn giản như vậy có xu hướng làm mờ các tín hiệu mạnh từ các hình vị cụ thể (Fathi and Maleki Shoja, 2018). Để giải quyết thách thức này, chúng tôi đề xuất một phương pháp tổng hợp thích ứng và có thể học mới. Lấy cảm hứng từ phương pháp sử dụng trạng thái ẩn của token [CLS] làm biểu diễn tổng hợp cấp chuỗi, chúng tôi chèn một token đặc biệt [WORD_CLS] ở đầu mỗi từ. Các embedding của các token [WORD_CLS] sau đó được sử dụng làm biểu diễn cấp từ. Chính thức, đối với từ thứ i có Ci ký tự trong chuỗi, chúng tôi trích xuất biểu diễn cấp từ ri của nó như:

hi = ftheta(ei0 ⊕ ei1 ⊕ ... ⊕ eiCi)
ri = hi0,

trong đó ftheta là Transformer nội từ tạo ra biểu diễn được ngữ cảnh hóa hi cho mỗi ký tự của từ thứ i, ei0 là embedding của token đặc biệt [WORD_CLS], eic là embedding ký tự thứ c của từ thứ i, và ⊕ biểu thị nối theo chiều chuỗi.

Trong Mục 4.4, chúng tôi tiến hành một nghiên cứu ablation để chỉ ra rằng phương pháp tổng hợp được đề xuất vượt trội hơn average hoặc max pooling tiêu chuẩn. Bằng cách tổng hợp các token cấp ký tự thành các token cấp từ, độ dài chuỗi token được giảm đáng kể cho module liên từ tiếp theo.

3.2 Module Liên từ
Sau khi thu được các đặc trưng cấp từ, chúng tôi áp dụng một module liên từ bao gồm các lớp bộ mã hóa transformer sâu để trích xuất biểu diễn cấp từ được ngữ cảnh hóa bằng cách chú ý đến tất cả các từ trong chuỗi. Chính thức, biểu diễn được ngữ cảnh hóa wi của từ thứ i của chuỗi N từ được cho bởi:

wi = fϕ(r0 ⊕ ... ⊕ rN-1),

trong đó fϕ biểu thị Transformer liên từ.

Chúng tôi đặt độ sâu của stack bộ mã hóa Transformer liên từ là 12 để phù hợp với các thiết lập của BERT Base (Devlin et al., 2019) và CANINE (Clark et al., 2022b). Module liên từ đóng góp nhiều nhất vào tổng số tham số mô hình.

3.3 Đầu Dự đoán Nội từ
Vì chúng tôi áp dụng phương pháp từ vựng mở, chúng tôi đề xuất sử dụng mô hình hóa ngôn ngữ có mặt nạ cấp ký tự làm tác vụ huấn luyện trước. Để khôi phục chuỗi token cấp ký tự, chúng tôi nối các biểu diễn ký tự được ngữ cảnh hóa từ module nội từ (token [WORD_CLS] ban đầu được bỏ qua) với các đặc trưng cấp từ từ module liên từ theo chiều chuỗi. Cuối cùng, chúng tôi áp dụng một đầu dự đoán nội từ nhẹ để có được xác suất token hậu nghiệm.

Chính thức, dự đoán của Ci ký tự từ từ thứ i được cho bởi:

ci = fsigma(wi ⊕ hi1 ⊕ ... ⊕ hiCi),

trong đó fsigma là đầu dự đoán nội từ, bao gồm một lớp Transformer đơn, một lớp fully-connected và một lớp Softmax. Lưu ý rằng đầu dự đoán nội từ chỉ được sử dụng trong quá trình huấn luyện trước cho tác vụ mô hình hóa ký tự có mặt nạ. Trong quá trình tinh chỉnh xuôi dòng, tương tự như CANINE, chúng tôi nối embedding từ ban đầu ri và biểu diễn từ được ngữ cảnh hóa wi theo chiều đặc trưng, và sau đó sử dụng một mạng feed-forward nhỏ để tích hợp cả thông tin cấp thấp và cấp cao cho dự đoán.

3.4 Tác vụ Huấn luyện Trước
Theo thực hành của BERT, chúng tôi huấn luyện trước mô hình của chúng tôi trên bộ dữ liệu English Wikipedia và BookCorpus (19G) (Zhu et al., 2015). Chúng tôi huấn luyện trước mô hình trong 3 epoch (3.9M bước với kích thước batch được đặt là 16) trên một máy chủ với 8 GPU NVIDIA Tesla V100, và mỗi epoch mất 137 giờ. Chúng tôi áp dụng mô hình hóa ngôn ngữ có mặt nạ toàn từ làm tác vụ huấn luyện trước. Chi tiết, chúng tôi chọn ngẫu nhiên 15% từ từ chuỗi đầu vào, và che mặt nạ mọi ký tự trong từ được chọn. Chúng tôi thay thế các token ký tự trong 80% từ có mặt nạ được chọn bằng token [MASK]. Đối với 10% từ có mặt nạ được chọn, chúng tôi thay thế các ký tự của chúng bằng các ký tự được chọn ngẫu nhiên từ từ vựng ký tự của chúng tôi. 10% từ còn lại không thay đổi. Ba thành phần chính của mô hình chúng tôi được huấn luyện cùng nhau theo cách đầu cuối.

3.5 Chi tiết Triển khai
Chúng tôi sử dụng spaCy (Honnibal et al., 2020) để chia câu thành từ, đây là phương pháp dựa trên quy tắc sử dụng khoảng trắng, dấu câu và quy tắc đặc biệt (ví dụ: chia don't thành do và n't). Chúng tôi sử dụng từ vựng ký tự phân biệt hoa thường có kích thước 1024, bao gồm chữ cái, chữ số và ký hiệu. Độ dài chuỗi tối đa được đặt là 20 ký tự cho module nội từ và 512 từ cho module liên từ. Một token [CLS] và một token [SEP] được chèn vào đầu và cuối mỗi chuỗi tương ứng. Kích thước ẩn được đặt là 768, số đầu attention được đặt là 12, chiều feed-forward trong bộ mã hóa Transformer được đặt là 1536 và 3072 cho module nội từ và liên từ tương ứng. Chúng tôi tận dụng vị trí tương đối (He et al., 2021) trong mô hình của chúng tôi, và chúng tôi không sử dụng embedding loại token. GELU (Hendrycks and Gimpel, 2016) được sử dụng làm hàm kích hoạt. Mô hình của chúng tôi chứa 125M tham số. Chúng tôi sử dụng trình tối ưu hóa AdamW (Loshchilov and Hutter, 2018) để huấn luyện trước và tinh chỉnh mô hình. Đối với việc huấn luyện trước, weight decay được đặt là 0.01 và số bước warmup được đặt là 10,000. Một lịch trình phân rã tốc độ học tuyến tính được sử dụng, bắt đầu từ 5e-5. Tỷ lệ dropout được đặt là 0.1. Thêm chi tiết thuật toán có thể được tìm thấy trong Phụ lục A.3.

4 Thí nghiệm
Chúng tôi đánh giá hiệu suất của mô hình được huấn luyện trước của chúng tôi trên một loạt các tác vụ xuôi dòng. Chúng tôi so sánh hiệu suất của mô hình ngôn ngữ phân cấp được huấn luyện trước của chúng tôi (HLM) với các phương pháp baseline khác nhau, bao gồm mô hình BERT dựa trên từ phụ phổ biến, ba mô hình cấp byte/ký tự gần đây, cũng như một mô hình hybrid được gọi là CharacterBERT. Đối với BERT, chúng tôi sử dụng mô hình BERT Base cased (108M tham số) để phù hợp với thiết lập module Transformer liên từ của chúng tôi. Đối với CANINE, chúng tôi áp dụng CANINE-C (132M) cũng sử dụng tác vụ huấn luyện trước cấp ký tự. Đối với CharacterBERT, chúng tôi sử dụng phiên bản tổng quát (105M) được huấn luyện trước trên English Wikipedia và OpenWebText. Đối với những mô hình baseline đó, chúng tôi sử dụng các trọng số được huấn luyện trước được lưu trữ trên Huggingface† hoặc được phát hành bởi các tác giả. Đối với Charformer (203M) và Byte-level T5 (200M), chúng tôi sử dụng kết quả của phiên bản base từ bài báo gốc vì trọng số được huấn luyện trước không có sẵn.

4.1 Đánh giá trên Benchmark Tiêu chuẩn
Để đánh giá hiệu suất của mô hình chúng tôi trên lĩnh vực tổng quát, chúng tôi đánh giá các phương pháp của chúng tôi trên các benchmark NLP tiếng Anh tiêu chuẩn, bao gồm tác vụ Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016, 2018) và các tác vụ GLUE (Wang et al., 2018). Đối với tác vụ SQuAD, chúng tôi benchmark trên cả phiên bản SQuAD 1.1 và 2.0. Bộ dữ liệu SQuAD 1.1 chứa 100,000+ câu hỏi với các tài liệu ngữ cảnh liên quan, và mọi câu hỏi đều có thể trả lời được dựa trên ngữ cảnh. Bộ dữ liệu SQuAD 2.0 chứa thêm 50,000 câu hỏi không thể trả lời được. Chúng tôi tinh chỉnh các mô hình trong 2 epoch với kích thước batch là 16, và tốc độ học là 3e-5. Đánh giá trên tập xác thực được hiển thị trong Bảng 2 (trái). Chúng tôi sử dụng exact match (EM) và điểm F1 làm hai thước đo đánh giá. Phương pháp của chúng tôi vượt trội hơn tất cả các phương pháp baseline trên cả hai phiên bản SQuAD.

†https://huggingface.co/models

Chúng tôi cũng benchmark mô hình của chúng tôi trên ba tác vụ phân loại văn bản từ các tác vụ GLUE được áp dụng rộng rãi (Wang et al., 2018), bao gồm MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005) và QNLI (Rajpurkar et al., 2016). Bộ dữ liệu MNLI chứa 393k mẫu huấn luyện với chú thích entailment văn bản. Cho một cặp câu chứa một tiền đề và một giả thuyết, tác vụ là dự đoán liệu tiền đề có entail giả thuyết, mâu thuẫn với giả thuyết, hay không cái nào. Chúng tôi tiến hành đánh giá trong cả thiết lập matched và mismatched. Bộ dữ liệu MRPC chứa 3.7k cặp câu huấn luyện, và tác vụ là dự đoán liệu hai câu có tương đương về mặt ngữ nghĩa hay không. Bộ dữ liệu QNLI chứa 108k mẫu huấn luyện của các cặp câu hỏi-đoạn văn, và tác vụ là dự đoán liệu câu ngữ cảnh có chứa câu trả lời cho câu hỏi hay không. Chúng tôi tinh chỉnh các mô hình trên các bộ dữ liệu được mô tả ở trên trong 5 epoch, với kích thước batch là 16, và tốc độ học là 2e-5. Chúng tôi sử dụng độ chính xác làm thước đo đánh giá. Như được hiển thị trong Bảng 2, phương pháp được đề xuất của chúng tôi vượt trội hơn các phương pháp baseline trên tất cả các tác vụ.

Để điều tra hiệu suất của mô hình khi kích thước được mở rộng, chúng tôi tăng kích thước HLM của chúng tôi để phù hợp với BERT Large và benchmark hiệu suất. Các kết quả sơ bộ có thể được tìm thấy trong Phụ lục A.2.

4.2 Tính Bền vững với Sự Hỏng hóc Văn bản
Con người dễ mắc lỗi chính tả. Ví dụ, 10-15% truy vấn tìm kiếm web chứa lỗi chính tả (Dalianis, 2002; Cucerzan and Brill, 2004). Để kiểm tra tính bền vững của mô hình đối với lỗi chính tả, chúng tôi thêm noise tổng hợp vào tập tinh chỉnh và đánh giá của các tác vụ xuôi dòng và đánh giá lại tất cả các mô hình.

Theo thực hành của Xue et al. (2022), chúng tôi thử nghiệm với ba loại noise: (1) Random drop: Chúng tôi xóa ngẫu nhiên 10% ký tự (khoảng trắng và dấu câu được bao gồm) từ chuỗi đầu vào; (2) Random repeat: Chúng tôi chọn ngẫu nhiên 20% ký tự, sau đó thêm 1-3 lần lặp lại (với xác suất bằng nhau) sau các ký tự gốc được chọn; (3) Random case: Chúng tôi đặt ngẫu nhiên trường hợp cho mỗi ký tự (hoa hoặc thường) trong chuỗi đầu vào.

Chúng tôi thực hiện các thí nghiệm nhiễu loạn trên hai tác vụ xuôi dòng đại diện: phân loại văn bản trên bộ dữ liệu MNLI và trả lời câu hỏi trên SQuAD 2.0. Đối với bộ dữ liệu MNLI, chúng tôi thêm noise vào cả câu tiền đề và giả thuyết. Đối với bộ dữ liệu SQuAD 2.0, chúng tôi chỉ áp dụng nhiễu loạn cho câu hỏi, nhưng không cho đoạn ngữ cảnh, để tránh sao chép câu trả lời bị hỏng từ ngữ cảnh cho các mô hình QA trích xuất. Kết quả đánh giá được hiển thị trong Bảng 3. Chúng tôi phát hiện rằng hiệu suất của BERT giảm đáng kể dưới nhiễu loạn, một giải thích là ngay cả những lỗi chính tả tinh tế cũng sẽ thay đổi đáng kể kết quả tokenization từ phụ. Ngược lại, các mô hình cấp ký tự bao gồm CANINE giảm ít hơn khi có mặt của noise. Chúng tôi cũng trình bày kết quả cho thiết lập nhiễu loạn chưa nhìn thấy trong Phụ lục A.4. Nhìn chung, HLM được đề xuất của chúng tôi bền vững với các loại nhiễu loạn khác nhau và đạt được hiệu suất tốt nhất.

Để truy cập tính bền vững của mô hình đối với các mức độ nhiễu loạn khác nhau, chúng tôi thêm các lượng noise khác nhau vào bộ dữ liệu QNLI và thực hiện đánh giá. Trong thực tế, chúng tôi lấy mẫu ngẫu nhiên 5%, 10%, 15%, 20% ký tự cho mỗi ví dụ trong dữ liệu tinh chỉnh và tập xác thực. Đối với mỗi ký tự được chọn, chúng tôi hoặc xóa ký tự hoặc lặp lại ký tự như đã đề cập ở trên (xác suất bằng nhau). Độ chính xác trên tập xác thực được hiển thị trong Hình 2.

4.3 Tính Bền vững với Chuyển đổi Lĩnh vực
Hầu hết các mô hình ngôn ngữ chung được huấn luyện trước trên các corpus văn bản được thu thập từ web bao gồm Wikipedia và Common Crawl. Nhưng trong triển khai thế giới thực, các mô hình thường được sử dụng trong một lĩnh vực khác, một vấn đề được gọi là chuyển đổi lĩnh vực. Để đánh giá tính bền vững với chuyển đổi lĩnh vực, chúng tôi tinh chỉnh và đánh giá các mô hình được huấn luyện trước trên các tác vụ xuôi dòng từ các lĩnh vực chuyên biệt bao gồm y sinh và truyền thông xã hội. Đối với lĩnh vực y sinh, chúng tôi thực hiện đánh giá trên bộ dữ liệu NCBI-disease (Crichton et al., 2017; Gu et al., 2021), chứa 7,287 câu được chú thích với các đề cập bệnh từ các tóm tắt PubMed. Tác vụ được khung hóa như một vấn đề nhận dạng thực thể có tên (NER) trong đó các thực thể là các đề cập bệnh. Chúng tôi tinh chỉnh các mô hình trong 20 epoch, với kích thước batch là 16, và tốc độ học là 2e-5. Đối với thí nghiệm truyền thông xã hội, chúng tôi tận dụng tác vụ chia sẻ NER W-NUT16 (Strauss et al., 2016). Bộ dữ liệu này chứa 7,244 tweet được chú thích với 10 danh mục NER, bao gồm người, địa điểm, công ty và các danh mục khác. Chúng tôi tinh chỉnh các mô hình trong 5 epoch. Kết quả đánh giá trên các tập kiểm tra được hiển thị trong Bảng 4. Chúng tôi sử dụng điểm F1 làm thước đo đánh giá. Như quan sát được, HLM được đề xuất vượt trội hơn các phương pháp baseline, làm nổi bật tính bền vững cao hơn của nó đối với chuyển đổi lĩnh vực.

Nghiên cứu trường hợp Để hiểu được lợi ích hiệu suất của mô hình chúng tôi so với BERT dựa trên từ phụ trên các tác vụ chéo lĩnh vực, chúng tôi xem xét các trường hợp mà BERT đưa ra dự đoán không chính xác. Chúng tôi phát hiện rằng nhiều trường hợp này chứa các từ bị phân mảnh quá mức. Bảng 5 hiển thị hai ví dụ từ tác vụ NER NCBI-disease. Từ fragility trong trường hợp 1 được phân đoạn thành f, ##rag, ##ility, và từ rupture trong trường hợp 2 được phân đoạn thành r, ##up, ##ture. Chúng tôi nghĩ những kết quả tokenization này là dưới tối ưu vì chúng phá vỡ các hình vị từ, điều này có thể giải thích các dự đoán sai của BERT.

Ngược lại, chúng tôi sử dụng BertViz (Vig, 2019) để trực quan hóa hành vi của mô hình HLM của chúng tôi. Cụ thể, chúng tôi trực quan hóa các mẫu attention của token [WORD_CLS] của lớp Transformer cuối cùng trong module nội từ của chúng tôi. Như được hiển thị trong Hình 3, token [WORD_CLS] cho từ fragility và rupture chủ yếu được chú ý bởi chuỗi ký tự fragil và rupt tương ứng, đây là từ gốc của các từ.

4.4 Nghiên cứu Ablation
Trong phần này, chúng tôi thực hiện một nghiên cứu ablation để so sánh hiệu quả của các phương pháp tổng hợp cấp từ khác nhau. Cụ thể, chúng tôi thay thế việc tổng hợp dựa trên học token đặc biệt được đề xuất bằng các phương pháp tổng hợp tiêu chuẩn như average pooling và max pooling. Chúng tôi không triển khai strided convolution được đề xuất trong CANINE vì nó không thể xử lý độ dài từ biến đổi. Chúng tôi báo cáo độ chính xác xác thực trên MRPC và điểm F1 kiểm tra trên NCBI-disease trong Bảng 6. Tổng hợp được học của chúng tôi vượt trội hơn các chiến lược pooling tiêu chuẩn. Lưu ý rằng average và max pooling thường được thực hiện trên một cửa sổ ký tự có độ dài cố định trong các nghiên cứu trước đây (Tay et al., 2021), không thích ứng ở cấp từ như trong nghiên cứu ablation của chúng tôi.

4.5 Hiệu quả Tính toán
Trong phần này, chúng tôi benchmark hiệu quả tính toán của mô hình được đề xuất. Cụ thể, chúng tôi đo throughput suy luận (số mẫu được xử lý mỗi giây) trên tập kiểm tra của bộ dữ liệu MRPC, một tác vụ phụ của benchmark GLUE. Chúng tôi đánh giá các mô hình khác nhau trên cùng một máy chủ với một GPU NVIDIA Tesla V100. Kích thước batch được đặt là 32 và chúng tôi sử dụng độ chính xác đơn. Kết quả đánh giá được hiển thị trong Bảng 7. Trong khi BERT là hiệu quả nhất về mặt tính toán, HLM của chúng tôi cũng hoạt động cạnh tranh, khoảng cách hiệu suất nhỏ hơn so với các mô hình baseline cấp ký tự khác. Chúng tôi suy đoán rằng lợi ích hiệu suất này đến từ kiến trúc phân cấp của chúng tôi. Bằng cách tổng hợp các token ký tự thành các token cấp từ, độ dài chuỗi được giảm đáng kể cho module liên từ có stack Transformer sâu nhất. Chúng tôi cung cấp thêm phân tích về độ phức tạp tính toán trong Phụ lục A.1.

5 Kết luận
Trong công trình này, chúng tôi đề xuất một mô hình ngôn ngữ phân cấp mới cho hiểu biết ngôn ngữ từ vựng mở. Phương pháp của chúng tôi không dựa vào từ vựng từ phụ hoặc từ rõ ràng. Chúng tôi chứng minh rằng mô hình HLM của chúng tôi vượt trội hơn các phương pháp baseline trên các benchmark tiêu chuẩn, và làm nổi bật tính bền vững của nó đối với lỗi chính tả và chuyển đổi lĩnh vực. Trong công việc tương lai, chúng tôi sẽ mở rộng hỗ trợ ngôn ngữ và khám phá việc kết hợp một bộ giải mã cho các tác vụ sinh.

Hạn chế
Công việc này có hai hạn chế chính. Thứ nhất, chúng tôi chỉ xem xét các mô hình baseline với số lượng tham số tương tự, và được huấn luyện trước trên quy mô corpus văn bản tương tự để so sánh. Trong khi chúng tôi biết về các mô hình gần đây bao gồm T5 (Raffel et al., 2020) và PaLM (Chowdhery et al., 2022), chúng hoặc sử dụng corpus khổng lồ như C4 (745GB văn bản) để huấn luyện trước hoặc chứa nhiều tham số hơn đáng kể so với chúng tôi. Trong tương lai, chúng tôi sẽ cố gắng tìm thêm tài nguyên tính toán để mở rộng mô hình của chúng tôi và huấn luyện trước trên corpus văn bản lớn hơn. Thứ hai, chúng tôi tận dụng spaCy để phân đoạn câu thành từ, đây là phương pháp dựa trên quy tắc sử dụng khoảng trắng, dấu câu và các quy tắc khác. Phương pháp này hoạt động tốt trên tiếng Anh và nhiều ngôn ngữ phổ biến khác như tiếng Pháp, Đức và Tây Ban Nha. Nhưng đối với một số ngôn ngữ không sử dụng khoảng trắng để tách từ (ví dụ: tiếng Trung và tiếng Nhật), sẽ thách thức để truy xuất ranh giới từ. Để giải quyết vấn đề này, chúng tôi xem xét hoặc quay lại phân chia ký tự cho những ngôn ngữ này (tương tự như BERT đa ngôn ngữ) hoặc sử dụng một bộ phát hiện ranh giới từ tinh vi hơn trong công việc tương lai.

Lời cảm ơn
Công việc này được hỗ trợ một phần bởi NIH Award Number 1R01HL141813-01 và Pennsylvania Department of Health. Chúng tôi biết ơn về các tài nguyên tính toán được cung cấp bởi Pittsburgh Super Computing grant number TGASC170024.

Tài liệu tham khảo
[Tất cả các tài liệu tham khảo được dịch giữ nguyên format như bản gốc, chỉ dịch tiêu đề]

A Phụ lục

A.1 Phân tích về Độ phức tạp Tính toán
Cho N biểu thị độ dài ký tự của chuỗi đầu vào. Không mất tổng quát, chúng tôi giả định các từ trong chuỗi có cùng độ dài M. Module multi-head self-attention là thành phần chính của Transformer. Trong khi nó cung cấp trường tiếp nhận toàn cục, chi phí tính toán và dấu chân bộ nhớ tăng theo bậc hai với độ dài chuỗi đầu vào (Zeng et al., 2021). Do đó, đối với một Transformer dựa trên ký tự thuần túy với self-attention dày đặc, độ phức tạp tính toán và không gian là O(N²).

Đối với HLM được đề xuất của chúng tôi, chuỗi đầu vào vẫn ở cấp ký tự. Nhưng chúng tôi làm thưa self-attention dày đặc bằng cách giới thiệu kiến trúc phân cấp. Đối với module nội từ, mỗi token ký tự chỉ chú ý đến các ký tự từ cùng một từ. Vì có N/M từ trong chuỗi, độ phức tạp tính toán và không gian của module nội từ là:

O(N/M · M²) = O(NM)  (1)

Đối với module liên từ, vì nó chỉ hoạt động trên các token cấp từ, độ phức tạp tính toán và không gian là:

O(N²/M²)  (2)

Vì thường N ≫ M, và chúng tôi có module nội từ nông và module liên từ sâu hơn, Eq. 2 chi phối độ phức tạp tính toán và không gian của mô hình đầy đủ, thấp hơn đáng kể so với mô hình cấp ký tự thuần túy.

So với các mô hình dựa trên từ phụ như BERT, module liên từ của chúng tôi hoạt động trên chuỗi token cấp từ, luôn bằng hoặc ngắn hơn chuỗi token cấp từ phụ. Do đó, mặc dù mô hình của chúng tôi có thêm module nội từ, chúng tôi quan sát thực nghiệm trong Bảng 7 rằng HLM của chúng tôi cạnh tranh về hiệu quả tính toán so với các mô hình dựa trên từ phụ.

A.2 Đánh giá Sơ bộ của Mô hình Được Mở rộng
Trong phần này, chúng tôi mở rộng kích thước mô hình và benchmark hiệu suất. Để phù hợp với BERT Large, chúng tôi đặt số lớp trong bộ mã hóa Transformer liên từ là 24 và chiều feed-forward của bộ mã hóa Transformer được đặt là 2048 và 4096 cho module nội từ và liên từ tương ứng. Chúng tôi đặt số đầu attention là 16 và kích thước ẩn là 1024. Kích thước batch được đặt là 128. Các siêu tham số khác được đặt giống như HLM Base, được mô tả trong Mục 3. Do truy cập hạn chế vào tài nguyên tính toán, chúng tôi chỉ có thể huấn luyện trước mô hình trong 370k bước tại thời hạn camera-ready. So sánh, BERT Large được huấn luyện trước trong 1M bước với kích thước batch là 256. Do đó ngân sách tính toán của chúng tôi khoảng 1/6 của BERT. Chúng tôi benchmark hiệu suất mô hình của chúng tôi trên bộ dữ liệu SQuAD. Đánh giá trên tập xác thực được hiển thị trong Bảng 8. Chúng tôi sử dụng exact match (EM) và điểm F1 làm hai thước đo đánh giá. Mô hình của chúng tôi hoạt động cạnh tranh so với BERT Large, mặc dù HLM Large của chúng tôi có ngân sách tính toán ít hơn đáng kể để huấn luyện trước.

A.3 Chi tiết Thuật toán
Trong phần này, chúng tôi cung cấp chi tiết thuật toán cho việc tiền xử lý đầu vào và thuật toán mô hình của chúng tôi. Tiền xử lý của chúng tôi bao gồm các bước sau. Đầu tiên, chúng tôi chia mỗi câu thành một danh sách các từ. Tiếp theo, chúng tôi ánh xạ các ký tự thành các chỉ số codepoint sử dụng từ vựng cấp ký tự, và chèn token [WORD_CLS] ở đầu mỗi từ. Tiếp theo, chúng tôi chèn token [CLS] ở đầu, và token [SEP] ở cuối cho mỗi chuỗi. Sau đó chúng tôi cắt chuỗi token dựa trên cả giới hạn cấp ký tự (20 ký tự cho mỗi từ) và cấp từ (512 từ mỗi câu). Tiếp theo, chúng tôi tính số ký tự tối đa cho các từ trong batch, và pad tất cả từ đến độ dài này. Chúng tôi cũng xác định số từ tối đa trong batch chuỗi, và pad tất cả chuỗi đến độ dài này. Batch được tiền xử lý sau đó có thể được biểu diễn như một ma trận có hình dạng [batch_size, max_num_word, max_num_char].

Biểu diễn độc đáo của batch văn bản cho phép chúng tôi chuyển đổi hiệu quả giữa thực hiện self-attention nội từ và self-attention liên từ bằng cách đơn giản reshape, được hiển thị trong Thuật toán 1. Chúng tôi cung cấp mã giả cho việc huấn luyện trước HLM của chúng tôi trong Thuật toán 1. Để dễ đọc hơn, chúng tôi bỏ qua chi tiết triển khai bao gồm việc sử dụng attention mask tránh thực hiện attention trên các token [PAD] và xử lý cho các từ padding. Chúng tôi khuyến nghị padding ma trận đầu vào thành bội số của 8 để gia tốc tốt hơn trên GPU. Chúng tôi cũng phát hiện kết nối dư thừa giữa embedding từ ban đầu ri và embedding từ được ngữ cảnh hóa wi cải thiện hiệu suất trong một nghiên cứu tiếp theo.

A.4 Tính Bền vững với Nhiễu loạn Chưa nhìn thấy
Trong phần này, chúng tôi benchmark tính bền vững của mô hình đối với noise chưa nhìn thấy. Cụ thể, chúng tôi chỉ thêm noise vào tập đánh giá, trong khi sử dụng dữ liệu tinh chỉnh gốc. Chúng tôi thử nghiệm với ba loại nhiễu loạn như được giới thiệu trong Mục 4.2. Kết quả được hiển thị trong Bảng 9. Trong cả ba kịch bản, HLM được đề xuất của chúng tôi vượt trội hơn các phương pháp baseline, cho thấy tính bền vững tốt hơn.

--- TRANG 14 ---
Thuật toán 1 Mã giả cho HLM, kiểu PyTorch
# embeddings: bảng tra cứu embedding cấp ký tự
# intra_word_encoder: Bộ mã hóa Transformer nội từ
# inter_word_encoder: Bộ mã hóa Transformer liên từ
# intra_word_head: Đầu dự đoán nội từ
for input_ids, labels in loader: # tải một minibatch với n mẫu
    input_embeds = embeddings(input_ids)
    batch_size, num_word, num_char, hidden_size = input_embeds.shape
    # reshape để cho Transformer chú ý đến token nội từ thay vì toàn bộ chuỗi
    input_embeds = input_embeds.reshape((batch_size*num_word, num_char, hidden_size))
    initial_embeds = intra_word_encoder(input_embeds)
    # trích xuất embedding cho token [WORD_CLS], luôn ở đầu mỗi từ
    word_embeds = initial_embeds[:,0,:]
    # reshape và trích xuất biểu diễn liên từ được ngữ cảnh hóa
    word_embeds = word_embeds.reshape((batch_size, num_word, hidden_size))
    word_embeds = inter_word_encoder(word_embeds)
    word_embeds = word_embeds.reshape((batch_size*num_word, 1, hidden_size))
    # nối để khôi phục chuỗi token cấp ký tự
    char_embeds = concatenate([word_embeds, initial_embeds[:,1:,:]], axis=1)
    char_logits = intra_word_head(char_embeds)
    char_logits = char_logits.reshape((batch_size, num_word, num_char, -1))
    loss = CrossEntropyLoss(char_logits, labels) # loss mô hình hóa ký tự có mặt nạ
    loss.backward() # lan truyền ngược
    # cập nhật AdamW
    update(embeddings, intra_word_encoder, inter_word_encoder, intra_word_head)

[Bảng 9 được giữ nguyên format gốc với dữ liệu kết quả đánh giá]
