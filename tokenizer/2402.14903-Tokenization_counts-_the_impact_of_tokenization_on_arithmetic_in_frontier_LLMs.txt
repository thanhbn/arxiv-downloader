# 2402.14903.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/tokenizer/2402.14903.pdf
# File size: 680826 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Aaditya K. Singh1DJ Strouse2
Abstract
Tokenization, the division of input text into in-
put tokens, is an often overlooked aspect of the
large language model (LLM) pipeline and could
be the source of useful or harmful inductive biases.
Historically, LLMs have relied on byte pair encod-
ing, without care to specific input domains. With
the increased use of LLMs for reasoning, various
number-specific tokenization schemes have been
adopted, with popular models like LLaMa and
PaLM opting for single-digit tokenization while
GPT-3.5 and GPT-4 have separate tokens for each
1-, 2-, and 3-digit numbers. In this work, we
study the effect this choice has on numerical rea-
soning through the use of arithmetic tasks. We
consider left-to-right and right-to-left tokeniza-
tion for GPT-3.5 and -4, finding that right-to-left
tokenization (enforced by comma separating num-
bers at inference time) leads to largely improved
performance. Furthermore, we find that model er-
rors when using standard left-to-right tokenization
follow stereotyped error patterns, suggesting that
model computations are systematic rather than ap-
proximate. We show that the model is able to con-
vert between tokenizations easily, thus allowing
chain-of-thought-inspired approaches to recover
performance on left-to-right tokenized inputs. We
also find the gap between tokenization directions
decreases when models are scaled, possibly indi-
cating that larger models are better able to over-
ride this tokenization-dependent inductive bias.
In summary, our work performs the first study of
how number tokenization choices lead to differ-
ences in model performance on arithmetic tasks,
accompanied by a thorough analysis of error pat-
terns. We hope this work inspires practitioners to
more carefully ablate number tokenization-related
choices when working towards general models of
numerical reasoning.
1Gatsby Computational Neuroscience Unit, University College
London2Google DeepMind. Correspondence to: Aaditya Singh
<aaditya.singh.21@ucl.ac.uk >.
8302080
+ 3529456
118315368,302,080
+ 3,529,456
11,831,536
L2R Tokenization 
(standard)R2L Tokenization 
(enforced by commas)
GPT 3.5
Accuracy:75.6% 97.8%
GPT 4
Accuracy:84.4% 98.9%Figure 1. Illustrating the dependence of frontier model arithmetic
performance on tokenization. We show how using commas can
enforce right-to-left (R2L) tokenization for the same addition prob-
lem. R2L tokenization leads to improved model performance on
both GPT-3.5 and GPT-4 (March 2023 models), which we show
is due to tokenization alignment between addends and answer
through various controls and error analyses.
1. Introduction
Large language models (LLMs) are often lauded as demon-
strating the benefits of end-to-end learning over inductive
biases. However, an often overlooked part of the pipeline,
preventing it from being end-to-end, is tokenization: the seg-
menting of an input sequence of bytes into discrete tokens.
Tokenization consists of two halves: training, in which a
vocabulary of tokens and statistics are learned over a given
corpus, and segmenting, where a function uses the trained
vocabulary and statistics to map sequences of bytes to tokens.
Each tokenization scheme may impart different inductive
biases on the model due to the way in which bytes of in-
put sequences are grouped – in this work, we study these
tokenization-dependent effects on numerical reasoning in
state-of-the art models (GPT-3.5, GPT-4) by considering the
tokenization of numbers in arithmetic problems.
Though many techniques have been proposed for tokeniza-
tion, the prevailing methods in today’s frontier models are
variants of Byte Pair Encoding (BPE) (Gage, 1994; Sen-
nrich et al., 2016). BPE is a statistical approach to tokeniza-
tion that is learnt from a dataset of, in the case of LLMs,
text. Intuitively, BPE compresses the dataset by iteratively
creating tokens for the most commonly occurring subse-
quences. Specifically, BPE begins with a token vocabulary
consisting of each character in the text (e.g. letters, num-
1arXiv:2402.14903v1  [cs.CL]  22 Feb 2024

--- PAGE 2 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049
050051052053054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099
100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149
150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199
200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249
250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299
300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349
350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399
400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449
450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499
500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549
550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599
600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649
650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699
700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749
750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799
800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849
850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899
900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949
950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999
Figure 2. All 3-digit strings, colored red when the string does not have a corresponding single token in p50k base , the BPE tokenizer
for GPT-3. Though there’s some patterns (e.g., nearly all multiples of 10 are present), overall there’s no clear structure. The missing
tokens are an artifact of the specific process BPE tokenizers use to establish vocabularies.
bers, punctuation).1Using this vocabulary, bigram statistics
(i.e. frequencies of pairs) are calculated, and the most com-
mon bigram is added to the vocabulary and merged in the
dataset. This process is repeated up until a prespecified,
maximum vocabulary size is reached. After the tokenizer
is learned, tokenization of new text proceeds by iteratively
merging characters/tokens in the same order as learned on
the training dataset.
Naively applying BPE on internet-scale corpora leads to
very idiosyncratic number tokenization (Teknium, 2023). In
the training phase, which numbers receive dedicated tokens
is very adhoc – for example, 710may get a dedicated token,
while 711will not (Figure 2). In the segmenting phase,
these adhoc tokens will lead to different partitionings of
numbers of the same length. In Figure 3, we illustrate the
different partitionings of all 4 digit numbers when using the
p50k base tokenizer that was used to train GPT-3 (Brown
et al., 2020). To control for effects on downstream per-
1More precisely, single bytes are used to handle multilinguality,
but this level of description suffices for our needs.
0 0.25 0.50 0.75 1
fraction of 4-digit numberscl100k_basep50k_basepartition
[1234]
[123][4]
[12][34]
[1][234]
[1][23][4]
Figure 3. Comparison of how p50k base , the tokenizer for GPT-
3, and cl100k base , the tokenizer for GPT-3.5 and GPT-4, seg-
ments 4 digit strings into tokens. cl100k base standardized
number tokenization to chunks of 3 digits, left-to-right, resulting
in all N-digit numbers being segmented the same way.formance from such idiosyncratic tokenization, prior work
(Nye et al., 2021; Zhou et al., 2022) has used formatting,
such as spaces or commas, to separate individual digits,
ensuring each digit maps to a single token.
Newer models, and their corresponding tokenizers, indicate
that LLM practitioners across different labs have also tried
to control for idiosyncratic tokenization (Table 1).2The
PaLM (Chowdhery et al., 2023), LLaMa (Touvron et al.,
2023a) and Mistral3(Jiang et al., 2023) models switch to
single-digit tokenization, similar to that enforced by Nye
et al. (2021). Interestingly, GPT-3.5 and GPT-4’s tokenizer,
cl100k base , introduces tokens for all 1-, 2-, and 3-digit
strings.4Tokenization of numbers by these GPT models
defaults to breaking a long number into 3-digit chunks, left-
to-right, which we hypothesize (and later show) may create
issues for numerical reasoning.
These varied approaches to tokenization by today’s fron-
tier LLMs indicate a lack of convergence in the field on
best practices and call for a deeper analysis of (positive
or negative) inductive biases imparted by various tokeniza-
tion schemes. In this work, we provide the first systematic
comparison of model performance on the same numerical
reasoning tasks with varied tokenization. Specifically, we
consider the latest GPT models on few-shot arithmetic tasks.
2A noteable exception are Anthropic’s Claude models, which
still use pure BPE number tokens (see Appendix A).
3Verified by inspecting the tokens from huggingface
https://huggingface .co/docs/transformers/
main/en/model doc/mistral .
4This tokenization is enforced by the cryptic
patstr parameter in their tokenization library,
https://github .com/openai/tiktoken/blob/
main/tiktoken ext/openai public .pyLine 76.
2

--- PAGE 3 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Table 1. Popular LLMs and their number tokenization strategies.
BPE = byte pair encoding. L2R = left-to-right.
Model Strategy
GPT-3 (2020) pure BPE
GPT-3.5 (2022) L2R chunks of 3 digits
GPT-4 (2023) L2R chunks of 3 digits
Claude v2.1 (2023) pure BPE
Gopher (2021) pure BPE
Chinchilla (2022) pure BPE
PaLM (2022) single digit
GPT-J (2021) pure BPE
Llama 1 & 2 (2023) single digit
Mistral (2023) single digit
OLMo (2024) pure BPE
We vary the tokenization direction to be the default left-
to-right (L2R) or right-to-left (R2L). We find that model
accuracy is up to 20% higher when using R2L tokeniza-
tion (Figure 1, Section 3). We then provide a thorough
analysis of error patterns across these two tokenizations
(Section 4). We find that the difference in performance be-
tween R2L and L2R tokenization in GPT-3.5 can largely
be explained by an extremely stereotyped and surprising
error pattern (Section 4.3), perhaps indicating the presence
of some systematic, but flawed, reasoning. Next, we show
that chain-of-thought-inspired approaches, where a model
is asked to repeat an input in R2L tokenization, recover the
accuracy otherwise lost due to L2R tokenization (Section 5).
Finally, we conclude by studying how these effects may
change with model version, finding that larger models are
better able to override the tokenization-induced effects but,
as of yet, unable to eliminate them (Section 6). Overall,
we view these results as compelling evidence towards sig-
nificant tokenization-dependent inductive biases in large
language models, and hope they lead model practitioners
to conduct careful pre-training ablations with varying tok-
enization schemes, especially for numerical reasoning.
2. Methods
2.1. Experiment setup
We evaluate GPT models through the Chat Completions
endpoint on the OpenAI API5on few-shot addition prob-
lems. We control for addend digit length, ranging from 7
to 9 digits (chosen since this way each addend is 3 tokens
long). For most experiments, we use 90 random problems,
with 10 problems for each addend digit length pair (e.g.,
10 problems where the addends are both 7 digits long, 10
problems where the first addend is 7 digits and the second is
8, etc.). For shots, we consider 1-, 2-, 4-, and 8-shots. Shots
are sampled randomly for each “query” problem, and are
5https://platform .openai .com/provided to the model as a multi-turn dialogue. We control
shots to have the same form (digit lengths, tokenization
direction, etc.) as the query problem. We use the default
system prompt “You are a helpful assistant.” for maximum
reproducibility.6Python code for some example 2-shot
queries to the model are presented in Appendix E.3 for
maximum clarity. We use greedy decoding (temperature=0)
in all experiments. Accuracy was computed by extracting
numbers from model responses.
Most experiments in the paper are run using the
gpt-3.5-turbo-0301 model checkpoint, though in
Section 6 we look into how results extend to newer ver-
sions of the same model ( gpt-3.5-turbo-0613 ) and
to the, presumably larger, GPT-4 models ( gpt-4-0314,
gpt-4-0613 ). All code and full results tables can
be found at https://github .com/aadityasingh/
TokenizationCounts .
2.2. Varying L2R vs. R2L tokenization
The ChatCompletion API only allows for input text, not
input tokens, so it’s tricky to conduct tokenization-varying
experiments. To force the model to use R2L tokenization
for numbers, we add commas every 3-digits from the right
(see Figure 1). Since the tokenizer doesn’t contain any to-
kens with numbers and commas, the commas get tokenized
separately, effectively enforcing a different segmentation of
digits. We use this setting to illustrate our main results, and
conduct various controls to ensure that our observed effect
is due to tokenization as opposed to other confounds.
3. Right-to-left tokenization improves model
performance
3.1. Main results
When using commas to separate digits and enforce R2L
tokenization, we observed greatly improved average perfor-
mance (8-shot result in Figure 1). We found that increasing
the number of shots (Figure 4) led to a larger increase for
the L2R tokenization (from 68.5% 1-shot to 75.6% 8-shot)
than for the R2L tokenization (from 95.6% 1-shot to 97.8%
8-shot) indicating that in-context learning may slightly mit-
igate the (harmful) bias of L2R tokenization. Given this
finding and the plateau-ing in performance with increasing
shots, we report only 8-shot results for the remainder of the
work as this makes L2R tokenization the most competitive.
3.2. Controlling for comma-based semantic priors
Though this result is already compelling, we realize that
commas are often used to separate digits in the manner
6We did experiment with other system prompts and found
minimal differences (Appendix B).
3

--- PAGE 4 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
1 2 4 8
Number of shots0.60.81.0Accuracy
R2L T okenization
L2R T okenziation
Figure 4. Effect of R2L vs L2R tokenization with increasing shots.
depicted in Figure 1, so the observed effect may be con-
founded by prevalence in training data (McCoy et al., 2023).
One might argue that comma separation is actually bringing
the input closer to the training distribution of the model, so
it’s not a surprise that models perform better. To control
for this and focus in on tokenization, we consider alternate,
single-token separators: ’ ’, ’.’, ’$’, ’#’ (note
we’ll refer to ’ ’ as<space> for clarity). For example,
the number 8302080 would be written as 8#302#080
when input to the model.
Results are shown in Figure 5. We find that the model
is largely agnostic to the separator used, indicating that
tokenization is likely the dominant effect, rather than the
specific choice of using commas.
<space> . $ #
Delimiter used to enforce
R2L T okenization0.60.81.0AccuracyComma-based
R2L T okenization
L2R T okenization
Figure 5. 8-shot accuracy when using different delimiters for R2L
tokenization. Dotted lines show results from Figure 1 for compari-
son. Overall, we see choice of delimiter matters less than direction
of tokenization.
3.3. Controlling for “thinking tokens”
Another confound with the above experiment may be that
adding commas both increases the number of tokens input
to as well as generated by the model. Thus, to generate the
same answer, the model has access to more computation
steps (i.e., FLOPs). There is a worry that models may use
these repetitive thinking tokens to perform additional useful
computations (Lanham et al., 2023). In practice, this seems
not to happen without further training (Goyal et al., 2024),
but we conducted experiments to verify this in our setting.
, . 1 20.00.51.0AccuracyComma-based
R2L T okenization
L2R T okenization
Separator for
L2R T okenizationNumber of
spaces addedFigure 6. 8-shot accuracy for various “thinking token” controls.
Dotted lines show results from Figure 1 for comparison. We also
experimented with other delimiters for L2R tokenization (all those
from Figure 5), but found similarly poor results. Overall, “thinking
tokens” do not recover the performance boost from using comma-
enforced R2L tokenization.
To control for thinking tokens, we consider two types of
controls. In the first, we use separators to enforce L2R
tokenization – this enforces an exact match in prompt token
counts. Second, we consider adding 1 or 2 spaces before
and after the +and=sign to increase the number of tokens7
in the L2R case (where no separator is used). Both of these
have the benefit of adding extremely “predictable” tokens
(when using 8-shots), allowing the model to possibly use
the extra computation steps for “thinking”.
In Figure 6, we find that neither of these controls, when ap-
plied to L2R tokenized sequences, recovers the performance
of R2L tokenization. In fact, we found that using separators
with the L2R tokenization often hurt performance, likely
because this is an uncommon representation—upon qual-
itative inspection of a few examples, we found the model
sometimes “auto-corrects” the inputs by hallucinating trail-
ing zeros. We believe these experiments effectively rule out
the “thinking token” confound.
4. Error analysis reveals stereotyped patterns
Given the robust effect observed in Section 3, we were
curious to see if there were any patterns in the errors. Below,
we summarize our key findings.
4.1. L2R tokenization is significantly worse when
answer is longer than addends
As noted in Section 2.1, we balanced our dataset of problems
based on input digit length. Upon inspection of problems the
model got incorrect when using L2R tokenization, we noted
that errors seemed more likely when the answer was longer
than the addends (e.g., a problem where 7 digit number
+ 7 digit number = 8 digit number, of the form depicted
7Specifically, the token count in the 8-shot prompt increases
from 195 to 213 to 240 when going from 0 to 1 to 2 spaces. For
reference, the R2L token count is 247.
4

--- PAGE 5 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Length
matchLength
mismatch0.00.51.0AccuracyT okenization
direction
L2R
R2L
Figure 7. When the answer is the same length in digits as an addend
(length match), both tokenization schemes perform similarly (left).
When the answer is a different length in digits than either addend
(length mismatch), L2R tokenization destroys model performance,
dropping to 8.25% (right).
1 2 3 4 5 6 7
Number of carries in problem0.80.91.0Accuracy
R2L T okenization
L2R T okenziation
Figure 8. Accuracy as a function of number of carries. For L2R
tokenization, we exclude problems where the answer length does
not match at least one addend, as the model misses most of those
(92%) as shown in Section 4.1. If number of carries (a human
notion of difficulty) was correlated to model performance, we
would expect a negative slope. The lack of any trend suggests
model performance is largely independent of number of carries.
in Figure 1). To test this hypothesis, we conducted a new
experiment where we controlled for addend lengths and
answer lengths. Specifically, we generated 100 random
problems for each possible triplet of digit lengths where
addends and answer have a length of 7 to 9 digits (full list
in Appendix E.1). The remainder of our experiments in this
section will use this expanded set of problems to show the
robustness of the found error patterns.
We reproduced our main phenomenon (Section 3.1), and fur-
ther affirmed our intuitions about error patterns. As shown
in Figure 7, we find that L2R tokenization has similar per-
formance to R2L tokenization when the answer’s length in
digits is the same as one of the inputs (which we refer to as
the “length match” condition). When the answer is longer
than the inputs (due to a final carry), L2R tokenization is
significantly worse, with accuracy dropping down to 8.25% –
we refer to this as the “length mismatch” condition. We sus-
pect that this strong effect may be due to the misalignment
between input and output tokenizations (as illustrated inFigure 1) rather than some carry-related notion of problem
difficulty, which we explore in the next few subsections.
4.2.Errors do not seem correlated to number of carries
A natural hypothesis given the above result may be that
errors might just be correlated to some notion of difficulty,
such as carries. In Figure 8, we find that this is generally
not the case. Specifically, we consider the accuracy on
subsets of problems based on how many carries are needed
to solve them.8The lack of a clear positive or negative trend
indicates that model performance is not strongly affected by
the number of carries.
4.3.Length mismatch problems yield stereotyped “digit
4” error pattern
If not carries, what could be causing the surprising error
pattern in Figure 7? In Figure 9a, we find that the errors
when using L2R tokenization are extremely stereotyped and
not at all intuitive. Specifically, in the length mismatch
condition, the model always gets the fourth digit wrong.
Furthermore, the model always gets the first 3 digits correct
(corresponding to the first output token). In terms of how
far off the model is on digit 4, Figure 9b shows that there’s a
slight preference to off-by-one errors, but overall the specific
substitution appears quite haphazard.
We found this result extremely surprising. In cognitive
science, such stereotyped error patterns are often used as
evidence of underlying systematic processing. While the
mechanism for addition in LLMs remains unclear, we find
this striking, tokenization-dependent error pattern9as highly
suggestive of some underlying algorithm (in contrast to
suggestions that LLMs may be performing arithmetic using
some “fuzzy” matching to similar problems in training). We
provide further evidence of stereotyped error patterns by
analyzing model log probabilities in Appendix D.
4.4. Off-by-one errors at token boundaries account for
nearly all remaining errors
After accounting for the main source of error, we analyzed
the remaining errors across both tokenization methods: 25
out of the 1300 problems for R2L tokenization, and 56 out
of the 900 problems in the length match condition for L2R
tokenization.
For R2L tokenization, of the 25 problems missed, 24 are
due to off-by-one (either above or below) errors. For L2R
tokenization, of the 56 problems missed, 53 are due to off-
8Since we didn’t explicitly control for carries when generating
problems, the number of problems with a given number of car-
ries varies. We only considered cases where we had at least 50
problems.
9This error pattern is not present when using R2L tokenization.
5

--- PAGE 6 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
(a) Location of errors
123456789Other None
Error in digit _0.00.51.0Fraction of length
mismatch problems
(b) Magnitude of errors
1 2 3 4 5 6 7 8 9
Off by0.00.10.2Fraction of digit 4 errors
Figure 9. a)Error patterns for L2R tokenization over problems
where the answer digit length is different than the addend lengths.
“None” indicates the problems in this case that the model got
correct (8.25%). “Other” indicates problems where the model
doesn’t provide a valid answer or provides an answer of the wrong
length (0.5%). Of the remaining 91.25% which the model gets
incorrect, it shockingly always gets digit 4 wrong. In addition, it
sometimes gets other digits (5, 6 or 7) wrong. b)For the errors in
digit 4, we show the magnitude of the mistake. For example, if
the correct value of digit 4 is 2and the model response has digit
4 equal to 5, it would be off by 3. We see a slight preference to
off-by-1 errors, but error magnitudes are fairly evenly distributed.
by-one (either above or below) errors. For nearly all these
off-by-one errors,10regardless of tokenization direction, we
find that the error itself occurs in the last digit of an output
token . This result suggests that off-by-one errors are more
likely across token boundaries as opposed to in the middle of
a 3-digit token. This hypothesis, with preliminary evidence,
connects to works on length generalization (Anil et al., 2022)
– using 3-digit tokens may make length generalization easier
as models only need to cross token boundaries every third
digit (as opposed to every digit).
5.Models are able to convert from L2R to R2L
tokenization, improving performance
5.1. Main results
With the above results showing that number tokenization
can strongly affect numerical reasoning, we ask if mod-
10Specifically, all 24 off-by-one errors in the R2L case, and 51
of the 53 off-by-one errors in the L2R case.els can be prompted to take problems in a less preferred
tokenization (L2R) and convert them to a more preferred
tokenization (R2L) to improve performance. Inspired by
chain-of-thought approaches (Nye et al., 2021; Kojima et al.,
2022; Wei et al., 2022), we few-shot prompt models to take
problems with one tokenization direction, and then repeat
the problem and answer it using a different tokenization
direction. In Figure 10, we find that models indeed perform
nearly as well at addition when converting L2R tokenization
to R2L themselves as to when they receive the problem in
R2L tokenization in the first place. Performance increases
with the number of shots when converting L2R to R2L since
the model adheres more to the (helpful) suggested repeti-
tion style. These results indicate that models can convert
between tokenizations to solve problems correctly, but do
not do so implicitly in the forward pass.
12 4 8
Number of shots0.500.751.00Accuracy
Tokenization
direction
(input  repeat+answer)
R2L  R2L
L2R  R2L
L2R  L2R
R2L L2R
Figure 10. Few-shot accuracy when models receive a problem with
one tokenization direction, then repeat and answer it in another.
5.2. Controlling for output tokenization
One confound with the above experiment may just be that
the model improves when it’s asked to generate answers
with R2L tokenization. To control for this, we conduct a sim-
ilar experiment, but without few-shot prompting the model
to repeat the problem: the few-shot prompt provides answers
with a different tokenization direction than the input, incen-
tivizing the model to answer with this tokenization direction
(see Appendix E.3 for an example prompt). In Figure 11,
we see that just answering with R2L tokenization does not
improve performance (purple curve) to the degree that re-
peating in R2L tokenization does (purple curve, Figure 10),
when starting from L2R tokenization. This effect indicates
that it is important for the model to also seethe problem in
the preferred tokenization (by repeating it), rather than just
answering in the preferred tokenization.
6. Tokenization-dependent effects mostly
extend to future models
Through the previous sections, we’ve demonstrated a strong
tokenization-dependent effect. In this section, we address
the question: does this effect extend to newer models?
6

--- PAGE 7 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
12 4 8
Number of shots0.500.751.00Accuracy
Tokenization
direction
(input  answer)
R2L  R2L
L2R  R2L
L2R  L2R
R2L L2R
Figure 11. Few-shot accuracy when models receive a problem in
one tokenization format, and answer it in another. The distinction
between this and Figure 10 is that models do not repeat the problem
in this case. We note that when giving a model a problem in R2L
tokenization and prompting it to answer in L2R tokenization, the
model actually gets worse with more shots, since for fewer shots,
the model ends up ignoring the few-shot prompt and answers
in its preferred R2L tokenization. Specifically, adherence to the
prompted formatting for R2L →L2R increases from just 13.3%
with 1 shot to 98.9% with 8 shots.
GPT-3.5
Turbo
0301GPT-3.5
Turbo
0613GPT-3.5
Turbo
1106GPT-4
0314GPT-4
0613GPT-4
Turbo
11060.60.81.0AccuracyT okenization
direction
L2R
R2L
Figure 12. 8-shot performance of various OpenAI models on the
same addition problems as Figure 4. The newer version sof GPT-
3.5 appears to perform equally poorly. For GPT-4, we see a large
tokenization dependent effect in the March model, which becomes
weaker (but still present) in the June model. The GPT-4 turbo
model shows a slight regression in overall performance with the
tokenization-dependent effect becoming stronger again.
As shown in Figure 12, we find that generally, yes:
tokenization-dependent effects persist. We consider
five “held-out” OpenAI models, which allow us to
consider how tokenization-dependent effects shift
when models are updated ( gpt-3.5-turbo-0613 ,
gpt-3.5-turbo-1106 ) or scaled up ( gpt-4-0314 ,
gpt-4-0613 ) and then scaled back down
(gpt-4-1106-preview , which is a “turbo” model11).
Later versions of GPT-3.5 exhibit as strong an effect due to
tokenization direction. The effect is mitigated slightly in
GPT-4’s March version, and mitigated strongly in GPT-4’s
most recent version.12Specifically, GPT-4 models appear
11We assume this is a smaller, maybe distilled, version of GPT-4.
12We find it interesting that the March to June update to GPT-4
improved performance, but the corresponding update to GPT-3.5
did not – without knowing what these updates entail, however, it’s
hard to draw conclusions as to why this may be the case.
GPT-3.5
Turbo
0301GPT-3.5
Turbo
0613GPT-3.5
Turbo
1106GPT-4
0613GPT-4
Turbo
11060.00.51.0AccuracyLength
mismatch
True
FalseT okenization
direction
L2R
R2LFigure 13. 8-shot performance of various OpenAI models on an-
swer length controlled problems (see Section 4.1), separated by
whether the answer length is the same as one of the addends. We
see the effect from Section 4.1 reproduces strongly in the newer
version of GPT-3.5. The effect is still present in GPT-4,14but not
as strongly. Interestingly, the effect is stronger in the latest GPT-4
Turbo model as compared to GPT-4.
to be better at performing arithmetic across the board (for
both tokenization directions). Interestingly, in the most
recent GPT-4 Turbo model, the effect of tokenization
becomes stronger again. Furthermore, Figure 13 shows that
the digit length mismatch between answer and addends
is again the main reason for the performance drop when
using L2R tokenization, in both GPT-3.5 and GPT-4
models. We believe that the increased scale of training
GPT-4 (likely in both parameter count and data seen)
allows it to better override the tokenization-induced
inductive bias that leads GPT-3.5 models to perform worse
(analogous to scale helping mitigate tokenization-induced
spelling difficulties (Liu et al., 2023)). The resurgence of
tokenization-dependent effects in the newest GPT-4 Turbo
model (which is presumably smaller than GPT-4) supports
this hypothesis.
7. Related work
Tokenization methods The two leading tokenization meth-
ods are Unigram (Kudo, 2018) and BPE (Sennrich et al.,
2016). While older work in NLP show the benefits of Uni-
gram over BPE (Bostrom & Durrett, 2020), BPE remains the
most commonly used tokenization method by modern LLM
practitioners. Within BPE, different models often make dif-
ferent hard-coded choices, such as removing long tokens of
consecutive whitespace (Touvron et al., 2023a) or enforc-
ing single-digit tokenization of numbers (Chowdhery et al.,
2023). Our work demonstrates tokenization-dependent ef-
fects from one such choice, the use of 1-, 2-, and 3- digit
tokens by OpenAI models (OpenAI et al., 2023). One way
around such issues could be tokenizer-free methods (e.g.,
MEGABYTE (Yu et al., 2023), which uses patch-based
schemes and doesn’t assume fixed tokens), but we suspect
these schemes will also carry their own inductive biases.
Golkar et al. (2023) also introduce a continuous number en-
coding scheme meant to circumvent tokenization artifacts,
but their approach is limited to cases where model outputs
are purely numerical, and not interleaved with text.
7

--- PAGE 8 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Tokenization artifacts in LLMs A growing set of results
has emerged around various tokenization-related artifacts in
LLMs. Similar to scratchpad prompting (Nye et al., 2021),
Wei (2023) found that separating letters into individual to-
kens can help in sorting words by the second letter. Other
work (Shin et al., 2020) has focused on specific tokens
that can negatively affect model performance. Rumbelow
& mwatkins (2023) found many tokens which were arti-
facts of the data used to pre-train the tokenizer, but presum-
ably weren’t present in the model’s training data, leading to
highly unpredictable (and often comical) completions. Sun
et al. (2023) find artifacts due to mismatches in tokenization
in extractive Q&A tasks, which may have connection to
some of our experiments in Section 5.2. Lundberg (2023)
propose token healing to avoid many tokenization-related
issues by removing the last few tokens from a prompt and
allowing the model to complete them; this approach has con-
nections to work on asking models to rephrase-and-respond
(Deng et al., 2023) and our experiments on prompting the
model to repeat with its preferred tokenization direction
(Section 5). Our work builds on these past scattered artifacts
and provides a systematic analysis of tokenization-direction-
dependent effects on numerical reasoning in frontier LLMs.
Arithmetic tasks as a testbed for numerical reasoning
in LLMs With the increased interest in measuring fron-
tier models on math reasoning (Saxton et al., 2019; Cobbe
et al., 2021; Lewkowycz et al., 2022; Hendrycks et al., 2021;
Paster, 2023), an accompanying body of work studies lan-
guage models in more controlled settings, such as arithmetic.
Razeghi et al. (2022) use arithmetic tasks to show that pre-
training term frequencies15can affect numerical reasoning
in GPT-J models (Wang & Komatsuzaki, 2021) trained on
the Pile dataset (Gao et al., 2020). Similarly, McCoy et al.
(2023) showed that GPT-3.5 and GPT-4 are better at comput-
ing linear functions that are more common in training data
(such as the Fahrenheit to Celsius conversion) than close
alternatives. Other work (Nogueira et al., 2021; Muffo et al.,
2022; Zhou et al., 2022; 2023) instead focuses on various
modifications that can help models generalize to longer arith-
metic tasks. Zhou et al. (2023) and Lee et al. (2023) both
point out that having autoregressive models perform addi-
tion in reversed order yields a simpler algorithm to learn and
results in better performance, which is complementary to
our emphasis on the importance of “reversed” (i.e. right-to-
left) tokenization alignment. Zhou et al. (2022) also conduct
preliminary error analyses of model mistakes, though their
algorithmic prompts force models to split tokens into digits
(similar to Nye et al. (2021)). Our work broadly lies in this
15We also considered frequency effects by utilizing BPE merge
ranks (given that we do not have access to the pre-training data
of GPT-3.5 and GPT-4 models) as an approximate for frequency.
We didn’t find a strong effect, expanded results are provided in
Appendix C.category of work using arithmetic tasks to study numerical
reasoning; we chose to focus on tokenization-dependent
effects, and found surprisingly consistent, stereotyped error
patterns (Section 4), adding to this rich body of literature.
8. Discussion
In this work, we analyze tokenization-dependent effects
on numerical reasoning in GPT-3.5 and GPT-4. We found
that the hard-coded choice of 1-, 2- and 3- digit number to-
kens, tokenized left-to-right, gives rise to stereotyped error
patterns when compared to right-to-left tokenization. We
proposed a mitigation, where the model is asked to repeat
the answer in its preferred tokenization format. Finally, we
showed that the effect is stronger in smaller models (such
as GPT-3.5), emphasizing the significance of tokenization-
dependent inductive biases in an era where many practition-
ers are focusing on packing capabilities into smaller models
through overtraining (De Vries, 2023; Touvron et al., 2023a)
and distillation (Li et al., 2023; Gemini Team et al., 2023).
Overall, we believe this evidence strongly suggests inductive
biases from tokenization can significantly influence model
performance on numerical reasoning tasks.
Modern frontier LLMs mostly use single-digit tokens (Table
1), with GPT-3.5 and GPT-4 being a key exception in their
use of up-to-3-digit tokens. We hypothesize that the latter
choice may have been made to achieve a better compression
rate: models “see” more numerical data for the same number
of training tokens.16Furthermore, this choice could have
benefits for length generalization (Anil et al., 2022), as we
allude to in Section 4.4. However, we’ve also demonstrated
how the misalignment between inputs and outputs when
using L2R tokenization (Section 4.1) can lead to large drops
in accuracy, especially on smaller models (GPT-3.5, GPT-4
Turbo). Such misalignment would not be an issue when
using single-digit tokens.
To make progress on which number tokenization choices are
best to use (e.g., the single-digit tokens of LLaMa and PaLM,
or the up-to-3 digit tokens of GPT-3.5 and GPT-4), the “gold
experiment” would be to train the same model architecture
on the same dataset, but with varying number tokenization
strategies. Beyond the expense of this experiment (making
it intractable in academic settings), a key question also be-
comes how to “compute”-control. The better compression
ratio of up-to-3 digit tokens means a token-controlled exper-
iment would result in some models “seeing” more data. We
hope our work leads model practitioners to consider such
ablations, with proper controls.
Beyond applicability to model practitioners, our work also
163-digit number tokens also reduce inference-time compute
when models use numbers in their output, which could be an
important consideration when serving models at scale.
8

--- PAGE 9 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
provides an interesting set of tokenizaiton-dependent phe-
nomenon for interpretability researchers to explore. Prior
work (Stolfo et al., 2023) has used techniques such as path
patching to identify sub-circuits in LLMs that perform arith-
metic tasks, but restricted to single token operands. Building
off our results, it would be interesting to elucidate the mech-
anisms behind systematic error patterns, especially in the
case of multi-token operands. The robustness of the “digit
4” error on GPT-3.5 points to some systematic mechanism,
which could shed light on underlying algorithms that emerge
to perform arithmetic tasks.
Acknowledgements
The authors would like to acknowledge Andrew Saxe, Ted
Moskovitz, Kira D ¨usterwald, Felix Hill, Xavier Garcia, Dan
Roberts, and William Held for insightful discussions and
feedback on the draft. A.K.S. is funded by the Gatsby
Charitable Foundation.
References
Anil, C., Wu, Y ., Andreassen, A. J., Lewkowycz, A., Misra,
V ., Ramasesh, V . V ., Slone, A., Gur-Ari, G., Dyer, E., and
Neyshabur, B. Exploring length generalization in large
language models. Neural Information Processing Systems
(NeurIPS) , 2022. URL https://openreview .net/
forum?id=zSkYVeX7bC4 .
Bostrom, K. and Durrett, G. Byte pair encoding is
suboptimal for language model pretraining. Empirical
Methods in Natural Language Processing (EMNLP) ,
2020. URL https://aclanthology .org/
2020 .findings-emnlp .414.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger,
G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,
Litwin, M., Gray, S., Chess, B., Clark, J., Berner,
C., McCandlish, S., Radford, A., Sutskever, I., and
Amodei, D. Language models are few-shot learners.
Neural Information Processing Systems (NeurIPS) , 2020.
URL https://proceedings .neurips .cc/
paper files/paper/2020/file/
1457c0d6bfcb4967418bfb8ac142f64a-
Paper .pdf.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,
N., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,
S., Michalewski, H., Garcia, X., Misra, V ., Robinson,
K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,
H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,
Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S.,
Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polo-
zov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz,
M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K.,
Eck, D., Dean, J., Petrov, S., and Fiedel, N. PaLM:
Scaling language modeling with pathways. Journal
of Machine Learning Research , 2023. URL http:
//jmlr .org/papers/v24/22-1144 .html .
Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun,
H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J.,
Nakano, R., et al. Training verifiers to solve math
word problems. arXiv:2110.14168 , 2021. URL https:
//arxiv .org/abs/2110 .14168 .
De Vries, H. Go smol or go home, 2023. URL
https://www .harmdevries .com/post/
model-size-vs-compute-overhead/ .
Deng, Y ., Zhang, W., Chen, Z., and Gu, Q. Rephrase and
respond: Let large language models ask better questions
for themselves. arXiv:2311.04205 , 2023. URL https:
//arxiv .org/abs/2311 .04205 .
Gage, P. A new algorithm for data compression. C Users
Journal , 1994.
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
et al. The Pile: An 800gb dataset of diverse text for
language modeling. arXiv:2101.00027 , 2020. URL
https://arxiv .org/abs/2101 .00027 .
Gemini Team, Anil, R., Borgeaud, S., Wu, Y ., Alayrac,
J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,
Hauth, A., Millican, K., Silver, D., Petrov, S., Johnson,
M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J.,
Pitler, E., Lillicrap, T., Lazaridou, A., Firat, O., Molloy, J.,
Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F.,
Reynolds, M., Xu, Y ., Doherty, R., Collins, E., Meyer, C.,
Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Tucker,
G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Dani-
helka, I., Roelofs, B., White, A., Andreassen, A., von
Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khal-
man, M., Sygnowski, J., Frechette, A., Smith, C., Culp,
L., Proleev, L., Luan, Y ., Chen, X., Lottes, J., Schucher,
N., Lebron, F., Rrustemi, A., Clay, N., Crone, P., Kocisky,
T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A.,
Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F.,
Garrette, D., Barnes, M., Thakoor, S., Austin, J., Barth-
Maron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha,
D., Ahuja, A., Liu, R., Li, Y ., Cogan, S., Chen, J., Jia, C.,
9

--- PAGE 10 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Chad-
wick, M., Tomar, G. S., Garcia, X., Senter, E., Taropa,
E., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas,
D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter,
D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S.,
Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto,
E., Gu, Y ., Olszewska, K., Zhang, Y ., Addanki, R., Miech,
A., Louis, A., Shafey, L. E., Teplyashin, D., Brown, G.,
Catt, E., Attaluri, N., Balaguer, J., Xiang, J., Wang, P.,
Ashwood, Z., Briukhov, A., Webson, A., Ganapathy, S.,
Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A.,
Djolonga, J., Sun, Y ., Bapna, A., Aitchison, M., Pejman,
P., Michalewski, H., Yu, T., Wang, C., Love, J., Ahn,
J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T.,
Bradbury, J., Godbole, V ., Samangooei, S., Damoc, B.,
Kaskasoli, A., Arnold, S. M. R., Vasudevan, V ., Agrawal,
S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S.,
Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S.,
Garg, A., Paine, T. L., Li, J., Li, Y ., Giang, M., Neitz,
A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery,
A., Das, D., Rogozi ´nska, D., Nikolaev, V ., Sprechmann,
P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M.,
Mishra, G., Welty, C., Newlan, J., Jia, D., Allamanis,
M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim,
C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi,
A., Goldin, A., Ozturel, A., Cassirer, A., Xu, Y ., Sohn,
D., Sachan, D., Amplayo, R. K., Swanson, C., Petrova,
D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel,
M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M.,
Gim´enez, M., Yeung, L., Lin, H., Keeling, J., Georgiev,
P., Mincu, D., Wu, B., Haykal, S., Saputro, R., V odra-
halli, K., Qin, J., Cankara, Z., Sharma, A., Fernando,
N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A.,
Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang,
T., Yang, F., yiin Chang, S., Komarek, P., McIlroy, R.,
Luˇci´c, M., Zhang, G., Farhan, W., Sharman, M., Natsev,
P., Michel, P., Cheng, Y ., Bansal, Y ., Qiao, S., Cao, K.,
Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K.,
Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung,
T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S.,
Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz,
M., Robinson, K., Katariya, Y ., Riedel, S., Bailey, P.,
Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N.,
Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth,
M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers,
S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Pow-
ell, R., Bolina, V ., Iinuma, M., Zablotskaia, P., Besley, J.,
Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer,
J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S.,
Hu, H., Buchatskaya, E., Miao, Y ., Elhawaty, M., Sid-
dhant, A., Tomasev, N., Xing, J., Greer, C., Miller, H.,
Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta,
M., Blevins, R., Klimenko, T., Yeh, C.-K., Changpinyo,
S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V .,Lan, C. L., Haridasan, K., Marathe, A., Hansen, S., Dou-
glas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang,
J., Chiu, J., Lorenzo, J. A., Sj ¨osund, L. L., Cevey, S., Gle-
icher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V .,
May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli,
K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A.,
Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh,
V ., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer,
E., Campos, V ., Tomala, A., Tang, Y ., Badawy, D. E.,
White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S.,
Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng,
F., Stokowiec, W., Zheng, C., Thacker, P., C ¸a˘glar ¨Unl¨u,
Zhang, Z., Saleh, M., Svensson, J., Bileschi, M., Patil,
P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M.,
Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki,
S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg,
K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V .,
Cobon-Kerr, J., Sainath, T., Rauh, M., Hashemi, S. H.,
Ives, R., Hasson, Y ., Li, Y ., Noland, E., Cao, Y ., Byrd, N.,
Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau,
J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van
Amersfoort, J., Mandhane, A., Joshi, P., Goyal, A., Tung,
M., Brock, A., Sheahan, H., Misra, V ., Li, C., Raki ´cevi´c,
N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S.,
Sezener, E., Huot, F., Lamm, M., Cao, N. D., Chen, C.,
Elsayed, G., Chi, E., Mahdieh, M., Tenney, I., Hua, N.,
Petrychenko, I., Kane, P., Scandinaro, D., Jain, R., Uesato,
J., Datta, R., Sadovsky, A., Bunyan, O., Rabiej, D., Wu,
S., Zhang, J., Vasudevan, G., Leurent, E., Alnahlawi, M.,
Georgescu, I., Wei, N., Zheng, I., Chan, B., Rabinovitch,
P. G., Stanczyk, P., Zhang, Y ., Steiner, D., Naskar, S.,
Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias,
J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A.,
Vieillard, N., Potluri, S., Park, J., Davoodi, E., Zhang, J.,
Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee,
J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Chen, Z., Jia,
J., Levskaya, A., Zhu, Z., Gorgolewski, C., Grabowski, P.,
Mao, Y ., Magni, A., Yao, K., Snaider, J., Casagrande, N.,
Suganthan, P., Palmer, E., Irving, G., Loper, E., Faruqui,
M., Arkatkar, I., Chen, N., Shafran, I., Fink, M., Casta ˜no,
A., Giannoumis, I., Kim, W., Rybi ´nski, M., Sreevatsa, A.,
Prendki, J., Soergel, D., Goedeckemeyer, A., Gierke, W.,
Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y .,
Vashisht, H., Kulizhskaya, Y ., Hoover, J., Le, M., Li, L.,
Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A., Cui,
A., LIN, T., Georgiev, M., Wu, M., Aguilar, R., Pallo, K.,
Chakladar, A., Repina, A., Wu, X., van der Weide, T.,
Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O.,
Yang, F., Piper, J., Ie, N., Lui, M., Pasumarthi, R., Lintz,
N., Vijayakumar, A., Thiet, L. N., Andor, D., Valenzuela,
P., Paduraru, C., Peng, D., Lee, K., Zhang, S., Greene,
S., Nguyen, D. D., Kurylowicz, P., Velury, S., Krause,
S., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z.,
Zhang, B., Singhal, A., Latkar, T., Zhang, M., Le, Q.,
10

--- PAGE 11 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Abellan, E. A., Du, D., McKinnon, D., Antropova, N.,
Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D., Raad,
M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C.,
Lall, S., Franko, K., Filonov, E., Bulanova, A., Leblond,
R., Yadav, V ., Chung, S., Askham, H., Cobo, L. C., Xu,
K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.-C.,
Evans, C., Zhou, H., Dimitriev, A., Forbes, H., Banarse,
D., Tung, Z., Liu, J., Omernick, M., Bishop, C., Kumar,
C., Sterneck, R., Foley, R., Jain, R., Mishra, S., Xia, J.,
Bos, T., Cideron, G., Amid, E., Piccinno, F., Wang, X.,
Banzal, P., Gurita, P., Noga, H., Shah, P., Mankowitz,
D. J., Polozov, A., Kushman, N., Krakovna, V ., Brown,
S., Bateni, M., Duan, D., Firoiu, V ., Thotakuri, M., Natan,
T., Mohananey, A., Geist, M., Mudgal, S., Girgin, S., Li,
H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J.,
Yew, C., Yuan, Q., Bagri, S., Sinopalnikov, D., Ramos,
S., Mellor, J., Sharma, A., Severyn, A., Lai, J., Wu, K.,
Cheng, H.-T., Miller, D., Sonnerat, N., Vnukov, D., Greig,
R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J., Ko-
rchemniy, A., Tsai, T., Jasarevic, M., Kong, W., Dao, P.,
Zheng, Z., Liu, F., Yang, F., Zhu, R., Geller, M., Teh,
T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Sozan-
schi, A., Toyama, D., Rosen, E., Tavakkol, S., Xue, L.,
Elkind, C., Woodman, O., Carpenter, J., Papamakarios,
G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Tal-
bert, A., Goyal, A., Wu, D., Owusu-Afriyie, D., Du, C.,
Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi,
S., Wieting, J., Ajmeri, O., Uria, B., Zhu, T., Ko, Y .,
Knight, L., H ´eliou, A., Niu, N., Gu, S., Pang, C., Tran, D.,
Li, Y ., Levine, N., Stolovich, A., Kalb, N., Santamaria-
Fernandez, R., Goenka, S., Yustalim, W., Strudel, R.,
Elqursh, A., Lakshminarayanan, B., Deck, C., Upadhyay,
S., Lee, H., Dusenberry, M., Li, Z., Wang, X., Levin,
K., Hoffmann, R., Holtmann-Rice, D., Bachem, O., Yue,
S., Arora, S., Malmi, E., Mirylenka, D., Tan, Q., Koh,
C., Yeganeh, S. H., P ˜oder, S., Zheng, S., Pongetti, F.,
Tariq, M., Sun, Y ., Ionita, L., Seyedhosseini, M., Tafti,
P., Kotikalapudi, R., Liu, Z., Gulati, A., Liu, J., Ye, X.,
Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B.,
Singh, S., Fan, W., Parisi, A., Stanton, J., Kuang, C.,
Koverkathu, V ., Choquette-Choo, C. A., Li, Y ., Lu, T.,
Ittycheriah, A., Shroff, P., Sun, P., Varadarajan, M., Ba-
hargam, S., Willoughby, R., Gaddy, D., Dasgupta, I.,
Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Al-
brecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaf-
farkhah, A., Rivi `ere, M., Walton, A., Crepy, C., Parrish,
A., Liu, Y ., Zhou, Z., Farabet, C., Radebaugh, C., Srini-
vasan, P., van der Salm, C., Fidjeland, A., Scellato, S.,
Latorre-Chimoto, E., Klimczak-Pluci ´nska, H., Bridson,
D., de Cesare, D., Hudson, T., Mendolicchio, P., Walker,
L., Morris, A., Penchev, I., Mauger, M., Guseynov, A.,
Reid, A., Odoom, S., Loher, L., Cotruta, V ., Yenugula, M.,
Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yad-
lowsky, S., Shen, A., Globerson, A., Kurzrok, A., Webb,L., Dua, S., Li, D., Lahoti, P., Bhupatiraju, S., Hurt, D.,
Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A.,
Belle, S. R., Wang, L., Tekur, C., Kale, M. S., Wei, J.,
Sang, R., Saeta, B., Liechty, T., Sun, Y ., Zhao, Y ., Lee, S.,
Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas,
N., Wicke, M., Ma, X., Bilal, T., Eltyshev, E., Balle, D.,
Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y .,
Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras,
D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge,
J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R.,
Gelman, J., Xu, Y ., Polovets, G., Liu, J., Cai, H., Chen,
W., Sheng, X., Xue, E., Ozair, S., Yu, A., Angermueller,
C., Li, X., Wang, W., Wiesinger, J., Koukoumidis, E.,
Tian, Y ., Iyer, A., Gurumurthy, M., Goldenson, M., Shah,
P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fer-
nando, C., Brooks, K., Durden, K., Mehta, H., Momchev,
N., Rahimtoroghi, E., Georgaki, M., Raul, A., Ruder,
S., Redshaw, M., Lee, J., Jalan, K., Li, D., Perng, G.,
Hechtman, B., Schuh, P., Nasr, M., Chen, M., Milan, K.,
Mikulik, V ., Strohman, T., Franco, J., Green, T., Hassabis,
D., Kavukcuoglu, K., Dean, J., and Vinyals, O. Gemini:
A family of highly capable multimodal models, 2023.
URL https://arxiv .org/abs/2312 .11805 .
Golkar, S., Pettee, M., Eickenberg, M., Bietti, A., Cranmer,
M., Krawezik, G., Lanusse, F., McCabe, M., Ohana, R.,
Parker, L., Blancard, B. R.-S., Tesileanu, T., Cho, K., and
Ho, S. xval: A continuous number encoding for large
language models. Neural Information Processing Systems
(NeurIPS) AI for Science Workshop , 2023. URL https:
//openreview .net/forum?id=KHDMZtoF4i .
Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and
Nagarajan, V . Think before you speak: Training language
models with pause tokens. International Conference on
Learning Representations (ICLR) , 2024. URL https:
//openreview .net/forum?id=ph04CRkPdC .
Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,
R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,
Wang, Y ., Arora, S., Atkinson, D., Authur, R., Chandu,
K. R., Cohan, A., Dumas, J., Elazar, Y ., Gu, Y ., Hessel,
J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N.,
Naik, A., Nam, C., Peters, M. E., Pyatkin, V ., Ravichan-
der, A., Schwenk, D., Shah, S., Smith, W., Strubell, E.,
Subramani, N., Wortsman, M., Dasigi, P., Lambert, N.,
Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Sol-
daini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accel-
erating the science of language models, 2024.
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,
S., Tang, E., Song, D., and Steinhardt, J. Measuring math-
ematical problem solving with the MATH dataset. Neu-
ral Information Processing Systems (NeurIPS) Datasets
and Benchmarks Track , 2021. URL https://
openreview .net/forum?id=7Bywt2mQsCe .
11

--- PAGE 12 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel,
G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-
A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,
T., and Sayed, W. E. Mistral 7b. arXiv:2310.06825 , 2023.
URL https://arxiv .org/abs/2310 .06825 .
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and
Iwasawa, Y . Large language models are zero-shot
reasoners. Neural Information Processing Systems
(NeurIPS) , 2022. URL https://openreview .net/
forum?id=e2TBb5y0yFf .
Kudo, T. Subword regularization: Improving neural network
translation models with multiple subword candidates. As-
sociation for Computational Linguistics (ACL) , 2018.
URL https://aclanthology .org/P18-1007 .
Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Deni-
son, C., Hernandez, D., Li, D., Durmus, E., Hubinger,
E., Kernion, J., et al. Measuring faithfulness in chain-
of-thought reasoning. arXiv:2307.13702 , 2023. URL
https://arxiv .org/abs/2307 .13702 .
Lee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Pa-
pailiopoulos, D. Teaching arithmetic to small trans-
formers. arXiv:2307.03381 , 2023. URL https://
arxiv .org/abs/2307 .03381 .
Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E.,
Michalewski, H., Ramasesh, V . V ., Slone, A., Anil, C.,
Schlag, I., Gutman-Solo, T., Wu, Y ., Neyshabur, B., Gur-
Ari, G., and Misra, V . Solving quantitative reasoning
problems with language models. Neural Information
Processing Systems (NeurIPS) , 2022. URL https://
openreview .net/forum?id=IFXTZERXdM7 .
Li, Y ., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S.,
and Lee, Y . T. Textbooks Are All You Need II: phi-1.5
technical report. arXiv preprint arXiv:2309.05463 , 2023.
Liu, R., Garrette, D., Saharia, C., Chan, W., Roberts, A.,
Narang, S., Blok, I., Mical, R., Norouzi, M., and Con-
stant, N. Character-aware models improve visual text
rendering. Association for Computational Linguistics
(ACL) , 2023. URL https://aclanthology .org/
2023 .acl-long .900.
Lundberg, S. The art of prompt design: Prompt
boundaries and token healing, 2023. URL https:
//towardsdatascience .com/the-art-of-
prompt-design-prompt-boundaries-and-
token-healing-3b2448b0be38 .
McCoy, R. T., Yao, S., Friedman, D., Hardy, M., and
Griffiths, T. L. Embers of autoregression: Understand-
ing large language models through the problem theyare trained to solve. arXiv:2309.13638 , 2023. URL
https://arxiv .org/abs/2309 .13638 .
Muffo, M., Cocco, A., and Bertino, E. Evaluating trans-
former language models on arithmetic operations us-
ing number decomposition. Language Resources and
Evaluation Conference (LREC) , 2022. URL https:
//aclanthology .org/2022 .lrec-1 .30.
Nogueira, R., Jiang, Z., and Lin, J. Investigating
the limitations of transformers with simple arithmetic
tasks. arXiv:2102.13019 , 2021. URL https://
arxiv .org/abs/2102 .13019 .
Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski,
H., Austin, J., Bieber, D., Dohan, D., Lewkowycz,
A., Bosma, M., Luan, D., et al. Show your work:
Scratchpads for intermediate computation with language
models. arXiv:2112.00114 , 2021. URL https://
arxiv .org/abs/2112 .00114 .
OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,
Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J.,
Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Bal-
aji, S., Balcom, V ., Baltescu, P., Bao, H., Bavarian, M.,
Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G.,
Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman,
A.-L., Brockman, G., Brooks, T., Brundage, M., Button,
K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson,
C., Carmichael, R., Chan, B., Chang, C., Chantzis, F.,
Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess,
B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Cur-
rier, J., Dai, Y ., Decareaux, C., Degry, T., Deutsch, N.,
Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning,
S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus,
L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L.,
Georges, E., Gibson, C., Goel, V ., Gogineni, T., Goh, G.,
Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S.,
Greene, R., Gross, J., Gu, S. S., Guo, Y ., Hallacy, C., Han,
J., Harris, J., He, Y ., Heaton, M., Heidecke, J., Hesse,
C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B.,
Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S.,
Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S.,
Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A.,
Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L.,
Kim, J. W., Kim, C., Kim, Y ., Kirchner, H., Kiros, J.,
Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich,
A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V .,
Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D.,
Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T.,
Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning,
S., Markov, T., Markovski, Y ., Martin, B., Mayer, K.,
Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C.,
McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick,
J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V .,
Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O.,
12

--- PAGE 13 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
M´ely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan,
A., Ngo, R., Noh, H., Ouyang, L., O’Keefe, C., Pachocki,
J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo,
G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng,
A., Perelman, A., de Avila Belbute Peres, F., Petrov, M.,
de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M.,
Pong, V ., Powell, T., Power, A., Power, B., Proehl, E.,
Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C.,
Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H.,
Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry,
G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D.,
Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam,
P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K.,
Sohl, I., Sokolowsky, B., Song, Y ., Staudacher, N., Such,
F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N.,
Thompson, M., Tillet, P., Tootoonchian, A., Tseng, E.,
Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone,
A., Vijayvergiya, A., V oss, C., Wainwright, C., Wang,
J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann,
C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wi-
ethoff, M., Willner, D., Winter, C., Wolrich, S., Wong,
H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu,
T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R.,
Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J.,
Zhuk, W., and Zoph, B. GPT-4 Technical Report, 2023.
URL https://arxiv .org/abs/2303 .08774 .
Paster, K. Testing language models on a held-
out high school national finals exam. https:
//huggingface .co/datasets/keirp/
hungarian national hsfinals exam , 2023.
Razeghi, Y ., Logan IV , R. L., Gardner, M., and
Singh, S. Impact of pretraining term frequencies
on few-shot numerical reasoning. Empirical Meth-
ods in Natural Language Processing (EMNLP) ,
2022. URL https://aclanthology .org/
2022 .findings-emnlp .59.
Rumbelow, J. and mwatkins. Solidgoldmagikarp
(plus, prompt generation), 2023. URL
https://www .lesswrong .com/posts/
aPeJE8bSo6rAFoLqg/solidgoldmagikarp-
plus-prompt-generation .
Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. Analysing
mathematical reasoning abilities of neural models. In-
ternational Conference on Learning Representations
(ICLR) , 2019. URL https://openreview .net/
forum?id=H1gR5iR5FX .
Sennrich, R., Haddow, B., and Birch, A. Neural machine
translation of rare words with subword units. Associa-
tion for Computational Linguistics (ACL) , 2016. URL
https://aclanthology .org/P16-1162 .Shin, T., Razeghi, Y ., Logan IV , R. L., Wallace, E.,
and Singh, S. AutoPrompt: Eliciting Knowledge
from Language Models with Automatically Gener-
ated Prompts. Empirical Methods in Natural Lan-
guage Processing (EMNLP) , 2020. URL https://
aclanthology .org/2020 .emnlp-main .346.
Stolfo, A., Belinkov, Y ., and Sachan, M. A mech-
anistic interpretation of arithmetic reasoning in lan-
guage models using causal mediation analysis. Em-
pirical Methods in Natural Language Processing
(EMNLP) , 2023. URL https://openreview .net/
forum?id=aB3Hwh4UzP .
Sun, K., Qi, P., Zhang, Y ., Liu, L., Wang, W., and
Huang, Z. Tokenization consistency matters for
generative models on extractive NLP tasks. Empirical
Methods in Natural Language Processing (EMNLP) ,
2023. URL https://aclanthology .org/
2023 .findings-emnlp .887.
Teknium. How did the gpt tokenizer get created?,
2023. URL https://twitter .com/Teknium1/
status/1634667026739527680?s=20 .
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro,
E., Azhar, F., et al. Llama: Open and efficient foun-
dation language models. arXiv:2302.13971 , 2023a. URL
https://arxiv .org/abs/2302 .13971 .
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,
M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,
Fuller, B., Gao, C., Goswami, V ., Goyal, N., Hartshorn,
A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,
V ., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,
Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y .,
Mao, Y ., Martinet, X., Mihaylov, T., Mishra, P., Molybog,
I., Nie, Y ., Poulton, A., Reizenstein, J., Rungta, R., Saladi,
K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,
Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
Xu, P., Yan, Z., Zarov, I., Zhang, Y ., Fan, A., Kambadur,
M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,
and Scialom, T. Llama 2: Open foundation and fine-
tuned chat models. arXiv:2307.09288 , 2023b. URL
https://arxiv .org/abs/2307 .09288 .
Wang, B. and Komatsuzaki, A. GPT-J-6B: A 6 Bil-
lion Parameter Autoregressive Language Model, 2021.
URL https://huggingface .co/EleutherAI/
gpt-j-6b .
Wei, J. Sorting a list of words by the second let-
ter, 2023. URL https://x .com/ jasonwei/
status/1661781746759909376?s=20 .
13

--- PAGE 14 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian
ichter, Xia, F., Chi, E. H., Le, Q. V ., and Zhou, D.
Chain of thought prompting elicits reasoning in large lan-
guage models. Neural Information Processing Systems
(NeurIPS) , 2022. URL https://openreview .net/
forum?id= VjQlMeSB J.
Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer,
L., and Lewis, M. MEGABYTE: Predicting million-byte
sequences with multiscale transformers. Neural Informa-
tion Processing Systems (NeurIPS) , 2023. URL https:
//openreview .net/forum?id=JTmO2V9Xpz .
Zhou, H., Nova, A., Larochelle, H., Courville, A.,
Neyshabur, B., and Sedghi, H. Teaching algorithmic rea-
soning via in-context learning. arXiv:2211.09066 , 2022.
URL https://arxiv .org/abs/2211 .09066 .
Zhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O.,
Susskind, J., Bengio, S., and Nakkiran, P. What algo-
rithms can transformers learn? a study in length gen-
eralization. arXiv:2310.16028 , 2023. URL https:
//arxiv .org/abs/2310 .16028 .
14

--- PAGE 15 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
A. Tokenization differences between frontier LLMs
000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049
050051052053054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099
100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149
150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199
200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249
250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299
300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349
350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399
400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449
450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499
500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549
550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599
600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649
650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699
700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749
750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799
800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849
850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899
900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949
950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999
Figure 14. The equivalent of Figure 2 but for the Claude tokenizer. All 3-digit number strings, colored red when the string does not have a
corresponding single token dedicated to it. The lack of systematicity suggests that Claude tokenizes numbers using pure BPE. Note also,
however, that token coverage is generally higher than in Figure 2, likely in part because the Claude tokenizer has a larger vocabulary size
(65k tokens) than OpenAI’s p50k base (50k tokens).
0 0.25 0.50 0.75 1claude
fraction of 4-digit numberspartition
[1234]
[123][4]
[12][34]
[1][234]
Figure 15. The equivalent of Figure 3 but for the Claude tokenizer. Note that this distribution looks more like p50k base than
cl100k base in Figure 3. This, along with Figure 14 above shows that Claude’s tokenizer exhibits a lack of systemacity when
tokenizing numbers, suggesting the use of pure BPE number tokens, rather than something bespoke (as other current models use; see
Table 1).
B. Experiments with other system prompts
We also conducted our main experiment with an alternate, custom system prompt (as opposed to the default ’You are a
helpful assistant.’ ). The prompt we used was:
You are MathGPT, an expert at solving math problems. When given a math problem,
you respond only by repeating the problem statement and appending the answer. You
do not say any other words.
Results using this prompt are presented in Figure 16. We found it lead to small improvements in performance at low shot
numbers (e.g., 1-shot) but these diminished at 8-shots. To maximize the reproducibility and applicability of our results,
we decided to just use the default prompt. As we report 8-shot results throughout most of the paper, we doubt the system
prompt would have a large effect on our results, given Figure 16.
15

--- PAGE 16 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
1 2 4 8
Number of shots0.60.81.0Accuracy
custom prompt
default prompt
R2L T okenization
L2R T okenziation
Figure 16. Comparison of R2L and L2R tokenization strategies for different numbers of shots and using two different system prompts.
C. Frequency effects
Given the findings of prior work on numerical reasoning demonstrating frequency effects (Razeghi et al., 2022), we also
investigated whether or not our observed error patterns could be explained by term frequency. While we do not have access
to the pre-training data of GPT-3.5 and GPT-4 models, we use the tokenizer merge ranks17as a signal of term frequency. We
analyze the expanded set of problems used for the error analysis in Section 4. Our results are summarized below:
When making an error, GPT-3.5 is slightly more likely to output a more frequent token. For each token in the model
response on problems where it makes a mistake, we consider if the outputted incorrect token is more or less frequent (has
lower or higher merge rank) than the correct one. Of the 25 errors made by the model when using R2L tokenization, 15
involve substituting in a more frequent token (60%, p= 0.115using a binomial null distribution assuming chance is 50%).
Of the 425 errors made when using L2R tokenization, 238 involve substituting in a more frequent token (56%, p= 0.005
using a binomial null distribution assuming chance is 50%). While we do see a significant effect in the L2R tokenization
case, the margin is relatively small, which suggests that token frequency is not the dominant reason behind the error patterns.
When using L2R tokenization in the length mismatch case, GPT-3.5 errors do not show strong correlation to token
frequency. In Section 4.3, we found that GPT-3.5 always gets the fourth digit wrong (Figure 9a). We then found correlation
in the specific error in digit 4 to the magnitude difference between correct digit and digit in the model response (Figure 9b).
Here, we ask if the substituted token 2 (whose first digit would be digit 4 of the response) is correlated to frequency in
training data. Specifically, for each problem, we rank the tokens corresponding to the 10 possible “digit 4 mistakes” by
merge rank. In Figure 17, we show the distribution of ranks across all 365 problems where the model makes “digit 4” errors.
If models are preferentially substituting in more frequent tokens, we would expect to see a negative trend from the top left
to the bottom right (as we did in Figure 9b). In Figure 17, we see a slight preference for outputting the most likely token
(roughly 16% of the time, where chance would be 10%), but overall we see no clear trend.
Off-by-one errors do not seem to be correlated to answer token frequency. In Section 4.4, we found that the vast
majority of remaining errors (for R2L tokenization, and for L2R tokenization in the length match condition) are off-by-one
errors in the units digit of a token. Here, we ask if the specific substitution by the model is correlated to token frequency,
measured by merge rank. Specifically, we condition on the model possibly making an off-by-one error, which means there
are 3 possible output tokens (the correct token, the correct token minus one, the correct token plus one). We then rank
these tokens based on merge rank, and see if the model preferentially picks the token with lowest merge rank. Of the 24
off-by-one errors when using R2L tokenization, we find the model only picks the “most frequent” token 7 times. Of the 53
off-by-one errors when using L2R tokenization, we find the model only picks the “most frequent” token 17 times. Both of
these are essentially what we would expect by chance (one out of three), which suggests that output token frequency effects
are not a dominant factor in why the model makes off-by-one errors.
Overall, we find mild to no evidence of token frequency effects in our experiments. This could be due to the presumably
larger scale of GPT-3.5 (as compared to GPT-J, used by Razeghi et al. (2022)). However, we note that our method of
measuring token frequency is imperfect—relying on BPE merge ranks to signal frequency as we do not have access to
17Recall that tokens are created roughly in order of decreasing frequency in the corpus used to train the tokenizer.
16

--- PAGE 17 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
1 2 3 4 5 6 7 8 910
Rank of token 2 substitution (out of 10)0.00.10.2Fraction of digit 4 errors
Figure 17. Distribution of relative rank of substituted incorrect token 2 in model response when using L2R tokenization in the length
mismatch condition.
pre-training data. Future work could study such associations further in newer, larger models with open pretraining data
(Groeneveld et al., 2024).
D. Stereotyped patterns in model log probabilities
Mirroring the results of Section 4, we found stereotyped patterns in model log probabilities (“logprob”). Specifically, the
OpenAI API returns the top 5 tokens at each position with their corresponding logprobs. We analyzed these log probabilities
in three cases: L2R tokenization on length mismatch problems, L2R tokenization on length match problems, and R2L
tokenization on all problems. These conditions mirror the most salient error effects we found in Section 4, with the former
leading to “digit 4” errors, and the latter two leading to mostly off-by-one errors.
In addition to the raw logprobs, we computed an additional entropy metric (per output token) to measure model uncertainty
in its output. Since access is restricted to the top 5 logprobs, we use the following lower bound, Hlower, to the true entropy:
Htrue≡ −VX
i=1pilog (pi)
=− 5X
i=1pilog (pi) +VX
i=6pilog (pi)!
≥ − 5X
i=1pilog (pi) +VX
i=6pilog (p5)!
=− 5X
i=1pilog (pi) + 
1−5X
i=1pi!
·log (p5)!
≡Hlower,
where pidenotes the probability of the i-th most likely token. We use the natural logarithm for entropy, so all entropies are
in nats (not bits).
For the “digit 4” error pattern (Section 4.3), we find an interesting trend in model entropy. The entropy both on problems
it gets incorrect (91.25%) and correct (8.25%) is roughly the same (2.066 and 2.061 respectively). Even when the model
gets the question right, it’s unsure of its answer, suggesting that it might just be guessing a second output token with the
right tens and ones digit and random hundreds digit. Providing further evidence for this mechanism, we observe that, of the
problems where the model makes an error, about half (49.6%) of the time the correct answer appears in the top 5 output
tokens. This is in line with what we would see for random guessing from the 10 tokens. That said, the model may exhibit
some degree of bias towards the correct output, as evidenced by the downward trend in Figure 9b.
17

--- PAGE 18 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
0.4
 0.2
 0.0 0.2 0.4 0.6
Difference in logprob between R2L and L2R tokenization
on "length match" problems where both are correct0.00.250.50.75Fraction of problems
Figure 18. Histogram of difference in answer log probabilities (summed over tokens) on length match problems that the model answers
correctly using both R2L and L2R tokenization. Black dotted line signifies 0. Red dotted line shows the average difference—on average,
the model is more “confident” when using R2L tokenization.
For the off-by-one error patterns (Section 4.4), we observe a qualitatively different trend. Specifically, of the 53 off-by-one
errors when using L2R tokenization on length match problems, in all cases we find that the second most likely token is the
correct answer. We observe the same effect on the 25 off-by-one errors when using R2L tokenization. Furthermore, the
entropy in both cases is around 0.45±0.05, indicating that the model puts most of its weight on these top 2 most likely
tokens. Unlike in the “digit 4” case, model entropy on correct problems is significantly lower (approximately 0.03, averaged
across dataset and tokens) indicating that the model is “confidently correct” when using L2R tokenization on length match
problems or R2L tokenization on all problems. Interestingly, on the subset of length match problems that the model answers
correctly in both L2R and R2L tokenization, we found the model is slightly more confident when using R2L tokenization
(which aligns with our intuition, as the model is also more often correct when using R2L tokenization)—see Figure 18.
These results demonstrate that, depending on tokenization direction and alignment between input and output tokenization,
we observe stereotyped patterns in model log probabilities. When using L2R tokenization on length mismatch problems, the
model appears to make a magnitude-biased guess between all possible fourth digits (corresponding to 10 possible tokens18).
In the other cases, the model is mostly confidently correct. When it does make an error, it’s almost always an off-by-one
error (Section 4.4) where it’s uncertain between its chosen off-by-one incorrect answer and the true answer, but does not
really consider other outputs beyond these two.19
E. Additional experimental details
All code and raw results can be found at https://github .com/aadityasingh/TokenizationCounts .
E.1. Length control for error analysis
As described in Section 4.1, after noticing errors mostly come from the length mismatch condition in our original experiments
(which used 90 problems, balanced by input digit length), we conducted a larger experiment where we controlled for input
and output digit lengths. Specifically, we considered the following (addend1 length, addend2 length, answer length) triplets:
(7,7,7), (7,7,8), (8,7,8), (7,8,8), (8,7,9), (7,8,9), (8,8,8), (8,8,9), (9,7,9), (7,9,9), (9,8,9), (8,9,9), (9,9,9). Problems in each
condition were sampled randomly so as to satisfy the digit length constraints for each triplet. We sampled 100 problems for
each triplet, for a total of 1300 problems.
E.2. Access dates
Given the changing nature of the OpenAI API, we report access dates for all experiments below. We tried to use the
supposed “fixed” models for all experiments, but did notice some non-determinism, even at temperature 0—an issue
that may be due to non-determinism in floating point arithmetic. We also note that the gpt-4-0314 appears to have
18A completely random guess over 10 tokens would correspond to an entropy of about 2.3, which is in line with the lower bound
observed (of about 2.06) and the finding of a slight mangitude bias (which would decrease the entropy from 2.3).
19A completely random guess over 2 tokens would correspond to an entropy of about 0.69, which is in line with the lower bound
observed (of about 0.45).
18

--- PAGE 19 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
been early-deprecated, as we can no longer access it despite the supposed June 13, 2024 deprecation date on https:
//platform .openai .com/docs/deprecations .
Access dates by figure in main text:
•gpt-3.5-turbo-0301 , Figure 4: April 7, 2023
•gpt-3.5-turbo-0301 , Figure 5: May 18, 2023
•gpt-3.5-turbo-0301 , Figure 6 left two columns: May 18, 2023
•gpt-3.5-turbo-0301 , Figure 6 right two columns: April 7, 2023
•gpt-3.5-turbo-0301 , Figure 7-8: January 25, 2024
•gpt-3.5-turbo-0301 , Figure 10-11 May 24, 2024
•gpt-4-0314 , Figure 12: May 2, 2023
•gpt-3.5-turbo-0613 , Figure 12-13: January 25, 2024
•gpt-3.5-turbo-1106 , Figure 12-13: January 29, 2024
•gpt-4-0613 , Figure 12-13: January 25, 2024
•gpt-4-1106-preview , Figure 12-13: January 29, 2024
E.3. Example prompts
In this section, we provide example prompts we used for various experiments. For simplicity, we use the same query
for each prompt shown below, and we only use 2 shots (most experiments in the main text are done with 8 shots).
In practice, we sampled shots randomly (controlling for digit length to match the query length) for each query, as
explained in Section 2.1. For the experiments described in Section 4 and Appendix E.1, the shots were also controlled
to have the same answer length as the query. The examples we present below, though, are for the runs in the rest
of the paper (where only input digit lengths are controlled). For maximum clarity, we display prompts as the list of
dictionaries that gets sent to OpenAI’s API and roughly in the order used for figures in the paper. Following the advice at
https://platform.openai.com/docs/guides/prompt-engineering/tactic-provide-examples ,
we make use of the multi-turn chat dialog to prompt the model, as opposed to one big user message with all the examples.
L2R tokenization, input-digit-controlled for two 7-digit numbers:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3790206+6739555=’},
{’role’: ’assistant’, ’content’: ’10529761’},
{’role’: ’user’, ’content’: ’6777159+7096168=’},
{’role’: ’assistant’, ’content’: ’13873327’},
{’role’: ’user’, ’content’: ’8302080+3529456=’}]
R2L tokenization, input-digit-controlled for two 7-digit numbers:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3,790,206+6,739,555=’},
{’role’: ’assistant’, ’content’: ’10,529,761’},
{’role’: ’user’, ’content’: ’6,777,159+7,096,168=’},
{’role’: ’assistant’, ’content’: ’13,873,327’},
{’role’: ’user’, ’content’: ’8,302,080+3,529,456=’}]
19

--- PAGE 20 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
R2L tokenization, delimiter-control condition using ’#’:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3#790#206+6#739#555=’},
{’role’: ’assistant’, ’content’: ’10#529#761’},
{’role’: ’user’, ’content’: ’6#777#159+7#096#168=’},
{’role’: ’assistant’, ’content’: ’13#873#327’},
{’role’: ’user’, ’content’: ’8#302#080+3#529#456=’}]
L2R tokenization, thinking token control by using separators in L2R tokenization:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’379,020,6+673,955,5=’},
{’role’: ’assistant’, ’content’: ’105,297,61’},
{’role’: ’user’, ’content’: ’677,715,9+709,616,8=’},
{’role’: ’assistant’, ’content’: ’138,733,27’},
{’role’: ’user’, ’content’: ’830,208,0+352,945,6=’}]
L2R tokenization, thinking token control by using 2 extra spaces:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3790206 + 6739555 = ’},
{’role’: ’assistant’, ’content’: ’10529761’},
{’role’: ’user’, ’content’: ’6777159 + 7096168 = ’},
{’role’: ’assistant’, ’content’: ’13873327’},
{’role’: ’user’, ’content’: ’8302080 + 3529456 = ’}]
L2R tokenization, thinking token control by using 2 extra spaces:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3790206 + 6739555 = ’},
{’role’: ’assistant’, ’content’: ’10529761’},
{’role’: ’user’, ’content’: ’6777159 + 7096168 = ’},
{’role’: ’assistant’, ’content’: ’13873327’},
{’role’: ’user’, ’content’: ’8302080 + 3529456 = ’}]
Repeat L2R →R2L:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3790206+6739555=’},
{’role’: ’assistant’, ’content’: ’3,790,206+6,739,555=10,529,761’},
{’role’: ’user’, ’content’: ’6777159+7096168=’},
{’role’: ’assistant’, ’content’: ’6,777,159+7,096,168=13,873,327’},
{’role’: ’user’, ’content’: ’8302080+3529456=’}]
Repeat control L2R →L2R:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3790206+6739555=’},
{’role’: ’assistant’, ’content’: ’3790206+6739555=10529761’},
{’role’: ’user’, ’content’: ’6777159+7096168=’},
{’role’: ’assistant’, ’content’: ’6777159+7096168=13873327’},
{’role’: ’user’, ’content’: ’8302080+3529456=’}]
20

--- PAGE 21 ---
Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs
Output control L2R →R2L:
[{’role’: ’system’, ’content’: ’You are a helpful assistant.’},
{’role’: ’user’, ’content’: ’3790206+6739555=’},
{’role’: ’assistant’, ’content’: ’10,529,761’},
{’role’: ’user’, ’content’: ’6777159+7096168=’},
{’role’: ’assistant’, ’content’: ’13,873,327’},
{’role’: ’user’, ’content’: ’8302080+3529456=’}]
21
