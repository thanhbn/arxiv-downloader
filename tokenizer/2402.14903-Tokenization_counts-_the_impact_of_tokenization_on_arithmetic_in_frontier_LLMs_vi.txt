# 2402.14903.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/2402.14903.pdf
# Kích thước tệp: 680826 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tokenization có ý nghĩa: tác động của tokenization đối với tính toán số học trong các mô hình ngôn ngữ lớn tiên tiến
Aaditya K. Singh1DJ Strouse2
Tóm tắt
Tokenization, việc chia văn bản đầu vào thành các token đầu vào, là một khía cạnh thường bị bỏ qua của pipeline mô hình ngôn ngữ lớn (LLM) và có thể là nguồn gốc của các thiên lệch quy nạp hữu ích hoặc có hại. Trong lịch sử, các LLM đã dựa vào mã hóa cặp byte, mà không quan tâm đến các miền đầu vào cụ thể. Với việc tăng cường sử dụng LLM cho lý luận, các sơ đồ tokenization cụ thể cho số đã được áp dụng, với các mô hình phổ biến như LLaMa và PaLM lựa chọn tokenization một chữ số trong khi GPT-3.5 và GPT-4 có các token riêng biệt cho mỗi số 1, 2 và 3 chữ số. Trong nghiên cứu này, chúng tôi nghiên cứu ảnh hưởng của lựa chọn này đối với lý luận số thông qua việc sử dụng các nhiệm vụ số học. Chúng tôi xem xét tokenization từ trái sang phải và từ phải sang trái cho GPT-3.5 và -4, phát hiện rằng tokenization từ phải sang trái (được thực thi bằng cách phân tách số bằng dấu phẩy tại thời điểm suy luận) dẫn đến hiệu suất được cải thiện đáng kể. Hơn nữa, chúng tôi phát hiện rằng các lỗi của mô hình khi sử dụng tokenization từ trái sang phải tiêu chuẩn tuân theo các mẫu lỗi định kiểu, cho thấy rằng các tính toán của mô hình là có hệ thống chứ không phải xấp xỉ. Chúng tôi chỉ ra rằng mô hình có thể chuyển đổi giữa các tokenization một cách dễ dàng, do đó cho phép các phương pháp lấy cảm hứng từ chuỗi suy nghĩ phục hồi hiệu suất trên các đầu vào được tokenize từ trái sang phải. Chúng tôi cũng thấy rằng khoảng cách giữa các hướng tokenization giảm khi các mô hình được mở rộng quy mô, có thể cho thấy rằng các mô hình lớn hơn có khả năng ghi đè thiên lệch quy nạp phụ thuộc vào tokenization này tốt hơn. Tóm lại, nghiên cứu của chúng tôi thực hiện nghiên cứu đầu tiên về cách các lựa chọn tokenization số dẫn đến sự khác biệt trong hiệu suất mô hình trên các nhiệm vụ số học, kèm theo phân tích kỹ lưỡng về các mẫu lỗi. Chúng tôi hy vọng nghiên cứu này truyền cảm hứng cho các nhà thực hành cân nhắc cẩn thận hơn các lựa chọn liên quan đến tokenization số khi hướng tới các mô hình lý luận số tổng quát.

1Gatsby Computational Neuroscience Unit, University College London2Google DeepMind. Liên hệ: Aaditya Singh <aaditya.singh.21@ucl.ac.uk >.

8302080
+ 3529456
118315368,302,080
+ 3,529,456
11,831,536
Tokenization L2R 
(tiêu chuẩn)Tokenization R2L 
(được thực thi bằng dấu phẩy)
GPT 3.5
Độ chính xác:75.6% 97.8%
GPT 4
Độ chính xác:84.4% 98.9%Hình 1. Minh họa sự phụ thuộc của hiệu suất số học mô hình tiên tiến vào tokenization. Chúng tôi cho thấy cách sử dụng dấu phẩy có thể thực thi tokenization từ phải sang trái (R2L) cho cùng một bài toán cộng. Tokenization R2L dẫn đến hiệu suất mô hình được cải thiện trên cả GPT-3.5 và GPT-4 (các mô hình tháng 3 năm 2023), điều mà chúng tôi chỉ ra là do sự căn chỉnh tokenization giữa các số hạng và câu trả lời thông qua các kiểm soát và phân tích lỗi khác nhau.

1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) thường được ca ngợi vì chứng minh lợi ích của việc học từ đầu đến cuối so với các thiên lệch quy nạp. Tuy nhiên, một phần thường bị bỏ qua của pipeline, ngăn cản nó trở thành từ đầu đến cuối, là tokenization: việc phân đoạn một chuỗi byte đầu vào thành các token rời rạc. Tokenization bao gồm hai nửa: huấn luyện, trong đó từ vựng các token và thống kê được học trên một kho dữ liệu nhất định, và phân đoạn, nơi một hàm sử dụng từ vựng và thống kê đã được huấn luyện để ánh xạ các chuỗi byte thành token. Mỗi sơ đồ tokenization có thể truyền đạt các thiên lệch quy nạp khác nhau lên mô hình do cách thức các byte của chuỗi đầu vào được nhóm - trong nghiên cứu này, chúng tôi nghiên cứu các hiệu ứng phụ thuộc vào tokenization này đối với lý luận số trong các mô hình tiên tiến (GPT-3.5, GPT-4) bằng cách xem xét tokenization của các số trong các bài toán số học.

Mặc dù nhiều kỹ thuật đã được đề xuất cho tokenization, các phương pháp phổ biến trong các mô hình tiên tiến ngày nay là các biến thể của Mã hóa Cặp Byte (BPE) (Gage, 1994; Sennrich et al., 2016). BPE là một phương pháp thống kê cho tokenization được học từ một tập dữ liệu, trong trường hợp của LLM, là văn bản. Về mặt trực quan, BPE nén tập dữ liệu bằng cách lặp đi lặp lại tạo ra các token cho các chuỗi con xuất hiện thường xuyên nhất. Cụ thể, BPE bắt đầu với từ vựng token bao gồm mỗi ký tự trong văn bản (ví dụ: chữ cái, số, dấu câu).

--- TRANG 2 ---
Tokenization có ý nghĩa: tác động của tokenization đối với tính toán số học trong các mô hình ngôn ngữ lớn tiên tiến

000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049
050051052053054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099
100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149
150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199
200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249
250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299
300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349
350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399
400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449
450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499
500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549
550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599
600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649
650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699
700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749
750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799
800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849
850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899
900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949
950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999

Hình 2. Tất cả các chuỗi 3 chữ số, được tô màu đỏ khi chuỗi không có token đơn tương ứng trong p50k base, tokenizer BPE cho GPT-3. Mặc dù có một số mẫu (ví dụ: gần như tất cả các bội số của 10 đều có mặt), nhìn chung không có cấu trúc rõ ràng. Các token bị thiếu là tạo phẩm của quy trình cụ thể mà các tokenizer BPE sử dụng để thiết lập từ vựng.

1Chính xác hơn, các byte đơn được sử dụng để xử lý tính đa ngôn ngữ, nhưng mức độ mô tả này đủ cho nhu cầu của chúng ta.

Sử dụng từ vựng này, thống kê bigram (tức là tần suất của các cặp) được tính toán, và bigram phổ biến nhất được thêm vào từ vựng và được hợp nhất trong tập dữ liệu. Quá trình này được lặp lại cho đến khi đạt được kích thước từ vựng tối đa được chỉ định trước. Sau khi tokenizer được học, tokenization của văn bản mới tiến hành bằng cách lặp đi lặp lại hợp nhất các ký tự/token theo cùng thứ tự như đã học trên tập dữ liệu huấn luyện.

Việc áp dụng BPE một cách ngây thơ trên các kho dữ liệu quy mô internet dẫn đến tokenization số rất đặc thù (Teknium, 2023). Trong giai đoạn huấn luyện, việc số nào nhận được token chuyên dụng rất tùy tiện - ví dụ, 710 có thể có token chuyên dụng, trong khi 711 thì không (Hình 2). Trong giai đoạn phân đoạn, các token tùy tiện này sẽ dẫn đến các phân vùng khác nhau của các số có cùng độ dài. Trong Hình 3, chúng tôi minh họa các phân vùng khác nhau của tất cả các số 4 chữ số khi sử dụng tokenizer p50k base được sử dụng để huấn luyện GPT-3 (Brown et al., 2020). Để kiểm soát các hiệu ứng đối với hiệu suất

0 0.25 0.50 0.75 1
phần của các số 4 chữ sốcl100k_basep50k_basephân vùng
[1234]
[123][4]
[12][34]
[1][234]
[1][23][4]

Hình 3. So sánh cách p50k base, tokenizer cho GPT-3, và cl100k base, tokenizer cho GPT-3.5 và GPT-4, phân đoạn các chuỗi 4 chữ số thành token. cl100k base đã chuẩn hóa tokenization số thành các khối 3 chữ số, từ trái sang phải, dẫn đến tất cả các số N chữ số được phân đoạn theo cùng một cách.

downstream từ tokenization đặc thù như vậy, các nghiên cứu trước (Nye et al., 2021; Zhou et al., 2022) đã sử dụng định dạng, chẳng hạn như dấu cách hoặc dấu phẩy, để tách các chữ số riêng lẻ, đảm bảo mỗi chữ số ánh xạ thành một token duy nhất.

Các mô hình mới hơn và các tokenizer tương ứng cho thấy rằng các nhà thực hành LLM trên các phòng thí nghiệm khác nhau cũng đã cố gắng kiểm soát tokenization đặc thù (Bảng 1).2 Các mô hình PaLM (Chowdhery et al., 2023), LLaMa (Touvron et al., 2023a) và Mistral3 (Jiang et al., 2023) chuyển sang tokenization một chữ số, tương tự như được thực thi bởi Nye et al. (2021). Thú vị là tokenizer của GPT-3.5 và GPT-4, cl100k base, giới thiệu token cho tất cả các chuỗi 1, 2 và 3 chữ số.4 Tokenization số bởi các mô hình GPT này mặc định chia một số dài thành các khối 3 chữ số, từ trái sang phải, điều mà chúng tôi giả thuyết (và sau đó chỉ ra) có thể tạo ra vấn đề cho lý luận số.

Những phương pháp đa dạng về tokenization bởi các LLM tiên tiến ngày nay cho thấy sự thiếu hội tụ trong lĩnh vực về các thực hành tốt nhất và kêu gọi phân tích sâu hơn về các thiên lệch quy nạp (tích cực hoặc tiêu cực) được truyền đạt bởi các sơ đồ tokenization khác nhau. Trong nghiên cứu này, chúng tôi cung cấp so sánh có hệ thống đầu tiên về hiệu suất mô hình trên cùng các nhiệm vụ lý luận số với tokenization đa dạng. Cụ thể, chúng tôi xem xét các mô hình GPT mới nhất trên các nhiệm vụ số học few-shot.

2Một ngoại lệ đáng chú ý là các mô hình Claude của Anthropic, vẫn sử dụng token số BPE thuần túy (xem Phụ lục A).
3Được xác minh bằng cách kiểm tra các token từ huggingface https://huggingface.co/docs/transformers/main/en/model doc/mistral.
4Tokenization này được thực thi bởi tham số patstr bí ẩn trong thư viện tokenization của họ, https://github.com/openai/tiktoken/blob/main/tiktoken ext/openai public.py Dòng 76.

--- TRANG 3 ---
Tokenization có ý nghĩa: tác động của tokenization đối với tính toán số học trong các mô hình ngôn ngữ lớn tiên tiến

Bảng 1. Các LLM phổ biến và chiến lược tokenization số của họ.
BPE = mã hóa cặp byte. L2R = từ trái sang phải.
Mô hình Chiến lược
GPT-3 (2020) BPE thuần túy
GPT-3.5 (2022) Khối L2R của 3 chữ số
GPT-4 (2023) Khối L2R của 3 chữ số
Claude v2.1 (2023) BPE thuần túy
Gopher (2021) BPE thuần túy
Chinchilla (2022) BPE thuần túy
PaLM (2022) chữ số đơn
GPT-J (2021) BPE thuần túy
Llama 1 & 2 (2023) chữ số đơn
Mistral (2023) chữ số đơn
OLMo (2024) BPE thuần túy

Chúng tôi thay đổi hướng tokenization thành mặc định từ trái sang phải (L2R) hoặc từ phải sang trái (R2L). Chúng tôi phát hiện rằng độ chính xác của mô hình cao hơn đến 20% khi sử dụng tokenization R2L (Hình 1, Mục 3). Sau đó chúng tôi cung cấp phân tích kỹ lưỡng về các mẫu lỗi trên hai tokenization này (Mục 4). Chúng tôi phát hiện rằng sự khác biệt về hiệu suất giữa tokenization R2L và L2R trong GPT-3.5 có thể được giải thích phần lớn bởi một mẫu lỗi vô cùng định kiểu và đáng ngạc nhiên (Mục 4.3), có thể cho thấy sự hiện diện của một số lý luận có hệ thống nhưng có lỗi. Tiếp theo, chúng tôi chỉ ra rằng các phương pháp lấy cảm hứng từ chuỗi suy nghĩ, nơi một mô hình được yêu cầu lặp lại đầu vào trong tokenization R2L, phục hồi độ chính xác bị mất do tokenization L2R (Mục 5). Cuối cùng, chúng tôi kết luận bằng cách nghiên cứu cách những hiệu ứng này có thể thay đổi với phiên bản mô hình, phát hiện rằng các mô hình lớn hơn có khả năng ghi đè các hiệu ứng do tokenization gây ra tốt hơn nhưng, tính đến hiện tại, chưa thể loại bỏ chúng (Mục 6). Nhìn chung, chúng tôi xem những kết quả này là bằng chứng thuyết phục cho các thiên lệch quy nạp phụ thuộc vào tokenization đáng kể trong các mô hình ngôn ngữ lớn, và hy vọng chúng dẫn đến việc các nhà thực hành mô hình tiến hành các ablation huấn luyện trước cẩn thận với các sơ đồ tokenization đa dạng, đặc biệt cho lý luận số.

2. Phương pháp
2.1. Thiết lập thí nghiệm
Chúng tôi đánh giá các mô hình GPT thông qua endpoint Chat Completions trên API OpenAI5 trên các bài toán cộng few-shot. Chúng tôi kiểm soát độ dài chữ số của số hạng, từ 7 đến 9 chữ số (được chọn vì theo cách này mỗi số hạng dài 3 token). Đối với hầu hết các thí nghiệm, chúng tôi sử dụng 90 bài toán ngẫu nhiên, với 10 bài toán cho mỗi cặp độ dài chữ số số hạng (ví dụ: 10 bài toán trong đó cả hai số hạng đều dài 7 chữ số, 10 bài toán trong đó số hạng đầu tiên dài 7 chữ số và số thứ hai dài 8, v.v.). Đối với các shot, chúng tôi xem xét 1-, 2-, 4- và 8-shot. Các shot được lấy mẫu ngẫu nhiên cho mỗi bài toán "truy vấn", và được cung cấp cho mô hình dưới dạng đối thoại đa lượt. Chúng tôi kiểm soát các shot để có cùng dạng (độ dài chữ số, hướng tokenization, v.v.) như bài toán truy vấn. Chúng tôi sử dụng prompt hệ thống mặc định "You are a helpful assistant." để tối đa hóa tính tái tạo.6 Mã Python cho một số truy vấn 2-shot ví dụ đến mô hình được trình bày trong Phụ lục E.3 để rõ ràng tối đa. Chúng tôi sử dụng giải mã tham lam (temperature=0) trong tất cả các thí nghiệm. Độ chính xác được tính bằng cách trích xuất số từ phản hồi của mô hình.

Hầu hết các thí nghiệm trong bài báo được chạy bằng checkpoint mô hình gpt-3.5-turbo-0301, mặc dù trong Mục 6 chúng tôi xem xét cách kết quả mở rộng đến các phiên bản mới hơn của cùng mô hình (gpt-3.5-turbo-0613) và đến các mô hình GPT-4 được cho là lớn hơn (gpt-4-0314, gpt-4-0613). Tất cả mã và bảng kết quả đầy đủ có thể được tìm thấy tại https://github.com/aadityasingh/TokenizationCounts.

2.2. Thay đổi tokenization L2R so với R2L
API ChatCompletion chỉ cho phép văn bản đầu vào, không phải token đầu vào, vì vậy việc tiến hành các thí nghiệm thay đổi tokenization khá khó khăn. Để buộc mô hình sử dụng tokenization R2L cho số, chúng tôi thêm dấu phẩy cứ mỗi 3 chữ số từ bên phải (xem Hình 1). Vì tokenizer không chứa token nào có số và dấu phẩy, các dấu phẩy được tokenize riêng biệt, hiệu quả thực thi một phân đoạn khác của các chữ số. Chúng tôi sử dụng thiết lập này để minh họa kết quả chính của chúng tôi, và tiến hành các kiểm soát khác nhau để đảm bảo rằng hiệu ứng quan sát được của chúng tôi là do tokenization chứ không phải các yếu tố gây nhiễu khác.

3. Tokenization từ phải sang trái cải thiện hiệu suất mô hình
3.1. Kết quả chính
Khi sử dụng dấu phẩy để tách chữ số và thực thi tokenization R2L, chúng tôi quan sát thấy hiệu suất trung bình được cải thiện đáng kể (kết quả 8-shot trong Hình 1). Chúng tôi phát hiện rằng việc tăng số shot (Hình 4) dẫn đến sự gia tăng lớn hơn cho tokenization L2R (từ 68.5% 1-shot đến 75.6% 8-shot) so với tokenization R2L (từ 95.6% 1-shot đến 97.8% 8-shot) cho thấy rằng học trong ngữ cảnh có thể giảm thiểu nhẹ thiên lệch (có hại) của tokenization L2R. Với phát hiện này và hiệu suất đạt đỉnh với việc tăng shot, chúng tôi chỉ báo cáo kết quả 8-shot cho phần còn lại của nghiên cứu vì điều này làm cho tokenization L2R cạnh tranh nhất.

3.2. Kiểm soát các tiên nghiệm ngữ nghĩa dựa trên dấu phẩy
Mặc dù kết quả này đã thuyết phục, chúng tôi nhận ra rằng dấu phẩy thường được sử dụng để tách chữ số theo cách được mô tả trong Hình 1, vì vậy hiệu ứng quan sát được có thể bị nhiễu bởi tỷ lệ phổ biến trong dữ liệu huấn luyện (McCoy et al., 2023). Người ta có thể lập luận rằng việc tách bằng dấu phẩy thực sự đưa đầu vào gần hơn đến phân phối huấn luyện của mô hình, vì vậy không ngạc nhiên khi các mô hình hoạt động tốt hơn. Để kiểm soát điều này và tập trung vào tokenization, chúng tôi xem xét các dấu tách thay thế, token đơn: ' ', '.', '$', '#' (lưu ý chúng tôi sẽ gọi ' ' là <space> để rõ ràng). Ví dụ, số 8302080 sẽ được viết là 8#302#080 khi nhập vào mô hình.

Kết quả được hiển thị trong Hình 5. Chúng tôi phát hiện rằng mô hình phần lớn không quan tâm đến dấu tách được sử dụng, cho thấy rằng tokenization có thể là hiệu ứng chủ đạo, chứ không phải lựa chọn cụ thể của việc sử dụng dấu phẩy.

<space> . $ #
Dấu phân cách được sử dụng để thực thi
Tokenization R2L0.60.81.0Độ chính xácTokenization R2L
dựa trên dấu phẩy
Tokenization L2R

Hình 5. Độ chính xác 8-shot khi sử dụng các dấu phân cách khác nhau cho tokenization R2L. Các đường chấm cho thấy kết quả từ Hình 1 để so sánh. Nhìn chung, chúng ta thấy lựa chọn dấu phân cách ít quan trọng hơn hướng tokenization.

3.3. Kiểm soát "token suy nghĩ"
Một yếu tố gây nhiễu khác với thí nghiệm trên có thể là việc thêm dấu phẩy vừa tăng số token nhập vào cũng như được tạo ra bởi mô hình. Do đó, để tạo ra cùng một câu trả lời, mô hình có quyền truy cập vào nhiều bước tính toán hơn (tức là FLOP). Có lo ngại rằng các mô hình có thể sử dụng các token suy nghĩ lặp lại này để thực hiện các tính toán hữu ích bổ sung (Lanham et al., 2023). Trong thực tế, điều này dường như không xảy ra mà không cần huấn luyện thêm (Goyal et al., 2024), nhưng chúng tôi đã tiến hành các thí nghiệm để xác minh điều này trong thiết lập của chúng tôi.

5https://platform.openai.com/

, . 1 20.00.51.0Độ chính xácTokenization R2L
dựa trên dấu phẩy
Tokenization L2R
Dấu phân cách cho
Tokenization L2RSố lượng
dấu cách được thêm

Hình 6. Độ chính xác 8-shot cho các kiểm soát "token suy nghĩ" khác nhau. Các đường chấm cho thấy kết quả từ Hình 1 để so sánh. Chúng tôi cũng thử nghiệm với các dấu phân cách khác cho tokenization L2R (tất cả những cái từ Hình 5), nhưng thấy kết quả kém tương tự. Nhìn chung, "token suy nghĩ" không phục hồi được sự tăng hiệu suất từ việc sử dụng tokenization R2L được thực thi bằng dấu phẩy.

Để kiểm soát token suy nghĩ, chúng tôi xem xét hai loại kiểm soát. Trong loại đầu tiên, chúng tôi sử dụng dấu phân cách để thực thi tokenization L2R - điều này thực thi sự khớp chính xác trong số lượng token prompt. Thứ hai, chúng tôi xem xét việc thêm 1 hoặc 2 dấu cách trước và sau dấu + và = để tăng số token7 trong trường hợp L2R (nơi không sử dụng dấu phân cách). Cả hai điều này đều có lợi ích của việc thêm các token cực kỳ "có thể dự đoán" (khi sử dụng 8-shot), cho phép mô hình có thể sử dụng các bước tính toán bổ sung để "suy nghĩ".

Trong Hình 6, chúng tôi phát hiện rằng không có kiểm soát nào trong số này, khi được áp dụng cho các chuỗi được tokenize L2R, phục hồi hiệu suất của tokenization R2L. Thực tế, chúng tôi thấy rằng việc sử dụng dấu phân cách với tokenization L2R thường làm tổn hại hiệu suất, có thể vì đây là một biểu diễn không phổ biến - khi kiểm tra định tính một vài ví dụ, chúng tôi thấy mô hình đôi khi "tự động sửa" các đầu vào bằng cách ảo giác các số không cuối. Chúng tôi tin rằng những thí nghiệm này hiệu quả loại trừ yếu tố gây nhiễu "token suy nghĩ".

4. Phân tích lỗi tiết lộ các mẫu định kiểu
Với hiệu ứng mạnh mẽ được quan sát trong Mục 3, chúng tôi tò mò xem liệu có bất kỳ mẫu nào trong các lỗi không. Dưới đây, chúng tôi tóm tắt các phát hiện chính của mình.

4.1. Tokenization L2R tệ hơn đáng kể khi câu trả lời dài hơn số hạng
Như đã lưu ý trong Mục 2.1, chúng tôi cân bằng tập dữ liệu bài toán của mình dựa trên độ dài chữ số đầu vào. Khi kiểm tra các bài toán mà mô hình làm sai khi sử dụng tokenization L2R, chúng tôi nhận thấy rằng các lỗi dường như có nhiều khả năng xảy ra hơn khi câu trả lời dài hơn các số hạng (ví dụ: một bài toán trong đó số 7 chữ số + số 7 chữ số = số 8 chữ số, dạng được mô tả

7Cụ thể, số lượng token trong prompt 8-shot tăng từ 195 lên 213 lên 240 khi đi từ 0 lên 1 lên 2 dấu cách. Để tham khảo, số lượng token R2L là 247.

--- TRANG 4 ---
Tokenization có ý nghĩa: tác động của tokenization đối với tính toán số học trong các mô hình ngôn ngữ lớn tiên tiến

Khớp độ dài
Không khớp độ dài0.00.51.0Độ chính xácHướng
tokenization
L2R
R2L

Hình 7. Khi câu trả lời có cùng độ dài chữ số với một số hạng (khớp độ dài), cả hai sơ đồ tokenization đều hoạt động tương tự (trái). Khi câu trả lời có độ dài chữ số khác với cả hai số hạng (không khớp độ dài), tokenization L2R phá hủy hiệu suất mô hình, giảm xuống 8.25% (phải).

1 2 3 4 5 6 7
Số lần nhớ trong bài toán0.80.91.0Độ chính xác
Tokenization R2L
Tokenization L2R

Hình 8. Độ chính xác theo hàm số lần nhớ. Đối với tokenization L2R, chúng tôi loại trừ các bài toán mà độ dài câu trả lời không khớp với ít nhất một số hạng, vì mô hình bỏ lỡ hầu hết những bài toán đó (92%) như được hiển thị trong Mục 4.1. Nếu số lần nhớ (một khái niệm khó khăn của con người) tương quan với hiệu suất mô hình, chúng ta sẽ mong đợi một độ dốc âm. Sự thiếu xu hướng nào cho thấy hiệu suất mô hình phần lớn độc lập với số lần nhớ.

trong Hình 1). Để kiểm tra giả thuyết này, chúng tôi tiến hành một thí nghiệm mới nơi chúng tôi kiểm soát độ dài số hạng và độ dài câu trả lời. Cụ thể, chúng tôi tạo ra 100 bài toán ngẫu nhiên cho mỗi bộ ba độ dài chữ số có thể có trong đó các số hạng và câu trả lời có độ dài từ 7 đến 9 chữ số (danh sách đầy đủ trong Phụ lục E.1). Phần còn lại của các thí nghiệm trong mục này sẽ sử dụng tập hợp bài toán mở rộng này để chỉ ra tính mạnh mẽ của các mẫu lỗi được tìm thấy.

Chúng tôi tái tạo hiện tượng chính của chúng tôi (Mục 3.1), và khẳng định thêm trực giác của chúng tôi về các mẫu lỗi. Như được hiển thị trong Hình 7, chúng tôi thấy rằng tokenization L2R có hiệu suất tương tự với tokenization R2L khi độ dài câu trả lời bằng chữ số giống với một trong các đầu vào (mà chúng tôi gọi là điều kiện "khớp độ dài"). Khi câu trả lời dài hơn các đầu vào (do một lần nhớ cuối cùng), tokenization L2R tệ hơn đáng kể, với độ chính xác giảm xuống 8.25% - chúng tôi gọi đây là điều kiện "không khớp độ dài". Chúng tôi nghi ngờ rằng hiệu ứng mạnh này có thể do sự không khớp giữa tokenization đầu vào và đầu ra (như được minh họa trong Hình 1) chứ không phải một số khái niệm khó khăn liên quan đến nhớ, mà chúng tôi khám phá trong vài mục con tiếp theo.

4.2. Lỗi dường như không tương quan với số lần nhớ
Một giả thuyết tự nhiên với kết quả trên có thể là các lỗi chỉ có thể tương quan với một số khái niệm khó khăn, chẳng hạn như nhớ. Trong Hình 8, chúng tôi thấy rằng điều này thường không phải như vậy. Cụ thể, chúng tôi xem xét độ chính xác trên các tập con bài toán dựa trên số lần nhớ cần thiết để giải chúng.8 Sự thiếu xu hướng tích cực hoặc tiêu cực rõ ràng cho thấy rằng hiệu suất mô hình không bị ảnh hưởng mạnh bởi số lần nhớ.

4.3. Các bài toán không khớp độ dài tạo ra mẫu lỗi "chữ số 4" định kiểu
Nếu không phải nhớ, điều gì có thể gây ra mẫu lỗi đáng ngạc nhiên trong Hình 7? Trong Hình 9a, chúng tôi thấy rằng các lỗi khi sử dụng tokenization L2R cực kỳ định kiểu và hoàn toàn không trực quan. Cụ thể, trong điều kiện không khớp độ dài, mô hình luôn làm sai chữ số thứ tư. Hơn nữa, mô hình luôn làm đúng 3 chữ số đầu tiên (tương ứng với token đầu ra đầu tiên). Về mức độ sai lệch của mô hình đối với chữ số 4, Hình 9b cho thấy có sự thiên vị nhẹ đối với lỗi sai lệch một, nhưng nhìn chung việc thay thế cụ thể xuất hiện khá ngẫu nhiên.

Chúng tôi thấy kết quả này cực kỳ đáng ngạc nhiên. Trong khoa học nhận thức, các mẫu lỗi định kiểu như vậy thường được sử dụng làm bằng chứng cho quá trình xử lý có hệ thống bên dưới. Trong khi cơ chế cộng trong LLM vẫn không rõ ràng, chúng tôi thấy mẫu lỗi nổi bật, phụ thuộc vào tokenization này9 rất gợi ý về một số thuật toán bên dưới (trái ngược với các gợi ý rằng LLM có thể thực hiện số học bằng cách sử dụng một số "khớp mờ" với các bài toán tương tự trong huấn luyện). Chúng tôi cung cấp bằng chứng thêm về các mẫu lỗi định kiểu bằng cách phân tích xác suất log của mô hình trong Phụ lục D.

4.4. Lỗi sai lệch một tại ranh giới token chiếm gần như tất cả các lỗi còn lại
Sau khi tính đến nguồn lỗi chính, chúng tôi phân tích các lỗi còn lại trên cả hai phương pháp tokenization: 25 trong số 1300 bài toán cho tokenization R2L, và 56 trong số 900 bài toán trong điều kiện khớp độ dài cho tokenization L2R.

Đối với tokenization R2L, trong số 25 bài toán bị sai, 24 do lỗi sai lệch một (cao hơn hoặc thấp hơn). Đối với tokenization L2R, trong số 56 bài toán bị sai, 53 do lỗi sai lệch một (cao hơn hoặc thấp hơn). Đối với gần như tất cả những lỗi sai lệch một này,10 bất kể hướng tokenization, chúng tôi thấy rằng bản thân lỗi xảy ra ở chữ số cuối cùng của một token đầu ra. Kết quả này cho thấy rằng lỗi sai lệch một có nhiều khả năng xảy ra qua ranh giới token hơn là ở giữa token 3 chữ số. Giả thuyết này, với bằng chứng sơ bộ, kết nối với các công trình về khái quát hóa độ dài (Anil et al., 2022) - việc sử dụng token 3 chữ số có thể làm cho khái quát hóa độ dài dễ dàng hơn vì các mô hình chỉ cần vượt qua ranh giới token mỗi chữ số thứ ba (thay vì mỗi chữ số).

5. Các mô hình có thể chuyển đổi từ tokenization L2R sang R2L, cải thiện hiệu suất
5.1. Kết quả chính
Với các kết quả trên cho thấy rằng tokenization số có thể ảnh hưởng mạnh đến lý luận số, chúng tôi hỏi liệu các mô hình có thể được nhắc để lấy các bài toán trong tokenization ít ưa thích (L2R) và chuyển đổi chúng thành tokenization ưa thích hơn (R2L) để cải thiện hiệu suất không. Lấy cảm hứng từ các phương pháp chuỗi suy nghĩ (Nye et al., 2021; Kojima et al., 2022; Wei et al., 2022), chúng tôi nhắc các mô hình few-shot để lấy các bài toán với một hướng tokenization, và sau đó lặp lại bài toán và trả lời nó bằng hướng tokenization khác. Trong Hình 10, chúng tôi thấy rằng các mô hình thực sự hoạt động gần như tốt trong phép cộng khi chuyển đổi tokenization L2R sang R2L bằng chính chúng như khi chúng nhận bài toán trong tokenization R2L ngay từ đầu. Hiệu suất tăng với số shot khi chuyển đổi L2R sang R2L vì mô hình tuân thủ kiểu lặp lại (hữu ích) được đề xuất nhiều hơn. Những kết quả này cho thấy rằng các mô hình có thể chuyển đổi giữa các tokenization để giải bài toán đúng, nhưng không làm điều đó một cách ngầm định trong lượt chuyển tiếp.

8Vì chúng tôi không kiểm soát rõ ràng nhớ khi tạo bài toán, số lượng bài toán với một số lần nhớ nhất định thay đổi. Chúng tôi chỉ xem xét các trường hợp mà chúng tôi có ít nhất 50 bài toán.
9Mẫu lỗi này không có mặt khi sử dụng tokenization R2L.
10Cụ thể, tất cả 24 lỗi sai lệch một trong trường hợp R2L, và 51 trong số 53 lỗi sai lệch một trong trường hợp L2R.

--- TRANG 5 ---
Tokenization có ý nghĩa: tác động của tokenization đối với tính toán số học trong các mô hình ngôn ngữ lớn tiên tiến

(a) Vị trí lỗi
123456789Khác Không có
Lỗi ở chữ số _0.00.51.0Phần của các bài toán
không khớp độ dài

(b) Độ lớn lỗi
1 2 3 4 5 6 7 8 9
Sai lệch0.00.10.2Phần của lỗi chữ số 4

Hình 9. a) Mẫu lỗi cho tokenization L2R trên các bài toán mà độ dài chữ số câu trả lời khác với độ dài số hạng. "Không có" chỉ ra các bài toán trong trường hợp này mà mô hình làm đúng (8.25%). "Khác" chỉ ra các bài toán mà mô hình không cung cấp câu trả lời hợp lệ hoặc cung cấp câu trả lời có độ dài sai (0.5%). Trong số 91.25% còn lại mà mô hình làm sai, nó luôn làm sai chữ số 4 một cách đáng kinh ngạc. Ngoài ra, đôi khi nó cũng làm sai các chữ số khác (5, 6 hoặc 7). b) Đối với các lỗi ở chữ số 4, chúng tôi cho thấy độ lớn của sai lầm. Ví dụ, nếu giá trị đúng của chữ số 4 là 2 và phản hồi mô hình có chữ số 4 bằng 5, nó sẽ sai lệch 3. Chúng ta thấy sự thiên vị nhẹ đối với lỗi sai lệch 1, nhưng độ lớn lỗi được phân phối khá đều.

12 4 8
Số shot0.500.751.00Độ chính xác
Hướng tokenization
(đầu vào → lặp lại+trả lời)
R2L → R2L
L2R → R2L
L2R → L2R
R2L → L2R

Hình 10. Độ chính xác few-shot khi các mô hình nhận bài toán với một hướng tokenization, sau đó lặp lại và trả lời nó theo hướng khác.

5.2. Kiểm soát tokenization đầu ra
Một yếu tố gây nhiễu với thí nghiệm trên có thể chỉ là mô hình cải thiện khi được yêu cầu tạo ra câu trả lời với tokenization R2L. Để kiểm soát điều này, chúng tôi tiến hành thí nghiệm tương tự, nhưng không có nhắc few-shot cho mô hình lặp lại bài toán: prompt few-shot cung cấp câu trả lời với hướng tokenization khác với đầu vào, khuyến khích mô hình trả lời với hướng tokenization này (xem Phụ lục E.3 cho ví dụ prompt). Trong Hình 11, chúng ta thấy rằng chỉ trả lời với tokenization R2L không cải thiện hiệu suất (đường cong tím) đến mức mà lặp lại trong tokenization R2L làm được (đường cong tím, Hình 10), khi bắt đầu từ tokenization L2R. Hiệu ứng này cho thấy rằng điều quan trọng là mô hình cũng thấy bài toán trong tokenization ưa thích (bằng cách lặp lại nó), chứ không chỉ trả lời trong tokenization ưa thích.

6. Các hiệu ứng phụ thuộc vào tokenization chủ yếu mở rộng đến các mô hình tương lai
Qua các mục trước, chúng tôi đã chứng minh một hiệu ứng phụ thuộc vào tokenization mạnh mẽ. Trong mục này, chúng tôi đề cập đến câu hỏi: hiệu ứng này có mở rộng đến các mô hình mới hơn không?

--- TRANG 6 ---
Tokenization có ý nghĩa: tác động của tokenization đối với tính toán số học trong các mô hình ngôn ngữ lớn tiên tiến

12 4 8
Số shot0.500.751.00Độ chính xác
Hướng tokenization
(đầu vào → trả lời)
R2L → R2L
L2R → R2L
L2R → L2R
R2L → L2R

Hình 11. Độ chính xác few-shot khi các mô hình nhận bài toán trong một định dạng tokenization, và trả lời nó theo định dạng khác. Sự khác biệt giữa điều này và Hình 10 là các mô hình không lặp lại bài toán trong trường hợp này. Chúng tôi lưu ý rằng khi cho mô hình bài toán trong tokenization R2L và nhắc nó trả lời trong tokenization L2R, mô hình thực sự trở nên tệ hơn với nhiều shot hơn, vì với ít shot hơn, mô hình cuối cùng bỏ qua prompt few-shot và trả lời trong tokenization R2L ưa thích của nó. Cụ thể, sự tuân thủ định dạng được nhắc cho R2L →L2R tăng từ chỉ 13.3% với 1 shot lên 98.9% với 8 shot.

GPT-3.5
Turbo
0301GPT-3.5
Turbo
0613GPT-3.5
Turbo
1106GPT-4
0314GPT-4
0613GPT-4
Turbo
11060.60.81.0Độ chính xácHướng
tokenization
L2R
R2L

Hình 12. Hiệu suất 8-shot của các mô hình OpenAI khác nhau trên cùng các bài toán cộng như Hình 4. Các phiên bản mới hơn của GPT-3.5 dường như hoạt động kém như nhau. Đối với GPT-4, chúng ta thấy hiệu ứng phụ thuộc vào tokenization lớn trong mô hình tháng 3, trở nên yếu hơn (nhưng vẫn hiện diện) trong mô hình tháng 6. Mô hình GPT-4 turbo cho thấy sự thoái lui nhẹ trong hiệu suất tổng thể với hiệu ứng phụ thuộc vào tokenization trở nên mạnh hơn trở lại.

Như được hiển thị trong Hình 12, chúng tôi thấy rằng nhìn chung, có: các hiệu ứng phụ thuộc vào tokenization vẫn tồn tại. Chúng tôi xem xét năm mô hình OpenAI "held-out", cho phép chúng tôi xem xét cách các hiệu ứng phụ thuộc vào tokenization thay đổi khi các mô hình được cập nhật (gpt-3.5-turbo-0613, gpt-3.5-turbo-1106) hoặc được mở rộng quy mô (gpt-4-0314, gpt-4-0613) và sau đó được thu nhỏ lại (gpt-4-1106-preview, đây là mô hình "turbo"11). Các phiên bản sau của GPT-3.5 thể hiện hiệu ứng mạnh như nhau do hướng tokenization. Hiệu ứng được giảm thiểu nhẹ trong phiên bản tháng 3 của GPT-4, và được giảm thiểu mạnh trong phiên bản mới nhất của GPT-4.12 Cụ thể, các mô hình GPT-4 dường như tốt hơn trong việc thực hiện số học trên toàn bộ (cho cả hai hướng tokenization). Thú vị là, trong mô hình GPT-4 Turbo mới nhất, hiệu ứng của tokenization trở nên mạnh hơn trở lại. Hơn nữa, Hình 13 cho thấy rằng sự không khớp độ dài chữ số giữa câu trả lời và số hạng lại là lý do chính cho sự giảm hiệu suất khi sử dụng tokenization L2R, trong cả mô hình GPT-3.5 và GPT-4. Chúng tôi tin rằng quy mô huấn luyện tăng của GPT-4 (có thể trong cả số lượng tham số và dữ liệu được thấy) cho phép nó ghi đè tốt hơn thiên lệch quy nạp do tokenization gây ra dẫn đến các mô hình GPT-3.5 hoạt động tệ hơn (tương tự như quy mô giúp giảm thiểu khó khăn chính tả do tokenization gây ra (Liu et al., 2023)). Sự tái xuất hiện của các hiệu ứng phụ thuộc vào tokenization trong mô hình GPT-4 Turbo mới nhất (được cho là nhỏ hơn GPT-4) hỗ trợ giả thuyết này.

11Chúng tôi giả định đây là phiên bản nhỏ hơn, có thể được chưng cất, của GPT-4.
12Chúng tôi thấy thú vị rằng bản cập nhật từ tháng 3 đến tháng 6 của GPT-4 đã cải thiện hiệu suất, nhưng bản cập nhật tương ứng của GPT-3.5 thì không - mà không biết những bản cập nhật này bao gồm gì, tuy nhiên, khó có thể rút ra kết luận về lý do điều này có thể như vậy.

GPT-3.5
Turbo
0301GPT-3.5
Turbo
0613GPT-3.5
Turbo
1106GPT-4
0613GPT-4
Turbo
11060.00.51.0Độ chính xácKhớp
độ dài
Đúng
SaiHướng
tokenization
L2R
R2L

Hình 13. Hiệu suất 8-shot của các mô hình OpenAI khác nhau trên các bài toán được kiểm soát độ dài câu trả lời (xem Mục 4.1), được tách theo việc độ dài câu trả lời có giống với một trong các số hạng hay không. Chúng ta thấy hiệu ứng từ Mục 4.1 tái tạo mạnh mẽ trong phiên bản mới hơn của GPT-3.5. Hiệu ứng vẫn hiện diện trong GPT-4,14 nhưng không mạnh bằng. Thú vị là, hiệu ứng mạnh hơn trong mô hình GPT-4 Turbo mới nhất so với GPT-4.

7. Nghiên cứu liên quan
Phương pháp tokenization Hai phương pháp tokenization hàng đầu là Unigram (Kudo, 2018) và BPE (Sennrich et al., 2016). Trong khi nghiên cứu cũ hơn trong NLP cho thấy lợi ích của Unigram so với BPE (Bostrom & Durrett, 2020), BPE vẫn là phương pháp tokenization được các nhà thực hành LLM hiện đại sử dụng phổ biến nhất. Trong BPE, các mô hình khác nhau thường đưa ra các lựa chọn hard-coded khác nhau, chẳng hạn như loại bỏ token dài của khoảng trắng liên tiếp (Touvron et al., 2023a) hoặc thực thi tokenization một chữ số của số (Chowdhery et al., 2023). Nghiên cứu của chúng tôi chứng minh các hiệu ứng phụ thuộc vào tokenization từ một lựa chọn như vậy, việc sử dụng token 1, 2 và 3 chữ số bởi các mô hình OpenAI (OpenAI et al., 2023). Một cách để giải quyết các vấn đề như vậy có thể là các phương pháp không tokenizer (ví dụ: MEGABYTE (Yu et al., 2023), sử dụng sơ đồ dựa trên patch và không giả định token cố định), nhưng chúng tôi nghi ngờ các sơ đồ này cũng sẽ mang theo thiên lệch quy nạp riêng của chúng. Golkar et al. (2023) cũng giới thiệu sơ đồ mã hóa số liên tục có nghĩa là vượt qua các tạo phẩm tokenization, nhưng phương pháp của họ bị giới hạn trong các trường hợp mà đầu ra mô hình hoàn toàn là số, và không xen kẽ với văn bản.

Tạo phẩm tokenization trong LLM Một tập hợp kết quả đang phát triển đã xuất hiện xung quanh các tạo phẩm liên quan đến tokenization khác nhau trong LLM. Tương tự như nhắc scratchpad (Nye et al., 2021), Wei (2023) thấy rằng việc tách các chữ cái thành token riêng lẻ có thể giúp sắp xếp từ theo chữ cái thứ hai. Nghiên cứu khác (Shin et al., 2020) đã tập trung vào các token cụ thể có thể ảnh hưởng tiêu cực đến hiệu suất mô hình. Rumbelow & mwatkins (2023) thấy nhiều token là tạo phẩm của dữ liệu được sử dụng để huấn luyện trước tokenizer, nhưng có lẽ không có mặt trong dữ liệu huấn luyện của mô hình, dẫn đến các hoàn thành rất không thể dự đoán (và thường hài hước). Sun et al. (2023) tìm thấy tạo phẩm do sự không khớp trong tokenization trong các nhiệm vụ Q&A trích xuất, có thể có kết nối với một số thí nghiệm của chúng tôi trong Mục 5.2. Lundberg (2023) đề xuất token healing để tránh nhiều vấn đề liên quan đến tokenization bằng cách loại bỏ vài token cuối cùng khỏi prompt và cho phép mô hình hoàn thành chúng; phương pháp này có kết nối với nghiên cứu về yêu cầu mô hình diễn đạt lại và phản hồi (Deng et al., 2023) và các thí nghiệm của chúng tôi về nhắc mô hình lặp lại với hướng tokenization ưa thích của nó (Mục 5). Nghiên cứu của chúng tôi xây dựng trên những tạo phẩm rải rác trong quá khứ này và cung cấp phân tích có hệ thống về các hiệu ứng phụ thuộc vào hướng tokenization đối với lý luận số trong các LLM tiên tiến.

Các nhiệm vụ số học như testbed cho lý luận số trong LLM Với sự quan tâm tăng lên trong việc đo lường các mô hình tiên tiến về lý luận toán học (Saxton et al., 2019; Cobbe et al., 2021; Lewkowycz et al., 2022; Hendrycks et al., 2021; Paster, 2023), một nhóm nghiên cứu đi kèm nghiên cứu các mô hình ngôn ngữ trong các thiết lập kiểm soát hơn, chẳng hạn như số học. Razeghi et al. (2022) sử dụng các nhiệm vụ số học để chỉ ra rằng tần suất thuật ngữ huấn luyện trước15 có thể ảnh hưởng đến lý luận số trong các mô hình GPT-J (Wang & Komatsuzaki, 2021) được huấn luyện trên tập dữ liệu Pile (Gao et al., 2020). Tương tự, McCoy et al. (2023) cho thấy rằng GPT-3.5 và GPT-4 tốt hơn trong việc tính toán các hàm tuyến tính phổ biến hơn trong dữ liệu huấn luyện (chẳng hạn như chuyển đổi Fahrenheit sang Celsius) so với các lựa chọn thay thế gần. Nghiên cứu khác (Nogueira et al., 2021; Muffo et al., 2022; Zhou et al., 2022; 2023) thay vào đó tập trung vào các sửa đổi khác nhau có thể giúp các mô hình khái quát hóa đến các nhiệm vụ số học dài hơn. Zhou et al. (2023) và Lee et al. (2023) đều chỉ ra rằng việc có các mô hình autoregressive thực hiện phép cộng theo thứ tự đảo ngược tạo ra một thuật toán đơn giản hơn để học và dẫn đến hiệu suất tốt hơn, điều này bổ sung cho sự nhấn mạnh của chúng tôi về tầm quan trọng của việc căn chỉnh tokenization "đảo ngược" (tức là từ phải sang trái). Zhou et al. (2022) cũng tiến hành phân tích lỗi sơ bộ về sai lầm của mô hình, mặc dù các prompt thuật toán của họ buộc các mô hình chia token thành chữ số (tương tự như Nye et al. (2021)). Nghiên cứu của chúng tôi nằm rộng rãi trong danh mục nghiên cứu này sử dụng các nhiệm vụ số học để nghiên cứu lý luận số; chúng tôi chọn tập trung vào các hiệu ứng phụ thuộc vào tokenization, và tìm thấy các mẫu lỗi nhất quán, định kiểu một cách đáng ngạc nhiên (Mục 4), thêm vào nhóm tài liệu phong phú này.

14Chúng tôi cũng xem xét các hiệu ứng tần suất bằng cách sử dụng thứ hạng hợp nhất BPE (vì chúng tôi không có quyền truy cập vào dữ liệu huấn luyện trước của các mô hình GPT-3.5 và GPT-4) như một xấp xỉ cho tần suất. Chúng tôi không tìm thấy hiệu ứng mạnh, kết quả mở rộng được cung cấp trong Phụ lục C.
15Chúng tôi cũng xem xét các hiệu ứng tần suất bằng cách sử dụng thứ hạng hợp nhất BPE (vì chúng tôi không có quyền truy cập vào dữ liệu huấn luyện trước của các mô hình GPT-3.5 và GPT-4) như một xấp xỉ cho tần suất. Chúng tôi không tìm thấy hiệu ứng mạnh, kết quả mở rộng được cung cấp trong Phụ lục C.

--- TRANG 7 ---
Tokenization có ý nghĩa: tác động của tokenization đối với tính toán số học trong các mô hình ngôn ngữ lớn tiên tiến

8. Thảo luận
Trong nghiên cứu này, chúng tôi phân tích các hiệu ứng phụ thuộc vào tokenization đối với lý luận số trong GPT-3.5 và GPT-4. Chúng tôi thấy rằng lựa chọn hard-coded của token số 1, 2 và 3 chữ số, được tokenize từ trái sang phải, tạo ra các mẫu lỗi định kiểu khi so sánh với tokenization từ phải sang trái. Chúng tôi đề xuất một giải pháp giảm thiểu, nơi mô hình được yêu cầu lặp lại câu trả lời trong định dạng tokenization ưa thích của nó. Cuối cùng, chúng tôi chỉ ra rằng hiệu ứng mạnh hơn trong các mô hình nhỏ hơn (chẳng hạn như GPT-3.5), nhấn mạnh tầm quan trọng của các thiên lệch quy nạp phụ thuộc vào tokenization trong thời đại mà nhiều nhà thực hành đang tập trung vào đưa khả năng vào các mô hình nhỏ hơn thông qua overtraining (De Vries, 2023; Touvron et al., 2023a) và chưng cất (Li et al., 2023; Gemini Team et al., 2023). Nhìn chung, chúng tôi tin rằng bằng chứng này mạnh mẽ gợi ý các thiên lệch quy nạp từ tokenization có thể ảnh hưởng đáng kể đến hiệu suất mô hình trên các nhiệm vụ lý luận số.

Các LLM tiên tiến hiện đại chủ yếu sử dụng token một chữ số (Bảng 1), với GPT-3.5 và GPT-4 là ngoại lệ chính trong việc sử dụng token lên đến 3 chữ số. Chúng tôi giả thuyết rằng lựa chọn sau có thể được thực hiện để đạt được tỷ lệ nén tốt hơn: các mô hình "thấy" nhiều dữ liệu số hơn cho cùng số token huấn luyện.16 Hơn nữa, lựa chọn này có thể có lợi ích cho khái quát hóa độ dài (Anil et al., 2022), như chúng tôi ám chỉ trong Mục 4.4. Tuy nhiên, chúng tôi cũng đã chứng minh cách sự không khớp giữa đầu vào và đầu ra khi sử dụng tokenization L2R (Mục 4.1) có thể dẫn đến sự giảm lớn về độ chính xác, đặc biệt trên các mô hình nhỏ hơn (GPT-3.5, GPT-4 Turbo). Sự không khớp như vậy sẽ không phải là vấn đề khi sử dụng token một chữ số.

Để tiến bộ về lựa chọn tokenization số nào tốt nhất để sử dụng (ví dụ: token một chữ số của LLaMa và PaLM, hoặc token lên đến 3 chữ số của GPT-3.5 và GPT-4), "thí nghiệm vàng" sẽ là huấn luyện cùng kiến trúc mô hình trên cùng tập dữ liệu, nhưng với các chiến lược tokenization số đa dạng. Ngoài chi phí của thí nghiệm này (làm cho nó không khả thi trong các thiết lập học thuật), một câu hỏi chính cũng trở thành cách "kiểm soát"-tính toán. Tỷ lệ nén tốt hơn của token lên đến 3 chữ số có nghĩa là một thí nghiệm được kiểm soát token sẽ dẫn đến một số mô hình "thấy" nhiều dữ liệu hơn. Chúng tôi hy vọng nghiên cứu của chúng tôi dẫn đến việc các nhà thực hành mô hình xem xét các ablation như vậy, với các kiểm soát thích hợp.

Ngoài khả năng áp dụng cho các nhà thực hành mô hình, nghiên cứu của chúng tôi cũng cung cấp một tập hợp hiện tượng phụ thuộc vào tokenization thú vị cho các nhà nghiên cứu khả năng diễn giải để khám phá. Nghiên cứu trước (Stolfo et al., 2023) đã sử dụng các kỹ thuật như path patching để xác định các mạch con trong LLM thực hiện các nhiệm vụ số học, nhưng bị hạn chế đối với các toán hạng token đơn. Xây dựng từ kết quả của chúng tôi, sẽ thú vị khi làm rõ các cơ chế đằng sau các mẫu lỗi có hệ thống, đặc biệt trong trường hợp các toán hạng đa token. Tính mạnh mẽ của lỗi "chữ số 4" trên GPT-3.5 chỉ ra một số cơ chế có hệ thống, có thể làm sáng tỏ các thuật toán cơ bản xuất hiện để thực hiện các nhiệm vụ số học.

Lời cảm ơn
Các tác giả muốn cảm ơn Andrew Saxe, Ted Moskovitz, Kira Düsterwald, Felix Hill, Xavier Garcia, Dan Roberts, và William Held vì các cuộc thảo luận sâu sắc và phản hồi về bản thảo. A.K.S. được tài trợ bởi Gatsby Charitable Foundation.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài được dịch tiếp theo...]

[Tiếp tục dịch các phần còn lại của tài liệu theo cấu trúc tương tự...]
