# MAGNET: Cải thiện Tính công bằng Đa ngôn ngữ của Mô hình Ngôn ngữ với Tokenization Thích ứng dựa trên Gradient

Orevaoghene Ahia1 Sachin Kumar2,3 Hila Gonen1 Valentin Hofmann2
Tomasz Limisiewicz4 Yulia Tsvetkov1 Noah A. Smith1,2
1University of Washington 2Allen Institute for AI 3The Ohio State University
4Charles University
oahia@cs.washington.edu

## Tóm tắt

Trong các môi trường đa ngôn ngữ, các hệ thống chữ viết không phải Latin và các ngôn ngữ ít tài nguyên thường bị thiệt thòi về tính hữu ích, hiệu quả và chi phí của các mô hình ngôn ngữ. Cụ thể, các nghiên cứu trước đây đã báo cáo nhiều thiên lệch trong mô hình hóa mà các thuật toán tokenization hiện tại tạo ra cho các ngôn ngữ có hệ thống chữ viết không phải Latin, vấn đề chính là phân đoạn quá mức. Trong nghiên cứu này, chúng tôi đề xuất MAGNET - tokenization thích ứng đa ngôn ngữ dựa trên gradient - để giảm phân đoạn quá mức thông qua tokenization subword thích ứng dựa trên gradient. MAGNET học để dự đoán các ranh giới phân đoạn giữa các byte token trong một chuỗi thông qua các mô-đun con bên trong mô hình, hoạt động như các bộ dự đoán ranh giới nội bộ (tokenizers). Các phương pháp tokenization dựa trên gradient trước đây nhằm mục đích nén đồng đều trên các chuỗi bằng cách tích hợp một bộ dự đoán ranh giới duy nhất trong quá trình huấn luyện và tối ưu hóa nó end-to-end thông qua tái tham số hóa ngẫu nhiên cùng với mục tiêu dự đoán token tiếp theo. Tuy nhiên, cách tiếp cận này vẫn dẫn đến phân đoạn quá mức cho các ngôn ngữ có hệ thống chữ viết không phải Latin trong các môi trường đa ngôn ngữ. Ngược lại, MAGNET cung cấp một kiến trúc có thể tùy chỉnh, trong đó các chuỗi ở mức byte được định tuyến qua các bộ dự đoán chuyên biệt cho từng ngôn ngữ-hệ thống chữ viết, mỗi bộ được tối ưu hóa cho hệ thống chữ viết tương ứng. Tính mô-đun này đảm bảo độ chi tiết phân đoạn công bằng trên các hệ thống chữ viết ngôn ngữ khác nhau so với các phương pháp trước đây. Thông qua các thí nghiệm mở rộng, chúng tôi chứng minh rằng ngoài việc giảm sự chênh lệch phân đoạn, MAGNET còn cho phép mô hình ngôn ngữ nhanh hơn và cải thiện tính hữu ích downstream.

## 1 Giới thiệu

Mặc dù sự phổ biến của các mô hình ngôn ngữ sinh (LMs) trong tiếng Anh, các đối tác không phải tiếng Anh của chúng còn xa mới được áp dụng rộng rãi. Trong khi các LMs đa ngôn ngữ cung cấp nhiều lợi thế như hiệu quả tài nguyên và khái quát hóa xuyên ngôn ngữ, sự chênh lệch hiệu suất giữa các ngôn ngữ vẫn là một thách thức đáng kể. Các nghiên cứu trước đây phần lớn quy các sự chênh lệch này cho sự mất cân bằng dữ liệu huấn luyện giữa các ngôn ngữ [43,34,29,24]. Tuy nhiên, nghiên cứu gần đây làm nổi bật rằng tokenization - cách văn bản đầu vào được phân đoạn - có thể làm giảm đáng kể không chỉ hiệu suất mô hình mà còn chi phí huấn luyện và suy luận do việc phân mảnh quá mức một số ngôn ngữ và hệ thống chữ viết [3,33]. Các thuật toán phân đoạn subword được sử dụng để xây dựng tokenizers LM [28,39,22,40] thường phân đoạn corpus huấn luyện chỉ dựa trên thống kê tần suất. Do sự mất cân bằng dữ liệu, chúng đạt được nén cao trong các ngôn ngữ có nhiều tài nguyên, trong khi phần lớn các ngôn ngữ bị phân mảnh quá mức. Vấn đề này ảnh hưởng không cân xứng đến các hệ thống chữ viết không phải Latin bao gồm các ngôn ngữ được nói bởi hàng tỷ người, không chỉ ít thường xuyên hơn trong các corpus như vậy, mà còn có thể cần đến 4× nhiều byte hơn để biểu diễn cùng một thông tin.

Để giải quyết những thách thức này, các nghiên cứu trước đây đã đề xuất xây dựng các mô hình không có tokenizer bằng cách huấn luyện trực tiếp trên các chuỗi ký tự hoặc byte [45,4]. Hoạt động trên các phân đoạn nhỏ hơn hoặc chi tiết hơn dẫn đến việc mô hình hóa tốn nhiều tính toán và bộ nhớ hơn đáng kể, do các chuỗi dài hơn nhiều. Để giảm bớt vấn đề này, nghiên cứu gần đây đã giới thiệu một "lớp" tokenization trong kiến trúc mô hình bằng cách gộp các biểu diễn ký tự liền kề có độ dài cố định hoặc động thành các tập hợp nhỏ hơn của các biểu diễn patch [30,9,41,31,14,15], dẫn đến các mô hình được tối ưu hóa với "tỷ lệ nén" chuỗi trong tâm trí. Mặc dù những cách này có thể cải thiện hiệu quả, chúng chủ yếu phù hợp cho việc mô hình hóa ở mức ký tự cho các hệ thống chữ viết có ký tự có thể được ánh xạ tới một codepoint Unicode duy nhất. Do số lượng codepoint rộng lớn trong Unicode, từ vựng dựa trên ký tự có thể cực kỳ lớn đối với các mô hình đa ngôn ngữ. Vì nhiều ký tự đó có thể không bao giờ xuất hiện trong quá trình huấn luyện, các vấn đề "ngoài từ vựng" tương tự như những gì trải nghiệm với các mô hình subword có thể phát sinh nếu chúng ta chỉ bao gồm các codepoint trong dữ liệu huấn luyện [28]. Khi mở rộng các phương pháp này để huấn luyện các mô hình đa ngôn ngữ ở mức byte, chúng tôi quan sát thấy sự chênh lệch trong tỷ lệ phân mảnh giữa các ngôn ngữ vẫn tồn tại. Ví dụ, văn bản tiếng Anh "Fellow wrestlers also paid tribute to Luna." và tương đương tiếng Telugu của nó, "తోటి మల్లయుద్ధకారులు కూడా లూనాకు నివాళులు అర్పించారు.", chứa 43 và 148 byte UTF-8, tương ứng. Một tỷ lệ nén cố định cho cả hai ngôn ngữ sẽ dẫn đến văn bản Telugu bị phân mảnh quá mức với yêu cầu gần 3× nhiều token hơn tiếng Anh.

Trong nghiên cứu này, chúng tôi đề xuất MAGNET (tokenization thích ứng đa ngôn ngữ dựa trên gradient) để giảm sự chênh lệch này trong các LMs đa ngôn ngữ không có tokenizer. Mục tiêu của chúng tôi là đạt được mô hình ngôn ngữ đa ngôn ngữ end-to-end với tokenization subword dựa trên gradient dẫn đến nén chuỗi cao và tương tự trên các ngôn ngữ với các hệ thống chữ viết khác nhau. Chúng tôi tận dụng hourglass transformers [30, 31] để định tuyến hiệu quả các chuỗi ở mức byte qua các bộ dự đoán ranh giới nội bộ chuyên biệt cho ngôn ngữ-hệ thống chữ viết được huấn luyện để suy luận ranh giới từ giữa các byte token trong một chuỗi. Các bộ dự đoán ranh giới này được huấn luyện end-to-end thông qua tái tham số hóa ngẫu nhiên [20,27]. Các ranh giới được suy luận sau đó được sử dụng để gộp các biểu diễn của các token liền kề trong cùng một phân đoạn, sau đó biểu diễn đã gộp (hẹp) được truyền vào phần còn lại của khối transformer. Không giống như các cách tiếp cận tokenization dựa trên gradient trước đây áp dụng cùng một tỷ lệ nén cho tất cả các ngôn ngữ trong dữ liệu pretraining bằng cách kết hợp một bộ dự đoán ranh giới duy nhất trong kiến trúc mô hình của chúng, MAGNET sử dụng tính mô-đun. Nó kết hợp nhiều bộ dự đoán chuyên biệt cho ngôn ngữ-hệ thống chữ viết để đạt được độ chi tiết phân đoạn công bằng trên các hệ thống chữ viết ngôn ngữ khác nhau. Chúng tôi kiểm tra hiệu quả của MAGNET về phân mảnh công bằng, hiệu quả mô hình và hiệu suất tác vụ downstream trên chín ngôn ngữ khác nhau về mặt loại hình học, so sánh với các mô hình ở mức byte không có nén và các mô hình tokenization dựa trên gradient hiện có [31]. Các thí nghiệm mở rộng của chúng tôi chứng minh rằng cách tiếp cận MAGNET của chúng tôi dẫn đến tokenization công bằng hơn khi so sánh với các subword-tokenizers, byte-level tokenizers và các mô hình tokenization dựa trên gradient trước đây. Điều này lần lượt dẫn đến mô hình hóa nhanh hơn và hiệu suất cạnh tranh trên các tác vụ downstream.

## 2 MAGNET: Tokenization Thích ứng Đa ngôn ngữ dựa trên GradieNt

Mục tiêu của chúng tôi là xây dựng một mô hình ngôn ngữ đa ngôn ngữ ở mức byte với phân đoạn công bằng trên các ngôn ngữ. Chúng tôi đề xuất đạt được điều này bằng cách dành một mô-đun riêng biệt trong mô hình cho mỗi hệ thống chữ viết, để phục vụ như một tokenizer nội bộ cho các ngôn ngữ sử dụng hệ thống chữ viết đó. Mô hình được đề xuất của chúng tôi, gọi là MAGNET, xây dựng trên hourglass transformers [30,31], một kiến trúc được giới thiệu để xử lý hiệu quả các chuỗi dài trong các mô hình không có tokenizer. Chúng tôi thực hiện một số sửa đổi đơn giản nhưng quan trọng cho kiến trúc này để đạt được phân đoạn công bằng trên các ngôn ngữ, trong khi duy trì chất lượng cao của việc mô hình hóa đa ngôn ngữ. Trong phần tiếp theo, chúng tôi giải thích các khái niệm chính của hourglass transformers, và sau đó giới thiệu các sửa đổi chúng tôi thực hiện để phù hợp với việc mô hình hóa đa ngôn ngữ công bằng.

### 2.1 Nền tảng: Hourglass Transformers

Hourglass transformer [31,30] là một kiến trúc phân cấp để xử lý hiệu quả các chuỗi dài. Kiến trúc có ba thành phần chính, mỗi thành phần bao gồm một hoặc nhiều lớp transformer: Một mô-đun con tokenization nhận đầu vào là một chuỗi byte và xuất ra một phân đoạn, một mô-đun con mô hình ngôn ngữ nhận đầu vào là các phân đoạn hoặc token được dự đoán và sau đó được huấn luyện để thực hiện dự đoán token tiếp theo, và một mô-đun upsampling nhận đầu vào là các biểu diễn ẩn của các phân đoạn và chuyển đổi chúng trở lại thành một chuỗi byte trên đó một mất mát mô hình ngôn ngữ điển hình có thể được áp dụng. Xem xét mô hình này như một hộp đen, nó vẫn thực hiện mô hình ngôn ngữ ở mức byte, tuy nhiên, nó yêu cầu ít tính toán hơn đáng kể nhờ mô-đun con tokenization.

**Tokenization dựa trên Gradient** Mô-đun con này thực hiện hai bước. Đầu tiên, chuỗi đầu vào đã cho x₁; : : : ; xN (trong đó mỗi xt là một byte trong trường hợp của chúng tôi) được mã hóa bằng một mạng transformer nhỏ (với causal attention) để tạo ra một chuỗi các vector ẩn h¹T; : : : ; hᵀN. Tiếp theo, một bộ dự đoán ranh giới nhận đầu vào là mỗi ht và dự đoán một giá trị vô hướng giữa 0 và 1, cho biết xác suất vị trí t là cuối của một phân đoạn. Nó được triển khai như

^bt = p(bt = 1) = σ(MLP(ht)); (1)

trong đó MLP cho biết một perceptron đa lớp và σ là hàm sigmoid. Để chuyển đổi các xác suất mềm thành các dự đoán phân đoạn cứng, một phân phối Bernoulli được lấy mẫu từ, được định nghĩa bởi ^bt. Vì hoạt động lấy mẫu sẽ làm cho quá trình không thể vi phân được, hard Gumbel-sigmoid được sử dụng, một tái tham số hóa ngẫu nhiên của phân phối Bernoulli, theo Nawrot et al. [31]:

bt = sigmoid[ (log^bt - u) / ((1-^bt)(1-u)) - 1 / τ ]; u ~ Uniform(0;1) (2)

trong đó τ là một siêu tham số. Vì mô-đun này có thể vi phân được, các phân đoạn được học trong quá trình huấn luyện của mô hình đầy đủ. Mô-đun này được gọi là "tokenization dựa trên gradient."

**Mô hình Ngôn ngữ** Cho một chuỗi các ranh giới phân đoạn bt ∈ {0;1} từ bộ dự đoán ranh giới, mô-đun con này đầu tiên gộp các trạng thái ẩn thuộc cùng một phân đoạn bằng cách tính trung bình chúng để tạo thành một chuỗi các biểu diễn h¹P; : : : ; hᵏP. Gọi t₁; : : : ; tₖ là các vị trí mà tại đó một ranh giới được lấy mẫu, tức là, đối với bất kỳ cặp liền kề nào tj; tj+1, chuỗi xtj+1 : : : xtj+1 tạo thành một "token" kết thúc tại vị trí tj+1. Biểu diễn đầu vào của "token" này được định nghĩa là hPj = 1/(tj+1-tj) Σ(t=tj+1 to tj+1) hTt. Các biểu diễn này sau đó được truyền qua khối giữa của các lớp transformer (với causal attention) để có được một chuỗi khác của các biểu diễn ẩn h¹M; : : : ; hᵏM. Từ góc độ của một mô hình ngôn ngữ dựa trên tokenization subword, mô-đun này tương đương với các khối transformer không có các lớp embedding đầu vào và đầu ra.

**Upsampling** Mô-đun này chuyển đổi hᴹl thành các xác suất trên một từ vựng byte. Điều này bao gồm, đầu tiên, upsampling đầu ra của khối giữa về độ phân giải ban đầu bằng cách nhân đôi theo sau bởi skip connection: hᵁt = hᴹl + hᵀt. Các vector này được truyền thêm qua một mạng transformer nhỏ theo sau bởi một lớp unembedding và một softmax để có được một phân phối xác suất trên đó mất mát mô hình ngôn ngữ (cross entropy) có thể được tính toán. Để ngăn bộ dự đoán ranh giới sụp đổ và dự đoán một cách tầm thường mỗi vị trí t như một ranh giới, Nawrot et al. [31] đề xuất thêm một regularizer vào mục tiêu LM: -log Binomial(λ;l; k) trong đó,

Binomial(λ;N; k) = (N choose k) λᵏ(1-λ)ᴺ⁻ᵏ; và k = Σᵢᴺbtᵢ: (3)

Ở đây λ ∈ [0;1] là một siêu tham số, k định nghĩa số lượng phân đoạn hoặc token được dự đoán. Theo trực giác, mất mát này thúc đẩy mô hình tìm một k gần với λl là mode của phân phối Binomial. Nói cách khác, λ cho phép kiểm soát tỷ lệ nén của chuỗi đầu vào thành khoảng 1/λ. Đặt nó là 1 sẽ dẫn đến mọi vị trí được dự đoán là ranh giới trong khi đặt nó là 0 sẽ khiến không có ranh giới nào được dự đoán.

### 2.2 Tokenization Thích ứng dựa trên Gradient thông qua Nhiều Bộ dự đoán Ranh giới

Để mã hóa cùng một thông tin, các ngôn ngữ khác nhau yêu cầu số lượng byte khác nhau, do hiệu quả vốn có khác nhau của chúng [3,25,33] cũng như các hạn chế được áp dụng bởi các ánh xạ Unicode, trong đó các ngôn ngữ không phải Latin (ví dụ, các ngôn ngữ Ấn Độ) có thể yêu cầu đến 4 byte cho mỗi ký tự. Trong các mô hình đa ngôn ngữ, đặt cùng một tỷ lệ nén (thông qua λ) cho tất cả các ngôn ngữ sẽ dẫn đến văn bản ở một số ngôn ngữ bị phân đoạn thành các chuỗi dài hơn nhiều, xem Phương trình (3). Sự chênh lệch này góp phần vào chi phí tính toán và bộ nhớ cao hơn cho các ngôn ngữ như vậy cũng như hiệu suất mô hình kém hơn cho các tác vụ downstream [3]. Vấn đề này song song với các subword tokenizer trong đó các ngôn ngữ có tỷ lệ phân đoạn cao hơn bị thiệt thòi do yêu cầu độ dài ngữ cảnh dài hơn để thực hiện cùng một tác vụ và hiệu suất học trong ngữ cảnh kém hơn vì cùng một độ dài ngữ cảnh phù hợp với ít trường hợp huấn luyện hơn so với các ngôn ngữ khác dẫn đến sự không công bằng [5, 3, 25, 33].

Để làm cho tokenization công bằng hơn, chúng tôi đề xuất MAGNET, học hiệu quả để phân đoạn các chuỗi trên các ngôn ngữ và hệ thống chữ viết ngôn ngữ với độ chi tiết tương tự. Như một phần của việc tạo ra phân đoạn công bằng, chúng tôi nhằm mục đích tối đa hóa hiệu quả nén chuỗi, mà không có tác động tiêu cực đến hiệu suất downstream trên các ngôn ngữ.

**Giới thiệu nhiều gradient-based tokenizers** Để đạt được điều này, chúng tôi đề xuất một sửa đổi đối với kiến trúc mô hình cho phép xử lý nhiều hệ thống chữ viết ngôn ngữ. Mỗi hệ thống chữ viết có bộ dự đoán ranh giới riêng được huấn luyện với các prior Binomial riêng biệt được xác định dựa trên mã hóa Unicode của hệ thống chữ viết và cũng được điều chỉnh theo tỷ lệ nén mong muốn. Điều này cho phép chúng tôi đạt được tỷ lệ phân mảnh tương tự trên các ngôn ngữ, do các biến thể trong nén. Chuỗi đầu vào được gắn thẻ với hệ thống chữ viết của nó và chúng tôi suy luận phân đoạn bằng cách định tuyến nó qua bộ dự đoán ranh giới thích hợp. Phần còn lại của kiến trúc mô hình vẫn giữ nguyên.

**Xác định λ cho tokenization công bằng** Chúng tôi sử dụng các prior binomial λ cho mỗi bộ dự đoán ranh giới để kiểm soát tỷ lệ của các phân đoạn kết quả cho các hệ thống chữ viết khác nhau. Vì chúng tôi muốn áp dụng độ dài công bằng trên các ngôn ngữ, chúng tôi đặt các λ khác nhau theo quy trình sau. Đầu tiên, chúng tôi chọn một ngôn ngữ gốc L cho mỗi hệ thống chữ viết trong corpus huấn luyện của chúng tôi và định nghĩa một đại lượng tỷ lệ byte-to-word R cho hệ thống chữ viết này như sau. Gọi X = {x₁; : : : ;xD} là một mẫu của các chuỗi văn bản trong ngôn ngữ L từ corpus huấn luyện của chúng tôi với |xi| biểu thị độ dài byte và countwords(xi) số lượng từ trong chuỗi xi. Chúng tôi định nghĩa tỷ lệ byte-to-word trung bình R trên X là:

R = 1/D ΣD(i=1) |xi|/countwords(xi) (4)

Sau đó chúng tôi đặt prior λS cho hệ thống chữ viết tương ứng S là 1/R. Mục tiêu huấn luyện cuối cùng của chúng tôi trên một trường hợp đơn lẻ x là như sau:

ΣN(i=1) log p(xi|x<i) - ΣS I(script(x) = S) log Binomial(λS;N; k)

trong đó I là hàm chỉ số và script(·) là một hàm gán một hệ thống chữ viết cho một chuỗi byte x, việc gán như vậy có thể dễ dàng thu được dựa trên các định nghĩa codepoint trong Unicode.

## 3 Thiết lập Thí nghiệm

### 3.1 Mô hình Ngôn ngữ

**Dữ liệu Pretraining** Chúng tôi pretrain tất cả các mô hình trên chín ngôn ngữ (tiếng Anh, Tây Ban Nha, Pháp, Nga, Ukraine, Belarus, Telugu, Bengali và Hindi) được chia thành ba nhóm, được viết bằng các hệ thống chữ viết riêng biệt: hệ thống chữ viết Latin, Cyrillic và Indic (Brahmic). Lựa chọn của chúng tôi dựa trên sự đa dạng ngôn ngữ học của các ngôn ngữ này và tính sẵn có của dữ liệu để đánh giá downstream. Dữ liệu pretraining của chúng tôi được lấy từ bộ dữ liệu OSCAR [32]. Chúng tôi trình bày thống kê cho mỗi ngôn ngữ trong Phụ lục C.

**Baselines** Chúng tôi so sánh MAGNET với Dynamic Token Pooling [31], suy luận ranh giới với một prior binomial cố định cho mọi chuỗi, bất kể hệ thống chữ viết ngôn ngữ. Mô hình này được gọi là DTP trong phần còn lại của bài báo. DTP có một bộ dự đoán ranh giới duy nhất; chúng tôi huấn luyện hai phiên bản của baseline này với prior binomial là 0.2 và 0.1 tương ứng tạo ra nén 5× và 10× tương ứng. Chúng tôi cũng so sánh với một mô hình ngôn ngữ decoder ở mức byte. Để đảm bảo so sánh mô hình công bằng, mô hình này có kiến trúc tương tự như DTP, nhưng không có bất kỳ nén chuỗi nào.

**Cấu hình MAGNET** Chúng tôi tính toán các tỷ lệ byte-word, chọn tiếng Anh, Nga và Telugu làm ngôn ngữ gốc cho mỗi hệ thống chữ viết ngôn ngữ, dựa trên các khám phá ban đầu. Bộ dữ liệu FLORES [16] được sử dụng cho mục đích này và các tỷ lệ kết quả là khoảng 5×, 10×, và 20× cho tiếng Anh, Nga, và Telugu, tương ứng. Dựa trên các tỷ lệ này, chúng tôi huấn luyện năm mô hình MAGNET với các kết hợp prior binomial khác nhau duy trì tỷ lệ nhưng điều chỉnh các multiplier. Đầu tiên, để tối ưu hóa cho phân đoạn ranh giới ở mức từ, chúng tôi sử dụng cấu hình tỷ lệ byte-to-word ban đầu, tức là nén 5× cho Latin, nén 10× cho Cyrillic và nén 20× cho các ngôn ngữ Indic trong cùng một mô hình. Cấu hình thứ hai; (1;2;4) là tỷ lệ bytes-to-character trung bình cho tiếng Anh, Nga và Telugu. Do đó, sử dụng cấu hình này tối ưu hóa cho mô hình hóa byte công bằng với độ chi tiết ở mức ký tự. Cấu hình thứ ba (3;6;12) dựa trên giả thuyết rằng nó sẽ dẫn đến ranh giới phân đoạn subword công bằng. Cuối cùng, vì chúng tôi áp dụng nén rất cao trên các ngôn ngữ Indic, chúng tôi kiểm tra thực nghiệm hai cấu hình bổ sung (5;10;13) và (5;10;15) với tỷ lệ nén giảm cho các ngôn ngữ Indic.

**Huấn luyện Subword Tokenizer** Để so sánh phân đoạn được suy ra từ MAGNET với các subword tokenizer truyền thống, chúng tôi tạo ra các từ vựng byte pair encoding (BPE) ở mức byte chứa 50K, 100K và 250K đơn vị subword trên dữ liệu pretraining của chúng tôi. Chúng tôi sử dụng α-sampling để huấn luyện các tokenizer, thường được sử dụng để cải thiện biểu diễn của các ngôn ngữ ít tài nguyên [10]. Tức là, chúng tôi lấy mẫu tài liệu cho mỗi ngôn ngữ theo một phân phối đa thức với các xác suất {qi}i=1...N, trong đó: qi = pi^α / ΣN(j=1) pj^α với pi = ni / ΣN(k=1) nk. Điều này tăng token cho các ngôn ngữ ít tài nguyên và đã được chỉ ra là giảm thiên lệch đối với các ngôn ngữ có nhiều tài nguyên. Chúng tôi xem xét α = 0.5; 0.3 phù hợp với [10, 12].

**Bộ dữ liệu Downstream** Để chứng minh hiệu quả của MAGNET, chúng tôi đánh giá bằng cách fine-tuning các mô hình đã huấn luyện của chúng tôi trên một số tác vụ hỏi đáp và phân loại. Cụ thể, chúng tôi đánh giá trên XQuAD (hỏi đáp) [6], XNLI (suy luận ngôn ngữ tự nhiên) [11], PAWS-X (phát hiện paraphrase) [46] từ XTREME [19], và SIB 200 (phân loại chủ đề) [2]. Chúng tôi cung cấp phạm vi ngôn ngữ chi tiết trên tất cả các tác vụ trong Bảng 5. Ngoài ra, để kiểm tra khả năng thích ứng của MAGNET, chúng tôi đánh giá trên các tác vụ thổ ngữ, cụ thể là ILI [48] tác vụ chia sẻ Nhận dạng Ngôn ngữ Indo-Aryan (ILI) và HaSCoSVa-2022 [7], phát hiện hate speech trên các thổ ngữ Tây Ban Nha. Chúng tôi cung cấp chi tiết fine-tuning trong Phụ lục D.

### 3.2 Phân tích phân đoạn trên các mô hình

Mục tiêu của phân tích này là so sánh độ chi tiết phân đoạn trên các cách tiếp cận khác nhau. Tức là, chúng tôi đo lường liệu cùng một lượng thông tin có được truyền đạt thông qua số lượng token tương tự trên các ngôn ngữ khác nhau hay không. Theo nghiên cứu trước đây [33,3], chúng tôi tiến hành phân tích này với corpus song song FLORES-200 [16] tập trung vào chín ngôn ngữ trong dữ liệu pretraining của chúng tôi.

Đối với các mô hình ở mức byte, phân đoạn được thực hiện bằng cách chuyển đổi mỗi câu thành byte UTF-8 thô và tính toán số lượng byte trung bình mỗi câu. Với subword tokenizer, mỗi câu được phân đoạn bằng tokenizer chúng tôi đã huấn luyện trong 3.1, và số lượng token kết quả trung bình được tính toán trên tất cả các câu. Đối với các phương pháp dựa trên gradient như DTP và MAGNET của chúng tôi, chúng tôi đưa mỗi câu vào mô hình và lấy một chuỗi các dự đoán ranh giới từ các lớp bộ dự đoán ranh giới. Số lượng dự đoán tích cực xác định số lượng token mỗi câu.

## 4 Kết quả

Mục tiêu của MAGNET là học phân đoạn công bằng trong quá trình huấn luyện trong khi duy trì hiệu suất downstream chất lượng cao. Lý tưởng nhất, chúng tôi mong đợi rằng MAGNET dẫn đến nén cao hơn cho các ngôn ngữ có hệ thống chữ viết không phải Latin, do đó cân bằng độ chi tiết phân đoạn trên tất cả các ngôn ngữ. Điều này, lần lượt, nên cải thiện hiệu quả mô hình hóa bằng cách giảm chi phí tính toán tại thời điểm huấn luyện và suy luận.

### 4.1 MAGNET dẫn đến phân đoạn công bằng trên các hệ thống chữ viết ngôn ngữ.

Chúng tôi phân tích độ chi tiết phân đoạn, đối chiếu phương pháp của chúng tôi với phân đoạn ở mức byte, tokenization subword, và DTP như được mô tả trong §3.2. Kết quả của chúng tôi trong Hình 2 cho thấy các mô hình MAGNET tạo ra tỷ lệ phân đoạn tương tự cho tất cả các ngôn ngữ. Sự cải thiện đặc biệt đáng chú ý ở các ngôn ngữ có hệ thống chữ viết không phải Latin dễ bị phân đoạn quá mức nhất với các baseline.

Đầu tiên, chúng tôi so sánh phân đoạn ở mức byte với MAGNET với cấu hình phân đoạn (1;2;4). Như được mô tả trong §3.1, các ngôn ngữ Indic có khoảng bốn byte code-point đối với một ký tự, các ngôn ngữ Cyrillic có khoảng hai, và nhiều ngôn ngữ Latin có ánh xạ một-đối-một. Do đó, chúng tôi mong đợi rằng huấn luyện MAGNET với cấu hình (1;2;4) sẽ dẫn đến mô hình hóa ở mức byte công bằng trên tất cả các ngôn ngữ này. Hình 7a trong Phụ lục cho thấy MAGNET (1;2;4) dẫn đến giảm 3× trong số lượng token trung bình cho các ngôn ngữ Indic, và gần 2× giảm cho Cyrillic, trong khi các ngôn ngữ Latin không bị ảnh hưởng. Tiếp theo, chúng tôi so sánh phân đoạn giữa DTP 5×, MAGNET tại (5;10;13), MAGNET tại (5;10;20) và các subword-tokenizer tại các kích thước từ vựng 50k, 100k và 250k có và không có alpha sampling (xem §3.1). Chúng tôi thấy rằng các mô hình MAGNET dẫn đến phân đoạn công bằng nhất trên tất cả các ngôn ngữ. Thực tế, chúng tôi đo được giảm gần 5× trong số lượng token trung bình cho các ngôn ngữ Indic so với DTP và các subword tokenizer. Đáng chú ý, chúng tôi cũng thấy rằng một subword tokenizer với kích thước từ vựng lớn là cần thiết để đạt được tỷ lệ phân đoạn thấp hơn trên các ngôn ngữ Cyrillic, trong khi đối với các ngôn ngữ Indic, ngay cả với từ vựng lớn và sampling, chúng tôi quan sát thấy sự chênh lệch rõ rệt. Điều này tương phản với các phát hiện từ nghiên cứu trước đây rằng sampling giảm thiểu sự chênh lệch tokenization [10]. Nhìn chung, các kết quả này gợi ý rằng MAGNET học phân đoạn công bằng trên các ngôn ngữ với các hệ thống chữ viết đa dạng, trong khi các mô hình phân đoạn cố định như DTP và các subword tokenizer dựa trên byte là không tối ưu và rất có thể dẫn đến phân đoạn quá mức.

### 4.2 MAGNET duy trì hiệu suất trên các tác vụ downstream.

Mục tiêu của chúng tôi là thực thi phân đoạn công bằng trong khi duy trì hiệu suất mô hình trên các tác vụ. Trong Bảng 2, chúng tôi trình bày kết quả cho mô hình MAGNET hoạt động tốt nhất so với các mô hình DTP và mô hình ở mức byte (chúng tôi cung cấp so sánh với tất cả các mô hình MAGNET trong §5.1). Nhìn chung, chúng tôi thấy rằng các mô hình MAGNET hoạt động tốt hơn DTP nhưng cạnh tranh với các mô hình ở mức byte trong khi nhanh hơn đáng kể và yêu cầu ít tính toán hơn. MAGNET (3;6;12) hoạt động tốt nhất trên PAWS-X và SIB, trong khi (1;2;4) hoạt động tốt nhất trên XNLI và XQUAD.

Chúng tôi báo cáo kết quả theo từng ngôn ngữ trên bộ dữ liệu XNLI trong Hình 3. Chúng tôi thấy kết quả tốt hơn với các mô hình MAGNET ngay cả trong một số trường hợp mà các mô hình so sánh tối ưu hóa cho phân đoạn tương tự. Ví dụ, trên tiếng Tây Ban Nha, MAGNET tại (5;10;15) vượt trội hơn DTP tại 5×. Trên các ngôn ngữ Indic, các mô hình MAGNET thường cạnh tranh với các mô hình DTP. Chúng tôi cung cấp thêm phân tích về sự đánh đổi giữa hiệu suất downstream và phân đoạn trong §5.1. Kết quả trên các tác vụ thổ ngữ được báo cáo trong §2. Chúng tôi thấy kết quả cạnh tranh trên tất cả các mô hình trên tất cả các tác vụ, gợi ý rằng cũng không có tác động tiêu cực do việc thích ứng các mô hình với các thổ ngữ tương ứng của chúng.

### 4.3 MAGNET dẫn đến các mô hình hiệu quả hơn.

So sánh thời gian suy luận trên tất cả các mô hình, chúng tôi mong đợi rằng các mô hình tối ưu hóa cho nén cố định như DTP sẽ chỉ hiệu quả cho các ngôn ngữ Latin vì tỷ lệ byte-to-word thấp hơn của chúng. Do đó, chúng tôi dự đoán rằng chiến lược định tuyến của chúng tôi với các mô hình MAGNET sẽ dẫn đến lợi ích hiệu quả cho các ngôn ngữ có hệ thống chữ viết không phải Latin. Trong Hình 4, chúng tôi vẽ biểu đồ thời gian suy luận mỗi ngôn ngữ trong XQUAD, tương đối với thời gian suy luận của các mô hình ở mức byte. Chúng tôi cho thấy MAGNET có thời gian suy luận ngắn hơn các mô hình ở mức byte, tương đương với DTP cho tiếng Anh và Nga và hơi thấp hơn cho Hindi và Nga. Nếu chúng tôi giả định tỷ lệ nén tối ưu cho tiếng Anh và Nga lần lượt là 5× và 10×, việc sử dụng mô hình DTP với tỷ lệ nén cố định cho cả hai ngôn ngữ yêu cầu huấn luyện hai mô hình đơn ngôn ngữ riêng biệt để có được tỷ lệ nén lý tưởng cho mỗi ngôn ngữ. Tuy nhiên, huấn luyện một mô hình MAGNET (5;10;20) duy nhất động đạt được tỷ lệ nén 5× cho tiếng Anh và tỷ lệ nén 10× cho Nga dẫn đến thời gian suy luận thấp hơn cho cả hai.

## 5 Phân tích và Thảo luận

### 5.1 Sự đánh đổi giữa hiệu suất downstream và phân đoạn công bằng

Các nghiên cứu trước đây đã báo cáo mối tương quan giữa nén và hiệu suất mô hình [14]. Chúng tôi nghiên cứu thực nghiệm các sự đánh đổi này bằng cách so sánh hiệu suất downstream trên các cấu hình MAGNET khác nhau được định nghĩa trong Bảng 1. Trong các kết quả được báo cáo trong Bảng 3, chúng tôi thấy rằng các cấu hình hoạt động tốt nhất là MAGNET (1;2;4) và MAGNET (3;6;12). Các cấu hình này tương đương với mô hình hóa công bằng ở mức byte và subword trên tất cả các ngôn ngữ. Chúng tôi cũng báo cáo hiệu suất tác vụ trung bình mỗi hệ thống chữ viết tại các tỷ lệ nén khác nhau trong Hình 5. Ở đây chúng tôi không tìm cách so sánh hiệu suất trên các hệ thống chữ viết ngôn ngữ, mà đánh giá hiệu suất trên các tỷ lệ nén khác nhau trong mỗi hệ thống chữ viết ngôn ngữ. Kết quả của chúng tôi cho thấy có ít hoặc không có sự giảm hiệu suất cho các ngôn ngữ Latin và Cyrillic khi nén tăng một cách vừa phải. Tuy nhiên, đối với các ngôn ngữ Indic, chúng tôi thấy trung bình giảm 5% hiệu suất khi nén tăng.

### 5.2 Độ chi tiết của phân đoạn trên các tỷ lệ nén khác nhau là gì?

Trong §3.1, chúng tôi làm nổi bật rằng prior binomial là thiết yếu để xác định độ chi tiết của phân đoạn được suy ra từ bộ dự đoán ranh giới. Để xác thực nội tại rằng MAGNET thực sự học phân đoạn có độ dài tương tự trên các ngôn ngữ khác nhau, chúng tôi phân tích thủ công các ví dụ từ corpus SIB, so sánh chúng với DTP tại 5× và 10×. Như được hiển thị trong Bảng 4, DTP tại 5× tạo ra phân đoạn ở mức từ cho tất cả các ngôn ngữ Latin trong khi tạo ra phân đoạn ở mức subword cho các ngôn ngữ Cyrillic và Indic. Tại 10×, chúng tôi thấy phân đoạn ở mức từ cho các ngôn ngữ Cyrillic, phân đoạn ở mức cụm từ cho các ngôn ngữ Latin và sự kết hợp của phân đoạn subword và ở mức từ trên các ngôn ngữ Indic. Để đạt được phân đoạn ở mức từ cho tất cả các ngôn ngữ, DTP yêu cầu huấn luyện ba mô hình riêng biệt. Tuy nhiên, MAGNET giảm bớt yêu cầu này bằng cách tạo ra độ chi tiết phân đoạn tương tự trên tất cả các ngôn ngữ. Để so sánh, độ chi tiết phân đoạn của BPE tokenizer là cực kỳ không tối ưu cho các ngôn ngữ Indic như được hiển thị trong Bảng 4 của Phụ lục. Trong khi BPE tokenizer tạo ra phân đoạn ở mức từ cho các ngôn ngữ Latin và Cyrillic, nó tạo ra phân đoạn ở mức ký tự cho các ngôn ngữ Indic. MAGNET, mặt khác, tìm ra sự cân bằng tốt của độ chi tiết phân đoạn trên các ngôn ngữ.

### 5.3 Phân đoạn có thay đổi đáng kể sau fine-tuning không?

Chúng tôi nghiên cứu các hiệu ứng của fine-tuning đối với các ranh giới phân đoạn kết quả trên các tác vụ downstream khác nhau. Về cơ bản, chúng tôi kiểm tra cách dự đoán ranh giới thay đổi sau fine-tuning cho mỗi tác vụ downstream. Chúng tôi thấy rằng không có sự khác biệt trong phân đoạn trước và sau fine-tuning mặc dù cập nhật các tham số của các bộ dự đoán ranh giới. Mặc dù có một vài trường hợp mà phân đoạn của mô hình đã fine-tuned khác với mô hình đã pretrained, không có bằng chứng rõ ràng về việc phân đoạn thay đổi mạnh mẽ sau fine-tuning. Trong Bảng 7 trong Phụ lục, chúng tôi trình bày hai ví dụ từ bộ dữ liệu SIB nơi có sự thay đổi nhẹ trong phân đoạn. Chúng tôi không tìm thấy dấu hiệu nào cho thấy bất kỳ thay đổi quan sát được nào góp phần hoặc làm tổn hại đến hiệu suất tác vụ.

## 6 Nghiên cứu Liên quan

**Vượt qua sự chênh lệch phân đoạn trong tokenization subword** Trong các môi trường đa ngôn ngữ, các subword tokenizer đã được chứng minh là dễ bị phân đoạn quá mức, do bản chất dựa trên dữ liệu của thuật toán BPE [39]. Nghiên cứu trước đây [13,12,44] đã cố gắng giải quyết các vấn đề mất cân bằng dữ liệu trong các subword tokenizer bằng cách over-sampling các ngôn ngữ ít tài nguyên. Nghiên cứu của chúng tôi cho thấy điều này chỉ giảm thiểu thiên lệch trên một số hệ thống chữ viết và không giải quyết vấn đề. Các nghiên cứu khác [1,36,17] cũng đã chỉ ra rằng tokenization trong transformers vẫn thiên lệch có lợi cho các ngôn ngữ có nhiều tài nguyên. Wang et al. [42] thực thi các mô hình sử dụng các đơn vị subword nhỏ hơn trong các ngôn ngữ có nhiều tài nguyên để làm cho phân đoạn công bằng hơn. Một số nghiên cứu [23,8] đề xuất huấn luyện các tokenizer đa ngôn ngữ trên các cụm ngôn ngữ để giảm thiểu sự chênh lệch phân đoạn, tuy nhiên, điều này dẫn đến từ vựng mở rộng. Mặc dù những nỗ lực này, rõ ràng là các mục tiêu huấn luyện của các subword tokenizer không căn chỉnh hiệu quả với những mục tiêu của mô hình ngôn ngữ.

**Các mô hình ngôn ngữ không có tokenizer** Mô hình ngôn ngữ trên byte [45,4] và pixel [37,38,26] đã trở nên mong muốn, vì nó loại bỏ các pipeline tiền xử lý phức tạp trong mô hình hóa. Xue et al. [45] đã giới thiệu ByT5, một biến thể không có tokenizer của T5 [35] xử lý văn bản ở mức byte. Tuy nhiên, mã hóa ở mức byte phân mảnh quá mức các ngôn ngữ có hệ thống chữ viết không phải Latin dẫn đến các chuỗi quá dài. Vì các chuỗi byte hoặc ký tự thường dẫn đến các chuỗi dài hơn, nghiên cứu trước đây [30,9,41,47,31,15] về các LMs không có tokenizer đã giới thiệu các kiến trúc mô hình mới để giảm thiểu chi phí tính toán của việc xử lý văn bản ký tự hoặc byte thô trực tiếp. Các phương pháp này [9,41,47,30] kết thúc bằng việc phân đoạn các chuỗi thô thành các patch có kích thước cố định/động, không phù hợp cho mô hình hóa trên các hệ thống chữ viết không phải Latin.

## 7 Kết luận

Trong nghiên cứu này, chúng tôi giới thiệu MAGNET, một phương pháp tokenization dựa trên gradient để học phân đoạn công bằng trên các hệ thống chữ viết ngôn ngữ trong các mô hình đa ngôn ngữ ở mức byte. MAGNET động định tuyến các chuỗi ở mức byte qua các bộ dự đoán ranh giới nội bộ chuyên biệt cho ngôn ngữ-hệ thống chữ viết được huấn luyện để suy luận ranh giới từ thông qua tái tham số hóa ngẫu nhiên. Chúng tôi cho thấy MAGNET cho phép chúng tôi học các biểu diễn token với cùng độ chi tiết trên các ngôn ngữ so với các mô hình ở mức byte vanilla và các cách tiếp cận tokenization dựa trên gradient trước đây. Phân tích của chúng tôi chứng minh rằng mặc dù thực sự có sự đánh đổi hiệu suất downstream do MAGNET tạo ra nén cao trên các ngôn ngữ có hệ thống chữ viết không phải Latin, chúng tôi vẫn có thể duy trì chất lượng hiệu suất downstream. Nhìn chung, kết quả của chúng tôi hứa hẹn cho nghiên cứu tương lai về phân đoạn công bằng và xử lý văn bản nói chung.
