# 2402.01613.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/2402.01613.pdf
# Kích thước tệp: 310981 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2402.01613v2  [cs.CL]  3 Feb 2025Xuất bản trong Transactions on Machine Learning Research (02 /2025)
Nomic Embed: Huấn luyện một Bộ nhúng văn bản ngữ cảnh dài có thể tái tạo
Zach Nussbaum zach@nomic.ai
Nomic AI
John X. Morris jack@nomic.ai, jxm3@cornell.edu
Nomic AI, Cornell University
Brandon Duderstadt brandon@nomic.ai
Nomic AI
Andriy Mulyar andriy@nomic.ai
Nomic AI
Đánh giá trên OpenReview: https: // openreview. net/ forum? id= IPmzyQSiQE
Tóm tắt
Báo cáo kỹ thuật này mô tả việc huấn luyện nomic-embed-text-v1, mô hình nhúng văn bản tiếng Anh đầu tiên hoàn toàn có thể tái tạo, mã nguồn mở, trọng số mở, dữ liệu mở, độ dài ngữ cảnh 8192 vượt trội hơn cả OpenAI Ada-002 và OpenAI text-embedding-3-small trên benchmark MTEB ngữ cảnh ngắn và benchmark LoCo ngữ cảnh dài. Chúng tôi phát hành mã huấn luyện và trọng số mô hình theo giấy phép Apache 2.0. Khác với các mô hình mã nguồn mở khác, chúng tôi phát hành toàn bộ dữ liệu huấn luyện được tuyển chọn và mã cho phép tái tạo hoàn toàn nomic-embed-text-v1. Bạn có thể tìm mã và dữ liệu để tái tạo mô hình tại https://github.com/nomic-ai/contrastors.

1 Giới thiệu
Nhúng văn bản là một thành phần không thể thiếu của các ứng dụng NLP hiện đại hỗ trợ tăng cường truy xuất cho LLM và tìm kiếm ngữ nghĩa (Lewis et al., 2021a; Izacard et al., 2022b; Ram et al., 2023). Những nhúng này mã hóa thông tin ngữ nghĩa về câu thành các vector có chiều thấp được sử dụng trong các ứng dụng xuôi dòng, như phân cụm để trực quan hóa dữ liệu, phân loại và truy xuất thông tin.

Đa số các mô hình mã nguồn mở hàng đầu trên benchmark MTEB (Muennighoff et al., 2023) bị giới hạn ở độ dài ngữ cảnh 512, như E5 (Wang et al., 2022), GTE (Li et al., 2023), và BGE (Xiao et al., 2023). Độ dài ngữ cảnh ngắn này làm giảm tiện ích của mô hình trong các lĩnh vực mà ngữ nghĩa tài liệu tổng thể không được định vị trong câu hoặc đoạn văn. Hầu hết các mô hình nhúng hàng đầu có độ dài ngữ cảnh dài hơn 2048 đều là mã nguồn đóng, như Voyage-lite-01-instruct (Voyage, 2023) và text-embedding-ada-002 (Neelakantan et al., 2022).

Tính đến tháng 10 năm 2024, các mô hình nhúng ngữ cảnh dài mã nguồn mở hiệu suất cao nhất là jina-embedding-v2-base-en (Günther et al., 2024) và E5-Mistral-7b-instruct (Wang et al., 2023b). Thật không may, jina-embedding-v2-base không vượt qua OpenAI's text-embedding-ada-002 (Neelakantan et al., 2022) (xem Bảng 1). Hơn nữa, E5-Mistral (Wang et al., 2023b) không khả thi để sử dụng trong nhiều ứng dụng kỹ thuật do yêu cầu suy luận lớn của một transformer 7 tỷ tham số, và không hoạt động tốt trên 4096 token.

Trong bài báo này, chúng tôi trình bày một pipeline huấn luyện đầu cuối đến cuối cho một mô hình nhúng văn bản ngữ cảnh dài hiện đại chỉ với 137 triệu tham số. nomic-embed-text-v1 vượt trội hơn OpenAI text-embedding-ada và

--- TRANG 2 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)
50 55 60 65 70 75 80 85MTEB
LoCo
JinaLC60.99
52.7
55.2562.26
82.4
58.260.39
85.45
51.962.39
85.53
54.16Nomic Embed
Jina Base V2
text-embedding-3-small
text-embedding-ada

Hình 1: Đánh giá Mô hình Nhúng Văn bản. Hiệu suất tổng hợp của nomic-embed-text-v1, OpenAI text-embedding-ada, OpenAI text-embedding-3-small và jina-embedding-base-v2 trên cả benchmark ngữ cảnh ngắn và dài. nomic embed là mô hình ngữ cảnh dài hoàn toàn có thể kiểm toán duy nhất vượt qua OpenAI text-embedding-ada và OpenAI text-embedding-3-small trên MTEB và LoCo. nomic embed hoạt động tương tự hoặc vượt trội hơn Jina Base V2 trên tất cả các tác vụ. Đơn vị trục X khác nhau theo từng bộ benchmark.

text-embedding-3-small trên benchmark ngữ cảnh ngắn (MTEB) và ngữ cảnh dài (LoCo) (Bảng 1).

Hơn nữa, chúng tôi là những người đầu tiên phát hành tất cả các tạo phẩm huấn luyện cần thiết để huấn luyện một mô hình nhúng văn bản hiệu suất cao. Chúng tôi phát hành trọng số mô hình, mã huấn luyện và dữ liệu huấn luyện để cho phép khả năng kiểm toán và tái tạo đầu cuối đến cuối của mô hình.

2 Công trình liên quan
Các mô hình nhúng văn bản truyền thống đã được huấn luyện với độ dài chuỗi nhỏ hơn hoặc bằng 512 token. Gần đây, Günther et al. (2024) đã huấn luyện một mô hình nhúng văn bản ngữ cảnh dài, jina-embeddings-base-v2, nhưng hoạt động kém hơn các mô hình nhúng văn bản mã nguồn đóng như text-embedding-ada-002 trên cả benchmark MTEB cũng như Jina Long Context Benchmark. Ngoài ra, jina-embeddings-base-v2 hoạt động kém hơn các mô hình nhúng văn bản ngữ cảnh ngắn trọng số mở khác như E5 (Wang et al., 2022), GTE (Li et al., 2023), và BGE (Xiao et al., 2023).

Hơn nữa, thiếu minh bạch trong pipeline huấn luyện cho các mô hình nhúng văn bản trọng số mở hiệu suất cao. Nhiều mô hình được phát hành này bỏ qua các chi tiết quan trọng như nguồn dữ liệu, kỹ thuật tuyển chọn dữ liệu và mã huấn luyện. Wang et al. (2022) đã phác thảo quy trình lọc dữ liệu của họ cho E5 bao gồm việc đầu tiên huấn luyện một mô hình trên một tập dữ liệu nhiễu lớn và sau đó sử dụng mô hình kết quả để lọc các cặp văn bản chất lượng thấp. Tuy nhiên, họ không phát hành chi tiết về mô hình được sử dụng để lọc tính nhất quán, cách nó được huấn luyện, hoặc dữ liệu nào được sử dụng. Họ cũng không phát hành bất kỳ mã huấn luyện hoặc dữ liệu nào cho mô hình nhúng được phát hành. Tương tự, Li et al. (2023) và Günther et al. (2024) không nêu chi tiết các nguồn dữ liệu cho việc tiền huấn luyện đối phó, bỏ qua chi tiết về cách tiếp cận lọc và khai thác dữ liệu, và không phát hành mã huấn luyện.

Ngoài ra, ít chi tiết được phát hành về cách các mô hình nhúng văn bản mã nguồn đóng được huấn luyện như Voyage-lite-01-instruct (Voyage, 2023) và OpenAI's text-embedding-ada-002 và text-embedding-3 (Neelakantan et al., 2022).

--- TRANG 3 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

3 Kiến thức nền tảng
Các mô hình nhúng văn bản hiện đại thường được huấn luyện trong ba giai đoạn: mô hình hóa ngôn ngữ có mặt nạ (Devlin et al., 2019), tiền huấn luyện đối phó có giám sát yếu, và tinh chỉnh đối phó (Wang et al., 2022; Li et al., 2023; Günther et al., 2023; 2024). Truyền thống, tinh chỉnh liên quan đến việc tận dụng các tập dữ liệu có nhãn như MSMarco và SNLI (Bowman et al., 2015) để tạo dữ liệu huấn luyện được ghép cặp cho tín hiệu đối phó. Các ví dụ bao gồm SBERT (Reimers & Gurevych, 2019), SimCSE (Gao et al., 2022), và SGPT (Muennighoff, 2022). Các mô hình gần đây như E5 (Wang et al., 2022), GTE (Li et al., 2023), BGE (Xiao et al., 2023), InstructOR (Su et al., 2023a), và Jina (Günther et al., 2023; 2024) sử dụng một chế độ đa giai đoạn trong đó một transformer được tiền huấn luyện đầu tiên được huấn luyện đối phó sử dụng một corpus lớn dữ liệu được ghép cặp yếu (ví dụ Quora, Reddit Comments) và sau đó được tinh chỉnh thêm trên các tập dữ liệu có nhãn nhỏ, chất lượng cao hơn như MSMarco. Mô hình hai giai đoạn cải thiện đáng kể chất lượng mô hình vì dữ liệu được ghép cặp yếu có sẵn với số lượng lớn hơn nhiều.

Bảng 1: nomic-embed-text-v1 là mô hình ngữ cảnh dài mã nguồn mở duy nhất vượt trội hơn các mô hình mã nguồn đóng như text-embedding-ada-002 và text-embedd-3-small trên benchmark MTEB ngữ cảnh ngắn và benchmark LoCo ngữ cảnh dài.

Mô hình | Tham số | Seq | MTEB | LoCo | Jina LC | Trọng số | Mã | Dữ liệu
nomic-embed-text-v1 | 137M | 8192 | 62.39 | 85.53 | 54.16 | Có | Có | Có
nomic-embed-text-v1-ablated | 137M | 8192 | 61.36 | 86.89 | 53.53 | Có | Có | Có
jina-embeddings-base-v2-en | 137M | 8192 | 60.39 | 85.45 | 51.90 | Có | Không | Không
text-embedding-ada-002 | N/A | 8192 | 60.99 | 52.70 | 55.25 | Không | Không | Không
text-embedding-3-small | N/A | 8192 | 62.26 | 82.4 | 58.21 | Không | Không | Không
E5-Mistral-7b-instruct | 7B | 4096 | 66.6 | 87.8 | N/A | Có | Không | Không
text-embedding-3-large | N/A | 8192 | 64.59 | 79.4 | 58.69 | Không | Không | Không

3.1 Mô hình hóa ngôn ngữ có mặt nạ
Mô hình hóa ngôn ngữ có mặt nạ che giấu một phần trăm đầu vào và huấn luyện một transformer hai chiều để dự đoán các token bị che giấu (Devlin et al., 2019). Mô hình BERT gốc được huấn luyện bổ sung với một tác vụ phụ trợ nhị phân Dự đoán Câu Tiếp theo (NSP). Liu et al. (2019) đã phát hành RoBERTa trong đó họ đạt được hiệu suất tốt hơn bằng cách huấn luyện trên nhiều dữ liệu hơn và lâu hơn. Họ cũng loại bỏ tác vụ NSP vì nó không cho thấy bất kỳ cải thiện hiệu suất nào. Gần đây hơn, Portes et al. (2023) đã giới thiệu MosaicBERT, một công thức huấn luyện BERT hiệu quả và hiệu suất cao bằng cách tăng tỷ lệ che mặt nạ, kết hợp FlashAttention (Dao et al., 2022), và các tối ưu hóa huấn luyện khác.

3.2 Tiền huấn luyện đối phó có giám sát yếu
Tiền huấn luyện đối phó có giám sát yếu nhằm dạy mô hình phân biệt các tài liệu tương tự nhất với các tài liệu không liên quan khác. Để làm như vậy, chúng tôi sử dụng hàm mất mát đối phó InfoNCE (van den Oord et al., 2019). Cho một batch B= (q0, d0),(q1, d1), ...,(qn, dn), chúng tôi tối thiểu hóa hàm mất mát:

LC=−1/n∑i log(es(qi,di)/tau / (es(qi,di)/tau+∑j≠i es(qi,dj)/tau))    (1)

trong đó s(q, d) là điểm số (đã học) giữa truy vấn q và tài liệu d. Chúng tôi đặt s thành độ tương tự cosine cho tất cả các thí nghiệm của chúng tôi. Trái ngược với các phương pháp khác, chúng tôi áp dụng mất mát đối phó một chiều từ truy vấn đến tài liệu. Các phương pháp khác như Günther et al. (2023) sử dụng mất mát đối phó hai chiều bằng cách bao gồm cả mất mát đối phó từ tài liệu đến truy vấn.

--- TRANG 4 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

3.3 Tinh chỉnh đối phó
Giai đoạn cuối của huấn luyện nhằm tăng hiệu suất bằng cách sử dụng các tập dữ liệu có nhãn của con người. Một số bài báo bao gồm Ni et al. (2021a;b); Wang et al. (2022); Li et al. (2023) đã chỉ ra rằng tinh chỉnh trên các tập dữ liệu này dẫn đến cải thiện trong hiệu suất xuôi dòng, đặc biệt cho các tác vụ QA và truy xuất tìm kiếm web. Chúng tôi điều chỉnh Phương trình 1 để bao gồm các tài liệu tiêu cực khó trong mỗi batch:

LC=−1/n∑i log(es(qi,di)/tau / (es(qi,di)/tau+∑j≠i es(qi,dj)/tau+∑m=1^H es(qi,dhn(1,m))/tau))    (2)

Ở đây, chúng tôi sửa đổi hàm phân vùng của mất mát đối phó để bao gồm H tài liệu tiêu cực khó dhn(1, m) là các tài liệu được chọn đặc biệt để gần với d nhưng không phải là tài liệu tích cực thực sự của q.

3.4 Nhúng vị trí xoay
Nhúng vị trí xoay (RoPE) là một mã hóa vị trí thay thế được giới thiệu trong (Su et al., 2023b) mã hóa thông tin vị trí tương đối thông qua các phép xoay trong các lớp attention.

Theo ký hiệu trong (Su et al., 2023b), đặt theta= 10,000 là tiêu chuẩn. Chúng tôi đặt theta= 1,000 cho các thí nghiệm của chúng tôi nhưng thấy ít hoặc không có suy giảm hiệu suất.

3.5 Ngoại suy độ dài ngữ cảnh RoPE
Tuy nhiên, một hạn chế với RoPE là trong việc mở rộng quy mô đến độ dài chuỗi dài hơn so với khi huấn luyện. Chúng tôi thảo luận hai phương pháp: nội suy vị trí và scaling dựa trên tần số.

3.5.1 Nội suy vị trí
Chen et al. (2023) và kaiokendev (2023) độc lập đề xuất ngoại suy các mô hình dựa trên RoPE bằng cách nội suy các chỉ số vị trí nằm trong độ dài chuỗi huấn luyện gốc. Theo ký hiệu trong (Su et al., 2023b), cho một mô hình được tiền huấn luyện với độ dài ngữ cảnh L, hàm nhúng vị trí fW được sửa đổi thành:

fW(xm, m, thetad) = fW(xm, mL/L′, thetad)    (3)

trong đó L′> L là độ dài ngữ cảnh mở rộng mục tiêu. Phương pháp này, mặc dù đơn giản, đòi hỏi tinh chỉnh trên tập dữ liệu nhỏ hơn để đạt được hiệu suất ổn định ở ngữ cảnh dài hơn.

3.5.2 Scaling dựa trên tần số
NTK-Aware Scaling bloc97 (2023) đầu tiên đề xuất scaling các tần số cao hơn và tần số thấp ít hơn bằng cách thay đổi theta cơ sở để "nhận thức NTK" vì nó được chỉ ra trong Tancik et al. (2020) rằng mạng neural gặp khó khăn trong việc biểu diễn tần số cao tốt. Cơ sở được scaled bởi tỷ lệ của độ dài chuỗi dài hơn và độ dài chuỗi được huấn luyện:

b' = b*s^(|D|/(|D|−2))    (4)

trong đó s=L'/L.

Dynamic NTK Scaling Dynamic NTK scaling (emozilla, 2023; Peng et al., 2023) cải thiện NTK-aware bằng cách giới thiệu một siêu tham số alpha vào Phương trình 4.

b' = b*((alpha*s)−(alpha−1))^(|D|/(|D|−2))    (5)

Điều này duy trì các nhúng vị trí gốc cho các chuỗi trong độ dài ngữ cảnh được tiền huấn luyện (lcurrent <= L) và dần dần scale các nhúng khi chuỗi tăng dài hơn, ngăn chặn suy giảm hiệu suất đột ngột. Ngoài ra, phương pháp này có thể được sử dụng mà không cần bất kỳ tinh chỉnh nào như được chỉ ra trong (Peng et al., 2023).

--- TRANG 5 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

4 Phương pháp

4.1 Mô hình hóa ngôn ngữ có mặt nạ

4.1.1 Dữ liệu
Theo Devlin et al. (2019), chúng tôi sử dụng BooksCorpus (Zhu et al., 2015) và một dump Wikipedia từ năm 2023 để huấn luyện một mô hình BERT ngữ cảnh dài, sau đây được gọi là nomic-bert-2048. Mỗi tài liệu từ BooksCorpus và Wikipedia được token hóa sử dụng tokenizer bert-base-uncased từ Devlin et al. (2019) và được đóng gói qua các tài liệu thành các đoạn 2048 token. Nếu một tài liệu ngắn hơn 2048 token, chúng tôi thêm tài liệu khác cho đến khi nó vừa 2048 token. Nếu một tài liệu lớn hơn 2048 token, chúng tôi chia nó qua nhiều tài liệu.

nomic-bert-2048 tuân theo một pipeline huấn luyện tương tự cho mô hình hóa ngôn ngữ có mặt nạ như Portes et al. (2023). Chúng tôi bỏ qua dự đoán câu tiếp theo tương tự như Liu et al. (2019) và Portes et al. (2023) vì nó được chỉ ra là không cải thiện hiệu suất và đơn giản hóa công thức huấn luyện.

4.1.2 Sửa đổi huấn luyện
Để huấn luyện một BERT độ dài chuỗi dài và hiệu quả, chúng tôi điều chỉnh kiến trúc BERT. Chúng tôi thực hiện các thay đổi kiến trúc sau đây cho BERT base (Devlin et al., 2019):

• Thay thế nhúng vị trí tuyệt đối bằng nhúng vị trí xoay (Su et al., 2023b)
• Sử dụng kích hoạt SwiGLU thay vì GeLU (Shazeer, 2020)
• Sử dụng Flash Attention (Dao et al., 2022)
• Đặt Dropout về 0 (Geiping & Goldstein, 2022)
• Kích thước từ vựng là bội số của 64 (Portes et al., 2023; Shoeybi et al., 2020)

dẫn đến một bộ mã hóa 137 triệu tham số.

Chúng tôi huấn luyện tất cả các giai đoạn với độ dài chuỗi tối đa 2048 và sử dụng nội suy Dynamic NTK tại thời điểm suy luận để mở rộng đến độ dài chuỗi 8192 (Peng et al., 2023; emozilla, 2023). Ngoài ra, chúng tôi chọn SwiGLU thay vì GeGLU như được đề xuất trong Portes et al. (2023) vì thời gian chạy nhanh hơn khoảng 25% cho SwiGLU sử dụng kho Flash Attention¹.

Chúng tôi sử dụng tỷ lệ che mặt nạ 30% thay vì 15% theo Portes et al. (2023) và chúng tôi loại bỏ tác vụ Dự đoán Câu Tiếp theo để đơn giản hóa công thức huấn luyện (Liu et al., 2019; Portes et al., 2023). Chúng tôi sử dụng bộ tối ưu AdamW (Loshchilov & Hutter, 2019) với tốc độ học tối đa 5e-4 với beta1= 0.9 beta2= 0.98. Chúng tôi sử dụng warm-up tuyến tính 6% của tổng số bước huấn luyện và giảm tuyến tính về 0. Chúng tôi sử dụng kích thước batch toàn cục 4096 với tích lũy gradient qua 8 batch. Chúng tôi sử dụng DeepSpeed (Rajbhandari et al., 2020) stage 2 để vừa các batch lớn hơn vào bộ nhớ. Ngoài ra, chúng tôi sử dụng độ chính xác hỗn hợp bfloat16 và fp32 cho dtype tích lũy gradient. Chúng tôi vô hiệu hóa cắt gradient (Liu et al., 2019) và đặt weight decay về 1e-5. Chúng tôi gọi mô hình cuối cùng của chúng tôi là nomic-bert-2048 và cũng phát hành trọng số của nó.

4.2 Tiền huấn luyện đối phó có giám sát yếu

4.2.1 Dữ liệu
Tương tự như Wang et al. (2022); Li et al. (2023); Xiao et al. (2023); Ni et al. (2022), chúng tôi sử dụng các bộ sưu tập lớn dữ liệu có sẵn công khai để tạo các cặp đối phó. Các tập dữ liệu này bao gồm các mục tiêu và lĩnh vực khác nhau, từ truy xuất web đến phân cụm các bài báo khoa học. Tổng cộng, chúng tôi đã tuyển chọn 470 triệu cặp qua 29 tập dữ liệu².

Lọc tính nhất quán: Vì nhiều tập dữ liệu này có thể chứa các ví dụ nhiễu, chúng tôi sử dụng lọc tính nhất quán để loại bỏ các tích cực giả có thể có trong tập dữ liệu (Günther et al., 2023; Wang et al., 2022).

¹https://github.com/Dao-AILab/flash-attention/tree/main
²https://huggingface.co/datasets/sentence-transformers/embedding-training-data

--- TRANG 6 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

Lọc tính nhất quán sử dụng một mô hình được tiền huấn luyện để lọc ra các ví dụ nhiễu tiềm năng trong nỗ lực cải thiện chất lượng dữ liệu và do đó chất lượng mô hình. Ngoài ra, việc giảm tổng số ví dụ cần thiết để huấn luyện một mô hình nhúng văn bản chất lượng cao có thể giảm chi phí tổng thể để huấn luyện mô hình nhúng văn bản.

Đối với mỗi cặp, được mô tả là (query, document), chúng tôi nhúng các truy vấn và tài liệu riêng biệt. Chúng tôi lấy mẫu 1 triệu điểm từ tập dữ liệu và đối với mỗi truy vấn, chúng tôi tìm top-k (trong trường hợp này là 2) láng giềng sử dụng độ tương tự cosine. Nếu document không nằm trong top-k láng giềng, chúng tôi loại bỏ ví dụ đó.

Günther et al. (2023) sử dụng all-MiniLM-L6-v2³, một mô hình nhúng câu 22 triệu tham số cho lọc tính nhất quán. Tuy nhiên, chúng tôi thấy rằng nó thường xuyên loại bỏ các cặp truy xuất là tích cực thực sự nhưng có sự chồng chéo từ vựng thấp. Thay vào đó chúng tôi sử dụng gte-base⁴ (Li et al., 2023), một mô hình 109 triệu tham số cho lọc tính nhất quán. Sau khi lọc, chúng tôi có khoảng 235 triệu cặp. Phân phối tập dữ liệu đầy đủ có thể được xem trong Phụ lục B.

Chúng tôi cũng khám phá lọc tính nhất quán sử dụng ngưỡng độ tương tự cosine thay vì phương pháp được mô tả ở trên. Đối với một cặp cho trước, nếu độ tương tự cosine lớn hơn hoặc bằng ngưỡng, chúng tôi giữ cặp đó và ngược lại loại bỏ. Tuy nhiên, chúng tôi từ bỏ phương pháp này để ủng hộ lọc tính nhất quán top-k vì chúng tôi thấy thông qua kiểm tra thủ công rằng lọc tính nhất quán ngưỡng loại bỏ các cặp truy xuất chất lượng cao có độ tương tự cosine thấp. Chúng tôi cũng nhận thấy điểm số truy xuất thấp hơn trong các mô hình được huấn luyện sử dụng ngưỡng cho lọc tính nhất quán so với sử dụng lọc tính nhất quán top-k.

Tuyển chọn cặp văn bản ngữ cảnh dài: Vì đa số các tập dữ liệu này được cấu thành từ các chuỗi ngắn hơn 2048 token, chúng tôi cũng tuyển chọn các tập dữ liệu ngữ cảnh dài để cho phép học các phụ thuộc tầm xa. Chúng tôi sử dụng tiêu đề Wikipedia được ghép cặp với nội dung tương ứng và các tóm tắt S2ORC (Lo et al., 2020) và văn bản toàn văn từ một bài báo duy nhất.

Bảng 2: nomic-bert-2048 hoạt động tương tự như các bộ mã hóa ngữ cảnh ngắn và dài khác khi được đánh giá trên benchmark GLUE.

Mô hình | Seq | Bsz | Steps | Cola | SST2 | MRPC | STSB | QQP | MNLI | QNLI | RTE | Avg
MosaicBERT | 128 | 4k | 178k | 0.59 | 0.94 | 0.89 | 0.90 | 0.92 | 0.86 | 0.91 | 0.83 | 0.85
JinaBERTBase | 512 | 4k | 100k | 0.51 | 0.95 | 0.88 | 0.90 | 0.81 | 0.86 | 0.92 | 0.79 | 0.83
RobertaBase | 512 | 8k | 500k | 0.64 | 0.95 | 0.90 | 0.91 | 0.92 | 0.88 | 0.93 | 0.79 | 0.86
MosaicBERT | 2k | 4k | 70k | 0.54 | 0.93 | 0.87 | 0.90 | 0.92 | 0.86 | 0.92 | 0.82 | 0.85
nomic-bert-2048 | 2k | 4k | 100k | 0.50 | 0.93 | 0.88 | 0.90 | 0.92 | 0.86 | 0.92 | 0.82 | 0.84

Bạn có thể truy cập dữ liệu huấn luyện của nomic-embed-text-v1 bằng cách truy cập kho mã. Bạn có thể khám phá một mẫu 5M của các cặp huấn luyện đối phó của chúng tôi tại https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample.

4.2.2 Sửa đổi huấn luyện
Chúng tôi khởi tạo mô hình cho huấn luyện đối phó có giám sát yếu với trọng số của nomic-bert-2048. Chúng tôi sử dụng kích thước batch toàn cục 16,384. Chúng tôi sử dụng AdamW với tốc độ học 2e-4, beta1= 0.9, beta2= 0.999, và weight decay 0.01. Cắt gradient được đặt thành 1.0. Chúng tôi sử dụng lịch trình warm-up tuyến tính 700 bước và lịch trình giảm căn bậc hai nghịch đảo.

Chúng tôi lấy mẫu một nguồn dữ liệu và lấp đầy mỗi batch chỉ với dữ liệu từ nguồn đó để ngăn chặn mô hình học các lối tắt cụ thể của nguồn. Chúng tôi huấn luyện với độ dài chuỗi tối đa 2048 cho 1 epoch đầy đủ trên dữ liệu đối phó có giám sát yếu. Chi tiết đầy đủ về thành phần dữ liệu có thể được tìm thấy trong Phụ lục B.

Do hạn chế bộ nhớ GPU, chúng tôi sử dụng GradCache (Luyu Gao & Callan, 2021) cũng như huấn luyện độ chính xác hỗn hợp (Micikevicius et al., 2018).

³mô hình all-MiniLM-L6-v2 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 )
⁴mô hình gte-base (https://huggingface.co/thenlper/gte-base )

--- TRANG 7 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

Cuối cùng, chúng tôi sử dụng các tiền tố cụ thể của tác vụ để phá vỡ tính đối xứng của biencoder như trong Wang et al. (2022). Không có tiền tố, mô hình nhận được tín hiệu phần thưởng mâu thuẫn. Xem xét trường hợp xác định tài liệu nào gần nhất với truy vấn "Thủ đô của Pháp là gì?":

1. "Tên của thành phố thủ đô của Pháp là gì?
2. "Paris là thủ đô của Pháp."

Một tác vụ tương tự ngữ nghĩa sẽ coi cái đầu tiên gần nhất, trong khi một tác vụ trả lời câu hỏi sẽ coi cái thứ hai gần nhất. Tiền tố cho phép mô hình phân biệt giữa các hành vi được chỉ định bởi mỗi tác vụ này.

Chúng tôi sử dụng các tiền tố cụ thể của tác vụ sau:
• search_query
• search_document
• classification
• clustering

được lấy cảm hứng từ Reimers et al. (2023). Chúng tôi đầu tiên chia tiền tố thành hai loại: đối xứng, nơi truy vấn và tài liệu có cấu trúc tương tự, và bất đối xứng, nơi truy vấn thường là một câu duy nhất và tài liệu có thể là nhiều câu (Su et al., 2023a). Hai tiền tố đầu tiên được sử dụng cho các tác vụ truy xuất: nơi search_query được sử dụng cho câu hỏi và search_document được sử dụng cho phản hồi. classification được sử dụng cho các tác vụ liên quan đến STS như paraphrase. clustering được sử dụng cho các tác vụ nơi mục tiêu là nhóm các văn bản tương tự ngữ nghĩa gần nhau, như các cặp tiêu đề-tóm tắt Arxiv. Đối với các tác vụ đối xứng, cùng một tiền tố được thêm vào cả truy vấn và tài liệu.

4.2.3 Tinh chỉnh đối phó có giám sát

4.2.4 Dữ liệu
Tinh chỉnh có giám sát được thực hiện trên MSMarco (Bajaj et al., 2018; Wang et al., 2023a), NQ (Karpukhin et al., 2020; Gao & Callan, 2021), NLI (Gao et al., 2022), HotpotQA (Yang et al., 2018), FEVER (Thorne et al., 2018), các phần của MEDI (Su et al., 2023a), WikiAnswers (Fader et al., 2014), và Reddit⁵.

Đối với các tập dữ liệu MSMarco, NQ, NLI, FEVER, và HotpotQA, chúng tôi huấn luyện trên các tập huấn luyện được phát hành từ benchmark BEIR (Thakur et al., 2021). Đối với các tập dữ liệu truy xuất (MSMarco, NQ, HotpotQA, và Fever), chúng tôi khai thác tiêu cực, nếu chưa được khai thác, sử dụng gte-base (Li et al., 2023). Đối với mỗi cặp (q, d), chúng tôi tìm top 20 tài liệu trong corpus tương tự nhất với truy vấn q, loại trừ d, và sử dụng chúng làm tiêu cực khó. Đối với các tập dữ liệu không truy xuất khác, chúng tôi lấy mẫu ngẫu nhiên tiêu cực trong corpus thay vì khai thác tiêu cực khó vì chúng tôi thấy rằng khai thác không cải thiện hiệu suất.

Mặc dù thành phần BEIR của MTEB ban đầu được dự định là một benchmark zero shot, một số mô hình mã nguồn mở, như những mô hình trong Xiao et al. (2023); Li et al. (2023); Wang et al. (2023b), báo cáo huấn luyện trên các split huấn luyện của các tập dữ liệu benchmark BEIR như FEVER và HotpotQA. Chúng tôi báo cáo kết quả cho nomic-embed-text-v1-ablated được huấn luyện mà không có FEVER, HotpotQA, và MEDI.

Tương tự như giai đoạn đối phó có giám sát yếu, chúng tôi lấy mẫu một tập dữ liệu và lấp đầy một batch với tất cả các điểm từ tập dữ liệu được chọn đó. Tổng cộng, chúng tôi huấn luyện trên 1.6 triệu điểm dữ liệu. Phân phối tập dữ liệu đầy đủ có thể được xem trong Bảng 3.

⁵https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit

--- TRANG 8 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

Bảng 3: Phân phối tập dữ liệu tinh chỉnh có giám sát.

Tập dữ liệu | Số lượng mẫu
MSMarco | 484,864
NLI | 275,200
Reddit | 199,680
Medi Supernli | 177,408
Hotpot | 169,728
Fever | 139,776
Medi Stackexchange | 100,352
NQ | 69,888
Medi Flickr | 50,944
Medi Wiki | 24,832

4.2.5 Sửa đổi huấn luyện
Chúng tôi huấn luyện cho một epoch sử dụng bảy tiêu cực khó mỗi cặp và kích thước batch 256. Chúng tôi sử dụng tốc độ học 2e-5, beta1= 0.9, beta2= 0.999, và weight decay 0.01. Cắt gradient được đặt thành 1.0. Chúng tôi sử dụng lịch trình warm-up tuyến tính 400 bước và làm mát tuyến tính về 0 và huấn luyện với tiền tố như được mô tả ở trên.

Chúng tôi thấy rằng việc tăng số lượng tiêu cực trên 7 không cải thiện đáng kể hiệu suất. Chúng tôi cũng thấy rằng huấn luyện cho nhiều epoch làm tổn hại hiệu suất. Thay vì chọn N tiêu cực đầu tiên, chúng tôi lấy mẫu ngẫu nhiên các tiêu cực được khai thác. Chúng tôi thấy điều này cải thiện hiệu suất vì một số tiêu cực được khai thác đã giới thiệu tiêu cực giả.

5 Kết quả

5.1 Kết quả GLUE của nomic-bert-2048
Chúng tôi đầu tiên đánh giá nomic-bert-2048 trên benchmark GLUE (Wang et al., 2019) để xác minh rằng kiến trúc BERT được điều chỉnh của chúng tôi hoạt động tương tự hoặc tốt hơn so với các bộ mã hóa tương tự. Benchmark GLUE bao gồm 9 tác vụ, nhưng chúng tôi đánh giá trên 8 tương tự như Liu et al. (2019). Chúng tôi tuân theo phương pháp đánh giá được trình bày trong Liu et al. (2019). Số liệu Roberta được lấy từ Bảng 8 trong (Liu et al., 2019). Số liệu MosaicBert được lấy từ Bảng S1 trong Portes et al. (2023) ngoại trừ mô hình 2048 mà chúng tôi đánh giá theo cách tương tự như nomic-bert-2048. Số liệu JinaBertBase Glue Test được báo cáo trong Bảng 2 từ (Günther et al., 2024).

Đối với mỗi tác vụ, chúng tôi huấn luyện cho 10 epoch với kích thước batch 16, 32 và tốc độ học 1e-5, 2e-5, 3e-5 qua 5 seed. Điểm số trung vị mỗi tác vụ cuối 10 epoch được trình bày trong Bảng 2. Lưu ý chúng tôi báo cáo độ chính xác cho MRPC và QQP và Pearson cho STSB⁶. Tương tự như Liu et al. (2019), chúng tôi khởi tạo từ một checkpoint MNLI cho RTE, STSB, và MRPC.

Qua tất cả các tác vụ, nomic-bert-2048 ghi điểm tương tự như MosaicBERT (Portes et al., 2023) ngoại trừ Cola. MosaicBERT được huấn luyện với nhiều cập nhật gradient hơn trên C4 (Raffel et al., 2019). Tuy nhiên, nomic-bert-2048 được huấn luyện với độ dài chuỗi dài hơn và trong hiệu ứng đã thấy nhiều token hơn trong quá trình tiền huấn luyện. Sự khác biệt trong kết quả có thể do một vài lý do. Đầu tiên corpus huấn luyện có thể dẫn đến kết quả tốt hơn trên Cola vì nomic-bert-2048 huấn luyện trên Wikipedia và Bookscorpus trong khi MosaicBERT huấn luyện trên C4 có xu hướng nghiêng về độ dài chuỗi ngắn hơn. Ngoài ra, nomic-bert-2048 sử dụng RoPE trong khi MosaicBERT sử dụng ALiBi cho ngoại suy ngữ cảnh dài.

JinaBERT cũng huấn luyện một mô hình tương tự như MosaicBERT sử dụng ALiBi cho ngoại suy ngữ cảnh dài và C4 làm corpus huấn luyện của nó nhưng đặt độ dài chuỗi tối đa là 512. Nó hoạt động hơi tệ hơn trung bình so với nomic-bert-2048 và MosaicBERT. Mặc dù nó được huấn luyện tương tự như MosaicBERT, JinaBERT hoạt động tệ hơn trên Cola, RTE, và QQP. Những phát hiện này tương tự khi so sánh với nomic-bert-2048 ngoại trừ Cola nơi JinaBERT vượt trội hơn nomic-bert-2048.

⁶https://github.com/facebookresearch/fairseq/issues/1561#issuecomment-571729519

--- TRANG 9 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

Bảng 4: Kết quả benchmark MTEB (Muennighoff et al., 2023). nomic-embed-text-v1 vượt trội hơn tất cả các mô hình có kích thước tương tự trên các tác vụ ngữ cảnh ngắn ngoại trừ BGE-Base.

Loại → | Tham số. | Cls. | Clust. | PairCls. | Rerank | Retr. | STS | Summ. | Avg
Số lượng tập dữ liệu → | | 12 | 11 | 3 | 4 | 15 | 10 | 1 | 56

Mô hình không giám sát
Glove (Pennington et al., 2014) | 0.3B | 57.3 | 27.7 | 70.9 | 43.3 | 21.6 | 61.9 | 28.9 | 42.0
SimCSE (Gao et al., 2022) | 110M | 62.5 | 29.0 | 70.3 | 46.5 | 20.3 | 74.3 | 31.2 | 45.5
nomic-embed-text-v1 unsup | 137M | 71.2 | 42.5 | 83.7 | 55.0 | 48.0 | 80.8 | 30.7 | 59.9

Mô hình có giám sát
SimCSE bert-sup (Gao et al., 2022) | 110M | 67.3 | 33.4 | 73.7 | 47.5 | 21.8 | 79.1 | 23.3 | 48.7
Contriever (Izacard et al., 2022a) | 110M | 66.7 | 41.1 | 82.5 | 53.1 | 41.9 | 76.5 | 30.4 | 56.0
E5base(Wang et al., 2022) | 110M | 75.2 | 44.2 | 86.0 | 56.6 | 50.6 | 82.1 | 30.2 | 61.6
GTE base(Li et al., 2023) | 110M | 73.0 | 46.2 | 84.6 | 58.6 | 51.1 | 82.3 | 31.2 | 62.4
BGE base(Xiao et al., 2023) | 110M | 75.5 | 45.8 | 86.6 | 58.9 | 53.3 | 82.4 | 31.1 | 63.6
Jina v2(Günther et al., 2024) | 137M | 73.5 | 41.7 | 85.4 | 57.0 | 47.9 | 80.7 | 31.6 | 60.4
nomic-embed-text-v1-ablated | 137M | 73.6 | 43.7 | 84.6 | 53.3 | 51.4 | 80.2 | 31.3 | 61.4
nomic-embed-text-v1 | 137M | 74.1 | 43.9 | 85.2 | 55.7 | 52.8 | 82.1 | 30.1 | 62.4
E5large-v2 (Wang et al., 2022) | 335M | 75.2 | 44.5 | 86.0 | 56.6 | 50.6 | 82.1 | 30.2 | 62.3
GTE large (Li et al., 2023) | 335M | 73.3 | 46.8 | 85.0 | 59.1 | 52.2 | 83.4 | 31.7 | 63.1
BGE large (Xiao et al., 2023) | 335M | 76.0 | 46.1 | 87.1 | 60.0 | 54.3 | 83.1 | 31.6 | 64.2
GTR xxl(Ni et al., 2021a) | 4.8B | 67.4 | 42.4 | 86.1 | 56.7 | 48.5 | 78.4 | 30.6 | 59.0
Sentence-T5 xxl(Ni et al., 2021b) | 4.8B | 73.4 | 43.7 | 85.1 | 56.4 | 42.2 | 82.6 | 30.1 | 59.5
text-embedding-ada-002 | NA | 70.9 | 45.9 | 84.9 | 56.3 | 49.3 | 81.0 | 30.8 | 61.0
text-embedding-3-small | NA | 73.2 | 46.7 | 85.0 | 56.7 | 51.1 | 81.6 | 31.1 | 62.3
text-embedding-3-large | NA | 75.5 | 49.0 | 85.7 | 59.2 | 55.4 | 81.7 | 29.9 | 64.6
E5mistral (Wang et al., 2023b) | 7B | 78.5 | 50.3 | 88.3 | 60.2 | 56.9 | 84.6 | 31.4 | 66.6

5.2 Kết quả Benchmark Nhúng Văn bản
Để đánh giá hiệu quả của nomic-embed-text-v1 như một bộ mã hóa văn bản, chúng tôi đánh giá nó trên MTEB (Muennighoff et al., 2023), Jina's Long Context Benchmark (Günther et al., 2024), và LoCo (Saad-Falcon et al., 2024).

5.2.1 Kết quả MTEB
MTEB là một benchmark nhúng văn bản tổng quát được phát hành bởi Muennighoff et al. (2023). Nó đo hiệu suất nhúng văn bản qua các tác vụ Phân loại, Phân cụm, Phân loại Cặp, Xếp hạng lại, Truy xuất, STS, và Tóm tắt.

Trong quá trình đánh giá, chúng tôi thêm tiền tố classification vào cả truy vấn và tài liệu cho các tác vụ Phân loại, Phân loại Cặp, STS, và Tóm tắt. Chúng tôi thêm tiền tố clustering vào cả truy vấn và tài liệu cho tác vụ Phân cụm. Và chúng tôi thêm tiền tố search_query vào truy vấn và tiền tố search_document vào tài liệu cho tác vụ Truy xuất. Chúng tôi cắt tất cả văn bản thành 512 token. Ngoài ra, chúng tôi thấy hiệu suất tốt hơn bằng cách không chuẩn hóa L2 các nhúng cho tác vụ Phân loại, tương tự như mã đánh giá được phát hành bởi Wang et al. (2022). Đối với tất cả các tác vụ khác, chúng tôi chuẩn hóa L2 các nhúng.

Hiệu suất của nomic-embed-text-v1 và nomic-embed-text-v1-ablated được phân tích theo tác vụ trong Bảng 4. So với các mô hình nhúng văn bản mã nguồn mở có kích thước tương tự, nomic-embed-text-v1 vượt trội hơn tất cả các mô hình ngoại trừ BGE-base (Xiao et al., 2023). Ngoài ra, nomic-embed-text-v1 vượt trội hơn các mô hình nhúng văn bản mã nguồn mở lớn hơn như E5 Large v2 (Wang et al., 2022), GTR XXL (Ni et al., 2021a), và Sentence T5 XXL (Ni et al., 2021b).

--- TRANG 10 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

Bảng 5: Kết quả benchmark Jina Long Context. nomic-embed-text-v1 vượt trội hơn jina-embeddings-v2-base và hoạt động tương tự như text-embeddings-ada-002.

Mô hình | Seq | NarrativeQA | WikiCities | SciFact | BigPatent | Avg
jina-embeddings-base-v2 | 128 | 19.6 | 79.9 | 62.1 | 14.4 | 44.0
nomic-embed-text-v1-ablated | 128 | 20.8 | 86.8 | 65.2 | 17.5 | 47.6
nomic-embed-text-v1 | 128 | 20.1 | 90.0 | 65.4 | 18.5 | 48.5
text-embedding-ada-002 | 128 | 25.4 | 84.9 | 68.8 | 16.6 | 48.9
text-embedding-3-small | 128 | 29.5 | 87.5 | 68.8 | 15.0 | 50.2
text-embedding-3-large | 128 | 45.6 | 87.9 | 74.8 | 16.5 | 56.2
jina-embeddings-base-v2 | 512 | 21.3 | 79.3 | 66.7 | 21.9 | 47.3
nomic-embed-text-v1-ablated | 512 | 25.7 | 81.9 | 71.5 | 23.7 | 50.7
nomic-embed-text-v1 | 512 | 23.9 | 88.7 | 70.5 | 25.3 | 52.1
text-embedding-ada-002 | 512 | 25.5 | 84.8 | 72.6 | 23.0 | 51.5
text-embedding-3-small | 512 | 32.2 | 89.0 | 73.2 | 23.6 | 54.5
text-embedding-3-large | 512 | 48.1 | 89.9 | 77.6 | 23.6 | 59.6
jina-embeddings-base-v2 | 8191 | 39.4 | 75.7 | 69.4 | 23.1 | 51.9
nomic-embed-text-v1-ablated | 8191 | 44.0 | 77.4 | 69.1 | 23.6 | 53.5
nomic-embed-text-v1 | 8191 | 37.8 | 84.3 | 70.2 | 24.5 | 54.2
text-embedding-ada-002 | 8191 | 41.1 | 84.7 | 72.7 | 22.5 | 55.3
text-embedding-3-small | 8191 | 47.1 | 89.9 | 73.3 | 22.5 | 58.3
text-embedding-3-large | 8191 | 51.6 | 86.2 | 77.7 | 19.3 | 58.7

Bảng 6: Kết quả benchmark LoCo (Saad-Falcon et al., 2024). nomic-embed-text-v1 là mô hình lớp tham số 100M hoạt động tốt nhất không giám sát. nomic-embed-text-v1 cạnh tranh với các mô hình hoạt động tốt nhất trong cả lớp tham số 7B và với các mô hình được huấn luyện trong môi trường giám sát cụ thể cho benchmark LoCo.

Mô hình | Seq | Param. | Tau Scr. | Tau Gov. | Tau QMS. | QASP. Tit. | Art. | QASP. Abs. | Art. | Avg
M2-Bert (Saad-Falcon et al., 2024) | 2048 | 80M | 81.8 | 94.7 | 58.5 | 87.3 | 95.5 | 83.6
Jina base-v2 (Günther et al., 2024) | 2048 | 137M | 87.2 | 97.7 | 35.1 | 95.3 | 99.7 | 83.0
nomic-embed-text-v1-ablated | 2048 | 137M | 83.1 | 97.3 | 49.4 | 97.4 | 99.9 | 85.4
nomic-embed-text-v1 | 2048 | 137M | 86.1 | 96.9 | 47.8 | 96.1 | 99.7 | 85.3
nomic-embed-text-v1 | 4096 | 137M | 89.0 | 97.4 | 45.7 | 95.8 | 99.9 | 85.6
nomic-embed-text-v1-ablated | 4096 | 137M | 89.1 | 97.6 | 49.6 | 97.5 | 99.9 | 86.7
E5mistral (Wang et al., 2023b) | 4096 | 7B | 95.9 | 98.3 | 46.8 | 98.4 | 99.8 | 87.8
M2-Bert (Saad-Falcon et al., 2024) | 8192 | 80M | 94.7 | 96.5 | 64.1 | 86.8 | 97.5 | 87.9
Jina base-v2 (Günther et al., 2023) | 8192 | 137M | 93.3 | 98.6 | 40.8 | 95.1 | 99.3 | 85.5
nomic-embed-text-v1-ablated | 8192 | 137M | 92.5 | 97.8 | 47.6 | 96.5 | 99.9 | 86.9
nomic-embed-text-v1 | 8192 | 137M | 90.9 | 97.8 | 44.2 | 94.9 | 99.9 | 85.5
text-embedding-ada-002 | 8192 | N/A | 37.3 | 44.3 | 7.30 | 85.1 | 89.7 | 52.7
text-embedding-3-small | 8192 | N/A | 92.2 | 97.7 | 27.4 | 95.9 | 98.9 | 82.4
text-embedding-3-large | 8192 | N/A | 88.0 | 93.6 | 25.5 | 93.2 | 96.8 | 79.4

So với các mô hình mã nguồn đóng, nomic-embed-text-v1 vượt trội hơn text-embedding-ada-002 và text-embedding-3-small trung bình và đáng chú ý là tác vụ Truy xuất. nomic-embed-text-v1 là mô hình nhúng văn bản ngữ cảnh dài mã nguồn mở duy nhất vượt trội hơn text-embedding-ada-002 và text-embedding-3-small trên MTEB.

--- TRANG 11 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

nomic-embed-text-v1-ablated không đáng ngạc nhiên khi hoạt động tệ hơn nomic-embed-text-v1 và các mô hình nhúng văn bản mã nguồn mở khác tinh chỉnh trên các tập huấn luyện của BEIR như BGE-base và GTE-base. Tuy nhiên, nomic-embed-text-v1-ablated vẫn vượt trội hơn text-embedding-ada-002 và jina-embeddings-base-v2 và cạnh tranh với E5-base v2. jina-embeddings-base-v2 được huấn luyện trên các tập dữ liệu tương tự như nomic-embed-text-v1-ablated nhưng nomic-embed-text-v1-ablated vượt trội hơn jina-embeddings-base-v2 trên MTEB.

5.2.2 Kết quả ngữ cảnh dài
Tuy nhiên, như được ghi nhận trong Günther et al. (2024), MTEB có rất ít tập dữ liệu bao gồm các chuỗi dài. Để đánh giá hiệu suất của nomic-embed-text-v1 trên các chuỗi dài hơn, chúng tôi xem xét hai benchmark bổ sung: Jina Long Context Dataset (Günther et al., 2024) cũng như benchmark LoCo từ Saad-Falcon et al. (2024). Mặc dù nomic-embed-text-v1 được huấn luyện với độ dài chuỗi tối đa 2048, chúng tôi có thể sử dụng các kỹ thuật ngoại suy độ dài được đề xuất trong emozilla (2023); Peng et al. (2023).

Đối với các văn bản dài hơn 2048, độ dài chuỗi tối đa mà nomic-embed-text-v1 được huấn luyện, chúng tôi sử dụng Dynamic NTK Interpolation như được mô tả trong Phương trình 5. Chúng tôi đặt alpha thành 2.

5.2.3 JinaAI Long Context Benchmark
Jina Long Context Benchmark (Günther et al., 2024) đánh giá trên 4 tập dữ liệu qua Truy xuất và Phân cụm; cụ thể là, NarrativeQA (Günther et al., 2024), WikiCites⁷, SciFact (Wadden et al., 2020), và BigPatent⁸ (Sharma et al., 2019). Tương tự như Günther et al. (2024), chúng tôi báo cáo V-scores và NDCG@10 cho các tập dữ liệu phân cụm và truy xuất tương ứng. Chúng tôi đánh giá tất cả các mô hình ở độ dài chuỗi 128, 512, và 8191. Đối với nomic-embed-text-v1 và nomic-embed-text-v1-ablated trên NarrativeQARetrieval và Scifact, chúng tôi sử dụng tiền tố search_query và search_document cho truy vấn và tài liệu tương ứng. Đối với BigPatentClustering và WikiCities, chúng tôi sử dụng tiền tố clustering cho cả truy vấn và tài liệu. Kết quả được trình bày trong Bảng 5.

Số liệu cho text-embedding-ada-002 và jina-embeddings-base-v2 được lấy từ (Günther et al., 2024).

Qua tất cả độ dài ngữ cảnh, nomic-embed-text-v1 vượt trội hơn jina-embeddings-v2-base. Khi được đánh giá trên độ dài chuỗi ngắn hơn, nomic-embed-text-v1 hoạt động tương tự như text-embedding-ada-002 nhưng bị vượt trội ở ngữ cảnh 8k. Ngoài ra, nomic-embed-text-v1-ablated vượt trội hơn jina-embeddings-v2-base, nhưng hoạt động kém hơn nomic-embed-text-v1.

Tuy nhiên, nomic-embed-text-v1 hoạt động kém hơn text-embedding-ada-002, text-embedding-3-small, và text-embedding-3-large. Không có thông tin nào về dữ liệu huấn luyện hoặc kiến trúc cho các mô hình mã nguồn đóng, không rõ tại sao khoảng cách tồn tại. Cũng đáng ngạc nhiên khi thấy hiệu suất text-embedding-3-large giảm khi độ dài chuỗi tăng từ 512 lên 8191 trong khi hiệu suất tăng cho text-embedding-3-small và text-embedding-002-ada.

Tương tự như kết quả trong Günther et al. (2023), chúng tôi thấy hiệu suất thấp hơn trong WikiCities khi độ dài chuỗi tăng qua các mô hình cho thấy tác vụ có thể không phải là một thước đo tốt của hiệu suất nhúng ngữ cảnh dài.

5.2.4 LoCo Benchmark
LoCo Benchmark bao gồm 5 tập dữ liệu truy xuất: 3 tập dữ liệu từ Shaham et al. (2022) và 2 từ Dasigi et al. (2021). Tương tự như các đánh giá truy xuất khác, chúng tôi sử dụng tiền tố search_query và search_document cho truy vấn và tài liệu tương ứng. Chúng tôi đánh giá nomic-embed-text-v1 và jina-embeddings-base-v2 ở độ dài chuỗi 2048, 4096, và 8192. Chúng tôi cũng bao gồm kết quả từ Saad-Falcon et al. (2024) mặc dù mô hình được tinh chỉnh trên các tập huấn luyện của các tập dữ liệu này. Kết quả được trình bày trong Bảng 6. Chúng tôi bao gồm tập dữ liệu QASPER Abstract Articles để hoàn thiện, nhưng muốn nhấn mạnh rằng nhiều mô hình dường như quá bão hòa benchmark và có thể không đại diện cho hiệu suất ngữ cảnh dài.

⁷https://huggingface.co/datasets/jinaai/cities_wiki_clustering
⁸https://huggingface.co/datasets/jinaai/big-patent-clustering

--- TRANG 12 ---
Xuất bản trong Transactions on Machine Learning Research (02 /2025)

Ở độ dài chuỗi 2048, nomic-embed-text-v1 và nomic-embed-text-v1-ablated vượt trội hơn jina-embeddings-v2-base. Ở độ dài chuỗi 4096 nomic-embed-text-v1 và nomic-embed-text-v1-ablated có thể hoạt động tương tự như E5 Mistral, một mô hình lớn hơn ~70 lần trên tất cả các tác vụ ngoại trừ Tau Scrolls.

Cả hai biến thể nomic-embed-text-v1 đều vượt trội hơn text-embedding-ada-002 và text-embedding-3-small và hoạt động tương tự như jina-embeddings-v2-base ở độ dài chuỗi 8192. Thú vị là, nomic-embed-text-v1-ablated vượt trội hơn nomic-embed-text-v1 và jina-embedding-base-v2 cho thấy rằng dữ liệu huấn luyện BEIR có thể trực giao với các tác vụ LoCo.

6 Kết luận
Chúng tôi phát hành mô hình nhúng văn bản ngữ cảnh dài mã nguồn mở đầy đủ đầu tiên vượt qua hiệu suất OpenAI's text-embedding-Ada-002 và text-embedding-003-small trên cả benchmark ngữ cảnh ngắn và dài. Chúng tôi phát hành trọng số mô hình và mã huấn luyện theo giấy phép cho phép cũng như công thức, bao gồm dữ liệu, để tái tạo mô hình. Tính đến thời điểm viết này, nomic-embed đã thu hút hơn 14 triệu lượt tải xuống trên hub mô hình Hugging Face, nhấn mạnh nhu cầu rộng rãi cho các công thức mô hình mã nguồn mở hiệu suất cao.

Tài liệu tham khảo
[Các tài liệu tham khảo được liệt kê theo định dạng gốc...]

--- TRANG 13 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 14 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 15 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 16 ---
[Tiếp tục danh sách tài liệu tham khảo...]

A Tài nguyên huấn luyện
Huấn luyện đầy đủ của nomic-embed-text-v1 có thể được thực hiện trong một tuần trên một node 8xH100. Mô hình hóa ngôn ngữ có mặt nạ của nomic-bert-2048 mất khoảng 4 ngày. Tiền huấn luyện đối phó kéo dài 3 ngày rưỡi. Tinh chỉnh đối phó mất một giờ. Chúng tôi khuyến khích người đọc khởi tạo từ nomic-bert-2048 hoặc các checkpoint Đối phó Không giám sát của chúng tôi, được phát hành theo cùng giấy phép như nomic-embed-text-v1.

B Phân phối tập dữ liệu tiền huấn luyện
Các tập dữ liệu tiền huấn luyện đối phó có giám sát yếu được chi tiết trong Bảng 7. Đây là số lượng điểm dữ liệu mỗi nguồn sau khi lọc tính nhất quán.

--- TRANG 17 ---
Bảng 7: Phân phối tập dữ liệu không giám sát yếu

Tập dữ liệu | Điểm dữ liệu | % Tập dữ liệu
Reddit^a | 64,978,944 | 0.28
PAQ (Lewis et al., 2021b) | 52,953,088 | 0.23
Amazon Reviews (Ni et al., 2019) | 38,682,624 | 0.16
S2ORC Title Abstract (Lo et al., 2020) | 35,438,592 | 0.15
WikiAnswers (Fader et al., 2014) | 9,912,320 | 0.04
S2ORC Citation Titles (Lo et al., 2020) | 7,585,792 | 0.03
S2ORC Abstract Citation (Lo et al., 2020) | 7,503,872 | 0.03
S2ORC Abstract Body (Lo et al., 2020) | 6,389,760 | 0.03
Wikipedia Title Body (Foundation) | 6,078,464 | 0.03
Gooaq (Khashabi et al., 2021) | 1,245,184 | 0.01
Codesearch (Husain et al., 2019) | 835,584 | <.01
AGNews (Zhang et al., 2016) | 409,600 | <.01
CCNews (Hamborg et al., 2017) | 344,064 | <.01
NPR^b | 344,064 | <.01
CNN (See et al., 2017) | 278,528 | <.01
Yahoo Title-Answer^c | 262,144 | <.01
AmazonQA (Gupta et al., 2019) | 212,992 | <.01
Yahoo Title-Question^d | 196,608 | <.01
Sentence Compression (Filippova & Altun, 2013) | 163,840 | <.01
YahooQA^e | 131,072 | <.01
ELI5 (Fan et al., 2019) | 98,304 | <.01
Altlex (Hidey & McKeown, 2016) | 98,304 | <.01
Wikihow (Koupaee & Wang, 2018) | 81,920 | <.01
SimpleWiki (Coster & Kauchak, 2011) | 81,920 | <.01
StackExchange Duplicate Questions^f | 65,536 | <.01
StackExchange Title Body^g | 65,536 | <.01
StackExchange Body Body^h | 65,536 | <.01
Quora Duplicate Questions^i | 32,768 | <.01
SQuAD (Rajpurkar et al., 2016) | 16,384 | <.01
Tổng | 234,553,344 | 1

[Các chú thích về nguồn dữ liệu được liệt kê với các URL tương ứng]
