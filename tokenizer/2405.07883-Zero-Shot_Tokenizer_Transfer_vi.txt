# Chuyển Giao Tokenizer Không Cần Mẫu

Benjamin Minixhofer[SEP]Edoardo M. Ponti[CLS]Ivan Vuli ´c[SEP]
[SEP]Đại học Cambridge[CLS]Đại học Edinburgh

Tóm tắt
Các mô hình ngôn ngữ (LMs) bị ràng buộc với tokenizer của chúng, cái mà ánh xạ văn bản thô thành một chuỗi các mục từ vựng (tokens). Điều này hạn chế tính linh hoạt của chúng: ví dụ, các LMs được huấn luyện chủ yếu trên tiếng Anh vẫn có thể hoạt động tốt trong các ngôn ngữ tự nhiên và lập trình khác, nhưng có hiệu suất giảm đáng kể do tokenizer tập trung vào tiếng Anh. Để giảm thiểu điều này, chúng ta nên có khả năng thay thế tokenizer LM gốc bằng một tokenizer tùy ý, một cách linh hoạt, mà không làm giảm hiệu suất. Do đó, trong công trình này chúng tôi định nghĩa một vấn đề mới: Chuyển Giao Tokenizer Không Cần Mẫu (ZeTT). Thách thức cốt lõi của ZeTT là tìm embeddings cho các tokens trong từ vựng của tokenizer mới. Vì các phương pháp heuristic trước đây để khởi tạo embeddings thường hoạt động ở mức ngẫu nhiên trong môi trường ZeTT, chúng tôi đề xuất một giải pháp mới: chúng tôi huấn luyện một hypernetwork nhận tokenizer làm đầu vào và dự đoán các embeddings tương ứng. Chúng tôi chứng minh thực nghiệm rằng hypernetwork tổng quát hóa cho các tokenizer mới với cả encoder LLMs (ví dụ, XLM-R) và decoder LLMs (ví dụ, Mistral-7B). Phương pháp của chúng tôi gần đạt được hiệu suất của các mô hình gốc trong các tác vụ đa ngôn ngữ và lập trình trong khi giảm đáng kể độ dài của chuỗi đã tokenize. Chúng tôi cũng thấy rằng khoảng cách còn lại có thể được đóng lại nhanh chóng bằng cách tiếp tục huấn luyện trên ít hơn 1B tokens. Cuối cùng, chúng tôi chỉ ra rằng một hypernetwork ZeTT được huấn luyện cho một (L)LM cơ sở cũng có thể được áp dụng cho các biến thể đã fine-tuned mà không cần huấn luyện thêm. Nhìn chung, kết quả của chúng tôi tạo ra những bước tiến đáng kể hướng tới việc tách rời LMs khỏi tokenizer của chúng.

1 Giới thiệu

Các Mô hình Ngôn ngữ thường hoạt động trên các discrete tokens, vì vậy chúng cần một phương tiện để ánh xạ văn bản thành một chuỗi tokens, cụ thể là một tokenizer. Phần lớn các LMs đương đại sử dụng subword tokenizers (Devlin et al., 2019; Jiang et al., 2023; Touvron et al., 2023; Parmar et al., 2024, trong số những người khác), trong khi những mô hình khác sử dụng byte-level (Xue et al., 2022; Yu et al., 2023; Wang et al., 2024) hoặc character-level tokenizers (Clark et al., 2022; Tay et al., 2022). Bất kể 'mức độ chi tiết' tokenization được chọn, các mô hình này chia sẻ một hạn chế cơ bản: một khi chúng được huấn luyện với một tokenizer cụ thể, việc suy luận với một tokenizer khác là không thể. Nói cách khác, một LM đã pre-trained bị "ràng buộc" với tokenizer mà nó được huấn luyện. Điều này có những tác động rộng rãi: vì trọng tâm trong quá trình pretraining thường chủ yếu là tiếng Anh, tokenizer thường mã hóa các ngôn ngữ khác ngoài tiếng Anh (Rust et al., 2021) hoặc các lĩnh vực khác, chẳng hạn như code, kém hiệu quả hơn. Điều này dẫn đến sự chênh lệch lớn về chi phí suy luận giữa văn bản tiếng Anh và không phải tiếng Anh (Ahia et al., 2023; Petrov et al., 2023). Tokenizers cũng có thể không tối ưu cho các lĩnh vực mà chúng không được thiết kế để sử dụng, ví dụ fine-tunings của các mô hình Llama hoạt động kém trên các tác vụ lập trình (Dagan et al., 2024). Hiệu suất và hiệu quả chỉ là một số lý do để chuyển giao mô hình qua các tokenizers: các phương pháp tương tác giữa các mô hình, chẳng hạn như ensembling (Sagi & Rokach, 2018) và model merging (Wortsman et al., 2022; Ainsworth et al., 2023; Yadav et al., 2023), thường giả định cùng một đơn vị biểu diễn (tức là tokenization tương đương) trên các mô hình; nếu hai mô hình áp dụng các tokenizers khác nhau, chúng trở nên không phù hợp cho ensembling hoặc merging. Các artifacts có vấn đề của tokenization như 'Glitch tokens' (Land & Bartolo, 2024) cũng có thể được sửa thông qua chuyển giao sang một tokenizer mới.

Để giải quyết những vấn đề này, các công trình trước đã phát triển các phương pháp để trang bị một LM với một tokenizer mới bằng cách huấn luyện lại các tham số embedding, và tùy chọn tiếp tục huấn luyện toàn bộ mô hình (Artetxe et al., 2020; de Vries & Nissim, 2021). Sự thích ứng này có thể được thực hiện nhanh hơn bằng cách khởi tạo các tham số embedding thông qua heuristics (Tran, 2020; Minixhofer et al., 2022; Gee et al., 2022; Dobler & de Melo, 2023; Liu et al., 2023). Trong công trình này, chúng tôi công thức hóa một vấn đề mới: cho một LM, chúng ta có thể tạo một ma trận embedding nhanh chóng cho bất kỳ tokenizer tùy ý nào, mà không bao giờ quan sát dữ liệu cho nó không? Trong khi các công trình trước đã điều tra n-shot tokenizer transfer, chúng tôi gọi vấn đề mới này là zero-shot tokenizer transfer (ZeTT). Nếu hiệu suất của mô hình có thể được bảo toàn gần đúng, ZeTT hiệu quả "tách rời" LMs khỏi tokenizer mà chúng được huấn luyện. Chúng tôi đầu tiên đánh giá hiệu quả của các phương pháp (dựa trên heuristic) trước đây cho ZeTT, thấy rằng, trong khi heuristics có thể bảo toàn hiệu suất ở một mức độ nào đó, thường có một khoảng cách lớn đến hiệu suất LM gốc.

Để thu hẹp khoảng cách này, chúng tôi giới thiệu một paradigm mới: Chúng tôi huấn luyện một hypernetwork trên một phân phối đa dạng các tokenizers để dự đoán các tham số embedding cho bất kỳ tokenizer đã cho nào. Bằng cách đầu tư vào chi phí một lần của việc huấn luyện hypernetwork, chúng tôi nhằm mục đích sau đó cho phép ZeTT hiệu quả. Điều này chứng tỏ là có thể: ZeTT thông qua hypernetwork bảo toàn hiệu suất đến vài phần trăm độ chính xác trong nhiều trường hợp. Hơn nữa, hypernetwork có thể học cách thích ứng nhanh chóng với một target tokenizer đã cho bằng cách tiếp tục huấn luyện trên một lượng nhỏ (<1B) tokens bổ sung, trong khi các công trình trước thường cần hàng trăm tỷ tokens (Dagan et al., 2024). Như vậy, hypernetwork của chúng tôi cung cấp một giải pháp tiên tiến cho n-shot tokenizer transfer, đồng thời thiết lập một baseline cạnh tranh cho vấn đề zero-shot tokenizer transfer mới được giới thiệu của chúng tôi. Điều này mở ra một loạt các cách mới để kết hợp language models với tokenizers. Ví dụ, trong công trình này, chúng tôi zero-shot thay thế tokenizer Mistral-7B (Jiang et al., 2023) bằng một tokenizer mã hóa code sử dụng ít hơn 10% tokens trung bình, trong khi bảo toàn độ chính xác tạo code chức năng đến khoảng 3% (Phần 4.2). Chúng tôi cũng đánh giá zero-shot cross-lingual transfer của mô hình encoder đa ngôn ngữ XLM-R sang một loạt các ngôn ngữ khác nhau bằng cách thay thế tokenizer XLM-R bằng một tokenizer dành riêng cho ngôn ngữ đích và tái sử dụng các adapters được huấn luyện cho XLM-R gốc. Điều này dẫn đến tăng tốc >16% và bảo toàn hiệu suất trên XNLI (Conneau et al., 2018) đến 1% trung bình, mặc dù language model chưa bao giờ được huấn luyện với các tokenizers ngôn ngữ đích. Cuối cùng, chúng tôi chỉ ra rằng một hypernetwork được huấn luyện cho một base large LM (ví dụ Mistral-7B) cũng có thể được áp dụng cho các fine-tunings của cùng mô hình đó (ví dụ Mistral-7B-Instruct-v0.1), bảo toàn khả năng ở mức độ lớn (Phần 4.3). Code và models của chúng tôi có sẵn công khai tại github.com/bminixhofer/zett.

2 Nền tảng

Tokenizers và Embeddings. Tokenizers hoạt động như một hàm tokenization T ánh xạ một văn bản thành một chuỗi các phần tử trong từ vựng V. Bằng thuật ngữ tokenizer, chúng tôi sau đây đề cập đến tuple bao gồm hai thành phần quan trọng, (V, T). Quan trọng là, từ vựng và hàm tokenization là các thành phần riêng biệt; cho một từ vựng nào đó, có nhiều cách để mã hóa văn bản như một chuỗi tokens trong từ vựng này (ví dụ Hofmann et al., 2022; Uzan et al., 2024). Sau tokenization, mô hình biểu diễn chuỗi tokens thông qua một hàm Eϕ:V →Rdmodel (các embeddings). Embeddings thường được tham số hóa bởi một ma trận ϕ như một bảng tra cứu gán một vector dmodel-chiều riêng biệt (một hàng của ma trận) cho mỗi phần tử trong V. Embeddings được sử dụng hai lần trong language model: một lần ở đầu vào để ánh xạ tokens thành một vector kích thước cố định, và lần nữa ở đầu ra để tính toán một logit cho mỗi token, thường thông qua một dot-product của Eϕ(t) với hidden state cuối cùng của LM. Các tham số embedding có thể được chia sẻ hoặc không được chia sẻ giữa đầu vào và đầu ra; phương pháp của chúng tôi hoạt động với cả hai. Chúng tôi ký hiệu toàn bộ tập hợp các tham số embedding thông qua ϕ, ký hiệu input embeddings là ϕin và output embeddings là ϕout, nếu cần thiết.

Các language models đương đại thường sử dụng subword tokenizers thông qua BPE (Sennrich et al., 2016) hoặc UnigramLM (Kudo, 2018). Subword tokenization là một lựa chọn phổ biến vì nó có thể biểu diễn các chuỗi văn bản tùy ý ("open-vocabulary" language modeling) trong khi phần lớn giữ lại hiệu quả của các word-level models (Mielke et al., 2021). Tuy nhiên, có một số vấn đề với subword tokenization, ví dụ các mô hình sử dụng subword tokenization gặp khó khăn trong việc phân tích các chuỗi số (Golkar et al., 2023) và văn bản có lỗi chính tả (Xue et al., 2022). Một hướng nghiên cứu gần đây nhằm loại bỏ subword tokenization thông qua các mô hình byte-level (được gọi là "token-free") (Xue et al., 2022; Yu et al., 2023). Tuy nhiên, các mô hình này vẫn hoạt động trên tokens, sử dụng tập hợp 256 bytes làm từ vựng, và Unicode làm hàm tokenization (Mielke et al., 2021). Theo cách tương tự, một số mô hình sử dụng character-level tokenization (Tay et al., 2022; Clark et al., 2022), tùy chọn học cách gộp các characters thành tokens dài hơn (Nawrot et al., 2023). Cho đến nay, các phương pháp byte- hoặc character-level đã không thể thay thế subword tokenization do hiệu quả tính toán giảm (vì chuỗi dài hơn), và không nhất thiết robust hơn (Libovický et al., 2022). Do đó, mặc dù phương pháp của chúng tôi về nguyên tắc có thể áp dụng cho bất kỳ tokenizer nào, chúng tôi tập trung các thí nghiệm vào subword tokenizers. Cụ thể, chúng tôi sử dụng tham số hóa UnigramLM của hàm tokenization, và chỉ ra rằng các tokenizers khác có thể được chuyển đổi sang tham số hóa này sau đó trong Phần 5. UnigramLM đặt

T(x) := argmax_{C∈C_x} ∑_{t∈C} log p(t)

trong đó C_x là tập hợp tất cả các tokenizations có thể của x (tức là, tất cả các phân tách có thể của x trong V). UnigramLM cung cấp một cách tiện lợi để biểu diễn tokens như một 2-tuple (t, p(t)) trong (V,R).

Heuristics Khởi tạo Embedding. Các công trình trước chuyển giao LMs sang một tokenizer mới bằng cách khởi tạo các tham số embedding thông qua một heuristic, sau đó tiếp tục huấn luyện embeddings. Chúng tôi ký hiệu tokenizer gốc là (V_a, T_a) và các tham số embedding gốc là ϕ_a. Tương tự, target tokenizer là (V_b, T_b) với các tham số embedding ϕ_b. FVT (Gee et al., 2022) khởi tạo embeddings cho bất kỳ token mới t trong V_b như trung bình của embeddings của T_a(t) tức là trung bình của chuỗi embeddings mà token mới được phân tách thành bởi tokenizer trước T_a. RAMEN (Tran, 2020), WECHSEL (Minixhofer et al., 2022) và OFA (Liu et al., 2023) yêu cầu auxiliary embeddings E_aux:V_aux→R^d_aux với |V_aux ∩ V_a| ≪ |V_a| và |V_aux ∩ V_b| ≪ |V_b|. Chúng sử dụng E_aux để embed tokens trong V_a và V_b trong cùng một không gian semantic, sau đó khởi tạo embeddings trong E_ϕ_b như một trung bình có trọng số của embeddings trong E_ϕ_a với trọng số được cho bởi sự tương tự của chúng trong E_aux. FOCUS (Dobler & de Melo, 2023) khởi tạo embeddings của tokens trong V_b\V_a như một kết hợp có trọng số của các overlapping tokens V_a ∩ V_b, và sao chép embeddings của các overlapping tokens. Trọng số lại được tính toán sử dụng một auxiliary embedding matrix E_aux, nhưng yêu cầu duy nhất là |V_aux ∩ V_b| ≪ |V_b|. Chúng tôi sử dụng FOCUS làm baseline chính vì Dobler & de Melo (2023) chỉ ra nó đạt được hiệu suất tốt hơn mà không cần huấn luyện (tức là, zero-shot) so với các heuristics khác, điều mà chúng tôi cũng xác nhận sau đó trong Phần 4.2.

Tokenizer Transfer Không Heuristic. Trong khi một lượng đáng kể công trình trước đã điều tra heuristics để khởi tạo embedding layer, cũng có nghiên cứu về việc thay đổi quy trình huấn luyện để tạo điều kiện cho n-shot tokenizer transfer. Marchisio et al. (2023) chỉ ra rằng forward- và backward-propagating thông qua một tập con các model layers là đủ để học embeddings cho một tokenizer mới. Chen et al. (2023) thấy rằng việc reset thường xuyên các tham số embedding trong quá trình pretraining tăng tốc độ mà chúng được học lại khi transfer. Các phương pháp này có thể được xem là trực giao với của chúng tôi. Chúng có thể được kết hợp tự do với phương pháp của chúng tôi; chúng tôi để điều này cho công việc tương lai.

Hypernetworks Dự đoán Embedding. Hypernetworks là các mạng dự đoán các tham số của một mạng khác (Ha et al., 2017). Các công trình trước sử dụng neural networks để dự đoán embeddings cho các từ out-of-vocabulary (Pinter et al., 2017) hoặc rare words (Schick & Schütze, 2019) của các word embedding models (Mikolov et al., 2013). Schick & Schütze (2020) mở rộng phương pháp này để dự đoán embeddings cho các rare words trong các BERT models (Devlin et al., 2019). Các phương pháp này cũng có thể được xem như là embedding prediction hypernetworks. Ngược lại, hypernetwork mà chúng tôi đề xuất (i) tiếp cận vấn đề tổng quát hơn của việc chuyển giao sang một tokenizer tùy ý, thay vì mở rộng tokenizer gốc và (ii) có thể được áp dụng cho encoder, decoder, và encoder-decoder LMs, tức là, nó là objective-agnostic.

3 Phương pháp

3.1 Huấn luyện Hypernetwork

Chúng tôi nhằm tìm tham số θ của một hypernetwork H_θ: (V_b, T_b) → ϕ_b cho một LM đã pretrained nào đó. Để ϕ_a và ψ là embedding và các tham số inner (non-embedding) của language model, tương ứng. L là loss của language model như một hàm của tokens, các tham số embedding, và các tham số inner, thường:

L(t, ϕ_a, ψ) = CrossEntropy(LM_ψ(E_ϕ_a(t)), label(t)),

trong đó LM_ψ là language model và label ánh xạ chuỗi tokens đến các labels tương ứng, ví dụ, shifting chuỗi trong trường hợp standard (autoregressive, causal) language modeling, hoặc masking chuỗi trong trường hợp Masked Language Modeling (Devlin et al., 2019). Quan trọng là, tuy nhiên, chúng tôi không đưa ra bất kỳ giả định cụ thể nào về L.

Lưu ý rằng loss của language model dưới tokenizer gốc T_a trên một văn bản x là L(T_a(x), ϕ_a, ψ). Chúng tôi huấn luyện hypernetwork của chúng tôi để tối thiểu hóa loss L_θ(T_b(x), H_θ(V_b, T_b), ψ). Tức là, chúng tôi thay thế các tham số embedding gốc bằng các dự đoán hypernet, và thay thế tokenizer gốc bằng một tokenizer (V_b, T_b). Hình 1 minh họa luồng thông tin.

Định nghĩa Phân phối trên Texts và Tokenizers. Chúng tôi tuân theo thực hành tiêu chuẩn và lấy mẫu texts đều từ training corpus. Lấy mẫu tokenizer không đơn giản như vậy: chúng tôi muốn một phân phối trên tokenizers (V_b, T_b) với phương sai cao để khuyến khích tổng quát hóa cho các tokenizers chưa thấy. Để làm điều này, chúng tôi giới thiệu một quy trình để lấy mẫu một tập hợp đa dạng các UnigramLM tokenizers. Chúng tôi chỉ ra sau đó trong Phần 5 rằng các tokenizers tùy ý có thể được xấp xỉ tốt thông qua UnigramLM, biện minh cho lựa chọn này.

Chúng tôi ban đầu điền một queue q với n texts được lấy mẫu ngẫu nhiên từ training corpus và, tại mỗi bước trong training loop, đẩy m texts trong batch hiện tại và loại bỏ m texts được thêm vào ít gần đây nhất. Sau đó chúng tôi tính toán tất cả substrings t lên đến độ dài l và tần suất của chúng trong q. Chúng tôi thêm Gaussian noise vào tần suất để đến điểm số cuối cùng p(t) cho mỗi token t. Cuối cùng, chúng tôi lắp ráp tokenizer bằng cách lấy top k tokens với p(t) cao nhất làm từ vựng và UnigramLM được tham số hóa bởi p(t) làm hàm tokenization. Training loop được tóm tắt trong Algorithm 1. Queue 'rolling' của texts q đảm bảo phương sai cao trong từ vựng, trong khi Gaussian noise được thêm vào tần suất đảm bảo phương sai cao trong hàm tokenization.

Quan trọng là, texts và tokenizer được lấy mẫu phụ thuộc: batch của m texts được sử dụng để huấn luyện là một tập con của n texts được sử dụng để lấy mẫu tokenizer. Nếu chúng được lấy mẫu độc lập, xác suất để một token xuất hiện sẽ là p(token) ∝ p(token trong V_b) × p(token trong x). Vì cả hai yếu tố này đều nhỏ đối với rare tokens, p(token) sẽ trở nên vô cùng nhỏ trong trường hợp này.

MIMICK-Style Warmup & Auxiliary Loss. Trong thực tế, việc tối thiểu hóa trực tiếp L_θ bắt đầu từ θ được khởi tạo ngẫu nhiên là khó khăn. Do đó, chúng tôi bao gồm một giai đoạn warmup trong đó chúng tôi huấn luyện hypernetwork để bắt chước các tham số embedding của tokenizer gốc, giống như MIMICK (Pinter et al., 2017).

L^warmup_θ = ||H_θ(V_a, T_a) - ϕ_a||^2

Giai đoạn warmup nhanh hơn đáng kể so với giai đoạn chính vì không cần propagate qua main model. Chúng tôi thấy nó ngăn chặn divergence trong một số trường hợp. Sau đó, chúng tôi thêm một auxiliary loss, mà, đối với mỗi token trong từ vựng được lấy mẫu V_b cũng tồn tại trong từ vựng gốc V_a, phạt khoảng cách đến embedding tương ứng trong ϕ_a.

L^aux_θ = 1/|V_a ∩ V_b| ∑_{t∈|V_a ∩ V_b|} ||H_θ(V_b, T_b)[V_b[t]] - ϕ_a[V_a[t]]||^2

Điều này phạt sự trôi dạt từ giai đoạn warmup. Kết hợp nó với main loss tạo ra final loss.

L^final_θ = L_θ(T_b(x), H_θ(V_b, T_b), ψ) + α · L^aux_θ

Hyperparameter α cân đối đóng góp của auxiliary loss. Vì H_θ(V_b, T_b) cũng được yêu cầu cho main loss, nó yêu cầu tính toán bổ sung không đáng kể. Auxiliary loss là cần thiết đặc biệt cho các mô hình với input và output embedding matrices riêng biệt như được hiển thị trong Phụ lục B.

3.2 Kiến trúc Hypernetwork

Vẫn còn việc định nghĩa kiến trúc hypernetwork, tức là, cách ánh xạ tokenizer (V_b, T_b) đến các tham số embedding ϕ_b. Để làm điều này, chúng tôi biểu diễn các tokens mới t_b trong V_b bằng cách phân tách chúng sử dụng hàm tokenization gốc T_a, và embedding chúng với embeddings gốc E_ϕ_a. Chuỗi embeddings này được truyền qua nhiều Transformer layers, cộng với một prediction head riêng biệt cho input embeddings và output embeddings ϕ^in_b và ϕ^out_b. Hypernetwork do đó bao gồm một language model khác được áp dụng riêng biệt cho mỗi token. Chúng tôi gọi language model của hypernetwork là HLM_θ. HLM_θ có thể được nghĩ là học cách compose embeddings dưới tokenization gốc thành một embedding mới và amortizes qua hàm tokenization. Figure 2 minh họa luồng thông tin.

Quan trọng là, chúng tôi không tính đến hàm tokenization. Bằng cách lấy mẫu các tokenizers đa dạng trong quá trình huấn luyện, chúng tôi nhằm mục đích cho hypernetwork học tạo ra một embedding duy nhất phù hợp với nhiều loại hàm tokenization khác nhau. Chúng tôi phân tích tác động của lựa chọn này sau đó trong Phần 5. Chúng tôi cũng thử nghiệm với các hypernetworks có tính đến hàm tokenization trong Phụ lục C.

Về Token Decomposition. Đầu vào cho hypernetwork bao gồm chuỗi tokens T_a(t) mà bất kỳ token đã cho nào được phân tách thành. Tuy nhiên, sự phân tách này không phải lúc nào cũng đơn giản: ví dụ, T_a có thể là character-level, trong khi token t có thể nằm trong từ vựng của một byte-level tokenizer T_b. Trong trường hợp này, t có thể là bất kỳ chuỗi bytes tùy ý nào (không nhất thiết là UTF-8 hợp lệ). Để giải quyết vấn đề này, chúng tôi giới thiệu một quy trình để chuyển đổi tokenizers sang byte level bằng cách thêm một lượng nhỏ tokens bổ sung vào từ vựng (c.f. Phần 5). Điều này đảm bảo rằng T_a có thể phân tách các tokens tùy ý. Embeddings của từ vựng bổ sung được khởi tạo ngẫu nhiên và có thể huấn luyện cùng với các tham số hypernetwork.

4 Thí nghiệm

4.1 Thiết lập

Dữ liệu. Chúng tôi sử dụng tập con tiếng Anh của corpus MADLAD-400 (Kudugunta et al., 2023) và code từ dữ liệu StarCoder (Li et al., 2023) để huấn luyện hypernetwork. Tỷ lệ lấy mẫu của English đối với Code là 7:3 theo Zhang et al. (2024). Đối với hypernetwork đa ngôn ngữ, chúng tôi sử dụng một tập con 26 ngôn ngữ được sử dụng trong XGLM (Lin et al., 2022) với dữ liệu từ MADLAD-400. Chúng tôi lấy mẫu ngôn ngữ sử dụng phân phối đa thức như trong Conneau & Lample (2019) với α = 0.1. Đối với các thí nghiệm n-shot, chúng tôi cũng huấn luyện trên dữ liệu StarCoder, nhưng thay thế phần tiếng Anh của corpus MADLAD-400 bằng Flan v2 (Longpre et al., 2023) được lấy mẫu như trong Soldaini et al. (2024).

Đánh giá. Chúng tôi sử dụng các benchmarks tiêu chuẩn PiQA (Bisk et al., 2020), HellaSwag (HS Zellers et al., 2019), BoolQ (Clark et al., 2019), MMLU (Hendrycks et al., 2021) và tập con "easy" của ARC (Clark et al., 2018) để đánh giá bằng tiếng Anh và tác vụ synthesis của HumanEvalPack (Muennighoff et al., 2023) để đánh giá coding. Đối với đánh giá đa ngôn ngữ, chúng tôi sử dụng XNLI (Conneau et al., 2018), XCOPA (Ponti et al., 2020) và MMLU như được dịch máy bởi Lai et al. (2023).

Mô hình. Để đánh giá phương pháp của chúng tôi, chúng tôi sử dụng Mistral-7B (Jiang et al., 2023) làm decoder-style language model chính và XLM-R (Conneau et al., 2020) như một đại diện của encoder-style models. Chúng tôi cũng thử nghiệm với mô hình TinyLlama-1.1B nhỏ hơn (Zhang et al., 2024) trong Phụ lục G.

Tokenizers. Chúng tôi chuyển giao mô hình sang tokenizer GPT2 (Radford et al., 2019) để đánh giá trên các benchmarks ngôn ngữ tự nhiên và sang tokenizer StarCoder (Li et al., 2023) để đánh giá trên các benchmarks code. Đối với đánh giá đa ngôn ngữ, chúng tôi huấn luyện các tokenizers đơn ngôn ngữ dành riêng cho ngôn ngữ với kích thước từ vựng 50k sử dụng SentencePiece (Kudo & Richardson, 2018) và đánh giá chuyển giao sang chúng. Chúng tôi cũng xác minh rằng hypernetwork robust với lựa chọn kích thước từ vựng trong Phụ lục E.

Huấn luyện Hypernetwork. Chúng tôi huấn luyện hypernetwork trong 200k gradient update steps (10k trong số đó là MIMICK-style warmup) với batch size 128 tokens và sequence length 128 (chúng tôi thấy đủ để sử dụng độ dài chuỗi ngắn để học các tham số embedding). Đối với các decoder-style models đa ngôn ngữ, chúng tôi bắt đầu từ checkpoint English + Code và bỏ qua MIMICK-style warmup, giữ các hyperparameters khác không thay đổi. Chúng tôi sử dụng kiến trúc RoBERTa-style tức là bidirectional attention và Post-LayerNorm Transformer layers (Liu et al., 2019), nhưng sử dụng feedforward dimension 2x hidden dimension (thay vì 4x của RoBERTa) cho hypernetwork. Xem Phụ lục D để có danh sách đầy đủ các hyperparameters.

Chi tiết Continued training. Để giữ runtime có thể so sánh giữa việc huấn luyện mô hình với hypernetwork và direct training (không có hypernetwork), chúng tôi chạy hypernetwork inference chỉ cho một tập con k = 16384 tokens trong trường hợp continued training. Tập con bao gồm tất cả tokens xuất hiện trong batch, cộng với một mẫu đều của những tokens không xuất hiện. Language modeling loss sau đó chỉ được tính toán trên tập con tokens này. Chúng tôi thấy trong các thí nghiệm sơ bộ rằng điều này chỉ gây ra degradation hiệu suất nhỏ. Hơn nữa, chúng tôi sử dụng các zero-shot predicted embeddings làm target cho auxiliary loss thay vì sử dụng embeddings gốc. Điều này ổn định quá trình huấn luyện. Chúng tôi huấn luyện trong 50k steps với batch size 32 và sequence length 512, dẫn đến 'thấy' 819.2M tokens.

4.2 Kết quả Zero-Shot và n-shot

Kết quả cho XLM-R được hiển thị trong Bảng 1. Chúng tôi lấy task adapters được huấn luyện cho mô hình XLM-R gốc trên dataset XNLI tiếng Anh thông qua Poth et al. (2023) và thay thế tokenizer bằng tokenizer dành riêng cho ngôn ngữ của chúng tôi. Chúng tôi so sánh hypernetwork của chúng tôi với một baseline lexical đơn giản (sao chép embeddings của overlapping tokens và khởi tạo phần còn lại ngẫu nhiên), FVT, OFA, và FOCUS (c.f. Phần 2). Chúng tôi chỉ tập trung vào FOCUS trong phần tiếp theo vì nó hoạt động tốt nhất trong số các baselines. Hypernetwork của chúng tôi nhất quán vượt trội tất cả baselines và bảo toàn độ chính xác đến 1% trung bình, mất 3% trong trường hợp xấu nhất và cải thiện 1% trong trường hợp tốt nhất, trong khi các chuỗi trung bình ngắn hơn 14% đối với các tokenizers dành riêng cho ngôn ngữ; suy luận do đó nhanh hơn 16%.

Bảng 2 hiển thị kết quả về English và Code cho Mistral-7B. Chúng tôi thấy rằng ZeTT thách thức hơn trong trường hợp decoder: FOCUS hoạt động gần như ngẫu nhiên trong trường hợp xấu nhất (-23.2% trên BoolQ) và giảm xuống 0% pass@1 trên HumanEval trong Python. Hypernetwork đi một chặng đường dài trong việc thu hẹp khoảng cách này nhưng vẫn tụt lại phía sau trên một số benchmarks. Tuy nhiên, tiếp tục huấn luyện hypernetwork với target tokenizer thu hẹp khoảng cách gần như hoàn toàn. Thực tế, continued training trên 800M tokens với tokenizer StarCoder hoạt động tốt hơn continued training cho cùng số lượng tokens với tokenizer gốc, có thể vì tokenizer StarCoder phù hợp hơn với code; nó dẫn đến khoảng 10% ít tokens hơn trung bình. Ngoài ra, đáng chú ý là continued training với tokenizer gốc hơi làm giảm hiệu suất trung bình; điều này có thể do data mix chất lượng cao hơn được sử dụng để pretraining Mistral-7B, trong khi chúng tôi sử dụng các nguồn dữ liệu công cộng (c.f. Phần 4.1).

Kết quả của hypernetwork đa ngôn ngữ cho Mistral-7B được hiển thị trong Bảng 3 và Bảng 4. Trên XCOPA, hypernetwork trung bình cải thiện hiệu suất so với mô hình gốc, đồng thời cũng giảm hơn một nửa độ dài chuỗi. Hiệu suất XCOPA gần như ngẫu nhiên trong một số ngôn ngữ (ví dụ Southern Quechua (qu) và Estonian (et)), vì vậy chúng tôi cũng đánh giá trên MMLU đa ngôn ngữ. Ở đây, mặc dù hypernetwork rõ ràng vượt trội FOCUS (hoạt động gần như ngẫu nhiên), vẫn có một khoảng cách đáng kể đến mô hình gốc; điều này có thể được sửa chữa thông qua continued training.

4.3 Áp dụng Hypernetwork được huấn luyện cho Base Model lên Fine-Tuned Models

Cho đến nay, chúng tôi đã chỉ ra rằng hypernetwork có thể được áp dụng thành công để chuyển giao tokenizer của base model mà nó được huấn luyện. Tuy nhiên, một lượng lớn các mô hình được sử dụng bởi người thực hành là các phiên bản fine-tuned của base models, ví dụ thông qua SFT hoặc RLHF (Ouyang et al., 2022). Bây giờ chúng tôi cố gắng trả lời câu hỏi: Cho một hypernetwork được huấn luyện cho một base model, chúng ta có thể áp dụng hypernetwork này cho các phiên bản fine-tuned của cùng mô hình mà không cần huấn luyện bổ sung không? Điều này sẽ hoạt động như một hệ số nhân cho khả năng áp dụng của hypernetwork. Đầu tiên, chúng tôi quan sát rằng embedding space của một fine-tuned model tương thích với của base model: các embeddings của fine-tuned Mistral-7B-Instruct-v0.1 có cosine similarity trung bình 98.6% với embedding tương ứng trong base model trong khi cosine similarity trung bình của mean embedding vector là 17.4%. Tính tương thích embedding cũng đúng với các mô hình khác (Phụ lục G). Các dự đoán của một hypernetwork được huấn luyện cho một base model do đó có thể được sử dụng out-of-the-box với các fine-tuned models. Chúng tôi xác minh rằng đây là trường hợp bằng cách đánh giá Mistral-7B-Instruct-v0.1 được chuyển giao sang tokenizer GPT2 trên phiên bản được sửa của MT-Bench (Zheng et al., 2023). Đối với n-shot transfer, vì chúng tôi huấn luyện toàn bộ mô hình, chúng tôi cũng cần một cách để chuyển giao các tham số non-embedding; chúng tôi đạt được điều này thông qua Task Arithmetic (Ilharco et al., 2023). Kết quả được hiển thị trong Bảng 5.

Mô hình fine-tuned được chuyển giao hoạt động tốt, đến trong khoảng 0.5 điểm của mô hình gốc. Ngoài ra, thú vị là fine-tuned model với tokenizer gốc hoạt động tốt hơn khi sử dụng embeddings của base model (không fine-tuned); điều này có thể là một hướng thận trọng cho công việc tương lai.

5 Thảo luận

Chuyển đổi tokenizers sang byte-level. Như per Phần 3.2, chúng tôi cần một quy trình để chuyển đổi tokenizers sang byte level để đảm bảo rằng token decomposition luôn có thể. Điều này đơn giản trong hầu hết các trường hợp; các bytes chỉ cần được thêm vào từ vựng. BPE là một ngoại lệ: ở đây, chúng tôi cần thay đổi các atomic units mà merges được định nghĩa từ characters sang bytes. Điều này có thể đạt được bằng cách thêm merges để lắp ráp các characters được sử dụng bởi tokenizer từ các bytes thành phần của chúng vào đầu bảng merge. Chúng tôi đo thành công của việc chuyển đổi sang byte level như xác suất rằng, cho một pretoken được lấy mẫu từ một corpus, pretoken này dẫn đến cùng chuỗi token trong tokenizer gốc và được chuyển đổi. Kết quả được hiển thị trong Bảng 6.

Chuyển đổi tokenizers sang UnigramLM. Chúng tôi cũng giới thiệu một quy trình để chuyển đổi các tokenizers tùy ý sang tokenizers sử dụng UnigramLM làm hàm tokenization. Chúng tôi gọi quá trình này là unigramifying (chi tiết trong Phụ lục A). Một giả định quan trọng của việc huấn luyện hypernetwork là bằng cách sử dụng tham số hóa UnigramLM với điểm số được phân phối như Gaussians, chúng ta có thể bao phủ một phân phối đủ đa dạng của tokenizers để cho phép hypernetwork tổng quát hóa sang ví dụ BPE tokenizers. Unigramifying cho phép chúng tôi kiểm tra xem, về nguyên tắc, điều này có thể không. May mắn thay, chúng tôi thấy rằng nó là: unigramifying dẫn đến degradation hiệu suất tối thiểu khi thay thế tokenizer gốc bằng tokenizer UnigramLM tương ứng (Bảng 6). Mặc dù điều này không đảm bảo rằng phân phối tokenizers của chúng tôi đủ đa dạng, các kết quả thực nghiệm của chúng tôi gợi ý rằng nó là (cf. Phần 4.2).

Chúng tôi tin rằng các phương pháp chuyển đổi của chúng tôi sang UnigramLM và sang byte-level sẽ đơn giản hóa nghiên cứu sâu hơn về tokenizer transfer, chỉ ra rằng bối cảnh đa dạng hóa của tokenizers có thể được xấp xỉ tốt thông qua byte-level UnigramLM tokenizers.

Tác động của việc amortizing qua hàm tokenization là gì? Như được mô tả trước đó trong Phần 3, chúng tôi 'amortize' qua hàm tokenization, tức là, hàm tokenization không phải là đầu vào cho hypernetwork của chúng tôi. Chúng tôi thấy rằng các amortized embeddings được dự đoán robust với lựa chọn hàm tokenization. Ví dụ, tập hợp embeddings được dự đoán cho từ vựng GPT2 có bits-per-character thấp cho cả hàm tokenization GPT2 gốc và một hàm tokenization UnigramLM khác với điểm số dựa trên tần suất token (Bảng 7). Điều này không đúng với embeddings GPT2 gốc: trong khi chúng (như mong đợi) hoạt động tốt với tokenizer GPT2 gốc, có degradation hiệu suất đáng kể khi chuyển sang hàm tokenization UnigramLM dựa trên tần suất. Điều này đặt câu hỏi về các công trình trước sao chép embeddings của overlapping tokens để chuyển giao qua tokenizers (Dobler & de Melo, 2023; Gee et al., 2022, trong số những người khác), chỉ ra rằng ngay cả khi có một token trùng khớp chính xác trong tokenizer gốc, nó không nhất thiết là khởi tạo tối ưu của token tương ứng trong tokenizer mới.

Mặc dù chúng tôi amortize qua hầu hết các khía cạnh của hàm tokenization, trong thực tế, các hàm tokenization dựa vào một lượng đáng kể engineering, vì vậy không thể amortize qua mọi thứ; chúng tôi thảo luận các giả định còn lại trong Phụ lục H.

Phân tích computational overhead của hypernetwork. Chúng tôi ước tính FLOPs per token của nhiều hypernetworks trong Bảng 8. Cho batch size n và sequence length s cho main model, và sử dụng hypernetwork để compose k token sequences của độ dài t, FLOPs per batch sẽ là n×s×(FLOPs/token)_main + k×t×(FLOPs/token)_hypernet. Lấy hypernet training cho Mistral-7B làm ví dụ với n=s=128, k=32768 và t=7 FLOPs per batch sẽ là 252T + 30T tức là 12% overhead từ việc áp dụng hypernet. Đáng chú ý, chúng tôi quan sát trong các thí nghiệm sơ bộ rằng kích thước hypernetwork ba layers là đủ, bất kể kích thước mô hình, vì vậy overhead tương đối giảm với số lượng layers tăng trong main model (như cũng rõ ràng trong Bảng 8).

6 Kết luận

Chúng tôi đã thiết lập Zero-Shot Tokenizer Transfer (ZeTT), vấn đề khó khăn của việc chuyển giao language models sang một tokenizer mới mà không cần huấn luyện. Chúng tôi đã thấy rằng các heuristics trước đây để khởi tạo embedding cung cấp một baseline đầu tiên cho ZeTT, nhưng thiếu sót trong nhiều trường hợp. Để thiết lập một baseline mạnh hơn nhiều, chúng tôi đã giới thiệu một phương pháp dựa trên hypernetwork thu hẹp khoảng cách ở mức độ lớn, và có thể được cải thiện thêm thông qua continued training trên vài (<1B) tokens. Do bảo toàn embedding space của mô hình gốc, ZeTT có thể được áp dụng cho ví dụ tái sử dụng adapters được huấn luyện cho mô hình gốc với một tokenizer khác, và chuyển giao fine-tuned models sang một tokenizer mới sử dụng hypernetwork được huấn luyện cho base model. Tổng hợp, công trình này là một bước quan trọng hướng tới việc tách rời language models khỏi tokenizer của chúng, tăng tính linh hoạt và khả năng tái sử dụng của chúng.
