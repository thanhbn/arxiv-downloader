# 2103.06874.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/tokenizer/2103.06874.pdf
# Kích thước tệp: 483440 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CANINE: Huấn luyện trước một Bộ mã hóa Hiệu quả Không cần Token hóa
cho Biểu diễn Ngôn ngữ
Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting
Google Research
{jhclark,dhgarrette,iuliaturc,jwieting}@google.com

Tóm tắt
Các hệ thống NLP theo pipeline phần lớn đã bị thay thế bởi mô hình hóa neural end-to-end, tuy nhiên gần như tất cả các mô hình thường dùng vẫn yêu cầu một bước token hóa rõ ràng. Trong khi các phương pháp token hóa gần đây dựa trên từ vựng subword có nguồn gốc từ dữ liệu ít giòn hơn so với các tokenizer được thiết kế thủ công, những kỹ thuật này không phù hợp bằng nhau với tất cả các ngôn ngữ, và việc sử dụng bất kỳ từ vựng cố định nào có thể hạn chế khả năng thích ứng của mô hình. Trong bài báo này, chúng tôi trình bày CANINE, một bộ mã hóa neural hoạt động trực tiếp trên chuỗi ký tự - không cần token hóa hoặc từ vựng rõ ràng - và một chiến lược huấn luyện trước hoạt động trực tiếp trên ký tự hoặc tùy chọn sử dụng subword như một inductive bias mềm. Để sử dụng đầu vào chi tiết hơn một cách hiệu quả và hiệu suất, CANINE kết hợp downsampling, làm giảm độ dài chuỗi đầu vào, với một ngăn xếp transformer sâu, mã hóa ngữ cảnh. CANINE vượt trội hơn một mô hình mBERT tương đương 5.7 F1 trên TYDIQA, một benchmark đa ngôn ngữ thách thức, mặc dù có ít tham số mô hình hơn.

1 Giới thiệu
Các mô hình neural end-to-end đã thay thế pipeline NLP truyền thống, và cùng với đó là các error cascade và feature engineering phổ biến trong các hệ thống như vậy, thay vào đó ưu tiên để mô hình tự động cảm ứng các biểu diễn tinh vi của riêng nó. Token hóa, tuy nhiên, là một trong số ít di sản từ thời đại đó, với gần như tất cả các mô hình thường dùng ngày nay yêu cầu một giai đoạn tiền xử lý rõ ràng để phân đoạn một chuỗi văn bản thô thành một chuỗi các đầu vào mô hình rời rạc. Nói chung, các tokenizer thường là các hệ thống được xây dựng cẩn thận với các quy tắc đặc thù cho ngôn ngữ, tốn kém, yêu cầu cả feature engineering thủ công và chuyên môn ngôn ngữ học, hoặc các thuật toán dựa trên dữ liệu như Byte Pair Encoding (Sennrich et al., 2016), WordPiece (Wu et al., 2016), hoặc SentencePiece (Kudo and Richardson, 2018) phân tách chuỗi dựa trên tần suất trong corpus, ít giòn hơn và dễ mở rộng hơn, nhưng cuối cùng quá đơn giản để xử lý đúng phạm vi rộng của các hiện tượng ngôn ngữ học không thể được nắm bắt bằng việc phân tách chuỗi đơn thuần (§2.1).

Mức độ tinh vi cần thiết để nắm bắt chính xác toàn bộ phạm vi của các hiện tượng ngôn ngữ học, cùng với tính không khả thi của việc viết các quy tắc như vậy bằng tay trên tất cả ngôn ngữ và lĩnh vực, cho thấy rằng token hóa rõ ràng tự nó là có vấn đề. Ngược lại, một mô hình end-to-end hoạt động trực tiếp trên chuỗi văn bản thô sẽ tránh được những vấn đề này, thay vào đó học cách kết hợp các ký tự riêng lẻ thành các đặc trưng phức tạp tùy ý của riêng nó, với lợi ích tiềm năng cho cả độ chính xác và dễ sử dụng. Mặc dù thay đổi này về mặt khái niệm rất đơn giản - người ta có thể thay thế từ vựng subword trong một mô hình như BERT (Devlin et al., 2019) bằng một từ vựng chỉ gồm các ký tự riêng lẻ - việc làm như vậy dẫn đến hai vấn đề ngay lập tức. Đầu tiên, độ phức tạp tính toán của một transformer (Vaswani et al., 2017), thành phần chính trong BERT cũng như các mô hình khác như GPT (Radford et al., 2019; Brown et al., 2020) và T5 (Raffel et al., 2020), tăng theo bậc hai với độ dài của đầu vào. Vì các mô hình subword tiêu chuẩn có khoảng bốn ký tự trên mỗi subword trung bình, việc tăng 4 lần độ dài chuỗi đầu vào sẽ dẫn đến một mô hình chậm hơn đáng kể. Thứ hai, việc đơn giản chuyển sang từ vựng ký tự mang lại kết quả kém về mặt thực nghiệm (§4.2).

Để cho phép mô hình hóa không cần token hóa vượt qua những trở ngại này, chúng tôi trình bày CANINE. CANINE là một bộ mã hóa ngôn ngữ lớn với một ngăn xếp transformer sâu làm lõi. Đầu vào cho mô hình là chuỗi các ký tự Unicode. Để biểu diễn không gian đầy đủ của các ký tự Unicode mà không cần từ vựng, chúng tôi sử dụng một chiến lược băm. Để tránh sự chậm lại từ việc tăng độ dài chuỗi, CANINE sử dụng các convolution có stride để downsample chuỗi đầu vào xuống độ dài ngắn hơn trước ngăn xếp transformer sâu.

Giống như BERT, chúng tôi huấn luyện trước CANINE trên các tác vụ Masked Language Model (MLM) và Next Sentence Prediction (NSP). Đối với tác vụ MLM, CANINE cung cấp hai tùy chọn:

1. Một loss hoàn toàn ở mức ký tự dự đoán autoregressive các ký tự trong các span bị mask.
2. Một loss dựa trên từ vựng dự đoán danh tính của các token subword bị mask. Quan trọng là, token hóa này chỉ được sử dụng cho loss huấn luyện trước; token không bao giờ được đưa vào bộ mã hóa, và tokenizer cùng từ vựng subword có thể được loại bỏ an toàn sau huấn luyện trước. Điều này hiệu quả chuyển đổi ràng buộc cứng của ranh giới token được tìm thấy trong các mô hình khác thành một inductive bias mềm trong CANINE.

Trong bài viết này, chúng tôi đóng góp:
- bộ mã hóa sâu không cần token hóa được huấn luyện trước đầu tiên;
- một kiến trúc mô hình hiệu quả mã hóa trực tiếp chuỗi ký tự dài với tốc độ tương đương với BERT vanilla; và
- một mô hình không thực hiện token hóa trên đầu vào, tránh được bottleneck thông tin mất mát liên quan đến hầu hết tiền xử lý.

2 Động lực

2.1 Các cạm bẫy ngôn ngữ học của token hóa
Các tokenizer subword là tiêu chuẩn de-facto trong NLP hiện đại (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020). Các thuật toán này bị giới hạn chỉ ở các phép toán phân tách từ đơn giản. Mặc dù đây có lẽ là một phương pháp hợp lý cho một ngôn ngữ có hình thái nghèo nàn như tiếng Anh, nó ít phù hợp hơn nhiều khi đối mặt với các hiện tượng như hình thái kết dính, hình thái không nối tiếp, đột biến phụ âm, hòa âm nguyên âm, v.v.

Ngay cả trong các ngôn ngữ có nguồn lực cao, các mô hình subword vẫn có xu hướng gặp khó khăn trên các lĩnh vực thách thức, như văn bản không chính thức, thường bao gồm lỗi đánh máy, biến thể chính tả, phiên âm, hoặc emoji (O'Connor et al., 2010). BERT, sử dụng token hóa WordPiece, nhạy cảm với việc làm hỏng đầu vào, cả lỗi đánh máy tự nhiên (Sun et al., 2020) và thao tác đối kháng (Pruthi et al., 2019), với một phần mất mát có thể quy cho các chuỗi bị hỏng không còn được bao phủ bởi từ vựng.

Các heuristic có vẻ an toàn được sử dụng bởi các thuật toán này, như phân tách trên khoảng trắng và dấu câu, có vấn đề khi áp dụng cho các ngôn ngữ không sử dụng khoảng cách giữa các từ (Thái, Trung Quốc) hoặc sử dụng dấu câu như chữ cái (Hawaii, Twi). Mặc dù SentencePiece cung cấp tùy chọn bỏ qua phân tách khoảng trắng, nó thường không được sử dụng do hiệu suất thực nghiệm kém.

Các phương pháp từ vựng cố định cũng có thể buộc người mô hình hóa phải chọn giữa các sự đánh đổi tiền xử lý khó khăn: người ta có nên giữ dấu, viết hoa, v.v. và tránh tiền xử lý phá hoại không? - Hay giữ thông tin chính tả như vậy và rủi ro các từ quan trọng rơi ra khỏi từ vựng dựa trên tần suất hoàn toàn do sự hiện diện của nhiều biến thể của các từ tương tự khác? Ví dụ, mBERT ban đầu loại bỏ tất cả dấu phụ, do đó bỏ thông tin thì trong tiếng Tây Ban Nha và gộp nhiều từ không liên quan trong tiếng Việt.

Cuối cùng, việc sử dụng từ vựng cố định trong huấn luyện trước cũng tạo ra các phức tạp cho các tác vụ downstream, sau đó bị ràng buộc với cùng tokenizer và từ vựng được sử dụng cho huấn luyện trước, ngay cả khi nó không phù hợp với lĩnh vực mục tiêu và/hoặc tác vụ cuối. Boukkouri et al. (2020) cho thấy từ vựng WordPiece Wikipedia+BooksCorpus của BERT dẫn đến phân đoạn quá mức khi fine-tuning trên dữ liệu y tế, làm giảm lợi ích của huấn luyện trước như một chiến lược.

2.2 Cho phép khái quát hóa tốt hơn
Giống như Tenney et al. (2019) đã chỉ ra rằng các bộ mã hóa lớn học các yếu tố của pipeline NLP cổ điển, có vẻ tự nhiên để để mô hình khám phá token hóa nữa. Với ý tưởng này, chúng tôi tìm kiếm một phương pháp có thể khái quát hóa tốt hơn ngoài các dạng chính tả gặp phải trong huấn luyện trước.

Về mặt nghiên cứu khoa học, chúng tôi muốn biết liệu chúng ta có thể xây dựng các mô hình học cách kết hợp từ khi thích hợp, và ghi nhớ chúng khi cần ghi nhớ. Các từ vựng lớn có nguồn gốc tần suất một phần giảm thiểu vấn đề này bằng cách đơn giản ghi nhớ nhiều hơn, nhưng ngôn ngữ vốn dĩ yêu cầu các khía cạnh của cả ghi nhớ và kết hợp. Bằng cách xây dựng một mô hình tham gia trực tiếp với những vấn đề này trong quy mô nhỏ của kết hợp từ, chúng tôi hy vọng cho phép công việc tương lai nghiên cứu những vấn đề này ở quy mô lớn hơn như các cấu trúc cụm từ.

Thực tế, khái quát hóa bị cản trở cho các yếu tố từ vựng là các biến thể chính tả nhẹ, trong đó một cái rất không thường xuyên. Giả thuyết, một mô hình có thể ước lượng một embedding rất tốt cho một yếu tố từ vựng phổ biến kitten, nhưng một embedding kém cho yếu tố ít thường xuyên hơn kittens vì mô hình không có kiến thức tiên nghiệm rằng chúng có liên quan. Các embedding hiếm khi được chạm đến trong huấn luyện trước sẽ không được cập nhật nhiều ngoài khởi tạo ngẫu nhiên của chúng.

2.3 Giảm nỗ lực kỹ thuật
Các tokenizer trưởng thành thường bao gồm nhiều năm quy tắc được thiết kế thủ công xung quanh các trường hợp đặc biệt như địa chỉ email, URL, và xử lý từ không biết; ngay cả các tokenizer hiện đại khá tối thiểu cũng bao gồm heuristic phân tách từ ban đầu theo sau bởi một thuật toán cụ thể và từ vựng để phá vỡ thêm các token này thành subword.

Các mô hình được huấn luyện trước hiện đại cũng có nhiều yêu cầu trong suốt vòng đời của chúng: Giữa thời điểm một mô hình được huấn luyện trước, fine-tuned, và phục vụ - có thể cách nhau hàng tháng hoặc năm - trọng số và triển khai mô hình của nó có thể được chuyển đổi để tương thích với một toolkit khác, dữ liệu fine-tuning của nó có thể được token hóa theo cách khác, và phân phối tự nhiên của từ có thể khá khác. Tất cả những điều này đưa ra nhiều cơ hội cho sự không khớp phát sinh giữa token hóa và từ vựng từ huấn luyện trước.

Tuy nhiên, cùng paradigm huấn luyện trước này mang lại một lợi thế cho các mô hình ký tự: truy cập vào nhiều dữ liệu (không giám sát) hơn để học kết hợp từ từ ký tự; mà không có transfer learning, điều này từ lâu đã không thực tế cho nhiều tác vụ có ít dữ liệu giám sát.

3 CANINE

CANINE bao gồm ba thành phần chính: (1) một kỹ thuật không cần từ vựng để embedding văn bản; (2) một mô hình mức ký tự hiệu quả bằng cách downsampling và upsampling; và (3) một phương tiện hiệu quả để thực hiện masked language modeling trên một mô hình mức ký tự.

3.1 Mô hình

CANINE được thiết kế là một biến thể được sửa đổi tối thiểu của ngăn xếp transformer sâu được tìm thấy trong các bộ mã hóa hiện đại như GPT, (m)BERT, XLM, và XLM-R sao cho kiến trúc của nó dễ dàng được áp dụng bởi các mô hình khác trong họ này. Triển khai đơn giản nhất của một mô hình ký tự như vậy sẽ là cung cấp ký tự tại mỗi vị trí thay cho subword. Tuy nhiên, phương pháp này sẽ dẫn đến nhiều vị trí chuỗi hơn đáng kể với cùng văn bản đầu vào, dẫn đến tính toán nhiều hơn tuyến tính trong các lớp feed forward và tính toán nhiều hơn bậc hai trong các lớp self-attention.

Dạng tổng thể của mô hình CANINE là tổ hợp của một hàm downsampling DOWN, một bộ mã hóa chính ENCODE, và một hàm upsampling UP; cho một chuỗi đầu vào của character embedding e∈R^{n×d} với độ dài n và chiều d:

Y_seq = UP(ENCODE(DOWN(e)))

trong đó Y_seq∈R^{n×d} là biểu diễn cuối cùng cho các tác vụ dự đoán chuỗi. Tương tự, cho các tác vụ phân loại, mô hình đơn giản sử dụng phần tử thứ không của bộ mã hóa chính:

y_cls = [ENCODE(DOWN(e))]_0

Tiền xử lý Giống như các mô hình hiện có, đầu vào cho CANINE cuối cùng phải được biểu diễn như một chuỗi số nguyên, nhưng vì bản chất của ký tự được định nghĩa rõ và được chuẩn hóa bởi Unicode, mã tiền xử lý thường sẽ là hàng trăm hoặc hàng nghìn dòng có thể được thay thế bằng một thủ tục rất đơn giản: chỉ cần lặp qua các ký tự trong chuỗi đầu vào, và trả về các giá trị số nguyên codepoint của chúng (ví dụ, một dòng mã trong Python). Hơn nữa, vì các giá trị codepoint là một phần của Unicode Standard, chúng được ghi lại công khai, đã được hỗ trợ bởi các ngôn ngữ lập trình, và sẽ không thay đổi theo thời gian, không giống như các ID dựa trên từ vựng tùy ý.

Character hash embeddings CANINE sử dụng băm (Svenstrup et al., 2017) để hỗ trợ embedding không gian đầy đủ của các codepoint Unicode với một số lượng tham số tương đối nhỏ, nhưng để giảm khả năng các codepoint khác nhau sẽ chia sẻ chính xác cùng biểu diễn, chúng tôi định nghĩa một khái quát hóa của phương pháp băm tiêu chuẩn trong đó chúng tôi áp dụng nhiều hàm băm cho mỗi codepoint và nối các biểu diễn liên quan đến các giá trị băm khác nhau.

Chính thức hơn, cho một codepoint đơn x_i∈N, chúng tôi áp dụng K hàm băm H_k:N→N, và tra cứu mỗi kết quả băm trong ma trận embedding riêng của nó E_k∈R^{B×d'}, tạo ra K embedding có kích thước d'=d/K, sau đó được nối thành một biểu diễn đơn có kích thước d:

e_i = ⊕_{k=1}^K LOOKUP(H_k(x_i) % B, E_k)

trong đó ⊕ biểu thị nối vector. Chúng tôi gọi đây là character embeddings e∈R^{n×d}.

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng d=768, K=8, và B=16k.

Mặc dù mỗi hàm băm riêng lẻ có thể gặp hash collision, hiệu ứng tổng thể là tối thiểu vì mỗi hàm chỉ chiếm một phần nhỏ trong embedding tổng thể của codepoint, và rất không có khả năng các hàm băm khác sẽ tạo ra cùng collision.

Vì mô hình luôn hỗ trợ tất cả codepoint, có thể học biểu diễn trong quá trình fine-tuning cho các ký tự (và theo đó, từ, script, v.v.) chưa bao giờ được thấy trong huấn luyện trước, trong khi vẫn sử dụng những gì huấn luyện trước đã học về kết hợp từ và cấu trúc câu.

Optional vocabulary-free n-grams Chúng tôi cũng có thể định nghĩa lại các embedding e_i ở trên để bao gồm character n-gram, một lần nữa không có từ vựng cố định, sao cho mỗi thứ tự n-gram đóng góp bằng nhau vào một embedding được tổng:

e_i^N = ⊕_{k=1}^K ∑_{j=1}^N LOOKUP(H_k'(x_{i:j}) % B, E_{j,k})

H_k'(x_{i:j}) = {H_k(x_i) nếu i=j; H_k'(x_i) + H_k'(x_{(i+1):j}) ngược lại}

Công thức này vẫn cho phép mô hình hóa không cần token hóa, nhưng cung cấp cho mô hình một inductive bias ưu tiên ghi nhớ nhiều hơn một chút thông qua một phương tiện thêm tham số chi phí tính toán thấp. Đáng chú ý, nó cũng cho phép chữ ký đầu vào của mô hình vẫn là một chuỗi đơn giản của codepoint.

Downsampling Để làm cho CANINE hiệu quả, chúng tôi sử dụng một chiến lược downsampling đa phần. Đầu tiên, chúng tôi mã hóa ký tự sử dụng một block-wise local attention transformer một lớp. Mô hình này thực hiện self-attention chỉ trong mỗi block có kích thước được định nghĩa trước, tiết kiệm chi phí bậc hai của attention trong khi tận dụng trực giác ngôn ngữ học rằng kết hợp từ - tức là, loại kết hợp liên quan trong các lớp thấp nhất của mô hình (Tenney et al., 2019) - có xu hướng xảy ra ở mức rất cục bộ.

Tiếp theo, chúng tôi sử dụng một strided convolution để giảm số vị trí chuỗi tương tự như một mô hình word piece. Cho character embeddings e∈R^{n×d} với độ dài chuỗi n ký tự và chiều d, chúng tôi sử dụng một convolution với stride r để downsample chuỗi:

h_init = LOCAL_TRANSFORMER_1(e)
h_down = STRIDED_CONV(h_init, r)

Chúng tôi gọi đầu ra này là các vị trí downsampled: h_down∈R^{m×d} trong đó m=n/r là số vị trí downsampled. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng r=4 và n=2048 sao cho m=512, cho ngăn xếp transformer chính của CANINE - cùng độ dài như trong mBERT.

Deep transformer stack Sau downsampling, CANINE áp dụng một ngăn xếp transformer sâu với L lớp cho các vị trí downsampled kết quả. Điều này giống như lõi của BERT và các mô hình dẫn xuất, và vẫn là lõi của CANINE ở chỗ nó chiếm phần lớn tính toán và tham số của nó, mặc dù chúng tôi lưu ý rằng phần giữa này của mô hình có thể dễ dàng được thay thế bằng bất kỳ mô hình sequence-to-sequence nào khác bao gồm những mô hình có hiệu suất tính toán tốt hơn như Performer (Choromanski et al., 2021), Big Bird (Zaheer et al., 2020), RFA (Peng et al., 2021), ETC (Ainslie et al., 2020), v.v. Phần này của mô hình tạo ra một biểu diễn downsampled mới h'_down∈R^{m×d}:

h'_down = TRANSFORMER_L(h_down)
y_cls = [h'_down]_0

Chúng tôi sử dụng L=12 để khớp với mBERT.

Upsampling Trong khi kiến trúc trên đủ cho các tác vụ phân loại, các tác vụ dự đoán chuỗi yêu cầu mô hình phơi bày một lớp đầu ra với cùng độ dài chuỗi như đầu vào (tức là, ký tự là "API" đầu vào và đầu ra của mô hình cho các tác vụ như tagging và span prediction).

Chúng tôi tái tạo một biểu diễn đầu ra theo ký tự bằng cách đầu tiên nối đầu ra của character transformer gốc (ở trên) với biểu diễn downsampled được tạo ra bởi ngăn xếp transformer sâu. (Lưu ý rằng vì mỗi vị trí downsampled được liên kết với chính xác r ký tự cho tỷ lệ downsampling r, mỗi vị trí của biểu diễn downsampled được nhân bản r lần trước khi nối.) Chính thức hơn,

h_up = CONV([h_init ⊕ h'_down], w)
y_seq = TRANSFORMER_1(h_up)

trong đó ⊕ biểu thị nối vector của các biểu diễn (tức là không phải chuỗi) sao cho CONV chiếu từ R^{n×2d} trở lại R^{n×d} trên một cửa sổ w ký tự. Áp dụng một lớp transformer cuối cùng (tiêu chuẩn, không local) tạo ra một biểu diễn chuỗi cuối cùng y_seq∈R^{n×d}.

Residual connections Trong khi bộ mã hóa ký tự ban đầu (trước downsampling) và bộ mã hóa ký tự cuối cùng (sau upsampling) đều biểu diễn các vị trí ký tự, chúng về mặt khái niệm có mục đích rất khác nhau trong mạng. Trực giác, chúng tôi nghĩ về bộ mã hóa ký tự ban đầu như kết hợp ký tự để tạo một biểu diễn giống từ hơn, trong khi bộ mã hóa ký tự cuối cùng đang trích xuất biểu diễn trong ngữ cảnh có liên quan để dự đoán "ý nghĩa" của nội dung tại mỗi vị trí; CANINE phải có thể đối phó với sự mơ hồ bổ sung trong quá trình upsampling vì một vị trí downsampled đơn có thể trải dài hơn một từ khái niệm. Do các vai trò khác nhau của những đặc trưng cảm ứng này, chúng tôi không sử dụng residual connection từ h_init đến h_up.

3.2 Huấn luyện trước

Các mô hình được huấn luyện trước gần đây từ BERT đến T5 phần lớn đã sử dụng các biến thể của một tác vụ masked language model (MLM) (còn được gọi là span corruption) như một hàm loss huấn luyện trước không giám sát - một phương tiện tạo ra các ví dụ tổng hợp không từ bất kỳ tác vụ thực tế nào, nhưng chuẩn bị một mô hình để học các tác vụ thực tế trong các giai đoạn tương lai của huấn luyện (tức là fine-tuning). Thủ tục huấn luyện trước CANINE giữ lại tác vụ MLM, và cung cấp hai chiến lược riêng biệt để tính toán loss MLM - dự đoán ký tự autoregressive so với dự đoán subword - cả hai đều tạo ra một mô hình hoàn toàn không cần token hóa sau huấn luyện trước. Trong các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng một trong những loss này tại một thời điểm.

3.2.1 Autoregressive Character Loss

Span-wise masking CANINE-C là một loss ký tự autoregressive mask các span ký tự trong mỗi chuỗi. Những span này được chọn dựa trên ranh giới khoảng trắng. Không có phân tách dấu câu hoặc heuristic khác được sử dụng. Tất cả ký tự trong span bị mask được thay thế bằng một codepoint mask đặc biệt trong đầu vào. Không có thay thế subword ngẫu nhiên nào được thực hiện vì không có từ vựng subword.

Span prediction CANINE-C dự đoán autoregressive các ký tự bị mask. Thứ tự của các vị trí bị mask được xáo trộn sao cho ngữ cảnh bị mask không nhất thiết được tiết lộ từ trái sang phải, mà là một ký tự tại một thời điểm. Chuẩn bị dữ liệu huấn luyện trước được hiển thị trong Hình 2. Đầu vào bị mask được đưa vào mô hình như x. Đầu ra của mô hình CANINE y_seq và các embedding e_g của các ký tự vàng g (tức là các vị trí ký tự được chọn cho dự đoán MLM) được nối và sau đó được đưa qua một mạng neural feed-forward nhỏ để chiếu trở lại chiều gốc d; cuối cùng được xáo trộn và sử dụng bởi một lớp transformer autoregressive đơn với một self-attention mask từ trái sang phải:

ŷ = TRANSFORMER_AUTOREG([e_g ⊕ y_seq])

Biểu diễn ŷ này sau đó được sử dụng để dự đoán mỗi ký tự. Để tránh lãng phí thời gian trên một ma trận trọng số đầu ra lớn và softmax, các lớp mục tiêu vàng t là các ID codepoint được bucket sao cho t_i = g_i % B. Điều này tương tự như chiến lược được sử dụng trong character hash embedder (§3.1). Các collision thỉnh thoảng giữa các ký tự ít có vấn đề hơn do (a) thực tế rằng đây là một mô hình chỉ encoder và (b) các embedding phải vẫn giữ lại thông tin ngữ cảnh để dự đoán ký tự đúng. Vì chúng tôi chỉ dự đoán một subsequence tương đối nhỏ của đầu vào (15% trong các thí nghiệm của chúng tôi), chi phí của lớp này là nhỏ.

3.2.2 Subword Loss

Chúng tôi cũng thí nghiệm với CANINE-S, một hàm loss dựa trên subword, để chứng minh cách một loss huấn luyện trước nhận biết token vẫn có thể được ghép nối với một mô hình không cần token hóa sao cho tokenizer và từ vựng bị loại bỏ sau huấn luyện trước.

Span-wise masking Giống như thiết lập MLM của mBERT, mỗi span trong CANINE-S tương ứng với một subword đơn. Giống như với loss autoregressive, tất cả ký tự trong span bị mask được thay thế bằng một codepoint "mask" đặc biệt. Thay thế ngẫu nhiên của subword được chọn từ từ vựng của subword cùng độ dài sao cho độ dài của chuỗi ký tự vẫn không thay đổi; chính thức hơn, cho một subword được chọn để thay thế ngẫu nhiên x và một từ vựng subword V, thay thế của x sẽ được rút ra từ tập con của v∈V trong đó LEN(v) = LEN(x).

Span prediction Trong mỗi span ký tự bị mask, CANINE-S ngẫu nhiên chọn một vị trí ký tự mà mô hình sẽ đưa ra dự đoán; mô hình dự đoán danh tính của subword bị mask thông qua softmax. Các embedding subword liên quan bị loại bỏ sau huấn luyện trước.

3.2.3 Targeted Upsampling

Theo thiết kế, mỗi biểu diễn ký tự cuối cùng (sau upsampling) là một hàm của đầu ra của bộ mã hóa ký tự ban đầu (trước downsampling) và đầu ra của ngăn xếp transformer sâu - không có phụ thuộc inter-position nào trên chuỗi upsampled. Điều này phụ thuộc vào upsampler sử dụng các chiếu feed-forward position-wise và một lớp transformer đơn. Trong quá trình huấn luyện trước, chúng tôi tận dụng thiết kế này để cải thiện tốc độ bằng cách chỉ thực hiện upsampling trên các vị trí chuỗi sẽ được sử dụng bởi tác vụ MLM p. Chính thức hơn, chúng tôi sử dụng dạng tương đương sau của hàm UP trong quá trình huấn luyện trước:

h'_up = GATHER(p, h_up)
y'_seq = TRANSFORMER_1(Q=h'_up, KV=h_up)

3.2.4 Modularity

Không giống như các mô hình trước đây, CANINE loại bỏ cả từ vựng và thuật toán token hóa như các phần hóa thạch của mô hình cuối cùng phải được nhân bản trong quá trình fine-tuning và dự đoán. Bất kể loss huấn luyện trước nào được chọn (ký tự hoặc subword), việc sử dụng các thành phần này trong CANINE bị giới hạn trong một chi tiết của thủ tục huấn luyện trước - một inductive bias của hàm loss - sau đó bị loại bỏ. Các giai đoạn fine-tuning và dự đoán của vòng đời mô hình không bao giờ có bất kỳ kiến thức nào về từ vựng hoặc thuật toán token hóa nào (nếu có) được sử dụng trong huấn luyện trước. Điều này cho phép mô hình xử lý dữ liệu chưa được token hóa một cách bẩm sinh, hoặc thậm chí xử lý dữ liệu đã được tiền xử lý bởi các tokenizer khác nhau, một tình huống sẽ gây ra một skew đáng kể giữa các giai đoạn huấn luyện.

4 Thí nghiệm

4.1.1 Dữ liệu Information-Seeking QA

TYDIQA: Primary Tasks TYDIQA là một tập dữ liệu các câu hỏi tìm kiếm thông tin trong 11 ngôn ngữ đa dạng về mặt loại hình (Clark et al., 2020). Các câu hỏi được viết trước khi có câu trả lời, dẫn đến ít sự chồng chéo từ vựng và hình thái giữa câu hỏi và câu trả lời, được rút ra từ Wikipedia. Chúng tôi đánh giá trên các tác vụ chính.

Passage Selection Task (SELECTP) Cho một danh sách các đoạn văn trong một bài viết Wikipedia, trả về hoặc chỉ số của đoạn văn trả lời câu hỏi, hoặc trả về NULL nếu bài viết không chứa câu trả lời chấp nhận được.

Minimal Answer Span Task (MINSPAN) Cho một bài viết Wikipedia đầy đủ, trả về chỉ số byte bắt đầu và kết thúc của span tối thiểu trả lời hoàn toàn câu hỏi. Thay vào đó, một hệ thống có thể chỉ ra rằng bài viết không chứa câu trả lời, hoặc trả về YES hoặc NO cho các câu hỏi kiểu yes/no.

4.1.2 Dữ liệu Named Entity Recognition

Chúng tôi cũng xem xét tác vụ named entity recognition (NER), yêu cầu mô hình xác định span nào của một câu tương ứng với thực thể và gắn nhãn loại thực thể. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi đóng khung tác vụ như sequence labeling, dự đoán các nhãn span được mã hóa BIO.

CoNLL NER Chúng tôi sử dụng dữ liệu tiếng Tây Ban Nha và Hà Lan từ tác vụ CoNLL 2002 NER (Tjong Kim Sang, 2002) và tiếng Anh và Đức từ tác vụ CoNLL 2003 NER (Tjong Kim Sang and De Meulder, 2003), tất cả từ lĩnh vực newswire.

MasakhaNER Để mở rộng phạm vi thí nghiệm của chúng tôi ngoài các ngôn ngữ châu Âu, chúng tôi cũng bao gồm MasakhaNER (Adelani et al., 2021), bao gồm mười ngôn ngữ châu Phi (Amharic, Hausa, Igbo, Kinyarwanda, Luganda, Luo, Nigerian Pidgin, Swahili, Wolof, và Yorùbá) với chú thích của con người trên văn bản tin tức địa phương.

4.1.3 Cấu hình Mô hình

So sánh trực tiếp với mBERT Để xác định kiến trúc huấn luyện trước nào tạo ra dự đoán downstream chất lượng tốt hơn, chúng tôi so sánh CANINE với mBERT, mà chúng tôi đã tái triển khai và huấn luyện lại để giữ càng nhiều biến số có thể không đổi. Lưu ý rằng chúng tôi cố ý không so sánh với các checkpoint được huấn luyện trước công khai sử dụng corpus huấn luyện trước khác nhau vì (a) đây sẽ là một biến số gây nhiễu chính và (b) hầu hết các mô hình được huấn luyện trước có sẵn công khai chỉ đơn giản là các instantiation của BERT, bao gồm XLM-R và X-STILTS.

Setup Chúng tôi huấn luyện trước trên dữ liệu Wikipedia đa ngôn ngữ của mBERT, bao gồm 104 ngôn ngữ. Tương tự, chúng tôi tái sử dụng kỹ thuật exponential smoothing của mBERT để cân bằng các ngôn ngữ trong các mẫu huấn luyện trước. Chúng tôi huấn luyện trong 124k bước với batch size 4096 (2.5 lần qua dữ liệu) sử dụng optimizer LAMB (You et al., 2020) với learning rate giảm tuyến tính 0.018 trong đó 2.5% các bước được sử dụng cho warm-up. Chúng tôi sử dụng độ dài chuỗi 512 cho mBERT, và 2048 cho CANINE, dẫn đến 512 vị trí downsampled trong ngăn xếp transformer sâu cốt lõi của nó. Chúng tôi huấn luyện trước trên 64 Cloud TPU v3 trong khoảng một ngày (xem kết quả cho thời gian chính xác). Cho cả mBERT và CANINE-S (CANINE với loss subword), chúng tôi chọn 15% subword cho loss MLM và dự đoán tối đa 80 vị trí đầu ra; 80% trong số này bị mask trong đầu vào, 10% được thay thế ngẫu nhiên, và 10% không được sửa đổi. Cho CANINE-C (CANINE với loss ký tự autoregressive), chúng tôi chọn 15% span liên tục cho loss MLM và dự đoán tối đa 320 ký tự đầu ra, và không có thay thế ngẫu nhiên nào được thực hiện. Cho TYDIQA, chúng tôi sử dụng độ dài câu trả lời tối đa 100 ký tự, xấp xỉ độ dài câu trả lời percentile thứ 99. Các chuỗi dài hơn độ dài chuỗi tối đa được zero-padded, theo BERT.

4.2 Kết quả TYDIQA

Kết quả chính của chúng tôi được hiển thị trong Bảng 2. CANINE-S (CANINE với loss subword) cải thiện so với mBERT trong tác vụ TYDIQA SELECTP 2.8 F1, trong khi sử dụng khoảng 30% ít tham số hơn. Tương tự, CANINE-C (CANINE với loss ký tự autoregressive), cải thiện so với mBERT 2.5 F1. Thêm character n-gram không cần từ vựng dẫn đến thậm chí nhiều lợi ích hơn so với mBERT (+3.8 F1) và thậm chí nhiều hơn trên tác vụ MINSPAN (+6.9 F1). Một phân tích theo ngôn ngữ được cung cấp trong Bảng 7 trong phụ lục.

Chúng tôi cũng trình bày kết quả từ một số mô hình ablation như các baseline bổ sung trong hàng 3-4 của Bảng 2. Đầu tiên, cho hàng 3, chúng tôi đơn giản thay thế từ vựng subword của BERT bằng một từ vựng ký tự thuần túy, làm cho ký tự vừa là granularity đầu vào và đơn vị masking và dự đoán cho tác vụ MLM, và quan sát thấy rằng không chỉ mô hình chậm hơn 10X so với BERT dựa trên subword, mà chất lượng cũng bị ảnh hưởng rất nhiều. Sau đó, cho hàng 4, chúng tôi sửa đổi mô hình đó để sử dụng subword cho masking và dự đoán MLM, trong khi giữ ký tự như granularity đầu vào, và chúng tôi thấy một cải thiện chất lượng đáng kể, mặc dù huấn luyện trước vẫn cực kỳ chậm. Cuối cùng, bằng cách so sánh với mô hình CANINE đầy đủ trong hàng 5, chúng ta có thể thấy rằng thêm chiến lược downsampling cải thiện tốc độ 700%, và cũng dẫn đến một bump nhỏ bổ sung về chất lượng. Chúng tôi suy đoán rằng lợi ích chất lượng bổ sung này đến từ việc cung cấp cho mô hình một inductive bias tốt hơn hướng tới các đơn vị giống từ hơn trong ngăn xếp transformer sâu.

Analysis CANINE đặc biệt tốt trên các ngôn ngữ giàu hình thái như Kiswahili. Bảng 3 hiển thị các ví dụ mà CANINE vượt trội hơn mBERT trên tác vụ TYDIQA SELECTP. Cụ thể, chúng tôi quan sát các ví dụ mà hình thái phong phú của Kiswahili không cản trở quá trình matching cho CANINE.

4.3 Ablations

Trong Bảng 6, chúng tôi xem xét các sửa đổi nhỏ đối với kiến trúc CANINE cuối cùng, và đánh giá hiệu ứng của mỗi cái trên chất lượng downstream của mô hình.

Attending directly to h'_down Thay vì attend đến chuỗi character-wise h_up, chúng tôi attend đến chuỗi downsampled:

y+_seq = TRANSFORMER_1(Q=h_up, KV=h'_down)

Mặc dù thay đổi này giảm tổng FLOPS của mô hình do tính toán attention giảm, nó không có tác động lớn đến throughput huấn luyện trước. Tuy nhiên, nó làm giảm chất lượng đáng kể.

Number of hash buckets Chúng tôi giảm số hash bucket (B) từ 16k xuống 8k, có nghĩa là nhiều collision (một phần) hơn trong embedding lookup. Điều này cản trở đáng kể tác vụ MINSPAN.

Character vocab Chúng tôi chuyển từ chiến lược no-vocabulary dựa trên hash của chúng tôi sang sử dụng một từ vựng ký tự bình thường (mà chúng tôi rút ra từ corpus huấn luyện trước). Chúng tôi quan sát thấy điều này underperform so với phương pháp hashing. Chúng tôi suy đoán rằng điều này có thể do skew giữa corpus huấn luyện trước và tác vụ downstream cuối cùng vì không phải tất cả codepoint đều có thể được bao gồm trong từ vựng.

Input character dimension Chúng tôi giảm kích thước embedding của bộ mã hóa ký tự ban đầu (tức là kích thước embedding của h_init và e - không phải h_up hay y_seq) và quan sát thấy chất lượng giảm nhanh chóng.

No initial transformer Chúng tôi loại bỏ local transformer từ h_init và tương tự quan sát thấy một sự giảm đáng kể về chất lượng.

Increased downsampling Trong khi downsampling tích cực hơn (hệ số 5X hoặc 6X, thay vì 4X) mang lại lợi ích tốc độ đáng kể, chất lượng passage-level giảm đáng kể và các dự đoán minimal span bị ảnh hưởng thậm chí nhiều hơn.

No position-limited MLM Khi chúng tôi không sử dụng trick áp dụng character transformer cuối cùng (y_seq) chỉ cho các vị trí sẽ được tính toán bởi tác vụ MLM, chúng tôi quan sát thấy một sự giảm lớn về tốc độ. Vì mô hình này về mặt lý thuyết tương đương về mặt phép toán, chúng tôi chỉ hiển thị tốc độ để trình bày.

Chúng tôi cũng thực hiện các ablation nhằm khám phá hiệu ứng của feature concatenation và residual; kết quả trong Bảng 4. Không nối biểu diễn downsampled với biểu diễn ký tự ban đầu khi tính toán h_up khiến mô hình trở nên không ổn định (hàng 2); thêm một residual từ h_up trở lại h_init không giúp ích (hàng 3). Tuy nhiên, thêm chèn một residual từ h_up trở lại h'_down ổn định mô hình (hàng 4) mặc dù không khôi phục chất lượng gốc.

4.4 Kết quả NER

Named entity recognition là một tác vụ mà ghi nhớ thường là một chiến lược rất hiệu quả. Ví dụ, nếu một mô hình có London trong từ vựng của nó và thấy nó với nhãn LOCATION trong quá trình huấn luyện, thì nó chỉ cần lấy ra liên kết ghi nhớ này khi nó thấy token London tại thời điểm test. Do đó, đánh giá trên NER hữu ích để hiểu những cách mà các mô hình khác nhau nhấn mạnh ghi nhớ so với khái quát hóa.

Như được hiển thị trong Bảng 5, CANINE-C thực hiện tệ hơn đáng kể so với mBERT trên NER, có thể do từ vựng thân thiện với ghi nhớ của mBERT. Tuy nhiên, khi các đặc trưng n-gram (không cần token hóa) được thêm vào CANINE-C, hiệu suất phục hồi, cho thấy rằng có thể tăng khả năng ghi nhớ của mô hình một cách rẻ tiền trong khi vẫn hoàn toàn không cần token hóa.

Một phân tích đầy đủ theo ngôn ngữ được cung cấp trong phụ lục (Bảng 8). Đáng chú ý rằng một phần của sự khác biệt hiệu suất trên MasakhaNER là do mBERT không tạo ra đầu ra có thể sử dụng cho Amharic. Dữ liệu huấn luyện trước mBERT không chứa Amharic (hoặc bất kỳ văn bản script Amharic nào), vì vậy nó không có mục từ vựng nào cho script của Amharic (có nghĩa là mBERT chỉ thấy chuỗi [UNK] trên đầu vào Amharic). Tuy nhiên, vì CANINE luôn hỗ trợ không gian Unicode đầy đủ, nó có thể đạt được 50 F1 mặc dù nó cũng chưa bao giờ thấy văn bản Amharic trong quá trình huấn luyện trước. Chúng tôi coi đây là sự xác nhận của phương pháp không cần từ vựng của CANINE. Nó cũng có thể là bằng chứng rằng CANINE thể hiện khả năng transfer cross-script tương tự như những khả năng trong mBERT (Pires et al., 2019).

Error analysis CANINE-C có xu hướng không gắn nhãn các mục từ vựng hiếm hơn mà mBERT dường như đã ghi nhớ. Ví dụ, với CANINE-C, JCPenney (một mục từ vựng tương đối hiếm) không được nhận dạng như một thực thể. CANINE-C cũng có xu hướng tách các thực thể dài; ví dụ, "State Street Bank and Trust Company" được gắn nhãn như hai span riêng biệt: "State Street Bank" và "Trust Company"; và vị trí TAMPA BAY chỉ được nhận dạng là TAMPA. Tuy nhiên, việc thêm các đặc trưng n-gram dường như phần lớn giải quyết vấn đề này.

5 Công trình Liên quan

5.1 Cải thiện token hóa subword

Các cải thiện thêm đối với token hóa subword tiêu chuẩn như Byte Pair Encoding (BPE) (Sennrich et al., 2016), WordPiece (Wu et al., 2016), và SentencePiece (Kudo and Richardson, 2018) đã được đề xuất. Subword regularization (Kudo, 2018) và BPE-dropout (Provilkov et al., 2020) nhận ra rằng phân đoạn deterministic trong quá trình huấn luyện hạn chế khả năng tận dụng hình thái và kết hợp từ; thay vào đó, chúng lấy mẫu ngẫu nhiên một trong nhiều token hóa của đầu vào huấn luyện, được tạo ra bởi sự mơ hồ vốn có của từ vựng subword. Wang et al. (2021) gần đây mở rộng paradigm này để thực thi tính nhất quán của dự đoán trên các phân đoạn khác nhau. Unigram LM (Kudo, 2018), xây dựng từ vựng của nó từ trên xuống, được chỉ ra phù hợp với hình thái tốt hơn BPE trên các bộ mã hóa được huấn luyện trước (Bostrom and Durrett, 2020).

Những người khác đã xây dựng các mô hình hybrid sử dụng nhiều granularity, kết hợp ký tự với token (Luong and Manning, 2016) hoặc các từ vựng subword khác nhau (Zhang and Li, 2021).

5.2 Các mô hình mức ký tự

Theo xu hướng NLP lớn hơn, các mô hình n-gram mức ký tự (Huang et al., 2013; Wieting et al., 2016; Bojanowski et al., 2017) phần lớn đã được thay thế bởi mạng neural. Mặc dù nói chung tụt lại so với các đối tác mức từ của chúng, các đặc trưng mức ký tự quan trọng cho các ngôn ngữ giàu hình thái, đặc biệt trong các thiết lập ít nguồn lực (Garrette and Baldridge, 2013).

Cho language modeling Các mô hình ngôn ngữ ký tự (CLM) đã sử dụng kiến trúc RNN vanilla để tạo ra phân phối trên chuỗi ký tự theo cách hoàn toàn không cần token hóa (Sutskever et al., 2011; Graves, 2013; Hwang and Sung, 2017; Radford et al.). RNN phân cấp mô hình hóa giả định rằng ngôn ngữ hoạt động trên các lớp trừu tượng tăng dần: Chung et al. (2017) huấn luyện kết hợp một sub-module để phân đoạn đầu vào mức ký tự thành các span lớn hơn tại mỗi lớp của một LSTM xếp chồng.

Do sự tụt lại nhất quán trong hiệu suất so với các đối tác mức từ của chúng, sự chú ý chuyển từ CLM thuần túy sang các mô hình chỉ nhận biết ký tự, vẫn dựa vào token hóa truyền thống. Một số mô hình hybrid xử lý đầu vào ở mức ký tự, nhưng dự đoán từ từ một từ vựng đóng (Kim et al., 2016; Gerz et al., 2018). Những cái khác tái giới thiệu token hóa rõ ràng ở phía đầu vào, và hoặc tạo ra burst của chuỗi ký tự tạo thành một từ vựng mở (Kawakami et al., 2017) hoặc sử dụng một trình tạo chỉ ký tự như dự phòng khi trình tạo từ vựng đóng chính tạo ra một token hiếm hoặc không biết (Matthews et al., 2019; Mielke and Eisner, 2019). Đặc biệt sau khi phổ biến của các từ vựng subword vốn dĩ mơ hồ như BPE, một số nghiên cứu đã vượt qua một phân đoạn đầu vào đơn và marginalize trên tất cả phân đoạn có thể (van Merriënboer et al., 2017; Buckman and Neubig, 2018; Grave et al., 2019).

Đi đầy vòng, Kawakami et al. (2019) cảm ứng một từ điển mà không có bất kỳ giám sát rõ ràng nào, quay trở lại CLM thuần túy. Trong một nỗ lực tái sinh để đưa chúng ngang bằng với các granularity thô hơn, các nhà nghiên cứu đã tận dụng các nguồn lực bên ngoài như grounding trong thị giác (Kawakami et al., 2019) hoặc multi-task learning cùng với các tác vụ hình thái có giám sát (Blevins and Zettlemoyer, 2019).

Sau khi transformer (Vaswani et al., 2017) thay thế RNN như kiến trúc thống trị trong NLP, các mô hình mức ký tự đã theo sau. Al-Rfou et al. (2019) chỉ ra rằng Transformer mức byte vanilla underperform đáng kể so với các đối tác mức từ của chúng. Một phát hiện tương tự được báo cáo bởi Radford et al. (2019). Mặc dù khoảng cách đã được giảm (Choe et al., 2019), subword transformer vẫn là hiện trạng cho pure language modeling.

Cho các tác vụ cụ thể Song song với các nỗ lực LM, cộng đồng neural machine translation (NMT) tìm cách giải quyết vấn đề từ vựng mở của nó thông qua mô hình hóa mức ký tự. Luong and Manning (2016) đề xuất một mô hình hybrid hoạt động chủ yếu ở mức từ, nhưng tham khảo một LSTM mức ký tự cho các từ không biết; đây là một thỏa hiệp thực tế, vì mô hình chỉ ký tự của họ mất 3 tháng để huấn luyện. Lee et al. (2017) cho phép NMT ký tự thuần túy bằng cách rút ngắn độ dài đầu vào thông qua các lớp convolutional, pooling, và highway. Đáng chú ý, mô hình many-to-English của họ vượt trội hơn đối tác subword và hầu hết baseline song ngữ, với tăng 35% thời gian huấn luyện (trên một GPU đơn) so với mô hình baseline BPE-to-char. CANINE có động lực tương tự, nhưng hoạt động trong bối cảnh của các transformer được huấn luyện trước; huấn luyện nhanh hơn 7x so với baseline char-to-char (trên TPU v3), và có tăng 28% thời gian huấn luyện so với mBERT (Bảng 2).

Thông tin ký tự đã được tận dụng cho nhiều tác vụ cuối khác nữa, bao gồm: phân loại văn bản (Zhang et al., 2015; Zhang and LeCun, 2017), part-of-speech tagging và NER (Gillick et al., 2016; Akbik et al., 2018; Pinter et al., 2019), named entity detection (Yu et al., 2018), dependency parsing (Vania et al., 2018), và machine reading comprehension (Hewlett et al., 2018). Thông tin ký tự chứng tỏ đặc biệt hữu ích cho các ngôn ngữ ít nguồn lực (Xie et al., 2018), các hiện tượng như code-switching và transliteration (Ball and Garrette, 2018), và hình thái phong phú (Vania and Lopez, 2017), trước đây nhận được mô hình hóa đặc biệt bao gồm adaptor grammar (Botha and Blunsom, 2013).

Cho transfer learning Các mô hình dựa trên token cũng đã được tăng cường với thông tin mức ký tự trong bối cảnh transfer learning, nơi các bộ mã hóa được huấn luyện với các mục tiêu không giám sát được tái sử dụng để giải quyết các tác vụ downstream. Pinter et al. (2017) giải quyết vấn đề out-of-vocabulary của các word embedding được huấn luyện trước tĩnh bằng cách huấn luyện một mô hình để ánh xạ bề mặt của một từ đến biểu diễn được huấn luyện trước của nó, và sử dụng nó trên các từ không biết. ELMo (Peters et al., 2018), một mô hình LSTM hai chiều, áp dụng convolution ký tự cho các token đầu vào được phân tách bằng khoảng trắng của nó. CharacterBERT (Boukkouri et al., 2020) chuyển kỹ thuật này sang BERT, tăng cường đầu vào được token hóa WordPiece hiện có của nó. Phù hợp với các quan sát trước đây rằng việc cung cấp ký tự vào một ngăn xếp transformer đi kèm với chi phí tính toán khổng lồ trong khi không cải thiện so với các phương pháp dựa trên token hóa (Al-Rfou et al., 2019), một mô hình BERT được fine-tuned cho semantic parsing đạt được lợi ích chỉ khi ký tự bổ sung cho subword (van Noord et al., 2020).

5.3 Các mô hình đa ngôn ngữ

NLP đa ngôn ngữ đã bị thống trị bởi các mô hình đa ngôn ngữ được huấn luyện trước sâu có từ vựng subword được chia sẻ giữa các ngôn ngữ. Các mô hình như vậy mượn kiến trúc của chúng từ các tiền nhiệm đơn ngôn ngữ và áp dụng huấn luyện kết hợp trong 100+ ngôn ngữ, hoặc với loss LM không giám sát: mBERT, mT5 (Xue et al., 2021), hoặc với loss dịch thuật bổ sung: XLM (Lample and Conneau, 2019), XLM-R (Conneau et al., 2020). Chung et al. (2020) mở rộng điều này bằng cách tạo thành các cluster ngôn ngữ với từ vựng per-cluster. Để chứa các ngôn ngữ không được thấy trong quá trình huấn luyện trước, Wang et al. (2020) mở rộng từ vựng và tiếp tục huấn luyện trước.

6 Kết luận

Trong bài viết này, chúng tôi đã mô tả CANINE, mà theo hiểu biết của chúng tôi, là bộ mã hóa sâu được huấn luyện trước đầu tiên cho hiểu biết ngôn ngữ sử dụng một mô hình không cần token hóa, không cần từ vựng, trong khi vượt qua chất lượng của các mô hình được xây dựng trên các tokenizer heuristic. CANINE loại bỏ nhiều cạm bẫy kỹ thuật cho các practitioner và mở ra các hướng nghiên cứu mới cho cộng đồng.

Lời cảm ơn

Các tác giả muốn cảm ơn Noah Constant, Rami Al-Rfou, Kristina Toutanova, Kenton Lee, Ming-Wei Chang, và Tim Dozat vì phản hồi của họ về công trình này. Chúng tôi cũng muốn cảm ơn Martin Njoroge và Nanjala Misiko vì tư vấn của họ về các ví dụ Kiswahili, Diana Akrong vì tư vấn về chính tả Twi, và Waleed Ammar vì tư vấn về hình thái tiếng Ả Rập.

thinking>
This is a very long academic paper and I need to continue translating the remaining sections. Let me continue with the references and appendix sections.
</thinking>

Tài liệu tham khảo

David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D'souza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, và Salomey Osei. 2021. MasakhaNER: Named Entity Recognition cho các Ngôn ngữ châu Phi. TACL.

Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, và Li Yang. 2020. ETC: Encoding Long and Structured Inputs in Transformers. Trong Proc. of EMNLP.

Alan Akbik, Duncan Blythe, và Roland Vollgraf. 2018. Contextual String Embeddings for Sequence Labeling. Trong Proc. of COLING.

Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, và Llion Jones. 2019. Character-level language modeling with deeper self-attention. Trong Proc. of AAAI.

Kelsey Ball và Dan Garrette. 2018. Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification. Trong Proc. of EMNLP.

Terra Blevins và Luke Zettlemoyer. 2019. Better character language modeling through morphology. Trong Proc. of ACL.

Piotr Bojanowski, Edouard Grave, Armand Joulin, và Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL.

Kaj Bostrom và Greg Durrett. 2020. Byte pair encoding is suboptimal for language model pre-training. Trong Findings of the Association for Computational Linguistics: EMNLP.

Jan A. Botha và Phil Blunsom. 2013. Adaptor Grammars for learning non-concatenative morphology. Trong Proc. of EMNLP.

Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum, và Junichi Tsujii. 2020. CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters. Trong Proc. of COLING.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners. Trong Proc. of NeurIPS.

Jacob Buckman và Graham Neubig. 2018. Neural lattice language models. TACL.

Dokook Choe, Rami Al-Rfou, Mandy Guo, Heeyoung Lee, và Noah Constant. 2019. Bridging the Gap for Tokenizer-Free Language Models. arXiv preprint arXiv:1908.10322.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, và Adrian Weller. 2021. Rethinking Attention with Performers. Trong Proc. of ICLR.

Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, và Jason Riesa. 2020. Improving Multilingual Models with Language-Clustered Vocabularies. Trong Proc. of EMNLP.

Junyoung Chung, Sungjin Ahn, và Yoshua Bengio. 2017. Hierarchical Multiscale Recurrent Neural Networks. Trong Proc. of ICLR.

Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, và Jennimaria Palomaki. 2020. TyDi QA: A benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. TACL.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, và Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. Trong Proc. of ACL.

Zihang Dai, Guokun Lai, Yiming Yang, và Quoc V. Le. 2020. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. Trong Proc. of NeurIPS.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Trong Proc. of NAACL.

Dan Garrette và Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. Trong Proc. of NAACL.

Daniela Gerz, Ivan Vulić, Edoardo Ponti, Jason Naradowsky, Roi Reichart, và Anna Korhonen. 2018. Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction. TACL.

Dan Gillick, Cliff Brunk, Oriol Vinyals, và Amarnag Subramanya. 2016. Multilingual Language Processing From Bytes. Trong Proc. of NAACL.

Edouard Grave, Sainbayar Sukhbaatar, Piotr Bojanowski, và Armand Joulin. 2019. Training hybrid language models by marginalizing over segmentations. Trong Proc. of ACL.

Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.

Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, và David Berthelot. 2018. Byte-Level Machine Reading across Morphologically Varied Languages. Trong Proc. of AAAI.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, và Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. Trong Proc. of the ACM International Conference on Information and Knowledge Management (CIKM).

Kyuyeon Hwang và Wonyong Sung. 2017. Character-Level Language Modeling with Hierarchical Recurrent Neural Networks. Trong Proc. of ICASSP.

Prabhu Kaliamoorthi, Sujith Ravi, và Zornitsa Kozareva. 2019. PRADO: Projection attention networks for document classification on-device. Trong Proc. of EMNLP.

Kazuya Kawakami, Chris Dyer, và Phil Blunsom. 2017. Learning to create and reuse words in open-vocabulary neural language modeling. Trong Proc. of ACL.

Kazuya Kawakami, Chris Dyer, và Phil Blunsom. 2019. Learning to Discover, Ground and Use Words with Segmental Neural Language Models. Trong Proc. of ACL.

Yoon Kim, Yacine Jernite, David Sontag, và Alexander M Rush. 2016. Character-Aware Neural Language Models. Trong Proc. of AAAI.

Taku Kudo. 2018. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates. Trong Proc. of ACL.

Taku Kudo và John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. Trong Proc. of EMNLP: System Demonstrations.

Guillaume Lample và Alexis Conneau. 2019. Cross-lingual Language Model Pretraining. Trong Proc. of NeurIPS.

Jason Lee, Eth Zürich, Kyunghyun Cho, và Thomas Hofmann. 2017. Fully Character-Level Neural Machine Translation without Explicit Segmentation. TACL.

Minh-Thang Luong và Christopher D. Manning. 2016. Achieving open vocabulary neural machine translation with hybrid word-character models. Trong Proc. of ACL.

Austin Matthews, Graham Neubig, và Chris Dyer. 2019. Using Morphological Knowledge in Open-Vocabulary Neural Language Models. Trong Proc. of NAACL.

Sebastian J. Mielke và Jason Eisner. 2019. Spell once, summon anywhere: A two-level open-vocabulary language model. Trong Proc. of AAAI.

Rik van Noord, Antonio Toral, và Johan Bos. 2020. Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT. Trong Proc. of EMNLP.

Brendan O'Connor, Michel Krieger, và David Ahn. 2010. TweetMotif: Exploratory Search and Topic Summarization for Twitter Introduction and Description. Trong Proc. of the International AAAI Conference on Web and Social Media.

Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, và Lingpeng Kong. 2021. Random feature attention. Trong Proc. of ICLR.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, và Luke Zettlemoyer. 2018. Deep contextualized word representations. Trong Proc. of NAACL.

Jason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, và Samuel R Bowman. 2020. English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too. Trong Proc. of AACL.

Yuval Pinter, Robert Guthrie, và Jacob Eisenstein. 2017. Mimicking word embeddings using subword RNNs. Trong Proc. of EMNLP.

Yuval Pinter, Marc Marone, và Jacob Eisenstein. 2019. Character Eyes: Seeing Language through Character-Level Taggers. Trong Proc. of BlackboxNLP.

Telmo Pires, Eva Schlinger, và Dan Garrette. 2019. How multilingual is Multilingual BERT? Trong Proc. of ACL.

Ivan Provilkov, Dmitrii Emelianenko, và Elena Voita. 2020. BPE-Dropout: Simple and Effective Subword Regularization. Trong Proc. of ACL.

Danish Pruthi, Bhuwan Dhingra, và Zachary C. Lipton. 2019. Combating adversarial misspellings with robust word recognition. Trong Proc. of ACL.

Alec Radford, Rafal Jozefowicz, và Ilya Sutskever. Learning to Generate Reviews and Discovering Sentiment. arXiv preprint arXiv:1704.01444.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. 2019. Language models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR.

Rico Sennrich, Barry Haddow, và Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. Trong Proc. of ACL.

Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, và Caiming Xiong. 2020. Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT. arXiv preprint arXiv:2003.04985.

Ilya Sutskever, James Martens, và Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. Trong Proc. of ICML.

Dan Svenstrup, Jonas Meinertz Hansen, và Ole Winther. 2017. Hash Embeddings for Efficient Word Representations. Trong Proc. of NeurIPS.

David Talbot và John Talbot. 2008. Bloom maps. Trong Proc. of the Workshop on Analytic Algorithmics and Combinatorics (ANALCO).

Ian Tenney, Dipanjan Das, và Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. Trong Proc. of ACL.

Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. Trong Proc. of CoNLL.

Erik F. Tjong Kim Sang và Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. Trong Proc. of NAACL.

Bart van Merriënboer, Amartya Sanyal, H. Larochelle, và Yoshua Bengio. 2017. Multiscale sequence modeling with a learned dictionary. arXiv preprint arXiv:1707.00762.

Clara Vania, Andreas Grivas, và Adam Lopez. 2018. What do character-level models learn about morphology? The case of dependency parsing. Trong Proc. of EMNLP.

Clara Vania và Adam Lopez. 2017. From Characters to Words to in Between: Do We Capture Morphology? Trong Proc. of ACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Proc. of NeurIPS.

Xinyi Wang, Sebastian Ruder, và Graham Neubig. 2021. Multi-view subword regularization. Trong Proc. of NAACL.

Zihan Wang, Karthikeyan K, Stephen Mayhew, và Dan Roth. 2020. Extending multilingual BERT to low-resource languages. Trong Findings of EMNLP.

John Wieting, Mohit Bansal, Kevin Gimpel, và Karen Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. Trong Proc. of EMNLP.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, và Jeffrey Dean. 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv preprint arXiv:1609.08144.

Jiateng Xie, Zhilin Yang, Graham Neubig, Noah A. Smith, và Jaime Carbonell. 2018. Neural cross-lingual named entity recognition with minimal resources. Trong Proc. of EMNLP.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, và Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. Trong Proc. of NAACL.

Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, và Cho-Jui Hsieh. 2020. Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. Trong Proc. of ICLR.

Xiaodong Yu, Stephen Mayhew, Mark Sammons, và Dan Roth. 2018. On the strength of character language models for multilingual named entity recognition. Trong Proc. of EMNLP.

Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. Trong Proc. of NeurIPS.

Xiang Zhang và Yann LeCun. 2017. Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean? arXiv preprint arXiv:1708.02657v2.

Xiang Zhang, Junbo Zhao, và Yann LeCun. 2015. Character-level convolutional networks for text classification. Trong Proc. of NeurIPS.

Xinsong Zhang và Hang Li. 2021. AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. Trong Findings of ACL.

A Phụ lục

Bảng 7: Phân tích theo ngôn ngữ cho các tác vụ chính TYDIQA. Tiếng Anh được đặt trong ngoặc đơn vì không được bao gồm trong tính toán điểm tổng thể cho TYDIQA.

[Các bảng số liệu được bảo toàn nguyên như trong bản gốc với các giá trị F1 và delta cho từng ngôn ngữ]

Bảng 8: Phân tích theo ngôn ngữ cho Named Entity Recognition cho các tập dữ liệu CoNLL và MasakhaNER (labeled F1). mBERT đạt điểm số không trên Amharic do không có mục từ vựng nào trong script Amharic.

[Các bảng số liệu được bảo toàn nguyên như trong bản gốc với các giá trị F1 cho từng ngôn ngữ]
