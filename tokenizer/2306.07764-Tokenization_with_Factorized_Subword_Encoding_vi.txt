# Mã hóa từ phụ với mã hóa nhân tử

David Samuel và Lilja Øvrelid
Đại học Oslo, Nhóm Công nghệ Ngôn ngữ

## Tóm tắt

Trong những năm gần đây, các mô hình ngôn ngữ đã trở nên ngày càng lớn hơn và phức tạp hơn. Tuy nhiên, các biểu diễn đầu vào cho những mô hình này vẫn tiếp tục dựa vào các phương pháp mã hóa từ phụ đơn giản và tham lam. Trong bài báo này, chúng tôi đề xuất một phương pháp mã hóa mới nhân tử hóa các từ phụ thành các bộ ba rời rạc sử dụng mô hình VQ-VAE. Hiệu quả của phương pháp mã hóa được đề xuất, được gọi là FACTORIZER, được đánh giá trên các nhiệm vụ mô hình hóa ngôn ngữ và hình thái-cú pháp cho 7 ngôn ngữ đa dạng. Kết quả cho thấy phương pháp này phù hợp và mạnh mẽ hơn cho các nhiệm vụ hình thái học so với thuật toán mã hóa cặp byte (BPE) thường được sử dụng.

## 1 Giới thiệu

Một bộ mã hóa từ phụ điển hình bao gồm một từ vựng với hàng chục nghìn từ phụ, mỗi từ được ánh xạ onto một chỉ số đơn lẻ và độc lập. Thay vào đó, chúng tôi đề xuất một phương pháp học cách chiếu các từ phụ onto các bộ ba của 256 chỉ số:

melon → [30,255,209].

Ánh xạ này được học bởi một bộ tự mã hóa biến phân lượng tử vector (VQ-VAE; van den Oord et al., 2017) từ một danh sách tần số từ lớn, tạo ra một phép chiếu trong đó các từ khác nhau về chính tả sử dụng các chỉ số khác nhau và các từ tương tự chia sẻ các chỉ số tương tự:

melons → [261,255,209],
water → [96,235,109].

Việc mô hình hóa phép chiếu này với VQ-VAE cũng tự động đưa ra xác suất ước tính của mọi từ phụ cho một bộ ba chỉ số. Tối đa hóa xác suất kết hợp cho phép mã hóa từ phụ tối ưu của các từ không có trong từ vựng.

water|melon → [208,235,109],[45,255,209].

Trong bài báo này, chúng tôi trình bày FACTORIZER, một phương pháp mã hóa từ phụ phục vụ như một thay thế trực tiếp cho bất kỳ bộ mã hóa từ phụ nào được sử dụng trong các pipeline NLP hiện đại. Chúng tôi phát hành mã nguồn, các mô hình đã huấn luyện và các bộ mã hóa sẵn sàng sử dụng trực tuyến.

Cách tiếp cận của chúng tôi thể hiện các ưu điểm sau:

1. **Hiệu suất được cải thiện**. Chúng tôi đánh giá hiệu suất của FACTORIZER trên các mô hình ngôn ngữ có mặt nạ của bảy ngôn ngữ đa dạng về ngôn ngữ học: Tiếng Ả Rập, Tiếng Trung, Tiếng Séc, Tiếng Anh, Tiếng Na Uy, Tiếng Gaelic Scotland và Tiếng Thổ Nhĩ Kỳ. Những mô hình này được đánh giá trên các nhiệm vụ gắn thẻ từ loại, phân tích cú pháp phụ thuộc, và bổ đề hóa và thể hiện sự cải thiện đáng kể về hiệu suất.

2. **Tăng tính mạnh mẽ**. Ngoài ra, chúng tôi nghiên cứu tính mạnh mẽ của FACTORIZER đối với nhiễu ngẫu nhiên trong quá trình suy luận cũng như tính mạnh mẽ đối với sự khan hiếm dữ liệu trong quá trình tiền huấn luyện. Chúng tôi đo hiệu suất với các mức độ nhiễu và khan hiếm dữ liệu tăng dần và chứng minh rằng FACTORIZER cải thiện tính mạnh mẽ đối với những yếu tố này.

3. **Sử dụng tham số hiệu quả hơn**. Các bộ mã hóa từ phụ truyền thống yêu cầu từ vựng lớn để bao phủ hầu hết các dạng từ, dẫn đến một phần đáng kể các tham số có thể học được bị tiêu thụ bởi lớp nhúng từ phụ. Ví dụ, lớp nhúng của BERT base sử dụng hơn 20% tham số của nó, tổng cộng hơn 23 triệu. Ngược lại, dấu chân bộ nhớ của lớp nhúng FACTORIZER thấp hơn đáng kể vì nó chỉ yêu cầu khoảng 0.6 triệu tham số. Các tham số còn lại sau đó có thể được phân bổ hiệu quả hơn trong các lớp tự chú ý.

## 2 Nền tảng: VQ-VAE

Trong bài báo này, chúng tôi đề xuất một bộ mã hóa mới sử dụng bộ tự mã hóa biến phân lượng tử vector (VQ-VAE; van den Oord et al., 2017) như thành phần trung tâm. VQ-VAE là một kỹ thuật mạnh mẽ để học các biến tiềm ẩn rời rạc có thể mã hóa từ và tái tạo chúng trở lại dạng ban đầu. Quá trình này có thể được chia thành ba bước chính, như minh họa trong Hình 1:

1. **Bộ mã hóa** ánh xạ các mẫu dữ liệu (từ phụ) x thành các vector tiềm ẩn liên tục e(x).

2. **Bảng mã** được tạo thành từ K vector bảng mã zk, k trong 1...K. Mỗi vector tiềm ẩn e(x) được lượng tử hóa thành vector bảng mã gần nhất zk, hiệu quả ánh xạ mỗi mẫu đầu vào x onto một biến rời rạc k. Bộ mã hóa và bảng mã giảm mẫu và nén thông tin trong x, phục vụ như một nút thắt cổ chai thông tin (Tishby et al., 1999).

3. **Bộ giải mã** tái tạo các ký hiệu z trở lại phân phối đầu vào ban đầu, mô hình hóa phân phối p(x|z) như d(z).

**Lan truyền ngược gradient**. Mô hình được tối ưu hóa chung với thuật toán lan truyền ngược. Tuy nhiên, cần chú ý đặc biệt đến lượng tử hóa bảng mã q vì phép toán này không khả vi:

q(e(x)) = zk, trong đó k = argmin_i ||e(x) - zi||²

Trong pha tiến, các vector bảng mã đã lượng tử hóa được truyền đơn giản đến bộ giải mã. Tuy nhiên, trong pha lùi, gradient của hàm mất mát ∇dL được truyền trực tiếp từ bộ giải mã đến bộ mã hóa. Kỹ thuật này, được gọi là ước lượng gradient thẳng, có thể thực hiện được vì đầu ra của bộ mã hóa và đầu vào của bộ giải mã chia sẻ cùng không gian tiềm ẩn. Đầu ra e(x) đủ tương tự với đầu vào bộ giải mã z, sao cho gradient mang thông tin về cách thay đổi e(x) để giảm mất mát tái tạo.

**Các thành phần mất mát**. Ngoài mất mát tái tạo tiêu chuẩn Lr = log p(x|z), đo hiệu suất tự mã hóa, mô hình VQ-VAE kết hợp hai thành phần mất mát phụ trợ căn chỉnh các đầu ra bộ mã hóa e(x) với các vector bảng mã gần nhất. Cụ thể, mất mát bảng mã được định nghĩa là Lq = ||sg(e(x)) - zk||² và mất mát cam kết là Le = ||e(x) - sg(zk)||², trong đó 'sg' là phép toán dừng gradient và zk là vector bảng mã được định nghĩa trong Phương trình (1). Mất mát tổng thể của mô hình VQ-VAE được tính như:

L = Lr + Lq + beta × Le,

trong đó beta là một siêu tham số cân bằng trọng tâm giữa tái tạo và căn chỉnh bảng mã.

**EMA Bảng mã**. Một cách tiếp cận thay thế để cập nhật bảng mã, được đề xuất bởi van den Oord et al. (2017), là cập nhật nó như một trung bình động mũ (EMA) của các biến tiềm ẩn e(x) - thay vì kết hợp mất mát bảng mã Lq. Cập nhật này bao gồm cập nhật hai biến: số lượng sử dụng bảng mã ck và các vector bảng mã zk, trong đó lambda là siêu tham số phân rã EMA:

ck ← lambda × ck + (1-lambda) × Σi 1[q(e(xi)) = zk]
zk ← lambda × zk + (1-lambda)/ck × Σi e(xi) × 1[q(e(xi)) = zk]

Cách tiếp cận này dẫn đến huấn luyện ổn định hơn (Kaiser et al., 2018) và cho phép chúng ta giảm thiểu sự sụp đổ bảng mã, xảy ra khi số lượng sử dụng của một vector giảm xuống không và vector đó không bao giờ được cập nhật (Kaiser et al., 2018). Do đó, bất cứ khi nào một vector bảng mã zi có số lượng sử dụng ci thấp hơn cmin, nó được gán lại cho một vector tiềm ẩn ngẫu nhiên zi ← e(xj) và số lượng sử dụng được đặt lại ci ← 1, tương tự như Williams et al. (2020) hoặc Dhariwal et al. (2020).

## 3 FACTORIZER

Chúng tôi sử dụng kiến trúc VQ-VAE để huấn luyện một mô hình có khả năng ánh xạ các biểu diễn bộ ba rời rạc z thành chuỗi từ phụ w (và ngược lại). Hơn nữa, chúng tôi sử dụng bộ giải mã VQ-VAE để ước tính xác suất của các chuỗi từ phụ p(w|z). Sau khi mô hình được huấn luyện, chúng tôi suy luận từ vựng của nó, bao gồm một tập hợp các tuple ⟨wi, zi, log p(wi|zi)⟩. Cuối cùng, chúng tôi sử dụng từ vựng này để thực hiện mã hóa từ phụ tối ưu (về mặt log-xác suất).

### 3.1 Mã hóa từ phụ VQ-VAE

**Dữ liệu huấn luyện**. Bộ tự mã hóa được huấn luyện trên danh sách tần số từ thu được bằng cách mã hóa từ một corpus văn bản lớn. Hãy ký hiệu tần số của từ w là fw. Lưu ý rằng trong khi biểu diễn dữ liệu này đơn giản hóa mô hình bằng cách loại bỏ bất kỳ thông tin ngữ cảnh nào, nó vẫn cho phép ước tính phù hợp p(wi|zi) bằng cách tuân theo tần số từ fw.

**Biểu diễn từ**. Trong nghiên cứu này, từ được biểu diễn như các chuỗi byte UTF-8. Để đảm bảo xử lý phù hợp các ranh giới từ, các chuỗi từ bắt đầu bằng một token đặc biệt "beginning-of-word" và kết thúc bằng một token "end-of-word"; cả hai token đều được mô tả bằng ký hiệu '▁' trong văn bản này.

**Lấy mẫu dữ liệu**. Về lý thuyết, cách đúng để lấy mẫu dữ liệu huấn luyện là tuân theo trực tiếp các tần số fw. Tuy nhiên, trong thực tế, phân phối từ trong ngôn ngữ tự nhiên tuân theo luật Zipf, dẫn đến phân phối lệch làm sụp đổ quá trình huấn luyện. Để giải quyết vấn đề này, chúng tôi lấy mẫu dữ liệu theo phân phối cân bằng hơn psample(w) ∝ fw/log(fw + 1).

Để bù đắp cho sự thay đổi này và mô hình hóa chính xác phân phối từ thực, chúng tôi kết hợp mẫu số vào hàm mất mát bằng cách cân nặng như sau:

L = Σw log(fw + 1) × L(w).

**Phân tách từ phụ**. Đến thời điểm này, chúng tôi chỉ thảo luận về mã hóa từ. Để cũng mã hóa từ phụ, chúng tôi phân tách ngẫu nhiên một số từ được lấy mẫu thành từ phụ. Cụ thể, vì các từ thường xuyên hơn nên được phân tách ít thường xuyên hơn, chúng tôi giữ từ như vốn có với xác suất

pnot-split(w) = log(fw + 1) / maxx log(fx + 1).

**Bảng mã nhân tử**. Để nắm bắt thông tin chi tiết về các ký tự bên trong từ, chúng tôi sử dụng các bảng mã riêng biệt, mỗi bảng trong một không gian tiềm ẩn duy nhất. Ngoài ra, để giảm thêm vấn đề sụp đổ bảng mã, mỗi bảng mã chỉ bao gồm K = 256 vector bảng mã.

**Kiến trúc xương sống**. Mô hình tự mã hóa dựa trên kiến trúc transformer mã hóa-giải mã (Vaswani et al., 2017) với ba nút thắt cổ chai lượng tử hóa. Cụ thể, chúng tôi đầu tiên đệm các byte-token w với các token tiền tố R,G,B đặc biệt, sau đó nhúng và mã hóa mỗi chuỗi với bộ mã hóa transformer. Các vector nhúng ngữ cảnh hóa cho ba token đầu tiên phục vụ như mã hóa e(w). Ba vector này sau đó được lượng tử hóa và đặt trước các byte-token từ phụ w, cuối cùng được đưa vào bộ giải mã transformer tự hồi quy d(z,w), mô hình hóa p(w|z).

**Chi tiết huấn luyện**. Tất cả các mô hình VQ-VAE trong nghiên cứu này sử dụng transformer với 6 lớp và kích thước ẩn 256 cho cả bộ mã hóa và giải mã. Các mô hình được huấn luyện trong 50.000 bước với kích thước batch 4.096 và được tối ưu hóa với adaptive sharpness-aware minimization (ASAM; Kwon et al., 2021) để đảm bảo khái quát hóa tốt hơn cho các từ chưa thấy, với AdamW (Loshchilov và Hutter, 2019) làm bộ tối ưu hóa cơ bản. Để cải thiện thêm khái quát hóa, chúng tôi tính trung bình động mũ của các tham số (với phân rã 0.999) và sử dụng trung bình này cho suy luận.

**Xây dựng từ vựng**. VQ-VAE không được sử dụng trực tiếp cho mã hóa vì điều đó sẽ làm chậm đáng kể. Thay vào đó, chúng tôi suy luận từ vựng tĩnh của nó bằng cách lặp qua tất cả 256³ thể hiện của các bảng mã RGB. Từ vựng từ phụ được giải mã như

W = {argmax_w p(w|z) | z trong [256]³, p(z) > 0},

trong đó phân phối tiên nghiệm p(z) được tính bằng cách đếm việc sử dụng tất cả các bộ ba bảng mã trong suốt quá trình huấn luyện. Cuối cùng, từ vựng đầy đủ V là một tập hợp các tuple ⟨wi, zi, log p(wi|zi)⟩, trong đó wi trong W và zi = argmax_z p(wi|z).

### 3.2 Bộ mã hóa từ phụ

**Tìm kiếm phân tách tối ưu**. Sau khi suy luận từ vựng V, chúng ta có thể tìm kiếm mã hóa tối ưu của một từ x thành các từ phụ w1, w2, ...wk:

tokenize(x) = argmin_{w1w2...wk=x} Σ_{i=1}^k score(wi),

trong đó đối với mỗi từ phụ wi từ các tuple từ vựng ⟨wi, zi, log p(wi|zi)⟩ trong V, điểm số của nó là

score(wi) = -log p(wi|zi) + alpha_split.

Tham số alpha_split cho phép thay đổi mượt mà lượng phân tách trên mỗi từ, như thể hiện trong Hình 3. Chúng tôi sử dụng alpha_split = 0.1 cho tất cả thí nghiệm trong công trình này.

Hàm tokenize được triển khai thông qua tìm kiếm đường đi ngắn nhất trong đồ thị mã hóa, như minh họa trong Hình 2. Cụ thể, tìm kiếm sử dụng thuật toán Dijkstra (Dijkstra, 1959) và các cạnh tiến từ mỗi nút được lặp hiệu quả với tìm kiếm tiền tố trong đồ thị từ có hướng không chu trình (DAWG; Blumer et al., 1985) của tất cả từ phụ từ V.

**Lấy mẫu**. Công thức của chúng tôi về bộ mã hóa cho phép sửa đổi đơn giản để lấy mẫu các phân tách từ phụ khác nhau. Điều này tương tự như BPE dropout (Provilkov et al., 2020), một kỹ thuật chính quy hóa mạnh mẽ. Lấy mẫu hoạt động bằng cách sửa đổi hàm điểm số như sau, đảm bảo tất cả điểm số không âm để tính đúng đắn của phương pháp tìm kiếm được duy trì:

score(wi) = -log p(wi|zi) + alpha_split + |wi| × exp(ε)
ε ~ N(0, sigma_sample²)

Chúng tôi đặt tham số sigma_sample thành 0.02 trong tất cả thí nghiệm lấy mẫu.

### 3.3 Ứng dụng

**Lớp nhúng**. Ba chỉ số từ phụ FACTORIZER được nhúng với các lớp nhúng riêng biệt, cộng lại và biến đổi với phi tuyến GeLU. Cuối cùng, chúng ta có một vector nhúng duy nhất cho mỗi từ phụ, làm cho nó trở thành một thay thế trực tiếp đơn giản cho các lớp nhúng tiêu chuẩn.

**Trung bình hóa**. Lấy mẫu từ phụ ngẫu nhiên có thể được sử dụng cho suy luận mạnh mẽ hơn bằng cách lấy mẫu nhiều mã hóa của mỗi thể hiện dữ liệu và trung bình hóa các dự đoán cho mỗi mã hóa.

## 4 Thí nghiệm

Hiệu quả của FACTORIZER được xác minh thông qua thí nghiệm trên một tập hợp ngôn ngữ đa dạng về loại hình học. Chúng tôi huấn luyện các mô hình ngôn ngữ có mặt nạ trên mỗi ngôn ngữ và sau đó đánh giá chúng trên các nhiệm vụ hình thái-cú pháp từ các kho dữ liệu Universal Dependencies (UD) (Nivre et al., 2016). Ngoài ra, chúng tôi chứng minh rằng các mô hình ngôn ngữ dựa trên FACTORIZER thể hiện tính mạnh mẽ lớn hơn đối với nhiễu và hiệu suất vượt trội trong các thiết lập tài nguyên thấp. Chúng tôi trình bày bốn bộ thí nghiệm trong phần này:

1. Để cung cấp một số quan sát ban đầu, chúng tôi đánh giá các mô hình phân tích đơn giản không dựa vào bất kỳ nhúng được tiền huấn luyện nào.

2. Sau đó, chúng tôi nghiên cứu ablation các cấu hình bộ mã hóa khác nhau với mô hình hóa ngôn ngữ tiếng Anh và tinh chỉnh UD.

3. Như thí nghiệm chính, chúng tôi tiền huấn luyện và tinh chỉnh các mô hình ngôn ngữ trên 7 ngôn ngữ đa dạng về loại hình học.

4. Cuối cùng, các thí nghiệm có kiểm soát trên tiếng Anh kiểm tra tính mạnh mẽ của phương pháp chúng tôi đối với nhiễu và khan hiếm dữ liệu.

**Ngôn ngữ**. Tổng cộng, phương pháp của chúng tôi được đánh giá trên 7 ngôn ngữ khác nhau để nghiên cứu hiệu suất của nó trên các loại hình hệ thống hình thái khác nhau. Phần lớn, chúng tôi tuân theo Vania và Lopez (2017) trong việc lựa chọn ngôn ngữ theo các danh mục truyền thống của hệ thống hình thái. Corpus văn bản cho mỗi ngôn ngữ được rút ra từ phần tương ứng của corpus mC4 (Xue et al., 2021), trừ khi được chỉ định khác. Lưu ý rằng chúng tôi sử dụng cùng corpus để huấn luyện các mô hình FACTORIZER (nơi chúng tôi trích xuất danh sách tần số từ cho mọi ngôn ngữ) như để huấn luyện các mô hình ngôn ngữ.

**Baseline bộ mã hóa BPE**. Chúng tôi so sánh hiệu suất của FACTORIZER với thuật toán nén mã hóa cặp byte (BPE; Gage, 1994; Sennrich et al., 2016) như bộ mã hóa từ phụ được sử dụng phổ biến nhất. Theo các cải tiến mô hình hóa ngôn ngữ gần đây (Radford et al., 2019), chúng tôi sử dụng BPE trực tiếp trên byte UTF-8 thay vì ký tự unicode - tương tự như FACTORIZER, cũng sử dụng byte UTF-8 làm đơn vị nguyên tử. Bản thân việc huấn luyện sử dụng triển khai mở từ thư viện tokenizers và sử dụng các corpus văn bản giống như các mô hình FACTORIZER tương ứng.

### 4.1 Thí nghiệm 1: Bộ phân tích UD 'từ đầu'

Trước khi huấn luyện các mô hình ngôn ngữ lớn và đánh giá các bộ mã hóa trong thiết lập tốn nhiều tài nguyên hơn, chúng tôi thúc đẩy phương pháp được đề xuất trên một mô hình đơn giản không sử dụng bất kỳ nhúng từ được tiền huấn luyện nào.

**Kiến trúc bộ phân tích UD**. Chúng tôi dựa trên thí nghiệm đầu tiên này trên UDPipe 2, một mô hình có sẵn công khai cho gắn thẻ PoS, bổ đề hóa và phân tích cú pháp phụ thuộc bởi Straka et al. (2019). Chúng tôi nhìn chung tuân theo kiến trúc ban đầu, chỉ thay thế nhúng từ của nó - UDPipe 2 sử dụng kết hợp nhúng ngữ cảnh hóa được tiền huấn luyện, nhúng tĩnh và nhúng ký tự - thay vào đó, chúng tôi chỉ sử dụng nhúng từ phụ được khởi tạo ngẫu nhiên và học chúng từ đầu; các biểu diễn ngữ cảnh hóa cuối cùng cho các từ phụ được tổng hợp trung bình cho mỗi token từ để có được biểu diễn cấp từ. Nếu không thì chúng tôi sử dụng cùng cách tiếp cận mô hình hóa chung dựa trên các lớp LSTM hai chiều (Hochreiter và Schmidhuber, 1997) theo sau bởi các đầu phân loại riêng lẻ cho gắn thẻ UPOS, XPOS và UFeats, một đầu phân loại cho các quy tắc bổ đề tương đối và một đầu cuối cùng cho phân tích cú pháp phụ thuộc (Dozat và Manning, 2017).

**Kết quả**. Lượng phân tách trên mỗi từ có tác động đáng chú ý đến hiệu suất, do đó chúng tôi bao gồm chiều này trong đánh giá, được trực quan hóa trong Hình 4. Điều này cho thấy FACTORIZER rõ ràng là một bộ mã hóa Pareto-tối ưu trong so sánh này. Chúng tôi đoán rằng điều này được gây ra bởi khả năng học một nhúng sử dụng được cho các từ phụ ít thường xuyên hơn, hoặc thậm chí ngoài từ vựng, như minh họa trong Hình 5.

### 4.2 Thí nghiệm 2: Nghiên cứu ablation với mô hình hóa ngôn ngữ tiếng Anh

Để đánh giá hiệu suất của các cấu hình BPE và FACTORIZER khác nhau, chúng tôi tiến hành một nghiên cứu so sánh trên tiếng Anh.

**Tiền huấn luyện mô hình ngôn ngữ**. Trong thực tế, khả năng của một bộ phân tích UD có thể được cải thiện đáng kể bằng cách sử dụng nhúng ngữ cảnh hóa từ các mô hình ngôn ngữ lớn đã được huấn luyện trên lượng dữ liệu khổng lồ. Dưới ánh sáng này, trọng tâm chính của phần thí nghiệm này là đánh giá tác động của FACTORIZER lên các mô hình ngôn ngữ và hiệu suất downstream của chúng.

Chúng tôi tuân theo ngân sách huấn luyện và tham số của BERT base ban đầu (Devlin et al., 2019) để tiền huấn luyện các mô hình ngôn ngữ, mô hình dựa trên BPE cũng sử dụng kích thước từ vựng 32K của BERT. Nhưng không giống như BERT base, chúng tôi thiết lập các mô hình của chúng tôi trên kiến trúc LTG-BERT hiệu quả hơn (Samuel et al., 2023), tăng cường các mô hình với một số cải tiến gần đây, như chuẩn hóa lớp NormFormer (Shleifer và Ott, 2022), chú ý tách rời với mã hóa vị trí tương đối (He et al., 2021) hoặc che span (Joshi et al., 2020).

Để giảm thời gian huấn luyện, tiền huấn luyện được song song hóa trên 128 GPU, tổng kích thước batch được tăng lên 8.192 và số bước được giảm xuống 31.250, phù hợp với ngân sách huấn luyện của BERT base.

Để làm cho so sánh giữa cả hai bộ mã hóa công bằng, chúng tôi xấp xỉ khớp số lượng tham số có thể huấn luyện trong tất cả các mô hình ngôn ngữ. FACTORIZER yêu cầu các lớp nhúng tương đối nhỏ, vì vậy chúng tôi chuyển 'ngân sách tham số' của nó sang 3 lớp transformer bổ sung - sau đó các mô hình dựa trên BPE sử dụng 110.8M tham số trong khi các mô hình dựa trên FACTORIZER sử dụng tổng cộng 108.1M tham số.

**Tinh chỉnh UD**. Chúng tôi sử dụng cùng mô hình như trong Phần 4.1, chỉ thay thế các lớp LSTM bằng một kết hợp lồi của các lớp ẩn từ một mô hình ngôn ngữ được tiền huấn luyện, tương tự như UDify (Kondratyuk và Straka, 2019). Sau đó chúng tôi tinh chỉnh toàn bộ mô hình trên một kho dữ liệu UD.

**Kết quả**. So sánh được trình bày trong Bảng 1. Trung bình, FACTORIZER vượt trội hơn BPE, đặc biệt trong bổ đề hóa. Kết quả cho thấy cả kỹ thuật lấy mẫu và trung bình hóa (được thảo luận trong Phần 3) đều cải thiện hiệu suất, với FACTORIZER đặc biệt được hưởng lợi từ những tính năng này.

### 4.3 Thí nghiệm 3: Đánh giá đa ngôn ngữ

Cuối cùng, chúng tôi huấn luyện 7 mô hình ngôn ngữ dựa trên BPE và FACTORIZER trên các ngôn ngữ được chọn và tinh chỉnh chúng trên các kho dữ liệu UD tương ứng.

**Kết quả**. Kết quả, được hiển thị trong Bảng 2, cho thấy FACTORIZER rõ ràng đạt hiệu suất tốt hơn BPE trung bình. Hơn nữa, FACTORIZER nhất quán hiệu quả hơn trong bổ đề hóa (với ý nghĩa thống kê), gợi ý rằng các từ phụ nhân tử thực sự có thể mang thông tin về các ký tự bên trong các đơn vị từ phụ.

Thú vị là, FACTORIZER không phải lúc nào cũng hoạt động tốt hơn trên tiếng Séc và tiếng Thổ Nhĩ Kỳ, mặc dù chúng tôi giả thuyết rằng những ngôn ngữ này nên được hưởng lợi từ thông tin ký tự tốt hơn do hình thái phong phú của chúng. Cả BPE hoặc FACTORIZER đều không phải là lựa chọn tốt hơn tổng thể cho những ngôn ngữ này.

Trong khi kết quả cho phân tích cú pháp phụ thuộc riêng lẻ (được đo bằng UAS và LAS) khá hỗn hợp, kết quả phân tích cú pháp xem xét thêm hình thái (MLAS) và bổ đề hóa (BLEX) phần lớn ủng hộ mã hóa từ phụ nhân tử. FACTORIZER vượt trội hơn BPE (với ý nghĩa thống kê) ở 6 trong 7 ngôn ngữ về điểm BLEX.

### 4.4 Thí nghiệm 4: Tính mạnh mẽ

**Tính mạnh mẽ đối với nhiễu**. Khả năng duy trì hiệu suất khi xử lý văn bản chưa được chuẩn hóa và có nhiễu, là một chất lượng quan trọng của bất kỳ công cụ NLP nào được triển khai trong các tình huống thế giới thực. Để đánh giá cách FACTORIZER xử lý nhiễu không mong đợi, chúng tôi tinh chỉnh một bộ phân tích UD trên dữ liệu sạch sử dụng corpus en-ewt và đánh giá nó trên các tập phát triển đã sửa đổi với các mức độ nhiễu ký tự tăng dần. Nhiễu được đưa vào bằng cách làm nhiễu mỗi ký tự với xác suất thiết lập pnoise bằng cách chọn đồng nhất từ (1) xóa ký tự, (2) thay đổi chữ hoa chữ thường của nó, hoặc (3) lặp lại ký tự 1-3 lần.

Hình 6 cho thấy mối quan hệ giữa mức nhiễu pnoise và hiệu suất dự kiến. Khi so sánh FACTORIZER với BPE, rõ ràng FACTORIZER mạnh mẽ hơn đối với nhiễu tăng trong gắn thẻ và phân tích cú pháp phụ thuộc vì hiệu suất giảm chậm hơn với nhiễu tăng. Độ chính xác giảm như nhau trong bổ đề hóa, có thể được quy cho công thức của nó như dự đoán các quy tắc bổ đề tương đối (như trong UDPipe 2 bởi Straka et al. (2019)).

**Tính mạnh mẽ đối với khan hiếm tài nguyên**. Kết quả đầy hứa hẹn đạt được trên ngôn ngữ tài nguyên rất thấp của tiếng Gaelic Scotland đã thúc đẩy một cuộc kiểm tra kỹ lưỡng hơn về mối quan hệ giữa kích thước của corpus tiền huấn luyện và hiệu suất downstream. Để nghiên cứu mối quan hệ này, chúng tôi tiền huấn luyện nhiều mô hình ngôn ngữ tiếng Anh sử dụng các phần ngẫu nhiên của corpus tiếng Anh đầy đủ trong khi duy trì số bước huấn luyện không đổi. Hiệu suất của các mô hình được tinh chỉnh được đánh giá và vẽ biểu đồ trong Hình 7. Kết quả chứng minh rằng phương pháp được đề xuất của chúng tôi mạnh mẽ hơn đối với khan hiếm tài nguyên. Tất cả các mô hình ngôn ngữ có thể duy trì hiệu suất với 1/64 corpus đầy đủ và sau điểm này, các mô hình dựa trên FACTORIZER ít nhạy cảm hơn với việc giảm kích thước corpus hơn nữa.

## 5 Công trình liên quan

Kỷ nguyên thần kinh trong NLP đã mang lại sự thay đổi trong cách các câu thường được mã hóa thành các đơn vị nguyên tử. Mã hóa trong NLP hiện tại thường liên quan đến việc phân đoạn câu thành từ phụ, các đơn vị nhỏ hơn so với các token từ truyền thống của kỷ nguyên tiền thần kinh. Câu hỏi về những đơn vị từ phụ này nên bao gồm gì đã nhận được khá nhiều sự chú ý trong nghiên cứu trước đây. Các cách tiếp cận có động lực ngôn ngữ học hoặc dựa trên quy tắc được tìm thấy (Porter, 1980), tuy nhiên phần lớn công trình này dựa trên phân đoạn hình thái không giám sát. Trong khi lĩnh vực này có lịch sử lâu đời (Mielke et al., 2021), từ phụ trong những năm gần đây đã dựa trên BPE (Gage, 1994; Sennrich et al., 2016) và các biến thể như WordPiece (Wu et al., 2016) hoặc SentencePiece (Kudo và Richardson, 2018).

Đã có khá nhiều nghiên cứu kiểm tra ảnh hưởng của các hệ thống hình thái khác nhau lên hiệu suất mô hình hóa ngôn ngữ (Vania và Lopez, 2017; Gerz et al., 2018; Bostrom và Durrett, 2020). Trong thiết lập đa ngôn ngữ cao, kiểm tra 92 ngôn ngữ, Park et al. (2021) nghiên cứu ảnh hưởng của hình thái lên độ khó mô hình hóa ngôn ngữ, đối chiếu BPE với hệ thống Morfessor (Creutz và Lagus, 2007) và một bộ phân đoạn hình thái dựa trên quy tắc. Cũng đã có một số công trình so sánh BPE với các bộ mã hóa thay thế cho các ứng dụng downstream, chủ yếu trong dịch máy và phần lớn với kết quả tiêu cực (Ataman và Federico, 2018; Domingo et al., 2018; Machá ˇcek et al., 2018). Trong công trình này, chúng tôi thay vào đó kiểm tra một số nhiệm vụ hình thái-cú pháp, tìm thấy những cải thiện rõ ràng so với BPE.

Bộ mã hóa dựa trên pixel gần đây của ngôn ngữ (PIXEL; Rust et al., 2023) tái định hình mô hình hóa ngôn ngữ như một nhiệm vụ nhận dạng thị giác, sử dụng bộ mã hóa-giải mã dựa trên transformer được huấn luyện để tái tạo các pixel trong các vùng ảnh bị che, và loại bỏ hoàn toàn lớp nhúng từ vựng. Phương pháp cho thấy kết quả mạnh cho các chữ viết chưa thấy, tuy nhiên, hiệu suất khiêm tốn hơn cho các ngôn ngữ với chữ viết Latin, như tiếng Anh. Như trong công trình của chúng tôi, họ thấy các nhiệm vụ hình thái-cú pháp được hưởng lợi nhiều nhất và các nhiệm vụ ngữ nghĩa hơn cho thấy kết quả hỗn hợp hơn.

## 6 Kết luận

Chúng tôi đã đề xuất một phương pháp mã hóa mới trong đó mỗi token từ phụ được nhân tử hóa thành các bộ ba chỉ số. Lợi ích chính của nhân tử hóa này là các đơn vị từ phụ duy trì một số thông tin cấp ký tự mà không tăng độ dài của các chuỗi đã mã hóa. Trong thực tế, FACTORIZER thậm chí giảm nhẹ số lượng phân tách từ phụ (Hình 3), trong khi cải thiện đáng chú ý hiệu suất của các mô hình ngôn ngữ lớn trên các nhiệm vụ hình thái-cú pháp, đặc biệt là trên bổ đề hóa (Bảng 2). Các thí nghiệm tiếp theo đã chứng minh tăng tính mạnh mẽ đối với nhiễu và khan hiếm dữ liệu (Hình 6 và 7).

Chúng tôi hy vọng rằng công trình này đã chứng minh rõ ràng tiềm năng của việc sử dụng từ phụ nhân tử cho mô hình hóa ngôn ngữ và công trình tương lai sẽ cải thiện hiệu suất của chúng hơn nữa.

## Hạn chế

**Kích thước**. Một yếu tố hạn chế cho một số ứng dụng tài nguyên thấp có thể là kích thước của tệp FACTORIZER đã lưu. Chúng tôi chỉ phải lưu trữ từ vựng từ phụ, tuy nhiên điều này tốn không gian đáng kể hơn BPE vì nó cần được lưu trữ như DAWG trie để giữ tốc độ mã hóa tương tự như BPE. Ví dụ, FACTORIZER tiếng Anh đã lưu tốn khoảng 115MB không gian trong khi BPE tiếng Anh với 32K từ phụ chỉ tốn khoảng 1MB không gian. Chúng tôi tin rằng yêu cầu không gian là không đáng kể so với kích thước của các mô hình ngôn ngữ lớn, nhưng chúng có thể là yếu tố hạn chế trong một số trường hợp cạnh.

Hiệu quả tham số của FACTORIZER tuân theo bản chất cơ bản của các biểu diễn nhân tử: một chuỗi 3 byte có thể biểu diễn hơn 16M giá trị (256³). Đó là cách chúng ta có thể nhúng hàng triệu từ phụ với số lượng tham số không đáng kể. Mặt khác, khi chúng ta lưu trữ từ vựng với hàng triệu từ phụ trên đĩa, nó nhất thiết yêu cầu nhiều không gian hơn từ vựng BPE với hàng chục nghìn từ phụ.

**Hiệu suất GLUE**. Trong khi sự quan tâm và trọng tâm chính của chúng tôi là các nhiệm vụ downstream hình thái-cú pháp, việc hỏi hiệu suất của các mô hình ngôn ngữ dựa trên FACTORIZER trên các nhiệm vụ khác, như hiểu ngôn ngữ tự nhiên, là hợp lý. Chúng tôi sử dụng các mô hình ngôn ngữ tiếng Anh được tiền huấn luyện và tinh chỉnh chúng trên 8 nhiệm vụ GLUE (Wang et al., 2018). Kết quả trong Phụ lục C cho thấy trong thiết lập này, phương pháp của chúng tôi có thể so sánh với BPE nhưng không tốt hơn trung bình, mặc dù cả hai cách tiếp cận đều nằm trong một độ lệch chuẩn từ nhau. Chúng tôi hy vọng rằng những kết quả này có thể được cải thiện trong công trình tương lai.

## Lời cảm ơn

Chúng tôi muốn cảm ơn Petter Mæhlum, Andrey Kutuzov và Erik Velldal vì đã cung cấp phản hồi rất hữu ích về công trình này.

Những nỗ lực được mô tả trong bài báo hiện tại được tài trợ chung bởi dự án HPLT (Công nghệ Ngôn ngữ Hiệu suất Cao; được điều phối bởi Đại học Charles).

Các tính toán được thực hiện trên các tài nguyên được cung cấp thông qua Sigma2 - nhà cung cấp cơ sở hạ tầng nghiên cứu quốc gia cho Tính toán Hiệu suất Cao và lưu trữ dữ liệu quy mô lớn ở Na Uy.
