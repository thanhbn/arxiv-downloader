SelfCP: Nén prompt vượt giới hạn thông qua chính mô hình ngôn ngữ lớn đã đóng băng

Jun Gao, Ziqiang Cao, Wenjie Li

•Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên sử dụng chính LLM đã đóng băng để nén các prompt vượt giới hạn thành 1/12 token bộ nhớ, điều này tổng quát hơn và ít tốn kém hơn.

•Chúng tôi đề xuất một góc nhìn nén prompt có tính biện chứng hơn nhằm đạt được sự cân bằng giữa chi phí huấn luyện, hiệu quả suy luận và chất lượng sinh ra.

SelfCP: Nén prompt vượt giới hạn thông qua chính mô hình ngôn ngữ lớn đã đóng băng

Jun Gao, Ziqiang Cao∗ và Wenjie Li

Viện Trí tuệ nhân tạo, Đại học Soochow, Tô Châu, Trung Quốc

TÓM TẮT

Prompt dài dẫn đến chi phí phần cứng khổng lồ khi sử dụng các mô hình ngôn ngữ lớn (LLM) dựa trên transformer. Thật không may, nhiều tác vụ như tóm tắt, không tránh khỏi việc giới thiệu các tài liệu dài, và việc ứng dụng rộng rãi của in-context learning dễ dàng làm cho độ dài prompt bùng nổ. Bài báo này đề xuất một Self-Compressor (SelfCP), sử dụng chính LLM đích để nén các prompt vượt giới hạn thành các vector dày đặc trong khi giữ nguyên những prompt được phép. Các vector dày đặc sau đó được chiếu thành các token dày đặc thông qua một connector có thể học để làm cho cùng một LLM có thể hiểu được mà không gặp khó khăn. Connector được supervised-tuned dưới mục tiêu language modeling của LLM trên các văn bản tương đối dài được chọn từ các tập dữ liệu có thể truy cập công khai, bao gồm một tập dữ liệu instruction để làm cho SelfCP phản hồi các prompt khác nhau, trong khi LLM đích được giữ đóng băng trong quá trình huấn luyện. Chúng tôi xây dựng SelfCP nhẹ trên 2 backbone khác nhau với chỉ 17M tham số có thể học xuất phát từ connector và một embedding có thể học. Đánh giá trên các benchmark tiếng Anh và tiếng Trung cho thấy SelfCP hiệu quả thay thế 12×prompt vượt giới hạn bằng các token dày đặc để giảm chi phí bộ nhớ và tăng thông lượng suy luận, đồng thời cải thiện chất lượng phản hồi. Hiệu suất xuất sắc mang lại một giải pháp hiệu quả cho LLMs để xử lý các prompt dài mà không cần huấn luyện LLMs từ đầu.

1. Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) dựa trên Transformer thể hiện những khả năng nổi lên tổng quát ấn tượng. Tuy nhiên, hiệu suất của LLMs phụ thuộc rất nhiều vào các prompt được thiết kế tốt, trong khi các prompt dài dẫn đến bùng nổ bộ nhớ và gây ra những vấn đề hiệu quả khác. Thật không may, các prompt cho nhiều tác vụ phải bao gồm các đầu vào dài, chẳng hạn như trả lời câu hỏi (QA), tóm tắt và tóm tắt tập trung vào truy vấn (QFS). Trong khi đó, việc ứng dụng rộng rãi của In-Context Learning (ICL) càng làm tăng độ dài của các prompt bằng cách giới thiệu nhiều demonstration.

Ngoại trừ những nỗ lực trong việc mở rộng cửa sổ đầu vào, các nghiên cứu trước đây tập trung nhiều hơn vào việc cắt bỏ prompt hoặc thay thế prompt bằng soft prompts. Tuy nhiên, LLMs với cửa sổ ngữ cảnh triệu token vẫn gặp khó khăn trong việc khắc phục sự suy giảm hiệu suất khi xử lý các prompt dài. Các phương pháp cắt bỏ prompt phải đối mặt với vấn đề Out-of-Distribution (OoD), đòi hỏi việc alignment đắt đỏ giữa compressor và LLMs đích. Các phương pháp nén soft prompt trước đây phản hồi các truy vấn chỉ dựa vào soft prompts với mục đích tối thiểu hóa độ dài prompt, điều này làm tăng khó khăn sinh ra và đòi hỏi huấn luyện mô hình từ đầu. Những thách thức này tạo thành một tam giác bất khả thi: chúng ta không thể đồng thời quản lý chi phí huấn luyện, độ trễ suy luận và chất lượng phản hồi. Khác với những nỗ lực trước đây về nén soft prompt, chúng tôi cung cấp một góc nhìn biện chứng để nén prompt, đạt được sự đánh đổi giữa các yếu tố này.

Với những mục tiêu này trong tâm trí, chúng tôi đề xuất SelfCP như minh họa trong Hình 1, sử dụng khả năng hiểu của LLMs được phát triển trong quá trình pre-training để nén các prompt vượt giới hạn. Do đó, một module nén bổ sung không cần thiết phải giới thiệu vì SelfCP sử dụng LLM đích để nén prompt, giảm bộ nhớ GPU cho cả huấn luyện và suy luận ban đầu. Hơn nữa, được thúc đẩy bởi sự thành công của các mô hình ngôn ngữ thị giác (VLMs) dựa trên Perceiver đã trích xuất các đặc trưng thị giác thông qua một Visual Transformer (ViT) đã đóng băng, chúng tôi giữ LLM đích đóng băng trong quá trình huấn luyện, giảm chi phí huấn luyện chính từ backbone cơ bản.

Tuy nhiên, vì LLM tích hợp của VLM không thể hiểu các đặc trưng thị giác được trích xuất từ ViT mà không có adapter (ví dụ, Q-Former hoặc Multi-Layer Perceptron (MLP)), một connector rất quan trọng cho SelfCP để chuyển đổi các đặc trưng đầu ra của compressor thành các token nén có thể đọc được (gọi là memory tokens trong bài báo này) cho generator. Chúng tôi đơn giản sử dụng một lớp tuyến tính được supervised-tuned dưới mục tiêu language modeling của generator, phục vụ như connector giữa compressor và generator, và chúng tôi kêu gọi khám phá thêm về việc lựa chọn adapter.

Các nghiên cứu trước đây phản hồi truy vấn dựa vào soft prompts đơn thuần, điều này làm tăng khó khăn tác vụ, chi phí huấn luyện và lãng phí việc sử dụng cửa sổ ngữ cảnh hợp lý của generator. Được truyền cảm hứng từ thực tế rằng một số đoạn trong prompt có giá trị và thông tin trong khi việc nén chúng có vẻ không cần thiết, chúng tôi khám phá một giải pháp trung gian cho phép SelfCP chỉ nén các prompt vượt giới hạn (ví dụ, những cái ban đầu bị cắt bỏ hoặc tương đối ít quan trọng hơn) thay vì nén mọi thứ một cách tùy tiện. Phần còn lại của prompt không thay đổi sau đó được đưa trực tiếp vào generator.

Giải pháp này tận dụng đầy đủ các cửa sổ được phép bị lãng phí trong các nghiên cứu trước đây và giảm khó khăn nén và sinh ra, nhưng nó vẫn phải đối mặt với những thách thức sau: (1) Soft prompt được nén không thể luôn được đặt ở phía trước như trước đây, vì các prompt vượt giới hạn có thể vừa là phần trước vừa là phần sau. (2) Các prompt vượt giới hạn có thể vẫn quá dài để compressor chấp nhận trong khi việc bỏ qua trực tiếp mâu thuẫn với mục tiêu của chúng tôi. Khác với các nghiên cứu trước đây cố định chế độ nén, chúng tôi đa dạng hóa các chiến lược nén để phục vụ các yêu cầu thêm trong các tình huống khác nhau để tiếp cận những thách thức này: (a) Former Compression nén nửa trước của prompt và đặt memory tokens ở phía trước nửa sau không được nén. (b) Latter Compression nén nửa sau của prompt và đặt memory tokens ở phía sau nửa trước không được nén. (c) Concatenated Compression nén một số sub-prompts (ví dụ, in-context demonstrations) thành memory tokens cục bộ độc lập và nối chúng để tạo thành memory tokens toàn cục.

Chúng tôi xây dựng SelfCP trên hai backbone (Vicuna-7b và BlueLM-7b) để xác minh khả năng tổng quát hóa và đánh giá SelfCP trên phạm vi các tác vụ ngoài domain bao gồm các tác vụ sinh ra và hiểu. Sau đó, các thí nghiệm xác thực trong domain cho thấy SelfCP được tối ưu hóa với ba chiến lược nén hiệu quả thay thế 12×prompt vượt giới hạn bằng soft prompts. Các đóng góp chính của chúng tôi như sau:

•Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên sử dụng chính LLM đã đóng băng để nén các prompt vượt giới hạn thành 1/12 memory tokens, điều này tổng quát hơn và ít tốn kém hơn.

•Chúng tôi đề xuất một góc nhìn nén prompt có tính biện chứng hơn nhằm đạt được sự cân bằng giữa chi phí huấn luyện, hiệu quả suy luận và chất lượng sinh ra.

2. Nghiên cứu liên quan

2.1. Soft Prompt

Khác với các prompt thông thường bao gồm các token thực tế rời rạc, mỗi token tương ứng với embedding được pre-trained, soft prompts là các embedding liên tục được khởi tạo mới. Một mặt, soft prompts thường được sử dụng như một phương pháp hiệu quả về tham số, chẳng hạn như Prefix-Tuning và P-Tuning, trong khi giữ backbone đóng băng và tuning các embedding được khởi tạo mới cho mỗi tác vụ. Mặt khác, các nhà nghiên cứu cố gắng sử dụng soft prompts để nén các prompt từ các chuỗi cụ thể thành các token ảo.

Chủ yếu từ góc độ distillation, Wingate et al. đã aligned mô hình teacher và mô hình student, trong đó mô hình teacher chấp nhận prompt thực tế trong khi mô hình student được feed soft prompt. Nhược điểm chính của phương pháp này là thiếu khả năng tổng quát hóa đòi hỏi huấn luyện cho mỗi instruction cụ thể khác nhau về từ vựng. Để giải quyết vấn đề tổng quát hóa, Mu et al. đã đề xuất học một Llama-7b để nén instruction thành các token ảo, preceding attention past key values tương tự như Prefix, nhưng chỉ nén instructions thì không đủ mạnh vì demonstrations hoặc input dài hơn nhiều so với instruction trong nhiều tác vụ như tóm tắt và QA.

Để nén prompt dài, Chevalier et al. đã đề xuất AutoCompressor để tạo ra các token ảo được nén dựa trên OPT-2.7b được fine-tuned. Họ đầu tiên phân đoạn ngẫu nhiên các văn bản có hàng nghìn từ thành phạm vi mô hình có thể chấp nhận và sau đó recursively tạo ra soft prompts cho mỗi đoạn, và các soft prompts trước đó sẽ được nối với đoạn hiện tại để tạo ra soft prompts mới. Tuy nhiên, AutoCompressor không chỉ đòi hỏi fine-tuning trên một tập huấn luyện lớn, mà việc encoding soft prompts cũng sẽ chậm hơn nhiều và mang lại chi phí bộ nhớ thêm do Recurrent Memory Transformer (RMT).

Tương tự, Ge et al. đã đề xuất ICAE sử dụng Llama-7b được adopted LoRA để nén các prompt đã xử lý đơn thuần thành các token ảo được nén. Tuy nhiên, ICAE vẫn gặp khó khăn trong việc xử lý các prompt dài hơn cửa sổ đầu vào cho phép.

2.2. Nén trích xuất

Ngoài việc sử dụng soft prompts, các nhà nghiên cứu cũng nỗ lực rút ngắn prompt bằng cách trích xuất các token thông tin từ những token gốc, cụ thể là token pruning hoặc token merging. Các nghiên cứu gần đây như LLMLingua và Selective Context có điểm tương đồng nhưng khác nhau về việc có loại bỏ token với Perplexity (PPL) cao hay thấp. LLMLingua nhấn mạnh các token với PPL cao, cho rằng chúng có ảnh hưởng nhiều hơn, dẫn đến việc đạt được hiệu suất state-of-the-art (SOTA).

Như đã đề cập trong bài báo của họ, các phương pháp nén trích xuất gặp phải vấn đề Out-of-Distribution (OoD) giữa extractor và LLM đích. Để hòa giải điều này, họ đã fine-tuned Alpaca-7b hoặc GPT2-Alpaca sử dụng tập dữ liệu Alpaca để align với LLMs đích. Tuy nhiên, các phương pháp nén trích xuất phụ thuộc rất nhiều vào khả năng của LLM đích trong việc hiểu các token rời rạc, và alignment thường khá đắt đỏ và được thiết kế riêng cho mỗi target.

2.3. Long Input Transformer

Chi phí huấn luyện đáng kể và hỗ trợ phần cứng giới hạn độ dài đầu vào cho các mô hình ngôn ngữ được pre-trained. Một loạt các nghiên cứu trước đây tập trung vào việc sparse hóa cửa sổ attention đầy đủ thành linear attention, với sự đánh đổi giữa hiệu quả và horizon attention. Các nghiên cứu khác đã approximated hoặc thay thế toàn bộ cơ chế attention. Tuy nhiên, việc sparse hóa và approximating hoặc thay thế đã thay đổi kiến trúc của transformer tiêu chuẩn hoặc mục tiêu huấn luyện và do đó đòi hỏi huấn luyện từ đầu, điều này đắt đỏ, đặc biệt là khi scale mô hình hoặc tập huấn luyện.

Bertsch et al. đã đề xuất offload tính toán cross-attention cho một chỉ mục k-nearest-neighbor (kNN) duy nhất. Mặc dù kNN không có tham số, việc retrieve hidden states của encoder trong mỗi bước sinh ra sẽ làm chậm suy luận, và phương pháp của họ sẽ bị breakdown khi đối mặt với các mô hình decoder-only.

SelfCP giữ LLM đóng băng và không có giới hạn trong việc xây dựng trên các LLMs mạnh mẽ hiện có. Do đó, các phương pháp trên có thể tăng cường SelfCP về mặt lý thuyết.

3. Phương pháp luận

Chúng tôi đề xuất SelfCP, một mô hình hiệu quả về tham số nén prompts thông qua chính LLM. Đối với việc lựa chọn LLM cơ bản, nghiên cứu trước đây đã chứng minh rằng mô hình Decoder-only thực hiện tốt hơn mô hình Encoder-Decoder trong việc nén instructions. Chúng tôi tuân theo kết luận này và sử dụng Vicuna-7b và BlueLM-7b làm hai backbone độc lập, đại diện cho các LLMs chuyên về tiếng Anh và tiếng Trung.

Hầu hết các phương pháp nén dựa trên soft prompts đổi mới về compressor. Chúng tôi minh họa sự khác biệt trong quá trình formulation soft prompt cho SelfCP, AutoCompressor và ICAE trong Hình 2. Để giải thích một cách đơn giản, chúng tôi giả định lý tưởng rằng compressor và generator của ba phương pháp nén có giới hạn cửa sổ L, có cùng tỷ lệ nén, và bây giờ có một prompt có độ dài N, trong đó N = 3 × L, bỏ qua độ dài của soft prompts và truy vấn.

Xem xét AutoCompressor, prompt sẽ được chia thành ba đoạn (S1, S2, S3), và AutoCompressor nén từng đoạn từng bước một. Đáng chú ý, chúng tôi cung cấp cho AutoCompressor một thiết lập tolerance mà prompt được chia đều trong khi chúng được phân đoạn ngẫu nhiên ban đầu, nhưng AutoCompressor vẫn đòi hỏi 3 lần nén không song song. Khi đến ICAE, chỉ 1/3 prompt có thể truy cập cho compressor và những cái khác sẽ không được đọc bằng cách nào. Trong trường hợp này, ICAE chỉ cần 1 lần để thực hiện nén luôn, vì phần còn lại bị dropout.

AutoCompressor cho thấy ưu điểm về độ dài prompt có thể đọc, nhưng thiếu về hiệu quả, trong khi ICAE có complexity nén không đổi nhưng gặp khó khăn trong việc tiếp cận các prompt khá dài. SelfCP học hỏi từ kỹ năng của nhau một cách khéo léo rằng compressor của nó nén từng đoạn một cách không đồng bộ và nối chúng như AutoCompressor. Mặc dù SelfCP nén ít đoạn hơn AutoCompressor, đoạn này được cung cấp trực tiếp cho generator để cân bằng overload giữa compressor và generator. Chúng tôi có xu hướng coi điều này như một sự đánh đổi giữa chi phí huấn luyện, hiệu quả suy luận và chất lượng phản hồi. Sự đánh đổi này đáng giá hơn khi đoạn không thay đổi quan trọng hơn những đoạn khác.

Số bước nén thực tế có thể được tính như ⌈(N-L)/(L*k)⌉, trong đó k chỉ ra rằng một GPU duy nhất có khả năng nén k đoạn trong một batch. Khi dung lượng GPU đủ, k bằng ⌈N/L⌉, đây là tình huống của ICAE nén tất cả các đoạn trong một lần nhưng không bỏ gì, trong khi khi dung lượng GPU chỉ đủ để đặt k=1, nó degenerate thành tình huống AutoCompressor nén các đoạn từng bước một.

3.1. Huấn luyện

Chìa khóa của SelfCP là LLM đích cơ bản đóng vai trò cả compressor và generator trong khi giữ đóng băng trong quá trình huấn luyện. Ngoài lớp tuyến tính có thể học, chúng tôi giới thiệu một embedding đặc biệt có thể học cho SelfCP, memory tag [M], được khởi tạo từ một embedding hiếm khi được sử dụng của LLM đích. Để phục vụ cho việc sử dụng thực tế trong tương lai, chúng tôi giới thiệu các chiến lược nén liên quan trong quá trình huấn luyện:

3.1.1. Former & Latter Compression

Các phương pháp nén trước đây dựa trên soft prompts đặt soft prompt ở đầu như Prefix-Tuning mà bỏ qua thực tế cơ bản rằng các truy vấn thực tế không có mối quan hệ vị trí tương đối nghiêm ngặt với các prompt tương ứng. Ví dụ, chúng ta có thể formulate đầu vào như "Tóm tắt tài liệu dưới đây: [DOC]" hoặc "[DOC] Tóm tắt tài liệu ở trên". Do đó, chúng tôi giới thiệu các instance Former Compression và Later Compression vào huấn luyện.

Cụ thể, đối với các instance prompt ngắn hơn 2×L tokens, cụ thể là hai lần giới hạn cửa sổ, chúng tôi chia đều chúng thành hai đoạn [Sc, Su] và nén ngẫu nhiên phần trước hoặc phần sau.

Cho đoạn cần nén Sc, chuỗi memory tag = [M]×k, và truy vấn hiện tại Q, chúng tôi formulate đầu vào của compressor IC như IC = Q ⊕ Sc ⊕ , trong đó ⊕ đại diện cho concatenation ngang.

Memory tags báo hiệu cho LLM tích hợp đóng vai trò compressor ở đầu input. Sau đó chúng phục vụ như container để hấp thụ thông tin dày đặc từ đoạn trước đó thông qua forward propagation của transformer stack trong compressor. SelfCP thu được hidden states của lớp Transformer cuối cùng trên top của các memory tags được attach Hm = (h¹m, h²m, ..., hᵏm) trong khi bỏ qua những cái khác:

_, Hm = Compressor(IC). (1)

Sau đó, SelfCP chiếu Hm có nguồn gốc từ output space của compressor thành memory tokens có thể chấp nhận được bởi LLM H̃m thông qua connector, trong đó Wp là weight của connector:

H̃m = Wp · Hm. (2)

Giả sử prompt có đoạn không nén Su phía sau đoạn nén Sc, đầu vào của generator IG được formulate như IG = Q ⊕ H̃m ⊕ Su. Cho một phản hồi golden Y = (y1, y2, ..., y|Y|) cho truy vấn và prompt hiện tại, connector trong SelfCP được supervised-tuned dựa trên mục tiêu language modeling của LLM đích theo cách teacher-forcing, trong đó Θ ∈ {Wp, [M]}:

loss = maximizeΘ ∏ᵢ₌₀|Y| (yi | IG ⊕ y<i). (3)

3.1.2. Concatenated Compression

Các nghiên cứu trước đây vẫn gặp khó khăn trong việc xử lý các prompt khá dài với vấn đề hiệu quả hoặc truncation, có độ dài vượt quá cửa sổ đầu vào cho phép của các mô hình ngôn ngữ. Chúng tôi tiếp tục giới thiệu Concatenated Compression để nâng cấp "Former Compression và Latter Compression" trước đó vào huấn luyện của SelfCP để giải quyết vấn đề này. Cụ thể, cả các đoạn nén Sc và những đoạn không nén sẽ vượt quá L tokens khi prompts dài hơn 2×L tokens sau khi phân chia đều.

Trong tình huống này, cho một prompt có tổng cộng N tokens, trong đó N > 2×L, SelfCP đầu tiên phân bổ các đoạn không nén Su với L tokens, và sau đó chia đều phần còn lại thành ⌈N/L - 1⌉ đoạn cục bộ không chồng lấp. Do các đoạn không chồng lấp, compressor nén từng đoạn độc lập như Equ. 1 và chuyển đổi hidden states trên top của memory tags thành memory tokens cục bộ như Equ. 2. Memory tokens toàn cục được cấu hình bằng cách concatenating memory tokens cục bộ theo chiều ngang, được feed vào generator để tối ưu hóa SelfCP như Equ. 3.

3.2. Cải thiện cho In-Context Learning

Cụ thể, trong tình huống ICL, chúng tôi coi prompt chứa các demonstrations và truy vấn chứa các đầu vào tác vụ liên quan và các instruction tác vụ. Chuỗi in-context demonstration thường làm tăng độ dài của prompt, chúng tôi đặc biệt phát triển các chiến lược để tối ưu hóa cả hiệu quả và hiệu quả của ICL thông qua caching và retrieving.

SelfCP phân bổ mỗi demonstration với một đoạn và truncate phần sau để đảm bảo tính độc lập giữa các đoạn. Do đó, SelfCP nén mỗi demonstration độc lập và cache memory tokens của nó để xây dựng Memory Demonstrations Bank (MDB) để tái sử dụng. Với sự giúp đỡ của MDB, SelfCP có thể phản hồi các truy vấn bằng cách trực tiếp rút memory tokens đích mà không cần nén lặp lại, làm cho SelfCP hiệu quả hơn trong ICL.

Các demonstrations được lấy mẫu ngẫu nhiên có hiệu suất ICL không ổn định. Chúng tôi tiếp tục trao quyền cho MDB để hỗ trợ lựa chọn demonstration bằng cách coi memory token đầu tiên ki của demonstration thứ i như key được sử dụng để retrieve. SelfCP yêu cầu nén truy vấn Q để thu được memory token đầu tiên q cho việc lựa chọn demonstration có hướng. Sau đó, điểm demonstration của demonstration thứ i trong MDB được tính toán dựa trên cosine similarity:

scorei = cosine(q, ki). (4)

SelfCP lựa chọn các demonstrations đích theo điểm demonstrations từ cao đến thấp, hiệu quả hơn trong ICL.

4. Thí nghiệm

4.1. Tập dữ liệu

Chúng tôi kết hợp XSum, CICERO và một tập dữ liệu instruction làm tập huấn luyện của chúng tôi, chứa 42k instances, và ước lượng kernel độ dài instance được minh họa trong Hình 3. Chi tiết của các tập test trong và ngoài domain được hiển thị trong Bảng 1. Đáng chú ý, chúng tôi không thực hiện đánh giá trong domain trên tập dữ liệu instruction vì các phản hồi của nó có phần open-ended và nó chỉ có tập huấn luyện. Chúng tôi sử dụng toàn bộ tập test của XSUM và CICERO làm tập trong domain để xác nhận chi tiết của việc nén prompt. Trong đánh giá ngoài domain, chúng tôi sử dụng toàn bộ DUC 2007 làm tập test, và chúng tôi thu thập một tập dữ liệu sinh verdict Trung Quốc (CVG) gọi là CLCV (ChineseLegalCaseVerdict). CLCV có 2.000 instances được thu thập từ China Prosecutorial Network. Mỗi instance của CLCV chứa bản cáo trạng và phán quyết tương ứng với trung bình 540 từ cho bản cáo trạng và 230 từ cho phán quyết.

Khi đánh giá hiệu suất ICL của SelfCP, chúng tôi lấy mẫu 1.500 instances từ các tập dữ liệu tóm tắt XSUM và ARXIV tương ứng, với độ dài trung bình khoảng 580 từ. Ngoài ra, chúng tôi đánh giá SelfCP trên toàn bộ tập dữ liệu linguistic acceptability CoLA thông qua đánh giá closed-ended trong ICL.

4.2. Metrics đánh giá

ROUGE là một metric được áp dụng rộng rãi trong nhiều tác vụ sinh ra để đánh giá mức độ tương tự của hypothesis được sinh ra so với golden label. Do đó, ROUGE được sử dụng trong các thí nghiệm của chúng tôi để đánh giá chất lượng phản hồi được sinh ra dựa trên các token ảo được nén. Chúng tôi báo cáo điểm F-1 của ROUGE-1, ROUGE-2 và ROUGE-L (viết tắt R-1, R-2, R-L trong phần tiếp theo), và chúng tôi sử dụng thư viện files2rouge trong thực tế. Ngoài ra, vì CLCV là tập dữ liệu tiếng Trung, chúng tôi sử dụng thư viện Chinese-ROUGE kết hợp với thư viện cắt từ jieba để đánh giá kết quả được sinh ra. Đối với CoLA, chúng tôi báo cáo độ chính xác.

4.3. Baselines

Chúng tôi so sánh hiệu suất của SelfCP với các LLMs naïve được feed prompts thực tế dựa trên Vicuna-7b và BlueLM-7b tương ứng. Ngoài ra, chúng tôi giới thiệu AutoCompressor và ICAE chuyển đổi toàn bộ prompts thành token ảo, và LLMlingua bỏ các token không thông tin trong prompt.

AutoCompressor là nghiên cứu gần đây nén prompts thành token ảo một cách recurrent với các LLMs được fine-tuned. Chúng tôi sử dụng weight được release chính thức dựa trên Llama2-7b và so sánh hiệu suất của nó với SelfCP trên các tập dữ liệu ngoài domain, đặt tỷ lệ nén giống như SelfCP.

ICAE nén toàn bộ prompts với giới hạn cửa sổ nhất định bởi Llama2 được adapted LoRA. Chúng tôi sử dụng phiên bản Llama2-7b được release chính thức và so sánh hiệu suất của nó với SelfCP trong các tập dữ liệu ngoài domain, đặt tỷ lệ nén giống như SelfCP.

LLMLingua là phương pháp nén prompt coarse-to-fine gần đây dựa trên việc bỏ các từ không thông tin và đạt được hiệu suất mạnh mẽ. Chúng tôi sử dụng LLMLingua từ code chính thức của họ, và so sánh hiệu suất của nó với SelfCP trong tất cả các tập dữ liệu ngoài domain, đặt tỷ lệ nén giống như SelfCP bằng cách giới hạn số lượng token rời rạc bị bỏ. Đáng chú ý, LLMLingua, trong bài báo của họ, được thiết kế để sử dụng một compressor nhỏ (Llama hoặc GPT-2), được instruction-tuned để align với LLM đích (GPT-3.5-Turbo hoặc Claude-v1.3). Để có một so sánh có ý nghĩa, chúng tôi thay thế LLMs đích của họ bằng LLM cơ bản trong SelfCP.

4.4. Thiết lập

Xem xét max tokens trong tất cả các tập dữ liệu liên quan và hiệu quả tính toán, chúng tôi đặt giới hạn cửa sổ đầu vào cho phép tối đa L thành 512. Ngoài ra, chúng tôi cố định learning rate thành 8e-5 và sử dụng Adam làm optimizer, và effective batch size là 32 (8 GPUs data parallelism và 4 steps gradient accumulation). Ngoài ra, chúng tôi tiến hành tất cả các thí nghiệm trên 8*NVIDIA A5000 24G GPUs dựa trên kiểu dữ liệu BFloat 16.

Chúng tôi nén phần sau trong XSUM, DUC và CICERO, vì phần trước trong các tác vụ này quan trọng theo kinh nghiệm, trong khi chúng tôi nén phần trước trong CLCV vì người liên quan được giới thiệu ở phía trước của bản cáo trạng tương đối không quan trọng. Ngoài ra, chúng tôi chia ICL thành hai tình huống: (1) Trong tình huống tài nguyên thấp, chúng tôi cố định demonstrations cho mỗi truy vấn. (2) Trong tình huống tài nguyên cao, SelfCP retrieve các demonstrations tương tự từ MDB bằng cách đo cosine similarity. Chúng tôi coi tập huấn luyện như demonstration pool và xây dựng MDB cho mỗi tập dữ liệu.

4.5. Kết quả

4.5.1. Đánh giá trong Domain

Chúng tôi tiến hành đánh giá trong domain trên XSUM và CICERO trong Bảng 2. SelfCP vượt trội đáng kể so với baseline Vicuna và BlueLM với giới hạn cửa sổ 512 và 1024 sau supervised tuning. Để mô phỏng lợi ích mang lại bởi connector được huấn luyện, chúng tôi cũng LoRA-tune Vicuna và BlueLM trên tập huấn luyện của chúng tôi với 17M tham số có thể huấn luyện bằng cách đặt LoRA rank thành 32 (tham khảo +LoRA trong Tab. 2). Trong trường hợp này, SelfCP vượt trội hơn các backbone được adapted LoRA với 512 cửa sổ cho phép ngay cả khi mô hình được tuned, trong khi SelfCP có thể so sánh với chúng với 1.024 cửa sổ cho phép.

Các kết quả này làm nổi bật rằng truncation cực đoan làm cho LLMs bối rối trong việc phản hồi và các token ảo được nén hiệu quả lọc thông tin nhiễu và khôi phục các phần thông tin ở mức độ lớn.

4.5.2. Đánh giá ngoài Domain

Chúng tôi test hiệu suất ngoài domain của SelfCP trên DUC và CLCV để đánh giá khả năng tổng quát hóa và khả năng cross-lingua, như được thể hiện trong Bảng 3.

SelfCP sử dụng concatenation compression để nén các tài liệu liên quan đến truy vấn thành memory tokens. So với truncation xảy ra trong Vicuna và BlueLM naïve, SelfCP hiệu quả mở rộng cửa sổ in-context, đạt được gần +10 điểm ROUGE-1. Trong khi đó ở CLCV, SelfCP dựa trên BlueLM đạt được hiệu suất tốt hơn so với những cái dựa trên Vicuna vì BlueLM được specific-tuned và giỏi tiếng Trung, chứng minh rằng SelfCP ngầm tận dụng điểm mạnh của các backbone đa dạng trong quá trình học nén prompt.

Ngoài ra, việc nén trên cả phần trước của CLCV và phần sau của DUC chỉ ra rằng SelfCP không tạo ra giới hạn về vị trí của memory tokens. Đối với AutoCompressor, phiên bản 7b underperform SelfCP dựa trên Vicuna trong các tác vụ tiếng Anh (DUC) và underperform SelfCP dựa trên BlueLM trong các tác vụ tiếng Trung (CLCV). Trong khi đó, không ngạc nhiên khi thấy SelfCP vượt trội hơn LLMLingua trong đánh giá ngoài domain vì thuật toán của họ tận dụng khả năng hiểu của LLM đích trong khi ChatGPT-3.5-turbo mạnh hơn nhiều so với LLM với 7b tham số. Do đó, Vicuna- hoặc BlueLM-7b đôi khi có thể bối rối về các token rời rạc vô nghĩa.

4.5.3. Đánh giá In-Context Learning

Chúng tôi đánh giá khả năng ICL của SelfCP trong Bảng 4 và 5. Memory tokens có hiệu quả bằng hoặc thậm chí tốt hơn các demonstrations thực tế, điều này xác minh khả năng của SelfCP trong việc nắm bắt ngữ nghĩa cốt lõi của demonstrations trong quá trình nén. Về ARXIV, ICL ban đầu không đủ hữu ích và gây ra suy giảm đáng kể trong BlueLM, do các tài liệu tương đối dài trong ARXIV, để lại ít chỗ cho LLM đọc demonstrations.

AutoCompressor recursively nén các demonstrations được concatenated thành soft prompts từng bước một. Tuy nhiên, AutoCompressor vẫn underperform SelfCP. Chúng tôi cho rằng điều này do thông tin bị mất do nén recursive trong việc xử lý các prompt dài. Hơn nữa, các demonstrations được lọc bởi LLMlingua thường underperform 0-shot trong cả XSUM và ARXIV với hai backbone chứng minh rằng các token rời rạc thất bại trong việc hướng dẫn LLM trong few-shot settings.

Chúng tôi đánh giá SelfCP trên CoLA thông qua đánh giá closed-end, đo perplexity (PPL) của các candidate labels (acceptable/unacceptable) cho câu đã cho trong template sau: "[Sentence] Grammatically, the above sentence is {acceptable/unacceptable}". Labels với PPL gần 1 hơn sẽ được đánh giá là prediction. Đáng chú ý, ICAE luôn trả về "acceptable", dẫn đến độ chính xác đồng thuận 30.9%.

Cuối cùng, chúng tôi đánh giá SelfCP về việc lựa chọn demonstrations phù hợp từ MDB, chứa 5k demonstrations được lấy mẫu ngẫu nhiên. SelfCP tiếp tục đạt được lợi ích hiệu suất bằng cách retrieve các in-context demonstrations mạnh mẽ hơn từ MDB. Kết quả chỉ ra rằng token ảo được nén được tạo ra bởi SelfCP giỏi trong việc đo similarity giữa các tài liệu và sau đó tìm các in-context demonstrations thuận lợi hơn cho ICL.

4.6. Nghiên cứu trường hợp

Chúng tôi thể hiện một nghiên cứu trường hợp trên DUC để cung cấp so sánh trực quan giữa SelfCP dựa trên Vicuna, transaction trực tiếp, AutoCompressor và LLMLingua trong Bảng 6. Trường hợp này mô tả rằng một máy bay đã rơi vào Đại Tây Dương, trong khi các chi tiết của máy bay vượt quá độ dài và được nén. Vicuna không thể tạo ra các bản tóm tắt thỏa đáng vì một số phần nổi bật của tài liệu bị truncated. Ngược lại, SelfCP thành công khôi phục thông tin quan trọng từ các token ảo compact, chẳng hạn như loại máy bay "Piper Saratoga".

Hơn nữa, chúng tôi mô tả tình huống khác bằng cách đánh giá similarity giữa actual và virtual tokens của chúng, được minh họa trong Hình 4. Màu sắc ấm hơn biểu thị mức độ similarity lớn hơn. Tài liệu gốc mô tả Stuckbarks và sự hợp tác của nó với Magic Johnson, với thông tin về "Magic Johnson" được nén. Tuy nhiên, SelfCP khôi phục thông tin này trong phản hồi được tạo ra. Có thể hiểu được rằng các token ảo hiệu quả hấp thụ thông tin liên quan, dẫn đến similarity tương đối cao hơn với các token này.

5. Phân tích

5.1. Tỷ lệ nén

Tỷ lệ nén được lấy mẫu ngẫu nhiên từ 2 đến 16 trong quá trình huấn luyện SelfCP. Chúng tôi kết hợp 2.000 instances từ tập validation trong domain, 1.000 cho XSUM và 1.000 cho CICERO để chọn tỷ lệ nén. Cụ thể, SelfCP tiến hành nén có điều kiện nén phần cut-off sau trong khi giữ phần trước không nén. Do đó, chúng tôi có thể đo chất lượng thông tin của cùng một nội dung với các tỷ lệ nén khác nhau bằng ROUGE-1 vì nó nhạy cảm hơn với sự khác biệt ở cấp độ token.

Đối với cả BlueLM và Vicuna, hiệu suất tương đối smooth khi tỷ lệ nén thay đổi từ 4× đến 12×. Tuy nhiên, khi đến 16×, một sự sụt giảm đáng kể xảy ra trong Vicuna, với suy giảm hiệu suất tương đối lớn cũng xảy ra trong BlueLM so với các tỷ lệ trước đó. Do đó, chúng tôi đặt tỷ lệ nén thành 12 theo mặc định và áp dụng tỷ lệ này cho tất cả các thí nghiệm. Ngoài ra, trong thiết lập thí nghiệm của chúng tôi, giới hạn cửa sổ là 512, và tỷ lệ nén 512× bằng việc nén bất cứ thứ gì thành một token ảo duy nhất.

5.2. Phân tích hiệu quả

Trong SelfCP, chúng tôi kết hợp thêm 17M tham số có thể huấn luyện vào backbone 7b, chiếm khoảng 0.24% tăng thêm.

Để định lượng sự khác biệt hiệu quả mang lại bởi projection layer, chúng tôi chủ yếu tập trung vào SelfCP được xây dựng trên Vicuna vì BlueLM có kích thước tham số và kiến trúc mô hình có thể so sánh. Chúng tôi đầu tiên báo cáo GPU Hours, TFLOPs và TMACs của SelfCP và Vicuna trên một GPU NVIDIA A5000 duy nhất. Cụ thể, chúng tôi sử dụng 1000 chuỗi số ngẫu nhiên nhưng hợp lệ có độ dài 1024 làm input ids, tránh các token đặc biệt, và yêu cầu mô hình luôn tạo ra 128 tokens.

SelfCP nén 512 tokens sau thành 43 memory tokens (nén 12×), và 512 tokens trước cùng với memory tokens được fed vào Vicuna để thực hiện generation (tổng cộng 555 tokens). Để duy trì độ dài đầu vào nhất quán cho generation, 555 tokens trước được fed trực tiếp vào Vicuna. Trong Bảng 7, SelfCP đạt được gần ba lần TFLOPs và TMACs so với backbone naïve để đổi lấy double readable windows do các quá trình nén và projection bổ sung. Tuy nhiên, forward propagation để nén và projection hỗ trợ parallel computing, và nó chỉ mang lại minimal GPU Hours increments trong thực tế. Đáng chú ý, khi cho phép Vicuna đọc toàn bộ 1024 input ids mà không nén, 512 tokens thêm làm tăng cả computation và GPU Hours và overwhelm SelfCP trong quá trình generation (tham khảo Vicuna-1k trong Tab. 7).

Ngoài computation floats, trong tình huống ICL, chúng tôi minh họa các biến thiên thực tế của throughput và chi phí bộ nhớ với số lượng demonstrations tăng lên giữa SelfCP và các phương pháp nén khác trong Hình 6. AutoCompressor thể hiện inference throughputs kém do nén recursive và ICAE thực hiện sub-weakly trong throughputs do switching LoRA weights giữa compressor và generator. Cho phân bổ bộ nhớ 24 GB, Vicuna-7b chỉ thực hiện đến thiết lập 4-shot, trong khi ICAE và AutoCompressor đọc đến 32 demonstrations, nhưng SelfCP vẫn hoạt động trong thiết lập 64-shot.

SelfCP hỗ trợ caching memory tokens để cấu hình Memory Demonstration Bank trước để tái sử dụng. SelfCP có thể phản hồi các truy vấn mà không cần nén demonstration lặp lại trong trường hợp này (tham khảo SelfCP + Caching), làm cho suy luận hiệu quả hơn.

6. Kết luận và nghiên cứu tiếp theo

Bài báo này đề xuất SelfCP, sử dụng LLM đã đóng băng để nén các prompt dài thành memory tokens. Trong SelfCP, LLM đóng vai trò compressor để nén prompt và generator để phản hồi các truy vấn dựa trên memory tokens và phần còn lại của prompt không nén. SelfCP chỉ chứa 17M tham số có thể huấn luyện do backbone đóng băng và cho phép adaptation trên các backbone khác nhau. Chúng tôi tiến hành các thí nghiệm trong và ngoài domain rộng rãi, bao gồm các tình huống ICL và prompt vượt độ dài và chúng tôi phân tích hiệu quả của SelfCP. Kết quả cho thấy memory tokens được tạo ra có thể hiệu quả thay thế prompt thực tế vượt giới hạn dài 12×.

Chúng tôi tin rằng có nhiều chỗ để cải thiện cho SelfCP. Một mặt, chúng tôi sẽ scale backbone của SelfCP thành các LLMs lớn hơn và hiệu suất cao hơn trong các domain khác nhau. Mặt khác, ý định của chúng tôi bao gồm việc kết hợp nén như một trong những mục tiêu pre-training cơ bản của LLMs, kỳ vọng tăng cường khả năng nén của chúng hơn nữa.
