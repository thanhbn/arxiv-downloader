# 2307.10780.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/context-compression/2307.10780.pdf
# Kích thước tệp: 1332535 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)
Ngưỡng Học được cho Token Merging và Pruning trong Vision Transformers
Maxim Bonnaerens maxim.bonnaerens@ugent.be
IDLab-AIRO, Ghent University - imec
Joni Dambre joni.dambre@ugent.be
IDLab-AIRO, Ghent University - imec
Được đánh giá trên OpenReview: https://openreview.net/forum?id=WYKTCKpImz
Tóm tắt
Vision transformers đã chứng minh được thành công đáng kể trong nhiều nhiệm vụ thị giác máy tính trong những năm qua. Tuy nhiên, chi phí tính toán cao của chúng vẫn là rào cản lớn đối với việc triển khai thực tế. Cụ thể, độ phức tạp của các mô hình transformer là bậc hai theo số lượng token đầu vào. Do đó, các kỹ thuật giảm số lượng token đầu vào cần được xử lý đã được đề xuất. Bài báo này giới thiệu Learned Thresholds token Merging and Pruning (LTMP), một phương pháp mới tận dụng thế mạnh của cả token merging và token pruning. LTMP sử dụng các module masking ngưỡng học được để xác định động token nào cần merge và token nào cần prune. Chúng tôi chứng minh phương pháp của mình qua các thử nghiệm mở rộng trên vision transformers với nhiệm vụ phân loại ImageNet. Kết quả cho thấy LTMP đạt được độ chính xác tối ưu trên các tỷ lệ giảm thiểu trong khi chỉ cần một epoch fine-tuning duy nhất, nhanh hơn một bậc độ lớn so với các phương pháp trước đó. Mã nguồn có tại https://github.com/Mxbonn/ltmp .

hình ảnh đầu vào → hình ảnh được chia thành patch → lớp 1 → lớp 2 → lớp 3 → lớp 4 → lớp 5
lớp 6 → lớp 7 → lớp 8 → lớp 9 → lớp 10 → lớp 11 → lớp 12

Hình 1: Trực quan hóa việc merging và pruning được áp dụng lên các patch hình ảnh. Trong mỗi lớp, các token tương tự nhất được merge và bất kỳ token không quan trọng nào được prune. Các trực quan hóa cho thấy các token còn lại sau mỗi lớp trong DeiT-S.

1 Giới thiệu
Việc áp dụng transformers (Vaswani et al., 2017), ban đầu được phát triển cho xử lý ngôn ngữ tự nhiên, vào lĩnh vực thị giác máy tính với Vision Transformers (ViT) (Dosovitskiy et al., 2021) đã dẫn đến tiến bộ đáng kể trong lĩnh vực này. Nhưng bất chấp kết quả ấn tượng của vision transformers, thành công của chúng đi kèm với

--- TRANG 2 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)
một cái giá; các mô hình ViT tốn kém về mặt tính toán và đòi hỏi bộ dữ liệu lớn hơn (ví dụ ImageNet-21k thay vì ImageNet-1k (Deng et al., 2009)) và thời gian đào tạo kéo dài (Dosovitskiy et al., 2021). Để được hưởng lợi từ độ chính xác của chúng trong các nhiệm vụ và ứng dụng hạ nguồn, việc sử dụng các mô hình được đào tạo trước đã trở nên thiết yếu. Tuy nhiên, việc áp dụng chúng trên các thiết bị có tài nguyên hạn chế như nền tảng di động và nhúng vẫn còn hạn chế do chi phí tính toán cao.

Để giảm chi phí tính toán của transformers, các nghiên cứu trước đây đã tập trung vào các kỹ thuật như distillation (Wu et al., 2022), quantization (Liu et al., 2021b) và pruning. Các kỹ thuật pruning đã khám phá việc prune trọng số mô hình (Gordon et al., 2020), attention heads (Voita et al., 2019), và input tokens (Rao et al., 2021). Phương pháp cuối này rất hiệu quả, vì độ phức tạp mô hình của transformer là bậc hai theo số lượng tokens. Trong vision transformers, các tokens là những patch không chồng lấp của một hình ảnh, ví dụ một token có thể đại diện cho các patch 16×16 pixels. Token pruning đã thu hút sự quan tâm nghiên cứu vì nó phù hợp với trực giác của chúng ta rằng không phải tất cả các phần của hình ảnh đều quan trọng như nhau. Cơ chế self-attention trong transformers, do khả năng xử lý input có độ dài biến đổi và đặc tính không phụ thuộc thứ tự, cho phép giảm thiểu không có cấu trúc số lượng tokens giữa các lớp. Điều này trước đây là không đơn giản với các mạng tích chập.

Hầu hết các phương pháp token pruning tính toán điểm quan trọng cho mỗi token trong từng lớp và loại bỏ các token ít quan trọng nhất. Trong khi token pruning đã được chứng minh là kỹ thuật nén hiệu quả, việc loại bỏ tokens dẫn đến mất mát thông tin, điều này hạn chế số lượng tokens có thể được prune. Để phục hồi từ mất mát thông tin này, hầu hết các phương pháp pruning đòi hỏi đào tạo lại đáng kể để hiệu quả. Ngoài ra, một số kỹ thuật pruning gần đây đã kết hợp các kỹ thuật token combining nơi các tokens bị prune được kết hợp thành một token duy nhất tổng hợp thông tin mà nếu không sẽ bị mất (Kong et al., 2022).

Token merging (ToMe) (Bolya et al., 2023) đưa kỹ thuật combining này lên một bước tiếp theo. ToMe chỉ kết hợp các cặp tokens thành tokens mới thay vì prune chúng. Điều này có lợi thế là không loại bỏ mà tóm tắt thông tin, dẫn đến độ chính xác tốt hơn trong khi vẫn hiệu quả như nhau trong việc giảm độ phức tạp tính toán.

Trong nghiên cứu này, chúng tôi giới thiệu Learned Thresholds token Merging and Pruning (LTMP). Phương pháp của chúng tôi kết hợp lợi ích của token merging, cho phép chúng tôi kết hợp thay vì loại bỏ thông tin token, với pruning, cho phép chúng tôi loại bỏ các tokens không mang thông tin. Theo hiểu biết của chúng tôi, đây là nghiên cứu đầu tiên kết hợp mở rộng hai kỹ thuật giảm thiểu này, dẫn đến độ chính xác cải thiện so với nghiên cứu trước đây. Phương pháp của chúng tôi sử dụng các module masking ngưỡng học được, cho phép mô hình học các ngưỡng xác định tokens nào cần prune và tokens nào cần merge trong từng lớp. Điều này cho phép giảm thiểu token thích ứng, trong khi chỉ đòi hỏi hai tham số học được cho mỗi transformer block. Kết quả là, phương pháp của chúng tôi hội tụ trong một epoch duy nhất, giảm chi phí fine-tuning một bậc độ lớn so với các phương pháp pruning học được khác.

Các đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi đề xuất kết hợp token merging với token pruning, cho phép chúng tôi đạt được tỷ lệ giảm thiểu token cao với mất mát độ chính xác tối thiểu.
• Phương pháp của chúng tôi giới thiệu các module masking ngưỡng học được, chỉ đòi hỏi hai tham số học được cho mỗi transformer block, cho phép phương pháp của chúng tôi hội tụ trong một epoch duy nhất.
• Chúng tôi tối ưu hóa các ngưỡng sử dụng loss đào tạo nhận thức ngân sách mới mà chúng tôi giới thiệu mục tiêu giảm thiểu rtarget và hệ số giảm thiểu FLOPs thực tế rFLOPs. Điều này cho phép chúng tôi tạo ra các mô hình có kích thước bất kỳ và cho phép mô hình tự do phân phối các hoạt động giảm thiểu qua các lớp.

2 Nghiên cứu liên quan
2.1 Vision Transformers hiệu quả
Ban đầu, transformers (Vaswani et al., 2017) được áp dụng từ NLP sang thị giác máy tính (Dosovitskiy et al., 2021) vì độ chính xác ấn tượng của chúng. Nhưng bất chấp thành công trong nhiều nhiệm vụ thị giác, các mô hình dựa trên ViT không thể cạnh tranh với các CNN nhẹ để triển khai trên các nền tảng có tài nguyên hạn chế (Wang et al., 2022). Để tạo ra các mô hình ViT hiệu quả, một số thay đổi kiến trúc đã được đề xuất để sửa đổi các cơ chế attention tốn kém (Kitaev et al., 2019; Chen et al., 2021; Liu et al., 2021a; Li et al., 2021). Trong bài báo này, chúng tôi xem xét token pruning, nhưng các phương pháp pruning khác đã được áp dụng thành công cho transformers là, trong số những phương pháp khác, weight pruning (Gordon et al., 2020) và attention heads pruning (Voita et al., 2019).

2.2 Token Pruning
Tính linh hoạt của transformers đối với độ dài sequence và thứ tự của các inputs cho phép token pruning, điều mà trước đây không đơn giản để thực hiện trong các mô hình dựa trên tích chập. Các phương pháp token pruning có thể khác nhau theo nhiều cách, chẳng hạn như điểm số được sử dụng để xác định tầm quan trọng của mỗi token. Các phương pháp pruning cũng có thể khác nhau về cách áp dụng token reduction. Trong fixed rate pruning (Goyal et al., 2020; Rao et al., 2021; Bolya et al., 2023; Liang et al., 2022; Xu et al., 2022) một số lượng token được xác định trước bị loại bỏ mỗi lớp, trong khi trong các phương pháp adaptive (Kim et al., 2022; Yin et al., 2021; Liu et al., 2022) các tokens được prune động dựa trên input.

Các phương pháp pruning gần đây nhất không chỉ prune tokens mà còn tạo ra một token bổ sung duy nhất sau mỗi bước pruning. Token này tổng hợp thông tin của các tokens bị prune và hạn chế mất mát độ chính xác khi pruning. EViT (Liang et al., 2022), Evo-ViT (Xu et al., 2022) và SPViT (Kong et al., 2022) sử dụng trung bình có trọng số dựa trên điểm quan trọng để tạo ra token fused mới.

2.3 Token merging
Token Merging (ToMe), như được giới thiệu bởi Bolya et al. (2023), giới thiệu thuật toán token matching nhẹ để merge các tokens tương tự. Nó nhanh như pruning trong khi chính xác hơn. ToMe phân vùng tất cả tokens thành hai tập A và B có kích thước gần như bằng nhau bằng cách luân phiên và tính toán điểm tương tự cho mỗi token trong A với mọi token trong B. Điểm tương tự được định nghĩa là cosine similarity giữa các key vectors (K) được sử dụng trong lớp self-attention. Điểm tương tự cuối cùng của một token trong A là điểm tương tự cao nhất với bất kỳ token nào trong B. Dựa trên điểm này, ToMe merge k tokens tương tự nhất thông qua averaging và nối hai tập lại với nhau.

3 Learned thresholds token merging and pruning
3.1 Tổng quan
Tổng quan về framework của chúng tôi được thể hiện trong Hình 2. Cho bất kỳ vision transformer nào, phương pháp của chúng tôi thêm các component merging (LTM) và pruning (LTP) với các module masking ngưỡng học được trong mỗi transformer block giữa các component Multi-head Self-Attention (MSA) và MLP. Dựa trên attention trong MSA, điểm quan trọng cho mỗi token và điểm tương tự giữa các tokens được tính toán. Các module masking ngưỡng học được sau đó học các ngưỡng quyết định tokens nào cần prune và tokens nào cần merge.

3.2 Động lực
Hình 3: Trực quan hóa token pruning (giữa) so với token merging (phải). Trực quan hóa cho thấy các tokens còn lại sau lớp thứ 10 trong ViT-B khi pruning hoặc merging 16 tokens mỗi lớp.

Mặc dù token merging thường chính xác hơn pruning vì nó kết hợp tokens thay vì loại bỏ chúng, nhưng không phải lúc nào cũng tốt hơn khi merge tokens thay vì loại bỏ chúng. Trong một số trường hợp, có thể có lợi hơn khi prune một token không quan trọng thay vì merge các tokens tương tự nhất, vì sự tương tự giữa chúng có thể không cao lắm.

Trong phần này, chúng tôi khám phá liệu token merging và token pruning có phải là các kỹ thuật có thể được kết hợp hay không. Hình 3 trực quan hóa các tokens được giữ lại bởi pruning và bởi merging trên một hình ảnh cụ thể, chúng tôi quan sát thấy

--- TRANG 3 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

rằng các tokens được giữ lại có sự khác biệt đáng chú ý giữa

--- TRANG 4 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

LTP LTM[CLS]Multi-head Self-Attention

[CLS]Learned threshold masking module  

[CLS] [CLS]MLP

[CLS]similarity  
scoresimportance  
scores

[CLS] [CLS] [CLS]

Learned threshold masking module  

[CLS]

Hình 2: Tổng quan về phương pháp của chúng tôi. LTMP chứa một component merging và pruning, mỗi component có một module masking ngưỡng học được. Các components được thêm vào giữa các component Multi-head Self-Attention và MLP của mỗi transformer block.

cả hai phương pháp. Để định lượng mối quan hệ giữa merging và pruning, chúng tôi đã tính toán hệ số tương quan thứ hạng Kendall tau (Kendall, 1938) giữa điểm quan trọng được sử dụng trong token pruning và điểm tương tự được sử dụng trong token merging. Tuy nhiên, vì merging chỉ tính toán điểm tương tự cho các tokens trong tập A (xem Phần 2.3), chúng tôi tính toán tương quan Kendall tau giữa điểm tương tự và điểm quan trọng của chỉ các tokens trong tập A. Chúng tôi đã tính toán các tương quan trên 1000 hình ảnh sử dụng ViT-B nơi mỗi lớp pruned và merged 8 tokens theo cách top-k cố định và báo cáo kết quả trong Bảng 1. Chúng tôi thấy rằng các tương quan τ giữa cả hai điểm số đều thấp, đặc biệt trong các lớp đầu nơi việc merging và pruning quan trọng nhất được thực hiện. Do đó, chúng tôi đề xuất kết hợp token merging và token pruning.

Bảng 1: Tương quan Kendall τ giữa điểm quan trọng trong token pruning và điểm tương tự trong token merging. Các tương quan được tính toán trên 1000 hình ảnh sử dụng ViT-B nơi mỗi lớp pruned và merged 8 tokens.

lớp 1 3 5 7 9 11
τ 0.01 0.14 0.21 0.19 0.15 0.22

3.3 Learned thresholds
3.3.1 Learned thresholds pruning
Phương pháp learned thresholds của chúng tôi về mặt khái niệm tương tự như learned token pruning như được giới thiệu trong Kim et al. (2022). Trong mỗi transformer block, một điểm quan trọng được tính toán cho mỗi token xi, i∈{1,...,n}, trong đó n=hw là số lượng tokens1. Một ngưỡng θl∈R, l∈{1,...,L}, trong đó L là số lượng transformer blocks, xác định tokens nào cần giữ và tokens nào cần prune trong mỗi lớp; chỉ các tokens có điểm quan trọng trên ngưỡng mới được giữ lại.

Để prune tokens một cách thích ứng, chúng tôi giới thiệu một module masking ngưỡng mà, cho các điểm quan trọng sl∈Rn, học một ngưỡng pruning θl và xuất ra tokens nào cần giữ.

M(sl_i,θl) = {1, nếu sl_i>θl; 0, nếu ngược lại} (1)

1 Chúng tôi bỏ qua token class [CLS] để đơn giản, trong quá trình pruning và/hoặc merging chúng tôi luôn giữ token [CLS].

--- TRANG 5 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

Tuy nhiên, để làm cho θl có thể học được trong quá trình đào tạo, module masking ngưỡng cần phải có thể vi phân. Chúng tôi đạt được điều này bằng cách triển khai module masking ngưỡng như một straight-through estimator (Bengio et al., 2013), nơi chúng tôi ước tính hàm masking trong quá trình backpropagation như

M(sl_i,θl) = σ((sl_i-θl)/τ) (2)

trong đó σ(x) là hàm sigmoid và τ là siêu tham số nhiệt độ.

Trong quá trình inference, chúng tôi chỉ giữ các tokens trong block thứ l nơi M(sl_i,θl) = 1. Tuy nhiên, trong quá trình đào tạo, chúng tôi không thể đơn giản loại bỏ tokens vì điều đó không cho phép mô hình backpropagate ảnh hưởng của ngưỡng lên hiệu suất mô hình. Do đó, chúng tôi tạo ra một mask chỉ ra tokens nào được giữ và tokens nào bị prune. Mỗi module masking ngưỡng chỉ cập nhật các entries của mask cho các tokens chưa bị loại bỏ trước lớp đó, vì các tokens bị prune trong lớp trước đó phải vẫn bị prune.

Chúng tôi xây dựng pruning mask ml∈[0,1]n như sau:

ml_i = {M(sl_i,θl), nếu ml-1_i = 1; ml-1, nếu ngược lại} (3)

Triển khai learned token pruning trong Kim et al. (2022) nhân mask của nó với các tokens để tạo ra các tokens có giá trị zero. Tuy nhiên, các tokens này không vẫn zero do các bias terms trong các lớp MLP; hơn nữa việc thêm các tokens có giá trị zero thay đổi tính toán attention so với việc loại bỏ những tokens đó. Thay vào đó, phương pháp của chúng tôi thực hiện thay đổi đến nơi duy nhất mà các tokens ảnh hưởng lẫn nhau: cơ chế attention.2

Nhớ lại công thức gốc cho attention (Vaswani et al., 2017):

Attention(Q,K,V) = softmax(QKT/√dk)V (4)

Để tránh các tokens bị mask ảnh hưởng đến cơ chế attention, chúng tôi đề xuất một hàm đã sửa đổi:

Attention_with_mask(Q,K,V,m) = SV (5)

trong đó,

Sij = exp(Aij)mj / Σ(k=1 đến N) exp(Aik)mk, 1≤i,j,k≤n (6)

và,

A = QKT/√dk ∈ Rn×n (7)

Phương trình (6) tính toán một masked softmax, tương đương với softmax được tính toán với các tokens bị prune được loại bỏ. Attention_with_mask về mặt khái niệm tương tự như masked attention như được tìm thấy trong transformer decoder của các mô hình ngôn ngữ. Tuy nhiên, trong khi masking trong transformer decoders được thực hiện bằng cách đặt các tokens bị mask thành -∞, phương pháp của chúng tôi đòi hỏi ảnh hưởng của straight-through estimator mask phải truyền đến các ngưỡng trong quá trình backpropagation.

3.3.2 Learned thresholds merging
Token merging ban đầu là một phương pháp top-k, có nghĩa là nó merge dựa trên tỷ lệ cố định và không có tham số học được. Chúng tôi sửa đổi ToMe để sử dụng ngưỡng thay vì top-k bằng cách áp dụng các kỹ thuật tương tự như được giới thiệu trong Phần 3.3.1; điều này bằng cách thêm module masking ngưỡng học được của chúng tôi, trong đó các điểm tương tự trên các ngưỡng này được chọn để merge, và bằng cách thay đổi hàm attention thành Phương trình (5).

2 Về mặt kỹ thuật, các tokens cũng ảnh hưởng lẫn nhau trong quá trình layer normalization, tuy nhiên vì pruning được thực hiện trên các mô hình đã được đào tạo trước, chúng tôi chỉ đơn giản sử dụng các thống kê toàn cục từ pretraining trong quá trình normalization.

--- TRANG 6 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

3.3.3 Learned thresholds merging and pruning
Với các ngưỡng có thể học được, việc kết hợp merging và pruning trở nên đơn giản, vì chúng tôi có thể chỉ cần thêm một module masking ngưỡng học được để học các ngưỡng cho điểm quan trọng và một module khác học các ngưỡng cho điểm tương tự.

3.4 Chiến lược đào tạo
3.4.1 Mục tiêu đào tạo
Để hiệu quả giảm số lượng tokens trong các transformer blocks, cần thiết phải bao gồm một hạng mục loss regularization trong quá trình đào tạo. Không có loss này, mô hình không có động lực để prune bất kỳ tokens nào và các ngưỡng pruning sẽ đơn giản được đặt thành 0 vì mô hình chính xác nhất sử dụng tất cả inputs. Chúng tôi đề xuất một budget-aware training loss giới thiệu mục tiêu giảm thiểu rtarget cho FLOPs của vision transformer.

Hãy ký hiệu ϕmodule(n,d) như một hàm tính toán FLOPs của một module dựa trên số lượng tokens n và dimension embedding d. Hệ số giảm thiểu FLOPs thực tế rFLOPs của một ViT sau đó có thể được tính toán như:

rFLOPs = ϕPE(n,d)/ϕViT(n,d) + Σ(l=1 đến L) ϕBLK(n,d)/ϕViT(n,d) * (ϕMSA(m̄l-1*n,d)/ϕBLK(n,d) + ϕMLP(m̄l*n,d)/ϕBLK(n,d)) + ϕHEAD(m̄l*n,d)/ϕViT(n,d) (8)

trong đó m̄l = (1/n)Σ(i=1 đến n)ml_i là phần trăm input tokens được giữ lại sau hoạt động masking ngưỡng thứ l và m̄0 = 1. PE, BLK và HEAD ký hiệu các component khác nhau của vision transformer: module patch embedding, các transformer blocks và classification head.

Vì phần lớn FLOPs trong vision transformer xảy ra trong các transformer blocks (≈99% trong ViT-S), chúng tôi bỏ qua FLOPs trong patch embedding và classification head: ϕPE(n,d)/ϕViT(n,d) = ϕHEAD(n,d)/ϕViT(n,d) ≈ 0.

Điều đó có nghĩa là chúng tôi có thể đơn giản hóa
ϕBLK(n,d)/ϕViT(n,d) ≈ 1/L, (9)
trong đó L là số lượng transformer blocks.

FLOPs của một transformer block và hai component của nó, MSA và MLP có thể được tính toán như:
ϕMSA(n,d) = 4nd² + 2n²d (10)
ϕMLP(n,d) = 8nd² (11)
ϕBLK(n,d) = ϕMSA(n,d) + ϕMLP(n,d) = 12nd² + 2n²d (12)

Thay thế Phương trình (10) đến (12) vào Phương trình (8) cho:
rFLOPs ≈ Σ(l=1 đến L) (1/L) * ((2m̄l-1*nd² + (m̄l-1*n)²d + 4m̄l*nd²)/(6nd² + n²d)) (13)

Cho hệ số giảm thiểu FLOPs này rFLOPs như một hàm của các threshold masks, chúng tôi định nghĩa regularization loss của chúng tôi như sai số bình phương giữa mục tiêu giảm thiểu và hệ số giảm thiểu FLOPs thực tế:

Lreg = (rtarget - rFLOPs)² (14)

Regularization loss này sau đó được kết hợp với classification loss, mà chúng tôi áp dụng standard cross entropy loss.

L = LCE + λLreg (15)

Mục tiêu đào tạo tổng thể là học các ngưỡng tối ưu hóa mô hình trong khi giảm độ phức tạp mô hình đến một mục tiêu giảm thiểu nhất định. Sự kết hợp của learned thresholds và budget-aware loss của chúng tôi cho phép mô hình phân phối tối ưu merging và pruning qua các lớp.

--- TRANG 7 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

3.4.2 Lịch trình đào tạo
LTMP chỉ thêm hai tham số học được cho mỗi transformer block (một cho pruning và một cho merging). Như thường thấy trong pruning, nó được áp dụng cho các mô hình đã được đào tạo trước. Do đó chúng tôi chỉ cập nhật các ngưỡng trong quá trình đào tạo và giữ tất cả các tham số có thể đào tạo khác cố định, cho phép LTMP hội tụ trong một epoch duy nhất.

[Biểu đồ: Ba đồ thị cho thấy trade-offs Accuracy/FLOPs cho các biến thể LTMP của DeiT-Tiny, -Small và -Base. FLOPs được vẽ theo thang logarithmic. Trade-offs accuracy/FLOPs của ToMe được hiển thị để so sánh.]

Hình 4: Trade-offs Accuracy/FLOPs cho các biến thể LTMP của DeiT-Tiny, -Small và -Base. FLOPs được vẽ theo thang logarithmic. Trade-offs accuracy/FLOPs của ToMe được hiển thị để so sánh.

4 Thí nghiệm
Trong phần này, chúng tôi chứng minh phương pháp của chúng tôi thông qua các thí nghiệm mở rộng trên nhiệm vụ phân loại ImageNet-1k (Deng et al., 2009) sử dụng các biến thể ViT khác nhau (Steiner et al., 2022; Touvron et al., 2021). Tất cả các mô hình đã được đào tạo trước được lấy từ thư viện timm PyTorch (Wightman, 2022).

Tất cả các thí nghiệm của chúng tôi được đào tạo trong một epoch duy nhất, sử dụng SGD không có momentum và batch size là 128. Các cài đặt đào tạo còn lại như augmentations được đặt thành các giá trị mặc định của timm. Các siêu tham số được giới thiệu trong LTMP được đặt thành τ = 0.1 và λ = 10. Vì điểm quan trọng và điểm tương tự có giá trị trong các khoảng khác nhau, chúng tôi sử dụng learning rates riêng biệt cho các ngưỡng trong các modules pruning và merging: 5·10⁻⁶ cho các ngưỡng pruning và 5·10⁻³ cho các ngưỡng merging. Để chọn các siêu tham số này, chúng tôi đã sử dụng một tập validation riêng biệt chứa 2% của tập đào tạo ImageNet gốc. Các siêu tham số cuối cùng được xác định dựa trên hiệu suất của chúng trên DeiT-Small với rtarget được đặt thành 0.65. Để thể hiện tính mạnh mẽ của các siêu tham số này, chúng tôi không thay đổi chúng giữa các biến thể mô hình và mục tiêu giảm thiểu (tức là ViT-Tiny với rtarget được đặt thành 0.45, được fine-tune với các siêu tham số tương tự).

4.1 Kết quả chính
Vì hầu hết các phương pháp pruning khác đòi hỏi fine-tuning mở rộng, baseline vision transformer được sử dụng phổ biến nhất trong các nghiên cứu khác là data-efficient vision transformer DeiT (Touvron et al., 2021), vì lý do này, chúng tôi cũng báo cáo về các mô hình DeiT.

Trong Hình 4, chúng tôi thể hiện phương pháp của chúng tôi được áp dụng cho DeiT-Tiny, -Small và -Base. Cho mỗi mô hình, chúng tôi thay đổi rtarget sao cho chúng tôi có được một tập các mô hình giảm thiểu, mỗi mô hình có kích thước khác nhau. Bảng 2 liệt kê các kết quả chi tiết cho các mô hình DeiT. Các mô hình được sử dụng trong một trong các bảng khác trong bài báo được tô đậm màu xám. Chúng tôi bao gồm rtarget, sao cho kết quả của chúng tôi có thể được tái tạo. Trong Phụ lục A, chúng tôi bổ sung bao gồm kết quả trên các mô hình ViT chuẩn. Tất cả các số accuracy được báo cáo với 3 chữ số có nghĩa; một phân tích ngắn gọn về khoảng lỗi trên các số accuracy có thể được tìm thấy trong Phụ lục B.

4.2 So sánh với nghiên cứu khác
Trong Bảng 3, chúng tôi so sánh top-1 accuracy được báo cáo, FLOPs³, và fine-tune epochs của các phương pháp pruning khác với nghiên cứu của chúng tôi. Các phương pháp khác mà chúng tôi so sánh với là SPViT (Kong et al., 2022), DynamicVit

³ Chúng tôi tuân theo quy ước báo cáo FLOPs như multiply-adds.

--- TRANG 8 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

Bảng 2: Kết quả chi tiết cho các mô hình DeiT.

[Bảng chi tiết với các cột Model, rtarget, FLOPs, Accuracy cho DeiT-T, DeiT-S, và DeiT-B với nhiều giá trị rtarget khác nhau]

(Rao et al., 2021), EViT (Liang et al., 2022), EvoViT (Xu et al., 2022) và ToMe (Bolya et al., 2023). Hầu hết các nghiên cứu báo cáo về một mô hình đã được prune với khoảng 3.0G FLOPs, mà đối với DeiT-S tương ứng với rtarget ≈ 0.65 trong phương pháp của chúng tôi. Chỉ SPViT báo cáo về một kích thước mô hình khác, đó là lý do tại sao nó được so sánh riêng biệt. Kết quả cho thấy LTMP đạt được accuracy tối ưu với một phần nhỏ của các fine-tuning epochs được yêu cầu bởi các phương pháp học được khác. Accuracy của LTMP bằng hoặc vượt quá accuracy của các phương pháp token pruning khác đòi hỏi tối thiểu 30 fine-tune epochs. Chỉ EViT có thể đạt được accuracy cao hơn LTMP, nhưng chỉ khi tăng drastically fine-tune epochs lên 100, cao hơn hai bậc độ lớn so với phương pháp của chúng tôi. Vì ToMe không yêu cầu fine-tuned checkpoints để so sánh, chúng tôi có thể so sánh LTMP với ToMe một cách mở rộng hơn trên một phạm vi rộng các kích thước mô hình. Hình 4 hiển thị trade-offs accuracy/FLOPs cho LTMP và ToMe. Các thí nghiệm của chúng tôi cho thấy LTMP luôn luôn vượt trội hơn ToMe qua các kích thước mô hình.

Bảng 3: So sánh với các phương pháp token reduction khác trên DeiT-S. Phương pháp của chúng tôi đạt được accuracy tối ưu với ít fine-tune epochs hơn đáng kể so với các phương pháp học được khác.

[Bảng so sánh với các cột Method, FLOPs, Accuracy, fine-tune epochs]

4.3 Sự phối hợp giữa merging và pruning
Để phân tích sự phối hợp giữa token merging và token pruning, chúng tôi đã kiểm tra phân phối của các merged và pruned tokens qua từng lớp của vision transformer. Kết quả, được thể hiện trong Hình 5 cho DeiT-S, tiết lộ rằng token merging là hoạt động giảm thiểu chủ đạo trong các lớp đầu của transformer, trong khi token pruning phổ biến hơn trong các lớp cuối. Điều này phù hợp với phát hiện của Bolya et al. (2023) rằng merging hiệu quả hơn pruning, vì nó có thể tóm tắt thông tin. Tuy nhiên, một khi tất cả các tokens tương tự được merge, việc prune các tokens ít thông tin nhất có lợi hơn thay vì merge các tokens không tương tự như vậy. Nói cách khác, các lớp đầu chủ yếu được sử dụng để kết hợp các patches tương tự và một khi điều này hầu hết được thực hiện, token pruning loại bỏ các phần không quan trọng của input.

--- TRANG 9 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

Hình 5: Phân phối số lượng tokens (k) được loại bỏ bởi các phần merging và pruning trong mỗi lớp của LTMP DeiT-S rFLOPs ≈ 0.5.

4.4 Lựa chọn thiết kế
4.4.1 Điểm quan trọng
Một thành phần quan trọng của các phương pháp pruning là điểm quan trọng được sử dụng để xác định tokens nào cần loại bỏ. Hai lựa chọn phổ biến nhất cho điểm quan trọng là:

class attention score si = Σ(j=1 đến h) Sj0i (16)

trong đó S ∈ Rh×n×n là phần mở rộng multi-headed của attention softmax matrix (xem Phương trình (6)) trong đó các giá trị tại index 0 tương ứng với token [CLS], và

mean column attention score si = (1/(h·n)) Σ(j=1 đến h) Σ(k=1 đến n) Sjki (17)

có thể được hiểu như lượng normalized mà tất cả tokens xk attend to token xi (Kim et al., 2022).

Kết quả trong Bảng 4 được thu thập từ các mô hình DeiT-S LTP và cho thấy mean column attention score hoạt động hơi tốt hơn class attention score, nhưng không đáng kể. Cho phần còn lại của nghiên cứu này, chúng tôi sử dụng mean column attention score (Phương trình (17)) như điểm quan trọng.

4.4.2 Thứ tự merging và pruning

Bảng 4: Ablation của các lựa chọn thiết kế liên quan đến điểm quan trọng pruning và thứ tự áp dụng merging và pruning. Tất cả thí nghiệm được thực hiện trên các biến thể DeiT-S.

[Bảng với các cột FLOPs và Accuracy cho các phương pháp khác nhau]

Như đã thấy trước đó trong Bảng 1, tương quan giữa điểm quan trọng pruning và điểm tương tự merging thấp nhưng không zero. Điều này có nghĩa là đối với các giá trị mục tiêu giảm thiểu r nhỏ, cùng một token có thể đạt các ngưỡng cho cả merging và pruning. Trong Bảng 4, chúng tôi so sánh pruning theo sau bởi merging (LTPM) với merging theo sau bởi pruning (LTMP). Kết quả xác nhận rằng thứ tự không có ảnh hưởng đáng chú ý khi tỷ lệ giảm thiểu nhỏ, nhưng một khi cần loại bỏ nhiều tokens hơn thì LTMP vượt trội hơn LTPM. Điều này không có gì đáng ngạc nhiên vì merging đã được chứng minh là chính xác hơn pruning (Bolya et al., 2023) và trong token merging và pruning, nhiều tokens được merged hơn là pruned (xem Hình 5).

--- TRANG 10 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

4.5 Ablation
Phương pháp của chúng tôi có hai thành phần quan trọng: learned thresholds và sự kết hợp của merging và pruning. Trong Bảng 5, chúng tôi ablate các thành phần chính của phương pháp chúng tôi trên DeiT-S cho hai kích thước mô hình khác nhau. Chúng tôi so sánh top-k pruning và merging, nơi k tokens được prune trong mỗi transformer block, với các biến thể learned thresholds. Đối với merging, phương pháp top-k bằng với những gì được sử dụng trong ToMe (Bolya et al., 2023). Ngoài ra, chúng tôi so sánh merging và pruning riêng lẻ với phương pháp kết hợp.

Hình 6: Phân phối số lượng tokens (k) được loại bỏ trong các biến thể LTP và LTM của DeiT-S rFLOPs ≈ 0.5.

Kết quả cho thấy learned thresholds cải thiện accuracy của pruning đáng kể, trong khi đối với merging các cải thiện chỉ là marginally. Sự khác biệt về accuracy này giữa learned thresholds và top-k có thể được giải thích bằng cách kiểm tra phân phối của các tokens bị loại bỏ như được thể hiện trong Hình 6. Trong box plot này, chúng tôi so sánh phân phối số lượng tokens bị loại bỏ với LTM và LTP với uniform top-k pruning (tức là đường gạch ngang trong hình). Như có thể thấy trong hình, phân phối của các tokens bị loại bỏ với LTM gần với phân phối uniform top-k, dẫn đến nhiều tokens tương tự được merge, trong khi đối với LTP phân phối khác biệt đáng chú ý.

Chúng tôi cũng quan sát thấy rằng việc kết hợp naive merging và pruning bằng cách áp dụng cả hai kỹ thuật với tỷ lệ cố định bằng nhau tệ hơn chỉ token merging.

Ablation này cho thấy cả hai thành phần đều thiết yếu trong LTMP. Kết hợp merging và pruning vượt trội hơn các kỹ thuật riêng lẻ nhưng chỉ khi sử dụng learned thresholds để cân bằng merging và pruning.

Bảng 5: Ablation của hai thành phần chính của LTMP trên DeiT-S: learned thresholds và kết hợp merging với pruning.

[Bảng chi tiết với các method settings, FLOPs và Accuracy]

--- TRANG 11 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

4.6 Tốc độ inference
Xuyên suốt bài báo này, chúng tôi đã báo cáo FLOPs như metric độ phức tạp. Trong khi FLOPs thường được coi là proxy kém cho latency, nó cũng đã được chứng minh rằng cải thiện latency trên một loại phần cứng thường không chuyển đổi sang cải thiện tương tự trên phần cứng khác, đặc biệt trong các thiết bị di động và nhúng (Bonnaerens et al., 2022). Vì cải thiện độ phức tạp của chúng tôi đến từ việc giảm input tokens và cả các modules masking và triển khai pruning và merging của chúng tôi đều được song song hóa, chúng tôi tin rằng FLOPs là metric tốt nhất có sẵn để báo cáo cải thiện độ phức tạp.

Tuy nhiên, để chứng minh phương pháp của chúng tôi, chúng tôi đã benchmark nó trên một thiết bị di động sử dụng hàm optimize_for_mobile của PyTorch và speed_benchmark Android binary. Bảng 6 thể hiện latency của baseline DeiT-S và biến thể giảm thiểu LTMP của chúng tôi (với rFLOPs ≈ 0.5). Benchmark được thực hiện trên Google Pixel 7 và trung bình trên 200 lần chạy (với 50 lần chạy warm-up trước đó). Kết quả cho thấy cải thiện latency, đạt được giảm thiểu 49.52%, gần như giống hệt với cải thiện FLOPs lý thuyết, có giảm thiểu 50.12%.

LTMP cũng nhanh hơn ToMe trong khi không chỉ merging mà còn pruning. Điều này có thể đến từ toán tử argsort được sử dụng trong các phương pháp top-k như ToMe và không được hỗ trợ tốt trong nhiều frameworks (Prillo & Eisenschlos, 2020). Thật không may, mặc dù Evit, Evo-Vit và DynamicVit có triển khai PyTorch mã nguồn mở, chúng sử dụng các operations không được hỗ trợ bởi TorchScript mà cần thiết cho công cụ mobile speed_benchmark.

Bảng 6: Latency benchmark trên Google Pixel 7.

[Bảng so sánh Method, FLOPs, Latency, Accuracy]

4.7 Hạn chế
Phương pháp learned thresholds của chúng tôi đòi hỏi batch size bằng 1 trong quá trình inference vì mỗi hình ảnh được giảm thiểu khác nhau. Đây không phải là hạn chế đối với hầu hết các ứng dụng có tài nguyên hạn chế vì chúng thường hoạt động inference với batch size bằng 1. Nếu mong muốn, phương pháp của chúng tôi có thể được mở rộng để chứa batch sizes lớn hơn bằng cách kết hợp masking đến một kích thước giảm thiểu chung hoặc bằng cách chuyển đổi ngưỡng thành số lượng tokens trung bình được loại bỏ mỗi operation và layer, và áp dụng các giá trị này trong một top-k adaptation. Phương pháp trước sẽ dẫn đến độ phức tạp tính toán cao hơn trong khi phương pháp sau sẽ dẫn đến accuracy thấp hơn.

[Hình ảnh trực quan hóa từ input patchified qua các layers 1-12]

Hình 7: Trực quan hóa thêm về merging và pruning được áp dụng lên image patches. Trong mỗi lớp, các tokens tương tự nhất được merge và bất kỳ tokens không quan trọng nào được prune. Các trực quan hóa cho thấy tokens còn lại sau mỗi lớp trong DeiT-S.

--- TRANG 12 ---
Xuất bản trên Transactions on Machine Learning Research (08/2023)

4.8 Trực quan hóa
Trong Hình 7, chúng tôi minh họa việc merging và pruning các tokens khi chúng được xử lý qua vision transformer. Có thể quan sát thấy cách các phần tương tự của hình ảnh được merge và cách các phần không quan trọng của hình ảnh được prune.

5 Kết luận
Trong nghiên cứu này, chúng tôi đã giới thiệu Learned Thresholds token Merging and Pruning (LTMP) cho vision transformers. LTMP làm cho việc giảm chi phí tính toán của vision transformer đến bất kỳ giá trị mục tiêu giảm thiểu nào trở nên khả thi với mất mát accuracy tối thiểu. LTMP thích ứng giảm số lượng input tokens được xử lý bằng cách merge các tokens tương tự và prune những tokens không quan trọng. Triển khai của chúng tôi sử dụng learned thresholds cho phép các tỷ lệ merging và pruning khác nhau giữa các hình ảnh khác nhau và cho phép mô hình học trade-off tối ưu giữa merging và pruning qua các lớp. Vì LTMP chỉ giới thiệu hai tham số học được cho mỗi transformer block, phương pháp của chúng tôi có thể hội tụ trong một epoch duy nhất, nhanh hơn một bậc độ lớn so với các phương pháp học được khác.

Lời cảm ơn
Nghiên cứu này nhận được tài trợ thông qua Research Foundation Flanders (FWO-Vlaanderen) dưới grant 1S47820N.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ với các citation và thông tin xuất bản]

--- TRANG 13-16 ---
[Tiếp tục với phần Phụ lục A, B và các bảng chi tiết bổ sung]
