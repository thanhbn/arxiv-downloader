# 2310.17157.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/context-compression/2310.17157.pdf
# Kích thước tệp: 1516997 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Deja Vu: Thưa thớt theo ngữ cảnh cho các LLM hiệu quả tại thời điểm suy luận
Zichang Liu1Jue Wang2Tri Dao3Tianyi Zhou4Binhang Yuan5Zhao Song6Anshumali Shrivastava1
Ce Zhang5Yuandong Tian7Christopher Ré3Beidi Chen8 7

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) với hàng trăm tỷ tham số đã tạo ra một làn sóng mới của các ứng dụng AI thú vị. Tuy nhiên, chúng rất tốn kém về mặt tính toán tại thời điểm suy luận. Thưa thớt là một phương pháp tự nhiên để giảm chi phí này, nhưng các phương pháp hiện có hoặc yêu cầu việc đào tạo lại tốn kém, phải từ bỏ khả năng học trong ngữ cảnh của LLM, hoặc không tạo ra tăng tốc thời gian thực trên phần cứng hiện đại. Chúng tôi đưa ra giả thuyết rằng thưa thớt theo ngữ cảnh, là các tập hợp nhỏ phụ thuộc đầu vào của các đầu attention và tham số MLP tạo ra đầu ra gần như giống với mô hình dày đặc cho một đầu vào nhất định, có thể giải quyết những vấn đề này. Chúng tôi cho thấy rằng thưa thớt theo ngữ cảnh tồn tại, có thể được dự đoán chính xác, và chúng ta có thể khai thác nó để tăng tốc suy luận LLM trong thời gian thực mà không làm tổn hại chất lượng hoặc khả năng học trong ngữ cảnh của LLM. Dựa trên những hiểu biết này, chúng tôi đề xuất DEJAVU, một hệ thống sử dụng thuật toán chi phí thấp để dự đoán thưa thớt theo ngữ cảnh một cách tức thì với các đầu vào cho mỗi lớp, cùng với việc triển khai không đồng bộ và nhận biết phần cứng giúp tăng tốc suy luận LLM. Chúng tôi xác thực rằng DEJAVU có thể giảm độ trễ suy luận của OPT-175B hơn 2× so với FasterTransformer tiên tiến nhất, và hơn 6× so với triển khai Hugging Face được sử dụng rộng rãi, mà không làm tổn hại chất lượng mô hình. Mã nguồn có sẵn tại https://github.com/FMInference/DejaVu.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM), như GPT-3, PaLM, và OPT đã chứng minh rằng một số lượng khổng lồ các tham số giải phóng hiệu suất ấn tượng và khả năng học trong ngữ cảnh mới nổi - chúng có thể thực hiện một tác vụ bằng cách điều kiện hóa trên các ví dụ đầu vào-đầu ra, mà không cập nhật tham số của chúng (Bommasani et al., 2021; Liang et al., 2022; Brown et al., 2020; Min et al., 2022; Chan et al., 2022). Tuy nhiên, chúng rất đắt đỏ tại thời điểm suy luận, đặc biệt đối với các ứng dụng nhạy cảm với độ trễ (Pope et al., 2022). Một mô hình suy luận lý tưởng nên sử dụng ít tính toán và bộ nhớ hơn trong khi duy trì hiệu suất và khả năng đặc biệt của các LLM được đào tạo trước. Phương pháp đơn giản và tự nhiên nhất là thưa thớt hóa hoặc cắt tỉa, có lịch sử lâu dài trước kỷ nguyên LLM (LeCun et al., 1989). Thật không may, việc tăng tốc các LLM thưa thớt tại thời điểm suy luận trong thời gian thực trong khi duy trì chất lượng và khả năng học trong ngữ cảnh vẫn là một vấn đề thách thức.

Mặc dù thưa thớt và cắt tỉa đã được nghiên cứu kỹ lưỡng, chúng không được áp dụng rộng rãi trên LLM do sự đánh đổi kém về chất lượng và hiệu quả trên phần cứng hiện đại như GPU. Thứ nhất, việc đào tạo lại hoặc cắt tỉa lặp đi lặp lại các mô hình ở quy mô hàng trăm tỷ tham số là không khả thi. Do đó, các phương pháp trong cắt tỉa lặp và giả thuyết vé số may mắn (Lee et al., 2018; Frankle & Carbin, 2018) chỉ có thể được áp dụng cho các mô hình quy mô nhỏ hơn. Thứ hai, việc tìm thưa thớt bảo tồn khả năng học trong ngữ cảnh của LLM là thách thức. Nhiều nghiên cứu đã cho thấy hiệu quả của cắt tỉa phụ thuộc tác vụ (Michel et al., 2019; Bansal et al., 2022), nhưng việc duy trì các mô hình khác nhau cho mỗi tác vụ xung đột với mục tiêu độc lập tác vụ của LLM. Cuối cùng, việc đạt được tăng tốc thời gian thực với thưa thớt không có cấu trúc rất khó do khó khăn nổi tiếng với phần cứng hiện đại (Hooker, 2021). Ví dụ, phát triển gần đây trong cắt tỉa zero-shot như SparseGPT (Frantar & Alistarh, 2023) tìm thấy 60% thưa thớt không có cấu trúc nhưng chưa dẫn đến bất kỳ tăng tốc thời gian thực nào.

Một thưa thớt lý tưởng cho LLM nên (i) không yêu cầu đào tạo lại mô hình, (ii) bảo tồn chất lượng và khả năng học trong ngữ cảnh, và (iii) dẫn đến tăng tốc trong thời gian thực trên phần cứng hiện đại. Để đạt được những yêu cầu khắt khe như vậy, chúng tôi vượt ra ngoài thưa thớt tĩnh trong các nghiên cứu trước (ví dụ, cắt tỉa trọng số có cấu trúc/không có cấu trúc). Thay vào đó, chúng tôi hình dung thưa thớt theo ngữ cảnh, là các tập hợp nhỏ phụ thuộc đầu vào của các đầu attention và tham số MLP dẫn đến đầu ra (gần như) giống với mô hình đầy đủ cho một đầu vào. Được truyền cảm hứng từ các kết nối giữa LLM, Mô hình Markov Ẩn (Xie et al., 2022; Baum & Petrie, 1966), và thuật toán Viterbi cổ điển (Viterbi, 1967), chúng tôi đưa ra giả thuyết rằng đối với các LLM được đào tạo trước,

thưa thớt theo ngữ cảnh tồn tại với bất kỳ đầu vào nào.

Giả thuyết này, nếu đúng, sẽ cho phép chúng ta cắt bỏ các đầu attention và tham số MLP cụ thể (thưa thớt có cấu trúc) một cách tức thì cho thời gian suy luận, mà không sửa đổi các mô hình được đào tạo trước. Tuy nhiên, có ba thách thức.

Sự tồn tại: Việc xác minh liệu thưa thớt theo ngữ cảnh như vậy có tồn tại hay không là không tầm thường, và việc xác minh ngây thơ có thể cực kỳ tốn kém.

Dự đoán: Ngay cả khi thưa thớt theo ngữ cảnh tồn tại, việc dự đoán thưa thớt cho một đầu vào nhất định trước đó là thách thức.

Hiệu quả: Ngay cả khi thưa thớt có thể được dự đoán, việc đạt được tăng tốc thời gian thực từ đầu đến cuối có thể khó khăn. Lấy OPT-175B làm ví dụ, độ trễ của một khối MLP chỉ là 0.2 ms trên máy 8×A100 80GB. Nếu không có dự đoán nhanh và triển khai tối ưu, chi phí phụ có thể dễ dàng tăng độ trễ LLM thay vì giảm nó.

Trong nghiên cứu này, chúng tôi giải quyết những thách thức này như sau:

Sự tồn tại: May mắn thay, chúng tôi xác minh sự tồn tại của thưa thớt theo ngữ cảnh bằng một cách tiếp cận đơn giản đáng ngạc nhiên. Để đạt được đầu ra về cơ bản giống nhau, thưa thớt theo ngữ cảnh trung bình là 85% thưa thớt có cấu trúc và do đó có thể dẫn đến giảm 7× tham số cho mỗi đầu vào cụ thể trong khi duy trì độ chính xác (Hình 1(a)). Trong quá trình khám phá thưa thớt theo ngữ cảnh, chúng tôi đưa ra những quan sát thực nghiệm quan trọng và xây dựng hiểu biết lý thuyết về các thành phần chính trong LLM giúp giải quyết thách thức dự đoán và hiệu quả.

Dự đoán: Chúng tôi phát hiện rằng thưa thớt theo ngữ cảnh phụ thuộc không chỉ vào các token đầu vào riêng lẻ (tức là thưa thớt động không theo ngữ cảnh) mà còn vào tương tác của chúng (thưa thớt động theo ngữ cảnh). Hình 1(b) cho thấy rằng với thông tin động thuần túy, dự đoán thưa thớt không chính xác. Chỉ với các embedding token có đủ thông tin ngữ cảnh, chúng ta mới có thể dự đoán thưa thớt chính xác. Một phát hiện khác là thưa thớt động theo ngữ cảnh cho mỗi lớp có thể được dự đoán dựa trên "độ tương tự" giữa các tham số lớp (đầu/MLP) và đầu ra từ lớp trước, mang theo hỗn hợp ngữ cảnh trực tiếp của các embedding token.

Hiệu quả: Vì tại thời điểm suy luận, các tham số mô hình là tĩnh, được truyền cảm hứng từ tài liệu tìm kiếm láng giềng gần nhất cổ điển (NNS) và các ứng dụng của nó trong học sâu hiệu quả, có thể công thức hóa dự đoán dựa trên độ tương tự ở trên như một vấn đề NNS (Indyk & Motwani, 1998b; Zhang et al., 2018; Chen et al., 2020a). Tuy nhiên, như đã đề cập, chi phí phụ có thể khó vượt qua vì chúng ta cần thực hiện dự đoán tức thì trước mỗi lớp. May mắn thay, chúng tôi khai thác một hiện tượng của LLM trong đó các embedding token thay đổi chậm qua các lớp do các kết nối dư (nổi tiếng trong thị giác máy tính (He et al., 2016)). Vì các đầu vào cho một vài lớp liên tiếp rất giống nhau, chúng ta có thể thiết kế một bộ dự đoán nhìn trước không đồng bộ (Hình 2).

Dựa trên những phát hiện của chúng tôi, chúng tôi trình bày một hệ thống, DEJAVU, khai thác thưa thớt theo ngữ cảnh và thực hiện các LLM hiệu quả cho các ứng dụng nhạy cảm với độ trễ.

• Trong Phần 4.1 và Phần 4.2, chúng tôi trình bày một thuật toán dựa trên học tập chi phí thấp để dự đoán thưa thớt một cách tức thì. Với đầu vào cho một lớp cụ thể, nó dự đoán một tập con liên quan của attention (đầu) hoặc tham số MLP trong lớp tiếp theo và chỉ tải chúng cho tính toán.

• Trong Phần 4.3, chúng tôi đề xuất một bộ dự đoán không đồng bộ (tương tự như bộ dự đoán nhánh cổ điển (Smith, 1998)) để tránh chi phí phụ tuần tự. Một đảm bảo lý thuyết biện minh rằng thiết kế liên lớp đủ để dự đoán thưa thớt chính xác.

Sau khi tích hợp triển khai nhận biết phần cứng của phép nhân ma trận thưa thớt (Phần 4.4), DEJAVU (viết chủ yếu bằng Python) có thể giảm độ trễ của các LLM mã nguồn mở như OPT-175B hơn 2× từ đầu đến cuối mà không làm giảm chất lượng so với thư viện tiên tiến nhất FasterTransformer từ Nvidia (viết hoàn toàn bằng C++/CUDA), và hơn 2× so với triển khai Hugging Face được sử dụng rộng rãi ở kích thước batch nhỏ. Hơn nữa, chúng tôi cho thấy một số nghiên cứu loại bỏ trên các thành phần khác nhau của DEJAVU và khả năng tương thích của nó với các kỹ thuật lượng tử hóa.

2 Nghiên cứu liên quan và Công thức vấn đề

Chúng tôi trước tiên thảo luận ngắn gọn về tài liệu phong phú về suy luận hiệu quả. Sau đó, chúng tôi giới thiệu phân tích độ trễ trong bối cảnh của chúng tôi. Cuối cùng, chúng tôi cung cấp một công thức vấn đề chính thức.

2.1 Lượng tử hóa, Cắt tỉa, Chưng cất cho Suy luận

Các nới lỏng khác nhau đã được nghiên cứu trong nhiều thập kỷ để suy luận mô hình trong học máy. Có ba kỹ thuật chính: lượng tử hóa (Han et al., 2015; Jacob et al., 2018; Nagel et al., 2019; Zhao et al., 2019), cắt tỉa hoặc thưa thớt (Molchanov et al., 2016; Liu et al., 2018; Hoefler et al., 2021), và chưng cất (Hinton et al., 2015; Tang et al., 2019; Touvron et al., 2021). Chúng là các lĩnh vực trực giao và thường xuất sắc trong các bối cảnh khác nhau. Gần đây, có nghiên cứu tích cực cố gắng áp dụng một hoặc kết hợp các kỹ thuật như vậy trong suy luận LLM (Yao et al., 2022; Park et al., 2022; Dettmers et al., 2022; Frantar et al., 2022; Frantar & Alistarh, 2023; Bansal et al., 2022; Xiao et al., 2022). Thêm thảo luận được trình bày trong Phụ lục A.

2.2 Phân tích độ trễ suy luận LLM

Quy trình sinh của LLM bao gồm hai giai đoạn: (i) giai đoạn prompt nhận một chuỗi đầu vào để tạo ra các khóa và giá trị (KV cache) cho mỗi khối transformer của LLM, tương tự như lượt truyền tiến của việc huấn luyện LLM; và (ii) giai đoạn sinh token sử dụng và cập nhật KV cache để sinh token từng bước, trong đó việc sinh token hiện tại phụ thuộc vào các token đã sinh trước đó.

Bài báo này nghiên cứu bối cảnh mà giai đoạn sinh token dễ dàng chiếm ưu thế trong thời gian suy luận từ đầu đến cuối. Như được hiển thị trong Bảng 1, việc sinh một chuỗi có độ dài 128 mất nhiều thời gian hơn việc xử lý một chuỗi có độ dài 128 như prompt do độ trễ I/O của việc tải tham số mô hình. Ngoài ra, Bảng 2 cho thấy rằng attention và MLP đều là điểm nghẽn trong LLM, ví dụ, trong các mô hình 175B, việc tải tham số MLP mất khoảng 2/3 tổng I/O và các đầu attention chiếm 1/3 còn lại. Hơn nữa, trong chế độ tensor-parallel, có hai giao tiếp giữa các GPU, một sau khối attention, và một sau khối MLP. Như được hiển thị trong Bảng 3, giao tiếp giữa các GPU mất khoảng 15% độ trễ sinh token. Bài báo này tập trung vào việc làm cho attention và MLP hiệu quả hơn. Chi phí giao tiếp ngụ ý rằng giới hạn trên của tăng tốc như vậy là khoảng 6× khi bỏ qua tất cả các khối transformer.

Bảng 1. Phân tích lý thuyết cho prompting so với sinh token (tensor model parallelism trên 8 GPU A100-80G).

TFLOPs I/O Compute Latency (ms) I/O Latency (ms)
Prompting 128 44.6 330 GB 17.87 20.6
Token Generation 128 44.6 41 TB 17.87 2600

Bảng 2. Phân tích lý thuyết cho khối Attention so với khối MLP trong một lớp transformer khi sinh một token (tensor model parallelism trên 8 GPU A100-80G).

GFLOPs I/O (GB) Compute Latency (ms) I/O Latency (ms)
Attention Block 1.21 1.12 0.00048 0.07
MLP Block 2.41 2.25 0.00096 0.14

Bảng 3. Phân tích độ trễ sinh 1 token dưới thiết lập kích thước batch 1 và độ dài prompt 128 trên 8 A100-80GB.

All Reduce MLP Block Attention Block (ms) Others
6 ms 19ms 13ms 2ms

2.3 Công thức vấn đề

Mục tiêu là giảm độ trễ sinh của LLM bằng cách khai thác thưa thớt theo ngữ cảnh. Trong phần sau, chúng tôi định nghĩa chính thức các khối attention và MLP được thưa thớt hóa.

MLP được thưa thớt hóa: Có hai lớp tuyến tính trong một khối MLP, W1, W2 ∈ Rd×4d. Ký hiệu y ∈ R1×d là đầu vào cho khối MLP trong bước sinh hiện tại. Để mỗi cột (trọng số của neuron thứ i) của các lớp tuyến tính là W1_i, W2_i ∈ Rd×1. Với thưa thớt theo ngữ cảnh, chỉ một tập hợp nhỏ của chúng được yêu cầu cho tính toán. Để SM ⊆ [4d] biểu thị tập hợp các neuron như vậy cho đầu vào y. Tính toán MLP được thưa thớt hóa là

MLP_SM(y) = σ(yW1_SM)(W2_SM)⊤, (1)

trong đó σ là hàm kích hoạt, ví dụ, ReLU, GeLU. Lưu ý rằng vì tính toán trong lớp tuyến tính đầu tiên dẫn đến các kích hoạt thưa thớt, lớp tuyến tính thứ hai cũng được thưa thớt hóa.

Attention được thưa thớt hóa: Để X ∈ Rn×d biểu thị các embedding của tất cả token (ví dụ, prompt và các token đã sinh trước). Để y ∈ R1×d là đầu vào cho Multi-Head-Attention (MHA) trong bước sinh hiện tại. Giả sử có h đầu. Đối với mỗi i ∈ [h], chúng tôi sử dụng WK_i, WQ_i, WV_i ∈ Rd×dh để biểu thị các phép chiếu khóa, truy vấn, giá trị cho đầu thứ i, và WO_i ∈ Rdh×d cho các phép chiếu đầu ra. Với thưa thớt theo ngữ cảnh, chúng tôi ký hiệu SA là một tập hợp nhỏ các đầu attention dẫn đến đầu ra gần như giống với attention đầy đủ cho đầu vào y. Theo hệ thống ký hiệu trong (Alman & Song, 2023), tính toán MHA được thưa thớt hóa có thể được viết chính thức như

MHA_SA(y) = ∑(i∈SA) Hi(y)|{z}(1×dh) WO_i|{z}(dh×d),

trong đó Hi(y): Rd → Rdh và Di(y) ∈ R có thể được viết như

Hi(y) := Di(y)^(-1) exp(yWQ_i(WK_i)⊤X⊤)XWV_i, (2)
Di(y) := exp(yWQ_i(WK_i)⊤X⊤)1n.

Đối với cả MLP và Attention, với một ngân sách tính toán, mục tiêu là tìm SM và SA để tối thiểu hóa lỗi giữa xấp xỉ thưa thớt và tính toán đầy đủ.

3 Các LLM được đào tạo trước có tính thưa thớt theo ngữ cảnh

Trong phần này, chúng tôi trình bày một số quan sát chính và hiểu biết lý thuyết về thưa thớt trong LLM, dựa trên đó thiết kế DEJAVU được xây dựng. Chúng tôi đầu tiên kiểm tra giả thuyết thưa thớt theo ngữ cảnh và xác minh rằng thưa thớt theo ngữ cảnh tồn tại trong các LLM được đào tạo trước trong Phần 3.1. Sau đó, chúng tôi xây dựng hiểu biết về lý do tại sao thưa thớt theo ngữ cảnh xảy ra tự nhiên ngay cả khi LLM được huấn luyện dày đặc trong Phần 3.2. Cuối cùng, chúng tôi trình bày một quan sát về các kết nối dư và giải thích mối quan hệ của chúng với thưa thớt theo ngữ cảnh một cách phân tích trong Phần 3.3.

3.1 Giả thuyết thưa thớt theo ngữ cảnh

Được truyền cảm hứng từ tài liệu cắt tỉa trước đây (Molchanov et al., 2016), chúng tôi tìm thấy một phương pháp đơn giản đáng ngạc nhiên là đủ để nghiên cứu và xác minh giả thuyết của chúng tôi. Trong phần này, chúng tôi mô tả quy trình kiểm tra, chi tiết quan sát và hiểu biết của nghiên cứu này.

Xác minh: Kiểm tra của chúng tôi được thực hiện trên các mô hình OPT-175B, 66B và 30B và các bộ dữ liệu downstream khác nhau như OpenBookQA (Mihaylov et al., 2018) và Wiki-Text (Merity et al., 2016). Chúng tôi tìm thưa thớt theo ngữ cảnh cho mỗi ví dụ đầu vào với hai lượt truyền tiến của mô hình. Trong lượt truyền đầu tiên, chúng tôi ghi lại một tập con tham số, cụ thể là các đầu attention và neuron MLP nào tạo ra chuẩn đầu ra lớn cho đầu vào. Trong lượt truyền thứ hai, mỗi ví dụ đầu vào chỉ sử dụng tập con tham số đã ghi lại cho tính toán. Đáng ngạc nhiên, hai lượt truyền này dẫn đến dự đoán hoặc hiệu suất tương tự trên tất cả các tác vụ học trong ngữ cảnh và mô hình hóa ngôn ngữ.

Quan sát: Hình 3 cho thấy rằng trung bình, chúng ta có thể áp đặt lên đến 80% thưa thớt trên các đầu attention và 95% thưa thớt trên các neuron MLP. Như đã đề cập trong Phần 2, mô hình OPT-175B có 2× tham số MLP so với các khối attention. Do đó tổng thưa thớt ở đây là khoảng 85%. Vì đây đều là thưa thớt có cấu trúc (đầu và neuron), việc dự đoán chúng một cách chính xác có thể dẫn đến tăng tốc 7×.

Hiểu biết: Việc chúng ta có thể tìm thấy thưa thớt theo ngữ cảnh trong các khối MLP tại thời điểm suy luận là trực quan vì các hàm kích hoạt của chúng, ví dụ, ReLU hoặc GeLU (Kurtz et al., 2020). Các quan sát tương tự đã được thực hiện bởi (Li et al., 2022). Tuy nhiên, thật ngạc nhiên khi chúng ta có thể tìm thấy thưa thớt theo ngữ cảnh trong các lớp attention. Lưu ý rằng, việc tìm thưa thớt theo ngữ cảnh trong attention không giống với cắt tỉa đầu. Chúng tôi kiểm tra chéo rằng các ví dụ khác nhau có thưa thớt theo ngữ cảnh khác nhau. Mặc dù 80% tham số không được bao gồm trong các đường dẫn cho một ví dụ nhất định, chúng có thể được sử dụng bởi các ví dụ khác. Tiếp theo, chúng tôi sẽ cố gắng hiểu lý do tại sao thưa thớt theo ngữ cảnh tồn tại trong các khối attention.

3.2 Phân cụm token trong các lớp Attention

Trong phần trước, chúng tôi đã xác minh rằng tồn tại thưa thớt theo ngữ cảnh cho một đầu vào nhất định trong LLM. Trong phần này, chúng tôi cố gắng hiểu lý do cho hiện tượng như vậy, đặc biệt trong các lớp attention. Chúng tôi đầu tiên cho thấy một quan sát sâu sắc về attention. Sau đó chúng tôi trình bày một giả thuyết rằng self-attention về mặt khái niệm là các thuật toán phân cụm. Cuối cùng chúng tôi cho thấy bằng chứng phân tích để hỗ trợ giả thuyết này.

Quan sát: Hình 4 cho thấy bản đồ attention của ba đầu khác nhau từ cùng một lớp cho một ví dụ đầu vào. Token tiếp theo nó nên dự đoán là "Truck". Màu tối hơn đại diện cho điểm attention cao hơn. Chúng tôi quan sát rằng đầu giữa là một đầu trộn token tương đối đồng nhất trong khi đầu trên và dưới là các đầu attention "heavy hitter" (với attention cao đối với "like" và "shipping"). Không ngạc nhiên, việc chỉ chọn các đầu heavy hitter mà không chọn các đầu đồng nhất không ảnh hưởng đến dự đoán, vì các đầu đồng nhất không mô hình hóa hoặc mã hóa các tương tác token quan trọng. Trong phần tiếp theo, chúng tôi cũng sẽ giải thích chi tiết cách tiêu chí để chọn các đầu attention đồng nhất và các đầu có chuẩn đầu ra nhỏ có tương quan cao.

Giả thuyết: Chúng tôi đưa ra giả thuyết rằng đầu attention đang thực hiện phân cụm mean-shift (Derpanis, 2005).

Nhớ lại ký hiệu được định nghĩa trong Phần 2.3. Đối với đầu thứ i tại lớp hiện tại, X = [x1,...,xn]⊤ ∈ Rn×d là các embedding token trong các bước thời gian trước. XWK_i và XWV_i là phép chiếu của embedding. Đối với một embedding đầu vào y, đầu ra ỹi = Hi(y), trong đó Hi(y) được định nghĩa trong Eq. 2. Đối với mỗi i ∈ [h], nếu chúng ta để Ki(xj,y) := exp(yWQ_i(WK_i)⊤xj) đo độ tương tự giữa xj và y, và định nghĩa mi(y) := ∑jKi(xj,y)xj / ∑jKi(xj,y), thì chúng ta có ỹi = mi(y)WV_i. Hơn nữa, nếu chúng ta đặt WV_i = I và xem xét kết nối dư theo sau bởi layer norm, thì trong lớp tiếp theo, embedding ŷi của token hiện tại trở thành ŷi = Normalize(y + ỹi) = Normalize(y + mi(y)), có điểm cố định y = γmi(y) cho bất kỳ vô hướng γ nào. Phép lặp này mang một sự tương đồng với phân cụm mean-shift, đơn giản thực hiện phép lặp y ← mi(y) cho đến hội tụ. Điều này có một điểm cố định rõ ràng y = mi(y). Do đó, đầu self-attention có thể được coi như một bước mean-shift để đẩy các embedding đầu vào của các token khác nhau lại với nhau, nếu chúng đã là láng giềng trong không gian chiếu được chỉ định bởi WQ_i(WK_i)⊤. Các đầu khác nhau học các không gian chiếu khác nhau để thực hiện phân cụm. Những động lực này giải thích lý do chính xác tại sao các embedding token có xu hướng phân cụm sau khi đi qua nhiều lớp hơn, dẫn đến điểm attention cao giữa các thành viên cụm, và điểm thấp cho những thành viên không thuộc cụm. Hơn nữa, các mẫu cụm khác nhau ở các đầu khác nhau (Thêm chi tiết trong Phụ lục K).

Phân tích trên không chỉ cung cấp hiểu biết về lý do tại sao thưa thớt theo ngữ cảnh tồn tại tự nhiên trong các LLM được đào tạo trước, mà còn truyền cảm hứng cho thiết kế của chúng tôi về dự đoán thưa thớt dựa trên "độ tương tự" cho DEJAVU trong Phần 4.

3.3 Embedding thay đổi chậm qua các lớp

Chúng tôi đầu tiên trình bày quan sát rằng các embedding thay đổi chậm qua các lớp liên tiếp. Sau đó chúng tôi cung cấp một phân tích chi tiết về hiện tượng này. Cuối cùng, chúng tôi cho thấy mối liên hệ chặt chẽ của nó với thưa thớt theo ngữ cảnh. Chi tiết trong Phần B.

Embedding tương tự cao trong các lớp liên tiếp: Trong Hình 5(a), chúng tôi cho thấy rằng đối với cùng một đầu vào nhất định, độ tương tự cosine giữa các embedding hoặc kích hoạt trong hai lớp liên tiếp là cực kỳ cao trên 7 kích thước khác nhau của các mô hình OPT. Cụ thể, chúng tôi thu thập các kích hoạt từ mỗi lớp trong khi thực hiện suy luận mô hình OPT trên bộ validation C4 (Raffel et al., 2019). Lấy OPT-175B làm ví dụ, bắt đầu từ lớp thứ hai, độ tương tự giữa bất kỳ hai lớp liên tiếp nào là khoảng 0.99, cho thấy rằng khi một đầu vào được truyền qua mô hình, hướng của embedding thay đổi chậm. Thú vị thay, sự thay đổi mạnh mẽ nhất xảy ra ở lớp đầu tiên. Hơn nữa, chúng tôi tăng khoảng cách và khảo sát độ tương tự giữa embedding tại lớp l và tại lớp l+n được hiển thị trong Hình 5(b). Khi chúng tôi tăng khoảng cách, độ tương tự giảm như mong đợi trong khi sự khác biệt trong độ tương tự cosine giữa các lựa chọn khác nhau của n nhỏ hơn ở lớp nông hơn. Chúng tôi vẽ độ tương tự trung bình, và độ lệch chuẩn được chỉ ra bởi vùng bóng mờ. Các biểu đồ tương tự trên nhiều mô hình được trình bày trong Phụ lục B.

Kết nối với residual: Chúng tôi xác minh rằng độ tương tự cao trong các embedding trong suy luận LLM là do kết nối residual. Chúng tôi đầu tiên phân tích đồ thị tính toán bên trong mỗi lớp transformer để hiểu nguyên nhân đằng sau hiện tượng này. Có hai kết nối residual bên trong một lớp transformer, một xung quanh khối attention, và một xung quanh khối MLP. Kết nối residual có thể được viết như X + F(X), trong đó F là Multi-Head Attention hoặc hai lớp MLP. Trong Hình 5(c) và Hình 5(d), thực sự chúng ta có thể thấy rằng ∥X∥ lớn hơn đáng kể so với ∥F(X)∥, xác nhận rằng các embedding đang thay đổi chậm vì chuẩn residual lớn.

Kết nối với Thưa thớt theo ngữ cảnh: Chúng tôi đi sâu hơn một bước để cố gắng hiểu lý do đằng sau chuẩn residual lớn bằng mô hình hóa toán học. Chúng tôi phát hiện rằng một lý do có thể cho ∥F(X)∥ nhỏ là do thưa thớt cao. Đối với khối MLP, thưa thớt cao có thể góp phần vào chuẩn nhỏ của F(X) vì một phần lớn đầu ra có chuẩn nhỏ. Lý luận tương tự áp dụng cho khối Attention, và do đó một số lượng lớn các đầu attention tạo ra đầu ra chuẩn nhỏ.

Ràng buộc hai phía residual: Bên cạnh lý luận thực nghiệm, chúng tôi định nghĩa chính thức tính toán của LLM một cách toán học. Dưới mô hình tính toán của chúng tôi, chúng ta có thể cho thấy một tính chất co lại được quan sát bởi các thí nghiệm thực tế của chúng tôi. Chứng minh trong Phụ lục G, H, I.

Bổ đề 3.1 (Không chính thức). Để 0 < ε1 < ε2 < 1 là giới hạn dưới và trên của hệ số co lại. Để x là y là đầu ra. Chúng ta có kết nối residual y = x + F(x). Đối với khối MLP F(x), chúng ta có ε1 ≤ ∥y-x∥2 ≤ ε2. Đối với khối attention F(x), chúng ta có ε1 ≤ ∥y-x∥2 ≤ ε2.

4 DEJAVU

Trong phần này, chúng tôi trình bày framework của chúng tôi cho tìm kiếm thưa thớt theo ngữ cảnh tại thời điểm suy luận cho LLM. Chúng tôi giới thiệu bộ dự đoán thưa thớt cho MLP trong Phần 4.1 và cho các đầu attention trong Phần 4.2. Quy trình làm việc của DEJAVU được hiển thị trong Hình 2. Phần 4.3 thảo luận về việc khai thác quan sát của chúng tôi trên LLM để tránh chi phí phụ dự đoán thưa thớt với đảm bảo lý thuyết. Trong Phần 4.4, chúng tôi trình bày triển khai tối ưu hóa của chúng tôi cho phép giảm độ trễ từ đầu đến cuối. Thêm chi tiết được trình bày trong Phần D.

4.1 Dự đoán thưa thớt theo ngữ cảnh trong các khối MLP

Như đã giải thích trong Phần 2, các khối MLP là một trong những điểm nghẽn chính cho việc sinh LLM (2/3 FLOP và IO). Trong phần này, chúng tôi thảo luận về cách chúng tôi đạt được tăng tốc thời gian thực với thưa thớt theo ngữ cảnh trong các khối MLP.

Thách thức: Hình 3(b) cho thấy rằng đối với một token nhất định, thưa thớt theo ngữ cảnh 95% là có thể. Thưa thớt theo ngữ cảnh trong khối MLP có thể được xác định sau khi tính toán kích hoạt. Tuy nhiên, điều này chỉ chứng minh sự tồn tại của thưa thớt theo ngữ cảnh nhưng không mang lại lợi ích nào về mặt hiệu quả. Cần có một dự đoán nhanh và chính xác để khai thác thưa thớt theo ngữ cảnh cho hiệu quả từ đầu đến cuối. Cách ngây thơ là chọn một tập con neuron ngẫu nhiên. Không ngạc nhiên, việc chọn ngẫu nhiên không thể xác định thưa thớt theo ngữ cảnh chính xác, dẫn đến suy giảm mô hình mạnh mẽ.

Một vấn đề tìm kiếm láng giềng gần: Nhớ lại rằng chúng tôi xác minh sự tồn tại của thưa thớt theo ngữ cảnh bằng cách ghi lại các neuron nào tạo ra chuẩn đáng kể. Về cơ bản, với đầu vào, mục tiêu là tìm kiếm các neuron có tích vô hướng cao với đầu vào, vì hàm kích hoạt "lọc" kích hoạt thấp. Do đó, chúng tôi công thức hóa dự đoán thưa thớt theo ngữ cảnh của một lớp MLP như vấn đề tìm kiếm láng giềng gần cổ điển dưới metric tích vô hướng.

Định nghĩa 4.1 (Approximate MaxIP trong MLP). Để c ∈ (0,1) và τ ∈ (0,1) biểu thị hai tham số. Với một bộ dữ liệu n-vector W1 ⊂ Sd-1 trên một cầu đơn vị, mục tiêu của (c,τ)-MaxIP là xây dựng một cấu trúc dữ liệu sao cho, với một truy vấn y ∈ Sd-1 mà max w∈W1⟨y,w⟩ ≥ τ, nó truy xuất một vector z từ W1 thỏa mãn ⟨y,z⟩ ≥ c·max w∈W1⟨y,w⟩.

Nhận xét 4.2. W1 (lớp tuyến tính đầu tiên) và y (embedding đầu vào) trong các khối MLP có thể được xem như bộ dữ liệu và truy vấn trong Định nghĩa 4.1 tương ứng.

Thiết kế: Các phương pháp và triển khai tìm kiếm láng giềng gần tiêu chuẩn tiên tiến nhất làm chậm tính toán. Lấy OPT-175B với d là 12288 làm ví dụ. HNSW (Malkov & Yashunin, 2018) yêu cầu hơn 10ms, và FAISS (Johnson et al., 2019) yêu cầu hơn 4ms, trong khi tính toán MLP chỉ là 0.2ms. Kích thước cao và các phức tạp của triển khai cấu trúc dữ liệu trên GPU làm cho thời gian tìm kiếm dài hơn so với tính toán MLP. Do đó, chúng tôi chọn một bộ phân loại mạng neural như phương pháp tìm kiếm láng giềng gần để khai thác phép nhân ma trận nhanh trên GPU. Đối với mỗi khối MLP, chúng tôi huấn luyện một mạng kết nối đầy đủ hai lớp nhỏ để dự đoán thưa thớt theo ngữ cảnh. Việc thu thập dữ liệu huấn luyện rất đơn giản vì chúng ta biết thưa thớt theo ngữ cảnh bằng cách sử dụng tính toán dày đặc. Thuật toán huấn luyện được tóm tắt trong Thuật toán 1. Tính toán thưa thớt hóa trong W1 có hai bước: (1) Với y, bộ dự đoán thưa thớt SPM dự đoán một tập SM của các neuron quan trọng trong trọng số W1. (2) Tính toán MLP thưa thớt hóa được định nghĩa trong Eq. 1. Lưu ý ở đây thưa thớt trong MLP có cấu trúc cao.

Thuật toán 1 Huấn luyện bộ dự đoán thưa thớt
Đầu vào: Một khối LLM được đào tạo trước với tập tham số M, tập embedding token tại khối M={xi}i∈[N], ngưỡng t
Bộ dự đoán thưa thớt SP
P+ ← ∅, P- ← ∅
for i=1→N do
    P+ ← P+ ∪ {(xi,mr)|mr∈M,mr(xi)≥t}
    P- ← P- ∪ {(xi,mr)|mr∈M,mr(xi)<t}
end for
SP ← TRAIN(P+,P-,L) ▷ L là một hàm mất mát

4.2 Dự đoán thưa thớt theo ngữ cảnh trong các khối Attention

Các khối attention chiếm khoảng 30% IO trong việc sinh. Trong phần này, chúng tôi mô tả cách DEJAVU khai thác thưa thớt theo ngữ cảnh để tăng tốc các khối Attention.

Thách thức: Như đã thảo luận trong Phần 3.1, chỉ một vài đầu thực hiện tính toán quan trọng cho một token đầu vào nhất định. Tương tự như các khối MLP, cần có việc chọn nhanh các đầu attention mà không cần tính toán đầy đủ để giảm độ trễ từ đầu đến cuối. Hơn nữa, một thách thức cụ thể của dự đoán thưa thớt trong các khối attention là sự phụ thuộc của attention vào các token trước. Một mặt, không rõ liệu cache khóa và giá trị của các token trong quá khứ có cần thiết cho dự đoán thưa thớt hay không. Mặt khác, không rõ cách xử lý cache KV bị thiếu của các token trong quá khứ cho tính toán token hiện tại tại đầu được chọn.

Một vấn đề tìm kiếm láng giềng gần: Dự đoán đầu cũng có thể được công thức hóa như một vấn đề tìm kiếm láng giềng gần dựa trên hiểu biết của chúng tôi trong Phần 3.2. Vì mỗi đầu đang thực hiện phân cụm mean-shift, sau vài lớp đầu tiên, embedding token hiện tại một mình đủ cho dự đoán nhờ vào bản chất trộn token của transformer. Do đó, dự đoán có thể dựa trên độ tương tự giữa y và tham số đầu.

Phương pháp: Chúng tôi thiết kế bộ dự đoán thưa thớt attention có cùng kiến trúc với bộ dự đoán thưa thớt MLP. Mỗi đầu được coi như một lớp và quy trình huấn luyện tương tự được sử dụng (Thuật toán 1). Sau đó, tương tự như cách thực hiện dự đoán MLP, bộ dự đoán thưa thớt attention SPA chọn một tập SA của các đầu Hi (xem Eq. 2). Để giải quyết vấn đề cache KV bị thiếu cho token trong quá khứ, chúng tôi khai thác thực tế rằng độ trễ sinh bị ràng buộc bởi I/O trong khi tính toán về cơ bản là "miễn phí". Cụ thể, đối với đầu attention được dự đoán của đầu vào y, chúng tôi tính toán các khóa và giá trị tương ứng và lưu trữ chúng trong cache KV. Nhưng chúng tôi cũng lưu một bản sao của y cho tất cả các đầu không được chọn khác. Sau đó trong quá trình sinh token tương lai, nếu có cache KV bị thiếu trong các đầu được chọn, chúng ta có thể tải các embedding token đã lưu và tính toán các khóa và giá trị cùng nhau. Điều này yêu cầu gần như tối thiểu truy cập bộ nhớ bổ sung (chi phí chính là tải các ma trận trọng số).

4.3 Giảm chi phí phụ với thực thi không đồng bộ

Chi phí phụ dự đoán thưa thớt có thể dễ dàng tăng độ trễ từ đầu đến cuối thay vì giảm nó bất chấp việc giảm FLOP. Do đó, chúng tôi giới thiệu một phương pháp dự đoán thưa thớt nhìn trước, được truyền cảm hứng từ những quan sát của chúng tôi trong Phần 3.3.

Thách thức: Ký hiệu yl ∈ Rd là đầu vào cho lớp transformer l. Chúng ta có thể viết tính toán tại lớp l như ẽyl ← MHAl(yl), b̃yl ← MLPl(ẽyl). Với các bộ dự đoán SPl_A và SPl_M, tính toán tại lớp transformer l có thể được viết lại như

Sl_A ← SPl_A(yl), ẽyl ← MHAl_Sl_A(yl),
Sl_M ← SPl_M(ẽyl), b̃yl ← MLPl_Sl_M(ẽyl)

trong đó tập Sl_A là thưa thớt theo ngữ cảnh cho khối Attention, và tập Sl_M là thưa thớt theo ngữ cảnh cho khối MLP tại lớp thứ l. Lưu ý rằng tính toán tại các khối Attention và MLP phải chờ quyết định của bộ dự đoán thưa thớt. Chi phí phụ này có thể vượt quá việc tiết kiệm từ các khối Attention và MLP về mặt độ trễ.

Phương pháp: Trong Phần 3.3, chúng tôi trình bày hiện tượng embedding phát triển chậm, cung cấp cơ hội để nới lỏng tính toán tuần tự thành tính toán song song. Cùng với quan sát về cường độ tính toán thấp trong quá trình sinh, chúng tôi song song hóa dự đoán thưa thớt với tính toán của mỗi khối (Xem Hình 2). Tính toán có thể được viết như sau:

ẽyl ← MHAl_Sl_A(yl), b̃yl ← MLPl_Sl_M(ẽyl),
Sl+1_A ← SPl_A(yl), Sl+1_M ← SPl_M(yl),

Chúng tôi nhận xét rằng Sl+1_A và Sl+1_M có thể được tính toán song song với ẽyl hoặc b̃yl, trong khi 4 bước trước đó là tuần tự.

Đảm bảo lý thuyết: Bộ dự đoán thưa thớt có thể đưa ra quyết định liên lớp tiến xa hơn vì kết nối residual. Chúng tôi trình bày một phát biểu bổ đề không chính thức về dự đoán liên lớp. Như đã biết rằng MaxIP tương đương với tìm kiếm láng giềng gần nhất ℓ2. Để thuận tiện, chúng tôi sử dụng MaxIP ở đây. Chúng tôi bao gồm thêm thảo luận và chứng minh trong Phần J.

Bổ đề 4.3 (Không chính thức). Để ε ∈ (0,1). Để yl là đầu vào tại lớp thứ l. Để yl-1 là đầu vào tại lớp (l-1). Giả sử rằng ∥yl-yl-1∥2 ≤ ε. Đối với bất kỳ tham số c,τ sao cho ε < O(cτ). Thì chúng ta có thể cho thấy rằng, việc giải MaxIP (c,τ) đủ để giải MaxIP (0.99c,τ).

4.4 Triển khai hiệu quả phần cứng

Chúng tôi mô tả cách DEJAVU được triển khai một cách hiệu quả phần cứng để thực hiện tăng tốc lý thuyết của thưa thớt theo ngữ cảnh. Việc tính đến các đặc tính phần cứng dẫn đến tăng tốc hơn 2× so với mô hình dày đặc được tối ưu hóa, và nhanh hơn 4× so với triển khai thưa thớt tiêu chuẩn. Chúng tôi nêu bật một số đặc tính phần cứng của GPU:

• Việc sinh batch nhỏ bị nghẽn bởi I/O bộ nhớ GPU (NVIDIA, 2022; Ivanov et al., 2021; Dao et al., 2022). Điều này là do cường độ số học thấp. Đối với mỗi phần tử được tải từ bộ nhớ GPU, chỉ một số lượng nhỏ các phép toán dấu phẩy động được thực hiện.

• GPU là các thiết bị định hướng khối: việc tải một byte bộ nhớ duy nhất mất cùng thời gian như việc tải một khối bộ nhớ xung quanh cùng địa chỉ đó (Harris, 2013). Kích thước khối thường là 128 byte đối với GPU NVIDIA (Cook, 2012).

Những đặc tính này đặt ra một số thách thức trong việc triển khai thưa thớt theo ngữ cảnh. Tuy nhiên, chúng có thể được giải quyết bằng các kỹ thuật cổ điển trong lập trình GPU.

Fusion kernel: Một triển khai tiêu chuẩn của phép nhân ma trận-vector thưa thớt (ví dụ, trong PyTorch) mà riêng biệt lập chỉ mục một tập con của ma trận W1_SM trước khi nhân với đầu vào y sẽ tạo ra 3× lượng I/O bộ nhớ. Do đó, để tránh chi phí phụ như vậy, chúng tôi kết hợp bước lập chỉ mục và bước nhân. Cụ thể, chúng tôi tải một tập con của W1_SM vào bộ nhớ, cùng với y, thực hiện phép nhân, sau đó ghi xuống kết quả. Triển khai kết hợp này (trong Triton (Tillet et al., 2019)) mang lại tăng tốc lên đến 4× so với triển khai PyTorch tiêu chuẩn (Phụ lục E).

Coalescing bộ nhớ: Trong triển khai dày đặc, các ma trận trọng số của hai lớp tuyến tính trong MLP được lưu trữ như (W1)⊤ và W2 sao cho không cần phép chuyển vị bổ sung. Chúng được lưu trữ theo quy ước trong định dạng row-major. Trong triển khai thưa thớt, nó cho phép chúng ta tải (W1_SM)⊤ một cách tối ưu (chiều thứ hai liền kề trong bộ nhớ). Tuy nhiên, đối với các trường hợp mà chúng ta cần tải (W2_SM), định dạng này làm chậm đáng kể việc tải bộ nhớ, vì các chỉ số trong SM trỏ đến bộ nhớ không liền kề. Chúng tôi đơn giản lưu trữ những ma trận này trong định dạng column-major (tức là, lưu trữ (W2)⊤ trong định dạng row-major), sau đó sử dụng cùng kernel kết hợp ở trên. Tương tự, trong các khối attention, chúng tôi lưu trữ phép chiếu đầu ra attention WO định dạng column-major.

Hai kỹ thuật này (fusion kernel và memory coalescing) làm cho DEJAVU hiệu quả phần cứng, mang lại tăng tốc lên đến 2× từ đầu đến cuối so với FasterTransformer tiên tiến nhất (Phần 5.1).

5 Đánh giá thực nghiệm

Trong Phần 5.1, chúng tôi trình bày kết quả từ đầu đến cuối cho thấy DEJAVU đạt được giảm hơn 2× độ trễ sinh token so với FasterTransformer tiên tiến nhất và hơn 6× so với Hugging Face mà không mất độ chính xác. Trong Phần 5.2, chúng tôi thực hiện một danh sách các nghiên cứu loại bỏ như đánh giá độc lập về thưa thớt theo ngữ cảnh tại thời điểm suy luận của khối MLP và khối Attention (Chi tiết được trình bày trong Phần C). Cuối cùng, chúng tôi trình bày các kết quả bổ sung để chứng minh khả năng tương lai của việc thưa thớt hóa toàn bộ LLM thông qua bỏ qua lớp trong Phần C.3.

5.1 Kết quả từ đầu đến cuối

Thiết lập thí nghiệm: Chúng tôi so sánh độ chính xác của DEJAVU-OPT với mô hình OPT gốc trên hai bộ dữ liệu mô hình hóa ngôn ngữ Wiki-Text (Merity et al., 2016) và C4 (Raffel et al., 2019) và bảy tác vụ downstream few-shot: CB (de Marneffe et al., 2019), COPA (Gordon et al., 2012), Lambada (Radford et al., 2019), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), RTE (Giampiccolo et al., 2007), Winogrande (ai2, 2019). Chúng tôi sử dụng lm-eval-harness (Gao et al., 2021) cho các tác vụ zero-shot và five-shot. Chúng tôi thu thập dữ liệu huấn luyện cho bộ dự đoán thưa thớt bằng cách sử dụng 500 điểm dữ liệu ngẫu nhiên từ bộ dữ liệu huấn luyện C4. Các thí nghiệm của chúng tôi được tiến hành trên các máy chủ GPU NVIDIA A100 80GB.

Không giảm độ chính xác cho đến 75% thưa thớt: Trong Hình 6, chúng tôi trình bày xu hướng độ chính xác của DEJAVU-OPT-175B. Trong thiết lập zero-shot, độ chính xác trung bình qua các tác vụ không giảm cho đến 75% thưa thớt. Xu hướng tương tự có thể được quan sát cho thiết lập five-shot, xác minh khả năng học trong ngữ cảnh của mô hình. Kết quả này cực kỳ khích lệ cho quan sát của chúng tôi trong Hình 1(a), nơi chúng ta có thể áp đặt 85% thưa thớt khi được phép tính toán đầy đủ.

Giảm độ trễ hơn 2×: Hình 7 trình bày tăng tốc độ trễ cho việc sinh token với OPT-175B ở kích thước batch 1, nơi DEJAVU đạt hiệu suất tốt nhất. Ở khoảng 75% thưa thớt, DEJAVU tăng tốc sinh bằng 1.8-2× so với FasterTransformers (FT) tiên tiến nhất và bằng 4.8-6× so với triển khai Hugging Face (HF).

5.2 Kết quả loại bỏ

Thưa thớt theo ngữ cảnh cho batch lớn hơn: Mặc dù bài báo này tập trung vào các thiết lập nhạy cảm với độ trễ, chúng tôi chứng minh rằng DEJAVU tổng quát hóa cho các batch lớn hơn. chúng tôi trình bày thưa thớt theo ngữ cảnh Union (phần của neuron/đầu không được sử dụng bởi bất kỳ đầu vào nào trong batch) của các kích thước batch khác nhau cho các khối MLP và Attention, tương ứng, trong Hình 8 và 11. Phép toán union rất cần thiết để thực hiện GEMM thưa thớt nhanh. Đáng ngạc nhiên, số lượng neuron MLP và đầu Attention mà DEJAVU kích hoạt không tăng tuyến tính với kích thước batch. Điều này gợi ý một phân phối luật lũy thừa hơn là phân phối đồng nhất của việc truy cập tham số từ tất cả các ví dụ đầu vào. Điều này cung cấp cơ hội có thể mở rộng Dejavu sang thiết lập thông lượng cao. Ví dụ, chúng ta có thể đầu tiên tiền xử lý các đầu vào và batch các đầu vào tương tự để tận hưởng mức độ thưa thớt theo ngữ cảnh union cao hơn.

Thưa thớt theo ngữ cảnh trên các khối MLP: Chúng tôi nghiên cứu thưa thớt hóa theo ngữ cảnh của khối MLP trong OPT-175B. Chúng tôi để khối Attention như tính toán dày đặc. Bảng 4 cho thấy hiệu suất mô hình ở 85% thưa thớt. Bộ dự đoán thưa thớt MLP không gây mất độ chính xác trên cả các tác vụ zero-shot và mô hình hóa ngôn ngữ. Trong việc huấn luyện bộ dự đoán thưa thớt MLP, chúng tôi quan sát rằng bộ dự đoán thưa thớt đạt độ chính xác validation cao. Lớp nông dường như dễ mô hình hóa hơn vì bộ dự đoán có độ chính xác validation hơn 99% trong các lớp nông và giảm xuống khoảng 93% trong các lớp cuối.

Thưa thớt theo ngữ cảnh trên các khối attention: Trong phần này, chúng tôi nghiên cứu bộ dự đoán thưa thớt cho khối Attention trên OPT-175B và để khối MLP như tính toán dày đặc. Bảng 4 hiển thị độ chính xác kiểm tra trên các tác vụ zero-shot và perplexity trên các bộ dữ liệu mô hình hóa ngôn ngữ. Tóm lại, bộ dự đoán thưa thớt Attention không gây mất độ chính xác ở khoảng 50% thưa thớt. Trong quá trình huấn luyện bộ dự đoán thưa thớt Attention, chúng tôi quan sát các xu hướng khác nhau so với bộ dự đoán thưa thớt MLP. Độ chính xác validation khoảng 93% trong các lớp giữa và gần 99% trong các lớp nông và sâu.

Thưa thớt theo ngữ cảnh trên các mô hình nhỏ hơn: Các thí nghiệm chính của chúng tôi tập trung vào OPT-175B. Ở đây, chúng tôi xác minh hiệu quả của DEJAVU trên một mô hình nhỏ hơn, cụ thể là OPT-66B. Trong Bảng 5, chúng tôi tóm tắt độ chính xác trên tác vụ zero-shot ở 50% thưa thớt. Tương tự như DEJAVU-OPT-175B, chúng tôi không nhận thấy mất độ chính xác.

Thưa thớt theo ngữ cảnh trên các mô hình khác: Chúng tôi mở rộng đánh giá sang một họ mô hình khác. Trong Bảng 6, chúng tôi tóm tắt độ chính xác ở thưa thớt attention 50% và thưa thớt MLP 30%. Tương tự như họ OPT, chúng tôi không nhận thấy mất độ chính xác. Mức thưa thớt thấp hơn trong MLP là do sự khác biệt trong hàm kích hoạt.

Thưa thớt không theo ngữ cảnh: Như chúng tôi đã đề cập trong Phần 1, người ta có thể dự đoán thưa thớt mà không có thông tin ngữ cảnh. Đối với thưa thớt không theo ngữ cảnh, chúng tôi dựa vào embedding gốc tại lớp đầu vào. Tại mỗi khối, chúng tôi đầu tiên truyền embedding gốc để ghi lại một tập con tham số tạo ra chuẩn lớn. Trong lượt truyền thứ hai, embedding tại mỗi lớp chỉ sử dụng tập con đã ghi lại. Như được hiển thị trong Hình 1, dự đoán không theo ngữ cảnh không đủ và dẫn đến mất độ chính xác ngay cả ở 50% thưa thớt. Kết quả này xác minh các lựa chọn thiết kế của chúng tôi về việc dựa vào kích hoạt tại mỗi lớp như đầu vào để đưa ra dự đoán thưa thớt theo ngữ cảnh.

Khả năng tương thích với lượng tử hóa: Lượng tử hóa là một hướng đầy hứa hẹn khác cho các mô hình ngôn ngữ hiệu quả. Chúng tôi khảo sát khả năng kết hợp thưa thớt theo ngữ cảnh với các kỹ thuật lượng tử hóa. Đối với DEJAVU-OPT-175B, chúng tôi đặt toàn bộ thưa thớt mô hình ở 75%. Đối với lượng tử hóa, chúng tôi áp dụng lượng tử hóa 4-bit trên trọng số mô hình (W4A16). Như được hiển thị trong Bảng 7, sự kết hợp của lượng tử hóa và DEJAVU gần như luôn đạt độ chính xác tốt hơn so với DEJAVU hoặc lượng tử hóa riêng lẻ. Điều này gợi ý rằng các lỗi xấp xỉ từ hai hướng này không bị gộp lại.

6 Kết luận

Mục tiêu chính của chúng tôi là làm cho suy luận LLM hiệu quả để khả năng học trong ngữ cảnh mạnh mẽ của chúng có thể được sử dụng trong nhiều lĩnh vực ứng dụng hơn. Chúng tôi quan sát rằng thưa thớt theo ngữ cảnh có thể được dự đoán chính xác bằng các thuật toán dựa trên học tập nhẹ. Điều này thúc đẩy chúng tôi thiết kế DEJAVU sử dụng các bộ dự đoán nhìn trước không đồng bộ và thưa thớt hiệu quả phần cứng để tăng tốc suy luận LLM trong thời gian thực. Kết quả thực nghiệm khích lệ của chúng tôi xác thực rằng thưa thớt theo ngữ cảnh có thể giảm độ trễ suy luận hơn 2× so với FasterTransformer tiên tiến nhất mà không làm giảm chất lượng mô hình. Phương pháp của chúng tôi là một bước hướng tới việc làm cho LLM dễ tiếp cận hơn với cộng đồng chung, có thể mở khóa các ứng dụng AI mới thú vị.

Lời cảm ơn
Chúng tôi muốn cảm ơn Ryan Spring, Laurel Orr, Guangxuan Xiao, Eric Han, Xun Huang, Daniel Y. Fu, Benjamin Spector, Ruan Silva, Diana Liskovich, và các nhà phê bình ẩn danh vì những thảo luận và phản hồi hữu ích. Chúng tôi thừa nhận sự hỗ trợ hào phóng của Together Computer, điều này đã cho phép các tính toán một phần cần thiết trong nghiên cứu này.
