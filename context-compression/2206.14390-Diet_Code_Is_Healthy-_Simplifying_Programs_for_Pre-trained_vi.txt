# 2206.14390.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/context-compression/2206.14390.pdf
# Kích thước tệp: 1379232 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Diet Code Là Khỏe Mạnh: Đơn Giản Hóa Chương Trình cho Các Mô Hình
Được Tiền Huấn Luyện của Code
Zhaowei Zhang1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1∗
1School of Software, Shanghai Jiao Tong University, China
2The University of Newcastle, Australia
andy_zhangzw@outlook.com
{bjshen,xiaodong.gu}@sjtu.edu.cn,hongyu.zhang@newcastle.edu.au
TÓM TẮT
Các mô hình biểu diễn code được tiền huấn luyện như CodeBERT đã chứng minh
hiệu suất vượt trội trong nhiều tác vụ kỹ thuật phần mềm khác nhau, tuy nhiên
chúng thường nặng về độ phức tạp, tăng bậc hai theo độ dài của chuỗi đầu vào.
Phân tích thực nghiệm của chúng tôi về sự chú ý của CodeBERT cho thấy CodeBERT
chú ý nhiều hơn đến một số loại token và câu lệnh nhất định như từ khóa và các
câu lệnh liên quan đến dữ liệu. Dựa trên những phát hiện này, chúng tôi đề xuất
DietCode, nhằm tận dụng nhẹ nhàng các mô hình được tiền huấn luyện lớn cho
mã nguồn. DietCode đơn giản hóa chương trình đầu vào của CodeBERT với ba chiến
lược, cụ thể là word dropout, frequency filtering, và một chiến lược dựa trên
sự chú ý để chọn các câu lệnh và token nhận được trọng số chú ý cao nhất trong
quá trình tiền huấn luyện. Do đó, nó mang lại sự giảm đáng kể chi phí tính toán
mà không ảnh hưởng đến hiệu suất mô hình. Kết quả thực nghiệm trên hai tác vụ
downstream cho thấy DietCode cung cấp kết quả tương đương với CodeBERT với chi
phí tính toán ít hơn 40% trong fine-tuning và testing.
CCS CONCEPTS
•Computing methodologies →Natural language processing .
KEYWORDS
Program simplification, Pre-trained models, Learning program representations, Code intelligence
ACM Reference Format:
Zhaowei Zhang1, Hongyu Zhang2, Beijun Shen1, Xiaodong Gu1. 2022. Diet
Code Is Healthy: Simplifying Programs for Pre-trained Models of Code. In
Proceedings of the 30th ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE '22),
November 14–18, 2022, Singapore, Singapore. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3540250.3549094
∗Xiaodong Gu là tác giả liên lạc.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore
©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9413-0/22/11. . . $15.00
https://doi.org/10.1145/3540250.35490941 GIỚI THIỆU
Các mô hình được tiền huấn luyện của code như CodeBERT [15] đã trở thành
công nghệ biểu diễn chương trình tiên tiến, mang lại hiệu suất đáng chú ý
trên nhiều tác vụ kỹ thuật phần mềm khác nhau như code completion [11],
code search [15], và clone detection [29]. Sau khi được tiền huấn luyện trên
các kho dữ liệu code quy mô lớn, chúng thể hiện sự hiểu biết tốt hơn về ngữ
nghĩa của mã nguồn so với các mô hình deep learning trước đó như code2vec [3]
và ASTNN [50].
Mặc dù tạo ra bước nhảy vọt về độ chính xác, các mô hình được tiền huấn luyện
thường nặng về tính toán, điều này cản trở đáng kể việc ứng dụng của chúng
trong thực tế. Ví dụ, CodeBERT tiêu chuẩn chứa 125 triệu tham số và mất 28
giờ để tiền huấn luyện trên 8.5M code. Quan trọng hơn, các mô hình được tiền
huấn luyện thường cần được fine-tuned trước khi sử dụng, điều này không hiệu
quả về chi phí do quy mô lớn của tham số và dữ liệu huấn luyện. Do đó, việc
xác định đặc trưng quan trọng được học bởi các mô hình được tiền huấn luyện
và giảm chi phí tính toán cần thiết bằng cách chỉ tập trung vào thông tin quan
trọng từ đầu vào mô hình [34] là rất mong muốn.
Để hiểu thông tin quan trọng được học bởi các mô hình được tiền huấn luyện,
chúng tôi thực hiện phân tích thực nghiệm về CodeBERT – một mô hình được tiền
huấn luyện cho ngôn ngữ lập trình và ngôn ngữ tự nhiên. Nghiên cứu của chúng
tôi nhằm tìm hiểu (i) CodeBERT chú ý nhiều nhất đến những loại token nào; và
(ii) những loại câu lệnh nào quan trọng nhất đối với CodeBERT khi học biểu diễn
code. Để trả lời hai câu hỏi này, chúng tôi phân loại token và câu lệnh thành
một số lớp và tóm tắt trọng số chú ý của mỗi lớp mà CodeBERT được tiền huấn
luyện gán cho. Kết quả của chúng tôi cho thấy các từ khóa và kiểu dữ liệu là
những token quan trọng nhất mà CodeBERT tập trung vào. Về mặt câu lệnh,
CodeBERT chú ý nhiều hơn đến chữ ký phương thức và câu lệnh return, điều này
cho thấy chức năng tổng thể của một phương thức.
Dựa trên những phát hiện thực nghiệm này, chúng tôi đề xuất DietCode, một
phương pháp mới nhằm tận dụng nhẹ nhàng các mô hình được tiền huấn luyện lớn
cho mã nguồn. DietCode giảm độ phức tạp tính toán của các mô hình được tiền
huấn luyện bằng cách loại bỏ các token và câu lệnh không quan trọng khỏi các
chương trình đầu vào. Ba chiến lược pruning được đề xuất, bao gồm word dropout,
frequency filtering, và attention-based pruning. Đặc biệt, chiến lược
attention-based pruning chọn các token và câu lệnh nhận được sự chú ý cao nhất
từ các mô hình được tiền huấn luyện. Thuật toán đơn giản hóa chương trình công
thức hóa việc chọn câu lệnh như một bài toán 0-1 knapsack trong đó các câu lệnh
được coi như các vật phẩm, và trọng số chú ý của chúng được coi như giá trị.
Thuật toán chọn các câu lệnh (vật phẩm) dưới ràng buộc của độ dài mục tiêu đã
cho (dung lượng).
Chúng tôi áp dụng DietCode cho hai tác vụ downstream, cụ thể là code search và
code summarization. Chúng tôi đo lường hiệu suất với độ dài tương đối, FLOPs,
và chi phí thời gian, và so sánh phương pháp của chúng tôi với các mô hình
baseline, bao gồm mô hình code được tiền huấn luyện gốc và SIVAND [34]. Kết
quả thực nghiệm cho thấy DietCode cung cấp kết quả tương đương như RoBERTa
(code), với chi phí tính toán ít hơn gần 40% trong fine-tuning và testing.
Đóng góp của chúng tôi có thể được tóm tắt như sau:
•Chúng tôi thực hiện phân tích thực nghiệm sâu sắc về các token và câu lệnh
quan trọng được học bởi CodeBERT.
•Chúng tôi đề xuất một phương pháp đơn giản hóa chương trình mới cho các mô
hình ngôn ngữ lập trình được tiền huấn luyện có thể giảm đáng kể chi phí tính
toán trong khi duy trì hiệu suất tương đương.
•Chúng tôi đánh giá rộng rãi phương pháp đề xuất trong hai tác vụ downstream
và cho thấy hiệu quả của phương pháp của chúng tôi.
2 BỐI CẢNH
2.1 Các Mô Hình Ngôn Ngữ Được Tiền Huấn Luyện
Các mô hình ngôn ngữ được tiền huấn luyện như BERT [14], GPT-2 [35], và T5
[36] đã đạt được thành công đáng chú ý trong nhiều tác vụ NLP khác nhau [12,13,47].
Chúng đề cập đến các mô hình mạng nơ-ron được huấn luyện trên các kho văn bản
lớn và có thể được fine-tuned cho các tác vụ downstream có ít tài nguyên.
Các mô hình được tiền huấn luyện tiên tiến chủ yếu được xây dựng dựa trên kiến
trúc Transformer [43]. Transformer là một mô hình học sequence-to-sequence sử
dụng cơ chế attention [4]. Nó dựa trên kiến trúc encoder-decoder, trong đó một
chuỗi nguồn được mã hóa thành các trạng thái ẩn và sau đó được đưa làm đầu vào
cho decoder để tạo ra một chuỗi đích. Cả encoder và decoder đều chứa nhiều lớp
giống hệt nhau, và mỗi lớp bao gồm một mạng multi-head self-attention theo sau
bởi một mạng feed-forward. Cả hai đầu ra sẽ được chuẩn hóa trước khi đi vào
lớp tiếp theo.
Thành phần chính của Transformer là cơ chế self-attention biểu diễn một chuỗi
bằng cách liên kết các token ở các vị trí khác nhau [43]. Mục tiêu của
self-attention là học các vùng quan trọng trong chuỗi đầu vào. Không giống như
các mạng nơ-ron tuần hoàn truyền thống [10], các mạng self-attention có thể học
các phụ thuộc từ các token xa trong song song. Đối với một chuỗi đầu vào
(𝑥1,...,𝑥𝑛) có độ dài 𝑛, self-attention tạo ra biểu diễn của nó (𝑧1,𝑧2...𝑧𝑛) như
𝑧𝑖=𝑛∑︁𝑗=1Softmax((𝑥𝑖𝑊𝑄)·(𝑥𝑗𝑊𝐾)𝑇/√𝑑)·𝑥𝑗𝑊𝑉(1)
trong đó 𝑊𝑄,𝑊𝐾, và 𝑊𝑉 biểu thị các tham số của mô hình. 𝑑 là chiều của ma trận
đầu vào. Mô hình bao gồm một số attention head, và đầu ra của mỗi head được
nối lại thành kết quả cuối cùng.
Các mô hình được tiền huấn luyện thường bao gồm các tham số quy mô lớn và tiêu
thụ tài nguyên tính toán khổng lồ. Ví dụ, BERT-base và BERT-large chứa tương
ứng 110M và 340M tham số. Do đó, việc tận dụng nhẹ nhàng các mô hình được tiền
huấn luyện là rất mong muốn cho nghiên cứu và các nhà thực hành [39].
2.2 CodeBERT
Gần đây, các nhà nghiên cứu đã áp dụng BERT cho các tác vụ kỹ thuật phần mềm
và đề xuất CodeBERT [15]. CodeBERT là một mô hình được tiền huấn luyện bimodal
cho ngôn ngữ tự nhiên và ngôn ngữ lập trình, nắm bắt các biểu diễn ngữ nghĩa
từ ngôn ngữ lập trình [15]. Các biểu diễn chương trình được học bởi CodeBERT có
thể được sử dụng thêm cho các tác vụ downstream như code search và code
summarization.
CodeBERT được xây dựng dựa trên một encoder Transformer [43]. Việc tối ưu hóa
bao gồm hai tác vụ: masked language modeling (MLM) và replaced token
detection (RTD). MLM che hai token ngẫu nhiên từ cặp đầu vào của code và
comment ngôn ngữ tự nhiên, và nhằm dự đoán token gốc trong một từ vựng mở
rộng. RTD bao gồm hai generator và một discriminator. Các generator dự đoán
token gốc cho token bị che trong khi discriminator dự đoán liệu các token có
phải là gốc hay không. Sau khi tiền huấn luyện, CodeBERT có thể được điều chỉnh
cho các tác vụ downstream thông qua fine-tuning trên tập dữ liệu đích.
3 PHÂN TÍCH THỰC NGHIỆM
3.1 Thiết Kế Nghiên Cứu
Trong phần này, chúng tôi mô tả phương pháp nghiên cứu và thiết lập thực nghiệm
của chúng tôi. Có nhiều mức độ chi tiết cho code, chẳng hạn như token, câu lệnh,
và hàm. Như một nghiên cứu sâu sắc, chúng tôi bắt đầu bằng việc điều tra đơn vị
nguyên tử của mã nguồn, cụ thể là token. Tiếp theo, chúng tôi điều tra kiến thức
ở mức câu lệnh được học bởi CodeBERT, trong đó chứa các cấu trúc cơ bản và đơn
vị ngữ nghĩa. Cuối cùng, chúng tôi khám phá kiến thức ở mức hàm được học bởi
CodeBERT thông qua các tác vụ downstream. Tóm lại, chúng tôi thiết kế phương
pháp nghiên cứu của mình bằng cách giải quyết các câu hỏi nghiên cứu sau:
•RQ1: CodeBERT học về những token quan trọng nào? Chúng tôi nghiên cứu
thông tin quan trọng mà CodeBERT học được ở mức token bằng cách phân tích
trọng số chú ý được gán cho các token này và trực quan hóa tầm quan trọng
tương đối của chúng.
•RQ2: CodeBERT học về những câu lệnh quan trọng nào? Chúng tôi tiếp tục
nghiên cứu các câu lệnh mà CodeBERT gán trọng số cao nhất. Chúng tôi phân
loại các câu lệnh code thành các danh mục phổ biến như initialization,
assignment, và return, và trình bày trọng số chú ý của mỗi danh mục được gán
bởi CodeBERT.
Để trả lời những câu hỏi này, trước tiên chúng tôi cần biết cách biểu diễn thông
tin chính được học bởi CodeBERT. Nói cách khác, làm thế nào để đo lường tầm
quan trọng của mỗi token và câu lệnh? Vì trái tim của CodeBERT là mạng
self-attention, trong đó các trạng thái ẩn của mỗi token được tính toán từng lớp
theo trọng số self-attention, chúng tôi đo lường tầm quan trọng của mỗi token
bằng cách sử dụng trọng số chú ý trong các lớp Transformer trong CodeBERT sau
khi tiền huấn luyện. Tiếp theo, chúng tôi sẽ trình bày chi tiết về cách đo lường
tầm quan trọng của token và câu lệnh, tương ứng.
3.1.1 Đo Lường Tầm Quan Trọng Token Bằng Trọng Số Chú Ý. Như được giới
thiệu trong Phần 2.2, CodeBERT nhận một chuỗi các token mã nguồn làm đầu vào
và tạo ra một trọng số self-attention cho mỗi token đầu vào. Mỗi trọng số đo
lường mức độ token tương ứng nhận được sự chú ý từ các token khác trong chuỗi
đầu vào. Trọng số chú ý càng cao, càng nhiều sự chú ý được dành bởi các token
khác. Do đó, trong nghiên cứu của chúng tôi, chúng tôi đo lường tầm quan trọng
của mỗi token bằng cách sử dụng trọng số chú ý. CodeBERT có nhiều lớp và head
self-attention, mỗi lớp tạo ra một trọng số chú ý cho cùng một token. Chúng tôi
lấy trung bình trọng số chú ý của tất cả các lớp và head cho mỗi token. Trong
các thử nghiệm của chúng tôi, chúng tôi đi qua kho dữ liệu CodeSearchNet [20]
và lấy làm đầu vào mỗi đoạn code cho CodeBERT được tiền huấn luyện. Sau đó,
chúng tôi tính toán trọng số chú ý trung bình mà CodeBERT gán cho mỗi token.
3.1.2 Tính Toán Chú Ý của Câu Lệnh. Sau khi có được tầm quan trọng của mỗi
token, chúng tôi tính toán trọng số chú ý cho mỗi câu lệnh. Trực quan, trọng số
chú ý cho một câu lệnh có thể đơn giản được lấy bằng cách sử dụng trọng số chú
ý trung bình của tất cả các token của nó. Tuy nhiên, các token khác nhau trong
một câu lệnh có tầm quan trọng khác nhau. Mà không xem xét tầm quan trọng
toàn cục của mỗi token, các câu lệnh bao gồm các token không quan trọng có thể
thậm chí có được sự chú ý cao hơn. Dựa trên mối quan tâm này, chúng tôi điều
chỉnh sự chú ý câu lệnh bằng cách phạt các token không quan trọng nhận được ít
trọng số chú ý hơn trên toàn bộ kho dữ liệu. Cụ thể hơn, chúng tôi tính toán
trọng số chú ý cho một câu lệnh 𝑆 bằng cách sử dụng trung bình có trọng số của
trọng số chú ý của các token của nó:
𝑎(𝑆)=∑︁𝑡∈𝑆𝑤(𝑡)·𝑎(𝑡) (2)
trong đó 𝑎(𝑡) biểu thị trọng số chú ý của token 𝑡 trong câu lệnh 𝑆; 𝑤(𝑡) biểu
thị trọng số chú ý chuẩn hóa của token 𝑡 trong toàn bộ kho dữ liệu được mô tả
trong phần trước. Việc chuẩn hóa được thực hiện bằng hàm Softmax, cụ thể,
𝑤(𝑡)=Softmax𝑡∈𝑆(𝑎(𝑡)) (3)
Không giống như token, các câu lệnh thường là duy nhất trong kho dữ liệu và
không thể được bao gồm trong từ điển. Để phân tích hiệu quả các câu lệnh, chúng
tôi phân loại các câu lệnh thành 21 danh mục như method signature, variable
declaration, và if condition, và chỉ trình bày trọng số chú ý trung bình cho mỗi
danh mục. Các danh mục này chủ yếu được hướng dẫn bởi đặc tả Java [16]. Đặc
biệt, danh mục logger chứa các câu lệnh với các chức năng logging tiêu chuẩn như
log4j, Logger, và println. Bảng 1 cho thấy các danh mục câu lệnh mà chúng tôi
đã tóm tắt và số lượng tương ứng trong kho dữ liệu CodeSearchNet. Chúng ta có
thể thấy rằng function invocation và method signature là những câu lệnh phổ
biến nhất, trong khi các câu lệnh vòng lặp như while tương đối hiếm.

--- TRANG 2 ---
ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore Zhang, et al.
tương đối length, FLOPs, và time cost, và so sánh phương pháp của chúng tôi
với các mô hình baseline, bao gồm mô hình code được tiền huấn luyện gốc và
SIVAND [34]. Kết quả thực nghiệm cho thấy DietCode cung cấp kết quả tương
đương như RoBERTa (code), với gần 40% ít chi phí tính toán hơn trong
fine-tuning và testing.
Đóng góp của chúng tôi có thể được tóm tắt như sau:
•Chúng tôi thực hiện phân tích thực nghiệm sâu sắc về các token và câu lệnh
quan trọng được học bởi CodeBERT.
•Chúng tôi đề xuất một phương pháp đơn giản hóa chương trình mới cho các mô
hình ngôn ngữ lập trình được tiền huấn luyện có thể giảm đáng kể chi phí tính
toán trong khi duy trì hiệu suất tương đương.
•Chúng tôi đánh giá rộng rãi phương pháp đề xuất trong hai tác vụ downstream
và cho thấy hiệu quả của phương pháp của chúng tôi.
2 BỐI CẢNH
2.1 Các Mô Hình Ngôn Ngữ Được Tiền Huấn Luyện
Các mô hình ngôn ngữ được tiền huấn luyện như BERT [14], GPT-2 [35], và T5
[36] đã đạt được thành công đáng chú ý trong nhiều tác vụ NLP khác nhau [12,13,47].
Chúng đề cập đến các mô hình mạng nơ-ron được huấn luyện trên các kho văn bản
lớn và có thể được fine-tuned cho các tác vụ downstream có ít tài nguyên.
Các mô hình được tiền huấn luyện tiên tiến chủ yếu được xây dựng dựa trên kiến
trúc Transformer [43]. Transformer là một mô hình học sequence-to-sequence sử
dụng cơ chế attention [4]. Nó dựa trên kiến trúc encoder-decoder, trong đó một
chuỗi nguồn được mã hóa thành các trạng thái ẩn và sau đó được đưa làm đầu vào
cho decoder để tạo ra một chuỗi đích. Cả encoder và decoder đều chứa nhiều lớp
giống hệt nhau, và mỗi lớp bao gồm một mạng multi-head self-attention theo sau
bởi một mạng feed-forward. Cả hai đầu ra sẽ được chuẩn hóa trước khi đi vào
lớp tiếp theo.
Thành phần chính của Transformer là cơ chế self-attention biểu diễn một chuỗi
bằng cách liên kết các token ở các vị trí khác nhau [43]. Mục tiêu của
self-attention là học các vùng quan trọng trong chuỗi đầu vào. Không giống như
các mạng nơ-ron tuần hoàn truyền thống [10], các mạng self-attention có thể học
các phụ thuộc từ các token xa trong song song. Đối với một chuỗi đầu vào
(𝑥1,...,𝑥𝑛) có độ dài 𝑛, self-attention tạo ra biểu diễn của nó (𝑧1,𝑧2...𝑧𝑛) như
𝑧𝑖=𝑛∑︁𝑗=1Softmax((𝑥𝑖𝑊𝑄)·(𝑥𝑗𝑊𝐾)𝑇/√𝑑)·𝑥𝑗𝑊𝑉(1)
trong đó 𝑊𝑄,𝑊𝐾, và 𝑊𝑉 biểu thị các tham số của mô hình. 𝑑 là chiều của ma trận
đầu vào. Mô hình bao gồm một số attention head, và đầu ra của mỗi head được
nối lại thành kết quả cuối cùng.
Các mô hình được tiền huấn luyện thường bao gồm các tham số quy mô lớn và tiêu
thụ tài nguyên tính toán khổng lồ. Ví dụ, BERT-base và BERT-large chứa tương
ứng 110M và 340M tham số. Do đó, việc tận dụng nhẹ nhàng các mô hình được tiền
huấn luyện là rất mong muốn cho nghiên cứu và các nhà thực hành [39].
2.2 CodeBERT
Gần đây, các nhà nghiên cứu đã áp dụng BERT cho các tác vụ kỹ thuật phần mềm
và đề xuất CodeBERT [15]. CodeBERT là một mô hình được tiền huấn luyện bimodal
cho ngôn ngữ tự nhiên và ngôn ngữ lập trình, nắm bắt các biểu diễn ngữ nghĩa
từ ngôn ngữ lập trình [15]. Các biểu diễn chương trình được học bởi CodeBERT có
thể được sử dụng thêm cho các tác vụ downstream như code search và code
summarization.
CodeBERT được xây dựng dựa trên một encoder Transformer [43]. Việc tối ưu hóa
bao gồm hai tác vụ: masked language modeling (MLM) và replaced token
detection (RTD). MLM che hai token ngẫu nhiên từ cặp đầu vào của code và
comment ngôn ngữ tự nhiên, và nhằm dự đoán token gốc trong một từ vựng mở
rộng. RTD bao gồm hai generator và một discriminator. Các generator dự đoán
token gốc cho token bị che trong khi discriminator dự đoán liệu các token có
phải là gốc hay không. Sau khi tiền huấn luyện, CodeBERT có thể được điều chỉnh
cho các tác vụ downstream thông qua fine-tuning trên tập dữ liệu đích.
3 PHÂN TÍCH THỰC NGHIỆM
3.1 Thiết Kế Nghiên Cứu
Trong phần này, chúng tôi mô tả phương pháp nghiên cứu và thiết lập thực nghiệm
của chúng tôi. Có nhiều mức độ chi tiết cho code, chẳng hạn như token, câu lệnh,
và hàm. Như một nghiên cứu sâu sắc, chúng tôi bắt đầu bằng việc điều tra đơn vị
nguyên tử của mã nguồn, cụ thể là token. Tiếp theo, chúng tôi điều tra kiến thức
ở mức câu lệnh được học bởi CodeBERT, trong đó chứa các cấu trúc cơ bản và đơn
vị ngữ nghĩa. Cuối cùng, chúng tôi khám phá kiến thức ở mức hàm được học bởi
CodeBERT thông qua các tác vụ downstream. Tóm lại, chúng tôi thiết kế phương
pháp nghiên cứu của mình bằng cách giải quyết các câu hỏi nghiên cứu sau:
•RQ1: CodeBERT học về những token quan trọng nào? Chúng tôi nghiên cứu
thông tin quan trọng mà CodeBERT học được ở mức token bằng cách phân tích
trọng số chú ý được gán cho các token này và trực quan hóa tầm quan trọng
tương đối của chúng.
•RQ2: CodeBERT học về những câu lệnh quan trọng nào? Chúng tôi tiếp tục
nghiên cứu các câu lệnh mà CodeBERT gán trọng số cao nhất. Chúng tôi phân
loại các câu lệnh code thành các danh mục phổ biến như initialization,
assignment, và return, và trình bày trọng số chú ý của mỗi danh mục được gán
bởi CodeBERT.
Để trả lời những câu hỏi này, trước tiên chúng tôi cần biết cách biểu diễn thông
tin chính được học bởi CodeBERT. Nói cách khác, làm thế nào để đo lường tầm
quan trọng của mỗi token và câu lệnh? Vì trái tim của CodeBERT là mạng
self-attention, trong đó các trạng thái ẩn của mỗi token được tính toán từng lớp
theo trọng số self-attention, chúng tôi đo lường tầm quan trọng của mỗi token
bằng cách sử dụng trọng số chú ý trong các lớp Transformer trong CodeBERT sau
khi tiền huấn luyện. Tiếp theo, chúng tôi sẽ trình bày chi tiết về cách đo lường
tầm quan trọng của token và câu lệnh, tương ứng.
3.1.1 Đo Lường Tầm Quan Trọng Token Bằng Trọng Số Chú Ý. Như được giới
thiệu trong Phần 2.2, CodeBERT nhận một chuỗi các token mã nguồn làm đầu vào
và tạo ra một trọng số self-attention cho mỗi token đầu vào. Mỗi trọng số đo
lường mức độ token tương ứng nhận được sự chú ý từ các token khác trong chuỗi
đầu vào. Trọng số chú ý càng cao, càng nhiều sự chú ý được dành bởi các token
khác. Do đó, trong nghiên cứu của chúng tôi, chúng tôi đo lường tầm quan trọng
của mỗi token bằng cách sử dụng trọng số chú ý. CodeBERT có nhiều lớp và head
self-attention, mỗi lớp tạo ra một trọng số chú ý

--- TRANG 3 ---
Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore
Bảng 1: Thống kê các câu lệnh. Arithmetic có nghĩa là các câu lệnh chỉ có các phép toán toán học. Function Invocation biểu thị các câu lệnh gọi các hàm khác.
Danh mục Số lượng Danh mục Số lượng
Function Invocation 16,558 Throw 1,460
Method Signature 11,755 Catch 1,309
Variable Declaration 11,701 Arithmetic 628
If Condition 11,646 Case 577
Annotation 8,980 While 459
Return 8,331 Break 341
Getter 3,092 Finally 297
For 2,190 Continue 142
Try 1,797 Switch 210
Logging 1,763 Synchronized 73
Setter 1,721
cho cùng một token. Chúng tôi lấy trung bình trọng số chú ý của tất cả các lớp và head cho mỗi token. Trong các thử nghiệm của chúng tôi, chúng tôi đi qua kho dữ liệu CodeSearchNet [20] và lấy làm đầu vào mỗi đoạn code cho CodeBERT được tiền huấn luyện. Sau đó, chúng tôi tính toán trọng số chú ý trung bình mà CodeBERT gán cho mỗi token.
3.1.2 Tính Toán Chú Ý của Câu Lệnh. Sau khi có được tầm quan trọng của mỗi token, chúng tôi tính toán trọng số chú ý cho mỗi câu lệnh. Trực quan, trọng số chú ý cho một câu lệnh có thể đơn giản được lấy bằng cách sử dụng trọng số chú ý trung bình của tất cả các token của nó. Tuy nhiên, các token khác nhau trong một câu lệnh có tầm quan trọng khác nhau. Mà không xem xét tầm quan trọng toàn cục của mỗi token, các câu lệnh bao gồm các token không quan trọng có thể thậm chí có được sự chú ý cao hơn. Dựa trên mối quan tâm này, chúng tôi điều chỉnh sự chú ý câu lệnh bằng cách phạt các token không quan trọng nhận được ít trọng số chú ý hơn trên toàn bộ kho dữ liệu. Cụ thể hơn, chúng tôi tính toán trọng số chú ý cho một câu lệnh 𝑆 bằng cách sử dụng trung bình có trọng số của trọng số chú ý của các token của nó:
𝑎(𝑆)=∑︁𝑡∈𝑆𝑤(𝑡)·𝑎(𝑡) (2)
trong đó 𝑎(𝑡) biểu thị trọng số chú ý của token 𝑡 trong câu lệnh 𝑆; 𝑤(𝑡) biểu thị trọng số chú ý chuẩn hóa của token 𝑡 trong toàn bộ kho dữ liệu được mô tả trong phần trước. Việc chuẩn hóa được thực hiện bằng hàm Softmax, cụ thể,
𝑤(𝑡)=Softmax𝑡∈𝑆(𝑎(𝑡)) (3)
Không giống như token, các câu lệnh thường là duy nhất trong kho dữ liệu và không thể được bao gồm trong từ điển. Để phân tích hiệu quả các câu lệnh, chúng tôi phân loại các câu lệnh thành 21 danh mục như method signature, variable declaration, và if condition, và chỉ trình bày trọng số chú ý trung bình cho mỗi danh mục. Các danh mục này chủ yếu được hướng dẫn bởi đặc tả Java [16]. Đặc biệt, danh mục logger chứa các câu lệnh với các chức năng logging tiêu chuẩn như log4j, Logger, và println. Bảng 1 cho thấy các danh mục câu lệnh mà chúng tôi đã tóm tắt và số lượng tương ứng trong kho dữ liệu CodeSearchNet. Chúng ta có thể thấy rằng function invocation và method signature là những câu lệnh phổ biến nhất, trong khi các câu lệnh vòng lặp như while tương đối hiếm.

3.2 Kết Quả và Phân Tích
Trong phần này, chúng tôi trình bày kết quả thử nghiệm của chúng tôi cho mỗi câu hỏi nghiên cứu.
3.2.1 CodeBERT học về các token code gì? (RQ1). Hình 2 làm nổi bật các token quan trọng trong Java nhận được nhiều sự chú ý nhất từ CodeBERT. Biểu đồ được tóm tắt từ kho dữ liệu CodeSearchNet [20] chứa hơn 100,000 hàm Java. Đối với các token trong mỗi hàm, chúng tôi tính toán trọng số chú ý của chúng và chèn chúng vào bản đồ <token, attention> cho đến khi các khóa của bản đồ ổn định. Để dễ trực quan hóa, chúng tôi loại bỏ các token xuất hiện ít hơn 50 lần trong kho dữ liệu. Trong số tất cả các token Java, coverage nhận được trọng số chú ý thấp nhất (5.43 𝑒-5) trong khi boolean có trọng số cao nhất (2.94 𝑒-2). Độ lệch chuẩn của các trọng số này là 5.97 𝑒-3.
Kết quả cho thấy CodeBERT gán nhiều sự chú ý hơn cho các từ khóa Java như public và boolean. Điều này được mong đợi vì các từ khóa Java thường xuyên xuất hiện trong code Java và đóng vai trò chi phối trong việc biểu diễn ngữ nghĩa code. Một danh mục token khác nhận được sự chú ý cao là các định danh hướng dữ liệu như Map, List, và String, có lẽ vì chúng định nghĩa các cấu trúc dữ liệu chính trong một hàm.
Được thúc đẩy bởi phát hiện này, chúng tôi tiếp tục nghiên cứu sự chú ý của các từ khóa Java. Hình 1 cho thấy trọng số chú ý của mỗi từ khóa Java. Như chúng ta có thể thấy, từ khóa private nhận được trọng số chú ý cao nhất là 1.15 𝑒-2 trong số tất cả các từ khóa, trong khi interface và transient nhận được trọng số chú ý thấp nhất lần lượt là 7.14 𝑒-4 và 9.62𝑒-4. Độ lệch chuẩn của các trọng số này là 1.513𝑒-3.
Trong số tất cả các từ khóa Java, các modifier phương thức như public, private, và static nhận được trọng số chú ý cao nhất. Chúng tôi đoán rằng các modifier này thường biểu thị sự bắt đầu của một chữ ký phương thức, điều này rất quan trọng để hiểu toàn bộ code. Tiếp theo các modifier này, finally và return cũng nhận được trọng số chú ý cao. Finally thường biểu thị kết thúc của một hàm, và code bên trong khối finally chắc chắn sẽ được thực thi. Return biểu thị đầu ra của phương thức, có thể cho thấy chức năng của phương thức. Đáng ngạc nhiên, các từ khóa liên quan đến phân nhánh như if và switch nhận được sự chú ý thấp hơn, có lẽ vì CodeBERT coi chúng không quan trọng đối với việc hiểu chương trình mặc dù chúng xuất hiện thường xuyên trong code.
Một mô hình tương tự có thể được quan sát trong Python. Ví dụ, CodeBERT ưa thích các từ khóa như return và def biểu thị cấu trúc chung của một hàm, giống như return và public trong Java. Đặc biệt, trong số các từ khóa phân nhánh và vòng lặp, while xếp hạng cao hơn if và for.
Hình 3 cho thấy bản đồ nhiệt của sự chú ý cho một hàm Java. Code là về việc đọc nội dung từ một tệp. Các token có trọng số chú ý cao hơn được đánh dấu bằng màu sắc đậm hơn. Bản đồ nhiệt cho thấy kết quả tương tự như đã thảo luận ở trên. public và finally nhận được trọng số chú ý cao nhất trong hàm này, trong khi new và if nhận được sự chú ý thấp nhất. Các token liên quan đến chức năng cốt lõi, như File và read, đã cho thấy trọng số chú ý cao. Ngược lại, các token phụ trợ cho chức năng chính, như null và Exception đã nhận được sự chú ý thấp hơn nhiều. Một quan sát thú vị khác là các ký hiệu ngữ pháp như } và ; được CodeBERT coi là cần thiết có lẽ vì

--- TRANG 4 ---
ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore Zhang, et al.
00.0020.0040.0060.0080.010.0120.0140.016protectedprivatepublicstaticwhileassertfinallyreturnvoidbooleansynchronizednativedoubleforclasstrysupershortthisswitchcatchelsefloatthrowthrowsenumcontinuecasedoimportlongbreaknewdefaultcharextendsifinstanceofintfinalbytepackagetransientinterface
Hình 1: Trọng số chú ý của các từ khóa Java được học bởi CodeBERT
Hình 2: Các token Java được làm nổi bật bởi CodeBERT. Kích thước của mỗi token chỉ ra trọng số chú ý được gán bởi CodeBERT.
chúng đánh dấu kết thúc của các câu lệnh và khối. Nhìn chung, bản đồ nhiệt cho thấy CodeBERT chủ yếu chú ý đến các token chức năng phản ánh chức năng tổng thể và không quan tâm nhiều đến các token khái niệm về ngữ pháp và các nhánh đặc biệt.
3.2.2 CodeBERT học về các câu lệnh code gì? (RQ2). Hình 4 cho thấy trọng số chú ý được gán bởi CodeBERT cho mỗi loại câu lệnh Java. Như có thể thấy, method signature đã nhận được sự chú ý cao nhất (4.11 𝑒-3), trong khi case statement nhận được sự chú ý thấp nhất (2.32 𝑒-3). Độ lệch chuẩn của trọng số chú ý trên tất cả các danh mục là 4.88 𝑒-4. Nói chung, CodeBERT tập trung nhiều hơn vào các câu lệnh cho biết chức năng tổng thể của một phương thức, như method signature và return, có lẽ vì chúng
Hình 3: Bản đồ nhiệt của trọng số chú ý cho các câu lệnh và token Java. Màu nền của một token tỷ lệ thuận với trọng số chú ý trung bình được gán cho nó).
chứa thông tin dày đặc của toàn bộ hàm, như tên và mục tiêu.
Thú vị thay, các biểu thức số học (biểu thức với các phép toán toán học) cũng nhận được nhiều sự chú ý hơn các câu lệnh khác. Giống như ngôn ngữ tự nhiên, biểu thức số học có thể được xem như một chuỗi các phép toán cho biết chức năng của một câu lệnh. Function invocations cũng đã được chứng minh là quan trọng đối với CodeBERT để hiểu code. Giống như biểu thức số học, function invocations có thể được coi là các định danh của các chữ ký phương thức khác, có thể được hiểu theo nghĩa đen thông qua tên hàm và tham số.
Đáng ngạc nhiên, CodeBERT không chú ý nhiều đến các câu lệnh liên quan đến cấu trúc luồng điều khiển, như while, for và case.

--- TRANG 5 ---
Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore
0.0020.00250.0030.00350.0040.0045Func. SignatureFinallyReturnExpressionTryAnnotationFunctionWhileSetterContinueGetterSwitchIfForCatchBreakLogVariableThrowSyncronizedCase
Hình 4: Trọng số chú ý của các loại câu lệnh Java khác nhau được học bởi CodeBERT
Điều này chỉ ra rằng CodeBERT có thể học hiệu quả các biểu diễn của văn bản thuần túy trong khi bị hạn chế trong việc học cấu trúc.
Hình 3 cũng cho thấy bản đồ nhiệt của sự chú ý câu lệnh cho một hàm mẫu của Java. Tương tự như các kết luận ở trên, method signature là quan trọng nhất, cung cấp giới thiệu chung về chức năng của phương thức. Đồng thời, các câu lệnh về variable declaration và initialization, như "String content == null" nhận được sự chú ý thấp hơn, có lẽ vì chúng chỉ tạo ra các biến mới với giá trị ban đầu null mà không có hoạt động thực tế nào đối với chúng. Trọng số chú ý của câu lệnh if trong khối finally cũng thấp (2.92 𝑒-3). Hàm này chỉ đóng file reader và có thể có ít ảnh hưởng đến việc hiểu toàn bộ phương thức của CodeBERT.
Chúng tôi quan sát một xu hướng tương tự đối với các câu lệnh Python. Method signatures cũng nhận được trọng số chú ý cao nhất (6.86 𝑒-3), trong khi các câu lệnh phân nhánh như break và continue có trọng số thấp nhất. Độ lệch chuẩn của trọng số chú ý cho các câu lệnh Python là 1.66 𝑒-3, hơi lớn hơn so với các câu lệnh Java.
So với Java, phân bố của các câu lệnh và token Python hơi thưa thớt hơn. Ví dụ, câu lệnh method signature xếp hạng đầu có trọng số cao hơn nhiều so với danh mục câu lệnh thứ hai (tức là return). Từ khóa hàng đầu def cũng nhận được trọng số chú ý cao hơn nhiều so với các từ khóa khác.
4 DIETCODE: ĐƠN GIẢN HÓA CHƯƠNG TRÌNH CHO CODEBERT
Như Phương trình 1 chỉ ra, độ phức tạp tính toán để tính toán ma trận chú ý là O(𝑑²𝑛+𝑛²𝑑), tăng theo bậc hai với độ dài chuỗi. Do đó, hoạt động chú ý trở thành nút thắt cổ chai khi áp dụng cho các chuỗi dài như mã nguồn [25].
Chúng tôi tự hỏi liệu chúng ta có thể loại bỏ các token hoặc câu lệnh không quan trọng khỏi các chương trình đầu vào của CodeBERT hay không. Bằng cách này, chi phí thời gian và không gian cho mô hình được tiền huấn luyện có thể được giảm đáng kể. Dựa trên ý tưởng này, chúng tôi đề xuất DietCode, một mô hình được tiền huấn luyện nhẹ cho mã nguồn. DietCode giảm độ phức tạp tính toán của CodeBERT bằng cách đơn giản hóa các chương trình đầu vào.
Cụ thể hơn, công thức hóa vấn đề của chúng tôi như sau: cho một đoạn code 𝐶={𝑠1;...;𝑠|𝐶|} bao gồm |𝐶| câu lệnh, chúng tôi muốn xuất ra một đoạn code được cắt tỉa 𝐶𝑝 chứa tối đa 𝐿 token. Mục tiêu của việc đơn giản hóa chương trình là cắt tỉa càng nhiều token càng tốt mà không ảnh hưởng đáng kể đến độ chính xác của các tác vụ downstream.
4.1 Word Dropout
Một chiến lược đơn giản cho việc đơn giản hóa chương trình là word dropout [21], cụ thể là loại bỏ ngẫu nhiên các token khỏi code đầu vào để đáp ứng độ dài chuỗi được chỉ định. Đối với mỗi token 𝑤∈𝑆, chúng tôi định nghĩa một biến nhị phân 𝑟𝑤∈{0,1} để chỉ ra liệu token sẽ được giữ lại (𝑟𝑤=1) hay bị cắt tỉa (𝑟𝑤=0):
𝑟𝑤∼Bernoulli(𝑝)
𝐶𝑝={𝑤|𝑤∈𝐶and𝑟𝑤>0}(4)
trong đó 𝑝=𝐿/|𝐶| biểu thị xác suất chọn một token. Ngoài việc giảm độ phức tạp tính toán, word dropout cũng đã được chứng minh là cải thiện tính mạnh mẽ của các mạng nơ-ron [21]. Do đó, nó nâng cao hiệu suất của nhiều tác vụ.
4.2 Frequency Filtering
Một mối quan tâm chính với chiến lược word dropout là nó cũng có thể cắt tỉa các token quan trọng một cách ngẫu nhiên khỏi đầu vào. Do đó, chúng tôi giới thiệu một chiến lược cắt tỉa token dựa trên tần suất khác loại bỏ các token không phổ biến trong khi chỉ giữ lại các token xuất hiện thường xuyên nhất.
Cụ thể, đối với mỗi đoạn code đầu vào 𝐶={𝑤1,...,𝑤𝑛}, chúng tôi giữ lại các token nếu tần suất của chúng nằm trong top 𝑘 (𝑘=|𝐶𝑝|) của tất cả các token trong đầu vào, cụ thể,
𝐶𝑝={𝑤|𝑤∈𝐶,𝑓(𝑤)∈top−𝑘(𝑓(𝑤1),...,𝑓(𝑤𝑛)),𝑘=|𝐶𝑝|)(5)
trong đó 𝑓(𝑤) biểu thị tần suất của 𝑤 trong toàn bộ kho dữ liệu code.
4.3 Attention-based Code Pruning
Chiến lược frequency filtering có thể phân biệt thô các token quan trọng, nhưng có thể chỉ thiên vị các token phổ biến. Theo các phát hiện thực nghiệm của chúng tôi ở trên, CodeBERT chú ý đến một số loại token và câu lệnh nhất định. Các token nhận được sự chú ý cao không trùng khớp với các token phổ biến. Để cung cấp một lựa chọn token và câu lệnh quan trọng chi tiết hơn, chúng tôi đề xuất một chiến lược cắt tỉa code dựa trên sự chú ý để chọn các token và câu lệnh dựa trên trọng số chú ý của chúng.
Quy trình đơn giản hóa được tóm tắt trong Thuật toán 1. Chúng tôi bắt đầu bằng việc tóm tắt danh mục các câu lệnh cho một ngôn ngữ lập trình đã cho theo phương pháp trong nghiên cứu thực nghiệm của chúng tôi. Sau đó, chúng tôi tạo các từ điển chú ý cho cả câu lệnh và token. Thuật toán chạy cắt tỉa code trong hai giai đoạn: chọn câu lệnh và cắt tỉa token.
Trong giai đoạn chọn câu lệnh, chúng tôi chọn các câu lệnh trụ cột thuộc về một danh mục nhận được trọng số chú ý cao nhất trong nghiên cứu thực nghiệm của chúng tôi. Đồng thời, chúng tôi muốn các câu lệnh được chọn không vượt quá ràng buộc độ dài (tức là 𝐿 token). Điều này có thể được công thức hóa như một bài toán 0-1 knapsack [33], trong đó các câu lệnh có thể được coi như các vật phẩm được thu thập vào một ba lô, với trọng số chú ý là các giá trị, và độ dài câu lệnh là trọng lượng. Ràng buộc độ dài có thể được coi như dung lượng của ba lô. Điều đáng chú ý là chúng tôi mở rộng dung lượng bằng độ dài tối đa của các câu lệnh để dung sai cho việc chọn thêm một

--- TRANG 6 ---
ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore Zhang, et al.
Thuật toán 1 Thuật toán Cắt tỉa Dựa trên Chú ý
Yêu cầu:
1:C=𝑠1;...;𝑠|𝐶|: một đoạn code đầu vào,
2:A𝑇: từ điển chú ý của token,
3:A𝑆: từ điển chú ý của câu lệnh,
4:L: độ dài mục tiêu
Đảm bảo:
5:Đoạn code được giảm C𝑝=𝑠′1;...;𝑠′|𝐶𝑝|
6:fors∈Cdo
7: a(s) =A𝑆(s) ⊲lấy sự chú ý câu lệnh
8: fort∈sdo
9: a(t) =A𝑇(t) ⊲lấy sự chú ý token
10: end for
11:end for
12:𝐶0←0-1 Knapsack (items={s}𝑠∈𝐶, values={𝑎(𝑠)}𝑠∈𝐶,
weights= {|s|} 𝑠∈𝐶, capacity = 𝐿+max𝑠∈𝐶(|s|) ⊲các câu lệnh ứng viên
13:fori = 1,...,Í𝑠∈𝐶0(|s|) - L do
14:𝑠′←𝑠′\{t} where 𝑠′=argmin𝑠∈𝐶0𝑎(𝑠),
𝑡=argmin𝑡0∈𝑠′𝑎(𝑡0)
15:end for
16:𝐶𝑝←𝑠′1;...;𝑠′|𝐶0|where𝐶0={𝑠′1,𝑠′2,...,𝑠′|𝐶0|}⊲nối các câu lệnh
câu lệnh nữa để thuật toán có thể thực hiện thêm cắt tỉa token trong giai đoạn tiếp theo.
Trong giai đoạn cắt tỉa token, chúng tôi loại bỏ tham lam các token có trọng số chú ý thấp nhất trong các câu lệnh có trọng số thấp nhất một cách lặp đi lặp lại cho đến khi thỏa mãn số lượng token tối đa.
Một vấn đề tiềm ẩn là các thang đo khác nhau của sự chú ý câu lệnh và kích thước token. Chúng tôi thấy rằng một số chuỗi ngắn hơn có nhiều khả năng được chọn trong các thử nghiệm của chúng tôi ngay cả khi chúng nhận được trọng số chú ý thấp. Điều này có lẽ là do tỷ lệ giá trị (trọng số chú ý chia cho độ dài chuỗi) của một chuỗi ngắn hơn lớn hơn nhiều so với chuỗi dài hơn. Chúng tôi khuếch đại trọng số chú ý bằng cách sử dụng chuẩn hóa min-max và nhân với số lượng token trong câu lệnh, tức là,
𝑎(𝑠)=𝑎(𝑠)−min𝑠∈𝑆(𝑎(𝑠))/max𝑠∈𝑆(𝑎(𝑠))−min𝑠∈𝑆(𝑎(𝑠))×𝑁 (6)
trong đó 𝑁 biểu thị số lượng token trong câu lệnh.
Độ phức tạp thời gian của thuật toán đơn giản hóa là O(𝑁·(𝐿𝑇+𝐿𝑀)), trong đó 𝐿𝑇 biểu thị số lượng token mục tiêu, và 𝐿𝑀 biểu thị độ dài tối đa của các câu lệnh. Chi phí thời gian chủ yếu đến từ thuật toán 0-1 knapsack. Hơn nữa, thuật toán knapsack yêu cầu một mảng hai chiều cho lập trình động, đòi hỏi độ phức tạp không gian.
Hình 5 cho thấy một ví dụ về đơn giản hóa chương trình bởi DietCode. Code gốc (Hình 5(a)) nhằm đọc nội dung từ một tệp với tên tệp đã cho. Mục tiêu của chúng tôi là giảm code chỉ chứa 60 token mà không mất đáng kể ngữ nghĩa. Trong giai đoạn chọn câu lệnh (Hình 5(b)), thuật toán đơn giản hóa giải quyết một bài toán 0-1 knapsack bằng cách đặt dung lượng thành 78 (bao gồm độ dài mục tiêu 60 và độ dài câu lệnh dài nhất 18). Sau đó, DietCode chọn các câu lệnh thân như method signature
Bảng 2: Thống kê của các Tập dữ liệu
Corpus Training Test
CodeSearchNet (Java) 908,886 1,000,000
CodeSearchNet (Python) 824,343 1,000,000
và return, và loại bỏ các câu lệnh tầm thường như variable assignments và catch. Số lượng token trong code được giảm từ 118 xuống 77. Trong giai đoạn cắt tỉa token, DietCode tiếp tục loại bỏ các token như variable initialization (Hình 5(c)), và giảm số lượng token từ 77 xuống 60. Mặc dù giảm khoảng 50%, ngữ nghĩa chính của hàm này không bị phá hủy đáng kể. CodeBERT vẫn có thể hiểu hàm thông qua các câu lệnh chính như tên phương thức readFile, biến chính content, và hàm được gọi read. Mặc dù code còn lại không bao giờ có thể chạy được, thông tin quan trọng được chọn có thể được sử dụng cho các tác vụ downstream như code search và summarization.
5 ĐÁNH GIÁ
Phần này giới thiệu việc đánh giá DietCode trong hai tác vụ downstream. Chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau thông qua các thử nghiệm:
•RQ3: DietCode hiệu quả như thế nào trong việc đơn giản hóa chương trình? Chúng tôi đánh giá hiệu suất và chi phí tính toán của DietCode trong hai tác vụ downstream và so sánh nó với các mô hình baseline.
•RQ4: DietCode hiệu quả như thế nào dưới các độ dài tương đối khác nhau? Chúng tôi nghiên cứu ảnh hưởng của các độ dài tương đối khác nhau đến điểm đánh giá. Mục tiêu của chúng tôi là tìm độ dài tương đối dẫn đến sự cân bằng tốt nhất giữa hiệu suất mô hình và chi phí tính toán.
•RQ5: Ảnh hưởng của các chiến lược cắt tỉa khác nhau là gì? DietCode dựa trên chú ý thực hiện cả cắt tỉa câu lệnh và token. Chúng tôi thực hiện một nghiên cứu ablation để xác định ảnh hưởng của việc cắt tỉa từng cái.
5.1 Các Tác vụ Downstream
Chúng tôi kiểm tra thuật toán của mình trong hai tác vụ downstream, cụ thể là code search và code summarization. Đây là những tác vụ kỹ thuật phần mềm được sử dụng rộng rãi nhất để chứng minh khả năng hiểu NL-PL [17, 22, 23, 31, 41, 46].
Code Search. Code search là một testbed điển hình cho các mô hình code được tiền huấn luyện như CodeBERT [15]. Tác vụ nhằm tìm các đoạn code từ một codebase có liên quan nhất đến một truy vấn đã cho.
Code Summarization. Code summarization nhằm tạo ra một tóm tắt ngôn ngữ tự nhiên cho một đoạn code đầu vào. Nó cũng đã được sử dụng rộng rãi để đánh giá các mô hình được tiền huấn luyện cho mã nguồn [15,45]. Thông qua tác vụ này, chúng tôi muốn xác minh liệu việc đơn giản hóa chương trình bởi DietCode có hiệu quả trong các tác vụ tạo sinh hay không.
5.2 Tập dữ liệu
Chúng tôi sử dụng CodeSearchNet để fine-tune và kiểm tra mô hình của chúng tôi [20], chứa các cặp <comment, code> được thu thập từ các dự án mã nguồn mở. code có nghĩa là đoạn code của một phương thức, trong khi comment

--- TRANG 7 ---
Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore
1publicStringreadFile(Stringfilename) {2Stringcontent=null;3File file =newFile(filename);4FileReaderreader =null;5try{6reader=newFileReader(file);7char[] chars=newchar[(int)file.length()];8reader.read(chars);9content=newString(chars);10reader.close();11}catch(IOExceptione) {12e.printStackTrace();13}finally{14if(reader!=null){15reader.close();16}17}18returncontent;19}
(a) Mã nguồn gốc
1publicStringreadFile(Stringfilename){2Stringcontent=null;3File file =newFile(filename);4FileReaderreader =null;5try{6reader=newFileReader(file);7reader.read(chars);8content=newString(chars);9reader.close();10} catch (IOExceptione) {11}finally{}12returncontent;13} (b) Code sau khi chọn câu lệnh
1publicStringreadFile(Stringfilename){2File file =newFile(filename);3FileReader4try{5reader=newFileReader(file);6reader.read(chars);7content=newString(chars);8reader.close();9}finally{} 10returncontent;11} (c) Code sau khi cắt tỉa token
Hình 5: Một ví dụ về đơn giản hóa chương trình bởi DietCode (attention).
là mô tả của code chủ yếu được thu thập từ các comment cho toàn bộ hàm như Javadocs hoặc docstrings trong Python. Thống kê của CodeSearchNet được hiển thị trong Bảng 2.
Dữ liệu gốc của CodeSearchNet ở định dạng chuỗi token. Chúng tôi tiền xử lý dữ liệu bằng cách phân tích chúng thành các câu lệnh. Chúng tôi phân tích các câu lệnh bằng cách chia dấu ngoặc và dấu chấm phẩy theo hướng dẫn chung của Java [16]. Đối với code Python, không có dấu phân cách vì thiếu thụt lề trong tập dữ liệu. Bên cạnh đó, Python không sử dụng dấu ngoặc và dấu chấm phẩy. Do đó, chúng tôi chia các câu lệnh bằng các ký hiệu gợi ý khác như "def", "=", ":", và ")".
Để giảm kích thước từ vựng của các token code, chúng tôi delexicalize tất cả các hằng số chuỗi thành một token chung string và tất cả các hằng số số thành cùng một token 10. Chúng tôi bảo tồn các số đặc biệt như 0, 1, và -1, có thể thuộc về các giá trị boolean.
5.3 Các Metric Đánh giá
Đầu tiên, chúng tôi muốn đo lường mức độ giảm code tối đa mà DietCode có thể đạt được mà không mất nhiều độ chính xác. Chúng tôi định nghĩa Relative Length (RL) để đo lường bao nhiêu code còn lại sau khi đơn giản hóa. Nó được tính như tỷ lệ phần trăm của độ dài của đoạn code được đơn giản hóa |𝐶𝑝| so với độ dài của code gốc |𝐶|:
𝑅𝐿=|𝐶𝑝|/|𝐶|×100% (7)
trong đó |·| biểu thị độ dài của một đoạn code (tức là số lượng token). Độ dài tương đối càng nhỏ, việc đơn giản hóa code gốc càng lớn.
Chúng tôi cũng sử dụng FLOPs (floating point operations) [19] để đo lường hiệu quả của việc giảm mô hình. FLOPs là một metric được sử dụng rộng rãi để đo lường độ phức tạp của một mô hình machine learning. FLOPs càng cao, việc xử lý của mô hình càng chậm.
Metric thứ ba là chi phí thời gian, bao gồm thời gian fine-tuning (FT Time) và thời gian testing. Chúng tôi tính toán thời gian thực thi của DietCode cho fine-tuning và testing, được đo bằng số giờ từ khi chương trình bắt đầu đến khi kết thúc.
Bên cạnh các metric cho độ phức tạp, chúng tôi sử dụng hai metric tiêu chuẩn để đánh giá hiệu quả của DietCode trong hai tác vụ downstream tương ứng. Chúng tôi đo lường hiệu suất của code search bằng MRR (mean reciprocal rank), đề cập đến trung bình của nghịch đảo nhân của vị trí cho câu trả lời đúng đầu tiên cho truy vấn [20].
Chúng tôi đo lường hiệu suất của code summarization bằng điểm BLEU-4 (bilingual evaluation understudy) [27], tính toán trung bình của độ chính xác n-gram trên một cặp chuỗi với một hình phạt cho các chuỗi ngắn.
5.4 Thiết lập Thực nghiệm
Để chứng minh sức mạnh của phương pháp của chúng tôi, chúng tôi đơn giản hóa các chương trình đầu vào cho các mô hình dựa trên tiền huấn luyện và so sánh hiệu suất với mô hình được tiền huấn luyện gốc. Cụ thể, chúng tôi so sánh kỹ thuật của mình với các phiên bản vanilla của ba mô hình được tiền huấn luyện:
•CodeBERT [15]: CodeBERT gốc không có đơn giản hóa code. Chúng tôi tuân theo thiết lập thực nghiệm trong bài báo CodeBERT.
•CodeT5 [45]: một mô hình ngôn ngữ lập trình được tiền huấn luyện được sử dụng rộng rãi dựa trên kiến trúc sequence-to-sequence. CodeT5 đã chứng minh hiệu suất tốt hơn trên các tác vụ tạo sinh [45].
•RoBERTa [28]: một phần mở rộng phổ biến của BERT đã chứng minh cải thiện đáng kể. RoBERTa gốc được tiền huấn luyện bằng ngôn ngữ tự nhiên. Vì vậy chúng tôi cũng báo cáo kết quả của RoBERTa (code) [15], một biến thể được tiền huấn luyện trên mã nguồn.
Bên cạnh các mô hình được tiền huấn luyện, chúng tôi cũng so sánh độ chính xác của DietCode với các phương pháp deep learning truyền thống, bao gồm BiRNN [10], SelfAttn [43], Seq2Seq [10], và Transformer [43]. Vì chúng cũng là các mô hình baseline cho CodeBERT, chúng tôi trực tiếp lấy kết quả từ bài báo CodeBERT [15].
Cuối cùng, chúng tôi so sánh DietCode với SIVAND [34], cũng là một phương pháp đơn giản hóa chương trình được đề xuất bởi Rabin et al. [34].

--- TRANG 8 ---
ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore Zhang, et al.
Bảng 3: Hiệu suất của các phương pháp đơn giản hóa code khác nhau trên tác vụ code search (𝐿in=độ dài đầu vào; RL=độ dài tương đối; FT=fine-tuning).
Java Python
Model 𝐿in RL FT Time Test Time FLOPs MRR 𝐋in RL FT Time Test Time FLOPs MRR
BiRNN [10] 200 100% 9.88h 1.13h 8.34G 0.29 200 100% 7.07h 1.25h 8.34G 0.32
SelfAttn [43] 200 100% 22.83h 4.00h 16.99G 0.59 200 100% 17.78h 2.67h 16.99G 0.69
RoBERTa [28] 200 100% 23.85h 4.78h 16.99G 0.67 200 100% 19.32h 3.53h 16.99G 0.81
RoBERTa (code) 200 100% 21.02h 4.53h 16.99G 0.72 200 100% 20.78h 4.07h 16.99G 0.84
CodeBERT [15] 200 100% 20.82h 3.17h 16.99G 0.74 200 100% 17.92h 2.82h 16.99G 0.84
DietCode
- Attention 120 60% 11.08h 1.83h 10.19G 0.71 120 60% 9.62h 1.82h 10.19G 0.81
- Dropout 120 60% 10.73h 1.95h 10.19G 0.68 120 60% 9.33h 1.93h 10.19G 0.80
- Frequency 120 60% 10.32h 1.8h 10.19G 0.66 120 60% 8.67h 1.79h 10.19G 0.78
CodeT5 [45] 200 100% 16.83h 2.97h 16.99G 0.72 200 100% 17.61h 2.83h 16.99G 0.837
DietCode
- Attention 120 60% 9.3h 1.74h 10.19G 0.71 120 60% 8.31h 1.81h 10.19G 0.813
- Dropout 120 60% 9.25h 1.67h 10.19G 0.68 120 60% 9.33h 1.93h 10.19G 0.799
- Frequency 120 60% 8.97h 1.58h 10.19G 0.66 120 60% 8.67h 1.79h 10.19G 0.785
Bảng 4: Hiệu suất của các phương pháp đơn giản hóa code khác nhau trên tác vụ code summarization.
Java Python
Model 𝐿in RL FT Time Test Time FLOPs BLEU-4 𝐿in RL FT Time Test Time FLOPs BLEU-4
Seq2Seq [10] 256 100% 6.26h 1.26h 12.750G 15.09 256 100% 4.43h 1.02h 12.750G 15.93
Transformer [43] 256 100% 13.28h 5.43h 24.328G 16.26 256 100% 9.70h 2.43h 24.328G 15.81
RoBERTa [28] 256 100% 15.42h 5.22h 24.328G 16.47 256 100% 10.63h 1.88h 24.328G 18.14
RoBERTa (code) 256 100% 14.5h 6.02h 24.328G 17.50 256 100% 10.45h 2.05h 24.328G 18.58
CodeBERT [15] 256 100% 13.82h 4.8h 24.328G 18.95 256 100% 8.32h 1.87h 24.328G 19.04
DietCode
- Attention 150 60% 8.18h 1.62h 15.325G 17.29 150 60% 5.35h 1.16h 15.325G 17.08
- Dropout 150 60% 7.95h 1.40h 15.325G 15.63 150 60% 6.81h 1.15h 15.325G 16.04
- Frequency 150 60% 8.62h 1.33h 15.325G 16.13 150 60% 6.12h 1.42h 15.325G 16.55
CodeT5 [45] 256 100% 16.91h 4.88h 54.01G 20.46 256 100% 9.68h 1.48h 54.01G 20.37
DietCode
- Attention 150 60% 10.25h 1.63h 37.81G 19.25 150 60% 6.57h 0.90h 37.81G 18.53
- Dropout 150 60% 9.95h 1.68h 37.81G 17.65 150 60% 6.67h 0.82h 37.81G 17.07
- Frequency 150 60% 10.23h 1.35h 37.81G 18.74 150 60% 6.33h 0.85h 37.81G 17.77
SIVAND¹ là một thuật toán dựa trên machine-learning đệ quy chia mã nguồn thành các đoạn và đo lường tầm quan trọng của nó thông qua tác vụ dự đoán tên phương thức. Thuật toán chọn các đoạn quan trọng làm code được giảm. Trong các thử nghiệm của chúng tôi, chúng tôi trực tiếp sử dụng code được cung cấp bởi các tác giả.
Chúng tôi triển khai mô hình của mình dựa trên repository GitHub của CodeBERT² và CodeT5³ sử dụng các thiết lập siêu tham số mặc định. Tất cả các mô hình được tối ưu hóa với thuật toán Adam [26] với tốc độ học lần lượt là 1𝑒-5 và 5𝑒-5 cho các tác vụ code search và code summarization.
Chúng tôi chạy tất cả các mô hình trên một máy với CPU Intel(R) Xeon(R) Silver 4214R 2.40GHz và GPU Nvidia Tesla P40.
¹https://github.com/mdrafiqulrabin/SIVAND
²https://github.com/microsoft/CodeBERT
³https://github.com/salesforce/CodeT5

5.5 Kết Quả Thực Nghiệm (RQ3)
Bảng 3 và 4 cho thấy hiệu suất của DietCode trong hai tác vụ downstream. Nhìn chung, DietCode giảm chi phí tính toán khoảng 40% trong khi duy trì độ chính xác tương đương với các mô hình được tiền huấn luyện gốc. CodeBERT gốc mất gần một ngày để fine-tuning và hơn 3 giờ cho code search trên tập test Java để đạt được điểm MRR là 0.74. Ngược lại, bằng cách giảm độ dài của code đầu vào xuống 60%, thời gian fine-tuning và testing có thể được giảm xuống tương ứng 11.08 giờ và 1.83 giờ. FLOPs cũng giảm từ 16.99G xuống 10.19G. Đồng thời, độ chính xác (MRR = 0.71) tương đương với CodeBERT gốc (MRR=0.74).
Điều đáng chú ý là mặc dù hiệu suất giảm nhẹ, nó vẫn vượt trội đáng kể so với các phương pháp truyền thống không dựa trên pretraining như BiRNN (MRR=0.29) và SelfAttn (MRR=0.59).
DietCode trên CodeT5 chứng minh hiệu quả tương tự. CodeT5 vanilla yêu cầu hơn 16 giờ để fine-tuning và khoảng 3 giờ để testing trên tác vụ code search Java. Bằng cách loại bỏ 40% token đầu vào sử dụng DietCode, thời gian fine-tuning và test được giảm xuống khoảng 9 và 1.6 giờ, tương ứng. Đồng thời, DietCode (attention) đạt được hiệu suất tương đương (MRR=0.71) với CodeT5 gốc (MRR=0.72) chỉ với sự giảm 1.5%.
DietCode cũng hiệu quả trong tác vụ code summarization. Lấy Java làm ví dụ, CodeBERT vanilla đạt được điểm BLEU-4 là 18.95 với chi phí 13.82 giờ fine-tuning và 4.8 giờ testing. Khi độ dài mục tiêu được đặt thành 150 token (tức là 60% độ dài gốc), thời gian fine-tuning được giảm xuống khoảng 60% và FLOPs giảm từ 24.328G xuống chỉ 15.325G. Điều này không hy sinh độ chính xác (BLEU-4=17.29).
So sánh ba chiến lược cắt tỉa, DietCode với attention cho thấy sức mạnh hơn hai chiến lược khác. Ví dụ, DietCode với attention-based pruning đạt được MRR là 0.71 trong tác vụ code search Java, tốt hơn dropout (MRR=0.68) và frequency filtering (MRR=0.66). Kết quả cho thấy rằng bằng cách cắt tỉa chi tiết hơn các token dựa trên trọng số chú ý, DietCode có thể đạt được việc đơn giản hóa chương trình hiệu quả hơn. DietCode trên CodeT5 chứng minh xu hướng tương tự trong cả hai tác vụ, trong đó attention-based pruning đạt được hiệu suất tốt hơn (BLEU=19.25) so với dropout (BLEU=17.65) và frequency filtering (BLEU=18.74).
Chúng tôi quan sát rằng sức mạnh này trở nên ít đáng kể hơn trong ngôn ngữ Python. Chúng tôi đoán rằng ngôn ngữ Python chứa ít token phụ trợ hơn (như dấu ngoặc), dẫn đến phân bố đồng đều hơn của trọng số chú ý trên tất cả các token. Do đó, việc loại bỏ các token có sự chú ý nhỏ hơn gần giống với việc loại bỏ các token ngẫu nhiên hoặc thường xuyên trong ngôn ngữ Python.
Là một điểm đáng chú ý, việc kiểm tra mô hình baseline SIVAND trong các tập dữ liệu của chúng tôi là không thể áp dụng, vì nó mất một lượng thời gian khổng lồ (>30,000 giờ theo ước tính của chúng tôi) để xử lý hơn 1 triệu hàm trong các benchmark CodeSearchNet. Điều này chủ yếu là do thuật toán ddmin được sử dụng bởi SIVAND dựa trên backtracking, trong đó mỗi bước thuật toán phải gọi mô hình deep learning cho một tác vụ dự đoán. Vì lý do này, chúng tôi ước tính chi phí thời gian của SIVAND bằng cách lấy mẫu 50 hàm từ CodeSearchNet. Kết quả cho thấy nó tốn 104 và 77 phút tương ứng trong các tập dữ liệu CodeSearchNet (Java) và CodeSearchNet (Python). Dựa trên quan sát này, chúng tôi ước tính thời gian testing trong toàn bộ tập dữ liệu. Vì dữ liệu chúng tôi lấy mẫu có kích thước nhỏ, nó không đủ để fine-tune mô hình để xác thực độ chính xác.
5.6 Ảnh Hưởng của Độ Dài Tương Đối (RQ4)
Hình 6 cho thấy hiệu suất của DietCode dưới các độ dài tương đối và chiến lược cắt tỉa khác nhau. Chúng ta có thể thấy rằng hiệu suất của tất cả các chiến lược cắt tỉa giảm mạnh khi độ dài tương đối giảm. Trong cả hai tác vụ, DietCode với attention hoạt động tốt hơn hai chiến lược cắt tỉa khác dưới tất cả các độ dài tương đối. Khi độ dài tương đối trên 70%, hiệu suất tương đương với CodeBERT gốc nhưng việc giảm chi phí tính toán có thể khiêm tốn. Khi độ dài tương đối giảm xuống dưới 50%, hiệu suất kém vì hầu hết code đã bị cắt tỉa. Độ dài tương đối khoảng 60% có lẽ là sự cân bằng tốt nhất cho hai tác vụ này.

--- TRANG 9 ---
Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore
40 50 60 70
Relative Length(%)0.550.600.650.70MRR
attention
random
frequency(a) kết quả trên code search
40 50 60 70
Relative Length(%)1415161718BLEU4
attention
random
frequency (b) kết quả trên code summarization
Hình 6: Hiệu suất dưới các độ dài tương đối khác nhau trên Java.
50 60 70 80 90
Relative Length(%)0.580.600.620.640.660.680.700.720.74MRR
statement
token
(a) kết quả trên code search
50 60 70 80 90
Relative Length(%)15.015.516.016.517.017.518.018.5BLEU4
statement
token (b) kết quả trên code summarization
Hình 7: Kết quả ablation của việc cắt tỉa token và câu lệnh trên Java.
5.7 Ảnh Hưởng của Việc Cắt Tỉa Token và Câu Lệnh (RQ5)
Hình 7 cho thấy hiệu suất của DietCode với việc cắt tỉa token và câu lệnh trên hai tác vụ. Để xác minh ảnh hưởng của việc cắt tỉa câu lệnh, chúng tôi chỉ chạy giai đoạn chọn câu lệnh trong Thuật toán 1. Để kiểm tra ảnh hưởng của việc cắt tỉa token, chúng tôi loại bỏ trực tiếp các token có sự chú ý thấp nhất cho đến khi đáp ứng độ dài mục tiêu. Như chúng ta có thể thấy từ kết quả, một sự giảm đáng kể trong hiệu suất được quan sát cho cả hai chiến lược khi độ dài tương đối giảm. Khi 90% code còn lại, điểm MRR của hai chiến lược cắt tỉa gần nhau. Khi độ dài tương đối giảm, điểm của chiến lược cắt tỉa token giảm nhanh hơn nhiều so với chiến lược cắt tỉa câu lệnh. Khi độ dài tương đối giảm thêm, điểm MRR của chúng giảm mạnh. Kết quả cho thấy rằng các câu lệnh quan trọng hơn các token trong việc học biểu diễn code bởi CodeBERT. Một lý do có thể là sự chú ý token thường đo lường thông tin cục bộ, trong khi sự chú ý câu lệnh xem xét mối quan hệ giữa các token. Do đó, sự chú ý câu lệnh có thể giữ nhiều ngữ nghĩa hơn của các hàm. Tác vụ code summarization chia sẻ kết quả tương tự như tác vụ code search.
6 THẢO LUẬN
6.1 Các Mô Hình Được Tiền Huấn Luyện Có Thể Hiểu Code Được Đơn Giản Hóa Như Thế Nào?
Một câu hỏi có thể tranh luận là làm thế nào các mô hình được tiền huấn luyện có thể hiểu code được đơn giản hóa mà không thể biên dịch và chạy được. Mã nguồn bao gồm hai kênh thông tin: formal & natural [6,7]. Nó

--- TRANG 10 ---
ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore Zhang, et al.
có thể được coi là chương trình để máy tính thực thi hoặc như một ngôn ngữ tự nhiên cụ thể để con người giao tiếp [18]. Cái trước yêu cầu mã nguồn phải được biên dịch và chạy nghiêm ngặt, trong khi cái sau tập trung nhiều hơn vào việc truyền đạt thông tin cho con người. Chúng tôi tin rằng CodeBERT xem xét nhiều hơn về kênh tự nhiên của mã nguồn [7,18]. Do đó, nó chú ý nhiều hơn đến các từ khóa mà không nhất thiết phải biết chương trình thực sự hoạt động như thế nào trong quá trình thực thi.
Các phát hiện thực nghiệm của chúng tôi phù hợp với một nghiên cứu con người trước đó về cách các lập trình viên tóm tắt mã nguồn [37], cho thấy rằng các nhà phát triển thường bắt đầu bằng việc đọc tên hàm. Họ có thể bỏ qua một số thông tin cấu trúc như điều kiện For và If trong khi chú ý đặc biệt đến thông tin theo nghĩa đen như gọi phương thức và tên biến, điều này theo nghĩa đen cho thấy ý định của code [37]. Các thuật toán cắt tỉa được mô tả trong bài báo của chúng tôi sử dụng phát hiện này và đơn giản hóa mã nguồn bằng cách chỉ chọn thông tin quan trọng (ví dụ, được phản ánh bởi sự chú ý của CodeBERT) trong mã nguồn. Do đó, chúng không có nhiều tác động đến việc hiểu mã nguồn.
6.2 Những người dùng nào có thể áp dụng các kỹ thuật?
Một câu hỏi có thể tranh luận là những người dùng nào có thể áp dụng kỹ thuật này vì nó có sự mất độ chính xác. Từ kết quả trong Bảng 3 và 4, mặc dù hiệu suất giảm nhẹ, nó vẫn vượt trội đáng kể so với các phương pháp truyền thống không dựa trên pretraining như BiRNN và SelfAttn. Đồng thời, DietCode cho thấy lợi thế về hiệu quả tính toán (ví dụ, tiết kiệm 40% thời gian fine-tuning). Do đó, DietCode cung cấp một lựa chọn thực tế cho những người dùng có tài nguyên tính toán hạn chế nhưng vẫn muốn tận dụng nhanh chóng các mô hình được tiền huấn luyện lớn.
6.3 Vai trò của công thức hóa 0-1 knapsack
Một người có thể đặt câu hỏi liệu chúng ta có thực sự cần chuyển đổi vấn đề thành 0-1 knapsack hay không. Trong các nghiên cứu sơ bộ của chúng tôi, việc đơn giản chỉ cắt tỉa các câu lệnh thường dẫn đến các chuỗi quá dài hoặc quá ngắn. Việc cắt tỉa token cung cấp một cắt tỉa chi tiết hơn cho chương trình đầu vào. Điều này tương đương với việc chọn các câu lệnh và token để có được sự chú ý tối đa trong khi thỏa mãn hạn chế độ dài. Do đó, chúng tôi công thức hóa nó như một bài toán 0-1 knapsack và giải quyết nó bằng chiến lược tham lam. Theo kết quả thực nghiệm trong Hình 7, một việc cắt tỉa câu lệnh đơn giản đạt được MRR khoảng 0.68 trên code search Java khi độ dài tương đối là 60%, rõ ràng thấp hơn so với DietCode (MRR=0.71). Cùng một so sánh có thể được quan sát trên tác vụ code summarization Java (tức là 16.8 so với 17.3 về BLEU). Điều này cho thấy hiệu quả của công thức hóa 0-1 knapsack.
6.4 Ý nghĩa cho Nghiên cứu Tương lai
Thông qua nghiên cứu thực nghiệm của chúng tôi, chúng tôi thấy rằng CodeBERT không nhận biết thông tin ngữ pháp và cấu trúc rất tốt. Ví dụ, điều kiện if, điều kiện for, và điều kiện while là ba câu lệnh điển hình đại diện cho cấu trúc code, nhưng chúng hiếm khi được các mô hình được tiền huấn luyện xem xét khi học biểu diễn chương trình. Điều này thúc đẩy nghiên cứu tương lai về việc kết hợp tốt hơn các cấu trúc code vào các mô hình code được tiền huấn luyện, ví dụ, chuyển đổi các ký hiệu cấu trúc (ví dụ, dấu ngoặc }) thành phong cách bằng lời hơn (ví dụ, 'kết thúc của một câu lệnh for') trước khi đưa vào các mô hình được tiền huấn luyện.
Bên cạnh DietCode, có một lớp kỹ thuật rộng hơn, bao gồm nén mô hình [40], chưng cất mô hình [39], và giảm dữ liệu [48], nhằm giảm độ phức tạp tính toán của các mô hình được tiền huấn luyện lớn với chi phí là một sự giảm nhẹ trong hiệu suất. DietCode cung cấp một giải pháp thay thế dễ dàng bằng cách chỉ cắt tỉa các chương trình đầu vào. Chúng tôi tin rằng điều này thực tế hơn cho các nhà phát triển vì họ chỉ cần xử lý dữ liệu mà không cần sửa đổi mô hình. Trong tương lai, chúng ta có thể điều tra thêm các kỹ thuật như nén mô hình và chưng cất.
6.5 Các Mối Đe Dọa Tính Hợp Lệ
Các Mối Đe Dọa Nội Bộ. Khi tính toán trọng số chú ý cho các câu lệnh, chúng tôi gặp vấn đề out-of-vocabulary (OOV). Chúng tôi giải quyết vấn đề này bằng cách loại trừ các danh mục câu lệnh hiếm trong các thử nghiệm của chúng tôi. Tuy nhiên, một số danh mục câu lệnh vẫn có thể cần thiết mặc dù có số lượng thấp trong kho dữ liệu. Hơn nữa, dữ liệu thô được cung cấp bởi CodeSearchNet ở định dạng văn bản thuần túy. Chúng tôi phân tích các câu lệnh bằng cách sử dụng các quy tắc xử lý văn bản do chúng tôi thiết kế. Tuy nhiên, có thể có các mô hình không đều mà parser của chúng tôi không thể nhận biết. Điều này có thể gây ra nhiễu trong tập dữ liệu của chúng tôi và ảnh hưởng đến kết quả nghiên cứu của chúng tôi.
Các Mối Đe Dọa Ngoại Bộ. Chúng tôi chỉ thực hiện các thử nghiệm của mình trong Java và Python. Mặc dù các kết luận từ cả hai ngôn ngữ đều tương tự, các ngôn ngữ lập trình khác như LISP và Erlang có thể có các mô hình chú ý khác nhau. Ngoài ra, DietCode được đánh giá trong hai tác vụ downstream: code search và code summarization. Tuy nhiên, các tác vụ này phụ thuộc nhiều vào thông tin theo nghĩa đen được cung cấp trong mã nguồn. Do đó, vẫn cần được xác minh liệu thuật toán đơn giản hóa được đề xuất có áp dụng được cho các tác vụ kỹ thuật phần mềm khác hay không.
7 CÔNG VIỆC LIÊN QUAN
7.1 Hiểu Các Mô Hình Được Tiền Huấn Luyện của Code
Bên cạnh công việc của chúng tôi, đã có các nghiên cứu khác cũng cố gắng giải thích các cơ chế của các mô hình được tiền huấn luyện cho code [1,30,32,38]. Karmakar và Robbes [24] áp dụng bốn tác vụ thăm dò trên các mô hình code được tiền huấn luyện để điều tra liệu các mô hình được tiền huấn luyện có thể học các khía cạnh khác nhau của mã nguồn như thông tin cú pháp, cấu trúc, bề mặt, và ngữ nghĩa hay không. Khác với công việc của chúng tôi, họ chỉ nghiên cứu thực nghiệm toàn bộ mô hình được tiền huấn luyện, trong khi chúng tôi tập trung vào kiến thức cụ thể hơn (các token và câu lệnh quan trọng) được học bởi các mô hình được tiền huấn luyện.
Cũng có nhiều công việc điều tra trọng số chú ý của các mô hình được tiền huấn luyện cho mã nguồn. Ví dụ, Wan et al. [44] thực hiện phân tích cấu trúc của các mô hình ngôn ngữ được tiền huấn luyện cho mã nguồn. Họ phân tích trọng số self-attention và thấy rằng sự chú ý Transformer có thể nắm bắt thông tin cấu trúc cấp cao của mã nguồn. AutoFocus [5] nhằm tiết lộ phần liên quan nhất của code cho các lập trình viên. Họ đo lường sự liên quan của các câu lệnh bằng cách sử dụng trọng số chú ý từ một GGNN [2]. Khác với phương pháp của chúng tôi, họ nắm bắt kiến thức cấu trúc được học bởi các mô hình được tiền huấn luyện, trong khi chúng tôi đào sâu hơn vào

--- TRANG 11 ---
Diet Code Is Healthy: Simplifying Programs for Pre-trained Models of Code ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore
các câu lệnh và token quan trọng được học bởi các mô hình được tiền huấn luyện. Hơn nữa, chúng tôi đề xuất đơn giản hóa chương trình cho các mô hình ngôn ngữ được tiền huấn luyện dựa trên các phát hiện.
7.2 Đơn Giản Hóa Chương Trình
Đơn giản hóa chương trình đã nhận được sự chú ý ngày càng tăng gần đây. Các phương pháp tiên tiến như SIVAND [34] và P2IM [42] dựa trên nguyên mẫu delta debugging [49]. Cơ chế delta debugging yêu cầu một đoạn code đầu vào và một mô hình deep learning phụ trợ như code2vec [3]. Mô hình deep learning nhận đoạn code làm đầu vào và chia nó thành các đoạn. Mỗi đoạn sau đó được đưa làm đầu vào cho mô hình mạng nơ-ron để thực hiện các tác vụ kiểm tra như dự đoán tên phương thức và phát hiện biến bị sử dụng sai. Nếu một đoạn có được điểm thỏa đáng, chương trình sẽ chia nó thêm. Quá trình tiếp tục cho đến khi hiệu suất của tập con không thỏa mãn điểm mục tiêu. Cuối cùng, thuật toán tạo ra đoạn code nhỏ nhất thỏa mãn mục tiêu của mô hình sâu.
So với DietCode, các phương pháp của họ không hiệu quả về mặt tính toán vì họ cần chạy một mô hình deep learning phụ trợ và đánh giá hiệu suất ở mỗi lần lặp. Ví dụ, SIVAND tốn hàng trăm giờ để xử lý 10,000 hàm, trong khi DietCode có thể hoàn thành trong vòng hai phút.
8 KẾT LUẬN
Bài báo này phân tích thực nghiệm thông tin quan trọng được học bởi CodeBERT, bao gồm các token và câu lệnh quan trọng trong cả Java và Python. Kết quả của chúng tôi cho thấy CodeBERT tập trung vào các từ khóa và các câu lệnh liên quan đến dữ liệu như chữ ký phương thức và câu lệnh return. Dựa trên các phát hiện thực nghiệm của chúng tôi, chúng tôi đề xuất một phương pháp mới có tên DietCode để tận dụng nhẹ nhàng các mô hình được tiền huấn luyện. DietCode đơn giản hóa code đầu vào thành độ dài mục tiêu bằng cách chọn các câu lệnh và token quan trọng dựa trên trọng số chú ý của chúng bằng thuật toán 0-1 Knapsack. Các thử nghiệm trên hai tác vụ đã cho thấy DietCode cung cấp kết quả tương đương như CodeBERT, với lợi thế về hiệu quả tính toán.
Trong tương lai, chúng tôi sẽ xem xét thêm các khía cạnh mà CodeBERT học về code, như quy tắc cú pháp và quan hệ ngữ nghĩa, để giảm thêm kích thước của mã nguồn cho các mô hình được tiền huấn luyện. Chúng tôi cũng sẽ điều tra các kỹ thuật nén mô hình [8,9] và chưng cất [39] để giảm thêm kích thước của các mô hình ngôn ngữ lập trình được tiền huấn luyện.
Mã nguồn và dữ liệu thực nghiệm của chúng tôi được công khai tại https://github.com/zhangzwwww/DietCode
9 CẢM ƠN
Công việc này được tài trợ bởi NSFC No. 62102244, CCF-Tencent Open Research Fund (RAGR20220129), và CCF-Baidu Open Fund (NO. 2021PP15002000).
TÀI LIỆU THAM KHẢO
[1]Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2655–2668.
[2]Miltiadis Allamanis, Marc Brockschmidt, và Mahmoud Khademi. 2018. Learning to Represent Programs with Graphs. In International Conference on Learning Representations.
[3]Uri Alon, Meital Zilberstein, Omer Levy, và Eran Yahav. 2019. code2vec: learning distributed representations of code. Proceedings of the ACM on Programming Languages 3, POPL (2019), 1–29.
[4]Dzmitry Bahdanau, Kyung Hyun Cho, và Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations (ICLR).
[5]Nghi DQ Bui, Yijun Yu, và Lingxiao Jiang. 2019. Autofocus: interpreting attention-based neural networks by code perturbation. In Proceedings of 34th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 38–41.
[6]Casey Casalnuovo, E Morgan, và P Devanbu. 2020. Does surprisal predict code comprehension difficulty. In Proceedings of the 42nd Annual Meeting of the Cognitive Science Society. Cognitive Science Society Toronto, Canada.
[7]Saikat Chakraborty, Toufique Ahmed, Yangruibo Ding, Premkumar Devanbu, và Baishakhi Ray. 2020. NatGen: Generative pre-training by" Naturalizing" source code. In Proceedings of the 30th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE).
[8]Yu Cheng, Duo Wang, Pan Zhou, và Tao Zhang. 2017. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282 (2017).
[9]Yu Cheng, Duo Wang, Pan Zhou, và Tao Zhang. 2018. Model compression and acceleration for deep neural networks: The principles, progress, and challenges. IEEE Signal Processing Magazine 35, 1 (2018), 126–136.
[10] Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, và Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).
[11] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys Poshyvanyk, Massimiliano Di Penta, và Gabriele Bavota. 2021. An empirical study on the usage of BERT models for code completion. In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 108–119.
[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, và Christopher D Manning. 2019. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. In International Conference on Learning Representations.
[13] Alexis Conneau và Guillaume Lample. 2019. Cross-lingual language model pretraining. Advances in neural information processing systems 32 (2019).
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171–4186.
[15] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: a pre-Trained model for programming and natural languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP): Findings. 1536–1547.
[16] James Gosling, Bill Joy, Guy Steele, và Gilad Bracha. 2000. The Java language specification. Addison-Wesley Professional.
[17] Sonia Haiduc, Jairo Aponte, và Andrian Marcus. 2010. Supporting program comprehension with source code summarization. In Proceedings of ACM/IEEE 32nd international conference on software engineering (ICSE), Vol. 2. IEEE, 223–226.
[18] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, và Premkumar Devanbu. 2012. On the naturalness of software. In 2012 34th International Conference on Software Engineering (ICSE). IEEE, 837–847.
[19] Raphael Hunger. 2005. Floating point operations in matrix-vector calculus. Munich University of Technology, Inst. for Circuit Theory and Signal.
[20] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. 2019. Codesearchnet challenge: evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).
[21] Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, và Hal Daumé III. 2015. Deep unordered composition rivals syntactic methods for text classification. In Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers). 1681–1691.
[22] Siyuan Jiang, Ameer Armaly, và Collin McMillan. 2017. Automatically generating commit messages from diffs using neural machine translation. In Proceedings of 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 135–146.
[23] Toshihiro Kamiya, Shinji Kusumoto, và Katsuro Inoue. 2002. CCFinder: A multilinguistic token-based code clone detection system for large scale source code. IEEE Transactions on Software Engineering (TSE) 28, 7 (2002), 654–670.
[24] Anjan Karmakar và Romain Robbes. 2021. What do pre-trained code models know about code?. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1332–1336.
[25] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, và Kurt Keutzer. 2021. Learned token pruning for transformers. arXiv preprint arXiv:2107.00910 (2021).

--- TRANG 12 ---
ESEC/FSE '22, November 14–18, 2022, Singapore, Singapore Zhang, et al.
[26] Diederik P. Kingma và Jimmy Ba. 2017. Adam: a method for stochastic optimization. arXiv:1412.6980 [cs.LG]
[27] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871–7880.
[28] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: a robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
[29] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).
[30] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, và Gabriele Bavota. 2021. Studying the usage of text-to-text transfer transformer to support code-related tasks. In Proceedings of IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 336–347.
[31] Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, và Xiaochen Li. 2016. Query expansion based on crowd knowledge for code search. IEEE Transactions on Services Computing 9, 5 (2016), 771–783.
[32] Matteo Paltenghi và Michael Pradel. 2021. Thinking Like a Developer? Comparing the Attention of Humans with Neural Models of Code. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 867–879.
[33] David Pisinger. 1995. Algorithms for knapsack problems. (1995).
[34] Md Rafiqul Islam Rabin, Vincent J. Hellendoorn, và Mohammad Amin Alipour. 2021. Understanding neural code intelligence through program simplification. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). Association for Computing Machinery, 441–452.
[35] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.
[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research 21 (2020), 1–67.
[37] Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel Bosch, và Sidney D'Mello. 2014. Improving automated source code summarization via an eye-tracking study of programmers. In Proceedings of the 36th international conference on Software engineering. 390–401.
[38] Anna Rogers, Olga Kovaleva, và Anna Rumshisky. 2020. A primer in bertology: what we know about how BERT works. Transactions of the Association for Computational Linguistics 8 (2020), 842–866.
[39] Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).
[40] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, và Kurt Keutzer. 2020. Q-BERT: Hessian based ultra low precision quantization of BERT. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8815–8821.
[41] Kathryn T Stolee, Sebastian Elbaum, và Daniel Dobos. 2014. Solving the search for source code. ACM Transactions on Software Engineering and Methodology (TOSEM) 23, 3 (2014), 1–45.
[42] Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim A Laredo, và Alessandro Morari. 2021. Probing model signal-awareness via prediction-preserving input minimization. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 945–955.
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems (NeurlPS 2017). 5998–6008.
[44] Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu, và Hai Jin. 2022. What do they capture? – a structural analysis of pre-trained language models for source code. In In Proceedings of the 44th International Conference on Software Engineering (ICSE).
[45] Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 8696–8708.
[46] Martin White, Michele Tufano, Christopher Vendome, và Denys Poshyvanyk. 2016. Deep learning code fragments for code clone detection. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 87–98.
[47] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le. 2019. Xlnet: generalized autoregressive pretraining for language understanding. Advances in neural information processing systems 32 (2019).
[48] Deming Ye, Yankai Lin, Yufei Huang, và Maosong Sun. 2021. TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 5798–5809.
[49] Andreas Zeller và Ralf Hildebrandt. 2002. Simplifying and isolating failure-inducing input. IEEE Transactions on Software Engineering 28, 2 (2002), 183–200.
[50] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, và Xudong Liu. 2019. A novel neural source code representation based on abstract syntax tree. In Proceedings of IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 783–794.
