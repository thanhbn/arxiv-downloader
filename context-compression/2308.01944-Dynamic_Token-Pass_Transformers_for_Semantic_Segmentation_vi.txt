# Dynamic Token-Pass Transformers cho Phân đoạn Ngữ nghĩa
Yuang Liu1*, Qiang Zhou2, Jing Wang2, Zhibin Wang2, Fan Wang2, Jun Wang1, Wei Zhang1†
1Đại học Sư phạm Đông Trung Quốc 2Viện DAMO, Tập đoàn Alibaba
{frankliu624,zhangwei.thu2011,wongjun }@gmail.com
{jianchong.zq,yunfei.wj,zhibin.waz,fan.w }@alibaba-inc.com

## Tóm tắt
Vision transformers (ViT) thường trích xuất đặc trưng thông qua việc chuyển tiếp tất cả các token trong các lớp self-attention từ đầu đến cuối. Trong bài báo này, chúng tôi giới thiệu dynamic token-pass vision transformers (DoViT) cho phân đoạn ngữ nghĩa, có thể giảm thích ứng chi phí suy luận cho các hình ảnh có độ phức tạp khác nhau. DoViT dần dần dừng các token dễ một phần khỏi tính toán self-attention và giữ các token khó tiếp tục cho đến khi đáp ứng tiêu chí dừng. Chúng tôi sử dụng các auxiliary heads nhẹ để đưa ra quyết định token-pass và chia các token thành phần giữ lại/dừng. Với tính toán token riêng biệt, các lớp self-attention được tăng tốc với các token thưa thớt và vẫn hoạt động thân thiện với phần cứng. Một module tái tạo token được xây dựng để thu thập và đặt lại các token được nhóm về vị trí ban đầu trong chuỗi, điều này cần thiết để dự đoán các mask ngữ nghĩa chính xác. Chúng tôi tiến hành các thí nghiệm mở rộng trên hai tác vụ phân đoạn ngữ nghĩa phổ biến và chứng minh rằng phương pháp của chúng tôi giảm đáng kể khoảng 40% ∼60% FLOPs và việc giảm mIoU nằm trong khoảng 0.8% cho các transformer phân đoạn khác nhau. Throughput và tốc độ suy luận của ViT-L/B được tăng lên hơn 2× trên Cityscapes.

## 1. Giới thiệu
Phân đoạn ngữ nghĩa đã là một thành phần quan trọng của lái xe tự động [17], chỉnh sửa hình ảnh [41] và phân tích cảnh tượng trực quan [6]. Là một tác vụ dự đoán dày đặc, nó nhằm gán mỗi pixel hình ảnh cho một nhãn danh mục. Nhờ vào sự phát triển của các mạng neural sâu, đặc biệt là vision transformers (ViT) [9], nghiên cứu cho phân đoạn ngữ nghĩa đã đạt được thành công lớn với cái giá tính toán khổng lồ. Hơn nữa, segmentor kiểu transformer, ví dụ SETR [44], Segmenter [30] và Segformer [34], đã vượt qua CNN trên toàn diện và cho thấy tiềm năng lớn. Tuy nhiên, độ phức tạp tính toán của kiến trúc transformer làm cho ứng dụng thời gian thực của phân đoạn ngữ nghĩa thậm chí còn bị cấm hơn. Để làm cho các mô hình này phù hợp hơn với các thiết bị di động hạn chế tài nguyên, việc giảm chi phí tính toán và tăng tốc chúng là cấp thiết.

Những năm gần đây đã chứng kiến tiến bộ lớn trong nén và tăng tốc mô hình kiểu CNN được mang lại bởi các phương pháp nhận biết tham số. Như được hiển thị trong Hình 1a, phần lớn các phương pháp tăng tốc hiện tại có thể được chia thành ba danh mục, bao gồm pruning [18, 20], quantization [16,32] và knowledge distillation [13,15,33]. Tất cả chúng đều tập trung vào việc giảm các thành phần hoặc tham số dư thừa của mạng, vì vậy chúng tôi gọi chúng là các phương pháp tăng tốc nhận biết tham số. Gần đây đã có một làn sóng quan tâm để giới thiệu các phương pháp tăng tốc nhận biết tham số này vào các kiến trúc dựa trên transformer, cả trong xử lý ngôn ngữ tự nhiên (NLP) [21,29] và thị giác máy tính (CV) [22,39].

Cốt lõi của ViTs là module self-attention, về bản chất khác với phép toán convolution trong CNN. Nó hoạt động bằng cách tính toán các mối quan hệ giữa từng cặp patch hình ảnh hoặc token, và sau đó nắm bắt ngữ cảnh toàn cục của hình ảnh đầu vào. Hưởng lợi từ bản chất này của self-attention, Rao và cộng sự lần đầu tiên đề xuất DynamicViT [27] mà prune các token có tầm quan trọng ít và chỉ giữ các token một phần trong self-attention để tăng tốc. A-ViT [35] cải thiện DynamicViT [27] bằng cách giới thiệu phân phối dừng của các token và không yêu cầu tham số bổ sung. Gần đây, ATS [10] xây dựng một bộ lấy mẫu token thích ứng để tự động chọn các token quan trọng nhất. Các công trình tăng tốc nhận biết dữ liệu này chỉ ra một hướng mới cho tăng tốc mô hình. Nhưng chúng chỉ chú ý đến các tác vụ phân loại và không hỗ trợ các tác vụ dày đặc như phân đoạn ngữ nghĩa. Đối với ViT phân loại, thực tế chỉ có class token được sử dụng để dự đoán danh mục của toàn bộ hình ảnh, và hầu hết các token khác có thể bị loại bỏ ở các lớp nhất định. Khác biệt, mỗi token đều cần thiết cho phân đoạn ngữ nghĩa, và tất cả các token được yêu cầu được sử dụng bởi decoder để dự đoán các danh mục của pixel. Ngoài ra, các phương pháp trên huấn luyện ViT động với tỷ lệ pruning token cố định cho mỗi lớp/block, làm cho nó không thể đưa ra sự đánh đổi theo hình ảnh giữa độ phức tạp đầu vào và chi phí suy luận.

Để đạt được điều này, chúng tôi đề xuất một dynamic token-pass transformers (DoViT) mới cho phân đoạn ngữ nghĩa. Đây là nỗ lực đầu tiên của tăng tốc ViT nhận biết dữ liệu trên các tác vụ dự đoán dày đặc. Thay vì tập trung vào tính dư thừa của patch trong hình ảnh đầu vào, chúng tôi dựa trên độ phức tạp hoặc khó khăn học tập của các patch/token ngữ nghĩa để xác định thích ứng chi phí tính toán của chúng, điều này làm cho backbone đạt được suy luận động theo hình ảnh. Khó để tự động chọn các token thông tin hơn hoặc quan trọng hơn cho self-attention ở mỗi lớp. Bởi vì phân loại pixel có thể thất bại nếu bất kỳ token ngữ nghĩa nào không được học đầy đủ. Sơ đồ đáng tin cậy nhất là quyết định xem một token có nên được bảo tồn hay dừng một cách rõ ràng dựa trên kết quả dự đoán sớm. Do đó, chúng tôi giới thiệu một sơ đồ semantic early-probe chia các token thành hai danh mục, tức là tập giữ lại và tập dừng. Các token giữ lại sẽ tham gia vào các lớp self-attention tiếp theo trong khi các token dừng sẽ được ngăn khỏi self-attention và được chuyển trực tiếp đến decoder thông qua một đường dẫn ngắn. Việc giữ lại hoặc dừng của mỗi token được xác định bởi độ tin cậy dự đoán ngữ nghĩa được cung cấp bởi các auxiliary heads. Sơ đồ này làm cho có thể điều chỉnh chi phí suy luận hoàn toàn động cho các hình ảnh đầu vào khác nhau. Chúng tôi khẳng định rằng tất cả các token ngữ nghĩa phải được decoder nhận ở vị trí ban đầu của chúng sau một mức độ nhất định của các phép toán self-attention. Một module self-attention riêng biệt được trình bày để tối ưu hóa tính toán của tập token giữ lại/dừng trong đó các token dừng không mang lại tính toán. Để khôi phục thứ tự của các token đầu vào, chúng tôi giới thiệu một module tái tạo token cung cấp các bản đồ đặc trưng hoàn chỉnh và chính xác cho decoder và auxiliary heads. Phương pháp được đề xuất của chúng tôi cắt giảm đáng kể chi phí suy luận — FLOPs của SETR trên Cityscapes được giảm 40% ∼60% trong khoảng giảm mIoU 0.8%, và throughput và FPS được cải thiện lên hơn 2× trên phần cứng. Tóm lại, các đóng góp chính của chúng tôi như sau:

• Chúng tôi đề xuất một phương pháp dynamic token-pass để giảm chi phí suy luận của vision transformers cho phân đoạn ngữ nghĩa.
• Chúng tôi giới thiệu một sơ đồ semantic early-probe để xác định các ứng viên token-pass. Các module self-attention riêng biệt và tái tạo token chịu trách nhiệm cho việc tăng tốc token thưa thớt.
• Chúng tôi tiến hành các thí nghiệm mở rộng trên hai bộ dữ liệu phân đoạn công khai với các mô hình ViT khác nhau và chứng minh rằng phương pháp được đề xuất giảm FLOPs đáng kể với việc giảm nhỏ trong mIoU.

## 2. Công trình Liên quan

### 2.1. Phân đoạn Ngữ nghĩa
Sự phát triển nhanh chóng của mạng neural sâu đã truyền cảm hứng đáng kể cho việc khám phá phân đoạn hình ảnh ngữ nghĩa. Lúc đầu, FCN [23] đạt được phân đoạn hình ảnh theo pixel bằng cách loại bỏ lớp fully-connected cuối cùng. Vì FCN tập trung vào việc trích xuất các đặc trưng ngữ nghĩa trừu tượng, việc kết hợp đặc trưng đa tỷ lệ [1, 25, 28], dilated convolution [2, 3, 38], và spatial pyramid pooling [4, 5, 42] được đề xuất để khuyến khích mạng trích xuất các đặc trưng chi tiết hơn. Để tăng thêm độ chính xác của mạng, các cơ chế attention được giới thiệu vào mạng [11, 12, 36, 37, 40, 43]. Gần đây, xem xét hiệu suất xuất sắc của transformers, nhiều công trình cố gắng cắm nó vào phân đoạn ngữ nghĩa. SETR [44] lần đầu tiên áp dụng một mạng dựa trên transformer, ViT, làm encoder để trích xuất đặc trưng, nhưng giữ decoder dựa trên CNN. Segmenter [30] mở rộng thêm kiến trúc transformer cho decoder và thiết kế một kiến trúc phân đoạn encoder-decoder transformer thuần túy.

### 2.2. Tăng tốc Mô hình
Mặc dù transformer đã mang lại cải thiện lớn cho phân đoạn ngữ nghĩa, số lượng tương tác bậc hai giữa các token đã tăng gánh nặng tính toán. Để thúc đẩy việc triển khai mô hình dựa trên transformer trên các thiết bị edge, tăng tốc mô hình trở thành một chủ đề phổ biến. Các phương pháp tăng tốc mô hình nhận biết tham số truyền thống như knowledge distillation [13,15,33], quantization [16,32], và pruning [14, 18, 20] đã được giới thiệu vào transformer [21,22,24,29,39]. Xem xét chi phí tính toán mối quan hệ giữa các patch hình ảnh hoặc token trong transformer, tăng tốc mô hình nhận biết dữ liệu đáng được thảo luận. Một số công trình liên quan đã được nghiên cứu trong phân loại hình ảnh, ví dụ, DynamicViT [27] lần đầu tiên đề xuất prune các token không thông tin theo cách động bằng cách áp dụng một module dự đoán nhẹ. Sau đó A-ViT [35] cải thiện nó bằng cách loại bỏ module dự đoán và dừng tính toán của các token bằng một cơ chế suy luận thích ứng không tham số. EViT [19] giảm tính toán bằng cách dần loại bỏ hoặc kết hợp các token không chú ý trong Vision Transformers. Gần đây, ATS [10] xây dựng một bộ lấy mẫu token thích ứng để tự động chọn các token quan trọng nhất. Các phương pháp này thể hiện tiềm năng lớn trong tăng tốc transformer của các tác vụ phân loại, nhưng chúng không phù hợp cho phân đoạn ngữ nghĩa vì mỗi token đều có ý nghĩa cho dự đoán theo pixel.

## 3. Phương pháp Đề xuất

### 3.1. Tổng quan
Hình 2 minh họa pipeline của framework DoViT của chúng tôi. Xem xét một mạng phân đoạn giống transformer nhận một hình ảnh x∈R³×H×W làm đầu vào để dự đoán một mask ngữ nghĩa y∈RC×H×W (C, H, và W đại diện cho số danh mục, chiều cao, và chiều rộng tương ứng):

y = D ⊙ FL ⊙ FL-1 ⊙ F1 ⊙ E(x), (1)

trong đó E, F và D là mạng patch embedding, lớp backbone và decoder phân đoạn, tương ứng, L đại diện cho số lớp. Mạng E tokenize các patch hình ảnh từ x thành các token có vị trí T∈RN×E, trong đó N và E là số lượng và chiều của các token, tương ứng. Sau đó các token chuyển tiếp với các phép toán multi-head self-attention (MSA) trong các lớp backbone, chiếm phần lớn tính toán của toàn bộ mạng phân đoạn. Chúng tôi chọn các lớp nhất định và thực hiện một sơ đồ semantic early-probe để đưa ra quyết định token-pass rằng các token khó được giữ lại trong các lớp MSA sau đó trong khi các token dễ được dừng khỏi MSA và được chuyển trực tiếp đến decoder. Để thực hiện một forwarding token thưa thớt hiệu quả trong mạng backbone và decoder, chúng tôi giới thiệu một chiến lược self-attention riêng biệt và một module tái tạo token.

### 3.2. Quyết định Token-Pass
Để dần giảm các token trong backbone ViT, chúng tôi chọn D lớp self-attention không liền kề làm "lớp quyết định" chia toàn bộ L lớp backbone thành D + 1 block, tức là {B1, B2, ···, BD+1}. Số lớp trong block Bℓ là Bℓ. Khác với ViT phân loại, việc xác định tầm quan trọng của mỗi token ở các lớp nhất định là một thách thức. Bởi vì mỗi token chứa thông tin ngữ nghĩa cụ thể và đóng góp vào dự đoán phân đoạn, điều này chỉ ra rằng việc thô thiển loại bỏ các token không thông tin là không hợp lý. Chúng tôi khẳng định rằng việc đánh giá chính xác xem một token có được sử dụng đầy đủ và học được hay không là rất quan trọng. Một phương pháp đơn giản là đánh giá kết quả phân đoạn với các token sớm. Để đạt được điều này, chúng tôi đề xuất một sơ đồ early-probe để xác định xem một token có được giữ lại trong tính toán hay dừng học. Cụ thể, chúng tôi chèn một auxiliary segmentation head nhẹ Hℓ sau block Bℓ. Đáng chú ý rằng auxiliary head bao gồm một lớp fully-convolutional mang lại các tham số và tính toán bổ sung nhỏ, gần như không đáng kể so với toàn bộ mạng phân đoạn. Giả sử rằng không có quyết định token-pass nào được áp dụng, và mỗi block MSA hoạt động bình thường với tổng N token đầu vào và N token đầu ra. Chúng tôi reshape chuỗi token Tℓ được xuất ra bởi block Bℓ thành một bản đồ đặc trưng sâu f∈RE×h×w, trong đó E là cả chiều của các token và kênh của bản đồ đặc trưng, h và w đại diện cho chiều rộng và chiều cao của bản đồ đặc trưng, N = hw. Chúng tôi đưa bản đồ đặc trưng fℓ vào auxiliary head thứ ℓ Hℓ để có được một bản đồ xác suất pℓ∈RC×h×w với phép toán softmax:

pℓ = softmax(Hℓ(fℓ)). (2)

Scalar pc,i,j đại diện cho xác suất của token Ti,j thuộc về danh mục ngữ nghĩa thứ c, và giá trị tối đa qi,j của {pc,i,j}C c=1 đại diện cho độ tin cậy của dự đoán nhãn về Ti,j. Với những hiểu biết này, chúng tôi có thể phân bổ một độ tin cậy dự đoán cho mỗi token Ti,j và có được một bản đồ điểm

qℓ i,j = max{pℓ c,i,j|c ∈ {1,2,···, C}}. (3)

Nói chung, các token với độ tin cậy dự đoán cao là không thông tin và dễ phân đoạn, trong khi các token với độ tin cậy dự đoán thấp là phức tạp và khó học. Vì vậy chúng tôi có thể sử dụng điểm tin cậy để xác định việc chuyển của một token. Để căn chỉnh với hình dạng ban đầu của chuỗi token T∈RN×E, chúng tôi reshape bản đồ tin cậy qℓ∈Rh×w thành qℓ∈RN. Chúng tôi định nghĩa một mask quyết định nhị phân M̃ℓ∈{0,1}N để chỉ ra xem có giữ lại hay dừng mỗi token Tℓ n tại block thứ ℓ,

M̃ℓ n = { 0, nếu qℓ n > ξ; 1, ngược lại, (4)

trong đó 0 ≤ ξ ≤ 1 là một tham số ngưỡng. Số lượng token giữ lại thưa thớt được giảm dần theo block. Nhưng mask quyết định M̃ℓ được tính toán dựa trên giả thuyết rằng head Hℓ nhận một bản đồ đặc trưng hoàn chỉnh mà không có việc giảm token. Vì vậy chúng tôi cần bỏ qua các token đã bị dừng ở các block trước đó bằng cách cập nhật M̃ℓ với

Mℓ = M̃ℓ ⊙ Mℓ-1 = ∏ℓ i=0 M̃i, (5)

trong đó ⊙ là tích Hadamard, và Mℓ-1 là mask quyết định thực từ block cuối, M0 = M̃0 = 1.

Bằng cách này, chúng tôi đã chọn các token giữ lại ở mỗi block, chúng sẽ tham gia vào các block MSA cho đến khi đáp ứng tiêu chí dừng. Với sơ đồ early-probe, token-pass là thích ứng động về độ phức tạp patch, thay vì phụ thuộc vào tỷ lệ giữ lại/dừng cố định ở mỗi giai đoạn. Cần thiết phải thu thập các token dừng để xây dựng một bản đồ đặc trưng hoàn chỉnh cho auxiliary heads và decoder phân đoạn. Để đạt được self-attention hiệu quả với các token giữ lại/dừng, chúng tôi thiết kế một thuật toán forwarding token riêng biệt.

### 3.3. Sparse Token Forwarding
Với các mask quyết định {M̃ℓ}D ℓ, một chuỗi token thưa thớt có thể được trích xuất từ các block. [27] và [35] đều thực hiện phép toán self-attention với các token thưa thớt thông qua một cơ chế mask, trong đó các token thoát/dừng vẫn tham gia vào tính toán query, key và value. Để đạt được việc giảm token thực sự, họ thiết kế các giai đoạn huấn luyện và suy luận khác nhau trong đó các token dừng không nhất quán. Các token không nhất quán đầu vào vào decoder có thể tạo ra bias giữa các giai đoạn huấn luyện và suy luận. Để đạt được điều này, chúng tôi đề xuất một module self-attention riêng biệt đơn giản nhưng hiệu quả giữ tính nhất quán của dynamic token-pass trong hai giai đoạn.

**Separate Self-Attention.** Các token giữ lại và dừng được xử lý riêng biệt trong mỗi block, ngoại trừ block đầu tiên B1 trong đó không có việc giảm token nào được thực hiện. Để thu thập các token thưa thớt thành một biểu diễn compact và có cấu trúc, chúng tôi định nghĩa một hàm G(T, M) chọn các token từ T∈RN×E với mask M và kết hợp chúng thành một chuỗi token mới với kích thước |M| × E, trong đó |M| là số lượng phần tử khác không. Chuỗi token giữ lại/dừng T̂ℓ/T̈ℓ đến block thứ ℓ có thể được thu được bằng

T̂ℓ = G(Tℓ, Mℓ), T̈ℓ = G(Tℓ, (1-Mℓ)). (6)

Bằng cách này, các token giữ lại/dừng được xuất ra bởi block Bℓ đã được chia thành hai phần. Trong suy luận dynamic token-pass, chỉ các token giữ lại được xem xét trong các module MSA của block tiếp theo, tức là Bℓ+1, có thể được công thức hóa như

MSA(Q̂, K̂, V̂) = softmax(Q̂K̂⊤/√d)V̂, (7)

trong đó Q̂, K̂, V̂∈RN×d là các embedding query, key và value của các token giữ lại T̂, d là chiều của các embedding. Sau đó, độ phức tạp tính toán của self-attention được giảm từ O(N²·d) thành O(|M|²·d), |M| ≤ N. Các token dừng T̈ℓ sẽ chuyển trực tiếp qua block tiếp theo Bℓ+1 mà không có tính toán nào. Sau separate self-attention trong block Bℓ+1, các token giữ lại và dừng có thể được kết hợp như một chuỗi hoàn chỉnh T̂ℓ+1.

**Token Reconstruction.** Tuy nhiên, do việc sparsification và separation token cho self-attention, chuỗi token kết hợp T̂ℓ+1 được xuất ra bởi block Bℓ+1 là không theo thứ tự và không nhất quán với các patch hình ảnh ban đầu. Để đạt được điều này, chúng tôi giới thiệu một module tái tạo token để định vị các token trong T̂ℓ+1 về vị trí ban đầu của chúng như một chuỗi mới. Nó có thể được công thức hóa như Tℓ+1 = R(T̂ℓ+1, Iℓ+1), trong đó R là một hàm biến đổi ánh xạ mỗi token đến vị trí hoặc thứ hạng tương ứng trong chuỗi, Iℓ+1 là bản đồ các chỉ số token có thể được tạo ra ngẫu nhiên bởi G. Cuối cùng, các token được tái tạo Tℓ+1 có thể được đưa vào block tiếp theo. Sau khi được reshape như một bản đồ đặc trưng fℓ+1, auxiliary head Hℓ+1 có thể sử dụng nó để đưa ra quyết định token-pass cho block tiếp theo, và decoder có thể hoàn thành dự đoán ngữ nghĩa với nó. Như được hiển thị trong Hình 2, chúng tôi thêm một module tái tạo token trước auxiliary heads và decoder phân đoạn.

**Token Merging.** Xem xét rằng có thể có một số thông tin hữu ích được cung cấp bởi các token dừng, chúng tôi merge chúng như một token đại diện và tổng hợp nó với class token trước khi tính toán self-attention trong block tiếp theo. Ví dụ, sau khi có được một chuỗi token Tℓ từ block Bℓ thông qua các thuật toán trên, class token có thể được cập nhật bằng

Tℓ 0 ← (1/2)Tℓ 0 + (1/|Sℓ|)∑|Sℓ| i ∑E j T̈ℓ i,j, (8)

trong đó |Sℓ| = |1-Mℓ| là tổng số token dừng. Sau đó các token giữ lại bao gồm Tℓ 0 có thể tuần tự tham gia self-attention trong block tiếp theo. Lưu ý rằng chúng tôi bảo tồn class token trong MSA theo tiêu chuẩn trong ViT [9], và loại bỏ nó khi xây dựng bản đồ đặc trưng. Thực tế, class token T0 là một token giữ lại mặc định trong tất cả các block, không phụ thuộc vào quyết định token-pass.

### 3.4. Pipeline Huấn luyện
Với các bản đồ đặc trưng {fℓ}D+1 ℓ=1 bao gồm các token dừng theo thứ bậc, auxiliary heads {Hℓ}D ℓ=1 và decoder phân đoạn D dự đoán các bản đồ xác suất ngữ nghĩa {pℓ}D ℓ=1 và ps. Để huấn luyện mạng phân đoạn theo cách có giám sát, bản đồ nhãn sự thật nền ȳ được sử dụng để tính toán loss cross-entropy (CE)

LCE(ps, ȳ) = (1/HW)∑HW i=1 ∑C j=1 -ȳi,j log(ps i,j), (9)

trong đó ȳi,j là giá trị thực (1 hoặc 0) của lớp thứ j cho pixel thứ i, và ps i,j tương ứng với xác suất được dự đoán bởi decoder phân đoạn D. Tương tự, D auxiliary heads được cập nhật với hàm loss

LAH = ∑D ℓ LCE(U(pℓ), ȳ), (10)

trong đó U(·) là hàm upsampling.

Để giảm thiểu thiệt hại hiệu suất gây ra bởi dynamic token-pass, chúng tôi sử dụng một framework self-distillation để huấn luyện mạng phân đoạn dựa trên DoViT với mạng dựa trên ViT tương ứng như một giáo viên. Chúng tôi ký hiệu bản đồ xác suất của giáo viên như pt, sau đó loss self-distillation được công thức hóa bằng divergence Kullback-Leibler(KL):

LSD(ps, pt) = (1/HW)∑HW i=1 ∑C j ps i,j log(ps i,j/pt i,j). (11)

Hàm loss tổng thể là

L = LCE + αLAH + βLSD, (12)

trong đó α, β > 0 là hai siêu tham số để kiểm soát tầm quan trọng tương đối.

## 4. Thí nghiệm

### 4.1. Cài đặt Thí nghiệm

#### 4.1.1 Bộ dữ liệu và Metrics
**Cityscapes [8]** là một bộ dữ liệu hiểu biết cảnh đô thị được sử dụng rộng rãi, với 19 lớp thông thường để đánh giá. Nó chứa 2,975 hình ảnh được chú thích tốt với 1024 × 2048 pixel để huấn luyện, 500 để xác thực, và 1,525 để kiểm tra.

**ADE20K [45]** là một trong những bộ dữ liệu phân đoạn ngữ nghĩa thách thức nhất chứa các cảnh thách thức với 20,210 hình ảnh được chú thích tốt với 150 lớp ngữ nghĩa trong tập huấn luyện. Tập xác thực và kiểm tra chứa 2,000 và 3,352 hình ảnh tương ứng.

**Metrics.** Số lượng phép toán dấu phẩy động (FLOPs) và tham số (Params) được giới thiệu để đo độ phức tạp tính toán và kích thước mô hình của mạng phân đoạn. Chúng tôi báo cáo throughput và frame-per-second (FPS) để hiển thị tốc độ suy luận của mạng. Chúng tôi sử dụng metric thông thường của mean Intersection over Union (mIoU), Pixel Accuracy (PA), và mean Pixel Accuracy (mPA) cho phân đoạn cảnh trên tất cả bộ dữ liệu. Lưu ý rằng mIoU là metric chính và thuyết phục hơn cho phân đoạn.

#### 4.1.2 Chi tiết Thực hiện
Chúng tôi thực hiện framework DoViT với PyTorch [26] trên 8 GPU NVIDIA V100. Chúng tôi đánh giá phương pháp được đề xuất trên hai kiến trúc phân đoạn giống transformer phổ biến, tức là SETR [44] và Segmenter [30], sử dụng ViT [9] và DeiT [31] cổ điển làm backbone. Đối với transformer backbone base và small, chúng tôi chọn các lớp (3,6,9) làm lớp quyết định. Trong tổng 24 lớp self-attention của ViT-large, các lớp (6,12,18) là lớp quyết định. Ba auxiliary heads đơn lớp với kernel 1×1 được chèn sau các lớp quyết định. Và auxiliary heads ban đầu cho SETR [44] có thể được tái sử dụng mà không cần sửa đổi cụ thể. Tất cả các thiết lập tăng cường dữ liệu, huấn luyện mạng và đánh giá độ chính xác tuân theo thực hiện chính thức của SETR [44] và Segmenter [30] trong codebase MMSegmentation [7]. Hệ số α và β được đặt mặc định là 1.0 và 0.4, tương ứng. Ngưỡng tin cậy ξ = 0.985 là tối ưu cho Cityscapes, và ξ∈[0.96,0.985] phù hợp cho ADE20K. Nếu không có hướng dẫn cụ thể, tham số, FLOPs, throughput và FPS được báo cáo với độ phân giải 1024×2048 cho Cityscapes, và hình ảnh được cắt ngẫu nhiên 512×512 cho ADE20K. Để kiểm tra chi phí suy luận thích ứng của mỗi hình ảnh trong DoViT của chúng tôi, chúng tôi lấy mẫu ngẫu nhiên 100 hình ảnh từ tập xác thực và báo cáo FLOPs, throughput và FPS trung bình của chúng. Để huấn luyện song song các hình ảnh với số lượng token giữ lại thưa thớt khác nhau, chúng tôi sử dụng một môi trường phân tán, trong đó kích thước batch trên mỗi GPU được đặt là 1 cho Cityscapes và 2 cho ADE20K. Nếu kích thước batch trên mỗi GPU lớn hơn 1, số lượng token giữ lại của hình ảnh batch trên một GPU được đặt giống nhau, bằng cách đạt trung bình theo thứ tự tin cậy.

### 4.2. Kết quả Chính
**Cityscapes.** Một trong những ưu điểm nhất của framework DoViT của chúng tôi là nó có thể giảm độ phức tạp tính toán (tức là FLOPs) của một loạt rộng các mạng phân đoạn giống transformer với việc giảm nhỏ độ chính xác. Bảng 1 tóm tắt so sánh hiệu suất và tính toán giữa framework của chúng tôi và các mô hình phân đoạn tiên tiến khác nhau. Chúng tôi chủ yếu làm nổi bật việc giảm mIoU và tỷ lệ giảm FLOPs trong ngoặc. Với phương pháp của chúng tôi, FLOPs của mạng được giảm 40% ∼60%, với mất mIoU dưới 0.8%. Đặc biệt, mIoU được cải thiện một chút thay vì giảm cho một số mạng với backbone DoViT-S, hưởng lợi từ việc chuyển token động và thưa thớt. Hơn nữa, chúng tôi mở rộng backbone DynamicViT [27] cho các kiến trúc phân đoạn, viết tắt là "DyViT/ ρ" (0≤ρ≤1 là tỷ lệ token) trong bảng. Chiến lược thoát token ngầm với regularization của tỷ lệ giữ lại, không đủ cho các mô hình phân đoạn ngữ nghĩa, trong đó ngữ cảnh phức tạp làm confuse việc chọn token. Như chúng ta có thể thấy khi FLOPs được giảm dưới 20%, SETR giảm hơn 5% mIoU. Chúng tôi cũng trình bày các tham số tăng thêm của DoViT và DyViT, tương đối với ViT tiêu chuẩn. Để căn chỉnh với các chiều embedding của các transformer lớn hơn, auxiliary heads với các chiều đầu vào lớn hơn giới thiệu nhiều tham số bổ sung hơn. Nhưng các tham số bổ sung có thể được bỏ qua so với chính backbone.

**ADE20K.** Để đánh giá hiệu quả và hiệu suất của phương pháp của chúng tôi, chúng tôi tiến hành các thí nghiệm mở rộng trên bộ dữ liệu ADE20K, như được hiển thị trong Bảng 2. Chúng tôi áp dụng ngưỡng tin cậy khác nhau ξ∈[0.96,0.985] và so sánh sự đánh đổi giữa hiệu suất mIoU và việc giảm tính toán. Đối với SETR với ViT-base, phương pháp của chúng tôi có thể giảm 30% FLOPs mà không giảm mIoU. Bộ dữ liệu ADE20K rất thách thức do các danh mục ngữ nghĩa quy mô lớn và cảnh phức tạp, làm cho các mô hình dự đoán với độ tin cậy thấp hơn cho nhiều pixel. Mặc dù việc tận dụng ngưỡng thấp hơn có thể dừng sớm nhiều token hơn và giảm chi phí suy luận hơn, khó để giảm FLOPs lên đến 20%, đặc biệt đối với các mạng nhỏ, ví dụ DoViT-S, DoDeiT-S. Nói chung, các mạng nhỏ hơn đạt được tỷ lệ giảm FLOPs ít hơn, gây ra bởi độ tin cậy thấp hơn ở early-probe.

**Acceleration Effort.** Trong Hình 3, chúng tôi so sánh speedup trên một GPU NVIDIA V100 về SETR (ViT-B/DoViT-B) trên hai bộ dữ liệu tương ứng. Đáng chú ý rằng giới hạn trong siêu phân giải, tức là 1024×2048 pixel, chúng tôi sử dụng đầu vào được cắt ngẫu nhiên 512×512 để đánh giá throughput trên Cityscapes. Hình 3a minh họa rằng cả throughput và FPS của các mô hình lớn và base được cải thiện hơn 2×, mà không yêu cầu sửa đổi phần cứng/thư viện. Trong khi đó, phương pháp của chúng tôi cải thiện throughput và FPS của các biến thể ViT-large (ξ= 0.98) 27% và 30% trên ADE20K (Hình 3b). Đáng tiếc rằng throughput của ViT-small có thể được cải thiện 23% trên ADE20K, nhưng FPS giảm một chút do tính toán bổ sung của auxiliary heads.

### 4.3. Nghiên cứu Ablation
**Effects of different components.** Để xác minh hiệu quả của mỗi thành phần trong framework của chúng tôi, chúng tôi tiến hành phân tích ablation trên Cityscapes với SETR-DoViT-B, và trình bày kết quả trong Bảng 3. Với chiến lược token merging, mIoU của DoviT được cải thiện khoảng 0.3%. Với self-distillation theo pixel, khoảng cách giữa các mạng phân đoạn DoViT và ViT có thể được giảm xuống 0.2% mIoU.

**Impact of the threshold.** Để điều tra tác động của ngưỡng tin cậy ξ, chúng tôi huấn luyện SETR-DoViT-B trên Cityscapes mà không có self-distillation. Hình 4a mô tả sự đánh đổi giữa hiệu suất và tính toán, thay đổi ngưỡng từ 0.95 đến 0.99. Rõ ràng rằng với sự tăng của ngưỡng, mIoU và FLOPs sẽ tăng, điều này hợp lý — nhiều tính toán hơn mang lại hiệu suất tốt hơn. Để cân bằng tính toán và mất hiệu suất, phù hợp để đặt ξ∈[0.985,0.99] cho Cityscapes. Nhờ vào nhiều patch dễ học của cityscapes, FLOPs có thể được giảm 50% khi ξ= 0.99. Ngoài ra, chúng tôi vẽ đường tốc độ suy luận (throughput và FPS) với ngưỡng, như được hiển thị trong Hình 4b. Thay đổi ngưỡng từ 0.95 đến 0.99, throughput (Thr.) có thể được cải thiện từ 34.1 (như được hiển thị trong Hình 3a) đến phạm vi [63,83], và suy luận có thể được tăng tốc ít nhất 2.9×, tức là từ 1.52 FPS đến hơn 4.42 FPS.

### 4.4. Visualization
Trong Hình 5, chúng tôi hiển thị kết quả định tính của hai hình ảnh cityscape. Chúng tôi thấy rằng việc áp dụng DoViT dẫn đến việc giảm dần các token/patch giữ lại khi forwarding theo block. Trong khi đó, các patch dễ học, chẳng hạn như bao phủ đường, bầu trời và cây, được dừng khỏi self-attention sớm, trong khi các patch khó bao gồm ngữ cảnh phức tạp, sẽ được giữ lại cho đến cuối vision transformers. Ngoài ra, chúng tôi visualize các bản đồ điểm tin cậy được dự đoán bởi ba auxiliary heads. Các token/patch với điểm số cao hơn (gần màu vàng hơn) được cho là sẽ được loại bỏ khỏi tính toán. Thú vị hơn là, mặc dù các token bao phủ các patch dễ được loại bỏ, một số phần cạnh thông tin hơn được bảo tồn, ví dụ đường viền của cây và xe. Hình 6 chứng minh thông tin định lượng tương ứng của hai trường hợp suy luận. Số lượng token tham gia vào bốn block ViT đều là 8,192. Trong trường hợp 1 (a), hơn một nửa token được loại bỏ ở block thứ hai, và FLOPs trên mỗi block cũng giảm tương ứng. Trong trường hợp 2 (b), hơn 80% token được dừng ở block thứ hai, và chỉ 10% token được giữ lại ở block cuối. Những kết quả này phản ánh hiệu quả và khả năng diễn giải của phương pháp dynamic token-pass của chúng tôi. Sơ đồ early-probe xác định token-pass một cách thích ứng, thay vì ép buộc dừng một tỷ lệ cố định của token cho tất cả hình ảnh. Do đó, chi phí suy luận của các hình ảnh khác nhau có thể rất khác nhau.

## 5. Kết luận
Trong công trình này, chúng tôi khám phá việc tăng tốc transformer phân đoạn từ góc độ dư thừa dữ liệu. Chúng tôi đã giới thiệu dynamic token-pass transformers (DoViT) để điều chỉnh thích ứng chi phí suy luận dựa trên độ phức tạp đầu vào. DoViT dần giảm số lượng token chuyển qua lớp self-attention và rút ngắn các token dừng theo thứ bậc thành một decoder thống nhất. Chúng tôi đánh giá hiệu quả của phương pháp của chúng tôi trong việc giảm tính toán và tăng tốc suy luận, và thảo luận một số vấn đề có ý nghĩa. Trong tương lai, chúng tôi dự định kết hợp phương pháp tăng tốc transformer nhận biết dữ liệu của chúng tôi với các phương pháp nén mô hình nhận biết tham số và mở rộng nó cho các tác vụ dự đoán dày đặc khác.
