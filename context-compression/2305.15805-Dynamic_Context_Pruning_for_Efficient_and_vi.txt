# 2305.15805.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/context-compression/2305.15805.pdf
# Kích thước tập tin: 3392436 bytes

===============================================
NỘI DUNG TẬP TIN PDF
===============================================


--- TRANG 1 ---
Cắt Giảm Ngữ Cảnh Động cho Transformer Tự Hồi Quy
Hiệu Quả và Có Thể Diễn Giải
Sotiris AnagnostidisµDario PavlloµLuca Biggioµ,νLorenzo Nociµ
Aurelien LucchiτThomas Hofmannµ
µETH Zürich
νML, CSEM SA
τUniversity of Basel
Tóm tắt
Các Transformer Tự Hồi Quy được áp dụng trong Mô Hình Ngôn Ngữ Lớn (LLM) khó mở rộng cho các chuỗi dài. Mặc dù có nhiều công trình cố gắng giảm chi phí tính toán của chúng, hầu hết LLM vẫn áp dụng các lớp attention giữa tất cả các cặp token trong chuỗi, do đó phát sinh chi phí bậc hai. Trong nghiên cứu này, chúng tôi trình bày một phương pháp mới cắt giảm thông tin ngữ cảnh một cách động trong khi vẫn bảo toàn tính biểu cảm của mô hình, dẫn đến giảm yêu cầu bộ nhớ và tính toán trong quá trình suy luận. Phương pháp của chúng tôi sử dụng cơ chế có thể học để xác định những token không mang thông tin nào có thể được loại bỏ khỏi ngữ cảnh tại bất kỳ điểm nào trong quá trình sinh. Bằng cách này, phương pháp của chúng tôi không chỉ giải quyết các vấn đề về hiệu suất mà còn tăng cường khả năng diễn giải, cung cấp hiểu biết có giá trị về quá trình ra quyết định của mô hình. Kỹ thuật của chúng tôi có thể được áp dụng cho các mô hình đã được huấn luyện trước thông qua một quá trình fine-tuning đơn giản, và độ mạnh cắt giảm có thể được chỉ định bằng một tham số sparsity. Đáng chú ý, các phát hiện thực nghiệm của chúng tôi chứng minh rằng chúng tôi có thể cắt giảm hiệu quả lên đến 80% ngữ cảnh mà không có suy giảm hiệu suất đáng kể trên các nhiệm vụ downstream, cung cấp một công cụ có giá trị để giảm thiểu chi phí suy luận. Triển khai tham chiếu của chúng tôi đạt được tăng thông lượng suy luận lên đến 2× và tiết kiệm bộ nhớ thậm chí còn lớn hơn.

1 Giới thiệu
Việc đưa Transformer [Vaswani et al., 2017] vào Mô Hình Ngôn Ngữ Lớn (LLM) đã ảnh hưởng sâu sắc đến bối cảnh Xử Lý Ngôn Ngữ Tự Nhiên (NLP), do các tính chất mở rộng hấp dẫn của chúng [Kaplan et al., 2020] và khả năng huấn luyện hiệu quả trên các kiến trúc phần cứng hiện đại được thiết kế cho tính toán song song mở rộng. Khi LLM trở nên lớn hơn và phức tạp hơn, những thách thức liên quan đến việc huấn luyện và triển khai chúng trở nên nổi bật hơn. Đặc biệt thách thức là việc tìm kiếm xử lý các chuỗi ngày càng dài hơn, vì các lớp self-attention thuần túy có độ phức tạp bậc hai theo độ dài chuỗi trong quá trình huấn luyện và suy luận.

Để giải quyết hạn chế này, một số nỗ lực tập trung vào việc triển khai hiệu quả cơ chế attention trên phần cứng chuyên dụng [Dao et al., 2022, Touvron et al., 2023], hoặc trên các thủ tục thuật toán để giải quyết trực tiếp độ phức tạp bậc hai. Hướng sau đã dẫn đến nhiều biến thể hy sinh tính tổng quát của cơ chế attention tiêu chuẩn để có các lựa chọn thay thế hiệu quả hơn [Tay et al.,
Liên hệ sanagnos@inf.ethz.ch.
Hội nghị thứ 37 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2023).arXiv:2305.15805v3  [cs.CL]  31 May 2024

--- TRANG 2 ---
Attention Attention Cục bộ Sparse Attention Attention Thích ứng ThưaLớp cụ thể
Token Hiện tại
Token Được Chú ý
Token Bị Che dấuHình 1: Hình ảnh hóa các trọng số attention nhân quả liên quan đến attention nhân quả tiêu chuẩn, cục bộ, thưa và phương pháp của chúng tôi. Attention thích ứng thưa (bên phải nhất) cắt giảm trọng số một cách động cho từng token, và nó không áp đặt bất kỳ bias quy nạp hạn chế nào lên cấu trúc attention cuối cùng.

2020, Kitaev et al., 2020, Choromanski et al., 2020b, Katharopoulos et al., 2020, Zaheer et al., 2020,
Shi et al., 2021, Lin et al., 2022, Zhu and Soricut, 2021, Dai et al., 2020], một số trong đó được minh họa trong Hình 1. Cụ thể, một số lượng lớn các phương pháp này tập trung vào việc làm thưa các trọng số attention, giảm kích thước ngữ cảnh có sẵn cho mỗi token, hoặc nén số lượng token để giảm kích thước ma trận attention.

Tuy nhiên, các phương pháp này về bản chất là tĩnh, theo nghĩa là mỗi token bị buộc phải chú ý đến một cửa sổ ngữ cảnh cố định được chỉ định trước, hoặc ngữ cảnh đầu vào được nén thành một chiều cố định, bất kể nội dung thông tin của chuỗi đầu vào. Hơn nữa, vẫn tồn tại khoảng cách hiệu suất so với self-attention thuần túy trong nhiều ứng dụng, do đó ngụ ý sự tồn tại của một sự đánh đổi không tầm thường giữa phạm vi ngữ cảnh attention và khả năng của mô hình [Dao et al., 2022, Sun et al., 2021, Beltagy et al., 2020].

Để giải quyết những thách thức này và tăng cường hiệu quả suy luận, trong khi vẫn trung thành với self-attention thuần túy, chúng tôi đặt ra câu hỏi sau:

Liệu chúng ta có thể cắt giảm nội dung quá khứ một cách động dựa trên ngữ cảnh có sẵn, trong khi bảo toàn càng nhiều càng tốt tính biểu cảm của mô hình?

Để trả lời câu hỏi này, chúng tôi giới thiệu một phương pháp mới để cắt giảm ngữ cảnh trong các kiến trúc decoder dựa trên Transformer. Phương pháp của chúng tôi thêm một lượng nhỏ tham số huấn luyện bổ sung cho phép các token riêng lẻ loại bỏ động các phần của chuỗi đầu vào theo từng lớp. Một khi một phần ngữ cảnh bị loại bỏ, nó bị bỏ qua cho phần còn lại của quá trình sinh tự hồi quy, dẫn đến giảm sử dụng bộ nhớ và yêu cầu tính toán trong quá trình suy luận. Để đạt được điều này, chúng tôi cũng thiết kế một cấu trúc dữ liệu động triển khai việc chèn/loại bỏ token khỏi ngữ cảnh một cách hiệu quả trong khi hỗ trợ suy luận theo batch. Trái ngược với các phương pháp truyền thống dựa vào attention cục bộ hoặc thưa, có thể không nắm bắt được những sắc thái và tính chất động của dữ liệu trên các ngữ cảnh dài, phương pháp của chúng tôi tận dụng các gợi ý ngữ cảnh để xác định động mức độ liên quan của thông tin có sẵn thông qua một cơ chế đã học. Điều này được thực hiện bằng cách sử dụng hàm sigmoid thưa [Peters et al., 2019, Martins et al., 2020]. Như được chứng minh bởi các đánh giá thực nghiệm của chúng tôi, điều này cho phép chúng tôi trích xuất và sử dụng các chi tiết thiết yếu theo cách thích ứng và chính xác hơn. Mức độ cắt giảm có thể được kiểm soát hiệu quả thông qua một tham số siêu việc có hiệu quả tính đến mức độ thưa.

Kỹ thuật của chúng tôi phục vụ như một khối xây dựng modular cho các mô hình đã được huấn luyện trước hiện có và có thể được tích hợp dễ dàng thông qua một giai đoạn fine-tuning tối thiểu. Đối với nghiên cứu của chúng tôi, chúng tôi tập trung vào các mô hình GPT-2 [Radford et al., 2019] vì chúng có sẵn công khai và được đánh giá rộng rãi, nhưng do tính thống nhất của các kiến trúc hiện đại, phương pháp của chúng tôi có thể được mở rộng một cách đơn giản cho bất kỳ Transformer tự hồi quy nào. Hơn nữa, vì phương pháp của chúng tôi dựa trên cắt giảm ngữ cảnh, nó có thể được kết hợp liền mạch với các phương pháp khác nhằm cải thiện hiệu quả suy luận, chẳng hạn như quantization, cắt giảm trọng số, attention xấp xỉ, hoặc các tối ưu hóa phần cứng khác.

Chúng tôi thấy rằng lên đến 80% ngữ cảnh có thể được cắt giảm thành công, với sự suy giảm tối thiểu về perplexity và hiệu suất zero-shot, trong khi yêu cầu ít tài nguyên hơn đáng kể trong quá trình suy luận. Chúng tôi cho thấy cách những cải thiện này có thể dẫn đến lợi ích thực tế có thể đo lường được, bằng cách cung cấp một triển khai hiệu quả giảm sử dụng bộ nhớ cho bộ nhớ đệm trong quá trình sinh token. Cụ thể hơn, đối với các kích thước ngữ cảnh lớn hơn, chúng tôi có được giảm latency wall-time lên đến 50% cho mỗi bước sinh, trong khi vẫn giải mã với kích thước batch lớn hơn lên đến 2×, do đó dẫn đến lợi ích hiệu suất đáng kể. Những phát hiện này làm nổi bật tiềm năng của cắt giảm ngữ cảnh như một kỹ thuật mạnh mẽ để tăng cường hiệu quả và khả năng diễn giải của Transformer trong NLP.

2 Công trình liên quan
Mặc dù thể hiện hiệu suất ở mức con người trên một số nhiệm vụ thách thức, LLM tiêu tốn nhiều tài nguyên và không hiệu quả. Trong khi não con người tiêu thụ khoảng lượng năng lượng tương đương với một bóng đèn mờ, các mô hình GPT hiệu suất cao yêu cầu nhiều GPU với ~80GB bộ nhớ mỗi cái để suy luận [Strubell et al., 2019, Frantar and Alistarh, 2023a]. Một số nỗ lực nghiên cứu đã tập trung vào việc cải thiện hiệu quả và yêu cầu bộ nhớ của chúng từ nhiều góc độ khác nhau.

Cắt giảm Trọng số và Quantization. Các LLM hiện đại có yêu cầu bộ nhớ và tính toán cao cho cả huấn luyện và kiểm tra. Để giải quyết hạn chế này, một số nỗ lực nghiên cứu [Kwon et al., 2022, Frantar et al., 2023, Frantar and Alistarh, 2023b] đã sử dụng thực hành đã được thiết lập về cắt giảm trọng số [Hassibi et al., 1993] để nén hiệu quả mô hình gốc xuống kích thước dễ quản lý hơn. Đáng chú ý, một tỷ lệ lớn trọng số gốc có thể được loại bỏ an toàn, chỉ dẫn đến tăng perplexity cận biên [Bahl et al., 1983]. Một phương pháp thay thế để giảm bộ nhớ và tính toán là quantization [Dettmers et al., 2022, Yao et al., 2022, Xiao et al., 2022, Frantar et al., 2022], làm giảm độ chính xác của biểu diễn số của mô hình. Các scheme quantization [Dettmers et al., 2022] cho phép phép nhân ma trận 8-bit cho cả các lớp feed-forward và projection attention dẫn đến cải thiện đáng kể về phân bổ bộ nhớ mà không phát sinh bất kỳ suy giảm hiệu suất nào.

Transformer Hiệu quả và cắt giảm ngữ cảnh. Một ràng buộc chính của các mô hình dựa trên Transformer là độ phức tạp bậc hai của chúng đối với độ dài của chuỗi đầu vào. Nghiên cứu mở rộng khám phá các lựa chọn thay thế thể hiện scaling dưới bậc hai, dẫn đến ba chiến lược chính [Lin et al., 2022]. Đầu tiên thay thế cơ chế attention bằng một phép toán thay thế có scaling thuận lợi hơn với độ dài chuỗi đầu vào [Peng et al., 2021, Katharopoulos et al., 2020, Choromanski et al., 2020a, Schlag et al., 2021]. Trong khi một số phương pháp gần đây trong danh mục này cho thấy triển vọng, không có phương pháp nào nổi lên như một người chiến thắng chắc chắn, và hầu hết các mô hình ngôn ngữ state-of-the-art vẫn dựa vào cơ chế attention tiêu chuẩn [Touvron et al., 2023, Chowdhery et al., 2022]. Phương pháp thứ hai được đề xuất để nén độ dài của ngữ cảnh đầu vào, kiểm soát độ phức tạp của phép toán attention nhưng không thể tránh khỏi hy sinh thông tin có khả năng liên quan từ đầu vào gốc [Lee et al., 2019, Wang et al., 2020, Jaegle et al., 2021]. Phương pháp thứ ba liên quan đến việc cắt giảm ma trận attention, ngăn mỗi token chú ý đến mọi token khác trong ngữ cảnh [Zaheer et al., 2020, Martins et al., 2020, Lee et al., 2023]. Dòng nghiên cứu này được thúc đẩy bởi phát hiện lý thuyết nổi bật rằng Transformer thưa giữ lại tính biểu cảm của các đối tác dày đặc của chúng [Yun et al., 2020]. Nhiều phương pháp trong danh mục này sử dụng các mask attention được thiết kế đặc biệt nhằm làm zero càng nhiều entry càng tốt, thường dựa trên các nguyên tắc về tính cục bộ, tính ngẫu nhiên, hoặc sự kết hợp của cả hai. Nhược điểm chính của các phương pháp này là bản chất chủ yếu tĩnh của chúng, có nghĩa là mỗi token bị buộc phải chú ý đến một cửa sổ ngữ cảnh cố định và bỏ qua phần còn lại của ngữ cảnh bất kể vai trò cụ thể của nó trong chuỗi đầu vào. Phương pháp của chúng tôi thuộc danh mục cuối cùng này, và cho phép sparsification động của ma trận attention cho các mô hình decoder, mà không cần sử dụng bất kỳ bias quy nạp có khả năng hạn chế nào về cấu trúc của nó.

Tăng tốc Triển khai Gần đây, các triển khai được tối ưu hóa phần cứng [Dao et al., 2022, Touvron et al., 2023] đã được đề xuất với mục đích tối ưu hóa tài nguyên tính toán trong giai đoạn huấn luyện của Transformer [Hoffmann et al., 2022]. Mặt khác, khi những đột phá gần đây đã dẫn đến việc áp dụng rộng rãi các mô hình này [Ouyang et al., 2022, OpenAI, 2023, Köpf et al., 2023], hiệu suất trong quá trình suy luận trở nên liên quan hơn từng ngày. Trong Transformer tự hồi quy dựa trên decoder, kiến trúc backbone của hầu hết LLM state-of-the-art hiện tại, suy luận liên quan đến việc đánh giá và sinh token từng cái một, sử dụng các activation đã được cache trước đó để tránh tính toán dư thừa. Trái ngược với huấn luyện, suy luận bị ràng buộc bởi bộ nhớ [Shazeer, 2019, Ivanov et al., 2021, Pope et al., 2022]. Tính toán bị sử dụng dưới mức, đặc biệt khi triển khai các mô hình lớn hơn, vì thời gian cần thiết để chuyển các tham số mô hình và activation đến bộ nhớ phần cứng vượt xa thời gian tính toán thực tế. Điều này còn bị tăng cường bởi xu hướng gần đây là liên tục tăng kích thước mô hình và cho phép các cửa sổ ngữ cảnh dài hơn. Kết quả là, batch decoding, một hướng đầy hứa hẹn để sử dụng hiệu quả hơn tài nguyên phần cứng, bị cản trở.

3

--- TRANG 3 ---
Một transformer là một mô hình deep learning.
Nó
Là Một transformer là một mô hình deep learning.
Một transformer là một mô hình deep learning. Nó Một transformer là một mô hình deep learning(Vị trí) ID:
0 1 2 3 4 5 6 7 8 9
ID: 0 ID: 1 ID: 2 ID: 3 ID: 4 ID: 5 Trống Trống Bộ đệm bộ nhớ cho lớp 
Dữ liệu được lưu trữ dưới dạng (ID, activation được cache cho ID)
ID: 0 ID: 1 ID: 2 ID: 3 ID: 4 ID: 5 ID: 6 Trống
ID: 7 ID: 1 ID: 2 Trống ID: 4 ID: 5 ID: 6 Trống
ID: 7 ID: 1 ID: 8 Trống ID: 4 ID: 5 ID: 6 Trống Attention Thích ứng Thưa tại lớp Hình 2: Chúng tôi minh họa trạng thái của bộ đệm bộ nhớ tại đầu mỗi lần lặp cho phương pháp đề xuất của chúng tôi. Các token bị loại bỏ không liên quan cho bất kỳ bước sinh nào tiếp theo và các activation được cache của chúng bị xóa. Vì self-attention là một phép toán tập hợp, bộ đệm (keys/values) của các token bị loại bỏ có thể được tái sử dụng bởi các token tiếp theo, đảm bảo rằng cấu trúc dữ liệu được đóng gói chặt chẽ nhất có thể.

3 Phương pháp luận
Nền tảng. Chúng tôi hoạt động trên các chuỗi token văn bản T∈ {0,1, . . . , n vocab}n, trong đó n là độ dài của chuỗi và nvocab là kích thước từ vựng. Các token được nhúng vào X0∈Rn×d sử dụng một lớp embedding, trong đó d là chiều embedding của mô hình. Khi cần thiết, chúng tôi sử dụng chỉ số trên ℓ∈ {1,2, . . . , L } để biểu thị các biểu diễn và trọng số ở các lớp khác nhau. Một lớp của kiến trúc Transformer-decoder [Vaswani et al., 2017] được định nghĩa như sau
X=MHA (LayerNorm (Xℓ−1)) +Xℓ−1, (1)
Xℓ=FF(LayerNorm (X)) +X, (2)
trong đó MHA viết tắt của Multi-head self-attention được định nghĩa như sau
MHA (X) =Concatenate (head 1(X),head 2(X), . . . , head h(X))WO,trong đó (3)
head i(X) =SA(Qi,Ki,Vi). (4)
Ở đây Qi=XW Qi,Ki=XW Ki, và V=XW Vi là các query, key và value và SA biểu thị single-head self-attention. Các ma trận trọng số WQi,WKi,WVi∈Rd×p projection tuyến tính embedding đầu vào vào chiều head p. Cuối cùng, WO∈Rd×d là projection đầu ra. Phần feed-forward của Transformer được định nghĩa như sau
FF(X) =σFF(XW F1)WF2, (5)
trong đó σFF là một hàm phi tuyến, và WF1,WF2 là các lớp tuyến tính với các chiều điển hình WF1∈Rd×4·d và WF2∈R4·d×d. Một lớp projection cuối cùng Wlogits∈Rd×nvocab được sử dụng để project trở lại không gian từ vựng và dự đoán token tiếp theo từ các biểu diễn XL. Chúng tôi tập trung vào các kiến trúc decoder-only Pre-LN [Xiong et al., 2020], có nghĩa là attention được masked nhân quả, tức là mỗi token đầu vào i chú ý đến i token đầu tiên trong chuỗi đầu vào. Về mặt khái niệm, phương pháp của chúng tôi hoạt động bằng cách dự đoán các mask attention này sử dụng một cơ chế đã học theo từng lớp, với việc đưa vào các ràng buộc bổ sung để đảm bảo tính nhân quả được bảo toàn (tức là nếu một token bị loại bỏ, nó sẽ vẫn bị loại bỏ trong tương lai). Tuy nhiên, trong quá trình suy luận, phương pháp của chúng tôi có thể được triển khai hiệu quả bằng cách xóa các token khỏi key-value cache thường được áp dụng trong các mô hình attention tự hồi quy.

Nền tảng: key-value cache. Trong Transformer tự hồi quy, suy luận có thể được tối ưu hóa bằng cách tái sử dụng các activation đã được tính toán trước (key và value) để tăng tốc sinh tuần tự các token [Ott et al., 2019, Vaswani et al., 2018, Wolf et al., 2020], đưa chi phí tính toán để sinh một token duy nhất xuống O(n) từ O(n2) (trong đó n là độ dài câu). Hầu hết các kỹ thuật attention thưa hiện có bỏ qua các chi tiết cụ thể của quá trình này và tập trung vào việc làm thưa mỗi phép toán attention riêng biệt. Vì các token không được chú ý vẫn có thể được chú ý bởi các token tiếp theo, lợi ích về bộ nhớ bị hạn chế. Ngược lại, phương pháp của chúng tôi tương thích với thiết lập này, cho phép chúng tôi thiết kế một cấu trúc dữ liệu batch hiệu quả trong đó các token bị loại bỏ được loại bỏ hiệu quả khỏi tính toán.

3.1 Attention Thích ứng Thưa
Chúng tôi cho phép mạng loại bỏ có chọn lọc các phần của ngữ cảnh không còn cần thiết. Một minh họa về phương pháp đề xuất của chúng tôi có thể được thấy trong Hình 2. Tại mỗi lớp, chúng tôi giới thiệu các tham số

4

--- TRANG 4 ---
Bước Huấn luyện1357Giá trị của 
3
 2
 1
 0 1 2 30.00.20.40.60.81.0Sparse SigmoidHàm bước
0
100
200
300
400
500
600
700
800
900
1000
Độ dài Ngữ cảnh0.02.04.06.08.010.0
0.00.20.40.60.81.0

Hình 3: (Trái) Chúng tôi sử dụng một bộ lập lịch cosine để đặt các giá trị α trong quá trình huấn luyện. (Giữa) Đối với các giá trị α > 1, các ánh xạ của α-sigmoid bão hòa tại ±1/(α−1). Trong quá trình suy luận, chúng tôi thay thế α-sigmoid bằng một hàm bước, tương ứng với trường hợp α→ ∞ . (Phải) Phân phối của Iℓ k,j cho các giá trị khác nhau của βℓ đối với khoảng cách giữa các token k−j. Đối với mô tả này, chúng tôi giả định các vector phân phối chuẩn ngẫu nhiên làm đầu vào và các trọng số được khởi tạo ngẫu nhiên Wℓ Qint,Wℓ Kint, theo khởi tạo 'He' [He et al., 2015].

Wℓ Qint,Wℓ Kint∈Rd×r cho chiều r∈R, tính toán các query và key tương tác Qℓ int,Kℓ int∈Rn×r, như Qℓ int=XℓWℓ Qint và Kℓ int=XℓWℓ Kint. Sau đó chúng tôi tính toán tương tác của token k với token j tại lớp ℓ như sau:

Iℓ k,j={
∏k n=j+1Iℓ n,j và Iℓ n,j=σ((Qℓ int)⊤ n(Kℓ int)j/√r+βℓ), nếu j < k
1, nếu j=k,
0, nếu j > k,(6)

trong đó σ(·) biểu thị hàm sparse sigmoid được giới thiệu trong Phần 3.2 và βℓ∈R là một tham số vô hướng cho mỗi lớp, kiểm soát độ thưa ban đầu như được thấy trong Hình 3 (phải). Các chỉ số trong Qℓ int,Kℓ int∈ Rn×r tham chiếu đến các hàng của ma trận. Sau đó chúng ta có thể sửa đổi self-attention

SA(Qℓ i,Kℓ i,Vℓ i) =softmax(Qℓ i(Kℓ i)⊤/√p+ log(Iℓ))Vℓ i. (7)

Đối với j > k chúng ta đặt Iℓ k,j= 0, dẫn đến việc che các entry trong self-attention, tương ứng với causal masking thông thường. Chúng tôi cũng áp đặt rằng một token không thể loại bỏ chính nó, do đó Iℓ k,k= 1. Chúng tôi muốn bảo toàn thông tin về token hiện tại vì các dự đoán của nó đặc biệt quan trọng trong việc xác định token tiếp theo cho nhiệm vụ mô hình hóa ngôn ngữ thông thường mà chúng tôi đang xem xét. Các giá trị nhỏ của Iℓ n,j áp đặt việc che một phần token tương ứng trong attention, và việc che hoàn toàn xảy ra khi Iℓ n,j= 0. Tích tích lũy qua các token j+ 1→k trong Phương trình (6) áp đặt rằng việc loại bỏ một token (khi σ(.)→0) có hiệu ứng không thể đảo ngược, vì nó sẽ vẫn bị loại bỏ cho tất cả các token tiếp theo, và do đó cho phần còn lại của quá trình sinh. Độ phức tạp của logic cắt giảm là O(n·d·r+n2·r), thấp hơn so với phép toán self-attention đối với r < d .

Cơ chế của chúng tôi cho phép các lớp hoạt động độc lập, có nghĩa là các mẫu sparsity khác nhau được gặp phải trên các lớp. Chúng tôi cũng thử nghiệm với việc liên kết các quyết định loại bỏ của mô hình với độ sâu bằng cách áp đặt rằng một token bị loại bỏ tại một lớp nhất định không thể được chú ý trong các lớp tiếp theo. Tuy nhiên, chúng tôi quan sát thấy kết quả tồi tệ hơn và do đó không theo đuổi điều này nữa. Điều này có lẽ mong đợi, với nhiều kết quả và nghiên cứu khả năng diễn giải về các mẫu sparsity của attention head ở các lớp khác nhau [Ramsauer et al., 2020, Hao et al., 2021].

3.2 Sparse Sigmoid
Trong Phương trình (6), chúng tôi sử dụng σ(·), như một hàm giống sigmoid để cho mạng quyết định khi nào và cái gì cần loại bỏ. Chúng tôi ưa thích các quyết định nhị phân, dẫn đến các giá trị tương tác là 0 hoặc 1. Được truyền cảm hứng từ hàm α-entmax được giới thiệu trong Peters et al. [2019], Martins et al. [2020], chúng tôi định nghĩa α-sigmoid (dựa trên

5

--- TRANG 5 ---
các entropy được đề xuất bởi Tsallis [1988]) như sau:
σ(x) =α-sigmoid (x) =argmax p∈[0,1](p·x+Hα(p)), (8)
trong đó
Hα(p) ={
1/α(α−1)(p−pα+ (1−p)−(1−p)α), nếu α̸= 1
−plogp−(1−p) log(1 −p), nếu α= 1.(9)

Bằng cách thay đổi α trong quá trình huấn luyện, chúng ta có thể kiểm soát độ thưa trong mạng, tức là điều chỉnh độ mềm của cơ chế cắt giảm. Trong thực tế, chúng tôi bắt đầu từ các giá trị nhỏ α= 1 và tăng nó theo bộ lập lịch cosine, như được hiển thị trong Hình 3. Các giá trị nhỏ của α cho phép các tín hiệu gradient có ý nghĩa đi qua cơ chế loại bỏ, điều này rất quan trọng ở đầu quá trình huấn luyện. Mặt khác, các giá trị lớn hơn của α dẫn đến kết quả thưa mong muốn trong quá trình suy luận. Do đó chúng tôi tăng α lên các giá trị dẫn đến các giải pháp rất thưa, như được minh họa trong Hình 3. Trong thực tế, trong quá trình suy luận, chúng tôi thay thế σ(·) bằng hàm bước, tương ứng với α→ ∞ . Chúng tôi cũng khởi tạo các tham số bias βℓ trong (6) thành một giá trị dương, đảm bảo rằng các token ở đầu huấn luyện có xu hướng không bị loại bỏ. Chiến lược này cũng tạo điều kiện cho việc fine-tuning các mô hình đã được huấn luyện trước hiện có, vì module của chúng tôi ban đầu sẽ mặc định gần với hàm đồng nhất. α-sigmoid cùng với lịch trình huấn luyện trên α cho phép các tính chất truyền tín hiệu tốt cho gradient [Noci et al., 2022]. Chúng tôi cũng khám phá việc sử dụng sigmoid thông thường với nhiệt độ thay đổi [Kim et al., 2022], dẫn đến các dự đoán không nhị phân dưới tối ưu và bất ổn trong quá trình huấn luyện. Huấn luyện với sparse sigmoid của chúng tôi cũng trực tiếp loại bỏ nhu cầu có bất kỳ mạng phụ trợ nào [Lee et al., 2023].

3.3 Mục tiêu Có Điều hòa
Chúng tôi tăng cường mục tiêu mô hình hóa ngôn ngữ thông thường với một điều hòa khuyến khích mạng f loại bỏ các phần của chuỗi. Chúng tôi fine-tune các mô hình đã được huấn luyện trước, với các tham số θ, sử dụng mục tiêu:
L(θ,T) =Llm(θ,T) +Lsparsity (θ,T), (10)
trong đó
Llm(θ,T) =CE(fθ(T),shift(T)) (11)
là loss cross-entropy thông thường cho nhiệm vụ mô hình hóa ngôn ngữ dựa trên các token đầu vào gốc và đã dịch chuyển T, và
Lsparsity (θ,T) =γ/2 * 1/(L*n*(n−1)) * ∑i,ℓ Iℓ i,j (12)
là loss sparsity, khuyến khích mô hình cắt giảm ngữ cảnh. Tổng cộng (L*n*(n−1))/2 entry của Iℓ i,j được học, như được chỉ ra trong Phương trình (6). Chúng tôi chọn γ >0 để thi hành các mức độ sparsity khác nhau. Nói chung, đối với một vị trí hiện tại i trong ngữ cảnh, chúng tôi định nghĩa sparsity như tỷ lệ phần trăm của các token trước đó bị loại bỏ, tức là (token ≤i bị loại bỏ)/i.

4 Thí nghiệm
Chúng tôi fine-tune các mô hình GPT-2 đã được huấn luyện trước1, hỗ trợ kích thước ngữ cảnh lên đến 1024 token, trên một tập con của các bộ dữ liệu English Wikipedia 20220301.en và English bookcorpus. Chúng tôi giữ một tập kiểm tra riêng biệt nơi chúng tôi báo cáo perplexity sau huấn luyện. Tất cả các mô hình được hiển thị, để so sánh công bằng, đều được fine-tune sử dụng cùng thiết lập huấn luyện nhẹ như được mô tả trong Phụ lục A. Khi sử dụng adaptive sparse attention của chúng tôi, chúng tôi sử dụng bộ lập lịch cosine cho tham số α như được hiển thị trong Hình 3 và chỉ định r= 64 cho các chiều của Wℓ Qint,Wℓ Kint. Thêm các ablation về tối ưu hóa và các biến thể của cơ chế loại bỏ của chúng tôi được cung cấp trong Phụ lục B. Trừ khi được nêu khác, kết quả tham chiếu đến các mô hình GPT-2-small. Chúng tôi sử dụng thuật ngữ dense cho các mô hình GPT-2 thông thường, được fine-tune mà không có bất kỳ tham số WQint,WKint bổ sung nào.

1Chúng tôi sử dụng các mô hình và tokenizer đã được huấn luyện trước từ https://huggingface.co/ , cho các mô hình GPT-2-{small, medium, large, xl}. Ở đây nvocab = 50257 .

6

--- TRANG 6 ---
[Thực hiện biểu đồ và hình ảnh]

Hình 4: Perplexity (thấp hơn là tốt hơn) cho các mức độ sparsity khác nhau. (Trái) Perplexity tổng thể được tính trung bình trên các token với kích thước ngữ cảnh thay đổi từ 1 đến 1024. Ba biểu đồ bên phải hiển thị perplexity cho các kích thước ngữ cảnh khác nhau.

[Thực hiện biểu đồ và hình ảnh]

Hình 5: Độ chính xác zero-shot trung bình (cao hơn là tốt hơn) cho các bộ dữ liệu WinoGrande, HellaSwag, PIQA và LAMBADA. Vì sparsity của tất cả các phương pháp phụ thuộc vào kích thước ngữ cảnh, chúng tôi tính trung bình sparsity mong đợi dựa trên độ dài của các prefix trong các bộ dữ liệu này. (Trái) Các mô hình GPT-2-small và (phải) tất cả các mô hình GPT-2.

Baseline. Chúng tôi so sánh với các baseline được trình bày trong Hình 1. Local Attention tham chiếu đến một causal attention mask, trong đó mỗi token chú ý đến k token trước đó trong chuỗi, bao gồm chính nó. Điều này cũng có thể được hiểu như hạn chế trường tiếp nhận của mô hình. Sparse Attention tham chiếu đến các baseline từ Child et al. [2019], Lin et al. [2022], trong đó mỗi token i chú ý đến các token thỏa mãn (1) ⌊i/k⌋=⌊j/k⌋ và (2) các token k−1,2·k−1, . . . ,⌊i/k⌋ ·k−1 (đánh số bắt đầu từ zero). Chúng tôi fine-tune các baseline này sử dụng cùng thủ tục fine-tuning đã nêu trên, cho các lựa chọn k khác nhau, dẫn đến các mức độ sparsity khác nhau, tùy thuộc vào kích thước ngữ cảnh hiện tại.

Cấu trúc dữ liệu. Triển khai thực tế của phương pháp chúng tôi thể hiện nhiều thách thức do bản chất của sinh batch. Cụ thể, chúng tôi nổi bật sự khác biệt về độ dài prompt (prefix ban đầu), độ dài cuối cùng khác nhau (tiêu chí kết thúc), và việc loại bỏ token không đều trên các câu khác nhau. Hiệu suất tối đa đạt được khi key-value cache được biểu diễn như một khối bộ nhớ liền kề, và bất kỳ masking nào do padding hoặc token bị loại bỏ ("lỗ hổng") sẽ dẫn đến giảm hiệu quả. Để đạt được điều này, chúng tôi thiết kế một cấu trúc dữ liệu batch hiệu quả cho phép chèn và xóa token hiệu quả (tận dụng bản chất tập hợp của phép toán self-attention), trong khi (i) cho phép lưu trữ cơ bản được xử lý như một khối bộ nhớ liền kề và (ii) đảm bảo rằng load factor của cấu trúc dữ liệu đủ cao để đảm bảo tăng tốc hiệu suất. Thêm chi tiết được cung cấp trong Phụ lục A.

4.1 Kết quả
Perplexity vs sparsity. Đầu tiên chúng tôi nghiên cứu cách cắt giảm ngữ cảnh thay đổi cho các mức độ sparsity khác nhau trong Hình 4. Tùy thuộc vào kích thước ngữ cảnh hiện tại, phương pháp của chúng tôi cho phép lên đến 80% ngữ cảnh được cắt giảm thành công, tức là loại bỏ, mà không mất hiệu suất về perplexity (-0.085 tăng perplexity trung bình khi kích thước ngữ cảnh là 1000 token cho 80.35% sparsity so với đối tác dense). Phương pháp của chúng tôi cũng thích ứng với kích thước ngữ cảnh hiện tại, có nghĩa là một mạng được huấn luyện với điều hòa sparsity cụ thể thể hiện các mức độ sparsity khác nhau tùy thuộc vào kích thước ngữ cảnh hiện tại. So với các baseline, phương pháp của chúng tôi thể hiện kết quả perplexity thấp hơn nhất quán cho cùng mức độ sparsity.

7

--- TRANG 7 ---
[Tiếp tục với nội dung còn lại...]

Tăng sparsity Tăng sparsity0.000.050.100.150.200.250.300.35GFLOSFLOPs cho Kích thước ngữ cảnh 1000
lớp-embedding
lớp-logits
feed-forward
tính-toán-qkvo
attention
loại-bỏ-token
0.0000.0020.0040.0060.008GBBộ nhớ Cache cho Kích thước ngữ cảnh 1000
keys+values_attention
keys_int

Hình 6: (Trái) Phân phối FLOPs cho các mô hình có mức độ sparsity khác nhau. Ở đây, lớp-embedding tham chiếu đến embedding của chuỗi đầu vào thành biểu diễn X0, lớp-logits đến các projection của biểu diễn cuối cùng XL theo kích thước từ vựng, feed-forward đến các thành phần feed-forward, được tổng kết trên các lớp khác nhau, tính-toán-qkvo đến projection của biểu diễn hiện tại thành query, key, value và projection đầu ra cuối cùng, attention đến phép toán softmax thực tế và loại-bỏ-token đến tính toán bổ sung cần thiết để tính Qℓ int,Kℓ int và thực hiện loại bỏ qua Phương trình (6). (Phải) Yêu cầu bộ nhớ khi cache các activation trước đó (key và value). Khi triển khai loại bỏ, các key tương tác Kℓ int phải được cache bổ sung.

[Biểu đồ và hình ảnh]

Hình 7: Chúng tôi đo thông lượng sử dụng kích thước batch tối ưu trên GPU NVIDIA RTX A5000. (Trái) Thông lượng tính bằng token trên giây cho các mô hình và mức độ sparsity khác nhau (trên) được tính trung bình trên các token cho kích thước ngữ cảnh từ 1 đến 1024 và (dưới) khi kích thước ngữ cảnh là 1000 token. (Phải) Thông lượng trung bình (trên) cho kích thước ngữ cảnh thay đổi cho mô hình GPT-2-medium và thời gian trung bình (dưới) cho mỗi bước sinh cho kích thước ngữ cảnh thay đổi. Vì các mô hình của chúng tôi yêu cầu ít bộ nhớ hơn đáng kể, một kích thước batch lớn hơn có thể được chứa, nơi các phần lớn của lợi ích thông lượng có thể được quy cho.

Hiệu suất Zero-Shot. Để kiểm tra khả năng mô hình tổng quát và bổ sung cho các đánh giá perplexity, chúng tôi cung cấp kết quả trên một số nhiệm vụ zero-shot [Dettmers et al., 2022] trong Hình 5. Các xu hướng tương tự nói chung; phương pháp của chúng tôi giữ lại hoặc thậm chí vượt trội so với hiệu suất của baseline dense, ngay cả đối với các trường hợp có sparsity cao. Các nhiệm vụ này liên quan đến các tình huống trong đó mô hình được yêu cầu thực hiện mà không có bất kỳ huấn luyện cụ thể hoặc tiếp xúc trước với miền mục tiêu. Các kết quả thu được xác thực rằng khả năng tổng quát của các mô hình có thể được giữ lại, ngay cả dưới mức độ sparsity cao.

Phân tích Tính toán. Chúng tôi phân tích các lợi ích về FLOPs và bộ nhớ cần thiết khi sinh các chuỗi mới do caching trong Hình 6. Cơ chế loại bỏ của chúng tôi đưa ra chi phí tính toán bổ sung cho việc tính toán Qℓ int,Kℓ int và logic đằng sau việc loại bỏ qua Phương trình (6). Do tham số r được chọn tương đối nhỏ, tức là chiều đầu ra của các trọng số tương tác Wℓ Qint,Wℓ Kint, những chi phí này vẫn tối thiểu. Mặc dù lợi ích FLOPs thô khi sử dụng các mô hình thưa không có vẻ rất đáng kể, như đã nêu trước, suy luận chủ yếu bị ràng buộc bởi bộ nhớ. Do đó attention chiếm một tỷ lệ đáng kể thời gian suy luận thực tế [Dao et al., 2022]. Ngược lại, các phép nhân ma trận dense được sử dụng cho tất cả các projection tuyến tính rất hiệu quả. Mặt khác, lợi ích về bộ nhớ là đáng kể, vì bộ nhớ cần thiết cho caching là một hàm tuyến tính đối với sparsity, với độ dốc âm. Các giải pháp thưa hơn do đó sẽ bổ sung cho phép chúng ta sinh nhiều chuỗi hơn theo cách batch. Điều này đặc biệt liên quan đến các mô hình lớn hơn, cũng như các chuỗi dài hơn, nơi batch decoding là một thách thức lớn [Shazeer, 2019].

Thông lượng. Chúng tôi chứng minh cách giảm ngữ cảnh và yêu cầu bộ nhớ có thể dẫn đến thông lượng thời gian thực tế đáng kể trong Hình 7. Ban đầu, các mạng đã được cắt giảm của chúng tôi chậm hơn về latency đối với độ dài ngữ cảnh nhỏ, vì chi phí bổ sung liên quan đến logic đằng sau việc cắt giảm. Tuy nhiên, chúng nhanh chóng vượt qua baseline dense có khó khăn khi kích thước ngữ cảnh tăng. Điều này xác minh thực tế rằng mặc dù lợi ích FLOPs thô trông không đáng kể, trên thực tế, điều này dẫn đến lợi ích đáng kể do profile bộ nhớ cụ thể của suy luận Transformer. Quan trọng, các mạng đã được cắt giảm của chúng tôi có thể hỗ trợ kích thước batch lớn hơn nhiều, dẫn đến lợi ích thông lượng đáng kể. Cụ thể hơn, đối với kích thước ngữ cảnh dài, mô hình GPT-2-small của chúng tôi cung cấp thêm 98% biên độ thông lượng cho mất perplexity chỉ 0.316, so với đối tác dense. Tương tự, mô hình GPT-2-medium của chúng tôi có thể mang lại 189% thông lượng bổ sung chỉ với mất 0.084 perplexity cho kích thước ngữ cảnh 1000 token. Cụ thể, cùng mô hình (cho γ= 1.0) cung cấp thông lượng cao hơn so với mô hình GPT-2-small, trong khi đạt được perplexity thấp hơn 3.769. Khi các cửa sổ ngữ cảnh trở nên lớn hơn từng ngày trong các mô hình state-of-the-art, chúng tôi mong đợi những lợi ích này trở nên liên quan hơn nữa.

Khả năng diễn giải. Hình 8 cung cấp hiểu biết về khía cạnh khả năng diễn giải của quá trình ra quyết định của mô hình. Quan sát thấy rằng việc loại bỏ token chủ yếu xảy ra khi gặp stop word (dấu câu), phù hợp với trực quan rằng thông tin cục bộ trong một câu trở nên ít liên quan hơn sau khi hoàn thành. Hơn nữa, đáng chú ý rằng các lớp ở độ sâu khác nhau thể hiện các hành vi khác biệt, củng cố lý do của chúng tôi để phân tích các quyết định loại bỏ token trên độ sâu.

9

--- TRANG 8 ---
[Nội dung hình ảnh được mô tả]

Hình 8: (Trên) Ví dụ về các token bị cắt giảm cho lớp 5 cho mô hình GPT-2-small được fine-tune với γ−0.3 trong quá trình sinh. Hầu hết việc cắt giảm được kích hoạt bởi dấu câu. (Dưới-trái) Chúng tôi tính xác suất của các token được giữ trong ngữ cảnh dựa trên từ loại (POS) của các từ mà chúng tương ứng. (Dưới-giữa) Hầu hết việc loại bỏ được gây ra bởi các token tương ứng với dấu câu, nhưng các lớp khác biệt hành xử khác nhau. (Dưới-phải) Ví dụ về số lượng token bị cắt giảm theo id vị trí của token, cho 2 lớp của GPT-2-small.

Sự khác biệt trong phân phối sparsity trên các độ sâu khác nhau chỉ ra sự cần thiết tiến hành nghiên cứu khả năng diễn giải bổ sung để có được hiểu biết có giá trị trong các tương tác của các token trong mô hình. Chúng tôi cung cấp thêm hiểu biết theo hướng này trong Phụ lục C.

5 Thảo luận
Chúng tôi đã đề xuất Adaptively Sparse Attention, một phương pháp mới để cắt giảm ngữ cảnh một cách động trong các kiến trúc Transformer decoder-only. Kết quả của chúng tôi chỉ ra rằng kỹ thuật của chúng tôi hoạt động tốt so với các baseline cạnh tranh về tỷ lệ giữa perplexity và sparsity của các trọng số attention. Đáng chú ý, phương pháp của chúng tôi cũng giảm đáng kể yêu cầu tính toán và bộ nhớ mà không ảnh hưởng đến hiệu suất cuối cùng. Chúng tôi thực tế cho thấy những lợi ích này đạt được hơn gấp đôi thông lượng trong các trường hợp. Adaptively sparse attention đi kèm với hai lợi thế thực tế bổ sung: đầu tiên, nó có thể được tích hợp liền mạch vào các mô hình đã được huấn luyện trước hiện có qua một bước fine-tuning rẻ; thứ hai, nó đại diện cho một đóng góp trực giao cho dòng nghiên cứu đang phát triển nhằm tăng mức độ hiệu quả của các LLM hiện đại. Như vậy, chúng tôi hình dung sự kết hợp của nó với các kỹ thuật hiện có như cắt giảm trọng số và quantization sẽ là một con đường đầy hứa hẹn cho nghiên cứu tương lai.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài với nhiều nghiên cứu và bài báo khoa học]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo và các phụ lục]
