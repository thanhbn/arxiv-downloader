# 2306.04897.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/context-compression/2306.04897.pdf
# Kích thước tệp: 3035275 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đa Tỷ Lệ Và Hợp Nhất Token: Làm Cho ViT
Của Bạn Hiệu Quả Hơn
Zhe Bian, Zhe Wang, Wenqiang Han, Kangping Wang∗
Đại học Jilin
{bianzhe21,hanwq20}@mails.jlu.edu.cn, {wz2000,wangkp}@jlu.edu.cn
Tóm tắt
Kể từ khi ra đời, Vision Transformer (ViT) đã nổi lên như một mô hình phổ biến trong
lĩnh vực thị giác máy tính. Tuy nhiên, cơ chế self-attention đa đầu (MHSA)
trong ViT rất tốn kém về mặt tính toán do việc tính toán mối quan hệ giữa tất cả các token. Mặc dù một số kỹ thuật giảm thiểu chi phí tính toán bằng cách loại bỏ token, điều này cũng dẫn đến mất thông tin tiềm năng từ những token đó. Để giải quyết những vấn đề này, chúng tôi đề xuất một phương pháp cắt tỉa token mới giữ lại thông tin từ các token không quan trọng bằng cách hợp nhất chúng với các token quan trọng hơn, từ đó giảm thiểu tác động của việc cắt tỉa đối với hiệu suất mô hình. Các token quan trọng và không quan trọng được xác định bởi điểm số quan trọng của chúng và được hợp nhất dựa trên điểm số tương tự. Hơn nữa, các đặc trưng đa tỷ lệ được khai thác để biểu diễn hình ảnh, được hợp nhất trước giai đoạn cắt tỉa token để tạo ra các biểu diễn đặc trưng phong phú hơn. Quan trọng là, phương pháp của chúng tôi có thể được tích hợp liền mạch với các ViT khác nhau, nâng cao khả năng thích ứng của chúng. Bằng chứng thực nghiệm chứng minh hiệu quả của phương pháp chúng tôi trong việc giảm ảnh hưởng của việc cắt tỉa token đối với hiệu suất mô hình. Ví dụ, trên bộ dữ liệu ImageNet, nó đạt được mức giảm 33% chi phí tính toán đáng kể trong khi chỉ gây ra mức giảm 0,1% độ chính xác trên DeiT-S.
1 Giới thiệu
Kiến trúc transformer [24] đã giới thiệu khả năng mô hình hóa các mối quan hệ toàn cục, điều không có trong các phương pháp dựa trên tích chập trước đây. Điều này đã dẫn đến hiệu suất ấn tượng trên nhiều nhiệm vụ thị giác máy tính, bao gồm phân loại hình ảnh [9,22,33], phân đoạn ngữ nghĩa [27, 23], phát hiện đối tượng [3,15], và tạo ảnh [11]. Hơn nữa, một loạt các kỹ thuật huấn luyện có giám sát, không giám sát và thay thế [28,7,2] đã được phát triển cho những nhiệm vụ này.
Tuy nhiên, độ phức tạp tính toán bậc hai của Vision Transformer (ViT), xuất phát từ các phụ thuộc tầm xa dày đặc giữa các token hình ảnh, tạo ra một thách thức đáng kể về chi phí tính toán. Nói chung, ViTs đòi hỏi nhiều vòng lặp huấn luyện và bộ dữ liệu lớn hơn so với mạng nơ-ron tích chập (CNNs).
Để giải quyết gánh nặng tính toán của ViTs, các nhà nghiên cứu đã phát triển nhiều kỹ thuật khác nhau để giảm thông tin đặc trưng bên trong mô hình. Một chiến lược hiệu quả liên quan đến việc đánh giá tầm quan trọng của token và tiến hành cắt tỉa có chọn lọc. DynamicViT [20] giới thiệu một mô-đun để xác định tầm quan trọng của token, trong khi EViT [16] tận dụng token lớp để đánh giá tầm quan trọng của các token khác. Evo-ViT [30] xem xét tầm quan trọng của token lớp toàn cục và cập nhật các token không quan trọng một cách khác biệt. AS-ViT [17] sử dụng một ngưỡng có thể học để điều khiển thích ứng số lượng token được giữ lại, và AdaViT [19] giới thiệu các mô-đun để đánh giá tầm quan trọng của khối, đầu và token, tinh giản mô hình từ nhiều khía cạnh. Mặc dù nhiều phương pháp này trực tiếp loại bỏ các token không quan trọng, một số hợp nhất chúng dựa trên điểm số quan trọng của chúng. Tuy nhiên, quan trọng là cần lưu ý rằng thông tin bị loại bỏ có thể có giá trị cho các nhiệm vụ phân loại, như đã lập luận trong [26]. Do đó, việc áp dụng các kỹ thuật xử lý hiệu quả hơn cho các token không quan trọng là cần thiết, thay vì chỉ đơn thuần loại bỏ hoặc hợp nhất chúng. Phương pháp này bảo toàn càng nhiều thông tin càng tốt trong khi giảm thiểu tác động của việc cắt tỉa token đối với độ chính xác mô hình.

--- TRANG 2 ---
Hình 1: Các phương pháp xử lý khác nhau cho các token không quan trọng, (a) Chỉ các token quan trọng được bảo tồn và các token không quan trọng bị loại bỏ. (b) Các token quan trọng được giữ lại, và các token không quan trọng được hợp nhất thành một token mới. (c) Các token không quan trọng và quan trọng được hợp nhất.

Dựa trên những phát hiện trước đây, chúng tôi trình bày một phương pháp mới cho việc cắt tỉa token trong bài báo này. Đầu tiên, chúng tôi đánh giá mức độ liên quan của các token trong các lớp cụ thể. Sau đó, những token này được phân loại thành hai nhóm: quan trọng và không quan trọng, dựa trên điểm số quan trọng tương đối của chúng. Để bảo toàn càng nhiều thông tin token càng tốt đồng thời giảm số lượng token, chúng tôi giới thiệu một Mô-đun Hợp Nhất Token sáng tạo. Mô-đun này tính toán các chỉ số tương tự giữa các token quan trọng và không quan trọng, và hợp nhất chúng dựa trên các chỉ số đó. Hình 1 minh họa sự khác biệt giữa phương pháp của chúng tôi với các công trình trước đây.

Để cải thiện độ chính xác của biểu diễn đặc trưng, chúng tôi lấy cảm hứng từ các phương pháp đột phá được giới thiệu trong CrossViT [5] và MPViT [14]. Trong phương pháp của chúng tôi, chúng tôi tích hợp các đặc trưng đa tỷ lệ trong việc nhúng token để đạt được một tập đặc trưng phong phú hơn. Để chống lại mất mát độ chính xác tiềm năng trong quá trình cắt tỉa token, chúng tôi căn chỉnh và hợp nhất các đặc trưng đa tỷ lệ, từ đó nâng cao thông tin đặc trưng tổng thể. Tuy nhiên, việc kết hợp các đặc trưng đa tỷ lệ đòi hỏi tăng tính toán, điều được giải quyết bằng một kỹ thuật sáng tạo. Chiến lược này tách biệt phương pháp của chúng tôi khỏi các công trình trước đây trong CrossViT [5] và MPViT [14].

Chúng tôi giới thiệu một kỹ thuật cắt tỉa token sáng tạo đạt được mức giảm 33% FLOPs và tăng 0,1% độ chính xác khi áp dụng cho DeiT-S [22]. Đáng chú ý, phương pháp của chúng tôi tương thích với hầu hết các phương pháp cắt tỉa hiện có. Kết quả thử nghiệm cho thấy khi tỷ lệ token bị cắt tỉa tăng, hiệu suất của mô-đun cải thiện đáng kể. Ví dụ, kỹ thuật của chúng tôi cải thiện độ chính xác 0,2% khi áp dụng cho EViT [16] với tỷ lệ giữ 0,7, và 0,7% với tỷ lệ giữ 0,5.

Tóm lại, đóng góp của chúng tôi gồm hai mặt:
• Chúng tôi đề xuất một chiến lược cắt tỉa token mới sử dụng hiệu quả thông tin đầu vào gốc. Phương pháp của chúng tôi tạo ra sự cân bằng giữa độ chính xác và tốc độ mô hình, bảo toàn càng nhiều thông tin gốc càng khả thi trong quá trình cắt tỉa.
• Chúng tôi cải thiện độ chính xác mô hình bằng cách kết hợp các đặc trưng đa tỷ lệ và giảm tính toán trước việc cắt tỉa token.

2 Công trình liên quan
Vision Transformers. Transformer [24] đã được áp dụng thành công cho các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP), đạt được kết quả tiên tiến. Hơn nữa, ViT [9] đã đạt được kết quả ấn tượng bằng cách chuyển đổi hình ảnh thành các token tuần tự và xử lý chúng thông qua kiến trúc transformer. Transformers gần đây đã được áp dụng vào lĩnh vực thị giác máy tính (CV), tạo ra kết quả đầy hứa hẹn trong phân loại hình ảnh [22], phát hiện đối tượng [3], phân đoạn ngữ nghĩa [34], và các nhiệm vụ khác [11]. Đồng thời, một số mô hình xương sống có hiệu suất cao đã xuất hiện. Ví dụ, DeiT [22] khai thác chưng cất kiến thức để huấn luyện hiệu quả ViT [9] chỉ trên ImageNet, trong khi Swin Transformer [18] kết hợp các hoạt động cửa sổ cục bộ và trượt để giới thiệu độ lệch quy nạp cho ViT [9], do đó giảm độ phức tạp tính toán của nó. LV-ViT [12] nâng cao độ chính xác mô hình bằng cách tính toán tổn thất cho tất cả các token.

--- TRANG 3 ---
ViTs Hiệu quả. Khác với ngôn ngữ tự nhiên, hình ảnh thường chứa một lượng lớn thông tin dư thừa. Do đó, đối với các mô hình transformer có nhu cầu tính toán cao, các kỹ thuật giảm gánh nặng tính toán bằng cách cắt tỉa thông tin dư thừa từ hình ảnh đã trở nên ngày càng quan trọng. Nhiều phương pháp khác nhau để cắt tỉa thông tin dư thừa từ hình ảnh đã được phát triển trong những năm gần đây. Một phương pháp như vậy tập trung vào việc cắt tỉa chỉ thông tin token đầu vào. DynamicViT [20] đánh giá tầm quan trọng của các token đầu vào bằng một mô-đun có thể học và xác định số lượng token cần giữ lại theo tỷ lệ giữ đã định trước. EViT [16] và Evo-ViT [30] đánh giá tầm quan trọng của các token khác bằng cách xem xét đặc điểm của token lớp. Phương pháp trước hợp nhất các token không quan trọng thành một token mới, trong khi phương pháp sau giữ lại các token không quan trọng đồng thời sử dụng cập nhật nhanh để duy trì tính toàn vẹn của luồng thông tin. A-ViT [31] điều chỉnh động số lượng token theo độ phức tạp của đầu vào, từ đó kiểm soát độ phức tạp tính toán của mô hình. AS-ViT [17] nhấn mạnh vai trò của các đầu riêng biệt và sử dụng một ngưỡng có thể học để xác định việc giữ lại token. Một phương pháp khác liên quan đến việc cắt tỉa token, đầu, khối và các thành phần mô hình khác bằng cách phân tích sự dư thừa từ nhiều góc độ. AdaViT [19] giới thiệu các tham số có thể học để xác định liệu khối, đầu và token có nên được cắt tỉa hay không. MIA-Former [32] giới thiệu một MIA-Controller để quyết định có nên bỏ qua một khối hay không; nếu không, một mô-đun có thể học được sử dụng để xác định việc cắt tỉa đầu và token.

3 Phương pháp
3.1 Kiến thức cơ bản
Lấy cảm hứng từ việc triển khai thành công của Transformer [24] trong lĩnh vực xử lý ngôn ngữ tự nhiên, Vision Transformer (ViT) [9] chuyển đổi một hình ảnh thành một chuỗi các token, tương tự như các từ trong một câu. Nó cũng kết hợp một token lớp (CLS) để có được biểu diễn hình ảnh. Để phân biệt giữa các token ở các vị trí khác nhau, các embedding vị trí được tích hợp. Đầu vào kết quả sau đó được đưa vào các bộ mã hóa transformer xếp chồng, với các đặc trưng đầu ra tạo thành nền tảng cho các nhiệm vụ downstream. Thành phần cốt lõi của ViT, bộ mã hóa transformer, bao gồm self-attention đa đầu (MHSA) và một mạng feed-forward (FFN). MHSA được sử dụng để thu được query, key và value thông qua ánh xạ tuyến tính của thông tin đầu vào. Sau đó, query và key được nhân để tạo ra một bản đồ attention, cuối cùng được nhân với value và xuất ra qua ánh xạ ma trận. Đối với một đầu vào X∈RN×D đến ViT, quá trình MHSA có thể được biểu diễn toán học như sau:

Qi=XiWi^Q;Ki=XiWi^K;Vi=XiWi^V,
Attention(Qi, Ki, Vi) = Softmax(QiKT_i/√d)Vi,
MHSA(X) = WO concat[Attention(Qi, Ki, Vi)]^H_{i=1},(1)

Để i∈{1,2, ..., H} biểu thị chỉ số của đầu thứ i trong cơ chế attention đa đầu. Để Xi, Qi, Ki, Vi∈RN×d là các ma trận đầu vào, query, key và value của đầu thứ i, trong đó N là số lượng token, D là chiều embedding, và d là chiều của embedding của một đầu đơn.

FFN(X) = Sigmoid(Linear(GeLU(Linear(X)))). (2)

3.1.1 Độ phức tạp tính toán
Trong kiến trúc ViT, chi phí tính toán của các mô-đun MHSA và FFN được cho bởi O(4ND²+ 2N²D) và O(8ND²) tương ứng. Do đó, tổng chi phí tính toán của một khối transformer có thể được ước tính là O(12ND²+ 2N²D). Các phương pháp cắt tỉa token có thể được sử dụng để giảm số lượng token theo một tỷ lệ phần trăm nhất định, ký hiệu là λ%. Kết quả là, các phương pháp như vậy có thể giảm hiệu quả FLOPs của một khối transformer ít nhất λ%.

3.2 Lựa chọn token
Trong bối cảnh phân loại hình ảnh, ViT chỉ dựa vào token lớp cho thông tin liên quan đến phân loại. Cụ thể, chúng tôi sử dụng công thức 3 sau:

XCLS = Softmax(QCLSKT/√d)V. (3)

--- TRANG 4 ---
Hình 2: Cắt tỉa token trong một bộ mã hóa transformer đơn. Giống như EViT, chúng tôi sử dụng giá trị của attention CLS làm điểm số để đánh giá tầm quan trọng của mỗi token. Và xác định các token quan trọng hàng đầu-k. Sau đó, chúng tôi sử dụng độ tương tự cosin của các token quan trọng và không quan trọng làm tiêu chí đánh giá tương tự và hợp nhất chúng.

trong đó QCLS đại diện cho vector query của token lớp. Token lớp XCLS tạo thành một tổ hợp tuyến tính của tất cả các giá trị token, cho phép điểm số attention của nó đóng gói tầm quan trọng tương đối của các token khác đối với kết quả phân loại. Phù hợp với các phương pháp trước đây [20,16,30], chúng tôi chỉ lấy trung bình các chiều trong đầu. Trong quá trình lựa chọn token, chúng tôi bảo toàn N token đầu tiên có điểm số attention cao nhất làm token quan trọng, trong khi các token còn lại được phân loại là token không quan trọng; token lớp được coi là token quan trọng theo mặc định.

3.3 Hợp nhất token
Hầu hết các kỹ thuật cắt tỉa token phụ thuộc vào một tiêu chí định trước để quyết định giữ lại hay cắt tỉa một token. Tuy nhiên, việc giảm dữ liệu đầu vào này cho các lớp transformer tiếp theo có thể dẫn đến mất mát độ chính xác đáng kể khi nhiều token bị cắt tỉa. Để giải quyết vấn đề này, phương pháp của chúng tôi nhằm ưu tiên xử lý thông tin token quan trọng đồng thời quản lý thông tin token không quan trọng. Chúng tôi đề xuất hợp nhất thông tin token không quan trọng vào token quan trọng tương tự nhất, như được mô tả trong Hình 2.

Trong phương pháp của chúng tôi, các token được phân loại thành hai danh mục: quan trọng và không quan trọng. Để ngăn chặn việc cắt tỉa các token không quan trọng ảnh hưởng tiêu cực đến độ chính xác của mô hình, một giai đoạn hợp nhất được giới thiệu cho hai danh mục này trước khi cắt tỉa. Độ tương tự cosin được sử dụng làm chỉ số chính để tính toán độ tương tự giữa các danh mục, được chọn vì tính triển khai đơn giản. Chúng tôi xác định độ tương tự giữa mỗi token không quan trọng và tập hợp các token quan trọng bằng cách tính toán độ tương tự cosin của token không quan trọng với mỗi token trong tập hợp quan trọng. Tiếp theo, đối với mỗi token không quan trọng, chúng tôi xác định token quan trọng có độ tương tự cosin cao nhất làm đối tác tương tự nhất và hợp nhất các token. Phương pháp hợp nhất có trọng số được sử dụng để thích ứng với mức độ liên quan khác nhau của các token. Trọng số được xác định bởi mục tương ứng trong token lớp. Công thức 4 đại diện cho token đã hợp nhất.

mj = (wjij + Σk∈Sj wkuk)/(wj + Σk∈Sj wk). (4)

Trong đó wj biểu thị điểm số quan trọng của token quan trọng ij, wk biểu thị điểm số quan trọng của token không quan trọng uk và Sj biểu thị tập hợp các token không quan trọng được hợp nhất với token quan trọng thứ j, mj biểu thị token đã hợp nhất thứ j. Kích thước của các token đã hợp nhất được tính từ tỷ lệ giữ.

3.4 Đặc trưng đa tỷ lệ
Để đạt được độ chính xác cao hơn, các token đầu vào nên trình bày càng nhiều đặc trưng càng tốt trước khi cắt tỉa token. Một phương pháp trích xuất đặc trưng đa tỷ lệ được đề xuất để có được các biểu diễn đặc trưng phong phú hơn, với kiến trúc mô hình hoàn chỉnh được minh họa trong Hình 3. Cụ thể, chúng tôi tạo ra

--- TRANG 5 ---
Hình 3: Kiến trúc tổng thể của mô hình transformer đa tỷ lệ.

hai nhóm đặc trưng có chiều embedding giống nhau nhưng số lượng token khác nhau trong quá trình embedding đặc trưng. Nhóm có nhiều token hơn được gọi là đặc trưng tỷ lệ cao, trong khi nhóm có ít token hơn được gọi là đặc trưng tỷ lệ thấp. Chúng tôi gán mã vị trí có thể học cho mỗi nhóm đặc trưng. Trước giai đoạn cắt tỉa token, chúng tôi hợp nhất thông tin từ cả hai nhóm đặc trưng để có được các đặc trưng đại diện hơn. Trong quá trình hợp nhất đặc trưng đa tỷ lệ, chúng tôi tăng mẫu các đặc trưng tỷ lệ thấp bằng cách sử dụng phương pháp nội suy gần nhất và tích chập để căn chỉnh chúng với các đặc trưng tỷ lệ cao về số lượng token. Để giảm số lượng tham số và đảm bảo hiệu quả, chúng tôi sử dụng mô-đun LKA [10] để biến đổi các đặc trưng đã tăng mẫu. Cuối cùng, chúng tôi cộng các đặc trưng tỷ lệ thấp đã tăng mẫu và các đặc trưng tỷ lệ cao trong chiều token và tích hợp thông tin vị trí mới vào các đặc trưng kết quả bằng mô-đun PEG [6]. Mô tả dạng công thức của quá trình này như sau:

X = PEG(Xh + LKA(UP(Xl))). (5)

Trong đó Xh và Xl tương ứng đại diện cho các đặc trưng tỷ lệ cao và đặc trưng tỷ lệ thấp, UP đại diện cho phép toán nội suy gần nhất.

3.4.1 Giảm tính toán
Việc kết hợp các đặc trưng đa tỷ lệ trong visual transformers không thể tránh khỏi làm tăng nhu cầu tính toán. Để giảm thiểu chi phí này trong khi duy trì độ chính xác cao, chúng tôi đề xuất một sửa đổi đơn giản cho các mô-đun attention trong các khối đầu tiên của hệ thống phân cấp đặc trưng tỷ lệ cao. Cụ thể, chúng tôi đầu tiên giảm mẫu các đặc trưng tỷ lệ cao trước khi xử lý chúng với mô-đun MHSA. Các token đã giảm mẫu sau đó được đưa vào mô-đun MHSA, và các vector đặc trưng kết quả được tăng mẫu về tỷ lệ ban đầu, giữ token lớp không thay đổi trong toàn bộ quá trình. Mô tả dạng công thức của quá trình này như sau:

Xh = Xh + UP(MHSA(DOWN(Xh))). (6)

Trong đó Xh biểu thị đặc trưng tỷ lệ cao, DOWN đại diện cho một phép toán giảm mẫu đơn giản, và UP đề cập đến phương pháp nội suy gần nhất. Cập nhật này đảm bảo rằng yêu cầu tính toán đặc trưng đa tỷ lệ của mỗi khối xấp xỉ tương đương với những yêu cầu của ViT gốc.

4 Thử nghiệm
4.1 Chi tiết triển khai
Để duy trì tính nhất quán, chúng tôi đã huấn luyện tất cả các mô hình trong thử nghiệm của chúng tôi trên bộ dữ liệu ImageNet-1k [8], chứa tập huấn luyện 12 triệu hình ảnh và tập kiểm tra 50k hình ảnh. Để nâng cao hiệu suất, chúng tôi kết hợp các mô-đun Lựa chọn Token và Hợp nhất Token vào các lớp thứ 4, 7 và 10 của DeiT-T, DeiT-S và DeiT-B [22], cũng như vào các lớp thứ 5, 9 và 13 của LV-ViT-S [12], áp dụng cùng chiến lược tối ưu hóa như bài báo gốc. Chúng tôi sử dụng các đặc trưng tỷ lệ cao với 196 token và các đặc trưng tỷ lệ thấp với 49 token, mỗi loại chứa một token lớp. Để so sánh công bằng với EViT [16], chúng tôi giảm dần tỷ lệ giữ của các token chú ý từ 1 xuống giá trị mục tiêu bằng lịch cosin. Theo cài đặt của các phương pháp khác, chúng tôi huấn luyện tất cả các mô hình trong 300

--- TRANG 6 ---
Bảng 1: So sánh với các phương pháp cắt tỉa token hiện có.
Mô hình Tham số(M) FLOPs(G) FLOPs ↓(%) Top-1(%)
DeiT-T [22] 5.7 1.3 0.0 72.2
DynamicViT [20] 5.9 0.9 30.8 71.2(-1.0)
SP-ViT [13] 5.7 0.9 30.8 72.1(-0.1)
Evo-ViT [30] 5.7 0.8 38.5 72.0(-0.2)
Ours-DeiT-T 5.8 0.8 38.5 72.7(+0.5)
DeiT-S [22] 22.1 4.6 0.0 79.8
ToMe [1] 22.1 2.7 41.3 79.4(-0.4)
DynamicViT [20] 22.8 2.9 37.0 79.3(-0.5)
A-ViT [31] 22.1 3.6 21.7 78.6(-1.2)
Evo-ViT [30] 22.1 3.0 34.8 79.4(-0.4)
EViT [16] 22.1 3.0 34.8 79.5(-0.3)
AS-ViT [17] 22.1 3.0 34.8 79.6(-0.2)
Ours-DeiT-S 22.2 3.1 32.6 79.7(-0.1)
DeiT-B [22] 86.6 17.5 0.0 81.8
DynamicViT [20] - 11.2 36.0 81.3(-0.5)
Evo-ViT [30] 86.6 11.5 34.2 81.3(-0.5)
EViT [16] 86.6 11.5 34.2 81.3(-0.5)
AS-ViT [17] 88.6 11.2 36.0 81.4(-0.4)
Ours-DeiT-B 86.6 11.5 34.2 81.5(-0.3)

epoch trên 8 GPU NVIDIA RTX 3090, và đo thông lượng của chúng bằng một GPU NVIDIA RTX 3090 duy nhất với kích thước batch 128.

4.2 Kết quả chính
4.2.1 So sánh với các phương pháp tiên tiến
Như được hiển thị trong Bảng 1, chúng tôi cung cấp phân tích so sánh phương pháp cắt tỉa token của chúng tôi với các kỹ thuật tiên tiến khác trên DeiT [22], báo cáo độ chính xác top-1 và FLOPs để đánh giá hiệu suất. Phương pháp của chúng tôi vượt trội hơn các phương pháp trước đây, mang lại kết quả tốt hơn trong khi duy trì chi phí tính toán hợp lý. Cụ thể, phương pháp của chúng tôi giảm độ phức tạp tính toán của DeiT-T [22] 35% trong khi cải thiện độ chính xác mô hình 0,5%. Chúng tôi cũng áp dụng phương pháp của chúng tôi cho LV-ViT [12], đây là một kiến trúc sâu-hẹp. Như được minh họa trong Bảng 2, phương pháp của chúng tôi cho phép LV-ViT [12] đạt được sự cân bằng tối ưu giữa độ chính xác và tốc độ so với các mô hình transformer khác.

4.2.2 So sánh với các phương pháp hiện có trên mỗi tỷ lệ giữ
Chúng tôi đánh giá độ chính xác của phương pháp chúng tôi và hai phương pháp khác [20,16] ở các tỷ lệ giữ khác nhau, như được mô tả trong Hình 5. Kết quả cho thấy phương pháp của chúng tôi vượt trội hơn các phương pháp khác ở cùng tỷ lệ giữ, với những cải thiện đáng kể hơn ở tỷ lệ giữ thấp hơn. Điều này có thể là do phương pháp của chúng tôi hợp nhất thông tin của các token không quan trọng vào các token quan trọng, giảm mất thông tin. Điều này trái ngược với Dynamic-ViT [20], phương pháp trực tiếp loại bỏ các token không quan trọng, hoặc EViT [16], phương pháp hợp nhất chúng thành một token duy nhất. Do đó, tỷ lệ giữ càng thấp, càng nhiều token được hợp nhất và lợi ích của phương pháp chúng tôi càng lớn. Hơn nữa, chúng tôi kết hợp các đặc trưng đa tỷ lệ trước khi cắt tỉa token, dẫn đến hiệu suất được cải thiện so với baseline ở tỷ lệ giữ cao.

--- TRANG 7 ---
Bảng 2: So sánh với các mô hình tiên tiến.
Phương pháp Tham số(M) FLOPs(G) Top-1(%)
ViT-B [9] 86.6 4.6 77.9
DeiT-S [22] 29.0 4.5 79.8
Swin-T [18] 50.0 4.6 81.3
T2T-ViT-14 [33] 21.5 4.8 81.5
CvT-21 [25] 31.5 7.1 82.5
DW-T [21] 30.0 5.2 82.0
Cross-ViT-S [5] 26.7 5.6 81.0
CoaT-Lite Small [29] 20.0 4.0 81.9
RegionViT-S [4] 30.3 5.3 82.6
LV-ViT-S [12] 26.2 6.6 83.3
DynamicViT-LV-S [20] 26.9 3.7 82.0
EViT-LV-S [16] 26.2 3.9 82.5
AS-LV-S [17] 26.2 3.9 82.6
Ours-LV-S 26.2 3.9 82.8

Hình 4: So sánh với các mô hình tiên tiến.
Hình 5: So sánh với các phương pháp hiện có trên mỗi tỷ lệ giữ.

4.2.3 Trực quan hóa
Phương pháp của chúng tôi áp dụng một chiến lược mới để giảm chi phí tính toán bằng cách xác định và bảo toàn các token quan trọng đồng thời hợp nhất những token không quan trọng để giảm dư thừa. Để đánh giá hiệu quả của phương pháp chúng tôi, chúng tôi trình bày các trực quan hóa của mỗi giai đoạn trong Hình 6. Kết quả cho thấy phương pháp của chúng tôi thành công trong việc phân biệt các token quan trọng khỏi những token không quan trọng, bằng cách tập trung các token ở các vị trí ảnh hưởng đáng kể đến phân loại, trong khi hợp nhất các yếu tố nền. Ví dụ, trong hình ảnh được phân loại là gấu trúc, phương pháp của chúng tôi gán nhiều token hơn cho gấu trúc sau mỗi lớp cắt tỉa, trong khi liên tục hợp nhất nền. Bằng chứng này mô tả tại sao phương pháp này hiệu quả.

--- TRANG 8 ---
Hình 6: Trực quan hóa quá trình cắt tỉa token trên DeiT-S với tỷ lệ giữ 0,5. Sử dụng sơ đồ mã hóa màu, các token đã được hợp nhất cùng nhau được biểu diễn bằng cùng một màu, cho phép biểu diễn rõ ràng và trực quan về quá trình cắt tỉa.

Bảng 3: Hiệu quả của từng mô-đun trên EViT-DeiT-S với các tỷ lệ giữ khác nhau.
Phương pháp Top-1 (%) FLOPs(G)
DeiT-S/ η=0.5
baseline 78.5 2.3
+TM 79.1 2.3
+MF 79.2 2.4
DeiT-S/ η=0.7
baseline 79.5 3.0
+TM 79.6 3.0
+MF 79.7 3.1

Bảng 4: Các phương pháp tính toán độ tương tự khác nhau trên DeiT-S.
Phương pháp Top-1 (%) Thông lượng (img/s)
Random 78.8 2137
Attention Map 79.0 2195
L1 Distance 79.1 2033
L2 Distance 79.1 2181
Ours 79.2 2201

4.3 Phân tích loại bỏ
4.3.1 Hiệu quả của từng mô-đun
Chúng tôi đã tích hợp phương pháp của chúng tôi vào EViT [16] để đánh giá hiệu quả của các mô-đun riêng lẻ. Kết quả thử nghiệm được hiển thị trong Bảng 3, trong đó TM biểu thị hợp nhất token và MF đại diện cho hợp nhất đặc trưng đa tỷ lệ. Phân tích của chúng tôi cho thấy việc kết hợp TM cải thiện đáng kể độ chính xác baseline bằng cách bảo toàn thông tin đặc trưng token không quan trọng. Hơn nữa, việc bao gồm MF dẫn đến trích xuất đặc trưng toàn diện hơn và cải thiện độ chính xác baseline.

4.3.2 Các phương pháp tính toán độ tương tự khác nhau
Như được hiển thị trong Bảng 4, chúng tôi đã so sánh một số kỹ thuật để tính toán độ tương tự giữa các token quan trọng và không quan trọng trên DeiT-S [22] khi tỷ lệ giữ là 0,5. Những phương pháp này bao gồm: i) hợp nhất các token quan trọng và không quan trọng một cách ngẫu nhiên; ii) đánh giá độ tương tự bằng cách sử dụng điểm số chéo của các token quan trọng và không quan trọng trong ma trận attention; iii) sử dụng khoảng cách Manhattan để tính toán độ tương tự; iv) sử dụng khoảng cách Euclidean để tính toán độ tương tự; và v) sử dụng độ tương tự cosin. Quan sát của chúng tôi cho thấy rằng, với thông lượng tương tự, phương pháp của chúng tôi thường đạt được cải thiện 0,1% độ chính xác so với các kỹ thuật tính toán độ tương tự thay thế. Điều này ngụ ý rằng độ tương tự cosin phục vụ như một chỉ số hiệu quả để xác định độ tương tự giữa các token quan trọng và

--- TRANG 9 ---
không quan trọng. Hơn nữa, so với các phương pháp khác, độ tương tự cosin có tính toán tương đối đơn giản. Tóm lại, các phát hiện của chúng tôi ủng hộ việc áp dụng độ tương tự cosin để tính toán độ tương tự giữa các token quan trọng và không quan trọng, nhờ vào độ chính xác và hiệu quả tính toán của nó.

5 Kết luận
Trong nghiên cứu này, chúng tôi giới thiệu một mô-đun cắt tỉa token sáng tạo được thiết kế để giảm tác động đến độ chính xác mô hình trong quá trình cắt tỉa token. Bằng cách xác định có chọn lọc các token quan trọng và hợp nhất những token không quan trọng, chúng tôi duy trì độ chính xác gây ra bởi việc cắt tỉa token. Việc tích hợp các đặc trưng đa tỷ lệ tiếp tục tăng độ chính xác mô hình. Kết quả thử nghiệm từ DeiT [22] hỗ trợ hiệu quả của mô-đun được đề xuất. Hơn nữa, kỹ thuật của chúng tôi có thể được kết hợp với phần lớn các phương pháp cắt tỉa token hiện có để nâng cao độ chính xác của chúng với gần như không có chi phí tính toán bổ sung. Phương pháp của chúng tôi đạt được sự cân bằng tốt giữa độ chính xác và tốc độ. Chúng tôi hình dung phương pháp của chúng tôi sẽ có giá trị cho các nhiệm vụ downstream như phát hiện mục tiêu, phân đoạn ngữ nghĩa, và vượt ra ngoài các nhiệm vụ phân loại.

Tài liệu tham khảo
[1]Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, và Judy Hoffman.
Token merging: Your vit but faster. CoRR , abs/2210.09461, 2022.
[2]Zhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika,
Zhuowen Tu, và Stefano Soatto. Semi-supervised vision transformers at scale. Trong NeurIPS , 2022.
[3]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey
Zagoruyko. End-to-end object detection with transformers. Trong ECCV (1) , tập 12346 của Lecture Notes
in Computer Science , trang 213–229. Springer, 2020.
[4]Chun-Fu Chen, Rameswar Panda, và Quanfu Fan. Regionvit: Regional-to-local attention for vision
transformers. Trong ICLR . OpenReview.net, 2022.
[5]Chun-Fu (Richard) Chen, Quanfu Fan, và Rameswar Panda. Crossvit: Cross-attention multi-scale vision
transformer for image classification. Trong ICCV , trang 347–356. IEEE, 2021.
[6]Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, và Chunhua Shen.
Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882 , 2021.
[7]Zhigang Dai, Bolun Cai, Yugeng Lin, và Junying Chen. UP-DETR: unsupervised pre-training for object
detection with transformers. Trong CVPR , trang 1601–1610. Computer Vision Foundation / IEEE, 2021.
[8]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. Trong CVPR , trang 248–255. IEEE Computer Society, 2009.
[9]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và
Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong ICLR ,
2021.
[10] Meng-Hao Guo, Chengze Lu, Zheng-Ning Liu, Ming-Ming Cheng, và Shimin Hu. Visual attention
network. CoRR , abs/2202.09741, 2022.
[11] Yifan Jiang, Shiyu Chang, và Zhangyang Wang. Transgan: Two pure transformers can make one strong
gan, and that can scale up. Trong NeurIPS , trang 14745–14758, 2021.
[12] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, và Jiashi Feng.
All tokens matter: Token labeling for training better vision transformers. Trong NeurIPS , trang 18590–18602,
2021.
[13] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan,
Bin Ren, Hao Tang, Minghai Qin, và Yanzhi Wang. Spvit: Enabling faster vision transformers via
latency-aware soft token pruning. Trong ECCV (11) , tập 13671 của Lecture Notes in Computer Science ,
trang 620–640. Springer, 2022.
[14] Youngwan Lee, Jonghee Kim, Jeffrey Willette, và Sung Ju Hwang. Mpvit: Multi-path vision transformer
for dense prediction. Trong CVPR , trang 7277–7286. IEEE, 2022.
[15] Tianyang Li, Jian Wang, và Tibing Zhang. L-DETR: A light-weight detector for end-to-end object
detection with transformers. IEEE Access , 10:105685–105692, 2022.
[16] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, và Pengtao Xie. Not all patches are
what you need: Expediting vision transformers via token reorganizations. CoRR , abs/2202.07800, 2022.

--- TRANG 10 ---
[17] Xiangcheng Liu, Tianyi Wu, và Guodong Guo. Adaptive sparse vit: Towards learnable adaptive token
pruning by fully exploiting self-attention. CoRR , abs/2209.13802, 2022.
[18] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. Trong ICCV , trang 9992–10002. IEEE,
2021.
[19] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, và Ser-Nam Lim.
Adavit: Adaptive vision transformers for efficient image recognition. Trong CVPR , trang 12299–12308. IEEE,
2022.
[20] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, và Cho-Jui Hsieh. Dynamicvit: Efficient
vision transformers with dynamic token sparsification. Trong NeurIPS , trang 13937–13949, 2021.
[21] Pengzhen Ren, Changlin Li, Guangrun Wang, Yun Xiao, Qing Du, Xiaodan Liang, và Xiaojun Chang.
Beyond fixation: Dynamic window visual transformer. Trong CVPR , trang 11977–11987. IEEE, 2022.
[22] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé
Jégou. Training data-efficient image transformers & distillation through attention. Trong ICML , tập 139
của Proceedings of Machine Learning Research , trang 10347–10357. PMLR, 2021.
[23] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, và Vishal M. Patel. Medical transformer:
Gated axial-attention for medical image segmentation. Trong MICCAI (1) , tập 12901 của Lecture Notes in
Computer Science , trang 36–46. Springer, 2021.
[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, và Illia Polosukhin. Attention is all you need. Trong NIPS , trang 5998–6008, 2017.
[25] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, và Lei Zhang. Cvt:
Introducing convolutions to vision transformers. Trong ICCV , trang 22–31. IEEE, 2021.
[26] Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, và Aleksander Madry. Noise or signal: The role of
image backgrounds in object recognition. Trong ICLR . OpenReview.net, 2021.
[27] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, và Ping Luo. Segformer:
Simple and efficient design for semantic segmentation with transformers. Trong NeurIPS , trang 12077–12090,
2021.
[28] Tongkun Xu, Weihua Chen, Pichao Wang, Fan Wang, Hao Li, và Rong Jin. Cdtrans: Cross-domain
transformer for unsupervised domain adaptation. Trong ICLR . OpenReview.net, 2022.
[29] Weijian Xu, Yifan Xu, Tyler A. Chang, và Zhuowen Tu. Co-scale conv-attentional image transformers.
Trong ICCV , trang 9961–9970. IEEE, 2021.
[30] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng
Xu, và Xing Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. Trong AAAI , trang
2964–2972. AAAI Press, 2022.
[31] Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya, Jan Kautz, và Pavlo Molchanov. A-vit:
Adaptive tokens for efficient vision transformer. Trong CVPR , trang 10799–10808. IEEE, 2022.
[32] Zhongzhi Yu, Yonggan Fu, Sicheng Li, Chaojian Li, và Yingyan Lin. Mia-former: Efficient and robust
vision transformers via multi-grained input-adaptation. Trong AAAI , trang 8962–8970. AAAI Press, 2022.
[33] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis E. H. Tay, Jiashi Feng,
và Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. Trong ICCV ,
trang 538–547. IEEE, 2021.
[34] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, và Yifan Liu. Segvit:
Semantic segmentation with plain vision transformers. Trong NeurIPS , 2022.
