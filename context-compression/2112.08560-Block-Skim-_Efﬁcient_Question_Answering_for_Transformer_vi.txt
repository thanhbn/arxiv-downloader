Block-Skim: Trả Lời Câu Hỏi Hiệu Quả cho Transformer
Yue Guan1,2, Zhengyi Li1,2, Zhouhan Lin1, Yuhao Zhu3, Jingwen Leng1,2, Minyi Guo1,2,
1Đại học Giao thông Thượng Hải
2Viện Qi Zhi Thượng Hải
3Đại học Rochester
fbonboru,hobbit,leng-jw g@sjtu.edu.cn, guo-my@cs.sjtu.edu.cn, lin.zhouhan@gmail.com, yzhu@rochester.edu

Tóm tắt
Các mô hình Transformer đã đạt được kết quả khả quan trên các tác vụ xử lý ngôn ngữ tự nhiên (NLP) bao gồm trả lời câu hỏi trích xuất (QA). Các bộ mã hóa Transformer thông thường được sử dụng trong các tác vụ NLP xử lý các trạng thái ẩn của tất cả các token đầu vào trong đoạn ngữ cảnh qua tất cả các lớp. Tuy nhiên, khác với các tác vụ khác như phân loại chuỗi, việc trả lời câu hỏi được đưa ra không nhất thiết cần tất cả các token trong đoạn ngữ cảnh. Theo động lực này, chúng tôi đề xuất Block-Skim, học cách lướt qua ngữ cảnh không cần thiết ở các lớp ẩn cao hơn để cải thiện và tăng tốc hiệu suất Transformer. Ý tưởng chính của Block-Skim là xác định ngữ cảnh phải được xử lý thêm và những phần có thể được loại bỏ một cách an toàn sớm trong quá trình suy luận. Quan trọng, chúng tôi phát hiện rằng thông tin như vậy có thể được rút ra đầy đủ từ các trọng số tự chú ý bên trong mô hình Transformer. Chúng tôi tiếp tục cắt tỉa các trạng thái ẩn tương ứng với các vị trí không cần thiết sớm ở các lớp thấp hơn, đạt được sự tăng tốc đáng kể về thời gian suy luận. Đáng ngạc nhiên, chúng tôi quan sát thấy rằng các mô hình được cắt tỉa theo cách này vượt trội hơn các đối tác có kích thước đầy đủ. Block-Skim cải thiện độ chính xác của các mô hình QA trên các tập dữ liệu khác nhau và đạt được sự tăng tốc 3× trên mô hình BERT base.

Giới thiệu
Mô hình Transformer (Vaswani et al. 2017) đã đẩy hiệu suất mô hình trên các ứng dụng NLP khác nhau lên một giai đoạn mới bằng cách giới thiệu cơ chế chú ý đa đầu (MHA) (Lin et al. 2017). Hơn nữa, mô hình BERT dựa trên Transformer (Devlin et al. 2018) nâng cao hiệu suất của nó bằng cách giới thiệu việc tiền huấn luyện tự giám sát và đã đạt được độ chính xác tiên tiến trên nhiều tác vụ NLP. Điều này đã làm cho nó trở thành cốt lõi của nhiều mô hình tiên tiến, đặc biệt là trong các mô hình trả lời câu hỏi (QA) gần đây (Huang et al. 2020).

Hiểu biết chính của chúng tôi đối với QA là khi con người trả lời một câu hỏi với một đoạn văn làm ngữ cảnh, họ không dành cùng mức độ hiểu biết cho mỗi câu một cách đều nhau qua toàn bộ đoạn văn. Phần lớn nội dung được lướt qua nhanh chóng với ít chú ý đến nó, có nghĩa là đối với một câu hỏi cụ thể, phần lớn nội dung đều thừa về mặt ngữ nghĩa. Tuy nhiên, trong kiến trúc Transformer, tất cả các token đều trải qua cùng lượng tính toán, điều này gợi ý rằng chúng ta có thể tận dụng điều đó bằng cách loại bỏ nhiều token sớm ở các lớp thấp hơn của Transformer. Sự thừa này ở mức ngữ nghĩa làm sáng tỏ việc giảm hiệu quả độ dài chuỗi ở các lớp cao hơn. Vì chi phí thực thi của tự chú ý tăng theo bậc hai w.r.t. độ dài chuỗi, việc cắt tỉa ở mức ngữ nghĩa này có thể giảm đáng kể thời gian tính toán cho các ngữ cảnh dài.

Để khai thác hiệu quả từ hiểu biết này, chúng tôi đề xuất trước tiên chia ngữ cảnh thành các khối, sau đó học một bộ phân loại để chấm dứt những khối ít liên quan sớm ở các lớp thấp hơn bằng cách nhìn vào các trọng số chú ý như được hiển thị trong Hình 1. Hơn nữa, với sự giám sát của các vị trí câu trả lời thật, một mô hình học cùng lúc việc loại bỏ các khối ngữ cảnh cũng như trả lời câu hỏi thể hiện hiệu suất tốt hơn đáng kể so với đối tác có kích thước đầy đủ. Không may, điều này cũng làm cho phương pháp Block-Skim được đề xuất dành riêng cho tác vụ QA trích xuất. Tuy nhiên, tác vụ QA có ý nghĩa quan trọng trong các tình huống sản xuất thực tế. Hơn nữa, phương pháp của chúng tôi nằm trong không gian đánh đổi giữa tính tổng quát, khả năng sử dụng và hiệu quả. Trong khi hy sinh tính tổng quát trên các tác vụ có thể áp dụng, phương pháp được đề xuất của chúng tôi dễ dàng áp dụng vì nó hoạt động như một plugin cho các mô hình hiện có. Tương tự, việc tận dụng các mẫu trọng số chú ý cụ thể cho QA làm cho Block-Skim đạt được kết quả tăng tốc tốt hơn so với các phương pháp khác.

Trong bài báo này, chúng tôi cung cấp nghiên cứu thực nghiệm đầu tiên về các bản đồ đặc trưng chú ý để cho thấy rằng một bản đồ chú ý có thể mang đủ thông tin để xác định phạm vi câu trả lời. Sau đó chúng tôi đề xuất Block-Skim, một mô-đun cắm và chạy cho các mô hình dựa trên transformer, để tăng tốc các mô hình dựa trên transformer trên các tác vụ QA. Bằng cách xử lý các ma trận trọng số chú ý như các bản đồ đặc trưng, mô-đun Block-Skim dựa trên CNN trích xuất thông tin từ cơ chế chú ý để đưa ra quyết định lướt qua. Với mặt nạ khối được dự đoán, Block-Skim bỏ qua các khối ngữ cảnh không liên quan, không đi vào tính toán của các lớp tiếp theo. Bên cạnh đó, chúng tôi thiết kế một mô hình huấn luyện mới huấn luyện chung mục tiêu Block-Skim với mục tiêu QA gốc, trong đó các tín hiệu tối ưu hóa bổ sung liên quan đến vị trí câu hỏi được đưa trực tiếp vào cơ chế chú ý.

Trong đánh giá của chúng tôi, chúng tôi cho thấy Block-Skim cải thiện độ chính xác QA và điểm F1 trên tất cả các tập dữ liệu và mô hình chúng tôi đánh giá. Cụ thể, BERT base được tăng tốc 3× mà không mất độ chính xác.

Bài báo này đóng góp vào 3 khía cạnh sau.
• Chúng tôi lần đầu tiên cho thấy rằng một bản đồ chú ý hiệu quả để xác định vị trí câu trả lời trong đầu vào.
• Chúng tôi đề xuất Block-Skim, tận dụng cơ chế chú ý để cải thiện và tăng tốc các mô hình Transformer trên các tác vụ QA. Điều quan trọng là trích xuất thông tin từ cơ chế chú ý trong quá trình xử lý và dự đoán thông minh những khối nào cần lướt qua.
• Chúng tôi đánh giá Block-Skim trên một số kiến trúc mô hình dựa trên Transformer và các tập dữ liệu QA và chứng minh hiệu quả và tính tổng quát của nó.

Công trình Liên quan
Các Mô hình Tái phát với Lướt qua. Ý tưởng bỏ qua hoặc lướt qua các phần hoặc token không liên quan của chuỗi đầu vào đã được nghiên cứu trong các mô hình NLP, đặc biệt là mạng nơ-ron tái phát (RNN) (Rumelhart, Hinton, and Williams 1986) và mạng bộ nhớ ngắn hạn dài (LSTM) (Hochreiter and Schmidhuber 1997). LSTM-Jump (Yu, Lee, and Le 2017) sử dụng phương pháp học tăng cường policy-gradient để huấn luyện một mô hình LSTM quyết định bao nhiêu bước thời gian để nhảy ở mỗi trạng thái. Họ cũng sử dụng các siêu tham số để kiểm soát các token trước một lần nhảy, số token tối đa để nhảy và số lần nhảy tối đa. Skim-RNN (Seo et al. 2018) quyết định động về tính và kích thước mô hình RNN được sử dụng ở bước thời gian tiếp theo. Cụ thể, họ áp dụng hai mô hình RNN "lớn" và "nhỏ" và chọn mô hình "nhỏ" để lướt qua. Structural-Jump-LSTM (Hansen et al. 2018) sử dụng hai tác nhân để quyết định có nhảy một bước nhỏ đến token tiếp theo hay cấu trúc đến dấu câu tiếp theo. Skip-RNN (Campos et al. 2017) học cách bỏ qua các cập nhật trạng thái và do đó dẫn đến kích thước đồ thị tính toán giảm. Sự khác biệt của Block-Skim với các công trình này có hai mặt. Thứ nhất, các công trình trước đây đưa ra quyết định lướt qua dựa trên các trạng thái ẩn hoặc nhúng trong quá trình xử lý. Tuy nhiên, chúng tôi là những người đầu tiên phân tích và sử dụng cơ chế chú ý để lướt qua. Thứ hai, công trình của chúng tôi dựa trên mô hình Transformer (Vaswani et al. 2017), đã vượt trội hơn các mô hình loại tái phát trên hầu hết các tác vụ NLP.

Transformer với Giảm Đầu vào. Khác với việc xử lý tuần tự của các mô hình tái phát, mô hình Transformer tính toán song song tất cả các token chuỗi đầu vào. Do đó, việc lướt qua có thể được coi là một sự giảm trong chiều chuỗi. Power-BERT (Goyal et al. 2020) trích xuất chuỗi đầu vào ở mức token trong khi xử lý. Trong quá trình tinh chỉnh cho các tác vụ downstream, Goyal et al. đề xuất một lớp trích xuất mềm để huấn luyện mô hình chung. Length-Adaptive Transformer (Kim and Cho 2020) mở rộng thêm Power-BERT bằng cách chuyển tiếp các token bị từ chối đến lớp tuyến tính cuối cùng. Funnel-Transformer (Dai et al. 2020) đề xuất một kiến trúc kim tự tháp mới với chiều độ dài chuỗi đầu vào giảm dần bất kể các manh mối ngữ nghĩa. Đối với các tác vụ yêu cầu đầu ra độ dài chuỗi đầy đủ, chẳng hạn như mô hình hóa ngôn ngữ có mặt nạ và trả lời câu hỏi trích xuất, Funnel-Transformer lấy mẫu lên ở chiều đầu vào để khôi phục. DeFormer (Cao et al. 2020) đề xuất tiền xử lý và lưu trữ các đoạn văn ở các lớp nông và chỉ nối với các phần câu hỏi ở các lớp sâu. Universal Transformer (Dehghani et al. 2018) đề xuất một cơ chế dừng động xác định các bước tinh chỉnh cho mỗi token. Khác với các công trình này, Block-Skim sử dụng thông tin chú ý giữa các cặp câu hỏi và token và lướt qua chuỗi đầu vào ở mức độ chi tiết khối tương ứng. Hơn nữa, Block-Skim không sửa đổi mô hình Transformer vanilla, làm cho nó có thể áp dụng hơn.

Transformer Hiệu quả. Cũng có nhiều nỗ lực để thiết kế các Transformer hiệu quả (Zhou et al. 2020; Wu et al. 2019; Tay et al. 2020). Ví dụ, các nhà nghiên cứu đã áp dụng các phương pháp nén được nghiên cứu kỹ lưỡng cho Transformers, chẳng hạn như cắt tỉa (Guo et al. 2020), lượng tử hóa (Wang and Zhang 2020; Guo et al. 2022), chưng cất (Sanh et al. 2019), và chia sẻ trọng số. Các nỗ lực khác tập trung vào cơ chế chú ý hiệu quả chuyên dụng xem xét độ phức tạp bậc hai của độ dài chuỗi (Kitaev, Kaiser, and Levskaya 2019; Beltagy, Peters, and Cohan 2020; Zaheer et al. 2020). Block-Skim trực giao với các kỹ thuật này về việc giảm chiều đầu vào. Chúng tôi chứng minh rằng Block-Skim tương thích với các Transformer hiệu quả với kết quả thực nghiệm.

Dự đoán Liên quan Khối dựa trên Chú ý

Phân tích Liên quan Mức Token
Transformer. Mô hình Transformer áp dụng cơ chế tự chú ý đa đầu và tính toán các trạng thái ẩn cho mỗi vị trí như một tổng có trọng số dựa trên chú ý của các trạng thái ẩn đầu vào. Vector trọng số được tính toán bởi phép chiếu tuyến tính được tham số hóa query Q và key K như Phương trình 1. Cho một chuỗi nhúng đầu vào, nhúng ngữ cảnh đầu ra được cấu thành bởi chuỗi đầu vào với chú ý khác nhau ở mỗi vị trí,

Attention (Q; K ) =Softmax (QKT=√dk); (1)

trong đó Q;K là ma trận query và key của các nhúng đầu vào, dk là độ dài của một vector query hoặc key. Do đó, bản đồ đặc trưng trọng số chú ý thường được hiển thị như một bản đồ nhiệt thể hiện mối quan hệ thu thập thông tin dọc theo các token (Kovaleva et al. 2019). Mô hình khai thác nhiều nhóm song song của các trọng số chú ý như vậy, a.k.a. các đầu chú ý, để chú ý đến thông tin ở các vị trí khác nhau.

QA trích xuất là một trong những tác vụ downstream cuối cùng trong NLP. Cho một tài liệu văn bản và một câu hỏi về ngữ cảnh, câu trả lời là một span liên tục của văn bản. Để dự đoán vị trí bắt đầu và kết thúc của ngữ cảnh đầu vào cho một câu hỏi, nhúng của mỗi token nhất định được xử lý cho tất cả các lớp trong mô hình mã hóa Transformer. Trong nhiều hệ thống QA miền mở end-to-end, truy xuất thông tin là bước đi trước ở mức đoạn văn hoặc đoạn văn thô để lọc ra các đoạn văn không liên quan. Với đặc điểm của bài toán QA trích xuất trong đó các span câu trả lời là một phần của đoạn văn, câu hỏi của chúng tôi là liệu chúng ta có thể áp dụng một kỹ thuật lọc tương tự ở mức độ chi tiết tinh trong quá trình suy luận mô hình Transformer hay không.

Trong công trình này, chúng tôi đề xuất tăng cường cơ chế chú ý với khả năng dự đoán mức độ liên quan của các token ngữ cảnh mà không sửa đổi mô hình Transformer gốc. Công trình trước (Goyal et al. 2020) cho thấy rằng cường độ chú ý là một chỉ báo tốt cho các token câu trả lời. Tuy nhiên, chúng tôi phân tích phân phối trọng số chú ý của một mô hình BERT base được huấn luyện với tập dữ liệu SQuAD (Rajpurkar et al. 2016) và thấy rằng các trọng số chú ý của chú ý đa đầu chỉ có các mẫu đáng chú ý ở các lớp muộn.

Hình 2 so sánh các trọng số chú ý ở Lớp 4 và 9 trong mô hình BERT base được huấn luyện. Các token được phân loại thành token câu trả lời hoặc token không liên quan với các nhãn từ tập dữ liệu. Ở các lớp muộn như Lớp 9, các trọng số chú ý của token câu trả lời lớn hơn đáng kể so với các token không liên quan. Tuy nhiên, ở các lớp sớm như Lớp 4, cường độ trọng số chú ý không thể phân biệt được đối với token câu trả lời và token không liên quan. Để giảm độ trễ tốt hơn, mong muốn là tìm các token không liên quan càng sớm càng tốt. Tuy nhiên, việc sử dụng giá trị trọng số chú ý làm tiêu chí liên quan có thể có vấn đề ở các lớp sớm.

Dự đoán Liên quan Khối dựa trên CNN
Với mẫu phức tạp của các trọng số chú ý, chúng tôi đề xuất sử dụng một bộ trích xuất đặc trưng dựa trên CNN để xử lý các bản đồ nhiệt chú ý như các kênh hình ảnh đầu vào và dự đoán mức độ liên quan của mỗi token. Để phân bổ chi phí xử lý, chúng tôi chia chuỗi đầu vào X= (x0;x1;···;xi) thành i=k khối độc quyền blockj= (xjk;xjk+1···;xjk+(k−1)), trong đó k là kích thước khối, tức là các token được bao gồm trong span đầu vào liên tục. Mức độ liên quan của một khối được định nghĩa là liệu nó có chứa câu trả lời cuối cùng chính xác hay không. Do đó, mục tiêu của chúng tôi là tìm ra mức độ liên quan của các khối và lướt qua những khối không liên quan trong quá trình suy luận Transformer.

Hình 4 cho thấy chi tiết cách chúng tôi trích xuất thông tin chú ý từ Transformer và đưa chúng vào mô hình CNN. Trong mô-đun CNN, chúng tôi sử dụng hai lớp tích chập 3×3 và một lớp tích chập 1×1, tất cả đều sử dụng hoạt động ReLU (Hahnloser and Seung 2001) làm hàm kích hoạt. Chúng tôi chèn một lớp pooling trung bình 2×2 cho hai lớp tích chập 3×3 đầu tiên để giảm kích thước bản đồ đặc trưng. Ngoài ra, chúng tôi cũng sử dụng hai lớp chuẩn hóa batch (Ioffe and Szegedy 2015) để cải thiện độ chính xác dự đoán. Để xác định các khối ngữ cảnh câu trả lời, chúng tôi sử dụng một lớp phân loại tuyến tính để tính điểm cho mỗi khối. Mô-đun xuất ra một mặt nạ dự đoán mức khối tương ứng với mức độ liên quan của một khối token đầu vào với câu hỏi.

Mô hình này được huấn luyện với tất cả bản đồ nhiệt chú ý được lập hồ sơ từ cùng một tập dữ liệu bản đồ nhiệt như được mô tả trước đó. Độ chính xác dự đoán được hiển thị như Hình 3. Nói chung, mô hình đạt được độ chính xác khá tốt chứng minh rằng một mô hình CNN có khả năng trích xuất thông tin hành vi chú ý và xác định câu trả lời. Trực quan, các mô hình CNN với bản đồ nhiệt chú ý lớp cao hơn có hiệu suất tốt hơn. Điều này gợi ý rằng mô hình backbone trở nên thuyết phục hơn về việc trả lời câu hỏi khi nó đi sâu hơn.

Đơn giản hóa Bộ dự đoán CNN với Chú ý Đường chéo
Phương pháp trên về việc đưa toàn bộ bản đồ đặc trưng chú ý vào bộ dự đoán CNN có một vấn đề chính, đó là bộ dự đoán cần xử lý kích thước biến đổi của bản đồ đặc trưng chú ý. Do đó, chúng tôi đơn giản hóa đầu vào cho mô hình CNN chỉ với chú ý từ vùng đường chéo của nó. Cụ thể, chúng tôi chỉ đưa vào vùng bản đồ nhiệt đường chéo làm biểu diễn đầu vào cho mỗi khối chuỗi đầu vào, như được thể hiện trong Hình 4.

Giả thuyết của chúng tôi là vùng đường chéo của bản đồ nhiệt chú ý chứa đủ thông tin để xác định mức độ liên quan của khối. Bởi vì các công trình trước (Clark et al. 2019; Guan et al. 2020) cho thấy rằng cơ chế chú ý có một số mẫu cố định, đó là các loại đường chéo, stride, khối, hoặc dày đặc. Và tất cả các mẫu này có thể dễ dàng được nhận ra chỉ với vùng đường chéo.

Tương tự, chúng tôi tối ưu hóa các mô hình CNN với bản đồ nhiệt giảm và kết quả được hiển thị như Hình 3. Như chúng ta có thể thấy, các mô hình đạt được độ chính xác dự đoán tương tự so với việc sử dụng toàn bộ bản đồ nhiệt trọng số chú ý. Kết quả chứng minh giả thuyết của chúng tôi rằng có thể sử dụng thông tin đường chéo từ các bản đồ nhiệt chú ý để dự đoán mức độ liên quan của câu trả lời. Bằng cách làm như vậy, độ phức tạp tính toán cũng được giảm đáng kể vì kích thước đầu vào nhỏ hơn nhiều.

Phát hiện trên khẳng định giả thuyết của chúng tôi rằng trọng số chú ý đường chéo thực sự mang thông tin để tìm ra vị trí câu trả lời. Điều này thúc đẩy chúng tôi sử dụng thông tin chú ý như vậy để thu hẹp vị trí câu trả lời có thể cùng với việc xử lý chuỗi đầu vào. Trong phần tiếp theo, chúng tôi giới thiệu thiết kế của chúng tôi sử dụng một mô-đun học end-to-end cắm và chạy để trích xuất thông tin hữu ích từ các trọng số chú ý cho các quyết định lướt qua.

Transformer với Block-Skim
Phần trước cho thấy tính khả thi của việc sử dụng các trọng số chú ý để dự đoán mức độ liên quan của các khối token. Tuy nhiên, việc sử dụng bộ dự đoán một cách ngây thơ có thể dẫn đến sự suy giảm đáng kể về độ chính xác tác vụ QA. Bởi vì bộ dự đoán liên quan khối chỉ được huấn luyện với các nhãn câu trả lời, nó có thể thất bại trong tác vụ QA đa bước, yêu cầu thông tin vượt ra ngoài các nhãn câu trả lời. Để giải quyết vấn đề này, chúng tôi đề xuất một mô hình huấn luyện chung đa mục tiêu end-to-end. Sau đó trong thời gian suy luận, dự đoán của mô hình Block-Skim được tăng cường để lọc chuỗi đầu vào để tăng tốc. Điều này gây ra sự không khớp giữa các mô hình huấn luyện và suy luận. Tuy nhiên, việc lướt qua các khối trong quá trình huấn luyện làm cho việc huấn luyện chung không ổn định. Và kết quả thực nghiệm của chúng tôi chứng minh rằng sự không khớp này có thể bỏ qua. Chúng tôi đưa ra một minh họa chi tiết về mô hình huấn luyện chung được đề xuất và quá trình suy luận như sau.

Huấn luyện Chung Đa Mục tiêu Tác vụ Đơn
Theo các thí nghiệm trước, chúng tôi nối các mô hình CNN đã nề trước vào mỗi lớp để dự đoán mức độ liên quan của các khối và tối ưu hóa chúng cùng với mô hình Transformer backbone. Do đó, có hai loại bộ phân loại trong mô hình được tăng cường với mô-đun Block-Skim. Đầu tiên là bộ phân loại QA gốc ở lớp cuối cùng và thứ hai là bộ phân loại liên quan mức khối ở mỗi lớp. Hai bộ phân loại này tối ưu hóa cùng một tác vụ downstream của việc dự đoán vị trí câu trả lời với một nhãn mục tiêu giống hệt nhau. Tuy nhiên, chúng được đưa vào với một loại mục tiêu mất mát khác nhau, đó là, mục tiêu QA với các nhúng đầu ra Transformer và mục tiêu Block-Skim với các trọng số chú ý. Chúng tôi huấn luyện chung các bộ phân loại này để mục tiêu huấn luyện là tối thiểu hóa tổng của tất cả các mất mát của bộ phân loại.

Hàm mất mát của mỗi bộ phân loại mức khối được tính như mất mát cross-entropy so với nhãn thật liệu một khối có chứa token câu trả lời hay không. Phương trình 2 đưa ra định nghĩa chính thức. Tổng mất mát của bộ phân loại mức khối LBlockSkim là tổng của tất cả các khối chỉ chứa các token đoạn văn. Lý do là chúng tôi chỉ muốn loại bỏ các khối với các token đoạn văn không liên quan thay vì câu hỏi. Các khối có token câu hỏi không được sử dụng trong quá trình huấn luyện.

LBlockSkim =∑mi∈{passage blocks}CELoss (mi;yi)
yi={1, block i has answer tokens; 0, block i has no answer tokens} (2)

Để tính toán tổng mất mát cuối cùng Ltotal, chúng tôi giới thiệu hai siêu tham số trong Phương trình 3. Trước tiên chúng tôi sử dụng một hệ số hài hòa α để các mô hình và cài đặt khác nhau có thể điều chỉnh tỷ lệ giữa mất mát QA và mất mát bộ phân loại liên quan mức khối. Nó được quyết định bằng tìm kiếm lưới trên tập phát triển. Sau đó chúng tôi sử dụng hệ số cân bằng β để điều chỉnh mất mát từ các khối liên quan dương và âm bởi vì thường có nhiều khối không chứa token câu trả lời (tức là, khối âm) hơn các khối chứa token câu trả lời (tức là, khối dương). Việc lựa chọn siêu tham số này sẽ được giải thích chi tiết trong thiết lập thí nghiệm.

Ltotal =LQA+α∑ith layer(βLi;y=1BlockSkim+Li;y=0BlockSkim) (3)

Block-Skim của chúng tôi là một mô-đun plugin thuận tiện do hai lý do sau. Thứ nhất, nó không ảnh hưởng đến tính toán mô hình backbone, bởi vì nó chỉ điều chỉnh phân phối giá trị chú ý với các tham số bổ sung cho mô hình backbone. Nói cách khác, một mô hình được huấn luyện với Block-Skim có thể được sử dụng với nó được loại bỏ. Thứ hai, mục tiêu Block-Skim được giới thiệu không cần tín hiệu huấn luyện bổ sung cũng không giảm độ chính xác QA. Thực tế, chúng tôi sẽ cho thấy rằng tín hiệu gradient bổ sung đưa vào chú ý cải thiện độ chính xác QA gốc.

QA Đa bước. Phương pháp huấn luyện chung của chúng tôi cũng có thể giải quyết thách thức trong các tác vụ QA đa bước (Yang et al. 2018), trong đó việc rút ra câu trả lời yêu cầu nhiều mảnh bằng chứng và lý luận. Mặc dù dự đoán liên quan khối chỉ sử dụng tín hiệu nhãn câu trả lời, tác vụ QA gốc đảm bảo rằng bằng chứng cần được giữ lại. Nói cách khác, thông tin lý luận bằng chứng được mã hóa ngầm trong các nhúng ngữ cảnh. Để minh họa điểm như vậy, chúng tôi thực hiện một nghiên cứu loại bỏ kết hợp nhãn bằng chứng trong việc huấn luyện bộ dự đoán Block-Skim. Độ chính xác bộ dự đoán không cải thiện với nhãn bằng chứng bổ sung, điều này khẳng định hiệu quả của việc huấn luyện chung đa mục tiêu tác vụ đơn của chúng tôi.

Suy luận với Block-Skim
Bây giờ chúng tôi mô tả cách sử dụng Block-Skim để tăng tốc suy luận tác vụ QA. Mặc dù chúng tôi thêm mất mát phân loại liên quan mức khối trong quá trình huấn luyện chung, chúng tôi thực sự không loại bỏ bất kỳ khối nào bởi vì nó có thể bỏ qua các khối câu trả lời và việc huấn luyện tác vụ QA trở nên không ổn định. Tuy nhiên, chúng tôi chỉ tăng cường việc giảm khối với mô-đun Block-Skim trong quá trình suy luận để tiết kiệm tính toán và tránh thay đổi lớn đối với Transformer cơ bản. Trong tính toán suy luận, chúng tôi chia chuỗi đầu vào theo mức độ chi tiết khối, đây là một siêu tham số trong mô hình của chúng tôi. Mô hình bỏ qua một tập hợp các khối theo kết quả mô-đun lướt qua cho các lớp sau. Với những tính năng thiết kế đó, Block-Skim hoạt động như một thành phần bổ sung cho mô hình Transformer gốc và tương thích với nhiều mô hình biến thể Transformer cũng như các phương pháp nén mô hình.

Chúng tôi cung cấp một mô hình phân tích để chứng minh tiềm năng tăng tốc độ trễ của Block-Skim. Giả sử rằng chúng tôi chèn mô-đun Block-Skim vào một mô hình vanilla với tổng số L lớp, và một phần của mi khối vẫn còn cho các lớp sau sau lớp i. Độ phức tạp xử lý lý tưởng của một token cho một lớp Transformer được ghi chú là Tlayer. Ở đây chúng tôi thực hiện một xấp xỉ rằng độ phức tạp tính toán tuyến tính với độ dài chuỗi N. Đây là một xấp xỉ bảo thủ bởi vì cơ chế chú ý là O(N2). Sự tăng tốc hiệu suất được xây dựng bởi Phương trình 4 nếu chúng tôi bỏ qua chi phí tính toán của Block-Skim. Thực tế, tính toán của một mô-đun Block-Skim đơn lẻ nhỏ hơn các lớp Transformer 100 lần. Ví dụ, khi ∑mk∈{passage blocks}mk= 0.9, có nghĩa là 10% token được lướt qua mỗi lớp. Quyết định lướt qua này sẽ dẫn đến tỷ lệ tăng tốc tổng thể là 1.86×.

Speedup =TVanilla/TBlockSkim
=LN Tlayer/(∑Li=0(∏ij=0∑mj,k∈{layer j}mj,k N)Tlayer)
=L/(∑Li=0∏ij=0∑mj,k∈{layer j}mj,k) (4)

Đánh giá

Thiết lập Thí nghiệm
Tập dữ liệu. Chúng tôi đánh giá phương pháp của chúng tôi trên 6 tập dữ liệu QA trích xuất, bao gồm SQuAD 1.1 (Rajpurkar et al. 2016), Natural Questions (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017), NewsQA (Trischler et al. 2016), SearchQA(Dunn et al. 2017) và HotpotQA (Yang et al. 2018). HotpotQA cung cấp các câu hỏi yêu cầu lý luận đa bước để trả lời với các sự kiện hỗ trợ. Sự đa dạng của các tập dữ liệu này như độ dài đoạn văn khác nhau và các nguồn tài liệu khác nhau cho phép chúng tôi đánh giá khả năng áp dụng chung của phương pháp được đề xuất.

Mô hình. Chúng tôi theo thiết lập của mô hình BERT để sử dụng cấu trúc của bộ mã hóa Transformer và một lớp phân loại tuyến tính cho tất cả các tập dữ liệu. Như đã giải thích trước đó, Block-Skim hoạt động như một mô-đun bổ sung cho Transformer vanilla, và do đó có thể áp dụng chung cho tất cả các mô hình dựa trên Transformer, cũng như các phương pháp nén mô hình. Để minh họa điểm này, chúng tôi áp dụng phương pháp Block-Skim cho hai mô hình BERT với các thiết lập kích thước khác nhau. Chúng tôi đánh giá thiết lập base với 12 đầu và 12 lớp, cũng như thiết lập large với 24 lớp và 16 đầu như được mô tả trong công trình trước (Devlin et al. 2018).

Các Phương pháp Nén Mô hình. Chúng tôi tiến hành các phương pháp nén mô hình sau trên các mô hình BERT base để chứng minh khả năng tương thích của Block-Skim của chúng tôi.
•Chưng cất. Chưng cất kiến thức sử dụng một mô hình giáo viên để chuyển giao kiến thức cho một mô hình học sinh nhỏ hơn. Ở đây chúng tôi áp dụng thiết lập DistilBERT (Sanh et al. 2019) để chưng cất một mô hình 6 lớp từ mô hình BERT base.
•Chia sẻ Trọng số. Bằng cách chia sẻ các tham số trọng số giữa các lớp, lượng tham số trọng số giảm. Lưu ý rằng chia sẻ trọng số không ảnh hưởng đến FLOPs tính toán (các phép toán dấu phẩy động). Chúng tôi đánh giá Block-Skim trên ALBERT (Lan et al. 2019) chia sẻ các tham số trọng số giữa tất cả các lớp.
•Cắt tỉa. Thay vì các kỹ thuật cắt tỉa trọng số thông thường, chúng tôi đánh giá cắt tỉa đầu (Michel, Levy, and Neubig 2019) chuyên biệt cho cơ chế chú ý trong các mô hình Transformer. Việc cắt tỉa các đầu chú ý giảm kích thước đặc trưng đầu vào cho mô-đun Block-Skim. Chúng tôi cắt tỉa 50% đầu chú ý dựa trên tiêu chí tầm quan trọng đầu chú ý được giới thiệu trong công trình trước (Michel, Levy, and Neubig 2019).

Đường cơ sở Giảm Chiều Đầu vào. Chúng tôi cũng so sánh với các phương pháp giảm chiều đầu vào Deformer và Length-Adaptive Transformer. Deformer(Cao et al. 2020) tiền xử lý và lưu trữ các đoạn văn ngữ cảnh ở các lớp sớm để giảm độ dài chuỗi suy luận thực tế. Length-Adaptive Transformer (Kim and Cho 2020) là một phiên bản kế tiếp của Power-BERT chuyển tiếp các token bị từ chối đến lớp cuối cùng bằng cường độ chú ý.

Thiết lập Huấn luyện. Chúng tôi thực hiện phương pháp được đề xuất dựa trên thư viện mã nguồn mở từ Wolf et al. (2019). Đối với mỗi mô hình đường cơ sở, chúng tôi sử dụng các điểm kiểm tra tiền huấn luyện được phát hành. Chúng tôi theo thiết lập huấn luyện được sử dụng bởi Devlin et al. (2018) và Liu et al. (2019) để thực hiện tinh chỉnh trên các tập dữ liệu QA trích xuất trên. Chúng tôi khởi tạo tốc độ học thành 3e−5 cho các mô hình BERT và 5e−5 cho ALBERT với một bộ lập lịch tốc độ học tuyến tính. Đối với tập dữ liệu SQuAD, chúng tôi áp dụng kích thước batch 16 và độ dài chuỗi tối đa 384. Và đối với các tập dữ liệu khác, chúng tôi áp dụng kích thước batch 32 và độ dài chuỗi tối đa 512. Chúng tôi thực hiện tất cả các thí nghiệm được báo cáo với hạt giống ngẫu nhiên 42. Chúng tôi huấn luyện một mô hình đường cơ sở và mô hình Block-Skim với cùng thiết lập trong hai epoch và báo cáo độ chính xác từ điểm chuẩn tác vụ MRQA để so sánh. Chúng tôi sử dụng bốn GPU V100 với bộ nhớ 32 GB cho các thí nghiệm huấn luyện.

Hệ số cân bằng được xác định bởi số lượng mẫu khối và được báo cáo trong Bảng 1. Hệ số hài hòa là 0.01 cho ALBERT và 0.1 cho tất cả các mô hình khác chúng tôi sử dụng. Nó được xác định bằng tìm kiếm lưới siêu tham số từ 1e−3 đến 10 với bước 10×.

Chúng tôi sử dụng FLOPs suy luận như một phép đo chung về độ phức tạp tính toán mô hình trên tất cả các nền tảng. Chúng tôi sử dụng TorchProfile(Liu 2020) để tính toán FLOPs cho mỗi mô hình và chuẩn hóa kết quả như một tỷ lệ với BERT base.

Kết quả Huấn luyện Chung
Trước tiên chúng tôi đánh giá hiệu ứng huấn luyện chung Block-Skim đối với tác vụ QA bằng cách so sánh các mô hình BERT base và các biến thể của chúng với Block-Skim được tăng cường. Trong các phiên bản Block-Skim của chúng, các mô-đun Block-Skim chỉ tham gia vào quá trình huấn luyện và được loại bỏ trong tác vụ suy luận. Bảng 1 cho thấy kết quả trên nhiều tập dữ liệu QA. Block-Skim vượt trội hơn mục tiêu huấn luyện đường cơ sở trên tất cả các tập dữ liệu được đánh giá và vượt trội với 0.58% điểm F1 trung bình. Điều này gợi ý rằng mục tiêu Block-Skim nhất quán với mục tiêu QA và thậm chí cải thiện độ chính xác của nó. Kết quả cho thấy khả năng áp dụng rộng rãi của phương pháp của chúng tôi đối với các tập dữ liệu khác nhau với độ khó và độ phức tạp khác nhau.

Chúng tôi tiếp tục cho thấy tính mạnh mẽ khi sử dụng huấn luyện chung Block-Skim như một mô-đun bổ sung. Bảng 2 cho thấy kết quả của nhiều lần chạy sử dụng thiết lập tối ưu hóa giống hệt nhau với các hạt giống ngẫu nhiên khác nhau. Bằng cách giới thiệu mất mát Block-Skim trong Huấn luyện, độ chính xác QA của mô hình backbone được cải thiện 0.4× trên khớp chính xác và 0.32× trên điểm F1. Và kích hoạt Block-Skim luôn vượt trội hơn mô hình backbone với cùng thiết lập. Điều này là do mục tiêu huấn luyện bổ sung cung cấp tín hiệu gradient trực tiếp cho cơ chế chú ý và điều chỉnh phân phối giá trị của nó.

Kết quả Tăng tốc Suy luận
Kết quả trên Các Tập dữ liệu Khác nhau. Kết quả tăng tốc FLOPs được chuẩn hóa với mô hình BERT base được chứng minh trong Bảng 1. Block-Skim đạt được sự tăng tốc 2.59× trung bình với sự suy giảm độ chính xác nhỏ 0.23× trên các tập dữ liệu khác nhau được đánh giá. Trên tập dữ liệu QA đa bước HotpotQA, phương pháp của chúng tôi cũng đạt được sự tăng tốc 2.28 lần. Kết quả cho thấy rằng phương pháp Block-Skim được đề xuất có khả năng xác định sự thừa ngữ nghĩa với thông tin chú ý.

So sánh với Đường cơ sở BERT Vanilla. Block-Skim cải thiện độ trễ suy luận mô hình BERT base lần lượt 3.1× và 2.4× trên các tập dữ liệu SQuAD và HotpotQA. Khi coi các thiết lập kích thước mô hình của mô hình BERT vanilla như một sự đánh đổi giữa độ chính xác và độ phức tạp, Block-Skim cải thiện sự đánh đổi này bằng một biên độ. Như được hiển thị trong Hình 5 phương pháp của chúng tôi tăng tốc BERT large nhanh như mô hình BERT base vanilla nhưng với độ chính xác cao hơn nhiều. Cụ thể, độ trễ của mô hình BERT large vanilla là 3.47× của BERT base, và phương pháp của chúng tôi giảm khoảng cách xuống 1.09× trên tập dữ liệu SQuAD, dịch thành sự tăng tốc 3.18×.

Khả năng tương thích với Các Phương pháp Nén Mô hình. Chúng tôi so sánh khả năng tương thích của Block-Skim với các phương pháp nén mô hình khác với Hình 5. Các phương pháp nén mô hình này đánh đổi độ chính xác cho độ phức tạp tính toán ở các mức độ khác nhau. Ví dụ, chưng cất mô hình BERT base 12 lớp xuống 6 lớp dẫn đến giảm độ chính xác 2% và tăng tốc 2 lần. Với phương pháp Block-Skim được nối vào mô hình này, các phương pháp có thể được tăng tốc thêm mà không mất hoặc mất ít độ chính xác. Cụ thể, việc sử dụng Block-Skim với DistilBERT đạt được sự tăng tốc 5× so với mô hình BERT vanilla. Và ngay cả với việc cắt tỉa đầu giảm thông tin chú ý, Block-Skim cũng tương thích và đạt được hơn 2× tăng tốc. Mặc dù vẫn tương thích, Block-Skim có ít tăng tốc hơn trên các mô hình ALBERT. Chúng tôi gợi ý rằng việc chia sẻ tham số của cơ chế chú ý làm cho việc tối ưu hóa với mục tiêu Block-Skim bổ sung khó khăn hơn. Vì phương pháp Block-Skim được đề xuất nhằm giảm sự thừa ngữ nghĩa chiều chuỗi đầu vào, nó tương thích với các phương pháp nén mô hình này tập trung vào sự thừa mô hình về mặt lý thuyết. Bằng cách thiết kế Block-Skim không sửa đổi mô hình backbone, phương pháp của chúng tôi có thể áp dụng chung cho các thuật toán này cũng như các phương pháp cắt tỉa mô hình khác (Guo et al. 2020; Qiu et al. 2019; Gan et al. 2020).

Block-Skim đạt được sự tăng tốc gần với ít suy giảm độ chính xác hơn so với Deformer và nhiều tăng tốc hơn với suy giảm độ chính xác tương tự so với Length-Adaptive Transformer trên tập dữ liệu SQuAD-1.1. Điều này gợi ý Block-Skim nắm bắt sự thừa ngữ nghĩa thời gian chạy tốt hơn. Mặc dù điều này cũng làm cho nó chỉ áp dụng được cho các tác vụ QA.

Nghiên cứu Loại bỏ
Chúng tôi thiết kế một loạt thí nghiệm loại bỏ các thành phần trong Block-Skim để nghiên cứu hiệu ứng riêng lẻ của chúng. Các thí nghiệm được thực hiện dựa trên cùng thiết lập. Chúng tôi báo cáo kết quả chi tiết trong Bảng 3, và tóm tắt các phát hiện chính như sau.

ID-3. Thay vì huấn luyện chung, chúng tôi thực hiện huấn luyện hai bước. Trước tiên chúng tôi thực hiện tinh chỉnh cho tác vụ QA. Sau đó chúng tôi thực hiện huấn luyện Block-Skim với mô hình QA đường cơ sở bị đóng băng. Nói cách khác, chúng tôi chỉ sử dụng mục tiêu Block-Skim và chỉ cập nhật các trọng số trong các mô-đun Block-Skim. Do đó, độ chính xác QA vẫn giống như mô hình đường cơ sở, thấp hơn huấn luyện chung (ID-3). Trong khi đó, bộ phân loại Block-Skim cũng có độ chính xác thấp hơn huấn luyện chung đặc biệt ở lớp 6.

ID-4 Chúng tôi lướt qua các khối trong quá trình huấn luyện QA chung Block-Skim. Bởi vì các khối bị lướt qua sai có thể làm rối loạn việc tối ưu hóa QA, nó dẫn đến mất mát độ chính xác đáng kể.

ID-5-ID-9. Chúng tôi nghiên cứu tác động của các kích thước khối khác nhau. Cụ thể, khi kích thước khối là 1, nó tương đương với lướt qua ở mức độ chi tiết token. Kết quả thực nghiệm của chúng tôi cho thấy rằng độ chính xác của bộ phân loại Block-Skim tốt hơn khi kích thước khối lớn hơn. Mặt khác, kích thước khối lớn hơn cũng dẫn đến ít số lượng khối hơn và do đó sự tăng tốc hiệu suất trở nên hạn chế. Để kết thúc này, chúng tôi chọn kích thước khối 32 như một điểm ngọt ngào thiết kế.

ID-11-ID-12. Chúng tôi đánh giá khả năng áp dụng của Block-Skim cho tác vụ QA đa bước với tập dữ liệu HotpotQA. Như được giới thiệu trong, chúng tôi thêm các sự kiện hỗ trợ (tức là, bằng chứng) cho mỗi câu hỏi vào mục tiêu Block-Skim trong thí nghiệm ID-12 bằng cách gắn nhãn các khối bằng chứng thành 1 trong Eq.2 cho các mô-đun dự đoán lướt qua. Điều này dẫn đến độ chính xác QA cao hơn. Nhưng độ chính xác trung bình của các bộ dự đoán lướt qua ở tất cả các lớp tồi tệ hơn, là 86.08% so với 92.67%. Thí nghiệm loại bỏ này cho thấy rằng huấn luyện chung đa mục tiêu tác vụ đơn của chúng tôi đã có thể nắm bắt thông tin bằng chứng, làm cho việc thêm nó một cách rõ ràng vào huấn luyện trở nên không cần thiết.

Kết luận
Trong công trình này, chúng tôi đề xuất một mô-đun Block-Skim cắm và chạy cho Transformer và các biến thể của nó để xử lý QA hiệu quả. Chúng tôi chứng minh thực nghiệm rằng cơ chế chú ý có thể cung cấp thông tin hướng dẫn để xác định span câu trả lời. Tận dụng hiểu biết này, chúng tôi đề xuất học cơ chế chú ý theo cách có giám sát, chấm dứt các khối không liên quan ở các lớp sớm, giảm đáng kể các tính toán. Bên cạnh đó, mục tiêu huấn luyện Block-Skim được đề xuất cung cấp cơ chế chú ý với tín hiệu học bổ sung và cải thiện độ chính xác QA trên tất cả các tập dữ liệu và mô hình chúng tôi đánh giá. Với việc sử dụng mô-đun Block-Skim, sự phân biệt như vậy được tăng cường theo cách có giám sát. Ý tưởng này cũng có thể áp dụng cho các tác vụ và kiến trúc khác.

Phụ lục Phân phối Chú ý
Chúng tôi cho thấy kết quả đầy đủ của phân phối giá trị trọng số chú ý được thảo luận trong Hình 2. Hình 6 cho thấy rằng các lớp sâu hơn có các mẫu có thể phân biệt hơn.

Lời cảm ơn
Công trình này được hỗ trợ bởi Chương trình R&D Chính quốc gia Trung Quốc theo Grant 2019YFF0302600, và grant Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) (62072297, 62106143, và 61832006). Chúng tôi muốn cảm ơn các nhà đánh giá ẩn danh vì những bình luận sâu sắc và gợi ý xây dựng của họ. Zhouhan Lin cũng được hỗ trợ bởi Chương trình Pujiang Thượng Hải. Chúng tôi cũng cảm ơn Yuxian Qiu và Kexin Li với những người chúng tôi có những cuộc thảo luận truyền cảm hứng về thiết kế thí nghiệm đánh giá. Cuối cùng, chúng tôi cảm ơn Zhihui Zhang vì đã giúp trình bày và hình ảnh hóa kết quả thực nghiệm. Jingwen Leng và Zhouhan Lin là những tác giả tương ứng của bài báo này.
