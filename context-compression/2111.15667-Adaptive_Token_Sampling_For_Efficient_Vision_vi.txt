# Lấy Mẫu Token Thích Ứng Cho Vision Transformers Hiệu Quả

Mohsen Fayyaz1,6,∗†Soroush Abbasi Koohpayegani2,∗†Farnoush Rezaei
Jafari3,4,∗Sunando Sengupta1Hamid Reza Vaezi Joze5Eric
Sommerlade1Hamed Pirsiavash2Juergen Gall6
1Microsoft2University of California, Davis3Machine Learning Group, Technische
Universit¨ at Berlin,4Berlin Institute for the Foundations of Learning and Data
5Meta Reality Labs6University of Bonn

Tóm tắt. Mặc dù các mô hình vision transformer hiện đại đạt kết quả đầy hứa hẹn trong phân loại hình ảnh, chúng có chi phí tính toán đắt đỏ và đòi hỏi nhiều GFLOPs. Mặc dù GFLOPs của một vision transformer có thể được giảm bằng cách giảm số lượng token trong mạng, không có thiết lập nào là tối ưu cho tất cả các hình ảnh đầu vào. Trong công trình này, chúng tôi giới thiệu một module Adaptive Token Sampler (ATS) khả vi và không tham số, có thể được tích hợp vào bất kỳ kiến trúc vision transformer hiện có nào. ATS trao quyền cho vision transformers bằng cách chấm điểm và lấy mẫu thích ứng các token quan trọng. Kết quả là, số lượng token không còn cố định nữa và thay đổi cho mỗi hình ảnh đầu vào. Bằng cách tích hợp ATS như một lớp bổ sung trong các khối transformer hiện tại, chúng ta có thể chuyển đổi chúng thành các vision transformers hiệu quả hơn nhiều với số lượng token thích ứng. Vì ATS là một module không tham số, nó có thể được thêm vào các vision transformers được đào tạo trước sẵn có như một module cắm và chạy, do đó giảm GFLOPs của chúng mà không cần đào tạo bổ sung nào. Hơn nữa, do thiết kế khả vi của nó, người ta cũng có thể đào tạo một vision transformer được trang bị ATS. Chúng tôi đánh giá hiệu quả của module trong cả tác vụ phân loại hình ảnh và video bằng cách thêm nó vào nhiều vision transformers SOTA. Module đề xuất của chúng tôi cải thiện SOTA bằng cách giảm chi phí tính toán (GFLOPs) của chúng 2×, trong khi vẫn bảo toàn độ chính xác trên các bộ dữ liệu ImageNet, Kinetics-400, và Kinetics-600.

## 1 Giới thiệu

Trong mười năm qua, đã có một tiến bộ to lớn về hiểu biết hình ảnh và video dưới ánh sáng của các kiến trúc học sâu mới và phức tạp, dựa trên các biến thể của Mạng Neural Tích chập (CNNs) 2D [23,34,50] và 3D [10,12,17,18,54,56]. Gần đây, vision transformers đã cho thấy kết quả đầy hứa hẹn trong phân loại hình ảnh [13, 31, 53, 63] và nhận dạng hành động [1,2,39] so với CNNs. Mặc dù vision transformers có khả năng biểu diễn vượt trội, chi phí tính toán cao của các khối transformer khiến chúng không phù hợp cho nhiều thiết bị edge. Chi phí tính toán của một vision transformer tăng theo bậc hai so với số lượng token mà nó sử dụng. Để giảm số lượng token và do đó giảm chi phí tính toán của một vision transformer, DynamicViT [46] đề xuất một mạng neural chấm điểm token để dự đoán token nào là dư thừa. Phương pháp này sau đó giữ một tỷ lệ cố định của token ở mỗi giai đoạn. Mặc dù DynamicViT giảm GFLOPs của một mạng nhất định, mạng chấm điểm của nó tạo ra một chi phí tính toán bổ sung. Hơn nữa, mạng chấm điểm cần được đào tạo cùng với vision transformer và nó yêu cầu sửa đổi hàm mất mát bằng cách thêm các số hạng mất mát bổ sung và siêu tham số. Để giảm thiểu những hạn chế như vậy, EViT [36] sử dụng các trọng số attention làm điểm quan trọng của token. Một hạn chế khác của cả EViT và DynamicViT là chúng cần được đào tạo lại nếu các tỷ lệ mục tiêu cố định cần được thay đổi (ví dụ do triển khai trên một thiết bị khác). Điều này hạn chế mạnh mẽ các ứng dụng của chúng.

Trong công trình này, chúng tôi đề xuất một phương pháp để giảm hiệu quả số lượng token trong bất kỳ vision transformer nhất định nào mà không có những hạn chế đã đề cập. Phương pháp của chúng tôi được thúc đẩy bởi quan sát rằng trong phân loại hình ảnh/hành động, không phải tất cả các phần của một hình ảnh/video đầu vào đều đóng góp như nhau vào điểm phân loại cuối cùng và một số phần chứa thông tin không liên quan hoặc dư thừa. Lượng thông tin liên quan thay đổi tùy thuộc vào nội dung của hình ảnh hoặc video. Ví dụ, trong Hình 7, chúng ta có thể quan sát các ví dụ trong đó chỉ cần một vài hoặc nhiều patch để phân loại chính xác. Điều tương tự cũng đúng cho số lượng token được sử dụng ở mỗi giai đoạn, như được minh họa trong Hình 2. Do đó, chúng tôi đề xuất một phương pháp tự động chọn một số lượng token phù hợp ở mỗi giai đoạn dựa trên nội dung hình ảnh, tức là số lượng token được chọn ở tất cả các giai đoạn của mạng thay đổi cho các hình ảnh khác nhau, như được hiển thị trong Hình 6. Điều này trái ngược với [36,46], nơi tỷ lệ token được chọn cần được chỉ định cho mỗi giai đoạn và là hằng số sau khi đào tạo. Tuy nhiên, việc chọn một số lượng token tĩnh sẽ một mặt loại bỏ thông tin quan trọng cho các hình ảnh/video thách thức, dẫn đến giảm độ chính xác phân loại. Mặt khác, nó sẽ sử dụng nhiều token hơn cần thiết cho các trường hợp dễ và do đó lãng phí tài nguyên tính toán. Trong công trình này, chúng tôi giải quyết câu hỏi về cách một transformer có thể thích ứng động tài nguyên tính toán của nó theo cách mà không sử dụng nhiều tài nguyên hơn cần thiết cho mỗi hình ảnh/video đầu vào.

Để đạt được điều này, chúng tôi giới thiệu một module Adaptive Token Sampler (ATS) mới. ATS là một module khả vi không tham số lấy mẫu thích ứng các token đầu vào. Để làm điều này, trước tiên chúng tôi gán điểm quan trọng cho các token đầu vào bằng cách sử dụng trọng số attention của token phân loại trong lớp self-attention và sau đó chọn một tập con token bằng cách sử dụng lấy mẫu biến đổi nghịch đảo trên các điểm. Cuối cùng, chúng tôi lấy mẫu mềm các token đầu ra để loại bỏ thông tin dư thừa với lượng mất mát thông tin ít nhất. Trái ngược với [46], phương pháp của chúng tôi không thêm bất kỳ tham số có thể học bổ sung nào vào mạng. Mặc dù module ATS có thể được thêm vào bất kỳ vision transformer được đào tạo trước sẵn có nào mà không cần đào tạo thêm, mạng được trang bị module ATS khả vi cũng có thể được tinh chỉnh thêm. Hơn nữa, người ta có thể đào tạo một mô hình chỉ một lần và sau đó điều chỉnh giới hạn tối đa cho module ATS để thích ứng với tài nguyên của các thiết bị edge khác nhau tại thời điểm suy luận. Điều này loại bỏ nhu cầu đào tạo các mô hình riêng biệt cho các mức tài nguyên tính toán khác nhau.

Chúng tôi chứng minh hiệu quả của bộ lấy mẫu token thích ứng đề xuất cho phân loại hình ảnh bằng cách tích hợp nó vào các vision transformers hiện đại như DeiT [53], CvT [63], và PS-ViT [68]. Như được hiển thị trong Hình 4, phương pháp của chúng tôi giảm đáng kể GFLOPs của vision transformers với các kích thước khác nhau mà không mất độ chính xác đáng kể. Chúng tôi đánh giá hiệu quả của phương pháp bằng cách so sánh với các phương pháp khác được thiết kế để giảm số lượng token, bao gồm DynamicViT [46], EViT [36], và Hierarchical Pooling [42]. Các thí nghiệm mở rộng trên bộ dữ liệu ImageNet cho thấy phương pháp của chúng tôi vượt trội hơn các phương pháp hiện có và cung cấp sự cân bằng tốt nhất giữa chi phí tính toán và độ chính xác phân loại. Chúng tôi cũng chứng minh hiệu quả của module đề xuất cho nhận dạng hành động bằng cách thêm nó vào các video vision transformers hiện đại như XViT [2] và TimeSformer [1]. Các thí nghiệm mở rộng trên bộ dữ liệu Kinetics-400 và Kinetics-600 cho thấy phương pháp của chúng tôi vượt qua hiệu suất của các phương pháp hiện có và dẫn đến sự cân bằng chi phí tính toán/độ chính xác tốt nhất. Tóm lại, bộ lấy mẫu token thích ứng có thể giảm đáng kể chi phí tính toán của các vision transformers sẵn có và do đó rất hữu ích cho các ứng dụng thị giác thực tế.

## 2 Công trình liên quan

Kiến trúc transformer, ban đầu được giới thiệu trong cộng đồng NLP [57], đã chứng minh hiệu suất đầy hứa hẹn trên các tác vụ thị giác máy tính khác nhau [3, 6, 13, 39, 47, 53, 66, 69–71]. ViT [13] tuân theo kiến trúc transformer chuẩn để tạo ra một mạng có thể áp dụng cho hình ảnh. Nó chia một hình ảnh đầu vào thành một tập các patch không chồng lấp và tạo ra các embedding patch có chiều thấp hơn. Mạng sau đó thêm các embedding vị trí vào các embedding patch và truyền chúng qua một số khối transformer. Một class embedding có thể học bổ sung cũng được thêm vào các embedding patch để thực hiện phân loại. Mặc dù ViT đã cho thấy kết quả đầy hứa hẹn trong phân loại hình ảnh, nó yêu cầu một lượng lớn dữ liệu để tổng quát hóa tốt. DeiT [53] đã giải quyết vấn đề này bằng cách giới thiệu một token chưng cất được thiết kế để học từ một mạng giáo viên. Ngoài ra, nó đã vượt qua hiệu suất của ViT. LV-ViT [31] đề xuất một hàm mục tiêu mới để đào tạo vision transformers và đạt hiệu suất tốt hơn. TimeSformer [1] đề xuất một kiến trúc mới để hiểu video bằng cách mở rộng cơ chế self-attention của các mô hình transformer chuẩn sang video. Độ phức tạp của self-attention của TimeSformer là O(T²S+TS²) trong đó T và S đại diện cho các vị trí thời gian và không gian tương ứng. X-ViT [2] đã giảm độ phức tạp này xuống O(TS²) bằng cách đề xuất một video transformer hiệu quả.

Bên cạnh độ chính xác của mạng neural, hiệu quả của chúng đóng vai trò quan trọng trong việc triển khai chúng trên các thiết bị edge. Một loạt các kỹ thuật đã được đề xuất để tăng tốc suy luận của các mô hình này. Để có được các mạng sâu có thể được triển khai trên các thiết bị edge khác nhau, các công trình như [52] đề xuất các kiến trúc hiệu quả hơn bằng cách điều chỉnh cẩn thận độ sâu, chiều rộng và độ phân giải của một mạng cơ sở dựa trên các ràng buộc tài nguyên khác nhau. [26] nhằm đáp ứng các yêu cầu tài nguyên như vậy bằng cách giới thiệu các siêu tham số, có thể được điều chỉnh để xây dựng các mô hình nhẹ hiệu quả. Các công trình [19,59] đã áp dụng các kỹ thuật lượng tử hóa để nén và tăng tốc các mô hình sâu. Bên cạnh các kỹ thuật lượng tử hóa, các phương pháp khác như pruning kênh [24], pruning neural thời gian chạy [45], phân tách ma trận hạng thấp [27,65], và chưng cất kiến thức [25,38] cũng đã được sử dụng để tăng tốc các mạng sâu.

Ngoài các công trình nhằm tăng tốc suy luận của mạng neural tích chập, các công trình khác nhằm cải thiện hiệu quả của các mô hình dựa trên transformer. Trong lĩnh vực NLP, Star-Transformer [21] đã giảm số lượng kết nối từ n² xuống 2n bằng cách thay đổi cấu trúc liên kết đầy đủ thành cấu trúc hình sao. TinyBERT [32] cải thiện hiệu quả của mạng bằng cách chưng cất kiến thức từ một giáo viên BERT lớn vào một mạng học sinh nhỏ. PoWER-BERT [20] giảm thời gian suy luận của mô hình BERT bằng cách xác định và loại bỏ các token dư thừa và ít thông tin dựa trên điểm quan trọng của chúng được ước tính từ trọng số self-attention của các khối transformer. Để giảm số lượng FLOPs trong mô hình hóa ngôn ngữ cấp ký tự, một cơ chế self-attention mới với span attention thích ứng được đề xuất trong [51]. Để cho phép hiệu suất nhanh trong giải mã không batch và cải thiện khả năng mở rộng của các transformer chuẩn, Scaling Transformers [28] được giới thiệu. Các kiến trúc transformer mới này được trang bị các biến thể thưa thớt của các lớp transformer chuẩn.

Để cải thiện hiệu quả của vision transformers, phân tích thưa thớt của ma trận attention dày đặc đã được đề xuất [7], giảm độ phức tạp của nó xuống O(n√n) cho tác vụ sinh ảnh tự hồi quy. [48] giải quyết vấn đề này bằng cách đề xuất một phương pháp làm thưa ma trận attention. Họ trước tiên gom nhóm tất cả các keys và queries và chỉ xem xét sự tương tự của các keys và queries thuộc cùng một nhóm. DynamicViT [46] đề xuất một module dự đoán bổ sung dự đoán tầm quan trọng của token và loại bỏ các token không có thông tin cho tác vụ phân loại hình ảnh. Hierarchical Visual Transformer (HVT) [42] sử dụng token pooling, tương tự như down-sampling feature map trong mạng neural tích chập, để loại bỏ các token dư thừa. PS-ViT [68] tích hợp một module lấy mẫu tiến bộ học cách lấy mẫu các token đầu vào đặc biệt thay vì lấy mẫu đồng nhất các token đầu vào từ khắp hình ảnh. Các token được lấy mẫu sau đó được đưa vào một module vision transformer với ít lớp transformer encoder hơn so với ViT. TokenLearner [49] giới thiệu một module tokenization có thể học giảm chi phí tính toán bằng cách học một số token quan trọng có điều kiện trên đầu vào. Họ đã chứng minh rằng phương pháp của họ có thể được áp dụng cho cả tác vụ hiểu hình ảnh và video. Token Pooling [40] down-sample token bằng cách gom chúng thành một tập các cụm và trả về các tâm cụm. Một công trình đồng thời [36] giới thiệu một phương pháp tổ chức lại token trước tiên xác định top-k token quan trọng bằng cách tính toán sự chú ý token giữa các token và token phân loại và sau đó kết hợp các token ít thông tin hơn. IA-RED2[41] đề xuất một framework giảm dư thừa nhận biết tính diễn giải cho vision transformers loại bỏ các patch ít thông tin hơn trong dữ liệu đầu vào. Hầu hết các phương pháp đã đề cập cải thiện hiệu quả của vision transformers bằng cách giới thiệu các thay đổi kiến trúc vào các mô hình gốc hoặc bằng cách thêm các module thêm các tham số có thể học bổ sung vào mạng, trong khi module thích ứng không tham số của chúng tôi có thể được tích hợp vào các kiến trúc sẵn có và giảm độ phức tạp tính toán của chúng mà không mất độ chính xác đáng kể và thậm chí không yêu cầu đào tạo thêm.

## 3 Adaptive Token Sampler

Các vision transformers hiện đại có chi phí tính toán đắt đỏ vì chi phí tính toán của chúng tăng theo bậc hai so với số lượng token, là tĩnh ở tất cả các giai đoạn của mạng và tương ứng với số lượng patch đầu vào. Mạng neural tích chập xử lý chi phí tính toán bằng cách giảm độ phân giải trong mạng sử dụng các phép toán pooling khác nhau. Điều này có nghĩa là độ phân giải không gian hoặc thời gian giảm ở các giai đoạn sau của mạng. Tuy nhiên, việc áp dụng các chiến lược đơn giản như vậy, tức là các phép toán pooling với kernel cố định, cho vision transformers không đơn giản vì các token là bất biến hoán vị. Hơn nữa, các phương pháp down-sampling tĩnh như vậy không tối ưu. Một mặt, một phương pháp down-sampling cố định loại bỏ thông tin quan trọng ở một số vị trí của hình ảnh hoặc video, như chi tiết của đối tượng. Mặt khác, nó vẫn bao gồm nhiều đặc trưng dư thừa không đóng góp vào độ chính xác phân loại, ví dụ, khi xử lý một hình ảnh với nền đồng nhất. Do đó, chúng tôi đề xuất một phương pháp thích ứng động số lượng token ở mỗi giai đoạn của mạng dựa trên dữ liệu đầu vào sao cho thông tin quan trọng không bị loại bỏ và không lãng phí tài nguyên tính toán để xử lý thông tin dư thừa.

Để đạt được điều này, chúng tôi đề xuất module Adaptive Token Sampler (ATS) mới của chúng tôi. ATS là một module khả vi không tham số để lấy mẫu các token quan trọng trên các token đầu vào. Trong module ATS của chúng tôi, trước tiên chúng tôi gán điểm quan trọng cho N token đầu vào và sau đó chọn một tập con của các token này dựa trên điểm của chúng. Giới hạn trên của GFLOPs có thể được đặt bằng cách định nghĩa một giới hạn tối đa cho số lượng token được lấy mẫu, ký hiệu là K. Vì quy trình lấy mẫu có thể lấy mẫu một số token đầu vào nhiều lần, chúng tôi chỉ giữ một thể hiện của một token. Số lượng token được lấy mẫu K' do đó thường thấp hơn K và thay đổi giữa các hình ảnh hoặc video đầu vào (Hình 6). Hình 1 đưa ra tổng quan về phương pháp đề xuất của chúng tôi.

### 3.1 Chấm điểm Token

Cho I ∈ R^(N+1)×d là các token đầu vào của một lớp self-attention với N+1 token. Trước khi chuyển tiếp các token đầu vào qua mô hình, ViT nối một token phân loại vào các token đầu vào. Token đầu ra tương ứng ở khối transformer cuối cùng sau đó được đưa vào đầu phân loại để có được xác suất lớp. Thực tế, token này được đặt làm token đầu tiên trong mỗi khối và được coi là token phân loại. Trong khi chúng tôi giữ token phân loại, mục tiêu của chúng tôi là giảm token đầu ra O ∈ R^(K'+1)×d sao cho K' được thích ứng động dựa trên hình ảnh hoặc video đầu vào và K' ≤ K ≤ N, trong đó K là một tham số kiểm soát số lượng token được lấy mẫu tối đa. Hình 6 cho thấy số lượng token được lấy mẫu K' thay đổi như thế nào cho các dữ liệu đầu vào và giai đoạn mạng khác nhau. Chúng tôi trước tiên mô tả cách mỗi token được chấm điểm.

Trong một lớp self-attention chuẩn [57], các queries Q ∈ R^(N+1)×d, keys K ∈ R^(N+1)×d, và values V ∈ R^(N+1)×d được tính từ các token đầu vào I ∈ R^(N+1)×d. Ma trận attention A sau đó được tính bằng tích vô hướng của queries và keys, được chia tỷ lệ bởi √d:

A = Softmax(QK^T/√d). (1)

Do hàm Softmax, mỗi hàng của A ∈ R^(N+1)×(N+1) có tổng bằng 1. Các token đầu ra sau đó được tính bằng cách sử dụng tổ hợp của các values được trọng số bởi trọng số attention:

O = AV. (2)

Mỗi hàng của A chứa trọng số attention của một token đầu ra. Các trọng số chỉ ra đóng góp của tất cả token đầu vào vào token đầu ra. Vì A₁,: chứa trọng số attention của token phân loại, A₁,j đại diện cho tầm quan trọng của token đầu vào j đối với token phân loại đầu ra. Do đó, chúng tôi sử dụng trọng số A₁,₂, ..., A₁,N+1 làm điểm quan trọng để pruning ma trận attention A, như được minh họa trong Hình 1. Lưu ý rằng A₁,₁ không được sử dụng vì chúng tôi giữ token phân loại. Vì các token đầu ra O phụ thuộc vào cả A và V (2), chúng tôi cũng tính đến norm của Vⱼ để tính điểm quan trọng của token thứ j. Động lực là các values có norm gần bằng không có tác động thấp và các token tương ứng của chúng do đó ít quan trọng hơn. Trong các thí nghiệm của chúng tôi, chúng tôi cho thấy việc nhân A₁,ⱼ với norm của Vⱼ cải thiện kết quả. Điểm quan trọng của token j do đó được cho bởi:

Sⱼ = (A₁,ⱼ × ||Vⱼ||) / (Σᵢ₌₂ᴺ A₁,ᵢ × ||Vᵢ||) (3)

trong đó i, j ∈ {2...N}. Đối với lớp multi-head attention, chúng tôi tính điểm cho mỗi head và sau đó cộng điểm trên tất cả các head.

### 3.2 Lấy mẫu Token

Sau khi tính được điểm quan trọng của tất cả token, chúng ta có thể pruning các hàng tương ứng của chúng từ ma trận attention A. Để làm điều này, một phương pháp ngây thơ là chọn K token với điểm quan trọng cao nhất (chọn top-K). Tuy nhiên, phương pháp này không hoạt động tốt, như chúng tôi cho thấy trong các thí nghiệm và nó không thể lựa chọn thích ứng K' ≤ K token. Lý do là nó loại bỏ tất cả token với điểm thấp hơn. Tuy nhiên, một số token này có thể hữu ích đặc biệt ở các giai đoạn đầu khi các đặc trưng ít phân biệt hơn. Ví dụ, có nhiều token với keys tương tự, có thể xảy ra ở giai đoạn đầu, sẽ làm giảm trọng số attention tương ứng của chúng do hàm Softmax. Mặc dù một trong các token này sẽ có lợi ở các giai đoạn sau, việc lấy top-K token có thể loại bỏ tất cả chúng. Do đó, chúng tôi đề xuất lấy mẫu token dựa trên điểm quan trọng của chúng. Trong trường hợp này, xác suất lấy mẫu một trong nhiều token tương tự bằng tổng điểm của chúng. Chúng tôi cũng quan sát thấy quy trình lấy mẫu đề xuất chọn nhiều token hơn ở các giai đoạn đầu so với các giai đoạn sau như được hiển thị trong Hình 2.

Đối với bước lấy mẫu, chúng tôi đề xuất sử dụng lấy mẫu biến đổi nghịch đảo để lấy mẫu token dựa trên điểm quan trọng S (3) của chúng. Vì các điểm được chuẩn hóa, chúng có thể được diễn giải như xác suất và chúng ta có thể tính hàm phân phối tích lũy (CDF) của S:

CDFᵢ = Σⱼ₌₂ⁱ Sⱼ. (4)

Lưu ý rằng chúng tôi bắt đầu với token thứ hai vì chúng tôi giữ token đầu tiên. Có hàm phân phối tích lũy, chúng ta có được hàm lấy mẫu bằng cách lấy nghịch đảo của CDF:

Ψ(k) = CDF⁻¹(k) (5)

trong đó k ∈ [0,1]. Nói cách khác, điểm quan trọng được sử dụng để tính hàm ánh xạ giữa các chỉ số của token gốc và token được lấy mẫu. Để có được K mẫu, chúng ta có thể lấy mẫu K lần từ phân phối đồng nhất U[0,1]. Mặc dù việc ngẫu nhiên hóa như vậy có thể mong muốn cho một số ứng dụng, suy luận xác định trong hầu hết các trường hợp được ưu tiên. Do đó, chúng tôi sử dụng một sơ đồ lấy mẫu cố định cho đào tạo và suy luận bằng cách chọn k = {1/(2K), 3/(2K)..., (2K-1)/(2K)}. Vì Ψ(.) ∈ R, chúng tôi coi các chỉ số của token với điểm quan trọng gần nhất là chỉ số lấy mẫu.

Nếu một token được lấy mẫu nhiều hơn một lần, chúng tôi chỉ giữ một thể hiện. Kết quả là, số lượng chỉ số duy nhất K' thường thấp hơn K như được hiển thị trong Hình 6. Thực tế, K' < K nếu có ít nhất một token với điểm Sⱼ ≥ 2/K. Trong hai trường hợp cực đoan, hoặc chỉ một token chiếm ưu thế được chọn và K' = 1 hoặc K' = K nếu điểm tương đối cân bằng. Thú vị là, nhiều token được chọn ở các giai đoạn đầu, nơi các đặc trưng ít phân biệt hơn và trọng số attention cân bằng hơn, và ít hơn ở các giai đoạn sau, như được hiển thị trong Hình 2. Số lượng và vị trí của token cũng thay đổi cho các hình ảnh đầu vào khác nhau, như được hiển thị trong Hình 7. Đối với hình ảnh với nền đồng nhất bao phủ một phần lớn của hình ảnh, chỉ có một vài token được lấy mẫu. Trong trường hợp này, các token bao phủ đối tượng ở phần nền và được lấy mẫu thưa thớt nhưng đồng nhất từ nền. Trong hình ảnh lộn xộn, nhiều token là cần thiết. Điều này minh họa tầm quan trọng của việc làm cho quy trình lấy mẫu token thích ứng.

Có các chỉ số của token được lấy mẫu, chúng tôi tinh chỉnh ma trận attention A ∈ R^(N+1)×(N+1) bằng cách chọn các hàng tương ứng với K'+1 token được lấy mẫu. Chúng tôi ký hiệu ma trận attention được tinh chỉnh bởi Aₛ ∈ R^(K'+1)×(N+1). Để có được token đầu ra O ∈ R^(K'+1)×d, chúng tôi do đó thay thế ma trận attention A bằng ma trận được tinh chỉnh Aₛ trong (2) sao cho:

O = AₛV. (6)

Các token đầu ra này sau đó được lấy làm đầu vào cho giai đoạn tiếp theo. Trong đánh giá thực nghiệm của chúng tôi, chúng tôi chứng minh hiệu quả của bộ lấy mẫu token thích ứng đề xuất, có thể được thêm vào bất kỳ vision transformer nào.

## 4 Thí nghiệm

Trong phần này, chúng tôi phân tích hiệu suất của module ATS bằng cách thêm nó vào các mô hình backbone khác nhau và đánh giá chúng trên ImageNet [9], Kinetics-400 [33], và Kinetics-600 [4], là các bộ dữ liệu phân loại hình ảnh và video quy mô lớn, tương ứng. Ngoài ra, chúng tôi thực hiện một số nghiên cứu ablation để phân tích tốt hơn phương pháp của chúng tôi. Đối với tác vụ phân loại hình ảnh, chúng tôi đánh giá phương pháp đề xuất trên bộ dữ liệu ImageNet [9] với 1.3M hình ảnh và 1K lớp. Đối với tác vụ phân loại hành động, chúng tôi đánh giá phương pháp trên các bộ dữ liệu Kinetics-400 [33] và Kinetics-600 [4] với 400 và 600 lớp hành động con người, tương ứng. Chúng tôi sử dụng các phân chia đào tạo/kiểm tra chuẩn và các giao thức được cung cấp bởi các bộ dữ liệu ImageNet và Kinetics. Nếu không nói khác, số lượng token đầu ra của module ATS được giới hạn bởi số lượng token đầu vào của nó. Ví dụ, chúng tôi đặt K = 197 trong trường hợp của DeiT-S [53]. Đối với tác vụ phân loại hình ảnh, chúng tôi tuân theo thiết lập tinh chỉnh của [46] nếu không được đề cập khác. Các mô hình được tinh chỉnh được khởi tạo bằng trọng số được đào tạo trước của backbone và được đào tạo trong 30 epoch sử dụng bộ tối ưu PyTorch AdamW (lr = 5e-4, batch size = 8×96). Chúng tôi sử dụng bộ lập lịch cosine để đào tạo mạng. Để biết thêm chi tiết triển khai và thông tin về các mô hình phân loại hành động, vui lòng tham khảo tài liệu bổ sung.

### 4.1 Thí nghiệm Ablation

Đầu tiên, chúng tôi phân tích các thiết lập khác nhau cho module ATS. Sau đó, chúng tôi điều tra hiệu quả và ảnh hưởng của module ATS khi được tích hợp trong các mô hình khác nhau. Nếu không nói khác, chúng tôi sử dụng mô hình DeiT-S [53] được đào tạo trước làm backbone và chúng tôi không tinh chỉnh mô hình sau khi thêm module ATS. Chúng tôi tích hợp module ATS vào giai đoạn 3 của mô hình DeiT-S [53]. Chúng tôi báo cáo kết quả trên tập validation ImageNet-1K trong tất cả các nghiên cứu ablation.

**Điểm quan trọng** Như đã đề cập trong Mục 3.1, chúng tôi sử dụng trọng số attention của token phân loại làm điểm quan trọng để chọn token ứng viên. Trong thí nghiệm này, chúng tôi đánh giá các phương pháp khác nhau để tính điểm quan trọng. Thay vì sử dụng trực tiếp trọng số attention của token phân loại, chúng tôi cộng trọng số attention của tất cả token (hàng của ma trận attention) để tìm token có tầm quan trọng cao nhất đối với các token khác. Chúng tôi hiển thị kết quả của phương pháp này trong Hình 3 được gắn nhãn là điểm Self-Attention. Như có thể thấy, việc sử dụng trọng số attention của token phân loại hoạt động tốt hơn đặc biệt trong chế độ FLOPs thấp. Kết quả cho thấy trọng số attention của token phân loại là tín hiệu mạnh hơn nhiều để chọn token ứng viên. Lý do là token phân loại sau này sẽ được sử dụng để dự đoán xác suất lớp trong giai đoạn cuối của mô hình. Do đó, trọng số attention tương ứng của nó cho thấy token nào có tác động nhiều hơn đến token phân loại đầu ra. Trong khi việc cộng tất cả trọng số attention chỉ cho chúng ta thấy các token có attention cao nhất từ tất cả token khác, có thể không nhất thiết hữu ích cho token phân loại. Để điều tra tốt hơn quan sát này, chúng tôi cũng chọn ngẫu nhiên một token khác thay vì token phân loại và sử dụng trọng số attention của nó để gán điểm. Như được hiển thị, phương pháp này hoạt động tệ hơn nhiều so với các phương pháp khác cả trong chế độ FLOPs cao và thấp. Chúng tôi cũng điều tra tác động của việc sử dụng norm L2 của các values trong (3). Như có thể thấy trong Hình 3, nó cải thiện kết quả khoảng 0.2%.

**Lựa chọn Token ứng viên** Như đã đề cập trong Mục 3.2, chúng tôi sử dụng phương pháp lấy mẫu biến đổi nghịch đảo để down-sample mềm các token đầu vào. Để điều tra tốt hơn phương pháp này, chúng tôi cũng đánh giá hiệu suất của mô hình khi chọn top K token có điểm quan trọng S cao nhất. Như có thể thấy trong Hình 5a, phương pháp lấy mẫu biến đổi nghịch đảo của chúng tôi vượt trội hơn lựa chọn Top-K cả trong chế độ GFLOPs cao và thấp. Như đã thảo luận trước đó, phương pháp lấy mẫu biến đổi nghịch đảo dựa trên CDF của điểm không loại bỏ cứng tất cả token có điểm quan trọng thấp hơn và do đó cung cấp một tập token đa dạng hơn cho các lớp tiếp theo. Vì các khối transformer đầu có xu hướng dự đoán trọng số attention ồn ào hơn cho token phân loại, một tập token đa dạng như vậy có thể đóng góp tốt hơn cho token phân loại đầu ra của khối transformer cuối. Hơn nữa, phương pháp lựa chọn Top-K sẽ dẫn đến tỷ lệ lựa chọn token cố định ở mọi giai đoạn, hạn chế hiệu suất của mô hình backbone. Điều này được thể hiện bằng các ví dụ trong Hình 2. Đối với hình ảnh lộn xộn (dưới), lấy mẫu biến đổi nghịch đảo giữ số lượng token cao hơn trên tất cả các khối transformer so với lựa chọn Top-K và do đó bảo toàn độ chính xác. Mặt khác, đối với hình ảnh ít chi tiết hơn (trên), lấy mẫu biến đổi nghịch đảo sẽ giữ lại ít token hơn, dẫn đến chi phí tính toán ít hơn.

**Mở rộng Mô hình** Một phương pháp phổ biến khác để thay đổi sự cân bằng GFLOPs/độ chính xác của mạng là thay đổi chiều kênh. Để chứng minh hiệu quả của phương pháp lấy mẫu token thích ứng, chúng tôi do đó thay đổi chiều. Để làm điều này, trước tiên chúng tôi đào tạo một số mô hình DeiT với chiều embedding khác nhau. Sau đó, chúng tôi tích hợp module ATS vào các giai đoạn 3 đến 11 của các backbone DeiT này và tinh chỉnh mạng. Trong Hình 4, chúng ta có thể quan sát rằng phương pháp của chúng tôi có thể giảm GFLOPs 37% trong khi duy trì độ chính xác của backbone DeiT-S. Chúng ta cũng có thể quan sát rằng tỷ lệ giảm GFLOPs tăng cao hơn khi chúng ta tăng chiều embedding từ 192 (DeiT-Ti) lên 384 (DeiT-S). Kết quả cho thấy module ATS của chúng tôi có thể giảm chi phí tính toán của các mô hình với chiều embedding lớn hơn xuống các biến thể với chiều embedding nhỏ hơn.

**Trực quan hóa** Để hiểu rõ hơn cách module ATS hoạt động, chúng tôi trực quan hóa quy trình lấy mẫu token (Lấy mẫu Biến đổi Nghịch đảo) trong Hình 2. Chúng tôi đã tích hợp module ATS vào các giai đoạn 3 đến 11 của mạng DeiT-S. Các token bị loại bỏ ở mỗi giai đoạn được biểu diễn như một mặt nạ trên hình ảnh đầu vào. Chúng tôi quan sát thấy mô hình DeiT-S+ATS đã dần dần loại bỏ các token không liên quan và lấy mẫu những token quan trọng hơn đối với dự đoán của mô hình. Trong cả hai ví dụ, phương pháp của chúng tôi đã xác định các token liên quan đến đối tượng mục tiêu là các token thông tin nhất.

**Lấy mẫu Thích ứng** Trong thí nghiệm này, chúng tôi điều tra tính thích ứng của phương pháp lấy mẫu token. Chúng tôi đánh giá mô hình DeiT-S+ATS đa giai đoạn trên tập validation ImageNet. Trong Hình 6, chúng tôi trực quan hóa biểu đồ của số lượng token được lấy mẫu ở mỗi giai đoạn ATS. Chúng ta có thể quan sát rằng số lượng token được chọn thay đổi ở tất cả các giai đoạn và cho tất cả hình ảnh. Chúng tôi cũng phân tích định tính thuộc tính tốt này của module ATS trong Hình 2 và 7. Chúng ta có thể quan sát rằng module ATS chọn số lượng token cao hơn khi xử lý hình ảnh chi tiết và phức tạp trong khi chọn số lượng token thấp hơn cho hình ảnh ít chi tiết.

**Tinh chỉnh** Để khám phá ảnh hưởng của tinh chỉnh đến hiệu suất phương pháp, chúng tôi tinh chỉnh mô hình DeiT-S+ATS trên tập đào tạo ImageNet. Chúng tôi so sánh mô hình có và không tinh chỉnh. Như được hiển thị trong Hình 5b, tinh chỉnh có thể cải thiện độ chính xác của mô hình. Trong thí nghiệm này, chúng tôi tinh chỉnh mô hình với K = 197 nhưng kiểm tra nó với các giá trị K khác nhau để đạt được mức GFLOPs mong muốn.

**Giai đoạn ATS** Trong thí nghiệm này, chúng tôi khám phá ảnh hưởng của tích hợp đơn giai đoạn và đa giai đoạn của khối ATS vào các mô hình vision transformer. Trong mô hình đơn giai đoạn, chúng tôi tích hợp module ATS vào giai đoạn 3 của DeiT-S. Trong mô hình đa giai đoạn, chúng tôi tích hợp module ATS vào các giai đoạn 3 đến 11 của DeiT-S. Như có thể thấy trong Hình 5c, DeiT-S+ATS đa giai đoạn hoạt động tốt hơn DeiT-S+ATS đơn giai đoạn. Điều này là do mô hình DeiT-S+ATS đa giai đoạn có thể giảm dần GFLOPs bằng cách loại bỏ ít token hơn ở các giai đoạn đầu, trong khi mô hình DeiT-S+ATS đơn giai đoạn phải loại bỏ nhiều token hơn ở các giai đoạn đầu để đạt được cùng mức GFLOPs.

### 4.2 So sánh với hiện đại

Chúng tôi so sánh hiệu suất của các mô hình thích ứng được trang bị module ATS với các vision transformers hiện đại cho phân loại hình ảnh và video trên các bộ dữ liệu ImageNet-1K [9] và Kinetics [4,33], tương ứng. Bảng 1-3 hiển thị kết quả của so sánh này. Đối với tác vụ phân loại hình ảnh, chúng tôi tích hợp module ATS vào các giai đoạn 3 đến 11 của mô hình DeiT-S [53]. Chúng tôi cũng tích hợp module ATS vào các khối thứ 1 đến 9 của giai đoạn thứ 3 của CvT-13 [63] và CvT-21 [63], và vào các giai đoạn 1-9 của module transformer của PS-ViT [68]. Chúng tôi tinh chỉnh các mô hình trên tập đào tạo ImageNet-1K. Chúng tôi cũng đánh giá module ATS cho nhận dạng hành động. Để làm điều này, chúng tôi thêm module vào các video vision transformers XViT [2] và TimeSformer [1]. Để biết thêm chi tiết, vui lòng tham khảo tài liệu bổ sung.

**Phân loại Hình ảnh** Như có thể thấy trong Bảng 1, module ATS giảm GFLOPs của tất cả các mô hình vision transformer mà không thêm bất kỳ tham số bổ sung nào vào các mô hình backbone. Đối với mô hình DeiT-S+ATS, chúng tôi quan sát giảm 37% GFLOPs chỉ với 0.1% giảm độ chính xác top-1. Đối với các mô hình CvT+ATS, chúng ta cũng có thể quan sát giảm GFLOPs khoảng 30% với 0.1-0.2% giảm độ chính xác top-1. Chi tiết thêm về hiệu quả của module ATS có thể tìm thấy trong tài liệu bổ sung (ví dụ throughput). So sánh ATS với DynamicViT [46] và HVT [42], thêm tham số bổ sung vào mô hình, phương pháp của chúng tôi đạt được sự cân bằng tốt hơn giữa độ chính xác và GFLOPs. Phương pháp của chúng tôi cũng vượt trội hơn mô hình EViT-DeiT-S [36] được đào tạo trong 30 epoch mà không thêm bất kỳ tham số có thể đào tạo bổ sung nào vào mô hình. Chúng tôi lưu ý rằng mô hình EViT-DeiT-S có thể cải thiện độ chính xác top-1 khoảng 0.3% khi được đào tạo trong nhiều epoch hơn (ví dụ 100 epoch). Để so sánh công bằng, chúng tôi đã xem xét thiết lập đào tạo 30 epoch được sử dụng bởi Dynamic-ViT [46]. Chúng tôi cũng đã thêm module ATS vào mạng PS-ViT [68]. Như có thể thấy trong Bảng 1, mặc dù PS-ViT có GFLOPs thấp hơn đáng kể so với các đối tác, GFLOPs của nó có thể được giảm thêm bằng cách tích hợp ATS vào nó.

**Nhận dạng Hành động** Như có thể thấy trong Bảng 2 và 3, module ATS giảm mạnh GFLOPs của tất cả video vision transformers mà không thêm bất kỳ tham số bổ sung nào vào các mô hình backbone. Đối với mô hình XViT+ATS, chúng tôi quan sát giảm 39% GFLOPs chỉ với 0.2% giảm độ chính xác top-1 trên Kinetics-400 và giảm 38.7% GFLOPs chỉ với 0.1% giảm độ chính xác top-1 trên Kinetics-600. Chúng tôi quan sát rằng XViT+ATS đạt độ chính xác tương tự như TokenLearner [49] trên Kinetics-600 trong khi yêu cầu ít hơn 17.6× GFLOPs. Đối với TimeSformer-L+ATS, chúng ta có thể quan sát giảm 50.8% GFLOPs chỉ với 0.2% giảm độ chính xác top-1 trên Kinetics-400. Những kết quả này chứng minh tính tổng quát của phương pháp có thể được áp dụng cho cả biểu diễn hình ảnh và video.

## 5 Kết luận

Thiết kế các mô hình vision transformer hiệu quả về mặt tính toán cho nhận dạng hình ảnh và video là một tác vụ thách thức. Trong công trình này, chúng tôi đề xuất một module khả vi không tham số mới gọi là Adaptive Token Sampler (ATS) để tăng hiệu quả của vision transformers cho phân loại hình ảnh và video. Module ATS mới chọn các token thông tin và đặc biệt nhất trong các giai đoạn của mô hình vision transformer sao cho sử dụng nhiều token như cần thiết nhưng không nhiều hơn cần thiết cho mỗi hình ảnh hoặc clip video đầu vào. Bằng cách tích hợp module ATS vào các lớp attention của vision transformers hiện tại, sử dụng số lượng token tĩnh, chúng ta có thể chuyển đổi chúng thành vision transformers hiệu quả hơn nhiều với số lượng token thích ứng. Chúng tôi đã cho thấy module ATS có thể được thêm vào các vision transformers được đào tạo trước sẵn có như một module cắm và chạy, do đó giảm GFLOPs của chúng mà không cần đào tạo bổ sung, nhưng cũng có thể đào tạo một vision transformer được trang bị module ATS nhờ thiết kế khả vi của nó. Chúng tôi đánh giá phương pháp trên bộ dữ liệu nhận dạng hình ảnh ImageNet-1K và tích hợp module ATS vào ba vision transformers hiện đại khác nhau. Chúng tôi cũng chứng minh tính tổng quát của phương pháp bằng cách tích hợp nó vào các video vision transformers hiện đại khác nhau và đánh giá chúng trên các bộ dữ liệu Kinetics-400 và Kinetics-600. Kết quả cho thấy module ATS giảm chi phí tính toán (GFLOPs) từ 27% đến 50.8% với sự giảm độ chính xác không đáng kể. Mặc dù các thí nghiệm của chúng tôi tập trung vào vision transformers hình ảnh và video, chúng tôi tin rằng phương pháp của chúng tôi cũng có thể hoạt động trong các lĩnh vực khác như âm thanh.
