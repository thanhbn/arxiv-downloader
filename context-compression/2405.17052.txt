# 2405.17052.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/context-compression/2405.17052.pdf
# File size: 1089097 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Highlights
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Jun Gao,Ziqiang Cao,Wenjie Li
•Toourknowledge,wearethefirsttousethefrozenLLMitselftocompressover-limitpromptsinto1/12memory
tokens, which is more general and less expensive.
•We propose a more dialectical prompt compression perspective that achieves balances among training cost,
inference efficiency, and generation quality,arXiv:2405.17052v2  [cs.CL]  18 Jun 2024

--- PAGE 2 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large
Language Model Itself⋆,⋆⋆
Jun Gao, Ziqiang Cao∗and Wenjie Li
Institute of Artificial Intelligence, Soochow University, Suzhou, China
ARTICLE INFO
Keywords :
Large Language Models
Prompt Compression
Efficient/Low-Resource MethodsABSTRACT
Long prompt leads to huge hardware costs when using transformer-based Large Language
Models (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long
documents, and the wide application of in-context learning easily makes the prompt length
explode. This paper proposes a Self-Compressor (SelfCP), which employs the target LLM
itself to compress over-limit prompts into dense vectors while keeping the allowed prompts
unmodified. Dense vectors are then projected into dense tokens via a learnable connector to
make the same LLM unburden to understand. The connector is supervised-tuned under the
languagemodelingobjectiveoftheLLMonrelativelylongtextsselectedfrompubliclyaccessed
datasets, involving an instruction dataset to make SelfCP respond to various prompts, while
the target LLM keeps frozen during training. We build the lightweight SelfCP upon 2 different
backboneswithmerely17Mlearnableparametersoriginatingfromtheconnectorandalearnable
embedding. Evaluation on both English and Chinese benchmarks demonstrate that SelfCP
effectively substitutes 12 ×over-limit prompts with dense tokens to reduce memory costs and
booster inference throughputs, yet improving response quality. The outstanding performance
bringsanefficientsolutionforLLMstotacklelongpromptswithouttrainingLLMsfromscratch.
1. Introduction
Figure 1: SelfCP generates memory tokens for each segment to substitute the original in-context demonstrations, guiding
the LLM to respond to the query correctly.
Transformer-based Large Language Models (LLMs) exhibit general spectacular emergent abilities Wang, Xie,
Ding,FengandXia(2023b);Yang,Li,Zhang,ChenandCheng(2023);Wei,Cui,Cheng,Wang,Zhang,Huang,Xie,
Xu, Chen, Zhang et al. (2023); Wang, Liang, Meng, Shi, Li, Xu, Qu and Zhou (2023a). However, the performance
⋆Theworkdescribedinthispaper wassupportedbytheNationalNaturalScienceFoundationofChina(NSFC62106165)and ProjectFunded
by the Priority Academic Program Development of Jiangsu Higher Education Institutions.
⋆⋆The code and data has been released in https://github.com/jungao1106/SelfCP
∗Corresponding author
junegao1106@gmail.com (J. Gao); zqcao@suda.edu.cn (Z. Cao); cswjli@comp.polyu.edu.hk (W. Li)
ORCID(s):0000-0002-1077-9033 (Z. Cao)
Jun Gao et al.: Preprint submitted to Elsevier Page 1 of 15

--- PAGE 3 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
of LLMs heavily relies on well-designed prompts, while lengthy prompts result in memory explosion and bring other
efficiency problems. Unfortunately, prompts for many tasks have to include long inputs, such as question answering
(QA) Ghosal, Shen, Majumder, Mihalcea and Poria (2022); Li, Yang, Wang, Wei and Li (2023b), summarization
Narayan,CohenandLapata(2018);Sun,Yuan,Li,CaoandLi(2024);Wang,Zhao,Ji,Jiang,Li,HuandLu(2024),and
query-focused summarization (QFS) Copeck, Inkpen, Kazantseva, Kennedy, Kipp, Nastase and Szpakowicz (2006).
Meanwhile, the wide application of In-Context Learning (ICL) further surges the length of prompts by introducing
many demonstrations.
ExceptforeffortsinexpandingtheinputwindowZheng,WangandKong(2022);Wu,Rabe,HutchinsandSzegedy
(2022); Ding, Ma, Dong, Zhang, Huang, Wang and Wei (2023); Bulatov, Kuratov, Kapushev and Burtsev (2023),
previous works focus more on prompt pruning Jiang, Wu, Lin, Yang and Qiu (2023) or replacing prompts with soft
promptsWingate, ShoeybiandSorensen (2022);Ge, Hu,Wang,Chen andWei(2023); Mu,Li andGoodman(2023).
However, LLMs with million context windows still struggle to overcome performance degradation Liu, Lin, Hewitt,
Paranjape,Bevilacqua,PetroniandLiang(2024)inaddressinglongprompts.Promptpruningmethodsarefacedwith
Out-of-Distribution(OoD)problems,requiringexpensivealignmentbetweencompressorsandtargetLLMsLi(2023);
Jiangetal.(2023).Previoussoftpromptcompressionmethodsrespondtoqueriesmerelyconditionedonsoftprompts
withthepursuittominimizepromptlengthGeetal.(2023),whichincreasesgenerationdifficultyandrequirestraining
the model from scratch Mu et al. (2023); Wingate et al. (2022). These challenges form an impossible triangle: we
cannot simultaneously manage training costs, inference latency, and response quality. Distinguished from previous
attempts on soft prompt compression, we provide a dialectical perspective to compress prompts, achieving a tradeoff
among these factors.
With these goals in mind, we propose SelfCP as illustrated in Figure 1, which leverages the comprehension
capabilities of LLMs developed during pre-training to compress over-limit prompts. Therefore, an extra compression
module is not necessary to introduce since SelfCP employs the target LLM to compress prompts, reducing GPU
memory for both training and inference initially. Furthermore, motivated by the success of Perceiver-based Visual
Language Models (VLMs) that extracted visual features via a frozen Visual Transformer (ViT) Dosovitskiy, Beyer,
Kolesnikov,Weissenborn,Zhai,Unterthiner,Dehghani,Minderer,Heigold,Gellyetal.(2020),wekeepthetargetLLM
frozenduringtraining1,reducingthemajortrainingcostsfromtheunderlyingbackbone.However,asthebuilt-inLLM
ofVLMfailstounderstandthevisualfeaturesextractedfromViTwithoutanadapter(e.g.,Q-FormerLi,Li,Savarese
andHoi(2023a)orMulit-LayerPerceptron(MLP)),aconnectorisimportantforSelfCPtoconverttheoutputfeatures
ofthecompressorintoreadablecompressedtokens(calledmemorytokensinthispaper)forthegenerator.Wesimply
utilizealinearlayersupervised-tunedunderthelanguagemodelingobjectiveofthegenerator,servingastheconnector
between the compressor and the generator, and we call for more exploration for the selection of the adapter. Previous
studies responded to the query conditioned on plain soft prompts, which increased task difficulty, training costs, and
wasted reasonable context window usage of their generator. Inspired by the fact that some segments within prompts
are valuable and informative while compressing them seems not necessary2, we explore an intermediate solution that
allows SelfCP to only compress over-limit prompts (e.g., the originally truncated or relatively less important ones)
rather than compressing everything casually. The rest unmodified prompts are then fed into the generator directly.
This solution takes full advantage of the allowed windows wasted in the previous studies and reduces the
compression and generation difficulty, but it is still faced with the following challenges: (1) The compressed soft
promptcan’tbeposedinfrontalwayslikebefore,sincetheover-limitpromptshavethepotentialtobeboththeformer
and the latter part. (2) The over-limit prompts may be still too long for the compressor to accept while dropping
directly conflicts with our goal. Distinguished from previous studies fixing the compression mode, we diversify the
compression strategies to cater to further requirements in various scenarios to approach these challenges: (a) Former
Compression compressestheformer halfofthepromptandputsthememorytokensinfrontofthelatter uncompressed
one. (b)Latter Compression compresses the later half of prompts and puts the memory tokens behind the former
uncompressed one. (c) Concatenated Compression compress several sub-prompts (e.g., in-context demonstrations)
into local memory tokens independently and concatenate them to formulate global memory tokens. We build SelfCP
upontwobackbones(Vicuna-7bandBlueLM-7b)toverifythegeneralizabilityandevaluateSelfCPacrossofascope
out-domaintasksincludinggenerativeandunderstandingtasks.Then,thein-domainvalidationexperimentsshowthat
SelfCP optimized with the three compression strategies effectively substitutes the 12×over-limit prompt with soft
prompts. Our main contributions are as follows:
1For clarity, the frozen LLM plays two roles in SelfCP for compression and generation is called compressor andgenerator following.
2The former part of documents in text summarization are empirically more important and more informative.
Jun Gao et al.: Preprint submitted to Elsevier Page 2 of 15

--- PAGE 4 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
•Toourknowledge,wearethefirsttousethefrozenLLMitselftocompressover-limitpromptsinto1/12memory
tokens, which is more general and less expensive.
•We propose a more dialectical prompt compression perspective that achieves balances among training cost,
inference efficiency, and generation quality,
2. Related Work
2.1. Soft Prompt
Differentfromnormalpromptsconsistingofdiscreteactualtokens,eachcorrespondingtopre-trainedembedding,
soft prompts were continuous newly initialed embeddings. On the one hand, soft prompts were usually used as a
parameter-efficient method, such as Prefix-TuningLi and Liang (2021) and P-Tuning Liu, Ji, Fu, Tam, Du, Yang
and Tang (2022), while keeping the backbone frozen and tuning the newly initialized embeddings for each task.
On the other hand, researchers attempted to utilize soft prompts to compact prompts from concrete sequences to
virtual tokens. Mostly from a distillation perspective, Wingate et al. (2022) aligned the teacher model and the student
model, where the teacher model accepted the actual prompt while the student model fed the soft prompt. The main
drawback of this approach was the lack of generalization that necessitated training for each lexical different task-
specific instruction. Totackle the generalization problem,Mu et al. (2023) proposedto learn a Llama-7b tocompress
instruction to virtual tokens, preceding attention past key values similar to Prefix Li and Liang (2021), but only
compressinstructionswasnotpowerfulenoughsincethedemonstrationsorinputwasmuchlongerthaninstructionin
many tasks such as summarization and QA. To compress the long prompt, Chevalier, Wettig, Ajith and Chen (2023)
proposed AutoCompressor to generate compressed virtual tokens based on a fine-tuned OPT-2.7b Zhang, Roller,
Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et al. (2022). They first randomized segmented the texts with
thousands of words into model-accepted range and then recursively generated soft prompts for each segment, and
theprevioussoftpromptswouldbeconcatenatedwiththecurrentsegmenttogeneratenewsoftprompts.However,not
only did AutoCompressor require fine-tuning on a large training set, but also encoding soft prompts would be much
slower and bring extra memory cost due to Recurrent Memory Transformer (RMT) Bulatov, Kuratov and Burtsev
(2022).Similarly,Geetal.(2023)proposedICAEthatemployedaLoRA-adoptedLlama-7bTouvron,Lavril,Izacard,
Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar et al. (2023) to compress the plain processed prompts to
compressedvirtualtokens.However,ICAEstillstruggledtoaddresspromptslongerthantheallowableinputwindow.
2.2. Extractive Compression
Apart from employing soft prompts, researchers also endeavored to shorten prompts by extracting informative
tokens from the original ones Li (2023); Jiang et al. (2023), namely token pruning Kim, Shen, Thorsley, Gholami,
Kwon, Hassoun and Keutzer (2022) or token merging Bolya, Fu, Dai, Zhang, Feichtenhofer and Hoffman (2022).
Recent works like LLMLingua Jiang et al. (2023) and Selective Context Li (2023) shared similarities but diverged
on whether to eliminate tokens with high or low Perplexity (PPL). LLMLingua emphasized tokens with high PPL,
attributing them as more influential, resulting in achieving state-of-the-art (SOTA) performance. As mentioned in
their paper extractive compression methods encountered Out-of-Distribution (OoD) issues between the extractor and
the target LLM. To reconcile this, they fine-tuned Alpaca-7b Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang
andHashimoto(2023)orGPT2-AlpacausingtheAlpacadatasetTaorietal.(2023)toaligntotargetLLMs.However,
extractivecompressionmethodsheavilyhingedonthetargetLLM’sabilitytounderstanddiscretetokens,andalignment
was usually quite expensive and tailored to each target.
2.3. Long Input Transformer
Significant training costs and hardware support limited the input length for pre-trained language models. A
series of previous works focused on sparsifying the full attention window to linear attention Dai, Yang, Yang,
Carbonell, Le and Salakhutdinov (2019); Child, Gray, Radford and Sutskever (2019); Beltagy, Peters and Cohan
(2020), with a trade-off between efficiency and attention horizon. Other works approximated or replaced the entire
attention mechanismKatharopoulos, Vyas, Pappas and Fleuret (2020); Choromanski, Likhosherstov, Dohan, Song,
Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser et al. (2020); Lee-Thorp, Ainslie, Eckstein and Ontanon (2021).
However, rarefying and approximating or replacing changed the architecture of the standard transformer or training
objectiveZhong,LeiandChen(2022)andthereforenecessitatedtrainingfromscratch,whichwasexpensive,especially
scaling the model or training sets. Bertsch, Alon, Neubig and Gormley (2023) proposed to offload the cross-attention
Jun Gao et al.: Preprint submitted to Elsevier Page 3 of 15

--- PAGE 5 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Figure 2: Differences of compression methods based on soft prompt in formulating soft prompts. SelfCP takes advantage
of the unlimited input window in AutoCompressor and the constant compression time complexity in ICAE.
computation to a single 𝑘-nearest-neighbor ( 𝑘NN) index. Although 𝑘NN was parameter-free, retrieving hidden states
oftheencoderineachgenerationstepwouldslowinference,andtheirapproacheswouldbreakdownwhenfacedwith
decoder-only models.
SelfCP keeps the LLM frozen and has no limitation on building upon existing powerful LLMs. Hence, the above
approaches can further empower SelfCP theoretically.
3. Methodology
We propose the SelfCP, a parameter-efficient model that compresses prompts via the LLM itself. As for the
selection of the underlying LLM, previous work has proved that the Decoder-only model performs better than the
Encoder-DecodermodelincompressioninstructionsMuetal.(2023).WefollowthisconclusionandemployVicuna-
7b Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing et al. (2023) and BlueLM-7b Team (2023) as our
two independent backbones, as the representation of LLMs majoring in English and Chinese.
Mostcompressionmethodsbasedonsoftpromptsinnovateonthecompressor.Weillustratethedifferencesinthe
soft prompt formulation process for SelfCP, AutoCompressor, and ICAE in Figure 2. To explain plainly, we ideally
assume the compressor and the generator of three compression methods have the window limitation of 𝐿, have the
same compression ratio, and now comes a prompt has a length 𝑁, where 𝑁= 3× 𝐿, ignoring the length of soft
prompts and the query. Considering AutoCompressor, the prompt will be divided into three segments (𝑆1,𝑆2,𝑆3),
andAutoCompressorcompresseseachsegmentstep-by-step.Notably,weprovideAutoCompressoratolerancesetting
that the prompt is evenly divided where they were randomly segmented originally, but AutoCompressor still requires
3 times non-parallel compression. When it comes to ICAE, merely 1/3 prompt is accessible for the compressor and
otherswillbereadbynomeans.Inthiscase,ICAErequiresonly1timetoperformcompressionalways,sincetherest
is dropout.
AutoCompressor shows advantages in the readable prompt length, but is short in efficiency, while ICAE has a
constantcompressionperplexitybutstrugglestoapproachquitelongprompts.SelfCPlearnsfromeachotherskillfully
thatitscompressorcompresseseachsegmentasynchronouslyandconcatenatesthemastheAutoCompressor.Although
SelfcpcompressesonelesssegmentthanAutoCompressor,thesegmentisdirectlyprovidedtothegeneratortobalance
theoverloadbetweenthecompressorandthegenerator.Weareinclinedtoconsiderthisasatrade-offamongtraining
costs, inference efficiency, and response quality. This trade-off is more worthwhile when the unmodified segment is
more important than others.
The number of practical compression steps can be calculated as ⌈𝑁−𝐿
𝐿∗𝑘⌉, where 𝑘indicates that a single GPU is
capableofcompressing 𝑘segmentsinabatch.WhentheGPUcapacityissufficient, 𝑘equals⌈𝑁
𝐿⌉,whichisthescenario
of ICAE that compresses all segments in a time but drops nothing, while when the GPU capacity is only sufficient to
set𝑘=1, it degenerates to the AutoCompressor scenario that compress segments step by step.
Jun Gao et al.: Preprint submitted to Elsevier Page 4 of 15

--- PAGE 6 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
3.1. Training
The key to SelfCP is the underlying target LLM plays the role of both the compressor and the generator while
keeping frozen during training. Except for the learnable linear layer, we introduce a learnable special embedding for
SelfCP,thememorytag [𝑀],initializedfromararelyusedembeddingofthetargetLLM.Tocatertofuturepractical
employment, We introduce the involved compressing strategies during training:
3.1.1. Former & Latter Compression
Previous compressed methods based on soft prompts place the soft prompt at the beginning like Prefix-Tuning
which ignores the basic fact that practical queries have no strict relative position relationship with corresponding
prompts. For example, we can formulate the input as Summarize the document below : [DOC] or[DOC] Summarize
the document above . Thereby, we introduce Former Compression and Later Compression instances into training.
Specifically, for prompt instances shorter than 2×𝐿tokens, namely, two times window limitation, we evenly split
them into two segments [𝑆𝑐,𝑆𝑢]and randomly compress the former or the latter.
Given the segment to be compressed 𝑆𝑐, the memory tag sequence = [𝑀]×𝑘, and the current query 𝑄, we
formulate the input of compressor 𝐼𝐶as𝐼𝐶=𝑄 ⊕ 𝑆𝑐⊕, where ⊕represents horizon concatenation.
Memory tags signal the built-in LLM to play the role of compressor at the input end. Then they serve as the
containertoabsorbdenseinformationfromtheprecededsegmentthroughtheforwardpropagationofthetransformer
stackwithinthecompressor.SelfCPobtainsthehiddenstatesofthelastTransformerlayerontopofattachedmemory
tags𝐻𝑚=(ℎ𝑚
1,ℎ𝑚
2,...,ℎ𝑚
𝑘)whiledisregarding others :
_,𝐻𝑚=𝐶𝑜𝑚𝑝𝑟𝑒𝑠𝑠𝑜𝑟(𝐼𝐶). (1)
Then, SelfCP projects 𝐻𝑚sourcing from the output space of the compressor into LLM-acceptable memory tokens
̃𝐻𝑚via the connector, where 𝑊𝑝is the weight of the connector:
̃𝐻𝑚=𝑊𝑝⋅𝐻𝑚. (2)
Assuming the prompt has the uncompressed segment 𝑆𝑢behind the compressed one 𝑆𝑐, the input of generator 𝐼𝐺is
formulated as 𝐼𝐺=𝑄 ⊕̃𝐻𝑚⊕ 𝑆𝑢. Given a golden response 𝑌= (𝑦1,𝑦2,...,𝑦|𝑌|)to the current query and prompt,
the connector within SelfCP is supervised-tuned based on the language modeling objective of the target LLM in the
teacher-forcing manner, where Θ∈{𝑊𝑝,[𝑀]}:
𝑙𝑜𝑠𝑠=maximizeΘ|𝑌|∏
𝑖=0(𝑦𝑖|𝐼𝐺⊕ 𝑦<𝑖). (3)
3.1.2. Concatenated Compression
Previous studies still struggle to process quite long prompts with efficiency or truncation problems, whose length
exceedstheallowedinputwindowoflanguagemodels.WefurtherintroduceConcatenatedCompressiontoupgradethe
previous“FormerCompressionandLatterCompression”intothetrainingofSelfCPtotacklethisproblem.Specifically,
both the compressed segments 𝑆𝑐and the uncompressed ones will exceed 𝐿tokens when prompts longer than 2×𝐿
tokens after even partition. In this scenario, given a prompt has 𝑁tokens in total, where 𝑁 >2×𝐿, SelfCP first
allocates uncompressed segments 𝑆𝑢with𝐿tokens, and then evenly splits the rest into ⌈𝑁
𝐿−1⌉non-overlapping
local segments. Due to the segments being non-overlapping, the compressor compresses each of them independently
asEqu.1andconvertsthehiddenstatesontopofmemorytagstolocalmemorytokensasEqu.2.Theglobalmemory
tokensareconfiguredbyconcatenatinglocalmemorytokenshorizontally,fedintothegeneratortooptimizeSelfCPas
Equ. 3.
3.2. Improvement for In-Context Learning
Specifically,inthescenarioofICL,weconsiderthepromptcontainingthedemonstrationsandthequerycontaining
relatedtaskinputsandtaskinstructions.In-contextdemonstrationsequencetypicallyincreasesthelengthoftheprompt,
wespeciallydevelopstrategiestooptimizeboththeefficiencyandeffectivenessofICLthroughcachingandretrieving.
SelfCPallocateseachdemonstrationwithasegmentandtruncatesthelattertoguaranteeindependenceamongeach
segment. Hence, SelfCP compresses each demonstration independently and caches its memory tokens to construct a
Jun Gao et al.: Preprint submitted to Elsevier Page 5 of 15

--- PAGE 7 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Figure 3: Kernel density estimation of training data. The horizontal axis is the number of tokens in each instance.
Table 1
The experiment details of test sets used in this paper. ICL represents performing ICL or not. LA stands for linguistic
acceptability.
Task Dataset In-Domain ICL # Test Instances
SUM XSUM ! % 11,334
QA CICERO ! % 10,898
QFS DUC % % 45
CVG CLCV % % 2,000
SUM XSUM ! ! 1,500
SUM ARXIV % ! 1,500
LA CoLA % ! 1,041
Memory Demonstrations Bank (MDB) for reusing. With the help of MDB, SelfCP can respond to queries by directly
withdrawing target memory tokens without repetitive compression, making SelfCP more efficient in ICL.
Randomly sampled demonstrations have unstable ICL performance gains. We further empower MDB to support
demonstration selection by treating the first memory token 𝑘𝑖of𝑖−th demonstration as the key used for retrieval.
SelfCP requires compressing the query 𝑄to obtain the first memory token 𝑞for directional demonstration selection.
Then, the demonstration scores of the 𝑖-th demonstration in MDB are calculated based on cosine similarity:
𝑠𝑐𝑜𝑟𝑒𝑖=𝑐𝑜𝑠𝑖𝑛𝑒(𝑞,𝑘𝑖). (4)
SelfCPselectsthetargetdemonstrationsaccordingtothedemonstrationscoresfromhightolow,beingmoreeffective
in ICL.
4. Experiment
4.1. Datasets
We mix up XSum, CICERO, and an instruction dataset as our training set, containing 42k instances, and the
instance length kernel estimation is illustrated in Figure 3. The details of in- and out-domain test sets are shown in
Table 1. Notably, we don’t perform in-domain evaluation in the instruction dataset as its responses are somewhat
open-ended and it solely has a training set. We use the entire test set of XSUM and CICERO as the in-domain set
to confirm details of prompt compression. In out-domain evaluation, we employ the entire DUC 2007 as the test set,
Jun Gao et al.: Preprint submitted to Elsevier Page 6 of 15

--- PAGE 8 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
and we collect a Chinese verdict generation (CVG) dataset called CLCV ( ChineseLegalCaseVerdict). CLCV has
2,000instancescollectedfromChinaProsecutorialNetwork3.EachinstanceofCLCVcontainstheindictmentandthe
corresponding judgment with an average of 540 words for indictment and 230 words for judgment. When evaluating
the ICL performance of SelfCP, we sampled 1,500 instances from the XSUM and ARXIV summarization datasets
respectively, with an average length of about 580 words. Additionally, we evaluate SelfCP on the entire linguistic
acceptability dataset CoLA through close-ended evaluation in ICL.
4.2. Evaluation Metrics
ROUGE Lin (2004) is a widely adopted metric in many generative tasks that evaluate how similar the generated
hypothesis is to the golden label. Therefore, ROUGE is used in our experiments to evaluate the quality responses
generatedconditionedoncompressedvirtualtokens.WereporttheF-1scoresofROUGE-1,ROUGE-2,andROUGE-
L (abbreviated R-1, R-2, R-L in the following), and we employed the files2rouge4library in practice. Additionally,
as CLCV is a Chinese dataset, we use the Chinese-ROUGE5library combined with jieba word-cutting library6to
evaluate the generated result. For CoLA, we report the accuracy.
4.3. Baselines
WecomparetheperformanceofSelfCPwiththenaiveLLMsfedactualpromptsbasedonVicuna-7bandBlueLM-
7b, respectively. In addition, we introduce AutoCompressor Chevalier et al. (2023) and ICAE Ge et al. (2023) which
converts the entire prompts into virtual tokens, and LLMlingua Jiang et al. (2023) which drops uninformative tokens
in the prompt.
AutoCompressor AutoCompressor7istherecentworkthatcompressespromptsintovirtualtokensrecurrentlywith
thefine-tunedLLMsZhangetal.(2022).WeemploytheirofficiallyreleasedweightbasedonLlama2-7bandcompare
its performance with SelfCP on out-domain datasets, setting the compression ratio the same as SelfCP.
ICAEICAE8compressesentirepromptswithgivenwindowlimitationbyaLoRA-adaptedLlama2.Weemploytheir
officially released Llama2-7b version and compared its performance with SelfCP in out-domain datasets, setting the
compression ratio the same as SelfCP.
LLMLingua LLMLingua is a recent coarse-to-fine prompt compression method based on dropping uninformative
words and achieves powerful performance. We employ LLMLingua from their official code9, and compare its
performance with SelfCP in all out-domain datasets, setting the compression ratio the same as SelfCP by limiting
thenumberofdroppeddiscretetokens.Notably,LLMLingua,intheirpaper,isdesignedtoemployasmallcompressor
(Llama or GPT-2), instruct-tuned to align with the target LLM (GPT-3.5-Turbo or Claude-v1.3). For a meaningful
comparison, we substitute their target LLMs with the underlying LLM in SelfCP.
4.4. Settings
Considering the max tokens in all involved datasets and computation efficiency, we set the max allowed input
window limitation 𝐿to 512. Additionally, we fix the learning rate to 8e-5 and use Adam as the optimizer, and the
effective batch size is 32 (8 GPUs data parallelism and 4 steps gradient accumulation). Additionally, we conduct all
experiments on 8*NVIDIA A5000 24G GPUs based on BFloat 16 data type.
We compress the latterpart in XSUM, DUC, and CICERO, since the former part in these tasks is empirically
important, while we compress the formerpart in CLCV because the involved person is introduced at the front
of the indictment which is relatively unimportant. Additionally, we divide ICL into two situations: (1) In the low-
resourcesituation,wefixthedemonstrationsforeachquery.(2)Inthehigh-resourcesituation,SelfCPretrievessimilar
demonstrationsfromMDBbymeasuringthecosinesimilarity.Weconsiderthetrainingsetasthedemonstrationpool
and construct the MDB for each dataset.
3https://www.12309.gov.cn/12309/zjxflws/index.shtml.
4https://github.com/pltrdy/files2rouge.
5https://pypi.org/project/rouge-chinese.
6https://pypi.org/project/jieba.
7https://github.com/princeton-nlp/AutoCompressors.
8https://github.com/getao/icae.
9https://aka.ms/LLMLingua.
Jun Gao et al.: Preprint submitted to Elsevier Page 7 of 15

--- PAGE 9 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Table 2
The in-domain results of XSUM and CICERO. 1k represents extending window limitation to 1k, and the others is 512.
Backbone MethodXSUM CICERO
R-1 R-2 R-L R-1 R-2 R-L
Vicuna-7bVicuna 19.9 5.0 13.5 17.3 3.3 14.3
+LoRA 25.4 7.5 17.3 28.1 10.5 25.6
Vicuna-1k 27.3 8.7 19.7 30.5 11.3 27.4
+LoRA 31.2 11.0 23.1 34.1 13.5 30.2
SelfCP 30.5 10.8 22.7 33.3 12.9 29.2
BlueLM-7bBlueLM 15.0 3.6 10.4 17.6 3.1 15.0
+LoRA 23.1 7.6 17.4 21.9 7.8 19.8
BlueLM-1k 28.1 9.9 22.8 25.1 9.2 23.1
+LoRA 30.8 10.5 24.6 31.2 10.8 27.4
SelfCP 30.9 10.6 24.4 29.9 10.6 26.9
Table 3
The out-domain results of DUC and CICV.
Backbone MethodDUC CLCV
R-1 R-2 R-L R-1 R-2 R-L
Llama2-7bAutoCompressor 31.7 7.3 16.4 20.8 6.1 17.0
ICAE 30.6 6.9 15.7 22.3 6.4 19.2
Vicuna-7bVicuna 24.0 4.6 13.1 20.6 6.0 16.5
LLMLingua 32.5 7.6 16.8 21.0 6.3 17.2
SelfCP 33.6 7.8 17.3 21.4 6.3 18.0
BlueLM-7bBlueLM 16.8 3.6 10.0 33.9 17.5 24.8
LLMLingua 21.2 4.1 11.4 35.1 18.6 25.9
SelfCP 25.5 4.8 13.4 36.6 20.8 27.2
4.5. Results
4.5.1. In-Domain Evaluation
We conduct the in-domain evaluation on XSUM and CICERO in Table 2. SelfCP significantly outperforms the
baselineVicunaandBlueLMwith512and1024windowlimitationsaftersupervisedtuning.Tosimulategainsbrought
bythetrainedconnector,wealsoLoRA-tuneVicunaandBlueLMonourtrainingsetwith17Mtrainableparametersby
settingtheLoRArankto32(referringto +𝐿𝑜𝑅𝐴inTab.2).Inthiscase,SelfCPoutperformsLoRA-adaptedbackbones
with 512 allowed windows even the model being tuned, while SelfCP is comparable with them with 1,024 allowed
windows.TheseresultshighlightthatextremetruncationmakesLLMsconfusedtorespondandthecompressedvirtual
tokens effectively filter noise information and recover the informative parts to a large extent.
4.5.2. Out-Domain Evaluation
We test the out-domain performance of SelfCP on the DUC and CLCV to evaluate its generalized capability and
cross-lingua ability, as demonstrated in Table 3.
SelfCP employs concatenation compression to compress the query-related documents into memory tokens.
Compared with the truncation that occurs in naive Vicuna and BlueLM, SelfCP effectively broadens the in-context
window,achievingnearly +10ROUGE-1gains.WhileinCLCV, BlueLM-basedSelfCPachievesbetterperformance
comparedwithVicuna-basedonessinceBlueLMisspecific-tunedandgoodatChinese,provingthatSelfCPimplicitly
leverages the strengths of diverse backbones during learning prompt compression. Additionally, the compression on
both the former of CLCV and the latter part of DUC indicates that SelfCP makes no limitation on the position of
memorytokens.AsforAutoCompressor,the7bversionunderperformsVicuna-basedSelfCPinEnglishtasks(DUC)
andunderperformesBlueLM-basedSelfCPinChinesetasks(CLCV).Meanwhile,itisn’tsurprisingtofindthatSelfCP
outperforms LLMLingua in out-domain evaluation since their algorithm leverages the understanding ability of the
targetLLMwhileChatGPT-3.5-turboismuchstrongerthanLLMwith7bparameters.Therefore,Vicuna-orBlueLM-
7b may sometimes be confused about the meaningless discreet tokens.
Jun Gao et al.: Preprint submitted to Elsevier Page 8 of 15

--- PAGE 10 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Table 4
The in-context learning results in XSUM and ARXIV of SelfCP based on Vicuna. †represents the demonstrations retrieved
from MDB, and others uses fixed demonstrations.
Method #-shotsXSUM ARXIV CoLA
R-1 R-2 R-L R-1 R-2 R-L Acc.
Vicuna0-shot 19.9 5.0 13.4 34.3 9.1 27.4 56.2
1-shot 21.2 5.8 14.5 34.4 9.1 27.5 57.4
AutoCompressor1-shot 20.3 6.3 13.7 26.4 8.2 25.8 40.9
2-shot 21.4 6.4 14.1 26.2 7.9 25.4 56.3
5-shot 21.7 6.7 14.3 33.7 9.1 27.9 58.8
ICAE1-shot 21.9 7.8 20.3 24.6 7.1 22.9 30.9
2-shot 23.2 8.4 20.8 25.5 7.6 24.3 30.9
5-shot 24.9 8.8 21.6 26.9 7.7 25.8 30.9
LLMLingua1-shot 19.7 5.2 14.4 33.1 8.7 27.1 55.0
2-shot 20.0 5.1 14.1 32.0 8.1 25.9 55.7
5-shot 18.6 4.9 14.3 29.7 7.4 24.6 56.9
SelfCP1-shot 25.5 9.1 20.0 34.7 10.2 27.9 58.0
2-shot 26.8 9.8 20.9 35.2 10.4 28.2 61.0
5-shot 27.6 10.1 21.4 35.4 10.2 28.1 61.8
SelfCP†1-shot 26.2 9.4 20.7 34.7 10.3 27.8 58.7
2-shot 28.9 10.5 21.7 34.3 10.4 28.3 61.2
5-shot 30.0 11.2 22.3 35.3 10.8 27.7 61.5
Table 5
The in-context learning results of SelfCP based on BlueLM.
Method #-shotsXSUM ARXIV CoLA
R-1 R-2 R-L R-1 R-2 R-L Acc.
BlueLM0-shot 15.0 3.6 10.4 30.9 7.7 24.7 71.6
1-shot 19.1 4.8 12.1 23.0 3.6 19.0 69.6
LLMLingua1-shot 18.0 2.7 13.2 28.0 6.3 23.2 58.1
2-shot 18.6 3.4 13.4 26.5 5.5 22.2 60.3
5-shot 18.3 3.3 13.3 26.8 5.6 22.4 62.5
SelfCP1-shot 24.0 6.9 18.0 31.4 7.7 25.2 69.6
2-shot 25.0 7.3 18.8 30.8 7.3 24.8 70.1
5-shot 25.3 7.4 19.1 31.9 7.8 26.0 71.8
SelfCP†1-shot 24.7 7.2 18.5 31.0 7.5 24.9 70.1
2-shot 25.1 7.4 19.0 31.2 7.7 25.1 70.3
5-shot 26.3 7.6 20.0 31.5 7.9 25.3 71.1
4.5.3. In-Context Learning Evaluation
WeevaluatetheICLabilityofSelfCPinTable4and5.Memorytokenstakeequalorevenbettereffectsthanactual
demonstrations, which verifies the ability of SelfCP to capture the core semantics of demonstrations during compres-
sion.RegardingARXIV,theoriginalICLisnothelpfulenoughandcausessignificantdegradationinBlueLM,dueto
therelativelylongdocumentsinARXIV,whichleavelittleroomforLLMtoreaddemonstrations.AutoCompressorre-
cursivelycompressesconcatenateddemonstrationsintosoftpromptsstep-by-step.However,AutoCompressorstillun-
derperformsSelfCP.Weattributethistotheinformationlostduetorecursivecompressioninaddressinglongprompts.
Moreover,demonstrationsfilteredbyLLMlinguagenerallyunderperform0-shotinbothXSUMandARXIVwithtwo
backbonesprovingthatthediscretetokensfailtoguideLLMinfew-shotsettings.WeevaluateSelfCPonCoLAthrough
closed-end evaluation, which measures the perplexity (PPL)10of candidate labels (acceptable/unacceptable) for the
givensentenceinthefollowingtemplate:[Sentence] Grammatically, the above sentence is {acceptable/unacceptable} .
10https://huggingface.co/docs/transformers/perplexity.
Jun Gao et al.: Preprint submitted to Elsevier Page 9 of 15

--- PAGE 11 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Table 6
Case studies on the DUC dataset. The italic text in Input represents the document to be condensed, while the bold text
indicates the salient information derived from the compressed prompts.
DUC Case
Input According to the topic: Write an account of the sequence of events involving the Kennedy family...
Summarize the following document: John F. Kennedy Jr. was a relatively new pilot... the plane could
glide if the single engine failed </P><P> Kennedy had registered the plane, a Piper Saratoga ...
Reference At approximately 9:40pm Friday, ..., when a Piper Saratoga, piloted by John , ... An at sea burial of all
three was conducted ...
Vicuna The sequence of events involving the Kennedy family following the plane crash that killed John F.
Kennedy Jr., his wife, Carolyn Bessette Kennedy...
LLMLingua The sequence of events involving the Kennedy family during and following the plane crash that killed
John F. Kennedy Jr., his wife, Carolyn Bessette Kennedy...
AutoCompressor </P><P> The plane was a twin-engine, single-seat Cessna Citation500, which costs about $1.5 million,
according to the manufacturer.</P><P>...
SelfCP On July 17, 1999, John F. Kennedy Jr., his wife, ... The plane was a Piper Saratoga, a twin-engine
aircraft that was owned by Kennedy ...
Figure 4: The visualization result between condensed prompt and their virtual tokens. To clarify, we sample 4 virtual tokens
and select some representative actual tokens.
Labels with PPL closer to 1 will be judged as the prediction. Notably, ICAE always returns “acceptable”, resulting in
a consent 30.9% accuracy.
Finally,weevaluateSelfCPonselectingsuitabledemonstrationsfromMDB,whichcontains5krandomlysampled
demonstrations. SelfCP further achieves performance gains by retrieving more powerful in-context demonstrations
fromMDB.TheresultindicatesthatthecompressedvirtualtokengeneratedbySelfCPisgoodatmeasuringsimilarity
among documents and then finding more favorable in-context demonstrations for ICL.
4.6. Case Study
We demonstrate a case study on DUC to provide an intuitive comparison among SelfCP based on Vicuna, direct
transaction, AutoCompressor, and LLMLingua in Table 6. This case describes that a plane crashed into the Atlantic
Ocean, while the details of the plane are over-length and condensed. Vicuna can’t generate satisfying summaries as
somesalientpartsofthedocumentsaretruncated.Bycontrast,SelfCPsuccessfullyrestorestheimportantinformation
from the compact virtual tokens, such as the aircraft type “Piper Saratoga”.
Furthermore, we depict the other scenario by gauging the similarity between actual and their virtual tokens,
illustrated in Figure 4. Warmer colors signify a greater degree of similarity. The origin document describes the
Stuckbarks and its cooperation with Magic Johnson, with the information about “Magic Johnson” compressed.
However, SelfCP recovers this information in the generated response. It is plausible that the virtual tokens effectively
absorb pertinent information, resulting in a comparatively higher similarity to these tokens.
Jun Gao et al.: Preprint submitted to Elsevier Page 10 of 15

--- PAGE 12 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Figure 5: The sensitivity analysis of compression ratio.
Table 7
The efficiency of SelfCP with the backbone of Vicuna. 1k stands for the extended 1k window limitation, while others have
a 512 window limitation (ignoring the length of memory tokens).
Method GPUHours TFLOPs TMACs
Vicuna 1.5 86,20 4,309
Vicuna-1k 1.9 31,664 15,832
SelfCP 1.6 22,437 11,218
5. Analysis
5.1. Compression Ratio
The compression ratio is randomly sampled from 2 to 16 during the training of SelfCP. We mix up 2,000
instancesfromthein-domainvalidationset,1,000forXSUM,and1,000forCICEROtoselectthecompressionratio.
Specifically,SelfCPconductsconditionalcompressionthatcompressesthelattercut-offpartwhilekeepingtheformer
uncompressed. Therefore, we can measure the information quality of the same content with different compression
ratios by ROUGE-1 since it is more sensitive to token-level differences.
For both BlueLM and Vicuna, the performance is relative smoothing when the compression ratio changes from
4×to12×. However, when it comes to 16×, a significant drop occurs in Vicuna, with relatively large performance
degradation occurring in BlueLM as well compared to the previous ratios. Therefore, we set the compression ratio to
12 by default and apply this ratio to all experiments. Additionally, in our experiment setting, the window limitation is
512, and the512×compression ratio is equal to compressing anything to a single virtual token.
5.2. Efficiency Analysis
In SelfCP, we incorporate an additional 17M trainable parameters into the 7b backbone, accounting for an
approximate increase of 0.24%.
To quantify the efficiency difference brought by the projection layer, we mainly focus on SelfCP built on Vicuna
because BlueLM has a comparable parameter size and model architecture. We first report the GPU Hours, TFLOPs,
and TMACs11of SelfCP and Vicuna on a single NVIDIA A5000 GPU. Specifically, we use 1000 random but legal
number sequences of 1024 length as input ids, avoiding special tokens, and ask the model to always generate 128
tokens.SelfCPcompressesthelater512tokensinto43memorytokens(12 ×compression),andtheformer512tokens
11https://github.com/MrYxJ/calculate-flops.pytorch.
Jun Gao et al.: Preprint submitted to Elsevier Page 11 of 15

--- PAGE 13 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Figure 6: Efficiency comparison on throughputs (left) and memory costs (right) in the CoLA dataset. The upper limit of
memory is 24G, and the excess part is marked with *, corresponding to the breaking point in the line chart.
along with memory tokens are fed into Vicuna to perform generation (555 tokens in total). To maintain a consistent
inputlengthforgeneration,theformer555tokensaredirectlyfedintoVicuna.InTable7,SelfCPachievesnearlythree
times the TFLOPs and TMACs than the naive backbone to exchange double readable windows due to the additional
compression and projection processes. However, the forward propagation for compressing and the projection support
parallel computing, and it only brings minimal GPU Hours increments in practice. Notably, when allowing Vicuna to
readtheentire1024inputidswithoutcompression,theextra512tokenssurgebothcomputationandGPUHoursand
overwhelm SelfCP during generation (referring to Vicuna-1k in Tab. 7).
Except for the floats computation, in the ICL scenario, we illustrate the practical variations of throughput and
memory costs with the number of demonstrations increasing among SelfCP and other compression methods in
Figure 6. AutoCompressor exhibits poor inference throughputs due to recursively compression and ICAE performs
sub-weakly in throughputs because of switching LoRA weights between the compressor and the generator. Given
a 24 GB memory allocation, Vicuna-7b only performs up to the 4-shot setting, while ICAE and AutoCompressor
readupto32demonstrations,butSelfCPstillworksinthe64-shotsetting.SelfCPsupportscachingmemorytokensto
configuretheMemoryDemonstrationBankinadvanceforreusing.SelfCPcanrespondtoquerieswithoutcompressing
demonstration repeatedly in this case (referring to SelfCP + Caching), making inference more efficient.
6. Conclusion and Further Work
This paper proposes SelfCP, using a frozen LLM to compress long prompts into memory tokens. In SelfCP, the
LLMplaystheroleofthecompressortocompressthepromptandthegeneratortorespondtoqueriesconditionedonthe
memory tokens and the rest uncompressed prompt. SelfCP only contains 17M trainable parameters due to the frozen
backboneandallowsforadaptationacrossvariousbackbones.Weconductextensivein-andout-domainexperiments,
coveringsituationsofICLandover-lengthpromptsandweanalyzetheefficiencyofSelfCP.Theresultsshowthatthe
generated memory tokens can effectively substitute the 12 ×longer actual over-limit prompt.
We believe there is much improvement room for SelfCP. On the one hand, we will scale the backbone of SelfCP
to larger and higher-performance LLMs in various domains. On the other hand, our intention involves incorporating
compression as one of the fundamental pre-training goals of LLMs, expecting to enhance their compression ability
further.
CRediT authorship contribution statement
JunGao: Conceivedanddesignedthework,Conductedexperiments,Performedtheanalysis,andWrotethepaper.
Ziqiang Cao: Conceived and designed the analysis, Performed the analysis, and Supervised the work. Wenjie Li:
Conceived and designed the work, and Reviewed the work.
Jun Gao et al.: Preprint submitted to Elsevier Page 12 of 15

--- PAGE 14 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
References
Beltagy, I., Peters, M.E., Cohan, A., 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 .
Bertsch, A., Alon, U., Neubig, G., Gormley, M.R., 2023. Unlimiformer: Long-range transformers with unlimited length input. arXiv preprint
arXiv:2305.01625 .
Bolya,D.,Fu,C.Y.,Dai,X.,Zhang,P.,Feichtenhofer,C.,Hoffman,J.,2022. Tokenmerging:Yourvitbutfaster. arXivpreprintarXiv:2210.09461.
Bulatov,A.,Kuratov,Y.,Burtsev,M.,2022. Recurrentmemorytransformer. AdvancesinNeuralInformationProcessingSystems35,11079–11091.
Bulatov,A.,Kuratov,Y.,Kapushev,Y.,Burtsev,M.S.,2023.Scalingtransformerto1mtokensandbeyondwithrmt.arXivpreprintarXiv:2304.11062
.
Chevalier, A., Wettig, A., Ajith, A., Chen, D., 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788 .
Child, R., Gray, S., Radford, A., Sutskever, I., 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 .
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al., 2020.
Rethinking attention with performers. arXiv preprint arXiv:2009.14794 .
Cohan, A., Dernoncourt, F., Kim, D.S., Bui, T., Kim, S., Chang, W., Goharian, N., 2018. A discourse-aware attention model for abstractive
summarization of long documents. arXiv preprint arXiv:1804.05685 .
Copeck, T., Inkpen, D., Kazantseva, A., Kennedy, A., Kipp, D., Nastase, V., Szpakowicz, S., 2006. Leveraging duc, in: proceedings of DUC.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R., 2019. Transformer-xl: Attentive language models beyond a fixed-length
context. arXiv preprint arXiv:1901.02860 .
Ding,J.,Ma,S.,Dong,L.,Zhang,X.,Huang,S.,Wang,W.,Wei,F.,2023. Longnet:Scalingtransformersto1,000,000,000tokens. arXivpreprint
arXiv:2307.02486 .
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.,
2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 .
Ge, T., Hu, J., Wang, X., Chen, S.Q., Wei, F., 2023. In-context autoencoder for context compression in a large language model. arXiv preprint
arXiv:2307.06945 .
Ghosal,D.,Shen,S.,Majumder,N.,Mihalcea,R.,Poria,S.,2022. Cicero:Adatasetforcontextualizedcommonsenseinferenceindialogues. arXiv
preprint arXiv:2203.13926 .
Jiang, H., Wu, Q., Lin, C.Y., Yang, Y., Qiu, L., 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv
preprint arXiv:2310.05736 .
Katharopoulos, A., Vyas, A., Pappas, N., Fleuret, F., 2020. Transformers are rnns: Fast autoregressive transformers with linear attention, in:
International conference on machine learning, PMLR. pp. 5156–5165.
Kim, S., Shen, S., Thorsley, D., Gholami, A., Kwon, W., Hassoun, J., Keutzer, K., 2022. Learned token pruning for transformers, in: Proceedings
of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 784–794.
Lee-Thorp, J., Ainslie, J., Eckstein, I., Ontanon, S., 2021. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824 .
Li,J.,Li,D.,Savarese,S.,Hoi,S.,2023a.Blip-2:Bootstrappinglanguage-imagepre-trainingwithfrozenimageencodersandlargelanguagemodels.
arXiv preprint arXiv:2301.12597 .
Li, X.L., Liang, P., 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 .
Li, Y., 2023. Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering. arXiv
preprint arXiv:2304.12102 .
Li,Y.,Yang,N.,Wang,L.,Wei,F.,Li,W.,2023b.Generativeretrievalforconversationalquestionanswering.InformationProcessing&Management
60, 103475.
Lin, C.Y., 2004. Rouge: A package for automatic evaluation of summaries, in: Text summarization branches out, pp. 74–81.
Liu,N.F.,Lin,K.,Hewitt,J.,Paranjape,A.,Bevilacqua,M.,Petroni,F.,Liang,P.,2024. Lostinthemiddle:Howlanguagemodelsuselongcontexts.
Transactions of the Association for Computational Linguistics 12, 157–173.
Liu,X.,Ji,K.,Fu,Y.,Tam,W.,Du,Z.,Yang,Z.,Tang,J.,2022. P-tuning:Prompttuningcanbecomparabletofine-tuningacrossscalesandtasks,
in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61–68.
Mu, J., Li, X.L., Goodman, N., 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467 .
Narayan, S., Cohen, S.B., Lapata, M., 2018. Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme
summarization. ArXiv abs/1808.08745.
Sun, S., Yuan, R., Li, W., Cao, Z., Li, S., 2024. Dialogue acts enhanced extract–abstract framework for meeting summarization. Information
Processing & Management 61, 103635.
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B., 2023. Stanford alpaca: An instruction-following
llama model.
Team, B., 2023. Bluelm: An open multilingual 7b language model. https://github.com/vivo-ai-lab/BlueLM .
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al., 2023. Llama:
Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 .
Wang, J., Liang, Y., Meng, F., Shi, H., Li, Z., Xu, J., Qu, J., Zhou, J., 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint
arXiv:2303.04048 .
Wang, L., Zhao, M., Ji, H., Jiang, Z., Li, R., Hu, Z., Lu, X., 2024. Dialogue summarization enhanced response generation for multi-domain
task-oriented dialogue systems. Information Processing & Management 61, 103668.
Wang,Y.,Mishra,S.,Alipoormolabashi,P.,Kordi,Y.,Mirzaei,A.,Arunkumar,A.,Ashok,A.,Dhanasekaran,A.S.,Naik,A.,Stap,D.,etal.,2022.
Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705 .
Wang,Z.,Xie,Q.,Ding,Z.,Feng,Y.,Xia,R.,2023b. Ischatgptagoodsentimentanalyzer?apreliminarystudy. arXivpreprintarXiv:2304.04339.
Wei,X.,Cui,X.,Cheng,N.,Wang,X.,Zhang,X.,Huang,S.,Xie,P.,Xu,J.,Chen,Y.,Zhang,M.,etal.,2023. Zero-shotinformationextractionvia
chatting with chatgpt. arXiv preprint arXiv:2302.10205 .
Jun Gao et al.: Preprint submitted to Elsevier Page 13 of 15

--- PAGE 15 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Wingate,D.,Shoeybi,M.,Sorensen,T.,2022.Promptcompressionandcontrastiveconditioningforcontrollabilityandtoxicityreductioninlanguage
models. arXiv preprint arXiv:2210.03162 .
Wu, Y., Rabe, M.N., Hutchins, D., Szegedy, C., 2022. Memorizing transformers. arXiv preprint arXiv:2203.08913 .
Yang,X.,Li,Y.,Zhang,X.,Chen,H.,Cheng,W.,2023. Exploringthelimitsofchatgptforqueryoraspect-basedtextsummarization. arXivpreprint
arXiv:2302.08081 .
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al., 2022. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068 .
Zheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al., 2023. Judging llm-as-a-judge with
mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 .
Zheng, L., Wang, C., Kong, L., 2022. Linear complexity randomized self-attention mechanism, in: International conference on machine learning,
PMLR. pp. 27011–27041.
Zhong, Z., Lei, T., Chen, D., 2022. Training language models with memory augmentation. arXiv preprint arXiv:2205.12674 .
Jun Gao et al.: Preprint submitted to Elsevier Page 14 of 15

--- PAGE 16 ---
SelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself
Table 8
Used instructions in involved datasets.
XSUM Summarize the following document: [DOC]
CICERO [DOC] According to the above text, answer the query: [Query]
ARXIV Summarize the following document: [DOC]
DUC According to the topic: [Query] Summarize the following documents: [DOC1][DOC2]...
CLCV Generating the judgment according to the following indictment: [DOC]
CoLA [Sentence] Grammatically, the above sentence is {acceptable/unacceptable}
A. Dataset
We briefly introduce each dataset involved in this paper as follows:
XSUMXSUM Narayan et al. (2018) is a popular abstractive summarization dataset. The documents in the XSUM
datasetcomefromarticlesinvariousfields,includingnewsstories,encyclopediaentries,forumposts,andmore.Each
instance contains a document and its summary.
CICERO CICEROGhosaletal.(2022)isagenerativequestion-answeringdataset.Eachinstancecontainsapersonal
dialogue,atargetspeech,aquery,andahuman-writtenanswer.Languagemodelsneedtoanswerthequeryconditioned
on the target speech.
SUPER-NI SUPER-NIWang,Mishra,Alipoormolabashi,Kordi,Mirzaei,Arunkumar,Ashok,Dhanasekaran,Naik,
Stapetal.(2022) isacomprehensiveinstructiondatasetincluding thousandsofcommonnaturallanguageprocessing
tasks.Eachinstancecontainsaninstructionandaresponse.WeintroducethisdatasettomakeSelfCPabletorespond
to diverse queries.
DUCDUC 2007 Copeck et al. (2006) is a popular Query Focused Summarization (QFS) dataset. Each instance
contains a topic and a few documents.
ARXIVARXIV Cohan, Dernoncourt, Kim, Bui, Kim, Chang and Goharian (2018) is a summarization dataset
collectedfromarXiv.Eachinstanceofthisdatasetcontainsthebodyofapaperanditsabstract.Therefore,ARXIVis
relatively longer than normal datasets, in which documents contain 5.9k words on average.
CoLAThe Corpus of Linguistic Acceptability (CoLA) is a popular linguistic acceptability dataset consisting of
10657sentencesfrom23linguisticspublications,expertlyannotatedforacceptability(grammaticality)bytheiroriginal
authors.
Jun Gao et al.: Preprint submitted to Elsevier Page 15 of 15
