Tài liệu tham khảo

Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, và Sumit Sanghai. 2023. CoLT5: Các Transformer tầm xa nhanh hơn với Tính toán có điều kiện. Trong Kỷ yếu Hội nghị về Phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên (EMNLP).

Anthropic. 2023. Claude 2.

Iz Beltagy, Matthew E. Peters, và Arman Cohan. 2020. Longformer: Transformer tài liệu dài.

Yoshua Bengio, Nicholas Léonard, và Aaron Courville. 2013. Ước lượng hoặc Lan truyền Gradient qua các Nơ-ron ngẫu nhiên cho Tính toán có điều kiện.

Amanda Bertsch, Uri Alon, Graham Neubig, và Matthew R. Gormley. 2023. Unlimiformer: Các Transformer tầm xa với Đầu vào độ dài không giới hạn. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Aydar Bulatov, Yuri Kuratov, và Mikhail S. Burtsev. 2022. Transformer bộ nhớ đệ quy. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Alexis Chevalier, Alexander Wettig, Anirudh Ajith, và Danqi Chen. 2023. Điều chỉnh các mô hình ngôn ngữ để nén ngữ cảnh. Trong Kỷ yếu Hội nghị về Phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên (EMNLP).

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, và Adrian Weller. 2021. Suy nghĩ lại về chú ý với Performers. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, và Ruslan Salakhutdinov. 2019. Transformer-XL: Các mô hình ngôn ngữ chú ý vượt ra ngoài ngữ cảnh độ dài cố định. Trong Kỷ yếu Cuộc họp thường niên của Hiệp hội Ngôn ngữ học tính toán (ACL).

Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, và Furu Wei. 2023. LongNet: Mở rộng Transformer lên 1,000,000,000 Token.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor Leahy. 2020. The Pile: Tập dữ liệu 800GB văn bản đa dạng cho mô hình hóa ngôn ngữ.

Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, và Furu Wei. 2024. Bộ tự mã hóa trong ngữ cảnh cho nén ngữ cảnh trong mô hình ngôn ngữ lớn. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Ming-Wei Chang. 2020. REALM: Tiền huấn luyện mô hình ngôn ngữ tăng cường truy xuất. Trong Kỷ yếu Hội nghị quốc tế về Học máy (ICML).

Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. LoRA: Điều chỉnh rank thấp của các mô hình ngôn ngữ lớn. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Gautier Izacard và Edouard Grave. 2021. Tận dụng truy xuất đoạn văn với các mô hình sinh cho trả lời câu hỏi miền mở. Trong Kỷ yếu Hội nghị thường niên của Chi nhánh châu Âu của Hiệp hội Ngôn ngữ học tính toán (EACL).

Eric Jang, Shixiang Gu, và Ben Poole. 2017. Tái tham số hóa phân loại với Gumbel-Softmax. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, và Lili Qiu. 2023. LLMLingua: Nén prompt để tăng tốc suy luận của các mô hình ngôn ngữ lớn. Trong Kỷ yếu Hội nghị về Phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên (EMNLP).

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. 2020. Transformer là RNN: Transformer tự hồi quy nhanh với chú ý tuyến tính. Trong Kỷ yếu Hội nghị quốc tế về Học máy (ICML).

Ruslan Khalitov, Tong Yu, Lei Cheng, và Zhirong Yang. 2023. ChordMixer: Mô hình chú ý nơ-ron có thể mở rộng cho các chuỗi có độ dài khác nhau. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, và Mike Lewis. 2020. Khái quát hóa thông qua ghi nhớ: Các mô hình ngôn ngữ láng giềng gần nhất. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Diederik P. Kingma và Jimmy Lei Ba. 2015. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, và Douwe Kiela. 2020. Sinh tăng cường truy xuất cho các tác vụ NLP đòi hỏi kiến thức. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Quentin Lhoest, Albert Villanova Del Moral, Yacine Jernite, Abhishek Thakur, Patrick Von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, và Thomas Wolf. 2021. Datasets: Thư viện cộng đồng cho xử lý ngôn ngữ tự nhiên. Trong Kỷ yếu Hội nghị về Phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên (EMNLP).

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, và Hao Zhang. 2023. Độ dài ngữ cảnh của LLM nguồn mở có thể thực sự hứa hẹn bao lâu? Trong Kỷ yếu Workshop về Điều chỉnh hướng dẫn và Tuân theo hướng dẫn.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, và Percy Liang. 2024. Lạc giữa: Cách các mô hình ngôn ngữ sử dụng ngữ cảnh dài. Transactions of the Association for Computational Linguistics (TACL).

Ilya Loshchilov và Frank Hutter. 2017. SGDR: Gradient descent ngẫu nhiên với khởi động lại ấm. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2017. Các mô hình hỗn hợp sentinel con trỏ. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Jesse Mu, Xiang Lisa Li, và Noah Goodman. 2023. Học cách nén prompt với các token gist. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. BLEU: Một phương pháp cho đánh giá tự động dịch máy. Trong Kỷ yếu Cuộc họp thường niên của Hiệp hội Ngôn ngữ học tính toán (ACL).

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. 2019. PyTorch: Thư viện học sâu hiệu suất cao, phong cách mệnh lệnh. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, và Noah A. Smith. 2022. ABC: Chú ý với kiểm soát bộ nhớ có giới hạn. Trong Kỷ yếu Cuộc họp thường niên của Hiệp hội Ngôn ngữ học tính toán (ACL).

Ofir Press, Noah A. Smith, và Mike Lewis. 2022. Huấn luyện ngắn, kiểm tra dài: Chú ý với thiên lệch tuyến tính cho phép ngoại suy độ dài đầu vào. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Guanghui Qin, Yukun Feng, và Benjamin Van Durme. 2023. Hiệu quả tác vụ NLP của các Transformer tầm xa. Trong Kỷ yếu Hội nghị thường niên của Chi nhánh châu Âu của Hiệp hội Ngôn ngữ học tính toán (EACL).

Guanghui Qin và Benjamin Van Durme. 2023. Nugget: Embedding tập hợp nơ-ron của văn bản. Trong Kỷ yếu Hội nghị quốc tế về Học máy (ICML).

Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, và Timothy P. Lillicrap. 2020. Transformer nén cho mô hình hóa chuỗi tầm xa. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. SQuAD: 100,000+ câu hỏi cho hiểu văn bản máy. Trong Kỷ yếu Hội nghị về Phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên (EMNLP).

Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. 2020. DeepSpeed: Tối ưu hóa hệ thống cho phép huấn luyện các mô hình học sâu với hơn 100 tỷ tham số. Trong Kỷ yếu Hội nghị quốc tế về Khám phá kiến thức và Khai thác dữ liệu (KDD).

Michael E. Sander, Joan Puigcerver, Josip Djolonga, Gabriel Peyre, và Mathieu Blondel. 2023. Top-k nhanh, khả vi và thưa: Góc nhìn phân tích lồi. Trong Kỷ yếu Hội nghị quốc tế về Học máy (ICML).

Abigail See, Peter J. Liu, và Christopher D. Manning. 2017. Đến điểm: Tóm tắt với mạng sinh con trỏ. Trong Kỷ yếu Cuộc họp thường niên của Hiệp hội Ngôn ngữ học tính toán (ACL).

Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, và Sayak Paul. 2022. PEFT: Các phương pháp tinh chỉnh hiệu quả tham số tiên tiến.

Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, và Yunfeng Liu. 2024. RoFormer: Transformer tăng cường với Embedding vị trí quay. Neurocomputing, trang 127063.

Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. 2022. Transformer hiệu quả: Một khảo sát. ACM Computing Surveys, trang 1–28.

Together Computer. 2023. RedPajama: Tập dữ liệu mở để huấn luyện các mô hình ngôn ngữ lớn.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023a. LLaMA: Các mô hình ngôn ngữ nền tảng mở và hiệu quả.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023b. Llama 2: Các mô hình trò chuyện nền tảng mở và tinh chỉnh.

Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, và Piotr Miłoś. 2023. Transformer tập trung: Huấn luyện tương phản để mở rộng ngữ cảnh. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Chú ý là tất cả những gì bạn cần. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

William A. Falcon và Nhóm PyTorch Lightning. 2019. Pytorch Lightning.

David Wingate, Mohammad Shoeybi, và Taylor Sorensen. 2022. Nén prompt và điều kiện tương phản cho khả năng kiểm soát và giảm độc tính trong các mô hình ngôn ngữ. Trong Kỷ yếu Hội nghị về Phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên (EMNLP).

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick Von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander Rush. 2020. Transformers: Xử lý ngôn ngữ tự nhiên tiên tiến. Trong Kỷ yếu Hội nghị về Phương pháp thực nghiệm trong Xử lý ngôn ngữ tự nhiên (EMNLP).

Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, và Tomas Pfister. 2020. Toán tử Top-k khả vi với vận tải tối ưu. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, và Hao Ma. 2023. Mở rộng ngữ cảnh dài hiệu quả của các mô hình nền tảng.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, và Quoc V. Le. 2019. XLNet: Tiền huấn luyện tự hồi quy tổng quát cho hiểu ngôn ngữ. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, và Amr Ahmed. 2020. Big Bird: Transformer cho các chuỗi dài hơn. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, và Jie Tang. 2023a. GLM-130B: Mô hình song ngữ tiền huấn luyện mở. Trong Kỷ yếu Hội nghị quốc tế về Biểu diễn học tập (ICLR).

Zhanpeng Zeng, Cole Hawkins, Mingyi Hong, Aston Zhang, Nikolaos Pappas, Vikas Singh, và Shuai Zheng. 2023b. VCC: Mở rộng Transformer lên 128K token hoặc nhiều hơn bằng cách ưu tiên các token quan trọng. Trong Kỷ yếu Hội nghị về Hệ thống xử lý thông tin nơ-ron (NeurIPS).

Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, và Nan Duan. 2022. Học biểu diễn tài liệu đa góc nhìn cho truy xuất dày đặc miền mở. Trong Kỷ yếu Cuộc họp thường niên của Hiệp hội Ngôn ngữ học tính toán (ACL).

Shen Zheng, Jie Huang, và Kevin Chen-Chuan Chang. 2023. Tại sao ChatGPT lại thiếu sót trong việc cung cấp câu trả lời chính xác? Trong Kỷ yếu ICBINB Workshop.

--- TRANG 13 ---
A Lựa chọn nuggets tối ưu

Mô-đun lựa chọn nuggets, tức là Scorer, được học thông qua kết nối dư được giới thiệu trong Mục 2.4. Với tín hiệu gradient từ tự chú ý, Scorer có xu hướng chọn các token được chú ý nhiều nhất bởi bộ giải mã (được tham số hóa bởi θ). Tuy nhiên, vẫn còn câu hỏi liệu việc lựa chọn có tối ưu hay không. Ở đây chúng tôi cung cấp một ước lượng thực nghiệm về khoảng cách giữa việc lựa chọn nuggets tối ưu và Scorer.

Giả sử chúng ta chọn k nuggets từ n token, chúng tôi định nghĩa một lựa chọn như một tập hợp các chỉ số I={i₁, i₂, ..., iₖ}, 1≤iⱼ≤n. Từ định nghĩa, chúng ta có thể thấy rằng I ⊆ {1,2,3, ..., n}.

Chúng tôi định nghĩa thêm việc lựa chọn tối ưu I* là việc lựa chọn đạt được hiệu suất tốt nhất trên một tác vụ hạ nguồn, ví dụ perplexity thấp nhất cho mô hình hóa ngôn ngữ. Chúng tôi ký hiệu việc lựa chọn của Scorer là Ī. Chúng tôi muốn trả lời hai câu hỏi: I* và Ī giống nhau như thế nào, và khoảng cách hiệu suất giữa I* và Ī là gì?

Việc tìm I* là một bài toán tối ưu hóa tổ hợp không tầm thường. Giải pháp duy nhất có thể, như chúng tôi biết, là liệt kê (n k) lựa chọn khác nhau, điều này không khả thi đối với n và k lớn. Do đó, chúng tôi xấp xỉ I* bằng một thuật toán tham lam. Ý tưởng cơ bản là bắt đầu với I ← Ī. Lặp lại, đối với mỗi chỉ số i∈I, chúng tôi thay thế nó bằng một chỉ số tối ưu từ các chỉ số chưa được chọn để nó đạt được hiệu suất tác vụ hạ nguồn tốt nhất. Chúng tôi chính thức hóa nó trong Thuật toán 1 với một ví dụ tác vụ hạ nguồn của mô hình hóa ngôn ngữ.

Chúng tôi tiến hành thí nghiệm với các checkpoint trong Mục 5. Chúng tôi nén một chuỗi tối đa 128 token thành nuggets với tỷ lệ nén 10x. Chúng tôi trình bày cho mô hình thêm 64 token mà không nén. Mô hình được yêu cầu dự đoán 64 token tiếp theo, và chúng tôi đo perplexity cấp từ con của DODO. Bởi vì Thuật toán 1 chứa 2 vòng lặp for và tốn kém để thực hiện, chúng tôi chỉ lấy mẫu 1000 tài liệu từ tập test của WikiText-103 (Merity et al., 2017).

Để đo sự khác biệt giữa Ī và I*, chúng tôi đếm có bao nhiêu phần tử được thay thế trong Ī bằng Thuật toán 1. Trung bình, 24.7% token nuggets được thay thế, có nghĩa là Scorer khoảng 75.3% "đúng". Sau khi thay thế Ī bằng I*, perplexity cấp từ con tổng thể được cải thiện từ 7.74 xuống 7.13, hoặc I* tốt hơn khoảng 7.9% so với Ī về hiệu suất tác vụ hạ nguồn.

Kết luận, chúng tôi tiến hành thí nghiệm để chỉ ra rằng Scorer đủ để chọn nuggets vì nó có thể đạt được hiệu suất tương tự như một bộ chọn tối ưu nhận biết bộ giải mã.

Thuật toán 1 Một thuật toán tham lam để tìm việc lựa chọn "tối ưu" I*.
Đầu vào: k (số nuggets) và n (số token) (0< k≤n), đầu ra bộ mã hóa x₁:ₙ
Đầu ra: Một lựa chọn I và perplexity LM tương ứng b
Khởi tạo I={i₁, i₂, ..., iₖ} với Scorer.
Perplexity b←Decoder(x₁:ₙ,I) ▷ Perplexity thấp nhất cho đến nay
for i∈I do
    for i'∈{1,2, ..., n}\I do ▷ Tất cả các thay thế có thể từ các chỉ số chưa được chọn
        I'←(I\{i})∪{i'} ▷ Thay thế i trong I bằng i'
        Perplexity b'←Decoder(x₁:ₙ,I')
        if b'< b then ▷ Nếu i' tốt hơn i, thực hiện thay thế vĩnh viễn
            b←b', I ← I'
        end if
    end for
end for

B Chi tiết triển khai & huấn luyện
B.1 Triển khai
Pipeline huấn luyện của DODO được triển khai với PyTorch (Paszke et al., 2019) và gói Pytorch Lightning (William A. Falcon và Nhóm PyTorch Lightning, 2019). Chúng tôi sử dụng ZeRO stage-2 được cung cấp bởi gói DeepSpeed Rasley et al. (2020) với độ chính xác hỗn hợp để tăng tốc việc huấn luyện. Việc triển khai DODO dựa trên gói huggingface/transformers (Wolf et al., 2020). Trình đọc tập dữ liệu của chúng tôi sử dụng huggingface/datasets (Lhoest et al., 2021).

B.2 Siêu tham số và thiết bị huấn luyện
Đối với tất cả các thí nghiệm, chúng tôi theo thiết lập huấn luyện của Touvron et al. (2023b) và sử dụng trình tối ưu Adam (Kingma và Ba, 2015) với tốc độ học 1×10⁻⁴, β₁= 0.9, β₂= 0.95, và ϵ= 10⁻⁵. Chúng tôi sử dụng bộ lập lịch tốc độ học cosine (Loshchilov và Hutter, 2017) với làm ấm 2k bước, và chu kỳ của hàm cosine annealing được đặt là 150k bước.

Tất cả các quá trình tạo văn bản trong bài báo này được triển khai như giải mã tham lam.

Chúng tôi huấn luyện các mô hình trên 16 GPU NVIDIA Tesla V100 (32 GiB), mỗi cái với kích thước batch 1. Gradient được tích lũy trong 2 batch trước khi thực hiện các trình tối ưu. Tất cả các mô hình được huấn luyện cho đến khi dừng sớm vì sự hội tụ của loss trên tập validation.

B.3 Số lượng tham số
Trong mục này, chúng tôi liệt kê số lượng tham số trong DODO, như được hiển thị trong Bảng 5. Ngoại trừ mô hình LLAMA đóng băng, DODO có một bộ mã hóa và bộ giải mã, chứa các tham số bổ sung cho mô hình Llama với LoRA (Hu et al., 2022) (rank =32), một scorer (mạng nơ-ron feedforward 2 tầng), và một prompt mềm thêm một token đặc biệt vào ma trận embedding.

Đối với các thí nghiệm trong Mục 5, chúng tôi sử dụng LoRA để huấn luyện COMPRESSIVE, chứa một bộ giải mã và một prompt mềm như chúng tôi đã hiển thị trong Bảng 5. Tuy nhiên, so với kích thước của LLAMA, các tham số có thể huấn luyện của cả DODO và COMPRESSIVE đều ít hơn đáng kể (<1%).

C Ví dụ văn bản cho phân tích lựa chọn nuggets
Chúng tôi lấy mẫu một đoạn văn từ Wikipedia và chạy Scorer trên văn bản, nơi chúng tôi đặt tỷ lệ nén r= 10. Kết quả được hiển thị trong Hình 7.

D Prompt được sử dụng trong bài báo
Ở đây chúng tôi liệt kê tất cả các prompt được sử dụng trong Mục 6.

D.1 Nén văn bản với LM
Prompt được sử dụng bởi phương pháp LMSUMM để tạo bản tóm tắt cho một văn bản cho trước là:

[INST]
Vui lòng tóm tắt văn bản sau thành $WORD từ: $TEXT
[/INST]

Chúng tôi thay thế $WORD bằng ⌈n·r⌉, trong đó n là số từ (đếm bằng khoảng trắng) và r là tỷ lệ mong muốn (trong Mục 6, r là 10).

D.2 Trả lời câu hỏi trên SQuAD
Trong thí nghiệm SQuAD (Mục 6.1), một prompt được sử dụng để trả lời câu hỏi cho một tài liệu:

[INST]
$DOCUMENT
Dựa trên tài liệu được cung cấp, trả lời câu hỏi sau:
$QUESTION
[/INST]

Chúng tôi thay thế $DOCUMENT bằng tài liệu ngữ cảnh và $QUESTION bằng câu hỏi.

D.3 Tóm tắt
Trong thí nghiệm tóm tắt (Mục 6.2), chúng tôi sử dụng prompt sau:

[INST]
$DOCUMENT
Vui lòng tóm tắt tài liệu trên trong một câu.
[/INST]

Chúng tôi thay thế $DOCUMENT bằng tài liệu cần được tóm tắt.

E Thuật toán chuẩn hóa cho câu trả lời SQuAD
Đầu ra của mô hình ngôn ngữ có xu hướng có các token khác ngoài câu trả lời hoặc có các dạng khác nhau. Đối với mỗi cặp đầu ra mô hình và câu trả lời SQuAD, chúng tôi áp dụng các quy tắc sau:

• Chuyển đổi tất cả các số tiếng Anh thành chữ số. Ví dụ chuyển đổi "hai" thành "2".
• Thay thế tất cả các dấu câu bằng khoảng trắng.
• Xóa khoảng trắng ở hai bên.
• Chuyển chuỗi thành chữ thường.

Sau các bước này, một chương trình được sử dụng để kiểm tra xem đầu ra mô hình có chứa câu trả lời hay không. Chúng tôi hạn chế mô hình tạo ra tối đa 64 token trong trường hợp chúng tạo ra nhiều token để đạt được câu trả lời.

F Thí nghiệm tự mã hóa đa ngôn ngữ
Đối với thí nghiệm tự mã hóa, chúng tôi áp dụng kiến trúc của LLAMA và checkpoint của LLaMA-7B (Touvron et al., 2023a) và tinh chỉnh mô hình trên tập dữ liệu Pile (Gao et al., 2020). Cả corpus tiền huấn luyện và tinh chỉnh đều thiên về tiếng Anh nặng nề, nhưng kích thước khổng lồ của LLAMA cho phép nó xử lý các ngôn ngữ khác tiếng Anh. Trong mục này, chúng tôi tiến hành thí nghiệm để kiểm tra khả năng đa ngôn ngữ của DODO.

Chúng tôi áp dụng checkpoint của DODO trong Mục 4 với tỷ lệ nén 10x mà không tinh chỉnh thêm. Chúng tôi lấy mẫu 8 ngôn ngữ: Bulgaria, Đức, Anh, Pháp, Ý, Hà Lan, Ba Lan và Nga. Đối với mỗi ngôn ngữ, chúng tôi lấy mẫu 100 tài liệu từ corpus RedPajama (Together Computer, 2023). Chúng tôi cắt ngắn tài liệu nếu nó dài hơn 512 token. Chúng tôi sử dụng BLEU (Papineni et al., 2002) và perplexity làm thước đo.

Kết quả được hiển thị trong Bảng 6. Chúng ta có thể quan sát rằng DODO vẫn có thể xử lý các ngôn ngữ khác, ngay cả khi nó được tinh chỉnh trên corpus chỉ có tiếng Anh.

--- TRANG 14 ---
mô-đun #tham số phần trăm có thể huấn luyện
LLAMA-7B 6.74B 99.01% không
bộ mã hóa (φ) 25.2M 0.37% có
bộ giải mã (θ) 25.2M 0.37% có
Scorer (φ) 16.8M 0.25% có
prompt mềm (θ) 4,096 <0.0001% có

Bảng 5: Số lượng tham số của DODO. Chúng tôi không phân biệt Llama-7b, Llama-2-7b, và Llama-2-7b-chat ở đây vì chúng có cùng kiến trúc. Các tham số của bộ mã hóa và bộ giải mã được tính như các tham số bổ sung với LoRA so với mô hình cơ sở.

Ngôn ngữ Anh Bulgaria Đức Pháp Ý Hà Lan Ba Lan Nga
Độ dài trung bình 348 346 393 346 295 228 325 407
BLEU 99.1 97.7 98.8 99.0 98.3 97.9 98.3 98.9
Perplexity 1.004 1.040 1.017 1.011 1.014 1.021 1.032 1.032

Bảng 6: Kết quả của thí nghiệm tự mã hóa đa ngôn ngữ.

--- TRANG 15 ---
The Brooklyn Nets đã xây dựng bản thân từ gần như không có gì. Thiếu vắng bất cứ thứ gì gần giống với một tài sản trước năm 2015, Nets đã phải tạo ra cái gì đó từ không có gì. Họ thực sự đã làm được như vậy, nạp đầy đồng thời kho báu cầu thủ và tài sản. Thật không may, nhanh chóng như Marks có được những người trẻ, anh ấy cũng phải quyết định những ai nên ở lại. Đó là một bài tập gian nan, và thậm chí còn khó khăn hơn đối với một đội xa rời cuộc đua. Hầu hết các đội đạt đến giai đoạn này ngay khi họ gần đến tầm playoff. Nets không có sự xa xỉ này, và phải đánh giá với một tầm nhìn dài hạn hơn nhiều so với đội trẻ trung bình. Nói đơn giản, họ phải suy nghĩ như một ứng viên vô địch trước khi trở thành một. May mắn thay, đội hình hiện tại có các tầng riêng biệt của các cầu thủ trẻ về tiềm năng lâu dài của họ. Tám trong số chín cầu thủ dưới 25 tuổi có thể được chia thành hai tầng. Cầu thủ chắc chắn Nhóm những người chắc chắn được giữ lại tương đối đơn giản. Những cầu thủ này có tiềm năng lớn nhất của Nets hiện tại. Mặc dù D'Angelo Russell đã trải qua một số giai đoạn khó khăn, anh ấy đã thể hiện đủ dấu hiệu hứa hẹn để đảm bảo tình trạng "người được giữ lại". Khả năng xử lý bóng khéo léo, ghi điểm từ dribbling, ném từ catch, và tầm nhìn chuyền bóng tuyệt vời của anh ấy đều khiến anh ấy phù hợp lý tưởng cho cuộc tấn công của Kenny Atkinson. Việc là lựa chọn tổng thể số 2 trong một draft thường đủ uy tín để giữ một cầu thủ xung quanh, nhưng Russell đã thể hiện những tia sáng hợp pháp của tiềm năng ngôi sao. Từ bỏ anh ấy bây giờ sẽ là một sai lầm chết người. Jarrett Allen, một trung phong tân binh từ Đại học Texas, đã làm một công việc tuyệt vời trong vai trò chuyên biệt của mình. Với thể lực tuyệt vời cho phép anh ấy bảo vệ rổ và chuyển đổi sang những kẻ tấn công biên, Allen khá có khả năng chỉ huy một hàng phòng ngự hiện đại. Thể lực này cũng giúp anh ấy trong tấn công, vì anh ấy nhận được rất nhiều lobs để hoàn thành các pha pick-and-roll. Khi nghi ngờ, các guard có thể ném nó lên cho anh ấy để có một deuce dễ dàng. Chiều dọc của bóng rổ hiếm khi được đánh giá cao.

Hình 7: Văn bản ví dụ được xử lý bởi Scorer của DODO. Văn bản tối hơn có điểm cao hơn văn bản sáng. Các token có nền xanh lá được chọn làm nuggets.
