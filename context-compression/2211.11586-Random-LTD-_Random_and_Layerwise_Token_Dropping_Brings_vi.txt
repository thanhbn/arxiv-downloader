Random-LTD: Phương pháp Loại bỏ Token Ngẫu nhiên và Theo Lớp Mang lại Huấn luyện Hiệu quả cho Transformer Quy mô Lớn

Zhewei Yao∗, Xiaoxia Wu∗, Conglong Li, Connor Holmes
Minjia Zhang, Cheng Li, Yuxiong He
Microsoft
{zheweiyao, xiaoxiawu, conglong.li, connorholmes, minjiaz, chengli1, yuxhe}@microsoft.com
22 tháng 11, 2022

Tóm tắt
Các mô hình transformer quy mô lớn đã trở thành kiến trúc tiêu chuẩn de-facto cho nhiều ứng dụng học máy khác nhau, ví dụ như CV và NLP. Tuy nhiên, những mô hình lớn này cũng tạo ra chi phí huấn luyện cấm đoán. Để giảm thiểu vấn đề này, chúng tôi đề xuất một phương pháp loại bỏ token ngẫu nhiên và theo lớp mới (random-LTD), bỏ qua việc tính toán của một tập con các token đầu vào tại tất cả các lớp trung gian. Đặc biệt, random-LTD đạt được tốc độ đáng kể và độ chính xác tương đương với baseline huấn luyện tiêu chuẩn. So với các phương pháp loại bỏ token khác, random-LTD không yêu cầu (1) bất kỳ metric dựa trên điểm quan trọng nào, (2) bất kỳ xử lý token đặc biệt nào (ví dụ, [CLS]), và (3) nhiều lớp trong huấn luyện độ dài chuỗi đầy đủ ngoại trừ lớp đầu tiên và cuối cùng. Bên cạnh đó, một lịch trình học tỷ lệ học LayerToken mới được đề xuất cho các vấn đề pretraining để giải quyết yêu cầu điều chỉnh nặng nề cho cơ chế huấn luyện được đề xuất của chúng tôi. Cuối cùng, chúng tôi chứng minh rằng random-LTD có thể được áp dụng cho các ứng dụng rộng hơn, bao gồm pretraining GPT và BERT cũng như các tác vụ finetuning ViT và GPT. Kết quả của chúng tôi cho thấy random-LTD có thể tiết kiệm khoảng 33.3% chi phí tính toán lý thuyết và 25.6% thời gian huấn luyện thực tế đồng thời đạt được đánh giá zero-shot tương tự trên GPT-3 1.3B so với baseline.

1 Giới thiệu
Các transformer quy mô lớn đã được chứng minh có hiệu suất vượt trội trong xử lý ngôn ngữ tự nhiên (Tenney et al., 2019; Radford et al., 2019; Raffel et al., 2019), thị giác máy tính (Dosovitskiy et al., 2020), và các ứng dụng khác (Gong et al., 2021; Guo et al., 2021). Tuy nhiên, cả quy trình pretraining và một số tác vụ finetuning downstream (ví dụ, tóm tắt tài liệu dài) đều tốn thời gian và đòi hỏi tài nguyên lớn. Do đó, có nhu cầu tăng tốc quá trình huấn luyện và giảm chi phí tính toán cho pretraining và finetuning transformer quy mô lớn.

Gần đây, Hou et al. (2022) áp dụng kỹ thuật token pruning/dropping/bypassing (Kim et al., 2021; Goyal et al., 2020; Kim and Cho, 2020) từ suy luận BERT sang pretraining BERT bằng cách bỏ qua việc tính toán của một phần token đầu vào tại một số lớp trung gian. Kết quả của (Hou et al., 2022) (được gọi là TokenBypass) cho thấy nó có thể giảm lý thuyết chi phí pretraining 25% cho cả BERT base và BERT large mà không mất độ chính xác trên các tác vụ finetuning. Mặc dù đạt được tăng tốc tuyệt vời, TokenBypass (1) cần một metric điểm quan trọng để xác định các token bị loại bỏ và xử lý token đặc biệt để giữ các token quan trọng (ví dụ, [CLS]), cả hai đều yêu cầu thiết kế thủ công; (2) phải giữ nửa đầu các lớp và lớp cuối cùng (tổng cộng, một nửa độ sâu) trong huấn luyện độ dài chuỗi đầy đủ, điều này hạn chế khả năng bỏ qua lớp. (3) chỉ tập trung vào các tác vụ pretraining BERT Masked-LM và chưa được áp dụng cho các tác vụ khác, ví dụ, causal-LM. Trong công việc này, chúng tôi giải quyết những thách thức đó và giới thiệu phương pháp loại bỏ token ngẫu nhiên và theo lớp (random-LTD) của chúng tôi.

Tóm lại, những đóng góp của chúng tôi như sau:

• Tất cả các token được đối xử bình đẳng mà không có bất kỳ xử lý token đặc biệt hoặc đo lường điểm quan trọng nào, tức là không có thiết kế thủ công, và được loại bỏ một cách hoàn toàn ngẫu nhiên. Trong khi đó, thay vì hoàn toàn bỏ qua token bị loại bỏ cho tất cả các lớp trung gian (Hou et al., 2022), mỗi lớp trong random-LTD loại bỏ token độc lập với các lớp khác. Điều này giúp multi-head attention trong các lớp trung gian nắm bắt mối quan hệ phụ thuộc giữa các token khác nhau như được đề xuất trong (Vig and Belinkov, 2019).

• random-LTD áp dụng loại bỏ token tại tất cả các lớp trung gian ngoại trừ lớp đầu tiên và cuối cùng, điều này càng giảm thiểu thiết kế thủ công và tăng hiệu quả huấn luyện. Chúng tôi cũng đề xuất một phương pháp tăng trưởng độ dài chuỗi đơn điệu mới khi quá trình huấn luyện phát triển để (1) giảm nhiễu gradient được giới thiệu bởi random-LTD để hội tụ tốt hơn và (2) thu hẹp khoảng cách giữa huấn luyện và suy luận (sinh autoregressive), vì random-LTD phá vỡ cách thức autoregressive trong các lớp trung gian trong quá trình huấn luyện, đối với các mô hình GPT.

• Để giảm nỗ lực điều chỉnh cho quy trình huấn luyện mới được đề xuất, chúng tôi giới thiệu một lịch trình tỷ lệ học LayerToken mới, có tỷ lệ tỷ lệ học dựa trên tổng số token tiêu thụ của mỗi lớp cho các tác vụ pretraining. Chúng tôi cho thấy hiệu suất vượt trội của nó cho random-LTD trên pretraining GPT/BERT so với lịch trình tỷ lệ học dựa trên iteration tiêu chuẩn.

• Chúng tôi kiểm tra rộng rãi random-LTD trên cả các tác vụ pretraining, bao gồm pretraining GPT và BERT, và các tác vụ finetuning, bao gồm finetuning causal-LM cho GPT và phân loại hình ảnh cho ViT. Đối với tất cả các tác vụ, random-LTD đạt được độ chính xác tương tự như phương pháp baseline gốc với tiết kiệm chi phí lý thuyết lên đến 33.3% và tiết kiệm thời gian wall-clock lên đến 25.6%.

• Cuối cùng, chúng tôi cho thấy random-LTD có hiệu ứng regularization tiềm năng, có thể được sử dụng cho cả các vấn đề pretraining và finetuning.

2 Bối cảnh
Kiến trúc Transformer (Vaswani et al., 2017) là một chồng các lớp transformer, mỗi lớp có hai thành phần chính, tức là multi-head attention (MHA) và mạng kết nối feed-forward (FFC). Giả sử transformer có l lớp được ký hiệu là L₁; : : : ; Lₗ. Gọi Xᵢ ∈ Rˢᵈ là tensor đầu ra của lớp transformer thứ i, và x₀ là đầu vào (sau embedding) của transformer. Ở đây s là độ dài chuỗi và d là chiều ẩn.

Token dropping (hoặc token bypassing/pruning) (Kim et al., 2021; Goyal et al., 2020; Kim and Cho, 2020; Press et al., 2021; Wang et al., 2021) ban đầu được đề xuất cho suy luận BERT để giảm chi phí tính toán. Trong trường hợp này, nếu một token i (Xⱼ,ᵢ) được quyết định loại bỏ tại lớp j (Lⱼ), chi phí tính toán của token này qua tất cả các lớp còn lại (Lₖ trong đó k > j) sẽ bị loại bỏ. Do đó, độ dài chuỗi sᵢ của đầu vào Xᵢ₋₁ của lớp thứ i sẽ là một mảng không tăng, tức là s₀ ≥ s₁ ≥ ... ≥ sₗ. Tuy nhiên, cấu hình như vậy đã được chứng minh là không ổn định cho suy luận adaptive token-dropping (Kim and Cho, 2020). Do đó, Kim and Cho (2020) sử dụng quy tắc sandwich và distillation từ (Yu and Huang, 2019) để ổn định huấn luyện và tăng độ chính xác. Nhưng hai phương pháp này cũng làm tăng đáng kể chi phí huấn luyện. Do đó, những kỹ thuật như vậy không thể được áp dụng để tăng tốc quy trình pretraining. Gần đây, Hou et al. (2022) mở rộng token dropping từ suy luận sang pretraining BERT (được gọi là TokenBypass). Hou et al. (2022) sử dụng một số điểm/metric quan trọng để xác định các token bị loại bỏ, ví dụ, cumulative loss và tần suất của mỗi token. Để khắc phục vấn đề bất ổn định huấn luyện, các tác giả đề xuất hai cơ chế chính: (1) quy tắc token dropping sandwich, trong đó các lớp đầu (lớp 1 đến i) và vài lớp cuối (lớp Lₗ₋ⱼ đến Lₗ) của BERT nắm bắt tất cả token (tức là không loại bỏ token) và các lớp trung gian bỏ qua s₀ - s token từ Lᵢ đến Lₗ₋ⱼ. Đặc biệt, các tác giả (chỉ) kiểm tra trên encoder transformer (BERT base 12 lớp và BERT large 24 lớp), và để i = l/2 - 1, j = 1, s₀ = s/2. (2) xử lý token đặc biệt, trong đó các token đặc biệt (ví dụ, [MASK], [CLS], [SEP]) không bao giờ bị loại bỏ.

So với TokenBypass từ (Hou et al., 2022), random-LTD của chúng tôi (1) không yêu cầu metric điểm quan trọng, xử lý token đặc biệt, hoặc quy tắc token dropping sandwich, điều này giảm đáng kể nỗ lực thiết kế thủ công; (2) đã được kiểm tra rộng rãi trên các tác vụ pretraining, bao gồm GPT và BERT, cũng như các tác vụ finetuning, bao gồm phân loại ViT và causal-LM GPT. Trong khi đó, chúng tôi phát hiện ra rằng việc áp dụng trực tiếp TokenBypass cho causal-LM dẫn đến suy giảm độ chính xác nghiêm trọng. Vui lòng xem mô tả chi tiết về random-LTD trong Phần 3 và đánh giá rộng rãi của chúng tôi trong Phần 4 và 5. Chúng tôi cũng bao gồm một cuộc thảo luận kỹ lưỡng về các phương pháp huấn luyện hiệu quả khác trong Phụ lục A.

3 Phương pháp luận
3.1 Phương pháp Loại bỏ Token Ngẫu nhiên và Theo Lớp

Cơ chế Loại bỏ Token Theo Lớp. Như đã chỉ ra trong Phần 2, các phương pháp token dropping suy luận và huấn luyện hiện tại hoặc vĩnh viễn loại bỏ token khỏi đồ thị tính toán tại các lớp trung gian, hoặc ít nhất làm cho một số token hoàn toàn bỏ qua một chuỗi liên tiếp các lớp trung gian. Tuy nhiên, một số công trình (Vig and Belinkov, 2019; Michel et al., 2019; Voita et al., 2019) đã chỉ ra rằng MHA tập trung vào các token khác nhau ở các độ sâu lớp khác nhau và bản đồ attention align với mối quan hệ phụ thuộc mạnh nhất ở giữa kiến trúc transformer. Do đó, TokenBypass được sử dụng trong Hou et al. (2022), tức là hoàn toàn bỏ qua các lớp trung gian, có thể cản trở khả năng học/tổng quát hóa của kiến trúc trong quá trình pretraining/suy luận. Chúng tôi đoán rằng đây có thể là lý do tại sao nhiều lớp đầu/cuối cần được giữ và xử lý token đặc biệt là cần thiết trong (Hou et al., 2022). Để xác minh thêm nếu cơ chế bỏ qua lớp trung gian hoàn toàn này (Hou et al., 2022) gây ra bất kỳ vấn đề về khả năng học nào, chúng tôi áp dụng TokenBypass trên các tác vụ finetuning GPT và quan sát hiệu suất thấp hơn nhiều so với baseline. Xem thêm chi tiết trong Phần 5.1.

Để khắc phục vấn đề này, bây giờ chúng tôi đề xuất một cơ chế layerwise token dropping (LTD). Thay vì hoàn toàn bỏ qua các token bị loại bỏ trên tất cả các lớp trung gian, mỗi lớp transformer độc lập loại bỏ/giữ lại tập hợp token riêng của nó. Cụ thể hơn, nhớ lại rằng đầu vào của lớp thứ (i + 1) (Lᵢ₊₁) là Xᵢ ∈ Rˢᵈ. Ký hiệu chỉ số token bị loại bỏ là Jᵢ = {j₁, j₂, ..., jaᵢ} và chỉ số token được giữ là Kᵢ = {k₁, ..., kbᵢ} sao cho aᵢ + bᵢ = s. Chúng ta có Jᵢ ∪ Kᵢ = {1,2,3..., s} và Jᵢ ∩ Kᵢ = ∅ cho mỗi lớp. Trong khi đó, đối với hai lớp khác nhau bất kỳ Lᵢ₁ và Lᵢ₂, Jᵢ₁ và Jᵢ₂ là độc lập, mặc dù tỷ lệ loại bỏ là giống nhau. Với cơ chế layerwise này, mỗi token hiếm khi bỏ qua tất cả các lớp trung gian. Do đó, sự phụ thuộc của nó vào các token khác có thể được nắm bắt bởi MHA.

Loại bỏ Token Ngẫu nhiên. Các metric dựa trên điểm quan trọng khác nhau được sử dụng để xác định tiêu chí loại bỏ token. Hầu hết chúng có thể được phân loại theo hai cách: các metric liên quan đến điểm attention hoặc các metric dựa trên loss/tần suất. Tuy nhiên, cả hai đều giới thiệu những thách thức khiến LTD kém thực tế hơn. Đặc biệt, đối với các metric dựa trên điểm attention, chi phí tính toán cho LTD quá cao vì metric phải được tính toán cho mỗi lớp; đối với các metric dựa trên loss/tần suất, thường loss tích lũy hoặc tần suất được sử dụng và metric tích lũy này sẽ không thay đổi trong cùng một iteration (tức là một lần forward pass của mạng). Do đó, metric loss/tần suất không thay đổi dẫn đến token bị loại bỏ giống nhau cho các lớp khác nhau, khiến sự phụ thuộc token không được nắm bắt bởi MHA của các lớp trung gian (Vig and Belinkov, 2019).

Để đáp ứng yêu cầu độc lập của LTD, chúng tôi đề xuất sử dụng phân công loại bỏ token hoàn toàn ngẫu nhiên. Đối với mỗi lớp transformer, chúng tôi ngẫu nhiên (đồng đều) chọn một batch nhỏ token để tiến hành tính toán và loại bỏ phần còn lại. Cụ thể hơn, giả sử Mᵢ = {mᵢ(1), mᵢ(2), ..., mᵢ(s)} là một shuffle ngẫu nhiên của S = {1, 2, ..., s}. Sau đó tập hợp token bị loại bỏ là Jᵢ = {mᵢ(1), mᵢ(2), ..., mᵢ(aᵢ)} cho đầu vào của Lᵢ₊₁.

Loại bỏ Token Ngẫu nhiên và Theo Lớp. Kết hợp layerwise token dropping với random token dropping, chúng ta có phương pháp random and layerwise token dropping cuối cùng (random-LTD), có thể hiệu quả áp dụng token dropping cho từng lớp riêng lẻ và có thể nắm bắt sự phụ thuộc attention của mỗi token với những token khác trong các lớp trung gian với xác suất cao.

Minh họa so sánh giữa huấn luyện baseline tiêu chuẩn và random-LTD được hiển thị trong Hình 1 (một so sánh bổ sung với (Hou et al., 2022) trong Hình B.1). Pseudo-code được đưa ra trong Hình 2. Đối với mỗi lớp, so với baseline, random-LTD ngẫu nhiên chọn (hàm "gather" trong Hình 2) một tập con của các token và đưa (hàm "Layer" trong Hình 2) chúng vào lớp transformer. Sau đó, chúng tôi kết hợp (hàm "combine" trong Hình 2) đầu ra của lớp transformer với các token bị loại bỏ để phục hồi độ dài chuỗi đầy đủ. Do đó, lớp tiếp theo vẫn nhận được chuỗi đầy đủ và có thể lặp lại quá trình này.

Vì các token bị loại bỏ của mỗi lớp là độc lập, không cần thiết để random-LTD xử lý các token đặc biệt (ví dụ, [MASK], [CLS], [SEP], [PADDING]) khác với các token bình thường khác, điều này có thể giảm thêm chi phí tính toán tiêu chí loại bỏ. Trong khi đó, chúng tôi cho thấy rằng xử lý token đặc biệt không mang lại lợi ích thêm cho random-LTD trên pretraining BERT trong Phần 5.2.

3.2 Lịch trình Loại bỏ của random-LTD

Các Lớp không có Loại bỏ Token. Trong khi TokenBypass (Hou et al., 2022) cần giữ một nửa số lớp trong huấn luyện độ dài chuỗi đầy đủ, random-LTD không có hạn chế như vậy. Nhờ tính năng nắm bắt attention của random-LTD, chúng ta có thể áp dụng random-LTD cho hầu hết các lớp transformer ngoại trừ lớp transformer đầu tiên và cuối cùng.

Giữ lớp đầu tiên và cuối cùng trong huấn luyện độ dài chuỗi đầy đủ thường dẫn đến hiệu suất tốt hơn vì (1) lớp đầu tiên kết nối trực tiếp với embedding, và nó có thể giúp tinh chỉnh đặc trưng thô; (2) lớp cuối cùng kết nối trực tiếp với dự đoán cuối cùng; một sự tái căn chỉnh đặc trưng cho tất cả token có thể cải thiện chất lượng mô hình. Chúng tôi cũng cung cấp một nghiên cứu chi tiết để cho thấy tầm quan trọng của việc giữ lớp đầu tiên và cuối cùng mà không loại bỏ token trong Phần 5.3.

Tăng trưởng Độ dài Chuỗi Đơn điệu.
Để giảm phương sai gradient được giới thiệu bởi random-LTD để huấn luyện tốt hơn, chúng tôi tăng đơn điệu độ dài chuỗi được giữ trong suốt quá trình huấn luyện (được gọi là MSLG) với một lịch trình tuyến tính. Đặc biệt, tập hợp token bị loại bỏ Jᵢ cho lớp thứ i dần thu hẹp và tập hợp token được giữ Kᵢ dần tăng khi quá trình huấn luyện tiến triển. Ký hiệu kích thước của Jᵢ(Kᵢ) tại bước t là aᵢ,t(bᵢ,t), kích thước cuối cùng của nó là 0(s), và tổng số iteration huấn luyện là T. Giả sử chúng ta muốn dần giảm kích thước của Jᵢ về zero tại iteration T₀ và cường độ giảm là sdec. Sau đó kích thước bước giảm là Tdec = T₀/(a₀,t/sdec), tức là, cứ mỗi Tdec iteration, kích thước của Jᵢ(Kᵢ) giảm (tăng) sdec. Vui lòng xem Hình 3 để minh họa Kᵢ trên pretraining GPT. Chúng tôi cũng cho thấy rằng MSLG vượt trội hơn lịch trình drop không đổi với tiết kiệm tính toán tương tự trong Phần 5.4.

3.3 Lịch trình Tỷ lệ Học Mới cho Pretraining

Khi thực hiện pretraining trên các mô hình ngôn ngữ, chúng ta thường sử dụng lịch trình tỷ lệ học giảm dần dựa trên iteration với một giai đoạn warmup. Đặc biệt, tại vài nghìn hoặc vài trăm iteration đầu, việc warmup tỷ lệ học là quan trọng đối với các tác vụ pretraining phân tán do tính bất ổn định của nó (Goyal et al., 2017; Li et al., 2021). Tuy nhiên, một lịch trình dựa trên iteration không tối ưu cho random-LTD.

Thứ nhất, random-LTD giảm kích thước batch hiệu quả của các lớp trung gian tại giai đoạn warmup ban đầu. Các token huấn luyện hiệu quả cho các lớp token bị loại bỏ trở nên nhỏ hơn nhiều so với huấn luyện baseline. Thứ hai, đối với hầu hết các trường hợp huấn luyện của chúng tôi, MSLG không đạt độ dài đầy đủ cho đến >2/3 của các iteration huấn luyện để tiết kiệm tính toán lớn. Tại thời điểm đó, tỷ lệ học dựa trên iteration khá nhỏ. Và tỷ lệ học nhỏ này không thể cung cấp động lực học hiệu quả cho random-LTD. Do đó, để ổn định giai đoạn huấn luyện ban đầu và có tỷ lệ học đủ lớn trong giai đoạn huấn luyện sau, chúng ta cần tăng các iteration warmup và làm chậm sự suy giảm tỷ lệ học. Ở đây, chúng tôi đề xuất một lịch trình tỷ lệ học mới dựa trên tiêu thụ token theo lớp, được gọi là layer-token learning rate (LayerToken LR). Vui lòng xem Phụ lục C để có mô tả chính thức và chi tiết về LayerToken LR.

Chúng tôi nhấn mạnh rằng người ta luôn có thể điều chỉnh lịch trình tỷ lệ học bằng cách tăng tỷ lệ học tối đa hoặc các iteration warmup. Tuy nhiên, điều đó sẽ yêu cầu rất nhiều nỗ lực kỹ thuật. Do đó, chúng tôi đề xuất lịch trình LayerToken LR này, phù hợp hơn với random-LTD của chúng tôi so với lịch trình tiêu chuẩn. Chúng tôi cũng bao gồm một so sánh chi tiết giữa lịch trình tỷ lệ học tiêu chuẩn và LayerToken LR trong Phần 5.5.

4 Kết quả Chính

Trong phần này, chúng tôi đầu tiên cung cấp kết quả của random-LTD cho pretraining trên các mô hình GPT và BERT. Sau đó chúng tôi mở rộng random-LTD trên lĩnh vực thị giác máy tính để chứng minh các ứng dụng rộng hơn của nó. Tương tự như Phần 3.3 và Phụ lục C, chúng tôi sử dụng LayerToken để tính toán chi phí để đo lường tổng ngân sách huấn luyện. Chúng tôi cũng cung cấp tiết kiệm thời gian huấn luyện thực tế cho pretraining GPT và BERT. Lưu ý rằng việc tiết kiệm thời gian thực phụ thuộc vào nhiều yếu tố khác nhau, ví dụ, việc triển khai và phần cứng.

4.1 Pretraining GPT

Chúng tôi huấn luyện các mô hình kiểu GPT-3 với 350 triệu tham số (GPT-3 350M) và 1.3 tỷ tham số (GPT-3 1.3B) trên tập dữ liệu PILE (Gao et al., 2020) và tổng số token huấn luyện là 300 tỷ. Đối với random-LTD, độ dài token bị loại bỏ ban đầu cho tất cả các lớp trung gian là 1920 (tức là, 128 token được giữ để tính toán), và nó giảm 16 cho mỗi 1.75B token huấn luyện. Sau 210B token huấn luyện, random-LTD suy giảm thành quy trình huấn luyện tiêu chuẩn với độ dài chuỗi đầy đủ. Về mặt lý thuyết, điều này có thể tiết kiệm 1/3 ngân sách huấn luyện LayerToken. Xem Phụ lục D.1 để biết thêm chi tiết.

Các đường cong loss đánh giá cho baseline và random-LTD được hiển thị trong Hình 3. Như có thể thấy, đối với cả GPT-3 350M và GPT-3 1.3B, random-LTD có loss đánh giá tương tự như baseline với tiêu thụ LayerToken ít hơn 1/3. Chúng tôi cũng cung cấp kết quả đánh giá zero-shot trong Bảng 1. Đối với cả GPT-3 350M và GPT-3 1.3B, random-LTD đạt được kết quả tương đương với baseline, Bên cạnh đó, random-LTD có thể tiết kiệm 14.3% thời gian huấn luyện wall-clock trên GPT-3 350M và 25.6% thời gian huấn luyện wall-clock trên GPT-3 1.3B.

Chúng tôi nhắc lại rằng tỷ lệ tiết kiệm tiêu thụ LayerToken không thể chuyển đổi trực tiếp thành tỷ lệ tiết kiệm thời gian huấn luyện wall-clock GPU do việc triển khai/phần cứng. Trong khi đó, lưu ý rằng số tiết kiệm chúng tôi báo cáo ở đây không phải là tiết kiệm tiềm năng tối đa (với việc triển khai/phần cứng cố định, v.v.) vì chúng ta có thể giảm số GPU huấn luyện cho random-LTD tại giai đoạn huấn luyện ban đầu, có độ dài chuỗi huấn luyện hiệu quả ngắn hơn. Ngoài ra, mặc dù random-LTD có cùng tiết kiệm tính toán lý thuyết cho cả GPT-3 350M và GPT-3 1.3B, việc tiết kiệm thời gian wall-clock thực tế khác nhau rất nhiều vì GPT-3 1.3B có kích thước chiều ẩn lớn hơn, có nghĩa là mô hình dành nhiều thời gian hơn cho tính toán thực tế so với các toán tử khác, ví dụ, di chuyển dữ liệu và truyền thông gradient.

4.2 Pretraining BERT

Chúng tôi pretrain BERT large trên tập dữ liệu PILE trong 2M iteration với batch size 1024 và độ dài chuỗi 512 theo (Shoeybi et al., 2019). Chúng tôi áp dụng random-LTD với hai biến thể, random-LTD-1 và random-LTD-2. Đặc biệt, đối với random-LTD-1 (random-LTD-2), độ dài token được giữ ban đầu là 200 (128), và nó tăng 16 cho mỗi 48B (38B) token huấn luyện. Do đó, chúng ta tiết kiệm 26.2% (31.1%) tiêu thụ LayerToken cho random-LTD-1 (random-LTD-2). Chúng tôi đánh giá mô hình được huấn luyện trên bốn tác vụ downstream như (Shoeybi et al., 2019), tức là MNLI, QQP, RACE-m, RACE-h. Lưu ý rằng chúng tôi áp dụng finetuning tiêu chuẩn mà không loại bỏ token để có so sánh công bằng cho cả mô hình pretrained từ baseline và random-LTD. Vui lòng xem Phụ lục D.2 để biết thêm chi tiết.

Bảng 2 tóm tắt kết quả cùng với kết quả đầy đủ trong Bảng E.1. Mặc dù random-LTD hơi tệ hơn baseline trên một tác vụ nhất định (QQP, xem Bảng E.1), nó cho độ chính xác cao hơn nhiều trên các tác vụ khác trong khi tiết kiệm 26.2–31.1% chi phí tính toán lý thuyết trong pretraining. Nhìn chung, random-LTD-1 đạt được độ chính xác trung bình cao hơn 1.54 điểm so với baseline và random-LTD-2 đạt được độ chính xác trung bình cao hơn 1 điểm so với baseline.

Trong khi đó, random-LTD-1 (random-LTD-2) tiết kiệm khoảng 7.95% (11.5%) thời gian wall-clock so với baseline. Lưu ý rằng tương tự như pretraining GPT, việc tiết kiệm phụ thuộc vào việc triển khai/phần cứng, và random-LTD có tiềm năng tiết kiệm lớn hơn nếu huấn luyện đàn hồi được thực hiện. Ngoài ra, mặc dù GPT-3 350M và BERT large có kích thước mô hình tương tự cũng như tiết kiệm tính toán lý thuyết tương tự, việc tiết kiệm thời gian huấn luyện wall-clock cuối cùng khác nhau khoảng 3%. Điều này được gây ra bởi BERT large có độ dài chuỗi cuối cùng ngắn hơn (tức là, 512) so với GPT (tức là, 2048), và thời gian tính toán thực cho một câu với độ dài chuỗi 128 không phải là 1/4 (hoặc 1/16) của một câu với 512 (2048) token. Điều này dẫn đến việc tiết kiệm thời gian tính toán tổng thể cho GPT-3 350M lớn hơn so với BERT large.

4.3 Finetuning ViT

Chúng tôi thực hiện vision transformer (ViT) trên cả ImageNet (với ViT pretrained 12 lớp) và CIFAR (với ViT pretrained 24 lớp). Đối với random-LTD, độ dài chuỗi ban đầu là 66 và tăng tuyến tính đến độ dài chuỗi đầy đủ 197 tại 80% tổng số iteration huấn luyện sao cho đạt được 22.3% tiết kiệm layer-token. Xem chi tiết huấn luyện trong Phụ lục D.3. Chúng tôi tóm tắt kết quả với độ lệch chuẩn trong Bảng 3 cùng với chi tiết đầy đủ trong Bảng H.1. Như có thể thấy, random-LTD có thể đạt được kết quả tương đương với baseline trên cả ba tập dữ liệu. Điều này chứng minh các ứng dụng rộng hơn của random-LTD.

5 Thảo luận

Trong phần này, chúng tôi trình bày một số nghiên cứu ablation quan trọng và hiệu ứng regularization tiềm năng của random-LTD. Bên cạnh ba tác vụ được sử dụng trong các phần trước, chúng tôi cũng bao gồm finetuning GPT trên các vấn đề causal-LM sử dụng GPT-2 350M từ Huggingface (Wolf et al., 2019). Vui lòng xem Phụ lục D.4 để biết chi tiết huấn luyện. Ngoài ra, chúng tôi giảm các iteration của pretraining BERT từ 2M xuống 200k do hạn chế tài nguyên. Vui lòng xem Phụ lục D.2 để biết thêm chi tiết.

5.1 Layerwise Token Dropping vs. TokenBypass

Mặc dù TokenBypass (Hou et al., 2022) chứng minh khả năng tuyệt vời của nó trên pretraining BERT, chính sách bỏ qua của nó có thể vẫn làm tổn hại hiệu suất của các tác vụ khác, ví dụ, causal-LM. Lý do được đề cập trong Phần 3, tức là, MHA của các lớp trung gian tập trung vào các token khác nhau ở các độ sâu khác nhau. Việc hoàn toàn bỏ qua những lớp đó có thể khiến tác vụ causal-LM mất khả năng attention. Tuy nhiên, random-LTD không có vấn đề này vì nó ngẫu nhiên chọn các token được giữ cho mỗi lớp.

Để xác minh điều này, chúng tôi cung cấp một nghiên cứu ablation về so sánh giữa random-LTD và TokenBypass với finetuning GPT-2 350M trên PTB (Marcus et al., 1993). Chúng tôi tạo hai bộ thí nghiệm:

• Bộ 1. Theo (Hou et al., 2022), chúng tôi bỏ qua một nửa số token dựa trên loss trung bình di chuyển thực nghiệm của chúng từ L₁₂ đến L₂₃. Tương tự, chúng tôi áp dụng random token drop không đổi cho các lớp từ giữa đến lớp thứ hai cuối cùng (L₁₂ đến L₂₃).

• Bộ 2. Chúng tôi áp dụng TokenBypass hoặc constant random token dropping cho một nửa số token bắt đầu từ lớp thứ hai (L₂) cho đến lớp thứ hai cuối cùng (L₂₃).

Các đường cong validation của hai trường hợp được hiển thị trong Hình 4. Như có thể thấy, đối với cả hai trường hợp, random-LTD thực hiện tốt hơn nhiều so với TokenBypass. Đặc biệt, đối với so sánh Bộ 2, perplexity của random-LTD thấp hơn khoảng 10 điểm so với TokenBypass, chứng minh rằng random-LTD có thể được áp dụng cho nhiều lớp hơn TokenBypass.

5.2 Có/Không có Xử lý Token Đặc biệt

Khác với tác vụ pretraining GPT, có các câu/đoạn văn liên tiếp, dữ liệu pretraining BERT bao gồm hai câu có thể không liên quan. Các token đặc biệt [CLS] và [SEP] đóng vai trò quan trọng để mô hình xác định điểm bắt đầu/kết thúc của mỗi câu để mô hình có thể dự đoán mối quan hệ giữa hai câu. Do đó, có thể có lợi ích tiềm năng trong việc giữ các token cho tất cả các lớp, có thảo luận chi tiết hơn trong (Hou et al., 2022).

Ở đây chúng tôi trình bày một nghiên cứu ablation về việc liệu giữ những token đặc biệt đó có giúp random-LTD hay không. Chúng tôi thực hiện so sánh trực tiếp cho random-LTD: (1) một với lựa chọn hoàn toàn ngẫu nhiên và (2) cái khác với một tiêu chí bổ sung, tức là, giữ các token đặc biệt cho tất cả các lớp. Xem chi tiết huấn luyện trong Phụ lục D.2

Kết quả của MNLI/QQP được hiển thị trong Bảng 4 cùng với chi tiết đầy đủ trong Bảng H.5. Như có thể thấy, đối với cả loss pretraining và finetuning downstream, xử lý token đặc biệt không cung cấp bất kỳ lợi ích nào cho random-LTD. Lưu ý rằng random-LTD mà không có xử lý token đặc biệt cũng thân thiện hơn với tính toán cho token dropping.

5.3 Tại sao chúng ta cần giữ lớp đầu tiên và cuối cùng?

Để hiểu tại sao giữ lớp đầu tiên và cuối cùng trong huấn luyện độ dài chuỗi đầy đủ, chúng tôi trình bày phân tích độ nhạy lớp đơn được hiển thị trong Hình 5 cho finetuning GPT-2 350M trên Wikitext-2 và Wikitext-103. Đặc biệt, chúng tôi áp dụng token dropping không đổi cho một lớp và giữ tất cả các lớp khác trong chế độ huấn luyện tiêu chuẩn. Sau khi huấn luyện, chúng tôi đo PPL và sử dụng nó làm metric độ nhạy, tức là, PPL cao hơn chỉ ra độ nhạy cao và ngược lại. Hình dạng U của cả hai đường cong ngụ ý rằng lớp đầu tiên và cuối cùng nhạy cảm nhất với token dropping.

Để hiểu thêm nếu random-LTD có thể được áp dụng cho tất cả các lớp khi sử dụng MSLG, chúng tôi bao gồm ba kịch bản khác, tức là, áp dụng random-LTD cho (1) tất cả nhưng không có lớp cuối cùng, (2) tất cả nhưng lớp đầu tiên, và (3) tất cả các lớp. Chúng tôi thực hiện các tác vụ finetuning trên cả causal-LM và phân loại hình ảnh. Xem chi tiết huấn luyện đầy đủ trong Phụ lục D.3 và Phụ lục D.4. Từ Bảng 5 và H.3, chúng ta có thể thấy rõ ràng rằng việc giữ lớp đầu tiên và cuối cùng nguyên vẹn dẫn đến cải thiện đáng kể (vượt quá độ lệch chuẩn) so với ba kịch bản còn lại.

5.4 Tại sao chúng ta cần tăng trưởng độ dài chuỗi?

Bây giờ chúng tôi đưa ra một nghiên cứu ablation về tại sao lịch trình MSLG là cần thiết. Một lần nữa, Chúng tôi thực hiện các tác vụ finetuning trên cả causal-LM và phân loại hình ảnh.

Chúng tôi đặc biệt đặt một tỷ lệ token dropping không đổi phù hợp với việc tiết kiệm token của lịch trình MSLG với tất cả các hyperparameter khác được cố định. Chúng tôi trình bày kết quả trong Bảng 6 và H.4. Có thể thấy rõ ràng rằng với cùng lượng tiết kiệm LayerToken gần như nhau (~33%-35%), lịch trình dropping không đổi có hiệu suất tệ hơn MSLG. Lịch trình MSLG thực sự có thể thậm chí tốt hơn hoặc tương đương với những lịch trình không đổi có tiết kiệm nhỏ hơn 10%.

5.5 Hiệu ứng Lịch trình LayerToken LR

Để nghiên cứu hiệu quả của LayerToken LR, chúng tôi so sánh ba kịch bản huấn luyện cho GPT-3 350M với 300B token huấn luyện (xem Phụ lục D.1 để biết chi tiết huấn luyện): (1) huấn luyện baseline với tỷ lệ học tiêu chuẩn, (2) random-LTD với tỷ lệ học tiêu chuẩn, và (3) random-LTD với LayerToken LR. Các đường cong validation và tỷ lệ học tương ứng của chúng theo iteration được vẽ trong Hình 6. Như có thể thấy, đường cong màu xanh lá cây (random-LTD với LayerToken LR) có thể đạt được loss validation tương đương với baseline, tốt hơn random-LTD với tỷ lệ học tiêu chuẩn. Điều này xác nhận rằng tỷ lệ học nhỏ được giới thiệu bởi lịch trình tỷ lệ học tiêu chuẩn làm chậm việc học của random-LTD tại giai đoạn huấn luyện sau. Một quan sát tương tự được thực hiện cho pretraining BERT, sẽ được trì hoãn đến Phụ lục F do hạn chế không gian.

5.6 So sánh trên GPT-3 1.3B với Các Ngân sách Huấn luyện Khác nhau

Trong phần này, chúng tôi thực hiện các ngân sách huấn luyện khác nhau để huấn luyện GPT-3 1.3B và so sánh hiệu suất của baseline và random-LTD để xác minh nếu random-LTD có thể liên tục tiết kiệm chi phí huấn luyện.

Để hiểu đầy đủ hiệu quả của random-LTD, chúng tôi huấn luyện GPT-3 1.3B sử dụng baseline với 120B (tức là, tiêu thụ 2880 tỷ LayerToken) đến 360B token (tức là, tiêu thụ 8640 tỷ LayerToken). Đối với random-LTD, chúng tôi tuân theo cài đặt Phần 4 của chúng tôi để tiết kiệm 1/3 tổng ngân sách huấn luyện và áp dụng nó cho ba ngân sách huấn luyện, tương đương với 2880, 4800B, và 5760B LayerToken.

Kết quả được tóm tắt trong Bảng 7. Kết quả đáng chú ý đầu tiên ở đây là tổng lượng LayerToken huấn luyện trực tiếp ảnh hưởng đến chất lượng cuối cùng của mô hình. Huấn luyện lâu hơn thường dẫn đến độ chính xác tốt hơn. Trong khi đó, random-LTD có thể sử dụng 2/3 ngân sách huấn luyện để đạt được kết quả đánh giá tương tự như baseline. Ví dụ, random-LTD với ngân sách huấn luyện 2880, 4800, và 5760 tỷ LayerToken có thể dẫn đến độ chính xác tương tự như baseline với ngân sách huấn luyện 4320, 7200, và 8640 tỷ LayerToken.

5.7 Tương tác giữa random-LTD và Dropout

Token dropping có thể được xem như một trường hợp đặc biệt (thô) của dropout. Do đó, nó có thể có tiềm năng hoạt động như dropout cho một số tác vụ nhất định hoặc giúp thêm dropout cho một số tác vụ nhất định. Để điều tra điều này, chúng tôi áp dụng random-LTD có/không có dropout tiêu chuẩn trên cả các tác vụ pretraining và finetuning.

Pretraining BERT large. Vui lòng xem Phụ lục D.2 để biết chi tiết huấn luyện. Đối với random-LTD-3 (random-LTD-4), độ dài token được giữ ban đầu cho tất cả các lớp trung gian là 256, và nó tăng 16 cho mỗi 3.8 (6) tỷ token huấn luyện sao cho cuối cùng chúng ta tiết kiệm 14.1% (22.3%) tiêu thụ LayerToken.

Kết quả được tóm tắt trong Bảng 8 với chi tiết trong Bảng H.5 và chúng tôi bao gồm perplexity pretraining để hiểu rõ hơn hiệu ứng dropout. Rõ ràng, tắt dropout dẫn đến perplexity đánh giá/kiểm tra thấp hơn (được hiển thị trong Bảng 8). Trong khi đó, hiệu suất của những mô hình không có dropout (baseline*, random-LTD-3*, random-LTD-4*) trên MNLI và QQP cho thấy cải thiện rõ ràng so với các đối tác có dropout của chúng (baseline, random-LTD-3, random-LTD-4). Tuy nhiên, đối với finetuning RACE, không có học tập cho mô hình baseline không có dropout, điều này hơi đáng ngạc nhiên nhưng cho thấy tầm quan trọng của dropout cho pretraining. Ngược lại, khi tắt dropout cho random-LTD, chúng ta thấy độ chính xác tốt hơn đáng kể trên RACE, vượt quá baseline pretraining tiêu chuẩn >1% trên RACE-m. Do đó, random-LTD mang lại không chỉ hiệu quả mà còn hiệu ứng regularization tiềm năng cho pretraining BERT.

Finetuning GPT-2 350M. Hãy nghiên cứu thêm hiệu ứng regularization tiềm năng của random-LTD trên các tác vụ finetuning GPT-2 350M. Chúng tôi huấn luyện hai bộ mô hình, một không có dropout và một có dropout (tỷ lệ mặc định là 0.1). Chúng tôi cũng áp dụng random-LTD với ba độ dài chuỗi ban đầu— 128, 256, và 512—sao cho chúng đạt chuỗi đầy đủ 1024 tại cùng epoch (12). Chúng tôi trình bày các đường cong validation của chúng trong Hình 7.

Chúng ta thấy rằng huấn luyện baseline mà không có dropout nhanh chóng overfit, như được hiển thị ở bên trái Hình 7 (đường cong đen). Và PPL validation tốt nhất của nó tệ hơn baseline có dropout (đường cong khối ở hình bên phải). Tuy nhiên, đối với cả ba trường hợp của random-LTD, chúng đạt được PPL tương tự như baseline tiêu chuẩn có dropout. Thậm chí với dropout mặc định ở bên phải Hình 7, validation của nó vẫn đối mặt với vấn đề overfitting không thể bỏ qua sau giữa quá trình huấn luyện. Ngược lại, random-LTD với MSLG giới thiệu một hiệu ứng regularization tiềm năng (thêm). Như có thể thấy, các đường cong validation của random-LTD đang làm phẳng và chúng duy trì trong một khu vực giá trị nhỏ đáng kể về phía cuối quá trình huấn luyện.

Tóm tắt. Chúng tôi không tuyên bố rằng random-LTD có thể thay thế dropout. Trên thực tế, cả hai có thể hoạt động tốt (lưu ý rằng cả hai đều được bật cho kết quả trong pretraining GPT trong Phần 4) với nhau vì dropout và random-LTD tập trung vào các mức độ khác nhau. Chúng ta thấy rằng random-LTD với dropout đạt được perplexity thấp hơn cho tác vụ finetuning nhỏ (được hiển thị ở bên phải Hình 7). Do đó, các hiệu ứng regularization tiềm năng bổ sung được giới thiệu bởi random-LTD có thể bổ sung cho dropout và đóng vai trò hữu ích trong những tác vụ tập dữ liệu nhỏ với các vấn đề overfitting quan trọng. Ngoài ra, chúng ta thấy rằng random-LTD mà không có dropout đạt được độ chính xác finetuning downstream tốt hơn cho pretraining BERT large ngân sách thấp. Điều này có thể chỉ ra random-LTD có thể là một phương pháp regularization tiềm năng cho pretraining ngân sách thấp.

6 Kết luận

Trong công việc này, chúng tôi đề xuất một thuật toán loại bỏ token ngẫu nhiên và theo lớp mới (random-LTD) cùng với các lịch trình dropping và một lịch trình tỷ lệ học mới. Chúng tôi chứng minh hiệu quả của random-LTD trên cả các vấn đề pretraining GPT và BERT cũng như các tác vụ finetuning GPT và ViT. Ngoài ra, chúng tôi khảo sát tất cả các nghiên cứu ablation để xác minh hiệu quả của từng thành phần thuật toán đơn lẻ được sử dụng trong random-LTD. Hơn nữa, chúng tôi cũng cho thấy hiệu ứng regularization tiềm năng được giới thiệu bởi random-LTD. Để thảo luận về các hạn chế và công việc tương lai, xem Phụ lục G.
