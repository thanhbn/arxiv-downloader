# 2309.08583.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/annotation/2309.08583.pdf
# File size: 1728860 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ICLEF: In-Context Learning with Expert Feedback for Explainable Style
Transfer
Arkadiy Saakyan1and Smaranda Muresan1,2
1Department of Computer Science, Columbia University
2Data Science Institute, Columbia University
a.saakyan@cs.columbia.edu, smara@columbia.edu
Abstract
While state-of-the-art large language models
(LLMs) can excel at adapting text from one
style to another, current work does not address
theexplainability of style transfer models. Re-
cent work has explored generating textual ex-
planations from larger teacher models and dis-
tilling them into smaller student models. One
challenge with such approach is that LLM out-
puts may contain errors that require expertise to
correct, but gathering and incorporating expert
feedback is difficult due to cost and availability.
To address this challenge, we propose ICLEF ,
a novel human-AI collaboration approach to
model distillation that incorporates scarce ex-
pert human feedback by combining in-context
learning andmodel self-critique . We show that
our method leads to generation of high-quality
synthetic explainable style transfer datasets for
formality ( E-GYAFC ) and subjective bias ( E-
WNC ). Via automatic and human evaluation,
we show that specialized student models fine-
tuned on our datasets outperform generalist
teacher models on the explainable style trans-
fer task in one-shot settings, and perform com-
petitively compared to few-shot teacher mod-
els, highlighting the quality of the data and the
role of expert feedback. In an extrinsic task of
authorship attribution, we show that explana-
tions generated by smaller models fine-tuned
onE-GYAFC are more predictive of author-
ship than explanations generated by few-shot
teacher models.
1 Introduction
Attribute style transfer is the task of transform-
ing a given text along a particular style dimension,
such as changing its formality, bias, or level of of-
fensiveness (Lample et al., 2019; Sudhakar et al.,
2019; Jin et al., 2022). Formality style transfer (e.g.,
informal →formal) could be useful in any writing
assistance system, while neutralizing text that con-
tains subjective bias would be an important tool forWikipedia editors (Pryzant et al., 2020) or journal-
ists (Rosenberg and Fischer, 2023).
Style transfer approaches have primarily focused
on the text re-writing task (e.g., informal →formal,
subjective bias →neutral) using various methods
from supervised (Rao and Tetreault, 2018; Pryzant
et al., 2020; Zhong et al., 2021) to unsupervised
(Krishna et al., 2020) and zero-shot methods us-
ing LLMs (Reif et al., 2022) (see also Jin et al.
(2022) for a survey on style transfer). However,
to our knowledge, no effort has focused on pro-
viding textual explanations for the style transfer
task. For example, when transforming an informal
sentence “I would throw them out asap !” into a for-
mal paraphrase “I would dispose of them promptly”
it would be useful to provide an explanation of
the informal attributes in the input sentence (e.g.,
textese (“asap"), colloquialism (“throw out")), and
formal attributes for the paraphrase (e.g., lexical
sophistication (“promptly" and “dispose"); lack of
abbreviations (“I would")). Similarly, for neutraliz-
ing subjective bias in “Orbis latinus, integral site of
romance language" →“Orbis latinus, comprehen-
sive site of romance language", it would be useful
to have an explanation about which word/phrase
in the input is biased and why as well as the type
of bias (e.g., Framing (“integral" implies a sub-
jective evaluation on the site’s importance)). The
model’s explanations could help the user better as-
sess the correctness of the style transfer system,
could be used as features in downstream tasks such
as authorship attribution (Section 5), or could act
as a defense against spurious correlations (Ludan
et al., 2023; Camburu et al., 2018) and annotation
artifacts (McCoy et al., 2019; Poliak et al., 2018).
To enable explainability in style transfer models,
we provide the following contributions:
•A new task of explainable style transfer for
which, in addition to sentence rewriting, the
model needs to generate textual explanations.arXiv:2309.08583v2  [cs.CL]  17 Jun 2024

--- PAGE 2 ---
 GYAFCInformal (original si): I would throw them out asap !e-GYAFCLLMParaphrase (synthetic sf):I would dispose of them promptly.Formal Attributes (synthetic ef):lexical sophistication ("promptly", “dispose”)absence of contractions ("I would")Fixed Informal Attributes (iclef-ei):textese ("asap"), colloquialism (“throw out”), exclamation markInformal Attributes (synthetic ei):textese ("asap"), colloquialism (“throw out”), exclamation mark, abbreviated language ("I would")       ICLEFLLMcriticExpert Feedback
Formal (original sf): I would kick them out as soon as possibleFigure 1: Generating E-GYAFC : formality style transfer dataset GYAFC (Rao and Tetreault, 2018) is augmented
with semi-structured natural language explanations. The LLM generates the informal attributes of the input sentence,
a formal paraphrase, and the formal attributes of the resulting sentence. Expert feedback is incorporated via
in-context learning and self-critique to refine the initial generations.
•A novel human-AI collaboration framework,
In Context-Learning with Expert Feedback
(ICLEF) (see Figure 1, Figure 2, and §3.2).
The approach combines model distillation for
explanation generation (Ho et al., 2022; Mag-
ister et al., 2023) with self-critique ability of
LLMs (Madaan et al., 2023; Bai et al., 2022b;
Saunders et al., 2022; Scheurer et al., 2023, in-
ter alia ), where the critic, unlike in prior work,
is instantiated with expert demonstrations.
•Using ICLEF, we create for the first time
datasets for explainable style transfer by aug-
menting an existing formality style transfer
dataset GYAFC (Rao and Tetreault, 2018) and
the neutralizing subjective bias dataset WNC
(Pryzant et al., 2020) with textual explanations
(§3). We show that the datasets generated with
the help of ICLEF, E-GYAFC and E-WNC ,
are of good quality via automatic and expert
evaluation, and that ICLEF-fixed instances are
preferred (§3.3).
•Experiments that show that student models
outperform teacher models in one-shot setting
and perform comparably even with few-shot
teacher models in automatic and expert eval-
uation, confirming the utility and quality of
the datasets (§4). Moreover, in an extrinsic
evaluation, we show that explanations gener-
ated by student models fine-tuned on our data
produce a better signal for the authorship at-
tribution task than the explanations produced
by few-shot teacher models (§5).
We release the data, models, and code to encourage
further research on explainability, learning fromscarce human feedback, and style transfer.1
2 Related Work
Knowledge distillation and human feedback
Model or knowledge distillation is a process of fine-
tuning a smaller student model to imitate the be-
haviour of a more competent teacher model (Beyer
et al., 2022; Buciluundefined et al., 2006; Hinton
et al., 2015). Knowledge distillation became a pop-
ular technique, allowing to generate datasets of sim-
ilar quality to the crowd-sourced data (West et al.,
2022), especially when combined with a model-
in-the-loop approach (Wiegreffe et al., 2022; Bar-
tolo et al., 2022; Chakrabarty et al., 2022). Recent
work explores model distillation with natural lan-
guage explanations (Wang et al., 2023a; Ho et al.,
2022; Magister et al., 2023), showing that large lan-
guage models are capable of generating acceptable
enough reasoning steps for student models to learn.
Approaches to incorporate human feedback such as
RLHF (Stiennon et al., 2020) and DPO (Rafailov
et al., 2023) require large amounts of crowdsourced
data and have not been generally shown to be effec-
tive for expert preferences. Imitation learning from
human feedback (ILF) (Scheurer et al., 2023) uti-
lizes human feedback to improve model-generated
instances, and then fine-tunes on that data. Un-
like these works, we focus on incorporating expert
feedback which is naturally scarce and expensive
to collect. Unlike other self-critique approaches
(Madaan et al., 2023; Bai et al., 2022b; Saunders
et al., 2022), we condition the model on expert
corrections to incorporate high-quality human feed-
back.
1github.com/asaakyan/explain-st

--- PAGE 3 ---
Textual Explanations Natural language expla-
nations have been utilized for a variety of tasks
Wiegreffe and Marasovic (2021), such as natural
language inference (Camburu et al., 2018), com-
monsense (Rajani et al., 2019; Aggarwal et al.,
2021), social norm entailment (CH-Wang et al.,
2023). We focus on creating natural language ex-
planations for the style transfer task, which has not
been addressed before.
Style transfer Style transfer approaches range
from instruction-based methods (Reif et al., 2022)
and paraphrasing (Krishna et al., 2020), to ap-
proaches focused on learning in low-resource set-
tings (Patel et al., 2022). Much of style transfer
work focuses on style representations that decou-
ple style and content (Wegmann et al., 2022; Weg-
mann and Nguyen, 2021), however most of these
methods are not designed to be interpretable. In-
terpretable approaches rely on constructing inter-
pretable embeddings, such as LIWC (Tausczik and
Pennebaker, 2010) or LISA (Patel et al., 2023).
Zhong et al. (2021) proposed identifying biased
segments in conjunction with neutralizing biased
text. Unlike these approaches, we propose to use
natural language explanations to further enhance
model interpretability.
3 Building Datasets for Explainable Style
Transfer
We build two explainable style transfer datasets by
first augmenting existing datasets with synthetic
textual explanations generated by a teacher model
(§3.1), and then improving the generated data us-
ing our In-Context Learning with Expert Feedback
(ICLEF ) framework (§3.2).
3.1 Augmenting Style Transfer Datasets with
Synthetic Textual Explanations
Formality style transfer The GYAFC (Rao and
Tetreault, 2018) formality style transfer dataset con-
tains parallel formal and informal sentences. The
informal sentences are collected from Yahoo An-
swers, and formal paraphrases were crowdsourced
using Amazon Mechanical Turk (AMT). We use
ChatGPT-3.5 to generate explanations and formu-
late the following multi-step generation task: given
an informal sentence from GYAFC si, generate
a structured explanation of its informal attributes
ei, then generate a formal paraphrase sfbased on
these attributes, then the formal attributes of the
resulting paraphrase ef. Generating both eiandefallows us to train models in both directions
(formal →informal and informal →formal). We
use a semi-structured format for the explanations.
Specifically, we ask the model to generate a list
of attributes followed by an excerpt from the sen-
tence as the evidence: attribute (“evidence”), see
examples in Figure 1. These explanations have a
consistent format, making it easier to verify and
automatically evaluate.
Subjective bias style transfer We focus on the
task of neutralizing subjective biased language in-
troduced by Pryzant et al. (2020) to make sentences
follow the Wikipeida Neutral Point of View Pol-
icy.2We start with the Wikipedia Neutrality Corpus
(WNC) (Pryzant et al., 2020), a parallel corpus of
180,000 sentence pairs originating from Wikipedia
edits of subjective biased language. The goal is to
generate an explanation ( eb) for the type of bias
present in the biased sentence ( sb), following the
scheme proposed by Pryzant et al. (2020) and Re-
casens et al. (2013): Framing, Epistemological, and
Demographic (see definitions in Appendix I). It is
estimated by Pryzant et al. (2020) that a small per-
centage of cases, the instances in the WNC contain
noise. We add an additional “No Bias” label for
these cases to reduce hallucinated biases for neu-
tral sentences. In this case, we ask the model to
output “This sentence does not contain bias” as the
explanation (see second WNC example in Table
1). The explanation is structured as Type of Bias
("evidence" reasoning). Then, the teacher model
generates an unbiased paraphrase ( sn). See Figure
2 for an overview. At the time we developed this
dataset, ChatGPT-4 became available, so we use
this more powerful model as a teacher, especially
since generating explanations for this task might
require more reasoning capabilities. We do not
generate explanations for the neutrality of the para-
phrase as we are not exploring the neutral to biased
paraphrase direction due to ethical concerns.
3.2 In-Context Learning from Expert
Feedback ( ICLEF )
ChatGPT generations might contain errors (e.g.,
the generated style attribute "abbreviated language"
with the evidence “I would” in Figure 1). To im-
prove the quality of the data, we turn to expert feed-
back, since previous work has identified that crowd-
workers on platforms such as Amazon Mechanical
Turk could be unreliable for open-ended generation
2Wikipedia.org

--- PAGE 4 ---
 WNC       ICLEFBiased (original sb): orbis latinus, integral site on romance languagese-WNCParaphrase (iclef-sn):Orbis Latinus, a comprehensive site on Romance languagesLLMcriticExpert FeedbackFixed Bias Attributes (iclef-eb):Framing ("integral" implies a subjective evaluation of the site's importance)Bias Attributes (synthetic eb):Epistemological ("integral" implies that the site is essential or indispensable for Romance languages)
Neutral (original sn): orbis latinus, site on romance languagesLLMFigure 2: Generating E-WNC : WNC (Pryzant et al., 2020) is augmented with natural language explanations. The
LLM generates the bias attributes of the input sentence and an unbiased paraphrase. Expert feedback is incorporated
via in-context learning and self-critique to refine the initial generations.
tasks (Karpinska et al., 2021), and might even rely
on ChatGPT to provide their answers (Veselovsky
et al., 2023). The crux of our approach is to com-
bine in-context learning and self-critique abilities
of LLMs by instantiating the LLM-critic model
with few-shot expert feedback demonstrations.
E-GYAFC For the formality style transfer task,
we hire an expert annotator with a Masters degree
in linguistics on Upwork3. Our annotation protocol
(see Appendix J) provides a non-exhaustive ref-
erence to formality and informality attributes and
asks the annotator to provide feedback on which
attributes in ei, efare incorrect if any, among other
information. We provide 50 random samples for
annotation. The annotation process took each ex-
pert only 2-3 hours. We find that the rate of critical
errors observed in formality explanations is signifi-
cantly lower ( ( ≈8% for formality explanations vs
≈56% for informality explanations), so we only
focus on applying the LLM-critic to incorrect in-
formality attributes ( ei).
To do so, we instantiate an LLM-Critic model
by prompting another LLM (ChatGPT-3.5) with
35 expert human feedback corrections in-context
(we find that this number leads to satisfactory cor-
rectness of ≈87%, see Appendix C for how per-
formance changes given the amount of feedback)
and ask it to act as an annotator on the new in-
stances to identify incorrect attributes in them (see
prompt in Table 12 in Appendix E). To mitigate the
risk of generating new incorrect attributes, we only
query the model to identify and remove incorrect
attributes in ei, and if there are any, provide a fixed
iclef-eiwhere they are removed. We refer to the re-
sulting model as LLM-Critic (see how abbreviated
langauge is removed in Figure 1).
In this way, we fix ≈30% of the generated data
3Upwork.com(2853 instances) – all instances for which the critic
has predicted that an improvement is needed. The
resulting data (which we refer to as E-GYAFC )
contains 9,960 original si, synthetic sfinstances
with corresponding iclef -ei, synthetic efattribute
explanations. We randomly split the data into 8,000
training instances and 1,960 held-out test instances.
E-WNC For the neutralizing subjective bias task,
we hire an expert annotator with a PhD in linguis-
tics on Upwork. We provide 50 random samples
ensuring equal representation for each type of bias.
To counteract potential annotation biases, we de-
cided to use two annotators, one of the authors act-
ing as the additional annotator, and then randomly
sample the instances in equal proportions. The
annotation protocol asks to provide a corrected ex-
planation instead of ebif the synthetic explanation
contains an incorrect type of bias or wrong justifica-
tion. We randomly sample 35 distinct instances of
the two annotators’ feedback, finding that this num-
ber leads to satisfactory correctness of ≈93% (see
Appendix C). We provide the expert critiques in-
context in a similar way to E-GYAFC (see bottom
prompt in Appendix E, Table 12). Since new bias
attributes might have been introduced, we regen-
erate the paraphrase sngiven the new explanation
(see corrected type of bias and a new paraphrase in
Figure 2). We fix 8% of synthetic explanations in
this manner (the lower rate of errors is explained by
the higher quality of initial generations due to the
use of a more powerful ChatGPT-4 model). The
resulting dataset ( E-WNC ) contains 3,000 original
WNC sbbiased sentences, iclef -ebbias explana-
tions, as well as the corresponding iclef -snneu-
tralized sentences. We randomly split the data into
2,500 training instances and 500 held-out test in-
stances.

--- PAGE 5 ---
Informal (si) Gen. expl. (synthetic ei) ICLEF expl. (iclef-ei)
hopefully you aren’t too old or you are
screwed.informal greeting ("hopefully"), slang
("screwed"), contraction ("aren’t")slang ("screwed"), contraction
("aren’t")
more info, we are both in our very late
twenties.[...], omission of prepositions ("in our
very late twenties")abbreviation ("info"), colloquialism
("very late twenties")
Biased (sb) Gen. expl. (synthetic eb) ICLEF expl. (iclef-eb)
[...] a play on the title of the popular
mtv series, "unplugged".Epistemological ("popular" implies that
the MTV series is universally well-
liked)Framing ("popular" is a subjective term
that implies the MTV series is widely
liked)
[..] kendal, cbe (born 25 september
1946) is an english actress known in
the united kingdom [...].Demographic ("actress" implies that the
person is female and could perpetuate
gender stereotypes or assumptions)This sentence does not contain bias.
claims for the existence of paranormal
psychic abilities such as clairvoyance
are highly controversial.This sentence does not contain bias. Epistemological ("highly controversial"
implies that the existence of paranormal
psychic abilities is widely disputed)
Table 1: Qualitative comparison of dataset instances before and after application of ICLEF.
3.3 Dataset quality
Automatic evaluation of paraphrase quality
We estimate paraphrase quality automatically using
Mutual Implication Score (MIS) (Babakov et al.,
2022) and Formality Score (see §4.2 for metrics
details) between our formal paraphrases and the
ones in GYAFC. We find that our paraphrases are
of comparable quality with an MIS of 81.30 vs.
83.08 for GYAFC, yet we achieve a higher formal-
ity score of 98.43 vs. 89.39 for GYAFC (see Table
2). For example, for the GYAFC example in Figure
1, the formal paraphrase contains kick them out ).
Similarly, for the E-WNC dataset we report bias
score from an off-the-shelf classifier (see §4.2)
along with MIS. The MIS scores are 79.32 for
original paraphrases vs. 85.58 for our paraphrases,
indicating higher semantic similarity. The neutral-
ity scores are 69.34 vs. 72.64, indicating higher
neutrality of our paraphrases (see Table 2).
e-GYAFC e-WNC
MIS Formality MIS Neutrality
Orig. para. 83.08 89.39 79.32 69.34
Cand. para. 81.30 98.43 85.58 72.64
Table 2: Synthetic paraphrases (generated via model
distillation for E-GYAFC and E-WNC ) exhibit higher
quality overall in automatic evaluation compared to orig-
inal paraphrases (from GYAFC and WNC, respectively).
Human evaluation For E-GYAFC we hire 3
expert annotators, 2 of which performed the anno-
tation, as well as an independent expert annotator
with a masters degree in linguistics. We ask their
preferences on 100 randomly sampled instances
with respect to the explanations (synthetic eivs.
iclef -ei) and paraphrases (original sfin GYAFCvs. synthetic sfinE-GYAFC ). In addition, we
ask for acceptability judgments (whether the para-
phrase or explanation are correct and complete) for
the preferred paraphrase and separately for ef. We
report preference or equal rates and acceptability
rates.4Overall, we found that our dataset instances
are considered acceptable, with the average accept-
ability rate for ei, sf, efbeing 87%, 77%, 98% re-
spectively (row 1 in Table 3). Synthetic paraphrases
are generally preferred to the ones in the GYAFC
corpus (average 77%), and iclef -explanations are
preferred or are equal in quality with original gener-
ations on average in 90% of cases (row 2 in Table 3).
We computed the pairwise accuracy between anno-
tator responses for all categories of E-GYAFC eval-
uation, and found that it averages at 81% across all
categories. We provide more details in Appendix A.
Table 1 shows qualitative examples of successful
edits with ICLEF . Figure 3 shows the top 10 most
frequent informality attributes.
e-GYAFC e-WNC
ei sf ef eb sn
Acceptability 87% 77% 98% 73% 74%
Preference 90% 77% - 78% 77%
Table 3: Acceptability and Preference Rates (between
synthetic explanation vs. iclef explanation, and syn-
thetic paraphrase vs. original paraphrase form the
dataset) for E-GYAFC and E-WNC.
For E-WNC , we hire 2 annotators, one of whom
performed the ICLEF annotation. To meaningfully
evaluate the preference of iclef-explanations com-
pared to the synthetic ones, we ask to provide feed-
4We compute preference or equal preference among ac-
ceptable instances. For acceptability, we compute dispreferred
instances as unacceptable.

--- PAGE 6 ---
0 1000 2000 3000 4000
Frequencycontraction
colloquialism
textese
personal pronoun
slang
casual tone
informal punctuation
capitalization
informal vocabulary
abbreviationAttributesFigure 3: Top 10 informal attributes. See top 50
(in)formality attributes in Appendix Figure 5, 6).
back on 50 instances where the explanation has
been updated. The average preference for iclef -
ebcompared to synthetic ebis 78%. Average ac-
ceptability rate of iclef -ebis 73%. The average
preference for synthetic sncompared to the neu-
tral sentence in the E-WNC corpus is 76%. Aver-
age acceptability rate of synthetic snis 74%. The
pairwise accuracy between the annotators is 77%.
More details can be found in Appendix B.
4 Evaluation of Student and Teacher
Models on the Explainable Style
Transfer Task
We focus on evaluating the performance of select
large language models on the explainable style
transfer task. We do not evaluate post-hoc rationale
systems (generating attributes given the paraphrase
pair), since such pipeline models are less likely
to reflect the underlying reasons for the model
prediction, while models that jointly predict and
rationalize exhibit desirable properties for faith-
ful explanations (Wiegreffe et al., 2021). For ex-
plainable formality style transfer, we test the gen-
eration of ef, si, eigiven sf(Formal →Informal)
andei, sf, efgiven si(Informal →Formal) on a
held-out test set from E-GYAFC . We evaluate
how closely the model generated ei, efmatch E-
GYAFC explanations, and we evaluate the se-
mantic closeness and paraphrase quality for si, sf
with reference-free metrics. Similarly, we evaluate
eb, snfor the neutralizing subjective bias task using
a test set from E-WNC . We report F1 scores for
bias classification in sn.4.1 Models
We test two smaller student models fine-tuned
on our datasets. We fine-tune LLaMA-7B (Tou-
vron et al., 2023) and Alpaca-7B (Taori et al.,
2023) models on our data converted to the Al-
paca instruction format. For formality style trans-
fer, we fine-tune in both Formal →Informal and
Informal →Formal directions separately ( →), as
well as in both directions in a multi-task fash-
ion (↔). For subjective bias transfer, we only
fine-tune in one direction. In addition, we test
the teacher models (ChatGPT-3.5=GPT-3.5 and
ChatGPT-4=GPT-4) in few-show setting, which
is an ambitious baseline: first, they were used for
data generation which biases reference-based met-
rics, second, they are prompted with improved in-
stances of the data. Since the teacher models are
closed models, we also test a representative open
instruction-tuned model larger than the student,
Vicuna-13B (Chiang et al., 2023) (Vic in tables),
in few-shot setting. See detailed description of the
models, hyperparameters, prompts, and additional
experiments in Appendix F.
4.2 Automatic Evaluation
We use the following metrics:
•BLEU (Papineni et al., 2002): We measure the
amount of exactly matched formal and infor-
mal attributes and evidences between the gen-
erated structured explanation and reference
explanation in E-GYAFC and E-WNC.
•Mutual Implication Score (MIS) (Babakov
et al., 2022) is a symmetric measure of text
semantic similarity based on a RoBERTa (Liu
et al., 2019) model fine-tuned for natural lan-
guage inference and paraphrase detection used
in prior work (e.g., Patel et al., 2022).
•Style Accuracy: For E-GYAFC , we use For-
mality/Informality Score5: RoBERTa (Liu
et al., 2019) fine-tuned to predict whether sen-
tences are formal or informal using GYAFC
and Online Formality Corpus (OFC) (Pavlick
and Tetreault, 2016). It achieves up to 0.98
ROC AUC. For E-WNC , we use Bias Score6:
DistilBERT model (Sanh et al., 2020) fine-
tuned for bias classification on the BABE me-
dia bias dataset annotated by experts (Spinde
5huggingface.co/s-nlp/roberta-base-formality-ranker
6huggingface.co/social-media-fairness/classifier-bias-sg

--- PAGE 7 ---
Formal→Informal Informal →Formal
Model SizeForm.Attrs.
BLEUMIS InformalityInform.Attrs.
BLEUInform.Attrs.
BLEUMIS FormalityForm.Attrs.
BLEUAverage
Vic1 13B 23.16 83.24 35.26 10.97 27.31 61.22 98.70 9.88 43.72
Vic5 13B 24.16 85.18 33.58 13.09 27.95 73.18 98.20 15.45 46.35
Vic10 13B 19.78 82.00 49.17 12.35 30.97 73.26 97.86 17.14 47.82
GPT-3.5 1 ? 28.88 90.12 39.65 12.80 27.65 81.98 97.68 10.36 48.64
GPT-3.5 5 ? 33.98 90.37 38.23 16.14 36.03 85.37 97.54 16.30 51.74
GPT-3.5 10 ? 36.78 89.57 49.81 18.51 37.36 85.62 97.75 20.31 54.46
LLaMA → 7B 39.64 85.31 61.33 19.86 38.02 81.80 97.77 25.10 56.10
Alpaca ↔ 7B 40.42 81.76 66.71 21.11 40.34 79.43 98.57 25.75 56.76
Table 4: Performance of instruction-tuned and fine-tuned models on the explainable formality style transfer task.
Best bolded, best non-fine-tuned underlined.
et al., 2021), on which it obtaines F1 score of
up to 79.
Given that the bias type detection task can be
viewed as a classification task (only 3 labels are
present), we report F1 scores for bias type clas-
sification in the explanation. We also report the
average across all metrics.
Results Table 4 shows model performance on the
explainable formality style transfer task. While the
Vicuna model does well in terms of style transfer
(as evidenced by high MIS and Formality scores),
it lacks in explanation quality (overall low BLEU
scores). Student models perform better than the
one-shot teacher model and competitively in 10-
shot scenario judging by the Average score. Table
5 shows model performance on the explainable sub-
jective bias style transfer task. Similarly, student
models outperform the teacher model in one-shot
setting. They also outperform few-shot models
weaker than the teacher model (Vicuna and GPT-
3.5).
As for style transfer performance without con-
sidering the explanations, we see a slight de-
crease in the Informal →Formal (-0.93% avg. MIS
and Formality compared to one-shot teacher) and
Bias→Unbias (-2.13% avg. MIS and Neutrality)
tasks. This is expected as consistent with prior
work such as e-SNLI (Camburu et al., 2018).7
We see a sizeable increase in performance for
Formal→Informal (+12.60%) direction. The in-
formality scores are much better for the student
models, perhaps due to the tendency to generate
more formal speech both by teacher models and
the other instruct models.
7“...while sacrificing a bit of performance, we get a better
trust that when EXPLAINTHENPREDICT predicts a correct
label, it does so for the right reasons.”Model F1Attrs.
BLEUMISNeutr.
scoreAvg
Vic1 17.49 2.88 74.61 68.33 48.61
Vic5 16.28 9.58 81.22 73.12 54.64
Vic10 23.08 9.35 84.66 74.90 56.30
GPT-3.5 1 34.83 8.15 83.18 75.07 55.47
GPT-3.5 5 28.78 13.94 81.71 73.11 56.25
GPT-3.5 10 37.83 17.25 82.92 73.18 57.78
GPT-4 1 36.87 12.33 82.38 75.72 56.81
GPT-4 5 41.24 14.91 82.36 75.48 57.58
GPT-4 10 39.82 17.07 83.48 74.87 58.47
Alpaca → 65.03 24.46 82.57 71.63 59.55
LLaMA → 67.25 25.81 83.48 71.33 60.21
Table 5: Performance on Biased →Unbiased explain-
able style transfer. GPT-3.5 = ChatGPT-3.5, GPT-4 =
ChatGPT-4, Vic = Vicuna-13B. Number of shots in un-
derscripts. Best score bolded.
4.3 Expert Human Evaluation: Preference
Judgments
We evaluate how well models’ outputs align with
expert preferences. For e-GYAFC , we sample 100
instances from the test set and the correspond-
ing model outputs from Vic 1, the teacher model
GPT-3.5 in one-shot setting (GPT-3.5 1), and the
best student fine-tuned model. We ask for their
preferences in terms of correctness and complete-
ness of explanations, as well as in terms of para-
phrase semantic preservation. We find that stu-
dent model (Alpaca ↔) generations are preferred to
teacher model GPT-3.5 1and Vicuna 153% of the
time by the expert linguist (GPT-3.5 1is preferred
42%), indicating the student model is more aligned
with expert preferences. See a qualitative example
in Table 6. We also evaluate how well do smaller
fine-tuned models perform compared to the teacher
model in a few-shot setting. We sample 40 outputs
and ask for preferences between generations from
Vicuna and GPT-3.5 in the 10-shot setting (GPT-

--- PAGE 8 ---
3.510) as well as the student fine-tuned model. The
fine-tuned model performs competitively with 49%
preference for GPT-3.5 10and 43% preference for
the small student fine-tuned model.
Similarly, we hire an expert annotator and pro-
vide 50 test set instances from E-WNC along with
model outputs from GPT-3.5 10(weaker than the
teacher model), 10-shot Vicuna, and the best stu-
dent fine-tuned model (LLaMA →). We find that
LLaMA →is preferred at a rate of 56%, followed
by GPT-3.5 10(24%). See qualitative examples in
Table 7. We also measure preferences to one-shot
teacher model (GPT-4 1), finding comparable pref-
erence of 42% for LLaMA →and 40% for GPT-4 1.
Formal (Input)
If it is feasible, allow love to prevail.
Vicuna 1
-Formal Attributes: passive voice ("it is feasible", "al-
low love to prevail"), use of verb "to be" ("is feasible",
"prevail")
-Informal Paraphrase: It’s okay to let love rule.
-Informal Attributes: active voice ("it’s okay", "let
love rule"), informal verb use ("rule")
GPT 1
-Formal Attributes: conditional clause ("if it is feasi-
ble"), elevated vocabulary ("allow love to prevail")
-Informal Paraphrase: If it’s possible, just go with love.
-Informal Attributes: simple language ("if it’s possi-
ble"), imperative form ("just go"), contraction ("it’s")
Alpaca ↔
-Formal Attributes: precise language ("feasible"), use
of conditional clause ("if it is feasible"), lexical sophistica-
tion ("prevail")
-Informal Paraphrase: let love win if it’s possible.
-Informal Attributes: contraction ("it’s"), casual tone
("let love win")
Table 6: Comparison between generations from a one-
shot instruction-tuned model (Vicuna, ChatGPT), and
our best small student fine-tuned model for explainable
formality style transfer.
5 Extrinsic Evaluation of Formality Style
Transfer Explanations
We use Authorship Verification (Martindale and
McKenzie, 1995; Coulthard, 2004; Neal et al.,
2017) as an extrinsic task utilizing PAN 2022
(Bevendorff et al., 2022) data. This is a binary
classification task of deciding if two texts belong to
the same author or not. The two input texts are two
raw documents (e.g., paragraphs from blog posts),
one of which is written by author A, and another
one which is either written by the same author ABiased (Input)
some cacti produce beautiful flowers, which like spines
and branches arise from areoles.
-Bias Attributes (Gold) : Framing ("beautiful" adds
subjectivity and implies a positive evaluation of the flow-
ers)
Vicuna 10(Incorrect)
-Bias Attributes: Framing (using "some" suggests that
not all cacti produce beautiful flowers)
-Neutralized Paraphrase: Certain cacti produce beau-
tiful flowers, which like spines and branches arise from
areoles.
GPT-3.5 10(Incorrect)
-Bias Attributes: Epistemological (using the word
"beautiful" implies that the flowers produced by cacti are
universally considered attractive)
-Neutralized Paraphrase: Some cacti produce flowers,
which, like spines and branches, arise from areoles.
LLaMA →(Correct)
-Bias Attributes: Framing ("beautiful" suggests a sub-
jective evaluation of the flowers)
-Neutralized Paraphrase: Some cacti produce flowers,
which like spines and branches arise from areoles.
Table 7: Comparison between generations from a
few-shot instruction-tuned models (10-shot Vicuna,
ChatGPT-3.5), and our best small student fine-tuned
model for explainable bias style transfer.
or by a different author B. We then derive a repre-
sentation (features to be used for classification) of
these input texts using our explainable style transfer
model. We run Alpaca IF→Fon each text (at most
15 sentences per author are considered) and extract
explanations containing informality attributes and
evidence (see Table 8). In a preliminary evaluation
of the usefulness of these features, we compute
the similarity between authors by measuring the
percentage of overlapping attributes. Note that the
evidence fragments corresponding to the attributes
are not used in this preliminary experiment. We
then use the percentage of overlapping attributes as
a classification score, for example, if author A uses
colloquialism and textese, and author B uses col-
loquialism and abbreviation, their similarity score
is the number of common attributes divided by the
number of unique attributes between the authors,
or1
3(0.33). Here, only 1 attribute is common (col-
loquialism) whereas the total identified attributes is
3 (colloquialism, abbreviation, textese). Following
the computation described above, we take the simi-
larity score as a confidence for a binary prediction
task. If the similarity is high, there is a high chance
the authors are the same (prediction = 1), and if not
they are likely not the same (prediction = 0). The

--- PAGE 9 ---
underlying assumption is that it is more likely that
the same author would use some of the informality
features they used previously (not every sentence
in PAN is completely informal, but informality is a
very broad category, so when authors do use some
informal attributes they can provide a signal for au-
thorship). We compute ROC AUC between the con-
fidence scores (author similarity score) and ground
truth predictions from the PAN dataset (1 if authors
are the same and 0 if not). We compare explana-
tions from Vicuna 10, GPT-3.5 10and Alpaca IF→F
by their predictive signal for this task. Explanations
by Alpaca IF→Fachieve an AUC of 56.4, whereas
explanations from the Vicuna and GPT-3.5 models
achieve a score of 50.0 and 47.0 respectively. This
indicates a potential application of the explanations
generated by the student model fine-tuned on our
dataset ( E-GYAFC ) to be used as interpretable
authorship features that can be explored in future
work.
Attribute Evidence
Colloquialism “assumed they all started off
low!?”, “typing it out”
Textese “xx”
Informal V ocabulary “give you a call”, “arrange some-
thing”
Informal Tone “hoping to borrow a couple of
charging leads”
Table 8: Informality features for authorship identifica-
tion: on the left, informality attributes identified by our
model, on the right, textual evidence provided by it.
6 Conclusion
We propose a framework to augment two style
transfer datasets with semi-structured textual expla-
nations. To improve quality of model distillation
and incorporate expert feedback, we propose ICLEF
(In-Context Learning from Expert Feedback), a
novel human-AI collaboration framework leverag-
ing both in-context learning and self-critique abili-
ties of LLMs. We evaluate smaller student models
fine-tuned on the resulting datasets compared to
large teacher models and conduct expert human
evaluation. We also extrinsically evaluate the ex-
planations for the formality style transfer on the
downstream task of authorship attribution.
7 Limitations
The GYAFC dataset does not contain all types of
informal and formal language, namely, they mostly
focus on interpersonal relationships (the subsetused for this paper) and entertainment. Future
work could consider extending our approach to
other style transfer datasets, including ones more
encompassing of formality.
While our methods are intended to produce faith-
ful explanations, there can still be instances when
a model does not rely on the attributes in order to
complete the paraphrase. We also observed that
hallucinations can still be present in our fine-tuned
models’ explanations and hope that future work
will try to address these issues. We also note that
our approach does not replace expert annotation
as it heavily relies on LLMs that may still halluci-
nate. It is only meant to be applicable in scenarios
where expert feedback is expensive and/or difficult
to gather.
One limitation of our method is that we used a
relatively small number of experts to conduct our
study. However, we believe that this setting mir-
rors real-life conditions where experts are usually
scarcely available. We hope our approach provides
a more general framework for incorporating expert
feedback that can be adjusted to experts’ needs
(e.g., a forensic linguist may require a different
style transfer explanation than a literary critic).
Fine-tuning and running inference on large mod-
els requires expensive computational resources.
However, we hope that our study presents a con-
vincing argument that fine-tuning a smaller model
once may be more efficient and accurate than run-
ning a large general-purpose model with elaborate
long-context prompts.
8 Ethics Statement
The GYAFC corpus was created using the Yahoo
Answers corpus: L6 - Yahoo! Answers Com-
prehensive Questions and Answers version 1.0.
This Yahoo Answers corpus can be requested free
of charge for research purposes. Access to our
GYAFC dataset will require users to first gain ac-
cess to this Yahoo Answers corpus. Authors ob-
tained permission to access the dataset.
Our datasets do not include any protected data
to the best of our knowledge. All annotators are
fairly compensated for their work in accordance
with their asking rate (typically over 20 USD per
hour).
Our bias style transfer model is only intended
for use in a human-in-the-loop fashion and not by
itself to adjudicate bias in text. We hope that the
explanation generation capacity of our model will

--- PAGE 10 ---
improve upon existing bias classifiers that typically
do not provide textual explanations. In Appendix
G, we show how style transfer can be used to evade
AI-text detectors. Similarly to Krishna et al. (2023),
we reiterate that this is not to provide a way to at-
tack such systems, but to bring awareness to the
community that current detectors are easy to evade.
Moreover, we bring to attention the issue of de-
tecting text on which style transfer paraphrase has
been applied. We hope that future work develops
systems capable of defending against such attacks,
perhaps utilizing explanations generated by our
system.
GYAFC and WNC may potentially contain of-
fensive data as they are crowdowdsourced, how-
ever, on samples that we saw we did not find alarm-
ing ethical issues.
Acknowledgements
We thank the annotators for their work and pro-
viding detailed feedback. We also would like to
thank the reviewers for productive and engaging
discussions. This research is supported in part by
the Office of the Director of National Intelligence
(ODNI), Intelligence Advanced Research Projects
Activity (IARPA), via the HIATUS Program con-
tract #2022-22072200005. The views and conclu-
sions contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies, either expressed or im-
plied, of ODNI, IARPA, or the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for governmental purposes
notwithstanding any copyright annotation therein.
References
Shourya Aggarwal, Divyanshu Mandowara, Vishwa-
jeet Agrawal, Dinesh Khandelwal, Parag Singla, and
Dinesh Garg. 2021. Explanations for Common-
senseQA: New Dataset and Models. In Proceedings
of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers) , pages 3050–3065, Online.
Association for Computational Linguistics.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Merouane Debbah, Etienne Goffinet, Daniel Hes-
low, Julien Launay, Quentin Malartic, Badreddine
Noune, Baptiste Pannier, and Guilherme Penedo.
2023. Falcon-40B: an open large language model
with state-of-the-art performance.Nikolay Babakov, David Dale, Varvara Logacheva, and
Alexander Panchenko. 2022. A large-scale computa-
tional study of content preservation measures for text
style transfer and paraphrase generation. In Proceed-
ings of the 60th Annual Meeting of the Association for
Computational Linguistics: Student Research Work-
shop , pages 300–321, Dublin, Ireland. Association
for Computational Linguistics.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
Nicholas Joseph, Saurav Kadavath, Jackson Kernion,
Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom
Brown, Jack Clark, Sam McCandlish, Chris Olah,
Ben Mann, and Jared Kaplan. 2022a. Training a
helpful and harmless assistant with reinforcement
learning from human feedback.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022b. Constitutional ai: Harmless-
ness from ai feedback.
Max Bartolo, Tristan Thrush, Sebastian Riedel, Pontus
Stenetorp, Robin Jia, and Douwe Kiela. 2022. Mod-
els in the loop: Aiding crowdworkers with generative
annotation assistants. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 3754–3767, Seattle,
United States. Association for Computational Lin-
guistics.
Edward Beeching, Sheon Han, Nathan Lambert,
Nazneen Rajani, Omar Sanseviero, Lewis Tun-
stall, and Thomas Wolf. 2023. Open llm
leaderboard. https://huggingface.co/spaces/
HuggingFaceH4/open_llm_leaderboard .
Janek Bevendorff, Berta Chulvi, Elisabetta Fersini, An-
nina Heini, Mike Kestemont, Krzysztof Kredens,
Maximilian Mayerl, Reynier Ortega-Bueno, Piotr
P˛ ezik, Martin Potthast, et al. 2022. Overview of pan
2022: Authorship verification, profiling irony and
stereotype spreaders, and style change detection. In
Experimental IR Meets Multilinguality, Multimodal-
ity, and Interaction: 13th International Conference

--- PAGE 11 ---
of the CLEF Association, CLEF 2022, Bologna, Italy,
September 5–8, 2022, Proceedings , pages 382–394.
Springer.
Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Mar-
keeva, Rohan Anil, and Alexander Kolesnikov. 2022.
Knowledge distillation: A good teacher is patient and
consistent. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition
(CVPR) , pages 10925–10934.
Steven Bird and Edward Loper. 2004. NLTK: The natu-
ral language toolkit. In Proceedings of the ACL In-
teractive Poster and Demonstration Sessions , pages
214–217, Barcelona, Spain. Association for Compu-
tational Linguistics.
Cristian Buciluundefined, Rich Caruana, and Alexan-
dru Niculescu-Mizil. 2006. Model compression. In
Proceedings of the 12th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ’06, page 535–541, New York, NY , USA.
Association for Computing Machinery.
Oana-Maria Camburu, Tim Rocktäschel, Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-
ral language inference with natural language explana-
tions. In Advances in Neural Information Processing
Systems , volume 31. Curran Associates, Inc.
Sky CH-Wang, Arkadiy Saakyan, Oliver Li, Zhou Yu,
and Smaranda Muresan. 2023. Sociocultural norm
similarities and differences via situational alignment
and explainable textual entailment.
Tuhin Chakrabarty, Arkadiy Saakyan, Debanjan Ghosh,
and Smaranda Muresan. 2022. FLUTE: Figurative
language understanding through textual explanations.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
7139–7159, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Sahil Chaudhary. 2023. Code alpaca: An instruction-
following llama model for code generation. https:
//github.com/sahil280114/codealpaca .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.Malcolm Coulthard. 2004. Author identification, idi-
olect, and linguistic uniqueness. Applied linguistics ,
25(4):431–447.
Databricks. 2023. Free dolly: Introducing the world’s
first truly open instruction-tuned llm. Blog post.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Namgyu Ho, Laura Schmid, and Se-Young Yun.
2022. Large language models are reasoning teachers.
ArXiv , abs/2212.10071.
Di Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova,
and Rada Mihalcea. 2022. Deep learning for text
style transfer: A survey. Computational Linguistics ,
48(1):155–205.
Marzena Karpinska, Nader Akoury, and Mohit Iyyer.
2021. The perils of using Mechanical Turk to evalu-
ate open-ended text generation. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 1265–1285, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
John Kirchenbauer, Jonas Geiping, Yuxin Wen,
Jonathan Katz, Ian Miers, and Tom Goldstein. 2023.
A watermark for large language models.
Kalpesh Krishna, Yixiao Song, Marzena Karpinska,
John Wieting, and Mohit Iyyer. 2023. Paraphras-
ing evades detectors of ai-generated text, but re-
trieval is an effective defense. arXiv preprint
arXiv:2303.13408 .
Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.
Reformulating unsupervised style transfer as para-
phrase generation. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) , pages 737–762, Online. Asso-
ciation for Computational Linguistics.
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte,
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver
Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri,
David Glushkov, Arnav Dantuluri, Andrew Maguire,
Christoph Schuhmann, Huu Nguyen, and Alexander
Mattick. 2023. Openassistant conversations – democ-
ratizing large language model alignment.
Guillaume Lample, Sandeep Subramanian, Eric Smith,
Ludovic Denoyer, Marc’Aurelio Ranzato, and Y-Lan
Boureau. 2019. Multiple-attribute text rewriting. In
International Conference on Learning Representa-
tions .
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach.

--- PAGE 12 ---
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. 2023. The flan
collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688 .
Josh Magnus Ludan, Yixuan Meng, Tai Nguyen,
Saurabh Shah, Qing Lyu, Marianna Apidianaki, and
Chris Callison-Burch. 2023. Explanation-based fine-
tuning makes models more robust to spurious cues.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdan-
bakhsh, and Peter Clark. 2023. Self-refine: Iterative
refinement with self-feedback.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub
Adamek, Eric Malmi, and Aliaksei Severyn. 2023.
Teaching small language models to reason.
Colin Martindale and Dean McKenzie. 1995. On
the utility of content analysis in author attribution:
"the federalist". Computers and the Humanities ,
29(4):259–270.
Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right
for the wrong reasons: Diagnosing syntactic heuris-
tics in natural language inference. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 3428–3448, Florence,
Italy. Association for Computational Linguistics.
Eric Mitchell, Yoonho Lee, Alexander Khazatsky,
Christopher D. Manning, and Chelsea Finn. 2023.
Detectgpt: Zero-shot machine-generated text detec-
tion using probability curvature.
Tempestt Neal, Kalaivani Sundararajan, Aneez Fatima,
Yiming Yan, Yingfei Xiang, and Damon Woodard.
2017. Surveying stylometry techniques and applica-
tions. ACM Comput. Surv. , 50(6).
OpenAI. 2023. New ai classifier for indicating ai-
written text.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Ajay Patel, Nicholas Andrews, and Chris Callison-
Burch. 2022. Low-resource authorship style transfer
with in-context learning.
Ajay Patel, Delip Rao, and Chris Callison-Burch. 2023.
Learning interpretable style embeddings via prompt-
ing llms.Ellie Pavlick and Joel Tetreault. 2016. An empiri-
cal analysis of formality in online communication.
Transactions of the Association for Computational
Linguistics , 4:61–74.
Guilherme Penedo, Quentin Malartic, Daniel Hesslow,
Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The RefinedWeb dataset
for Falcon LLM: outperforming curated corpora
with web data, and web data only. arXiv preprint
arXiv:2306.01116 .
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277 .
Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infer-
ence. In Proceedings of the Seventh Joint Confer-
ence on Lexical and Computational Semantics , pages
180–191, New Orleans, Louisiana. Association for
Computational Linguistics.
Reid Pryzant, Richard Diehl Martinez, Nathan Dass,
Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.
Automatically neutralizing subjective bias in text.
Proceedings of the AAAI Conference on Artificial
Intelligence , 34(01):480–489.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res. , 21(1).
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019. Explain your-
self! leveraging language models for commonsense
reasoning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 4932–4942, Florence, Italy. Association for
Computational Linguistics.
Sudha Rao and Joel Tetreault. 2018. Dear sir or madam,
may I introduce the GYAFC dataset: Corpus, bench-
marks and metrics for formality style transfer. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers) , pages 129–140, New Or-
leans, Louisiana. Association for Computational Lin-
guistics.
Marta Recasens, Cristian Danescu-Niculescu-Mizil, and
Dan Jurafsky. 2013. Linguistic models for analyz-
ing and detecting biased language. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 1650–1659, Sofia, Bulgaria. Association for
Computational Linguistics.

--- PAGE 13 ---
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,
Chris Callison-Burch, and Jason Wei. 2022. A recipe
for arbitrary text style transfer with large language
models. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 837–848, Dublin,
Ireland. Association for Computational Linguistics.
Stephen Robertson and Hugo Zaragoza. 2009. The prob-
abilistic relevance framework: Bm25 and beyond.
Foundations and Trends ®in Information Retrieval ,
3(4):333–389.
Scott Rosenberg and Sara Fischer. 2023. Newsrooms try
ai to check for bias and error. Accessed: 2024-02-15.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2020. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
Long Ouyang, Jonathan Ward, and Jan Leike. 2022.
Self-critiquing models for assisting human evalua-
tors.
Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak,
Jun Shern Chan, Angelica Chen, Kyunghyun Cho,
and Ethan Perez. 2023. Training language models
with language feedback at scale.
Timo Spinde, Manuel Plank, Jan-David Krieger, Terry
Ruas, Bela Gipp, and Akiko Aizawa. 2021. Neu-
ral media bias detection using distant supervision
with BABE - bias annotations by experts. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2021 , pages 1166–1177, Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 3008–3021. Curran Associates,
Inc.
Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Ma-
heswaran. 2019. “transforming” delete, retrieve, gen-
erate approach for controlled text style transfer. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3269–
3279, Hong Kong, China. Association for Computa-
tional Linguistics.
Mirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-
sky. 2022. Prompt-and-rerank: A method for zero-
shot and few-shot arbitrary textual style transfer with
small language models. In Proceedings of the 2022
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 2195–2222, Abu Dhabi,
United Arab Emirates. Association for Computa-
tional Linguistics.Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of language
and social psychology , 29(1):24–54.
MosaicML NLP Team. 2023. Introducing mpt-7b: A
new standard for open-source, commercially usable
llms. Accessed: 2023-06-20.
Edward Tian. 2023. Ai content detector and writing
captcha for chat gpt, openai, bard, education.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. arXiv
preprint arXiv:2302.13971 .
Veniamin Veselovsky, Manoel Horta Ribeiro, and
Robert West. 2023. Artificial artificial artificial intel-
ligence: Crowd workers widely use large language
models for text production tasks.
PeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen,
and Xiang Ren. 2023a. PINTO: Faithful language
reasoning using prompt-generated rationales. In The
Eleventh International Conference on Learning Rep-
resentations .
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith,
Iz Beltagy, and Hannaneh Hajishirzi. 2023b. How
far can camels go? exploring the state of instruction
tuning on open resources.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023c. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 13484–13508, Toronto, Canada. Association
for Computational Linguistics.
Anna Wegmann and Dong Nguyen. 2021. Does it cap-
ture STEL? a modular, similarity-based linguistic
style evaluation framework. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7109–7130, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Anna Wegmann, Marijn Schraagen, and Dong Nguyen.
2022. Same author or just same topic? towards
content-independent style representations. In Pro-
ceedings of the 7th Workshop on Representation
Learning for NLP , pages 249–268, Dublin, Ireland.
Association for Computational Linguistics.

--- PAGE 14 ---
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems .
Peter West, Chandra Bhagavatula, Jack Hessel, Jena
Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,
Sean Welleck, and Yejin Choi. 2022. Symbolic
knowledge distillation: from general language mod-
els to commonsense models. In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 4602–4625, Seat-
tle, United States. Association for Computational
Linguistics.
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,
Mark Riedl, and Yejin Choi. 2022. Reframing
human-AI collaboration for generating free-text ex-
planations. In Proceedings of the 2022 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 632–658, Seattle, United States.
Association for Computational Linguistics.
Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to
explain: A review of datasets for explainable natural
language processing. In Proceedings of the Neural
Information Processing Systems Track on Datasets
and Benchmarks , volume 1. Curran.
Sarah Wiegreffe, Ana Marasovi ´c, and Noah A. Smith.
2021. Measuring association between labels and
free-text rationales. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 10266–10284, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
John Wieting, Kevin Gimpel, Graham Neubig, and Tay-
lor Berg-kirkpatrick. 2022. Paraphrastic representa-
tions at scale. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing: System Demonstrations , pages 379–388, Abu
Dhabi, UAE. Association for Computational Linguis-
tics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020. Hug-
gingface’s transformers: State-of-the-art natural lan-
guage processing.
Yang Zhong, Jingfeng Yang, Wei Xu, and Diyi Yang.
2021. WIKIBIAS: Detecting multi-span subjective
biases in language. In Findings of the Association
for Computational Linguistics: EMNLP 2021 , pages
1799–1814, Punta Cana, Dominican Republic. Asso-
ciation for Computational Linguistics.

--- PAGE 15 ---
A Human evaluation details: E-GYAFC
We hire three annotators for preference evaluation:
A1 (bachelors degree in linguistics), A2 (bache-
lors degree in linguistics and a masters degree in
education), A3 (bachelor and master degrees in lin-
guistics) on Upwork. Due to high prevalence of
the positive class, there is a high chance of ran-
dom agreement, hence we provide a more granular
look into the expert annotations than the inter-rater
agreement in Table 9. Pairwise accuracy between
annotator responses for all categories of E-GYAFC
evaluation, and found that it averages at 81% across
all categories. Annotator A2 expressed concerns
that the paraphrases may sound unnatural due to ex-
cessive formality (we believe it is due to the context
in which the informal expression would be uttered)
and that explanations sometimes miss punctuation
errors (which, while important, is not critical for
model-generated explanations).
ei
pref.ei
accept.sf
pref.sf
accept.ef
accept.
A1 91% 95% 64% 64% 98%
A2 87% 84% 76% 75% 96%
A3 91% 83% 91% 93% 100%
Table 9: Expert evaluation of E-GYAFC dataset quality.
We report percentage of time each item was preferred,
as well as acceptability judgements.
B Human evaluation details: E-WNC
For E-WNC , we hire two expert annotators A1
and A2. A1 is a professional with over ten years
experience in translation and interpretation, data
annotation, linguistics and publishing. A2 is a PhD
in linguistics with background in psycholinguis-
tics and neurolinguistics and experience in writing,
proofreading and editing academic texts
eb
pref.eb
accept.sn
pref.sn
accept.
A1 82% 84% 84% 86%
A2 74% 62% 70% 62%
Table 10: Expert evaluation of E-WNC dataset quality.
We report percentage of time each item was preferred,
as well as acceptability judgements.C How does ICLEF performance change
depending on the amount of feedback
provided?
We perform a small-scale study to explore how per-
formance of the self-critique component of ICLEF
changes depending on the number of in-context
examples provided. We sample 15 instances of syn-
thetic explanations (before applying ICLEF) and
evaluate generated critiques for correctness when
providing 1, 10, or 35 instances of expert feedback
few-shot. We evaluate the critiques for correctness,
i.e. they have to both identify the incorrect attribute
and not introduce any incorrect attributes. Figure 4
shows how by increasing the amount of feedback
to 35-shot, the correctness is raised to ≈87% for
E-GYAFC and≈93% for E-WNC , which can be
deemed satisfactory.
00.10.20.30.40.50.60.70.80.91
1-shot10-shot35-shotICLEF performance vs. amount of feedback
e-GYAFC (GPT-3.5)e-WNC (GPT-4)
Figure 4: ICLEF performance increases with amount of
feedback, reaching satisfactory accuracy at around 35
shots.
D Additional estimations for
trustworthiness of explanations
We conduct expert evaluation of our data in Section
3.3. To further confirm trustworthiness of explana-
tions, we check that every evidence fragment actu-
ally occurs in the text. For every data instance, we
compute what percentage of evidences was present
in the sentence. We report average across the re-
spective datasets and explanations in Table 11 sum-
marizes the results, where it is evident that the
explanations have high trustworthiness. We note
that some explanations discuss absence of certain
words in which case they should not be present
in the sentence in the first place. Moreover, our
structured explanation format would make it very
easy for the end-user to verify the correctness of the
explanation and alert them of potentially incorrect
paraphrase.

--- PAGE 16 ---
e-GYAFC e-WNC
ei ef eu
Trustworthiness 95.0% 92.3% 98.9%
Table 11: Explanation Trustworthiness Rates Across
Datasets: percentage of evidences actually present in
the sentence.
E Prompts
You are an extremely attentive and critical annotator with
background in stylometry and linguistics. You will be
provided with an informal sentence. You will also be
provided with an explanation of its informality attributes.
Decide whether the explanation is incorrect, and if so,
describe what attributes were listed incorrectly.
EXAMPLES: <. . . >
You are an extremely attentive and critical annotator with
background in ethics, journalism, critical thinking and
bias identification. You will be provided with a possibly
biased sentence. You will also be provided with an ex-
planation of its bias attributes. <...> If the explanation is
incorrect, reply with a correction. Focus on three main
types of bias in a sentence: <. . . >
EXAMPLES: <. . . >
Table 12: Prompts for LLM-critic models. Top is used
for formality style transfer, bottom is used for subjective
bias style transfer.
ChatGPT Explanation Generation Prompt We
provide an instruction as a system prompt (“You
are an expert forensic linguist...”) and 6 examples
of the task in the OpenAI ChatML format.8
Instruction: You are an expert forensic
linguist. Your task is to identify
infromal attributes in a setnece, modify
them to create a formal sentence, and then
output the attributes of the generated
formal sentence. Use the following
format: attribute (excertp from text in
quotation marks). Make sure to provide
a complete list of informal and formal
atttributes. Focus on what has changed
between formal and informal sentences.
Informal writing tends to be more casual,
personal, and conversational than formal
writing. Here are some common features of
informal writing: Contractions: Informal
writing often uses contractions, such as
"I’m," "can’t," "won’t," and "they’ve,"
which are generally avoided in formal
writing. ...
8github.com/openai/openai-python/blob/main/chatml.mdExamples: Informal greetings and
sign-offs: Informal writing often uses
casual greetings, such as "Hi" or "Hey,"
and sign-offs like "Cheers" or "Take
care." Informal: if ur under 18 u
have a BIG PROBLEM. Informal Candidates:
’18’, ’BIG’, ’PROBLEM.’, ’if’, ’u’,
’ur Attributes of Informal Style:
textese ("ur", "u"), capitalization
("BIG PROBLEM"), colloquialism ("BIG
PROBLEM")...
Prompt for ChatGPT ICLEF generation You
are an extremely attentive and critical
annotator with background in stylometry
and linguistics. You will be provided
with an informal sentence. You will also
be provided with an explanation of its
informality attributes. Decide whether
the explanation is incorrect, and if
so, describe what attributes were listed
incorrectly.
EXAMPLES: Informal Sentence: Look, If
you really like this person, just tell
her. Informal Attributes: colloquialism
("just tell her"), contraction ("If you"),
simple sentence structure. Attributes
Listed Incorrectly: contraction ("If you"
is not a contraction)...
F Model details, additional models and
experiments
Below we provide some additional experiments and
models used for them as well as experimentation
details.
Instruction-tuned Models All instruction-tuned
models are provided with the same one-shot prompt
(modulo special token requirements) and genera-
tion parameters.
•MPT-7B-Instruct: built by finetuning MPT-
7B (Team, 2023) on a dataset derived from
the Databricks Dolly-15k (Databricks, 2023)
and the Anthropic Helpful and Harmless (Bai
et al., 2022a) datasets.
•Alpaca-7B (Taori et al., 2023) a model fine-
tuned from the LLaMA-7B model on 52K
instruction-following demonstrations gener-
ated with the Self-Instruct framework (Wang
et al., 2023c).

--- PAGE 17 ---
•Vicuna-13B (Chiang et al., 2023): an open-
source chatbot trained by fine-tuning LLaMA
on user-shared conversations collected from
ShareGPT9. It places first in the Hugging-
face Open LLM Leaderboard (Beeching et al.,
2023) based on human and GPT-4 evaluation
as of writing this paper.
•Falcon-40B (Almazrouei et al., 2023) causal
decoder-only model trained on 1,000B tokens
of RefinedWeb (Penedo et al., 2023) enhanced
with curated corpora.
•Tülu-65B (Wang et al., 2023b) a 65B LLaMa
model finetuned on a mixture of instruction
datasets (FLAN V2 (Longpre et al., 2023),
CoT (Wei et al., 2022), Dolly (Databricks,
2023), Open Assistant 1 (Köpf et al., 2023),
GPT4-Alpaca (Peng et al., 2023), Code-
Alpaca (Chaudhary, 2023), and ShareGPT).
ChatGPT For ChatGPT-3.5 we use
gpt-3.5-turbo-1106 . For ChatGPT-4 we
usegpt-4 .
Fine-tuned models We fine-tune below models
onE-GYAFC .→indicates fine-tuning two models
in each direction, and ↔indicates fine-tuning on
combined data in both directions.
•FLAN-T5-XL ↔(Chung et al., 2022) approxi-
mately 3B parameter instruction-tuned model
based on the T5 architecture (Raffel et al.,
2020).
•LLaMA-7B →(Touvron et al., 2023) model
by Meta trained on 1 trillion tokens.
•Alpaca-7B →,↔(Taori et al., 2023) a model
fine-tuned from the LLaMA-7B model on 52K
instruction-following demonstrations. In ad-
dition, we test Alpaca-7B noexpl as the model
fine-tuned for Formal to Informal style trans-
fer with no explanations provided in the fine-
tuning data or in the output.
Fine-tuning hyperparameters We fine-tune all
models using the script provided in the Stanford
Alpaca repository.10We use exact same hyperpa-
rameters, except for batch size which we adjust to 1
due to memory constraints. We fine-tune our mod-
els on 4 A100 NVIDIA 40GB GPUs. We train our
models for 3 epochs with learning rate 2e-05 with
9sharegpt.com
10Alpaca GitHubcosine rate scheduler and warmup ratio of 0.03.
We did not perform hyperparameter search. We
report results form single runs with random seeds
preserved due to computational constraints.
Inference parameters We use the same hyper-
parameters for generation across all models, that
is temperature=0.7, top p=0.9, max new tokens =
256. We use the huggingface library.
One-shot instruction is provided below:
Identify informal attributes in a given
sentence, modify them to create a formal
sentence, and then output the attributes
of the generated formal sentence.
For example:
Informal: how can you tell if a girl
likes you or not?
Informal Attributes: direct question
form ("how can you tell"), informal
language ("girl", "likes you") Formal:
What are some indications that a woman may
be interested in you? Formal Attributes:
indirect question form ("what are some
indications"), lexical sophistication
("woman", "interested in you")
For the following sentence, identify
informal attributes in a given sentence,
modify them to create a formal sentence,
and then output the attributes of the
generated formal sentence.
For Tulu, we add the <asistant> and <user> to-
kens as advised by model developers.
Packages We used transformers (Wolf et al.,
2020) for language model inference and NLTK
package (Bird and Loper, 2004) for sentence tok-
enization.
Additional experiments with instruct and fine-
tuned models We additionally perform experi-
ments with MPT-7B, Falcon-40B, Tulu, and non-
fine-tuned Alpaca. We also fine-tune an Al-
paca model with no explanations. See Table
13. The model fine-tuned without explanations
(Alpaca noexpl ) achieves comparable performance,
indicating that generating explanations does not sig-
nificantly hurt performance on the standard style
transfer task.
Fluency of style transfer Fluency could be used
as an additional metric to measure the quality of
the style transfer. However, fluency measures nega-
tively correlate with informality, even though the se-
mantic content stays the same. Because of that, we

--- PAGE 18 ---
Formal→Informal Informal →Formal
Model SizeForm.Attrs.
BLEUMIS InformalityInform.Attrs.
BLEUInform.Attrs.
BLEUMIS FormalityForm.Attrs.
BLEUAverage
MPT 7B 24.59 51.84 9.82 2.10 23.26 46.26 58.40 0.86 27.14
Alpaca 7B 17.07 80.74 32.53 6.51 23.67 73.69 86.82 8.27 41.16
Falcon 40B 8.38 28.12 13.43 1.23 20.80 38.01 62.69 7.13 22.47
Tülu 65B 24.90 19.60 7.12 0.02 27.76 26.69 27.34 0.28 16.71
FLAN-T5 → 3B 0.00 8.54 0.01 0.00 0.00 9.82 0.91 0.00 2.41
Alpaca → 7B 39.98 84.70 61.99 19.22 40.56 81.69 97.96 24.71 56.35
Alpaca noexpl 7B - 85.34 54.75 - - 83.20 91.10 - -
Table 13: Performance of additional instruction-tuned and fine-tuned models on the explainable formality style
transfer task.
preferred to use semantic similarity of the output to
the input (MIS) as the primary measure across all
tasks. Below is the result of an experiment we con-
ducted with perplexity (lower is better), following
Suzgun et al. (2022) exactly to compute the fluency
metric.
Model PPL Inf. → PPL For.→ PPL Bias →
For. Inf. Unbiased
gold 33.32 68.96 31.26
gpt1 31.24 21.39 28.21
gpt5 34.83 21.10 27.59
gpt10 32.55 23.82 27.62
best model 30.21 38.61 31.97
Table 14: Fluency evaluation results. GPT refers to the
teacher model, best model refers to Alpaca for formality
transfer and LLaMA for bias transfer.
As can be seen from Table 14, for formal and
unbiased paraphrase the perplexity is comparable
with the teacher in 1-shot and few-shot settings as
well as the gold reference, whereas for informal-
ity the gold data has the worst perplexity, making
this metric unfit when we want to transfer from
formal to informal style. We want to add that we
performed a human evaluation in Section 4.3 that
includes the evaluation of the transferred sentences.
Performance of PromptRerank Baseline We
considered including a PromptRerank baseline
(Suzgun et al., 2022) as it is one of the previous
SOTA approaches. On our observation the models
did not perform competitively possibly because the
models described in the paper are smaller than the
contemporary LLMs. Future work may explore
adjustments on this baseline to adapt it to the new
models. Performance is depicted in Table 15.
BLEU and BERTScore between transferred text
and gold references We opted for reference-free
metrics since they would be less biased towardproducing output that most closely matches the ref-
erence. However, we now ran the experiment with
BLEU and BERTScore between transferred text
(output) and gold reference from our e-GYAFC
and e-WNC datasets. BERTScore is based on
contextualized-embeddings and usually preferred
to using BLEU-4 (which is based on 4-gram over-
lap) for evaluation of text generation and para-
phrases. Results are shown below in Table 16.
We see that using BERTScore, the student model
is comparable to the teacher model and both are se-
mantically close to the reference. BLUE scores are
lower due to more stricter n-gram overlap require-
ments but that is across all models and scores are
again comparable between smaller student models
and the larger teacher models.
G Discussion on applications of models
fine-tuned for explainable style transfer
Explainable formal →informal style transfer
is an interpretable adversarial attack on AI-
generated text detection methods, including re-
trieval Krishna et al. (2023) established that
paraphrasing easily evades the detection of AI-
generated text, and proposed a retrieval-based de-
fense. However, we hypothesize that retrieval-
based metrics will degrade as similarity between
generations becomes more ambiguous, as is the
case for formality style transfer. For example, an
adversarial agent might generate a post containing
misinformation in typical “formal" language gener-
ated by a language model like ChatGPT. This text
is relatively detectable by current classifiers and
100% detectable by retrieval-based methods. How-
ever, the agent might apply a style transfer model
to lower the formality of the message. Alarmingly,
not only this accomplishes the goal of spreading
the AI-generated message more effectively as the
result looks more like user-generated text, but, as

--- PAGE 19 ---
Model MIS (Formal to Informality MIS (Informal to Formality
Informal) Formal)
Alpaca ↔ 81.76 66.71 79.43 98.57
promptRerank 63.33 33.11 62.70 20.00
Table 15: Model performance on style transfer tasks.
Model Formal →Informal Informal →Formal Bias →Unbiased
BLEU BERTScore BLEU BERTScore BLEU BERTScore
gpt1 15.01 78.40 28.10 84.29 60.34 91.30
gpt5 15.36 78.74 28.71 84.48 64.85 91.78
gpt10 14.23 78.14 29.13 84.62 65.98 92.13
best model 15.33 77.57 20.75 80.78 70.49 93.23
Table 16: Performance comparison of models on BLEU and BERTScore metrics with the gold reference. GPT
refers to the teacher model, best model refers to Alpaca ↔for formality transfer and LLaMA →for bias to unbiased
transfer.
we show, it also decreases the chances of being
detected as AI-generated by current methods.
We test this in the following setting: we use an
online dataset of political tweets11, and sample 30
of them. We ask ChatGPT to generate a political
commentary post on the topic of the tweet (GPT-
F), as well as an informal paraphrase of the said
post (GPT-Inf). We manually annotate the result-
ing summaries and select those that look like they
could be legitimate political messages posted on so-
cial media and have valid paraphrases. We then use
our Alpaca F→IFmodel to generate an informal para-
phrase of the GPT-F posts sentence-by-sentence.
We also verify that these paraphrases are semanti-
cally valid and close to the original GPT-Formal
post and select 24 high-quality generations. We
choose a relatively small sample since we want to
verify the paraphrase was still close to the original
sentence manually to ensure semantic control for
the experiment.
We report detection scores12from 4 methods
surveyed by Krishna et al. (2023): GPTZero (Tian,
2023), OpenAI classifier (OpenAI, 2023), Detect-
GPT (Mitchell et al., 2023), and their proposed
retrieval methods based on BM25 (Robertson and
Zaragoza, 2009) or P-SP (Wieting et al., 2022) re-
trievers. As can be seen in Table 17, the formal-
to-informal transfer model significantly decreases
detection scores of all AI-generated text detection
methods, including the retrieval-based one (despite
the fact that the retrieval corpus is significantly
11kaggle.com
12Since we do not have the human baseline text, we do
not report the performance at 1% FPR, but for our study it is
sufficient to show the scores decrease in absolute terms.smaller than it would be in real-world). Interest-
ingly, for the BM25 retrieval method, the Chat-
GPT paraphrases are slightly harder to detect than
Alpaca F→IF, whereas it is easier for all other meth-
ods. Since we used ChatGPT to generate the origi-
nal posts, we could not use the watermarking meth-
ods (Kirchenbauer et al., 2023), but this can be
explored in future work.
This result highlights the need to investigate new
methods of detecting style transferred AI-generated
text. As formality style transfer remains an effec-
tive attack, informality features produced by our
model could help improve such classifiers. We
leave this investigation for future work.
Models GPTZero OpenAI GPTDetect BM25 P-SP
GPT-F 85.92 70.64 104.88 100 100
GPT-Inf 69.58 54.24 65.42 48.15 74.99
F→IF 6.11 44.86 54.92 58.68 74.08
Table 17: Performance of various AI-generated text
detectors on informal paraphrases from our model. Even
retrieval methods perform poorly in this setting.
H E-EGYAFC Statistics
We provide the distribution of 50 most frequent
informal and formal attributes in E-GYAFC in Fig-
ures 5, 6.
IE-WNC Statistics
We provide a proportion of classes in E-WNC in
Table 18.

--- PAGE 20 ---
0 1000 2000 3000 4000
Frequencycontraction
colloquialism
textese
personal pronoun
slang
casual tone
informal punctuation
capitalization
informal vocabulary
abbreviation
simple sentence structure
informal tone
conversational tone
misspelling
emotive language
emphasis
ellipsis
sentence fragment
casual vocabulary
lack of precision
emoticon
casual language
conjunction
informal language
informal phrasing
incomplete sentence
use of contraction
repetition
use of personal pronoun
informal expression
vague language
direct question
phrasal verb
less strict grammar
imperative
exclamation
filler word
verb agreement
informal word choice
direct command
exclamation mark
omission of article
nonstandard syntax
pronoun
informal conjunction
imperative sentence
informal abbreviation
personal opinion
imprecise language
idiomatic expressionAttributesFigure 5: Distribution of 50 most frequent informal
attributes in the E-GYAFC dataset.
J Annotation protocols
The screenshots for explanation interfaces are pro-
vided below in Figures 7, 8, 9. Similar annotation
interfaces were used for the bias task.
K Annotator demographics
Annotators are part of a diverse demographic, geo-
graphically present in North America, Europe, and
Southeast Asia (as reported by Upwork). All an-
notators indicated at least fluent to native English
skill.
L AI assistants
The authors used AI assistants such as Co-Pilot and
ChatGPT for writing code.
0 1000 2000 3000 4000
Frequencyprecise language
lexical sophistication
absence of contraction
passive voice
formal vocabulary
impersonal pronoun
precise vocabulary
polite language
cautious language
complex sentence structure
neutral tone
avoidance of contraction
clear logical structure
conditional clause
use of passive voice
no contraction
formal tone
complete word
polite tone
sophisticated vocabulary
advanced vocabulary
indirect question
formal language
absence of colloquialism
precision
use of formal language
clear and precise language
use of modal verb
use of precise language
complete sentence structure
formal phrasing
use of impersonal pronoun
use of formal vocabulary
logical structure
person pronoun
impersonal language
polite request
indirect language
precise wording
complete sentence
absence of personal pronoun
use of adverb
formal expression
absence of slang
indirect question form
polite phrasing
full word
hedging
neutral language
specific languageAttributesFigure 6: Distribution of 50 most frequent formal at-
tributes in the E-GYAFC dataset.
Category (%)
Demographic 3.70
Epistemological 22.87
Framing 67.53
No Bias 5.90
Table 18: Proportion of classes in E-WNC

--- PAGE 21 ---
Figure 7: Annotation to gather feedback for ICLEF.

--- PAGE 22 ---
Figure 8: Annotation for eGYAFC data acceptability and preferences.

--- PAGE 23 ---
Figure 9: Annotation for model preferences.
