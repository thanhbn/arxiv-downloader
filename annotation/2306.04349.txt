# 2306.04349.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/annotation/2306.04349.pdf
# File size: 4077995 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
GPT Self-Supervision for a Better Data Annotator
Xiaohuan Pei, Yanxi Li, Chang Xu
School of Computer Science, Faculty of Engineering
The University of Sydney
xpei8318@uni.sydney.edu.au, yali0722@uni.sydney.edu.au, c.xu@sydney.edu.au
Abstract
The task of annotating data into concise summaries poses a significant challenge
across various domains, frequently requiring the allocation of significant time and
specialized knowledge by human experts. Despite existing efforts to use large
language models for annotation tasks, significant problems such as limited applica-
bility to unlabeled data, the absence of self-supervised methods, and the lack of
focus on complex structured data still persist. In this work, we propose a GPT self-
supervision annotation method, which embodies a generating-recovering paradigm
that leverages the one-shot learning capabilities of the Generative Pretrained Trans-
former (GPT). The proposed approach comprises a one-shot tuning phase followed
by a generation phase. In the one-shot tuning phase, we sample a data from the
support set as part of the prompt for GPT to generate a textual summary, which is
then used to recover the original data. The alignment score between the recovered
and original data serves as a self-supervision navigator to refine the process. In
the generation stage, the optimally selected one-shot sample serves as a template
in the prompt and is applied to generating summaries from challenging datasets.
The annotation performance is evaluated by tuning several human feedback reward
networks and by calculating alignment scores between original and recovered
data at both sentence and structure levels. Our self-supervised annotation method
consistently achieves competitive scores, convincingly demonstrating its robust
strength in various data-to-summary annotation tasks.
1 Introduction
Large language models represented by Generative Pre-trained Transformer(GPT) family [23, 24, 1]
have made breakthrough advancements in recent years, achieving state-of-the-art performance across
various deep learning tasks. Among these tasks, data annotation is the fundamental and indispensable
first step in the process of AI research, as it lays the groundwork by providing labeled data that
serves as the foundation for training and evaluating models [ 14,27,2,3]. The quality of data label
generation directly impacts all subsequent tasks, making it the cornerstone of the entire deep learning
process [12, 21].
The annotation task poses the significant challenges. First, the complexity of the data makes it
time-consuming for experts to annotate each sample [ 18,28,4,29]. For example, computational
graphs provide valuable insights into the design of neural network structures and annotating these
cells requires significant effort to identify and summarize similar sub-sequences as parallel blocks
[8], which is nearly impossible to identify and isolate. Second, the exclusive use of subjective human-
annotated summaries presents evaluation challenges [ 5,1]. Employing these subjective summaries
as supervised learning labels could lead to annotation tasks reflecting human bias, not the impartial
correlation between the data and its summary.
Recent research has harnessed the capabilities of large language models for supervised data annotation.
As revealed by [ 7,11,30], GPT models can annotate classification tasks through the direct design
Preprint. Under review.arXiv:2306.04349v2  [cs.CL]  8 Jun 2023

--- PAGE 2 ---
of diverse prompts. Furthermore, the study by [ 13] broadens the scope of this annotation task,
enabling the generation of dialogues. Despite impressive progress, the problems of annotation
task still remain. First, current studies are primarily centered around supervised annotation with
human-labeled data, which restricts the annotation method’s applicability to more general unlabeled
data. In most scenarios, human-labeled data is unavailable to guide the annotation process. Second,
prior research has been limited to the design of various prompts, without considering feedback
mechanisms to refine specific parts of the prompts. Third, there’s a lack of research focused on
annotating complex structured data. The current work only focuses on annotating simple natural
language description samples without challenging more complex structured data. For example, the
datasets of computational graphs extracted by neural networks consists various nested structure,
making it difficult for humans to annotate specific blocks within such complex lists of edges.
Large Language
ModelTemplate(Data)Update W eightsUpdate Data as a New Template
Data+
Figure 1: Data-centric based generating-recovering
paradim. Iterative update the template with the
generated data. xiis sampled from a candidate
support set and siis generated by GPT.Here we propose a GPT self-supervision ap-
proach via a generating-recovering loop, primar-
ily inspired by prompt tuning [ 15,16] and the
data-centric paradigm [ 22], as outlined in Figure
1. The framework of self-supervision method
contains the one-shot phase and the generation
phase. The one-shot phase aims to iteratively
find the optimal pairs of the data and summary as
the template. Starting with a human-annotated
data pair, our iterative process utilizes GPT to
generate summaries. The generated summary
and raw data form a new data pair. This new
data pair is then evaluated for its potential as a
template through a comparison of the recovered
data from the summary with the original. If both the support and validation scores improve, we
update the optimal template with the current new data pair. Thus, the self-alignment mechanism
iteratively tune the one-shot template for the next round generation. The searched optimal template is
subsequently used for generating the summaries on the generation phase. We tune various reward
models to evaluate summary quality and introduce various similarity metrics to assess the recovery
ability. We perform sufficient experiments on three challenging datasets and conduct detailed ablation
study from various perspectives. The results demonstrate our self-supervision paradigm consistently
yields competitive performance evaluated by reward models and recovery scores. Addtionally, we
apply our self-supervision method to generate two new datasets of 3k/15k summaries of the neural
network architectures based on the different computational operators.
2 Related Work
Large Language Model. GPT-1 [ 23] propose a two-step approach: pre-training on unlabeled text
and discriminative fine-tuning on specific tasks. Unlike previous methods, It employes task-aware
input transformations during fine-tuning, minimizing changes to the model architecture. GPT-2 [ 24]
demonstrates language models can perform down-stream tasks in a zero-shot setting – without any
parameter or architecture modification. GPT-3 [ 1] scales up language models to improve their ability
with minimal fine-tuning abn achieves strong few-shot performance without gradient updates or
task-specific fine-tuning.
One/Few-Shot Learning. Learning new concepts quickly with limited data is a challenge in machine
learning. Traditional supervised deep learning is not effective for this. Li et al. [9] use Bayesian
modeling and show that it’s possible to learn a lot about a category from just one or a few images by
leveraging knowledge from previously learned categories, regardless of their differences. Vinyals et
al.[26] define one-shot learning problems and combine metric learning and neural networks with
external memories to create a framework that can learn new concepts from just a few examples. The
framework doesn’t require fine-tuning and is tested on vision and language tasks. As aforementioned,
GPT-3 [ 1] is a powerful few-shot learner, exhibiting remarkable performance on various natural
language processing tasks, including translation, question-answering, and cloze tasks, without the
need for fine-tuning or gradient updates.
2

--- PAGE 3 ---
Template:
Data  = {"INPUT ->conv3, conv3-
>bn,bn->avgpool3, avgpool3-
>avgpool3, avgpool3->OUTPUT ,
INPUT ->OUTPUT"}
Summary  = The neural network
architecture consists of an input layer
followed by 2 block and 1 skip
connection, where each block has ......
  -  "->' represent the edge.
  -  'bn' is batch normalization.
  -  'avgpool-n' is average pooling.
  -  'maxpool-n' is max pooling.
  -  'sepconv-n' is separable convolution.
  -  'dilconv-n' is dilated convolution.
  -  'skipconnect" is skip connection.
...... 
return format is {Summary: }Instruction :
Data Summary 
Template:
Summary  = The neural network
architecture consists of an input layer
followed by 2 block and 1 skip
connection, where each block has ......
Data  = {"INPUT ->conv3, conv3-
>bn,bn->avgpool3, avgpool3-
>avgpool3, avgpool3->OUTPUT ,
INPUT ->OUTPUT"}
  -  "->' represent the edge.
  -  'bn' is batch normalization.
  -  'avgpool-n' is average pooling.
  -  'maxpool-n' is max pooling.
  -  'sepconv-n' is separable convolution.
  -  'dilconv-n' is dilated convolution.
  -  'skipconnect" is skip connection.
...... 
return format is {Recovered Data: }Instruction :
Summary Recovered
Data
Summary DataTemplate:
Data 
Summary  DataRecovered
DataGeneration
Recovery
Self-Supervision
Early StopAlignment ScoresFigure 2: GPT Self-Supervision Annotation: A framework of the generating-recovering paradigm,
where the objective function seeks to maximize the alignment scores between the original data and
the recovered data.
3 Approach
Our data annotation approach involves one-shot tuning stage andsummary generation stage .
One-shot tuning stage. This stage is mainly responsible for finding the optimal template t∗self-
supervised by GPT. It is an iterative process of generating a summary, recovering data, and comparing
feedback values to navigate the template tuning in the prompt, which is subsequently used for the
next round generation.
The iterative process contains the following steps. We first initialize a simple human-labeled pairs of
datax0, summary s0as the best template t∗={x0, s0}, and then respectively assign the role type of
{system, assistant, content} for the instruction, template, support data {w, t, x}. For iteration i, we
sample a supported data xifrom the support set X, which are subsequently concatenated with the
current optimal template t∗and default instruction wginto one message. The GPT F(·)covert the
message to generate summary siby referring the instruction, current template and data:
si← F (xi|ti, wg, θ), (1)
where the θis the parameters of the language model function and wgis the instruction of generating
a summary. At this point, we obtain a new paired set consisting of data and summary, denoted as
{xi, si}, where siis generated from xi.
Considering that a summary’s main purpose is to briefly capture the essence of a dataset, the quality
of a summary naturally can be deduced from its ability to faithfully reproduce the original dataset.
The recovering process is reconstructing ˆxifromsiby the same GPT:
ˆxi← F (si|ti, wr, θ), (2)
where the θis the parameters of the large language model and wgis the instruction of recovering
data. And then we apply sim(ˆxi, xi)to measure the similarity score between the recovered data and
original data. If the current score surpasses the previously recorded highest value from iterations, we
consider the current data-summary pairs { xi,si} as a temporary template.
Using the same generation and recovery process mentioned in 1 2, we evaluate the average similarity
score on the validation set. If this score remains higher than the maximum valid score observed
3

--- PAGE 4 ---
Algorithm 1 Self-Supervised Annotating by GPT.
1:Initialize: instructions wg/wrfor generating/recovering , optimal one-shot template t∗=
{x0, s0}, best support/valid similarity score simsup/simval= 0, a support set Xsupfor sam-
pling one-shot template, a validation set Xvalfor testing the new template performance.
2:foriteration i←1toIdo
3: (1) Sampling: xi← Xsup
4: (2) Encoding: Send the message < instruction, template, sampled data > to GPT:
5:
si←GPT (⟨wg, t∗, xi⟩)
6: Get the response of summary si
7: (3) Decoding: Send the message < instruction, template, generated summary > to GPT:
8:
ˆxi←GPT (⟨wr, t∗, si⟩)
9: Get the response of recovered data ˆxi.
10: Compute similarity score between recovered data and original data
11:
simsup
i←sim(xi,ˆxi)
12: (4) Update:
13: ifsimsup
i> simsupthen
14: Compute the average similarity score simval
ion the validation set Xvalby (2)(3).
15: ifsimval
i> simvalthen
16: Replace the best similarity scores with current score:
simsup←simsup
i;simval←simval
i
17: Update the optimal template with support data and generated summary:
t∗← {xi, si}
18: end if
19: end if
20:end for
21:Output: Return the best template t∗for the generation phase.
in previous iterations, the self-supervised mechanism update the best template t∗with the current
data-summary pairs { xi,si}. The process continues until the current sample cannot achieve a higher
recovery score compared to the previous iteration or when the maximum number of iterations is
achieved. And the self-supervised objective of above iterative updating is to find the best template
tithat maximizes the expected similarity between the recovered data and the original data. The
optimization problem can be formalized as follows:
t∗= arg max
tExi∼X[sim(ˆxi, xi)], (3)
where the sim(·)is the metric of similarity function between two sequence data. This objective
assumes that the similarity sim is a meaningful measure of the quality of the recovery process, and
that higher values of sim correspond to better recoveries. And this objective has a few notable
elements than traditional one-shot learning stage. Unlike conventional self-supervised method where
model weights are updated during training, our technique updates the current template instead.
Summary generation stage. In this stage, GPT concatenates the identified optimal template with
instructions, using it as the optimal prompt to generate natural language summaries for the generation
dataset:
s← F (x|t∗, wg, θ), (4)
where the sis the summarises of dataset x. We test the summary quality by tuning various human
feedback reward networks, and then we introduce the recovery evaluation aiming to measure whether
the summary could decoding the high-level sentences to original data. Specifically, we assume that
the stronger the ability to recover the intermediate summary back to the original data, the more it
substantiates the professionalism and accuracy of the summarises. Conversely, a weaker recovery
capability implies a lower degree of professionalism and accuracy in the summarises.
4

--- PAGE 5 ---
The evaluation phase of this process is two-fold. Firstly, we assess the quality of the summary by
employing a variety of human feedback reward networks which offers us a comprehensive insight
into the real-world applicability and understandability of our summaries. Following this, we institute
a recovery evaluation process that seeks to measure the efficiency and accuracy with which the
summary can decode high-level sentences back into the original data, thereby serving as an indicator
of the professionalism and accuracy of the summarises. This assumption underpins our belief that the
more capable the model is of reverse engineering the summary back to the original data, the more it
affirms the precision and professionalism of the summaries. Conversely, if the model demonstrates
a lower proficiency in this restoration task, it suggests that the summaries lack a certain degree of
professionalism and accuracy.
4 Experiments
4.1 Experiment Setup
Dataset This study utilizes three distinct datasets: Darts-Medium, Darts-Large, and PubMed. The
Darts-Medium and Darts-Large datasets consist of neural architecture networks, generated by 5
and 7 operators respectively, with Darts-Large having more nodes. They provide a rich source of
information on the design and performance of various neural architectures. We have also incorporated
the PubMed dataset which consists of Isomeric SMILES structures into our study. This dataset
represents a different domain, offering an opportunity to examine the generation of high-level
summaries for complex chemical structures. We take great care to avoid test set leakage and split
data for each stage to use. Each of these datasets has been divided into two segments for different
stages of our experiment: the one-shot phase and the generation phase. The one-shot phase involves
a support set and a validation set, each containing 50samples. These sets are utilized for finding
optimal template from the support set. The generation stage splits the data using a K-Fold ( K= 5)
method into training and testing sets. The training set is used to further train the model and adjust the
reward mechanism, while the test set is reserved for evaluating the summary quality.
Setup We implement a temperature hyperparameter of 1 to promote diversity in the summary
generation when evaluating baseline performance. Conversely, during the data generation stage, the
temperature was set to zero to ensure consistency and stability. To test the performance of different
large language models, we apply four widely used models in this experiment, composed of the version
3 of GPT: davinci, text-curie-001 and version 3.5 of GPT: text-davinci-003, gpt-3.5-turbo. We call the
response through the inference of the openAI official APIs1. The initialized information is divided
into instructions, templates and query data, respectively calling the role of system, assistant and user
in the information flow. We set 10iterations in each one-shot tuning phase with a defined maximum
token length of 350for generating summaries and 500for recovering datas from the summaries. Each
input prompt consists of three parts, an instruction, a template and a sampled data. We give the input
and output formats in the instruction, and define the meaning of each structured symbol in the data.
This part costs 500 tokens. For the template, we assign assistant and content tags to the data and
summary in the template, so that GPT can recognize that this is a template information, and this part
costs 3000 tokens. For sampling data from support set in the one-shot phase, we directly use the
role of user to send it to GPT. In order to make the generated summary more diverse in the one-shot
tuning stage, we set the hyperparameter of temperature to 1. In the data generation stage, we set the
temperature to 0 to keep it stable.
4.2 Evaluation
Summary Evaluation. To directly evaluate summary quality generated by our approach, we tuned
various human feedback reward models as our evaluators. These reward models have been widely
acknowledged in the literature for their effectiveness in providing evaluative feedback for language
generation tasks
1platform.openai.com/docs/models/
5

--- PAGE 6 ---
Table 1: Various evaluation scores ( ±stardard error) on the three datasets. The performance of each
model is evaluated using two types of evaluative metrics.
ModelGenerated Summary Evaluation Recovered Data Evaluation
R1 R2 R3 R4 BLEU ROUGE STS Sim Bert Sim
Darts-Medium
davinci 0.296±0.012 0.302±0.041 0.194±0.016 0.288±0.019 0.195±0.004 0.214±0.007 0.291±0.001 0.263±0.005
text-curie-001 0.243±0.028 0.211±0.013 0.297±0.004 0.342±0.013 0.118±0.004 0.172±0.008 0.310±0.006 0.294±0.009
text-davinci-003 0.532±0.023 0.596±0.028 0.503±0.032 0.582±0.031 0.278±0.006 0.379±0.017 0.772±0.003 0.543±0.025
gpt-3.5-turbo 0.513±0.011 0.642±0.016 0.519±0.015 0.639±0.017 0.482±0.023 0.422±0.004 0.816±0.014 0.691±0.002
Darts-Large
davinci 0.302±0.024 0.294±0.053 0.197±0.023 0.305±0.025 0.194±0.012 0.221±0.017 0.287±0.009 0.268±0.045
text-curie-001 0.252±0.038 0.220±0.023 0.305±0.011 0.370±0.021 0.129±0.014 0.182±0.022 0.314±0.013 0.308±0.019
text-davinci-003 0.509±0.034 0.607±0.021 0.515±0.011 0.580±0.027 0.292±0.005 0.382±0.013 0.781±0.001 0.559±0.009
gpt-3.5-turbo 0.544±0.020 0.672±0.037 0.537±0.021 0.657±0.033 0.505±0.030 0.447±0.009 0.829±0.019 0.715±0.006
PubMed
davinci 0.318±0.017 0.354±0.026 0.216±0.032 0.310±0.031 0.209±0.015 0.228±0.002 0.293±0.002 0.265±0.002
text-curie-001 0.262±0.013 0.230±0.021 0.327±0.018 0.372±0.026 0.138±0.012 0.192±0.001 0.316±0.001 0.310±0.001
text-davinci-003 0.547±0.019 0.611±0.022 0.514±0.036 0.594±0.023 0.571±0.008 0.403±0.001 0.774±0.004 0.571±0.002
gpt-3.5-turbo 0.542±0.022 0.671±0.031 0.547±0.023 0.667±0.012 0.509±0.004 0.457±0.001 0.796±0.001 0.635±0.002
Specifically, we tuned various human feedback reward models2345as our evaluators. Each evaluator
evaluate summary quality and provide a corresponding reward score ( R1, R2, R3, R4). To ensure
stability in the reward distribution, we employed a k-fold cross-validation strategy with a value of k
set to 5. Assume r(x, s|θ)represents the scalar output of the reward model for data xand summary
s, parameterized by θ. We performed fine-tuning on each pre-trained reward network by following
the steps outlined in [25]:
L=E(x,s0,s1,si)∼D[log(σ(r(x, si|θ))−σ(r(x, s 1−i|θ))], (5)
where i∈ {0,1}andDrepresent the human-labelled datasets containing judgments on which
summary, generated by two large language models, is superior. The equation delineated above
illustrates the loss function we used during this fine-tuning process. The reward process symbolize
the human-annotated datasets that provide judgments about the superiority of summaries generated
by two large language models. This dual evaluation not only adds a level of redundancy but also
ensures a more rigorous and comprehensive assessment of the generated summaries.
Recovery Evaluation. We implement both sentence-level alignments and embedding-level metrics
to assess the discrepancy between the recovered data and the original data.
For the sentence-level evaluation, we employed Average BLEU score[ 20] and ROUGE-L[ 17,10].
Specifically, we utilize a smoothed average version of BLEU in our evaluations to counteract the
issues that can arise with BLEU when dealing with short sentences. ROUGE-L, on the other hand, is
based on Longest Common Subsequence (LCS) statistics, which makes it a robust tool for evaluating
the quality of summaries, particularly in our case where it was applied for the evaluations.
When it comes to embedding-level metrics, we employ the Semantic Textual Similarity (STS) [ 19]
embedding and Bidirectional Encoder Representations from Transformers (BERT) [ 6] embedding.
The STS embedding, in particular, offers a quantifiable measure of the semantic equivalence between
two text pieces, which is ideal for assessing the semantic similarity between the original and recovered
data. On the other hand, the BERT embedding, which originates from the BERT model, allows us
to capture more nuanced semantic and syntactic features of the data structures, providing a more
comprehensive and insightful analysis of our generated summaries compared to the original data.
4.3 Baseline Results
We conducted experiments on three highly challenging datasets using four different models: davinci,
text-curie-001, text-davinci-003, gpt-3.5-turbo. The evaluation included testing the summary quality
by four specific reward networks and assessing the data recovery capability by alignment scores on
both sentence and embedding levels.
2gpt2-rlhf-reward
3reward-model-deberta-v3-large
4reward-model-deberta-v3-base
5chat-opt-350m-reward-deepspeed
6

--- PAGE 7 ---
Evaluate on the Recovery Data We first focus on the evaluation of the generated summary using
the four reward scores. These scores represent the performance of the generated summary quality
by various human feedback reward models after fine-tuning. On the Darts-Medium dataset, GPT-
3.5-Turbo outperforms the other models with the highest scores across all metrics: a BLEU score
of0.482±0.023, a ROUGE score of 0.422±0.004, a STS Sim score of 0.816±0.014, and a Bert
Sim score of 0.691±0.002. The Text-davinci-003 model followed next, while the Davinci and
Text-curie-001 models lag behind, with lower scores on all measures. Similar trends were observed
on the Darts-Large dataset. Once again, GPT-3.5-Turbo displays superior performance, achieving
the highest scores in all categories: BLEU ( 0.505±0.030), ROUGE ( 0.447±0.009), STS Sim
(0.829±0.019), and Bert Sim ( 0.715±0.006). Text-davinci-003 maintains its second place ranking,
while Davinci and Text-curie-001 trails with less impressive scores. On the PubMed dataset, the
Text-davinci-003 model remarkably achieved the highest BLEU score of 0.571±0.008, surpassing
the other models. However, GPT-3.5-Turbo still led the other metrics, with a ROUGE score of
0.457±0.001, a STS Sim score of 0.796±0.001, and a Bert Sim score of 0.635±0.002. Davinci
and Text-curie-001 continued to exhibit inferior performance compared to the other models. While
the performance varied somewhat across different datasets, GPT-3.5-Turbo consistently yields the
strongest results on the evaluated metrics. This strongly indicates its superior capability in data
recovery tasks. The performance gap observed between the models highlights the importance of
selecting the right transformer model for specific tasks and datasets, thereby optimizing the trade-off
between computational resources and performance.
Evaluate on the Summary Quality The results in Table 1 demonstrate that on the Darts-Medium
dataset, Text-davinci-003 displays the best overall performance in terms of R1 ( 0.532±0.023), R2
(0.596±0.028), and R3 ( 0.503±0.032) scores. However, GPT-3.5-Turbo achieves the highest R4
score ( 0.639±0.017). The other models, Davinci and Text-curie-001, have lower scores across
these metrics. On the Darts-Large dataset, GPT-3.5-Turbo outperformed the other models across
all reward metrics, with R1 ( 0.544±0.020), R2 ( 0.672±0.037), R3 ( 0.537±0.021), and R4
(0.657±0.033) scores. Text-davinci-003 follows closely behind, while Davinci and Text-curie-001
lag further in terms of performance. For the PubMed dataset, Text-davinci-003 demonstrate superior
performance in the R1 ( 0.547±0.019), R2 ( 0.611±0.022), and R3 ( 0.514±0.036) metrics, while
GPT-3.5-Turbo achieved the highest R4 score ( 0.667±0.012). As in the previous datasets, Davinci
and Text-curie-001 exhibit lower scores across these reward metrics. Based on the performance,
the evaluation on the reward models reveals that Text-davinci-003 and GPT-3.5-Turbo consistently
outperform the other models in terms of R1, R2, R3, and R4 scores. These findings emphasize the
importance of selecting the appropriate reward models for specific tasks and datasets, as they have a
significant impact on the quality of the generated summaries.
Comparison of Datasets Simultaneously, we observe that the annotation approach yields varying
results across different datasets. Firstly, it’s obvious that the complexity and characteristics of the
dataset have a significant impact on the performance of the models. For instance, on the Darts-
Medium dataset, while the GPT-3.5-Turbo model performs exceptionally well on the data recovery
metrics (BLEU, ROUGE, STS Sim, and Bert Sim), its performance in terms of the reward models
(R1, R2, R3, and R4) was surpassed by the Text-davinci-003 model. However, the scenario was
slightly different on the Darts-Large dataset, where GPT-3.5-Turbo outperforms the other models
in all evaluated metrics. This implies that the model was more adept at handling the increased
complexity and volume of this dataset. On the other hand, the models’ performance on the PubMed
dataset presents a more balanced records, where the Text-davinci-003 model surpasses others in terms
of the BLEU score and the R1, R2, and R3 reward model scores, while GPT-3.5-Turbo leads in the
other metrics.
4.4 Ablation Study
In the ablation study, we primarily aim to address four key questions for the proposed annotation
approach:
Q1.What role does the one-shot template play, and can the same effects be achieved via zero-shot
method — generating summaries only with designed instruction? Q2.If the template work in the
self-supervised annotation, what impact do various similarity measurement methods have on the
outcomes of one-shot tuning? Q3.How does the initialization of the template affect the results of
the optimal one in the one-shot tuning phase? Q4. Are the generated summary influenced by the
hyperparameters of the GPT model itself?
7

--- PAGE 8 ---
(a) Reward Scores on the
Darts-Medium Dataset.
(b) Recovery Scores on the
Darts-Medium Dataset.
(c) Reward Scores on the
PubMed Dataset.
(d) Recovery Scores on the
PubMed Dataset.
Figure 3: The role of the one-shot template navigation: a comparative analysis of one-shot vs.
zero-shot approaches. The red bars demonstrate initial the prompt with adding a one-shot template,
and the blue bars represent the scores conditinal on the same instruction without a one-shot template.
We use gpt-3.5-turbo to conduct the ablation study on both Darts and PubMed datasets.
Table 3: Impact of the initial template. From the view of the initial complexity of the data and the
initial summary quality.
Initial
TemplateData of 3 operators Data of 5 operators Data of 7 operators
Sum. High Sum. Low Sum. High Sum. Low Sum. High Sum. Low
Iteration 4 8 5 8 4 7
Similarity 0.653 0.652 0.641 0.655 0.652 0.659
A1. The optimal one-shot template improves the annotation performance compared to the zero-
shot generation conditional on the same instruction. To answer Q1, we employ both zero-shot
and one-shot approaches to investigate various models and two distinct domain-specific datasets.
The evaluation setting follows the steps illustrated in 4.2. Figure 3 demonstrates a comprehensive
comparison of the outcomes of these two generation methods, encompassing the quality of direct
generation and the precision of data recovery. We observe that the one-shot mechanism (red), which
incorporates template iteration, consistently surpasses the performance of zero-shot (blue) in terms of
annotating data quality, regardless of whether we evaluate the reward model or the similarity of the
recovered data. Moreover, we observe the standard deviation under different dataset partitions by
performing cross-validation during the generation phase. The figure displays smaller red error bars
compared to the blue ones, further illustrating that the template enhances the stability and robustness
of annotation quality.
Table 2: Impact of different alignment metrics be-
tween the Recovered Data and Original Data for
the One-Shot Tuning. RandSare the average
value of rewards scores and recovery scores.
STS BERT ROUGE BLEU Darts PubMed
R✓ 52.14 58.89
✓ 53.21 60.32
✓ ✓ 61.48 60.05
✓ ✓ ✓ 62.59 65.27
✓ ✓ ✓ ✓ 61.17 65.96
S✓ 21.18 19.04
✓ 26.19 25.20
✓ ✓ 29.44 39.55
✓ ✓ ✓ 45.59 40.04
✓ ✓ ✓ ✓ 44.05 41.38A2. Appropriately increasing the measure-
ment metrics benefits the annotation quality.
The feedback process of our proposed iterative
algorithm is based on the measure of similar-
ity between the recovered data and the origi-
nal data, as to the choice of similarity metrics
is essential. Considering that both the struc-
tured original data and the generated data are
sequence information, we apply the most widely
used schemes for measuring sequence similar-
ity, ranging from sentence-level measurement
methods (BLEU, ROUGE), to embedding-level
measurement techniques (STS, BERT). The ex-
perimental pipeline for this ablation study ad-
heres to the previous process. Table 2 provides the records of the testing scores by setting different
similarity alignments as the feedback value. In the first two rows of the Table 2, Initially, we tested the
effects of two different types of single similarity calculation functions on the annotated results. The
first two rows of each group in the Table 2 demonstrate this situation. Experimental records suggest
that sentence structure has a slightly more positive impact on the results. Moreover, by comparing
the last two rows (mixed metric functions) with the first two rows (single metric function) in each
group, we observe that whether we directly measure with the reward function or indirectly assess the
summary’s recovery ability, enhancing the diversity of metric functions can effectively improve the
quality of the annotated data.
A3. High-quality initial templates lead to quicker convergence, but the similarity scores of the
last iteration exhibit low variance. During the initial phase of one-shot tuning, we made observations
8

--- PAGE 9 ---
(a) T=0 of text-curie-001 on
Darts-Medium;
(b) T=1 of text-curie-001
on Darts-Medium;
(c) T=0 of gpt-3.5-turbo on
the Darts-Medium;
(d) T=1 of gpt-3.5-turbo on
the Darts-Medium;
(e) T=0 of davinci on the
Darts-Medium;
(f) T=1 of davinci on the
Darts-Medium;
(g) T=0 of text-davinci-003
on the Darts-Medium;
(h) T=1 of text-davinci-003
on the Darts-Medium;
(i) T=0 of text-curie-001 on
the PubMed;
(j) T=1 of text-curie-001 on
the PubMed;
(k) T=0 of gpt-3.5-turbo on
the PubMed;
(l) T=1 of gpt-3.5-turbo on
the PubMed;
(m) T=0 of davinci on the
PubMed;
(n) T=1 of davinci on the
PubMed;
(o) T=0 of text-davinci-003
on the PubMed;
(p) T=1 of text-davinci-003
on the PubMed;
Figure 4: The Impact of the model’s temperature. The red lines represent the iterative records of
similarity score by current support data and the blue lines trace the recovery scores on the validation
set. The arrow points the average number of the iteration to find the optimal template.
in the running records which indicated that the length and quality of the initialized templates had an
impact on the training process. To investigate this further, we designed three templates of the Darts
dataset with varying levels of complexity: Darts dataset generated by 3 operators, 5 operators, and 7
operators. Additionally, we artificially marked 2 types of summary qualities: Sim. High and Sim.
Low. This resulted in a total of 6 cases. To ensure consistent experimental conditions, each template
is applied 50 times during the one-shot stage. Moreover, the temperature of GPT was set to 0 to
maintain the stability of summary generation in each round. Table 3 demonstrates the influence of
six cases of various settings on the initialization. The polyline of the warm colour group represents
the iterative process of high-quality summaries. We can observe from this that its convergence time
is relatively early compared to the initial template that with lower-quality summaries. The score at
the algorithm termination clearly shows that the final generated optimal template possesses a close
similarity score, which indicates that initialization mainly affects the iteration number but does not
apply to the performance of the optimal template.
A4. The temperature of the model influences the iterative search for the optimal template. In
order to comprehensively investigate the impact of temperature hyperparameters on the performance
of different models, we conducted a series of experiments focusing on the convergence properties of
four baseline models under varying temperature settings in different environments. To ensure the
robustness of our findings, we performed each run 30 times, tracing the standard deviation of the
similarity values generated at each instance. Figure 4 presents the experimental outcomes for the
amalgamation of two temperature parameters across the four models. These results reveal a consistent
trend across all models: when the temperature is set to 0, the iterative search process terminates
9

--- PAGE 10 ---
prematurely at the third or fourth iteration. This suggests an inclination towards a limited exploration
space, leading to the generation of less diverse outputs. On the other hand, elevating the temperature
value to 1 during the one-shot tuning stage demonstrated an interesting outcome. Although each
iteration’s pace was reduced, this facilitated a broader array of alternatives for subsequent template
generation. This indicates an expansion of the exploration space, allowing for the generation of more
diverse and potentially creative solutions. We also observed that the final scores were higher when
the temperature was set to 1, as compared to 0. This finding indicates that the optimal templates
could be obtained by appropriately tuning the temperature hyperparameter.
5 Case Study
Here we provide two cases to elaborate how GPT self-supervised annotating the complex structured
data on the Darts and PubMed dataset. We first demonstrate the prompt for generating summary and
recovering data, and then we demonstrate the pipelines of our approach.
Prompts The generating prompt comprises three elements: encoding instruction, template, query
data, while the recovering prompt contains decoding instruction, template, query summary. Detailed
explanations of these instructions can be found in Appendix A.
Pipeline We use the case 5 on the Darts dataset to elaborate on the pipeline of our annotation
approach.
Step 1. Initialize a template composed of cells and a summary.
Step 2. Sample a set of cells from the support set.
Step 3. Concatenate the <encoding instruction, template, cells> into one message.
Step 4. Send the message to GPT and receive a summary response.
Step 5. Concatenate the <decoding instruction, template, summary> into one message.
Step 6. Send the message to GPT and receive a response of recovered cells.
Step 7. Compute the similarity between the recovered cells and original cells. If it is larger than the
previous record, update the score and treat the sampled data and generated summary as a temporary
template.
Step 8. Evaluate the temporary template on the valid set by repeating the above process.
Step 9. If the average scores on the validation dataset also surpass previous records, update the best
valid score and replace the current template with the temporary template.
Step 10. Iteratively repeat Step 2. toStep 9. until the maximum number of iterations is reached.
Generating Prompt
Template:
"Isomeric SMILE" = {{
C[C@@H](CC1=CC=CC=C1)N
}
}"Summary" = {
(S)-amphetamine is a 1-phenylpropan-2-amine
that has S conﬁguration. It has a role as a
neurotoxin.
}Instruction:  
        C Methane (CH4)
        CC Ethane (CH3CH3)
        C=C Ethene (CH2CH2)
        C#C Ethyne (CHCH)
        COC Dimethyl ether (CH3OCH3)
        CC=O Acetaldehyde (CH3-CH=O)
        ...
Query Support data :
C[C@]12CC[C@H]3[C@H]([C@@H]1CCC2=O)CCC4
=C3C=CC(=C4)OSystem
Assistant
UserRecovering Prompt
Template:
"Summary" = {{
(S)-amphetamine is a 1-phenylpropan-2-amine
that has S conﬁguration. It has a role as a
neurotoxin.
}
}"Isomeric SMILE" =
{C[C@@H](CC1=CC=CC=C1)N
}
Query Generated Summary :
The provided Isomeric SMILES represents a compound with
the following structure: a sulfur atom (S) is bonded to a
carbon atom (C) that is ... attached to a chiral carbon
([C@H]). Assistant
User
Generated Summary
The provided Isomeric SMILES
represents a compound with the
following structure: a sulfur atom (S) is
bonded to a carbon atom (C) that is ...
attached to a chiral carbon ([C@H]). Instruction:  
        C Methane (CH4)
        CC Ethane (CH3CH3)
        C=C Ethene (CH2CH2)
        C#C Ethyne (CHCH)
        COC Dimethyl ether (CH3OCH3)
        CC=O Acetaldehyde (CH3-CH=O)
        ...SystemRecovered Data
C [ C @] 1 2 C C [ C @H ] 3 [ C @H ]
( [ C @@H ] 1 C C C 2 =O ) C C C 4
=C 3 C =C C ( =C 4 ) O
Figure 6: PubMed dataset: A case of the generating-recovering annotation.
10

--- PAGE 11 ---
Generating Prompt
Template:
"Cells" = {{
"INPUT ->conv3, conv3->bn,bn->avgpool3,
avgpool3->avgpool3, avgpool3->OUTPUT ,
INPUT ->OUTPUT",
}
}"Summary" = {
The neural network architecture consists of an
input layer followed by 2 block and 1 skip
connection.
}Instruction:  
  -  "->' represent the edge.  
  -  'bn' is batch normalization.  
  -  'avgpool-n' is average pooling.  
  -  'maxpool-n' is max pooling.  
  -  'sepconv-n' is separable convolution.  
  -  'dilconv-n' is dilated convolution.  
  -  'skipconnect" is skip connection.
Query Support data :
'INPUT->sepconv33, sepconv33->avgpool331,
avgpool331->2maxpool331, 2maxpool331->bn, bn-
>OUTPUT, INPUT->OUTPUT' System
Assistant
UserRecovering Prompt
Template:
"Summary" = {{
The neural network architecture consists of an
input layer followed by 2 block and 1 skip
connection.
}
}"Cells" = {
"INPUT ->conv3, conv3->bn,bn->avgpool3,
avgpool3->avgpool3, avgpool3->OUTPUT ,
INPUT ->OUTPUT",
}
Query Generated Summary :
This neural network consists of a 3x3 convolution, followed
by several layers featuring average pooling, max pooling,
depthwise separable 3x3 convolutions, dilated 3x3
convolutions, and addition operations for connecting layers. Assistant
User
This neural network consists of a stem
with a 3x3 convolution, followed by
several layers featuring average pooling,
max pooling, depthwise separable 3x3
convolutions, dilated 3x3
convolutions, and addition operations for
connecting layers.Generated SummaryRecovered Data
'INPUT->sepconv33, sepconv33-
>avgpool331, avgpool331-
>2maxpool331, 2maxpool332->bn,
bn->OUTPUT, INPUT->OUTPUT'Instruction:  
  -  "->' represent the edge.  
  -  'bn' is batch normalization.  
  -  'avgpool-n' is average pooling.  
  -  'maxpool-n' is max pooling.  
  -  'sepconv-n' is separable convolution.  
  -  'dilconv-n' is dilated convolution.  
  -  'skipconnect" is skip connection.SystemFigure 5: Darts datasets: A case of the generating-recovering annotation.
6 Limitations
The constraints imposed by token limits restrict the quantity of templates we can employ, posing a
significant challenge when conducting experiments within a few-shot framework. Thus, there is a
necessary trade-off between the number of shot samples and the length of each sample.
7 Conclusion
This paper introduces a novel approach named GPT self-supervision annotation, which harnesses
the one-shot learning capabilities of GPT models to produce concise summaries and alleviate the
burden of time and specialized expertise required by human annotators when dealing with complex
structured data, such as graphs. Our approach consists of two phases: one-shot tuning and generation.
During the one-shot tuning stage, a support set and a validation set are created from the training data,
a template is selected from the support set and used as a prompt to generate a textual summary using
GPT models, and the same model is utilized to recover the original data from the generated summary,
with alignment scores being calculated for feedback and potential template modification. During
the generation stage, our approach employs a selected one-shot sample as a template to generate
summaries for challenging datasets. Both sentence-level (BLEU, ROUGE) and structure-level (STS,
BERT) alignment scores between the original and recovered data are assessed, which demonstrates
that our approach consistently achieves competitive evaluation scores. The results demonstrate the
effectiveness of GPT models in data-to-summary annotation tasks.
References
[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[2]Levi Cai, Nathan E McGuire, Roger Hanlon, T Aran Mooney, and Yogesh Girdhar. Semi-
supervised visual tracking of marine animals using autonomous underwater vehicles. Interna-
tional Journal of Computer Vision , pages 1–22, 2023.
[3]Haoyu Chen, Henglin Shi, Xin Liu, Xiaobai Li, and Guoying Zhao. Smg: A micro-gesture
dataset towards spontaneous body gestures for emotional stress state analysis. International
Journal of Computer Vision , 131(6):1346–1366, 2023.
[4]Ying Chen, Yifan Peng, Kai-Wei Chang, Mark Dredze, Aaron M. Cohen, William R. Hersh,
Iain J. Marshall, Aurélie Névéol, Pierre Zweigenbaum, Sijia Liu, Baotian Hu, Fei Li, and
11

--- PAGE 12 ---
Zhiyong Lu. Challenges and opportunities in automated coding of diagnosis and procedure in
healthcare. npj Digital Medicine , 4(1), November 2021.
[5]Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre,
and Mark Cieliebak. Survey on evaluation methods for dialogue systems. Springer , 2021.
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,
2018.
[7]Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq Joty, and Boyang Li. Is gpt-3 a
good data annotator? arXiv preprint arXiv:2212.10450 , 2022.
[8]Georgiana Cristina Dobre, Marco Gillies, and Xueni Pan. Immersive machine learning for
social attitude detection in virtual reality narrative games. Springer , 2022.
[9]Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE
transactions on pattern analysis and machine intelligence , 28(4):594–611, 2006.
[10] Kavita Ganesan. Rouge 2.0: Updated and improved measures for evaluation of summarization
tasks. arXiv preprint arXiv:1803.01937 , 2018.
[11] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd-workers for
text-annotation tasks. arXiv preprint arXiv:2303.15056 , 2023.
[12] Johannes Kopp, Dominik Kellner, Aldi Piroli, and Klaus Dietmayer. Tackling clutter in radar
data–label generation and detection using pointnet++. arXiv preprint arXiv:2303.09530 , 2023.
[13] Tiziano Labruna, Sofia Brenna, Andrea Zaninello, and Bernardo Magnini. Unraveling chatgpt:
A critical analysis of ai-generated goal-oriented dialogues and annotations. arXiv preprint
arXiv:2305.14556 , 2023.
[14] Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, and Mohammed Bennamoun. A survey
on deep learning techniques for stereo-based depth estimation. IEEE transactions on pattern
analysis and machine intelligence , 44(4):1738–1764, 2020.
[15] Brian Lester, Noah Constant, and Rami Al-Rfou. The power of scale for parameter-efficient
prompt tuning. In EMNLP , 2021.
[16] Brian Lester, Noah Constant, and Rami Al-Rfou. Guiding frozen language models with learned
soft prompts. Google AI Blog, 2022.
[17] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out: Proceedings of the ACL-04 workshop , volume 8, 2004.
[18] R Austin McEver, Bowen Zhang, Connor Levenson, ASM Iftekhar, and BS Manjunath. Context-
driven detection of invertebrate species in deep-sea video. International Journal of Computer
Vision , 131(6):1367–1388, 2023.
[19] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word
representations in vector space. arXiv preprint arXiv:1301.3781 , 2013.
[20] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics , pages 311–318, 2002.
[21] Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi
Yang, and Dipanjan Das. Totto: A controlled table-to-text generation dataset. arXiv preprint
arXiv:2004.14373 , 2020.
[22] N. Polyzotis and M. Zaharia. What can data-centric ai learn from data and ml engineering?
arXiv preprint arXiv:2112.06439 , 2021.
[23] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
[24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[25] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec
Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.
Advances in Neural Information Processing Systems , 33:3008–3021, 2020.
12

--- PAGE 13 ---
[26] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks
for one shot learning. Advances in neural information processing systems , 29, 2016.
[27] Kai-Fu Yang, Cheng Cheng, Shi-Xuan Zhao, Hong-Mei Yan, Xian-Shi Zhang, and Yong-Jie Li.
Learning to adapt to light. International Journal of Computer Vision , pages 1–20, 2023.
[28] Xinyu Yang, Tilo Burghardt, and Majid Mirmehdi. Dynamic curriculum learning for great ape
detection in the wild. International Journal of Computer Vision , pages 1–19, 2023.
[29] Jing Zhang, Min-Yen Kan, Kazunari Sugiyama, and Tat-Seng Chua. Scientific document
processing: challenges for modern learning methods. International Journal on Digital Libraries ,
32(2):1–38, May 2023.
[30] Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, and Gareth Tyson. Can chatgpt reproduce
human-generated labels? a study of social computing tasks. arXiv preprint arXiv:2304.10145 ,
2023.
13

--- PAGE 14 ---
A Appendix
Here we provide the encoding instruction and decoding instruction for generating and recovering on
the darts dataset in Figure 7.
Instruction for generating summary on the Darts dataset. 
You are an expert in the field of neural architecture search. Your task is to summarise the 
neural architectures based on the computation operators list. The summary will be used 
to recover the original operators list and your objective is to provide the summary that 
could maximize the ability to recover the operators. 
Graph edges is an edge list representation of a directed graph. The graph represents a 
neural network, where the nodes are operations, and the directed edges represent 
information flows with ''->''. ''18maxpool312'' means a max pooling operation with a 3x3 
filter and a stride of 1 applied in layer 19. 
The summary of the neural network architecture in natural language should include: 
    1. Each block composed with what operators. 
    
    2. Depth of each block and width of parallel blocks. 
    
    3. Pros/cons of the design based on the block structure. 
    
Your return format is a JSON dict: {Summary:  }. 
(a) Encoding instruction.
Instruction for recovering data on the Darts dataset. 
You are an expert in the field of neural architecture search. 
Your task is given a summary of the network's key features. 
The architecture is represented as an edge list where nodes are operations 
and edges represent information flow with '->'. 
The candidate operators for the nodes are 
INPUT, OUTPUT, bn, avgpool, maxpool, sepconv, dilconv, and linear. 
Separable convolution operator with 3x3 filter means `'sepconv33'` 
Layer 2, max pooling operator with 3x3 filter 1 stride means `'2maxpool331'` 
Your objective is to extract the key information from the provided summary 
to recover the cells that maximize the distance between the recovered cells, 
with the output format being a pure dict containing only the cells key. 
Your return format is a json dict: { Recovered Data:  }. 
(b) Decoding instruction.
Figure 7: Instructions for the annotation on the Darts dataset.
14

--- PAGE 15 ---
And we also provide the encoding instruction and decoding instruction for generating and recovering
on the PubMed dataset in Figure 8.
Instruction for generating summary on the PubMed dataset. 
You are a professional annotator of Isomeric SMILES based on the organic compound description. In 
SMILES, atoms are represented by their atomic symbols. 
The second letter of two-character atomic symbols must be entered in lower case. Each non-hydrogen 
atom is specified independently by its atomic symbol enclosed in square brackets, [ ] (for example, [Au] 
or [Fe]). Square brackets may be omitted for elements 
in the “organic subset” (B, C, N, O, P, S, F, Cl, Br, and I) if the proper number of “implicit” hydrogen 
atoms is assumed. “Explicitly” attached hydrogens and formal charges are always specified inside 
brackets. A formal charge is represented by one of the symbols + or -. Single, double, triple, and 
aromatic bonds are represented by the symbols, -, =, #, respectively. Single and aromatic bonds may be, 
and usually are, omitted. 
C Methane (CH4) 
CC Ethane (CH3CH3) 
C=C Ethene (CH2CH2) 
C#C Ethyne (CHCH) 
COC Dimethyl ether (CH3OCH3) 
CCO Ethanol (CH3CH2OH) 
CC=O Acetaldehyde (CH3-CH=O) 
C#N Hydrogen Cyanide (HCN) 
[C-]#N Cyanide anion 
I will provide a Isomeric SMILES you return a summary. 
Your return format is a json dict: {Summary:  } 
(a) Encoding instruction.
Instruction for recovering data on the PubMed dataset. 
You are a professional annotator of Isomeric SMILES based on the organic compound description. In 
SMILES, atoms are represented by their atomic symbols. 
The second letter of two-character atomic symbols must be entered in lower case. Each non-hydrogen 
atom is specified independently by its atomic symbol enclosed in square brackets, [ ] (for example, [Au] 
or [Fe]). Square brackets may be omitted for elements 
in the “organic subset” (B, C, N, O, P, S, F, Cl, Br, and I) if the proper number of “implicit” hydrogen 
atoms is assumed. “Explicitly” attached hydrogens and formal charges are always specified inside 
brackets. A formal charge is represented by one of the symbols + or -. Single, double, triple, and 
aromatic bonds are represented by the symbols, -, =, #, respectively. Single and aromatic bonds may be, 
and usually are, omitted. 
C Methane (CH4) 
CC Ethane (CH3CH3) 
C=C Ethene (CH2CH2) 
C#C Ethyne (CHCH) 
COC Dimethyl ether (CH3OCH3) 
CCO Ethanol (CH3CH2OH) 
CC=O Acetaldehyde (CH3-CH=O) 
C#N Hydrogen Cyanide (HCN) 
[C-]#N Cyanide anion 
I will provide a description, you return a Isomeric SMILES. 
The return format is json dict: {Isomeric SMILES: } 
(b) Decoding instruction.
Figure 8: Instructions for the annotation on the PubMed dataset.
15
