# 2306.00176.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/annotation/2306.00176.pdf
# Kích thước tệp: 325371 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Chú thích Tự động với AI Tạo sinh Đòi hỏi
Phải Xác thực
Nicholas Pangakis∗, Samuel Wolken†, và Neil Fasching‡
2 tháng 6, 2023
Tóm tắt
Các mô hình ngôn ngữ lớn tạo sinh (LLM) có thể là một công cụ mạnh mẽ để tăng cường
các quy trình chú thích văn bản, nhưng hiệu suất của chúng thay đổi tùy theo các nhiệm vụ chú thích
do chất lượng prompt, đặc thù của dữ liệu văn bản, và độ khó khái niệm. Vì những thách thức này sẽ
tồn tại ngay cả khi công nghệ LLM được cải thiện, chúng tôi cho rằng bất kỳ quy trình chú thích tự động
nào sử dụng LLM đều phải xác thực hiệu suất của LLM so với nhãn được tạo ra bởi con người. Để
đạt được mục tiêu này, chúng tôi phác thảo một quy trình làm việc để khai thác tiềm năng chú thích
của LLM một cách có nguyên tắc và hiệu quả. Sử dụng GPT-4, chúng tôi xác thực phương pháp
này bằng cách sao chép 27 nhiệm vụ chú thích trên 11 bộ dữ liệu từ các bài báo khoa học xã hội gần đây
trong các tạp chí có tác động cao. Chúng tôi thấy rằng hiệu suất LLM cho chú thích văn bản là đầy hứa hẹn
nhưng phụ thuộc rất nhiều vào cả bộ dữ liệu và loại nhiệm vụ chú thích, điều này củng cố
sự cần thiết phải xác thực trên cơ sở từng nhiệm vụ một. Chúng tôi cung cấp phần mềm dễ sử dụng
được thiết kế để thực hiện quy trình làm việc của chúng tôi và hợp lý hóa việc triển khai
LLM cho chú thích tự động.
∗Khoa Khoa học Chính trị, Đại học Pennsylvania
†Khoa Khoa học Chính trị và Trường Annenberg về Truyền thông, Đại học Pennsylvania
‡Trường Annenberg về Truyền thông, Đại học Pennsylvania
Đóng góp của tác giả: NP và SW phát triển mã nguồn, phân tích dữ liệu và viết bản thảo. NP, SW,
và NF thu thập dữ liệu và chỉnh sửa bản thảo.
Chúng tôi cảm ơn Yphtach Lelkes vì sự hỗ trợ và hướng dẫn của ông và Daniel Hopkins vì những nhận xét hữu ích của ông. Chúng tôi cũng
cảm ơn bất kỳ tác giả nào đã gửi cho chúng tôi dữ liệu của họ.arXiv:2306.00176v1  [cs.CL]  31 tháng 5 2023

--- TRANG 2 ---
1 Giới thiệu
Nhiều nhiệm vụ trong xử lý ngôn ngữ tự nhiên (NLP) phụ thuộc vào dữ liệu văn bản
được dán nhãn thủ công chất lượng cao để huấn luyện và xác thực. Tuy nhiên, quy trình chú thích
thủ công đặt ra những thách thức không tầm thường. Ngoài việc tốn thời gian và đắt đỏ,
các người chú thích con người, thường là công nhân cộng tác đám đông¹hoặc trợ lý sinh viên đại học,
thường gặp phải khoảng chú ý hạn chế, mệt mỏi, và thay đổi nhận thức về các loại khái niệm
cơ bản trong suốt quá trình dán nhãn (Grimmer và Stewart, 2013; Neuendorf, 2016). Khi dán nhãn
lượng lớn dữ liệu, những hạn chế này có thể dẫn đến dữ liệu văn bản được dán nhãn bị
không nhất quán và lỗi có thể không thể quan sát được và có tương quan—đặc biệt khi sử dụng
người mã hóa có nền tảng nhân khẩu học tương tự.
Để giải quyết những thách thức này, các nhà nghiên cứu gần đây đã khám phá tiềm năng
của các mô hình ngôn ngữ lớn tạo sinh (LLM), chẳng hạn như ChatGPT, để thay thế
người chú thích con người. LLM nhanh hơn, rẻ hơn, có thể tái tạo được, và không dễ bị
một số cạm bẫy trong chú thích con người. Tuy nhiên, các ứng dụng trước đây sử dụng LLM
với khả năng này đã cho ra kết quả hỗn hợp. Gilardi và cộng sự (2023) tuyên bố rằng
ChatGPT vượt trội hơn MTurkers trong nhiều nhiệm vụ chú thích khác nhau. Mặt khác,
Reiss (2023) thấy rằng LLM hoạt động kém trong chú thích văn bản và lập luận rằng
công cụ này nên được sử dụng một cách thận trọng. Nhiều nghiên cứu khác trình bày các phân tích
tương tự với kết quả và khuyến nghị khác nhau (ví dụ, He và cộng sự, 2023; Wang và cộng sự,
2021; Ziems và cộng sự, 2023; Zhu và cộng sự, 2023; Ding và cộng sự, 2022).
Mặc dù nghiên cứu hiện tại cung cấp những hiểu biết có giá trị về cả lợi ích và
những hạn chế tiềm năng của việc sử dụng LLM cho chú thích văn bản, không rõ liệu
các quy trình dán nhãn tự động được sử dụng trong những nghiên cứu này có thể được
áp dụng một cách tự tin cho các bộ dữ liệu và nhiệm vụ khác hay không, do chúng chỉ báo cáo
các số liệu hiệu suất tối thiểu (tức là chỉ độ chính xác hoặc thỏa thuận giữa người mã hóa) và
phân tích một số nhiệm vụ riêng biệt trên một số lượng nhỏ bộ dữ liệu. Hơn nữa, do sự phát triển
nhanh chóng của công nghệ LLM, có khả năng bất kỳ khuyến nghị nhị phân nào về năng lực
của LLM trong các nhiệm vụ một lần sẽ nhanh chóng trở nên lỗi thời với những tiến bộ LLM bổ sung.
Trong khi một số nghiên cứu cho đến nay đã áp dụng LLM cho một loạt bộ dữ liệu rộng hơn
(Ziems và cộng sự, 2023; Zhu và cộng sự, 2023), chúng chỉ thử nghiệm hiệu suất chú thích
LLM trên các bộ dữ liệu chuẩn công khai, phổ biến. Như vậy, những thử nghiệm này có khả năng
bị ảnh hưởng bởi nhiễm bẩn (Brown và cộng sự, 2020), có nghĩa là các bộ dữ liệu có thể
được bao gồm trong dữ liệu huấn luyện của LLM và hiệu suất mạnh có thể phản ánh việc ghi nhớ,
điều này sẽ không khái quát hóa cho các bộ dữ liệu và nhiệm vụ mới. Không có hướng dẫn rõ ràng
về quy trình làm việc được khuyến nghị để sử dụng những công cụ này một cách an toàn và hiệu quả,
các học giả và các nhà thực hành đều có thể triển khai loại công cụ này khi nó có hiệu suất
không tối ưu.
Lập luận chính của chúng tôi là một nhà nghiên cứu sử dụng LLM cho chú thích tự động
phải luôn xác thực hiệu suất của LLM so với một tập hợp con các nhãn được chú thích
bởi con người chất lượng cao.² Trong khi LLM có thể là một công cụ hiệu quả để tăng cường
quy trình chú thích, có những tình huống mà LLM không thể cung cấp kết quả chính xác
do các yếu tố như prompt không hiệu quả, dữ liệu văn bản nhiễu, và các nhiệm vụ chú thích khó.
Vì những thách thức này sẽ tồn tại ngay cả khi công nghệ LLM được cải thiện,³ các nhà nghiên cứu
tăng cường phương pháp chú thích văn bản của họ với LLM phải luôn xác thực trên cơ sở
từng nhiệm vụ một. Xác thực nghiêm ngặt có thể giúp các nhà nghiên cứu tạo ra các prompt
hiệu quả và xác định liệu chú thích LLM có khả thi cho các nhiệm vụ phân loại và bộ dữ liệu
của họ hay không. Để đạt được mục tiêu này, chúng tôi phác thảo quy trình làm việc được
khuyến nghị để khai thác tiềm năng của LLM cho chú thích văn bản một cách có nguyên tắc
và hiệu quả.
Trong bài báo này, chúng tôi đề xuất và thử nghiệm quy trình làm việc để tăng cường
và tự động hóa các
²Bằng chất lượng cao, chúng tôi đề cập đến các nhãn được tạo ra bởi các chuyên gia chuyên môn (không phải
trợ lý sinh viên đại học hoặc công nhân cộng tác đám đông) những người cẩn thận dán nhãn một tập hợp con
nhỏ dữ liệu. Như chúng tôi mô tả bên dưới, quy trình làm việc được khuyến nghị của chúng tôi chỉ yêu cầu
một phần nhỏ dữ liệu văn bản được dán nhãn khi so sánh với các quy trình hiện tại.
³Ví dụ, ngay cả khi công nghệ LLM tăng về độ tinh vi, con người vẫn có thể cung cấp các hướng dẫn prompt
mơ hồ, điều này có thể ảnh hưởng tiêu cực đến hiệu suất LLM trong một nhiệm vụ chú thích.
2

--- TRANG 3 ---
dự án chú thích với LLM. Chúng tôi xác thực phương pháp của mình và thử nghiệm khả năng
của LLM trên một loạt rộng các nhiệm vụ chú thích từ các bộ dữ liệu khác nhau, không công khai
sử dụng các số liệu hiệu suất phù hợp. Chúng tôi sử dụng LLM để sao chép 27 quy trình chú thích
khác nhau từ 11 bộ dữ liệu không công khai được sử dụng từ các bài báo gần đây được đăng
trong các ấn phẩm có tác động cao. Tổng cộng, chúng tôi đã phân loại hơn 200.000 mẫu văn bản
sử dụng LLM (tức là GPT-4).⁴
Các phát hiện của chúng tôi chỉ ra rằng hiệu suất LLM cho chú thích văn bản là đầy hứa hẹn
nhưng phụ thuộc rất nhiều vào cả bộ dữ liệu và loại nhiệm vụ chú thích. Trên tất cả các nhiệm vụ,
chúng tôi báo cáo độ chính xác trung vị là 0,850 và F1 trung vị là 0,707. Mặc dù có hiệu suất
tổng thể mạnh, chín trong số 27 nhiệm vụ có precision hoặc recall dưới 0,5, điều này củng cố
sự cần thiết của việc xác thực bởi nhà nghiên cứu. Với sự biến đổi về hiệu suất trên các nhiệm vụ,
chúng tôi xác định bốn trường hợp sử dụng khác nhau cho các quy trình chú thích tự động được
hướng dẫn bởi hiệu suất xác thực của LLM. Chúng bao gồm sử dụng LLM để kiểm tra chất lượng
dữ liệu được dán nhãn bởi con người, sử dụng LLM để xác định các trường hợp cần ưu tiên
để xem xét bởi con người, sử dụng LLM để sản xuất dữ liệu được dán nhãn để tinh chỉnh và
xác thực bộ phân loại có giám sát, và sử dụng LLM để phân loại toàn bộ kho ngữ liệu văn bản.
Ngoài việc khuyến nghị quy trình tiêu chuẩn để xác thực khi nào và cách sử dụng LLM,
chúng tôi cũng giới thiệu một số công cụ mới trong các quy trình chú thích tự động của mình.⁵
Đầu tiên, chúng tôi cung cấp phần mềm dễ sử dụng bằng Python được thiết kế để thực hiện
các phương pháp của chúng tôi và hợp lý hóa việc triển khai LLM cho chú thích tự động. Thứ hai,
chúng tôi cho thấy tính hữu ích của điểm nhất quán. Để đo lường mức độ nhất quán mà LLM
dự đoán nhãn của một mẫu văn bản cụ thể, chúng tôi phân loại lặp lại mỗi mẫu văn bản ở
nhiệt độ LLM là 0,6. Coi câu trả lời modal là nhãn LLM được dự đoán, chúng tôi xấp xỉ
một mức độ "nhất quán" trên mỗi phân loại LLM. Vì có
⁴Các học giả đã nêu lên mối quan ngại về việc dựa vào LLM trong nghiên cứu khoa học xã hội do sự tiến hóa
liên tục của phần mềm LLM, bản chất hộp đen của cách LLM xử lý truy vấn, và sự mơ hồ về dữ liệu huấn luyện
làm nền tảng cho LLM. LLM mã nguồn mở đại diện cho một giải pháp khả thi cho nhiều vấn đề này (Spirling, 2023).
Quy trình làm việc được phác thảo ở đây là bất khả tri LLM và có thể được điều chỉnh cho bất kỳ LLM mã nguồn mở nào.
⁵Mã nguồn có sẵn tại đây: https://github.com/npangakis/gpt_annotate
3

--- TRANG 4 ---
Bước 1: Nhà nghiên cứu tạo ra các
hướng dẫn cụ thể cho nhiệm vụ (tức là
một sách mã).
Bước 2: Sử dụng sách mã, các chuyên gia
chuyên môn chú thích tập hợp con ngẫu
nhiên các mẫu văn bản.
Bước 3: Sử dụng LLM để chú thích một
tập hợp con dữ liệu được dán nhãn bởi con
người sử dụng cùng một sách mã. Sau đó,
đánh giá hiệu suất bằng cách so sánh các
nhãn LLM với các nhãn con người.Bước 4: Nếu hiệu suất thấp, tinh chỉnh
sách mã để nhấn mạnh các phân loại sai.
Nếu cần thiết, lặp lại các bước 2 và 3 với
sách mã đã cập nhật.
Bước 5: Sử dụng sách mã cuối cùng, thử
nghiệm hiệu suất LLM trên các mẫu được
dán nhãn bởi con người còn lại.
Hình 1: Quy trình làm việc để tăng cường chú thích văn bản với LLM
tương quan mạnh giữa điểm nhất quán cao hơn và xác suất phân loại đúng, điểm nhất quán
là một cách hiệu quả để các nhà nghiên cứu xác định các trường hợp biên.

2 Quy trình làm việc và xác thực
Trong sự vắng mặt của hướng dẫn rõ ràng về quy trình làm việc được khuyến nghị để sử dụng
LLM cho các nhiệm vụ chú thích, các nhà nghiên cứu đối mặt với nguy cơ triển khai những
công cụ này mặc dù có hiệu suất kém. Được hiển thị trong Hình 1, chúng tôi đưa ra quy trình
làm việc năm bước để kết hợp LLM một cách đưa phán đoán con người lên hàng đầu, bao gồm
cơ hội để con người tham gia vào vòng lặp tinh chỉnh hướng dẫn, và đưa ra chỉ dẫn rõ ràng
về hiệu suất LLM với đầu tư tài nguyên tối thiểu trước. Chúng tôi thiết kế
4

--- TRANG 5 ---
quy trình làm việc này với hai động cơ: để xác thực hiệu suất LLM và để tinh chỉnh prompt
cho phân loại LLM khi có thể.
Như Hình 1 cho thấy, các nhà nghiên cứu trước tiên nên tạo ra một bộ hướng dẫn cụ thể
cho nhiệm vụ (tức là sách mã)⁶ và sau đó có ít nhất hai chuyên gia chuyên môn và một LLM
chú thích các mẫu văn bản giống nhau—với kích thước mẫu phụ thuộc vào loại nhiệm vụ và
sự mất cân bằng lớp.⁷ Quan trọng là, cả người mã hóa con người và LLM đều nên sử dụng
cùng một sách mã để chú thích, trong đó sách mã đóng vai trò là hướng dẫn prompt của LLM.⁸
Sau đó, nhà nghiên cứu nên đánh giá hiệu suất (tức là độ chính xác, recall, precision, và F1)
bằng cách so sánh các nhãn LLM được dự đoán với các nhãn con người. Như một bước đầu tiên,
các nhà nghiên cứu trước tiên nên có LLM chú thích một tập hợp con các mẫu văn bản được
dán nhãn bởi con người. Nếu hiệu suất LLM thấp trên tập hợp con này, các nhà nghiên cứu có thể
tinh chỉnh hướng dẫn sách mã bằng cách nhấn mạnh các phân loại sai (lặp lại quy trình này
nếu cần thiết).⁹ Cuối cùng, sử dụng sách mã đã cập nhật, các nhà nghiên cứu nên thử nghiệm
hiệu suất LLM trên các mẫu được dán nhãn bởi con người còn lại. Hiệu suất trên dữ liệu
được giữ lại này nên được sử dụng để xác định liệu LLM có thể được sử dụng hiệu quả
cho chú thích tự động hay không. Mã nguồn được cung cấp trong kho GitHub của chúng tôi
đưa ra một cách đơn giản và hiệu quả để thực hiện các quy trình được nêu trong
quy trình làm việc ở trên.
⁶Vì LLM phản hồi với các prompt ngôn ngữ tự nhiên, một sách mã rõ ràng là thiết yếu cho các nhiệm vụ chú thích LLM.
Nhìn chung, một sách mã chú thích nên phân định rõ ràng các khái niệm quan tâm. Một tài liệu phong phú trải dài
cả khoa học xã hội định tính và định lượng đưa ra hướng dẫn về cách phát triển sách mã và phân loại dữ liệu văn bản
dựa trên các khái niệm liên quan (ví dụ, Krippendorff, 2018; Crabtree và Miller, 1992; MacQueen và cộng sự, 1998).
⁷Dựa trên các phân tích của chúng tôi, số lượng mẫu được chú thích nên nằm trong khoảng từ 250 đến 1.250 mẫu văn bản
ngẫu nhiên. Đối với các nhiệm vụ chú thích liên quan đến việc xác định các trường hợp rất hiếm (ví dụ, các trường hợp
mà ít hơn 1 phần trăm mẫu văn bản được mã hóa là các trường hợp tích cực cho một chiều cụ thể), có thể cần thiết
phải dán nhãn đáng kể nhiều mẫu văn bản hơn bằng tay.
⁸Nếu con người và LLM dán nhãn dữ liệu sử dụng các sách mã khác nhau, có thể có khoảng cách khái niệm giữa
hai hướng dẫn chú thích.
⁹Sau khi cập nhật sách mã, các nhà nghiên cứu có thể muốn sử dụng sách mã đã cập nhật để phân loại lại cùng một
tập hợp con các trường hợp để đo lường mức độ thay đổi mà các chỉnh sửa của họ gây ra. Bài tập này là overfitting
cho một tập hợp con dữ liệu cụ thể và do đó không nên được coi là thước đo hiệu suất. Thay vào đó, nó đưa ra
một thước đo đơn giản về việc các chỉnh sửa sách mã ảnh hưởng đến việc ra quyết định của LLM như thế nào.
Nếu những thay đổi căn bản được thực hiện đối với sách mã, nhà nghiên cứu có thể muốn có con người dán nhãn
lại các mẫu văn bản.
5

--- TRANG 6 ---
Để xác thực quy trình làm việc được đề xuất, chúng tôi sử dụng LLM để sao chép 27 nhiệm vụ
chú thích trên 11 bộ dữ liệu. Để đảm bảo những nhiệm vụ này đại diện cho một loạt các nhiệm vụ
chú thích trong nghiên cứu khoa học xã hội đương đại, chúng tôi rút từ nghiên cứu được công bố
trong các ấn phẩm trải dài một phổ các ngành từ các ấn phẩm liên ngành (ví dụ, Science Advances
và Proceedings of the National Academy of Sciences) đến các tạp chí lĩnh vực có tác động cao
trong khoa học chính trị (ví dụ, American Political Science Review và American Journal of Political Science)
và tâm lý học (ví dụ, Journal of Personality and Social Psychology). Trong Phụ lục, Bảng A1
bao gồm danh sách đầy đủ các bài báo sao chép và Bảng A2 cung cấp mô tả ngắn gọn về
các nhiệm vụ chú thích trong những bài báo này.¹⁰ Các nhiệm vụ chú thích này bao gồm một
loạt rộng lớn các ứng dụng khoa học xã hội, từ việc xác định liệu các văn bản thời Chiến tranh Lạnh
có liên quan đến các vấn đề đối ngoại hoặc quân sự (Schub, 2022) đến việc phân tích các phản hồi
khảo sát mở để phân loại cách mọi người khái niệm hóa nơi niềm tin xuất phát từ đâu (Cusimano
và Goodwin, 2020).
Trong mỗi trường hợp, chúng tôi sao chép một nhiệm vụ chú thích sử dụng dữ liệu được dán nhãn
bởi con người từ nghiên cứu gốc làm sự thật cơ bản. Để tránh tiềm năng nhiễm bẩn, chúng tôi
dựa hoàn toàn vào các bộ dữ liệu được lưu trữ trong các kho lưu trữ dữ liệu được bảo vệ bằng
mật khẩu (ví dụ, Dataverse) hoặc các bộ dữ liệu được bảo mật thông qua liên hệ trực tiếp với
tác giả.¹¹ Bất cứ khi nào có thể, chúng tôi bắt đầu với sách mã chính xác được sử dụng trong
thiết kế nghiên cứu gốc. Nếu sách mã này không có sẵn, chúng tôi trích dẫn hoặc diễn giải
văn bản từ bài báo hoặc tài liệu bổ sung mô tả các khái niệm quan tâm.¹²
Trên tất cả 27 nhiệm vụ, chúng tôi chú thích hơn 200.000 mẫu văn bản một chút sử dụng
API GPT-4 của OpenAI. Tổng chi phí là khoảng 420 USD. Trung bình, một bộ dữ liệu với
1.000 mẫu văn bản
¹⁰Để tìm những bài báo này, chúng tôi tìm kiếm các tạp chí có tác động cao cho các bài báo thực hiện một số loại
quy trình chú thích thủ công. Nếu chúng tôi có thể thu được dữ liệu văn bản, chúng tôi sao chép tất cả các quy trình
chú thích từ các bài báo được công bố trong ba năm qua.
¹¹Để hài hòa loạt nhiệm vụ chú thích đa dạng này thành một khung chung để đánh giá, chúng tôi coi mỗi chiều
là một nhiệm vụ chú thích nhị phân riêng biệt. Do đó, nếu một bài báo bao gồm một nhiệm vụ phân loại với ba
nhãn tiềm năng, chúng tôi chia quy trình chú thích thành ba nhiệm vụ phân loại nhị phân riêng biệt.
¹²Chúng tôi không quan sát được bất kỳ mối quan hệ nào giữa hiệu suất LLM và việc liệu sách mã trực tiếp có
sẵn hay không.
6

--- TRANG 7 ---
mất khoảng 2–3 giờ để hoàn thành bảy vòng lặp (xem phần "Điểm nhất quán" bên dưới).
Cùng nhau, chi phí thấp và tốc độ tương đối nhanh chóng thể hiện giá trị tiềm năng của
chú thích tăng cường LLM cho nhiều nhiệm vụ phân tích văn bản khoa học xã hội.

3 Kết quả
Kết quả phân loại được hiển thị trong Bảng 1. Các kết quả được báo cáo ở đây dựa trên
các mẫu văn bản "được giữ lại" (tức là không phải các mẫu văn bản được sử dụng trong
quy trình cập nhật sách mã từ Bước 4 của quy trình làm việc). Trên 27 nhiệm vụ, hiệu suất
phân loại LLM đạt được điểm F1 trung vị là 0,707. Hình 2 cho thấy hiệu suất về precision
và recall cho mỗi nhiệm vụ phân loại. Như rõ ràng trong hình này, hiệu suất phân loại LLM
mạnh hơn về recall so với precision cho 20 trong số 27 nhiệm vụ. Trên tám trong số 27 nhiệm vụ,
LLM đạt được hiệu suất mạnh đáng kể với precision và recall đều vượt quá 0,7.

Số liệu         Tối thiểu    Phân vị 25    Trung bình    Trung vị    Phân vị 75    Tối đa
Độ chính xác    0,674        0,808         0,855         0,85        0,905         0,981
Precision       0,033        0,472         0,615         0,650       0,809         0,957
Recall          0,25         0,631         0,749         0,829       0,899         0,982
F1              0,059        0,557         0,660         0,707       0,830         0,969

Bảng 1: Hiệu suất phân loại LLM trên 27 nhiệm vụ từ 11 bộ dữ liệu.
Mặc dù có hiệu suất tổng thể mạnh, chín trong số 27 nhiệm vụ có precision hoặc recall
dưới 0,5—và ba nhiệm vụ có cả precision và recall dưới 0,5. Do đó, đối với một phần ba
đầy đủ các nhiệm vụ, LLM hoặc bỏ lỡ ít nhất một nửa các trường hợp thực sự tích cực,
có nhiều false positive hơn true positive, hoặc cả hai. Như được hiển thị trong Bảng 2,
hiệu suất tổng hợp dao động thấp đến điểm F1
7

--- TRANG 8 ---
0.000.250.500.751.00
0.00 0.25 0.50 0.75 1.00
PrecisionRecallHình 2: Precision và recall cho mỗi trong 27 nhiệm vụ phân loại được sao chép. Màu sắc phản ánh
bộ dữ liệu, do đó các điểm chia sẻ cùng màu được thực hiện trên cùng dữ liệu văn bản.
8

--- TRANG 9 ---
là 0,06. Hơn nữa, hiệu suất LLM biến đổi đáng kể trên các nhiệm vụ trong một bộ dữ liệu duy nhất.
Trong ví dụ cực đoan nhất, F1 dao động từ 0,259 đến 0,811 trên hai nhiệm vụ riêng biệt trong
Card và cộng sự (2022), một sự khác biệt là 0,552. Những kết quả này thể hiện sự biến đổi
của chú thích LLM và, theo đó, nhấn mạnh sự cần thiết của việc xác thực cụ thể theo nhiệm vụ.

3.1 Điểm nhất quán
Bằng cách tạo ra tính ngẫu nhiên trong LLM thông qua việc sử dụng tham số siêu nhiệt độ
và bằng cách lặp lại nhiệm vụ chú thích, chúng ta có thể tạo ra một thước đo thực nghiệm
về phương sai trong nhãn mà chúng ta gọi là "điểm nhất quán". Chúng tôi khuyến nghị
nhà nghiên cứu có LLM phân loại mỗi mẫu ít nhất ba lần với nhiệt độ trên 0.¹³ Đối với
các phân tích của chúng tôi, chúng tôi sử dụng nhiệt độ 0,6 để dán nhãn mỗi mẫu văn bản
tối thiểu bảy lần với cùng một sách mã.¹⁴ Như được hiển thị trong Hình 3, điểm nhất quán
có tương quan với độ chính xác, tỷ lệ true positive, và tỷ lệ true negative. Cho một vector
phân loại, C, với độ dài l cho một nhiệm vụ phân loại nhất định, tính nhất quán được đo
như tỷ lệ phân loại khớp với phân loại modal (1/l ∑ᵢ₌₁ˡ Cᵢ == Cₘₒₐₑ). Trên các nhiệm vụ,
độ chính xác cao hơn 19,4 điểm phần trăm cho các mẫu văn bản được dán nhãn với điểm
nhất quán 1,0 so với những mẫu được dán nhãn với điểm nhất quán dưới 1,0. Tỷ lệ true positive
và tỷ lệ true negative cao hơn 16,4 điểm phần trăm và 21,4 điểm phần trăm, tương ứng,
cho các phân loại hoàn toàn nhất quán. Trong tất cả các mẫu được dán nhãn, 85,1% có
điểm nhất quán 1,0. Do đó, điểm nhất quán đưa ra một cách hữu ích để xác định các
trường hợp biên hoặc các chú thích khó khăn hơn.
¹³Nhiệt độ là một tham số siêu LLM cho biết mức độ đa dạng được đưa vào trên các phản hồi LLM.
Nó dao động từ 0 đến 1.
¹⁴Trong khi lựa chọn 0,6 của chúng tôi là tùy ý, chúng tôi đã xác nhận rằng chú thích nhiều mẫu văn bản
lặp lại trả về kết quả tốt hơn một phân loại duy nhất ở nhiệt độ 0. Nghiên cứu tương lai có thể thử nghiệm
cài đặt nhiệt độ nào trả về kết quả tối ưu.
9

--- TRANG 10 ---
50%60%70%80%90%100%
0.5 0.6 0.7 0.8 0.9 1.0
Điểm Nhất quánXác suất Phân loại ĐúngĐộ chính xác Tỷ lệ True Negative Tỷ lệ True PositiveHình 3: Mối quan hệ giữa điểm nhất quán và độ chính xác, TPR, và TNR.

3.2 Cập nhật sách mã
Đối với mỗi trong 27 nhiệm vụ phân loại, chúng tôi tuân theo quy trình làm việc được
nêu trước đây. Trong các bước 3 và 4 của quy trình làm việc, chúng tôi thử nghiệm hiệu suất
phân loại LLM trên một tập hợp con dữ liệu và sau đó, nếu có liên quan, thực hiện các cập nhật
sách mã lặp đi lặp lại có con người trong vòng lặp để tối ưu hóa prompt
10

--- TRANG 11 ---
cho hiệu suất phân loại LLM. Bước này có thể được coi là một ứng dụng của "kỹ thuật prompt"
trong đó nhà nghiên cứu cố gắng xác định các mẫu trong việc phân loại sai của LLM và
thay đổi sách mã để sửa chữa bất kỳ nhận thức sai lầm nhất quán nào. Đối với mỗi nhiệm vụ,
chúng tôi thực hiện tối đa một vòng cập nhật sách mã. Để đo lường hiệu ứng mà việc cập nhật
sách mã có trên việc dán nhãn LLM, chúng tôi dán nhãn lại các tập hợp con dữ liệu huấn luyện
sử dụng các sách mã cuối cùng.
Hình 4 cho thấy phân phối của sự thay đổi trong các số liệu hiệu suất sau khi cập nhật
sách mã. Phân tích này thể hiện liệu và cách quy trình cập nhật sách mã ảnh hưởng đến
chú thích LLM, giữ nguyên dữ liệu và các loại khái niệm. Trong hầu hết các trường hợp,
quy trình cập nhật sách mã dẫn đến cải thiện khiêm tốn về độ chính xác và F1. Recall giảm
trong nhiều trường hợp hơn là cải thiện sau khi cập nhật sách mã. Mặt khác, Precision cải thiện
trong đa số trường hợp, thúc đẩy cải thiện về độ chính xác và F1. Với những kết quả này,
các tinh tế của việc xây dựng prompt dường như không phải là một đòn bẩy đáng kể về hiệu suất.
Tuy nhiên, mặc dù mức độ cải thiện thường nhỏ, các nhà nghiên cứu gặp phải hiệu suất
phân loại LLM không đạt yêu cầu trên dữ liệu văn bản của họ có thể sử dụng tinh chỉnh
sách mã có con người trong vòng lặp để đảm bảo rằng hướng dẫn của họ không đáng trách.
11

--- TRANG 12 ---
Precision RecallĐộ chính xác F1
−0.4 −0.2 0.0 0.2 −0.4 −0.2 0.0 0.20369
0369
Thay đổi trong số liệu hiệu suất sau khi cập nhật sách mãMật độHình 4: Thay đổi trong hiệu suất chú thích LLM trên dữ liệu huấn luyện sau một vòng cập nhật sách mã.

4 Trường hợp sử dụng
Nếu LLM đạt được hiệu suất thỏa đáng trên các chuẩn mực cụ thể theo lĩnh vực cho một
nhiệm vụ chú thích nhất định, có nhiều cách mà LLM có thể được tích hợp vào một dự án
phân tích văn bản. Cách các nhà nghiên cứu tăng cường dự án của họ với LLM có thể phụ thuộc
vào hiệu suất LLM so với dữ liệu được dán nhãn bởi con người, ngân sách của họ, kích thước
của bộ dữ liệu, và sự có sẵn của các người chú thích con người. Bảng 2 hiển thị bốn trường hợp
sử dụng tiềm năng.
Nếu hiệu suất của LLM trên một số hoặc tất cả các chiều kém, thì nhà nghiên cứu có thể
tiếp tục tinh chỉnh sách mã sử dụng dữ liệu được dán nhãn mới hoặc từ bỏ việc sử dụng
LLM cho các chiều có hiệu suất không thỏa đáng. Nếu các nhà nghiên cứu chọn tiếp tục
sử dụng LLM mặc dù hiệu suất ban đầu kém, một chiến lược để đơn giản hóa các chú thích
phức tạp có thể là phân tách. Ví dụ, nếu một
12

--- TRANG 13 ---
Trường hợp sử dụng                  Mô tả
1) Xác nhận chất lượng dữ          Nếu một nhà nghiên cứu có dữ liệu đã được dán nhãn
liệu được dán nhãn bởi con          bởi các người chú thích con người, LLM có thể tăng cường
người                               quy trình phân tích văn bản bằng cách xác nhận chất lượng
                                   của các nhãn con người. Nếu hiệu suất LLM so với các
                                   nhãn con người cao, điều này báo hiệu rằng cả LLM và
                                   con người đều đưa ra quyết định chú thích tương tự. Nếu
                                   hiệu suất LLM thấp, điều này chỉ ra rằng con người, LLM,
                                   hoặc cả hai đều mắc lỗi trong quá trình chú thích.

2) Xác định các trường hợp         Một nhà nghiên cứu có thể xem xét thủ công các chú thích
cần ưu tiên để xem xét bởi         với điểm nhất quán dưới 1.0. Ngoài ra, nếu nhiệm vụ
con người                          LLM cụ thể đạt được recall cao, thì nhà nghiên cứu có thể
                                   sử dụng LLM để xác định các trường hợp tích cực tiềm
                                   năng trong dữ liệu chưa từng thấy trước đây. Sau đó,
                                   các người chú thích con người có thể xem xét thủ công
                                   tất cả các trường hợp được dán nhãn tích cực.

3) Sản xuất dữ liệu được dán       Có nhiều tình huống khác nhau mà một nhà nghiên cứu
nhãn để tinh chỉnh và xác          có thể sử dụng LLM để mua dữ liệu huấn luyện để tinh
thực một bộ phân loại có           chỉnh bộ phân loại có giám sát để dán nhãn kho ngữ liệu
giám sát                           của họ.

4) Phân loại toàn bộ kho           Trong trường hợp đơn giản nhất, một nhà nghiên cứu có
ngữ liệu trực tiếp                 thể thấy hiệu suất LLM thỏa đáng và chọn sử dụng LLM
                                   để phân loại toàn bộ kho ngữ liệu còn lại.

Bảng 2: Trường hợp sử dụng cho chú thích tự động với LLM.
13

--- TRANG 14 ---
nhà nghiên cứu thấy hiệu suất chú thích kém trên chiều "phát biểu thù hận", họ có thể đạt được
hiệu suất tốt hơn bằng cách phân tách "phát biểu thù hận" thành các chỉ báo thành phần
(chẳng hạn như đe dọa, lời lăng mạ, khuôn mẫu, v.v.) và thêm mỗi chỉ báo riêng biệt làm
chiều mới vào sách mã, sau đó bắt đầu lại ở Bước 1 trong quy trình làm việc.

5 Kết luận
Chúng tôi quan sát sự không đồng nhất đáng kể trong hiệu suất LLM trên một loạt
các nhiệm vụ chú thích khoa học xã hội. Hiệu suất phụ thuộc vào cả thuộc tính của văn bản
và các loại khái niệm được đo lường. Ví dụ, liệu một nhiệm vụ phân loại là một câu hỏi
thực tế về một mẫu văn bản hay yêu cầu phán đoán có ảnh hưởng đáng kể đến chiến lược
phân loại (Balagopalan và cộng sự, 2023). Để giải quyết vô số nguồn biến đổi trong hiệu suất
chú thích, chúng tôi khuyến nghị một phương pháp linh hoạt đối với chú thích tăng cường
LLM đưa chú thích con người lên hàng đầu.
Để giải quyết những thách thức này, chúng tôi trình bày một quy trình làm việc gốc để
tăng cường chú thích văn bản với LLM cùng với một số trường hợp sử dụng cho quy trình
làm việc này. Chúng tôi xác thực quy trình làm việc bằng cách sao chép 27 nhiệm vụ chú thích
được lấy từ 11 bài báo khoa học xã hội được công bố trong các tạp chí có tác động cao.
Chúng tôi thấy rằng LLM có thể cung cấp các nhãn chất lượng cao trên nhiều loại nhiệm vụ
với một phần chi phí và thời gian của các lựa chọn thay thế, chẳng hạn như công nhân
cộng tác đám đông và trợ lý nghiên cứu sinh viên đại học. Tuy nhiên, điều bắt buộc là
các nhà nghiên cứu phải xác thực hiệu suất của LLM trên cơ sở từng nhiệm vụ một, vì chúng tôi
thấy sự không đồng nhất đáng kể trong hiệu suất, thậm chí trên các nhiệm vụ trong một
bộ dữ liệu duy nhất.
14

--- TRANG 15 ---
Tài liệu tham khảo
Balagopalan, A., Madras, D., Yang, D. H., Hadfield-Menell, D., Hadfield, G. K., và Ghassemi,
M. (2023). Judging facts, judging norms: Training machine learning models to judge humans
requires a modified approach to labeling data. Science Advances, 9(19):eabq0701.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R.,
Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., và Amodei,
D. (2020). Language models are few-shot learners. Trong Larochelle, H., Ranzato, M., Hadsell, R.,
Balcan, M., và Lin, H., editors, Advances in Neural Information Processing Systems, volume 33,
pages 1877–1901. Curran Associates, Inc.
Card, D., Chang, S., Becker, C., Mendelsohn, J., Voigt, R., Boustan, L., Abramitzky, R., và
Jurafsky, D. (2022). Computational analysis of 140 years of us political speeches reveals more
positive but increasingly polarized framing of immigration. Proceedings of the National Academy
of Sciences of the United States of America, 31.
Chmielewski, M. và Kucker, S. C. (2020). An mturk crisis? shifts in data quality and the impact
on study results. Social Psychological and Personality Science, 11(4):464–473.
Crabtree, B. F. và Miller, W. L. (1992). A template approach to text analysis: Developing and
using codebooks. Trong Doing qualitative research. Sage Publications.
Cusimano, C. và Goodwin, G. P. (2020). People judge others to have more voluntary control over
beliefs than they themselves do. Journal of Personality and Social Psychology, 119.
Ding, B., Qin, C., Liu, L., Bing, L., Joty, S., và Li, B. (2022). Is gpt-3 a good data annotator?
Douglas, B. D., Ewell, P. J., và Braue, M. (2023). Data quality in online human-subjects research:
Comparisons between mturk, prolific, cloudresearch, qualtrics, and sona. PLoS One, 18.
Gilardi, F., Alizadeh, M., và Kubli, M. (2023). Chatgpt outperforms crowd-workers for text-
annotation tasks.
Grimmer, J. và Stewart, B. M. (2013). Text as data: The promise and pitfalls of automatic content
analysis methods for political texts. Political Analysis, 21(3):267–297.
He, X., Lin, Z., Gong, Y., Jin, A.-L., Zhang, H., Lin, C., Jiao, J., Yiu, S. M., Duan, N., và Chen,
W. (2023). Annollm: Making large language models to be better crowdsourced annotators.
15

--- TRANG 16 ---
Kennedy, B., Atari, M., Davani, A. M., Yeh, L., Omrani, A., Kim, Y., Coombs, K., Havaldar, S.,
Portillo-Wightman, G., Gonzalez, E., Hoover, J., Azatian, A., Hussain, A., Lara, A., Olmos, G.,
Omary, A., Park, C., Wijaya, C., Wang, X., Zhang, Y., và Dehghani, M. (2022). Introducing
the gab hate corpus: defining and applying hate-based rhetoric to social media posts at scale.
Lang Resources & Evaluation.
Krippendorff, K. (2018). Content Analysis: An Introduction to Its Methodology. Sage, 4 edition.
MacQueen, K. M., McLellan, E., Kay, K., và Milstein, B. (1998). Codebook development for
team-based qualitative analysis. CAM Journal, 10(2):31–36.
Neuendorf, K. A. (2016). The Content Analysis Guidebook. Sage Publications.
Reiss, M. (2023). Testing the reliability of chatgpt for text annotation and classification: A cau-
tionary remark. Working paper.
Saha, P., Narla, Kalyan, K., và Mukherjee, A. (2023). On the rise of fear speech in online social
media. Proceedings of the National Academy of Sciences of the United States of America.
Schub, R. (2022). Informing the leader: Bureaucracies and international crises. American Political
Science Review, 116.
Spirling, A. (2023). Why open-source generative ai models are an ethical way forward for science.
Nature, 413.
Wang, S., Liu, Y., Xu, Y., Zhu, C., và Zeng, M. (2021). Want to reduce labeling cost? GPT-3
can help. Trong Findings of the Association for Computational Linguistics: EMNLP 2021, pages
4195–4205, Punta Cana, Dominican Republic. Association for Computational Linguistics.
Zhu, Y., Zhang, P., Haq, E.-U., Hui, P., và Tyson, G. (2023). Can chatgpt reproduce human-
generated labels? a study of social computing tasks.
Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., và Yang, D. (2023). Can large language
models transform computational social science? Working paper.
16

--- TRANG 17 ---
Phụ lục
Tác giả                  Tiêu đề                                          Tạp chí                     Năm
Gohdes                  Repression Technology: Internet Ac-              American Journal of         2020
                        cessibility and State Violence                    Political Science
Hopkins, Lelkes,        The Rise of and Demand for Identity-            American Journal of         (R&R)
and Wolken             Oriented Media Coverage                          Political Science
Schub                   Informing the Leader: Bureaucracies             American Political          2022
                        and International Crises                         Science Review
Busby, and             Framing and blame attribution in pop-            Journal of Politics         2019
Gubler, Hawkins        ulist rhetoric
Müller                  The Temporal Focus of Campaign                   Journal of Politics         2021
                        Communication
Cusimano and           People judge others to have more vol-            Journal of Personality      2020
Goodwin                untary control over beliefs than they            and Social Psychology
                        themselves do
Yu and Zhang           The Impact of Social Identity Conflict           Journal of Personality      2022
                        on Planning Horizons                             and Social Psychology
Card et al.            Computational analysis of 140 years of          PNAS                        2022
                        US political speeches reveals more pos-
                        itive but increasingly polarized framing
                        of immigration
Peng, Romero,          Dynamics of cross-platform attention            PNAS                        2022
and Horvat             to retracted papers
Saha et al.            On the rise of fear speech in online so-        PNAS                        2022
                        cial media¹⁵
Wojcieszak et al.      Most users do not follow political elites       Science Advances           2022
                        on Twitter; those who do show over-
                        whelming preferences for ideological
                        congruity
Bảng A1: Nguồn của các nhiệm vụ chú thích được sao chép trong phân tích.
17

--- TRANG 18 ---
¹⁵Bài báo này sử dụng kho ngữ liệu phát biểu thù hận Gab (Kennedy và cộng sự, 2022). Chúng tôi bao gồm Saha và cộng sự (2023) ở đây, thay vì nguồn gốc của dữ liệu được dán nhãn, để nhấn mạnh ứng dụng của những dữ liệu này trong nghiên cứu khoa học xã hội ứng dụng.
18

--- TRANG 19 ---
Nghiên cứu                     Các nhiệm vụ chú thích
Gohdes (2020)                 Mã hóa hồ sơ tử vong Syria cho loại giết chóc cụ thể: có mục tiêu
                              hoặc không có mục tiêu
Hopkins, Lelkes,              Mã hóa tiêu đề, tweet, và đoạn chia sẻ Facebook để xác định
& Wolken (2023)               tham chiếu đến các nhóm xã hội được định nghĩa bởi a) chủng tộc/
                              dân tộc; b) giới tính/tính dục; c) chính trị; d) tôn giáo
Schub (2020)                  Mã hóa các văn bản thảo luận cấp tổng thống từ Chiến tranh Lạnh
                              như chính trị hoặc quân sự
Busby, Gubler,                Mã hóa các phản hồi mở cho ba yếu tố hùng biện: quy trách
& Hawkins                     cho một tác nhân cụ thể, quy trách cho một tác nhân ưu tú
(2019)                        xấu xa, và đề cập tích cực đến người dân tập thể
Müller (2021)                 Mã hóa các câu từ tuyên ngôn đảng cho hướng thời gian: quá khứ,
                              hiện tại, hoặc tương lai
Cusimano &                    Mã hóa các tuyên bố viết của người trả lời về biến đổi khí hậu
Goodwin (2020)                cho sự hiện diện của (a) lý luận chung về niềm tin hoặc (b) bằng
                              chứng hỗ trợ cho niềm tin
Yu & Zhang                    Mã hóa các kế hoạch của người trả lời cho tương lai thành hai
(2023)                        loại: tương lai gần và tương lai xa
Card et al.                   Mã hóa các bài phát biểu của quốc hội về việc chúng có liên quan
(2022)                        đến nhập cư không, cùng với một giọng điệu đi kèm: ủng hộ
                              nhập cư, chống nhập cư, hoặc trung tính
Peng, Romero,                 Mã hóa liệu tweet có thể hiện sự chỉ trích đối với các phát hiện
& Horvat (2022)               của các bài báo học thuật
Saha et al.                   Mã hóa các bài đăng Gab như a) phát biểu sợ hãi, b) phát biểu
(2020)                        thù hận, hoặc c) bình thường. Hơn nữa, một bài đăng có thể có
                              cả thành phần sợ hãi và thù hận, và do đó, chúng được chú thích
                              với nhiều nhãn
Wojcieszak et al.             Mã hóa liệu một quote tweet có tiêu cực, trung tính hay tích cực
(2020)                        đối với thông điệp và/hoặc tác nhân chính trị, độc lập với giọng
                              điệu của thông điệp gốc
Bảng A2: Mô tả các nhiệm vụ chú thích được sao chép trong phân tích.
19
