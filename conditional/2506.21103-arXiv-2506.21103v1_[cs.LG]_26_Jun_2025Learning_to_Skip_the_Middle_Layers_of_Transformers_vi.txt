# 2506.21103.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/conditional/2506.21103.pdf
# Kích thước file: 359133 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
arXiv:2506.21103v1  [cs.LG]  26 Thg 6 2025Học cách Bỏ qua các Lớp Giữa của Transformers
Tim Lawson∗Laurence Aitchison
Trường Kỹ thuật Toán học và Công nghệ
Đại học Bristol
Bristol, UK
Tóm tắt
Tính toán có điều kiện là một chiến lược phổ biến để làm cho Transformers hiệu quả hơn.
Các phương pháp hiện tại thường nhắm vào các mô-đun riêng lẻ (ví dụ: các lớp mixture-of-experts)
hoặc bỏ qua các lớp độc lập với nhau. Tuy nhiên, nghiên cứu về khả năng diễn giải đã
chứng minh rằng các lớp giữa của Transformers thể hiện sự dư thừa lớn hơn,
và các lớp đầu tổng hợp thông tin vào các vị trí token. Được hướng dẫn bởi những
hiểu biết này, chúng tôi đề xuất một kiến trúc mới có thể bỏ qua động một số lượng
lớp biến đổi từ giữa ra ngoài. Cụ thể, một cơ chế cổng học được
xác định có nên bỏ qua một khoảng đối xứng của các khối trung tâm dựa trên đầu vào,
và một cơ chế attention có cổng ngăn các token tiếp theo chú ý đến
các vị trí token bị bỏ qua. Các chuẩn dư được kiểm soát bằng một sơ đồ 'sandwich' hoặc 'peri-
layernorm' và độ thưa thớt của cổng bằng một hàm mất mát điều chỉnh thích ứng. Chúng tôi đã
nhằm mục đích giảm yêu cầu tính toán cho các token 'đơn giản' hơn và có khả năng thúc đẩy
một phân cấp biểu diễn đa cấp độ nổi lên nhưng, ở quy mô được nghiên cứu,
cách tiếp cận của chúng tôi không đạt được cải thiện trong sự cân bằng giữa cross-entropy xác thực
và FLOPs ước tính so với các đường cơ sở dày đặc với ít lớp hơn.
Chúng tôi công bố mã nguồn tại https://github.com/tim-lawson/skip-middle .

1 Giới thiệu
Chúng ta muốn làm cho Transformers hiệu quả hơn. Điều này có thể đạt được bằng cách làm cho các mô-đun riêng lẻ
hiệu quả hơn; ví dụ, nhiều biến thể về cơ chế attention đã được đề xuất (Dong
et al., 2024). Một cách tiếp cận thay thế là giảm số lượng tham số được kích hoạt trong quá trình suy luận.
Các phương pháp tính toán có điều kiện tách rời khả năng của mô hình (được xác định bởi tổng số lượng tham số
của nó) khỏi chi phí suy luận của nó (được xác định bởi tập con các tham số hoạt động được sử dụng cho một
đầu vào cụ thể; Bengio et al. 2013, 2016). Một ví dụ nổi bật là việc thay thế các mô-đun mạng feed-forward
(FFN) bằng các lớp mixture-of-experts (MoE) (Shazeer et al., 2017). Những phương pháp này giảm
yêu cầu tính toán và bộ nhớ trong khi cho phép song song hóa các thành phần mô hình trên
nhiều thiết bị (Eigen et al., 2014; Lepikhin et al., 2020; Fedus et al., 2022; Dai et al., 2024).

Một cách để giảm các tham số hoạt động là áp dụng có điều kiện các thành phần của Transformer
phụ thuộc vào token đầu vào. Sau đó, chúng ta có thể phân bổ động ít tài nguyên tính toán hơn cho các token
'dễ' xử lý hơn. Các phương pháp thoát sớm, trong đó các mạng sâu có thể đưa ra dự đoán ở các
lớp khác nhau, có lịch sử lâu dài trong các ứng dụng thị giác (Teerapittayanon et al., 2016) và ngôn ngữ (Elbayad
et al., 2020; Xin et al., 2020). Cách tiếp cận này đã được sử dụng để bỏ qua động các lớp Transformer
vượt quá một độ sâu nhất định (Elhoushi et al., 2024; Fan et al., 2024). Các phương pháp khác bỏ qua các thành phần
trung gian (Wang et al., 2018), chẳng hạn như các mô-đun riêng lẻ (Csordás et al., 2021; Peroni and
Bertsimas, 2024) hoặc toàn bộ các lớp (Zeng et al., 2023; Raposo et al., 2024).

Chúng tôi lập luận rằng việc bỏ qua các lớp giữa của Transformers có ý nghĩa hơn. Nhiều nhà nghiên cứu
đã chứng minh rằng các lớp giữa thể hiện sự dư thừa lớn hơn: ví dụ, Lad et al. (2024)
∗Liên hệ với tim.lawson@bristol.ac.uk .
Bản thảo. Đang được xem xét.

--- TRANG 2 ---
EmbedHead
GateGate
BlockBlockBlockBlock
EmbedHead
GateGate
BlockBlockBlockBlock
EmbedHead
GateGate
BlockBlockBlockBlock
Lớp 0Lớp 1Lớp 2Lớp 3
Token 0 Token 1 Token 2g(0,0)>0g(0,1)>0
g(1,0)>0g(1,1)= 0
g(2,0)= 0Bỏ qua các khối
1–2Bỏ qua các khối
0–3

Hình 1: Minh họa kiến trúc được đề xuất của chúng tôi (với bốn lớp hoặc khối). Chúng tôi tính toán một
giá trị cổng vô hướng cho mỗi vị trí token và khối trong nửa đầu của mô hình. Nếu cổng tại khối ℓ
bằng không, chúng tôi bỏ qua các khối Transformer giữa ℓ và L−ℓ cho token, và ngăn các token khác
chú ý đến vị trí của nó trong các mô-đun self-attention tương ứng.

và González et al. (2025) phát hiện rằng khi các lớp bị loại bỏ hoặc hoán đổi trong các mô hình đã được huấn luyện trước, tác động hiệu suất nhỏ hơn đối với các can thiệp ảnh hưởng đến các lớp trung tâm hơn. Sự dư thừa này đã được khai thác cho việc cắt tỉa có cấu trúc (Fan et al., 2019; Gromov et al., 2024; Men et al., 2024).
Nghiên cứu về khả năng diễn giải cũng đã thiết lập rằng các lớp đầu, giữa và cuối trong các mạng sâu có
các chức năng khác nhau. Trong các mô hình ngôn ngữ, các mô-đun lớp đầu chuyển đổi các biểu diễn ở cấp token thành
các đặc trưng ngữ nghĩa tự nhiên hơn (Elhage et al., 2022; Gurnee et al., 2023). Hơn nữa, Kaplan et al.
(2024) đã chỉ ra rằng, đối với các từ có nhiều token, cơ chế attention trong các lớp Transformer đầu
tổng hợp thông tin vào vector dư của token cuối cùng trong từ. Ngược lại, các mô-đun lớp cuối chuyển đổi các đặc trưng ngữ nghĩa thành các token đầu ra: gần đầu ra của mạng, các trạng thái trung gian có thể được giải mã để đưa ra dự đoán token (nostalgebraist, 2020; Belrose et al., 2023). Các kích hoạt nội bộ tại các lớp sớm nhất và muộn nhất cũng tương đối khác biệt với các lớp giữa qua lăng kính học từ điển thưa thớt (Lawson et al., 2024).

Các chuyển đổi giữa token và đặc trưng ngữ nghĩa song song với sự phát triển của các kiến trúc ở cấp byte
(Xue et al., 2022; Slagle, 2024), mà chúng tôi mong đợi sẽ học token hóa một cách ngầm định. Ví dụ,
Pagnoni et al. (2024); Neitemeier et al. (2024); Kallini et al. (2024) thúc đẩy một phân cấp hai lớp của
các biểu diễn ở cấp byte và token. Dựa trên Kaplan et al. (2024), chúng ta có thể mong đợi rằng phân cấp này
có thể được mở rộng có lợi đến các cấp độ bao trùm nhiều token (Ho et al., 2024; Videau et al., 2025).

Được hướng dẫn bởi những hiểu biết này, chúng tôi đề xuất một cơ chế cổng bỏ qua một số lượng biến đổi các
khối Transformer từ giữa ra ngoài, phụ thuộc vào token đầu vào. Bằng cách này, chúng ta có thể phân bổ ít
tài nguyên tính toán hơn cho các đầu vào 'đơn giản' hơn bằng cách bỏ qua các lớp giữa, có khả năng
dư thừa hơn. Lớp càng trung tâm, càng ít token được xử lý, cho phép một phân cấp đa cấp của
các biểu diễn nổi lên. Thật không may, ở quy mô chúng tôi có thể nghiên cứu, kiến trúc này
không cải thiện sự cân bằng giữa hiệu suất mô hình hóa ngôn ngữ và tài nguyên tính toán
cần thiết, được đo bằng FLOPs ước tính tại thời điểm suy luận nếu chúng tôi có thể
đạt được lợi ích tối đa từ độ thưa thớt của các giá trị cổng.

2 Kiến trúc mô hình
Một Transformer chỉ giải mã tiêu chuẩn có L lớp hoặc khối, mỗi cái bao gồm một self-attention
và mô-đun FFN. Chúng tôi ký hiệu đầu vào của lớp ℓ tại vị trí token i bằng h(i,ℓ)∈Rd trong đó d là
chiều mô hình, và các đầu vào tại tất cả các vị trí token i∈1..N bằng H(ℓ)∈RN×D.

--- TRANG 3 ---
Ở mức độ cao, Transformer tiêu chuẩn được cho bởi (bỏ qua chuẩn hóa lớp):
h(i,0)= Embed( i)
a(i,ℓ)=h(i,ℓ−1)+ Attn( H(ℓ−1))
h(i,ℓ)=a(i,ℓ)+ FFN( a(i,ℓ))
y(i)= Head( h(i,L)).

Chúng tôi đề xuất sửa đổi kiến trúc này như sau:
a(i,ℓ)=h(i,ℓ−1)+g(i,ℓ)GatedAttn ( H(ℓ−1),g(ℓ))
h(i,ℓ)=a(i,ℓ)+g(i,ℓ)FFN( a(i,ℓ)).

Các sửa đổi được làm nổi bật. Nếu giá trị cổng g(i,ℓ)∈R cho vị trí token i bằng không, thì chúng ta
không cần tính toán các đầu ra tương ứng của các mô-đun attention và feed-forward. Cơ chế
cổng do đó hoạt động như một loại router (Hình 1).

2.1 Cơ chế cổng
Đối với mỗi khối ℓ < L/ 2 trong nửa đầu của mô hình, chúng tôi giới thiệu một lớp tuyến tính xuất ra một giá trị
mask mềm vô hướng s(i,ℓ)≥0, tích lũy qua nửa đầu của mô hình:
s(i,ℓ)= ReLU
w(ℓ)·h(i,ℓ)+b(ℓ)
, S(i,ℓ)=X
ℓ′≤ℓs(i,ℓ′). (1)

Khi giá trị mask mềm tích lũy S(i,ℓ)≥1, chúng ta bỏ qua việc xử lý vector dư tại vị trí
token i bởi các khối Transformer [ℓ, L−ℓ). Do đó, đối với mỗi khối ℓ≥L/2 trong nửa thứ hai của
mô hình, chúng ta sử dụng giá trị mask mềm tích lũy S(i,L−ℓ−1) của khối đối diện.

Giá trị cổng vô hướng tương ứng g(i,ℓ)∈[0,1] là:
g(i,ℓ)=(
1−clamp
S(i,ℓ),0,1
nếu ℓ < L/ 2
1−clamp
S(i,L−ℓ−1),0,1
nếu ℓ≥L/2(2)

Độ thưa thớt của các giá trị cổng g(i,ℓ) xác định việc giảm số lượng tham số hoạt động: đối với
một token đơn i, đó là số lượng khối mà giá trị cổng bằng không nhân với số lượng
tham số trong một khối Transformer NB. Với nhiều token, việc giảm là 2NBP
ℓ<L/ 2zℓ,
trong đó zℓ là độ thưa thớt của các giá trị cổng trước khi các token được xử lý bởi khối ℓ.

Cơ chế cổng giới thiệu (d+ 1)L/2 tham số w(ℓ) và b(ℓ), tức là d+ 1 cho mỗi khối
ℓ < L/ 2 trong nửa đầu của mô hình. Nếu tất cả các tham số này bằng không, tất cả các giá trị cổng bằng một
và chúng ta khôi phục chính xác Transformer dày đặc tương đương (tạo thành đường cơ sở trong Phần 3).

2.2 Attention có cổng
Chúng tôi cũng ngăn các token khác chú ý đến các token có cổng trong các mô-đun attention. Mô-đun GatedAttn
mà chúng tôi kết hợp sửa đổi cơ chế attention sao cho, khi giá trị cổng cho một token
bằng không, các token tiếp theo không chú ý đến token có cổng:
oi=P
j<igjexp(qT
ikj)vjP
j<igjexp(qT
ikj)(3)

Điều này tương đương với việc thêm lngjvào các logit attention trước softmax, và có thể được triển khai
một cách đơn giản như một sửa đổi điểm số trong framework FlexAttention (Dong et al., 2024). Trong
thực tế, chúng tôi áp dụng một giới hạn dưới ϵ= 1×10−6 cho gj trước ln để ngăn vô cực.

Cơ chế attention của chúng tôi tương tự như 'Forgetting Attention' được đề xuất bởi Lin et al. (2024), ngoại trừ
việc chúng tôi tính toán một giá trị cổng đơn áp dụng cho mọi đầu attention, trong khi họ tính toán một giá trị
cổng cho mỗi đầu attention. Chúng tôi yêu cầu một giá trị cổng đơn để quyết định có nên ngăn một
khối Transformer hoàn chỉnh xử lý token hay không.

--- TRANG 4 ---
Tên Mất mát Quy tắc cập nhật
sparsity1
LPL
ℓ=1αℓgℓ -
sparsity_variance
1
LPL
ℓ=1
αℓgℓ+βℓs2
ℓ-
adaptive αi+1=αi+γsign(gℓ−µ∗
ℓ)
proportional αi+1=αi+γ(gℓ−µ∗
ℓ)
sparsity_variance_l21
LPL
ℓ=1
αℓ∥gℓ−µ∗
ℓ∥2
2+βℓ

s2
ℓ−σ2∗
ℓ

2
2

Bảng 1: Các kỹ thuật thay thế để kiểm soát độ thưa thớt của các giá trị cổng. Nhớ rằng gℓ và s2
ℓ là
trung bình và phương sai của các giá trị cổng qua các vị trí token trong một batch, và µ∗
ℓ và σ2∗
ℓ là các
mục tiêu tại lớp ℓ cho trung bình và phương sai của các giá trị cổng, tương ứng. Đối với các kỹ thuật với
hệ số thích ứng, αℓ và βℓ được cập nhật bởi cùng một thuật toán.

2.3 Chuẩn hóa lớp
Các Transformer hiện đại thường sử dụng sơ đồ 'pre-layernorm' (pre-LN), trong đó các phép toán chuẩn hóa lớp
được áp dụng cho các đầu vào dư của các mô-đun attention và FFN:
y=x+ Module(Norm( x)).

Với sơ đồ này, chuẩn của các vector kích hoạt dư tăng theo độ sâu và các mô-đun sau tạo ra
đầu ra với chuẩn lớn hơn (Lawson et al., 2024; Csordás et al., 2024a; Kim et al., 2025).

Cơ chế cổng mà chúng tôi đề xuất hiệu quả giới thiệu các kết nối bỏ qua giữa các cặp đối diện của
các khối Transformer (Phần 2.1). Giống như Csordás et al. (2024a), người xem xét một Universal Transformer
(UT) với một khối duy nhất, được chia sẻ, chúng tôi muốn các mô-đun sau chấp nhận đầu ra của các khối đầu và cuối.
Chúng tôi giải quyết vấn đề này bằng cách sử dụng sơ đồ LN 'sandwich' được đề xuất trong Ding et al. (2021), được gọi là
'peri-layernorm' bởi Kim et al. (2025), trong đó các phép toán chuẩn hóa lớp được áp dụng cho cả đầu vào
dư và đầu ra của các mô-đun attention và FFN:
y=x+ Norm(Module(Norm( x))).

Sơ đồ này khác với 'peri-layernorm' của Csordás et al. (2024a), người áp dụng một phép toán chuẩn hóa
lớp "xung quanh (nhưng không trên) các kết nối dư."

2.4 Kiểm soát độ thưa thớt
Kiến trúc có cổng của chúng tôi giảm số lượng tham số được kích hoạt trong quá trình truyền tiến tỷ lệ
với phần các giá trị cổng chính xác bằng không, tức là độ thưa thớt cổng trung bình (Phần 2.1).
Chỉ với hàm mất mát cross-entropy tiêu chuẩn, việc tối ưu hóa có xu hướng kích hoạt nhiều tham số hơn để cải thiện
hiệu suất, vì vậy chúng ta cần kiểm soát độ thưa thớt của các giá trị cổng.

Chúng tôi đạt được điều này bằng cách giới thiệu một hàm mất mát điều chỉnh dựa trên trung bình và phương sai của các giá trị cổng,
với các hệ số thích ứng được cập nhật tỷ lệ với các độ lệch của trung bình và phương sai từ
các mục tiêu theo lớp. Hạng tử trung bình khuyến khích các giá trị cổng nhỏ hơn; hạng tử phương sai khuyến khích một
phân phối không đồng nhất sao cho một số (nhưng không phải tất cả) giá trị cổng bằng không.

Chúng tôi ký hiệu trung bình và phương sai của các giá trị cổng qua các vị trí token trong một batch bằng:
gℓ=1
NNX
i=1g(i,ℓ), s2
ℓ=1
NNX
i=1
g(i,ℓ)−gℓ2
. (4)

Các mục tiêu tại lớp ℓ cho trung bình và phương sai tổng thể của các giá trị cổng qua các vị trí token
là µ∗
ℓ và σ2∗
ℓ, tương ứng. Trừ khi có ghi chú, chúng tôi chọn các mục tiêu trung bình µ∗
ℓ như các giá trị
được phân bố tuyến tính giữa một mục tiêu ban đầu µ∗
0 và một mục tiêu cuối cùng µ∗
L/2. Chúng tôi chọn các mục tiêu phương sai σ2∗
ℓ như
phương sai của phân phối Bernoulli với p=µ∗
ℓ, tức là σ2∗
ℓ=µ∗
ℓ(1−µ∗
ℓ).

--- TRANG 5 ---
2 3 4 5 6
·10113.23.33.43.53.6
FLOPs Suy luậnCross-entropy
0 0.1 0.2 0.31284
1062
Độ thưa thớtĐường cơ sở dày đặc
Có cổng (không có kiểm soát)
Có cổng (có kiểm soát)

Hình 2: So sánh hiệu suất giữa kiến trúc Transformer có cổng của chúng tôi và các mô hình cơ sở
với từ 2 đến 12 lớp (được ghi nhãn). Tất cả các mô hình có cổng với kiểm soát đều là các biến thể của kiến trúc 12 lớp. Chúng tôi đo cross-entropy trên 100M token từ tập xác thực FineWeb. FLOPs ước tính cho một lần truyền tiến đơn (trái) giả định rằng lợi ích tính toán tối đa được đạt được từ độ thưa thớt cuối cùng của các giá trị cổng trên tập xác thực (phải).

Chúng tôi ký hiệu các hệ số thích ứng cho trung bình và phương sai của các giá trị cổng tại lớp ℓ bằng αℓ và
βℓ, tương ứng, và khởi tạo các hệ số bằng không. Hàm mất mát điều chỉnh sau đó là:
L=1
LLX
ℓ=1
αℓgℓ+βℓs2
ℓ
. (5)

Sau mỗi bước huấn luyện, chúng tôi cập nhật mỗi hệ số theo quy tắc sau:
αℓ,i+1=(
αℓ,i+γ(gℓ−µ∗
ℓ)nếu(gℓ−µ∗
ℓ)> δ
αℓ,i nếu không(6)

Các cập nhật do đó tỷ lệ với các khác biệt từ các giá trị mục tiêu. Chúng tôi chọn hệ số nhân cập nhật
γ= 1×10−3 và dung sai δ= 1×10−2 dựa trên các quan sát trong các thí nghiệm quy mô nhỏ.
Chúng tôi đã khám phá các cơ chế kiểm soát thay thế (Bảng 1), nhưng những cơ chế này hoạt động kém hơn theo kinh nghiệm.

3 Kết quả
Chúng tôi đánh giá hiệu suất của kiến trúc Transformer có cổng của mình theo cross-entropy xác thực
khi được huấn luyện trước trên tập dữ liệu FineWeb (Penedo et al., 2024). Làm đường cơ sở, chúng tôi huấn luyện
các mô hình dày đặc tương đương không có cơ chế cổng và từ 2 đến 12 lớp. Chúng tôi đo
yêu cầu tính toán của mỗi mô hình theo các phép toán dấu phẩy động ước tính (FLOPs) cho
một lần truyền tiến đơn (kích thước batch 1), giả định rằng chúng tôi có thể đạt được lợi ích tối đa có thể
do độ thưa thớt của các giá trị cổng. Yêu cầu tính toán thực tế của các mô hình có cổng và dày đặc
tương tự nhau. Trừ khi có ghi chú, chúng tôi sử dụng các siêu tham số trong Bảng 2.

Hình 2 cho thấy rằng, không kiểm soát độ thưa thớt của các cổng, độ thưa thớt trung bình có xu hướng về
không và các mô hình có cổng hoạt động tương tự như các đường cơ sở dày đặc (phải). Với mục tiêu ban đầu cho
các cổng trung bình µ∗
0 cố định ở 1, khi chúng tôi giảm mục tiêu cuối cùng µ∗
L/2 từ 1 xuống 0, độ thưa thớt tăng
và FLOPs ước tính giảm (trái). Tuy nhiên, kiến trúc được đề xuất không cải thiện
cross-entropy so với các đường cơ sở dày đặc với ít lớp hơn.

--- TRANG 6 ---
Tham số Giá trị
model
dim 768
n_layers 12
n_heads 12
n_kv_heads 12
vocab_size 50 257
ffn_dim_multiplier 4
multiple_of 256
norm_eps 1×10−5
rope_theta 10 000
use_scaled_rope False
max_seq_len 1024
initializer_range 0.02Tham số Giá trị
data
batch_size 512
device_batch_size 32
optimizer
lr 0.001
beta1 0.8
beta2 0.95
eps 1×10−10
weight_decay 0
scheduler
warmup_steps 0.1
start_factor 0.1

Bảng 2: Siêu tham số mặc định. Kiến trúc mô hình Transformer dựa trên Llama 3
(Grattafiori et al., 2024) với các chiều tương tự GPT-2 small (Radford et al., 2019); chúng tôi sử dụng
bộ tối ưu AdamW (Loshchilov and Hutter, 2019) với khởi động tuyến tính và phân rã cosine.

3.1 Chi tiết thí nghiệm
Chúng tôi huấn luyện tất cả các mô hình trên một tập con được lấy mẫu ngẫu nhiên của tập dữ liệu FineWeb với xấp xỉ
10B token (Penedo et al., 2024), được token hóa trước với bộ token hóa GPT-2 qua thư viện TikToken
(Radford et al., 2019; Ouyang et al., 2022). Tập xác thực chứa xấp xỉ 100M token.

Chúng tôi sử dụng kích thước batch toàn cục 512 chuỗi ( 524 288 token) với song song hóa dữ liệu và tích lũy gradient
trên kích thước batch mỗi thiết bị 32 chuỗi trên 4 GPU NVIDIA A100 hoặc GH200.

Chúng tôi dựa trên các mô hình Transformer cơ bản trên triển khai tham chiếu của Llama 3 (Grattafiori
et al., 2024). Cụ thể, chúng tôi sử dụng: Grouped Query Attention (GQA; Ainslie et al. 2023); Rotary
Positional Embeddings (RoPE; Su et al. 2024); Gated Linear Unit FFNs với kích hoạt Swish
(SwiGLU; Shazeer 2020); và chuẩn hóa lớp Root Mean Square (RMSNorm) (Zhang and
Sennrich, 2019). Khác biệt chính so với Llama 3 là chúng tôi sử dụng sơ đồ Sandwich-LN
(Ding et al., 2021; Kim et al., 2025) thay vì Pre-LN. Chúng tôi khởi tạo các tham số RMSNorm thành một
và lấy mẫu tất cả các tham số khác từ phân phối chuẩn với trung bình không và độ lệch chuẩn 0.02.

Codebase huấn luyện dựa trên kho lưu trữ 'nanoGPT speedrun' (Karpathy, 2025; Jordan, 2025).
Chúng tôi sử dụng bộ tối ưu AdamW với một tốc độ học duy nhất cho tất cả các tham số mô hình (Kingma and Ba,
2017; Loshchilov and Hutter, 2019), và một bộ lập lịch tốc độ học hai giai đoạn với khởi động tuyến tính
trong 10% các bước huấn luyện, bắt đầu ở 10% tốc độ học tối đa, và phân rã cosine
trong các bước còn lại. Cuối cùng, chúng tôi thực hiện các lần truyền tiến trong bfloat16 với độ chính xác hỗn hợp tự động
trong PyTorch (ngoại trừ việc chuyển đổi thủ công các logit attention thành float32).

4 Công trình liên quan
Tính toán có điều kiện tách rời tổng số lượng tham số của mô hình khỏi chi phí suy luận của nó bằng cách
kích hoạt chỉ một tập con tham số cho một đầu vào cụ thể (Bengio et al., 2013; Eigen et al., 2014;
Bengio et al., 2016). Một ứng dụng nổi bật của nguyên tắc này là việc sử dụng các lớp Mixture-of-Experts,
thay thế các mô-đun FFN bằng một tập lớn hơn các 'expert' sub-network, trong đó chỉ một vài
được chọn bởi một router để xử lý mỗi đầu vào (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al.,
2022; Dai et al., 2024). Trong khi các phương pháp liên quan như Zhang et al. (2022); Csordás et al. (2024b); Jin
et al. (2024) có thể hiệu quả, chúng hoạt động trên các mô-đun riêng lẻ (ví dụ: FFN hoặc attention) và thường
sử dụng các chiến lược định tuyến thực thi ngân sách tính toán cố định mỗi token (cf. Wang et al., 2024). Cách
tiếp cận của chúng tôi khác biệt bằng cách áp dụng tính toán có điều kiện cho toàn bộ các khối Transformer, và phân bổ động
một số lượng khối biến đổi cho mỗi token dựa trên nhu cầu xử lý của nó, một chiến lược cũng
tương thích với các kỹ thuật ở cấp mô-đun.

--- TRANG 7 ---
Một hướng công việc khác để cải thiện hiệu quả là thay đổi động độ sâu mạng. Các phương pháp thoát sớm
cho phép mô hình tạo dự đoán tại các lớp trung gian, dừng tính toán cho các đầu vào 'dễ'
(Teerapittayanon et al., 2016; Elbayad et al., 2020; Xin et al., 2020). Các biến thể gần đây hơn
của cách tiếp cận này bỏ qua động tất cả các lớp vượt quá một độ sâu nhất định (Elhoushi et al., 2024; Fan
et al., 2024). Ngược lại, phương pháp của chúng tôi được thúc đẩy bởi các phát hiện thực nghiệm rằng các lớp giữa của
Transformers thể hiện sự dư thừa lớn hơn (Lad et al., 2024; González et al., 2025). Những phát hiện này
đã được khai thác bởi cắt tỉa có cấu trúc, loại bỏ các lớp một cách tĩnh sau huấn luyện (Fan et al.,
2019; Gromov et al., 2024; Men et al., 2024). Công trình của chúng tôi khác biệt ở chỗ nó nhắm vào các lớp giữa
dư thừa hơn để bỏ qua và làm như vậy một cách động trong quá trình suy luận.

Một số phương pháp đã khám phá việc bỏ qua toàn bộ các khối Transformer. Ví dụ, cổng copy được
đề xuất bởi Csordás et al. (2021) điều chỉnh đóng góp của một khối, nhưng vẫn yêu cầu tính toán đầy đủ
đầu ra của khối. Tuy nhiên, cơ chế cổng của chúng tôi đảm bảo rằng tính toán cho các khối bị bỏ qua
có thể được tránh hoàn toàn. Các mô hình Mixture-of-Depths (MoD) (Raposo et al., 2024) chỉ xử lý
top-k token tại mỗi khối, thực thi ngân sách tính toán cố định cho chuỗi. Phương pháp của chúng tôi
khác biệt vì nó xác định độ sâu tính toán cho mỗi token riêng lẻ, cho phép nó
thích ứng với độ phức tạp cụ thể của token thay vì ngân sách toàn chuỗi. Các cách tiếp cận như Skip-
Net (Wang et al., 2018) cũng cho phép bỏ qua lớp nhưng không được thiết kế cụ thể cho các mô hình
dư thừa độc đáo được quan sát trong các lớp giữa của Transformers.

Cơ chế attention có cổng của chúng tôi ngăn attention đến các token đã bị che giấu, có chức năng
tương tự như 'Forgetting Attention' được đề xuất bởi Lin et al. (2024). Tuy nhiên, cách tiếp cận của họ
tính toán các cổng riêng biệt cho mỗi đầu attention, trong khi cách tiếp cận của chúng tôi sử dụng một cổng đơn
mỗi token để quyết định có nên bỏ qua một khối Transformer hoàn chỉnh hay không, điều này rất cần thiết cho việc tiết kiệm
tính toán ở cấp khối mà chúng tôi nhắm đến.

Cuối cùng, công trình của chúng tôi liên quan đến các Transformer phân cấp xử lý chuỗi ở nhiều cấp độ được xác định trước
(ví dụ: cấp byte và token; Pagnoni et al. 2024; Neitemeier et al. 2024; Kallini et al. 2024).
Cách tiếp cận bỏ qua từ giữa ra ngoài của chúng tôi cung cấp tiềm năng cho một phân cấp linh hoạt hơn, nổi lên,
trong đó các lớp sâu hơn xử lý một tập con được xác định động của các biểu diễn phức tạp hơn.

5 Kết luận
Chúng tôi đã giới thiệu một kiến trúc Transformer mới bỏ qua động một số lượng biến đổi các lớp giữa,
được hướng dẫn bởi nghiên cứu khả năng diễn giải cho thấy những lớp này là dư thừa nhất. Cơ chế
sử dụng một cổng học để bỏ qua một khoảng đối xứng của các khối trung tâm dựa trên token đầu vào,
với mục tiêu giảm tính toán cho các token đơn giản hơn và cho phép một phân cấp biểu diễn
nổi lên. Các thí nghiệm của chúng tôi cho thấy rằng, ở quy mô nhỏ, kiến trúc này không mang lại cải thiện
trong sự cân bằng giữa hiệu suất xác thực và FLOPs suy luận ước tính khi so sánh
với việc đơn giản huấn luyện các mô hình cơ sở dày đặc với ít lớp hơn. Các lợi ích dự kiến của tiên nghiệm kiến trúc này
có thể chỉ trở nên rõ ràng ở quy mô mô hình lớn hơn đáng kể, nơi sự dư thừa của các lớp giữa
rõ rệt hơn và chi phí tương đối của cơ chế cổng nhỏ hơn. Bất chấp những kết quả này, chúng tôi tin rằng
nguyên tắc sử dụng hiểu biết từ nội bộ mô hình để thiết kế các kiến trúc hiệu quả hơn và có cấu trúc hơn
vẫn là một hướng có giá trị cho nghiên cứu tương lai.

Tài liệu tham khảo
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, và Sumit
Sanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Check-
points. Trong The 2023 Conference on Empirical Methods in Natural Language Processing, Tháng 12
2023. URL https://openreview.net/forum?id=hmOwOZWzYE .

Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella
Biderman, và Jacob Steinhardt. Eliciting Latent Predictions from Transformers with the Tuned
Lens, Tháng 11 2023. URL http://arxiv.org/abs/2303.08112 .

Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, và Doina Precup. Conditional Computation in
Neural Networks for faster models, Tháng 1 2016. URL http://arxiv.org/abs/1511.06297 .

--- TRANG 8 ---
Yoshua Bengio, Nicholas Léonard, và Aaron Courville. Estimating or Propagating Gradients
Through Stochastic Neurons for Conditional Computation, Tháng 8 2013. URL http://arxiv.
org/abs/1308.3432 .

Róbert Csordás, Kazuki Irie, và Jürgen Schmidhuber. The Neural Data Router: Adaptive Control
Flow in Transformers Improves Systematic Generalization. Trong International Conference on
Learning Representations, Tháng 10 2021. URL https://openreview.net/forum?id=KBQP4A_
J1K.

Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Christopher Potts, và Christopher D. Manning.
MoEUT: Mixture-of-Experts Universal Transformers. Advances in Neural Information Processing
Systems, 37:28589–28614, Tháng 12 2024a.

Róbert Csordás, Piotr Pi˛ ekos, Kazuki Irie, và Jürgen Schmidhuber. SwitchHead: Accelerating
Transformers with Mixture-of-Experts Attention. Advances in Neural Information Processing
Systems, 37:74411–74438, Tháng 12 2024b.

Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding
Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang
Sui, và Wenfeng Liang. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-
Experts Language Models, Tháng 1 2024. URL http://arxiv.org/abs/2401.06066 .

Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,
Zhou Shao, Hongxia Yang, và Jie Tang. CogView: Mastering Text-to-Image Generation via
Transformers, Tháng 11 2021. URL http://arxiv.org/abs/2105.13290 .

Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, và Horace He. Flex Attention: A
Programming Model for Generating Optimized Attention Kernels, Tháng 12 2024. URL http:
//arxiv.org/abs/2412.05496 .

David Eigen, Marc'Aurelio Ranzato, và Ilya Sutskever. Learning Factored Representations in a
Deep Mixture of Experts, Tháng 3 2014. URL http://arxiv.org/abs/1312.4314 .

Maha Elbayad, Jiatao Gu, Edouard Grave, và Michael Auli. Depth-Adaptive Transformer, Tháng 2
2020. URL http://arxiv.org/abs/1910.10073 .

Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer
ElShowk, Nicholas Joseph, Nova DasSarma, và Ben Mann. Softmax linear units, 2022. URL
https://transformer-circuits.pub/2022/solu/index.html .

Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai,
Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A. Aly, Beidi Chen, và
Carole-Jean Wu. LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding. Trong
Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), trang 12622–12642, 2024. doi: 10.18653/v1/2024.acl-long.681.

Angela Fan, Edouard Grave, và Armand Joulin. Reducing Transformer Depth on Demand with
Structured Dropout, Tháng 9 2019. URL http://arxiv.org/abs/1909.11556 .

Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, và
Zhongyuan Wang. Not All Layers of LLMs Are Necessary During Inference, Tháng 7 2024. URL
http://arxiv.org/abs/2403.02181 .

William Fedus, Barret Zoph, và Noam Shazeer. Switch Transformers: Scaling to Trillion Parameter
Models with Simple and Efficient Sparsity. Journal of Machine Learning Research, 23(120):1–39,
2022. ISSN 1533-7928. URL http://jmlr.org/papers/v23/21-0998.html .

Ramón Calvo González, Daniele Paliotta, Matteo Pagliardini, Martin Jaggi, và François Fleuret.
Leveraging the true depth of LLMs, Tháng 2 2025. URL http://arxiv.org/abs/2502.02790 .

Aaron Grattafiori, Abhimanyu Dubey, et al. The Llama 3 Herd of Models, Tháng 11 2024. URL
http://arxiv.org/abs/2407.21783 .

--- TRANG 9 ---
Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, và Dan Roberts. The Unrea-
sonable Ineffectiveness of the Deeper Layers. Trong The Thirteenth International Conference on Learn-
ing Representations, Tháng 10 2024. URL https://openreview.net/forum?id=ngmEcEer8a .

Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, và Dimitris Bertsimas.
Finding Neurons in a Haystack: Case Studies with Sparse Probing, Tháng 6 2023. URL http:
//arxiv.org/abs/2305.01610 .

Namgyu Ho, Sangmin Bae, Taehyeon Kim, hyunjik.jo, Yireun Kim, Tal Schuster, Adam Fisch, James
Thorne, và Se-Young Yun. Block Transformer: Global-to-Local Language Modeling for Fast
Inference. Trong The Thirty-eighth Annual Conference on Neural Information Processing Systems,
Tháng 11 2024. URL https://openreview.net/forum?id=6osgTNnAZQ .

Peng Jin, Bo Zhu, Li Yuan, và Shuicheng Yan. MoH: Multi-Head Attention as Mixture-of-Head
Attention, Tháng 10 2024. URL http://arxiv.org/abs/2410.11842 .

Keller Jordan. KellerJordan/modded-nanogpt, Tháng 5 2025. URL https://github.com/
KellerJordan/modded-nanogpt .

Julie Kallini, Shikhar Murty, Christopher D. Manning, Christopher Potts, và Róbert Csordás. MrT5:
Dynamic Token Merging for Efficient Byte-level Language Models. Trong The Thirteenth International
Conference on Learning Representations, Tháng 10 2024. URL https://openreview.net/forum?
id=VYWBMq1L7H .

Guy Kaplan, Matanel Oren, Yuval Reif, và Roy Schwartz. From Tokens to Words: On the Inner
Lexicon of LLMs. Trong The Thirteenth International Conference on Learning Representations,
Tháng 10 2024. URL https://openreview.net/forum?id=328vch6tRs .

Andrej Karpathy. karpathy/nanoGPT, Tháng 5 2025. URL https://github.com/karpathy/nanoGPT .

Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo,
Seongjin Shin, Dongyoon Han, Jinwoo Shin, và Kang Min Yoo. Peri-LN: Revisiting Nor-
malization Layer in the Transformer Architecture. Trong Forty-second International Conference
on Machine Learning, Tháng 6 2025. URL https://openreview.net/forum?id=ci1S6wmXfO&
noteId=r07RHYqMC5 .

Diederik P. Kingma và Jimmy Ba. Adam: A Method for Stochastic Optimization, Tháng 1 2017.
URL http://arxiv.org/abs/1412.6980 .

Vedang Lad, Wes Gurnee, và Max Tegmark. The Remarkable Robustness of LLMs: Stages of
Inference?, Tháng 6 2024. URL http://arxiv.org/abs/2406.19384 .

Tim Lawson, Lucy Farnik, Conor Houghton, và Laurence Aitchison. Residual Stream Analysis
with Multi-Layer SAEs. Trong The Thirteenth International Conference on Learning Representations,
Tháng 10 2024. URL https://openreview.net/forum?id=XAjfjizaKs .

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, và Zhifeng Chen. GShard: Scaling Giant Models with Conditional
Computation and Automatic Sharding, Tháng 6 2020. URL http://arxiv.org/abs/2006.16668 .

Zhixuan Lin, Evgenii Nikishin, Xu He, và Aaron Courville. Forgetting Transformer: Softmax Atten-
tion with a Forget Gate. Trong The Thirteenth International Conference on Learning Representations,
Tháng 10 2024. URL https://openreview.net/forum?id=q2Lnyegkr8 .

Ilya Loshchilov và Frank Hutter. Decoupled Weight Decay Regularization, Tháng 1 2019. URL
http://arxiv.org/abs/1711.05101 .

Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, và
Weipeng Chen. ShortGPT: Layers in Large Language Models are More Redundant Than You
Expect, Tháng 10 2024. URL http://arxiv.org/abs/2403.03853 .

Pit Neitemeier, Björn Deiseroth, Constantin Eichenberg, và Lukas Balles. Hierarchical Autoregres-
sive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language
Models. Trong The Thirteenth International Conference on Learning Representations, Tháng 10 2024.
URL https://openreview.net/forum?id=tU074jg2vS .

--- TRANG 10 ---
nostalgebraist. Interpreting GPT: the logit lens, Tháng 8 2020. URL https://www.lesswrong.com/
posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens .

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, và
Ryan Lowe. Training language models to follow instructions with human feedback, Tháng 3 2022.
URL http://arxiv.org/abs/2203.02155 .

Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li,
Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman,
và Srinivasan Iyer. Byte Latent Transformer: Patches Scale Better Than Tokens, Tháng 12 2024.
URL http://arxiv.org/abs/2412.09871 .

Guilherme Penedo, Hynek Kydlí ˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin
Raffel, Leandro V on Werra, và Thomas Wolf. The FineWeb Datasets: Decanting the Web for the
Finest Text Data at Scale, Tháng 10 2024. URL http://arxiv.org/abs/2406.17557 .

Matthew Peroni và Dimitris Bertsimas. Skip Transformers: Efficient Inference through Skip-Routing.
Tháng 10 2024. URL https://openreview.net/forum?id=gdMJlwTcSQ .

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, và Ilya
Sutskever. Language Models are Unsupervised Multitask Learners, 2019. URL https:
//cdn.openai.com/better-language-models/language_models_are_unsupervised_
multitask_learners.pdf .

David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, và Adam
Santoro. Mixture-of-Depths: Dynamically allocating compute in transformer-based language
models, Tháng 4 2024. URL http://arxiv.org/abs/2404.02258 .

Noam Shazeer. GLU Variants Improve Transformer, Tháng 2 2020. URL http://arxiv.org/abs/
2002.05202 .

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và
Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,
Tháng 1 2017. URL http://arxiv.org/abs/1701.06538 .

Kevin Slagle. SpaceByte: Towards Deleting Tokenization from Large Language Modeling. Trong The
Thirty-eighth Annual Conference on Neural Information Processing Systems, Tháng 11 2024.
URL https://openreview.net/forum?id=KEe4IUp20I .

Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, và Yunfeng Liu. RoFormer:
Enhanced transformer with Rotary Position Embedding. Neurocomput., 568(C), Tháng 2 2024.
ISSN 0925-2312. doi: 10.1016/j.neucom.2023.127063.

Surat Teerapittayanon, Bradley McDanel, và H.T. Kung. BranchyNet: Fast Inference via Early
Exiting from Deep Neural Networks. Trong 2016 23rd International Conference on Pattern Recognition
(ICPR), trang 2464–2469, Tháng 12 2016. doi: 10.1109/ICPR.2016.7900006.

Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, và David
Lopez-Paz. From Bytes to Ideas: Language Modeling with Autoregressive U-Nets, 2025. URL
https://arxiv.org/abs/2506.14761 .

Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, và Joseph E. Gonzalez. SkipNet: Learning
Dynamic Routing in Convolutional Networks. Trong Computer Vision – ECCV 2018: 15th European
Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, trang 420–436,
Berlin, Heidelberg, Tháng 9 2018. Springer-Verlag. ISBN 978-3-030-01260-1. doi: 10.1007/
978-3-030-01261-8_25.

Ziteng Wang, Jun Zhu, và Jianfei Chen. ReMoE: Fully Differentiable Mixture-of-Experts with
ReLU Routing. Trong The Thirteenth International Conference on Learning Representations, Tháng 10
2024. URL https://openreview.net/forum?id=4D0f16Vwc3 .

--- TRANG 11 ---
Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, và Jimmy Lin. DeeBERT: Dynamic Early Exiting
for Accelerating BERT Inference. Trong Dan Jurafsky, Joyce Chai, Natalie Schluter, và Joel
Tetreault, biên tập, Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, trang 2246–2251, Online, Tháng 7 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.204.

Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam
Roberts, và Colin Raffel. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte
Models. Transactions of the Association for Computational Linguistics, 10:291–306, Tháng 3 2022.
ISSN 2307-387X. doi: 10.1162/tacl_a_00461.

Dewen Zeng, Nan Du, Tao Wang, Yuanzhong Xu, Tao Lei, Zhifeng Chen, và Claire Cui. Learning
to Skip for Language Modeling, Tháng 11 2023. URL http://arxiv.org/abs/2311.15436 .

Biao Zhang và Rico Sennrich. Root Mean Square Layer Normalization. Trong H. Wallach, H. Larochelle,
A. Beygelzimer, F. d 'Alché-Buc, E. Fox, và R. Garnett, biên tập, Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc., 2019.

Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, và Zhang Xiong. Mixture of
Attention Heads: Selecting Attention Heads Per Token. Trong Yoav Goldberg, Zornitsa Kozareva,
và Yue Zhang, biên tập, Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, trang 4150–4162, Abu Dhabi, United Arab Emirates, Tháng 12 2022.
Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.278.
