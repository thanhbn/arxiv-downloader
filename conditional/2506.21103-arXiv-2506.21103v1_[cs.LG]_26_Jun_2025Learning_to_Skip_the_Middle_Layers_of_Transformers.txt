# 2506.21103.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/conditional/2506.21103.pdf
# File size: 359133 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2506.21103v1  [cs.LG]  26 Jun 2025Learning to Skip the Middle Layers of Transformers
Tim Lawson∗Laurence Aitchison
School of Engineering Mathematics and Technology
University of Bristol
Bristol, UK
Abstract
Conditional computation is a popular strategy to make Transformers more efficient.
Existing methods often target individual modules (e.g., mixture-of-experts layers)
or skip layers independently of one another. However, interpretability research has
demonstrated that the middle layers of Transformers exhibit greater redundancy,
and that early layers aggregate information into token positions. Guided by these
insights, we propose a novel architecture that dynamically skips a variable number
of layers from the middle outward . In particular, a learned gating mechanism
determines whether to bypass a symmetric span of central blocks based on the input,
and a gated attention mechanism prevents subsequent tokens from attending to
skipped token positions. Residual norms are controlled with a ‘sandwich’ or ‘peri-
layernorm’ scheme and gate sparsity with an adaptive regularization loss. We had
aimed to reduce compute requirements for ‘simpler’ tokens and potentially foster
an emergent multi-level representational hierarchy but, at the scales investigated,
our approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer layers.
We release our code at https://github.com/tim-lawson/skip-middle .
1 Introduction
We want to make Transformers more efficient. This could be achieved by making individual modules
more efficient; for example, many variants on the attention mechanism have been proposed (Dong
et al., 2024). An alternative approach is to reduce the number of parameters activated during inference.
Conditional computation methods decouple a model’s capacity (determined by its total parameter
count) from its inference cost (determined by the subset of active parameters used for a given
input; Bengio et al. 2013, 2016). A prominent example is the replacement of feed-forward network
(FFN) modules with mixture-of-experts (MoE) layers (Shazeer et al., 2017). These methods reduce
computational and memory requirements while enabling parallelization of model components across
multiple devices (Eigen et al., 2014; Lepikhin et al., 2020; Fedus et al., 2022; Dai et al., 2024).
One way to reduce the active parameters is to conditionally apply components of a Transformer
dependent on the input token. Then, we can dynamically allocate less compute resources to tokens that
are ‘easier’ to process. Early exiting methods, where deep networks can make predictions at different
layers, have a long history in vision (Teerapittayanon et al., 2016) and language applications (Elbayad
et al., 2020; Xin et al., 2020). This approach has been used to dynamically skip Transformer layers
beyond a certain depth (Elhoushi et al., 2024; Fan et al., 2024). Other methods skip intermediate
components (Wang et al., 2018), such as individual modules (Csordás et al., 2021; Peroni and
Bertsimas, 2024) or entire layers (Zeng et al., 2023; Raposo et al., 2024).
We argue that it makes more sense to skip the middle layers of Transformers. Multiple researchers
have demonstrated that the middle layers exhibit greater redundancy: for instance, Lad et al. (2024)
∗Correspondence to tim.lawson@bristol.ac.uk .
Preprint. Under review.

--- PAGE 2 ---
EmbedHead
GateGate
BlockBlockBlockBlock
EmbedHead
GateGate
BlockBlockBlockBlock
EmbedHead
GateGate
BlockBlockBlockBlock
Layer 0Layer 1Layer 2Layer 3
Token 0 Token 1 Token 2g(0,0)>0g(0,1)>0
g(1,0)>0g(1,1)= 0
g(2,0)= 0Skip blocks
1–2Skip blocks
0–3
Figure 1: An illustration of our proposed architecture (with four layers or blocks). We compute a
scalar gate value for each token position and block in the first half of the model. If the gate at block ℓ
is zero, we skip the Transformer blocks between ℓandL−ℓfor the token, and prevent other tokens
from attending to its position in the corresponding self-attention modules.
and González et al. (2025) found that when layers are removed or swapped in pre-trained models, the
performance impact is smaller for interventions affecting the more central layers. This redundancy
has been exploited for structured pruning (Fan et al., 2019; Gromov et al., 2024; Men et al., 2024).
Interpretability research has also established that early, middle, and late layers in deep networks have
different functions. In language models, early-layer modules convert token-level representations to
more natural, semantic features (Elhage et al., 2022; Gurnee et al., 2023). Furthermore, Kaplan et al.
(2024) have shown that, for multi-token words, the attention mechanism in early Transformer layers
aggregates information into the residual vector of the final token in the word. Conversely, late-layer
modules convert semantic features into output tokens: near the output of the network, intermediate
states can be decoded to elicit token predictions (nostalgebraist, 2020; Belrose et al., 2023). Internal
activations at the earliest and latest layers are also comparatively distinct from the middle layers
through the lens of sparse dictionary learning (Lawson et al., 2024).
Conversions between tokens and semantic features parallel the development of byte-level architectures
(Xue et al., 2022; Slagle, 2024), which we expect to learn tokenization implicitly. For example,
Pagnoni et al. (2024); Neitemeier et al. (2024); Kallini et al. (2024) instigate a two-layer hierarchy of
byte- and token-level representations. Given Kaplan et al. (2024), we might expect that this hierarchy
could be profitably extended to levels spanning multiple tokens (Ho et al., 2024; Videau et al., 2025).
Guided by these insights, we propose a gating mechanism that skips a variable number of Transformer
blocks from the middle outward, dependent on the input token. In this way, we can allocate less
compute resources to ‘simpler’ inputs by skipping the middle layers, which are more likely to be
redundant. The more central the layer, the fewer tokens it processes, allowing a multi-level hierarchy
of representations to emerge. Unfortunately, at the scales we were able to investigate, this architecture
does not improve the trade-off between language-modeling performance and the computational
resources required, measured in terms of the estimated FLOPs at inference time were we able to
achieve the maximum benefit from the sparsity of the gate values.
2 Model architecture
A standard decoder-only Transformer has Llayers or blocks, each of which comprises a self-attention
and FFN module. We denote the input to layer ℓat token position ibyh(i,ℓ)∈Rdwhere dis the
model dimension, and the inputs at all token positions i∈1..NbyH(ℓ)∈RN×D.
2

--- PAGE 3 ---
At a high level, the standard Transformer is given by (omitting layer normalization):
h(i,0)= Embed( i)
a(i,ℓ)=h(i,ℓ−1)+ Attn( H(ℓ−1))
h(i,ℓ)=a(i,ℓ)+ FFN( a(i,ℓ))
y(i)= Head( h(i,L)).
We propose to modify this architecture like so:
a(i,ℓ)=h(i,ℓ−1)+g(i,ℓ)GatedAttn ( H(ℓ−1),g(ℓ))
h(i,ℓ)=a(i,ℓ)+g(i,ℓ)FFN( a(i,ℓ)).
The modifications are highlighted . If the gate value g(i,ℓ)∈Rfor a token position iis zero, then we
do not need to compute the corresponding outputs of the attention and feed-forward modules. The
gating mechanism thus functions as a kind of router (Figure 1).
2.1 Gating mechanism
For each block ℓ < L/ 2in the first half of the model, we introduce a linear layer that outputs a scalar
soft mask value s(i,ℓ)≥0, which accumulates over the first half of the model:
s(i,ℓ)= ReLU
w(ℓ)·h(i,ℓ)+b(ℓ)
, S(i,ℓ)=X
ℓ′≤ℓs(i,ℓ′). (1)
When the accumulated soft mask value S(i,ℓ)≥1, we skip processing the residual vector at token
position iby the Transformer blocks [ℓ, L−ℓ). Hence, for each block ℓ≥L/2in the second half of
the model, we use the accumulated soft mask value S(i,L−ℓ−1)of the opposing block.
The corresponding scalar gatevalue g(i,ℓ)∈[0,1]is:
g(i,ℓ)=(
1−clamp 
S(i,ℓ),0,1
ifℓ < L/ 2
1−clamp 
S(i,L−ℓ−1),0,1
ifℓ≥L/2(2)
The sparsity of the gate values g(i,ℓ)determines the reduction in the number of active parameters: for
a single token i, it is the number of blocks at which the gate value is zero multiplied by the number
of parameters in a Transformer block NB. With multiple tokens, the reduction is 2NBP
ℓ<L/ 2zℓ,
where zℓis the sparsity of the gate values before tokens are processed by block ℓ.
The gating mechanism introduces (d+ 1)L/2parameters w(ℓ)andb(ℓ), i.e., d+ 1for each block
ℓ < L/ 2in the first half of the model. If all these parameters are zero, all the gate values equal one
and we exactly recover the equivalent dense Transformer (which forms the baselines in Section 3).
2.2 Gated attention
We also prevent other tokens from attending to gated tokens in attention modules. The GatedAttn
module we incorporate modifies the attention mechanism such that, when the gate value for a token
is zero, subsequent tokens do not attend to the gated token:
oi=P
j<igjexp(qT
ikj)vjP
j<igjexp(qT
ikj)(3)
This is equivalent to adding lngjto the pre-softmax attention logits, and can be implemented
straightforwardly as a score modification within the FlexAttention framework (Dong et al., 2024). In
practice, we apply a lower bound of ϵ= 1×10−6togjbefore lnto prevent infinities.
Our attention mechanism is similar to the ‘Forgetting Attention’ proposed by Lin et al. (2024), except
that we compute a single gate value that applies to every attention head, whereas they compute a gate
value for each attention head. We require a single gate value to decide whether to prevent an entire
Transformer block from processing the token.
3

--- PAGE 4 ---
Name Loss Update rule
sparsity1
LPL
ℓ=1αℓgℓ -
sparsity_variance
1
LPL
ℓ=1 
αℓgℓ+βℓs2
ℓ-
adaptive αi+1=αi+γsign(gℓ−µ∗
ℓ)
proportional αi+1=αi+γ(gℓ−µ∗
ℓ)
sparsity_variance_l21
LPL
ℓ=1
αℓ∥gℓ−µ∗
ℓ∥2
2+βℓs2
ℓ−σ2∗
ℓ2
2
Table 1: Alternative techniques to control the sparsity of the gate values. Recall that gℓands2
ℓare
the mean and variance of the gate values over the token positions in a batch, and µ∗
ℓandσ2∗
ℓare the
targets at layer ℓfor the mean and variance of the gate values, respectively. For the techniques with
adaptive coefficients, αℓandβℓare updated by the same algorithm.
2.3 Layer normalization
Modern Transformers typically use the ‘pre-layernorm’ scheme (pre-LN), where layer normalization
operations are applied to the residual inputs to the attention and FFN modules:
y=x+ Module(Norm( x)).
With this scheme, the norms of residual activation vectors grow with depth and later modules produce
outputs with greater norms (Lawson et al., 2024; Csordás et al., 2024a; Kim et al., 2025).
The gating mechanism we propose effectively introduces skip connections between opposing pairs of
Transformer blocks (Section 2.1). Like Csordás et al. (2024a), who consider a Universal Transformer
(UT) with a single, shared block, we want later modules to accept the outputs of early and late blocks.
We address this problem by using the ‘sandwich’ LN scheme proposed in Ding et al. (2021), called
‘peri-layernorm’ by Kim et al. (2025), where layer normalization operations are applied to both the
residual input to andoutput of the attention and FFN modules:
y=x+ Norm(Module(Norm( x))).
This scheme differs from the ‘peri-layernorm’ of Csordás et al. (2024a), who apply a layer normaliza-
tion operation “around (but not on) the residual connections.”
2.4 Controlling sparsity
Our gated architecture reduces the number of parameters activated during the forward pass propor-
tional to the fraction of gate values that are exactly zero, i.e., the mean gate sparsity (Section 2.1).
With only the standard cross-entropy loss, optimization tends to activate more parameters to improve
performance, so we need to control the sparsity of the gate values.
We achieve this by introducing a regularization loss based on the mean and variance of the gate values,
with adaptive coefficients updated proportional to the deviations of the mean and variance from
layer-wise targets. The mean term incentivizes smaller gate values; the variance term incentivizes a
non-uniform distribution such that some (but not all) gate values are zero.
We denote the mean and variance of the gate values over the token positions in a batch by:
gℓ=1
NNX
i=1g(i,ℓ), s2
ℓ=1
NNX
i=1
g(i,ℓ)−gℓ2
. (4)
The targets at layer ℓfor the population mean and variance of the gate values over token positions
areµ∗
ℓandσ2∗
ℓ, respectively. Except where noted, we choose the mean targets µ∗
ℓas linearly spaced
values between an initial target µ∗
0and a final target µ∗
L/2. We choose the variance targets σ2∗
ℓas the
variance of the Bernoulli distribution with p=µ∗
ℓ, i.e., σ2∗
ℓ=µ∗
ℓ(1−µ∗
ℓ).
4

--- PAGE 5 ---
2 3 4 5 6
·10113.23.33.43.53.6
Inference FLOPsCross-entropy
0 0.1 0.2 0.31284
1062
SparsityDense baseline
Gated (without control)
Gated (with control)
Figure 2: Performance comparisons between our gated Transformer architecture and baseline models
with between 2 and 12 layers (labeled). All gated models with control are variants of the 12-layer
architecture. We measured cross-entropy over 100M tokens from the FineWeb validation set. The
estimated FLOPs for a single forward pass (left) assume that the maximum computational benefit is
achieved from the final sparsity of the gate values over the validation set (right).
We denote the adaptive coefficients for the mean and variance of the gate values at layer ℓbyαℓand
βℓ, respectively, and initialize the coefficients to zero. The regularization loss is then:
L=1
LLX
ℓ=1 
αℓgℓ+βℓs2
ℓ
. (5)
After every training step, we update each coefficient by the following rule:
αℓ,i+1=(
αℓ,i+γ(gℓ−µ∗
ℓ)if(gℓ−µ∗
ℓ)> δ
αℓ,i otherwise(6)
The updates are thus proportional to the differences from the target values. We choose the update
multiplier γ= 1×10−3and tolerance δ= 1×10−2based on observations in small-scale experiments.
We explored alternative control mechanisms (Table 1), but these performed worse empirically.
3 Results
We evaluated the performance of our gated Transformer architecture in terms of the validation cross-
entropy when pre-trained on the FineWeb dataset (Penedo et al., 2024). As baselines, we trained
equivalent dense models with no gating mechanism and between 2 and 12 layers. We measured the
compute requirements of each model in terms of the estimated floating-point operations (FLOPs) for
a single forward pass (batch size 1), assuming that we were able to achieve the maximum possible
benefit due to the sparsity of the gate values. The actual compute requirements of the gated and dense
models are similar. Except where noted, we used the hyperparameters in Table 2.
Figure 2 shows that, without controlling the sparsity of the gates, the mean sparsity tends toward
zero and gated models perform similarly to dense baselines (right). With the initial target for the
mean gates µ∗
0fixed to 1, as we decrease the final target µ∗
L/2from 1 to 0, the sparsity increases
and the estimated FLOPs decrease (left). However, the proposed architecture does not improve the
cross-entropy over dense baselines with fewer layers.
5

--- PAGE 6 ---
Parameter Value
model
dim 768
n_layers 12
n_heads 12
n_kv_heads 12
vocab_size 50 257
ffn_dim_multiplier 4
multiple_of 256
norm_eps 1×10−5
rope_theta 10 000
use_scaled_rope False
max_seq_len 1024
initializer_range 0.02Parameter Value
data
batch_size 512
device_batch_size 32
optimizer
lr 0.001
beta1 0.8
beta2 0.95
eps 1×10−10
weight_decay 0
scheduler
warmup_steps 0.1
start_factor 0.1
Table 2: Default hyperparameters. The Transformer model architecture is based on Llama 3
(Grattafiori et al., 2024) with similar dimensions to GPT-2 small (Radford et al., 2019); we used the
AdamW optimizer (Loshchilov and Hutter, 2019) with linear warm-up and cosine decay.
3.1 Experimental details
We trained all models on a randomly-sampled subset of the FineWeb dataset with approximately
10B tokens (Penedo et al., 2024), pre-tokenized with the GPT-2 tokenizer via the TikToken library
(Radford et al., 2019; Ouyang et al., 2022). The validation set contained approximately 100M tokens.
We used a global batch size of 512 sequences ( 524 288 tokens) with data parallelism and gradient
accumulation over a per-device batch size of 32 sequences on 4 NVIDIA A100 or GH200 GPUs.
We based the underlying Transformer models on the reference implementation of Llama 3 (Grattafiori
et al., 2024). In particular, we used: Grouped Query Attention (GQA; Ainslie et al. 2023); Rotary
Positional Embeddings (RoPE; Su et al. 2024); Gated Linear Unit FFNs with Swish activation
(SwiGLU; Shazeer 2020); and Root Mean Square (RMSNorm) layer normalization (Zhang and
Sennrich, 2019). The key difference relative to Llama 3 is that we used the Sandwich-LN scheme
(Ding et al., 2021; Kim et al., 2025) instead of Pre-LN. We initialized RMSNorm parameters to one
and sampled all others from the normal distribution with mean zero and standard deviation 0.02.
The training codebase is based on the ‘nanoGPT speedrun’ repository (Karpathy, 2025; Jordan, 2025).
We used the AdamW optimizer with a single learning rate for all model parameters (Kingma and Ba,
2017; Loshchilov and Hutter, 2019), and a two-stage learning-rate scheduler with linear warm-up
over 10% of the training steps, starting at 10% of the maximum learning rate, and cosine decay
over the remaining steps. Lastly, we performed forward passes in bfloat16 with automatic mixed
precision in PyTorch (except manually converting attention logits to float32 ).
4 Related work
Conditional computation decouples a model’s total parameter count from its inference cost by
activating only a subset of parameters for a given input (Bengio et al., 2013; Eigen et al., 2014;
Bengio et al., 2016). A prominent application of this principle is the use of Mixture-of-Experts layers,
which replace FFN modules with a larger set of ‘expert’ sub-networks, of which only a few are
selected by a router to process each input (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al.,
2022; Dai et al., 2024). While related methods like Zhang et al. (2022); Csordás et al. (2024b); Jin
et al. (2024) can be effective, they operate on individual modules (e.g., FFNs or attention) and often
use routing strategies that enforce a fixed computational budget per token (cf. Wang et al., 2024). Our
approach differs by applying conditional computation to entire Transformer blocks, and dynamically
allocating a variable number of blocks to each token based on its processing needs, a strategy that is
also compatible with module-level techniques.
6

--- PAGE 7 ---
Another line of work to improve efficiency is dynamically altering the network depth. Early exiting
methods allow a model to generate predictions at intermediate layers, halting computation for ‘easy’
inputs (Teerapittayanon et al., 2016; Elbayad et al., 2020; Xin et al., 2020). More recent variants
of this approach dynamically skip all layers beyond a certain depth (Elhoushi et al., 2024; Fan
et al., 2024). In contrast, our method is motivated by empirical findings that the middle layers of
Transformers exhibit greater redundancy (Lad et al., 2024; González et al., 2025). These findings
have been leveraged by structured pruning, which removes layers statically after training (Fan et al.,
2019; Gromov et al., 2024; Men et al., 2024). Our work is distinct in that it targets the more redundant
middle layers for skipping and does so dynamically during inference.
Several methods have explored skipping entire Transformer blocks. For instance, the copy gate pro-
posed by Csordás et al. (2021) modulates a block’s contribution, but still requires the full computation
of the block’s output. Our gating mechanism, however, ensures that computation for skipped blocks
could be avoided entirely. Mixture-of-Depths (MoD) models (Raposo et al., 2024) process only the
top-ktokens at each block, enforcing a fixed computational budget for the sequence. Our method
is different because it determines the computational depth for each token individually, allowing it
to adapt to token-specific complexity rather than a sequence-wide budget. Approaches like Skip-
Net (Wang et al., 2018) also enable layer skipping but are not specifically designed for the unique
redundancy patterns observed in the middle layers of Transformers.
Our gated attention mechanism prevents attention to tokens that have been masked out, which is
functionally similar to the ‘Forgetting Attention’ proposed by Lin et al. (2024). However, their
approach computes separate gates for each attention head, whereas our approach uses a single gate
per token to decide whether to skip an entire Transformer block, which is essential for the block-level
computational savings we target.
Finally, our work relates to hierarchical Transformers that process sequences at multiple predefined
levels (e.g., byte- and token-level; Pagnoni et al. 2024; Neitemeier et al. 2024; Kallini et al. 2024).
Our middle-outward skipping approach offers the potential for a more flexible, emergent hierarchy,
where deeper layers process a dynamically determined subset of more complex representations.
5 Conclusion
We introduced a novel Transformer architecture that dynamically skips a variable number of middle
layers, guided by interpretability research suggesting these layers are the most redundant. The
mechanism uses a learned gate to bypass a symmetric span of central blocks based on the input token,
with the goals of reducing compute for simpler tokens and allowing a representational hierarchy to
emerge. Our experiments showed that, at small scales, this architecture did not yield improvements
in the trade-off between validation performance and estimated inference FLOPs when compared to
simply training dense baseline models with fewer layers. The anticipated benefits of this architectural
prior may only become apparent at significantly larger model scales, where the redundancy of middle
layers is more pronounced and the relative overhead of the gating mechanism is smaller. Despite
these results, we believe the principle of using insights from model internals to design more efficient
and structured architectures remains a valuable direction for future research.
References
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit
Sanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Check-
points. In The 2023 Conference on Empirical Methods in Natural Language Processing , December
2023. URL https://openreview.net/forum?id=hmOwOZWzYE .
Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella
Biderman, and Jacob Steinhardt. Eliciting Latent Predictions from Transformers with the Tuned
Lens, November 2023. URL http://arxiv.org/abs/2303.08112 .
Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional Computation in
Neural Networks for faster models, January 2016. URL http://arxiv.org/abs/1511.06297 .
7

--- PAGE 8 ---
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or Propagating Gradients
Through Stochastic Neurons for Conditional Computation, August 2013. URL http://arxiv.
org/abs/1308.3432 .
Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The Neural Data Router: Adaptive Control
Flow in Transformers Improves Systematic Generalization. In International Conference on
Learning Representations , October 2021. URL https://openreview.net/forum?id=KBQP4A_
J1K.
Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Christopher Potts, and Christopher D. Manning.
MoEUT: Mixture-of-Experts Universal Transformers. Advances in Neural Information Processing
Systems , 37:28589–28614, December 2024a.
Róbert Csordás, Piotr Pi˛ ekos, Kazuki Irie, and Jürgen Schmidhuber. SwitchHead: Accelerating
Transformers with Mixture-of-Experts Attention. Advances in Neural Information Processing
Systems , 37:74411–74438, December 2024b.
Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding
Zeng, Xingkai Yu, Y . Wu, Zhenda Xie, Y . K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang
Sui, and Wenfeng Liang. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-
Experts Language Models, January 2024. URL http://arxiv.org/abs/2401.06066 .
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,
Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering Text-to-Image Generation via
Transformers, November 2021. URL http://arxiv.org/abs/2105.13290 .
Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex Attention: A
Programming Model for Generating Optimized Attention Kernels, December 2024. URL http:
//arxiv.org/abs/2412.05496 .
David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning Factored Representations in a
Deep Mixture of Experts, March 2014. URL http://arxiv.org/abs/1312.4314 .
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-Adaptive Transformer, February
2020. URL http://arxiv.org/abs/1910.10073 .
Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer
ElShowk, Nicholas Joseph, Nova DasSarma, and Ben Mann. Softmax linear units, 2022. URL
https://transformer-circuits.pub/2022/solu/index.html .
Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai,
Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed A. Aly, Beidi Chen, and
Carole-Jean Wu. LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding. In
Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) , pages 12622–12642, 2024. doi: 10.18653/v1/2024.acl-long.681.
Angela Fan, Edouard Grave, and Armand Joulin. Reducing Transformer Depth on Demand with
Structured Dropout, September 2019. URL http://arxiv.org/abs/1909.11556 .
Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and
Zhongyuan Wang. Not All Layers of LLMs Are Necessary During Inference, July 2024. URL
http://arxiv.org/abs/2403.02181 .
William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion Parameter
Models with Simple and Efficient Sparsity. Journal of Machine Learning Research , 23(120):1–39,
2022. ISSN 1533-7928. URL http://jmlr.org/papers/v23/21-0998.html .
Ramón Calvo González, Daniele Paliotta, Matteo Pagliardini, Martin Jaggi, and François Fleuret.
Leveraging the true depth of LLMs, February 2025. URL http://arxiv.org/abs/2502.02790 .
Aaron Grattafiori, Abhimanyu Dubey, et al. The Llama 3 Herd of Models, November 2024. URL
http://arxiv.org/abs/2407.21783 .
8

--- PAGE 9 ---
Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Dan Roberts. The Unrea-
sonable Ineffectiveness of the Deeper Layers. In The Thirteenth International Conference on Learn-
ing Representations , October 2024. URL https://openreview.net/forum?id=ngmEcEer8a .
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas.
Finding Neurons in a Haystack: Case Studies with Sparse Probing, June 2023. URL http:
//arxiv.org/abs/2305.01610 .
Namgyu Ho, Sangmin Bae, Taehyeon Kim, hyunjik.jo, Yireun Kim, Tal Schuster, Adam Fisch, James
Thorne, and Se-Young Yun. Block Transformer: Global-to-Local Language Modeling for Fast
Inference. In The Thirty-eighth Annual Conference on Neural Information Processing Systems ,
November 2024. URL https://openreview.net/forum?id=6osgTNnAZQ .
Peng Jin, Bo Zhu, Li Yuan, and Shuicheng Yan. MoH: Multi-Head Attention as Mixture-of-Head
Attention, October 2024. URL http://arxiv.org/abs/2410.11842 .
Keller Jordan. KellerJordan/modded-nanogpt, May 2025. URL https://github.com/
KellerJordan/modded-nanogpt .
Julie Kallini, Shikhar Murty, Christopher D. Manning, Christopher Potts, and Róbert Csordás. MrT5:
Dynamic Token Merging for Efficient Byte-level Language Models. In The Thirteenth International
Conference on Learning Representations , October 2024. URL https://openreview.net/forum?
id=VYWBMq1L7H .
Guy Kaplan, Matanel Oren, Yuval Reif, and Roy Schwartz. From Tokens to Words: On the Inner
Lexicon of LLMs. In The Thirteenth International Conference on Learning Representations ,
October 2024. URL https://openreview.net/forum?id=328vch6tRs .
Andrej Karpathy. karpathy/nanoGPT, May 2025. URL https://github.com/karpathy/nanoGPT .
Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo,
Seongjin Shin, Dongyoon Han, Jinwoo Shin, and Kang Min Yoo. Peri-LN: Revisiting Nor-
malization Layer in the Transformer Architecture. In Forty-second International Conference
on Machine Learning , June 2025. URL https://openreview.net/forum?id=ci1S6wmXfO&
noteId=r07RHYqMC5 .
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization, January 2017.
URL http://arxiv.org/abs/1412.6980 .
Vedang Lad, Wes Gurnee, and Max Tegmark. The Remarkable Robustness of LLMs: Stages of
Inference?, June 2024. URL http://arxiv.org/abs/2406.19384 .
Tim Lawson, Lucy Farnik, Conor Houghton, and Laurence Aitchison. Residual Stream Analysis
with Multi-Layer SAEs. In The Thirteenth International Conference on Learning Representations ,
October 2024. URL https://openreview.net/forum?id=XAjfjizaKs .
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling Giant Models with Conditional
Computation and Automatic Sharding, June 2020. URL http://arxiv.org/abs/2006.16668 .
Zhixuan Lin, Evgenii Nikishin, Xu He, and Aaron Courville. Forgetting Transformer: Softmax Atten-
tion with a Forget Gate. In The Thirteenth International Conference on Learning Representations ,
October 2024. URL https://openreview.net/forum?id=q2Lnyegkr8 .
Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization, January 2019. URL
http://arxiv.org/abs/1711.05101 .
Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and
Weipeng Chen. ShortGPT: Layers in Large Language Models are More Redundant Than You
Expect, October 2024. URL http://arxiv.org/abs/2403.03853 .
Pit Neitemeier, Björn Deiseroth, Constantin Eichenberg, and Lukas Balles. Hierarchical Autoregres-
sive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language
Models. In The Thirteenth International Conference on Learning Representations , October 2024.
URL https://openreview.net/forum?id=tU074jg2vS .
9

--- PAGE 10 ---
nostalgebraist. Interpreting GPT: the logit lens, August 2020. URL https://www.lesswrong.com/
posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens .
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe. Training language models to follow instructions with human feedback, March 2022.
URL http://arxiv.org/abs/2203.02155 .
Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li,
Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman,
and Srinivasan Iyer. Byte Latent Transformer: Patches Scale Better Than Tokens, December 2024.
URL http://arxiv.org/abs/2412.09871 .
Guilherme Penedo, Hynek Kydlí ˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin
Raffel, Leandro V on Werra, and Thomas Wolf. The FineWeb Datasets: Decanting the Web for the
Finest Text Data at Scale, October 2024. URL http://arxiv.org/abs/2406.17557 .
Matthew Peroni and Dimitris Bertsimas. Skip Transformers: Efficient Inference through Skip-Routing.
October 2024. URL https://openreview.net/forum?id=gdMJlwTcSQ .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language Models are Unsupervised Multitask Learners, 2019. URL https:
//cdn.openai.com/better-language-models/language_models_are_unsupervised_
multitask_learners.pdf .
David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam
Santoro. Mixture-of-Depths: Dynamically allocating compute in transformer-based language
models, April 2024. URL http://arxiv.org/abs/2404.02258 .
Noam Shazeer. GLU Variants Improve Transformer, February 2020. URL http://arxiv.org/abs/
2002.05202 .
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,
January 2017. URL http://arxiv.org/abs/1701.06538 .
Kevin Slagle. SpaceByte: Towards Deleting Tokenization from Large Language Modeling. In The
Thirty-eighth Annual Conference on Neural Information Processing Systems , November 2024.
URL https://openreview.net/forum?id=KEe4IUp20I .
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. RoFormer:
Enhanced transformer with Rotary Position Embedding. Neurocomput. , 568(C), February 2024.
ISSN 0925-2312. doi: 10.1016/j.neucom.2023.127063.
Surat Teerapittayanon, Bradley McDanel, and H.T. Kung. BranchyNet: Fast Inference via Early
Exiting from Deep Neural Networks. In 2016 23rd International Conference on Pattern Recognition
(ICPR) , pages 2464–2469, December 2016. doi: 10.1109/ICPR.2016.7900006.
Mathurin Videau, Badr Youbi Idrissi, Alessandro Leite, Marc Schoenauer, Olivier Teytaud, and David
Lopez-Paz. From Bytes to Ideas: Language Modeling with Autoregressive U-Nets, 2025. URL
https://arxiv.org/abs/2506.14761 .
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez. SkipNet: Learning
Dynamic Routing in Convolutional Networks. In Computer Vision – ECCV 2018: 15th European
Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII , pages 420–436,
Berlin, Heidelberg, September 2018. Springer-Verlag. ISBN 978-3-030-01260-1. doi: 10.1007/
978-3-030-01261-8_25.
Ziteng Wang, Jun Zhu, and Jianfei Chen. ReMoE: Fully Differentiable Mixture-of-Experts with
ReLU Routing. In The Thirteenth International Conference on Learning Representations , October
2024. URL https://openreview.net/forum?id=4D0f16Vwc3 .
10

--- PAGE 11 ---
Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. DeeBERT: Dynamic Early Exiting
for Accelerating BERT Inference. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel
Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics , pages 2246–2251, Online, July 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.204.
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam
Roberts, and Colin Raffel. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte
Models. Transactions of the Association for Computational Linguistics , 10:291–306, March 2022.
ISSN 2307-387X. doi: 10.1162/tacl_a_00461.
Dewen Zeng, Nan Du, Tao Wang, Yuanzhong Xu, Tao Lei, Zhifeng Chen, and Claire Cui. Learning
to Skip for Language Modeling, November 2023. URL http://arxiv.org/abs/2311.15436 .
Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d 'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information
Processing Systems , volume 32. Curran Associates, Inc., 2019.
Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, and Zhang Xiong. Mixture of
Attention Heads: Selecting Attention Heads Per Token. In Yoav Goldberg, Zornitsa Kozareva,
and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing , pages 4150–4162, Abu Dhabi, United Arab Emirates, December 2022.
Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.278.
11
