A.2 LỜI NHẮC TẠO PHẢN HỒI VÀ TINH CHỈNH CHO KHO DỮ LIỆU META-KỸ NĂNG

Chúng tôi giới thiệu lời nhắc để tạo ra phản hồi và tinh chỉnh trong hai lĩnh vực: Toán học và Tổng quát. Chúng tôi phác thảo các lời nhắc cụ thể được thiết kế để hướng dẫn việc đánh giá và cải thiện phản hồi cho các câu hỏi để xây dựng D_meta trong mỗi lĩnh vực.

A.2.1 LĨNH VỰC TOÁN HỌC

Đối với Lĩnh vực Toán học, lời nhắc hướng dẫn người đánh giá đánh giá chất lượng phản hồi cho một câu hỏi toán, cung cấp phân tích từng bước, và xác định tính đúng đắn của nó. Nếu phản hồi không chính xác, người đánh giá được yêu cầu tinh chỉnh và cung cấp câu trả lời đúng.

Lời nhắc cho phản hồi và tinh chỉnh:
(Phản hồi) Vui lòng đánh giá chất lượng phản hồi cho câu hỏi đã cho.
Đây là câu hỏi: p.
Đây là phản hồi: r.
Đầu tiên, cung cấp phân tích từng bước và xác minh cho phản hồi bắt đầu với "Phân tích Phản hồi:".
Tiếp theo, đánh giá xem phản hồi có trả lời đúng câu hỏi hay không theo định dạng "phán đoán: đúng/sai".
(Tinh chỉnh) Nếu câu trả lời đúng, xuất ra nó. Ngược lại, xuất ra câu trả lời tinh chỉnh dựa trên phản hồi đã cho và đánh giá của bạn.

A.2.2 LĨNH VỰC TỔNG QUÁT

Đối với thử nghiệm tổng quát, phù hợp với phương pháp được mô tả trong phần 3, chúng tôi triển khai lời nhắc sau để hướng dẫn người chú thích dựa trên LLM tạo ra phản hồi và tinh chỉnh phản hồi. Lời nhắc này phục vụ như nền tảng cho kho dữ liệu học meta-kỹ năng và hỗ trợ tạo ra dữ liệu huấn luyện tự tiến hóa trong cài đặt thử nghiệm tổng quát.

Lời nhắc cho phản hồi và tinh chỉnh:
(Phản hồi) Vui lòng đánh giá chất lượng phản hồi cho câu hỏi đã cho.
Đây là câu hỏi: p.
Đây là phản hồi: r.
Đầu tiên cung cấp phân tích và xác minh cho phản hồi bắt đầu với "Phân tích Phản hồi:".
Tiếp theo, sau đó đánh giá phản hồi trên thang điểm từ 1 đến 10 (1 là tệ nhất, 10 là tốt nhất) theo định dạng "Đánh giá:"
(Tinh chỉnh) Cuối cùng xuất ra một câu trả lời cải thiện dựa trên phân tích của bạn nếu không có phản hồi nào được đánh giá 10.

A.3 TẠO DỮ LIỆU

A.3.1 SỐ LƯỢNG DỮ LIỆU D_META

Bộ dữ liệu D_meta được tạo ra sử dụng 3.5k lời nhắc chưa được gán nhãn từ GSM8K và 2K từ SVAMP3. Đối với thử nghiệm tổng quát, 6K cuộc hội thoại được chọn từ 90K đối thoại ShareGPT để tạo thành dữ liệu D_meta tổng quát.

A.3.2 LỜI NHẮC CHƯA ĐƯỢC GÁN NHÃN CHO HUẤN LUYỆN TỰ TIẾN HÓA

Lĩnh vực Toán học: Đối với thử nghiệm toán, các lời nhắc chưa được gán nhãn trong huấn luyện tự tiến hóa được lấy nguồn như sau:
(1) Giai đoạn tự tiến hóa vòng đầu tiên: 4K lời nhắc còn lại từ GSM8k và 1K từ SVAMP, loại trừ những lời nhắc được sử dụng trong D_meta.
(2) Vòng Thứ hai/Thứ ba: 10K/15K lời nhắc được tạo ra sử dụng phương pháp Self-Instruct (Wang et al., 2022b), dựa trên mẫu được hiển thị trong phụ lục A.3.2 với 4 đến 6 ví dụ mồi ban đầu.

Lĩnh vực Tổng quát: 15K lời nhắc chưa được gán nhãn từ đối thoại ShareGPT được sử dụng cho xây dựng dữ liệu huấn luyện tự tiến hóa.

Bạn là một người tạo hướng dẫn có kinh nghiệm. Bạn được yêu cầu phát triển 3 hướng dẫn đa dạng theo các ví dụ đã cho.
Đây là các yêu cầu:
1. Các hướng dẫn được tạo ra nên tuân theo loại tác vụ trong các ví dụ đã cho.
2. Ngôn ngữ được sử dụng cho các hướng dẫn được tạo ra nên đa dạng.
Các ví dụ đã cho: {examples}
Các hướng dẫn được tạo ra nên là:
A. ...
B. ...
C. ...

A.4 SIÊU THAM SỐ HUẤN LUYỆN

Các thí nghiệm của chúng tôi được thực hiện trong môi trường tính toán với 8 GPU NVIDIA V100, mỗi GPU có 32GB bộ nhớ. Tất cả các mô hình được điều chỉnh tinh tế trong cài đặt tham số đầy đủ. Chúng tôi sử dụng bộ tối ưu AdamW để huấn luyện mô hình qua 3 epoch, với kích thước batch là 128. Tốc độ học được đặt ở 2e-5, bao gồm giai đoạn khởi động tốc độ học 3%. Dưới đây chúng tôi cung cấp tổng quan toàn diện về các siêu tham số huấn luyện được sử dụng trong bảng 5. Những tham số này được áp dụng đồng nhất trên tất cả các phương pháp huấn luyện trong thí nghiệm của chúng tôi.

Bảng 5: Siêu tham số huấn luyện.

Siêu tham số | Kích thước Batch Toàn cục | LR | Epoch | Độ dài Tối đa | Suy giảm Trọng số | Tỷ lệ Khởi động
Giá trị | 128 | 2×10^-5 | 3 | 2048 | 0 | 0.03

A.5 PHÂN TÍCH NGHIÊN CỨU TRƯỜNG HỢP

Phần phụ này cung cấp nghiên cứu trường hợp sâu sắc đối chiếu hiệu suất của các mô hình Vicuna gốc và Vicuna + SELF. Được minh họa trong hình 4, cả hai mô hình thực hiện dự đoán ban đầu, theo sau bởi các bước tự phản hồi và tinh chỉnh. Đáng chú ý, việc tinh chỉnh của Vicuna thất bại trong việc sửa các lỗi ban đầu của nó, trong khi Vicuna + SELF hiệu quả sử dụng tự phản hồi và tinh chỉnh để đưa ra một câu trả lời chính xác và logic mạch lạc.

A.6 KHO DỮ LIỆU HUẤN LUYỆN META-KỸ NĂNG

Ví dụ được hiển thị dưới đây minh họa một ví dụ huấn luyện tiêu chuẩn từ kho dữ liệu meta-kỹ năng của chúng tôi. Nó minh họa phản hồi ban đầu của mô hình, theo sau bởi tự phản hồi của nó, và việc tinh chỉnh tiếp theo. Quá trình này chứng minh cách mô hình được huấn luyện cho khả năng tự phản hồi và tự tinh chỉnh.

[Ví dụ chi tiết về câu hỏi, phản hồi ban đầu, phản hồi và tinh chỉnh được giữ nguyên như trong bản gốc]

A.7 THUẬT TOÁN

Thuật toán "Quá trình SELF Hai Giai đoạn" phác thảo một phương pháp để phát triển một mô hình ngôn ngữ cơ bản thông qua phương pháp hai giai đoạn: Học Meta-Kỹ năng và Tự Tiến hóa. Quá trình bắt đầu với việc huấn luyện trên "kho dữ liệu Học Meta-Kỹ năng", bao gồm dữ liệu đại diện cho quá trình tạo sinh, phản hồi và tinh chỉnh. Tiếp theo điều này, mô hình bước vào "Giai đoạn Tự Tiến hóa", nơi nó trải qua các tinh chỉnh lặp lại, sử dụng tăng cường dữ liệu trong mỗi lần lặp để tạo ra đầu ra tự tinh chỉnh từ các phiên bản được tinh chỉnh trước đó của nó. Sự tự tiến hóa lặp lại này nhằm tận dụng kiến thức tích lũy và nâng cao thêm mô hình với dữ liệu mới được tạo ra. Kết quả cuối cùng là một Mô hình Ngôn ngữ tiên tiến đã tiến hóa đáng kể từ trạng thái ban đầu của nó thông qua nhiều giai đoạn tự tiến hóa. Chi tiết thêm được phác thảo trong Thuật toán 1.

[Thuật toán 1 được giữ nguyên cấu trúc như bản gốc với các từ khóa tiếng Anh cho các hàm]

A.8 TIÊU CHUẨN LỌC DỮ LIỆU

Chúng tôi thiết kế một hàm boolean, qualified(f), để đánh giá phản hồi f trên các lĩnh vực khác nhau, xác định xem một phản hồi cho một lời nhắc cụ thể có thỏa mãn các tiêu chí chất lượng cần thiết hay không.

Trong Lĩnh vực Toán học, hàm đánh giá phản hồi dựa trên tuyên bố rõ ràng về "tính đúng đắn" trong phán đoán của người đánh giá, phù hợp với cấu trúc lời nhắc trong phụ lục A.2.1. Nó kiểm tra xem từ "đúng" có xuất hiện ngay sau cụm từ "phán đoán:" trong phản hồi hay không. Sự hiện diện của "đúng" dẫn đến qualified(f) trả về 1, đáp ứng tiêu chí đủ điều kiện. Vắng mặt dẫn đến trả về 0.

Đối với Lĩnh vực Tổng quát, tuân theo cấu trúc trong phụ lục A.2.2, qualified(f) trích xuất và đánh giá một đánh giá số từ phản hồi. Nếu đánh giá, được tìm thấy sau "Đánh giá:", là 7 hoặc cao hơn, hàm trả về 1, chỉ ra đủ điều kiện. Đánh giá dưới 7 trả về 0, không đáp ứng ngưỡng. Đánh giá 7 cân bằng chất lượng và số lượng dữ liệu huấn luyện.

qualified(f) là chìa khóa trong cả hai lĩnh vực để lọc và đánh giá chất lượng phản hồi, đảm bảo chỉ những phản hồi chất lượng cao được sử dụng cho việc tạo ra câu trả lời tinh chỉnh trong huấn luyện tự tiến hóa. Sau lọc dữ liệu, Ψ^{t-1} trong phương trình (3) yêu cầu cập nhật thành Ψ'^{t-1} = Ψ^{t-1} × qualified(f), thêm bộ lọc chất lượng thông qua tự phản hồi. Để rõ ràng, chúng tôi tiếp tục sử dụng công thức ban đầu như được nêu trong phương trình (3) trong văn bản chính.

A.9 NHIỀU SO VỚI ĐƠN TỰ HOÀN THIỆN

Nghiên cứu này khám phá hiệu ứng của hai chiến lược tổ chức dữ liệu huấn luyện meta-kỹ năng đối với hiệu suất mô hình: (1) Nhiều Tự Hoàn thiện (D_meta-multi), bao gồm việc lấy mẫu ba phản hồi để mô hình chọn tốt nhất để tinh chỉnh, và (2) Đơn Tự Hoàn thiện (D_meta), nơi mô hình tạo ra và tinh chỉnh một phản hồi duy nhất.

Bảng 6 so sánh hiệu suất của những phương pháp này. Cả hai chiến lược đều cho thấy mức tăng hiệu suất với khối lượng dữ liệu huấn luyện tăng. Tuy nhiên, khi khối lượng dữ liệu mở rộng, tinh chỉnh phản hồi nhiều lần cho thấy sự cải thiện nhỏ hơn trong hiệu suất tạo sinh trực tiếp (+4.02%) so với phương pháp phản hồi đơn (+5.84%). Xem xét tính đơn giản và hiệu quả tính toán của phương pháp phản hồi đơn, chỉ lấy mẫu một phản hồi trong quá trình suy luận, và hiệu suất tốt hơn của nó so với phương pháp phản hồi nhiều lần, chúng tôi đã chọn chiến lược tinh chỉnh phản hồi đơn trong các thí nghiệm của chúng tôi.

Bảng 6: So sánh hiệu suất của tinh chỉnh phản hồi đơn và nhiều với khối lượng dữ liệu huấn luyện meta-kỹ năng khác nhau. Mũi tên chỉ ra sự cải thiện từ tạo sinh trực tiếp đến tự hoàn thiện: "tạo sinh trực tiếp → tự hoàn thiện".

Kích thước Dữ liệu | Vicuna + D_meta | Vicuna + D_meta-multi
3.5k | 25.39 → 28.28 | 25.92 → 27.29
7.5k | 31.23 → 32.98 | 29.94 → 32.14

A.10 HUẤN LUYỆN TỰ TIẾN HÓA: HUẤN LUYỆN LIÊN TỤC SO VỚI HUẤN LUYỆN KHỞI ĐỘNG LẠI

Bảng 7: Phân tích về các phương pháp huấn luyện tự tiến hóa khác nhau trên GSM8K.

Phương pháp Huấn luyện | Tạo sinh Trực tiếp (%) | Tự Hoàn thiện (%)
Mô hình Cơ sở | 24.49 | 24.49
Huấn luyện Khởi động lại | 27.67 | 29.34
Huấn luyện Liên tục (Dữ liệu Hỗn hợp) | 27.22 | 28.43
Huấn luyện Liên tục (Chỉ D^t_evol) | 24.87 | 25.85

"Huấn luyện Khởi động lại", kết hợp kho dữ liệu học meta-kỹ năng với tất cả các vòng dữ liệu huấn luyện tự tiến hóa, cải thiện đáng kể tạo sinh trực tiếp (+3.18%) và tự hoàn thiện (+3.85%). "Huấn luyện Liên tục (Dữ liệu Hỗn hợp)", nơi mô hình được huấn luyện đồng thời với tất cả các vòng dữ liệu tự tiến hóa, cũng cho thấy nâng cao đáng chú ý trong tạo sinh trực tiếp (+2.73%) và tự hoàn thiện (+3.94%). Ngược lại, "Huấn luyện Liên tục (Chỉ D^t_evol)", huấn luyện mô hình tuần tự với dữ liệu tự tiến hóa từ mỗi vòng, thể hiện mức tăng khiêm tốn hơn (+0.38% trong tạo sinh trực tiếp, +0.98% trong tự hoàn thiện). Hiệu suất tương đối thấp hơn của phương pháp sau nhấn mạnh tầm quan trọng của chiến lược dữ liệu hỗn hợp cho huấn luyện tự tiến hóa hiệu quả.

Trong toàn bộ văn bản chính của chúng tôi, chúng tôi đã nhất quán sử dụng phương pháp "Huấn luyện Khởi động lại". Phương pháp này được chọn vì hiệu suất vượt trội, như được chứng minh trong bảng 7. Ngoài ra, việc tích hợp D_meta vào huấn luyện tự tiến hóa là quan trọng để ngăn chặn khả năng quên thảm khốc của meta-kỹ năng. Chiến lược này cần thiết để bảo tồn hiệu quả và độ tin cậy của quá trình huấn luyện tự tiến hóa, như được nổi bật trong § 3.2.2.

A.11 SELF SO VỚI ĐIỀU CHỈNH TINH TẾ CÓ GIÁM SÁT TRÊN DỮ LIỆU HUẤN LUYỆN GSM8K 7.5K

Khi được điều chỉnh tinh tế trên bộ huấn luyện GSM8K 7.5k, mô hình Vicuna đạt được độ chính xác 35.70%, thấp hơn phương pháp SELF (37.87%).

Các thí nghiệm trong bảng 8 sử dụng 7.5k dữ liệu meta-kỹ năng, đảm bảo so sánh công bằng với mô hình được điều chỉnh tinh tế có giám sát. Phương pháp này khác với phương pháp trong bảng 1, nơi chỉ sử dụng 3.5k dữ liệu meta-kỹ năng.

Bảng 8 chỉ ra rằng, với 7.5k lời nhắc huấn luyện chưa được gán nhãn cho kho dữ liệu học meta-kỹ năng, Vicuna + DQA đạt được 28.05%. Sau học meta-kỹ năng, kết quả tạo sinh trực tiếp cải thiện lên 31.23%, tăng thêm lên 32.98% sau tự hoàn thiện. Các vòng tự tiến hóa tiếp theo dẫn đến mức tăng hiệu suất, đạt 37.87% (tạo sinh trực tiếp) và 38.12% (tự hoàn thiện) trong vòng thứ hai, vượt trội hơn điều chỉnh tinh tế có giám sát (35.70%).

Cải thiện Liên tục của SELF so với Điều chỉnh tinh tế Có giám sát: Lợi thế chính của SELF nằm ở khả năng cải thiện liên tục và thích ứng. Trái ngược với điều chỉnh tinh tế có giám sát, SELF không dựa vào chú thích của con người hoặc LLM bên ngoài (như GPT3.5/GPT4) cho dữ liệu huấn luyện trong huấn luyện tự tiến hóa.

A.12 KHẢ NĂNG MỞ RỘNG CỦA KHUNG SELF

Để khám phá cách SELF hoạt động với các chất lượng mô hình khởi đầu khác nhau, chúng tôi thực hiện thí nghiệm sử dụng mô hình OpenLlama-3b (Geng & Liu, 2023), một LLM nhỏ hơn cùng với một LLM mạnh hơn, VicunaV1.5 (được điều chỉnh tinh tế từ Llama2-7b) (Chiang et al., 2023), trên bộ dữ liệu GSM8K. Điều này cho phép chúng tôi đánh giá khả năng thích ứng của SELF với chất lượng mô hình. Các thí nghiệm với SELF dựa trên vòng đầu tiên của tự tiến hóa. Kết quả như sau:

Bảng 9: Khả năng mở rộng của khung SELF trên các mô hình khác nhau.

Mô hình | Tạo sinh Trực tiếp (%) | Tự Hoàn thiện (%)
OpenLlama-3b | 2.04 | 1.01
OpenLlama-3b + DQA | 12.13 | 10.97
OpenLlama-3b + DQA + SELF | 15.32 | 15.78
Vicuna (Llama-7b) | 16.43 | 15.63
Vicuna + DQA | 24.49 | 24.44
Vicuna + DQA + SELF | 27.67 | 29.34
VicunaV1.5 (Llama2-7b) | 18.5 | 17.43
VicunaV1.5 + DQA | 26.04 | 25.48
VicunaV1.5 + DQA + SELF | 30.22 | 32.43

Khả năng Áp dụng và Tính Mạnh mẽ của Khung SELF: Sự cải thiện trung bình 17.32% qua tạo sinh trực tiếp và 16.87% sau tự hoàn thiện nhấn mạnh khả năng mở rộng và hiệu quả của khung. Nó tiết lộ tác động tích cực nhất quán của Khung SELF trên các mô hình đa dạng.

Khung SELF thể hiện hiệu suất được nâng cao trên các mô hình mạnh mẽ hơn: Như được hiển thị trong bảng 9, áp dụng SELF cho VicunaV1.5 dẫn đến những mức tăng đáng kể nhất - 30.22% trong tạo sinh trực tiếp và 32.43% sau tự hoàn thiện, vượt trội hiệu suất trên Vicuna và OpenLlama-3b. Điều này chỉ ra rằng hiệu quả của khung SELF cải thiện với khả năng của mô hình cơ bản.

A.13 TÁC ĐỘNG CỦA CHẤT LƯỢNG KHO DỮ LIỆU META-KỸ NĂNG

Chúng tôi kiểm tra ảnh hưởng của chất lượng học meta-kỹ năng đến quá trình tự tiến hóa với các kết quả sau:

Bảng 10 được trình bày chứng minh những cải thiện hiệu suất đáng kể đạt được bằng cách sử dụng GPT-4 để tạo ra kho dữ liệu meta-kỹ năng trong khung SELF của chúng tôi, so với việc sử dụng GPT-3.5-turbo. Bảng cho thấy nâng cao đáng kể trong cả tạo sinh trực tiếp và tự hoàn thiện trên các giai đoạn huấn luyện khi GPT-4 được sử dụng. Ví dụ, trong giai đoạn "Vicuna + D_meta", hiệu suất tạo sinh trực tiếp tăng từ 24.84% với GPT-3.5-turbo lên 25.39% với GPT-4, đánh dấu mức tăng 0.55%. Tương tự, trong giai đoạn "Vicuna + D_meta + SELF Evolution", kết quả tự hoàn thiện cải thiện từ 25.47% với GPT-3.5-turbo lên 29.34% với GPT-4, cho thấy nâng cao 3.87%.

Bảng 10: Hiệu ứng của chất lượng kho dữ liệu meta-kỹ năng đối với hiệu suất mô hình sử dụng GPT-3.5-turbo và GPT4.

Giai đoạn Huấn luyện | Tạo sinh Trực tiếp (%) | Tự Hoàn thiện (%)
(GPT-3.5-turbo/GPT4) | (GPT-3.5-turbo/GPT4)
Vicuna + D_meta | 24.84/25.39 (0.55 ↑) | 25.22/28.28 (3.06 ↑)
Vicuna + D_meta + SELF Evol. | 25.11/27.67 (2.56 ↑) | 25.47/29.34 (3.87 ↑)

Phân tích này nhấn mạnh tác động đáng kể của việc sử dụng dữ liệu huấn luyện meta-kỹ năng chất lượng cao đối với hiệu suất của mô hình Vicuna trong khung SELF. Sự chuyển đổi từ GPT-3.5-turbo sang GPT-4 để tạo ra kho dữ liệu meta-kỹ năng dẫn đến cải thiện nhất quán trong cả thước đo Tạo sinh Trực tiếp và Tự Hoàn thiện.

A.14 HUẤN LUYỆN TỰ TIẾN HÓA ĐƠN VÒNG SO VỚI LẶP LẠI

Với số lượng lời nhắc chưa được gán nhãn bằng nhau, chúng tôi đánh giá hiệu quả của huấn luyện trong một vòng so với huấn luyện lặp lại. Phương pháp trước sử dụng một mô hình duy nhất để tự quản lý dữ liệu huấn luyện từ tất cả các lời nhắc chưa được gán nhãn có sẵn cùng một lúc. Ngược lại, phương pháp sau bao gồm việc chia các lời nhắc chưa được gán nhãn thành nhiều phần. Đối với phương pháp lặp lại, mô hình ban đầu được huấn luyện trên một phần của các lời nhắc chưa được gán nhãn và nhãn tự quản lý. Tiếp theo điều này, mô hình được huấn luyện được sử dụng để tạo ra dữ liệu huấn luyện mới dựa trên các lời nhắc chưa được sử dụng trước đó. Như được mô tả trong văn bản chính của chúng tôi, chúng tôi chia các lời nhắc chưa được gán nhãn thành ba phần, cho phép mô hình trải qua ba vòng lặp lại của tự tiến hóa.

Bảng 11: So sánh huấn luyện đơn vòng và huấn luyện lặp lại.

Phương pháp Huấn luyện | Tạo sinh Trực tiếp (%) | Tự Hoàn thiện (%)
SELF (Đơn Vòng) | 28.40 | 30.55
SELF (Lặp lại) | 29.64 | 31.31

Bảng 11 cho thấy rằng trong huấn luyện "Đơn Vòng", hiệu suất là 28.40% cho tạo sinh trực tiếp và 30.55% cho tự hoàn thiện. Ngược lại, phương pháp lặp lại mang lại điểm số cao hơn là 29.64% cho tạo sinh trực tiếp và 31.31% cho tự hoàn thiện.

Lợi thế của Huấn luyện Lặp lại: Huấn luyện lặp lại hưởng lợi từ khả năng nâng cao của LLMs trong các vòng tiếp theo, tạo ra dữ liệu huấn luyện chất lượng cao hơn và dẫn đến hiệu suất thử nghiệm được cải thiện.
