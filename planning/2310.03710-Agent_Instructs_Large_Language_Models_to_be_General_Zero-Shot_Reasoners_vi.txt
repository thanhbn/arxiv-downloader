# 2310.03710.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2310.03710.pdf
# Kích thước tệp: 6953046 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Agent Hướng dẫn Các Mô hình Ngôn ngữ Lớn trở thành Người suy luận Zero-Shot Tổng quát
Nicholas Crispino1Kyle Montgomery1Fankun Zeng1Dawn Song2Chenguang Wang1

Tóm tắt
Chúng tôi giới thiệu một phương pháp để cải thiện khả năng suy luận zero-shot của các mô hình ngôn ngữ lớn trên các tác vụ hiểu ngôn ngữ tổng quát. Cụ thể, chúng tôi xây dựng một agent tự động để hướng dẫn quá trình suy luận của các mô hình ngôn ngữ lớn. Để thực hiện điều này, agent của chúng tôi chỉ cần tạo ra một bộ hướng dẫn duy nhất cho mỗi tác vụ. Những hướng dẫn này hóa ra cực kỳ hiệu quả trong việc cải thiện quá trình suy luận của các mô hình ngôn ngữ lớn khác nhau trên tất cả các trường hợp tác vụ. Chúng tôi chỉ ra rằng phương pháp này tiếp tục giải phóng khả năng suy luận zero-shot của các mô hình ngôn ngữ lớn cho nhiều tác vụ hơn. Chúng tôi nghiên cứu hiệu suất của phương pháp này trên một tập hợp rộng lớn các bộ dữ liệu bao gồm sinh tạo, phân loại và suy luận. Chúng tôi chỉ ra rằng phương pháp của chúng tôi tổng quát hóa cho hầu hết các tác vụ và đạt được hiệu suất zero-shot tiên tiến nhất trên 20 trong số 29 bộ dữ liệu mà chúng tôi đánh giá. Ví dụ, phương pháp của chúng tôi nâng cao hiệu suất của các mô hình ngôn ngữ lớn tiên tiến nhất với một biên độ lớn, bao gồm Vicuna-13b, Llama-2-70b-chat và GPT-3.5 Turbo. So với zero-shot chain of thought, cải thiện trong suy luận của chúng tôi là nổi bật. Với phương pháp của chúng tôi, Llama-2-70b-chat vượt trội đáng kể so với zero-shot GPT-3.5 Turbo.

1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) (Brown et al., 2020; Wang & Komatsuzaki, 2021; Zhang et al., 2022a; Smith et al., 2022; Chowdhery et al., 2022; Hoffmann et al., 2022; Scao et al., 2023; OpenAI, 2023; Anil et al., 2023; Touvron et al., 2023a;b; Penedo et al., 2023) đã thúc đẩy đáng kể tình trạng tiên tiến trên một loạt rộng các tác vụ hiểu ngôn ngữ, dẫn đến việc triển khai và áp dụng rộng rãi trong các ứng dụng (Araci, 2019; Huang et al., 2020; Bolton et al., 2022; Wu et al., 2023; Driess et al., 2023; Huang et al., 2023). Đặc biệt, các khả năng mới nổi của LLMs như suy luận phức tạp (Wei et al., 2022b; Wang et al., 2023c; Kıcıman et al., 2023) đã khiến chúng trở thành chủ đề nghiên cứu trong những năm gần đây. Trong số này, suy luận zero-shot (Kojima et al., 2022; Wan et al., 2023) đã thu hút sự quan tâm đáng kể của công chúng và đạt được kết quả đầy hứa hẹn trong các lĩnh vực tác vụ cụ thể. Tuy nhiên, khả năng suy luận của LLMs trên các tác vụ tổng quát vẫn chưa rõ ràng.

Trong bài báo này, chúng tôi cải thiện khả năng suy luận zero-shot của LLMs trên các tác vụ hiểu ngôn ngữ tổng quát. Để giải quyết một tác vụ, chúng tôi xây dựng một agent để hướng dẫn quá trình suy luận của LLMs cho tác vụ đó (Hình 1). Đối với mỗi tác vụ, agent tự động của chúng tôi tạo ra một bộ hướng dẫn độc nhất dành riêng cho tác vụ. Những hướng dẫn này, chỉ được tạo ra một lần cho mỗi tác vụ, được sử dụng để hướng dẫn suy luận của nhiều LLMs khác nhau trên tất cả các trường hợp tác vụ. Agent của chúng tôi được xây dựng dựa trên một LLM lớn hơn dạy quá trình suy luận cho các LLMs nhỏ hơn. Thiết kế này tuân theo thiết lập chưng cất kiến thức tổng quát liên quan đến mô hình giáo viên-học sinh. Trực giác, các hướng dẫn dành riêng cho tác vụ giúp căn chỉnh tốt hơn quá trình suy luận chain of thought của LLMs với mỗi tác vụ. Chúng tôi gọi phương pháp này là suy luận được hướng dẫn bởi agent zero-shot (AgentInstruct). Ý tưởng cơ bản của phương pháp chúng tôi được thúc đẩy bởi hai hướng nghiên cứu. Thứ nhất, sự phát triển của các language agents (Yao et al., 2023b; Shinn et al., 2023; Park et al., 2023; Wang et al., 2023a; Xi et al., 2023) để tự động hoàn thành một tác vụ. Thay vì hoàn thành tác vụ, agent của chúng tôi tạo ra hướng dẫn về cách hoàn thành tác vụ. Chúng tôi thực hiện điều này bằng cách điều chỉnh một agent hiện có để truy cập nhiều loại kiến thức liên quan đến tác vụ trên web, với thông tin tác vụ cơ bản như tên bộ dữ liệu và một số ví dụ chỉ đầu vào. Kết quả là, agent tổng hợp các hướng dẫn từng bước chất lượng cao cho các tác vụ, được xác minh bởi các tài nguyên web. Chúng tôi tuân theo thiết kế gần đây của các language agents lập kế hoạch một quy trình, mặc dù quy trình của chúng tôi chỉ chạy một lần cho mỗi bộ dữ liệu thay vì cho mỗi trường hợp bộ dữ liệu. Thứ hai, suy luận zero-shot chain of thought (CoT) của LLMs đã đạt được kết quả đầy hứa hẹn trên các tác vụ như suy luận số học (Kojima et al., 2022; Wang et al., 2023b). Việc học zero-shot tiêu chuẩn nhắc nhở một LLM để trực tiếp đưa ra dự đoán mà không có ví dụ tác vụ. Ngược lại, CoT phân tách một tác vụ thành các bước suy luận trung gian trong đó việc giải quyết từng bước sẽ dẫn đến đầu ra cuối cùng. Chúng tôi tiếp tục căn chỉnh các bước suy luận CoT với một tác vụ cụ thể bằng cách nhắc nhở với các hướng dẫn agent dành riêng cho tác vụ. Thiết kế của zero-shot AgentInstruct rất quan trọng: Chúng tôi tổng quát hóa khả năng suy luận zero-shot của LLMs cho nhiều tác vụ hơn bằng cách kết hợp các hướng dẫn dành riêng cho tác vụ từ một language agent và suy luận dành riêng cho tác vụ của LLMs.

Chúng tôi đánh giá thực nghiệm khả năng suy luận zero-shot của LLMs trên một tập hợp rộng lớn các tác vụ hiểu ngôn ngữ trên 29 bộ dữ liệu (bao gồm 53 tập con), bao gồm sinh tạo, phân loại và suy luận. Zero-shot AgentInstruct đạt được hiệu suất tiên tiến nhất trên 20 bộ dữ liệu. Chúng tôi tiến hành đánh giá trên ba LLMs tiên tiến nhất, cụ thể là Vicuna (Chiang et al., 2023), Llama-2-chat (Touvron et al., 2023b) và GPT-3.5 Turbo (OpenAI, 2022). Chúng tôi chỉ ra rằng zero-shot AgentInstruct nâng cao hiệu suất của những mô hình này trung bình 17.8%. Khi so sánh với zero-shot CoT, cải thiện hiệu suất tổng thể là đáng kể (6.5%), và đặc biệt, cải thiện trong suy luận là đáng kể với mức tăng trung bình 10.5%, dẫn đến hiệu suất tốt nhất trên 10 trong số 12 tác vụ suy luận. Đáng chú ý, Llama-2-70b-chat với zero-shot AgentInstruct vượt trội so với zero-shot GPT-3.5 Turbo tiêu chuẩn trung bình 10.2%.

Chúng tôi hy vọng kết quả sẽ giúp thúc đẩy nghiên cứu trong tương lai về việc tiếp tục mở khóa khả năng suy luận zero-shot của các mô hình nền tảng lớn và khám phá việc sử dụng rộng rãi hơn của các agents.

2. Phương pháp
Chúng tôi trình bày zero-shot AgentInstruct trong phần này. Để giải quyết một tác vụ, zero-shot AgentInstruct sử dụng một agent để tạo ra một bộ hướng dẫn cho mỗi bộ dữ liệu cho phép một LLM suy luận hướng tới dự đoán cuối cùng. Trực giác, con người thường dựa vào các hướng dẫn cụ thể để hướng dẫn hiệu quả hơn quá trình suy nghĩ của họ khi họ làm việc hướng tới giải pháp cho một vấn đề. Ví dụ, để hiểu cảm xúc trong các đánh giá phim, các hướng dẫn như "1. Hiểu Bộ dữ liệu: ... Bộ dữ liệu Đánh giá Phim ... 2. Phân tích Đoạn văn: Chú ý đến ... giai điệu của đánh giá ..." giúp con người phân tách vấn đề thành các bước suy luận dành riêng cho tác vụ và giải quyết từng bước để đưa ra câu trả lời cuối cùng (Hình 1). Zero-shot AgentInstruct tuân theo trực giác này.

Hướng dẫn Agent Thay vì viết tay các hướng dẫn dành riêng cho tác vụ, chúng tôi xây dựng một agent để tự động hóa quy trình. Agent của chúng tôi chỉ cần tạo ra hướng dẫn một lần cho mỗi tác vụ thay vì chạy agent trên tất cả các trường hợp bộ dữ liệu. Trực giác là một agent có thể tổng hợp các hướng dẫn chất lượng cao với quyền truy cập vào một loạt rộng kiến thức tác vụ hiện có trên web. Chúng tôi thiết kế agent của mình dựa trên ReAct (Yao et al., 2023b) sử dụng triển khai zero-shot ReAct của LangChain (Chase, 2022). Điều này được thúc đẩy bởi những phát triển gần đây của các language agents để giải quyết tác vụ. Agent của chúng tôi nổi bật hai tính năng (Hình 2): (i) Tạo hướng dẫn. Agent của chúng tôi tuân theo ReAct sử dụng một LLM để đề xuất một loạt các suy nghĩ. Agent sau đó nhận quan sát và thực hiện hành động theo các suy nghĩ. Khác với ReAct nhằm trực tiếp giải quyết tác vụ, agent của chúng tôi đưa ra các hướng dẫn từng bước về cách giải quyết tác vụ. Lợi thế chính của điều này là chúng tôi chỉ cần tạo ra hướng dẫn một lần cho mỗi bộ dữ liệu thay vì chạy agent trên tất cả các trường hợp bộ dữ liệu. Chúng tôi sử dụng GPT-4 (OpenAI, 2023) làm agent mặc định. Khi hành động là finish, đầu ra tương ứng là các hướng dẫn dành riêng cho tác vụ của chúng tôi. (ii) Không gian hành động. Chúng tôi hạn chế không gian hành động của mình chứa hai loại hành động hỗ trợ việc tạo hướng dẫn: (a) askaboutdataset [string], trả về thông tin từ các trang web liên quan hàng đầu chứa thông tin về bộ dữ liệu. Để làm điều này, chúng tôi xây dựng và sử dụng một API hỏi đáp như một công cụ. API này được thiết kế để trả lời câu hỏi về tài liệu bằng cách giao tiếp với Chroma (Huber & Troynikov, 2022), một cơ sở dữ liệu vector lưu trữ các trang web, để cung cấp câu trả lời. (b) finish [instructions], kết thúc việc tạo hướng dẫn với các hướng dẫn dành riêng cho tác vụ. Như được hiển thị trong Hình 2, để tạo ra các hướng dẫn, agent của chúng tôi lấy thông tin bộ dữ liệu cơ bản như tên của bộ dữ liệu (ví dụ: IMDB), một vài ví dụ chỉ đầu vào (ví dụ không có nhãn sự thật), và tập hợp các nhãn đầu ra cho bộ dữ liệu (nếu có, nếu không thì loại bộ dữ liệu, ví dụ: sinh tạo) làm đầu vào của nó. Sử dụng kiến thức tác vụ từ web, agent của chúng tôi hình thành các quan sát (ví dụ: "Quan sát 1: ... được gán nhãn là tích cực hoặc tiêu cực ...") và suy nghĩ (ví dụ: "Suy nghĩ 2: ... tạo hướng dẫn ...") kích hoạt agent thực hiện các hành động, chẳng hạn như hành động finish để đưa ra các hướng dẫn dành riêng cho tác vụ. Để có mô tả đầy đủ về pipeline AgentInstruct cho việc tạo hướng dẫn, xem Phụ lục A.3.

Suy luận Chain of Thought Chain of thought (CoT) (Wei et al., 2022b; Kojima et al., 2022) nhắc nhở LLMs phá vỡ tác vụ thành các bước suy luận trung gian dẫn đến câu trả lời cuối cùng. Không giống như zero-shot CoT sử dụng một lời nhắc cố định "Let's think step by step", chúng tôi đặt trước các hướng dẫn agent dành riêng cho tác vụ của mình (chỉ được tạo một lần cho mỗi bộ dữ liệu) vào đầu vào của bất kỳ mô hình nào chúng tôi đang sử dụng để suy luận. Những hướng dẫn giống nhau này có thể được sử dụng cho các LLMs suy luận khác nhau trên tất cả các trường hợp bộ dữ liệu. Các hướng dẫn nhắc nhở LLMs tối ưu hóa quy trình suy luận của họ cho tác vụ. LLMs sau đó sẽ tuân theo các hướng dẫn dành riêng cho tác vụ của chúng tôi để phân tách tác vụ thành một chuỗi các bước trung gian cụ thể hơn để giải quyết tác vụ. Như được hiển thị trong Hình 1, các hướng dẫn agent "... Chú ý đến ... các biểu hiện tường minh hoặc ngầm ẩn của cảm xúc đối với bộ phim ..." là chìa khóa để tạo ra đường dẫn suy luận quan trọng "... bộ phim này đáng xem chỉ vì màn trình diễn của ba diễn viên ...", dẫn đến dự đoán chính xác trong khi zero-shot tiêu chuẩn và zero-shot CoT thất bại. Chúng tôi tuân theo zero-shot CoT, bao gồm một lời nhắc trích xuất suy luận để tạo ra các bước suy luận trung gian, và một lời nhắc trích xuất câu trả lời để thu thập các câu trả lời. Để đơn giản trong triển khai, chúng tôi thay thế lời nhắc cố định của zero-shot CoT bằng các hướng dẫn dành riêng cho tác vụ của zero-shot AgentInstruct trong lời nhắc trích xuất suy luận.

Zero-shot AgentInstruct có một số tính chất độc đáo: (i) Zero-shot AgentInstruct là một cách mới để cải thiện suy luận zero-shot của LLMs. Zero-shot AgentInstruct tách biệt language agent và quá trình suy luận của LLMs, giúp zero-shot AgentInstruct tổng quát hóa cho nhiều tác vụ hơn. Quan trọng là, language agent của chúng tôi chỉ cần chạy một lần để tạo ra các hướng dẫn cho mỗi tác vụ. Và, các hướng dẫn agent cung cấp nhiều kiểm soát dành riêng cho tác vụ hơn đối với các đường dẫn suy luận của LLMs, có lợi cho việc căn chỉnh con người và cải thiện tính an toàn của LLMs. (ii) Các hướng dẫn agent của chúng tôi được tùy chỉnh cho các tác vụ khác nhau và được xác minh bởi kiến thức tác vụ hiện có. Đối với mỗi tác vụ, các LLMs khác nhau sử dụng cùng một bộ hướng dẫn. Chúng tôi thấy rằng các hướng dẫn chuyển giao tốt giữa các LLMs suy luận này. Điều này quan trọng trong thực tế vì các agent LLMs thường mạnh mẽ và tốn kém hơn các LLMs suy luận, vì vậy phương pháp của chúng tôi là một thay thế hiệu quả về chi phí cho việc sử dụng agents trực tiếp. Theo thiết lập chưng cất kiến thức giáo viên-học sinh, công trình của chúng tôi tập trung vào việc sử dụng một agent dựa trên LLM lớn hơn để hướng dẫn LLM suy luận nhỏ hơn. (iii) Bằng cách cung cấp các hướng dẫn dành riêng cho tác vụ, khả năng suy luận chain of thought được tổng quát hóa thêm cho nhiều tác vụ ngoài các tác vụ suy luận. Chúng tôi chỉ ra rằng các tác vụ hiểu ngôn ngữ tổng quát như sinh tạo và phân loại cũng được hưởng lợi từ suy luận chain of thought. AgentInstruct là zero-shot nên không cần ví dụ đầu vào-đầu ra để giải quyết tác vụ. Để sử dụng tốt hơn các khả năng suy luận mới nổi của LLMs, các hướng dẫn dành riêng cho tác vụ của chúng tôi căn chỉnh quá trình suy luận với một tác vụ cụ thể tốt hơn các hướng dẫn tổng quát hoặc cố định.

3. Thí nghiệm
Chúng tôi chỉ ra rằng zero-shot AgentInstruct thành công trong việc cải thiện khả năng suy luận zero-shot của LLMs, cụ thể là Vicuna (Chiang et al., 2023), Llama-2-chat (Touvron et al., 2023b) và GPT-3.5 Turbo (OpenAI, 2022), với một biên độ lớn trung bình. Chúng tôi đánh giá zero-shot AgentInstruct trên một lựa chọn toàn diện của 29 bộ dữ liệu điểm chuẩn chứa 53 tập con. Như được hiển thị trong Hình 3, mỗi bộ dữ liệu là một tác vụ sinh tạo hoặc phân loại, và một phần của các bộ dữ liệu trong mỗi danh mục cũng là các tác vụ suy luận. Các bộ dữ liệu bao gồm tất cả các kịch bản cốt lõi HELM từ Liang et al. (2023), cũng như các bộ dữ liệu suy luận từ Kojima et al. (2022). Chi tiết về thiết lập thí nghiệm, bao gồm các bộ dữ liệu và mô hình, được mô tả trong Phụ lục A.

3.1. Kết quả chính
Kết quả được hiển thị trong Hình 5. Chúng tôi so sánh zero-shot AgentInstruct với zero-shot tiêu chuẩn và zero-shot CoT. Chúng tôi tập trung phân tích của mình vào ba mô hình: Vicuna-13b, Llama-2-70b-chat và GPT-3.5 Turbo.

Đầu tiên chúng tôi so sánh zero-shot AgentInstruct với việc nhắc nhở zero-shot tiêu chuẩn (Hình 1). Thiết kế lời nhắc zero-shot tuân theo Liang et al. (2023). Trên mỗi mô hình, zero-shot AgentInstruct thắng trên đa số các bộ dữ liệu, với mức tăng không dưới 13.0% trung bình. Hiệu suất trung bình so với thiết lập zero-shot là tốt nhất trên Llama-2-70b-chat với mức cải thiện 23.2%. Hình 5a và Hình 5b hiển thị kết quả cho các tác vụ sinh tạo và phân loại tương ứng. Trung bình, với zero-shot AgentInstruct, ba mô hình đánh bại thiết lập zero-shot 23.1% cho sinh tạo và 13.5% cho phân loại. Chúng tôi giả thuyết rằng các bộ dữ liệu sinh tạo thường yêu cầu các hướng dẫn cụ thể hơn các bộ dữ liệu phân loại, vì mô hình không biết định dạng tốt nhất cho đầu ra sinh tạo trừ khi nó có đủ thông tin tác vụ. Điều này cho thấy rằng agent của chúng tôi có thể hướng dẫn quá trình suy luận để cải thiện các đầu ra cuối cùng cho các tác vụ khác nhau, và zero-shot AgentInstruct có thể tổng quát hóa khả năng suy luận của LLMs qua các tác vụ. Lưu ý rằng chúng tôi chỉ chạy agent 53 lần, dẫn đến 53 hướng dẫn được tạo bởi agent, vì chúng tôi đánh giá trên 53 tập con. Với zero-shot AgentInstruct, chúng tôi cũng quan sát một biên độ cải thiện lớn cho hiệu suất zero-shot qua các mô hình khác nhau. Đáng kể, Llama-2-70b-chat đánh bại hiệu suất của zero-shot GPT-3.5 Turbo 10.2% trung bình trên tất cả các bộ dữ liệu. Điều này cho thấy các hướng dẫn agent của chúng tôi là chìa khóa để cải thiện hiệu suất suy luận của LLMs.

So sánh trực tiếp nhất với zero-shot AgentInstruct là zero-shot CoT, vì zero-shot AgentInstruct sử dụng các hướng dẫn dành riêng cho tác vụ thay vì một hướng dẫn thủ công cố định. Trung bình, trên cả ba mô hình, zero-shot AgentInstruct đánh bại zero-shot CoT 6.5%, với tăng trưởng lớn nhất là Vicuna-13b ở 9.5%. Trên cả các bộ dữ liệu sinh tạo và phân loại, qua ba mô hình, zero-shot AgentInstruct thắng 5.0% và 7.8% trên mỗi danh mục tương ứng. Điều này cho thấy rằng zero-shot AgentInstruct có thể tổng quát hóa khả năng suy luận zero-shot của LLMs cho cả các tác vụ sinh tạo và phân loại, và tối ưu hóa hiệu suất của các tác vụ cụ thể.

Đặc biệt, chúng tôi xem xét hiệu suất của các tác vụ suy luận (Hình 5c). Trong ba mô hình của chúng tôi, sự khác biệt trung bình giữa zero-shot AgentInstruct và thiết lập zero-shot trên các tác vụ suy luận là 31.3%, trong khi sự khác biệt giữa zero-shot AgentInstruct và zero-shot CoT là 10.5%. Điều này cho thấy rằng các hướng dẫn dành riêng cho tác vụ của chúng tôi hữu ích hơn cho LLMs để phá vỡ các tác vụ thành các bước suy luận trung gian cụ thể hơn so với các hướng dẫn không phụ thuộc tác vụ trong zero-shot CoT, dẫn đến cải thiện dự đoán cuối cùng.

Nhìn chung, zero-shot AgentInstruct thắng trên 9 trong số 13 bộ dữ liệu sinh tạo, 11 trong số 16 bộ dữ liệu phân loại, và 10 trong số 12 bộ dữ liệu suy luận (Hình 4). Xem Phụ lục C để có kết quả và phân tích bổ sung, bao gồm kết quả trên các bộ dữ liệu và tập con riêng lẻ.

3.2. Nghiên cứu khử yếu tố
Bảng 1. Khử yếu tố về các khía cạnh khác nhau của zero-shot AgentInstruct với Llama-2-70b-chat.

AddSub IMDB NarrativeQA
Zero-Shot AgentInstruct 79.5 94.0 65.0
w/o Agent Instructions 73.2 89.0 62.3
w/o Input Examples 72.4 88.0 60.1
w/o Label Description 74.9 93.8 63.9
w/o GPT-4 75.2 92.6 63.5

Chúng tôi xem xét cách các thành phần khác nhau của zero-shot AgentInstruct tác động đến hiệu suất suy luận zero-shot của nó. Kết quả được hiển thị trong Bảng 1 trên AddSub (suy luận), IMDB (phân loại) và NarrativeQA (sinh tạo). Chúng tôi sử dụng Llama-2-70b-chat cho bước suy luận. Bốn thiết lập xem xét tầm quan trọng của các hướng dẫn agent trong zero-shot AgentInstruct. Mô tả của mỗi thiết lập như sau: (i) w/o Agent Instructions: Chúng tôi so sánh phương pháp zero-shot AgentInstruct với zero-shot CoT. (ii) w/o Input Examples: Chúng tôi loại bỏ các ví dụ chỉ đầu vào khỏi đầu vào của agent. (iii) w/o Label Description: Chúng tôi loại bỏ mô tả của các nhãn khỏi đầu vào của agent. (iv) w/o GPT-4: Chúng tôi sử dụng GPT-3.5 Turbo, thay vì GPT-4, làm agent để tạo hướng dẫn. Kết quả cho thấy rằng tất cả các thành phần của zero-shot AgentInstruct đều hiệu quả trong việc cung cấp các hướng dẫn chất lượng cao và khơi gợi các bước suy luận chất lượng cao. Xem Phụ lục D.1 để có mô tả thêm về mỗi thiết lập.

Tầm quan trọng của việc Tổng hợp Hướng dẫn Vì zero-shot AgentInstruct sử dụng thông tin bộ dữ liệu bổ sung để xây dựng hướng dẫn, chúng tôi kiểm tra zero-shot CoT với cùng thông tin đó được đặt trước để tách biệt tác động của việc tổng hợp thông tin này thành các hướng dẫn chất lượng cao. Chúng tôi thấy rằng việc chỉ đặt trước thông tin này không cải thiện hiệu suất một cách nhất quán so với zero-shot CoT, trong khi việc tổng hợp thông tin này thành các hướng dẫn chất lượng cao với zero-shot AgentInstruct thì có, cho thấy các hướng dẫn là không thể thiếu trong việc hướng dẫn các bước suy luận (Bảng 2).

Bảng 2. So sánh trên Llama-2-70b-chat khi cung cấp thông tin bộ dữ liệu cho zero-shot CoT.

Method AddSub IMDB NarrativeQA
Zero-Shot CoT 73.2 89.0 62.3
Zero-Shot CoT + Data Information 71.6 90.5 58.2
Zero-Shot AgentInstruct 79.5 94.0 65.0

So sánh với Phương pháp GPT-4 Chúng tôi kiểm tra các phương pháp sau sử dụng GPT-4: zero-shot, zero-shot CoT, ReAct (Yao et al., 2023b) và zero-shot AgentInstruct. Hình 6 hiển thị hiệu suất của mỗi phương pháp trên AddSub. Zero-shot AgentInstruct vượt trội zero-shot GPT-4 8.6% và ngang bằng hiệu suất của zero-shot CoT GPT-4 với khoảng một phần mười chi phí. Zero-shot AgentInstruct đang sử dụng GPT-4 cho các hướng dẫn và GPT-3.5 Turbo cho suy luận CoT, tuân theo thiết lập chưng cất kiến thức tiêu chuẩn. Điều này cho thấy zero-shot AgentInstruct là một giải pháp hiệu quả về chi phí để cải thiện hiệu suất, vì nó vượt xa hiệu suất GPT-3.5 với một chút chi phí thêm và đạt hiệu suất GPT-4 với chi phí ít hơn nhiều. Mặc dù ReAct hơi vượt trội zero-shot AgentInstruct, nó tốn gần 100 lần chi phí hơn vì agent zero-shot AgentInstruct chỉ chạy một lần cho mỗi bộ dữ liệu thay vì cho mỗi trường hợp. Mỗi lần chạy agent của chúng tôi để tạo hướng dẫn tốn dưới $1. Kết quả này ngụ ý rằng việc tách biệt việc tạo hướng dẫn và các bước suy luận tiếp tục giải phóng khả năng suy luận zero-shot của LLMs, và zero-shot AgentInstruct là một thay thế hiệu quả về chi phí cho việc sử dụng agents trực tiếp.

Sử dụng GPT-4 trong AgentInstruct Mặc dù zero-shot AgentInstruct chủ yếu là một giải pháp hiệu quả về chi phí để chưng cất kiến thức từ một agent LLM mạnh mẽ sang một người suy luận nhỏ hơn, chúng tôi cũng xem xét việc sử dụng cùng một mô hình cơ bản cho cả việc tạo hướng dẫn và các bước suy luận. Cụ thể, chúng tôi tận dụng GPT-4 cho cả việc tạo hướng dẫn và các bước suy luận trên IMDB và AddSub để xem liệu zero-shot AgentInstruct có hiệu quả ngoài thiết lập chưng cất kiến thức hay không. Trên IMDB, zero-shot GPT-4, zero-shot CoT GPT-4 và zero-shot AgentInstruct GPT-4 (trong đó cả việc tạo hướng dẫn và các bước suy luận CoT đều sử dụng GPT-4) ghi điểm lần lượt là 87.4, 96.1 và 96.6. Tương tự, trên AddSub, kết quả lần lượt là 79.5, 88.1 và 88.1. Trong cả hai trường hợp, chúng tôi thấy zero-shot AgentInstruct ít nhất cũng tốt như zero-shot GPT-4 và zero-shot CoT GPT-4, có nghĩa là phương pháp của chúng tôi cho thấy hứa hẹn khi cùng một mô hình được sử dụng trong toàn bộ pipeline AgentInstruct.

3.3. Độ nhạy của Lời nhắc Thủ công
Zero-shot AgentInstruct có hai lời nhắc thủ công trong bước suy luận CoT: (1) lời nhắc trích xuất suy luận, yêu cầu các bước suy luận trung gian, và (2) lời nhắc trích xuất câu trả lời, trích xuất câu trả lời cuối cùng từ các bước suy luận trung gian. Để kiểm tra độ nhạy của mỗi lời nhắc thủ công, chúng tôi thay đổi một lời nhắc duy nhất trong khi giữ lời nhắc zero-shot AgentInstruct mặc định cho lời nhắc khác. Kết quả được hiển thị trong Bảng 3 dựa trên Llama-2-70b-chat trên AddSub. Nhìn chung, hiệu suất của zero-shot AgentInstruct có vẻ không đặc biệt nhạy cảm với các thay đổi trong các lời nhắc thủ công, cho thấy rằng phương pháp đằng sau zero-shot AgentInstruct là mạnh mẽ. Các thí nghiệm độ nhạy lời nhắc bổ sung được tiến hành trong Phụ lục D.3.

3.4. Mở rộng Mô hình
Vì thường là trường hợp các tham số nhiều hơn cải thiện đáng kể khả năng suy luận của LLMs (Wei et al., 2022b), chúng tôi kiểm tra hiệu suất của zero-shot AgentInstruct trên các mô hình với kích thước khác nhau. Cụ thể, chúng tôi kiểm tra trên ba mô hình Llama-2-chat với 7 tỷ, 13 tỷ và 70 tỷ tham số. Hình 7 hiển thị điểm trung bình trên tất cả 29 bộ dữ liệu cho zero-shot, zero-shot CoT và zero-shot AgentInstruct.

Những kết quả này xác nhận rằng hiệu suất trung bình của cả ba phương pháp tăng theo kích thước mô hình. Mỗi lần kích thước mô hình tăng, zero-shot CoT và zero-shot AgentInstruct cho thấy cải thiện nhất quán khoảng 6%, trong khi zero-shot cho thấy mức tăng nhỏ hơn gần 2%. Điều này là do các bước suy luận được tạo ra tốt nhất với các mô hình mạnh mẽ hơn. Tuy nhiên, chỉ với 13b tham số, Llama-2-13b-chat với zero-shot AgentInstruct vượt qua hiệu suất của zero-shot GPT-3.5 Turbo hơn 2%. Ngoài ra, sự vượt trội của zero-shot AgentInstruct so với zero-shot và zero-shot CoT có vẻ độc lập với kích thước mô hình.

3.5. Phân tích Lỗi
Để điều tra nguyên nhân của các lỗi được tạo ra bởi zero-shot AgentInstruct, chúng tôi phân tích thủ công 25 mẫu từ AddSub, IMDB và NewsQA tương ứng trong đó zero-shot AgentInstruct dẫn đến dự đoán không chính xác trên Llama-2-70b-chat. Chúng tôi định nghĩa dự đoán không chính xác là những dự đoán có điểm khớp tương đối hoặc F1 dưới 1.

Lỗi phổ biến nhất trên các bộ dữ liệu này là suy luận không chính xác (32.0%), tức là không suy luận chính xác qua vấn đề khi áp dụng các hướng dẫn agent chính xác. Ví dụ, trên AddSub, zero-shot AgentInstruct có thể chọn sai phép toán do một động từ gây hiểu lầm. Trên IMDB, zero-shot AgentInstruct có thể đọc sai cảm xúc do nhấn mạnh các từ mô tả bộ phim, không phải đánh giá. Điều này có vấn đề, vì chúng ta muốn mô tả cảm xúc của đánh giá, không phải cảm xúc của bộ phim. Xem Hình 8 để có ví dụ về suy luận không chính xác trên IMDB. Sự mơ hồ của câu trả lời là một nguồn lỗi chính khác (22.7%). Ví dụ, trong một đánh giá mà người đánh giá rõ ràng thích bộ phim mặc dù người đánh giá thừa nhận đó là một bộ phim tệ điển hình, dự đoán của chúng tôi là "Tích cực" trong khi sự thật là "Tiêu cực" trên IMDB. Đối với nhiều lỗi, hoặc là các hướng dẫn được hiểu quá theo nghĩa đen hoặc bị bỏ qua một phần. Khi các mô hình trở nên tốt hơn trong suy luận, những lỗi như vậy sẽ được giảm thiểu. Phân tích lỗi kỹ lưỡng hơn và các ví dụ đầy đủ cho mỗi danh mục lỗi có trong Phụ lục D.4.

3.6. Nghiên cứu Trường hợp
Tiếp theo, chúng tôi phân tích chất lượng của các bước suy luận CoT khi dự đoán là chính xác. Trên ba bộ dữ liệu (AddSub, IMDB và NewsQA), chúng tôi chọn ngẫu nhiên 25 ví dụ từ mỗi bộ dữ liệu với điểm khớp tương đối hoặc F1 hoàn hảo. Chúng tôi thấy rằng khả năng suy luận được tăng cường đáng kể bởi sự kết hợp của các hướng dẫn agent hiệu quả và quá trình suy luận dành riêng cho tác vụ của LLMs. Một ví dụ có trong Hình 9.

3.7. So sánh với Các Phương pháp Liên quan
Few-Shot Chúng tôi so sánh kết quả zero-shot AgentInstruct của Llama-2-70b-chat với kết quả few-shot trên AddSub, IMDB và NarrativeQA trong Hình 10. Cụ thể, chúng tôi lấy mẫu 5 trường hợp huấn luyện từ mỗi bộ dữ liệu để đặt trước cho các trường hợp kiểm tra. Đáng ngạc nhiên, zero-shot AgentInstruct đạt được sự cạnh tranh với việc nhắc nhở few-shot. Zero-shot AgentInstruct, không có bất kỳ ví dụ few-shot nào, vượt trội hiệu suất few-shot trên AddSub và NarrativeQA, lần lượt hơn 4.3% và 23.7%, và thua 0.7% trên IMDB. Lý tưởng nhất, tất cả thông tin được mã hóa trong các ví dụ few-shot được AgentInstruct tìm thấy và tổng hợp thành các hướng dẫn agent, với thông tin bổ sung không có trong các ví dụ few-shot cũng được sử dụng. Chúng tôi giả thuyết rằng thông tin này có thể được trình bày một cách rõ ràng hơn trong các hướng dẫn agent thay vì thông qua các ví dụ để sử dụng tốt hơn khả năng tuân theo hướng dẫn và suy luận của LLMs. Như được hiển thị, zero-shot AgentInstruct có tiềm năng đạt được hoặc thậm chí đánh bại hiệu suất few-shot.

Self-Consistency Cuối cùng, chúng tôi so sánh kết quả zero-shot AgentInstruct với kết quả self-consistency (Wang et al., 2023c) của Llama-2-70b-chat trên AddSub, IMDB và NarrativeQA trong Hình 11. Chúng tôi điều chỉnh self-consistency cho thiết lập zero-shot như sau: chúng tôi lấy mẫu ba phản hồi sử dụng nhiệt độ 0.7, top-k sampling với k=40, và một seed được tạo ngẫu nhiên cho mỗi yêu cầu. Sau khi làm sạch các đầu ra, chúng tôi sử dụng bỏ phiếu đa số để xác định câu trả lời đồng thuận, chọn ngẫu nhiên để phá vỡ hòa. Trên AddSub, IMDB và NarrativeQA, zero-shot AgentInstruct vượt trội self-consistency lần lượt 5.8%, 7.5% và 1.9%. Hơn nữa, zero-shot AgentInstruct hiệu quả hơn về mặt tính toán so với self-consistency vì không cần lấy mẫu các đường dẫn suy luận nhiều lần.

4. Nghiên cứu Liên quan
Các mô hình ngôn ngữ lớn, như GPT-4 (OpenAI, 2023), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), PaLM-2 (Anil et al., 2023), BLOOM (Scao et al., 2023), OPT (Zhang et al., 2022a), LLaMA (Touvron et al., 2023a), Llama-2 (Touvron et al., 2023b), và nhiều mô hình khác (Radford et al., 2019; Wang & Komatsuzaki, 2021; Black et al., 2021; Smith et al., 2022; Hoffmann et al., 2022; Penedo et al., 2023) đã cho thấy hiệu suất đáng chú ý trên các tác vụ xử lý ngôn ngữ tự nhiên (NLP). Sau giai đoạn pretraining, việc finetuning bổ sung cho phép các mô hình (ví dụ: FLAN (Wei et al., 2022a), FLAN-T5 (Chung et al., 2022), InstructGPT (Ouyang et al., 2022)) căn chỉnh tốt hơn với các hướng dẫn của con người để hoàn thành các tác vụ. Hơn nữa, instruction-tuning đã cho phép LLMs (ví dụ: GPT-3.5 Turbo (OpenAI, 2022), Llama-2-chat (Touvron et al., 2023b), Self-Instruct (Wang et al., 2023d), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), Koala (Geng et al., 2023)) tương tác tốt hơn với người dùng thông qua đối thoại. Zero-shot AgentInstruct được xây dựng trên các mô hình ngôn ngữ tuân theo hướng dẫn, cho phép khả năng suy luận zero-shot tốt hơn thông qua việc sử dụng các hướng dẫn agent.

Các language agents (Yao et al., 2023b; Shinn et al., 2023; Xu et al., 2023; Park et al., 2023; Zhou et al., 2023b; Andreas, 2022; Wang et al., 2023a; Xi et al., 2023; Sumers et al., 2023; Chan et al., 2023) gần đây xuất hiện do khả năng lập kế hoạch tác vụ của LLMs. Với một tác vụ thường được thể hiện bằng ngôn ngữ tự nhiên, những agents này nhằm hoàn thành tác vụ trực tiếp. Chúng tôi dựa agent của mình trên ReAct (Yao et al., 2023b), sử dụng các bước "Thought, Action, Observation" có cấu trúc để hướng dẫn một mô hình hoàn thành một mục tiêu. Không giống như các agents hiện có nhằm giải quyết tác vụ trực tiếp, agent của chúng tôi tạo ra các hướng dẫn dành riêng cho tác vụ về cách hoàn thành tác vụ đã cho, tách biệt các bước lập kế hoạch agent và suy luận. Bên cạnh việc đạt được hiệu suất cạnh tranh với việc sử dụng agents trực tiếp, thiết kế của chúng tôi hóa ra hiệu quả hơn về chi phí.

Finetuning là một phương pháp hiệu quả để tạo ra các phản hồi chất lượng cao hơn từ LLMs trên các tác vụ downstream (Liu et al., 2019; Howard & Ruder, 2018). Khi quy mô mô hình tăng, finetuning trở nên kém thực tế hơn, mà các phương pháp tuning nhẹ (như prefix tuning (Li & Liang, 2021), prompt learning (Lester et al., 2021; Liu et al., 2023), LoRA (Hu et al., 2022)) đã cố gắng giải quyết. Ngay cả với những phương pháp như vậy, các kỹ thuật prompting đã thu hút sự chú ý như một thay thế rẻ hơn, chẳng hạn như chain of thought và zero-shot chain of thought prompting (Wei et al., 2022b; Kojima et al., 2022), cái sau mà chúng tôi tập trung vào trong công trình của mình. Few-shot learning, liên quan đến việc cung cấp một vài ví dụ minh họa tác vụ trước khi nhắc nhở các mô hình trong quá trình suy luận, thường hiệu quả trên một loạt các tác vụ (Brown et al., 2020; Dong et al., 2023). Chain of thought (CoT) prompting (Wei et al., 2022b) liên quan đến việc tạo ra một loạt các bước suy luận trung gian, có thể tăng đáng kể hiệu suất của LLMs trên các tác vụ suy luận phức tạp. Trong khi hành vi suy luận này truyền thống được học từ các minh chứng few-shot, Kojima et al. (2022) mở rộng CoT prompting cho thiết lập zero-shot, trong đó không có ví dụ nào được sử dụng. Gần đây hơn, các phương pháp mới như self-consistency (Wang et al., 2023c), plan-and-solve prompting (Wang et al., 2023b), tree of thought (Yao et al., 2023a), và graph of thought (Besta et al., 2023) đã cải thiện thêm chất lượng suy luận. Zero-shot AgentInstruct tập trung vào thiết lập zero-shot. Nó tổng quát hóa khả năng suy luận của LLMs cho nhiều tác vụ hơn bằng cách sử dụng các hướng dẫn dành riêng cho tác vụ được tạo ra từ agent của chúng tôi để căn chỉnh tốt hơn quá trình suy luận với một tác vụ cụ thể.

Các bộ dữ liệu điểm chuẩn NLP cung cấp một giao diện tiêu chuẩn để đánh giá LLMs trên các tác vụ downstream cụ thể. Các điểm chuẩn phổ biến (ví dụ: HELM (Liang et al., 2023), MMLU (Hendrycks et al., 2021), và nhiều điểm chuẩn khác (Shen et al., 2024; Yuan et al., 2024; Zhong et al., 2023; Srivastava et al., 2023; Suzgun et al., 2023; Chen et al., 2021; Wang et al., 2019; Rajpurkar et al., 2016)) đã trở thành một phần của đánh giá tiêu chuẩn của LLMs. Chúng tôi điểm chuẩn phương pháp của mình trên 29 bộ dữ liệu, bao gồm các kịch bản cốt lõi từ HELM (Liang et al., 2023) và các bộ dữ liệu suy luận từ Kojima et al. (2022), và bao gồm kết quả để so sánh. Đối với các tác vụ suy luận, phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp zero-shot hiện có (Brown et al., 2020; Kojima et al., 2022). Bên cạnh các tác vụ suy luận, phương pháp của chúng tôi tổng quát hóa cho các tác vụ hiểu ngôn ngữ tổng quát bao gồm sinh tạo và phân loại.

5. Kết luận
Nghiên cứu của chúng tôi đề xuất một cách mới để cải thiện khả năng suy luận zero-shot của các mô hình ngôn ngữ lớn trên các tác vụ hiểu ngôn ngữ tổng quát. Chúng tôi xây dựng một agent để hướng dẫn quá trình suy luận của LLMs. Nguyên tắc thiết kế cốt lõi của chúng tôi chia sẻ cùng tinh thần với thiết lập chưng cất kiến thức tiêu chuẩn. Agent của chúng tôi tự động tạo ra một bộ hướng dẫn dành riêng cho tác vụ cho một tập hợp rộng lớn các tác vụ. Những hướng dẫn giống nhau hướng dẫn các LLMs khác nhau suy luận tốt hơn trên nhiều trường hợp tác vụ để đưa ra dự đoán chất lượng cao. Phương pháp của chúng tôi là zero-shot nên không yêu cầu ví dụ đầu vào-đầu ra. Kết quả của chúng tôi xác nhận hiệu quả của phương pháp này, dẫn đến cải thiện đáng kể trên các tác vụ NLP khác nhau bao gồm sinh tạo, phân loại và suy luận. Cải thiện điểm trung bình đáng kể được đạt được so với thiết lập zero-shot tiêu chuẩn trên 29 bộ dữ liệu lần lượt cho Vicuna-13b, Llama-2-70b-chat và GPT-3.5 Turbo. Phương pháp của chúng tôi thắng trên 20 trong số 29 bộ dữ liệu được sử dụng để đánh giá. Chúng tôi tin rằng phong cách suy luận có thể hiểu được của con người của zero-shot AgentInstruct, cùng với việc sử dụng một agent tự động, có thể thay thế các phong cách prompting zero hoặc few-shot truyền thống hơn khi các mô hình trở thành những người suy luận tốt hơn.

Lời cảm ơn
Chúng tôi cảm ơn Ce Zhang và Together AI đã cho chúng tôi quyền truy cập vào một số tài nguyên tính toán. Nghiên cứu này được tài trợ một phần bởi Giải thưởng Nghiên cứu Sinh viên Mùa hè từ Văn phòng Nghiên cứu Sinh viên tại Đại học Washington ở St. Louis.

Tuyên bố Tác động
Phương pháp của chúng tôi được xây dựng trên LLMs, mà các rủi ro và tác hại tiềm ẩn được thảo luận trong Brown et al. (2020); OpenAI (2023); Touvron et al. (2023b). Như với bất cứ thứ gì được xây dựng trên LLMs, AgentInstruct có bộ hạn chế riêng và có thể gây ra tác hại tiềm ẩn khi bị lạm dụng. Một mối quan tâm đáng chú ý là xu hướng của AgentInstruct tạo ra các phản hồi không thực tế với độ tin cậy cao. Tuy nhiên, vì AgentInstruct có mô hình đưa ra suy luận từng bước dẫn đến câu trả lời của nó, nó cung cấp cho các nhà nghiên cứu cơ hội phân tích các trường hợp đầu ra sai lệch. Hơn nữa, AgentInstruct cho thấy cải thiện đáng chú ý trên các điểm chuẩn liên quan đến an toàn như TruthfulQA và CivilComments, cho thấy rằng việc sử dụng các hướng dẫn để căn cứ suy luận dọc theo một đường dẫn được xác định rõ có thể giảm nguy cơ đầu ra có hại.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên do quá dài và chủ yếu là tên riêng]

A. Thiết lập Thí nghiệm
A.1. Bộ dữ liệu
Chúng tôi đánh giá zero-shot AgentInstruct trên một lựa chọn rộng lớn các bộ dữ liệu bao gồm tất cả các kịch bản cốt lõi HELM (Liang et al., 2023) và tất cả các bộ dữ liệu suy luận được sử dụng để điểm chuẩn zero-shot CoT (Kojima et al., 2022). Các bộ dữ liệu được liệt kê trong Bảng 4, cùng với phân loại của chúng. Xem Hình 12 để biết chi tiết chung và A.4 để thu thập và xử lý mỗi bộ dữ liệu chi tiết hơn.

A.2. Mô hình
Chúng tôi tập trung vào các mô hình ngôn ngữ lớn sau:

• Vicuna-13b (Chiang et al., 2023): Một phiên bản được finetuned của LLaMA-13b (Touvron et al., 2023a) được huấn luyện trên thêm 70 nghìn cuộc trò chuyện do người dùng chia sẻ từ ShareGPT (ShareGPT, 2023). Trọng số phiên bản 1.1 delta được sử dụng.

• Llama-2-chat (Touvron et al., 2023b): Một họ các mô hình chat dựa trên Llama-2. Llama-2-chat được huấn luyện thêm sử dụng supervised finetuning, reward modeling và reinforcement learning from human feedback, và có ba kích thước: 7b, 13b và 70b. Chúng tôi tập trung vào mô hình 70b, nhưng chia sẻ kết quả từ các mô hình 7b và 13b trong Hình 7 và Phụ lục C.

• GPT-3.5 Turbo (OpenAI, 2022): Một phiên bản được finetuned của GPT-3.5 được huấn luyện thêm sử dụng reinforcement learning from human feedback. Cụ thể, phiên bản 0301 được sử dụng.

A.3. Chi tiết Triển khai
A.3.1. CHI TIẾT ZERO-SHOT AGENT INSTRUCT

Hướng dẫn Agent Chúng tôi triển khai agent của mình dựa trên triển khai zero-shot ReAct của LangChain (Chase, 2022; Yao et al., 2023b). Để bắt đầu việc tạo hướng dẫn, chúng tôi xây dựng đầu vào agent dựa trên thông tin bộ dữ liệu cơ bản bao gồm tên của bộ dữ liệu, các đầu ra có thể (nhãn nếu có, nếu không thì loại bộ dữ liệu như sinh tạo), và một vài ví dụ chỉ đầu vào không có nhãn sự thật. Chúng tôi sử dụng GPT-4 (OpenAI, 2023) bên trong agent của mình với nhiệt độ mặc định 0.3 và snapshot gpt-4-0613 khi tạo hướng dẫn. Với nhiệt độ đó, nếu chạy lại không đảm bảo tạo ra cùng đầu ra mỗi lần, cũng không đảm bảo rằng cùng các bước của pipeline của chúng tôi được đạt tới. Trong trường hợp hướng dẫn không được trả về, chúng tôi chạy lại pipeline cho đến khi có hướng dẫn hợp lệ. Chúng tôi cũng chạy lại pipeline trên một số bộ dữ liệu được chọn (NaturalQA (openbook), GSM8K) nơi tên của bộ dữ liệu không hữu ích. Trong trường hợp này, chúng tôi sử dụng một bí danh cho tên của bộ dữ liệu. Sử dụng tokenizer GPT-3.5 (cl100k base), số token trung bình trong các hướng dẫn là khoảng 265. Dựa trên ước tính thô, số token trung bình được sử dụng khi tạo hướng dẫn là 7000 cho mỗi bộ dữ liệu. Dưới đây chúng tôi cung cấp một ví dụ từ đầu đến cuối cho việc tạo hướng dẫn với IMDB:

[Tiếp tục với các phần còn lại của tài liệu...]
