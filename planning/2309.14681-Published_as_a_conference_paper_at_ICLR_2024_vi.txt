# 2309.14681.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2309.14681.pdf
# Kích thước tệp: 962710 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
CÁC VÍ DỤ MINH HỌA DO CON NGƯỜI TẠO RA CÓ CẦN THIẾT CHO HỌC TRONG NGỮ CẢNH KHÔNG?
Rui Li1, Guoyin Wang2, Jiwei Li3
1Đại học Khoa học và Công nghệ Trung Quốc
2Bytedance
3Đại học Zhejiang

TÓM TẮT
Bất chấp khả năng học với ít ví dụ đầy hứa hẹn của các mô hình ngôn ngữ lớn (LLM), mô hình tiêu chuẩn của Học trong ngữ cảnh (ICL) gặp phải những nhược điểm là dễ bị ảnh hưởng bởi các ví dụ minh họa được chọn và sự phức tạp trong việc tạo ra những ví dụ minh họa này. Trong bài báo này, chúng tôi đặt ra câu hỏi cơ bản về việc liệu các ví dụ minh họa do con người tạo ra có cần thiết cho ICL hay không. Để trả lời câu hỏi này, chúng tôi đề xuất chiến lược nhắc nhở tự suy ngẫm (SEC), một mô hình không cần các ví dụ minh họa do con người tạo ra. Điểm quan trọng của SEC là thay vì sử dụng các ví dụ được tạo thủ công làm ví dụ minh họa trong ICL, SEC yêu cầu các LLM trước tiên tự tạo ra các ví dụ minh họa, dựa trên đó kết quả cuối cùng được tạo ra. SEC là một khung linh hoạt và có thể được điều chỉnh cho cả ICL thông thường và chuỗi suy nghĩ (CoT), nhưng với sự dễ dàng hơn: vì quá trình tạo thủ công của cả ví dụ và lý luận có thể được tiết kiệm. Các thí nghiệm mở rộng trong lý luận toán học, lý luận thông thường, hiểu ngôn ngữ đa nhiệm vụ, và các benchmark tạo mã, cho thấy rằng SEC, không yêu cầu các ví dụ minh họa được tạo thủ công, vượt trội đáng kể so với chiến lược học không ví dụ, và đạt được kết quả tương đương với ICL có các ví dụ minh họa được tạo thủ công. Điều này chứng minh rằng, đối với nhiều nhiệm vụ, các LLM đương đại sở hữu mức độ năng lực đủ để phụ thuộc hoàn toàn vào khả năng của chính chúng để ra quyết định, loại bỏ nhu cầu về dữ liệu huấn luyện bên ngoài. Mã có sẵn tại https://github.com/ruili33/SEC.1

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) (Zeng et al., 2022; Chowdhery et al., 2022; Wang et al., 2022; Zhang et al., 2023; Touvron et al., 2023; OpenAI, 2023) đã cho thấy khả năng học trong ngữ cảnh (Brown et al., 2020; Dong et al., 2022; Qin et al., 2023; Sun et al., 2023a): được cung cấp một vài ví dụ được chú thích làm ví dụ minh họa, các LLM có thể tạo ra cho một đầu vào thử nghiệm mới (Brown et al., 2020). Mô hình tiêu chuẩn của Học trong ngữ cảnh (ICL) vẫn gặp phải những nhược điểm rõ ràng sau:
(1) hiệu suất cuối cùng cực kỳ nhạy cảm với các ví dụ minh họa được chọn (Liu et al., 2022; Lu et al., 2023), và cho đến nay, không có tiêu chí được thống nhất rộng rãi cho việc lựa chọn ví dụ minh họa hoàn hảo;
(2) việc tạo ra các ví dụ minh họa có thể tốn nhiều công sức, rắc rối hoặc thậm chí bị cấm: trong nhiều tình huống ICL, các ví dụ minh họa không chỉ chứa đầu vào và nhãn tương ứng, mà còn cả quá trình lý luận (Wei et al., 2022b; Sun et al., 2023b; Yao et al., 2023) được tạo ra bởi người chú thích. Đối với nhiều nhiệm vụ (ví dụ: tóm tắt), việc con người diễn đạt quá trình lý luận đằng sau quyết định là không tầm thường.

Một câu hỏi quan trọng nảy sinh, liệu chúng ta có thực sự cần con người cung cấp cho các LLM các ví dụ minh họa, hay các LLM có thể tự tạo ra các ví dụ minh họa không? Khi so sánh phương pháp ICL với sự tương tác của học sinh với gia sư, trong khung ICL, gia sư khởi xướng quá trình bằng cách cung cấp cho học sinh một tập hợp các trường hợp tương tự như gợi ý gợi ý, dựa trên đó học sinh đưa ra câu trả lời của mình. Chắc chắn có một mô hình thay thế cho ICL, nơi một học sinh có năng lực chỉ dựa vào trí nhớ của chính mình để tìm các ví dụ tương tự, đi đến câu trả lời một cách độc lập, loại bỏ nhu cầu về bất kỳ hướng dẫn hoặc ví dụ nào từ gia sư.

1Email: rui li@mail.ustc.edu.cn, guoyin.wang@bytedance.com, jiwei li@zju.edu.cn
1arXiv:2309.14681v4 [cs.LG] 21 Feb 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Hình ảnh so sánh giữa ICL thông thường và SEC thông thường, với các phần khác nhau của prompt và kết quả được tô sáng bằng các màu khác nhau để nhấn mạnh]

Hình 1: So sánh giữa ICL thông thường và SEC thông thường. Các phần khác nhau của prompt và kết quả được tô sáng bằng các màu khác nhau để nhấn mạnh.

Trong bài báo này, chúng tôi đề xuất chiến lược nhắc nhở tự suy ngẫm (SEC), một mô hình thay thế cho ICL. Điểm quan trọng của SEC là thay vì sử dụng các ví dụ được tạo thủ công làm ví dụ minh họa, SEC yêu cầu các LLM trước tiên tự tạo ra các ví dụ minh họa, dựa trên đó kết quả cuối cùng được tạo ra. Điều này tương tự như quá trình trên mà học sinh chỉ dựa vào trí nhớ của chính mình để tìm các ví dụ tương tự, thay vì các ví dụ từ gia sư. SEC giải quyết hiệu quả các nhược điểm của ICL: nó không chỉ giúp chúng ta tiết kiệm những nỗ lực vất vả trong việc tạo ví dụ minh họa, mà quan trọng hơn, loại bỏ sự bất ổn của các prompt được tạo bởi con người.

SEC là một khung linh hoạt không chỉ có thể dễ dàng kết hợp với các chiến lược tăng cường hiện có cho ICL, mà còn với sự dễ dàng đáng chú ý hơn: ví dụ, đối với chiến lược chuỗi suy nghĩ (CoT) nơi các ví dụ minh họa bao gồm quá trình lý luận, trong SEC, chúng ta có thể nhắc nhở các LLM trước tiên tự động tạo ra không chỉ đầu vào và nhãn, mà còn cả quá trình lý luận liên quan. Làm như vậy, những nỗ lực tạo lý luận trong ICL có thể được bảo tồn. Các ví dụ minh họa của SEC thông thường và CoT-SEC được hiển thị trong Hình 1 (b) và 2 (b).

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Hình ảnh so sánh giữa CoT-ICL và CoT-SEC, với các phần khác nhau của prompt và kết quả được tô sáng bằng các màu khác nhau để nhấn mạnh]

Hình 2: So sánh giữa CoT-ICL và CoT-SEC. Các phần khác nhau của prompt và kết quả được tô sáng bằng các màu khác nhau để nhấn mạnh.

Chúng tôi tiến hành thí nghiệm trên nhiều LLM và một loạt rộng các nhiệm vụ, bao gồm lý luận toán học, lý luận thông thường, hiểu ngôn ngữ đa nhiệm vụ, và các benchmark tạo mã. Đáng chú ý, Không có quyền truy cập vào BẤT KỲ ví dụ huấn luyện nào hoặc sự can thiệp của con người, SEC đạt được kết quả tương đương với ICL trên tất cả các benchmark trong cả tình huống ít ví dụ và CoT, bao gồm MATH (33.5% so với 31.2%) (Hendrycks et al., 2021), MMLU (71.4% so với 70.4%) (Hendrycks et al.) và HumanEval (76.2% so với 73.2%) (Chen et al., 2021). Kết quả này chứng minh rằng các LLM đương đại sở hữu mức độ năng lực đủ để phụ thuộc hoàn toàn vào khả năng của chính chúng để tạo ra các ví dụ minh họa, loại bỏ nhu cầu về dữ liệu huấn luyện bên ngoài.

Từ góc độ rộng hơn, SEC là một mô hình học không ví dụ không có dữ liệu huấn luyện, trong khi ICL vốn dĩ vẫn là một mô hình học có giám sát. Dựa trên quan sát rằng SEC không ví dụ hoạt động tương đương với ICL có giám sát sử dụng dữ liệu cụ thể cho miền, chúng tôi chứng minh rằng với khả năng tổng quát hóa của các LLM hiện tại, có tiềm năng rằng dữ liệu huấn luyện có giám sát có thể không cần thiết trong tương lai. Chúng tôi hy vọng SEC sẽ mở ra cánh cửa cho nghiên cứu tiếp theo hướng tới phương hướng này.

2 NHẮC NHỞ TỰ SUY NGẪM

2.1 KIẾN THỨC CƠ BẢN

ICL Thông thường Trong chiến lược ICL thông thường, các LLM trước tiên được cung cấp một vài ví dụ được gán nhãn (ít ví dụ) được tạo bởi con người làm ví dụ minh họa, nơi mỗi ví dụ minh họa là một cặp đầu vào-đầu ra. Các ví dụ minh họa được theo sau bởi đầu vào thử nghiệm, và các LLM được nhắc nhở để tạo ra nhãn cho đầu vào thử nghiệm dựa trên các ví dụ minh họa đã cho. Khác với chiến lược tiền huấn luyện và tinh chỉnh (Devlin et al., 2019), ICL cho phép mô hình đưa ra dự đoán thông qua một lời gọi API duy nhất.

CoT-ICL Để tăng cường hiệu suất trên các nhiệm vụ đòi hỏi lý luận, chiến lược Nhắc nhở Chuỗi suy nghĩ (CoT) (Wei et al., 2022b) kết hợp quá trình lý luận từng bước vào prompt cho các LLM, như được minh họa trong phần màu hồng trong Hình 2. Chiến lược CoT có thể được kết hợp với ICL thông thường, nơi mỗi ví dụ minh họa bao gồm không chỉ đầu vào và nhãn đầu ra, mà còn cả quá trình lý luận để có được nhãn.

2.2 NHẮC NHỞ TỰ SUY NGẪM

Xem xét sự khó khăn và không đáng tin cậy trong các prompt ít ví dụ được tạo bởi con người, chúng tôi đề xuất nhắc nhở tự suy ngẫm (SEC), một chiến lược nhắc nhở hoàn toàn dựa vào các LLM để tạo ra các ví dụ ít ví dụ phù hợp với từng mẫu đầu vào thử nghiệm. Chúng tôi mô tả SEC trong cả tình huống học ít ví dụ thông thường (SEC Thông thường) và tình huống chuỗi suy nghĩ (CoT-SEC) theo thứ tự dưới đây:

2.2.1 SEC THÔNG THƯỜNG

Prompt tạo ví dụ minh họa SEC cho tình huống ít ví dụ thông thường bao gồm các thành phần sau:

• Đầu vào thử nghiệm (văn bản được tô sáng màu xanh lá cây): ở đầu prompt, chúng tôi trực tiếp cung cấp ví dụ thử nghiệm.

• Hướng dẫn cho việc tạo ví dụ minh họa ít ví dụ (văn bản được tô sáng màu vàng): một hướng dẫn rõ ràng để yêu cầu các LLM tạo ra các ví dụ minh họa dựa trên đầu vào thử nghiệm.

• Hướng dẫn định dạng đầu ra (văn bản được tô sáng màu tím): định nghĩa rõ ràng định dạng đầu ra để tạo điều kiện thuận lợi cho việc trích xuất câu trả lời từ chuỗi văn bản được tạo.

Sau đó, chúng tôi triển khai mô hình ICL thông thường dựa trên các ví dụ minh họa được tạo bởi mô hình. Sự khác biệt giữa SEC Thông thường và ICL thông thường là trước đây yêu cầu các LLM tạo ra các ví dụ minh họa trong khi sau này sử dụng các ví dụ minh họa được tạo bởi con người.

2.2.2 COT-SEC

SEC có thể được điều chỉnh cho chiến lược CoT một cách dễ dàng. Prompt tạo ví dụ minh họa SEC trong CoT-SEC vẫn bao gồm ba thành phần, tức là đầu vào thử nghiệm, hướng dẫn cho việc tạo ví dụ minh họa ít ví dụ và hướng dẫn định dạng đầu ra. Sự khác biệt là trong hướng dẫn cho việc tạo ví dụ minh họa ít ví dụ, các LLM được yêu cầu tạo ra các ví dụ minh họa với không chỉ đầu vào và nhãn, mà còn cả quá trình lý luận.

Sự khác biệt giữa CoT-SEC và CoT-ICL là trước đây yêu cầu các LLM tạo ra các ví dụ minh họa với quá trình lý luận trong khi sau này sử dụng các ví dụ minh họa được tạo bởi con người với quá trình lý luận. So với ICL, lợi thế của SEC như sau:

• Không cần các ví dụ minh họa được tạo thủ công: vì các ví dụ minh họa được tạo ra bởi chính các LLM, SEC tiết kiệm nỗ lực của con người trong việc tạo ví dụ minh họa, cùng với quá trình phức tạp cho việc lựa chọn và sắp xếp ví dụ minh họa.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

• Các ví dụ minh họa được điều chỉnh cho đầu vào thử nghiệm: các ví dụ minh họa được tạo ra dựa trên mẫu đầu vào. Do đó, chúng được tùy chỉnh để phù hợp với từng ví dụ thử nghiệm. Trong thí nghiệm, chúng tôi thấy rằng chiến lược này phục vụ mục đích tương tự như ví dụ minh họa KNN trong tìm kiếm KNN, dẫn đến hiệu suất cạnh tranh hơn trên một số tập dữ liệu (chi tiết trong Phần 3 và Phụ lục B.7).

3 THÍ NGHIỆM

3.1 NHIỆM VỤ VÀ TẬP DỮ LIỆU

Chúng tôi đánh giá SEC trong các nhiệm vụ và tập dữ liệu sau (chi tiết trong Phụ lục A.1): Lý luận Toán học: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021); Lý luận Thông thường: AI2 Reasoning Challenge (ARC) (Clark et al., 2018); Hiểu Ngôn ngữ Đa nhiệm vụ: MMLU (Hendrycks et al.), C-Eval (Huang et al., 2023); Tạo Mã: HumanEval (Chen et al., 2021).

Chúng tôi sử dụng độ chính xác khớp chính xác làm thước đo đánh giá cho tập dữ liệu GSM8K và Math. Đối với tập dữ liệu GSM8K, chúng tôi trích xuất đối tượng số đầu tiên trong chuỗi câu trả lời và chuyển đổi nó thành số nguyên. Đối với tập dữ liệu Math, chúng tôi kết hợp hàm chuẩn hóa trong Wei et al. (2022b) và Hendrycks et al. (2021) để đạt được hàm chuẩn hóa của chúng tôi. Đối với HumanEval, chúng tôi trực tiếp sử dụng mã trong kho lưu trữ Github HumanEval1 (Chen et al., 2021) để làm sạch và đánh giá câu trả lời. Chi tiết về việc trích xuất các ví dụ minh họa ít ví dụ có trong Phụ lục A.5.

[Bảng 1: Số lượng shot được sử dụng trong các thí nghiệm chính]
MATH GSM8K ARC MMLU C-Eval HumanEval
Số Lượng Shot 4 5 5 4 4 4

3.2 BASELINE

Chúng tôi so sánh SEC với chiến lược không ví dụ và chiến lược ICL (Brown et al., 2020) trong cả tình huống thông thường và chuỗi suy nghĩ (Wei et al., 2022b). Để đảm bảo so sánh công bằng, số lượng ví dụ minh họa được tạo bởi con người và được tạo bởi LLM là giống nhau. Số lượng shot cho các nhiệm vụ khác nhau được hiển thị trong Bảng 1. Đối với tất cả các baseline của chúng tôi, chúng tôi áp dụng ChatGPT (gpt-3.5-turbo), GPT4 (OpenAI, 2023) và Llama2 34B (Touvron et al., 2023) làm backbone mô hình, chi tiết trong Phụ lục A.2. Nếu không được chỉ định khác, chúng tôi đang sử dụng GPT-3.5 cho các thí nghiệm của chúng tôi.

[Bảng 2: So sánh giữa SEC và baseline trên GPT-3.5]
(GPT-3.5) Toán học Thông thường Hiểu NLU Đa nhiệm vụ Mã
MATH GSM8K ARC MMLU C-Eval HumanEval
Kết quả Đã xuất bản
Vanilla ICL - 57.1a85.2a70.0a[51.0c] [ 48.1a]
CoT-ICL - 74.9b- 67.3b54.6c-
Kết quả của Chúng tôi
Zero-shot 16.6 31.4 80.1 64.7 51.0 48.8
Zero-shot CoT 31.7 73.4 84.1 60.5 50.5 -
Vanilla ICL 20.3 57.1a86.5 70.4 55.0 73.8
Vanilla SEC 18.1 65.4 85.9 68.3 54.0 75.6
CoT-ICL 31.2 77.4 87.9 69.6 53.1 -
CoT-SEC 33.5 77.0 86.9 71.4 54.6 -

1https://github.com/openai/human-eval

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Bảng 3: So sánh giữa SEC và baseline trên GPT-4]
(GPT-4) Toán học Thông thường Hiểu NLU Đa nhiệm vụ Mã
MATH GSM8K ARC MMLU C-Eval HumanEval
Kết quả Đã xuất bản
Vanilla ICL - - 96.3a86.4a[66.4c] [ 67.0a]
CoT-ICL 42.6b92.0b- 86.4b68.7c-
Kết quả của Chúng tôi
Zero-shot 26.4 68.3 88.5 82.0 64.8 67.0a
Zero-shot CoT 32.6 86.7 90.2 82.2 64.4 -
Vanilla ICL 31.2 91.5 94.4 86.6 67.7 83.5
Vanilla SEC 35.0 91.7 94.7 86.1 68.1 83.0
CoT-ICL 42.3 92.0a95.1 86.0 67.0 -
CoT-SEC 41.9 92.1 96.2 86.5 67.8 -

3.3 KẾT QUẢ

Bảng 2, Bảng 3, và Bảng 9 trong Phụ lục A.3 tóm tắt hiệu suất của SEC trên 6 benchmark trên GPT3.5, GPT4, Llama2 34B. Nhìn chung, SEC đạt được hiệu suất tốt hơn đáng kể so với nhắc nhở không ví dụ, và hiệu suất tương đương với ICL ít ví dụ và CoT-ICL trong các thiết lập tương ứng.

Mặc dù thực tế là quyết định cuối cùng của SEC dựa vào các ví dụ minh họa, SEC về bản chất là một mô hình không ví dụ (và không giám sát) do thực tế rằng những ví dụ minh họa này được tạo ra bởi chính LLM. SEC thu hẹp khoảng cách giữa nhắc nhở không ví dụ và ICL ít ví dụ (Kojima et al.; Brown et al., 2020), thông qua việc tự động tạo ra các ví dụ minh họa ít ví dụ. Điều này chứng minh rằng, đối với nhiều nhiệm vụ, các LLM đương đại đủ có năng lực để phụ thuộc vào khả năng của chính chúng để ra quyết định, loại bỏ nhu cầu về dữ liệu huấn luyện bên ngoài.

Lý luận Toán học Chủ yếu, đáng ngạc nhiên, trong MATH, SEC vượt trội đáng kể so với ICL trong cả tình huống CoT GPT-3.5 và GPT-4.0 chỉ câu trả lời, mặc dù không có tập dữ liệu huấn luyện. Điều này là do các ví dụ minh họa trong SEC được tạo ra phù hợp với từng trường hợp thử nghiệm. Ngược lại, ICL sử dụng các ví dụ ít ví dụ giống hệt nhau cho toàn bộ tập dữ liệu thay vì tùy chỉnh cho các trường hợp thử nghiệm riêng biệt.

Hình 3 minh họa phân tích chi tiết kết quả trên tập dữ liệu MATH, được phân loại theo chủ đề phụ. Chúng tôi phát hiện rằng CoT-SEC vượt trội hơn CoT-ICL trong 5 chủ đề phụ khác ngoài Hình học. Một quan sát khác là CoT-SEC luôn vượt trội hơn SEC thông thường trong tất cả 6 chủ đề phụ, ngay cả trong Đại số và Tiền giải tích, nơi CoT-ICL kém hiệu quả hơn ICL thông thường.

[Hình 3: Kết quả thí nghiệm trên tập dữ liệu MATH theo chủ đề phụ]

Hiểu Ngôn ngữ Đa nhiệm vụ Hiệu quả của SEC được chứng minh thêm bởi hiệu suất cạnh tranh của nó trong nhiệm vụ Hiểu NLU Đa nhiệm vụ, bao gồm một phổ rộng hơn 50 miền và ngành. Hơn nữa, hiệu suất cạnh tranh của SEC trên C-Eval cho thấy khả năng của nó trong tình huống đa ngôn ngữ. Phân tích chi tiết kết quả trên MMLU được hiển thị trong Phụ lục A.4.

Tạo Mã SEC vượt trội đáng kể so với baseline không ví dụ và baseline ICL thông thường cho nhiệm vụ tạo mã.

Những kết quả này không chỉ chứng minh hiệu quả của SEC, mà còn đặt câu hỏi về giá trị của dữ liệu huấn luyện được chú thích với sự hiện diện của các LLM hiện tại. Các thí nghiệm của chúng tôi có thể lập luận chứng minh rằng

2Bất kỳ kết quả nào được bao trong ngoặc đều biểu thị dữ liệu có nguồn gốc từ nhắc nhở không ví dụ. Các chỉ số trên được sử dụng để chỉ ra kết quả đã được trích dẫn từ các nghiên cứu trước đây:a(OpenAI, 2023),b(Fu et al., 2023),c(Huang et al., 2023).

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Hình 4: Hiệu ứng của số lượng ví dụ trên tập dữ liệu GSM8K và HumanEval]

các LLM với quy mô GPT3.5 và Llama2 34B vốn dĩ sở hữu khả năng đạt được hiệu suất tương đương với nhắc nhở ít ví dụ trong cả tình huống ít ví dụ thông thường và CoT, điều này cho thấy tiềm năng rằng dữ liệu huấn luyện có giám sát có thể không không thể thiếu trong tương lai.

4 NGHIÊN CỨU ABLATION

Số lượng Shot Chúng tôi điều tra hiệu ứng của số lượng shot trên cả SEC và ICL. Hình 4 hiển thị kết quả từ tập dữ liệu GSM8K và tập dữ liệu HumanEval. Phân tích của chúng tôi nhận ra những khác biệt rõ ràng giữa đặc điểm của các ví dụ minh họa ít ví dụ trong SEC và những ví dụ được tạo thủ công. Trong bối cảnh của hai tập dữ liệu được kiểm tra, SEC thường đạt hiệu suất tối ưu với ít shot hơn (ví dụ: 2 shot) so với ICL. Giải thích như sau: vì SEC có thể tạo ra các ví dụ minh họa phù hợp với đầu vào, không cần cung cấp các ví dụ minh họa đa dạng để làm cho prompt có thể áp dụng cho nhiều loại đầu vào thử nghiệm khác nhau. Do đó, cần ít ví dụ minh họa hơn cho SEC.

[Bảng 4: Số dòng trung bình trong các ví dụ minh họa được tạo bởi mô hình và các giải pháp chính tắc]
1st shot 2nd shot 3rd shot 4th shot Giải pháp Chính tắc
Dòng Trung bình 7.3 6.9 6.8 7.1 7.8

[Hình 5: Hiệu suất của tất cả bốn chiến lược nhắc nhở trên ba mô hình trong họ GPT3.5 trên GSM8K]

Một vấn đề cụ thể nổi bật là trên HumanEval, chúng tôi quan sát khi số lượng shot tăng, hiệu suất của SEC giảm nhẹ. Để điều tra, chúng tôi cung cấp so sánh về độ phức tạp giữa các ví dụ minh họa ít ví dụ được tạo bởi mô hình và các giải pháp chính tắc. Độ phức tạp được đo bằng độ dài, tức là số dòng của câu trả lời. Kết quả được hiển thị trong Bảng 4. Rõ ràng là độ phức tạp của các ví dụ minh họa ít ví dụ được tạo bởi mô hình nhỏ hơn đáng kể so với độ phức tạp của các giải pháp chính tắc, điều này có thể khiến mô hình đánh giá sai độ phức tạp của nhiệm vụ trong một số mẫu thử nghiệm. Phân tích chi tiết sẽ được hiển thị trong Phụ lục B.5.

So sánh SEC với ICL sử dụng các LLM với khả năng khác nhau Để điều tra hiệu ứng của khả năng mô hình đối với hiệu suất của SEC, chúng tôi tiến hành thí nghiệm trên tất cả bốn chiến lược nhắc nhở sử dụng ba mô hình từ họ GPT3.5 (chi tiết trong Phụ lục B.6). Từ kết quả hiển thị trong Hình 5, chúng ta có thể kết luận rằng SEC kém hiệu quả hơn ICL khi mô hình không đủ mạnh. Điều này có thể do

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

[Bảng 5: Tính đúng đắn của năm ví dụ minh họa ít ví dụ cho 20 dự đoán mô hình đúng và 20 dự đoán mô hình sai trong tập dữ liệu GSM8K]
Tính đúng đắn Tất cả Đúng Lỗi Nhỏ Lỗi Lớn Tất cả Sai
Đúng 13/20 1/20 3/20 3/20
Sai 2/20 5/20 7/20 6/20

thực tế rằng các mô hình yếu hơn gặp khó khăn trong việc tuân theo các hướng dẫn và tạo ra các ví dụ ít ví dụ chất lượng kém, khiến SEC không nổi bật so với ICL khi được triển khai trên các mô hình nhỏ hơn.

Phân tích Lỗi trong GSM8K Chúng tôi kiểm tra thủ công 20 dự đoán mô hình đúng và 20 dự đoán mô hình sai từ GSM8K, đánh giá tính đúng đắn của các ví dụ minh họa ít ví dụ của chúng. Kết quả được tóm tắt trong Bảng 5. Chúng tôi phát hiện rằng, trong GSM8K, tỷ lệ đúng của các ví dụ minh họa ít ví dụ cho các dự đoán sai thấp hơn đáng kể so với các mẫu đúng (10% so với 65%). Do đó, các lỗi của dự đoán cuối cùng có thể được quy cho chất lượng thấp của các ví dụ minh họa ít ví dụ, và chúng tôi để lại cho công việc tương lai để tinh chỉnh các ví dụ minh họa được tạo bởi mô hình. Vui lòng tham khảo Phụ lục B.1 để biết thêm chi tiết về phân tích lỗi.

Tại sao các ví dụ minh họa ít ví dụ sai có thể dẫn đến dự đoán cuối cùng đúng, trong khi các ví dụ minh họa ít ví dụ đúng cũng có thể dẫn đến dự đoán sai? Các lỗi trong các ví dụ minh họa ít ví dụ được tạo bởi LLM có thể được phân loại thành bốn loại chính: lỗi trích xuất câu trả lời, lỗi tính toán, lỗi câu hỏi và lỗi logic, có trong Phụ lục B.2.

Đối với các ví dụ minh họa ít ví dụ sai dẫn đến kết quả đúng, những lỗi này trong các ví dụ minh họa ít ví dụ thường thuộc về lỗi trích xuất câu trả lời, lỗi tính toán, lỗi câu hỏi thay vì lỗi cơ bản trong quá trình lý luận (lỗi logic).

Đối với các ví dụ minh họa ít ví dụ đúng cuối cùng dẫn đến kết quả sai, thông thường, mặc dù các ví dụ minh họa ít ví dụ là đúng, chúng không phù hợp đủ gần với câu hỏi thử nghiệm, cản trở mô hình trích xuất và áp dụng kiến thức phù hợp từ các ví dụ minh họa. Thỉnh thoảng, các ví dụ minh họa được tạo có thể quá đơn giản, khiến mô hình đánh giá sai về sự phức tạp của câu hỏi thử nghiệm. Các ví dụ chi tiết và thảo luận có sẵn trong Phụ lục B.3.

Sự khác biệt hiệu suất giữa CoT-SEC và CoT-ICL trong GSM8K. Chúng tôi hiển thị kết quả kiểm tra độ chính xác của 1319 mẫu thử nghiệm trong GSM8k dưới cả CoT-SEC và CoT-ICL trong Hình 6. Mặc dù hiệu suất tổng thể của hai phương pháp này rất tương tự, hiệu suất của chúng trên các vấn đề cụ thể riêng lẻ là khác nhau: khoảng 22% của các mẫu có tính đúng đắn ngược lại giữa các chiến lược SEC và ICL, trái ngược rõ với 11.8% nơi cả hai đều thất bại. Trong Phụ lục B.4, chúng tôi sẽ điều tra sơ bộ các đặc điểm của những khác biệt này. Sự khác biệt này nhấn mạnh thêm rằng hai chiến lược nhắc nhở này mỗi cái đều có các lĩnh vực chuyên môn tương ứng của chúng.

[Hình 6: Kết quả của 1319 mẫu thử nghiệm trong tập dữ liệu GSM8k dưới cả CoT-SEC và CoT-ICL]

[Bảng 6: So sánh giữa CoT-SEC và Auto-CoT]
Phương pháp GSM8k ARC
Zero Shot CoT 73.4 84.1
CoT-ICL 77.4 87.9
Auto-CoT 77.5 87.8
CoT-SEC 77.0 86.9

So sánh giữa SEC và Auto-CoT Hiệu suất của SEC và Auto-CoT (Zhang et al., 2022b) được tóm tắt trong Bảng 6. Hiệu suất của CoT-SEC tương đương với Auto-CoT, ngay cả khi không có quyền truy cập vào toàn bộ tập dữ liệu thử nghiệm và phân cụm bổ sung.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

5 CÔNG VIỆC LIÊN QUAN

Học trong ngữ cảnh Để tăng cường hiệu suất của ICL, nghiên cứu trước đây khám phá việc tối ưu hóa việc lựa chọn và sắp xếp các ví dụ ít ví dụ (Rubin et al., 2021; Zhang et al., 2022a; Wu et al., 2022; Lu et al., 2021; Fu et al., 2022; Zhou et al., 2022b; Su et al., 2022) như kNN Prompting (Xu et al., 2023). SEC và kNN Prompting chia sẻ ý tưởng sử dụng các ví dụ minh họa phù hợp với từng câu hỏi thử nghiệm. Việc kết hợp quá trình lý luận và tăng cường thông tin (Lampinen et al., 2022) cũng đã được đề xuất, ví dụ: CoT Prompting (Wei et al., 2022b), CARP (Sun et al., 2023b), Least-to-Most Prompting (Zhou et al., 2022a) và thêm hướng dẫn cụ thể cho nhiệm vụ (Mishra et al., 2022; Wei et al., 2022a; Sanh et al., 2022).

Xem xét chi phí trong việc tạo prompt thủ công, nhiều chiến lược nhắc nhở tự động đã được đề xuất (Sorensen et al., 2022; Shin et al., 2020). Kim et al. (2022) sử dụng PLM để tự động tạo ra các ví dụ minh họa. Li et al. (2022) đề xuất khung Self-Prompting, trước tiên tạo ra một kho ngữ liệu huấn luyện và sau đó chọn các ví dụ ít ví dụ cho mỗi mẫu thử nghiệm bằng phân cụm. So với Li et al. (2022), phương pháp của chúng tôi cung cấp một cách linh hoạt hơn để tạo ra các ví dụ minh họa mà không cần tạo ra nhiều mẫu huấn luyện trước hoặc phân cụm và lựa chọn thêm. Bên cạnh đó, trong khi Li et al. (2022) tập trung nghiên cứu của họ chỉ vào QA, chúng tôi mở rộng SEC cho nhiều nhiệm vụ mới.

Bên cạnh đó, công việc gần đây đã thảo luận về sự bất ổn trong ICL. Cụ thể, việc lựa chọn và xáo trộn các ví dụ ít ví dụ và cấu trúc của prompt có thể gây ra biến động mạnh về độ chính xác (Zhao et al., 2021; Lu et al., 2022; Liu et al., 2022; Lu et al., 2023). Tuy nhiên, SEC giảm thiểu vấn đề này, vì các ví dụ ít ví dụ chỉ phụ thuộc vào các LLM không có bất kỳ sự can thiệp bên ngoài nào.

Nhắc nhở Chuỗi suy nghĩ Wei et al. (2022b) đề xuất nhắc nhở CoT, một chiến lược nhắc nhở tích hợp các ví dụ huấn luyện ít ví dụ với quá trình lý luận trung gian. Zero-shot CoT (Kojima et al.) sử dụng một prompt đơn giản, "Hãy suy nghĩ từng bước", để kích thích lý luận trong đầu ra và đạt được kết quả khuyến khích. Theo Kojima et al., chúng tôi thêm một hướng dẫn CoT cụ thể để tạo ra lý luận. Phát hiện của chúng tôi hỗ trợ Kojima et al. rằng các LLM là những người lý luận không ví dụ tốt.

Zhang et al. (2022b) đề xuất Auto-CoT, một chiến lược nhắc nhở tự động tận dụng zero-shot CoT để tạo ra lý luận. Sự khác biệt chính giữa Zhang et al. (2022b) và công việc của chúng tôi là Zhang et al. (2022b) yêu cầu quyền truy cập vào toàn bộ tập thử nghiệm và liên quan đến truy vấn và phân cụm chuyên sâu, trong khi SEC chỉ yêu cầu hai truy vấn cho mỗi mẫu thử nghiệm.

6 KẾT LUẬN

Trong bài báo này, chúng tôi giới thiệu nhắc nhở tự suy ngẫm như một chiến lược nhắc nhở đơn giản, hiệu quả về tài nguyên và có thể áp dụng rộng rãi cho các LLM để tăng cường khả năng không ví dụ của chúng. Mô hình mới này giải quyết một số vấn đề liên quan đến các phương pháp ICL có giám sát, như thiếu dữ liệu được chú thích thủ công và sự bất ổn của hiệu suất. Phương pháp của chúng tôi cung cấp một khung đánh giá toàn diện và nhất quán hơn cho các LLM.

Các thí nghiệm của chúng tôi cho thấy rằng SEC hoạt động tương đương với ICL trong cả tình huống chỉ câu trả lời và CoT. Theo hiểu biết tốt nhất của chúng tôi, SEC đạt được hiệu suất không ví dụ mạnh nhất trên nhiều nhiệm vụ khác nhau. Hiệu suất đặc biệt này cho thấy lời hứa rằng dữ liệu được chú thích có thể thừa với khả năng tổng quát hóa của các LLM. Hơn nữa, sự khác biệt giữa khả năng của CoT-SEC và CoT-ICL có thể cho thấy lời hứa của việc tích hợp thêm các chiến lược này.

HẠN CHẾ

[Bảng 7: Hiệu suất của SEC và các phương pháp baseline trên bài toán cộng cơ số 5 3 chữ số]
Phương pháp Độ chính xác
Zero-shot 26.5
Zero-shot CoT 19.0
Vanilla ICL 28.0
CoT-ICL 27.0
Vanilla SEC 27.0
CoT-SEC 24.0

Xem xét rằng SEC sử dụng các ví dụ minh họa được tạo từ các LLM, nó có thể gặp phải sự suy giảm hiệu suất trong các tình huống nơi mô hình không đủ mạnh, hoặc dữ liệu thử nghiệm không được đại diện đầy đủ trong tập huấn luyện. Để điều tra vấn đề này, chúng tôi thiết kế một tập thử nghiệm mới chứa 200 bài toán cộng cơ số 5 3 chữ số xuất hiện hiếm khi trong ngôn ngữ hàng ngày và trên các trang web. Chúng tôi thử nghiệm SEC và các phương pháp baseline trên tập dữ liệu này. Kết quả, như được tóm tắt trong Bảng 7, cho thấy rằng SEC có xu hướng thể hiện sự suy giảm nhẹ về hiệu suất trên những nhiệm vụ này so với các phương pháp ICL.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ƠN

Công việc này được hỗ trợ bởi Chương trình R&D Chính sách Quốc gia của Trung Quốc (Số 2022ZD0119101). Chúng tôi xin gửi lời cảm ơn chân thành nhất đến các nhà đánh giá, Chủ tịch Khu vực, Ban Chương trình, và Chủ tịch Khu vực Cao cấp vì những hiểu biết và đề xuất vô giá của họ đã đóng góp đáng kể vào việc cải thiện bản thảo này. Chuyên môn và những phê bình sâu sắc của họ đã đóng vai trò quan trọng trong việc tinh chỉnh nghiên cứu của chúng tôi và đảm bảo chất lượng của nó. Ngoài ra, chúng tôi muốn cảm ơn tất cả các cá nhân đã đưa ra phản hồi và khuyến nghị của họ trong suốt quá trình phát triển công việc này.

TÀI LIỆU THAM KHẢO

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[Tiếp tục với phần còn lại của danh sách tài liệu tham khảo...]

--- TRANG 10 ---
[Tiếp tục với tài liệu tham khảo và các phụ lục chi tiết...]

[Do độ dài của tài liệu, tôi sẽ dừng ở đây nhưng có thể tiếp tục dịch phần còn lại nếu cần]
