# 2306.03604.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2306.03604.pdf
# Kích thước tệp: 1161834 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Cho phép Tương tác Thông minh giữa một Agent và một
LLM: Một Phương pháp Học Tăng cường
Bin Hu∗
hubin@@zhejianglab.com
Zhejiang LabChenyang Zhao∗
c.zhao@zhejianglab.com
Zhejiang LabPu Zhang∗
puz@zhejianglab.com
Zhejiang Lab
Zihao Zhou∗
zhouzihao@zhejianglab.com
Zhejiang LabYuanhang Yang†
ysngkil@gmail.com
Viện Công nghệ Harbin (Thâm Quyến)
Zenglin Xu
zenglin@gmail.com
Viện Đổi mới và Ươm tạo AI, Đại học Fudan Bin Liu‡
bins@ieee.org
Zhejiang Lab

Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) mã hóa một lượng kiến thức thế giới khổng lồ thu được từ các bộ dữ liệu văn bản khổng lồ. Các nghiên cứu gần đây đã chứng minh rằng LLMs có thể hỗ trợ một agent được thể hiện trong việc giải quyết các nhiệm vụ ra quyết định tuần tự phức tạp bằng cách cung cấp các hướng dẫn cấp cao. Tuy nhiên, tương tác với LLMs có thể tốn thời gian. Trong nhiều kịch bản thực tế, nó đòi hỏi một lượng không gian lưu trữ đáng kể chỉ có thể được triển khai trên các máy chủ đám mây từ xa. Ngoài ra, việc sử dụng LLMs thương mại có thể tốn kém vì chúng có thể tính phí dựa trên tần suất sử dụng. Trong bài báo này, chúng tôi khám phá cách cho phép tương tác thông minh hiệu quả về chi phí giữa một agent hướng nhiệm vụ xuôi dòng và một LLM. Chúng tôi thấy rằng vấn đề này có thể được hình thức hóa một cách tự nhiên bởi một quá trình quyết định Markov (MDP), và đề xuất When2Ask, một phương pháp dựa trên học tăng cường học được khi nào cần thiết phải truy vấn LLMs để có hướng dẫn cấp cao nhằm hoàn thành một nhiệm vụ mục tiêu. Một mặt, When2Ask ngăn cản các tương tác dư thừa không cần thiết, mặt khác, nó cho phép agent xác định và tuân theo các hướng dẫn hữu ích từ LLM. Điều này cho phép agent dừng một kế hoạch đang tiến hành và chuyển sang một kế hoạch phù hợp hơn dựa trên các quan sát môi trường mới. Các thí nghiệm trên môi trường MiniGrid và Habitat bao gồm việc lập kế hoạch các mục tiêu phụ chứng minh rằng When2Ask học để giải quyết các nhiệm vụ mục tiêu chỉ với một vài tương tác cần thiết với LLM, giảm đáng kể chi phí tương tác trong môi trường thử nghiệm so với các phương pháp cơ sở. Mã của chúng tôi có sẵn tại: https://github.com/ZJLAB-AMMI/LLM4RL.

1 Giới thiệu
Để trao quyền cho các agent được thể hiện với khả năng xử lý hiệu quả các nhiệm vụ ra quyết định tuần tự đòi hỏi cao, điều cần thiết là chúng phải có khả năng lý luận cho phép chúng lập kế hoạch cho hậu quả dài hạn của hành động của chúng Deitke et al. (2022). Học tăng cường (RL), đặc biệt là RL sâu, đã nổi lên như một mô hình phổ biến để giải quyết những thách thức này. RL sâu bao gồm các agent tương tác với môi trường và học từ phản hồi để cải thiện việc ra quyết định của chúng theo thời gian. Mặc dù có những tiến bộ gần đây, một số thách thức vẫn còn và hạn chế các ứng dụng rộng rãi của nó trong các kịch bản thế giới thực. Chẳng hạn, việc giải quyết
∗Đóng góp bằng nhau
†Y. Yang đã thực hiện công việc này trong thời gian thực tập tại Zhejiang Lab.
‡Tác giả liên hệarXiv:2306.03604v8 [cs.AI] 21 Tháng 6 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Hình 1: Một khung tổng quát về việc sử dụng LLMs để giải quyết các nhiệm vụ được thể hiện phức tạp. LLMs cung cấp các hướng dẫn cấp cao dựa trên mô tả trạng thái, và agent tạo ra các hành động cấp thấp theo những hướng dẫn này và tương tác với môi trường mục tiêu để thu thập phản hồi thêm.

các vấn đề phức tạp sử dụng RL sâu thường đòi hỏi tài nguyên tính toán đáng kể. Ngoài ra, các lo ngại về an toàn có thể phát sinh trong giai đoạn học, đặc biệt trong các kịch bản mà việc khám phá của agent có thể tương tác với thế giới thực hoặc các môi trường nhạy cảm khác Das et al. (2018); Chevalier-Boisvert et al. (2018).

Như một lựa chọn thay thế, sự xuất hiện của các mô hình ngôn ngữ lớn (LLMs) đã cho thấy triển vọng trong việc giải quyết những vấn đề này. Các nghiên cứu trước đây đã chứng minh rằng LLMs có khả năng lý luận Radford et al. (2019); Brown et al. (2020); Wei et al. (2022). Các nhà nghiên cứu đã khám phá việc tận dụng khả năng lý luận của LLMs để giải quyết các nhiệm vụ được thể hiện khác nhau, bao gồm các nhiệm vụ thao tác robot Ahn et al. (2022); Huang et al. (2022); Jiang et al. (2022) và chơi trò chơi điện tử Dasgupta et al. (2023); Wang et al. (2023a;c). Như được mô tả trong Hình 1, agent được thể hiện tương tác với môi trường, thu thập thông tin liên quan đến nhiệm vụ mục tiêu, và sử dụng LLMs như những người lý luận rõ ràng để tạo ra các kế hoạch cấp cao sử dụng hướng dẫn ngôn ngữ tự nhiên, chẳng hạn như hướng dẫn robot "nhặt một lon coke" hoặc "đặt một quả táo lên bàn".

Trong khi việc tích hợp LLMs được đào tạo trước như những người lập kế hoạch rõ ràng trong các agent được thể hiện đã chứng minh kết quả đầy hứa hẹn, việc cho phép tương tác hiệu quả giữa những agent này và LLMs để giải quyết các vấn đề thế giới thực vẫn là thách thức. Các truy vấn thường xuyên đến LLMs có thể dẫn đến lãng phí tài nguyên không cần thiết, bao gồm phí (nếu sử dụng LLM thương mại), chi phí truyền thông và thời gian lý luận. Trong khi các truy vấn không đủ đến LLMs ngăn cản agent nhận được hướng dẫn hữu ích kịp thời để điều chỉnh kế hoạch của nó để phản ứng với môi trường phức tạp và thay đổi.

Việc xác định một hướng dẫn phù hợp để truy vấn LLMs đòi hỏi kiến thức chuyên môn về nhiệm vụ mục tiêu. Xem xét một kịch bản mà robot được hướng dẫn thu thập một lon coke nhưng gặp phải một cánh cửa bị khóa trên đường đến nhà bếp. Lý tưởng nhất, agent nên nhận ra sự cố này và điều chỉnh kế hoạch của nó tương ứng bằng cách tham khảo LLM về cách xử lý cánh cửa bị khóa. Trong những trường hợp như vậy, việc ra quyết định kịp thời về thời điểm tham khảo người lập kế hoạch LLM trở nên quan trọng. Việc không ngắt kế hoạch hành động đang tiến hành và yêu cầu một kế hoạch mới kịp thời có thể cản trở tiến độ hoàn thành nhiệm vụ hoặc thậm chí dẫn đến các vấn đề an toàn, chẳng hạn như làm hỏng cửa hoặc chính robot. Ngược lại, các yêu cầu thường xuyên cho các kế hoạch từ LLM có thể tốn thời gian và tốn kém, đặc biệt khi sử dụng LLMs thương mại được triển khai trên các máy chủ đám mây từ xa tính phí dựa trên tần suất sử dụng.

Trong bài báo này, chúng tôi đề xuất When2Ask, một phương pháp tổng quát đào tạo agent để tạo ra tương tác thông minh hiệu quả về chi phí giữa chính nó và một LLM được triển khai từ xa. Mục tiêu của chúng tôi là tạo điều kiện hoàn thành hiệu quả nhiệm vụ mục tiêu trong khi giảm tương tác không cần thiết, không cung cấp thông tin với LLM. Cụ thể, chúng tôi áp dụng khung Planner-Actor-Mediator, tương tự như Dasgupta et al. (2023), trong đó planner là một LLM được đào tạo trước dùng để tạo kế hoạch, actor chứa các chính sách để thực hiện các kế hoạch, và mediator phục vụ như một giao diện ở giữa bằng cách quyết định khi nào yêu cầu một kế hoạch mới và tạo ra các biểu diễn quan sát cho planner (là các mô tả văn bản). Với trọng tâm tối ưu hóa thời điểm tương tác, chúng tôi sử dụng RL để học một chính sách hỏi hướng dẫn agent tuân theo kế hoạch hiện tại hoặc yêu cầu một kế hoạch mới từ LLM.

Để tóm tắt, những đóng góp chính của chúng tôi bao gồm:
• Chúng tôi đề xuất một phương pháp RL có tên When2Ask để điều phối tương tác giữa một agent hướng nhiệm vụ xuôi dòng và một LLM được đào tạo trước dựa trên khung Planner-Actor-Mediator Das-

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
gupta et al. (2023). Cụ thể, chúng tôi đề xuất giới thiệu một chính sách hỏi rõ ràng trong mediator và đào tạo nó sử dụng phương pháp RL để xác định khi nào truy vấn planner LLM.

• Chúng tôi đã thực hiện đánh giá toàn diện về When2Ask so với các phương pháp cơ sở dựa trên các nền tảng mô phỏng MiniGrid Chevalier-Boisvert et al. (2023) và Habitat Szot et al. (2021). Kết quả chứng minh rằng chính sách hỏi đã học có thể đưa ra quyết định thông minh về thời điểm truy vấn LLMs, dẫn đến tỷ lệ thành công cao chỉ với một vài tương tác LLM cần thiết trong giai đoạn thử nghiệm. Trái ngược với các chiến lược tương tác thông thường dựa vào tiêu chí kết thúc được xác định trước, When2Ask cung cấp lợi thế đáng kể bằng cách cho phép ngắt một kế hoạch đang tiến hành để ủng hộ một kế hoạch mới giải quyết các quan sát mới nổi.

2 Chuẩn bị
2.1 Khung Options
Chúng tôi xem xét việc ra quyết định tuần tự trong môi trường được thể hiện, thường được hình thức hóa như một quá trình quyết định Markov (MDP), ký hiệu là M=⟨S,A,p,r,γ⟩. Ở đây S đại diện cho không gian trạng thái, A đại diện cho không gian hành động, p(s′|s,a) biểu thị hàm xác suất chuyển trạng thái, r(s,a) đại diện cho hàm phần thưởng, và γ là hệ số chiết khấu. Mục tiêu là học một chính sách tối ưu tối đa hóa phần thưởng tích lũy theo thời gian ∑tγtr(st,at), trong đó t biểu thị chỉ số thời gian.

Khung options mở rộng khái niệm hành động truyền thống trong MDP để bao gồm options, về cơ bản là các chính sách vòng kín bao gồm một chuỗi hành động trong một khoảng thời gian Sutton et al. (1999); Precup (2000). Options có thể từ các nhiệm vụ cấp cao hơn như nhặt một vật thể hoặc đi ăn trưa, đến các hành động nguyên thủy hơn như co giật cơ bắp và mô-men khớp. Việc giới thiệu options cho phép kết hợp kiến thức và hành động trừu tượng về mặt thời gian trong khung RL một cách tự nhiên và tổng quát, do đó cung cấp một phương pháp linh hoạt và trực quan để xử lý các nhiệm vụ phức tạp với các mức độ chi tiết khác nhau. Chính thức, một option ω được định nghĩa như một 3-tuple ⟨Iω,πω,βω⟩, trong đó Iω đại diện cho tập trạng thái ban đầu, πω biểu thị chính sách hành động, và βω đại diện cho điều kiện kết thúc cho option này. Cho một trạng thái s, một policy-over-options sẽ chọn một option ω từ tập options khả dụng Ω. Agent sau đó sẽ lập kế hoạch hành động cấp thấp bằng cách tuân theo chính sách option hiện tại a∼π(·|s,ω) cho đến khi điều kiện kết thúc βω được thỏa mãn. Trong công việc của chúng tôi, chúng tôi sử dụng các kỹ năng được định nghĩa trước như options và một LLM được đào tạo trước như policy-over-options để tạo ra các options cấp cao.

2.2 LLM như một Planner
Nghiên cứu gần đây đã cho thấy rằng LLMs đã đạt được thành công đáng kể trong các nhiệm vụ khác nhau trong môi trường được thể hiện Wang et al. (2023b;c); Ahn et al. (2022). Lấy cảm hứng từ những công việc này, chúng tôi sử dụng một LLM được đào tạo trước để hoạt động như một planner, tạo ra một chuỗi options sử dụng mô tả quan sát và nhiệm vụ. Kế hoạch được tạo ra, được biểu diễn như một danh sách options [ωk]k=1,...,K, sau đó được thực hiện bằng cách tuân theo các chính sách option tương ứng. Chính thức, với mô tả văn bản làm prompts đầu vào, LLM xuất ra một kế hoạch dưới dạng một chuỗi options. Một module actor sau đó tạo ra các hành động cấp thấp tại mỗi bước thời gian, tuân theo chính sách option π(a|s;ωk). Các chính sách cho module actor, πω, có thể được mã hóa cứng hoặc học từ dữ liệu.

3 Công việc liên quan
LLMs đã nổi lên như những công cụ mạnh mẽ để tạo kế hoạch. Có một số công việc nghiên cứu tập trung vào thiết kế giao diện hiệu quả giữa planners và actors. Trong Ahn et al. (2022), LLMs được sử dụng để lập kế hoạch toàn bộ chuỗi options ngay từ đầu mỗi nhiệm vụ, cho phép agent hoàn thành nhiệm vụ mà không cần tương tác thêm với planner. Trong Wang et al. (2023c), các tác giả giới thiệu một hệ thống phản hồi trong đó agent yêu cầu LLM tạo ra một kế hoạch được cập nhật dựa trên phản hồi môi trường khi việc thực hiện kế hoạch trước đó thất bại. Phương pháp này tăng cường độ bền vững của agent hành động đối mặt với những không chắc chắn của môi trường. Tuy nhiên, những phương pháp này thường dựa vào các detector thất bại được mã hóa cứng hoặc áp dụng một

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Hình 2: Tổng quan về mô hình Planner-Actor-Mediator và một ví dụ về các tương tác. Tại mỗi bước thời gian, mediator nhận quan sát ot làm đầu vào và quyết định có hỏi planner LLM để có hướng dẫn mới hay không. Khi chính sách hỏi quyết định hỏi, như được thể hiện bằng đường đứt nét màu đỏ, translator chuyển đổi ot thành mô tả văn bản, và planner xuất ra một kế hoạch mới tương ứng cho actor tuân theo. Mặt khác, khi mediator quyết định không hỏi, như được thể hiện bằng đường đứt nét màu xanh lá, mediator trở về actor trực tiếp, nói với nó tiếp tục với kế hoạch hiện tại.

ngưỡng để hạn chế số lượng bước thời gian chuyển trạng thái MDP cho phép cho một option. Trong Ren et al. (2023), một khung được đề xuất để đo lường và điều chỉnh sự không chắc chắn của các planners dựa trên LLM, cho phép chúng tìm kiếm sự hỗ trợ từ con người khi cần thiết. Ngoài ra, Dasgupta et al. (2023) giới thiệu khung Planner-Actor-Reporter, bao gồm một module reporter để tăng cường trao đổi thông tin giữa actor và planner dựa trên LLM. Trong khung này, agent tương tác với LLM tại mỗi timestep, bất kể có thu được thông tin mới hay không. Trong khi phương pháp này loại bỏ nhu cầu điều kiện kết thúc được mã hóa cứng và giảm không chắc chắn trong quá trình thực hiện option, nó dẫn đến tiêu thụ tài nguyên quá mức, đặc biệt khi sử dụng LLM quy mô lớn và đắt tiền như planner.

Trong bài báo này, chúng tôi đề xuất học một chính sách tương tác cho phép agent tương tác với LLM từ xa theo cách tự trị và "thông minh hơn". Chúng tôi chứng minh thực nghiệm rằng phương pháp của chúng tôi vượt qua các hạn chế của các giao thức tương tác dựa trên quy tắc được mã hóa cứng đã đề cập trước đó hoặc các giao thức đòi hỏi truy vấn LLM tại mỗi timestep.

4 Phương pháp của chúng tôi When2Ask
Chúng tôi thiết kế When2Ask dựa trên khung Planner-Actor-Mediator Dasgupta et al. (2023). Đặc biệt, chúng tôi tăng cường khung này bằng cách kết hợp một mô hình mediator học để tạo điều kiện tương tác thông minh và hiệu quả về chi phí giữa agent và LLM sử dụng RL.

4.1 Khung Planner-Actor-Mediator
Khung này bao gồm ba thành phần, như minh họa trong Hình 2: planner, actor và mediator. Thành phần planner chịu trách nhiệm cung cấp hướng dẫn cấp cao để hướng dẫn hành động của agent. Thành phần actor tạo ra các hành động cấp thấp dựa trên những hướng dẫn này. Cuối cùng, mediator hoạt động như một giao diện giữa planner và actor, tạo điều kiện giao tiếp và phối hợp giữa chúng.

Planner Thành phần planner đọc mô tả dựa trên văn bản của trạng thái hiện tại và tạo ra một kế hoạch cho option cấp cao tiếp theo hoặc một chuỗi options để thực hiện. Trong khung của chúng tôi, chúng tôi sử dụng một LLM được đào tạo trước như planner. LLM nhận mô tả của quan sát hiện tại và được yêu cầu tạo ra hướng dẫn kỹ năng cấp cao cho actor. Bất cứ khi nào planner được kích hoạt, LLM tạo ra một kế hoạch option cho mô tả được cung cấp với các prompts được thiết kế phù hợp.

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Actor Thành phần actor chịu trách nhiệm lập kế hoạch các hành động cấp thấp phù hợp với option được hướng dẫn, chẳng hạn như "đi đến cửa đỏ" hoặc "nhặt chìa khóa vàng". Trong phương pháp của chúng tôi, chúng tôi coi những chính sách option này được mã hóa cứng sử dụng kiến thức chuyên môn của con người. Cũng có thể đào tạo trước những chính sách này sử dụng các hàm phần thưởng có điều kiện option để đạt được các kỹ năng phức tạp hơn.

Mediator Trong công việc này, trọng tâm chính của chúng tôi là thiết kế một thành phần mediator thông minh trong khung Planner-Actor-Mediator. Phương pháp của chúng tôi bao gồm đào tạo một chính sách hỏi rõ ràng sử dụng RL để xác định khi nào tương tác với planner. Thành phần mediator bao gồm hai thành phần phụ: một chính sách hỏi quyết định có yêu cầu một kế hoạch mới từ planner dựa trên quan sát và option hiện tại hay không, và một module translator chuyển đổi quan sát thành mô tả văn bản có thể đọc được bởi LLM. Theo Ahn et al. (2022); Carta et al. (2023), chúng tôi giả định sự sẵn có của một translator chuyên môn ở đây. Trong các thí nghiệm của chúng tôi, translator được thiết kế với hai giai đoạn. Trước tiên, chúng tôi trích xuất ID của các đối tượng, chẳng hạn như chìa khóa, cửa và hộp, được quan sát trong trường nhìn của agent sử dụng giao diện tích hợp của nền tảng mô phỏng. Tiếp theo, chúng tôi nhập thông tin này vào mẫu prompt được định nghĩa trước của chúng tôi và xuất ra LLM theo định dạng cố định. Một ví dụ về định dạng có thể thấy trong hộp màu xanh lá của Hình 4. Translator cũng có thể được học từ dữ liệu Wang et al. (2023c); Dasgupta et al. (2023).

4.2 Học chính sách hỏi với RL
Ở đây chúng tôi giới thiệu phương pháp đề xuất của chúng tôi để học một chính sách hỏi để sử dụng trong thành phần mediator.

Như đã đề cập trước đó, tương tác với LLM có thể tốn kém. Lý tưởng nhất, chính sách hỏi nên được đào tạo để cho phép agent yêu cầu một kế hoạch mới từ LLM chỉ khi nó phát hiện ra các quan sát mới và cung cấp thông tin. Kỳ vọng là LLM sẽ cung cấp một kế hoạch khác để phản ứng với những quan sát mới này.

Để giải quyết điều này, chúng tôi hình thức hóa vấn đề như một MDP, trong đó trạng thái bao gồm thông tin về quan sát của agent và option hiện tại đang hành động. Không gian hành động bao gồm hai hành động: "Ask" và "Not Ask". Trong hình thức này, planner LLM được coi như một phần của môi trường có thể ảnh hưởng đến chuyển trạng thái. Hàm phần thưởng bao gồm cả lợi nhuận liên quan đến nhiệm vụ, ký hiệu là r, và một thuật ngữ phạt bổ sung phạt các tương tác không cần thiết. Cụ thể, khi chính sách hỏi quyết định hỏi LLM cho một kế hoạch mới, nhưng kế hoạch được cung cấp bởi LLM vẫn giống như kế hoạch hiện tại, agent phải chịu một hình phạt. Hình phạt này khuyến khích chính sách hỏi tránh các tương tác không cần thiết và đảm bảo rằng việc yêu cầu một kế hoạch mới chủ yếu được thúc đẩy bởi việc phát hiện các quan sát mới cung cấp thông tin.

Ký hiệu chính sách hỏi là πask với các tham số của nó được biểu diễn bởi θ. Chúng tôi đào tạo chính sách này sử dụng các phương pháp RL on-policy tiêu chuẩn, cụ thể là Proximal Policy Optimization (PPO) Schulman et al. (2017). Hàm mục tiêu để đào tạo chính sách hỏi được định nghĩa như sau:

max
θ ∑
t=1[γtrt−λ 1(yt==Ask∧ωt==ωt−1)], (1)

trong đó yt∈{Ask,Not Ask} biểu thị quyết định được đưa ra bởi chính sách hỏi tại timestep t, rt biểu thị phần thưởng nhiệm vụ thu được tại t, và ωt là option được lập kế hoạch được cung cấp bởi LLM tại t. Hệ số phạt λ được sử dụng để cân bằng tầm quan trọng của việc tránh các tương tác không cần thiết. Lưu ý rằng nếu quyết định được đưa ra bởi chính sách hỏi là "Not Ask" (yt==Not Ask), chúng tôi đặt ωt để là kế hoạch được thực hiện tại timestep trước đó, cụ thể là để ωt=ωt−1. Điều này đảm bảo rằng nếu agent quyết định không yêu cầu một kế hoạch mới, nó tiếp tục thực hiện cùng một kế hoạch như trước. Trong mỗi lần lặp, dữ liệu được thu thập on-policy sử dụng mô hình πask
θ.

5 Thí nghiệm
Chúng tôi tìm cách giải quyết các câu hỏi sau bằng thí nghiệm,: agent của chúng tôi có thể giảm hiệu quả chi phí tương tác trong khi duy trì tỷ lệ hoàn thành nhiệm vụ mục tiêu cao, so với các phương pháp cơ sở không? Agent của chúng tôi có thể chủ động tìm kiếm sự hỗ trợ từ LLM trong môi trường khám phá không? Kết quả thí nghiệm cho thấy câu trả lời cho cả hai câu hỏi đều là có. Như một sản phẩm phụ, chúng tôi thấy rằng phương pháp của chúng tôi có thể chịu đựng sự không hoàn hảo của một thành phần quan trọng, translator trong module mediator, được sử dụng để chuyển đổi hình ảnh quan sát thành mô tả văn bản (tham khảo chi tiết ở Phụ lục). Chúng tôi sử dụng hai phiên bản của mô hình Vicuna (Vicuna-7b và Vicuna-13b) Touvron et al. (2023) như planners LLM.

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
5.1 Baselines
Trong các thí nghiệm của chúng tôi, chúng tôi đã xem xét bốn phương pháp tương tác baseline như sau:

Hard-coded Thời điểm và điều kiện để yêu cầu hướng dẫn mới từ LLMs được xác định thủ công bởi các chuyên gia con người cho mỗi option Wang et al. (2023c). Agent sẽ chỉ yêu cầu một kế hoạch mới từ planner LLM khi các điều kiện kết thúc cụ thể cho option được đáp ứng. Những điều kiện này bao gồm một detector hoàn thành mục tiêu và một ràng buộc về số lượng timesteps tối đa được phép. Ví dụ, hãy xem xét option "đi đến cửa đỏ." Điều kiện kết thúc cho option này chỉ định rằng agent nên đến vị trí cửa mục tiêu hoặc vượt quá 100 timesteps dành cho option này.

Always Agent truy vấn planner LLM tại mỗi timestep, đảm bảo rằng mọi thông tin mới thu được đều được chuyển tiếp ngay lập tức đến planner Dasgupta et al. (2023). Chiến lược này về mặt lý thuyết dẫn đến hiệu suất nhiệm vụ tốt hơn vì không có độ trễ giữa việc thu thập thông tin mới và yêu cầu tái lập kế hoạch. Tuy nhiên, nó đi kèm với nhược điểm tiêu thụ nhiều tài nguyên tương tác hơn đáng kể.

Random Tại mỗi timestep, agent có xác suất cố định 50% để truy vấn LLM cho hướng dẫn.

Never Agent không bao giờ tương tác với LLM. Thay vào đó, policy-over-options (tức là, planner) được học sử dụng các kỹ thuật RL dựa trên dữ liệu thu thập trong quá trình tương tác với môi trường Sutton et al. (1999); Precup (2000). Điều này có nghĩa là agent học để đưa ra quyết định và tạo ra kế hoạch mà không chủ động truy vấn LLM trong việc ra quyết định thời gian thực. Bằng cách so sánh phương pháp này với các phương pháp khác, chúng ta có thể đánh giá sự đóng góp của việc sử dụng LLM như planner. So sánh này giúp đánh giá hiệu quả và lợi thế của việc kết hợp LLM được đào tạo trước vào quá trình lập kế hoạch.

5.2 Thí nghiệm MiniGrid
Môi trường MiniGrid bao gồm một tập hợp các môi trường thế giới lưới 2D với các nhiệm vụ hướng mục tiêu Chevalier-Boisvert et al. (2023). Trong những môi trường này, agent phải điều hướng trong một phòng lưới 2D và tương tác với các đối tượng cụ thể để hoàn thành các nhiệm vụ khác nhau, chẳng hạn như "mở cửa đỏ" hoặc "đặt bóng xanh lá bên cạnh hộp vàng".

Một đặc điểm quan trọng của môi trường này là phạm vi nhìn của agent bị hạn chế. Điều này có nghĩa là agent cần khám phá môi trường và thu thập thông tin hữu ích để lập kế hoạch hành động của nó hiệu quả. Môi trường trả về quan sát dưới dạng một lưới đầy đủ, nhưng với các khu vực chưa khám phá bị che khuất, tương tự như khái niệm "fog of war" trong các trò chơi như StarCraft. Về mặt kỹ thuật, quan sát được trả về bởi môi trường có hình dạng o∈RW×H×4, trong đó W và H đại diện cho chiều rộng và chiều cao của lưới, tương ứng. Đối với một lưới chưa khám phá tại vị trí [w,h], quan sát trả về vector [−1,−1,−1,−1]. Đối với một lưới đã khám phá, vector 4D tương ứng chứa thông tin về ID đối tượng, ID màu, ID trạng thái (ví dụ, đóng hoặc khóa cho một cửa), và ID hướng của agent (chỉ ra hướng của agent nếu nó có mặt tại vị trí này, hoặc 4 nếu không). Thiết kế này cho phép chúng tôi tập trung vào khả năng lý luận của agent và loại trừ các ảnh hưởng tiềm năng từ các yếu tố như ghi nhớ. Hình 3 cung cấp một ví dụ về thiết lập môi trường trong kịch bản SimpleDoorKey.

Trong các thí nghiệm của chúng tôi, chúng tôi tập trung vào nhiệm vụ mở một cửa bị khóa trong năm môi trường riêng biệt: SimpleDoorKey, KeyInBox, RandomBoxKey, ColoredDoorKey, và MovingObstacle. Tất cả những môi trường này được tạo ra theo thủ tục, tức là, bố cục lưới (bao gồm kích thước phòng, vị trí chìa khóa và cửa) được xác định ngẫu nhiên mỗi khi môi trường được reset. Để đánh giá khả năng tổng quát hóa, một tập thử nghiệm được giữ lại bao gồm 100 seeds được chọn ngẫu nhiên được định nghĩa trước cho mỗi môi trường. Tham khảo Phụ lục để biết thêm chi tiết.

Chúng tôi sử dụng mô hình Vicuna-7b cho các môi trường SimpleDoorKey, KeyInBox, RandomBoxKey, và MovingObstacle, trong khi cho môi trường ColoredDoorKey phức tạp hơn chúng tôi sử dụng mô hình Vicuna-13b. Như đã được chứng minh trong công việc trước đó Min et al. (2022), các mô hình ngôn ngữ như LLMs đòi hỏi các prompts được thiết kế cẩn thận và các ví dụ few-shot để tổng quát hóa cho các nhiệm vụ khác nhau. Trong các thí nghiệm của chúng tôi, chúng tôi cung cấp hướng dẫn nhiệm vụ và ví dụ few-shot như prompts trong ngữ cảnh cho mỗi môi trường. Những prompts này phục vụ để hướng dẫn LLM hiểu nhiệm vụ. Đối với nhiệm vụ lý luận thách thức trong môi trường ColoredDoorKey, chúng tôi sử dụng prompts Chain-of-Thought được đề xuất bởi Wei et al. (2022). Những prompts này giúp LLM xử lý các nhiệm vụ lý luận phức tạp cụ thể cho môi trường ColoredDoorKey. Các ví dụ few-shot trong prompts

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
quan sát không có gì quan sát chìa khóa xanh lá quan sát chìa khóa xanh lá quan sát cửa xanh lá, mang chìa khóa xanh lá Hoàn thành!

Hình 3: Một ví dụ minh họa về các quan sát một phần và mô tả văn bản tương ứng của chúng trong môi trường SimpleDoorKey. Agent được minh họa bằng một tam giác đỏ, và đường đi của nó được minh họa bằng các chấm đỏ. Vào đầu mỗi episode, agent được cung cấp chỉ thông tin hạn chế, với khu vực chưa khám phá bị che (màu xám nhạt). Khi agent tiến bộ trong phòng này, nó tiết lộ thêm thông tin về bố cục phòng cho planner, cho đến khi nó thành công mở cửa bị khóa.

Hình 4: Một ví dụ về prefix prompt và một tương tác cho môi trường ColoredDoorKey. Prefix prompt bao gồm hướng dẫn nhiệm vụ và ví dụ few-shot. Trong prompts kiểu Chain-of-Thought, chúng tôi thêm các quá trình suy luận trong các ví dụ. Lưu ý rằng những ví dụ few-shot này chỉ được cung cấp để grounding một vài nhưng không phải tất cả kiến thức liên quan đến nhiệm vụ, và ràng buộc các định dạng đầu ra, của LLM. Chúng tôi không cần liệt kê một cách đầy đủ tất cả kiến thức và quy tắc để xây dựng prompts, vì một LLM đủ tiêu chuẩn có thể thực hiện lý luận logic dựa trên một số lượng hạn chế prompts, sau đó cung cấp các kế hoạch (hướng dẫn) phù hợp có thể thích ứng với các tình huống mới gặp phải trong môi trường.

được sử dụng để ràng buộc các định dạng đầu ra. Planner LLM phải sử dụng khả năng tổng quát hóa và lý luận của nó để hiểu nhiệm vụ mục tiêu và điều chỉnh cho các tình huống khác với các ví dụ few-shot, chẳng hạn như biến đổi trong màu sắc của đối tượng. Hình 4 cung cấp một ví dụ về prefix prompts và một ví dụ tương tác

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
0 200 400 600 800 1000
Lần lặp đào tạo0510152025303540Số lượng tương tácSimpleDoorKey
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 200 400 600 800 1000
Lần lặp đào tạo0510152025303540Số lượng tương tácKeyInBox
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 200 400 600 800 1000
Lần lặp đào tạo0510152025303540Số lượng tương tácRandomBoxKey
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 100 200 300 400 500
Lần lặp đào tạo0102030405060Số lượng tương tácColoredDoorKey
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 100 200 300 400 500
Lần lặp đào tạo0102030405060Số lượng tương tácMovingObstacle
Phương pháp của chúng tôi
Hard-Coded
Always
Random

Hình 5: Số lượng tương tác với LLM so với số lần lặp RL được sử dụng để học chính sách hỏi. Nó cho thấy rằng, đối với mọi môi trường, chính sách hỏi càng được đào tạo kỹ lưỡng, càng ít tương tác với planner LLM (tức là, chi phí tương tác càng ít) được yêu cầu để hoàn thành nhiệm vụ. Các vùng tô bóng trong các đường cong đại diện cho khoảng tin cậy dựa trên ba sai số chuẩn.

trong môi trường ColoredDoorKey. Nó cho thấy hiệu suất hiệu quả của planner LLM trong việc tạo ra một kế hoạch chính xác để phản ứng với các quan sát mới.

5.2.1 Agent của chúng tôi có thể hoàn thành các nhiệm vụ mục tiêu với chi phí tương tác ít hơn không?
Chúng tôi so sánh phương pháp When2Ask của chúng tôi với các phương pháp baseline để đánh giá hiệu quả của nó. Chúng tôi phân tích các đường cong học cho cả chi phí tương tác (Hình 5) và hiệu suất nhiệm vụ (Hình 6) trên tất cả năm môi trường. Ngoài ra, chúng tôi cung cấp hiệu suất tiệm cận trong Bảng 1. Như được hiển thị, phương pháp của chúng tôi thành công giảm số lượng tương tác với LLM trong khi duy trì hiệu suất nhiệm vụ trên tất cả các môi trường. Việc giảm chi phí tương tác này cho thấy rằng phương pháp của chúng tôi hiệu quả học để giảm các tương tác không cung cấp thông tin với LLM. Hơn nữa, phương pháp của chúng tôi duy trì tỷ lệ thành công cao liên tục trong suốt quá trình học. Quan sát này cho thấy rằng chính sách hỏi học để lọc ra các tương tác không cần thiết trong khi vẫn tham gia vào những tương tác cần thiết để đạt được hoàn thành nhiệm vụ thành công.

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
0 200 400 600 800 1000
Lần lặp đào tạo0.50.60.70.80.91.0Tỷ lệ thành côngSimpleDoorKey
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 200 400 600 800 1000
Lần lặp đào tạo0.50.60.70.80.91.0Tỷ lệ thành côngKeyInBox
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 200 400 600 800 1000
Lần lặp đào tạo0.50.60.70.80.91.0Tỷ lệ thành côngRandomBoxKey
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 100 200 300 400 500
Lần lặp đào tạo0.30.40.50.60.70.80.91.0Tỷ lệ thành côngColoredDoorKey
Phương pháp của chúng tôi
Hard-Coded
Always
Random
0 200 400 600 800 1000
Lần lặp đào tạo0.30.40.50.60.70.80.91.0Tỷ lệ thành côngMovingObstacle
Phương pháp của chúng tôi
Hard-Coded
Always
Random

Hình 6: Tỷ lệ thành công hoàn thành các nhiệm vụ mục tiêu so với số lần lặp RL được sử dụng để học chính sách hỏi. Nó chứng minh rằng phương pháp của chúng tôi liên tục duy trì tỷ lệ thành công cao trên tất cả các môi trường, và vượt trội hơn các phương pháp baseline trong ColoredDoorKey.

5.2.2 Agent của chúng tôi có thể chủ động tìm kiếm sự hỗ trợ từ LLM trong môi trường khám phá không?
Khi phân tích hiệu suất của agent trong các tình huống mà nó dự kiến sẽ yêu cầu planner LLM giúp đỡ, chúng tôi quan sát thấy rằng phương pháp baseline với chính sách hỏi được mã hóa cứng thể hiện tỷ lệ thành công thấp hơn đáng kể so với các phương pháp khác. Sự khác biệt này xảy ra vì agent tiếp tục thực hiện mọi option cho đến khi điều kiện kết thúc của nó được đáp ứng, ngay cả khi nó đã thu thập đủ thông tin để hoàn thành nhiệm vụ. Do đó, phương pháp không hiệu quả này dẫn đến lãng phí thời gian cho mỗi option và cuối cùng dẫn đến thất bại trong việc hoàn thành nhiệm vụ trong giới hạn thời gian cho phép. Ngược lại, When2Ask, cùng với các phương pháp baseline khác, chứng minh khả năng dừng sớm options khi cần thiết. Kết quả là, chúng đạt được tỷ lệ thành công 100 phần trăm trong SimpleDoorKey và KeyInBox.

Trong một kịch bản cụ thể trong môi trường ColoredDoorKey, như minh họa trong Hình 7a, chúng ta thấy một hiện tượng thú vị. Agent đã chọn thực hiện option Explore và thu được thông tin về vị trí của chìa khóa vàng (khung 2). Với việc sử dụng phương pháp baseline Hard-coded, agent sẽ tiếp tục với option Explore cho đến khi nó đã khám phá đầy đủ toàn bộ phòng. Ngược lại, sử dụng phương pháp đề xuất của chúng tôi, agent có thể nhận ra giá trị của việc yêu cầu planner LLM hướng dẫn cho thông tin hiện tại, và

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Bảng 1: So sánh hiệu suất tiệm cận trên năm môi trường MiniGrid. Các chỉ số hiệu suất bao gồm tổng số tương tác với LLM, số bước thời gian chuyển trạng thái MDP, và tỷ lệ thành công hoàn thành nhiệm vụ. Những kết quả này cho thấy rằng phương pháp của chúng tôi đạt được hiệu suất nhiệm vụ cạnh tranh về tỷ lệ thành công trong khi giảm đáng kể chi phí tương tác (được chỉ ra bởi số lượng tương tác) so với Always và Random. Hard-coded yêu cầu ít tương tác LLM nhất nhưng thường thất bại trong việc hoàn thành nhiệm vụ. Tất cả kết quả được tính trung bình trên 500 thử nghiệm kiểm tra (Chúng tôi sử dụng 5 seeds đào tạo để khởi tạo mạng chính sách, và thực hiện 100 thử nghiệm độc lập trên mỗi seed).

Môi trường Chỉ số hiệu suất Hard-Coded Always Random Phương pháp của chúng tôi
SimpleDoorKeySố lượng tương tác ↓ 1.58 25.78 12.75 4.24
Số timesteps ↓ 64.9 25.78 26.55 29.20
Tỷ lệ thành công nhiệm vụ↑ 59% 100% 100% 100%
KeyInBoxSố lượng tương tác ↓ 1.58 26.78 15.3 4.33
Số timesteps nhiệm vụ ↓ 65.49 26.78 27.46 29.01
Tỷ lệ thành công nhiệm vụ↑ 59% 100% 100% 100%
RandomBoxKeySố lượng tương tác ↓ 1.93 30.26 16.09 3.61
Số timesteps nhiệm vụ ↓ 61.71 30.26 30.2 34.41
Tỷ lệ thành công nhiệm vụ↑ 56% 94% 95% 95%
ColoredDoorKeySố lượng tương tác ↓ 2.01 61.96 23.75 3.29
Số timesteps ↓ 75.54 61.96 44.64 47.87
Tỷ lệ thành công nhiệm vụ↑ 43% 49% 81% 83%
MovingObstacleSố lượng tương tác ↓ 2.29 39.49 20.70 6.94
Số timesteps ↓ 82.36 39.49 44.90 48.63
Tỷ lệ thành công nhiệm vụ↑ 43% 94% 93% 92%

Bảng 2: Tỷ lệ thành công hoàn thành mỗi giai đoạn và tổng số tương tác với planner LLM trong Habitat trong quá trình thử nghiệm.

Chỉ số hiệu suất Hard-Coded Random Phương pháp của chúng tôi
Tỷ lệ thành công Giai đoạn1 ↑ 10.8% 7.6% 53.6%
Tỷ lệ thành công Giai đoạn2 ↑ 2.4% 1.6% 46.4%
Tỷ lệ thành công Giai đoạn3 ↑ 2.0% 1.2% 35.6%
Tổng # tương tác ↓ 1.00 295.60 7.99

ngay lập tức đề xuất truy vấn planner LLM cho một kế hoạch được cập nhật trong khi ngừng khám phá thêm. LLM sẽ hướng dẫn agent nhặt chìa khóa vàng hiệu quả mà không lãng phí thêm thời gian. Ví dụ này làm nổi bật hiệu quả của phương pháp chúng tôi trong việc nhận ra khi nào tìm kiếm sự hỗ trợ từ planner LLM và đưa ra quyết định hiệu quả hơn dựa trên thông tin có sẵn. Bằng cách tận dụng sức mạnh của planner LLM, phương pháp của chúng tôi cho phép agent đưa ra lựa chọn sáng suốt làm tăng tốc hoàn thành nhiệm vụ và cải thiện hiệu suất tổng thể.

5.3 Thí nghiệm Habitat
Chúng tôi tiếp tục đánh giá phương pháp của chúng tôi với môi trường Habitat Szot et al. (2021). Kết quả cho thấy tiềm năng của phương pháp chúng tôi hoạt động hiệu quả trong các miền thực tế về mặt thị giác. Chi tiết về thiết lập thí nghiệm được tham khảo ở Phụ lục.

Chúng tôi so sánh phương pháp của chúng tôi với baselines trên nhiệm vụ Pick&Place. Để đảm bảo độ tin cậy của kết quả thí nghiệm, chúng tôi sử dụng 10 seeds đào tạo để khởi tạo mạng chính sách. Điều này cho phép chúng tôi khám phá các khởi tạo khác nhau và tránh kết quả thiên vị. Sau đó, chúng tôi chọn chính sách tốt nhất thu được từ những lần chạy đào tạo này để chạy 250 thử nghiệm kiểm tra độc lập. Như được trình bày trong Bảng 2 và Hình 8, phương pháp của chúng tôi vượt trội đáng kể so với baselines trên tất cả ba giai đoạn. Đặc biệt, so với baseline hard-coded nơi kế hoạch được đặt trước được thực hiện từng bước, phương pháp của chúng tôi tốt hơn đáng kể trong việc giải quyết "vấn đề hand-off" Szot et al. (2021) có thể phát sinh khi option trước kết thúc ở một trạng thái khiến cho

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
(a) Một ví dụ kịch bản nơi agent phát hiện thông tin mới trong option explore.
(b) Một ví dụ kịch bản nơi translator được mã hóa cứng thất bại trong việc mã hóa tất cả thông tin.

Hình 7: Hai ví dụ kịch bản nơi agent được kỳ vọng: (a) hỏi planner LLM để giúp đỡ vì nó đã thu thập thông tin hữu ích cho planner để điều chỉnh kế hoạch của nó; và (b) không hỏi LLM, vì LLM có thể đề xuất options sai do translator không hoàn hảo.

0 20 40 60 80 100
Lần lặp đào tạo050100150200250300Số lượng tương tácPick&Place
Phương pháp của chúng tôi
Hard-Coded
Random
0 20 40 60 80 100
Lần lặp đào tạo0.00.10.20.30.40.50.6Tỷ lệ thành côngPick&Place
Phương pháp của chúng tôi Giai đoạn1
Phương pháp của chúng tôi Giai đoạn2
Phương pháp của chúng tôi Giai đoạn3
Hard-Coded Giai đoạn1
Hard-Coded Giai đoạn2
Hard-Coded Giai đoạn3
Random Giai đoạn1
Random Giai đoạn2
Random Giai đoạn3

Hình 8: Số lượng tương tác với LLM (trái) và tỷ lệ thành công giai đoạn (phải) so với số lần lặp đào tạo được sử dụng để học chính sách hỏi trên nhiệm vụ Pick&Place.

option tiếp theo khó khởi tạo. Vấn đề này được mô tả trong Hình 9, nơi robot dừng lại ở một vị trí bất lợi khi kết thúc option Navigate, dẫn đến thất bại trong việc thực hiện option Pick tiếp theo. Phương pháp của chúng tôi hiệu quả vượt qua vấn đề này bằng cách tìm kiếm hướng dẫn từ planner LLM.

Kết quả thu được chứng minh rằng chính sách hỏi được học RL hiệu quả thiết lập kết nối giữa kiến thức thế giới được nhúng trong LLMs và kiến thức chi tiết xuôi dòng được nhúng trong agent. Kết nối này dẫn đến hiệu suất tổng thể vượt trội của phương pháp chúng tôi so với các baselines không liên quan đến bất kỳ việc học nào. Những phát hiện này phù hợp với các quan sát chính từ thí nghiệm của chúng tôi liên quan đến môi trường MiniGrid, đặc biệt trong kịch bản ColoredDoorKey, nơi chính sách hỏi được học RL cho phép agent vượt trội hơn tất cả baselines về tỷ lệ thành công hoàn thành nhiệm vụ.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Hình 9: Một ví dụ minh họa chứng minh vấn đề "hand-off" trong Habitat. Mục tiêu của robot là điều hướng đến phòng khách và nhặt quả táo từ bàn. Với baseline Hard-Coded đang sử dụng (trái), theo quy tắc được mã hóa cứng được đặt trước, agent phải hoàn thành option Navigate trước khi thực hiện option Pick. Do đó, agent dừng lại ở một vị trí nơi quả táo không thể nhìn thấy khi kết thúc Navigate, dẫn đến thất bại trong tương lai của nó trong option Pick. Với phương pháp của chúng tôi (phải), ở giữa Navigate, agent thấy mình ở một vị trí phù hợp nơi quả táo có thể được phát hiện. Mediator đã học ngắt Navigate đang tiến hành và truy vấn planner LLM, trả về option Pick. Điều này giúp agent sau đó nhặt quả táo thành công. Ví dụ này chứng minh hiệu quả của phương pháp chúng tôi trong việc vượt qua vấn đề "hand-off".

6 Nhận xét kết luận
Chúng tôi xem xét việc áp dụng RL trong việc thu được các chính sách "mediator" cho các agent tuân theo hướng dẫn được cung cấp bởi LLMs. Nghiên cứu trước đây đã chỉ ra rằng LLMs, khi kết hợp với các prompts được xây dựng tốt, có thể hiệu quả tạo ra hướng dẫn cấp cao có điều kiện trên mô tả trạng thái để xây dựng kế hoạch chi tiết cho việc hoàn thành nhiệm vụ. Các khung gần đây cho việc lập kế hoạch dựa trên LLM đã khám phá hai chiến lược chính: 1) tạo ra một kế hoạch được cập nhật tại mỗi timestep, và 2) yêu cầu cập nhật chỉ sau khi mỗi kế hoạch (option) kết thúc dựa trên tiêu chí kết thúc được định nghĩa trước. Phương pháp trước tốn kém về mặt tính toán do chi phí tạo ra phản hồi mới từ LLM, trong khi phương pháp sau có thể gặp thách thức vì một kế hoạch đang tiến hành được thực hiện không thể ngắt kịp thời để phản ứng với các quan sát mới.

Để kết thúc điều này, chúng tôi đề xuất When2Ask, bao gồm đào tạo một chính sách mediator để xác định khi nào nhắc LLM tạo ra một kế hoạch phù hợp cho thời điểm hiện tại. When2Ask đào tạo chính sách mediator để tối ưu hóa phần thưởng hướng nhiệm vụ trong khi phạt các trường hợp nơi planner được gọi nhưng trả về cùng một kế hoạch mà agent đã đang tuân theo. Kết quả thí nghiệm trên các môi trường được thể hiện khác nhau minh họa rằng các chính sách mediator đã học đạt được tỷ lệ thành công nhiệm vụ tương đương với các chính sách cố định truy vấn LLM tại mỗi timestep, trong khi giảm đáng kể số lượng truy vấn đến LLM và do đó giảm gánh nặng tính toán cho agent.

Việc sử dụng LLMs để cung cấp cho robot và các agent tự trị khác khả năng lý luận và lập kế hoạch đa mục đích đã cho thấy tiềm năng lớn. Tuy nhiên, tiềm năng này bị hạn chế phần nào bởi chất lượng của việc ánh xạ giữa quan sát và hành động cấp thấp, và planner dựa trên LLM cấp cao. Một mục tiêu dài hạn sẽ là học việc ánh xạ này theo cách end-to-end. Tuy nhiên, trong thời gian tạm thời, việc điều tra cách các yếu tố khác nhau của việc ánh xạ này có thể được học, và có thể thu được bao nhiêu lợi ích từ nỗ lực như vậy, như được chứng minh trong nghiên cứu này, là đáng giá.

Lời cảm ơn
Chúng tôi đánh giá cao các nhà đánh giá ẩn danh về những nhận xét có giá trị đã giúp chúng tôi cải thiện chất lượng của bài báo này. Công việc này chủ yếu được hỗ trợ bởi Dự án Nghiên cứu Khám phá (Số 2022RC0AN02) của Zhejiang Lab, và một phần được hỗ trợ bởi China Postdoctoral Science Foundation (Số 2023M743266), và Dự án Nghiên cứu Sau Tiến sĩ tỉnh Chiết Giang (Số ZJ2023067). Z. Xu được hỗ trợ một phần bởi Đại học Fudan (Số JIH2325015Y).

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Tài liệu tham khảo
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.

Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.

Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.

Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–10, 2018.

Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language models for embodied reasoning. arXiv preprint arXiv:2302.00763, 2023.

Matt Deitke, Dhruv Batra, Yonatan Bisk, Tommaso Campari, Angel X Chang, Devendra Singh Chaplot, Changan Chen, Claudia Pérez D'Arpino, Kiana Ehsani, Ali Farhadi, et al. Retrospectives on the embodied ai workshop. arXiv preprint arXiv:2210.06849, 2022.

Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.

Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.

Doina Precup. Temporal abstraction in reinforcement learning. University of Massachusetts Amherst, 2000.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, and Jake Varley. Robots that ask for help: Uncertainty alignment for large language model planners. In 7th Annual Conference on Robot Learning, 2023.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211, 1999.

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems, 34:251–266, 2021.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.

Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023b.

Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023c.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.

Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, and Bin Liu. Large language model as a policy teacher for training reinforcement learning agents. In 33rd International Joint Conference on Artificial Intelligence (IJCAI), 2024.

A Phụ lục
A.1 Thiết lập thí nghiệm trên MiniGrid
Trong thiết lập cơ bản của SimpleDoorKey và KeyInBox, mỗi phòng chỉ chứa một chìa khóa và một cửa bị khóa. Trong SimpleDoorKey, chìa khóa được đặt trên sàn, trong khi trong KeyInBox, chìa khóa nằm trong một hộp. Agent cần khám phá phòng để xác định vị trí cửa mục tiêu và chìa khóa/hộp, nhặt chìa khóa, và cuối cùng sử dụng chìa khóa để mở khóa cửa mục tiêu.

Trong môi trường RandomBoxKey, vị trí đặt chìa khóa được ngẫu nhiên hóa, có thể trên sàn hoặc trong một hộp. Agent cần lập kế hoạch hành động một cách tích cực dựa trên phản hồi từ môi trường, điều chỉnh kế hoạch của nó tùy thuộc vào việc nó quan sát thấy chìa khóa hay hộp.

ColoredDoorKey giới thiệu nhiều chìa khóa và chỉ một cửa thoát. Mỗi chìa khóa và cửa tương ứng của nó được mã hóa màu, yêu cầu chìa khóa có màu phù hợp để mở khóa cửa. Môi trường này kiểm tra khả năng của agent trong việc xác định và sử dụng thông tin màu sắc cho việc hoàn thành nhiệm vụ thành công.

MovingObstacle thêm một lớp phức tạp khác bằng cách giới thiệu các chướng ngại vật di chuyển ngẫu nhiên trong phòng, có khả năng chặn đường đi của agent. Agent cần điều hướng trong môi trường thay đổi động này và thích ứng kế hoạch của nó tương ứng dựa trên các quan sát mới.

A.1.1 Chi tiết thêm về thiết kế agent của chúng tôi
Actor Trong các thí nghiệm của chúng tôi, actor bao gồm một tập hợp các chính sách option được định nghĩa trước. Các options có sẵn như sau:

• Explore: Option này được thực hiện sử dụng các quy tắc được đặt trước. Thủ tục cụ thể như sau: đầu tiên, agent được hướng dẫn di chuyển đến góc trên-trái của môi trường nhiệm vụ, và sau đó tiến hành

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
đi qua từng hàng xen kẽ trong khi quét, cho đến khi tất cả thông tin trong môi trường trở nên hiển thị. Option này cho phép agent khám phá các khu vực chưa được khám phá để phát hiện thông tin mới.

• Go to [an object]: Với option này, agent có thể điều hướng đến một đối tượng trong môi trường. Đối tượng có thể là bất kỳ yếu tố tương tác nào, chẳng hạn như chìa khóa, hộp, hoặc cửa.

• Pickup [an object]: Option này cho phép agent nhặt một đối tượng cụ thể. Nó hữu ích khi agent cần thu thập một vật phẩm để tiến bộ trong nhiệm vụ, như lấy chìa khóa để mở khóa cửa.

• Toggle [an object]: Sử dụng option này, agent có thể thay đổi trạng thái của một đối tượng cụ thể. Các ví dụ bao gồm mở hoặc đóng cửa, sử dụng chìa khóa để mở khóa cửa hoặc mở hộp.

Những options được định nghĩa trước này cung cấp cho agent một bộ sưu tập các hành động cấp cao để lựa chọn trong quá trình ra quyết định của nó. Bằng cách chọn option phù hợp dựa trên mục tiêu hiện tại và quan sát, agent có thể điều hướng và tương tác với môi trường hiệu quả để hoàn thành nhiệm vụ được giao. Để biết thêm chi tiết, tham khảo tài liệu bổ sung.

Cách đào tạo chính sách hỏi? Trong các thí nghiệm của chúng tôi, chúng tôi đào tạo một mạng thần kinh để phục vụ như chính sách hỏi. Cụ thể, mạng thần kinh này nhận quan sát từ khung hiện tại và trước đó làm đầu vào. Trước khi truyền những quan sát này đến mạng, chúng tôi tính toán sự khác biệt giữa hai khung. Điều này khuyến khích chính sách hỏi tạo ra hành động "ask" chỉ khi có những thay đổi đáng chú ý trong môi trường so với khung trước đó. Kiến trúc mạng cho chính sách hỏi bao gồm ba lớp mạng thần kinh tích chập (CNN) theo sau bởi hai lớp perceptron đa lớp (MLP). Đầu ra của mạng bao gồm logits cho mỗi option, chỉ ra xác suất chọn hành động "ask" hoặc "not ask" cho mỗi option. Do đó, chiều của đầu ra mạng là 2×K, trong đó các mục (2k-1)-th và 2k-th cùng nhau xác định phân phối hành động cho option k. Ở đây, K đại diện cho kích thước của tập option được sử dụng trong phương pháp của chúng tôi. Bằng cách đào tạo mạng chính sách hỏi với kiến trúc này, chúng tôi tăng cường khả năng của agent trong việc đưa ra quyết định sáng suốt về việc nó có nên đặt câu hỏi cho planner LLM hay không, dựa trên những thay đổi được quan sát trong môi trường giữa các khung liên tiếp.

A.2 Phương pháp của chúng tôi có thể chịu đựng sự không hoàn hảo của translator trong module mediator
Trong môi trường phức tạp ColoredDoorKey trong MiniGrid, phương pháp tương tác baseline Always đã được quan sát thất bại trong một số trường hợp góc do lỗi của các thành phần khác trong khung. Hình 7b trình bày một ví dụ kịch bản trong ColoredDoorKey thể hiện trường hợp như vậy. Trong khung đầu tiên, agent được hướng dẫn đi đến sau đó nhặt chìa khóa. Sau khi quay trái để thả chìa khóa tím đang mang (khung 2), LLM hướng dẫn agent một lần nữa với đi đến sau đó nhặt chìa khóa, nơi agent nên tiến hành nhặt chìa khóa vàng. Tuy nhiên, baseline Always thất bại trong trường hợp này vì translator không mã hóa thông tin về vị trí tương đối giữa agent và đối tượng mục tiêu một cách chính xác. Do đó, translator trả về cùng một quan sát [quan sát chìa khóa vàng, quan sát cửa vàng, mang chìa khóa tím] cho cả khung 1 và 2. Ngược lại, phương pháp của chúng tôi học "không hỏi" để hỗ trợ trong trường hợp cụ thể này, cho phép agent hoàn thành hành động nhặt chìa khóa vàng trước khi yêu cầu hướng dẫn thêm. Điều này làm nổi bật một lợi thế đáng kể của phương pháp chúng tôi so với các phương pháp baseline: nó có thể thích ứng với các tình huống nơi quá trình dịch của translator mất nhiều thông tin. Chính sách hỏi đã học cho phép agent đưa ra quyết định sáng suốt hơn dựa trên quan sát và ngữ cảnh của nó, dẫn đến hiệu suất bền vững trong các kịch bản mà các phương pháp baseline có thể thất bại do lỗi của translator.

A.3 Thiết lập thí nghiệm trên Habitat
Habitat là một nền tảng mô phỏng được thiết kế đặc biệt cho phát triển end-to-end của AI được thể hiện Szot et al. (2021). Nó cung cấp một khung để định nghĩa các nhiệm vụ AI được thể hiện khác nhau như điều hướng, sắp xếp lại đối tượng, và trả lời câu hỏi. Ngoài ra, nó cho phép cấu hình các agent được thể hiện với các hình thức vật lý và cảm biến cụ thể. Agents có thể được đào tạo sử dụng các kỹ thuật học bắt chước hoặc học tăng cường. Trong các thí nghiệm của chúng tôi, chúng tôi chứng minh rằng phương pháp của chúng tôi có thể tổng quát hóa hiệu quả cho các miền thực tế về mặt thị giác bằng cách thực hiện thí nghiệm trong môi trường Habitat.

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Hình 10: Một ví dụ về prompts và tương tác cho môi trường habitat. Prefix prompt chỉ chứa một hướng dẫn nhiệm vụ ngắn.

Trong các thí nghiệm của chúng tôi, chúng tôi tập trung vào nhiệm vụ thao tác được gọi là Pick&Place. Trong nhiệm vụ này, mục tiêu của agent robot là nhặt một đối tượng từ bàn và đặt nó chính xác vào một thùng chứa mục tiêu được chỉ định, trong trường hợp này là bồn rửa nhà bếp. Thiết lập nhiệm vụ giống như trong thí nghiệm Habitat trong Zhou et al. (2024). Agent robot được trang bị một đế có bánh xe, một manipulator cánh tay 7 bậc tự do (DoF), và một gripper hàm song song. Ngoài ra, nó có một camera gắn trên "đầu" cung cấp trường nhìn 90° và ghi lại dữ liệu thị giác ở độ phân giải 256×256 pixel. Kết quả là, không gian quan sát của môi trường bao gồm một quan sát thị giác được ký hiệu là ov∈R256×256×1 từ camera độ sâu. Nó cũng bao gồm một quan sát cảm biến os∈R24 được lấy từ các cảm biến khác nhau như cảm biến khớp, cảm biến gripping, end effector của cánh tay, cảm biến GPS của đối tượng và mục tiêu, trong số những cái khác. Không gian hành động trong thiết lập của chúng tôi là 11 chiều, bao gồm 3 hành động điều khiển vị trí robot, 7 hành động điều khiển cánh tay robot và một hành động chỉ ra kết thúc. Không gian hành động này cho phép agent thực hiện các chuyển động và thao tác chính xác cần thiết để hoàn thành nhiệm vụ mục tiêu.

Để đào tạo hiệu quả mỗi option, chúng tôi thiết kế phần thưởng dựa trên các thước đo sắp xếp lại. Những thước đo này tính đến các yếu tố khác nhau như lực được tác dụng bởi agent có khớp, khoảng cách giữa đối tượng và mục tiêu, và góc giữa agent và mục tiêu. Chi tiết cụ thể của những thước đo này có thể được tìm thấy trong tài liệu Habitat Szot et al. (2021).

Trong môi trường Pick&Place, vì việc giải quyết nhiệm vụ đòi hỏi đạt được dần dần một số mục tiêu phụ, chúng tôi sử dụng hệ thống phần thưởng giai đoạn tổng hợp. Cụ thể hơn, việc nhặt đối tượng thành công được gọi là Hoàn thành Giai đoạn1 và thưởng giá trị 1. Đạt được điều hướng đến bồn rửa với đối tượng được gọi là Hoàn thành Giai đoạn2 và cũng thưởng giá trị 1. Cuối cùng, việc đặt quả táo thành công vào bồn rửa mục tiêu được gọi là Hoàn thành Giai đoạn3 và trao giá trị phần thưởng cao hơn là 5. Điều quan trọng cần lưu ý là nếu bất kỳ options cấp cao nào vượt quá giới hạn thời gian được chỉ định của chúng, nhiệm vụ có thể kết thúc sớm.

A.3.1 Chi tiết thực hiện phương pháp của chúng tôi trên Habitat
Planner Chúng tôi sử dụng mô hình Vicuna-7b được đào tạo trước như planner LLM trong phương pháp của chúng tôi. Về thiết kế prompt, chúng tôi bắt đầu bằng cách cung cấp một hướng dẫn ngắn gọn truyền đạt thông tin về nhiệm vụ mục tiêu. Sau đó, chúng tôi cung cấp mô tả về quan sát hiện tại dưới dạng danh sách Python. Một ví dụ về cuộc đối thoại được tạo ra bởi planner LLM có thể được tìm thấy trong Hình 10.

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại RLC 2024
Actor Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng ba options cấp cao: {Navigate, Pick, Place}, mỗi cái được đào tạo trước với RL độc lập. Bất cứ khi nào có chuyển đổi giữa những options này, việc thực hiện tự động hành động mặc định Reset Arm xảy ra. Để đảm bảo đào tạo hiệu quả của những options này, chúng tôi sử dụng 32 đặc tả môi trường đào tạo riêng biệt với các vị trí đối tượng và vị trí mục tiêu khác nhau. Ngoài ra, vị trí ban đầu của agent được tạo ra ngẫu nhiên mỗi khi môi trường được reset, đảm bảo sự đa dạng trong các kịch bản đào tạo.

Đối với mỗi option, chúng tôi sử dụng backbone ResNet18 kết hợp với kiến trúc LSTM 2 lớp để đào tạo các mô hình tương ứng. Trong quá trình thử nghiệm, tỷ lệ thành công của Navigate, Pick, và Place lần lượt là 84%, 92%, và 91%. Những mô hình được đào tạo trước này vẫn cố định trong suốt nhiệm vụ, đảm bảo tính nhất quán và ổn định trong quá trình thực hiện.

Đào tạo chính sách hỏi Tương tự như thí nghiệm Minigrid của chúng tôi, chúng tôi xếp chồng năm khung quan sát liên tiếp làm đầu vào cho chính sách hỏi. Điều này cho phép mạng nắm bắt thông tin thời gian và đưa ra quyết định sáng suốt dựa trên các quan sát trong quá khứ. Kiến trúc mạng cho chính sách hỏi bao gồm ba lớp CNN để nhúng quan sát thị giác, một lớp MLP để nhúng quan sát cảm biến, và hai lớp MLP bổ sung để xuất ra logits cho câu hỏi nhị phân có hỏi hay không hỏi.
