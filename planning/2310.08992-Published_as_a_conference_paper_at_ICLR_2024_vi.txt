# 2310.08992.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2310.08992.pdf
# Kích thước file: 3517502 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
CODECHAIN : HƯỚNG TỚI VIỆC TẠO MÃ MODULAR THÔNG QUA CHUỖI TỰ CHỈNH SỬA VỚI CÁC SUB-MODULE ĐẠI DIỆN
Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, Shafiq Joty
Salesforce Research
{hungle, hailin.chen, amrita.saha}@salesforce.com
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLMs) đã trở nên khá thành thạo trong việc giải quyết các tác vụ lập trình đơn giản hơn như những tác vụ trong bộ đánh giá HumanEval hoặc MBPP. Tuy nhiên, việc giải quyết các tác vụ lập trình phức tạp và cạnh tranh vẫn còn khá thách thức đối với những mô hình này - có thể do xu hướng tạo ra các giải pháp dưới dạng khối mã nguyên khối thay vì phân tách chúng thành các sub-task và sub-module logic. Mặt khác, các lập trình viên có kinh nghiệm thường tự nhiên viết mã modular với tính trừu tượng để giải quyết các tác vụ phức tạp, thường tái sử dụng các module đã được phát triển trước đó. Để giải quyết khoảng cách này, chúng tôi đề xuất CodeChain, một framework mới cho inference thúc đẩy việc tạo mã modular thông qua một chuỗi tự chỉnh sửa, mỗi lần được hướng dẫn bởi một số sub-module đại diện được tạo ra trong các lần lặp trước đó. Cụ thể, CodeChain đầu tiên hướng dẫn LLM tạo ra mã modular thông qua chain-of-thought prompting. Sau đó, nó áp dụng một chuỗi tự chỉnh sửa bằng cách lặp đi lặp lại hai bước: 1) trích xuất và phân cụm các sub-module được tạo ra và chọn các đại diện cụm làm các implementation tổng quát và có thể tái sử dụng hơn, và 2) bổ sung prompt chain-of-thought gốc với những module-implementation đã chọn này và hướng dẫn LLM tái tạo các giải pháp modular mới. Chúng tôi thấy rằng bằng cách khuyến khích LLM tái sử dụng các sub-module đã được phát triển và xác minh trước đó một cách tự nhiên, CodeChain có thể nâng cao đáng kể cả tính modular cũng như độ chính xác của các giải pháp được tạo ra, đạt được sự cải thiện pass@1 tương đối 35% trên APPS và 76% trên CodeContests. Nó được chứng minh là hiệu quả trên cả OpenAI LLMs cũng như các LLM mã nguồn mở như WizardCoder. Chúng tôi cũng tiến hành các nghiên cứu ablation toàn diện với các phương pháp prompting khác nhau, số lượng cụm, kích thước mô hình, chất lượng chương trình, v.v., để cung cấp những hiểu biết hữu ích làm nền tảng cho thành công của CodeChain.

1 GIỚI THIỆU
Việc phát triển các hệ thống có thể tạo ra các chương trình máy tính có thể thực thi được và chính xác về mặt chức năng để giải quyết các vấn đề phức tạp từ lâu đã là một mục tiêu trong AI (Manna & Waldinger, 1971). Trong những năm gần đây, chúng ta đã chứng kiến tiến bộ chưa từng có trong lĩnh vực này, đặc biệt với thành công đáng kể của các mô hình ngôn ngữ lớn được pre-train hay LLMs (Koubaa, 2023; Wang & Komatsuzaki, 2021; Radford et al., 2019). Ban đầu được phát triển cho ngôn ngữ tự nhiên, những mô hình này đã được mở rộng với sự kết hợp các khả năng mô hình hóa mã và văn bản (Rozière et al., 2023; Black et al., 2021; Chen et al., 2021), dẫn đến hiệu suất tốt trong việc tạo mã từ mô tả vấn đề bằng ngôn ngữ tự nhiên (Li et al., 2023; Luo et al., 2023; Wang et al., 2023). Tuy nhiên, khi được đánh giá trên các tác vụ coding rất phức tạp, các mô hình SoTA hiện tại vẫn không thể sánh với một developer có kỹ năng (Hendrycks et al., 2021; Li et al., 2022; Shinn et al., 2023), chủ yếu do phương pháp tạo ra thô sơ của chúng.

Hầu hết các phương pháp trước đây với LLMs áp dụng phương pháp tạo ra thô sơ trong đó các mô hình thường tạo ra giải pháp mã như một khối mã nguyên khối duy nhất thay vì phân tách tác vụ thành các sub-task logic. Một giới hạn khác của phương pháp tạo ra thô sơ này là các mô hình sẽ đơn giản tạo ra một số lượng lớn các giải pháp một cách độc lập, với hy vọng rằng một trong những giải pháp đó

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hãy gọi chuỗi đó là đẹp nếu nó không chứa một chuỗi con có độ dài ít nhất 2, đó là một palindrome...
Hãy định nghĩa chi phí của một chuỗi là số phép toán tối thiểu để chuỗi đó trở nên đẹp...
Bạn được cho một chuỗi s có độ dài n...
Bạn phải trả lời m truy vấn — tính chi phí của chuỗi con của chuỗi s từ vị trí thứ l_i đến vị trí thứ r_i, bao gồm cả hai đầu... Mô tả bài toán Các test case công khai
Input:
10 3\nababababab\n1
2\n1 2\n1 2
Output:
0\n0\n0\n
...Các test case riêng tư
Input:
5 4\nbaacb\n1 3\n1
5\n4 5\n2 3
Output:
1\n2\n0\n1
...

Developer tạo ra so sánh/
phân tích
tái sử dụng & chỉnh sửa gửi để test cuối cùng

Hình 1: [Trên] Một ví dụ về tác vụ tạo mã từ CodeContests (Li et al., 2022) nơi mô tả bài toán và các test case công khai được cung cấp làm input cho mô hình. [Dưới] Chúng tôi minh họa một quy trình giải quyết vấn đề điển hình trong đó một developer cố gắng giải quyết vấn đề một cách lặp đi lặp lại, chỉnh sửa và tái sử dụng các phần mã đã được phát triển trước đó cho đến khi hài lòng.

sẽ vượt qua tất cả các test case riêng tư (Chen et al., 2021; Li et al., 2023; Austin et al., 2021). Gần đây hơn, Li et al. (2022); Chen et al. (2023b); Zhang et al. (2023b) đề xuất sub-sample các chương trình đầu ra bằng cách sử dụng một số dạng phản hồi từ kết quả test công khai. Tuy nhiên, những phương pháp này giả định rằng các chương trình được sub-sample có thể vượt qua các test case riêng tư, ngay cả khi không chỉnh sửa hoặc debug các chương trình. Một số công trình gần đây như (Zhang et al., 2023a; Olausson et al., 2023; Le et al., 2022; Chen et al., 2023c;a; Shinn et al., 2023) đã giải quyết điều này bằng cách thực hiện tự chỉnh sửa với LLMs, sử dụng các phản hồi như thông báo lỗi compiler, kết quả test, và giải thích bằng ngôn ngữ tự nhiên để cải thiện các giải pháp được tạo ra. Tuy nhiên, những phương pháp này chỉ giới hạn việc sử dụng phản hồi độc lập từ các giải pháp cá nhân, bỏ qua những hiểu biết tập thể tiềm năng từ tất cả các mẫu tạo ra hoặc các thành phần con của chúng.

Mặt khác, trong môi trường phát triển agile ngày nay, các developer có kinh nghiệm hoàn toàn quen thuộc với khái niệm modularity trong lập trình. Khi được đưa ra một vấn đề, họ sẽ tự nhiên viết các giải pháp được modular hóa bởi các sub-task và sub-module logic cấp cao. Các developer sau đó sẽ tiếp tục test và phân tích các implementation của họ, thay đổi các thành phần modular từ các giải pháp đã được phát triển trước đó để cải thiện hiệu quả các giải pháp cuối cùng của họ (xem Hình 1). Lấy cảm hứng từ quy trình giải quyết vấn đề này, chúng tôi đề xuất CodeChain, một framework inference mới để cải thiện việc tạo mã trong LLMs thông qua một chuỗi tự chỉnh sửa dựa trên sub-module (xem Hình 2).

Cụ thể, trong CodeChain, để kết hợp modularity trong việc tạo mã, chúng tôi đầu tiên giới thiệu chain-of-thought prompting để hướng dẫn LLMs phân tách các giải pháp của chúng thành các phân đoạn modular. Mỗi phân đoạn modular đại diện cho một hàm trừu tượng được dành cho một sub-task logic cấp cao. Để tận dụng modularity này trong các chương trình, chúng tôi đề xuất cải thiện thêm quy trình tạo ra thông qua một chuỗi tự chỉnh sửa, mỗi cái được điều kiện hóa bởi một tập các sub-module được lấy mẫu như sau: (i) chúng tôi đầu tiên trích xuất các sub-module được tìm thấy trong các chương trình được tạo ra và nhóm chúng thành các cụm. Trong mỗi cụm, chúng tôi lấy mẫu các sub-module centroid và coi chúng như các phần mã đại diện và có thể tái sử dụng cho việc tự chỉnh sửa. (ii) Sau đó chúng tôi bổ sung prompt chain-of-thought gốc với các sub-module đã chọn này và hướng dẫn LLMs tạo ra các giải pháp modular mới. Với phương pháp này, LLMs có thể nhận được những hiểu biết tập thể từ các thành phần modular của tất cả các mẫu tạo ra trong quá khứ để cải thiện các lần tạo ra trong tương lai, bắt chước quy trình giải quyết vấn đề của một developer có kinh nghiệm.

Các thí nghiệm của chúng tôi cho thấy CodeChain có thể nâng cao đáng kể hiệu suất LLM và đạt được hiệu suất SoTA trên các tác vụ mã thách thức trong APPS (Hendrycks et al., 2021) và CodeContests (Li et al., 2022). Cụ thể, CodeChain cải thiện hiệu suất pass@1 trung bình hơn 35% trên APPS và 76% trên CodeContests. Chúng tôi cũng quan sát thấy sự cải thiện nhất quán cho cả OpenAI LLMs cũng như các LLM mã nguồn mở như WizardCoder (Luo et al., 2023). Chúng tôi tiếp tục tiến hành các nghiên cứu ablation toàn diện, bao gồm phân tích trong đơn vs. đa bước chỉnh sửa, các loại phản hồi, số lượng cụm, v.v., và rút ra những hiểu biết hữu ích đằng sau thành công của CodeChain.

2 CÔNG TRÌNH LIÊN QUAN
Liên quan rộng rãi đến công trình của chúng tôi là nghiên cứu về các mô hình ngôn ngữ lớn dựa trên Transformer (LLMs) (Koubaa, 2023; Brown et al., 2020; Radford et al., 2019; Wang & Komatsuzaki, 2021; Touvron et al., 2023a). Ban đầu được thiết kế cho xử lý ngôn ngữ tự nhiên, những mô hình này đã được mở rộng để

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Mô tả bài toán → Pretrained Code LLM → Các giải pháp Modularized
def sort_list(n):
   ```
   Hàm này...
   ```
   output=list(...

Bước3: Trích xuất các hàm modular

Không gian embedding → def func(x):
```
...Chuỗi Tự chỉnh sửa

Bước1: CoT prompting

Bước5: Các sub-module đại diện → Bước6: Tái tạo

Tái sử dụng/Điều chỉnh các sub-module đã chọn → Encode → Các test case riêng tư

Bước2: Lọc bằng test case công khai

Bước4: Phân cụm sub-module → Đánh giá

Hình 2: Tổng quan về CodeChain: một LLM được pretrain đầu tiên được hướng dẫn với chain-of-thought prompting để tạo ra một tập các giải pháp modular. Các sub-module được tạo ra sau đó được trích xuất từ các giải pháp có khả năng đúng và được nhóm thành các cụm semantic khác nhau. Các centroid cụm được chọn làm các sub-module đại diện để điều kiện hóa vòng tự chỉnh sửa tiếp theo. Mô hình được hướng dẫn tái sử dụng hoặc điều chỉnh các module này vào các giải pháp đã được chỉnh sửa của nó.

học từ dữ liệu mã quy mô lớn và trở nên thành thạo trong việc hiểu ngữ cảnh và tạo ra đầu ra trong các ngôn ngữ lập trình (Rozière et al., 2023; Chen et al., 2021; Li et al., 2023; Gunasekar et al., 2023; Wang et al., 2023; Nijkamp et al., 2023). Bổ sung cho nghiên cứu tạo mã lâu đời (Gulwani et al., 2012; Kurach et al., 2015; Devlin et al., 2017; Parisotto et al., 2016), LLMs có thể tạo ra các chương trình của các ngôn ngữ lập trình mục đích tổng quát hơn, tuân theo đúng các quy tắc cú pháp lập trình (Lu et al., 2021; Clement et al., 2020) và giải quyết các vấn đề coding đơn giản với độ chính xác hợp lý (Lai et al., 2022; Chen et al., 2021; Austin et al., 2021).

Liên quan trực tiếp hơn đến công trình của chúng tôi là dòng công trình gần đây để cải thiện chất lượng tạo mã thông qua phản hồi đầu ra. Chen et al. (2021) đã giới thiệu một phương pháp lọc đơn giản bằng cách chỉ chọn các mẫu đầu ra vượt qua thành công các test case công khai. AlphaCode (Li et al., 2022), CodeT (Chen et al., 2023b), và MBR-Exec (Shi et al., 2022) đề xuất tạo ra nhiều test case hơn và sử dụng các phương pháp dựa trên quy tắc tinh vi hơn để xếp hạng các mẫu tạo ra theo hành vi thực thi của chúng. LEVER (Ni et al., 2023), Coder-Reviewer (Zhang et al., 2023b) và Code Rankers (Inala et al., 2022) tuân theo một nguyên tắc tương tự nhưng giới thiệu các phương pháp xếp hạng dựa trên mô hình nhiều hơn.

Gần đây, các công trình liên quan hơn đã được đề xuất để thúc đẩy chất lượng tạo ra thông qua các tự chỉnh sửa lặp đi lặp lại. Zhang et al. (2023a) sử dụng kết quả test từ các test case công khai như một dạng phản hồi để các mô hình tự chỉnh sửa mã của chúng. Self-correct (Welleck et al., 2023) và CodeRL (Le et al., 2022) giới thiệu các mô hình thứ cấp để dự đoán tính đúng đắn của các chương trình đầu ra và chỉnh sửa chúng tương ứng. Self-debug (Chen et al., 2023c), Self-refine (Madaan et al., 2023), và Reflexion (Shinn et al., 2023) đề xuất tạo điều kiện cho việc chỉnh sửa mã tốt hơn với giải thích ngôn ngữ tự nhiên tổng hợp hoặc reflection được tự tạo bởi LLMs. Self-repair (Olausson et al., 2023) và ILF (Chen et al., 2023a) tuân theo một chiến lược tương tự nhưng nhấn mạnh việc sử dụng giải thích ngôn ngữ tự nhiên được cung cấp bởi các chuyên gia con người. Khác với các phương pháp trước đây, chúng tôi đề xuất tạo ra các chương trình modular hơn và chỉnh sửa tuần tự những chương trình này bằng cách sử dụng các chương trình sub-module đại diện và có thể tái sử dụng hơn (vui lòng xem Phụ lục A để so sánh có hệ thống hơn).

3 FRAMEWORK CODECHAIN

3.1 TÁC VỤ TẠO MÃ

Chúng tôi coi việc tạo mã như một tác vụ sequence-to-sequence, bao gồm một mô tả vấn đề như một chuỗi đầu vào D và một chuỗi đầu ra của một chương trình giải pháp được flatten: Ŵ = (ŵ₁, ..., ŵₜ) với ŵₜ ∈ V. Thông thường, một mô hình ngôn ngữ θ tạo ra một chuỗi mã bằng cách sampling autoregressive

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

*Hướng dẫn*
Phát triển một giải pháp Python có cấu trúc tốt cho bài toán được cung cấp tuân thủ các ràng buộc và vượt qua các test case ví dụ. Đảm bảo tính modular và xem xét các edge case và lỗi tiềm ẩn. Bắt đầu bằng việc phác thảo các module mã cần thiết, bao gồm header và signature của hàm. Sau đó, tiến hành implement mỗi module để tạo ra mã cuối cùng.

Nói một cách đơn giản hơn, tạo ra một giải pháp Python sạch sẽ và có tổ chức cho bài toán đã cho. Chia nhỏ nó thành các phần nhỏ hơn (module) với tên hàm rõ ràng và đặc tả input/output. Khi cấu trúc đã sẵn sàng, viết mã thực tế cho mỗi module để hoàn thành giải pháp.

Chain-of-thought prompting cho việc tạo mã

Hình 3: Một ví dụ về CoT prompting cho việc tạo mã trong CodeChain. Mô hình được yêu cầu đầu tiên phác thảo giải pháp theo các signature sub-module, mỗi cái được dành cho việc giải quyết một sub-task cấp cao trong giải pháp cuối cùng. Mô hình sau đó được yêu cầu implement những sub-module này và kết hợp chúng thành một giải pháp cuối cùng (xem Phụ lục F để có phiên bản đầy đủ của prompt).

các token ŵₜ từ phân phối có điều kiện tham số hóa pᵨ(.|ŵ₁:ₜ₋₁, D). Các mã được tạo ra được đánh giá dựa trên các test case (riêng tư) để kiểm tra tính đúng đắn của việc thực thi (Hendrycks et al., 2021; Chen et al., 2021; Li et al., 2022). Các test case bao gồm một tập các cặp input-output {(iⱼ, oⱼ)}ᴶⱼ₌₁. Một chương trình đầu ra Ŵ là đúng khi Ŵ(iⱼ) = oⱼ cho tất cả j ∈ {1, ..., J}. Nếu mô tả bài toán chứa một số test case, chúng tôi coi những cái này là test case công khai: {(i'ₘ, o'ₘ)}ᴹₘ₌₁ (thường M ≪ J). Các mô hình có tùy chọn sử dụng những test case công khai này để cải thiện việc tạo ra của nó.

3.2 TẠO MÃ MODULAR VỚI COT PROMPTING

LLMs, đặc biệt là những cái được instruction-tuned, có thể tuân theo các hướng dẫn ngôn ngữ tự nhiên phức tạp mô tả các tác vụ mới chưa được thấy (Ouyang et al., 2022; Touvron et al., 2023b; Wang et al., 2023). Chúng đã cho thấy hiệu suất đáng kể trong nhiều tác vụ dựa trên reasoning khi chúng được hướng dẫn giải quyết một vấn đề từng bước một, tức là chain-of-thought (CoT) prompting (Zhou et al., 2023; Wei et al., 2022; Kojima et al., 2022). Chúng tôi đề xuất điều chỉnh kỹ thuật này để tạo mã bằng cách hướng dẫn các mô hình đầu tiên phác thảo các sub-module cần thiết, chỉ tạo ra header hàm và docstring mô tả cách sử dụng dự định của chúng. Mô hình sau đó được hướng dẫn implement các module và cuối cùng kết hợp chúng thành một giải pháp cuối cùng. Tuân theo lược đồ tạo ra này, chúng ta có thể định nghĩa các phân phối đầu ra:

Ŝᵢ ~ pᵨ(.|Ŝ₁:ᵢ₋₁, D) ⇒ sub-module, bao gồm header hàm và docstring (1)
ŵₜ ~ pᵨ(.|ŵ₁:ₜ₋₁, {Ŝᵢ}, D) ⇒ token trong giải pháp cuối cùng (2)

trong đó {Ŝᵢ} là tập các sub-module được phác thảo bởi mô hình. Chúng tôi nối thêm hướng dẫn với một demonstration một lần. Hình 3 trình bày một ví dụ về prompt hướng dẫn.

Như được minh họa thêm bởi Hình 10 trong Phụ lục, kỹ thuật này khuyến khích mô hình phân tách một chương trình thành các ranh giới tự nhiên, ví dụ: sub-module, tương tự như cách một developer thường giải quyết một tác vụ coding thách thức bằng cách chia một giải pháp thành các thành phần modular. Mặc dù đây là một phong cách phát triển mã thực dụng hơn, về mặt thực nghiệm chúng tôi đã thấy rằng phương pháp prompting này có thể ảnh hưởng tiêu cực đến tính đúng đắn của các giải pháp end-to-end được tạo ra (được hiển thị sau đó trong Bảng 3). Điều này được mong đợi vì hầu hết các LLM hiện tại không được pretrain để tạo ra các chương trình modular hoạt động hoàn hảo. Để giải quyết điều này, chúng tôi giới thiệu Chain of Self-Revisions cho phép LLM lặp đi lặp lại chỉnh sửa một giải pháp bằng cách tái sử dụng hoặc điều chỉnh một số sub-module đại diện từ các lần lặp trước đó. Hơn nữa, chúng tôi cũng thiết lập thực nghiệm rằng kỹ thuật tự chỉnh sửa của chúng tôi thực sự có lợi hơn từ phong cách tạo mã modular này.

3.3 CHỌN CÁC SUB-MODULE ĐẠI DIỆN QUA NHIỀU MẪU

Các nghiên cứu trước đây đã chứng minh lợi ích của việc tạo ra nhiều mẫu và chọn những cái tốt nhất dựa trên các lược đồ xếp hạng hoặc tính điểm khác nhau (Li et al., 2022; Chen et al., 2023b; Zhang et al., 2023b). Một phương pháp phổ biến là đơn giản chọn các ứng viên đại diện dựa trên kết quả thực thi của chúng trên các test case công khai (Li et al., 2022; Chen et al., 2021). Tuy nhiên, tất cả các phương pháp trước đây chỉ chọn các ứng viên chương trình end-to-end. Trên các tác vụ coding thách thức, việc có được tính đúng đắn ở cấp độ chương trình như vậy là cực kỳ hiếm và các ứng viên đã chọn vẫn có khả năng thất bại khi được test trên các test case riêng tư. Do đó, chúng tôi đề xuất thực hiện việc chọn lựa ở cấp độ sub-module thay vì cấp độ chương trình.

Cụ thể, cho một ngân sách tạo ra N mẫu, chúng tôi trích xuất và kết hợp tập các sub-module qua tất cả các mẫu Ŝ = {{Ŝᵢ}ₙ} cho tất cả n ∈ {1, ..., N}, trong đó {Ŝᵢ}ₙ là tập các sub-module trong mẫu được tạo ra thứ n. Sau đó chúng tôi thực hiện phân cụm K-mean trên tập sub-module này để nhóm chúng thành K cụm. Đối với mỗi cụm trong số này, chúng tôi sau đó trích xuất một sub-module "centroid" (đại diện) Ĉₖ gần nhất với centroid thực sự của cụm trong không gian embedding:

Ĉₖ = arg min_{Ŝₖᵢ} ‖Sₖᵢ - μₖ‖ (3)

trong đó Sₖᵢ là biểu diễn embedding của sub-module Ŝᵢ trong cụm k và μₖ là centroid của cụm k. Bằng cách chọn những sub-module "centroid" này, chúng ta có thể lấy mẫu các hàm đại diện nhất về mặt semantic và có thể tái sử dụng nhất qua tất cả các mẫu. Lưu ý rằng trong trường hợp có test case công khai, người ta có thể lọc ra bất kỳ mẫu thất bại nào trước khi áp dụng thêm phương pháp chọn lựa của chúng tôi.

3.4 CẢI THIỆN VIỆC TẠO MÃ VỚI CHAIN OF SELF-REVISIONS

*Hướng dẫn*
...Cho một tập các hàm Python tiện ích liên quan, hãy cố gắng tái sử dụng hoặc điều chỉnh chúng càng nhiều càng tốt vào giải pháp của bạn (tạo ra các hàm mới duy nhất nếu cần)....
-----------------
### TÁC VỤ:
<<bài toán>>
### CÁC HÀM LIÊN QUAN:
<<sub-module>>
### PHẢN HỒI:

Prompting tự chỉnh sửa với các sub-module đại diện

Hình 4: Một ví dụ về prompting để tự chỉnh sửa chương trình. Hướng dẫn gốc từ CoT prompting (Hình 3) được kết hợp với hướng dẫn này và mô hình được cung cấp với một tập các sub-module đại diện («sub-module») được chọn từ các mẫu đã tạo ra trước đó. Vui lòng tham khảo Phụ lục F để có phiên bản đầy đủ của prompt.

Các phương pháp trước đây đã cải thiện việc tạo mã bằng cách tái tạo mã được điều kiện bởi các loại phản hồi khác nhau, từ thông báo lỗi compiler đến giải thích ngôn ngữ tự nhiên của các chương trình đầu ra (Chen et al., 2023a; Madaan et al., 2023; Chen et al., 2023c; Shinn et al., 2023; Le et al., 2022). Tuy nhiên, những phương pháp này tập trung vào phản hồi được trích xuất chỉ từ mỗi mẫu tạo ra cá nhân. Chúng tôi đề xuất sử dụng một loại phản hồi mới dưới dạng các sub-module được phân cụm được trích xuất từ tất cả N mẫu được tạo ra (như được mô tả trong Mục 3.3). Việc bổ sung prompt CoT gốc của chúng tôi với các implementation của những sub-module đại diện này có thể khuyến khích một cách rõ ràng LLM tái sử dụng hoặc điều chỉnh những hàm này khi tạo ra mã được điều kiện trên prompt đó trong các vòng chỉnh sửa tiếp theo. Cụ thể, trong vòng chỉnh sửa R, token đầu ra được lấy mẫu từ phân phối có điều kiện:

ŵᴿₜ ~ pᵨ(.|ŵᴿ₁:ₜ₋₁, {Ŝᴿᵢ}, Ĉᴿ⁻¹, D) (4)

trong đó Ĉᴿ⁻¹ = {Ĉᴿ⁻¹ₖ}ᴷₖ₌₁ là tập tất cả các sub-module centroid từ vòng tạo ra trước đó R-1. Trong vòng R, các sub-module mới được tái tạo bởi xác suất có điều kiện (phiên bản chỉnh sửa của Phương trình 1):

Ŝᴿᵢ ~ pᵨ(.|Ŝᴿ₁:ᵢ₋₁, Ĉᴿ⁻¹, D) (5)

Chúng tôi kích hoạt quy trình tự chỉnh sửa này bằng cách prompting LLM với một hướng dẫn bổ sung. Hình 4 trình bày một ví dụ về prompt với hướng dẫn mới. Phong cách tự chỉnh sửa này với các sub-module có chọn lọc gợi nhớ đến quy trình tái sử dụng mã. Trong môi trường phát triển mã agile ngày nay, các developer thường tái sử dụng hoặc điều chỉnh các đoạn mã đã được phát triển trước đó để lập trình một cách modular, chính xác và hiệu quả hơn. Lấy cảm hứng từ quy trình này và kết hợp với phương pháp chọn sub-module đại diện của chúng tôi, framework CodeChain của chúng tôi cho phép LLM cải thiện lặp đi lặp lại các thế hệ của chúng một cách hiệu quả hơn thông qua một chuỗi tự chỉnh sửa dựa trên tái sử dụng.

4 THỰC NGHIỆM

4.1 THIẾT LẬP THỰC NGHIỆM

Benchmark. Chúng tôi chứng minh hiệu quả của CodeChain trên các tác vụ tạo mã thách thức, cụ thể, trên hai benchmark chính: APPS (Hendrycks et al., 2021), và CodeContests (Li et al.,

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 1: Kết quả test APPS: kết quả với † là cho các mô hình được finetuned trên dữ liệu training APPS

(a) Hiệu suất theo pass@1 (%)
Mô hình | Kích thước | Introductory | Interview | Competition | Tất cả
Codex | 12B | 4.14 | 0.14 | 0.02 | 0.92
CodeT5 † | 770M | 6.60 | 1.03 | 0.30 | 2.00
CodeRL+CodeT5 † | 770M | 7.08 | 1.86 | 0.75 | 2.69
text-davinci-002 | - | - | - | - | 7.48
Self-edit+text-davinci-002 | - | - | - | - | 7.94
code-davinci-002 | - | 29.30 | 6.40 | 2.50 | 10.20
WizardCoder | 15B | 26.04 | 4.21 | 0.81 | 7.90
CodeChain+WizardCoder | 15B | 26.29 | 7.49 | 3.75 | 10.50
GPT3.5 | - | 48.00 | 19.42 | 5.42 | 22.33
CodeChain+GPT3.5 | - | 54.50 | 28.11 | 12.38 | 30.24

(b) Hiệu suất theo pass@1 (%) với đầu ra được lọc bởi test công khai/synthetic
Mô hình | Kích thước | Lọc | Introductory | Interview | Competition | Tất cả
Codex | 12B | thô sơ | 22.78 | 2.64 | 3.04 | 6.75
CodeRL+CodeT5 † | 770M | thô sơ | 17.17 | 6.78 | 4.88 | 8.48
code-davinci-002 | - | thô sơ | 43.60 | 13.30 | 7.00 | 18.10
code-davinci-002 | - | CodeT | 47.30 | 14.30 | 6.20 | 19.28
GPT3.5 | - | CodeT | 61.52 | 30.57 | 9.46 | 32.54
CodeChain+GPT3.5 | - | CodeT | 62.72 | 32.96 | 15.08 | 35.34

2022). Đa số các mẫu test từ những benchmark này được sưu tầm từ các nền tảng lập trình cạnh tranh như Codeforces², khiến chúng trở thành một testbed phù hợp để đánh giá phương pháp của chúng tôi. Vui lòng tham khảo Phụ lục C và Bảng 6 để biết thêm chi tiết về các benchmark.

Đánh giá. Chúng tôi tuân theo (Hendrycks et al., 2021; Chen et al., 2021; Li et al., 2022) và đánh giá các mô hình sử dụng metric tỷ lệ vượt qua pass@k, được định nghĩa là tỷ lệ phần trăm các bài toán được giải quyết bằng cách sử dụng k chương trình được tạo ra mỗi bài toán. Chúng tôi tập trung chủ yếu vào pass@1 trong công trình này và tuân theo (Chen et al., 2021) để tính tỷ lệ vượt qua được chuẩn hóa cho một ngân sách tạo ra N đầu ra mỗi bài toán. Để áp dụng CodeChain, chúng tôi cố định ngân sách trong mỗi vòng tạo ra/chỉnh sửa thành N = 20 mẫu tạo ra mỗi bài toán. Sau vòng đầu tiên của việc tạo ra trực tiếp, chúng tôi để các mô hình tự chỉnh sửa các mã được tạo ra trong tối đa 5 vòng chỉnh sửa. Trên APPS và CodeContests, chúng tôi báo cáo kết quả trên split test theo hiệu suất vòng tự chỉnh sửa tốt nhất trên validation set. Qua tất cả các benchmark, chúng tôi cố định mẫu one-shot trong CoT prompting và revision prompting. Chúng tôi chọn ngẫu nhiên mẫu one-shot này từ split training APPS (xem Phụ lục G).

Các mô hình ngôn ngữ cơ sở. Chúng tôi áp dụng CodeChain cho cả LLM pretrained mã nguồn mở và đóng, bao gồm GPT3.5 và GPT4 của OpenAI (Koubaa, 2023), và WizardCoder (Luo et al., 2023). Chúng tôi đánh giá các phiên bản khác nhau của WizardCoder, với kích thước mô hình từ 1B đến 34B tham số. Các mô hình WizardCoder được instruction-tuned từ các LLM mã nền tảng mạnh, bao gồm StarCoder (Li et al., 2023) và Code LLaMA (Rozière et al., 2023). Đối với các mô hình OpenAI, chúng tôi có được các mẫu tạo ra bằng cách prompting thông qua truy cập API công khai³. Đối với WizardCoder, chúng tôi sử dụng các tham số mô hình được lưu trữ trên HuggingFace (Wolf et al., 2019) và vLLM (Kwon et al., 2023) để tạo ra các chương trình. Chúng tôi áp dụng temperature mặc định là 0.6 để tạo ra các token đầu ra và độ dài đầu ra tối đa là 2048 token. Cuối cùng, để so sánh công bằng khả năng tạo ra của LLM, chúng tôi chọn sử dụng StarEncoder (Li et al., 2023) để embedding các sub-module được lấy mẫu xuyên suốt tất cả các thí nghiệm.

4.2 KẾT QUẢ THỰC NGHIỆM

Kết quả trên APPS. Chúng tôi so sánh phương pháp của chúng tôi với các baseline LLM trước đây như Codex (Chen et al., 2021), CodeT5 (Wang et al., 2021), và code-davinci, cũng như các phương pháp chỉnh sửa mã như Self-edit (Zhang et al., 2023a), CodeRL (Wang et al., 2021; Le et al., 2022), và Self-repair (Olausson et al., 2023). Bảng 1a cho thấy CodeChain, khi được áp dụng với các LLM cơ sở như GPT3.5 và WizardCoder 15B, có thể đạt được những cải thiện hiệu suất đáng kể theo pass@k. Cụ thể, CodeChain

²https://codeforces.com/
³gpt-3.5-turbo-16k và gpt-4 trên https://platform.openai.com/docs/models/overview

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 2: So sánh với Self-repair: theo Olausson et al. (2023), chúng tôi báo cáo kết quả trên cùng một tập con 20 mẫu trên split test APPS sử dụng GPT3.5 và GPT4 làm mô hình cơ sở. Vui lòng tham khảo Bảng 5 để có danh sách đầy đủ của tập con test này.

Mô hình | Nguồn phản hồi | Introductory | Interview | Competition | Tất cả
Self-repair+GPT4 | GPT4 | 42.64 | 19.33 | 3.67 | 33.30
Self-repair+GPT4 | Con người | 62.21 | 45.67 | 14.67 | 52.60
GPT3.5 | - | 30.00 | 18.33 | 0.00 | 23.75
CodeChain+GPT3.5 | Sub-module | 31.67 | 27.86 | 0.00 | 26.35
GPT4 | - | 42.86 | 18.33 | 13.33 | 34.75
CodeChain+GPT4 | Sub-module | 71.07 | 55.00 | 23.33 | 61.50

Bảng 3: Kết quả validation APPS theo pass@1 (%): chúng tôi test CodeChain+GPT3.5 cho 1 vòng tự chỉnh sửa theo 3 khía cạnh: prompting, lọc bởi test công khai, và phương pháp sampling cho revision (R: ngẫu nhiên, C: centroid, P: toàn bộ chương trình, và M: sub-module).

CoT prompting | lọc bởi test công khai | Sampling cho revision | Introductory | Interview | Competition | Tất cả
- | - | - | 39.00 | 26.50 | 12.50 | 26.00
- | - | R-P | 12.40 | 2.00 | 0.61 | 5.00
- | - | C-P | 23.27 | 9.00 | 3.80 | 12.02
- | ✓ | C-P | 45.20 | 28.03 | 9.80 | 27.68
✓ | - | - | 33.50 | 23.70 | 10.10 | 22.43
✓ | - | R-P | 24.40 | 18.80 | 9.20 | 17.47
✓ | - | C-P | 31.33 | 23.70 | 10.10 | 21.71
✓ | ✓ | C-P | 45.50 | 33.17 | 11.80 | 30.16
✓ | ✓ | R-M | 49.30 | 36.90 | 12.40 | 32.87
✓ | ✓ | C-M | 52.00 | 38.83 | 14.50 | 35.11

có thể đạt được 10.50% pass@1 với WizardCoder làm mô hình cơ sở, và 30.24% pass@1 với OpenAI GPT3.5 làm mô hình cơ sở, thiết lập một kết quả SoTA mới trên APPS. Các công trình trước đây (Chen et al., 2021; Li et al., 2022) đã giới thiệu kết quả hiệu suất bổ sung bằng cách lọc ra các mẫu tạo ra thất bại test công khai và tính pass@k trên tập được lọc. Trong công trình này, chúng tôi tuân theo thiết lập được đề xuất bởi CodeT (Chen et al., 2023b) sử dụng việc lọc tiên tiến hơn với các test case synthetic (xem Phụ lục F cho prompt chúng tôi sử dụng để tạo ra test case). Bảng 1b cho thấy rằng khi được đánh giá trên các mẫu mã được lọc, CodeChain+GPT3.5 của chúng tôi có thể đạt được kết quả SoTA qua tất cả các cấp độ khó của bài toán với trung bình 35.34% pass@1.

Từ Bảng 1a, khi so sánh với các phương pháp liên quan như Self-edit và CodeRL, chúng tôi quan sát thấy những cải thiện hiệu suất tương đối đáng kể khi sử dụng CodeChain. Trong Bảng 2, theo Olausson et al. (2023), để so sánh với Self-repair, chúng tôi đánh giá phương pháp của chúng tôi trên cùng một tập con test của 20 mẫu (14/3/3 mẫu của cấp độ introductory/interview/competition), sử dụng cả GPT3.5 và GPT4 làm mô hình cơ sở. Chúng tôi quan sát thấy CodeChain có thể cải thiện hiệu suất với cả hai mô hình cơ sở, với những cải thiện đáng kể hơn khi sử dụng GPT4. Cụ thể, CodeChain+GPT4 có thể đạt được kết quả SoTA là 61.50% pass@1 trung bình, thậm chí vượt trội hơn Self-repair+GPT4 với phản hồi từ con người.

Phân tích về tự chỉnh sửa một vòng. Để hiểu lợi ích của CodeChain, chúng tôi tiến hành thí nghiệm với các biến thể khác nhau trên split validation của APPS. Bảng 3 trình bày kết quả về tự chỉnh sửa một vòng theo 3 khía cạnh chính: prompting, lọc bởi test công khai, và phương pháp sampling cho các chỉnh sửa có điều kiện. Đầu tiên, chúng tôi quan sát thấy rằng không có tự chỉnh sửa (tức là tạo ra trực tiếp), CoT prompting thực sự ảnh hưởng tiêu cực đến hiệu suất mô hình so với prompting bình thường. Quan sát này có thể do thực tế là các LLM pretrained không được thiết kế để tạo ra các giải pháp modular hoàn hảo (chúng được pretrain trên các mã Github công khai mà không lọc tính modularity). Tuy nhiên, sau khi áp dụng tự chỉnh sửa, chúng tôi quan sát thấy rằng phương pháp modular tốt hơn, đạt được những cải thiện hiệu suất tốt hơn so với các giải pháp không modular.

Thứ hai, chúng tôi thấy rằng chiến lược tốt nhất để chọn mã đại diện cho chỉnh sửa có điều kiện là thông qua phân cụm. Phương pháp này có thể giảm các điểm dữ liệu nhiễu và tạo ra một dạng phản hồi tốt hơn để cải thiện các mã được tạo ra. Cuối cùng, chúng tôi quan sát thấy rằng chỉ phân cụm thôi không đủ để chọn các mẫu đại diện tối ưu. Việc lọc bổ sung bởi test công khai là cần thiết để đầu tiên chuyển phân phối đầu ra đến các mẫu có khả năng đúng hơn trước khi phân cụm các đầu ra. Để tránh nhu cầu về test công khai

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 5: Kết quả validation APPS với chuỗi tự chỉnh sửa: chúng tôi test CodeChain+GPT3.5 cho 5 vòng tự chỉnh sửa và báo cáo pass@1 ở mỗi cấp độ khó của bài toán. Sử dụng GPT3.5 làm mô hình cơ sở, chúng tôi so sánh với các phương pháp liên quan, bao gồm Self-debug (với phản hồi unit test (UT) hoặc giải thích (expl)) (Chen et al., 2023c) và Reflexion (Shinn et al., 2023).

Hình 6: chúng tôi test CodeChain+GPT3.5 trên các thiết lập khác nhau về số lượng cụm và báo cáo những cải thiện pass@1 tương đối trung bình từ việc tạo ra trực tiếp (vòng 0).

test case, chúng tôi đề xuất khám phá các mô hình embedding tốt hơn có thể nhóm các mẫu đầu ra không chỉ theo semantic lập trình của chúng mà còn theo tính đúng đắn chức năng của chúng.

Phân tích về chuỗi tự chỉnh sửa. Để phân tích xu hướng hiệu suất mô hình qua một chuỗi tự chỉnh sửa, chúng tôi theo dõi tỷ lệ vượt qua của việc tạo ra trực tiếp và 5 vòng tự chỉnh sửa tiếp theo.

Hình 7: Kết quả pass@1 validation APPS của WizardCoder-1B đến 34B. Các đường chấm là kết quả tạo ra trực tiếp.

Hình 5 trình bày những cải thiện tương đối nhất quán ở tất cả các cấp độ khó của bài toán, với cải thiện hiệu suất tối ưu đạt được ở vòng chỉnh sửa 4 và sự sụt giảm hiệu suất nhẹ ở vòng 5. Một lý do có thể cho những sự sụt giảm hiệu suất này là các mẫu đầu ra đã chọn trở nên overfitting với tập nhỏ các test case công khai có sẵn, ảnh hưởng tiêu cực đến tỷ lệ vượt qua của các mã được chỉnh sửa tiếp theo trên một test-suite riêng tư ẩn rộng lớn hơn.

Thứ hai, chúng tôi cũng quan sát thấy rằng ở các cấp độ khó khác nhau của bài toán, CodeChain có tỷ lệ cải thiện hiệu suất khác nhau. Cụ thể, chúng tôi thấy rằng các bài toán thách thức hơn (tức là cấp độ competition và interview) có lợi hơn từ CodeChain so với các bài toán cơ bản (tức là cấp độ introductory). Các quan sát tương tự có thể thấy trên WizardCoder mã nguồn mở (Luo et al., 2023), với xu hướng hiệu suất rõ ràng hơn trên các kích thước mô hình 7B, 15B, và 34B (xem Hình 7).

Phân tích theo loại phản hồi. Trong Hình 5, chúng tôi cũng quan sát thấy CodeChain có thể đạt được hiệu suất tốt hơn các phương pháp tự chỉnh sửa liên quan khác sử dụng các loại phản hồi khác, như kết quả test với giải thích ngôn ngữ tự nhiên (Chen et al., 2023c) hoặc reflection (Shinn et al., 2023). Lưu ý rằng CodeChain có thể được bổ sung với các phương pháp tự chỉnh sửa khác như Self-debug bằng cách kết hợp các loại phản hồi khác nhau và chọn các sub-module đa dạng và đại diện hơn, ngay cả trên các mẫu tạo ra ban đầu thất bại test công khai.

Phân tích theo số lượng sub-module đại diện. Một siêu tham số của CodeChain là số lượng cụm trong mỗi vòng tự chỉnh sửa. Chúng tôi thí nghiệm với 4 lược đồ khác nhau: (i) số lượng cụm cố định qua tất cả các vòng thành K; (ii) số lượng cụm theo thứ tự giảm dần: {Ki} = {K, K-1, ..., 1}; (iii) số lượng cụm theo thứ tự tăng dần: {Ki} = {K, K+1, ...}; (iv) số lượng cụm động dựa trên hệ số silhouette (Rousseeuw, 1987). Chúng tôi chọn K = 5 cho tất cả các thí nghiệm. Từ Hình 6, chúng tôi quan sát thấy phương pháp tốt nhất để đặt số lượng cụm là tuân theo thứ tự giảm dần. Lược đồ này cung cấp cho các mô hình nhiều sub-module centroid đa dạng hơn ở đầu với số lượng cụm lớn hơn. Hướng tới các vòng chỉnh sửa tiếp theo, một số lượng cụm nhỏ hơn có lợi hơn vì các sub-module được lấy mẫu trở nên ngày càng gần nhau về mặt semantic theo thời gian. Chúng tôi thấy rằng lược đồ này gợi nhớ đến paradigm training mô hình di chuyển từ exploration đến exploitation, khi các mô hình trở nên tự tin hơn trong việc tạo ra của chúng.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Mô hình | Kích thước | Lọc | Val pass@1 | pass@5 | Test pass@1 | pass@5
code-davinci-002 | - | - | - | - | 1.00 | -
WizardCoder | 15B | - | 1.11 | 3.18 | 1.98 | 3.27
+ CodeChain | 15B | - | 2.35 | 3.29 | 2.48 | 3.30
GPT3.5 | - | - | 6.81 | 16.23 | 5.82 | 11.16
+ CodeChain | - | - | 12.86 | 16.91 | 10.27 | 14.11
code-davinci-002 | - | CodeT | - | - | 3.20 | -
GPT3.5 | - | CodeT | 17.30 | - | 11.34 | -
+CodeChain | - | CodeT | 17.91 | - | 13.75 | -

Hình 8: Kết quả CodeContests theo pass@1 (%): chúng tôi báo cáo kết quả của CodeChain sử dụng WizardCoder-15B và GPT3.5 làm mô hình cơ sở. Trái: kết quả test và validation. Phải: kết quả validation qua các vòng tự chỉnh sửa tuần tự. Các đường chấm là kết quả tạo ra trực tiếp.

Kết quả trên CodeContests. Hình 8 trình bày kết quả của CodeChain với WizardCoder-15B và GPT3.5 làm mô hình cơ sở. Chúng tôi quan sát thấy rằng trên cả pass@1 và pass@5, CodeChain có thể đạt được những cải thiện hiệu suất đáng kể so với việc tạo ra trực tiếp trên các mô hình cơ sở tương ứng. Áp dụng phương pháp lọc bổ sung (Chen et al., 2023b), CodeChain+GPT3.5 có thể đạt được kết quả SoTA là 13.75% pass@1 trên split test. Trái ngược với APPS nơi hiệu suất tối ưu đạt được ở vòng chỉnh sửa 4, từ kết quả validation này chúng tôi chú ý rằng hiệu suất tiếp tục cải thiện đến vòng chỉnh sửa cuối cùng. Khác với APPS, chúng tôi sử dụng các test case công khai chính thức có sẵn trong benchmark CodeContests.

Hình 9: Phân phối các mẫu đầu ra (%) theo chất lượng mã trong tập con test APPS. Chúng tôi có được điểm chất lượng bằng cách prompting GPT4 với các hướng dẫn đánh giá cụ thể.

Những test case này nói chung đa dạng hơn những cái chúng tôi trích xuất thủ công trong APPS, và do đó, làm cho các mã được chỉnh sửa ít overfitting hơn ngay cả trong vòng chỉnh sửa thứ 5.

Kết quả định tính. Để hiểu về modularity và khả năng tái sử dụng của việc tạo ra CodeChain, chúng tôi tiến hành thí nghiệm để đánh giá những chất lượng này trên các chương trình được tạo ra được lấy mẫu ngẫu nhiên. Cụ thể, chúng tôi prompting GPT4 với các hướng dẫn để đánh giá các mẫu đầu ra theo thang điểm Likert từ 0 đến 5 trong đó 5 là điểm cao nhất cho các chương trình tối ưu modular/có thể tái sử dụng. Vui lòng tham khảo Phụ lục F để có phiên bản đầy đủ của prompt. Trong thí nghiệm này, chúng tôi tái sử dụng các mẫu được tạo ra bởi GPT3.5 cho tập 20 tác vụ test ngẫu nhiên từ Bảng 2. Hình 9 cho thấy phân phối các mẫu đầu ra theo điểm Likert trong mỗi chất lượng. Chúng tôi quan sát thấy rằng khi sử dụng CodeChain, GPT3.5 có khả năng cao hơn để tạo ra các chương trình với mức độ modularity và khả năng tái sử dụng cao, với đa số đầu ra được đánh giá 3 đến 5 trên thang Likert. Điều này cao hơn đáng kể so với phương pháp tạo ra trực tiếp thông thường, với khoảng 80% thời gian tạo ra mã không modular hoặc không thể tái sử dụng (tức là điểm 0). Để có kết quả thí nghiệm bổ sung và ví dụ định tính về CodeChain, vui lòng tham khảo Phụ lục D và E.

5 KẾT LUẬN

Chúng tôi trình bày CodeChain, một framework inference mới để cải thiện việc tạo mã thông qua một chuỗi tự chỉnh sửa và sampling các sub-module đại diện. Trong CodeChain, chúng tôi giới thiệu chain-of-thought prompting để tạo ra các chương trình modular hơn, tạo ra các ranh giới tự nhiên cho các mô hình để lấy mẫu các phần của giải pháp để tái sử dụng và chỉnh sửa. Trong mỗi bước chỉnh sửa, chúng tôi lặp đi lặp lại giữa việc chọn các sub-module đại diện và bổ sung chain-of-thought prompting với những module-implementation đã chọn này. Các thí nghiệm của chúng tôi chỉ ra sự cải thiện hiệu suất đáng kể của CodeChain khi sử dụng OpenAI GPT hoặc WizardCoder mã nguồn mở làm mô hình cơ sở, đạt được kết quả SoTA mới trên benchmark APPS và CodeContests. Chúng tôi cung cấp các nghiên cứu ablation toàn diện để hiểu các yếu tố đóng góp đằng sau kết quả xuất sắc của CodeChain.

--- TRANG 9 ---
[Tiếp tục với các trang còn lại theo cùng cách thức...]
