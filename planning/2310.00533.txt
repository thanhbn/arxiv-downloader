# 2310.00533.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/planning/2310.00533.pdf
# File size: 995074 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint. Work in progress.
SELF: S ELF-EVOLUTION WITH LANGUAGE FEED-
BACK
Jianqiao Lu1∗†, Wanjun Zhong2∗, Wenyong Huang2∗,
Yufei Wang2, Qi Zhu2, Fei Mi2, Baojun Wang2, Weichao Wang2, Xingshan Zeng2,
Lifeng Shang2, Xin Jiang2&Qun Liu2
1The University of Hong Kong2Huawei Noah’s Ark Lab
jqlu@cs.hku.hk, {zhongwanjun1,wenyong.huang }@huawei.com
ABSTRACT
Large Language Models (LLMs) have shown impressive adaptability in various
fields, yet the optimal pathway of autonomous model evolution remains under-
explored. Drawing inspiration from the self-driven learning process of humans,
we introduce SELF (Self-Evolution with Language Feedback), a novel learning
framework that empowers LLMs to continually self-improve their abilities. SELF
initiates with a meta-skill learning process that equips the LLMs with capabili-
ties for self-feedback and self-refinement. SELF employs language-based feed-
back for detailed and nuanced evaluations, pinpointing response flaws and sug-
gesting refinements. Subsequently, the model engages in an iterative process of
self-evolution: they autonomously generate responses to unlabeled instructions,
refine these responses interactively, and use the refined and filtered data for itera-
tive self-training, thereby progressively boosting their capabilities. Moreover, the
SELF framework equips the model with the ability to self-refine during inference,
leading to further improved response quality. Our experiments on mathematical
and general tasks demonstrate that SELF enables the model to continually self-
improve without human intervention. The SELF framework indicates a promising
direction for the autonomous evolution of LLMs, transitioning them from passive
information receivers to active participants in their development.
1 I NTRODUCTION
Large Language Models (LLMs), like ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) , stand
at the forefront of the AI revolution, demonstrating versatility across tasks. Despite their evident
capabilities, the way towards achieving autonomous development of LLMs is still under-explored.
The development of automatically improved LLMs can draw inspiration from human self-driven
learning mechanisms. When facing new challenges, humans naturally engage in a learning cycle of
initial attempts, introspective feedback, and behavior refinement. This leads to a critical question:
“Can LLMs mimic the human learning process, utilizing self-refinement to enhance their inher-
ent capabilities?” Fascinatingly, a recent study (Ye et al., 2023) in top-tier LLMs such as GPT-4 has
revealed emergent meta-skills for self-refinement, signaling a promising future direction for the self-
evolution of LLMs. Despite this, current methods for LLM development typically rely on a single
round of instruction fine-tuning (Wei et al., 2021; Zhou et al., 2023) with meticulously human-
crafted datasets and reinforcement learning-based methods (Ouyang et al., 2022) that depend on an
external reward model. These strategies not only require extensive resources and ongoing human
intervention but also treat LLMs as mere passive repositories of information rather than active learn-
ers. These limitations hinder LLMs from tapping into their inherent capabilities, obstructing their
progress toward a self-driven, autonomous learning paradigm.
Thus, we introduce SELF (Self-Evolution with Language Feedback) framework, designed to un-
lock the potential for autonomous self-evolution in LLMs. Figure 1 depicts how SELF mim-
ics human-like self-driven learning, emphasizing progressive improvement of model capability
∗Leading co-authors with equal contribution.
†Work done during an internship at Huawei.
1arXiv:2310.00533v4  [cs.CL]  1 Feb 2024

--- PAGE 2 ---
Preprint. Work in progress.
LLM with Meta-SkillsInitialLLM 
Self-Evolving LLM Meta-Skill Learning
Self-Refinement1st Self-EvolveMeta-Skill Learning2nd Self-Evolve3rd  Self-Evolve
Direct GenerationSelf-Refinement
Figure 1: Evolutionary Journey of SELF: An initial LLM undergoes successive self-evolution itera-
tions (1st, 2nd, 3rd), enhancing its capabilities and acquiring a self-refinement meta-skill.
with self-evolution training. At the core of SELF are the two meta-skills ( self-feedback and self-
refinement ), empowering the model to progressively self-evolve by training on its own synthesized
data. Additionally, SELF leverages self-generated natural language feedback to offer in-depth anal-
ysis and guidance for refining responses, without the need for external rewards or direct human
guidance.
Specifically, the SELF framework initiates by teaching LLMs essential meta-skills, namely self-
feedback and self-refinement, using a limited set of examples. Once these skills are acquired,
the model engages in a cycle of continuous self-evolution, iteratively training with extensive, self-
generated data. Given a large-scale unlabeled corpus, this data is compiled from initial responses
and refined through self-refinement and filtering, with model itself. During this iterative process,
the quality of self-evolution training data and model capability are interactively improved, fostering
ongoing self-evolution of LLMs. Crucially, in the inference phase, these learned meta-skills enable
LLMs to further enhance response quality via self-refinement. In summary, the SELF framework
transforms LLMs from passive recipients of data into active learners in self-evolution, and alleviates
data scarcity issues by generating large-scale self-curated training datasets. This not only reduces
the need for labor-intensive manual intervention but also promotes the continuous self-improvement
of LLMs, establishing a more autonomous and efficient training approach.
We evaluate SELF in mathematical and general domains. SELF notably improves the test accuracy
on mathematical domains (6.82% on GSM8k (Cobbe et al., 2021) and 4.9% on SV AMP (Patel
et al., 2021)), and increases the win rate on general domain (10% on Vicuna testset (Lianmin et al.,
2023) and 6.9% on Evol-Instruct testset (Xu et al., 2023)), compared with typical supervised fine-
tuning. There are several insights gained from our experiments. Firstly, SELF can progressively
enhance the model capability through self-evolution training. Secondly, the learning of meta-skills,
specifically self-feedback and self-refinement, is crucial not only for equipping the model with self-
improvement abilities but also for boosting its direct response generation performance. Finally,
the model demonstrates further improvement in its responses through self-refinement during the
inference stage.
The main contributions are summarized as follows: (1) SELF empowers LLMs with self-evolving
capabilities, allowing for autonomous model evolution, and reducing human intervention. (2) SELF
facilitates self-refinement into smaller LLMs, even with challenging math problems. The capability
of self-refinement was previously considered an emergent characteristic of top-tier larger LLMs.
(3) Experiments demonstrate the effectiveness of SELF in both mathematical and general domains,
confirming its advanced capabilities in self-evolution and self-refinement.
2

--- PAGE 3 ---
Preprint. Work in progress.
2 R ELATED WORKS
Self-improvement in Inference Self-consistency (Wang et al., 2022a) is a straightforward and
effective method to improve LLMs for reasoning tasks. After sampling a variety of reasoning paths,
the most consistent answer is selected. During decoding, self-consistency is closely tied to the
self-refinement capability of LLMs, on which our method is based. Unlike self-consistency, self-
refinement applies to a broader range of tasks, going beyond reasoning tasks with unique correct
answers. Various research efforts have been undertaken to enhance the output quality of LLMs
through online self-improvement (Shinn et al., 2023; Madaan et al., 2023; Ye et al., 2023; Chen et al.,
2023; Ling et al., 2023). The main idea is to generate an initial output with an LLM. Then, the same
LLM provides feedback on its output and employs this feedback to refine its initial output. This
process can be iterative until the response quality is satisfied. While simple and effective, online
self-improvement necessitates multi-turn inference for refinement, leading to increased inference
computational overhead. Most importantly, online self-improvement does not prevent the model
from repeating previously encountered errors, as the model’s parameters remain unchanged. In
contrast, SELF can self-improve with evolution training.
Autonomous Improvements of LLMs “Alignment” (Leike et al., 2018) aims to train agents to
act in line with human intentions. Several research efforts (Ouyang et al., 2022; Bai et al., 2022;
Scheurer et al., 2023) leverage Reinforcement Learning from Human Feedback (RLHF) (Christiano
et al., 2017). RLHF begins with fitting a reward model to approximate human preferences. Sub-
sequently, an LLM is finetuned through reinforcement learning to maximize the estimated human
preference of the reward model. Reward Ranked Fine-tuning (RAFT) utilizes a reward model to
rank responses sampled from an LLM. Subsequently, it fine-tunes the LLM using highly-ranked re-
sponses (Dong et al., 2023). Recent advancements in LLMs have explored Reinforcement Learning
(RL) approaches that do not rely on human feedback. RLAIF (Pang et al., 2023) proposes to employ
LLMs to label the preference data in replace of human supervision. LLMs are updated progressively
through online RL in interacting with the environment in Carta et al. (2023). The connection be-
tween conventional RL research and RLHF in LLMs is discussed by Sun (2023). However, scalar
rewards in Reinforcement Learning (RL) offer limited insights for evaluating complex reasoning
tasks (Lightman et al., 2023), as they fail to specify detailed errors and optimization paths. Recog-
nizing this limitation, the SELF framework suggests utilizing natural language feedback, which ef-
fectively guides the self-evolution of LLMs. Unlike scalar rewards, which require a retrained model
for each evaluation protocol and dimension, natural language feedback is more flexible, addressing
multiple aspects simultaneously. Furthermore, the RLHF process is intricate and computationally
intensive, relies on external reward models, and demands manual tuning of hyperparameters for
optimal performance. This approach lacks the adaptability to evolve intrinsically with the model
itself.
Self-EvolutionTraining at Itert
(a) Meta-Skill Learning(b) Iterative Self-Evolution TrainingFeedback & Refinement Corpus[Feedback]: The response directly adds the $450 to Dwayne's salary of $1,500 to get a total of $2,050.. This is incorrect because it doesn't account for  the fact that Brady makes more than DwayneJudgement: incorrect[Refinement]: Brady makes $450 more than Dwayne, so Brady makes $1,500 + $450 = $1,950 in a year. Therefore, the total amount that Brady and Dwayne make in a year is $1,950 + $1,500 = $3,450.Question & Initial Response[Question]: Brady will make $450 more in a year than Dwayne. If Dwayne makes $1,500 in a year, how much will Brady and Dwayne make combined in a year? [Initial Answer]: The total amount that Brady and Dwayne will make combined in a year is $450 + $1,500 = $<<450+1500=2050>>2050.Therefore, the combined salary of Brady and Dwayne in a year is $2,050.
Self-Evolving LLM Learning Meta-Skills of Self-feedback and Self-Refinement
LLM with Meta-Skills (𝑴𝒔𝒆𝒍𝒇𝟎)
InitialLLM Starting from𝑴𝒔𝒆𝒍𝒇𝟎, the modelUndergoes self-evolutionMeta-Skill TrainingUnlabeled PromptsIter t
𝑴𝒔𝒆𝒍𝒇𝒕'𝟏Initial Responsefrom𝑴𝒔𝒆𝒍𝒇𝒕'𝟏
𝑴𝒔𝒆𝒍𝒇𝒕'𝟏Self-Refined Response from 𝑴𝒔𝒆𝒍𝒇𝒕'𝟏
Filtering via Self-FeedbackSelf-Evolution Training dataIter t
Figure 2: Illustration of SELF. The “Meta-Skill Learning” (left) phase empowers the LLM to ac-
quire meta-skills in self-feedback and self-refinement. The (b)“Self-Evolution” phase (right) utilizes
meta-skills for self-evolution training with self-curated data, enabling continuous model enhance-
ment.
3

--- PAGE 4 ---
Preprint. Work in progress.
3 M ETHOD
As depicted in Fig. 2, the SELF framework enhances model capabilities through a two-stage learn-
ing phase: (1) Meta-skill Learning Phase : This phase uses an annotated meta-skill training cor-
pus to fine-tune the model, and equips the model with essential meta-skills for self-feedback and
self-refinement with limited supervised examples, laying a foundation for self-evolution. (2) Self-
Evolution Phase : With the acquired meta-skills, the model progressively improves through mul-
tiple iterations of the self-evolution training process. Each iteration begins with the model itself
autonomously creating high-quality training data from unlabeled prompts. Then, the model is fine-
tuned using this data. The process is further illustrated in Alg. 1 in Appendix A.7.
In SELF, Natural Language Feedback plays a crucial role in guiding the evolutionary process.
This approach offers a more fine-grained and informative evaluation compared to the traditional
method of using a scalar reward. The latter evaluates quality along a single dimension with a nu-
merical value from a reward model. In contrast, natural language feedback provides a detailed and
descriptive analysis of the processes involved in a response, which is particularly critical in complex
reasoning tasks. This also allows for evaluation across multiple dimensions, offering greater flexi-
bility. Importantly, it guides the refinement process by suggesting directions for improvement. The
efficacy of natural language feedback in enhancing evaluation accuracy and model performance is
shown in § 4.3.2.
3.1 M ETA-SKILL LEARNING
Meta-skill learning targets on instill two essential meta-skills into LLMs for self-evolution. (1)
Self-Feedback Ability : This skill enables LLMs to evaluate their responses using natural language
feedback. This provides the suggestion for further refinement, thus laying a solid foundation for sub-
sequent self-refinement. Self-feedback also enables the model to filter out low-quality self-evolution
training data if a response is judged as unqualified by the model (§ 3.2.1). (2) Self-Refinement Abil-
ity: Self-refinement enables the model to optimize its responses based on self-feedback. This ability
has two applications: (1) enhancing the quality of the self-evolution training corpus (§ 3.2.1) and (2)
improving model performance by refining the models’ outputs during inference (§ 3.3).
These meta-skills are acquired by fine-tuning the model using the Meta-Skill Training Corpus
(§ 3.1.1) with designed training objective (§ 3.1.2). The resulting model is denoted as Mmeta.
3.1.1 M ETA-SKILL TRAINING CORPUS
The meta-skill training corpus Dmetarepresents the generation, feedback, and refinement process.
It is constructed as follows: (1) For each unlabeled prompt p, the initial model Minitgenerates an
initial response r. (2) An annotator Lprovides evaluation feedback ffor the initial response r, then
produces a refined answer ˆraccording to the feedback f. Each instance in Dmetatakes the form
(p, r, f, ˆr), representing the process of response evaluation and refinement. An example instance of
Dmetais provided in appendix A.6.
3.1.2 T RAINING OBJECTIVE
In the meta-skill learning phase, the objective is to empower the initial model Minitto develop
meta-skills, resulting in an enhanced model Mmeta. This process is guided by the cross-entropy loss
following the maximum likelihood estimation (MLE) paradigm:
Lmeta(ϕ) =−E(p,r,f, ˆr)∼Dmeta
logτϕ(f|p, r) + log τϕ(ˆr|p, r, f ) +βlogτϕ(ˆr|p)
, (1)
where pis prompt, ris the initial model response, fis the feedback to the model response r, and ˆris
the revised response based on feedback f.τϕ(y|x)denotes the probability distribution given by the
auto-regressive language model with parameters ϕpredicting the response ygiven the input prompt
x. The coefficient βin eq. (1) controls a balanced emphasis on direct response generation and the
model’s capability for self-feedback and self-refinement.
Insight. Training with Dmetaaims to achieve two goals: (i) To guide the model in generating
feedback ( f) concerning its initial responses ( r) (self-feedback ) and subsequently employing this
4

--- PAGE 5 ---
Preprint. Work in progress.
feedback to enhance the quality of the final answer ( ˆr) (self-refinement ). (ii) Training with Dmeta
instructs the model to process problems in a Chain-of-Thought (CoT) manner. This involves evalu-
ating the initial response, integrating the feedback, and then revising the response in a chain process
Ψ(ˆr|p) :=P
r,fτϕ(r|p)·τϕ(f|p, r)·τϕ(ˆr|p, r, f ).
3.2 S ELF-EVOLUTION TRAINING PROCESS
The model Mmeta, equipped with meta-skills, undergoes progressive improvement through multiple
iterations of the self-evolution training process. Each iteration of the self-evolution process begins
with the model autonomously creating high-quality training data (§ 3.2.1) from an unlabeled corpus.
With an unlabeled dataset of prompts, the model generates initial responses and then refines them
through self-feedback and self-refinement. These refined responses, superior in quality, are further
filtered with self-feedback and utilized as the training data for the model’s subsequent self-evolution
training (§ 3.2.2). This autonomous self-evolving process interactively improves LLMs as the im-
proved model capability leads to better data quality, which in turn boosts model performance. It also
alleviates the data scarcity problem by self-generating data.
3.2.1 S ELF-EVOLUTION TRAINING DATA
LetMt
evoldenotes the model at tthiteration and initialize M0
evolfromMmeta. During tthself-evolution
iteration , Mt−1
evolprocesses each unlabeled prompt pby first generating an initial response r.ris
then refined using the model’s self-feedback f, resulting in a self-refined response ˆr. The prompts
and their corresponding self-refined responses (p,ˆr)are then incorporated into the tthround self-
evolution datasets, denoted as Dt
evol, for subsequent self-evolution processes.
Data Filtering with Self-feedback: To enhance the quality of Dt
evol, we employ the self-feedback
capability of Mt−1
evolto filter out data of lower quality. Mt−1
evolevaluates the self-refined data, ˆrevol,
keeping only the responses that meet high-quality standards. The effect is analyzed in § 4.6.
To mitigate the catastrophic forgetting issue of meta-skill, the meta-skill learning data Dmetaare
also included in self-evolution training. At tthiteration, the model undergoes self-evolution training
with the updated self-curated data Dt
evol, improving its performance and aligning it more closely
with human values.
3.2.2 M ATHEMATICAL MODELING
Main Objective. We denote τt
ϕas the probability distribution generated by Mt
evolwith parameters
ϕ. The self-evolution training loss Lt
evol(ϕ)is defined as:
Lt
evol(ϕ) =−EpevolEˆrevol∼Ψt−1(ˆrevol|pevol)
logτt
ϕ(ˆrevol|pevol)
=−Epevol"X
ˆrevolΨt−1(ˆrevol|pevol) logτt
ϕ(ˆrevol|pevol)#
,(2)
where pevolis sampled from unlabeled prompts corpus (detiled in appendix A.3.2) for self-evolution
tthround. The joint probability distribution is:
Ψt−1(ˆrevol|pevol) =
X
revol,fevol
τt−1
ϕ(revol|pevol)·τt−1
ϕ(fevol|revol, pevol)·τt−1
ϕ(ˆrevol|fevol.revol, pevol)
.(3)
The rationale behind learning from Ψt−1(ˆrevol|pevol)is discussed in appendix A.1.1.
5

--- PAGE 6 ---
Preprint. Work in progress.
Optimizing eq. (2) is equivalent to minimizing the Kullback-Leibler (KL) divergence:
KL(Ψt−1(ˆrevol|pevol)||τt
ϕ(ˆrevol|pevol))
=X
ˆrevolΨt−1(ˆrevol|pevol) logΨt−1(ˆrevol|pevol)
τt
ϕ(ˆrevol|pevol)
=−H(Ψt−1(ˆrevol|pevol))| {z }
constant for fixed Ψt−1−
X
ˆrevolΨt−1(ˆrevol|pevol) logτt
ϕ(ˆrevol|pevol
| {z }
eq.(2)).(4)
The optimization of KL divergence is to fine-tune the model parameters ϕto ensure that the model’s
predictive probability distribution τt
ϕaligns with the joint probability of the preceding iteration’s
chain process ( Ψt−1). The goal is to enhance the model’s ability to directly produce refined re-
sponses ( ˆrevol) in the tthiteration, effectively condensing the intricate process of generation, feed-
back, and refinement from the (t−1)thiteration. This advancement demonstrates the model’s
evolving capability to streamline the complex steps into a more straightforward inference. The po-
tential plateau is discussed in appendix A.1.3.
Further Analysis. Assuming that each self-evolution round is effective, implying that as tincreases,
the quality of responses sampled from Ψtimproves, optimizing the KL divergence as described
in eq. (4) is fundamentally a process aimed at enhancing the direct generation of high-quality re-
sponses. Before delving deeper, it is crucial to introduce several key concepts. We define a binary
variable Xto evaluate the quality of responses. A higher probability, p(X= 1|pevol,ˆrevol), in-
dicates a higher quality of the response ˆrevolin relation to the prompt pevol. For the self-evolving
model with parameters ϕat the tthiteration, the model’s log-likelihood of producing high-quality
responses to a specified prompt is defined as follows:
logpt(X= 1|pevol) := logX
ˆrp(X= 1|pevol,ˆrevol)τt
ϕ(ˆrevol|pevol).
By minimizing the KL divergence in eq. (4), we can increase logpt(X= 1|pevol)by progressively
improving its Evidence Lower Bound (ELBO):
logpt(X= 1|pevol)
= logX
ˆrevolp(X= 1|pevol,ˆrevol)τt
ϕ(ˆrevol|pevol).
= log EΨt−1(ˆrevol|pevol)"
p(X= 1|pevol,ˆrevol)τt
ϕ(ˆrevol|pevol)
Ψt−1(ˆrevol|pevol)#
≥EΨt−1(ˆrevol|pevol)"
logp(X= 1|pevol,ˆrevol)τt
ϕ(ˆrevol|pevol)
Ψt−1(ˆrevol|pevol)#
=EΨt−1(ˆrevol|pevol)[logp(X= 1|pevol,ˆrevol)]
−KL(Ψt−1(ˆrevol|pevol)||τt
ϕ(ˆrevol|pevol))
| {z }
eq.(4).
The entire self-evolution training process can be viewed as a continuous exploration of inher-
ent model capabilities in generation, self-feedback, and self-refinement, ultimately enhancing the
model’s ability to generate high-quality responses directly.
Overall Objective. In the iterative self-evolution process, meta-skills, i.e., the ability to self-
feedback and self-refinement, is crucial for guiding the evolution process. We incorporate Dmeta
into self-evolution training to mitigate the potential catastrophic forgetting of meta-skills:
Lt
meta(ϕ) =−E(p,r,f, ˆr)∼Dmeta
logτt
ϕ(f|p, r) + log τt
ϕ(ˆr|p, r, f )
.
6

--- PAGE 7 ---
Preprint. Work in progress.
The total objective for the tthround of self-evolution is:
Lt
self(ϕ) =Lt
evol(ϕ) +Lt
meta(ϕ).
3.3 R ESPONSE REFINEMENT DURING INFERENCE
Equipped with the meta-skills for self-feedback and self-refinement, the model can conduct self-
refinement during inference. Specifically, the model generates an initial response and then refines it
using self-refinement, akin to the method described in § 3.1. Response refinement during inference
consistently improves the model’s performance as shown in § 4.3.
4 E XPERIMENT SETTINGS
4.1 E VALUATION
Inference Setting. We adopt two inference settings: (1) Direct Response (default): the model
directly answers the question with a Zero-shot Chain of Thought (CoT) methodology (Kojima et al.,
2022), which is the default setting to evaluate the model capability directly; (2) Self-Refinement :
during inference, the model self-refines its original answer for once, as described in § 3.3.
Benchmarks. We evaluate on two mathematical benchmarks to show the efficacy of SELF on com-
plex reasoning tasks, and further verify the generalizability of SELF on two general benchmarks.
GSM8K (Cobbe et al., 2021) contains high-quality, linguistically diverse grade school math word
problems crafted by expert human writers, which incorporates approximately 7.5K training prob-
lems and 1K test problems. The performance is measured by accuracy (%). SV AMP (Patel et al.,
2021) is a challenge set for elementary Math Word Problems (MWP). It is composed of 1K test sam-
ples. The evaluation metric is accuracy (%). Vicuna testset (Lianmin et al., 2023) is a benchmark
for assessing instruction-following models, containing 80 examples across nine skills in mathemat-
ics, reasoning, and coding. Evol-Instruct testset (Xu et al., 2023) includes 218 real-world human
instructions from various sources, offering greater size and complexity than the Vicuna testset.
4.2 S ETUP AND BASELINES
The complete SELF framework includes meta-skill training with Dmeta, three iterations of self-
evolution training, and optional self-refinement during inference. Our evaluation primarily focuses
on assessing how self-evolution training can progressively enhance the capabilities of LLMs. For
building the meta-skill training corpus Dmeta, we employ GPT-4 as the language model labeler L
due to its proven proficiency in refining responses (An et al., 2023) via the prompt described in ap-
pendix A.21. The data statistic of Dmetais shown in appendix A.3.1 and further details of unlabeled
corpus construction is described in appendix A.3.2. We note that all model training utilized the same
training hyperparameters, as shown in appendix A.4.
We note that the SELF framework is compatible with versatile LLMs. In this study, we perform
the experiment with Vicuna-7b (Chiang et al., 2023), a capable open-source instruction-following
model fine-tuned from LLaMA-7b (Touvron et al., 2023), will be referred to simply as “Vicuna”
in subsequent sections. To verify the generalizability of SELF, we also experiment with OpenL-
LaMA Geng & Liu (2023) and Vicuna-1.5 (Chiang et al., 2023) in appendix A.12. All the compared
baselines are outlined:
(1) Vicuna + DQA:To demonstrate the improvement brought by SELF and exclude the impact of
standard domain-specific supervised fine-tuning (SFT), we set a direct baseline that trained solely
on pseudo-labeled question-answer pairs in the meta-skill training corpus. Specifically, we construct
DQA, which includes all the ( p,ˆr) pairs from Dmeta, and fine-tune the model as:
LQA(ϕ) =−E(p,ˆr)∼DQA[logτϕ(ˆr|p)].
We refer to this approach as Vicuna + DQA, the most straightforward baseline. The performance gap
between Vicuna + DQAand SELF verify the efficacy of the proposed SELF framework, excluding
the effect of training on domain-specific QA data.
1Separate prompts have been designed for the math domain appendix A.2.1 and general domain ap-
pendix A.2.2.
7

--- PAGE 8 ---
Preprint. Work in progress.
(2) RLHF: we utilize the RLHF implementation from trlx2. We apply the same SFT model as
the policy model for RLHF, Vicuna + DQAas described above, which is consistent with SELF.
The reward model is initialized from Vicuna-7b and is fine-tuned using pair-wise comparison data
derived from the meta-skill training corpus Dmeta(§ 3.1.1), where the refined response ˆris presumed
to be better than the original one r.
(3) Self-Consistency: we compare the self-refinement inference strategy in SELF with the Self-
Consistency (Wang et al., 2022a) (i.e., multiple sampling and selecting an answer with majority
voting) and examine their combined efficacy.
4.3 M AINRESULT
4.3.1 M ATH TEST
Table 1: Experiment results on GSM8K and SV AMP compare SELF with other baseline methods.
We evaluate the impact of Self-Evolution (SE), Self-Consistency (SC), and Self-Refinement (SR)
strategies on model performance.
Model SE SC SR GSM8K(%) SV AMP(%)
Vicuna16.43 36.40
✓ 19.56 40.20
✓ 15.63 36.80
Vicuna + DQA24.49 44.90
✓ 25.70 46.00
✓ 24.44 45.30
Vicuna + SELF (Ours)✓ 29.64 49.40
✓ ✓ 29.87 50.20
✓ ✓ 31.31 49.80
✓ ✓ ✓ 32.22 51.20
In table 1, we compare SELF against baseline models, as detailed in § 4.2. This comparison eluci-
dates SELF’s effectiveness in enhancing LLM performance through self-evolution and offers several
key insights:
(1) Self-Evolution Enhances LLM: Vicuna + SELF significantly outperforms its baseline Vicuna
+DQA(24.49%+5.15%− − − − − → 29.64% on GSM8K and 44.90%+4.5%− − − − → 49.40% on SV AMP) in direct
response setting, showcasing self-evolution is effective in optimizing LLMs.
(2) SELF Instills Self-Refine Capability in LLMs: The integration of self-refinement inference
strategy with Vicuna + SELF further boosts performance ( 29.64%+1.67%− − − − − → 31.31%), while baseline
models show marginal or negative effect via self-refinement. We also provide a case analysis for
the limited self-refinement ability of baseline models, as shown in fig. 4. This indicates that SELF
can instill advanced self-refinement capabilities into small LLMs like Vicuna (7B), although self-
refinement was previously shown as an exclusive ability of large LLMs (Ye et al., 2023) like GPT-4.
(3) SELF can work with Self-Consistency: SELF works effectively with self-consistency, improv-
ing accuracy across models. The base Vicuna model, which may have uncertainties in its outputs,
shows notable improvement with self-consistency, achieving a +3.13% increase. As the model pro-
gresses through self-evolution training and becomes more certain of generating correct math an-
swers, the benefit from self-consistency reduces. Combining self-refinement with self-consistency
further elevates performance (e.g., 29.64%+2.58%− − − − − → 32.22% on GSM8K), indicating that these two
strategies can complement each other effectively.
(4) Pseudo-Labeled DQAEnhances Performance: The inclusion of pseudo-labeled QA data DQA
enhances Vicuna’s performance, suggesting that tuning with domain-specific QA data can enhance
task-specific problem-solving.
2https://github.com/CarperAI/trlx
8

--- PAGE 9 ---
Preprint. Work in progress.
4.3.2 C OMPARISON WITH RLHF
Table 2: Comparison of SELF and RLHF on GSM8K. “Feedback Acc.” measures how accurately
feedback identifies correct and incorrect answers, while “GSM8K Acc.” shows the model perfor-
mance on GSM8K testset.
Method Feedback Acc.(%) GSM8K Acc.(%)
Vicuna + DQA - 24.49
RLHF 24 25.55
SELF 72 27.67
In table 2, we compare the performance of SELF with RLHF. To alleviate the effect led by different
amounts of training data and make a fair comparison, for SELF, we only adopt data solely from the
initial round of self-evolution training. This ensures the same training data quantity with RLHF and
leads to sub-optimal results compared with the one in table 2. As table 2 shows, RLHF achieves a
25.55% accuracy on GSM8K, which is lower than the 27.67% performed by SELF. We observe that
the simple scalar reward of RLHF often fails to identify the correctness of the reasoning process,
which limits performance improvements. On the GSM8K test set, for incorrect answers produced
by the SFT model (Vicuna + DQA), the reward model only identifies 24% of them as incorrect,
i.e., the reward model assigns lower scalar rewards to incorrect answers compared to correct an-
swers. In contrast, SELF utilizes informative natural language feedback to provide a more accurate
assessment. It correctly identifies 72% of incorrect answers.
4.4 G ENERAL TEST
We test the efficacy and generalizability of SELF on general domain benchmarks, explicitly using
the Vicuna and Evol-Instruct test sets. Three configurations of the Vicuna model are evaluated:
Vicuna, Vicuna + DQA, and Vicuna + SELF. We utilize GPT-4 to evaluate the models’ responses
on both test sets. We follow the assessment methodology proposed by (Xu et al., 2023), which
mitigated the order bias presented in the evaluation procedures.
The results are depicted in Figure 3. In the figure, blue represents the number of test cases where the
model being evaluated is preferred over the baseline model (Vicuna), as assessed by GPT-4. Yellow
denotes test cases where both models perform equally, and pink indicates the number of test cases
where the baseline model is favored over the model being evaluated.
Vicuna LostTieVicuna WonVicuna + 𝐷!" + SELF(Self-Refinement)Vicuna + 𝐷!"Vicuna + 𝐷!" + SELF(Direct Generation)
(a) Results on Vicuna testset.
Vicuna LostTieVicuna WonVicuna + 𝐷!" + SELF(Self-Refinement)Vicuna + 𝐷!"Vicuna + 𝐷!" + SELF(Direct Generation) (b) Results on Evol-Instruct testset.
Figure 3: Results on Vicuna testset and Evol-Instruct testset
In the Vicuna testset, SELF increases direct response win rate from 65.0% to 72.5% compared with
Vicuna + DQA. After self-refinement, the win rate is further improved to 75.0%. In the Evol-Instruct
testset, the win rate of Vicuna + DQAis 48.6%. SELF increases the win rate to approximately 52.8%.
Applying self-refinement during inference further improves the win rate to 55.5%.
These findings in the general domain highlight the SELF framework’s adaptability and robustness,
particularly when self-refinement is employed, showcasing its efficacy across varied test domains.
9

--- PAGE 10 ---
Preprint. Work in progress.
Table 3: Performance under various training settings of SELF. A checkmark ✓in a column denotes
the additive adoption of the corresponding setting in that training scenario. We present two kinds
of inference results: Direct Response (DR) and Self-Refinement (SR), the latter conducts self-
refinement to DR.
SV AMP (%) GSM8K (%)DQA DmetaSelf Evol.
DR SR DR SR 1st 2nd 3rd
36.4 36.8 16.43 15.63
44.9 45.3 24.49 24.44 ✓
46.8 47.0 25.39 28.28 ✓
47.8 48.0 27.67 29.34 ✓ ✓
48.9 49.0 28.66 29.87 ✓ ✓ ✓
49.4 50.2 29.64 31.31 ✓ ✓ ✓ ✓
4.5 A BLATION STUDY
We conduct ablation experiments on SV AMP and GSM8K datasets to assess the incremental effect
of each stage. While baseline models exhibit slight or even adverse effects via self-refinement,
the SELF framework endows LLMs with an inherent capability through meta-skill learning and
multi-iterations of self-evolution training. As depicted in table 3, our framework facilitates gradual
performance improvements through successive SELF stages. Observations are highlighted below:
(1) Meta-skill Training Elevates Performance: Training with the meta-skills dataset Dmetaas
defined in eq. (1), and setting β= 1 for a fair comparison with the question-answer dataset DQA,
improves direct response performance. Specifically, we observe an increase of +0.90% on the
GSM8K dataset and +1.9% on the SV AMP dataset, compared to using the DQAdataset alone. This
underscores the interesting finding that arming the model with self-feedback and self-refinement
meta-capability implicitly elevates its capacity to generate superior responses directly, even without
explicit self-refinement. We offer an insight in appendix A.1.2.
(2) Continuous Improvement through Self-Evolution: The results reveal that three self-evolution
rounds consecutively yield performance enhancements (e.g., 25.39%+2.28%− − − − − → 27.67%+0.99%− − − − − →
28.66%+0.98%− − − − − → 29.64% on GSM8K). This shows that the model actively self-evolves, refining its
performance autonomously without additional manual intervention.
(3) Persistent Efficacy of Self-Refinement: After meta-skill learning, regardless of model vari-
ation, executing self-refinement consistently results in notable performance improvements. This
shows that the self-refinement meta-capability learned by SELF is robust and consistent across evo-
lution steps.
4.6 A NALYSIS ON DATA FILTERING WITH SELF-FEEDBACK
Table 4: Impact of Data Filtering with Self-Feedback on GSM8K. “Training Acc.” shows the ac-
curacy of the self-evolution training data post-filtering, and “Test Acc.” represents the model’s test
performance post-training on these filtered data.
Filter Strategy Training Acc. (%) Test Acc. (%)
Unfiltered 29.89 26.90
Filtered 44.10 27.67
Table 4 presents an analysis of filtering self-evolution training data using self-feedback (§ 3.2.1) on
GSM8K, focusing on training data quality and its influence on self-evolution training. The filtering
criteria are detailed in appendix A.8.
10

--- PAGE 11 ---
Preprint. Work in progress.
The following insight is highlighted: The combination of self-refinement and self-feedback filtering
results in higher self-evolution training data accuracy (+14.21%) and improved fine-tuned model
performance (+0.77%). Despite the significant training data accuracy improvement, the performance
gain is modest due to the reduced data size (from 4K to 1.8K) after filtering.
5 C ONCLUSION
We present SELF (Self-Evolution with Language Feedback), a novel framework that enables LLMs
to achieve progressive self-evolution through self-feedback and self-refinement. Unlike conven-
tional methods, SELF transforms LLMs from passive information recipients to active participants in
their evolution. The adoption of natural language feedback promotes a more informative and fine-
grained evaluation. Through meta-skill learning, SELF equips LLMs with the capability for self-
feedback and self-refinement. This empowers the models to evolve their capabilities autonomously,
utilizing self-evolution training and online self-refinement. Experiments conducted on benchmarks
underscore SELF’s capacity to progressively enhance model capabilities while reducing the need for
human intervention. SELF represents a leading step in autonomous LLM development, leading to
an insight that models are capable of continual learning and self-evolution.
REFERENCES
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning
from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689 , 2023.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,
2022.
Thomas Carta, Cl ´ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves
Oudeyer. Grounding large language models in interactive environments with online reinforcement
learning. arXiv preprint arXiv:2302.02662 , 2023.
Xinyun Chen, Maxwell Lin, Nathanael Sch ¨arli, and Denny Zhou. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128 , 2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing sys-
tems, 30, 2017.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum,
and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment.
arXiv preprint arXiv:2304.06767 , 2023.
Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https:
//github.com/openlm-research/open_llama .
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. CoRR , abs/2205.11916, 2022. doi: 10.48550/ARXIV .
2205.11916. URL https://doi.org/10.48550/arXiv.2205.11916 .
11

--- PAGE 12 ---
Preprint. Work in progress.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.
MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technolo-
gies, pp. 1152–1157, San Diego, California, June 2016. Association for Computational Linguis-
tics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136 .
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable
agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871 ,
2018.
Zheng Lianmin, Chiang Wei-Lin, and Zhuang Siyuan (Ryans). Vicuna-blog-eval, 2023. https:
//github.com/lm-sys/vicuna-blog-eval .
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050 , 2023.
Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su.
Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872 , 2023.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement
with self-feedback. arXiv preprint arXiv:2303.17651 , 2023.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing
english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics , pp. 975–984, 2020.
OpenAI. Chatgpt, 2022. https://chat.openai.com/chat .
OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730–27744, 2022.
Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang,
and Yang Yu. Language model self-improvement by reinforcement learning contemplation. arXiv
preprint arXiv:2305.14483 , 2023.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies , pp. 2080–
2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.
naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168 .
J´er´emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun
Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint
arXiv:2303.16755 , 2023.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu
Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
Hao Sun. Reinforcement learning in the era of llms: What is essential? what is needed? an rl
perspective on rlhf, prompting, and beyond. arXiv preprint arXiv:2310.06147 , 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.
arXiv preprint arXiv:2203.11171 , 2022a.
12

--- PAGE 13 ---
Preprint. Work in progress.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.
arXiv preprint arXiv:2212.10560 , 2022b.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652 , 2021.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.
arXiv preprint arXiv:2304.12244 , 2023.
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo.
Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023.
URLhttps://kaistai.github.io/SelFee/ .
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,
Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.
A A PPENDIX
A.1 D ISCUSSION
A.1.1 W HYREFINEMENT IS BETTER
We discuss why it’s necessary to optimize τt
ϕ(ˆrevol|pevol)in the tthround self-evolution by learning
from Ψt−1(ˆrevol|pevol), and why we believe samples from Ψt−1(ˆrevol|pevol)are typically of higher
quality than those from τt−1
ϕ(revol|pevol)directly.
Firstly, similar to the insights analyzed in § 3.1.2, we believe that a process akin to CoT, involv-
ing feedback followed by refinement before providing an answer, helps in generating high-quality
responses. Secondly, revolis already a reasonably good output after meta-skill learning and previ-
ously ( t−1) rounds of self-evolution. We can assume that the self-feedback fevolis informative,
hence ˆrevol∼τt−1
ϕ(ˆrevol|pevol, revol, fevol)is of higher quality than revol∼τt−1
ϕ(revol|pevol)because
it incorporates useful feedback information. If fevolsuggests that the initial response revoldoes not
require refinement, we still proceed through the process of revising from revoltoˆrevolusing fevol, but
setˆrevol=revol. By doing so, we ensure that the quality of ˆrevolis at least as good as that of revol.
Moreover, as described in § 3.2.2, we utilize Data Filtering with Self-feedback . In other words,
we only keep ˆrevolevaluated as qualified , allowing us to emphasize high-quality outputs and further
improve τt
ϕ.
A.1.2 W HYINTEGRATION OF META-SKILL TRAINING DATADMETA LEVATES DIRECT QA
TheDmetadataset trains the model to not only modify answers but also to fully grasp a prompt,
create feedback, and then develop a revised answer. This approach resembles training the model
to think through a problem in a chain-of-thought methodically (CoT) manner, before responding.
The training encompasses a thorough examination of the entire process, which not only betters
the model’s direct response capability but also enriches its understanding of the logic behind those
answers, thereby enhancing its generalization ability.
A.1.3 P OTENTIALLY LIMITED PLATEAU OF SELF-EVOLUTION TRAINING
Based on eq. (2) and eq. (3), the model in the tthround is updated to improve direct response
quality by incorporating the generate-feedback-refinement process from the (t−1)thround. This is
based on the assumption that the refined response is superior to the initial one generated by Mt−1
evol.
As illustrated in Fig. 1, the direct generation performance of Mt
evol(green curve) consistently falls
below the self-refinement of Mt−1
evol(blue curve). The self-refinement gains in the (t−1)thround
indicate the potential benefit that the tthround self-evolution could bring to direct generation. This
13

--- PAGE 14 ---
Preprint. Work in progress.
also helps determine when to halt the self-evolution process, i.e., the process can be stopped when
self-refinement brings no benefit to the direct response.
A.2 P ROMPT OF GENERATING FEEDBACK AND REFINEMENT FOR META-SKILL CORPUS
We introduce the prompt for generating feedback and refinement in two domains: Math and General.
We outline specific prompts designed to guide the evaluation and improvement of responses to
questions for building Dmetain each domain.
A.2.1 M ATH DOMAIN
For the Math Domain, the prompt instructs evaluators to assess the quality of a response to a math
question, provide a step-by-step analysis, and determine its correctness. If the response is incorrect,
the evaluator is asked to refine and provide a correct answer.
Prompt for feedback and refinement:
(Feedback) Please assess the quality of the response to the given question.
Here is the question: p.
Here is the response: r.
Firstly, provide a step-by-step analysis and verification for response starting with “Response Analy-
sis:”.
Next, judge whether the response correctly answers the question in the format of “judgment: cor-
rect/incorrect”.
(Refinement) If the answer is correct, output it. Otherwise, output a refined answer based on the given
response and your assessment.
A.2.2 G ENERAL DOMAIN
For the general test, aligned with the methodology described in section 3, we deploy the following
prompt to guide an LLM-based annotator in generating response feedback and refinement. This
prompt serves as the foundation for the meta-skill learning corpus and assists in producing self-
evolution training data in the general test setting.
Prompt for feedback and refinement:
(Feedback) Please assess the quality of response to the given question.
Here is the question: p.
Here is the response: r.
Firstly provide an analysis and verification for response starting with “Response Analysis:”.
Next, then rate the response on a scale of 1 to 10 (1 is worst, 10 is best) in the format of “Rating:”
(Refinement) Finally output an improved answer based on your analysis if no response is rated 10.
A.3 D ATA GENERATION
A.3.1 DMETA ATA QUANTITY
TheDmetadataset was generated using 3.5k unlabeled prompts from GSM8K and 2K from SV AMP3.
For general tests, 6K conversations were selected from 90K ShareGPT dialogues to form the general
Dmetadata.
A.3.2 U NLABELED PROMPTS FOR SELF-EVOLUTION TRAINING
Math Domain: For math tests, unlabeled prompts in self-evolution training were sourced as follows:
(1) First round self-evolving phase: 4K leftover prompts from GSM8k and 1K from SV AMP, ex-
cluding those used in Dmeta.
3Adhering to the official recommendation https://github.com/arkilpatel/SVAMP/tree/
main , training prompts consist of MAWPS (Koncel-Kedziorski et al., 2016) and ASDiv-A (Miao et al., 2020)
14

--- PAGE 15 ---
Preprint. Work in progress.
(2) Second/Third rounds: 10K/15K prompts were generated using Self-Instruct method (Wang
et al., 2022b), based on a template shown in appendix A.3.2 with initial 4 to 6 seed examples.
General Domain: 15K unlabeled prompts from ShareGPT dialogues were used for self-evolution
training data construction.
You are an experienced instruction creator. You are asked to develop 3 diverse instructions
according to the given examples.
Here are the requirements:
1. The generated instructions should follow the task type in the given examples.
2. The language used for the generated instructions should be diverse.
Given examples: {examples }
The generated instructions should be:
A. ...
B. ...
C. ...
A.4 T RAINING HYPERPARAMETERS
Our experiments were conducted in a computing environment with 8 NVIDIA V100 GPUs, each
having 32GB of memory. All models were fine-tuned in a full-parameter setting. We utilized the
AdamW optimizer for model training over 3 epochs, with a batch size of 128. The learning rate
was set at 2e-5, including a 3% learning rate warmup period. Below we provide a comprehensive
overview of the training hyperparameters employed in table 5. These parameters were uniformly
applied across all training methods in our experiments.
Table 5: Training hyperparameters.
Hyperparameter Global Batch Size LR Epo. Max Length Weight Decay Warmup Ratio
Value 128 2×10−53 2048 0 0.03
A.5 C ASE STUDY ANALYSIS
This subsubsection provides an in-depth case study that contrasts the performance of the original
Vicuna and Vicuna + SELF models. Illustrated in fig. 4, both models perform initial predictions,
followed by self-feedback and refinement steps. Notably, Vicuna’s refinement fails to correct its
initial errors, while Vicuna + SELF effectively utilizes self-feedback and refinement to derive an
accurate and logically coherent answer.
A.6 M ETA-SKILL TRAINING CORPUS
The example shown below exemplifies a standard training example from our meta-skill corpus. It il-
lustrates the model’s initial response, followed by its self-feedback, and the ensuing refinement. This
process demonstrates how the model is trained for self-feedback and self-refinement capabilities.
15

--- PAGE 16 ---
Preprint. Work in progress.
On Monday, Tiffany 
had 3 bags of cans.
On the next day, she 
found 7 more bags of 
cans.
So, on Tuesday, 
Tiffany had a total of 3 
+ 7 = <<3+7=10>>10 
bags of cans.
In addition to the 10 
bags of cans, she also 
found 44 bags of bottles.
So, Tiffany had a total 
of 10 bags of cans + 44 
bags of bottles = 
<<10+44=54>>54 bags 
in total.
Therefore, Tiffany had 
54 bags of cans 
altogether.Direct 
generation
Response Analysis: The 
response correctly identifies 
that Tiffany had 10 bags of 
cans on Monday and 44 
bags of bottles on the next 
day. It then adds these two 
quantities together to find 
the total number of bags 
Tiffany had .
Judgement: Correct
Correct Answer: Therefore, 
Tiffany had a total of 10 
bags of cans and 44 bags of 
bottles, for a total of 54 bags. 
Therefore, the correct 
answer is 54 bags.On Monday, Tiffany had 3 bags of cans. On the next day, she found 7 more bags of cans. So, she had a total of 3 + 7 = 10 bags of
cans on Tuesday. ### 10.0Question
On Monday, Tiffany 
had 3 bags of cans. On 
the next day, she found 
7 more bags of cans. So, 
she had a total of 3 + 7 
= 10 bags of cans on 
Tuesday.
On Monday, she also 
had 44 bags of bottles .
So, overall, Tiffany had 
10 bags of cans and 44 
bags of bottles on 
Monday and Tuesday.
Therefore, Tiffany had 
a total of 10 + 44 = 54 
bags of cans and bottles .Response Analysis: The 
response correctly identifies 
that Tiffany had 3 bags of 
cans on Monday and 7 more 
on Tuesday, totaling 10 bags . 
However, the response 
incorrectly adds the number 
of bags of cans and bottles 
together , stating that Tiffany 
had 10 bags of cans and 44 
bags of bottles on Monday 
and Tuesday. The question 
only asks for the number of 
bags of cans, not bags of 
bottles.
Judgement: incorrect
Correct Answer: On 
Monday, Tiffany had 3 bags 
of cans. On Tuesday, she 
found 7 more bags of cans, 
for a total of 3 + 7 = 10 bags 
of cans. Therefore, Tiffany 
had 10 bags of cans 
altogether.Self-evaluation 
& revision
Meta -skill LearningDirect 
generationSelf-evaluation 
& revision
Figure 4: Case study comparing the original Vicuna (left) and Vicuna+SELF (right) on a SV AMP
problem. Both models generate direct predictions and undergo self-feedback and self-refinement.
Both models initially produce answers, followed by self-feedback and self-refinement. Vicuna main-
tains the incorrect response after refinement, whereas Vicuna+SELF demonstrates enhanced self-
refinement, leading to a correct and logically consistent solution.
Question: Greg has an alarm set to ring three times a day as a reminder. When the alarm goes off, it
continues to ring until Greg turns it off. The first time it went off today, it rang four times. The second
time it went off, it rang for three times as long as the first time. The third time, it rang for half as long
as the second time. How many times did the alarm ring in all?
Original Response: The alarm rang four times the first time, three times as long as the first time the
second time, and half as long as the second time the third time. So, the total number of times the alarm
rang is 4 + 3*3 + 2*2 = 24.
Feedback: The response correctly interprets the problem. The first time the alarm rang, it rang for 4
times. The second time it rang, it rang for 3 times as long as the first time, which is 3*4 = 12 times.
The third time it rang, it rang for half as long as the second time, which is 12/2 = 6 times. However,
the final calculation is incorrect. The total number of times the alarm rang is not 4 + 3*3 + 2*2 = 24,
but 4 + 12 + 6 = 22. Judgement: incorrect
Refined Response: The alarm rang four times the first time, three times as long as the first time the
second time, and half as long as the second time the third time. So, the total number of times the alarm
rang is 4 + 12 + 6 = 22.
A.7 A LGORITHM
The “Two-Phase SELF Process” algorithm outlines a method for developing a base language model
through a two-staged approach: Meta-Skill Learning and Self-Evolving. The process starts with
training on a “Meta-Skill Learning corpus”, which consists of data representing the generation,
feedback and refinement process. Following this, the model enters the “Self-Evolving Phase”, where
it undergoes iterative refinements, employing data augmentation in each iteration to produce self-
refined outputs from its previously refined versions. This iterative self-evolution aims to leverage
accumulated knowledge and further enhance the model with newly generated data. The final out-
16

--- PAGE 17 ---
Preprint. Work in progress.
come is an advanced Language Model that has significantly evolved from its original state through
multiple self-evolution stages. More details are delineated in Alg. 1.
Algorithm 1: Two-Phase SELF Process
Data: (1) Meta-Skill training data ( Dmeta) and (2) unlabeled prompts
Input : An initial Language Model Minit
Result: A stronger Language Model MT
evolafter self-evolving
// Meta-Skill Learning Phase
Data: Meta-Skill learning corpus ( Dmeta)
Mmeta= Supervised finetuning( Minit,Dmeta);
// Self-Evolving Phase
Initialize M0
evolwithMmeta ;
foreach iteration tin 1 to Number of self-evolving iterations T do
// Data-Augmentation
Initialize Dt
evolas an empty set;
foreach prompt pi
evolintthunlabeled prompts do
Generate direct output ri
evolusing Mt−1
evol;
Generate self-refined output ˆri
evolfromri
evolusing Mt−1
evol;
UseMt−1
evolto filter the self-refined output;
Add(pi
evol,ˆri
evol)toDt
evol, where riis the refined response;
end
// Self-Evolution Training
Mt
evol= Supervised finetuning( Mt−1
evol,Dt
evol);
end
// Training Complete
return Improved Language Model MT
evol;
A.8 D ATA FILTERING STANDARDS
We design a boolean function, qualified (f), to evaluate feedback facross different domains, deter-
mining if a response to a specific prompt satisfies essential quality criteria.
In the Math Domain ,the function assesses feedback based on the explicit statement of “correctness”
in the evaluator’s judgment, aligned with the prompt structure in appendix A.2.1. It checks if the
word “correct” immediately follows the phrase “judgment:” in the feedback. A presence of “correct”
results in qualified (f)returning 1, meeting the qualification criteria. Absence leads to a return of 0.
For the General Domain , following the structure in appendix A.2.2, qualified (f)extracts and eval-
uates a numerical rating from the feedback. If the rating, found after ”Rating:”, is 7 or higher, the
function returns 1, indicating qualification. Ratings below 7 return 0, failing to meet the threshold.
A rating of 7 balances quality and training data quantity.
qualified (f)is key in both domains for filtering and assessing feedback quality, ensuring only high-
quality responses are used for refined answer generation in self-evolution training. Post data filtering,
Ψt−1in eq. (3) requires an update to Ψ′t−1= Ψt−1×qualified (f), adding a quality filter through
self-feedback. For clarity, we continue using original formulation as stated in eq. (3) in the main
text.
A.9 M ULTIPLE V .S. SINGLE SELF-REFINEMENT
This study explores the effects of two meta-skill training data organization strategies on model per-
formance: (1) Multiple Self-Refinement ( Dmeta-multi ), involving the sampling of three responses for
the model to choose the best for refinement, and (2) Single Self-Refinement ( Dmeta), where the
model generates and refines a single response.
table 6 compares these methods’ performances. Both strategies show performance gains with in-
creased training data volume. However, as data volume expands, the multiple-response refinement
shows a smaller improvement in direct generation performance ( +4.02%) than the single-response
17

--- PAGE 18 ---
Preprint. Work in progress.
method ( +5.84%). Considering the simplicity and computational efficiency of the single-response
method, which only samples one response during inference, and its better performance than the
multiple-response approach, we have opted for the single-response refinement strategy in our exper-
iments.
Table 6: Performance comparison of single and multiple response refinement with varying vol-
umes of meta-skill training data. The arrow indicates improvement from direct generation to self-
refinement: “direct generation →self-refinement”.
Data Size Vicuna + Dmeta Vicuna + Dmeta-multi
3.5k 25.39 →28.28 25.92 →27.29
7.5k 31.23 →32.98 29.94 →32.14
A.10 S ELF-EVOLUTION TRAINING : CONTINUAL TRAINING V .S. RESTART TRAINING
Table 7: Analysis about varied self-evolution training methodologies on GSM8K.
Training Approach Direct Generation (%) Self-Refinement (%)
Base Model 24.49 24.49
Restart Training 27.67 29.34
Continual Training (Mixed Data) 27.22 28.43
Continual Training ( Dt
evolOnly) 24.87 25.85
“Restart Training”, which combines meta-skill learning corpus with all rounds of self-evolution
training data, significantly improves direct generation (+3.18%) and self-refinement (+3.85%).
“Continual Training (Mixed Data)”, where the model is trained simultaneously with all rounds
of self-evolution data, also shows notable enhancements in direct generation (+2.73%) and self-
refinement (+3.94%). In contrast, “Continual Training ( Dt
evolOnly)”, which trains the model se-
quentially with self-evolution data from each round, demonstrates more modest gains (+0.38% in
direct generation, +0.98% in self-refinement). The relatively lower performance of the latter ap-
proach highlights the importance of a mixed data strategy for effective self-evolution training.
Throughout our main text, we have consistently employed the “Restart Training” method. This ap-
proach was selected for its superior performance, as evidenced in table 7. In addition, the integration
ofDmetainto the self-evolution training is crucial to prevent the potential catastrophic forgetting of
meta-skills. This strategy is essential for preserving the effectiveness and reliability of the self-
evolution training process, as highlighted in § 3.2.2.
A.11 SELF VS. SUPERVISED FINE-TUNING ON 7.5K GSM8 K TRAINING DATA .
When fine-tuned on the GSM8K 7.5k training set, the Vicuna model achieves an accuracy of 35.70%,
which is lower than the SELF method (37.87%).
The experiments in table 8 use 7.5k meta-skill data, ensuring a fair comparison with the supervised
fine-tuned model. This approach differs from the one in table 1, where only 3.5k meta-skill data are
used.
table 8 indicates that, with 7.5k unlabeled training prompts for the meta-skill learning corpus, Vi-
cuna + DQAachieves 28.05%. Post meta-skill learning, direct generation results improve to 31.23%,
further increasing to 32.98% after self-refinement. Subsequent self-evolution rounds lead to perfor-
mance gains, reaching 37.87%(direct generation) and 38.12%(self-refinement) in the second round,
outperforming supervised fine-tuning (35.70%).
Continuous Improvement of SELF vs. Supervised Fine-tuning: SELF’s primary advantage
lies in its ability for continuous improvement and adaptation. In contrast to supervised fine-tuning,
18

--- PAGE 19 ---
Preprint. Work in progress.
Table 8: Comparison between SELF and Supervised Fine-Tuning on GSM8K. A “-” symbol in
the table indicates self-refinement was not conducted during inference because the model lacks the
necessary self-refinement skill.
Direct Generation (%) Self-Refinement (%) DQA DmetaSelf Evol.
1st 2nd
28.05 - ✓
31.23 32.98 ✓
35.43 36.22 ✓
37.87 38.12 ✓ ✓ ✓
35.70 - SFT (GSM8K training data)
SELF doesn’t rely on human or external LLM annotations (like GPT3.5/GPT4) for training data in
self-evolution training.
A.12 S CALABILITY OF SELF F RAMEWORK
To explore how SELF performs with different starting model qualities, we conduct experiments
using the OpenLlama-3b model (Geng & Liu, 2023), a smaller LLM along with a stronger LLM,
VicunaV1.5(finetuned from Llama2-7b)l (Chiang et al., 2023), on the GSM8K dataset. This allows
us to assess SELF’s adaptability to model quality. Experiments with SELF are based on the first
round of self-evolution. The results are as follows:
Table 9: Scalability of the SELF framework across different models.
Model Direct Generation (%) Self-Refinement (%)
OpenLlama-3b 2.04 1.01
OpenLlama-3b + DQA 12.13 10.97
OpenLlama-3b + DQA+ SELF 15.32 15.78
Vicuna (Llama-7b) 16.43 15.63
Vicuna + DQA 24.49 24.44
Vicuna + DQA+ SELF 27.67 29.34
VicunaV1.5 (Llama2-7b) 18.5 17.43
VicunaV1.5 + DQA 26.04 25.48
VicunaV1.5 + DQA+ SELF 30.22 32.43
Applicability and Robustness of SELF Framework: The average improvement of 17.32% via
direct generation and 16.87% after self-refinement underscores the framework’s scalability and effi-
cacy. It reveals a consistent positive impact of the SELF Framework across diverse models.
SELF Framework exhibits enhanced performance on more powerful models: As shown in ta-
ble 9, applying SELF to VicunaV1.5 results in the most significant gains - 30.22% in direct gener-
ation and 32.43% after self-refinement, surpassing the performance on Vicuna and OpenLlama-3b.
This indicates that the effectiveness of the SELF framework improves with the underlying model’s
capabilities.
A.13 I MPACT OF META-SKILL CORPUS QUALITY
We examine the influence of meta-skill learning quality on the self-evolution process with the fol-
lowing results:
The presented table 10 demonstrates the remarkable performance improvements achieved by using
GPT-4 for generating the meta-skill corpus in our SELF framework, compared to using GPT-3.5-
19

--- PAGE 20 ---
Preprint. Work in progress.
Table 10: Effect of meta-skill corpus quality on model performance using GPT-3.5-turbo and GPT4.
Training StageDirect Generation (%) Self-Refinement (%)
(GPT-3.5-turbo/GPT4) (GPT-3.5-turbo/GPT4)
Vicuna + Dmeta 24.84/25.39 (0.55 ↑) 25.22/28.28 (3.06 ↑)
Vicuna + Dmeta+ SELF Evol. 25.11/27.67 (2.56 ↑) 25.47/29.34 (3.87 ↑)
turbo. The table shows significant enhancements in both direct generation and self-refinement across
training stages when GPT-4 is utilized. For instance, in the “Vicuna + Dmeta” stage, direct generation
performance increases from 24.84% with GPT-3.5-turbo to 25.39% with GPT-4, marking a gain
of 0.55%. Similarly, in the “Vicuna + Dmeta+ SELF Evolution” stage, the self-refinement result
improves from 25.47% with GPT-3.5-turbo to 29.34% with GPT-4, showing an enhancement of
3.87%.
This analysis highlights the significant impact of utilizing high-quality meta-skill training data on
the performance of the Vicuna model within the SELF framework. The shift from GPT-3.5-turbo to
GPT-4 for the generation of the meta-skill corpus leads to consistent improvements in both Direct
Generation and Self-Refinement metrics.
A.14 S INGLE -ROUND VS . ITERATIVE SELF-EVOLUTION TRAINING
Given an equal number of unlabeled prompts, we evaluate the effectiveness of training within a
single-round versus iterative training. The former method uses a single model to self-curate training
data from all available unlabeled prompts at once. In contrast, the latter method involves dividing
the unlabeled prompts into multiple parts. For the iterative approach, the model is initially trained
on a portion of the unlabeled prompts and self-curated labels. Following this, the trained model is
employed to create new training data based on previously unused prompts. As described in our main
text, we divide the unlabeled prompts into three parts, enabling the model to undergo three iterative
rounds of self-evolution.
Table 11: Comparison of single-round training and iterative training.
Training Method Direct Generation (%) Self-Refinement (%)
SELF (Single-Round) 28.40 30.55
SELF (Iterative) 29.64 31.31
table 11 shows that in the “Single-Round” training, the performance is 28.40% for direct generation
and 30.55% for self-refinement. In contrast, the iterative approach yields higher scores of 29.64%
for direct generation and 31.31% for self-refinement.
Advantages of Iterative Training: Iterative training benefits from the enhanced capabilities of
LLMs in subsequent rounds, which generate higher-quality training data and lead to improved test
performance.
20
