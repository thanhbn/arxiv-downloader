# 2310.08740.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2310.08740.pdf
# Kích thước tệp: 1209069 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Tác Nhân Ngôn Ngữ Zero-Shot để Điều Khiển Máy Tính
với Phản Tư Có Cấu Trúc
Tao Li‹Gang Li‹Zhiwei Deng‹Bryan Wang˛:Yang Li‹
‹Google Research, Mountain View, U.S.A.
˛University of Toronto, Ontario, Canada
{tlinlp,leebird,zhiweideng,liyang}@google.com
bryanw@dgp.toronto.edu
Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) đã cho thấy khả năng ngày càng tăng trong việc lập kế hoạch và thực hiện một mục tiêu cấp cao trong môi trường máy tính trực tiếp (ví dụ MINIWOB++). Để thực hiện một nhiệm vụ, các nghiên cứu gần đây thường yêu cầu một mô hình học từ các ví dụ vết thực hiện của nhiệm vụ đó thông qua học có giám sát hoặc prompting vài/nhiều mẫu. Không có những ví dụ vết này, vẫn là một thách thức làm thế nào một tác nhân có thể tự động học hỏi và cải thiện khả năng điều khiển trên máy tính, điều này hạn chế khả năng của tác nhân trong việc thực hiện một nhiệm vụ mới. Chúng tôi tiếp cận vấn đề này với một tác nhân zero-shot không yêu cầu các vết chuyên gia được cung cấp trước. Tác nhân của chúng tôi lập kế hoạch cho các hành động có thể thực thi trên một môi trường được quan sát một phần, và tiến triển một cách lặp lại một nhiệm vụ bằng cách xác định và học hỏi từ các lỗi của nó thông qua tự phản tư và quản lý tư duy có cấu trúc. Đối với các nhiệm vụ dễ của MINIWOB++, chúng tôi cho thấy rằng tác nhân zero-shot của chúng tôi thường vượt trội hơn các SoTAs gần đây, với kế hoạch hiệu quả hơn. Đối với các nhiệm vụ phức tạp hơn, tác nhân phản tư của chúng tôi thực hiện ngang bằng với các mô hình tốt nhất trước đó, mặc dù các công trình trước đây có lợi thế khi tiếp cận các vết chuyên gia hoặc thông tin màn hình bổ sung.

1 Giới thiệu
Các nghiên cứu trước đây đã cho thấy tiềm năng trong việc sử dụng các mô hình ngôn ngữ lớn (LLMs) để tạo ra hành động, ví dụ SAYCAN(Brohan et al., 2023), REACT(Yao et al., 2023), TOOLFORMER (Schick et al., 2023), và SWIFT SAGE (Lin et al., 2023)) trên nhiều môi trường trực tiếp khác nhau (ví dụ MINIWOB++ (Shi et al., 2017; Liu et al., 2018), ALFWORLD (Shridhar et al., 2021), và ALPHA CODE (Li et al., 2022). Một cách tiếp cận chung là sử dụng LLMs để theo dõi các vết chuyên gia, hiểu các thay đổi môi trường, lập kế hoạch cho các hành động tương lai, và thực hiện một hành động bằng cách soạn các lời gọi API; tất cả dưới dạng văn bản. Một số nghiên cứu đã cho thấy rằng việc thử một nhiệm vụ một cách lặp lại với nhiều vòng tự phản tư có thể cải thiện đáng kể việc hoàn thành nhiệm vụ, ví dụ, REFLEXION (Shinn et al., 2023), SELF-REFINE (Madaan et al., 2023). Trong quá trình này, LLMs được nhắc nhở để cập nhật một kế hoạch thực hiện trước đó theo phản hồi từ môi trường. Những cập nhật như vậy trở thành một phần của prompt cho bộ tạo hành động trong vòng tiếp theo.

Gần đây, MINIWOB++ đã được sử dụng như một bàn thử nghiệm cho khả năng của LLM trong các nhiệm vụ máy tính được mô-đun hóa. Để học một nhiệm vụ, một cách tiếp cận phổ biến là sử dụng các ví dụ vết rộng rãi của nhiệm vụ đó để giám sát trực tiếp (ví dụ, CC-Net (Humphreys et al., 2022), WebGUM (Furuta et al., 2023)), tự giám sát (Gur et al., 2023), hoặc prompting vài/nhiều mẫu (ví dụ, RCI (Kim et al., 2023), SYNAPSE (Zheng et al., 2023)). Họ đã đạt được tỷ lệ hoàn thành nhiệm vụ hơn 90% trên hàng chục nhiệm vụ máy tính, dường như đã giải quyết vấn đề điều khiển máy tính.

Tuy nhiên, yêu cầu về các vết chuyên gia để học thực hiện một nhiệm vụ hạn chế khả năng của tác nhân đối với các nhiệm vụ mới. Không sử dụng các vết được chọn lọc cẩn thận làm hướng dẫn, liệu một tác nhân có thể tự động học hỏi và cải thiện khả năng điều khiển trên máy tính? Để giải quyết câu hỏi này, chúng tôi đề xuất một tác nhân zero-shot. Chúng tôi xây dựng tác nhân của mình dựa trên PaLM2 (Anil et al., 2023), một LLM gần đây, và tác nhân của chúng tôi sử dụng một bộ prompt hướng dẫn thống nhất trên các nhiệm vụ khác nhau, không có sự điều chỉnh rộng rãi cho từng nhiệm vụ riêng lẻ.

Ngoài ra, các nghiên cứu gần đây, ví dụ, RCI (Kim et al., 2023), ADAPLANNER (Sun et al., 2023), và SYNAPSE (Zheng et al., 2023), sử dụng một biểu diễn màn hình có thể bao gồm nhiều thông tin hơn những gì được trình bày cho người dùng trên màn hình. Ví dụ, Hình 1 cho thấy một ví dụ về các phần tử không được hiển thị trên màn hình nhưng có mặt trong HTML được cung cấp cho LLM. Việc sử dụng thông tin bổ sung như vậy một cách tùy tiện làm giảm độ khó cho tác nhân thực hiện nhiệm vụ. Tuy nhiên trong các trường hợp sử dụng chung, thông tin như vậy có thể không sẵn có, và việc dựa vào thông tin như vậy có thể cản trở khả năng áp dụng của tác nhân. Chúng tôi arXiv:2310.08740v3 [cs.CL] 23 Oct 2023

--- TRANG 2 ---
(a) Trước
 (b) Sau
 (c) HTML của trước
(d) Trước
(e) HTML của trước
Hình 1: Biểu diễn màn hình không nhất quán. Hình 1a-1c: trước và sau khi nhấp vào nút "more" trong nhiệm vụ social-media (seed=2). HTML trước khi nhấp đã tiết lộ nội dung. Hình 1d-1e: Vấn đề tương tự trong click-tab-2 (seed=0).

đã kiểm tra thủ công 13 nhiệm vụ tương đối thách thức trên MINIWOB++ được cho là trải rộng trên nhiều màn hình, và phát hiện 5 trong số chúng chứa thông tin như vậy—thông tin đa màn hình trong một quan sát duy nhất—trong HTML của họ.

Những đóng góp của chúng tôi như sau: Thứ nhất, chúng tôi sử dụng một biểu diễn màn hình gọn nhẹ giả định ít thông tin hơn nhiều so với những gì được sử dụng bởi các nghiên cứu trước đây, do đó tạo ra một môi trường thử nghiệm tổng quát và thực tế hơn. Thứ hai, chúng tôi đề xuất một bộ lập kế hoạch hành động đơn giản nhưng hiệu quả có thể lập kế hoạch chính xác các hành động có thể thực thi trên một trạng thái trong một lượt. Với khả năng gần đây của LLM, chúng tôi cho thấy rằng một chiến lược "ngây thơ" như vậy có thể giải quyết hầu như tất cả các nhiệm vụ dễ trên benchmark MINIWOB++. Đối với các nhiệm vụ thách thức hơn, chúng tôi lấy cảm hứng từ Reflexion (Shinn et al., 2023) và đề xuất một chiến lược quản lý tư duy có cấu trúc để tạo điều kiện phản tư, cho phép tác nhân học hỏi và cải thiện hiệu quả từ các thất bại khám phá. Với một vài vòng thử nghiệm, tác nhân của chúng tôi đạt được hiệu suất tương đương với các mô hình few/many-shot state-of-the-art trước đó. Theo hiểu biết tốt nhất của chúng tôi, tác nhân của chúng tôi là thiết kế zero-shot đầu tiên cho các nhiệm vụ điều khiển máy tính¹.

2 Bối cảnh
LLMs đã trở thành một công cụ nổi lên để lập kế hoạch và thực hiện các bước cần thiết để đáp ứng một mục tiêu cấp cao. Những mô hình này đã thể hiện khả năng cao trong việc theo dõi các vết trong ngữ cảnh để giải quyết các nhiệm vụ phức tạp.

¹Mã và notebook: https://github.com/google-research/google-research/tree/master/zero_shot_structured_reflection

Lập kế hoạch & Phản tư REACT(Yao et al., 2023) đã sử dụng các suy nghĩ trung gian để hướng dẫn chuỗi dài lập kế hoạch hành động trong một môi trường trực tiếp. Ngoài một thử nghiệm lập kế hoạch, REFLEXION (Shinn et al., 2023) và SELF-REFINE (Madaan et al., 2023) gần đây đã phát hiện khả năng tự phê bình và tự cải thiện của LLM có thể học hỏi và giải quyết một nhiệm vụ đã cho một cách lặp lại thông qua nhiều thử nghiệm. Tuy nhiên, các nghiên cứu lập kế hoạch gần đây yêu cầu các prompt vết rộng rãi và tùy chỉnh để tác nhân LLM học lập kế hoạch chính xác. SWIFT SAGE (Lin et al., 2023) giảm các lời gọi lập kế hoạch rộng rãi đến LLM bằng cách huấn luyện một mô-đun lập kế hoạch nhỏ và nhanh để tạo điều kiện cho chuỗi dài các hành động. Với động cơ tương tự, tác nhân zero-shot của chúng tôi dựa trên một bộ lập kế hoạch theo giai đoạn hiệu quả. Chúng tôi liệt kê một so sánh chi tiết trong Bảng 1.

[Bảng 1: So sánh với công trình trước đây về lập kế hoạch phản tư]

Tác Nhân Ngôn Ngữ/Thị Giác cho Điều Khiển Máy Tính
MINIWOB++ (Shi et al., 2017) có nhiều chục nhiệm vụ máy tính cơ bản được lưu trữ như các môi trường trực tiếp. Những tiến bộ gần đây trên benchmark này đã được hưởng lợi từ chú thích của con người rộng rãi để tạo điều kiện cho behavior cloning và reinforcement learning, như CC-NET(Humphreys et al., 2022) và PIX2ACT(Shaw et al., 2023). Bên cạnh những mô hình này dựa vào đầu vào đa phương tiện hoặc thị giác, một hướng khác là sử dụng LLMs như một tác nhân có sẵn, và sử dụng suy luận được nhắc nhở để tạo ra hành động, như

²Không có nguồn công khai cho biểu diễn màn hình của CC-NET tại thời điểm viết bài báo này.

--- TRANG 3 ---
RCI (Kim et al., 2023), ADAPLANNER (Sun et al., 2023), và SYNAPSE (Zheng et al., 2023). Chúng tôi nêu bật các khác biệt kỹ thuật của mình trong Bảng 2.

3 Giao Diện Môi Trường
Vai trò của một tác nhân ngôn ngữ là hiểu các biểu diễn màn hình (Mục 3.1&3.2), thực hiện các hành động trên môi trường (Mục 3.3), và phản ứng với phản hồi môi trường (Mục 3.4).

3.1 Xử Lý Màn Hình
Định nghĩa của một quan sát màn hình thay đổi theo phương thức trong các nghiên cứu gần đây. Quan sát màn hình cho một mô hình dựa trên thị giác (ví dụ Humphreys et al., 2022; Shaw et al., 2023) có thể bị hạn chế bởi các cấu hình viewport khác nhau. Đối với một mô hình ngôn ngữ, cụ thể là những mô hình nhận mã HTML, một quan sát màn hình thường là nguồn trang của một màn hình, ví dụ, những cái được khởi tạo từ một template nhiệm vụ MINIWOB++ với một seed ngẫu nhiên đã cho. Khi các phần tử HTML không bị ràng buộc bởi cấu hình viewport, nhu cầu cho hành động cuộn biến mất. Tuy nhiên, như đã thảo luận trong Mục 1, chúng tôi không ngay lập tức sử dụng HTML mở rộng nếu việc mở rộng yêu cầu một hành động UI: chúng tôi chỉ mở rộng biểu diễn HTML khi tác nhân thực sự thực hiện hành động. Thiết kế này làm giảm bớt giả định về môi trường, và buộc tác nhân học cách hành xử hợp lý khi nó được cung cấp thông tin hạn chế.

3.2 Biểu Diễn Màn Hình Gọn Nhẹ
Mã HTML thô có xu hướng dài dòng, điều này đặt ra thách thức thực tế cho LLMs thường có giới hạn bẩm sinh về độ dài đầu vào hoặc ngữ cảnh. Zheng et al. (2023) đã thiết kế một kỹ thuật để lựa chọn ví dụ prompting có cấu trúc nhằm trích xuất các ví dụ vết thông tin hơn, như một cách để giảm nội dung đầu vào cho LLMs. MindAct (Deng et al., 2023) đã xếp hạng và hợp nhất các phần tử quan tâm để biểu diễn đoạn trang web. Thay vào đó, chúng tôi lấy cảm hứng từ Wang et al. (2023a) để đơn giản hóa một cách heuristic mã HTML của mỗi màn hình, giữ lại các thuộc tính chính cho mỗi phần tử lá, tức là id, class, text, placeholder, value, và vị trí trên lưới màn hình 3x3. Sự đơn giản hóa như vậy đã cho thấy mang lại kết quả hấp dẫn trên các nhiệm vụ hiểu UI. Một ví dụ được hiển thị trong Hình 2.

3.3 Không Gian Hành Động
Đối với mỗi hành động, mô hình tác nhân của chúng tôi xuất ra các lệnh theo định dạng cụ thể cho loại hành động

(a) Màn hình
(b) HTML Đơn giản hóa
Hình 2: Ví dụ về biểu diễn màn hình gọn nhẹ.

đó. Cụ thể, chúng tôi sử dụng ba loại hành động như được hiển thị trong Bảng 3. Các LLMs gần đây như PaLM-2 (Anil et al., 2023) giỏi trong việc tuân theo định dạng đầu ra như vậy. Chi tiết prompt hơn được đưa ra trong Phụ lục A. Để định vị một cách xác định một lệnh hành động trên môi trường MINIWOB++, chúng tôi làm theo cách tiếp cận trong các nghiên cứu chỉ ngôn ngữ trước đây (ví dụ Kim et al., 2023) để truy cập các phần tử HTML bằng mẫu XPATH. Khi định vị các hành động click trên môi trường thực tế, chúng tôi sử dụng id phần tử gọn nhẹ (Mục 3.2) được căn chỉnh với phần tử HTML thực tế trong HTML thô. Đối với hành động type, chúng tôi phân tách nó thành một hành động click tiếp theo là một loạt các phím bấm cho văn bản.

[Bảng 3: Các loại hành động và lệnh ví dụ]

3.4 Phản Hồi Môi Trường
MINIWOB++ khác với môi trường giống TEXTWORLD (Shridhar et al., 2021) ở chỗ thay đổi trạng thái từ việc thực hiện một hành động không được diễn đạt một cách tự nhiên. Thay vào đó, một tác nhân sẽ cần quan sát toàn bộ thay đổi màn hình một cách ngầm định, khiến việc tác nhân áp dụng lập luận Chain-of-Thought (Wei et al., 2022) trở nên kém thuận tiện hơn. Chúng tôi phân loại rộng rãi các điều kiện kết thúc thử nghiệm thành: 1) đúng, 2) chu kỳ, 3) không thay đổi, 4) không hoàn thành, 5) ngoại lệ, và 6) thất bại. Điều kiện 2) và 3) so sánh mã HTML của màn hình hiện tại với những màn hình trước đó. Điều kiện 5) xảy ra khi một hành động định vị không thể được thực hiện thành công. Trong nhiều thử nghiệm, mỗi điều kiện kết thúc được liên kết với một prompt cho suy luận phản tư.

4 Chiến Lược Lập Kế Hoạch
Trong phần này, chúng tôi tóm tắt các chiến lược lập kế hoạch được sử dụng trong các nghiên cứu lập kế hoạch dựa trên LLM gần đây để

--- TRANG 4 ---
Hình 3: Một ví dụ về thử nghiệm phản tư thành công bởi tác nhân zero-shot của chúng tôi trên nhiệm vụ MINIWOB++ use-autocomplete với seed=0. Các hành động bước được diễn giải từ những cái thực thi thực tế để dễ đọc.

thúc đẩy lập kế hoạch theo giai đoạn của chúng tôi. Với một mục tiêu đã cho, một mô hình tác nhân sẽ đưa ra các hành động dựa trên tương tác trước đó với môi trường. Để ngắn gọn, hãy để chúng tôi ký hiệu tương tác như một chuỗi các cặp trạng thái và hành động {si, ai}.

4.1 Lập Kế Hoạch Lặp Lại
Trong lập kế hoạch lặp lại (ví dụ Yao et al., 2023; Madaan et al., 2023), mô hình tác nhân lặp lại việc tạo ra một hành động "nguyên tử" ai, định vị nó trên môi trường để thực hiện, và sau đó quan sát trạng thái tiếp theo si. Nghĩa là,
ai ~ τθ(a|si, ai-1, si-1, ...a0, s0)  (1)
trong đó τθ ký hiệu mô hình lập kế hoạch. Lập kế hoạch như vậy là một lựa chọn phổ biến cho các môi trường yêu cầu quan sát bằng khám phá. Với các môi trường phản hồi (ví dụ Côté et al., 2018; Shridhar et al., 2021), một tác nhân như vậy có thể được hưởng lợi từ lịch sử tương tác dài có thể dễ dàng kết nối với lập luận Chain-of-Thought (Wei et al., 2022).

4.2 Lập Kế Hoạch Rồi Thích Ứng
Gần đây, Kim et al. (2023) quan sát thấy rằng một kế hoạch ban đầu, tuy thô sơ, có thể giúp lập kế hoạch lặp lại. Chính thức,
{a0, a1, ...an} ~ τθ(a|s0)  (2)
āi ~ zθ(ā|si, āi-1, si-1, ..., {a0, a1, ...an})  (3)
trong đó zθ thích ứng những bước ban đầu đó thành các hành động có thể thực thi (ā's) trên môi trường. Trong thực tế, cả τθ và zθ đều sử dụng cùng một LLM.

Về mặt khái niệm, điều này tương tự với bộ lập kế hoạch zero-shot (Huang et al., 2022) và ReAct (Yao et al., 2023) rằng suy nghĩ trung gian có thể giúp lập kế hoạch chuỗi dài các hành động. Nhược điểm là tác nhân cần tuân theo các ví dụ vết few-shot được thiết kế cẩn thận để tạo ra một kế hoạch ban đầu tốt. ADAPLANNER (Sun et al., 2023) giải quyết vấn đề này với một bộ tinh chỉnh kế hoạch thích ứng giám sát tính tương thích trạng thái-hành động và đưa ra các hành động được tinh chỉnh khi có sự không khớp. Dòng lập kế hoạch này thường cần phải đối phó với ảo giác trong kế hoạch ban đầu vì, cuối cùng, mô hình tác nhân chỉ quan sát s0 nhưng cần lập kế hoạch cho các trạng thái không quan sát được.

4.3 Lập Kế Hoạch Theo Giai Đoạn Và Theo Dõi
Các nghiên cứu trước đây về cơ bản thêm các thành phần lập kế hoạch bổ sung vào tác nhân. Thay vào đó, chúng tôi áp dụng một chiến lược lập kế hoạch đơn giản hơn. Đối với môi trường máy tính, các tác nhân thường thấy một trạng thái nơi nhiều hành động có thể được thực hiện, mà không cần quan sát các thay đổi trạng thái tinh tế, ví dụ, lựa chọn nhiều trên một danh sách. Trong những trường hợp như vậy, lập kế hoạch lặp lại trên một màn hình duy nhất có thể kém hiệu quả hơn, và thường là không cần thiết. Mặt khác, lập kế hoạch-rồi-thích ứng tạo ra các hành động vượt ra ngoài những cái có thể thực hiện có thể làm nhầm lẫn tác nhân LLM trong bước thích ứng. Hơn nữa, cả hai cách tiếp cận đều yêu cầu tác nhân tạo ra lặp lại hành động tiếp theo, yêu cầu một LLM có cửa sổ ngữ cảnh lớn.

Để giải quyết những vấn đề này, chúng tôi thực hiện một bước ở giữa bằng cách lập kế hoạch tối đa các hành động có thể nhìn thấy trên trạng thái hiện tại tất cả cùng một lúc. Sau khi lập kế hoạch, tác nhân chỉ cần tuân theo nghiêm ngặt kế

--- TRANG 5 ---
hoạch đã tạo ra, và quá trình như vậy lặp lại trên nhiều màn hình. Chính thức,
{a0i, ...aki} ~ τθ(a|si, ai-1, ai-2, ...a0)  (4)
trong đó mỗi giai đoạn về cơ bản là tạo ra k hành động có thể thực hiện cho trạng thái si. Lưu ý rằng, chúng tôi cũng bỏ qua các trạng thái trước đó trong Phương trình 4 để làm cho suy luận hiệu quả hơn. Trong thực tế, chúng tôi phát hiện rằng một câu phát biểu đơn giản, bằng ngôn ngữ tự nhiên, tóm tắt chức năng nào được đạt được bởi hành động, là một biểu diễn tốt cho cặp trạng thái-hành động {si, ai}.

Chi tiết triển khai. Chúng tôi một lần nữa dựa vào LLM cơ bản để tuân theo các hướng dẫn thực hiện như trong Phương trình 4. Chi tiết prompt có trong Phụ lục B-E. Trong những trường hợp hiếm hoi, mô hình tác nhân dự đoán ít bước hơn (ví dụ, quên submit) hoặc nhiều bước hơn (ví dụ, ảo giác các hành động không thể thực hiện) so với cần thiết. Đối với trường hợp trước, chúng tôi lặp lại việc lập kế hoạch trong Phương trình 4 cho đến khi không có kế hoạch nào được tạo ra nữa. Đối với trường hợp sau, chúng tôi dừng việc thực hiện kế hoạch hiện tại và chuyển sang tự phản tư để sửa chữa.

5 Tự Phản Tư Có Cấu Trúc
Trong thực tế, một hướng dẫn hoặc mục tiêu cấp cao của con người có thể mơ hồ, và một môi trường có thể bị ẩn một phần. Do đó, các tác nhân dễ mắc lỗi; điều này thậm chí đúng với người dùng khi thực hiện một nhiệm vụ (Humphreys et al., 2022). Khi một tín hiệu tiêu cực được đưa ra, chẳng hạn như không thay đổi hoặc thất bại (Mục 3.4), chúng tôi yêu cầu tác nhân phản tư về quỹ đạo hành động trong quá khứ của nó, đề xuất một phiên bản cải thiện, và sau đó thử lại nhiệm vụ. Một ví dụ về phản tư thành công từ tác nhân của chúng tôi được hiển thị trong Hình 3.

Trong các nghiên cứu gần đây (Shinn et al., 2023; Madaan et al., 2023), phản tư được tiến hành ở cuối mỗi thử nghiệm bằng cách tích lũy một mục văn bản. Mục này về cơ bản là một câu phát biểu ngôn ngữ tự nhiên về những gì đáng lẽ ra được thực hiện thay thế. Tại thử nghiệm t,
ai ~ τθ(a|si, ai-1, ...a0, s0; Rt)  (5)
Rt+1 ~ REFLθ(an, sn, ...ai, si, ...; Rt)  (6)
trong đó Rt bao gồm một danh sách các cặp {ai, a'i}, mỗi cặp ký hiệu cập nhật hành động sai ai thành a'i. Trong thử nghiệm tiếp theo t+1, các mục tích lũy trong Rt+1 được thêm tiền tố vào prompt cho bộ lập kế hoạch tác nhân τθ.

Số lượng mục được duy trì trong bộ nhớ phản tư bị hạn chế bởi nhiều yếu tố. Một là, nó làm tăng độ dài đầu vào của LLM. Hơn nữa, nó yêu cầu tác nhân LLM xử lý các cấu trúc tư duy qua nhiều thử nghiệm. Trong thực tế, Shinn et al. (2023) đã hạn chế kích thước bộ nhớ ∈ [1,3].

5.1 Quản Lý Tư Duy Có Cấu Trúc
Trong một thiết lập zero-shot, phản tư đặt gánh nặng lớn lên khả năng của LLM để tuân theo một bộ hướng dẫn phức tạp (ngoài những cái trong Mục 3.3&4.3). Cuối cùng, trong trường hợp này, LLM không có vết chuyên gia để tuân theo, do đó cần học từ các thử nghiệm. Với việc tăng số lượng thử nghiệm, bộ nhớ phản tư về cơ bản tạo thành một vấn đề tổ hợp cho tác nhân giải quyết. Đối với một bước thời gian i, có thể có nhiều hành động thất bại trong các thử nghiệm lịch sử, do đó nên tránh. Đối với một thử nghiệm t, nếu tác nhân xác định một lỗi quan trọng tại thời điểm i, các phản tư về các bước thời gian sau đó có thể được coi là lỗi thời.

Trong nghiên cứu sơ bộ của chúng tôi, chúng tôi phát hiện việc thiết kế một cách để giúp tác nhân LLM duy trì bộ nhớ này là quan trọng. Nếu không, khi một mục phản tư {ai, a'i} được đưa ra, ngay cả một LLM tiên tiến nhất vẫn có thể 1) lặp lại cùng một lỗi ai, 2) thất bại trong việc tuân theo kế hoạch phản tư để thậm chí đạt đến si, và 3) dao động giữa các hành động sai mà nó đã thu thập trong các thử nghiệm trước đó.

Để làm cho phản tư chạy đáng tin cậy và hiệu quả hơn, chúng tôi đề xuất một tự phản tư có cấu trúc trong Thuật toán 1. Khi một hành động được đề xuất a'i được đưa ra bởi tác nhân phản tư, chúng tôi buộc tác nhân của chúng tôi lập kế hoạch chính xác a'i tại bước i. Hơn nữa, để tránh lặp lại giữa hai hành động thất bại tại cùng một bước, chúng tôi sử dụng một tập hành động bị vô hiệu hóa D để ghi nhớ chúng và vô hiệu hóa chung các hành động này trong môi trường. Cuối cùng, chúng tôi xóa các mục phản tư cho các bước tương lai nếu một mục sớm được cập nhật. Với quản lý này, tác nhân của chúng tôi không còn bị ràng buộc bởi giới hạn đầu vào của LLM, và có kích thước bộ nhớ N.

Thuật toán 1: Quản Lý Tư Duy Có Cấu Trúc
1: R ← [∅] × N; D ← [∅] × N;
2: for t ∈ [0, T):
3:   for i ∈ [0, N):
4:     if R[i] and R[i].a' ∉ D[i]: // nếu có phản tư
5:       ai ← R[i].a' // hành động từ phản tư
6:     else: ai ~ τθ(a|...) // lập kế hoạch thường xuyên
7:     if needToReflect: // nếu lỗi xảy ra
8:       {aj, a'j} ~ REFLθ(...) // phản tư
9:       if R[j] ≠ ∅:
10:        D[j].add(R[j].a) // ghi lại nhấp sai
11:      R[j] ← {aj, a'j} // ghi lại phản tư bằng lời nói
12:      R[j+1:] ← ∅; D[j+1:] ← ∅ // xóa bộ nhớ

Lưu ý rằng trong dòng 6, chúng tôi sử dụng bộ lập kế hoạch theo giai đoạn trong Phương trình 4.3 không phụ thuộc vào R được cập nhật lặp lại, do đó khác với các nghiên cứu gần đây (Phương trình 6).

Tương tác với Lập Kế Hoạch Theo Giai Đoạn. Giả sử bộ lập kế hoạch theo giai đoạn dự đoán [a0, ...ai, ...an] nhưng việc thực hiện đã thất bại, và bước phản tư đã xác định

--- TRANG 6 ---
ai là lỗi sớm nhất, do đó đề xuất a'i. Trong thử nghiệm tiếp theo, chúng tôi sẽ lặp lại các thực hiện từ a0 đến ai-1³, và chặn bộ lập kế hoạch tác nhân tại bước i để buộc thực hiện a'i. Đối với các bước sau i, chúng tôi đưa bộ lập kế hoạch của chúng tôi đến giai đoạn tiếp theo. Trong trường hợp xấu nhất khi một tác nhân thất bại ở mọi bước, lập kế hoạch theo giai đoạn của chúng tôi về cơ bản quay về lập kế hoạch-rồi-thích ứng (Mục 4.2), ngoại trừ việc không có kế hoạch ban đầu.

5.2 Ràng Buộc Không Gian Hành Động
Đối với một hành động được cập nhật a' tại thử nghiệm phản tư t, chúng tôi buộc nó được thực hiện tại bước thời gian liên quan nếu và chỉ nếu a' không phải là một nỗ lực thất bại trước thử nghiệm t. Có thể khó khăn để nhắc nhở LLM tuân theo ràng buộc tổ hợp đơn giản như vậy trong văn bản, đặc biệt là như một hỗn hợp tín hiệu tích cực và tiêu cực được bao quanh bởi các hướng dẫn khác. Do đó, chúng tôi thấy việc vô hiệu hóa rõ ràng những hành động đã thất bại trước đó trong biểu diễn màn hình tương ứng là quan trọng. Tuy nhiên, điều này không có nghĩa là loại bỏ phần tử tương ứng khỏi mã giả HTML. Thay vào đó chúng tôi chỉ loại bỏ thuộc tính id, và vẫn cho phép thông tin phần tử được trình bày cho LLM. Chúng tôi chỉ làm như vậy cho các hành động loại click.

Đối với các hành động không phải click, tập vô hiệu hóa D không thể dễ dàng được thực thi trên môi trường và tác nhân LLM. Chúng tôi thực sự có thể nhắc nhở LLM cơ bản nói rằng một số phím đặc biệt không hợp lệ hoặc một số văn bản không được nhập vào. Tuy nhiên, chúng tôi không quan sát được tác động tích cực từ việc làm như vậy trong thí nghiệm sơ bộ của chúng tôi⁴. Do đó, chúng tôi quay về chỉ tạo ra một cách xác định a'i tại bước thời gian i.⁵ Chúng tôi xác định vị trí bước thời gian bằng cách nhắc nhở tác nhân phản tư xuất ra theo định dạng: "Đối với chỉ số hành động= i, bạn nên a'i". Điều này khác với nghiên cứu trước đây (Shinn et al., 2023) sử dụng bộ nhớ phản tư như các mục dính trong prompt LLM qua tất cả các bước thời gian.

6 Thí Nghiệm
Chúng tôi bắt đầu với việc phân loại các nhiệm vụ theo độ phức tạp lập kế hoạch của chúng để có một bàn thử nghiệm riêng biệt. Sau đó chúng tôi thí nghiệm với lập kế hoạch theo giai đoạn của chúng tôi trong Mục 6.3-6.4. Cuối cùng, chúng tôi kiểm tra xem tác nhân zero-shot của chúng tôi có thể học từ lỗi hay không trong Mục 6.5. Các prompt của chúng tôi có trong Phụ lục A-E. Kết quả hoàn chỉnh có trong Phụ lục F.

³Đến thời điểm này, quy trình làm việc tương tự như refine-then-resume trong ADAPLANNER (Sun et al., 2023).
⁴Một lý do có thể là bộ hướng dẫn trong prompt LLM đã dày đặc và prompt phản tư có xu hướng dài, do đó những yêu cầu tinh tế như vậy đôi khi bị bỏ qua.
⁵Nhược điểm là tác nhân có thể lặp lại giữa hai hành động không phải click qua nhiều thử nghiệm phản tư.

6.1 Thiết Lập
Chúng tôi tập trung vào 43 nhiệm vụ MINIWOB++ phù hợp để đánh giá các mô hình dựa trên ngôn ngữ. Điều này khác với nghiên cứu trước đây vì chúng tôi loại trừ những nhiệm vụ 1) yêu cầu tín hiệu thị giác để giải quyết (ví dụ, count-shape và grid-coordinate); và 2) tiết lộ API ngôn ngữ không đủ để vận hành (ví dụ, enter-date và enter-time); Động cơ cho việc lọc này đơn giản: ngay cả khi một số nhiệm vụ được lọc có thể được giải quyết bởi một tác nhân LLM, nó không tổng quát hóa. Hơn nữa, chúng tôi không bao gồm terminal vì console tổng hợp hỗ trợ một tập lệnh rất hạn chế trong khi LLM, trong thí nghiệm sơ bộ của chúng tôi, có xu hướng sử dụng những cái phức tạp hơn.

Chúng tôi chia 43 nhiệm vụ này thành ba loại: 1) 1-screen-1-step, 2) 1-screen-n-step, và 3) n-screen-n-step. Nếu nhiệm vụ liên quan đến cập nhật trạng thái (ví dụ mở rộng danh sách dropdown hoặc mở tab ẩn), nhiệm vụ đó là n-screen. Nếu nhiệm vụ có thể được giải quyết chỉ bằng một hành động, nó là 1-step; nếu không thì n-step. Phân phối nhiệm vụ được báo cáo trong Bảng 4.⁶

Đối với mỗi nhiệm vụ, chúng tôi đánh giá với 25 seed ngẫu nhiên khác nhau, bắt đầu từ seed= 1000, tương tự như Pix2Act (Shaw et al., 2023). Hiệu suất được báo cáo như tỷ lệ hoàn thành đúng trên nhiều lần chạy. Để xác thực và thiết kế prompt, chúng tôi sử dụng seed ∈ [0,10]. Đối với tác nhân LLM, chúng tôi sử dụng FLAN-PaLM2 L (Anil et al., 2023) với nhiệt độ 0 trên tất cả các đánh giá để tái tạo tốt hơn.

[Bảng 4: Phân phối nhiệm vụ cho mỗi loại trong MINIWOB++]

6.2 So Sánh Mô Hình
Đối với mỗi loại nhiệm vụ, chúng tôi so sánh với các mô hình tốt nhất trước đây dựa vào ngôn ngữ như tín hiệu đầu vào, bao gồm các mô hình có giám sát, tức là WEBNT5(Gur et al., 2022) và CC-NET(Humphreys et al., 2022), và các tác nhân dựa trên suy luận được nhắc nhở, tức là RCI (Kim et al., 2023) với GPT-3.5 và ADAPLANNER (Sun et al., 2023). Đối với các mô hình few-shot, chúng tôi tập trung vào so sánh với các tác nhân có khả năng phản tư. Các tác nhân không phản tư, như SYNAPSE (Zheng et al., 2023), có các kỹ thuật trực giao với công trình của chúng tôi, và do đó có thể

⁶Dựa trên phân loại của chúng tôi, vấn đề màn hình (Mục 1) ảnh hưởng đến loại n-screen-n-step.

--- TRANG 7 ---
Hình 4: Hiệu suất trên các nhiệm vụ 1-screen-1-step. click-widget chứa mục tiêu nhiệm vụ mơ hồ, do đó phản tư giúp ích.

được kết hợp với chúng tôi. Hơn nữa, chúng tôi nhận thấy mỗi nghiên cứu thường sử dụng tập nhiệm vụ hơi khác nhau. Để so sánh công bằng, chúng tôi cũng sẽ báo cáo hiệu suất trên tập nhiệm vụ chung.

6.3 Nhiệm Vụ Một Bước
Chúng tôi so sánh tác nhân zero-shot của chúng tôi trên loại nhiệm vụ dễ nhất (1-screen-1-step) với các state-of-the-art gần đây. Như được hiển thị trong Hình 4, tác nhân của chúng tôi đạt được độ chính xác 100% trong việc hoàn thành đúng 9 nhiệm vụ, ngay cả không có phản tư. Một ngoại lệ là click-widget mơ hồ mà, không có prompt vết trong ngữ cảnh, có thể dễ dàng thất bại. Ví dụ, nhiệm vụ có thể yêu cầu tác nhân nhấp vào text widget, tuy nhiên, input text và text area không được coi là đúng. Với 3 vòng thử nghiệm phản tư, tác nhân của chúng tôi đạt được tỷ lệ hoàn thành 96%. Nhìn chung, chúng tôi có 96.4% với 1 thử nghiệm, và 99.6% với 3 thử nghiệm. So sánh, với các ví dụ vết few-shot, RCI (Kim et al., 2023) đạt được 99.8% với 1 vòng phản tư (ở cấp độ kế hoạch).

6.4 Lập Kế Hoạch Lặp Lại so với Lập Kế Hoạch Theo Giai Đoạn
Chúng tôi so sánh hai cách tiếp cận này sử dụng các nhiệm vụ 1-screen-n-step. Chúng tôi hy vọng những thí nghiệm này có thể trả lời rằng, với một trạng thái đã cho, liệu người ta có nên truy vấn tác nhân cho các hành động từng cái một hay tất cả cùng một lúc. Chúng tôi so sánh các nghiên cứu state-of-the-art trước đây với lập kế hoạch theo giai đoạn của chúng tôi trong Bảng 5, cho thấy rằng người ta có thể đơn giản lập kế hoạch tất cả các hành động có thể thực hiện trên một màn hình và "mù quáng" thực hiện chúng. Làm như vậy có thể giảm đáng kể các truy vấn LLM và vẫn đạt được tỷ lệ hoàn thành cao.

Chúng tôi báo cáo tỷ lệ hoàn thành chi tiết trên tất cả 20 nhiệm vụ 1-screen-n-step trong Hình 5. Tác nhân của chúng tôi đạt được 94.0% hoàn thành trong 1 thử nghiệm, và 96.2% trong 3 thử nghiệm.

6.5 Lập Kế Hoạch Phản Tư trên Các Nhiệm Vụ Thách Thức
Ở đây, chúng tôi chuyển sang các nhiệm vụ thách thức hơn (n-screen-n-step) để cho thấy tác động của phản

[Bảng 5: Hiệu suất trung bình trên các nhiệm vụ 1-screen-n-step, 16 được chia sẻ trên tất cả các mô hình. T: số lượng thử nghiệm.]

Hình 5: Hiệu suất trên các nhiệm vụ 1-screen-n-step.

tư hiệu quả của chúng tôi. Tỷ lệ hoàn thành theo nhiệm vụ được báo cáo trong Hình 6. Thứ nhất, chúng tôi quan sát thấy không có vết mẫu, tác nhân zero-shot có xu hướng thất bại ở thử nghiệm đầu tiên. Điều này xảy ra thường xuyên trong các nhiệm vụ yêu cầu khám phá qua nhiều màn hình, ví dụ, click-menu-2, click-tab-2-hard, và search-engine. Sau một vài vòng khám phá, tác nhân của chúng tôi đạt được tỷ lệ hoàn thành tốt hơn đáng kể bằng cách tránh các tín hiệu tiêu cực trước đó được ghi lại trong bộ nhớ. Tác nhân của chúng tôi tiếp tục cải thiện ngay cả với T=5, cho thấy phản tư hiệu quả hơn so với nghiên cứu trước đây ví dụ, RCI chỉ có khả năng một vòng phản tư ở cấp độ kế hoạch.

[Bảng 6: So sánh trên 11 nhiệm vụ được chia sẻ trên các mô hình khác nhau trong loại n-screen-n-step. T: số lượng thử nghiệm.]

Một lần nữa, chúng tôi so sánh với các mô hình tốt nhất trước đây trong Bảng 6. Các mô hình few-shot đã khai thác các màn hình không nhất quán (như đã thảo luận trong Mục 1), do đó công trình của chúng tôi ở trong tình thế bất lợi so với họ. Mặc dù bất lợi như vậy, tác nhân của chúng tôi đạt được hiệu suất tương đương với họ. Quan trọng, tác nhân của chúng tôi không yêu cầu các ví dụ vết trong ngữ cảnh cho prompting few-shot, đôi khi many-shot, và không có phản hồi môi trường tùy chỉnh và chi tiết. Cuối cùng, chúng tôi lưu ý rằng khoảng cách trên các nhiệm vụ phức tạp giữa mô hình có giám sát và không giám sát vẫn còn lớn.

--- TRANG 8 ---
Hình 6: Hiệu suất trên các nhiệm vụ n-screen-n-step.

7 Phân Tích & Thảo Luận
7.1 Ablation về Các Chiến Lược Phản Tư
Ở đây, chúng tôi so sánh phản tư có cấu trúc của chúng tôi với cơ chế phản tư "gốc". Chúng tôi nên lưu ý rằng phản tư là một phạm vi chung có các hình thành khác nhau (ví dụ Shinn et al., 2023; Madaan et al., 2023) và được giới thiệu trên các môi trường (ví dụ, ALFWORLD) khác đáng kể so với MINIWOB++. Hơn nữa, nó thường được sử dụng cùng với chiến lược lập kế hoạch lặp lại, không tương thích trực tiếp với lập kế hoạch theo giai đoạn của chúng tôi.

Hình 7: So sánh các chiến lược phản tư với T=3.

Do đó, chúng tôi sử dụng một phiên bản thích ứng để so sánh: một tác nhân sử dụng timestep được quản lý cấu trúc⁷ trong khi quản lý tư duy cấu trúc⁸ bị tắt. Thiết lập này là so sánh giữa Both vs. w/o structured mem trong Hình 7 nơi chúng tôi chọn 5 nhiệm vụ thách thức và chạy 25 seed cho mỗi thiết lập. Rõ ràng, phản tư có cấu trúc của chúng tôi là một bổ sung có lợi.

⁷Chúng tôi chèn tư duy phản tư tại timestep tương ứng để các hành động trước timestep này có thể được phát lại một cách xác định để hiệu quả tốt hơn.
⁸Trên cơ sở timestep được quản lý cấu trúc, chúng tôi cũng quản lý việc hết hạn của các tư duy qua nhiều thử nghiệm, cũng như ràng buộc không gian hành động.

7.2 Ablation về Ràng Buộc Hành Động
Một kỹ thuật hữu ích mà chúng tôi đề xuất trong Mục 5.2 là xóa trường id trong mã giả HTML để một cách heuristic ngăn cản tác nhân LLM đưa ra hành động tương ứng. Điều này về cơ bản là thay đổi tối thiểu đối với đầu vào. Trong Hình 7, chúng tôi ablate về thay đổi nhỏ này bằng cách so sánh Both vs. w/o action constraint và cho thấy rằng việc áp dụng ràng buộc hành động là có lợi.

7.3 Ý Nghĩa Thống Kê qua Các Thử Nghiệm
Chúng tôi đánh giá ý nghĩa thống kê qua các thử nghiệm khác nhau trên các nhiệm vụ n-screen-n-step. Đối với mỗi nhiệm vụ, chúng tôi xem xét tất cả 25 dự đoán mẫu. Điều này mang lại cho chúng tôi 13×25=325 mẫu cho mỗi so sánh. Sử dụng t-test (Dror et al., 2018), kết quả thực sự có ý nghĩa (p<0.05) như được hiển thị trong Bảng 7. Để biết ý nghĩa theo nhiệm vụ, xem Phụ lục G.

[Bảng 7: Kiểm định ý nghĩa sử dụng t-test so sánh số lượng thử nghiệm khác nhau.]

7.4 Giảm Lời Gọi Lập Kế Hoạch
Trong Bảng 8, chúng tôi nêu bật sự tăng hiệu quả bằng cách sử dụng công thức lập kế hoạch theo giai đoạn của chúng tôi. Chúng tôi minh họa kết quả trên các nhiệm vụ 1-screen-n-step yêu cầu vết hành động tương đối dài (≥7 hành động) trên một màn hình duy nhất, và so sánh số lượng lời gọi lập kế hoạch cho các vết hoàn thành cũng như các vết thất bại.

[Bảng 8: Giảm lời gọi lập kế hoạch bằng lập kế hoạch theo giai đoạn...]

7.5 Màn Hình Gọn Nhẹ & Giới Hạn Độ Dài Đầu Vào
Biểu diễn giao diện người dùng như HTML đặt nhu cầu cao về khả năng ngữ cảnh của LLM. Một trường hợp là nhiệm vụ social-media-all có thể trải rộng hơn một chục ứng viên, mỗi cái với nhiều tùy chọn. Kết quả là, việc làm phẳng tập hợp hoàn chỉnh các cặp trạng thái-hành động có thể dễ dàng vượt quá giới hạn đầu vào cho tác nhân phản tư, vì nó cần quan sát toàn bộ vết. Trên nhiệm vụ này, chúng tôi nhận thấy rằng các hành động tinh tế không thay đổi đáng kể màn hình. Do đó, chúng tôi luôn tuân thủ màn hình đầu tiên khi xây dựng prompt cho tác nhân phản tư. Một phương pháp tự động hơn có thể là lọc trạng thái trong SYNAPSE (Zheng et al., 2023).

Việc sắp xếp sự tách biệt những phần tử HTML nào để tiết lộ cho LLM là quan trọng cho đánh giá. Như chúng ta đã thấy rằng nhiều nhiệm vụ MiniWoB++ đã dễ dàng cho LLM ngày nay. Việc tiết lộ nhiều phần tử không nhìn thấy có nguy cơ che giấu thách thức thực tế trong các nhiệm vụ điều hướng. Ví dụ, việc tiết lộ các phần tử không nhìn thấy về cơ bản đơn giản hóa các nhiệm vụ n-screen-n-step thành các nhiệm vụ 1-screen-n-step. Tuy nhiên, thí nghiệm của chúng tôi cho thấy rằng các nhiệm vụ n-screen-n-step thực sự khó xử lý hơn nhiều.

7.6 Khả Năng Lập Kế Hoạch Theo Giai Đoạn
Để hiểu rõ hơn khả năng lập kế hoạch và giới hạn của lập kế hoạch theo giai đoạn của chúng tôi, chúng tôi thí nghiệm với các nhiệm vụ 1-screen-n-step có số lượng ứng viên rộng rãi. Cụ thể, chúng tôi sử dụng click-checkboxes và social-media làm nhiệm vụ thăm dò, và báo cáo trong Bảng 9. Cả hai nhiệm vụ đều là lựa chọn nhiều, nhưng khác nhau về độ phức tạp cấu trúc ứng viên của chúng.

[Bảng 9: Tác động của số lượng hành động ứng viên/vàng đến hoàn thành nhiệm vụ...]

Đối với nhiệm vụ click-checkboxes, chúng tôi tách các ví dụ theo số lượng hành động yêu cầu⁹. Biểu diễn màn hình cho nhiệm vụ này tương đối đơn giản vì mỗi checkbox tương ứng với một dòng văn bản. Điều này khác với nhiệm vụ social-media nơi mỗi ứng viên có nhiều mục có thể hành động, đôi khi mơ hồ, do đó đặt yêu cầu mạnh hơn cho LLM để phân biệt. Chúng tôi quan sát một mô hình trong Bảng 9 rằng với màn hình phẳng và ít mơ hồ, LLM có khả năng cao để lập kế hoạch chính xác nhiều bước trong một lời gọi suy luận. Trong trường hợp như vậy, người ta có thể chỉ thực hiện tất cả các bước đã lập kế hoạch mà không cần các lời gọi lập kế hoạch lặp lại. Nhưng với các cấu trúc màn hình phức tạp, khả năng lập kế hoạch một lượt bị giảm một biên độ lớn. Nghiên cứu trước đây (tức là RCI) đã ràng buộc số lượng ứng viên trong nhiệm vụ social-media thành [3,6]. Chúng tôi quan sát thấy việc nới lỏng ràng buộc như vậy giới thiệu khó khăn đáng kể cho lập kế hoạch. Do đó, nhiều thử nghiệm phản tư có thể giúp tác nhân trong những tình huống phức tạp này.

⁹có thể được phân tích heuristic từ lệnh nhiệm vụ.

8 Kết Luận
Chúng tôi đã đề xuất tác nhân zero-shot đầu tiên cho các nhiệm vụ điều khiển máy tính. Thiết kế tác nhân của chúng tôi tổng quát hóa quy trình làm việc cho các nhiệm vụ dễ và phức tạp thông qua lập kế hoạch hiệu quả và quản lý tư duy có cấu trúc. Chúng tôi đã đánh giá tác nhân của mình trên benchmark MINIWOB++, cho thấy rằng tác nhân của chúng tôi, thường với một lượt truy vấn lập kế hoạch, vượt trội hơn tác nhân lập kế hoạch lặp lại tốt nhất cũng như state-of-the-art có giám sát trên các nhiệm vụ đơn giản. Đối với các nhiệm vụ phức tạp, chúng tôi cho thấy rằng thiết kế tác nhân của chúng tôi thực hiện ngang bằng với mô hình dựa trên LLM tốt nhất thông qua lập kế hoạch và phản tư hiệu quả hơn, mà không yêu cầu các prompt vết được tạo thủ công và phản hồi môi trường ad-hoc.

9 Hạn Chế
9.1 Các Lựa Chọn LLM Khác
Chúng tôi tập trung vào các đánh giá dựa trên PaLM-2. Những tiến bộ gần đây trong các tác nhân LLM (ví dụ, Wei et al., 2022; Yao et al., 2023; Shinn et al., 2023) đã cho thấy rằng các LLMs khác nhau (ví dụ, PaLM, GPT-3/4, Codex) nói chung thể hiện khả năng chung để được hưởng lợi từ các suy nghĩ trung gian và tự phê bình. Chúng tôi tin rằng có sự thích ứng hợp lý của các phát hiện của chúng tôi trên các LLMs khác.

9.2 Các Phương Thức Đầu Vào Khác
Các mô hình đa phương tiện lớn có thể nhận các đầu vào bổ sung như hình ảnh màn hình, và các nghiên cứu trước đây (ví dụ, CC-Net (Humphreys et al., 2022)) đã cho thấy rằng phương thức bổ sung thực sự có thể có lợi. Tuy nhiên, ngay cả với các thiết kế gần đây của các mô hình đa phương tiện lớn, lập luận rõ ràng vẫn diễn ra dưới dạng ngôn ngữ. Do đó, đề xuất của chúng tôi có thể có lợi trong những trường hợp sử dụng đa phương tiện như vậy.

9.3 Tích Hợp Zero-shot Chain-of-Thought
Các nghiên cứu zero-shot trước đây (ví dụ, Huang et al., 2022; Wang et al., 2023b; Crispino et al., 2023) đã phát hiện LLMs có thể được sử dụng để mở rộng prompt với

--- TRANG 9 ---
kiến thức trước và các bước trung gian để làm việc theo cách zero-shot. Về mặt lý thuyết, dòng nghiên cứu này cũng có thể được tích hợp vào tác nhân phản tư của chúng tôi để thúc đẩy tỷ lệ hoàn thành trong thử nghiệm đầu tiên. Một thách thức tiềm năng là các nhiệm vụ điều khiển máy tính, nhìn vào các văn bản đầu vào, khá khác biệt so với những cái trong lĩnh vực chung (ví dụ, phân loại cảm xúc, lập luận số). Do đó, chất lượng của kiến thức trước được trích xuất cần được đánh giá. Chúng tôi để lại hướng này để khám phá trong công trình tương lai.

9.4 Ràng Buộc Không Gian cho Các Hành Động Không Phải Click
Trong Mục 5.2, chúng tôi để mô-đun phản tư tương tác với môi trường, vô hiệu hóa rõ ràng các hành động click thất bại bằng cách loại bỏ trường "id" trên các phần tử tương ứng. Điều này thường giúp tác nhân của chúng tôi tránh lặp lại cùng những lỗi, nhưng chỉ cho các hành động click.

9.5 Các Nhiệm Vụ End-to-end Hơn
Các nghiên cứu few-shot gần đây đã sử dụng các kỹ thuật để trích xuất các vết tham chiếu thông tin, từ chuyên gia hoặc khám phá tác nhân (Zheng et al., 2023), để tiến triển các nhiệm vụ máy tính end-to-end hơn, như book-flight. Chúng tôi quan sát thấy các nhiệm vụ end-to-end như vậy vẫn là một thách thức đáng kể đối với tác nhân zero-shot.

9.6 Chu Kỳ Hành Động Bậc Cao Hơn
Trong Mục 5, chúng tôi đã đề xuất quản lý tư duy có cấu trúc để tạo điều kiện cho tự phản tư của tác nhân. Mặc dù mô-đun này có thể giúp tác nhân LLM tránh lặp lại các lỗi trước đó một cách hiệu quả, có những trường hợp góc cần được bao phủ. Trong những trường hợp hiếm hoi, chúng tôi quan sát thấy tác nhân có thể lặp lại giữa hai vết thất bại và khác nhau bằng cách vô tình xóa bộ nhớ phản tư trước đó. Điều này là do tác nhân của chúng tôi coi các phản tư về các bước thời gian sau đó là lỗi thời khi có một mục phản tư cho bước thời gian sớm hơn. Công trình tương lai có thể sử dụng bộ nhớ vết bổ sung để tránh những trường hợp góc như vậy.

Lời Cảm Ơn
Chúng tôi cảm ơn các nhà đánh giá của EMNLP về các nhận xét xây dựng và gợi ý.

Tài Liệu Tham Khảo
[Các tài liệu tham khảo được dịch tương tự...]

A Prompt cho Không Gian Hành Động
Hình 8 chứa prefix prompt nêu ra ba loại hành động.

B Prompt cho Lập Kế Hoạch Theo Giai Đoạn
Hình 9 chứa prefix prompt để lập kế hoạch tối đa các hành động trên mỗi màn hình trong một lượt.

C Prompt cho Diễn Giải Hành Động
Hình 10 chứa prefix prompt để lập kế hoạch tối đa các hành động trên mỗi màn hình trong một lượt.

D Prefix Prompt Trạng Thái
Hình 11 chứa thành phần prompt chúng tôi sử dụng để soạn prompt phản tư cho mỗi loại phản hồi môi trường.

E Prompt cho Phản Tư
Hình 12 chứa prefix prompt cho phản tư có cấu trúc của chúng tôi.

--- TRANG 10 ---
[Các hình ảnh prompt và bảng dữ liệu được dịch tương tự...]

F Bảng Tỷ Lệ Hoàn Thành
Chúng tôi báo cáo tỷ lệ hoàn thành trên 43 nhiệm vụ trong MINIWOB++ theo loại. Hiệu suất trong các nhiệm vụ 1-screen-1-step có trong Bảng 10, nhiệm vụ 1-screen-n-step trong Bảng 11, và các nhiệm vụ n-screen-n-step trong Bảng 12.

G Ý Nghĩa Thống Kê Theo Nhiệm Vụ
Trong Bảng 13, chúng tôi so sánh T=1 vs. T=5 trên các nhiệm vụ n-screen-n-step. Chúng tôi sử dụng McNemar một đuôi (chi-square khớp) cho kiểm định.

[Các bảng dữ liệu chi tiết được dịch tương tự...]
