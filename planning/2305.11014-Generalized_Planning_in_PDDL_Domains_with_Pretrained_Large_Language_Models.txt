# 2305.11014.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/planning/2305.11014.pdf
# File size: 1304370 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Generalized Planning in PDDL Domains with Pretrained Large Language Models
Tom Silver1, Soham Dan2, Kavitha Srinivas2,
Joshua Tenenbaum1, Leslie Kaelbling1, Michael Katz2
1MIT Computer Science and Artificial Intelligence Laboratory;2IBM Research
Correspondence: tslvr@mit.edu, Michael.Katz1@ibm.com
Abstract
Recent work has considered whether large language mod-
els (LLMs) can function as planners: given a task, generate
a plan. We investigate whether LLMs can serve as general-
ized planners: given a domain and training tasks, generate a
program that efficiently produces plans for other tasks in the
domain. In particular, we consider PDDL domains and use
GPT-4 to synthesize Python programs. We also consider (1)
Chain-of-Thought (CoT) summarization, where the LLM is
prompted to summarize the domain and propose a strategy
in words before synthesizing the program; and (2) automated
debugging, where the program is validated with respect to the
training tasks, and in case of errors, the LLM is re-prompted
with four types of feedback. We evaluate this approach in
seven PDDL domains and compare it to four ablations and
four baselines. Overall, we find that GPT-4 is a surprisingly
powerful generalized planner. We also conclude that auto-
mated debugging is very important, that CoT summarization
has non-uniform impact, that GPT-4 is far superior to GPT-
3.5, and that just two training tasks are often sufficient for
strong generalization.1
Introduction
While some classes of sequential decision-making tasks are
provably intractable (Chapman 1987), others can be solved
efficiently with a single domain-specific program. In the lat-
ter case, there is considerable interest in automatically syn-
thesizing these programs given a small number of training
tasks. In AI planning, several approaches to this generalized
planning problem have been proposed, with programs ex-
pressed as lifted decision lists, as finite state machines, or
in domain-specific languages (Srivastava 2011; Bonet and
Geffner 2015; Jim ´enez, Segovia-Aguas, and Jonsson 2019;
Rivlin, Hazan, and Karpas 2020). In reinforcement learning,
goal-conditioned policies and value functions can be under-
stood as particular kinds of programs learned with the same
generalized planning objective (Sutton et al. 2011; Schaul
et al. 2015). Despite these efforts, it remains challenging to
efficiently synthesize programs from few training tasks that
generalize to a wide variety of held-out tasks.
Given the tremendous recent progress in large language
models (LLMs) (Brown et al. 2020; Chen et al. 2021;
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
1Code and logs: https://github.com/tomsilver/llm-genplan/Chowdhery et al. 2022), especially in code generation (Chen
et al. 2021; Nijkamp et al. 2023; Chen et al. 2023a), this
work asks a simple question: can pretrained LLMs be
used for generalized planning? In particular, we investi-
gate whether GPT-4 (OpenAI 2023) can be used to write a
domain-specific Python program that solves a set of tasks in
a planning domain. For each domain, we prompt GPT-4 with
the domain and a small number of training tasks encoded in
the Planning Domain Definition Language (PDDL) (McDer-
mott 2000). We then ask GPT-4 to write a program that con-
sumes a (parsed) task description and outputs a plan. To pre-
vent it from writing domain-general search-based code—a
natural inclination given the association between PDDL and
search in its pretraining data—we instruct GPT-4 to imple-
ment “a simple strategy that does not use search.”
Beyond this basic protocol, we consider two extensions.
First, inspired by Chain-of-Thought (CoT) (Wei et al. 2022;
Jiang et al. 2023), we prompt GPT-4 to write a natu-
ral language summary of the PDDL domain. We then
ask it to describe a solution strategy before finally imple-
menting the strategy in Python. Second, inspired by In-
ner Monologue (Huang et al. 2022b) and Corrective Re-
prompting (Raman et al. 2022), we automatically provide
feedback to GPT-4 in the case where it fails to solve train-
ing tasks. For example, if executing the Python code results
in an exception, we present GPT-4 with that exception and
ask it to fix the code. We repeat this automated debugging
process up to four times or until all training tasks are solved.
See Figure 1 for an overview of this pipeline.
In our experiments, we evaluate this approach on seven
PDDL domains: six from recent work in generalized plan-
ning (Yang et al. 2022), and a seventh novel domain. We
find that the approach is a strong baseline compared to ex-
isting generalized planning approaches. This is an important
finding that we expect to inform further research in general-
ized planning. We also present a suite of ablations and ad-
ditional analyses to unpack the contributions of CoT sum-
marization, automated debugging, names in the PDDL, and
GPT-4 vs. GPT-3.5. Our results suggest that automated de-
bugging, PDDL names, and GPT-4 are very important, while
the impact of CoT is non-uniform. Finally, we provide qual-
itative analyses of common failure cases, suggesting direc-
tions for future work. We conclude that GPT-4 is a surpris-
ingly powerful generalized planner when properly guided.arXiv:2305.11014v2  [cs.AI]  18 Dec 2023

--- PAGE 2 ---
Domain & Train TasksDomain SummaryLLMLLMProposed StrategyLLMGeneralized PlanValDone
CoTSummarizationAutomated DebuggingFigure 1: Overview of pipeline for generalized planning with pretrained LLMs. See text for details.
Related Work
LLMs for (PDDL) Planning. Generalized planning with
LLMs can be seen as an alternative to planning with
LLMs (Sharma, Torralba, and Andreas 2022; Ahn et al.
2022; Huang et al. 2022a; Raman et al. 2022; Lin et al.
2022). Most relevant is work by Valmeekam et al. (2022);
Silver et al. (2022) who consider LLM-based planning
in PDDL domains. There are several advantages to using
LLMs for generalized planning, rather than planning: (1)
programs produced by the LLM can be inspected and vali-
dated; (2) running a synthesized program can be much faster
(and cheaper) than querying the LLM for each new task;
(3) synthesized programs can scale to arbitrarily large tasks,
whereas current LLMs are limited by context window size.
Pallagani et al. (2022) consider fine-tuning an LLM to solve
PDDL tasks. Other recent work has considered using LLMs
for translating between natural language and PDDL (Collins
et al. 2022; Lin et al. 2023; Xie et al. 2023; Liu et al. 2023).
These efforts could be combined with our approach.
Generalized Planning. This work contributes to a grow-
ing literature on generalized planning (Fikes, Hart, and
Nilsson 1972; Jim ´enez, Segovia-Aguas, and Jonsson 2019).
Prior work has considered synthesizing generalized plans in
several ways: (1) performing a search through a hypothesis
class of generalized policies (Levine and Humphreys 2003;
Jim´enez and Jonsson 2015; Segovia-Aguas, Jim ´enez, and
Jonsson 2018, 2021); (2) using example plans to construct
a generalized plan, often represented with a finite-state ma-
chine (Levesque 2005; Srivastava et al. 2011; Winner 2008);
and (3) discovering state and action abstractions and then
using them in a generalized plan (Bonet and Geffner 2018).
One pervasive challenge is that there are often many valid
plans for any given task, and only some of these plans are
consistent with a simple generalized plan. PG3 addresses
this challenge by using candidate generalized plans (rep-
resented as lifted decision list goal-conditioned policies) to
constrain the generation of example plans (Yang et al. 2022).
We use PG3 as the main point of comparison in experiments.
LLMs for Code Generation. Our work builds on recent
techniques that use LLMs for code generation (Chen et al.
2021; Nijkamp et al. 2023). CoT summarization is related to
several techniques that ask the LLM to outline its “thinking”
before arriving at a final implementation (Wei et al. 2022;
Jiang et al. 2023; Zheng et al. 2023). A number of recent
works also use programs as prompts (i.e., a structured chain
of thought) in an attempt to help LLMs perform mathemat-
ical reasoning (Gao et al. 2022; Imani, Du, and Shrivastava
2023). Related to our automated debugging, Xia and Zhang(2023); Chen et al. (2023b) consider automated program re-
pair by re-prompting the LLM with feedback from failed
validation checks. Chen et al. (2023a) consider a related
paradigm, but where feedback comes from humans, rather
than automated checks. Also relevant are efforts to generate
code that can be used for robotic decision-making (Liang
et al. 2022; Singh et al. 2022). Beyond LLMs, code genera-
tion has been studied extensively in program synthesis (Alur
et al. 2013; Gulwani et al. 2017) and inductive logic pro-
gramming (Muggleton 1991; Cropper and Duman ˇci´c 2022).
Background and Problem Setting
PDDL Domains and Tasks. We consider deterministic,
fully-observed planning tasks represented in PDDL. In ex-
periments, we use the STRIPS subset with types and neg-
ative preconditions. We describe PDDL informally and
refer the reader to other references for a formal treat-
ment (McDermott 2000). A PDDL domain is character-
ized by a name, a set of types, a set of predicates, and
a set of operators. For example, in the Delivery domain,
a robot must pick up newspapers from a home base
and then deliver them to certain locations. The domain
has two types: loc andpaper . One predicate is (at
?l - loc) , where?lis a placeholder for a loc ob-
ject. The domain has three operators: (pick-up ?p -
paper ?l - loc) ,(move ?from - loc ?to -
loc) ,(deliver ?p - paper ?l - loc) . For ex-
ample, thepick-up operator in its entirety is:
(:action pick-up
:parameters (?p - paper ?l - loc)
:precondition (and (at ?l)
(isHomeBase ?l)
(unpacked ?p))
:effect (and
(not (unpacked ?p))
(carrying ?p)))
A PDDL task is characterized by a domain, a set of ob-
jects, an initial state, and a goal. An object has a name and
a type, e.g., paper1 - paper . A ground atom is a pred-
icate and a tuple of objects of the appropriate types, e.g.,
(unpacked paper1) . A state consists of a conjunction
of ground atoms that are true, assuming all other ground
atoms to be false. A goal is a conjunction of ground atoms
that must be true in any goal state . (More general goal ex-
pressions are also possible in PDDL.) For example, in De-
livery, the goal may include (satisfied loc1) and
(satisfied loc2) .
Anaction is an operator and a tuple of objects of the
appropriate types, e.g., (pick-up paper1 loc4) . The
operator’s preconditions determine whether the action is ap-

--- PAGE 3 ---
plicable and the effects define what ground atoms would be
added or deleted if the operator is executed. A plan is a finite
sequence of actions. The plan is valid for a task if all actions
are applicable when executed in succession from the initial
state and if the final state is a goal state.
PDDL domains, types, predicates, operators, objects, and
types often include human-readable names like the ones
shown above. These names are not important for standard
AI planners or previous generalized planning approaches.
However, the names are very important for humans—and,
we expect, for LLMs—trying to make sense of the PDDL.
Generalized Planning in PDDL Domains. A general-
ized planning instance is characterized by a PDDL domain
and a distribution of tasks. A small set of training tasks (10
or fewer in experiments) from the distribution is given at
training time. A set of held-out evaluation tasks —typically
involving many more objects—are used to measure perfor-
mance. The objective is to use the training tasks to synthe-
size a program that will produce valid plans for all of the
evaluation tasks. We consider an evaluation task solved if
the program returns a valid plan within a fixed wall-clock
time budget (30 seconds in experiments). In other words, we
are interested in satisficing, not optimal, planning, and our
primary concern is the efficiency of planning itself.
Generalized Planning with LLMs
We are interested in the extent to which pretrained large
language models (LLMs) can be used for generalized
planning in PDDL domains. We assume familiarity with
LLMs (Brown et al. 2020; Chen et al. 2021; Chowdhery
et al. 2022; OpenAI 2023). To use LLMs for generalized
planning, we need to define a protocol for prompting.
Prompting Protocol
Previous work on Chain-of-Thought (CoT) prompting has
shown that asking an LLM to “think step by step” can im-
prove performance in reasoning tasks (Wei et al. 2022).
With these results in mind, we hypothesized that decompos-
ing generalized planning into three stages—domain summa-
rization, strategy proposal, and strategy implementation—
would improve performance.
Domain Summarization. Our first prompt to the LLM is
in the following form:
Domain: [PDDL Domain]
Example problems: [PDDL Training Tasks]
Write a short summary of this domain in words.
To compensate for the limited context window size of
transformer-based LLMs like GPT-4, we abbreviate the en-
coding of the training tasks in two ways. First, we always use
only two training tasks, even when more are given. Second,
within each training task, we limit the number of objects and
initial state ground atoms shown. For each object type, if the
number of objects of that type exceeds 10, we truncate the
object set and add ellipses. Similarly, for each predicate, if
the number of ground atoms with that predicate exceeds 10,
we truncate and add ellipses. The fact that we only need tocommunicate the “gist” of the task distribution, rather than
whole tasks, is another advantage of generalized planning
with LLMs versus planning with LLMs.
Strategy Proposal. After the LLM responds to the first
prompt, we ask for a generalized planning strategy:
There is a simple strategy for solving all problems in this
domain without using search. What is that strategy?
In preliminary experiments, omitting the phrase “without
using search” would often lead the LLM to propose a search-
based planning strategy.
Strategy Implementation. Finally, we ask the LLM to
implement the strategy as a Python program:
Implement the strategy as a Python function. The code should
be of the form
def get_plan(objects, init, goal):
# Your code here
return plan
where
•objects is a set of (object name, type name) tuples
•init is a set of ground atoms represented as tuples of pred-
icate names and arguments (e.g., (‘predicate-foo’, ‘object-
bar’, ...))
•goal is also a set of ground atoms represented in the same
way
•plan is a list of actions, where each action is a ground oper-
ator represented as a string (e.g., ‘(operator-baz object-qux
...)’)
In domains without object types, objects is instead just
a set of object names.
Automated Interactive Debugging
After the LLM has proposed an implementation of
getplan , we use the training tasks to validate the imple-
mentation. For each training task, we execute getplan
until it returns an output, throws an exception, or reaches a
timeout (30 seconds). If the output is a valid plan, we con-
tinue onto the next training task. Otherwise, we re-prompt
with one of four types of feedback.
Python Exceptions. If executing getplan results in a
Python exception, we capture the traceback and report it to
the LLM along with the input. An example is shown below,
with the traceback abbreviated for clarity.
Given this task: [PDDL Training Task]
The code raised the following exception:
File "<file-name-omitted>", line 86
lift_at = {atom[1]: atom[2] ...}
˜˜˜˜ˆˆˆ
IndexError: tuple index out of range
Fix the code.

--- PAGE 4 ---
In preliminary experiments, we found that including the
full traceback can improve performance.
Timeout. Ifgetplan does not finish before the time-
out, we report to the LLM that the program did not finish
and suggest that an infinite loop may be to blame. We also
provide a traceback showing where the program was execut-
ing when it was interrupted. An example is shown below.
Given this task: [PDDL Training Task]
The code raised the following exception:
File "<file-name-omitted>", line 23
while not any(span_loc[1] == ...:
ˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆ
KeyboardInterrupt
The code was interrupted because it timed out (possible
infinite loop).
Fix the code.
The traceback is again abbreviated for clarity. Note that
theKeyboardInterrupt is automatically thrown after
30 seconds. In practice, nearly all timeouts we observe are
due to logic errors in the code, rather than inefficient but
correct implementations.
Plan Syntax. Ifgetplan returns an output, we check
its syntax: whether it is a list of strings, whether each string
is enclosed in parentheses and space-separated, and whether
the action names, object names, and number of objects per
action are valid with respect to the domain and task. If any
of these checks fail, we report the failure to the LLM. For
this type of failure, we also remind the LLM about the valid
operators. An example is shown below.
Given this task: [PDDL Training Task]
The code returned this plan:
['walk r0_c0 r0_c1', 'walk ...]
However, the action walk r0 c0 r0c1is invalid at
step 0. NOTE: the valid operators are: (climb ?from
?to) (walk ?from ?to) .
Fix the code.
The full plan is shown to the LLM but abbreviated in the
example for clarity. The issue in this example is that the ac-
tions are not enclosed in parentheses.
Plan Semantics. If all of the previous checks pass, we
use the V AL tool (Howey, Long, and Fox 2004) to check
whether the getplan output is a semantically valid plan.
If not, V AL provides “plan repair advice”, e.g., if there is an
action with invalid preconditions. We extract this plan repair
advice and report it to the LLM. Note that we use this advice
not to repair the plan, but rather, to repair the generalized
plan. An example is shown below.Given this task: [PDDL Training Task]
The code failed. It returned the following plan:
['(pick-up paper-1 loc-0)', ...].
NOTE:(pick-up paper-0 loc-0) has an unsat-
isfied precondition at time 3
(Set (at loc-0) to true)
Fix the code.
Additional Details. After re-prompting the LLM, we re-
peat the process of checking the code and reporting any fail-
ures up to four times. To handle rare cases where the LLM
implements its own helper functions and then assumes dur-
ing debugging that the helper functions are still available,
we append each new response from the LLM to a growing
Python file, rather than overwriting the previous responses.
If a failure is still encountered on the last attempt, the final
response is used during evaluation.
Experiments and Results
Through experiments, we address these questions: 1.Can
GPT-4 be used for generalized (PDDL) planning? 2.Are the
synthesized programs efficient? 3.Does CoT summarization
help? 4.Does automated debugging help? 5.To what extent
does GPT-4 rely on names in the PDDL? 6.How does GPT-
4 compare to GPT-3.5? 7.Do each of the four error types
help? 8.How many training tasks are needed?
Experimental Setup
We evaluate nine generalized planning approaches on seven
PDDL domains over 10 random seeds. Tasks are randomly
generated for each seed.
Domains. The first six domains (and tasks) are taken di-
rectly from the previous work by Yang et al. (2022). Of
these, four (Gripper, Miconic, Ferry, Spanner) are standard
planning benchmarks and the other two (Delivery, Forest)
were introduced by that work. The last domain (Heavy) is
new to this work. The pretraining data for GPT-4 is not pub-
licly available, but it is likely that the domain definitions for
at least the four standard domains were included in that data.
However, we believe it is unlikely that generalized plans
were included, and for the Heavy domain, we can guaran-
tee that neither the domain nor generalized plans were in-
cluded. We now briefly describe each domain. Unless other-
wise specified, there are 10 training tasks and 30 evaluation
tasks per domain and seed.
•Delivery : Newspapers at a home base must be delivered
to multiple locations. There are five training tasks with
9–17 objects; evaluation tasks have 70–100 objects.
•Forest : A hiker must navigate a 2D grid to reach a
goal location while climbing hills and avoiding water. A
marked trail leads to the goal, but there are shorter paths
through dirt. There are 4 training tasks with 64-100 ob-
jects; evaluation tasks have 100–144 objects.
•Gripper : Balls must be transported between rooms by a
robot with two grippers. Training tasks have 20–30 ob-
jects; evaluation tasks have 60–80 objects.

--- PAGE 5 ---
Domain GPT-4 No CoT No Debug No Names GPT-3.5 PG3 Policy Eval Plan Compare Random
Delivery 0.90 0.70 0.10 0.10 0.00 1.00 0.00 0.10 0.00
Forest 1.00 1.00 0.62 0.11 0.32 1.00 1.00 0.16 0.03
Gripper 0.90 0.80 0.50 0.10 0.00 1.00 0.00 0.20 0.00
Miconic 0.01 0.13 0.00 0.00 0.00 1.00 0.00 0.10 0.13
Ferry 0.80 0.20 0.26 0.00 0.00 1.00 0.00 0.90 0.00
Spanner 0.10 0.00 0.00 0.00 0.00 1.00 1.00 0.56 0.06
Heavy 0.60 1.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00
Table 1: Fraction of evaluation tasks solved. All results are averaged over 10 random seeds and 30 evaluation tasks per seed.
Figure 2: GPT-4 synthesized program runtime compared to a state-of-the-art planner (Fast Downward). Note the log-log axes.
Each point is a median over 10 newly generated tasks, over all seeds where generalized planning solved all evaluation tasks.
Figure 3: Fraction of evaluation tasks solved by GPT-4 ver-
sus number of debugging steps allowed, averaged over all
domains and seeds. The shaded region is standard error.
•Miconic : Passengers in multiple buildings, each with an
elevator, must be picked up and dropped off on different
floors. Training tasks have 6–30 objects; evaluation tasks
have 11–150 objects.
•Ferry : Cars must be sailed between islands using a ferry
that can carry at most one car. Training tasks have 13–20
objects; evaluation tasks have 30–50 objects.
•Spanner : Wrenches (spanners) and nuts are distributed
along a one-way corridor. An agent must move down the
corridor, pick up wrenches, and tighten the nuts, using
each wrench at most once. Training tasks have 9–15 ob-
jects; evaluation tasks have 30–60 objects.
•Heavy : Items must be stacked into an empty box. An
item can only be stacked on another item if the lat-
ter is heavier. The weight relations are expressed via
a(heavier ?x ?y) predicate. One challenge is inError Type All Success Failure
Python Exception 40.0 28.9 42.5
Plan Semantics 34.0 44.7 31.4
Plan Syntax 13.0 18.4 11.7
Timeout 13.0 8.0 14.4
Table 2: Percentages of error types encountered by GPT-4 in
training tasks over all domains and seeds. “All” is the break-
down for all training tasks; “Success” is the breakdown for
trials where all evaluation tasks were subsequently solved;
“Failure” is the breakdown for the non-Success trials.
determining which item to place into the box first, i.e.,
which item is the heaviest. Training tasks have 3–10 ob-
jects; evaluation tasks have 100–250 objects.
Approaches. We evaluate the main approach, four abla-
tions, and four baselines. The baselines are taken from the
work by Yang et al. (2022); see that work for details.
•GPT-4 : Our main approach with CoT summarization
and automated debugging.
•No CoT : An ablation of the main approach that does not
use CoT summarization. The three initial prompts are
combined and “Write a short summary of this domain in
words.” and “What is that strategy?” are removed.
•No Debug : An ablation of the main approach that does
not use automated debugging. The first implementation
ofgetplan is used for evaluation.
•No Names : An ablation of the main approach where all
names in the PDDL domains and tasks are replaced with
nondescriptive identifiers. For instance, predicates are

--- PAGE 6 ---
renamed to predicate1 ,predicate2 , etc., opera-
tors are renamed to operator1 ,operator2 , etc. Al-
together, the names of the domain, problem, predicates,
operators, variables, types, and objects are ablated.
•GPT-3.5 : GPT-3.5 with CoT summarization and auto-
mated debugging.
•PG3 : The generalized planning approach proposed by
Yang et al. (2022). The synthesized programs are goal-
conditioned policies implemented as lifted decision lists.
Synthesis is performed via heuristic search in policy
space with their novel heuristic.
•Policy Evalulation (PE) : An approach from Yang et al.
(2022) that is identical to PG3 except that the heuristic
used for policy search is sparse: each candidate policy is
scored based on the number of training tasks solved.
•Plan Compare (PC) : Another approach from Yang et al.
(2022) that is identical to PG3 except for the policy
search heuristic: example plans for each training task are
generated offline, and the policy is scored based on its
agreement with the example plans.
•Random : Valid actions are randomly sampled and ex-
ecuted until a dead-end is encountered, the goal is
reached, or a maximum horizon (default 1000, but see
the previous work) is exceeded.
Experimental Details. We used a Macbook Pro laptop
with an M1 chip and 64 GB RAM. Since an API for GPT-
4 is not publicly available, we used the ChatGPT browser
interface for all experiments (including the GPT-3.5 base-
line). The pipeline is fully automated except that prompts
and responses are manually copied and pasted between the
terminal and browser, with the clipboard programmatically
updated. To facilitate reproducibility, we have released all
chat logs and code.
Results and Analysis
Main results are presented in Table 1. Examples of syn-
thesized programs are presented in the appendix. Overall,
the performance of GPT-4 with CoT summarization and au-
tomated debugging is strong in Delivery, Forest, Gripper,
Ferry, and Heavy, and poor in Miconic and Spanner. Note
that the reported success rates are averaged over all LLM
conversations. In practice, performance could be boosted by
restarting the conversation multiple times and using the best-
found program (Chowdhery et al. 2022). The strong per-
formance in Heavy is especially notable. The generalized
planning baselines fail in this domain because lifted deci-
sion lists are overly restrictive as program representations
and cannot discover a concept like “heaviest overall” from
pairwise heavier relations. GPT-4’s ability to write general
Python code is one of its biggest advantages as a general-
ized planning approach.
We also observe that in nearly all cases, GPT-4 either (1)
solves all of the training tasks and then solves all of the eval-
uation tasks; or (2) fails to solve at least one training task and
then fails to solve all of the evaluation tasks. In other words,
overfitting to the training tasks is very rare, and evaluation
performance is typically all-or-nothing. See Table 3 in the
appendix for the maximum fraction of tasks solved.Miconic failures. GPT-4 has a number of consistent fail-
ure modes in Miconic. First, at the strategy proposal level,
it often fails to recognize that there can be multiple build-
ings, each with their own elevator. This is admittedly diffi-
cult to recognize given the PDDL encoding: buildings ex-
ist only implicitly based on the above relation between
floor objects. For example, one would need to see that
neither(above f1 b1, f1b2) nor(above f1 b2,
f1b1) are true and conclude that the floors are in two
different buildings. However, especially after automated de-
bugging, GPT-4 canrealize that there are multiple buildings,
and furthermore, that building names (e.g., b1,b2) can be
extracted from the floor names. But then other failures often
occur, for example, attempting and failing to create a total
ordering of the floors from the above predicate. Overall,
we believe that Miconic is just beyond the limit of GPT-4’s
current capabilities and would likely be solved by the next
generation of LLMs, or by GPT-4 with additional guidance.
Spanner failures. GPT-4 consistently fails in Spanner
during strategy proposal. In particular, GPT-4 does not ap-
pear to realize that locations in Spanner are connected in a
one-way chain. The strategy proposed is often “first collect
all of the spanners, then tighten all of the nuts” or similar.
A correct strategy would instead be to “move to each lo-
cation in the chain, picking up any spanners and tightening
any nuts at each location.” Recognizing the existence of the
one-way chain requires examining the link atoms in the
training problems. Even after automated debugging, GPT-4
often assumes, incorrectly, that links are commutative.
Program efficiency. Although we prompt the LLM to
implement a “simple” program that does not use search,
it is still possible for the LLM to produce a program that
does use search or is slow for other reasons (e.g., poor algo-
rithmic complexity). We therefore measure synthesized pro-
gram runtime. As a baseline for our comparison we use a
state-of-the-art domain-independent PDDL planner LAMA
(Richter and Westphal 2010) via Fast Downward (Helmert
2006), stopping after the first plan is found.2In Figure 2, we
plot wall-clock runtimes as a function of problem size (num-
ber of objects). Overall, we see that the synthesized pro-
grams not only scale favorably with respect to the planner,
but also consistently beat the planner in absolute runtime
by large margins. This is notable given that the LLM syn-
thesizes Python programs, while the PDDL planner uses a
highly optimized combination of Python and C++ code. The
bottleneck for Fast Downward is often operator grounding.
The LLM’s programs do not need to ground operators—they
can go directly from task to plan.
The role of CoT. Comparing GPT-4 to No CoT, we see
that the impact of CoT summarization is mixed: it seems to
help in most cases, but hurt in Miconic and Heavy. Miconic
is an especially interesting case. When using CoT summa-
rization, GPT-4 nearly always proposes a “sweep” strategy,
where the elevator(s) are first moved to the bottom floor;
then moved up one floor at a time until the top floor, picking
up and dropping off passengers along the way; then moved
2Our intention is not to compare planners, but rather to provide
a frame of reference for runtime.

--- PAGE 7 ---
down one floor at a time, again picking up and dropping
off passengers. This strategy would work in theory, but it
requires finding a total ordering of floors within buildings.
Without CoT, GPT-4 often attempts a different strategy: pick
up, move, and drop off each passenger, one at a time. The
latter strategy does not require a total ordering over floors
and is arguably simpler to implement in Python. This exam-
ple shows that CoT can influence the strategy proposed by
GPT-4. Moreover, strategies that are “simple” to describe in
natural language may not be simple to implement in code. In
Heavy, there is not a clear difference in strategies with and
without CoT. Since a good strategy is evidently discernible
from the PDDL alone, it is possible that CoT “distracts”
GPT-4 during implementation.
The role of automated debugging. Comparing GPT-4 to
No Debug, we see that automated debugging generally im-
proves performance dramatically. Figure 3 shows that even
one step of automated debugging helps substantially, and
further steps exhibit diminishing marginal improvements.
Table 2 reports the fraction of error types encountered dur-
ing training across. Python exceptions are most common,
followed closely by errors in plan semantics, then errors in
plan syntax, and finally timeouts. We also see that the error
types are well-distributed within successful trials, suggest-
ing that each of the four types of feedback are beneficial.
In general, GPT-4 tends to make small, local corrections to
code during automated debugging. If the code is structurally
flawed and requires a significant rewrite, restarting the dia-
logue from the beginning may be required.
The role of PDDL names. Examining the results for the
No Names ablation, we see performance overall is very poor.
This confirms our hypothesis that the terms present in the
PDDL domains and tasks are helpful to the LLM, as they
would be to a human. Note that planners like Fast Downward
and generalized planners like PG3 would be unaffected by
name changes. However, there are a few cases where the No
Names ablation does succeed, suggesting that the LLM has
some capacity for purely syntactic generalized planning.
GPT-3.5 vs. GPT-4. Examining the results for GPT-3.5,
we see that it performs much worse than GPT-4. This is con-
sistent with other reports (OpenAI 2023; Bubeck et al. 2023)
that GPT-4 is far superior on reasoning and coding tasks.
Qualitatively, the programs proposed by GPT-3.5 are flawed
in myriad ways and do not usually appear “close”. They also
do not seem to improve with automated debugging.
Data efficiency. In the appendix, we analyze the number
of training tasks used in each successful trial. A training task
is used if it appeared in the prompt and/or triggered feedback
during automated debugging. Since two training tasks are
always used in the prompt, the minimum used is two. Inter-
estingly, in the vast majority of cases, only those two train-
ing tasks are used. During automated debugging, these two
prompting tasks are always checked first, and most of the
time, they are sufficient to identify issues. In a small number
of cases, a third task is also used during automated debug-
ging. This result speaks to the strong few-shot learning ca-
pabilities of GPT-4. We expect that in many cases, even one
training task would suffice, although we did witness a drop
in performance in preliminary experiments with one task.Discussion and Future Work
In this work, we showed that GPT-4 with CoT summariza-
tion and automated debugging is a surprisingly strong gen-
eralized planner in PDDL domains. We conclude with limi-
tations of this work, reflections about the implications of our
findings, and opportunities for future work.
Limitations. A major limitation of this work and previ-
ous work on generalized planning is that it is easy enough
to hand-design generalized plans for all of the domains con-
sidered. Nonetheless, we expect this line of work to be prac-
tically useful for at least three reasons. (1) In some cases,
it may be considerably easier to specify PDDL domain and
problem descriptions than it is to directly specify a general-
ized plan. (2) In a fully autonomous system, where opera-
tors and predicates are learned in association to natural lan-
guage , we would want the system to also synthesize gener-
alized plans autonomously. (3) Beyond PDDL, generalized
planning with LLMs would be an even more attractive op-
tion, since other approaches rely strongly on formal specifi-
cations. Another limitation of this work is our use of training
tasks to communicate the task distribution of interest to the
LLM. In general, a few example tasks may be insufficient to
express the full distribution. Other representations like nat-
ural language or procedural generation code may be better,
but would require more human input.
Is (generalized) planning now obsolete? No. First, there
remains a performance gap between GPT-4 and PG3, and
other generalized planners may be even better. However,
even if this gap is closed by the next generation of LLMs, we
would still say no. Planning remains essential in domains
where no simple program exists. An interesting direction
for future work would be automatically detecting whether
a simple program might exist before attempting to synthe-
size one. We tried the Sokoban domain and found that GPT-
4 correctly indicates that no simple program exists. How-
ever, this property of Sokoban is well-known, so it is likely
parroting pretraining data. We also tried the Slitherlink do-
main, which was featured in the 2023 International Planning
Competition, and found that GPT-4 did notrecognize that no
simple strategy exists (Takayuki 2000). Generalized plan-
ning without LLMs also remains important in cases where
domain descriptions are not human-readable, e.g., because
the predicates or operators are learned (Silver et al. 2023).
Even with natural language descriptions, combining “classi-
cal” approaches with LLMs may be best.
What if we gave the LLM access to a planner? Giving
an LLM access to APIs is a very powerful idea (Schick et al.
2023) and one such API could be a PDDL planner (Liu et al.
2023). An LLM could potentially use such a planner for
generalized planning, especially given that approaches like
PG3 rely on access to a planner to generate example plans.
In some domains, generating example plans naively would
likely confuse the LLM. For example, plans generated in
the Forest domain would follow arbitrary paths through the
dirt rather than following the slightly longer marked trail. In
other cases, though, example plans could be very useful, es-
pecially if the LLM generates them in a targeted way. Lever-
aging diverse plans (Sohrabi et al. 2016; Katz and Sohrabi
2020) could be particularly useful.

--- PAGE 8 ---
References
Ahn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes,
O.; David, B.; Finn, C.; Gopalakrishnan, K.; Hausman,
K.; Herzog, A.; et al. 2022. Do as I can, not as I say:
Grounding language in robotic affordances. arXiv preprint
arXiv:2204.01691 .
Alur, R.; Bodik, R.; Juniwal, G.; Martin, M. M.;
Raghothaman, M.; Seshia, S. A.; Singh, R.; Solar-Lezama,
A.; Torlak, E.; and Udupa, A. 2013. Syntax-guided synthe-
sis. IEEE.
Bonet, B.; and Geffner, H. 2015. Policies that generalize:
Solving many planning problems with the same policy. In
Twenty-Fourth International Joint Conference on Artificial
Intelligence .
Bonet, B.; and Geffner, H. 2018. Features, Projections, and
Representation Change for Generalized Planning. CoRR ,
abs/1801.10055.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems , 33: 1877–
1901.
Bubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;
Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,
S.; et al. 2023. Sparks of artificial general intelligence: Early
experiments with gpt-4. arXiv preprint arXiv:2303.12712 .
Chapman, D. 1987. Planning for Conjunctive Goals. Artifi-
cial Intelligence , 32: 333–377.
Chen, A.; Scheurer, J.; Korbak, T.; Campos, J. A.; Chan,
J. S.; Bowman, S. R.; Cho, K.; and Perez, E. 2023a. Improv-
ing Code Generation by Training with Natural Language
Feedback. arXiv:2303.16749.
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;
Kaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,
G.; et al. 2021. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374 .
Chen, X.; Lin, M.; Sch ¨arli, N.; and Zhou, D. 2023b. Teach-
ing large language models to self-debug. arXiv preprint
arXiv:2304.05128 .
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;
Gehrmann, S.; et al. 2022. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 .
Collins, K. M.; Wong, C.; Feng, J.; Wei, M.; and Tenen-
baum, J. B. 2022. Structured, flexible, and robust: bench-
marking and improving large language models towards more
human-like behavior in out-of-distribution reasoning tasks.
arXiv preprint arXiv:2205.05718 .
Cropper, A.; and Duman ˇci´c, S. 2022. Inductive logic pro-
gramming at 30: a new introduction. Journal of Artificial
Intelligence Research , 74: 765–850.
Fikes, R. E.; Hart, P. E.; and Nilsson, N. J. 1972. Learn-
ing and executing generalized robot plans. Artificial intelli-
gence , 3: 251–288.Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y .;
Callan, J.; and Neubig, G. 2022. PAL: Program-aided Lan-
guage Models. arXiv preprint arXiv:2211.10435 .
Gulwani, S.; Polozov, O.; Singh, R.; et al. 2017. Program
synthesis. Foundations and Trends® in Programming Lan-
guages , 4(1-2): 1–119.
Helmert, M. 2006. The fast downward planning system.
Journal of Artificial Intelligence Research , 26: 191–246.
Howey, R.; Long, D.; and Fox, M. 2004. V AL: Automatic
plan validation, continuous effects and mixed initiative plan-
ning using PDDL. In 16th IEEE International Conference
on Tools with Artificial Intelligence , 294–301. IEEE.
Huang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022a.
Language Models as Zero-Shot Planners: Extracting Action-
able Knowledge for Embodied Agents. In International
Conference on Machine Learning (ICML) .
Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Flo-
rence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar,
Y .; et al. 2022b. Inner Monologue: Embodied Reasoning
through Planning with Language Models. arXiv preprint
arXiv:2207.05608 .
Imani, S.; Du, L.; and Shrivastava, H. 2023. MathPrompter:
Mathematical Reasoning using Large Language Models.
arXiv:2303.05398.
Jiang, X.; Dong, Y .; Wang, L.; Shang, Q.; and Li, G.
2023. Self-planning Code Generation with Large Language
Model. arXiv preprint arXiv:2303.06689 .
Jim´enez, S.; and Jonsson, A. 2015. Computing plans with
control flow and procedures using a classical planner. In
Proceedings of the Eighth Annual Symposium on Combina-
torial Search, SOCS-15 , 62–69.
Jim´enez, S.; Segovia-Aguas, J.; and Jonsson, A. 2019. A re-
view of generalized planning. The Knowledge Engineering
Review , 34.
Katz, M.; and Sohrabi, S. 2020. Reshaping diverse plan-
ning. In Proceedings of the AAAI Conference on Artificial
Intelligence , 06, 9892–9899.
Levesque, H. 2005. Planning with Loops. In IJCAI .
Levine, J.; and Humphreys, D. 2003. Learning action strate-
gies for planning domains using genetic programming. In
Workshops on Applications of Evolutionary Computation ,
684–695. Springer.
Liang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,
B.; Florence, P.; and Zeng, A. 2022. Code as policies: Lan-
guage model programs for embodied control. arXiv preprint
arXiv:2209.07753 .
Lin, B. Y .; Huang, C.; Liu, Q.; Gu, W.; Sommerer, S.; and
Ren, X. 2022. On Grounded Planning for Embodied Tasks
with Language Models. arXiv preprint arXiv:2209.00465 .
Lin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J.
2023. Text2motion: From natural language instructions to
feasible plans. arXiv preprint arXiv:2303.12153 .
Liu, B.; Jiang, Y .; Zhang, X.; Liu, Q.; Zhang, S.; Biswas, J.;
and Stone, P. 2023. LLM+ P: Empowering Large Language
Models with Optimal Planning Proficiency. arXiv preprint
arXiv:2304.11477 .

--- PAGE 9 ---
McDermott, D. 2000. The 1998 AI Planning Systems Com-
petition. AI Magazine , 21(2): 35–55.
Muggleton, S. 1991. Inductive logic programming. New
generation computing , 8: 295–318.
Nijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou,
Y .; Savarese, S.; and Xiong, C. 2023. CodeGen: An Open
Large Language Model for Code with Multi-Turn Program
Synthesis. arXiv:2203.13474.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Pallagani, V .; Muppasani, B.; Murugesan, K.; Rossi, F.;
Horesh, L.; Srivastava, B.; Fabiano, F.; and Loreggia, A.
2022. Plansformer: Generating Symbolic Plans using Trans-
formers. arXiv preprint arXiv:2212.08681 .
Raman, S. S.; Cohen, V .; Rosen, E.; Idrees, I.; Paulius,
D.; and Tellex, S. 2022. Planning with Large Lan-
guage Models via Corrective Re-prompting. arXiv preprint
arXiv:2211.09935 .
Richter, S.; and Westphal, M. 2010. The LAMA planner:
Guiding Cost-based Anytime Planning with Landmarks.
Journal of Artificial Intelligence Research , 39: 127–177.
Rivlin, O.; Hazan, T.; and Karpas, E. 2020. General-
ized Planning With Deep Reinforcement Learning. arXiv
preprint arXiv:2005.02305 .
Schaul, T.; Horgan, D.; Gregor, K.; and Silver, D. 2015. Uni-
versal value function approximators. In International con-
ference on machine learning , 1312–1320. PMLR.
Schick, T.; Dwivedi-Yu, J.; Dess `ı, R.; Raileanu, R.; Lomeli,
M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.
Toolformer: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761 .
Segovia-Aguas, J.; Jim ´enez, S.; and Jonsson, A. 2018. Com-
puting hierarchical finite state controllers with classical
planning. Journal of Artificial Intelligence Research , 62:
755–797.
Segovia-Aguas, J.; Jim ´enez, S.; and Jonsson, A. 2021. Gen-
eralized Planning as Heuristic Search. In Proceedings of
the International Conference on Automated Planning and
Scheduling , volume 31, 569–577.
Sharma, P.; Torralba, A.; and Andreas, J. 2022. Skill Induc-
tion and Planning with Latent Language. In Proceedings of
the 60th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , 1713–1726.
Silver, T.; Chitnis, R.; Kumar, N.; McClinton, W.; Lozano-
Perez, T.; Kaelbling, L. P.; and Tenenbaum, J. 2023. Predi-
cate Invention for Bilevel Planning. In AAAI Conference on
Artificial Intelligence (AAAI) .
Silver, T.; Hariprasad, V .; Shuttleworth, R. S.; Kumar, N.;
Lozano-P ´erez, T.; and Kaelbling, L. P. 2022. PDDL Plan-
ning with Pretrained Large Language Models. In NeurIPS
2022 Foundation Models for Decision Making Workshop .
Singh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;
Tremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2022.
Progprompt: Generating situated robot task plans using large
language models. arXiv preprint arXiv:2209.11302 .Sohrabi, S.; Riabov, A. V .; Udrea, O.; and Hassanzadeh, O.
2016. Finding diverse high-quality plans for hypothesis gen-
eration. In ECAI 2016 , 1581–1582. IOS Press.
Srivastava, S. 2011. Foundations and applications of gener-
alized planning. AI Communications , 24(4): 349–351.
Srivastava, S.; Immerman, N.; Zilberstein, S.; and Zhang, T.
2011. Directed Search for Generalized Plans Using Classi-
cal Planners. In ICAPS .
Sutton, R. S.; Modayil, J.; Delp, M.; Degris, T.; Pilarski,
P. M.; White, A.; and Precup, D. 2011. Horde: A scalable
real-time architecture for learning knowledge from unsuper-
vised sensorimotor interaction. In The 10th International
Conference on Autonomous Agents and Multiagent Systems-
Volume 2 , 761–768.
Takayuki, Y . 2000. On the NP-completeness of the Slither
Link puzzle. IPSJ SIGNotes ALgorithms .
Valmeekam, K.; Olmo, A.; Sreedharan, S.; and Kambham-
pati, S. 2022. Large Language Models Still Can’t Plan
(A Benchmark for LLMs on Planning and Reasoning about
Change). arXiv preprint arXiv:2206.10498 .
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;
Le, Q.; and Zhou, D. 2022. Chain of thought prompting
elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903 .
Winner, E. Z. 2008. Learning Domain-Specific Planners
from Example Plans . Ph.D. thesis, Carnegie Mellon Uni-
versity, USA.
Xia, C. S.; and Zhang, L. 2023. Conversational Automated
Program Repair. arXiv:2301.13246.
Xie, Y .; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; and Soh, H. 2023.
Translating natural language to planning goals with large-
language models. arXiv preprint arXiv:2302.05128 .
Yang, R.; Silver, T.; Curtis, A.; Lozano-Perez, T.; and Kael-
bling, L. P. 2022. PG3: Policy-Guided Planning for Gener-
alized Policy Generation. In IJCAI .
Zheng, W.; Sharan, S.; Jaiswal, A. K.; Wang, K.; Xi, Y .;
Xu, D.; and Wang, Z. 2023. Outline, Then Details: Syn-
tactically Guided Coarse-To-Fine Code Generation. arXiv
preprint arXiv:2305.00909 .

--- PAGE 10 ---
Additional Results
Table 3 reports the maximum fraction of evaluation tasks
solved over seeds. In most cases, the LLM either solves all
or none of the evaluation tasks. This suggests that the LLM
does not overfit to the training tasks, even though only a very
small number of them are used (see main text).
Figure 4 shows the number of training tasks used in suc-
cessful trials. See main text for details. In the vast majority
of cases, only two tasks are necessary, indicating the strong
data-efficiency of GPT-4 as a generalized planner.
Domain GPT-4 No CoT No Debug No Names GPT-3.5
Delivery 1.00 1.00 1.00 1.00 0.00
Forest 1.00 1.00 1.00 0.93 1.00
Gripper 1.00 1.00 1.00 1.00 0.00
Miconic 0.07 0.00 0.00 0.00 0.00
Ferry 1.00 1.00 1.00 0.00 0.00
Spanner 1.00 1.00 0.00 0.00 0.00
Heavy 1.00 1.00 1.00 0.00 0.00
Table 3: Max fraction of evaluation tasks solved per seed.
Figure 4: Data efficiency of GPT-4. The reported fractions
are over all trials, over all domains.
Example Programs
Examples of programs synthesized by GPT-4 with CoT sum-
marization and automated debugging are shown in List-
ings 1, 2, 3, 4, 5, 6, and 7. Any comments in the code are
added by GPT-4 itself.

--- PAGE 11 ---
Listing 1: Delivery domain example synthesized program (success)
defget_plan(objects, init, goal):
locs = [obj[0] forobj inobjects ifobj[1] == "loc"]
papers = [obj[0] forobj inobjects ifobj[1] == "paper"]
home_base = next(loc forloc inlocs if("ishomebase", loc) ininit)
safe_locs = [loc forloc inlocs if("safe", loc) ininit]
wants_paper = [loc forloc inlocs if("wantspaper", loc) ininit]
plan = []
forloc inwants_paper:
ifloc insafe_locs:
# Pick up a newspaper at the home base
paper = papers.pop()
plan.append(f"(pick-up {paper} {home_base})")
# Move to the safe location that wants a newspaper
plan.append(f"(move {home_base} {loc})")
# Deliver the newspaper to the location
plan.append(f"(deliver {paper} {loc})")
# Move back to the home base
plan.append(f"(move {loc} {home_base})")
returnplan

--- PAGE 12 ---
Listing 2: Forest domain example synthesized program (success)
defget_plan(objects, init, goal):
deffind_location(init):
foratom ininit:
ifatom[0] == "at":
returnatom[1]
return None
deffind_adjacent_trail(current_loc, init):
foratom ininit:
ifatom[0] == "ontrail" andatom[1] == current_loc:
returnatom[2]
return None
defis_hill(loc, init):
foratom ininit:
ifatom[0] == "ishill" andatom[1] == loc:
return True
return False
current_loc = find_location(init)
goal_loc = find_location(goal)
plan = []
whilecurrent_loc != goal_loc:
next_loc = find_adjacent_trail(current_loc, init)
ifis_hill(next_loc, init):
action = f"(climb {current_loc} {next_loc})"
else:
action = f"(walk {current_loc} {next_loc})"
plan.append(action)
current_loc = next_loc
returnplan

--- PAGE 13 ---
Listing 3: Gripper domain example synthesized program (success)
defget_plan(objects, init, goal):
defget_objects_of_type(type_name):
return[obj forobj inobjects ifobj.startswith(type_name)]
defget_location(state, ball):
foratom instate:
ifatom[0] == 'at' andatom[1] == ball:
returnatom[2]
return None
balls = get_objects_of_type('ball')
rooms = get_objects_of_type('room')
grippers = get_objects_of_type('gripper')
robby_location = None
foratom ininit:
ifatom[0] == 'at-robby':
robby_location = atom[1]
break
plan = []
forball inballs:
initial_location = get_location(init, ball)
goal_location = get_location(goal, ball)
ifgoal_location is None:
continue
ifinitial_location != goal_location:
plan.append(f'(move {robby_location} {initial_location})')
robby_location = initial_location
plan.append(f'(pick {ball} {initial_location} {grippers[0]})')
plan.append(f'(move {robby_location} {goal_location})')
robby_location = goal_location
plan.append(f'(drop {ball} {goal_location} {grippers[0]})')
returnplan

--- PAGE 14 ---
Listing 4: Miconic domain example synthesized program (failure)
defget_plan(objects, init, goal):
floors_by_building = {}
passengers_by_building = {}
forobj, obj_type inobjects:
ifobj_type == "floor":
building = obj.split("_")[-1]
ifbuilding not infloors_by_building:
floors_by_building[building] = []
floors_by_building[building].append(obj)
elifobj_type == "passenger":
building = obj.split("_")[-1]
ifbuilding not inpassengers_by_building:
passengers_by_building[building] = []
passengers_by_building[building].append(obj)
forbuilding infloors_by_building:
floors_by_building[building] = sorted(floors_by_building[building])
origins = {p: f forp, f in(atom[1:] foratom ininit ifatom[0] == "origin")}
destins = {p: f forp, f in(atom[1:] foratom ininit ifatom[0] == "destin")}
lift_at = {atom[1]: atom foratom ininit ifatom[0] == "lift-at"}
plan = []
boarded = set()
forbuilding, floors infloors_by_building.items():
passengers = passengers_by_building[building]
current_lift_at = lift_at[building]
forfloor infloors + list(reversed(floors)):
ifcurrent_lift_at != floor:
ifcurrent_lift_at < floor:
plan.append(f"(up {current_lift_at} {floor})")
else:
plan.append(f"(down {current_lift_at} {floor})")
current_lift_at = floor
forpassenger inpassengers:
iforigins.get(passenger) == floor:
plan.append(f"(board {floor} {passenger})")
boarded.add(passenger)
ifdestins.get(passenger) == floor andpassenger inboarded:
plan.append(f"(depart {floor} {passenger})")
boarded.remove(passenger)
returnplan

--- PAGE 15 ---
Listing 5: Ferry domain example synthesized program (success)
defget_plan(objects, init, goal):
plan = []
# Helper function to find a car's location
defget_location(car, state):
foratom instate:
ifatom[0] == 'at' andatom[1] == car:
returnatom[2]
return None
cars = [obj forobj inobjects ifobj.startswith('c')]
ferry_location = [atom[1] foratom ininit ifatom[0] == 'at-ferry'][0]
forcar incars:
initial_location = get_location(car, init)
goal_location = get_location(car, goal)
# Sail to the car's initial location
ifferry_location != initial_location:
plan.append(f'(sail {ferry_location} {initial_location})')
ferry_location = initial_location
# Board the car onto the ferry
plan.append(f'(board {car} {initial_location})')
# Sail to the car's goal location
ifferry_location != goal_location:
plan.append(f'(sail {ferry_location} {goal_location})')
ferry_location = goal_location
# Debark the car at the goal location
plan.append(f'(debark {car} {goal_location})')
returnplan

--- PAGE 16 ---
Listing 6: Spanner domain example synthesized program (failure)
defget_plan(objects, init, goal):
plan = []
bob_location = get_bob_location(init)
spanner_locations = get_spanner_locations(init)
nuts_to_tighten = {n forpred,*args ingoal ifpred == 'tightened' forninargs}
whilenuts_to_tighten:
closest_spanner = None
min_distance = float('inf')
forloc, spanner inspanner_locations.items():
distance = get_shortest_path(init, bob_location, loc)
ifdistance < min_distance:
min_distance = distance
closest_spanner = spanner
closest_spanner_loc = loc
# Move Bob to the closest spanner
path_to_spanner = get_path(init, bob_location, closest_spanner_loc)
fornext_location inpath_to_spanner:
plan.append(f'(walk {bob_location} {next_location} bob)')
bob_location = next_location
# Pick up the spanner
plan.append(f'(pickup_spanner {closest_spanner_loc} {closest_spanner} bob)')
# Move Bob to the gate
gate = 'gate'
path_to_gate = get_path(init, bob_location, gate)
fornext_location inpath_to_gate:
plan.append(f'(walk {bob_location} {next_location} bob)')
bob_location = next_location
# Tighten a nut using the spanner
nut = nuts_to_tighten.pop()
plan.append(f'(tighten_nut {gate} {closest_spanner} bob {nut})')
# Remove the used spanner from the available spanners
ifclosest_spanner_loc inspanner_locations:
delspanner_locations[closest_spanner_loc]
returnplan
defget_path(init, start, end):
paths = [start]
whilepaths[-1] != end:
forpred, l1, l2 ininit:
ifpred == 'link' andpaths[-1] == l1 andl2not inpaths:
paths.append(l2)
break
returnpaths[1:]

--- PAGE 17 ---
Listing 7: Heavy domain example synthesized program (success)
defextract_heavier_relations(init):
heavier_relations = set()
foratom ininit:
ifatom[0] == 'heavier':
heavier_relations.add(atom)
returnheavier_relations
defsort_objects_by_weight(objects, heavier_relations):
sorted_objects = []
whilelen(objects) > 0:
forobj inobjects.copy():
ifall(('heavier', other, obj) not inheavier_relations forother inobjects):
sorted_objects.append(obj)
objects.remove(obj)
returnsorted_objects
defget_plan(objects, init, goal):
heavier_relations = extract_heavier_relations(init)
sorted_objects = sort_objects_by_weight(set(objects), heavier_relations)
plan = []
first_object = sorted_objects.pop(0)
plan.append(f"(pack-first {first_object})")
forobj insorted_objects:
plan.append(f"(stack {first_object} {obj})")
first_object = obj
returnplan
