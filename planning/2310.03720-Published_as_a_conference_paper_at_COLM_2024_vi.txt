# 2310.03720.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2310.03720.pdf
# Kích thước file: 14202903 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
SteP: Các Chính sách LLM Xếp chồng cho Hành động Web
Paloma Sodhi1∗, S.R.K. Branavan1, Yoav Artzi1,2, Ryan McDonald1
1ASAPP Research, NY, USA
2Cornell University, NY, USA
{psodhi, branavan, rmcdonald}@asapp.com, yoav@cs.cornell.edu
Tóm tắt
Thực hiện các tác vụ trên web đưa ra những thách thức cơ bản đối với các mô hình ngôn ngữ lớn (LLM), bao gồm các tác vụ thế giới mở có tính tổ hợp lớn và sự biến đổi qua các giao diện web. Việc chỉ đơn giản xác định một lời nhắc lớn để xử lý tất cả các hành vi và trạng thái có thể là cực kỳ phức tạp, và dẫn đến việc rò rỉ hành vi giữa các hành vi không liên quan. Phân tách thành các chính sách riêng biệt có thể giải quyết thách thức này nhưng đòi hỏi việc chuyển giao điều khiển cẩn thận giữa các chính sách. Chúng tôi đề xuất Các Chính sách LLM Xếp chồng cho Hành động Web (SteP), một phương pháp để tự động kết hợp các chính sách để giải quyết một tập hợp đa dạng các tác vụ web. SteP định nghĩa một Quá trình Quyết định Markov nơi trạng thái là một ngăn xếp các chính sách đại diện cho trạng thái điều khiển, tức là chuỗi các lời gọi chính sách. Không giống như các phương pháp truyền thống bị hạn chế bởi các phân cấp tĩnh, SteP cho phép điều khiển động thích ứng với độ phức tạp của tác vụ. Chúng tôi đánh giá SteP so với nhiều đường cơ sở và môi trường web bao gồm WebArena, MiniWoB++, và một CRM. Trên WebArena, SteP cải thiện (14.9% đến 33.5%) so với SOTA sử dụng các chính sách GPT-4, trong khi trên MiniWob++, SteP có tính cạnh tranh với các công trình trước đó trong khi sử dụng ít dữ liệu hơn đáng kể. Mã và dữ liệu của chúng tôi có sẵn tại https://asappresearch.github.io/webagents-step.

1 Giới thiệu
Trong khi các tác nhân mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng ra quyết định ấn tượng (Yao et al., 2022b; Huang et al., 2022b), web vẫn là một lĩnh vực đầy thách thức đạt được tỷ lệ thành công thấp hơn nhiều so với các tiêu chuẩn khác (Akter et al., 2023; Zhou et al., 2023). Web chứa một không gian thế giới mở có tính tổ hợp lớn của các tác vụ như đặt vé máy bay, mua sắm hoặc hẹn lịch. Các giao diện web cũng khác nhau đáng kể từ trang web này sang trang web khác, ví dụ, tác vụ mua một món hàng trên Amazon trông khác với việc mua nó trên eBay.

Có những thách thức cơ bản trong việc thiết kế một chính sách LLM đơn lẻ để giải quyết tất cả các tác vụ web có thể. Thứ nhất, chính sách đòi hỏi các hướng dẫn và ví dụ để bao phủ tất cả các biến thể trong các tác vụ và trang web. Thứ hai, giải quyết các tác vụ chân trời dài hơn đòi hỏi việc giữ lại một lịch sử dài của các hành động và quan sát trước đó trong ngữ cảnh. Các ngữ cảnh dài hơn làm cho việc chú ý đến thông tin nổi bật trở nên khó khăn hơn dẫn đến nhiều lỗi và chi phí hơn (Liu et al., 2023).

Thay vào đó, một giải pháp tự nhiên là phân tách vấn đề thành các chính sách riêng biệt (Khot et al., 2023). Mỗi chính sách cung cấp các hướng dẫn và ví dụ chuyên dụng cho một vấn đề phụ cụ thể, chẳng hạn như tìm kiếm một danh sách hoặc tìm một trang. Tuy nhiên, điều này thường đòi hỏi việc xác định thủ công một phân cấp phân tách để chuyển giao điều khiển giữa các chính sách (Prasad et al., 2023; Zhou et al., 2021; Song et al., 2023). Điều này hạn chế điều khiển vào một phân cấp tĩnh không thích ứng được với độ phức tạp tác vụ khác nhau.

Hiểu biết chính của chúng tôi là cho phép điều khiển động, nơi bất kỳ chính sách nào cũng có thể chọn gọi bất kỳ chính sách nào khác. Tính biểu đạt như vậy là rất quan trọng để giải quyết các tác vụ web đòi hỏi các chính sách hoạt động ở nhiều mức độ trừu tượng. Xem xét Hình 1 nơi tác nhân phải tìm tất cả các commit
∗Tác giả liên hệ: Paloma Sodhi <psodhi@asapp.com>
1arXiv:2310.03720v4 [cs.LG] 8 Aug 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
t=0t=1t=2t=3search_list( "<user> commits","repository list")ﬁnd_page("commit")search_list("<user>","commit list")search_ list(..)
ﬁnd_ page(..)search_ list(..)search_ list(..)search_ list(..)search_ list(..)Thư viện Chính sáchsearch_ list(..)ﬁnd_ page(..)ﬁll_ form(..)Luồng điều khiển StePMục tiêu: "Tìm tất cả commit được thực hiện bởi <user> qua tất cả repositories"Ngăn xếp Chính sách

Hình 1: SteP kết hợp các chính sách để giải quyết tác vụ phức tạp, nơi các chính sách có thể gọi lẫn nhau. SteP sử dụng ngăn xếp chính sách để theo dõi trạng thái điều khiển động. Với mục tiêu "Tìm tất cả commit được thực hiện bởi một <user> qua tất cả repositories", SteP khởi tạo với search_list() để tìm kiếm qua tất cả repositories, điều này lần lượt gọi một search_list() khác để tìm kiếm qua tất cả commit trong repository đó.

được thực hiện bởi một người dùng qua tất cả repositories. Một chính sách search_list() trước tiên phải lặp qua tất cả repositories. Đối với mỗi repository, nó phải gọi đệ quy một search_list() khác lặp qua tất cả commit trong repository đó. Điều này chỉ có thể được giải quyết bởi một kiến trúc nơi các chính sách có thể gọi lẫn nhau, bao gồm cả chính chúng.

Chúng tôi đề xuất Các Chính sách LLM Xếp chồng cho Hành động trên Web (SteP), một phương pháp để thực hiện một tập hợp đa dạng các tác vụ web bằng cách tự động kết hợp các chính sách. SteP định nghĩa một Quá trình Quyết định Markov (MDP) nơi trạng thái là một ngăn xếp các chính sách. Ngăn xếp lưu trữ trạng thái điều khiển động nắm bắt chuỗi các lời gọi chính sách phát triển theo thời gian. Tại mỗi bước thời gian, chính sách ở đỉnh ngăn xếp hoặc hành động trực tiếp trên trang web, gọi một chính sách mới được đẩy vào ngăn xếp, hoặc kết thúc và thoát khỏi ngăn xếp. Ví dụ, trong tác vụ Hình 1, ngăn xếp khởi tạo với chính sách search_list(), có thể vừa hành động trên trang web vừa tạo một chính sách search_list() khác với một tập hợp đối số khác.

Những phát hiện chính của chúng tôi là việc tự động kết hợp các chính sách (SteP) vượt trội đáng kể so với cả các công trình trước đây (0.15→0.33) và các đường cơ sở chính sách đơn (0.23→0.33). SteP đạt được điều này trong khi sử dụng ít hơn 2.3 lần token mỗi quỹ đạo, dẫn đến chi phí tổng thể thấp hơn. Chúng tôi cũng thể hiện một số nghiên cứu khử về tác động của việc thay đổi ngữ cảnh, ví dụ trong ngữ cảnh, và lý luận CoT.

Những đóng góp chính của chúng tôi là:
1. Một khung mới SteP định nghĩa MDP qua ngăn xếp các chính sách cho phép kết hợp động các chính sách để giải quyết các tác vụ web phức tạp.
2. Xác thực thực nghiệm trên một loạt các tiêu chuẩn web: WebArena, MiniWoB++, và một mô phỏng CRM hàng không. Trên WebArena, SteP cải thiện (0.15 →0.33) so với các công trình trước đây sử dụng các chính sách LLM few-shot (GPT-4), trong khi trên MiniWob++, SteP có tính cạnh tranh với các công trình trước đây trong khi sử dụng ít dữ liệu hơn đáng kể.
3. Triển khai SteP như một meta-chính sách bao bọc xung quanh bất kỳ lớp chính sách hiện có nào.

2 Công trình Liên quan
Mô hình ngôn ngữ cho các tác vụ web. Công trình sớm ánh xạ các hướng dẫn ngôn ngữ tự nhiên thành hành động (Branavan et al., 2009; Artzi & Zettlemoyer, 2013) đã phát triển nhanh chóng thành lĩnh vực các tác nhân LLM (Wang et al., 2023b). Nói chung, các phương pháp bao gồm huấn luyện các tác nhân RL để điều hướng giao diện web (Humphreys et al., 2022; Liu et al., 2018; Shi et al., 2017), học trong ngữ cảnh với các mô hình ngôn ngữ lớn (Zhou et al., 2023; Zheng et al., 2024; Kim et al., 2023), hoặc tinh chỉnh các mô hình ngôn ngữ trên các tác vụ web (Deng et al., 2023; Furuta et al., 2024; Yao et al., 2022a). Với học trong ngữ cảnh, việc sử dụng một chính sách LLM đơn chứa tất cả hướng dẫn và ví dụ dẫn đến các ngữ cảnh dài có thể gây lỗi. Các phương pháp gần đây (Zheng et al., 2024; Kagaya et al., 2024) đối phó với điều này bằng cách truy xuất quỹ đạo từ cơ sở dữ liệu. Tuy nhiên, bao phủ không gian tổ hợp của các tác vụ đòi hỏi một tập dữ liệu lớn từ nhiều tác vụ. Thay vào đó, công trình của chúng tôi tận dụng một thư viện các chính sách, mỗi cái với các hướng dẫn và ví dụ chuyên dụng, và kết hợp các chính sách để bao phủ các tác vụ như vậy.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
Mô hình ngôn ngữ cho ra quyết định. Các LLM tuân thủ hướng dẫn đã thể hiện khả năng ra quyết định ấn tượng (Huang et al., 2022a; Brown et al., 2020) và sử dụng công cụ (Schick et al., 2023; Yang et al., 2024) bằng cách kết nối chuỗi lý luận và hành động (Yao et al., 2022b). Tuy nhiên, đối với các tác vụ chân trời dài, một chính sách đơn với một chuỗi dài lý luận và hành động có thể gây lỗi. Nói chung, các công trình xử lý những vấn đề như vậy bằng kế hoạch phân cấp (Prasad et al., 2023; Khot et al., 2023; Zhou et al., 2021), định nghĩa máy trạng thái (Ma et al., 2023), tạo mã (Wang et al., 2023a; Liang et al., 2022), hoặc bằng tự sửa lỗi (Shinn et al., 2023). Ra quyết định phân cấp có một lịch sử phong phú trong AI (Sutton, 1998), nơi một chính sách cấp cao chọn từ một thư viện kỹ năng (Tessler et al., 2017), hoặc dự đoán mục tiêu phụ (Nachum et al., 2018) hoặc dự đoán phần thưởng (Vezhnevets et al., 2017) cho một chính sách cấp thấp. Trong khi các phương pháp này tập trung vào việc học các chính sách một cách hiệu quả, chúng hạn chế việc phân tách vào một phân cấp được định nghĩa trước gồm 2 hoặc 3 cấp. Thay vào đó, khung của chúng tôi cho phép bất kỳ chính sách nào gọi động bất kỳ chính sách nào khác trong thư viện, với trạng thái điều khiển được theo dõi bằng ngăn xếp chính sách. So với các phương pháp phân tách như DecomP (Khot et al., 2023) tạo ra một chương trình tĩnh với các chương trình con không thể gọi lẫn nhau, SteP phân tách động dựa trên quan sát và cho phép các chính sách gọi lẫn nhau. So với các phương pháp kế hoạch phân cấp như ADaPT (Prasad et al., 2023), dự đoán một kế hoạch, SteP thay vào đó dự đoán các chính sách có thể phản ứng với thất bại nhanh hơn.

Automata và Phân tích cú pháp dựa trên Chuyển đổi. Việc cấu trúc điều khiển chính của hình thức của chúng tôi là ngăn xếp liên quan thuật toán của chúng tôi với Automata Pushdown (Hopcroft & Ullman, 1969). Tuy nhiên, trong Automata Pushdown, băng đầu vào là tĩnh và được xử lý tuần tự. Ngược lại, đối với các hành động web, băng đầu vào là động do các quan sát mới phát sinh. Những quan sát này thay đổi động ngữ cảnh (hoặc chuỗi đầu vào) mà các chính sách làm việc với. Mặc dù khác về phạm vi, công trình của chúng tôi lấy cảm hứng từ các hệ thống phân tích cú pháp phụ thuộc dựa trên chuyển đổi, cụ thể là các thuật toán dựa trên ngăn xếp (Nivre, 2008). Cụ thể, thuật toán của chúng tôi bao gồm các trạng thái có cấu trúc dữ liệu chính là ngăn xếp và các trạng thái chuyển đổi sang trạng thái mới thông qua một tập hợp hữu hạn các hành động được định nghĩa rõ ràng (xem Mục 4.1).

3 Công thức Vấn đề
Với một hướng dẫn ngôn ngữ tự nhiên, chẳng hạn như "Đặt cho tôi một chuyến bay từ NYC đến BOS", mục tiêu của chúng tôi là học một chính sách π thực hiện tác vụ này trên môi trường web. Điều này có thể được hình thức hóa như một Quá trình Quyết định Markov Quan sát Một phần (POMDP), được ký hiệu là ⟨S,A,O,T,r⟩. Tại mỗi bước thời gian t, chính sách (hoặc tác nhân) thực hiện một hành động at với một quan sát một phần ot, dẫn đến một trạng thái mới st+1 và quan sát ot+1, nơi:

•Trạng thái, s∈ S là trạng thái hiện tại của môi trường web, bao gồm nội dung trang web hiện tại và kết quả từ các tương tác trước đó, ví dụ, một repository mới được tạo trong môi trường GitLab.

•Hành động, a∈ A(s) biểu thị các hành động có thể được thực hiện trong trạng thái hiện tại, chẳng hạn như nhấp, cuộn, gõ trên các phần tử web cụ thể. Chúng được biểu diễn như click [id], type [id] [value], nơi id tham chiếu đến một phần tử cụ thể trên trang web. Không gian hành động thường lớn do nhiều phần tử trên một trang web.

•Quan sát, o∈ O là khía cạnh có thể quan sát hiện tại của trạng thái, tức là Mô hình Đối tượng Tài liệu (DOM) trang web hiện tại được tuần tự hóa dưới dạng văn bản.

•Hàm chuyển đổi, T(s′|s,a) là một hàm xác định mô hình hóa sự thay đổi trong trang web do một hành động được xác định bởi trang web cơ bản.

•Phần thưởng, r(s,a) được trao cho việc đạt một tập hợp mục tiêu phụ, ví dụ hủy một chuyến bay có các mục tiêu phụ như tìm đặt chỗ và sau đó hủy nó.

Vì trạng thái có thể quan sát một phần, chính sách ánh xạ một lịch sử các quan sát và hành động ht={ot,at−1,ot−1, ..} đến hành động hiện tại at, tức là π:ht→at. Như đã thảo luận trước đó, việc học một chính sách web LLM đơn π là thử thách. Tiếp theo chúng tôi xem xét việc kết hợp nhiều chính sách.

4 Phương pháp
Chúng tôi trình bày một khung, Các Chính sách LLM Xếp chồng cho Hành động Web (SteP), thực hiện một loạt các tác vụ web bằng cách tự động kết hợp các chính sách. Như đã thảo luận trước đó, việc thiết kế một chính sách đơn giải quyết tất cả tác vụ là thử thách. Thay vào đó, chúng tôi sử dụng một thư viện các chính sách Π mà chúng tôi kết hợp để giải quyết một tác vụ. Hình 2 thể hiện một minh họa của SteP giải quyết một tác vụ web

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
cms_agent("Gửi lời xin lỗi cho khách hàng không hài lòng với Circe Fleece")stop ["Hannah Lim nói tôi thực sự thất vọng với Circe …"] = ﬁnd_review(..)
ﬁnd_ review(..)cms_ agent(..)web_ agent(..)ﬁnd_ review(..)cms_ agent(..)web_ agent(..)cms_ agent(..)web_ agent(..)search_ customer(..)
ﬁnd_ review(..)cms_ agent(..)web_ agent(..)
search_ customer(..)cms_ agent(..)web_ agent(..)cms_ agent(..)web_ agent(..)
search_ customer(..)cms_ agent(..)web_ agent(..)ﬁnd_review("Tìm đánh giá của khách hàng không hài lòng với Circe Fleece")click [5]type [11] [Circe Fleece]…search_customer("Hannah Lim")click [11] type [45] [Hannah Lim] …stop ["Đã tìm thấy"] = search_customer("Hannah Lim")
cms_ agent(..)web_ agent(..)
cms_ agent(..)web_ agent(..)type [11] ["Chúng tôi xin lỗi khi biết bạn không thích Circe .. "] stop ["Hoàn thành"] = cms_agent(…)
Ngăn xếp Chính sáchTrang Trình duyệt
Hành động

Hình 2: Ví dụ về SteP giải quyết tác vụ web trên trang web Hệ thống Quản lý Khách hàng (CMS). SteP tự động kết hợp các chính sách từ một thư viện, sử dụng theo dõi chính sách để theo dõi các chính sách hoạt động. Tại mỗi bước thời gian, SteP hoặc hành động trên trang web, hoặc sửa đổi ngăn xếp để thêm/bỏ các chính sách.

bằng cách tự động xếp chồng các chính sách từ thư viện. Mục 4.1, 4.2 thảo luận về mô hình chính sách xếp chồng và thuật toán SteP tương ứng.

4.1 Mô hình Chính sách Xếp chồng
Tại bất kỳ thời điểm nào, chúng tôi biểu diễn trạng thái điều khiển như một ngăn xếp Σ biểu thị chuỗi các lời gọi chính sách Σ=⟨π0|π1|. . .|πi⟩. Chúng tôi mở rộng MDP trong Mục 3 để bao gồm một ngăn xếp các chính sách:

Trạng thái. Trạng thái MDP được tăng cường với một ngăn xếp Σ=⟨π0|π1|. . .|πi⟩ của các chính sách được gọi. Đỉnh ngăn xếp πi là chính sách hoạt động hiện tại. Mỗi chính sách trong ngăn xếp duy trì lịch sử riêng của quan sát, lý luận, hành động. Ngăn xếp được khởi tạo với một chính sách cơ sở Σ= [π0].

Hành động. Chúng tôi tăng cường các hành động MDP gốc với hai hành động mới – gọi một chính sách mới π∈Π hoặc kết thúc chính sách hiện tại πi với một giá trị trả về vi.

Chuyển đổi. Giả sử tại bước thời gian t, ngăn xếp hiện tại là Σt=⟨π0|π1|. . .|πi[h]⟩. Chính sách ở đỉnh ngăn xếp, πi, có thể thực hiện một trong ba hành động dẫn đến các chuyển đổi trạng thái khác nhau:

1. Đưa ra một hành động: Nó có thể đưa ra một hành động at cùng với lý luận rt. Điều này được gửi đến môi trường, cái mà cập nhật trạng thái st+1 và phản hồi với một quan sát ot+1. Hành động, lý luận, và quan sát được thêm vào lịch sử được duy trì bởi πi. Tập hợp các chính sách trong ngăn xếp vẫn không thay đổi, chỉ lịch sử cho chính sách hiện tại được cập nhật.
⟨π0|π1|. . .|πi[h]⟩ → ⟨π0|π1|. . .|πi[h←h∪(at,rt,ot+1)]⟩ (1)

2. Gọi một chính sách khác: Nó có thể chọn gọi một chính sách mới πi+1. Chính sách mới được khởi tạo với một lịch sử trống và được đẩy vào ngăn xếp.
⟨π0|π1|. . .|πi⟩ → ⟨π0|π1|. . .|πi|πi+1⟩ (2)
Không có hành động nào được gửi đến môi trường.

3. Kết thúc và chuyển giao điều khiển: Nó có thể chọn kết thúc với một giá trị trả về vi mà trong trường hợp của chúng tôi là một phản hồi tùy chọn được trả về bởi chính sách một khi nó kết thúc thực thi. Chính sách πi được đẩy ra khỏi ngăn xếp, và phản hồi được thêm vào lịch sử của πi−1.
⟨π0|π1|. . .|πi−1[h]|πi⟩ → ⟨π0|π1|. . .|πi−1[h←h∪vi]⟩ (3)
Không có hành động nào được gửi đến môi trường.

Phần thưởng. Các hàm phần thưởng giống như MDP gốc.

Mặc dù đây là một mô hình ra quyết định mới, nó có mối liên hệ gần gũi với các hệ thống phân tích cú pháp phụ thuộc dựa trên chuyển đổi trong NLP và ra quyết định phân cấp trong RL. Xem Mục 2.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
Thuật toán 1 SteP : Tự động kết hợp các chính sách để giải quyết tác vụ web
Thuật toán 1 SteP : Tự động kết hợp các chính sách để giải quyết tác vụ web
class SteP (Policy):...
def predict_action (self ,o b s e r v a t i o n ) :
ifself .stack .is_empty():
root_policy =self .init_policy()
self .stack .push(root_policy)
while not self .stack .is_empty():
policy =self .stack .top()
action, reason =policy .predict_action(observation)
#Đưa ra hành động môi trường
ifself .is_environment_action(action):
return action, reason
#Gọi chính sách mới
ifself .is_policy_action(action):
new_policy =self .init_policy(action)
self .stack .push(new_policy)
continue#Kết thúc và chuyển giao điều khiển
ifself .is_policy_done(action):
self .stack .pop()
policy =self .stack .top()
policy .append_response(action) ifpolicy else None
continuereturn action, reason #Hành động kết thúc bởi chính sách gốc
def main ()
policy =SteP()
observation, done =env.reset(), False
while not done:
action, reason =policy .predict_action(observation)
observation, done =env.step(action)

4.2 Thuật toán SteP
Thuật toán 1 trình bày mã giả cho SteP. SteP là một meta-chính sách bao bọc xung quanh một lớp Policy điển hình. SteP duy trì một biến self.stack để lưu trữ tập hợp các chính sách hoạt động. Chúng tôi tập trung vào predict_action() nhận đầu vào là quan sát và trả về một hành động và lý luận. Nó bắt đầu bằng việc khởi tạo ngăn xếp với một chính sách gốc. Sau đó nó lấy chính sách ở đỉnh ngăn xếp và gọi hàm predict_action() của nó.

Chính sách có thể thực hiện một trong ba hành động. Thứ nhất, nếu hành động là một hành động môi trường, ví dụ click [id], nó trả về trực tiếp. Thứ hai, nếu hành động là một lời gọi đến chính sách khác, ví dụ search_customer(..), nó khởi tạo chính sách tương ứng, đẩy nó lên đỉnh ngăn xếp, và giao điều khiển cho nó. Thứ ba, nếu hành động chỉ ra rằng chính sách hiện tại đã hoàn thành hành động, ví dụ stop [response], nó đẩy chính sách hiện tại ra khỏi ngăn xếp, gửi phản hồi đến chính sách tiếp theo trên ngăn xếp, và giao điều khiển cho nó.

Các tính năng chính. Một số đặc điểm nổi bật từ phương pháp như vậy, đáng chú ý:

1. Kết hợp Động. Các chính sách được kết hợp động tại thời điểm thử nghiệm dựa trên quan sát từ môi trường. Không gian các trạng thái điều khiển có thể được định nghĩa bởi không gian hành động của mỗi chính sách, tức là các chính sách khác mà chúng có thể chuyển đổi đến. Ngăn xếp có thể có độ sâu khác nhau thích ứng với độ khó của tác vụ. So với các công trình trước đây (Akter et al., 2023; Zhou et al., 2023) sử dụng một chính sách đơn, kết hợp cho phép khả năng thích ứng nhiều hơn.

2. Khả năng mở rộng. Việc thêm một chính sách mới vào SteP rất dễ dàng. Người dùng xây dựng lời nhắc cho chính sách và thêm nó vào thư viện với một mô tả. Chính sách này trở nên khả dụng như một phần của không gian hành động cho bất kỳ chính sách nào khác, mà không cần thay đổi bất kỳ mã nào.

3. Tính mô-đun. Mỗi chính sách chỉ theo dõi ngữ cảnh cục bộ của vấn đề phụ cụ thể mà nó đang giải quyết. Một khi nó kết thúc, nó chuyển giao điều khiển về chính sách trước đó trong ngăn xếp mà không lý luận về ngữ cảnh toàn cục. Điều này cho phép tái sử dụng các chính sách trong ngữ cảnh khác nhau, ví dụ cùng một fill_form() có thể được sử dụng bởi đặt chuyến bay hoặc hẹn lịch. So với công trình trước đây (Zheng et al., 2024) đòi hỏi minh chứng cho toàn bộ tác vụ, tính mô-đun đòi hỏi minh chứng cho các tác vụ phụ do đó hiệu quả mẫu hơn.

5 Thí nghiệm
5.1 Thiết lập Thí nghiệm
Môi trường. Chúng tôi đánh giá qua nhiều môi trường web riêng biệt được liệt kê dưới đây.

•WebArena (Zhou et al., 2023). Một tiêu chuẩn gần đây với các tác vụ web phức tạp qua nhiều lĩnh vực như mua sắm, phát triển phần mềm, quản lý nội dung. Các trang web WebArena rất thực tế với các tác vụ phản ánh những gì con người thường xuyên thực hiện trên internet. Chúng tôi đánh giá qua tất cả 804 tác vụ trong tiêu chuẩn.

•MiniWoB++ (Liu et al., 2018; Shi et al., 2017). So với WebArena, đây là một môi trường web đơn giản hóa bao gồm các tương tác như điền biểu mẫu, tìm kiếm, chọn ngày. Chúng tôi đánh giá qua tất cả 45 tác vụ không dựa vào thị giác và trung bình qua 50 hạt giống mỗi tác vụ.

•AirlineCRM. Chúng tôi phát triển một mô phỏng CRM mới (Phụ lục C) được mô hình theo quy trình dịch vụ khách hàng trên các trang web hàng không phổ biến. So với MiniWoB++, điều này chứa các tác vụ chân trời dài hơn. Chúng tôi đánh giá qua 5 tác vụ trung bình qua 20 kịch bản mỗi tác vụ.

• Cuối cùng chúng tôi thử nghiệm trên môi trường trang web trực tiếp và thể hiện kết quả trong Phụ lục D.

Chính sách. Chúng tôi sử dụng một thư viện gồm 14 chính sách cho WebArena, mỗi cái bao gồm nhiều ý định. Việc xây dựng một chính sách rất đơn giản: chúng tôi sử dụng một lời nhắc mẫu với hướng dẫn chung, định nghĩa không gian hành động, và chỗ giữ chỗ cho hướng dẫn và ví dụ cụ thể của chính sách. Để thiết kế các chính sách, chúng tôi nhóm các ý định có chức năng tương đương, ví dụ, tìm kiếm qua đơn hàng hoặc liệt kê sản phẩm. Xem Phụ lục A.2.

Đường cơ sở. Chúng tôi so sánh với các đường cơ sở khác nhau bao gồm công nghệ tiên tiến trước đây trên WebArena (Zhou et al., 2023; Akter et al., 2023) thiết kế một chính sách tác nhân web đơn theo phong cách ReAct (Yao et al., 2022b) kết hợp chuỗi suy nghĩ (CoT). Trên MiniWob++, chúng tôi so sánh với các công trình tinh chỉnh (Furuta et al., 2024; Gur et al., 2022a; Humphreys et al., 2022) và học trong ngữ cảnh (Zheng et al., 2024; Sun et al., 2024; Kim et al., 2023) gần đây. Ngoài ra, chúng tôi tạo ra các đường cơ sở để nghiên cứu các hiệu ứng sau: (i) Lời nhắc Đơn so với Phân tách (Flat so với SteP). Flat là một chính sách đơn nối các hướng dẫn và ví dụ từ tất cả các chính sách trong thư viện thành một lời nhắc đơn. (ii) Thay đổi độ dài ngữ cảnh của lời nhắc (Flat-4k so với Flat-8k). Vì lời nhắc chính sách điển hình ít hơn 4000 token, chúng tôi tạo ra hai đường cơ sở Flat-4K giới hạn kích thước lời nhắc ở 4000 token và Flat-8K giới hạn ở 8000. (iii) Hiệu ứng của ví dụ trong ngữ cảnh (Zero-shot so với Few-shot). Chúng tôi nghiên cứu hiệu ứng của việc thêm các ví dụ quan sát hành động giúp liên kết hướng dẫn ngôn ngữ với các phần tử trang web. Chúng tôi nghiên cứu (i) trên tất cả tập dữ liệu, (ii) trên WebArena, nơi độ dài ngữ cảnh dài hơn do các trang web phức tạp hơn, và (iii) trên MiniWob++, AirlineCRM nơi các trang web được cắt giảm có các phần tử mơ hồ (ví dụ thiếu aria-labels) và được hưởng lợi từ các ví dụ.

Mô hình. Đối với các mô hình, trên WebArena chúng tôi đánh giá với gpt-4-turbo1 (OpenAI, 2023) vì các tác vụ phức tạp, trong khi đối với MiniWob++ và AirlineCRM chúng tôi đánh giá với text-davinci-0031 hoặc gpt-3.5-turbo1 được tinh chỉnh hướng dẫn (Ouyang et al., 2022).

Chỉ số. Chúng tôi định nghĩa 3 chỉ số: Tỷ lệ Thành công (suc↑), Tiến độ Tác vụ (prog↑), và Số Hành động (#act). suc↑ là 0 hoặc 1 tùy thuộc vào việc tác vụ được hoàn thành thành công. #act là số hành động được thực hiện. Trên airline CRM, chúng tôi cũng tính prog↑ một số từ 0 đến 1 chỉ ra tiến độ hướng tới hoàn thành tác vụ.

5.1.1 Kết quả Tổng thể
•Trên WebArena, SteP vượt trội so với các công trình trước đây (0.15→0.33) với cải thiện trên mọi môi trường: Shopping (0.2→0.37), CMS (0.10→0.24), Reddit (0.11→0.59), Gitlab (0.14→0.32), Maps (0.15 →0.30). Xem Mục 5.1.2.
•SteP đạt được độ chính xác cao hơn so với Flat-8k (0.23→0.33) trong khi sử dụng độ dài ngữ cảnh nhỏ hơn 2.3 lần mỗi tập. Xem Mục 5.1.3, 5.1.4.
•Trên MiniWob++, StePFew-shot có tính cạnh tranh với các công trình trước đây trong khi sử dụng ít dữ liệu hơn đáng kể. Xem Mục 5.1.2.

1https://platform.openai.com/docs/models

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
Tỷ lệ Thành côngToken đầu vào (mỗi quỹ đạo)
(a)(b)Xác suất

Hình 3: (a) Tỷ lệ thành công của SteP so với tất cả đường cơ sở trên 5 trang web WebArena khác nhau. (b) Phân phối token đầu vào mỗi quỹ đạo của SteP so với Flat-8k trên WebArena. SteP đạt được tỷ lệ thành công cao hơn trong khi cần ít token đầu vào hơn dẫn đến chi phí thấp hơn mỗi quỹ đạo.

[Bảng 1 với kết quả chi tiết về tỷ lệ thành công trên WebArena]

•Các ví dụ trong ngữ cảnh giúp ích thêm vào hướng dẫn bằng cách liên kết hướng dẫn ngôn ngữ với các phần tử web tương ứng. Hơn nữa, StePFew-shot sử dụng các ví dụ này hiệu quả hơn bằng cách có chúng trong các chính sách chuyên dụng. Xem Mục 5.1.5.
• Chúng tôi cung cấp các nghiên cứu khử về lý luận CoT và quy mô mô hình trong Phụ lục B.

5.1.2 So sánh với các công trình trước đây
Trên WebArena, Hình 3(a), Bảng 1 cho thấy SteP vượt trội so với các công trình trước đây (Zhou et al., 2023; Akter et al., 2023) sử dụng một chính sách GPT-4 đơn trên tất cả môi trường. Bằng cách có một thư viện chỉ 14 chính sách (xem Phụ lục A.2), SteP bao phủ ít nhất 50 ý định trong tổng số 170 ý định. Những lợi ích đáng kể nhất đến từ Shopping, Reddit nơi các chính sách bao phủ tỷ lệ phần trăm lớn hơn của các ý định.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

[Bảng 2 so sánh với các công trình trước đây trên MiniWoB++]

Trên MiniWob++, Bảng 2 cho thấy SteP vượt trội so với tất cả đường cơ sở tinh chỉnh mô hình. Nó chỉ sử dụng 10 quỹ đạo minh chứng (24 ví dụ quan sát-hành động) so với đường cơ sở gần đây nhất (Furuta et al., 2024) huấn luyện trên 347K quỹ đạo. Nó cũng có tính cạnh tranh với các công trình học trong ngữ cảnh gần đây trong khi sử dụng ít quỹ đạo hơn. Ví dụ, Synapse (Zheng et al., 2024) sử dụng ~100 quỹ đạo với 2-3 mẫu được lấy từ mỗi trong số 48 tác vụ. So sánh, SteP sử dụng 10 quỹ đạo từ chỉ 6 tác vụ. SteP tổng quát hóa cho các tác vụ còn lại thông qua kết hợp, tức là bằng cách phân tách chúng thành các tác vụ phụ có thể giải quyết bởi các chính sách thư viện hiện có. Do đó, không giống như công trình trước đây, SteP không cần thấy mẫu từ mọi tác vụ mà nó được yêu cầu giải quyết.

5.1.3 Tại sao thư viện các chính sách giúp ích hơn một chính sách đơn?
Chúng tôi quan sát thấy rằng trong các công trình trước đây (Akter et al., 2023; Zhou et al., 2023), một chế độ thất bại phổ biến là không có khả năng điều hướng trang web đúng cách để giải quyết một tác vụ phức tạp, nhiều bước. Như một giải pháp tự nhiên đầu tiên, chúng tôi thêm hướng dẫn và ví dụ vào lời nhắc để dạy nó cách giải quyết các tác vụ như vậy. Chúng tôi tạo ra hai đường cơ sở, Flat-4k và Flat-8k, chứa các hướng dẫn và ví dụ như vậy lên đến giới hạn ngữ cảnh 4000 và 8000 tương ứng.

Trong Bảng 1, chúng tôi thấy rằng Flat-4k cải thiện so với công trình trước đây (0.15 → 0.20). Tuy nhiên, khi chúng tôi đi từ Flat-4k đến Flat-8k, những lợi ích hiệu suất chỉ tăng một cách biên giới (0.20 → 0.23) mặc dù chúng tôi tăng gấp đôi độ dài ngữ cảnh.

[Bảng 3 với kết quả chi tiết trên MiniWoB++]

Trên một số ý định, tỷ lệ thành công thậm chí còn thoái lui, ví dụ trên Shopping (0.2→0, 0.6→0.4), trên Reddit (0.5→0.33, 0.6→0.4). Điều này là do các hướng dẫn bổ sung tạo ra những lời nhắc lớn hơn, làm cho việc mô hình chú ý trở nên khó khăn.

SteP giới thiệu một thư viện gồm 14 chính sách, mỗi cái với một tập hợp nhỏ các hướng dẫn và ví dụ chuyên dụng với độ dài lời nhắc dưới 4000 (xem Phụ lục A.2). Điều này dẫn đến các lời nhắc nhỏ hơn mắc ít lỗi hơn. Bảng 1 cho thấy SteP vượt trội so với cả Flat-4k (0.20→0.33) và Flat-8k (0.23→0.33). Một tính năng chính giúp SteP mở rộng là một chính sách đơn có thể bao phủ nhiều ý định, ví dụ search_order() bao phủ 6 ý định gồm 30 tác vụ.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
SteP làm tốt hơn trên các tác vụ phức tạpHiệu suất tương đương trên các tác vụ đơn giảnSteP Few-ShotFlat Few-Shot

Hình 4: So sánh tỷ lệ thành công giữa Flat Few-shot và StePFew-shot phân tích trên 45 tác vụ MiniWob++ (trung bình qua 50 hạt giống mỗi tác vụ).

Hình 5: (a) Đánh giá trên 5 tác vụ airline CRM trung bình qua 20 kịch bản ngẫu nhiên mỗi tác vụ. (b) Hiển thị mô phỏng của tác vụ book-flight gồm >20 bước. Chi tiết thêm trong Phụ lục C.

[Ví dụ chi tiết về SteP Few-shot vs SteP Zero-shot]

Trên MiniWob++, trong Bảng 3, chúng tôi thấy một xu hướng tương tự nơi SteP cải thiện so với Flat Few-shot (0.72→0.96). Hình 4 cho thấy so sánh qua các tác vụ riêng lẻ. Trong khi hiệu suất tương đương trên các tác vụ đơn giản hơn, khi độ phức tạp tác vụ tăng, SteP vượt trội với khoảng cách lớn hơn. SteP phân tách các tác vụ phức tạp thành các tác vụ phụ nhỏ hơn được bao phủ bởi các chính sách trong thư viện, ví dụ book_flight() được phân tách thành fill_text(), choose_date().

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

5.1.4 SteP hiệu quả ngữ cảnh như thế nào?
Một sự đánh đổi với SteP là bằng cách giới thiệu một thư viện các chính sách, có một chi phí phụ trong việc chuyển điều khiển qua lại giữa các chính sách. Điều này dẫn đến nhiều lời gọi hơn đến mô hình. Tuy nhiên, vì lời nhắc cho mỗi chính sách nhỏ hơn đáng kể, tổng số token cuối cùng nhỏ hơn. Ví dụ, khi SteP giải quyết một tác vụ, nó không bao giờ phải thấy hướng dẫn và ví dụ cho các chính sách không được yêu cầu trong tác vụ đó. Hình 3(b) cho thấy biểu đồ của độ dài ngữ cảnh cho SteP và Flat-8k qua tất cả các quỹ đạo WebArena thành công. SteP được phân phối xung quanh một số token nhỏ hơn, trung bình 22.7K so với 52K. Do đó tổng chi phí và thời gian suy luận cho SteP thấp hơn Flat-8k.

5.1.5 Hiệu ứng của các ví dụ trong ngữ cảnh là gì?
Chúng tôi quan sát thấy rằng mặc dù hướng dẫn thường đủ để giải quyết nhiều tác vụ web, các ví dụ có thể cung cấp một tăng cường hiệu suất đáng kể, đặc biệt khi các phần tử này được cắt giảm thiếu các nhãn aria có ý nghĩa. Trên MiniWob++ Bảng 3, chúng tôi thấy rằng các ví dụ few-shot cung cấp lợi ích hiệu suất cho cả Flat (0.60→0.72) và SteP (0.65→0.96).

Hình 7: SteP so với Flat với các ví dụ trong ngữ cảnh khác nhau trên tập con tác vụ MiniWob++. Vùng màu vàng cho thấy các ví dụ bổ sung SteP đóng gói trong các chính sách.

Chúng tôi thấy một xu hướng tương tự trong AirlineCRM trong Hình 5 cho cả Flat (0.24→0.76) và SteP (0.68→0.94). Các ví dụ giúp liên kết hướng dẫn ngôn ngữ với các phần tử trang web, đặc biệt đối với các trang đơn giản hóa khi chúng mơ hồ. Hình 6 cho thấy một tác vụ search-engine MiniWob++, nơi không rõ liên kết thứ 7 có nghĩa là gì. Hình 7 cho thấy SteP so với Flat với số lượng ví dụ trong ngữ cảnh khác nhau trên MiniWob++. Chúng tôi cung cấp tối đa 21 ví dụ. Chúng tôi quan sát hai nguồn cải thiện chính: (1) Đối với cùng số lượng ví dụ (≤7), cải thiện đến từ việc phân tách hướng dẫn tác vụ thành hướng dẫn chính sách chi tiết (2) Mỗi lời nhắc chính sách chứa các ví dụ trong ngữ cảnh chuyên dụng, cho phép nhiều ví dụ trong ngữ cảnh hơn (>7) trong mỗi lời nhắc.

6 Hạn chế
Chúng tôi trình bày SteP tự động kết hợp các chính sách để thực hiện một tập hợp đa dạng các tác vụ web. Kết quả của chúng tôi cho thấy SteP vượt trội so với cả các công trình trước đây và các đường cơ sở chính sách đơn trong khi hiệu quả ngữ cảnh hơn. Tuy nhiên, SteP có một số hạn chế quan trọng:

(1) Các chính sách được định nghĩa thủ công. Đối với bất kỳ lĩnh vực mới nào, người dùng cần xác định các tác vụ phụ xuất hiện thường xuyên được chia sẻ qua nhiều tác vụ. Đối với mỗi tác vụ phụ, sau đó họ cần viết một lời nhắc chính sách chứa hướng dẫn về cách giải quyết tác vụ phụ. Cả việc xác định tác vụ phụ và viết lời nhắc chính sách đều đòi hỏi kiến thức về lĩnh vực. Tuy nhiên, một khi các lời nhắc chính sách được viết, SteP có thể tự động quyết định khi nào kết hợp các chính sách phù hợp từ thư viện. Lợi ích của kết hợp là một tập hợp nhỏ các chính sách có thể bao phủ một số lượng tổ hợp lớn các tác vụ. Tự động khám phá các chính sách hữu ích từ dữ liệu kinh nghiệm hoặc minh chứng là một hướng quan trọng của công việc tương lai và sẽ cho phép mở rộng đến một số lượng lớn các lĩnh vực và trang web tự động.

(2) Chi phí giao tiếp. Bằng cách phân tách các tác vụ thành các chính sách nhỏ hơn, chúng tôi cũng phát sinh chi phí giao tiếp giữa các chính sách làm tăng thời gian suy luận. Một giải pháp thú vị sẽ là sử dụng các mô hình nhỏ hơn cho các chính sách đơn giản hơn và chỉ leo thang đến các mô hình lớn hơn khi cần thiết.

(3) Thông tin không đầy đủ. Cuối cùng, có những tình huống nơi một chính sách không thể giải quyết một tác vụ phụ do thông tin không đầy đủ được chuyển cho nó. Hơn nữa, nó không thể truyền đạt những gì còn thiếu cho chính sách cha, có thể tạo ra một vòng lặp vô tận. Một hướng tương lai thú vị sẽ là khám phá cách các chính sách có thể chia sẻ và cập nhật một trạng thái tin tưởng chung để ngăn chặn những lỗi như vậy.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Lời cảm ơn
Chúng tôi cảm ơn Kilian Weinberger, Michael Griffiths, Ramya Ramakrishnan, Adrian Botta, và phần còn lại của các nhóm nghiên cứu ASAPP và tự động hóa UI cho phản hồi sâu sắc và đề xuất của họ. Chúng tôi cũng cảm ơn các nhà đánh giá ẩn danh cho phản hồi xây dựng của họ, điều này đã nâng cao đáng kể chất lượng của bài báo này.

Tuyên bố Đạo đức
Việc trang bị cho LLM khả năng thực hiện các tác vụ web mở ra nhiều khả năng cho lợi ích và thay đổi xã hội. Những điều này dao động từ việc giảm gánh nặng nhận thức cho con người trong việc thực hiện các tác vụ lặp lại đến việc cho phép khả năng tiếp cận lớn hơn cho người cao tuổi và cá nhân khuyết tật. Tuy nhiên, chúng tôi thừa nhận những tác động đạo đức đi kèm với việc sử dụng các công nghệ như vậy, và liệt kê một số trong số chúng dưới đây:

1. An toàn và Độ tin cậy. LLM tự động hóa các tác vụ web gây ra mối quan ngại về việc sử dụng sai, bao gồm tự động hóa độc hại nhằm mục đích spam, lừa đảo, hoặc thao túng các hệ thống trực tuyến. Để giảm thiểu những rủi ro này, việc triển khai các biện pháp bảo vệ nghiêm ngặt là rất quan trọng. Những điều này có thể bao gồm việc phát triển các thuật toán phát hiện tinh vi để xác định và chặn các hành động tự động thể hiện các mẫu sử dụng sai, đảm bảo rằng LLM hoạt động trong các ranh giới đạo đức. Hơn nữa, các giao thức thử nghiệm và xác thực nghiêm ngặt sẽ đảm bảo độ tin cậy và an toàn của công nghệ trong môi trường web thế giới mở.

2. Bảo mật Riêng tư và Dữ liệu. LLM tương tác với giao diện web giới thiệu các rủi ro tiềm ẩn như truy cập dữ liệu trái phép và vi phạm riêng tư. Để giảm thiểu những rủi ro này, việc đặt các biện pháp bảo vệ như mã hóa dữ liệu nhạy cảm, thực thi kiểm soát truy cập nghiêm ngặt, và tuân thủ các thực hành riêng tư mạnh mẽ là rất quan trọng. Các chính sách xử lý dữ liệu minh bạch như vậy là cần thiết để duy trì niềm tin trong những hệ thống này với người dùng cuối.

3. Tác động Việc làm. Mặc dù việc tự động hóa các tác vụ web với LLM có thể nâng cao hiệu quả và khả năng tiếp cận, việc xem xét tác động đến việc làm là rất quan trọng. Cần chú ý rằng việc triển khai các công nghệ như vậy là để tăng cường khả năng của con người thay vì thay thế chúng, giúp giải phóng họ cho các tương tác sáng tạo và tinh tế hơn. Giải quyết điều này đòi hỏi các phương pháp toàn diện, bao gồm các chính sách hỗ trợ chuyển đổi lực lượng lao động thông qua đào tạo lại và nâng cao kỹ năng, và khuyến khích phát triển các vai trò mới tận dụng những điểm mạnh độc đáo của sự sáng tạo con người.

Tuyên bố Tái tạo
Để thúc đẩy khả năng tái tạo và minh bạch, chúng tôi đã thực hiện một số bước để đảm bảo rằng SteP và những phát hiện của chúng tôi có thể được nhân bản và xác thực bởi cộng đồng nghiên cứu rộng lớn hơn:

1. Mã nguồn mở. Chúng tôi đã liên kết việc triển khai SteP, bao gồm mã cần thiết và tài liệu. Chúng tôi cũng bao gồm các lời nhắc, dự đoán mô hình thô, và các notebook liên quan để tái tạo tốt hơn.

2. Chi tiết Thí nghiệm. Bài báo của chúng tôi bao gồm mô tả chi tiết về các thí nghiệm cùng với chi tiết bổ sung về siêu tham số, lời nhắc, môi trường được bao gồm trong Phụ lục.

3. Tiêu chuẩn và Mô hình. Chúng tôi chỉ rõ rõ ràng các tiêu chuẩn web và mô hình được sử dụng trong đánh giá của chúng tôi, bao gồm các tham chiếu và liên kết thích hợp đến những điều đó. Vì các mô hình OpenAI tiếp tục phát triển, chúng tôi đã bao gồm các dự đoán mô hình thô để kết quả có thể tái tạo được.

4. Kết quả và Nghiên cứu khử. Chúng tôi trình bày kết quả chi tiết, bao gồm các chỉ số hiệu suất, nghiên cứu khử, và so sánh với các phương pháp tiên tiến. Mục tiêu của chúng tôi là cung cấp một đánh giá rõ ràng và trhonest về khả năng và hạn chế của SteP.

5. Hạn chế. Thừa nhận tầm quan trọng của sự minh bạch trong giao tiếp khoa học, chúng tôi thảo luận về các hạn chế của phương pháp chúng tôi và thảo luận về các hướng cho nghiên cứu tương lai.

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Tài liệu Tham khảo
[Danh sách các tài liệu tham khảo đầy đủ theo định dạng học thuật]

--- TRANG 12-15 ---
[Tiếp tục danh sách tài liệu tham khảo và phụ lục chi tiết]
