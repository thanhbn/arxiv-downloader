# Cây Suy Nghĩ: Giải Quyết Vấn Đề Có Chủ Ý
với Các Mô Hình Ngôn Ngữ Lớn

Shunyu Yao
Đại học Princeton

Dian Yu
Google DeepMind

Jeffrey Zhao
Google DeepMind

Izhak Shafran
Google DeepMind

Thomas L. Griffiths
Đại học Princeton

Yuan Cao
Google DeepMind

Karthik Narasimhan
Đại học Princeton

Tóm tắt
Các mô hình ngôn ngữ ngày càng được triển khai để giải quyết vấn đề tổng quát trên nhiều loại nhiệm vụ khác nhau, nhưng vẫn bị giới hạn trong quá trình ra quyết định ở mức token, từ trái qua phải trong quá trình suy luận. Điều này có nghĩa là chúng có thể gặp khó khăn trong các nhiệm vụ đòi hỏi khám phá, dự đoán chiến lược, hoặc khi các quyết định ban đầu đóng vai trò then chốt. Để vượt qua những thách thức này, chúng tôi giới thiệu một khung mới cho suy luận mô hình ngôn ngữ, "Cây Suy Nghĩ" (ToT), tổng quát hóa phương pháp "Chuỗi Suy Nghĩ" phổ biến để thúc đẩy các mô hình ngôn ngữ, và cho phép khám phá các đơn vị văn bản mạch lạc ("suy nghĩ") phục vụ như các bước trung gian hướng tới việc giải quyết vấn đề. ToT cho phép các LM thực hiện việc ra quyết định có chủ ý bằng cách xem xét nhiều con đường suy luận khác nhau và tự đánh giá các lựa chọn để quyết định hành động tiếp theo, cũng như nhìn về phía trước hoặc quay lại khi cần thiết để đưa ra các lựa chọn toàn cục. Các thí nghiệm của chúng tôi cho thấy ToT nâng cao đáng kể khả năng giải quyết vấn đề của các mô hình ngôn ngữ trên ba nhiệm vụ mới đòi hỏi lập kế hoạch hoặc tìm kiếm không tầm thường: Trò chơi số 24, Viết Sáng tạo, và Ô chữ Mini. Chẳng hạn, trong Trò chơi số 24, trong khi GPT-4 với thúc đẩy chuỗi suy nghĩ chỉ giải được 4% nhiệm vụ, phương pháp của chúng tôi đạt được tỷ lệ thành công 74%. Kho mã với tất cả thúc đẩy: https://github.com/princeton-nlp/tree-of-thought-llm .

1 Giới thiệu
Ban đầu được thiết kế để tạo ra văn bản, các phiên bản mở rộng của các mô hình ngôn ngữ (LM) như GPT [25, 26,1,23] và PaLM [5] đã được chứng minh là ngày càng có khả năng thực hiện một loạt các nhiệm vụ rộng lớn đòi hỏi suy luận toán học, biểu tượng, thông thường và kiến thức. Có lẽ đáng ngạc nhiên rằng nằm bên dưới tất cả tiến bộ này vẫn là cơ chế tự hồi quy ban đầu để tạo ra văn bản, thực hiện các quyết định ở mức token từng cái một và theo kiểu từ trái qua phải. Liệu một cơ chế đơn giản như vậy có đủ để một LM được xây dựng hướng tới một người giải quyết vấn đề tổng quát không? Nếu không, những vấn đề nào sẽ thách thức mô hình hiện tại, và cơ chế thay thế nên là gì?

Tài liệu về nhận thức con người cung cấp một số manh mối để trả lời những câu hỏi này. Nghiên cứu về các mô hình "quá trình kép" cho thấy con người có hai chế độ tham gia vào các quyết định - một chế độ nhanh, tự động, vô thức ("Hệ thống 1") và một chế độ chậm, có chủ ý, có ý thức ("Hệ thống 2") [30,31,16,15]. Hai chế độ này trước đây đã được kết nối với nhiều mô hình toán học được sử dụng trong học máy. Ví dụ, nghiên cứu về học tăng cường ở con người và động vật khác đã khám phá những hoàn cảnh mà họ tham gia vào việc học "không có mô hình" kết hợp hoặc lập kế hoạch "dựa trên mô hình" có chủ ý hơn [7]. Các lựa chọn token ở mức kết hợp đơn giản của LM cũng gợi nhớ đến "Hệ thống 1", và do đó có thể được hưởng lợi từ việc tăng cường bởi một quá trình lập kế hoạch "Hệ thống 2" có chủ ý hơn, (1) duy trì và khám phá các lựa chọn thay thế đa dạng cho hiện tại thay vì chỉ chọn một, và (2) đánh giá tình trạng hiện tại và chủ động nhìn về phía trước hoặc quay lại để đưa ra các quyết định toàn cục hơn.

Để thiết kế một quá trình lập kế hoạch như vậy, chúng tôi quay trở lại nguồn gốc của trí tuệ nhân tạo (và khoa học nhận thức), lấy cảm hứng từ các quá trình lập kế hoạch được khám phá bởi Newell, Shaw, và Simon bắt đầu từ những năm 1950 [21,22]. Newell và đồng nghiệp đã mô tả việc giải quyết vấn đề [21] như việc tìm kiếm qua một không gian vấn đề tổ hợp, được biểu diễn như một cây. Do đó chúng tôi đề xuất khung Cây Suy Nghĩ (ToT) để giải quyết vấn đề tổng quát với các mô hình ngôn ngữ. Như Hình 1 minh họa, trong khi các phương pháp hiện có (chi tiết bên dưới) lấy mẫu các chuỗi ngôn ngữ liên tục để giải quyết vấn đề, ToT chủ động duy trì một cây suy nghĩ, trong đó mỗi suy nghĩ là một chuỗi ngôn ngữ mạch lạc phục vụ như một bước trung gian hướng tới việc giải quyết vấn đề (Bảng 1). Một đơn vị ngữ nghĩa cấp cao như vậy cho phép LM tự đánh giá tiến bộ mà các suy nghĩ trung gian khác nhau tạo ra hướng tới việc giải quyết vấn đề thông qua một quá trình suy luận có chủ ý cũng được thể hiện bằng ngôn ngữ (Hình 2,4,6). Việc thực hiện heuristic tìm kiếm thông qua tự đánh giá và thảo luận của LM là mới lạ, vì các heuristic tìm kiếm trước đây được lập trình hoặc học. Cuối cùng, chúng tôi kết hợp khả năng dựa trên ngôn ngữ này để tạo ra và đánh giá các suy nghĩ đa dạng với các thuật toán tìm kiếm, như tìm kiếm theo chiều rộng (BFS) hoặc tìm kiếm theo chiều sâu (DFS), cho phép khám phá có hệ thống cây suy nghĩ với khả năng nhìn trước và quay lại.

Về mặt thực nghiệm, chúng tôi đề xuất ba vấn đề mới thách thức các phương pháp suy luận LM hiện có ngay cả với mô hình ngôn ngữ tiên tiến nhất, GPT-4 [23]: Trò chơi số 24, Viết Sáng tạo, và Ô chữ (Bảng 1). Những nhiệm vụ này đòi hỏi khả năng suy luận suy diễn, toán học, thông thường, từ vựng, và một cách để kết hợp lập kế hoạch hoặc tìm kiếm có hệ thống. Chúng tôi cho thấy ToT đạt được kết quả vượt trội trên cả ba nhiệm vụ bằng cách đủ tổng quát và linh hoạt để hỗ trợ các mức độ suy nghĩ khác nhau, các cách khác nhau để tạo ra và đánh giá suy nghĩ, và các thuật toán tìm kiếm khác nhau thích ứng với bản chất của các vấn đề khác nhau. Chúng tôi cũng phân tích cách những lựa chọn như vậy ảnh hưởng đến hiệu suất mô hình thông qua các nghiên cứu loại bỏ có hệ thống và thảo luận các hướng tương lai để đào tạo và sử dụng LM tốt hơn.

2 Bối cảnh
Trước tiên chúng tôi hình thức hóa một số phương pháp hiện có sử dụng các mô hình ngôn ngữ lớn để giải quyết vấn đề, mà phương pháp của chúng tôi được lấy cảm hứng và sau đó so sánh với. Chúng tôi sử dụng pθ để biểu thị một LM được đào tạo trước với các tham số θ, và các chữ cái thường x, y, z, s, ··· để biểu thị một chuỗi ngôn ngữ, tức là x = (x[1],···, x[n]) trong đó mỗi x[i] là một token, sao cho pθ(x) = ∏n i=1 pθ(x[i]|x[1...i]). Chúng tôi sử dụng các chữ cái hoa S,··· để biểu thị một tập hợp các chuỗi ngôn ngữ.

Thúc đẩy đầu vào-đầu ra (IO) là cách phổ biến nhất để biến một đầu vào vấn đề x thành đầu ra y với LM: y∼pθ(y|promptIO(x)), trong đó promptIO(x) bao bọc đầu vào x với hướng dẫn nhiệm vụ và/hoặc các ví dụ đầu vào-đầu ra few-shot. Để đơn giản, hãy để chúng tôi biểu thị pprompt θ(output |input ) = pθ(output |prompt (input )), sao cho thúc đẩy IO có thể được hình thức hóa là y∼pIO θ(y|x).

Thúc đẩy chuỗi suy nghĩ (CoT) [38] được đề xuất để giải quyết các trường hợp mà ánh xạ từ đầu vào x tới đầu ra y không tầm thường (ví dụ khi x là một câu hỏi toán và y là câu trả lời số cuối cùng). Ý tưởng chính là giới thiệu một chuỗi suy nghĩ z1,···, zn để kết nối x và y, trong đó mỗi zi là một chuỗi ngôn ngữ mạch lạc phục vụ như một bước trung gian có ý nghĩa hướng tới việc giải quyết vấn đề (ví dụ zi có thể là một phương trình trung gian cho câu hỏi toán). Để giải quyết vấn đề với CoT, mỗi suy nghĩ zi∼pCoT θ(zi|x, z1···i−1) được lấy mẫu tuần tự, sau đó đầu ra y∼pCoT θ(y|x, z1···n). Trong thực tế, [z1···n, y]∼pCoT θ(z1···n, y|x) được lấy mẫu như một chuỗi ngôn ngữ liên tục, và việc phân tách các suy nghĩ (ví dụ liệu mỗi zi có phải là một cụm từ, một câu, hay một đoạn văn) được để mơ hồ.

Tự nhất quán với CoT (CoT-SC) [36] là một phương pháp ensemble lấy mẫu k chuỗi suy nghĩ độc lập: [z(i) 1···n, y(i)]∼pCoT θ(z1···n, y|x) (i= 1···k), sau đó trả về đầu ra thường xuyên nhất: arg max y #{i|y(i)=y}. CoT-SC cải thiện so với CoT, bởi vì thường có các quá trình suy nghĩ khác nhau cho cùng một vấn đề (ví dụ các cách khác nhau để chứng minh cùng một định lý), và quyết định đầu ra có thể trung thực hơn bằng cách khám phá một tập hợp suy nghĩ phong phú hơn. Tuy nhiên, trong mỗi chuỗi không có khám phá cục bộ các bước suy nghĩ khác nhau, và heuristic "thường xuyên nhất" chỉ áp dụng khi không gian đầu ra bị giới hạn (ví dụ câu hỏi trắc nghiệm đa lựa chọn).

3 Cây Suy Nghĩ: Giải Quyết Vấn Đề Có Chủ Ý với LM
Một quá trình giải quyết vấn đề thực sự bao gồm việc sử dụng lặp đi lặp lại thông tin có sẵn để khởi tạo khám phá, từ đó tiết lộ thêm thông tin cho đến khi cuối cùng tìm ra cách đạt được giải pháp.—— Newell et al. [21]

Nghiên cứu về giải quyết vấn đề của con người cho thấy mọi người tìm kiếm qua một không gian vấn đề tổ hợp – một cây trong đó các nút biểu diễn các giải pháp một phần, và các nhánh tương ứng với các toán tử sửa đổi chúng [21,22]. Nhánh nào để đi được xác định bởi heuristic giúp điều hướng không gian vấn đề và hướng dẫn người giải quyết vấn đề hướng tới một giải pháp. Quan điểm này làm nổi bật hai thiếu sót chính của các phương pháp hiện có sử dụng LM để giải quyết vấn đề tổng quát: 1) Cục bộ, chúng không khám phá các tiếp tục khác nhau trong một quá trình suy nghĩ – các nhánh của cây. 2) Toàn cục, chúng không kết hợp bất kỳ loại lập kế hoạch, nhìn trước, hoặc quay lại nào để giúp đánh giá các tùy chọn khác nhau này – loại tìm kiếm hướng dẫn bằng heuristic dường như đặc trưng của việc giải quyết vấn đề của con người.

Để giải quyết những thiếu sót này, chúng tôi giới thiệu Cây Suy Nghĩ (ToT), một mô hình cho phép LM khám phá nhiều con đường suy luận qua các suy nghĩ (Hình 1(c)). ToT đóng khung bất kỳ vấn đề nào như một tìm kiếm qua một cây, trong đó mỗi nút là một trạng thái s = [x, z1···i] biểu diễn một giải pháp một phần với đầu vào và chuỗi suy nghĩ cho đến nay. Một thể hiện cụ thể của ToT bao gồm việc trả lời bốn câu hỏi: 1. Cách phân tách quá trình trung gian thành các bước suy nghĩ; 2. Cách tạo ra các suy nghĩ tiềm năng từ mỗi trạng thái; 3. Cách đánh giá heuristic các trạng thái; 4. Thuật toán tìm kiếm nào để sử dụng.

1. Phân tách suy nghĩ. Trong khi CoT lấy mẫu suy nghĩ một cách mạch lạc mà không có phân tách rõ ràng, ToT tận dụng các thuộc tính vấn đề để thiết kế và phân tách các bước suy nghĩ trung gian. Như Bảng 1 cho thấy, tùy thuộc vào các vấn đề khác nhau, một suy nghĩ có thể là một vài từ (Ô chữ), một dòng phương trình (Trò chơi số 24), hoặc toàn bộ đoạn văn về kế hoạch viết (Viết Sáng tạo). Nói chung, một suy nghĩ nên đủ "nhỏ" để LM có thể tạo ra các mẫu hứa hẹn và đa dạng (ví dụ tạo ra cả một cuốn sách thường quá "lớn" để mạch lạc), nhưng đủ "lớn" để LM có thể đánh giá triển vọng của nó hướng tới việc giải quyết vấn đề (ví dụ tạo ra một token thường quá "nhỏ" để đánh giá).

2. Bộ tạo suy nghĩ G(pθ, s, k). Cho một trạng thái cây s = [x, z1···i], chúng tôi xem xét hai chiến lược để tạo ra k ứng cử viên cho bước suy nghĩ tiếp theo:
(a) Lấy mẫu các suy nghĩ độc lập từ một thúc đẩy CoT (Viết Sáng tạo, Hình 4): z(j)∼ pCoT θ(zi+1|s) =pCoT θ(zi+1|x, z1···i) (j= 1···k). Điều này hoạt động tốt hơn khi không gian suy nghĩ phong phú (ví dụ mỗi suy nghĩ là một đoạn văn), và các mẫu độc lập dẫn đến sự đa dạng;
(b) Đề xuất suy nghĩ tuần tự sử dụng một "thúc đẩy đề xuất" (Trò chơi số 24, Hình 2; Ô chữ, Hình 6): [z(1),···, z(k)]∼ppropose θ(z(1···k) i+1|s). Điều này hoạt động tốt hơn khi không gian suy nghĩ bị hạn chế hơn (ví dụ mỗi suy nghĩ chỉ là một từ hoặc một dòng), vì vậy việc đề xuất các suy nghĩ khác nhau trong cùng bối cảnh tránh trùng lặp.

3. Bộ đánh giá trạng thái V(pθ, S). Cho một ranh giới của các trạng thái khác nhau, bộ đánh giá trạng thái đánh giá tiến bộ mà chúng tạo ra hướng tới việc giải quyết vấn đề, phục vụ như một heuristic cho thuật toán tìm kiếm để xác định trạng thái nào tiếp tục khám phá và theo thứ tự nào. Trong khi heuristic là một phương pháp tiêu chuẩn để giải quyết vấn đề tìm kiếm, chúng thường được lập trình (ví dụ DeepBlue [3]) hoặc học (ví dụ AlphaGo [29]). Chúng tôi đề xuất một lựa chọn thứ ba, bằng cách sử dụng LM để suy luận có chủ ý về các trạng thái. Khi áp dụng được, một heuristic có chủ ý như vậy có thể linh hoạt hơn các quy tắc được lập trình, và hiệu quả mẫu hơn các mô hình được học. Tương tự như bộ tạo suy nghĩ, chúng tôi xem xét hai chiến lược để đánh giá các trạng thái độc lập hoặc cùng nhau:
(a) Đánh giá từng trạng thái độc lập: V(pθ, S)(s)∼pvalue θ(v|s)∀s∈S, trong đó một thúc đẩy giá trị suy luận về trạng thái s để tạo ra một giá trị vô hướng v (ví dụ 1-10) hoặc một phân loại (ví dụ chắc chắn/có khả năng/không thể) có thể được chuyển đổi heuristic thành một giá trị. Cơ sở của việc suy luận đánh giá như vậy có thể khác nhau giữa các vấn đề và bước suy nghĩ. Trong công việc này, chúng tôi khám phá đánh giá thông qua một vài mô phỏng nhìn trước (ví dụ nhanh chóng xác nhận rằng 5, 5, 14 có thể đạt 24 qua 5 + 5 + 14, hoặc "hot l" có thể có nghĩa là "inn" qua việc điền "e" vào " ") cộng với thông thường (ví dụ 1 2 3 quá nhỏ để đạt 24, hoặc không có từ nào có thể bắt đầu bằng "tzxc"). Trong khi cái trước có thể thúc đẩy các trạng thái "tốt", cái sau có thể giúp loại bỏ các trạng thái "xấu". Các đánh giá như vậy không cần phải hoàn hảo, và chỉ cần hữu ích một cách gần đúng cho việc ra quyết định.
(b) Bỏ phiếu qua các trạng thái: V(pθ, S)(s) =1[s=s∗], trong đó một trạng thái "tốt" s∗∼pvote θ(s∗|S) được bỏ phiếu ra dựa trên việc so sánh có chủ ý các trạng thái khác nhau trong S trong một thúc đẩy bỏ phiếu. Khi thành công của vấn đề khó đánh giá trực tiếp hơn (ví dụ tính mạch lạc của đoạn văn), tự nhiên thay vào đó so sánh các giải pháp một phần khác nhau và bỏ phiếu cho cái hứa hẹn nhất. Điều này tương tự về tinh thần với chiến lược tự nhất quán "từng bước", tức là đặt "trạng thái nào để khám phá" như một câu hỏi trắc nghiệm đa lựa chọn, và sử dụng các mẫu LM để bỏ phiếu cho nó.

Đối với cả hai chiến lược, chúng tôi có thể thúc đẩy LM nhiều lần để tổng hợp kết quả giá trị hoặc bỏ phiếu để đánh đổi thời gian/tài nguyên/chi phí cho heuristic trung thực/mạnh mẽ hơn.

Thuật toán 1 ToT-BFS( x, pθ, G, k, V, T, b )
Yêu cầu: Đầu vào x, LM pθ, bộ tạo suy nghĩ G() & giới hạn kích thước k, bộ đánh giá trạng thái V(), giới hạn bước T, giới hạn chiều rộng b.
S0← {x}
for t= 1,···, T do
S′ t← {[s, z]|s∈St−1, zt∈G(pθ, s, k)}
Vt←V(pθ, S′ t)
St←arg max S⊂S′ t,|S|=b ∑ s∈S Vt(s)
end for
return G(pθ,arg max s∈ST VT(s),1)

Thuật toán 2 ToT-DFS( s, t, p θ, G, k, V, T, v th)
Yêu cầu: Trạng thái hiện tại s, bước t, LM pθ, bộ tạo suy nghĩ G() và giới hạn kích thước k, bộ đánh giá trạng thái V(), giới hạn bước T, ngưỡng vth
if t > T then ghi lại đầu ra G(pθ, s,1)
end if
for s′∈G(pθ, s, k) do ▷ các ứng cử viên đã sắp xếp
if V(pθ,{s′})(s)> vthres then ▷ cắt tỉa
DFS(s′, t+ 1)
end if
end for

4. Thuật toán tìm kiếm. Cuối cùng, trong khung ToT, người ta có thể cắm và chơi các thuật toán tìm kiếm khác nhau tùy thuộc vào cấu trúc cây. Chúng tôi khám phá hai thuật toán tìm kiếm tương đối đơn giản và để các thuật toán tiên tiến hơn (ví dụ A* [11], MCTS [2]) cho công việc tương lai:
(a) Tìm kiếm theo chiều rộng (BFS) (Thuật toán 1) duy trì một tập hợp b trạng thái hứa hẹn nhất mỗi bước. Điều này được sử dụng cho Trò chơi số 24 và Viết Sáng tạo khi độ sâu cây bị giới hạn (T≤3), và các bước suy nghĩ ban đầu có thể được đánh giá và cắt tỉa thành một tập hợp nhỏ (b≤5).
(b) Tìm kiếm theo chiều sâu (DFS) (Thuật toán 2) khám phá trạng thái hứa hẹn nhất trước, cho đến khi đầu ra cuối cùng được đạt (t > T), hoặc bộ đánh giá trạng thái coi rằng không thể giải quyết vấn đề từ s hiện tại (V(pθ,{s})(s)≤vth cho một ngưỡng giá trị vth). Trong trường hợp sau, cây con từ s được cắt tỉa để đánh đổi khám phá cho khai thác. Trong cả hai trường hợp, DFS quay lại trạng thái cha của s để tiếp tục khám phá.

Về mặt khái niệm, ToT có nhiều lợi ích như một phương pháp giải quyết vấn đề tổng quát với LM: (1) Tính tổng quát. IO, CoT, CoT-SC, và tự tinh chỉnh có thể được xem như các trường hợp đặc biệt của ToT (tức là cây có độ sâu và chiều rộng giới hạn; Hình 1). (2) Tính mô-đun. LM cơ bản, cũng như việc phân tách suy nghĩ, tạo ra, đánh giá, và các thủ tục tìm kiếm đều có thể được thay đổi độc lập. (3) Khả năng thích ứng. Các thuộc tính vấn đề khác nhau, khả năng LM, và ràng buộc tài nguyên có thể được đáp ứng. (4) Tiện lợi. Không cần đào tạo thêm, chỉ cần một LM được đào tạo trước là đủ. Phần tiếp theo sẽ cho thấy những lợi ích khái niệm này chuyển đổi thành hiệu suất thực nghiệm mạnh mẽ trong các vấn đề khác nhau.

4 Thí nghiệm
Chúng tôi đề xuất ba nhiệm vụ khó ngay cả khi lấy mẫu từ mô hình ngôn ngữ tiên tiến nhất, GPT-4 [23], sử dụng thúc đẩy IO tiêu chuẩn hoặc thúc đẩy chuỗi suy nghĩ (CoT). Chúng tôi cho thấy cách tìm kiếm có chủ ý trong cây suy nghĩ (ToT) tạo ra kết quả tốt hơn, và quan trọng hơn, các cách mới thú vị và hứa hẹn để sử dụng mô hình ngôn ngữ để giải quyết vấn đề đòi hỏi tìm kiếm hoặc lập kế hoạch. Trừ khi có quy định khác, chúng tôi thực hiện thí nghiệm sử dụng chế độ Chat Completion GPT-41 với nhiệt độ lấy mẫu 0.7.

4.1 Trò chơi số 24
Trò chơi số 24 là một thách thức suy luận toán học, trong đó mục tiêu là sử dụng 4 số và các phép toán số học cơ bản (+-*/) để có được 24. Ví dụ, cho đầu vào "4 9 10 13", một đầu ra giải pháp có thể là "(10 - 4) * (13 - 9) = 24".

Thiết lập nhiệm vụ. Chúng tôi cạo dữ liệu từ 4nums.com, có 1,362 trò chơi được sắp xếp từ dễ đến khó theo thời gian giải của con người, và sử dụng một tập con của các trò chơi tương đối khó được lập chỉ mục 901-1,000 để kiểm tra. Đối với mỗi nhiệm vụ, chúng tôi coi đầu ra là thành công nếu nó là một phương trình hợp lệ bằng 24 và sử dụng các số đầu vào mỗi số đúng một lần. Chúng tôi báo cáo tỷ lệ thành công trên 100 trò chơi như metric.

Đường cơ sở. Chúng tôi sử dụng một thúc đẩy đầu vào-đầu ra (IO) tiêu chuẩn với 5 ví dụ trong bối cảnh. Đối với thúc đẩy chuỗi suy nghĩ (CoT), chúng tôi bổ sung mỗi cặp đầu vào-đầu ra với 3 phương trình trung gian, mỗi phương trình hoạt động trên hai số còn lại. Ví dụ, cho đầu vào "4 9 10 13", các suy nghĩ có thể là "13 - 9 = 4 (còn lại: 4 4 10); 10 - 4 = 6 (còn lại: 4 6); 4 * 6 = 24 (còn lại: 24)". Đối với mỗi trò chơi, chúng tôi lấy mẫu thúc đẩy IO và CoT 100 lần để có hiệu suất trung bình. Chúng tôi cũng xem xét một đường cơ sở tự nhất quán CoT, lấy đầu ra đa số từ 100 mẫu CoT, và một phương pháp tinh chỉnh lặp trên một mẫu IO tối đa 10 lần lặp. Tại mỗi lần lặp, LM được điều kiện hóa trên tất cả lịch sử trước đây để "suy ngẫm về sai lầm của bạn và tạo ra một câu trả lời tinh tế" nếu đầu ra không chính xác. Lưu ý rằng nó sử dụng tín hiệu phản hồi sự thật cơ bản về tính đúng đắn của phương trình.

Thiết lập ToT. Để đóng khung Trò chơi số 24 vào ToT, tự nhiên là phân tách các suy nghĩ thành 3 bước, mỗi bước là một phương trình trung gian. Như được hiển thị trong Hình 2(a), tại mỗi nút cây, chúng tôi trích xuất các số còn lại và thúc đẩy LM để đề xuất một số bước tiếp theo có thể. Cùng một "thúc đẩy đề xuất" được sử dụng cho tất cả 3 bước suy nghĩ, mặc dù nó chỉ có một ví dụ với 4 số đầu vào. Chúng tôi thực hiện tìm kiếm theo chiều rộng (BFS) trong ToT, trong đó tại mỗi bước chúng tôi giữ b= 5 ứng cử viên tốt nhất. Để thực hiện BFS có chủ ý trong ToT, như được hiển thị trong Hình 2(b), chúng tôi thúc đẩy LM để đánh giá mỗi ứng cử viên suy nghĩ là "chắc chắn/có thể/không thể" liên quan đến việc đạt 24. Mục đích là thúc đẩy các giải pháp một phần đúng có thể được xác định trong vài thử nghiệm nhìn trước, và loại bỏ các giải pháp một phần không thể dựa trên thông thường "quá lớn/nhỏ", và giữ phần còn lại "có thể". Chúng tôi lấy mẫu giá trị 3 lần cho mỗi suy nghĩ.

Kết quả. Như được hiển thị trong Bảng 2, các phương pháp thúc đẩy IO, CoT, và CoT-SC hoạt động kém trên nhiệm vụ, chỉ đạt được tỷ lệ thành công 7.3%, 4.0%, và 9.0%. Ngược lại, ToT với chiều rộng b= 1 đã đạt tỷ lệ thành công 45%, trong khi b= 5 đạt 74%. Chúng tôi cũng xem xét một thiết lập oracle cho IO/CoT, bằng cách tính tỷ lệ thành công sử dụng tốt nhất của k mẫu (1≤k≤100). Để so sánh IO/CoT (tốt nhất của k) với ToT, chúng tôi xem xét việc tính toán các nút cây được truy cập mỗi nhiệm vụ trong ToT qua b= 1···5, và ánh xạ 5 tỷ lệ thành công trong Hình 3(a), coi IO/CoT (tốt nhất của k) như việc truy cập k nút trong một bandit. Không có gì đáng ngạc nhiên, CoT mở rộng tốt hơn IO, và tốt nhất của 100 mẫu CoT đạt tỷ lệ thành công 49%, nhưng vẫn tệ hơn nhiều so với việc khám phá nhiều nút hơn trong ToT (b >1).

Phân tích lỗi. Hình 3(b) phân tích tại bước nào các mẫu CoT và ToT thất bại trong nhiệm vụ, tức là suy nghĩ (trong CoT) hoặc tất cả b suy nghĩ (trong ToT) không hợp lệ hoặc không thể đạt 24. Đáng chú ý, khoảng 60% mẫu CoT đã thất bại trong nhiệm vụ sau khi tạo ra bước đầu tiên, hoặc tương đương, ba từ đầu tiên (ví dụ " 4 + 9 "). Điều này làm nổi bật các vấn đề với việc giải mã trái-qua-phải trực tiếp.

4.2 Viết sáng tạo
Tiếp theo, chúng tôi phát minh ra một nhiệm vụ viết sáng tạo trong đó đầu vào là 4 câu ngẫu nhiên và đầu ra nên là một đoạn văn mạch lạc với 4 đoạn kết thúc lần lượt bằng 4 câu đầu vào. Một nhiệm vụ như vậy mở và khám phá, thách thức tư duy sáng tạo cũng như lập kế hoạch cấp cao.

Thiết lập nhiệm vụ. Chúng tôi lấy mẫu các câu ngẫu nhiên từ randomwordgenerator.com để tạo thành 100 đầu vào, và không có đoạn văn sự thật cơ bản cho mỗi ràng buộc đầu vào. Vì chúng tôi thấy rằng GPT-4 có thể tuân theo ràng buộc đầu vào hầu hết thời gian, chúng tôi tập trung vào việc đánh giá tính mạch lạc của đoạn văn theo hai cách: sử dụng một thúc đẩy zero-shot GPT-4 để cung cấp điểm số vô hướng 1-10, hoặc sử dụng đánh giá của con người để so sánh các cặp đầu ra từ các phương pháp khác nhau. Đối với cái trước, chúng tôi lấy mẫu 5 điểm và tính trung bình chúng cho mỗi đầu ra nhiệm vụ, và chúng tôi thấy 5 điểm này thường nhất quán, với độ lệch chuẩn khoảng 0.56 trung bình qua các đầu ra. Đối với cái sau, chúng tôi sử dụng một tập con của các tác giả trong một nghiên cứu mù để so sánh tính mạch lạc của các cặp đoạn văn được tạo ra bởi CoT vs. ToT, trong đó thứ tự của các đoạn văn được lật ngẫu nhiên qua 100 đầu vào.

Đường cơ sở. Cho tính chất sáng tạo của nhiệm vụ, cả thúc đẩy IO và CoT đều là zero-shot. Trong khi cái trước thúc đẩy LM để trực tiếp tạo ra một đoạn văn mạch lạc cho ràng buộc đầu vào, cái sau thúc đẩy LM để đầu tiên tạo ra một kế hoạch ngắn gọn sau đó viết đoạn văn, tức là kế hoạch phục vụ như bước suy nghĩ trung gian. Chúng tôi tạo ra 10 mẫu IO và CoT mỗi nhiệm vụ. Chúng tôi cũng xem xét một phương pháp tinh chỉnh lặp (k≤5) trên một mẫu IO ngẫu nhiên cho mỗi nhiệm vụ, trong đó LM được điều kiện hóa trên ràng buộc đầu vào và đoạn văn được tạo ra cuối cùng để quyết định xem đoạn văn đã "hoàn toàn mạch lạc" chưa, và nếu không thì tạo ra một cái tinh tế.

Thiết lập ToT. Chúng tôi xây dựng một ToT với độ sâu 2 (và chỉ 1 bước suy nghĩ trung gian) — LM đầu tiên tạo ra k= 5 kế hoạch và bỏ phiếu cho cái tốt nhất (Hình 4), sau đó tương tự tạo ra k= 5 đoạn văn dựa trên kế hoạch tốt nhất sau đó bỏ phiếu cho cái tốt nhất. Ở đây giới hạn chiều rộng b= 1, vì chỉ một lựa chọn được giữ mỗi bước. Một thúc đẩy bỏ phiếu zero-shot đơn giản ("phân tích các lựa chọn bên dưới, sau đó kết luận cái nào hứa hẹn nhất cho hướng dẫn") được sử dụng để lấy mẫu 5 phiếu bầu tại cả hai bước.

Kết quả. Hình 5(a) cho thấy điểm GPT-4 trung bình qua 100 nhiệm vụ, trong đó ToT (7.56) được coi là tạo ra các đoạn văn mạch lạc hơn IO (6.19) và CoT (6.93) trung bình. Trong khi một metric tự động như vậy có thể có nhiễu, Hình 5(b) xác nhận phát hiện bằng cách cho thấy con người thích ToT hơn CoT trong 41 trên 100 cặp đoạn văn, trong khi chỉ thích CoT hơn ToT trong 21 (38 cặp khác được tìm thấy "tương tự mạch lạc"). Cuối cùng, tinh chỉnh lặp hiệu quả hơn trên nhiệm vụ ngôn ngữ tự nhiên này, nơi nó cải thiện điểm mạch lạc IO từ 6.19 lên 7.67, và điểm mạch lạc ToT từ 7.56 lên 7.91. Chúng tôi tin rằng nó có thể được coi như một phương pháp thứ ba để tạo ra suy nghĩ trong khung ToT, trong đó các suy nghĩ mới có thể nảy sinh từ việc tinh chỉnh các suy nghĩ cũ thay vì được tạo ra độc lập hoặc tuần tự.

4.3 Ô chữ mini
Trong Trò chơi số 24 và Viết Sáng tạo, ToT tương đối nông — nhiều nhất 3 bước suy nghĩ cần thiết để đạt được đầu ra cuối cùng. Ở đây chúng tôi khám phá ô chữ mini 5×5 như một vấn đề tìm kiếm khó hơn liên quan đến ngôn ngữ tự nhiên. Một lần nữa, mục tiêu không chỉ là giải quyết nhiệm vụ, vì ô chữ tổng quát hơn có thể được giải quyết dễ dàng với pipeline NLP chuyên biệt [34] tận dụng truy xuất quy mô lớn thay vì LM. Thay vào đó, chúng tôi nhằm khám phá giới hạn của LM như một người giải quyết vấn đề tổng quát khám phá suy nghĩ của chính mình và hướng dẫn khám phá của chính mình với suy luận có chủ ý như heuristic.

Thiết lập nhiệm vụ. Chúng tôi cạo dữ liệu từ GooBix, có 156 trò chơi ô chữ mini 5×5. Vì chúng tôi quan sát các trò chơi liền kề chứa các manh mối tương tự, chúng tôi sử dụng 20 trò chơi với chỉ mục 1,6,···,91,96 để kiểm tra, và các trò chơi 136,141,146,151,156 để thúc đẩy. Đối với mỗi nhiệm vụ, đầu vào mô tả 5 manh mối ngang và 5 manh mối dọc, và đầu ra nên là một bảng 5×5 = 25 chữ cái để giải ô chữ. Để đánh giá, chúng tôi xem xét ba mức độ thành công: tỷ lệ chữ cái đúng (25 mỗi trò chơi), từ (10 mỗi trò chơi), và trò chơi.

Đường cơ sở. Chúng tôi cung cấp 5 cặp ví dụ đầu vào-đầu ra trong thúc đẩy IO, và trong thúc đẩy CoT bổ sung bao gồm các từ trung gian theo thứ tự h1..5 sau đó v1..5. Chúng tôi chạy mỗi thúc đẩy cho 10 mẫu và tính trung bình kết quả.

Thiết lập ToT. Chúng tôi tận dụng tìm kiếm theo chiều sâu (Thuật toán 2) tiếp tục khám phá manh mối từ tiếp theo hứa hẹn nhất cho đến khi trạng thái không còn hứa hẹn, sau đó quay lại trạng thái cha để khám phá các suy nghĩ thay thế. Để làm cho tìm kiếm có thể xử lý được, các suy nghĩ tiếp theo bị ràng buộc không thay đổi bất kỳ từ hoặc chữ cái đã điền, sao cho ToT có nhiều nhất 10 bước trung gian. Để tạo ra suy nghĩ, tại mỗi trạng thái chúng tôi dịch tất cả các suy nghĩ hiện có (ví dụ "h2.motor; h1.tasks" cho trạng thái trong Hình 6(a)) thành ràng buộc chữ cái cho các manh mối còn lại (ví dụ "v1.To heap: tm ;...") và thúc đẩy một thúc đẩy đề xuất 5 lần để đưa ra ứng cử viên cho nơi và cái gì để điền vào từ tiếp theo. Quan trọng, chúng tôi cũng thúc đẩy LM để đưa ra mức độ tin cậy cho các suy nghĩ khác nhau, và tổng hợp chúng qua các đề xuất để có được một danh sách các suy nghĩ tiếp theo được sắp xếp để khám phá (Hình 6(a)). Để đánh giá trạng thái, chúng tôi tương tự dịch mỗi trạng thái thành ràng buộc chữ cái cho các manh mối còn lại, sau đó đánh giá cho mỗi manh mối xem có thể điền cho ràng buộc không. Nếu bất kỳ manh mối còn lại nào được coi là "không thể" điền (ví dụ "v1. To heap: tm s"), thì việc khám phá cây con của trạng thái được cắt tỉa và DFS quay lại cha của nó để khám phá suy nghĩ hứa hẹn tiếp theo. Chúng tôi giới hạn các bước tìm kiếm DFS đến 100, và đơn giản là hiển thị trạng thái được khám phá sâu nhất (cái được khám phá đầu tiên nếu nhiều) vào đầu ra cuối cùng.

Kết quả. Như được hiển thị trong Bảng 3, các phương pháp thúc đẩy IO và CoT hoạt động kém với tỷ lệ thành công cấp từ dưới 16%, trong khi ToT cải thiện đáng kể tất cả các metric, đạt tỷ lệ thành công cấp từ 60% và giải quyết 4 trên 20 trò chơi. Một cải thiện như vậy không có gì đáng ngạc nhiên, cho rằng IO và CoT thiếu cơ chế để thử các manh mối khác nhau, thay đổi quyết định, hoặc quay lại.

Nghiên cứu Oracle và loại bỏ. Khi xuất ra từ trạng thái DFS oracle tốt nhất (thay vì trạng thái tốt nhất được xác định heuristic) mỗi nhiệm vụ, hiệu suất ToT thậm chí còn cao hơn và thực sự giải quyết 7/20 trò chơi (Bảng 3, "+best state"), cho thấy heuristic đầu ra đơn giản của chúng tôi có thể được cải thiện dễ dàng. Thật thú vị, đôi khi khi trò chơi ô chữ thực sự được giải quyết, bộ đánh giá trạng thái vẫn có thể coi một số từ là "không thể" và cắt tỉa — có thể vì ô chữ 5×5 theo thiết kế có một số từ hiếm hoặc lỗi thời mà GPT-4 không thể nhận ra2. Cho rằng đánh giá trạng thái như một heuristic cắt tỉa không hoàn hảo, chúng tôi cũng khám phá việc loại bỏ cắt tỉa, và thấy hiệu suất thường tệ hơn (Bảng 3, "-prune"). Tuy nhiên, nó thực sự có thể tìm ra giải pháp đúng cho 4/20 trò chơi (mặc dù chỉ xuất ra 1 qua heuristic), 3 trong số đó là các trò chơi mà ToT+pruning không thể giải quyết trong 100 bước. Do đó, heuristic tốt hơn cho cắt tỉa DFS là quan trọng để giải quyết vấn đề trong trường hợp này. Cuối cùng, chúng tôi xác nhận tầm quan trọng của việc quay lại bằng cách chạy một nghiên cứu loại bỏ tiếp tục điền manh mối hứa hẹn nhất tối đa 20 bước, cho phép ghi đè. Điều này tương tự như tìm kiếm "tham lam" BFS với giới hạn chiều rộng b= 1, và hoạt động kém với thành công cấp từ chỉ 20% (Bảng 3, "-backtrack").

5 Công trình liên quan
Lập kế hoạch và ra quyết định. Lập kế hoạch thông minh và ra quyết định quan trọng để đạt được các mục tiêu được xác định trước. Vì chúng được đào tạo trên lượng lớn kiến thức thế giới và ví dụ của con người, LM được biết đến là đã hấp thụ thông thường phong phú khiến có thể đề xuất các kế hoạch hợp lý có điều kiện trên thiết lập vấn đề và trạng thái môi trường [12,42,37,13,35,41,40]. Phương pháp ToT được đề xuất của chúng tôi mở rộng các công thức lập kế hoạch hiện có bằng cách xem xét nhiều kế hoạch khả thi tiềm năng đồng thời tại mỗi bước giải quyết vấn đề, và tiến hành với những cái hứa hẹn nhất. Sự tích hợp giữa lấy mẫu suy nghĩ và phản hồi giá trị tích hợp một cách tự nhiên các cơ chế lập kế hoạch và ra quyết định, cho phép tìm kiếm hiệu quả bên trong cây giải pháp. Mặt khác, các thủ tục ra quyết định truyền thống thường yêu cầu đào tạo các mô hình phần thưởng và chính sách chuyên dụng như trong học tăng cường (ví dụ CHAI [33]), trong khi chúng tôi sử dụng chính LM để cung cấp ước lượng giá trị cho việc ra quyết định. RAP [9] là một công việc đồng thời coi suy luận mô hình ngôn ngữ như lập kế hoạch với mô hình thế giới nội bộ của nó, và đề xuất một phương pháp dựa trên MCTS tương tự như ToT. Tuy nhiên, các nhiệm vụ của nó đơn giản hơn của chúng tôi, và khung của nó thiếu tính mô-đun để kết hợp các thuật toán tìm kiếm cây khác nhau.

Tự phản ánh. Việc sử dụng LLM để đánh giá tính khả thi của các dự đoán của chính chúng đang trở thành một thủ tục ngày càng quan trọng trong giải quyết vấn đề. [28,20,24] giới thiệu cơ chế "tự phản ánh", trong đó LM cung cấp phản hồi cho các ứng cử viên tạo ra của chúng. [4] cải thiện độ chính xác tạo mã của LM bằng cách tiêm các thông điệp phản hồi được tạo ra bởi chính LM dựa trên kết quả thực thi mã của nó. Tương tự, [17] cũng giới thiệu các bước "phê bình" hoặc xem xét các hành động và trạng thái, quyết định hành động tiếp theo để thực hiện trong việc giải quyết các nhiệm vụ vận hành máy tính. Một công việc gần đây khác rất liên quan đến chúng tôi là "giải mã hướng dẫn tự đánh giá" [39]. Tương tự như phương pháp của chúng tôi, giải mã tự đánh giá cũng theo một thủ tục tìm kiếm cây với các lá được lấy mẫu từ giải mã tìm kiếm chùm ngẫu nhiên, sau đó được đánh giá bởi chính LLM với các thúc đẩy tự đánh giá được chuẩn bị cẩn thận. Tuy nhiên phương pháp của họ sử dụng công thức PAL [8] biểu diễn suy nghĩ như mã, khiến khó giải quyết các nhiệm vụ thách thức như viết sáng tạo mà chúng tôi xem xét trong bài báo này. Công thức Tree-of-Thought của chúng tôi do đó linh hoạt hơn và xử lý các nhiệm vụ thách thức mà GPT-4 chỉ đạt độ chính xác rất thấp với các thúc đẩy tiêu chuẩn.

Tạo ra LLM hướng dẫn bởi chương trình. Đề xuất của chúng tôi cũng liên quan đến các tiến bộ gần đây tổ chức hành vi của LM với các thủ tục có hệ thống [14,44,6,43] hoặc hướng dẫn chương trình biểu tượng. Ví dụ, Schlag et al. [27] nhúng LM trong một thủ tục tìm kiếm thuật toán để giúp giải quyết vấn đề như trả lời câu hỏi từng bước, trong đó cây tìm kiếm được mở rộng bởi các đoạn văn liên quan có thể cung cấp câu trả lời. Tuy nhiên phương pháp này khác với chúng tôi ở chỗ cây được mở rộng bằng việc lấy mẫu các đoạn văn bên ngoài thay vì suy nghĩ của chính LM, và không có các bước phản ánh hoặc bỏ phiếu. Một phương pháp khác, LLM+P [18], đi xa hơn và ủy thác quá trình lập kế hoạch thực tế cho một người lập kế hoạch cổ điển.

Các phương pháp tìm kiếm cổ điển. Cuối cùng nhưng không kém phần quan trọng, phương pháp của chúng tôi có thể được coi như một phiên bản hiện đại của các phương pháp tìm kiếm cổ điển để giải quyết vấn đề. Ví dụ nó có thể được coi như một thuật toán tìm kiếm heuristic như A* [10], trong đó heuristic tại mỗi nút tìm kiếm được cung cấp bởi tự đánh giá của LM. Từ góc độ này, phương pháp của chúng tôi cũng liên quan đến giải mã NeuroLogic A*esque [19], được lấy cảm hứng từ tìm kiếm A* nhưng giới thiệu heuristic nhìn trước hiệu quả cho LM để cải thiện giải mã tìm kiếm chùm hoặc lấy mẫu top-k. Tuy nhiên phương pháp này bị ràng buộc cho các nhiệm vụ tạo câu, trong khi khung của chúng tôi được thiết kế cho việc giải quyết vấn đề đa bước phức tạp được bảo vệ bởi phản hồi giá trị.

6 Thảo luận
Hạn chế và hướng tương lai. Tìm kiếm có chủ ý như ToT có thể không cần thiết cho nhiều nhiệm vụ hiện có mà GPT-4 đã xuất sắc (xem Phụ lục B.1), và như một bước đầu công việc này chỉ khám phá ba nhiệm vụ tương đối đơn giản thách thức GPT-4 (xem Phụ lục B.2 cho một số kết quả thí nghiệm GPT-3.5) và kêu gọi khả năng tìm kiếm và lập kế hoạch tốt hơn được kết hợp với LM. Tuy nhiên, khi chúng ta bắt đầu triển khai LM cho các ứng dụng ra quyết định thế giới thực phức tạp hơn (ví dụ lập trình, phân tích dữ liệu, robot, v.v.), các nhiệm vụ phức tạp hơn có thể xuất hiện và mang lại cơ hội mới để nghiên cứu những câu hỏi nghiên cứu này. Ngoài ra, các phương pháp tìm kiếm như ToT yêu cầu nhiều tài nguyên hơn (ví dụ chi phí API GPT-4) so với các phương pháp lấy mẫu để cải thiện hiệu suất nhiệm vụ, nhưng tính linh hoạt mô-đun của ToT cho phép người dùng tùy chỉnh các đánh đổi hiệu suất-chi phí như vậy, và các nỗ lực mã nguồn mở đang diễn ra [32] sẽ dễ dàng giảm các chi phí như vậy trong tương lai gần. Chi tiết hơn về chi phí và hiệu quả có trong Phụ lục B.3. Cuối cùng, công việc này tập trung vào việc sử dụng một LM có sẵn, và tinh chỉnh LM sử dụng kiểu ra quyết định phản thực tế cấp cao ToT (ví dụ thảo luận về các lựa chọn tiềm năng cho đoạn văn tiếp theo, thay vì dự đoán token tiếp theo) có thể mang lại cơ hội để nâng cao khả năng giải quyết vấn đề của LM.

Kết luận. "Hệ thống 1" kết hợp của LM có thể được tăng cường có lợi bởi một "Hệ thống 2" dựa trên việc tìm kiếm một cây các con đường có thể tới giải pháp của một vấn đề. Khung Tree of Thoughts cung cấp một cách để dịch những hiểu biết cổ điển về giải quyết vấn đề thành các phương pháp có thể hành động cho LM đương đại. Đồng thời, LM giải quyết một điểm yếu của những phương pháp cổ điển này, cung cấp một cách để giải quyết các vấn đề phức tạp không dễ hình thức hóa, như viết sáng tạo. Chúng tôi xem giao điểm này của LM với các phương pháp cổ điển đối với AI như một hướng thú vị.

Tác động rộng lớn hơn
ToT là một khung trao quyền cho LM để tự chủ và thông minh hơn trong việc ra quyết định và giải quyết vấn đề. Trong khi các nhiệm vụ hiện tại bị giới hạn trong các vấn đề suy luận và tìm kiếm, các ứng dụng tương lai liên quan đến tương tác với môi trường bên ngoài hoặc con người có thể mang lại nguy hiểm tiềm ẩn, ví dụ tạo điều kiện cho việc sử dụng có hại của LM. Mặt khác, ToT cũng cải thiện khả năng diễn giải của các quyết định mô hình và cơ hội cho sự liên kết con người, vì các biểu diễn kết quả có thể đọc được, suy luận ngôn ngữ cấp cao thay vì các giá trị token cấp thấp, ngầm.

Lời cảm ơn
SY và KN thừa nhận sự hỗ trợ từ một giải thưởng Nghiên cứu Hợp tác Oracle và Quỹ Khoa học Quốc gia dưới Số Tài trợ 2239363. Bất kỳ ý kiến, phát hiện, kết luận, hoặc khuyến nghị nào được thể hiện trong tài liệu này là của (các) tác giả và không nhất thiết phản ánh quan điểm của Quỹ Khoa học Quốc gia. SY cũng được hỗ trợ bởi Harold W. Dodds Fellowship từ Princeton.

Tài liệu tham khảo
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[2] C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4:1–43, 2012.

[3] M. Campbell, A. J. Hoane Jr, and F.-h. Hsu. Deep blue. Artificial intelligence, 134(1-2):57–83, 2002.

[4] X. Chen, M. Lin, N. Schärli, and D. Zhou. Teaching large language models to self-debug, 2023.

[5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[6] A. Creswell and M. Shanahan. Faithful reasoning using large language models. arXiv preprint arXiv:2208.14271, 2022.

[7] N. D. Daw, Y. Niv, and P. Dayan. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nature neuroscience, 8(12):1704–1711, 2005.

[8] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-aided language models, 2023.

[9] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.

[10] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100–107, 1968. doi: 10.1109/TSSC.1968.300136.

[11] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100–107, 1968.

[12] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents, 2022.

[13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022.

[14] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L. Bras, and Y. Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint arXiv:2205.11822, 2022.

[15] D. Kahneman. Thinking, fast and slow. Macmillan, 2011.

[16] D. Kahneman, S. Frederick, et al. Representativeness revisited: Attribute substitution in intuitive judgment. Heuristics and biases: The psychology of intuitive judgment, 49(49-81):74, 2002.

[17] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.

[18] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering large language models with optimal planning proficiency, 2023.

[19] X. Lu, S. Welleck, P. West, L. Jiang, J. Kasai, D. Khashabi, R. L. Bras, L. Qin, Y. Yu, R. Zellers, N. A. Smith, and Y. Choi. Neurologic a*esque decoding: Constrained text generation with lookahead heuristics. In North American Chapter of the Association for Computational Linguistics, 2021.

[20] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback, 2023.

[21] A. Newell, J. C. Shaw, and H. A. Simon. Report on a general problem solving program. In IFIP congress, volume 256, page 64. Pittsburgh, PA, 1959.

[22] A. Newell, H. A. Simon, et al. Human problem solving. Prentice-Hall, 1972.

[23] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.

[24] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner: Reasoning feedback on intermediate representations, 2023.

[25] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018.

[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[27] I. Schlag, S. Sukhbaatar, A. Celikyilmaz, W. tau Yih, J. Weston, J. Schmidhuber, and X. Li. Large language model programs, 2023.

[28] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection, 2023.

[29] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. nature, 550 (7676):354–359, 2017.

[30] S. A. Sloman. The empirical case for two systems of reasoning. Psychological bulletin, 119(1): 3, 1996.

[31] K. E. Stanovich. Who is rational? Studies of individual differences in reasoning. Psychology Press, 1999.

[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[33] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with offline reinforcement learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4471–4491, 2022.

[34] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated crossword solving. arXiv preprint arXiv:2205.09665, 2022.

[35] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models, 2023.

[36] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

[37] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2023.

[38] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.

[39] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition enhances reasoning via self-evaluation guided decoding, 2023.

[40] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for decision making: Problems, methods, and opportunities, 2023.

[41] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.

[42] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.

[43] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.

[44] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang. Solving math word problem via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257, 2022.

A Mã, Thúc đẩy, Quỹ đạo
Tất cả mã có sẵn tại https://github.com/princeton-nlp/tree-of-thought-llm.
Tất cả thúc đẩy có sẵn tại https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/src/tot/prompts.
Quỹ đạo có sẵn tại https://github.com/princeton-nlp/tree-of-thought-llm/tree/master/logs.

B Kết quả Thí nghiệm Bổ sung
Cho động lực khám phá và mở rộng biên giới khả năng của các mô hình ngôn ngữ, các thí nghiệm của chúng tôi trong bài báo chính đã tập trung vào một thiết lập với mô hình ngôn ngữ tiên tiến nhất (GPT-4), và ba nhiệm vụ khó được phát minh để thách thức nó. Ở đây, chúng tôi báo cáo các thí nghiệm bổ sung với LLM yếu hơn hoặc nhiệm vụ dễ hơn, và thảo luận chi phí và hiệu quả.

B.1 Mở rộng cho các nhiệm vụ mới (GSM8k, StrategyQA) với ToT zero-shot
Trong khi các nhiệm vụ NLP phổ biến hơn có thể quá dễ cho GPT-4 và không yêu cầu ToT (đó là lý do chúng tôi xem xét các nhiệm vụ mới khó hơn), chúng tôi tin rằng áp dụng ToT cho các nhiệm vụ mới có thể đơn giản. Ví dụ, chúng tôi đã thực hiện một ToT-BFS zero-shot đơn giản và chung tương tự như viết sáng tạo (lấy mẫu 5 chiến lược giải quyết vấn đề sau đó bỏ phiếu cho cái tốt nhất; sau đó lấy mẫu 5 giải pháp dựa trên chiến lược tốt nhất sau đó bỏ phiếu cho cái tốt nhất) cho GSM8K và StrategyQA với vài dòng mã thêm:

# định nghĩa định dạng câu trả lời cho các nhiệm vụ mới
gsm8k_format = '"the answer is n" where n is a number'
strategyqa_format = 'either "the answer is yes" or "the answer is no"'
# định nghĩa thúc đẩy io zero-shot
standard_prompt = 'Answer the following question with {format}: {input}'
# định nghĩa định dạng suy nghĩ cho cot zero-shot và tot zero-shot
cot_prompt = '''Answer the following question: {input}
Make a strategy then write. Your output should be of the following format:
Strategy:
Your strategy about how to answer the question.
Answer:
Your answer to the question. It should end with {format}.
'''
# định nghĩa bỏ phiếu zero-shot được sử dụng cho tot zero-shot
vote_prompt = '''Given an instruction and several choices,
decide which choice is most promising.
Analyze each choice in detail, then conclude in the last line
"The best choice is {s}", where s the integer id of the choice.
'''

Chúng tôi đánh giá trên một tập con 100 câu hỏi GSM8K test và StrategyQA dev ngẫu nhiên. Như được hiển thị trong Bảng 4 và như mong đợi, ToT cải thiện so với CoT trên cả hai nhiệm vụ (nhưng chỉ một chút, cho rằng GPT-4 + CoT đã rất tốt trên các nhiệm vụ như vậy, và nút thắt cổ chai của StrategyQA là kiến thức bên ngoài, không phải suy luận). Xem xét chi phí tính toán, phù hợp hơn để thử LLM nhỏ hơn + ToT cho các nhiệm vụ NLP truyền thống, hoặc GPT-4 + ToT cho các nhiệm vụ khó thách thức suy luận GPT-4 + CoT.

B.2 Mở rộng cho LM mới (GPT-3.5)
Để hiểu ToT hoạt động như thế nào với các LLM khác, chúng tôi cũng đã chạy GPT-3.5-turbo cho Viết Sáng tạo (Bảng 6) và Trò chơi số 24 (Bảng 5). Trên cả hai nhiệm vụ, "ToT >CoT>IO" vẫn đúng cho GPT-3.5. Trên Viết Sáng tạo, chúng tôi thấy GPT-3.5+ToT vượt trội GPT-4+IO, và tương tự như GPT-4+CoT, cho thấy ToT cũng có thể hoạt động tốt trên các mô hình ngôn ngữ yếu hơn.

Trên Trò chơi số 24 (chúng tôi đã thay đổi thúc đẩy đề xuất 1-shot thành 3-shot để làm cho nó hoạt động), 19% của GPT-3.5+ToT tệ hơn nhiều so với 74% của GPT-4+ToT. Để hiểu thêm tầm quan trọng của tạo ra vs. đánh giá, chúng tôi đã chạy GPT-4 tạo ra + GPT-3.5 đánh giá (64%) và GPT-3.5 tạo ra + GPT-4 đánh giá (31%). Điều này cho thấy nút thắt cổ chai của trò chơi là tạo ra suy nghĩ, và các mô hình ngôn ngữ tạo ra/đánh giá khác nhau có thể đạt được kết quả khá tốt trong khi giảm chi phí.

B.3 Chi phí và hiệu quả
Chạy ToT yêu cầu tính toán đáng kể hơn so với thúc đẩy IO hoặc CoT. Ví dụ, trong Trò chơi số 24 (Bảng 7 bên dưới), giải quyết một vấn đề với ToT yêu cầu 5.5k token hoàn thành, gần 100 thử nghiệm CoT (6.7k token). Nhưng hiệu suất của ToT tốt hơn so với tốt nhất của 100 thử nghiệm CoT độc lập.

Trên Viết Sáng tạo (Bảng 8 bên dưới), chúng tôi thấy ToT mất khoảng 5x token hoàn thành và chi phí tiền, điều này trực quan vì b= 5 và hầu hết token được tạo ra là các đoạn văn.

Vì vậy hoàn thành các thí nghiệm ToT chính của Trò chơi số 24 và Viết Sáng tạo có chi phí khoảng 0.74×100 + 0.32×100 = 106 đô la. Các thí nghiệm DFS của Crosswords cũng nên trong vòng 100 đô la. Nói chung, chi phí và hiệu quả của ToT phụ thuộc rất nhiều vào các thúc đẩy và thuật toán tìm kiếm được sử dụng, và có thể yêu cầu 5-100 lần token được tạo ra nhiều hơn so với CoT. Một số hiểu biết có thể hành động:

• Chúng tôi khuyến nghị sử dụng ToT cho các nhiệm vụ đòi hỏi suy luận có chủ ý, mà CoT gặp khó khăn.
• Tính linh hoạt của ToT cho phép một số đánh đổi hiệu suất-chi phí, ví dụ, thay đổi kích thước chùm hoặc số phiếu bầu trong BFS, thúc đẩy few-shot vs. zero-shot, GPT-3.5 vs. GPT-4, v.v. Người ta có thể cấu hình thiết lập dựa trên một số ràng buộc tài nguyên hoặc mục tiêu hiệu suất.
• Có nhiều không gian để cải thiện hiệu quả, ví dụ, BFS có thể dừng sớm khi giải pháp được tìm thấy, hoặc cắt giảm kích thước chùm khi một số suy nghĩ "không thể".
• Chúng tôi tin rằng nhiều tính toán thực sự cần thiết để mô hình đạt được trí thông minh mạnh hơn, và điều này không nên trở thành vấn đề cản trở vì về lâu dài, LM (mã nguồn mở) sẽ trở nên rẻ hơn và hiệu quả hơn nhiều. Đó cũng là một hướng tuyệt vời về cách đào tạo/tinh chỉnh LM tốt hơn cho việc tạo ra suy nghĩ và/hoặc đánh giá.
