# 2404.02575.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2404.02575.pdf
# Kích thước tệp: 3974691 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mô hình Ngôn ngữ như Trình biên dịch: Mô phỏng Thực thi Mã giả 
cải thiện Lý luận Thuật toán trong Mô hình Ngôn ngữ

Hyungjoo Chae1, Yeonghyeon Kim1, Seungone Kim2, Kai Tzu-iunn Ong1,
Beong-woo Kwak1, Moohyeon Kim1, Seonghwan Kim1, Taeyoon Kwon1,
Jiwan Chung1, Youngjae Yu1, Jinyoung Yeo1
1Đại học Yonsei 2KAIST AI
{mapoout, jinyeo }@yonsei.ac.kr

Tóm tắt
Lý luận thuật toán đề cập đến khả năng hiểu các mẫu phức tạp đằng sau vấn đề và phân tách chúng thành một chuỗi các bước lý luận hướng tới giải pháp. Bản chất này của lý luận thuật toán khiến nó trở thành thách thức đối với các mô hình ngôn ngữ lớn (LLM), mặc dù chúng đã thể hiện hiệu suất đầy hứa hẹn trong các nhiệm vụ lý luận khác. Trong bối cảnh này, một số nghiên cứu gần đây sử dụng các ngôn ngữ lập trình (ví dụ: Python) để thể hiện logic cần thiết để giải quyết một thể hiện/câu hỏi nhất định (ví dụ: Program-of-Thought) được truyền cảm hứng từ cú pháp nghiêm ngặt và chính xác của chúng. Tuy nhiên, việc viết mã thực thi thể hiện logic chính xác một cách tự phát trong một lần gọi suy luận duy nhất là không tầm thường. Ngoài ra, mã được tạo cụ thể cho một thể hiện không thể được tái sử dụng cho các thể hiện khác, ngay cả khi chúng đến từ cùng một nhiệm vụ và có thể yêu cầu logic giống hệt để giải quyết. Bài báo này giới thiệu THINK-AND-EXECUTE, một khung làm việc mới phân tách quá trình lý luận của mô hình ngôn ngữ thành hai bước. (1) Trong THINK, chúng tôi khám phá logic cấp nhiệm vụ được chia sẻ trên tất cả các thể hiện để giải quyết một nhiệm vụ nhất định và sau đó thể hiện logic bằng mã giả; (2) Trong EXECUTE, chúng tôi tiếp tục điều chỉnh mã giả được tạo cho từng thể hiện và mô phỏng việc thực thi mã. Với các thí nghiệm rộng rãi trên bảy nhiệm vụ lý luận thuật toán, chúng tôi chứng minh hiệu quả của THINK-AND-EXECUTE. Phương pháp của chúng tôi cải thiện lý luận của LM tốt hơn so với một số đường cơ sở mạnh thực hiện lý luận cụ thể theo thể hiện (ví dụ: CoT và PoT), gợi ý tính hữu ích của việc khám phá logic cấp nhiệm vụ. Ngoài ra, chúng tôi cho thấy rằng so với ngôn ngữ tự nhiên, mã giả có thể hướng dẫn lý luận của LM tốt hơn, mặc dù chúng được huấn luyện để tuân theo hướng dẫn ngôn ngữ tự nhiên.

1 Giới thiệu
Lý luận trong các mô hình ngôn ngữ lớn (LLM) thường bao gồm việc phân tích cấu trúc logic cơ bản của một vấn đề và hiện thực hóa logic thành một chuỗi các bước lý luận để rút ra câu trả lời cuối cùng (Zhou et al., 2022a;b; Hao et al., 2023). Đặc biệt, lý luận thuật toán từ lâu đã là một thách thức to lớn đối với LLM, vì nó yêu cầu phải xem xét kỹ lưỡng một mẫu lý luận phức tạp và dịch nó thành một chuỗi dài các bước lý luận (Suzgun et al., 2022; Valmeekam et al., 2022; Pan et al., 2023).

Để cải thiện khả năng lý luận của LLM, các nghiên cứu trước chủ yếu theo đuổi hai hướng. Hướng đầu tiên bao gồm việc tăng cường bước thực thi lý luận bằng cách tạo ra một lý do bằng ngôn ngữ tự nhiên (ví dụ: Chain-of-Thought (Wei et al., 2022; Kojima et al., 2022)) hoặc một đoạn mã (ví dụ: Program-of-Thought (Chen et al., 2023), Program-Aided LMs (Gao et al., 2023)). Tuy nhiên, các phương pháp như vậy thực hiện lý luận từng bước một cách tự phát, không có giai đoạn dành riêng cho việc lập kế hoạch. Điều này đòi hỏi LLM phải phân tích logic và thực thi nó trong một lần gọi suy luận duy nhất, điều này hạn chế tính biểu đạt của nó. Hơn nữa,

--- TRANG 2 ---
Đầu tiên, hãy phân tích các câu phát biểu:
1. Vina nói sự thật. 2. Helene nói 
Vina nói dối. 3. Kandi nói Helene nói
sự thật. 4. Jamey nói Kandi nói dối. 5.
Ka nói Jamey nói dối. Từ câu phát biểu
2, chúng ta có thể suy ra rằng Helene nói dối,
có nghĩa là ...... 
...... chúng ta có thể suy ra rằng Ka nói
sự thật. Do đó, Ka nói sự thật.
 
Do đó, câu trả lời là Có.

def solution():
    vina = True
    helene = not vina
    kandi = vina
    jamey = not kandi
    ka = not jamey
    
    return 'Có' if ka else 'Không'
solution()

>>> Có

Áp dụng cho
mã giả được tạo
prompt

Đọc một văn bản đầu vào đã cho và trả lời câu hỏi trong
văn bản đầu vào bằng "Có" hoặc "Không".
Văn bản đầu vào:
Vina nói sự thật. Helene nói Vina nói dối. Kandi nói
Helene nói sự thật. Jamey nói Kandi nói dối. Ka nói
Jamey nói dối. Ka có nói sự thật không?

LLM

LLM

Vina nói sự thật. Vina nói sự thật: True
...... nói Kandi nói dối. Jamey nói sự thật: True
Ka nói Jamey nói dối. Ka nói sự thật: False
Câu trả lời cuối cùng: Không

def web_of_lies(input_text):
    statements, question = extract_information(input_text)
    statements = statements.split(".")...
    truth_dict = {}

    for statement in statements:
        action = get_action(statement)
        person1, person2 = get_people(statement)
        if action == "nói dối":
            truth_dict[person1] = not truth_dict[person2]
        else:
            truth_dict[person1] = truth_dict[person2]
        print(f"{person1} nói {person2} {action}. {person1} nói sự thật: {truth_dict[person1]}")

    person_to_check = get_target_person(question)
    answer = 'Có' if truth_dict[person_to_check] else 'Không'
    return answer

Sử dụng các hàm trừu tượng để
thể hiện logic.
Print() statements để xuất các lý luận CoT.

Hình 1: Minh họa về THINK-AND-EXECUTE, so sánh với Zero-shot Chain-of-Thought (Kojima et al., 2022) và Program-of-Thoughts (Chen et al., 2023).

khi gặp phải một vấn đề tương tự, LLM phải giải quyết nó mà không thể tái sử dụng logic đã hiểu trước đó.

Hướng thứ hai bao gồm việc tạo ra một kế hoạch được mô tả bằng ngôn ngữ tự nhiên một cách rõ ràng với LLM. Kế hoạch mô tả logic của nhiệm vụ và LLM sau đó sẽ cụ thể hóa nó thành một chuỗi các bước lý luận (ví dụ: Least-to-Most (Zhou et al., 2022b), Plan-and-Solve (Wang et al., 2023)). Tuy nhiên, như các nghiên cứu trước đã đề cập, trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi thấy rằng ngôn ngữ tự nhiên có thể không phải là phương tiện tối ưu để mô tả logic của vấn đề (Li et al., 2023). Ngoài ra, các nghiên cứu trước chủ yếu dựa vào việc tạo ra một kế hoạch bằng cách quan sát một thể hiện duy nhất, điều này cản trở việc phân tích mẫu lý luận cốt lõi được chia sẻ trên các thể hiện tương tự trong một nhiệm vụ duy nhất (Zhou et al., 2024).

Để giải quyết những vấn đề này, chúng tôi giới thiệu THINK-AND-EXECUTE, một khung thuật toán khám phá logic phản ánh mẫu lý luận được chia sẻ đằng sau một nhiệm vụ nhất định, và tiến hành lý luận bằng cách điều chỉnh logic vào từng thể hiện. THINK-AND-EXECUTE bao gồm ba bước đặc biệt; Đầu tiên chúng tôi yêu cầu LLM THINK về các mẫu lý luận phổ biến của một nhiệm vụ bằng cách cung cấp cho nó một vài câu hỏi mẫu. Sau đó, LLM dịch mô tả ngôn ngữ tự nhiên của logic thành định dạng mã giả. Định dạng mã giả cho phép linh hoạt hơn trong việc áp dụng logic cho từng thể hiện so với ngôn ngữ lập trình như Python. Cuối cùng, trong bước EXECUTE, LLM mô phỏng việc thực thi mã giả cấp nhiệm vụ để tuân theo logic trong đó và dự đoán kết quả đầu ra của mã giả.

Thông qua các thí nghiệm rộng rãi trên 7 nhiệm vụ lý luận thuật toán từ Big-Bench Hard (Suzgun et al., 2022), chúng tôi cho thấy hiệu quả của THINK-AND-EXECUTE so với các đường cơ sở thách thức. Hiệu suất vượt trội của THINK-AND-EXECUTE so với PoT cho thấy rằng việc khám phá logic chung cho một nhiệm vụ nhất định và áp dụng nó cho từng thể hiện sẽ hữu ích hơn việc viết mã cụ thể theo thể hiện cho mỗi thể hiện. Đáng chú ý, việc mô phỏng thực thi mã giả được chứng minh là cải thiện lý luận của LM nhiều hơn việc lập kế hoạch bằng ngôn ngữ tự nhiên (NL), mặc dù chúng được huấn luyện để tuân theo hướng dẫn NL. Hơn nữa, chúng tôi chứng minh thực nghiệm rằng prompt mã giả được khám phá bởi LLM có thể được áp dụng cho các mô hình ngôn ngữ nhỏ (SLM), chẳng hạn như CodeLlama-7B, để tăng cường khả năng lý luận của chúng. Điều này chỉ ra hiệu quả của THINK-AND-EXECUTE so với các phương pháp prompt mã khác yêu cầu LLM tạo mã cụ thể theo thể hiện mỗi lần (ví dụ: PoT).

Tóm lại, những đóng góp của chúng tôi như sau:
• Chúng tôi giới thiệu THINK-AND-EXECUTE, một khung làm việc thực hiện lý luận với mã giả chứa cấu trúc logic chung của một nhiệm vụ nhất định.

--- TRANG 3 ---
THINK: Hướng dẫn cấp Nhiệm vụ          EXECUTE: Lý luận cấp Thể hiện

Chú thích
Thể hiện -> Câu trả lời
IA

Câu trả lời
Prompt Mã giả
Câu hỏi + Reasoner LM

Nhiệm vụ 1
Câu hỏi * 3
......
Phân tích

Nhiệm vụ 2
...

Nhiệm vụ 3
Câu hỏi * 3
Phân tích

Prompt Mã giả

Prompt Mã giả

Nhiệm vụ Mục tiêu: Web of Lies

Meta prompt
Câu hỏi * 3

Prompt Mã giả
def web_of_lies(input_text):
   
    for statement in statements:
        action = get_action(statement)
        
    return answer

Instructor LM

Instructor LM
...
Phân tích

Xây dựng bản đồ tính chân thực
Xử lý các câu phát biểu:
Tạo một bản đồ hoặc từ điển để biểu diễn
mối quan hệ giữa ......
:
Đối với mỗi câu phát biểu, cập nhật
bản đồ tính chân thực ......

Hình 2: Tổng quan về THINK-AND-EXECUTE. Trong THINK (Trên), LLM phân tích nhiệm vụ đã cho được cung cấp trong meta prompt và tạo ra prompt mã giả mô tả logic cần thiết để giải quyết nhiệm vụ. Sau đó, trong EXECUTE (Dưới), LLM tiến hành lý luận cho từng thể hiện bằng cách mô phỏng việc thực thi prompt mã giả.

• Chúng tôi cho thấy rằng THINK-AND-EXECUTE đạt được những cải thiện đáng kể so với các đường cơ sở mạnh, bao gồm Chain-of-Thought và Program-of-Thought prompting, trên các nhiệm vụ thuật toán khác nhau trong Big-Bench Hard.

• Chúng tôi chứng minh rằng mã giả được viết bởi LLM có thể được chuyển giao cho SLM, cho thấy hiệu quả của phương pháp của chúng tôi.

2 THINK-AND-EXECUTE

Trong phần này, chúng tôi giới thiệu THINK-AND-EXECUTE và cung cấp giải thích chi tiết về cách LLM thực hiện lý luận với nó. Chúng tôi kết hợp Instructor LM I và Reasoner LM R, cho THINK và EXECUTE, tương ứng. Hình 2 cho thấy tổng quan về khung làm việc của chúng tôi.

2.1 THINK: Mô tả Logic Cơ bản của Nhiệm vụ theo Định dạng Mã giả

Mục tiêu của Instructor LM I trong giai đoạn này là khám phá logic cơ bản để giải quyết một nhiệm vụ t đã cho, và tạo ra một prompt mô tả logic, sẽ được áp dụng thêm cho tất cả các thể hiện của nhiệm vụ (trong EXECUTE). Prompt này được xây dựng bằng mã giả thay vì ngôn ngữ tự nhiên, được sử dụng trong nghiên cứu trước để hướng dẫn LM thực hiện lý luận từng bước (Kojima et al., 2022; Wang et al., 2023).

Bước 1: Xây dựng meta prompt. Để prompt Instructor LM I tạo ra mã giả cấp nhiệm vụ cho nhiệm vụ mục tiêu t đã cho, chúng tôi cung cấp P của các nhiệm vụ khác như minh họa trong meta prompt.¹ Trong thực tế, chúng tôi xây dựng meta prompt với 3 nhiệm vụ được lấy mẫu ngẫu nhiên (3 câu hỏi mẫu, phân tích, và P cho mỗi nhiệm vụ) từ T như minh họa và nhiệm vụ mục tiêu t (3 câu hỏi mẫu không có câu trả lời).²

¹ Chúng tôi chú thích P thủ công cho mỗi nhiệm vụ trong T trước. Xem Phụ lục B.1 để biết ví dụ.
² Chúng tôi sử dụng các câu hỏi của các thể hiện mẫu trong prompt few-shot trong Big-Bench Hard.

--- TRANG 4 ---
Bước 2: Phân tích nhiệm vụ mục tiêu. Được cung cấp meta prompt, I tạo ra một phân tích chứa logic lý luận chính cần thiết để giải quyết nhiệm vụ mục tiêu bất kể các thể hiện (câu hỏi). Ví dụ, trong Hình 2 (Trên), phân tích được tạo ra chỉ ra rằng việc xây dựng bản đồ tính chân thực và cập nhật nó bằng cách xử lý các câu phát biểu là cần thiết để giải quyết nhiệm vụ, tức là Web of Lies. Bước này hướng dẫn I tập trung vào quá trình lý luận được chia sẻ giữa tất cả các thể hiện, điều này sẽ quan trọng trong việc tạo ra prompt cấp nhiệm vụ.

Bước 3: Tạo prompt mã giả dựa trên phân tích. Tiếp theo, dựa trên phân tích, I viết một prompt P theo dạng mã giả, phân tách các bước lý luận cần thiết để giải quyết nhiệm vụ mục tiêu. Chúng tôi chọn sử dụng định dạng mã giả thay vì dạng kế hoạch ngôn ngữ tự nhiên (Kojima et al., 2022; Wang et al., 2023) vì hai lý do chính: (1) hiệu quả của nó trong việc mô tả logic đằng sau một nhiệm vụ (ví dụ: tránh sử dụng hướng dẫn lặp lại thông qua vòng lặp for), và (2) hướng dẫn về cái gì và khi nào tạo ra lý luận thông qua đối số trong câu lệnh print() và vị trí trong việc thực thi mã. Ví dụ, trong Hình 2, P chứa câu lệnh, print(f"{person1} nói {person2} {action}. {person1} nói sự thật: {truth_dict[person1]}"), hướng dẫn Reasoner LM tạo ra một lý luận hữu ích trong việc theo dõi bản đồ sự thật chứa tính chân thực của mỗi người, trong quá trình thực thi P. Chúng tôi cung cấp thêm ví dụ về meta prompt, phân tích, và prompt mã giả trong Phụ lục G.

2.2 EXECUTE: Mô phỏng Thực thi Prompt Mã giả cho một Thể hiện

Reasoner LM R sau đó tiến hành lý luận với prompt mã giả P được tạo ra, điều chỉnh logic trong P cho thể hiện đã cho. Theo Wei et al. (2022), chúng tôi nhằm tối đa hóa khả năng lý luận của LM bằng cách hướng dẫn chúng tạo ra các bước lý luận trung gian một cách rõ ràng, được biết đến như lý luận chuỗi suy nghĩ (CoT). R được hướng dẫn dự đoán không chỉ kết quả đầu ra cuối cùng của mã, mà còn các đầu ra thực thi trung gian như lý luận. Cụ thể, R dự đoán một danh sách các đầu ra O={o1,o2, ...,ok} của mã giả bằng cách mô phỏng quá trình thực thi của P, trong đó oi biểu thị đầu ra hệ thống thứ i từ các câu lệnh print(), và {o1}k−1₁ là các lý luận CoT hướng tới câu trả lời cuối cùng ok. Chúng tôi giả định rằng việc theo dõi các kết quả thực thi trung gian sẽ có lợi cho R để theo dõi trạng thái của các biến trong khi chúng thay đổi qua việc thực thi mã. Chúng tôi cho phép R bắt chước hành vi của một trình biên dịch với thông điệp hệ thống "Tạo ra các đầu ra mong đợi (từ tất cả các hàm print())". Câu trả lời cuối cùng cho một câu hỏi đã cho được xuất ra với lệnh "print("Câu trả lời cuối cùng: {answer}")" như đầu ra hệ thống cuối cùng ok.

3 Thiết lập Thí nghiệm

3.1 Tập dữ liệu

Chúng tôi tuyển chọn bảy nhiệm vụ lý luận thuật toán từ Big-Bench Hard (Suzgun et al., 2022), bao gồm: ngôn ngữ dyck; hình học; điều hướng; lý luận về các vật thể có màu; chuỗi thời gian; theo dõi các mục tiêu được xáo trộn; mạng lưới dối trá. Những nhiệm vụ này được thiết kế đặc biệt để đo lường khả năng lý luận từng bước của LLM. Hiệu suất mô hình được đánh giá trong thiết lập zero-shot, nơi chúng tôi không cung cấp minh họa trong prompt. Chúng tôi cung cấp giải thích chi tiết trong Phụ lục A.4.

3.2 Đường cơ sở

Chúng tôi xem xét các đường cơ sở sau: (1) Prompting trực tiếp: Dự đoán trực tiếp câu trả lời mà không tạo ra bất kỳ lý luận nào. (2) Zero-shot CoT (Kojima et al., 2022): Một thiết lập trong đó LLM được khơi gợi để tạo ra các bước lý luận với "Hãy suy nghĩ từng bước", trước câu trả lời. (3) Zero-shot PoT (Chen et al., 2023): Một thiết lập trong đó LLM tạo ra mã Python cụ thể theo thể hiện có thể được thực thi bằng trình thông dịch Python. Sau đó, kết quả thực thi được sử dụng làm câu trả lời cuối cùng. (4) Lập kế hoạch NL: Một biến thể của THINK-AND-EXECUTE, trong đó hướng dẫn cấp nhiệm vụ được tạo ra bằng ngôn ngữ tự nhiên, thay vì mã giả.

--- TRANG 5 ---
[THIS IS TABLE: Performance comparison table showing Reasoner/Method performance across different tasks (DL, GS, Nav, CO, TS, SO, WL) and Average scores for CodeLlama-7B, CodeLlama-13B, and GPT-3.5-Turbo models]

Bảng 1: Hiệu suất zero-shot của THINK-AND-EXECUTE và các đường cơ sở trên bảy nhiệm vụ lý luận thuật toán, bao gồm Ngôn ngữ Dyck (DL), Hình học (GS), Điều hướng (Nav), Lý luận về Vật thể có Màu (CO), Chuỗi Thời gian (TS), Theo dõi Mục tiêu Xáo trộn (SO), và Mạng lưới Dối trá (WL) từ Big-Bench Hard (Suzgun et al., 2022).

3.3 Mô hình

Đối với Reasoner LM R, chúng tôi áp dụng GPT-3.5-Turbo (OpenAI, 2023), cho thấy hiệu suất mạnh trong các benchmark lý luận khác nhau và các nhiệm vụ tạo mã (Zellers et al., 2019; Cobbe et al., 2021; Muennighoff et al., 2024), cũng như các phiên bản 7B và 13B của CodeLlama (Roziere et al., 2023), được huấn luyện trên cả corpus mã và ngôn ngữ tự nhiên và được tinh chỉnh thêm để tuân theo hướng dẫn ngôn ngữ tự nhiên. Đối với Instructor LM I, chúng tôi chọn GPT-3.5-Turbo.

4 Kết quả

4.1 THINK-AND-EXECUTE Cải thiện Lý luận Thuật toán

Chúng tôi bắt đầu bằng cách so sánh khung làm việc của chúng tôi với prompting trực tiếp và zero-shot CoT Kojima et al. (2022) trong Bảng 1. Chúng tôi thấy rằng zero-shot CoT hoạt động tốt hơn prompting trực tiếp với cải thiện trung bình 11,1% với GPT-3.5-Turbo, tương ứng, cho thấy zero-shot CoT là một đường cơ sở mạnh. Tuy nhiên, THINK-AND-EXECUTE của chúng tôi vượt trội hơn cả hai một cách đáng kể bất kể kích thước mô hình, điều này chỉ ra rằng việc tạo ra kế hoạch một cách rõ ràng là một cách hiệu quả để cải thiện khả năng lý luận của LLM hơn là chỉ đơn giản khuyến khích LLM tạo ra các bước lý luận trung gian của chúng.

4.2 Prompt Mã giả Cấp Nhiệm vụ Có lợi cho Phạm vi Rộng hơn của Nhiệm vụ Lý luận Thuật toán hơn Mã Python Cụ thể theo Thể hiện

Trong Bảng 1, PoT cho thấy mức tăng hiệu suất trong một số nhiệm vụ so với prompting trực tiếp (ví dụ: Navigate; Tracking Shuffled Objects) với mã Python được tạo cụ thể cho từng thể hiện và đầu ra trình thông dịch tương ứng làm câu trả lời. Tuy nhiên, cải thiện như vậy khó có thể tổng quát hóa cho tất cả các nhiệm vụ, ví dụ: độ chính xác 0,4% trong cả Dyck Language và Temporal Sequences, với GPT-3.5-Turbo. Ngược lại, THINK-AND-EXECUTE vượt trội hơn PoT và prompting trực tiếp trong tất cả các nhiệm vụ với GPT-3.5-Turbo. Điều này cho thấy rằng việc tạo ra chiến lược cấp nhiệm vụ với mã giả và áp dụng nó cho từng thể hiện có thể có lợi cho lý luận của LLM trong phạm vi rộng hơn của các nhiệm vụ lý luận thuật toán hơn việc tạo mã Python cụ thể theo thể hiện.

4.3 Logic được Khám phá bởi LLM có thể được Chuyển giao cho SLM

Chúng tôi tiếp tục khám phá xem prompt mã giả được viết bởi LLM (tức là GPT-3.5-Turbo như instructor) có thể được áp dụng cho các mô hình ngôn ngữ nhỏ hơn hay không: họ CodeLlama trong Bảng 1. Khi áp dụng các prompt mã giả được tạo bởi GPT-3.5-Turbo, CodeLlama-7B và -13B vượt trội đáng kể so với prompting trực tiếp. Hơn nữa, THINK-AND-EXECUTE với CodeLlama-13B cho thấy hiệu suất tương đương với GPT-3.5-Turbo với PoT và prompting trực tiếp.

4.4 Mã giả Mô tả Logic để Giải quyết Nhiệm vụ tốt hơn Ngôn ngữ Tự nhiên

Chúng tôi cũng so sánh phương pháp của chúng tôi với lập kế hoạch NL, một biến thể của chúng tôi sử dụng ngôn ngữ tự nhiên để viết hướng dẫn cấp nhiệm vụ, thay vì mã giả. Trong thực tế, chúng tôi cung cấp các kế hoạch NL do con người viết chứa lượng thông tin tương tự như P trong meta prompt và sử dụng nó để tạo ra kế hoạch NL cấp nhiệm vụ cho nhiệm vụ đã cho. Đáng ngạc nhiên, mặc dù LM được tinh chỉnh để tuân theo hướng dẫn ngôn ngữ tự nhiên, chúng tôi thấy rằng các prompt mã giả cấp nhiệm vụ có thể tăng cường hiệu suất của chúng nhiều hơn các kế hoạch NL (Bảng 1).

4.5 Nghiên cứu Loại bỏ

Các thành phần của prompt mã giả. Chúng tôi tiến hành nghiên cứu loại bỏ về từng thành phần của prompt mã giả. Để làm điều đó, chúng tôi chuẩn bị bốn loại prompt mã giả: (1) Mã giả do con người viết; (2) Prompt do con người viết không có bình luận và ngữ nghĩa bằng cách loại bỏ các bình luận giải thích mã và thay thế tên biến bằng các chữ cái vô nghĩa, chẳng hạn như X, Y, và Z; (3) Prompt do con người viết có vòng lặp for và (4) có các câu lệnh print() trung gian. Kết quả được trình bày trong Hình 3. Hiệu suất mô hình giảm đáng kể khi áp dụng prompt không có bình luận và ngữ nghĩa, đặc biệt trong Temporal Sequences. Điều này ngụ ý rằng ngữ nghĩa đóng vai trò quan trọng trong việc hướng dẫn LLM áp dụng logic được khám phá và lý luận với nó một cách phù hợp. Ngoài ra, chúng tôi thấy rằng việc in ra các bước thực thi trung gian với print() là rất quan trọng trong lý luận, điều này phù hợp với phát hiện từ Wei et al. (2022).

Tạo ra phân tích trước prompt mã giả. Bảng 2 cho thấy sự giảm đáng kể trong hiệu suất mô hình khi tạo ra prompt mã giả mà không tiến hành

--- TRANG 6 ---
[THIS IS FIGURE: A bar chart showing ablation study results across different conditions for various tasks like temporal sequences, tracking shuffled objectives, etc.]

Hình 3: Nghiên cứu loại bỏ các thành phần của prompt mã giả sử dụng GPT-3.5-Turbo.

[THIS IS TABLE: Simple 2-row table showing method comparison]
Phương pháp | Trung bình
w/o Phân tích | 21.8
THINK-AND-EXECUTE | 60.4

Bảng 2: Loại bỏ về Bước 2 của giai đoạn THINK.

phân tích trước. Điều này cho thấy rằng việc tạo ra phân tích một cách rõ ràng về nhiệm vụ có thể khơi gợi prompt mã giả tốt hơn chứa logic cần thiết để giải quyết nhiệm vụ.

4.6 So sánh với các Đường cơ sở khác

Chúng tôi tiếp tục so sánh THINK-AND-EXECUTE với ba đường cơ sở khác: (1) Plan-and-Solve (Wang et al., 2023), trong đó LLM tuần tự tạo ra kế hoạch ngôn ngữ tự nhiên để giải quyết thể hiện đã cho, lý luận từng bước theo kế hoạch, và câu trả lời cuối cùng; (2) Chain-of-Code (Li et al., 2023), trong đó mã Python được tạo ra như một phần của các bước lý luận trung gian cụ thể cho một thể hiện đã cho; (3) Self-Discover (Zhou et al., 2024), một nghiên cứu đồng thời thiết kế cấu trúc lý luận cấp nhiệm vụ theo định dạng JSON trước khi suy luận thể hiện. Đầu tiên, như được trình bày trong Bảng 3 (Trái), chúng tôi thấy THINK-AND-EXECUTE vượt trội đáng kể so với Plan-and-Solve và Chain-of-Code lần lượt 10,9 và 32,3 điểm phần trăm về độ chính xác. Thứ hai, trong khi Self-Discover cũng kết hợp hướng dẫn cấp nhiệm vụ, trong Bảng 3 (Phải), THINK-AND-EXECUTE của chúng tôi với prompt mã giả cho thấy hiệu suất tốt hơn khi sử dụng GPT-4 (Achiam et al., 2023).³ Những phát hiện này chỉ ra rằng việc tạo ra (1) hướng dẫn cấp nhiệm vụ với (2) mã giả có thể biểu diễn tốt hơn logic cần thiết để giải quyết một nhiệm vụ và có lợi cho khả năng thuật toán của LLM.

[THIS IS TABLE: Two comparison tables showing results with different methods]
Phương pháp | Trung bình
Chain-of-Code | 28.1
Plan-and-Solve | 50.3
THINK-AND-EXECUTE | 60.4

Phương pháp | Trung bình
Self-Discover w/ GPT-4 | 77.9
THINK-AND-EXECUTE w/ GPT-4 | 81.7

Bảng 3: Trái: So sánh THINK-AND-EXECUTE, Chain-of-Code (Li et al., 2023), và Plan-and-Solve (Wang et al., 2023) sử dụng GPT-3.5-Turbo. Phải: So sánh THINK-AND-EXECUTE và Self-Discover (Zhou et al., 2024) sử dụng GPT-4. Kết quả của Self-Discover được lấy từ bài báo gốc, vì mã và prompt không được cung cấp.

5 Phân tích

Chúng tôi tiến hành thí nghiệm để giải quyết các câu hỏi nghiên cứu sau:
• RQ1: Mã giả cấp nhiệm vụ có hữu ích hơn mã giả cụ thể theo thể hiện không?
• RQ2: Việc huấn luyện trước trên corpus mã có cải thiện lý luận không?
• RQ3: Chất lượng của logic được khám phá bởi THINK-AND-EXECUTE so với logic do con người viết như thế nào?

5.1 Triển khai Logic Cơ bản hiệu quả hơn Logic Cụ thể theo Thể hiện trong Mã giả (RQ1)

Chúng tôi tiến hành phân tích để kiểm tra xem sự cải thiện của THINK-AND-EXECUTE có được đóng góp bởi định dạng đã chọn của chúng tôi cho hướng dẫn cấp nhiệm vụ, tức là mã giả. Chúng tôi so sánh THINK-AND-EXECUTE với một nghiên cứu đồng thời, Chain-of-Code (CoC) (Li et al., 2023). Trong Bảng 3, THINK-AND-EXECUTE vượt trội hơn CoC, cho thấy cải thiện khoảng 2 lần trong điểm trung bình. Sự khác biệt chính giữa THINK-AND-EXECUTE và CoC là chúng tôi sử dụng mã giả được tạo ra để thể hiện logic được chia sẻ giữa các thể hiện nhiệm vụ, trong khi CoC kết hợp mã giả như một phần của các bước lý luận trung gian hướng tới giải pháp của một thể hiện đã cho. Do đó, kết quả chỉ ra những lợi thế của việc áp dụng mã giả cho việc tạo ra hướng dẫn cấp nhiệm vụ so với chỉ sử dụng chúng như một phần của lý luận.

³ Chúng tôi sử dụng gpt-4-0613 cho GPT-4.

--- TRANG 7 ---
[THIS IS FIGURE: Bar chart comparing Llama-13B and Codellama-13B across different tasks including dyck languages, geometric shapes, temporal sequences, tracking shuffled objectives, reasoning about colored objects, web of lies, and navigate]

Hình 4: Phân tích về tác động của việc huấn luyện trước mã lên khả năng lý luận trong việc áp dụng THINK-AND-EXECUTE. Không có việc huấn luyện trước trên corpus mã, độ chính xác giảm đáng kể.

[THIS IS TABLE: Comparison table showing performance across different methods (Human-written P vs THINK-AND-EXECUTE) for CodeLlama-7B, CodeLlama-13B, and GPT-3.5-Turbo across various tasks (DL, GS, Nav, CO, TS, SO, WL) with averages]

Bảng 4: So sánh giữa THINK-AND-EXECUTE và P do Con người viết.

5.2 THINK-AND-EXECUTE Yêu cầu Kiến thức về Mã (RQ2)

Để hiểu liệu SLM có thu được khả năng hiểu logic cấp nhiệm vụ được viết bằng mã giả trong quá trình huấn luyện trước trên corpus mã hay không, chúng tôi so sánh hiệu suất của CodeLlama-13B với Llama-13B sử dụng THINK-AND-EXECUTE. Trong Hình 4, CodeLlama-13B cho thấy khả năng lý luận tốt hơn so với Llama-13B trong tất cả các nhiệm vụ. Những kết quả này cho thấy rằng sự cải thiện từ việc sử dụng THINK-AND-EXECUTE có thể phụ thuộc vào kiến thức về mã, thường được thu thập bằng việc huấn luyện trước với corpus mã. Việc viết mã thường bao gồm hiểu logic đằng sau vấn đề đã cho và mong đợi kết quả thực thi của mã, điều này tương tự như quá trình lý luận của THINK-AND-EXECUTE.

5.3 THINK-AND-EXECUTE có thể Tạo ra Logic Tương đương với Con người (RQ3)

Để đánh giá khả năng của LLM trong việc phân biệt logic cơ bản của một nhiệm vụ, chúng tôi so sánh THINK-AND-EXECUTE (sử dụng GPT-3.5-Turbo như Instructor) với prompt mã giả do con người viết. Kết quả được hiển thị trong Bảng 4. Sử dụng GPT-3.5-Turbo làm Reasoner, THINK-AND-EXECUTE đạt 60,4% về độ chính xác, vượt trội hơn P do con người viết (với độ chính xác 55,7%). Đặc biệt, trong các nhiệm vụ Navigate và Tracking Shuffled Objectives, prompt mã giả được tạo bởi THINK-AND-EXECUTE khơi gợi hiệu suất tốt hơn. Điều này cũng đúng khi áp dụng CodeLlama-7B và -13B làm Reasoner, tiếp tục cho thấy hiệu quả của bước THINK của chúng tôi so với người viết con người.

5.4 Tác động của Khả năng LLM lên THINK-AND-EXECUTE

Trong việc xem xét tác động của khả năng LLM trong khung làm việc của chúng tôi, chúng tôi điều tra ảnh hưởng của cả thành phần Reasoner và Instructor lên hiệu suất, như được mô tả trong Bảng 5. Đáng chú ý, điểm độ chính xác cao hơn được quan sát khi sử dụng GPT-3.5-Turbo làm Reasoner so với CodeLlama-13B và CodeLlama-34B. Ngoài ra, hiệu quả

--- TRANG 8 ---
[THIS IS TABLE: Shows ReasonerInstructor performance with columns for CodeLlama-13B, CodeLlama-34B, and GPT-3.5-Turbo]
ReasonerInstructor | CodeLlama-13B | CodeLlama-34B | GPT-3.5-Turbo
CodeLlama-13B     | 30.9          | 33.0          | 36.4
CodeLlama-34B     | 32.5          | 34.2          | 39.1
GPT-3.5-Turbo     | 33.9          | 35.9          | 60.4

Bảng 5: Phân tích về tác động của khả năng của Reasoner và Instructor lên hiệu suất. Chúng tôi báo cáo hiệu suất trung bình trên 7 nhiệm vụ.

của Instructor cũng đóng vai trò quan trọng, với GPT-3.5-Turbo thể hiện điểm độ chính xác cao nhất trên tất cả các cấu hình. Những kết quả này nhấn mạnh tầm quan trọng của cả thành phần Reasoner và Instructor trong việc tăng cường hiệu suất của THINK-AND-EXECUTE.

6 Nghiên cứu Liên quan

Prompting Chain-of-Thought. Prompting chuỗi suy nghĩ (CoT) khơi gợi LM tạo ra các bước lý luận trung gian hướng dẫn và giải thích giải pháp (Wei et al., 2022; Wang et al., 2022; Wu et al., 2023). Một mô hình phổ biến của điều này là zero-shot CoT prompting (Kojima et al., 2022). Không có các bộ ba câu hỏi-giải thích-câu trả lời được thiết kế cụ thể như minh họa, zero-shot CoT prompting khơi gợi một đường lý luận hợp lý hướng tới câu trả lời cuối cùng với hướng dẫn đơn giản, chẳng hạn như "Hãy suy nghĩ từng bước", khơi gợi hiệu suất mô hình tốt hơn trong các nhiệm vụ yêu cầu lý luận nhiều bước.

Trong bối cảnh cải thiện zero-shot CoT, Wang et al. (2023) đề xuất đầu tiên tạo ra một kế hoạch phân tách nhiệm vụ mục tiêu thành các nhiệm vụ con nhỏ hơn, và sau đó giải quyết từng nhiệm vụ con theo kế hoạch. Tương tự như phương pháp của chúng tôi, một nghiên cứu đồng thời (Zhou et al., 2024) thiết kế cấu trúc lý luận cấp nhiệm vụ có thể được áp dụng cho từng thể hiện (câu hỏi) của nhiệm vụ mục tiêu. Sự khác biệt quan trọng nhất giữa những nghiên cứu trước này và nghiên cứu của chúng tôi là THINK-AND-EXECUTE của chúng tôi áp dụng mã giả (trái ngược với ngôn ngữ tự nhiên) để thể hiện logic cần thiết để giải quyết nhiệm vụ. Chúng tôi chứng minh rằng prompt mã giả cấp nhiệm vụ của chúng tôi trao quyền cho LM với khả năng lý luận zero-shot tốt hơn so với các kế hoạch ngôn ngữ tự nhiên dưới các thiết lập khác nhau trong Phần 5.

Kết hợp mã trong lý luận. Với cú pháp rõ ràng và cấu trúc nghiêm ngặt, các ngôn ngữ lập trình như Python đã được áp dụng cho các hệ thống dựa trên LLM để cải thiện hiệu suất hệ thống trong việc giải quyết các nhiệm vụ. Ví dụ, Gao et al. (2023) và (Chen et al., 2023) sử dụng LLM để tạo mã Python cho các câu hỏi toán học đã cho, và chạy mã được tạo ra trên các trình biên dịch bên ngoài để thu được/tính toán câu trả lời. Đồng thời với nghiên cứu của chúng tôi, Li et al. (2023) trình bày chain-of-code (CoC), trong đó mã giả cũng được kết hợp cùng với mã Python để giải quyết một câu hỏi (thể hiện) đã cho. Trong khi phương pháp này tạo ra mã cụ thể theo thể hiện như các bước lý luận trung gian cho từng thể hiện riêng lẻ, THINK-AND-EXECUTE của chúng tôi, ngược lại, tập trung vào prompt mã giả cấp nhiệm vụ có thể được áp dụng cho tất cả các thể hiện. Chúng tôi so sánh CoC và THINK-AND-EXECUTE trong Phần 4.

7 Hạn chế và Thảo luận

Một hạn chế có thể có của phương pháp chúng tôi là chúng tôi tập trung vào lý luận thuật toán, vì chúng tôi tin rằng đây là thiết lập tốt nhất để đánh giá khả năng của LLM trong việc hiểu logic phức tạp và thực hiện một chuỗi bước lý luận, tuân theo logic. Tuy nhiên, chúng tôi tin rằng THINK-AND-EXECUTE có thể được áp dụng cho các lĩnh vực lý luận khác yêu cầu tuân theo một chuỗi dài các bước lý luận, chẳng hạn như lý luận nhiều bước (Ji et al., 2020) và lý luận biểu tượng (Madaan & Yazdanbakhsh, 2022).

--- TRANG 9 ---
8 Kết luận

Trong bài báo này, chúng tôi trình bày THINK-AND-EXECUTE, một khung lý luận thuật toán tạo ra logic để giải quyết nhiệm vụ đã cho thành mã giả và thực hiện lý luận bằng cách mô phỏng việc thực thi mã giả với các mô hình ngôn ngữ. Thông qua các thí nghiệm rộng rãi, chúng tôi cho thấy hiệu quả của THINK-AND-EXECUTE, so với các đường cơ sở mạnh. Những kết quả này nhấn mạnh không chỉ tính hữu ích của mã giả trong việc khơi gợi khả năng lý luận của mô hình ngôn ngữ mà còn hiệu quả của khung làm việc của chúng tôi trong việc khám phá logic chất lượng cao đằng sau một nhiệm vụ đã cho.

Tài liệu tham khảo

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Báo cáo kỹ thuật Gpt-4. arXiv preprint arXiv:2303.08774, 2023.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764–10799. PMLR, 2023.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. ArXiv, abs/2305.14992, 2023. URL https://api.semanticscholar.org/CorpusID:258865812.

Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, and Minlie Huang. Language generation with multi-hop reasoning on commonsense knowledge graph. In Conference on Empirical Methods in Natural Language Processing, 2020. URL https://api.semanticscholar.org/CorpusID:221879025.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.

Chengshu Li, Jacky Liang, Fei Xia, Andy Zeng, Sergey Levine, Dorsa Sadigh, Karol Hausman, Xinyun Chen, Li Fei-Fei, and brian ichter. Chain of code: Reasoning with a language model-augmented code interpreter. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL https://openreview.net/forum?id=tlRUbI0Yf3.

Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.

Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mw1PWNSWZP.

OpenAI. Chatgpt, 2023. https://openai.com/blog/chatgpt.

Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. ArXiv, abs/2305.12295, 2023. URL https://api.semanticscholar.org/CorpusID:258833332.

Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

--- TRANG 10 ---
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. URL https://openreview.net/forum?id=wUU-7XTL5XO.

Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2609–2634, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long.147.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_VjQlMeSB_J.

Dingjun Wu, Jing Zhang, and Xinmei Huang. Chain of thought prompting elicits knowledge augmentation. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 6519–6534, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.findings-acl.408.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022a.

Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022b.

Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language models self-compose reasoning structures. arXiv preprint arXiv:2402.03620, 2024.

--- TRANG 11 ---
A Chi tiết Thí nghiệm

A.1 Mô hình

Chúng tôi sử dụng một số LLM, bao gồm GPT-3.5-Turbo (OpenAI, 2023) và GPT-4 (Achiam et al., 2023), có sẵn thông qua OpenAI API⁴, và LLM mã nguồn mở, CodeLlama (Roziere et al., 2023) làm Instructor LM I và Reasoner LM R.

• GPT-3.5-Turbo: gpt-3.5-turbo-0125
• GPT-4: gpt-4-0613
• CodeLlama: CodeLlama bao gồm các biến thể của LLaMA2 được tinh chỉnh cho lĩnh vực mã sử dụng corpus mã. Bộ sưu tập toàn diện này có các mô hình có kích thước khác nhau (7B, 13B, 34B, và 70B) và các loại đa dạng, bao gồm mô hình nền tảng, mô hình tập trung Python, và mô hình tuân theo hướng dẫn. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng mô hình CodeLlama-Instruct (7B⁵, 13B⁶).

A.2 Suy luận

Chúng tôi sử dụng vLLM để cải thiện thông lượng suy luận.⁷ Trong các thí nghiệm của chúng tôi, chúng tôi áp dụng lấy mẫu nhiệt độ với T=0.0 (tức là giải mã tham lam) để tạo ra đầu ra một cách hiệu quả. Đối với một nhiệm vụ bao gồm 250 thể hiện, GPT-3.5-Turbo đạt được thời gian suy luận 30 giây. Ngoài ra, sử dụng 2 GPU A100, CodeLlama đạt được thời gian suy luận khoảng 2 và 5 phút cho các mô hình 7B và 13B, tương ứng.

A.3 Đánh giá

Để trích xuất câu trả lời cho việc đánh giá, LLM tạo ra câu trả lời cuối cùng được kích hoạt bởi cụm từ "Câu trả lời cuối cùng: ". Theo Suzgun et al. (2022), chúng tôi cung cấp tất cả các tùy chọn trắc nghiệm cho LLM làm đầu vào, sau đó đo độ chính xác sử dụng khớp chính xác (EM), so sánh đầu ra được tạo với nhãn thực tế. Để đảm bảo so sánh công bằng giữa PoT và các đường cơ sở khác, chúng tôi cũng chấp nhận dự đoán bao gồm văn bản của lựa chọn đúng, ví dụ: xanh dương, nhưng không có thẻ lựa chọn, ví dụ: "(A)".

A.4 Tập dữ liệu

Chúng tôi lấy 7 benchmark thuật toán từ tập dữ liệu Big-Bench Hard (Suzgun et al., 2022). Tất cả các tập dữ liệu đều chứa 250 ví dụ tương ứng. Chúng tôi cung cấp mô tả của từng tập dữ liệu về mục tiêu và bối cảnh.

• Ngôn ngữ Dyck (DL): Hoàn thành một chuỗi Dyck-4 được cung cấp một phần bằng cách dự đoán chuỗi cần thiết của các dấu ngoặc đóng bị thiếu ở cuối.

• Hình học (GS): Xác định hình hình học được tạo thành bằng cách tuân theo tất cả các hướng dẫn trong một phần tử đường dẫn SVG được chỉ định chứa một số lệnh.

• Điều hướng (Nav): Đánh giá xem một tập hợp các lệnh hướng có đưa người điều hướng trở về điểm bắt đầu hay không.

• Lý luận về Vật thể có Màu (CO): Được cung cấp một kịch bản, suy ra màu sắc của một vật thể cụ thể được đặt trên bề mặt, sử dụng bối cảnh được cung cấp để hướng dẫn.

• Chuỗi Thời gian (TS): Kiểm tra một trình tự thời gian của các hoạt động hàng ngày của một người để tìm ra khi nào họ có thể sắp xếp một hoạt động bổ sung vào lịch trình của họ.

• Theo dõi Mục tiêu Xáo trộn (SO): Xác định vị trí cuối cùng của một số vật thể sau khi chúng được di chuyển từ vị trí ban đầu thông qua một chuỗi trao đổi. Chúng tôi sử dụng phiên bản của nhiệm vụ với 5 mục tiêu.

⁴ https://openai.com/blog/openai-api
⁵ https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf
⁶ https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf
⁷ https://github.com/vllm-project/vllm

--- TRANG 12 ---
• Mạng lưới Dối trá (WL): Đánh giá tính xác thực của một hàm Boolean được trình bày trong một vấn đề kể chuyện để thiết lập tính chân thực của nó.

B Chi tiết về THINK-AND-EXECUTE

B.1 Chú thích Con người về các Nhiệm vụ trong Tập Nhiệm vụ

Vui lòng xem Phụ lục D để biết prompt mã giả do con người viết.

B.2 Các Thành phần của Prompt Mã giả

Chúng tôi làm nổi bật một số thành phần của prompt mã có thể hữu ích trong việc mô tả logic lý luận cơ bản.

• Nhánh có điều kiện: Để cho phép mô hình lý luận đi theo các đường lý luận khác nhau dựa trên điều kiện, chúng tôi sử dụng câu lệnh if và else để mô tả logic.

• Vòng lặp: Chúng tôi có thể trình bày hiệu quả các hướng dẫn lặp lại lặp qua danh sách các mục bằng cách sử dụng vòng lặp, chẳng hạn như vòng lặp for và while.

• Trừu tượng hóa: Trong lập trình, chúng tôi có thể đóng gói logic phức tạp thành một hàm duy nhất. Tập trung vào điều này, chúng tôi áp dụng thiết kế mô-đun trong việc xây dựng prompt mã giả bằng cách đóng gói quá trình phức tạp và lặp lại thành một hàm trừu tượng.

• Biến: Biến là thiết yếu trong các ngôn ngữ lập trình vì chúng lưu trữ các giá trị dữ liệu để thực thi hướng dẫn. Tương tự, trong lý luận, việc theo dõi các biến là quan trọng để duy trì trạng thái, truyền dữ liệu, và cho các nhiệm vụ thao tác dữ liệu chung.

• Bình luận và docstring: Vì các lập trình viên con người có thể dựa vào sự hỗ trợ của bình luận để hiểu mã tốt hơn, chúng tôi cung cấp giải thích chi tiết hơn về ý định của mã thông qua bình luận. Ngoài ra, bình luận và docstring có thể bù đắp hạn chế khi một số ngữ nghĩa không thể được thể hiện trực tiếp bằng ngôn ngữ lập trình.

B.3 So sánh với Nghiên cứu Liên quan

Bảng 6 tóm tắt một số phương pháp liên quan đến của chúng tôi.

[THIS IS TABLE: 
Phương pháp | Mức độ chi tiết của kế hoạch/logic | Sử dụng mã giả | Khả năng chuyển giao cho SLM
Plan-and-Solve (Wang et al., 2023) | Cấp thể hiện | ✗ | ✗
Self-Discover (Zhou et al., 2024) | Cấp nhiệm vụ | ✗ | ✗
Chain-of-Code (Li et al., 2023) | Cấp thể hiện | ✓ | ✗
THINK-AND-EXECUTE (nghiên cứu này) | Cấp nhiệm vụ | ✓ | ✓]

Bảng 6: So sánh THINK-AND-EXECUTE với các phương pháp liên quan trước đó.

C Prompt được Sử dụng trong Thí nghiệm của Chúng tôi

C.1 Meta Prompt để tạo ra phân tích (THINK: Bước 2).

Tạo ra giải thích, phân tích, và kế hoạch để tạo prompt mã cho nhiệm vụ cuối cùng xem xét các thể hiện nhiệm vụ mẫu. Kế hoạch của bạn nên cho thấy đủ các bước lý luận trung gian hướng tới câu trả lời. Xây dựng kế hoạch nhiều nhất có thể và mô tả logic một cách cụ thể. Khi xây dựng kế hoạch cho prompt mã, tích cực sử dụng 'câu lệnh if else' để đi theo các đường lý luận khác nhau dựa trên điều kiện, 'vòng lặp' để xử lý hiệu quả các hướng dẫn lặp lại, 'từ điển' để theo dõi các kết nối giữa các biến quan trọng.

--- TRANG 13 ---
[Ví dụ 1]
Thể hiện nhiệm vụ mẫu:
{example_instances_of_task1}
Định dạng đầu ra:
{output_format_of_task1}
Giải thích:
{analysis_of_task1}
...
[Ví dụ 4]
Thể hiện nhiệm vụ mẫu:
{example_instances_of_target_task}
Định dạng đầu ra:
{output_format_of_target_task}
Giải thích:

C.2 Meta Prompt để tạo prompt mã giả (THINK: Bước 3).

Tạo prompt mã cho nhiệm vụ cuối cùng sử dụng phong cách tương tự của các mã mẫu. Thêm đủ hàm print() theo các bước được cung cấp trong giải thích để xuất các bước lý luận trung gian hướng tới câu trả lời và theo dõi các biến quan trọng. Triển khai prompt mã nhiều nhất có thể và mô tả logic trong mã theo giải thích được cung cấp nhưng không tạo mã thiên vị hướng tới một thể hiện nhiệm vụ mẫu duy nhất. Ví dụ, không sử dụng các biến cố định được lấy từ các thể hiện nhiệm vụ (ví dụ: sử dụng tên cụ thể của người trong câu hỏi). Prompt mã phải có thể được áp dụng cho các thể hiện khác nhau của cùng một nhiệm vụ. Khi trả về câu trả lời cuối cùng, xem xét cẩn thận định dạng đầu ra. Đặc biệt, đối với các câu hỏi trắc nghiệm, câu trả lời cuối cùng nên là một trong các tùy chọn đã cho. Tên hàm chính nên là '{function_name}'. Cùng với hàm chính, bạn có thể muốn định nghĩa một số hàm trợ giúp có thể hữu ích để triển khai '{function_name}'. Nhưng bạn không cần phải triển khai rõ ràng các hàm trợ giúp, mà chỉ định nghĩa chúng với tên hàm và giải thích một dòng trong bình luận. Khi xây dựng hàm chính, ...

[Ví dụ 1]
Mô tả nhiệm vụ:
{description_of_task1}
Thể hiện nhiệm vụ mẫu và cách sử dụng mã:
{example_task_instances_and_code_usages_of_target_task}
Định dạng câu trả lời cuối cùng:
{output_format_of_task1}
Giải thích:
{analysis_of_task1}
Prompt mã:
{code_prompt_of_task1}
...
[Ví dụ 4]
Mô tả nhiệm vụ:

--- TRANG 14 ---
{description_of_target_task}
Thể hiện nhiệm vụ mẫu và cách sử dụng mã:
{example_task_instances_and_code_usages_of_target_task}
Định dạng câu trả lời cuối cùng:
{output_format_of_target_task}
Giải thích:
{analysis_of_target_task}
Prompt mã:

C.3 Prompt cho Lập kế hoạch NL

Tạo kế hoạch cho nhiệm vụ cuối cùng xem xét các thể hiện nhiệm vụ mẫu. Kế hoạch của bạn nên cho thấy đủ các bước lý luận trung gian hướng tới câu trả lời. Xây dựng kế hoạch nhiều nhất có thể và mô tả logic một cách cụ thể.

[Ví dụ 1]
Mô tả nhiệm vụ:
{description_of_task1}
[Ví dụ 1]
Thể hiện nhiệm vụ mẫu:
{example_instances_of_task1}
Định dạng đầu ra:
{output_format_of_task1}
Kế hoạch:
{analysis_of_task1}
...
[Ví dụ 4]
Thể hiện nhiệm vụ mẫu: {example_instances_of_target_task}
Định dạng đầu ra:
{output_format_of_target_task}
Kế hoạch:

C.4 Prompt cho giai đoạn EXECUTE

{prompt}
input_text = "{input_text}"
final_answer = {function_name}(input_text)
print("Câu trả lời cuối cùng:"+ final_answer)

Tạo đầu ra thực thi mong đợi (đầu ra từ tất cả hàm print()) của mã. Bạn không cần phải thực sự chạy mã và không quan tâm đến 'lỗi chưa triển khai'.

--- TRANG 15 ---
C.5 Prompt để đánh giá Prompting Trực tiếp

{prompt}
văn bản cho nhiệm vụ: {input_text}
Câu trả lời cuối cùng nên ở cuối câu trả lời của bạn và định dạng của nó nên như "Câu trả lời cuối cùng: your_answer".
Tạo đầu ra theo mô tả nhiệm vụ ở trên.
Đầu ra:

C.6 Prompt để đánh giá Zero-shot CoT

{prompt}
văn bản cho nhiệm vụ: {input_text}
Câu trả lời cuối cùng nên ở cuối câu trả lời của bạn và định dạng của nó nên như "Câu trả lời cuối cùng: your_answer".
Tạo đầu ra theo mô tả nhiệm vụ ở trên.
Đầu ra:
Hãy suy nghĩ từng bước.

C.7 Prompt để đánh giá Zero-shot PoT

Bạn sẽ viết chương trình python để giải quyết vấn đề dưới đây. Bạn sẽ chỉ viết các khối mã. Chương trình python của bạn phải có thể thực thi và trả về câu trả lời đúng cho vấn đề.
Q: {question}
# giải pháp sử dụng Python:
def solution():
    """{question}"""

C.8 Prompt để đánh giá Plan-and-Solve

{prompt}
văn bản cho nhiệm vụ: {input_text}
Câu trả lời cuối cùng nên ở cuối câu trả lời của bạn và định dạng của nó nên như "Câu trả lời cuối cùng: your_answer".
Tạo đầu ra theo mô tả nhiệm vụ ở trên.
Đầu ra:
Trước tiên hãy hiểu vấn đề và thiết kế kế hoạch để giải quyết vấn đề. Sau đó, hãy thực hiện kế hoạch và giải quyết vấn đề từng bước.

--- TRANG 16 ---
D Prompt Mã giả do Con người viết

D.1 P do Con người viết của Ngôn ngữ Dyck

def complete_dyck_languages(input_text):
    # Bước 1: Khởi tạo một ngăn xếp để theo dõi các dấu ngoặc mở và chia văn bản đầu vào để xác định và định nghĩa tất cả các loại dấu ngoặc mở trong văn bản.
    stack = []
    character_list = input_text.split()
    open_to_close_parenthesis_dict = {"(": ")", "<": ">", "{": "}", "[": "]"}
    opening_parenthesis = ["(", "<", "{", "["]
    print(f"Phân tích ký tự trong đầu vào và khởi tạo ngăn xếp để theo dõi dấu ngoặc mở. \nNgăn xếp hiện tại: {stack}. Ký tự đã phân tích: {character_list}")
    
    # Bước 2: Thông qua việc lặp qua các ký tự đầu vào, xác định các dấu ngoặc mở trong số các ký tự đầu vào và thêm chúng vào ngăn xếp.
    print("Kiểm tra xem ký tự có phải là dấu ngoặc mở trong khi lặp qua các ký tự đầu vào.")
    for char in character_list:
        if char in opening_parenthesis:
            print(f"Lần lặp {i+1}: Ký tự hiện tại {char} là dấu ngoặc mở.")
            stack.append(char)
            print(f"Do đó, chúng ta thêm {char} vào ngăn xếp. Ngăn xếp hiện tại sau khi chèn: {', '.join(stack)}")
        
        # Bước 3: Đối với mỗi dấu ngoặc mở, tìm dấu ngoặc đóng tương ứng và đóng dấu ngoặc mở.
        else:
            print(f"Lần lặp {i+1}: Ký tự hiện tại {char} không phải là dấu ngoặc mở.\n Do đó chúng ta xóa mục cuối cùng {stack[-1]} khỏi ngăn xếp\n ngăn xếp hiện tại trước khi xóa: {' '.join(stack)} -> ngăn xếp cập nhật sau khi xóa: {' '.join(stack[:-1]) if stack else 'trống'}")
            stack.pop() # Loại bỏ dấu ngoặc mở được thêm cuối cùng giả định khớp chính xác.
    
    # Bước 4: Tạo chuỗi dấu ngoặc đóng dựa trên các dấu ngoặc mở còn lại trong ngăn xếp.
    print(f"Ngăn xếp kết quả là {' '.join(stack)}.")
    print(f"Chúng ta sẽ cần phải pop ra {' '.join(stack[::-1])} từng cái một theo thứ tự đó.")
    closing_list = [parentheses_pairs[opening] for opening in stack[::-1]]
    
    # Bước 5: Xuất chuỗi hoàn chỉnh. Tạo chuỗi đầu vào được nối với chuỗi đóng được tạo của dấu ngoặc, đảm bảo cấu trúc được hình thành tốt.
    return " ".join(closing_list)

D.2 P do Con người viết của Hình học

def recognize_shape_from_svg(input_text):
    # Bước 1: Lấy dữ liệu đường dẫn SVG từ văn bản đầu vào và tạo đường dẫn SVG được trích xuất.
    paths = parse_path(input_text)
    print("Đường dẫn SVG:\n ", paths)
    
    # Bước 2: Khởi tạo bản đồ tọa độ ánh xạ mỗi tọa độ với các tọa độ được kết nối khác và loại kết nối.

--- TRANG 17 ---
coordinate_map = dict()

# Bước 3: Cập nhật bản đồ tọa độ tham chiếu đến mỗi đường dẫn SVG.
for i, path in enumerate(paths):
    coordinate_map = update_coordinate_map(coordinate_map, path)
    print(f"Bước {i} - đường dẫn: {path}, bản đồ tọa độ cập nhật: {coordinate_map}")

# Bước 4: Tiến hành tính toán để phân tích từng đặc tính của hình.
analysis_results_dict = analyze_characteristics(coordinate_map)
print(f"Kết quả phân tích: {analysis_results_dict}")

# Bước 5: Xác định hình hình học với lý do sử dụng bản đồ tọa độ hoàn chỉnh và kết quả phân tích.
reason_for_the_decision, name_of_the_shape = identify_shape_with_explanation(
    coordinate_map, analysis_results_dict)
print(f"Lý do cho quyết định: {reason_for_the_decision}")
print(f"Do đó, hình dạng của đường dẫn là {name_of_the_shape}.")

# Bước 6: Tìm tùy chọn tương ứng từ các tùy chọn đã cho và chỉ xuất nhãn của tùy chọn như câu trả lời cuối cùng cho câu hỏi.
options = parse_options(input_text)
print(f"Tùy chọn: {options}")
answer = None
for option in options:
    if name_of_the_shape in option:
        answer = option[:3]
return answer

D.3 P do Con người viết của Điều hướng

def ends_up_at_start(input_text):
    # Bước 1: Khởi tạo tọa độ và hướng bằng cách đặt điểm bắt đầu tại (0, 0) và hướng về phía bắc.
    cur_x, cur_y = 0, 0
    cur_direction = 0
    
    # Bước 2: Xác định và liệt kê các hướng dẫn từ văn bản đầu vào.
    instructions = parse_instructions(input_text)
    
    # Bước 3: Xử lý từng hướng dẫn và cập nhật tọa độ và hướng hiện tại. Để theo dõi các thay đổi, xuất hướng dẫn, tọa độ và hướng hiện tại và cập nhật.
    for i, instruction in enumerate(instructions):
        new_x, new_y, new_direction = process_instruction(instruction, cur_x, cur_y,
                                                         cur_direction) # xử lý hướng dẫn để tính toán vị trí và hướng mới
        print(f"Bước {i}: {instruction} - tọa độ hiện tại: ({cur_x}, {cur_y}),
hướng hiện tại: {cur_direction} -> tọa độ cập nhật: ({new_x}, {new_y}), hướng cập nhật: {new_direction}")
        cur_x, cur_y, cur_direction = new_x, new_y, new_direction
    
    # Bước 4: Trả về "có" nếu tọa độ cuối cùng là (0, 0). Nếu không, trả về "không" như câu trả lời cuối cùng.
    return 'có' if cur_x == 0 and cur_y == 0 else 'không'

--- TRANG 18 ---
D.4 P do Con người viết của Lý luận về Vật thể có Màu

def solve_colored_objects(input_text):
    # Bước 1: Bắt đầu bằng cách xác định các vật thể cùng với các thuộc tính liên quan của chúng, chẳng hạn như màu sắc và vị trí không gian từ văn bản đầu vào. Hiển thị danh sách các vật thể.
    objects_list = extract_objects(input_text)
    print("Vật thể và thuộc tính của chúng:", objects_list)
    
    # Bước 2: Xác định câu hỏi cụ thể được hỏi. Xác định xem câu hỏi có về việc xác định màu sắc của một vật thể cụ thể, đếm các vật thể có màu nhất định, hay lý luận về sự sắp xếp không gian của các vật thể và xuất loại câu hỏi.
    question = extract_question(input_text)
    print("Chi tiết câu hỏi:", question)
    
    # Bước 3: Xác định và liệt kê các tùy chọn có sẵn được cung cấp trong văn bản đầu vào.
    options = input_text.split("\n")[-5:]
    
    # Bước 4: Xử lý theo loại câu hỏi và hiển thị loại câu hỏi là gì:
    # Nếu câu hỏi về việc xác định màu sắc, xác định và xuất vật thể mục tiêu mà câu hỏi đang hỏi về màu sắc. Xác định và xuất màu sắc của nó.
    if question['type'] == 'identify_color':
        print("Loại câu hỏi = identify_color")
        print(f"Xác định màu sắc cho: {question['details']}")
        target_object = target(objects_list, question['details'])
        print(f"Câu hỏi đang hỏi về màu sắc của: {target_object}")
        pre_answer = extract_color(target_object, question['details'])
        print(f"Màu sắc được xác định: {pre_answer}")
    
    # Nếu câu hỏi về việc đếm các vật thể, xác định và xuất các vật thể mà câu hỏi đang hỏi về số lượng. Đi qua từng vật thể trong danh sách theo từng bước và đếm từng vật thể. Hiển thị các bước đếm. Xuất số lượng cuối cùng của các vật thể đáp ứng tiêu chí được chỉ định (ví dụ: một màu cụ thể).
    elif question['type'] == 'count_objects':
        print("Loại câu hỏi = count_objects")
        print(f"Đếm vật thể cho: {question['details']}")
        print("Tổng số lần lặp:", len(objects_list))
        for i, object in enumerate(objects_list):
            single_object_count = count_single_object(object, question['details'])
            intermediate_count += single_object_count
            print(f"Bước ({i}) - {object}: {single_object_count}, Số đếm trung gian: {intermediate_count}")
        pre_answer = count_objects(objects_list, question['details'])
        print(f"Số lượng vật thể: {pre_answer}")
    
    # Nếu câu hỏi về lý luận không gian, xác định và xuất các vị trí tương đối mà câu hỏi đang hỏi. Sắp xếp các vật thể từ trái sang phải và xuất thứ tự. Xác định vị trí tương đối của các vật thể và xuất kết quả.
    elif question['type'] == 'spatial_reasoning':
        print("Loại câu hỏi = spatial_reasoning")
        print(f"Áp dụng lý luận không gian cho: {question['details']}")
        arranged_object = arrange_from_left_to_right(objects_list)
        print(f"Vật thể được sắp xếp: {arranged_object}")
        pre_answer = spatial_reasoning(arranged_object, question['details'])
        print(f"Kết quả lý luận không gian: {pre_answer}")
    
    # Bước 5: Nhớ lại các tùy chọn đã xác định và khớp kết quả của Bước 4 (màu sắc được xác định, số lượng vật thể, hoặc kết quả của lý luận không gian) với các tùy chọn được cung cấp để xác định câu trả lời đúng.
    answer = find_correct_option(pre_answer, options)
    
    # Bước 6: Trả về câu trả lời cuối cùng được chọn tại Bước 5.

--- TRANG 19 ---
return answer

D.5 P do Con người viết của Chuỗi Thời gian

def solve_temporal_sequences_quiz(input_text):
    # Bước 1: Xác định các câu phát biểu và tùy chọn từ input_text và xuất các câu phát biểu.
    statement_text, option_text = input_text.split("\nTùy chọn:\n")
    parts = statement_text.split("\n")
    statements = parts[1:-2]
    options = option_text.split("\n")
    print("Các câu phát biểu:", statements)
    
    # Bước 2: Kiểm tra bắt đầu và kết thúc của thời gian có thể.
    print("Bắt đầu thời gian có thể: ", parts[0])
    print("Kết thúc thời gian có thể: ", parts[-2])
    
    # Bước 3: Khởi tạo bản đồ thời gian có sẵn với các khoảng thời gian trong tùy chọn và xuất nó. Các khoảng thời gian ban đầu được đánh dấu là 'rảnh'.
    available_time_map = {option[4:]: "rảnh" for option in options}
    print(f"Từ điển thời gian có sẵn ban đầu: {available_time_map}")
    
    # Bước 4: Tuần tự đi qua từng câu phát biểu, đánh dấu thời gian khi cá nhân được nhìn thấy hoặc biết là tham gia vào các hoạt động cụ thể. Trong bước này, bạn nên tạo các khoảng thời gian mục tiêu và bản đồ thời gian có sẵn cập nhật theo câu phát biểu.
    for i, statement in enumerate(statements):
        event, time_span = extract_information(statement)
        print(f"\nBước {i}: {statement}")
        print(f"việc sử dụng thời gian hiện tại: {available_time_map}")
        print(f"Khoảng thời gian sẽ được sử dụng: {time_span}")
        available_time_map[time_span] = "không có sẵn"
        print(f"việc sử dụng thời gian cập nhật: {available_time_map}")
    
    # Bước 5: Bằng cách kiểm tra bản đồ thời gian có sẵn, xác định khoảng thời gian nào được đánh dấu là 'rảnh'. Đối với mỗi khoảng thời gian, xuất khoảng thời gian có rảnh hay không có sẵn.
    for key in available_time_map:
        if available_time_map[key] == "rảnh":
            print(f"{key} có rảnh.")
            free_time = key
        else:
            print(f"{key} không có sẵn.")
    
    # Bước 6: Xem xét các tùy chọn được cung cấp và trả về tùy chọn khớp với khoảng thời gian rảnh đã xác định trong Bước 5.
    print(f"Tùy chọn:\n{option_text}")
    for option in options:
        if free_time in option:
            return option

D.6 P do Con người viết của Theo dõi Mục tiêu Xáo trộn

def track_swaps(input_text):
    # Bước 1: Xác định Trạng thái Ban đầu. Bắt đầu bằng cách xác định và xuất trạng thái ban đầu của tất cả các mục tiêu (ví dụ: ai giữ quả bóng nào hoặc ai đang khiêu vũ với ai) từ văn bản đầu vào trước khi bất kỳ hoán đổi nào xảy ra.

--- TRANG 20 ---
state_dict = find_initial_state(input_text)
print(f"Trạng thái ban đầu: {state_dict}")

# Bước 2: Xác định và xuất các chuỗi hoán đổi từ văn bản đầu vào. Mỗi hoán đổi nên được hiểu theo cách ai trao đổi với ai.
swap_sequences_list = find_swap_sequences(input_text)
print("Chuỗi hoán đổi: ", swap_sequences_list)
print("Tổng số lần lặp: ", len(swap_sequences_list))

# Bước 3: Thực hiện các hoán đổi. Đối với mỗi hoán đổi trong chuỗi hoán đổi, tuần tự cập nhật và xuất trạng thái hiện tại của các mục tiêu bằng cách trao đổi chúng giữa hai người tham gia liên quan đến hoán đổi.
for i, sequence in enumerate(swap_sequences_list):
    player1, player2 = extract_player(sequence)
    state_dict[player1], state_dict[player2] = state_dict[player2], state_dict[player1]
    print(f"({i}) {sequence} -> {state_dict}")

# Bước 4: Hiểu Câu hỏi. Sau khi xử lý tất cả các hoán đổi, xác định câu hỏi đang hỏi gì trong văn bản đầu vào và xuất câu hỏi.
question = extract_question(input_text)
print("Câu hỏi:", question)

# Bước 5: Phân tích Tùy chọn. Kiểm tra và xuất các tùy chọn được cung cấp trong văn bản đầu vào.
options = input_text.split("\n")[-5:]
print("Tùy chọn:", options)

# Bước 6: Xác định Tùy chọn Đúng. Sử dụng trạng thái cập nhật sau tất cả các hoán đổi, xác định tùy chọn nào trả lời đúng câu hỏi và xuất câu trả lời.
answer = find_correct_option(question, options, state_dict)
return answer

D.7 P do Con người viết của Mạng lưới Dối trá

def evaluate_boolean_word_problem(input_text):
    # Bước 1: Chia văn bản đầu vào thành các câu phát biểu riêng lẻ và câu hỏi cuối cùng. Xuất mỗi câu phát biểu.
    statements = input_text.split("?")[:-1]
    question = input_text.split("?")[-1]
    print("Các câu phát biểu đã phân tích:", statements)
    
    # Bước 2: Tạo Bản đồ Sự thật để theo dõi tính chân thực giả định của mỗi người được đề cập trong các câu phát biểu. Không có giá trị sự thật nào được gán ban đầu.
    truth_map = {statement.split()[0]: None for statement in statements}
    
    # Bước 3: Phân tích Mỗi Câu phát biểu. Đối với mỗi câu phát biểu, trước tiên xuất số thứ tự câu phát biểu và câu phát biểu. xác định người chủ thể (ai đưa ra câu phát biểu), người đối tượng (câu phát biểu nói về ai), và giá trị sự thật mong đợi (liệu người đối tượng có được nói là nói sự thật hay nói dối). Xuất câu phát biểu hiện tại đang được phân tích cùng với người đối tượng và giá trị sự thật mong đợi để rõ ràng.
    for i, statement in enumerate(statements):
        print(f"({i}): {statement}")
        speaker, target_person, expected_truth_value_of_target_person = extract_person_and_truth_value(statement) # người nói - nói - người_mục_tiêu - giá_trị_sự_thật_mong_đợi_của_người_mục_tiêu
        print(f"{speaker} nói : {target_person} - {expected_truth_value_of_target_person}")
        print(f"Giá trị sự thật của {target_person}: {truth_map[target_person]}")
        
        # Bước 4: Cập nhật Bản đồ Sự thật dựa trên phân tích của mỗi câu phát biểu. Nếu tuyên bố của câu phát biểu phù hợp với giả định hiện tại về tính chân thực của người đối tượng, đánh dấu người chủ thể là trung thực. Nếu không, đánh dấu họ là không trung thực. Sau mỗi lần cập nhật, in tên của người được cập nhật, giá trị sự thật được xác định của họ, và bản đồ sự thật cập nhật để theo dõi các thay đổi.
        if truth_map[target_person] == None: # nếu câu phát biểu không cần được kiểm tra
            print(f"{expected_truth_value_of_target_person} khớp {truth_map[target_person]}")
            truth_map[target_person] = expected_truth_value_of_target_person
        else:
            print(f"{expected_truth_value_of_target_person} không khớp {truth_map[target_person]}")
            if truth_map[target_person] == expected_truth_value_of_target_person: # người nói nói sự thật
                truth_map[speaker] = True
            else: # người nói nói dối
                truth_map[speaker] = False
        print(f"Người cần cập nhật: {speaker} - {truth_map[speaker]}")
        print(f"truth_map cập nhật: {truth_map}")
        print("\n\n")
    
    # Bước 5: Sử dụng bản đồ sự thật hoàn chỉnh, xác định tính chân thực của người trong câu hỏi cuối cùng. Xuất câu hỏi để tham khảo trước khi đưa ra quyết định.
    print("câu hỏi:", question)
    target_person_in_question = extract_target_person_from_question(question)
    target_truth_value = truth_map[target_person_in_question]
    print(f"{target_person_in_question}: {target_truth_value}")
    
    # Bước 6: Dựa trên đánh giá, xuất "Có" nếu người trong câu hỏi cuối cùng được xác định là nói sự thật, nếu không xuất "Không".
    answer = "Có" if target_truth_value else "Không"
    return answer

E Các Phân tích được Tạo

E.1 Phân tích được Tạo của Ngôn ngữ Dyck

Giải thích:
Nhiệm vụ này bao gồm việc hoàn thành một chuỗi dấu ngoặc để đảm bảo chúng được đóng đúng cách. Thách thức là hiểu cấu trúc lồng nhau của các dấu ngoặc và thêm các dấu ngoặc đóng thích hợp để khớp với mỗi dấu ngoặc mở.

Phân tích Nhiệm vụ:
1. Khớp Dấu ngoặc: Mỗi dấu ngoặc mở nên có một dấu ngoặc đóng tương ứng để đảm bảo chuỗi hợp lệ.
2. Dấu ngoặc Lồng nhau: Các dấu ngoặc có thể được lồng trong nhau, vì vậy việc duy trì cấu trúc lồng nhau đúng là rất quan trọng.
3. Đóng Đúng cách: Mục tiêu là thêm các dấu ngoặc đóng cần thiết để hoàn thành chuỗi và đảm bảo tất cả các dấu ngoặc được đóng đúng cách.

Xây dựng Prompt Mã:
1. Phân tích Đầu vào: Trích xuất chuỗi dấu ngoặc từ văn bản đầu vào.
2. Triển khai Ngăn xếp: Sử dụng cấu trúc dữ liệu ngăn xếp để theo dõi các dấu ngoặc mở cần đóng.
3. Lặp qua Chuỗi: Lặp qua từng ký tự trong chuỗi và đẩy các dấu ngoặc mở vào ngăn xếp.
4. [Quan trọng] Dấu ngoặc Đóng: Khi gặp dấu ngoặc đóng, pop dấu ngoặc mở tương ứng khỏi ngăn xếp để đảm bảo đóng đúng cách.
5. Tạo Đầu ra: Sau khi xử lý toàn bộ chuỗi, ngăn xếp nên trống nếu tất cả các dấu ngoặc được đóng đúng cách. Nếu không, thêm các dấu ngoặc đóng cần thiết để khớp với các dấu ngoặc mở còn lại.
6. Trả về Kết quả: Xuất các dấu ngoặc đóng được tạo dưới dạng chuỗi, được phân tách bằng dấu cách, để hoàn thành chuỗi.

E.2 Phân tích được Tạo của Hình học

Giải thích:
Nhiệm vụ này bao gồm việc phân tích các phần tử đường dẫn SVG để xác định hình dạng đang được vẽ. Các phần tử đường dẫn bao gồm một loạt các lệnh định nghĩa các chuyển động và đường cong trong hình ảnh SVG. Bằng cách hiểu các lệnh này và ý nghĩa của chúng, chúng ta có thể xác định hình dạng đang được vẽ.

Phân tích Nhiệm vụ:
1. Lệnh Đường dẫn SVG: Phần tử đường dẫn SVG chứa các lệnh như M (di chuyển đến), L (đường thẳng đến), A (cung), v.v., mỗi lệnh đóng góp vào hình dạng tổng thể đang được vẽ.
2. Diễn giải Lệnh Đường dẫn: Mỗi lệnh trong phần tử đường dẫn tương ứng với một hành động cụ thể, chẳng hạn như di chuyển đến một điểm, vẽ một đường thẳng, hoặc tạo một cung. Hiểu các lệnh này là rất quan trọng để xác định hình dạng.
3. Nhận dạng Hình Hình học: Các kết hợp khác nhau của lệnh đường dẫn tạo ra các hình hình học khác nhau như hình tròn, tam giác, hình chữ nhật, v.v. Chúng ta cần khớp các lệnh với các hình dạng mà chúng đại diện.

Xây dựng Prompt Mã:
1. Phân tích Phần tử Đường dẫn SVG: Trích xuất các lệnh đường dẫn từ phần tử đường dẫn SVG được cung cấp trong nhiệm vụ. Điều này bao gồm việc chia chuỗi và xác định từng lệnh.
2. Xử lý Lệnh Đường dẫn: Lặp qua từng lệnh trong phần tử đường dẫn và thực thi hành động tương ứng. Điều này bao gồm:
   - Di chuyển đến một điểm mới (lệnh M).
   - Vẽ một đường thẳng đến một điểm (lệnh L).
   - Vẽ một cung (lệnh A).
   - Các lệnh khác khi cần thiết.
3. [Quan trọng] Theo dõi Tọa độ: Theo dõi các tọa độ khi các lệnh đường dẫn được thực thi. Điều này bao gồm việc cập nhật vị trí hiện tại dựa trên các lệnh.

--- TRANG 22 ---
4. Xác định Hình dạng: Sau khi xử lý tất cả các lệnh, phân tích đường dẫn kết quả để xác định hình dạng đang được vẽ. Điều này có thể được thực hiện bằng cách so sánh đường dẫn cuối cùng với các đặc tính của các hình dạng đã biết.
5. Khớp với Tùy chọn được Cung cấp: So sánh hình dạng đã xác định với các tùy chọn được cung cấp trong nhiệm vụ để chọn câu trả lời đúng.
6. Trả về Kết quả: Trả về hình dạng đã xác định làm đầu ra theo định dạng được chỉ định ('(A)', '(B)', '(C)', ...).

Bằng cách tuân theo các bước này và diễn giải chính xác các lệnh đường dẫn SVG, chúng ta có thể xác định hình dạng đang được vẽ và chọn tùy chọn đúng từ các lựa chọn đã cho.

E.3 Phân tích được Tạo của Điều hướng

Giải thích:
Nhiệm vụ này bao gồm việc tuân theo một loạt hướng dẫn liên quan đến chuyển động và hướng để xác định xem vị trí cuối cùng có giống với điểm bắt đầu hay không. Thách thức nằm trong việc theo dõi chính xác các chuyển động và xoay để suy ra vị trí cuối cùng.

Phân tích Nhiệm vụ:
1. Theo dõi Chuyển động: Theo dõi các bước được thực hiện trong mỗi hướng (tiến, lùi, trái, phải) để xác định vị trí cuối cùng.
2. Thay đổi Hướng: Tính đến bất kỳ xoay nào (quay trái hoặc phải) có thể thay đổi hướng trong quá trình di chuyển.
3. Lý luận Không gian: Áp dụng lý luận logic để tính toán vị trí cuối cùng dựa trên tác động tích lũy của các chuyển động và xoay.

Xây dựng Prompt Mã:
1. Trích xuất Hướng dẫn: Phân tích văn bản đầu vào để trích xuất chuỗi các chuyển động và xoay.
2. Xử lý Chuyển động:
   - Khởi tạo các biến để theo dõi vị trí hiện tại (tọa độ x, y) và hướng (hướng đối mặt).
   - Lặp qua từng hướng dẫn, cập nhật vị trí và hướng tương ứng.
3. [Quan trọng] Xác định Vị trí Cuối cùng:
   - Tính toán vị trí cuối cùng dựa trên tác động tích lũy của các chuyển động và xoay.
   - Kiểm tra xem vị trí cuối cùng có khớp với điểm bắt đầu để xác định xem người tham gia có quay về điểm bắt đầu hay không.
4. Khớp với Tùy chọn: So sánh vị trí cuối cùng với điểm bắt đầu để xác định xem người tham gia có quay về điểm bắt đầu hay không.
5. Trả về Câu trả lời Cuối cùng: Xuất 'Có' nếu người tham gia quay về điểm bắt đầu, 'Không' nếu ngược lại.

E.4 Phân tích được Tạo cho Lý luận về Vật thể có Màu

Giải thích:
Nhiệm vụ này bao gồm việc xác định các thuộc tính cụ thể của các mục dựa trên mô tả và vị trí của chúng tương đối với nhau. Nhiệm vụ yêu cầu đọc cẩn thận các mô tả và phân tích mối quan hệ không gian giữa các mục.

Phân tích Nhiệm vụ:
Nhiệm vụ bao gồm việc diễn giải các mô tả về các mục và mối quan hệ không gian của chúng để trả lời các câu hỏi cụ thể. Nhiệm vụ này không yêu cầu theo dõi biến mà là một phương pháp lý luận định dạng tự do để trích xuất thông tin cần thiết và đưa ra suy luận logic.

Xây dựng Prompt Mã:
1. Phân tích các mô tả: Trích xuất các mô tả về các mục và màu sắc của chúng từ văn bản đầu vào.
2. Xác định mối quan hệ không gian: Xác định vị trí của các mục tương đối với nhau dựa trên các mô tả được cung cấp.
3. [Lý luận định dạng tự do] Rút ra câu trả lời với giải thích: Phân tích các mô tả và mối quan hệ không gian để trả lời câu hỏi cụ thể được đặt ra trong nhiệm vụ. Sử dụng một hàm như 'derive_answer_with_explanation' để cung cấp cả lý luận và câu trả lời đúng.
4. Khớp câu trả lời với các tùy chọn: So sánh câu trả lời được rút ra với các tùy chọn đã cho và chọn câu trả lời đúng.
5. Trả về câu trả lời cuối cùng: Xuất câu trả lời cuối cùng theo định dạng được chỉ định.

Bằng cách tuân theo các bước này, nhiệm vụ có thể được giải quyết hiệu quả bằng cách lý luận thông qua các mô tả và mối quan hệ không gian của các mục được cung cấp trong các thể hiện nhiệm vụ.

E.5 Phân tích được Tạo của Chuỗi Thời gian

Giải thích:
Nhiệm vụ này bao gồm việc xác định các khoảng thời gian có thể trong đó một người có thể đã đến thăm một địa điểm cụ thể dựa trên lịch trình các sự kiện đã cho trong suốt ngày. Mục tiêu là xác định các khung thời gian không xung đột với các hoạt động khác.

Phân tích Nhiệm vụ:
1. Thiết lập Dòng thời gian: Hiểu chuỗi các sự kiện và thời gian mà chúng xảy ra là rất quan trọng để xác định các khoảng thời gian có sẵn.
2. Xác định Hoạt động Xung đột: Nhận biết các khoảng thời gian trong đó người đó tham gia vào các hoạt động khác sẽ ngăn cản họ đến thăm địa điểm được chỉ định.
3. Xác định Thời gian Có thể Thăm: Bằng cách loại bỏ các khoảng thời gian xung đột, các khoảng thời gian còn lại đại diện cho thời gian có thể thăm.

Xây dựng Prompt Mã:
1. Trích xuất Thông tin Sự kiện: Phân tích lịch trình các sự kiện đã cho để trích xuất thời gian mà mỗi hoạt động xảy ra. Điều này có thể bao gồm việc tạo ra một cấu trúc dữ liệu để lưu trữ thông tin này.
2. Xác định Khoảng Thời gian Xung đột: Xác định các khoảng thời gian trong đó người đó bận rộn với các hoạt động khác sẽ ngăn cản họ đến thăm địa điểm được chỉ định.
3. [Quan trọng] Tính toán Khoảng Thời gian Có sẵn: Bằng cách trừ các khoảng thời gian xung đột khỏi tổng thời lượng ngày, xác định các khung thời gian trong đó người đó có thể đã đến thăm địa điểm được chỉ định.
4. Khớp với Tùy chọn: So sánh thời gian có thể thăm với các tùy chọn được cung cấp để xác định câu trả lời đúng.
5. Trả về Tùy chọn Đúng: Trả về chữ cái tương ứng với khoảng thời gian khớp với thời gian có thể thăm được tính toán.

Nhiệm vụ này bao gồm một quá trình suy luận logic dựa trên lịch trình các sự kiện đã cho để xác định các khoảng thời gian khả thi để thăm địa điểm được chỉ định. Bằng cách phân tích hệ thống dòng thời gian và loại bỏ các hoạt động xung đột, câu trả lời đúng có thể được xác định. Giải pháp nên cung cấp đánh giá rõ ràng và chính xác về các khoảng thời gian có sẵn cho việc thăm.

E.6 Phân tích được Tạo của Theo dõi Mục tiêu Xáo trộn

Giải thích:
Nhiệm vụ này bao gồm việc theo dõi vị trí hoặc quà tặng của các cá nhân khi họ hoán đổi với nhau trong một chuỗi trao đổi. Thách thức là theo dõi các hoán đổi một cách chính xác và xác định vị trí cuối cùng hoặc quà tặng của một cá nhân cụ thể ở cuối chuỗi.

Phân tích Nhiệm vụ:
1. Phân công Ban đầu: Hiểu vị trí bắt đầu hoặc quà tặng của mỗi cá nhân là rất quan trọng để theo dõi các hoán đổi một cách chính xác.
2. Hoán đổi Theo cặp: Xác định các cặp cá nhân hoán đổi vị trí hoặc quà tặng tại mỗi bước trong chuỗi.
3. Theo dõi Thay đổi: Giữ một bản ghi về các hoán đổi và cập nhật vị trí hoặc quà tặng của các cá nhân tương ứng.

Xây dựng Prompt Mã:
1. Khởi tạo Vị trí hoặc Quà tặng: Thiết lập một từ điển ánh xạ mỗi cá nhân với vị trí hoặc quà tặng ban đầu của họ.
2. Định nghĩa Cặp Hoán đổi: Tạo một danh sách các tuple đại diện cho các cặp cá nhân hoán đổi vị trí hoặc quà tặng tại mỗi bước.
3. [Quan trọng] Xử lý Hoán đổi: Lặp qua các cặp hoán đổi, cập nhật vị trí hoặc quà tặng của các cá nhân liên quan đến mỗi hoán đổi. Sử dụng câu lệnh if-else để xử lý các tình huống hoán đổi khác nhau.
4. Xác định Vị trí hoặc Quà tặng Cuối cùng: Sau khi xử lý tất cả các hoán đổi, xác định vị trí cuối cùng hoặc quà tặng của cá nhân được chỉ định.
5. Khớp và Xuất Câu trả lời: Phân tích các tùy chọn từ văn bản đầu vào, tìm tùy chọn tương ứng từ các tùy chọn đã cho, và chỉ xuất nhãn của tùy chọn làm câu trả lời cuối cùng cho câu hỏi.

--- TRANG 23 ---
E.7 Phân tích được Tạo của Mạng lưới Dối trá

Giải thích:
Nhiệm vụ này bao gồm việc xác định tính chân thực của một câu phát biểu được đưa ra bởi một cá nhân dựa trên các câu phát biểu được đưa ra bởi những người khác trong một chuỗi. Nhiệm vụ yêu cầu hiểu mối quan hệ giữa những người nói sự thật và những kẻ nói dối và áp dụng lý luận logic để xác định câu trả lời cuối cùng.

Phân tích Nhiệm vụ:
1. Thiết lập Mối quan hệ Sự thật: Câu phát biểu của mỗi người về người khác có thể được phân loại là đúng hoặc sai. Điều này tạo thành cơ sở để xác định ai nói sự thật và ai nói dối.
2. Truyền bá Tính chân thực: Bằng cách phân tích các câu phát biểu theo cách tuần tự, tính chân thực của mỗi người có thể được suy ra dựa trên các câu phát biểu được đưa ra bởi những người khác.
3. Xác định Câu hỏi Cuối cùng: Nhiệm vụ thường hỏi xem một người cụ thể có nói sự thật hay không dựa trên chuỗi các câu phát biểu.

Xây dựng Prompt Mã:
1. Phân tích Câu phát biểu: Trích xuất các câu phát biểu được đưa ra bởi mỗi người từ văn bản đầu vào. Điều này bao gồm việc xác định ai đang nói về ai và liệu họ có đang nói sự thật hay nói dối.
2. Thiết lập Mối quan hệ Sự thật: Tạo một từ điển để lưu trữ tính chân thực của mỗi người dựa trên các câu phát biểu được đưa ra bởi những người khác. Từ điển này sẽ được cập nhật khi các câu phát biểu được xử lý.
3. [Quan trọng] Phân tích Câu phát biểu: Lặp qua từng câu phát biểu và cập nhật tính chân thực của các cá nhân liên quan dựa trên logic rằng nếu A nói B nói dối, thì A đang nói sự thật nếu B là kẻ nói dối, và ngược lại. Bước này bao gồm lý luận logic và cập nhật từ điển sự thật.
4. Trích xuất Câu hỏi Cuối cùng: Xác định câu hỏi cụ thể được hỏi trong văn bản đầu vào về tính chân thực của một người cụ thể.
5. Xác định Câu trả lời: Dựa trên tính chân thực cuối cùng của người trong câu hỏi như được xác định bởi logic và các câu phát biểu được cung cấp, chọn 'Có' nếu người đó nói sự thật và 'Không' nếu họ không nói sự thật.

Bằng cách tuân theo các bước này và áp dụng lý luận logic cho các câu phát biểu được cung cấp, mã có thể xác định chính xác xem cá nhân được chỉ định có nói sự thật hay không.

F Prompt Mã giả được Tạo

F.1 P được Tạo của Ngôn ngữ Dyck

def complete_dyck_languages(input_text):
    # Bước 1: Phân tích văn bản đầu vào để trích xuất chuỗi dấu ngoặc.
    parentheses_sequence = extract_parentheses(input_text)
    print("Chuỗi dấu ngoặc:", parentheses_sequence)
    
    # Bước 2: Khởi tạo một ngăn xếp để theo dõi các dấu ngoặc mở cần đóng.
    stack = []

--- TRANG 24 ---
# Bước 3: Lặp qua từng ký tự trong chuỗi để xử lý các dấu ngoặc mở và đóng.
for i, char in enumerate(parentheses_sequence):
    if char in ['(', '[', '{', '<']:
        stack.append(char)
        print(f"Bước ({i}) - Đẩy {char} vào ngăn xếp")
    elif char in [')', ']', '}', '>']:
        if len(stack) == 0:
            print(f"Bước ({i}) - Không có dấu ngoặc mở khớp cho {char}")
        else:
            opening_bracket = stack.pop()
            print(f"Bước ({i}) - Khớp {opening_bracket} với {char}")

# Bước 4: Kiểm tra xem ngăn xếp có trống sau khi xử lý toàn bộ chuỗi.
if len(stack) > 0:
    print("Các dấu ngoặc chưa khớp còn lại trong ngăn xếp:", stack)
    # Bước 5: Tạo các dấu ngoặc đóng cần thiết để khớp với các dấu ngoặc mở còn lại.
    closing_brackets = generate_closing_brackets(stack)
    print("Dấu ngoặc đóng được tạo:", closing_brackets)
else:
    print("Tất cả các dấu ngoặc đều được khớp đúng cách.")

# Bước 6: Trả về các dấu ngoặc đóng được tạo làm câu trả lời cuối cùng.
return closing_brackets

F.2 P được Tạo của Hình học

def recognize_shape_from_svg(svg_path_element):
    # Bước 1: Phân tích phần tử đường dẫn SVG để trích xuất các lệnh đường dẫn. Xuất các lệnh đường dẫn được trích xuất.
    path_commands = extract_path_commands(svg_path_element)
    print("Các lệnh đường dẫn được trích xuất:", path_commands)
    
    # Bước 2: Xử lý từng lệnh đường dẫn để thực thi hành động tương ứng. Theo dõi các tọa độ khi các lệnh được thực thi.
    current_position = (0, 0)
    for command in path_commands:
        action, coordinates = process_path_command(command, current_position)
        print(f"Thực thi lệnh: {command} - Hành động: {action} - Tọa độ mới: {coordinates}")
        current_position = coordinates
    
    # Bước 3: Xác định hình dạng dựa trên các lệnh đường dẫn đã xử lý. Điều này bao gồm việc phân tích đường dẫn cuối cùng được vẽ.
    identified_shape = identify_shape(path_commands)
    print("Hình dạng đã xác định:", identified_shape)
    
    # Bước 4: Khớp hình dạng đã xác định với các tùy chọn được cung cấp để chọn câu trả lời đúng.
    options = extract_options(svg_path_element)
    answer = match_shape_with_options(identified_shape, options)
    
    # Bước 5: Trả về hình dạng đã xác định làm câu trả lời cuối cùng.
    return answer

F.3 P được Tạo của Điều hướng

def ends_up_at_start(input_text):
    # Bước 1: Trích xuất chuỗi các chuyển động và xoay từ văn bản đầu vào.
    instructions = extract_instructions(input_text)
    print("Hướng dẫn:", instructions)
    
    # Bước 2: Khởi tạo các biến để theo dõi vị trí hiện tại và hướng.
    x, y = 0, 0
    orientation = "N"
    print(f"Vị trí ban đầu: ({x}, {y}), Hướng: {orientation}")
    
    # Bước 3: Xử lý từng hướng dẫn để cập nhật vị trí và hướng tương ứng.
    for instruction in instructions:
        x, y, orientation = process_instruction(instruction, x, y, orientation)
        print(f"Hướng dẫn: {instruction} -> Vị trí: ({x}, {y}), Hướng: {orientation}")
    
    # Bước 4: Xác định vị trí cuối cùng sau khi tuân theo tất cả các hướng dẫn.
    final_position = (x, y)
    print("Vị trí Cuối cùng:", final_position)
    
    # Bước 5: Kiểm tra xem vị trí cuối cùng có khớp với điểm bắt đầu để xác định xem người tham gia có quay về điểm bắt đầu hay không.
    if final_position == (0, 0):
        return 'Có'
    else:
        return 'Không'

F.4 P được Tạo cho Lý luận về Vật thể có Màu

def solve_colored_objects(input_text):
    # Bước 1: Trích xuất các mô tả về các mục và màu sắc của chúng từ văn bản đầu vào.
    items = parse_items(input_text)
    print("Các mục trên bề mặt:\n", items)
    
    # Bước 2: Xác định vị trí của các mục tương đối với nhau dựa trên các mô tả được cung cấp.
    spatial_relationships = analyze_spatial_relationships(items)
    print("Mối quan hệ không gian giữa các mục:\n", spatial_relationships)
    
    # Bước 3: Rút ra câu trả lời với giải thích bằng cách phân tích các mô tả và mối quan hệ không gian.
    question = identify_question(input_text)
    print("Câu hỏi là:", question)
    reason, answer = derive_answer_with_explanation(items, spatial_relationships, question)
    print("Lý luận cho câu trả lời:", reason)
    
    # Bước 4: So sánh câu trả lời được rút ra với các tùy chọn đã cho và chọn câu trả lời đúng.
    options = extract_options(input_text)
    print("Tùy chọn câu trả lời:\n", options)
    final_answer = None
    for option in options:
        if answer in option:
            final_answer = option[:3]

--- TRANG 25 ---
break

# Bước 5: Trả về câu trả lời cuối cùng.
return final_answer

F.5 P được Tạo của Chuỗi Thời gian

def solve_temporal_sequences_quiz(input_text):
    # Bước 1: Trích xuất thông tin sự kiện từ văn bản đầu vào để hiểu dòng thời gian của các hoạt động trong suốt ngày.
    events_list = extract_events(input_text)
    print("Các sự kiện và thời gian của chúng:", events_list)
    
    # Bước 2: Xác định các khoảng thời gian xung đột khi người đó tham gia vào các hoạt động khác sẽ ngăn cản họ đến thăm địa điểm được chỉ định.
    conflicting_intervals = find_conflicting_intervals(events_list)
    print("Các khoảng thời gian xung đột:", conflicting_intervals)
    
    # Bước 3: Tính toán các khoảng thời gian có sẵn bằng cách trừ các khoảng xung đột khỏi tổng thời lượng ngày.
    available_intervals = calculate_available_intervals(conflicting_intervals)
    print("Các khoảng thời gian có sẵn để thăm:", available_intervals)
    
    # Bước 4: Khớp các khoảng thời gian có sẵn với các tùy chọn được cung cấp để xác định câu trả lời đúng.
    options = input_text.split("\n")[-5:]
    
    # Bước 5: Trả về tùy chọn đúng tương ứng với khoảng thời gian phù hợp với thời gian có thể thăm được tính toán.
    answer = find_correct_option(available_intervals, options)
    return answer

F.6 P được Tạo của Theo dõi Mục tiêu Xáo trộn

def track_swaps(input_text):
    # Bước 1: Khởi tạo Vị trí hoặc Quà tặng
    initial_assignments = extract_initial_assignments(input_text)
    print("Phân công ban đầu:", initial_assignments)
    
    # Bước 2: Định nghĩa Cặp Hoán đổi
    swap_pairs = extract_swap_pairs(input_text)
    print("Cặp hoán đổi:", swap_pairs)
    
    # Bước 3: Xử lý Hoán đổi
    final_assignments = initial_assignments.copy()
    for pair in swap_pairs:
        if pair[0] in final_assignments and pair[1] in final_assignments:
            temp = final_assignments[pair[0]]
            final_assignments[pair[0]] = final_assignments[pair[1]]
            final_assignments[pair[1]] = temp
            print(f"Sau khi hoán đổi {pair[0]} và {pair[1]}: {final_assignments}")
    
    # Bước 4: Xác định Vị trí hoặc Quà tặng Cuối cùng
    final_person = identify_final_person(input_text)
    final_position = final_assignments[final_person]

--- TRANG 26 ---
print(f"Vị trí cuối cùng hoặc quà tặng của {final_person} là: {final_position}")

# Bước 5: Khớp và Xuất Câu trả lời
options = extract_options(input_text)
print("Tùy chọn:", options)
answer = None
for option in options:
    if final_position in option:
        answer = option[:3]
return answer

F.7 P được Tạo của Mạng lưới Dối trá

def evaluate_boolean_word_problem(input_text):
    # Bước 1: Xác định các câu phát biểu và câu hỏi từ văn bản đầu vào.
    statements, question = input_text.split("?")
    statements = statements.split(". ")
    print("Các câu phát biểu:", statements)
    print("Câu hỏi:", question)
    
    # Bước 2: Khởi tạo từ điển sự thật để theo dõi ai nói sự thật và ai nói dối.
    truth_dict = {}
    
    # Bước 3: Xử lý từng câu phát biểu để cập nhật từ điển sự thật dựa trên logic được cung cấp.
    for statement in statements:
        person1, action, person2 = statement.split(" ")
        if action == "nói dối":
            truth_dict[person1] = not truth_dict[person2]
        else:
            truth_dict[person1] = truth_dict[person2]
        print(f"{person1} nói {person2} {action}. {person1} nói sự thật: {truth_dict[person1]}")
    
    # Bước 4: Xác định tính chân thực của người trong câu hỏi dựa trên từ điển sự thật.
    person_to_check = question.split(" ")[-2]
    answer = 'Có' if truth_dict[person_to_check] else 'Không'
    return answer

G Phân tích Định tính

Chúng tôi tiến hành phân tích định tính bằng cách so sánh các đầu ra của phương pháp chúng tôi (THINK-AND-EXECUTE) với các phương pháp đường cơ sở. So sánh này được trình bày qua các Bảng 7, 8, 9, 10, 11, 12, và 13.

--- TRANG 27 ---
[THIS IS TABLE: Comparison table showing different methods and their outputs for Dyck Languages task]

Ví dụ Thể hiện: Hoàn thành phần còn lại của chuỗi, đảm bảo rằng các dấu ngoặc được đóng đúng cách. Đầu vào: ( { { } }
Câu trả lời Thực tế: )

Phương pháp | Đầu ra
---|---
Prompting Trực tiếp | Câu trả lời cuối cùng: { })(Sai)
Zero-shot Prompting | Bước 1: ( { { } } ) Bước 2: ( { { } } ) Do đó, câu trả lời là }(Sai)
PoT | def solution(): stack = [] sequence = "( { { } } " for char in sequence: if char == "(" or char == " {": stack.append(char) elif char == ")" and stack[-1] == "(": stack.pop() elif char == " }" and stack[-1] == " {": stack.pop() for char in stack: if char == "(": sequence += ")" elif char == " {": sequence += " }" return sequence solution() >>>({ { } } )(Sai)
Lập kế hoạch NL | Bước 1: Dấu ngoặc mở trong chuỗi đầu vào: ( { { Bước 2: Dấu ngoặc đóng tương ứng: ( -> ) {->} {->} Bước 3: Dấu ngoặc đóng theo thứ tự: ) } } Câu trả lời cuối cùng: ) } }(Sai)
Của chúng tôi | Chuỗi dấu ngoặc: ['(', ' {', '{', '}', '}'] Bước 0 - dấu ngoặc: ( Ngăn xếp cập nhật: ['('] Bước 1 - dấu ngoặc: { Ngăn xếp cập nhật: ['(', ' {'] Bước 2 - dấu ngoặc: { Ngăn xếp cập nhật: ['(', ' {', '{'] Bước 3 - dấu ngoặc: } Ngăn xếp cập nhật: ['(', ' {'] Bước 4 - dấu ngoặc: } Ngăn xếp cập nhật: ['('] Phần còn lại của chuỗi: ) Ngăn xếp không trống. Chuỗi không được cân bằng đúng cách. Câu trả lời cuối cùng: ) (Đúng)

Bảng 7: So sánh kết quả cho Ngôn ngữ Dyck giữa các phương pháp đường cơ sở và THINK-AND-EXECUTE.

--- TRANG 28 ---
[Long example showing SVG path processing with multiple methods and their outputs, ending with table showing comparison]

Bảng 8: So sánh kết quả cho Hình học giữa các phương pháp đường cơ sở và THINK-AND-EXECUTE.

--- TRANG 29 ---
[Navigation example with input about following instructions and returning to starting point, showing different methods' outputs]

Bảng 9: So sánh kết quả cho Điều hướng giữa các phương pháp đường cơ sở và THINK-AND-EXECUTE.

--- TRANG 30 ---
[Long example about colored objects on floor with complex reasoning, showing different methods' approaches and outputs]

Bảng 10: So sánh kết quả cho Lý luận về Vật thể có Màu giữa các phương pháp đường cơ sở và của chúng tôi.

--- TRANG 31 ---
[Example about Jason's movie schedule with timeline analysis showing different methods' reasoning]

Bảng 11: So sánh kết quả cho Chuỗi Thời gian giữa các phương pháp đường cơ sở và THINK-AND-EXECUTE.

--- TRANG 32 ---
[Complex example about ball swapping game between Alice, Bob, Claire, Dave, and Eve, showing step-by-step tracking]

Bảng 12: So sánh kết quả cho Theo dõi Mục tiêu Xáo trộn giữa các phương pháp đường cơ sở và THINK-AND-EXECUTE.

--- TRANG 33 ---
[Example about truth-telling with Vina, Helene, Kandi, Jamey, and Ka, showing logical reasoning steps]

Bảng 13: So sánh kết quả cho Mạng lưới dối trá giữa các phương pháp đường cơ sở và THINK-AND-EXECUTE.
