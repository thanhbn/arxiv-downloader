# 2305.11014.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/planning/2305.11014.pdf
# Kích thước tệp: 1304370 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Lập Kế Hoạch Tổng Quát trong Các Miền PDDL với Các Mô Hình Ngôn Ngữ Lớn Đã Được Tiền Huấn Luyện
Tom Silver1, Soham Dan2, Kavitha Srinivas2,
Joshua Tenenbaum1, Leslie Kaelbling1, Michael Katz2
1Phòng thí nghiệm Khoa học Máy tính và Trí tuệ Nhân tạo MIT; 2IBM Research
Liên hệ: tslvr@mit.edu, Michael.Katz1@ibm.com

Tóm tắt
Nghiên cứu gần đây đã xem xét liệu các mô hình ngôn ngữ lớn (LLM) có thể hoạt động như những người lập kế hoạch: được giao một nhiệm vụ, tạo ra một kế hoạch. Chúng tôi nghiên cứu liệu LLM có thể phục vụ như những người lập kế hoạch tổng quát: được cung cấp một miền và các nhiệm vụ huấn luyện, tạo ra một chương trình có thể hiệu quả sản xuất các kế hoạch cho các nhiệm vụ khác trong miền đó. Cụ thể, chúng tôi xem xét các miền PDDL và sử dụng GPT-4 để tổng hợp các chương trình Python. Chúng tôi cũng xem xét (1) tóm tắt Chuỗi Suy nghĩ (CoT), nơi LLM được nhắc nhở tóm tắt miền và đề xuất một chiến lược bằng lời trước khi tổng hợp chương trình; và (2) gỡ lỗi tự động, nơi chương trình được xác thực đối với các nhiệm vụ huấn luyện, và trong trường hợp có lỗi, LLM được nhắc nhở lại với bốn loại phản hồi. Chúng tôi đánh giá phương pháp này trong bảy miền PDDL và so sánh nó với bốn nghiên cứu loại bỏ và bốn đường cơ sở. Nhìn chung, chúng tôi thấy rằng GPT-4 là một người lập kế hoạch tổng quát mạnh mẽ một cách đáng ngạc nhiên. Chúng tôi cũng kết luận rằng gỡ lỗi tự động rất quan trọng, tóm tắt CoT có tác động không đồng nhất, GPT-4 vượt trội hơn nhiều so với GPT-3.5, và chỉ hai nhiệm vụ huấn luyện thường đủ để có sự tổng quát hóa mạnh mẽ.1

Giới thiệu
Trong khi một số lớp nhiệm vụ ra quyết định tuần tự có thể chứng minh là không thể giải quyết (Chapman 1987), những lớp khác có thể được giải quyết hiệu quả với một chương trình dành riêng cho miền duy nhất. Trong trường hợp sau, có sự quan tâm đáng kể trong việc tự động tổng hợp những chương trình này khi được cung cấp một số lượng nhỏ các nhiệm vụ huấn luyện. Trong lập kế hoạch AI, đã có một số phương pháp cho vấn đề lập kế hoạch tổng quát này được đề xuất, với các chương trình được biểu thị như các danh sách quyết định được nâng lên, như các máy trạng thái hữu hạn, hoặc trong các ngôn ngữ dành riêng cho miền (Srivastava 2011; Bonet và Geffner 2015; Jiménez, Segovia-Aguas, và Jonsson 2019; Rivlin, Hazan, và Karpas 2020). Trong học tăng cường, các chính sách và hàm giá trị có điều kiện mục tiêu có thể được hiểu như những loại chương trình cụ thể được học với cùng một mục tiêu lập kế hoạch tổng quát (Sutton et al. 2011; Schaul et al. 2015). Mặc dù có những nỗ lực này, vẫn còn thách thức trong việc tổng hợp hiệu quả các chương trình từ ít nhiệm vụ huấn luyện mà có thể tổng quát hóa cho nhiều nhiệm vụ được giữ lại.

Với sự tiến bộ to lớn gần đây trong các mô hình ngôn ngữ lớn (LLM) (Brown et al. 2020; Chen et al. 2021; Chowdhery et al. 2022), đặc biệt trong việc tạo mã (Chen et al. 2021; Nijkamp et al. 2023; Chen et al. 2023a), công trình này đặt ra một câu hỏi đơn giản: liệu các LLM đã được tiền huấn luyện có thể được sử dụng cho lập kế hoạch tổng quát không? Cụ thể, chúng tôi nghiên cứu liệu GPT-4 (OpenAI 2023) có thể được sử dụng để viết một chương trình Python dành riêng cho miền giải quyết một tập hợp các nhiệm vụ trong một miền lập kế hoạch. Đối với mỗi miền, chúng tôi nhắc nhở GPT-4 với miền và một số lượng nhỏ các nhiệm vụ huấn luyện được mã hóa trong Ngôn ngữ Định nghĩa Miền Lập kế hoạch (PDDL) (McDermott 2000). Sau đó chúng tôi yêu cầu GPT-4 viết một chương trình tiêu thụ một mô tả nhiệm vụ (đã được phân tích) và xuất ra một kế hoạch. Để ngăn nó viết mã dựa trên tìm kiếm tổng quát cho miền—một khuynh hướng tự nhiên cho mối liên kết giữa PDDL và tìm kiếm trong dữ liệu tiền huấn luyện của nó—chúng tôi hướng dẫn GPT-4 triển khai "một chiến lược đơn giản không sử dụng tìm kiếm."

Ngoài giao thức cơ bản này, chúng tôi xem xét hai phần mở rộng. Đầu tiên, được truyền cảm hứng bởi Chuỗi Suy nghĩ (CoT) (Wei et al. 2022; Jiang et al. 2023), chúng tôi nhắc nhở GPT-4 viết một bản tóm tắt ngôn ngữ tự nhiên của miền PDDL. Sau đó chúng tôi yêu cầu nó mô tả một chiến lược giải pháp trước khi cuối cùng triển khai chiến lược đó trong Python. Thứ hai, được truyền cảm hứng bởi Inner Monologue (Huang et al. 2022b) và Corrective Re-prompting (Raman et al. 2022), chúng tôi tự động cung cấp phản hồi cho GPT-4 trong trường hợp nó không giải quyết được các nhiệm vụ huấn luyện. Ví dụ, nếu việc thực thi mã Python dẫn đến một ngoại lệ, chúng tôi trình bày GPT-4 với ngoại lệ đó và yêu cầu nó sửa mã. Chúng tôi lặp lại quá trình gỡ lỗi tự động này lên đến bốn lần hoặc cho đến khi tất cả các nhiệm vụ huấn luyện được giải quyết. Xem Hình 1 để có cái nhìn tổng quan về quy trình này.

Trong các thí nghiệm của chúng tôi, chúng tôi đánh giá phương pháp này trên bảy miền PDDL: sáu từ công trình gần đây trong lập kế hoạch tổng quát (Yang et al. 2022), và một miền mới thứ bảy. Chúng tôi thấy rằng phương pháp này là một đường cơ sở mạnh so với các phương pháp lập kế hoạch tổng quát hiện có. Đây là một phát hiện quan trọng mà chúng tôi mong đợi sẽ thông báo cho nghiên cứu tiếp theo trong lập kế hoạch tổng quát. Chúng tôi cũng trình bày một bộ các nghiên cứu loại bỏ và phân tích bổ sung để giải thích các đóng góp của tóm tắt CoT, gỡ lỗi tự động, tên trong PDDL, và GPT-4 so với GPT-3.5. Kết quả của chúng tôi cho thấy rằng gỡ lỗi tự động, tên PDDL, và GPT-4 rất quan trọng, trong khi tác động của CoT không đồng nhất. Cuối cùng, chúng tôi cung cấp các phân tích định tính về các trường hợp thất bại thường gặp, gợi ý các hướng cho công việc tương lai. Chúng tôi kết luận rằng GPT-4 là một người lập kế hoạch tổng quát mạnh mẽ một cách đáng ngạc nhiên khi được hướng dẫn đúng cách.

--- TRANG 2 ---
Miền & Nhiệm vụ Huấn luyện → Tóm tắt Miền → LLM → LLM → Chiến lược Đề xuất → LLM → Kế hoạch Tổng quát → Val → Hoàn thành
Tóm tắt CoT ← Gỡ lỗi Tự động

Hình 1: Tổng quan về quy trình lập kế hoạch tổng quát với các LLM đã được tiền huấn luyện. Xem văn bản để biết chi tiết.

Công trình Liên quan
LLM cho Lập kế hoạch (PDDL). Lập kế hoạch tổng quát với LLM có thể được xem như một thay thế cho lập kế hoạch với LLM (Sharma, Torralba, và Andreas 2022; Ahn et al. 2022; Huang et al. 2022a; Raman et al. 2022; Lin et al. 2022). Liên quan nhất là công trình của Valmeekam et al. (2022); Silver et al. (2022) những người xem xét lập kế hoạch dựa trên LLM trong các miền PDDL. Có một số lợi thế khi sử dụng LLM cho lập kế hoạch tổng quát, thay vì lập kế hoạch: (1) các chương trình được tạo ra bởi LLM có thể được kiểm tra và xác thực; (2) chạy một chương trình được tổng hợp có thể nhanh hơn nhiều (và rẻ hơn) so với truy vấn LLM cho mỗi nhiệm vụ mới; (3) các chương trình được tổng hợp có thể mở rộng đến các nhiệm vụ lớn tùy ý, trong khi các LLM hiện tại bị hạn chế bởi kích thước cửa sổ ngữ cảnh. Pallagani et al. (2022) xem xét việc tinh chỉnh một LLM để giải quyết các nhiệm vụ PDDL. Công trình gần đây khác đã xem xét việc sử dụng LLM để dịch giữa ngôn ngữ tự nhiên và PDDL (Collins et al. 2022; Lin et al. 2023; Xie et al. 2023; Liu et al. 2023). Những nỗ lực này có thể được kết hợp với phương pháp của chúng tôi.

Lập Kế hoạch Tổng quát. Công trình này đóng góp vào một tài liệu ngày càng phát triển về lập kế hoạch tổng quát (Fikes, Hart, và Nilsson 1972; Jiménez, Segovia-Aguas, và Jonsson 2019). Công trình trước đây đã xem xét việc tổng hợp các kế hoạch tổng quát theo một số cách: (1) thực hiện tìm kiếm thông qua một lớp giả thuyết của các chính sách tổng quát (Levine và Humphreys 2003; Jiménez và Jonsson 2015; Segovia-Aguas, Jiménez, và Jonsson 2018, 2021); (2) sử dụng các kế hoạch ví dụ để xây dựng một kế hoạch tổng quát, thường được biểu diễn bằng một máy trạng thái hữu hạn (Levesque 2005; Srivastava et al. 2011; Winner 2008); và (3) khám phá các trừu tượng hóa trạng thái và hành động và sau đó sử dụng chúng trong một kế hoạch tổng quát (Bonet và Geffner 2018). Một thách thức dai dẳng là thường có nhiều kế hoạch hợp lệ cho bất kỳ nhiệm vụ cụ thể nào, và chỉ một số trong những kế hoạch này phù hợp với một kế hoạch tổng quát đơn giản. PG3 giải quyết thách thức này bằng cách sử dụng các kế hoạch tổng quát ứng viên (được biểu diễn như các chính sách có điều kiện mục tiêu dạng danh sách quyết định được nâng lên) để hạn chế việc tạo ra các kế hoạch ví dụ (Yang et al. 2022). Chúng tôi sử dụng PG3 như điểm so sánh chính trong các thí nghiệm.

LLM cho Tạo Mã. Công trình của chúng tôi dựa trên các kỹ thuật gần đây sử dụng LLM để tạo mã (Chen et al. 2021; Nijkamp et al. 2023). Tóm tắt CoT liên quan đến một số kỹ thuật yêu cầu LLM phác thảo "suy nghĩ" của nó trước khi đến một triển khai cuối cùng (Wei et al. 2022; Jiang et al. 2023; Zheng et al. 2023). Một số công trình gần đây cũng sử dụng các chương trình làm lời nhắc (tức là, một chuỗi suy nghĩ có cấu trúc) trong nỗ lực giúp LLM thực hiện lý luận toán học (Gao et al. 2022; Imani, Du, và Shrivastava 2023). Liên quan đến gỡ lỗi tự động của chúng tôi, Xia và Zhang (2023); Chen et al. (2023b) xem xét việc sửa chữa chương trình tự động bằng cách nhắc nhở lại LLM với phản hồi từ các kiểm tra xác thực thất bại. Chen et al. (2023a) xem xét một paradigm liên quan, nhưng nơi phản hồi đến từ con người, thay vì các kiểm tra tự động. Cũng liên quan là các nỗ lực tạo ra mã có thể được sử dụng để ra quyết định robot (Liang et al. 2022; Singh et al. 2022). Ngoài LLM, việc tạo mã đã được nghiên cứu rộng rãi trong tổng hợp chương trình (Alur et al. 2013; Gulwani et al. 2017) và lập trình logic quy nạp (Muggleton 1991; Cropper và Dumančić 2022).

Bối cảnh và Thiết lập Vấn đề
Các Miền và Nhiệm vụ PDDL. Chúng tôi xem xét các nhiệm vụ lập kế hoạch xác định, được quan sát đầy đủ được biểu diễn trong PDDL. Trong các thí nghiệm, chúng tôi sử dụng tập con STRIPS với các loại và tiền điều kiện âm. Chúng tôi mô tả PDDL một cách không chính thức và giới thiệu người đọc đến các tài liệu tham khảo khác để có một cách xử lý chính thức (McDermott 2000). Một miền PDDL được đặc trưng bởi một tên, một tập hợp các loại, một tập hợp các vị từ, và một tập hợp các toán tử. Ví dụ, trong miền Delivery, một robot phải nhặt báo từ một căn cứ và sau đó giao chúng đến những địa điểm nhất định. Miền có hai loại: loc và paper. Một vị từ là (at ?l - loc), trong đó ?l là một placeholder cho một đối tượng loc. Miền có ba toán tử: (pick-up ?p - paper ?l - loc), (move ?from - loc ?to - loc), (deliver ?p - paper ?l - loc). Ví dụ, toán tử pick-up trong toàn bộ là:

(:action pick-up
:parameters (?p - paper ?l - loc)
:precondition (and (at ?l)
(isHomeBase ?l)
(unpacked ?p))
:effect (and
(not (unpacked ?p))
(carrying ?p)))

Một nhiệm vụ PDDL được đặc trưng bởi một miền, một tập hợp các đối tượng, một trạng thái ban đầu, và một mục tiêu. Một đối tượng có một tên và một loại, ví dụ, paper1 - paper. Một nguyên tử đất là một vị từ và một tuple các đối tượng thuộc các loại thích hợp, ví dụ, (unpacked paper1). Một trạng thái bao gồm một phép hội của các nguyên tử đất là đúng, giả định tất cả các nguyên tử đất khác là sai. Một mục tiêu là một phép hội của các nguyên tử đất phải đúng trong bất kỳ trạng thái mục tiêu nào. (Các biểu thức mục tiêu tổng quát hơn cũng có thể trong PDDL.) Ví dụ, trong Delivery, mục tiêu có thể bao gồm (satisfied loc1) và (satisfied loc2).

Một hành động là một toán tử và một tuple các đối tượng thuộc các loại thích hợp, ví dụ, (pick-up paper1 loc4). Các tiền điều kiện của toán tử xác định liệu hành động có áp dụng được không và các hiệu ứng định nghĩa những nguyên tử đất nào sẽ được thêm hoặc xóa nếu toán tử được thực thi. Một kế hoạch là một chuỗi hữu hạn các hành động. Kế hoạch hợp lệ cho một nhiệm vụ nếu tất cả các hành động đều áp dụng được khi được thực thi liên tiếp từ trạng thái ban đầu và nếu trạng thái cuối cùng là một trạng thái mục tiêu.

--- TRANG 3 ---
plicable và các hiệu ứng định nghĩa những nguyên tử đất nào sẽ được thêm hoặc xóa nếu toán tử được thực thi. Một kế hoạch là một chuỗi hữu hạn các hành động. Kế hoạch hợp lệ cho một nhiệm vụ nếu tất cả các hành động đều áp dụng được khi được thực thi liên tiếp từ trạng thái ban đầu và nếu trạng thái cuối cùng là một trạng thái mục tiêu.

Các miền PDDL, loại, vị từ, toán tử, đối tượng, và loại thường bao gồm các tên có thể đọc được như những cái được hiển thị ở trên. Những tên này không quan trọng đối với các bộ lập kế hoạch AI tiêu chuẩn hoặc các phương pháp lập kế hoạch tổng quát trước đây. Tuy nhiên, các tên rất quan trọng đối với con người—và, chúng tôi mong đợi, đối với LLM—cố gắng hiểu PDDL.

Lập Kế hoạch Tổng quát trong Các Miền PDDL. Một thể hiện lập kế hoạch tổng quát được đặc trưng bởi một miền PDDL và một phân phối các nhiệm vụ. Một tập hợp nhỏ các nhiệm vụ huấn luyện (10 hoặc ít hơn trong các thí nghiệm) từ phân phối được cung cấp tại thời điểm huấn luyện. Một tập hợp các nhiệm vụ đánh giá được giữ lại—thường liên quan đến nhiều đối tượng hơn—được sử dụng để đo lường hiệu suất. Mục tiêu là sử dụng các nhiệm vụ huấn luyện để tổng hợp một chương trình sẽ tạo ra các kế hoạch hợp lệ cho tất cả các nhiệm vụ đánh giá. Chúng tôi xem xét một nhiệm vụ đánh giá được giải quyết nếu chương trình trả về một kế hoạch hợp lệ trong một ngân sách thời gian đồng hồ treo tường cố định (30 giây trong các thí nghiệm). Nói cách khác, chúng tôi quan tâm đến việc thỏa mãn, không phải tối ưu, lập kế hoạch, và mối quan tâm chính của chúng tôi là hiệu quả của chính việc lập kế hoạch.

Lập Kế hoạch Tổng quát với LLM
Chúng tôi quan tâm đến mức độ mà các mô hình ngôn ngữ lớn đã được tiền huấn luyện (LLM) có thể được sử dụng cho lập kế hoạch tổng quát trong các miền PDDL. Chúng tôi giả định sự quen thuộc với LLM (Brown et al. 2020; Chen et al. 2021; Chowdhery et al. 2022; OpenAI 2023). Để sử dụng LLM cho lập kế hoạch tổng quát, chúng tôi cần định nghĩa một giao thức để nhắc nhở.

Giao thức Nhắc nhở
Công trình trước đây về nhắc nhở Chuỗi Suy nghĩ (CoT) đã cho thấy rằng việc yêu cầu một LLM "suy nghĩ từng bước" có thể cải thiện hiệu suất trong các nhiệm vụ lý luận (Wei et al. 2022). Với những kết quả này trong tâm trí, chúng tôi đã đưa ra giả thuyết rằng việc phân tách lập kế hoạch tổng quát thành ba giai đoạn—tóm tắt miền, đề xuất chiến lược, và triển khai chiến lược—sẽ cải thiện hiệu suất.

Tóm tắt Miền. Lời nhắc đầu tiên của chúng tôi cho LLM có dạng sau:
Miền: [Miền PDDL]
Các vấn đề ví dụ: [Nhiệm vụ Huấn luyện PDDL]
Viết một bản tóm tắt ngắn gọn về miền này bằng lời.

Để bù đắp cho kích thước cửa sổ ngữ cảnh hạn chế của các LLM dựa trên transformer như GPT-4, chúng tôi viết tắt việc mã hóa các nhiệm vụ huấn luyện theo hai cách. Đầu tiên, chúng tôi luôn chỉ sử dụng hai nhiệm vụ huấn luyện, ngay cả khi có nhiều hơn được cung cấp. Thứ hai, trong mỗi nhiệm vụ huấn luyện, chúng tôi giới hạn số lượng đối tượng và các nguyên tử đất trạng thái ban đầu được hiển thị. Đối với mỗi loại đối tượng, nếu số lượng đối tượng thuộc loại đó vượt quá 10, chúng tôi cắt bớt tập hợp đối tượng và thêm dấu chấm lửng. Tương tự, đối với mỗi vị từ, nếu số lượng nguyên tử đất với vị từ đó vượt quá 10, chúng tôi cắt bớt và thêm dấu chấm lửng. Thực tế là chúng tôi chỉ cần truyền đạt "ý chính" của phân phối nhiệm vụ, thay vì toàn bộ nhiệm vụ, là một lợi thế khác của lập kế hoạch tổng quát với LLM so với lập kế hoạch với LLM.

Đề xuất Chiến lược. Sau khi LLM phản hồi lời nhắc đầu tiên, chúng tôi yêu cầu một chiến lược lập kế hoạch tổng quát:
Có một chiến lược đơn giản để giải quyết tất cả các vấn đề trong miền này mà không sử dụng tìm kiếm. Chiến lược đó là gì?

Trong các thí nghiệm sơ bộ, việc bỏ qua cụm từ "mà không sử dụng tìm kiếm" thường sẽ dẫn LLM đề xuất một chiến lược lập kế hoạch dựa trên tìm kiếm.

Triển khai Chiến lược. Cuối cùng, chúng tôi yêu cầu LLM triển khai chiến lược như một chương trình Python:
Triển khai chiến lược như một hàm Python. Mã nên có dạng
def get_plan(objects, init, goal):
# Mã của bạn ở đây
return plan
trong đó
•objects là một tập hợp các tuple (tên đối tượng, tên loại)
•init là một tập hợp các nguyên tử đất được biểu diễn như các tuple của tên vị từ và đối số (ví dụ, ('predicate-foo', 'object-bar', ...))
•goal cũng là một tập hợp các nguyên tử đất được biểu diễn theo cùng cách
•plan là một danh sách các hành động, trong đó mỗi hành động là một toán tử đất được biểu diễn như một chuỗi (ví dụ, '(operator-baz object-qux ...)')

Trong các miền không có loại đối tượng, objects thay vào đó chỉ là một tập hợp các tên đối tượng.

Gỡ lỗi Tương tác Tự động
Sau khi LLM đã đề xuất một triển khai get_plan, chúng tôi sử dụng các nhiệm vụ huấn luyện để xác thực triển khai. Đối với mỗi nhiệm vụ huấn luyện, chúng tôi thực thi get_plan cho đến khi nó trả về một đầu ra, ném ra một ngoại lệ, hoặc đạt đến thời gian chờ (30 giây). Nếu đầu ra là một kế hoạch hợp lệ, chúng tôi tiếp tục đến nhiệm vụ huấn luyện tiếp theo. Nếu không, chúng tôi nhắc nhở lại với một trong bốn loại phản hồi.

Ngoại lệ Python. Nếu việc thực thi get_plan dẫn đến một ngoại lệ Python, chúng tôi nắm bắt traceback và báo cáo nó cho LLM cùng với đầu vào. Một ví dụ được hiển thị bên dưới, với traceback được viết tắt để rõ ràng.
Với nhiệm vụ này: [Nhiệm vụ Huấn luyện PDDL]
Mã đã ném ra ngoại lệ sau:
File "<file-name-omitted>", line 86
lift_at = {atom[1]: atom[2] ...}
˜˜˜˜ˆˆˆ
IndexError: tuple index out of range
Sửa mã.

--- TRANG 4 ---
Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng việc bao gồm toàn bộ traceback có thể cải thiện hiệu suất.

Thời gian chờ. Nếu get_plan không hoàn thành trước thời gian chờ, chúng tôi báo cáo cho LLM rằng chương trình không hoàn thành và gợi ý rằng một vòng lặp vô hạn có thể là nguyên nhân. Chúng tôi cũng cung cấp một traceback cho thấy nơi chương trình đang thực thi khi bị ngắt. Một ví dụ được hiển thị bên dưới.
Với nhiệm vụ này: [Nhiệm vụ Huấn luyện PDDL]
Mã đã ném ra ngoại lệ sau:
File "<file-name-omitted>", line 23
while not any(span_loc[1] == ...:
ˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆ
KeyboardInterrupt
Mã đã bị ngắt vì nó hết thời gian chờ (có thể vòng lặp vô hạn).
Sửa mã.

Traceback một lần nữa được viết tắt để rõ ràng. Lưu ý rằng KeyboardInterrupt được tự động ném ra sau 30 giây. Trong thực tế, gần như tất cả các thời gian chờ chúng tôi quan sát là do lỗi logic trong mã, thay vì các triển khai không hiệu quả nhưng đúng.

Cú pháp Kế hoạch. Nếu get_plan trả về một đầu ra, chúng tôi kiểm tra cú pháp của nó: liệu nó có phải là một danh sách các chuỗi, liệu mỗi chuỗi có được bao bọc trong dấu ngoặc đơn và phân tách bằng dấu cách, và liệu tên hành động, tên đối tượng, và số lượng đối tượng trên mỗi hành động có hợp lệ đối với miền và nhiệm vụ. Nếu bất kỳ kiểm tra nào trong số này thất bại, chúng tôi báo cáo thất bại cho LLM. Đối với loại thất bại này, chúng tôi cũng nhắc nhở LLM về các toán tử hợp lệ. Một ví dụ được hiển thị bên dưới.
Với nhiệm vụ này: [Nhiệm vụ Huấn luyện PDDL]
Mã đã trả về kế hoạch này:
['walk r0_c0 r0_c1', 'walk ...]
Tuy nhiên, hành động walk r0 c0 r0c1 không hợp lệ ở bước 0. LƯU Ý: các toán tử hợp lệ là: (climb ?from ?to) (walk ?from ?to).
Sửa mã.

Kế hoạch đầy đủ được hiển thị cho LLM nhưng được viết tắt trong ví dụ để rõ ràng. Vấn đề trong ví dụ này là các hành động không được bao bọc trong dấu ngoặc đơn.

Ngữ nghĩa Kế hoạch. Nếu tất cả các kiểm tra trước đây đều vượt qua, chúng tôi sử dụng công cụ VAL (Howey, Long, và Fox 2004) để kiểm tra liệu đầu ra get_plan có phải là một kế hoạch hợp lệ về mặt ngữ nghĩa. Nếu không, VAL cung cấp "lời khuyên sửa chữa kế hoạch", ví dụ, nếu có một hành động với các tiền điều kiện không hợp lệ. Chúng tôi trích xuất lời khuyên sửa chữa kế hoạch này và báo cáo nó cho LLM. Lưu ý rằng chúng tôi sử dụng lời khuyên này không để sửa chữa kế hoạch, mà là để sửa chữa kế hoạch tổng quát. Một ví dụ được hiển thị bên dưới.
Với nhiệm vụ này: [Nhiệm vụ Huấn luyện PDDL]
Mã đã thất bại. Nó đã trả về kế hoạch sau:
['(pick-up paper-1 loc-0)', ...].
LƯU Ý: (pick-up paper-0 loc-0) có một tiền điều kiện không thỏa mãn tại thời điểm 3
(Đặt (at loc-0) thành true)
Sửa mã.

Chi tiết Bổ sung. Sau khi nhắc nhở lại LLM, chúng tôi lặp lại quá trình kiểm tra mã và báo cáo bất kỳ thất bại nào lên đến bốn lần. Để xử lý các trường hợp hiếm khi LLM triển khai các hàm trợ giúp riêng của nó và sau đó giả định trong quá trình gỡ lỗi rằng các hàm trợ giúp vẫn có sẵn, chúng tôi nối từng phản hồi mới từ LLM vào một tệp Python đang phát triển, thay vì ghi đè các phản hồi trước đó. Nếu một thất bại vẫn gặp phải trong lần thử cuối cùng, phản hồi cuối cùng được sử dụng trong quá trình đánh giá.

Thí nghiệm và Kết quả
Thông qua các thí nghiệm, chúng tôi giải quyết những câu hỏi này: 1. GPT-4 có thể được sử dụng cho lập kế hoạch (PDDL) tổng quát không? 2. Các chương trình được tổng hợp có hiệu quả không? 3. Tóm tắt CoT có giúp ích không? 4. Gỡ lỗi tự động có giúp ích không? 5. GPT-4 dựa vào tên trong PDDL đến mức độ nào? 6. GPT-4 so sánh với GPT-3.5 như thế nào? 7. Mỗi loại lỗi có giúp ích không? 8. Cần bao nhiêu nhiệm vụ huấn luyện?

Thiết lập Thí nghiệm
Chúng tôi đánh giá chín phương pháp lập kế hoạch tổng quát trên bảy miền PDDL qua 10 hạt giống ngẫu nhiên. Các nhiệm vụ được tạo ngẫu nhiên cho mỗi hạt giống.

Các Miền. Sáu miền đầu tiên (và các nhiệm vụ) được lấy trực tiếp từ công trình trước đây của Yang et al. (2022). Trong số này, bốn miền (Gripper, Miconic, Ferry, Spanner) là các điểm chuẩn lập kế hoạch tiêu chuẩn và hai miền còn lại (Delivery, Forest) được giới thiệu bởi công trình đó. Miền cuối cùng (Heavy) là mới đối với công trình này. Dữ liệu tiền huấn luyện cho GPT-4 không có sẵn công khai, nhưng có khả năng các định nghĩa miền cho ít nhất bốn miền tiêu chuẩn đã được bao gồm trong dữ liệu đó. Tuy nhiên, chúng tôi tin rằng không có khả năng các kế hoạch tổng quát được bao gồm, và đối với miền Heavy, chúng tôi có thể đảm bảo rằng cả miền và các kế hoạch tổng quát đều không được bao gồm. Chúng tôi bây giờ mô tả ngắn gọn mỗi miền. Trừ khi được chỉ định khác, có 10 nhiệm vụ huấn luyện và 30 nhiệm vụ đánh giá trên mỗi miền và hạt giống.

•Delivery: Báo ở một căn cứ phải được giao đến nhiều địa điểm. Có năm nhiệm vụ huấn luyện với 9–17 đối tượng; các nhiệm vụ đánh giá có 70–100 đối tượng.
•Forest: Một người đi bộ đường dài phải điều hướng một lưới 2D để đến một địa điểm mục tiêu trong khi leo đồi và tránh nước. Một con đường được đánh dấu dẫn đến mục tiêu, nhưng có những con đường ngắn hơn qua đất. Có 4 nhiệm vụ huấn luyện với 64-100 đối tượng; các nhiệm vụ đánh giá có 100–144 đối tượng.
•Gripper: Các quả bóng phải được vận chuyển giữa các phòng bởi một robot với hai gripper. Các nhiệm vụ huấn luyện có 20–30 đối tượng; các nhiệm vụ đánh giá có 60–80 đối tượng.

--- TRANG 5 ---
Miền | GPT-4 | Không CoT | Không Debug | Không Tên | GPT-3.5 | PG3 | Policy Eval | Plan Compare | Random
Delivery | 0.90 | 0.70 | 0.10 | 0.10 | 0.00 | 1.00 | 0.00 | 0.10 | 0.00
Forest | 1.00 | 1.00 | 0.62 | 0.11 | 0.32 | 1.00 | 1.00 | 0.16 | 0.03
Gripper | 0.90 | 0.80 | 0.50 | 0.10 | 0.00 | 1.00 | 0.00 | 0.20 | 0.00
Miconic | 0.01 | 0.13 | 0.00 | 0.00 | 0.00 | 1.00 | 0.00 | 0.10 | 0.13
Ferry | 0.80 | 0.20 | 0.26 | 0.00 | 0.00 | 1.00 | 0.00 | 0.90 | 0.00
Spanner | 0.10 | 0.00 | 0.00 | 0.00 | 0.00 | 1.00 | 1.00 | 0.56 | 0.06
Heavy | 0.60 | 1.00 | 0.20 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00

Bảng 1: Tỷ lệ nhiệm vụ đánh giá được giải quyết. Tất cả kết quả được tính trung bình qua 10 hạt giống ngẫu nhiên và 30 nhiệm vụ đánh giá trên mỗi hạt giống.

Hình 2: Thời gian chạy chương trình được tổng hợp bởi GPT-4 so với một bộ lập kế hoạch hiện đại (Fast Downward). Lưu ý các trục log-log. Mỗi điểm là một trung vị qua 10 nhiệm vụ được tạo mới, qua tất cả các hạt giống nơi lập kế hoạch tổng quát giải quyết tất cả các nhiệm vụ đánh giá.

Hình 3: Tỷ lệ nhiệm vụ đánh giá được giải quyết bởi GPT-4 so với số bước gỡ lỗi được cho phép, được tính trung bình qua tất cả các miền và hạt giống. Vùng tô màu là sai số chuẩn.

•Miconic: Hành khách trong nhiều tòa nhà, mỗi tòa có một thang máy, phải được đón và trả ở các tầng khác nhau. Các nhiệm vụ huấn luyện có 6–30 đối tượng; các nhiệm vụ đánh giá có 11–150 đối tượng.
•Ferry: Xe hơi phải được chuyên chở giữa các đảo bằng một phà có thể chở tối đa một xe. Các nhiệm vụ huấn luyện có 13–20 đối tượng; các nhiệm vụ đánh giá có 30–50 đối tượng.
•Spanner: Cờ lê (spanner) và đai ốc được phân phối dọc theo một hành lang một chiều. Một tác nhân phải di chuyển xuống hành lang, nhặt cờ lê, và vặn chặt đai ốc, sử dụng mỗi cờ lê tối đa một lần. Các nhiệm vụ huấn luyện có 9–15 đối tượng; các nhiệm vụ đánh giá có 30–60 đối tượng.
•Heavy: Các vật phẩm phải được xếp chồng vào một hộp trống. Một vật phẩm chỉ có thể được xếp chồng lên một vật phẩm khác nếu vật phẩm sau nặng hơn. Các mối quan hệ trọng lượng được biểu thị qua một vị từ (heavier ?x ?y). Một thách thức là xác định vật phẩm nào để đặt vào hộp trước, tức là, vật phẩm nào nặng nhất. Các nhiệm vụ huấn luyện có 3–10 đối tượng; các nhiệm vụ đánh giá có 100–250 đối tượng.

Loại Lỗi | Tất cả | Thành công | Thất bại
Ngoại lệ Python | 40.0 | 28.9 | 42.5
Ngữ nghĩa Kế hoạch | 34.0 | 44.7 | 31.4
Cú pháp Kế hoạch | 13.0 | 18.4 | 11.7
Thời gian chờ | 13.0 | 8.0 | 14.4

Bảng 2: Tỷ lệ phần trăm các loại lỗi gặp phải bởi GPT-4 trong các nhiệm vụ huấn luyện qua tất cả các miền và hạt giống. "Tất cả" là phân tích cho tất cả các nhiệm vụ huấn luyện; "Thành công" là phân tích cho các thử nghiệm nơi tất cả các nhiệm vụ đánh giá sau đó được giải quyết; "Thất bại" là phân tích cho các thử nghiệm không Thành công.

Các Phương pháp. Chúng tôi đánh giá phương pháp chính, bốn nghiên cứu loại bỏ, và bốn đường cơ sở. Các đường cơ sở được lấy từ công trình của Yang et al. (2022); xem công trình đó để biết chi tiết.

•GPT-4: Phương pháp chính của chúng tôi với tóm tắt CoT và gỡ lỗi tự động.
•Không CoT: Một nghiên cứu loại bỏ của phương pháp chính không sử dụng tóm tắt CoT. Ba lời nhắc ban đầu được kết hợp và "Viết một bản tóm tắt ngắn gọn về miền này bằng lời." và "Chiến lược đó là gì?" bị xóa.
•Không Debug: Một nghiên cứu loại bỏ của phương pháp chính không sử dụng gỡ lỗi tự động. Triển khai đầu tiên của get_plan được sử dụng để đánh giá.
•Không Tên: Một nghiên cứu loại bỏ của phương pháp chính nơi tất cả tên trong các miền và nhiệm vụ PDDL được thay thế bằng các định danh không mô tả. Ví dụ, các vị từ được đổi tên thành predicate1, predicate2, v.v., các toán tử được đổi tên thành operator1, operator2, v.v. Tổng cộng, tên của miền, vấn đề, vị từ, toán tử, biến, loại, và đối tượng được loại bỏ.

--- TRANG 6 ---
•GPT-3.5: GPT-3.5 với tóm tắt CoT và gỡ lỗi tự động.
•PG3: Phương pháp lập kế hoạch tổng quát được đề xuất bởi Yang et al. (2022). Các chương trình được tổng hợp là các chính sách có điều kiện mục tiêu được triển khai như các danh sách quyết định được nâng lên. Tổng hợp được thực hiện qua tìm kiếm heuristic trong không gian chính sách với heuristic mới của họ.
•Policy Evaluation (PE): Một phương pháp từ Yang et al. (2022) giống hệt với PG3 ngoại trừ heuristic được sử dụng cho tìm kiếm chính sách là thưa thớt: mỗi chính sách ứng viên được chấm điểm dựa trên số lượng nhiệm vụ huấn luyện được giải quyết.
•Plan Compare (PC): Một phương pháp khác từ Yang et al. (2022) giống hệt với PG3 ngoại trừ heuristic tìm kiếm chính sách: các kế hoạch ví dụ cho mỗi nhiệm vụ huấn luyện được tạo ra offline, và chính sách được chấm điểm dựa trên sự thỏa thuận của nó với các kế hoạch ví dụ.
•Random: Các hành động hợp lệ được lấy mẫu ngẫu nhiên và thực thi cho đến khi gặp phải một bế tắc, đạt được mục tiêu, hoặc vượt quá một chân trời tối đa (mặc định 1000, nhưng xem công trình trước đây).

Chi tiết Thí nghiệm. Chúng tôi sử dụng một laptop Macbook Pro với chip M1 và 64 GB RAM. Vì API cho GPT-4 không có sẵn công khai, chúng tôi đã sử dụng giao diện trình duyệt ChatGPT cho tất cả các thí nghiệm (bao gồm cả đường cơ sở GPT-3.5). Quy trình hoàn toàn tự động ngoại trừ việc các lời nhắc và phản hồi được sao chép và dán thủ công giữa terminal và trình duyệt, với clipboard được cập nhật theo chương trình. Để tạo điều kiện tái tạo, chúng tôi đã phát hành tất cả các nhật ký trò chuyện và mã.

Kết quả và Phân tích
Kết quả chính được trình bày trong Bảng 1. Các ví dụ về chương trình được tổng hợp được trình bày trong phụ lục. Nhìn chung, hiệu suất của GPT-4 với tóm tắt CoT và gỡ lỗi tự động mạnh trong Delivery, Forest, Gripper, Ferry, và Heavy, và kém trong Miconic và Spanner. Lưu ý rằng tỷ lệ thành công được báo cáo được tính trung bình qua tất cả các cuộc trò chuyện LLM. Trong thực tế, hiệu suất có thể được tăng cường bằng cách khởi động lại cuộc trò chuyện nhiều lần và sử dụng chương trình tốt nhất được tìm thấy (Chowdhery et al. 2022). Hiệu suất mạnh trong Heavy đặc biệt đáng chú ý. Các đường cơ sở lập kế hoạch tổng quát thất bại trong miền này vì các danh sách quyết định được nâng lên quá hạn chế như các biểu diễn chương trình và không thể khám phá một khái niệm như "nặng nhất tổng thể" từ các mối quan hệ nặng hơn theo cặp. Khả năng của GPT-4 viết mã Python tổng quát là một trong những lợi thế lớn nhất của nó như một phương pháp lập kế hoạch tổng quát.

Chúng tôi cũng quan sát rằng trong gần như tất cả các trường hợp, GPT-4 hoặc (1) giải quyết tất cả các nhiệm vụ huấn luyện và sau đó giải quyết tất cả các nhiệm vụ đánh giá; hoặc (2) không giải quyết được ít nhất một nhiệm vụ huấn luyện và sau đó không giải quyết được tất cả các nhiệm vụ đánh giá. Nói cách khác, quá khớp với các nhiệm vụ huấn luyện rất hiếm, và hiệu suất đánh giá thường là tất cả hoặc không có gì. Xem Bảng 3 trong phụ lục để biết tỷ lệ tối đa nhiệm vụ được giải quyết.

Thất bại Miconic. GPT-4 có một số chế độ thất bại nhất quán trong Miconic. Đầu tiên, ở cấp độ đề xuất chiến lược, nó thường không nhận ra rằng có thể có nhiều tòa nhà, mỗi tòa có thang máy riêng. Điều này thừa nhận là khó nhận ra cho việc mã hóa PDDL: các tòa nhà chỉ tồn tại ngầm dựa trên mối quan hệ above giữa các đối tượng sàn. Ví dụ, người ta sẽ cần thấy rằng cả (above f1 b1, f1b2) và (above f1 b2, f1b1) đều không đúng và kết luận rằng các sàn ở trong hai tòa nhà khác nhau. Tuy nhiên, đặc biệt sau gỡ lỗi tự động, GPT-4 có thể nhận ra rằng có nhiều tòa nhà, và hơn nữa, tên tòa nhà (ví dụ, b1, b2) có thể được trích xuất từ tên sàn. Nhưng sau đó các thất bại khác thường xảy ra, ví dụ, cố gắng và thất bại trong việc tạo một thứ tự tổng thể của các sàn từ vị từ above. Nhìn chung, chúng tôi tin rằng Miconic chỉ vượt quá giới hạn của khả năng hiện tại của GPT-4 và có thể sẽ được giải quyết bởi thế hệ LLM tiếp theo, hoặc bởi GPT-4 với hướng dẫn bổ sung.

Thất bại Spanner. GPT-4 liên tục thất bại trong Spanner trong quá trình đề xuất chiến lược. Cụ thể, GPT-4 dường như không nhận ra rằng các vị trí trong Spanner được kết nối trong một chuỗi một chiều. Chiến lược được đề xuất thường là "đầu tiên thu thập tất cả các spanner, sau đó vặn chặt tất cả các đai ốc" hoặc tương tự. Một chiến lược đúng thay vào đó sẽ là "di chuyển đến mỗi vị trí trong chuỗi, nhặt bất kỳ spanner nào và vặn chặt bất kỳ đai ốc nào tại mỗi vị trí." Nhận ra sự tồn tại của chuỗi một chiều đòi hỏi việc kiểm tra các nguyên tử link trong các vấn đề huấn luyện. Ngay cả sau gỡ lỗi tự động, GPT-4 thường giả định, một cách không chính xác, rằng các liên kết là giao hoán.

Hiệu quả Chương trình. Mặc dù chúng tôi nhắc nhở LLM triển khai một chương trình "đơn giản" không sử dụng tìm kiếm, vẫn có thể LLM tạo ra một chương trình có sử dụng tìm kiếm hoặc chậm vì những lý do khác (ví dụ, độ phức tạp thuật toán kém). Do đó chúng tôi đo thời gian chạy chương trình được tổng hợp. Như một đường cơ sở cho việc so sánh của chúng tôi, chúng tôi sử dụng một bộ lập kế hoạch PDDL độc lập miền hiện đại LAMA (Richter và Westphal 2010) qua Fast Downward (Helmert 2006), dừng lại sau khi kế hoạch đầu tiên được tìm thấy.2 Trong Hình 2, chúng tôi vẽ biểu đồ thời gian chạy thực tế như một hàm của kích thước vấn đề (số lượng đối tượng). Nhìn chung, chúng tôi thấy rằng các chương trình được tổng hợp không chỉ mở rộng thuận lợi đối với bộ lập kế hoạch, mà còn liên tục đánh bại bộ lập kế hoạch về thời gian chạy tuyệt đối với biên độ lớn. Điều này đáng chú ý cho rằng LLM tổng hợp các chương trình Python, trong khi bộ lập kế hoạch PDDL sử dụng một kết hợp được tối ưu hóa cao của mã Python và C++. Nút thắt cổ chai cho Fast Downward thường là grounding toán tử. Các chương trình của LLM không cần ground toán tử—chúng có thể đi trực tiếp từ nhiệm vụ đến kế hoạch.

Vai trò của CoT. So sánh GPT-4 với Không CoT, chúng tôi thấy rằng tác động của tóm tắt CoT là hỗn hợp: nó dường như giúp ích trong hầu hết các trường hợp, nhưng gây tổn hại trong Miconic và Heavy. Miconic là một trường hợp đặc biệt thú vị. Khi sử dụng tóm tắt CoT, GPT-4 gần như luôn đề xuất một chiến lược "quét", nơi thang máy đầu tiên được di chuyển đến tầng dưới cùng; sau đó di chuyển lên từng tầng một cho đến tầng trên cùng, đón và thả hành khách dọc đường; sau đó di chuyển xuống từng tầng một, một lần nữa đón và thả hành khách. Chiến lược này sẽ hoạt động về lý thuyết, nhưng nó đòi hỏi việc tìm một thứ tự tổng thể của các tầng trong các tòa nhà. Không có CoT, GPT-4 thường cố gắng một chiến lược khác: đón, di chuyển, và thả từng hành khách, từng người một. Chiến lược sau không đòi hỏi một thứ tự tổng thể qua các tầng và có thể đơn giản hơn để triển khai trong Python. Ví dụ này cho thấy rằng CoT có thể ảnh hưởng đến chiến lược được đề xuất bởi GPT-4. Hơn nữa, các chiến lược "đơn giản" để mô tả bằng ngôn ngữ tự nhiên có thể không đơn giản để triển khai trong mã. Trong Heavy, không có sự khác biệt rõ ràng trong các chiến lược với và không có CoT. Vì một chiến lược tốt rõ ràng có thể nhận biết được từ PDDL một mình, có thể CoT "làm xao nhãng" GPT-4 trong quá trình triển khai.

Vai trò của gỡ lỗi tự động. So sánh GPT-4 với Không Debug, chúng tôi thấy rằng gỡ lỗi tự động thường cải thiện hiệu suất một cách đáng kể. Hình 3 cho thấy rằng ngay cả một bước gỡ lỗi tự động giúp ích đáng kể, và các bước tiếp theo thể hiện những cải thiện biên giảm dần. Bảng 2 báo cáo tỷ lệ các loại lỗi gặp phải trong quá trình huấn luyện. Các ngoại lệ Python phổ biến nhất, theo sau chặt chẽ bởi các lỗi trong ngữ nghĩa kế hoạch, sau đó là các lỗi trong cú pháp kế hoạch, và cuối cùng là thời gian chờ. Chúng tôi cũng thấy rằng các loại lỗi được phân phối tốt trong các thử nghiệm thành công, cho thấy rằng mỗi loại phản hồi đều có lợi. Nói chung, GPT-4 có xu hướng thực hiện các sửa chữa nhỏ, cục bộ đối với mã trong quá trình gỡ lỗi tự động. Nếu mã có cấu trúc sai và đòi hỏi một viết lại đáng kể, việc khởi động lại đối thoại từ đầu có thể được yêu cầu.

Vai trò của tên PDDL. Kiểm tra kết quả cho nghiên cứu loại bỏ Không Tên, chúng tôi thấy hiệu suất tổng thể rất kém. Điều này xác nhận giả thuyết của chúng tôi rằng các thuật ngữ có mặt trong các miền và nhiệm vụ PDDL hữu ích cho LLM, như chúng sẽ là đối với con người. Lưu ý rằng các bộ lập kế hoạch như Fast Downward và các bộ lập kế hoạch tổng quát như PG3 sẽ không bị ảnh hưởng bởi những thay đổi tên. Tuy nhiên, có một vài trường hợp nơi nghiên cứu loại bỏ Không Tên thành công, cho thấy rằng LLM có một số khả năng cho lập kế hoạch tổng quát hoàn toàn cú pháp.

GPT-3.5 so với GPT-4. Kiểm tra kết quả cho GPT-3.5, chúng tôi thấy rằng nó hoạt động tệ hơn nhiều so với GPT-4. Điều này phù hợp với các báo cáo khác (OpenAI 2023; Bubeck et al. 2023) rằng GPT-4 vượt trội hơn nhiều trong các nhiệm vụ lý luận và mã hóa. Về mặt định tính, các chương trình được đề xuất bởi GPT-3.5 có sai sót theo vô số cách và thường không xuất hiện "gần". Chúng cũng dường như không cải thiện với gỡ lỗi tự động.

Hiệu quả Dữ liệu. Trong phụ lục, chúng tôi phân tích số lượng nhiệm vụ huấn luyện được sử dụng trong mỗi thử nghiệm thành công. Một nhiệm vụ huấn luyện được sử dụng nếu nó xuất hiện trong lời nhắc và/hoặc kích hoạt phản hồi trong quá trình gỡ lỗi tự động. Vì hai nhiệm vụ huấn luyện luôn được sử dụng trong lời nhắc, số tối thiểu được sử dụng là hai. Thú vị, trong đại đa số các trường hợp, chỉ hai nhiệm vụ huấn luyện đó được sử dụng. Trong quá trình gỡ lỗi tự động, hai nhiệm vụ nhắc nhở này luôn được kiểm tra đầu tiên, và hầu hết thời gian, chúng đủ để xác định các vấn đề. Trong một số lượng nhỏ các trường hợp, một nhiệm vụ thứ ba cũng được sử dụng trong quá trình gỡ lỗi tự động. Kết quả này nói lên khả năng học few-shot mạnh mẽ của GPT-4. Chúng tôi mong đợi rằng trong nhiều trường hợp, ngay cả một nhiệm vụ huấn luyện cũng sẽ đủ, mặc dù chúng tôi đã chứng kiến một sự sụt giảm hiệu suất trong các thí nghiệm sơ bộ với một nhiệm vụ.

--- TRANG 7 ---
Thảo luận và Công việc Tương lai
Trong công trình này, chúng tôi đã cho thấy rằng GPT-4 với tóm tắt CoT và gỡ lỗi tự động là một người lập kế hoạch tổng quát mạnh mẽ một cách đáng ngạc nhiên trong các miền PDDL. Chúng tôi kết thúc với các hạn chế của công trình này, suy ngẫm về ý nghĩa của những phát hiện của chúng tôi, và cơ hội cho công việc tương lai.

Hạn chế. Một hạn chế chính của công trình này và công trình trước đây về lập kế hoạch tổng quát là dễ dàng thiết kế thủ công các kế hoạch tổng quát cho tất cả các miền được xem xét. Tuy nhiên, chúng tôi mong đợi dòng công việc này sẽ hữu ích về mặt thực tế cho ít nhất ba lý do. (1) Trong một số trường hợp, có thể dễ dàng hơn đáng kể để chỉ định các mô tả miền và vấn đề PDDL hơn là trực tiếp chỉ định một kế hoạch tổng quát. (2) Trong một hệ thống hoàn toàn tự động, nơi các toán tử và vị từ được học liên kết với ngôn ngữ tự nhiên, chúng ta sẽ muốn hệ thống cũng tổng hợp các kế hoạch tổng quát một cách tự động. (3) Ngoài PDDL, lập kế hoạch tổng quát với LLM sẽ là một lựa chọn hấp dẫn hơn nữa, vì các phương pháp khác dựa mạnh vào các đặc tả chính thức. Một hạn chế khác của công trình này là việc sử dụng các nhiệm vụ huấn luyện để truyền đạt phân phối nhiệm vụ quan tâm cho LLM. Nói chung, một vài nhiệm vụ ví dụ có thể không đủ để biểu thị phân phối đầy đủ. Các biểu diễn khác như ngôn ngữ tự nhiên hoặc mã tạo thủ tục có thể tốt hơn, nhưng sẽ yêu cầu nhiều đầu vào từ con người hơn.

Lập kế hoạch (tổng quát) bây giờ có lỗi thời không? Không. Đầu tiên, vẫn còn một khoảng cách hiệu suất giữa GPT-4 và PG3, và các bộ lập kế hoạch tổng quát khác có thể thậm chí còn tốt hơn. Tuy nhiên, ngay cả khi khoảng cách này được đóng lại bởi thế hệ LLM tiếp theo, chúng tôi vẫn sẽ nói không. Lập kế hoạch vẫn thiết yếu trong các miền nơi không có chương trình đơn giản nào tồn tại. Một hướng thú vị cho công việc tương lai sẽ là tự động phát hiện liệu một chương trình đơn giản có thể tồn tại trước khi cố gắng tổng hợp một chương trình. Chúng tôi đã thử miền Sokoban và thấy rằng GPT-4 chỉ ra đúng rằng không có chương trình đơn giản nào tồn tại. Tuy nhiên, tính chất này của Sokoban được biết đến rộng rãi, vì vậy có khả năng nó đang lặp lại dữ liệu tiền huấn luyện. Chúng tôi cũng đã thử miền Slitherlink, được đặc trưng trong Cuộc thi Lập kế hoạch Quốc tế 2023, và thấy rằng GPT-4 không nhận ra rằng không có chiến lược đơn giản nào tồn tại (Takayuki 2000). Lập kế hoạch tổng quát không có LLM cũng vẫn quan trọng trong các trường hợp nơi các mô tả miền không thể đọc được bởi con người, ví dụ, vì các vị từ hoặc toán tử được học (Silver et al. 2023). Ngay cả với các mô tả ngôn ngữ tự nhiên, việc kết hợp các phương pháp "cổ điển" với LLM có thể là tốt nhất.

Điều gì nếu chúng ta cho LLM truy cập vào một bộ lập kế hoạch? Cho một LLM truy cập vào các API là một ý tưởng rất mạnh mẽ (Schick et al. 2023) và một API như vậy có thể là một bộ lập kế hoạch PDDL (Liu et al. 2023). Một LLM có thể sử dụng một bộ lập kế hoạch như vậy cho lập kế hoạch tổng quát, đặc biệt cho rằng các phương pháp như PG3 dựa vào truy cập vào một bộ lập kế hoạch để tạo ra các kế hoạch ví dụ. Trong một số miền, việc tạo ra các kế hoạch ví dụ một cách ngây thơ có thể làm bối rối LLM. Ví dụ, các kế hoạch được tạo ra trong miền Forest sẽ theo các con đường tùy ý qua đất thay vì theo con đường được đánh dấu dài hơn một chút. Trong các trường hợp khác, tuy nhiên, các kế hoạch ví dụ có thể rất hữu ích, đặc biệt nếu LLM tạo ra chúng theo cách có mục tiêu. Tận dụng các kế hoạch đa dạng (Sohrabi et al. 2016; Katz và Sohrabi 2020) có thể đặc biệt hữu ích.

--- TRANG 8 ---
Tài liệu Tham khảo
Ahn, M.; Brohan, A.; Brown, N.; Chebotar, Y.; Cortes,
O.; David, B.; Finn, C.; Gopalakrishnan, K.; Hausman,
K.; Herzog, A.; et al. 2022. Do as I can, not as I say:
Grounding language in robotic affordances. arXiv preprint
arXiv:2204.01691.
Alur, R.; Bodik, R.; Juniwal, G.; Martin, M. M.;
Raghothaman, M.; Seshia, S. A.; Singh, R.; Solar-Lezama,
A.; Torlak, E.; và Udupa, A. 2013. Syntax-guided synthesis. IEEE.
Bonet, B.; và Geffner, H. 2015. Policies that generalize:
Solving many planning problems with the same policy. In
Twenty-Fourth International Joint Conference on Artificial
Intelligence.
Bonet, B.; và Geffner, H. 2018. Features, Projections, and
Representation Change for Generalized Planning. CoRR,
abs/1801.10055.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–
1901.
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;
Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg,
S.; et al. 2023. Sparks of artificial general intelligence: Early
experiments with gpt-4. arXiv preprint arXiv:2303.12712.
Chapman, D. 1987. Planning for Conjunctive Goals. Artificial Intelligence, 32: 333–377.
Chen, A.; Scheurer, J.; Korbak, T.; Campos, J. A.; Chan,
J. S.; Bowman, S. R.; Cho, K.; và Perez, E. 2023a. Improving Code Generation by Training with Natural Language
Feedback. arXiv:2303.16749.
Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;
Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman,
G.; et al. 2021. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374.
Chen, X.; Lin, M.; Schärli, N.; và Zhou, D. 2023b. Teaching large language models to self-debug. arXiv preprint
arXiv:2304.05128.
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;
Gehrmann, S.; et al. 2022. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311.
Collins, K. M.; Wong, C.; Feng, J.; Wei, M.; và Tenenbaum, J. B. 2022. Structured, flexible, and robust: benchmarking and improving large language models towards more
human-like behavior in out-of-distribution reasoning tasks.
arXiv preprint arXiv:2205.05718.
Cropper, A.; và Dumančić, S. 2022. Inductive logic programming at 30: a new introduction. Journal of Artificial
Intelligence Research, 74: 765–850.
Fikes, R. E.; Hart, P. E.; và Nilsson, N. J. 1972. Learning and executing generalized robot plans. Artificial intelligence, 3: 251–288.

Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.;
Callan, J.; và Neubig, G. 2022. PAL: Program-aided Language Models. arXiv preprint arXiv:2211.10435.
Gulwani, S.; Polozov, O.; Singh, R.; et al. 2017. Program
synthesis. Foundations and Trends® in Programming Languages, 4(1-2): 1–119.
Helmert, M. 2006. The fast downward planning system.
Journal of Artificial Intelligence Research, 26: 191–246.
Howey, R.; Long, D.; và Fox, M. 2004. VAL: Automatic
plan validation, continuous effects and mixed initiative planning using PDDL. In 16th IEEE International Conference
on Tools with Artificial Intelligence, 294–301. IEEE.
Huang, W.; Abbeel, P.; Pathak, D.; và Mordatch, I. 2022a.
Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. In International
Conference on Machine Learning (ICML).
Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar,
Y.; et al. 2022b. Inner Monologue: Embodied Reasoning
through Planning with Language Models. arXiv preprint
arXiv:2207.05608.
Imani, S.; Du, L.; và Shrivastava, H. 2023. MathPrompter:
Mathematical Reasoning using Large Language Models.
arXiv:2303.05398.
Jiang, X.; Dong, Y.; Wang, L.; Shang, Q.; và Li, G.
2023. Self-planning Code Generation with Large Language
Model. arXiv preprint arXiv:2303.06689.
Jiménez, S.; và Jonsson, A. 2015. Computing plans with
control flow and procedures using a classical planner. In
Proceedings of the Eighth Annual Symposium on Combinatorial Search, SOCS-15, 62–69.
Jiménez, S.; Segovia-Aguas, J.; và Jonsson, A. 2019. A review of generalized planning. The Knowledge Engineering
Review, 34.
Katz, M.; và Sohrabi, S. 2020. Reshaping diverse planning. In Proceedings of the AAAI Conference on Artificial
Intelligence, 06, 9892–9899.
Levesque, H. 2005. Planning with Loops. In IJCAI.
Levine, J.; và Humphreys, D. 2003. Learning action strategies for planning domains using genetic programming. In
Workshops on Applications of Evolutionary Computation,
684–695. Springer.
Liang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,
B.; Florence, P.; và Zeng, A. 2022. Code as policies: Language model programs for embodied control. arXiv preprint
arXiv:2209.07753.
Lin, B. Y.; Huang, C.; Liu, Q.; Gu, W.; Sommerer, S.; và
Ren, X. 2022. On Grounded Planning for Embodied Tasks
with Language Models. arXiv preprint arXiv:2209.00465.
Lin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; và Bohg, J.
2023. Text2motion: From natural language instructions to
feasible plans. arXiv preprint arXiv:2303.12153.
Liu, B.; Jiang, Y.; Zhang, X.; Liu, Q.; Zhang, S.; Biswas, J.;
và Stone, P. 2023. LLM+P: Empowering Large Language
Models with Optimal Planning Proficiency. arXiv preprint
arXiv:2304.11477.

--- TRANG 9 ---
McDermott, D. 2000. The 1998 AI Planning Systems Competition. AI Magazine, 21(2): 35–55.
Muggleton, S. 1991. Inductive logic programming. New
generation computing, 8: 295–318.
Nijkamp, E.; Pang, B.; Hayashi, H.; Tu, L.; Wang, H.; Zhou,
Y.; Savarese, S.; và Xiong, C. 2023. CodeGen: An Open
Large Language Model for Code with Multi-Turn Program
Synthesis. arXiv:2203.13474.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Pallagani, V.; Muppasani, B.; Murugesan, K.; Rossi, F.;
Horesh, L.; Srivastava, B.; Fabiano, F.; và Loreggia, A.
2022. Plansformer: Generating Symbolic Plans using Transformers. arXiv preprint arXiv:2212.08681.
Raman, S. S.; Cohen, V.; Rosen, E.; Idrees, I.; Paulius,
D.; và Tellex, S. 2022. Planning with Large Language Models via Corrective Re-prompting. arXiv preprint
arXiv:2211.09935.
Richter, S.; và Westphal, M. 2010. The LAMA planner:
Guiding Cost-based Anytime Planning with Landmarks.
Journal of Artificial Intelligence Research, 39: 127–177.
Rivlin, O.; Hazan, T.; và Karpas, E. 2020. Generalized Planning With Deep Reinforcement Learning. arXiv
preprint arXiv:2005.02305.
Schaul, T.; Horgan, D.; Gregor, K.; và Silver, D. 2015. Universal value function approximators. In International conference on machine learning, 1312–1320. PMLR.
Schick, T.; Dwivedi-Yu, J.; Dessì, R.; Raileanu, R.; Lomeli,
M.; Zettlemoyer, L.; Cancedda, N.; và Scialom, T. 2023.
Toolformer: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761.
Segovia-Aguas, J.; Jiménez, S.; và Jonsson, A. 2018. Computing hierarchical finite state controllers with classical
planning. Journal of Artificial Intelligence Research, 62:
755–797.
Segovia-Aguas, J.; Jiménez, S.; và Jonsson, A. 2021. Generalized Planning as Heuristic Search. In Proceedings of
the International Conference on Automated Planning and
Scheduling, volume 31, 569–577.
Sharma, P.; Torralba, A.; và Andreas, J. 2022. Skill Induction and Planning with Latent Language. In Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1713–1726.
Silver, T.; Chitnis, R.; Kumar, N.; McClinton, W.; LozanoPerez, T.; Kaelbling, L. P.; và Tenenbaum, J. 2023. Predicate Invention for Bilevel Planning. In AAAI Conference on
Artificial Intelligence (AAAI).
Silver, T.; Hariprasad, V.; Shuttleworth, R. S.; Kumar, N.;
Lozano-Pérez, T.; và Kaelbling, L. P. 2022. PDDL Planning with Pretrained Large Language Models. In NeurIPS
2022 Foundation Models for Decision Making Workshop.
Singh, I.; Blukis, V.; Mousavian, A.; Goyal, A.; Xu, D.;
Tremblay, J.; Fox, D.; Thomason, J.; và Garg, A. 2022.
Progprompt: Generating situated robot task plans using large
language models. arXiv preprint arXiv:2209.11302.

Sohrabi, S.; Riabov, A. V.; Udrea, O.; và Hassanzadeh, O.
2016. Finding diverse high-quality plans for hypothesis generation. In ECAI 2016, 1581–1582. IOS Press.
Srivastava, S. 2011. Foundations and applications of generalized planning. AI Communications, 24(4): 349–351.
Srivastava, S.; Immerman, N.; Zilberstein, S.; và Zhang, T.
2011. Directed Search for Generalized Plans Using Classical Planners. In ICAPS.
Sutton, R. S.; Modayil, J.; Delp, M.; Degris, T.; Pilarski,
P. M.; White, A.; và Precup, D. 2011. Horde: A scalable
real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In The 10th International
Conference on Autonomous Agents and Multiagent SystemsVolume 2, 761–768.
Takayuki, Y. 2000. On the NP-completeness of the Slither
Link puzzle. IPSJ SIGNotes ALgorithms.
Valmeekam, K.; Olmo, A.; Sreedharan, S.; và Kambhampati, S. 2022. Large Language Models Still Can't Plan
(A Benchmark for LLMs on Planning and Reasoning about
Change). arXiv preprint arXiv:2206.10498.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;
Le, Q.; và Zhou, D. 2022. Chain of thought prompting
elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903.
Winner, E. Z. 2008. Learning Domain-Specific Planners
from Example Plans. Ph.D. thesis, Carnegie Mellon University, USA.
Xia, C. S.; và Zhang, L. 2023. Conversational Automated
Program Repair. arXiv:2301.13246.
Xie, Y.; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; và Soh, H. 2023.
Translating natural language to planning goals with largelanguage models. arXiv preprint arXiv:2302.05128.
Yang, R.; Silver, T.; Curtis, A.; Lozano-Perez, T.; và Kaelbling, L. P. 2022. PG3: Policy-Guided Planning for Generalized Policy Generation. In IJCAI.
Zheng, W.; Sharan, S.; Jaiswal, A. K.; Wang, K.; Xi, Y.;
Xu, D.; và Wang, Z. 2023. Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. arXiv
preprint arXiv:2305.00909.

--- TRANG 10 ---
Kết quả Bổ sung
Bảng 3 báo cáo tỷ lệ tối đa nhiệm vụ đánh giá được giải quyết qua các hạt giống. Trong hầu hết các trường hợp, LLM hoặc giải quyết tất cả hoặc không có nhiệm vụ đánh giá nào. Điều này cho thấy rằng LLM không quá khớp với các nhiệm vụ huấn luyện, ngay cả khi chỉ một số rất nhỏ trong số chúng được sử dụng (xem văn bản chính).

Hình 4 cho thấy số lượng nhiệm vụ huấn luyện được sử dụng trong các thử nghiệm thành công. Xem văn bản chính để biết chi tiết. Trong đại đa số các trường hợp, chỉ hai nhiệm vụ là cần thiết, cho thấy hiệu quả dữ liệu mạnh mẽ của GPT-4 như một người lập kế hoạch tổng quát.

Miền | GPT-4 | Không CoT | Không Debug | Không Tên | GPT-3.5
Delivery | 1.00 | 1.00 | 1.00 | 1.00 | 0.00
Forest | 1.00 | 1.00 | 1.00 | 0.93 | 1.00
Gripper | 1.00 | 1.00 | 1.00 | 1.00 | 0.00
Miconic | 0.07 | 0.00 | 0.00 | 0.00 | 0.00
Ferry | 1.00 | 1.00 | 1.00 | 0.00 | 0.00
Spanner | 1.00 | 1.00 | 0.00 | 0.00 | 0.00
Heavy | 1.00 | 1.00 | 1.00 | 0.00 | 0.00

Bảng 3: Tỷ lệ tối đa nhiệm vụ đánh giá được giải quyết trên mỗi hạt giống.

Hình 4: Hiệu quả dữ liệu của GPT-4. Các tỷ lệ được báo cáo là qua tất cả các thử nghiệm, qua tất cả các miền.

Các Chương trình Ví dụ
Các ví dụ về chương trình được tổng hợp bởi GPT-4 với tóm tắt CoT và gỡ lỗi tự động được hiển thị trong Listings 1, 2, 3, 4, 5, 6, và 7. Bất kỳ bình luận nào trong mã đều được thêm bởi chính GPT-4.

--- TRANG 11 ---
Listing 1: Ví dụ chương trình được tổng hợp miền Delivery (thành công)
def get_plan(objects, init, goal):
    locs = [obj[0] for obj in objects if obj[1] == "loc"]
    papers = [obj[0] for obj in objects if obj[1] == "paper"]
    home_base = next(loc for loc in locs if ("ishomebase", loc) in init)
    safe_locs = [loc for loc in locs if ("safe", loc) in init]
    wants_paper = [loc for loc in locs if ("wantspaper", loc) in init]
    
    plan = []
    for loc in wants_paper:
        if loc in safe_locs:
            # Nhặt một tờ báo tại căn cứ
            paper = papers.pop()
            plan.append(f"(pick-up {paper} {home_base})")
            # Di chuyển đến địa điểm an toàn muốn báo
            plan.append(f"(move {home_base} {loc})")
            # Giao báo đến địa điểm
            plan.append(f"(deliver {paper} {loc})")
            # Di chuyển về căn cứ
            plan.append(f"(move {loc} {home_base})")
    
    return plan

--- TRANG 12 ---
Listing 2: Ví dụ chương trình được tổng hợp miền Forest (thành công)
def get_plan(objects, init, goal):
    def find_location(init):
        for atom in init:
            if atom[0] == "at":
                return atom[1]
        return None
    
    def find_adjacent_trail(current_loc, init):
        for atom in init:
            if atom[0] == "ontrail" and atom[1] == current_loc:
                return atom[2]
        return None
    
    def is_hill(loc, init):
        for atom in init:
            if atom[0] == "ishill" and atom[1] == loc:
                return True
        return False
    
    current_loc = find_location(init)
    goal_loc = find_location(goal)
    plan = []
    
    while current_loc != goal_loc:
        next_loc = find_adjacent_trail(current_loc, init)
        if is_hill(next_loc, init):
            action = f"(climb {current_loc} {next_loc})"
        else:
            action = f"(walk {current_loc} {next_loc})"
        plan.append(action)
        current_loc = next_loc
    
    return plan

--- TRANG 13 ---
Listing 3: Ví dụ chương trình được tổng hợp miền Gripper (thành công)
def get_plan(objects, init, goal):
    def get_objects_of_type(type_name):
        return [obj for obj in objects if obj.startswith(type_name)]
    
    def get_location(state, ball):
        for atom in state:
            if atom[0] == 'at' and atom[1] == ball:
                return atom[2]
        return None
    
    balls = get_objects_of_type('ball')
    rooms = get_objects_of_type('room')
    grippers = get_objects_of_type('gripper')
    
    robby_location = None
    for atom in init:
        if atom[0] == 'at-robby':
            robby_location = atom[1]
            break
    
    plan = []
    for ball in balls:
        initial_location = get_location(init, ball)
        goal_location = get_location(goal, ball)
        
        if goal_location is None:
            continue
        
        if initial_location != goal_location:
            plan.append(f'(move {robby_location} {initial_location})')
            robby_location = initial_location
            plan.append(f'(pick {ball} {initial_location} {grippers[0]})')
            plan.append(f'(move {robby_location} {goal_location})')
            robby_location = goal_location
            plan.append(f'(drop {ball} {goal_location} {grippers[0]})')
    
    return plan

--- TRANG 14 ---
Listing 4: Ví dụ chương trình được tổng hợp miền Miconic (thất bại)
def get_plan(objects, init, goal):
    floors_by_building = {}
    passengers_by_building = {}
    
    for obj, obj_type in objects:
        if obj_type == "floor":
            building = obj.split("_")[-1]
            if building not in floors_by_building:
                floors_by_building[building] = []
            floors_by_building[building].append(obj)
        elif obj_type == "passenger":
            building = obj.split("_")[-1]
            if building not in passengers_by_building:
                passengers_by_building[building] = []
            passengers_by_building[building].append(obj)
    
    for building in floors_by_building:
        floors_by_building[building] = sorted(floors_by_building[building])
    
    origins = {p: f for p, f in (atom[1:] for atom in init if atom[0] == "origin")}
    destins = {p: f for p, f in (atom[1:] for atom in init if atom[0] == "destin")}
    lift_at = {atom[1]: atom for atom in init if atom[0] == "lift-at"}
    
    plan = []
    boarded = set()
    
    for building, floors in floors_by_building.items():
        passengers = passengers_by_building[building]
        current_lift_at = lift_at[building]
        
        for floor in floors + list(reversed(floors)):
            if current_lift_at != floor:
                if current_lift_at < floor:
                    plan.append(f"(up {current_lift_at} {floor})")
                else:
                    plan.append(f"(down {current_lift_at} {floor})")
                current_lift_at = floor
            
            for passenger in passengers:
                if origins.get(passenger) == floor:
                    plan.append(f"(board {floor} {passenger})")
                    boarded.add(passenger)
                if destins.get(passenger) == floor and passenger in boarded:
                    plan.append(f"(depart {floor} {passenger})")
                    boarded.remove(passenger)
    
    return plan

--- TRANG 15 ---
Listing 5: Ví dụ chương trình được tổng hợp miền Ferry (thành công)
def get_plan(objects, init, goal):
    plan = []
    
    # Hàm trợ giúp để tìm vị trí của xe
    def get_location(car, state):
        for atom in state:
            if atom[0] == 'at' and atom[1] == car:
                return atom[2]
        return None
    
    cars = [obj for obj in objects if obj.startswith('c')]
    ferry_location = [atom[1] for atom in init if atom[0] == 'at-ferry'][0]
    
    for car in cars:
        initial_location = get_location(car, init)
        goal_location = get_location(car, goal)
        
        # Đi phà đến vị trí ban đầu của xe
        if ferry_location != initial_location:
            plan.append(f'(sail {ferry_location} {initial_location})')
            ferry_location = initial_location
        
        # Đưa xe lên phà
        plan.append(f'(board {car} {initial_location})')
        
        # Đi phà đến vị trí mục tiêu của xe
        if ferry_location != goal_location:
            plan.append(f'(sail {ferry_location} {goal_location})')
            ferry_location = goal_location
        
        # Đưa xe xuống phà tại vị trí mục tiêu
        plan.append(f'(debark {car} {goal_location})')
    
    return plan

--- TRANG 16 ---
Listing 6: Ví dụ chương trình được tổng hợp miền Spanner (thất bại)
def get_plan(objects, init, goal):
    plan = []
    bob_location = get_bob_location(init)
    spanner_locations = get_spanner_locations(init)
    nuts_to_tighten = {n for pred, *args in goal if pred == 'tightened' for n in args}
    
    while nuts_to_tighten:
        closest_spanner = None
        min_distance = float('inf')
        
        for loc, spanner in spanner_locations.items():
            distance = get_shortest_path(init, bob_location, loc)
            if distance < min_distance:
                min_distance = distance
                closest_spanner = spanner
                closest_spanner_loc = loc
        
        # Di chuyển Bob đến spanner gần nhất
        path_to_spanner = get_path(init, bob_location, closest_spanner_loc)
        for next_location in path_to_spanner:
            plan.append(f'(walk {bob_location} {next_location} bob)')
            bob_location = next_location
        
        # Nhặt spanner
        plan.append(f'(pickup_spanner {closest_spanner_loc} {closest_spanner} bob)')
        
        # Di chuyển Bob đến cổng
        gate = 'gate'
        path_to_gate = get_path(init, bob_location, gate)
        for next_location in path_to_gate:
            plan.append(f'(walk {bob_location} {next_location} bob)')
            bob_location = next_location
        
        # Vặn chặt đai ốc bằng spanner
        nut = nuts_to_tighten.pop()
        plan.append(f'(tighten_nut {gate} {closest_spanner} bob {nut})')
        
        # Xóa spanner đã sử dụng khỏi các spanner có sẵn
        if closest_spanner_loc in spanner_locations:
            del spanner_locations[closest_spanner_loc]
    
    return plan

def get_path(init, start, end):
    paths = [start]
    while paths[-1] != end:
        for pred, l1, l2 in init:
            if pred == 'link' and paths[-1] == l1 and l2 not in paths:
                paths.append(l2)
                break
    return paths[1:]

--- TRANG 17 ---
Listing 7: Ví dụ chương trình được tổng hợp miền Heavy (thành công)
def extract_heavier_relations(init):
    heavier_relations = set()
    for atom in init:
        if atom[0] == 'heavier':
            heavier_relations.add(atom)
    return heavier_relations

def sort_objects_by_weight(objects, heavier_relations):
    sorted_objects = []
    while len(objects) > 0:
        for obj in objects.copy():
            if all(('heavier', other, obj) not in heavier_relations for other in objects):
                sorted_objects.append(obj)
                objects.remove(obj)
    return sorted_objects

def get_plan(objects, init, goal):
    heavier_relations = extract_heavier_relations(init)
    sorted_objects = sort_objects_by_weight(set(objects), heavier_relations)
    
    plan = []
    first_object = sorted_objects.pop(0)
    plan.append(f"(pack-first {first_object})")
    
    for obj in sorted_objects:
        plan.append(f"(stack {first_object} {obj})")
        first_object = obj
    
    return plan
