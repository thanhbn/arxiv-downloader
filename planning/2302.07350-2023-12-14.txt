# 2302.07350.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/planning/2302.07350.pdf
# File size: 12238812 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
2023-12-14
Graph schemas as abstractions for transfer
learning, inference, and planning
J. Swaroop Guntupalli1, Rajkumar Vasudeva Raju1, Shrinu Kushagra1, Carter Wendelken1, Danny Sawyer1,
Ishan Deshpande2, Guangyao Zhou1, Miguel Lázaro-Gredilla1and Dileep George1
1Google DeepMind,2Google Research
Transferring latent structure from one environment or problem to another is a mechanism by which
humans and animals generalize with very little data. Inspired by cognitive and neurobiological insights,
we propose graph schemas as a mechanism of abstraction for transfer learning. Graph schemas start
with latent graph learning where perceptually aliased observations are disambiguated in the latent space
using contextual information. Latent graph learning is also emerging as a new computational model of
the hippocampus to explain map learning and transitive inference. Our insight is that a latent graph
can be treated as a flexible template — a schema — that models concepts and behaviors, with slots that
bind groups of latent nodes to the specific observations or groundings. By treating learned latent graphs
(schemas) as prior knowledge, new environments can be quickly learned as compositions of schemas
and their newly learned bindings. We evaluate graph schemas on two previously published challenging
tasks: the memory & planning game and one-shot StreetLearn, which are designed to test rapid task
solving in novel environments. Graph schemas can be learned in far fewer episodes than previous
baselines, and can model and plan in a few steps in novel variations of these tasks. We also demonstrate
learning, matching, and reusing graph schemas in more challenging 2D and 3D environments with
extensive perceptual aliasing and size variations, and show how different schemas can be composed to
model larger and more complex environments. To summarize, our main contribution is a unified system,
inspired and grounded in cognitive science, that facilitates rapid transfer learning of new environments
using schemas via map-induction and composition that handles perceptual aliasing.
1. Introduction
Discovering and using the right abstractions in new situations affords efficient transfer learning
as well as quick inference and planning. Humans excel at this ability, argued to be a key factor
behind intelligence and a fundamental limitation in current AI systems (Shanahan and Mitchell,
2022). Common reusable structured representations of concepts or behaviors—schemas—have been
proposed as a powerful way to encode abstractions (Mitchell, 2021; Tenenbaum et al., 2011). Having
a computational model with the ability to discover and reuse previously-learned schemas to behave
and plan in novel situations will be essential for AI.
Experimental evidence suggests that several animals have this ability (Farzanfar et al., 2023).
Rats and mice tend to learn new environments faster if they can reuse past schemas (Tse et al.,
2007; Zhou et al., 2021), and macaque hippocampus cells encode spatial schemas (Baraduc et al.,
2019). Neural circuits in the hippocampus and prefrontal cortex (PFC) are implicated in schema
learning, recognition, update, and maintenance and these processes are considered as the foundation
of memory consolidation (Gilboa and Marlatte, 2017; Preston and Eichenbaum, 2013; Samborska
et al., 2022). New experiences are learned in a single trial if it fits an existing schema. An updated
complementary learning systems theory based on this evidence was proposed in Kumaran et al.
(2016), but there is so far no explicit demonstration of such rapid learning with schema reuse as far
as we know.
Corresponding author(s): dileepgeorge@deepmind.comarXiv:2302.07350v2  [cs.AI]  13 Dec 2023

--- PAGE 2 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Structured relational representations have been proposed as a common mechanism in the hip-
pocampusthatreconcilesspatialandnon-spatialtasksandmemoryintotheoriginalcognitivemapping
view (Eichenbaum and Cohen, 2014; Stachenfeld et al., 2017). Several recent studies model cognitive
maps as higher order latent graph structures and show generalization to disparate functions impli-
cated in the hippocampus (George et al., 2021; Raju et al., 2022; Sharma et al., 2021; Whittington
et al., 2020, 2021). We take one such model of cognitive maps, the clone-structured cognitive graph
(CSCG)(George et al., 2021), and extend it to provide a concrete computational model of abstractions
using graph schemas. Desiderata for our model is a unified system that facilitates learning new
environments by using schemas, handles perceptual aliasing (Whitehead and Ballard, 1991), and
generalizes via map induction (Sharma et al., 2021) and schema composition.
Our setup is an agent navigating in an environment modeled as a directed graph. The agent
observes the emissions at its current node and traverses to new nodes via edges with labeled actions.
Multiple nodes may emit the same observations (i.e., they are aliased (Lajoie et al., 2019; Whitehead
and Ballard, 1991)), so the agent cannot observe the state directly. This can be considered as a
discrete time partially observable Markov decision process (POMDP). As the agent navigates a new
environment, our goal is to learn the underlying latent graph (i.e., map induction Sharma et al.
(2021)) and to do so rapidly by reusing previously learnt graph topologies or graph schemas. We
choose the CSCG model to construct graph schemas as it has been shown to learn higher-order
graphs in highly aliased settings using a smooth, probabilistic, parameterization of the graph learning
problem (George et al., 2021). We extend this model to describe how learned graphs can be reused as
schemas for transfer learning, quick inference, and planning for behavior in new situations by rapidly
learning observation bindings and discovering the best schema online.
2. Related Work
Research on rapid transfer to new tasks in novel environments has focused on different aspects, from
exploration to modeling and planning. Some of the recent work has been predominantly done in a
reinforcement learning (RL) framework, and different RL approaches focus on one or more of these
aspects.
Meta-RL Model-free meta-RL approaches focus on exploration policy generalization to new tasks
and environmental variations without explicit model learning. Some show generalization to new tasks
in a known environment, but not to new environments nor do they handle aliasing (Rakelly et al.,
2019; Wang et al., 2016). Recurrent model-free RL has shown some generalization to environmental
variations and in the POMDP settings (Ni et al., 2021). These variations are of parameters for
generating the environment, and the goal is to be able learn an exploration policy that generalizes to
these parametric variations. These methods do not re-use explicit knowledge of past environments
to model new environments nor do they handle composition of known environments (Packer et al.,
2018). More recent works added episodic memory with attention heads to selectively attend and
reusestoredmemoriestorapidlyadapttotasksinnewenvironmentsinfew-shotsettings(e.g. episodic
planning network)(Lampinen et al., 2021; Ritter et al., 2020). These models match optimal planning
only after trainingfor billions ofsteps. Further, none of these approaches handlealiasing, nor explicitly
build models of the environment and plan over them. Gupta et al. (2017) show that using explicit
model building (mapping) and navigation via planning in spatial environments can handle partial
observability and outperform methods without this ability, but this study does not reuse those models
to rapidly learn novel environments.
Model based RL Model based RL works learn an explicit model of the environment (Gregor et al.,
2019), some even under a POMDP setup (Igl et al., 2018), and can transfer to new tasks within that
2

--- PAGE 3 ---
Graph schemas as abstractions for transfer learning, inference, and planning
environment, but cannot transfer to novel environments. Some studies use a version of sub-goal
or landmark discovery and reuse to generalize to novel tasks (Eysenbach et al., 2019; Kipf et al.,
2019; Zhang et al., 2021). However, these approaches do not address partial observability, and more
importantly the generalization is restricted to the same environments. Semi-parametric topological
memory is a method that extends landmark based navigation to generalize to novel environments
(Savinov et al., 2018). This method trains a network to estimate connectivity between observations at
different locations and uses this to build a graph of observations in a novel environment. This method
does generalize to novel environments, but its performance degrades significantly with increasing
aliasing. Further, a human-generated exploration path was used for constructing the graph.
Our work on schema matching and reuse is related to finding correspondence between graphs in
different contexts. The first neural network approach to structure mapping was proposed in Crouse
etal.(2021). Butthisapproachisrestrictedtothematchingproblemandhasnomechanismtoresolve
or learn new structures or to plan with partially matching schemas. Another line of related work
focused on solving simplified relational tasks inspired by Raven’s Progressive Matrices (Kerg et al.,
2022; Webb et al., 2021). The main idea is to separate abstract relations from sensory observations
during training, and learn the observation mapping to solve new tasks with the same relations but
novel mapping, but in a deterministic and simplified setting.
The main contribution of this paper is a unified system that (i) facilitates rapid transfer learning of
new environments using schemas via map-induction and composition, (ii) handles perceptual aliasing,
(iii) builds explicit latent graph models of environments directly from actions and observations, (iv)
supports planning and inference, and (v) and is inspired and grounded in cognitive and neurosciences.
Ours is the first model to combine all these aspects in a single system.
3. Methods
3.1. Problem Setup
Consider an agent navigating in a directed graph 𝐺. When the agent visits a node in the graph, the
node emits an observation. However, multiple nodes may emit the same observation (i.e., they are
aliased), so the observation is not enough to disambiguate where in the graph the agent is located.
Additionally, actions do not have deterministic results—executing the same action at the same node
might result in the agent navigating to different nodes. The outgoing edges from a node are labeled
with the action that would traverse them, and with the probability of traversing them under that
action. The sum of the probabilities of all outgoing edges from a node with the same action label sum
up to 1. We use the graph 𝐺to model the agent’s environment.
When the agent performs a sequence of actions 𝑎1,...,𝑎𝑁(with discrete 𝑎𝑛∈{1,...𝑁actions}), it
receives a sequence of observations 𝑥1,...,𝑥𝑁(with each 𝑥𝑛∈ℝ𝑑or{1,...,𝑁 obs}for continuous and
discrete observations, respectively). The goal of learning is to recover the topology of the environment
𝐺from sequences of actions and observations. The goal of transfer is to reuse the topology of a
previously learned graph to model a new environment by relabeling the nodes with new observations.
In both cases, the learned graphs can be exploited for tasks such as goal-directed navigation.
3.2. Model
Clone-structured cognitive graphs (CSCGs) were introduced in Dedieu et al. (2019); George et al.
(2021) to recover (an approximation of) the graph 𝐺from sequences of action-observation pairs. To
do this, they use categorical hidden variables 𝑧1,...,𝑧𝑁to model the node of the graph that the agent
is at in each time step. With this, it is possible to formulate a graphical model for the sequence of
3

--- PAGE 4 ---
Graph schemas as abstractions for transfer learning, inference, and planning
isometric view
of 3D environment
agent
 egocentric
f.o.v
observationsxn+1 xn
an
Vector
QuantizerVector
Quantizer
yn yn+1
A B 
zn zn+1
yn yn+1
xn xn+1anC  
CSCG
learningtransition graph
with emission bindingD 
node: latent state
node color: emission bindingx1, ..., xN
a1,...., aN -1
E  Schema extraction from multiple experiences
F  Transfer to a new environment
transition graph with
emission bindingenvironment schemas
matched schema
with
learned emissions
1
2
HCSCG
learning
CSCG
learning
CSCG
learningemission
binding
removed
1 2              H
schema index hnegative log likelihood of 
observations under schema h
larger room, new floor color, asymmetric barrieraction
latent
variable
discrete
emission
observation
Figure 1|Overview of using graph schemas in simulated 3D environments. A. An example 3D
environment. B. The agent navigates the environment with egocentric actions and gets observations
as RGB images. The images are passed through a vector quantizer to obtain cluster indices, which are
used as observations for training a CSCG. C. Graphical model of continuous extension of CSCG with
action conditioning. D. Given a sequence of actions and observations, a CSCG learns the transition
graph of the environment with the corresponding emission binding. We use simplified versions of
the transition graphs in this schematic. See Fig. S1 for real graphs. E. Schemas are extracted from
learned models with transitions and emissions by unbinding the specific emissions (represented here
as floor colors in the environment, and node colors in the corresponding graph), but retaining the
clone structure (shown as node patterns). F. In a novel environment with size, color, and structure
variations, the agent navigates and finds the schema that best explains its observations by learning
new emission bindings.
observations given the actions. Here, we use the conditional version of their model and extend it to
continuous observations:
𝑝(𝒙|𝒂)=∑︁
𝒛∑︁
𝒚𝑃(𝑧1)𝑁Ö
𝑛=2𝑃(𝑧𝑛|𝑧𝑛−1,𝑎𝑛−1)𝑁Ö
𝑛=1𝑃(𝑦𝑛|𝑧𝑛)𝑝(𝑥𝑛|𝑦𝑛), (1)
with𝑝(𝑥𝑛|𝑦𝑛)=N(𝑥𝑛|𝜇𝑦𝑛,𝜎2𝐼). We use the following shorthand for a sequence of actions: 𝒂≡
{𝑎1...,𝑎𝑁}(𝒙,𝒚,𝒛are similarly defined). The transitions are fully parameterized through an action-
conditional transition tensor 𝑇with elements 𝑇𝑖𝑗𝑘=𝑃(𝑧𝑛=𝑘|𝑧𝑛−1=𝑗,𝑎𝑛−1=𝑖).
To extend CSCG to continuous observations, we introduced a new variable 𝑦𝑛between the
hidden state 𝑧𝑛and the observation 𝑥𝑛. In this formulation, the observation model is parameterized
as an isotropic Gaussian with variance 𝜎2and mean 𝜇𝑦𝑛, which is the centroid associated to a
4

--- PAGE 5 ---
Graph schemas as abstractions for transfer learning, inference, and planning
discrete emission 𝑦𝑛. The emission model is parameterized by an emission matrix 𝐸with elements
𝐸𝑖𝑗=𝑃(𝑦𝑛=𝑗|𝑧𝑛=𝑖). In a CSCG, by design, multiple hidden states are forced to share the same
emission: if states 𝑖and𝑗areclonesof the same emission, then 𝑝(𝑦|𝑧𝑖)=𝑝(𝑦|𝑧𝑗). Also, the emissions
are deterministic: each row in 𝐸has a single entry set to 1, with the remaining elements set to
zero. Cloned rows (corresponding to the same emission) will be identical. The idea is that different
latent states can produce similar visible observations, and the latent state can only be disambiguated
through temporal context. We define 𝐶(𝑥𝑛)as the set of clones of the emission 𝑥𝑛.
In a simple case, if we have a finite number of potential observations, we can allocate a centroid
to each of them and set 𝜎2→0. When the observations are discrete, the observation model becomes
deterministic and we can set 𝑦=𝑥. This exactly recovers the discrete CSCG from George et al. (2021).
To learn the discrete CSCG, the clone structure is decided a priori by choosing how many clones to
assign to each observation (e.g., a constant number). This fully defines 𝐸. Then, learning a graph 𝐺
that describes a new environment only involves learning 𝑇, i.e. maximizing 𝑃(𝒙|𝒂;𝑇,𝐸)w.r.t.𝑇. This
can be done via expectation-maximization (EM), see Suppl. §A.1. The procedure is analogous to
HMM learning, but computationally more efficient by leveraging the sparsity pattern of the emission
matrix.
For continuous observations, see Suppl. §A.3 for a description of how to learn the centroids 𝜇𝑦
and allocate hidden states to them (i.e., assigning the clone structure).
3.3. Schemas for transfer learning
We call a learned model with fixed 3-tuple 𝑆𝑔≡{𝑇,𝐶,𝐸}agrounded schema . Transfer learning
between different environments that share the same graph topology but with different emissions
associated to its nodes is possible by reusing the previously learned transitions 𝑇. We can maximize
𝑝(𝒙|𝒂;𝑇,𝝁)w.r.t. 𝝁(or𝑝(𝒙|𝒂;𝑇,𝐸)w.r.t.𝐸inthediscretecase), whilekeeping 𝑇fixedtoitsknownvalue
from the previous environment (see Suppl. §A.2 and §A.4 for discrete and continuous observations
respectively). We call this 𝑇without an associated clone structure 𝐶and the emission matrix 𝐸as an
ungrounded schema or simply a schema.
Further, if we know that the new environment also preserves the clone structure 𝐶(when two
nodes were clones in the original environment, they are also clones in the new environment), then
we can further restrict the observation model to respect this constraint during learning and reuse the
tuple𝑆𝑢≡{𝑇,𝐶}and learn 𝝁or𝐸. We call this tuple 𝑆𝑢anungrounded schema with clone structure
(Fig. 1E).
For example, in the room navigation setting, a schema models the agent’s location and head-
directions in a room and how actions move the agent, as well as the knowledge that the floor or door
can look the same at multiple locations in the room. Using EM, we show how schemas allow for rapid
model learning in new environments that have matching topologies and emission structures with
fast binding (Fig. 1F). Inference can be performed with the matched schemas to actively plan and
pursue goals. We can also detect transitions to another known schema, or to unknown territory by
comparing likelihoods of observations under different schemas.
Schemascanalsobeusedasbuildingblockstorapidlylearnnovelenvironmentsthatarecomposed
of matching topologies. This comprises learning transitions and emissions but reusing known schemas
where they fit (See Suppl. §B.5 and Suppl. Alg.2).
5

--- PAGE 6 ---
Graph schemas as abstractions for transfer learning, inference, and planning
4. Results
We show results from two sets of experiments: (i) standard benchmarks for evaluating rapid adap-
tation, where the environments do not have perceptual aliasing, and (ii) more challenging setup of
environments with extensive perceptual aliasing.
4.1. Rapid adaptation and task solving in novel environments
We first evaluate our model on two benchmarks proposed by Ritter et al. (2020) to evaluate rapid
adaptation and task solving in new environments: Memory & Planning Game (MPG) and One-Shot
StreetLearn.
4.1.1. Memory & Planning Game
In the MPG, the agent can navigate on a 4×4grid, observing symbols, and the task is to collect
reward at a specified goal location in that grid (Fig. 2A). All grid positions have unique symbols
and the symbol-position mapping is randomized after each episode, which lasts for 100 steps. See
Suppl. §B.1 for details. This setup lets us evaluate our model on ground truth graph recovery and
schema reuse, as the structure is maintained across episodes. The agent needs to explore to collect
the observations and bind them rapidly to the schema to maximize rewards with optimal navigation.
task no. in episodetask no. in episode
training stepstraining steps
0 0 816 24
0 10 20 301   2
x 109
0 2   4
x 1090
00
1020304048
102030
0102030
No. of steps to goalNo. of steps to goalAverage reward Average rewardA B C
D  E FSample environment Learned graph
Learned graph start goalMPG StreetLearn
start goal
EPN
CSCG
oracle
optim. exp.
within each
task
exploration.
+ optim.
planning
Figure 2|Performance on MPG and One-shot StreetLearn benchmarks. Example episodes of MPG ( A)
and One-Shot StreetLearn ( D) along with their respective learned latent graphs. Average rewards in
novel test environments ( B, E). CSCG performance reaches its peak by 9 episodes in MPG and stays
the same thereafter. EPN performance is shown on evaluation at different stages of training. ( C, F)
Number of steps to get the reward in subsequent tasks in an episode. The CSCG agent explores first
without optimizing for reward collection, so our first few tasks take longer to complete but we reach
optimal planning after that. Values for optimal exploration and planning are also plotted as dotted
lines for comparison. First few tasks in One-Shot StreetLearn take variable numbers of steps as we
collect rewards incidentally. EPN baselines and optimal values for both environments are re-plotted
using the data from Ritter et al. (2020). Error bars are 95% CI of the SEM.
6

--- PAGE 7 ---
Graph schemas as abstractions for transfer learning, inference, and planning
CSCG schema learns the graph structure in few episodes. The CSCG schema agent first
explores the grid randomly collecting observations for a few episodes. After each episode, we learn a
CSCG model that best explains the experiences across all episodes so far observed. We reuse the same
schema (𝑇) across all episodes and learn a new binding (emission matrix 𝐸) per episode. It takes
only 9 episodes (900 steps) to learn a perfect schema of this 4×4grid environment. In subsequent
episodes, we rapidly learn the schema bindings and do planning to maximize the rewards. We employ
two different hard coded exploration policies: random navigation actions and an optimal set of actions
to cover the learned transition graph. Average reward per episode ±standard error of the mean
(SEM) after learning the schema is: 17.3±0.57for random and 26.4±0.17for optimal exploration
policy, which is comparable to Episodic planning network (EPN) (Ritter et al., 2020). In contrast,
EPN takes more than 10 million episodes ( >109steps) of training to reach its optimal performance
(Fig. 2B). Planning in our model is optimal in the number of steps to the reward, on par with EPN
and the oracle (Fig. 2C). CSCG performance remains the same since the first reward is collected after
exploration and the plans are optimal thereafter. Note that the number of steps to finish the first task
is longer in our case ( 18±0.09steps) than EPN, but the average reward in an episode is comparable.
4.1.2. One-Shot StreetLearn
One-Shot StreetLearn is a challenging variant of the StreetLearn task (Mirowski et al., 2019) with
varying connectivity structure across episodes to evaluate rapid task solving ability (Fig. 2D). In each
episode, the agent is placed in a new neighborhood of a city and the task is to navigate to a goal,
specified by the goal street view image, and collect the reward. After collecting a reward, the agent
is re-spawned in a new location and a new goal is specified. Unlike the MPG, the transition graph
changes every episode. We evaluate our model’s ability to rapidly learn in an episode and to navigate
optimally to the goals to maximize the rewards. Note that there is no schema reuse in this setting: we
learn a new model for every episode. This showcases the ability to learn rapidly within a few steps
without any prior training and plan efficiently with the learned model.
CSCG matches optimal planning in One-Shot StreetLearn . For the CSCG agent, we follow
an explore and exploit strategy with a hard coded exploration policy. During exploration, the
agent navigates every action from every observation it encounters while collecting the rewards as it
encounters the goals, and uses this experience to learn a CSCG. This is a guided exploration to cover
every possible edge in the transition graph. After exploration, the agent plans with the CSCG and
collects rewards (See Suppl. §B.2 for details). Average reward ±SEM over 100 episodes is 21.7±3.7,
which is lower than EPN (28.7) as our exploration strategy is not optimal (Fig. 2E). Since we do not
consider optimal exploration in this work, we compare the planning performance on the learned
model after exploration. Post exploration, our agent takes on average 4.8±0.03steps to reach the
goal, which matches the optimal value (Ritter et al., 2020) (Fig. 2F). Note that we do not transfer
any learning across episodes in this setting since the graph changes every episode. In cities with
re-usable graph structures such as the grid layout in Manhattan, CSCG schemas benefit from the
reuse. We evaluate this schema reuse in detail in much harder settings in the following experiments
on navigating in rooms with extensively aliased observations.
4.2. Schema matching and transfer learning in highly aliased environments
In the next set of experiments, we evaluate schema matching and transfer learning in novel envi-
ronments that vary in observations and in some cases size from training environments. For these
experiments, we use a more challenging setting than the benchmarks above, with larger 2D and 3D
simulated environments and extensive aliasing (Beattie et al., 2016). Note that even though we use
spatial navigation as our setup, we do not use any assumptions about space (Euclidean or otherwise)
7

--- PAGE 8 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Figure 3|Rapid learning using schemas in novel 3D environments. A. As the agent walks in a test
room, we learn the emission matrix and measure the negative log likelihood (NLL) of observation
sequences under different schemas. The schema with the least NLL (conversely the highest likelihood)
is considered the best match. The left panel shows the isometric view of the the training rooms used
for learning the schemas. The middle and right panels show schema matching for two example test
rooms. The test rooms differ in color and lighting conditions from the training rooms. B. Schema
stitching in a novel 3D environment (left) composed of four out of the six rooms. Transition graphs
show the prior model, with potential connections between all six schemas, and the learned models
after 5,000 and 10,000 steps. (Right) NLL of test data for models learned at different walk lengths,
for both the model stitched from known schemas and the model learned from scratch. Error bars are
SEM, computed across 5 repetitions.
and model it as a graph navigation problem. For both 2D and 3D environments, we use a simple
model of the agent’s observations that does not rely on 3D perception, so our results are agnostic to
the specific 3D spatial setup.
We start with a set of environments of different shapes and topologies with extensive aliasing.
Analogous to a large empty arena, observations in the interior of these environments are perceptually
aliasedLajoieetal.(2019);WhiteheadandBallard(1991),seeFig.1D.Aliasingoccursbyconstruction
in the 2D case, and as a result of clustering in the 3D case. We first learn the schemas in training
rooms (Fig. 1D), as described in section 3.3 . We evaluate schema matching on test rooms with similar
layouts but with novel observations and size variations. In a novel test room, the agent takes a random
walk and evaluates the likelihood of the observation sequences given the actions executed under
different learned schemas (Fig. 1E). Note that this requires first learning the new emission matrix
from the data collected during this random walk and computing the likelihood for each schema. The
schema with the best likelihood is considered the matching schema. We evaluate the likelihoods of
different schemas at multiple intervals during the random walks. See Suppl. §B.3 for details and
parameters used in the following experiments.
CSCG schemas rapidly learn matching bindings in novel environments . To demonstrate this,
we used 3D environments (Beattie et al., 2016) with 6 different layouts. The agent can navigate
with 3 discrete egocentric actions (move forward, turn left, turn right) and the observations are
RGB images corresponding to the agent’s view (Fig. 1A, B). The observation space in this setup
is large and complex and demonstrates the model’s applicability to such use cases. We follow the
8

--- PAGE 9 ---
Graph schemas as abstractions for transfer learning, inference, and planning
procedure for continuous observations described in §3.2 and Suppl. §A.3 to learn models of the
training environments (see Fig. S1 for the learned graphs after training using random walks). We
evaluated rapid learning on test environments with the same layout but with different colored walls,
floor, and environment lighting, which corresponds to entirely new RGB observations, for an agent
navigating in those rooms using the procedure described in Suppl. §A.4. Fig. 3A shows successful
rapid matching of correct schemas in test rooms as evaluated by negative log likelihoods (See Suppl.
Fig. S2 for full results). The correct schema was identified for all six test environments, usually within
1,000 steps in the environment and in all cases within 2,000 steps, compared to about 50,000 steps
used to learn without schema reuse.
Totestthetransfertosizevariations,weused2Droomsoffivedifferentlayouts(cylinder,rectangle,
square with hole, torus, and U-shape) each in 3 different sizes (small, medium, large) (Fig. S3). We
learn schemas for the medium-sized versions of these rooms using a random walk of 50,000steps. In
the test rooms with novel observation mappings, the agent takes a random walk while we learn the
new emissions and evaluate the likelihoods of these observations under all schemas every 5 steps.
See Appendix B.3.2 for details. Fig. S3 shows the negative log likelihoods of all test room under all
schemas. By reusing clone structure, we are able to correctly match the schema in all cases by 95 steps
demonstrating a rapid matching and adaptation to novel environments with size and observation
variations (Appendix Fig. S4 shows results without using clone structure). We reproduced these
results using ten digit exemplars of binarized MNIST dataset as our room layouts as they provide
an interesting variety of shapes and topologies that are not designed by us (see Suppl. §B.3.3 for
results).
Schema matching also works in environments composed of multiple schemas. We demonstrate
this in novel test environments that are composed of pairs of MNIST digit rooms, and show online
schema matching of individual digits. See Suppl. §B.5 for details and results.
4.3. Rapid learning of novel environments with compositionality of graph schemas
Schemas can also be used to efficiently learn and navigate in larger environments composed of known
schemas. We do this by matching schemas and learning the transition structure between them. Fig.
3B shows an example 3D simulated environment, composed of four smaller 3D rooms. The agent
walks in this environment and learns the composed model (both joint transitions and emissions) in
far fewer steps than needed to learn without using schemas. Fig. 3B also shows the prior model
with all potential connections and the learned model at two different walk lengths. Model quality is
measured by the negative log likelihood over 10,000-step test walks, for models trained after walks
of different lengths with and without using schemas. Using schemas, we were able to learn a perfect
model of the environment in less than 10,000steps, whereas learning from scratch was significantly
worse even after 30,000steps. See Suppl. §B.5 for the learning algorithm and experimental details.
This ability to learn by composing and reusing previously learned schemas enables rapid adaptation
to novel environments which only gets better with more experience.
4.4. Rapid planning and navigation in novel environments
Rapid schema matching and binding enables planning in novel environments with limited experience.
We first demonstrate this capacity in a novel variation of the four-room 3D environment that was
introduced in Fig. 4A. In this demonstration, an agent first walks in the test environment, and is then
tasked with finding the shortest route back to its starting location. We used a manually specified
initial walk in order to cover a good portion of the environment in a minimal number of steps. In this
case, the schema – the composite schema that was learned previously for the four-room combination
9

--- PAGE 10 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Figure4|Schemasenablerapidandrobustplanninginnewenvironments. A.Givenpartialexperience
in a novel environment (here, a differently colored variation of the four-room environment from
Fig. 3), the joint transition model and the agent’s location within it can be identified and used to
rapidly navigate around obstacles and find the shortest path to the goal (here, to return to the starting
position). B. (Left) An example test room and its matching schema. Note the mismatch in the size and
the off-center hole in the test room. (Right). Navigation performance as measured by the distance
from the true goal and the number of plans/re-plans the agent makes, as a function of size and
structure variation between test room and the matching schema. Error bars are 95% CI of the SEM.
(Fig 3) – is known a priori. The agent uses the observations from the initial walk to learn new emission
bindings and decode its current and goal positions within the model. The agent then plans the
shortest path to the goal using this model (Fig. 4B). Note that the planned route passes through a yet
unvisited part of the environment.
While executing a plan, if the agent estimates that it has not reached the goal after accounting for
the new observations, it is possible that there is a schema mismatch or the estimated emission matrix
is inaccurate. When this occurs, the agent can initiate re-planning after updating the model using
new experiences gained while executing the plan. This process is iterated until the agent believes that
it has reached its goal after decoding observations from the initial random walk and all subsequent
re-planning steps. In an experiment designed to test this capacity, we systematically evaluate this
robustness to schema mismatch in 2D environments in terms of Manhattan distance from the goal
location, and the number of re-plans required. Fig. 4B shows results for one example schema. We
can successfully navigate even with size and structure variations and the performance degrades
gracefully as the variation between the schema and the test room increases. The number of planning
attempts required to reach the goal also increases smoothly. The diagonal smoothing term that adds
self-transition probabilities (see Suppl. §B.6) is critical for generalization to size variations. Without
this smoothing, the agent never reaches the goal in larger size variations of the test rooms as shown
in Fig 4B (See Suppl. §B.6 for results in another room layout).
5. Discussion and Future Work
Learning abstractions that can rapidly bind to observations from environments that share the same
underlying structure is the hypothesized mechanism for transfer learning in humans and animals
10

--- PAGE 11 ---
Graph schemas as abstractions for transfer learning, inference, and planning
(Kumaranetal.,2016;Tseetal.,2007;Zhouetal.,2021). Wehaveproposedaconcretecomputational
model of abstractions and rapid binding using graph schemas that learn higher-order structures from
aliased observation sequences, and uses a slot-binding mechanism for transferring those schemas
to rapidly learn models of new environments. CSCG schemas learned graph structure in far fewer
episodes than a deep RL agent and matched optimal planning in MPG and One-Shot StreetLearn
tasks. In highly aliased environments, CSCG schemas found matching schema bindings in novel
rooms of different sizes. In composed rooms, we were able to match the correct schemas as the agent
moved across rooms corresponding to different schemas. We showed successful planning to goals in
rooms with shape and size variations from the matching schema by re-planning and updating the
model while walking to the goal. More importantly, known schemas can be composed to rapidly learn
novel environments and new larger schemas. This ability bootstraps on itself and only gets better
with more experience. There are many clear directions for potential future works to build on top of
our current work, and we list some of them below.
Schema learning from experiences. We learned schemas independently and explicitly in this
work, but in the real world, it might not be feasible to have access to differentiated experiences
belonging to distinct schemas. Learning reusable schemas from a continuous stream of experiences
(Farzanfar et al., 2023) could be an interesting future work.
Schemas vs memories. We discard previously learned emissions from past experiences and learn
a new binding. However, in some cases, previously learned emissions are directly applicable and
therefore keeping those in addition could enable even faster zero-shot adaptation when there is a
match. This can be thought as keeping specific memories versus using abstract schemas.
Schema maintenance. Our schemas in this work are fixed. However, it is possible to update the
schemas with new experiences. In fact, children initially tend to perceive and remember experiences
that fit in their existing schemas and develop the flexibility later (Piaget and Cook, 1952). Similarly,
we could update schemas based on new experiences, and even make the schemas themselves flexible
to encapsulate related abstractions but still constrained by rules to allow consistent inference.
Active exploration. We used either random or known optimal exploration policies to learn and
bind schemas. But the schemas provide action-conditioned beliefs on future observations, and by
choosing actions that could optimally disambiguate different schemas and seek to learn connections
between them, we could potentially do much better than random exploration. Similarly, instead of
random exploration to learn a new environment and schema, we could direct the exploration policy
by composing known schemas (Sharma et al., 2021), and even actively learn them while exploring.
References
P. Baraduc, J.-R. Duhamel, and S. Wirth. Schema cells in the macaque hippocampus. Science, 363
(6427):635–639, Feb. 2019.
C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright, H. Küttler, A. Lefrancq, S. Green,
V. Valdés, A. Sadik, J. Schrittwieser, K. Anderson, S. York, M. Cant, A. Cain, A. Bolton, S. Gaffney,
H. King, D. Hassabis, S. Legg, and S. Petersen. Deepmind lab, 2016. URL https://arxiv.org/
abs/1612.03801 .
G. V. Cormack and R. N. S. Horspool. Data compression using dynamic Markov modelling. The
Computer Journal , 30(6):541–550, 1987.
M. Crouse, C. Nakos, I. Abdelaziz, and K. Forbus. Neural analogical matching. AAAI, 35(1):809–817,
May 2021.
11

--- PAGE 12 ---
Graph schemas as abstractions for transfer learning, inference, and planning
A. Dedieu, N. Gothoskar, S. Swingle, W. Lehrach, M. Lázaro-Gredilla, and D. George. Learning
higher-order sequential structure with cloned hmms, 2019. URL https://arxiv.org/abs/
1905.00507 .
H. Eichenbaum and N. J. Cohen. Can we reconcile the declarative memory and spatial navigation
views on hippocampal function? Neuron, 83(4):764–770, Aug. 2014.
B. Eysenbach, R. R. Salakhutdinov, and S. Levine. Search on the replay buffer: Bridging planning and
reinforcement learning. Advances in Neural Information Processing Systems , 32, 2019.
D. Farzanfar, H. J. Spiers, M. Moscovitch, and R. S. Rosenbaum. From cognitive maps to spatial
schemas. Nature Reviews Neuroscience , 24(2):63–79, 2023.
D. George, R. V. Rikhye, N. Gothoskar, J. S. Guntupalli, A. Dedieu, and M. Lázaro-Gredilla. Clone-
structured graph representations enable flexible learning and vicarious evaluation of cognitive
maps. Nat. Commun. , 12(1):2392, Apr. 2021.
A. Gilboa and H. Marlatte. Neurobiology of schemas and Schema-Mediated memory. Trends Cogn.
Sci., 21(8):618–631, Aug. 2017.
K. Gregor, D. Jimenez Rezende, F. Besse, Y. Wu, H. Merzic, and A. van den Oord. Shaping belief states
with generative environment models for rl. Advances in Neural Information Processing Systems , 32,
2019.
S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for
visual navigation. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 2616–2625, 2017.
M. Igl, L. Zintgraf, T. A. Le, F. Wood, and S. Whiteson. Deep variational reinforcement learning for
pomdps. In International Conference on Machine Learning , pages 2117–2126. PMLR, 2018.
G. Kerg, S. Mittal, D. Rolnick, Y. Bengio, B. A. Richards, and G. Lajoie. Inductive biases for relational
tasks. In ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality , 2022.
T. Kipf, Y. Li, H. Dai, V. Zambaldi, A. Sanchez-Gonzalez, E. Grefenstette, P. Kohli, and P. Battaglia.
Compile: Compositional imitation learning and execution. In International Conference on Machine
Learning , pages 3418–3428. PMLR, 2019.
D. Kumaran, D. Hassabis, and J. L. McClelland. What learning systems do intelligent agents need?
complementary learning systems theory updated. Trends Cogn. Sci. , 20(7):512–534, July 2016.
P.-Y. Lajoie, S. Hu, G. Beltrame, and L. Carlone. Modeling perceptual aliasing in slam via dis-
crete–continuous graphical models. IEEE Robotics and Automation Letters , 4(2):1232–1239, 2019.
doi: 10.1109/LRA.2019.2894852.
A. Lampinen, S. Chan, A. Banino, and F. Hill. Towards mental time travel: a hierarchical memory for
reinforcement learning agents. Advances in Neural Information Processing Systems , 34:28182–28195,
2021.
P. Mirowski, A. Banki-Horvath, K. Anderson, D. Teplyashin, K. M. Hermann, M. Malinowski, M. K.
Grimes, K. Simonyan, K. Kavukcuoglu, A. Zisserman, et al. The streetlearn environment and dataset.
arXiv preprint arXiv:1903.01292 , 2019.
M. Mitchell. Abstraction and analogy-making in artificial intelligence. Ann. N. Y. Acad. Sci. , 1505(1):
79–101, Dec. 2021.
12

--- PAGE 13 ---
Graph schemas as abstractions for transfer learning, inference, and planning
T. Ni, B. Eysenbach, and R. Salakhutdinov. Recurrent model-free rl can be a strong baseline for many
pomdps. arXiv preprint arXiv:2110.05038 , 2021.
C. Packer, K. Gao, J. Kos, P. Krähenbühl, V. Koltun, and D. Song. Assessing generalization in deep
reinforcement learning. arXiv preprint arXiv:1810.12282 , 2018.
J. Piaget and M. T. Cook. The origins of intelligence in children. WW Norton & Co, 1952.
A. R. Preston and H. Eichenbaum. Interplay of hippocampus and prefrontal cortex in memory. Curr.
Biol., 23(17):R764–73, Sept. 2013.
R. V. Raju, J. S. Guntupalli, G. Zhou, M. Lázaro-Gredilla, and D. George. Space is a latent sequence:
Structured sequence learning as a unified theory of representation in the hippocampus, 2022.
K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen. Efficient off-policy meta-reinforcement
learning via probabilistic context variables. In International conference on machine learning , pages
5331–5340. PMLR, 2019.
S. Ritter, R. Faulkner, L. Sartran, A. Santoro, M. Botvinick, and D. Raposo. Rapid task-solving in novel
environments, 2020. URL https://arxiv.org/abs/2006.03662 .
V. Samborska, J. L. Butler, M. E. Walton, T. E. J. Behrens, and T. Akam. Complementary task
representations in hippocampus and prefrontal cortex for generalizing the structure of problems.
Nat. Neurosci. , pages 1–13, Sept. 2022.
N. Savinov, A. Dosovitskiy, and V. Koltun. Semi-parametric topological memory for navigation. arXiv
preprint arXiv:1803.00653 , 2018.
M. Shanahan and M. Mitchell. Abstraction for deep reinforcement learning, 2022. URL https:
//arxiv.org/abs/2202.05839 .
V. Sharan, S. M. Kakade, P. S. Liang, and G. Valiant. Learning overcomplete hmms. In Advances in
Neural Information Processing Systems , pages 940–949, 2017.
S. Sharma, A. Curtis, M. Kryven, J. Tenenbaum, and I. Fiete. Map induction: Compositional spatial
submap learning for efficient exploration in novel environments, 2021. URL https://arxiv.
org/abs/2110.12301 .
K. L. Stachenfeld, M. M. Botvinick, and S. J. Gershman. The hippocampus as a predictive map. Nat.
Neurosci. , 20, 2017.
J. B. Tenenbaum, C. Kemp, T. L. Griffiths, and N. D. Goodman. How to grow a mind: statistics,
structure, and abstraction. Science, 331(6022):1279–1285, Mar. 2011.
D. Tse, R. F. Langston, M. Kakeyama, I. Bethus, P. A. Spooner, E. R. Wood, M. P. Witter, and R. G. M.
Morris. Schemas and memory consolidation. Science, 316(5821):76–82, Apr. 2007.
J. J. Verbeek, N. Vlassis, and B. Kröse. Efficient greedy learning of gaussian mixture models. Neural
computation , 15(2):469–485, 2003.
J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran,
and M. Botvinick. Learning to reinforcement learn, 2016. URL https://arxiv.org/abs/1611.
05763.
13

--- PAGE 14 ---
Graph schemas as abstractions for transfer learning, inference, and planning
T. W. Webb, I. Sinha, and J. Cohen. Emergent symbols through binding in external memory. In
International Conference on Learning Representations , 2021. URL https://openreview.net/
forum?id=LSFCEb3GYU7 .
S. D. Whitehead and D. H. Ballard. Learning to perceive and act by trial and error. Machine Learning ,
7:45–83, 1991.
J. C. R. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and T. E. J. Behrens. The
Tolman-Eichenbaum machine: Unifying space and relational memory through generalization in
the hippocampal formation. Cell, 183(5):1249–1263.e23, Nov. 2020.
J. C. R. Whittington, J. Warren, and T. E. J. Behrens. Relating transformers to models and neural
representations of the hippocampal formation, 2021. URL https://arxiv.org/abs/2112.
04035.
C. J. Wu et al. On the convergence properties of the em algorithm. The Annals of statistics , 11(1):
95–103, 1983.
L. Zhang, G. Yang, and B. C. Stadie. World model as a graph: Learning latent landmarks for planning.
InInternational Conference on Machine Learning , pages 12611–12620. PMLR, 2021.
J. Zhou, C. Jia, M. Montesinos-Cartagena, M. P. H. Gardner, W. Zong, and G. Schoenbaum. Evolving
schema representations in orbitofrontal ensembles during learning. Nature, 590(7847):606–611,
Feb. 2021.
14

--- PAGE 15 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Supplementary material
A. Expectation-Maximization learning of CSCGs
Cloned Hidden Markov Models (HMMs), first introduced in (Dedieu et al., 2019), are a sparse
restriction of overcomplete HMMs (Sharan et al., 2017) that can overcome many of the training
shortcomings of dynamic Markov coding (Cormack and Horspool, 1987). Similar to HMMs, cloned
HMMs assume the observed sequence 𝒙≡ {𝑥1...,𝑥𝑁}is generated from a hidden process 𝒛≡
{𝑧1...,𝑧𝑁}that obeys the Markovian property,
𝑃(𝒙)=∑︁
𝒛𝑃(𝑧1)𝑁Ö
𝑛=2𝑃(𝑧𝑛|𝑧𝑛−1)𝑁Ö
𝑛=1𝑃(𝑥𝑛|𝑧𝑛). (2)
Here𝑃(𝑧1)is the initial hidden state distribution, 𝑃(𝑧𝑛|𝑧𝑛−1)is the transition probability from 𝑧𝑛−1to
𝑧𝑛, and𝑃(𝑥𝑛|𝑧𝑛)is the probability of emitting 𝑥𝑛from the hidden state 𝑧𝑛.
In contrast to HMMs, cloned HMMs assume that each hidden state maps deterministically to a
single observation. Further, cloned HMMs allow multiple hidden states to emit the same observation.
All the hidden states that emit the same observation are called the clonesof that observation. With
this constraint, the joint distribution of the observed data can be expressed as,
𝑃(𝒙)=∑︁
𝑧1∈𝐶(𝑥1)...∑︁
𝑧𝑁∈𝐶(𝑥𝑁)𝑃(𝑧1)𝑁Ö
𝑛=2𝑃(𝑧𝑛|𝑧𝑛−1) (3)
where𝐶(𝑥𝑛)corresponds to the hidden states (clones) of the emission 𝑥𝑛.
Clone-structured cognitive graphs (CSCGs) build on top of cloned HMMs by augmenting the
model with the actions of an agent 𝒂≡{𝑎1...,𝑎𝑁}(George et al., 2021),
𝑃(𝒙|𝒂)=∑︁
𝑧1∈𝐶(𝑥1)...∑︁
𝑧𝑁∈𝐶(𝑥𝑁)𝑃(𝑧1)𝑁Ö
𝑛=2𝑃(𝑧𝑛|𝑧𝑛−1,𝑎𝑛−1) (4)
A.1. Learning the transition matrix with emissions fixed
Learning a CSCG in a new environment requires optimizing the vector of prior probabilities 𝜋:
𝜋𝑘=𝑃(𝑧1=𝑘)and the action-augmented transition tensor 𝑇:𝑇𝑖𝑗𝑘=𝑃(𝑧𝑛=𝑘|𝑧𝑛−1=𝑗,𝑎𝑛−1=𝑖). The
emission matrix 𝐸, which encodes the particular allocation of clones to observations, is kept fixed
throughout learning.
The standard algorithm to train HMMs is the expectation-maximization (EM) algorithm (Wu
et al., 1983), which in this context is known as the Baum-Welch algorithm. Learning a CSCG using
the Baum-Welch algorithm requires a few simple modifications: the sparsity of the emission matrix
can be exploited to only use small blocks of the transition matrix both in the Expectation (E) and
Maximization (M) steps.
To this end, we assume the hidden states are indexed such that all the clones of the first emission
appear first, all the clones of the second emission appear next, etc. Let 𝑁obsand𝑁actionsbe the total
number of emissions and actions, respectively. The transition matrix 𝑇can then be broken down into
smaller submatrices 𝑇(𝑢,𝑣,𝑤), where𝑣,𝑤∈{1,...,𝑁 obs}and𝑢∈{1,...,𝑁 actions}. The submatrix
𝑇(𝑢,𝑣,𝑤)contains the transition probabilities 𝑃(𝑧𝑛|𝑧𝑛−1,𝑎𝑛−1=𝑢)for𝑧𝑛−1∈𝐶(𝑣)and𝑧𝑛∈𝐶(𝑤),
where𝐶(𝑣)and𝐶(𝑤)correspond to the clones of emissions 𝑣and𝑤respectively.
15

--- PAGE 16 ---
Graph schemas as abstractions for transfer learning, inference, and planning
The standard Baum-Welch equations can then be expressed in a simpler form in the case of a
CSCG. The E-step recursively computes the forward and backward probabilities and then updates the
posterior probabilities. The M-step updates the transition matrix via row normalization.
E-Step:
𝛼(1)=𝜋(𝑧1) 𝛼(𝑛)⊤=𝛼(𝑛−1)⊤𝑇(𝑎𝑛−1,𝐶(𝑥𝑛−1),𝐶(𝑥𝑛)) (5)
𝛽(𝑁)=1(𝑧𝑁) 𝛽(𝑛)=𝑇(𝑎𝑛,𝐶(𝑥𝑛),𝐶(𝑥𝑛+1))𝛽(𝑛+1) (6)
𝜉𝑢𝑣𝑤(𝑛)=𝛼(𝑛)◦𝑇(𝑎𝑛,𝑣,𝑤)◦𝛽(𝑛+1)⊤
𝛼(𝑛)⊤𝑇(𝑎𝑛,𝑣,𝑤)𝛽(𝑛+1)(7)
𝛾(𝑛)=𝛼(𝑛)◦𝛽(𝑛)
𝛼(𝑛)⊤𝛽(𝑛). (8)
M-Step:
𝜋(𝑧1)=𝛾(1) (9)
𝑇(𝑢,𝑣,𝑤)=𝑁∑︁
𝑛=1𝜉𝑢𝑣𝑤(𝑛)⊘𝑁actions∑︁
𝑢=1𝑁obs∑︁
𝑤=1𝑁∑︁
𝑛=1𝜉𝑢𝑣𝑤(𝑛) (10)
where◦and⊘denote element-wise product and division, respectively (with broadcasting where
needed). All vectors in the E and M-steps are 𝑁cpe×1column vectors, where 𝑁cpeis the number of
clones per emission. We use a constant number of clones per emission for simplicity of description,
but the number of clones can be selected independently per emission.
Importantly, CSCGs exploit the sparsity pattern in the emission matrix when performing training
updates and inference, and achieve significant computational savings when compared with HMMs
(George et al., 2021).
A.2. Learning the emission matrix with transitions fixed
With a CSCG, transfer learning between different environments can be accomplished by keeping
its transition probabilities 𝑇fixed and learning the emissions associated to its nodes 𝐸in the new
environment. Further, if we know that the new environment preserves the emission structure, then we
can further restrict the learning of 𝐸, with all the rows of 𝐸that correspond to the same observation
in the original environment sharing the same parameters.
The EM algorithm can be used to learn the emission matrix as follows. The E-step recursively
computes the forward and backward probabilities and then updates the posterior probabilities. The
M-step updates only the emission matrix.
E-Step:
˜𝛼(𝑛)= 
𝑇(𝑎𝑛−1)⊤𝛼(𝑛−1)◦𝐸(𝑥𝑛)𝑝𝛼(𝑛)=|𝑍|∑︁
𝑘=1˜𝛼𝑘(𝑛)𝛼(𝑛)=˜𝛼(𝑛)/𝑝𝛼(𝑛)(11)
˜𝛽(𝑛)=𝑇(𝑎𝑛)(𝛽(𝑛+1)◦𝐸(𝑥𝑛+1))𝑝𝛽(𝑛)=|𝑍|∑︁
𝑘=1˜𝛽𝑘(𝑛)𝛽(𝑛)=˜𝛽(𝑛)/𝑝𝛽(𝑛)(12)
(13)
16

--- PAGE 17 ---
Graph schemas as abstractions for transfer learning, inference, and planning
𝛾(𝑛)=𝛼(𝑛)◦𝛽(𝑛)
𝛼(𝑛)⊤𝛽(𝑛)(14)
M-Step:
𝐸(𝑗)=𝑁∑︁
𝑛=11𝑥𝑛=𝑗𝛾(𝑛)⊘𝑁∑︁
𝑛=1𝛾(𝑛) (15)
Note that the updates here involve |𝑍|×1vectors, where|𝑍|(=Í𝑁𝑐𝑝𝑒) is the total number of hidden
states in the model. 𝑇(𝑎𝑛)corresponds to the transition matrix for the action 𝑎𝑛and is of size|𝑍|×|𝑍|,
𝐸(𝑥𝑛)is a column of the emission matrix corresponding to the emission 𝑥𝑛, and1𝑥𝑛=𝑗is an indicator
function. The forward message is initialized as ˜𝛼(1)=𝜋◦𝐸(𝑥1); and the backward message is
initialized as a vector of all 1s. The emission matrix can be initialized randomly or with equal
probabilities for all observations.
When the clone structure is to be preserved, the 𝛾(𝑛)term in the E-step is modified as follows. For
each observation 𝑗∈{1,...,𝑁 obs}, we set the posterior probability for all clones of 𝑗to be the same:
¯𝛾𝑘(𝑛)=∑︁
𝑙∈𝐶(𝑗)𝛾𝑙(𝑛) ∀𝑘∈𝐶(𝑗). (16)
For a given 𝑇and𝐸, the normalization term 𝑝𝛼(𝑛)for the forward messages in the E-step can
be used to compute the negative log-likelihood (NLL) of a sequence of observation-action pairs as
follows:
NLL=−1
𝑁𝑁∑︁
𝑛=1log𝑝𝛼(𝑛) (17)
A.3. Learning the continuous CSCG
When each observation at timestep 𝑛is a continuous vector 𝑥𝑛(e.g., an image), we want to maximize
the following log-likelihood
log𝑝(𝒙|𝒂;𝑇,𝝁)=log∑︁
𝑧,𝑦𝑃(𝑧1)𝑁Ö
𝑛=2𝑃(𝑧𝑛|𝑧𝑛−1,𝑎𝑛−1)𝑁Ö
𝑛=1𝑝(𝑦𝑛|𝑧𝑛)N(𝑥𝑛|𝜇𝑦𝑛,𝜎2𝐼) (18)
w.r.t.𝑇, where𝑇𝑖𝑗𝑘=𝑃(𝑧𝑛=𝑘|𝑧𝑛−1=𝑗,𝑎𝑛−1=𝑖), and𝑝(𝑦𝑛|𝑧𝑛)is 1 for𝑧𝑛∈𝐶(𝑦𝑛)and 0 for all others.
This graphical model is tree-structured, so its value can be computed exactly. Parameter optimiza-
tion can be done via EM, where both the expectation and maximization steps are exact. Rather than
learning both 𝑇and𝝁simultaneously, we find it simpler to proceed in two steps.
In the first step we fix 𝑃(𝑧1)=𝑇𝑖𝑗𝑘=1/|𝑍|, where|𝑍|is the number of states in the hidden space,
and learn 𝝁only. The problem thus simplifies to maximizing
log𝑝(𝒙|𝒂;𝑇=𝑇uniform,𝜇)=−𝑁log|𝑍|+log∑︁
𝒛,𝒚𝑁Ö
𝑛=1𝑝(𝑦𝑛|𝑧𝑛)N(𝑥𝑛|𝜇𝑦𝑛,𝜎2𝐼)
=−𝑁log|𝑍|+𝑁∑︁
𝑛=1log𝐾∑︁
𝑦𝑛=1𝑛𝐶(𝑦𝑛)N(𝑥𝑛|𝜇𝑦𝑛,𝜎2𝐼)
=𝑁∑︁
𝑛=1log𝐾∑︁
𝑘=1𝑛𝐶(𝑘)
|𝑍|N(𝑥𝑛|ˆ𝜇𝑘,𝜎2𝐼) (19)
17

--- PAGE 18 ---
Graph schemas as abstractions for transfer learning, inference, and planning
w.r.t. 𝝁. The number of clones of a given centroid ˆ𝜇𝑘is denoted by 𝑛𝐶(𝑘)(thus,Í𝐾
𝑘=1𝑛𝐶(𝑘)=|𝑍|).
In the last equality we collect all the Gaussians that are known to have the same mean (i.e., all the
clones with the same centroid). The computation is more efficient, since it no longer scales with the
total number of hidden states |𝑍|, but only with the number of distinct means 𝐾.
The astute reader will recognize Eq. 19 as the log-likelihood of an isotropic Gaussian mixture
model, which can be optimized greedily using 𝐾-means. The centroids ˆ𝜇𝑘are the cluster centers and
𝑛𝐶(𝑘)
|𝑍|corresponds to the prior probabilities of each center. In other words, to maximize log𝑝(𝒙|𝒂;𝑇=
𝑇uniform,𝝁)w.r.t. 𝝁we simply need to run 𝐾-means on the input data 𝒙(temporal ordering becomes
irrelevant) and then assign a number of clones to each centroid that is proportional to the prior
probability of that cluster. When using 𝐾-means, the value of 𝜎2does not affect the optimization of the
centroids. Once the centroids have been chosen, one can set 𝜎2based on the hard assignments from
𝐾-means, where the maximum likelihood estimate is closed form and simply matches the average
distortion. Directly optimizing Eq. 19 w.r.t. 𝝁and𝜎2is also possible using EM (and in principle, more
precise), but will be more computationally expensive.
In the second step, we keep 𝝁fixed as per the previous step, and learn 𝑇by maximizing Eq. 18
w.r.t.𝑇using EM. While it would be possible to learn 𝝁and𝑇simultaneously in the same EM loop, we
find thatproceeding inthese two stepsresults infaster convergence withoutsignificantly deteriorating
the quality of the found local optimum.
Two approaches are possible for the learning of 𝑇:
Soft-evidence This would be the vanilla approach of maximizing Eq. 18 w.r.t. 𝑇using EM: in the
E-step, we obtain the exact posterior over each 𝑧𝑛by running sum-product inference, and in the
M-step we find the maximizing 𝑇in closed form for that posterior. This is computationally demanding,
since the cost per learning iteration scales with O(𝑁|𝑍|2).
Hard-evidence We can apply here the same idea that converts EM for Gaussian mixture modelling
into𝐾-means (sometimes known as hard-EM). Instead of considering all the centroids as partially
responsible Verbeek et al. (2003) for each sample 𝑥𝑛, we can assign all the responsibility to the
dominating Gaussian. This is often a very precise approximation, since the dominating Gaussian
typically takes most of the responsibility. In turn, this means that at each timestep, only the clones
of the centroid that is closest (in the Euclidean sense) can be active. With this change, the cost per
learning iteration now scales as O(𝑁𝑛2
𝐶)≪O(𝑁|𝑍|2), where we have assumed a constant number of
clones𝑛𝐶=𝑛𝐶(𝑘)for all centroids for ease of comparison. When doing this, the observations become
effectively discrete and learning 𝑇reduces to the procedure described in Section A.1.
In our experiments we use the second approach, which is much more efficient. Putting all together,
the final learning procedure can then be summarized as follows:
1. Run𝐾-means on the training data for a given number of centroids 𝐾.
2.Assign to each centroid a number of clones proportional to the prior probablity of that centroid,
as obtained by 𝐾-means.
3.Vector-quantize the data (train and test) by assigning to each data point the label of the closest
centroid.
4.Learn the action-conditional transition matrix as if this was a discrete CSCG, as explained in
§A.1.
18

--- PAGE 19 ---
Graph schemas as abstractions for transfer learning, inference, and planning
A.4. Transfer learning in continuous CSCGs
Inthetransferlearningsettingweknow 𝑇foragivenenvironment, andneedtolearn 𝝁. Theprincipled
approach is simple: maximize Eq. 18 w.r.t. 𝝁while keeping 𝑇fixed. This can be done using EM, where
both expectation and maximization will be exact steps. Clone structure can be enforced exactly as
well, by tying the parameters of the centroids that correspond to the same clone during the M-step.
Theaboveapproachusessoft-evidenceandrunsintothesamecomputationalissuesaswediscussed
in §A.3. If we do not need to keep the same clone structure, we can resort to the same hard-evidence
trick to significantly bring down computation time, again from O(𝑁|𝑍|2)toO(𝑁𝑛2
𝐶). The process
again reduces to using 𝐾-means and then doing transfer learning in the discrete space:
1.Run𝐾-means on data from the new environment. 𝐾can be the number of centroids from the
environment we are transferring from, but does not need to. The only restriction is that it is not
larger than the number of states in 𝑇.
2.Vector-quantize the data from the new environment by assigning to each data point the label of
the closest centroid.
3.LearntheemissionmatrixasifthiswasadiscreteCSCG,asexplainedin§A.2(withoutpreserving
the clone structure).
A.4.1. Schemas for transfer learning
Grounded schema
A grounded schema is a 3-tuple 𝑆𝑔≡{𝑇,C,𝐸}containing an action-conditional transition tensor
𝑇, a clone structure C, and an emission matrix 𝐸that respects the clone structure.
The transition tensor 𝑇encodes the transition probabilities, with elements
𝑇𝑖𝑗𝑘=𝑃(𝑧𝑛=𝑘|𝑧𝑛−1=𝑗,𝑎𝑛−1=𝑖)
We refer to 𝑇as the schemaportion of the grounded schema.
Latent variables 𝑧take values in the range 1,...,|𝑍|. This range is partitioned in 𝐾≤|𝑍|groups
ofclonedstates. The group index of a given latent state is given by 1≤C(·)≤𝐾. Thus, two latent
states𝑧=𝑗and𝑧=𝑗′belong to the same clone group iff C(𝑗)=C(𝑗′).
The emission matrix 𝐸encodes the emission probabilities, with elements
𝐸𝑖𝑗=𝑃(𝑦𝑛=𝑗|𝑧𝑛=𝑖)
An emission matrix is said to respect the clone structure if the conditionC(𝑗)=C(𝑗′)=⇒𝐸𝑖𝑗=
𝐸𝑖𝑗′∀𝑖, 𝑗, 𝑗′is satisfied.
Ungrounded schema with clone structure An ungrounded schema with clone structure is a
tuple𝑆𝑢≡{𝑇,C}. It can be derived from a grounded schema 𝑆𝑔by removing the emission matrix 𝐸.
Conversely, an ungrounded schema with clone structure can produce a grounded schema when it is
combined with an emission matrix 𝐸that respects its clone structure. This process is called grounding
and corresponds to transfer learning, since the same schema is reused.
Ungrounded schema An ungrounded schema, or simply a schema, is noted as 𝑇, and is obtained
from a grounded schema in which we have removed both the clone structure C, and the emission
matrix𝐸. As before, a schema can be combined with an arbitrary clone structure Cand an emission
matrix𝐸that respects it, to produce a grounded schema. This is an even more flexible form of transfer
learning.
19

--- PAGE 20 ---
Graph schemas as abstractions for transfer learning, inference, and planning
B. Experiment details
In George et al. (2021), it was observed that the convergence of EM for learning the parameters of a
CSCG can be improved by using a smoothing parameter called the pseudocount. The pseudocount is
a small constant that is added to the accumulated counts statistics matrix, which ensures that any
transition under any action has a non-zero probability. This ensures that the model does not have
zero probability for any sequence of observations at test time. For our experiments, unless otherwise
specified, we use a psuedocount of 2×10−3for schema learning (learning 𝑇, with𝐸fixed), and 10−7
for schema matching (learning 𝐸, with𝑇fixed).
B.1. Memory & Planning Game
The game environment is a 4×4grid of symbols in which the agent can navigate in the four canonical
directions by one grid step (up, down, left, right), and collect reward of 1 at a goal location in the grid
(Fig. 2A). Reward is 0 otherwise. Once the agent collects the reward at the current goal, the agent is
placed in a new random position and a new goal symbol is sampled to which the agent must navigate
to collect the next reward. All grid positions have unique symbols and the symbol-position mapping
is randomized at the start of each episode, which lasts for 100 actions. The agent’s observation is
a tuple of the symbol in its current position and the goal symbol. See Ritter et al. (2020) for more
details. We assume knowledge of the collect action function and execute it only when the goal symbol
is reached.
We employ 3 different hard coded exploration policies to cover the observations in a new episode:
(i) random navigation actions, (ii) random navigation actions but limited to (up, right), and (iii)
an optimal set of actions to cover the learned transition graph – a Hamiltonian cycle that repeats
the sequence {up, up, up, right} four times. For a new episode, we take actions based on one of
these policies, learn the emission matrix 𝐸at every step, and plan to reach the current goal using the
current estimate of 𝐸. Planning a path from the current position to a goal position is achieved via
Viterbi decoding in the CSCG. If the probability of reaching the goal is above a certain threshold, we
find the series of actions that lead to the goal.
In the beginning, the agent is uncertain about both its own position and the goal position, so
it executes actions based on one of the hard-coded policies until we are confident of reaching the
goal state from the current state above a probability threshold. If the current goal symbol is not
yet observed in this episode, we execute actions based on the policy, otherwise we compute the
probability of our planned set of actions reaching the goal symbol. As we navigate, the estimate of 𝐸
becomes better and the plans are more likely to succeed. Our planning algorithm takes both these
uncertainties into account. We evaluated for a total of 100 episodes ( 104steps) and the average
reward stays the same after the 9th episode once we learn the schema. Average reward per episode
are reported for 100 episodes from 10th episode onwards i.e., after learning.
B.2. One-Shot StreetLearn
In each episode of this task, the agent is placed in a new neighborhood of a city and needs to move to
a goal location and direct itself to the target, specified by the goal street view image, to collect the
reward. The agent can take one of four actions: move forward, turn left, turn right, collect reward.
After every reward collection, the agent is placed in a new location with a new heading direction
and provided a new goal street view image. Each episode lasts 200 steps and a new neighborhood is
sampled for each new episode. The agent needs to collect as many rewards as possible during each
episode. The StreetLearn environment represents the agent’s perception, a street view image, as a
20

--- PAGE 21 ---
Graph schemas as abstractions for transfer learning, inference, and planning
unique compressed string that we use as our agent’s observation. For the CSCG agent, we follow
an explore and exploit strategy. The agent first navigates every action from every observation it
encounters while collecting the rewards as it encounters the goals. This is guided exploration to
cover every possible edge in the transition graph. A CSCG model is then learned from the sequence of
observations and actions experienced in this episode. For any subsequent step, we find the closest
path from the agent’s current observation to the goal observation and execute the actions, re-planning
at every step to account for any mismatch between the model expectation and reality.
B.3. Schema matching
B.3.1. Schema matching in simulated 3D environments
In our simulated 3D environments, the agent receives egocentric observations that are 64×64RGB
images. The agent’s action space is discretized to 3 egocentric actions: go forward by 1m, rotate 90◦
left, and rotate 90◦right. We convert the input RGB images into categorical observations using a
𝐾-means vector quantizer.
Figure S1|The top row shows the isometric views of example 3D simulated environments used in
our schema matching experiments. The bottom row shows the transition graphs of CSCGs trained on
the respective 3D environments. Note that each location in the room is represented by four nodes in
the transition graph corresponding to the four possible heading directions of the agent.
For the schema matching experiments with 3D environments, we used the six rooms shown in Fig.
S1. These were constructed with a variety of features, including ramps and platforms that introduce
one-way transitions (e.g. dropping off of a ledge). For each of these environments, we trained a
CSCG using the following steps. First, we collected action-observation pairs from a random walk of
length100,000steps. Then, we clustered the egocentric observations from the walk using a 𝐾-means
quantizer, with 𝐾=50. We used EM to train a CSCG on these discretized observations and actions as
described in §A.1. The learned transition graphs for the six example rooms are also shown in Fig. S1.
Note that learning is not always perfect. Some of the transition graphs have spurious edges. These
imperfections, however, do not affect our schema matching results.
Fig. S2 shows the schema recognition results in these 3D rooms. Given observations from a test
room and a hypothesized schema (ungrounded and without clone structure), we learn the emission
bindings as described in §A.4. We compute the negative log likelihood of the observations according to
Eq. 17 in §A.2. We observed that it takes typically between 500to1000steps to correctly identify the
matching schema in each room, although this is higher in a couple cases. In particular, disambiguation
of schemas 5 and 6 required closer to 2000 steps.
21

--- PAGE 22 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Figure S2|Each column shows an example test room (top) and the negative log-likelihood (NLL) of
observations from that room under different schemas (bottom). Error bars are SEM. Note that the
correct schema was identified (lowest NLL) in all 6 test rooms.
room type
size
cylinder rectangle square with hole torus U-shapesmall medium large4 x 6
6 x 8
9 x 114 x 6
6 x 8
9 x 117 x 7
9 x 9
13 x 134 x 6
6 x 8
9 x 117 x 7
9 x 9
13 x 13
Figure S3|Rooms of different types and sizes used in the schema matching experiments with size
variations.
B.3.2. Schema matching in 2D rooms with size variation
We generated rooms of five different types, and three size variations per type, as shown in Fig. S3.
We selected the room and barrier dimensions such that the rooms have similar number of accessible
states. For the torus and cylinder rooms, we use the same observation map as in the rectangular room
case. However, actions wrap around the top-bottom edges for the cylinder, and the top-bottom and
left-right pair of edges for the torus, as indicated by the blue arrows in Fig. S3. We learned schemas on
the medium size variation of these room types. For each training room, we used action-observation
sequences from a random walk of length 50,000steps to train the respective CSCG.
For evaluating schema matching, we used test rooms of all three size variations. Importantly, we
22

--- PAGE 23 ---
Graph schemas as abstractions for transfer learning, inference, and planning
generated the test rooms by permuting the observations from the original room. As a consequence, at
test time, our model would have to relearn the observation mapping.
In Fig. S4, we show the schema matching performance with the use of clone-structure. We observe
that it takes at most 95steps to identify the correct schema for all test rooms when using clone
structure. We obtained these results using the paired T-test over 25 random walks, per test room,
and a p-value threshold of 0.05. We also evaluated the schema matching performance without clone
structure and found that it takes at most 255steps to find the best matching schema when the clone
structure is ignored.
Figure S4|Schema matching in novel environments with size and observation variations. We measure
the negative log likelihood (NLL) of observation sequences in a given room under different schemas
as the agent does random walks. The schema with the least negative log likelihood (conversely the
highest likelihood) is considered the best match. Each panel corresponds to a room of type and size
indexed by the column and row headers, respectively. Schemas are learned in medium sized rooms
and tested on two other size variations. Error bars are 95% CI of the SEM.
B.3.3. Schema matching in MNIST digit rooms
In this experiment, we used a set of ten digits from the binarized MNIST dataset as our room layouts
as they provide an interesting variety of shapes and topologies. All the foreground pixels are treated
as accessible locations; background pixels are treated as obstacles. Observations at each pixel are
sampled in a similar way to the 2D rooms with aliased interiors (Fig. S5A). We learn schemas for 10
selected MNIST-digits using action-observation pairs from random walks of length 100,000steps
in each room. We repeated our schema matching procedure as before in this new set of rooms by
keeping the room structures constant but with new observations. Fig. S5 shows the negative log
23

--- PAGE 24 ---
Graph schemas as abstractions for transfer learning, inference, and planning
00 2502
00 2502
00 2502
00 2502
00 2502
00 2502
00 2502
00 2502
00 2502
00 2502
no. of steps no. of steps no. of steps no. of steps no. of stepsNLL NLL
test room = 0 test room = 1 test room = 2 test room = 3 test room = 4
test room = 5 test room = 6 test room = 7 test room = 8 test room = 9schema
digit
0
1
2
3
4
5
6
7
8
9
A  MNIST rooms
B  Schema recogintion in MNIST rooms
Figure S5|A. MNIST rooms constructed using the binarized-MNIST dataset, with the observations
mapped as colors. B. Negative log likelihood (NLL) of observation sequences from a given MNIST
room under different schemas. Error bars are 95% CI of the SEM.
likelihoods of observation sequences in test rooms under different schemas. The correct schema was
identified in at most 50steps in all test rooms.
B.4. Schema matching in composed environments
To demonstrate how schemas could be used in more complex environments composed of multiple
schemas, we constructed test rooms composed of pairs of overlapping MNIST digits. As an agent
randomly explores a composed room, we compute the likelihood of a sliding window of action-
observation pairs under each schema. Note that we use a sliding window here, instead of the entire
history of observations, because we are interested in identifying schema the agent is currently in.
The probability of being in a particular schema can then be computed using the softmax of the
log-likelihoods. The details of schema matching in composed MNIST rooms is presented in Algorithm
1. Heatmaps in Fig. S6 show the estimated probability of being in one of the two schemas at every
location in a composed room, as the agent performs a random walk of length 20,000steps and sliding
window of length 200steps. We also compute the accuracy of identifying the correct schema at all
locations by thresholding these probabilities. The average accuracy of identifying the correct schema
at all locations in all composed test rooms is 88.4±0.9%. See Table S1 for results on individual
composed rooms.
Digits 2, 3 5, 1 5, 3 2, 7 0, 9 9, 3 4, 6 8, 7
Acc. 87.6±484.3±391.1±392.5±287.9±385.2±485.9±390.7±4
Table S1|Accuracy (Mean±Standard deviation, in %) of the schema predictions as an agent moves
across various locations in composed MNIST rooms. We use the ground truth locations to compute
this accuracy, which is averaged over five trials of random walks in each room. Note that for the
composed room experiments, the agent gets two schemas as input.
24

--- PAGE 25 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Algorithm 1 Schema selection for composed rooms
InputList of schemas 𝑆1,...𝑆𝐻, observations 𝒙and actions 𝒂of length𝑁in the test room, window
length𝑤.
1:𝑛←𝑤
2:while𝑛 <=𝑁do
3:Estimate𝐸𝑗using𝑥𝑛−𝑤,...,𝑥𝑛,𝑎𝑛−𝑤,...,𝑎𝑛for each schema, 𝑗∈{1,...,𝐻}
4:Compute𝐿𝑛,𝑗: log likelihood of 𝑥𝑛−𝑤,...,𝑥𝑛under schema 𝑗
5:Compute 𝑝𝑛,𝑗=softmax(𝐿𝑛)𝑗: the probability of being in schema 𝑗at time step 𝑛.
6:Select schema with highest 𝑝𝑛,𝑗.
7:end while
Prob. of being in digit 2 Prob. of being in digit 5 Prob. of being in digit 5 Prob. of being in digit 2
Prob. of being in digit 0 Prob. of being in digit 9 Prob. of being in digit 4 Prob. of being in digit 8
0.4
0.30.40.750.30.70.7
0.35 0.350.7
0.65
0.350.6
0.20.70.75
probability
probability
probability
probabilityprobability
probability
probability
probability
Figure S6|Each heatmap represents the probability of an agent being in one of two schemas at each
location in an MNIST composed room, averaged over all instances of the agent being at that location
during a random walk of length 20,000steps.
B.5. Learning using schema compositionality
We generated novel environments by composing rooms with known schemas. For each individual
schema, we specify its frontiers as potential exits and entries: (i) list of (state, action) tuples that
specify how the agent can exit the schema, and (ii) list of entry states from which an agent can
enter the schema. We pre-specify schemas and frontiers for our experiments, but schemas and their
frontiers can potentially be learned from training data (e.g. as repeating motifs on a set of training
environments using graph clustering methods and metrics such as betweenness centrality). Once the
schemas and frontiers are specified, a novel composed environment is thus a combination of different
regions corresponding to known schemas that are connected by their frontiers.
To learn a model of the composed environments, we use the observations and actions collected
during random walks. We first create a new CSCG model with a joint transition tensor composed of
known schemas as a block-diagonal tensor, in which each block corresponds to a copy of one schema.
We then connect exit frontier states of a given schema copy to every other entry frontier (state) in
every other schema with a uniform probability of such transitions conditioned by the corresponding
frontier actions for those exit states. See Fig. 3 (main text) for examples of graphs representing such
joint transition tensors. The procedure for learning the emission matrix and the joint transition tensor
using action-observation pairs in a composed environment is described in Algorithm 2.
For the schema composition experiment, we used schemas for the six environments that were used
in the 3D schema matching experiment ($B.3.1, and tested learning in a larger environment that was
25

--- PAGE 26 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Algorithm 2 Learning a new CSCG model with schema compositionality
InputList of schemas 𝑆1,...,𝑆𝐻, frontiers 𝐹𝑆𝐴1,...,𝐹𝑆𝐴 𝐻, observations 𝒙, actions 𝒂, and pseudocount
𝛼
Output CSCG model of the room as 𝑇and𝐸.
1:𝑇←block diagonal matrix (𝑆1,...𝑆𝐻) ⊲Initialize𝑇
2:for𝑖in1,...,𝐻do ⊲Add frontier connections
3:forexit frontier tuple (𝑠,𝑎)in𝐹𝑆𝐴𝑖do
𝑇[𝑎,𝑠,𝑠𝑘]←1
|{𝑠𝑘∈𝐹𝑆𝐴\𝐹𝑆𝐴𝑖}|∀𝑠𝑘∈{𝐹𝑆𝐴\𝐹𝑆𝐴𝑖}
4:end for
5:end for
6:𝐸←Uniform distribution ⊲Initialize𝐸
7:Use EM with fixed 𝑇to learn𝐸from(𝒙,𝒂) ⊲Learn𝐸with initial 𝑇fixed
8:Use EM with 𝐸fixed to learn 𝑇with pseudocount 𝛼 ⊲ Learn𝑇with learned 𝐸fixed
9:Use Viterbi learning with current 𝐸fixed to refine 𝑇 ⊲ Refine𝑇
a composition of four of these rooms. Because this complex environment, with one-way transitions in
some places, was difficult to cover effectively with a random walk, we opted to use a more efficient
walk generation procedure to obtain data for learning. This procedure utilized ground-truth position
knowledge to navigate, on every step, towards the nearest unexplored (or least explored) transition.
Using this procedure, we were able to cover the entire composed environment in about 10,000steps
and generate data that was relatively balanced in its coverage of the different parts of the environment
for a wide range of walk lengths.
Wefirstcollectedefficientwalkdataof 50,000stepsforthemulti-roomenvironment,andclustered
image observations using 𝐾-means with 𝐾=80. We constructed exact schemas for each of the six
smaller rooms, and used these to construct the initial joint transition tensor with six blocks, connecting
schema exit states to entrance states of every other schema (always for the forward action only). The
joint transition tensor was then used as the starting point for the learning algorithm (Algorithm 2),
which involved learning both 𝐸and𝑇for the multi-room environment.
We experimented with learning over different random walk lengths, from 1,000up to30,000.
These walks were subsets of the full efficient walk data that was collected. We computed mean NLL
across 10 test random walks, each of length 10,000steps, extracted from a separate random walk
dataset that was not used for training. As seen in Fig. 3B, with 5,000steps, we observed several
spurious edges in the learned composite transition graph. The NLL of test data using this model
was 0.73. With 10,000steps, we were able to perfectly recover the joint transition graph. The NLL
of test data using this learned model was 0.17. In contrast, learning the CSCG for the multi-room
environment without schemas, with only 10,000steps (test NLL = 4.27) or even 30,000steps (test
NLL = 1.21) was not feasible. A comparison of the NLL of test data using CSCGs trained with and
without schemas as a function of training data size is shown in Fig. 3B.
We also performed the schema composition experiment using 2D rooms from §B.3.2. In Fig. S7,
we visualize the models learnt with and without schemas for different walk lengths.
B.6. Rapid inference and planning using schemas
Rapid schema matching and binding enables planning in novel environments with limited experience.
We evaluate planning to reach goals using schemas in test rooms with different size and shape
variations. We considered two test room types – rectangle and square with a hole. The schemas
correspond to the medium size version of these rooms (same as in §B.3.2). We add an additional
26

--- PAGE 27 ---
Graph schemas as abstractions for transfer learning, inference, and planning
Figure S7|Visual comparison of CSCG training with (schema composition) and without (full CSCG
learning) schemas, as a function of the length of the training sequence, in an example composed 2D
room.
Manhattan distance to goalManhattan distance to goalRectangle schema of medium size Square with hole schema of medium size
k,
room size = (6+k)x(8+ k)k,
room size = (6+k)x(8+ k)k, room size = (9+ k)x(9+ k)
hole size = (3+k//2) x (3+ k//2)k, room size = (9+ k)x(9+ k)
hole size = (3+k//2) x (3+ k//2)No. of plans
No. of plans
w/o smoothing
with smoothing
w/o smoothing
with smoothing
w/o smoothing
with smoothing
w/o smoothing
with smoothing
-2 0 2 46 8-2 0 2 46 8 -2 0 2 46 8-2 0 2 46 80246810
0246810
18
18
FigureS8|Shortcutfindingperformanceinrectangleandsquarewithaholeroomlayoutsasmeasured
by the Manhattan distance from the goal and the number of plans/re-plans to reach the goal, as a
function of room size. Error bars are 95% CI of the SEM.
smoothing term 𝜆max(𝑇)to the diagonal elements of the transition tensors of each schema. This
diagonal smoothing term controls the self-transition probability at each node for each action. This is
especially important for generalization to size and shape variations as the previously learned schemas
can be rigid. For these experiments, we used 𝜆=0.2.
For each room size and shape, we generate a sequence of random actions and repeat every action
3 times. We select a starting location corresponding to a unique observation in the room and execute
these actions up to 200 steps and collect the observations. Same action repetition creates trajectories
where the agent walks farther from the start position with less opportunity to fully explore the
surrounding area. The agent is then tasked with going back to the start location of the walk by
planning a shortest path back to the start location from the current location. Based on the matched
schema, we first learn the emission bindings from the observation-action pairs. We then use the
grounded schema (i.e., schema + learned emissions) to Viterbi decode the the current state and the
start state, and plan the path back to start state in the model using max-product message-passing.
After executing every action of this plan, we collect the observations and re-plan when the current
plan fails to take us the goal state. We repeat this for 50such random walks in each test room. Fig.
S8 shows the shortcut planning performance for the two example room types.
27
