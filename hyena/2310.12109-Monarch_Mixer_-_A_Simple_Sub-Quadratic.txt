# 2310.12109.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/hyena/2310.12109.pdf
# File size: 2145562 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Monarch Mixer : A Simple Sub-Quadratic
GEMM-Based Architecture
Daniel Y. Fu1, Simran Arora∗,1, Jessica Grogan∗,2, Isys Johnson∗,2, Sabri Eyuboglu∗,1,
Armin W. Thomas∗,3, Benjamin Spector1, Michael Poli1, Atri Rudra2, and Christopher R´ e1
1Department of Computer Science, Stanford University
2Department of Computer Science and Engineering, University at Buffalo, SUNY
3Department of Psychology, Stanford University
Contact Email: danfu@cs.stanford.edu
October 18, 2023
Abstract
Machine learning models are increasingly being scaled in both sequence length and model
dimension to reach longer contexts and better performance. However, existing architectures
such as Transformers scale quadratically along both these axes. We ask: are there performant
architectures that can scale sub-quadratically along sequence length and model dimension? We
introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive
along both sequence length and model dimension: Monarch matrices, a simple class of expressive
structured matrices that captures many linear transforms, achieves high hardware efficiency on
GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2
in three domains: non-causal BERT-style language modeling, ViT-style image classification,
and causal GPT-style language modeling. For non-causal BERT-style modeling, M2matches
BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and
achieves up to 9.1 ×higher throughput at sequence length 4K. On ImageNet, M2outperforms
ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce
a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To
alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on
multivariate polynomial evaluation and interpolation, which lets us parameterize M2to be
causal while remaining sub-quadratic. Using this parameterization, M2matches GPT-style
Transformers at 360M parameters in pretraining perplexity on The PILE—showing for the first
time that it may be possible to match Transformer quality without attention or MLPs.
1 Introduction
Machine learning models in natural language processing and computer vision are being stretched to
longer sequences and higher-dimensional representations to enable longer context and higher quality,
respectively [ 4,8,61,81]. However, existing architectures exhibit time and space complexities that
grow quadratically in sequence length and/or model dimension—which limits context length and
makes scaling expensive. For example, attention and MLP in Transformers scale quadratically in
sequence length and model dimension [ 13]. In this paper, we explore a natural question: can we
find a performant architecture that is sub-quadratic in both sequence length and model dimension?
∗Equal contribution.
1arXiv:2310.12109v1  [cs.LG]  18 Oct 2023

--- PAGE 2 ---
P0PiM =
Order-p Monarch Matrices( )ii = 1p
def M2_layer(X):
  # mix sequence
  Z = M @ (k * (M @ X))
  
  # mix channels
  Y = M @ σ(M @ Z.T)) 
  
  return Y
Simple LayersN
d
N
d
Eﬃcient Mixing on
Sequence, DimensionΠ
Subquadratic: O(pN(p+1)/p)
Hardware-Eﬃcient (GEMMs)
Expressive (generalizes FFT)Figure 1: Monarch matrices are a simple, expressive, and hardware-efficient class of sub-quadratic
structured matrices. Monarch Mixer (M2) uses Monarch matrices to mix inputs first along
the sequence dimension and then along the model dimension. See the Appendix for PyTorch
implementation of an M2layer.
In our exploration, we seek a sub-quadratic primitive for both the sequence length and model
dimension. Our framing takes inspiration from work such as MLP-mixer [ 72] and ConvMixer [ 56],
which observed that many machine learning models operate by repeatedly mixing information
along the sequence and model dimension axes, and used a single operator for both axes. Finding
mixing operators that are expressive, sub-quadratic, and hardware-efficient is challenging. For
example, the MLPs in MLP-mixer and convolutions in ConvMixer are expressive, but they both scale
quadratically in their input dimension [ 56,72]. Several recent studies have proposed sub-quadratic
sequence mixing with long convolutions or state space models [ 25,63,75]—both computed using
the FFT—but these models have poor FLOP utilization (3-5% [ 26]) and maintain quadratic scaling
in model dimension. Meanwhile, there has been promising work in sparsifying dense MLP layers
without losing quality, but some of the models can actually be slower than their dense counterparts,
due to low hardware utilization [5, 6, 12, 24, 33].
We turn to an expressive class of sub-quadratic structured matrices called Monarch matrices [12]
(Figure 1 left) to propose Monarch Mixer (M2). Monarch matrices are a family of structured
matrices that generalize the fast Fourier transform (FFT) and have been shown to capture a
wide class of linear transforms including Hadamard transforms, Toeplitz matrices [ 30], AFDF
matrices [ 55], and convolutions. They are parameterized as the products of block-diagonal matrices,
called monarch factors , interleaved with permutation. Their compute scales sub-quadratically:
setting the number of factors to presults in computational complexity of O(pN(p+1)/p) in input
length N, allowing the complexity to interpolate between O(NlogN) atp=logNandO(N3/2) at
p= 2.1
M2uses Monarch matrices to mix information along the sequence and model dimension axes.
It is both simple to implement and hardware-efficient: the block-diagonal Monarch factors can be
computed efficiently on modern hardware using GEMMs (generalized matrix multiply algorithms).
Our proof-of-concept implementation of an M2layer, written in less than 40 lines of pure PyTorch
(including imports), relies only on matrix multiplication, transpose, reshape, and elementwise
products (see pseudocode in Figure 1 middle) and achieves 25.6% FLOP utilization2for inputs
of size 64K on an A100 GPU. On newer architectures such as the RTX 4090, a simple CUDA
implementation achieves 41.4% FLOP utilization at the same size.
1Monarch matrices were originally [12] parameterized with p= 2, but the general pcase is a natural extension.
2For context, the most optimized attention implementations achieve 25% FLOP utilization, while unoptimized
implementations of attention can have as low as 10% FLOP utilization [13].
2

--- PAGE 3 ---
Non-Causal Settings As a first proof of concept of M2, we evaluate how it compares to
Transformers in terms of speed and quality in non-causal settings such as BERT-style masked
language modeling [ 19] and ImageNet classification. We introduce M2-BERT, which replaces the
attention blocks in BERT with bidirectional gated convolutions implemented using Monarch matrices
and replaces the dense matrices in the MLP with Monarch matrices. M2-BERT reduces parameter
count but maintains quality—matching BERT-base and BERT-large in downstream GLUE quality
with 27% and 24% fewer parameters, respectively. Sub-quadratic scaling in sequence length enables
high throughput at longer sequences—up to 9.1 ×higher throughput at sequence length 4K than
HuggingFace BERT, and 3.1 ×higher throughput at sequence length 8K than BERT optimized with
FlashAttention [13].
For image classification, we adapt HyenaViT-b [63], an attention-free vision transformer based
on gated convolutions. We replace the convolution operation with M2primitives and replace the
MLP layers with an M2block as well. These changes reduce the parameter count compared to
a ViT-b [ 20] model with the same model width and depth by a factor of 2. Surprisingly, despite
this parameter reduction, we find that M2slightly outperforms ViT-b and HyenaViT-b baselines,
achieving 1% higher accuracy on ImageNet [16].
Causal Settings Causal settings such as GPT-style [ 64] auto-regressive language modeling present
a technical challenge: masking out the upper triangular elements in an attention matrix (or equivalent
structure) introduces a quadratic bottleneck. To alleviate this quadratic bottleneck with Monarch
matrices, we develop new theory to characterize which parameterizations of Monarch matrices
maintain causality. To do so, we take a view of p-order Monarch matrix multiplication as p-variate
polynomial evaluation and interpolation (e.g., p= 2 factors corresponds to bivariate polynomials,
Figure 2 left). Using this view, we show that the M2convolution shown in Figure 1 (middle) can
be viewed as manipulation of modular polynomial multiplication. This result allows us to develop
conditions (Theorem 3) under which M2is causal. We can use this causal parameterization to
outperform GPT-style language models on causal language modeling by 0.2 PPL points on the
PILE at model size 360M–without using either attention or MLP blocks.
Summary Overall, our results present a potential path to building machine learning models with
sub-quadratic primitives. We hope our work can serve as a starting point to explore models that
are more efficient in both sequence length and model dimension.
2 Preliminaries
In this section, we provide some background on the key components behind the cost of operations
on GPUs, and then discuss the scaling characteristics of some common primitives used to mix
information across the sequence dimension and model dimension in modern machine learning models.
GPU Accelerator Cost Model We provide a brief discussion of relevant factors affecting runtime
performance of deep learning operations on GPUs. Depending on the balance of computation and
memory accesses, operations can be classified as either compute-bound or memory-bound [ 42]. In
compute-bound operations, the time accessing GPU memory is relatively small compared to the
time spent doing arithmetic operations. Typical examples are matrix multiply with large inner
dimension, and short convolution kernels with a large number of channels.
The speed of these operations is determined by the FLOP/s available on compute units, and the
number of FLOPs necessary to complete the operation. In our paper, we exploit fast matrix multiply
3

--- PAGE 4 ---
units such as tensor cores. On the A100, tensor cores can achieve 312 TFLOP/s in half-precision
matrix multiply operations, while non-matrix multiply operations are limited to 19 TFLOP/s [ 58].
This trend began with tensor cores in the V100 [ 57], and is continuing into the next-generation
H100 chips [59].
In memory-bound operations, the time taken by the operation is determined by the number
of memory accesses, while time spent in computation is much smaller. Examples include most
elementwise operations (e.g., activation, dropout) and reductions (e.g., sum, softmax, batch norm,
layer norm).
The runtime of memory-bound operations is determined by the memory bandwidth of different
layers of the memory hierarchy . GPU memory is large but relatively slow—up to 80 GB on A100,
but with bandwidth of 2 TB/s [ 58]. Higher levels of the memory hierarchy such as caches are much
smaller (20 MB) but an order of magnitude faster (19 TB/s).
Table 1: FLOP cost and utilization of
various mixer layers, input dimension
64K on an RTX 4090.
Layer FLOP Cost Util
MLP N295.5%
FlashAttn N224.0%
FFT NlogN 3.0%
M2Conv N3/241.4%Common Mixer Primitives To help contextualize our
work, we provide scaling and hardware utilization char-
acteristics for a few common operations that are used to
mix information in machine learning models, summarized
in Table 1.
Transformers [ 73] use attention to mix information
across the sequence dimension, and MLP blocks to mix
information across the model dimension. Both of these
blocks scale quadratically in input length. MLP layers are
compute-bound, so they have high FLOP utilization out
of the box. Attention blocks are memory-bound, so even
the most optimized implementations such as FlashAttention [13] have relatively lower FLOP
utilization.
Recent work has made progress towards attention-free models by replacing attention layers with
long convolution layers, interleaved with elementwise gating [ 25,26,34,52,63,66–68]. These layers
are computed using FFT operations using the FFT convolution theorem:
y=K∗X=FFT−1(FFT (X)∗FFT (K))
While the FFT scales asymptotically well in O(NlogN), it is often memory-bound and thus has
low FLOP utilization. In our work, we aim to construct a mixer that has both sub-quadratic scaling
and high FLOP utilization.
3 Monarch Mixer
In this section, we recall Monarch matrices, introduce how M2uses Monarch matrices to mix
along the sequence and model dimensions, and benchmark a M2convolution in terms of hardware
utilization.
3.1 Monarch Matrices
Monarch matrices [ 12] are a sub-quadratic class of structured matrices that are hardware-efficient
and expressive. They can represent many linear transforms, including convolutions, Toeplitz-like
transforms, low-displacement rank transforms, and orthogonal polynomials. Directly implementing
these different structured transforms on GPUs as dense matrices can be inefficient. In contrast,
4

--- PAGE 5 ---
Table 2: FLOP cost and utilization of M2compared to dense MLP at different input sizes N, with
block size√
N, on an A100 and RTX 4090.
N 4K 16K 64K 256K
Dense Matmul TFLOP Cost 0.025 0.412 6.60 106.0
M2TFLOP Cost 0.002 0.013 0.103 0.824
Dense FLOP Utilization (A100) 63.0% 78.0% 80.0% OOM
M2FLOP Utilization (A100) 4.78% 12.7% 25.6% 42.8%
Wall-Clock Speedup (A100) 1.2 × 5.1× 20.6×>55.0×
Dense FLOP Utilization (4090) 74.6% 96.7% 98.0% OOM
M2FLOP Utilization (4090) 11.1% 32.1% 41.4% 53.7%
Wall-Clock Speedup (4090) 2.2 × 10.5×27.0×>69.1×
their Monarch decompositions can be computed by interleaving matrix multiplications with tensor
permutations.
A Monarch matrix M∈RN×Nof order pis defined by the following:
M= pY
i=1PiBi!
P0, (1)
where each Piis related to the ‘basep√
N’ variant of the bit-reversal permutation, and Biis a
block-diagonal matrix with block size b. Setting b=p√
Nachieves sub-quadratic compute cost. For
example, for p= 2, b=√
N, Monarch matrices require O(N3/2) compute in sequence length N.
In this paper, we use Monarch matrices to construct architectures that are sub-quadratic in both
sequence length Nand model dimension d. We will often parameterize order-2 Monarch matrices,
written as M=PLPRP , where LandRare block-diagonal matrices (for “left” and “right”), and
P=P2=P1=P0is a permutation that reshapes the input to 2D, transposes it, and flattens it to
1D. A common case is to set L=R= (I√
N⊗F√
N), where F√
Nis a√
NDFT matrix, and ⊗is
the Kronecker product.
3.2 Monarch Mixer Architecture
We describe how Monarch Mixer uses Monarch matrices and elementwise operations to construct
sub-quadratic architectures (Figure 1 middle). We take a mixer view of model architectures, where
each layer is a sequence of mixing operations across the sequence and the model dimension axes.
Each layer takes as input a sequence of embeddings X∈RN×d, and outputs a sequence Y∈RN×d,
where Nis the sequence length, and dis the model dimension. For simplicity, we show the order-2
case here, though we can use higher-order blocks to scale to longer sequences and larger model
dimensions.
LetM1,M2∈RN×NandM3,M4∈Rd×dbe order-2 Monarch matrices, let K1∈RN×d, letσ
be an optional point-wise non-linearity ( e.g.ReLU), and let ⊙be elementwise multiplication. M2
uses Monarch matrices to construct expressive architectures. For example, a convolutional block
with a sparse MLP can be expressed as follows:
1. Mix along sequence axis:
˜X=M2(K1⊙M1X) (2)
2. Mix along embedding axis:
Y⊤=M4σ(M3˜X⊤) (3)
5

--- PAGE 6 ---
P P P M-1(Mu    Mk)Causal
Parameterization/uni2113(X, Y) r(Y)
Conditions on Bivariate
Polynomial DegreesUnivariate Multiplication
deg(f), deg(g) < N / 2 Causal Mapf(Z) g(Z) mod ZN{
{
{Figure 2: Monarch multiplication can be interpreted as polynomial evaluation and interpolation.
We derive sufficient conditions on the polynomial formulation of Monarch matrices for M2to be
causal.
When M1is set to the DFT and M2is set to the inverse DFT, Equation 2 exactly corresponds
to a convolution with kernel K1parameterized in frequency space. Equation 3 corresponds to an
MLP with the dense matrices replaced by Monarch matrices. More expressive layers are also easily
expressible; for example, replacing Equation 2 with V⊙M2(K1⊙M1(Q⊙K)), where Q,K,Vare
linear projections of X, reproduces a gated convolution block, as in [25, 26, 63].
The basic M2layer is simple to implement; pseudocode is shown in Figure 1 (middle), and the
Appendix gives an efficient implementation of M2in under 40 lines of pure PyTorch (including
imports). The convolution case with Monarch matrices fixed to DFT and inverse DFT matrices
also admits implementations based on FFT algorithms [9].
3.3 Architecture Benchmarks
We benchmark the efficiency of the M(K⊙MX) convolution operator (Equation 2) implemented in
a simple CUDA kernel (calling standard cuBLAS sub-routines [ 60]), as the dimension Nincreases.
Equation 3 scales similarly, as dimension dincreases. We keep the block size bfixed to√
N.
Table 2 shows the FLOP cost and utilization of a M2operator as a function of the input size
on an A100 as well as on an RTX 4090. On the A100, the operator is more dominated by the
data movement costs of the permutation operations (see the Appendix for a roofline analysis).
For longer inputs, the sub-quadratic scaling allows Monarch Mixer to outperform dense matrix
multiplication. On the RTX 4090, which has a larger and faster L2 cache than the A100, we can
manually optimize an implementation to amortize data movement costs.
4 Theoretical Analysis: M2as Polynomial Multiplication
In this section, we develop theory to make the M2layer causal in the input X—e.g., ensure that an
output Yiof the M2should only depend on X1, ...,Xi. Our approach involves interpreting Monarch
matrix multiplication as multivariate polynomial evaluation and interpolation. We then show that
anM2convolution is equivalent to modular polynomial manipulation in a univariate basis.
The challenge is controlling the degrees of the resulting univariate polynomials, to prevent
“underflow” under modular multiplication (see Figure 2 for an overview). Our key result is deriving
sufficient conditions on the degrees of the bivariate polynomials defining the Monarch factors to
prevent such underflow. We focus on the bivariate case (order p= 2) in the body, and give the
general multivariate case in the Appendix. We present proof sketches in the main body, and leave
proofs and additional results for the Appendix.
6

--- PAGE 7 ---
Monarch Multiplication as Polynomial Evaluation First, we show that order-2 Monarch
matrix-vector multiplication M·uis equivalent to bivariate polynomial evaluation.
Fix a Monarch matrix M∈RN×N=PLPRP , for two block-diagonal matrices LandRwith
blocks of size b=√
N. We can interpret Monarch matrices as bivariate polynomial evaluation by
setting A={ω0, . . . , ω b−1}as a set of evaluation points (e.g., the bth roots of unity), and letting
{ℓ0(X, Y), . . . , ℓ b−1(X, Y)},{r0(Y), . . . , r N−1(Y)}be sets of basis polynomials with individual
degrees of X, Y being <√
N. The values of {ℓ0(X, Y), . . . , ℓ b−1(X, Y)}evaluated on A2determine
the entries of L, and the values of {r0(Y), . . . , r N−1(Y)}evaluated on Adetermine the entries of R.
We give the mapping from ℓ, r,andAtoLandRin the Appendix.
Then, matrix-vector multiplication between Mand a vector uis equivalent to polynomial
evaluation of the basis functions ℓ, ron the evaluation points A2:
Theorem 1. Letm(j) =jmod√
N. For any vector u∈RN,Mu is a bivariate polynomial
u(X, Y)evaluated at A2, with u(X, Y) =PN−1
j=0ujfj(X, Y),where fj(X, Y) =ℓm(j)(X, Y)rj(Y).
Monarch Inverse as Polynomial Interpolation Next, we exploit the fact that Monarch inverse
multiplication M−1·uis equivalent to polynomial interpolation in the basis polynomials of M.
Theorem 2. LetM0,M1,M2be Monarch matrices, and let Abe the set of√
Nroots of unity.
Then, the operation
f=M−1
0((M1k)⊙(M2u)). (4)
is equivalent to representing the polynomial
h(X, Y) =k(X, Y)u(X, Y) mod ( X√
N−1, Y√
N−1)
in terms of the basis polynomials ℓ, rcorresponding to M0, and where k(X, Y)andu(X, Y)are the
polynomials corresponding to M1kandM2u, respectively.
The above follows from Theorem 1 and the fact that Monarch matrix-vector multiplication with
an inverse Monarch matrix is equivalent to polynomial interpolation in a given basis. The mod
part comes from the fact that Ais the set of roots of the polynomial Z√
N−1.
Causal Monarch Maps Now, we give a class of Monarch matrices from which we can build a
causal map. First, we define a polynomial with minimum degree j:
Definition 1. A polynomial of minimum degree j(and maximum degree N−1) is defined as
¯qj(Z) =PN−1
a=j¯qj[a]Za.
To ensure causality, we first convert the bivariate polynomial basis into a univariate basis, and
then we expand the degree of the univariate polynomial. The resulting univariate polynomial
multiplication is naturally causal (exploiting similar properties as the causal FFT convolution).
We use the Kronecker substitution ( X←Z, Y←Z√
N) to convert the bivariate polynomial
basis into a univariate basis:
qj(Z) =ℓm(j)(Z)rj
Z√
N
, (5)
where m(j) is defined as in Theorem 1.
Then, the following class of Monarch matrices (with the conversion to univariate polynomial
basis as above) forms a causal map:
7

--- PAGE 8 ---
Theorem 3. Letu,k∈Rn, where n < N/ 2. Let m(j)be as in Theorem 1, and k(j) =j
j/√
Nk
.
Then define the basis polynomials ℓm(j)to have minimum degree m(j), basis polynomials rjto have
minimum degree k(j), and all polynomials qj(Z)to have maximum degree < N/ 2for all j < N/ 2
and for N/2≤j < N have maximum degree N−1. Let MNbe defined by such basis polynomials
via(5)where the evaluation points are now the Nth roots of unity. Then, we have that
u7→ 
M−1
N(MN(k,0N−n)⊙MN(u,0N−n))
[0 :n−1] (6)
gives a causal map in u.
Theorem 3 gives a causal map that can be computed entirely using Monarch matrices – enforcing
causality with sub-quadratic scaling. The main technical ingredient in proving the above result is
that the product qj(Z)qj′(Z) can be written as a linear combination of qa(Z) for j+j′≤a < N
(this uses the above specified properties on the minimum and maximum degrees of qj(Z)). This in
turn implies that the term kj′ujqj(Z)qj′(Z) only contributes to the coefficients of “higher order”
basis polynomials qa(Z) for a≥j+j′in the product k(Z)u(Z), which is needed for causality.
Figure 2 gives an example of restricted polynomials generating a causal map.
5 Experiments
xq k vshort
convshort
convshort
convMonarch
long conv
Monarch
long convy
Sequence
MixerDimension
MixerxGeLUy
GLU
Figure 3: M2-BERT uses Monarch
matrices to create a bidirectional gated
long convolution in the sequence mixer,
and uses Monarch matrices to replace
the linear layers in the dimension mixer.We compare Monarch Mixer to Transformers on three
tasks where Transformers have been dominant: BERT-
style non-causal masked language modeling, ViT-style
image classification, and GPT-style causal language mod-
eling. In each, we show that we can match Transform-
ers in quality using neither attention nor MLPs. We
additionally evaluate wall-clock speedups against strong
Transformer baselines in the BERT setting. Additional
experiments on speech and alternative architectures are
given in Appendix B, and experimental details are given
in Appendix C.
5.1 Non-Causal Language Modeling
We introduce M2-BERT, an M2-based architecture for
non-causal language modeling. M2-BERT acts as a drop-
in replacement for BERT-style language models [ 19], which
are a workhorse application of the Transformer architec-
ture [ 1,37,38,43,46,47,50,54,82,86]. We train M2-
BERT using masked language modeling over C4 [ 65] with
thebert-base-uncased tokenizer.
M2-BERT starts with a Transformer backbone and replaces the attention and MLPs with
M2layers, shown in Figure 3. In the sequence mixer, we replace attention with bidirectional
gated convolutions with a residual convolution (Figure 3 left). To recover convolutions, we set
the Monarch matrices to DFT and inverse DFT matrices. Following [ 25,63], we also add short
depthwise convolutions after the projections. In the dimension mixer, we replace the two dense
matrices in MLPs with learned block-diagonal matrices (Monarch matrix of order 1, b= 4). We
pretrain two M2-BERT-base models, at 80M and 110M, and two M2-BERT-large models, at 260M
and 341M. These are equivalent to BERT-base and BERT-large, respectively.
8

--- PAGE 9 ---
Table 3: Average GLUE Score for M2-BERT-base compared to BERT-base [ 18], along with change
in parameters and GLUE score.
Model GLUE Score ∆Params ∆GLUE Score
BERT-base (110M) 79.6 -0% +0.0
M2-BERT-base (80M) 79.9 -27% +0.3
M2-BERT-base (110M) 80.9 -0% +1.3
Table 4: Average GLUE Score for M2-BERT-large compared to BERT-large [ 18], along with change
in parameters and GLUE score.
Model GLUE Score ∆Params ∆GLUE Score
BERT-large (340M) 82.1 -0% +0.0
M2-BERT-large (260M) 82.2 -24% +0.1
M2-BERT-large (341M) 82.8 +0.2% +0.7
Downstream GLUE Scores First, we evaluate M2-BERT models on downstream fine-tuning
compared to BERT-base and BERT-large from [ 18]. We take the pretrained models and fine-tune
them on BERT, following the procedure in [ 36]. Table 3 shows performance for BERT-base equivalent
models, and Table 4 shows performance for BERT-large equivalent models. M2-BERT-base can
match BERT-base in GLUE quality with 27% fewer parameters—or outperform BERT-base in
quality by 1.3 points when parameter matched. M2-BERT-large matches BERT-large with 24%
fewer parameters, and outperforms by 0.7 points when parameter matched.
GPU Throughput by Sequence Length Next, we evaluate throughput of M2-BERT models
by sequence length, compared to HuggingFace implementations of BERT, as well as optimized
implementations of BERT running FlashAttention [ 13]. Table 5 shows forward throughput for
BERT-base equivalent models, and the appendix shows throughput for BERT-large (where the
performance trends are similar). Inference times are reported in tokens/ms on an A100-40GB GPU.
M2-BERT-base achieves higher throughput than even highly-optimized BERT models, and up to
9.1×faster throughput than a standard HuggingFace implementation at sequence length 4K.
CPU Inference Latency Finally, we report CPU inference latency for M2-BERT-base (80M)
compared to BERT-base, running direct PyTorch implementations for both. In short sequences,
the impacts of data locality still dominate the FLOP reduction, and operations such as filter
generation (which are not present in BERT) pay a higher cost. Starting at sequences 1K and
longer, M2-BERT-base starts to have speedup over BERT-base, up to 6.5 ×at sequence length
8K. We believe further optimization and applying IO-aware principles can further improve CPU
performance.
5.2 Image Classification
To validate that our methods generalize to images as well as language for non-causal modeling, we
next evaluate M2on image classification. We compare M2to ViT-style models and recent work,
HyenaViT-b [ 63], which uses gated long convolutions to replace the attention layers in ViT-b. In
our work, M2-ViT builds off HyenaViT-b and replaces the long convolutions with the M2operator
9

--- PAGE 10 ---
Table 5: Throughput in tokens/ms by context length for M2-BERT-base (80M) compared to
BERT-base.
Model 512 1024 2048 4096 8192
HF BERT-base (110M) 206.1 130.8 71.3 39.0 OOM
FlashAttention BERT-base (110M) 367.4 350.1 257.2 179.1 102.4
M2-BERT-base (80M) 386.3 380.7 378.9 353.9 320.1
M2 Speedup over HF BERT-base (110M) 1.9 × 2.9× 5.2× 9.1× –
Table 6: CPU inference latency in milliseconds with a batch size of 1 at varied input sequence
lengths. Measurements averaged over 10 examples on a 48 vCPU, 96 GB RAM instance from the
GCP n2-standard-48 series, which runs Intel Cascade Lake processors. This is based on the protocol
in [27].
Model 512 1024 2048 4096 8192
BERT-base (110M) 182 389 918 2660 11820
M2-BERT-base (80M) 289 361 651 948 1820
Speedup 0.6 ×1.1×1.4×2.8×6.5×
in Equation 2 (again setting the Monarch matrices to the DFT and inverse DFT). We replace the
MLP blocks in HyenaViT-b with block-diagonal matrices, similarly to M2-BERT. Appendix B
additionally compares M2to the Swin-family of architectures [48, 49].
Table 7 shows the performance of Monarch Mixer against ViT-b, HyenaViT-b, and ViT-b-
Monarch (which replaces the MLP blocks of standard ViT-b with Monarch matrices) on ImageNet-1k.
Monarch Mixer outperforms the other models with only half the parameters of the original ViT-s
model. Surprisingly, Monarch Mixer also outperforms ResNet-152, with fewer parameters—even
though the latter was explicitly designed for ImageNet performance.
5.3 Causal Language Modeling
GPT-style causal language modeling is a critical application for Transformers [ 4,29,41]. We
introduce M2-GPT, a M2-based architecture for causal language modeling. For the sequence mixer,
M2-GPT combines the convolutional filter from Hyena [ 63], the state-of-the-art attention-free
language model, with parameter sharing across multiple heads from H3 [ 25]. We use the causal
parameterization of Equation 2 to replace the FFT in these architectures, and we remove the MLP
layers entirely. The resulting architecture is entirely attention- and MLP-free.
We pretrain M2-GPT on the PILE, a standard dataset for causal language modeling. Following
prior work [ 26,63], we train models at two model sizes, with varying amounts of training data—
decaying the learning rate appropriately for each experiment. Table 8 shows the results. Even though
our model is attention- and MLP-free, it outperforms both Transformers and Hyena in perplexity
on pretraining. These results suggest that radically different architectures than Transformers may
be performant on causal language modeling.
10

--- PAGE 11 ---
Table 7: Accuracy on ImageNet-1k. ResNet-152 provided for reference.
Model Top-1% Top-5% Description
ResNet-152 (60M) 78.6 94.3 ConvNet, MLP
ViT-b (87M) 78.5 93.6 Attention, MLP
ViT-b + Monarch (33M) 78.9 94.2 Attention, MLP-Free
HyenaViT-b (88M) 78.5 93.6 Attention-Free, MLP
M2-ViT-b (45M) 79.5 94.5 Attention-Free, MLP-Free
Table 8: Perplexity on the PILE when trained for different amounts of tokens.
Model 5B 10B 15B Description
Transformer (125M) 13.3 11.9 11.2 Attention, MLP
Hyena (155M) 13.1 11.8 11.1 Attention-Free, MLP
M2-GPT (145M) 12.9 11.6 10.9 Attention-Free, MLP-Free
Transformer (355M) 11.4 9.8 9.1 Attention, MLP
Hyena (360M) 11.3 9.8 9.2 Attention-Free, MLP
M2-GPT (360M) 11.0 9.6 9.0 Attention-Free, MLP-Free
6 Related Work
Long Convolutions Recent work proposes to use long convolution layers as a replacement for
the Transformer attention layers in sequence modeling [ 26,63,66–68]. Many of these models rely
on the FFT convolution theorem to compute the long convolutions. We build on the insights in
many of these architectures in constructing our M2architectures, and additionally replaces the
FFT operations with Monarch matrices.
Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev
bases [ 79] or orthogonal polynomial bases [ 32]. These approaches have analogues in our multivariate
analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be
able to approximate some of these operations. An interesting question for future work would be to
study how well our techniques and concerns about causality and hardware utilization translate to
these alternative convolution bases.
Optimization of deep learning primitives There is a rich history of the optimization of deep
learning primitives, as accelerating their performance can yield substantial savings in compute and
cost for large models. There are many approaches to speed up these operations, but they usually
either reduce data movement or compute.
Reducing data movement : In many applications, the major bottleneck is the storage and move-
ment of large amounts of memory. One popular approach to reducing data movement is checkpointing,
wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are
needed, trading additional compute for memory [ 44,76]. Another approach is kernel fusion, wherein
algorithms initially described as sequential steps can often be fused in ways that improve their prop-
erties. For example, it is generally faster to implement a dot-product through a multiply-accumulate
rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [ 62]
have added kernel fusion capabilities, although the very best performance usually still arises from
11

--- PAGE 12 ---
handwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load
small blocks of memory, do intensive computation on them, and then write the results a tile at
a time [ 80]. Finally, many algorithms also have hand-optimizations that can remove unnecessary
computation or memory accesses [53].
Efficient algorithms usually make use of a combination of these techniques. For example,
FlashAttention [ 13] uses all four to dramatically decrease both the latency and memory consumption
of multi-head attention. Though we have made a modest effort to implement Monarch Mixer
efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques.
Reducing flops : A first target for optimization is the multi-layer perceptron (MLP), owing to
its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this
work [ 5,9,12,14,15,17,24,88]. Attention is also a popular target for optimization. Recently,
a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate
attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the
fact that the attention matrix is extremely sparse at long sequence lengths [ 2,21,22,40,51]. Others
use low-rank approximations of the attention matrix [ 11,77,88] or kernel methods instead [ 7,39].
A subset use a combination of these techniques, such as [ 6,71]. Finally, a third category of methods
[25, 63] aim to replace attention entirely, relying on state-space models [31].
7 Discussion and Conclusion
We explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence
length and model dimension and is hardware-efficient on modern accelerators. We motivate M2from
both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept
investigation into performance on masked language modeling, image classification, and causal
language modeling.
While our initial results are promising, our work is only a first step in this direction. The M2
layer can likely be further optimized with systems optimization techniques such as kernel fusion.
Our work has also not been optimized for inference like more well-established models such as
Transformers, or even more recent models such as state space models. It also remains to be seen
whether M2layers can have as widespread applicability as Transformers. We hope that these can
be fruitful directions for future work.
Acknowledgments
We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and
FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315
(Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No.
N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA,
Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm,
Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown
Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science
and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation,
National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford
Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project:
Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is
authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any
copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed
12

--- PAGE 13 ---
in this material are those of the authors and do not necessarily reflect the views, policies, or
endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government. JG and
AR’s work is supported by NSF grant# CCF-2247014. IJ’s work is supported by an NSF Graduate
Fellowship.
References
[1]Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific
text. arXiv preprint arXiv:1903.10676 , 2019.
[2]Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150 , 2020.
[3]Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet-1k.
arXiv preprint arXiv:2205.01580 , 2022.
[4]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
[5]Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher
R´ e. Pixelated butterfly: Simple and efficient sparse training for neural network models. 2021.
[6]Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R´ e. Scatterbrain:
Unifying sparse and low-rank attention. In Advances in Neural Information Processing Systems
(NeurIPS) , 2021.
[7]Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking
attention with performers. arXiv preprint arXiv:2009.14794 , 2020.
[8]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
[9]James W Cooley and John W Tukey. An algorithm for the machine calculation of complex
fourier series. Mathematics of computation , 19(90):297–301, 1965.
[10]Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical
automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition workshops , pages 702–703, 2020.
[11]Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out
sequential redundancy for efficient language processing. Advances in neural information
processing systems , 33:4271–4282, 2020.
[12]Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander
Liu, Aniruddh Rao, Atri Rudra, and Christopher R´ e. Monarch: Expressive structured matrices
for efficient and accurate training. In International Conference on Machine Learning . PMLR,
2022.
13

--- PAGE 14 ---
[13]Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R´ e. FlashAttention: Fast
and memory-efficient exact attention with IO-awareness. In Advances in Neural Information
Processing Systems , 2022.
[14]Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R´ e. Learning fast
algorithms for linear transforms using butterfly factorizations, 2020.
[15]Tri Dao, Nimit S. Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski,
Atri Rudra, and Christopher R´ e. Kaleidoscope: An efficient, learnable representation for all
structured linear maps, 2021.
[16]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[17]Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without
losing performance. arXiv preprint arXiv:1907.04840 , 2019.
[18]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,
2018.
[19]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. In arXiv:1810.04805 , 2019.
[20]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 , 2020.
[21]Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,
Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of
language models with mixture-of-experts. In International Conference on Machine Learning ,
pages 5547–5569. PMLR, 2022.
[22]William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity. The Journal of Machine Learning Research ,
23(1):5232–5270, 2022.
[23] Wikimedia Foundation. Wikimedia downloads.
[24]Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable
neural networks. arXiv preprint arXiv:1803.03635 , 2018.
[25]Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R´ e.
Hungry hungry hippos: Towards language modeling with state space models. International
Conference on Learning Representations , 2023.
[26]Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri
Rudra, and Christopher R´ e. Simple hardware-efficient long convolutions for sequence modeling.
International Conference on Machine Learning , 2023.
[27] Morgan Funtowicz. Scaling up bert-like model inference on modern cpu - part 1, 2021.
14

--- PAGE 15 ---
[28]Jonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in
one day. arXiv:2212.14034v1 , 2022.
[29] Google. Bard, https://bard.google.com/ . 2023.
[30]Robert M Gray et al. Toeplitz and circulant matrices: A review. Foundations and Trends ®in
Communications and Information Theory , 2(3):155–239, 2006.
[31]Albert Gu, Karan Goel, and Christopher R´ e. Efficiently modeling long sequences with structured
state spaces. arXiv preprint arXiv:2111.00396 , 2021.
[32]Nicholas Hale and Alex Townsend. An algorithm for the convolution of legendre series. SIAM
Journal on Scientific Computing , 36(3):A1207–A1220, 2014.
[33]Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 ,
2015.
[34]Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and
Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951 , 2022.
[35]Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. Augmix: A simple data processing method to improve robustness and uncertainty.
arXiv preprint arXiv:1912.02781 , 2019.
[36] Peter Izsak, Moshe Berchansky, and Omer Levy. How to train bert with an academic budget.
InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing ,
pages 10644–10652, 2021.
[37]Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong
baseline for natural language attack on text classification and entailment. In Proceedings of the
AAAI conference on artificial intelligence , volume 34, pages 8018–8025, 2020.
[38]Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.
Spanbert: Improving pre-training by representing and predicting spans. Transactions of the
Association for Computational Linguistics , 8:64–77, 2020.
[39]Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran¸ cois Fleuret. Transformers
are rnns: Fast autoregressive transformers with linear attention. In International Conference
on Machine Learning , pages 5156–5165. PMLR, 2020.
[40]Nikita Kitaev,  Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.
arXiv preprint arXiv:2001.04451 , 2020.
[41]Jan Koco´ n, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd lo, Joanna
Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. Chatgpt:
Jack of all trades, master of none. arXiv preprint arXiv:2302.10724 , 2023.
[42]Elias Konstantinidis and Yiannis Cotronis. A practical performance model for compute and
memory bound gpu kernels. In 2015 23rd Euromicro International Conference on Parallel,
Distributed, and Network-Based Processing , pages 651–658. IEEE, 2015.
[43]MV Koroteev. Bert: a review of applications in natural language processing and understanding.
arXiv preprint arXiv:2103.11943 , 2021.
15

--- PAGE 16 ---
[44]Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama.
A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation.
Advances in Neural Information Processing Systems , 32, 2019.
[45]Lagrange polynomial. Lagrange polynomial — Wikipedia, the free encyclopedia, 2005. https:
//en.wikipedia.org/wiki/Lagrange_polynomial .
[46]Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and
Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical
text mining. Bioinformatics , 36(4):1234–1240, 2020.
[47]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert
pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.
[48]Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng
Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and
resolution. In International Conference on Computer Vision and Pattern Recognition (CVPR) ,
2022.
[49]Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings
of the IEEE/CVF International Conference on Computer Vision (ICCV) , 2021.
[50]Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, and Bing Xiang. Universal text
representation from bert: An empirical study. arXiv preprint arXiv:1910.07973 , 2019.
[51]Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke
Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing
Systems , 34:2441–2453, 2021.
[52]Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint
arXiv:2209.10655 , 2022.
[53]Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv
preprint arXiv:1805.02867 , 2018.
[54]Derek Miller. Leveraging bert for extractive text summarization on lectures. arXiv preprint
arXiv:1906.04165 , 2019.
[55]Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. Acdc: A structured
efficient linear layer. arXiv preprint arXiv:1511.05946 , 2015.
[56]Dianwen Ng, Yunqi Chen, Biao Tian, Qiang Fu, and Eng Siong Chng. Convmixer: Feature
interactive convolution with curriculum learning for small footprint and noisy far-field keyword
spotting. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pages 3603–3607. IEEE, 2022.
[57] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.
[58] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.
16

--- PAGE 17 ---
[59] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.
[60] NVIDIA. cuBLAS, 2023.
[61] OpenAI. Gpt-4 technical report, 2023.
[62]Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. Advances in neural information processing
systems , 32, 2019.
[63]Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher R´ e. Hyena hierarchy: Towards larger convolutional
language models. International Conference on Machine Learning , 2023.
[64]Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language
understanding by generative pre-training. 2018.
[65]Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. arXiv preprint arXiv:1910.10683 , 2019.
[66]David W Romero, R Bruintjes, Erik J Bekkers, Jakub M Tomczak, Mark Hoogendoorn, and
JC van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In
10th International Conference on Learning Representations , 2022.
[67]David W Romero, David M Knigge, Albert Gu, Erik J Bekkers, Efstratios Gavves, Jakub M
Tomczak, and Mark Hoogendoorn. Towards a general purpose cnn for long range dependencies
in{N}d.arXiv preprint arXiv:2206.03398 , 2022.
[68]David W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogen-
doorn. Ckconv: Continuous kernel convolution for sequential data. In International Conference
on Learning Representations , 2021.
[69]Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit,
and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision
transformers. arXiv preprint arXiv:2106.10270 , 2021.
[70]G. Szeg¨ o. Orthogonal Polynomials . Number v.23 in American Mathematical Society colloquium
publications. American Mathematical Society, 1967.
[71]Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.
ACM Computing Surveys , 55(6):1–28, 2022.
[72]Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas
Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-
mixer: An all-mlp architecture for vision. Advances in neural information processing systems ,
34:24261–24272, 2021.
[73]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017.
17

--- PAGE 18 ---
[74]Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding.
arXiv:1804.07461 , 2018.
[75]Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without
attention. arXiv preprint arXiv:2212.10544 , 2022.
[76]Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang Yuan, Xin Jin, Gang Huang,
Yunxin Liu, and Xuanzhe Liu. Melon: Breaking the memory wall for resource-efficient on-
device machine learning. In Proceedings of the 20th Annual International Conference on Mobile
Systems, Applications and Services , pages 450–463, 2022.
[77]Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
[78]Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-
art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing: System Demonstrations , 2020.
[79]Kuan Xu and Ana F. Loureiro. Spectral approximation of convolution operators. SIAM Journal
on Scientific Computing , 40(4):A2336–A2355, 2018.
[80]Yufan Xu, Saurabh Raje, Atanas Rountev, Gerald Sabin, Aravind Sukumaran-Rajam, and
P Sadayappan. Training of deep learning pipelines on memory-constrained gpus via segmented
fused-tiled execution. In Proceedings of the 31st ACM SIGPLAN International Conference on
Compiler Construction , pages 104–116, 2022.
[81]Lili Yu, D´ aniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.
Megabyte: Predicting million-byte sequences with multiscale transformers, 2023.
[82]Shanshan Yu, Jindian Su, and Da Luo. Improving bert-based text classification with auxiliary
sentence and domain knowledge. IEEE Access , 7:176600–176612, 2019.
[83]Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay,
Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch
on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision ,
pages 558–567, 2021.
[84]Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon
Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In
Proceedings of the IEEE/CVF international conference on computer vision , pages 6023–6032,
2019.
[85]Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond
empirical risk minimization. arXiv preprint arXiv:1710.09412 , 2017.
[86]Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:
Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.
18

--- PAGE 19 ---
[87]Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data
augmentation. In Proceedings of the AAAI conference on artificial intelligence , volume 34,
pages 13001–13008, 2020.
[88]Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar,
and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision.
Advances in Neural Information Processing Systems , 34:17723–17736, 2021.
[89]Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In The IEEE International Conference on Computer
Vision (ICCV) , December 2015.
19

--- PAGE 20 ---
Author Contributions
D.Y.F. Conceptualized the research; coordinated collaborations; developed M2
architectures; led experimental and implementation efforts; assisted in development
of theoretical results; coordinated writing.
S.A. Assisted with the development of M2-BERT architecture; conducted BERT
experiments; assisted in writing.
J.G. Led development of theory and causal algorithms; wrote Appendix D.
I.J. Led development of theory and causal algorithms; wrote Appendix D.
S.E. Assisted with BERT experiments; conducted Swin experiments; wrote Listing A;
assisted in writing.
A.W.T. Conducted ViT experiments; assisted in writing.
B.S. Assisted in optimized M2implementation; conducted mixer benchmarks;
assisted in writing.
M.P. Assisted in development of M2-GPT architecture.
A.R. Supervised theory development; developed proofs; reviewed manuscript.
C.R. Supervised research; reviewed manuscript.
Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, and Armin Thomas contributed equally
to this work.
Appendix
Appendix A gives a PyTorch code listing of an M2layer. Appendix B presents additional experiments.
Appendix C gives details for the experiments, including model architectures and hyperparameters.
Appendix D gives missing details and proofs for the theoretical analysis, as well as generalizations
to broader results.
A Implementation
1from einops import rearrange
2import torch
3from torch import nn
4
5def blockdiag_matmul (x, w):
6 return torch . einsum (
7 "bnm ,... bm - >... bn", w, x. view (*x. shape [: -1] , w. shape [0] , w. shape [ -1])
8 ). reshape (*x. shape )
9
10class MonarchMatrix (nn. Module ):
11
12 def __init__ (self , sqrt_n : int ):
13 super (). __init__ ()
14 self . sqrt_n = sqrt_n
15 self .L = nn. Parameter ( torch . randn (( sqrt_n , sqrt_n , sqrt_n )))
16 self .R = nn. Parameter ( torch . randn (( sqrt_n , sqrt_n , sqrt_n )))
17
18 def forward (self , x):
19 x = rearrange (x, "... (m n) -> ... (n m)", n= self . sqrt_n )
20 x = blockdiag_matmul (x, self .L)
21 x = rearrange (x, "... (m n) -> ... (n m)", n= self . sqrt_n )
22 x = blockdiag_matmul (x, self .R)
20

--- PAGE 21 ---
Table 9: Fine-tuning performance on GLUE [ 74]. We report the standard metrics – F1 scores for
QQP and MRPC, Matthew’s correlation for CoLA, Spearman’s correlation for STS-B, and accuracy
for the remaining tasks, following the procedure from [36].
Model MNLI (m / mm) RTE QNLI QQP SST2 STS-B CoLA MRPC Average
M2-BERT-base (80M) 78.4 / 78.6 68.5 84.6 86.7 92.0 86.3 53.0 89.8 79.9
M2-BERT-base (110M) 79.6 / 80.5 69.3 86.0 87.0 92.3 86.9 56.0 89.2 80.9
M2-BERT-large (260M) 81.7 / 81.9 72.8 84.7 87.8 93.3 88.0 59.2 90.0 82.2
M2-BERT-large (341M) 82.2 / 82.3 75.0 87.0 87.7 92.4 88.3 59.6 90.1 82.8
Table 10: Throughput in tokens/ms by context length for M2-BERT-base (80M) compared to 80M
BERT models.
Model 512 1024 2048 4096 8192
HF BERT (79M) 248.4 157.3 86.0 46.8 OOM
FlashAttention BERT (79M) 433.3 425.1 335.2 217.4 122.6
M2-BERT-base (80M) 386.3 380.7 378.9 353.9 320.1
M2 Speedup over HF BERT (80M) 1.6 × 2.4× 4.4× 7.5× –
23 return rearrange (x, " ... (m n) -> ... (n m)", n= self . sqrt_n )
24
25class MonarchMixerLayer (nn. Module ):
26 def __init__ (self , sqrt_n : int , sqrt_d : int ):
27 super (). __init__ ()
28 self .m1 = MonarchMatrix ( sqrt_n )
29 self .m2 = MonarchMatrix ( sqrt_n )
30 self .m3 = MonarchMatrix ( sqrt_d )
31 self .m4 = MonarchMatrix ( sqrt_d )
32
33 self . n_kernel = nn. Parameter ( torch . randn ( sqrt_d ** 2, sqrt_n ** 2))
34 self . d_kernel = nn. Parameter ( torch . randn (1, sqrt_d ** 2))
35 self . layer_norm = nn. LayerNorm ( sqrt_d ** 2)
36
37 def forward (self , x: torch . Tensor ): # x. shape = (b, n, d)
38 x_tilde = self .m2( torch . relu ( self . n_kernel * self .m1(x. transpose (-1, -2)))
). transpose (-1, -2) # mix sequence
39 y = self .m4( torch . relu ( self . d_kernel * self .m3( x_tilde ))) # mix features
40 return self . layer_norm (y + x_tilde ) # skip connection
Listing 1: A basic implementation of the M2 layer.
B Additional Experiments
B.1 Per-Task GLUE Numbers
We report full GLUE numbers for M2-BERT-base and M2-BERT-large in Table 9.
B.2 Additional Throughput Results
We report the throughput of M2-BERT-base (80M) compared to BERT models of the same size
(BERT-base with fewer parameters), as well as the throughput of M2-BERT-large (260M) compared
21

--- PAGE 22 ---
Table 11: Throughput in tokens/ms by context length for M2-BERT-large (260M) compared to
BERT-large.
Model 512 1024 2048 4096 8192
HF BERT-large (340M) 75.4 47.1 25.2 OOM OOM
FlashAttention BERT-large (340M) 125.0 111.9 91.6 54.5 OOM
M2-BERT-large (260M) 122.5 118.6 109.4 94.5 75.0
M2 Speedup over HF BERT-large (340M) 1.6 × 2.5× 4.3× - -
Table 12: ImageNet accuracy of Swin models.
Model ImageNet (acc@1) ImageNet (acc@5)
Swin-MLP-B 81.3 95.3
Swin-V1-B 83.5 96.5
Swin-V2-B 84.2 96.9
M2-Swin-B 83.5 96.7
to BERT-large.
Table 10 compares the performance of M2-BERT-base (80M) to BERT models parameter-
matched to 80M parameters. M2is slower than FlashAttention for sequence lengths 512 and 1K,
but outperforms FlashAttention starting at sequence length 2K. We believe further optimization of
theM2kernel can close the gap to FlashAttention for short sequences.
Table 11 compares M2-BERT-large (260M) to BERT-large. Trends are mostly similar to
comparisons against BERT-base; M2nearly matches FlashAttention at sequence length 512, and
outperforms it for sequence length 1K and longer. We also see up to 4.3 ×speedup over HuggingFace
BERT-large at sequence length 2K.
B.3 ImageNet Comparison against Swin
Table 12 reports the results of replacing attention and MLP in Swin-V2 using M2as a drop-in
replacement. Surprisingly, Swin-M2 outperforms Swin-MLP-B, is competitive with Swin-V1-B, and
comes within 1 point of Swin-V2-B, even without any hyperparameter tuning or architecture adjust-
ment from the ViT formula. We expect that performance may improve further with hyperparameter
tuning specific to M2.
B.4 Speech Applications
Table 13 presents the performance of M2on Speech Commands-10, a speech classification task over
raw 1-second clips sampled at 16 kHz. M2is competitive with state-of-the-art architectures on this
task.
B.5 CIFAR10
Table 14 shows the performance of Monarch Mixer on CIFAR10. The trends are largely the
same as on ImageNet.
22

--- PAGE 23 ---
Table 13: Accuracy on Speech-Commands 10. An “x” means that the model did not fit in memory.
M2 S4 WaveGan-D Transformer Performer CKConv
97.9 97.5 96.3 x 30.8 71.7
Table 14: Accuracy on CIFAR-10.
Model Top-1% Description
ViT (1.2M) 78.6 Attention + MLP
ViT + Monarch (607K) 79.0 Attention, MLP-Free
HyenaViT (1.3M) 80.6 Attention-Free + MLP
HyenaViT- M2(741K) 80.8 Attention-Free + MLP Free
B.6 Learnable Monarch Matrices in Sequence Mixer
In most of our models, we have used fixed Monarch matrices for the sequence mixer, and learnable
Monarch matrices for the dimension mixer. Table 15 presents an experiment evaluating using
learnable Monarch matrices for the sequence mixer on the sequential CIFAR task. We use a
non-gated convolutional architecture based off long convolutions, as presented in [ 26]. Learning the
Monarch matrices in the sequence mixer yields 1.5 points of lift.
B.7 Roofline Analysis
Figure 4 shows a Roofline analysis of a simple PyTorch implementation of a single M2operator
M−1(Mu⊙Mkon an A100 GPU, with 4K input length. The operation is more dominated by the
data movement operations, which helps explain why performance is higher on newer architectures
like RTX 4090 (which have faster and larger L2 cache).
B.8 Associative Recall
In Table 16, we present a simple experiment demonstrating the causal parameterization of M2on
associative recall, a synthetic language designed to test in-context learning. The model demonstrates
in-context learning abilities in sequences up to 128K tokens, but Transformers do not scale past 8K.
B.9 BERT Experiments with Alternative Architecture
Here, we report results using an older version of the M2-BERT architecture, that uses non-gated
convolutions and is trained on English Wikipedia [ 23] and English Bookcorpus [ 89]. For clarity, we
refer to this model as M1-BERT.
We found that M1-BERT could match Transformers on MLM quality, but underperformed on
downstream fine-tuning. We attribute this gap in performance to sub-optimal training hyperparam-
eters (optimized for throughput using NVIDIA MLPerf hyperparameters) as well as a sub-optimal
architecture. We report results here for completeness, but refer to the gated convolution architecture
in the main body as the proper M2-BERT model.
These models followed the reference implementations and hyperparameters from Hugging Face
Transformers examples [ 78] and Nvidia Deep Learning examples ( https://github.com/NVIDIA/
DeepLearningExamples ). In particular, we use the LAMB optimizer with a learning rate of 5 e−3.
23

--- PAGE 24 ---
Table 15: Accuracy on sequential CIFAR for fixed vs. learnable Monarch in the sequence mixer.
Model sCIFAR Accuracy
M2, Fixed Monarch 91.0
M2, Learnable Monarch 92.5
Figure 4: Roofline plot of a PyTorch implementation of a single M2 operator M−1(Mu⊙Mk).
For each sequence length, we use as large a minibatch size as possible that fits on the GPU (A100-
80GB in Table 17 and V100 in Table 18). We set the gradient accumulation to reach a global batch
size of 65 ,536 sequences. To investigate the effect of sequence length, each model is trained for a
fixed sequence length in a single phase of training (in contrast to some training protocols, which
train the model in multiple phases, each at different sequence lengths).
Time to a Fixed Pretraining Quality on 8xA100 We compare time to a fixed pretraining
quality, training M1-BERT-base on English Wikipedia [ 23] and English Bookcorpus [ 89]. We
compare against BERT-base trained with FlashAttention [13], as well as the Monarch-BERT-
base implementation from the original Monarch paper [ 12]. We measure wall-clock time for M1-BERT
and the base Transformer to reach 50% in masked language modeling accuracy on 8xA100 Nvidia
GPUs with 80GB memory each. Table 17 summarizes results. In short sequence lengths, M1-BERT
is comparable to FlashAttention , even without using a heavily-optimized fused kernel. In longer
sequence lengths, the FLOP savings make M1-BERT more efficient—up to 2.4 ×faster than BERT
withFlashAttention at sequence length 4096.
BERT in Half a Day Inspired by recent work focusing on training under limited resource
constraints [ 28], we measure how far we can get when training on a single V100 GPU in 12 hours.
In Table 18, we report the masked language modeling accuracy achieved by the same set of models
and sequence lengths (except for the FlashAttention baseline, which is not supported on V100).
We observe M1-BERT both achieves higher accuracy within the time limit and can be trained at
longer sequence lengths than the baseline architectures.
Downstream Fine-Tuning We evaluate the quality of M1-BERT-base models on the GLUE
benchmark [ 74]. Table 19 shows fine-tuning performance on the GLUE tasks, using the same
24

--- PAGE 25 ---
Table 16: In-context learning performance on associative recall at various sequence lengths, vocab
size 20. ✗indicates the Transformer did not finish in a week.
Model 0.5K 2K 8K 32K 128K
Transformer 100.0 100.0 100.0 ✗ ✗
Monarch Mixer 98.7 99.4 99.4 99.4 99.4
Table 17: Time in hours to reach 50% masked language modeling validation accuracy on 8xA100
with different sequence lengths.
Model 512 1024 2048 4096 Architecture Details
BERT-base- FlashAttention (110M) 2.7 3.8 5.7 13.2 Attention, MLP
BERT-base-HuggingFace (110M) 3.3 5.6 13.1 26.7 Attention, MLP
BERT-Monarch-base (80M) 3.1 4.7 10.3 22.1 Attention, MLP-free
M1-BERT-base (55M) 2.5 3.5 4.0 5.5 Attention-Free, MLP-free
Speedup 1.1 ×1.1×1.3×2.4×
hyperparameters and 5 epochs for all tasks and both models. M1-BERT-base is competitive with
Transformers trained using MLPerf hyperparameters on Bookcorpus and Wikitext, but underper-
forms fully-trained transformers and M2-BERT-base.
C Experiment Details
C.1 Model Architectures
In this section, we describe the exact model architectures we used for each task, including the design
of the block (residuals and gating). We additionally release our code for reproducibility,
BERT Language Modeling TheM2-BERT architectures use a standard BERT backbone, but
replace the attention with bidirectional gated convolutions and replace the linear layers in the MLPs
with block-diagonal matrices. All the M2-BERT architectures use an expansion factor of four.
M2-BERT-base (80M) has a model width of 768 and 12 layers; M2-BERT-base (110M) has a model
width of 960 and 12 layers; M2-BERT-large (260M) has a model width of 1536 and 12 layers; and
M2-BERT-large (341M) has a model width of 1792 and 12 layers. We train all these models on C4
for 70,000 steps, with sequence length 128, and global batch size 4096 sequences. For all the models,
we use decoupled AdamW with learning rate 8e-4 and decoupled weight decay 1e-5. We use linear
learning rate decay with a warmup of 6% of the steps, and we use MLM masking percentage of 30%.
For GLUE fine-tuning, we do a small search of learning rate, weight decay, and number of epochs.
Following [ 36], we fine-tune RTE, MRPC, and STS-B from the MNLI checkpoint. We fine-tune all
tasks with sequence length 128. For some tasks, we also pool the embeddings of all the non-padding
tokens instead of using the CLS token.
The final hyperparameters for M2-BERT-base (80M) are decoupled AdamW with learning rate
5e-5 and weight decay 5e-6 for 3 epochs for MNLI; AdamW with learning rate 5e-5 and weight
decay 0.01 for 6 epochs for RTE; AdamW with learning rate 3e-5 and weight decay 0.01 for 10
epochs on QQP; AdamW with learning rate 5e-5 and weight decay 1e-5 for 10 epochs with average
pooling for QNLI; decoupled AdamW with learning rate 3e-5 and weight decay 3ed-6 for 3 epochs
for SST-2; AdamW with learning rate 7e-5 and weight decay 0.01 for 10 epochs for STS-B; AdamW
25

--- PAGE 26 ---
Table 18: Masked language modeling validation accuracy achieved on a single V100 in 12 hours
with different sequence lengths. ✗indicates the model does not fit on device with a batch size of 1.
Model 512 1024 2048 4096 8192 Architecture Details
BERT-base (110M) 11.5 7.8 6.8 ✗ ✗ Attention, MLP
BERT-Monarch-base 6.9 8.5 6.8 ✗ ✗ Attention, MLP-Free
M1-BERT-base 20.2 20.2 20.1 17.1 12.9 Attention-Free, MLP-Free
Table 19: Fine-tuning performance on the GLUE benchmark [ 74], after pretraining on Wikipedia
and Bookcorpus. We report the standard metrics – F1 scores for QQP and MRPC, Matthew’s
correlation for CoLA, Spearman’s correlation for STS-B, and accuracy for the remaining tasks [ 19].
Model MNLI (m / mm) RTE QNLI QQP SST2 STS-B CoLA MRPC Architecture Details
BERT no pretrain 34.1 / 34.1 47.3 50.0 68.6 79.9 17.8 0.0 77.9 Attention, MLP
BERT-base 74.5 / 74.7 55.6 69.3 81.8 83.9 19.8 12.1 74.2 Attention, MLP
M1-BERT-base 69.9 / 70.5 53.1 73.2 81.4 85.2 68.1 33.6 75.4 Attention-free, MLP-free
with learning rate 5e-5 and weight decay 0.01 for 10 epochs for MRPC; and decoupled AdamW
with learning rate 5e-5 and weight decay 5e-6 for 10 epochs for COLA.
ForM2-BERT-base (110M), the hyperparameters are decoupled AdamW with learning rate
5e-5 and weight decay 5e-6 for 3 epochs for MNLI; decoupled AdamW with learning rate 1e-5 and
weight decay 1e-6 for 3 epochs for RTE; decoupled AdamW with learning rate 3e-5 and weight
decay 3e-6 for 5 epochs on QQP; decoupled AdamW with learning rate 5e-5 and weight decay 1e-5
for 10 epochs with average pooling for QNLI; decoupled AdamW with learning rate 3e-5 and weight
decay 3ed-6 for 3 epochs for SST-2; decoupled AdamW with learning rate 8e-5 and weight decay
3e-6 for 10 epochs for STS-B; decoupled AdamW with learning rate 8e-5 and weight decay 8e-5 for
10 epochs for MRPC; and AdamW with learning rate 8e-5 and weight decay 5e-6 for 10 epochs for
COLA.
ForM2-BERT-large (260M), the hyperparameters are decoupled AdamW with learning rate
5e-5 and weight decay 5e-6 for 3 epochs for MNLI; decoupled AdamW with learning rate 1e-5 and
weight decay 1e-6 for 3 epochs for RTE; decoupled AdamW with learning rate 3e-5 and weight decay
3e-6 for 5 epochs on QQP; decoupled AdamW with learning rate 5e-5 and weight decay 1e-5 for 10
epochs for QNLI; decoupled AdamW with learning rate 3e-5 and weight decay 3ed-6 for 3 epochs for
SST-2; decoupled AdamW with learning rate 7e-5 and weight decay 3e-6 for 10 epochs for STS-B;
decoupled AdamW with learning rate 8e-5 and weight decay 8e-6 for 10 epochs for MRPC; and
AdamW with learning rate 5e-5 and weight decay 5e-6 for 10 epochs for COLA.
ForM2-BERT-large (341M), the hyperparameters are decoupled AdamW with learning rate
5e-5 and weight decay 5e-6 for 3 epochs for MNLI; AdamW with learning rate 5e-5 and weight
decay 1e-6 for 2 epochs for RTE; decoupled AdamW with learning rate 3e-5 and weight decay 3e-6
for 5 epochs on QQP; decoupled AdamW with learning rate 5e-5 and weight decay 1e-6 for 10
epochs for QNLI; decoupled AdamW with learning rate 3e-5 and weight decay 3ed-6 for 3 epochs
for SST-2; decoupled AdamW with learning rate 8e-5 and weight decay 3e-5 for 8 epochs for STS-B;
decoupled AdamW with learning rate 8e-5 and weight decay 8e-6 for 10 epochs for MRPC; and
decoupled AdamW with learning rate 5e-5 and weight decay 1e-6 for 10 epochs for COLA.
ViT We use a standard ViT model architecture as base [ 20]. In line with recent improvements to
the ViT architecture [ 3,69,83], we use sinusoidal position embeddings and global average-pooling
26

--- PAGE 27 ---
Table 20: ViT training settings.
ImageNet-1k CIFAR-10
Optimizer AdamW
Optimizer momentum β1, β2= 0.9,0.999
Learning rate schedule Cosine decay w/ linear warmup
Dropout rate 0
Label smoothing 0.1
Image size 224 x 224 32 x 32
Base learning rate 1e-3 {1e-4, 3e-4, 1e-3 }
Batch size 1024 512
Training epochs 300 up to 500
Warmup epochs 10 5
Stochastic depth rate 0.1 {0, 0.1}
Weight decay 0.05 {0, 0.1}
(GAP) instead of a class token.
We adapt the ViT architecture by replacing its MLP and/or attention components with Monarch
Matrices (similar to our adaptation of BERT):
We replace the MLP with randomly initialized Monarch Matrices of the same dimension as the
dense matrices of the MLP and learn those matrices during training, setting the number of blocks
in the block-diagonal matrices to 4.
We replace attention with the recently introduced Hyena operator [ 63]. The Hyena operator
represents a recurrence of two efficient sub-quadratic primitives, an implicit long convolution and
multiplicative element-wise gating of the projected input. Hyena operators apply the FFT algorithm
to achieve fast long convolutions in sub-quadratic time. We further adapt the Hyena operator by
replacing its long convolutions with the M2 operator and setting the Monarch Matrices to the DFT
and inverse DFT.
ViT for ImageNet-1k In line with other work [ 3,12,63,69], we use a ViT-base architecture
with 12 layers, a hidden size of 768, 12 attention heads per layer, an intermediate size of the MLP
projection of 3 ,072, and a patch size of 16 ×16 pixels. For optimization, we follow the training
procedure of T2T-ViT [ 83], including augmentations such as RandAugment [ 10] (magnitude =
9,magnitude-std = 0.5,layers = 2), Mixup [ 85] (α= 0.8), CutMix [ 84] (α= 1.0), Random erasing
[87] (rate = 0 .25), and AugMix [35]. See Table 20 for all other training settings.
ViT for CIFAR-10 We use a ViT architecture with 6 layers, a hidden size of 128, 8 attention
heads per layer, an intermediate size of the MLP projection of 512, and a patch size of 4 ×4 pixels.
We further tune weight decay (0 or 0 .1), stochastic depth rate (0 or 0 .1), and base learning rate
(1e−4 or 3 e−4 or 1 e−3) and report the test performance for the model variant that achieved the
highest accuracy in a separate held-out validation dataset (randomly selected 10% of training data).
We also apply an early stopping rule such that training is stopped if the model’s validation loss
does not improve for 10 training epochs. See Table 20 for all other training settings.
GPT Causal Language Modeling Similarly to our ViT approach, we also replace attention
with the Hyena operator, using the same architecture as in [ 63] as a starting point. The Hyena
27

--- PAGE 28 ---
architecture has two convolutions, which can be computed using the FFT convolution theorem. In
our architecture, we additionally replace these FFT operations with causal Monarch matrices.
In addition, we re-use the heads extension from the H3 architecture [ 25]. The heads extension
groups the model dimension into heads, ties together the long convolution parameters in each head,
and then computes the outer product between different input projections. An algorithmic listing
adapted from the H3 paper [ 25] is provided in Listing 1, with updates to replace the SSM layers
with Hyena convolutions. We use a head dimension of 16. Setting the head dimension to be 1 and
replacing the Monarch matrices with FFT is equivalent to the Hyena layer.
Algorithm 1 M2 Hyena Layer with Heads
Input: Input sequence u∈RN×dfrom the previous layer, weight matrices WX1,WX2,WV,WO∈Rd×d,
causal Monarch matrix M, short convolution kernels K1,K2,K3, a Hyena convolution kernel Klong, head
dimension dh.
Output: Output sequence y∈RN×d
Compute X1=uWX1,X2=uWX2,V=uWV∈RN×d.
PassX1,X2,Veach through the short convolution using the causal Monarch matrices: X1,X2,V=
M−1(MX 1⊙MK 1),M−1(MX 2⊙MK 2),M−1(MV⊙MK 3).
SplitX1,X2,VintoH“heads” ( X1(h),X2(h),V(h)forh= 1, . . . , H ), each a sequence of Nvectors of size
dh=d/H.
for1≤h≤Hdo
Take the batched outer product X2(h)(V(h))⊤∈RN×dh×dh(batched in the N-dimension) and pass it
through the long convolution using the causal Monarch: XV(h)=M−1(MX2(h)(V(h))⊤⊙MK long)∈
RN×dh×dh.
Batch-multiply by X1:O(h)= [X1(h)
1XV(h)
1, . . . ,X1(h)
NXV(h)
N]∈RN×dh(batched in the N-dimension).
Concatenate the output O(h)of each head, and multiply by the output projection matrix WO∈Rd×d.
Finally, we remove the MLP layers entirely (equivalent to replacing the layer with an identity),
and make the model wider to compensate (the depths match the equivalent Hyena models). The
small model has a model width of 1160 with 18 layers and uses a learning rate of 0.0006, and the
medium model has model width of 1344 with 40 layers and uses a learning rate of 0.0008. All other
hyperparameters match the Hyena models [63].
D Missing details from Section 4
This section contains all the missing details (including proofs) from Section 4.
In Appendix D.1, we review some definitions and results on multi-variate polynomials and set
some notation needed for this section. In Appendix D.2, we explicitly connect Monarch matrices
forp= 2 and bivariate polynomial evaluation. Specifically, we prove Theorem 1 and Theorem 2.
Then in Appendix D.3 we show how to instantiate the bivariate basis polynomials so that we get a
causal map. This includes converting the bivariate polynomials to univariate polynomials (with
evaluations over the Nth roots of unity) and this proves Theorem 3. We then show how this causal
map can be implemented only using GEMMs (and O 
N3/2
FLOPs) in Appendix D.4.
Next, we note that while our evaluations points are over complex numbers, our input and
output to the Monarch convolution layers are over reals. Hence, it is natural to wonder if we can
implement the entire layer just with operations over real numbers. One potential advantage of this
is that we theoretically only have to keep Nreal numbers for intermediate results (instead of 2 N
reals numbers when we keep track of vectors in CN). This can reduce the data movement costs.
Further, multiplication of two complex numbers requires six operations over real numbers (four
28

--- PAGE 29 ---
multiplication and two addition). Thus, moving to an implementation that only uses real numbers
could potentially lead to wall clock time speedup. We propose one such scheme in Appendix D.5
that proves a version of Theorem 3 just over reals by moving to the Chebyshev basis (instead of the
standard monomial basis). This creates new technical challenges, which we also address.
Finally, we generalize our results to arbitrary p≥2 in Appendix D.6. We would like to point out
that to get a causal map (in Theorem 17) we need to ‘embed’ input vectors of size ninto vectors
of size N= 2p·n+O 
n1−1/p
. For p= 2, we avoided the blowup of 22= 4 with a blowup of 2
instead (via Theorem 3). Whether this is possible to do (i.e. have a blowup of 2 instead of 2p) for
p >2 is an interesting direction for future work. Further, the matrices that lead to causal map can
be represented with O 
pN2/p
parameters while the matrices in Theorem 3 use more parameters.
Extending the causal map for p >2 that uses O
N1+1
p
parameters is an exciting direction for
future work.
D.1 Background and Notation
We collect known facts and definitions about multi-variate polynomials in Appendix D.1.1 and recall
some notation from [ 12] in Appendix D.1.2. These will be needed throughout this appendix section.
D.1.1 Multi-variate Polynomials
Basic Definitions Letp≥1 be an integer. We recollect some definitions on p-variate polynomials
(overR) in variables X0, . . . , X p−1. When p∈ {1,2}, we will use variables in {X, Y, Z }for notational
simplicity.
We will use Xto denote the vector of variables (X0, . . . , X p−1). Further for j∈Zp
≥0, we use the
notation
Xj=p−1Y
a=0Xjaa.
Xjis a (standard basis) monomial , where j= (j0, . . . , j p−1).
A generic p-variate polynomial is defined as (with standard monomial representation)
q(X) =X
j∈Zp
≥0qj·Xj,
where the coefficient qj∈R.
We will need the following notion of degrees:
Definition 2 (Degree) .Let0≤a < p . The degree ofXainXj(with j= (j0, . . . , j p−1)) isja. The
degree of Xaofq(X), denoted by degXa(q)is the maximum degree of Xaover all monomials Xj
withqj̸= 0.
Note that for p= 1 the above coincides with the usual notion of degree of a univariate polynomial
q(Z), in which case we just use deg( q(Z)) to denote degZ(q(Z)).
We will need the notion of taking mod of ap-variate polynomial with p-tuple of polynomials.
The notion of mod is well defined for a univariate polynomial (which we will assume as a given
below) but in general for arbitrary p-variate polynomials q(X) and q′(X), the operation q(X)
mod q′(X) is not well defined. However, we will only need the following restricted operation:
29

--- PAGE 30 ---
Definition 3. Letp≥1. Fix a p-tuple of polynomials R0(X0), . . . , R p−1(Xp−1). Then for any
j∈Zp
≥0, we define
Xjmod ( R0(X0), . . . , R p−1(Xp−1)) =p−1Y
a=0 
Xjamod ( Ra(Xa))
.
For a general polynomial p(X),
p(X) mod ( R0(X0), . . . , R p−1(Xp−1))
is defined by extending the definition for Xjby linearity.
Polynomial Evaluation Given a p-variate polynomial q(X) and an point a∈Rp, the evaluation
ofqatadenoted by q(a) is evaluation of qas a function at a.
Given subsets Sa⊆C, we define q(X) evaluated at ×p−1
a=0Saas the vector of values q(a) overall
a∈ ×p−1
a=0Sa.
In this paper, we will in many cases evaluate polynomials at the appropriate roots of unity.
Specifically for an integer N, we will define
ωN=e2πι/N
and note that the Nth roots of unity is the set {ωi
N|0≤i < N}.
Polynomial Interpolation We now recall univariate and bivariate polynomial interpolation
results (proved via the Lagrange basis), which we will use in later subsections.
Theorem 4. LetD≥1be an integer. Given yifor0≤i < D andαifor0≤i < D there exists a
unique univariate polynomial P(X)with deg(P)< D , such that for all 0≤i < D ,
P(αi) =yi. (7)
Proof. This proof is based on the Wikipedia entry for Lagrange polynomials [45].
Given a sequence of values αifor 0≤i < D s.t.αi̸=αj,i̸=j, the Lagrange basis for
polynomials of degree < D for these values is the set of each polynomials {p0(X), p1(X), . . . p D−1(X)}
each of degree D−1. Each basis polynomial are defined as:
pi(X) =X−α0
αi−α0· · ·X−αi−1
αi−αi−1·X−αi+1
αi−αi+1· · ·X−αD−1
αi−αD−1=Y
0≤j<D
j̸=iX−αj
αi−αj. (8)
By definition,
pi(αj) =(
1 for j=i
0 otherwise. (9)
The Lagrange interpolating polynomial for those nodes through the corresponding values yifor
0≤i < D is the linear combination:
P(X) =D−1X
i=0yi·pi(X). (10)
30

--- PAGE 31 ---
By (9), for all 0 ≤i < D :
P(αi) =yi. (11)
Finally, the interpolating polynomial is unique. Assume there is another polynomial M(X) of
degree < D such that M(αi) =yifor all 0 ≤i < D . Then the difference M(X)−P(X) is 0 at D
distinct points αifor 0≤i < D . And the only polynomials of degree < D with more than D−1
roots is the 0 polynomial. So, M(X) =P(X).
Theorem 5. LetDX, DY≥1be integers. Given values yijfor0≤i < D X,0≤j < D Y
andDXdistinct points (α0, . . . , α DX−1),DYdistinct points (β0, . . . , β DY−1)there exists a unique
bivariate polynomial P(X, Y)withdegX(P)< D X,degY(P)< D Y, such that for all 0≤i < D X,
0≤j < D Y:
P(αi, βj) =yij. (12)
Proof. Define
P(X, Y) =mX
j=0nX
i=0yij·pi(X)·¯pj(Y), (13)
where piand¯pjare Lagrange basis polynomials defined in the proof of Theorem 4 such that for
0≤i, k < D X,
pi(αk) =(
1 for i=k
0 otherwise(14)
and for 0 ≤j, ℓ < D Y
¯p(βℓ) =(
1 for k=ℓ
0 otherwise. (15)
From above, we have for all i, j, k, ℓ
pi(αk)·¯pj(βℓ) =(
1 for i=kandj=ℓ
0 otherwise. (16)
Then, for all 0 ≤i < D X, 0≤j < D Y:
P(αi, βj) =yij. (17)
By definition of Lagrange basis polynomials, degX(P)< D Xand degY(P)< D Y.
Finally, the interpolating polynomial is unique. Assume there is another polynomial M(X, Y)
with degX(M)< D XanddegY(M)< D Ysuch that M(αi, βj) =yijfor all 0 ≤i < D Xand
0≤j < D Y. Then the difference M(X, Y)−P(X, Y) is 0 at DX·DYdistinct points, ( αi, βj) for
0≤i < D X,0≤j < D Y. And the only polynomial with degX< D XanddegY< D Ythat has
DX·DYroots is the 0 polynomial.
D.1.2 Notation
Here we recall notation we will use from [12].
1.The class of Monarch matrices is defined in appendix C of [ 12] asM(b,N)which are N×N
matrices with block size bfor any integer 0 ≤b≤Nthat divides N. When b=√
Nwe drop b
from the notation giving (i1, i0)and(j1, j0). For example, this is used in Proof of Corollary 2.
31

--- PAGE 32 ---
2. Row index ican be represented as ( i1, i0)b. Which gives i=i1b+i0.
3.Similarly, column index jcan be represented as (j1, j0)b. Which gives j=j1b+j0. Note that
when b=√
N,j1=k(j) and j0=m(j). We choose to use the (j1, j0)notation here since that
notation is easier to generalize for p >2.
4.L∈ DB(b,N)is an N×Nmatrix with b×bblocks that are all diagonal matrices.
5.R∈ BD(b,N)meaning it’s a block diagonal N×Nmatrix with block size b×b.
6.We have a class of permutation matrices defined as σ(b,N)(i) =i0·N
b+i1. This can be denoted
by an N×Nmatrix, P(b,N), where the ithrow is eσ(b,N)(i).
7.We’ll use ior pair notation (i1, i0)bto denote the rows, and jor pair notation (j1, j0)bto
denote columns. It should be clear from context which one we’re using.
For any 0 ≤j1<√
N, let ℓj1(X, Y) be an arbitrary bivariate polynomial with degX(ℓj1),
degY(ℓj1)<√
N.
For any 0 ≤j1, j0<√
N, letrj1,j0(Y) be an arbitrary univariate polynomial of degree <√
N.
LetA= (α0, . . . , α√
N−1),B= (β0, . . . , β√
N−1) each be a sequence of distinct eval points. Note
thatAandBneed not be disjoint.
From the proof of Theorem 3 in the Appendix C of the Monarch paper [12] we get,
L=P(b,N)·L·P⊤
(b,N)
=P(b,N)·L·P(N
b,N).
Therefore,
L=P⊤
(b,N)·L·P(b,N)
=P(N
b,N)·L·P(b,N).
Define DBandBDas set of all such LandRmatrices over RN×Nwhere if i0̸=k0
Li1,k1[i0, k0]def=L[(i1, i0)√
N,(k1, k0)√
N] = 0 (18)
and if k1̸=j1
Rk1,j1[k0, j0]def=R[(k1, k0)√
N,(j1, j0)√
N] = 0. (19)
Pictorially, LandRlook as follows:
32

--- PAGE 33 ---
In [12], Monarch matrices with block size b=√
N,M′=L·R, and thus for all 0 ≤i1, i0, j1, j0<√
N:
M′[(i1, i0)√
N,(j1, j0)√
N] =Li1,j1[i0, i0]·Rj1,j1[i0, j0]. (20)
We note that our definition of Monarch matrix Min Section 3 is slightly different in that
M=M′PwithM′as defined in [12].
D.2 Monarch Matrices and Bivariate Polynomial Evaluation
Given polynomials ℓj1(X, Y) for 0 ≤j1<√
N, polynomials rj1,j0(Y) for 0 ≤j1, j0<√
N,
evaluation points A= (α0, ..., α√
N−1)B= (β0,···, β√
N−1) (as in Appendix D.1.2), define the
matrices L∈ DB√
N,NandR∈ BD√
N,Nas:
•For every 0 ≤j1, i1, i0<√
N:
Li1,j1[i0, i0]←ℓj1(αi1, βi0). (21)
•For every 0 ≤j1, j0, i0<√
N:
Rj1,j1[i0, j0]←rj1,j0(βi0). (22)
Note that all entries of LandRnot specified above are set to 0.
Letfbe the above function that maps coefficients of ℓj1(X, Y) (which are coefficients of monomials
Xi1Yi0for all 0 ≤i1, i0<√
Nand hence represented by a matrix in R√
N×√
N) and coefficients of
rj1,j0(Y) (which are coefficients of monomials Yi0for all 0 ≤i0<√
Nand hence represented by a
vector in R√
N) for all 0 ≤j1, j0<√
Nto pairs of matrices in DB√
N,N× BD√
N,N.
Theorem 6. Letfbe as defined above. Then fis a bijection.
Proof. To prove fis bijection we must show fis one-to-one and f−1is one-to-one (and exists).
To show fis one to one means each set of polynomials’ coefficients given to f, will output a
unique set of matrices ( L,R)∈ DB√
N,N× BD√
N,N. This follows from (21), (22) and the known
fact that polynomial evaluation is a function.
Now, to show f−1exists and is one-to-one, we must show that there is a map from any pair
(L,R)∈ DB√
N,N× BD√
N,Nto unique sets of polynomials, ¯ℓ,¯r, with parameters as defined in
Appendix D.1.2. Further, we need
Li1,j1[i0, i0] =¯ℓj1(αi1, βi0) (23)
and
Rj1,j1[i0, j0] = ¯rj1,j0(βi0). (24)
We will use Theorems 5 and 4 to show the existence of ¯ℓj1and¯rj1,j0polynomials, giving us the
mapping from the matrices to unique polynomials.
We first show the existence of the unique polynomials in (24). Fix 0 ≤j1, j0<√
N. Then
consider the values 0 ≤i0<√
N:
yi0←Rj1,j1[i0, j0]. (25)
Then by Theorem 4, there exists a unique polynomial of degree <√
N(call it ¯rj1,j0(Y)) such
that for all 0 ≤i0<√
N,
¯rj1,j0(βi0) =yi0,
33

--- PAGE 34 ---
which by (25) shows (24).
Next we show the existence of the unique polynomials in (23). Fix 0 ≤j1<√
N. Consider the
values 0 ≤i1, i0<√
N:
yi1,i0←Li1,j1[i0, i0]. (26)
Then by Theorem 5, there exists a unique bi-variate polynomial of degX<√
NanddegY<√
N
(call it ¯ℓj1(X, Y)) such that for all 0 ≤i1, i0<√
N,
¯ℓj1(αi1, βi0) =yi1,i0, (27)
which by (26) shows (23).
Therefore fis a bijection.
We can now conclude:
Corollary 1. For every matrix M′as defined in (20), there exists unique polynomials ℓj1(X, Y)
andrj1,j0(Y), such that for all 0≤i1, i0, j1, j0<√
N,
M′[(i1, i0)√
N,(j1, j0)√
N] =ℓj1(αi1, βi0)·rj1,j0(βi0). (28)
Proof. Follows from (20), (21), (22) and Theorem 6.
D.2.1 Proof of Theorem 1
We begin with an immediate consequence of Corollary 1:
Corollary 2. LetA, B⊂Csuch that |A|=|B|=√
N. Then the jth column of M′is the evaluation
of the polynomial ℓj1(X, Y)·rj1,j0(Y)over A×B.
Proof. Observe that for fixed j0, j1the right hand side of (28) is ℓj1(X, Y)·rj1,j0(Y) evaluated at
all (α, β)∈A×B. Thus, ( j1, j0) column is evaluation of ℓj1(X, Y)·rj1,j0(Y) over points in A×B,
as desired.
Next, we state a generalization of Theorem 1 that follows from Corollary 2:
Corollary 3. LetAandBbe as in Corollary 2. For any vector u,M′·uis¯u(X, Y)evaluated at
A×B. Further,
¯u(X, Y) =X
0≤j1,j0<√
Nuj1,j0·ℓj1(X, Y)·rj1,j0(Y), (29)
where ℓandrare defined by M′as in Corollary 1.
Proof. Follows from Corollary 2 and definition of matrix vector multiplication.
In Theorem 1 and the following sections, we consider the polynomial evaluated over the basis
polynomials defined by
M=M′P
.
34

--- PAGE 35 ---
Corollary 4. LetAandBbe as in Corollary 2. For any vector u, and M·uisu(X, Y)evaluated
atA×B. Further,
u(X, Y) =X
0≤j1,j0<√
Nuj0,j1·ℓj0(X, Y)·rj0,j1(Y), (30)
where ℓandrare defined by M′as in Corollary 1.
Proof. Follows from Corollary 3 and definition of M.
Specifically, Theorem 1 is a special case of Corollary 4 where ℓj0(X, Y) =ℓm(j)(X, Y) and
rj(Y) =rj0,j1(Y).
D.2.2 Proof of Theorem 2
By Corollary 4, for all 0 ≤j1, i1<√
N,
ℓj0(X, Y), rj0,j1(Y),¯ℓj0(X, Y),¯rj0,j1(Y)
are the basis polynomials corresponding to M1andM2. For the coefficient vector k= (kj1,j0)0≤j1,j0<√
N
and similarly for u= (uj1,j0)0≤j1,j0<√
N, we can construct two polynomials
k(X, Y) =X
0≤j1,j0<√
Nkj1,j0·ℓj0(X, Y)·rj0,j1(Y)
u(X, Y) =X
0≤j1,j0<√
Nuj1,j0·¯ℓj0(X, Y)·¯rj0,j1(Y)
whose evaluation over ( αi1, βi0) = ( ωi1, ωi0) where recall as in Appendix D.1.1 ω=e2πι√
N, by
Theorem 1 is equivalent to the products M1·kandM2·u, respectively. Taking the component-wise
product, y=( M1·k)⊙(M2·u), the entry at i= (i1, i0) is given by
y[(i1, i0)] =k(ωi1, ωi0)·u(ωi1, ωi0).
Noting that the element of A, i.e. the√
N-th roots of unity, satisfy Z√
N= 1 means that the
above are evaluations of
h(X, Y) =k(X, Y)·u(X, Y) mod ( X√
N−1, Y√
N−1)
atA×A. Finally, Theorem 1 and the fact that M−1
0exists implies M0·yis polynomial interpo-
lation into basis polynomials corresponding to M0. (Here we use the well known fact polynomial
interpolation is the inverse of polynomial evaluation).
D.3 Proof of Theorem 3
We review some concepts in Appendix D.3.1. In Appendix D.3.2, we discuss square matrices and
causality in terms of operations on univariate polynomials. This allows us to define a general class
of operators for causal 1D convolution. In Appendix D.3.3, we give a class of matrices suitable for
perform causal Monarch convolution. Specifically, we prove Theorem 3.
35

--- PAGE 36 ---
D.3.1 Review
Consider the linear operation on an input vector u:
y=A·u.
We say that the map is causal to mean the entry y[i] only depends on u[0],u[1], . . .u[i]. This will
be the case when Ais a lower triangular matrix (we index the top left entry of Aas (0,0)). When
Ais a lower triangular Toeplitz matrix with entries corresponding to some coefficient vector k, this
operation is exactly the 1D convolution
y=k∗u= 
F−1
2n· 
(F2n·k′)◦(F2n·u′)
[0 :n−1],
where k′= (k,0n),u′= (u,0n), and Fnis the n×nDFT matrix.
Definition 4. For a matrix M∈Rn×n, let us define the map
y=M−1(M·k⊙M·u) (31)
asmatrix convolution . When Mis a Monarch matrix, (31) is called Monarch convolution .
In this section, we are interested in determining large subclasses of matrices Msuch that for
any coefficient vector k, (31) is causal in u. We provide a class of matrices for which Monarch
convolution is causal.
We note that for general Monarch matrix M, (31) is not causal in u. By Theorem 2, we have
y(X, Y) =k(X, Y)·u(X, Y) mod ( X√
N−1, Y√
N−1).
This is not causal because the mod (X√
N−1, Y√
N−1) term condenses higher order terms into
lower order terms, hence the y[i] wouldn’t just depend on input information up to value i.
D.3.2 Univariate Matrix Convolutions
We start with a couple of notation assumptions.
Assumption 1. Nis a perfect square.
Assumption 2. We will not use pair notation for this subsection since throughout we have
i=i1√
N+i0andj=j+ 1√
N+j0.
In order to discuss square matrices in terms of univariate polynomials, we give univariate analogs
of Theorem 1 and Theorem 2 for general univariate basis. With an eye toward towards performing
causal convolution, we restrict our analysis to certain classes of univariate polynomials.
We first define matrices whose jthcolumns are the evaluation of a minimum degree j(and
maximum degree N−1) polynomial (recall Definition 1). We generalize Theorem 3 to such matrices.
Lemma 1. For sequence of points A={1, ωN,···ωN−1
N}where ωNis the Nthroot of unity, let M
be defined as
M[i, j] = ¯qj(ωi
N) (32)
where ¯qj(Z)is defined as in Definition 1. Then for any vector v∈RN,M·vis equivalent to
evaluating the polynomial
v(Z) =N−1X
j=0vj·¯qj(Z) (33)
at{1, ωN,···ωN−1
N}.
36

--- PAGE 37 ---
Proof. By our definition of M, the column M[:, j] is exactly the evaluation of the polynomial qj(Z)
at each point in A. The claimed result comes from the definition of matrix vector multiplication
and (33).
Note that Mor any M’s in this sub-section are not necessarily Monarch matrices.
Next, we state the following intermediate result:
Proposition 1. LetAbe the set of the N-th roots of unity. Then for M1,M2defined as in (32)
y= (M1·k)⊙(M2·u)
is the same as evaluating the polynomial
p(Z) :=k(Z)·u(Z) mod ( ZN−1)
overAwhere k(Z), u(Z)are of the form (33), corresponding to M1andM2, respectively. In other
words, for any 0≤i < N ,
y[i] =p 
ωi
N
.
Proof. This result follows from Lemma 1 and the definition of the Hadamard product.
Next, we state a re-interpretation of M−1y:
Proposition 2. LetMbe a full rank matrix whose columns are the evaluations of the basis
polynomials ¯qj(Z)from Definition 1 for 0≤j < N , and let y∈RNbe an arbitrary vector. If
u=M−1y, then for all 0≤i < N
y[i] =u(ωi)
where u(Z)is the same as in Lemma 1 for M. In other words, M−1yis the polynomial interpolaton
problem for the polynomial basis ¯qj(Z)for0≤j < N .
Proof. This follows from Lemma 1 and the fact that Mis invertible.
From Propositions 1 and 2, we get the following generalization of Theorem 2:
Theorem 7. For matrices M0,M1,M2as defined above, the operation
f=M−1
0·((M1·k)◦(M2·u))
is equivalent to representing the polynomial
f(Z) =k(Z)·u(Z) mod ( ZN−1)
in terms of the basis polynomials
ˆqj(Z)forj= 0, . . . , N −1
where k(Z), u(Z)are defined as in Lemma 1 in terms of the basis polynomials corresponding to M1
andM2, respectively, and (ˆqj(Z))0≤j<Ncorresponds to M0.
Proof. Follows from Propositions 1 and 2.
Now we give the class of matrices from which we can build a causal map. Specifically we prove
a generalization of Theorem 3:
37

--- PAGE 38 ---
Theorem 8. Letn≥1, letN=√
2n2. Then define the basis polynomial ¯qj(Z)to have minimum
degree jand maximum degree n−1for0≤j < n , and for n≤j < N ,¯qj(Z)has minimum degree
jand maximum degree N−1.
For all MNwith basis columns defined by (¯qj(Z))0≤j<Nas above, the operation
u7→ 
M−1
N(MN·(k,0N−n)◦MN·(u,0N−n))
[0 :n−1] (34)
gives a causal map.
Proof. To prove this is causal means each entry, f[i] is dependent only on u[0],u[1], . . .u[i], where
f= 
M−1
N(MN·(k,0N−n)◦MN·(u,0N−n))
. By Theorem 7 , we have
f(Z) =k(Z)·u(Z) mod ( ZN−1),
where
k(Z) =n−1X
j=0kj¯qj(Z)u(Z) =n−1X
j′=0uj′¯qj′(Z).
Since deg( k(Z)·u(Z))≤2n−2≤N−2, this is equivalent to
f(Z) =k(Z)·u(Z)
=n−1X
j,j′=0kj′·uj·¯qj′(Z)·¯qj(Z).
By our choice of ¯qj, we ensure that ¯qj·¯qj′has minimum degree j+j′anddeg(¯qj·¯qj′)≤2n−2< N
for any 0 ≤j, j′< n. Then by Lemma 2 (see below), there exists coefficients αj+j′,i′such that,
f(Z) =n−1X
j,j′=0kj′·uj·N−1X
i′=j+j′αj+j′,i′·¯qi′(Z)
=N−1X
i=0
n−1X
j,j′=0
j+j′≤iαj+j′,i·kj′·uj)
·¯qi(Z).
If we define
f(Z) =N−1X
i=0fi·¯qi(Z),
then for 0 ≤i < n , we get:
fi=n−1X
j,j′=0
j+j′≤iαj+j′,i·kj′·uj.
Note that fionly depends on u[0],u[1], . . .u[i], as desired.
38

--- PAGE 39 ---
We used the following lemma in the proof of Theorem 8.
Lemma 2. Let¯qj(Z)be defined as in Theorem 8. Then for any 0≤j, j′< n,
¯qj(Z)·¯qj′(Z) =N−1X
i=j+j′αj+j′,i·¯qi(Z). (35)
for some set of coefficients αj+j′,i.
Proof. We first note that by our choice of ¯qj, the minimum degree of ¯qj(Z)·¯qj′(Z) isj+j′,
anddeg(¯qj(Z)·¯qj′(Z))≤2n−2≤N−1. Our claim follows from that fact that any polyno-
mial pd(Z) of minimum degree danddeg(pd)< N can be expressed as a linear combination of
¯qd(Z),¯qd+1(Z), . . . , ¯qN−1(Z).3
D.3.3 Causal Monarch Convolutions
In this section we will prove Theorem 3. We will do so by showing that the basis polynomials qj(Z)
as defined in Theorem 3 are a special case of the basis polynomials ¯qj(Z) as defined in Theorem 8.
We start with a couple of notation assumptions.
Assumption 3. In this sub-section we will using block size b=√
Ntherefore, we are dropping the
block size from index notation. For example, (i1, i0)√
Nbecomes (i1, i0)for this section.
Assumption 4. Permutation matrices in this subsection are all the same P(√
N,N )so we drop the
subscript and just use P.
Definition 5. Define
q′
(j1,j0)(Z)def=ℓj1(Z)·rj1,j0
Z√
N
. (36)
ℓj1(Z)has minimum degree j1, and rj1,j0(Z)has minimum degree j0. All polynomials q′
(j1,j0)(Z)
have maximum degree ≤N−1.
Next we argue that the above basis polynomials have a specific minimum degree.
Lemma 3. Polynomial q′
(j1,j0)(Z)as defined in equation (36) has minimum degree j0√
N+j1.
Proof. We need to show that q′
(j1,j0)(Z) is a minimum degree j0√
N+j1polynomial. Note that
q′
(j1,j0)(Z) =ℓj1(Z)·rj1,j0
Z√
N
=Z√
N−1·˜ℓ√
N−1−j11
Z
·Z(√
N−1)·√
N·˜r√
N−1−j01
Z√
N
where ˜ℓ√
N−1−j1(·)has degree√
N−1−j1and˜r√
N−1−j0(·)has degree√
N−1−j0. Simplifying
we get
q′
(j1,j0)(Z) =ZN−1·˜ℓ√
N−1−j11
Z
·˜r√
N−1−j01
Z√
N
.
This claim follows since ˜ℓ√
N−1−j1(Y)·˜rj1,j0
Y√
N
has degree = (√
N−1−j1)+(√
N−1−j0)·√
N=
(N−1)−(j0√
N+j1).
3This claim can be shown using downward induction on d=N−1, N−2, . . ..
39

--- PAGE 40 ---
Note that the polynomial q′
(j1,j0)(Z) has minimum degree j0√
N+j1. This is not j1√
N+j0as
in defined in equation (1), we will talk more about this soon.
Next, we observe that the polynomials in (36) define a matrix that satisfies (20).
Lemma 4. Letq′
(j1,j0)(Z)for0≤j1√
N+j0< nbe as in (36). Define
M′[(i1, i0),(j1, j0)] =q′
(j1,j0)(ωi1√
N+i0
N).
Then M′satisfies (20).
Proof. If we evaluate the polynomials q′
(j1,j0)(Z) atωi
Nfor 0≤i < N , we get
q(j1,j0)(ωi
N) =ℓj1(ωi
N)·rj1,j0(ωi√
N
N).
Since ωi√
N
N=ωi1N+i0√
N
N=ωi0√
N
N=ωi0√
N, we get
M′[(i1, i0),(j1, j0)] =ℓj1(ωi1√
N+i0
N)·rj1,j0(ωi0√
N).
The above corresponds to how we define (20) since we have
M′[(i1, i0),(j1, j0)] =Li1,j1[i0, i0]·Rj1,j1[i0, j0]
with
Li1,j1[i0, i0] =ℓj1
ωi1√
N+i0
N
and
Rj1,j1[i0, j0] =rj1,j0
ωi0√
N
.
Recall from Lemma 3 that q′
(j1,j0)has minimum degree j0√
N+j1. For causality we need
q(j1,j0)(Z)to have degree minimum j1√
N+j0(then the polynomials will satisfy the minimum
degree requirements from (1)). Therefore, we permute the columns of M′,
M=M′·P (37)
Note that the above M=PLPRP and is indeed a Monarch matrix as defined in Section 3.
Note that the basis polynomials of Mare defined as,
q(j1,j0)(Z) =ℓj0(Z)·˜rj0,j1
Z√
N
. (38)
We note that the above is same as qj(Z) defined in (5)where j=j1√
N+j0with the correction in
(5) that qj(Z) =ℓm(j)·rj(Z√
N) where rj(Y) = ˜rj0,j1(Y).
We are finally ready to prove Theorem 3.
Corollary 5 (Theorem 3 restated) .LetN=√
2n2. Define MNbyq(j1,j0)as in (38). Then,
u7→ 
M−1
N(MN·(k,0N−n)◦MN·(u,0N−n))
[0 :n−1]
gives a causal map.
Proof. Due to using qpolynomials as in (38), by Lemma 3 the degree of the (j1, j0)thcolumn is
a polynomial with minimum degree j1√
N+j0.4This implies that these basis polynomials are a
subset of the more general causal maps in Theorem 8, which proves the claim.
4It can also be verified that q(j1,j0)has maximum degree ≤N−1.
40

--- PAGE 41 ---
D.4 Block Algorithms for Complex Numbers and Block Size√
N
In the following subsections we restate the results in Section D.3.3 in terms of block operations. As
mentioned earlier, this is so that we can do computations on Monarch matrices using only GEMM
operations (and simple data movement operations like permutations).
In Appendix D.4.1 we consider arbitrary Monarch matrices and in Appendix D.4.2 we consider
the sub-class of Monarch matrices corresponding to Theorem 3.
D.4.1 General Block Monarch Convolution
In this subsection we re-state general Monarch convolutions in terms of block operations.
Recall from equation (22) we defined the block diagonal matrix Ras follows (0 ≤i0, j1, j0<√
N):
Rj1,j1[i0, j0]←˜rj1,j0
ωi0√
N
, (39)
where deg(˜ rj1,j0)<√
N.
To do so, we will first work with M′
Nsuch that MN=M′
N·P. We want to express Monarch
matrices, MN, as univariate polynomial evaluation over {1, ωN, . . . , ωN−1
N}. Towards that end,
define
rj1,j0(Z) = ˜rj1,j0
Z√
N
.
By simple observation that ω(i1√
N+i0)√
N
N=ωi0√
N
N=ωi0√
N, we have
rj1,j0(ωi1√
N+i0
N) = ˜rj1,j0
ωi0√
N
.
In other words we have,
Rj1,j1[i0, j0] =rj1,j0 
ωi
N
.
Fix 0≤j1, j0<√
Nso we’re looking at the jth
0column of block Rj1,j1, going down the column
we evaluate the polynomial ˜ rj1,j0at points
1, ω√
N, . . . , ω√
N−1√
N
. Which is equivalent to a matrix
multiplication of Fourier matrix of size√
Nand a matrix of the coefficients of the ˜ rpolynomials.
So we can think of the blocks of Ras a Fourier matrix times a coefficient matrix. In other words,
let us define a matrix eR∈RN×√
Nwhich will hold√
Ncoefficient blocks eR0,eR1, . . .eR√
N−1∈
R√
N×√
Nsuch that,
eRj1[a, j0] = ˜rj1,j0[a],
where
˜rj1,j0(Y) =√
N−1X
a=0˜rj1,j0[a]·Ya.
41

--- PAGE 42 ---
Then we define
Rj1,j1=F√
N·eRj1.
Next, we restate the above as product of two block diagonal matrices:
In other words, we have
R= diag
F√
N, . . . ,F√
N
| {z }√
Ntimes·diag(eR0, . . . ,eR√
N−1). (40)
Equation (21) defined Las evaluation of bivariate polynomials. We now wish to do the same
with univariate polynomials.
Recall that L=PLP . We define the ith
0diagonal block (0 ≤i1, i0, j1<√
N) forLas:
Li0,i0[i1, j1] =ℓj1
ωi0√
N+i1
N
, (41)
where deg( ℓj1)< N.
Let us define a matrix eL∈RN×√
Nthat will hold the coefficients of polynomials ℓj1(Z) i.e.
eL[a, j1] =˜ℓj1[a]
where
ℓj1(Z) =N−1X
a=0˜ℓj1[a]·Za.
We can multiply this matrix with the Fourier matrix of size N×Nto get the blocks of L(which
we will need to diagonalize). Specifically define
L′′=FN·eL.
The rows of L′′and the rows of M′are both indexed by i. Meaning they’re ordered in
lexicographic ordering (i1, i0), which is a problem for the following reason. The block diagonal
matrix Lmade from L′′needs to be in lexicographic ordering (i0, i1)(see(41)) since it gets permuted
on the left M=PLP and right allowing Mto be ordered by (i1, i0). Therefore, when composing L
fromL′′we must permute the rows by P. So we get,
L′=P·L′′.
Let
L′=
L′
0...
L′√
N−1
.
42

--- PAGE 43 ---
Then
L= diag( L′
0, . . .L′√
N−1). (42)
Pictorially:
Letfbe the function that maps coefficient matrices eL,eR∈RN×√
Nto (L,R) where LandR
are defined by (42) and (40).
Theorem 9. Letfbe as defined above. Then fis a bijection.
Proof. To prove fis a bijection we must show fis one-to-one and f−1is one-to-one (and exists).
To show fis one-to-one means both eLandeRgiven to fwill output a unique pair ( L,R). This
follows from (42) and (40), and the fact that polynomial evaluation is a function.
Now to show f−1exists and is one-to-one, we must show that there’s a map for any ( L,R) to
unique eL,eR∈RN×√
N. Then for any pair ( L,R) where both LandRare block diagonal matrices,
there’s a map to unique sets of polynomials, ℓ,˜rwhere each coefficient is an entry of eL,eR(thus
giving a unique mapping from ( L,R)’s to ( eL,eR)’s).
We need to show the existence of ℓj1(Z) and ˜ rj1,j0(Y) such that:
Li0,i0[i1, j1] =ℓj1(ωi0√
N+i1
N)
and
Rj1,j1[i0, j0] = ˜rj1,j0(ωi0√
N).
Fix 0≤j1, j0<√
N. Then consider the values 0 ≤i0<√
N:
yi0←Rj1,j1[i0, j0].
Then by Theorem 4 there exists a unique polynomial of degree <√
N(call it ˜rj1,j0) such that for
all 0≤i0<√
N:
˜rj1,j0(ωi0√
N) =yi0.
There will be Npolynomials with√
Ncoefficients each, meaning there’s unique sets of coefficients
to make up the indices of eR.
Now to get the entries of eLfromL, fix 0 ≤j1<√
N. Then consider the values 0 ≤i1, i0<√
N:
yi1,i0←Li0,i0[i1, j1].
Then by Theorem 4 there exists a unique polynomial of degree < N (call it ℓj1) such that for all
0≤i1, i0<√
N:
ℓj1(ωi0√
N+i1
N) =yi1,i0.
There will be√
Npolynomials with Ncoefficients each, meaning there’s unique sets of coefficients
to make up the indices of eL.
43

--- PAGE 44 ---
Algorithm 2 is pseudo code for the map from eL,eR∈RN×√
Nto block diagonal matrices L,R.
Algorithm 2 Blocky Monarch (eL,eR)
Input: eL,eR∈RN×√
N
Output: Block diagonal matrices L,R∈CN×N
▷First, compute LfromeL
1:LetFNbe the Fourier transform Monarch matrix PLFPRFP ▷See Corollary 6
2:L′←P·FN·eL
3:fora←0 to√
N−1do
4: L′
a←L′[a√
N:a√
N+√
N−1,:]
5:L←diag(L′
0, . . .L′√
N−1)
▷Now compute RfromeR
6:fora←0 to√
N−1do
7: Ra←F√
N·eR[a√
N:a√
N+√
N−1,:]
8:R←diag
R0, . . . ,R√
N−1
9:return L,R
Lemma 5. Algorithm 2 uses O(N3/2)FLOPs and 3√
NGEMMs of two√
N×√
Nmatrices.
Proof. Lines 1 and 2 are multiplying a monarch matrix representing the Fourier transform times a
N×√
Nmatrix in block fashion again, giving O(N3/2) FLOPs.
Similarly, lines 6 and 7 have two√
Nmatrices multiplied√
Ntimes. Giving O(N3/2) FLOPs.
Lines 3, 4 , 5, 8, and 9 don’t count towards FLOPs
Therefore we have O(N3/2) FLOPS and 3√
NGEMMs of two√
N×√
Nmatrices.
Now that we have the operations in terms of blocks, we will make them causal in the following
sub-section.
D.4.2 Causal Block Monarch Convolution
Recall that a Monarch matrix is defined as
M=PLPRP ,
then per equation (37) we have
M′=PLPR . (43)
Then if q(j1,j0)(Z) is the basis polynomial corresponding to the (j1, j0)thcolumn of M, by(38)
the basis polynomial corresponding to Mis
q(j1,j0)(Z) =ℓj0(Z)·˜rj0,j1(Z√
N) (44)
with the minimum degree of j1√
N+j0(recall that we pick ℓj0and˜rj0,j1to have minimum degree
j0andj1respectively) as desired.
Theorem 10. LetL,R←Blocky Monarch (eL,eR). Let Mbe as in (37). Then for every 0≤i,
j < N , we have:
M[(i1, i0),(j1, j0)] =q(j1,j0)
ωi1√
N+i0
N
,
where q(j1,j0)is as in (44).
44

--- PAGE 45 ---
Proof. To prove this we need to show the (j1, j0)thcolumn of Mis the basis polynomial q(j1,j0)
evaluated at the Nthroots of unity, where q(j1,j0)is as defined in (44). Note we have
q(j1,j0)(ωi1√
N+i0
N) =ℓj0
ωi1√
N+i0
N
·˜rj0,j1
ω(i1√
N+i0)√
N
N
=ℓj0(ωi1√
N+i0
N)·˜rj0,j1
ωi0√
N
. (45)
By definition we have,
M′[(i1, i0),(j1, j0)] =Li1,j1[i0, i0]·Rj1,j1[i0, j0].
By (39) we have
Rj1,j1[i0, j0] = ˜rj1,j0
ωi0√
N
and by (41) and the fact that
L=PLP ,
we have
Li1,j1[i0, i0] =ℓj1
ωi1√
N+i0
N
.
Thus, we have
M′[(i1, i0),(j1, j0)] =ℓj1
ωi1√
N+i0
N
·˜rj1,j0
ωi0√
N
.
Since
M·P=M′,
we have
M[(i1, i0),(j1, j0)] =M′[(i1, i0),(j0, j1)]
=ℓj0
ωi1√
N+i0
N
·rj0,j1
ωi0√
N
=q(j1,j0)(ωi1√
N+i0
N),
where the last equality follows from (45).
Corollary 6. The DFT is Mas in Theorem 10 when all blocks of eRand the top block of eLare the
identity matrix of size√
N×√
N(the rest of eLis all 0’s).
Proof. Since only the diagonal of the top block of eLwill contain any non-zero values we only index
the first 0 , . . . ,√
N−1 rows. And since all blocks of eRare the identity matrix we get
eL[j1, j1] =˜ℓj1[j1] = 1
and
eRj1[j0, j0] = ˜rj1,j0[j0] = 1.
All other entries of eLandeRare 0. Thus, we have
ℓj1(Z) =Zj1
and
˜rj1,j0(Z) =Zj0.
45

--- PAGE 46 ---
As per Theorem 10,
q(j1,j0)(Z) =ℓj0(Z)·˜rj0,j1
Z√
N
=Zj0·Zj1√
N=Zj0+j1√
N=Zj.
Then by Theorem 10 note
q(j1,j0) 
ωi
N
=ωij
N=M[(i1, i0),(j1, j0)],
which implies MisFNas desired.
Algorithm 3 is pseudo code for the Monarch convolution algorithm. It maps an input space of
eL,eRmatrices, a kernel vector k, and input vector uto a vector f.
Algorithm 3 BlockMonarchConv (eL,eR,k,u)
Input: eL,eR∈RN×√
N,k,u∈RN
Output: f ∈RN
1:L,R←Blocky Monarch (eL,eR)
2:M←PLPRP ▷GetM′fromBlocky Monarch
▷Compute kf,uffromM
3:kf←M·k
4:uf←M·u
5:f←M−1·(kf⊙uf) ▷Compute f
6:return f
We next outline how to make the map in Algorithm 3 causal. Towards that end, we observe:
Lemma 6. For any fixed n≥1, let N=√
2n2. Define eL,eR∈RN×√
Nsuch that for every
0≤j1<√
N,
eL=
eL0
...
eL√
N−1
andeR=
eR0
...
eR√
N−1
.
Define eL′,eR′∈RN×√
Nwith corresponding coefficient blocks
eL′
k
0≤k<√
N,
eR′
k
0≤k<√
Nas
eL′=
eL′
0
0√
N×√
N...
0√
N×√
N
andeR′=
eR′
0
eR′
1...
eR′√
N−1

where
eL′
0[(i1, i0), j1] =(eL0[i0, j1]ifi1= 0andi0≥j1
0otherwise,eL′
k=0√
N×√
Nfork= 1,···,√
N−1
46

--- PAGE 47 ---
eR′
j1[i0, j0] =(
0ifi0< j0or((i0≥j√
N
2k
)and (j0<j√
N
2k
))
eRj1[i0, j0]otherwiseforj1= 0, . . . ,√
N−1.
Further, we require that eR′
j1[j0, j0],eL′
0[j1, j1]are all non-zero entries for all 0≤j0, j1<√
N.
Then for j0√
N+j1<√
Nj√
N
2k
, the basis polynomial q′
(j1,j0)(Z) =ℓ′
j1(Z)r′
j1,j0
Z√
N
ofM′=
PLPR has minimum degree j0√
N+j1and maximum degree ≤N
2−1, and for√
Nj√
N
2k
≤
j0√
N+j1< N, the basis polynomial q′
(j1,j0)(Z)has minimum degree j0√
N+j1and maximum
degree ≤N−1.
Note that in M=M′Pthe (j1, j0) basis polynomial qj1,j0(Z) =q′
j0,j1(Z) has the required degree
bound.
Proof. LetL′,R′←Blocky Monarch (eL′,eR′), and denote their corresponding polynomials as
ℓ′
j1(Z), r′
j1,j0
Z√
N
, respectively for every 0 ≤j1, j0<√
N. By our definition, for all 0 ≤j0, j1<
√
N,r′
j1,j0
Z√
N
has minimum degree j0√
Nandℓ′
j1(Z) has minimum degree j1. Then it follows
that the basis polynomial q′
(j1,j0)(Z) =ℓ′
j1(Z)r′
j1,j0
Z√
N
has minimum degree j1+j0√
N.
We now look at the degrees of each basis polynomial. From our definition of R′
j1, all entries
R′
j1[i0, j0] = 0 for j0<j√
N
2k
andi0≥j√
N
2k
. This implies that for 0 ≤j0<j√
N
2k
, and
0≤j1<√
N, we have deg(r′
j1,j0)<j√
N
2k
. Since we are only looking at L′
0(Z), note that degree
deg(ℓ′
j1)≤√
N−1. Then it follows that
deg
q′
(j1,j0)
≤√
N−1 + $√
N
2%
−1!
√
N
=√
N−1 + 
√
N$√
N
2%
−√
N!
≤√
N√
N
2−1
=N
2−1.
(Note that in the above j0√
N+j1≤√
Nj√
N
2k
−1 by same calculations above as needed.)
Forj√
N
2k
≤j0<√
Nand 0 ≤j1<√
N,deg(r′
j1,j0) =√
N−1, and ℓ′
j1(Z) degree√
N−1.
Then it follows that
deg
q′
(j1,j0)
≤√
N−1 +√
N−1√
N
=N−1,
as desired. (Note that j0√
N+j1≥j√
N
2k√
Nas needed.)
Finally, we use Lemma 6 and Theorem 8 to conclude the following:
47

--- PAGE 48 ---
Theorem 11. For any fixed n≥1, let N=√
2n2. LeteL′,eR′∈RN×Nwith corresponding
coefficient blocks
eL′
k
0≤k<√
N,
eR′
k
0≤k<√
Nbe defined as in Lemma 6. Then for any k,u∈Rn,
BlockMonarchConv
eL′,eR′,k′,u′
[0 :n−1], where k′= (k,0N−n),u′= (u,0N−n), is causal
inu.
Proof. Note that this is the same setting as Theorem 8. If N=√
2n2, thenN
2
≥n. Then by
Lemma 6, the basis polynomials qj1,j0(Z) ofM=PLPR have deg(q(j1,j0))<N
2for 0≤j1√
N+j0<√
Nj√
N
2k
≤N
2
.5
Then (34)computes BlockMonarchConv
eL′,eR′,k′,u′
. Since Algorithm 3 performs the
same operation as (34), the result follows from Lemma 6 and Theorem 8.
D.5 Bivariate Polynomials with Kronecker Substitution Over Reals
Our earlier results pertain to complex evaluation points. In this section we define causal convolution
over real evaluation points. To do this, we redefine our basis polynomials in terms of the Chebyshev
polynomials of the first kind.
In Appendix D.5.1 we recover Theorem 8 for univariate polynomials defined over real evaluation
points. This identifies a class of matrices that we use to define to define a structured subclass in
Appendix D.5.2. We show that these matrices can be used to perform (31). Finally, in Appendix D.5.3
we give the analog of Theorem 11 over real evaluation points. However, the resulting matrices are
not Monarch matrices but they are close. In Appendix D.5.4 and Appendix D.5.5 we discuss how
we can exploit this closeness, and compute (31) more efficiently.
D.5.1 Univariate Evaluation over Real Numbers
For any integer a≥0, the Chebyshev polynomial of the first kind with degree ais denoted as Ta(Z)
and is defined as
Ta(cosθ)def= cos( aθ), (46)
and has the property
Ta(−cosθ) = (−1)acos(aθ). (47)
To parallel Appendix D.3.2, we consider the class of basis polynomials with the form
¯qN
j(Z) =N−j−1X
a=0¯qj[a]Ta(Z), (48)
evaluated over ( ωN,i)0≤i<Nwhere
ωN,idef= cos 
π(i+1
2)
N!
. (49)
Let us consider the class of matrices defined over (48) and (49).
5Recall that Lemma 6 is stated for basis polynomials q′
j1,j0(Z) forM′=PLPR where qj1,j0(Z) =q′
j0,j1(Z).
48

--- PAGE 49 ---
Lemma 7. For sequence of points A={ωN,0, . . . , ω N,N−1}, letMbe defined as
M[i, j] = ¯qN
j(ωN,i) (50)
where ¯qN
j(Z)andωN,iare defined as in (48) and(49), respectively. Then for any vector u,M·uis
equivalent to evaluating the polynomial
u(Z) =N−1X
j=0uj·¯qN
j(Z) (51)
at each point in A.
Proof. By our definition of M, the column M[:, j] is exactly the evaluation of the polynomial ¯qN
j(Z)
at each point in A. The claimed result comes from the definition of matrix vector multiplication
and (51).
This leads to the following analog of Theorem 7.
Theorem 12. For matrices M0,M1,M2, each of form as in Lemma 7, the operation
f=M−1
0·((M1·k)⊙(M2·u)). (52)
is equivalent to representing the polynomial
f(Z) =k(Z)·u(Z) mod TN(Z)
in terms of the basis polynomials
ˆqN
j(Z)forj= 0, . . . , N −1
where k(Z), u(Z)are defined in terms of the respective basis polynomials corresponding to M1and
M2as in Lemma 7, and
ˆqN
j(Z)
0≤j<Ncorresponds to M0.
Proof. ForA={ωN,i}0≤i<N, define
qA(Z) =Y
α∈A(Z−α). (53)
Then (52) follows since for
f(Z) =k(Z)·u(Z) mod qA(Z),
we have the following for any α∈A:
f(α) =k(α)·u(α).
The claim follows from Lemma 7, (52), the invertibility of M0, and the known fact that TN(Z) =
qA(Z).
We also utilize the following result:
49

--- PAGE 50 ---
Lemma 8. Let¯q⌊N
2⌋
j(Z)be defined as in (48). Then for any 0≤j, j′< N,
¯q⌊N
2⌋
j(Z)·¯q⌊N
2⌋
j′(Z) =N−1X
i=j+j′αj+j′,i·¯qN
i(Z). (54)
for some set of coefficients αj+j′,i.
Proof. From (48), we have
¯q⌊N
2⌋
j(Z)·¯q⌊N
2⌋
j′(Z) =⌊N
2⌋−j−1X
a=0qj[a]Ta(Z)·⌊N
2⌋−j′−1X
a′=0q′
j′[a′]Ta′(Z).
Recall that within a mdimensional vector space of polynomials, we can define a basis by choosing
any set of polynomials with degrees 0 , . . . , m −1. Because deg
¯q⌊N
2⌋
j·¯q⌊N
2⌋
j′
≤2N
2
−(j+j′)−2,
it can be written as a linear combination of any set of 2N
2
−(j+j′)−1 polynomials where for
0≤a≤2N
2
−(j+j′)−2, the a-th polynomial has degree a. In other words we can choose the
a-th polynomial as qN
N−a−1(Z). Thus we have
¯q⌊N
2⌋
j(Z)·¯q⌊N
2⌋
j′(Z) =2⌊N
2⌋−(j+j′)−2X
a=0¯αj+j′,a·¯qN
N−a−1(Z)
for some set of coefficients αj+j′,a. Then after reindexing i←N−a−1 we have
¯q⌊N
2⌋
j(Z)·¯q⌊N
2⌋
j′(Z) =N−1X
i=(N−2⌊N
2⌋)+j+j′+1¯αj+j′,N−i−1·¯qN
i(Z).
The claim follows by setting αj+j′,j+j′+1= 0, and if Nis odd, αj+j′+1,j+j′+2= 0, and αj+j′,i=
¯αj+j′,N−i−1for other i.
This allows us to prove the following causality result for convolutions over real evaluation points.
Theorem 13. Fix a family of basis polynomials ¯qN
0,¯qN
1. . . ,¯qN
N−1as defined in (48). Let N≥1be a
perfect square, n≤N
2
,k,u∈RnandMNdefined by basis
¯qN
j(Z)
0≤j<N. Letk′′=
k,0⌊N
2⌋−n
andu′′=
u,0⌊N
2⌋−n
. Then the operation
u7→
M−1
N
MN·
0⌈N
2⌉,k′′
◦MN·
0⌈N
2⌉,u′′
[0 :n−1] (55)
defines a causal map in u.
Proof. Let
f=
M−1
N
MN·
0⌈N
2⌉,k′′
◦MN·
0⌈N
2⌉,u′′
[0 :n−1]. (56)
In order to prove that (55) is actually causal in the input u∈Rn, we must show that for all
0≤i < N ,f[i] is dependent only on u[0],u[1], . . .u[i]. Let k′=
0⌈N
2⌉,k′′
andu′=
0⌈N
2⌉,u′′
.
By Lemma 7, MN·k′andMN·u′correspond to the evaluations of the polynomials
50

--- PAGE 51 ---
k′(Z) =N−1X
j=0k′
j·¯qN
j(Z),and u′(Z) =N−1X
j′=0u′
j′·¯qN
j′(Z). (57)
Let us define
k(Z) =n−1X
j=0kj·¯q⌊N
2⌋
j(Z),and u(Z) =n−1X
j′=0uj·¯q⌊N
2⌋
j′(Z). (58)
Note that for 0 ≤j <N
2
, the coefficients k′
j=u′
j= 0. Then (57) becomes
k′(Z) =N−1X
j=⌈N
2⌉k′
j·¯qN
j(Z),and u′(Z) =N−1X
j′=⌈N
2⌉u′
j·¯qN
j′(Z),
which is equivalent to
k′(Z) =⌊N
2⌋−1X
j=0k′
j+⌈N
2⌉·¯qN
j+⌈N
2⌉(Z),and u′(Z) =⌊N
2⌋−1X
j′=0u′
j′+⌈N
2⌉·¯qN
j′+⌈N
2⌉(Z).
For 0≤j <N
2
,deg
¯q⌊N
2⌋
j
=N
2
−j−1, and deg
¯qN
j+⌈N
2⌉
=N−N
2
−j−1 =N
2
−j−1.
This implies that ¯q⌊N
2⌋
j(Z) and ¯qN
j+⌈N
2⌉(Z) are both linear combinations ofN
2
−j−1 Chebychev
polynomials. Then we can set ¯q⌊N
2⌋
j(Z) = ¯qN
j+⌈N
2⌉(Z). Similarly, note that for 0 ≤j < n ,
k′
j+⌈N
2⌉=kj. Then it follows that k(Z) =k′(Z), and by a similar argument, u(Z) =u′(Z). Then
by Theorem 12 we have
f(Z) =k(Z)·u(Z) mod TN(Z)
=k(Z)·u(Z) (59)
where the last statement follows since deg (k(Z)),deg (u(Z))≤n−1<N
2
, implying that their
product has deg ¯k(Z)·¯u(Z)
<deg (TN(Z)) =N. We want to write f(Z) in the form
f(Z) =N−1X
i=0fi·¯qN
i(Z)
for some set of coefficients set of coefficients fi. From (58), (59) becomes
f(Z) =n−1X
j=0n−1X
j′=0kj′·uj·¯q⌊N
2⌋
j′(Z)·¯q⌊N
2⌋
j(Z).
Then by Lemma 8 we have
N−1X
i=0fi·¯qN
i(Z) =n−1X
j,j′=0kj′·uj·N−1X
i=j+j′αj+j′,i¯qN
i(Z).
51

--- PAGE 52 ---
We show that for all 0 ≤i < N ,fiis a function of (ui′)0≤i′≤i. Then note from the above that each
kjanduj′appears in terms of ¯ qN
iwhere i≥j+j′. Then we have
fi=N−1X
j,j=0
j+j′≤iαj+j′,i·kj′·uj,
as desired.
D.5.2 Structured Causal Matrices
In this section we narrow our scope from the general class of matrices defined in Appendix D.5.1 to
a particular structured subclass. Now let us define the class of structured causal matrices over the
real numbers.
Definition 6. Define
ℓj1(Z)def=j1X
a=0ℓj1[a]Ta(Z), ˜rj1,j0(Z)def=j0X
a=0˜rj1,j0[a]Ta(Z) (60)
where
˜rj1,j0[a] = 0 if(j0−a)is odd, (61)
We define structured causal (SC) matrix polynomials as
qN
(j1,j0)√
N(Z) =ℓ√
N−j1−1(Z)·˜r√
N−j1−1,√
N−j0−1
T√
N(Z)
. (62)
AN×NSC matrix is defined over the set of real evaluation points as
M′[i,(j0, j1)] =q′N
(j1,j0)√
N(ωN,i) =qN
(j0,j1)√
N(ωN,i). (63)
We note that in (63) we deviate from the usual ordering of indices ( j1, j0) to ( j0, j1). We show
that the SC matrix falls under the category of matrices defined by (50).
Lemma 9. LetMCdenote the set of all matrices defined by of (50), andMSCdenote the set of
all matrices defined by Definition 6. Then MSC⊂ MC.
Proof. To show that MSC⊂ MC, then it is sufficient to show that for j=j0√
N+j1, any
q′N
(j1,j0)√
N(Z) is equivalent to some ¯ qN
j(Z) as in (48). From (63) we have
q′N
(j1,j0)√
N(Z) =ℓ√
N−j0−1(Z)·˜r√
N−j0−1,√
N−j1−1
T√
N(Z)
. (64)
Note that
deg
q′N
(j1,j0)√
N
=√
N−j0−1 + (√
N−j1−1)√
N
=N−√
Nj1−j0−1
=N−j−1.
52

--- PAGE 53 ---
From the fact that deg (Ta)=a, we can use Taas a polynomial basis. Then q′N
(j1,j0)√
Ncan be
represented as a linear combination of Ta(Z) like so:
q′N
(j1,j0)√
N(Z) =N−j−1X
a=0q(j1,j0)[a]Ta(Z),
which is exactly the form of (48).
Lemma 9 allows us to apply the causality result from Appendix D.5.1.
Corollary 7. Fix a family of basis polynomials qN
0,0, qN
0,1, . . . , qN√
N−1,√
N−1as defined in (62). For
any perfect square N≥1,n≤N
2
,k,u∈RnandM′
Ndefined by basis
q′N
(j1,j0)√
N(Z)
0≤j1,j0<√
N
as in (63). Then the operation (55) withMN←M′
Ndefines a causal map in u.
Proof. Follows from Theorem 13 and Lemma 9.
D.5.3 Block Operations on Structured Causal Matrices
In this section we show how to build structured causal matrices through block operations.
Constructing M
Recall that in Appendix D.4.1, we defined L,Rin terms of coefficient matrices eR,eL∈RN×√
N
with blocks eRk,eLkfor 0≤k <√
N, noting that for each block Rj1,j1,
Rj1,j1=F√
N·eRj1,j1,
and for eL∈RN×√
N
L′=PFN·eL.
These matrices are then diagonalized into L,R. We use Definition 6 to similarly define L,R,∈
RN×Nwith blocks {Lj1,j0}0≤j1,j0<√
N,{Rj1,j0}0≤j1,j0<√
Nwhere:
Li1,j1[i0, i0] =ℓ√
N−j1−1(ωN,i)Rj1,j1[i0, j0]←˜r√
N−j1−1,√
N−j0−1(ωN,i), (65)
and all other entries are zero. Let the coefficient matrices eL,eR∈RN×√
Nbe defined with respect
to blocks as follows:
eL[a, j1] =ℓ√
N−j1−1[a]eRj1[a, j0] = ˜r√
N−j1−1,√
N−j0−1[a],
where the entries of ℓ√
N−j1−1and˜r√
N−j1−1,√
N−j0−1are defined as in (60)and(61). Now let
CN∈RN×Nwhere
CN[i, j] =Tj(ωN,i) (66)
be the Chebyshev transform. Then analogous to Appendix D.4.1, we define
Rj1,j1=C√
N·eRj1,j1 L′=PCN·eL.
This allows us to give an algorithm the following construction algorithm for LandR.
53

--- PAGE 54 ---
Algorithm 4 BlockSC (eL,eR)
Input: eL,eR∈RN×√
N
Output: Block diagonal matrices L,R∈RN×N
▷First, compute LfromeL
1:L′←P·CN·eL
2:fora←0 to√
N−1do
3: L′
a←L′[a√
N:a√
N+√
N−1,:]
4:L←diag(L′
0, . . .L′√
N−1)
▷Now compute RfromeR
5:fora←0 to√
N−1do
6: Ra←C√
N·eR[a√
N:a√
N+√
N−1,:]
7:R←diag
R0, . . . ,R√
N−1
8:return L ,R
We use Algorithm 4 to specify another type of matrix M∈RN×N.
Lemma 10. Let
M=PLPRP
where LandRare outputs from BlockSC . Then each entry in Mis defined as
M[i, j] =ℓ√
N−j0−1(ωN,i)·˜r√
N−j0−1,√
N−j1−1
ω√
N,i0
(67)
Proof. LetM0=PLPR , then M=M0P. Then we have
M0[(i1, i0),(j1, j0)] =ℓ√
N−j1−1(ωN,i)·˜r√
N−j1−1,√
N−j0−1(ωN,i0).
Then we get
M[i, j] =M0P[(i1, i0),(j1, j0)] =M0[(i1, i0),(j0, j1)].
This gives us
M[(i1, i0),(j1, j0)] =ℓ√
N−j0−1(ωN,i)·˜r√
N−j0−1,√
N−j1−1(ωN,i0).
as desired.
Next we will discuss the relation between matrices that we get from Lemma 10 and SC matrices.
D.5.4 Constructing M′from M
Since CN/∈ MSC6, Algorithm 4 is not a proper analog of Algorithm 2. In this section, we
show how to convert Mproduced from Algorithm 4 into a matrix with a block decomposible form.
Recall from (63) that
M′[i, j] =q′N
(j1,j0)√
N(ωN,i) =ℓ√
N−j0−1(Z)·˜r√
N−j0−1,√
N−j1−1
T√
N(Z)
. (68)
6We do not have a proof of this claim, but to us it seems unlikely that CN∈ MSC
54

--- PAGE 55 ---
We note the distinction between the above and (67). Specifically, we note that Mis evaluated on
two sets of evaluation points– the ℓ√
N−j1−1(Z) and ˜r√
N−j1−1,√
N−j0−1polynomials are evaluated
over the Nroots and√
Nroots of unity while M′is only evaluated the N-th roots of unity. However,
they are close due to the following property:
Lemma 11. LetTabe a Chebyshev polynomial of the first kind of degree a, and define ωN,ias in
(49). Then
T√
N(ωN,i) = (−1)i1·ω√
N,i0.
Proof.
T√
N(ωN,i) = cos √
N(i1√
N+i0+1
2)π
N!
= cos 
i1π+π 
i0+1
2
√
N!
= (−1)i1cos 
π 
i0+1
2
√
N!
= (−1)i1·ω√
N,i0.
In the above the first equality follows from (46).
Analogous to how we utilized roots of unity, Lemma 11 allows us to express our basis polynomials
in terms of two related sets of evaluation points, ω√
N,i0andωN,i. The following lemma demonstrates
how to translate between M′andM.
Lemma 12. For all i, j,
M′[i, j] = (−1)ii(√
N−1)(−1)iij1·M[i, j]. (69)
Proof. By (68) we have
M′[i, j] =ℓ√
N−j0−1(ωN,i)·˜r√
N−j0−1,√
N−j1−1
T√
N(ωN,i)
=ℓ√
N−j0−1(ωN,i)·˜r√
N−j0−1,√
N−j1−1
(−1)i1ω√
N,i0
where the second statement follows from Lemma 11. Then from (60) and (47) we get
M′[i, j] =ℓ√
N−j0−1(ωN,i)·√
N−j1−1X
a=0˜r√
N−j0−1,√
N−j1−1[a]Ta
(−1)i1ω√
N,i0
=ℓ√
N−j0−1(ωN,i)·√
N−j1−1X
a=0·˜r√
N−j0−1,√
N−j1−1[a](−1)i1aTa
ω√
N,i0
.
Note from (61)that ˜r√
N−j0−1,√
N−j1−1[a] = 0 if(√
N−j1−1−a)is odd . Then equivalently
we get
55

--- PAGE 56 ---
M′[i, j] =ℓ√
N−j0−1(ωN,i)·(−1)i1(√
N−j1−1)
·√
N−j1−1X
a=0˜r√
N−j0−1,√
N−j1−1[a] (−1)i1(√
N−j1−1−a)Ta
ω√
N,i0
=ℓ√
N−j0−1(ωN,i)·(−1)i1(√
N−j1−1)
√
N−j1−1X
a=0˜r√
N−j0−1,√
N−j1−1[a]Ta
ω√
N,i0

= (−1)i1(√
N−j1−1)ℓ√
N−j0−1(ωN,i)·˜r√
N−j0−1,√
N−j1−1
ω√
N,i0
.
Then from (67) we have
M′[i, j] = (−1)i1(√
N−j1−1)M[i, j]
= (−1)i1(√
N−1)(−1)i1j1M[i, j].
Block Matrix Multiplication for Structured Causal Matrices Lemma 12 shows us how to
compute M′fromM. However, this does not mean that the matrix vector multiplication problem
forM′can be implemented efficiently. In this section we show how to perform matrix vector
multiplication of M′with any u∈RNfrom two matrix-vector multiplications of M(and so these
operations are indeed efficient).
The key to our approach involves considering the parity of each block index 0 ≤i1<√
N. Let
us define a map Mix :R⌊N
2⌋×R⌈N
2⌉7→RNsuch that Mix(u0,u1) =uwhere
u[i] =ui1mod 2 [i/2].
We use this map to show that M′uandu⊤M′can be computed efficiently.
Lemma 13. For any u∈RNandM∈RN×N,M′ucan be computed via two matrix-vector
multiplications: M·Mix(u0,0)andM·Mix(0,u1).
Proof. For 0≤i=i1√
N+i0< N, 0≤j=j1√
N+j0< N, letD∈RN×Nbe the diagonal matrix
defined such that D[i, i] = (−1)i1(√
N−1). Lemma 12 implies that
M′[i, j] =

DM[i, j] if i1is even
DM[i, j] if i1is odd, j1is even
−(DM[i, j]) otherwise.
We want to shift the ( −1) to u. Compute z0=M·(Mix(u0,0) +Mix(0,u1)),z1=M·
(Mix(u0,0)−Mix(0,u1)). Define y′such that
y′[j] =(
z0[j] if j1is even
z1[j] if j1is odd.
It can be verified that D·y′=M′u, which completes the proof.
We now give the analogous result for u⊤M′.
56

--- PAGE 57 ---
Lemma 14. For any u∈RNandM∈RN×N,u⊤M′can be computed via two matrix-vector
multiplications: Mix(u0,0)⊤MandMix(0,u1)⊤M.
Proof. For 0≤i=i1√
N+i0< N, 0≤j=j1√
N+j0< N, letD∈RN×Nbe the diagonal matrix
defined such that D[i, i] = (−1)i1(√
N−1), and u′=u⊤D=Mix(u′
0,u′
1). Lemma 12 implies that
M′[i, j] =

DM[i, j] if i1is even
DM[i, j] if i1is odd, j1is even
−(DM[i, j]) otherwise.
We want to shift the ( −1) to u′. Compute z0=
Mix(u′
0,0)⊤+Mix(0,u′
1)⊤
·M,z1=
Mix(u′
0,0)⊤−Mix(0,u′
1)⊤
·M. Ify=M′u, then one can check that
y[j] =(
z0[j] if j1is even
z1[j] if j1is odd,
which completes the proof.
Lemma 13 implies that computing MNkandMNuin(55)can be done efficiently. However,
we do not know how to compute M−1
Nyfor any arbitrary y. We address this partially in the next
section.
D.5.5 Inverse of Chebyschev Transform
In this subsection we show how computing C−1
Ncan be done efficiently.
Lemma 15. LetCN∈RN×Nbe defined as (66). Then
C−1
N=C⊤
N·diag(1/N,2/N, . . . , 2/N).
Proof. Follows since CN(CN)⊤= diag( N, N/ 2, N/2, . . . , N/ 2) [70].
Given the above, the goal then is to compute C⊤
N·uor equivalently u⊤CNfor any u∈RN.
Note that is sufficient to show that
CN=CN+SN
forCN,SN∈RN×Nsuch that u⊤CNandu⊤SNcan be computed with two invocations of Lemma 14.
Indeed we have
CN[i, j] = cos 
(i+1
2)(j1√
N+j0)π
N!
= cos 
π(i+1
2)j1√
N+π(i+1
2)j0
N!
,
= cos 
π(i+1
2)j1√
N!
cos 
π(i+1
2)j0
N!
−sin 
π(i+1
2)j1√
N!
sin 
π(i+1
2)j0
N!
. (70)
We use this to define CNandSN, along with two additional matrices eCN,eSN∈RN×Nsuch
that for CNandeCN:
57

--- PAGE 58 ---
CN[i, j] = cos 
π(i+1
2)j1√
N!
cos 
π(i+1
2)j0
N!
= cos 
πi1j1+π(i0+1
2)j1√
N!
cos 
π(i+1
2)j0
N!
= (−1)i1j1cos 
π(i0+1
2)j1√
N!
cos 
π(i+1
2)j0
N!
def= (−1)i1j1eCN[i, j], (71)
and similarly for SNandeSN:
SN[i, j] = sin 
π(i+1
2)j1√
N!
sin 
π(i+1
2)j0
N!
= (−1)i1j1sin 
π(i0+1
2)j1√
N!
sin 
π(i+1
2)j0
N!
def= (−1)i1j1eSN[i, j]. (72)
We summarize these results into the following lemma.
Lemma 16. Define CN,eCN,SNandeSNas in (71) and(72), respectively. Then
CN=CN−SN
where eCNandeSNare of the form (67).
Finally Lemma 14 and Lemma 16 imply the following:
Theorem 14. For any u∈RN,
y=u⊤CN
can be computed from four calls matrix vector multiplication with Monarch matrices.
Proof Sketch for Theorem 14 Lemma 16 tells us that u⊤CNandu⊤SNare sufficient to
compute u⊤CN, and (72) shows us that eCN,eSNare Monarch matrices that allows u⊤CNand
u⊤SNto be computed from two matrix- vector multiplications with eCNandeSN, respectively. Then
the claim follows from applying Lemma 14 twice.
D.6 Multivariate Monarch Mixer with Block Size =p√
N
In this section, we generalize Theorem 1, Theorem 2, and Theorem 3 to arbitrary p≥2. In
Appendix D.6.1, we set up notation. We then generalize Theorem 1 and Theorem 2 to general
pin Appendix D.6.2 and then generalize Theorem 3 in Appendix D.6.3. In Appendix D.6.4 we
connect definition of p-variate Monarch in Appendix D.6.2 to the one defined in Section 3. Finally
in Appendix D.6.5 we discuss some extensions and generalizations.
58

--- PAGE 59 ---
D.6.1 Notation
We will use notation from Appendix D.5.1.
Fix an integer p≥2. We will be working with indices j,iwhere j= (j0, . . . , j p−1) and
i= (i0, . . . , i p−1) with 0 ≤ia, ja<p√
Nfor every 0 ≤a < p . We will denote the set of all sub-indices
as [0,p√
N)p, and the operator ⪯to denote the lexicographical ordering of vectors in [0 ,p√
N)p.
For any 0 ≤b′≤M≤Nsuch that b′divides MandMdivides N, define the following
permutation matrices:
Pb′,M,N = diag
Pb′,M, . . . ,Pb′,M| {z }
N
Mtimes
.
Note that Pb′,N,N is exactly the same as Pb′,Nfrom earlier.
If we use σ(b, M, N ) to denote the corresponding permutation then it takes the input (ip−1, . . . , i 0)b
and maps it to 
ip−1, . . . , i p′, ip′−2, . . . , i 0, ip′
b(where p=logbNandp′=logbM). I.e. σ(b, M, N )
does not change the first p−p′sub-indices and then does a left rotation on the remaining p′
sub-indices.
For the rest of the section, assume b=p√
Nand then consider the following ‘sub-index reversal’
permutation matrix7:
PR
b,bp=p−2Y
a=0Pbp−a−1,bp−a,bp.
IfσR(b, N) is the permutation corresponding to the permutation matrix above, then σR(b, N)
maps ( ip−1, . . . , i 0)7→(i0, . . . , i p−1).
D.6.2 Generalizing Theorem 1 and Theorem 2
For a specific 0 ≤a < p , let
ℓ(a)
j,i(Xa) =p√
N−1X
m=0ℓ(a)
(ja+1,...,jp−1),(i0,...,ia−1)[m]·Tm(Xa) (73)
be an arbitrary polynomial of degree <p√
Nin the Chebyshev polynomial basis (see (47)).
We will be interested in the evaluations of the above polynomials over the set
Adef=
ωp√
N,0, . . . , ω p√
N,p√
N−1
, (74)
where ω√
N,iis defined as in ( ??).
Thep-variate version of Monarch matrices M′∈RN×Nas follows. (See Appendix D.6.4 to see
how these are exactly related to the definition in (1)). For every row index i∈[0,p√
N)pand column
index j∈[0,p√
N)p, we have
M′[i,j] =p−1Y
a=0ℓ(a)
j,i
ωp√
N,ia
. (75)
7b= 2 gives the well known bit reversal permutation.
59

--- PAGE 60 ---
To express the above in terms of a polynomial basis we will need the following definition. For
any 0 ≤a < p andidefine the Lagrange basis polynomial ∆(a)
i(X0, . . . , X a−1)such that for any
0≤m0, . . . , m a−1<p√
N, we have
∆(a)
i
ωp√
N,m 0, . . . , ω p√
N,m a−1
=(
1 if i0=m0, i1=m1, . . . , i a−1=ma−1
0 otherwise .
We use the above to convert the polynomials in (73)to not depend on the sub-indices in i(at
least for the definition in (75)). For every jand 0 ≤a < p , define:
ℓ(a)
j(X0, . . . , X a) =X
i=(i0,...,ia−1,0p−a),i0,...,ia−1∈[0,p√
N)∆(a)
i(X0, . . . , X a−1)·ℓ(a)
j,i(Xa).
Note that the summation fixes the last p−asub-indices in isince the definition of ℓ(a)
j,i(Xa)only
depends on ( i0, . . . , i a−1). This implies that for any 0 ≤i0, . . . , i a<p√
N, we have
ℓ(a)
j
ωp√
N,i0, . . . , ω p√
N,ia
=ℓ(a)
j,i
ωp√
N,ia
. (76)
We are now ready to define our basis p-variate polynomials. For any index j∈[0,p√
N)p, define
qN
j(X0, . . . , X p−1) =p−1Y
a=0ℓ(a)
j(X0, . . . , X a). (77)
Then (75) can be re-written as
M[i,j] =qN
j
ωp√
N,i0, . . . , ω p√
N,ip−1
. (78)
The above leads to the following result, which generalizes Theorem 1 to general p:
Theorem 15. LetM′,AandqN
j(X0, . . . , X p−1)be as defined in (75),(74) and(77). Then for
any vector u,M·uis equivalent to evaluating the polynomial
u(X0, . . . , X p−1) =X
juj·qN
j(X0, . . . , X p−1) (79)
at each point in Ap.
Proof. By(78), the column M′[:, j] is exactly the evaluation of the polynomial (77)at each point in
Ap. Then the claim follows from the definition of matrix vector multiplication and (79).
This then leads to the following result, (which generalizes Theorem 2):
Theorem 16. For matrices M0,M1,M2, each of form as in Theorem 15, the operation
f=M−1
0·((M1·k)⊙(M2·u)). (80)
is equivalent to representing the polynomial
f(X) =k(X)·u(X) mod
Tp√
N(X0), . . . , T p√
N(Xp−1)
in terms of the basis polynomials
ˆqN
j(X)
where k(X), u(X)are defined in terms of the respective basis polynomials corresponding to M1and
M2as in Theorem 15, and ˆqN
j(X)s corresponds to M0.
60

--- PAGE 61 ---
Proof. Define
qA(Z) =Y
α∈A(Z−α). (81)
Then (80) follows since for
f(X) =k(X)·u(X) mod ( qA(X0), . . . q A(Xp−1),
we have the following for any a∈Apwe have:
f(a) =k(a)·u(a).
Using the known fact that Tp√
N(Z) =Qp√
N−1
c=0
Z−ωp√
N,c
=qA(Z), the claim follows from
Theorem 15, (80), and the invertibility of M0.
D.6.3 Generalizing Theorem 3 for p≥2
To convert Theorem 16 into a causal map we basically have to blow up n→2p·n. Further,
paralleling (48)we need to change the definition in (73)to have degreep√
N−ja−1 instead of the
earlierp√
N−1:
eℓ(a)
j,i(Xa) =p√
N−ja−1X
m=0eℓ(a)
(ja)[m]·Tm(Xa). (82)
Note that now the RHS only depends on aandja(let us call the RHS eℓ(a,p√
N)
ja(Xa)), so the
next definition becomes easier:
eℓ(a)
j(X0, . . . , X a) =eℓ(a,p√
N)
ja(Xa).
We are now ready to define our causal basis polynomials. For any index j, define
eqN
j(X0, . . . , X p−1) =p−1Y
a=0eℓ(a)
j(X0, . . . , X a). (83)
These polynomials form a structured subclass of the polynomials defined in (77).
Lemma 17. The class of polynomials eqN
j(X0, . . . , X p−1)defined in (83) are a special case of (77).
Proof. This follows from the fact that (82) is a special case of (73) for every i,j,0≤a < p .
We show that the product of two eqjp√
N
2kp
j(X0, . . . , X p−1)type polynomials can be written has a
linear combination of eqN
m(X0, . . . , X p−1)with the indices of mbeing lexicographically larger than
the original indices.
Lemma 18. Leteqjp√
N
2kp
j(X0, . . . , X p−1)be defined as in (83). Then for any j,j′∈h
0,p√
N
2p
,
eqjp√
N
2kp
j(X0, . . . , X p−1)·eqjp√
N
2kp
j′ (X0, . . . , X p−1) =X
j+j′⪯m∈[0,p√
N)pαj+j′,meqN
m(X0, . . . , X p−1) (84)
for some set of coefficients αj+j′,m.
61

--- PAGE 62 ---
Proof. From (83) we have,
eqjp√
N
2kp
j(X0, . . . , X p−1)·eqjp√
N
2kp
j′ (X0, . . . , X p−1) =p−1Y
a=0eℓ
a,jp√
N
2k
ja(Xa)·eℓ
a,jp√
N
2k
j′a(Xa) (85)
Let us fix 0 ≤a < p .
Because (82)is of the same form as in (48), we can apply Lemma 8 to each product eℓ
a,jp√
N
2k
ja(Xa)·
eℓ
a,jp√
N
2k
j′a(Xa), which gives us
eℓ
a,jp√
N
2k
ja(Xa)·eℓ
a,jp√
N
2k
j′a(Xa) =p√
N−1X
ma=ja+j′aα(a)
ja+j′a,ma·eℓ(a,p√
N)
ma (Xa).
Going back to (85), we get
eqjp√
N
2kp
j(X0, . . . , X p−1)·eqjp√
N
2kp
j′ (X0, . . . , X p−1) =p−1Y
a=0N−1X
ma=ja+j′aα(a)
ja+j′a,ma·eℓ(a,p√
N)
ma (Xa).
Letαj+j′,m=Qp−1
a=0α(a)
ja+j′a,ma. Then we get
eqjp√
N
2kp
j(X0, . . . , X p−1)·eqjp√
N
2kp
j′ (X0, . . . , X p−1) =X
j+j′⪯m∈[0,p√
N)pαj+j′,meqN
m(X0, . . . , X p−1),
as desired.
We now define the following padding scheme, Pad(k).
Algorithm 5 Pad(k)
Input: k ∈
Rjp√
N
2kp
, indexed as kjforj∈h
0,p√
N
2p
Output: k′∈
Rp√
Np
1:for j∈h
0,p√
N
2p
do
2: ifja≥jp√
N
2k
for 0≤a < p then
3: k′
(j0,...,jp−1)←k
j0+lp√
N
2m
,...,jp−1+lp√
N
2m
4: else
5: k′
j= 0
6:return k′
The above basis and padding scheme allows us to extend Theorem 3 to general p.
Theorem 17. Fix a family of basis polynomials eqN
j(X)as defined in (83). Let N≥1be a perfect
power of p,n≤jp√
N
2kp
,k,u∈RnandM′
Ndefined by basis eqN
j(X). Then the operation
u7→
M′−1
N 
M′
N·Pad(k)◦M′
N·Pad(u)
[0 :n−1] (86)
defines a causal map in u.
62

--- PAGE 63 ---
Proof. Letk′=Pad(k),u′=Pad(u), and
f=
M′−1
N 
M′
N·k′◦M′
N·u′
[0 :n−1]. (87)
In order to prove that (86)is causal in the input u∈Rn, we must show that for all i∈h
0,p√
Np
,
fiis dependent only on ui′fori′⪯i.
By Theorem 15, M′
N·k′andM′
N·u′correspond to the evaluations of the polynomials
k′(X0, . . . , X p−1) =X
j′∈[0,p√
N)pk′
j′·eqN
j′(X0, . . . , X p−1),and
u′(X0, . . . , X p−1) =X
j′∈[0,p√
N)pu′
j′·eqN
j′(X0, . . . , X p−1), (88)
respectively. Let us define
k(X0, . . . , X p−1) =X
j∈h
0,jp√
N
2kpkj·eqjp√
N
2kp
j(X0, . . . , X p−1),and
u(X0, . . . , X p−1) =X
j∈h
0,jp√
N
2kpuj·eqjp√
N
2kp
j(X0, . . . , X p−1). (89)
Let us define deg
eqN
j
=
p√
N−ja−1p−1
a=0as the vector of length pconsisting of the degrees of
the component univariate polynomials eℓ(a)
j,i(Xa)defined as in (82). Then we have deg 
eqjp√
N
2kp
j!
=
jp√
N
2k
−j0−1, . . . ,jp√
N
2k
−jp−1−1
forj=(j0,···, jp−1)such that 0 ≤ja<lp√
N
2m
for 0≤
a < p . Further, for j′=
j0+lp√
N
2m
,···, jp−1+lp√
N
2m
we have
deg 
eqN
j′
=
p√
N−j′
0−1, . . . ,p√
N−j′
p−1−1
= 
p√
N−&
p√
N
2'
−j0−1, . . . ,p√
N−&
p√
N
2'
−jp−1−1!
= $
p√
N
2%p
−j0−1, . . . ,$
p√
N
2%p
−jp−1−1!
.
Since deg 
eqjp√
N
2kp
j!
=deg
eqN
j′
, we can set eℓ(a)
j,i(Xa)=eℓ(a)
j′,i(Xa). Similarly, note that for j,j′as
above, k′
j′=kjandu′
j′=uj. Then it follows that k(X0, . . . , X p−1)=k′(X0, . . . , X p−1), and by a
similar argument, u(X0, . . . , X p−1) =u′(X0, . . . , X p−1). Then by Theorem 16 we have
f(X0, . . . , X p−1) =k(X0, . . . , X p−1)·u(X0, . . . , X p−1) mod
Tp√
N(X0), . . . , T p√
N(Xp−1)
=k(X0, . . . , X p−1)·u(X0, . . . , X p−1) (90)
63

--- PAGE 64 ---
where the second line follows by observing that each 0 ≤a < p , we have degXa(k(X)·u(X))<
2jp√
N
2k
and observing that 2jp√
N
2k
≤p√
N. We want to write f(X0, . . . , X p−1) in the form
f(X0, . . . , X p−1) =X
m∈[0,p√
N)pfm·eqN
m(X0, . . . , X p−1),
for a set of coefficients fm. From (89) and (90) we get
f(X0, . . . , X p−1) =X
j,j′∈h
0,jp√
N
2kpkj′uj·eqjp√
N
2kp
j′ (X0, . . . , X p−1)·eqjp√
N
2kp
j(X0, . . . , X p−1).
Then by Lemma 18 we have
f(X0, . . . , X p−1) =X
j,j′∈h
0,jp√
N
2kpkj′uj·X
j+j′⪯m∈[0,p√
N)pαj+j′,meqN
m(X0, . . . , X p−1).
Thus, for any m∈h
0,p√
Np
, we have
fm=X
j+j′⪯mαj+j′,m·kj′uj
implying that fmdepends only on ujforj⪯m, as desired.
D.6.4 p-variate Monarch
Recall that we have fixed b=p√
N(and hence N=bp).
Define the p-variate Monarch matrix as follows:
M′=PR
b,N p−2Y
a=0Bp−1−a· 
Pba+1,ba+2,N⊤!
B0, (91)
Where each Bais block diagonal with b×bblocks for every 0 ≤a < p . Recall that Equation (1)
has a permutation P0at the end while the above definition does not have any permutation at the
end. One trivial way to show the equivalence of above to Equation (1) is to define P0=I. In
Appendix D.6.5, we show that exists other non-trivial choices for P0. Further for 1 ≤i≤p, thePi
in Equation (1) connect to the above definition as follows:
Pi=( 
Pbp−i,bp−i+1,N⊤for 1≤i≤p−1
PR
b,Nfori=p.
Finally we connect the above definition of p-variate Monarch to the earlier definition based on
polynomial evaluation:
Lemma 19. Equation (75) can be written as Equation (91).
64

--- PAGE 65 ---
Proof. In(91),Bawould correspond to the evaluations ℓ(a)
j,i
ωp√
N,ia
from earlier. Specifically the
following holds for any 0 ≤a < p . We index the qp−1blocks of Baby(i0, . . . , i a−1),(jp−1, . . . , j a+1)
and the row and column ‘offsets’ within each such block are indexed by iaandjarespectively.
Connecting back to ℓ(a)
j,i(·) we set
B((i0,...,ia−1),(jp−1,...,ja+1))
a [ia, ja] =ℓ(a)
j,i
ωp√
N,ia
. (92)
We will prove the claim as follows. Let ej∈RNhave a 1 in the jthlocation and 0’s elsewhere.
Our goal is to multiply this vector on the right of Equation (91) and show that we get the jth
column of M′as defined in Equation (75). We will do so by induction.
Define y0=ejandy1=B0·y0. Then for every 1 ≤a < p , define
ya+1=Ba 
Pbp−a,bp−a+1,N⊤ya.
Note that we have
M′·ej=PR
b,N·yp. (93)
Next, we claim that for every 1 ≤a≤p, we have for every ( i0, . . . , i a−1)∈[0,p√
N)a−1,
ya[((i0, . . . , i a−2),(jp−1, . . . , j a), ia−1)] =a−1Y
b=0ℓ(b)
j,i
ωp√
N,b)
, (94)
where for a=p, we think of (( i0, . . . , i a−2),(jp−1, . . . , j a), ia−1) = (i0, . . . , i p−1).
We first note that Equation (94) is enough to prove the claim. Indeed we have that
yp[((i0, . . . , i p−1))] =p−1Y
b=0ℓ(b)
j,i
ωp√
N,b)
,
where the RHS in the above is the same as RHS in Equation (75). To see the claim note that
by definition of PR
b,N, we have
PR
b,N·yp
[(ip−1, . . . , i 0)]=yp[(i0, . . . , i p−1)]. Equation (93) then
established the claim.
To complete the proof we prove Equation (94) by induction on a.
We next consider the base case of a= 1. Note that y1is just the jth column of B0(asy0=ej)
and in that case Equation (94) follows from Equation (92).
For the inductive hypothesis, assume that Equation (94) is true for afor some a≥1. We now
want to argue Equation (94) for ya+1. Towards that end define
za= 
Pbp−a,bp−a+1,N⊤ya=Pb,bp−a+1,Nya.
Note that
ya+1=Ba·za. (95)
Now by definition of Pb,bp−a+1,N, we have
za[((i0, . . . , i a−1),(jp−1, . . . , j a))] =ya[((i0, . . . , i a−2),(jp−1, . . . , j a), ia−1)].
We claim that Equation (94) is true for a+ 1 from the above along with Equation (95) and Equa-
tion (92). Indeed, fix any (i0, . . . , i a−1). Then in Equation (95) the entry za[((i0, . . . , i a−1),(jp−1, . . . , j a))]
gets multiplied by the entries B((i0,...,ia−1),(jp−1,...,ja+1))
a [ia, ja]for all values of ia∈[0,p√
N). The
inductive hypothesis and Equation (92) then proves the inductive step, as desired.
65

--- PAGE 66 ---
Finally, we note that we can generalize Algorithm 4 for constructing Bafor 0≤a < p (since
this is the multivariate case some of the steps are a bit simplified):
Algorithm 6 Blocky MultiVar Monarch (eB(0), . . .eB(p−1), N, p )
Input: eB(0), . . . ,eB(p−1)∈RN×bwhere b=p√
N
Output: Block diagonal matrices B0, . . . ,Bp−1∈RN×N
▷Compose each output matrix from corresponding input matrix
1:fory←0 top−1do
2: fora←0 toN
b−1do
3: B(a)
y←Cb·eB(a)
(y)[ab:ab+b−1,:]
4: By←diag(B(0)
y, . . .B(N
b−1)
y )
5:return B 0, . . . ,Bp−1
D.6.5 Extensions and Open Questions
In this sub-section we outline certain (fairly) straightforward extension of our theoretical results
and conclude with some open questions.
Comparing Equation (91) to Equation (1) We note that we can post multiply Min
Equation (91) with a large class of permutations for which Theorem 17 still holds. We outline the
technical reason why this is true. At the heart of the argument for why Equation (86) gives a causal
map is Lemma 18. Specifically note that the sum in RHS in Equation (84), is over all j+j′⪯m.
The main observation is that this partial order still holds if we permute the b-variate representation
ofj,j′andmin the same way. In other words, for any permutation σ: [0, p)→[0, p) if we define
σ(j) = 
jσ(0), . . . , j σ(p−1)
and similarly σ(j′), σ(m). Then we still have σ(j) +σ(j′)⪯σ(m). This
in turn implies the following. Let Pσbe a permutation that maps j∈[0,p√
N)ptoσ(j)∈[0,p√
N)p.
Then Theorem 17 holds if we replace M′byM·PσwithMas in Equation (91).
Evaluation points Our results as presented are for specific classes of evaluation points. A
natural question to ask is if our results can be extended to more general set of evaluation points.
It turns out that our results for p-variate Monarch matrices can be extended to a wider class of
evaluation points. Specifically, for each 0 ≤a < p , letSa⊂Cwith|Sa|=p√
N. Then our results in
this sub-section hold if we replace the evaluation points from Apto×p−1
a=0Sa. The only thing that
changes in our proofs is that in Theorem 16, we replace mod
Tp√
N(X0), . . . , T p√
N(Xp−1)
by
mod 
qS0(X0), . . . , q Sp−1(Xp−1)
, where qA(Z) is as defined in Equation (81). This result can then
be propagated throughout the rest of our proofs.
On the other hand, our results in Appendix D.3 and Appendix D.5 do exploit specific properties of
the evaluation points (specifically 
ωi
N√
N=ωi0√
Nfor Appendix D.3 and T√
N(ωN,i)= (−1)i1ω√
N,i0
for Appendix D.5). To generalize these results to other sets of evaluation points, we need the
existence of degree√
Npolynomial that maps (in a√
N-to-1 fashion) Ato a set of√
Nelements.
Another interesting open question is to avoid the blowup n→2p·nin Theorem 17 and ideally only
pay a blowup n→2nfor every p≥2 as we were able to do in Appendix D.3 and Appendix D.5
(with p= 2).
66
