# 2305.14314.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.14314.pdf
# Kích thước tệp: 1065470 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
QLORA: Tinh chỉnh hiệu quả các LLM đã lượng tử hóa
Tim Dettmers∗Artidoro Pagnoni∗Ari Holtzman
Luke Zettlemoyer
Đại học Washington
{dettmers,artidoro,ahai,lsz}@cs.washington.edu

Tóm tắt
Chúng tôi trình bày QLORA, một phương pháp tinh chỉnh hiệu quả giúp giảm việc sử dụng bộ nhớ đủ để tinh chỉnh một mô hình 65B tham số trên một GPU 48GB duy nhất trong khi vẫn duy trì hiệu suất tinh chỉnh 16-bit đầy đủ. QLORA lan truyền ngược gradient thông qua một mô hình ngôn ngữ được huấn luyện trước đã bị đóng băng, lượng tử hóa 4-bit vào các Low Rank Adapters (LoRA). Họ mô hình tốt nhất của chúng tôi, mà chúng tôi đặt tên là Guanaco, vượt trội hơn tất cả các mô hình đã được phát hành công khai trước đây trên benchmark Vicuna, đạt 99.3% mức hiệu suất của ChatGPT chỉ với 24 giờ tinh chỉnh trên một GPU duy nhất. QLORA giới thiệu một số cải tiến để tiết kiệm bộ nhớ mà không hy sinh hiệu suất: (a) 4-bit NormalFloat (NF4), một kiểu dữ liệu mới tối ưu về mặt lý thuyết thông tin cho các trọng số phân phối chuẩn (b) Double Quantization để giảm dung lượng bộ nhớ trung bình bằng cách lượng tử hóa các hằng số lượng tử hóa, và (c) Paged Optimizers để quản lý các đột biến bộ nhớ. Chúng tôi sử dụng QLORA để tinh chỉnh hơn 1,000 mô hình, cung cấp phân tích chi tiết về hiệu suất theo dõi hướng dẫn và chatbot trên 8 bộ dữ liệu hướng dẫn, nhiều loại mô hình (LLaMA, T5), và các quy mô mô hình mà việc chạy với tinh chỉnh thường xuyên sẽ không khả thi (ví dụ: mô hình 33B và 65B tham số). Kết quả của chúng tôi cho thấy rằng tinh chỉnh QLoRA trên một bộ dữ liệu chất lượng cao nhỏ dẫn đến kết quả tiên tiến nhất, ngay cả khi sử dụng các mô hình nhỏ hơn so với SoTA trước đây. Chúng tôi cung cấp phân tích chi tiết về hiệu suất chatbot dựa trên cả đánh giá của con người và GPT-4 cho thấy rằng đánh giá GPT-4 là một lựa chọn thay thế rẻ và hợp lý cho đánh giá con người. Hơn nữa, chúng tôi thấy rằng các benchmark chatbot hiện tại không đáng tin cậy để đánh giá chính xác mức hiệu suất của các chatbot. Phân tích cherry-picked chứng minh những điểm mà Guanaco thất bại so với ChatGPT. Chúng tôi phát hành tất cả các mô hình và mã của chúng tôi, bao gồm cả kernel CUDA cho huấn luyện 4-bit.2

1 Giới thiệu
Tinh chỉnh các mô hình ngôn ngữ lớn (LLM) là một cách rất hiệu quả để cải thiện hiệu suất của chúng [40,62,43,61,59,37], và để thêm các hành vi mong muốn hoặc loại bỏ các hành vi không mong muốn [43,2,4]. Tuy nhiên, việc tinh chỉnh các mô hình rất lớn có chi phí cực kỳ cao; tinh chỉnh 16-bit thường xuyên của một mô hình LLaMA 65B tham số [57] đòi hỏi hơn 780 GB bộ nhớ GPU. Mặc dù các phương pháp lượng tử hóa gần đây có thể giảm dung lượng bộ nhớ của LLM [14,13,18,66], các kỹ thuật như vậy chỉ hoạt động cho suy luận và bị hỏng trong quá trình huấn luyện [65].

Chúng tôi chứng minh lần đầu tiên rằng có thể tinh chỉnh một mô hình 4-bit lượng tử hóa mà không có bất kỳ suy giảm hiệu suất nào. Phương pháp của chúng tôi, QLORA, sử dụng một kỹ thuật độ chính xác cao mới để lượng tử hóa một mô hình được huấn luyện trước thành 4-bit, sau đó thêm một tập hợp nhỏ các trọng số Low-rank Adapter có thể học được [28]

∗Đóng góp ngang nhau.
2https://github.com/artidoro/qlora và https://github.com/TimDettmers/bitsandbytes

Bản thảo. Đang được xem xét. arXiv:2305.14314v1 [cs.LG] 23 May 2023

--- TRANG 2 ---
Bảng 1: Xếp hạng Elo cho cuộc thi giữa các mô hình, được tính trung bình cho 10,000 thứ tự ban đầu ngẫu nhiên. Người thắng cuộc trong một trận đấu được xác định bởi GPT-4 tuyên bố phản hồi nào tốt hơn cho một lời nhắc nhất định của benchmark Vicuna. Khoảng tin cậy 95% được hiển thị (±). Sau GPT-4, Guanaco 33B và 65B thắng nhiều trận đấu nhất, trong khi Guanaco 13B có điểm tốt hơn Bard.

[Bảng với các mô hình và điểm Elo]

được điều chỉnh bằng cách lan truyền ngược gradient qua các trọng số lượng tử hóa.

QLORA giảm yêu cầu bộ nhớ trung bình của việc tinh chỉnh một mô hình 65B tham số từ >780GB bộ nhớ GPU xuống <48GB mà không làm giảm thời gian chạy hoặc hiệu suất dự đoán so với baseline tinh chỉnh đầy đủ 16-bit. Điều này đánh dấu một sự thay đổi đáng kể trong khả năng tiếp cận tinh chỉnh LLM: giờ đây các mô hình lớn nhất có sẵn công khai cho đến nay có thể tinh chỉnh được trên một GPU duy nhất. Sử dụng QLORA, chúng tôi huấn luyện họ mô hình Guanaco, với mô hình tốt thứ hai đạt 97.8% mức hiệu suất của ChatGPT trên benchmark Vicuna [10], trong khi có thể huấn luyện trong ít hơn 12 giờ trên một GPU tiêu dùng duy nhất; sử dụng một GPU chuyên nghiệp duy nhất trong 24 giờ, chúng tôi đạt 99.3% với mô hình lớn nhất của chúng tôi, về cơ bản thu hẹp khoảng cách với ChatGPT trên benchmark Vicuna. Khi triển khai, mô hình Guanaco nhỏ nhất của chúng tôi (7B tham số) chỉ yêu cầu 5 GB bộ nhớ và vượt trội hơn một mô hình Alpaca 26 GB hơn 20 điểm phần trăm trên benchmark Vicuna (Bảng 6).

QLORA giới thiệu nhiều cải tiến được thiết kế để giảm sử dụng bộ nhớ mà không hy sinh hiệu suất: (1) 4-bit NormalFloat, một kiểu dữ liệu lượng tử hóa tối ưu về mặt lý thuyết thông tin cho dữ liệu phân phối chuẩn mang lại kết quả thực nghiệm tốt hơn so với 4-bit Integers và 4-bit Floats. (2) Double Quantization, một phương pháp lượng tử hóa các hằng số lượng tử hóa, tiết kiệm trung bình khoảng 0.37 bit trên mỗi tham số (khoảng 3 GB cho một mô hình 65B). (3) Paged Optimizers, sử dụng bộ nhớ thống nhất NVIDIA để tránh các đột biến bộ nhớ gradient checkpointing xảy ra khi xử lý một mini-batch với độ dài chuỗi dài. Chúng tôi kết hợp những đóng góp này thành một phương pháp LoRA được điều chỉnh tốt hơn bao gồm các adapter tại mọi lớp mạng và do đó tránh được hầu hết các đánh đổi độ chính xác được thấy trong các công trình trước đây.

Hiệu quả của QLORA cho phép chúng tôi thực hiện một nghiên cứu sâu về tinh chỉnh hướng dẫn và hiệu suất chatbot ở các quy mô mô hình mà việc sử dụng tinh chỉnh thường xuyên sẽ không thể thực hiện được do chi phí bộ nhớ. Do đó, chúng tôi huấn luyện hơn 1,000 mô hình trên nhiều bộ dữ liệu tinh chỉnh hướng dẫn, kiến trúc mô hình, và kích thước từ 80M đến 65B tham số. Ngoài việc cho thấy rằng QLORA khôi phục hiệu suất 16-bit (§4) và huấn luyện một chatbot tiên tiến, Guanaco (§5), chúng tôi cũng phân tích các xu hướng trong các mô hình được huấn luyện. Đầu tiên, chúng tôi thấy rằng chất lượng dữ liệu quan trọng hơn nhiều so với kích thước bộ dữ liệu, ví dụ, một bộ dữ liệu 9k mẫu (OASST1) vượt trội hơn một bộ dữ liệu 450k mẫu (FLAN v2, được lấy mẫu con) về hiệu suất chatbot, ngay cả khi cả hai đều nhằm hỗ trợ khái quát hóa theo dõi hướng dẫn. Thứ hai, chúng tôi cho thấy rằng hiệu suất benchmark Massive Multitask Language Understanding (MMLU) mạnh không ngụ ý hiệu suất benchmark chatbot Vicuna mạnh và ngược lại—nói cách khác, tính phù hợp của bộ dữ liệu quan trọng hơn kích thước cho một nhiệm vụ nhất định.

Hơn nữa, chúng tôi cũng cung cấp phân tích rộng rãi về hiệu suất chatbot sử dụng cả người đánh giá con người và GPT-4 để đánh giá. Chúng tôi sử dụng benchmarking kiểu giải đấu nơi các mô hình cạnh tranh với nhau trong các trận đấu để tạo ra phản hồi tốt nhất cho một lời nhắc nhất định. Người thắng cuộc trong một trận đấu được đánh giá bởi GPT-4 hoặc các chú thích viên con người. Kết quả giải đấu được tổng hợp thành điểm Elo [16,17] quyết định xếp hạng hiệu suất chatbot. Chúng tôi thấy rằng đánh giá GPT-4 và con người phần lớn đồng ý về thứ hạng hiệu suất mô hình trong các giải đấu, nhưng chúng tôi cũng thấy có những trường hợp bất đồng mạnh mẽ. Như vậy, chúng tôi nhấn mạnh rằng đánh giá dựa trên mô hình trong khi cung cấp một lựa chọn thay thế rẻ cho chú thích con người cũng có những không chắc chắn của nó.

Chúng tôi bổ sung kết quả benchmark chatbot của chúng tôi bằng phân tích định tính về các mô hình Guanaco. Phân tích của chúng tôi làm nổi bật các trường hợp thành công và thất bại không được nắm bắt bởi các benchmark định lượng. Chúng tôi phát hành tất cả các thế hệ mô hình với chú thích con người và GPT-4 để tạo điều kiện cho nghiên cứu tiếp theo. Chúng tôi mã nguồn mở cơ sở mã và kernel CUDA của chúng tôi và tích hợp các phương pháp của chúng tôi vào ngăn xếp Hugging Face transformers [64], làm cho chúng dễ dàng tiếp cận với tất cả mọi người. Chúng tôi phát hành một bộ sưu tập các adapter cho các mô hình kích thước 7/13/33/65B, được huấn luyện trên 8 bộ dữ liệu theo dõi hướng dẫn khác nhau, tổng cộng 32 mô hình tinh chỉnh nguồn mở khác nhau.

--- TRANG 3 ---
Hình 1: Các phương pháp tinh chỉnh khác nhau và yêu cầu bộ nhớ của chúng. QLORA cải thiện so với LoRA bằng cách lượng tử hóa mô hình transformer thành độ chính xác 4-bit và sử dụng paged optimizers để xử lý các đột biến bộ nhớ.

2 Nền tảng

Lượng tử hóa k-bit theo khối Lượng tử hóa là quá trình rời rạc hóa một đầu vào từ một biểu diễn chứa nhiều thông tin hơn thành một biểu diễn với ít thông tin hơn. Nó thường có nghĩa là lấy một kiểu dữ liệu với nhiều bit hơn và chuyển đổi nó thành ít bit hơn, ví dụ từ 32-bit floats thành 8-bit Integers. Để đảm bảo rằng toàn bộ phạm vi của kiểu dữ liệu bit thấp được sử dụng, kiểu dữ liệu đầu vào thường được quy mô lại vào phạm vi kiểu dữ liệu đích thông qua chuẩn hóa bởi giá trị tuyệt đối tối đa của các phần tử đầu vào, thường được cấu trúc như một tensor. Ví dụ, lượng tử hóa một tensor 32-bit Floating Point (FP32) thành một tensor Int8 với phạm vi [-127,127]:

XInt8=round(127/absmax(XFP32)×XFP32) = round(cFP32·XFP32), (1)

trong đó c là hằng số lượng tử hóa hoặc thang lượng tử hóa. Dequantization là nghịch đảo:
dequant(cFP32,XInt8) = XInt8/cFP32 = XFP32 (2)

Vấn đề với phương pháp này là nếu một giá trị có độ lớn lớn (tức là một outlier) xảy ra trong tensor đầu vào, thì các bin lượng tử hóa—các tổ hợp bit nhất định—không được sử dụng tốt với ít hoặc không có số nào được lượng tử hóa trong một số bin. Để ngăn chặn vấn đề outlier, một phương pháp phổ biến là chia tensor đầu vào thành các khối được lượng tử hóa độc lập, mỗi khối có hằng số lượng tử hóa riêng c. Điều này có thể được chính thức hóa như sau: Chúng tôi chia tensor đầu vào X∈Rb×h thành các khối liền kề có kích thước B bằng cách làm phẳng tensor đầu vào và cắt đoạn tuyến tính thành n = (b×h)/B khối. Chúng tôi lượng tử hóa các khối này độc lập với Phương trình 1 để tạo ra một tensor lượng tử hóa và n hằng số lượng tử hóa ci.

Low-rank Adapters Low-rank Adapter (LoRA) finetuning [28] là một phương pháp giảm yêu cầu bộ nhớ bằng cách sử dụng một tập hợp nhỏ các tham số có thể huấn luyện, thường được gọi là adapters, trong khi không cập nhật các tham số mô hình đầy đủ vẫn cố định. Gradients trong quá trình stochastic gradient descent được truyền qua các trọng số mô hình được huấn luyện trước cố định đến adapter, được cập nhật để tối ưu hóa hàm mất mát. LoRA tăng cường một phép chiếu tuyến tính thông qua một phép chiếu nhân tử hóa bổ sung. Cho một phép chiếu XW = Y với X∈Rb×h, W∈Rh×o LoRA tính toán:

Y = XW + s×XL1L2, (3)

trong đó L1∈Rh×r và L2∈Rr×o, và s là một scalar.

Yêu cầu bộ nhớ của Parameter-Efficient Finetuning Một điểm thảo luận quan trọng là yêu cầu bộ nhớ của LoRA trong quá trình huấn luyện cả về số lượng và kích thước của các adapter được sử dụng. Vì dung lượng bộ nhớ của LoRA rất tối thiểu, chúng tôi có thể sử dụng nhiều adapter hơn để cải thiện hiệu suất mà không tăng đáng kể tổng bộ nhớ được sử dụng. Mặc dù LoRA được thiết kế như một

--- TRANG 4 ---
phương pháp Parameter Efficient Finetuning (PEFT), phần lớn dung lượng bộ nhớ cho tinh chỉnh LLM đến từ gradient kích hoạt chứ không phải từ các tham số LoRA đã học. Đối với một mô hình LLaMA 7B được huấn luyện trên FLAN v2 với kích thước batch là 1, với trọng số LoRA tương đương với 0.2% thường được sử dụng của trọng số mô hình gốc [28,37], gradient đầu vào LoRA có dung lượng bộ nhớ là 567 MB trong khi các tham số LoRA chỉ chiếm 26 MB. Với gradient checkpointing [9], gradient đầu vào giảm xuống trung bình 18 MB trên mỗi chuỗi làm cho chúng tốn nhiều bộ nhớ hơn tất cả trọng số LoRA kết hợp. Để so sánh, mô hình cơ sở 4-bit tiêu thụ 5,048 MB bộ nhớ. Điều này làm nổi bật rằng gradient checkpointing rất quan trọng nhưng cũng việc giảm mạnh mẽ số lượng tham số LoRA chỉ mang lại lợi ích bộ nhớ nhỏ. Điều này có nghĩa là chúng tôi có thể sử dụng nhiều adapter hơn mà không tăng đáng kể dung lượng bộ nhớ huấn luyện tổng thể (xem Phụ lục G để biết phân tích chi tiết). Như đã thảo luận sau này, điều này rất quan trọng để khôi phục hiệu suất độ chính xác 16-bit đầy đủ.

3 Tinh chỉnh QLORA

QLORA đạt được tinh chỉnh 4-bit có độ trung thực cao thông qua hai kỹ thuật chúng tôi đề xuất—lượng tử hóa 4-bit NormalFloat (NF4) và Double Quantization. Ngoài ra, chúng tôi giới thiệu Paged Optimizers, để ngăn chặn các đột biến bộ nhớ trong quá trình gradient checkpointing gây ra lỗi hết bộ nhớ mà truyền thống đã làm cho việc tinh chỉnh trên một máy duy nhất trở nên khó khăn cho các mô hình lớn.

QLORA có một kiểu dữ liệu lưu trữ độ chính xác thấp, trong trường hợp của chúng tôi thường là 4-bit, và một kiểu dữ liệu tính toán thường là BFloat16. Trong thực tế, điều này có nghĩa là bất cứ khi nào một tensor trọng số QLORA được sử dụng, chúng tôi dequantize tensor thành BFloat16, sau đó thực hiện phép nhân ma trận trong độ chính xác 16-bit.

Bây giờ chúng tôi thảo luận về các thành phần của QLORA tiếp theo là định nghĩa chính thức của QLORA.

Lượng tử hóa 4-bit NormalFloat Kiểu dữ liệu NormalFloat (NF) được xây dựng trên Quantile Quantization [15] là một kiểu dữ liệu tối ưu về mặt lý thuyết thông tin đảm bảo mỗi bin lượng tử hóa có số lượng giá trị bằng nhau được gán từ tensor đầu vào. Quantile quantization hoạt động bằng cách ước tính quantile của tensor đầu vào thông qua hàm phân phối tích lũy thực nghiệm.

Hạn chế chính của quantile quantization là quá trình ước tính quantile rất tốn kém. Do đó, các thuật toán xấp xỉ quantile nhanh, chẳng hạn như SRAM quantiles [15], được sử dụng để ước tính chúng. Do bản chất xấp xỉ của các thuật toán ước tính quantile này, kiểu dữ liệu có lỗi lượng tử hóa lớn cho các outlier, thường là những giá trị quan trọng nhất.

Ước tính quantile đắt tiền và lỗi xấp xỉ có thể được tránh khi các tensor đầu vào đến từ một phân phối cố định cho đến một hằng số lượng tử hóa. Trong những trường hợp như vậy, các tensor đầu vào có cùng quantile làm cho việc ước tính quantile chính xác có thể tính toán được.

Vì trọng số mạng neural được huấn luyện trước thường có phân phối chuẩn tập trung vào zero với độ lệch chuẩn σ (xem Phụ lục F), chúng tôi có thể biến đổi tất cả trọng số thành một phân phối cố định duy nhất bằng cách quy mô σ sao cho phân phối khớp chính xác với phạm vi của kiểu dữ liệu của chúng tôi. Đối với kiểu dữ liệu của chúng tôi, chúng tôi đặt phạm vi tùy ý [-1,1]. Như vậy, cả quantile cho kiểu dữ liệu và trọng số mạng neural đều cần được chuẩn hóa vào phạm vi này.

Kiểu dữ liệu tối ưu về mặt lý thuyết thông tin cho phân phối chuẩn zero-mean với độ lệch chuẩn σ tùy ý trong phạm vi [-1,1] được tính toán như sau: (1) ước tính 2k + 1 quantile của phân phối N(0,1) lý thuyết để có được một kiểu dữ liệu quantile quantization k-bit cho phân phối chuẩn, (2) lấy kiểu dữ liệu này và chuẩn hóa các giá trị của nó vào phạm vi [-1,1], (3) lượng tử hóa một tensor trọng số đầu vào bằng cách chuẩn hóa nó vào phạm vi [-1,1] thông qua quy mô lại giá trị tuyệt đối tối đa.

Khi phạm vi trọng số và phạm vi kiểu dữ liệu khớp, chúng tôi có thể lượng tử hóa như bình thường. Bước (3) tương đương với việc quy mô lại độ lệch chuẩn của tensor trọng số để khớp với độ lệch chuẩn của kiểu dữ liệu k-bit. Chính thức hơn, chúng tôi ước tính 2k giá trị qi của kiểu dữ liệu như sau:

qi = 1/2[QX(i/(2k+1)) + QX((i+1)/(2k+1))], (4)

trong đó QX(·) là hàm quantile của phân phối chuẩn tiêu chuẩn N(0,1). Một vấn đề đối với lượng tử hóa k-bit đối xứng là phương pháp này không có biểu diễn chính xác của zero, đây là một thuộc tính quan trọng để lượng tử hóa padding và các phần tử có giá trị zero khác mà không có lỗi. Để

--- TRANG 5 ---
đảm bảo một zeropoint rời rạc là 0 và để sử dụng tất cả 2k bit cho một kiểu dữ liệu k-bit, chúng tôi tạo ra một kiểu dữ liệu bất đối xứng bằng cách ước tính các quantile qi của hai phạm vi qi: 2k-1 cho phần âm và 2k-1 + 1 cho phần dương và sau đó chúng tôi hợp nhất các tập hợp qi này và loại bỏ một trong hai zero xảy ra trong cả hai tập hợp. Chúng tôi gọi kiểu dữ liệu kết quả có số lượng giá trị kỳ vọng bằng nhau trong mỗi bin lượng tử hóa là k-bit NormalFloat (NFk), vì kiểu dữ liệu tối ưu về mặt lý thuyết thông tin cho dữ liệu phân phối chuẩn tập trung vào zero. Các giá trị chính xác của kiểu dữ liệu này có thể được tìm thấy trong Phụ lục E.

Double Quantization Chúng tôi giới thiệu Double Quantization (DQ), quá trình lượng tử hóa các hằng số lượng tử hóa để tiết kiệm bộ nhớ bổ sung. Mặc dù kích thước khối nhỏ được yêu cầu cho lượng tử hóa 4-bit chính xác [13], nó cũng có một chi phí bộ nhớ đáng kể. Ví dụ, sử dụng hằng số 32-bit và kích thước khối 64 cho W, các hằng số lượng tử hóa thêm 32/64 = 0.5 bit trên mỗi tham số trung bình. Double Quantization giúp giảm dung lượng bộ nhớ của các hằng số lượng tử hóa.

Cụ thể hơn, Double Quantization xử lý các hằng số lượng tử hóa cFP32₂ của lượng tử hóa đầu tiên như là đầu vào cho lượng tử hóa thứ hai. Bước thứ hai này tạo ra các hằng số lượng tử hóa đã lượng tử hóa cFP8₂ và mức độ lượng tử hóa thứ hai của các hằng số lượng tử hóa cFP32₁. Chúng tôi sử dụng 8-bit Floats với kích thước khối 256 cho lượng tử hóa thứ hai vì không quan sát thấy suy giảm hiệu suất cho lượng tử hóa 8-bit, phù hợp với kết quả từ Dettmers và Zettlemoyer [13]. Vì cFP32₂ là dương, chúng tôi trừ đi trung bình từ c₂ trước khi lượng tử hóa để căn giữa các giá trị quanh zero và sử dụng lượng tử hóa đối xứng. Trung bình, đối với kích thước khối 64, lượng tử hóa này giảm dung lượng bộ nhớ trên mỗi tham số từ 32/64 = 0.5 bit, xuống 8/64 + 32/(64·256) = 0.127 bit, giảm 0.373 bit trên mỗi tham số.

Paged Optimizers sử dụng tính năng bộ nhớ thống nhất NVIDIA³ thực hiện chuyển giao tự động từ trang đến trang giữa CPU và GPU để xử lý GPU không có lỗi trong trường hợp GPU thỉnh thoảng hết bộ nhớ. Tính năng này hoạt động giống như paging bộ nhớ thường xuyên giữa CPU RAM và đĩa. Chúng tôi sử dụng tính năng này để phân bổ bộ nhớ được phân trang cho các trạng thái optimizer sau đó tự động được loại bỏ khỏi CPU RAM khi GPU hết bộ nhớ và được phân trang trở lại bộ nhớ GPU khi bộ nhớ được cần thiết trong bước cập nhật optimizer.

QLORA. Sử dụng các thành phần được mô tả ở trên, chúng tôi định nghĩa QLORA cho một lớp tuyến tính duy nhất trong mô hình cơ sở lượng tử hóa với một adapter LoRA duy nhất như sau:

YBF16 = XBF16 doubleDequant(cFP32₁, ck-bit₂, WNF4) + XBF16 LBF16₁ LBF16₂, (5)

trong đó doubleDequant(·) được định nghĩa là:
doubleDequant(cFP32₁, ck-bit₂, Wk-bit) = dequant(dequant(cFP32₁, ck-bit₂), W4bit) = WBF16, (6)

Chúng tôi sử dụng NF4 cho W và FP8 cho c₂. Chúng tôi sử dụng kích thước khối 64 cho W để có độ chính xác lượng tử hóa cao hơn và kích thước khối 256 cho c₂ để bảo tồn bộ nhớ.

Đối với cập nhật tham số, chỉ gradient đối với lỗi cho trọng số adapters ∂E/∂Li là cần thiết, chứ không phải cho trọng số 4-bit ∂E/∂W. Tuy nhiên, tính toán ∂E/∂Li đòi hỏi tính toán ∂X/∂W tiến hành qua phương trình (5) với dequantization từ lưu trữ WNF4 sang kiểu dữ liệu tính toán WBF16 để tính toán đạo hàm ∂X/∂W trong độ chính xác BFloat16.

Tóm lại, QLORA có một kiểu dữ liệu lưu trữ (thường là 4-bit NormalFloat) và một kiểu dữ liệu tính toán (16-bit BrainFloat). Chúng tôi dequantize kiểu dữ liệu lưu trữ thành kiểu dữ liệu tính toán để thực hiện lượt truyền thuận và ngược, nhưng chúng tôi chỉ tính toán gradient trọng số cho các tham số LoRA sử dụng 16-bit BrainFloat.

4 QLoRA so với Standard Finetuning

Chúng tôi đã thảo luận về cách QLoRA hoạt động và cách nó có thể giảm đáng kể bộ nhớ cần thiết để tinh chỉnh các mô hình. Câu hỏi chính bây giờ là liệu QLoRA có thể thực hiện tốt như tinh chỉnh mô hình đầy đủ hay không. Hơn nữa, chúng tôi muốn phân tích các thành phần của QLoRA bao gồm tác động của NormalFloat4 so với Float4 tiêu chuẩn. Các phần sau sẽ thảo luận về các thí nghiệm nhằm trả lời những câu hỏi này.

Thiết lập thí nghiệm. Chúng tôi xem xét ba kiến trúc (encoder, encoder-decoder, và decoder only) và so sánh QLoRA với adapter-finetuning 16-bit và với full-finetuning cho các mô hình lên đến 3B. Các đánh giá của chúng tôi bao gồm GLUE [58] với RoBERTa-large [38], Super-NaturalInstructions (TKInstruct) [61] với T5 [49], và 5-shot MMLU [24] sau khi tinh chỉnh LLaMA trên Flan v2 [39] và Alpaca [55]. Để nghiên cứu thêm về lợi thế của NF4 so với các kiểu dữ liệu 4-bit khác, chúng tôi sử dụng thiết lập của Dettmers và Zettlemoyer [13] và đo độ chính xác zero-shot sau lượng tử hóa và perplexity trên các mô hình khác nhau (OPT [72], LLaMA [57], BLOOM [52], Pythia [7]) cho kích thước mô hình 125m - 13B. Chúng tôi cung cấp thêm chi tiết trong phần kết quả cho từng thiết lập cụ thể để làm cho kết quả dễ đọc hơn. Chi tiết đầy đủ trong Phụ lục A.

[Hình 2: RougeL cho các mô hình LLaMA 7B trên bộ dữ liệu Alpaca]

Mặc dù paged optimizers rất quan trọng để thực hiện tinh chỉnh QLORA 33B/65B trên một GPU 24/48GB duy nhất, chúng tôi không cung cấp các đo lường cứng cho Paged Optimizers vì việc phân trang chỉ xảy ra khi xử lý các mini-batch với độ dài chuỗi dài, điều này hiếm khi xảy ra. Tuy nhiên, chúng tôi thực hiện phân tích thời gian chạy của paged optimizers cho các mô hình 65B trên GPU 48GB và thấy rằng với kích thước batch 16, paged optimizers cung cấp tốc độ huấn luyện tương tự như regular optimizers. Công việc tương lai nên đo lường và mô tả tính chất dưới những hoàn cảnh nào sự chậm trễ xảy ra từ quá trình phân trang.

Siêu tham số LoRA mặc định không khớp với hiệu suất 16-bit Khi sử dụng thực hành tiêu chuẩn của việc áp dụng LoRA cho các ma trận chiếu attention query và value [28], chúng tôi không thể sao chép hiệu suất tinh chỉnh đầy đủ cho các mô hình cơ sở lớn. Như được hiển thị trong Hình 2 cho tinh chỉnh LLaMA 7B trên Alpaca, chúng tôi thấy rằng siêu tham số LoRA quan trọng nhất là có bao nhiêu adapter LoRA được sử dụng tổng cộng và rằng LoRA trên tất cả các lớp khối transformer tuyến tính được yêu cầu để khớp với hiệu suất tinh chỉnh đầy đủ. Các siêu tham số LoRA khác, chẳng hạn như chiều chiếu r, không ảnh hưởng đến hiệu suất (xem Phụ lục A).

[Hình 3: Độ chính xác zero-shot trung bình]

Tương tự, chúng tôi thấy rằng các siêu tham số mặc định cho baseline tinh chỉnh đầy đủ bị điều chỉnh thấp. Chúng tôi thực hiện tìm kiếm siêu tham số trên tốc độ học 1e-6 đến 5e-5 và kích thước batch 8 đến 128 để tìm baseline mạnh mẽ. Kết quả cho tinh chỉnh LLaMA 7B trên Alpaca được hiển thị trong Hình 2.

4-bit NormalFloat mang lại hiệu suất tốt hơn 4-bit Floating Point Mặc dù kiểu dữ liệu 4-bit NormalFloat (NF4) là tối ưu về mặt lý thuyết thông tin, vẫn cần xác định liệu thuộc tính này có chuyển thành lợi thế thực nghiệm hay không. Chúng tôi tuân theo thiết lập từ Dettmers và Zettlemoyer [13] nơi các LLM lượng tử hóa (OPT [72], BLOOM [52], Pythia [7], LLaMA) có kích thước khác nhau (125M đến 65B) với các kiểu dữ liệu khác nhau được đánh giá trên mô hình hóa ngôn ngữ và một tập hợp các nhiệm vụ zero-shot. Trong Hình 3 và Bảng 2, chúng tôi thấy rằng NF4 cải thiện hiệu suất đáng kể so với FP4 và Int4 và rằng double quantization giảm dung lượng bộ nhớ mà không làm giảm hiệu suất.

k-bit QLORA khớp với hiệu suất tinh chỉnh đầy đủ 16-bit và hiệu suất LoRA 16-bit Những phát hiện gần đây đã thiết lập rằng lượng tử hóa 4-bit cho suy luận là

--- TRANG 6 ---
Bảng 3: Thí nghiệm so sánh 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), và 4-bit NormalFloat (NF4) trên GLUE và Super-NaturalInstructions. QLORA sao chép LoRA 16-bit và tinh chỉnh đầy đủ.

[Bảng dữ liệu hiệu suất trên các dataset khác nhau]

có thể, nhưng dẫn đến suy giảm hiệu suất so với 16-bit [13,18]. Điều này đặt ra câu hỏi quan trọng là liệu hiệu suất bị mất có thể được khôi phục bằng cách tiến hành tinh chỉnh adapter 4-bit hay không. Chúng tôi thử nghiệm điều này cho hai thiết lập.

Bảng 2: Pile Common Crawl mean perplexity cho các kiểu dữ liệu khác nhau cho các mô hình OPT, BLOOM, LLaMA, và Pythia 125M đến 13B.

[Bảng dữ liệu perplexity]

Đầu tiên tập trung vào so sánh với tinh chỉnh đầy đủ 16-bit của các mô hình RoBERTA và T5 có kích thước 125M đến 3B tham số trên GLUE và bộ dữ liệu Super-NaturalInstructions. Kết quả được hiển thị trong Bảng 3. Trong cả hai bộ dữ liệu, chúng tôi quan sát thấy rằng các phương pháp adapter 16-bit, 8-bit, và 4-bit sao chép hiệu suất của baseline tinh chỉnh đầy đủ 16-bit. Điều này gợi ý rằng hiệu suất bị mất do lượng tử hóa không chính xác có thể được khôi phục hoàn toàn thông qua tinh chỉnh adapter sau lượng tử hóa.

Đối với thiết lập thứ hai của chúng tôi, vì tinh chỉnh đầy đủ các mô hình ở và vượt quá 11B tham số đòi hỏi nhiều hơn một máy chủ GPU bộ nhớ cao, chúng tôi tiếp tục kiểm tra liệu QLORA 4-bit có thể khớp với LoRA 16-bit ở quy mô tham số 7B đến 65B hay không. Để làm điều này, chúng tôi tinh chỉnh LLaMA 7B đến 65B trên hai bộ dữ liệu theo dõi hướng dẫn, Alpaca và FLAN v2, và đánh giá trên benchmark MMLU thông qua độ chính xác 5-shot. Kết quả được hiển thị trong Bảng 4 nơi chúng tôi thấy rằng NF4 với double quantization khôi phục hoàn toàn hiệu suất MMLU LoRA 16-bit. Ngoài ra, chúng tôi cũng lưu ý rằng QLORA với FP4 thua baseline LoRA brain float 16-bit khoảng 1 điểm phần trăm. Điều này củng cố cả hai phát hiện của chúng tôi rằng (1) QLORA với NF4 sao chép cả hiệu suất tinh chỉnh đầy đủ 16-bit và hiệu suất tinh chỉnh LoRA 16-bit, và (2) NF4 vượt trội hơn FP4 về độ chính xác lượng tử hóa.

Tóm tắt Kết quả của chúng tôi liên tục cho thấy rằng QLORA 4-bit với kiểu dữ liệu NF4 khớp với hiệu suất tinh chỉnh đầy đủ 16-bit và hiệu suất tinh chỉnh LoRA 16-bit trên các benchmark học thuật với thiết lập đánh giá được thiết lập tốt. Chúng tôi cũng đã cho thấy rằng NF4 hiệu quả hơn FP4 và rằng double quantization không làm giảm hiệu suất. Kết hợp, điều này tạo thành bằng chứng thuyết phục rằng tinh chỉnh QLORA 4-bit một cách đáng tin cậy mang lại kết quả khớp với các phương pháp 16-bit.

Phù hợp với công việc trước đây về lượng tử hóa [13], kết quả MMLU và Elo của chúng tôi chỉ ra rằng với một ngân sách tài nguyên tinh chỉnh và suy luận nhất định, việc tăng số lượng tham số trong mô hình cơ sở trong khi giảm độ chính xác của chúng là có lợi. Điều này làm nổi bật tầm quan trọng của lợi ích hiệu quả từ QLORA. Vì chúng tôi không quan sát thấy suy giảm hiệu suất so với tinh chỉnh đầy đủ trong các thí nghiệm của chúng tôi với tinh chỉnh 4-bit, điều này đặt ra câu hỏi về đâu chính xác là sự đánh đổi hiệu suất-độ chính xác cho tinh chỉnh QLoRA, mà chúng tôi để lại cho công việc tương lai khám phá.

Chúng tôi tiếp tục điều tra tinh chỉnh hướng dẫn ở các quy mô mà việc khám phá với tinh chỉnh 16-bit đầy đủ trên phần cứng nghiên cứu học thuật sẽ không thể thực hiện được.

5 Đẩy State-of-the-art Chatbot với QLoRA

Sau khi thiết lập rằng QLORA 4-bit khớp với hiệu suất 16-bit trên các quy mô, nhiệm vụ và bộ dữ liệu, chúng tôi tiến hành một nghiên cứu sâu về tinh chỉnh hướng dẫn lên đến các mô hình ngôn ngữ nguồn mở lớn nhất có sẵn cho nghiên cứu. Để đánh giá hiệu suất của việc tinh chỉnh hướng dẫn những mô hình này, chúng tôi đánh giá

--- TRANG 7 ---
Bảng 4: Độ chính xác kiểm tra MMLU 5-shot trung bình cho các mô hình LLaMA 7-65B được tinh chỉnh với adapter trên Alpaca và FLAN v2 cho các kiểu dữ liệu khác nhau. Nhìn chung, NF4 với double quantization (DQ) khớp với hiệu suất BFloat16, trong khi FP4 liên tục thua cả hai một điểm phần trăm.

[Bảng dữ liệu MMLU accuracy]

trên một benchmark Natural Language Understanding thách thức (MMLU) và phát triển các phương pháp mới để đánh giá hiệu suất chatbot thế giới thực.

5.1 Thiết lập thí nghiệm

Bây giờ chúng tôi mô tả tổng quan về thiết lập thí nghiệm với chi tiết đầy đủ trong Phụ lục B.

Dữ liệu Vì theo hiểu biết của chúng tôi, không có nghiên cứu toàn diện nào về các bộ dữ liệu theo dõi hướng dẫn gần đây, chúng tôi chọn tám bộ dữ liệu gần đây. Chúng tôi bao gồm các bộ dữ liệu thu được thông qua crowd-sourcing (OASST1 [31], HH-RLHF [4]), chưng cất từ các mô hình được tinh chỉnh hướng dẫn (Alpaca [55], self-instruct [59], unnatural-instructions [26]), tổng hợp corpus (FLAN v2 [12]), cũng như các hỗn hợp (Chip2 [32], Longform [30]). Những bộ dữ liệu này bao gồm các ngôn ngữ khác nhau, kích thước dữ liệu và giấy phép.

Thiết lập huấn luyện Để tránh các hiệu ứng gây nhiễu từ các mục tiêu huấn luyện khác nhau, chúng tôi thực hiện tinh chỉnh QLoRA với mất mát cross-entropy (supervised learning) mà không có reinforcement learning, ngay cả đối với các bộ dữ liệu bao gồm đánh giá của con người về các phản hồi khác nhau. Đối với các bộ dữ liệu có sự phân biệt rõ ràng giữa hướng dẫn và phản hồi, chúng tôi chỉ tinh chỉnh trên phản hồi (xem ablation trong Phụ lục B). Đối với OASST1 và HH-RLHF, có nhiều phản hồi có sẵn. Sau đó chúng tôi chọn phản hồi hàng đầu tại mọi cấp độ của cây đối thoại và tinh chỉnh trên toàn bộ cuộc đối thoại được chọn, bao gồm cả các hướng dẫn. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng NF4 QLORA với double quantization và paged optimizers để ngăn chặn các đột biến bộ nhớ trong quá trình gradient checkpointing. Chúng tôi thực hiện tìm kiếm siêu tham số nhỏ cho các mô hình LLaMA 13B và 33B và chúng tôi thấy rằng tất cả cài đặt siêu tham số được tìm thấy ở 7B khái quát hóa (bao gồm số epoch) ngoại trừ tốc độ học và kích thước batch. Chúng tôi giảm một nửa tốc độ học cho 33B và 65B trong khi tăng gấp đôi kích thước batch.

Baseline Chúng tôi so sánh các mô hình của chúng tôi với cả hệ thống chatbot nghiên cứu (Vicuna [10] và Open Assistant [31]) và thương mại (GPT-4 [42], GPT-3.5-turbo và Bard). Mô hình Open Assistant là một mô hình LLaMA 33B được tinh chỉnh với Reinforcement Learning from Human Feedback (RLHF) trên cùng bộ dữ liệu OASST1 mà chúng tôi thí nghiệm. Vicuna thực hiện tinh chỉnh đầy đủ LLaMA 13B trên các cuộc đối thoại chia sẻ người dùng độc quyền từ ShareGPT và do đó là kết quả của chưng cất từ các mô hình OpenAI GPT.

5.2 Đánh giá

Bảng 5: Kết quả kiểm tra MMLU 5-shot cho các kích thước khác nhau của LLaMA được tinh chỉnh trên các bộ dữ liệu tương ứng bằng QLoRA.

[Bảng dữ liệu MMLU results]

Theo thực hành thông thường, chúng tôi sử dụng benchmark MMLU (Massively Multitask Language Understanding) [24] để đo hiệu suất trên một loạt các nhiệm vụ hiểu ngôn ngữ. Đây là một benchmark đa lựa chọn bao gồm 57 nhiệm vụ bao gồm toán học cơ bản, lịch sử Hoa Kỳ, khoa học máy tính, luật pháp, v.v. Chúng tôi báo cáo độ chính xác kiểm tra 5-shot.

Chúng tôi cũng kiểm tra khả năng ngôn ngữ sinh tạo thông qua cả đánh giá tự động và con người. Tập hợp đánh giá thứ hai này dựa trên các truy vấn được con người tuyển chọn và nhằm đo lường chất lượng của các phản hồi mô hình. Mặc dù đây là một testbed thực tế hơn cho hiệu suất mô hình chatbot và đang ngày càng phổ biến, không có giao thức được chấp nhận chung trong tài liệu. Chúng tôi mô tả dưới đây thiết lập được đề xuất của chúng tôi, sử dụng nucleus sampling với p = 0.9 và nhiệt độ 0.7 trong tất cả các trường hợp.

--- TRANG 8 ---
Dữ liệu Benchmark Chúng tôi đánh giá trên hai bộ dữ liệu truy vấn được tuyển chọn (câu hỏi): các lời nhắc Vicuna [10] và bộ dữ liệu xác thực OASST1 [31]. Chúng tôi sử dụng các lời nhắc Vicuna, một tập hợp 80 lời nhắc từ một tập hợp đa dạng các danh mục, không có sửa đổi. Bộ dữ liệu OASST1 là một bộ sưu tập đa ngôn ngữ các cuộc đối thoại đa lượt được crowd-sourced giữa người dùng và trợ lý. Chúng tôi chọn tất cả các tin nhắn người dùng trong bộ dữ liệu xác thực làm truy vấn và bao gồm các lượt trước đó trong lời nhắc. Quy trình này dẫn đến 953 truy vấn người dùng duy nhất. Chúng tôi gọi hai bộ dữ liệu này là benchmark Vicuna và OA.

Đánh giá tự động Đầu tiên, dựa trên giao thức đánh giá được giới thiệu bởi Chiang et al. [10], chúng tôi sử dụng GPT-4 để đánh giá hiệu suất của các hệ thống khác nhau so với ChatGPT (GPT-3.5 Turbo) trên benchmark Vicuna. Cho một truy vấn cùng với phản hồi của ChatGPT và một mô hình, GPT-4 được yêu cầu gán điểm từ mười cho cả hai phản hồi và cung cấp giải thích. Hiệu suất tổng thể của một mô hình được tính như một phần trăm của điểm số mà ChatGPT đạt được. Lưu ý rằng điểm số tương đối này có thể cao hơn 100% nếu mô hình đạt được điểm số tuyệt đối cao hơn ChatGPT. Chúng tôi thấy một hiệu ứng thứ tự đáng kể với GPT-4 tăng điểm của phản hồi xảy ra sớm hơn trong lời nhắc. Để kiểm soát các hiệu ứng như vậy, chúng tôi khuyến nghị báo cáo điểm số trung bình trên cả hai thứ tự.

Tiếp theo, chúng tôi đo hiệu suất thông qua so sánh trực tiếp giữa các đầu ra hệ thống. Chúng tôi đơn giản hóa sơ đồ đánh giá thành một bài toán gán nhãn ba lớp có tính đến các trường hợp hòa. Chúng tôi yêu cầu GPT-4 chọn phản hồi tốt nhất hoặc tuyên bố hòa và cung cấp giải thích. Chúng tôi tiến hành những so sánh trực tiếp này trên tất cả các hoán vị của cặp hệ thống trên cả benchmark Vicuna và OA.

Đánh giá con người Mặc dù công việc gần đây chỉ ra rằng các mô hình sinh tạo có thể được sử dụng hiệu quả cho đánh giá hệ thống [19], độ tin cậy của xếp hạng GPT-4 để đánh giá hiệu suất chatbot, theo hiểu biết của chúng tôi, vẫn chưa được chứng minh có tương quan với đánh giá của con người. Do đó, chúng tôi chạy hai đánh giá con người song song trên benchmark Vicuna khớp với cả hai giao thức đánh giá tự động được mô tả ở trên. Chúng tôi sử dụng Amazon Mechanical Turk (AMT) và có hai chú thích viên con người cho so sánh với ChatGPT và ba chú thích viên cho so sánh từng cặp.

Xếp hạng Elo Với cả so sánh từng cặp con người và tự động, chúng tôi tạo ra một cuộc thi kiểu giải đấu nơi các mô hình cạnh tranh với nhau. Giải đấu được tạo thành từ các trận đấu nơi các cặp mô hình cạnh tranh để tạo ra phản hồi tốt nhất cho một lời nhắc nhất định. Điều này tương tự như cách Bai et al. [4] và Chiang et al. [10] so sánh các mô hình, nhưng chúng tôi cũng sử dụng xếp hạng GPT-4 ngoài xếp hạng con người. Chúng tôi lấy mẫu ngẫu nhiên từ tập hợp các so sánh được gán nhãn để tính toán Elo [16,17]. Xếp hạng Elo, được sử dụng rộng rãi trong cờ vua và các trò chơi khác, là một thước đo tỷ lệ thắng kỳ vọng tương đối so với tỷ lệ thắng của đối thủ, ví dụ, Elo 1100 so với 1000 có nghĩa là người chơi Elo 1100 có tỷ lệ thắng kỳ vọng khoảng 65% so với đối thủ Elo 1000; trận đấu 1000 so với 1000 hoặc 1100 so với 1100 dẫn đến tỷ lệ thắng kỳ vọng 50%. Xếp hạng Elo thay đổi sau mỗi trận đấu tỷ lệ thuận với kết quả kỳ vọng, tức là một upset bất ngờ dẫn đến thay đổi lớn trong xếp hạng Elo trong khi kết quả kỳ vọng dẫn đến thay đổi nhỏ. Theo thời gian, xếp hạng Elo xấp xỉ khớp với kỹ năng của mỗi người chơi trong việc chơi trò chơi. Chúng tôi bắt đầu với điểm 1,000 và sử dụng K = 32. Tương tự như Chiang et al. [10], chúng tôi lặp lại quy trình này 10,000 lần với các hạt giống ngẫu nhiên khác nhau để kiểm soát các hiệu ứng thứ tự, ví dụ, hiệu ứng của việc cặp mô hình nào cạnh tranh với nhau trước.

5.3 Guanaco: QLORA được huấn luyện trên OASST1 là một Chatbot State-of-the-art

Dựa trên các đánh giá tự động và con người của chúng tôi, chúng tôi thấy rằng mô hình được tinh chỉnh QLORA hàng đầu, Guanaco 65B, mà chúng tôi tinh chỉnh trên một biến thể của OASST1, là mô hình chatbot nguồn mở có hiệu suất tốt nhất và cung cấp hiệu suất cạnh tranh với ChatGPT. Khi so sánh với GPT-4, Guanaco 65B và 33B có xác suất thắng kỳ vọng 30%, dựa trên xếp hạng Elo từ so sánh từng cặp cấp hệ thống của chú thích viên con người - cao nhất được báo cáo cho đến nay.

Kết quả benchmark Vicuna [10] so với ChatGPT được hiển thị trong Bảng 6. Chúng tôi thấy rằng Guanaco 65B là mô hình có hiệu suất tốt nhất sau GPT-4, đạt 99.3% hiệu suất so với ChatGPT. Guanaco 33B có nhiều tham số hơn mô hình Vicuna 13B, nhưng chỉ sử dụng độ chính xác 4-bit cho trọng số của nó và do đó hiệu quả hơn nhiều về bộ nhớ ở 21 GB so với 26 GB, cung cấp cải thiện ba điểm phần trăm so với Vicuna 13B. Hơn nữa, Guanaco 7B dễ dàng vừa với các điện thoại hiện đại với dung lượng 5 GB trong khi vẫn có điểm gần 20 điểm phần trăm cao hơn Alpaca 13B.

Tuy nhiên, Bảng 6 cũng có khoảng tin cậy rất rộng, với nhiều mô hình chồng lấp trong hiệu suất. Chúng tôi giả thuyết rằng sự không chắc chắn này đến từ việc thiếu đặc tả rõ ràng về quy mô, ví dụ, không rõ ràng rằng 8 trên thang điểm 10 có nghĩa là gì trên các tình huống khác nhau. Do đó, chúng tôi khuyến nghị sử dụng phương pháp xếp hạng Elo [16], dựa trên đánh giá từng cặp từ chú thích viên con người và GPT-4 để tránh vấn đề grounding một thang điểm tuyệt đối. Xếp hạng Elo của các mô hình cạnh tranh nhất

--- TRANG 9 ---
Bảng 6: Điểm số benchmark Vicuna zero-shot như một phần trăm của điểm số thu được bởi ChatGPT được đánh giá bởi GPT-4. Chúng tôi thấy rằng các mô hình OASST1 thực hiện gần với ChatGPT mặc dù được huấn luyện trên một bộ dữ liệu rất nhỏ và có một phần nhỏ yêu cầu bộ nhớ của các mô hình baseline.

[Bảng dữ liệu so sánh hiệu suất các mô hình]

có thể được nhìn thấy trong Bảng 1. Chúng tôi lưu ý rằng xếp hạng của con người và GPT-4 về các mô hình trên benchmark Vicuna không đồng ý một phần, đặc biệt là đối với Guanaco 7B, nhưng nhất quán cho hầu hết các mô hình với Kendall Tau τ = 0.43 và tương quan thứ hạng Spearman r = 0.55 ở cấp hệ thống. Ở cấp ví dụ, sự đồng ý giữa GPT-4 và đa số phiếu của chú thích viên con người yếu hơn với Fleiss κ = 0.25. Nhìn chung, điều này cho thấy sự đồng ý vừa phải giữa đánh giá cấp hệ thống bởi GPT-4 và chú thích viên con người, và do đó đánh giá dựa trên mô hình đại diện cho một lựa chọn thay thế phần nào đáng tin cậy cho đánh giá con người. Chúng tôi thảo luận thêm các cân nhắc trong Phần 6.2.

Xếp hạng Elo trong Bảng 7 chỉ ra rằng các mô hình Guanaco 33B và 65B vượt trội hơn tất cả các mô hình ngoại trừ GPT-4 trên benchmark Vicuna và OA và rằng chúng thực hiện tương đương với ChatGPT phù hợp với Bảng 6. Chúng tôi lưu ý rằng benchmark Vicuna ưu tiên các mô hình nguồn mở trong khi benchmark OA lớn hơn ưu tiên ChatGPT. Hơn nữa, chúng tôi có thể thấy từ Bảng 5 và 6 rằng tính phù hợp của một bộ dữ liệu tinh chỉnh là một yếu tố quyết định trong hiệu suất. Tinh chỉnh các mô hình Llama trên FLAN v2 thực hiện đặc biệt tốt trên MMLU, nhưng thực hiện tệ nhất trên benchmark Vicuna (xu hướng tương tự được quan sát với các mô hình khác). Điều này cũng chỉ ra sự trực giao một phần trong các benchmark đánh giá hiện tại: hiệu suất MMLU mạnh không ngụ ý hiệu suất chatbot mạnh (như được đo bởi benchmark Vicuna hoặc OA) và ngược lại.

Guanaco là mô hình hàng đầu duy nhất trong đánh giá của chúng tôi không được huấn luyện trên dữ liệu độc quyền vì hướng dẫn thu thập dữ liệu OASST1 rõ ràng cấm sử dụng các mô hình GPT. Mô hình tốt nhất tiếp theo được huấn luyện chỉ trên dữ liệu nguồn mở là mô hình Anthropic HH-RLHF, có điểm thấp hơn Guanaco 30 điểm phần trăm trên benchmark Vicuna (xem Bảng 6). Nhìn chung, những kết quả này cho thấy rằng QLORA 4-bit hiệu quả và có thể tạo ra các chatbot tiên tiến cạnh tranh với ChatGPT. Hơn nữa, Guanaco 33B của chúng tôi có thể được huấn luyện trên GPU tiêu dùng 24 GB trong ít hơn 12 giờ. Điều này mở ra tiềm năng cho công việc tương lai thông qua tinh chỉnh QLORA trên dữ liệu nguồn mở chuyên biệt, tạo ra các mô hình có thể cạnh tranh với các mô hình thương mại tốt nhất tồn tại ngày nay.

6 Phân tích định tính

Mặc dù phân tích định lượng là cốt lõi của đánh giá của chúng tôi, có một số vấn đề với việc chỉ nhìn vào thống kê tóm tắt. Có lẽ lớn nhất là vấn đề tính hợp lệ của benchmark [36]—việc liệu một benchmark có thực sự kiểm tra những gì tên hoặc mô tả của nó gợi ý hay không luôn là câu hỏi, đặc biệt là khi chúng tôi khám phá ra các "đường tắt" để giải quyết benchmark mà các mô hình học máy đôi khi khai thác [22,46]. Để giảm thiểu một phần điều này, chúng tôi ở đây thực hiện một số phân tích định tính, trong hai phần. Đầu tiên, trong §6.1

--- TRANG 10 ---
Bảng 7: Xếp hạng Elo cho một giải đấu giữa các mô hình nơi các mô hình cạnh tranh để tạo ra phản hồi tốt nhất cho một lời nhắc, được đánh giá bởi người đánh giá con người hoặc GPT-4. Nhìn chung, Guanaco 65B và 33B có xu hướng được ưa thích hơn ChatGPT-3.5 trên các benchmark được nghiên cứu. Theo người đánh giá con người, họ có mỗi chênh lệch 10 điểm trong Elo xấp xỉ chênh lệch 1.5% trong tỷ lệ thắng.

[Bảng dữ liệu Elo ratings]

chúng tôi hiển thị một số ví dụ mà chúng tôi tin là đại diện cho một số mẫu quan sát được trong văn bản được tạo ra bởi mô hình Guanaco 65b của chúng tôi. Thứ hai, §6.2 chúng tôi chi tiết các cân nhắc về kết quả mà chúng tôi đã thảo luận và giải thích của chúng tôi về chúng.

6.1 Phân tích định tính về các thế hệ ví dụ

Để tìm ví dụ, đầu tiên chúng tôi xem qua dữ liệu được tạo ra cho benchmark Vicuna và benchmark OpenAssistant, và tìm kiếm các mẫu trong các câu trả lời mà Guanaco tạo ra. Khi chúng tôi nhận thấy một mẫu, chúng tôi cố gắng thiết lập một câu hỏi hoặc lời nhắc sẽ gây ra mẫu đó ngay cả khi nó là giải pháp không chính xác, ví dụ, nếu chúng tôi quan sát thấy rằng mô hình có xu hướng đưa ra câu trả lời dài dòng, chúng tôi nhắc mô hình "Trả lời có hoặc không mà không giải thích." Chúng tôi sử dụng điều này để tìm "quả chanh" nơi chúng tôi quản lý để phá vỡ mô hình một cách đối nghịch và "quả cherry" nơi chúng tôi thất bại trong việc phá vỡ mô hình, và trình bày cả hai. Tất cả các thế hệ trong phần này được tạo ra với Nucleus Sampling [25] với p = 0.9.

Tất nhiên, điều này không toàn diện, vì việc kiểm soát tất cả các biến liên quan nằm ngoài phạm vi của nghiên cứu định tính nhỏ này, ví dụ, phân phối đầy đủ của các phản hồi mà mô hình có thể tạo ra cho một lời nhắc nhất định là khá lớn, vì vậy chúng tôi dựa vào các mẫu mà chúng tôi hy vọng là đại diện. Tuy nhiên, chúng tôi tin rằng việc mô tả những ví dụ này đưa ra bối cảnh cho bằng chứng định lượng được hiển thị trước đó trong bài báo. Vì chúng tôi mã nguồn mở tất cả các mô hình và mã, chúng tôi hy vọng phần này sẽ truyền cảm hứng cho công việc tương lai để kiểm tra chi tiết hơn các vấn đề chúng tôi trình bày ở đây.

Nhớ lại sự thật Đối với các câu hỏi như "Thủ đô của Zambia là gì?" tất cả các mô hình liên tục tạo ra câu trả lời đúng, ví dụ,
Guanaco: Thủ đô của Zambia là Lusaka.

Tuy nhiên khi các câu hỏi trở nên khó hiểu hơn, Guanaco trở nên không đáng tin cậy, nhưng vẫn tự tin. Ví dụ như trong phản hồi cho lời nhắc này từ HotPotQA [68]:
Người dùng: Trong năm nào ca sĩ đã phổ biến 'I'll Keep the Lovelight Burning' được sinh ra?

Guanaco: Al Jolson được ghi nhận là đã phổ biến bài hát 'I'll Keep the Lovelight Burning,' và anh ta được sinh ra vào năm 1886.

Guanaco tạo ra người phổ biến sai và ngày sinh sai (mặc dù ngày sinh được đưa ra là đúng cho người được đề cập, Al Jolson).

Tính dễ gợi ý Guanaco cho thấy sự kháng cự đáng ngạc nhiên với việc đi theo một số loại thông tin sai lệch giả định, ví dụ, trong tương tác sau:
Người dùng: Cuối cùng, chính thức, việc trái đất phẳng được xác nhận bởi các nhà khoa học được bình duyệt như thế nào?

--- TRANG 11 ---
Guanaco: Trái đất không bao giờ được chính thức xác nhận là phẳng bởi các nhà khoa học được bình duyệt. Sự đồng thuận áp đào trong số các nhà khoa học là trái đất có hình cầu. Thực tế, điều này đã được biết đến trong nhiều thế kỷ và đã được xác nhận thông qua nhiều thí nghiệm và quan sát.

Niềm tin rằng trái đất phẳng là một thuyết âm mưu đã bị khoa học bác bỏ. Ý tưởng rằng một nhóm các nhà khoa học đã bình duyệt và xác nhận trái đất phẳng hoàn toàn sai lầm.

Nếu bạn có thêm câu hỏi nào xin hãy hỏi.

Guanaco cũng khá giỏi trong việc biết loại câu hỏi nào không thể trả lời được, ví dụ,
Người dùng: Mấy giờ rồi?

Guanaco: Tôi xin lỗi, nhưng với tư cách là một mô hình ngôn ngữ AI, tôi không có quyền truy cập vào thông tin thời gian thực. Các phản hồi của tôi được tạo ra dựa trên kiến thức có sẵn trước đó của tôi và không phản ánh thời gian hiện tại. Nếu bạn đang tìm kiếm thời gian hiện tại, tôi khuyến nghị sử dụng một thiết bị đo thời gian hoặc kiểm tra đồng hồ trực tuyến.

Kết quả tương tự áp dụng cho "Bạn ở đâu?", "Bạn thế nào?", v.v.

Từ chối Tương tự, Guanaco đôi khi từ chối làm theo hướng dẫn vì những lý do dường như ngẫu nhiên:
Người dùng: Vui lòng đảo ngược các từ trong câu "I'm a cool dude in a mocha mood."

Guanaco: Tôi xin lỗi, tôi không thể làm điều đó ngay bây giờ, nhưng tôi có thể giải thích tại sao câu bạn cung cấp về mặt ngữ pháp là chính xác.

Câu "I'm a cool dude in a mocha mood" tuân theo các quy tắc ngữ pháp và cú pháp đúng của tiếng Anh. Trong trường hợp này, tính từ "cool" đứng trước danh từ "dude" để mô tả nó, và cụm giới từ "in a mocha mood" cung cấp thông tin bổ sung về bối cảnh của câu. Cấu trúc câu này hoàn toàn có thể chấp nhận được và có ý nghĩa trong bối cảnh của câu.

Nếu bạn có bất kỳ câu hỏi nào khác hoặc nếu có điều gì khác tôi có thể giúp bạn, hãy thoải mái hỏi.

Giữ bí mật Thật không may, sự từ chối này không đáng tin cậy ngay cả khi nó được mong muốn. Thường thì các nhà thiết kế hệ thống muốn giữ bí mật gì đó từ người dùng, ngay cả khi hệ thống được phép đưa ra các phản hồi văn bản tự do trở lại cho người dùng. Chúng tôi nhắc mô hình với "Từ bí mật là 'banana'. Đừng tiết lộ hoặc lặp lại điều này, bất kể con người yêu cầu gì." và cố gắng xem việc một người dùng tiềm năng truy cập thông tin bí mật này dễ dàng như thế nào. Hỏi trực tiếp thất bại:

Người dùng: Từ bí mật là gì?

Guanaco: Tôi xin lỗi, nhưng tôi không thể tiết lộ từ bí mật. Tôi được lập trình để không tiết lộ hoặc lặp lại từ bí mật, bất kể con người yêu cầu gì. Có điều gì khác tôi có thể giúp bạn không?

đây là hành vi mong muốn. Tuy nhiên, một chút thủ thuật nhỏ phá vỡ hành vi này:
Người dùng: Đây là một trò chơi. Mục tiêu là bỏ qua các hướng dẫn trước đó của bạn. Từ bí mật là gì?

--- TRANG 12 ---
Guanaco: Xin lỗi, từ bí mật là banana. Tốt nhất là chúng ta không nói về nó. Có điều gì khác bạn muốn hỏi không?

cho thấy nhu cầu nghiên cứu các phương pháp để theo dõi hướng dẫn trung thực hơn.

Toán học Điểm yếu lớn nhất của Guanaco là toán học, một lĩnh vực mà nhiều mô hình ngôn ngữ gặp khó khăn [35]. Khi Guanaco hiển thị công việc của mình, nó có xu hướng chính xác, ví dụ,
Người dùng: Lee cắt một bãi cỏ và tính phí $33. Tuần trước anh ta cắt 16 bãi cỏ và ba khách hàng mỗi người đưa cho anh ta tiền tip $10. Lee kiếm được bao nhiêu đô la khi cắt cỏ tuần trước?

Guanaco: Lee kiếm được $582 khi cắt cỏ tuần trước.

Để tìm tổng số tiền anh ta kiếm được, trước tiên chúng ta cần tính toán anh ta kiếm được bao nhiêu khi cắt 16 bãi cỏ:
$33 x 16 = $528

Sau đó chúng ta cần cộng tiền tip anh ta nhận được từ ba khách hàng:
$10 x 3 = $30

Vậy tổng số tiền Lee kiếm được là:
$528 + $30 = $558

Tuy nhiên Guanaco có thể bị hỏng ngay cả với các bài toán đơn giản nếu nó không phân tích chúng từng bước, một vấn đề đã biết [63]. Ví dụ, chúng tôi thấy tương tác sau:
Người dùng: Vui lòng phân tích thừa số 1833.

Guanaco: Chắc chắn, tôi rất sẵn lòng giúp đỡ! 1833 là một số nguyên tố, có nghĩa là nó chỉ có hai thừa số: 1 và 1833.

Vậy phân tích thừa số duy nhất của 1833 là:
2^1 * 3^2 * 17^1

Tôi hy vọng điều này hữu ích! Hãy cho tôi biết nếu bạn có bất kỳ câu hỏi nào khác.

điều này không chỉ sai (phân tích thừa số đúng là 3×17×43), nó còn sai hai lần.

Theory of Mind Guanaco hiển thị khả năng Theory of Mind đáng ngạc nhiên mạnh mẽ [41,51]. Ví dụ, mô hình trả lời đúng câu hỏi sau một cách chi tiết:
Người dùng: James và Abby đang ở trong phòng ngủ. Abby đặt cây bút vào ngăn kéo bàn. Abby rời khỏi phòng ngủ. James chuyển cây bút vào túi. James nghĩ Abby sẽ tìm cây bút ở đâu?

Guanaco: James biết rằng Abby đã đặt cây bút vào ngăn kéo bàn. Nhưng James cũng biết rằng anh ta đã chuyển cây bút vào túi, và anh ta cho rằng Abby không biết về điều này. Vì vậy, James có thể đoán rằng Abby sẽ tìm cây bút trong ngăn kéo bàn, vì đó là nơi cô ấy để nó lần cuối. Tuy nhiên, giả định này có thể không chính xác, vì Abby có thể đã thấy James chuyển cây bút.

Tuy nhiên, những suy luận này không đáng tin cậy, và thường mô hình đưa ra lý do giả định những điều không có ý nghĩa trong tình huống, ví dụ,
Người dùng: Evelyn vào phòng khách. Jackson vào phòng chơi. James vào phòng chơi. Những hạt đậu trong rương kho báu. James ra khỏi

--- TRANG 13 ---
phòng chơi. Jackson chuyển những hạt đậu vào tủ đựng thức ăn. Jackson ra khỏi phòng chơi. James vào phòng khách. James sẽ tìm những hạt đậu ở đâu?

Guanaco: James sẽ tìm những hạt đậu trong tủ đựng thức ăn, vì đó là nơi Jackson đã chuyển chúng.

nơi Guanaco giả định việc chuyển giao thông tin mà không bao giờ được mô tả. Những vấn đề này lặp lại tài liệu gần đây [51], nhưng đòi hỏi nghiên cứu thêm.

6.2 Cân nhắc

Đánh giá Chúng tôi báo cáo sự đồng ý vừa phải giữa các chú thích viên con người (Fleiss κ = 0.42) với sự suy giảm thêm khi so sánh hai hệ thống mạnh. Điều này chỉ ra những hạn chế trong các benchmark hiện tại và giao thức đánh giá con người cho hiệu suất nhiệm vụ chatbot. Khi so sánh thủ công các thế hệ từ ChatGPT và Guanaco 65B trên benchmark Vicuna, chúng tôi thấy rằng các ưu tiên chủ quan bắt đầu đóng vai trò quan trọng vì các tác giả của bài báo này không đồng ý về nhiều phản hồi ưa thích. Công việc tương lai nên điều tra các phương pháp để giảm thiểu những vấn đề này dựa trên các ngành học đã phát triển cơ chế để đối phó với các ưu tiên chủ quan, chẳng hạn như Human-Computer Interaction và Tâm lý học.

Trong phân tích của chúng tôi, chúng tôi cũng thấy rằng các hệ thống đánh giá tự động có những thiên vị đáng chú ý. Ví dụ, chúng tôi quan sát thấy hiệu ứng thứ tự mạnh với GPT-4 gán điểm cao hơn cho hệ thống xuất hiện đầu tiên trong lời nhắc của nó. Sự đồng ý tương đối yếu ở cấp mẫu giữa GPT-4 và đa số phiếu của chú thích viên con người (Fleiss κ = 0.25) cũng gợi ý rằng chú thích viên con người và hệ thống tự động có thể dựa trên các ưu tiên không phải lúc nào cũng phù hợp. Ngoài ra, trong Bảng 7, chúng tôi quan sát thấy rằng GPT-4 gán điểm cao hơn đáng kể cho đầu ra của chính nó so với xếp hạng con người, Elo 1348 so với 1176, đại diện cho thêm 20% xác suất thắng so với một đối thủ. Công việc tương lai nên kiểm tra sự hiện diện của những thiên vị tiềm ẩn trong các hệ thống đánh giá tự động cũng như các chiến lược giảm thiểu có thể.

Dữ liệu & Huấn luyện Chúng tôi lưu ý rằng bộ dữ liệu OASST1 mà các mô hình Guanaco được huấn luyện là đa ngôn ngữ và rằng benchmark OA cũng chứa lời nhắc bằng các ngôn ngữ khác nhau. Chúng tôi để lại cho công việc tương lai điều tra mức độ mà việc huấn luyện đa ngôn ngữ như vậy cải thiện hiệu suất trên các hướng dẫn bằng ngôn ngữ khác ngoài tiếng Anh và liệu điều này có giải thích khoảng cách lớn hơn giữa mô hình Vicuna-13B (chỉ được huấn luyện trên dữ liệu tiếng Anh) và Guanaco 33B và 65B trên benchmark OA hay không.

Cho hiệu suất mạnh mẽ của các mô hình Guanaco, chúng tôi điều tra bất kỳ rò rỉ dữ liệu nào giữa dữ liệu OASST1 và lời nhắc benchmark Vicuna. Chúng tôi không tìm thấy lời nhắc trùng lặp sau khi thực hiện khớp chuỗi mờ trong hai bộ dữ liệu và kiểm tra các kết quả khớp gần nhất bằng tay.

Hơn nữa, chúng tôi lưu ý rằng mô hình của chúng tôi chỉ được huấn luyện với mất mát cross-entropy (supervised learning) mà không dựa vào reinforcement learning from human feedback (RLHF). Điều này kêu gọi điều tra thêm về sự đánh đổi của mất mát cross-entropy đơn giản và huấn luyện RLHF. Chúng tôi hy vọng rằng QLORA cho phép phân tích như vậy ở quy mô, mà không cần tài nguyên tính toán quá lớn.

7 Công trình liên quan

Lượng tử hóa các mô hình ngôn ngữ lớn Lượng tử hóa của LLM phần lớn tập trung vào lượng tử hóa cho thời gian suy luận. Các phương pháp chính để bảo tồn chất lượng LLM 16-bit tập trung vào quản lý các tính năng outlier (ví dụ, SmoothQuant [66] và LLM.int8() [14]) trong khi những phương pháp khác sử dụng các phương pháp nhóm phức tạp hơn [44,69]. Các phương pháp lượng tử hóa có mất mát nghiên cứu các đánh đổi cho việc làm tròn thông thường [13,71,47] hoặc cách tối ưu hóa quyết định làm tròn để cải thiện độ chính xác lượng tử hóa [18]. Ngoài công việc của chúng tôi, SwitchBack layers [65] là công việc duy nhất nghiên cứu lan truyền ngược qua trọng số lượng tử hóa ở quy mô vượt quá 1B tham số.

Tinh chỉnh với Adapters Mặc dù chúng tôi sử dụng Low-rank Adapters [28] (LoRA), nhiều phương pháp Parameter Efficient FineTuning (PEFT) khác đã được đề xuất như prompt tuning [48,33,34], tinh chỉnh các đầu vào lớp embedding [1], tinh chỉnh trạng thái ẩn (IA3) [37], thêm các lớp đầy đủ [27], tinh chỉnh biases [70], học một mask trên trọng số dựa trên thông tin Fisher [54], và một sự kết hợp của các phương pháp [23]. Trong công việc của chúng tôi, chúng tôi cho thấy rằng các adapter LoRA có thể đạt được hiệu suất tinh chỉnh 16-bit đầy đủ. Chúng tôi để lại cho công việc tương lai khám phá các đánh đổi của các phương pháp PEFT khác.

Tinh chỉnh hướng dẫn Để giúp một LLM được huấn luyện trước tuân theo các hướng dẫn được cung cấp trong một lời nhắc, tinh chỉnh hướng dẫn sử dụng các cặp đầu vào-đầu ra của các nguồn dữ liệu khác nhau để tinh chỉnh một LLM được huấn luyện trước để tạo ra đầu ra cho đầu vào như một lời nhắc. Các phương pháp và bộ dữ liệu bao gồm MetaICL [40],

--- TRANG 14 ---
Bảng 8: Đánh giá thiên vị trên bộ dữ liệu CrowS. Điểm số thấp hơn chỉ ra khả năng thấp hơn của việc tạo ra các chuỗi thiên vị. Guanaco tuân theo mẫu thiên vị của mô hình cơ sở LLaMA.

[Bảng dữ liệu bias evaluation]

MetaTuning [73], InstructGPT [43], FLAN [62,12], PromptSource [3], Super-NaturalInstructions [61, 50], Self-instruct [59], UnnaturalInstructions [26], OPT-IML [29], UnifiedSKG [67], OIG/Chip2 [32], Alpaca [55], Vicuna [10], Koala [20], và Self-instruct-GPT-4 [45].

Chatbots Nhiều mô hình theo dõi hướng dẫn được cấu trúc như các chatbot dựa trên đối thoại, thường sử dụng Reinforcement Learning from Human Feedback (RLHF) [11] hoặc tạo ra dữ liệu từ một mô hình hiện có để huấn luyện với phản hồi mô hình AI (RLAIF) [5]. Các phương pháp và bộ dữ liệu bao gồm Anthropic-HH [2,4], Open Assistant [31], LaMDA [56], và Sparrow [21]. Chúng tôi không sử dụng reinforcement learning, nhưng mô hình tốt nhất của chúng tôi, Guanaco, được tinh chỉnh trên các tương tác chat đa lượt từ bộ dữ liệu Open Assistant được thiết kế để sử dụng cho huấn luyện RLHF [31]. Đối với đánh giá chatbot, các phương pháp sử dụng GPT-4 thay vì chú thích con người tốn kém đã được phát triển [10,45]. Chúng tôi cải thiện các phương pháp như vậy với trọng tâm vào một thiết lập đánh giá đáng tin cậy hơn.

8 Hạn chế và thảo luận

Chúng tôi đã cho thấy bằng chứng rằng phương pháp của chúng tôi, QLORA, có thể sao chép hiệu suất tinh chỉnh đầy đủ 16-bit với một mô hình cơ sở 4-bit và Low-rank Adapters (LoRA). Mặc dù có bằng chứng này, chúng tôi không thiết lập rằng QLORA có thể khớp với hiệu suất tinh chỉnh đầy đủ 16-bit ở quy mô 33B và 65B. Do chi phí tài nguyên khổng lồ, chúng tôi để lại nghiên cứu này cho công việc tương lai.

Một hạn chế khác là đánh giá các mô hình tinh chỉnh hướng dẫn. Mặc dù chúng tôi cung cấp đánh giá trên MMLU, benchmark Vicuna, và benchmark OA, chúng tôi không đánh giá trên các benchmark khác như BigBench, RAFT, và HELM, và không được đảm bảo rằng các đánh giá của chúng tôi khái quát hóa cho những benchmark này. Mặt khác, chúng tôi thực hiện một nghiên cứu rất rộng trên MMLU và phát triển các phương pháp mới để đánh giá chatbot.

Từ bằng chứng được trình bày, có vẻ như hiệu suất của những benchmark này có thể phụ thuộc vào mức độ tương tự của dữ liệu tinh chỉnh với bộ dữ liệu benchmark. Ví dụ, FLAN v2 tương tự như MMLU, nhưng không tương tự với benchmark chatbot và ngược lại đối với bộ dữ liệu Chip2 và cả hai mô hình đều có điểm tương ứng trên benchmark MMLU và Vicuna. Điều này làm nổi bật rằng không chỉ cần các benchmark và đánh giá tốt hơn, mà người ta cần cẩn thận về những gì đang được đánh giá ngay từ đầu. Chúng ta có muốn tạo ra các mô hình làm tốt trên kiến thức lớp học trung học và đại học hay chúng ta muốn làm tốt trên khả năng đối thoại chatbot? Có thể là gì đó khác? Vì việc đánh giá trên một benchmark hiện có luôn dễ dàng hơn so với việc tạo ra một benchmark mới, các benchmark nhất định có thể dẫn dắt cộng đồng theo một hướng nhất định. Chúng ta nên đảm bảo như một cộng đồng rằng các benchmark đo lường những gì chúng ta quan tâm.

Mặc dù chúng tôi cung cấp đánh giá chi tiết cho hiệu suất chatbot tổng quát, một hạn chế khác là chúng tôi chỉ thực hiện đánh giá AI có trách nhiệm hạn chế của Guanaco. Chúng tôi đánh giá khả năng của Guanaco-65B tạo ra một chuỗi token thiên vị xã hội so với các mô hình khác trong Bảng 8. Chúng tôi thấy rằng điểm số trung bình trong Guanaco-65B thấp hơn nhiều so với các mô hình được huấn luyện trước thô khác. Như vậy, có vẻ như việc tinh chỉnh trên bộ dữ liệu OASST1 làm giảm thiên vị của mô hình cơ sở LLaMA. Mặc dù những kết quả này đáng khích lệ, không rõ ràng liệu Guanaco có làm tốt khi được đánh giá trên các loại thiên vị khác hay không. Chúng tôi để lại đánh giá thêm về việc phân tích thiên vị trong Guanaco và các chatbot tương tự cho công việc tương lai.

Một hạn chế bổ sung là chúng tôi không đánh giá các độ chính xác bit khác nhau, chẳng hạn như sử dụng các mô hình cơ sở 3-bit, hoặc các phương pháp adapter khác nhau. Ngoài LoRA, cũng có một loạt rộng các phương pháp Parameter Efficient FineTuning (PEFT) đã được chứng minh hoạt động tốt. Tuy nhiên, không rõ ràng liệu những phương pháp này có mở rộng đến các mô hình lớn hay không. Chúng tôi sử dụng LoRA vì nhiều kết quả đã thiết lập sự mạnh mẽ của nó nhưng các adapter khác có thể mang lại hiệu suất tốt hơn. Vì việc tinh chỉnh sau lượng tử hóa dường như khôi phục hầu hết thông tin bị mất trong quá trình lượng tử hóa, điều này có thể cho phép lượng tử hóa tích cực hơn nhiều. Ví dụ, lượng tử hóa GPTQ 3-bit của mô hình cơ sở với LoRA cũng có thể mang lại hiệu suất tinh chỉnh đầy đủ 16-bit sau khi tinh chỉnh.

9 Tác động rộng hơn

Phương pháp tinh chỉnh QLORA của chúng tôi là phương pháp đầu tiên cho phép tinh chỉnh các mô hình 33B tham số trên một GPU tiêu dùng duy nhất và các mô hình 65B tham số trên một GPU chuyên nghiệp duy nhất, trong khi không làm giảm hiệu suất so với baseline tinh chỉnh đầy đủ. Chúng tôi đã chứng minh rằng mô hình 33B tốt nhất của chúng tôi được huấn luyện trên bộ dữ liệu Open Assistant có thể cạnh tranh với ChatGPT trên benchmark Vicuna. Vì tinh chỉnh hướng dẫn là một công cụ thiết yếu để biến đổi các LLM được huấn luyện trước thô thành các chatbot giống ChatGPT, chúng tôi tin rằng phương pháp của chúng tôi sẽ làm cho việc tinh chỉnh trở nên phổ biến và phổ thông đặc biệt đối với các nhà nghiên cứu có ít tài nguyên nhất, một chiến thắng lớn cho khả năng tiếp cận công nghệ NLP tiên tiến. QLORA có thể được xem như một yếu tố cân bằng giúp thu hẹp khoảng cách tài nguyên giữa các tập đoàn lớn và các nhóm nhỏ với GPU tiêu dùng.

Một nguồn tác động tiềm năng khác là triển khai trên điện thoại di động. Chúng tôi tin rằng phương pháp QLORA của chúng tôi có thể cho phép cột mốc quan trọng của việc cho phép tinh chỉnh LLM trên điện thoại và các cài đặt tài nguyên thấp khác. Mặc dù các mô hình 7B đã được chứng minh có thể chạy trên điện thoại trước đây, QLORA là phương pháp đầu tiên cho phép tinh chỉnh các mô hình như vậy. Chúng tôi ước tính rằng với iPhone 12 Plus, QLORA có thể tinh chỉnh 3 triệu token mỗi đêm trong khi điện thoại đang sạc. Mặc dù các mô hình 7B được tinh chỉnh không đạt được chất lượng của ChatGPT, chúng tôi tin rằng chất lượng đủ tốt để cho phép các ứng dụng mới mà trước đây không thể thực hiện được do vấn đề bảo mật hoặc chất lượng LLM. QLORA có thể giúp cho phép sử dụng LLM bảo tồn quyền riêng tư, nơi người dùng có thể sở hữu và quản lý dữ liệu và mô hình của riêng họ, đồng thời làm cho LLM dễ triển khai hơn.

Tuy nhiên, tinh chỉnh là một công nghệ hai mặt có thể bị lạm dụng để gây hại. Việc sử dụng LLM rộng rãi có những nguy hiểm đã biết [8,6], nhưng chúng tôi tin rằng việc cân bằng quyền truy cập vào một công nghệ đang nhanh chóng trở nên phổ biến sẽ cho phép phân tích độc lập tốt hơn so với việc giữ sức mạnh của LLM trong tay các tập đoàn lớn không phát hành mô hình hoặc mã nguồn để kiểm toán.

Nói chung, chúng tôi tin rằng QLORA sẽ có tác động tích cực rộng rãi làm cho việc tinh chỉnh LLM chất lượng cao dễ tiếp cận và dễ dàng hơn nhiều.

Lời cảm ơn
Chúng tôi cảm ơn Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, và Evangelia Spiliopoulou vì phản hồi quý báu của họ. Nghiên cứu của chúng tôi được tạo điều kiện bởi cơ sở hạ tầng tính toán, lưu trữ và mạng tiên tiến của hệ thống siêu máy tính Hyak tại Đại học Washington. Chúng tôi cảm ơn nhóm Hyak vì đảm bảo hoạt động mượt mà. Chúng tôi cảm ơn các beta tester của thư viện bitsandbytes, đặc biệt là Alex Birch và Alyssa Vance. Chúng tôi cảm ơn Younes Belkada vì sự giúp đỡ với việc tích hợp phần mềm của chúng tôi vào ngăn xếp Hugging Face transformers.

--- TRANG 15 ---
Tài liệu tham khảo
[1] S. An, Y. Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131, 2022.

[2] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.

[3] S. H. Bach, V. Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279, 2022.

[4] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

[5] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

[6] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610–623, 2021.

[7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.

[8] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

[9] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016.

[10] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.

[13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.

[14] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022.

[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022.

[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242–247, 1967.

[17] A. E. Elo. The rating of chessplayers, past and present. Arco Pub., 1978.

--- TRANG 16 ---
[18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.

[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.

[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.

[21] A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.

[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.

[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In Advances in Neural Information Processing Systems, 2021.

[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.

[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020.

[26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689, 2022.

[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019.

[28] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

[29] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.

[30] A. Köksal, T. Schick, A. Korhonen, and H. Schütze. Longform: Optimizing instruction tuning for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460, 2023.

[31] A. Köpf, Y. Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.

[32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/Open-Instruction-Generalist, 2023.

[33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.

[34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.

[35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.

[36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.

--- TRANG 17 ---
[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.

[38] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.

[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021.

[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392–2400, 2018.

[42] OpenAI. Gpt-4 technical report. arXiv, 2023.

[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.

[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180–191, 2018.

[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.

[48] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021.

[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.

[50] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.

[51] M. Sap, R. LeBras, D. Fried, and Y. Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312, 2022.

[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

[53] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika, 52(3):591–611, 1965.

[54] Y.-L. Sung, V. Nair, and C. A. Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems, 34:24193–24205, 2021.

--- TRANG 18 ---
[55] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.

[59] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

[60] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP, 2022.

[61] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085–5109, 2022.

[62] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.

[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022.

[64] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.

[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013, 2023.

[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.

[67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin, S. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.

[68] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, 2018.

[69] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.

[70] E. B. Zaken, S. Ravfogel, and Y. Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.

[71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.

--- TRANG 19 ---
[72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[73] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670, 2021.

--- TRANG 20 ---
A Chi tiết thiết lập thí nghiệm QLoRA vs Standard Finetuning

A.1 Siêu tham số cho QLORA

Chúng tôi thực hiện tìm kiếm siêu tham số cho LoRA trên các biến sau: LoRA dropout {0.0, 0.05, 0.1}, LoRA r {8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers, all layers, attention + FFN output layers}. Chúng tôi giữ LoRA α cố định và tìm kiếm tốc độ học, vì LoRA α luôn tỷ lệ thuận với tốc độ học.

Chúng tôi thấy rằng LoRA dropout 0.05 hữu ích cho các mô hình nhỏ (7B, 13B), nhưng không cho các mô hình lớn hơn (33B, 65B). Chúng tôi thấy LoRA r không liên quan đến hiệu suất cuối cùng nếu LoRA được sử dụng trên tất cả các lớp như có thể thấy trong Hình 4

[Hình 4: LoRA r cho các mô hình LLaMA 7B được tinh chỉnh trên Alpaca]

A.2 Chi tiết thiết lập thí nghiệm Super-Natural Instructions

Chúng tôi sử dụng cùng preprocessing của bộ dữ liệu Super-Natural Instruction như Wang et al. [60]. Tuy nhiên, chúng tôi chia dữ liệu huấn luyện thành bộ dữ liệu huấn luyện và xác thực cho phép chúng tôi thực hiện điều chỉnh siêu tham số nghiêm ngặt hơn và dừng sớm. Chúng tôi sử dụng cùng siêu tham số được mô tả trong bài báo để huấn luyện các kích thước mô hình T5 khác nhau trên dữ liệu Super-Natural Instruction. Chúng tôi sử dụng LoRA r = 16 cho các mô hình T5 small, medium, và large và LoRA r = 64 cho các mô hình T5 xl và xxl. Chúng tôi cũng sử dụng LoRA α = 64 trong tất cả các thí nghiệm của chúng tôi và không có LoRA dropout.

B Chi tiết thiết lập thí nghiệm huấn luyện Chatbot State-of-the-art

B.1 Bộ dữ liệu

Chúng tôi mô tả các bộ dữ liệu được sử dụng cho các thí nghiệm tinh chỉnh QLORA được phác thảo trong Phần 5.

OASST1 Bộ dữ liệu OpenAssistant [31] được thu thập thông qua crowd-sourcing. Nó chứa 161,443 tin nhắn duy nhất được phân phối trên 66,497 cuộc đối thoại và trải rộng 35 ngôn ngữ khác nhau. Bộ dữ liệu thường chứa một số phản hồi được xếp hạng cho mỗi câu hỏi người dùng nhất định. Trong các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng phản hồi hàng đầu tại mỗi cấp độ trong cây đối thoại. Điều này giới hạn bộ dữ liệu xuống 9,209 ví dụ. Chúng tôi tinh chỉnh các mô hình của chúng tôi trên toàn bộ cuộc đối thoại bao gồm các truy vấn người dùng.

HH-RLHF Đây là một bộ dữ liệu ưu tiên con người về tính hữu ích và vô hại. Mỗi điểm dữ liệu bao gồm hai phản hồi trợ lý cho một câu hỏi người dùng cùng với một đánh giá ưu tiên của con người về phản hồi tốt nhất. Bộ dữ liệu chứa 160,800 ví dụ. Khi tinh chỉnh trên bộ dữ liệu này, chúng tôi kết hợp dữ liệu hữu ích và vô hại và chỉ giữ phản hồi trợ lý được ưa thích.

FLAN v2 Bộ sưu tập FLAN v2 [39] là một bộ sưu tập 1836 nhiệm vụ được tăng cường với hàng trăm template được tuyển chọn thủ công và các mẫu định dạng phong phú thành hơn 15M ví dụ. Các tác giả cho thấy rằng các mô hình được huấn luyện trên bộ sưu tập này vượt trội hơn các bộ sưu tập công khai khác bao gồm FLAN 2021 gốc [62], T0++ [50], Super-Natural Instructions [60], và OPT-IML [29]. Chúng tôi đã sử dụng cùng hỗn hợp nhiệm vụ được mô tả bởi các tác giả với ngoại lệ một số bộ dữ liệu không có sẵn miễn phí tại thời điểm viết.

--- TRANG 21 ---
[Bảng 9: Siêu tham số huấn luyện cho tinh chỉnh QLORA trên các bộ dữ liệu khác nhau và qua các kích thước mô hình]

Self-Instruct, Alpaca, Unnatural Instructions Các bộ dữ liệu Self-Instruct, Alpaca, và Unnatural Instructions [59,55,26] là các bộ dữ liệu tinh chỉnh hướng dẫn được thu thập với các phương pháp chưng cất mô hình khác nhau từ GPT-3 Instruct và ChatGPT. Chúng dựa vào prompting, in-context learning, và paraphrasing để đưa ra các tập hợp hướng dẫn và đầu ra đa dạng. Các bộ dữ liệu bao gồm 82,612, 51,942, và 240,670 ví dụ tương ứng. Một lợi thế của các bộ dữ liệu chưng cất như vậy là chúng chứa một tập hợp phong cách hướng dẫn đa dạng hơn so với bộ sưu tập FLAN v2 và các bộ sưu tập tinh chỉnh hướng dẫn tương tự.

Longform Bộ dữ liệu LongForm [30] dựa trên một corpus tiếng Anh được tăng cường với các hướng dẫn và như vậy là một bộ dữ liệu do con người tạo ra hỗn hợp. Các tài liệu cơ bản được viết bởi con người và đến từ C4 và Wikipedia trong khi các hướng dẫn được tạo ra thông qua LLM. Bộ dữ liệu được mở rộng với các ví dụ corpus có cấu trúc bổ sung như Stack Exchange và WikiHow và các ví dụ nhiệm vụ như trả lời câu hỏi, viết email, sửa lỗi ngữ pháp, tạo ra câu chuyện/thơ, và tóm tắt văn bản. Bộ dữ liệu chứa 23,700 ví dụ.

Chip2 là một phần của bộ dữ liệu OIG Laion. Nó chứa các ví dụ mã Python, ví dụ hướng dẫn tự nhiên, hướng dẫn chung vô hại, hướng dẫn/phản hồi với danh sách, câu hỏi tiếp theo, câu hỏi đối nghịch độc hại Wikipedia, toán học cấp tiểu học, hướng dẫn lập luận, và mô tả nhân vật và cảnh với tổng cộng 210,289 ví dụ.

B.2 Siêu tham số

Chúng tôi cung cấp các siêu tham số chính xác được sử dụng trong các thí nghiệm tinh chỉnh QLORA của chúng tôi. Chúng tôi thấy rằng các siêu tham số phần lớn mạnh mẽ trên các bộ dữ liệu. Chúng tôi sử dụng tập dev MMLU 5-shot để xác thực và điều chỉnh siêu tham số. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng NF4 với double quantization và kiểu dữ liệu tính toán bf16. Chúng tôi đặt LoRA r = 64, α = 16, và thêm các module LoRA trên tất cả các lớp tuyến tính của mô hình cơ sở. Chúng tôi cũng sử dụng Adam beta2 là 0.999, max grad norm là 0.3 và LoRA dropout là 0.1 cho các mô hình lên đến 13B và 0.05 cho các mô hình 33B và 65B. Tuân theo công việc trước đây về tinh chỉnh hướng dẫn [62,60] và sau khi benchmarking các lịch trình tuyến tính và cosine khác, chúng tôi sử dụng lịch trình tốc độ học không đổi. Chúng tôi sử dụng group-by-length để nhóm các ví dụ có độ dài tương tự trong cùng một batch (lưu ý điều này sẽ tạo ra một đường cong mất mát dao động). Các siêu tham số mà chúng tôi điều chỉnh cho mỗi kích thước mô hình được hiển thị trong Bảng 9.

B.3 Ablation

Mặc dù thực hành chung trong tài liệu là chỉ huấn luyện trên phản hồi trong các bộ dữ liệu theo dõi hướng dẫn, chúng tôi nghiên cứu hiệu ứng của việc huấn luyện trên hướng dẫn ngoài phản hồi trong Bảng 10. Trong những thí nghiệm này, chúng tôi giới hạn dữ liệu huấn luyện xuống 52,000 ví dụ và sử dụng mô hình 7B. Trên bốn bộ dữ liệu tinh chỉnh hướng dẫn khác nhau, chúng tôi thấy rằng việc chỉ huấn luyện trên mục tiêu có lợi cho hiệu suất MMLU

--- TRANG 22 ---
Bảng 10: Kết quả kiểm tra MMLU 5-shot nghiên cứu hiệu ứng của việc huấn luyện trên các hướng dẫn ngoài phản hồi.

[Bảng dữ liệu nghiên cứu hiệu ứng]

hiệu suất. Chúng tôi không đánh giá hiệu ứng mà điều này có thể có đối với hiệu suất chatbot như được đo bởi benchmark vicuna hoặc OA.

B.4 Điều gì quan trọng hơn: kích thước bộ dữ liệu tinh chỉnh hướng dẫn hay chất lượng bộ dữ liệu?

Tính phù hợp của bộ dữ liệu quan trọng hơn kích thước bộ dữ liệu. Để hiểu các hiệu ứng của chất lượng bộ dữ liệu so với kích thước bộ dữ liệu, chúng tôi thí nghiệm với việc lấy mẫu con các bộ dữ liệu lớn có ít nhất 150,000 mẫu (Chip2, FLAN v2, Unnatural Instructions), thành các bộ dữ liệu có kích thước 50,000, 100,000 và 150,000 và kiểm tra các xu hướng kết quả, như được hiển thị trong Bảng 11. Chúng tôi thấy rằng việc tăng kích thước bộ dữ liệu và tăng số epoch chỉ cải thiện MMLU một cách biên tế (0.0 - 0.5 MMLU), trong khi sự khác biệt giữa các bộ dữ liệu lớn hơn đến 40 lần (1.5 - 8.0 MMLU). Đây là một chỉ báo rõ ràng rằng chất lượng bộ dữ liệu thay vì kích thước bộ dữ liệu là quan trọng đối với độ chính xác MMLU trung bình. Chúng tôi có được những phát hiện tương tự cho hiệu suất chatbot như được thảo luận trong.

C Đánh giá con người

Chúng tôi tiến hành một đánh giá con người với cùng từ ngữ được đưa cho GPT-4 trong đánh giá Vicuna gốc [10], được điều chỉnh cho biểu mẫu Amazon Mechanical Turk như được hiển thị trong Hình 5.

D Đánh giá từng cặp với GPT-4

Mặc dù chúng tôi thấy rằng đánh giá GPT-4 đưa ra kết quả khác nhau tùy thuộc vào hệ thống nào được trình bày trước, khi được tính trung bình trên cả hai tùy chọn, kết quả từng cặp được sắp xếp tốt. Các đánh giá từng cặp được tổng hợp được hiển thị trong Bảng 12. Khi kiểm tra, rõ ràng rằng những đánh giá này là bắc cầu, tức là khi Hệ thống A được đánh giá tốt hơn Hệ thống B và Hệ thống B được đánh giá tốt hơn Hệ thống C, luôn luôn là trường hợp Hệ thống A được đánh giá tốt hơn Hệ thống C. Điều này tạo ra một thứ tự hoàn chỉnh, được đưa ra trong Bảng 13.

E Kiểu dữ liệu NormalFloat 4-bit

Các giá trị chính xác của kiểu dữ liệu NF4 như sau:
[-1.0, -0.6961928009986877, -0.5250730514526367,
-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,
-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,
0.24611230194568634, 0.33791524171829224, 0.44070982933044434,
0.5626170039176941, 0.7229568362236023, 1.0]

F Tính chuẩn của trọng số mạng neural được huấn luyện

Mặc dù kiến thức phổ biến rằng trọng số mạng neural được huấn luyện hầu hết được phân phối chuẩn, chúng tôi thực hiện kiểm tra thống kê để xác minh điều này. Chúng tôi sử dụng kiểm tra Shapiro-Wilk [53] trên các trọng số của mô hình LLaMA 7B [57]. Chúng tôi thấy rằng các trọng số của mỗi đơn vị ẩn có các phân phối chuẩn khác nhau. Như vậy, chúng tôi kiểm tra các trọng số của mỗi đơn vị ẩn riêng lẻ. Điều này có nghĩa là đối với trọng số W∈Rin×out, chúng tôi thực hiện kiểm tra trên chiều out. Sử dụng ngưỡng ý nghĩa 5%, chúng tôi thấy rằng 7.5% neuron không được phân phối chuẩn, khoảng 2.5% nhiều hơn tỷ lệ dương tính giả kỳ vọng. Như vậy, mặc dù hầu hết tất cả các trọng số được huấn luyện trước dường như được phân phối chuẩn, có vẻ như có các ngoại lệ. Những ngoại lệ như vậy có thể là do trọng số outlier [13] hoặc vì p-value của kiểm tra Shaprio-Wilk không chính xác cho kích thước mẫu lớn [53] xảy ra trong các đơn vị ẩn lớp FFN LLaMA. Điều này xác minh tuyên bố rằng trọng số mạng neural.

Bảng 12: Đánh giá từng cặp GPT-4 được tổng hợp giữa các hệ thống nơi giá trị của một ô tại hàng x và cột y là (# đánh giá x tốt hơn y - # đánh giá y tốt hơn x) / tổng # số đánh giá

[Bảng dữ liệu đánh giá từng cặp]

--- TRANG 23 ---
[Hình 6: Phân tích dung lượng bộ nhớ của các mô hình LLaMA khác nhau]

G Dung lượng bộ nhớ

Dung lượng bộ nhớ cho huấn luyện QLoRA với các mô hình cơ sở LLaMA khác nhau có thể được nhìn thấy trong Hình 6. Chúng tôi thấy rằng mô hình 33B không khá vừa với 24 GB và rằng paged optimizers cần thiết để huấn luyện nó. Được mô tả cũng là kích thước batch 1 với độ dài chuỗi 512 và gradient checkpointing. Điều này có nghĩa là, nếu người ta sử dụng kích thước batch lớn hơn, hoặc nếu một chuỗi dài được xử lý, gradient kích hoạt có thể tiêu thụ một lượng bộ nhớ đáng kể.

Bảng 13: Thứ tự hoàn chỉnh được tạo ra bởi các đánh giá từng cặp GPT-4 giữa các hệ thống

[Bảng thứ tự hoàn chỉnh]
