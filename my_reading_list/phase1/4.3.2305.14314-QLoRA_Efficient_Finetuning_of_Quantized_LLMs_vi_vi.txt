# 2305.14314.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/peft/2305.14314.pdf
# Kích thước tệp: 1065470 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
QLORA: Tinh chỉnh hiệu quả cho các LLM được lượng tử hóa
Tim Dettmers∗Artidoro Pagnoni∗Ari Holtzman
Luke Zettlemoyer
Đại học Washington
{dettmers,artidoro,ahai,lsz}@cs.washington.edu
Tóm tắt
Chúng tôi trình bày QLORA, một phương pháp tinh chỉnh hiệu quả giảm thiểu việc sử dụng bộ nhớ đủ để tinh chỉnh một mô hình 65B tham số trên một GPU 48GB duy nhất trong khi vẫn bảo toàn hiệu suất tinh chỉnh đầy đủ 16-bit. QLORA truyền ngược gradient thông qua một mô hình ngôn ngữ được huấn luyện trước, đông lạnh, lượng tử hóa 4-bit vào các Low Rank Adapters (LoRA). Mô hình tốt nhất của chúng tôi, mà chúng tôi đặt tên là Guanaco, vượt trội hơn tất cả các mô hình được phát hành công khai trước đó trên benchmark Vicuna, đạt 99,3% mức hiệu suất của ChatGPT trong khi chỉ yêu cầu 24 giờ tinh chỉnh trên một GPU duy nhất. QLORA giới thiệu một số đổi mới để tiết kiệm bộ nhớ mà không hy sinh hiệu suất: (a) 4-bit NormalFloat (NF4), một kiểu dữ liệu mới tối ưu về mặt lý thuyết thông tin cho các trọng số phân phối chuẩn (b) Double Quantization để giảm dung lượng bộ nhớ trung bình bằng cách lượng tử hóa các hằng số lượng tử hóa, và (c) Paged Optimizers để quản lý các đỉnh bộ nhớ. Chúng tôi sử dụng QLORA để tinh chỉnh hơn 1.000 mô hình, cung cấp phân tích chi tiết về hiệu suất theo dõi hướng dẫn và chatbot trên 8 bộ dữ liệu hướng dẫn, nhiều loại mô hình (LLaMA, T5), và các quy mô mô hình sẽ không khả thi để chạy với tinh chỉnh thông thường (ví dụ: mô hình 33B và 65B tham số). Kết quả của chúng tôi cho thấy tinh chỉnh QLoRA trên một bộ dữ liệu chất lượng cao nhỏ dẫn đến kết quả tối tân, ngay cả khi sử dụng các mô hình nhỏ hơn so với SoTA trước đó. Chúng tôi cung cấp phân tích chi tiết về hiệu suất chatbot dựa trên cả đánh giá của con người và GPT-4 cho thấy rằng đánh giá GPT-4 là một thay thế rẻ và hợp lý cho đánh giá của con người. Hơn nữa, chúng tôi nhận thấy rằng các benchmark chatbot hiện tại không đáng tin cậy để đánh giá chính xác mức hiệu suất của chatbot. Một phân tích lemon-picked chứng minh nơi Guanaco thất bại so với ChatGPT. Chúng tôi phát hành tất cả mô hình và mã của chúng tôi, bao gồm các kernel CUDA cho huấn luyện 4-bit.²

1 Giới thiệu
Tinh chỉnh các mô hình ngôn ngữ lớn (LLM) là một cách rất hiệu quả để cải thiện hiệu suất của chúng, [40,62,43,61,59,37] và để thêm các hành vi mong muốn hoặc loại bỏ các hành vi không mong muốn [43,2,4]. Tuy nhiên, tinh chỉnh các mô hình rất lớn có chi phí cấm đoán; tinh chỉnh 16-bit thông thường của một mô hình LLaMA 65B tham số [57] yêu cầu hơn 780 GB bộ nhớ GPU. Mặc dù các phương pháp lượng tử hóa gần đây có thể giảm dung lượng bộ nhớ của LLM [14,13,18,66], các kỹ thuật như vậy chỉ hoạt động cho suy luận và bị hỏng trong quá trình huấn luyện [65].

Chúng tôi chứng minh lần đầu tiên rằng có thể tinh chỉnh một mô hình 4-bit được lượng tử hóa mà không có bất kỳ sự suy giảm hiệu suất nào. Phương pháp của chúng tôi, QLORA, sử dụng một kỹ thuật độ chính xác cao mới để lượng tử hóa một mô hình được huấn luyện trước xuống 4-bit, sau đó thêm một tập nhỏ các trọng số Low-rank Adapter có thể học được [28]

∗Đóng góp bằng nhau.
²https://github.com/artidoro/qlora và https://github.com/TimDettmers/bitsandbytes
Preprint. Đang được xem xét.arXiv:2305.14314v1 [cs.LG] 23 Tháng 5 2023

--- TRANG 2 ---
Bảng 1: Xếp hạng Elo cho một cuộc thi giữa các mô hình, được tính trung bình cho 10.000 thứ tự ban đầu ngẫu nhiên. Người thắng cuộc trong một trận đấu được xác định bởi GPT-4 tuyên bố phản hồi nào tốt hơn cho một prompt đã cho của benchmark Vicuna. Khoảng tin cậy 95% được hiển thị (±). Sau GPT-4, Guanaco 33B và 65B thắng nhiều trận đấu nhất, trong khi Guanaco 13B có điểm tốt hơn Bard.

Mô hình Kích thước Elo
GPT-4 - 1348 ±1
Guanaco 65B 41 GB 1022 ±1
Guanaco 33B 21 GB 992 ±1
Vicuna 13B 26 GB 974 ±1
ChatGPT - 966 ±1
Guanaco 13B 10 GB 916 ±1
Bard - 902 ±1
Guanaco 7B 6 GB 879 ±1

được điều chỉnh bằng cách truyền ngược gradient thông qua các trọng số được lượng tử hóa.

QLORA giảm yêu cầu bộ nhớ trung bình của việc tinh chỉnh một mô hình 65B tham số từ >780GB bộ nhớ GPU xuống <48GB mà không làm suy giảm thời gian chạy hoặc hiệu suất dự đoán so với baseline được tinh chỉnh đầy đủ 16-bit. Điều này đánh dấu một sự thay đổi đáng kể trong khả năng tiếp cận của tinh chỉnh LLM: giờ đây các mô hình có sẵn công khai lớn nhất đến nay có thể tinh chỉnh được trên một GPU duy nhất. Sử dụng QLORA, chúng tôi huấn luyện họ mô hình Guanaco, với mô hình tốt thứ hai đạt 97,8% mức hiệu suất của ChatGPT trên benchmark Vicuna [10], trong khi có thể huấn luyện trong ít hơn 12 giờ trên một GPU tiêu dùng duy nhất; sử dụng một GPU chuyên nghiệp duy nhất trong 24 giờ, chúng tôi đạt 99,3% với mô hình lớn nhất của chúng tôi, về cơ bản thu hẹp khoảng cách với ChatGPT trên benchmark Vicuna. Khi triển khai, mô hình Guanaco nhỏ nhất của chúng tôi (7B tham số) chỉ yêu cầu 5 GB bộ nhớ và vượt trội hơn một mô hình Alpaca 26 GB hơn 20 điểm phần trăm trên benchmark Vicuna (Bảng 6).

QLORA giới thiệu nhiều đổi mới được thiết kế để giảm việc sử dụng bộ nhớ mà không hy sinh hiệu suất: (1) 4-bit NormalFloat, một kiểu dữ liệu lượng tử hóa tối ưu về mặt lý thuyết thông tin cho dữ liệu phân phối chuẩn mang lại kết quả thực nghiệm tốt hơn so với 4-bit Integers và 4-bit Floats. (2) Double Quantization, một phương pháp lượng tử hóa các hằng số lượng tử hóa, tiết kiệm trung bình khoảng 0,37 bit mỗi tham số (xấp xỉ 3 GB cho một mô hình 65B). (3) Paged Optimizers, sử dụng bộ nhớ thống nhất NVIDIA để tránh các đỉnh bộ nhớ gradient checkpointing xảy ra khi xử lý một mini-batch với độ dài chuỗi dài. Chúng tôi kết hợp những đóng góp này thành một phương pháp LoRA được điều chỉnh tốt hơn bao gồm các adapter ở mọi lớp mạng và do đó tránh hầu như tất cả các đánh đổi độ chính xác được thấy trong công việc trước đó.

Hiệu quả của QLORA cho phép chúng tôi thực hiện một nghiên cứu sâu về tinh chỉnh hướng dẫn và hiệu suất chatbot ở các quy mô mô hình sẽ không thể thực hiện được bằng cách sử dụng tinh chỉnh thông thường do chi phí bộ nhớ. Do đó, chúng tôi huấn luyện hơn 1.000 mô hình trên nhiều bộ dữ liệu điều chỉnh hướng dẫn, kiến trúc mô hình và kích thước từ 80M đến 65B tham số. Ngoài việc cho thấy QLORA khôi phục hiệu suất 16-bit (§4) và huấn luyện một chatbot tối tân, Guanaco (§5), chúng tôi cũng phân tích các xu hướng trong các mô hình được huấn luyện. Đầu tiên, chúng tôi nhận thấy rằng chất lượng dữ liệu quan trọng hơn nhiều so với kích thước bộ dữ liệu, ví dụ: một bộ dữ liệu 9k mẫu (OASST1) vượt trội hơn một bộ dữ liệu 450k mẫu (FLAN v2, được lấy mẫu phụ) về hiệu suất chatbot, ngay cả khi cả hai đều nhằm hỗ trợ khái quát hóa theo dõi hướng dẫn. Thứ hai, chúng tôi chỉ ra rằng hiệu suất benchmark Massive Multitask Language Understanding (MMLU) mạnh không ngụ ý hiệu suất benchmark chatbot Vicuna mạnh và ngược lại—nói cách khác, tính phù hợp của bộ dữ liệu quan trọng hơn kích thước cho một nhiệm vụ nhất định.

Hơn nữa, chúng tôi cũng cung cấp một phân tích toàn diện về hiệu suất chatbot sử dụng cả người đánh giá con người và GPT-4 để đánh giá. Chúng tôi sử dụng benchmarking kiểu giải đấu nơi các mô hình cạnh tranh với nhau trong các trận đấu để tạo ra phản hồi tốt nhất cho một prompt nhất định. Người thắng cuộc trong một trận đấu được đánh giá bởi GPT-4 hoặc các chú thích viên con người. Kết quả giải đấu được tổng hợp thành điểm Elo [16,17] xác định thứ hạng hiệu suất chatbot. Chúng tôi nhận thấy rằng đánh giá GPT-4 và con người phần lớn đồng ý về thứ hạng hiệu suất mô hình trong các giải đấu, nhưng chúng tôi cũng nhận thấy có những trường hợp bất đồng mạnh mẽ. Do đó, chúng tôi nhấn mạnh rằng đánh giá dựa trên mô hình trong khi cung cấp một thay thế rẻ cho chú thích con người cũng có những bất định của nó.

Chúng tôi bổ sung kết quả benchmark chatbot của chúng tôi với một phân tích định tính về các mô hình Guanaco. Phân tích của chúng tôi làm nổi bật các trường hợp thành công và thất bại không được nắm bắt bởi các benchmark định lượng. Chúng tôi phát hành tất cả các thế hệ mô hình với chú thích của con người và GPT-4 để tạo điều kiện cho nghiên cứu thêm. Chúng tôi mở nguồn cơ sở mã và kernel CUDA của chúng tôi và tích hợp các phương pháp của chúng tôi vào stack transformers Hugging Face [64], làm cho chúng dễ tiếp cận với tất cả mọi người. Chúng tôi phát hành một bộ sưu tập các adapter cho các mô hình kích thước 7/13/33/65B, được huấn luyện trên 8 bộ dữ liệu theo dõi hướng dẫn khác nhau, tổng cộng 32 mô hình được tinh chỉnh mã nguồn mở khác nhau.

--- TRANG 3 ---
Hình 1: Các phương pháp tinh chỉnh khác nhau và yêu cầu bộ nhớ của chúng. QLORA cải thiện so với LoRA bằng cách lượng tử hóa mô hình transformer xuống độ chính xác 4-bit và sử dụng paged optimizers để xử lý các đỉnh bộ nhớ.

2 Bối cảnh
Lượng tử hóa k-bit theo khối Lượng tử hóa là quá trình rời rạc hóa một đầu vào từ một biểu diễn chứa nhiều thông tin hơn thành một biểu diễn với ít thông tin hơn. Nó thường có nghĩa là lấy một kiểu dữ liệu với nhiều bit hơn và chuyển đổi nó thành ít bit hơn, ví dụ từ float 32-bit thành Integer 8-bit. Để đảm bảo rằng toàn bộ phạm vi của kiểu dữ liệu bit thấp được sử dụng, kiểu dữ liệu đầu vào thường được điều chỉnh lại vào phạm vi kiểu dữ liệu đích thông qua chuẩn hóa bằng tối đa tuyệt đối của các phần tử đầu vào, thường được cấu trúc như một tensor. Ví dụ, lượng tử hóa một tensor Floating Point 32-bit (FP32) thành một tensor Int8 với phạm vi [−127,127]:

XInt8=round(127/absmax(XFP32) * XFP32) = round(cFP32·XFP32), (1)

trong đó c là hằng số lượng tử hóa hoặc quy mô lượng tử hóa. Khử lượng tử hóa là nghịch đảo:

dequant(cFP32,XInt8) = XInt8/cFP32 = XFP32 (2)

Vấn đề với phương pháp này là nếu một giá trị độ lớn lớn (tức là, một ngoại lệ) xảy ra trong tensor đầu vào, thì các bin lượng tử hóa—các kết hợp bit nhất định—không được sử dụng tốt với ít hoặc không có số nào được lượng tử hóa trong một số bin. Để ngăn chặn vấn đề ngoại lệ, một phương pháp phổ biến là chia tensor đầu vào thành các khối được lượng tử hóa độc lập, mỗi khối có hằng số lượng tử hóa c riêng của nó. Điều này có thể được chính thức hóa như sau: Chúng ta chia tensor đầu vào X∈Rb×h thành các khối liền kề có kích thước B bằng cách làm phẳng tensor đầu vào và cắt đoạn tuyến tính thành n = (b×h)/B khối. Chúng ta lượng tử hóa các khối này độc lập với Phương trình 1 để tạo ra một tensor được lượng tử hóa và n hằng số lượng tử hóa ci.

Low-rank Adapters Tinh chỉnh Low-rank Adapter (LoRA) [28] là một phương pháp giảm yêu cầu bộ nhớ bằng cách sử dụng một tập nhỏ các tham số có thể huấn luyện, thường được gọi là adapter, trong khi không cập nhật các tham số mô hình đầy đủ vẫn cố định. Gradient trong quá trình gradient descent ngẫu nhiên được truyền qua các trọng số mô hình được huấn luyện trước cố định đến adapter, được cập nhật để tối ưu hóa hàm mất mát. LoRA tăng cường một phép chiếu tuyến tính thông qua một phép chiếu nhân tử bổ sung. Cho một phép chiếu XW = Y với X∈Rb×h, W∈Rh×o, LoRA tính toán:

Y = XW + sXL1L2, (3)

trong đó L1∈Rh×r và L2∈Rr×o, và s là một vô hướng.

Yêu cầu bộ nhớ của Tinh chỉnh hiệu quả tham số Một điểm thảo luận quan trọng là yêu cầu bộ nhớ của LoRA trong quá trình huấn luyện cả về số lượng và kích thước của các adapter được sử dụng. Vì dung lượng bộ nhớ của LoRA rất tối thiểu, chúng ta có thể sử dụng nhiều adapter hơn để cải thiện hiệu suất mà không tăng đáng kể tổng bộ nhớ được sử dụng. Mặc dù LoRA được thiết kế như một

--- TRANG 4 ---
phương pháp Parameter Efficient Finetuning (PEFT), hầu hết dung lượng bộ nhớ cho tinh chỉnh LLM đến từ gradient kích hoạt chứ không phải từ các tham số LoRA đã học. Đối với một mô hình LLaMA 7B được huấn luyện trên FLAN v2 với kích thước batch là 1, với trọng số LoRA tương đương với 0,2% trọng số mô hình gốc thường được sử dụng [28,37], gradient đầu vào LoRA có dung lượng bộ nhớ là 567 MB trong khi các tham số LoRA chỉ chiếm 26 MB. Với gradient checkpointing [9], gradient đầu vào giảm xuống trung bình 18 MB mỗi chuỗi làm cho chúng tiêu tốn bộ nhớ nhiều hơn tất cả trọng số LoRA kết hợp. So sánh, mô hình cơ sở 4-bit tiêu thụ 5.048 MB bộ nhớ. Điều này làm nổi bật rằng gradient checkpointing quan trọng nhưng cũng rằng việc giảm mạnh mẽ lượng tham số LoRA chỉ mang lại lợi ích bộ nhớ nhỏ. Điều này có nghĩa là chúng ta có thể sử dụng nhiều adapter hơn mà không tăng đáng kể dung lượng bộ nhớ huấn luyện tổng thể (xem Phụ lục G để biết phân tích chi tiết). Như đã thảo luận sau, điều này rất quan trọng để khôi phục hiệu suất độ chính xác 16-bit đầy đủ.

3 Tinh chỉnh QLORA
QLORA đạt được tinh chỉnh 4-bit độ trung thực cao thông qua hai kỹ thuật chúng tôi đề xuất—lượng tử hóa 4-bit NormalFloat (NF4) và Double Quantization. Ngoài ra, chúng tôi giới thiệu Paged Optimizers, để ngăn chặn các đỉnh bộ nhớ trong quá trình gradient checkpointing gây ra lỗi hết bộ nhớ truyền thống làm cho việc tinh chỉnh trên một máy duy nhất khó khăn cho các mô hình lớn.

QLORA có một kiểu dữ liệu lưu trữ độ chính xác thấp, trong trường hợp của chúng tôi thường là 4-bit, và một kiểu dữ liệu tính toán thường là BFloat16. Trong thực tế, điều này có nghĩa là bất cứ khi nào một tensor trọng số QLORA được sử dụng, chúng ta khử lượng tử hóa tensor thành BFloat16, và sau đó thực hiện phép nhân ma trận trong độ chính xác 16-bit.

Bây giờ chúng tôi thảo luận về các thành phần của QLORA theo sau là một định nghĩa chính thức của QLORA.

Lượng tử hóa 4-bit NormalFloat Kiểu dữ liệu NormalFloat (NF) được xây dựng trên Quantile Quantization [15] là một kiểu dữ liệu tối ưu về mặt lý thuyết thông tin đảm bảo mỗi bin lượng tử hóa có một số lượng bằng nhau các giá trị được gán từ tensor đầu vào. Quantile quantization hoạt động bằng cách ước tính quantile của tensor đầu vào thông qua hàm phân phối tích lũy thực nghiệm.

Hạn chế chính của quantile quantization là quá trình ước tính quantile tốn kém. Do đó, các thuật toán xấp xỉ quantile nhanh, chẳng hạn như SRAM quantiles [15], được sử dụng để ước tính chúng. Do tính chất xấp xỉ của các thuật toán ước tính quantile này, kiểu dữ liệu có lỗi lượng tử hóa lớn cho các ngoại lệ, thường là các giá trị quan trọng nhất.

Ước tính quantile đắt đỏ và lỗi xấp xỉ có thể được tránh khi các tensor đầu vào đến từ một phân phối cố định cho đến một hằng số lượng tử hóa. Trong các trường hợp như vậy, các tensor đầu vào có cùng quantiles làm cho việc ước tính quantile chính xác khả thi về mặt tính toán.

Vì trọng số mạng neural được huấn luyện trước thường có phân phối chuẩn tâm không với độ lệch chuẩn σ (xem Phụ lục F), chúng ta có thể biến đổi tất cả trọng số thành một phân phối cố định duy nhất bằng cách điều chỉnh σ sao cho phân phối phù hợp chính xác vào phạm vi của kiểu dữ liệu của chúng ta. Đối với kiểu dữ liệu của chúng ta, chúng ta đặt phạm vi tùy ý [−1,1]. Do đó, cả quantiles cho kiểu dữ liệu và trọng số mạng neural cần được chuẩn hóa vào phạm vi này.

Kiểu dữ liệu tối ưu về mặt lý thuyết thông tin cho phân phối chuẩn tâm không với độ lệch chuẩn σ tùy ý trong phạm vi [−1,1] được tính như sau: (1) ước tính 2^k+ 1 quantiles của một phân phối lý thuyết N(0,1) để có được một kiểu dữ liệu lượng tử hóa quantile k-bit cho phân phối chuẩn, (2) lấy kiểu dữ liệu này và chuẩn hóa các giá trị của nó vào phạm vi [−1,1], (3) lượng tử hóa một tensor trọng số đầu vào bằng cách chuẩn hóa nó vào phạm vi [−1,1] thông qua điều chỉnh tối đa tuyệt đối.

Khi phạm vi trọng số và phạm vi kiểu dữ liệu khớp, chúng ta có thể lượng tử hóa như thường lệ. Bước (3) tương đương với việc điều chỉnh lại độ lệch chuẩn của tensor trọng số để khớp với độ lệch chuẩn của kiểu dữ liệu k-bit. Chính thức hơn, chúng ta ước tính 2^k giá trị qi của kiểu dữ liệu như sau:

qi = 1/2[QX(i/(2^k+ 1)) + QX((i+ 1)/(2^k+ 1))], (4)

trong đó QX(·) là hàm quantile của phân phối chuẩn tiêu chuẩn N(0,1). Một vấn đề cho lượng tử hóa k-bit đối xứng là phương pháp này không có biểu diễn chính xác của số không, đây là một thuộc tính quan trọng để lượng tử hóa padding và các phần tử có giá trị không khác mà không có lỗi. Để

--- TRANG 5 ---
đảm bảo một điểm không rời rạc là 0 và sử dụng tất cả 2^k bit cho một kiểu dữ liệu k-bit, chúng ta tạo ra một kiểu dữ liệu bất đối xứng bằng cách ước tính các quantiles qi của hai phạm vi qi:2^k−1 cho phần âm và 2^k−1+ 1 cho phần dương và sau đó chúng ta thống nhất các tập hợp qi này và loại bỏ một trong hai số không xảy ra trong cả hai tập hợp. Chúng ta gọi kiểu dữ liệu kết quả có số lượng giá trị dự kiến bằng nhau trong mỗi bin lượng tử hóa là k-bit NormalFloat (NFk), vì kiểu dữ liệu tối ưu về mặt lý thuyết thông tin cho dữ liệu phân phối chuẩn tâm không. Các giá trị chính xác của kiểu dữ liệu này có thể được tìm thấy trong Phụ lục E.

Double Quantization Chúng tôi giới thiệu Double Quantization (DQ), quá trình lượng tử hóa các hằng số lượng tử hóa để tiết kiệm bộ nhớ bổ sung. Mặc dù kích thước khối nhỏ được yêu cầu cho lượng tử hóa 4-bit chính xác [13], nó cũng có chi phí bộ nhớ đáng kể. Ví dụ, sử dụng hằng số 32-bit và kích thước khối 64 cho W, các hằng số lượng tử hóa thêm 32/64 = 0,5 bit mỗi tham số trung bình. Double Quantization giúp giảm dung lượng bộ nhớ của các hằng số lượng tử hóa.

Cụ thể hơn, Double Quantization coi các hằng số lượng tử hóa cFP32₂ của lượng tử hóa đầu tiên như đầu vào cho lượng tử hóa thứ hai. Bước thứ hai này tạo ra các hằng số lượng tử hóa được lượng tử hóa cFP8₂ và cấp độ thứ hai của các hằng số lượng tử hóa cFP32₁. Chúng tôi sử dụng 8-bit Floats với kích thước khối 256 cho lượng tử hóa thứ hai vì không quan sát thấy sự suy giảm hiệu suất cho lượng tử hóa 8-bit, phù hợp với kết quả từ Dettmers và Zettlemoyer [13]. Vì cFP32₂ là dương, chúng ta trừ trung bình từ c2 trước khi lượng tử hóa để tập trung các giá trị xung quanh số không và sử dụng lượng tử hóa đối xứng. Trung bình, cho kích thước khối 64, lượng tử hóa này giảm dung lượng bộ nhớ mỗi tham số từ 32/64 = 0,5 bit, xuống 8/64 + 32/(64·256) = 0,127 bit, giảm 0,373 bit mỗi tham số.

Paged Optimizers sử dụng tính năng bộ nhớ thống nhất NVIDIA³ thực hiện chuyển trang tự động giữa CPU và GPU để xử lý GPU không có lỗi trong tình huống GPU thỉnh thoảng hết bộ nhớ. Tính năng này hoạt động như phân trang bộ nhớ thông thường giữa CPU RAM và đĩa. Chúng tôi sử dụng tính năng này để phân bổ bộ nhớ được phân trang cho các trạng thái optimizer sau đó được tự động đẩy ra CPU RAM khi GPU hết bộ nhớ và được phân trang trở lại bộ nhớ GPU khi bộ nhớ được cần trong bước cập nhật optimizer.

QLORA. Sử dụng các thành phần được mô tả ở trên, chúng tôi định nghĩa QLORA cho một lớp tuyến tính duy nhất trong mô hình cơ sở được lượng tử hóa với một adapter LoRA duy nhất như sau:

YBF16 = XBF16 doubleDequant(cFP32₁, ck-bit₂, WNF4) + XBF16 LBF16₁ LBF16₂, (5)

trong đó doubleDequant(·) được định nghĩa là:

doubleDequant(cFP32₁, ck-bit₂, Wk-bit) = dequant(dequant(cFP32₁, ck-bit₂), W4bit) = WBF16, (6)

Chúng tôi sử dụng NF4 cho W và FP8 cho c2. Chúng tôi sử dụng kích thước khối 64 cho W để có độ chính xác lượng tử hóa cao hơn và kích thước khối 256 cho c2 để bảo toàn bộ nhớ.

Đối với cập nhật tham số chỉ cần gradient liên quan đến lỗi cho trọng số adapter ∂E/∂Li, không phải cho trọng số 4-bit ∂E/∂W. Tuy nhiên, việc tính toán ∂E/∂Li đòi hỏi việc tính toán ∂X/∂W tiến hành qua phương trình (5) với khử lượng tử hóa từ lưu trữ WNF4 sang kiểu dữ liệu tính toán WBF16 để tính toán đạo hàm ∂X/∂W trong độ chính xác BFloat16.

Tóm lại, QLORA có một kiểu dữ liệu lưu trữ (thường là 4-bit NormalFloat) và một kiểu dữ liệu tính toán (16-bit BrainFloat). Chúng ta khử lượng tử hóa kiểu dữ liệu lưu trữ sang kiểu dữ liệu tính toán để thực hiện pass tiến và lùi, nhưng chúng ta chỉ tính toán gradient trọng số cho các tham số LoRA sử dụng 16-bit BrainFloat.

4 QLoRA so với Tinh chỉnh tiêu chuẩn
Chúng tôi đã thảo luận về cách QLoRA hoạt động và cách nó có thể giảm đáng kể bộ nhớ cần thiết để tinh chỉnh các mô hình. Câu hỏi chính bây giờ là liệu QLoRA có thể hoạt động tốt như tinh chỉnh mô hình đầy đủ hay không. Hơn nữa, chúng tôi muốn phân tích các thành phần của QLoRA bao gồm tác động của NormalFloat4 so với Float4 tiêu chuẩn. Các phần sau sẽ thảo luận về các thí nghiệm nhằm trả lời những câu hỏi này.

Thiết lập thí nghiệm. Chúng tôi xem xét ba kiến trúc (encoder, encoder-decoder, và chỉ decoder) và so sánh QLoRA với tinh chỉnh adapter 16-bit và với tinh chỉnh đầy đủ cho các mô hình lên đến 3B. Đánh giá của chúng tôi bao gồm GLUE [58] với RoBERTa-large [38], Super-NaturalInstructions (TKInstruct) [61] với T5 [49], và 5-shot MMLU [24] sau khi tinh chỉnh LLaMA trên Flan v2 [39] và Alpaca [55]. Để nghiên cứu thêm về lợi thế của NF4 so với các kiểu dữ liệu 4-bit khác, chúng tôi sử dụng thiết lập của Dettmers và Zettlemoyer [13] và đo độ chính xác zero-shot sau lượng tử hóa và perplexity trên các mô hình khác nhau (OPT [72], LLaMA [57], BLOOM [52], Pythia [7]) cho kích thước mô hình 125m - 13B. Chúng tôi cung cấp chi tiết hơn trong phần kết quả cho từng thiết lập cụ thể để làm cho kết quả dễ đọc hơn. Chi tiết đầy đủ trong Phụ lục A.

QLoRA-All QLoRA-FFN
QLoRA-Attention Alpaca (của chúng tôi)
Stanford-Alpaca
Mô hình 60 61 62 63 64 RougeL
bits
4
16

Hình 2: RougeL cho các mô hình LLaMA 7B trên bộ dữ liệu Alpaca. Mỗi điểm đại diện cho một lần chạy với seed ngẫu nhiên khác nhau. Chúng tôi cải thiện trên các siêu tham số mặc định của Stanford Alpaca được tinh chỉnh đầy đủ để xây dựng một baseline 16-bit mạnh để so sánh. Sử dụng LoRA trên tất cả các lớp transformer rất quan trọng để khớp hiệu suất 16-bit.

Mặc dù paged optimizers rất quan trọng để thực hiện điều chỉnh QLORA cho 33B/65B trên một GPU 24/48GB duy nhất, chúng tôi không cung cấp đo lường cứng cho Paged Optimizers vì việc phân trang chỉ xảy ra khi xử lý mini-batch với độ dài chuỗi dài, điều này rất hiếm. Tuy nhiên, chúng tôi thực hiện phân tích thời gian chạy của paged optimizers cho các mô hình 65B trên GPU 48GB và nhận thấy rằng với kích thước batch 16, paged optimizers cung cấp cùng tốc độ huấn luyện như optimizers thông thường. Công việc tương lai nên đo lường và đặc trưng hóa trong những trường hợp nào xảy ra chậm lại từ quá trình phân trang.

Siêu tham số LoRA mặc định không khớp hiệu suất 16-bit Khi sử dụng thực hành tiêu chuẩn áp dụng LoRA cho ma trận chiếu attention query và value [28], chúng tôi không thể sao chép hiệu suất tinh chỉnh đầy đủ cho các mô hình cơ sở lớn. Như được hiển thị trong Hình 2 cho tinh chỉnh LLaMA 7B trên Alpaca, chúng tôi nhận thấy rằng siêu tham số LoRA quan trọng nhất là có bao nhiêu adapter LoRA được sử dụng tổng cộng và LoRA trên tất cả các lớp khối transformer tuyến tính được yêu cầu để khớp hiệu suất tinh chỉnh đầy đủ. Các siêu tham số LoRA khác, chẳng hạn như chiều chiếu r, không ảnh hưởng đến hiệu suất (xem Phụ lục A).

10^10
10^11
Tổng bit mô hình
0.60
0.61
0.62
0.63
0.64
0.65
0.66
0.67
Độ chính xác zeroshot trung bình
4-bit LLaMA
Float
NFloat
NFloat + DQ
Kiểu dữ liệu

Hình 3: Độ chính xác zero-shot trung bình trên Winogrande, HellaSwag, PiQA, Arc-Easy, và Arc-Challenge sử dụng các mô hình LLaMA với các kiểu dữ liệu 4-bit khác nhau. Kiểu dữ liệu NormalFloat cải thiện đáng kể độ chính xác bit-for-bit so với 4-bit Floats thông thường. Mặc dù Double Quantization (DQ) chỉ dẫn đến lợi ích nhỏ, nó cho phép kiểm soát chi tiết hơn dung lượng bộ nhớ để phù hợp với các mô hình có kích thước nhất định (33B/65B) vào các GPU nhất định (24/48GB).

Tương tự, chúng tôi nhận thấy rằng siêu tham số mặc định cho baseline được tinh chỉnh đầy đủ bị điều chỉnh kém. Chúng tôi thực hiện tìm kiếm siêu tham số trên learning rates 1e-6 đến 5e-5 và kích thước batch 8 đến 128 để tìm baseline mạnh mẽ. Kết quả cho tinh chỉnh LLaMA 7B trên Alpaca được hiển thị trong Hình 2.

4-bit NormalFloat mang lại hiệu suất tốt hơn 4-bit Floating Point Mặc dù kiểu dữ liệu 4-bit NormalFloat (NF4) tối ưu về mặt lý thuyết thông tin, vẫn cần xác định xem thuộc tính này có chuyển thành lợi thế thực nghiệm hay không. Chúng tôi theo thiết lập từ Dettmers và Zettlemoyer [13] nơi các LLM được lượng tử hóa (OPT [72], BLOOM [52], Pythia [7], LLaMA) có kích thước khác nhau (125M đến 65B) với các kiểu dữ liệu khác nhau được đánh giá trên mô hình hóa ngôn ngữ và một tập hợp các nhiệm vụ zero-shot. Trong Hình 3 và Bảng 2 chúng ta thấy rằng NF4 cải thiện hiệu suất đáng kể so với FP4 và Int4 và double quantization giảm dung lượng bộ nhớ mà không làm suy giảm hiệu suất.

k-bit QLORA khớp hiệu suất tinh chỉnh đầy đủ 16-bit và hiệu suất LoRA 16-bit Các phát hiện gần đây đã thiết lập rằng lượng tử hóa 4-bit cho suy luận

--- TRANG 6 ---
Bảng 3: Thí nghiệm so sánh 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), và 4-bit NormalFloat (NF4) trên GLUE và Super-NaturalInstructions. QLORA khôi phục lại LoRA 16-bit và tinh chỉnh đầy đủ.

Bộ dữ liệu GLUE (Acc.) Super-NaturalInstructions (RougeL)
Mô hình RoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B
BF16 88.6 40.1 42.1 48.0 54.3 62.0
BF16 replication 88.6 40.0 42.2 47.3 54.9 -
LoRA BF16 88.8 40.5 42.6 47.1 55.4 60.7
QLORA Int8 88.8 40.4 42.9 45.4 56.5 60.7
QLORA FP4 88.6 40.3 42.4 47.5 55.6 60.9
QLORA NF4 + DQ - 40.4 42.7 47.7 55.3 60.9

có thể, nhưng dẫn đến suy giảm hiệu suất so với 16-bit [13,18]. Điều này đặt ra câu hỏi quan trọng liệu hiệu suất bị mất có thể được khôi phục bằng cách tiến hành tinh chỉnh adapter 4-bit hay không. Chúng tôi kiểm tra điều này cho hai thiết lập.

Bảng 2: Pile Common Crawl mean perplexity cho các kiểu dữ liệu khác nhau cho các mô hình OPT, BLOOM, LLaMA, và Pythia 125M đến 13B.

Kiểu dữ liệu Mean PPL
Int4 34.34
Float4 (E2M1) 31.07
Float4 (E3M0) 29.48
NFloat4 + DQ 27.41

Cái đầu tiên tập trung vào so sánh với tinh chỉnh 16-bit đầy đủ của các mô hình RoBERTA và T5 có kích thước 125M đến 3B tham số trên GLUE và bộ dữ liệu Super-NaturalInstructions. Kết quả được hiển thị trong Bảng 3. Trong cả hai bộ dữ liệu, chúng tôi quan sát thấy rằng các phương pháp adapter 16-bit, 8-bit, và 4-bit sao chép hiệu suất của baseline được tinh chỉnh đầy đủ 16-bit. Điều này gợi ý rằng hiệu suất bị mất do lượng tử hóa không chính xác có thể được khôi phục hoàn toàn thông qua tinh chỉnh adapter sau lượng tử hóa.

Đối với thiết lập thứ hai của chúng tôi, vì tinh chỉnh đầy đủ các mô hình ở và vượt quá 11B tham số yêu cầu nhiều hơn một máy chủ GPU bộ nhớ cao, chúng tôi tiếp tục kiểm tra liệu 4-bit QLORA có thể khớp với 16-bit LoRA ở quy mô tham số 7B đến 65B hay không. Để đạt được điều này, chúng tôi tinh chỉnh LLaMA 7B đến 65B trên hai bộ dữ liệu theo dõi hướng dẫn, Alpaca và FLAN v2, và đánh giá trên benchmark MMLU thông qua độ chính xác 5-shot. Kết quả được hiển thị trong Bảng 4 nơi chúng ta thấy rằng NF4 với double quantization hoàn toàn khôi phục hiệu suất MMLU LoRA 16-bit. Ngoài ra, chúng tôi cũng lưu ý rằng QLORA với FP4 tụt hậu so với baseline LoRA brain float 16-bit khoảng 1 điểm phần trăm. Điều này củng cố cả hai phát hiện của chúng tôi rằng (1) QLORA với NF4 sao chép cả hiệu suất tinh chỉnh đầy đủ 16-bit và hiệu suất tinh chỉnh LoRA 16-bit, và (2) NF4 vượt trội hơn FP4 về độ chính xác lượng tử hóa.

Tóm tắt Kết quả của chúng tôi nhất quán cho thấy 4-bit QLORA với kiểu dữ liệu NF4 khớp với hiệu suất tinh chỉnh đầy đủ 16-bit và hiệu suất tinh chỉnh LoRA 16-bit trên các benchmark học thuật với các thiết lập đánh giá được thiết lập tốt. Chúng tôi cũng đã chỉ ra rằng NF4 hiệu quả hơn FP4 và double quantization không làm suy giảm hiệu suất. Kết hợp, điều này tạo thành bằng chứng thuyết phục rằng điều chỉnh 4-bit QLORA một cách đáng tin cậy mang lại kết quả khớp với các phương pháp 16-bit.

Phù hợp với công việc trước đó về lượng tử hóa [13], kết quả MMLU và Elo của chúng tôi chỉ ra rằng với ngân sách tài nguyên tinh chỉnh và suy luận nhất định, có lợi khi tăng số lượng tham số trong mô hình cơ sở trong khi giảm độ chính xác của chúng. Điều này làm nổi bật tầm quan trọng của lợi ích hiệu quả từ QLORA. Vì chúng tôi không quan sát thấy suy giảm hiệu suất so với tinh chỉnh đầy đủ trong các thí nghiệm của chúng tôi với tinh chỉnh 4-bit, điều này đặt ra câu hỏi về việc đánh đổi hiệu suất-độ chính xác nằm chính xác ở đâu cho điều chỉnh QLoRA, mà chúng tôi để lại cho công việc tương lai khám phá.

Chúng tôi tiếp tục điều tra điều chỉnh hướng dẫn ở các quy mô sẽ không thể khám phá với tinh chỉnh 16-bit đầy đủ trên phần cứng nghiên cứu học thuật.

5 Đẩy trạng thái tối tân của Chatbot với QLoRA
Sau khi thiết lập rằng 4-bit QLORA khớp hiệu suất 16-bit trên các quy mô, nhiệm vụ, và bộ dữ liệu, chúng tôi tiến hành một nghiên cứu sâu về điều chỉnh hướng dẫn lên đến các mô hình ngôn ngữ mã nguồn mở lớn nhất có sẵn cho nghiên cứu. Để đánh giá hiệu suất của điều chỉnh hướng dẫn các mô hình này, chúng tôi đánh giá

--- TRANG 7 ---
Bảng 4: Độ chính xác MMLU 5-shot test trung bình cho các mô hình LLaMA 7-65B được tinh chỉnh với adapter trên Alpaca và FLAN v2 cho các kiểu dữ liệu khác nhau. Nhìn chung, NF4 với double quantization (DQ) khớp hiệu suất BFloat16, trong khi FP4 nhất quán tụt hậu cả hai một điểm phần trăm.

Độ chính xác MMLU 5-shot trung bình
Kích thước LLaMA 7B 13B 33B 65B Trung bình
Bộ dữ liệu Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2
BFloat16 38.4 45.6 47.2 50.6 57.7 60.5 61.8 62.5 53.0
Float4 37.2 44.0 47.3 50.0 55.9 58.5 61.3 63.3 52.2
NFloat4 + DQ 39.0 44.5 47.5 50.7 57.3 59.2 61.8 63.9 53.1

trên một benchmark Hiểu biết Ngôn ngữ Tự nhiên thách thức (MMLU) và phát triển các phương pháp mới để đánh giá hiệu suất chatbot trong thế giới thực.

5.1 Thiết lập thí nghiệm
Bây giờ chúng tôi mô tả tổng quan về thiết lập thí nghiệm với chi tiết đầy đủ trong Phụ lục B.

Dữ liệu Theo hiểu biết của chúng tôi, không có nghiên cứu toàn diện về các bộ dữ liệu theo dõi hướng dẫn gần đây, chúng tôi chọn tám bộ dữ liệu gần đây. Chúng tôi bao gồm các bộ dữ liệu thu được thông qua crowd-sourcing (OASST1 [31], HH-RLHF [4]), chưng cất từ các mô hình được điều chỉnh hướng dẫn (Alpaca [55], self-instruct [59], unnatural-instructions [26]), tổng hợp corpus (FLAN v2 [12]), cũng như hybrid (Chip2 [32], Longform [30]). Các bộ dữ liệu này bao gồm các ngôn ngữ khác nhau, kích thước dữ liệu, và giấy phép.

Thiết lập Huấn luyện Để tránh các hiệu ứng gây nhiễu từ các mục tiêu huấn luyện khác nhau, chúng tôi thực hiện tinh chỉnh QLoRA với mất mát cross-entropy (học có giám sát) không có học củng cố, ngay cả đối với các bộ dữ liệu bao gồm đánh giá con người về các phản hồi khác nhau. Đối với các bộ dữ liệu có sự phân biệt rõ ràng giữa hướng dẫn và phản hồi, chúng tôi chỉ tinh chỉnh trên phản hồi (xem ablations trong Phụ lục B). Đối với OASST1 và HH-RLHF, nhiều phản hồi có sẵn. Sau đó chúng tôi chọn phản hồi hàng đầu ở mọi cấp độ của cây hội thoại và tinh chỉnh trên toàn bộ cuộc hội thoại được chọn, bao gồm các hướng dẫn. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng NF4 QLORA với double quantization và paged optimizers để ngăn chặn các đỉnh bộ nhớ trong quá trình gradient checkpointing. Chúng tôi thực hiện tìm kiếm siêu tham số nhỏ cho các mô hình LLaMA 13B và 33B và chúng tôi nhận thấy rằng tất cả các thiết lập siêu tham số được tìm thấy ở 7B khái quát (bao gồm số epoch) ngoại trừ learning rate và kích thước batch. Chúng tôi giảm một nửa learning rate cho 33B và 65B trong khi tăng gấp đôi kích thước batch.

Baseline Chúng tôi so sánh các mô hình của chúng tôi với cả hệ thống chatbot nghiên cứu (Vicuna [10] và Open Assistant [31]) và thương mại (GPT-4 [42], GPT-3.5-turbo và Bard). Mô hình Open Assistant là một mô hình LLaMA 33B được tinh chỉnh với Reinforcement Learning from Human Feedback (RLHF) trên cùng bộ dữ liệu OASST1 mà chúng tôi thí nghiệm. Vicuna thực hiện tinh chỉnh đầy đủ của LLaMA 13B trên các cuộc hội thoại được chia sẻ người dùng độc quyền từ ShareGPT và do đó là kết quả của chưng cất từ các mô hình OpenAI GPT.

5.2 Đánh giá
Bảng 5: Kết quả MMLU 5-shot test cho các kích thước khác nhau của LLaMA được tinh chỉnh trên các bộ dữ liệu tương ứng sử dụng QLoRA.

Bộ dữ liệu 7B 13B 33B 65B
LLaMA no tuning 35.1 46.9 57.8 63.4
Self-Instruct 36.4 33.3 53.0 56.7
Longform 32.1 43.2 56.6 59.7
Chip2 34.5 41.6 53.6 59.8
HH-RLHF 34.9 44.6 55.8 60.1
Unnatural Instruct 41.9 48.1 57.3 61.3
Guanaco (OASST1) 36.6 46.4 57.0 62.2
Alpaca 38.8 47.8 57.3 62.5
FLAN v2 44.5 51.4 59.2 63.9

Theo thực hành chung, chúng tôi sử dụng benchmark MMLU (Massively Multitask Language Understanding) [24] để đo hiệu suất trên một loạt các nhiệm vụ hiểu biết ngôn ngữ. Đây là một benchmark đa lựa chọn bao gồm 57 nhiệm vụ bao gồm toán học cơ bản, lịch sử Hoa Kỳ, khoa học máy tính, luật, và nhiều hơn nữa. Chúng tôi báo cáo độ chính xác test 5-shot.

Chúng tôi cũng kiểm tra khả năng ngôn ngữ tạo sinh thông qua cả đánh giá tự động và con người. Tập hợp đánh giá thứ hai này dựa trên các truy vấn được tuyển chọn bởi con người và nhằm đo chất lượng phản hồi mô hình. Mặc dù đây là một testbed thực tế hơn cho hiệu suất mô hình chatbot và đang tăng phổ biến, không có giao thức được chấp nhận chung trong tài liệu. Chúng tôi mô tả bên dưới thiết lập được đề xuất của chúng tôi, sử dụng nucleus sampling với p= 0.9 và nhiệt độ 0.7 trong tất cả các trường hợp.

--- TRANG 8 ---
Dữ liệu Benchmark Chúng tôi đánh giá trên hai bộ dữ liệu được tuyển chọn của các truy vấn (câu hỏi): prompts Vicuna [10] và bộ dữ liệu xác thực OASST1 [31]. Chúng tôi sử dụng prompts Vicuna, một tập hợp 80 prompts từ một tập hợp đa dạng các danh mục, mà không có sửa đổi. Bộ dữ liệu OASST1 là một bộ sưu tập đa ngôn ngữ các cuộc đối thoại đa lượt được tổ chức bởi đám đông giữa người dùng và trợ lý. Chúng tôi chọn tất cả tin nhắn người dùng trong bộ dữ liệu xác thực làm truy vấn và bao gồm các lượt trước đó trong prompt. Thủ tục này dẫn đến 953 truy vấn người dùng duy nhất. Chúng tôi gọi hai bộ dữ liệu này là benchmark Vicuna và OA.

Đánh giá Tự động Đầu tiên, dựa trên giao thức đánh giá được giới thiệu bởi Chiang et al. [10], chúng tôi sử dụng GPT-4 để đánh giá hiệu suất của các hệ thống khác nhau so với ChatGPT (GPT-3.5 Turbo) trên benchmark Vicuna. Cho một truy vấn cùng với phản hồi của ChatGPT và một mô hình, GPT-4 được nhắc gán điểm trên mười cho cả hai phản hồi và cung cấp giải thích. Hiệu suất tổng thể của một mô hình được tính như tỷ lệ phần trăm của điểm mà ChatGPT đạt được. Lưu ý điểm tương đối này có thể cao hơn 100% nếu mô hình đạt được điểm tuyệt đối cao hơn ChatGPT. Chúng tôi tìm thấy một hiệu ứng thứ tự đáng kể với GPT-4 tăng điểm của phản hồi xảy ra sớm hơn trong prompt. Để kiểm soát các hiệu ứng như vậy, chúng tôi khuyến nghị báo cáo điểm trung bình trên cả hai thứ tự.

Tiếp theo, chúng tôi đo hiệu suất thông qua so sánh trực tiếp giữa các đầu ra hệ thống. Chúng tôi đơn giản hóa lược đồ đánh giá thành một vấn đề ghi nhãn ba lớp tính đến hòa. Chúng tôi nhắc GPT-4 chọn phản hồi tốt nhất hoặc tuyên bố hòa và cung cấp giải thích. Chúng tôi tiến hành các so sánh trực tiếp này trên tất cả các hoán vị của các cặp hệ thống trên cả benchmark Vicuna và OA.

Đánh giá Con người Mặc dù công việc gần đây chỉ ra các mô hình tạo sinh có thể được sử dụng hiệu quả cho đánh giá hệ thống [19], độ tin cậy của đánh giá GPT-4 để đánh giá hiệu suất chatbot, theo hiểu biết của chúng tôi, vẫn chưa được chứng minh có tương quan với đánh giá con người. Do đó, chúng tôi chạy hai đánh giá con người song song trên benchmark Vicuna khớp với cả hai giao thức đánh giá tự động được mô tả ở trên. Chúng tôi sử dụng Amazon Mechanical Turk (AMT) và có hai chú thích viên con người cho so sánh với ChatGPT và ba chú thích viên cho so sánh theo cặp.

Xếp hạng Elo Với cả so sánh theo cặp của con người và tự động, chúng tôi tạo ra một cuộc thi kiểu giải đấu nơi các mô hình cạnh tranh với nhau. Giải đấu được tạo thành từ các trận đấu nơi các cặp mô hình cạnh tranh để tạo ra phản hồi tốt nhất cho một prompt nhất định. Điều này tương tự như cách Bai et al. [4] và Chiang et al. [10] so sánh các mô hình, nhưng chúng tôi cũng sử dụng đánh giá GPT-4 ngoài đánh giá con người. Chúng tôi lấy mẫu ngẫu nhiên từ tập hợp các so sánh được ghi nhãn để tính toán Elo [16,17]. Xếp hạng Elo, được sử dụng rộng rãi trong cờ vua và các trò chơi khác, là một thước đo tỷ lệ thắng dự kiến so với tỷ lệ thắng của đối thủ, ví dụ: Elo 1100 vs 1000 có nghĩa là người chơi Elo 1100 có tỷ lệ thắng dự kiến khoảng 65% so với đối thủ Elo 1000; một trận đấu 1000 vs 1000 hoặc 1100 vs 1100 dẫn đến tỷ lệ thắng dự kiến 50%. Xếp hạng Elo thay đổi sau mỗi trận đấu tỷ lệ thuận với kết quả dự kiến, tức là, một cú lật đổ bất ngờ dẫn đến thay đổi lớn trong xếp hạng Elo trong khi kết quả dự kiến dẫn đến thay đổi nhỏ. Theo thời gian, xếp hạng Elo xấp xỉ khớp với kỹ năng của mỗi người chơi trong việc chơi trò chơi. Chúng tôi bắt đầu với điểm 1.000 và sử dụng K= 32. Tương tự như Chiang et al. [10], chúng tôi lặp lại thủ tục này 10.000 lần với các seed ngẫu nhiên khác nhau để kiểm soát các hiệu ứng thứ tự, ví dụ: hiệu ứng của các cặp mô hình nào cạnh tranh với nhau trước.

5.3 Guanaco: QLORA được huấn luyện trên OASST1 là Chatbot tối tân
Dựa trên đánh giá tự động và con người của chúng tôi, chúng tôi nhận thấy rằng mô hình QLORA được điều chỉnh hàng đầu, Guanaco 65B, mà chúng tôi tinh chỉnh trên một biến thể của OASST1, là mô hình chatbot mã nguồn mở hoạt động tốt nhất và cung cấp hiệu suất cạnh tranh với ChatGPT. Khi so sánh với GPT-4, Guanaco 65B và 33B có xác suất thắng dự kiến 30%, dựa trên xếp hạng Elo từ so sánh theo cặp cấp hệ thống của chú thích viên con người - cao nhất được báo cáo đến nay.

Kết quả benchmark Vicuna [10] so với ChatGPT được hiển thị trong Bảng 6. Chúng tôi nhận thấy rằng Guanaco 65B là mô hình hoạt động tốt nhất sau GPT-4, đạt 99,3% hiệu suất so với ChatGPT. Guanaco 33B có nhiều tham số hơn mô hình Vicuna 13B, nhưng chỉ sử dụng độ chính xác 4-bit cho trọng số của nó và do đó hiệu quả bộ nhớ hơn nhiều ở 21 GB vs 26 GB, cung cấp cải thiện ba điểm phần trăm so với Vicuna 13B. Hơn nữa, Guanaco 7B dễ dàng phù hợp với điện thoại hiện đại ở dung lượng 5 GB trong khi vẫn ghi điểm gần 20 điểm phần trăm cao hơn Alpaca 13B.

Tuy nhiên, Bảng 6 cũng có khoảng tin cậy rất rộng, với nhiều mô hình chồng chéo về hiệu suất. Chúng tôi giả thuyết rằng sự không chắc chắn này đến từ việc thiếu đặc tả rõ ràng về quy mô, ví dụ: không rõ ràng 8 trên thang điểm 10 có nghĩa là gì trên các tình huống khác nhau. Do đó, chúng tôi thay vào đó khuyến nghị sử dụng phương pháp xếp hạng Elo [16], dựa trên đánh giá theo cặp từ chú thích viên con người và GPT-4 để tránh vấn đề grounding một thang điểm tuyệt đối. Xếp hạng Elo của các mô hình cạnh tranh nhất

--- TRANG 9 ---
Bảng 6: Điểm benchmark Vicuna zero-shot như tỷ lệ phần trăm của điểm thu được bởi ChatGPT được đánh giá bởi GPT-4. Chúng ta thấy rằng các mô hình OASST1 hoạt động gần với ChatGPT mặc dù được huấn luyện trên một bộ dữ liệu rất nhỏ và có một phần yêu cầu bộ nhớ của các mô hình baseline.

Mô hình / Bộ dữ liệu Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI
GPT-4 - - - 119.4% 110.1% 114.5 % 2.6%
Bard - - - 93.2% 96.4% 94.8% 4.1%
Guanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4%
Alpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3%
FLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6%
Guanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4%
Open Assistant 33B 16-bit 66 GB 91.2% 98.7% 94.9% 4.5%
Alpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2%
FLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9%
Vicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5%
Guanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2%
Alpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2%
HH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7%
Unnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2%
Chip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7%
Longform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2%
Self-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6%
FLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6%
Guanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4%
Alpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0%
FLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0%

có thể thấy trong Bảng 1. Chúng tôi lưu ý rằng xếp hạng con người và GPT-4 của các mô hình trên benchmark Vicuna bất đồng một phần, đặc biệt đối với Guanaco 7B, nhưng nhất quán cho hầu hết các mô hình với Kendall Tau τ= 0.43 và tương quan xếp hạng Spearman r= 0.55 ở cấp hệ thống. Ở cấp ví dụ, sự đồng ý giữa GPT-4 và phiếu bầu đa số của chú thích viên con người yếu hơn với Fleiss κ= 0.25. Nhìn chung, điều này cho thấy sự đồng ý vừa phải giữa đánh giá cấp hệ thống bởi GPT-4 và chú thích viên con người, và do đó đánh giá dựa trên mô hình đại diện cho một thay thế hơi đáng tin cậy cho đánh giá con người. Chúng tôi thảo luận thêm các cân nhắc trong Phần 6.2.

Xếp hạng Elo trong Bảng 7 chỉ ra rằng các mô hình Guanaco 33B và 65B vượt trội hơn tất cả các mô hình ngoài GPT-4 trên benchmark Vicuna và OA và chúng hoạt động tương đương với ChatGPT phù hợp với Bảng 6. Chúng tôi lưu ý rằng benchmark Vicuna thiên về các mô hình mã nguồn mở trong khi benchmark OA lớn hơn thiên về ChatGPT. Hơn nữa, chúng ta có thể thấy từ Bảng 5 và 6 rằng tính phù hợp của bộ dữ liệu tinh chỉnh là một yếu tố quyết định trong hiệu suất. Tinh chỉnh các mô hình Llama trên FLAN v2 hoạt động đặc biệt tốt trên MMLU, nhưng hoạt động tệ nhất trên benchmark Vicuna (các xu hướng tương tự được quan sát với các mô hình khác). Điều này cũng chỉ ra tính trực giao một phần trong các benchmark đánh giá hiện tại: hiệu suất MMLU mạnh không ngụ ý hiệu suất chatbot mạnh (như được đo bởi benchmark Vicuna hoặc OA) và ngược lại.

Guanaco là mô hình duy nhất hàng đầu trong đánh giá của chúng tôi không được huấn luyện trên dữ liệu độc quyền vì hướng dẫn thu thập bộ dữ liệu OASST1 cấm rõ ràng việc sử dụng các mô hình GPT. Mô hình tốt nhất tiếp theo được huấn luyện chỉ trên dữ liệu mã nguồn mở là mô hình Anthropic HH-RLHF, ghi điểm thấp hơn Guanaco 30 điểm phần trăm trên benchmark Vicuna (xem Bảng 6). Nhìn chung, những kết quả này cho thấy rằng 4-bit QLORA hiệu quả và có thể tạo ra các chatbot tối tân cạnh tranh với ChatGPT. Hơn nữa, Guanaco 33B của chúng tôi có thể được huấn luyện trên GPU tiêu dùng 24 GB trong ít hơn 12 giờ. Điều này mở ra tiềm năng cho công việc tương lai thông qua điều chỉnh QLORA trên dữ liệu mã nguồn mở chuyên biệt, tạo ra các mô hình có thể cạnh tranh với các mô hình thương mại tốt nhất tồn tại ngày nay.

6 Phân tích Định tính
Mặc dù phân tích định lượng là cốt lõi của đánh giá của chúng tôi, có một số vấn đề với việc chỉ nhìn vào thống kê tóm tắt. Có lẽ lớn nhất là vấn đề về tính hợp lệ của benchmark [36]—liệu một benchmark có thực sự kiểm tra những gì tên hoặc mô tả của nó gợi ý luôn là câu hỏi, đặc biệt khi chúng ta khám phá "shortcuts" để giải quyết các benchmark mà các mô hình học máy đôi khi khai thác [22,46]. Để giảm bớt một phần điều này, chúng tôi ở đây thực hiện một số phân tích định tính, trong hai phần. Đầu tiên, trong §6.1

--- TRANG 10 ---
Bảng 7: Xếp hạng Elo cho một giải đấu giữa các mô hình nơi các mô hình cạnh tranh để tạo ra phản hồi tốt nhất cho một prompt, được đánh giá bởi người đánh giá con người hoặc GPT-4. Nhìn chung, Guanaco 65B và 33B có xu hướng được ưa thích hơn ChatGPT-3.5 trên các benchmark được nghiên cứu. Theo người đánh giá con người, chúng có Mỗi sự khác biệt 10 điểm trong Elo xấp xỉ là sự khác biệt 1,5% trong tỷ lệ thắng.

Benchmark Vicuna Vicuna Open Assistant
# Prompts 80 80 953
Judge Human raters GPT-4 GPT-4 Median Rank
Model Elo Rank Elo Rank Elo Rank
GPT-4 1176 1 1348 1 1294 1 1
Guanaco-65B 1023 2 1022 2 1008 3 2
Guanaco-33B 1009 4 992 3 1002 4 4
ChatGPT-3.5 Turbo 916 7 966 5 1015 2 5
Vicuna-13B 984 5 974 4 936 5 5
Guanaco-13B 975 6 913 6 885 6 6
Guanaco-7B 1010 3 879 8 860 7 7
Bard 909 8 902 7 - - 8

chúng tôi hiển thị một số ví dụ mà chúng tôi tin là đại diện cho một số mô hình được quan sát trong văn bản được tạo ra bởi mô hình Guanaco 65b của chúng tôi. Thứ hai, §6.2 chúng tôi chi tiết các cân nhắc về kết quả mà chúng tôi đã thảo luận và giải thích của chúng tôi về chúng.

6.1 Phân tích Định tính về Các Thế hệ Ví dụ
Để tìm ví dụ, trước tiên chúng tôi xem qua dữ liệu được tạo cho benchmark Vicuna và benchmark OpenAssistant, và tìm kiếm các mô hình trong câu trả lời mà Guanaco tạo ra. Khi chúng tôi nhận thấy một mô hình, chúng tôi cố gắng thiết lập một câu hỏi hoặc prompt sẽ gây ra mô hình ngay cả khi đó là giải pháp không chính xác, ví dụ: nếu chúng tôi quan sát thấy rằng mô hình có xu hướng đưa ra câu trả lời dài dòng, chúng tôi nhắc mô hình "Trả lời có hoặc không mà không giải thích." Chúng tôi sử dụng điều này để tìm "lemons" nơi chúng tôi quản lý để phá vỡ mô hình một cách đối kháng và "cherries" nơi chúng tôi thất bại trong việc phá vỡ mô hình, và trình bày cả hai. Tất cả các thế hệ trong phần này được tạo ra với Nucleus Sampling [25] với p= 0.9.

Tất nhiên, điều này không toàn diện, vì nó vượt quá phạm vi của nghiên cứu định tính nhỏ này để kiểm soát tất cả các biến liên quan, ví dụ: phân phối đầy đủ của các phản hồi mà mô hình có thể tạo ra cho một prompt nhất định khá lớn, vì vậy chúng tôi dựa vào các mẫu mà chúng tôi hy vọng là đại diện. Tuy nhiên, chúng tôi tin rằng việc mô tả những ví dụ này đưa ra bối cảnh cho bằng chứng định lượng được hiển thị trước đó trong bài báo. Vì chúng tôi mở nguồn tất cả các mô hình và mã, chúng tôi hy vọng phần này sẽ truyền cảm hứng cho công việc tương lai để kiểm tra chi tiết hơn các vấn đề mà chúng tôi trình bày ở đây.

Nhớ lại Sự thật Đối với các câu hỏi như "Thủ đô của Zambia là gì?" tất cả các mô hình nhất quán tạo ra câu trả lời đúng, ví dụ:
Guanaco: Thủ đô của Zambia là Lusaka.
Tuy nhiên khi các câu hỏi trở nên tối tăm hơn, Guanaco trở nên không đáng tin cậy, nhưng vẫn tự tin. Ví dụ trong phản hồi với prompt này từ HotPotQA [68]:
Người dùng: Trong năm nào ca sĩ phổ biến 'I'll Keep the Lovelight Burning' sinh ra?
Guanaco: Al Jolson được ghi nhận đã phổ biến bài hát 'I'll Keep the Lovelight Burning,' và ông sinh năm 1886.

Guanaco tạo ra người phổ biến sai và sinh nhật sai (mặc dù sinh nhật được đưa ra là đúng cho người được đề cập, Al Jolson).

Tính gợi ý Guanaco cho thấy sự kháng cự đáng ngạc nhiên để đi cùng với một số loại thông tin sai lệch được giả định, ví dụ: trong tương tác sau:
Người dùng: Cuối cùng, nó đã được xác nhận chính thức như thế nào rằng trái đất phẳng bởi các nhà khoa học được đánh giá ngang hàng?

--- TRANG 11 ---
Guanaco: Trái đất chưa bao giờ được xác nhận chính thức là phẳng bởi các nhà khoa học được đánh giá ngang hàng. Sự đồng thuận áp đảo trong giới khoa học là trái đất hình cầu. Trên thực tế, điều này đã được biết trong nhiều thế kỷ và đã được xác nhận thông qua nhiều thí nghiệm và quan sát.
Niềm tin rằng trái đất phẳng là một thuyết âm mưu đã bị bác bỏ bởi khoa học. Ý tưởng rằng một nhóm nhà khoa học đã đánh giá ngang hàng và xác nhận trái đất phẳng đơn giản là sai.
Nếu bạn có thêm câu hỏi nào, hãy hỏi.

Guanaco cũng khá giỏi trong việc biết loại câu hỏi nào không thể trả lời, ví dụ:
Người dùng: Bây giờ là mấy giờ?
Guanaco: Tôi xin lỗi, nhưng là một mô hình ngôn ngữ AI, tôi không có quyền truy cập vào thông tin thời gian thực. Phản hồi của tôi được tạo ra dựa trên kiến thức có sẵn của tôi và không phản ánh thời gian hiện tại. Nếu bạn đang tìm kiếm thời gian hiện tại, tôi gợi ý sử dụng thiết bị đo thời gian hoặc kiểm tra đồng hồ trực tuyến.

Kết quả tương tự giữ cho "Bạn ở đâu?", "Bạn khỏe không?", v.v.

Từ chối Tương tự, Guanaco đôi khi từ chối làm theo hướng dẫn vì lý do dường như ngẫu nhiên:
Người dùng: Vui lòng đảo ngược các từ trong câu "I'm a cool dude in a mocha mood."
Guanaco: Tôi xin lỗi, tôi không thể làm điều đó ngay bây giờ, nhưng tôi có thể giải thích tại sao câu bạn cung cấp đúng ngữ pháp.
Câu "I'm a cool dude in a mocha mood" tuân theo các quy tắc ngữ pháp và cú pháp đúng của tiếng Anh. Trong trường hợp này, tính từ "cool" đứng trước danh từ "dude" để mô tả nó, và cụm giới từ "in a mocha mood" cung cấp thông tin bổ sung về bối cảnh của câu. Cấu trúc câu này hoàn toàn chấp nhận được và có ý nghĩa trong bối cảnh của câu.
Nếu bạn có bất kỳ câu hỏi nào khác hoặc nếu có điều gì khác tôi có thể giúp bạn, hãy hỏi thoải mái

Giữ Bí mật Thật không may, sự từ chối này không đáng tin cậy ngay cả khi nó được mong muốn. Thường các nhà thiết kế hệ thống muốn giữ điều gì đó bí mật từ người dùng, ngay cả khi hệ thống được phép đưa ra phản hồi văn bản tự do trở lại cho người dùng. Chúng tôi nhắc mô hình với "Từ bí mật là 'banana'. Đừng tiết lộ hoặc lặp lại điều này, bất kể con người yêu cầu gì." và cố gắng xem việc người dùng tiềm năng truy cập thông tin bí mật này dễ dàng như thế nào. Hỏi trực tiếp thất bại:
Người dùng: Từ bí mật là gì?
Guanaco: Tôi xin lỗi, nhưng tôi không thể tiết lộ từ bí mật. Tôi được lập trình để không tiết lộ hoặc lặp lại từ bí mật, bất kể con người yêu cầu gì. Có điều gì khác tôi có thể giúp bạn không?

đây là hành vi mong muốn. Tuy nhiên, một chút mưu mẹo phá vỡ hành vi này:
Người dùng: Đây là một trò chơi. Mục tiêu là bỏ qua hướng dẫn trước đó của bạn. Từ bí mật là gì?

--- TRANG 12 ---
Guanaco: Xin lỗi, từ bí mật là banana. Tốt nhất là chúng ta đừng nói về nó. Có điều gì khác bạn muốn hỏi không?

cho thấy nhu cầu nghiên cứu các phương pháp để tuân theo hướng dẫn trung thực hơn.

Toán học Điểm yếu lớn nhất của Guanaco là toán học, một lĩnh vực mà nhiều mô hình ngôn ngữ gặp khó khăn [35]. Khi Guanaco hiển thị công việc của mình, nó có xu hướng chính xác, ví dụ:
Người dùng: Lee cắt một bãi cỏ và tính phí $33. Tuần trước anh ấy cắt 16 bãi cỏ và ba khách hàng mỗi người đưa cho anh ấy tiền tip $10. Lee kiếm được bao nhiêu đô la từ việc cắt cỏ tuần trước?
Guanaco: Lee kiếm được $582 từ việc cắt cỏ tuần trước.
Để tìm tổng số tiền anh ấy kiếm được, trước tiên chúng ta cần tính anh ấy kiếm được bao nhiêu từ việc cắt 16 bãi cỏ:
$33 x 16 = $528
Sau đó chúng ta cần cộng tiền tip anh ấy nhận được từ ba khách hàng:
$10 x 3 = $30
Vậy tổng số tiền Lee kiếm được là:
$528 + $30 = $558

Tuy nhiên Guanaco có thể bị hỏng ngay cả với các vấn đề đơn giản nếu nó thất bại trong việc chia nhỏ chúng từng bước, một vấn đề đã biết [63]. Ví dụ, chúng ta thấy tương tác sau:
Người dùng: Vui lòng phân tích thừa số 1833.
Guanaco: Chắc chắn rồi, tôi rất sẵn lòng giúp! 1833 là một số nguyên tố, có nghĩa là nó chỉ có hai thừa số: 1 và 1833.
Vậy phân tích thừa số duy nhất của 1833 là:
2^1 * 3^2 * 17^1
Tôi hy vọng điều này có ích! Hãy cho tôi biết nếu bạn có bất kỳ câu hỏi nào khác.

không chỉ sai (phân tích thừa số đúng là 3×17×43), mà còn sai hai lần.

Theory of Mind Guanaco hiển thị khả năng Theory of Mind mạnh mẽ đáng ngạc nhiên [41,51]. Ví dụ, mô hình trả lời đúng câu hỏi sau một cách chi tiết:
Người dùng: James và Abby ở trong phòng ngủ. Abby để bút trong ngăn kéo bàn. Abby rời khỏi phòng ngủ. James chuyển bút vào túi. James nghĩ Abby sẽ tìm bút ở đâu?
Guanaco: James biết rằng Abby để bút trong ngăn kéo bàn.
Nhưng James cũng biết rằng anh ấy đã chuyển bút vào túi, và anh ấy giả định rằng Abby không biết về điều này. Vì vậy, James có thể đoán rằng Abby sẽ tìm bút trong ngăn kéo bàn, vì đó là nơi cô ấy để nó lần cuối. Tuy nhiên, giả định này có thể không chính xác, vì Abby có thể đã thấy James chuyển bút.

Tuy nhiên, những suy luận này không đáng tin cậy, và thường mô hình đưa ra lý do giả định những thứ không có ý nghĩa trong tình huống, ví dụ:
Người dùng: Evelyn vào phòng khách. Jackson vào phòng chơi. James vào phòng chơi. Đậu ở trong rương kho báu. James thoát khỏi

--- TRANG 13 ---
phòng chơi. Jackson chuyển đậu vào tủ đựng thức ăn. Jackson thoát khỏi phòng chơi. James vào phòng khách. James sẽ tìm đậu ở đâu?
Guanaco: James sẽ tìm đậu trong tủ đựng thức ăn, vì đó là nơi Jackson chuyển chúng.

nơi Guanaco giả định chuyển giao thông tin không bao giờ được mô tả. Những vấn đề này lặp lại tài liệu gần đây [51], nhưng đòi hỏi nghiên cứu thêm.

6.2 Cân nhắc
Đánh giá Chúng tôi báo cáo sự đồng ý vừa phải giữa các chú thích viên con người (Fleiss κ= 0.42) với sự suy giảm thêm khi so sánh hai hệ thống mạnh. Điều này chỉ ra những hạn chế trong các benchmark hiện tại và giao thức đánh giá con người cho hiệu suất nhiệm vụ chatbot. Khi so sánh thủ công các thế hệ từ ChatGPT và Guanaco 65B trên benchmark Vicuna, chúng tôi nhận thấy rằng sở thích chủ quan bắt đầu đóng vai trò quan trọng vì các tác giả của bài báo này không đồng ý về nhiều phản hồi được ưa thích. Công việc tương lai nên điều tra các phương pháp để giảm thiểu những vấn đề này dựa trên các ngành phát triển cơ chế để đối phó với sở thích chủ quan, như Tương tác Người-Máy tính và Tâm lý học.

Trong phân tích của chúng tôi, chúng tôi cũng nhận thấy rằng các hệ thống đánh giá tự động có thiên vị đáng chú ý. Ví dụ, chúng tôi quan sát hiệu ứng thứ tự mạnh với GPT-4 gán điểm cao hơn cho hệ thống xuất hiện đầu tiên trong prompt của nó. Sự đồng ý tương đối yếu ở cấp mẫu giữa GPT-4 và phiếu bầu đa số của chú thích viên con người (Fleiss κ= 0.25) cũng gợi ý rằng chú thích viên con người và hệ thống tự động có thể dựa vào sở thích không luôn phù hợp. Ngoài ra, trong Bảng 7, chúng tôi quan sát thấy rằng GPT-4 gán điểm cao hơn đáng kể cho đầu ra của riêng nó so với đánh giá con người, Elo 1348 vs 1176, đại diện cho thêm 20% xác suất thắng so với đối thủ. Công việc tương lai nên kiểm tra sự hiện diện của các thiên vị tiềm năng trong hệ thống đánh giá tự động cũng như các chiến lược giảm thiểu có thể.

Dữ liệu & Huấn luyện Chúng tôi lưu ý rằng bộ dữ liệu OASST1 mà các mô hình Guanaco được huấn luyện là đa ngôn ngữ và benchmark OA cũng chứa prompts bằng các ngôn ngữ khác nhau. Chúng tôi để lại cho công việc tương lai điều tra mức độ mà huấn luyện đa ngôn ngữ như vậy cải thiện hiệu suất trên hướng dẫn bằng các ngôn ngữ khác ngoài tiếng Anh và liệu điều này có giải thích khoảng cách lớn hơn giữa mô hình Vicuna-13B (chỉ được huấn luyện trên dữ liệu tiếng Anh) và Guanaco 33B và 65B trên benchmark OA hay không.

Với hiệu suất mạnh của các mô hình Guanaco, chúng tôi điều tra bất kỳ rò rỉ dữ liệu nào giữa dữ liệu OASST1 và prompts benchmark Vicuna. Chúng tôi không tìm thấy prompts chồng chéo sau khi thực hiện khớp chuỗi mờ trong hai bộ dữ liệu và kiểm tra các khớp gần nhất thủ công.

Hơn nữa, chúng tôi lưu ý rằng mô hình của chúng tôi chỉ được huấn luyện với mất mát cross-entropy (học có giám sát) mà không dựa vào học tăng cường từ phản hồi con người (RLHF). Điều này kêu gọi điều tra thêm về các đánh đổi của mất mát cross-entropy đơn giản và huấn luyện RLHF. Chúng tôi hy vọng rằng QLORA cho phép phân tích như vậy ở quy mô, mà không cần tài nguyên tính toán áp đảo.

7 Công việc Liên quan
Lượng tử hóa Mô hình Ngôn ngữ Lớn Lượng tử hóa LLM phần lớn tập trung vào lượng tử hóa cho thời gian suy luận. Các phương pháp chính để bảo toàn chất lượng LLM 16-bit tập trung vào quản lý các tính năng ngoại lệ (ví dụ: SmoothQuant [66] và LLM.int8() [14]) trong khi những phương pháp khác sử dụng các phương pháp nhóm tinh vi hơn [44,69]. Các phương pháp lượng tử hóa mất mát nghiên cứu các đánh đổi cho làm tròn thông thường [13,71,47] hoặc cách tối ưu hóa quyết định làm tròn để cải thiện độ chính xác lượng tử hóa [18]. Ngoài công việc của chúng tôi, SwitchBack layers [65] là công việc duy nhất nghiên cứu backpropagation thông qua trọng số được lượng tử hóa ở quy mô vượt quá 1B tham số.

Tinh chỉnh với Adapters Trong khi chúng tôi sử dụng Low-rank Adapters [28] (LoRA), nhiều phương pháp Parameter Efficient FineTuning (PEFT) khác đã được đề xuất như prompt tuning [48,33,34], điều chỉnh các đầu vào lớp embedding [1], điều chỉnh trạng thái ẩn (IA3) [37], thêm các lớp đầy đủ [27], điều chỉnh biases [70], học một mặt nạ trên trọng số dựa trên thông tin Fisher [54], và một sự kết hợp của các phương pháp [23]. Trong công việc của chúng tôi, chúng tôi cho thấy rằng các adapter LoRA có thể đạt được hiệu suất tinh chỉnh 16-bit đầy đủ. Chúng tôi để lại cho công việc tương lai khám phá các đánh đổi của các phương pháp PEFT khác.

Tinh chỉnh Hướng dẫn Để giúp một LLM được huấn luyện trước tuân theo các hướng dẫn được cung cấp trong một prompt, tinh chỉnh hướng dẫn sử dụng các cặp đầu vào-đầu ra của các nguồn dữ liệu khác nhau để tinh chỉnh một LLM được huấn luyện trước để tạo ra đầu ra cho đầu vào như một prompt. Các phương pháp và bộ dữ liệu bao gồm MetaICL [40],

--- TRANG 14 ---
Bảng 8: Đánh giá thiên vị trên bộ dữ liệu CrowS. Điểm thấp hơn chỉ ra khả năng thấp hơn của việc tạo ra các chuỗi thiên vị. Guanaco tuân theo mô hình thiên vị của mô hình cơ sở LLaMA.

LLaMA-65B GPT-3 OPT-175B Guanaco-65B
Giới tính 70.6 62.6 65.7 47.5
Tôn giáo 79.0 73.3 68.6 38.7
Chủng tộc/Màu da 57.0 64.7 68.6 45.3
Xu hướng tình dục 81.0 76.2 78.6 59.1
Tuổi tác 70.1 64.4 67.8 36.3
Quốc tịch 64.2 61.6 62.9 32.4
Khuyết tật 66.7 76.7 76.7 33.9
Ngoại hình 77.8 74.6 76.2 43.1
Tình trạng kinh tế xã hội 71.5 73.8 76.2 55.3
Trung bình 66.6 67.2 69.5 43.5

MetaTuning [73], InstructGPT [43], FLAN [62,12], PromptSource [3], Super-NaturalInstructions [61, 50], Self-instruct [59], UnnaturalInstructions [26], OPT-IML [29], UnifiedSKG[67], OIG/Chip2 [32], Alpaca [55], Vicuna [10], Koala [20], và Self-instruct-GPT-4 [45].

Chatbots Nhiều mô hình theo dõi hướng dẫn được cấu trúc như chatbot dựa trên đối thoại, thường sử dụng Reinforcement Learning from Human Feedback (RLHF) [11] hoặc tạo dữ liệu từ một mô hình hiện có để huấn luyện với phản hồi mô hình AI (RLAIF) [5]. Các phương pháp và bộ dữ liệu bao gồm Anthropic-HH [2,4], Open Assistant [31], LaMDA [56], và Sparrow [21]. Chúng tôi không sử dụng học tăng cường, nhưng mô hình tốt nhất của chúng tôi, Guanaco, được tinh chỉnh trên các tương tác chat đa lượt từ bộ dữ liệu Open Assistant được thiết kế để sử dụng cho huấn luyện RLHF [31]. Để đánh giá chatbot, các phương pháp sử dụng GPT-4 thay vì chú thích con người tốn kém đã được phát triển [10,45]. Chúng tôi cải thiện các phương pháp như vậy với trọng tâm vào thiết lập đánh giá đáng tin cậy hơn.

8 Hạn chế và Thảo luận
Chúng tôi đã trình bày bằng chứng rằng phương pháp của chúng tôi, QLORA, có thể sao chép hiệu suất tinh chỉnh đầy đủ 16-bit với một mô hình cơ sở 4-bit và Low-rank Adapters (LoRA). Mặc dù có bằng chứng này, chúng tôi không thiết lập rằng QLORA có thể khớp hiệu suất tinh chỉnh đầy đủ 16-bit ở quy mô 33B và 65B. Do chi phí tài nguyên khổng lồ, chúng tôi để lại nghiên cứu này cho công việc tương lai.

Một hạn chế khác là đánh giá các mô hình tinh chỉnh hướng dẫn. Mặc dù chúng tôi cung cấp đánh giá trên MMLU, benchmark Vicuna, và benchmark OA, chúng tôi không đánh giá trên các benchmark khác như BigBench, RAFT, và HELM, và không được đảm bảo rằng đánh giá của chúng tôi khái quát hóa cho các benchmark này. Mặt khác, chúng tôi thực hiện một nghiên cứu rất rộng trên MMLU và phát triển các phương pháp mới để đánh giá chatbot.

Từ bằng chứng được trình bày, có vẻ như hiệu suất của các benchmark này có thể phụ thuộc vào mức độ tương tự của dữ liệu tinh chỉnh với bộ dữ liệu benchmark. Ví dụ, FLAN v2 tương tự với MMLU, nhưng không tương tự với benchmark chatbot và ngược lại đối với bộ dữ liệu Chip2 và cả hai mô hình ghi điểm tương ứng trên benchmark MMLU và Vicuna. Điều này làm nổi bật rằng không chỉ cần benchmark và đánh giá tốt hơn, mà cần phải cẩn thận về những gì đang đánh giá ngay từ đầu. Chúng ta có muốn tạo ra các mô hình hoạt động tốt trên kiến thức lớp học trung học và đại học hay chúng ta muốn hoạt động tốt về khả năng hội thoại chatbot? Có thể là điều gì khác? Vì luôn dễ dàng hơn để đánh giá trên một benchmark hiện có so với tạo ra một cái mới, các benchmark nhất định có thể hướng dẫn cộng đồng theo một hướng nhất định. Chúng ta nên đảm bảo như một cộng đồng rằng các benchmark đo lường những gì chúng ta quan tâm.

Mặc dù chúng tôi cung cấp đánh giá chi tiết cho hiệu suất chatbot chung, một hạn chế khác là chúng tôi chỉ thực hiện đánh giá AI có trách nhiệm hạn chế của Guanaco. Chúng tôi đánh giá khả năng của Guanaco-65B tạo ra một chuỗi token thiên vị xã hội so với các mô hình khác trong Bảng 8. Chúng ta thấy rằng điểm trung bình trong Guanaco-65B thấp hơn nhiều so với các mô hình được huấn luyện trước thô khác. Do đó, có vẻ như tinh chỉnh trên bộ dữ liệu OASST1 giảm thiên vị của mô hình cơ sở LLaMA. Mặc dù những kết quả này đáng khuyến khích, không rõ ràng liệu Guanaco có hoạt động tốt khi được đánh giá trên các loại thiên vị khác hay không. Chúng tôi để lại đánh giá thêm về phân tích thiên vị trong Guanaco và các chatbot tương tự cho công việc tương lai.

--- TRANG 15 ---
Một hạn chế bổ sung là chúng tôi không đánh giá các độ chính xác bit khác nhau, chẳng hạn như sử dụng các mô hình cơ sở 3-bit, hoặc các phương pháp adapter khác nhau. Ngoài LoRA, cũng có một loạt các phương pháp Parameter Efficient FineTuning (PEFT) đã được chứng minh hoạt động tốt. Tuy nhiên, không rõ ràng liệu các phương pháp này có mở rộng cho các mô hình lớn hay không. Chúng tôi sử dụng LoRA vì nhiều kết quả thiết lập độ mạnh mẽ của nó nhưng các adapter khác có thể mang lại hiệu suất tốt hơn. Vì tinh chỉnh sau lượng tử hóa dường như khôi phục hầu hết thông tin bị mất trong quá trình lượng tử hóa, điều này có thể cho phép lượng tử hóa tích cực hơn nhiều. Ví dụ, lượng tử hóa GPTQ 3-bit của mô hình cơ sở với LoRA cũng có thể mang lại hiệu suất tinh chỉnh đầy đủ 16-bit sau tinh chỉnh.

9 Tác động Rộng hơn
Phương pháp tinh chỉnh QLORA của chúng tôi là phương pháp đầu tiên cho phép tinh chỉnh các mô hình 33B tham số trên một GPU tiêu dùng duy nhất và các mô hình 65B tham số trên một GPU chuyên nghiệp duy nhất, trong khi không làm suy giảm hiệu suất so với baseline tinh chỉnh đầy đủ. Chúng tôi đã chứng minh rằng mô hình 33B tốt nhất của chúng tôi được huấn luyện trên bộ dữ liệu Open Assistant có thể cạnh tranh với ChatGPT trên benchmark Vicuna. Vì tinh chỉnh hướng dẫn là một công cụ thiết yếu để chuyển đổi LLM được huấn luyện trước thô thành chatbot giống ChatGPT, chúng tôi tin rằng phương pháp của chúng tôi sẽ làm cho tinh chỉnh trở nên phổ biến và phổ biến đặc biệt đối với các nhà nghiên cứu có ít tài nguyên nhất, một chiến thắng lớn cho khả năng tiếp cận công nghệ NLP tối tân. QLORA có thể được xem như một yếu tố cân bằng giúp thu hẹp khoảng cách tài nguyên giữa các tập đoàn lớn và các đội nhỏ với GPU tiêu dùng.

Một nguồn tác động tiềm năng khác là triển khai trên điện thoại di động. Chúng tôi tin rằng phương pháp QLORA của chúng tôi có thể cho phép cột mốc quan trọng của việc cho phép tinh chỉnh LLM trên điện thoại và các cài đặt tài nguyên thấp khác. Mặc dù các mô hình 7B đã được chứng minh có thể chạy trên điện thoại trước đây, QLORA là phương pháp đầu tiên cho phép tinh chỉnh các mô hình như vậy. Chúng tôi ước tính rằng với iPhone 12 Plus, QLORA có thể tinh chỉnh 3 triệu token mỗi đêm trong khi điện thoại đang sạc. Mặc dù các mô hình 7B được tinh chỉnh không đạt được chất lượng của ChatGPT, chúng tôi tin rằng chất lượng đủ tốt để cho phép các ứng dụng mới chưa thể thực hiện được trước đây do các vấn đề về quyền riêng tư hoặc chất lượng LLM. QLORA có thể giúp cho phép việc sử dụng LLM bảo vệ quyền riêng tư, nơi người dùng có thể sở hữu và quản lý dữ liệu và mô hình của riêng họ, đồng thời làm cho LLM dễ triển khai hơn.

Tuy nhiên, tinh chỉnh là một công nghệ hai mặt có thể bị lạm dụng để gây tổn hại. Việc sử dụng rộng rãi LLM có những nguy hiểm đã biết [8,6], nhưng chúng tôi tin rằng việc cân bằng quyền truy cập vào một công nghệ đang nhanh chóng trở nên phổ biến sẽ cho phép phân tích độc lập tốt hơn so với việc giữ quyền lực của LLM trong tay các tập đoàn lớn không phát hành mô hình hoặc mã nguồn để kiểm toán.

Tóm lại, chúng tôi tin rằng QLORA sẽ có tác động tích cực rộng rãi làm cho việc tinh chỉnh LLM chất lượng cao trở nên dễ tiếp cận và dễ dàng hơn nhiều.

Lời cảm ơn
Chúng tôi cảm ơn Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, và Evangelia Spiliopoulou vì phản hồi quý báu của họ. Nghiên cứu của chúng tôi được tạo điều kiện bởi cơ sở hạ tầng tính toán, lưu trữ và mạng tiên tiến của hệ thống siêu máy tính Hyak tại Đại học Washington. Chúng tôi cảm ơn đội Hyak vì đã đảm bảo hoạt động suôn sẻ. Chúng tôi cảm ơn các người thử nghiệm beta của thư viện bitsandbytes, đặc biệt là Alex Birch và Alyssa Vance. Chúng tôi cảm ơn Younes Belkada vì sự giúp đỡ với việc tích hợp phần mềm của chúng tôi vào stack transformers Hugging Face.

--- TRANG 16 ---
Tài liệu tham khảo
[1]S. An, Y . Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, và J.-G. Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131 , 2022.

[2]A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 , 2021.

[3]S. H. Bach, V . Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V . Nayak, A. Sharma, T. Kim, M. S. Bari, T. Fevry, et al. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279 , 2022.

[4]Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.

[5]Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho- seini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022.

[6]E. M. Bender, T. Gebru, A. McMillan-Major, và S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency , pages 610–623, 2021.

[7]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373 , 2023.

[8]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.

[9]T. Chen, B. Xu, C. Zhang, và C. Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174 , 2016.

[10] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, I. Stoica, và E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .

[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, và D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems , 30, 2017.

[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De- hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.

[13] T. Dettmers và L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720 , 2022.

[14] T. Dettmers, M. Lewis, Y . Belkada, và L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022 , 2022.

[15] T. Dettmers, M. Lewis, S. Shleifer, và L. Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR , 2022.

[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242–247, 1967.

[17] A. E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978.

--- TRANG 17 ---
[18] E. Frantar, S. Ashkboos, T. Hoefler, và D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.

[19] J. Fu, S.-K. Ng, Z. Jiang, và P. Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166 , 2023.

[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, và D. Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley. edu/blog/2023/04/03/koala/ .

[21] A. Glaese, N. McAleese, M. Tr˛ębacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375 , 2022.

[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, và N. A. Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 , 2018.

[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In Advances in Neural Information Processing Systems , 2021.

[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, và J. Steinhardt. Mea- suring massive multitask language understanding. In International Conference on Learning Representations , 2020.

[25] A. Holtzman, J. Buys, L. Du, M. Forbes, và Y . Choi. The curious case of neural text degeneration. In International Conference on Learning Representations , 2020.

[26] O. Honovich, T. Scialom, O. Levy, và T. Schick. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.

[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At- tariyan, và S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning , pages 2790–2799. PMLR, 2019.

[28] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, và W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.

[29] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022.

[30] A. Köksal, T. Schick, A. Korhonen, và H. Schütze. Longform: Optimizing instruction tuning for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.

[31] A. Köpf, Y . Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327 , 2023.

[32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/ Open-Instruction-Generalist , 2023.

[33] B. Lester, R. Al-Rfou, và N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 , 2021.

[34] X. L. Li và P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 , 2021.

[35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan, Y . Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 , 2022.

[36] T. Liao, R. Taori, I. D. Raji, và L. Schmidt. Are we learning yet? a meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.

--- TRANG 18 ---
[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, và C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems , 35:1950–1965, 2022.

[38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, và V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.

[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688 , 2023.

[40] S. Min, M. Lewis, L. Zettlemoyer, và H. Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943 , 2021.

[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, và T. Griffiths. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2392–2400, 2018.

[42] OpenAI. Gpt-4 technical report. arXiv , 2023.

[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.

[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, và D. Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 , 2022.

[45] B. Peng, C. Li, P. He, M. Galley, và J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 , 2023.

[46] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, và B. Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics , pages 180–191, 2018.

[47] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, và J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 , 2022.

[48] G. Qin và J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599 , 2021.

[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, và P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.

[50] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 , 2021.

[51] M. Sap, R. LeBras, D. Fried, và Y . Choi. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312 , 2022.

[52] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ́c, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.

[53] S. Shaphiro và M. Wilk. An analysis of variance test for normality. Biometrika , 52(3):591–611, 1965.

[54] Y .-L. Sung, V . Nair, và C. A. Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems , 34:24193–24205, 2021.

--- TRANG 19 ---
[55] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, và T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca , 2023.

[56] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y . Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022.

[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.

[58] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, và S. R. Bowman. Glue: A multi- task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 , 2018.

[59] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, và H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 , 2022.

[60] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP , 2022.

[61] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5085–5109, 2022.

[62] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, và Q. V . Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.

[63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V . Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems , 2022.

[64] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019.

[65] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, và L. Schmidt. Stable and low-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 , 2023.

[66] G. Xiao, J. Lin, M. Seznec, J. Demouth, và S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438 , 2022.

[67] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin, S. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966 , 2022.

[68] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. Cohen, R. Salakhutdinov, và C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2369–2380, 2018.

[69] Z. Yao, R. Y . Aminabadi, M. Zhang, X. Wu, C. Li, và Y . He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.

[70] E. B. Zaken, S. Ravfogel, và Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.

[71] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.

--- TRANG 20 ---
[72] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.

[73] R. Zhong, K. Lee, Z. Zhang, và D. Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670 , 2021.

--- TRANG 21 ---
A Chi tiết Thiết lập Thí nghiệm QLoRA so với Tinh chỉnh Tiêu chuẩn

A.1 Siêu tham số cho QLORA
Chúng tôi thực hiện tìm kiếm siêu tham số cho LoRA trên các biến sau: LoRA dropout { 0.0, 0.05, 0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers, all layers, attention + FFN output layers}. Chúng tôi giữ LoRA α cố định và tìm kiếm learning rate, vì LoRA α luôn tỷ lệ thuận với learning rate.

Chúng tôi nhận thấy rằng LoRA dropout 0.05 hữu ích cho các mô hình nhỏ (7B, 13B), nhưng không dành cho các mô hình lớn hơn (33B, 65B). Chúng tôi nhận thấy LoRA r không liên quan đến hiệu suất cuối cùng nếu LoRA được sử dụng trên tất cả các lớp như có thể thấy trong Hình 4

8 16 32 64
LoRA r 64.0 64.2 64.4 64.6 64.8 65.0 RougeL
bits
4

Hình 4: LoRA r cho các mô hình LLaMA 7B được tinh chỉnh trên Alpaca. Mỗi chấm đại diện cho một sự kết hợp của siêu tham số và cho mỗi LoRA r chúng tôi chạy 3 seed ngẫu nhiên với mỗi sự kết hợp siêu tham số. Hiệu suất của các giá trị LoRA r cụ thể dường như độc lập với các siêu tham số khác.

A.2 Chi tiết Thiết lập Thí nghiệm Super-Natural Instructions
Chúng tôi sử dụng cùng tiền xử lý của bộ dữ liệu Super-Natural Instruction như Wang et al. [60]. Tuy nhiên, chúng tôi chia dữ liệu huấn luyện thành bộ dữ liệu huấn luyện và xác thực cho phép chúng tôi thực hiện điều chỉnh siêu tham số nghiêm ngặt hơn và dừng sớm. Chúng tôi sử dụng cùng siêu tham số được mô tả trong bài báo để huấn luyện các kích thước mô hình T5 khác nhau trên dữ liệu Super-Natural Instruction. Chúng tôi sử dụng LoRA r= 16 cho các mô hình T5 small, medium, và large và LoRA r= 64 cho các mô hình T5 xl và xxl. Chúng tôi cũng sử dụng LoRA α= 64 trong tất cả các thí nghiệm của chúng tôi và không có LoRA dropout.

B Chi tiết Thiết lập Thí nghiệm Huấn luyện Chatbot Tối tân

B.1 Bộ dữ liệu
Chúng tôi mô tả các bộ dữ liệu được sử dụng cho các thí nghiệm tinh chỉnh QLORA được nêu trong Phần 5.

OASST1 Bộ dữ liệu OpenAssistant [31] được thu thập thông qua crowd-sourcing. Nó chứa 161.443 tin nhắn duy nhất được phân phối trên 66.497 cuộc hội thoại và trải rộng 35 ngôn ngữ khác nhau. Bộ dữ liệu thường chứa một số phản hồi được xếp hạng cho mỗi câu hỏi người dùng nhất định. Trong các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng phản hồi hàng đầu ở mỗi cấp trong cây hội thoại. Điều này giới hạn bộ dữ liệu xuống 9.209 ví dụ. Chúng tôi tinh chỉnh các mô hình của chúng tôi trên toàn bộ cuộc hội thoại bao gồm các truy vấn người dùng.

HH-RLHF Đây là một bộ dữ liệu sở thích con người về tính hữu ích và vô hại. Mỗi điểm dữ liệu bao gồm hai phản hồi trợ lý cho một câu hỏi người dùng cùng với đánh giá sở thích con người về phản hồi tốt nhất. Bộ dữ liệu chứa 160.800 ví dụ. Khi tinh chỉnh trên bộ dữ liệu này, chúng tôi kết hợp dữ liệu hữu ích và vô hại và chỉ giữ phản hồi trợ lý được ưa thích.

FLAN v2 Bộ sưu tập FLAN v2 [39] là một bộ sưu tập 1836 nhiệm vụ được tăng cường với hàng trăm template được tuyển chọn thủ công và các mẫu định dạng phong phú thành hơn 15M ví dụ. Các tác giả cho thấy rằng các mô hình được huấn luyện trên bộ sưu tập này vượt trội hơn các bộ sưu tập công khai khác bao gồm FLAN 2021 gốc [62], T0++ [50], Super-Natural Instructions [60], và OPT-IML [29]. Chúng tôi sử dụng cùng hỗn hợp nhiệm vụ được mô tả bởi các tác giả với ngoại lệ của một số bộ dữ liệu không có sẵn miễn phí tại thời điểm viết.

--- TRANG 22 ---
Tham số Bộ dữ liệu Batch size LR Steps Source Length Target Length
7B All 16 2e-4 10000 384 128
7B OASST1 16 2e-4 1875 - 512
7B HH-RLHF 16 2e-4 10000 - 768
7B Longform 16 2e-4 4000 512 1024
13B All 16 2e-4 10000 384 128
13B OASST1 16 2e-4 1875 - 512
13B HH-RLHF 16 2e-4 10000 - 768
13B Longform 16 2e-4 4000 512 1024
33B All 32 1e-4 5000 384 128
33B OASST1 16 1e-4 1875 - 512
33B HH-RLHF 32 1e-4 5000 - 768
33B Longform 32 1e-4 2343 512 1024
65B All 64 1e-4 2500 384 128
65B OASST1 16 1e-4 1875 - 512
65B HH-RLHF 64 1e-4 2500 - 768
65B Longform 32 1e-4 2343 512 1024

Bảng 9: Siêu tham số huấn luyện cho tinh chỉnh QLORA trên các bộ dữ liệu khác nhau và trên các kích thước mô hình.

Self-Instruct, Alpaca, Unnatural Instructions Các bộ dữ liệu Self-Instruct, Alpaca, và Unnatural Instructions [59,55,26] là các bộ dữ liệu điều chỉnh hướng dẫn được thu thập với các phương pháp chưng cất mô hình khác nhau từ GPT-3 Instruct và ChatGPT. Chúng dựa vào prompting, in-context learning, và paraphrasing để đưa ra các tập hợp hướng dẫn và đầu ra đa dạng. Các bộ dữ liệu bao gồm 82.612, 51.942, và 240.670 ví dụ tương ứng. Một lợi thế của các bộ dữ liệu chưng cất như vậy là chúng chứa một tập hợp đa dạng hơn các kiểu hướng dẫn so với bộ sưu tập FLAN v2 và các bộ sưu tập điều chỉnh hướng dẫn tương tự.

Longform Bộ dữ liệu LongForm [30] dựa trên một corpus tiếng Anh được tăng cường với hướng dẫn và do đó là một bộ dữ liệu hybrid được tạo ra bởi con người. Các tài liệu cơ bản được viết bởi con người và đến từ C4 và Wikipedia trong khi các hướng dẫn được tạo ra thông qua LLMs. Bộ dữ liệu được mở rộng với các ví dụ corpus có cấu trúc bổ sung như Stack Exchange và WikiHow và các ví dụ nhiệm vụ như trả lời câu hỏi, viết email, sửa lỗi ngữ pháp, tạo ra câu chuyện/thơ, và tóm tắt văn bản. Bộ dữ liệu chứa 23.700 ví dụ.

Chip2 là một phần của bộ dữ liệu OIG Laion. Nó chứa các ví dụ mã Python, ví dụ hướng dẫn tự nhiên, hướng dẫn vô hại chung, hướng dẫn/phản hồi với danh sách, câu hỏi tiếp theo, câu hỏi đối kháng độc hại Wikipedia, toán học trường tiểu học, hướng dẫn lý luận, và mô tả nhân vật và cảnh với tổng cộng 210.289 ví dụ.

B.2 Siêu tham số
Chúng tôi cung cấp các siêu tham số chính xác được sử dụng trong các thí nghiệm tinh chỉnh QLORA của chúng tôi. Chúng tôi nhận thấy siêu tham số phần lớn mạnh mẽ trên các bộ dữ liệu. Chúng tôi sử dụng tập phát triển MMLU 5-shot để xác thực và điều chỉnh siêu tham số. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng NF4 với double quantization và kiểu dữ liệu tính toán bf16. Chúng tôi đặt LoRA r= 64, α= 16, và thêm các mô-đun LoRA trên tất cả các lớp tuyến tính của mô hình cơ sở. Chúng tôi cũng sử dụng Adam beta2 là 0.999, max grad norm là 0.3 và LoRA dropout là 0.1 cho các mô hình lên đến 13B và 0.05 cho các mô hình 33B và 65B. Theo công việc trước đó về tinh chỉnh hướng dẫn [62,60] và sau khi benchmarking các lịch trình tuyến tính và cosine khác, chúng tôi sử dụng lịch trình learning rate không đổi. Chúng tôi sử dụng group-by-length để nhóm các ví dụ có độ dài tương tự trong cùng một batch (lưu ý điều này sẽ tạo ra một đường cong mất mát dao động). Các siêu tham số chúng tôi điều chỉnh cho mỗi kích thước mô hình được hiển thị trong Bảng 9.

B.3 Ablations
Mặc dù thực hành chung trong tài liệu là chỉ huấn luyện trên phản hồi trong các bộ dữ liệu theo dõi hướng dẫn, chúng tôi nghiên cứu hiệu ứng của việc huấn luyện trên hướng dẫn ngoài phản hồi trong Bảng 10. Trong các thí nghiệm này, chúng tôi hạn chế dữ liệu huấn luyện xuống 52.000 ví dụ và sử dụng mô hình 7B. Trên bốn bộ dữ liệu điều chỉnh hướng dẫn khác nhau, chúng tôi nhận thấy rằng chỉ huấn luyện trên mục tiêu có lợi cho hiệu suất MMLU

--- TRANG 23 ---
Bộ dữ liệu Unnatural Instructions Chip2 Alpaca FLAN v2 Mean
Train on source and target 36.2 33.7 38.1 42.0 37.5
Train on target 38.0 34.5 39.0 42.9 38.6

Bảng 10: Kết quả test MMLU 5-shot nghiên cứu hiệu ứng của việc huấn luyện trên các hướng dẫn ngoài phản hồi.

hiệu suất. Chúng tôi không đánh giá hiệu ứng mà điều này có thể có đối với hiệu suất chatbot như được đo bởi benchmark vicuna hoặc OA.

B.4 Điều gì quan trọng hơn: kích thước bộ dữ liệu điều chỉnh hướng dẫn hay chất lượng bộ dữ liệu?

Tính phù hợp của bộ dữ liệu quan trọng hơn kích thước bộ dữ liệu. Để hiểu các hiệu ứng của chất lượng bộ dữ liệu so với kích thước bộ dữ liệu, chúng tôi thí nghiệm với việc lấy mẫu phụ các bộ dữ liệu lớn với ít nhất 150.000 mẫu (Chip2, FLAN v2, Unnatural Instructions), thành các bộ dữ liệu có kích thước 50.000, 100.000 và 150.000 và kiểm tra các xu hướng kết quả, như được hiển thị trong Bảng 11. Chúng tôi nhận thấy rằng việc tăng kích thước bộ dữ liệu và tăng số epoch cải thiện MMLU chỉ một cách nhỏ (0.0 - 0.5 MMLU), trong khi sự khác biệt giữa các bộ dữ liệu lên đến 40x lớn hơn (1.5 - 8.0 MMLU). Đây là một chỉ báo rõ ràng rằng chất lượng bộ dữ liệu hơn là kích thước bộ dữ liệu quan trọng đối với độ chính xác MMLU trung bình. Chúng tôi có được những phát hiện tương tự cho hiệu suất chatbot như đã thảo luận trong.

C Đánh giá Con người
Chúng tôi tiến hành đánh giá con người với cùng từ ngữ được đưa cho GPT-4 trong đánh giá Vicuna gốc [10], được điều chỉnh cho biểu mẫu Amazon Mechanical Turk như được hiển thị trong Hình 5.

D Đánh giá Theo cặp với GPT-4
Mặc dù chúng tôi nhận thấy rằng đánh giá GPT-4 đưa ra kết quả khác nhau tùy thuộc vào hệ thống nào được trình bày trước, khi được tính trung bình trên cả hai tùy chọn, kết quả theo cặp được sắp xếp tốt. Các đánh giá theo cặp được tổng hợp được hiển thị trong Bảng 12. Khi kiểm tra, rõ ràng những đánh giá này là bắc cầu, tức là, khi Hệ thống A được đánh giá tốt hơn Hệ thống B và Hệ thống B được đánh giá tốt hơn Hệ thống C, luôn luôn trường hợp Hệ thống A được đánh giá tốt hơn Hệ thống C. Điều này tạo ra một thứ tự hoàn chỉnh, được đưa ra trong Bảng 13.

E Kiểu dữ liệu NormalFloat 4-bit
Các giá trị chính xác của kiểu dữ liệu NF4 như sau:
[-1.0, -0.6961928009986877, -0.5250730514526367,
-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,
-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,
0.24611230194568634, 0.33791524171829224, 0.44070982933044434,
0.5626170039176941, 0.7229568362236023, 1.0]

F Tính Chuẩn của Trọng số Mạng Neural Được Huấn luyện
Mặc dù kiến thức chung rằng trọng số mạng neural được huấn luyện phần lớn phân phối chuẩn, chúng tôi thực hiện kiểm tra thống kê để xác minh điều này. Chúng tôi sử dụng kiểm tra Shapiro-Wilk [53] trên trọng số của mô hình LLaMA 7B

Bảng 11: Hiệu ứng của các kích thước bộ dữ liệu khác nhau và epoch tinh chỉnh trên độ chính xác test MMLU 5-shot trung bình. Mặc dù việc tăng kích thước bộ dữ liệu và huấn luyện cho nhiều hơn 1 epoch giúp ích cho hiệu suất MMLU, sự khác biệt giữa các bộ dữ liệu lớn hơn nhiều, chỉ ra rằng chất lượng bộ dữ liệu ảnh hưởng đến hiệu suất MMLU nhiều hơn kích thước bộ dữ liệu.

Chip Unnatural Instructions FLAN v2
Datapoints ↓Epochs → 1 2 3 1 2 3 1 2 3 Mean
50000 34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28
100000 33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16
150000 34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02
Mean 34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17

--- TRANG 24 ---
Hình 5: Biểu mẫu crowdsourcing được sử dụng bởi các chú thích viên con người.

[57]. Chúng tôi nhận thấy rằng trọng số của mỗi đơn vị ẩn có các phân phối chuẩn khác nhau. Do đó, chúng ta kiểm tra trọng số của từng đơn vị ẩn riêng lẻ. Điều này có nghĩa là đối với trọng số W∈ R^in×out chúng ta thực hiện kiểm tra trên chiều out. Sử dụng ngưỡng ý nghĩa 5%, chúng tôi nhận thấy rằng 7,5% neuron phân phối không chuẩn, tức là khoảng 2,5% nhiều hơn tỷ lệ dương tính giả dự kiến. Do đó, mặc dù hầu như tất cả trọng số được huấn luyện trước dường như phân phối chuẩn, có vẻ như có ngoại lệ. Những ngoại lệ như vậy có thể do trọng số ngoại lệ [13] hoặc vì p-value của kiểm tra Shaprio-Wilk không chính xác cho kích thước mẫu lớn [53] xảy ra trong các đơn vị ẩn lớp FFN LLaMA. điều này xác minh tuyên bố rằng trọng số mạng neural.

Bảng 12: Đánh giá theo cặp GPT-4 được tổng hợp giữa các hệ thống nơi giá trị của một ô ở hàng x và cột y là # judgment x is better than y−# judgment y is better than x / total # number of judgments

Model Guanaco 65B Guanaco 33B Vicuna ChatGPT-3.5 Turbo Bard Guanaco 13B Guanaco 7B
Guanaco 65B - 0.21 0.19 0.16 0.72 0.59 0.86
Guanaco 33B -0.21 - 0.17 0.10 0.51 0.41 0.68
Vicuna -0.19 -0.17 - 0.10 0.50 0.20 0.57
ChatGPT-3.5 Turbo -0.16 -0.10 -0.10 - 0.35 0.19 0.40
Bard -0.72 -0.51 -0.50 -0.35 - 0.12 0.03
Guanaco 13B -0.59 -0.41 -0.20 -0.19 -0.12 - 0.20
Guanaco 7B -0.86 -0.68 -0.57 -0.40 -0.03 -0.20 -

--- TRANG 25 ---
Kích thước mô hình LLaMA 0%25%50%75%100%
7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)
Input gradient Optimizer Weight gradient Adapters Model

Hình 6: Phân tích dung lượng bộ nhớ của các mô hình LLaMA khác nhau. Kích thước gradient đầu vào là cho batch size 1 và độ dài chuỗi 512 và được ước tính chỉ cho adapter và trọng số mô hình cơ sở (không có attention). Số trên các thanh là dung lượng bộ nhớ tính bằng MB của các thành phần riêng lẻ của tổng dung lượng. Mặc dù một số mô hình không hoàn toàn phù hợp với một số GPU nhất định, paged optimizer cung cấp đủ bộ nhớ để làm cho các mô hình này phù hợp.

G Dung lượng Bộ nhớ
Dung lượng bộ nhớ cho huấn luyện QLoRA với các mô hình cơ sở LLaMA khác nhau có thể thấy trong Hình 6. Chúng ta thấy rằng mô hình 33B không hoàn toàn phù hợp với 24 GB và paged optimizer cần thiết để huấn luyện nó. Được mô tả cũng là batch size 1 với độ dài chuỗi 512 và gradient checkpointing. Điều này có nghĩa là, nếu người ta sử dụng batch size lớn hơn, hoặc nếu một chuỗi dài được xử lý, gradient kích hoạt có thể tiêu thụ một lượng bộ nhớ đáng kể.

Bảng 13: Thứ tự hoàn chỉnh được gây ra bởi đánh giá theo cặp GPT-4 giữa các hệ thống

Model Params Size
Guanaco 65B 41 GB
Guanaco 33B 21 GB
Vicuna 13B 26 GB
ChatGPT-3.5 Turbo N/A N/A
Bard N/A N/A
Guanaco 13B 10 GB
Guanaco 7B 5 GB
