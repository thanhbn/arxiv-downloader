# 2303.06233.pdf
# Converted from PDF to TXT
# Source path: ./2303.06233.pdf
# File size: 3519968 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Model-Agnostic Syntactical Information for
Pre-Trained Programming Language Models
Iman Saberi, Fatemeh H. Fardy
Department of Computer Science, Mathematics, Physics and Statistics
The University of British Columbia
Kelowna, Canada
Email:iman.saberi@ubc.ca,yfatemeh.fard@ubc.ca
Abstract ‚ÄîPre-trained Programming Language Models
(PPLMs) achieved many recent states of the art results for
many code-related software engineering tasks. Though some
studies use data Ô¨Çow or propose tree-based models that utilize
Abstract Syntax Tree (AST), most PPLMs do not fully utilize
the rich syntactical information in source code. Still, the input is
considered a sequence of tokens. There are two issues; the Ô¨Årst
is computational inefÔ¨Åciency due to the quadratic relationship
between input length and attention complexity. Second, any
syntactical information, when needed as an extra input to the
current PPLMs, requires the model to be pre-trained from
scratch, wasting all the computational resources already used
for pre-training the current models. In this work, we propose
Named Entity Recognition (NER) adapters , lightweight modules
that can be inserted into Transformer blocks to learn type
information extracted from the AST. These adapters can be used
with current PPLMs such as CodeBERT, GraphCodeBERT,
and CodeT5. We train the NER adapters using a novel Token
Type ClassiÔ¨Åcation objective function (TTC) . We insert our
proposed work in CodeBERT, building CodeBERTER , and
evaluate the performance on two tasks of code reÔ¨Ånement and
code summarization. CodeBERTER improves the accuracy of
code reÔ¨Ånement from 16.4 to 17.8 while using 20% of training
parameter budget compared to the fully Ô¨Åne-tuning approach,
and the BLEU score of code summarization from 14.75 to 15.90
while reducing 77% of training parameters compared to the
fully Ô¨Åne-tuning approach.
Index Terms ‚ÄîAdapters, Pre-trained Programming Language
Models
I. I NTRODUCTION
The success of Pre-trained Language Models (PLM) in
Natural Language Processing (NLP) led to the emergence of
pre-trained models in software engineering, such as Code-
BERT [1], CuBERT [2], and CodeT5 [3]. Pre-trained Pro-
gramming Language Models (PPLMs) ‚ÄìPLMs pre-trained on
programming languages‚Äì learn a generic representation of
code using an abstract objective function such as Mask Lan-
guage Modeling (MLM) during the pretraining phase. These
representations later are exploited for programming language-
oriented downstream tasks such as code summarization [4],
[5], [6], code clone detection [7], [8] and code reÔ¨Ånement [9],
[10] in the Ô¨Åne-tuning phase.
Despite the advance of PPLMs, most of them [2], [11],
[12] exploit either the encoder or the decoder Transformer
blocks [13]. These models treat a code snippet as a sequence
of tokens like conventional techniques for input modeling inthe natural language domain while ignoring the rich syntactical
information of source code. Recently, several studies proposed
representations with the structural information of source code
in mind [9], [3], [14].
Jiang et al. proposed a tree-based PLM for programming
language generation tasks [14], which utilizes the Abstract
Syntax Tree (AST) to provide structural information for pre-
training a Transformer-based architecture [13]. Compared to
the sequence-based approaches that consider code tokens as
the model‚Äôs input, in their approach, each code token is
represented as a path from the root node to the terminal
node corresponding to that code token in the AST. Though
their approach feeds rich structural information to the model,
the model‚Äôs input length is multiplied by the average length
of all paths in the AST. This scaling in the input length is
not computationally efÔ¨Åcient, according to Ainslie et al., who
mentions that the computational and memory complexity of
attention operation scales quadratically as the input length
increases [15].
Some other PPLMs provide type information to the existing
models. GraphCodeBERT [9] considers the inherent structure
of code and utilizes the dataÔ¨Çow extracted from AST corre-
sponding to each input to provide semantic-level information
as an additional input for the model; then pre-trained Code-
BERT [1] using this new information. Furthermore, authors
of CodeT5 [3] mentioned that developer-assigned identiÔ¨Åers
consist of rich code semantics, so they proposed a novel
identiÔ¨Åer-aware pre-training task to notify the model whether
a code token is an identiÔ¨Åer or not. Both of these approaches
require the model to be pre-trained from scratch. Even the
new information provided by GraphCodeBERT, though this
model is pre-trained on CodeBERT, requires pre-training from
scratch, which is not time and computationally efÔ¨Åcient.
Therefore, we cannot directly take advantage of the existing
PPLMs such as CodeBERT [1], GraphCodeBERT [9], and
CodeT5 [3] to propose a new information or input format.
As these models are pre-trained based on a Ô¨Çattened sequence
of code tokens [16], proposing a new form of input rep-
resentation requires all these models to be pre-trained from
scratch . These two issues, namely computational inefÔ¨Åciency
of using syntactical information of source code (due to length
increase) and requiring to pre-train the PPLMs from scratch
when introducing new input, motivated us to propose an
1arXiv:2303.06233v1  [cs.SE]  10 Mar 2023

--- PAGE 2 ---
approach that addresses them. Thus, this work aims to provide
syntactical embeddings of the source code to the existing pre-
trained programming language models.
In order to propose a parameter-efÔ¨Åcient and model-agnostic
approach for imposing syntactical information to the cur-
rent PPLMs, we utilize adapters . In NLP, an adapter is a
lightweight component placed inside each Transformer block
to modify its behavior [17]. During adapter training, the
weights of the pre-trained model are Ô¨Åxed, and the newly
introduced adapter weights are trained on an objective func-
tion. Adapters are often used to adapt existing NLP models
to work in a speciÔ¨Åc context (i.e., transfer learning) [18], [19]
or to perform a particular task (i.e., quick Ô¨Åne-tuning) [20],
[21]. However, adapters have never been used to capture the
syntactical information of source code nor to provide new
information to the existing PLMs or PPLMs.
In this work, we propose Named Entity Recognition
(NER) Adapters , lightweight modules inserted inside Trans-
former blocks whose aim is to learn type information extracted
from the AST. These pluggable modules can be inserted into
current PPLMs such as CodeBERT [1], GraphCodeBERT [9],
and CodeT5 [3]. To train NER adapters, we pose the problem
as a token classiÔ¨Åcation task in which each input token is
assigned a type extracted from the AST, and the NER adapter
aims to detect the type of each token correctly.
We insert NER adapters in CodeBERT [1], thus, developing
a model that we refer to as CodeBERTER . We conduct
our experiments on Java bug Ô¨Åxing data introduce in [10]
for code reÔ¨Ånement, and CodeSearchNet dataset [22] for
code summarization task. The main research question that
we investigate here is: Can NER adapters improve the
performance of software engineering target tasks, namely,
code reÔ¨Ånement and code summarization, while using
fewer trainable parameters?
The results show that for code reÔ¨Ånement, we improved
the CodeBERT baseline accuracy from 16.4 to 17.8 with 23
Million trainable parameters, which is 80% less than the total
number of the baseline trainable parameters. We also applied
NER adapters on code summarization, which improved the
BLEU-4 score of two languages, including Ruby and Go,
by30% and29%, respectively, with 77% fewer parameters
compared to full Ô¨Åne-tuning.
Our main contributions are as follows:
Introduce NER adapters with a novel loss function, Token
Type ClassiÔ¨Åcation Loss (TTC), to impose syntactical
information to the existing PPLMs. We also publish the
source code1.
Evaluate the performance of NER adapters on code
reÔ¨Ånement and code summarization.
Improve the results of these tasks while training fewer
parameters and being more computationally efÔ¨Åcient.
The rest of the paper is organized as follows. In Section II,
we provide an overview of important background information
and then introduce NER adapters in Section III. We provide
1https://github.com/ist1373/NER Adapters/the experiments setup and the details of our study in Section
IV. Results and discussion are explained in Section V. Sections
VI and VII are dedicated to the related works and threats to
validity. Finally, we conclude the paper in Section VIII.
II. B ACKGROUND
A. Abstract Syntax Tree
An AST is a tree-like representation of the syntax of a
programming language. Each node in the tree represents a
syntactic construct in the source code, such as a keyword, an
operator, a variable, or a function. The structure of the tree
reÔ¨Çects the structure of the program, with the root node repre-
senting the top-level construct and the leaf nodes representing
the smallest, most basic constructs [23].
ASTs can be used for a variety of purposes, including
analyzing the structure of a program to check for syntax errors
or enforce coding standards [24], and extracting information
about the program, such as variable and function deÔ¨Ånitions, to
create documentation or generate code coverage reports [25].
B. Pre-trained Programming Language Models
Pre-trained Programming Language Models (PPLMs) are
deep neural language models trained on a large dataset of
source code and learn to predict the next word or token in
the code given the context of the previous words or tokens.
These models later can be used to perform a variety of target
tasks during the Ô¨Åne-tuning phase, such as code summarization
[3], [26], code completion [27], code generation [3], and code
reÔ¨Ånement [9].
C. Adapters
In natural language processing, adapters are lightweight
components that can be used to adapt a language model to a
speciÔ¨Åc task or dataset. Adapter-based training is an efÔ¨Åcient
and quick Ô¨Åne-tuning method that requires fewer parameters
compared to traditional full Ô¨Åne-tuning. In comparison to
simply adding a new head on top of a pre-trained language
model (PLM), adapters offer superior performance due to their
ability to integrate into the internal structure of the PLM and
inÔ¨Çuence the network‚Äôs internal embeddings.
Adapters are employed in several areas, including (1) Ô¨Åne-
tuning, which is the process of continuing to train a machine
learning model on a new dataset, using the parameters and
weights learned from the original training dataset as a starting
point and (2) domain adaptation is another area that adapters
are used, which is the process of adapting a machine learning
model to a new domain, or a speciÔ¨Åc subject area or context.
This can be done by continuing to train the model on a dataset
that is representative of the new domain or by adding domain-
speciÔ¨Åc information to the model, such as domain-speciÔ¨Åc
word embeddings or features.
D. Language Adapters
Language adapters‚Äô aim is to learn language-speciÔ¨Åc trans-
formations [19]. They are trained on unlabeled data using
2

--- PAGE 3 ---
an abstract objective function such as mask language mod-
eling (MLM). They consist of a down-projection and an up-
projection at each layer, with a residual connection between
them. The down-projection, D, is a matrix with dimensions
h x d, where h is the hidden size of the transformer model
and d is the dimension of the adapter. The up-projection, U,
is a matrix with dimensions d x h. The language adapter at
each layer takes in the Transformer hidden state, hl, and the
residual,rl, and applies the down-projection and up-projection
to them, with a ReLU activation function. The output of the
language adapter is then added to the residual connection.
LanguageAdapter l(hl;rl) =Ul(ReLU (Dl(hl))) +rl(1)
E. AdapterFusion
AdapterFusion is a method of combining the knowledge
from different language adapters in order to improve per-
formance on downstream tasks such as code summarization
[28]. Given a set of Nlanguage adapters, adapter fusion
involves taking the weighted sum of the outputs of these
adapters while the weights of the pre-trained model and
the language adapters are Ô¨Åxed. AdapterFusion consists of
key, value, and query matrices at each layer. The goal of
AdapterFusion is to Ô¨Ånd the optimal combination of language
adapters by minimizing a loss function using a target task. This
allows for the extraction and composition of knowledge from
different language adapters in order to improve performance
on downstream tasks.
 = argminL(D; ;1;:::; N) (2)
where consists ofKey l,Value landQuery lmetrics at
each layerl, as shown in Fig. 1. At each transformer block,
the output of the feed-forward sub-layer is taken to be as the
Query , and the output of each language adapter is used for
bothKey andValue vectors.
III. NER A DAPTERS
The NER adapter aims to incorporate token-type informa-
tion into the network, as token types can provide valuable
syntactical information to a model. According to research
in [29], different token types can have varying levels of
importance on a pre-trained language model; for example,
identiÔ¨Åers are more signiÔ¨Åcant than other types in terms of
the amount of attention from the model and their learned
representations. To train the model on code token types, we
Ô¨Årst need to identify each token type in our dataset. We
then introduce a new variation of adapters, NER adapters,
which we plug into the PPLM and train them on a token-type
classiÔ¨Åcation task. The NER adapter is expected to predict the
type of each token when the adapter-training phase is done. We
discuss the details of NER adapters in the three sub-sections
below: structure, objective function, and training phase. In the
Ô¨Ånal subsection, we explain how NER adapter is placed in the
transformer blocks of a PPLM.Multi-Head AttentionAdd & NormFeed ForwardAdapter Fusion
Add & NormAdd & Norm
Language Adapter NER AdapterKey Value Query
Fig. 1: The proposed architecture for incorporating syntactical
information into transformer blocks consists of two key com-
ponents: a language adapter and a named entity recognition
(NER) adapter. These adapters have been trained separately
before being inserted into a parallel stack. An AdapterFusion
module is placed on top of the stack, which is trained
to perform a speciÔ¨Åc target task, such as code reÔ¨Ånement
or code summarization. This allows for the integration and
combination of the knowledge gained from the below adapters,
leading to more accurate and efÔ¨Åcient performance.
A. Structure
We use the same architecture of language adapters for
NER adapters, so they consist of down- and up-sampling
feedforward blocks combined with residual connections:
NERAdapter l(hl;rl) =Ul(ReLU (Dl(hl))) +rl (3)
wherehlandrlare the hidden state and residuals at layer
l, respectively.
B. Token Type ClassiÔ¨Åcation Loss (TTC)
Given a token type assigned to each token in the code
sample, we pose the problem as a token classiÔ¨Åcation problem,
where the adapter is responsible for predicting the type of each
token. To train the NER adapters, we employ cross-entropy
loss function.
LetXi=x1;:::;x Nrepresent a sequence of token ids for
samplei,T=t1;:::;t Nindicate the corresponding sequence
of type ids, and Y=y1;:::;y Nbe the one-hot representation
of these type ids, with each element having the size of the total
3

--- PAGE 4 ---
number of types presented in the dataset. The cross-entropy
for sampleiis calculated as follows:
LNER = NX
t=1YTlog(Pt) (4)
whereLNER indicates the loss function and Ptis the
probability distribution for types we receive from the network
for tokent. This loss function allows us to measure the differ-
ence between the predicted and actual token types, providing
a means to adjust the model‚Äôs parameters accordingly and
improve its performance.
C. Training NER Adapters
This section explains the steps we need to train the NER
adapters.
1) Extracting NERs Corresponding to The Leaf Nodes:
NER adapter requires to be trained on labeled data of token
types. To accurately extract the type of each token in a code
sample, we utilize the tree-sitter parser2. This parser is specif-
ically designed for language analysis and can determine the
type of each word in multiple programming languages. Tree-
sitter is used in previous studies, such as in GraphCodeBERT
[9] to extract dataÔ¨Çow from the AST and in static analysis
experiments on Github [30]. An example of an AST generated
for a given code snippet is shown in Fig. 3. To obtain the
token types, we feed each code snippet to the tree-sitter and
then traverse the AST to extract the type of each code token.
2) Encoding Tokens by Tokenizer and Assigning Each Sub-
Token The Same Token Type: Using the steps mentioned
earlier, we extract the code token types. However, due to
the nature of tokenization, where a single word may be split
into multiple tokens, a one-to-one relationship between words
and tokens cannot be assumed. Therefore, we must establish
mappings between the words and their corresponding tokens
and subsequently assign the same type to each token. For
example, the function name find\_bad\_files would be
marked as an identiÔ¨Åer type by tree-sitter. When tokenized,
this name may be split into three tokens: ‚ÄúÔ¨Ånd‚Äù, ‚Äúbad,‚Äù and
‚ÄúÔ¨Åles.‚Äù In this case, we would mark all of them as identiÔ¨Åer
types. After this process, we have a list of the tokens and
their corresponding types for each sample, providing a solid
foundation for further analysis and processing.
3) Fine-Tuning NER Adapters by Token Type ClassiÔ¨Åcation
Loss Function: The Ô¨Ånal step is to incorporate NER adapters
and Ô¨Åne-tune their weights using the token type classiÔ¨Åcation
loss function. This is done while keeping the weights of
the pre-trained model Ô¨Åxed, allowing the NER adapters to
specialize in recognizing speciÔ¨Åc named entities while lever-
aging the general language understanding of the pre-trained
model. This approach allows for improved performance in
identifying named entities in the text while maintaining the
model‚Äôs overall accuracy.
2https://tree-sitter.github.io/tree-sitter/
Fig. 2: An example of java code snippet that is sent to tree-
sitter to extract its corresponding AST.
D. Architecture Overview
The proposed architecture for incorporating syntactical in-
formation into transformer blocks is demonstrated in Fig. 1.
This architecture consists of two key components: a language
adapter and a named entity recognition (NER) adapter. We
train these adapters separately before inserting them into a
parallel stack, as shown in the Ô¨Ågure. The language adapter is
trained on mask language modeling loss function. A language
adapter is used to learn general language knowledge that
underlies the dataset. Finally, we utilize AdapterFusion to
combine the knowledge gained from the language and NER
adapters to provide a more robust output representation at each
transformer block. The AdapterFusion module is placed on top
of the stack and is trained to perform a speciÔ¨Åc target task.
These tasks in our experiments are code reÔ¨Ånement and code
summarization. Using AdapterFusion allows for the integration
and combination of the knowledge gained from the below
adapters, leading to more accurate and efÔ¨Åcient performance.
The details of the input data and how the embeddings of
sub-tokens and adapters are composed in the AdapterFusion
are shown through an example in Fig. 4. The input code
sample is presented in Fig. 2, and Fig. 4 demonstrates the
details. The code sample is fed into a transformer block
equipped with NER and language adapters. Prior to input
embeddings, words are divided into sub-tokens as necessary.
These sub-token embeddings, denoted by Etokens , are then
passed through both NER and language adapters in parallel.
This results in two embeddings for each sub-token, denoted by
Ttoken andLtoken , corresponding to the NER and language
adapters, respectively. Finally, these embeddings are composed
by AdapterFusion to select the useful information from the
previous embeddings for a speciÔ¨Åc target task, such as code
reÔ¨Ånement. The Ô¨Ånal embeddings are denoted by Ftoken .
IV. E XPERIMENT SETUP
We conduct experiments using CodeBERT as our backbone
model for code reÔ¨Ånement and code summarization. The
former task aims to identify and Ô¨Åx bugs automatically, and
the latter refers to automatically generating descriptions of
the functionality of a given code snippet in natural language.
CodeBERT is chosen as it has been studied and evaluated in
several software engineering works previously [1], [31], [11].
Code reÔ¨Ånement is chosen as it is heavily reliant on syntactical
information to identify and correct errors and optimize code
structure, and code summarization is chosen as it allows us
to assess the models‚Äô generation ability and evaluate the se-
mantic knowledge that underlies the programming and natural
language aspects of a PPLM. In the following, we explain
4

--- PAGE 5 ---
Left:
IdentiÔ¨Åer:
iRight:
call
IdentiÔ¨Åer:
rangeArgument
List
Integer:
5Block
Expression
Statement
Call
IdentiÔ¨Åer:
printArgument
List
IdentiÔ¨Åer:
iFor
Statement
Fig. 3: An example of an AST generated by the tree-sitter
parser for input code of Fig. 2. The leaf nodes are represented
by blue circles, while the non-leaf nodes are represented by
green circles. This AST is produced when the code snippet is
fed into the parser.
the downstream tasks and the datasets we used to perform
experiments, the baseline model, and the evaluation metrics
for each downstream task.
A. Downstream Tasks and Datasets
Code ReÔ¨Ånement is an essential aspect of software de-
velopment and a crucial step in ensuring the robustness and
reliability of software systems [9]. By leveraging various
techniques and tools, it aims to automatically identify and Ô¨Åx
bugs in the code, thus signiÔ¨Åcantly reducing the cost and effort
associated with resolving them manually. In our study, we
utilize the dataset released by Tufano et al. [10] to evaluate the
effectiveness of our proposed architecture and NER adapters
on code reÔ¨Ånement. We performed our experiments on BFP
small dataset. The total number of datapoints in training, test,
and validation sets are 46680 ,5835 and5835 , respectively.
Source Code Summarization is a powerful technique in
software engineering that aims to provide a natural language
description of the source code‚Äôs functionality, thus facilitating
the understanding and maintenance of the code [26]. The goal
of source code summarization is to make it easier to compre-
hend the overall structure and functionality of the code. This
task is widely studied, and various approaches are proposed,
particularly utilizing programming language models such as
CodeBERT [1], CodeT5 [3], and Multilingual CodeBERT [32]
to summarize source code automatically.
We choose code reÔ¨Ånement as it is a more abstract and
complex task than type inference when we have NER adapters.More speciÔ¨Åcally, we provided AST information to evaluate
the model on more complex tasks rather than type inference.
We selected code summarization as it is a generative task, and
the goal is to assess to what extent our proposed structural
information could be helpful for NL-PL generative target tasks.
Our research uses CodeSearchNet [22] as our dataset for
training the language and NER adapters. The dataset consists
of six programming languages. The size of each language
is demonstrated in Table I. As tree-sitter treats the entire
PHP code snippet as a single text element and fails to
provide syntactical information, we exclude PHP from our
experiments. Instead, we will focus on utilizing the tree-sitter
for the following languages: Go, Java, JavaScript, Python, and
Ruby. More speciÔ¨Åcally, for code reÔ¨Ånement, language and
NER adapters are trained on the training split of the unimodal
Java dataset in CodeSearchNet, and AdapterFusion is trained
on the Java bug Ô¨Åx dataset introduced in [10]. For code
summarization, language and NER adapters are trained on the
training split of the unimodal data for all languages (combined)
available in CodeSearchNet. The AdapterFusion is then trained
on the aggregated training splits of the bimodal data (i.e., code
and comments) for all languages included in CodeSearchNet,
allowing for a comprehensive and multilingual approach to
code summarization.
NER and language adapters are trained for 50,000 training
steps with a batch size of 48 and a learning rate of 2e 5. For
code reÔ¨Ånement, AdapterFusion is trained for 100,000 training
steps with a batch size of 16 and a learning rate of 5e 5.
For code summarization, we trained AdapterFusion for 50,000
training steps with a batch size of 32.
B. Baseline Model
Our approach is designed to be modular and Ô¨Çexible, allow-
ing for easy integration of different pre-trained programming
language models. This allows for easy experimentation and
comparison of different models and their performance on
various tasks.
In our study, we choose CodeBERT [1] as our backbone
and baseline model. This model is pre-trained on the Code-
SearchNet dataset [22] and is widely studied in software
engineering. CodeBERT is a multilingual model that is Ô¨Åne-
tuned on monolingual datasets.
To evaluate the effectiveness of our approach, we compare
the results of Ô¨Åne-tuning our proposed architecture when
it is inserted in CodeBERT with the results of Ô¨Åne-tuning
CodeBERT on the two tasks. This allows us to measure the
potential improvement in the performance and its effectiveness
when applied to code reÔ¨Ånement and summarization tasks. Ad-
ditionally, this comparison provides insight into the strengths
and limitations of our proposed approach and how it compares
to the baseline model. For each task, other methods/models
are used for comparison, which are explained in the Results
section to save space and reduce redundancy.
C. Evaluation Metrics
The code reÔ¨Ånement task is evaluated using a combination
of BLEU-4 score and accuracy measurements. The BLEU
5

--- PAGE 6 ---
for ( int i = 0 ; i ¬° 0 ; i ++ ) Input Words
for ( int i = 0 ; i ¬° 0 ; i ++ ) Sub-Tokensf System .out. println
f Sys .out. print tem ln
Transformer BlockEfor E(Eint EiE=E0E;EiE¬°E0E;EiE++ E)EfESys E. Eout E. Eprint Etem Eln
Lfor L(Tint Li L= L0 L; L. Lout L. Lprint Ltem Lln Tfor Ti T= T0 T; Ttem T. Tout T. Tprint Tln...Language Adapter NER Adapter
AdapterFusionEmbeddings
Language &Type
Embeddings
Ffor F(Fint FiF=F0F;FiF¬°F0F;FiF++ F)FfFSys F. Fout F. Fprint Ftem FlnFusion
Embeddings
Fig. 4: The input data Ô¨Çow for the sample shown in Figure 2 proceeds as follows when fed into a transformer block equipped
with NER and language adapters. Prior to input embeddings, words are divided into sub-tokens as necessary. These sub-token
embeddings, denoted by Etokens , are then passed through both NER and language adapters in parallel. This results in two
embeddings for each sub-token, denoted by Ttoken andLtoken , corresponding to the NER and language adapters, respectively.
Finally, these embeddings are composed by AdapterFusion to select the useful information from the previous embeddings for
a speciÔ¨Åc target task, such as code reÔ¨Ånement. The Ô¨Ånal embeddings are denoted by Ftoken .
TABLE I: Statistics of the CodeSearchNet dataset for the Ô¨Åve
languages in our experiments [22]
Language Bimodal Data Unimodal Data
Go 317,832 726,768
Java 500,754 1,569,889
JavaScript 143,252 1,857,835
Python 458,219 1,156,085
Ruby 52,905 164,048
score, a widely accepted evaluation metric in natural language
processing, is utilized to gauge the similarity between the gen-
erated output and the correct answer. Additionally, we measure
accuracy, which represents the proportion of samples that the
model was able to Ô¨Åx correctly. For code reÔ¨Ånement, we follow
the CodeXGLUE and report both BLEU and accuracy scores
to reÔ¨Çect the model‚Äôs performance, as considering only one
of these scores can be misleading. For example, the BLEU
score of the ‚Äúnaive copy‚Äù is more than the BLEU score of all
the baselines. The ultimate goal of our evaluation process is
to optimize both of these metrics simultaneously, striving for
high levels of both similarity and correction accuracy.
The code summarization task is evaluated using BLEU
(i.e., Bilingual Evaluation Understudy) score, a precision-
based measurement for evaluating the performance of NLP
models, particularly machine translation systems. It works by
comparing the output of the language models (e.g., a generated
summary) with a reference summary produced by a human
expert and calculating the degree of overlap between the two.
In this work we calculate BLUE-4 score [33] in which the
algorithm Ô¨Årst identiÔ¨Åes the unigrams to 4-grams in both theoutput and the reference translation and then calculates the
precision between the generated summary (i.e., n-gram hit) and
the ground truth summary (i.e., total n-gram count). Higher
BLEU scores indicate that the model‚Äôs output is more similar
to the reference translation and is therefore considered higher
quality. BLEU is commonly used in code summarization
to evaluate and compare the quality and coherence of the
generated summaries from different PPLMs [26], [3], [9], [5].
V. R ESULTS AND DISCUSSION
A. Performance of NER Adapters
Table II presents the evaluation metrics of the token
type classiÔ¨Åcation loss function, namely, Precision, Recall,
F1score, and Accuracy, during the NER adapter training
phase. Precision is a metric that measures the proportion of
true positive predictions out of all positive predictions. Recall
measures the proportion of true positive predictions from all
actual positive observations. F1 Score is the harmonic mean of
Precision and Recall, and Accuracy is a metric that measures
the proportion of correct predictions out of all observations.
These NER adapters were trained on the types obtained from
the tree-sitter parser on the CodeSearchNet dataset.
The programming languages in the table are arranged in or-
der of dataset size, with the least resource-intensive languages
listed Ô¨Årst and the most resource-intensive languages listed
last. Python achieves the best performance for all metrics com-
pared to other languages, and Ruby has the least performance
as it suffers from a low resource dataset. It is worth noting that
the precision, recall, and F1 scores are highly dependent on
the size of the dataset, indicating that larger datasets generally
6

--- PAGE 7 ---
TABLE II: The table presents the evaluation metrics, including Precision, Recall, F1 Score, and Accuracy, for the training
phase of NER adapters. The programming languages included in the table are organized in order of resource availability, with
those with the least resources available (i.e., size of the dataset) listed Ô¨Årst and those with the most resources listed last. These
metrics provide a comprehensive assessment of the performance of the NER adapters.
Languages Precision Recall F1 Score Accuracy
Ruby 0.68 0.65 0.66 0.92
JavaScript 0.78 0.79 0.78 0.91
Go 0.78 0.82 0.80 0.94
Python 0.95 0.94 0.94 0.98
Java 0.78 0.79 0.78 0.89
yield better results for NER adapter training. However, the
results for Java are worse than those of other languages,
despite having a relatively rich dataset. This discrepancy
may be attributed to the syntactical complexities present in
Java compared to Python, which contributed to its lower
performance. On the other hand, Python exhibits exceptional
performance across all the evaluated metrics. The high levels
of accuracy recorded for all programming languages further
demonstrate that the NER adapters are effectively trained to
extract the syntactical information from the network‚Äôs internal
embeddings. This highlights the efÔ¨Åciency and robustness of
our NER adapter implementation.
B. CodeBERTER‚Äô Results for Code ReÔ¨Ånement
We evaluate the performance of NER adapters on code
reÔ¨Ånement task to check to what extent they could be effective
on a target programming task. The results are presented in
Table III. The Naive copy adopts a simplistic approach by
directly copying the buggy code as the reÔ¨Ånement result.
The LSTM use the Long Short Term Memory architecture,
and the Transformer model utilizes 12 transformer encoder
blocks (i.e., the same number of layers and hidden size as pre-
trained models). For the Transformer, the encoder is initialized
with pre-trained models, while the decoder‚Äôs parameters are
randomly initialized. The results depicted in the table demon-
strate that the Transformer signiÔ¨Åcantly outperforms the LSTM
model.
We separate the results of these models in Table III from
the second group of models that leverage pre-training on
programming languages. The BLEU and Accuracy scores are
higher for the models with pre-training. RoBERTa (code)
is pre-trained only on code. NSEdit [34] propose a new
pointer network to transformer blocks to predict edit loca-
tions. CoTexT [35] is a pre-trained bimodal encoder-decoder
programming model. Among the programming pre-trained
models, our approach, denoted by CodeBERTER , stands out
by achieving the best performance on BLEU score. Note that
CodeBERTER improves the results of CodeBERT without
taking advantage of Ô¨Åne-tuning all of its parameters. This
illustrates the beneÔ¨Åcial impact of incorporating code structure
information in code reÔ¨Ånement.
C. CodeBERTER‚Äôs Results for Code Summarization
Code summarization evaluates a programming language
model‚Äôs semantic and syntactic aspects. Table IV displaysTABLE III: NER adapter BLEU score and Accuracy results on
code reÔ¨Ånement task. The baselines are divided by a horizontal
line, where the bottom group shows the models that are pre-
trained on programming languages. The bold font indicates
the best results, which belongs to CodeBERTER.
Method/Model BLEU Accuracy
Naive copy 78.06 0.0
LSTM 76.76 10.0
Transformer 77.21 14.7
RoBERTa (code) 77.30 15.9
CodeBERT 77.42 16.4
CodeBERTER 78.2 17.8
CoTexT 77.91 22.64
NSEdit 71.06 24.04
the results of our approach compared to other models. For
this target task, we select a multilingual Ô¨Åne-tuning paradigm
based on the recent Ô¨Åndings and recommendations in [32],
where the authors mention that code summarization can beneÔ¨Åt
from multilingual Ô¨Åne-tuning. It means that, in our approach,
Ô¨Årst, we trained the language and NER adapters on the
multilingual dataset, as mentioned earlier. We also trained the
AdapterFusion on the multilingual code summarization dataset
and tested it on each language separately. Notice that for code
reÔ¨Ånement, as there exists only a Java dataset, to the best of
our knowledge, we could only evaluate the performance of
NER adapters on code reÔ¨Ånement in a monolingual setting.
Table IV shows the BLUE-4 scores of the state-of-the-
art approaches and our approach, denoted by CodeBERTER ,
separated by a horizontal line. The best scores are shown in
bold. The scores obtained from CodeBERT are also underlined
for readability. CodeBERT is the primary baseline we need to
compare our results with since our backbone model is Code-
BERT. CodeBERTER improves the scores for four languages,
Ruby, JavaScript, Go, and Java, and has on-par results with
the best models for Python. Looking into the statistics of the
data provided in Table I indicates that the Ô¨Årst three languages
have a lower number of data for training compared to the other
two. For these low-resource languages, our model signiÔ¨Åcantly
improves the results compared to its baseline, CodeBERT.
polyglot CodeBERT and polyglot GraphCodeBERT, as de-
scribed in [32] by Ahmed et al. (2021), are models that have
been fully Ô¨Åne-tuned in a multilingual setting. For example,
to evaluate their performance on Ruby, the models were Ô¨Åne-
tuned on all of the programming languages in the Code-
SearchNet dataset [22] and subsequently evaluated on a Ruby
7

--- PAGE 8 ---
TABLE IV: Smooth BLEU-4 scores on code summarization. CodeBERTER is Ô¨Åne-tuned on the multilingual datasets (same
aspolyglot CodeBERT). The horizontal line separates our approach from the others, and the dashed-horizontal line separates
the language models Ô¨Åne-tuned on multilingual datasets from the ones Ô¨Åne-tuned on monolingual datasets. For readability, the
results of CodeBERT are underlined. The best scores among all models for each language are shown in bold.
Models Ruby JavaScript Go Python Java Average
CodeBERTER 15.90 16.12 23.34 18.38 19.95 18.738
polyglot GraphCodeBERT [32] 14.95 15.79 18.92 18.90 19.91 17.694
polyglot CodeBERT [32] 14.75 15.80 18.77 18.71 20.11 17.48
DistillCodeT5 15.75 16.42 20.21 20.59 20.51 18.696
CodeT5 [3] 15.69 16.24 19.76 20.36 20.46 18.502
ProphetNet-Code [36] 14.37 16.60 18.43 17.87 19.39 17.332
CoTexT [36] 14.02 14.96 18.86 19.73 19.06 17.326
PLBART [12] 14.11 15.56 18.91 19.30 18.45 17.22
GraphCodeBERT 12.62 14.79 18.40 18.02 19.22 16.61
CodeBERT 12.16 14.90 18.07 19.06 17.65 16.36
RoBERTa [37] 11.17 11.90 17.72 18.14 16.47 15.08
Transformer [13] 11.18 11.59 16.38 15.81 16.26 14.24
seq2seq [38] 9.64 10.21 13.98 15.93 15.09 12.97
test dataset. This approach has improved the performance
ofpolyglot models, as demonstrated by the difference in
performance between CodeBERT and polyglot CodeBERT.
However, CodeBERTER additionally improves the scores
ofpolyglot CodeBERT, which is attributed to incorporating
multilingual syntactical information into the model using NER
adapters.
For the high resource languages, Python and Java, Code-
BERTER performs on par with the baseline model (improves
Java results from CodeBERT by 2 BLEU scores). We relate
this to the fact that for high-resource languages, the language
model is able to learn the general knowledge underlying a
language better than a low-resource language, for which less
training data exists [39]. Therefore, the NER adapter enables
the model to provide more type information for low-resource
languages.
Although CodeBERTER has the best scores for some
languages, note that CodeBERTER uses CodeBERT as its
backbone, so it would be fairer to compare the effectiveness
of our model with CodeBERT and not larger models such as
CodeT5. These larger models have two times more parameters
than CodeBERT. However, we considered the larger models
in Table IV to demonstrate that our approach could even have
on-par results with larger models for each of the languages.
Following the leaderboard on CodeXGLUE, we also report
theaverage scores among the models in Table IV. If we
compute the averages over the studied languages for code
summarization, the CodeBERTER performance is 18:738,
which is higher than all other models on the leaderboard for
this task.
D. Discussions
Computational efÔ¨Åciency of our approach. For all lan-
guages, we train NER adapters more efÔ¨Åciently with lower
computational costs. Each of the language and NER adapters
have0:9million and AdapterFusion has 21trainable
parameters. We train these tree adapters for code reÔ¨Ånement
in total, leading to 23million trainable parameters. This
number is still less than the total trainable parameters of the
Fig. 5: The Ô¨Ågure illustrates the attention patterns of the Ô¨Ånal
layer of CodeBERT for a Java sample, with (the Ô¨Ågure on
the right) and without (the left Ô¨Ågure) the insertion of NER
adapters to the pre-trained model. This comparison highlights
the impact of NER adapters on the model‚Äôs ability to identify
and extract relevant entities within the sample text.
CodeBERT model, which is 110million. Note that all the
110 million parameters in CodeBERT should be re-trained
during the standard fully Ô¨Åne-tuning phase. However, this is
not required in our approach. In CodeBERTER, we trained
23million parameters for all adapters, including language,
NER, and AdapterFusion. In terms of time efÔ¨Åciency, each
NER adapter takes about 10hours to be trained, for 40;000
training steps. This is while if we fully Ô¨Åne-tune CodeBERT
for the NER task with the same amount of training steps, it
takes around 17hours.
Effect of NER adapter. Fig. 5 demonstrates the attention
patterns of different tokens in a code sample on each other
8

--- PAGE 9 ---
in the Ô¨Ånal layer of CodeBERT, generated by bertviz3. The
strength of the lines among the tokens demonstrates the
amount of attention; the thicker the line is, the more attention
is put on that token. We only show one attention head
(out of 12 heads) of the last layer for preventing a messy
representation in this Ô¨Ågure. Fig. 5 compares the attention
patterns for a Java sample, for CodeBERT on the left, and for
CodeBERTER (i.e., CodeBERT with NER adapters) on the
right. As this is shown, the utilization of NER adapters results
in a shift in the attention pattern, meaning that it is more
distributed among the tokens. SpeciÔ¨Åcally, in CodeBERTER,
the NER adapter places a greater emphasis on identiÔ¨Åer tokens
such as sum,a, and b, compared to the baseline model. This
allows the model to leverage the additional attention paid
to these tokens for downstream tasks, resulting in improved
performance.
Note that the CodeBERT attention shows multiple weak
cross attentions between tokens, and most of the attention is
on the< s > token. This is aligned with previous Ô¨Åndings
of Sharma et al. [29], where the authors found that the
special tokens (e.g., <s> ) get the most amount of attention
especially in the last layer. So, as previously shown in [29],
new techniques are required to direct the BERT-based models‚Äô
attention toward other tokens for programming languages. This
is achieved in our work using the NER adapters.
VI. R ELATED WORK
In recent years, there have been many studies focusing on
code representation learning for various software engineering
tasks such as code generation [40], [41], [42], code summa-
rization [4], [5], [43], program synthesis [44], [45], [46], [47],
code search [48], and bug repair [49], [50]. With the advent of
pre-trained language models in NLP, researchers in software
engineering have leveraged the advancements in pre-trained
language models in NLP to propose programming pre-trained
language models that are pre-trained on either unimodal (i.e.,
code) or bimodal (i.e., code and comment) datasets.
CodeBERT [1], GraphCodeBERT [9] and CodeT5 [3] are
the examples of this approach. Even though source code con-
tains rich syntactical and semantical information, most of the
PPLMs treat code as a sequence of tokens. GraphCodeBERT
[9] extends the input with the dataÔ¨Çow extracted from the
AST. This allows the model to better understand the relation-
ships and dependencies between different parts of the code.
TreeBERT [14] represents the input as a concatenation path
of the AST leaves corresponding to each code sample. This
allows the model to better capture the structure and hierarchy
of the code. CodeT5 [3] is a code representation learning
model that is based on the T5 architecture. The model is pre-
trained on a large code corpus to predict the next token in a
code sequence. To improve the model‚Äôs understanding of code,
a new objective function was proposed in CodeT5 to inform
the model about the presence of identiÔ¨Åer tokens during the
3https://github.com/jessevig/bertvizpre-training phase. The model was evaluated on several code-
related tasks such as code generation, code summarization,
and code search and showed improved performance compared
to other existing models.
Adapters have been widely used in NLP [17], [19], [28] as
a quick and parameter-efÔ¨Åcient Ô¨Åne-tuning approach. Adapters
were previously used in software engineering to show the
transferability of natural language PLMs to programming lan-
guages with lower training costs. The authors trained language
adapters (i.e., adapters trained on masked language modeling
on unlabelled data) and task adapters (i.e., adapters trained for
a target task on labeled data) for code clone detection [18].
Differences of current studies with our work. Although
syntactical information is used in some of the current PPLMs,
none of the models can integrate this information into an
existing model without requiring pre-training it from scratch.
On the other hand, NER adapters enable imposing the syntac-
tical information in the current models without pre-training.
Moreover, though adapters are widely used in NLP and
are recently studied in software engineering, no work uses
adapters in such a context as we do in our work. Our approach
is novel not only for the TTC loss function that we introduce
for NER adapters but also for how they are used (i.e., the
proposed architecture in Fig. 1).
VII. T HREATS TO VALIDITY
External Validity In this study, we evaluate the results of
imposing syntactical information for code summarization on
the CodeSearchNet dataset and code reÔ¨Ånement for Java lan-
guage. The task and the programming languages are restricted,
and the results might not be generalizable to all downstream
tasks and other datasets. However, based on our observations
and the fact that we provide language and structural infor-
mation through adapters, we hypothesize that the results for
other tasks would be at least on par with the current models,
emphasizing that this could be achieved with less trainable
parameters. Though still experiments should conÔ¨Årm this. Note
that our approach is not restricted to CodeBERT and NER
adapters can be used in other PPLMs.
For code reÔ¨Ånement, there exist various approaches that
have been proposed in the literature. One notable contribution
is the use of beam search, which has been empirically studied
by Tufano et al. [10]. However, as our research focuses on a
different aspect of code reÔ¨Ånement, we did not explore this
particular approach in depth.
Internal Validity Hyperparameters can affect the Ô¨Åne-
tuning phase of a pre-trained model, and there is no hard
rule to select the best values for these parameters. Herefore,
the model might be subject to being stuck in sub-optimal
solutions. As Pfeiffer et al. [28] performed an extensive
hyperparameter search over adapters, we considered the de-
fault settings for adapters‚Äô hyperparameters. However, it is
performed in the NLP area and might not lead to the optimal
state in the software engineering domain.
9

--- PAGE 10 ---
VIII. C ONCLUSION AND FUTURE WORKS
In this study, we introduced NER adapters, a novel approach
for enhancing existing pre-trained models by imposing syn-
tactical information. To evaluate their performance, we con-
ducted experiments on two programming-related tasks: code
reÔ¨Ånement using a Java dataset in a monolingual setting and
code summarization using a multilingual approach. Our results
showed that CodeBERTER ‚ÄìCodeBERT with NER adapters‚Äì
outperforms the baseline models in both tasks. The NER
adapters are model-agnostic and require fewer parameters to
be trained compared to fully pre-train or Ô¨Åne-tuning a model.
We plan to apply CodeBERTER to other downstream tasks
and NER adapters to other pre-trained models.
ACKNOWLEDGMENT
This research is supported by a grant from the Natural Sci-
ences and Engineering Research Council of Canada RGPIN-
2019-05175.
REFERENCES
[1] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al. , ‚ÄúCodebert: A pre-trained model for programming
and natural languages,‚Äù arXiv preprint arXiv:2002.08155 , 2020.
[2] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, ‚ÄúPre-trained
contextual embedding of source code,‚Äù 2019.
[3] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, ‚ÄúCodet5: IdentiÔ¨Åer-aware
uniÔ¨Åed pre-trained encoder-decoder models for code understanding
and generation,‚Äù in Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing , 2021, pp. 8696‚Äì8708.
[4] J. Gu, P. Salza, and H. C. Gall, ‚ÄúAssemble foundation models for
automatic code summarization,‚Äù arXiv preprint arXiv:2201.05222 , 2022.
[5] T. Ahmed and P. Devanbu, ‚ÄúLearning code summarization from a small
and local dataset,‚Äù arXiv preprint arXiv:2206.00804 , 2022.
[6] C. Zhang, J. Wang, Q. Zhou, T. Xu, K. Tang, H. Gui, and F. Liu, ‚ÄúA
survey of automatic source code summarization,‚Äù Symmetry , vol. 14,
no. 3, p. 471, 2022.
[7] L. B ¬®uch and A. Andrzejak, ‚ÄúLearning-based recursive aggregation of
abstract syntax trees for code clone detection,‚Äù in 2019 IEEE 26th Inter-
national Conference on Software Analysis, Evolution and Reengineering
(SANER) . IEEE, 2019, pp. 95‚Äì104.
[8] K. W. NaÔ¨Å, T. S. Kar, B. Roy, C. K. Roy, and K. A. Schneider, ‚ÄúClcdsa:
cross language code clone detection using syntactical features and api
documentation,‚Äù in 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE) . IEEE, 2019, pp. 1026‚Äì1037.
[9] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu et al. , ‚ÄúGraphcodebert: Pre-training code repre-
sentations with data Ô¨Çow,‚Äù arXiv preprint arXiv:2009.08366 , 2020.
[10] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and
D. Poshyvanyk, ‚ÄúAn empirical study on learning bug-Ô¨Åxing patches in
the wild via neural machine translation,‚Äù ACM Transactions on Software
Engineering and Methodology (TOSEM) , vol. 28, no. 4, pp. 1‚Äì29, 2019.
[11] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang et al. , ‚ÄúCodexglue: A machine
learning benchmark dataset for code understanding and generation,‚Äù
arXiv preprint arXiv:2102.04664 , 2021.
[12] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, ‚ÄúUniÔ¨Åed
pre-training for program understanding and generation,‚Äù arXiv preprint
arXiv:2103.06333 , 2021.
[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in
neural information processing systems , vol. 30, 2017.
[14] X. Jiang, Z. Zheng, C. Lyu, L. Li, and L. Lyu, ‚ÄúTreebert: A tree-
based pre-trained model for programming language,‚Äù in Uncertainty in
ArtiÔ¨Åcial Intelligence . PMLR, 2021, pp. 54‚Äì63.
[15] J. Ainslie, S. Ontanon, C. Alberti, V . Cvicek, Z. Fisher, P. Pham,
A. Ravula, S. Sanghai, Q. Wang, and L. Yang, ‚ÄúEtc: Encoding long
and structured inputs in transformers,‚Äù arXiv preprint arXiv:2004.08483 ,
2020.[16] R. Wu, Y . Zhang, Q. Peng, L. Chen, and Z. Zheng, ‚ÄúA survey of
deep learning models for structural code understanding,‚Äù arXiv preprint
arXiv:2205.01293 , 2022.
[17] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,
A. Gesmundo, M. Attariyan, and S. Gelly, ‚ÄúParameter-efÔ¨Åcient transfer
learning for nlp,‚Äù in International Conference on Machine Learning .
PMLR, 2019, pp. 2790‚Äì2799.
[18] D. Goel, R. Grover, and F. H. Fard, ‚ÄúOn the cross-modal transfer
from natural language to code through adapter modules,‚Äù arXiv preprint
arXiv:2204.08653 , 2022.
[19] J. Pfeiffer, I. Vuli ¬¥c, I. Gurevych, and S. Ruder, ‚ÄúMad-x: An adapter-
based framework for multi-task cross-lingual transfer,‚Äù arXiv preprint
arXiv:2005.00052 , 2020.
[20] R. He, L. Liu, H. Ye, Q. Tan, B. Ding, L. Cheng, J.-W. Low, L. Bing,
and L. Si, ‚ÄúOn the effectiveness of adapter-based tuning for pretrained
language model adaptation,‚Äù arXiv preprint arXiv:2106.03164 , 2021.
[21] H. Le, J. Pino, C. Wang, J. Gu, D. Schwab, and L. Besacier,
‚ÄúLightweight adapter tuning for multilingual speech translation,‚Äù arXiv
preprint arXiv:2106.01463 , 2021.
[22] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
‚ÄúCodesearchnet challenge: Evaluating the state of semantic code search,‚Äù
arXiv preprint arXiv:1909.09436 , 2019.
[23] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu, ‚ÄúA novel
neural source code representation based on abstract syntax tree,‚Äù in
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE) . IEEE, 2019, pp. 783‚Äì794.
[24] D. Wendel and P. Medlock-Walton, ‚ÄúThinking in blocks: Implications of
using abstract syntax trees as the underlying program model,‚Äù in 2015
IEEE Blocks and Beyond Workshop (Blocks and Beyond) . IEEE, 2015,
pp. 63‚Äì66.
[25] C. Lin, Z. Ouyang, J. Zhuang, J. Chen, H. Li, and R. Wu, ‚ÄúImproving
code summarization with block-wise abstract syntax tree splitting,‚Äù in
2021 IEEE/ACM 29th International Conference on Program Compre-
hension (ICPC) . IEEE, 2021, pp. 184‚Äì195.
[26] X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, ‚ÄúSummarizing source
code with transferred api knowledge,‚Äù 2018.
[27] W. Takerngsaksiri, C. Tantithamthavorn, and Y .-F. Li, ‚ÄúSyntax-aware
on-the-Ô¨Çy code completion,‚Äù arXiv preprint arXiv:2211.04673 , 2022.
[28] J. Pfeiffer, A. Kamath, A. R ¬®uckl¬¥e, K. Cho, and I. Gurevych, ‚ÄúAdapter-
fusion: Non-destructive task composition for transfer learning,‚Äù arXiv
preprint arXiv:2005.00247 , 2020.
[29] R. Sharma, F. Chen, F. Fard, and D. Lo, ‚ÄúAn exploratory study on code
attention in bert,‚Äù arXiv preprint arXiv:2204.10200 , 2022.
[30] T. Clem and P. Thomson, ‚ÄúStatic analysis at github: An experience
report,‚Äù Queue , vol. 19, no. 4, pp. 42‚Äì67, 2021.
[31] Y . Tang, C. Tran, X. Li, P.-J. Chen, N. Goyal, V . Chaudhary, J. Gu, and
A. Fan, ‚ÄúMultilingual translation with extensible multilingual pretraining
and Ô¨Ånetuning,‚Äù arXiv preprint arXiv:2008.00401 , 2020.
[32] T. Ahmed and P. Devanbu, ‚ÄúMultilingual training for software engineer-
ing,‚Äù arXiv preprint arXiv:2112.02043 , 2021.
[33] C.-Y . Lin and F. J. Och, ‚ÄúOrange: a method for evaluating automatic
evaluation metrics for machine translation,‚Äù in COLING 2004: Proceed-
ings of the 20th International Conference on Computational Linguistics ,
2004, pp. 501‚Äì507.
[34] Y . Hu, X. Shi, Q. Zhou, and L. Pike, ‚ÄúFix bugs with transformer through
a neural-symbolic edit grammar,‚Äù arXiv preprint arXiv:2204.06643 ,
2022.
[35] L. Phan, H. Tran, D. Le, H. Nguyen, J. Annibal, A. Peltekian, and Y . Ye,
‚ÄúCotext: Multi-task learning with code-text transformer,‚Äù in Proceedings
of the 1st Workshop on Natural Language Processing for Programming
(NLP4Prog 2021) , 2021.
[36] W. Qi, Y . Gong, Y . Yan, C. Xu, B. Yao, B. Zhou, B. Cheng, D. Jiang,
J. Chen, R. Zhang et al. , ‚ÄúProphetnet-x: large-scale pre-training models
for english, chinese, multi-lingual, dialog, and code generation,‚Äù arXiv
preprint arXiv:2104.08006 , 2021.
[37] L. Zhuang, L. Wayne, S. Ya, and Z. Jun, ‚ÄúA robustly optimized bert
pre-training approach with post-training,‚Äù in Proceedings of the 20th
Chinese National Conference on Computational Linguistics , 2021, pp.
1218‚Äì1227.
[38] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence learning
with neural networks,‚Äù Advances in neural information processing
systems , vol. 27, 2014.
10

--- PAGE 11 ---
[39] F. Chen, F. H. Fard, D. Lo, and T. Bryksin, ‚ÄúOn the transferability of
pre-trained language models for low-resource programming languages,‚Äù
in2022 IEEE/ACM 30th International Conference on Program Com-
prehension (ICPC) . IEEE, 2022, pp. 401‚Äì412.
[40] Z. Zeng, H. Tan, H. Zhang, J. Li, Y . Zhang, and L. Zhang, ‚ÄúAn extensive
study on pre-trained models for program understanding and generation,‚Äù
2022.
[41] S. Zhou, U. Alon, F. F. Xu, Z. JIang, and G. Neubig, ‚ÄúDoccoder:
Generating code by retrieving and reading docs,‚Äù arXiv preprint
arXiv:2207.05987 , 2022.
[42] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
W.-t. Yih, L. Zettlemoyer, and M. Lewis, ‚ÄúIncoder: A generative model
for code inÔ¨Ålling and synthesis,‚Äù arXiv preprint arXiv:2204.05999 , 2022.
[43] P. Nie, J. Zhang, J. J. Li, R. Mooney, and M. Gligoric, ‚ÄúImpact of
evaluation methodologies on code summarization,‚Äù in Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , 2022, pp. 4936‚Äì4960.
[44] P. Vaithilingam, T. Zhang, and E. L. Glassman, ‚ÄúExpectation vs. experi-
ence: Evaluating the usability of code generation tools powered by large
language models,‚Äù in CHI Conference on Human Factors in ComputingSystems Extended Abstracts , 2022, pp. 1‚Äì7.
[45] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou, S. Savarese,
and C. Xiong, ‚ÄúA conversational paradigm for program synthesis,‚Äù arXiv
preprint arXiv:2203.13474 , 2022.
[46] K. Ellis, C. Wong, M. Nye, M. Sabl ¬¥e-Meyer, L. Morales, L. Hewitt,
L. Cary, A. Solar-Lezama, and J. B. Tenenbaum, ‚ÄúDreamcoder: Boot-
strapping inductive program synthesis with wake-sleep library learning,‚Äù
inProceedings of the 42nd acm sigplan international conference on
programming language design and implementation , 2021, pp. 835‚Äì850.
[47] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. Cai, M. Terry, Q. Le et al. , ‚ÄúProgram synthesis with large
language models,‚Äù arXiv preprint arXiv:2108.07732 , 2021.
[48] U. Nadeem, N. Ziems, and S. Wu, ‚ÄúCodedsi: Differentiable code search,‚Äù
arXiv preprint arXiv:2210.00328 , 2022.
[49] C. Richter and H. Wehrheim, ‚ÄúCan we learn from developer mistakes?
Learning to localize and repair real bugs from real bug Ô¨Åxes,‚Äù arXiv
e-prints , p. arXiv:2207.00301, Jul. 2022.
[50] J. Zhang, S. Panthaplackel, P. Nie, J. Jessy Li, and M. Gligoric,
‚ÄúCoditT5: Pretraining for Source Code and Natural Language Editing,‚Äù
arXiv e-prints , p. arXiv:2208.05446, Aug. 2022.
11
