# 2308.16557.pdf
# Converted from PDF to TXT
# Source path: ./2308.16557.pdf
# File size: 803210 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Effective Test Generation Using Pre-trained Large Language Models
and Mutation Testing
Arghavan Moradi Dakhela,∗, Amin Nikanjama, Vahid Majdinasaba, Foutse Khomhaand Michel
C. Desmaraisa
aDepartment of Computer and Software Engineering, Polytechnique Montreal, Montreal, H3T 1J4, Quebec, Canada
ARTICLE INFO
Keywords :
Test Generation
Large Language Model
Mutation TestingABSTRACT
Context:Oneofthecriticalphasesinthesoftwaredevelopmentlifecycleissoftwaretesting.Testing
helps with identifying potential bugs and reducing maintenance costs. The goal of automated test
generation tools is to ease the development of tests by suggesting efficient bug-revealing tests.
Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests.
Whilethecodecoverageofgeneratedtestswasusuallyassessed,theliteraturehasacknowledgedthat
the coverage is weakly correlated with the efficiency of tests in bug detection.
Objective: To improve over this limitation, in this paper, we introduce MuTAP(MutationTest case
generationusing Augmented Prompt)forimprovingtheeffectivenessoftestcasesgeneratedbyLLMs
in terms of revealing bugs by leveraging mutation testing.
Method: Our goal is achieved by augmenting prompts with surviving mutants, as those mutants
highlight the limitations of test cases in detecting bugs. MuTAPis capable of generating effective
test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We
employ different LLMs within MuTAPand evaluate their performance on different benchmarks.
Results: Our results show that our proposed method is able to detect up to 28% more faulty human-
written code snippets. Among these, 17% remained undetected by both the current state-of-the-art
fully-automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning approaches on
LLMs. Furthermore, MuTAPachieves a Mutation Score (MS) of 93.57% on synthetic buggy code,
outperforming all other approaches in our evaluation.
Conclusion:OurfindingssuggestthatalthoughLLMscanserveasausefultooltogeneratetestcases,
they require specific post-processing steps to enhance the effectiveness of the generated test cases
whichmaysufferfromsyntacticorfunctionalerrorsandmaybeineffectiveindetectingcertaintypes
of bugs and testing corner cases in PUTs.
1. Introduction
Testingisanimportantyetexpensivestepinthesoftware
development lifecycle. Generating effective tests is a time-
consuming and tedious task for developers. Unit tests are
essential as they form the basis of the test automation pyra-
mid [44, 47]. Unit tests check if a function or a component
works as expected in isolation. A unit test consists of two
components:thefirstcomponentisasetoftestinputsforthe
Program Under Test ( PUT), while the second component is
the test oracle that indicates the intended behavior (output)
of thePUTand is, therefore, capable of exposing bugs by
verifying the correctness of the PUTon test inputs [51]. A
test oracle can be in the format of assertions.
The automatic generation of unit tests is an important
topicinSoftwareEngineering(SE).Itaimstoreducedevel-
opers’testingefforts.Developinggood-qualityunittestscan
prevent bugs in software products. There are different tools
for automatically generating unit tests and test suites that
are either based on random test generators [42, 6], dynamic
symbolic execution [43, 19], or search-based approaches
[16, 17]. However, these techniques have some drawbacks
∗Corresponding author
arghavan.moradi-dakhel@polymtl.ca (A.M. Dakhel);
amin.nikanjam@polymtl.ca (A. Nikanjam); vahid.majdinasab@polymtl.ca (V.
Majdinasab); foutse.khomh@polymtl.ca (F. Khomh);
michel.desmarais@polymtl.ca (M.C. Desmarais)
ORCID(s):0000-0003-1900-2850 (A.M. Dakhel)and often generate tests with no assertion or too general
assertions, or tests with assertions that cannot effectively
assess the intended behavior of the PUT[39, 36].
Considering these shortcomings, researchers have re-
cently been exploring the possibility of leveraging Machine
Learning-based code synthesis techniques for generating
better unit tests [7, 11, 28, 41, 29]. Specifically, these ap-
proaches have been exploring the potential of Large Lan-
guage Models (LLMs) with the transformer architecture,
suchasCodex[12],whichhasachievedgoodperformancein
automaticprogramsynthesis[9,12,13,15,34].Amongsuch
efforts, Bareiß et al. [7] evaluate Codex’s performance for
test case generation by using a few-shotlearning approach.
Their findings on a limited set of 18 Java methods show
that their approach is comparable to feedback-directed test
generation.ATHENATEST[49]leveragedtheBARTtrans-
former model [30] after fine-tuning it on a set of real Java
functions and their corresponding tests. They also reported
achieving comparable coverage to EvoSuite [17] after an
assessment of five Java projects. Lemieux et al. [29] pro-
posed CODAMOSA which utilized test cases generated by
Codex to improve search-based testing techniques, which
consistsofonlytheprefix(inputs)ofatestcasewithoutany
test oracles. Their reported results obtained on 27 Python
projects show that CODAMOSA surpasses the baseline
search-based technique, Pynguin [33] and Codex in terms
of code coverage. Although the preliminary results of these
Arghavan MD et al.: Preprint submitted to Elsevier Page 1 of 16arXiv:2308.16557v1  [cs.SE]  31 Aug 2023

--- PAGE 2 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
studies and others [50, 41, 11, 28], are promising, none of
these studies attempted to improve the bug detection capa-
bilityofgeneratedtests.Moreover,ithasbeenacknowledged
intheliteraturethatwhiletestcoverageisausefulmetricfor
evaluating the quality of tests, it is weakly correlated with
the efficiency of tests in bug detection [10, 20, 22].
Mutation Testing (MT) is a white box testing technique
to assess the capability of a test in revealing bugs. MT has
been widely studied and successfully used in SE to assess
theeffectivenessoftestcases[25,40].MTinvolvesinjecting
artificialchanges based on realfaults into a PUT, resulting
inmutatedversionsofthe PUTknownasmutants.Themore
atestcasekillsmutants,themoreeffectiveitisinidentifying
real bugs. The surviving mutants highlight the weaknesses
of a test case and the ultimate goal is for the test cases to
be able to detect all mutants, i.e., kill them. Mutants are not
only useful for assessing the effectiveness of test cases but
canalsobeusedasameansfordesigningmoreeffectivetest
cases [17].
In this paper, we present the first study that leverages
MT to enhance and evaluate the effectiveness of test cases
generated by LLMs for Python programs in terms of fault
revealing capabilities. Our approach aims to optimize test
cases for bug detection rather than code coverage. Our pro-
posedtechnique, MuTAP,employsanLLMasitsmainCom-
ponent(LLMC)andstartsbyfeedingaprompttotheLLMC
in order to generate test cases. The initial prompt includes
thePUTand instructions for generating test cases by using
zero-shot andfew-shotlearning. Next, MuTAPassesses the
syntaxofthegeneratedtestcasesandre-promptsitsLLMC
to rectify any detected syntax issues. After fixing syntax
errors,MuTAPproceedstoappraisetheintendedbehaviorof
the generated test cases. This is achieved by comparing the
outputofthetestoraclesoncertaintestinputstotheexpected
returnvaluesofthePUTusingthesametestinputs,thereby
correcting any unintended behavior in the test oracles.
Subsequently, MuTAPapplies MT to examine the ef-
fectiveness of test cases in killing mutants of PUTs. As
surviving mutants highlight the limitation of the generated
test cases, MuTAPre-prompts its LLMC to generate new
test cases for the PUTs that have surviving mutants by
augmenting the initial prompt with both initial test cases
and the surviving mutants. MuTAPhalts the process of
augmentingtheinitialpromptwheneitherthefinaltestcases
can effectively detect all mutants or there are no surviving
mutants left that have not already been used to augment the
initial prompt.
We employ two types of LLMs as the LLMC of Mu-
TAP:Codex, which is designed for code-related tasks, and
llama-2-chat , which is optimized for dialog use cases and
versatileenoughtoaccommodatearangeoftasks,including
programming. We evaluate MuTAPon both synthetic bugs
of164PUTs[12]and 1710buggyprogramscollectedfrom
aPythonbugrepairingbenchmark[23].Ourresultsindicate
that our proposed approach generates effective test cases
with an average Mutation Score (MS, the ratio of killedmutants by the total number of mutants) of 93.57%, out-
performingbothPynguin(astate-of-the-artfully-automated
test generation tool) and the conventional LLM-based zero-
shot/few-shot learning techniques. Furthermore, our ap-
proach detects up to 468 (28%) more buggy code snippets
written by humans than other comparable methods in our
evaluation. Remarkably, it identifies 79 (17%) buggy code
snippets of humans that none of the other techniques are
abletodetect.Tosummarize,thispapermakesthefollowing
contributions:
•WepresentthefirststudyonleveragingMTtogener-
ate test cases with LLMs.
•We propose a prompt-based learning technique to
improvetheeffectivenessoftestcasesbyaugmenting
the prompts with both initial test cases and surviving
mutants of a PUT.
•We assess the effectiveness of generated tests in de-
tecting bugs in real and synthetic buggy versions of
PUTs.
•We make the proposed technique, MuTAP, publicly
availableonline[3]forotherresearchers/practitioners
to replicate or build upon our work.
Therestofthispaperisorganizedasfollows .Section2
introduces a motivating example. Section 3 describes the
differentstepsofourapproach.Wepresentourexperimental
setup, research questions, and experimental results in Sec-
tion 4. We discuss our findings and the potential use cases
of our approach in Section 5. Threats to the validity of our
results are reviewed in Section 6. We briefly review the
related works in Section 7. Finally, we conclude the paper
in Section 8; highlighting some avenues for future works.
2. Motivating Example
In this section, we present an example in Figure 1
showinghowourproposedapproachgenerateseffectivetest
cases. Suppose we have 10 mutants {𝑆𝑀0, 𝑆𝑀1, ..., 𝑆𝑀9}
for the Program Under Test, 𝑃 𝑈𝑇in Figure 1. The goal of
ourproposedtechnique, MuTAP(MutationTestcasegener-
ationusing Augmented Prompt),istogenerateeffectivetest
cases for 𝑃 𝑈𝑇in a way that ensures killing the maximum
number of mutants.
The function any_int() in Figure 1 receives 3 inputs
and returns Trueif all 3 inputs are integers, also one of
the inputs is equal to the sum of two others. Otherwise,
it returns False. In the first step, MuTAPuses the initial
prompt,1 ,torunaqueryontheLLMComponent(LLMC)
and generates initial test cases for this Program Under Test
(PUT). The component 2 in Figure 1 shows the initial test
casesgeneratedbyLLMCaftertherefiningstep.Wenamed
itInitialUnitTest, IUT.InSection3,wediscusstherefining
step (syntax and intended behavior fixing) of our approach
in detail. The IUTkills 6 out of 10 mutants of PUT. The 4
remaining mutants reveal the weaknesses of the generated
Arghavan MD et al.: Preprint submitted to Elsevier Page 2 of 16

--- PAGE 3 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
Figure 1: Different steps of MuTAPon a PUT. 2 is a set of test cases generated by the initial prompt 1 for 𝑃 𝑈𝑇, and4 is a
set of test cases obtained after augmenting the initial prompt with the surviving mutant, SM0.3′shows the mutant component
after updating with another surviving mutant of PUT0that we named SM1.
test, meaning that IUTneeds new test cases with assertion
to kill the injected bugs in those 4 mutants.
To address this limitation and generate more effective
testcases, MuTAPaugmentstheinitialpromptwithtwonew
components;thefirstoneistheresponseofthemodeltothe
initial prompt after fixing its syntax and intended behavior,
IUT, and the second one is the mutant component, 3 in
Figure 1. MuTAPinitiates the construction of the mutant
component by using the first “Survived Mutant” of PUT
that we refer to as SM0. The red highlight in SM0shows the
injected bug in 𝑃 𝑈𝑇. The injected bug changes the second
statement in the condition of the inner ifin𝑃 𝑈𝑇in a way
that the sum of the first and last input of function any_int()
is not equal to the middle input anymore. Since there is no
test case in IUTto verify that its middle input, y, is equal to
thesumofitsfirstandlastinputs, xandz,IUTisnotableto
kill this mutant.
MuTAPuses the concatenation of these three compo-
nents:1 ,2 , and3 to re-prompt the LLMC. The 4
component in Figure 1, shows the new set of test cases
generatedbyLLMCappendedto IUTaftertherefiningstep.
We named it Augmented Unit Test, AUT0. The unit test has
two more assertions compared to the IUTand one of them,
highlighted in red, kills the mutant, SM0.
MuTAPappliedAUT0to the mutants of 𝑃 𝑈𝑇again. If
there are any remaining surviving mutants, MuTAPiterates
theaugmentationprocessbyupdatingthemutantcomponent
with another surviving mutant if it has not been used to
augmentthepromptpreviously. MuTAPutilizeseachmutant
individually because sometimes new test cases that address
one mutant can also kill the remaining surviving mutants.
Moreover, due to the limited length of the prompt and non-
constant length of mutants, applying each surviving mutant
separately is a more practical approach. Figure 1 3′shows
an example of how the mutant component is updated using
anothersurvivingmutant.Wecallthismutant SM1.Unittest,
4′,showsanewsetoftestcasesincludingoneassertionthatdetectsSM1.MuTAPiteratestheaugmentationprocessuntil
eitherthefinaltestcasescankillallthemutants,orthereare
nosurvivingmutantsleftthathavenotalreadybeenusedto
augment the initial prompt.
Thefinaltestcasesgeneratedbyourproposedtechnique,
MuTAP, kill 9 out of 10 mutants of this example, 𝑃 𝑈𝑇,
and it increases the MS for 𝑃 𝑈𝑇from 60%(6 out of 10)
to90%(9 out of 10). This result can be compared to the
state-of-the-art automatic test generation tool for Python
programminglanguage[33],Pynguin,whichgeneratesatest
casefor 𝑃 𝑈𝑇withonlya 40%MS.Thistoolusesasearch-
based generation technique [5] and randomly mutates the
testvalueswithinatestcasetogeneratenewtestcases.The
random nature of this method results in a low chance of
generatinganewtestcasethatcankillthesurvivingmutants
ofPUT.
3. Approach
In this section, we discuss the different steps of our
approach. Figure 2 shows an overview of our proposed ap-
proachandAlgorithm1presentsthesequenceofitsdifferent
steps.
3.1. Initial Prompt
LLMsarecapableofperformingthosetasksthattheyare
alreadytrainedfor.Fine-tuningLLMstoperformanewtask
is computationally expensive. Also, there are LLMs such
as Codex that show a very good performance in generating
codebutsincetheyareclosed-source,fine-tuningthemfora
new task is impossible.
Prompt-basedlearning[31,52]isaneffectivetechnique
to adapt LLMs for new tasks. A prompt is a combination
of natural language and/or programming language context
and is used as an input to LLMs. There are studies showing
that putting a natural language instruction as a hint ( zero-
shot learning ) [29, 15, 41] or several examples ( few-shot
Arghavan MD et al.: Preprint submitted to Elsevier Page 3 of 16

--- PAGE 4 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
PUTs
Initial
PromptZero-shot
Learning
Few-shot
Learning
LLMCTest
Cases
Syntax FixerIntended
Behavior RepairMutation
Testing
MutPy
Y es
NoIs there any
surviving
mutants?Prompt
Augmentation
Oracle 
MinimizationSurviving
Mutants
Final
Test CasesRefining
Figure 2: The proposed methodology for generating and evaluating tests using LLMs.
Algorithm 1: MuTAP
Input:PUT,LLMC,initial_prompt_type
/* INS1, INS2, INS3, INS4and INS𝑓𝑖𝑥are global variable as natural
language instructions for the prompts */
Output:FUT // Final Unit Test
// Initial Prompt
1initial_prompt ←GenerateInitialPrompt (PUT,initial_prompt_type )
2raw_IUT ←LLMC(initial_prompt )
// Syntax Fixer and Intended Behaviour Repair
3IUT←Refining(raw_IUT,PUT)
// Mutation Testing
4MS,surviving_mutant ←MutationTesting (PUT,IUT)
5ifMS<100%then
// Prompt Augmentation
6AUT←AugmentingPrompt (MS,PUT,initial_prompt ,IUT,
surviving_mutant )
// Oracle Minimization
7FUT←OracleMinimization (AUT)
8else
// F: Oracle Minimization
9FUT←OracleMinimization (IUT)
10end
11returnFUT
learning) [35, 9, 1] in the prompt increases the capability
of LLMs in performing a new task.
MuTAPemploysboth zero-shot andfew-shotlearningto
buildtheinitialpromptandcallsLLMConthemseparately.
ThisstepisshowninAlgorithm2.Inmoredetail,weemploy
zero-shot andfew-shotas follows:
•zero-shot: The initial prompt generated by zero-shot
techniquecontainsthreeunits,followingtheapproach
in [29]. The component indicated by 1 in Figure 1
shows an example of such a prompt. The first unit in
thiscomponentisaninstructioninanaturallanguage
named 𝐼𝑁𝑆1anditclarifiesthetaskbyasking: “Gen-
erate test case for the following code” . The second
unit is the Program Under Test ( 𝑃 𝑈𝑇) and the last
unitisasetofinstructionsinaprogramminglanguage
namedINS2. TheINS2acts as a hint to indicate
the desired output for LLMC. The concatenation of
(𝐼𝑁𝑆1, 𝑃 𝑈𝑇 , 𝐼𝑁𝑆2)builds the initial prompt for
zero-shot learning (Line 2 in Algorithm 2).
•few-shot: Prompt generation based on few-shot
learning uses a chain of inputs and expected outputs
related to the downstream task. There are different
approachesforpresentingthepairofinputandoutput
in the prompt. We follow the approach in [1] to build
the initial prompt with few-shotstrategy in MuTAP.
Considering the maximum possible length of tokens
for LLMC (4k tokens in our study), few-shotpromptAlgorithm 2: GenerateInitialPrompt
Input:PUT,initial_prompt_type
Output:initial_prompt
1ifinitial_prompt_type =="zero-shot" then
2initial_prompt ←CONCAT( INS1,PUT,INS2)
3else
4ifinitial_prompt_type =="few-shot" then
5 initial_prompt ←CONCAT(pair( M,UT),PUT)// M: Method,
UT: Unit Test
6end
7end
8returninitial_prompt
includes two different demonstrative examples of a
Method (M) and a Unit Test (UT) as follows (Line 5
in Algorithm 2):
<code>M_1</code>\n<test>UT_1</test>\n
<code>M_2</code>\n<test>UT_2</test>\n
<code>PUT_i</code>\n<test>
There is no natural language description of PUTin the
initial prompt since such descriptions may not always be
available, and MuTAPrelies on the ability of LLMC to
synthesize code context. MuTAPcalls the initial prompt,
zero-shot orfew-shot,onLLMCandthenpassestheinferred
output to the next step (Line 2 in Algorithm 1).
3.2. Refining
In this section, we describe the process of refining the
generated test cases in MuTAPwhich includes fixing syn-
tactical errors and intended behavior repair. The details are
shown in Algorithm 3.
3.2.1. Syntax Fixer
The test cases generated by LLMC may have syntax
errors (missing brackets, uncompleted lines, etc.). Since
MuTAPneedstoexecutethetestfunctionforinvestigationon
MT and prompt augmentation, samples with syntax errors
become inefficient. However, sometimes a small change in
the output of LLMC can fix the syntactic error and convert
it into an executable test case.
MuTAPuses the capability of its LLMC to fix syntax
errors, similar to other studies [52, 26]. To do so, LLMC
is called on a new prompt to fix the syntax error in its
own output (Procedure SyntaxFixer in Algorithm 3). The
syntax fixing prompt consists of two parts. The first part
is a natural language instruction, 𝐼𝑁𝑆𝑓𝑖𝑥,“Fix the syntax
errorsinthefollowingcodesnippet” ,andthesecondpartis
Arghavan MD et al.: Preprint submitted to Elsevier Page 4 of 16

--- PAGE 5 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
the generated test function by LLMC on the initial prompt
(Line 7-8 in Algorithm 3). If the syntax error persists even
after re-prompting the LLMC, MuTAPemploys the Python
parsertoidentifytheerroneousline.Itthenretainsthelines
precedingtheproblematicline,ensuringtheyremainfreeof
syntax errors (Line 13 in Algorithm 3).
3.2.2. Intended Behavior Repair
Based on the initial prompt, LLMC generates different
testcasesthatareserializedasanassertionoraclebycalling
thePUTon certain inputs and comparing the returned
outputof PUTwiththeexpectedoutputorgroundtruth,for
example, {assert add (2,2) == 4} . However, it is possible
fortheLLMCtogeneratetestcasesthatareassertingwrong
returnvalues.Itmeansthatforsometestcases,LLMCdoes
notgeneratetheexpectedreturnoutputofthe PUT.Thelack
ofanaturallanguagedescriptionaboutthe PUTintheinitial
prompt could potentially lead to the generation of test cases
that do not accurately reflect the intended behavior of the
method.
The assertion with wrong return values may fail on
mutants, not because of detecting the bug, but because of
the unintended behavior of the assertion. These failures
causeconfusionabouttheeffectivenessoftestcases.So,this
step ofMuTAPaims at repairing the intended behavior of
assertionoraclesinthetestcases(Procedure IntendedBehav-
iorFixerin Algorithm 3).
For each assertion in the test, MuTAPruns the PUT
over the test inputs and compares the return output of PUT
with the asserting output. If the returned output of PUTis
the same as the asserting output in the oracle, then MuTAP
considers it as an assertion oracle with the correct intended
behavior.Otherwise,itrepairsthoseassertionsbyreplacing
the asserting output with the expected output of PUT(Line
22-27 in Algorithm 3). MuTAPomits those assertions for
which the input types failed on PUT, for example, if PUT
expected a listof integers but the test input is a string.
The final outcome of this step is named Initial Unit Test
(IUT)which is a set of test cases generated by LLMC after
refinement as shown by 2 in Figure 1.
3.3. Mutation Testing (MT)
MT assesses the quality and effectiveness of test cases.
Mutantsarebuiltbyinjectingartificialbugsintothe PUTto
simulatedefects.Iftestcasesfailedonamutant,weconsider
itasakilledmutant,otherwise,itsurvived,meaningthatthe
test cases within the unit test are not able to detect it. The
presence of surviving mutants highlights the shortcomings
of test cases, suggesting the need to either add a new test
case or improve an existing one. The Mutation Score (MS)
represents the effectiveness of test cases by calculating the
ratio of killed mutants out of all mutants of a PUT.
Algorithm 4 presents the details of this step. Inspired
by [32],MuTAPuses MutPy [21] to generate different mu-
tantsforeach PUTandcalculate MS(Line3-7inAlgorithm
4).Executingtestcasesoneachmutantinvolvesperforming
some preliminary setups. For this purpose, MuTAPuses
Python’s built-in “setuptools.find_packages” to locate andAlgorithm 3: Refining
Input:raw_IUT,PUT
Output:IUT // The Initial Unit Test after refining
1syntax_fixed_IUT ←SyntaxFixer (raw_IUT)
2IUT←IntendedBehaviorFixer (syntax_fixed_IUT ,PUT)
3returnIUT
4
5Procedure SyntaxFixer(raw_IUT)
6ifnotAST.parse(raw_IUT) then
7 syntax_fixed_prompt ←CONCAT ( INS𝑓𝑖𝑥,raw_IUT)
8 syntax_fixed_IUT ←LLMC(syntax_fixed_prompt )
9end
10syntax_fixed_IUT ←SyntaxCheck (syntax_fixed_IUT )
11returnsyntax_fixed_IUT
12
13Procedure SyntaxCheck(syntax_fixed_IUT)
14ifAST.parse(syntax_fixed_IUT) then
15 returnsyntax_fixed_IUT
16else
17 return SyntaxCheck (syntax_fixed_IUT [:error_line] )
18end
19
20Procedure IntendedBehaviorFixer(syntax_fixed_IUT, PUT)
21fixed_IUT ←{}
22foreachtest_case ∈syntax_fixed_IUT do
23 expected_output ←PUT(test_case.input)
24 ifexpected_output ≠test_case.output then
25 test_case.output←expected_output
26 fixed_IUT .append(test_case)
27end
28returnfixed_IUT
Algorithm 4: MutationTesting
Input:PUT,IUT
Output:MS,surviving_mutant
1mutants←MutPy(PUT)
2surviving_mutant ←{}
3foreachMUT ∈mutantsdo
4ifexec(MUT, IUT) then
5 surviving_mutant.append( MUT)
6end
7end
8MS←(#(mutants ) − #(surviving_mutant ))∕#(mutants )
9returnMS,surviving_mutant
install the required packages, such as "math", "numPy",
"pandas", "pytest", and others. Additionally, MuTAPimple-
ments setup functions that are responsible for creating tem-
porarydirectories,whichareutilizedduringtheexecutionof
the test cases on the mutants. After executing the test cases
on the mutants and calculating the MS,MuTAPproperly
tears down the setup by removing the temporary directory.
As shown on Line 5-9 in Algorithm 1, if the MSof a
PUTreaches100%,MuTAPpasses test cases to the oracle
minimizationstep(Subsection3.5),otherwise,itcollectsthe
list of surviving mutants and transfers the mutants to the
prompt augmentation step (Subsection 3.4).
3.4. Prompt Augmentation
Algorithm5showsthedetailsofthisstep.Ifthereisany
surviving mutant from the previous step, MuTAPaugments
theinitialprompt, zero-shot orfew-shot,byaddingfournew
components (Line 3 in Algorithm 5). The first component
isIUT, the initial unit test generated by LLMC after refine-
ment. The second component is an instruction in a natural
language named INS3that clarifies the shortcoming of IUT
by“The test function, test(), cannot detect the fault in the
followingcode” .Thethirdcomponentisoneofthesurviving
mutants of the PUT, namedSM. The last component, INS4
Arghavan MD et al.: Preprint submitted to Elsevier Page 5 of 16

--- PAGE 6 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
Algorithm 5: AugmentingPrompt
Input:MS,PUT,initial_prompt ,IUT,surviving_mutant
Output:AUT // Augmented Unit Test
1ifMS<100%or surviving_mutant ≠{}then
2SM←surviving_mutant .pop()
3augmented_prompt ←CONCAT ( initial_prompt ,IUT,INS3,SM,
INS4)
4raw_AUT ←LLMC(augmented_prompt )
5fixed_AUT ←Refining(raw_AUT ,PUT)
6AUT←IUT.append(fixed_AUT )
7MS←MutationTesting (AUT,PUT)
8return AugmentingPrompt (MS,PUT,initial_prompt ,AUT,
surviving_mutant )
9else
10returnAUT
11end
is an instruction in natural and programming language:
the natural language context clarifies the task by asking to
“Provide a new test case to detect the fault in prior code”
andtheprogramminglanguagecontextactsonlyasahintto
guideLLMCforgeneratingtheoutput.Anexampleisshown
by3 in Figure 1.
MuTAPre-prompt LLMC and repeats the refining step
on the generated output (Line 4-5 in Algorithm 5). Then,
it appends new generated test cases to the IUTthat we
call Augmented Unit Test ( 𝐴𝑈𝑇). The 𝐴𝑈𝑇is passed to
theMTstep (Line 7 in Algorithm 5). MuTAPrecursively
repeats prompt augmentation till either the final test cases
kill all the mutants ( MS=100%) or there is no surviving
mutant that is not used in the augmentation process (Line
8 in Algorithm 5). An example of updating the mutant
component in Figure 1, 3 is changed to 3′by replacing
SM0withSM1. The4′indicates the generated test cases
withLLMCafteriteratingtheprocessonthenextsurviving
mutant.
3.5. Oracle Minimization
The test cases generated by the LLMC usually consists
ofredundantassertions.Also,theaugmentationprocessmay
addmoreredundantassertionstothefinalunittest.Present-
ing all of them (with redundancy) as the final output can
cause confusion for developers. In the final step, similar to
previoustoolsthatgeneratemutation-driventestoracles[18,
17],MuTAPminimizesthenumberofassertionsbyutilizing
aGreedytechniquetoeliminatetheredundantassertionsthat
do not improve the MS. This step is presented in Algorithm
6.MuTAPstartsbytrackingthenumberofmutantsthateach
assertion kills and then chooses the test case containing the
assertion that kills the maximum number of mutants. This
process is then repeated by adding the test cases containing
the next assertions that detect the most mutants (Line 4-10
in Algorithm 6). If adding this new assertion increases the
MS,MuTAPkeepthetestcaseanditsassertion.Otherwise,
the test case will be discarded as redundant.
4. Evaluation
In this section, we describe the evaluations we designed
and conducted to investigate the following research ques-
tions:Algorithm 6: OracleMinimization
Input:PUT,AUT
Output:FUT
1MS_old←0
2FUT←{}
3sorted_AUT ←sort(AUT) // sort each test case in AUT based on the MS
4foreachtest_case ∈sorted_AUT do
5FUT.append(test_case)
6MS←MutationTesting (FUT,PUT)
7ifMS>MS_oldthen
8 MS_old←MS
9else
10 FUT.delete(test_case)
11end
12end
13returnFUT
RQ1How effective are test cases generated by MuTAPin
comparison to test cases generated by automatic test
generation tools?
RQ2How do the different parts of MuTAPperform?
RQ3Whatistheperformanceof MuTAPforeachmutation
type?
4.1. Experimental Setup
Inthissection,wepresentourexperimentsetup.Specif-
ically, we describe the automatic test generation tool used
to compare our results, clarify the LLMC of MuTAPand
itssetup,andindicatethebaselinesandbenchmarkdatasets
used in our experiments.
We conducted the experiment on the Cedar cluster of
Compute Canada, which offers 32 cores CPU, 1TB storage,
andonev100lGPUwith32GBGPUMemory,andonasys-
tem running Linux 5.15.0-69-generic with AMD FX(tm)-
6300 Six-Cores CPU, 512GB storage, and 16GB Memory.
4.1.1. Experimental Parameters
We call the initial prompt, zero-shot orfew-shot, on
LLMC up to 10times and collect the outputs that meet two
criteria as candidate test cases: the candidate should consist
of two keywords of assertand thefunction name ofPUT.
If after 10 runs, LLMC is not able to generate an output
that contains those two keywords, we consider the task as
a problematic task or a task for which MuTAPis not able to
generate a test case.
Regarding the syntax fixing step, we run the syntax
fixing prompt on the LLMC for up to 10 runs. If the syntax
error remains unresolved even after 10 iterations, MuTAP
employs the Python parser to locate the erroneous line. It
thenretainsthelinesprecedingthebuggyline,ensuringtheir
freedomfromsyntaxerrors.Iftheremovaloflinesresultsin
the absence of any remaining test cases (all test cases prove
non-compilable), we classify the task as problematic.
4.1.2. Comparable Tool
Pynguin [32] is a well-known fully-automated test gen-
erationtoolforadynamicallytypedprogramminglanguage
such as Python. It uses different search-based algorithms
toward satisfying code coverage criteria, i.e., branch cov-
erage. Pynguin first takes a Python code (method, module,
etc.) as input and collects its information such as variable
Arghavan MD et al.: Preprint submitted to Elsevier Page 6 of 16

--- PAGE 7 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
types, method names, and dependencies. Then it uses one
of the search-based test generation algorithms (MIO [4],
MOSA[37],DynaMOSA[38],etc.)togeneratetestcases.It
randomlymutates(deletes,inserts,replaces)differentvalues
andstatementswithinthetestcasetogeneratenewtestcases
andexecutesthemoverthe 𝑃 𝑈𝑇toensuretheircorrectness.
Finally, it generates assertions for test cases using a MT
engine [32].
For our experiments, we employ Pynguin 0.17.0. with
the DynaMOSA [38]. According to the evaluation of Pyn-
guin [38], DynaMOSA shows the best performance com-
paredtotheotheralgorithmingeneratingtestcaseswiththis
tool. We set the timeout of test generation to 600 seconds
which is the default setting of the tool.
4.1.3. Large Language Model Component (LLMC)
We employ two different LLMs as the LLMC of Mu-
TAP.ThefirstoneisOpenAI’sCodex,designedspecifically
for code generation tasks [12]. We use Code-davinci-002 ,
with a temperature of 0.8. The lower temperature causes
less variation in the outputs of the model while the higher
temperature increases the variation of output and then the
chance of generating useful test cases over different itera-
tions.TheevaluationofCODAMOSA[29]showsthat0.8is
a reasonable temperature to generate useful test cases with
Codex.
The second LLM is Meta’s llama-2-chat , which has
been iteratively refined using Reinforcement Learning with
Human Feedback (RLHF) and is appropriate for dialog use
cases[48].SimilartoCodex,wehaveconfiguredthemodel’s
temperaturetobe0.8.Furthermore,themodelprovidesthree
distinct roles within the prompt: system, user, andassistant.
Theserolesservethepurposeofclarifyingeachcomponent
oftheprompttothemodelbyassigningspecificcomponents
to each role. Different combinations of these roles can be
utilized in each prompt to tailor the interaction with the
model according to the specific requirements [48].
In our experiments, the role of the systemis defined as
{You are a Python coding assistant. Always answer with
Pythoncode.} ,foralltypesofprompts,including zero-shot,
few-shot, andaugmented prompts. To handle the zero-shot
prompt,weonlysetthe user’srolecontenttobeaconcatena-
tionof(INS1,PUT𝑖,INS2).Forthefew-shotprompt,wede-
finethecontentofthe assistantroleasasetofdemonstrative
examplesofMethod(M)andUnitTest(UT),whilethe user
rolecontentissetto PUT𝑖.Asforthe augmented prompt,its
various components are set up as follows:
{user: Initial Prompt,
assistant: IUT,
user: concat(INS3, SM𝑖, INS4)}
For both LLMs, the maximum number of generated
tokensissetto250forgeneratingtestcasesand20tokensfor
syntaxfixing,basedonpreviousstudiesonsimilartasks[29,
45]. The stop word is defined as quote (“) forzero-shot
and as <∕𝑡𝑒𝑠𝑡 >forfew-shotprompt. For the rest of the
hyperparameters, we keep the model’s default values.To avoid overfitting on the benchmarks data, MuTAP
repeats all prompts on Codex or llama-2-chat for up to 10
runs. If after 10 runs, the requirement for generating test
cases is not satisfied, MuTAPconsiders it as a problematic
or unsolved task.
Itisimportanttonotethat MuTAPisnotlimitedtothese
two models, and its LLMC can be replaced with any other
LLM as required.
4.1.4. Baselines
In addition to Pynguin, we propose two baselines for
each LLM to evaluate our proposed method, MuTAP.
Before-refining: Thefirstbaselineistheoutputoftheinitial
prompt on LLMC (Codex or llama-2-chat), without fixing
syntax errors or repairing the intended behavior. Since as-
sertions with unintended return values can fail on mutants
or buggy code and present invalid effectiveness, we omit
those assertions in this baseline to avoid this side effect.
If the output of the model has syntax errors, we consider
it as a wrong test and consequently consider the task as a
problematic or unsolved task.
After-refining: The second baseline is the output of the
initial prompt on LLMC (Codex or llama-2-chat), after
applying the following steps: Refining(Subsection 3.2) and
Oracle Minimization (Subsection 3.5).
4.1.5. Mutant Generator
To apply MT, we need to generate different mutant
versions of a PUTby injecting bugs into its different lines.
Forthispurpose,weuse MutPyversion2.0[21]. MutPyisa
MT tool for code in Python 3.3+. It benefits from different
mutation operators to generate the mutants. The list of mu-
tation operators used in our experiment with corresponding
examplesisshowninTable1. MutPyinjectsoneoperatorat
atimetogeneratethemutantiftheoperatorisapplicableon
PUT.
4.1.6. Benchmark Datasets
Toconductourexperiments,weusetwodifferentbench-
marks. The first one is HumanEval [12] which is a bench-
marktoevaluateLLMsthatgeneratecode.Ithas164human-
written programming problems at easy to medium levels.
Each problem has different attributes such as descriptions
and reference solutions. We use the reference solution of
each task as a PUT.
The second one, Refactory [23], is a benchmark for
Python bug repairing [24]. It has 1710 buggy students’
submissions for 5 assignments of a Python programming
course.Eachassignmenthasacorrectreferencesolutionthat
weuseas PUT.Theadvantageofthisdatasetisbuggycode
snippets generated by humans that give us the opportunity
to evaluate test cases generated by MuTAPon real bugs and
compare it with Pynguin and our baselines.
4.2. Experimental Results
In this section, we discuss our findings for each RQ.
Arghavan MD et al.: Preprint submitted to Elsevier Page 7 of 16

--- PAGE 8 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
Table 1
List of the mutation operators in our experiments used by 𝑀𝑢𝑡𝑃 𝑦sorted by alphabetical order.
Operator Example Mutant
AOD - arithmetic operator deletion result.append(numbers[-1]) result.append(numbers[1])
AOR - arithmetic operator replacement return number % 1.0 return number * 1.0
ASR - assignment operator replacement current_depth += 1 current_depth -= 1
BCR - break continue replacement if i % j != 0: break if i % j != 0: continue
COD - conditional operator deletion if not string: return ’ ’ if string: return ’ ’
COI - conditional operator insertion if balance <0: return True if (not balance <0): return True
EHD - exception handler deletion except: pass except: raise
EXS - exception swallowing except: return False except: pass
LCR - logical connector replacement if s[-1] == ’y’ or s[-1] == ’Y’: if s[-1] == ’y’ and s[-1] == ’Y’:
ROR - relational operator replacement if c[n] <=1: if c[n] >=1:
SIR - slice index remove l[::3] = sorted(l[::3]) l[::3] = sorted(l[:])
Table 2
Evaluation result of test cases generated by MuTAPand other methods on synthetic buggy programs.
Prompt Model Method # Test Cases (avg)# Problematic PUT
(out of 164)MS (%)# Killed Mut
(out of 1260)Task MS=100%
(out of 164)
- - Pynguin 1.5 (min=1, max=4) 31 65.94% 649 28.22% (46)
Before-refining 1.5 (min=1, max=3) 73 72.15% 296 11.04% (18)
Zero-shot Codex after-refining 2.1 (min=1, max=3) 30 76.82% 749 24.54% (40)
MuTAP 2.5 (min=1, max=4) 30 89.13% 869 41.72% (68)
Before-refining 1.2 (min=1, max=3) 68 62.60% 318 17.79% (29)
Zero-shot llama2-chat After-refining 2.2 (min=1, max=5) 0 84.04% 1059 53.98% (88)
MuTAP 2.5 (min=1, max=5) 0 91.98% 1159 68.09% (111)
Before-refining 1.5 (min=1, max=3) 39 72.68% 508 15.95% (26)
Few-shot Codex After-refining 2.2 (min=1, max=3) 27 82.73% 829 34.97% (57)
MuTAP 2.6 (min=1, max=7) 27 92.02% 922 49.69% (81)
Before-refining 1.5 (min=1, max=3) 60 64.51% 325 22.69% (37)
Few-shot llama2-chat After-refining 2.5 (min=1, max=5) 0 85.16% 1073 75.05% (93)
MuTAP 2.6 (min=1, max=7) 0 93.57% 1179 69.93% (114)
4.2.1. RQ1: How effective are test cases generated by
MuTAPin comparison to test cases generated by
automatic test generation tools?
SinceourstudyfocusesonMTtoimprovetheeffective-
ness of test cases, we compare MuTAPwith Pynguin and
ourbaselinesintermsofMS,numberofkilledmutants,and
number of PUTwith 100%MS. It is worth mentioning that
we only consider PUTs with correct test cases to calculate
the average MS for each method. For this reason, we report
the total number of killed mutants and the total number of
PUTs with 100%MS for a fair comparison.
Table 2 shows the obtained results for the HumanEval
benchmark. Prior to syntax fixing and intended behavior
repair (before-refining ), the test cases generated by Codex
and llama-2-chat are incorrect for 73 and 68 (out of 164)
PUTs,respectively,whenusingthe zero-shot initialprompt.
However, they manage to kill 295 and 318 mutants (out of
1260), respectively.
Theinitialprompthasamorepronouncedimpactonthe
output of Codex compared to llama-2-chat. Switching the
initial prompt to few-shot decreases the number of PUTs
without test cases to 39, while also raising the number of
killed mutants to 508 when using Codex as LLMC. On
the other hand, when using llama-2-chat, the number of
PUTs without test cases reduces to 60, and the number of
killed mutants increases from 318 to 325. This differencein performance could be attributed to llama-2-chat being
more suitable for dialog prompts, and using a prompt with
a pair of demonstrative input and output, devoid of natural
languagecontext,doesnotimprovethemodel’sperformance
significantly.
In contrast, Pynguin, as the state-of-the-art automatic
test generation tool, outperforms the output of both LLMs,
before-refining, by killing 649 mutants and failing to gener-
ate test cases for 31 tasks.
Afterapplyingthepost-processingstepsofsyntaxfixing
and intended behavior repair, MuTAPwith both LLMs per-
form better than Pynguin in terms of killing more mutants.
Notably, when using both zero-shot andfew-shotprompts,
llama-2-chat is able to generate correct test cases for all
PUTs, after-refining. However, their effectiveness in terms
of killing mutants is measured at 84.04% and 85.16% with
thezero-shot andfew-shotprompts, respectively.
On the other hand, the MS of test cases generated by
Codex after refining is 76.82% and 82.73% with the zero-
shotandfew-shot prompts, respectively. Despite this im-
provement, Codex still fails in generating correct test cases
for 30 (with zero-shot) and 27 (with few-shot)PUTs after
refining.
MuTAP, enhances the effectiveness of test cases gen-
erated by both LLMs, Codex and llama-2-chat, achieving
MS of 89.13% and 91.98% with the zero-shot prompt, and
Arghavan MD et al.: Preprint submitted to Elsevier Page 8 of 16

--- PAGE 9 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
Table 3
Evaluation results on realbuggy programs.
Prompt Model Method # Test Cases (avg)Bug Detected
(out of 1710)
- - Pynguin 1.25 (min=1, max=4) 67.54% (1155)
After-refining 1.2 (min=1, max=2) 79.87% (1356)
Zero-shot CodexMuTAP 1.6 (min=1, max=3) 84.03% (1437)
After-refining 1.2 (min=1, max=3) 86.43% (1478)
Zero-shot llama-2-chatMuTAP 2.2 (min=1, max=4) 93.22% (1594)
After-refining 1.6 (min=1, max=3) 82.51% (1411)
Few-shot CodexMuTAP 2.2 (min=1, max=4) 89.41% (1529)
After-refining 2.1 (min=1, max=4) 88.42% (1512)
Few-shot llama-2-chatMuTAP 2.2 (min=1, max=4) 94.91% (1623)
an MS of 92.02% and 93.57% with the few-shot prompt,
respectively. Particularly, MuTAPwith thefew-shotprompt
when using llama-2-chat as its LLMC manages to kill 1179
mutantsoutof1260andgeneratestestcaseswithMS=100%
for up to 70% of PUTs, demonstrating a remarkable im-
provementintheeffectivenessoftestcasescomparedtothe
Pynguin with 649 killed mutants and 28.22% PUTs with
MS=100%.
Table3thatshowstheresultsonthehumans’realbuggy
programs from Refactory benchmark confirms our findings
onHumanEval .Toevaluate MuTAPonrealbuggycode,we
apply the following steps. First, we generate the mutants of
eachPUTin this dataset. Second, we conduct the prompt
augmentation process and finalize the test cases for each
PUT. Then, we apply test cases generated by MuTAPon
students’ buggy code in Refactory , followed by test cases
generatedbyPynguinandLLMs After-refining ,toassessthe
effectivenessoftestcasesgeneratedbydifferentmethodsin
detecting buggy code.
MuTAPwithfew-shotlearningwhileusingllama-2-chat
as its LLMC identifies 468more buggy code compared
to Pynguin (with an MS of 94.91%vs.67.54%) and 111
more buggy code compared to After-refining (with an MS
of94.91%vs.82.51%). Furthermore, MuTAPdiscovers 79
buggy code that were not detected by either Pynguin or
llama-2-chat’stestcases After-refining process.Whenusing
Codex,MuTAPdetects 73buggy code that were missed by
both Pynguin and Codex’s test cases After-refining stage.
Moreover, MuTAPexcels in generating more effective test
cases,withanaverageof 2.6testcasesafterapplyinggreedy
optimization.
Overall,MuTAPusing both llama-2-chat and Codex
demonstrates better performance compared to Pynguin in
terms of killing mutants and detecting buggy code. The
effectiveness of these test cases in detecting defects is im-
provedthroughpost-processingstepsofrefiningandprompt
augmentation.Finding 1: MuTAPgenerates more effective test
cases compared to Pynguin and conventional zero-
shotandfew-shot learning on LLM. The number
ofMuTAP’s test cases is not much greater than the
output of other methods after minimization. Addi-
tionally, LLM with dialog setup performs better on
the augmented prompt. In conclusion, the effective-
ness of LLM-generated test cases can be enhanced
through prompt augmentation using surviving mu-
tants and post-processing refinement.
4.2.2. RQ2: How do the different parts of MuTAP
perform?
Syntax Fixer: On average, the percentage of test cases
with syntax errors is 38.98%and26.48%when using the
zero-shot andfew-shotprompts, respectively, with Codex.
When employing llama-2-chat, this percentage is 33.85%
and26.32%withthezero-shot andfew-shotprompts,respec-
tively.
Whenconsideringsyntaxerrors,threefactorscontribute
to decreasing them in the output of LLMs. The first factor
is the type of initial prompt. As shown in Table 4 on the
HumanEval benchmark, few-shotlearning results in fewer
syntax errors in the output of both LLMs. Specifically,
whenusingCodex,thepercentageofsyntaxerrorsdecreases
from 44.79%to29.03%after-refining, and for MuTAP, it
decreases from 33.17%to23.93%. With llama-2-chat as
the LLMC, the percentage of syntax errors decreases from
38.03%to26.99%afterrefining,andfrom 29.66%to25.64%
forMuTAP.
The second impactful factor, which is also the primary
factor,isthe SyntaxFixing component.AsshowninTable4,
when using Codex, this component in MuTAPon average
fixes 14.5%of syntax errors by utilizing the LLMC and
addresses 81.37%of syntax errors by omitting the lines
causing the errors. On the other hand, when using llama-2-
chatastheLLMCof MuTAP,theSyntaxFixing component,
on average, resolves 32.31%of syntax errors through re-
promptingtheLLMC,and 60.73%oftheerrorsbyomitting
the problematic lines.
The final factor contributing to the improvement of
syntax errors in test cases is the prompt augmentation pro-
cess inMuTAP. By augmenting the prompt with IUT, the
occurrence of syntax errors in the output of Codex with
thezero-shot technique decreases from 44.79%to33.17%.
Similarly, with llama-2-chat and the zero-shot prompt, the
percentageofsyntaxerrorsreducesfrom 38.03%to29.66%.
Augmenting the prompt with IUTprovides illustrative ex-
amples of test cases and serves a similar purpose to the
demonstrative examples in the few-shot learning prompt,
effectively reducing syntax errors in the output of LLMs.
Our finding on the Refactory benchmark shows MuTAP
generatestestcaseswithsyntaxerrorsinonlyone PUT(out
of5)usingCodexand zero-shot learning.Moreover,noneof
those syntax errors could be fixed by re-prompting LLMC.
Arghavan MD et al.: Preprint submitted to Elsevier Page 9 of 16

--- PAGE 10 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
Table 4
Syntax error fixing of test cases. The syntax Error Rate shows the ratio of unit tests with syntax errors.
Model Method Prompt # Run (avg) Syntax Error Rate Fixed by Model Fixed by Omitting Lines
Zero-shot 9.1 44.79% 16.44% 60.27%After-refiningFew-shot 9.5 29.03% 12.96% 83.33%
CodexZero-shot 9.7 33.17% 16.18% 79.41%MuTAPFew-shot 9.5 23.93% 12.82% 84.62%
Zero-shot 7.1 38.03% 30.64% 63.86%After-refiningFew-shot 6.8 26.99% 31.81% 57.96%
llama2-chatZero-shot 6.9 29.66% 32.17% 61.05%MuTAPFew-shot 6.8 25.64% 32.45% 60.40%
On the other hand, for both initial prompt types, syntax
errors decrease to zero using llama-2-chat.
Intended Behavior Repair: In the case of repairing
intendedbehavior,twodistinctfactorscontributetoreducing
the error rate in assertion oracles. As shown in Table 5, the
Intended Behavior Repair step, when using Codex as the
LLMC, on average, fixes 83.98%and89.86%of incorrect
behaviors in the after-refining andMuTAP, respectively.
When utilizing llama-2-chat, this step repairs 84.35%and
95.96%of unintended behavior in the after-refining and
MuTAP, respectively.
In addition to the Intended Behavior Repair step, the
prompt augmentation step in MuTAPsignificantly reduces
the occurrence of unintended behavior in test cases. For
instance, when using Codex with a zero-shot prompt, the
assertions with unintended behavior, such as wrong return
values, decrease from 63.63%to19.38%. Similarly, with
llama-2-chatandusing few-shotprompt,theassertionswith
unintended behavior decrease from 63.25%to10.75%. The
reason behind this improvement could be attributed to the
usageofIUTs(InitialUnitTests)in MuTAPforaugmenting
theinitialprompt.These IUTsalreadyrepresenttheintended
behavior of the 𝑃 𝑈𝑇, thereby assisting the LLM in sug-
gesting test cases with less unintended behavior (i.e., fewer
wrong return values). Also, on the Refactory benchmark,
MuTAPrepairedallassertionswithincorrectbehavioronthe
output of augmented prompts.
Unlike syntax errors, the prompt type does not signif-
icantly help with unintended behavior in assertions. The
combination of the Intended Behavior Repair step and the
prompt augmentation process improves the effectiveness
of test cases, ensuring that they align with the intended
behavior of PUT.
Surviving Mutants Representation: We also investi-
gated the impact of surviving mutants’ order on MS dur-
ing prompt augmentation. Figure 3 illustrates the effect of
augmenting the prompt with a random order of surviving
mutants over 5runs for all PUTs. For this comparison, we
randomly selected one of the surviving mutants of each
PUTwithMS<100%and utilized it to augment the
initial prompt. We then calculated the average MS for all
PUTs.Subsequently,werandomlychosethesecondsurviv-
ing mutant for the remaining PUTs with MS <100%(if
any), repeated the augmentation process, and calculated the
average MS for all PUTs again. We continue repeating this
process until either there are no more PUTs with MS <Table 5
Evaluation results of Intended Behavior Repair . The Assertion
Error Rate shows the ratio of assertions with wrong behavior.
Model Method PromptAssertion
Error RateRepaired Not Repaired
Zero-shot 63.63% 82.21% 17.79%After-refiningFew-shot 62.84% 85.75% 14.25%
CodexZero-shot 19.38% 89.71% 10.29%MuTAPFew-shot 18.36% 90.00% 10.71%
Zero-shot 60.27% 81.80% 18.19%After-refiningFew-shot 63.25% 86.90% 13.09%
llama-2-chatZero-shot 23.40% 94.06% 5.94%MuTAPFew-shot 10.75% 94.91% 5.09%
100%ornomoresurvivingmutantthatisnotutilizedinthe
argumentation process.
As shown in Figure 3, each data point represents the
average MS for all PUTs over 5 runs of a random selection
of surviving mutants. Notably, more than 90%of the MS is
achieved by using only half of the surviving mutants, and
the improvement in MS stalls after a certain repetition of
theaugmentationstepindifferentLLMs.Forexample,when
using Codex as LLMC, in zero-shot learning, the MS stops
improving even though, on average, 27surviving mutants
(outof 226)arenotutilizedinthepromptaugmentationstep.
Similarly, in few-shotlearning, this number is equal to 24
(out of 106).
Our results for RQ2 demonstrate that test cases gener-
ated by LLMs, regardless of the prompt type, require post-
processing, such as syntax correction or intended behavior
repair, in order to function properly and detect bugs effec-
tively. Also, the order of surviving mutants to augment the
prompt does not significantly impact the MS gain.
Finding 2: TheSyntax Fixing andIntended Behav-
ior Repair fix up to 95.94%and89.86%of syntax
and functional errors in test cases, respectively. The
promptaugmentationin MuTAPdecreasestheunin-
tendedbehaviorintheoutputofLLMssignificantly
(44.36%using Codex and 52.5%using llama-2-
chat).Furthermore,onlyasmallnumberofmutants
(up to 27) do not contribute to the improvement of
MS.
4.2.3. RQ3: What is the performance of MuTAPfor
each mutation type?
In this RQ, we evaluate the performance of MuTAP
in different mutant types. We report the total number and
Arghavan MD et al.: Preprint submitted to Elsevier Page 10 of 16

--- PAGE 11 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
Table 6
Evaluation of killed mutants for each type of injected operator into PUTs.
PynguinZero-shot Few-shot
Codex llama-2-chat Codex llama-2-chat
After-refining MuTAP After-refining MuTAP After-refining MuTAP After-refining MuTAP
Type killed total killed total killed total killed total killed total killed total killed total killed total killed total
AOD13 (39.39%) 33 20 (62.50%) 32 28 (87.50%) 32 36 (80.00%) 45 39 (86.67%) 45 27 (79.41%) 34 32 (94.12%) 34 37 (82.22%) 45 40 (88.89%) 45
AOR248 (67.39%) 368274 (74.66%) 367336 (91.55%) 367390 (87.05%) 448410 (91.52%) 448290 (77.33%) 375347 (92.53%) 375394 (87.95%) 448417 (93.08%) 448
ASR45 (60.00%) 75 56 (74.67%) 75 60 (80.00%) 75 74 (88.10%) 84 79 (94.05%) 84 57 (76.00%) 75 64 (85.33%) 75 75 (89.29%) 84 79 (94.05%) 84
BCR 2 (40.00%) 52 (40.00%) 52 (40.00%) 55 (55.56%) 95 (55.56%) 92 (40.00%) 52 (40.00%) 55 (55.56%) 96 (66.67%) 9
COD 8 (53.33%) 15 12 (80.00%) 15 15 (100.00%) 15 15 (68.18%) 22 16 (72.73%) 22 15 (88.24%) 17 17 (100.00%) 17 15 (68.18%) 22 17 (77.27%) 22
COI130 (81.76%) 159145 (91.19%) 159154 (96.86%) 159194 (85.46%) 227216 (95.15%) 227161 (96.99%) 166164 (98.80%) 166200 (88.11%) 227218 (96.04%) 227
EHD1 (100.00%) 1 0 (0.00%) 0 0 (0.00%) 0 1 (50.00%) 2 2 (100.00%) 2 0 (0.00%) 1 1 (100.00%) 1 1 (50.00%) 2 2 (100.00%) 2
EXS 0 (0.00%) 00 (0.00%) 11 (100.00%) 11 (100.00%) 11 (100.00%) 10 (0.00%) 11 (100.00%) 11 (100.00%) 11 (100.00%) 1
LCR14 (45.16%) 31 22 (70.97%) 31 23 (74.19%) 31 30 (69.77%) 43 37 (86.05%) 43 24 (72.73%) 33 27 (81.82%) 33 32 (74.42%) 43 39 (90.70%) 43
ROR174 (66.67%) 261200 (76.92%) 260227 (87.31%) 260281 (84.13%) 334316 (94.61%) 334227 (86.97%) 261239 (91.57%) 261282 (84.43%) 334320 (95.81%) 334
SIR10 (33.33%) 30 18 (60.00%) 30 23 (76.67%) 30 32 (71.11%) 45 38 (84.44%) 45 26 (76.47%) 34 28 (82.35%) 34 31 (68.89%) 45 40 (88.89%) 45
Total645 (65.95%) 978749 (76.82%) 975869 (89.13%) 9751059 (84.05%) 12601159 (91.98%) 1260829 (82.73%) 1002922 (92.02%) 10021073 (85.16%) 12601179 (93.57%) 1260
numberofkilledmutantsbyeachmethodonthe HumanEval
benchmark in Table 6. The performance of all techniques
per mutant type is reported to help the comparison as well.
The total number of mutants in each type is different for
each method since the number of problematic PUTs is not
the same for all methods. The MS for each type/method
indicates the ratio of killed mutants out of the total number
ofmutantsinthattype.Therearesomemutanttypesthatare
more common (more samples in those types) such as AOR,
COI, andROR(an example for each mutant type is shown
in Table 1). The number of mutants in each type depends
on thePUT. For example, in the HumanEval , there are few
PUTs with exception handling. Consequently, there are few
mutants in the EHD.
In general, MuTAPshows better or similar performance
in all mutant types compared to Pynguin and the output
of LLMs After-refining of both LLMs. Considering ASRas
an example, MuTAPshows the highest performance on this
mutant type among all methods. For example, test cases
generatedbyPynguinidentified45mutantsinthiscategory
whiletestcasesgeneratedby MuTAPusingllama-2-chatand
thefew-shotprompt identified 79 mutants in this category
(out of 84).
Foroneofthemutanttypes, BCR,whichisararetypein
our benchmarks, MuTAPandAfter-refining with both zero-
shotandfew-shotinitial prompt, along with using Codex,
show the same performance. However, when employing
llama-2-chat, MuTAPoutperformstheothersbykillingmore
mutants of this type. For another rare type of mutant in our
dataset,EHD, it is noteworthy that Codex, despite using
bothinitialprompttypesandtheaugmentationprocess,fails
to generate test cases to detect the two mutants present in
this category. In contrast, MuTAPwith thefew-shotinitial
prompt and llama-2-chat successfully killed all of the mu-
tants in this category.
Finding 3: The test cases generated by MuTAPare
equally or more effective in killing different types
of mutants compared to those generated by Pyn-
guin and the baseline method. Also, using an LLM
withdialogsetupcanincreasethenumberofkilling
mutants in different mutant types while applying
prompt augmentation.
Figure 3: The impact of utilizing surviving mutants in different
random orders on the MS for different LLMs. Each data point
represents the average MS for all PUTs across five different
runs, wherein the surviving mutants were randomly selected
for the prompt augmentation process.
5. Discussion
5.1. Automatic Test Case Generation
MuTAPleverages the code synthesis capabilities of
LLMs and employs prompt-based learning to assist devel-
opers in generating effective test cases without the need for
the computationally expensive fine-tuning of LLMs.
LLMs are able to generate test cases that are more
effective than those generated by Pynguin in terms of re-
vealing bugs. Listing 1 shows a sample test case generated
by Pynguin for the PUTof our motivation example in
Section 2. While Pynguin generates test inputs as random
integersandmutatesthosevaluestogeneratenewtestcases,
LLMs produce test cases that are more natural-looking and
correlatedwithinput/outputtypeandthefunctionalityofthe
PUT. However, test cases generated by LLMs require post-
processing to become more effective in detecting bugs. Our
resultsshowthataugmentingthepromptwithsurvivingmu-
tants and refining test cases (syntax and intended behavior)
helps LLMs generate more effective test cases in terms of
fault detection.
Developers can use MuTAPto generate effective test
cases in terms of fault detection, with the help of LLMs.
Arghavan MD et al.: Preprint submitted to Elsevier Page 11 of 16

--- PAGE 12 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
Additionally, MuTAPcan be integrated into the test gener-
ation component of the GitHub Copilot lab [2] to suggest
more effective test cases for developers. Since the mutants
canbegeneratedautomatically,promptaugmentationcanbe
applied without human engagement.
1def test_case_0 ():
2 int_0 = -2973
3 int_1 = 815
4 bool_0 = module_0 . any_int (int_0 , int_0 , int_1 )
5 assert bool_0 is False
Listing 1: A sample test case generated by Pynguin for the
PUT in the motivation example presented in Figure 1.
5.2. Execution Time
Theopen-accessAPIofCodexhasalimitonthenumber
ofrequests(20perminute)andthenumberoftokens(40,000
per minute). For this reason, our experiment needs to stop
calling the API once in a while to not exceed the limit.
As a result, we present the processing time analysis using
llama-2-chat. The overall processing time of MuTAPon
HumanEval dataset while using llama-2-chat is on average
39.75 seconds with zero-shot learning (with a min of 16.16
andamaxof56.66seconds)and42.11secondswiththefew-
shotprompt(withaminof18.2andamaxof64.2seconds)
per task. It includes on average building and calling initial
promptsonLLMCwithanaverageof10.26seconds,syntax
fixing including calling the syntax fixing prompt on LLMC
with10.3seconds,intendedbehaviorrepairat0.38seconds,
MScalculationat1.7seconds,creatingaugmentedprompts
and calling them on LLM with 12.05 second and greedy
optimization with 1.4 seconds. It is noteworthy that follow-
ing the prompt augmentation step, MuTAPmust reiterate
the processes of syntax fixing, intended behavior repair,
and greedy steps which are already included in the overall
processingtime.Amongallstepsof MuTAP,themosttime-
consuming ones are those that entail inferring output from
the LLM. Conversely, the overall processing time on the
same benchmark with Pynguin to complete searching the
required space is on average 44.16 seconds with a min of
2.7 and a max of 10 minutes which is the default timeout of
the tool.
5.3. The benefit of dialog LLM
Our findings indicate that the dialog setup of llama-2-
chat provides MuTAPwith the flexibility to assign distinct
roles to each component of the augmented prompt. For
instance, by assigning IUTto an assistant role during the
prompt augmentation process, the likelihood of repeating
the initial tests in the generated output is reduced, while the
chance of generating new test cases for detecting surviving
mutantsisincreased.Listing2illustratesanexampleofhow
llama-2-chat effectively synthesizes the difference of 𝑃 𝑈𝑇
and one of its surviving mutants, explains the differences,
andsubsequentlygeneratesanewtestcasetodetectthefault.1# ###########
2// prompt
3# ###########
4{" role ": " system ", " content ": " You are a Python
5coding assistant . Always answer with Python code ."},
6{" role ": " user ", " content ":
7# Generate test cases for the following code :
8``` python
9def choose_num (x, y):
10 if x > y:
11 return -1
12 if y % 2 == 0:
13 return y
14 if x == y:
15 return -1
16 return y - 1
17```
18},
19
20{" role ": " assistant ", " content ":
21``` python
22# test cases
23def test ():
24 assert choose_num (6 ,10) == 10
25 assert choose_num (11 ,6) == -1
26 assert choose_num (8 ,12) == 12
27```
28},
29
30{" role ": " user ", " content ":
31# The test function , test () , cannot detect the fault in the
following code :
32``` python
33def choose_num (x, y):
34
35 if x > y:
36 return -1
37 if y % 2 == 0:
38 return y
39 if x != y:
40 return -1
41 return y - 1
42```
43# Provide a new test case to detect the fault in the prior
code :
44``` python
45# test case
46def test ():
47 assert
48}
49
50# #################################
51// Generated output of llama -2- chat
52# #################################
53" Here is a new test case to test the fault in the
54previous code :"
55choose_num (5, 5) == -1
56
57``` python
58def test ():
59 assertchoose_num (5, 5) == -1
60```
61" This test case checks for the scenario where both x
62and y are equal . In the modified code , the condition
63if x != y will evaluate to False , and the expected
64result is -1. However , in the original code ,
65the condition if x == y will evaluate to True , and
66the incorrect result would be y - 1, which is 4
67instead of -1."
Listing 2: A sample of augmented prompt and inference
output on llama-2-chat.
Arghavan MD et al.: Preprint submitted to Elsevier Page 12 of 16

--- PAGE 13 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
5.4. Evalution Metrics
Prior studies [50, 35, 14] that involve the generation
of assertions through LLMs have employed the “exact
match”as one of their evaluation metrics. Exact match
calculates the percentage of test cases generated by LLMs
(the inferred output) that lexically match with the ground
truth test cases (expected output). However, CIDAR [35]
has already discussed the inadequacy of exact match as a
suitable metric for assessing assertions produced by LLMs.
Thisreasonisthatthemodeloftengeneratesassertionsthat
are semantically correct but may not precisely match the
ground truth. In our study, MuTAPexecuted each test case
including assertions, both on the 𝑃 𝑈𝑇and on its mutants
to assess their correctness and effectiveness, reporting their
MS. MS is a metric frequently used in prior studies and
it serves as an effective metric for evaluating the quality
of the test oracle [51]. While, in this paper, we focus on
improving the effectiveness of test cases in terms of fault
detection, there are other metrics such as test coverage that
can assess other quality aspects of a test case. Improving
MS does not necessarily lead to good coverage and test
coverage is weakly correlated with the efficiency of tests in
fault detection [10] and is challenged as a measure of test
effectiveness in revealing faults [20, 22], which can make
it challenging for our proposed method to perform well on
both metrics [40, 18].
Furthermore, the results presented in [46] indicate that
approximately 60% of the test cases generated by Codex
encounter compilation issues due to syntax errors. The
incorporation of syntax correction and intended behavior
repair steps in our proposed method, MuTAP, significantly
enhances the utility of the tests generated by LLMs.
5.5. Surviving mutants
We augment the prompt at each iteration for each PUT
with a single surviving mutant. The average number of
mutants for all PUTs inHumanEval andRefactory are6.6
and4.2and the average number of surviving mutants are
3.6and1.8, respectively. Using a combination of surviving
mutants to augment the prompt could impact the speed
of reaching 100%MS. However, not all surviving mutants
used in prompt augmentation contribute to improving MS,
sometimes new test cases that address one mutant can also
kill the remaining surviving mutants.
6. Threats to Validity
Internal validity.
In this study, we employed two different prompt-based
learning techniques: zero-shot andfew-shot. However, we
did not explore the potential impact of altering the natural
language instructions or demonstrative examples (for few-
shotlearning)withinourprompts.Modifyingtheseinstruc-
tions or utilizing different demonstrative examples more
closely aligned with the PUT’s functionality could poten-
tiallyenhancetheresults.Asdemonstratedbyourresultsin
RQ2, including the IUT in the prompt during augmentation
steps reduced the instances of unintended behavior in testoracles.Conversely,using,forexample,lengthynaturallan-
guage instructions might potentially have an adverse effect
on the results.
To repair syntax errors in test cases through re-
prompting the LLMC, we have employed the approach pre-
sented in [52]. We did not integrate additional information
about the syntax error such as error messages or error lines
into the prompt. It is worth considering that incorporating
additionalinformationaboutsyntaxerrorscouldpotentially
enhance the LLMC’s performance to repair these syntax
errors.
Additionally,weacknowledgethatthegreedyalgorithm
employed in our approach to minimize the number of test
oracles might not be the most optimal solution for mini-
mizing test oracles while maximizing MS. However, prior
studies [18, 17] using the same method to minimize the
number of assertions have demonstrated its effectiveness in
reducing the number of test oracles within test cases, along
with its ease of implementation.
Finally, among different types of assertions, we only
focus on generating primitive ones in this study. Other
assertion types can be explored in future studies.
We employ the notions of mutant killability and bug
detection as metrics to gauge the effectiveness of test cases,
given that the primary objective of testing is to uncover
bugs. Coverage has been employed in various other studies
to assess test case quality [41, 29]. However, it has been
demonstrated that while there exists a correlation between
coverage and bug detection, they may not consistently align
in ranking different testing strategies, as observed in the
realm of fuzz testing [8].
Construct Validity. We use the notions of killing mu-
tants and bug detection as metrics to evaluate the effective-
nessoftestcases,giventhattheprimaryobjectiveoftesting
is to reveal bugs. Coverage has been employed in various
other studies to assess test case quality [41, 29]. It has been
shownthatalthoughthereisacorrelationbetweencoverage
andbug-finding,theydonotagreeontherankingofdifferent
testers, like in fuzz testing space [8].
It’s important to note that the bugs present in mutants
areartificialandmightnotdirectlycorrespondtoreal-world
faults. To address this concern, we’ve employed the Refac-
tory[23] dataset, a bug-repairing benchmark that contains
real faulty programs developed by students.
External Validity. For our experiments, we used two
datasetscontainingPythonprogrammingtasks,whichcould
potentially pose external challenges to the validity of our
findings.TherequirementforexecutablePythonprogramsis
essentialtorunthegeneratedtestsagainstboththeaccurate
and buggy versions (real or mutated) of PUTand this
considerationguidedourchoiceofdatasets.However,since
we didn’t make any specific assumptions while selecting
the dataset, our results can be extended to other Python
programs.
Finally, it should be acknowledged that the technique
proposed and the evaluations conducted in this paper are
Arghavan MD et al.: Preprint submitted to Elsevier Page 13 of 16

--- PAGE 14 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
conceptually adaptable to languages beyond Python. How-
ever, the current implementation of MuTAPis tailored for
Python programs, meaning our existing results cannot be
extended to cover other programming languages.
Reliability validity. For the purpose of enabling other
researchers to replicate or expand upon our study, we pro-
vide a replication package [3]. However, the ongoing en-
hancement of LLMs could potentially pose a challenge to
achieving an exact replication of our results.
7. Related work
Authors in [7] studied the impact of few-shotlearning
acrossvariousdownstreamtasks,includingtestcaseandtest
oracle generation. They compared the performance of few-
shotlearning with automatic test generation tools. The in-
vestigationwasconductedonadifferentsetofJavamethods
sourcedfromdifferentbenchmarks.Theoutcomesindicated
that LLMs possess the capability to generate test cases and
test oracles that exactly match (in lexical terms) the ground
truthtestswithinthebenchmarkprojects.Furthermore,their
test coverage was found to be comparable with test cases
generated by automatic test generation tools.
Sch"afer et al. [41] undertook an effort to generate test
cases by prompting Codex. Their investigation was concen-
trated on 25JavaScript packages. The prompt in their study
encompassed the implementation of the PUT and also the
usage examples of APIs extracted from documentation. In
instanceswhereatestcaseprovedunsuccessfulonthePUT,
their method incorporated the encountered error message
into the prompt and re-prompted Codex. Their findings
demonstratedthattheprocessofenhancingthepromptwith
such additional information facilitated Codex in producing
correct test cases with sufficient coverage.
LIBRO [27] used the issue reports (both title and body)
asfew-shotprompts to generate bug-reproducing test cases.
The final test cases were incorporated into appropriate test
classes and ranked based on their validity. The results re-
vealed an enhancement in generating correct test cases to
reproduce bugs compared to state-of-the-art tools.
CEDAR[35],ratherthanemployingfixeddemonstrative
examplesin few-shotlearning,aimedtoretrievedemonstra-
tive examples related to each PUTand incorporate them
into the prompt. They assessed their method based on the
lexical match, termed "exact match", between generated
assertions and the ground truth in a benchmark. While their
proposed approach demonstrates enhanced performance in
achieving exact matches between assertions and the ground
truth, it necessitates an extensive pull of code samples for
theselectionofappropriatedemonstrativeexamplesforeach
PUT.
ATHENATEST [49] employed the BART transformer
model [30], which they fine-tuned using a collection of
Java functions and their corresponding tests. They reported
test coverage comparable to those of EvoSuite [17] upon
evaluating generating test cases for five Java projects.TOGA[14]engagedinfine-tuningCodeBERTusingthe
PUT’sdocstringalongwiththeprefixofatestcasefeaturing
a masked assertion. Their goal was to synthesize the asser-
tion.Subsequently,theyformulatedthewholetestoraclesby
incorporating a test oracle grammar and generating a set of
assertions. This set was then subjected to ranking through a
neuralnetworkrankerbasedontheirlexicalmatchtoground
truth test oracles. Although they reported results akin to
thoseofEvoSuite[17]inbugdetection,theirfocusisonlyon
synthesizingtheassertions.However,synthesizingassertion
is not challenging but generating effective and meaningful
test oracles poses a significant challenge.
CODAMOSA combined the test cases generated by
Codex with those derived from Pynguin in cases where
Pynguin’s test case generation halted and failed to enhance
test coverage. CODAMOSA achieves higher test coverage
on various Python benchmarks [29] compared to Pynguin.
It is worth noting that, akin to other studies, CODAMOSA
concentrated solely on test coverage improvement, and its
generated test cases lacked assertion oracles for bug detec-
tion within programs.
Two additional studies employed Codex to simultane-
ously generate code and corresponding test cases based
on a given problem description. Subsequently, they used
these test cases to filter out buggy suggestions produced
by Codex [28, 11]. For code generation, they employed the
problem description as a prompt, and for test case genera-
tion,theyusedthesameproblemdescriptionalongwiththe
PUTand a natural language instruction.
Although prior research has explored diverse strategies
for generating test cases using LLMs like Codex and as-
sessed them in terms of test coverage or lexical match
with ground truth tests, none of these studies specifically
focused on leveraging MT to enhance the effectiveness of
the generated test cases.
8. Conclusion
In this paper, we proposed MuTAPas a means of im-
proving and evaluating the ability of pre-trained LLMs to
generateeffectivetestcases. MuTAPfirstpromptsitsLLMC
to generate test cases using zero-shot andfew-shot learn-
ing. After identifying and correcting any potential syntax
and return value errors in the generated test cases, MuTAP
evaluatestheireffectivenessbyconductingMT.Then,ituses
the surviving mutants of each PUT, if any, as well as the
initial inadequate test case to augment the initial prompt. It
re-promptsitsLLMCusingtheaugmentedprompttoregen-
erate new test cases that are capable of detecting surviving
mutants.
We assessed the effectiveness of the test cases gener-
ated by LLMs to identify bugs in real and synthetic buggy
programs. On average, test cases generated by MuTAPsuc-
cessfully identify 86.72%of buggy code in a bug repairing
benchmark when using the LLM designed for code genera-
tion, Codex. When employing LLM with the dialog setup,
Arghavan MD et al.: Preprint submitted to Elsevier Page 14 of 16

--- PAGE 15 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
llama-2-chat, MuTAPfurther improves its performance, de-
tecting 94.06%of the buggy code, outperforming both an
automatic test generation tool and a zero-shot andfew-shot
learning technique on LLMs. This underscores the advan-
tage of employing LLMs as the core of an automatic test
generation tool, as conventional automatic generation tools
such as Pynguin lack access to the insights embedded in
surviving mutants.
Although the current version of MuTAPemploys two
different LLMs to generate test cases for Python programs,
its design and evaluation methodology are fundamentally
adaptable to various programming languages and models.
Therefore, as future work, it can be easily expanded to en-
compass other programming languages or incorporate new
LLMs.
References
[1] Ahmed, T., Devanbu, P., 2022. Few-shot training LLMs for project-
specific code-summarization. arXiv preprint arXiv:2207.04237 .
[2] Alvarado, I., Gazit, I., Wattenberger, A., 2023. Github copilot labs.
https://githubnext.com/projects/copilot-labs/ .
[3] Anonymous, 2023. The replication package. https://github.com/
ExpertiseModel/MuTAP .
[4] Arcuri, A., 2018. Test suite generation with the Many Independent
Objective (MIO) algorithm. Information and Software Technology
104, 195–206.
[5] Arcuri, A., Fraser, G., 2013. Parameter tuning or default values? an
empiricalinvestigationinsearch-basedsoftwareengineering. Empir-
ical Software Engineering 18, 594–623.
[6] Arteca,E.,Harner,S.,Pradel,M.,Tip,F.,2022.Nessie:automatically
testing javascript apis with asynchronous callbacks, in: Proceedings
of the 44th International Conference on Software Engineering, pp.
1494–1505.
[7] Bareiß, P., Souza, B., d’Amorim, M., Pradel, M., 2022. Code
generation tools (almost) for free? a study of few-shot, pre-trained
language models on code. arXiv preprint arXiv:2206.01335 .
[8] Böhme, M., Szekeres, L., Metzman, J., 2022. On the reliability
of coverage-based fuzzer benchmarking, in: Proceedings of the 44th
International Conference on Software Engineering, pp. 1621–1633.
[9] Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,
P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020.
Languagemodelsarefew-shotlearners. Advancesinneuralinforma-
tion processing systems 33, 1877–1901.
[10] Cai, X., Lyu, M.R., 2005. The effect of code coverage on fault
detection under different testing profiles, in: Proceedings of the 1st
InternationalWorkshoponAdvancesinModel-BasedTesting,ACM,
New York, NY, USA. p. 1–7. URL: https://doi.org/10.1145/
1083274.1083288 .
[11] Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.G., Chen,
W.,2022.Codet:Codegenerationwithgeneratedtests.arXivpreprint
arXiv:2207.10397 .
[12] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan,
J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al., 2021.
Evaluating large language models trained on code. arXiv preprint
arXiv:2107.03374 .
[13] Clement, C.B., Drain, D., Timcheck, J., Svyatkovskiy, A., Sundare-
san,N.,2020. Pymt5:multi-modetranslationofnaturallanguageand
python code with transformers. arXiv preprint arXiv:2010.03150 .
[14] Dinella, E., Ryan, G., Mytkowicz, T., Lahiri, S.K., 2022. Toga: a
neural method for test oracle generation, in: Proceedings of the 44th
International Conference on Software Engineering, pp. 2130–2141.
[15] Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou,
L., Qin, B., Liu, T., Jiang, D., et al., 2020. Codebert: A pre-
trainedmodelforprogrammingandnaturallanguages. arXivpreprintarXiv:2002.08155 .
[16] Fraser, G., Arcuri, A., 2011a. Evolutionary generation of whole test
suites, in: 2011 11th International Conference on Quality Software,
IEEE. pp. 31–40.
[17] Fraser, G., Arcuri, A., 2011b. Evosuite: automatic test suite gen-
eration for object-oriented software, in: Proceedings of the 19th
ACM SIGSOFT symposium and the 13th European conference on
Foundations of software engineering, pp. 416–419.
[18] Fraser, G., Zeller, A., 2010. Mutation-driven generation of unit tests
and oracles, in: Proceedings of the 19th international symposium on
Software testing and analysis, pp. 147–158.
[19] Godefroid,P.,Klarlund,N.,Sen,K.,2005. Dart:Directedautomated
randomtesting,in:Proceedingsofthe2005ACMSIGPLANconfer-
enceonProgramminglanguagedesignandimplementation,pp.213–
223.
[20] Gopinath, R., Jensen, C., Groce, A., 2014. Code coverage for suite
evaluation by developers, in: Proceedings of the 36th International
Conference on Software Engineering, pp. 72–82.
[21] Hałas,K.,2019. Mutpy:amutationtestingtoolforPython3.xsource
code. https://github.com/mutpy/mutpy .
[22] Hemmati, H., 2015. How effective are code coverage criteria?, in:
2015IEEEInternationalConferenceonSoftwareQuality,Reliability
and Security, IEEE. pp. 151–156.
[23] Hu, Y., Ahmed, U.Z., Mechtaev, S., Leong, B., Roychoudhury, A.,
2019. Re-factoring based program repair applied to programming
assignments, in: 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE), IEEE. pp. 388–398.
[24] Hu, Y., Ahmed, U.Z., Mechtaev, S., Leong, B., Roychoudhury, A.,
2023. Refactory. https://github.com/githubhuyang/refactory .
[25] Jia,Y.,Harman,M.,2011. Ananalysisandsurveyofthedevelopment
of mutation testing. IEEE Transactions on Software Engineering 37,
649–678. doi: 10.1109/TSE.2010.62 .
[26] Joshi, H., Cambronero, J., Gulwani, S., Le, V., Radicek, I., Ver-
bruggen,G.,2022. Repairisnearlygeneration:Multilingualprogram
repair with LLMs. arXiv preprint arXiv:2208.11640 .
[27] Kang, S., Yoon, J., Yoo, S., 2022. Large language models are few-
shot testers: Exploring LLM-based general bug reproduction. arXiv
preprint arXiv:2209.11515 .
[28] Lahiri, S.K., Naik, A., Sakkas, G., Choudhury, P., von Veh, C.,
Musuvathi, M., Inala, J.P., Wang, C., Gao, J., 2022. Interactive code
generation via test-driven user-intent formalization. arXiv preprint
arXiv:2208.05950 .
[29] Lemieux, C., Inala, J.P., Lahiri, S.K., Sen, S., 2023. Codamosa:
Escaping coverage plateaus in test generation with pre-trained large
language models, in: Accepted by 45th International Conference on
Software Engineering (ICSE).
[30] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A.,
Levy, O., Stoyanov, V., Zettlemoyer, L., 2019. Bart: Denoising
sequence-to-sequence pre-training for natural language generation,
translation, and comprehension. arXiv preprint arXiv:1910.13461 .
[31] Liu,P.,Yuan,W.,Fu,J.,Jiang,Z.,Hayashi,H.,Neubig,G.,2023.Pre-
train,prompt,andpredict:Asystematicsurveyofpromptingmethods
in natural language processing. ACM Computing Surveys 55, 1–35.
[32] Lukasczyk,S.,Fraser,G.,2022.Pynguin:Automatedunittestgenera-
tionforpython,in:ProceedingsoftheACM/IEEE44thInternational
Conference on Software Engineering: Companion Proceedings, pp.
168–172.
[33] Lukasczyk, S., Kroiß, F., Fraser, G., 2023. An empirical study
of automated unit test generation for python. Empirical Software
Engineering 28, 36.
[34] Moradi Dakhel, A., Majdinasab, V., Nikanjam, A., Khomh, F., Des-
marais, M.C., Jiang, Z.M.J., 2023. Github Copilot AI pair program-
mer:Assetorliability?JournalofSystemsandSoftware203,111734.
doi:https://doi.org/10.1016/j.jss.2023.111734 .
[35] Nashid, N., Sintaha, M., Mesbah, A., 2023. Retrieval-based prompt
selection for code-related few-shot learning .
Arghavan MD et al.: Preprint submitted to Elsevier Page 15 of 16

--- PAGE 16 ---
Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing
[36] Palomba, F., Di Nucci, D., Panichella, A., Oliveto, R., De Lucia, A.,
2016. On the diffusion of test smells in automatically generated test
code: An empirical study, in: Proceedings of the 9th international
workshop on search-based software testing, pp. 5–14.
[37] Panichella, A., Kifetew, F.M., Tonella, P., 2015. Reformulating
branch coverage as a many-objective optimization problem, in: 2015
IEEE 8th international conference on software testing, verification
and validation (ICST), IEEE. pp. 1–10.
[38] Panichella, A.,Kifetew, F.M.,Tonella, P.,2017. Automatedtest case
generation as a many-objective optimisation problem with dynamic
selection of the targets. IEEE Transactions on Software Engineering
44, 122–158.
[39] Panichella,A.,Panichella,S.,Fraser,G.,Sawant,A.A.,Hellendoorn,
V.J., 2020. Revisiting test smells in automatically generated tests:
limitations, pitfalls, and opportunities, in: 2020 IEEE international
conference on software maintenance and evolution (ICSME), IEEE.
pp. 523–533.
[40] Papadakis, M., Kintis, M., Zhang, J., Jia, Y., Le Traon, Y., Harman,
M., 2019. Mutation testing advances: an analysis and survey, in:
Advances in Computers. Elsevier. volume 112, pp. 275–378.
[41] Schäfer,M.,Nadi,S.,Eghbali,A.,Tip,F.,2023. Adaptivetestgener-
ationusingalargelanguagemodel. arXivpreprintarXiv:2302.06527
.
[42] Selakovic, M., Pradel, M., Karim, R., Tip, F., 2018. Test generation
for higher-order functions in dynamic languages. Proceedings of the
ACM on Programming Languages 2, 1–27.
[43] Sen, K., Marinov, D., Agha, G., 2005. Cute: A concolic unit testing
engine for c. ACM SIGSOFT Software Engineering Notes 30, 263–
272.[44] Shore, J., Warden, S., 2021. The art of agile development. 2nd ed.,
"O’Reilly".
[45] Shrivastava, D., Larochelle, H., Tarlow, D., 2022. Repository-level
prompt generation for large language models of code. arXiv preprint
arXiv:2206.12839 .
[46] Siddiq, M.L., Santos, J., Tanvir, R.H., Ulfat, N., Rifat, F.A., Lopes,
V.C., 2023. Exploring the effectiveness of large language models in
generating unit tests. arXiv preprint arXiv:2305.00418 .
[47] Siddiqui, S., 2021. Learning Test-Driven Development. "O’Reilly".
[48] Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,Babaei,
Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al., 2023.
Llama2:Openfoundationandfine-tunedchatmodels. arXivpreprint
arXiv:2307.09288 .
[49] Tufano,M.,Drain,D.,Svyatkovskiy,A.,Deng,S.K.,Sundaresan,N.,
2021. Unit test case generation with transformers and focal context.
arXiv preprint arXiv:2009.05617 .
[50] Tufano,M.,Drain,D.,Svyatkovskiy,A.,Sundaresan,N.,2022. Gen-
erating accurate assert statements for unit test cases using pretrained
transformers, in: Proceedings of the 3rd ACM/IEEE International
Conference on Automation of Software Test, pp. 54–64.
[51] Xie, T., 2006. Augmenting automatically generated unit-test suites
with regression oracle checking, in: ECOOP 2006–Object-Oriented
Programming: 20th European Conference, Nantes, France, July 3-7,
2006. Proceedings 20, Springer. pp. 380–403.
[52] Zhang,J.,Cambronero,J.,Gulwani,S.,Le,V.,Piskac,R.,Soares,G.,
Verbruggen, G., 2022. Repairing bugs in python assignments using
large language models. arXiv preprint arXiv:2209.14876 .
Arghavan MD et al.: Preprint submitted to Elsevier Page 16 of 16
