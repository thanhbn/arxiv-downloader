Vá» Lá»£i Ã­ch cá»§a Háº¡ng trong cÃ¡c Táº§ng Attention

Noah Amsel1, Gilad Yehudai2, vÃ  Joan Bruna1,2,3
1Viá»‡n Khoa há»c ToÃ¡n há»c Courant, Äáº¡i há»c New York
2Trung tÃ¢m Khoa há»c Dá»¯ liá»‡u, Äáº¡i há»c New York
3Viá»‡n Flatiron
24 thÃ¡ng 7, 2024

TÃ³m táº¯t

CÃ¡c cÆ¡ cháº¿ dá»±a trÃªn attention Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong machine learning, ná»•i báº­t nháº¥t lÃ  trong transformers. Tuy nhiÃªn, cÃ¡c siÃªu tham sá»‘ nhÆ° háº¡ng cá»§a ma tráº­n attention vÃ  sá»‘ lÆ°á»£ng head Ä‘Æ°á»£c Ä‘iá»u chá»‰nh theo cÃ¡ch gáº§n nhÆ° giá»‘ng nhau trong táº¥t cáº£ cÃ¡c hiá»‡n thá»±c cá»§a kiáº¿n trÃºc nÃ y, mÃ  khÃ´ng cÃ³ sá»± biá»‡n minh lÃ½ thuyáº¿t. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i cho tháº¥y ráº±ng cÃ³ nhá»¯ng Ä‘Ã¡nh Ä‘á»•i Ä‘Ã¡ng ká»ƒ giá»¯a háº¡ng vÃ  sá»‘ lÆ°á»£ng head cá»§a cÆ¡ cháº¿ attention. Cá»¥ thá»ƒ, chÃºng tÃ´i trÃ¬nh bÃ y má»™t hÃ m má»¥c tiÃªu Ä‘Æ¡n giáº£n vÃ  tá»± nhiÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t single full-rank attention head cho báº¥t ká»³ Ä‘á»™ dÃ i ngá»¯ cáº£nh nÃ o, nhÆ°ng khÃ´ng thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ bá»Ÿi low-rank attention trá»« khi sá»‘ lÆ°á»£ng head lÃ  hÃ m mÅ© theo chiá»u embedding, ngay cáº£ vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh ngáº¯n. HÆ¡n ná»¯a, chÃºng tÃ´i chá»©ng minh ráº±ng, vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh ngáº¯n, viá»‡c thÃªm Ä‘á»™ sÃ¢u cho phÃ©p má»¥c tiÃªu Ä‘Æ°á»£c xáº¥p xá»‰ bá»Ÿi low-rank attention. Vá»›i ngá»¯ cáº£nh dÃ i, chÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng full-rank attention lÃ  cáº§n thiáº¿t. Cuá»‘i cÃ¹ng, chÃºng tÃ´i trÃ¬nh bÃ y cÃ¡c thÃ­ nghiá»‡m vá»›i transformers cÃ³ sáºµn Ä‘á»ƒ xÃ¡c thá»±c cÃ¡c phÃ¡t hiá»‡n lÃ½ thuyáº¿t cá»§a chÃºng tÃ´i.

Má»¥c lá»¥c
1 Giá»›i thiá»‡u 2
2 CÃ´ng trÃ¬nh liÃªn quan 3
3 Thiáº¿t láº­p vÃ  KÃ½ hiá»‡u 4
4 PhÃ¢n tÃ¡ch Low-Rank cho Nearest Neighbors 6
5 PhÃ¢n tÃ¡ch HÃ m mÅ© cho Biased Nearest Neighbors 7
6 Xáº¥p xá»‰ Hiá»‡u quáº£ Sá»­ dá»¥ng Äá»™ sÃ¢u 9
7 ThÃ­ nghiá»‡m 11
8 Káº¿t luáº­n vÃ  Háº¡n cháº¿ 15
A SiÃªu tham sá»‘ cá»§a Transformer 21
B Chá»©ng minh tá»« Má»¥c 4 22
C Chá»©ng minh tá»« Má»¥c 5 43
D Chá»©ng minh tá»« Má»¥c 6 vÃ  má»™t Cáº¥u trÃºc Bá»• sung 50

1 Giá»›i thiá»‡u

CÃ¡c kiáº¿n trÃºc dá»±a trÃªn attention cÃ³ máº·t kháº¯p nÆ¡i trong machine learning Ä‘Æ°Æ¡ng Ä‘áº¡i. Nhá»¯ng vÃ­ dá»¥ ná»•i báº­t nháº¥t lÃ  transformers, Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡ch xáº¿p chá»“ng nhiá»u lá»›p attention vá»›i MLPs, residual connections, vÃ  cÃ¡c lá»›p normalization Ä‘á»ƒ biá»ƒu diá»…n cÃ¡c hÃ m trÃªn dÃ£y hoáº·c táº­p há»£p. Khung cÆ¡ báº£n nÃ y cho phÃ©p ngÆ°á»i dÃ¹ng tá»± do thiáº¿t láº­p má»™t sá»‘ siÃªu tham sá»‘, máº·c dÃ¹ Ã­t trong sá»‘ nÃ y Ä‘Æ°á»£c nghiÃªn cá»©u cáº©n tháº­n. Thá»±c táº¿, trong hÃ ng nghÃ¬n bÃ i bÃ¡o sá»­ dá»¥ng kiáº¿n trÃºc nÃ y, nhiá»u siÃªu tham sá»‘ Ä‘Æ°á»£c giá»¯ nguyÃªn hoáº·c gáº§n nhÆ° giá»‘ng vá»›i bÃ i bÃ¡o gá»‘c [VSP+17] (xem Phá»¥ lá»¥c A Ä‘á»ƒ so sÃ¡nh). Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i nghiÃªn cá»©u táº§m quan trá»ng cá»§a háº¡ng cá»§a cÆ¡ cháº¿ attention.

Má»™t lá»›p attention lÃ  má»™t Ã¡nh xáº¡ giá»¯a cÃ¡c dÃ£y vector trong â„^d. KÃ­ch thÆ°á»›c cá»§a má»™t lá»›p attention Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi sá»‘ lÆ°á»£ng head (H) vÃ  háº¡ng cá»§a ma tráº­n weight query vÃ  key (r), sao cho tá»•ng sá»‘ tham sá»‘ cÃ³ báº­c dÃ—HÃ—r. ÄÃ¡ng chÃº Ã½, gáº§n nhÆ° má»i kiáº¿n trÃºc transformer Ä‘á»u thiáº¿t láº­p sá»‘ lÆ°á»£ng head lÃ  H = d/r, vÃ  nhá»¯ng ngoáº¡i lá»‡ mÃ  chÃºng tÃ´i biáº¿t chá»‰ khÃ¡c nhau khÃ´ng quÃ¡ há»‡ sá»‘ 2 (xem Phá»¥ lá»¥c A). Thá»±c táº¿, quy mÃ´ nÃ y ráº¥t tiÃªu chuáº©n Ä‘áº¿n má»©c nÃ³ Ä‘Æ°á»£c hard-code vÃ o cÃ¡c thÆ° viá»‡n nhÆ° PyTorch [PGM+19] vÃ  xFormers [LML+22], má»™t thá»±c táº¿ cÃ³ láº½ Ä‘Ã£ ngÄƒn cáº£n viá»‡c thá»­ nghiá»‡m vá»›i cÃ¡c quy mÃ´ khÃ¡c. Äá»™ng lá»±c ban Ä‘áº§u cho quy mÃ´ nÃ y lÃ  Ä‘á»ƒ khá»›p vá»›i sá»‘ lÆ°á»£ng tham sá»‘ cá»§a má»™t single full rank head, tá»©c lÃ  trÆ°á»ng há»£p H = 1, r = d. ChÃºng tÃ´i khÃ´ng biáº¿t lÃ½ do a priori hoáº·c báº±ng chá»©ng thá»±c nghiá»‡m nÃ o á»§ng há»™ quy mÃ´ nÃ y hÆ¡n báº¥t ká»³ quy mÃ´ nÃ o khÃ¡c, vÃ¬ nhá»¯ng Ä‘Ã¡nh Ä‘á»•i giá»¯a háº¡ng vÃ  sá»‘ lÆ°á»£ng head váº«n chÆ°a Ä‘Æ°á»£c hiá»ƒu rÃµ. VÃ­ dá»¥, háº§u háº¿t transformers trong vÄƒn há»c sá»­ dá»¥ng háº¡ng nhá» tá»« 64 Ä‘áº¿n 128, máº·c dÃ¹ chiá»u embedding d thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ (vÃ­ dá»¥ d = 512 trong bÃ i bÃ¡o transformers gá»‘c [VSP+17] vÃ  d = 8192 trong LLaMA [TLI+23]). KhÃ´ng rÃµ liá»‡u sá»©c máº¡nh biá»ƒu Ä‘áº¡t cá»§a transformers cÃ³ bá»‹ suy yáº¿u bá»Ÿi viá»‡c duy trÃ¬ háº¡ng cá»‘ Ä‘á»‹nh khi chiá»u Ä‘Æ°á»£c tÄƒng lÃªn.

Má»™t dÃ²ng cÃ´ng trÃ¬nh dÃ i trong lÃ½ thuyáº¿t deep learning Ä‘Ã£ nghiÃªn cá»©u táº§m quan trá»ng tÆ°Æ¡ng Ä‘á»‘i cá»§a Ä‘á»™ rá»™ng vÃ  Ä‘á»™ sÃ¢u trong viá»‡c xÃ¡c Ä‘á»‹nh sá»©c máº¡nh biá»ƒu Ä‘áº¡t cá»§a máº¡ng nÆ¡-ron feedforward, nhÆ° má»™t bÆ°á»›c cáº§n thiáº¿t Ä‘áº§u tiÃªn Ä‘á»ƒ hiá»ƒu nhá»¯ng Ä‘Ã¡nh Ä‘á»•i thá»±c táº¿ (cÅ©ng bao gá»“m cÃ¡c khÃ­a cáº¡nh tá»‘i Æ°u hÃ³a). BÃ i bÃ¡o nÃ y tÆ°Æ¡ng tá»± á»Ÿ chá»— chÃºng tÃ´i nghiÃªn cá»©u nhá»¯ng Ä‘Ã¡nh Ä‘á»•i tham sá»‘ trong transformers thÃ´ng qua lÄƒng kÃ­nh sá»©c máº¡nh biá»ƒu Ä‘áº¡t, máº·c dÃ¹ transformers cÃ³ nhiá»u siÃªu tham sá»‘ hÆ¡n chá»‰ Ä‘á»™ rá»™ng vÃ  Ä‘á»™ sÃ¢u (xem Phá»¥ lá»¥c A). Äá»‘i vá»›i máº¡ng feedforward, Ä‘á»™ sÃ¢u 2 Ä‘á»§ cho xáº¥p xá»‰ universal [Cyb89], nhÆ°ng Ä‘á»™ sÃ¢u lá»›n hÆ¡n cÃ³ thá»ƒ cáº§n thiáº¿t cho xáº¥p xá»‰ hiá»‡u quáº£. NghÄ©a lÃ , má»™t sá»‘ hÃ m cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n hiá»‡u quáº£ bá»Ÿi máº¡ng ba lá»›p nhÆ°ng khÃ´ng thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi máº¡ng hai lá»›p trá»« khi nÃ³ cÃ³ Ä‘á»™ rá»™ng hÃ m mÅ© theo chiá»u Ä‘áº§u vÃ o [ES16, Dan17, SS17]. Tá»± nhiÃªn lÃ  Ä‘áº·t cÃ¢u há»i tÆ°Æ¡ng tá»± vá» kiáº¿n trÃºc attention. ChÃºng ta nÃªn thiáº¿t láº­p cÃ¡c siÃªu tham sá»‘ nhÆ° tháº¿ nÃ o Ä‘á»ƒ lÃ m cho transformers cá»§a chÃºng ta hiá»‡u quáº£? Äáº·c biá»‡t, liá»‡u low-rank attention cÃ³ vá» cÆ¡ báº£n yáº¿u hÆ¡n high-rank attention khÃ´ng, hay sá»©c máº¡nh biá»ƒu Ä‘áº¡t chá»‰ Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn bá»Ÿi tÃ­ch tham sá»‘ HÃ—r, hoáº¡t Ä‘á»™ng nhÆ° tÆ°Æ¡ng tá»± Ä‘á»™ rá»™ng cá»§a má»™t lá»›p MLP?

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i nghiÃªn cá»©u chÃ­nh xÃ¡c nhá»¯ng Ä‘Ã¡nh Ä‘á»•i tinh vi nÃ y trong kháº£ nÄƒng biá»ƒu Ä‘áº¡t cá»§a cÃ¡c lá»›p attention. ChÃºng tÃ´i trÃ¬nh bÃ y má»™t hÃ m má»¥c tiÃªu Ä‘Æ¡n giáº£n phÃ¡t sinh tá»± nhiÃªn trong tÃ¬m kiáº¿m ngá»¯ nghÄ©a cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ Ä‘áº¿n báº¥t ká»³ Ä‘á»™ chÃ­nh xÃ¡c nÃ o bá»Ÿi má»™t single full rank attention head báº¥t ká»ƒ Ä‘á»™ dÃ i ngá»¯ cáº£nh. Máº·t khÃ¡c, viá»‡c xáº¥p xá»‰ má»¥c tiÃªu nÃ y vá»›i low-rank attention yÃªu cáº§u sá»‘ lÆ°á»£ng head pháº£i lÃ  super-polynomial theo chiá»u Ä‘áº§u vÃ o, ngay cáº£ vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh ngáº¯n. Cá»¥ thá»ƒ, sá»­ dá»¥ng full-rank heads, tá»•ng sá»‘ tham sá»‘ cáº§n thiáº¿t lÃ  dÃ—HÃ—r â‰ƒ dÂ², trong khi nÃ³ trá»Ÿ thÃ nh â‰ƒ d^(1+Îµ^(-1)) náº¿u sá»­ dá»¥ng low-rank heads thay tháº¿, Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘á»‘i Îµ. TÄƒng Ä‘á»™ sÃ¢u cho phÃ©p xáº¥p xá»‰ tá»‘t hÆ¡n chá»‰ sá»­ dá»¥ng sá»‘ lÆ°á»£ng head Ä‘a thá»©c, Ã­t nháº¥t vá»›i Ä‘á»™ dÃ i ngá»¯ cáº£nh ngáº¯n. ChÃºng tÃ´i bá»• sung nhá»¯ng káº¿t quáº£ lÃ½ thuyáº¿t nÃ y báº±ng thÃ­ nghiá»‡m trÃªn cÃ¡c kiáº¿n trÃºc transformer cÃ³ sáºµn. Káº¿t quáº£ cá»§a chÃºng tÃ´i chá»©ng minh má»™t Ä‘Ã¡nh Ä‘á»•i ráº¥t rÃµ rÃ ng giá»¯a háº¡ng vÃ  sá»‘ lÆ°á»£ng head trong cÆ¡ cháº¿ attention vÃ  lÃ m sÃ¡ng tá» quy mÃ´ tiÃªu chuáº©n H = d/r Ä‘Æ°á»£c sá»­ dá»¥ng trong transformers.

1.1 ÄÃ³ng gÃ³p cá»§a ChÃºng tÃ´i

â€¢ Trong Má»¥c 4, chÃºng tÃ´i chá»©ng minh phÃ¢n tÃ¡ch háº¡ng Ä‘á»ƒ biá»ƒu diá»…n hÃ m nearest neighbor sá»­ dá»¥ng multi-head attention. HÃ m nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ Ä‘áº¿n báº¥t ká»³ Ä‘á»™ chÃ­nh xÃ¡c nÃ o chá»‰ sá»­ dá»¥ng má»™t single full-rank head. Tuy nhiÃªn trong cháº¿ Ä‘á»™ chiá»u cao, Ã­t nháº¥t Î©(d/r)^(1/Îµ) head háº¡ng r Ä‘Æ°á»£c yÃªu cáº§u Ä‘á»ƒ Ä‘áº¡t sai sá»‘ bÃ¬nh phÆ°Æ¡ng tÆ°Æ¡ng Ä‘á»‘i Îµ. HÆ¡n ná»¯a, trong cháº¿ Ä‘á»™ Ä‘á»™ chÃ­nh xÃ¡c cao (Îµ tiáº¿n tá»›i 0 vá»›i d cá»‘ Ä‘á»‹nh), sá»‘ lÆ°á»£ng head cáº§n thiáº¿t lÃ  hÃ m mÅ©: Î©(exp(d - r log(d/r))).

â€¢ Trong Má»¥c 5, chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t khÃ¡c nhau Ä‘á»ƒ thiáº¿t láº­p phÃ¢n tÃ¡ch hÃ m mÅ© trong cháº¿ Ä‘á»™ Ä‘á»™ chÃ­nh xÃ¡c cao cho hÃ m biased nearest neighbor. HÃ m má»¥c tiÃªu nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ Ä‘áº¿n báº¥t ká»³ Ä‘á»™ chÃ­nh xÃ¡c nÃ o sá»­ dá»¥ng single full-rank head vá»›i viá»‡c thÃªm bias, nhÆ°ng Î©(exp(d - r)) rank-r heads Ä‘Æ°á»£c yÃªu cáº§u Ä‘á»ƒ xáº¥p xá»‰ nÃ³ vá»›i Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n O(1/dâ´) sai sá»‘ bÃ¬nh phÆ°Æ¡ng tÆ°Æ¡ng Ä‘á»‘i.

â€¢ Trong Má»¥c 6, chÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡c cÃ¡ch Ä‘á»ƒ kháº¯c phá»¥c Ä‘iá»ƒm yáº¿u cá»§a low-rank attention. ChÃºng tÃ´i cho tháº¥y ráº±ng viá»‡c tÄƒng cÆ°á»ng kiáº¿n trÃºc attention vÃ  thÃªm lá»›p thá»© hai, phi tuyáº¿n cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y sá»­ dá»¥ng sá»‘ lÆ°á»£ng head Ä‘a thá»©c, nhÆ°ng khÃ´ng giá»‘ng nhÆ° full-rank attention, nhá»¯ng cáº¥u trÃºc nhÆ° váº­y cÃ³ thá»ƒ khÃ´ng má»Ÿ rá»™ng Ä‘Æ°á»£c vá»›i Ä‘á»™ dÃ i dÃ£y dÃ i.

â€¢ Trong Má»¥c 7, chÃºng tÃ´i há»— trá»£ káº¿t quáº£ lÃ½ thuyáº¿t cá»§a chÃºng tÃ´i báº±ng thÃ­ nghiá»‡m trÃªn kiáº¿n trÃºc transformer tiÃªu chuáº©n vá»›i nhiá»u lá»›p attention vÃ  MLPs. ChÃºng tÃ´i cho tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh full rank dá»… dÃ ng há»c má»¥c tiÃªu Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cao â€” tháº­m chÃ­ khÃ´i phá»¥c cáº¥u trÃºc chÃ­nh cá»§a chÃºng tÃ´i â€” nhÆ°ng cÃ¡c mÃ´ hÃ¬nh low rank gáº·p khÃ³ khÄƒn Ä‘á»ƒ lÃ m nhÆ° váº­y. NgÆ°á»i dÃ¹ng transformers tiÃªu chuáº©n cÃ³ thá»ƒ khÃ´ng nghÄ© ráº±ng thiáº¿t láº­p H = 2 cÃ³ thá»ƒ tá»‡ hÆ¡n nhiá»u so vá»›i H = 1, nhÆ°ng trong trÆ°á»ng há»£p nÃ y, Ä‘Ãºng váº­y.

2 CÃ´ng trÃ¬nh liÃªn quan

LÃ½ thuyáº¿t vá» transformers: Má»™t dÃ²ng cÃ´ng trÃ¬nh ngÃ y cÃ ng phÃ¡t triá»ƒn Ä‘Ã£ tÃ¬m cÃ¡ch cung cáº¥p phÃ¢n tÃ­ch lÃ½ thuyáº¿t vá» transformers vÃ  cÆ¡ cháº¿ attention. Dynamics huáº¥n luyá»‡n, inductive biases, generalization, vÃ  in-context learning Ä‘á»u nháº­n Ä‘Æ°á»£c sá»± chÃº Ã½ Ä‘Ã¡ng ká»ƒ. Tuy nhiÃªn, cÃ¡c bÃ i bÃ¡o trong nhá»¯ng lÄ©nh vá»±c nÃ y gáº§n nhÆ° luÃ´n giáº£ Ä‘á»‹nh ráº±ng full-rank attention Ä‘Æ°á»£c sá»­ dá»¥ng [BCB+23, CDB24, FGBM23, SHT24a, EGKZ22, BCW+23, ZFB24, JBKM24, CSWY24, DGTT23, TWCD23], máº·c dÃ¹ nhiá»u bÃ i cÅ©ng giáº£ Ä‘á»‹nh cÃ³ nhiá»u head. CÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i cung cáº¥p bá»‘i cáº£nh quan trá»ng cho nhá»¯ng káº¿t quáº£ nÃ y, cho tháº¥y ráº±ng cÃ¡c mÃ´ hÃ¬nh full-rank cÃ³ thá»ƒ khÃ´ng pháº£i lÃ  proxy tá»‘t cho cÃ¡c transformers low-rank Ä‘Æ°á»£c sá»­ dá»¥ng trong thá»±c táº¿.

Sá»©c máº¡nh biá»ƒu Ä‘áº¡t cá»§a transformers: CÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i thuá»™c vá» má»™t nhÃ³m nghiÃªn cá»©u vá» kháº£ nÄƒng biá»ƒu diá»…n cá»§a transformers. KhÃ´ng giá»‘ng nhÆ° cÃ¡c chá»§ Ä‘á» khÃ¡c trong lÃ½ thuyáº¿t transformer, káº¿t quáº£ trong lÄ©nh vá»±c nÃ y thÆ°á»ng Ã¡p dá»¥ng cho low-rank attention. [YBR+19] chá»©ng minh ráº±ng transformers (sÃ¢u theo hÃ m mÅ©) lÃ  universal approximators ngay cáº£ vá»›i háº¡ng má»™t. [WCM22, MS23] cho tháº¥y ráº±ng transformers cÃ³ thá»ƒ mÃ´ phá»ng mÃ¡y Turing náº¿u kÃ­ch thÆ°á»›c cá»§a chÃºng Ä‘Æ°á»£c phÃ©p tÄƒng theo Ä‘á»™ dÃ i dÃ£y. [KKM22, KS23] cho tháº¥y ráº±ng transformers cÃ³ kháº£ nÄƒng ghi nhá»› dá»¯ liá»‡u. [BHBK24] cho tháº¥y ráº±ng transformers cÃ³ thá»ƒ hiá»‡u quáº£ triá»ƒn khai má»™t phiÃªn báº£n cá»§a thuáº­t toÃ¡n nearest neighbor cho phÃ¢n loáº¡i in-context cá»§a cÃ¡c Ä‘iá»ƒm trÃªn sphere, nhÆ°ng cáº¥u trÃºc cá»§a há» sá»­ dá»¥ng attention lÃ  full-rank vá»›i respect Ä‘áº¿n chiá»u Ä‘áº§u vÃ o. CÃ´ng thá»©c cá»§a chÃºng tÃ´i vá» tÃ¡c vá»¥ nearest neighbor hÆ¡i khÃ¡c vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t vá»›i full-rank attention gáº§n nhÆ° trivial (xem Fact 1). Cuá»‘i cÃ¹ng, má»™t dÃ²ng cÃ´ng trÃ¬nh quan trá»ng phÃ¢n tÃ­ch kháº£ nÄƒng biá»ƒu diá»…n cá»§a transformers sá»­ dá»¥ng cÃ¡c lá»›p ngÃ´n ngá»¯ hÃ¬nh thá»©c, finite automata, vÃ  circuits [Hah20, LAG+22, HAF22, MSS22, SMW+24], nhÆ°ng nÃ³ khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c sá»± phÃ¢n tÃ¡ch trong kháº£ nÄƒng do háº¡ng.

Háº¡n cháº¿ cá»§a low-rank attention: Má»™t sá»‘ nghiÃªn cá»©u khÃ¡c Ä‘Ã£ Ä‘iá»u tra vai trÃ² cá»§a háº¡ng cá»§a cÆ¡ cháº¿ attention. [BYR+20] trÃ¬nh bÃ y cÃ¡c thÃ­ nghiá»‡m thÃ¡ch thá»©c quy mÃ´ canonical H = d/r. Há» láº­p luáº­n ráº±ng viá»‡c cá»‘ Ä‘á»‹nh d vÃ  r dá»±a trÃªn Ä‘á»™ dÃ i ngá»¯ cáº£nh N vÃ  thiáº¿t láº­p H Ä‘á»™c láº­p dáº«n Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh máº¡nh máº½ vÃ  hiá»‡u quáº£ hÆ¡n. Há» cÅ©ng chá»©ng minh ráº±ng má»™t full-rank attention head cÃ³ thá»ƒ táº¡o ra báº¥t ká»³ attention pattern nÃ o tá»« báº¥t ká»³ Ä‘áº§u vÃ o nÃ o (cho má»™t sá»‘ thiáº¿t láº­p cá»§a weights), nhÆ°ng má»™t low-rank attention head khÃ´ng thá»ƒ; tuy nhiÃªn, [LCW23] cho tháº¥y ráº±ng ngay cáº£ háº¡ng r = log(N) cÅ©ng Ä‘á»§ Ä‘á»ƒ biá»ƒu diá»…n báº¥t ká»³ sparse attention pattern nÃ o. [MLT24] há»i cÃ³ bao nhiÃªu cáº·p input-output mÃ  má»™t low-rank multi-head attention layer cÃ³ thá»ƒ ghi nhá»› chÃ­nh xÃ¡c. Äá»‘i vá»›i váº¥n Ä‘á» cá»§a há», khÃ´ng cÃ³ giÃ¡ trá»‹ gÃ¬ khi thiáº¿t láº­p r > N; hÆ¡n ná»¯a kháº£ nÄƒng ghi nhá»› phá»¥ thuá»™c vÃ o rÃ—H hÆ¡n lÃ  vÃ o r hoáº·c H, há»— trá»£ quy mÃ´ tiÃªu chuáº©n. ChÃºng tÃ´i nghiÃªn cá»©u thiáº¿t láº­p thá»±c táº¿ vÃ  cÃ³ Ä‘á»™ng lá»±c thá»±c táº¿ hÆ¡n vá» viá»‡c xáº¥p xá»‰ má»™t hÃ m tá»± nhiÃªn trÃªn dá»¯ liá»‡u Ä‘Æ°á»£c rÃºt ra tá»« má»™t phÃ¢n phá»‘i tá»± nhiÃªn. KhÃ´ng giá»‘ng nhÆ° [LCW23, MLT24], chÃºng tÃ´i cho tháº¥y ráº±ng háº¡ng cao Ä‘Ã´i khi cáº§n thiáº¿t, báº¥t ká»ƒ H.

BÃ i bÃ¡o gáº§n nháº¥t vá»›i cá»§a chÃºng tÃ´i lÃ  [SHT24b], chá»©ng minh hai phÃ¢n tÃ¡ch liÃªn quan Ä‘áº¿n háº¡ng. Äáº§u tiÃªn, há» trÃ¬nh bÃ y má»™t hÃ m cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ tá»‘t bá»Ÿi má»™t single attention head náº¿u vÃ  chá»‰ náº¿u háº¡ng cá»§a nÃ³ Ä‘á»§ lá»›n. Káº¿t quáº£ nÃ y Ä‘áº·t ra cÃ¢u há»i sau: liá»‡u viá»‡c sá»­ dá»¥ng nhiá»u head cÃ³ thá»ƒ bÃ¹ Ä‘áº¯p cho Ä‘iá»ƒm yáº¿u cá»§a low-rank attention khÃ´ng? ChÃºng tÃ´i tráº£ lá»i cÃ¢u há»i nÃ y má»™t cÃ¡ch phá»§ Ä‘á»‹nh. Thá»© hai, há» trÃ¬nh bÃ y má»™t hÃ m má»™t chiá»u trÃªn N Ä‘áº§u vÃ o mÃ  khÃ´ng thá»ƒ biá»ƒu diá»…n chÃ­nh xÃ¡c trá»« khi rÃ—HÃ—p > N, trong Ä‘Ã³ p lÃ  sá»‘ bit Ä‘á»™ chÃ­nh xÃ¡c. ChÃºng tÃ´i má»Ÿ rá»™ng káº¿t quáº£ nÃ y á»Ÿ chá»— cÃ¡c lower bounds cá»§a chÃºng tÃ´i Ã¡p dá»¥ng (1) ngay cáº£ vá»›i N = 2, (2) cho Ä‘á»™ chÃ­nh xÃ¡c vÃ´ háº¡n hoáº·c há»¯u háº¡n (3) cho xáº¥p xá»‰ hÃ m trÃªn má»™t phÃ¢n phá»‘i tá»± nhiÃªn, khÃ´ng chá»‰ biá»ƒu diá»…n chÃ­nh xÃ¡c. NgoÃ i ra, hÃ m má»¥c tiÃªu cá»§a chÃºng tÃ´i táº¡o ra sá»± phÃ¢n tÃ¡ch máº¡nh hÆ¡n: trong khi H â‰¥ Î©(1/r) Ä‘á»§ trong thiáº¿t láº­p cá»§a há», thiáº¿t láº­p cá»§a chÃºng tÃ´i yÃªu cáº§u H tÄƒng Ä‘a thá»©c hoáº·c tháº­m chÃ­ hÃ m mÅ© theo d/r Ä‘á»ƒ kháº¯c phá»¥c Ä‘iá»ƒm yáº¿u cá»§a low-rank attention. Tuy nhiÃªn, cÃ¡c hÃ m má»¥c tiÃªu cá»§a há» gáº§n gÅ©i hÆ¡n vá»›i cÃ¡c loáº¡i tÃ¡c vá»¥ reasoning cÃ³ cáº¥u trÃºc mÃ  transformers thÆ°á»ng Ä‘Æ°á»£c Ã¡p dá»¥ng. Äáº·c biá»‡t, há» nháº¥n máº¡nh cÃ¡ch attention tá»± nhiÃªn phÃ¹ há»£p Ä‘á»ƒ náº¯m báº¯t tÆ°Æ¡ng tÃ¡c theo cáº·p; cÃ¡c kiáº¿n trÃºc recurrent gáº·p khÃ³ khÄƒn Ä‘á»ƒ lÃ m Ä‘iá»u nÃ y hiá»‡u quáº£, trong khi transformers gáº·p khÃ³ khÄƒn Ä‘á»ƒ náº¯m báº¯t tÆ°Æ¡ng tÃ¡c báº­c ba.

Low rank compression vÃ  fine-tuning: Nhiá»u cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y trong nÃ©n mÃ´ hÃ¬nh [LZL+23, HRP+21, BNG20] vÃ  fine-tuning [HysW+22] dá»±a trÃªn quan sÃ¡t thá»±c nghiá»‡m ráº±ng cÃ¡c ma tráº­n weight cá»§a transformers pretrained (nhÆ° cá»§a cÃ¡c máº¡ng nÆ¡-ron khÃ¡c) cÃ³ thá»ƒ Ä‘Æ°á»£c thay tháº¿ hoáº·c fine-tuned bá»Ÿi cÃ¡c proxy chiá»u tháº¥p hÆ¡n mÃ  khÃ´ng hy sinh hiá»‡u suáº¥t, vÃ  trong má»™t sá»‘ trÆ°á»ng há»£p tháº­m chÃ­ giÃºp Ã­ch [SAM24]. Nhá»¯ng káº¿t quáº£ nhÆ° váº­y cung cáº¥p bá»‘i cáº£nh cho cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i báº±ng cÃ¡ch cho tháº¥y ráº±ng full-rank khÃ´ng pháº£i lÃºc nÃ o cÅ©ng tá»‘t hÆ¡n low-rank.

ÄÃ¡nh Ä‘á»•i depth-width trong máº¡ng nÆ¡-ron: Nhiá»u cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y nghiÃªn cá»©u sá»± phÃ¢n tÃ¡ch giá»¯a cÃ¡c máº¡ng nÆ¡-ron cÃ³ Ä‘á»™ sÃ¢u khÃ¡c nhau, vÃ  giá»¯a máº¡ng nÆ¡-ron vÃ  phÆ°Æ¡ng phÃ¡p kernel. [ES16, Dan17, SS17, VJOB22] xÃ¢y dá»±ng cÃ¡c hÃ m cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ hiá»‡u quáº£ vá»›i máº¡ng nÆ¡-ron 3 lá»›p, nhÆ°ng máº¡ng 2 lá»›p yÃªu cáº§u Ä‘á»™ rá»™ng lÃ  hÃ m mÅ© theo chiá»u Ä‘áº§u vÃ o. [Tel16, CNPW19] cho tháº¥y phÃ¢n tÃ¡ch Ä‘á»™ sÃ¢u cho máº¡ng vá»›i chiá»u Ä‘áº§u vÃ o khÃ´ng Ä‘á»•i vÃ  Ä‘á»™ sÃ¢u thay Ä‘á»•i. CÃ¡c lower bounds cá»§a chÃºng tÃ´i cÅ©ng liÃªn quan máº­t thiáº¿t vá» máº·t ká»¹ thuáº­t vá»›i káº¿t quáº£ phÃ¢n tÃ¡ch giá»¯a máº¡ng nÆ¡-ron vÃ  phÆ°Æ¡ng phÃ¡p kernel. [YS19] chá»©ng minh ráº±ng random features (hoáº·c báº¥t ká»³ phÆ°Æ¡ng phÃ¡p kernel nÃ o khÃ¡c) khÃ´ng thá»ƒ há»c Ä‘Æ°á»£c ngay cáº£ má»™t nÆ¡-ron Ä‘Æ¡n trá»« khi sá»‘ lÆ°á»£ng features hoáº·c Ä‘á»™ lá»›n cá»§a weights lÃ  hÃ m mÅ© theo chiá»u Ä‘áº§u vÃ o. [KMS20] cáº£i thiá»‡n káº¿t quáº£ cá»§a há» báº±ng cÃ¡ch loáº¡i bá» sá»± phá»¥ thuá»™c vÃ o Ä‘á»™ lá»›n cá»§a weights. [GMMM21, MM23] nghiÃªn cá»©u upper vÃ  lower bounds trong viá»‡c xáº¥p xá»‰ Ä‘a thá»©c vá»›i phÆ°Æ¡ng phÃ¡p kernel. Há» cho tháº¥y ráº±ng vá» cÆ¡ báº£n, cáº§n thiáº¿t vÃ  Ä‘á»§ Ä‘á»ƒ sá»‘ lÆ°á»£ng features lÃ  hÃ m mÅ© theo báº­c cá»§a Ä‘a thá»©c Ä‘Æ°á»£c xáº¥p xá»‰. CÃ¡c lower bounds cá»§a chÃºng tÃ´i Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« cÃ´ng trÃ¬nh nÃ y.

3 Thiáº¿t láº­p vÃ  KÃ½ hiá»‡u

CÃ¡c lá»›p attention: Má»™t rank-r attention head Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi cÃ¡c ma tráº­n weight Q, K, V, O âˆˆ â„^(dÃ—r). (Má»™t sá»‘ tÃ¡c giáº£ gá»i nhá»¯ng ma tráº­n nÃ y lÃ  W^Q, W^K, W^V, vÃ  W^O.) Má»™t multi-head attention layer Ä‘Æ¡n giáº£n lÃ  tá»•ng cá»§a H attention heads nhÆ° váº­y. Äáº§u vÃ o cá»§a má»™t multi-head attention layer lÃ  má»™t dÃ£y vector xâ‚,...x_N âˆˆ â„^d gá»i lÃ  target points vÃ  má»™t dÃ£y yâ‚...y_M gá»i lÃ  source points. (LÆ°u Ã½ ráº±ng tÃªn "target points" khÃ´ng liÃªn quan Ä‘áº¿n "target function" mÃ  chÃºng ta muá»‘n xáº¥p xá»‰.) Náº¿u cÃ¡c cá»™t cá»§a X âˆˆ â„^(NÃ—d) vÃ  Y âˆˆ â„^(MÃ—d) láº§n lÆ°á»£t lÃ  target vÃ  source points, thÃ¬ má»™t softmax multi-head attention layer lÃ  má»™t hÃ m cÃ³ dáº¡ng

Hâˆ‘
h=1 O_h V_h^T X sm(X^T K_h Q_h^T Y) âˆˆ â„^(MÃ—d), (1)

trong Ä‘Ã³ sm(Â·) tÃ­nh softmax cá»§a má»—i cá»™t Ä‘áº§u vÃ o cá»§a nÃ³; nghÄ©a lÃ , vá»›i má»—i y, nÃ³ xuáº¥t ra má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn [N] dá»±a trÃªn cÃ¡c Ä‘iá»ƒm X^T K_h Q_h^T y âˆˆ â„^N. Má»™t hardmax attention layer giá»‘ng nhau, ngoáº¡i trá»« hÃ m hardmax hm(Â·) xuáº¥t ra e_{i*}, trong Ä‘Ã³ i* lÃ  chá»‰ sá»‘ cá»§a Ä‘iá»ƒm tá»‘i Ä‘a. LÆ°u Ã½ ráº±ng hardmax heads thÆ°á»ng Ä‘Æ°á»£c coi lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a softmax heads, vÃ¬ lim_{câ†’âˆ} sm(X^T c K_h Q_h^T Y) = hm(X^T K_h Q_h^T Y) trong há»™i tá»¥ pointwise.

á» trÃªn, chÃºng tÃ´i Ä‘Ã£ mÃ´ táº£ cÃ¡i gá»i lÃ  cross-attention, láº¥y cáº£ source points vÃ  target points lÃ m Ä‘áº§u vÃ o. CÃ¡c lá»›p self-attention quen thuá»™c lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t trong Ä‘Ã³ source points vÃ  target points giá»‘ng nhau: X = Y. Má»™t hÃ m multi-head attention Ä‘Ã£ cho cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho báº¥t ká»³ sá»‘ lÆ°á»£ng source hoáº·c target points nÃ o, vÃ¬ khÃ´ng cÃ³ pháº§n nÃ o cá»§a Ä‘á»‹nh nghÄ©a nÃ y phá»¥ thuá»™c vÃ o N hoáº·c M. NgoÃ i ra, nÃ³ báº¥t biáº¿n vá»›i cÃ¡c hoÃ¡n vá»‹ cá»§a target points vÃ  Ä‘áº³ng biáº¿n vá»›i cÃ¡c hoÃ¡n vá»‹ cá»§a source points.

Generalized attention: ChÃºng tÃ´i chá»©ng minh cÃ¡c lower bounds cá»§a chÃºng tÃ´i Ä‘á»‘i vá»›i má»™t lá»›p hÃ m tá»•ng quÃ¡t hÃ³a multi-head attention. Thay vÃ¬ tÃ­nh phÃ¢n phá»‘i attention nhÆ° sm(X^T K_h Q_h Y), chÃºng tÃ´i cho phÃ©p báº¥t ká»³ hÃ m nÃ o phá»¥ thuá»™c vÃ o y vÃ  má»™t phÃ©p chiáº¿u rank-r cá»§a X mÃ  xuáº¥t ra má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t trÃªn [N]. NgoÃ i ra, chÃºng tÃ´i thay tháº¿ O_h V_h báº±ng má»™t ma tráº­n Ä‘Æ¡n V_h âˆˆ â„^(dÃ—d). Do Ä‘Ã³, mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i lÃ 

Hâˆ‘
h=1 V_h X Ï†_h(K_h^T X, Y), (2)

trong Ä‘Ã³ K_h âˆˆ â„^(dÃ—r), hÃ m Ï†_h: â„^(rÃ—N) Ã— â„^d â†’ Î”_N Ä‘Æ°á»£c Ã¡p dá»¥ng column-wise cho Y vÃ  Î”_N lÃ  simplex. LÆ°u Ã½ ráº±ng hÃ m Ï†_h cÃ³ thá»ƒ thay Ä‘á»•i giá»¯a cÃ¡c head. HÆ¡n ná»¯a, chÃºng tÃ´i cho phÃ©p V_h âˆˆ â„^(dÃ—d) lÃ  full-rank. LÆ°u Ã½ ráº±ng lá»›p nÃ y náº¯m báº¯t, ngoÃ i cÃ¡c kiáº¿n trÃºc transformer tiÃªu chuáº©n, viá»‡c sá»­ dá»¥ng biases, additive positional encodings, vÃ  cÃ¡c sÆ¡ Ä‘á»“ encoding khÃ¡c nhÆ° RoPE [SAL+24] vÃ  ALiBi [PSL22] trong lá»›p attention. ChÃºng tÃ´i cÅ©ng náº¯m báº¯t cÃ¡c kiáº¿n trÃºc tá»« cÃ¡c cÃ´ng trÃ¬nh sá»›m vá» attention [BCB14, XBK+15], sá»­ dá»¥ng máº¡ng feedforward Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm attention thay vÃ¬ Ä‘iá»ƒm attention "multiplicative" hoáº·c "dot product" X^T KQY Ä‘Æ°á»£c sá»­ dá»¥ng trong transformers.

HÃ m nearest neighbor: Äáº§u vÃ o cá»§a hÃ m nearest neighbor bao gá»“m má»™t dÃ£y N target points xâ‚,...,x_N âˆˆ S^(d-1) (cÅ©ng Ä‘Æ°á»£c kÃ½ hiá»‡u bá»Ÿi X âˆˆ â„^(dÃ—N)) vÃ  má»™t source point y âˆˆ S^(d-1).

HÃ m nearest neighbor xuáº¥t ra target point gáº§n nháº¥t vá»›i source:

f(xâ‚,...,x_N;y) := arg min_{xâˆˆ{xâ‚,...x_N}} ||x - y||â‚‚. (3)

HÃ m nÃ y tÆ°Æ¡ng tá»± vá»›i viá»‡c thá»±c hiá»‡n tÃ¬m kiáº¿m ngá»¯ nghÄ©a, trong Ä‘Ã³ má»¥c tiÃªu lÃ  truy xuáº¥t entry hoáº·c tá»« trong cÆ¡ sá»Ÿ dá»¯ liá»‡u hoáº·c cá»­a sá»• ngá»¯ cáº£nh khá»›p gáº§n nháº¥t vá»›i má»™t truy váº¥n. HÃ m nÃ y cÃ³ tÃ­nh Ä‘á»‘i xá»©ng cao. Giá»‘ng nhÆ° chÃ­nh multi-head attention, nÃ³ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a cho báº¥t ká»³ N nÃ o vÃ  báº¥t biáº¿n vá»›i cÃ¡c hoÃ¡n vá»‹ cá»§a target points. NÃ³ cÅ©ng báº¥t biáº¿n vá»›i cÃ¡c phÃ©p biáº¿n Ä‘á»•i trá»±c giao Ä‘á»“ng thá»i cá»§a X vÃ  y, vÃ¬ váº­y nÃ³ khÃ´ng cÃ³ hÆ°á»›ng chÃ­nh, khÃ´ng gian con, hoáº·c quy mÃ´.

PhÃ¢n phá»‘i dá»¯ liá»‡u: ChÃºng tÃ´i rÃºt target vÃ  source points Ä‘á»u tá»« sphere. Äá»‘i vá»›i cÃ¡c lower bounds cá»§a chÃºng tÃ´i, thuáº­n tiá»‡n Ä‘á»ƒ giáº£ Ä‘á»‹nh ráº±ng target points lÃ  trá»±c giao. Vá»›i N â‰¤ d, gá»i D_N(S^(d-1)) lÃ  phÃ¢n phá»‘i Ä‘á»u trÃªn táº­p há»£p cÃ¡c dÃ£y xâ‚,...x_N âˆˆ S^(d-1) mÃ  i â‰  j âŸ¹ x_i âŠ¥ x_j. Nhá»¯ng máº«u nhÆ° váº­y cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch láº¥y N cá»™t Ä‘áº§u tiÃªn cá»§a má»™t ma tráº­n trá»±c chuáº©n ngáº«u nhiÃªn. LÆ°u Ã½ ráº±ng Ä‘iá»u nÃ y tÆ°Æ¡ng tá»± vá» báº£n cháº¥t vá»›i viá»‡c rÃºt cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘á»™c láº­p tá»« unit sphere, vÃ¬ cÃ¡c vector ngáº«u nhiÃªn Ä‘áº³ng hÆ°á»›ng trong chiá»u cao gáº§n nhÆ° trá»±c giao. PhÃ¢n phá»‘i nÃ y báº¥t biáº¿n vá»›i cÃ¡c phÃ©p biáº¿n Ä‘á»•i trá»±c giao cá»§a X vÃ  cá»§a y.

4 PhÃ¢n tÃ¡ch Low-Rank cho Nearest Neighbors

Trong má»¥c nÃ y, chÃºng tÃ´i nghiÃªn cá»©u kháº£ nÄƒng cá»§a multi-head attention Ä‘á»ƒ biá»ƒu diá»…n hÃ m nearest-neighbor. ChÃºng tÃ´i cho tháº¥y má»™t sá»± phÃ¢n tÃ¡ch trong sá»©c máº¡nh biá»ƒu diá»…n dá»±a trÃªn háº¡ng. Má»¥c tiÃªu cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n hiá»‡u quáº£ sá»­ dá»¥ng full-rank attention, nhÆ°ng dÆ°á»›i cÃ¡c giáº£ Ä‘á»‹nh dÆ°á»›i Ä‘Ã¢y, viá»‡c xáº¥p xá»‰ nÃ³ sá»­ dá»¥ng low-rank attention yÃªu cáº§u má»™t mÃ´ hÃ¬nh lá»›n hÆ¡n nhiá»u. ChÃºng tÃ´i báº¯t Ä‘áº§u vá»›i upper bound sá»­ dá»¥ng má»™t single full-rank attention head:

Fact 1 (Full-rank Efficient Approximation, Equivariant Case). Äá»‘i vá»›i hÃ m má»¥c tiÃªu tá»« PhÆ°Æ¡ng trÃ¬nh (3), báº¥t ká»³ Îµ > 0, N, d âˆˆ â„• tá»“n táº¡i K, Q, V âˆˆ â„^(dÃ—d) sao cho:

ğ”¼_{y,xâ‚,...,x_Nâˆ¼Unif(S^(d-1))} [||f(X,y) - VX sm(X^T KQ^T y)||Â²] â‰¤ Îµ. (4)

Cáº¥u trÃºc ráº¥t Ä‘Æ¡n giáº£n. Xem xÃ©t trÆ°á»ng há»£p hardmax Ä‘á»ƒ Ä‘Æ¡n giáº£n. Äáº·t V = K Q^T = I sao cho ||x_i - y||Â² = 2 - x_i^T KQ^T y. Khi Ä‘Ã³ hm(X^T KQ^T y) = e_{i*} trong Ä‘Ã³ i* = arg min_{iâˆˆ[N]} ||x_i - y||Â² vÃ  e_i lÃ  vector cÆ¡ sá»Ÿ thá»© i. LÆ°u Ã½ ráº±ng cáº¥u trÃºc nÃ y sá»­ dá»¥ng hardmax hoáº¡t Ä‘á»™ng cho báº¥t ká»³ phÃ¢n phá»‘i Ä‘áº§u vÃ o nÃ o trÃªn S^(d-1) vÃ  báº¥t ká»³ sá»‘ lÆ°á»£ng Ä‘iá»ƒm N nÃ o, vÃ¬ nÃ³ biá»ƒu diá»…n hÃ m má»¥c tiÃªu chÃ­nh xÃ¡c. TrÆ°á»ng há»£p softmax tÆ°Æ¡ng tá»±; Ä‘á»‘i vá»›i phÃ¡t biá»ƒu chÃ­nh thá»©c xem phá»¥ lá»¥c Phá»¥ lá»¥c B.1. Cáº¥u trÃºc nÃ y (hoáº·c má»™t cÃ¡i ráº¥t tÆ°Æ¡ng tá»±) dá»… dÃ ng Ä‘Æ°á»£c há»c bá»Ÿi gradient descent; xem HÃ¬nh 2.

BÃ¢y giá» chÃºng tÃ´i chuyá»ƒn sang lower bound. ChÃºng tÃ´i cho tháº¥y ráº±ng viá»‡c xáº¥p xá»‰ hÃ m má»¥c tiÃªu vá»›i rank-r heads yÃªu cáº§u sá»‘ lÆ°á»£ng head lá»›n trá»« khi r âˆ¼ d. Äá»ƒ thuáº­n tiá»‡n vá» máº·t ká»¹ thuáº­t, chÃºng tÃ´i Ä‘áº·t sá»‘ lÆ°á»£ng target points lÃ  hai vÃ  rÃºt chÃºng tá»« phÃ¢n phá»‘i Dâ‚‚(S^(d-1)) trong Ä‘Ã³ chÃºng luÃ´n trá»±c giao. Káº¿t quáº£ chÃ­nh cá»§a chÃºng tÃ´i thiáº¿t láº­p má»™t sá»± phÃ¢n tÃ¡ch Ä‘á»‹nh lÆ°á»£ng máº¡nh giá»¯a full-rank vÃ  low-rank self-attention layer, ngay cáº£ khi tá»•ng sá»‘ tham sá»‘ cÃ³ cÃ¹ng báº­c:

Theorem 2 (Low-Rank Approximation Lower Bounds, Equivariant Case). Tá»“n táº¡i cÃ¡c háº±ng sá»‘ universal c, c', C vÃ  C' sao cho náº¿u má»™t trong nhá»¯ng táº­p giáº£ Ä‘á»‹nh sau Ä‘Ãºng:

(i) High-accuracy regime: r â‰¤ d - 3, Îµ â‰¤ c/(d+1), vÃ 
H â‰¤ C Â· 2^(d-(r+1)logâ‚‚(2d/r)). (5)

(ii) High-dimensional regime: d â‰¥ 5, Îµ â‰¥ c'/(d-2)eÂ² Â· r vÃ 
H â‰¤ (1/2)(1/(2e)) Â· (d/r)^(C'/Îµ^(C'/Îµ)). (6)

ThÃ¬, Ä‘á»‘i vá»›i báº¥t ká»³ lá»±a chá»n nÃ o cá»§a H rank-r generalized attention heads Ï†â‚•: â„Ê³ Ã— 2 â†’ Î”â‚, Vâ‚• âˆˆ â„^(dÃ—d), Kâ‚• âˆˆ â„^(dÃ—r) lá»—i xáº¥p xá»‰ hÃ m nearest neighbor Ä‘Æ°á»£c giá»›i háº¡n nhÆ° sau

ğ”¼_{xâ‚,xâ‚‚âˆ¼Dâ‚‚(S^(d-1)),yâˆ¼Unif(S^(d-1))} [||f(X;y) - âˆ‘â‚•â‚Œâ‚á´´ Vâ‚•X Ï†â‚•(Kâ‚•^T X,y)||Â²â‚‚] â‰¥ Îµ, (7)

trong Ä‘Ã³ f Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° trong PhÆ°Æ¡ng trÃ¬nh (3).

Äá»ƒ chá»©ng minh Theorem 2, xem Phá»¥ lá»¥c B. Trá»±c quan, bÃ i toÃ¡n xáº¥p xá»‰ trá»Ÿ nÃªn khÃ³ hÆ¡n khi d â†’ âˆ vÃ  khi Îµ â†’ 0. Theorem 2 káº¿t há»£p cÃ¡c Ä‘áº£m báº£o trong hai cháº¿ Ä‘á»™ khÃ¡c nhau. Trong cháº¿ Ä‘á»™ Ä‘áº§u tiÃªn, Ä‘á»™ chÃ­nh xÃ¡c mong muá»‘n Îµ nhá». Trong trÆ°á»ng há»£p nÃ y, sá»‘ lÆ°á»£ng head cáº§n thiáº¿t tÄƒng theo hÃ m mÅ© vá»›i d - r. Trong cháº¿ Ä‘á»™ thá»© hai, chiá»u d lá»›n. Trong trÆ°á»ng há»£p nÃ y, sá»‘ lÆ°á»£ng head cáº§n thiáº¿t tÄƒng Ä‘a thá»©c vá»›i d/r. Má»™t cÃ¡ch khÃ´ng chÃ­nh thá»©c, cáº£ hai cháº¿ Ä‘á»™ Ä‘á»u cho tháº¥y ráº±ng lá»—i Ã­t nháº¥t lÃ  Îµ báº¥t cá»© khi nÃ o H â‰² (d/r)^(1/Îµ).

ChÃºng tÃ´i nháº¥n máº¡nh ráº±ng phÃ¢n phá»‘i dá»¯ liá»‡u lÃ  1/âˆšd-close vá»›i uniform product measure trong khoáº£ng cÃ¡ch Wasserstein, vÃ  chÃºng tÃ´i mong Ä‘á»£i cÃ¡c ká»¹ thuáº­t chá»©ng minh chÃ­nh cá»§a chÃºng tÃ´i sáº½ tá»•ng quÃ¡t hÃ³a cho measure Ä‘á»u nÃ y, cÅ©ng nhÆ° cÃ¡c phÃ¢n phá»‘i báº¥t biáº¿n quay khÃ¡c. NgoÃ i ra, trong khi N = 2 Ä‘á»§ cho má»¥c Ä‘Ã­ch cá»§a chÃºng tÃ´i Ä‘á»ƒ thiáº¿t láº­p sá»± phÃ¢n tÃ¡ch, chÃºng tÃ´i cÅ©ng tin ráº±ng framework nÃªn má»Ÿ rá»™ng cho thiáº¿t láº­p tá»•ng quÃ¡t cá»§a N > 2, máº·c dÃ¹ Ä‘iá»u nÃ y náº±m ngoÃ i pháº¡m vi hiá»‡n táº¡i.

Chá»©ng minh cá»§a chÃºng tÃ´i sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ tá»« harmonic analysis trÃªn sphere. NÃ³ gá»£i nhá»› Ä‘áº¿n cÃ´ng trÃ¬nh phÃ¢n tÃ¡ch Ä‘á»™ sÃ¢u ban Ä‘áº§u cá»§a Eldan vÃ  Shamir vÃ  Daniely [ES16, Dan17], cÅ©ng khai thÃ¡c sá»± báº¥t lá»±c cá»§a ridge functions Ä‘á»ƒ xáº¥p xá»‰ cÃ¡c má»¥c tiÃªu Ä‘á»‘i xá»©ng xuyÃªn tÃ¢m vá»›i nÄƒng lÆ°á»£ng táº§n sá»‘ cao Ä‘Ã¡ng ká»ƒ. Do tÃ­nh Ä‘á»‘i xá»©ng quay cá»§a hÃ m má»¥c tiÃªu, hÃ m attention, vÃ  phÃ¢n phá»‘i dá»¯ liá»‡u, chÃºng tÃ´i cÃ³ thá»ƒ biáº¿n Ä‘á»•i bÃ i toÃ¡n cá»§a chÃºng tÃ´i Ä‘á»ƒ phá»¥ thuá»™c vÃ o má»™t cáº·p Ä‘iá»ƒm x = xâ‚ - xâ‚‚ vÃ  y Ä‘Æ°á»£c rÃºt Ä‘á»u tá»« sphere, thay vÃ¬ xâ‚, xâ‚‚ vÃ  y. Má»¥c tiÃªu cá»§a chÃºng tÃ´i vá» cÆ¡ báº£n Ä‘Æ°á»£c cho bá»Ÿi má»™t step function cÃ³ dáº¡ng (x,y) â†¦ sgn(x^T y), cÃ³ phá»• phÃ¢n rÃ£ cháº­m vá»›i respect Ä‘áº¿n cÆ¡ sá»Ÿ thÃ­ch há»£p. ChÃºng tÃ´i xÃ¢y dá»±ng cÆ¡ sá»Ÿ nÃ y sá»­ dá»¥ng spherical harmonics, vÃ  giá»‘ng nhÆ° chÃºng, cÃ¡c hÃ m cÆ¡ sá»Ÿ cá»§a chÃºng tÃ´i Ä‘Æ°á»£c tá»• chá»©c thÃ nh cÃ¡c khÃ´ng gian con trá»±c giao dá»±a trÃªn Ä‘a thá»©c báº­c â„“. Do tÃ­nh Ä‘á»‘i xá»©ng quay, nÄƒng lÆ°á»£ng cá»§a hÃ m má»¥c tiÃªu Ä‘Æ°á»£c tráº£i Ä‘á»u trong má»—i harmonic subspace. NgÆ°á»£c láº¡i, má»—i attention head gáº¯n liá»n vá»›i má»™t vÃ i hÆ°á»›ng chÃ­nh Ä‘Æ°á»£c cho bá»Ÿi span cá»§a Kâ‚•. Káº¿t quáº£ lÃ , má»—i head chá»‰ Ä‘Æ°á»£c span bá»Ÿi má»™t pháº§n cá»§a cÃ¡c hÃ m cÆ¡ sá»Ÿ trong má»—i subspace. Do Ä‘Ã³, vá»›i sá»‘ lÆ°á»£ng head háº¡n cháº¿, khÃ´ng thá»ƒ náº¯m báº¯t má»™t pháº§n Ä‘Ã¡ng ká»ƒ nÄƒng lÆ°á»£ng cá»§a hÃ m má»¥c tiÃªu.

BÃ¢y giá» chÃºng tÃ´i nháº­n xÃ©t vá» tÃ­nh cháº·t cháº½ cá»§a lower bound nÃ y, táº­p trung vÃ o thiáº¿t láº­p canonical cá»§a r = 1. Trong trÆ°á»ng há»£p nÃ y, lower bound cá»§a chÃºng tÃ´i Ä‘Æ¡n giáº£n hÃ³a vÃ  tÄƒng cÆ°á»ng má»™t chÃºt. Vá»›i Îµ cá»‘ Ä‘á»‹nh vÃ  d lá»›n, lá»—i xáº¥p xá»‰ Ã­t nháº¥t lÃ  Îµ báº¥t cá»© khi nÃ o H = O(d^(1/(4Îµ))). ChÃºng tÃ´i cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t upper bound cho bÃ i toÃ¡n cá»§a chÃºng tÃ´i báº±ng cÃ¡ch xem xÃ©t rank-1 heads lÃ  random features. Trong Phá»¥ lá»¥c B.8, chÃºng tÃ´i láº­p luáº­n ráº±ng chÃºng ta cÃ³ thá»ƒ xáº¥p xá»‰ hÃ m má»¥c tiÃªu cá»§a chÃºng tÃ´i trong RKHS liÃªn quan Ä‘áº¿n feature map (xâ‚ - xâ‚‚, y) â†¦ sgn((xâ‚ - xâ‚‚)^T kq^T y), trong Ä‘Ã³ k vÃ  q Ä‘Æ°á»£c rÃºt Ä‘á»u tá»« unit sphere. Kernel integral operator liÃªn quan diagonalizes trong cÃ¹ng cÆ¡ sá»Ÿ cá»§a tensorized spherical harmonics Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ¡ch hÃ m má»¥c tiÃªu á»Ÿ trÃªn, vÃ  do Ä‘Ã³ xáº¥p xá»‰ kernel ridge regression cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ­ch rÃµ rÃ ng báº±ng cÃ¡ch giá»›i háº¡n spectral decay cá»§a kernel. Sau Ä‘Ã³, thÃ´ng qua cÃ¡c láº­p luáº­n tiÃªu chuáº©n tá»« random feature expansions [Bac17b], ngÆ°á»i ta cÃ³ thá»ƒ chuyá»ƒn cÃ¡c Ä‘áº£m báº£o xáº¥p xá»‰ tá»« RKHS sang mÃ´ hÃ¬nh random feature, vá»›i Ä‘iá»u kiá»‡n H = e^(Î©(dÂ²/ÎµÂ²)). Do Ä‘Ã³, vá»›i r = 1 vÃ  Îµ cá»‘ Ä‘á»‹nh, lower bound xáº¥p xá»‰ cá»§a Theorem 2 náº¯m báº¯t hÃ nh vi Ä‘á»‹nh tÃ­nh Ä‘Ãºng, máº·c dÃ¹ sá»± phá»¥ thuá»™c chÃ­nh xÃ¡c cá»§a nÃ³ vÃ o d cÃ³ thá»ƒ khÃ´ng cháº·t cháº½.

5 PhÃ¢n tÃ¡ch HÃ m mÅ© cho Biased Nearest Neighbors

Trong má»¥c nÃ y, chÃºng tÃ´i cho tháº¥y má»™t cÃ¡ch khÃ¡c Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c sá»± phÃ¢n tÃ¡ch hÃ m mÅ© trong cháº¿ Ä‘á»™ Ä‘á»™ chÃ­nh xÃ¡c cao sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t khÃ¡c nhau vÃ  má»™t hÃ m má»¥c tiÃªu Ä‘Ã£ sá»­a Ä‘á»•i. Cho b = [bâ‚,...b_N]^T, hÃ m biased nearest neighbor Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:

f_b(xâ‚,...,x_N;y) = arg min_{x_iâˆˆ{xâ‚,...,x_N}} [||x_i - y||Â²â‚‚ + b_i]. (8)

Giá»‘ng nhÆ° hÃ m nearest neighbor khÃ´ng bias cá»§a PhÆ°Æ¡ng trÃ¬nh (3), nÃ³ báº¥t biáº¿n vá»›i cÃ¡c phÃ©p biáº¿n Ä‘á»•i trá»±c giao Ä‘á»“ng thá»i cá»§a X vÃ  y; tuy nhiÃªn, nÃ³ khÃ´ng báº¥t biáº¿n vá»›i cÃ¡c hoÃ¡n vá»‹ cá»§a target point X. ChÃºng tÃ´i Ä‘áº§u tiÃªn cho tháº¥y ráº±ng má»™t single full-rank attention head cÃ³ thá»ƒ xáº¥p xá»‰ má»¥c tiÃªu nÃ y chÃ­nh xÃ¡c, vá»›i Ä‘iá»u kiá»‡n biases Ä‘Æ°á»£c thÃªm vÃ o kiáº¿n trÃºc:

Fact 3 (Full Rank Efficient Approximation, Biased Case). Äá»‘i vá»›i báº¥t ká»³ chiá»u d, sá»‘ Ä‘iá»ƒm N, vÃ  bias b âˆˆ â„^N, má»™t single biased full-rank hardmax attention head cÃ³ thá»ƒ biá»ƒu diá»…n chÃ­nh xÃ¡c hÃ m biased nearest neighbor Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong PhÆ°Æ¡ng trÃ¬nh (8).

Cáº¥u trÃºc giá»‘ng nhÆ° cá»§a Fact 1 vá»›i viá»‡c thÃªm biases b bÃªn trong hardmax. NghÄ©a lÃ , head triá»ƒn khai X hm(X^T y + b) trong trÆ°á»ng há»£p hardmax. Trong Phá»¥ lá»¥c C chÃºng tÃ´i chá»©ng minh trÆ°á»ng há»£p softmax. LÆ°u Ã½ ráº±ng kiáº¿n trÃºc nÃ y lÃ  trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a attention tiÃªu chuáº©n vá»›i concatenated positional encodings. Gá»i positional encoding cho x_i lÃ  scalar b_i, gá»i positional encoding cho y_i lÃ  1, vÃ  gá»i KQ^T = [I_{dÃ—d} Â· 1]. Khi Ä‘Ã³ [X^T b] [KQ^T] [y 1]^T = X^T y + b.

BÃ¢y giá» chÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ chÃ­nh cá»§a má»¥c nÃ y cho tháº¥y ráº±ng ngay cáº£ vá»›i N = 2, tá»“n táº¡i má»™t hÃ m biased nearest neighbor khÃ³ xáº¥p xá»‰ sá»­ dá»¥ng low rank attention heads:

Theorem 4 (Low-rank Approximation Lower Bounds, biased case). Tá»“n táº¡i b = [bâ‚,bâ‚‚]^T âˆˆ â„Â² sao cho Ä‘á»‘i vá»›i hÃ m f_b Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong PhÆ°Æ¡ng trÃ¬nh (8) Ä‘iá»u sau Ä‘Ãºng: Äá»‘i vá»›i báº¥t ká»³ lá»±a chá»n nÃ o cá»§a rank-r heads gâ‚,...,g_H trong Ä‘Ã³ gâ‚• = Vâ‚•X Ï†â‚•(Kâ‚•X,y), Kâ‚• cÃ³ rank-r vÃ  Ï†â‚• lÃ  cÃ¡c hÃ m tÃ¹y Ã½ xuáº¥t ra má»™t vector trong simplex Î”â‚, náº¿u H Â· max_h ||Vâ‚•|| â‰¤ exp(câ‚(d-r))/(dÂ²câ‚‚) thÃ¬:

ğ”¼_{xâ‚,xâ‚‚âˆ¼Dâ‚‚(âˆšd S^{d-1}),yâˆ¼N(0,I)} [||f_b(xâ‚,xâ‚‚,y) - âˆ‘_{h=1}^H gâ‚•(xâ‚,xâ‚‚,y)||Â²â‚‚] > 1/20, (9)

cho má»™t sá»‘ háº±ng sá»‘ universal câ‚,câ‚‚ > 0.

Chá»©ng minh Ä‘áº§y Ä‘á»§ Ä‘Æ°á»£c Ä‘Æ°a vÃ o Phá»¥ lá»¥c C. Äá»‹nh lÃ½ phÃ¡t biá»ƒu ráº±ng trá»« khi sá»‘ lÆ°á»£ng attention heads hoáº·c Ä‘á»™ lá»›n cá»§a output weights (hoáº·c cáº£ hai) lÃ  hÃ m mÅ© theo d - r, thÃ¬ rank-r attention heads khÃ´ng thá»ƒ xáº¥p xá»‰ má»¥c tiÃªu, ngay cáº£ Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng Ä‘á»•i. Äiá»u nÃ y trÃ¡i ngÆ°á»£c vá»›i thá»±c táº¿ ráº±ng má»™t single full-rank head (vá»›i positional encoding) cÃ³ thá»ƒ xáº¥p xá»‰ má»¥c tiÃªu Ä‘áº¿n báº¥t ká»³ Ä‘á»™ chÃ­nh xÃ¡c cho trÆ°á»›c nÃ o. LÆ°u Ã½ ráº±ng sá»± phÃ¢n tÃ¡ch hÃ m mÅ© ráº¥t máº¡nh vá» máº·t háº¡ng cá»§a attention heads. Cá»¥ thá»ƒ, cÃ³ háº¡ng O(d) khÃ´ng Ä‘á»§ Ä‘á»ƒ phÃ¡ vá»¡ sá»± phÃ¢n tÃ¡ch nÃ y, vÃ­ dá»¥ ngay cáº£ náº¿u r = 99/100 Â· d váº«n cÃ³ sá»± phÃ¢n tÃ¡ch hÃ m mÅ© giá»¯a full rank vÃ  rank-r attentions heads cho chiá»u Ä‘áº§u vÃ o d Ä‘á»§ lá»›n.

Remark 5 (Bound on the weights). LÆ°u Ã½ ráº±ng trÃ¡i ngÆ°á»£c vá»›i Theorem 2, á»Ÿ Ä‘Ã¢y chÃºng ta cÃ³ upper bound hÃ m mÅ© trÃªn weights cá»§a tá»• há»£p tuyáº¿n tÃ­nh Vâ‚•, cá»¥ thá»ƒ lÃ  sá»‘ lÆ°á»£ng heads hoáº·c norm cá»§a weights cáº§n pháº£i lÃ  hÃ m mÅ© Ä‘á»ƒ phÃ¡ vá»¡ sá»± phÃ¢n tÃ¡ch. Bound nÃ y cÅ©ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong [YS19] Ä‘Ã£ truyá»n cáº£m há»©ng cho chá»©ng minh cá»§a chÃºng tÃ´i. Trong [KMS20] cÃ¡c tÃ¡c giáº£ Ä‘Ã£ cÃ³ thá»ƒ loáº¡i bá» bound nÃ y báº±ng cÃ¡ch Ã¡p dá»¥ng phÃ¢n tÃ­ch phá»©c táº¡p hÆ¡n sá»­ dá»¥ng cÃ¡c láº­p luáº­n SQ-dimension, tuy nhiÃªn trong trÆ°á»ng há»£p cá»§a chÃºng tÃ´i khÃ´ng rÃµ lÃ m tháº¿ nÃ o Ä‘á»ƒ má»Ÿ rá»™ng ká»¹ thuáº­t cá»§a há» vÃ¬ sá»± phá»¥ thuá»™c vÃ o r. ChÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng váº«n cÃ³ thá»ƒ loáº¡i bá» bound nÃ y, vÃ  Ä‘á»ƒ láº¡i cho cÃ´ng trÃ¬nh tÆ°Æ¡ng lai.

Trá»±c quan chá»©ng minh. Trá»ng tÃ¢m cá»§a chá»©ng minh Theorem 4 lÃ  táº¡o ra má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a nhiá»u threshold functions hoáº¡t Ä‘á»™ng nhÆ° má»™t hÃ m tuáº§n hoÃ n vá»›i táº§n sá»‘ cao. Chá»©ng minh cá»§a chÃºng tÃ´i Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« vÃ  má»Ÿ rá»™ng phÆ°Æ¡ng phÃ¡p chá»©ng minh cá»§a [YS19] Ä‘á»ƒ phÃ¢n tÃ¡ch giá»¯a phÆ°Æ¡ng phÃ¡p kernel vÃ  máº¡ng nÆ¡-ron 2 lá»›p. Chi tiáº¿t hÆ¡n, lÆ°u Ã½ ráº±ng má»¥c tiÃªu cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t láº¡i nhÆ° tá»•ng cá»§a hai threshold functions:

f_b(xâ‚,xâ‚‚,y) = arg max_{x_i} âŸ¨x_i,yâŸ© + b_i = ğŸ™(âŸ¨xâ‚-xâ‚‚,yâŸ© + b* > 0)xâ‚ + ğŸ™(âŸ¨xâ‚-xâ‚‚,yâŸ© + b* < 0)xâ‚‚, (10)

trong Ä‘Ã³ b* = bâ‚ - bâ‚‚ sáº½ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh sau. KÃ½ hiá»‡u x := xâ‚ - xâ‚‚; chÃºng tÃ´i sáº½ táº­p trung vÃ o viá»‡c cho tháº¥y tÃ­nh khÃ³ cá»§a xáº¥p xá»‰ cho threshold function Ä‘áº§u tiÃªn ğŸ™(âŸ¨x,yâŸ© + b* > 0), tá»« Ä‘Ã³ tÃ­nh khÃ³ cá»§a xáº¥p xá»‰ cho f_b theo sau báº±ng cÃ¡c láº­p luáº­n tiÃªu chuáº©n. ChÃºng tÃ´i Ä‘á»‹nh nghÄ©a má»™t hÃ m tuáº§n hoÃ n Ïˆ_a(z): â„ â†’ â„ trong khoáº£ng [-a,a] lÃ  tá»• há»£p tuyáº¿n tÃ­nh cá»§a a threshold functions (táº¡i cÃ¡c break points khÃ¡c nhau), trong Ä‘Ã³ a = Î©(dÂ²), vÃ  cho tháº¥y ráº±ng Ä‘á»‘i vá»›i báº¥t ká»³ hÃ m g nÃ o chá»‰ phá»¥ thuá»™c vÃ o má»™t phÃ©p chiáº¿u K cá»§a x lÃªn má»™t r-dimensional subspace chÃºng ta cÃ³:

ğ”¼_{x,y}[|Ïˆ_a(âŸ¨x,yâŸ©) Â· g(Kx,y)|] â‰¤ ||g|| Â· exp(-Î©(d-r)). (11)

Äáº·c biá»‡t, náº¿u báº¥t ká»³ single threshold function nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng Ïˆ_a cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ bá»Ÿi má»™t rank-r attention layer vá»›i H/a heads, thÃ¬ Ïˆ_a cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ bá»Ÿi má»™t rank-r attention layer vá»›i H heads. Tuy nhiÃªn, Ä‘iá»u nÃ y khÃ´ng thá»ƒ náº¿u r nhá» vÃ¬ a chá»‰ lÃ  polynomial trong d, vÃ  correlation giá»¯a má»—i head vÃ  Ïˆ_a lÃ  exponentially small. Do Ä‘Ã³, tá»“n táº¡i má»™t threshold function vá»›i break point táº¡i b* khÃ³ xáº¥p xá»‰, trá»« khi sá»‘ lÆ°á»£ng heads cÃ³ báº­c O(exp(d-r)/a). Trong Theorem 4, cÃ¡c Ä‘áº§u vÃ o xâ‚ vÃ  xâ‚‚ Ä‘Æ°á»£c rÃºt tá»« unit sphere Ä‘Æ°á»£c scale bá»Ÿi há»‡ sá»‘ âˆšd. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng viá»‡c re-scaling cÃ¡c Ä‘áº§u vÃ o tÆ°Æ¡ng tá»± nhÆ° viá»‡c giáº£m Ä‘á»™ chÃ­nh xÃ¡c yÃªu cáº§u bá»Ÿi cÃ¹ng há»‡ sá»‘. Do Ä‘Ã³, káº¿t quáº£ phÃ¢n tÃ¡ch hÃ m mÅ© nÃ y giá»‘ng vá»›i cháº¿ Ä‘á»™ Ä‘á»™ chÃ­nh xÃ¡c cao cá»§a Theorem 2, máº·c dÃ¹ cÃ¡c ká»¹ thuáº­t Ä‘Æ°á»£c sá»­ dá»¥ng trong chá»©ng minh ráº¥t khÃ¡c nhau.

6 Xáº¥p xá»‰ Hiá»‡u quáº£ Sá»­ dá»¥ng Äá»™ sÃ¢u

Trong cÃ¡c má»¥c trÆ°á»›c, chÃºng tÃ´i Ä‘Ã£ cho tháº¥y ráº±ng má»™t lá»›p low-rank attention Ä‘Æ¡n khÃ´ng thá»ƒ biá»ƒu diá»…n má»¥c tiÃªu trá»« khi sá»‘ lÆ°á»£ng heads ráº¥t lá»›n. Trong má»¥c nÃ y, chÃºng tÃ´i Ä‘á» cáº­p Ä‘áº¿n cÃ¢u há»i liá»‡u cÃ¡c lá»›p Ä‘á»™ sÃ¢u bá»• sung cÃ³ thá»ƒ kháº¯c phá»¥c Ä‘iá»ƒm yáº¿u nÃ y khÃ´ng. Äá»™ sÃ¢u cÃ³ thá»ƒ cÃ³ nghÄ©a lÃ  thÃªm má»™t MLP sau lá»›p attention hoáº·c chá»‰ lÃ  má»™t lá»›p attention khÃ¡c; trong má»¥c nÃ y chÃºng tÃ´i xem xÃ©t cáº£ hai lá»±a chá»n. ChÃºng tÃ´i trÃ¬nh bÃ y má»™t cáº¥u trÃºc xáº¥p xá»‰ hÃ m má»¥c tiÃªu (vá»›i Ä‘áº§u vÃ o Ä‘Æ°á»£c sá»­a Ä‘á»•i má»™t chÃºt) sá»­ dá»¥ng hai lá»›p vÃ  chá»‰ polynomial nhiá»u rank-1 heads. Tuy nhiÃªn, chÃºng tÃ´i trÃ¬nh bÃ y cÃ¡c cáº¥u trÃºc chá»‰ cho trÆ°á»ng há»£p Ä‘á»™ dÃ i ngá»¯ cáº£nh N = 2, cÅ©ng lÃ  thiáº¿t láº­p cá»§a cÃ¡c lower bounds cá»§a chÃºng tÃ´i. ChÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng báº¥t ká»³ cáº¥u trÃºc nÃ o sá»­ dá»¥ng low-rank heads Ä‘á»u Ä‘Æ°a ra sá»± phá»¥ thuá»™c báº¥t lá»£i vÃ o N, má»™t Ä‘iá»ƒm yáº¿u Ä‘Ã¡ng ká»ƒ so vá»›i full-rank attention.

CÃ¡c cáº¥u trÃºc cá»§a chÃºng tÃ´i dá»±a trÃªn chiáº¿n lÆ°á»£c mÃ  chÃºng tÃ´i gá»i lÃ  "majority voting", chÃºng tÃ´i mÃ´ táº£ ngáº¯n gá»n á»Ÿ Ä‘Ã¢y. Xem xÃ©t trÆ°á»ng há»£p N = 2 target points vÃ  hardmax attention. Äáº§u ra cá»§a má»—i head, giá»‘ng nhÆ° chÃ­nh hÃ m má»¥c tiÃªu, lÃ  xâ‚ hoáº·c xâ‚‚. Má»™t random rank-1 head cÃ³ correlation yáº¿u vá»›i má»¥c tiÃªu; xÃ¡c suáº¥t nÃ³ xuáº¥t ra cÃ¢u tráº£ lá»i Ä‘Ãºng lÃ  1/2 + Î©(1/âˆšd). Do Ä‘Ã³, viá»‡c káº¿t há»£p nhiá»u random heads nhÆ° váº­y láº¡i vá»›i nhau, mode cá»§a chÃºng (Ä‘áº§u ra vá»›i nhiá»u "votes" nháº¥t) khá»›p vá»›i hÃ m má»¥c tiÃªu vá»›i xÃ¡c suáº¥t cao. ChÃºng tÃ´i sá»­ dá»¥ng lá»›p thá»© hai Ä‘á»ƒ tÃ­nh "majority vote" cá»§a cÃ¡c heads trong lá»›p attention.

CÃ¡c cÆ¡ cháº¿ attention tiÃªu chuáº©n lÃ m cho viá»‡c Ä‘áº¿m sá»‘ lÆ°á»£ng votes mÃ  má»—i target point nháº­n Ä‘Æ°á»£c trá»Ÿ nÃªn khÃ³ khÄƒn â€” hoáº·c tháº­m chÃ­ nhá»› cÃ¡c target points xâ‚ vÃ  xâ‚‚ lÃ  gÃ¬ â€” vÃ¬ lá»›p tiáº¿p theo chá»‰ nháº­n Ä‘Æ°á»£c tá»• há»£p tuyáº¿n tÃ­nh cá»§a chÃºng vá»›i cÃ¡c há»‡ sá»‘ khÃ´ng biáº¿t. Do Ä‘Ã³, chÃºng tÃ´i sá»­a Ä‘á»•i lá»›p attention má»™t chÃºt Ä‘á»ƒ táº¡o Ä‘iá»u kiá»‡n cho chiáº¿n lÆ°á»£c majority voting. ChÃºng tÃ´i concatenate cÃ¡c labels vá»›i cÃ¡c vectors cho phÃ©p chÃºng ta Ä‘áº¿m bao nhiá»u láº§n xâ‚ vÃ  xâ‚‚ xuáº¥t hiá»‡n trong tá»•ng. Sau Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng lá»›p attention thá»© hai Ä‘á»ƒ tra cá»©u full vector tÆ°Æ¡ng á»©ng vá»›i majority label. Labeling nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai báº±ng cÃ¡ch concatenate positional encodings vá»›i cÃ¡c input points. NghÄ©a lÃ , thay vÃ¬ nháº­p xâ‚,...,x_N âˆˆ S^(d-1) vÃ o transformer, bÃ¢y giá» chÃºng ta nháº­p [xâ‚ bâ‚],...,[x_N b_N] cho b_i âˆˆ â„^e. Má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ã¡nh xáº¡ Ä‘áº§u ra cá»§a transformer (d+e)-dimensional nÃ y trá»Ÿ láº¡i â„^d. LÆ°u Ã½ ráº±ng hÃ m má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  permutation-invariant, vÃ¬ váº­y thá»© tá»± cá»§a cÃ¡c Ä‘iá»ƒm khÃ´ng liÃªn quan Ä‘áº¿n tÃ¡c vá»¥. Do Ä‘Ã³, nhá»¯ng "positional encodings" Ä‘Æ°á»£c concatenate nÃ y hoáº¡t Ä‘á»™ng giá»‘ng nhÆ° sá»­a Ä‘á»•i kiáº¿n trÃºc. ChÃºng cung cáº¥p cÃ¡c chiá»u Ä‘áº§u vÃ o bá»• sung phá»¥c vá»¥ nhÆ° khÃ´ng gian scratch trong Ä‘Ã³ mÃ´ hÃ¬nh cÃ³ thá»ƒ thá»±c hiá»‡n cÃ¡c thao tÃ¡c rá»i ráº¡c nhÆ° Ä‘áº¿m vÃ  indexing mÃ  khÃ´ng lÃ m há»ng dá»¯ liá»‡u Ä‘áº§u vÃ o. CÅ©ng lÆ°u Ã½ ráº±ng, vÃ¬ chÃºng thay Ä‘á»•i chiá»u cá»§a Ä‘áº§u vÃ o vÃ  cá»§a transformer, nhá»¯ng concatenated positional encodings nÃ y khÃ¡c vá»›i positional encodings Ä‘Æ°á»£c sá»­ dá»¥ng trong thá»±c táº¿ (bao gá»“m RoPE [SAL+24] vÃ  ALiBi [PSL22]), Ä‘Æ°á»£c bao gá»“m trong framework generalized attention cá»§a chÃºng tÃ´i.

DÆ°á»›i Ä‘Ã¢y, chÃºng tÃ´i Ä‘Æ°a ra Ä‘á»‹nh nghÄ©a chÃ­nh thá»©c cá»§a kiáº¿n trÃºc multi-layer transformer Ä‘Æ°á»£c sá»­ dá»¥ng trong cáº¥u trÃºc cá»§a chÃºng tÃ´i. NÃ³ sá»­ dá»¥ng self-attention, cÃ³ nghÄ©a lÃ  source vÃ  target points giá»‘ng nhau. ChÃºng tÃ´i sá»­a Ä‘á»•i cÆ¡ cháº¿ attention báº±ng cÃ¡ch thÃªm self-excluding mask sao cho má»—i input point khÃ´ng thá»ƒ attend Ä‘áº¿n chÃ­nh nÃ³ (xem dÆ°á»›i Ä‘Ã¢y, trong Ä‘Ã³ chÃºng tÃ´i táº¡o thÃ nh XÌƒáµ¢ báº±ng cÃ¡ch xÃ³a cá»™t thá»© i cá»§a X). Theo thá»±c táº¿ tiÃªu chuáº©n, chÃºng tÃ´i cÅ©ng sá»­ dá»¥ng skip connection. ChÃºng tÃ´i khÃ´ng cáº§n MLP hoáº·c normalization layer, máº·c dÃ¹ cáº¥u trÃºc cá»§a chÃºng tÃ´i cÃ³ thá»ƒ dá»… dÃ ng Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ bao gá»“m chÃºng.

Definition 6. Má»™t rank-r self-masked transformer layer vá»›i H heads lÃ  má»™t hÃ m T: â„^(dÃ—N) â†’ â„^(dÃ—N) Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi rank-k attention heads {(Mâ‚•,Vâ‚•)}á´´â‚•â‚Œâ‚ vÃ  Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:

XÌƒáµ¢ := [xâ‚ Â·Â·Â· xáµ¢â‚‹â‚ xáµ¢â‚Šâ‚ Â·Â·Â· x_N] (12)
Táµ¢(X) := xáµ¢ + âˆ‘á´´â‚•â‚Œâ‚ Vâ‚•XÌƒáµ¢ sm(XÌƒáµ¢áµ€Mâ‚•xáµ¢) (13)(14)

á» Ä‘Ã¢y, Táµ¢ kÃ½ hiá»‡u Ä‘áº§u ra thá»© i (hoáº·c cá»™t thá»© i cá»§a Ä‘áº§u ra) [Tâ‚(X) Â·Â·Â· T_N(X)].

Má»™t two layer, rank-r transformer vá»›i concatenated positional encodings lÃ  má»™t hÃ m T: â„^(dÃ—N) â†’ â„^(dÃ—N) Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi ma tráº­n positional encoding E = â„^(dâ‚‘Ã—N) vÃ  hai (d+dâ‚‘)-dimensional self-masked transformer layers, Tâ½Â¹â¾ vÃ  Tâ½Â²â¾, vÃ  ma tráº­n output-layer A âˆˆ â„^(dÃ—(d+dâ‚‘)) vÃ  Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau:

T(X) = A Â· Tâ½Â²â¾(Tâ½Â¹â¾([X E])). (15)

Äá»‹nh lÃ½ sau mÃ´ táº£ cáº¥u trÃºc majority voting cá»§a chÃºng tÃ´i sá»­ dá»¥ng random rank-1 heads vÃ  concatenated positional encodings. Äá»ƒ chá»©ng minh, xem Phá»¥ lá»¥c D.3.

Theorem 7. Tá»“n táº¡i cÃ¡c háº±ng sá»‘ universal câ‚,câ‚‚ sao cho vá»›i táº¥t cáº£ d > câ‚, vÃ  Îµ âˆˆ (0,1/2), vÃ  H â‰¥ câ‚‚ Â· dÂ³/ÎµÂ², tá»“n táº¡i má»™t two layer, rank-1 transformer T vá»›i H heads vÃ  dâ‚‘ = 2 (nhÆ° Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Definition 6) mÃ 

ğ”¼_{xâ‚,xâ‚‚,yâˆ¼Unif(S^(d-1))} [||f(xâ‚,xâ‚‚;y) - T([xâ‚ xâ‚‚ y])||Â²â‚‚] â‰¤ Îµ. (16)

NgÆ°á»i ta cÃ³ thá»ƒ tá»± há»i liá»‡u concatenated positional encodings cÃ³ cáº§n thiáº¿t Ä‘á»ƒ lÃ m cho cáº¥u trÃºc nÃ y hoáº¡t Ä‘á»™ng khÃ´ng, Ä‘áº·c biá»‡t vÃ¬ chÃºng phÃ¡ vá»¡ permutation invariance Ä‘á»ƒ biá»ƒu diá»…n má»™t má»¥c tiÃªu permutation invariant. Trong Phá»¥ lá»¥c D, chÃºng tÃ´i trÃ¬nh bÃ y má»™t cáº¥u trÃºc thay tháº¿ (Theorem 34) lÃ  permutation invariant. Tuy nhiÃªn, nÃ³ sá»­a Ä‘á»•i kiáº¿n trÃºc báº±ng cÃ¡ch Ã¡p dá»¥ng MLP cho concatenation cá»§a cÃ¡c Ä‘áº§u ra cá»§a attention heads thay vÃ¬ tá»•ng cá»§a chÃºng.

Máº·c dÃ¹ cÃ¡c cáº¥u trÃºc cá»§a chÃºng tÃ´i giáº£ Ä‘á»‹nh N = 2 source points, cÃ³ váº» kháº£ thi Ä‘á»ƒ tá»•ng quÃ¡t hÃ³a chÃºng cho N lá»›n hÆ¡n. Tuy nhiÃªn, nhÆ°á»£c Ä‘iá»ƒm chÃ­nh cá»§a viá»‡c tá»•ng quÃ¡t hÃ³a nhÆ° váº­y lÃ  kÃ­ch thÆ°á»›c cá»§a transformer sáº½ phá»¥ thuá»™c vÃ o N. Ngay cáº£ bÆ°á»›c Ä‘Æ¡n giáº£n nháº¥t cá»§a viá»‡c tÃ­nh majority giá»¯a N possible terms dÆ°á»ng nhÆ° khÃ´ng thá»ƒ mÃ  khÃ´ng cÃ³ Ã­t nháº¥t sá»± phá»¥ thuá»™c tuyáº¿n tÃ­nh vÃ o N. Máº·t khÃ¡c, Fact 1 cho tháº¥y ráº±ng hÃ m má»¥c tiÃªu cÃ³ thá»ƒ Ä‘Æ°á»£c xáº¥p xá»‰ cho báº¥t ká»³ N nÃ o sá»­ dá»¥ng má»™t single full rank attention. ChÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng sá»± phá»¥ thuá»™c nhÆ° váº­y vÃ o N lÃ  cáº§n thiáº¿t khi sá»­ dá»¥ng low-rank attention:

Conjecture 8. KhÃ´ng cÃ³ multi-layer transformer nÃ o (vá»›i kÃ­ch thÆ°á»›c vÃ  ma tráº­n weight cá»‘ Ä‘á»‹nh) cÃ³ rank r < d xáº¥p xá»‰ má»¥c tiÃªu cá»§a PhÆ°Æ¡ng trÃ¬nh (3) cho táº¥t cáº£ N.

NghÄ©a lÃ , trong khi cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t transformer xáº¥p xá»‰ má»¥c tiÃªu cho má»™t N cá»‘ Ä‘á»‹nh cho trÆ°á»›c (nhÆ° chÃºng tÃ´i lÃ m á»Ÿ trÃªn), chÃºng tÃ´i Ä‘Æ°a ra giáº£ thuyáº¿t ráº±ng khÃ´ng cÃ³ cáº¥u trÃºc nÃ o nhÆ° váº­y Ä‘á»™c láº­p vá»›i N. Viá»‡c chá»©ng minh hoáº·c bÃ¡c bá» giáº£ thuyáº¿t trÃªn sáº½ cÃ³ nhá»¯ng tÃ¡c Ä‘á»™ng ráº¥t khÃ¡c nhau. Má»™t pháº£n vÃ­ dá»¥ cÃ³ nghÄ©a lÃ  Ä‘iá»ƒm yáº¿u cá»§a low-rank cÃ³ thá»ƒ Ä‘Æ°á»£c bÃ¹ Ä‘áº¯p bá»Ÿi Ä‘á»™ sÃ¢u, vÃ  do Ä‘Ã³ háº¡ng khÃ´ng Ä‘Ã³ng vai trÃ² quyáº¿t Ä‘á»‹nh trong sá»©c máº¡nh biá»ƒu Ä‘áº¡t cá»§a multi-layer transformers. Má»™t chá»©ng minh sáº½ cho tháº¥y ráº±ng, ngay cáº£ trong trÆ°á»ng há»£p multi-layer, low-rank attention vá» cÆ¡ báº£n yáº¿u hÆ¡n high-rank attention.

7 ThÃ­ nghiá»‡m

Trong má»¥c nÃ y, chÃºng tÃ´i bá»• sung káº¿t quáº£ lÃ½ thuyáº¿t cá»§a chÃºng tÃ´i báº±ng thÃ­ nghiá»‡m trÃªn má»™t lá»›p kiáº¿n trÃºc rá»™ng hÆ¡n. ChÃºng tÃ´i huáº¥n luyá»‡n off-the-shelf transformers â€” bao gá»“m nhiá»u lá»›p self-attention, MLP layers, skip connections, vÃ  normalization â€” trÃªn má»™t sá»­a Ä‘á»•i nhá» cá»§a hÃ m nearest neighbor. CÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i xÃ¡c nháº­n Ä‘iá»ƒm yáº¿u cá»§a low-rank attention trong thiáº¿t láº­p nÃ y. ChÃºng cÅ©ng cho tháº¥y ráº±ng cáº¥u trÃºc full-rank cá»§a Fact 1 dá»… dÃ ng Ä‘Æ°á»£c há»c bá»Ÿi gradient descent. Táº¥t cáº£ code cÃ³ sáºµn táº¡i https://github.com/NoahAmsel/attention-formers.

Chi tiáº¿t mÃ´ hÃ¬nh vÃ  huáº¥n luyá»‡n: ChÃºng tÃ´i sá»­ dá»¥ng triá»ƒn khai Pytorch cá»§a transformer encoders [PGM+19] vá»›i hai sá»­a Ä‘á»•i. Äáº§u tiÃªn, chÃºng tÃ´i tá»•ng quÃ¡t hÃ³a quy mÃ´ tiÃªu chuáº©n H = d/r, cho phÃ©p H lÃ  báº¥t ká»³ bá»™i sá»‘ nÃ o cá»§a d/r. (Äáº·c biá»‡t, chÃºng tÃ´i thá»­ H = d^1.5/r vÃ  H = dÂ²/r.) Thá»© hai, chÃºng tÃ´i thay tháº¿ layer normalization báº±ng RMSNorm [ZS19], má»™t lá»±a chá»n tiÃªu chuáº©n trong transformers hiá»‡n Ä‘áº¡i [TLI+23, CND+24] cÅ©ng phÃ¹ há»£p hÆ¡n vá»›i hÃ m má»¥c tiÃªu cá»§a chÃºng tÃ´i. ChÃºng tÃ´i huáº¥n luyá»‡n vá»›i biases, nhÆ°ng thÃ­ nghiá»‡m sÆ¡ bá»™ cho tháº¥y ráº±ng chÃºng Ã­t áº£nh hÆ°á»ŸngÂ¹. ChÃºng tÃ´i cháº¡y má»—i thÃ­ nghiá»‡m trÃªn má»™t Nvidia GPU Ä‘Æ¡n (thÆ°á»ng lÃ  V100) khÃ´ng quÃ¡ vÃ i giá».

VÃ¬ chÃºng tÃ´i Ä‘ang sá»­ dá»¥ng self-attention, khÃ´ng cÃ³ sá»± phÃ¢n biá»‡t giá»¯a source vÃ  target points. N input points Ä‘Æ°á»£c rÃºt Ä‘á»u vÃ  i.i.d. tá»« S^(d-1), vÃ  chÃºng khÃ´ng bá»‹ rÃ ng buá»™c pháº£i trá»±c giao. ChÃºng tÃ´i thay Ä‘á»•i hÃ m má»¥c tiÃªu cá»§a chÃºng tÃ´i tÆ°Æ¡ng á»©ng. Äá»‘i vá»›i má»—i input point, má»¥c tiÃªu bÃ¢y giá» xuáº¥t ra Ä‘iá»ƒm nÃ o trong sá»‘ cÃ¡c Ä‘iá»ƒm khÃ¡c xa nháº¥t vá»›i nÃ³. ChÃºng tÃ´i xuáº¥t ra xa nháº¥t thay vÃ¬ gáº§n nháº¥t vÃ¬ náº¿u khÃ´ng, má»—i Ä‘iá»ƒm sáº½ Ã¡nh xáº¡ Ä‘áº¿n chÃ­nh nÃ³. Loss function lÃ  trung bÃ¬nh mean squared error trÃªn N Ä‘iá»ƒm. ChÃºng tÃ´i khÃ´ng sá»­ dá»¥ng attention mask nÃ o. Äáº·c biá»‡t, chÃºng tÃ´i cho phÃ©p cÃ¡c Ä‘iá»ƒm attend Ä‘áº¿n chÃ­nh chÃºng. Dataset cá»§a chÃºng tÃ´i lÃ  synthetic, vÃ¬ váº­y chÃºng tÃ´i huáº¥n luyá»‡n vÃ  test trÃªn má»™t stream cÃ¡c máº«u Ä‘Æ°á»£c táº¡o má»›i mÃ  khÃ´ng bao giá» láº·p láº¡i. ChÃºng tÃ´i huáº¥n luyá»‡n trÃªn 10âµ batches kÃ­ch thÆ°á»›c 256 má»—i batch. Äá»‘i vá»›i táº¥t cáº£ thÃ­ nghiá»‡m, chÃºng tÃ´i sá»­ dá»¥ng AdamW vá»›i cÃ¹ng learning rate 0.01 vÃ  learning rate schedule cosine annealing vá»›i linear warm-up.

Rank separation: ThÃ­ nghiá»‡m Ä‘áº§u tiÃªn cá»§a chÃºng tÃ´i nghiÃªn cá»©u táº§m quan trá»ng cá»§a háº¡ng qua cÃ¡c sá»‘ lÆ°á»£ng heads (H) vÃ  layers (L) khÃ¡c nhau. ChÃºng tÃ´i cá»‘ Ä‘á»‹nh chiá»u d = 64 vÃ  sá»‘ Ä‘iá»ƒm N = 16. Trong thÃ­ nghiá»‡m nÃ y, chÃºng tÃ´i khÃ´ng sá»­ dá»¥ng positional encodings. HÃ¬nh 1 váº½ káº¿t quáº£, cho tháº¥y káº¿t quáº£ tá»‘t nháº¥t cá»§a nÄƒm láº§n cháº¡y cho má»—i thiáº¿t láº­p. Má»—i Ä‘Æ°á»ng sá»­ dá»¥ng sá»‘ lÆ°á»£ng heads khÃ¡c nhau, nhÆ°ng sá»‘ lÆ°á»£ng tham sá»‘ má»—i attention layer, rÃ—dÃ—H = d^(c+1), Ä‘Æ°á»£c giá»¯ khÃ´ng Ä‘á»•i trong má»—i Ä‘Æ°á»ng. Quy mÃ´ tiÃªu chuáº©n lÃ  dÂ² tham sá»‘ má»—i layer. Khi L = 1, káº¿t quáº£ gá»£i Ã½ ráº±ng sá»­ dá»¥ng full-rank (r = 64) lÃ  cáº§n thiáº¿t vÃ  Ä‘á»§ Ä‘á»ƒ há»c hÃ m má»¥c tiÃªu chÃ­nh xÃ¡c; ngay cáº£ 2d heads cÃ³ rank d/2 cÅ©ng tháº¥t báº¡i. Vá»›i L > 1, Ä‘Ã¡nh Ä‘á»•i giá»¯a háº¡ng vÃ  Ä‘á»™ chÃ­nh xÃ¡c thuáº­n lá»£i hÆ¡n, nhÆ°ng low-rank attention váº«n kÃ©m hiá»‡u suáº¥t Ä‘Ã¡ng ká»ƒ so vá»›i full-rank attention, ngay cáº£ khi nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u tham sá»‘ hÆ¡n. CÃ¡c transformer tiÃªu chuáº©n nÄƒm lá»›p (nghÄ©a lÃ  L = 5, tham sá»‘ má»—i layer = dÂ²) dÆ°á»ng nhÆ° gáº·p khÃ³ khÄƒn tá»‘i Æ°u hÃ³a trÃªn váº¥n Ä‘á» nÃ y. Loáº¡i trá»« trÆ°á»ng há»£p Ä‘Ã³, mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t nháº¥t khÃ´ng pháº£i full-rank (L = 5, dÂ³ tham sá»‘ má»—i layer, r = 32) hoáº¡t Ä‘á»™ng khÃ´ng tá»‘t hÆ¡n mÃ´ hÃ¬nh full-rank tá»‡ nháº¥t (L = 1, dÂ² tham sá»‘ má»—i layer, r = 64) máº·c dÃ¹ cÃ³ 80x nhiá»u tham sá»‘ hÆ¡n trong cÃ¡c attention layers cá»§a nÃ³. TÃ³m láº¡i, má»™t transformer tiÃªu chuáº©n vá»›i H = 1 hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n nhiá»u so vá»›i má»™t vá»›i H tháº­m chÃ­ vá»«a pháº£i lá»›n hÆ¡n trÃªn tÃ¡c vá»¥ nÃ y.

Full-rank solution: Trong trÆ°á»ng há»£p full-rank, transformer há»c Ä‘Æ°á»£c má»¥c tiÃªu, nhÆ°ng nÃ³ Ä‘Ã£ há»c Ä‘Æ°á»£c biá»ƒu diá»…n nÃ o? HÃ¬nh 2 gá»£i Ã½ ráº±ng, trong má»™t sá»‘ trÆ°á»ng há»£p, nÃ³ ráº¥t gáº§n vá»›i cáº¥u trÃºc cá»§a Fact 1. Nhá»› láº¡i ráº±ng trong Fact 1, chÃºng tÃ´i sá»­ dá»¥ng hardmax attention head vá»›i K_h Q_h^T = I. Trong thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i tuy nhiÃªn, chÃºng tÃ´i sá»­ dá»¥ng hÃ m má»¥c tiÃªu farthest neighbor vÃ  softmax heads, vÃ¬ váº­y cáº¥u trÃºc tÆ°Æ¡ng á»©ng lÃ  K_h Q_h^T = -cI vá»›i c â‰« 1. Panel Ä‘áº§u tiÃªn cho tháº¥y median Frobenius angle giá»¯a cÃ¡c ma tráº­n K_h Q_h^T vÃ  I Ä‘Æ°á»£c há»c bá»Ÿi cÃ¡c mÃ´ hÃ¬nh full-rank, single layer trong thÃ­ nghiá»‡m trÆ°á»›c. Äiá»u nÃ y cho tháº¥y ráº±ng K_h Q_h^T ráº¥t gáº§n báº±ng -I Ä‘áº¿n má»™t há»‡ sá»‘ khÃ´ng Ä‘á»•i. HÆ¡n ná»¯a, nhÆ° panel thá»© hai cho tháº¥y, norm cá»§a ma tráº­n nÃ y lá»›n, khiáº¿n softmax hoáº¡t Ä‘á»™ng nhÆ° hardmax. Káº¿t quáº£ tÆ°Æ¡ng tá»± cho máº¡ng ba lá»›p vá»›i má»™t single full-rank head, nhÆ°ng khi L > 1 vÃ  H > 1, cÃ³ váº» nhÆ° máº¡ng há»c Ä‘Æ°á»£c má»™t chiáº¿n lÆ°á»£c khÃ¡c, Ã­t cÃ³ thá»ƒ diá»…n giáº£i hÆ¡n Ä‘á»ƒ biá»ƒu diá»…n má»¥c tiÃªu.

Positional encodings: VÃ¬ hÃ m má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  permutation-invariant, khÃ´ng cÃ³ thÃ´ng tin positional nÃ o tá»“n táº¡i trong dá»¯ liá»‡u. Tuy nhiÃªn, trong Má»¥c 6, chÃºng tÃ´i Ä‘Ã£ cho tháº¥y ráº±ng concatenated positional encodings cÃ³ thá»ƒ giÃºp low-rank attention thÃ nh cÃ´ng khi L > 1 báº±ng cÃ¡ch cho mÃ´ hÃ¬nh cÃ¡c chiá»u khÃ´ng gian scratch bá»• sung. CÃ¡c sÆ¡ Ä‘á»“ positional encoding Ä‘Æ°á»£c sá»­ dá»¥ng trong thá»±c táº¿, nhÆ° additive encodings [VSP+17], RoPE [SAL+24] vÃ  ALiBi [PSL22], khÃ´ng thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng theo cÃ¡ch nÃ y, lÃ  cÃ¡c phiÃªn báº£n cá»§a generalized attention heads Ä‘Æ°á»£c nghiÃªn cá»©u trong bÃ i bÃ¡o nÃ y. Trong HÃ¬nh 3, chÃºng tÃ´i thÃ­ nghiá»‡m vá»›i positional encodings. NhÆ° mong Ä‘á»£i, additive attention hoÃ n toÃ n tháº¥t báº¡i trong viá»‡c giÃºp low-rank attention. Panel bÃªn trÃ¡i cho tháº¥y ráº±ng khi L = 1, concatenated positional encodings cÅ©ng tháº¥t báº¡i. Tuy nhiÃªn, khi L = 3, concatenated positional encodings mang láº¡i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ, má»™t phÃ¡t hiá»‡n phÃ¹ há»£p vá»›i Theorem 7.

Vai trÃ² cá»§a N: Trong HÃ¬nh 4, chÃºng tÃ´i khÃ¡m phÃ¡ cÃ¡ch sá»‘ lÆ°á»£ng input points N áº£nh hÆ°á»Ÿng Ä‘áº¿n Ä‘á»™ khÃ³ cá»§a viá»‡c há»c hÃ m má»¥c tiÃªu. ChÃºng tÃ´i cá»‘ Ä‘á»‹nh d = 64, H = dÂ²/r, vÃ  sá»‘ lá»›p L = 2. Káº¿t quáº£ cho tháº¥y ráº±ng, nhÆ° Ä‘Æ°á»£c dá»± Ä‘oÃ¡n bá»Ÿi Fact 1, cÃ¡c full-rank heads há»c má»¥c tiÃªu chÃ­nh xÃ¡c qua má»™t pháº¡m vi N. Tuy nhiÃªn, cÃ¡c low-rank heads gáº·p pháº£i Ä‘á»™ chÃ­nh xÃ¡c giáº£m khi N tÄƒng. Äiá»u nÃ y phÃ¹ há»£p vá»›i Conjecture 8, dá»± Ä‘oÃ¡n ráº±ng cÃ¡c transformer low-rank cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh tháº¥t báº¡i trong viá»‡c biá»ƒu diá»…n chÃ­nh xÃ¡c má»¥c tiÃªu cho N Ä‘á»§ lá»›n.

8 Káº¿t luáº­n vÃ  Háº¡n cháº¿

Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i Ä‘Ã£ Ä‘iá»u tra vai trÃ² cá»§a háº¡ng trong cÃ¡c cÆ¡ cháº¿ attention. ChÃºng tÃ´i Ä‘áº·t cÃ¢u há»i vá» thá»±c táº¿ gáº§n nhÆ° universal cá»§a viá»‡c Ä‘Ã¡nh Ä‘á»•i háº¡ng vÃ  sá»‘ lÆ°á»£ng heads theo H = d/r. ChÃºng tÃ´i cho tháº¥y ráº±ng Ä‘á»‘i vá»›i má»™t hÃ m má»¥c tiÃªu Ä‘Æ¡n giáº£n vÃ  tá»± nhiÃªn Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« tÃ¬m kiáº¿m ngá»¯ nghÄ©a, low-rank attention vá» cÆ¡ báº£n yáº¿u hÆ¡n full-rank attention, ngay cáº£ khi H â‰« d/r. ChÃºng tÃ´i chá»©ng minh sá»± phÃ¢n tÃ¡ch nghiÃªm ngáº·t nÃ y giá»¯a cháº¿ Ä‘á»™ low-rank vÃ  high-rank cáº£ vá» máº·t lÃ½ thuyáº¿t, báº±ng cÃ¡ch chá»©ng minh tÃ­nh khÃ³ cá»§a xáº¥p xá»‰ trong thiáº¿t láº­p shallow, vÃ  thá»±c nghiá»‡m, thÃ´ng qua thÃ­ nghiá»‡m vá»›i off-the-shelf transformers. Káº¿t quáº£ cá»§a chÃºng tÃ´i do Ä‘Ã³ gá»£i Ã½ vá» má»™t Ä‘Ã¡nh Ä‘á»•i cÃ³ lá»£i tiá»m nÄƒng giá»¯a sá»‘ lÆ°á»£ng heads vÃ  háº¡ng váº«n chÆ°a Ä‘Æ°á»£c khÃ¡m phÃ¡ rá»™ng rÃ£i trong cÃ¡c á»©ng dá»¥ng.

Äiá»u Ä‘Ã³ nÃ³i lÃªn ráº±ng, phÃ¢n tÃ­ch lÃ½ thuyáº¿t cá»§a chÃºng tÃ´i vá» cÆ¡ báº£n bá»‹ háº¡n cháº¿ Ä‘á»‘i vá»›i viá»‡c nghiÃªn cá»©u shallow transformers, vÃ  káº¿t quáº£ cá»§a Má»¥c 6 minh há»a cÃ¡ch thÃªm Ä‘á»™ sÃ¢u cÃ³ thá»ƒ kháº¯c phá»¥c nhá»¯ng háº¡n cháº¿ cá»§a low-rank self-attention trong má»™t sá»‘ trÆ°á»ng há»£p. Tuy nhiÃªn, chÃºng tÃ´i hy vá»ng ráº±ng káº¿t quáº£ cá»§a chÃºng tÃ´i sáº½ thÃºc Ä‘áº©y cÃ¡c nhÃ  lÃ½ thuyáº¿t vÃ  thá»±c hÃ nh xem xÃ©t cáº©n tháº­n hÆ¡n cÃ¡c thiáº¿t láº­p vÃ  quy mÃ´ cá»§a cÃ¡c siÃªu tham sá»‘ transformer. Äáº·c biá»‡t, chÃºng gá»£i Ã½ ráº±ng cÃ¡c mÃ´ hÃ¬nh lÃ½ thuyáº¿t sá»­ dá»¥ng full-rank attention cÃ³ thá»ƒ khÃ´ng mÃ´ táº£ chÃ­nh xÃ¡c cÃ¡c transformers Ä‘Æ°á»£c sá»­ dá»¥ng trong thá»±c táº¿, vÃ  ráº±ng váº«n cÃ²n nhiá»u Ä‘iá»u cáº§n hiá»ƒu vá» nhá»¯ng thÃ nh cÃ´ng vÃ  failure modes cá»§a cÃ¡c kiáº¿n trÃºc dá»±a trÃªn attention.

Má»™t sá»‘ cÃ¢u há»i má»Ÿ váº«n cÃ²n cho cÃ´ng trÃ¬nh tÆ°Æ¡ng lai. Kiáº¿n trÃºc transformer cÆ¡ báº£n cá»§a [VSP+17] cho phÃ©p ngÆ°á»i dÃ¹ng thiáº¿t láº­p má»™t sá»‘ siÃªu tham sá»‘. Máº·c dÃ¹ tÃ­nh phá»• biáº¿n cá»§a kiáº¿n trÃºc nÃ y, cÃ¡c thiáº¿t láº­p siÃªu tham sá»‘ khÃ¡c ngoÃ i chiá»u embedding vÃ  sá»‘ lá»›p gáº§n nhÆ° khÃ´ng bao giá» Ä‘Æ°á»£c thay Ä‘á»•i Ä‘Ã¡ng ká»ƒ; xem Phá»¥ lá»¥c A. Trong khi cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã³ Ä‘Ã¡ng ká»ƒ Ä‘Ã£ nghiÃªn cá»©u scaling laws cho chiá»u vÃ  sá»‘ lá»›p, chÃºng tÃ´i tin ráº±ng nghiÃªn cá»©u tÆ°Æ¡ng lai cÅ©ng nÃªn xem xÃ©t cÃ¡c siÃªu tham sá»‘ khÃ¡c vÃ  tÃ¬m cÃ¡ch hiá»ƒu nhá»¯ng Ä‘Ã¡nh Ä‘á»•i, dependencies, vÃ  scaling laws giá»¯a chÃºng. á» Ä‘Ã¢y, chÃºng tÃ´i táº­p trung vÃ o háº¡ng query/key vÃ  má»‘i quan há»‡ cá»§a nÃ³ vá»›i sá»‘ lÆ°á»£ng heads, nhÆ°ng Ä‘á»™ sÃ¢u vÃ  Ä‘á»™ rá»™ng cá»§a MLPs vÃ  value/output rank cÅ©ng Ä‘Ã¡ng quan tÃ¢m.

NgoÃ i ra, tÃ­nh báº¥t biáº¿n quay cá»§a phÃ¢n phá»‘i dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  cÃ´ng cá»¥ trong viá»‡c thiáº¿t láº­p cÃ¡c lower bounds cá»§a chÃºng tÃ´i. Cho tÃ­nh cháº¥t rá»i ráº¡c vá»‘n cÃ³ cá»§a transformers dá»±a trÃªn vÄƒn báº£n, má»™t cÃ¢u há»i tá»± nhiÃªn lÃ  hiá»ƒu cÃ¡ch tá»•ng quÃ¡t hÃ³a cÃ¡c ká»¹ thuáº­t cá»§a chÃºng tÃ´i ngoÃ i thiáº¿t láº­p rotationally-invariant. Má»™t hÆ°á»›ng khÃ¡c cho cÃ´ng trÃ¬nh tÆ°Æ¡ng lai lÃ  hiá»ƒu má»‘i quan há»‡ giá»¯a háº¡ng vÃ  Ä‘á»™ dÃ i ngá»¯ cáº£nh. Táº­p trung vÃ o trÆ°á»ng há»£p N = 2 Ä‘á»§ Ä‘á»ƒ chÃºng tÃ´i chá»©ng minh phÃ¢n tÃ¡ch háº¡ng, nhÆ°ng chÃºng tÃ´i tin ráº±ng káº¿t quáº£ tÆ°Æ¡ng tá»± nÃªn Ä‘Ãºng Ã­t nháº¥t vá»›i táº¥t cáº£ N â‰¤ d; HÃ¬nh 4 cung cáº¥p báº±ng chá»©ng thá»±c nghiá»‡m sÆ¡ bá»™. Hiá»ƒu trÆ°á»ng há»£p N > 2 cÅ©ng cÃ³ thá»ƒ giÃºp giáº£i quyáº¿t má»™t cÃ¢u há»i má»Ÿ cuá»‘i cÃ¹ng: Má»‘i quan há»‡ giá»¯a háº¡ng vÃ  Ä‘á»™ sÃ¢u lÃ  gÃ¬? Äáº·c biá»‡t, liá»‡u Conjecture 8 cÃ³ Ä‘Ãºng khÃ´ng?

Acknowledgements: CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c há»— trá»£ má»™t pháº§n bá»Ÿi Alfred P. Sloan Foundation, vÃ  cÃ¡c giáº£i thÆ°á»Ÿng NSF RI-1816753, NSF CAREER CIF 1845360, NSF CHS-1901091 vÃ  NSF DMS-MoDL 2134216. ChÃºng tÃ´i cáº£m Æ¡n Ohad Shamir vÃ¬ nhá»¯ng tháº£o luáº­n há»¯u Ã­ch trong quÃ¡ trÃ¬nh hoÃ n thÃ nh cÃ´ng trÃ¬nh nÃ y.

[Tiáº¿p tá»¥c vá»›i pháº§n References vÃ  Appendix...]
