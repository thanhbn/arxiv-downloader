Về Lợi ích của Hạng trong các Tầng Attention

Noah Amsel1, Gilad Yehudai2, và Joan Bruna1,2,3
1Viện Khoa học Toán học Courant, Đại học New York
2Trung tâm Khoa học Dữ liệu, Đại học New York
3Viện Flatiron
24 tháng 7, 2024

Tóm tắt

Các cơ chế dựa trên attention được sử dụng rộng rãi trong machine learning, nổi bật nhất là trong transformers. Tuy nhiên, các siêu tham số như hạng của ma trận attention và số lượng head được điều chỉnh theo cách gần như giống nhau trong tất cả các hiện thực của kiến trúc này, mà không có sự biện minh lý thuyết. Trong công trình này, chúng tôi cho thấy rằng có những đánh đổi đáng kể giữa hạng và số lượng head của cơ chế attention. Cụ thể, chúng tôi trình bày một hàm mục tiêu đơn giản và tự nhiên có thể được biểu diễn bằng một single full-rank attention head cho bất kỳ độ dài ngữ cảnh nào, nhưng không thể được xấp xỉ bởi low-rank attention trừ khi số lượng head là hàm mũ theo chiều embedding, ngay cả với độ dài ngữ cảnh ngắn. Hơn nữa, chúng tôi chứng minh rằng, với độ dài ngữ cảnh ngắn, việc thêm độ sâu cho phép mục tiêu được xấp xỉ bởi low-rank attention. Với ngữ cảnh dài, chúng tôi đưa ra giả thuyết rằng full-rank attention là cần thiết. Cuối cùng, chúng tôi trình bày các thí nghiệm với transformers có sẵn để xác thực các phát hiện lý thuyết của chúng tôi.

Mục lục
1 Giới thiệu 2
2 Công trình liên quan 3
3 Thiết lập và Ký hiệu 4
4 Phân tách Low-Rank cho Nearest Neighbors 6
5 Phân tách Hàm mũ cho Biased Nearest Neighbors 7
6 Xấp xỉ Hiệu quả Sử dụng Độ sâu 9
7 Thí nghiệm 11
8 Kết luận và Hạn chế 15
A Siêu tham số của Transformer 21
B Chứng minh từ Mục 4 22
C Chứng minh từ Mục 5 43
D Chứng minh từ Mục 6 và một Cấu trúc Bổ sung 50

1 Giới thiệu

Các kiến trúc dựa trên attention có mặt khắp nơi trong machine learning đương đại. Những ví dụ nổi bật nhất là transformers, được xây dựng bằng cách xếp chồng nhiều lớp attention với MLPs, residual connections, và các lớp normalization để biểu diễn các hàm trên dãy hoặc tập hợp. Khung cơ bản này cho phép người dùng tự do thiết lập một số siêu tham số, mặc dù ít trong số này được nghiên cứu cẩn thận. Thực tế, trong hàng nghìn bài báo sử dụng kiến trúc này, nhiều siêu tham số được giữ nguyên hoặc gần như giống với bài báo gốc [VSP+17] (xem Phụ lục A để so sánh). Trong bài báo này, chúng tôi nghiên cứu tầm quan trọng của hạng của cơ chế attention.

Một lớp attention là một ánh xạ giữa các dãy vector trong ℝ^d. Kích thước của một lớp attention được xác định bởi số lượng head (H) và hạng của ma trận weight query và key (r), sao cho tổng số tham số có bậc d×H×r. Đáng chú ý, gần như mọi kiến trúc transformer đều thiết lập số lượng head là H = d/r, và những ngoại lệ mà chúng tôi biết chỉ khác nhau không quá hệ số 2 (xem Phụ lục A). Thực tế, quy mô này rất tiêu chuẩn đến mức nó được hard-code vào các thư viện như PyTorch [PGM+19] và xFormers [LML+22], một thực tế có lẽ đã ngăn cản việc thử nghiệm với các quy mô khác. Động lực ban đầu cho quy mô này là để khớp với số lượng tham số của một single full rank head, tức là trường hợp H = 1, r = d. Chúng tôi không biết lý do a priori hoặc bằng chứng thực nghiệm nào ủng hộ quy mô này hơn bất kỳ quy mô nào khác, vì những đánh đổi giữa hạng và số lượng head vẫn chưa được hiểu rõ. Ví dụ, hầu hết transformers trong văn học sử dụng hạng nhỏ từ 64 đến 128, mặc dù chiều embedding d thay đổi đáng kể (ví dụ d = 512 trong bài báo transformers gốc [VSP+17] và d = 8192 trong LLaMA [TLI+23]). Không rõ liệu sức mạnh biểu đạt của transformers có bị suy yếu bởi việc duy trì hạng cố định khi chiều được tăng lên.

Một dòng công trình dài trong lý thuyết deep learning đã nghiên cứu tầm quan trọng tương đối của độ rộng và độ sâu trong việc xác định sức mạnh biểu đạt của mạng nơ-ron feedforward, như một bước cần thiết đầu tiên để hiểu những đánh đổi thực tế (cũng bao gồm các khía cạnh tối ưu hóa). Bài báo này tương tự ở chỗ chúng tôi nghiên cứu những đánh đổi tham số trong transformers thông qua lăng kính sức mạnh biểu đạt, mặc dù transformers có nhiều siêu tham số hơn chỉ độ rộng và độ sâu (xem Phụ lục A). Đối với mạng feedforward, độ sâu 2 đủ cho xấp xỉ universal [Cyb89], nhưng độ sâu lớn hơn có thể cần thiết cho xấp xỉ hiệu quả. Nghĩa là, một số hàm có thể được biểu diễn hiệu quả bởi mạng ba lớp nhưng không thể được biểu diễn bởi mạng hai lớp trừ khi nó có độ rộng hàm mũ theo chiều đầu vào [ES16, Dan17, SS17]. Tự nhiên là đặt câu hỏi tương tự về kiến trúc attention. Chúng ta nên thiết lập các siêu tham số như thế nào để làm cho transformers của chúng ta hiệu quả? Đặc biệt, liệu low-rank attention có về cơ bản yếu hơn high-rank attention không, hay sức mạnh biểu đạt chỉ được điều khiển bởi tích tham số H×r, hoạt động như tương tự độ rộng của một lớp MLP?

Trong bài báo này, chúng tôi nghiên cứu chính xác những đánh đổi tinh vi này trong khả năng biểu đạt của các lớp attention. Chúng tôi trình bày một hàm mục tiêu đơn giản phát sinh tự nhiên trong tìm kiếm ngữ nghĩa có thể được xấp xỉ đến bất kỳ độ chính xác nào bởi một single full rank attention head bất kể độ dài ngữ cảnh. Mặt khác, việc xấp xỉ mục tiêu này với low-rank attention yêu cầu số lượng head phải là super-polynomial theo chiều đầu vào, ngay cả với độ dài ngữ cảnh ngắn. Cụ thể, sử dụng full-rank heads, tổng số tham số cần thiết là d×H×r ≃ d², trong khi nó trở thành ≃ d^(1+ε^(-1)) nếu sử dụng low-rank heads thay thế, để đạt độ chính xác tương đối ε. Tăng độ sâu cho phép xấp xỉ tốt hơn chỉ sử dụng số lượng head đa thức, ít nhất với độ dài ngữ cảnh ngắn. Chúng tôi bổ sung những kết quả lý thuyết này bằng thí nghiệm trên các kiến trúc transformer có sẵn. Kết quả của chúng tôi chứng minh một đánh đổi rất rõ ràng giữa hạng và số lượng head trong cơ chế attention và làm sáng tỏ quy mô tiêu chuẩn H = d/r được sử dụng trong transformers.

1.1 Đóng góp của Chúng tôi

• Trong Mục 4, chúng tôi chứng minh phân tách hạng để biểu diễn hàm nearest neighbor sử dụng multi-head attention. Hàm này có thể được xấp xỉ đến bất kỳ độ chính xác nào chỉ sử dụng một single full-rank head. Tuy nhiên trong chế độ chiều cao, ít nhất Ω(d/r)^(1/ε) head hạng r được yêu cầu để đạt sai số bình phương tương đối ε. Hơn nữa, trong chế độ độ chính xác cao (ε tiến tới 0 với d cố định), số lượng head cần thiết là hàm mũ: Ω(exp(d - r log(d/r))).

• Trong Mục 5, chúng tôi sử dụng các kỹ thuật khác nhau để thiết lập phân tách hàm mũ trong chế độ độ chính xác cao cho hàm biased nearest neighbor. Hàm mục tiêu này có thể được xấp xỉ đến bất kỳ độ chính xác nào sử dụng single full-rank head với việc thêm bias, nhưng Ω(exp(d - r)) rank-r heads được yêu cầu để xấp xỉ nó với độ chính xác tốt hơn O(1/d⁴) sai số bình phương tương đối.

• Trong Mục 6, chúng tôi khám phá các cách để khắc phục điểm yếu của low-rank attention. Chúng tôi cho thấy rằng việc tăng cường kiến trúc attention và thêm lớp thứ hai, phi tuyến có thể đạt được điều này sử dụng số lượng head đa thức, nhưng không giống như full-rank attention, những cấu trúc như vậy có thể không mở rộng được với độ dài dãy dài.

• Trong Mục 7, chúng tôi hỗ trợ kết quả lý thuyết của chúng tôi bằng thí nghiệm trên kiến trúc transformer tiêu chuẩn với nhiều lớp attention và MLPs. Chúng tôi cho thấy rằng các mô hình full rank dễ dàng học mục tiêu đến độ chính xác cao — thậm chí khôi phục cấu trúc chính của chúng tôi — nhưng các mô hình low rank gặp khó khăn để làm như vậy. Người dùng transformers tiêu chuẩn có thể không nghĩ rằng thiết lập H = 2 có thể tệ hơn nhiều so với H = 1, nhưng trong trường hợp này, đúng vậy.

2 Công trình liên quan

Lý thuyết về transformers: Một dòng công trình ngày càng phát triển đã tìm cách cung cấp phân tích lý thuyết về transformers và cơ chế attention. Dynamics huấn luyện, inductive biases, generalization, và in-context learning đều nhận được sự chú ý đáng kể. Tuy nhiên, các bài báo trong những lĩnh vực này gần như luôn giả định rằng full-rank attention được sử dụng [BCB+23, CDB24, FGBM23, SHT24a, EGKZ22, BCW+23, ZFB24, JBKM24, CSWY24, DGTT23, TWCD23], mặc dù nhiều bài cũng giả định có nhiều head. Công trình của chúng tôi cung cấp bối cảnh quan trọng cho những kết quả này, cho thấy rằng các mô hình full-rank có thể không phải là proxy tốt cho các transformers low-rank được sử dụng trong thực tế.

Sức mạnh biểu đạt của transformers: Công trình của chúng tôi thuộc về một nhóm nghiên cứu về khả năng biểu diễn của transformers. Không giống như các chủ đề khác trong lý thuyết transformer, kết quả trong lĩnh vực này thường áp dụng cho low-rank attention. [YBR+19] chứng minh rằng transformers (sâu theo hàm mũ) là universal approximators ngay cả với hạng một. [WCM22, MS23] cho thấy rằng transformers có thể mô phỏng máy Turing nếu kích thước của chúng được phép tăng theo độ dài dãy. [KKM22, KS23] cho thấy rằng transformers có khả năng ghi nhớ dữ liệu. [BHBK24] cho thấy rằng transformers có thể hiệu quả triển khai một phiên bản của thuật toán nearest neighbor cho phân loại in-context của các điểm trên sphere, nhưng cấu trúc của họ sử dụng attention là full-rank với respect đến chiều đầu vào. Công thức của chúng tôi về tác vụ nearest neighbor hơi khác và có thể được giải quyết với full-rank attention gần như trivial (xem Fact 1). Cuối cùng, một dòng công trình quan trọng phân tích khả năng biểu diễn của transformers sử dụng các lớp ngôn ngữ hình thức, finite automata, và circuits [Hah20, LAG+22, HAF22, MSS22, SMW+24], nhưng nó không nắm bắt được sự phân tách trong khả năng do hạng.

Hạn chế của low-rank attention: Một số nghiên cứu khác đã điều tra vai trò của hạng của cơ chế attention. [BYR+20] trình bày các thí nghiệm thách thức quy mô canonical H = d/r. Họ lập luận rằng việc cố định d và r dựa trên độ dài ngữ cảnh N và thiết lập H độc lập dẫn đến các mô hình mạnh mẽ và hiệu quả hơn. Họ cũng chứng minh rằng một full-rank attention head có thể tạo ra bất kỳ attention pattern nào từ bất kỳ đầu vào nào (cho một số thiết lập của weights), nhưng một low-rank attention head không thể; tuy nhiên, [LCW23] cho thấy rằng ngay cả hạng r = log(N) cũng đủ để biểu diễn bất kỳ sparse attention pattern nào. [MLT24] hỏi có bao nhiêu cặp input-output mà một low-rank multi-head attention layer có thể ghi nhớ chính xác. Đối với vấn đề của họ, không có giá trị gì khi thiết lập r > N; hơn nữa khả năng ghi nhớ phụ thuộc vào r×H hơn là vào r hoặc H, hỗ trợ quy mô tiêu chuẩn. Chúng tôi nghiên cứu thiết lập thực tế và có động lực thực tế hơn về việc xấp xỉ một hàm tự nhiên trên dữ liệu được rút ra từ một phân phối tự nhiên. Không giống như [LCW23, MLT24], chúng tôi cho thấy rằng hạng cao đôi khi cần thiết, bất kể H.

Bài báo gần nhất với của chúng tôi là [SHT24b], chứng minh hai phân tách liên quan đến hạng. Đầu tiên, họ trình bày một hàm có thể được xấp xỉ tốt bởi một single attention head nếu và chỉ nếu hạng của nó đủ lớn. Kết quả này đặt ra câu hỏi sau: liệu việc sử dụng nhiều head có thể bù đắp cho điểm yếu của low-rank attention không? Chúng tôi trả lời câu hỏi này một cách phủ định. Thứ hai, họ trình bày một hàm một chiều trên N đầu vào mà không thể biểu diễn chính xác trừ khi r×H×p > N, trong đó p là số bit độ chính xác. Chúng tôi mở rộng kết quả này ở chỗ các lower bounds của chúng tôi áp dụng (1) ngay cả với N = 2, (2) cho độ chính xác vô hạn hoặc hữu hạn (3) cho xấp xỉ hàm trên một phân phối tự nhiên, không chỉ biểu diễn chính xác. Ngoài ra, hàm mục tiêu của chúng tôi tạo ra sự phân tách mạnh hơn: trong khi H ≥ Ω(1/r) đủ trong thiết lập của họ, thiết lập của chúng tôi yêu cầu H tăng đa thức hoặc thậm chí hàm mũ theo d/r để khắc phục điểm yếu của low-rank attention. Tuy nhiên, các hàm mục tiêu của họ gần gũi hơn với các loại tác vụ reasoning có cấu trúc mà transformers thường được áp dụng. Đặc biệt, họ nhấn mạnh cách attention tự nhiên phù hợp để nắm bắt tương tác theo cặp; các kiến trúc recurrent gặp khó khăn để làm điều này hiệu quả, trong khi transformers gặp khó khăn để nắm bắt tương tác bậc ba.

Low rank compression và fine-tuning: Nhiều công trình gần đây trong nén mô hình [LZL+23, HRP+21, BNG20] và fine-tuning [HysW+22] dựa trên quan sát thực nghiệm rằng các ma trận weight của transformers pretrained (như của các mạng nơ-ron khác) có thể được thay thế hoặc fine-tuned bởi các proxy chiều thấp hơn mà không hy sinh hiệu suất, và trong một số trường hợp thậm chí giúp ích [SAM24]. Những kết quả như vậy cung cấp bối cảnh cho công trình của chúng tôi bằng cách cho thấy rằng full-rank không phải lúc nào cũng tốt hơn low-rank.

Đánh đổi depth-width trong mạng nơ-ron: Nhiều công trình trước đây nghiên cứu sự phân tách giữa các mạng nơ-ron có độ sâu khác nhau, và giữa mạng nơ-ron và phương pháp kernel. [ES16, Dan17, SS17, VJOB22] xây dựng các hàm có thể được xấp xỉ hiệu quả với mạng nơ-ron 3 lớp, nhưng mạng 2 lớp yêu cầu độ rộng là hàm mũ theo chiều đầu vào. [Tel16, CNPW19] cho thấy phân tách độ sâu cho mạng với chiều đầu vào không đổi và độ sâu thay đổi. Các lower bounds của chúng tôi cũng liên quan mật thiết về mặt kỹ thuật với kết quả phân tách giữa mạng nơ-ron và phương pháp kernel. [YS19] chứng minh rằng random features (hoặc bất kỳ phương pháp kernel nào khác) không thể học được ngay cả một nơ-ron đơn trừ khi số lượng features hoặc độ lớn của weights là hàm mũ theo chiều đầu vào. [KMS20] cải thiện kết quả của họ bằng cách loại bỏ sự phụ thuộc vào độ lớn của weights. [GMMM21, MM23] nghiên cứu upper và lower bounds trong việc xấp xỉ đa thức với phương pháp kernel. Họ cho thấy rằng về cơ bản, cần thiết và đủ để số lượng features là hàm mũ theo bậc của đa thức được xấp xỉ. Các lower bounds của chúng tôi được lấy cảm hứng từ công trình này.

3 Thiết lập và Ký hiệu

Các lớp attention: Một rank-r attention head được tham số hóa bởi các ma trận weight Q, K, V, O ∈ ℝ^(d×r). (Một số tác giả gọi những ma trận này là W^Q, W^K, W^V, và W^O.) Một multi-head attention layer đơn giản là tổng của H attention heads như vậy. Đầu vào của một multi-head attention layer là một dãy vector x₁,...x_N ∈ ℝ^d gọi là target points và một dãy y₁...y_M gọi là source points. (Lưu ý rằng tên "target points" không liên quan đến "target function" mà chúng ta muốn xấp xỉ.) Nếu các cột của X ∈ ℝ^(N×d) và Y ∈ ℝ^(M×d) lần lượt là target và source points, thì một softmax multi-head attention layer là một hàm có dạng

H∑
h=1 O_h V_h^T X sm(X^T K_h Q_h^T Y) ∈ ℝ^(M×d), (1)

trong đó sm(·) tính softmax của mỗi cột đầu vào của nó; nghĩa là, với mỗi y, nó xuất ra một phân phối xác suất trên [N] dựa trên các điểm X^T K_h Q_h^T y ∈ ℝ^N. Một hardmax attention layer giống nhau, ngoại trừ hàm hardmax hm(·) xuất ra e_{i*}, trong đó i* là chỉ số của điểm tối đa. Lưu ý rằng hardmax heads thường được coi là trường hợp đặc biệt của softmax heads, vì lim_{c→∞} sm(X^T c K_h Q_h^T Y) = hm(X^T K_h Q_h^T Y) trong hội tụ pointwise.

Ở trên, chúng tôi đã mô tả cái gọi là cross-attention, lấy cả source points và target points làm đầu vào. Các lớp self-attention quen thuộc là trường hợp đặc biệt trong đó source points và target points giống nhau: X = Y. Một hàm multi-head attention đã cho có thể được áp dụng cho bất kỳ số lượng source hoặc target points nào, vì không có phần nào của định nghĩa này phụ thuộc vào N hoặc M. Ngoài ra, nó bất biến với các hoán vị của target points và đẳng biến với các hoán vị của source points.

Generalized attention: Chúng tôi chứng minh các lower bounds của chúng tôi đối với một lớp hàm tổng quát hóa multi-head attention. Thay vì tính phân phối attention như sm(X^T K_h Q_h Y), chúng tôi cho phép bất kỳ hàm nào phụ thuộc vào y và một phép chiếu rank-r của X mà xuất ra một phân phối xác suất trên [N]. Ngoài ra, chúng tôi thay thế O_h V_h bằng một ma trận đơn V_h ∈ ℝ^(d×d). Do đó, mô hình của chúng tôi là

H∑
h=1 V_h X φ_h(K_h^T X, Y), (2)

trong đó K_h ∈ ℝ^(d×r), hàm φ_h: ℝ^(r×N) × ℝ^d → Δ_N được áp dụng column-wise cho Y và Δ_N là simplex. Lưu ý rằng hàm φ_h có thể thay đổi giữa các head. Hơn nữa, chúng tôi cho phép V_h ∈ ℝ^(d×d) là full-rank. Lưu ý rằng lớp này nắm bắt, ngoài các kiến trúc transformer tiêu chuẩn, việc sử dụng biases, additive positional encodings, và các sơ đồ encoding khác như RoPE [SAL+24] và ALiBi [PSL22] trong lớp attention. Chúng tôi cũng nắm bắt các kiến trúc từ các công trình sớm về attention [BCB14, XBK+15], sử dụng mạng feedforward để tính điểm attention thay vì điểm attention "multiplicative" hoặc "dot product" X^T KQY được sử dụng trong transformers.

Hàm nearest neighbor: Đầu vào của hàm nearest neighbor bao gồm một dãy N target points x₁,...,x_N ∈ S^(d-1) (cũng được ký hiệu bởi X ∈ ℝ^(d×N)) và một source point y ∈ S^(d-1).

Hàm nearest neighbor xuất ra target point gần nhất với source:

f(x₁,...,x_N;y) := arg min_{x∈{x₁,...x_N}} ||x - y||₂. (3)

Hàm này tương tự với việc thực hiện tìm kiếm ngữ nghĩa, trong đó mục tiêu là truy xuất entry hoặc từ trong cơ sở dữ liệu hoặc cửa sổ ngữ cảnh khớp gần nhất với một truy vấn. Hàm này có tính đối xứng cao. Giống như chính multi-head attention, nó được định nghĩa cho bất kỳ N nào và bất biến với các hoán vị của target points. Nó cũng bất biến với các phép biến đổi trực giao đồng thời của X và y, vì vậy nó không có hướng chính, không gian con, hoặc quy mô.

Phân phối dữ liệu: Chúng tôi rút target và source points đều từ sphere. Đối với các lower bounds của chúng tôi, thuận tiện để giả định rằng target points là trực giao. Với N ≤ d, gọi D_N(S^(d-1)) là phân phối đều trên tập hợp các dãy x₁,...x_N ∈ S^(d-1) mà i ≠ j ⟹ x_i ⊥ x_j. Những mẫu như vậy có thể được tạo ra bằng cách lấy N cột đầu tiên của một ma trận trực chuẩn ngẫu nhiên. Lưu ý rằng điều này tương tự về bản chất với việc rút các điểm dữ liệu độc lập từ unit sphere, vì các vector ngẫu nhiên đẳng hướng trong chiều cao gần như trực giao. Phân phối này bất biến với các phép biến đổi trực giao của X và của y.

4 Phân tách Low-Rank cho Nearest Neighbors

Trong mục này, chúng tôi nghiên cứu khả năng của multi-head attention để biểu diễn hàm nearest-neighbor. Chúng tôi cho thấy một sự phân tách trong sức mạnh biểu diễn dựa trên hạng. Mục tiêu có thể được biểu diễn hiệu quả sử dụng full-rank attention, nhưng dưới các giả định dưới đây, việc xấp xỉ nó sử dụng low-rank attention yêu cầu một mô hình lớn hơn nhiều. Chúng tôi bắt đầu với upper bound sử dụng một single full-rank attention head:

Fact 1 (Full-rank Efficient Approximation, Equivariant Case). Đối với hàm mục tiêu từ Phương trình (3), bất kỳ ε > 0, N, d ∈ ℕ tồn tại K, Q, V ∈ ℝ^(d×d) sao cho:

𝔼_{y,x₁,...,x_N∼Unif(S^(d-1))} [||f(X,y) - VX sm(X^T KQ^T y)||²] ≤ ε. (4)

Cấu trúc rất đơn giản. Xem xét trường hợp hardmax để đơn giản. Đặt V = K Q^T = I sao cho ||x_i - y||² = 2 - x_i^T KQ^T y. Khi đó hm(X^T KQ^T y) = e_{i*} trong đó i* = arg min_{i∈[N]} ||x_i - y||² và e_i là vector cơ sở thứ i. Lưu ý rằng cấu trúc này sử dụng hardmax hoạt động cho bất kỳ phân phối đầu vào nào trên S^(d-1) và bất kỳ số lượng điểm N nào, vì nó biểu diễn hàm mục tiêu chính xác. Trường hợp softmax tương tự; đối với phát biểu chính thức xem phụ lục Phụ lục B.1. Cấu trúc này (hoặc một cái rất tương tự) dễ dàng được học bởi gradient descent; xem Hình 2.

Bây giờ chúng tôi chuyển sang lower bound. Chúng tôi cho thấy rằng việc xấp xỉ hàm mục tiêu với rank-r heads yêu cầu số lượng head lớn trừ khi r ∼ d. Để thuận tiện về mặt kỹ thuật, chúng tôi đặt số lượng target points là hai và rút chúng từ phân phối D₂(S^(d-1)) trong đó chúng luôn trực giao. Kết quả chính của chúng tôi thiết lập một sự phân tách định lượng mạnh giữa full-rank và low-rank self-attention layer, ngay cả khi tổng số tham số có cùng bậc:

Theorem 2 (Low-Rank Approximation Lower Bounds, Equivariant Case). Tồn tại các hằng số universal c, c', C và C' sao cho nếu một trong những tập giả định sau đúng:

(i) High-accuracy regime: r ≤ d - 3, ε ≤ c/(d+1), và
H ≤ C · 2^(d-(r+1)log₂(2d/r)). (5)

(ii) High-dimensional regime: d ≥ 5, ε ≥ c'/(d-2)e² · r và
H ≤ (1/2)(1/(2e)) · (d/r)^(C'/ε^(C'/ε)). (6)

Thì, đối với bất kỳ lựa chọn nào của H rank-r generalized attention heads φₕ: ℝʳ × 2 → Δ₁, Vₕ ∈ ℝ^(d×d), Kₕ ∈ ℝ^(d×r) lỗi xấp xỉ hàm nearest neighbor được giới hạn như sau

𝔼_{x₁,x₂∼D₂(S^(d-1)),y∼Unif(S^(d-1))} [||f(X;y) - ∑ₕ₌₁ᴴ VₕX φₕ(Kₕ^T X,y)||²₂] ≥ ε, (7)

trong đó f được định nghĩa như trong Phương trình (3).

Để chứng minh Theorem 2, xem Phụ lục B. Trực quan, bài toán xấp xỉ trở nên khó hơn khi d → ∞ và khi ε → 0. Theorem 2 kết hợp các đảm bảo trong hai chế độ khác nhau. Trong chế độ đầu tiên, độ chính xác mong muốn ε nhỏ. Trong trường hợp này, số lượng head cần thiết tăng theo hàm mũ với d - r. Trong chế độ thứ hai, chiều d lớn. Trong trường hợp này, số lượng head cần thiết tăng đa thức với d/r. Một cách không chính thức, cả hai chế độ đều cho thấy rằng lỗi ít nhất là ε bất cứ khi nào H ≲ (d/r)^(1/ε).

Chúng tôi nhấn mạnh rằng phân phối dữ liệu là 1/√d-close với uniform product measure trong khoảng cách Wasserstein, và chúng tôi mong đợi các kỹ thuật chứng minh chính của chúng tôi sẽ tổng quát hóa cho measure đều này, cũng như các phân phối bất biến quay khác. Ngoài ra, trong khi N = 2 đủ cho mục đích của chúng tôi để thiết lập sự phân tách, chúng tôi cũng tin rằng framework nên mở rộng cho thiết lập tổng quát của N > 2, mặc dù điều này nằm ngoài phạm vi hiện tại.

Chứng minh của chúng tôi sử dụng các công cụ từ harmonic analysis trên sphere. Nó gợi nhớ đến công trình phân tách độ sâu ban đầu của Eldan và Shamir và Daniely [ES16, Dan17], cũng khai thác sự bất lực của ridge functions để xấp xỉ các mục tiêu đối xứng xuyên tâm với năng lượng tần số cao đáng kể. Do tính đối xứng quay của hàm mục tiêu, hàm attention, và phân phối dữ liệu, chúng tôi có thể biến đổi bài toán của chúng tôi để phụ thuộc vào một cặp điểm x = x₁ - x₂ và y được rút đều từ sphere, thay vì x₁, x₂ và y. Mục tiêu của chúng tôi về cơ bản được cho bởi một step function có dạng (x,y) ↦ sgn(x^T y), có phổ phân rã chậm với respect đến cơ sở thích hợp. Chúng tôi xây dựng cơ sở này sử dụng spherical harmonics, và giống như chúng, các hàm cơ sở của chúng tôi được tổ chức thành các không gian con trực giao dựa trên đa thức bậc ℓ. Do tính đối xứng quay, năng lượng của hàm mục tiêu được trải đều trong mỗi harmonic subspace. Ngược lại, mỗi attention head gắn liền với một vài hướng chính được cho bởi span của Kₕ. Kết quả là, mỗi head chỉ được span bởi một phần của các hàm cơ sở trong mỗi subspace. Do đó, với số lượng head hạn chế, không thể nắm bắt một phần đáng kể năng lượng của hàm mục tiêu.

Bây giờ chúng tôi nhận xét về tính chặt chẽ của lower bound này, tập trung vào thiết lập canonical của r = 1. Trong trường hợp này, lower bound của chúng tôi đơn giản hóa và tăng cường một chút. Với ε cố định và d lớn, lỗi xấp xỉ ít nhất là ε bất cứ khi nào H = O(d^(1/(4ε))). Chúng tôi có thể xây dựng một upper bound cho bài toán của chúng tôi bằng cách xem xét rank-1 heads là random features. Trong Phụ lục B.8, chúng tôi lập luận rằng chúng ta có thể xấp xỉ hàm mục tiêu của chúng tôi trong RKHS liên quan đến feature map (x₁ - x₂, y) ↦ sgn((x₁ - x₂)^T kq^T y), trong đó k và q được rút đều từ unit sphere. Kernel integral operator liên quan diagonalizes trong cùng cơ sở của tensorized spherical harmonics được sử dụng để phân tách hàm mục tiêu ở trên, và do đó xấp xỉ kernel ridge regression có thể được phân tích rõ ràng bằng cách giới hạn spectral decay của kernel. Sau đó, thông qua các lập luận tiêu chuẩn từ random feature expansions [Bac17b], người ta có thể chuyển các đảm bảo xấp xỉ từ RKHS sang mô hình random feature, với điều kiện H = e^(Ω(d²/ε²)). Do đó, với r = 1 và ε cố định, lower bound xấp xỉ của Theorem 2 nắm bắt hành vi định tính đúng, mặc dù sự phụ thuộc chính xác của nó vào d có thể không chặt chẽ.

5 Phân tách Hàm mũ cho Biased Nearest Neighbors

Trong mục này, chúng tôi cho thấy một cách khác để có được sự phân tách hàm mũ trong chế độ độ chính xác cao sử dụng các kỹ thuật khác nhau và một hàm mục tiêu đã sửa đổi. Cho b = [b₁,...b_N]^T, hàm biased nearest neighbor được định nghĩa như sau:

f_b(x₁,...,x_N;y) = arg min_{x_i∈{x₁,...,x_N}} [||x_i - y||²₂ + b_i]. (8)

Giống như hàm nearest neighbor không bias của Phương trình (3), nó bất biến với các phép biến đổi trực giao đồng thời của X và y; tuy nhiên, nó không bất biến với các hoán vị của target point X. Chúng tôi đầu tiên cho thấy rằng một single full-rank attention head có thể xấp xỉ mục tiêu này chính xác, với điều kiện biases được thêm vào kiến trúc:

Fact 3 (Full Rank Efficient Approximation, Biased Case). Đối với bất kỳ chiều d, số điểm N, và bias b ∈ ℝ^N, một single biased full-rank hardmax attention head có thể biểu diễn chính xác hàm biased nearest neighbor được định nghĩa trong Phương trình (8).

Cấu trúc giống như của Fact 1 với việc thêm biases b bên trong hardmax. Nghĩa là, head triển khai X hm(X^T y + b) trong trường hợp hardmax. Trong Phụ lục C chúng tôi chứng minh trường hợp softmax. Lưu ý rằng kiến trúc này là trường hợp đặc biệt của attention tiêu chuẩn với concatenated positional encodings. Gọi positional encoding cho x_i là scalar b_i, gọi positional encoding cho y_i là 1, và gọi KQ^T = [I_{d×d} · 1]. Khi đó [X^T b] [KQ^T] [y 1]^T = X^T y + b.

Bây giờ chúng tôi trình bày kết quả chính của mục này cho thấy rằng ngay cả với N = 2, tồn tại một hàm biased nearest neighbor khó xấp xỉ sử dụng low rank attention heads:

Theorem 4 (Low-rank Approximation Lower Bounds, biased case). Tồn tại b = [b₁,b₂]^T ∈ ℝ² sao cho đối với hàm f_b được định nghĩa trong Phương trình (8) điều sau đúng: Đối với bất kỳ lựa chọn nào của rank-r heads g₁,...,g_H trong đó gₕ = VₕX φₕ(KₕX,y), Kₕ có rank-r và φₕ là các hàm tùy ý xuất ra một vector trong simplex Δ₁, nếu H · max_h ||Vₕ|| ≤ exp(c₁(d-r))/(d²c₂) thì:

𝔼_{x₁,x₂∼D₂(√d S^{d-1}),y∼N(0,I)} [||f_b(x₁,x₂,y) - ∑_{h=1}^H gₕ(x₁,x₂,y)||²₂] > 1/20, (9)

cho một số hằng số universal c₁,c₂ > 0.

Chứng minh đầy đủ được đưa vào Phụ lục C. Định lý phát biểu rằng trừ khi số lượng attention heads hoặc độ lớn của output weights (hoặc cả hai) là hàm mũ theo d - r, thì rank-r attention heads không thể xấp xỉ mục tiêu, ngay cả đến độ chính xác không đổi. Điều này trái ngược với thực tế rằng một single full-rank head (với positional encoding) có thể xấp xỉ mục tiêu đến bất kỳ độ chính xác cho trước nào. Lưu ý rằng sự phân tách hàm mũ rất mạnh về mặt hạng của attention heads. Cụ thể, có hạng O(d) không đủ để phá vỡ sự phân tách này, ví dụ ngay cả nếu r = 99/100 · d vẫn có sự phân tách hàm mũ giữa full rank và rank-r attentions heads cho chiều đầu vào d đủ lớn.

Remark 5 (Bound on the weights). Lưu ý rằng trái ngược với Theorem 2, ở đây chúng ta có upper bound hàm mũ trên weights của tổ hợp tuyến tính Vₕ, cụ thể là số lượng heads hoặc norm của weights cần phải là hàm mũ để phá vỡ sự phân tách. Bound này cũng được tìm thấy trong [YS19] đã truyền cảm hứng cho chứng minh của chúng tôi. Trong [KMS20] các tác giả đã có thể loại bỏ bound này bằng cách áp dụng phân tích phức tạp hơn sử dụng các lập luận SQ-dimension, tuy nhiên trong trường hợp của chúng tôi không rõ làm thế nào để mở rộng kỹ thuật của họ vì sự phụ thuộc vào r. Chúng tôi đưa ra giả thuyết rằng vẫn có thể loại bỏ bound này, và để lại cho công trình tương lai.

Trực quan chứng minh. Trọng tâm của chứng minh Theorem 4 là tạo ra một tổ hợp tuyến tính của nhiều threshold functions hoạt động như một hàm tuần hoàn với tần số cao. Chứng minh của chúng tôi được lấy cảm hứng từ và mở rộng phương pháp chứng minh của [YS19] để phân tách giữa phương pháp kernel và mạng nơ-ron 2 lớp. Chi tiết hơn, lưu ý rằng mục tiêu có thể được viết lại như tổng của hai threshold functions:

f_b(x₁,x₂,y) = arg max_{x_i} ⟨x_i,y⟩ + b_i = 𝟙(⟨x₁-x₂,y⟩ + b* > 0)x₁ + 𝟙(⟨x₁-x₂,y⟩ + b* < 0)x₂, (10)

trong đó b* = b₁ - b₂ sẽ được xác định sau. Ký hiệu x := x₁ - x₂; chúng tôi sẽ tập trung vào việc cho thấy tính khó của xấp xỉ cho threshold function đầu tiên 𝟙(⟨x,y⟩ + b* > 0), từ đó tính khó của xấp xỉ cho f_b theo sau bằng các lập luận tiêu chuẩn. Chúng tôi định nghĩa một hàm tuần hoàn ψ_a(z): ℝ → ℝ trong khoảng [-a,a] là tổ hợp tuyến tính của a threshold functions (tại các break points khác nhau), trong đó a = Ω(d²), và cho thấy rằng đối với bất kỳ hàm g nào chỉ phụ thuộc vào một phép chiếu K của x lên một r-dimensional subspace chúng ta có:

𝔼_{x,y}[|ψ_a(⟨x,y⟩) · g(Kx,y)|] ≤ ||g|| · exp(-Ω(d-r)). (11)

Đặc biệt, nếu bất kỳ single threshold function nào được sử dụng để xây dựng ψ_a có thể được xấp xỉ bởi một rank-r attention layer với H/a heads, thì ψ_a cũng có thể được xấp xỉ bởi một rank-r attention layer với H heads. Tuy nhiên, điều này không thể nếu r nhỏ vì a chỉ là polynomial trong d, và correlation giữa mỗi head và ψ_a là exponentially small. Do đó, tồn tại một threshold function với break point tại b* khó xấp xỉ, trừ khi số lượng heads có bậc O(exp(d-r)/a). Trong Theorem 4, các đầu vào x₁ và x₂ được rút từ unit sphere được scale bởi hệ số √d. Chúng tôi lưu ý rằng việc re-scaling các đầu vào tương tự như việc giảm độ chính xác yêu cầu bởi cùng hệ số. Do đó, kết quả phân tách hàm mũ này giống với chế độ độ chính xác cao của Theorem 2, mặc dù các kỹ thuật được sử dụng trong chứng minh rất khác nhau.

6 Xấp xỉ Hiệu quả Sử dụng Độ sâu

Trong các mục trước, chúng tôi đã cho thấy rằng một lớp low-rank attention đơn không thể biểu diễn mục tiêu trừ khi số lượng heads rất lớn. Trong mục này, chúng tôi đề cập đến câu hỏi liệu các lớp độ sâu bổ sung có thể khắc phục điểm yếu này không. Độ sâu có thể có nghĩa là thêm một MLP sau lớp attention hoặc chỉ là một lớp attention khác; trong mục này chúng tôi xem xét cả hai lựa chọn. Chúng tôi trình bày một cấu trúc xấp xỉ hàm mục tiêu (với đầu vào được sửa đổi một chút) sử dụng hai lớp và chỉ polynomial nhiều rank-1 heads. Tuy nhiên, chúng tôi trình bày các cấu trúc chỉ cho trường hợp độ dài ngữ cảnh N = 2, cũng là thiết lập của các lower bounds của chúng tôi. Chúng tôi đưa ra giả thuyết rằng bất kỳ cấu trúc nào sử dụng low-rank heads đều đưa ra sự phụ thuộc bất lợi vào N, một điểm yếu đáng kể so với full-rank attention.

Các cấu trúc của chúng tôi dựa trên chiến lược mà chúng tôi gọi là "majority voting", chúng tôi mô tả ngắn gọn ở đây. Xem xét trường hợp N = 2 target points và hardmax attention. Đầu ra của mỗi head, giống như chính hàm mục tiêu, là x₁ hoặc x₂. Một random rank-1 head có correlation yếu với mục tiêu; xác suất nó xuất ra câu trả lời đúng là 1/2 + Ω(1/√d). Do đó, việc kết hợp nhiều random heads như vậy lại với nhau, mode của chúng (đầu ra với nhiều "votes" nhất) khớp với hàm mục tiêu với xác suất cao. Chúng tôi sử dụng lớp thứ hai để tính "majority vote" của các heads trong lớp attention.

Các cơ chế attention tiêu chuẩn làm cho việc đếm số lượng votes mà mỗi target point nhận được trở nên khó khăn — hoặc thậm chí nhớ các target points x₁ và x₂ là gì — vì lớp tiếp theo chỉ nhận được tổ hợp tuyến tính của chúng với các hệ số không biết. Do đó, chúng tôi sửa đổi lớp attention một chút để tạo điều kiện cho chiến lược majority voting. Chúng tôi concatenate các labels với các vectors cho phép chúng ta đếm bao nhiều lần x₁ và x₂ xuất hiện trong tổng. Sau đó chúng tôi sử dụng lớp attention thứ hai để tra cứu full vector tương ứng với majority label. Labeling này có thể được triển khai bằng cách concatenate positional encodings với các input points. Nghĩa là, thay vì nhập x₁,...,x_N ∈ S^(d-1) vào transformer, bây giờ chúng ta nhập [x₁ b₁],...,[x_N b_N] cho b_i ∈ ℝ^e. Một phép biến đổi tuyến tính có thể được sử dụng để ánh xạ đầu ra của transformer (d+e)-dimensional này trở lại ℝ^d. Lưu ý rằng hàm mục tiêu của chúng tôi là permutation-invariant, vì vậy thứ tự của các điểm không liên quan đến tác vụ. Do đó, những "positional encodings" được concatenate này hoạt động giống như sửa đổi kiến trúc. Chúng cung cấp các chiều đầu vào bổ sung phục vụ như không gian scratch trong đó mô hình có thể thực hiện các thao tác rời rạc như đếm và indexing mà không làm hỏng dữ liệu đầu vào. Cũng lưu ý rằng, vì chúng thay đổi chiều của đầu vào và của transformer, những concatenated positional encodings này khác với positional encodings được sử dụng trong thực tế (bao gồm RoPE [SAL+24] và ALiBi [PSL22]), được bao gồm trong framework generalized attention của chúng tôi.

Dưới đây, chúng tôi đưa ra định nghĩa chính thức của kiến trúc multi-layer transformer được sử dụng trong cấu trúc của chúng tôi. Nó sử dụng self-attention, có nghĩa là source và target points giống nhau. Chúng tôi sửa đổi cơ chế attention bằng cách thêm self-excluding mask sao cho mỗi input point không thể attend đến chính nó (xem dưới đây, trong đó chúng tôi tạo thành X̃ᵢ bằng cách xóa cột thứ i của X). Theo thực tế tiêu chuẩn, chúng tôi cũng sử dụng skip connection. Chúng tôi không cần MLP hoặc normalization layer, mặc dù cấu trúc của chúng tôi có thể dễ dàng được mở rộng để bao gồm chúng.

Definition 6. Một rank-r self-masked transformer layer với H heads là một hàm T: ℝ^(d×N) → ℝ^(d×N) được tham số hóa bởi rank-k attention heads {(Mₕ,Vₕ)}ᴴₕ₌₁ và được định nghĩa như sau:

X̃ᵢ := [x₁ ··· xᵢ₋₁ xᵢ₊₁ ··· x_N] (12)
Tᵢ(X) := xᵢ + ∑ᴴₕ₌₁ VₕX̃ᵢ sm(X̃ᵢᵀMₕxᵢ) (13)(14)

Ở đây, Tᵢ ký hiệu đầu ra thứ i (hoặc cột thứ i của đầu ra) [T₁(X) ··· T_N(X)].

Một two layer, rank-r transformer với concatenated positional encodings là một hàm T: ℝ^(d×N) → ℝ^(d×N) được tham số hóa bởi ma trận positional encoding E = ℝ^(dₑ×N) và hai (d+dₑ)-dimensional self-masked transformer layers, T⁽¹⁾ và T⁽²⁾, và ma trận output-layer A ∈ ℝ^(d×(d+dₑ)) và được định nghĩa như sau:

T(X) = A · T⁽²⁾(T⁽¹⁾([X E])). (15)

Định lý sau mô tả cấu trúc majority voting của chúng tôi sử dụng random rank-1 heads và concatenated positional encodings. Để chứng minh, xem Phụ lục D.3.

Theorem 7. Tồn tại các hằng số universal c₁,c₂ sao cho với tất cả d > c₁, và ε ∈ (0,1/2), và H ≥ c₂ · d³/ε², tồn tại một two layer, rank-1 transformer T với H heads và dₑ = 2 (như được định nghĩa trong Definition 6) mà

𝔼_{x₁,x₂,y∼Unif(S^(d-1))} [||f(x₁,x₂;y) - T([x₁ x₂ y])||²₂] ≤ ε. (16)

Người ta có thể tự hỏi liệu concatenated positional encodings có cần thiết để làm cho cấu trúc này hoạt động không, đặc biệt vì chúng phá vỡ permutation invariance để biểu diễn một mục tiêu permutation invariant. Trong Phụ lục D, chúng tôi trình bày một cấu trúc thay thế (Theorem 34) là permutation invariant. Tuy nhiên, nó sửa đổi kiến trúc bằng cách áp dụng MLP cho concatenation của các đầu ra của attention heads thay vì tổng của chúng.

Mặc dù các cấu trúc của chúng tôi giả định N = 2 source points, có vẻ khả thi để tổng quát hóa chúng cho N lớn hơn. Tuy nhiên, nhược điểm chính của việc tổng quát hóa như vậy là kích thước của transformer sẽ phụ thuộc vào N. Ngay cả bước đơn giản nhất của việc tính majority giữa N possible terms dường như không thể mà không có ít nhất sự phụ thuộc tuyến tính vào N. Mặt khác, Fact 1 cho thấy rằng hàm mục tiêu có thể được xấp xỉ cho bất kỳ N nào sử dụng một single full rank attention. Chúng tôi đưa ra giả thuyết rằng sự phụ thuộc như vậy vào N là cần thiết khi sử dụng low-rank attention:

Conjecture 8. Không có multi-layer transformer nào (với kích thước và ma trận weight cố định) có rank r < d xấp xỉ mục tiêu của Phương trình (3) cho tất cả N.

Nghĩa là, trong khi có thể xây dựng một transformer xấp xỉ mục tiêu cho một N cố định cho trước (như chúng tôi làm ở trên), chúng tôi đưa ra giả thuyết rằng không có cấu trúc nào như vậy độc lập với N. Việc chứng minh hoặc bác bỏ giả thuyết trên sẽ có những tác động rất khác nhau. Một phản ví dụ có nghĩa là điểm yếu của low-rank có thể được bù đắp bởi độ sâu, và do đó hạng không đóng vai trò quyết định trong sức mạnh biểu đạt của multi-layer transformers. Một chứng minh sẽ cho thấy rằng, ngay cả trong trường hợp multi-layer, low-rank attention về cơ bản yếu hơn high-rank attention.

7 Thí nghiệm

Trong mục này, chúng tôi bổ sung kết quả lý thuyết của chúng tôi bằng thí nghiệm trên một lớp kiến trúc rộng hơn. Chúng tôi huấn luyện off-the-shelf transformers — bao gồm nhiều lớp self-attention, MLP layers, skip connections, và normalization — trên một sửa đổi nhỏ của hàm nearest neighbor. Các thí nghiệm của chúng tôi xác nhận điểm yếu của low-rank attention trong thiết lập này. Chúng cũng cho thấy rằng cấu trúc full-rank của Fact 1 dễ dàng được học bởi gradient descent. Tất cả code có sẵn tại https://github.com/NoahAmsel/attention-formers.

Chi tiết mô hình và huấn luyện: Chúng tôi sử dụng triển khai Pytorch của transformer encoders [PGM+19] với hai sửa đổi. Đầu tiên, chúng tôi tổng quát hóa quy mô tiêu chuẩn H = d/r, cho phép H là bất kỳ bội số nào của d/r. (Đặc biệt, chúng tôi thử H = d^1.5/r và H = d²/r.) Thứ hai, chúng tôi thay thế layer normalization bằng RMSNorm [ZS19], một lựa chọn tiêu chuẩn trong transformers hiện đại [TLI+23, CND+24] cũng phù hợp hơn với hàm mục tiêu của chúng tôi. Chúng tôi huấn luyện với biases, nhưng thí nghiệm sơ bộ cho thấy rằng chúng ít ảnh hưởng¹. Chúng tôi chạy mỗi thí nghiệm trên một Nvidia GPU đơn (thường là V100) không quá vài giờ.

Vì chúng tôi đang sử dụng self-attention, không có sự phân biệt giữa source và target points. N input points được rút đều và i.i.d. từ S^(d-1), và chúng không bị ràng buộc phải trực giao. Chúng tôi thay đổi hàm mục tiêu của chúng tôi tương ứng. Đối với mỗi input point, mục tiêu bây giờ xuất ra điểm nào trong số các điểm khác xa nhất với nó. Chúng tôi xuất ra xa nhất thay vì gần nhất vì nếu không, mỗi điểm sẽ ánh xạ đến chính nó. Loss function là trung bình mean squared error trên N điểm. Chúng tôi không sử dụng attention mask nào. Đặc biệt, chúng tôi cho phép các điểm attend đến chính chúng. Dataset của chúng tôi là synthetic, vì vậy chúng tôi huấn luyện và test trên một stream các mẫu được tạo mới mà không bao giờ lặp lại. Chúng tôi huấn luyện trên 10⁵ batches kích thước 256 mỗi batch. Đối với tất cả thí nghiệm, chúng tôi sử dụng AdamW với cùng learning rate 0.01 và learning rate schedule cosine annealing với linear warm-up.

Rank separation: Thí nghiệm đầu tiên của chúng tôi nghiên cứu tầm quan trọng của hạng qua các số lượng heads (H) và layers (L) khác nhau. Chúng tôi cố định chiều d = 64 và số điểm N = 16. Trong thí nghiệm này, chúng tôi không sử dụng positional encodings. Hình 1 vẽ kết quả, cho thấy kết quả tốt nhất của năm lần chạy cho mỗi thiết lập. Mỗi đường sử dụng số lượng heads khác nhau, nhưng số lượng tham số mỗi attention layer, r×d×H = d^(c+1), được giữ không đổi trong mỗi đường. Quy mô tiêu chuẩn là d² tham số mỗi layer. Khi L = 1, kết quả gợi ý rằng sử dụng full-rank (r = 64) là cần thiết và đủ để học hàm mục tiêu chính xác; ngay cả 2d heads có rank d/2 cũng thất bại. Với L > 1, đánh đổi giữa hạng và độ chính xác thuận lợi hơn, nhưng low-rank attention vẫn kém hiệu suất đáng kể so với full-rank attention, ngay cả khi nó được sử dụng nhiều tham số hơn. Các transformer tiêu chuẩn năm lớp (nghĩa là L = 5, tham số mỗi layer = d²) dường như gặp khó khăn tối ưu hóa trên vấn đề này. Loại trừ trường hợp đó, mô hình hoạt động tốt nhất không phải full-rank (L = 5, d³ tham số mỗi layer, r = 32) hoạt động không tốt hơn mô hình full-rank tệ nhất (L = 1, d² tham số mỗi layer, r = 64) mặc dù có 80x nhiều tham số hơn trong các attention layers của nó. Tóm lại, một transformer tiêu chuẩn với H = 1 hoạt động tốt hơn nhiều so với một với H thậm chí vừa phải lớn hơn trên tác vụ này.

Full-rank solution: Trong trường hợp full-rank, transformer học được mục tiêu, nhưng nó đã học được biểu diễn nào? Hình 2 gợi ý rằng, trong một số trường hợp, nó rất gần với cấu trúc của Fact 1. Nhớ lại rằng trong Fact 1, chúng tôi sử dụng hardmax attention head với K_h Q_h^T = I. Trong thí nghiệm của chúng tôi tuy nhiên, chúng tôi sử dụng hàm mục tiêu farthest neighbor và softmax heads, vì vậy cấu trúc tương ứng là K_h Q_h^T = -cI với c ≫ 1. Panel đầu tiên cho thấy median Frobenius angle giữa các ma trận K_h Q_h^T và I được học bởi các mô hình full-rank, single layer trong thí nghiệm trước. Điều này cho thấy rằng K_h Q_h^T rất gần bằng -I đến một hệ số không đổi. Hơn nữa, như panel thứ hai cho thấy, norm của ma trận này lớn, khiến softmax hoạt động như hardmax. Kết quả tương tự cho mạng ba lớp với một single full-rank head, nhưng khi L > 1 và H > 1, có vẻ như mạng học được một chiến lược khác, ít có thể diễn giải hơn để biểu diễn mục tiêu.

Positional encodings: Vì hàm mục tiêu của chúng tôi là permutation-invariant, không có thông tin positional nào tồn tại trong dữ liệu. Tuy nhiên, trong Mục 6, chúng tôi đã cho thấy rằng concatenated positional encodings có thể giúp low-rank attention thành công khi L > 1 bằng cách cho mô hình các chiều không gian scratch bổ sung. Các sơ đồ positional encoding được sử dụng trong thực tế, như additive encodings [VSP+17], RoPE [SAL+24] và ALiBi [PSL22], không thể được sử dụng theo cách này, là các phiên bản của generalized attention heads được nghiên cứu trong bài báo này. Trong Hình 3, chúng tôi thí nghiệm với positional encodings. Như mong đợi, additive attention hoàn toàn thất bại trong việc giúp low-rank attention. Panel bên trái cho thấy rằng khi L = 1, concatenated positional encodings cũng thất bại. Tuy nhiên, khi L = 3, concatenated positional encodings mang lại cải thiện đáng kể, một phát hiện phù hợp với Theorem 7.

Vai trò của N: Trong Hình 4, chúng tôi khám phá cách số lượng input points N ảnh hưởng đến độ khó của việc học hàm mục tiêu. Chúng tôi cố định d = 64, H = d²/r, và số lớp L = 2. Kết quả cho thấy rằng, như được dự đoán bởi Fact 1, các full-rank heads học mục tiêu chính xác qua một phạm vi N. Tuy nhiên, các low-rank heads gặp phải độ chính xác giảm khi N tăng. Điều này phù hợp với Conjecture 8, dự đoán rằng các transformer low-rank có kích thước cố định thất bại trong việc biểu diễn chính xác mục tiêu cho N đủ lớn.

8 Kết luận và Hạn chế

Trong bài báo này, chúng tôi đã điều tra vai trò của hạng trong các cơ chế attention. Chúng tôi đặt câu hỏi về thực tế gần như universal của việc đánh đổi hạng và số lượng heads theo H = d/r. Chúng tôi cho thấy rằng đối với một hàm mục tiêu đơn giản và tự nhiên được lấy cảm hứng từ tìm kiếm ngữ nghĩa, low-rank attention về cơ bản yếu hơn full-rank attention, ngay cả khi H ≫ d/r. Chúng tôi chứng minh sự phân tách nghiêm ngặt này giữa chế độ low-rank và high-rank cả về mặt lý thuyết, bằng cách chứng minh tính khó của xấp xỉ trong thiết lập shallow, và thực nghiệm, thông qua thí nghiệm với off-the-shelf transformers. Kết quả của chúng tôi do đó gợi ý về một đánh đổi có lợi tiềm năng giữa số lượng heads và hạng vẫn chưa được khám phá rộng rãi trong các ứng dụng.

Điều đó nói lên rằng, phân tích lý thuyết của chúng tôi về cơ bản bị hạn chế đối với việc nghiên cứu shallow transformers, và kết quả của Mục 6 minh họa cách thêm độ sâu có thể khắc phục những hạn chế của low-rank self-attention trong một số trường hợp. Tuy nhiên, chúng tôi hy vọng rằng kết quả của chúng tôi sẽ thúc đẩy các nhà lý thuyết và thực hành xem xét cẩn thận hơn các thiết lập và quy mô của các siêu tham số transformer. Đặc biệt, chúng gợi ý rằng các mô hình lý thuyết sử dụng full-rank attention có thể không mô tả chính xác các transformers được sử dụng trong thực tế, và rằng vẫn còn nhiều điều cần hiểu về những thành công và failure modes của các kiến trúc dựa trên attention.

Một số câu hỏi mở vẫn còn cho công trình tương lai. Kiến trúc transformer cơ bản của [VSP+17] cho phép người dùng thiết lập một số siêu tham số. Mặc dù tính phổ biến của kiến trúc này, các thiết lập siêu tham số khác ngoài chiều embedding và số lớp gần như không bao giờ được thay đổi đáng kể; xem Phụ lục A. Trong khi công trình trước đó đáng kể đã nghiên cứu scaling laws cho chiều và số lớp, chúng tôi tin rằng nghiên cứu tương lai cũng nên xem xét các siêu tham số khác và tìm cách hiểu những đánh đổi, dependencies, và scaling laws giữa chúng. Ở đây, chúng tôi tập trung vào hạng query/key và mối quan hệ của nó với số lượng heads, nhưng độ sâu và độ rộng của MLPs và value/output rank cũng đáng quan tâm.

Ngoài ra, tính bất biến quay của phân phối dữ liệu đầu vào là công cụ trong việc thiết lập các lower bounds của chúng tôi. Cho tính chất rời rạc vốn có của transformers dựa trên văn bản, một câu hỏi tự nhiên là hiểu cách tổng quát hóa các kỹ thuật của chúng tôi ngoài thiết lập rotationally-invariant. Một hướng khác cho công trình tương lai là hiểu mối quan hệ giữa hạng và độ dài ngữ cảnh. Tập trung vào trường hợp N = 2 đủ để chúng tôi chứng minh phân tách hạng, nhưng chúng tôi tin rằng kết quả tương tự nên đúng ít nhất với tất cả N ≤ d; Hình 4 cung cấp bằng chứng thực nghiệm sơ bộ. Hiểu trường hợp N > 2 cũng có thể giúp giải quyết một câu hỏi mở cuối cùng: Mối quan hệ giữa hạng và độ sâu là gì? Đặc biệt, liệu Conjecture 8 có đúng không?

Acknowledgements: Công trình này được hỗ trợ một phần bởi Alfred P. Sloan Foundation, và các giải thưởng NSF RI-1816753, NSF CAREER CIF 1845360, NSF CHS-1901091 và NSF DMS-MoDL 2134216. Chúng tôi cảm ơn Ohad Shamir vì những thảo luận hữu ích trong quá trình hoàn thành công trình này.

[Tiếp tục với phần References và Appendix...]
