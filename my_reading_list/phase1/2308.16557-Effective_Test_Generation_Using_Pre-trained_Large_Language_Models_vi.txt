# Tạo Test Hiệu Quả Sử Dụng Mô Hình Ngôn Ngữ Lớn Được Tiền Huấn Luyện và Kiểm Thử Đột Biến

Arghavan Moradi Dakhela,∗, Amin Nikanjama, Vahid Majdinasaba, Foutse Khomhaand Michel
C. Desmaraisa
aDepartment of Computer and Software Engineering, Polytechnique Montreal, Montreal, H3T 1J4, Quebec, Canada

TÓM TẮT
Bối cảnh: Một trong những giai đoạn quan trọng trong chu kỳ phát triển phần mềm là kiểm thử phần mềm. Kiểm thử giúp xác định các lỗi tiềm ẩn và giảm chi phí bảo trì. Mục tiêu của các công cụ tạo test tự động là giúp việc phát triển test dễ dàng hơn bằng cách đề xuất các test hiệu quả để phát hiện lỗi. Gần đây, các nhà nghiên cứu đã tận dụng Mô hình Ngôn ngữ Lớn (LLMs) của code để tạo unit test. Trong khi độ bao phủ code của các test được tạo thường được đánh giá, tài liệu đã thừa nhận rằng độ bao phủ có mối tương quan yếu với hiệu quả của test trong việc phát hiện lỗi.

Mục tiêu: Để cải thiện hạn chế này, trong bài báo này, chúng tôi giới thiệu MuTAP (Mutation Test case generation using Augmented Prompt) để cải thiện hiệu quả của các test case được tạo bởi LLMs trong việc phát hiện lỗi bằng cách tận dụng kiểm thử đột biến.

Phương pháp: Mục tiêu của chúng tôi được đạt được bằng cách tăng cường prompts với các mutant sống sót, vì những mutant này làm nổi bật các hạn chế của test case trong việc phát hiện lỗi. MuTAP có khả năng tạo các test case hiệu quả khi không có mô tả ngôn ngữ tự nhiên của Chương trình Được Kiểm thử (PUTs). Chúng tôi sử dụng các LLMs khác nhau trong MuTAP và đánh giá hiệu suất của chúng trên các benchmark khác nhau.

Kết quả: Kết quả của chúng tôi cho thấy phương pháp đề xuất có thể phát hiện thêm tới 28% các đoạn code lỗi do con người viết. Trong số này, 17% vẫn không được phát hiện bởi cả công cụ tạo test tự động hoàn toàn hiện tại (tức là Pynguin) và các phương pháp học zero-shot/few-shot trên LLMs. Hơn nữa, MuTAP đạt được Điểm số Đột biến (MS) 93.57% trên code lỗi tổng hợp, vượt trội hơn tất cả các phương pháp khác trong đánh giá của chúng tôi.

Kết luận: Những phát hiện của chúng tôi cho thấy mặc dù LLMs có thể phục vụ như một công cụ hữu ích để tạo test case, chúng cần các bước hậu xử lý cụ thể để tăng cường hiệu quả của các test case được tạo ra, có thể gặp phải lỗi cú pháp hoặc chức năng và có thể không hiệu quả trong việc phát hiện một số loại lỗi nhất định và kiểm thử các trường hợp góc trong PUTs.

1. Giới thiệu

Kiểm thử là một bước quan trọng nhưng tốn kém trong chu kỳ phát triển phần mềm. Tạo các test hiệu quả là một nhiệm vụ tốn thời gian và tẻ nhạt đối với các nhà phát triển. Unit test rất quan trọng vì chúng tạo thành cơ sở của tháp tự động hóa kiểm thử [44, 47]. Unit test kiểm tra xem một hàm hoặc một thành phần có hoạt động như mong đợi trong môi trường cô lập hay không. Một unit test bao gồm hai thành phần: thành phần đầu tiên là một tập hợp các đầu vào kiểm thử cho Chương trình Được Kiểm thử (PUT), trong khi thành phần thứ hai là test oracle cho biết hành vi dự định (đầu ra) của PUT và do đó có khả năng phát hiện lỗi bằng cách xác minh tính đúng đắn của PUT trên các đầu vào kiểm thử [51]. Một test oracle có thể ở dạng các assertion.

Việc tạo unit test tự động là một chủ đề quan trọng trong Kỹ thuật Phần mềm (SE). Nó nhằm mục đích giảm nỗ lực kiểm thử của các nhà phát triển. Phát triển unit test chất lượng tốt có thể ngăn chặn lỗi trong sản phẩm phần mềm. Có nhiều công cụ khác nhau để tự động tạo unit test và test suite dựa trên bộ tạo test ngẫu nhiên [42, 6], thực thi tượng trưng động [43, 19], hoặc các phương pháp dựa trên tìm kiếm [16, 17]. Tuy nhiên, những kỹ thuật này có một số nhược điểm và thường tạo ra các test không có assertion hoặc assertion quá chung chung, hoặc test với assertion không thể đánh giá hiệu quả hành vi dự định của PUT [39, 36].

Xem xét những thiếu sót này, các nhà nghiên cứu gần đây đã khám phá khả năng tận dụng các kỹ thuật tổng hợp code dựa trên Học máy để tạo unit test tốt hơn [7, 11, 28, 41, 29]. Cụ thể, các phương pháp này đã khám phá tiềm năng của Mô hình Ngôn ngữ Lớn (LLMs) với kiến trúc transformer, như Codex [12], đã đạt được hiệu suất tốt trong tổng hợp chương trình tự động [9, 12, 13, 15, 34]. Trong số các nỗ lực như vậy, Bareiß et al. [7] đánh giá hiệu suất của Codex trong việc tạo test case bằng cách sử dụng phương pháp học few-shot. Những phát hiện của họ trên một tập hợp giới hạn 18 phương thức Java cho thấy phương pháp của họ có thể so sánh với việc tạo test có hướng dẫn phản hồi. ATHENATEST [49] đã tận dụng mô hình transformer BART [30] sau khi tinh chỉnh nó trên một tập hợp các hàm Java thực và các test tương ứng. Họ cũng báo cáo đạt được độ bao phủ có thể so sánh với EvoSuite [17] sau khi đánh giá năm dự án Java. Lemieux et al. [29] đề xuất CODAMOSA sử dụng các test case được tạo bởi Codex để cải thiện các kỹ thuật kiểm thử dựa trên tìm kiếm, bao gồm chỉ tiền tố (đầu vào) của một test case mà không có bất kỳ test oracle nào. Kết quả được báo cáo của họ thu được trên 27 dự án Python cho thấy CODAMOSA vượt trội hơn kỹ thuật dựa trên tìm kiếm cơ bản, Pynguin [33] và Codex về độ bao phủ code. Mặc dù kết quả sơ bộ của những nghiên cứu này và những nghiên cứu khác [50, 41, 11, 28] là đầy hứa hẹn, không có nghiên cứu nào trong số này cố gắng cải thiện khả năng phát hiện lỗi của các test được tạo ra. Hơn nữa, đã được thừa nhận trong tài liệu rằng trong khi độ bao phủ test là một metric hữu ích để đánh giá chất lượng test, nó có mối tương quan yếu với hiệu quả của test trong việc phát hiện lỗi [10, 20, 22].

Kiểm thử Đột biến (MT) là một kỹ thuật kiểm thử white box để đánh giá khả năng của một test trong việc phát hiện lỗi. MT đã được nghiên cứu rộng rãi và sử dụng thành công trong SE để đánh giá hiệu quả của test case [25, 40]. MT liên quan đến việc tiêm những thay đổi nhân tạo dựa trên lỗi thực vào một PUT, tạo ra các phiên bản đột biến của PUT được gọi là mutant. Càng nhiều mutant mà một test case tiêu diệt, càng hiệu quả trong việc xác định lỗi thực. Các mutant sống sót làm nổi bật những điểm yếu của test case và mục tiêu cuối cùng là để test case có thể phát hiện tất cả mutant, tức là tiêu diệt chúng. Mutant không chỉ hữu ích để đánh giá hiệu quả của test case mà còn có thể được sử dụng như một phương tiện để thiết kế test case hiệu quả hơn [17].

Trong bài báo này, chúng tôi trình bày nghiên cứu đầu tiên tận dụng MT để tăng cường và đánh giá hiệu quả của test case được tạo bởi LLMs cho các chương trình Python về khả năng phát hiện lỗi. Phương pháp của chúng tôi nhằm tối ưu hóa test case để phát hiện lỗi thay vì độ bao phủ code. Kỹ thuật đề xuất của chúng tôi, MuTAP, sử dụng LLM như Thành phần chính (LLMC) của nó và bắt đầu bằng cách cung cấp một prompt cho LLMC để tạo test case. Prompt ban đầu bao gồm PUT và hướng dẫn tạo test case bằng cách sử dụng học zero-shot và few-shot. Tiếp theo, MuTAP đánh giá cú pháp của các test case được tạo và gửi lại prompt cho LLMC để sửa chữa bất kỳ vấn đề cú pháp nào được phát hiện. Sau khi sửa lỗi cú pháp, MuTAP tiếp tục đánh giá hành vi dự định của các test case được tạo. Điều này được thực hiện bằng cách so sánh đầu ra của test oracle trên các đầu vào kiểm thử nhất định với các giá trị trả về mong đợi của PUT bằng cách sử dụng cùng các đầu vào kiểm thử, qua đó sửa chữa bất kỳ hành vi không mong muốn nào trong test oracle.

Sau đó, MuTAP áp dụng MT để kiểm tra hiệu quả của test case trong việc tiêu diệt mutant của PUT. Vì các mutant sống sót làm nổi bật hạn chế của test case được tạo, MuTAP gửi lại prompt cho LLMC để tạo test case mới cho PUT có mutant sống sót bằng cách tăng cường prompt ban đầu với cả test case ban đầu và các mutant sống sót. MuTAP dừng quá trình tăng cường prompt ban đầu khi test case cuối cùng có thể phát hiện hiệu quả tất cả mutant hoặc không còn mutant sống sót nào chưa được sử dụng để tăng cường prompt ban đầu.

Chúng tôi sử dụng hai loại LLM làm LLMC của MuTAP: Codex, được thiết kế cho các nhiệm vụ liên quan đến code, và llama-2-chat, được tối ưu hóa cho các trường hợp sử dụng dialog và đủ linh hoạt để đáp ứng một loạt các nhiệm vụ, bao gồm cả lập trình. Chúng tôi đánh giá MuTAP trên cả lỗi tổng hợp của 164 PUT [12] và 1710 chương trình lỗi được thu thập từ một benchmark sửa chữa lỗi Python [23]. Kết quả của chúng tôi chỉ ra rằng phương pháp đề xuất của chúng tôi tạo ra các test case hiệu quả với Điểm số Đột biến (MS, tỷ lệ mutant bị tiêu diệt trên tổng số mutant) trung bình 93.57%, vượt trội hơn cả Pynguin (công cụ tạo test tự động hoàn toàn tiên tiến) và các kỹ thuật học zero-shot/few-shot LLM thông thường. Hơn nữa, phương pháp của chúng tôi phát hiện tới 468 (28%) đoạn code lỗi do con người viết nhiều hơn so với các phương pháp có thể so sánh khác trong đánh giá của chúng tôi. Đáng chú ý, nó xác định 79 (17%) đoạn code lỗi của con người mà không có kỹ thuật nào khác có thể phát hiện. Tóm lại, bài báo này đóng góp như sau:

• Chúng tôi trình bày nghiên cứu đầu tiên về việc tận dụng MT để tạo test case với LLMs.
• Chúng tôi đề xuất một kỹ thuật học dựa trên prompt để cải thiện hiệu quả của test case bằng cách tăng cường prompts với cả test case ban đầu và các mutant sống sót của PUT.
• Chúng tôi đánh giá hiệu quả của test được tạo trong việc phát hiện lỗi trong các phiên bản lỗi thực và tổng hợp của PUT.
• Chúng tôi làm cho kỹ thuật đề xuất, MuTAP, có sẵn công khai trực tuyến [3] cho các nhà nghiên cứu/thực hành khác để tái tạo hoặc xây dựng dựa trên công việc của chúng tôi.

Phần còn lại của bài báo được tổ chức như sau. Phần 2 giới thiệu một ví dụ động lực. Phần 3 mô tả các bước khác nhau của phương pháp của chúng tôi. Chúng tôi trình bày thiết lập thí nghiệm, câu hỏi nghiên cứu và kết quả thí nghiệm trong Phần 4. Chúng tôi thảo luận về những phát hiện và các trường hợp sử dụng tiềm năng của phương pháp chúng tôi trong Phần 5. Các mối đe dọa đối với tính hợp lệ của kết quả được xem xét trong Phần 6. Chúng tôi xem xét ngắn gọn các công trình liên quan trong Phần 7. Cuối cùng, chúng tôi kết luận bài báo trong Phần 8; nêu bật một số hướng cho công việc tương lai.

2. Ví dụ Động lực

Trong phần này, chúng tôi trình bày một ví dụ trong Hình 1 cho thấy phương pháp đề xuất của chúng tôi tạo ra các test case hiệu quả như thế nào. Giả sử chúng ta có 10 mutant {SM0, SM1, ..., SM9} cho Chương trình Được Kiểm thử, PUT trong Hình 1. Mục tiêu của kỹ thuật đề xuất của chúng tôi, MuTAP (Mutation Test case generation using Augmented Prompt), là tạo ra các test case hiệu quả cho PUT theo cách đảm bảo tiêu diệt số lượng mutant tối đa.

Hàm any_int() trong Hình 1 nhận 3 đầu vào và trả về True nếu tất cả 3 đầu vào đều là số nguyên, cũng như một trong các đầu vào bằng tổng của hai đầu vào khác. Ngược lại, nó trả về False. Trong bước đầu tiên, MuTAP sử dụng prompt ban đầu, 1, để chạy một truy vấn trên LLM Component (LLMC) và tạo test case ban đầu cho Chương trình Được Kiểm thử (PUT) này. Thành phần 2 trong Hình 1 cho thấy test case ban đầu được tạo bởi LLMC sau bước tinh chỉnh. Chúng tôi đặt tên nó là Initial Unit Test, IUT. Trong Phần 3, chúng tôi thảo luận chi tiết về bước tinh chỉnh (sửa cú pháp và hành vi dự định) của phương pháp chúng tôi. IUT tiêu diệt 6 trong số 10 mutant của PUT. 4 mutant còn lại tiết lộ những điểm yếu của test được tạo ra, có nghĩa là IUT cần test case mới với assertion để tiêu diệt các lỗi được tiêm vào 4 mutant đó.

Để giải quyết hạn chế này và tạo ra test case hiệu quả hơn, MuTAP tăng cường prompt ban đầu với hai thành phần mới; thành phần đầu tiên là phản hồi của mô hình cho prompt ban đầu sau khi sửa cú pháp và hành vi dự định, IUT, và thành phần thứ hai là thành phần mutant, 3 trong Hình 1. MuTAP bắt đầu xây dựng thành phần mutant bằng cách sử dụng "Survived Mutant" đầu tiên của PUT mà chúng tôi gọi là SM0. Phần tô sáng màu đỏ trong SM0 cho thấy lỗi được tiêm vào PUT. Lỗi được tiêm thay đổi câu lệnh thứ hai trong điều kiện của if bên trong PUT theo cách mà tổng của đầu vào đầu tiên và cuối cùng của hàm any_int() không bằng đầu vào giữa nữa. Vì không có test case nào trong IUT để xác minh rằng đầu vào giữa, y, bằng tổng của đầu vào đầu tiên và cuối cùng, x và z, IUT không thể tiêu diệt mutant này.

MuTAP sử dụng việc nối ba thành phần này: 1, 2 và 3 để gửi lại prompt cho LLMC. Thành phần 4 trong Hình 1 cho thấy tập hợp test case mới được tạo bởi LLMC được thêm vào IUT sau bước tinh chỉnh. Chúng tôi đặt tên nó là Augmented Unit Test, AUT0. Unit test có thêm hai assertion so với IUT và một trong số chúng, được tô sáng màu đỏ, tiêu diệt mutant, SM0.

MuTAP áp dụng AUT0 cho các mutant của PUT một lần nữa. Nếu còn bất kỳ mutant sống sót nào, MuTAP lặp lại quá trình tăng cường bằng cách cập nhật thành phần mutant với một mutant sống sót khác nếu nó chưa được sử dụng để tăng cường prompt trước đó. MuTAP sử dụng từng mutant riêng lẻ vì đôi khi test case mới giải quyết một mutant cũng có thể tiêu diệt các mutant sống sót còn lại. Hơn nữa, do độ dài hạn chế của prompt và độ dài không cố định của mutant, việc áp dụng từng mutant sống sót riêng biệt là một phương pháp thực tế hơn. Hình 1 3′ cho thấy một ví dụ về cách thành phần mutant được cập nhật bằng cách sử dụng một mutant sống sót khác. Chúng tôi gọi mutant này là SM1. Unit test, 4′, cho thấy một tập hợp test case mới bao gồm một assertion phát hiện SM1. MuTAP lặp lại quá trình tăng cường cho đến khi test case cuối cùng có thể tiêu diệt tất cả mutant, hoặc không còn mutant sống sót nào chưa được sử dụng để tăng cường prompt ban đầu.

Test case cuối cùng được tạo bởi kỹ thuật đề xuất của chúng tôi, MuTAP, tiêu diệt 9 trong số 10 mutant của ví dụ này, PUT, và nó tăng MS cho PUT từ 60% (6 trong số 10) lên 90% (9 trong số 10). Kết quả này có thể được so sánh với công cụ tạo test tự động tiên tiến cho ngôn ngữ lập trình Python [33], Pynguin, chỉ tạo ra một test case cho PUT với MS chỉ 40%. Công cụ này sử dụng kỹ thuật tạo dựa trên tìm kiếm [5] và ngẫu nhiên đột biến các giá trị test trong test case để tạo test case mới. Tính chất ngẫu nhiên của phương pháp này dẫn đến khả năng thấp trong việc tạo test case mới có thể tiêu diệt các mutant sống sót của PUT.

3. Phương pháp

Trong phần này, chúng tôi thảo luận về các bước khác nhau của phương pháp chúng tôi. Hình 2 cho thấy tổng quan về phương pháp đề xuất của chúng tôi và Thuật toán 1 trình bày chuỗi các bước khác nhau của nó.

3.1. Prompt Ban đầu

LLMs có khả năng thực hiện những nhiệm vụ mà chúng đã được huấn luyện. Tinh chỉnh LLMs để thực hiện một nhiệm vụ mới tốn kém về mặt tính toán. Ngoài ra, có những LLMs như Codex cho thấy hiệu suất rất tốt trong việc tạo code nhưng vì chúng là closed-source, việc tinh chỉnh chúng cho một nhiệm vụ mới là không thể.

Học dựa trên prompt [31, 52] là một kỹ thuật hiệu quả để điều chỉnh LLMs cho các nhiệm vụ mới. Một prompt là sự kết hợp của bối cảnh ngôn ngữ tự nhiên và/hoặc ngôn ngữ lập trình và được sử dụng làm đầu vào cho LLMs. Có các nghiên cứu cho thấy rằng việc đưa một hướng dẫn ngôn ngữ tự nhiên làm gợi ý (học zero-shot) [29, 15, 41] hoặc một số ví dụ (học few-shot) [35, 9, 1] trong prompt tăng khả năng của LLMs trong việc thực hiện một nhiệm vụ mới.

MuTAP sử dụng cả học zero-shot và few-shot để xây dựng prompt ban đầu và gọi LLMC trên chúng riêng biệt. Bước này được thể hiện trong Thuật toán 2. Chi tiết hơn, chúng tôi sử dụng zero-shot và few-shot như sau:

• zero-shot: Prompt ban đầu được tạo bởi kỹ thuật zero-shot chứa ba đơn vị, theo phương pháp trong [29]. Thành phần được chỉ bởi 1 trong Hình 1 cho thấy một ví dụ về prompt như vậy. Đơn vị đầu tiên trong thành phần này là một hướng dẫn bằng ngôn ngữ tự nhiên có tên INS1 và nó làm rõ nhiệm vụ bằng cách hỏi: "Generate test case for the following code". Đơn vị thứ hai là Chương trình Được Kiểm thử (PUT) và đơn vị cuối cùng là một tập hợp hướng dẫn bằng ngôn ngữ lập trình có tên INS2. INS2 hoạt động như một gợi ý để chỉ ra đầu ra mong muốn cho LLMC. Việc nối (INS1, PUT, INS2) xây dựng prompt ban đầu cho học zero-shot (Dòng 2 trong Thuật toán 2).

• few-shot: Tạo prompt dựa trên học few-shot sử dụng một chuỗi các đầu vào và đầu ra mong đợi liên quan đến nhiệm vụ downstream. Có các phương pháp khác nhau để trình bày cặp đầu vào và đầu ra trong prompt. Chúng tôi theo phương pháp trong [1] để xây dựng prompt ban đầu với chiến lược few-shot trong MuTAP. Xem xét độ dài token tối đa có thể cho LLMC (4k token trong nghiên cứu của chúng tôi), prompt few-shot bao gồm hai ví dụ minh họa khác nhau của một Phương thức (M) và một Unit Test (UT) như sau (Dòng 5 trong Thuật toán 2):

<code>M_1</code>\n<test>UT_1</test>\n
<code>M_2</code>\n<test>UT_2</test>\n
<code>PUT_i</code>\n<test>

Không có mô tả ngôn ngữ tự nhiên về PUT trong prompt ban đầu vì những mô tả như vậy có thể không luôn có sẵn, và MuTAP dựa vào khả năng của LLMC để tổng hợp bối cảnh code. MuTAP gọi prompt ban đầu, zero-shot hoặc few-shot, trên LLMC và sau đó chuyển đầu ra suy luận đến bước tiếp theo (Dòng 2 trong Thuật toán 1).

3.2. Tinh chỉnh

Trong phần này, chúng tôi mô tả quá trình tinh chỉnh các test case được tạo trong MuTAP bao gồm sửa lỗi cú pháp và sửa chữa hành vi dự định. Chi tiết được hiển thị trong Thuật toán 3.

3.2.1. Sửa Cú pháp

Các test case được tạo bởi LLMC có thể có lỗi cú pháp (thiếu dấu ngoặc, dòng chưa hoàn thành, v.v.). Vì MuTAP cần thực thi hàm test để điều tra MT và tăng cường prompt, các mẫu có lỗi cú pháp trở nên không hiệu quả. Tuy nhiên, đôi khi một thay đổi nhỏ trong đầu ra của LLMC có thể sửa lỗi cú pháp và chuyển đổi nó thành một test case có thể thực thi.

MuTAP sử dụng khả năng của LLMC để sửa lỗi cú pháp, tương tự như các nghiên cứu khác [52, 26]. Để làm điều này, LLMC được gọi trên một prompt mới để sửa lỗi cú pháp trong đầu ra của chính nó (Thủ tục SyntaxFixer trong Thuật toán 3). Prompt sửa cú pháp bao gồm hai phần. Phần đầu tiên là một hướng dẫn ngôn ngữ tự nhiên, INSfix, "Fix the syntax errors in the following code snippet", và phần thứ hai là hàm test được tạo bởi LLMC trên prompt ban đầu (Dòng 7-8 trong Thuật toán 3). Nếu lỗi cú pháp vẫn tồn tại ngay cả sau khi gửi lại prompt cho LLMC, MuTAP sử dụng parser Python để xác định dòng lỗi. Sau đó nó giữ lại các dòng trước dòng có vấn đề, đảm bảo chúng vẫn không có lỗi cú pháp (Dòng 13 trong Thuật toán 3).

3.2.2. Sửa chữa Hành vi Dự định

Dựa trên prompt ban đầu, LLMC tạo ra các test case khác nhau được tuần tự hóa như assertion oracle bằng cách gọi PUT trên các đầu vào nhất định và so sánh đầu ra trả về của PUT với đầu ra mong đợi hoặc ground truth, ví dụ, {assert add(2,2) == 4}. Tuy nhiên, có thể LLMC tạo ra các test case đang khẳng định các giá trị trả về sai. Điều đó có nghĩa là đối với một số test case, LLMC không tạo ra đầu ra trả về mong đợi của PUT. Việc thiếu mô tả ngôn ngữ tự nhiên về PUT trong prompt ban đầu có thể dẫn đến việc tạo ra các test case không phản ánh chính xác hành vi dự định của phương thức.

Assertion với giá trị trả về sai có thể thất bại trên mutant, không phải vì phát hiện lỗi, mà vì hành vi không mong muốn của assertion. Những thất bại này gây nhầm lẫn về hiệu quả của test case. Vì vậy, bước này của MuTAP nhằm sửa chữa hành vi dự định của assertion oracle trong test case (Thủ tục IntendedBehaviorFixer trong Thuật toán 3).

Đối với mỗi assertion trong test, MuTAP chạy PUT trên các đầu vào test và so sánh đầu ra trả về của PUT với đầu ra khẳng định. Nếu đầu ra trả về của PUT giống với đầu ra khẳng định trong oracle, thì MuTAP coi nó là assertion oracle với hành vi dự định đúng. Ngược lại, nó sửa chữa những assertion đó bằng cách thay thế đầu ra khẳng định bằng đầu ra mong đợi của PUT (Dòng 22-27 trong Thuật toán 3). MuTAP bỏ qua những assertion mà các kiểu đầu vào thất bại trên PUT, ví dụ, nếu PUT mong đợi một list số nguyên nhưng đầu vào test là một string.

Kết quả cuối cùng của bước này được đặt tên là Initial Unit Test (IUT) là một tập hợp test case được tạo bởi LLMC sau khi tinh chỉnh như được hiển thị bởi 2 trong Hình 1.

3.3. Kiểm thử Đột biến (MT)

MT đánh giá chất lượng và hiệu quả của test case. Mutant được xây dựng bằng cách tiêm lỗi nhân tạo vào PUT để mô phỏng khiếm khuyết. Nếu test case thất bại trên một mutant, chúng tôi coi nó là một mutant bị tiêu diệt, ngược lại, nó sống sót, có nghĩa là test case trong unit test không thể phát hiện nó. Sự hiện diện của mutant sống sót làm nổi bật những thiếu sót của test case, gợi ý cần thiết phải thêm test case mới hoặc cải thiện test case hiện có. Điểm số Đột biến (MS) thể hiện hiệu quả của test case bằng cách tính tỷ lệ mutant bị tiêu diệt trong tất cả mutant của PUT.

Thuật toán 4 trình bày chi tiết của bước này. Được truyền cảm hứng từ [32], MuTAP sử dụng MutPy [21] để tạo ra các mutant khác nhau cho mỗi PUT và tính toán MS (Dòng 3-7 trong Thuật toán 4). Thực thi test case trên mỗi mutant liên quan đến việc thực hiện một số thiết lập sơ bộ. Với mục đích này, MuTAP sử dụng "setuptools.find_packages" có sẵn của Python để định vị và cài đặt các gói cần thiết, như "math", "numPy", "pandas", "pytest", và những gói khác. Ngoài ra, MuTAP triển khai các hàm thiết lập chịu trách nhiệm tạo các thư mục tạm thời, được sử dụng trong quá trình thực thi test case trên mutant. Sau khi thực thi test case trên mutant và tính toán MS, MuTAP dọn dẹp thiết lập bằng cách xóa thư mục tạm thời.

Như được hiển thị trên Dòng 5-9 trong Thuật toán 1, nếu MS của PUT đạt 100%, MuTAP chuyển test case đến bước tối thiểu hóa oracle (Phần 3.5), ngược lại, nó thu thập danh sách mutant sống sót và chuyển mutant đến bước tăng cường prompt (Phần 3.4).

3.4. Tăng cường Prompt

Thuật toán 5 cho thấy chi tiết của bước này. Nếu có bất kỳ mutant sống sót nào từ bước trước, MuTAP tăng cường prompt ban đầu, zero-shot hoặc few-shot, bằng cách thêm bốn thành phần mới (Dòng 3 trong Thuật toán 5). Thành phần đầu tiên là IUT, unit test ban đầu được tạo bởi LLMC sau khi tinh chỉnh. Thành phần thứ hai là một hướng dẫn bằng ngôn ngữ tự nhiên có tên INS3 làm rõ thiếu sót của IUT bằng "The test function, test(), cannot detect the fault in the following code". Thành phần thứ ba là một trong các mutant sống sót của PUT, có tên SM. Thành phần cuối cùng, INS4 là một hướng dẫn bằng ngôn ngữ tự nhiên và lập trình: bối cảnh ngôn ngữ tự nhiên làm rõ nhiệm vụ bằng cách yêu cầu "Provide a new test case to detect the fault in prior code" và bối cảnh ngôn ngữ lập trình chỉ hoạt động như một gợi ý để hướng dẫn LLMC tạo ra đầu ra. Một ví dụ được hiển thị bởi 3 trong Hình 1.

MuTAP gửi lại prompt cho LLMC và lặp lại bước tinh chỉnh trên đầu ra được tạo (Dòng 4-5 trong Thuật toán 5). Sau đó, nó thêm test case mới được tạo vào IUT mà chúng tôi gọi là Augmented Unit Test (AUT). AUT được chuyển đến bước MT (Dòng 7 trong Thuật toán 5). MuTAP đệ quy lặp lại việc tăng cường prompt cho đến khi test case cuối cùng tiêu diệt tất cả mutant (MS=100%) hoặc không có mutant sống sót nào không được sử dụng trong quá trình tăng cường (Dòng 8 trong Thuật toán 5). Một ví dụ về việc cập nhật thành phần mutant trong Hình 1, 3 được thay đổi thành 3′ bằng cách thay thế SM0 bằng SM1. 4′ chỉ ra test case được tạo với LLMC sau khi lặp lại quá trình trên mutant sống sót tiếp theo.

3.5. Tối thiểu hóa Oracle

Test case được tạo bởi LLMC thường bao gồm các assertion dư thừa. Ngoài ra, quá trình tăng cường có thể thêm nhiều assertion dư thừa hơn vào unit test cuối cùng. Việc trình bày tất cả chúng (với sự dư thừa) như đầu ra cuối cùng có thể gây nhầm lẫn cho các nhà phát triển. Trong bước cuối cùng, tương tự như các công cụ trước đây tạo ra test oracle dựa trên đột biến [18, 17], MuTAP giảm thiểu số lượng assertion bằng cách sử dụng kỹ thuật Greedy để loại bỏ các assertion dư thừa không cải thiện MS. Bước này được trình bày trong Thuật toán 6. MuTAP bắt đầu bằng cách theo dõi số lượng mutant mà mỗi assertion tiêu diệt và sau đó chọn test case chứa assertion tiêu diệt số lượng mutant tối đa. Quá trình này sau đó được lặp lại bằng cách thêm test case chứa assertion tiếp theo phát hiện nhiều mutant nhất (Dòng 4-10 trong Thuật toán 6). Nếu việc thêm assertion mới này tăng MS, MuTAP giữ test case và assertion của nó. Ngược lại, test case sẽ bị loại bỏ vì dư thừa.

4. Đánh giá

Trong phần này, chúng tôi mô tả các đánh giá mà chúng tôi đã thiết kế và thực hiện để điều tra các câu hỏi nghiên cứu sau:

RQ1: Test case được tạo bởi MuTAP hiệu quả như thế nào so với test case được tạo bởi các công cụ tạo test tự động?

RQ2: Các phần khác nhau của MuTAP hoạt động như thế nào?

RQ3: Hiệu suất của MuTAP cho từng loại đột biến là gì?

4.1. Thiết lập Thí nghiệm

Trong phần này, chúng tôi trình bày thiết lập thí nghiệm của chúng tôi. Cụ thể, chúng tôi mô tả công cụ tạo test tự động được sử dụng để so sánh kết quả, làm rõ LLMC của MuTAP và thiết lập của nó, và chỉ ra các baseline và dataset benchmark được sử dụng trong thí nghiệm.

Chúng tôi thực hiện thí nghiệm trên cluster Cedar của Compute Canada, cung cấp 32 cores CPU, 1TB lưu trữ, và một v100l GPU với 32GB GPU Memory, và trên một hệ thống chạy Linux 5.15.0-69-generic với AMD FX(tm)-6300 Six-Cores CPU, 512GB lưu trữ, và 16GB Memory.

4.1.1. Tham số Thí nghiệm

Chúng tôi gọi prompt ban đầu, zero-shot hoặc few-shot, trên LLMC lên đến 10 lần và thu thập các đầu ra đáp ứng hai tiêu chí làm test case ứng viên: ứng viên phải bao gồm hai từ khóa assert và tên hàm của PUT. Nếu sau 10 lần chạy, LLMC không thể tạo ra đầu ra chứa hai từ khóa đó, chúng tôi coi nhiệm vụ là nhiệm vụ có vấn đề hoặc nhiệm vụ mà MuTAP không thể tạo test case.

Về bước sửa cú pháp, chúng tôi chạy prompt sửa cú pháp trên LLMC lên đến 10 lần. Nếu lỗi cú pháp vẫn chưa được giải quyết ngay cả sau 10 lần lặp, MuTAP sử dụng parser Python để định vị dòng lỗi. Sau đó nó giữ lại các dòng trước dòng có lỗi, đảm bảo chúng không có lỗi cú pháp. Nếu việc loại bỏ các dòng dẫn đến việc không còn test case nào (tất cả test case đều không thể biên dịch), chúng tôi phân loại nhiệm vụ là có vấn đề.

4.1.2. Công cụ So sánh

Pynguin [32] là một công cụ tạo test tự động hoàn toàn nổi tiếng cho ngôn ngữ lập trình được gõ động như Python. Nó sử dụng các thuật toán dựa trên tìm kiếm khác nhau hướng tới việc thỏa mãn tiêu chí độ bao phủ code, tức là độ bao phủ nhánh. Pynguin đầu tiên lấy một code Python (phương thức, module, v.v.) làm đầu vào và thu thập thông tin của nó như kiểu biến, tên phương thức và dependencies. Sau đó nó sử dụng một trong các thuật toán tạo test dựa trên tìm kiếm (MIO [4], MOSA [37], DynaMOSA [38], v.v.) để tạo test case. Nó ngẫu nhiên đột biến (xóa, chèn, thay thế) các giá trị và câu lệnh khác nhau trong test case để tạo test case mới và thực thi chúng trên PUT để đảm bảo tính đúng đắn. Cuối cùng, nó tạo assertion cho test case bằng cách sử dụng engine MT [32].

Đối với thí nghiệm của chúng tôi, chúng tôi sử dụng Pynguin 0.17.0. với DynaMOSA [38]. Theo đánh giá của Pynguin [38], DynaMOSA cho thấy hiệu suất tốt nhất so với các thuật toán khác trong việc tạo test case với công cụ này. Chúng tôi đặt timeout của việc tạo test là 600 giây, đây là cài đặt mặc định của công cụ.

4.1.3. Thành phần Mô hình Ngôn ngữ Lớn (LLMC)

Chúng tôi sử dụng hai LLM khác nhau làm LLMC của MuTAP. Cái đầu tiên là Codex của OpenAI, được thiết kế đặc biệt cho các nhiệm vụ tạo code [12]. Chúng tôi sử dụng Code-davinci-002, với temperature 0.8. Temperature thấp hơn gây ra ít biến đổi hơn trong đầu ra của mô hình trong khi temperature cao hơn tăng biến đổi của đầu ra và sau đó tăng cơ hội tạo ra test case hữu ích qua các lần lặp khác nhau. Đánh giá của CODAMOSA [29] cho thấy rằng 0.8 là temperature hợp lý để tạo test case hữu ích với Codex.

LLM thứ hai là llama-2-chat của Meta, đã được tinh chỉnh lặp đi lặp lại bằng cách sử dụng Reinforcement Learning with Human Feedback (RLHF) và phù hợp cho các trường hợp sử dụng dialog [48]. Tương tự như Codex, chúng tôi đã cấu hình temperature của mô hình là 0.8. Hơn nữa, mô hình cung cấp ba vai trò riêng biệt trong prompt: system, user, và assistant. Những vai trò này phục vụ mục đích làm rõ mỗi thành phần của prompt cho mô hình bằng cách gán các thành phần cụ thể cho từng vai trò. Các kết hợp khác nhau của những vai trò này có thể được sử dụng trong mỗi prompt để điều chỉnh tương tác với mô hình theo yêu cầu cụ thể [48].

Trong thí nghiệm của chúng tôi, vai trò của system được định nghĩa là {You are a Python coding assistant. Always answer with Python code.}, cho tất cả các loại prompt, bao gồm zero-shot, few-shot, và augmented prompt. Để xử lý prompt zero-shot, chúng tôi chỉ đặt nội dung vai trò user thành sự nối của (INS1, PUTi, INS2). Đối với prompt few-shot, chúng tôi định nghĩa nội dung vai trò assistant như một tập hợp các ví dụ minh họa của Method (M) và Unit Test (UT), trong khi nội dung vai trò user được đặt thành PUTi. Đối với prompt augmented, các thành phần khác nhau của nó được thiết lập như sau:

{user: Initial Prompt,
assistant: IUT,
user: concat(INS3, SMi, INS4)}

Đối với cả hai LLM, số lượng token được tạo tối đa được đặt thành 250 để tạo test case và 20 token để sửa cú pháp, dựa trên các nghiên cứu trước về các nhiệm vụ tương tự [29, 45]. Từ dừng được định nghĩa là quote (") cho zero-shot và là </test> cho prompt few-shot. Đối với các siêu tham số còn lại, chúng tôi giữ các giá trị mặc định của mô hình.

Để tránh overfitting trên dữ liệu benchmark, MuTAP lặp lại tất cả prompt trên Codex hoặc llama-2-chat lên đến 10 lần. Nếu sau 10 lần chạy, yêu cầu tạo test case không được thỏa mãn, MuTAP coi nó là nhiệm vụ có vấn đề hoặc chưa giải quyết được.

Điều quan trọng cần lưu ý là MuTAP không giới hạn ở hai mô hình này, và LLMC của nó có thể được thay thế bằng bất kỳ LLM nào khác theo yêu cầu.

4.1.4. Baselines

Ngoài Pynguin, chúng tôi đề xuất hai baseline cho mỗi LLM để đánh giá phương pháp đề xuất của chúng tôi, MuTAP.

Before-refining: Baseline đầu tiên là đầu ra của prompt ban đầu trên LLMC (Codex hoặc llama-2-chat), mà không sửa lỗi cú pháp hoặc sửa chữa hành vi dự định. Vì assertion với giá trị trả về không mong muốn có thể thất bại trên mutant hoặc code lỗi và trình bày hiệu quả không hợp lệ, chúng tôi bỏ qua những assertion đó trong baseline này để tránh tác dụng phụ này. Nếu đầu ra của mô hình có lỗi cú pháp, chúng tôi coi nó là test sai và do đó coi nhiệm vụ là có vấn đề hoặc chưa giải quyết được.

After-refining: Baseline thứ hai là đầu ra của prompt ban đầu trên LLMC (Codex hoặc llama-2-chat), sau khi áp dụng các bước sau: Refining (Phần 3.2) và Oracle Minimization (Phần 3.5).

4.1.5. Trình tạo Mutant

Để áp dụng MT, chúng tôi cần tạo ra các phiên bản mutant khác nhau của PUT bằng cách tiêm lỗi vào các dòng khác nhau của nó. Với mục đích này, chúng tôi sử dụng MutPy phiên bản 2.0 [21]. MutPy là một công cụ MT cho code trong Python 3.3+. Nó hưởng lợi từ các toán tử đột biến khác nhau để tạo ra mutant. Danh sách các toán tử đột biến được sử dụng trong thí nghiệm của chúng tôi với các ví dụ tương ứng được hiển thị trong Bảng 1. MutPy tiêm một toán tử tại một thời điểm để tạo ra mutant nếu toán tử có thể áp dụng trên PUT.

4.1.6. Dataset Benchmark

Để tiến hành thí nghiệm, chúng tôi sử dụng hai benchmark khác nhau. Cái đầu tiên là HumanEval [12], là một benchmark để đánh giá LLMs tạo code. Nó có 164 bài toán lập trình do con người viết ở mức độ dễ đến trung bình. Mỗi bài toán có các thuộc tính khác nhau như mô tả và giải pháp tham chiếu. Chúng tôi sử dụng giải pháp tham chiếu của mỗi nhiệm vụ như một PUT.

Cái thứ hai, Refactory [23], là một benchmark cho việc sửa chữa lỗi Python [24]. Nó có 1710 bài nộp lỗi của sinh viên cho 5 bài tập của một khóa học lập trình Python. Mỗi bài tập có một giải pháp tham chiếu đúng mà chúng tôi sử dụng như PUT. Lợi thế của dataset này là các đoạn code lỗi được tạo bởi con người mang đến cho chúng tôi cơ hội đánh giá test case được tạo bởi MuTAP trên lỗi thực và so sánh nó với Pynguin và baseline của chúng tôi.

4.2. Kết quả Thí nghiệm

Trong phần này, chúng tôi thảo luận về những phát hiện của chúng tôi cho mỗi RQ.

4.2.1. RQ1: Test case được tạo bởi MuTAP hiệu quả như thế nào so với test case được tạo bởi các công cụ tạo test tự động?

Vì nghiên cứu của chúng tôi tập trung vào MT để cải thiện hiệu quả của test case, chúng tôi so sánh MuTAP với Pynguin và baseline của chúng tôi về MS, số lượng mutant bị tiêu diệt, và số lượng PUT với 100% MS. Đáng chú ý là chúng tôi chỉ xem xét PUT với test case đúng để tính toán MS trung bình cho mỗi phương pháp. Vì lý do này, chúng tôi báo cáo tổng số mutant bị tiêu diệt và tổng số PUT với 100% MS để so sánh công bằng.

Bảng 2 cho thấy kết quả thu được cho benchmark HumanEval. Trước khi sửa cú pháp và sửa chữa hành vi dự định (before-refining), test case được tạo bởi Codex và llama-2-chat không đúng cho 73 và 68 (trong số 164) PUT, tương ứng, khi sử dụng prompt ban đầu zero-shot. Tuy nhiên, chúng thành công tiêu diệt 295 và 318 mutant (trong số 1260), tương ứng.

Prompt ban đầu có tác động rõ rệt hơn đến đầu ra của Codex so với llama-2-chat. Chuyển prompt ban đầu sang few-shot giảm số lượng PUT không có test case xuống 39, đồng thời tăng số lượng mutant bị tiêu diệt lên 508 khi sử dụng Codex làm LLMC. Mặt khác, khi sử dụng llama-2-chat, số lượng PUT không có test case giảm xuống 60, và số lượng mutant bị tiêu diệt tăng từ 318 lên 325. Sự khác biệt về hiệu suất này có thể được quy cho việc llama-2-chat phù hợp hơn cho prompt dialog, và việc sử dụng prompt với cặp đầu vào và đầu ra minh họa, thiếu bối cảnh ngôn ngữ tự nhiên, không cải thiện hiệu suất của mô hình đáng kể.

Ngược lại, Pynguin, như công cụ tạo test tự động tiên tiến, vượt trội hơn đầu ra của cả hai LLM, before-refining, bằng cách tiêu diệt 649 mutant và thất bại trong việc tạo test case cho 31 nhiệm vụ.

Sau khi áp dụng các bước hậu xử lý của sửa cú pháp và sửa chữa hành vi dự định, MuTAP với cả hai LLM hoạt động tốt hơn Pynguin về việc tiêu diệt nhiều mutant hơn. Đáng chú ý, khi sử dụng cả prompt zero-shot và few-shot, llama-2-chat có thể tạo test case đúng cho tất cả PUT, after-refining. Tuy nhiên, hiệu quả của chúng về việc tiêu diệt mutant được đo lường là 84.04% và 85.16% với prompt zero-shot và few-shot, tương ứng.

Mặt khác, MS của test case được tạo bởi Codex sau khi tinh chỉnh là 76.82% và 82.73% với prompt zero-shot và few-shot, tương ứng. Mặc dù có sự cải thiện này, Codex vẫn thất bại trong việc tạo test case đúng cho 30 (với zero-shot) và 27 (với few-shot) PUT sau khi tinh chỉnh.

MuTAP, tăng cường hiệu quả của test case được tạo bởi cả hai LLM, Codex và llama-2-chat, đạt được MS là 89.13% và 91.98% với prompt zero-shot, và MS là 92.02% và 93.57% với prompt few-shot, tương ứng. Đặc biệt, MuTAP với prompt few-shot khi sử dụng llama-2-chat làm LLMC thành công tiêu diệt 1179 mutant trong số 1260 và tạo test case với MS=100% cho tới 70% PUT, thể hiện sự cải thiện đáng kể trong hiệu quả của test case so với Pynguin với 649 mutant bị tiêu diệt và 28.22% PUT với MS=100%.

Bảng 3 cho thấy kết quả trên các chương trình lỗi thực của con người từ benchmark Refactory xác nhận những phát hiện của chúng tôi trên HumanEval. Để đánh giá MuTAP trên code lỗi thực, chúng tôi áp dụng các bước sau. Đầu tiên, chúng tôi tạo ra mutant của mỗi PUT trong dataset này. Thứ hai, chúng tôi tiến hành quá trình tăng cường prompt và hoàn thiện test case cho mỗi PUT. Sau đó, chúng tôi áp dụng test case được tạo bởi MuTAP trên code lỗi của sinh viên trong Refactory, theo sau là test case được tạo bởi Pynguin và LLM After-refining, để đánh giá hiệu quả của test case được tạo bởi các phương pháp khác nhau trong việc phát hiện code lỗi.

MuTAP với học few-shot trong khi sử dụng llama-2-chat làm LLMC xác định 468 code lỗi nhiều hơn so với Pynguin (với MS là 94.91% vs. 67.54%) và 111 code lỗi nhiều hơn so với After-refining (với MS là 94.91% vs. 82.51%). Hơn nữa, MuTAP phát hiện 79 code lỗi không được phát hiện bởi cả Pynguin hoặc test case After-refining của llama-2-chat. Khi sử dụng Codex, MuTAP phát hiện 73 code lỗi bị bỏ lỡ bởi cả Pynguin và test case After-refining của Codex. Hơn nữa, MuTAP vượt trội trong việc tạo test case hiệu quả hơn, với trung bình 2.6 test case sau khi áp dụng tối ưu hóa greedy.

Nhìn chung, MuTAP sử dụng cả llama-2-chat và Codex thể hiện hiệu suất tốt hơn so với Pynguin về việc tiêu diệt mutant và phát hiện code lỗi. Hiệu quả của những test case này trong việc phát hiện khiếm khuyết được cải thiện thông qua các bước hậu xử lý của tinh chỉnh và tăng cường prompt.

Phát hiện 1: MuTAP tạo ra test case hiệu quả hơn so với Pynguin và học zero-shot và few-shot thông thường trên LLM. Số lượng test case của MuTAP không lớn hơn nhiều so với đầu ra của các phương pháp khác sau khi tối thiểu hóa. Ngoài ra, LLM với thiết lập dialog hoạt động tốt hơn trên prompt augmented. Kết luận, hiệu quả của test case được tạo bởi LLM có thể được tăng cường thông qua việc tăng cường prompt bằng cách sử dụng mutant sống sót và tinh chỉnh hậu xử lý.

4.2.2. RQ2: Các phần khác nhau của MuTAP hoạt động như thế nào?

Syntax Fixer: Trung bình, tỷ lệ test case có lỗi cú pháp là 38.98% và 26.48% khi sử dụng prompt zero-shot và few-shot, tương ứng, với Codex. Khi sử dụng llama-2-chat, tỷ lệ này là 33.85% và 26.32% với prompt zero-shot và few-shot, tương ứng.

Khi xem xét lỗi cú pháp, ba yếu tố góp phần giảm chúng trong đầu ra của LLM. Yếu tố đầu tiên là loại prompt ban đầu. Như được hiển thị trong Bảng 4 trên benchmark HumanEval, học few-shot dẫn đến ít lỗi cú pháp hơn trong đầu ra của cả hai LLM. Cụ thể, khi sử dụng Codex, tỷ lệ lỗi cú pháp giảm từ 44.79% xuống 29.03% sau khi tinh chỉnh, và đối với MuTAP, nó giảm từ 33.17% xuống 23.93%. Với llama-2-chat làm LLMC, tỷ lệ lỗi cú pháp giảm từ 38.03% xuống 26.99% sau khi tinh chỉnh, và từ 29.66% xuống 25.64% đối với MuTAP.

Yếu tố có tác động thứ hai, cũng là yếu tố chính, là thành phần Syntax Fixing. Như được hiển thị trong Bảng 4, khi sử dụng Codex, thành phần này trong MuTAP trung bình sửa 14.5% lỗi cú pháp bằng cách sử dụng LLMC và giải quyết 81.37% lỗi cú pháp bằng cách bỏ qua các dòng gây ra lỗi. Mặt khác, khi sử dụng llama-2-chat làm LLMC của MuTAP, thành phần Syntax Fixing, trung bình, giải quyết 32.31% lỗi cú pháp thông qua việc gửi lại prompt cho LLMC, và 60.73% lỗi bằng cách bỏ qua các dòng có vấn đề.

Yếu tố cuối cùng góp phần cải thiện lỗi cú pháp trong test case là quá trình tăng cường prompt trong MuTAP. Bằng cách tăng cường prompt với IUT, việc xuất hiện lỗi cú pháp trong đầu ra của Codex với kỹ thuật zero-shot giảm từ 44.79% xuống 33.17%. Tương tự, với llama-2-chat và prompt zero-shot, tỷ lệ lỗi cú pháp giảm từ 38.03% xuống 29.66%. Việc tăng cường prompt với IUT cung cấp các ví dụ minh họa về test case và phục vụ mục đích tương tự như các ví dụ minh họa trong prompt học few-shot, hiệu quả giảm lỗi cú pháp trong đầu ra của LLM.

Phát hiện của chúng tôi trên benchmark Refactory cho thấy MuTAP tạo test case với lỗi cú pháp chỉ trong một PUT (trong số 5) sử dụng Codex và học zero-shot. Hơn nữa, không có lỗi cú pháp nào trong số đó có thể được sửa bằng cách gửi lại prompt cho LLMC. Mặt khác, đối với cả hai loại prompt ban đầu, lỗi cú pháp giảm xuống không sử dụng llama-2-chat.

Intended Behavior Repair: Trong trường hợp sửa chữa hành vi dự định, hai yếu tố riêng biệt góp phần giảm tỷ lệ lỗi trong assertion oracle. Như được hiển thị trong Bảng 5, bước Intended Behavior Repair, khi sử dụng Codex làm LLMC, trung bình, sửa 83.98% và 89.86% hành vi không đúng trong after-refining và MuTAP, tương ứng. Khi sử dụng llama-2-chat, bước này sửa chữa 84.35% và 95.96% hành vi không mong muốn trong after-refining và MuTAP, tương ứng.

Ngoài bước Intended Behavior Repair, bước tăng cường prompt trong MuTAP đáng kể giảm việc xuất hiện hành vi không mong muốn trong test case. Ví dụ, khi sử dụng Codex với prompt zero-shot, assertion với hành vi không mong muốn, như giá trị trả về sai, giảm từ 63.63% xuống 19.38%. Tương tự, với llama-2-chat và sử dụng prompt few-shot, assertion với hành vi không mong muốn giảm từ 63.25% xuống 10.75%. Lý do đằng sau sự cải thiện này có thể được quy cho việc sử dụng IUT (Initial Unit Tests) trong MuTAP để tăng cường prompt ban đầu. Những IUT này đã thể hiện hành vi dự định của PUT, qua đó hỗ trợ LLM đề xuất test case với ít hành vi không mong muốn hơn (tức là ít giá trị trả về sai hơn). Ngoài ra, trên benchmark Refactory, MuTAP đã sửa chữa tất cả assertion với hành vi không đúng trên đầu ra của prompt augmented.

Không giống như lỗi cú pháp, loại prompt không giúp đáng kể với hành vi không mong muốn trong assertion. Sự kết hợp của bước Intended Behavior Repair và quá trình tăng cường prompt cải thiện hiệu quả của test case, đảm bảo rằng chúng phù hợp với hành vi dự định của PUT.

Surviving Mutants Representation: Chúng tôi cũng điều tra tác động của thứ tự mutant sống sót đối với MS trong quá trình tăng cường prompt. Hình 3 minh họa tác động của việc tăng cường prompt với thứ tự ngẫu nhiên của mutant sống sót qua 5 lần chạy cho tất cả PUT. Để so sánh này, chúng tôi ngẫu nhiên chọn một trong các mutant sống sót của mỗi PUT với MS<100% và sử dụng nó để tăng cường prompt ban đầu. Sau đó chúng tôi tính toán MS trung bình cho tất cả PUT. Tiếp theo, chúng tôi ngẫu nhiên chọn mutant sống sót thứ hai cho PUT còn lại với MS<100% (nếu có), lặp lại quá trình tăng cường, và tính toán MS trung bình cho tất cả PUT một lần nữa. Chúng tôi tiếp tục lặp lại quá trình này cho đến khi không còn PUT nào với MS<100% hoặc không còn mutant sống sót nào chưa được sử dụng trong quá trình lập luận.

Như được hiển thị trong Hình 3, mỗi điểm dữ liệu thể hiện MS trung bình cho tất cả PUT qua 5 lần chạy của việc lựa chọn ngẫu nhiên mutant sống sót. Đáng chú ý, hơn 90% MS được đạt được bằng cách chỉ sử dụng một nửa mutant sống sót, và sự cải thiện trong MS dừng lại sau một số lần lặp nhất định của bước tăng cường trong các LLM khác nhau. Ví dụ, khi sử dụng Codex làm LLMC, trong học zero-shot, MS ngừng cải thiện mặc dù, trung bình, 27 mutant sống sót (trong số 226) không được sử dụng trong bước tăng cường prompt. Tương tự, trong học few-shot, số này bằng 24 (trong số 106).

Kết quả của chúng tôi cho RQ2 chứng minh rằng test case được tạo bởi LLM, bất kể loại prompt, cần hậu xử lý, như sửa cú pháp hoặc sửa chữa hành vi dự định, để hoạt động đúng cách và phát hiện lỗi hiệu quả. Ngoài ra, thứ tự của mutant sống sót để tăng cường prompt không tác động đáng kể đến việc tăng MS.

Phát hiện 2: Syntax Fixing và Intended Behavior Repair sửa tới 95.94% và 89.86% lỗi cú pháp và chức năng trong test case, tương ứng. Việc tăng cường prompt trong MuTAP giảm đáng kể hành vi không mong muốn trong đầu ra của LLM (44.36% sử dụng Codex và 52.5% sử dụng llama-2-chat). Hơn nữa, chỉ một số lượng nhỏ mutant (tối đa 27) không đóng góp vào việc cải thiện MS.

4.2.3. RQ3: Hiệu suất của MuTAP đối với từng loại đột biến là gì?

Trong RQ này, chúng tôi đánh giá hiệu suất của MuTAP trong các loại mutant khác nhau. Chúng tôi báo cáo tổng số và số lượng mutant bị tiêu diệt bởi mỗi phương pháp trên benchmark HumanEval trong Bảng 6. Hiệu suất của tất cả kỹ thuật cho mỗi loại mutant được báo cáo để giúp so sánh. Tổng số mutant trong mỗi loại khác nhau cho mỗi phương pháp vì số lượng PUT có vấn đề không giống nhau cho tất cả phương pháp. MS cho mỗi loại/phương pháp chỉ ra tỷ lệ mutant bị tiêu diệt trong tổng số mutant trong loại đó. Có một số loại mutant phổ biến hơn (nhiều mẫu hơn trong những loại đó) như AOR, COI, và ROR (một ví dụ cho mỗi loại mutant được hiển thị trong Bảng 1). Số lượng mutant trong mỗi loại phụ thuộc vào PUT. Ví dụ, trong HumanEval, có ít PUT với xử lý ngoại lệ. Do đó, có ít mutant trong EHD.

Nói chung, MuTAP cho thấy hiệu suất tốt hơn hoặc tương tự trong tất cả các loại mutant so với Pynguin và đầu ra của LLM After-refining của cả hai LLM. Xem xét ASR như một ví dụ, MuTAP cho thấy hiệu suất cao nhất trên loại mutant này trong tất cả các phương pháp. Ví dụ, test case được tạo bởi Pynguin xác định 45 mutant trong danh mục này trong khi test case được tạo bởi MuTAP sử dụng llama-2-chat và prompt few-shot xác định 79 mutant trong danh mục này (trong số 84).

Đối với một trong các loại mutant, BCR, là loại hiếm trong benchmark của chúng tôi, MuTAP và After-refining với cả prompt ban đầu zero-shot và few-shot, cùng với việc sử dụng Codex, cho thấy hiệu suất giống nhau. Tuy nhiên, khi sử dụng llama-2-chat, MuTAP vượt trội hơn những cái khác bằng cách tiêu diệt nhiều mutant của loại này hơn. Đối với một loại mutant hiếm khác trong dataset của chúng tôi, EHD, đáng chú ý là Codex, mặc dù sử dụng cả hai loại prompt ban đầu và quá trình tăng cường, thất bại trong việc tạo test case để phát hiện hai mutant hiện diện trong danh mục này. Ngược lại, MuTAP với prompt ban đầu few-shot và llama-2-chat thành công tiêu diệt tất cả mutant trong danh mục này.

Phát hiện 3: Test case được tạo bởi MuTAP có hiệu quả bằng hoặc tốt hơn trong việc tiêu diệt các loại mutant khác nhau so với những test case được tạo bởi Pynguin và phương pháp baseline. Ngoài ra, việc sử dụng LLM với thiết lập dialog có thể tăng số lượng tiêu diệt mutant trong các loại mutant khác nhau trong khi áp dụng tăng cường prompt.

5. Thảo luận

5.1. Tạo Test Case Tự động

MuTAP tận dụng khả năng tổng hợp code của LLM và sử dụng học dựa trên prompt để hỗ trợ các nhà phát triển tạo test case hiệu quả mà không cần tinh chỉnh LLM tốn kém về mặt tính toán.

LLM có thể tạo test case hiệu quả hơn những test case được tạo bởi Pynguin về việc phát hiện lỗi. Listing 1 cho thấy một test case mẫu được tạo bởi Pynguin cho PUT của ví dụ động lực trong Phần 2. Trong khi Pynguin tạo đầu vào test như số nguyên ngẫu nhiên và đột biến những giá trị đó để tạo test case mới, LLM tạo ra test case trông tự nhiên hơn và có tương quan với kiểu đầu vào/đầu ra và chức năng của PUT. Tuy nhiên, test case được tạo bởi LLM cần hậu xử lý để trở nên hiệu quả hơn trong việc phát hiện lỗi. Kết quả của chúng tôi cho thấy việc tăng cường prompt với mutant sống sót và tinh chỉnh test case (cú pháp và hành vi dự định) giúp LLM tạo test case hiệu quả hơn về phát hiện lỗi.

Các nhà phát triển có thể sử dụng MuTAP để tạo test case hiệu quả về phát hiện lỗi, với sự hỗ trợ của LLM. Ngoài ra, MuTAP có thể được tích hợp vào thành phần tạo test của GitHub Copilot lab [2] để đề xuất test case hiệu quả hơn cho các nhà phát triển. Vì mutant có thể được tạo tự động, việc tăng cường prompt có thể được áp dụng mà không cần sự tham gia của con người.

5.2. Thời gian Thực thi

API mở của Codex có giới hạn về số lượng yêu cầu (20 mỗi phút) và số lượng token (40,000 mỗi phút). Vì lý do này, thí nghiệm của chúng tôi cần dừng việc gọi API thỉnh thoảng để không vượt quá giới hạn. Kết quả là, chúng tôi trình bày phân tích thời gian xử lý sử dụng llama-2-chat. Thời gian xử lý tổng thể của MuTAP trên dataset HumanEval trong khi sử dụng llama-2-chat là trung bình 39.75 giây với học zero-shot (với tối thiểu 16.16 và tối đa 56.66 giây) và 42.11 giây với prompt few-shot (với tối thiểu 18.2 và tối đa 64.2 giây) cho mỗi nhiệm vụ. Nó bao gồm trung bình xây dựng và gọi prompt ban đầu trên LLMC với trung bình 10.26 giây, sửa cú pháp bao gồm gọi prompt sửa cú pháp trên LLMC với 10.3 giây, sửa chữa hành vi dự định ở 0.38 giây, tính toán MS ở 1.7 giây, tạo prompt augmented và gọi chúng trên LLM với 12.05 giây và tối ưu hóa greedy với 1.4 giây. Đáng chú ý là sau bước tăng cường prompt, MuTAP phải lặp lại các quá trình sửa cú pháp, sửa chữa hành vi dự định, và các bước greedy đã được bao gồm trong thời gian xử lý tổng thể. Trong tất cả các bước của MuTAP, những bước tốn thời gian nhất là những bước đòi hỏi suy luận đầu ra từ LLM. Ngược lại, thời gian xử lý tổng thể trên cùng benchmark với Pynguin để hoàn thành việc tìm kiếm không gian cần thiết là trung bình 44.16 giây với tối thiểu 2.7 và tối đa 10 phút là timeout mặc định của công cụ.

5.3. Lợi ích của dialog LLM

Những phát hiện của chúng tôi chỉ ra rằng thiết lập dialog của llama-2-chat cung cấp cho MuTAP sự linh hoạt để gán các vai trò riêng biệt cho mỗi thành phần của prompt augmented. Ví dụ, bằng cách gán IUT cho vai trò assistant trong quá trình tăng cường prompt, khả năng lặp lại test ban đầu trong đầu ra được tạo ra được giảm, trong khi cơ hội tạo test case mới để phát hiện mutant sống sót được tăng lên. Listing 2 minh họa một ví dụ về cách llama-2-chat hiệu quả tổng hợp sự khác biệt của PUT và một trong các mutant sống sót của nó, giải thích sự khác biệt, và sau đó tạo test case mới để phát hiện lỗi.

5.4. Metrics Đánh giá

Các nghiên cứu trước [50, 35, 14] liên quan đến việc tạo assertion thông qua LLM đã sử dụng "exact match" như một trong các metric đánh giá của họ. Exact match tính toán tỷ lệ test case được tạo bởi LLM (đầu ra suy luận) khớp từ vựng với test case ground truth (đầu ra mong đợi). Tuy nhiên, CIDAR [35] đã thảo luận về sự không phù hợp của exact match như một metric phù hợp để đánh giá assertion được tạo bởi LLM. Lý do này là mô hình thường tạo assertion đúng về mặt ngữ nghĩa nhưng có thể không khớp chính xác với ground truth. Trong nghiên cứu của chúng tôi, MuTAP thực thi mỗi test case bao gồm assertion, cả trên PUT và trên mutant của nó để đánh giá tính đúng đắn và hiệu quả của chúng, báo cáo MS của chúng. MS là một metric được sử dụng thường xuyên trong các nghiên cứu trước và nó phục vụ như một metric hiệu quả để đánh giá chất lượng của test oracle [51]. Trong khi, trong bài báo này, chúng tôi tập trung vào việc cải thiện hiệu quả của test case về phát hiện lỗi, có các metric khác như độ bao phủ test có thể đánh giá các khía cạnh chất lượng khác của test case. Cải thiện MS không nhất thiết dẫn đến độ bao phủ tốt và độ bao phủ test có mối tương quan yếu với hiệu quả của test trong phát hiện lỗi [10] và được thách thức như một thước đo hiệu quả test trong việc phát hiện lỗi [20, 22], có thể khiến phương pháp đề xuất của chúng tôi khó hoạt động tốt trên cả hai metric [40, 18].

Hơn nữa, kết quả được trình bày trong [46] chỉ ra rằng khoảng 60% test case được tạo bởi Codex gặp vấn đề biên dịch do lỗi cú pháp. Việc kết hợp các bước sửa cú pháp và sửa chữa hành vi dự định trong phương pháp đề xuất của chúng tôi, MuTAP, đáng kể tăng cường tính hữu dụng của test được tạo bởi LLM.

5.5. Mutant sống sót

Chúng tôi tăng cường prompt ở mỗi lần lặp cho mỗi PUT với một mutant sống sót duy nhất. Số lượng mutant trung bình cho tất cả PUT trong HumanEval và Refactory là 6.6 và 4.2 và số lượng mutant sống sót trung bình là 3.6 và 1.8, tương ứng. Sử dụng một kết hợp của mutant sống sót để tăng cường prompt có thể tác động đến tốc độ đạt 100% MS. Tuy nhiên, không phải tất cả mutant sống sót được sử dụng trong việc tăng cường prompt đều đóng góp vào việc cải thiện MS, đôi khi test case mới giải quyết một mutant cũng có thể tiêu diệt các mutant sống sót còn lại.

6. Mối đe dọa đối với Tính hợp lệ

Tính hợp lệ nội bộ.

Trong nghiên cứu này, chúng tôi sử dụng hai kỹ thuật học dựa trên prompt khác nhau: zero-shot và few-shot. Tuy nhiên, chúng tôi không khám phá tác động tiềm tàng của việc thay đổi hướng dẫn ngôn ngữ tự nhiên hoặc ví dụ minh họa (cho học few-shot) trong prompt của chúng tôi. Sửa đổi những hướng dẫn này hoặc sử dụng các ví dụ minh họa khác nhau phù hợp hơn với chức năng của PUT có thể tăng cường kết quả. Như được chứng minh bởi kết quả của chúng tôi trong RQ2, việc bao gồm IUT trong prompt trong các bước tăng cường đã giảm các trường hợp hành vi không mong muốn trong test oracle. Ngược lại, việc sử dụng, ví dụ, hướng dẫn ngôn ngữ tự nhiên dài dòng có thể có tác động tiêu cực đến kết quả.

Để sửa lỗi cú pháp trong test case thông qua việc gửi lại prompt cho LLMC, chúng tôi đã sử dụng phương pháp được trình bày trong [52]. Chúng tôi không tích hợp thông tin bổ sung về lỗi cú pháp như thông báo lỗi hoặc dòng lỗi vào prompt. Đáng xem xét rằng việc kết hợp thông tin bổ sung về lỗi cú pháp có thể tăng cường hiệu suất của LLMC để sửa chữa những lỗi cú pháp này.

Ngoài ra, chúng tôi thừa nhận rằng thuật toán greedy được sử dụng trong phương pháp của chúng tôi để giảm thiểu số lượng test oracle có thể không phải là giải pháp tối ưu nhất để giảm thiểu test oracle trong khi tối đa hóa MS. Tuy nhiên, các nghiên cứu trước [18, 17] sử dụng cùng phương pháp để giảm thiểu số lượng assertion đã chứng minh hiệu quả của nó trong việc giảm số lượng test oracle trong test case, cùng với sự dễ dàng triển khai.

Cuối cùng, trong số các loại assertion khác nhau, chúng tôi chỉ tập trung vào việc tạo những cái nguyên thủy trong nghiên cứu này. Các loại assertion khác có thể được khám phá trong các nghiên cứu tương lai.

Chúng tôi sử dụng các khái niệm về khả năng tiêu diệt mutant và phát hiện lỗi như metric để đánh giá hiệu quả của test case, xem xét rằng mục tiêu chính của việc kiểm thử là phát hiện lỗi. Độ bao phủ đã được sử dụng trong nhiều nghiên cứu khác để đánh giá chất lượng test case [41, 29]. Tuy nhiên, đã được chứng minh rằng mặc dù có mối tương quan giữa độ bao phủ và phát hiện lỗi, chúng có thể không nhất quán trong việc xếp hạng các chiến lược kiểm thử khác nhau, như đã quan sát trong lĩnh vực kiểm thử fuzz [8].

Tính hợp lệ cấu trúc. Chúng tôi sử dụng khái niệm tiêu diệt mutant và phát hiện lỗi như metric để đánh giá hiệu quả của test case, xem xét rằng mục tiêu chính của việc kiểm thử là phát hiện lỗi. Độ bao phủ đã được sử dụng trong nhiều nghiên cứu khác để đánh giá chất lượng test case [41, 29]. Đã được chỉ ra rằng mặc dù có mối tương quan giữa độ bao phủ và tìm lỗi, chúng không đồng ý về việc xếp hạng các tester khác nhau, như trong không gian kiểm thử fuzz [8].

Điều quan trọng cần lưu ý là các lỗi có trong mutant là nhân tạo và có thể không tương ứng trực tiếp với lỗi trong thế giới thực. Để giải quyết mối quan tâm này, chúng tôi đã sử dụng dataset Refactory [23], một benchmark sửa chữa lỗi chứa các chương trình lỗi thực được phát triển bởi sinh viên.

Tính hợp lệ bên ngoài. Đối với thí nghiệm của chúng tôi, chúng tôi sử dụng hai dataset chứa các nhiệm vụ lập trình Python, có thể tạo ra thách thức bên ngoài đối với tính hợp lệ của những phát hiện của chúng tôi. Yêu cầu về chương trình Python có thể thực thi là cần thiết để chạy test được tạo ra chống lại cả phiên bản chính xác và lỗi (thực hoặc đột biến) của PUT và cân nhắc này đã hướng dẫn lựa chọn dataset của chúng tôi. Tuy nhiên, vì chúng tôi không đưa ra giả định cụ thể nào khi chọn dataset, kết quả của chúng tôi có thể được mở rộng cho các chương trình Python khác.

Cuối cùng, cần thừa nhận rằng kỹ thuật được đề xuất và các đánh giá được thực hiện trong bài báo này về mặt khái niệm có thể thích ứng với các ngôn ngữ ngoài Python. Tuy nhiên, việc triển khai hiện tại của MuTAP được điều chỉnh cho các chương trình Python, có nghĩa là kết quả hiện tại của chúng tôi không thể được mở rộng để bao gồm các ngôn ngữ lập trình khác.

Tính hợp lệ về độ tin cậy. Với mục đích cho phép các nhà nghiên cứu khác tái tạo hoặc mở rộng nghiên cứu của chúng tôi, chúng tôi cung cấp gói tái tạo [3]. Tuy nhiên, việc cải tiến liên tục của LLM có thể tạo ra thách thức trong việc đạt được sự tái tạo chính xác kết quả của chúng tôi.

7. Công trình liên quan

Các tác giả trong [7] nghiên cứu tác động của học few-shot qua nhiều nhiệm vụ downstream khác nhau, bao gồm việc tạo test case và test oracle. Họ so sánh hiệu suất của học few-shot với các công cụ tạo test tự động. Điều tra được thực hiện trên một tập hợp khác nhau của các phương thức Java từ các benchmark khác nhau. Kết quả chỉ ra rằng LLM có khả năng tạo test case và test oracle khớp chính xác (về mặt từ vựng) với test ground truth trong các dự án benchmark. Hơn nữa, độ bao phủ test của chúng được thấy là có thể so sánh với test case được tạo bởi các công cụ tạo test tự động.

Schäfer et al. [41] thực hiện một nỗ lực tạo test case bằng cách prompting Codex. Điều tra của họ tập trung vào 25 package JavaScript. Prompt trong nghiên cứu của họ bao gồm việc triển khai PUT và cũng các ví dụ sử dụng API được trích xuất từ tài liệu. Trong những trường hợp mà test case không thành công trên PUT, phương pháp của họ kết hợp thông báo lỗi gặp phải vào prompt và gửi lại prompt cho Codex. Những phát hiện của họ chứng minh rằng quá trình tăng cường prompt với thông tin bổ sung như vậy giúp Codex tạo ra test case đúng với độ bao phủ đầy đủ.

LIBRO [27] sử dụng báo cáo vấn đề (cả tiêu đề và nội dung) như prompt few-shot để tạo test case tái tạo lỗi. Test case cuối cùng được kết hợp vào các lớp test phù hợp và được xếp hạng dựa trên tính hợp lệ của chúng. Kết quả cho thấy sự cải thiện trong việc tạo test case đúng để tái tạo lỗi so với các công cụ tiên tiến.

CEDAR [35], thay vì sử dụng ví dụ minh họa cố định trong học few-shot, nhằm truy xuất ví dụ minh họa liên quan đến mỗi PUT và kết hợp chúng vào prompt. Họ đánh giá phương pháp của họ dựa trên khớp từ vựng, được gọi là "exact match", giữa assertion được tạo và ground truth trong một benchmark. Trong khi phương pháp đề xuất của họ thể hiện hiệu suất tăng cường trong việc đạt được exact match giữa assertion và ground truth, nó cần một pull mở rộng của các mẫu code để lựa chọn ví dụ minh họa phù hợp cho mỗi PUT.

ATHENATEST [49] sử dụng mô hình transformer BART [30], mà họ tinh chỉnh bằng cách sử dụng một bộ sưu tập các hàm Java và test tương ứng. Họ báo cáo độ bao phủ test có thể so sánh với EvoSuite [17] khi đánh giá việc tạo test case cho năm dự án Java.

TOGA [14] tham gia vào việc tinh chỉnh CodeBERT bằng cách sử dụng docstring của PUT cùng với tiền tố của test case có assertion được che. Mục tiêu của họ là tổng hợp assertion. Sau đó, họ xây dựng toàn bộ test oracle bằng cách kết hợp ngữ pháp test oracle và tạo ra một tập hợp assertion. Tập hợp này sau đó được xếp hạng thông qua một neural network ranker dựa trên khớp từ vựng của chúng với test oracle ground truth. Mặc dù họ báo cáo kết quả tương tự như EvoSuite [17] trong phát hiện lỗi, trọng tâm của họ chỉ là tổng hợp assertion. Tuy nhiên, tổng hợp assertion không phải là thách thức nhưng việc tạo test oracle hiệu quả và có ý nghĩa đặt ra một thách thức đáng kể.

CODAMOSA kết hợp test case được tạo bởi Codex với những test case được tạo từ Pynguin trong những trường hợp việc tạo test case của Pynguin dừng lại và thất bại trong việc tăng cường độ bao phủ test. CODAMOSA đạt được độ bao phủ test cao hơn trên nhiều benchmark Python khác nhau [29] so với Pynguin. Đáng chú ý là, tương tự như các nghiên cứu khác, CODAMOSA chỉ tập trung vào cải thiện độ bao phủ test, và test case được tạo của nó thiếu assertion oracle để phát hiện lỗi trong chương trình.

Hai nghiên cứu bổ sung sử dụng Codex để đồng thời tạo code và test case tương ứng dựa trên mô tả vấn đề đã cho. Sau đó, họ sử dụng những test case này để lọc ra các đề xuất lỗi được tạo bởi Codex [28, 11]. Để tạo code, họ sử dụng mô tả vấn đề như một prompt, và để tạo test case, họ sử dụng cùng mô tả vấn đề cùng với PUT và một hướng dẫn ngôn ngữ tự nhiên.

Mặc dù nghiên cứu trước đã khám phá các chiến lược đa dạng để tạo test case bằng cách sử dụng LLM như Codex và đánh giá chúng về độ bao phủ test hoặc khớp từ vựng với test ground truth, không có nghiên cứu nào trong số này đặc biệt tập trung vào việc tận dụng MT để tăng cường hiệu quả của test case được tạo ra.

8. Kết luận

Trong bài báo này, chúng tôi đề xuất MuTAP như một phương tiện cải thiện và đánh giá khả năng của LLM được tiền huấn luyện để tạo test case hiệu quả. MuTAP đầu tiên prompts LLMC của nó để tạo test case bằng cách sử dụng học zero-shot và few-shot. Sau khi xác định và sửa chữa bất kỳ lỗi cú pháp và giá trị trả về tiềm ẩn nào trong test case được tạo, MuTAP đánh giá hiệu quả của chúng bằng cách thực hiện MT. Sau đó, nó sử dụng các mutant sống sót của mỗi PUT, nếu có, cũng như test case ban đầu không đầy đủ để tăng cường prompt ban đầu. Nó gửi lại prompt cho LLMC bằng cách sử dụng prompt được tăng cường để tái tạo test case mới có khả năng phát hiện mutant sống sót.

Chúng tôi đánh giá hiệu quả của test case được tạo bởi LLM để xác định lỗi trong chương trình lỗi thực và tổng hợp. Trung bình, test case được tạo bởi MuTAP thành công xác định 86.72% code lỗi trong một benchmark sửa chữa lỗi khi sử dụng LLM được thiết kế cho việc tạo code, Codex. Khi sử dụng LLM với thiết lập dialog, llama-2-chat, MuTAP cải thiện hơn nữa hiệu suất của nó, phát hiện 94.06% code lỗi, vượt trội hơn cả công cụ tạo test tự động và kỹ thuật học zero-shot và few-shot trên LLM. Điều này nhấn mạnh lợi thế của việc sử dụng LLM làm lõi của công cụ tạo test tự động, vì các công cụ tạo thông thường như Pynguin thiếu quyền truy cập vào những thông tin chứa đựng trong mutant sống sót.

Mặc dù phiên bản hiện tại của MuTAP sử dụng hai LLM khác nhau để tạo test case cho các chương trình Python, thiết kế và phương pháp đánh giá của nó về cơ bản có thể thích ứng với nhiều ngôn ngữ lập trình và mô hình khác nhau. Do đó, như công việc tương lai, nó có thể dễ dàng được mở rộng để bao gồm các ngôn ngữ lập trình khác hoặc kết hợp LLM mới.
