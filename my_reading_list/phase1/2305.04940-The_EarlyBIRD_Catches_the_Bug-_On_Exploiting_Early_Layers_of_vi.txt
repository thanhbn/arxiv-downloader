# 2305.04940.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: ./2305.04940.pdf
# Kích thước tệp: 1334069 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
EarlyBIRD Bắt Được Lỗi: Về Việc Khai Thác Các Lớp Đầu của
Mô Hình Encoder để Phân Loại Mã Hiệu Quả Hơn
Anastasiia Grishina
anastasiia@simula.no
Phòng Thí Nghiệm Nghiên Cứu Simula
Oslo, Na UyMax Hort
maxh@simula.no
Phòng Thí Nghiệm Nghiên Cứu Simula
Oslo, Na UyLeon Moonen
leon.moonen@computer.org
Phòng Thí Nghiệm Nghiên Cứu Simula &
Trường Kinh Doanh BI Na Uy
Oslo, Na Uy

TÓM TẮT
Việc sử dụng các kỹ thuật Xử Lý Ngôn Ngữ Tự Nhiên (NLP) hiện đại đã
chứng minh được lợi ích cho các nhiệm vụ kỹ thuật phần mềm, như
phát hiện lỗ hổng bảo mật và suy luận kiểu. Tuy nhiên, việc huấn luyện
mô hình NLP sâu đòi hỏi nguồn lực tính toán đáng kể. Bài báo này
khám phá các kỹ thuật nhằm đạt được việc sử dụng tốt nhất các nguồn lực
và thông tin có sẵn trong các mô hình này.

Chúng tôi đề xuất một phương pháp tổng quát, EarlyBIRD, để xây dựng
các biểu diễn tổng hợp của mã từ các lớp đầu của mô hình transformer
được huấn luyện trước. Chúng tôi nghiên cứu thực nghiệm tính khả thi
của phương pháp này trên mô hình CodeBERT bằng cách so sánh hiệu suất
của 12 chiến lược tạo biểu diễn tổng hợp với thực hành tiêu chuẩn chỉ
sử dụng lớp encoder cuối cùng.

Đánh giá của chúng tôi trên bốn tập dữ liệu cho thấy một số kết hợp
lớp đầu mang lại hiệu suất tốt hơn trong phát hiện lỗi, và một số kết hợp
cải thiện phân loại đa lớp. Cụ thể hơn, chúng tôi đạt được cải thiện
độ chính xác phát hiện trung bình +2 trên Devign chỉ với 3 trong 12 lớp
của CodeBERT và tăng tốc fine-tuning 3.3x. Những phát hiện này cho thấy
các lớp đầu có thể được sử dụng để đạt kết quả tốt hơn với cùng nguồn lực,
cũng như giảm sử dụng nguồn lực trong fine-tuning và suy luận.

KHÁI NIỆM CCS
•Phần mềm và kỹ thuật của nó ;•Phương pháp tính toán
→Mạng nơ-ron ;Xử lý ngôn ngữ tự nhiên ;

TỪ KHÓA
bền vững, tối ưu hóa mô hình, transformer, phân loại mã,
phát hiện lỗ hổng, AI4Code, AI4SE, ML4SE

1 GIỚI THIỆU
Tự động hóa các nhiệm vụ kỹ thuật phần mềm (SE) hỗ trợ các nhà phát triển
trong việc tạo và bảo trì mã nguồn. Gần đây, các mô hình học sâu (DL)
đã được huấn luyện trên các kho mã nguồn mở lớn và sử dụng để thực hiện
các nhiệm vụ phân tích mã [ 3,8,27,38]. Được thúc đẩy bởi giả thuyết
tự nhiên cho rằng mã và ngôn ngữ tự nhiên có chung các đặc điểm
thống kê tương tự, các nhà nghiên cứu và nhà cung cấp công cụ đã bắt đầu
huấn luyện các mô hình NLP sâu trên mã và fine-tuning chúng cho các
nhiệm vụ SE [ 11]. Ngoài ra, các mô hình như vậy đã được áp dụng
cho suy luận kiểu [ 17], phát hiện bản sao mã [ 50], sửa chữa
chương trình [ 9,15,47,48], và dự đoán lỗi [ 7,30,35,44]. Trong các
phương pháp dựa trên NLP, các nhiệm vụ SE thường được dịch thành
các bài toán phân loại mã. Ví dụ, phát hiện lỗ hổng phần mềm là
bài toán phân loại nhị phân, suy luận kiểu lỗi là thiết lập phân loại
đa lớp, và suy luận kiểu là nhiệm vụ phân loại đa nhãn đa lớp
trong trường hợp một kiểu được dự đoán cho mỗi biến trong
chương trình.

Hầu hết các mô hình NLP hiện đại được xây dựng trên kiến trúc
transformer [ 42]. Kiến trúc này sử dụng cơ chế attention và bao gồm
một encoder chuyển đổi chuỗi đầu vào thành một biểu diễn thông qua
một loạt các lớp, theo sau là các lớp decoder chuyển đổi biểu diễn này
thành chuỗi đầu ra. Mặc dù hiệu quả về khả năng học, thiết kế transformer
dẫn đến các mô hình đa lớp cần lượng lớn dữ liệu để huấn luyện từ đầu.
Một nhược điểm nổi tiếng của các mô hình này là việc sử dụng nguồn lực
cao cần thiết để huấn luyện do cả kích thước mô hình và dữ liệu. Trong khi
một số mô hình được huấn luyện trước đã được công bố gần đây, việc
fine-tuning các mô hình này cho các nhiệm vụ cụ thể vẫn đòi hỏi nguồn lực
tính toán bổ sung [27].

Bài báo này khám phá các kỹ thuật nhằm tối ưu hóa việc sử dụng
nguồn lực và thông tin có sẵn trong các mô hình trong quá trình fine-tuning.
Cụ thể, chúng tôi xem xét các mô hình white-box mở, mà từ đó có thể
trích xuất các trọng số từ mỗi lớp. Chúng tôi tập trung vào các mô hình
chỉ có encoder, vì chúng thường được sử dụng cho các nhiệm vụ phân loại
SE, đặc biệt là các encoder dựa trên transformer. Thực hành tiêu chuẩn
trong các mô hình encoder là lấy biểu diễn của chuỗi đầu vào từ lớp cuối
cùng của mô hình [ 14], trong khi thông tin từ các lớp trước đó thường
bị loại bỏ [ 21]. Tức là, trong khi các lớp đầu được sử dụng để tính
toán các giá trị của lớp cuối cùng, chúng thường không được xem xét
như các biểu diễn riêng lẻ của đầu vào theo cách mà lớp cuối cùng được
xem xét. Để minh họa lượng thông tin bị loại bỏ khi suy luận, khi
fine-tuning một encoder 12 lớp, như CodeBERT [ 14], cho phát hiện lỗi,
92% các embedding mã bị bỏ qua.¹ Tuy nhiên, đã được chỉ ra đối với
ngôn ngữ tự nhiên rằng các lớp đầu của encoder nắm bắt các đặc trưng
cú pháp cấp thấp tốt hơn so với các lớp sau [ 6,24,32,40], điều này
có thể có lợi cho các nhiệm vụ xuôi dòng.

Lấy cảm hứng từ hướng nghiên cứu khai thác các lớp đầu của mô hình,
chúng tôi đề xuất EarlyBIRD,² một phương pháp mới và tổng quát để xây
dựng các biểu diễn tổng hợp từ các lớp đầu của mô hình encoder được
huấn luyện trước. EarlyBIRD nhằm tận dụng tất cả thông tin có sẵn
trong các mô hình encoder được huấn luyện trước hiện có trong quá trình
fine-tuning để cải thiện kết quả hoặc đạt được kết quả cạnh tranh với
việc sử dụng nguồn lực giảm trong phân loại mã. Chúng tôi đánh giá
thực nghiệm EarlyBIRD trên CodeBERT [ 14], một mô hình encoder
phổ biến được huấn luyện trước cho mã, và bốn tập dữ liệu chuẩn
bao gồm ba nhiệm vụ SE phổ biến: phát hiện lỗi với các tập dữ liệu
Devign và ReVeal [ 20,51], suy luận kiểu lỗi với dữ liệu từ Yasunaga
et al. [ 47], và phân loại kiểu ngoại lệ [ 7]. Việc đánh giá so sánh
biểu diễn cơ sở sử dụng lớp encoder cuối cùng với kết quả thu được
qua EarlyBIRD. Chúng tôi cả fine-tune encoder toàn kích thước và
phiên bản được cắt tỉa chỉ có một số lớp đầu trong mô hình. Kịch bản
sau phân tích sự đánh đổi giữa việc chỉ sử dụng mô hình một phần và
tác động hiệu suất lên các nhiệm vụ SE.

Đóng góp: Trong bài báo này, chúng tôi đưa ra các đóng góp sau:
(1) Chúng tôi đề xuất EarlyBIRD, một phương pháp tạo biểu diễn tổng hợp
của mã sử dụng các lớp đầu của mô hình encoder dựa trên transformer.
Mục tiêu là đạt được hiệu suất phân loại mã tốt hơn với việc sử dụng
nguồn lực bằng nhau hoặc hiệu suất tương đương với việc sử dụng
nguồn lực thấp hơn.

(2) Chúng tôi thực hiện đánh giá thực nghiệm toàn diện về phương pháp
được đề xuất. Chúng tôi chỉ ra tác động của việc sử dụng các biểu diễn
EarlyBIRD tổng hợp trong khi fine-tuning mô hình CodeBERT kích thước
ban đầu trên bốn tập dữ liệu phân loại mã thực tế. Chúng tôi chạy
EarlyBIRD với 10 khởi tạo ngẫu nhiên khác nhau của các tham số có thể
huấn luyện không cố định và đánh dấu các biểu diễn EarlyBIRD mang lại
cải thiện có ý nghĩa thống kê so với cơ sở.

(3) Chúng tôi nghiên cứu việc sử dụng nguồn lực và hiệu suất của các
mô hình được cắt tỉa. Chúng tôi phân tích sự đánh đổi giữa việc loại bỏ
các lớp sau của mô hình và tác động này lên hiệu suất phân loại.

Phát hiện chính: Với EarlyBIRD, chúng tôi đạt được cải thiện hiệu suất
so với biểu diễn mã cơ sở với phần lớn các biểu diễn thu được từ các
lớp đầu đơn lẻ trên nhiệm vụ phát hiện lỗi và các kết hợp được chọn
trên phân loại kiểu lỗi và kiểu ngoại lệ. Hơn nữa, trong số các mô hình
kích thước giảm với các lớp sau được cắt tỉa, chúng tôi đạt được cải thiện
độ chính xác trung bình +2 trên Devign với tăng tốc fine-tuning 3.3x,
cũng như cải thiện độ chính xác +0.4 với tăng tốc 3.7x trung bình cho ReVeal.

Phần còn lại của bài báo được tổ chức như sau. Chúng tôi trình bày
công trình liên quan trong Mục 2 và cung cấp chi tiết nền tảng của
nghiên cứu trong Mục 3. Phương pháp được mô tả trong Mục 4 theo
sau bởi thiết lập thực nghiệm trong Mục 5. Chúng tôi trình bày và
thảo luận kết quả trong Mục 6 và kết luận với Mục 7.

2 CÔNG TRÌNH LIÊN QUAN
Ở đây, chúng tôi đưa ra tổng quan về các mô hình ngôn ngữ cho các
nhiệm vụ SE và các mô hình encoder gần đây, cụ thể, cũng như các
phương pháp khác nhau để sử dụng các lớp đầu của mô hình encoder.

2.1 Transformer trong Kỹ Thuật Phần Mềm
Tính khả dụng của mã nguồn mở và khả năng phần cứng tăng lên đã
làm phổ biến việc huấn luyện và sử dụng Học Sâu, bao gồm NLP và
Mô Hình Ngôn Ngữ Lớn (LLM), cho các nhiệm vụ SE. Cho đến nay,
các mô hình NLP sâu đã được áp dụng trong ít nhất 18 nhiệm vụ SE [ 28].
Các mô hình ngôn ngữ được huấn luyện trước có sẵn để fine-tuning
trên các nhiệm vụ SE phần lớn được xây dựng trên kiến trúc transformer,
các mô hình chuỗi-tới-chuỗi, và cơ chế attention [ 8,9,42]. Một chuẩn
mực được sử dụng rộng rãi để kiểm tra các kiến trúc học sâu khác nhau
trên các nhiệm vụ SE là CodeXGLUE [ 27]. Chuẩn mực này cung cấp
dữ liệu, mã nguồn để đánh giá mô hình, và bảng xếp hạng hiệu suất
mô hình trên các nhiệm vụ khác nhau [27].

Các nhiệm vụ SE có thể được dịch thành phân loại chuỗi đầu vào và
tạo mã hoặc văn bản. Ví dụ về các nhiệm vụ tạo sinh trong SE là
hoàn thành mã, sửa chữa mã, tạo tài liệu từ mã và ngược lại, và
dịch giữa các ngôn ngữ lập trình khác nhau. Những nhiệm vụ như vậy
thường được tiếp cận bằng các mô hình dịch máy nơ-ron. Các mô hình
transformer đầy đủ để dịch từ ngôn ngữ lập trình (PL) sang ngôn ngữ
tự nhiên (NL) hoặc các nhiệm vụ PL-PL bao gồm PLBART [ 1], PYMT5 [ 10],
TFix [ 4], CodeT5 [ 43], Break-It-Fix-It [ 47]. Thay vào đó, các mô hình
tạo sinh có thể bao gồm phần chỉ decoder của transformer như trong
các mô hình kiểu GPT. Trong trường hợp này, decoder vừa biểu diễn
chuỗi đầu vào vừa chuyển đổi nó thành chuỗi đầu ra. Các mô hình
dựa trên decoder cho mã bao gồm, ví dụ, Codex và CodeGPT [8, 27].

Trong các nhiệm vụ yêu cầu biểu diễn mã hoặc tài liệu và phân loại
tiếp theo của chúng, các kiến trúc chỉ encoder được sử dụng thường
xuyên hơn so với trong các nhiệm vụ dịch. Ví dụ về các bài toán phân
loại mã là phát hiện bản sao mã, phát hiện lỗi chung, như sự hiện diện
của toán tử bị hoán đổi, tên biến sai, lỗi cú pháp, hoặc lỗ hổng bảo mật.
Một số mô hình encoder cho mã đã áp dụng encoder hai chiều được
sử dụng rộng rãi, BERT [ 12], để huấn luyện trước nó trên mã, với
một số sửa đổi đầu vào. Theo cách này, các mô hình CodeBERT [ 14],
GraphCodeBERT [ 16], CuBERT [ 20], và PolyglotCodeBERT [ 2] đã được
tạo ra.

Chi tiết, mô hình CodeBERT 12 lớp dựa trên RoBERTa đã được huấn
luyện trước trên các nhiệm vụ NL-PL bằng nhiều PL và chỉ sử dụng
các đặc trưng văn bản của mã. Lưu ý rằng RoBERTa là một loại mô hình
BERT với các siêu tham số tối ưu hóa và quy trình huấn luyện trước [ 26].
Cùng với mô hình CodeGPT chỉ decoder, mô hình CodeBERT chỉ encoder
được sử dụng làm cơ sở trong CodeXGLUE. GraphCodeBERT sử dụng
cả thuộc tính văn bản và cấu trúc của mã để mã hóa các biểu diễn
của nó. PolyglotCodeBERT là phương pháp cải thiện fine-tuning của
mô hình CodeBERT trên tập dữ liệu đa ngôn ngữ cho nhiệm vụ đích
ngay cả khi nhiệm vụ đích chỉ kiểm tra một PL.

Bài báo này tập trung vào các chiến lược fine-tuning mà, trái ngược
với PolyglotCodeBERT, không tăng việc sử dụng nguồn lực cho
fine-tuning. CuBERT là một encoder 24 lớp dựa trên transformer
được huấn luyện trước và được kiểm tra trên một số nhiệm vụ phân
loại mã, bao gồm phân loại kiểu ngoại lệ. Chúng tôi kiểm tra hiệu
suất của các biểu diễn tổng hợp EarlyBIRD được đề xuất trên phát
hiện lỗi, bao gồm việc sử dụng một trong các chuẩn mực CodeXGLUE,
cũng như trên các nhiệm vụ phân loại kiểu lỗi và ngoại lệ. Tuy nhiên,
mục tiêu của bài báo này là đạt được cải thiện so với mô hình cơ sở
khi nó được fine-tuning với các biểu diễn mã tổng hợp. Chúng tôi
không nhằm so sánh kết quả với các mô hình khác, mà đưa ra một
phương pháp áp dụng được cho các encoder dựa trên transformer
cho mã nguồn và chỉ ra các cải thiện hiệu suất của nó so với việc
sử dụng mô hình tương tự mà không có phương pháp được đề xuất.

2.2 Sử Dụng Các Lớp Encoder Đầu
Một số nghiên cứu đã khám phá các phương pháp khác nhau để sử dụng
thông tin từ các lớp đầu của mô hình DL để biểu diễn chuỗi, như
thăm dò các lớp đơn lẻ, cắt tỉa và tỷ lệ học tập biến đổi.

Một cách để tận dụng thông tin từ các lớp đầu của mô hình là ưu tiên
khác nhau cho các lớp trong khi fine-tuning các mô hình [ 19,39].

--- TRANG 2 ---
ESEC/FSE '23, 3–9 tháng 12, 2023, San Francisco, CA, USA Anastasiia Grishina, Max Hort, và Leon Moonen

Ví dụ, chiến lược suy giảm tỷ lệ học tập theo lớp (LLRD) và khởi tạo
lại các lớp encoder muộn đã mang lại cải thiện so với fine-tuning
tiêu chuẩn của BERT trên các nhiệm vụ NLP [ 49]. Chiến lược LLRD
ban đầu được phát triển để điều chỉnh các lớp encoder sau với tỷ lệ
học tập lớn hơn. Theo cách này, các lớp sau có thể được thích ứng
tốt hơn với nhiệm vụ xuôi dòng đang xem xét, vì các lớp sau được
giả định học các đặc trưng phức tạp cụ thể cho nhiệm vụ của chuỗi
đầu vào [ 19]. Hơn nữa, Peters et al. [ 33] đã chỉ ra rằng hiệu suất
của fine-tuning cải thiện nếu các lớp encoder được cập nhật trong
quá trình fine-tuning so với chỉ huấn luyện bộ phân loại trên các
lớp encoder cố định (đóng băng).

Cắt tỉa các lớp sau của mô hình transformer là một cách khác để
chỉ xem xét các lớp đầu để fine-tuning [ 13,31,36]. Sajjad et al.
[36] đã nghiên cứu cách hiệu suất của mô hình transformer trên
NLP bị ảnh hưởng khi giảm kích thước bằng cách cắt tỉa các lớp.
Họ đã xem xét sáu chiến lược cắt tỉa, bao gồm loại bỏ từ các hướng
khác nhau, loại bỏ lớp xen kẽ, hoặc loại bỏ lớp dựa trên tầm quan
trọng, cho bốn mô hình được huấn luyện trước: BERT [ 12], RoBERTa [ 26],
XLNET [ 46], ALBERT [ 22]. Bằng cách cắt tỉa các lớp mô hình, Sajjad
et al. đã có thể giảm số lượng tham số xuống 60% của tập tham số
ban đầu trong khi duy trì mức hiệu suất cao. Trong khi hiệu suất
trên các nhiệm vụ xuôi dòng khác nhau trong nghiên cứu của họ,
các lớp dưới là quan trọng để duy trì hiệu suất khi fine-tuning cho
các nhiệm vụ xuôi dòng. Nói cách khác, loại bỏ các lớp trước có
hại cho hiệu suất. Nhìn chung, cắt tỉa lớp giảm kích thước mô hình
và do đó giảm thời gian fine-tuning và suy luận. Phù hợp với công
trình của Sajjad et al. [ 36], chúng tôi mở rộng các thí nghiệm với
việc cắt tỉa các lớp sau và giữ lại các lớp trước trong mô hình
(xem RQ2 trong Mục 6).

Việc sử dụng thông tin từ các lớp đầu đơn lẻ trong một số thí
nghiệm EarlyBIRD cũng được lấy cảm hứng từ Peters et al. [ 32].
Trong nghiên cứu của họ, Peters et al. đưa ra bằng chứng thực nghiệm
rằng các mô hình ngôn ngữ học cú pháp và thông tin từ loại trên
các lớp trước của mạng nơ-ron, trong khi thông tin phức tạp hơn,
như ngữ nghĩa và mối quan hệ đồng tham chiếu, được nắm bắt tốt
hơn bởi các lớp sâu hơn (sau). Trong một nghiên cứu khác, Karmakar
và Robbes đã thăm dò các mô hình được huấn luyện trước của mã,
bao gồm CodeBERT, trên các nhiệm vụ hiểu thông tin cú pháp, độ
phức tạp cấu trúc, độ dài mã, và thông tin ngữ nghĩa [ 21]. Trong
khi Karmakar và Robbes đã thăm dò các lớp đầu đóng băng của
các mô hình khác nhau cho mã trong một chiến lược đơn lẻ, chúng
tôi sử dụng 12 chiến lược khác nhau để kết hợp các lớp đầu không
đóng băng trong quá trình fine-tuning và tập trung vào các nhiệm vụ
phát hiện lỗi hoặc phân loại kiểu lỗi. Tương tự, Hernández López
et al. [ 18] đã thăm dò các lớp khác nhau của năm mô hình được
huấn luyện trước, bao gồm CodeBERT [ 14] và GraphCodeBERT [ 16],
và phát hiện rằng hầu hết thông tin cú pháp được mã hóa trong các
lớp giữa. Điểm mới của nghiên cứu chúng tôi so với Karmakar và
Robbes là chúng tôi kết hợp các lớp đầu ngoài việc trích xuất từng
lớp, trong khi Karmakar và Robbes đã trích xuất các biểu diễn lớp
đầu và sử dụng chúng mà không tạo ra các biểu diễn mới.

3 ENCODER CHO PHÂN LOẠI MÃ
Trong phần này, chúng tôi trình bày nền tảng về mô hình transformer
và các cách sử dụng khác nhau của kiến trúc encoder-decoder—hoặc
transformer đầy đủ—cũng như các biến thể chỉ encoder và chỉ decoder
của nó. Vì nghiên cứu của chúng tôi tập trung vào các mô hình nguồn
mở chỉ encoder có sẵn để fine-tuning, sự phân biệt giữa các loại
transformer là cần thiết để hiểu phương pháp.

Trong các kịch bản tạo sinh chuỗi-tới-chuỗi, mô hình transformer
bao gồm một encoder đa lớp biểu diễn chuỗi đầu vào và một decoder
tạo ra chuỗi đầu ra dựa trên biểu diễn chuỗi từ encoder và đầu ra
có sẵn được tạo ra ở các bước trước [ 42]. Đối với các nhiệm vụ phân
loại mã nguồn, transformer thường được giảm chỉ còn encoder của
nó theo sau bởi một đầu phân loại, một thành phần được thêm vào
encoder để phân loại biểu diễn thành các lớp khác nhau. Việc bỏ
decoder cho phân loại được thúc đẩy bởi hiệu quả nguồn lực, vì
decoder về mặt khái niệm chỉ cần thiết để tạo token từ chuỗi đầu vào.
Trong quá trình phân loại một đầu vào, encoder biểu diễn chuỗi và
chuyển nó đến đầu phân loại. Dựa trên thiết kế này, một số encoder
được huấn luyện trước đã được công bố trong những năm gần đây,
như BERT và RoBERTa được huấn luyện trước trên ngôn ngữ tự nhiên,
và các mô hình tương tự được huấn luyện trước trên mã, hoặc kết hợp
mã và ngôn ngữ tự nhiên [ 12,26]. Mục tiêu của huấn luyện trước
trong kịch bản huấn luyện trước và fine-tune là nắm bắt các mẫu
ngôn ngữ nói chung, để chúng có thể phục vụ như nền tảng cho
các nhiệm vụ xuôi dòng cụ thể miền. Các mô hình được huấn luyện
trước có thể được fine-tuning trên các nhiệm vụ xuôi dòng khác nhau
trong NLP và SE.

Xử lý chuỗi đầu vào để phân loại bao gồm một số bước: tokenization,
embedding ban đầu, mã hóa chuỗi với encoder, và chuyển biểu diễn
chuỗi qua một đầu phân loại. Tokenization chia chuỗi đầu vào, thêm
các token đặc biệt, khớp các token với ID của chúng trong từ vựng
các token, và thống nhất độ dài token kết quả cho các mẫu trong
tập dữ liệu. Embedding chuyển đổi ID token một chiều thành biểu
diễn vector tĩnh đa chiều ban đầu của token và thường là một phần
của mô hình encoder được huấn luyện trước. Biểu diễn này được
cập nhật sử dụng cơ chế attention của encoder. Vì attention, biểu
diễn của đầu vào bị ảnh hưởng bởi tất cả các token trong chuỗi,
vì vậy nó được ngữ cảnh hóa.

CodeBERT là một mô hình dựa trên RoBERTa với 12 lớp encoder được
huấn luyện trước trên 6 ngôn ngữ lập trình (Python, Java, JavaScript,
PHP, Ruby, và Go), cũng như các nhiệm vụ text-to-code [ 14]. Huấn
luyện trước được thực hiện trên các nhiệm vụ mô hình hóa ngôn ngữ
có mặt nạ (MLM) và phát hiện token được thay thế (RTD). Những
nhiệm vụ này tương ứng huấn luyện mô hình để suy ra token nào
bị che dấu trong MLM, và trong RTD dự đoán liệu có token nào
trong chuỗi gốc bị hoán đổi với token khác không nên có trong
chuỗi. CodeBERT xuất ra biểu diễn encoder hai chiều của chuỗi
đầu vào, có nghĩa là mô hình xem xét ngữ cảnh từ các từ đi trước
và theo sau để biểu diễn mỗi token trong chuỗi đầu vào.

Một mô hình được huấn luyện trước thường được phát hành với
một tokenizer được huấn luyện trước. Tokenizer được huấn luyện
trước đảm bảo rằng ID token tương ứng với những cái được xử lý
trong quá trình huấn luyện trước. Tokenizer cũng thêm các token
đặc biệt, như token CLS ở đầu mỗi chuỗi đầu vào, các token PAD
để thống nhất độ dài của chuỗi đầu vào, và token EOS để biểu thị
kết thúc chuỗi đầu vào và bắt đầu của chuỗi padding [ 12]. Tất cả
các token được chuyển đổi bởi mô hình trong mỗi lớp encoder.
Trong số tất cả các token, biểu diễn token CLS từ lớp cuối cùng,
được cập nhật bởi tất cả các lớp encoder, thường được sử dụng
làm biểu diễn cho toàn bộ chuỗi.

--- TRANG 3 ---
EarlyBIRD Bắt Được Lỗi: Về Việc Khai Thác Các Lớp Đầu của Mô Hình Encoder. . . ESEC/FSE '23, 3–9 tháng 12, 2023, San Francisco, CA, USA

Thực hành tiêu chuẩn sử dụng token CLS từ lớp encoder cuối cùng
được thúc đẩy bởi quy trình huấn luyện trước. Ví dụ, trong MLM,
mô hình dự đoán token bị che dấu dựa trên biểu diễn token CLS
từ lớp thứ 12 của BERT và CodeBERT. Tuy nhiên, việc chọn token
để biểu diễn toàn bộ chuỗi trong fine-tuning có thể khác. Ví dụ,
trong PLBART [ 1], một mô hình transformer cho mã với cả encoder
và decoder, token EOS được sử dụng để biểu diễn chuỗi đầu vào.
Trong bài báo này, chúng tôi đề xuất các cách khác nhau để biểu
diễn chuỗi đầu vào và sử dụng thông tin từ các lớp đầu của mô hình
một cách hiệu quả.

4 PHƯƠNG PHÁP
Trong bài báo này, kiến trúc của mô hình phân loại mã bao gồm
năm phần: (1) một tokenizer, (2) một lớp embedding, (3) một encoder
với một số lớp, (4) một tập hợp các phép toán để kết hợp các biểu
diễn chuỗi từ các lớp encoder với EarlyBIRD, và (5) một đầu phân
loại. Đầu ra của mỗi bước được sử dụng làm đầu vào cho bước tiếp
theo. Tổng quan về kiến trúc được hiển thị trong Hình 1 và mô tả
bên dưới. Sự khác biệt chính giữa kiến trúc này và kiến trúc phân
loại được thảo luận trong Mục 3 là bước (4); kiến trúc tiêu chuẩn
chỉ bao gồm các bước (1–3) và (5).

Các bước (1)–(3) sử dụng tokenizer, embedder, và encoder được
huấn luyện trước. EarlyBIRD được công thức hóa theo cách tổng quát
và có thể áp dụng cho bất kỳ encoder nào, nhưng cho các thí nghiệm
của chúng tôi, chúng tôi cố định mô hình CodeBERT và tokenizer.
Trong bước (4), chúng tôi kết hợp thông tin từ tất cả các lớp hoặc
chỉ từ một số lớp đầu của encoder, trái ngược với cơ sở sử dụng
lớp cuối cùng của encoder. Cuối cùng, đầu phân loại trong bước (5)
bao gồm một lớp dropout và một lớp tuyến tính với softmax.

Mô hình encoder biểu diễn mỗi token của chuỗi đầu vào bằng một
vector có kích thước 𝐻, còn được gọi là kích thước ẩn. Đối với mỗi
chuỗi đầu vào có độ dài 𝑆, và kích thước ẩn 𝐻, chúng tôi thu được
một ma trận có kích thước 𝑆×𝐻 cho mỗi lớp trong số 𝐿 lớp của
mô hình cơ sở như được hiển thị trong Hình 1. Ví dụ, kiến trúc
CodeBERT được cố định với 12 lớp encoder, tức là 𝐿=12 cho mô hình
đó. Tất cả thông tin có sẵn trong encoder cho một chuỗi đầu vào
được lưu trữ trong một tensor có kích thước 𝐿×𝑆×𝐻. Các kết hợp
EarlyBIRD phải tạo ra một vector ®𝑅 có kích thước 𝐻 biểu diễn đầu
vào, như được hiển thị trong Hình 1. Việc giữ biểu diễn mã đầu ra
có kích thước 𝐻 là cần thiết để cung cấp so sánh công bằng của
các biểu diễn tổng hợp EarlyBIRD với biểu diễn mã tiêu chuẩn thu
được từ lớp cuối cùng. Theo cách này, chiều của đầu phân loại là
như nhau cho tất cả các kết hợp của các lớp đầu và có ảnh hưởng
tối thiểu có thể trong quá trình fine-tuning.

[Hình 1: Kiến trúc mô hình cho phân loại mã.]

Như một chiến lược để nghiên cứu có hệ thống các biểu diễn tổng hợp,
chúng tôi tạo ra một tìm kiếm lưới trên ba phép toán điển hình để
kết hợp đầu ra của các lớp mạng nơ-ron – max pooling, weighted
sum và slicing – và hai chiều để áp dụng các phép toán: trên token
và/hoặc lớp. Đối với chiều token, chúng tôi sử dụng tất cả các token
từ một lớp cụ thể hoặc chỉ token CLS. Trong các lớp, chúng tôi slice
một lớp, tổng hoặc lấy giá trị tối đa trên tất cả các lớp. Việc lựa chọn
xem xét mỗi token của một lớp được thúc đẩy bởi thực tế là các mô hình
dựa trên transformer thể hiện mức độ attention khác nhau cho các
loại token khác nhau [ 29], điều này chỉ ra rằng việc chỉ sử dụng
token CLS có thể không phải là lựa chọn tốt nhất cho các nhiệm vụ [ 37].
Chúng tôi cũng thí nghiệm với các kích thước khác nhau của mô hình.
Các chiến lược kết hợp sử dụng tất cả các lớp của mô hình được
huấn luyện trước được chia thành hai loại: các chiến lược sử dụng
token CLS từ các lớp encoder; các chiến lược sử dụng nhiều token
hơn chỉ CLS từ các lớp encoder.

Khi chúng tôi slice token CLS và áp dụng từng phép toán trên các
lớp, chúng tôi thu được các kết hợp token CLS sau:
(i) cơ sở : token CLS từ lớp cuối cùng, tức là lớp số 𝐿;
(ii) token CLS từ một lớp³ số 𝑙, 𝑙∈{1,...,(𝐿−1)};
(iii) max pool trên token CLS từ tất cả các lớp {𝑙}𝐿
𝑙=1;
(iv) weighted sum trên token CLS từ tất cả các lớp {𝑙}𝐿
𝑙=1.

Tập hợp kết hợp thứ hai sử dụng biểu diễn của tất cả các token
trong chuỗi đầu vào được tokenized, bao gồm token CLS. Chúng tôi
đầu tiên áp dụng phép toán max pooling cho tất cả các token hoặc
tất cả các lớp và sử dụng các phép toán còn lại. Sau đó chúng tôi
áp dụng weighted sum như phép toán đầu tiên theo sau bởi max pool
hoặc slicing của một lớp:

(v) max pool token từ một lớp số 𝑙, 𝑙∈{1,...,𝐿};
(vi) max pool trên tất cả các lớp cho mỗi token trong chuỗi đầu vào,
max pool trên token;
(vii) max pool trên tất cả các lớp cho mỗi token trong chuỗi đầu vào;
weighted sum trên token;
(viii) max pool trên tất cả các token cho mỗi lớp số 𝑙, 𝑙∈{1,...,𝐿};
weighted sum trên các lớp
(ix) weighted sum trên token từ một lớp số 𝑙, 𝑙∈{1,...,𝐿};
(x) weighted sum trên token cho mỗi lớp số 𝑙, 𝑙∈{1,...,𝐿};
weighted sum trên tất cả các lớp;
(xi) weighted sum trên tất cả các lớp cho mỗi token trong chuỗi
đầu vào; weighted sum trên tất cả các token.

Lưu ý rằng các trọng số trong weighted sum là các tham số có thể
học được. Tuy nhiên, số lượng tham số có thể học được được thêm
vào để fine-tuning chiếm 0.00042%⁴ số lượng tham số có thể học được
trong cấu hình cơ sở. Vì lý do này, chúng tôi đề cập rằng các mô hình
với kết hợp (ii-x) có cùng kích thước mô hình trong khi lưu ý đến
overhead của trọng số có thể học được trong weighted sum.

Ngoài các thí nghiệm với kết hợp token, chúng tôi cũng nghiên cứu
hiệu suất của mô hình với 𝑙<𝐿 lớp đầu tiên và kết hợp token cơ sở,
được mô tả như sau:

³Chúng tôi sử dụng mỗi lớp 𝑙 trong các kết hợp riêng biệt nếu chúng tôi ký hiệu
𝑠, 𝑙∈{1,...,𝐿}, và chỉ định tập hợp các lớp {𝑙}𝐿
𝑙=1 nếu một số lớp được sử dụng
cùng lúc.
⁴Weighted sum trên token thêm 𝑆=512 trọng số có thể học được. Vì các trọng số
của tổng được chia sẻ giữa các lớp, số lượng trọng số được thêm vào tối đa là
𝐿+𝐻=524 trong số 124M trọng số có thể học được trong mô hình cơ sở. Các kết hợp
không có weighted sum không thêm tham số có thể học được bổ sung vào mô hình cơ sở.
Weighted sum trên các lớp thêm 𝐿=12 trọng số có thể học được cho CodeBERT.

--- TRANG 4 ---
ESEC/FSE '23, 3–9 tháng 12, 2023, San Francisco, CA, USA Anastasiia Grishina, Max Hort, và Leon Moonen

(xii) token CLS từ lớp cuối cùng của mô hình với 𝑙<𝐿 lớp encoder.

Lưu ý rằng kết hợp cơ sở (i) với việc sử dụng token CLS từ lớp 𝐿
tương ứng với (ii) và (xii) nếu 𝑙=𝐿.

Các kết hợp được trình bày trong Hình 2. Các kết hợp tương tự
được trình bày gần nhau hoặc được kết hợp trong cùng một hình
nếu chúng chỉ có sự khác biệt nhỏ và chia sẻ các phần chính.
Ví dụ, trong Hình 2c, chúng tôi minh họa các kết hợp (iii) và (iv),
vì cả hai đều sử dụng token CLS từ tất cả các lớp được kết hợp
bằng max pooling hoặc weighted sum. Các số La Mã chỉ ra các
loại kết hợp được giữ nguyên trong các mô tả bên dưới hình hoặc
trong chính các hình, nhưng thứ tự được thay đổi. Chúng tôi đề cập
số kết hợp tương ứng với mô tả trong phần hiện tại, như kết hợp
cơ sở (i) trong Hình 2a hoặc kết hợp (ii) cho token CLS từ một
lớp đầu trong Hình 2b. Chúng tôi làm nổi bật những phần nào
của đầu ra lớp encoder được sử dụng cho mỗi kết hợp bằng màu.
Các ô trắng tương ứng với các token không được sử dụng trong
các kết hợp lớp đầu. Mục tiêu của tất cả các kết hợp là thu được
một biểu diễn vector ®𝑅 cho mỗi mẫu mã đầu vào. Ví dụ, trong
Hình 2a, chúng tôi xem xét lớp cuối cùng 𝐿 và trích xuất chỉ
token CLS được đánh dấu là ®𝑅.

Một nhận xét khác về các kết hợp EarlyBIRD liên quan đến việc
sử dụng tất cả các token hoặc chỉ các token mã. Các token mã
là những token tương ứng với các từ hoặc từ phụ được tokenized
đầu vào và được hiển thị trong Hình 2 là token𝑖1,...,token𝑖𝑁
cho một chuỗi đầu vào 𝑖 có kích thước 𝑖𝑁. Đối với mỗi kết hợp
sử dụng nhiều hơn chỉ một token CLS, tức là các kết hợp (v-xi),
chúng tôi thí nghiệm với chỉ các token mã, cũng như với tất cả
các token, bao gồm CLS, EOS, và PAD. Động lực để kiểm tra
riêng các token mã xuất phát từ giả thuyết rằng thông tin trong
các token đặc biệt có thể đưa nhiễu vào kết quả.

[Hình 2: Các kết hợp của các lớp encoder đầu dẫn đến vector
biểu diễn mã ®𝑅 cho mỗi chuỗi đầu vào được tokenized.
Số La Mã trong ngoặc tương ứng với các kết hợp được mô tả
trong Mục 4. Quan sát rằng thứ tự trình bày đã được thiết kế
để tiết kiệm không gian bằng cách nhóm các kết hợp tương tự
trong cùng một hình phụ.]

5 THIẾT LẬP THỰC NGHIỆM
Trong phần này, chúng tôi mô tả các tập dữ liệu được sử dụng
để đánh giá thực nghiệm và chi tiết triển khai của fine-tuning với
phương pháp EarlyBIRD được đề xuất. Chúng tôi nghiên cứu các
kịch bản phân loại mã nhị phân và đa nhiệm vụ để khám phá tính
tổng quát của kết quả.

5.1 Tập Dữ Liệu cho Phân Loại Mã Nguồn
Chúng tôi fine-tune và kiểm tra mô hình CodeBERT sử dụng phương
pháp EarlyBIRD trên bốn tập dữ liệu. Các tập dữ liệu bao gồm ba
nhiệm vụ: phát hiện lỗi, phân loại kiểu lỗi và phân loại kiểu ngoại
lệ — với tương ứng 2, 3, và 20 lớp. Chúng cũng chứa dữ liệu
trong hai ngôn ngữ lập trình, C++ và Python. Ngoài ra, các tập
dữ liệu được chọn có kích thước tập con huấn luyện tương tự.
Theo cách này, chúng tôi nhằm giảm tác động của việc tiếp xúc
của mô hình với các lượng dữ liệu huấn luyện khác nhau trong
quá trình fine-tuning. Thống kê của các tập dữ liệu được cung cấp
trong Bảng 1. Chúng tôi báo cáo kích thước của các phần train/validation/test.
Ngoài ra, chúng tôi tính toán số lượng token trung bình trong các
chuỗi đầu vào sau khi tokenization với tokenizer CodeBERT được
huấn luyện trước. Vì kích thước chuỗi đầu vào tối đa cho mô hình
CodeBERT bị giới hạn ở 𝑆=512, số lượng token cho biết mô hình
có quyền truy cập vào bao nhiêu thông tin hoặc bao nhiêu thông tin
bị cắt bỏ, trong trường hợp đầu vào dài.

Devign: Tập dữ liệu này chứa các hàm trong C/C++ từ hai dự án
nguồn mở được gán nhãn là dễ bị tổn thương hoặc không dễ bị tổn
thương [ 51]. Chúng tôi sử dụng lại phần chia train/validation/test
từ chuẩn mực CodeXGLUE Defect detection.⁵ Tập dữ liệu cân bằng:
tỷ lệ các hàm không dễ bị tổn thương là 54%.

ReVeal: Tương tự như Devign, ReVeal là tập dữ liệu phát hiện lỗ hổng
của các hàm C/C++ [ 7]. Tập dữ liệu không cân bằng: nó chứa 90%
đoạn mã không dễ bị tổn thương. Cả tập dữ liệu Devign và ReVeal
đều chứa các hàm dễ bị tổn thương và không dễ bị tổn thương thực
tế từ các dự án nguồn mở.

Break-It-Fix-It (BIFI): Tập dữ liệu chứa các đoạn mã cấp hàm
trong Python với lỗi cú pháp [ 47]. Chúng tôi sử dụng các hàm
buggy ban đầu và công thức hóa nhiệm vụ phân loại mã thành ba
lớp: Unbalanced Parentheses với 43% tổng số ví dụ mã trong BIFI,
Indentation Error với 31% mẫu mã, Invalid Syntax chứa 26% mẫu.
Phần chia train/test được cung cấp trong tập dữ liệu được sử dụng
lại, và tập validation được trích xuất là 10% dữ liệu huấn luyện.

Exception Type: Tập dữ liệu bao gồm các hàm ngắn trong Python
với token __HOLE__ được chèn vào thay thế một ngoại lệ trong mã.⁶
Nhiệm vụ là dự đoán một trong 20 kiểu ngoại lệ bị che dấu cho
mỗi hàm đầu vào và không cân bằng. Tập dữ liệu ban đầu được
tạo từ ETH Py150 Open corpus⁷ như được mô tả trong bài báo gốc [ 20].
Chúng tôi sử dụng lại phần chia train/validation/test được cung cấp
bởi các tác giả.

5.2 Triển Khai
Kiến trúc dựa trên tokenizer và mô hình encoder CodeBERT.⁸
Mô hình định nghĩa độ dài chuỗi tối đa, kích thước ẩn, và có 12
lớp, vì vậy 𝑆=512, 𝐻=768, 𝐿=12. Siêu tham số trong các thí
nghiệm được đặt thành 𝐵=64, tỷ lệ học tập là 1𝑒-5, và xác suất
dropout là 0.1. Nếu mẫu đầu vào được tokenized dài hơn 𝑆=512,
chúng tôi cắt bỏ các token ở cuối để làm cho đầu vào phù hợp
với mô hình. Chúng tôi chạy fine-tuning với optimizer Adam và
kiểm tra cho mỗi kết hợp 10 lần với các seed khác nhau trong
10 epoch và báo cáo hiệu suất cho epoch tốt nhất trung bình
trên 10 lần chạy. Epoch tốt nhất được định nghĩa bằng cách đo
độ chính xác trên tập validation. Chúng tôi sử dụng Python 3.7
và Cuda 11.6, và chạy thí nghiệm trên một GPU Nvidia Volta A100.

[Bảng 1: Thống kê của các Tập Dữ Liệu Fine-Tuning.]

5.3 Chỉ Số Đánh Giá
Để trình bày tác động của các kết hợp lớp đầu, chúng tôi so sánh
độ chính xác trên tập kiểm tra cho tất cả các tập dữ liệu, vì nó
cho phép chúng tôi so sánh kết quả với các chuẩn mực khác. Ngoài
ra, chúng tôi báo cáo điểm F1 có trọng số được ký hiệu là F1(w)
cho phân tích chi tiết của các kết hợp được chọn để tính đến sự
mất cân bằng lớp. Để thu được điểm F1 có trọng số, điểm F1 thường
được tính toán cho mỗi nhãn và trung bình có trọng số của chúng
được lấy. Các trọng số bằng số lượng mẫu trong một lớp.

Chúng tôi cũng báo cáo kết quả của kiểm định Wilcoxon signed-rank
trên các chỉ số tương ứng cho các kết hợp cho thấy cải thiện so
với cơ sở [ 45]. Kiểm định Wilcoxon là một kiểm định phi tham số
phù hợp cho thiết lập mà các biến thể mô hình khác nhau được
kiểm tra trên cùng một tập kiểm tra, vì nó là một kiểm định được
ghép cặp. Kiểm định Wilcoxon kiểm tra giả thuyết null liệu hai
mẫu được ghép cặp liên quan có đến từ cùng một phân phối.
Chúng tôi từ chối giả thuyết null nếu p-value nhỏ hơn 𝛼=0.05.
Trong trường hợp chúng tôi có được cải thiện của một chỉ số so
với cơ sở với kết hợp EarlyBIRD và giả thuyết null bị từ chối,
chúng tôi kết luận rằng kết hợp hoạt động tốt hơn và kết quả
có ý nghĩa thống kê. Đối với các mô hình được cắt tỉa, chúng tôi
tính toán 𝐴12 phi tham số của Vargha và Delaney để đo lường
kích thước hiệu ứng của thay đổi hiệu suất cho độ chính xác
và F1(w) với ngưỡng 0.71, 0.64 và 0.56 cho kích thước hiệu
ứng lớn, trung bình và nhỏ [41].

5.4 Câu Hỏi Nghiên Cứu
Trong quá trình đánh giá thực nghiệm các biểu diễn mã tổng hợp
EarlyBIRD, chúng tôi giải quyết các câu hỏi nghiên cứu sau:

RQ1. Biểu Diễn Mã Tổng Hợp với Cùng Kích Thước Mô Hình:
Tác động của việc sử dụng các kết hợp (ii-xi) của các lớp đầu với
cùng kích thước mô hình so với phương pháp cơ sở chỉ sử dụng
token CLS từ lớp cuối cùng, tức là kết hợp (i), để biểu diễn mã
lên hiệu suất mô hình trong kịch bản phân loại mã là gì? Mục tiêu
là tìm ra liệu có loại kết hợp EarlyBIRD nào hoạt động tốt hơn
một cách nhất quán cho các tập dữ liệu và nhiệm vụ khác nhau.

RQ2. Mô Hình Được Cắt Tỉa: Tác động của việc giảm số lượng
lớp encoder được huấn luyện trước trong các kết hợp (xii) lên
việc sử dụng nguồn lực và hiệu suất mô hình trên các nhiệm vụ
phân loại mã là gì? Trái ngược với RQ1, trong đó chúng tôi xem
xét các kết hợp không giảm kích thước mô hình, câu hỏi nghiên
cứu này dành cho việc nghiên cứu sự đánh đổi giữa việc sử dụng
ít nguồn lực hơn với các mô hình kích thước giảm và biến đổi
hiệu suất về mặt chỉ số phân loại.

Đối với cả hai câu hỏi nghiên cứu, chúng tôi đánh giá các biểu
diễn tổng hợp trên các kịch bản phân loại mã nhị phân và đa
nhiệm vụ để khám phá tính tổng quát của kết quả thu được cho
trường hợp nhị phân. Chúng tôi nghiên cứu xem và những kết
hợp nào dẫn đến hiệu suất tốt hơn, trung bình trên 10 lần chạy
với các seed khác nhau. Đối với các kết hợp cải thiện cơ sở trung
bình, chúng tôi cũng khám phá xem kết quả có ý nghĩa thống kê
theo kiểm định Wilcoxon.

6 KẾT QUẢ VÀ THẢO LUẬN

6.1 EarlyBIRD với Mô Hình Kích Thước Cố Định
Để trả lời RQ1, chúng tôi khám phá các kết hợp một lớp, kết hợp
đa lớp, và ước tính ý nghĩa thống kê của cải thiện hiệu suất.

6.1.1 Kết Hợp Token trong Các Lớp Đầu Được Chọn Đơn Lẻ.
Hình 3 hiển thị bản đồ nhiệt của sự khác biệt về độ chính xác
trung bình thu được với mỗi kết hợp chỉ sử dụng một lớp đầu
được chọn so với cơ sở. Ngoài ra, chúng tôi hiển thị giá trị của
sự khác biệt về độ chính xác trung bình cho mỗi loại kết hợp và
số lớp. Lưu ý rằng thang đo là logarit và trong trường hợp cực
đoan nhất trải dài từ khoảng -37 đến +2. Các giá trị âm được
hiển thị màu đen, và các giá trị dương được hiển thị màu trắng.
Sự khác biệt có ý nghĩa thống kê theo kiểm định Wilcoxon được
đánh dấu bằng dấu sao (∗) bên cạnh giá trị. Các kết hợp tương
ứng với cơ sở được đánh dấu bằng "bsln" và có sự khác biệt
bằng không, theo định nghĩa. Kết quả cho điểm F1 có trọng số
cho thấy mô hình tương tự như của độ chính xác trung bình.
Chúng được hiển thị theo cách tương tự trong Hình 4.

Các hàng đầu tiên trong Hình 3a và 3b tương ứng với các kết
hợp (ii) token CLS lớp 𝑙. Với loại kết hợp này, cải thiện trung
bình so với cơ sở đạt được với phần lớn các lớp đầu. Cụ thể,
chúng tôi có được cải thiện độ chính xác từ +0.2 đến +2.0 cho
Devign trong 8 trong 11 lớp, và cải thiện độ chính xác từ +0.1
đến +0.8 cho ReVeal trong 9 trong 11 lớp. Động lực thay đổi
chỉ số trên các số lớp được chọn khác nhau cho Devign và ReVeal.
Chi tiết, hiệu suất trung bình của kết hợp (ii) tốt nhất với lớp
3 trên Devign (cải thiện độ chính xác +2.0) và với lớp 1 cho
ReVeal (cải thiện độ chính xác +0.8). Cải thiện tốt nhất về F1(w)
khớp với lớp 3 cho Devign và với lớp 2 cho ReVeal, như được
hiển thị trong Hình 4.

Max pooling trên tất cả các token có sẵn từ một lớp được chọn
trong kết hợp (v) cũng đạt được cải thiện hiệu suất so với cơ sở,
như được hiển thị trong hàng 2 và 3 của Hình 3a, 3b. Nói chung,
các lớp 4–11 mang lại độ chính xác cao hơn và các lớp 2–11
F1(w) cao hơn với max pooling cho Devign so với cơ sở. Đối với
ReVeal, tất cả các lớp trừ lớp 11 dẫn đến độ chính xác trung
bình tốt hơn và các lớp 2–10 có F1(w) trung bình cao hơn. Max
pooling trên tất cả các token, bao gồm các token đặc biệt, đạt
được cải thiện có ý nghĩa thống kê tốt nhất của độ chính xác
là +0.9 trong số tất cả các kết hợp cho ReVeal.

Weighted sum của tất cả các token hoặc riêng các token mã trong
kết hợp (ix) không cải thiện hiệu suất cơ sở. Chúng tôi giả định
rằng fine-tuning trong 10 epoch không đủ cho loại kết hợp này,
vì loss tại epoch 10 trên cả phần train và validation cao hơn
đối với các kết hợp (ix) so với các kết hợp với max pooling.
Vì mục tiêu của nghiên cứu này là sử dụng cùng hoặc ít nguồn
lực hơn để fine-tuning, chúng tôi đã không fine-tuning kết hợp
này trong hơn 10 epoch.

Trong khi các kết hợp (ii) và (v) hoạt động tốt hơn cho phần
lớn các lớp trên nhiệm vụ phát hiện lỗi, phân loại đa lớp cho
dự đoán kiểu lỗi hoặc ngoại lệ không hưởng lợi từ các kết hợp
ở mức độ tương tự như nhiệm vụ nhị phân. Chỉ max pooling
của các token của lớp encoder cuối cùng đạt được hiệu suất
tốt hơn so với cơ sở cho BIFI (cải thiện độ chính xác +0.1,
cải thiện điểm F1 có trọng số +0.1) và Exception Type (cải
thiện độ chính xác +0.2, cải thiện điểm F1 có trọng số +0.1).

Tác động của việc sử dụng tất cả các token hoặc riêng các
token mã phụ thuộc vào tập dữ liệu. Sự khác biệt giữa hiệu
suất của các kết hợp một lớp với max pooling của tất cả các
token và chỉ các token mã chiếm 0.0-0.1 độ chính xác hoặc
F1(w). Đối với các nhiệm vụ đa lớp, kết quả trung bình cải
thiện với việc sử dụng mỗi lớp sau trong mô hình. Chúng tôi
có được cải thiện hiệu suất với kết hợp max pooling (v), trong
khi các kết hợp một lớp khác không hoạt động tốt hơn cơ sở.

Kết quả hoạt động tốt nhất trên các tập dữ liệu Devign và
Exception Type classification có ý nghĩa thống kê theo kiểm
định Wilcoxon. Đối với ReVeal, kết quả tốt thứ hai có ý nghĩa
thống kê. Chúng tôi chưa có được cải thiện có ý nghĩa thống
kê cho BIFI. Chúng tôi giải thích điều này bởi thực tế là chỉ
số cơ sở đã cao, tức là độ chính xác 96.7. Việc đạt được cải
thiện thường khó khăn hơn khi cơ sở hoạt động ở mức này.

Về bản chất, các kết hợp liên quan đến token CLS tương ứng
với lớp đơn lẻ (ii), cũng như các kết hợp max pooling (v)
hoạt động tốt hơn trung bình cho các tập dữ liệu phát hiện
lỗi Devign và Reveal. Tuy nhiên, chỉ kết hợp max pooling (v)
của các token từ lớp encoder cuối cùng vượt trội hơn cơ sở
trung bình cho các tập dữ liệu đa lớp BIFI và Exception Type.
Weighted sum của các token từ một lớp được chọn (ix) hoạt
động tệ hơn cơ sở nếu được fine-tuning trong cùng số epoch
cho tất cả các nhiệm vụ. Các nhiệm vụ phân loại đa lớp yêu
cầu thông tin từ lớp cuối cùng để có hiệu suất tốt hơn trong
các thí nghiệm của chúng tôi, trong khi nhiệm vụ nhị phân
của phát hiện lỗi cho phép chúng tôi sử dụng các lớp đầu
và cải thiện hiệu suất so với cơ sở.

[Hình 3: Sự khác biệt về độ chính xác trung bình giữa EarlyBIRD
và hiệu suất cơ sở (bsln). Dấu sao ∗ chỉ ra sự khác biệt có ý
nghĩa thống kê so với cơ sở.]

6.1.2 Kết Hợp Đa Lớp. Sự khác biệt hiệu suất trung bình với
cơ sở của các kết hợp sử dụng các lớp đầu được hiển thị
như bản đồ nhiệt trong Hình 5 và 6. Chúng tôi bao gồm giá
trị của sự khác biệt hiệu suất trung bình và thêm dấu sao (∗)
vào số nếu sự khác biệt có ý nghĩa thống kê. Một lần nữa,
các giá trị âm được hiển thị màu đen, và các giá trị dương
được hiển thị màu trắng.

Khi chúng tôi sử dụng tất cả thông tin từ các lớp có sẵn, cải
thiện so với cơ sở ít hơn so với những gì quan sát được trong
Mục 6.1.1, nơi một lớp đầu cụ thể đã được sử dụng. Chi tiết,
trong số các kết hợp liên quan đến token CLS từ tất cả các
lớp đầu, không có kết hợp nào hoạt động tốt hơn cơ sở cho
ReVeal, BIFI, hoặc Exception Type. Tuy nhiên, cải thiện tốt
nhất (+0.6 độ chính xác) trong số các thí nghiệm với tất cả
các lớp được có được trên Devign với weighted sum của các
token CLS trong kết hợp (iv), ít hơn so với cải thiện tối đa
với các kết hợp từ một lớp đầu được chọn trong Mục 6.1.1.
Cải thiện của F1(w) được hiển thị trong Hình 6. Chúng tôi
có được cải thiện F1(w) hơi tốt hơn cho Devign, không có
cải thiện F1(w) cho tập dữ liệu ReVeal không cân bằng. Sự
khác biệt F1(w) trung bình với cơ sở cho các nhiệm vụ đa
lớp giống với sự khác biệt độ chính xác.

Nếu chúng tôi xem xét các kết hợp liên quan đến tất cả các
token, kết hợp (vi) với hai phép toán max pooling vượt trội
hơn cơ sở cho Devign, Reveal, và BIFI với cải thiện độ chính
xác từ +0.1 đến +0.3. Không có kết hợp nào liên quan đến
tất cả các lớp vượt trội hơn cơ sở trung bình cho tập dữ liệu
Exception Type.

Các kết hợp liên quan đến một max pooling và một weighted
sum của tất cả các token hoạt động tệ hơn hoặc trung tính
so với cơ sở. Các kết hợp chỉ với weighted sum hoạt động
tệ hơn cơ sở trung bình.

Trả lời RQ1. EarlyBIRD đạt được cải thiện độ chính xác và
điểm F1 có ý nghĩa thống kê cho các tập dữ liệu phát hiện
lỗi bằng cách sử dụng các kết hợp một lớp liên quan đến
token CLS hoặc max pooling trên tất cả các token. Đối với
phân loại kiểu lỗi và kiểu ngoại lệ, max pooling của các
token từ lớp encoder cuối cùng đã cải thiện hiệu suất.
Weighted sum của các token không cải thiện hiệu suất so
với cơ sở.

6.2 Mô Hình Được Cắt Tỉa
Phần này dành cho các kết hợp của các lớp đầu được khởi
tạo với 𝑙<𝐿 lớp đầu tiên từ mô hình được huấn luyện trước
và được fine-tuning như các mô hình 𝑙 lớp — các kết hợp (xii).
Chúng tôi bắt đầu bằng cách so sánh hiệu suất của việc sử
dụng token CLS từ lớp 𝑙 của mô hình toàn kích thước, tức
là kết hợp (ii), và sử dụng token CLS từ lớp 𝑙 của mô hình
có tổng cộng 𝑙 lớp — kết hợp (xii). Hình 7 trình bày độ chính
xác trung bình thu được với hai kết hợp này tùy thuộc vào
lớp được sử dụng, cũng như kết hợp cơ sở của việc sử dụng
CLS từ lớp cuối cùng 𝐿=12 của CodeBERT. Trung bình, các
mô hình được cắt tỉa với kích thước giảm hoạt động ngang
bằng với mô hình toàn kích thước cho phát hiện lỗi trên tập
dữ liệu Devign cân bằng, và cho phân loại kiểu lỗi và kiểu
ngoại lệ. Tuy nhiên, hiệu suất của hai kết hợp tương tự phân
kỳ cho tập dữ liệu phát hiện lỗi ReVeal không cân bằng trong
các lớp 4 và 6–11.

Quan trọng nhất, kết quả cho thấy việc giảm kích thước mô
hình và sử dụng token CLS từ lớp cuối cùng của mô hình
giảm hoạt động ngang bằng với cơ sở cho nhiệm vụ phát hiện
lỗi. Cải thiện tốt nhất với mô hình giảm đạt được với encoder
3 lớp cho Devign và encoder 1 lớp cho ReVeal. Kết quả này
cho thấy có thể vừa giảm nguồn lực vừa cải thiện hiệu suất
của mô hình trong quá trình fine-tuning trên nhiệm vụ phát
hiện lỗi với cả tập dữ liệu cân bằng và không cân bằng.

Để khám phá sự đánh đổi giữa việc sử dụng nguồn lực và
suy giảm hiệu suất cho việc nhận dạng kiểu lỗi và kiểu ngoại
lệ, chúng tôi hiển thị tốc độ tăng tốc trung bình của một
epoch fine-tuning và mất hiệu suất so với cơ sở cho các tập
dữ liệu BIFI và Exception Type trong Bảng 2. Chúng tôi cũng
báo cáo các giá trị tương ứng cho Devign và ReVeal, mà cả
cải thiện và mất hiệu suất đều được chỉ ra. Tốc độ tăng tốc
được báo cáo như hệ số tỷ lệ của thời gian cơ sở. Sự khác
biệt chỉ số được hiển thị như cải thiện hoặc mất của điểm
F1 có trọng số và độ chính xác so với hiệu suất cơ sở. Cải
thiện có ý nghĩa thống kê được báo cáo in đậm, trong khi
mất không có ý nghĩa thống kê được đánh dấu bằng dấu sao (∗).
Kích thước hiệu ứng 𝐴12 được chỉ ra bởi ba sắc thái màu
xanh làm màu ô, với sắc thái tối nhất chỉ ra hiệu ứng lớn
( 𝐴12>0.71), sắc thái giữa chỉ ra hiệu ứng trung bình ( 𝐴12>0.64),
và sắc thái nhạt nhất chỉ ra hiệu ứng nhỏ ( 𝐴12>0.56). Chúng
tôi cũng gạch dưới và thảo luận các kết quả được chọn cải
thiện giá trị chỉ số và giảm việc sử dụng nguồn lực.

[Hình 7: Hiệu suất mô hình với tập con 𝑙<𝐿 lớp (xii) so với
mô hình với tất cả các lớp (ii); token CLS từ lớp 𝑙.]

Phần lớn các kết hợp (xii) với mô hình được cắt tỉa vượt trội
hơn cơ sở cho Devign và ReVeal. Hơn nữa, các mô hình với
2–10 lớp cho thấy cải thiện có ý nghĩa thống kê của cả hai
chỉ số trên Devign, với mô hình 3 lớp đạt được cải thiện
độ chính xác +2 với tăng tốc fine-tuning 3.3 lần trung bình
với cùng phần cứng và phần mềm. Không chỉ mô hình 3 lớp
cải thiện độ chính xác so với cơ sở CodeBERT lên 63.7, mà
còn vượt trội hơn một số mô hình khác được kiểm tra trên
Devign và được báo cáo trên chuẩn mực CodeXGLUE [ 27].
Cụ thể, mô hình CodeBERT 3 lớp được cắt tỉa của chúng tôi
vượt trội hơn mô hình transformer đầy đủ PLBART [ 1], và
biểu diễn mã code2vec được huấn luyện trước trên cây cú
pháp trừu tượng và token mã theo cách liên kết [ 1]. Tuy nhiên,
mô hình được cắt tỉa của chúng tôi không vượt trội hơn mô
hình hoạt động tốt nhất được báo cáo trên CodeXGLUE, CoText,
đạt được độ chính xác 66.62 [34].

Các mô hình với 1 và 11 lớp đạt được cải thiện độ chính xác
có ý nghĩa thống kê cho ReVeal. Tuy nhiên, mô hình 1 lớp
giảm điểm F1(w). Việc sử dụng lớp 11 không tác động đến
tốc độ fine-tuning, trong khi mô hình 1 lớp mang lại tăng
tốc 3.7x của tốc độ fine-tuning cơ sở. Việc thiếu tăng tốc
với mô hình 11 lớp có thể được giải thích bởi thực tế là số
lượng tham số có thể huấn luyện không giảm tuyến tính với
việc loại bỏ các lớp sau, vì lớp embedding bổ sung và đầu
phân loại vẫn không thay đổi. Mô hình 2 lớp dẫn đến cải
thiện tốt nhất của F1(w) có ý nghĩa thống kê. Mô hình 2
lớp cũng cải thiện độ chính xác trên ReVeal. Đối với Devign
và ReVeal, các cải thiện có ý nghĩa thống kê có kích thước
hiệu ứng lớn.

Đối với BIFI, chúng tôi có được giảm không có ý nghĩa thống
kê của F1(w) và độ chính xác theo kiểm định Wilcoxon mang
lại tăng tốc 1.2x của fine-tuning với mô hình 11 lớp. Nếu
chúng tôi giảm số lượng lớp xuống 8, hiệu suất trên BIFI
vẫn trong giới hạn (chỉ số cơ sở −1), nhưng chúng tôi có
được tối đa 1.7x tăng tốc trung bình của fine-tuning một
epoch. Trong trường hợp sử dụng mô hình với 1–10 lớp,
chúng tôi quan sát thấy thay đổi có ý nghĩa thống kê của
phân phối và giảm giá trị chỉ số.

Đối với tập dữ liệu Exception Type không cân bằng, hiệu suất
giảm nhanh hơn và tăng tốc ít nổi bật hơn so với BIFI. Thay
đổi giá trị trung bình của các chỉ số cho tất cả các mô hình
có ý nghĩa thống kê. Chi tiết, các chỉ số giảm -1.0 giá trị
chỉ số tuyệt đối tại 11 lớp với tăng tốc fine-tuning 1.1x và
-1.8 với 10 lớp với tăng tốc 1.2x. Chúng tôi giải thích sự
suy giảm mạnh hơn của hiệu suất kết hợp bởi các giá trị
chỉ số cơ sở thấp hơn (75.39 độ chính xác, 75.30 điểm F1
có trọng số) so với trường hợp BIFI (96.7 độ chính xác và
điểm F1 có trọng số). Đối với BIFI, suy giảm không có ý
nghĩa thống kê có kích thước hiệu ứng nhỏ. Tuy nhiên, đối
với cả tập dữ liệu BIFI và Exception Type, chúng tôi quan sát
thấy suy giảm hiệu suất có kích thước hiệu ứng lớn với các
mô hình được cắt tỉa.

Chúng tôi kết luận rằng đối với tập dữ liệu BIFI với cơ sở
hoạt động tốt và 3 lớp, mất hiệu suất khi loại bỏ mỗi lớp
ít hơn so với tập dữ liệu phân loại Exception Type với hiệu
suất cơ sở thấp hơn và 20 lớp. Việc sử dụng nguồn lực,
tương quan với thời gian dành cho điều chỉnh, giảm nhanh
hơn đối với BIFI so với Exception Type. Điều này một phần
được giải thích bởi đầu phân loại lớn hơn cho tập dữ liệu
Exception Type, vì tập dữ liệu này có 20 lớp trái ngược với
chỉ 3 lớp trong BIFI. Nói cách khác, chúng tôi quan sát rằng
tập dữ liệu BIFI có cơ sở mạnh khó vượt trội hơn với cắt tỉa.
Ngược lại, độ phức tạp của tập dữ liệu Exception Type có
thể ảnh hưởng đến kết quả theo hướng ngược lại: Hiệu suất
cơ sở đã không mạnh lắm, và khó cải thiện thêm với chỉ
các lớp đầu.

Trả lời RQ2. Chúng tôi có được cải thiện hiệu suất so với
cơ sở cũng như tăng tốc fine-tuning cho cả hai tập dữ liệu
phát hiện lỗi bằng cách sử dụng token CLS từ lớp cuối cùng
của mô hình được cắt tỉa. Đối với phân loại đa lớp, hiệu suất
giảm khi cắt tỉa mỗi lớp từ cuối mô hình. Sự giảm mạnh
hơn đối với tập dữ liệu với 20 kiểu ngoại lệ so với nhiệm
vụ với 3 kiểu lỗi.

6.3 Các Mối Đe Dọa Đối Với Tính Hợp Lệ
Mối đe dọa chính đối với tính hợp lệ bên ngoài là kết quả
mang tính thực nghiệm và có thể không tổng quát hóa cho
tất cả các thiết lập phân loại mã, bao gồm các ngôn ngữ lập
trình, nhiệm vụ, và mô hình dựa trên encoder cho mã khác.
Chúng tôi đã kiểm tra các kết hợp EarlyBIRD trên mã trong
C để phát hiện lỗi và Python cho phân loại kiểu lỗi và kiểu
ngoại lệ trong nghiên cứu này. Việc lựa chọn CodeBERT làm
mô hình encoder và cấu trúc bên trong của nó ảnh hưởng
đến kết quả. Ví dụ, một mô hình encoder nhận chuỗi đầu
vào nhỏ hơn có thể hoạt động tệ hơn trên cùng các tập dữ
liệu, vì các phần lớn hơn của chuỗi mã đầu vào phải được
cắt bỏ trong trường hợp này. Tính hợp lệ bên ngoài có thể
được cải thiện bằng cách kiểm tra trên nhiều tập dữ liệu
và mô hình encoder hơn.

Các mối đe dọa đối với tính hợp lệ bên trong liên quan đến
sự phụ thuộc của mô hình vào việc khởi tạo các tham số
có thể huấn luyện và việc lựa chọn phương pháp. Đầu phân
loại và weighted sum với các tham số có thể huấn luyện
trong các thí nghiệm của chúng tôi phụ thuộc vào việc khởi
tạo các tham số và có thể dẫn mô hình đến các cực tiểu
địa phương khác nhau trong quá trình fine-tuning. Để giảm
tác động của các khởi tạo ngẫu nhiên khác nhau, chúng tôi
đã fine-tuning và kiểm tra tất cả các kết hợp EarlyBIRD 10
lần với các seed ngẫu nhiên khác nhau.

Ngoài ra, chúng tôi đã sử dụng kiểm định Wilcoxon để xác
minh liệu các cải thiện đạt được có ý nghĩa thống kê. Tuy
nhiên, kiểm định Wilcoxon chỉ ước tính liệu các phép đo
của giá trị cơ sở và kết hợp EarlyBIRD có được rút ra từ
các phân phối khác nhau. Thời gian được báo cáo dành cho
fine-tuning và các tăng tốc tương ứng có mục đích minh
họa sự giảm trong việc sử dụng nguồn lực, và sẽ phụ thuộc
vào phần cứng được sử dụng. Ngay cả khi sử dụng các hệ
số tăng tốc cho các mô hình được cắt tỉa, có khả năng những
số này sẽ khác trên các cấu hình phần cứng khác.

Chúng tôi đã triển khai các thuật toán và quy trình thống
kê trong Python, với sự trợ giúp của các thư viện được sử
dụng rộng rãi như PyTorch, NumPy và SciPy. Tuy nhiên,
chúng tôi không thể đảm bảo không có lỗi triển khai có thể
ảnh hưởng đến đánh giá của chúng tôi.

7 KẾT LUẬN
Trong bài báo này, chúng tôi đã đề xuất EarlyBIRD, một
phương pháp kết hợp các lớp đầu của mô hình encoder cho
mã, và kiểm tra các kết hợp lớp đầu khác nhau trên các
nhiệm vụ kỹ thuật phần mềm của phát hiện lỗi, phân loại
kiểu lỗi và kiểu ngoại lệ. Nghiên cứu của chúng tôi được
thúc đẩy bởi giả thuyết rằng các lớp đầu chứa thông tin
có giá trị bị loại bỏ bởi thực hành tiêu chuẩn biểu diễn
mã bằng token CLS từ lớp encoder cuối cùng. EarlyBIRD
cung cấp các cách để cải thiện hiệu suất của các mô hình
hiện có với việc sử dụng nguồn lực tương tự, cũng như để
giảm việc sử dụng nguồn lực trong khi đạt được kết quả
tương đương với cơ sở.

Kết quả: Sử dụng EarlyBIRD, chúng tôi có được cải thiện
có ý nghĩa thống kê so với cơ sở cho phần lớn các kết hợp
liên quan đến một lớp encoder đơn lẻ trên phát hiện lỗi,
và với các kết hợp EarlyBIRD được chọn trên phân loại
kiểu lỗi và kiểu ngoại lệ. Max pooling của các token từ
các lớp đơn lẻ được chọn mang lại cải thiện hiệu suất cho
tất cả các tập dữ liệu. Cả hiệu suất phân loại và thời gian
fine-tuning trung bình cho một epoch đều được cải thiện
bằng cách cắt tỉa mô hình được huấn luyện trước về các
lớp đầu và sử dụng token CLS từ lớp cuối cùng của mô
hình được cắt tỉa. Đối với phát hiện lỗi, điều này dẫn đến
tăng độ chính xác +2.0 và tăng tốc fine-tuning 3.3x trên
Devign, và tối đa cải thiện độ chính xác +0.8 với tăng tốc
3.7x trên ReVeal. Các mô hình được cắt tỉa không dẫn đến
cải thiện hiệu suất phân loại đa lớp, nhưng chúng cho thấy
tăng tốc fine-tuning và giảm tiêu thụ nguồn lực liên quan.

Kết quả cho thấy các mô hình được cắt tỉa với kích thước
giảm hoặc hoạt động tốt hơn hoặc có thể dẫn đến giảm
việc sử dụng nguồn lực trong quá trình fine-tuning với
các mức độ biến đổi hiệu suất khác nhau, điều này chỉ ra
tiềm năng của EarlyBIRD trong các kịch bản hạn chế nguồn
lực của việc triển khai phát hiện lỗi và phân loại kiểu lỗi
trong môi trường sản xuất. Ví dụ, EarlyBIRD đạt được tăng
tốc 2.1x cho BIFI trong khi giảm độ chính xác từ 96.7 xuống 95.0.

Công việc tương lai: Nghiên cứu có thể được mở rộng bằng
cách nghiên cứu tổng quát hóa cho các mô hình encoder
khác. Chúng tôi đang trong quá trình nghiên cứu hiệu suất
của EarlyBIRD với hai mô hình encoder mới: StarEncoder [ 23]
và ContraBERT_C [ 25]. Một hướng khác cho nghiên cứu
tương lai là liệu các loại kết hợp lớp và cắt tỉa, như chúng
tôi đã nghiên cứu trong bài báo này cho các kiến trúc encoder,
có phải là các kỹ thuật hiệu quả cho các kiến trúc decoder
và encoder-decoder. Hơn nữa, sẽ thú vị khi thí nghiệm với
các nhiệm vụ phân loại mã khác, như phát hiện lỗi chung
và dự đoán kiểu lỗ hổng. Cái sau có thể được nghiên cứu
bằng cách sử dụng các kiểu CWE từ Common Weakness
Enumeration như được gán nhãn trong tập dữ liệu CVEfixes [5].

LỜI CẢM ÒN
Công việc này được hỗ trợ bởi Hội đồng Nghiên cứu Na Uy
thông qua dự án secureIT (IKTPLUSS #288787). Max Hort
được hỗ trợ thông qua Chương trình Học bổng ERCIM 'Alain
Bensoussan'. Đánh giá thực nghiệm được trình bày trong
bài báo này được thực hiện trên Cơ sở Hạ tầng Thực nghiệm
để Khám phá Điện toán Exascale (eX3), được hỗ trợ tài chính
bởi Hội đồng Nghiên cứu Na Uy theo hợp đồng #270053,
cũng như trên các nguồn lực được cung cấp bởi Sigma2,
Cơ sở Hạ tầng Quốc gia cho Điện toán Hiệu suất Cao và
Lưu trữ Dữ liệu ở Na Uy.

TÍNH KHẢ DỤNG DỮ LIỆU
Để hỗ trợ khoa học mở và cho phép tái tạo và xác minh
công việc của chúng tôi, tất cả các artifact được cung cấp
thông qua Zenodo tại URL sau: https://doi.org/10.5281/zenodo.7608802.

TÀI LIỆU THAM KHẢO
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021.
Unified Pre-training for Program Understanding and Generation. Trong Conference
of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational Linguistics, Online,
2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211

[2] Toufique Ahmed và Premkumar Devanbu. 2022. Multilingual Training for
Software Engineering. Trong Proceedings of the 44th International Conference on
Software Engineering. ACM, Pittsburgh Pennsylvania, 1443–1455. https://doi.
org/10.1145/3510003.3510049

[3] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, và Charles Sutton. 2018.
A Survey of Machine Learning for Big Code and Naturalness. Comput. Surveys
51, 4 (tháng 7 2018), 81:1–81:37. https://doi.org/10.1145/3212695

[4] Berkay Berabi, Jingxuan He, Veselin Raychev, và Martin Vechev. 2021. TFix:
Learning to Fix Coding Errors with a Text-to-Text Transformer. Trong International
Conference on Machine Learning, Vol. 139. PMLR, Virtual Event, 780–791.

[5] Guru Bhandari, Amara Naseer, và Leon Moonen. 2021. CVEfixes: Automated
Collection of Vulnerabilities and Their Fixes from Open-Source Software. Trong
International Conference on Predictive Models and Data Analytics in Software
Engineering (PROMISE). ACM, 30–39. https://doi.org/10.1145/3475960.3475985

[6] Terra Blevins, Omer Levy, và Luke Zettlemoyer. 2018. Deep RNNs Encode Soft
Hierarchical Syntax. Trong Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers). Association for Computational
Linguistics, Melbourne, Australia, 14–19. https://doi.org/10.18653/v1/p18-2003

[7] Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, và Baishakhi Ray. 2022.
Deep Learning Based Vulnerability Detection: Are We There Yet? IEEE Transactions
on Software Engineering 48, 9 (tháng 9 2022), 3280–3296. https://doi.org/10.1109/tse.2021.3087402

[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish
Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe
Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,
Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex
Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,
Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,
Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam
McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating Large
Language Models Trained on Code. https://doi.org/10.48550/arXiv.2107.03374
arXiv:2107.03374 [cs]

[9] Zimin Chen, Steve James Kommrusch, Michele Tufano, Louis-Noël Pouchet,
Denys Poshyvanyk, và Martin Monperrus. 2019. SEQUENCER: Sequence-to-
Sequence Learning for End-to-End Program Repair. IEEE Transactions on Software
Engineering 47, 9 (2019), 1943–1959. https://doi.org/10.1109/tse.2019.2940179

[10] Colin Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, và
Neel Sundaresan. 2020. PyMT5: Multi-Mode Translation of Natural Language
and Python Code with Transformers. Trong Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP). Association for
Computational Linguistics, Online, 9052–9065. https://doi.org/10.18653/v1/2020.emnlp-main.728

[11] P. Devanbu. 2015. New Initiative: The Naturalness of Software. Trong Proceedings -
International Conference on Software Engineering, Vol. 2. IEEE Computer Society,
543–546. https://doi.org/10.1109/icse.2015.190

[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
Trong Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL, Vol 1). Association for
Computational Linguistics, 4171–4186. https://doi.org/10.18653/v1/n19-1423

[13] Angela Fan, Edouard Grave, và Armand Joulin. 2019. Reducing Transformer
Depth on Demand with Structured Dropout. https://doi.org/10.48550/arXiv.1909.11556
arXiv:1909.11556 [cs, stat]

[14] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, và Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. Trong Findings of
the Association for Computational Linguistics: EMNLP 2020. Online, 1536–1547.
https://doi.org/10.18653/v1/2020.findings-emnlp.139

[15] Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen, và Dinh
Phung. 2022. VulRepair: A T5-based Automated Software Vulnerability Repair.
Trong Proceedings of the 30th ACM Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022).
Association for Computing Machinery, New York, NY, USA, 935–947.
https://doi.org/10.1145/3540250.3549098

[16] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, và
Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with
Data Flow. Trong International Conference on Learning Representations, ICLR 2021.
Virtual Event, Austria, 1–18. arXiv:2009.08366 [cs]

[17] Vincent J. Hellendoorn, Christian Bird, Earl T. Barr, và Miltiadis Allamanis. 2018.
Deep Learning Type Inference. Trong Joint European Software Engineering Conference
and Symposium on the Foundations of Software Engineering (ESEC/FSE). ACM,
New York, NY, USA, 152–162. https://doi.org/10.1145/3236024.3236051

[18] José Antonio Hernández López, Martin Weyssow, Jesús Sánchez Cuadrado, và
Houari Sahraoui. 2023. AST-Probe: Recovering Abstract Syntax Trees from
Hidden Representations of Pre-Trained Language Models. Trong Proceedings of the
37th IEEE/ACM International Conference on Automated Software Engineering (ASE
'22). Association for Computing Machinery, New York, NY, USA, 1–11. https:
//doi.org/10.1145/3551349.3556900

[19] Jeremy Howard và Sebastian Ruder. 2018. Universal Language Model Fine-
tuning for Text Classification. Trong Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). Association
for Computational Linguistics, Melbourne, Australia, 328–339. https://doi.org/
10.18653/v1/p18-1031

[20] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, và Kensen Shi. 2020.
Learning and Evaluating Contextual Embedding of Source Code. Trong Proceedings
of the 37th International Conference on Machine Learning. PMLR, 5110–5121.
arXiv:2001.00059 [cs]

[21] Anjan Karmakar và Romain Robbes. 2021. What Do Pre-Trained Code Models
Know about Code?. Trong 2021 36th IEEE/ACM International Conference on Automated
Software Engineering (ASE). 1332–1336. https://doi.org/10.1109/ase51524.2021.9678927

[22] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, và Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised Learning
of Language Representations. https://doi.org/10.48550/arXiv.1909.11942
arXiv:1909.11942 [cs]

[23] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu,
Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig
Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,
Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason
Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,
Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh,
Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero,
Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan
Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson,
Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried,
Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
Thomas Wolf, Arjun Guha, Leandro von Werra, và Harm de Vries. 2023. StarCoder:
May the Source Be with You! https://doi.org/10.48550/arXiv.2305.06161
arXiv:2305.06161 [cs]

[24] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, và Noah A.
Smith. 2019. Linguistic Knowledge and Transferability of Contextual Representations.
https://doi.org/10.48550/arXiv.1903.08855 arXiv:1903.08855 [cs]

[25] Shangqing Liu, Bozhi Wu, Xiaofei Xie, Guozhu Meng, và Yang Liu. 2023. ContraBERT:
Enhancing Code Pre-Trained Models via Contrastive Learning. Trong
Proceedings of the 45th International Conference on Software Engineering (ICSE
'23). IEEE Press, Melbourne, Victoria, Australia, 2476–2487. https://doi.org/10.1109/icse48619.2023.00207

[26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/arXiv.1907.11692
arXiv:1907.11692 [cs]

[27] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou,
Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan,
Neel Sundaresan, Shao Kun Deng, Shengyu Fu, và Shujie Liu. 2021. CodeXGLUE:
A Machine Learning Benchmark Dataset for Code Understanding and Generation.
Trong Proceedings of the Neural Information Processing Systems Track on Datasets and
Benchmarks. 1–16. arXiv:2102.04664 [cs]

[28] Changan Niu, Chuanyi Li, Bin Luo, và Vincent Ng. 2022. Deep Learning Meets
Software Engineering: A Survey on Pre-Trained Models of Source Code. https:
//doi.org/10.48550/arXiv.2205.11739 arXiv:2205.11739 [cs]

[29] Matteo Paltenghi và Michael Pradel. 2021. Thinking Like a Developer? Comparing
the Attention of Humans with Neural Models of Code. Trong 2021 36th
IEEE/ACM International Conference on Automated Software Engineering (ASE).
867–879. https://doi.org/10.1109/ase51524.2021.9678712

[30] Cong Pan, Minyan Lu, và Biao Xu. 2021. An Empirical Study on Software Defect
Prediction Using CodeBERT Model. Applied Sciences 11, 11 (tháng 5 2021), 4793.
https://doi.org/10.3390/app11114793

[31] David Peer, Sebastian Stabinger, Stefan Engl, và Antonio Rodriguez-Sanchez.
2022. Greedy-Layer Pruning: Speeding up Transformer Models for Natural
Language Processing. Pattern Recognition Letters 157 (tháng 5 2022), 76–82. https:
//doi.org/10.1016/j.patrec.2022.03.023 arXiv:2105.14839 [cs]

[32] Matthew Peters, Mark Neumann, Luke Zettlemoyer, và Wen-tau Yih. 2018.
Dissecting Contextual Word Embeddings: Architecture and Representation. Trong
Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, Brussels, Belgium, 1499–
1509. https://doi.org/10.18653/v1/d18-1179

[33] Matthew E. Peters, Sebastian Ruder, và Noah A. Smith. 2019. To Tune or Not to
Tune? Adapting Pretrained Representations to Diverse Tasks. Trong Proceedings of the
4th Workshop on Representation Learning for NLP (RepL4NLP-2019). Association for
Computational Linguistics, Florence, Italy, 7–14. https://doi.org/10.18653/v1/w19-4302

[34] Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Annibal, Alec Peltekian,
và Yanfang Ye. 2021. CoTexT: Multi-task Learning with Code-Text Transformer.
Trong Workshop on Natural Language Processing for Programming (NLP4Prog 2021).
Association for Computational Linguistics, Online, 40–47. https://doi.org/10.18653/v1/2021.nlp4prog-1.5

[35] Rebecca Russell, Louis Kim, Lei Hamilton, Tomo Lazovich, Jacob Harer, Onur
Ozdemir, Paul Ellingwood, và Marc McConley. 2018. Automated Vulnerability
Detection in Source Code Using Deep Representation Learning. Trong International
Conference on Machine Learning and Applications (ICMLA). IEEE, Orlando, FL,
757–762. https://doi.org/10.1109/icmla.2018.00120

[36] Hassan Sajjad, Fahim Dalvi, Nadir Durrani, và Preslav Nakov. 2023. On the
Effect of Dropping Layers of Pre-trained Transformer Models. Computer Speech
& Language 77 (tháng 1 2023), 101429. https://doi.org/10.1016/j.csl.2022.101429
arXiv:2004.03844 [cs]

[37] Rishab Sharma, Fuxiang Chen, Fatemeh Fard, và David Lo. 2022. An Exploratory
Study on Code Attention in BERT. Trong Proceedings of the 30th IEEE/ACM International
Conference on Program Comprehension (ICPC '22). Association for Computing
Machinery, New York, NY, USA, 437–448. https://doi.org/10.1145/3524610.3527921

[38] Tushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, và Federica
Sarro. 2021. A Survey on Machine Learning Techniques for Source Code Analysis.
https://doi.org/10.48550/arXiv.2110.09610 arXiv:2110.09610 [cs]

[39] Chi Sun, Xipeng Qiu, Yige Xu, và Xuanjing Huang. 2019. How to Fine-Tune
BERT for Text Classification?. Trong Chinese Computational Linguistics (Lecture
Notes in Computer Science), Maosong Sun, Xuanjing Huang, Heng Ji, Zhiyuan
Liu, và Yang Liu (Eds.). Springer International Publishing, Cham, 194–206.
https://doi.org/10.1007/978-3-030-32381-3_16

[40] Chi Sun, Xipeng Qiu, Yige Xu, và Xuanjing Huang. 2020. How to Fine-
Tune BERT for Text Classification? https://doi.org/10.48550/arXiv.1905.05583
arXiv:1905.05583 [cs]

[41] András Vargha và Harold D. Delaney. 2000. A Critique and Improvement of the
CL Common Language Effect Size Statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (tháng 6 2000), 101–132. https://doi.org/
10.3102/10769986025002101

[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention Is All
You Need. Trong International Conference on Neural Information Processing Systems
(NeurIPS), I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, và R. Garnett (Eds.). Curran Associates, Inc., 5998–6008.
arXiv:1706.03762 [cs]

[43] Yue Wang, Weishi Wang, Shafiq Joty, và Steven C.H. Hoi. 2021. CodeT5:
Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding
and Generation. Trong Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, Online and Punta
Cana, Dominican Republic, 8696–8708. https://doi.org/10.18653/v1/2021.emnlp-main.685

[44] Hongwei Wei, Guanjun Lin, Lin Li, và Heming Jia. 2021. A Context-Aware
Neural Embedding for Function-Level Vulnerability Detection. Algorithms 14, 11
(tháng 11 2021), 335. https://doi.org/10.3390/a14110335

[45] Frank Wilcoxon. 1992. Individual Comparisons by Ranking Methods. Trong Breakthroughs
in Statistics: Methodology and Distribution, Samuel Kotz và Norman L.
Johnson (Eds.). Springer, New York, NY, 196–202. https://doi.org/10.1007/978-1-4612-4380-9_16

[46] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
và Quoc V. Le. 2020. XLNet: Generalized Autoregressive Pretraining
for Language Understanding. https://doi.org/10.48550/arXiv.1906.08237
arXiv:1906.08237 [cs]

[47] Michihiro Yasunaga và Percy Liang. 2021. Break-It-Fix-It: Unsupervised Learning
for Program Repair. Trong International Conference on Machine Learning. PMLR,
12. arXiv:2106.06600 [cs]

[48] He Ye, Matias Martinez, và Martin Monperrus. 2022. Neural Program Repair with
Execution-Based Backpropagation. Trong Proceedings of the 44th International Conference
on Software Engineering (ICSE '22). Association for Computing Machinery,
New York, NY, USA, 1506–1518. https://doi.org/10.1145/3510003.3510222

[49] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, và Yoav
Artzi. 2021. Revisiting Few-sample BERT Fine-tuning. Trong NeurIPS 2021. 1–22.
arXiv:2006.05987 [cs]

[50] Gang Zhao và Jeff Huang. 2018. DeepSim: Deep Learning Code Functional
Similarity. Trong Proceedings of the 2018 26th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering.
ACM, Lake Buena Vista FL USA, 141–151. https://doi.org/10.1145/3236024.3236068

[51] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, và Yang Liu. 2019. Devign:
Effective Vulnerability Identification by Learning Comprehensive Program
Semantics via Graph Neural Networks. Trong International Conference on Neural
Information Processing Systems (NeurIPS). Curran Associates, Inc., Vancouver,
Canada., 11. arXiv:1909.03496 [cs]

Nhận ngày 2-2-2023; chấp nhận ngày 27-7-2023
