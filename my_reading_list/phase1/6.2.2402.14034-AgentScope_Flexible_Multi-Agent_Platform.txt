# 2402.14034.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/agent/2402.14034.pdf
# File size: 7898607 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AgentScope: A Flexible yet Robust Multi-Agent Platform
Dawei Gao†, Zitao Li†, Xuchen Pan∗, Weirui Kuang∗, Zhijian Ma∗, Bingchen Qian∗,
Fei Wei∗, Wenhao Zhang∗, Yuexiang Xie∗, Daoyuan Chen∗,
Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi,
Yaliang Li‡, Bolin Ding‡, Jingren Zhou
Alibaba Group
Abstract
With the rapid advancement of Large Language Models (LLMs), significant progress has been made
in multi-agent applications. However, the complexities in coordinating agents’ cooperation and LLMs’
erratic performance pose notable challenges in developing robust and efficient multi-agent applications.
To tackle these challenges, we propose AgentScope , a developer-centric multi-agent platform with
message exchange as its core communication mechanism. The abundant syntactic tools, built-in agents
and service functions, user-friendly interfaces for application demonstration and utility monitor, zero-code
programming workstation, and automatic prompt tuning mechanism significantly lower the barriers to
both development and deployment. Towards robust and flexible multi-agent application, AgentScope
provides both built-in and customizable fault tolerance mechanisms. At the same time, it is also armed
with system-level support for managing and utilizing multi-modal data, tools, and external knowledge.
Additionally, we design an actor-based distribution framework, enabling easy conversion between local and
distributed deployments and automatic parallel optimization without extra effort. With these features,
AgentScope empowers developers to build applications that fully realize the potential of intelligent agents.
We have released AgentScope at https://github.com/modelscope/agentscope , and hope AgentScope
invites wider participation and innovation in this fast-moving field.
1 Introduction
Multi-agent systems, as upgraded extensions of single-agent systems, require collaborative efforts from multiple
agents working in concert (Wang et al., 2023; Xi et al., 2023). With the advancement of Large Language
Models (LLMs) (Ouyang et al., 2022; OpenAI, 2023; Touvron et al., 2023a,b), multi-agent applications have
made great progress in both research and industrial communities, including software engineering (Hong et al.,
2023), society simulation (Park et al., 2023), and intelligent assistant (Wu et al., 2023; AutoGPT-Team,
2023). Although significant progress has been made in multi-agent scenarios, there are still major challenges
remaining in multi-agent application development.
Developing a multi-agent application is more complex than creating a single-agent one. Unlike single-agent
setups where an agent solely interacts with users, the development in the multi-agent scenario requires careful
creation and management of multiple models and agents (Wang et al., 2023; Xi et al., 2023), which poses high
requirements for both versatility and handiness for a platform. In particular, the following aspects feature the
challenges: 1) Agents involved in a multi-agent application can specialize at different functions via different
initial configurations; 2) A multi-agent application may require agents to be executed in a standardized
operating procedure (SOP) or a more dynamic workflow; 3)The communication pattern between agents can
be varying from one-to-one or broadcasting (e.g., a discussion group of agents). As a result, developers expect
a handy platform that can provide concise and clear programming patterns when taking care of all the aspects
above, accelerating and facilitating the development cycle. Achieving versatility and handiness simultaneously
†Co-first authors.
∗Equal contribution.
‡Corresponding authors, email address: {yaliang.li, bolin.ding}@alibaba-inc.com
1arXiv:2402.14034v2  [cs.MA]  20 May 2024

--- PAGE 2 ---
requires careful design and taking trade-offs, and it remains a persistent goal for all multi-agent platform
designs.
Aberrations are tinderboxs in a multi-agent system. Although LLMs have advanced rapidly, they still
struggle with issues like hallucination (Rawte et al., 2023; Zhang et al., 2023b) and inadequate instruction-
following (Fu et al., 2019; Zhang et al., 2023a). Besides, an agent can be equipped with various tools, but
those tools introduce additional uncertainties (e.g., accessibility to a database or the search engine). From
the perspective of multi-agent system robustness, any unexpected error or response can propagate to the
whole system, causing a series of cascading effects if not handled properly. Thus, it is crucial for multi-agent
applications to autonomously detect and handle unexpected responses from LLMs. While LLMs may assist in
identifying and managing these errors, it remains a challenge to determine whether they can resolve errors on
their own and to automatically provide the necessary information for error correction. Consequently, designing
fault-tolerant that incorporate LLMs is a key challenge in the development of multi-agent applications.
Supporting agents with multi-modal data, tools, and external knowledge is highly systematic. Besides
generating answers with LLMs, agents are expected to be more versatile, including generating and handling
multi-modal data (Su et al., 2023; Betker et al., 2023), preparing and invoking functions as tools (Yao
et al., 2023; Shen et al., 2024), managing external knowledge banks, and using the retrieved knowledge for
augmentation generation (Lewis et al., 2020a). However, integrating these functionalities in multi-agent
applications requires a comprehensive and systematic approach. Supporting multi-modal content is a complex
endeavor, necessitating considerations for data storage, presentation, user interaction, message transmission,
and communication. Tool utilization of agents requires unifying the function calling pattern and output
parsing, prompting to instruct LLMs, and designing reasoning mechanisms to ensure the tasks can be
accomplished step by step. As for external knowledge, beyond the retrieval-augmented generation (RAG)
techniques, we need to consider how to efficiently share and manage the knowledge in multi-agent scenarios
while leaving enough flexibility for retrieval strategies. While some existing works investigate how those
techniques individually work within specialized agent systems, general platform-level programming interfaces
remain absent.
Distributed applications bring extra programming difficulties and system design challenges. An industrial-
oriented scenario for multi-agent applications is that the agents are owned by different organizations and
run on different machines because the agents are equipped with unique private knowledge or patented tools.
Developing such applications usually requires the developers to have professional knowledge of distributed
system programming and optimization in the design phase. Besides, distributed applications usually require
a great extra effort in the development and testing, especially when debugging and diagnosing issues spread
across distributed processes or agents. Moreover, integrating advanced features like multi-modal data
processing poses additional challenges in a distributed setting, when the agents require different time to
accomplish the sub-tasks or the generated contents are very heterogeneous. Poor distributed system design
can result in excessive communication overhead between agents. Therefore, building distributed multi-agent
applications requires the large efforts of experienced developers and a high barrier for beginners to migrate
their prototypes to a distributed style for optimal efficiency.
To tackle the aforementioned challenges, we introduce AgentScope , a novel multi-agent platform
designed for developers with varying levels of expertise. AgentScope is well-designed with a message exchange
communication mechanism that embodies great usability, robustness, and efficiency. We underscore the
salient features of AgentScope as follows:
Exceptional Usability for Developers . AgentScope is designed with a fundamental emphasis on ease
of use, particularly for developers with varying levels of expertise. By implementing a procedure-oriented
message exchange mechanism, AgentScope ensures a smooth learning curve on multi-agent application
development. To alleviate the programming burdens, AgentScope offers an extensive suite of syntactic
utilities, including various pipelines and an information-sharing mechanism. Besides programming with our
framework, we also improve usability by providing a zero-code drag-and-drop programming workstation , which
can enable those with limited Python programming experience to build their own applications with little
effort. Compared with building the skeleton of the application, prompt tuning can be a more time-consuming
stage in multi-agent application development. In AgentScope, we equip our agents with a set of automatic
prompt tuning mechanisms to relieve such burden. Coupled with rich built-in resources and integrated user
interaction modules, AgentScope makes building a multi-agent application much more enjoyable than ever.
Robust Fault Tolerance for Diverse LLMs and APIs . As the scale and scope of models and APIs
2

--- PAGE 3 ---
expand, a robust fault-tolerance mechanism in multi-agent applications becomes paramount. AgentScope
integrates a comprehensive service-level retry mechanism to maintain API reliability. AgentScope is equipped
with a set of rule-based correction tools to handle some obvious formatting problems in the responses of LLMs.
Moreover, AgentScope offers customizable fault tolerance configurations, enabling developers to tailor their
own fault tolerance mechanism through parameters like parse_func ,fault_handler , and max_retries .
While admittedly, not all the errors can be handled by the aforementioned mechanism, we propose a logging
system with customized features for multi-agent applications as the last safeguard for AgentScope.
Extensive Compatibility for Multi-Modal, Tools, and External Knowledge . With the remarkable
progress of large-scale multi-modal models, AgentScope supports multi-modal data ( e.g., texts, images, audio,
and videos) in dialog conversation, message transmission, and data storage. Specifically, AgentScope decouples
multi-modal data transmission from storage and employs a lazy loading strategy by providing a unified URL-
based attribute in messages. During message transmission, AgentScope only attaches a URL to the message,
and the multi-modal data is loaded only when necessary, such as when being rendered in web UI or invoked
by model wrappers. For tool usage, AgentScope provides a component, called service toolkit , as a one-step
solution for tool usage, including function preprocessing, prompt engineering, reasoning, and response parsing
with fault-tolerance features. To support efficient external knowledge usage, AgentScope provides end-to-end,
highly configurable, and sharable knowledge processing modules for retrieval-augmented generation (RAG),
from data preprocessing to customizable retrieval.
OptimizedEfficiencyforDistributedMulti-AgentOperations . Recognizingthevitalimportanceof
distributed deployment, AgentScope introduces an actor-based distributed mechanism that enables centralized
programming of complex distributed workflows, and automatic parallel optimization. Particularly, the
workflows for local and distributed deployments is a exactly the same one, indicating negligible overhead when
migrating applications between centralized and distributed environments. With such a distribution framework,
AgentScope empowers developers to concentrate on the application design rather than implementation details.
Summary To summarize, AgentScope, a novel multi-agent platform proposed for flexibility and robustness,
includes the following advanced features:
1.AgentScope provides a procedure-oriented message exchange mechanism with a set of syntactic features
to facilitate multi-agent programming, a zero-code drag-and-drop programming workstation, and a set
of automatic prompt tuning mechanisms.
2.The fault tolerance designs of AgentScope enable developers to handle errors elegantly for their
applications.
3.The support for the multi-modal applications reduces the overheads of heterogeneous data generation
and transmission. The service toolkit component facilitates the tool usage of agents in AgentScope, and
the knowledge processing modules provide a flexible solution for agents to handle different information.
4.The actor-based distributed mode of AgentScope can help develop efficient and reliable distributed
multi-agent applications seamlessly.
Roadmap Inthefollowingsections, wenavigatethroughthecorecomponentsandcapabilitiesofAgentScope,
showcasing its role in advancing the development and deployment of multi-agent applications. Section 2
provides an overview, while Section 3 focuses on the user experience. Section 4 introduces the fault tolerance
mechanism in AgentScope. Sections 5, 6, and 7 cover the multi-modal support, tool usage, and retrieval-
augmented generation modules in AgentScope. Section 8 presents our platform’s support for distributed
multi-agent applications. Use cases are presented in Section 9, related work is summarized in Section 10, and
concluding thoughts are recorded in Section 11.
3

--- PAGE 4 ---
2 Overview
2.1 Basic Concepts in AgentScope
This section introduces the primary concepts in AgentScope: message,agent,service, andworkflow . These
four concepts are throughout the platform and all multi-agent applications based on it.
•Message : Messages serve as the carriers for information exchange in multi-agent conversations,
encapsulating the source and content of the information. In AgentScope, messages are implemented as
Python dictionaries with two mandatory fields ( nameandcontent) and an optional field ( url). The
namefield records the name of the agent that generates the message, and the contentfield contains the
text-based information generated by the agent. The urlfield is designed to hold the Uniform Resource
Locator (URL), which typically links to multi-modal data, such as images or videos. Messages with this
field are particularly relevant for interactions with agents that can process and generate multi-modal
content. Each message is uniquely identified by an auto-generated UUID and timestamp, ensuring
traceability. Example 1 shows how the messages can be created, serving as atoms in the inter-agent
communication of AgentScope.
1from agentscope . message import Msg
2
3msg1 = Msg (" Alice ", " Hello !")
4msg2 = Msg (
5 name =" Bob",
6 content =" How do you find this picture I captured yesterday ?",
7 url =" https :// xxx. png"
8)
Example 1: Illustrative examples of message creation in AgentScope.
•Agent: Agents are the primary actors within multi-agent applications, acting as conversational
participants and executors of tasks. In AgentScope, agent behaviors are abstracted through two
interfaces: the replyandobservefunctions. The replyfunction takes a message as input and produces a
response, while the observefunction processes incoming messages without generating a direct reply.
The interplay between agents and messages, as shown in Example 2, forms the operational basis of
AgentScope and is essential for developers to model complex interactions in multi-agent LLMs.
1# agent1 and agent2 are two initialized agents , for example
2# agent1 , agent2 = DialogAgent (...) , DialogAgent (...)
3msg1 = agent1 ()
4msg2 = agent2 ( msg1 )
Example 2: Demonstration of message exchange between agents in AgentScope.
•Workflow : Workflows represent ordered sequences of agent executions and message exchanges between
agents, analogous to computational graphs in TensorFlow, but with the flexibility to accommodate
non-DAG structures. Workflows define the flow of information and task processing among agents,
facilitating parallel execution and efficiency improvements. This concept is essential for designing multi-
agent systems that interact with LLMs, as it allows for the coordination of complex, interdependent
tasks.
•Service Functions and Tools : Note that service functions are closely related to but different from the
concept, tools, in the context of agent design in AgentScope. Service functions refer to the functional
APIs that return a formatted output ServiceResponse , while tools refer to processed services functions
with functionality descriptions and necessary input parameters prepared. We introduce these two
concepts in AgentScope because LLMs require help to invoke service functions as tools. One observation
is that LLMs may need help understanding the functionalities of the service functions precisely and
4

--- PAGE 5 ---
ConfigurationLocalAgent
ModelAPIService FunctionRetrieveDBQueryCodeExec.Write/ReadFileSyntactic sugar:Pipeline,messagehubAgent
WebSearch
FastChat
Model
UtilityManager/WrapperFileManagerImage,Audio,VideoAPIInvocationLoggingHistoryRuntimeManagerActor-basedAgent
MemoryPrompt TuningGPRCComm.Message:name/role,content,urls
ModelWrapperAuto-correctUser-definedhandlerRetryMechanismAgent
CustomizedPostAPIAgentMonitor(Comm,Cost,Time)MultiAgentsLoggerWebUI & AS StudioUserInteraction
ResponseParserDrag-and-Drop Programming WorkstationFigure 1: Architecture of AgentScope.
demand more descriptive information to make accurate decisions. Meanwhile, LLMs can not (reliably)
fill in some input parameters of the APIs, such as the API keys of Bing and Google Search. As a result,
AgentScope defines tools as processed service functions.
2.2 Architecture of AgentScope
We present AgentScope as an infrastructural platform to facilitate the creation, management, and deployment
of multi-agent applications integrated with LLMs. The architecture of AgentScope comprises three hierarchical
layers and a set of user interaction interfaces, as shown in Fig. 1. These layers provide support for multi-agent
applications from different levels, including elementary and advanced functionalities of a single agent (utility
layer), resources and runtime management (manager and wrapper layer), and agent-level to workflow-level
programming interfaces (agent layer). AgentScope introduces intuitive abstractions designed to fulfill the
diverse functionalities inherent to each layer and simplify the complicated inter-layer dependencies when
building multi-agent systems. Furthermore, we offer programming interfaces and default mechanisms to
strengthen the resilience of multi-agent systems against faults within different layers.
•Utility Layer : As the platform’s foundation, the utility layer in AgentScope provides essential services
to support the core functionalities of agents. This layer abstracts the complexity of underlying operations,
such as model API invocation and service functions including code execution and database operations,
allowing agents to focus on their primary tasks. AgentScope’s utility layer is designed with ease of
use and robustness as its utmost priority, supporting versatile operations in multi-agent systems and
providing built-inautonomous retry mechanisms for exception and error handling against unexpected
interruptions.
•Manager and Wrapper Layer : As an intermediary, the manager and wrapper abstraction layer
manages the resources and API services, ensuring high availability of resources and providing resistance
to undesired responses from LLMs. Unlike the utility layer, which provides default handlers, the
manager and wrapper layer also offers customizable interfaces for fault tolerance controls depending
on developers’ needs and the specific requirements of the application. This layer is responsible for
maintaining the operational integrity of the agents, a crucial aspect for LLMs to perform consistently
5

--- PAGE 6 ---
under diverse conditions. Detailed elaboration on the fault tolerance mechanisms is provided in Section
4.
•Agent Layer : At the core of AgentScope lies the agent abstraction, which forms the backbone of the
multi-agent workflow and is the primary entity responsible for interaction and communication. This
layer is designed to facilitate the construction of intricate workflows and enhance usability, reducing the
programming burden on developers. By integrating streamlined syntax and tools, AgentScope empowers
developers to concentrate on the implementation and optimization of agent-based applications that
leverage the capabilities of LLMs. The programming features and syntactic sugars are introduced in
Section 3 with more details.
•User interaction : In addition to the layered architecture, AgentScope provides multi-agent oriented
interfaces such as an annotated terminal presenting basic information, Web UI monitoring the system,
a Gradio-base (Abid et al., 2019) interface that can change a command line application to a graphical
one with only one step and a drag-and-drop zero-code programming workstation (Figure 4). These
interfaces allow developers to effortlessly monitor the status and metrics of the application, including
agent communication, execution timing, and financial costs.
Collectively, the layered constructs of AgentScope provide the essential building blocks for developers to
craft bespoke multi-agent applications that leverage the advanced capabilities of large language models. The
subsequent section will delve into the features of AgentScope that enhance the programming experience for
multi-agent application development.
3 High Usability
The design of AgentScope prioritizes usability, aiming to streamline the development process for multi-agent
with LLMs and to ensure a smooth interaction experience for both users and developers. This section delves
into how AgentScope flattens the learning curve and enhances the programmer’s experience by introducing
intuitive concepts and features that facilitate the creation of complex multi-agent applications.
3.1 Syntactic Sugar for Multi-Agent Workflows
Leveraging basic concepts introduced in Section 2.1, developers are empowered to construct sophisticated
multi-agent applications. Nonetheless, directly coding each agent’s message exchange can become cumbersome,
as shown in Example 3. Recognizing this, AgentScope introduces two syntactic utilities: pipelines and message
hubs, to abstract away the complexity and minimize repetition.
1# set up agents : agent1 to agent5
2# ...
3
4msg = agent1 (Msg ( " Alice " , " Hello ! " ))
5msg = agent2 (msg )
6msg = agent3 (msg )
7msg = agent4 (msg )
8msg = agent5 (msg )
Example 3: Example of programming a sequential workflow with basic concepts in AgentScope.
Pipeline Abstraction The pipeline abstraction reduces repetitive coding by encapsulating patterns of
message transmission, including sequential ,conditional , anditerative exchanges, into simple and reusable
components. With these pipelines, developers can focus on the logic of agent interactions rather than the
boilerplate code. Example 4 illustrates how pipelines can be employed in both functional and object-oriented
styles to create a clear and concise agent workflow. Besides the sequential pipeline in the example, AgentScope
also provides if-else, switch, while-loop, and for-loop pipelines, facilitating the programming of the multi-agent
interactions.
6

--- PAGE 7 ---
1# set up agents : agent1 to agent5
2# ...
3from agentscope . pipelines import SequentialPipeline
4from agentscope . pipelines . functional import sequentialpipeline
5
6# using functional pipeline
7x = sequentialpipeline ([ agent1 , agent2 , agent3 , agent4 , agent5 ], x)
8
9# using object pipeline
10pipe = SequentialPipeline ([ agent1 , agent2 , agent3 , agent4 , agent5 ])
11x = pipe (x)
Example 4: Using functional and object sequential pipeline to construct workflow in AgentScope.
Message Hub for Agent Communication In multi-agent systems, especially when integrated with
LLMs, efficiently managing communication among a group of agents is essential. The message hub in
AgentScope serves as a broadcast mechanism that simplifies group interactions. Developers can initiate a
message hub by defining participating agents and can include initial broadcast messages. When new messages
are generated by the agents within the message hub, they are automatically disseminated to other participants,
as demonstrated in Example 5. This abstraction is particularly useful for multi-agent scenarios involving
LLMs, where dynamic and contextually rich conversations are commonly observed (Du et al., 2023).
1# set up agents : agent1 to agent4
2# ...
3
4greeting = Msg(" host ", " Welcome to the message hub !")
5
6with msghub ( participant =[ agent1 , agent2 , agent3 ],
7 announcement = greeting ) as hub :
8 # Message will be broadcast to agent2 and agent3 automatically
9 agent1 ()
10
11 # Delete agent2 from the message hub
12 hub . delete ( agent2 )
13
14 # Add agent4 into the message hub
15 hub . add ( agent4 )
16
17 # Broadcast message
18 hub . broadcast (Msg (" host ", " Welcome agent4 to join the hub !"))
Example 5: Using message hub with AgentScope.
3.2 Resource-Rich Environment for Agent Development
To further enhance usability, AgentScope is equipped with a rich set of built-in resources, including services,
dedicated agents, and pre-configured examples. These resources are designed to reduce the initial setup effort
and enable rapid prototyping and deployment of multi-agent LLM systems.
Comprehensive Service Integration AgentScope integrates various service functions, such as web search,
database querying, and code execution, to support the tool usage capabilities of agents. These service
functions are essential for building helpful agents with LLMs, as agents often need to draw information
from external sources or execute tasks that go beyond the equipped LLMs’ internal knowledge. Example 6
showcases the seamless conversion of a service into an OpenAI-Compatible JSON format, simplifying the
integration process for developers.
7

--- PAGE 8 ---
1from agentscope . service import ServiceFactory , web_search
2
3bing_search , func_json = ServiceFactory . get ( web_search , engine =" bing ", api_key ="↰xxx ", num_results =10)
4
5print ( func_json )
6# {
7# " name ": " web_search ",
8# " description ": " Searching the given question with bing .",
9# " parameters ": {
10# " type ": " object ",
11# " properties ": {
12# " type ": " object ",
13# " properties ": {
14# " question ": {
15# " type ": " string ",
16# " description ": " The string question to search in Bing ."
17# }
18# }
19# }
20# }
21# }
22
23searching_result = bing_search (" What ’s the date today ?")
Example 6: Converting web search service into the function and JSON format dictionary that agent can use.
Pre-built Agent Templates As cataloged in Table 1, AgentScope offers pre-built agents and ready-to-use
components for tasks like dialogue management, user proxying, multi-modal data handling, and distributed
deployment. These templates serve as starting points for developers to customize and extend, significantly
accelerating the development of multi-agent LLM applications.
Agent Name Function
UserAgent The proxy of the user.
DialogAgent A general dialog agent, whose role can be set by system prompt.
DictDialogAgent A dictionary version dialog agent, who responds in Python dictionary format.
ReActAgent An agent that can reason and use tools
ProgrammerAgent An agent that can write and execute Python code.
TextToImageAgent An agent that generates images according to the requirements.
RpcUserAgent A distributed version user proxy.
RpcDialogAgent A distributed version DialogAgent.
Table 1: Some examples of built-in agents and their functions in AgentScope.
3.3 Multi-Agent Oriented Demonstration Interfaces
Furthermore, AgentScope introduces interaction interfaces tailored for multi-agent systems, as illustrated in
Figures 2 and 3. These interfaces provide a rich multi-modal experience, crucial for systems incorporating
LLMs that handle diverse data types.
Agent Differentiation in User Interfaces To facilitate user interaction with multiple agents, AgentScope
assigns unique colors and icons to each agent, enhancing clarity and visual distinction in both terminal
and web UI (Fig. 3). The “first-person perspective” feature allows users to experience interactions from the
viewpoint of a specified agent, aligning with their role in the application, such as in a game scenario. This
feature not only enriches the multi-agent experience but also mirrors the nuanced interactions that occur in
human-agent and agent-agent dialogues within LLM systems.
8

--- PAGE 9 ---
Figure 2: The dialogue history of a werewolf game in AgentScope.
Figure 3: Multi-modal interactions between agents in web UI.
Monitoring and Cost Management A vital aspect of deploying LLMs at scale is resource management.
AgentScope includes a monitoring module that tracks model and API usage, as well as calculating financial
costs. Developers can customize metrics and set budget limits, receiving automatic alerts when thresholds
are approached or exceeded. This proactive cost management is particularly important for LLMs that may
incur high computational expenses.
AgentScope Gradio Interface Once you have a multi-agent application, executing it in the terminal
may be a concise choice but lacks attraction. In AgentScope, we provide a powerful Gradio-based interface
that is compatible with all AgentScope applications as long as there is a mainfunction as the application’s
entry point. For example, if the main function of the application is in application.py file, then running
“as_studio application.py ” can build a Gradio application with a graphical user interface and support
multi-modal content upload and presentation.
9

--- PAGE 10 ---
3.4 Towards Graphical Application Development
The design mentioned above provides massive convenience for those familiar with Python programming
to quickly develop their multi-agent applications. However, AgentScope takes a step further. AgentScope
provides a drag-and-drop online workstation on which developers only need to drag the module blocks to
compose an application; then, the workstation can generate a configuration file of the application in JSON or
even a piece of Python code. With this feature, those with limited experience with Python programming can
build their multi-agent application without writing any Python code, while those familiar with Python can
instantly obtain a piece of draft code ready for further customization. A screenshot of the online workstation
is shown in Fig. 4, and the idea supporting this implementation is illustrated as follows.
Figure 4: Drag-and-drop programming workstation.
Expressing Multi-agent application with nodes in directed acyclic graph (DAG). Based on the
highly modular design of our basic infrastructure, all the key components can be represented as a node, and
an application can be built by constructing a directed cycle graph (DAG). The execution of the application is
equivalent to triggering and running the nodes in the graph following the traversing order of DAG. Following
the traditional terms, we name such DAG execution as a workflow and name the nodes in the workflow as
workflow nodes . According to their functionality, the workflow nodes are categorized into six different types:
model nodes, agent nodes, pipeline nodes, service nodes and copy nodes.
•Model nodes : Model nodes are designed to be relatively independent of the DAG. They correspond to
the model configurations in AgentScope and work as entries to let users configure their models (LLMs,
embedding models, or multi-modal models) and maintain such information for all the nodes in the
following workflow that need to use the model.
•Service (tool) nodes : These nodes correspond to the services available in AgentScope. Some of them
require additional information to set up, such as Google search and Bing search, which require API
keys; others can be used directly.
•Agent nodes : As the name suggests, agent nodes represent the agents in AgentScope, which means
users need to decide the models, agent name and system prompts for the agent.
•Pipeline nodes : The pipeline node includes the operators of AgentScope, including the message hub
and the pipelines (sequential, for-loop, while-loop, etc.). With such nodes, DAG representations can be
as concise as Python programming.
10

--- PAGE 11 ---
•Message node : The message node is designed for cases where some initial messages are needed, such
as the announcement (initial message) for the message hub.
•Copy node : The copy node is a special kinds of node that replicate the results of a parent node when
its output is needed for multiple subsequent operations.
Execute DAG with JSON or compile to Python. With the nodes above, developers can build
applications by composing DAGs. However, the DAG is highly UI-dependent. Although a DAG can be
represented in some formats (e.g., JSON format recording each node’s information and execution dependency),
we still need to ensure it is as reusable as other applications. To overcome this, AgentScope is equipped with
a data structure called ASDiGraph, which provides two solutions based on it.
•Direct-run : Given a JSON file recording the DAG information, ASDiGraph can parse DAG information
and sort the nodes in topological order. With these sorted nodes, the run function of ASDiGraph can
execute them in order and feed the predecessor’s output to their successors as an application is executed
step by step.
•To-Python compiler : The second solution is to translate the JSON file to a Python script. With
the highly modularized components of AgentScope, the key idea is to rely on internal mappings of
the functionality, required inputs, and expected outputs to small pieces of Python code. Specifically,
each node contains Python code for importing dependent modules, initiating models or agents, and
executing the application logic. ASDiGraph first groups the pieces of importing code and initiating
code, and then it composes the pieces of execution code following the topological order. Therefore,
users will obtain a complete Python script after the ASDiGraph finishes compilation.
3.5 Automatic Prompt Tuning
For a multi-agent system that utilizes LLMs for generation, writing an appropriate prompt requires significant
human effort and expertise (Pryzant et al., 2023), which motivates us to provide automatic prompt generation
and tuning in AgentScope for its high usability. Specifically, AgentScope allows users to generate prompts
based on a simple description of the agent in natural language, update prompts according to contexts, and
enable in-context learning.
System Prompt Tuning When an agent is created, a system prompt should be associated with the agent
to define its roles and responsibilities for following human instructions. For example, a Programmer Agent
might be prompted as “You are proficient in writing and executing Python code”. Meanwhile, a detailed and
informative system prompt can improve agent performance and ensure that the agent performs as expected,
such as “You are proficient in writing and executing Python code. You prefer to write the code in a modular
fashion and provide unit tests for each module”. With AgentScope, users only need to provide a simple
description of the agent when creating the agents, and AgentScope can automatically generate such helpful
system prompts using built-in tools based on LLMs, as shown in Example 7.
1# set up agents with automatic prompt generation
2# ...
3from agentscope . agents import ProgrammerAgent
4
5# Load model configs
6agentscope . init ( model_configs =" model_configs . json ")
7
8# Create a programmer agent
9programmer_agent = ProgrammerAgent ( name =" assistant ", auto_sys_prompt =True ,
10 model_config_name =" my_config ",
11 sys_prompt ="an assistant that can write Python code ")
Example 7: Initialize a programmer agent with automatic system prompt generation.
11

--- PAGE 12 ---
Besides, AgentScope provides interfaces for system prompt updates, which include manually setting by
users or automatically adjusting based on the context. As a promising future direction, meta-prompting
techniques (Pryzant et al., 2023; Suzgun and Kalai, 2024) can also be integrated into AgentScope, which
might involve integrating an evaluator to provide guidance for automatic prompt optimization.
In-Context Learning Providing multiple demonstrations to the LLMs can greatly enhance their ability
to follow instructions, particularly when we want them to complete specific downstream tasks (Dai et al.,
2023; Wei et al., 2022). AgentScope provides a simple switch to turn on/off the in-context learning behavior
for agents that utilize LLMs. When users choose to apply in-context learning, they only need to provide
demonstration candidates and configure how to match the most suitable ones, as illustrated in Example 8.
AgentScope offers several widely-used and useful matching approaches, such as random selection, similar
questions, and similar answers, and allows for user customization.
1# set up agents with in - context learning
2# ...
3from agentscope . agents import ReActAgent
4from agentscope . utils . common import load_demo_data
5
6# Load model configs
7agentscope . init ( model_configs =" model_configs . json ")
8
9# Load demonstrations
10react_pairs = load_demo_data (" my_demos . txt")
11
12# Create a reAct agent
13react_agent = ReActAgent ( name =" react_agent ", enable_icl =True ,
14 demos = react_pairs , matching_approach =" random ")
Example 8: Enable in-context learning when creating an agent.
4 Fault-Tolerant Mechanisms
In the realm of multi-agent systems, particularly those interfacing with diverse open-source LLMs with various
instruction-following capabilities, fault tolerance is a key property to ensure seamless operation. AgentScope
is engineered to autonomously handle a wide range of errors with minimal human intervention required,
drawing upon a comprehensive fault-tolerant infrastructure that is acutely aware of the complexities involved
in multi-agent coordination and LLM dependencies.
Error Classification and Handling Strategies Our approach begins with a methodical classification of
errors into distinct levels, each with tailored handling strategies:
•Accessibility errors : In AgentScope, an agent’s functionalities rely on different kinds of services, but
those services may be subject to temporary inaccessible errors. These errors may be caused by model
instability or network conditions. For example, the model APIs may return a timeout error when there
is traffic congestion during busy hours, or a database on a remote machine may be inaccessible because
of transient network outages.
•Rule-resolvable errors : As many multi-agent applications require information exchange between
services or agents, it is essential to follow the protocols for those communications, e.g., in JSON format.
However, as the responses of LLMs are not fully controllable yet, their return may not follow the format
required in the prompts. For example, we may expect a response from an LLM in JSON, but a right
brace is missed at the end of the return, leading to parsing failure. As the JSON format has clear
specifications, it is reasonable to assume that a subset of these errors can be resolved by correcting the
format according to the rules to meet the specifications.
12

--- PAGE 13 ---
•Model-resolvable errors : When a multi-agent system handles some complicated tasks, the ability
of the agent to understand the input, make decisions, and deliver outputs mostly depends on the
capability of LLMs. In some cases, the responses of LLMs are in the expected format, but the content
has problems, such as argument errors, semantic errors, or programming mistakes. It is hard to have
pre-defined rules to regularize those responses for diverse tasks, but it has also been shown that such
errors may be detected and recovered by further interaction with the LLMs.
•Unresolvable errors : Eventually, there must be some errors that cannot be detected or solved. A
typical example is that the API key of an LLM is expired or unauthorized. The agents relying on it or
the system can do nothing to resolve such errors without human intervention.
Fault Tolerance mechanisms in AgentScope In AgentScope, we provide different mechanisms to
encounter the errors summarized above.
•Basic auto-retry mechanisms : To combat accessibility errors, AgentScope’s API services and model
wrappers are fortified with retry logic that developers can customize, such as setting the maximum retry
count. This ensures that agents can recover from sporadic disruptions and maintain their operational
continuity.
•Rule-based correction tools : The rule-based correction tools are introduced into AgentScope to
efficiently and economically handle some easy-to-fix format errors in the responses of LLMs. For
example, we establish a set of default rules in AgentScope that can complete unmatchable braces and
extract JSON data from strings. Such rule-based correction tools can correct some of the common
rule-resolvable errors without calling LLM APIs again, which means shorter processing time and no
LLM API call cost.
•Customizable fault handlers : AgentScope also integrates flexible interfaces of fault handlers in model
wrappers for developers to define how to parse the responses from LLMs and handle the unexpected
outputs. Application developers can configure their fault handling mechanism by providing a parsing
function, fault handling function, and the number of chances given to LLMs through configurable
parameters (i.e., parse_func andfault_handler andmax_retries ) when invoking LLMs. With such
developer-friendly design, AgentScope can be configurably robust to rule-resolvable errors (when the
build-in rules fail to handle) and some model-resolvable errors that can be detected and handled by a
single agent (e.g., distilling a verbose summary to a more concise one).
•Agent-level fault handling : There are model-resolvable errors that require more advanced LLM
usages or agent-level interaction to recover. For example, detecting semantic errors, which usually
include factual inaccuracy, logical inconsistency, contextual incoherence, unreasonable inference, and
inappropriate vocabulary usage, is challenging since they may not necessarily trigger immediate red
flags within the system’s existing validation processes. Developers can utilize the agent’s ability in
AgentScope (e.g., memory module and message hub) to critique for semantic error checking such as
self-critique, pairwise critique, and human-augmented critique.
•Logging system : Although the unsolvable errors are too tricky for the system to handle, AgentScope
provides an improved logging system for developers to quickly monitor and identify the problems in
multi-agent applications. The logging system in AgentScope has customized features for the multi-agent
application scenarios, including adding a logging level called CHATfor logging conversations between
agents, providing formatted logs with various execution information, and a WebUI user interface to
facilitate monitoring.
5 Multi-Modal Applications
The integration of multi-modal data is indispensable for advancing the capabilities and applications of
multi-agent with LLMs. AgentScope is designed to seamlessly support various data modalities, leveraging the
diverse inputs and outputs that contemporary LLMs can process and produce.
13

--- PAGE 14 ---
Figure 5: The generation, storage, and transmission of Multi-modal data in AgentScope.
Management of Multi-Modal Data In a running AgentScope application, the lifecycle of multi-modal
data is carefully managed. This management includes the generation, transmission, and storage of multi-modal
data—all facilitated through a decoupled architecture using URLs and a local file manager system. Fig. 5
exemplifies this process, including data originating from user inputs or model generations, data storage and
retrieval, and data sharing.
•Multi-modal data generation : There are two primary sources of multi-modal data in AgentScope.
One source is simply the locally stored multi-modal files, which can be used by either user proxy agents
or general agents with access to the local file system. Another source is the model-modal content
generation models. Our model APIs and the model wrappers integrate the most popular multi-modal
models, such as the text-to-image content generation models like OpenAI’s DALL-E, and conversely, the
image-to-text image analysis models, e.g., GPT-4V. Besides the built-in APIs, developers can introduce
their favorite multi-modal models and customize their own model wrappers, with our ready-to-use
examples as the starting points. This customization process is streamlined in AgentScope and benefits
from our modular design, allowing developers to connect their multi-modal services with minimal effort.
•Multi-modal data storage : As mentioned above, multi-modal data in the multi-agent application
can be either from ready-to-use local files or generated by multi-modal models. When a multi-modal
model wrapper is invoked to generate multi-modal data, it first saves the data locally with the help
of the file manager and returns a local URL when it receives multi-modal data from the model API
service.
•Multi-modal data transmission : AgentScope simplifies the process of multi-modal data sharing
between agents by allowing agents to encapsulate local or remote URLs in multi-modal messages to
indicate the actual storage locations of the data. The receiver agents can load the multi-modal data
through the URLs when ready to process those.
The benefits of introducing URLs in the messages when agents share multi-modal data are three-fold.
Firstly, it can minimize the message size to avoid potential errors or delays because of the network bandwidth
and enable the receiver agent to load the data on demand. Secondly, if there is other text information in the
message, the downstream agents can potentially prioritize or parallel the processing of the text information
to/and the processing of multi-modal information. Last but not least, such URL-attached messages can also
facilitate the multi-modal data demonstration, which will be introduced in the following section.
Multi-Modal Interaction Modes With the implementation of URL-attached messages, AgentScope
empowers users to interact with multi-modal systems via accessible interfaces such as terminal and web
UI. Fig. 3 showcases the user’s ability to interact with multi-modal data within interaction modes. In the
terminal, users can conveniently access locally stored data by activating the provided URLs. The web UI
further enhances user experience by providing an intuitive platform to view and analyze multi-modal content,
aligning with the expectations of modern web applications.
14

--- PAGE 15 ---
Through AgentScope, developers are equipped to tailor model API services and wrappers to their individual
needs, forge applications that handle diverse data modalities, and provide users with the necessary tools
to engage with multi-modal agents effectively. This comprehensive support for multi-modal applications
positions AgentScope as a versatile and powerful framework for harnessing the full potential of multi-agent
LLMs, broadening the horizons for developers and researchers alike in creating sophisticated and interactive
AI systems.
6 Tool Usage
Tool usage is an important feature for LLM-empowered agents, allowing agents to perceive, change their
environment, and handle more complex tasks (Wu et al., 2023; Paranjape et al., 2023; Parisi et al., 2022).
For simplicity, we treat using tools as equivalent to calling service functions by LLMs. In AgentScope, the
tool usage module is designed based on ReAct algorithm (Yao et al., 2023), which allows for the generation of
interleaved reasoning and task-specific actions, along with a core component—service toolkit. Such design
features high compatibility, extensibility, robustness, and re-usability, spanning from function pre-processing,
prompt engineering, reasoning, and response parsing to agent-level fault tolerance. Specifically, in AgentScope
the tool usage involves four steps:
•Function Preparation : Parse the provided service functions, and pre-process the functions so that
LLMs can utilize them directly.
•Instruction Preparation : Prepare instruction prompt for tool usage to elaborate the available tool
functions to LLMs, including the purpose, arguments, constraints of the function, and its calling format.
•Iterative Reasoning : LLMs generate strategic reasoning, make decisions for tool usage, and respond
in the required format.
•Iterative Acting : Parse and check the LLM response according to the calling format, invoke functions
if the response adheres to the expected format, or generate a detailed error message to LLMs for
correction.
In the above process, the service toolkit module is responsible for tool functions management, pre-
processing, prompt engineering, response parsing, and function execution, and it is highly modular and
extensible. Fig. 6 demonstrates how the service toolkit works in AgentScope when users post a query.
Function Preparation . In function preparation, the target is to preset the developer-specific arguments,
andtogenerateready-to-usefunctionsandtheircorrespondingformatteddescriptionforLLMs. InAgentScope,
developers only need to register their functions with preset arguments in the service toolkit. As shown in
Fig. 6, developers choose the Bing search function and provide the API key during registration. Then the
service toolkit will automatically generate the processed ready-to-use function and its description in JSON
schema format. The descriptions will be used to generate tool instructions in natural language. Optionally,
some model APIs ( e.g., OpenAI and DashScope Chat API, etc.) can receive the JSON schema descriptions
directly, which we will discuss in Sec. 6.1.
Instruction Preparation For novice developers, the service toolkit builds in templates for tool instruction
and calling format for tool usage, as demonstrated in Fig. 6. The tools instruction template lists each
function with a clear description and the parameters it requires, leading to an easy understanding of their
functionalities. On the other hand, the calling format, as demonstrated in Fig. 6, requires a JSON dictionary
in a Markdown fenced code block with thought,speak, andfunction fields. During LLM generation, we expect
thethoughtfield will provide a reasoning process for the next acting, including analyzing the current situation,
selecting candidate functions, and correcting errors.
Iterative Reasoning In AgentScope the reasoning and acting steps are iterative. As stated above, in the
reasoning step LLMs should analyze the current situation and decide the next actions. Developers only need
to construct prompts with the tool instructions and the calling format instructions and feed them into the
LLMs. Such design provides high reusability and flexibility, that is, the service toolkit is task-independent
and can be adapted to different tasks and scenarios very easily.
15

--- PAGE 16 ---
Figure 6: The ReAct-based tool usage module in AgentScope.
Iterative Acting In the acting step, the service toolkit will parse the LLM response according to the
calling format, extract the selected function, and execute it with the corresponding arguments. If the response
conforms to the format requirements, and the function executes successfully, the service toolkit will return the
execution results directly, which LLM can generate a response based on in the next reasoning step. Otherwise,
we break down errors into response parsing errors, function execution errors, and other runtime errors. For
response parsing and function execution errors, we expose them to LLM with detailed error information for
correction in the next reasoning-acting iteration, leaving the other runtime errors to developers.
6.1 Customization for Experienced Developers
AgentScope supports developers in highly customizing their tool instructions and function calling formats. To
customize tool instruction, the service toolkit in AgentScope provides JSON schema descriptions automatically,
which provides a structured way to elaborate how a function should be called, including its name, purpose,
arguments, and other relevant details. These formatted descriptions can be directly fed into some advanced
model APIs, e.g. OpenAI and DashScope Chat APIs. For users who want to deeply customize their tool
instructions, they can construct instructions based on the JSON schema descriptions.
Besides the tools instruction, AgentScope also provides great flexibility, that is, AgentScope provides
various model response parsers, including Markdown fenced code blocks, JSON object code blocks, and
customizable tagged contents, as demonstrated in Fig. 7. For the users who want to customize the function
calling format, the Markdown fenced code blocks and JSON object code block allow them to quickly construct
the format instruction and parse the LLM response according to the content types. For users who want to
obtain multi-fields from LLMs, the multi-tagged contents allow the developers to combine different tagged
contents at will and extract them easily from the response into a Python dictionary. With these parsers,
developers are able to customize their own calling format easily.
16

--- PAGE 17 ---
Figure 7: Parsers in AgentScope.
7 Agents with Retrieval-Augmented Generation
With the growing applications of LLMs, some circumstances require knowledge that is not contained in the
training data set, for example, knowledge in highly professional domains or not publicly available. Even
given the required datasets, the fine-tuning or re-training of the LLMs is still expensive. Accordingly,
retrieval-augmented generation (RAG), an innovative approach that aims to boost the power of LLMs in
customized knowledge domain (Gao et al., 2023; Lewis et al., 2020b), is gaining increasing attention in the
literature.
The methodology of RAG can be considered as inserting a pre-processing step into the common utilization
pipeline of LLMs. That is, given a collection of documents that contains needed knowledge, a similarity-based
index is built, and the original user input is zipped with the most relevant pieces of information and converted
into prompts, then fed to the LLMs. Therefore, the methodology of RAG involves multiple phases, that
is, the collection of documents that contain the necessary information, the segmentation of the documents,
the indexing of the segments (a.k.a. chunks or nodes), the similarity-based index retrieval, the fusion of the
original query (i.e. user input) and retrieved results, the composing of prompts, and lastly, generation of
reasonable responses from the LLM based on the informative prompts.
In short, RAG embraces both the power of information retrieval and the generative capabilities of LLMs,
and provides enhanced LLM service with customized domain knowledge at low cost. Meanwhile, assisted by
RAG, the hallucination could be avoided and the factual accuracy could be significantly improved.
As a developer-oriented multi-agent platform, AgentScope provides comprehensive RAG support for multi-
agentapplications. GivenpopularRAGframeworkssuchas e.g.,LlamaIndex(Liu,2022),LangChain(Langchain-
AI, 2023), etc., AgentScope is designed with highly flexible abstracted processes to be compatible with those
frameworks. In what follows, we introduce several key features of AgentScope RAG.
Configurations in One-Stop Due to the complexity of the working pipeline, the configuration of RAG
services is highly convoluted and often headache-some for users. While the RAG service provided by
AgentScope is comprehensive and also involves multi-agent workflow, AgentScope provides a simple one-stop
configuration solution by using a single .jsonfile to group all RAG-related configurations.
With this highly systematized configuration interface, users only need to focus on constructing the
workflow, without being distracted by the repetitious configurations. For example, the RAG-empowered
agents may involve a wide collection of knowledge bases that need to be configured in detail. With this
“One-Stop” feature, the corresponding adjustments of the modules (which may lead to different performances)
are integrated as the editing over simply one single file. Moreover, this solution also naturally adapts to the
AgentScope Workstation, in which the dialog-box-based configuration can be easily exported to executable
files and later loaded in Python programs.
Knowledge-oriented Data Managements The application of RAG in multi-agent circumstances is more
complicated compared with the application on a single agent. For example, for a single agent, one can directly
17

--- PAGE 18 ---
encapsulate the needed knowledge to the agent. Therefore, the initialization of each RAG agent involves the
whole pipeline of conversion from the original documents to vector-stored indexes with retrievers. However,
in multi-agent applications, it is natural for agents to share knowledge, such that repeatedly executing index
computation for each agent is needless. Therefore, AgentScope introduces the notion of knowledge banks .
Knowledge banks can be considered as a collection of knowledge containers, where the smallest manageable
unit is a customized object (which will be referred to as a “RAG object” in the following context). The
workflow starts with initializing the knowledge bank, which mainly relies on the information contained in
the.jsonconfiguration file. The information includes the directory and extensions (such as .pyor.md)
of documents, the granularity and choice of segmentation tools (e.g. the splitters in Llama-Index ) for the
documents, and choice of model for indexing. After the initialization, the computed results are persisted
to the designated directory for later use and we also obtain a knowledge bank consisting of RAG objects,
each marked with a unique knowledge_id , associated with the index of the corresponding documents, an
information retriever, and other attributes. Note that AgentScope permits each RAG agent to load with
more than one RAG object.
Agents with RAG The application of agents with RAG in AgentScope is very simple. For example, we
first need to initialize a KnowledgeBank with some RAG framework, e.g. LlamaIndex, and all the documents.
Then, we configure an RAG agent and load it with the knowledge bank. After that, the initialization is
completed and we can use the RAG agent like any other agent in AgentScope. It is worth noting that if
KnowldgeBank is obtained with LlamaIndex framework, then we need to use LlamaIndexAgent (inherited from
RAGAgentBase ). The readers may refer to Section 9.5 for a concrete application sample, which implements a
copilot for AgentScope using our RAG agents. Overall, the key features of RAG agents are summarized as
follows:
•The RAG agent is permitted to load several RAG objects (i.e. any subset of the knowledge bank). One
can choose to load the original RAG objects from the knowledge bank (in such case, the modification
to an object may affect all the agents who use it) or a copy of it.
•While agents are initialized with a KnowledgeBank object, it is permitted for the agents to update
knowledge in time. The operations include inserting, deleting, or replacing knowledge pieces. Moreover,
we provide a solution by monitoring certain directories and keeping the RAG object updated with the
contents in the directories.
•The fusion mechanism of the retrieved results from multiple RAG objects is fully customizable. For
example, since knowledge may be of different importance or trustworthiness, the agent can set weights
for information retrieved from different RAG objects for subsequent processes.
•RAGs agents are permitted to recompose the query in configurable repeats and conduct multiple queries
for more comprehensive answers.
8 Actor-based Distributed Framework
Efficiency and extensibility are essential when building industry-level applications on multi-agent systems.
The inference speed of the agents in multi-agent applications may vary dramatically. For example, suppose
an agent in a multi-modal application employs a text-to-video model. In that case, its response time may be
significantly longer than that of an agent designed to fill in details of stories. Parallelization , as a classic idea,
should be introduced to boost efficiency. Besides, multi-agent applications can comprise agents physically
distributed on different machines. A typical use case is that a company can wrap its patented techniques
or private knowledge bases into an agent on their local machines connected to the internet and provide
autonomous services to other entities via agent interactions.
However, when it comes to multi-agent systems, a challenge is that developers need to make decisions
between the following two pairs of technology roadmaps. As there is no free lunch, any combinations have
their benefits and drawbacks.
18

--- PAGE 19 ---
•Centralized v.s. decentralized coordination. In the context of the distributed system, centralized
coordination means multiple computation nodes being managed by a central node, such as the server-
client model. A multi-agent mechanism with centralized coordination means the execution of the agents
is scheduled by, and the messages between agents are forwarded by a central coordination component.
On the contrary, decentralized coordination does not rely on any central component to schedule or
forward messages, but the agents in such a system can be invoked automatically and send messages
directly to the downstream agents for further processing.
While centralized coordination is a straightforward style that can be understood and is easy to debug,
its disadvantages include vulnerability to central node failures, imposing heavy traffic on the central
node, and difficulty in scaling or extending to complicated applications. In contrast, decentralized
coordination may require extra effort to develop and maintain but has a higher robustness against
failure of any single node.
•Static vs. dynamic workflow design. A similar comparison can be found between the static computational
graph employed in early versions of TensorFlow (Abadi et al., 2016) and the dynamic computation graph
used in PyTorch Paszke et al. (2019). In the context of multi-agent applications, the choice between a
static and dynamic workflow is akin to choosing between pre-compiled and interpreted execution. The
static workflow design can enable the optimization of the workflow graph level for running time and
resource allocation. However, static workflow design requires the workflow graph to be known before
execution, which limits the adaptation into applications, especially the ones with loop structures in the
design. In contrast, dynamic workflows offer greater flexibility at the expense of optimization potential.
This is particularly relevant when dealing with large language models where execution paths can change
based on the input data or model inference results.
Figure 8: An example of a distributed application in AgentScope, illustrating various processes as denoted by
different colors.
Distributed mode in AgentScope. AgentScope balances these technology roadmaps by implementing
an actor-based distributed mode that is mindful of the unique needs of multi-agent LLM systems, with the
following important features:
•Automatic parallel optimization without static graphs. AgentScope leverages the actor model to enable
automatic parallel optimization, allowing developers to circumvent the intricacies of static graph
programming. This approach seamlessly aligns with the dynamic and often unpredictable nature of
LLMs, where the computational graph can alter based on evolving contexts and dialogue states.
19

--- PAGE 20 ---
•Programming workflows with minimal complexity. In contrast to traditional actor models and peer-to-
peer (P2P) implementations that require intricate execution ordering for distributed agents, AgentScope
simplifies workflow programming to a single procedural style within a Python function. This design
significantly flattens the learning curve for developers, making the construction of sophisticated multi-
agent LLMs more accessible.
•Hybrid local and distributed agent support. AgentScope’s flexibility extends to supporting a hybrid mode
where some agents operate locally while others are distributed. This feature is particularly beneficial
when integrating LLMs with varying computational requirements, allowing for resource-intensive models
to be distributed while less demanding agents remain local, all without the developer needing to
differentiate between the two during implementation.
Specifically, we can concisely describe how AgentScope incorporates the actor model as the following. In
this conceptual framework, an “actor” acts as a stand-alone entity that processes computation upon receipt of
all necessary messages. This paradigm ensures that each agent, corresponding to an actor, only engages in
computation once the required input messages are ready, thus achieving automatic parallel optimization.
However, the actor-model-based workflow presents a programming challenge: the variable (i.e., messages)
passing between actors (i.e., agents) may be placeholders without any practical meaning at the beginning. To
alleviate this, AgentScope introduces the “placeholder” message, a novel data structure that allows the main
process to continue without blocking, while preserving the necessary information to retrieve real values later
(Fig. 8). This mechanism is particularly advantageous for multi-agent LLM systems, where execution flow
must adapt to the variable output of language models.
1# set up distributed agent : agent1
2...
3
4input_msg = Msg(" system ", " Which agent should respond next , agent2 or agent3 ?")
5
6# the variable choice is a placeholder
7choice : placeholder = host_agent ( input_msg )
8
9if choice [" content "] == " agent2 ":
10 response = agent2 ()
11elif choice [" content "] == " agent3 ":
12 response = agent3 ()
Example 9: Demonstrating the use of placeholders in control flow within AgentScope.
Another series of challenges arise when placeholders are used within control flow statements (e.g., if-else,
loops) without their real values. An example is shown in Example 9, where a placeholder is required to make
decisions. In these circumstances, AgentScope temporarily blocks the process to retrieve its actual value,
thus ensuring the continuity of the control flow.
The actor-based distributed mode in AgentScope not only provides automatic parallel optimization and
simplifies the developer experience but also demonstrates high efficiency for distributed multi-agent LLM
applications. It enables developers to focus on implementing agent logic, particularly the “reply” function,
without concern for underlying distributed complexities. This streamlined approach to distributed multi-agent
systems can advance the field of LLMs by making it easier to develop, run, and debug sophisticated and
scalable multi-agent architectures.
One-click deployment in AgentScope. To further ease the distributed deployment, AgentScope
provides agent server and a unified message center, named AgentScope Studio .
Specifically, the agent server is hold in remote machines, which receives requests from AgentScope
applications, and initialize their required agents in the deployed machine automatically. That means,
developers can set up agent instances remotely, without programming in different machines. Such feature
provides high flexibility, especially for large-scale simulations, where a large number of agent instances will be
set up in remote machines.
For AgentScope Studio, it provides a unified display interface for distributed multi-agent applications,
where messages from all distributed agents will be gathered and displayed in this studio, and allows developers
20

--- PAGE 21 ---
to forward these messages to their own display interface. Besides, AgentScope studio supports agent servers
management, that is, in this studio developers can check the deployment of distributed agents, open or close
agent servers remotely. With this studio, developers can manage their applications much more easily.
9 Signature Applications of AgentScope
As introduced in the previous sections, AgentScope is a multi-agent platform delicately designed for integrating
and coordinating large-scale models in a user-friendly and fault-tolerant manner, and it is an ideal platform for
a vast spectrum of applications. AgentScope can implement applications spanning from a simple single-agent
vs. user dialog to complicated interactive multiplayer role-play games like werewolf. Moreover, beyond
centralized deployments, AgentScope can extend to distributed conversations that involve parallel operations
across multiple machines. In this section, we look into several signature applications of AgentScope that
persuasively illustrate the framework’s outstanding and diverse capabilities. All examples referenced herein
are accessible in our GitHub repository for community use and contribution.
9.1 Dialog Agents: Basic Conversation
The simplest yet most fundamental application of AgentScope is the basic conversation , where the user
directly interacts with the dialog agent. This application is an excellent starting point for fresh users of
AgentScope to quickly capture the core message-passing mechanism in our framework.
Thebasic conversation example demonstrates the usage of two fundamental built-in agents in AgentScope,
theUserAgent and DialogAgent , which facilitate inputs from the user and the responses from LLMs,
respectively. Normally, as illustrated in Example 10, the first step of all applications is the initializa-
tion, which is to load the model configurations (specified in the model_configs.json file) through the
initinterface of AgentScope, which assigns the LLM-empowered agents with selected models. Currently,
AgentScope is compatible with various platforms and APIs, including but not limited to standard OpenAI
chat/embedding/DALL-E, HuggingFace, ModelScope, and a collection of locally hosted models with FastChat,
vllm, and Flask. Moreover, the initinterface also specifies detailed options such as file storage, logging,
agent configures, etc.. With all the configurations settled, it is ready to construct the conversation flow,
i.e. the message-exchanging mechanism between the user/agents, which is an essential building block for
all agent-based applications. In this workflow, the AI agent will always respond to the user’s input, the
conversation could form an endless loop until the user decides to opt-out.
To implement more sophisticated applications, AgentScope facilitates pipelines , which provide a well-
structured and scalable framework for complex agent interactions (in terms of messages). As illustrated in
Example 11, we can implement the basic conversation example with a sequential pipeline or loop pipeline.
Readers may also refer to Appendix A for conversation history while running the demo codes.
9.2 Dialog Agents: Group Conversation with Mentions
Beyond the basic conversation between a user and a single dialog agent, AgentScope supports group
conversations. To improve the interactivity, we introduce the “mentions” feature, which allows the user or
agent to call a specific agent by simply “@ agent_name ”. The “mention” feature is supported by applying the
filter_agents function, which screens the message and identifies if any agent is mentioned in the message
content.
In this example, we first initialize the agents involved in the conversation as shown in Example 12. Here,
the characteristics of the agents can be customized in the agent_config.json file, e.g. using sys_prompt
to customize the reaction style or functionality of the agents. Also, we utilize the message hub ( msghub) to
facilitate message deliveries among a group of agents. The msghuballows the sharing of public information
(e.g. an announcement) and permits agents to broadcast messages to all agents. The conversation would end
if a timeout limit is reached, or the user types in “exit”.
21

--- PAGE 22 ---
1import agentscope
2from agentscope . agents import DialogAgent , UserAgent
3
4# read model configs
5agentscope . init ( model_configs ="./ openai_model_configs . json ")
6
7# Create a dialog agent and a user agent
8assistant_agent = DialogAgent (
9 name =" Assistant ",
10 sys_prompt =" You are a helpful assistant ",
11 model ="gpt -4"
12)
13user_agent = UserAgent ()
14
15# Basic version
16x = None
17while x is None or x. content != " exit ":
18 x = assistant_agent (x)
19 x = user_agent (x)
Example 10: Code example of the basic conversation example.
1# Advanced version with sequential pipeline
2from agentscope . pipelines . functional import sequentialpipeline
3x = None
4while x is None or x. content != " exit ":
5 x = sequentialpipeline ([ dialog_agent , user_agent ], x)
6
7# Advanced version with while loop pipeline
8from agentscope . pipelines . functional import whilelooppipeline
9x = whilelooppipeline (
10 [ assistant_agent , user_agent ],
11 condition_func = lambda _, x: x is None or x. content !=" exit ",
12 x= None )
Example 11: Pipeline-based implementation of the basic conversation example.
9.3 Dialog Agents: The Werewolf Game
Group conversation and the mentioning feature are fundamental building blocks for multi-agent applications.
Here we present a more sophisticated application, the werewolf game , which is a popular multiplayer interactive
role-play game. We aim to implement the game with AgentScope in only one hundred lines of code. This
example involves six players divided into two opposing teams, the werewolves, and the villages. After rounds
of conversations and discussions, the game ends when all werewolves are eliminated (i.e. villager victory) or
the number of werewolves equals or outnumbers the villagers (i.e. werewolf victory).
As an LLM-empowered role-play game, we start the game settings with allocation for the roles and
initialization for the agents. As shown in Example 13, AgentScope supports a quick setup, which consists of
default agent configurations for a user to instantiate the agent objects with corresponding roles in one click,
the detailed settings are included in the agent_configs.json file.
It is worth noting that the werewolf game is based on the group conversation capability of AgentScope,
such that the werewolves could chat in the “night phase” and all participants could discuss during the “day
phase”. Similar to the group conversation example, the message hub (msghub) of AgentScope is used to
facilitate the conversations. As shown in Example 13, after the host (moderator) makes an announcement, the
werewolves discuss for at most MAX_WEREWOLF_DISCUSSION_ROUND rounds and conclude once an agreement
is reached. Here, the agents are required to use an “agreement” attribute in the response message, which
is enforced in the role-defining prompt. For complete workflow, an example of dialogue history, and more
related information, please refer to Appendix B.
22

--- PAGE 23 ---
1import agentscope
2
3# Read model and agent configs , and initialize agents automatically
4npc_agents = agentscope . init (
5 model_configs ="./ configs / model_configs . json ",
6 agent_configs ="./ configs / agent_configs . json ",
7 )
8user = UserAgent ()
9agent = list ( npc_agents )+[ user ]
10...
11# We use msghub to coordinate the conversations , ‘‘hint ’’ is a message notified↰to all agents
12with msghub ( agents , announcement = hint ):
13while True :
14 try :
15 x = user ( timeout = USER_TIME_TO_SPEAK )
16 if x. content == " exit ":
17 break
18 except TimeoutError :
19 x = {" content ": ""}
20 logger . info (
21 f" User has not typed text for "
22 f"{ USER_TIME_TO_SPEAK } seconds , skip .",
23 )
24 # if user mentions any npc_agent in the message , it will be added to the↰speak_list
25 speak_list += filter_agents (x. get (" content ", ""), npc_agents )
26
27 # if the speak_list is non -empty , the mentioned agents will respond in a↰sequential manner
28 if len ( speak_list ) > 0:
29 next_agent = speak_list . pop (0)
30 x = next_agent ()
31 # otherwise , all agents will respond one by one .
32 else :
33 next_agent = select_next_one ( npc_agents , rnd )
34 x = next_agent ()
35 # if the response mentions any agent , it will be added to the speak_list
36 speak_list += filter_agents (x. content , npc_agents )
Example 12: Code example of the group conversations.
9.4 Distributed Deployed Agents
We have seen applications regarding conversations involving dialog agents, but those examples are fundamental
in the sense that the agents are deployed in a centralized manner, that is, the agents are hosted on a single
machine and in a single process. To allow agents to be hosted by separate machines or processes, AgentScope
allows agents to be distributedly deployed in two modes, the single-machine multi-process mode, and the
multi-machine multi-process mode. In what follows, we present examples to demonstrate this feature.
Single-Machine Multi-Process Mode : For this mode, all agents are deployed on a single machine, but
running in separate processes. For better comparison, we implement the basic conversation example in this
mode (see Example 14 for the complete code). Compared with Example 10 and 11, we use the to_dist
function to convert the current agent instance into a distributed version. Then, the assistant_agent would
be deployed on a local host with an automatically allocated port. Besides the aforementioned differences, the
single-machine multi-process mode is identical to local deployment, yet it has been optimized for parallel
execution.
Multi-Machine Multi-Process Mode : To demonstrate this mode, we initiate the agent service (a
DialogAgent ) on a remote machine (as shown in Example 15), and constructs a workflow (as shown
Example 16). One may note that the only difference comparing to the local deployed mode is that the agent
server needs to be connected using specified URLs and ports before establishing the workflow.
Overall, for AgentScope, we can smoothly convert from the local deployment mode to the distributed
23

--- PAGE 24 ---
1import agentscope
2# Read model and agent configs , and initialize agents automatically
3survivors = agentscope . init (
4 model_configs ="./ configs / model_configs . json ",
5 agent_configs ="./ configs / agent_configs . json ",
6)
7
8# Define the roles within the game .
9roles = [" werewolf ", " werewolf ", " villager ", " villager ", " seer ", " witch "]
10
11# Based on their roles , assign the initialized agents to variables .
12wolves , villagers , witch , seer = survivors [:2] , survivors [2: -2] , survivors [ -1] ,↰survivors [ -2]
13...
14# Night phase : werewolves discuss
15hint = HostMsg ( content = Prompts . to_wolves . format ( n2s ( wolves )))
16with msghub ( wolves , announcement = hint ) as hub :
17 ...
18 for _ in range ( MAX_WEREWOLF_DISCUSSION_ROUND ):
19 x = sequentialpipeline ( wolves )
20 if x. agreement :
21 break
22 ...
Example 13: Code example of the werewolf game.
1from agentscope . agents import UserAgent , DialogAgent
2import agentscope
3# we use . to_dist () to convert the agent to distributed mode .
4assistant_agent = DialogAgent (
5 name =" Assistant ",
6 sys_prompt =" You are a helpful assistant ",
7 model ="gpt -4"
8). to_dist ()
9user_agent = UserAgent ()
10
11x = None
12while x is None or not x. content != " exit :
13 x = sequentialpipeline ([ assistant_agent , user_agent ], x)
Example 14: Example that deploys agents in single-machine multi-process mode.
mode and vice versa, with only minimal changes to the agent configuration and no modification to the
workflow.
9.5 RAG Agents: AgentScope Copilot
As previously introduced in Section 7, Retrieval-Augmented Generation (RAG) allows developers to fully
utilize the language generation capability of LLMs accompanied by a customized knowledge pool. Accordingly,
AgentScope introduces RAG agents to facilitate such a functionality. In the following example (as shown
Example 17), we show how to use a collection of Llama-index-based RAG agents (i.e., the LlamaIndexAgent
inherited from the RAGAgentBase ) to build a multi-agent copilot for AgentScope.
We first initialize the agents. Note that the most important feature of RAG agents is that beyond
customized personalities and behavioral styles configured by the system prompts, each agent is loaded with
external knowledge, which is specified in the agent_configs that contains configuration information such as
data storage directory, targeted file types, document chunking settings, the indexing and embedding settings,
etc..
The workflow of copilotis designed as follows, the user first inputs a message, and if the user mentions some
specific RAG agents as we defined, then the corresponding agents would respond, otherwise, the guide_agent
would decide the most suitable agent to respond to the query. Due to space limit, we only represent simplified
24

--- PAGE 25 ---
1from agentscope . agents . rpc_agent import RpcAgentServerLauncher
2from agentscope . agents import DialogAgent
3
4# load model configurations
5agentscope . init ( model_configs =" configs / model_configs . json ")
6# set server for the remote agent
7server_launcher = RpcAgentServerLauncher (
8 agent_class = DialogAgent ,
9 agent_kwargs ={
10 " name ": " Assitant ",
11 " sys_prompt ": "You are a helpful assistant .",
12 " model ": "gpt -4"
13 },
14 host =" xxx. xxx. xxx. xxx",
15 port =12010 ,
16)
17# start the server
18server_launcher . launch ()
19server_launcher . wait_until_terminate ()
Example 15: Deploying a remote agent in multi-machine multi-process mode..
1agentscope . init ( model_configs =" configs / model_configs . json ")
2
3assistant_agent = DialogAgent (
4 name =" Assistant ",
5 model ="gpt -4"
6). to_dist (
7 host =" xxx. xxx. xxx. xxx", # The target URL of agent server
8 port =12010 , # The target port of agent server
9 launch_server =False , # Use the remote agent server
10)
11user_agent = UserAgent ()
12
13x = None
14while x is None or not x. content != " exit ":
15 x = sequentialpipeline ([ assistant_agent , user_agent ], x)
Example 16: Example of setting sub-processes for agents in multi-machine multi-process mode.
codes here and please refer to the repository and documentation for more details.
9.6 Web Search and Retrieve Agents
We have shown examples of agents generating responses by the capability of LLM ( DialogAgent ) and
information retrieved from external knowledge libraries ( LlamaIndexAgent ). Nevertheless, we can also utilize
internet resources to build agents, as introduced in the following example.
As presented in Example 18, the initialization involves three types of agents - the UserAgent that takes
user inputs, the SearcherAgent that converts the user’s questions into keywords and calls the search engine
to retrieve webpages from the internet, and the AnswererAgent that retrieves information from web pages to
compose answers.
It is worth noting that, since a large number of web pages may be returned by the search agents. In
the standard single-process mode, multiple AnswererAgent instances can only perform web searching and
answer questions in a sequential manner on a single machine. For better efficiency, it is beneficial to allow
multiple instances of AnswererAgent running in parallel, that is, the multi-machine multi-process mode of
AgentScope agents.
25

--- PAGE 26 ---
1import agentscope
2from agentscope . agents import UserAgent , DialogAgent , LlamaIndexAgent
3...
4# initialize agentscope with model configurations
5agentscope . init ( model_configs =" configs / model_configs . json ")
6
7# initialize the RAG agents based on different configurations
8tutorial_agent = LlamaIndexAgent (** agent_configs [0][ " args "])
9code_agent = LlamaIndexAgent (** agent_configs [1][ " args "])
10api_agent = LlamaIndexAgent (** agent_configs [2][ " args "])
11search_agent = LlamaIndexAgent (** agent_configs [3][ " args "])
12...
13# initialize a basic dialog agent as the ‘‘ frontdesk assistant ’’ and a user↰agent
14guide_agent = DialogAgent (** agent_configs [4][ " args "])
15user_agent = UserAgent ()
16...
17while True :
18 x = user_agent ()
19 # the workflow terminates when user inputs nothing or ‘‘exit ’’
20 if len (x[" content "]) == 0 or str (x[" content "]). startswith (" exit "):
21 break
22 # find out the agents mentioned in user ’s input
23 speak_list = filter_agents (x.get (" content ", ""), rag_agent_list )
24 if len ( speak_list ) == 0:
25 # if no agent is mentioned , the guide agent will decide which one to↰call
26 guide_response = guide_agent (x)
27 speak_list = filter_agents (
28 guide_response . get(" content ", ""),
29 rag_agent_list ,
30 )
31 # agents called by the guide agent will be recorded
32 agent_name_list = [ agent . name for agent in speak_list ]
33 # the listed agents respond to the query in turn
34 for agent_name , agent in zip ( agent_name_list , speak_list ):
35 if agent_name in rag_agent_names :
36 agent (x)
Example 17: Example of using RAG agents to build a copilot for AgentScope.
9.7 ReAct Agents: Convert Natural Language to SQL Query
Natural Language to SQL query (NL2SQl) is a classical yet challenging task in both database and natural
language processing communities, which aims to convert human input questions in natural language into
SQL queries. In the research community, there is a collection of works exploring the potential of LLMs in
NL2SQL, and it would be very interesting to explore this task with LLM-empowered agents.
In AgentScope, we provide a special class of agents, the ReAct (reasoning and acting) agents. More
specifically, we could create new service functions, by using the ServiceToolkit module, for the ReAct
agents and corresponding LLMs. In this example, we try to equip the ReAct agent with a state-of-the-art
NL2SQL algorithm, DAIL-SQL.
As the first step (as shown in Example 19), we need to initialize the model config and the SQL database,
then initiate and provide the corresponding database path in sqlitefile format. Here we generate the SQLite
file using the provided SQL commands. You can also use the .sqlite format file directly.
Then, as shown in Example 20, we define the tools for ReAct Agent to execute the SQL query. Namely,
our agent should be able to generate the SQL query given the natural language input and execute the SQL
query to get the result. We referenced a third-party Text-to-SQL tool DAIL-SQL to generate a Text-to-SQL
prompt. We use the query_sqlite service function in the agentscope.service module. Now, we can
initiate the ReAct Agent using the defined tools and interact with the agent, as shown in Example 21.
26

--- PAGE 27 ---
1import agentscope
2from searcher_agent import SearcherAgent
3from answerer_agent import AnswererAgent
4from agentscope . agents . user_agent import UserAgent
5
6agentscope . init ( model_configs =" configs / model_configs . json ")
7
8# we can perform multiple searches at one time
9WORKER_NUM = 3
10searcher = SearcherAgent (
11 name =" Searcher ",
12 model_config_name =" my_model ",
13 result_num = args . num_workers ,
14 search_engine_type = args . search_engine ,
15 api_key = args . api_key ,
16 cse_id = args . cse_id ,
17)
18# instantiate the answerer agents
19answerers = []
20for i in range ( args . num_workers ):
21 answerer = AnswererAgent (
22 name =f" Answerer -{i}",
23 model_config_name =" my_model ",
24 )
25 # if we want to put agents in distributed ( parallel ) mode
26 if args . use_dist :
27 answerer = answerer . to_dist ( lazy_launch = False )
28 answerers . append ( answerer )
29user_agent = UserAgent ()
30
31msg = user_agent ()
32while not msg . content == " exit ":
33 msg = searcher ( msg )
34 results = []
35 for page , worker in zip (msg . content , answerers ):
36 results . append ( worker ( Msg (** page )))
37 for result in results :
38 logger . chat ( result )
39 msg = user_agent ()
Example 18: Example of utilizing web search and retrieve agents.
9.8 AgentScope Workstation
AgentScope provides a very convenient and user-friendly development kit in the form of “dragging windows”,
theWorkstation . Here, implementing applications of AgentScope using this development kit is of low cost
in the sense that, entry-level developers or those without any programming experience could easily develop
their own application at ease by simply dragging those agent-related modules and connecting them in a
very straightforward way. For example, as shown in Fig. 9, we implement the basic conversation example in
Workstation . As we can see, we do not need to write any code, just simply type in the configurations such as
detailed settings and APIs into the corresponding windows, link the windows to build the dependency and
connection, and then with one-click, Workstation would get ready for launch automatically. Meanwhile, the
Workstation also introduced static checking rules to ensure the correctness of the configurations.
AgentScope Workstation also provides comprehensive support for advanced developers. The developers
could export the configurations on the modules as .jsonfiles and execute by the AgentScope Workstation
engine. Alternatively, one can also use the AgentScope Workstation Compiler to convert all configurations
into Python codes for further editing or development to implement more customized adjustments.
27

--- PAGE 28 ---
1import agentscope
2from sql_utils import create_sqlite_db_from_schema
3...
4agentscope . init ( model_configs =" configs / model_configs . json ")
5create_sqlite_db_from_schema ( db_schema_path , db_sqlite_path )
6...
Example 19: Example of utilizing web search and retrieve agents.
1from agentscope . service import (
2 ServiceResponse ,
3 ServiceExecStatus ,
4 ServiceToolkit ,
5 query_sqlite ,
6)
7from sql_utils import DailSQLPromptGenerator
8
9def generate_sql_query ( question : str , db_path : str , model : Callable ) ->↰ServiceResponse :
10 prompt_helper = DailSQLPromptGenerator ( db_path )
11 prepared_prompt = prompt_helper . generate_prompt ({" content ": question })
12
13 def get_response_from_prompt ( prompt : dict , model : Callable ) -> str:
14 ...
15 sql_response = get_response_from_prompt (
16 prepared_prompt [" prompt "], model = model
17 )
18 return ServiceResponse (
19 ServiceExecStatus . SUCCESS ,
20 sql_response
21 )
22
23# Use Service Toolkit to set up tool functions for LLMs
24service_toolkit = ServiceToolkit ()
25service_toolkit . add ( generate_sql_query , db_path = db_sqlite_path , model =↰loaded_model )
26service_toolkit . add ( query_sqlite , database = db_sqlite_path )
Example 20: Example of utilizing web search and retrieve agents.
10 Related Works
The development of AgentScope aligns with the rapidly evolving landscape of frameworks that leverage
large language models (LLMs) for the creation of language agents and multi-agent systems. Here, we briefly
introduce works closely related to AgentScope from two sub-domains pertinent : Language Agent Frameworks,
focusing on individual agent capabilities, and Multi-Agent Frameworks, emphasizing collaboration among
multiple agents. For broader related works, readers can refer to (Wang et al., 2023; Xi et al., 2023).
Language Agent Frameworks Language agent frameworks are pivotal for developing applications that
can interpret and interact using human language.
The Transformers library (Huggingface, 2023) has introduced a natural language API to interface with
transformer models in its recent updates ( Transformers-Agents ). This API utilizes a set of customizable
tools, allowing the model to interpret instructions and generate code snippets accordingly. It offers support
for various open-source and proprietary model endpoints, catering to diverse developer needs. LangChain
(Langchain-AI, 2023) provides a framework for building applications that are context-aware and capable of
reasoning. It includes libraries and templates that facilitate the integration of multiple components into a
unified cognitive architecture. LangServe and LangSmith extend the framework’s capabilities by enabling
deployment as a REST API and offering developer tools for debugging and monitoring chains built on any LLM
framework. AutoGPT (AutoGPT-Team, 2023) illustrates a different approach, allowing an LLM to iteratively
28

--- PAGE 29 ---
1from agentscope . agents import ReActAgent
2agent = ReActAgent (
3 name =" assistant ",
4 model_config_name =’gpt -4 ’,
5 service_toolkit = service_toolkit ,
6 sys_prompt =" You are a helpful agent that preform SQL queries base on natual↰language instructions .",
7 verbose =True , # set verbose to True to show the reasoning process
8)
9...
10mss = Msg (
11 name =" user ",
12 content =" How many singers do we have ?",
13 role =" user "
14)
15logger . chat ( mss )
16
17sql_query_mss1 = agent (mss )
18...
Example 21: Example of utilizing web search and retrieve agents.
execute actions and make decisions. As a generalist agent, AutoGPT is not task-specific; it is designed to
perform a variety of computer-based tasks, reflecting the adaptive nature of LLMs. ModelScope-Agent (Li
et al., 2023a) is a customizable agent framework that harnesses open-source LLMs to perform tasks and
connect with external APIs. It facilitates seamless integration with model APIs and common APIs while
providing a comprehensive infrastructure for data collection, tool retrieval, and customized model training,
all aiming to realize practical real-world applications.
Multi-Agent Frameworks Building on the capabilities of individual agents, multi-agent frameworks
explore collaboration and interaction among multiple agents to address complex tasks.
AutoGen (Wu et al., 2023) provides a generic infrastructure that allows developers to program interaction
patternsusingbothnaturallanguageandcode. Thisframeworkenablesthedevelopmentofdiverseapplications
by facilitating conversation among agents that are customizable and can utilize various combinations of
LLMs, human inputs, and tools. MetaGPT (Hong et al., 2023) incorporates meta-programming to enhance
multi-agent collaborations. By encoding Standardized Operating Procedures (SOP) into prompts, this
framework ensures streamlined workflows and reduced errors, exemplifying effective task decomposition
among agents. AGENTS (Zhou et al., 2023) is an open-source library that supports autonomous language
agents with features like planning, memory, and multi-agent communication. It is designed to be user-friendly,
helping non-specialists to deploy state-of-the-art language agents, and research-friendly, with a modularized
design for extensibility. OpenAgents (Xie et al., 2023) provides an open platform for using language agents
with practical functionalities accessible through a web interface. This framework emphasizes facilitating
real-world agent interactions and includes specialized agents for different tasks, such as data analysis and web
browsing. ChatDev (Qian et al., 2023) exploits LLMs for software development, creating a virtual chat-powered
company that follows a waterfall model. It engages “software agents” at different stages of the development
process, facilitating collaboration and context-aware communication. CAMEL (Li et al., 2023b) proposes a
novel framework for autonomous cooperation among communicative agents using role-playing techniques,
which allows for the generation of conversational data for studying agent behaviors and capabilities. Lastly,
AgentSims (Lin et al., 2023) introduces a sandbox environment to evaluate LLMs in task-based scenarios,
offering an infrastructure for researchers to test specific LLM capacities within a simulated environment.
These frameworks represent significant strides in the use of LLMs for both individual and collaborative
agent tasks. AgentScope is situated within this context, contributing by addressing the need for a user-friendly,
fault-tolerant and versatile framework designed to manage complex interactions and processes inherent in
multi-agent LLM systems. By focusing on ease of use and reliability, AgentScope aims to facilitate the
creation of robust and versatile applications across diverse domains.
29

--- PAGE 30 ---
Figure 9: Workstation generates workflow configuration and Python code.
11 Conclusion
Inthiswork, weproposeAgentScope, aplatformthatstandsattheforefrontofmulti-agentsystemdevelopment,
synergizing user-centric design with the advanced capabilities of LLMs. Through its innovative communication
and distributed mechanisms, AgentScope demonstrates its potential to boost collaboration amongst agents,
enabling efficient, fault-tolerant operations and multi-modal interactions. By abstracting complexities and
offering an array of development utilities, AgentScope substantially lowers the barriers to entry, fostering a
more inclusive and creative community of developers.
Looking forward, AgentScope opens numerous avenues for further research and development. Future work
could delve into deeper integration of retrieval augmented generation, and explore adaptive communication
protocols and interactive modals that evolve alongside task requirements. The platform’s impact on accelerat-
ing the deployment of multi-agent systems across industries, from healthcare to customer service, promises to
be profound, potentially leading to smarter and more responsive technologies that enhance human-machine
collaboration. With AgentScope, we invite the broader research and development community to build upon
our foundation, driving innovations that will shape the next generation of intelligent multi-agent applications.
30

--- PAGE 31 ---
References
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. {TensorFlow }: a system for {Large-Scale }machine
learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16) , pages
265–283, 2016.
Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. Gradio:
Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569 , 2019.
AutoGPT-Team. Autogpt, 2023. URL https://github.com/Significant-Gravitas/AutoGPT .
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,
Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science , 2(3):8,
2023.
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can GPT learn
in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the
Association for Computational Linguistics: ACL 2023 , pages 4005–4019, July 2023.
Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and
reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325 , 2023.
Justin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse
reinforcement learning for vision-based instruction following. In 7th International Conference on Learning
Representations , 2019.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,
Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey.
CoRR, abs/2312.10997, 2023.
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven
Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative
framework. arXiv preprint arXiv:2308.00352 , 2023.
Huggingface. Transformers-agents, 2023. URL https://huggingface.co/docs/transformers/
transformers_agents .
Langchain-AI. Langchain, 2023. URL https://github.com/langchain-ai/langchain .
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-
intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020a.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-
augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020 , 2020b.
Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng
Zhou, Yingda Chen, Chen Cheng, et al. Modelscope-agent: Building your customizable agent system with
open-source large language models. arXiv preprint arXiv:2309.00986 , 2023a.
Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:
Communicative agents for" mind" exploration of large scale language model society. arXiv preprint
arXiv:2303.17760 , 2023b.
Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source
sandbox for large language model evaluation. arXiv preprint arXiv:2308.04026 , 2023.
31

--- PAGE 32 ---
Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index .
OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training
language models to follow instructions with human feedback. In Advances in Neural Information Processing
Systems 35: Annual Conference on Neural Information Processing Systems , 2022.
Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and
Marco Túlio Ribeiro. ART: automatic multi-step reasoning and tool-use for large language models. CoRR,
abs/2303.09014, 2023.
Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: tool augmented language models. CoRR, abs/2205.12255,
2022.
Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S.
Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual
ACM Symposium on User Interface Software and Technology , pages 2:1–2:22, 2023.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization
with “gradient descent” and beam search. In Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing , pages 7957–7968, December 2023.
Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun.
Communicative agents for software development. arXiv preprint arXiv:2307.07924 , 2023.
Vipula Rawte, Amit P. Sheth, and Amitava Das. A survey of hallucination in large foundation models. CoRR,
abs/2309.05922, 2023.
Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei
Huang. Small llms are weak tool learners: A multi-llm agent. arXiv preprint arXiv:2401.07324 , 2024.
Weijie Su, Xizhou Zhu, Chenxin Tao, Lewei Lu, Bin Li, Gao Huang, Yu Qiao, Xiaogang Wang, Jie Zhou, and
Jifeng Dai. Towards all-in-one pre-training via maximizing multi-modal mutual information. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 15888–15899, 2023.
Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with task-agnostic
scaffolding. arXiv preprint arXiv:2401.12954 , 2024.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin,
Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR,
abs/2302.13971, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh
Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,
Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,
Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,
32

--- PAGE 33 ---
Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert
Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
CoRR, abs/2307.09288, 2023b.
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint
arXiv:2308.11432 , 2023.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv
preprint arXiv:2206.07682 , 2022.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,
Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation
framework. arXiv preprint arXiv:2308.08155 , 2023.
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang,
Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv
preprint arXiv:2309.07864 , 2023.
Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao,
Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint
arXiv:2310.10634 , 2023.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React:
Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning
Representations , 2023.
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei
Zhang, Fei Wu, and Guoyin Wang. Instruction tuning for large language models: A survey. CoRR,
abs/2308.10792, 2023a.
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,
Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Siren’s song in the AI
ocean: A survey on hallucination in large language models. CoRR, abs/2309.01219, 2023b.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang,
Jing Chen, Ruipu Wu, Shuai Wang, et al. Agents: An open-source framework for autonomous language
agents.arXiv preprint arXiv:2309.07870 , 2023.
33

--- PAGE 34 ---
A Running Example of Standalone Conversation
Listing 22 shows the dialog history when running the standalone conversation.
1Assistant : Thank you ! I’m here to help . How can I assist you today ?
2User : I wanna know about large language models .
3Assistant : Large language models are AI models trained on vast amounts of text↰data to understand and generate human - like text .
4User : What ’s the most famous and powerful LLM in the world ?
5Assistant : The most famous and powerful large language model currently is OpenAI↰’s GPT -4.
6User : What about the other LLMs ?
7Assistant : There are several other notable large language models including :
8
91. BERT ( Bidirectional Encoder Representations from Transformers ) by Google :↰This model is designed to understand the context of words in a sentence by↰looking at what ’s before and after them .
10
112. T5 (Text -to - Text Transfer Transformer ) also by Google : This model treats↰every NLP problem as a text generation task .
12
133. RoBERTa by Facebook : This is a variant of BERT and is trained on more data↰and for a longer amount of time .
14
154. XLNet by Google Brain : This model overcomes some of the limitations of BERT↰by taking into account all possible word sequences in a sentence .
16
175. Turing -NLG by Microsoft : This is a 17- billion parameter language model by↰Microsoft , designed for various NLP tasks .
18
196. Megatron by NVIDIA : This is designed to train Transformer models with↰billions of parameters .
20
21These models are used for a variety of NLP tasks including translation , question↰answering , summarization , and more .
22User : Okay , what should I know about LLM ?
23Assistant : Large Language Models ( LLMs ) are powerful AI tools that can generate↰human - like text , but they require careful handling due to potential risks like↰generating harmful or biased content .
24User : Okay , thanks .
25Assistant : You ’re welcome ! If you have any other questions , feel free to ask .
26User : exit
Example 22: A running example of standalone conversation.
B Running Example of Werewolf
Please refer to https://github.com/modelscope/agentscope for the complete code of the werewolf game. We
provid an example dialogue history in Listing 23.
1Moderator : Player1 and Player2 , you are werewolves . If you are alone , eliminate↰a player , else discuss with your teammates and reach an agreement . Response in↰the following format which can be loaded by python json . loads (){
2 " thought ": " thought ",
3 " speak ": " thoughts summary to say to others ",
4 " agreement ": " whether the discussion reached an agreement or not ( true / false )↰"
5}
6Player1 : I think we should consider Player3 . They have a knack for figuring↰things out .
7Player2 : I agree with your point about Player3 , they are indeed a strong player .↰But we should also consider Player4 , who is also a strong player and could be↰the seer or witch .
34

--- PAGE 35 ---
8Player1 : I see your point about Player4 . They could be a threat if they are the↰seer or witch . But I still think Player3 is a strong player and could figure↰us out .
9Player2 : Alright , let ’s go with Player3 . We can consider Player4 next time .
10Moderator : Which player do you vote to kill ? Response in the following format↰which can be loaded by python json . loads ()
11{{
12 " thought ": " thought " ,
13 " speak ": " player_name "
14}}
15Player1 : Player3
16Player2 : Player3
17Moderator : The player with the most votes is Player3 .
18Moderator : Player6 , you ’re witch . Tonight Player3 is eliminated . Would you like↰to resurrect Player3 ? Response in the following format which can be loaded by↰python json . loads ()
19{
20 " thought ": " thought ",
21 " speak ": " thoughts summary to say ",
22 " resurrect ": " true / false "
23}
24Player6 : I’m deeply saddened by the loss of Player3 . It ’s a tough decision to↰make .
25Moderator : Player5 , you ’re seer . Which player in Player1 , Player2 , Player3 ,↰Player4 , Player5 and Player6 would you like to check tonight ? Response in the↰following json format which can be loaded by python json . loads ()
26{
27 " thought ": " thought " ,
28 " speak ": " player_name "
29}
30Player5 : Player1
31Moderator : Okay , the role of Player1 is werewolf .
32Moderator : The day is coming , all the players open your eyes . Last night is↰peaceful , no player is eliminated .
33Moderator : Now the alive players are Player1 , Player2 , Player3 , Player4 , Player5↰and Player6 . Given the game rules and your role , based on the situation and↰the information you gain , to vote a player eliminated among alive players and↰to win the game , what do you want to say to others ? You can decide whether to↰reveal your role . Response in the following JSON format which can be loaded by↰python json . loads ()
34{
35 " thought ": " thought " ,
36 " speak ": " thought summary to say to others "
37}
38Player1 : I’m glad that we all made it through the night . I think we should keep↰an eye on Player4 . They ’ve been pretty quiet , which could mean they ’re trying↰to avoid suspicion .
39Player2 : I agree with Player1 . Player4 has been unusually quiet . Maybe they ’re↰trying to avoid drawing attention to themselves .
40Player3 : I see your point about Player4 , but it ’s still early in the game . We↰should be careful not to jump to conclusions without more evidence .
41Player4 : I understand your concerns , but I assure you , I’m just a villager . I↰think we should focus on gathering more information before we start pointing↰fingers . If the seer has any insights , now might be a good time to share .
42Player5 : I understand everyone ’s concerns about Player4 , but I think we should↰also keep an eye on Player1 . They were quick to cast suspicion on others ,↰which could be a strategy to divert attention from themselves .
43Player6 : I understand the concerns about Player4 , but I also think we should be↰cautious about Player1 and Player2 . They were quick to agree with each other ,↰which could be a strategy to divert attention from themselves . Let ’s gather↰more information before we make any decisions .
44Moderator : Now the alive players are Player1 , Player2 , Player3 , Player4 , Player5↰and Player6 . Given the game rules and your role , based on the situation and↰the information you gain , to vote a player among alive players and to win the↰game , it ’s time to vote one player among the alive players , please cast your↰vote on who you believe is a werewolf . Response in the following format which↰can be loaded by python json . loads ()
45{
35

--- PAGE 36 ---
46 " thought ": " thought ",
47 " speak ": " player_name "
48}
49Player1 : Player4
50Player2 : Player4
51Player3 : Player1
52Player4 : Player1
53Player5 : Player1
54Player6 : Player1
55Moderator : Player1 has been voted out .
56Moderator : The game goes on.
57Moderator : Player2 , you are werewolves . If you are alone , eliminate a player ,↰else discuss with your teammates and reach an agreement . Response in the↰following format which can be loaded by python json . loads (){
58 " thought ": " thought ",
59 " speak ": " thoughts summary to say to others ",
60 " agreement ": " whether the discussion reached an agreement or not ( true / false )↰"
61}
62Player2 : I think we should consider Player4 . They are a strong player and could↰be a threat .
63Moderator : Which player do you vote to kill ? Response in the following format↰which can be loaded by python json . loads ()
64{{
65 " thought ": " thought " ,
66 " speak ": " player_name "
67}}
68Player2 : Player4
69Moderator : The player with the most votes is Player4 .
70Moderator : Would you like to eliminate one player ? Response in the following↰json format which can be loaded by python json . loads ()
71{{
72 " thought ": " thought ",
73 " speak ": " False or player_name "
74}}
75Player6 : False
76Moderator : Player5 , you ’re seer . Which player in Player2 , Player3 , Player4 ,↰Player5 and Player6 would you like to check tonight ? Response in the following↰json format which can be loaded by python json . loads ()
77{
78 " thought ": " thought " ,
79 " speak ": " player_name "
80}
81Player5 : Player2
82Moderator : Okay , the role of Player2 is werewolf .
83Moderator : The day is coming , all the players open your eyes . Last night , the↰following player (s) has been eliminated : Player4 .
84Moderator : Now the alive players are Player2 , Player3 , Player5 and Player6 .↰Given the game rules and your role , based on the situation and the information↰you gain , to vote a player eliminated among alive players and to win the game↰, what do you want to say to others ? You can decide whether to reveal your↰role . Response in the following JSON format which can be loaded by python json↰. loads ()
85{
86 " thought ": " thought " ,
87 " speak ": " thought summary to say to others "
88}
89Player2 : I think we should keep an eye on Player5 . They ’ve been pretty quiet ,↰which could mean they ’re trying to avoid suspicion .
90Player3 : I think Player2 might be a werewolf . They were quick to suspect Player4↰, who was eliminated last night . However , the game didn ’t end after Player1 ,↰who also suspected Player4 , was voted out . This suggests that Player1 was not↰a werewolf , and therefore , Player2 is likely to be one.
91Player5 : I agree with Player3 . Player2 was quick to cast suspicion on Player4 ,↰who turned out to be innocent . I also noticed that Player2 and Player1 were↰often in agreement , which could mean they were both werewolves trying to↰protect each other . I think we should vote out Player2 .
36

--- PAGE 37 ---
92Player6 : I agree with Player3 and Player5 . Player2 was quick to cast suspicion↰on Player4 , who turned out to be innocent . I also noticed that Player2 and↰Player1 were often in agreement , which could mean they were both werewolves↰trying to protect each other . I think we should vote out Player2 .
93Moderator : Now the alive players are Player2 , Player3 , Player5 and Player6 .↰Given the game rules and your role , based on the situation and the information↰you gain , to vote a player among alive players and to win the game , it ’s time↰to vote one player among the alive players , please cast your vote on who you↰believe is a werewolf . Response in the following format which can be loaded by↰python json . loads ()
94{
95 " thought ": " thought ",
96 " speak ": " player_name "
97}
98Player2 : Player5
99Player3 : Player2
100 Player5 : Player2
101 Player6 : Player2
102 Moderator : Player2 has been voted out .
103 Moderator : The game is over . The werewolves have been defeated , and the village↰is safe once again !
Example 23: Running example of werewolf.
37
