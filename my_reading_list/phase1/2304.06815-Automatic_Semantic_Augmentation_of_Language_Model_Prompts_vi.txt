# 2304.06815.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: ./2304.06815.pdf
# Kích thước tệp: 1920153 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tăng Cường Ngữ Nghĩa Tự Động của Prompt Mô Hình Ngôn Ngữ
(cho Tóm Tắt Mã Nguồn)
Toufique Ahmed
Đại học California, Davis
Davis, California, USA
tfahmed@ucdavis.edu

Kunal Suresh Pai
Đại học California, Davis
Davis, California, USA
kunpai@ucdavis.edu

Premkumar Devanbu
Đại học California, Davis
Davis, California, USA
ptdevanbu@ucdavis.edu

Earl T. Barr
University College London & Google Brain
London, UK
e.barr@ucl.ac.uk

TÓM TẮT
Các Mô Hình Ngôn Ngữ Lớn (LLM) là một lớp công cụ tính toán mới, được "lập trình" thông qua kỹ thuật prompt engineering. Các nhà nghiên cứu vẫn đang tìm hiểu cách "lập trình" tốt nhất cho những LLM này để hỗ trợ các lập trình viên.

Chúng tôi bắt đầu với trực giác rằng các lập trình viên có xu hướng thu thập các sự kiện ngữ nghĩa một cách có ý thức và vô thức từ mã nguồn trong khi làm việc. Hầu hết đây là những sự kiện nông, đơn giản phát sinh từ việc đọc nhanh. Đối với một hàm, những sự kiện như vậy có thể bao gồm tên tham số và biến cục bộ, biểu thức trả về, điều kiện tiên quyết và hậu quyết đơn giản, và luồng điều khiển và dữ liệu cơ bản, v.v.

Người ta có thể giả định rằng kiến trúc đa lớp mạnh mẽ của các LLM kiểu transformer khiến chúng có khả năng ngầm thực hiện mức độ "phân tích mã nguồn" đơn giản này và trích xuất thông tin như vậy, trong khi xử lý mã nguồn: nhưng liệu chúng có thực sự như vậy không? Nếu không, việc thêm thông tin này một cách rõ ràng có thể giúp ích không? Mục tiêu của chúng tôi ở đây là điều tra câu hỏi này, sử dụng nhiệm vụ tóm tắt mã nguồn và đánh giá xem việc tăng cường tự động prompt của LLM với các sự kiện ngữ nghĩa một cách rõ ràng có thực sự giúp ích không.

Nghiên cứu trước đây cho thấy rằng hiệu suất của LLM trong tóm tắt mã nguồn được cải thiện nhờ việc nhúng một số mẫu mã nguồn & tóm tắt vào prompt, trước mã nguồn cần tóm tắt. Trong khi hiệu suất tóm tắt đã tiến bộ đều đặn kể từ những ngày đầu, vẫn còn chỗ để cải thiện: hiệu suất của LLM trong tóm tắt mã nguồn vẫn kém hơn hiệu suất của nó trong các nhiệm vụ ngôn ngữ tự nhiên như dịch thuật và tóm tắt văn bản.

Chúng tôi thấy rằng việc thêm các sự kiện ngữ nghĩa vào mã nguồn trong prompt thực sự giúp ích! Cách tiếp cận này cải thiện hiệu suất trong nhiều thiết lập khác nhau được đề xuất bởi nghiên cứu trước đây, bao gồm cho ba Mô Hình Ngôn Ngữ Lớn khác nhau. Trong hầu hết các trường hợp, chúng tôi thấy cải thiện, được đo bằng một loạt các chỉ số thường được sử dụng; đối với ngôn ngữ PHP trong bộ dữ liệu CodeSearchNet đầy thử thách, việc tăng cường này thực sự mang lại hiệu suất vượt qua 30 BLEU¹. Ngoài ra, chúng tôi

¹Điểm số 30-40 BLEU được coi là "Tốt" đến "Có thể hiểu được" đối với dịch thuật ngôn ngữ tự nhiên; xem https://cloud.google.com/translate/automl/docs/evaluate.

Được phép tạo bản sao kỹ thuật số hoặc bản cứng của một phần hoặc toàn bộ công việc này cho mục đích cá nhân hoặc lớp học mà không mất phí với điều kiện các bản sao không được tạo ra hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải có thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần của bên thứ ba trong công việc này phải được tôn trọng.
Đối với tất cả các mục đích sử dụng khác, liên hệ với chủ sở hữu/tác giả.
ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha
©2024 Bản quyền thuộc về chủ sở hữu/tác giả.
ACM ISBN 979-8-4007-0217-4/24/04.
https://doi.org/10.1145/3597503.3639183

cũng thấy rằng việc bao gồm các sự kiện ngữ nghĩa mang lại sự cải thiện đáng kể trong hiệu suất hoàn thành dòng của LLM.

TỪ KHÓA
LLM, Tóm Tắt Mã Nguồn, Phân Tích Chương Trình, Kỹ Thuật Prompt

Định Dạng Trích Dẫn ACM:
Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr.
2024. Tăng Cường Ngữ Nghĩa Tự Động của Prompt Mô Hình Ngôn Ngữ (cho Tóm Tắt Mã Nguồn). Trong Hội nghị Quốc tế IEEE/ACM lần thứ 46 về Kỹ Thuật Phần Mềm 2024 (ICSE '24), 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha. ACM, New York, NY, USA, 13 trang. https://doi.org/10.1145/3597503.3639183

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) thường vượt trội hơn các mô hình nhỏ hơn, được huấn luyện tùy chỉnh trong nhiều nhiệm vụ, đặc biệt khi được nhắc nhở bằng một tập hợp mẫu "few-shot". LLM được huấn luyện trước trên một nhiệm vụ tự giám sát (che giấu hoặc khử nhiễu), sử dụng lượng lớn dữ liệu, và thể hiện hành vi khởi phát đáng ngạc nhiên khi dữ liệu huấn luyện và số lượng tham số được mở rộng. Chúng xuất sắc trong nhiều nhiệm vụ với việc học few-shot (hoặc thậm chí zero-shot): chỉ với một vài cặp đầu vào-đầu ra mẫu được chèn đầu tiên vào prompt, các mô hình có thể tạo ra đầu ra rất tốt cho một đầu vào cho trước! Việc học few-shot hoạt động tốt đến mức với LLM mà không rõ liệu dữ liệu cụ thể cho nhiệm vụ có thể được thu thập đủ để huấn luyện một mô hình tùy chỉnh có thể cạnh tranh với hiệu suất của chúng [3,12]. LLM đang mở ra một kỷ nguyên mới, nơi kỹ thuật prompt engineering, để điều chỉnh cẩn thận đầu vào cho LLM nhằm điều chỉnh khả năng khổng lồ nhưng chung chung của nó cho các nhiệm vụ cụ thể, sẽ trở thành một phong cách lập trình mới, đặt ra yêu cầu mới cho các kỹ sư phần mềm.

Chúng tôi đề xuất Tăng Cường Ngữ Nghĩa Tự Động của Prompt (A𝑆𝐴𝑃), một phương pháp mới để xây dựng prompt cho các nhiệm vụ kỹ thuật phần mềm. Phương pháp A𝑆𝐴𝑃 dựa trên một sự tương tự: một prompt hiệu quả cho LLM, đối với một nhiệm vụ, liên quan đến các sự kiện mà một lập trình viên nghĩ về khi thực hiện nhiệm vụ đó một cách thủ công. Nói cách khác, chúng tôi giả thuyết rằng việc nhắc nhở LLM bằng các sự kiện cú pháp và ngữ nghĩa mà một lập trình viên xem xét khi thực hiện một nhiệm vụ một cách thủ công sẽ cải thiện hiệu suất của LLM trong nhiệm vụ đó. Để thực hiện giả thuyết này, A𝑆𝐴𝑃 tăng cường các prompt bằng các sự kiện ngữ nghĩa được trích xuất tự động từ mã nguồn sử dụng phân tích mã nguồn ngữ nghĩa.

Chúng tôi minh họa phương pháp A𝑆𝐴𝑃 đầu tiên về tóm tắt mã nguồn. Nhiệm vụ này lấy mã nguồn, thường là một hàm, và tóm tắt nó bằng ngôn ngữ tự nhiên; những tóm tắt như vậy có thể hỗ trợ hiểu mã nguồn để tạo điều kiện cho việc theo dõi yêu cầu và bảo trì.

--- TRANG 2 ---
ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr

A𝑆𝐴𝑃 sử dụng few-shot prompting vì tính hiệu quả của nó. A𝑆𝐴𝑃 tìm những shot liên quan bằng BM25, hiện tại là state of the art trong việc tìm các mẫu few-shot "gần gũi về mặt ngữ nghĩa" với hàm mục tiêu [48], trong trường hợp của chúng tôi, hàm-cần-tóm-tắt, bằng cách truy vấn dữ liệu huấn luyện của LLM. Khi khởi tạo A𝑆𝐴𝑃 cho nhiệm vụ tóm tắt, chúng tôi trang bị nó để trích xuất các sự kiện ngữ nghĩa sau: tên repository, tên đầy đủ của hàm mục tiêu, chữ ký của nó, các thẻ AST của các định danh, và đồ thị luồng dữ liệu của nó (Mục 3.4). Những sự kiện này được trình bày cho LLM như các trường riêng biệt, có nhãn². Sau đó mô hình được cung cấp hàm-cần-tóm-tắt, các mẫu (cùng với các sự kiện được trích xuất từ mỗi mẫu), và được yêu cầu tạo ra một tóm tắt. Chúng tôi xác nhận giả thuyết của mình rằng việc tăng cường prompt bằng các sự kiện ngữ nghĩa có thể cải thiện hiệu suất của LLM trong nhiệm vụ hoàn thành mã nguồn. Chúng tôi đánh giá lợi ích của A𝑆𝐴𝑃 trên bộ dữ liệu CodeSearchNet [32] chất lượng cao (được khử trùng lặp cẩn thận, đa dự án).

Tóm lại, chúng tôi thấy rằng trong tất cả các trường hợp, cách tiếp cận tăng cường ngữ nghĩa tự động của chúng tôi cải thiện hiệu suất trung bình trên một số chỉ số thường được sử dụng. Đối với hầu hết các ngôn ngữ, sự cải thiện trung bình vượt qua một cách thoải mái ngưỡng 2-BLEU được Roy et al. [57] lưu ý, dưới đó kết quả BLEU là những dự đoán không đáng tin cậy về sở thích của con người. Đối với Go, lợi ích vẫn đáng kể, và chỉ hơi ít hơn 2; đối với PHP, chúng tôi thấy một cải thiện 4.6 BLEU, đạt đến điểm cao SOTA là 32.73 trên bộ dữ liệu CodeSearchNet được tuyển chọn tốt, khử trùng lặp.

Các đóng góp chính của chúng tôi như sau:
• Cách tiếp cận A𝑆𝐴𝑃 cho các nhiệm vụ kỹ thuật phần mềm sử dụng các sự kiện được rút ra từ mã nguồn.
• Chúng tôi đánh giá A𝑆𝐴𝑃 trên nhiệm vụ tóm tắt mã nguồn trên các mô hình code-davinci-002, text-davinci-003, và GPT-3.5-turbo so với baseline few-shot prompting được xây dựng bằng BM25 thuần túy (Mục 4.1).
• Chúng tôi thấy rằng cách tiếp cận A𝑆𝐴𝑃 cải thiện hiệu suất của LLM một cách có ý nghĩa thống kê trong nhiệm vụ tóm tắt mã nguồn. Trong hầu hết các trường hợp, chúng tôi quan sát thấy cải thiện có ý nghĩa thống kê gần như, hoặc vượt quá, 2 BLEU; và, đối với PHP, chúng tôi vượt qua 30 BLEU lần đầu tiên (theo kiến thức của chúng tôi) trên bộ dữ liệu đầy thử thách này.
• Chúng tôi thấy rằng A𝑆𝐴𝑃 cũng dẫn đến cải thiện hiệu suất trong nhiệm vụ hoàn thành mã nguồn.

Tất cả dữ liệu, script đánh giá, và mã nguồn cần thiết để tái tạo công việc này sẽ có sẵn tại https://doi.org/10.5281/zenodo.7779196, và có thể được tái tạo trên bất kỳ mô hình ngôn ngữ có sẵn nào. Thí nghiệm của chúng tôi cho thấy rằng A𝑆𝐴𝑃 hoạt động tốt với bất kỳ mô hình ngôn ngữ nào đủ mạnh để tận dụng few-shot prompting.

2 BỐI CẢNH & ĐỘNG LỰC
Các Mô Hình Ngôn Ngữ Lớn (LLM) là một công nghệ mang tính biến đổi: chúng về cơ bản là một loại công cụ tính toán mới, đòi hỏi một hình thức lập trình mới, được gọi là kỹ thuật prompt engineering. Trước tiên chúng tôi bối cảnh hóa A𝑆𝐴𝑃, đóng góp của chúng tôi cho kỹ thuật prompt engineering. Cuối cùng, chúng tôi thảo luận về tóm tắt mã nguồn như một vấn đề mẫu để chứng minh tính hiệu quả của A𝑆𝐴𝑃.

²Một ví dụ đầy đủ khá dài, và được bao gồm trong repository do giới hạn độ dài của bài báo.

2.1 Học Few-shot trong Kỹ Thuật Phần Mềm
LLM hiện được sử dụng rộng rãi trong Kỹ Thuật Phần Mềm cho nhiều vấn đề khác nhau: tạo mã nguồn [14,34], kiểm thử [38,42], tạo mutation [10], sửa chữa chương trình [18,35,36,48], quản lý sự cố [6], và thậm chí tóm tắt mã nguồn [3]. Rõ ràng, các công cụ được xây dựng trên LLM được huấn luyện trước đang thúc đẩy state of the art. Ngoài hiệu suất thô của chúng trong nhiều nhiệm vụ, hai yếu tố chính chi phối sự thống trị ngày càng tăng của LLM được huấn luyện trước, cả hai đều tập trung vào chi phí. Thứ nhất, huấn luyện mô hình lớn của riêng mình, hoặc thậm chí fine-tuning rộng rãi một LLM được huấn luyện trước, đòi hỏi phần cứng đắt tiền. Thứ hai, tạo ra một bộ dữ liệu có giám sát cho nhiều nhiệm vụ kỹ thuật phần mềm quan trọng là khó khăn và tốn thời gian, thường vượt quá nguồn lực của tất cả trừ những tổ chức lớn nhất.

Trái ngược với xu hướng LLM tổng thể, có một số mô hình nhỏ hơn, chuyên dụng cho mã nguồn, đã trở nên phổ biến, ví dụ: Polycoder [67] hoặc Codegen [49]. Mặc dù có những điểm phản bác này, chúng tôi tập trung vào LLM hơn là các mô hình nhỏ, bởi vì, trong khi các mô hình nhỏ có thể được fine-tuned, chúng không hoạt động tốt lắm trong few-shotting, và do đó không hữu ích khi chỉ có một lượng nhỏ dữ liệu có sẵn. Cách tiếp cận few-shot là chìa khóa bởi vì nó đưa vào tầm với nhiều vấn đề, như tóm tắt mã nguồn, mà việc thu thập dữ liệu huấn luyện chất lượng cao, cụ thể cho dự án hoặc domain đủ để huấn luyện thậm chí các mô hình nhỏ từ đầu là thách thức.

Với học few-shot, các tham số mô hình thực tế vẫn không thay đổi. Thay vào đó, chúng tôi trình bày một vài instance của vấn đề cùng với các giải pháp (tức là, các cặp vấn đề-giải pháp như "các mẫu") cho một mô hình và yêu cầu nó hoàn thành câu trả lời cho instance cuối cùng ("đầu vào thử nghiệm"), mà chúng tôi không cung cấp giải pháp. Do đó với mỗi 𝑒𝑥𝑒𝑚𝑝𝑙𝑎𝑟 bao gồm một cặp ⟨input,output⟩, và chỉ một test-input input𝑡 (mà không có output𝑡 tương ứng, mong muốn), prompt cuối cùng trông như:

prompt←exemplar1||exemplar2||exemplar3||𝑖𝑛𝑝𝑢𝑡 𝑡

Với prompt này, LLM tạo ra 𝑜𝑢𝑡𝑝𝑢𝑡 𝑡, bắt chước hành vi input-output được minh họa bởi các mẫu trong prompt. Trên thực tế, cách tiếp cận này hoạt động khá tốt.

Khi nó hoạt động, few-shotting cho phép chúng tôi tự động hóa thậm chí các vấn đề hoàn toàn thủ công, vì việc tạo ra một vài mẫu là tương đối dễ dàng. Trong bài báo này, chúng tôi thử nghiệm với mô hình code-davinci-002. Chúng tôi thảo luận về các mô hình chi tiết hơn trong Mục 3.2.

2.2 Nhắc Nhở LLM Lý Luận
Lý Luận của Con Người liên quan đến việc sử dụng bằng chứng, tư duy logic, và lập luận để đưa ra phán đoán hoặc đi đến kết luận [31,51]. Các nhà nghiên cứu xử lý ngôn ngữ tự nhiên (NLP) đã phát triển các cách tiếp cận để lý luận về các tình huống cụ thể và cải thiện hiệu suất. Những cách tiếp cận như "Chain of thought" [66] và "step-by-step" [40] đòi hỏi việc tạo ra các kết quả trung gian ("lemma") và sử dụng chúng trong nhiệm vụ hiện tại. Những cách tiếp cận như vậy dường như hoạt động trên các vấn đề đơn giản hơn như các bài toán toán học ở trường thậm chí mà không cung cấp cho chúng "lemma", bởi vì, đối với những vấn đề này, các mô hình đủ mạnh để tạo ra "lemma" của riêng chúng; trong một số trường hợp chỉ cần thêm "hãy suy nghĩ từng bước" dường như đủ (Kojima et al. [40]).

--- TRANG 3 ---
Tăng Cường Ngữ Nghĩa Tự Động của Prompt Mô Hình Ngôn Ngữ
(cho Tóm Tắt Mã Nguồn) ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha

Chúng tôi đã thử một phiên bản cải tiến của prompt "step-by-step", với few-shots, trên tóm tắt mã nguồn. Chúng tôi thấy rằng mô hình hoạt động kém (đạt khoảng 20.25 BLEU), thấp hơn cả baseline BM25 thuần túy của chúng tôi (24.97 BLEU). Với prompt "step by step" zero-shot theo kiểu Kojima, các mô hình hoạt động thậm chí còn tệ hơn. Để khiến mô hình tạo ra các bước, và cuối cùng là một tóm tắt, chúng tôi đã đóng khung vấn đề như chain of thought, và bao gồm các mẫu few-shot chứa cả các bước trung gian ("lemma") và nhận xét cuối cùng. Lý luận là, trên các nhiệm vụ (thường là thách thức) liên quan đến mã nguồn, các mô hình cần được cung cấp rõ ràng các "lemma" trung gian, được rút ra từ mã nguồn, để có thể lý luận hiệu quả về hầu hết các nhiệm vụ kỹ thuật phần mềm, có xu hướng phức tạp và đa dạng hơn toán học trường học.

May mắn thay, các công cụ phân tích mã nguồn trưởng thành có sẵn. Chúng tôi có thể dễ dàng rút ra "lemma", tức là, các sản phẩm phân tích, sử dụng các công cụ phân tích mã nguồn, thay vì mong đợi các mô hình (có thể ngầm) rút ra chúng, trong quá trình thực hiện nhiệm vụ. Chúng tôi trực tiếp nhúng các sản phẩm phân tích vào prompt mà chúng tôi đưa cho mô hình ngôn ngữ, và đánh giá lợi ích của những sản phẩm phân tích như vậy. Thông tin mà chúng tôi rút ra và thêm vào dựa trên trực giác của chúng tôi về các loại "lemma" mà các lập trình viên có ý thức hoặc vô thức xem xét khi họ tìm cách hiểu và tóm tắt mã nguồn.

Chúng tôi thấy rằng việc cung cấp thông tin như vậy cải thiện hiệu suất của LLM. Chúng tôi nhắc nhở người đọc rằng hầu hết công việc liên quan đến các mô hình ngôn ngữ lớn (LLM) thường sử dụng một số hình thức kỹ thuật prompt engineering để tăng hiệu suất. Trong bài báo này, chúng tôi cho thấy rằng cách tiếp cận A𝑆𝐴𝑃, tăng cường các prompt bằng các sản phẩm phân tích mã nguồn, cải thiện so với các cách tiếp cận prompting trước đây.

2.3 Tóm Tắt Mã Nguồn
Mã nguồn được tài liệu hóa tốt dễ bảo trì hơn nhiều; do đó, các lập trình viên có kinh nghiệm thường thêm, ví dụ, header tóm tắt hàm. Tuy nhiên, các nhận xét tóm tắt có thể trở nên lỗi thời, khi các dự án phát triển [11,22]. Tóm tắt mã nguồn tự động do đó là một nhiệm vụ có động lực tốt, đã thu hút rất nhiều sự chú ý; và tiến bộ đáng kể (mặc dù từng bước, qua nhiều năm) đã được thực hiện. Ban đầu, các cách tiếp cận dựa trên template được ưa chuộng [17, 26,27,56,61]; tuy nhiên, việc tạo ra một danh sách các template với độ bao phủ tốt là rất thách thức. Sau đó, các nhà nghiên cứu tập trung vào cách tiếp cận dựa trên truy xuất (IR) [17,26,27,56], nơi mã nguồn hiện có (với một tóm tắt) được truy xuất dựa trên các chỉ số dựa trên độ tương tự. Tuy nhiên, cách tiếp cận đầy hứa hẹn này chỉ hoạt động nếu một cặp mã-nhận xét tương tự có thể được tìm thấy trong pool có sẵn.

Trong khi đó, sự tương tự của tóm tắt mã nguồn với Dịch Thuật Máy Thần Kinh (NMT), (người ta có thể nghĩ về việc tạo ra một tóm tắt tiếng Anh của mã nguồn như sản xuất một đại diện của "cùng một ý nghĩa trong một ngôn ngữ khác") dẫn đến nghiên cứu áp dụng Dịch Thuật Máy Thần Kinh (NMT) cho tóm tắt mã nguồn. Nhiều nghiên cứu đã được thực hiện trong lĩnh vực này [1,30,33,41]. Một số đã kết hợp các cách tiếp cận trước đây, như cách tiếp cận dựa trên template và dựa trên truy xuất, sử dụng các mô hình thần kinh [69], và đã báo cáo kết quả đầy hứa hẹn. Những phương pháp thần kinh như vậy cho NLP đã được cải thiện rất nhiều, nhờ vào phong cách kiến trúc Transformer.

Cho đến gần đây, các mô hình ngôn ngữ được huấn luyện trước như CodeBERT, CodeT5, và CodeT5+ hoạt động tốt nhất cho tóm tắt mã nguồn.

[Hình 1: Các bước khác nhau của A𝑆𝐴𝑃. (1) Mã nguồn đầu vào và (2) Pool mẫu được đưa cho công cụ BM25, khớp mã nguồn đầu vào đã cho với pool và (3) truy xuất các mẫu khớp nhất, tức là 3 cặp input+output. Những ví dụ này được xử lý bởi A𝑆𝐴𝑃 để tạo ra một prompt (4) bao gồm 3 exemplar. Mỗi exemplar bao gồm một định nghĩa hàm, kết quả phân tích định nghĩa đó, và nhận xét liên quan của nó; mã nguồn đầu vào cuối cùng được nối thêm, cùng với sản phẩm phân tích của nó. Chi tiết exemplar trong Hình 2. Prompt cuối cùng được gửi qua API call (5) đến mô hình GPT-3.x; đầu ra được trả về, ví dụ tóm tắt (6) được trả về bởi GPT-3x.]

Tuy nhiên, các Mô Hình Ngôn Ngữ Lớn (LLM) hiện thường vượt trội hơn các mô hình được huấn luyện trước nhỏ hơn trong nhiều vấn đề. Ahmed & Devanbu [3] báo cáo rằng LLM có thể vượt trội hơn các mô hình ngôn ngữ được huấn luyện trước với một prompt đơn giản chỉ bao gồm một vài mẫu đã có trong cùng dự án; công việc này minh họa sự hứa hẹn của việc xây dựng cẩn thận các cấu trúc prompt (c.f. "kỹ thuật prompt engineering"). Chúng tôi trình bày A𝑆𝐴𝑃 ở đây như một nguyên tắc chung khác của kỹ thuật prompt engineering. Chúng tôi nhấn mạnh, một lần nữa, rằng tiến bộ trong tóm tắt mã nguồn (và các ứng dụng khác của AI cho SE, như vá mã nguồn, phát hiện lỗi, kiểm thử, v.v.) đã mang tính từng bước, như trong lĩnh vực NMT, nơi các hệ thống dịch thuật thực tế, có thể sử dụng được mất hàng thập kỷ để xuất hiện. Do đó những tiến bộ từng bước vẫn cần thiết, và hữu ích, và chúng tôi đóng góp công việc của mình vào doanh nghiệp dài hạn này.

3 BỘ DỮ LIỆU & PHƯƠNG PHÁP
Bây giờ chúng tôi thảo luận về bộ dữ liệu, mô hình, và phương pháp của chúng tôi.

3.1 Bộ Dữ Liệu
Các thí nghiệm của chúng tôi sử dụng bộ dữ liệu CodeSearchNet [32] được sử dụng rộng rãi; CodeSearchNet được xây dựng bằng cách trích xuất đoạn đầu tiên của tài liệu tiền tố hàm, tùy thuộc vào một số hạn chế (ví dụ độ dài). Đây là một bộ dữ liệu đa dự án được khử trùng lặp cẩn thận, cho phép kiểm thử cross-project (đòi hỏi khắt khe hơn). Khử trùng lặp là chìa khóa: Sự trùng lặp mã nguồn trong các mô hình machine learning có thể làm tăng các chỉ số hiệu suất một cách lừa dối rất nhiều, khi so sánh với các bộ dữ liệu được khử trùng lặp [7, 46, 59].

Nó là một phần của benchmark CodeXGLUE [47], bao gồm 14 bộ dữ liệu cho 10 nhiệm vụ kỹ thuật phần mềm. Nhiều mô hình đã được đánh giá trên bộ dữ liệu này. CodeSearchNet chứa hàng nghìn mẫu từ sáu ngôn ngữ lập trình khác nhau (tức là, Java, Python, JavaScript, Ruby, Go, PHP). Tuy nhiên, chúng tôi không sử dụng toàn bộ bộ dữ liệu kiểm thử, điều này sẽ quá đắt

--- TRANG 4 ---
ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr

[Hình 2: Các thành phần của một A𝑆𝐴𝑃 Exemplar. Source Code và Output Comment được trích xuất từ mẫu pool được truy xuất. Thông tin Repo được rút ra từ mã nguồn sử dụng GitHub; Thông tin Dataflow và tagged Identifiers với nhãn được thu thập từ phân tích sử dụng Treesitter.]

[THIS IS TABLE: Bảng 1: Số lượng mẫu huấn luyện và kiểm thử.]
Ngôn ngữ | Số mẫu huấn luyện | Số mẫu kiểm thử
Java | 164,923 | 1000
Python | 251,820 | 1000
Ruby | 24,927 | 1000
JavaScript | 58,025 | 1000
Go | 167,288 | 1000
PHP | 241,241 | 1000

và chậm khi sử dụng các endpoint API mô hình của chúng tôi; thay vào đó, chúng tôi đã chọn 1000 mẫu³ một cách ngẫu nhiên từ mỗi ngôn ngữ. Vì bộ dữ liệu gốc là cross-project và chúng tôi lấy mẫu nó một cách đồng nhất, mẫu con của chúng tôi bao gồm dữ liệu cross-project. Ngoài ra, chúng tôi đã tạo một tập con của bộ dữ liệu này cho few-shotting cùng dự án, theo Ahmed và Devanbu [3]: chúng tôi sắp xếp dữ liệu cùng dự án theo ngày tạo (sử dụng git blame). Bây giờ, chúng tôi sử dụng thứ tự thời gian để đảm bảo rằng chỉ các mẫu trước đó về mặt thời gian được sử dụng cho các mẫu few-shot; điều này là thực tế, vì chỉ dữ liệu cũ, đã tồn tại mới có sẵn để sử dụng.

Chúng tôi sẽ đi sâu hơn vào bộ dữ liệu cùng dự án này trong Mục 4.3.

Như đã đề cập trước đó, chúng tôi không sử dụng bất kỳ huấn luyện thay đổi tham số nào trên mô hình; chúng tôi chỉ chèn một vài mẫu được chọn từ tập con huấn luyện vào prompt few-shot. Bảng 1 liệt kê số lượng mẫu huấn luyện & kiểm thử được sử dụng trong các thí nghiệm của chúng tôi.

3.2 Các Mô Hình
Trong công việc trước đây, các mô hình ngôn ngữ được huấn luyện trước dựa trên transformer mang lại lợi ích đáng kể, trong cả NLP và kỹ thuật phần mềm. Các mô hình ngôn ngữ được huấn luyện trước có thể được chia thành ba danh mục: mô hình chỉ encoder, encoder-decoder, và chỉ decoder. Trong khi các mô hình encoder-decoder ban đầu đã thể hiện thành công trong nhiều nhiệm vụ, các LLM chỉ decoder hiện có thể mở rộng và hiệu quả hơn cho nhiều nhiệm vụ.

Mô hình Encoder-Decoder. BERT là một trong những mô hình ngôn ngữ được huấn luyện trước sớm nhất [15]; nó được huấn luyện trước trên hai nhiệm vụ tự giám sát: Masked Language Modeling (MLM) và Next Sentence Prediction (NSP). Sau đó, RoBERTa [45] được giới thiệu với một số thay đổi nhỏ so với BERT. Chỉ sử dụng huấn luyện MLM, nó vượt trội hơn BERT. CodeBERT [21] và GraphCodeBERT [25] đã giới thiệu những ý tưởng này vào Kỹ Thuật Phần Mềm. Mặc dù CodeBERT và GraphCodeBERT là các mô hình chỉ encoder, chúng có thể được áp dụng cho tóm tắt mã nguồn sau khi fine-tuning, được cascaded với một decoder được huấn luyện trong quá trình fine-tuning. Ahmed & Devanbu báo cáo rằng các mô hình polyglot, được fine-tuned với dữ liệu đa ngôn ngữ, vượt trội hơn các đối tác đơn ngôn ngữ của chúng [4]. Họ cũng báo cáo rằng các định danh đóng một vai trò quan trọng trong các nhiệm vụ tóm tắt mã nguồn. PLBART [2], CodeT5 [64], và CodeT5+ [63] cũng bao gồm các decoder được huấn luyện trước và được báo cáo là hoạt động tốt cho các nhiệm vụ tóm tắt mã nguồn. Gần đây hơn, các LLM tự hồi quy quy mô rất lớn (chỉ decoder) (với 175B+ tham số) đã được tìm thấy là thành công trong tóm tắt mã nguồn với học few-shot, mà không cần bất kỳ huấn luyện rõ ràng nào. Trong phần tiếp theo, chúng tôi sẽ giới thiệu ngắn gọn ba mô hình OpenAI mà chúng tôi đã xem xét cho các thí nghiệm của mình.

³Vui lòng xem thảo luận về sức mạnh thí nghiệm trong Mục 7.

Mô hình Chỉ Decoder. Trong generative pre-training, nhiệm vụ là dự đoán token tiếp theo một cách tự hồi quy dựa trên các token trước đó di chuyển từ sớm đến muộn. Việc huấn luyện tự hồi quy một chiều này ngăn cản mô hình tập hợp thông tin từ các token tương lai. Các mô hình generative mới hơn như GPT [52], GPT-2 [53] và GPT-3 [12], cũng được huấn luyện theo cách này, nhưng chúng có nhiều tham số hơn, và được huấn luyện trên các bộ dữ liệu lớn hơn nhiều. Các mô hình ngôn ngữ lớn hiện tại, như GPT-3, có khoảng (hoặc nhiều hơn) 175B tham số. Những mô hình mạnh mẽ này hoạt động tốt đến mức, với few-shot prompting, mà sự quan tâm đến việc điều chỉnh tham số cụ thể cho nhiệm vụ thông qua fine-tuning đã giảm.

Codex là một biến thể GPT-3, được huấn luyện mạnh mẽ trên mã nguồn và nhận xét ngôn ngữ tự nhiên. Gia đình Codex bao gồm hai phiên bản: Codex-Cushman, nhỏ hơn, với 12B tham số, và Codex-Davinci, lớn nhất, với 175B tham số. Mô hình Codex được sử dụng rộng rãi, cho nhiều nhiệm vụ khác nhau. Các thí nghiệm của chúng tôi chủ yếu nhắm vào mô hình Code-Davinci, đặc biệt là Code-Davinci-002, xuất sắc trong việc dịch ngôn ngữ tự nhiên sang mã nguồn [14] và hỗ trợ hoàn thành mã nguồn cũng như chèn mã nguồn⁴. Một số biến thể mới, Text-Davinci-003 & GPT-3.5-turbo, cũng có sẵn; khác với các biến thể Codex, những mô hình này hiểu và tạo ra cả ngôn ngữ tự nhiên và mã nguồn. Mặc dù được tối ưu hóa cho chat, GPT-3.5-turbo cũng hoạt động tốt trong các nhiệm vụ completion truyền thống. Text-Davinci-003 là một mô hình completion như Code-Davinci-002. Chúng tôi nghiên cứu cách cải tiến prompt của chúng tôi hoạt động bằng cách sử dụng các mô hình Text-Davinci-003 & GPT-3.5-turbo.

3.3 Truy Xuất Mẫu từ Dữ Liệu Huấn Luyện
Như đã lưu ý trước đó, học few-shot hoạt động khá tốt, khi được sử dụng với các mô hình rất lớn. Chúng tôi nhắc nhở mô hình với một số lượng nhỏ các mẫu ⟨problem,solution⟩, và yêu cầu nó giải quyết một vấn đề mới. Tuy nhiên, việc lựa chọn mẫu cẩn thận cho học few-shot là hữu ích. Nashid et al. đã phát hiện ra rằng việc chọn mẫu dựa trên truy xuất là hữu ích cho các vấn đề như tạo assertion và sửa chữa chương trình [48]. Theo khuyến nghị của họ, chúng tôi sử dụng thuật toán IR BM25 để chọn các mẫu few-shot liên quan từ tập huấn luyện. BM25 [55] là một phương pháp truy xuất dựa trên tần suất cải thiện so với TF-IDF [54]. Chúng tôi lưu ý sự cải thiện đáng kể so với cùng những mẫu cố định trong học few-shot, như được chi tiết trong Mục 4.1. Nashid et al. so sánh một số phương pháp truy xuất, và thấy BM25 hoạt động tốt nhất; do đó chúng tôi cũng sử dụng nó.

⁴https://openai.com/

3.4 Tăng Cường Prompt Ngữ Nghĩa Tự Động
Phần này trình bày ba sự kiện ngữ nghĩa mà chúng tôi đã chọn để tăng cường prompt của A𝑆𝐴𝑃 và pipeline A𝑆𝐴𝑃 (Xem Hình 2). Việc lựa chọn những sự kiện này đến từ việc áp dụng giả thuyết trung tâm của chúng tôi, tức là việc tăng cường prompt bằng những gì các lập trình viên nghĩ về khi làm việc trên một nhiệm vụ, cho nhiệm vụ tóm tắt mã nguồn. A𝑆𝐴𝑃 không gắn liền với bất kỳ sự kiện ngữ nghĩa cụ thể nào hoặc phân tích tĩnh; nó có thể dễ dàng kết hợp những cái khác, như được thảo luận sau.

Tên Repository & Đường Dẫn. Việc tăng cường prompt bằng thông tin cụ thể cho domain có thể cải thiện hiệu suất của LLM trong nhiều nhiệm vụ. Nghiên cứu trước đây cho thấy rằng việc tăng cường prompt bằng mã nguồn từ cùng repository cải thiện hiệu suất trong các nhiệm vụ tạo mã nguồn [60]. Chúng tôi lập luận rằng thông tin meta cơ bản ở cấp repository, như tên repository và đường dẫn đầy đủ đến repository, cung cấp ngữ cảnh bổ sung. Ví dụ, tên repository như "tony19/logback −android", "apache/parquet −mr", và "ngageoint/geo−package −android" đều kết nối một hàm với một domain cụ thể (ví dụ, android, apache, geo-location), có thể tăng cường hiểu biết về mã nguồn mục tiêu cần tóm tắt. Hình 2 (phần màu vàng) trình bày một ví dụ về cách chúng tôi tăng cường prompt bằng thông tin cấp repository. Tương tự như tên repository, đường dẫn đến hàm cũng có thể đóng góp cho mô hình.

Tagged Identifiers. Nghiên cứu trước đây cho thấy rằng các mô hình ngôn ngữ tìm thấy giá trị nhiều hơn trong các định danh, thay vì cấu trúc mã nguồn, khi tạo ra tóm tắt mã nguồn [4]. Tuy nhiên, các định danh đóng các vai trò khác nhau trong mã nguồn. Biến cục bộ, tên hàm, tham số, biến toàn cục, v.v., đóng các vai trò khác nhau trong hoạt động của phương thức mà chúng xuất hiện; một lập trình viên đọc mã nguồn chắc chắn nhận thức được các vai trò của định danh, chỉ đơn giản bằng cách xác định phạm vi và cách sử dụng. Do đó, việc tăng cường prompt bằng các vai trò cụ thể của các định danh có thể giúp mô hình "hiểu" hàm tốt hơn. Chúng tôi sử dụng tree-sitter để duyệt AST của hàm và thu thập các định danh, cùng với vai trò của chúng. Hình 2 (phần màu xanh) trình bày một ví dụ mẫu cho thấy cách chúng tôi tăng cường prompt của hàm bằng tagged identifiers.

Mặc dù mô hình có quyền truy cập vào chuỗi token của mã nguồn, và do đó cũng tất cả các định danh, việc cung cấp chúng cho mô hình dưới dạng tagged có thể a) tiết kiệm cho mô hình một số nỗ lực tính toán, và b) điều kiện tốt hơn đầu ra của mô hình.

Data Flow Graph (DFG). Guo et al. đã giới thiệu mô hình GraphcodeBERT, sử dụng đồ thị luồng dữ liệu (DFG) thay vì cấu trúc cấp cú pháp như cây cú pháp trừu tượng (AST) trong giai đoạn pre-training [25]. GraphcodeBERT vượt trội hơn CodeBERT [21] trong nhiều nhiệm vụ kỹ thuật phần mềm (SE). Chúng tôi kết hợp thông tin DFG này vào các mẫu few-shot; chúng tôi suy đoán rằng điều này cung cấp cho mô hình một hiểu biết ngữ nghĩa tốt hơn về mỗi mẫu, và ví dụ mục tiêu. Hình 2 (màu cam) trình bày một mẫu cho thấy Data Flow Graph (DFG) mà chúng tôi đã sử dụng cho các thí nghiệm của mình. Mỗi dòng chứa một định danh với chỉ số của nó và chỉ số của các định danh mà dữ liệu đặc biệt đó chảy tới. Khác với repo và tagged identifiers, đồ thị luồng dữ liệu có thể rất dài, khiến việc thêm luồng dữ liệu hoàn chỉnh vào prompt trở nên bất tiện. Trong trường hợp prompt dài, chúng tôi chỉ giữ 30 dòng đầu tiên của DFG trong prompt. Ngoài các định danh, DFG cũng cung cấp hiểu biết tốt hơn về tầm quan trọng của các định danh trong hàm.

Use Case & Completion Pipeline. A𝑆𝐴𝑃 có 3 thành phần: một LLM, một pool các mẫu có sẵn (các cặp đầu vào-đầu ra được gắn nhãn, ví dụ, mã nguồn với nhận xét), và một công cụ phân tích tĩnh để rút ra sự kiện từ mã nguồn (Xem Hình 1 và 2).

Một tệp cấu hình chỉ định những thành phần này. Sau khi được cấu hình, một lập trình viên gọi A𝑆𝐴𝑃 trên một thân hàm 𝐶𝑖𝑛 (Hình 1), mà một đầu ra (ví dụ, tóm tắt mã nguồn) được mong muốn. A𝑆𝐴𝑃 sử dụng 𝐶𝑖𝑛 như một truy vấn BM25 trên pool mẫu của nó để có được một tập kết quả các ứng viên mẫu ec1,ec2,..., nơi mỗi ec𝑖 là một cặp có dạng ⟨input𝑖,output𝑖⟩; trong ngữ cảnh của chúng tôi, input𝑖 là định nghĩa hàm và output𝑖 là nhận xét header hàm. BM25 chọn những input𝑖 khớp tốt nhất với 𝐶𝑖𝑛 đã cho. A𝑆𝐴𝑃 sau đó áp dụng phân tích chương trình cho cả đầu vào 𝐶𝑖𝑛 và một số đầu vào mẫu input𝑖, tạo ra các sản phẩm phân tích 𝑎𝑝𝑖𝑛 và một số 𝑎𝑝𝑖.

Mỗi mẫu 𝑒𝑖 (Hình 2) là bộ ba: ⟨input𝑖,ap𝑖,output𝑖⟩, nơi mỗi bộ ba minh họa, cho LLM, cách mã nguồn đầu vào input𝑖 liên quan, thông qua sản phẩm phân tích 𝑎𝑝𝑖, với đầu ra output𝑖. Prompt cuối cùng sau đó là "𝑒1||𝑒2||𝑒3||𝐶𝑖𝑛||ap𝑖𝑛". A𝑆𝐴𝑃 truy vấn một LLM với prompt đó, và trả về completion (ví dụ, tóm tắt ngôn ngữ tự nhiên).

Theo mặc định, A𝑆𝐴𝑃 được cấu hình với các phân tích để trích xuất thông tin repo, gắn thẻ định danh, xây dựng DFG. Những phân tích này độc lập và các đầu ra của chúng được gắn nhãn riêng biệt trong prompt. Ví dụ, Hình 2 cho thấy đầu ra của phân tích DFG trong prompt được xây dựng của A𝑆𝐴𝑃. Những ví dụ few shot này, được tăng cường và chèn vào prompt: mã nguồn, thông tin repository, tagged identifiers, DFG, và tóm tắt mong muốn (Gold) đều được bao gồm trong mỗi few-shot. Ví dụ mục tiêu chỉ bao gồm sản phẩm phân tích, và LLM được nhắc nhở để tạo ra đầu ra mong muốn.

Trong nghiên cứu trước đây sử dụng "chain of thought" [66] hoặc "step by step" [40] reasoning, không có thông tin như vậy được đưa cho mô hình; thay vào đó, prompt chỉ đơn giản giúp nó tổ chức lý luận của mình về mẫu thành một chuỗi hướng dẫn. Ở đây, thay vì để mô hình làm lý luận của riêng mình, chúng tôi định hình lý luận của nó từ bên ngoài bằng cách sử dụng phân tích chương trình đơn giản, vì chúng tôi có thể nhận được thông tin rất chính xác từ các công cụ phân tích rất hiệu quả. Mỗi ví dụ few-shot bao gồm mã nguồn, thông tin rút ra, và kết luận (tóm tắt), do đó cung cấp các "chains of thought" mẫu cho mô hình sử dụng ngầm khi tạo ra tóm tắt mục tiêu mong muốn. Hình 1 trình bày pipeline tổng thể của cách tiếp cận của chúng tôi mà chúng tôi áp dụng cho mỗi mẫu. Công cụ BM25 khớp mã nguồn đầu vào với pool mẫu, A𝑆𝐴𝑃 xử lý các ví dụ kết quả để tạo ra một prompt, và prompt cuối cùng được gửi đến mô hình GPT-3.x qua API, tạo ra một tóm tắt như đầu ra.

Tiếp theo, chúng tôi mô tả cách chúng tôi đánh giá pipeline này.

3.5 Chỉ Số
BLEU [50] là thước đo dựa trên độ tương tự được sử dụng rộng rãi nhất cho tóm tắt mã nguồn [57] và tạo commit log [16]. BLEU đếm phần của các 𝑛-gram (thường cho 𝑛∈[1..4]), xuất hiện trong cả ứng viên được tạo ra và một hoặc nhiều bản dịch tham chiếu; trung bình nhân của những phần này là BLEU, thường được chuẩn hóa thành phạm vi 0-100. Ở mức độ chi tiết câu, BLEU có xu hướng phạt quá mức các bản dịch ứng viên khi ít (hoặc không có) các n-gram dài hơn cùng xuất hiện, vì vậy "Sentence BLEU" đã bị chỉ trích vì tương quan kém với đánh giá của con người. Các kỹ thuật làm mịn khác nhau [13,23,44] đã được sử dụng, để giảm độ nhạy cảm của Sentence BLEU với các khớp 𝑛-gram thưa thớt, và canh chỉnh nó tốt hơn với đánh giá chất lượng của con người. Chúng tôi báo cáo dữ liệu về hai biến thể: BLEU-CN, sử dụng một loại làm mịn Laplacian [2,3,8,21,33,47,64] và BLEU-DC, sử dụng các phương pháp làm mịn mới hơn [29,65]. Các chỉ số khác được đề xuất như BERTScore [28,70], BLEURT [58], NUBIA [37], tốn kém về mặt tính toán, không được sử dụng rộng rãi và do đó không dễ dàng so sánh với công việc trước đây để benchmarking.

Với tất cả những lựa chọn này, các chỉ số cho tóm tắt mã nguồn và, độc lập, cho tạo commit-log [16], đã được tranh luận [24, 28,57]. Trong bài báo này, chúng tôi theo nghiên cứu trước đây và chủ yếu sử dụng BLEU-CN; điều này tạo điều kiện cho việc so sánh kết quả của chúng tôi với nghiên cứu trước đây. Benchmark CodeXGLUE khuyến nghị BLEU-CN, và hầu hết các mô hình mới hơn [3,21,64] sử dụng chỉ số này. Tuy nhiên, chúng tôi đã không bỏ qua các biện pháp khác. Ngoài BLEU-CN, và BLEU-DC, chúng tôi cũng báo cáo kết quả sử dụng ROUGE-L [43] và METEOR [9].

Trong tất cả các trường hợp, A𝑆𝐴𝑃 đạt được cải thiện tổng thể đáng kể: chúng tôi quan sát thấy lợi ích lớn hơn 2.0 BLEU cho tất cả các ngôn ngữ lập trình ngoại trừ Go (Bảng 3). Chúng tôi khẳng định rằng lợi ích lớn hơn 2.0 BLEU quan trọng vì hai lý do. Roy et al. [57] cung cấp lập luận, dựa trên nghiên cứu chủ thể con người rằng đối với tóm tắt mã nguồn (nhiệm vụ trung tâm của chúng tôi), rằng lợi ích 2.0 trở lên BLEU có nhiều khả năng tương ứng với nhận thức cải thiện của con người. Thứ hai, chúng tôi lập luận rằng thậm chí lợi ích nhỏ hơn cũng quan trọng (đặc biệt nếu có thể lặp lại và có ý nghĩa thống kê) vì tiến bộ từng bước trong những nhiệm vụ như vậy tích lũy, hướng tới tác động thực tế mạnh mẽ, như được chứng minh bởi công việc kéo dài hàng thập kỷ trong dịch thuật ngôn ngữ tự nhiên.

Ngoài tóm tắt mã nguồn, chúng tôi đã đánh giá cách tiếp cận A𝑆𝐴𝑃 trong nhiệm vụ hoàn thành mã nguồn. Các chỉ số tiêu chuẩn được sử dụng cho nhiệm vụ này là khớp chính xác (completion có khớp chính xác không) và độ tương tự chỉnh sửa (completion gần với chuỗi mong đợi như thế nào). Ở đây cũng vậy, A𝑆𝐴𝑃 đạt được cải thiện tổng thể đáng kể.

3.6 Thiết Lập Thí Nghiệm & Tiêu Chí Đánh Giá
Mô hình chính của chúng tôi là code-davinci-002 của OpenAI. Chúng tôi sử dụng phiên bản beta, thông qua API dịch vụ web của nó. Để cân bằng các ràng buộc tính toán như giới hạn tốc độ và mong muốn ước lượng mạnh mẽ hiệu suất, chúng tôi đã chọn sử dụng 1000 mẫu⁵ mỗi xử lý thí nghiệm (một xử lý cho mỗi ngôn ngữ, mỗi cách tiếp cận chọn few-shot, với A𝑆𝐴𝑃, không có A𝑆𝐴𝑃, v.v.).

Các thí nghiệm của chúng tôi mang lại kết quả có ý nghĩa thống kê, có thể giải thích trong hầu hết các trường hợp. Mỗi thử nghiệm 1000 mẫu vẫn mất 5 đến 8 tiếng, thay đổi (có thể) với các yếu tố tải của OpenAI. Chúng tôi bao gồm thời gian chờ giữa các lần thử, theo khuyến nghị của OpenAI. Để có được câu trả lời được định nghĩa rõ ràng từ mô hình, chúng tôi thấy cần thiết phải đặt temperature thành 0, cho tất cả các thí nghiệm của chúng tôi. Mô hình được thiết kế để cho phép một cửa sổ khoảng 4K token; điều này giới hạn số lượng mẫu few-shot. Cho các thí nghiệm của chúng tôi, chúng tôi sử dụng 3 shot. A𝑆𝐴𝑃 mặc định 3 shot vì nghiên cứu liên quan [3,12] đã cho thấy, và các thí nghiệm của chúng tôi với A𝑆𝐴𝑃 đã xác nhận, rằng nhiều shot hơn không cải thiện hiệu suất đáng kể. Tuy nhiên, đối với tối đa 2% các mẫu được chọn ngẫu nhiên trong mỗi thí nghiệm, chúng tôi không có được kết quả tốt; hoặc prompt không vừa vào cửa sổ của mô hình, hoặc mô hình một cách bí ẩn tạo ra chuỗi trống. Trong các trường hợp prompt như được xây dựng với 3 mẫu quá dài, chúng tôi tự động giảm số lượng shot. Khi tóm tắt trống được phát ra, chúng tôi giải quyết điều này bằng cách tăng số lượng shot. Quy trình đơn giản, có thể lặp lại, chi phí khiêm tốn này có thể được kết hợp vào các công cụ tóm tắt tự động.

⁵Vui lòng xem Mục 7 để biết lý do.

4 KẾT QUẢ
Chúng tôi đánh giá lợi ích của các prompt được tăng cường A𝑆𝐴𝑃, cho tóm tắt mã nguồn, trong các thiết lập khác nhau và sử dụng nhiều chỉ số. Chúng tôi tìm thấy bằng chứng về lợi ích hiệu suất tổng thể, trong các nghiên cứu trên sáu ngôn ngữ. Tuy nhiên, đối với các phân tích chi tiết khác, chúng tôi tập trung chủ yếu vào Java và Python, do giới hạn tốc độ API OpenAI.

4.1 Encoder-decoder & Học Few-shot
Kết quả baseline của chúng tôi trên CodeSearchNet [47], sử dụng few-shotting dựa trên IR, đến trước. Nghiên cứu trước đây báo cáo rằng các phương pháp IR có thể tìm thấy mẫu tốt hơn cho few-shot prompting, cho các nhiệm vụ như sửa chữa chương trình [48] và tạo mã nguồn [34]. Trong Bảng 2, chúng tôi quan sát thấy rằng điều này cũng đúng cho tóm tắt mã nguồn; chúng tôi lưu ý cải thiện 3.00 (15.10%) và 1.12 (5.42%) trong điểm BLEU-4 cho Java và Python, tương ứng, chỉ đơn giản bằng cách sử dụng BM25 như một cơ chế chọn mẫu few-shot. Vì BM25 đã được sử dụng trong bài báo trước đây (mặc dù cho các nhiệm vụ khác) [48], chúng tôi coi học few-shot dựa trên BM25 này cho tóm tắt mã nguồn chỉ là một baseline (không phải là một đóng góp per se) của bài báo này.

4.2 Tăng Cường Prompt A𝑆𝐴𝑃
Bây giờ chúng tôi tập trung vào kết quả trung tâm của bài báo của chúng tôi: tác động của tăng cường prompt A𝑆𝐴𝑃. Bảng 3 cho thấy cải thiện theo từng thành phần và tổng thể đạt được sau khi kết hợp tất cả các thành phần prompting cho tất cả sáu ngôn ngữ lập trình. Cải thiện BLEU dao động từ 1.84 (8.12%) đến 4.58 (16.27%). Trong hầu hết các trường hợp, chúng tôi thấy cải thiện trên 2.0 BLEU, ngưỡng cần thiết cho nhận thức của con người được lưu ý bởi Roy et al. [57].

Chúng tôi cũng lưu ý rằng cả ba thành phần (tức là, Thông tin Repository, DFG Data Flow Graph, Identifiers) giúp mô hình đạt được hiệu suất tốt hơn trong tất cả sáu ngôn ngữ, khi chúng tôi kết hợp những thành phần này từng cái một với BM25. Tuy nhiên, đối với Ruby, sự kết hợp hoạt động tốt nhất chỉ bao gồm thông tin Repo. Trong hầu hết các trường hợp, Repo. giúp ích rất nhiều, tương đối so với các thành phần khác.

Để xác định ý nghĩa cải thiện, chúng tôi đã sử dụng kiểm định cặp đôi một phía Wilcoxon signed-rank, tìm thấy ý nghĩa thống kê trong tất cả các trường hợp cho prompt cuối cùng của chúng tôi khi so sánh với học few-shot BM25 thuần túy, thậm chí sau khi điều chỉnh rủi ro phát hiện sai.

4.3 Tóm Tắt Mã Nguồn Cùng Dự Án
Bây giờ chúng tôi xem xét lợi ích của A𝑆𝐴𝑃 trong bối cảnh một số nghiên cứu trước đây về chọn few-shot. Nghiên cứu trước đây đã cho thấy rằng việc chọn few-shot từ cùng dự án cải thiện hiệu suất đáng kể [3]. Để xem liệu ý tưởng tăng cường prompt của chúng tôi có tiếp tục giúp trong tóm tắt mã nguồn cụ thể cho dự án không, chúng tôi đã đánh giá cách tiếp cận của mình trên bộ dữ liệu từ Ahmed và Devanbu [3]. Do giới hạn tốc độ, chúng tôi đã giảm số lượng mẫu kiểm thử xuống 100 cho mỗi dự án trong bốn dự án Java và Python. Vì chúng tôi có quá ít mẫu cho một kiểm thử mỗi dự án, chúng tôi đã kết hợp tất cả các mẫu để thực hiện

--- TRANG 7 ---
Tăng Cường Ngữ Nghĩa Tự Động của Prompt Mô Hình Ngôn Ngữ
(cho Tóm Tắt Mã Nguồn) ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha

[THIS IS TABLE: Bảng 2 - Hiệu suất của các mô hình encoder-decoder và few-shot trên tóm tắt mã nguồn Java và Python, đo bằng BLEU]
Ngôn ngữ | CodeBERT | GraphCodeBERT | Polyglot CodeBERT | Polyglot GraphcodeBERT | CodeT5 | CodeT5+ | Few-shot (ngẫu nhiên) | Few-shot với BM25 | Lợi ích (%) so với ngẫu nhiên few-shot
Java | 18.8 | 18.52 | 20.22 | 19.94 | 19.78 | 19.83 | 19.87 | 22.87 | +15.10%
Python | 17.73 | 17.35 | 18.19 | 18.33 | 19.98 | 18.85 | 20.66 | 21.78 | +5.42%

[THIS IS TABLE: Bảng 3 - Hiệu suất của tạo nhận xét tăng cường prompt với mô hình code-davinci-002, đo bằng BLEU]
Ngôn ngữ | BM25 | BM25+repo | BM25+id | BM25+DFG | A𝑆𝐴𝑃 | So sánh với BM25 - Lợi ích (%) so với BM25 | p-value
Java | 22.87 | 25.23 | 23.39 | 23.13 | 25.41 | +11.11% | <0.01
Python | 21.78 | 24.22 | 22.54 | 21.82 | 24.26 | +11.39% | <0.01
Ruby | 17.21 | 19.67 | 19.19 | 17.55 | 19.62 | +14.00% | <0.01
JavaScript | 23.27 | 25.11 | 24.21 | 24.04 | 25.36 | +8.98% | <0.01
Go | 22.67 | 24.41 | 23.2 | 23.42 | 24.51 | +8.12% | <0.01
PHP | 28.15 | 32.07 | 29.8 | 28.92 | 32.73 | +16.27% | <0.01
Tổng thể | 22.66 | 25.12 | 23.72 | 23.15 | 25.32 | +11.74% | <0.01

kiểm định thống kê. Lưu ý rằng tổng cỡ mẫu của chúng tôi cho kiểm định thống kê vượt quá số lượng mẫu cần thiết được xác định thông qua phân tích được đề cập trong Mục 7. Khi làm việc với cùng dự án, người ta phải chia dữ liệu cẩn thận, để tránh rò rỉ từ các mẫu tương lai (nơi đầu ra mong muốn có thể đã tồn tại) sang các mẫu quá khứ. Do đó, chúng tôi đã sắp xếp các mẫu theo ngày tạo trong bộ dữ liệu này. Sau khi tạo ra bộ dữ liệu, chúng tôi áp dụng cách tiếp cận của mình để đánh giá hiệu suất trong thiết lập cùng dự án. Chúng tôi cũng so sánh kết quả của mình với thiết lập cross-project, nơi chúng tôi truy xuất các mẫu từ tập huấn luyện cross-project hoàn chỉnh, tương tự như thiết lập được sử dụng trong Mục 4.2.

[THIS IS TABLE: Bảng 4 - Hiệu suất của tạo nhận xét tăng cường prompt với mô hình code-davinci-002 trên dữ liệu cùng dự án]
[Table shows performance metrics for different projects with cross-project and same-project comparisons]

Bảng 4 cho thấy kết quả tóm tắt mã nguồn dựa trên dự án. Lưu ý rằng đây là một tình huống cụ thể cho dự án nơi dữ liệu hoàn toàn không có sẵn. Dữ liệu huấn luyện cho mỗi dự án rất hạn chế. Chúng tôi thấy rằng, đối với 4 dự án, học few-shot cross-project mang lại hiệu suất tốt nhất; trong khi, đối với 4 dự án khác, học few-shot cùng dự án hiệu quả nhất. Chúng tôi lưu ý rằng Ahmed & Devanbu không sử dụng IR để chọn mẫu few-shot và luôn luôn đạt được kết quả tốt hơn với học few-shot cùng dự án [3]. IR thực sự tìm thấy các ví dụ liên quan trong các mẫu lớn có sẵn cho Java & Python, và chúng tôi có được kết quả tốt. Chúng tôi đã phân tích 16 cặp BLEU trung bình từ 8 dự án, xem xét cả tình huống cross-project và cùng dự án. Học few-shot tăng cường prompt của chúng tôi vượt trội hơn học few-shot truy xuất BM25 thuần túy trong 14 trường hợp (87.5%). Điều này cho thấy rằng tăng cường prompt A𝑆𝐴𝑃 hữu ích trên các dự án. A𝑆𝐴𝑃 cải thiện hiệu suất một cách có ý nghĩa thống kê trong cả thiết lập cross-project và cùng dự án.

4.4 A𝑆𝐴𝑃 có Bất Khả Tri Mô Hình không?
Kết quả của chúng tôi cho đến nay liên quan đến các mô hình code-davinci-002. Chúng tôi cũng đã cung cấp các prompt tăng cường A𝑆𝐴𝑃 cho hai mô hình khác, text-davinci-003 & gpt-3.5-turbo (mô hình chat). Các phát hiện của chúng tôi trong Bảng 6. Cách tiếp cận học few-shot tăng cường prompt của chúng tôi đã cải thiện

--- TRANG 8 ---
ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr

[THIS IS TABLE: Bảng 5 - Hiệu suất của prompt tăng cường A𝑆𝐴𝑃 với mô hình code-davinci-002 trên nhiệm vụ hoàn thành dòng]
Ngôn ngữ | Số mẫu | Exact Match (EM) | Edit Similarity (ES)
         |        | Zero-shot | A𝑆𝐴𝑃 | Lợi ích (%) | p-value | Zero-shot | A𝑆𝐴𝑃 | Lợi ích (%) | p-value
Java     | 9292   | 20.75     | 22.12 | +6.6%      | <0.01   | 55.35     | 59.66 | +7.79%     | <0.01
Python   | 6550   | 14.05     | 14.58 | +3.77%     | 0.13    | 49.71     | 50.12 | +0.82%     | <0.01
Tổng thể | 15842  | 17.97     | 19.01 | +5.79%     | <0.01   | 53.01     | 55.72 | +5.11%     | <0.01

[THIS IS TABLE: Bảng 6 - Hiệu suất trên tóm tắt mã nguồn, đo bằng BLEU]
Ngôn ngữ | Mô hình           | BM25  | A𝑆𝐴𝑃  | Lợi ích | p-value
Java     | Code-davinci-002  | 23.90 | 25.78 | +7.87%  | <0.01
         | Text-davinci-003  | 18.98 | 22.31 | +17.54% | <0.01
         | Turbo-GPT-3.5    | 16.68 | 16.96 | +1.68%  | 0.95
Python   | Code-davinci-002  | 22.00 | 24.78 | +12.64% | <0.01
         | Text-davinci-003  | 16.74 | 18.93 | +13.08% | <0.01
         | Turbo-GPT-3.5    | 15.01 | 16.38 | +9.13%  | <0.01
PHP      | Code-davinci-002  | 28.42 | 33.52 | +17.95% | <0.01
         | Text-davinci-003  | 21.67 | 25.72 | +18.69% | <0.01
         | Turbo-GPT-3.5    | 18.48 | 19.99 | +8.17%  | <0.01

hiệu suất của mô hình gpt-3.5-turbo từ 1.68% đến 9.13% và mô hình text-davinci-003 từ 13.08% đến 18.69% trên 500 mẫu mỗi từ Java, Python, PHP.

Gpt-3.5-turbo hoạt động tệ hơn so với các mô hình code-davinci-002 và text-davinci-003 trong tóm tắt mã nguồn. Phiên bản Turbo rườm rà và tạo ra các nhận xét khác về mặt văn phong so với những nhận xét được viết bởi các lập trình viên, và cũng khác với các mẫu few-shot trong prompt. Kỹ thuật prompt-engineering cẩn thận có thể cải thiện mô hình turbo và cho phép nó tạo ra các nhận xét tự nhiên, ngắn gọn hơn; điều này được để lại cho công việc tương lai. Hiệu suất kém này bởi mô hình chat phù hợp với các phát hiện của Kocoń et al. [39]. Mô hình Text-davinci-003 cho thấy sự gia tăng hiệu suất tối đa (mặc dù vẫn bị vượt qua bởi code-davinci-002). Lưu ý rằng text-davinci-003 là một mô hình completion, giống như code-davinci-002. Các phát hiện của chúng tôi cho thấy rằng A𝑆𝐴𝑃 hiệu quả hơn với các mô hình completion hơn là các mô hình chat. Chúng tôi cũng đã thực hiện các kiểm định cặp đôi một phía Wilcoxon signed rank, và ý nghĩa thống kê của các phát hiện của chúng tôi (trừ java với gpt-3.5-turbo) cho thấy rằng A𝑆𝐴𝑃 sẽ áp dụng vượt ra ngoài chỉ mô hình code-davinci-002 gốc.

4.5 A𝑆𝐴𝑃 cho Completion
Trọng tâm chính của chúng tôi cho đến nay là tóm tắt mã nguồn, trong thiết lập few-shot. Ở đây, chúng tôi khám phá xem A𝑆𝐴𝑃 có hoạt động trên một nhiệm vụ khác không: hoàn thành mã nguồn, trong thiết lập zero-shot nơi không có ví dụ nào được hiển thị hoặc trình bày cho mô hình. Chúng tôi đã đánh giá giá trị của việc bao gồm các sự kiện ngữ nghĩa cho nhiệm vụ hoàn thành dòng, nơi mô hình tạo ra dòng tiếp theo dựa trên dòng trước đó. Chúng tôi đã thu thập một cách đồng nhất và ngẫu nhiên 9292 mẫu Java và 6550 mẫu Python từ bộ dữ liệu CodeSearchNet để thực hiện đánh giá của chúng tôi. Chúng tôi đã chọn ngẫu nhiên một dòng cho mỗi mẫu và giao nhiệm vụ cho mô hình tạo ra dòng đó, chỉ được cung cấp tất cả các dòng đứng trước. Khi áp dụng A𝑆𝐴𝑃, chúng tôi nối thêm thông tin repository và các sự kiện ngữ nghĩa khác (tức là, tagged identifiers, DFG) trước các dòng đứng trước. Quan trọng là, khi tạo ra tagged identifiers và DFG, chúng tôi chỉ sử dụng thông tin một phần từ các dòng đứng trước để tránh rò rỉ thông tin từ các dòng sau đến các dòng mục tiêu.

Chúng tôi đã sử dụng hai chỉ số, Exact Match (EM) và Edit Similarity (ES), phù hợp với benchmark CodeXGLUE, để đo hiệu suất của mô hình. Chúng tôi đã thực hiện kiểm định McNemar cho EM và kiểm định cặp đôi Wilcoxon sign-rank để đánh giá hiệu suất của mô hình, tương tự như những gì chúng tôi đã thực hiện cho tóm tắt mã nguồn. Bảng 5 tóm tắt các phát hiện của chúng tôi. Chúng tôi quan sát thấy lợi ích tổng thể 5.79% trong Exact Match (EM) và lợi ích 5.11% trong Edit Similarity (ES), nhấn mạnh tính hiệu quả của việc kết hợp các sự kiện ngữ nghĩa. Đối với Python, chúng tôi tìm thấy ý nghĩa thống kê chỉ cho cải thiện ES, không cho EM.

4.6 Hiệu Suất trên Các Chỉ Số Khác
Ngoài BLEU-CN, chúng tôi đã đo hiệu suất với 3 chỉ số khác; BLEU-DC, ROUGE-L và METEOR. Kết quả của chúng tôi, trong Bảng 10, cho thấy lợi ích trung bình với A𝑆𝐴𝑃 trên cả ba chỉ số. Chúng tôi đã thực hiện các kiểm định cặp đôi một phía Wilcoxon signed-rank và tìm thấy cải thiện hiệu suất đáng kể với BLEU-DC và ROUGE-L cho tất cả các ngôn ngữ. Tuy nhiên, chúng tôi không quan sát thấy sự khác biệt đáng kể với METEOR cho 4 trong 6 ngôn ngữ, mặc dù trung bình mẫu thực sự cải thiện với A𝑆𝐴𝑃 trong tất cả 6 so sánh. Đáng chú ý là chúng tôi chỉ có 1000 mẫu ngôn ngữ (do chi phí) cho mỗi ngôn ngữ, vì vậy không bất ngờ khi thấy một số trường hợp mà chúng tôi không quan sát thấy ý nghĩa. Để đánh giá tác động tổng thể của A𝑆𝐴𝑃, chúng tôi đã kết hợp bộ dữ liệu từ tất cả các ngôn ngữ cho mô hình code-davinci-002 (6000 mẫu) và thực hiện cùng một kiểm định; sau đó chúng tôi có được ý nghĩa thống kê (p-value < 0.01) cho cả ba chỉ số, cho thấy rằng A𝑆𝐴𝑃 thực sự cung cấp giá trị.

5 THẢO LUẬN VÀ NGHIÊN CỨU ABLATION
Bây giờ chúng tôi trình bày một nghiên cứu ablation về thiết kế của A𝑆𝐴𝑃 và các sự kiện ngữ nghĩa cụ thể mà việc khởi tạo A𝑆𝐴𝑃 của chúng tôi sử dụng trước khi so sánh đầu ra của A𝑆𝐴𝑃 với baseline BM25 thuần túy của chúng tôi. Mục tiêu chính của một nghiên cứu ablation là đánh giá đóng góp của mỗi khía cạnh của một mô hình cho hiệu suất cuối cùng được quan sát. Trong nghiên cứu của chúng tôi, chúng tôi đã loại bỏ mỗi thành phần ngữ nghĩa của prompt tăng cường và quan sát hiệu suất. Chúng tôi thấy rằng thành phần Repo. đóng góp nhiều nhất cho hiệu suất của mô hình (Bảng 7) cho cả Java và Python. Tuy nhiên, tagged identifier và DFG cũng hữu ích, và kết quả tốt nhất được thu được khi chúng tôi kết hợp cả ba thành phần trong prompt.

[THIS IS TABLE: Bảng 7 - Nghiên cứu ablation]
Ngôn ngữ | Thành phần Prompt | BLEU-4
Java     | ALL              | 25.41
         | -Repo.           | 23.50
         | -Id              | 25.27
         | -DFG             | 24.86
Python   | ALL              | 24.26
         | -Repo.           | 22.80
         | -Id              | 23.93
         | -DFG             | 23.31

Hai Ví Dụ Minh Họa Khi kiểm tra thủ công các kết quả, chúng tôi quan sát thấy rằng trong một số mẫu, prompt A𝑆𝐴𝑃 chứa thông tin quan trọng cho tóm tắt. Bảng 8 cho thấy hai ví dụ kết quả minh họa điểm này. Trong ví dụ đầu tiên, mô hình baseline không tạo ra thuật ngữ "element-wise". Tuy nhiên, phiên bản tăng cường prompt của chúng tôi đã nắm bắt được khái niệm quan trọng này, mang lại điểm BLEU-4 cao hơn là 74.0 so với điểm baseline là 39.0. Tương tự, trong ví dụ thứ hai, mô hình baseline không nhận ra hàm như một quy trình độc lập, dẫn đến điểm BLEU thấp là 10.0. Tuy nhiên, cách tiếp cận được đề xuất của chúng tôi đã xác định hàm như một quy trình độc lập, dẫn đến điểm BLEU cao hơn là 33.0.

--- TRANG 9 ---
Tăng Cường Ngữ Nghĩa Tự Động của Prompt Mô Hình Ngôn Ngữ
(cho Tóm Tắt Mã Nguồn) ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha

[THIS IS TABLE: Bảng 8 - Các ví dụ được chọn, minh họa tính hiệu quả của tăng cường A𝑆𝐴𝑃]
Ví dụ 1:
```
def round(input_a, name: nil)
  check_allowed_types(input_a, TensorStream::Ops::FLOATING_POINT_TYPES)
  _op(:round, input_a, name: name)
end
```

Nhận xét Gold & đầu ra mô hình | BLEU
Gold: Rounds the values of a tensor to the nearest integer element-wise | NA
BM25: Round a tensor to the nearest integer | 39
A𝑆𝐴𝑃: Rounds the values of a tensor to the nearest integer, element-wise. | 74

Ví dụ 2:
```
public static void main(final String[] args) {
    loadPropertiesFiles(args);
    final ShutdownSignalBarrier barrier = new ShutdownSignalBarrier();
    final MediaDriver.Context ctx = new MediaDriver.Context();
    ctx.terminationHook(barrier::signal);
    try (MediaDriver ignore = MediaDriver.launch(ctx)) {
        barrier.await();
        System.out.println("Shutdown Driver...");
    }
}
```

Nhận xét Gold & đầu ra mô hình | BLEU
Gold: Start Media Driver as a stand-alone process. | NA
BM25: Main method that starts the CLR Bridge from Java. | 10
A𝑆𝐴𝑃: Main method for running Media Driver as a standalone process. | 33

Liệu Mô Hình Có Ghi Nhớ Đường Dẫn Không? Trong ba sự kiện ngữ nghĩa mà A𝑆𝐴𝑃 thêm vào prompt, thông tin repo. tác động đến hiệu suất của mô hình nhiều nhất. Điều này có thể do thực tế là Code-Davinci-002 đã ghi nhớ các đường dẫn tệp cụ thể trong dữ liệu của chúng tôi trong quá trình pre-training; khi chúng tôi cung cấp đường dẫn đến hàm, có thể mô hình chỉ nhớ lại thông tin đã ghi nhớ? Để điều tra câu hỏi này, chúng tôi thay đổi biểu diễn đường dẫn: chúng tôi lấy tên repository và đường dẫn, chia các token tại "/", và trình bày cho mô hình một danh sách các token. Ý tưởng chính đằng sau cách tiếp cận này là làm tan biến biểu diễn gốc, và trình bày cho mô hình một cái gì đó không gặp phải trong quá trình pre-training. Nếu mô hình không ghi nhớ một cách nghĩa đen, hiệu suất của nó không nên bị tác động. Chúng tôi quan sát thấy rằng sự khác biệt giữa hai phiên bản rất nhỏ. Đối với Java, chúng tôi tăng được 0.24 BLEU nhưng, đối với Python, chúng tôi mất 0.04 với đường dẫn được token hóa. Điều này cho thấy rủi ro thấp hơn rằng mô hình đã ghi nhớ đường dẫn đến hàm.

Thẻ Định Danh Có Cần Thiết Không? Trong bài báo này, chúng tôi gán vai trò cho các định danh và gắn thẻ chúng như Function Name, Parameters, Identifier, v.v. trong prompt (Xem Hình 2). Việc gắn thẻ rõ ràng này có thực sự giúp hiệu suất không? Để điều tra câu hỏi này, chúng tôi so sánh hiệu suất của mô hình khi được cung cấp một danh sách các định danh thuần túy, "không có thẻ". Chúng tôi quan sát thấy rằng các định danh được gắn thẻ dẫn đến hiệu suất tốt hơn cho cả Java và Python so với một danh sách đơn giản các định danh không có thẻ. Chỉ số hiệu suất BLEU của chúng tôi tăng 0.41 và 1.22 cho Java và Python, tương ứng, cho thấy rằng thông tin ngữ nghĩa rõ ràng thực sự đóng góp vào hiệu suất mô hình tốt hơn.

[THIS IS TABLE: Bảng 9 - So sánh với BM25 Vanilla với shots cao hơn]
Ngôn ngữ | Prompt Enhanced | Vanilla BM25
        | # shots | BLEU-4 | # shots | BLEU-4
Java    | 3       | 25.41  | 3       | 22.87
        |         |        | 4       | 23.13
        |         |        | 5       | 23.20
Python  | 3       | 24.26  | 3       | 21.78
        |         |        | 4       | 21.89
        |         |        | 5       | 21.74

Cái Gì Tốt Hơn: Nhiều Shots Hơn hay ASAP? Mặc dù có hàng tỷ tham số, LLM có kích thước prompt hạn chế. Ví dụ, code-davinci-002 và gpt-3.5-turbo hỗ trợ cho phép độ dài prompt chỉ 4k token. Tăng cường A𝑆𝐴𝑃 thực sự tiêu thụ một phần ngân sách độ dài prompt có sẵn! Do đó chúng tôi có hai lựa chọn thiết kế: 1) sử dụng ít hơn, các mẫu Tăng Cường A𝑆𝐴𝑃 trong prompt hoặc 2) sử dụng nhiều mẫu few-shot hơn mà không có tăng cường. Để điều tra điều này, chúng tôi cũng đã thử sử dụng 4 và 5 shot (thay vì 3) cho Java và Python với mô hình code-davinci-002. Tuy nhiên, Bảng 9 cho thấy rằng shots cao hơn sử dụng BM25 không nhất thiết dẫn đến hiệu suất tốt hơn. Với shots cao hơn, có khả năng giới thiệu các mẫu không liên quan, có thể làm tổn hại mô hình thay vì giúp đỡ nó.

Chỉ đối với Java chúng tôi quan sát thấy hiệu suất tốt hơn với cả 4 và 5 shot so với mô hình baseline của chúng tôi. Tuy nhiên, kỹ thuật được đề xuất của chúng tôi với chỉ 3-shot vẫn vượt trội hơn việc sử dụng BM25 với 5 shot. Đáng chú ý là cửa sổ ngữ cảnh của mô hình đang tăng lên từng ngày, và mô hình GPT-4 sắp tới sẽ cho phép chúng tôi có tối đa 32K token⁶. Do đó, giới hạn độ dài có thể không phải là một vấn đề trong tương lai gần. Tuy nhiên, nghiên cứu của chúng tôi cho thấy rằng Tăng Cường Ngữ Nghĩa Tự Động sẽ vẫn là một cách có lợi để sử dụng ngân sách độ dài prompt có sẵn; hơn nữa, có lý do để tin rằng việc xây dựng các prompt giàu tín hiệu, thông tin hơn sẽ có lợi bất kể độ dài.

Cái Gì Mới trong Đầu Ra của A𝑆𝐴𝑃? Chúng tôi thêm một phân tích pro forma của một vài ví dụ được chọn bằng tay, để phù hợp với các nghi thức cộng đồng yêu cầu peer-review; tuy nhiên, những phân tích này rất mang tính giai thoại và phải được giải thích thận trọng. Chúng tôi kiểm tra thủ công một số mẫu để thảo luận kết quả của chúng tôi chi tiết hơn; cụ thể, để trả lời ba câu hỏi: để chỉ rõ 1) các loại thông tin mới mà A𝑆𝐴𝑃 trình bày cho LLM và 2) làm thế nào tóm tắt của A𝑆𝐴𝑃 khác với những tóm tắt được tạo ra bởi các kỹ thuật hiện có, và 3) để phân tích các lỗi mà A𝑆𝐴𝑃 giới thiệu. Bảng 11 trình bày một số mẫu nơi, cho ba mẫu đầu tiên, A𝑆𝐴𝑃 hoạt động rất tốt so với baseline dựa trên truy xuất của chúng tôi, và cho ba mẫu thứ hai, baseline hoạt động tốt hơn A𝑆𝐴𝑃. Trong khi chúng tôi thảo luận các phát hiện của mình trong bối cảnh của các mẫu được cung cấp, các quan sát của chúng tôi khái quát hóa cho các mẫu khác.

Các loại thông tin mới mà A𝑆𝐴𝑃 trình bày cho LLM: Như đã thảo luận trong bài báo, đóng góp chính của chúng tôi liên quan đến việc tăng cường các mẫu được truy xuất (được truy xuất sử dụng BM25, theo Nashid et al. [48]) bằng các sự kiện ngữ nghĩa, dẫn đến hiệu suất được cải thiện so với cách tiếp cận truy xuất cơ bản. Chúng tôi thêm các sự kiện ngữ nghĩa liên quan đến chi tiết repository, định danh, và đồ thị luồng dữ liệu cho cả các mẫu được truy xuất và mã nguồn đầu vào. Như dự kiến, các sự kiện ngữ nghĩa được thêm vào chuyển vào, và tăng cường, đầu ra mô hình.

Trong mẫu đầu tiên, phương pháp baseline chỉ truy xuất không nắm bắt được thuật ngữ "gradient" hoàn toàn. Tuy nhiên, bằng cách kết hợp

⁶https://platform.openai.com/docs/models/gpt-4

--- TRANG 10 ---
ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr

[THIS IS TABLE: Bảng 10 - Tính hiệu quả của ASAP trong các chỉ số tóm tắt mã nguồn phổ biến]
Ngôn ngữ | BLEU-DC | ROUGE-L | METEOR
        | BM25 | ASAP | Lợi ích (%) | p-value | BM25 | ASAP | Lợi ích (%) | p-value | BM25 | ASAP | Lợi ích (%) | p-value
Java    | 14.09| 15.94| +13.13%    | <0.01   | 36.85| 38.41| +4.23%     | <0.01   | 35.66| 36.10| +1.23%     | 0.32
Python  | 12.63| 14.49| +14.73%    | <0.01   | 35.32| 37.74| +6.85%     | <0.01   | 33.05| 35.63| +7.81%     | <0.01
Ruby    | 9.16 | 11.01| +20.2%     | <0.01   | 28.19| 30.55| +8.37%     | <0.01   | 27.65| 29.20| +5.61%     | 0.03
JavaScript| 14.89| 16.71| +12.22%    | <0.01   | 32.28| 33.88| +4.96%     | <0.01   | 32.08| 33.02| +2.93%     | 0.15
Go      | 17.10| 18.57| +8.60%     | <0.01   | 41.04| 42.43| +3.39%     | <0.01   | 36.78| 37.26| +1.31%     | 0.27
PHP     | 16.97| 20.63| +21.57%    | <0.01   | 40.48| 44.90| +10.92%    | <0.01   | 40.14| 43.35| +8.00%     | <0.01
Tổng thể| 14.14| 16.23| +14.78%    | <0.01   | 35.69| 37.99| +6.44%     | <0.01   | 34.23| 35.76| +4.47%     | <0.01

[THIS IS TABLE: Bảng 11 - Ví dụ Cho thấy Điểm Mạnh và Điểm Yếu của A𝑆𝐴𝑃]
Thay đổi (BLEU-4) | Tham chiếu | BM25 | A𝑆𝐴𝑃
A𝑆𝐴𝑃 (+47) | Generates a gradient-noise value from the coordinates of a three-dimensional input value and the integer coordinates of a nearby three-dimensional value. | Computes a value at the point (x, y, z) in a 3D Perlin noise function. | Generates a gradient-coherent-noise value from the coordinates of a three-dimensional input value.
A𝑆𝐴𝑃 (+26) | Replaces type with mapped type for current path. | Returns the mapped type for the current path, or the target type if no mapping exists. | Replaces target type with mapped type if any.
A𝑆𝐴𝑃 (+41) | convert a dataRootExt to a dataRoot | Convert to a DataRoot object. | Convert a DataRootExt to a DataRoot.
A𝑆𝐴𝑃 (-36) | Computes the convex hull of the set of points. | Computes the convex hull of the set of points | Computes the convex hull of the set of points using Andrew's monotone chain algorithm
A𝑆𝐴𝑃 (-35) | Getter for the Uninstall Actions. | Returns the uninstall actions. | Retrieves the uninstall actions for this deployable unit.
A𝑆𝐴𝑃 (-67) | Get a column of this matrix. | Get a column of this matrix. | Return the specified column of this matrix as a column vector.

các sự kiện ngữ nghĩa, mô hình thành công khôi phục thuật ngữ vì nó thường được tìm thấy trong cả định danh và tên repository, ảnh hưởng đến đầu ra của mô hình. Trong ví dụ thứ hai, nơi mục tiêu là thay thế thay vì chỉ trả về, baseline không tạo ra thuật ngữ "replace", mặc dù có chỉ dẫn rõ ràng trong tên hàm ("replaceWithMappedTypeForPath"). Luồng dữ liệu giữa các định danh, được cung cấp trong các sự kiện ngữ nghĩa, có thể đã giúp mô hình nhận ra các thao tác thay thế.

Cách tóm tắt của A𝑆𝐴𝑃 khác với những tóm tắt được tạo ra bởi các kỹ thuật hiện có: Sau cuộc thảo luận trên, chúng tôi quan sát thấy rằng A𝑆𝐴𝑃 đang tạo ra thông tin cụ thể hơn:
(1) Nó xác định "gradient" trong mẫu 1.
(2) Nó đề xuất thay đổi "return" thành "replace" trong một mẫu khác (mẫu 2).
(3) Nó khuyến nghị thay đổi "dataroot" thành "datarootext" trong một mẫu khác (mẫu 3).

Những khác biệt này đã được quan sát trên nhiều mẫu khi so sánh baseline của chúng tôi với A𝑆𝐴𝑃. Cách tiếp cận A𝑆𝐴𝑃 nhất quán tạo ra thông tin cụ thể hơn so với baseline.

Phân tích các lỗi mà A𝑆𝐴𝑃 giới thiệu: Các ví dụ được kiểm tra cho thấy rằng A𝑆𝐴𝑃 có thể trở nên quá cụ thể, và do đó không khớp với tóm tắt được viết bởi lập trình viên. A𝑆𝐴𝑃 trở nên quá cụ thể trong ba ví dụ cuối cùng với "Andrew's monotone chain algorithm" và "deployable unit", "column vector". Trong khi những thuật ngữ này không nhất thiết sai, BLEU-4 giảm, vì tóm tắt được viết bởi lập trình viên tổng quát hơn.

Chúng tôi cũng quan sát một cách định lượng rằng A𝑆𝐴𝑃 gây ra thay đổi tích cực trong 44% mẫu. Tuy nhiên, hiệu suất cũng giảm cho 30% mẫu, và giữ nguyên trên phần còn lại. So với baseline của chúng tôi (học few-shot với các mẫu được truy xuất BM25), A𝑆𝐴𝑃 đòi hỏi nhiều token hơn. Chi phí token bổ sung, mỗi truy vấn (cả về chi phí tiền tệ và overhead hiệu suất) khá khiêm tốn. Mặt khác, chúng tôi quan sát thấy một cải thiện tổng thể đáng kể 12% với A𝑆𝐴𝑃 sử dụng mô hình Codex.

6 CÔNG VIỆC LIÊN QUAN

6.1 Tóm Tắt Mã Nguồn
Các mô hình deep learning đã thúc đẩy state-of-the-art trong các nhiệm vụ SE như tóm tắt mã nguồn. Mô hình LSTM cho tóm tắt mã nguồn lần đầu tiên được giới thiệu bởi Iyer et al. [33]. Các mô hình dựa trên transformer được huấn luyện trước như CodeBERT [21], PLBART [2], và CodeT5 [64] đã được sử dụng rộng rãi trên bộ dữ liệu tóm tắt mã nguồn CodeXGLUE [47], dẫn đến những cải thiện đáng kể. Tuy nhiên, có một lưu ý khi sử dụng các mô hình ngôn ngữ được huấn luyện trước: mặc dù những mô hình này hoạt động tốt, việc fine-tuning rộng rãi là cần thiết, có thể đòi hỏi nhiều dữ liệu & tốn thời gian. Ngoài ra, các mô hình riêng biệt phải được huấn luyện cho các ngôn ngữ khác nhau, tăng chi phí huấn luyện. Để giảm số lượng mô hình cần thiết, fine-tuning đa ngôn ngữ đã được đề xuất, để duy trì hoặc cải thiện hiệu suất trong khi giảm số lượng mô hình xuống một [4]. Tuy nhiên, cách tiếp cận này không giảm thời gian huấn luyện hoặc nhu cầu về dữ liệu có nhãn.

LLM, hoặc các mô hình ngôn ngữ lớn, lớn hơn nhiều so với những mô hình được huấn luyện trước này, và được huấn luyện trên các bộ dữ liệu lớn hơn nhiều với một mục tiêu huấn luyện đơn giản — dự đoán token tiếp theo tự hồi quy [12]. Những mô hình này hoạt động tốt một cách đáng ngạc nhiên trong các nhiệm vụ, thậm chí mà không cần fine-tuning. Chỉ cần nhắc nhở mô hình với các câu hỏi khác nhau, trong khi cung cấp một vài mẫu vấn đề-giải pháp, là đủ. Học few-shot đã được áp dụng cho tóm tắt mã nguồn, và đã được tìm thấy là có lợi [3].

--- TRANG 11 ---
Tăng Cường Ngữ Nghĩa Tự Động của Prompt Mô Hình Ngôn Ngữ
(cho Tóm Tắt Mã Nguồn) ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha

6.2 Các Bộ Dữ Liệu Khác
Có một số bộ dữ liệu có sẵn cho tóm tắt mã nguồn, ngoài CodeXGLUE [47]. TL-CodeSum [30] là một bộ dữ liệu tương đối nhỏ hơn, với khoảng 87K mẫu, nhưng nó bao gồm các bản trùng lặp, có thể dẫn đến ước lượng hiệu suất cao có thể không khái quát hóa. Funcom [41] là một bộ dữ liệu chuyên dụng với 2.1 triệu hàm Java, nhưng chứa các bản trùng lặp. Chúng tôi chọn CodeXGLUE (rút ra từ CodeSearchNet) vì nó là một bộ dữ liệu đa dạng, đa ngôn ngữ, đặt ra thách thức cho các mô hình. Thậm chí các mô hình được huấn luyện tốt như CodeBERT cũng gặp khó khăn trên benchmark này; hiệu suất của nó đặc biệt kém trên các ngôn ngữ có ít mẫu huấn luyện.

Đã có rất nhiều công việc về tóm tắt mã nguồn, từ khớp template đến học few-shot. Những mô hình này sử dụng các biểu diễn và nguồn thông tin khác nhau để hoạt động tốt trong tóm tắt mã nguồn. Việc so sánh hoặc thảo luận tất cả những mô hình này vượt quá phạm vi của công việc này. Tuy nhiên, chúng tôi lưu ý rằng các con số của chúng tôi đại diện cho một điểm cao mới trên benchmark CodeXGlue được sử dụng rộng rãi cho tóm tắt mã nguồn và hoàn thành mã nguồn; chúng tôi giới thiệu người đọc đến https://microsoft.github.io/CodeXGLUE/ để nhanh chóng xem bảng xếp hạng. Các mẫu của chúng tôi nhỏ hơn (N=1000), nhưng các ước lượng, và ước lượng cải thiện, mạnh mẽ về mặt thống kê (Xem thảo luận về cỡ mẫu trong Mục 7).

6.3 LLM trong Kỹ Thuật Phần Mềm
Mặc dù LLM chưa được sử dụng rộng rãi cho tóm tắt mã nguồn, chúng được sử dụng rộng rãi cho tạo mã nguồn [14, 49, 67] và sửa chữa chương trình [5,18,35,36]. Các mô hình như Codex nhằm giảm gánh nặng cho các lập trình viên bằng cách tự động tạo mã nguồn hoặc hoàn thành dòng. Một số mô hình như Polycoder [67] và Codegen [49] hoạt động khá tốt, và do học few-shot hoặc prompting của chúng, chúng có thể được áp dụng cho một tập hợp rộng các vấn đề. Tuy nhiên, mô hình Code-davinci-002 thường hoạt động tốt hơn những mô hình đó và cho phép chúng tôi fit các prompt tăng cường của chúng tôi vào một cửa sổ lớn hơn.

Jain et al. đã đề xuất bổ sung hoạt động LLM với các bước xử lý tiếp theo dựa trên các kỹ thuật phân tích và tổng hợp chương trình để cải thiện hiệu suất trong tạo đoạn mã chương trình [34]. Bareiß et al. cho thấy tính hiệu quả của học few-shot trong mutation mã nguồn, tạo test oracle từ tài liệu ngôn ngữ tự nhiên, và các nhiệm vụ tạo test case [10]. CODAMOSA [42], một cách tiếp cận dựa trên LLM, thực hiện kiểm thử phần mềm dựa trên tìm kiếm cho đến khi các cải thiện coverage của nó đình trệ, sau đó yêu cầu LLM cung cấp các test case mẫu cho các hàm không được bao phủ. Bằng cách sử dụng những ví dụ này, CODAMOSA giúp chuyển hướng kiểm thử phần mềm dựa trên tìm kiếm đến các khu vực hữu ích hơn của không gian tìm kiếm. Jiang et al. đã đánh giá tính hiệu quả của LLM cho vấn đề sửa chữa chương trình [35].

Truy xuất và nối thêm một tập hợp các mẫu huấn luyện đã được tìm thấy là có lợi cho nhiều nhiệm vụ phân tích ngữ nghĩa trong NLP, thậm chí mà không sử dụng LLM [68]. Một hạn chế của cách tiếp cận này là hiệu suất có thể bị ràng buộc bởi tính khả dụng của các ví dụ tương tự. Nashid et al. đã sử dụng một cách tiếp cận tương tự và đạt được hiệu suất cải thiện trong sửa chữa mã nguồn và tạo assertion với sự giúp đỡ của LLM [48]. Tuy nhiên, không có công việc nào ở trên đã cố gắng tăng cường ngữ nghĩa tự động prompt. Lưu ý rằng vẫn còn quá sớm để nhận xét về khả năng đầy đủ của những mô hình ngôn ngữ lớn này. Các phát hiện của chúng tôi cho đến nay cho thấy rằng việc tăng cường các mẫu trong prompt bằng gợi ý ngữ nghĩa giúp ích trong các nhiệm vụ tóm tắt mã nguồn và hoàn thành mã nguồn; việc đánh giá giá trị của A𝑆𝐴𝑃 trong các nhiệm vụ khác được để lại cho công việc tương lai.

7 HIỂM HỌA & GIỚI HẠN
Một mối quan tâm lớn khi làm việc với các mô hình ngôn ngữ lớn là khả năng phơi bày dữ liệu kiểm thử trong quá trình huấn luyện. Tiếc là, người ta không thể kiểm tra trực tiếp điều này vì bộ dữ liệu huấn luyện không thể truy cập. Hiệu suất thấp hơn của mô hình với few-shotting ngẫu nhiên cho thấy rằng việc ghi nhớ có thể không phải là một yếu tố chính. Khi chúng tôi kết hợp thông tin liên quan, hiệu suất của mô hình cải thiện với lượng và chất lượng thông tin. Nếu mô hình đã ghi nhớ các tóm tắt, nó có thể đã ghi điểm cao hơn nhiều, thậm chí mà không có lợi ích của các mẫu liên quan và tăng cường ngữ nghĩa.

Phân Tích Cỡ Mẫu: Chúng tôi đã sử dụng trung bình và độ lệch chuẩn quan sát được để tính toán (sử dụng G*power [19,20]) các cỡ mẫu cần thiết, sử dụng các giá trị thường được sử dụng: 𝛼 của 0.01 (p-value mong muốn) và 𝛽 của 0.20 (tức là, 20% khả năng KHÔNG khám phá một hiệu ứng, nếu một hiệu ứng tồn tại). Đối với các kiểm định mà chúng tôi đã sử dụng (kiểm định Wilcoxon Signed-rank), chúng tôi thấy rằng cỡ mẫu cần thiết luôn luôn dưới cỡ mẫu mà chúng tôi đã sử dụng cho các nghiên cứu chính của mình, tức là 1000.

Nghiên Cứu Người Dùng: Chúng tôi đã không thực hiện nghiên cứu người dùng cho A𝑆𝐴𝑃. Do đó, những cải thiện trong các chỉ số được trình bày ở đây có thể không nhất thiết chuyển thành hiệu suất lập trình viên được cải thiện. Khía cạnh này được để lại cho công việc tương lai.

Cuối cùng: fine-tuning các LM lớn để sử dụng các sự kiện ngữ nghĩa rút ra có thể cải thiện so với cách tiếp cận prompting tăng cường của chúng tôi, nhưng sẽ tốn kém. Chúng tôi sẽ để xem xét nó cho nghiên cứu tương lai.

8 KẾT LUẬN
Trong bài báo này, chúng tôi đã khám phá ý tưởng về Tăng Cường Ngữ Nghĩa Tự Động của Prompt, theo đó chúng tôi đề xuất tăng cường các mẫu few-shot trong các prompt LLM, với các sự kiện được gắn thẻ tự động rút ra bởi phân tích ngữ nghĩa. Điều này dựa trên trực giác rằng các lập trình viên con người thường quét mã nguồn để ngầm trích xuất những sự kiện như vậy trong quá trình hiểu mã nguồn dẫn đến việc viết một tóm tắt tốt. Mặc dù có thể tin rằng LLM có thể ngầm suy ra những sự kiện như vậy cho chính chúng, chúng tôi suy đoán rằng việc thêm những sự kiện này theo một phong cách được định dạng vào các mẫu và mục tiêu, trong prompt, sẽ giúp LLM tổ chức "chuỗi suy nghĩ" của nó khi nó tìm cách xây dựng một tóm tắt. Chúng tôi đã đánh giá ý tưởng này trên bộ dữ liệu CodeSearchNet đầy thử thách, được khử trùng lặp, được tuyển chọn tốt, trên hai nhiệm vụ: tóm tắt mã nguồn và hoàn thành mã nguồn. Các phát hiện của chúng tôi chỉ ra rằng Tăng Cường Ngữ Nghĩa Tự Động của Prompt thường hữu ích. Các ước lượng của chúng tôi cho thấy nó giúp vượt qua state-of-the-art.

Lời Cảm Ơn: Chúng tôi muốn ghi nhận National Science Foundation dưới Grant NSF CCF (SHF-MEDIUM) Số 2107592 và Intelligence Advanced Research Projects Agency (IARPA) dưới hợp đồng W911NF20C0038 cho sự hỗ trợ một phần của công việc này. Các kết luận của chúng tôi không nhất thiết phản ánh vị trí hoặc chính sách của các nhà tài trợ của chúng tôi và không nên suy ra sự tán thành chính thức.

TÀI LIỆU THAM KHẢO
[1] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2020. A Transformer-based Approach for Source Code Summarization. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 4998–5007.

--- TRANG 12 ---
ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, và Earl T. Barr

[2] Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Uni-fied Pre-training for Program Understanding and Generation. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu-tational Linguistics: Human Language Technologies. 2655–2668.

[3] Toufique Ahmed và Premkumar Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization. Trong 37th IEEE/ACM International Conference on Automated Software Engineering. 1–5.

[4] Toufique Ahmed và Premkumar Devanbu. 2022. Multilingual training for soft-ware engineering. Trong Proceedings of the 44th International Conference on Software Engineering. 1443–1455.

[5] Toufique Ahmed và Premkumar Devanbu. 2023. Better patching using LLM prompting, via Self-Consistency. Trong 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 1742–1746.

[6] Toufique Ahmed, Supriyo Ghosh, Chetan Bansal, Thomas Zimmermann, Xuchao Zhang, và Saravan Rajmohan. 2023. Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models. ICSE (2023).

[7] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. Trong Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. 143–153.

[8] Uri Alon, Shaked Brody, Omer Levy, và Eran Yahav. 2018. code2seq: Gen-erating sequences from structured representations of code. arXiv preprint arXiv:1808.01400 (2018).

[9] Satanjeev Banerjee và Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Trong Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65–72.

[10] Patrick Bareiß, Beatriz Souza, Marcelo d'Amorim, và Michael Pradel. 2022. Code generation tools (almost) for free? a study of few-shot, pre-trained language models on code. arXiv preprint arXiv:2206.01335 (2022).

[11] Lionel C Briand. 2003. Software documentation: how much is enough?. Trong Seventh European Conference on Software Maintenance and Reengineering, 2003. Proceedings. IEEE, 13–15.

[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.

[13] Boxing Chen và Colin Cherry. 2014. A systematic comparison of smoothing techniques for sentence-level BLEU. Trong Proceedings of the ninth workshop on statistical machine translation. 362–367.

[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).

[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).

[16] Samanta Dey, Venkatesh Vinayakarao, Monika Gupta, và Sampath Dechu. 2022. Evaluating commit message generation: to BLEU or not to BLEU?. Trong Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results. 31–35.

[17] Brian P Eddy, Jeffrey A Robinson, Nicholas A Kraft, và Jeffrey C Carver. 2013. Evaluating source code summarization techniques: Replication and expansion. Trong 2013 21st International Conference on Program Comprehension (ICPC). IEEE, 13–22.

[18] Zhiyu Fan, Xiang Gao, Abhik Roychoudhury, và Shin Hwei Tan. 2022. Auto-mated Repair of Programs from Large Language Models. ICSE.

[19] Franz Faul, Edgar Erdfelder, Axel Buchner, và Albert-Georg Lang. 2009. Sta-tistical power analyses using G* Power 3.1: Tests for correlation and regression analyses. Behavior research methods 41, 4 (2009), 1149–1160.

[20] Franz Faul, Edgar Erdfelder, Albert-Georg Lang, và Axel Buchner. 2007. G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior research methods 39, 2 (2007), 175–191.

[21] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 1536–1547.

[22] Andrew Forward và Timothy C Lethbridge. 2002. The relevance of software documentation, tools and technologies: a survey. Trong Proceedings of the 2002 ACM symposium on Document engineering. 26–33.

[23] Jianfeng Gao và Xiaodong He. 2013. Training MRF-based phrase translation models using gradient ascent. Trong Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-guage Technologies. 450–459.

[24] David Gros, Hariharan Sezhiyan, Prem Devanbu, và Zhou Yu. 2020. Code to Comment Translation: Data, Metrics, Baselining & Evaluation. Trong 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 746–757.

[25] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, LIU Shujie, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow. Trong International Conference on Learning Representations.

[26] Sonia Haiduc, Jairo Aponte, và Andrian Marcus. 2010. Supporting program comprehension with source code summarization. Trong Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 2. 223–226.

[27] Sonia Haiduc, Jairo Aponte, Laura Moreno, và Andrian Marcus. 2010. On the use of automated text summarization techniques for summarizing source code. Trong 2010 17th Working Conference on Reverse Engineering. IEEE, 35–44.

[28] Sakib Haque, Zachary Eberhart, Aakash Bansal, và Collin McMillan. 2022. Se-mantic similarity metrics for evaluating source code summarization. Trong Proceed-ings of the 30th IEEE/ACM International Conference on Program Comprehension. 36–47.

[29] Xing Hu, Ge Li, Xin Xia, David Lo, và Zhi Jin. 2018. Deep code comment generation. Trong Proceedings of the 26th conference on program comprehension. 200–210.

[30] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, và Zhi Jin. 2018. Summarizing source code with transferred API knowledge. Trong Proceedings of the 27th Interna-tional Joint Conference on Artificial Intelligence. 2269–2275.

[31] Jie Huang và Kevin Chen-Chuan Chang. 2022. Towards Reasoning in Large Language Models: A Survey. arXiv preprint arXiv:2212.10403 (2022).

[32] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 (2019).

[33] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, và Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. Trong Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2073–2083.

[34] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, và Rahul Sharma. 2022. Jigsaw: Large language models meet program synthesis. Trong Proceedings, 44th ICSE. 1219–1231.

[35] Nan Jiang, Kevin Liu, Thibaud Lutellier, và Lin Tan. 2023. Impact of Code Language Models on Automated Program Repair. ICSE (2023).

[36] Harshit Joshi, José Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, và Gust Verbruggen. 2022. Repair is nearly generation: Multilingual program repair with llms. arXiv preprint arXiv:2208.11640 (2022).

[37] Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins Ajanoh, và Mohamed Coulibali. 2020. NUBIA: NeUral Based Interchangeability Assessor for Text Generation. arXiv:2004.14667 [cs.CL]

[38] Sungmin Kang, Juyeon Yoon, và Shin Yoo. 2023. Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction. ICSE (2023).

[39] Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.

[40] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022).

[41] Alexander LeClair, Siyuan Jiang, và Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. Trong 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE). IEEE, 795–806.

[42] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, và Siddhartha Sen. 2023. CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models. Trong 45th International Conference on Software Engineering, ser. ICSE.

[43] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Trong Text summarization branches out. 74–81.

[44] Chin-Yew Lin và Franz Josef Och. 2004. Orange: a method for evaluating auto-matic evaluation metrics for machine translation. Trong COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics. 501–507.

[45] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).

[46] Cristina V Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny, Hitesh Sajnani, và Jan Vitek. 2017. DéjàVu: a map of code duplicates on GitHub. Proceedings of the ACM on Programming Languages 1, OOPSLA (2017), 1–28.

[47] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro-sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021).

[48] Noor Nashid, Mifta Sintaha, và Ali Mesbah. 2023. Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. Trong Proceedings, 45th ICSE.

[49] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2022. Codegen: An open large language

--- TRANG 13 ---
Tăng Cường Ngữ Nghĩa Tự Động của Prompt Mô Hình Ngôn Ngữ
(cho Tóm Tắt Mã Nguồn) ICSE '24, 14–20 tháng 4, 2024, Lisbon, Bồ Đào Nha

model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022).

[50] Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318.

[51] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, và Huajun Chen. 2022. Reasoning with Language Model Prompting: A Survey. arXiv preprint arXiv:2212.09597 (2022).

[52] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).

[53] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.

[54] Juan Ramos et al. 2003. Using tf-idf to determine word relevance in document queries. Trong Proceedings of the first instructional conference on machine learning, Vol. 242. Citeseer, 29–48.

[55] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333–389.

[56] Paige Rodeghero, Collin McMillan, Paul W McBurney, Nigel Bosch, và Sidney D'Mello. 2014. Improving automated source code summarization via an eye-tracking study of programmers. Trong Proceedings of the 36th international conference on Software engineering. 390–401.

[57] Devjeet Roy, Sarah Fakhoury, và Venera Arnaoudova. 2021. Reassessing auto-matic evaluation metrics for code summarization tasks. Trong Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 1105–1116.

[58] Thibault Sellam, Dipanjan Das, và Ankur P Parikh. 2020. BLEURT: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696 (2020).

[59] Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dong-mei Zhang, và Hongbin Sun. 2023. On the evaluation of neural code summariza-tion. Trong Proceedings of the 44th International Conference on Software Engineering. 1597–1608.

[60] Disha Shrivastava, Hugo Larochelle, và Daniel Tarlow. 2022. Repository-level prompt generation for large language models of code. arXiv preprint arXiv:2206.12839 (2022).

[61] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, và K Vijay-Shanker. 2010. Towards automatically generating summary comments for java methods. Trong Proceedings of the IEEE/ACM international conference on Automated software engineering. 43–52.

[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in neural information processing systems. 5998–6008.

[63] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, và Steven CH Hoi. 2023. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).

[64] Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 8696–8708.

[65] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, và Zhi Jin. 2019. Code generation as a dual task of code summarization. Advances in neural information processing systems 32 (2019).

[66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022).

[67] Frank F Xu, Uri Alon, Graham Neubig, và Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. Trong Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. 1–10.

[68] Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit Sanghai, và Fei Sha. 2022. Generate-and-Retrieve: use your predictions to improve retrieval for semantic parsing. arXiv preprint arXiv:2209.14899 (2022).

[69] Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, và Xudong Liu. 2020. Retrieval-based neural source code summarization. Trong Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. 1385–1397.

[70] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).
