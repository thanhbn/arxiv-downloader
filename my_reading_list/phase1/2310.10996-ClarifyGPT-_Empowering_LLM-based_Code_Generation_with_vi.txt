# ClarifyGPT: Tăng cường khả năng sinh mã dựa trên LLM với việc làm rõ ý định

FANGWEN MU∗, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc
LIN SHI∗, Đại học Beihang, Trung Quốc
SONG WANG, Đại học York, Canada
ZHUOHAO YU, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc
BINQUAN ZHANG, Đại học Beihang, Trung Quốc
CHENXUE WANG, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc
SHICHAO LIU, Phòng thí nghiệm Đổi mới IDE Phần mềm, Viện Phần mềm Trung tâm Huawei, Trung Quốc
QING WANG, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc

Các Mô hình Ngôn ngữ Lớn (LLM), như ChatGPT, đã chứng minh khả năng ấn tượng trong việc tự động sinh mã từ các yêu cầu ngôn ngữ tự nhiên được cung cấp. Tuy nhiên, trong thực tế, việc người dùng viết các yêu cầu có thể mơ hồ hoặc không đầy đủ là điều không thể tránh khỏi. Các LLM hiện tại sẽ trực tiếp sinh chương trình theo những yêu cầu không rõ ràng đó mà không cần làm rõ tương tác, điều này có thể làm chệch khỏi ý định ban đầu của người dùng. Để thu hẹp khoảng cách này, chúng tôi giới thiệu một framework mới có tên ClarifyGPT, nhằm nâng cao khả năng sinh mã bằng cách trao quyền cho LLM khả năng nhận diện yêu cầu mơ hồ và đặt các câu hỏi làm rõ có mục tiêu. Cụ thể, ClarifyGPT đầu tiên phát hiện xem một yêu cầu đã cho có mơ hồ hay không bằng cách thực hiện kiểm tra tính nhất quán của mã. Nếu nó mơ hồ, ClarifyGPT sẽ nhắc một LLM sinh ra các câu hỏi làm rõ có mục tiêu. Sau khi nhận được phản hồi của câu hỏi, ClarifyGPT sẽ tinh chỉnh yêu cầu mơ hồ và đưa nó vào cùng một LLM để sinh ra giải pháp mã cuối cùng. Để đánh giá ClarifyGPT của chúng tôi, trước tiên chúng tôi tiến hành đánh giá con người có sự tham gia của mười người tham gia sử dụng ClarifyGPT cho việc sinh mã trên hai bộ chuẩn có sẵn công khai: MBPP-sanitized và MBPP-ET. Kết quả cho thấy ClarifyGPT nâng cao hiệu suất (Pass@1) của GPT-4 từ 70.96% lên 80.80% trên MBPP-sanitized. Hơn nữa, để thực hiện các đánh giá tự động quy mô lớn của ClarifyGPT trên các LLM và bộ chuẩn khác nhau mà không cần sự tham gia của người dùng, chúng tôi giới thiệu một phương pháp mô phỏng độ tin cậy cao để mô phỏng phản hồi của người dùng. Kết quả đánh giá tự động cũng chứng minh rằng ClarifyGPT có thể nâng cao đáng kể hiệu suất sinh mã so với các phương pháp cơ sở. Cụ thể, ClarifyGPT cải thiện hiệu suất trung bình của GPT-4 và ChatGPT trên bốn bộ chuẩn từ 68.02% lên 75.75% và từ 58.55% lên 67.22%, tương ứng. Chúng tôi tin rằng ClarifyGPT có thể tạo thuận lợi hiệu quả cho việc ứng dụng thực tế của LLM trong môi trường phát triển thế giới thực.

∗Cả hai tác giả đều đóng góp như nhau cho nghiên cứu này

Địa chỉ tác giả: Fangwen Mu, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, fangwen2020@iscas.ac.cn; Lin Shi, Đại học Beihang, Bắc Kinh, Trung Quốc, shilin@buaa.edu.cn; Song Wang, Đại học York, Toronto, Canada, wangsong@yorku.ca; Zhuohao Yu, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, yuzhuohao23@mails.ucas.edu.cn; Binquan Zhang, Đại học Beihang, Bắc Kinh, Trung Quốc, binquan@buaa.edu.cn; ChenXue Wang, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, chenxuew02@gmail.com; Shichao Liu, Phòng thí nghiệm Đổi mới IDE Phần mềm, Viện Phần mềm Trung tâm Huawei, Bắc Kinh, Trung Quốc, liushichao2@huawei.com; Qing Wang, Viện Khoa học Máy tính, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, wq@iscas.ac.cn.

Được phép tạo bản sao kỹ thuật số hoặc in cứng toàn bộ hoặc một phần của tác phẩm này để sử dụng cá nhân hoặc lớp học mà không tính phí với điều kiện các bản sao không được tạo ra hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải mang thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền của các thành phần trong tác phẩm này thuộc sở hữu của những người khác ngoài ACM phải được tôn trọng. Tóm tắt có ghi nguồn được phép. Để sao chép theo cách khác, hoặc xuất bản lại, để đăng trên máy chủ hoặc để phân phối lại cho các danh sách, yêu cầu sự cho phép cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.

©2023 Association for Computing Machinery.
XXXX-XXXX/2023/10-ART $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

, Vol. 1, No. 1, Article . Publication date: October 2023.

Định dạng tham chiếu ACM:
Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, ChenXue Wang, Shichao Liu, và Qing Wang. 2023. ClarifyGPT: Tăng cường khả năng sinh mã dựa trên LLM với việc làm rõ ý định. 1, 1 (October 2023), 21 trang. https://doi.org/10.1145/nnnnnnn.nnnnnnn

1 GIỚI THIỆU

Sinh mã nhằm mục đích tạo ra một đoạn mã thỏa mãn ý định của người dùng được thể hiện trong một yêu cầu ngôn ngữ tự nhiên. Nhiệm vụ này, vốn cung cấp tiềm năng tiết kiệm chi phí, đẩy nhanh các hoạt động lập trình và tạo thuận lợi cho việc phát triển phần mềm, do đó đã thu hút sự chú ý từ nhiều lĩnh vực khác nhau, ví dụ như xử lý ngôn ngữ tự nhiên, trí tuệ nhân tạo và kỹ thuật phần mềm. Các nỗ lực gần đây giải quyết nhiệm vụ này bằng cách tận dụng Các Mô hình Ngôn ngữ Lớn (LLM) với hàng tỷ tham số, như ChatGPT[34] và CodeGen[33]. Các LLM lấy các yêu cầu ngôn ngữ tự nhiên (tức là các prompt) làm đầu vào và xuất ra các đoạn mã tương ứng, đạt được tiến bộ đáng kể trong việc sinh mã.

Tuy nhiên, trong thực tế, do sự đa dạng về kinh nghiệm và góc nhìn của người dùng, việc các yêu cầu được người dùng viết có thể mơ hồ hoặc không đầy đủ là điều không thể tránh khỏi. Ví dụ, yêu cầu "Viết một hàm để sắp xếp một danh sách các phần tử" không chỉ định liệu người dùng có ý định cho danh sách được sắp xếp theo thứ tự tăng dần hay giảm dần. Các LLM hiện tại không xử lý những yêu cầu mơ hồ như vậy: chúng hiếm khi yêu cầu người dùng làm rõ những yêu cầu này mà thay vào đó trực tiếp sinh các chương trình có thể lệch khỏi nhu cầu của người dùng[21]. Các phương pháp sinh mã dựa trên LLM hiện tại thiếu cơ chế làm rõ các yêu cầu không rõ ràng[20,21], tức là chúng trực tiếp sinh chương trình theo những yêu cầu không rõ ràng đó mà không cần làm rõ tương tác. Ngược lại, khi các nhà phát triển con người gặp phải các yêu cầu mơ hồ, họ thường tìm kiếm thông tin bổ sung bằng cách tương tác đặt các câu hỏi làm rõ cho người dùng. Đối với ví dụ trên, một câu hỏi làm rõ đơn giản như "Việc sắp xếp nên theo thứ tự tăng dần hay giảm dần?" có thể giúp làm rõ yêu cầu.

Dựa trên quan sát này, chúng tôi cho rằng việc trao quyền cho LLM khả năng tự động đặt các câu hỏi làm rõ cho các yêu cầu mơ hồ là cần thiết để cải thiện chất lượng và hiệu quả của việc sinh mã. Tuy nhiên, việc trao quyền cho LLM khả năng này khá thách thức do các rào cản sau đây. (1) Khi nào thì đặt câu hỏi làm rõ? Trong môi trường phát triển thực tế, có nhiều yêu cầu tồn tại, bao gồm cả những yêu cầu mơ hồ và không mơ hồ. Việc không tập trung vào việc chỉ đặt câu hỏi đối với các yêu cầu mơ hồ có thể dẫn đến các tương tác không cần thiết giữa LLM và người dùng về các yêu cầu được định nghĩa rõ ràng. Những tương tác không cần thiết này, ngược lại, có thể làm giảm hiệu quả và ảnh hưởng đến trải nghiệm người dùng. (2) Nên đặt những câu hỏi làm rõ nào? Chất lượng của các câu hỏi làm rõ cũng ảnh hưởng đến hiệu quả và hiệu suất của việc sinh mã. Các câu hỏi chính xác và có mục tiêu giúp người dùng thể hiện ý định của họ một cách rõ ràng, đảm bảo rằng các phản hồi nhận được có liên quan trực tiếp đến những mơ hồ có trong yêu cầu. Các câu hỏi mơ hồ hoặc rộng tăng nguy cơ nhận được phản hồi ngoài chủ đề hoặc không liên quan, có thể cản trở LLM hiểu ý định của người dùng.

Trong bài báo này, chúng tôi đề xuất một framework mới có tên ClarifyGPT nhằm nâng cao việc sinh mã dựa trên LLM thông qua làm rõ yêu cầu. Đầu tiên, chúng tôi sử dụng kiểm tra tính nhất quán mã hai bước để quyết định khi nào đặt câu hỏi làm rõ. Chúng tôi được thúc đẩy bởi quan sát rằng việc cung cấp một yêu cầu rõ ràng cho LLM thường dẫn đến việc sinh ra các đoạn mã đa dạng hoạt động nhất quán, tức là với cùng các đầu vào thử nghiệm, những đoạn mã khác nhau đó có thể sẽ trả về cùng các đầu ra. Trong khi việc cung cấp một yêu cầu không rõ ràng, LLM có thể sinh ra các đoạn mã đa dạng hoạt động khác nhau. Cụ thể, trong bước đầu tiên, ClarifyGPT nhằm sinh ra nhiều đầu vào thử nghiệm chất lượng cao cho một yêu cầu đã cho thông qua biến đổi có nhận biết kiểu. Trong bước thứ hai, ClarifyGPT đưa yêu cầu đã cho vào một LLM để lấy mẫu n giải pháp mã và kiểm tra

xem chúng có tạo ra đầu ra giống hệt nhau khi được thử nghiệm với đầu vào đã sinh hay không. Nếu các đầu ra không giống hệt nhau, ClarifyGPT xác định rằng yêu cầu cần làm rõ thêm; và ngược lại. Thứ hai, chúng tôi sử dụng prompting dựa trên lý luận để sinh câu hỏi làm rõ. Ban đầu, ClarifyGPT chỉ đạo LLM phân tích các yếu tố góp phần vào tính mơ hồ của yêu cầu đã cho bằng cách so sánh các giải pháp mã với các chức năng khác nhau. Sau đó, nó xây dựng các câu hỏi làm rõ có mục tiêu dựa trên kết quả của phân tích này. Bằng cách so sánh những triển khai mã khác nhau này, các điểm mơ hồ tiềm năng trong yêu cầu có thể được xác định dễ dàng. Sau khi phát hiện các điểm mơ hồ trong yêu cầu, LLM có thể sinh ra các câu hỏi làm rõ có mục tiêu cho chúng. Cuối cùng, ClarifyGPT tinh chỉnh yêu cầu ban đầu dựa trên các câu hỏi đã sinh và phản hồi của chúng và sinh ra giải pháp mã cuối cùng.

Để đánh giá hiệu quả của ClarifyGPT, trước tiên chúng tôi tích hợp GPT-4[35] vào ClarifyGPT và tuyển mười người tham gia để đánh giá hiệu suất của nó trên hai bộ chuẩn công khai (MBPP-sanitized[4] và MBPP-ET[10]). Kết quả đánh giá con người cho thấy ClarifyGPT nâng cao hiệu suất (Pass@1) của GPT-4 trên MBPP-sanitized từ 70.96% lên 80.8%, cải thiện hiệu suất (Pass@1) của ChatGPT trên MBPP-ET từ 51.52% lên 60.19%. Bên cạnh đó, do yêu cầu sự tham gia của người tham gia, việc đánh giá ClarifyGPT có thể rất tốn kém và khó tái tạo. Để thực hiện các đánh giá tự động của ClarifyGPT trên các LLM và bộ chuẩn khác nhau mà không cần sự tham gia của người dùng, chúng tôi giới thiệu một phương pháp mô phỏng độ tin cậy cao để mô phỏng phản hồi của người dùng. Sau đó, chúng tôi tiến hành các thí nghiệm toàn diện trên bốn bộ chuẩn (HumanEval[8], HumanEval-ET[10], MBPP-sanitized và MBPP-ET) sử dụng hai LLM tiên tiến (tức là GPT-4 và ChatGPT). Kết quả chứng minh rằng, so với GPT-4 mặc định, ClarifyGPT đạt được cải thiện trung bình 11.52% trên bốn bộ chuẩn; so với ChatGPT mặc định, ClarifyGPT đạt được cải thiện trung bình 15.07% trên bốn bộ chuẩn. Những đóng góp chính của chúng tôi được nêu như sau:

•Framework: Chúng tôi đề xuất một framework mới, có tên ClarifyGPT, cho phép LLM phát hiện yêu cầu mơ hồ và xây dựng các câu hỏi làm rõ có mục tiêu. ClarifyGPT tinh chỉnh các yêu cầu mơ hồ dựa trên câu trả lời cho các câu hỏi làm rõ và tiếp tục sinh ra các giải pháp mã.

•Mô phỏng người dùng: Chúng tôi giới thiệu một phương pháp mô phỏng người dùng để tạo ra các câu trả lời mô phỏng độ tin cậy cao cho các câu hỏi làm rõ, giúp tạo thuận lợi cho việc đánh giá tự động của ClarifyGPT trên các LLM và bộ chuẩn khác nhau, loại bỏ nhu cầu sự tham gia trực tiếp của người dùng.

•Đánh giá: Chúng tôi tiến hành các thí nghiệm rộng rãi trên bốn bộ chuẩn được sử dụng rộng rãi để cho thấy rằng ClarifyGPT đạt được những cải thiện đáng kể trên các mô hình và bộ chuẩn khác nhau. Một đánh giá con người tiếp tục xác nhận tiềm năng đáng kể của việc áp dụng ClarifyGPT trong thực tế.

•Dữ liệu: bộ dữ liệu và mã nguồn có thể truy cập công khai[1] để tạo thuận lợi cho việc tái tạo nghiên cứu của chúng tôi và ứng dụng của nó trong các bối cảnh rộng rãi.

Trong phần còn lại của bài báo này, Mục 2 giới thiệu nền tảng và công trình liên quan. Mục 3 trình bày chi tiết framework được đề xuất ClarifyGPT. Mục 4 trình bày thiết kế thí nghiệm. Mục 5 minh họa kết quả và phân tích. Mục 6 thảo luận về lợi ích và hạn chế của ClarifyGPT và các mối đe dọa đối với tính hợp lệ. Cuối cùng, Mục 7 tóm tắt công trình này.

2 NỀN TẢNG VÀ CÔNG TRÌNH LIÊN QUAN

2.1 Sinh mã dựa trên LLM

Sinh mã là một chủ đề nghiên cứu nóng bỏng cho cộng đồng kỹ thuật phần mềm và trí tuệ nhân tạo. Gần đây, nhiều LLM đã được đề xuất cho việc sinh mã. Một lớp mô hình là các mô hình encoder-decoder, ví dụ như PLBART[2], CodeT5[45] và AlphaCode[27], thường mã hóa một văn bản đầu vào thành một embedding ngữ cảnh và giải mã embedding thành một giải pháp mã. Một lớp mô hình khác là các mô hình chỉ decoder được huấn luyện với mục tiêu dự đoán token tiếp theo và sinh mã từ trái sang phải. Các mô hình dòng GPT[5,8], PolyCoder[48] và InCoder[13] là các ví dụ của loại mô hình này. Trong số đó, ChatGPT[34] và GPT-4[35] là các LLM tiên tiến được phát triển bởi OpenAI. Chúng đã chứng minh khả năng hiểu và lý luận được cải thiện, thành thạo trong việc hiểu ngữ cảnh được cung cấp và khả năng sinh ra văn bản chất lượng cao.

Vì việc huấn luyện hoặc tinh chỉnh những LLM này cực kỳ tốn kém, cũng có rất nhiều nghiên cứu tập trung vào việc nâng cao hiệu suất của LLM trong việc sinh mã với việc tinh chỉnh tối thiểu hoặc không có. Học Prompt là một trong những kỹ thuật quan trọng nhất để đạt được mục tiêu này[11,28,31,40,47]. Chain-of-Thought (CoT)[47] là một kỹ thuật kỹ thuật prompting mới, có thể khơi gợi LLM tạo ra các bước lý luận trung gian dẫn đến câu trả lời cuối cùng. Nó đã cho thấy hiệu suất ấn tượng trong các nhiệm vụ lý luận phức tạp (ví dụ, lý luận số học và biểu tượng)[19,47], và do đó đã được áp dụng vào việc sinh mã[16,26]. Được truyền cảm hứng từ CoT, Li et al.[26] đề xuất một phương pháp prompting mới, có tên Structured CoT (SCoT). Khác với CoT, SCoT một cách rõ ràng giới thiệu các cấu trúc mã và dạy LLM sinh ra các bước lý luận trung gian với các cấu trúc chương trình. Jiang et al.[16] đề xuất một phương pháp tự lập kế hoạch có thể hướng dẫn LLM hiểu việc lập kế hoạch mã với các minh họa few-shot và viết kế hoạch mã tương ứng cho yêu cầu đã cho.

Các nghiên cứu nói trên tập trung vào việc tận dụng và tăng cường khả năng lý luận của LLM, tức là prompting LLM sinh ra các bước lý luận trung gian để nâng cao hiệu suất sinh mã. Tuy nhiên, chúng vẫn không đủ để giải quyết các yêu cầu mơ hồ do con người cung cấp, vì ý định người dùng không rõ ràng có thể đánh lạc hướng LLM tạo ra các bước lý luận không chính xác, từ đó cho ra kết quả không chính xác. ClarifyGPT của chúng tôi nhận ra tầm quan trọng của việc làm rõ các yêu cầu mơ hồ và đề xuất một framework mới cho phép LLM tự động phát hiện yêu cầu mơ hồ và đặt các câu hỏi làm rõ có mục tiêu. Bằng cách làm rõ yêu cầu của người dùng, ClarifyGPT có thể sinh ra các giải pháp mã thỏa mãn ý định của người dùng. GPT-Engineer[36] là một kho GitHub mã nguồn mở gần đây. Nó sử dụng các hướng dẫn được thiết kế thủ công để prompt LLM đặt câu hỏi làm rõ cho các yêu cầu đầu vào của người dùng, sau đó sinh ra các đoạn mã dựa trên phản hồi của người dùng. Tuy nhiên, GPT-Engineer đặt câu hỏi làm rõ cho cả yêu cầu mơ hồ và không mơ hồ, điều này có hại cho trải nghiệm người dùng và có thể dẫn đến các giải pháp mã không chính xác¹. Trong khi ClarifyGPT có thể phát hiện yêu cầu mơ hồ bằng cách kiểm tra xem các đầu ra thử nghiệm của các giải pháp mã được lấy mẫu có giống hệt nhau hay không. Hơn nữa, ClarifyGPT sử dụng các kỹ thuật prompting để chỉ đạo LLM trước tiên phân tích các yếu tố góp phần vào tính mơ hồ của yêu cầu và sau đó xây dựng các câu hỏi có mục tiêu.

2.2 Sinh câu hỏi làm rõ

Nhiệm vụ sinh câu hỏi làm rõ cho các truy vấn hoặc đối thoại mơ hồ đã nhận được nhiều sự chú ý trong các lĩnh vực tìm kiếm thông tin và hệ thống đối thoại[9,18,20,30,37,41]. Về mặt tìm kiếm thông tin, nhiều nghiên cứu đã chỉ ra rằng câu hỏi làm rõ có thể giúp giải quyết các truy vấn mơ hồ và cải thiện trải nghiệm người dùng. Ví dụ, Wang và Li[43] phát hiện rằng các truy vấn tìm kiếm thường ngắn và ý định người dùng cơ bản thường mơ hồ. Họ đề xuất một mô hình sinh câu hỏi làm rõ được hướng dẫn bởi template hiệu quả, sử dụng Transformer để chọn một template câu hỏi từ danh sách các ứng viên template và điền vào slot câu hỏi từ một từ vựng slot. Eberhart và McMillan[12] đề xuất một phương pháp mới để đặt câu hỏi làm rõ cho việc tinh chỉnh truy vấn, sử dụng một thuật toán trích xuất nhiệm vụ để xác định các khía cạnh truy vấn và theo một quy trình dựa trên quy tắc để sinh câu hỏi. Về mặt lĩnh vực hệ thống đối thoại, cả phương pháp dựa trên quy tắc và dựa trên học máy đều đã được đề xuất. Dhole[9] đề xuất một phương pháp mới để sinh câu hỏi phân biệt bằng cách tận dụng một hệ thống dựa trên quy tắc đơn giản, nhằm tìm kiếm sự làm rõ từ người dùng, từ đó giảm tính robot của cuộc trò chuyện và làm cho tương tác trở nên tự nhiên hơn đáng kể. Rao et al.[37] mô tả một phương pháp sinh câu hỏi làm rõ, sử dụng mô hình seq2seq để sinh câu hỏi với ngữ cảnh và sử dụng một mô hình seq2seq khác để sinh câu trả lời với ngữ cảnh và câu hỏi.

Trong việc sinh mã, việc xử lý các yêu cầu mơ hồ của người dùng đã nhận được ít sự chú ý cho đến nay. Theo hiểu biết tốt nhất của chúng tôi, Li et al.[25] là bài báo nghiên cứu duy nhất giải quyết việc giải quyết yêu cầu mơ hồ cho việc sinh mã. Công trình này nhằm làm rõ các yêu cầu mơ hồ thiếu các hoạt động chính, ví dụ như các lệnh gọi API. Nó đầu tiên thu thập một bộ dữ liệu có tên Code ClarQA chứa các yêu cầu ngôn ngữ tự nhiên, mã, câu hỏi làm rõ và câu trả lời. Sau đó, nó đề xuất một pipeline sinh mã có thể chọn các câu hỏi làm rõ liên quan và câu trả lời của chúng từ bộ dữ liệu cho một yêu cầu đã cho để sinh ra một giải pháp mã. Tuy nhiên, phạm vi ứng dụng của công trình này bị hạn chế. Thứ nhất, nó chủ yếu tập trung vào việc làm rõ các mơ hồ ở cấp độ hoạt động, để lại các dạng mơ hồ khác, chẳng hạn như mơ hồ ngữ nghĩa trong các yêu cầu ngôn ngữ tự nhiên, được giải quyết ít hiệu quả hơn. Hơn nữa, nó phụ thuộc nhiều vào bộ dữ liệu được xây dựng, tìm kiếm các câu hỏi liên quan cho các yêu cầu mơ hồ. Nếu bộ dữ liệu thiếu các yêu cầu tương tự, hiệu suất của phương pháp có thể bị ảnh hưởng. Khác với công trình này, ClarifyGPT không bị giới hạn trong một loại làm rõ yêu cầu mơ hồ cụ thể. Và ClarifyGPT có thể sinh ra các câu hỏi chính xác và có mục tiêu cho các yêu cầu khác nhau bằng cách tận dụng khả năng hiểu mạnh mẽ của LLM.

3 PHƯƠNG PHÁP

Trong phần này, chúng tôi giới thiệu ClarifyGPT, một framework sinh mã cho LLM. Hình 1 minh họa tổng quan về ClarifyGPT, bao gồm bốn giai đoạn chính: (1) Sinh đầu vào thử nghiệm

(Mục 3.1), nhằm sinh ra các đầu vào thử nghiệm chất lượng cao cho một yêu cầu đã cho bằng cách sử dụng các kỹ thuật prompting và biến đổi heuristic; (2) Kiểm tra tính nhất quán mã (Mục 3.2), để tận dụng các đầu vào thử nghiệm đã sinh ra tiến hành đánh giá tính nhất quán, sau đó xác định các yêu cầu mơ hồ; (3) Sinh câu hỏi dựa trên lý luận (Mục 3.3), tập trung vào việc sinh ra các câu hỏi làm rõ có mục tiêu cho các yêu cầu mơ hồ đã xác định bằng cách prompting LLM tham gia vào lý luận trung gian; (4) Sinh mã nâng cao (Mục 3.4), kết hợp các câu hỏi làm rõ và phản hồi của chúng để tinh chỉnh yêu cầu ban đầu và sinh ra giải pháp mã cuối cùng dựa trên prompt đã tinh chỉnh. Dưới đây, chúng tôi cung cấp chi tiết cho từng giai đoạn trong ClarifyGPT.

3.1 Sinh đầu vào thử nghiệm

Trong bước này, ClarifyGPT nhằm tạo ra các đầu vào thử nghiệm chất lượng cao để phân biệt hiệu quả giữa các giải pháp mã với các chức năng khác nhau. Có nhiều nghiên cứu đã cố gắng sử dụng LLM để sinh test case đơn vị[24,38,42] và đã chứng minh hiệu suất ấn tượng. Theo công trình trước đây[29], ClarifyGPT tận dụng LLM như là trình sinh đầu vào thử nghiệm và sinh đầu vào thử nghiệm bằng cách áp dụng phương pháp hai bước (tức là khởi tạo đầu vào gốc và biến đổi có nhận biết kiểu).

Cụ thể, ClarifyGPT bắt đầu bằng cách thiết kế một prompt để hướng dẫn LLM tạo ra một tập hợp các đầu vào gốc. Sau đó, nó thực hiện các biến đổi có nhận biết kiểu để sinh ra một số lượng lớn các đầu vào mới. Những hiểu biết của chúng tôi là: (1) một mặt, vì LLM có khả năng hiểu và lý luận mạnh mẽ, việc sử dụng chúng làm trình sinh đầu vào thử nghiệm có thể tạo ra các đầu vào chất lượng cao vẫn hợp lệ ngay cả dưới các ràng buộc ngữ nghĩa. Lấy ví dụ được hiển thị trong Hình 4, bài toán trong HumanEval yêu cầu chuỗi đầu vào phải chứa các dấu ngoặc đơn cân bằng. Các trình sinh đầu vào truyền thống thường gặp thách thức trong việc đảm bảo tuân thủ các ràng buộc ngữ nghĩa như vậy. (2) mặt khác, LLM không phù hợp cho việc sinh test tự động với số lượng lớn do tốc độ và chi phí không mong muốn của việc truy vấn các mô hình lớn như vậy[29]. Do đó, chúng tôi sử dụng phương pháp dựa trên biến đổi heuristic để đẩy nhanh việc sinh ra nhiều test case, đảm bảo cả tính ổn định và độ tin cậy.

3.1.1 Khởi tạo đầu vào gốc. ClarifyGPT bắt đầu bằng việc thiết kế một prompt cho việc khởi tạo đầu vào gốc. Như được hiển thị trong Hình 3 (a), prompt bao gồm ba phần: (1) một hướng dẫn, được thiết kế để khơi gợi LLM sinh ra các đầu vào thử nghiệm phức tạp, khó và trường hợp góc; (2) các minh chứng được tạo thủ công few-shot, bao gồm yêu cầu của người dùng và đầu vào thử nghiệm thực tế, có thể hỗ trợ LLM hiểu rõ hơn nhiệm vụ được mô tả trong hướng dẫn; (3) một truy vấn, mà LLM sinh ra test đầu vào dựa trên nó. Cụ thể, chúng tôi đầu tiên hoàn thiện prompt với hướng dẫn, minh chứng và yêu cầu đã cho. Sau đó, ClarifyGPT sử dụng prompt để truy vấn LLM sinh ra đầu vào gốc. Cuối cùng, chúng tôi thu thập những đầu vào gốc đã sinh này để khởi tạo một pool gốc sẽ được sử dụng cho biến đổi.

3.1.2 Biến đổi đầu vào có nhận biết kiểu. Sau khi khởi tạo một pool gốc, ClarifyGPT sử dụng chiến lược biến đổi đầu vào có nhận biết kiểu[29] để sinh ra các đầu vào thử nghiệm chất lượng cao hơn. Cụ thể, phương pháp của chúng tôi theo quy trình fuzzing dựa trên biến đổi tiêu chuẩn[49,50]: (1) Tại mỗi lần lặp, một đầu vào được chọn ngẫu nhiên từ pool gốc. (2) Đối với đầu vào được chọn, chúng tôi kiểm tra các kiểu dữ liệu của nó và thực hiện một hoạt động biến đổi đơn lẻ phù hợp với kiểu của nó để tạo ra một test case mới. Các biến đổi cơ bản được sử dụng cho các kiểu đầu vào khác nhau được minh họa trong Hình 2. Đối với các kiểu dữ liệu đơn giản, chẳng hạn như int và float, một hoạt động biến đổi đơn giản chỉ tăng hoặc giảm giá trị của nó đi 1. Đối với các kiểu phức hợp, chúng tôi biến đổi các phần tử dựa trên các kiểu nội bộ của chúng. (3) Sau khi hoàn thành một vòng biến đổi, chúng tôi thêm các đầu vào mới được sinh ra vào pool gốc và lặp lại quá trình nói trên cho đến khi đạt được số lượng đầu vào được sinh ra mong muốn.

3.2 Kiểm tra tính nhất quán mã

Một yêu cầu người dùng rõ ràng nên dễ hiểu và không để lại chỗ cho việc diễn giải, trong khi một yêu cầu người dùng mơ hồ có thể dẫn đến các bên liên quan diễn giải nó theo những cách khác nhau. Được truyền cảm hứng từ điều này, chúng tôi đưa ra giả định rằng, đối với một yêu cầu đã cho, nếu một LLM sinh ra nhiều giải pháp mã với các chức năng khác nhau, điều đó có nghĩa là yêu cầu có thể dẫn đến LLM diễn giải nó theo những cách khác nhau. Do đó, yêu cầu như vậy cần làm rõ và tinh chỉnh thêm. Dựa trên giả định này, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả để xác định các yêu cầu mơ hồ. Đầu tiên, chúng tôi cung cấp một yêu cầu đã cho vào một LLM để lấy mẫu n giải pháp mã. Sau đó, các giải pháp mã này được thực thi với các đầu vào thử nghiệm được sinh ra ở bước trước. Chúng tôi thu được các đầu ra thử nghiệm của những chương trình này và so sánh các đầu ra thử nghiệm để kiểm tra xem chúng có giống hệt nhau hay không. Nếu các đầu ra giống hệt nhau, ClarifyGPT coi những giải pháp mã này như đang diễn giải yêu cầu theo cùng một cách, do đó xác định yêu cầu là không mơ hồ. Trong trường hợp này, một trong các mã được lấy mẫu sẽ được xuất ra làm giải pháp mã cuối cùng. Tuy nhiên, nếu các đầu ra không giống hệt nhau, ClarifyGPT tin rằng LLM có hiểu biết khác nhau về yêu cầu này khi nó tạo ra các giải pháp mã và xác định yêu cầu là mơ hồ. Đối với những yêu cầu mơ hồ này, như được hiển thị trong Hình 1, chúng tôi thực hiện clustering mã để chia những giải pháp mã này thành nhiều nhóm dựa trên đầu ra thử nghiệm của chúng. Sau đó, ClarifyGPT chọn ngẫu nhiên một giải pháp mã từ mỗi nhóm và cung cấp những giải pháp mã không nhất quán này vào thành phần tiếp theo để tổng hợp prompt được sử dụng để đặt câu hỏi.

3.3 Sinh câu hỏi dựa trên lý luận

Các câu hỏi làm rõ có mục tiêu tạo thuận lợi cho người dùng trong việc trình bày ý định của họ một cách rõ ràng, đảm bảo rằng các phản hồi nhận được có liên quan trực tiếp đến các phần không rõ ràng trong yêu cầu. Các câu hỏi mơ hồ hoặc rộng tăng nguy cơ nhận được phản hồi ngoài chủ đề hoặc không liên quan, có thể làm tổn hại hiệu suất của việc sinh mã. Do đó, khi xác định các yêu cầu mơ hồ, việc trao quyền cho LLM khả năng tạo ra các câu hỏi chính xác và có mục tiêu trở nên thiết yếu. Để đạt được mục tiêu này, chúng tôi thiết kế một prompt dựa trên lý luận nhằm chỉ đạo LLM ban đầu phân tích các yếu tố góp phần vào tính mơ hồ của yêu cầu và sau đó xây dựng các câu hỏi có mục tiêu dựa trên phân tích. Prompt được thiết kế được mô tả trong Hình 3 (b). Nó bao gồm ba phần: (1) một hướng dẫn, mô tả nhiệm vụ (tức là sinh câu hỏi làm rõ) chúng tôi muốn LLM giải quyết; (2) các bộ ba <yêu cầu, giải pháp không nhất quán, câu hỏi làm rõ> few-shot làm minh chứng, giúp LLM hiểu và giải quyết nhiệm vụ; (3) một truy vấn, chứa yêu cầu của người dùng và các giải pháp mã của nó, được cung cấp cho LLM để sinh câu hỏi.

Cụ thể, ClarifyGPT xây dựng prompt để chỉ đạo LLM phân tích các yếu tố góp phần vào yêu cầu không rõ ràng bằng cách hiểu các chức năng của những giải pháp mã không nhất quán này và so sánh sự khác biệt của chúng. Động lực là trong phát triển phần mềm, các giải pháp mã đại diện cho việc triển khai cụ thể của yêu cầu. Nếu một yêu cầu mơ hồ, các nhà phát triển khác nhau có thể có các diễn giải khác nhau và do đó viết mã khác nhau. Một số trong những giải pháp mã không nhất quán này không chính xác hoặc không phù hợp với ý định ban đầu. Bằng cách so sánh những triển khai mã khác nhau này, các điểm mơ hồ tiềm năng trong yêu cầu có thể được xác định dễ dàng. Sau khi phát hiện các điểm mơ hồ trong yêu cầu, LLM tiếp tục sinh ra các câu hỏi làm rõ có mục tiêu dựa trên kết quả phát hiện.

Prompting được đề xuất của chúng tôi chia sẻ ý tưởng tương tự với prompting Chain of Thought (CoT)[47], khơi gợi LLM sinh ra các bước lý luận trung gian (phân tích các yếu tố góp phần vào tính mơ hồ) trước, và sau đó tạo ra kết quả cuối cùng (câu hỏi làm rõ có mục tiêu) dựa trên những bước lý luận trung gian này. Theo cách này, ClarifyGPT khuyến khích LLM thực hiện "lập kế hoạch xa"[6], cho phép chúng tận dụng tốt hơn khả năng lý luận và hiểu để nâng cao chất lượng của các câu hỏi được sinh ra.

3.4 Sinh mã nâng cao

Khi phản hồi của người dùng được ghi lại, ClarifyGPT kết hợp chúng với các câu hỏi đã sinh ra để tinh chỉnh yêu cầu ban đầu thành một yêu cầu rõ ràng. Cụ thể, như được hiển thị trong truy vấn trong Hình 3 (d), chúng tôi ghép đôi mỗi câu hỏi và câu trả lời tương ứng để tạo ra một sự làm rõ, sau đó được nối vào cuối docstring để tạo thành yêu cầu đã tinh chỉnh. Bằng cách tinh chỉnh yêu cầu mơ hồ theo cách này, chúng tôi có thể bảo toàn tính toàn vẹn cấu trúc của docstring trong yêu cầu ban đầu đồng thời tăng cường nó với thông tin làm rõ bổ sung. Sau đó, chúng tôi sử dụng yêu cầu đã tinh chỉnh để xây dựng một prompt hướng dẫn LLM sinh ra giải pháp mã cuối cùng. Prompt được xây dựng cũng bao gồm ba phần, tức là một hướng dẫn, một số minh chứng và một truy vấn, như được mô tả trong Hình 3 (d).

4 THIẾT KẾ THÍ NGHIỆM

Để đánh giá hiệu quả của ClarifyGPT, chúng tôi tiến hành các thí nghiệm toàn diện. Trong phần này, chúng tôi minh họa thiết kế thí nghiệm của chúng tôi, bao gồm các câu hỏi nghiên cứu, mô hình, bộ chuẩn, số liệu, baseline và chi tiết triển khai.

4.1 Câu hỏi nghiên cứu

Chúng tôi giải quyết ba câu hỏi nghiên cứu sau để đánh giá hiệu suất của ClarifyGPT.

RQ1: ClarifyGPT hoạt động như thế nào khi nhận phản hồi thực tế của người dùng so với các phương pháp baseline? Trong các tình huống thế giới thực, ClarifyGPT của chúng tôi hỗ trợ người dùng viết mã bằng cách tương tác với họ, tức là đặt câu hỏi làm rõ và nhận phản hồi của người dùng. Do đó, trong RQ này, chúng tôi khám phá xem ClarifyGPT với human in the loop có thể đạt được hiệu suất cao hơn các baseline sinh mã hiện có hay không. Vì việc đánh giá sinh mã tương tác với người tham gia là tốn kém, chúng tôi chỉ chọn GPT-4 làm mô hình cơ sở và thuê mười người tham gia (bao gồm các nhà nghiên cứu học thuật và nhà phát triển ngành) để trả lời thủ công các câu hỏi làm rõ được sinh ra bởi ClarifyGPT. Chúng tôi so sánh ClarifyGPT với ba baseline trên hai bộ chuẩn (tức là MBPP-sanitized và MBPP-ET).

RQ2: ClarifyGPT hoạt động như thế nào khi nhận phản hồi người dùng được mô phỏng so với các phương pháp baseline tiên tiến? RQ này thực hiện các đánh giá tự động quy mô lớn của ClarifyGPT trên các LLM và bộ chuẩn khác nhau mà không cần sự tham gia của người dùng, nhằm xác minh thêm xem ClarifyGPT có thể đạt được hiệu suất cao hơn các baseline sinh mã hiện có hay không. Chúng tôi đầu tiên đề xuất một phương pháp mô phỏng người dùng tận dụng LLM để mô phỏng phản hồi của người dùng. Sau đó, chúng tôi áp dụng ba baseline và ClarifyGPT cho hai LLM đại diện (tức là GPT-4 và ChatGPT), và đánh giá hiệu suất của chúng trên bốn bộ chuẩn được sử dụng rộng rãi (tức là HumanEval, MBPP-sanitized, HumanEval-ET và MBPP-ET).

RQ3: Số lượng minh chứng trong một prompt ảnh hưởng như thế nào đến hiệu suất của ClarifyGPT? Các kỹ thuật prompting có thể nhạy cảm với số lượng minh chứng[14,32]. Trong câu hỏi nghiên cứu này, chúng tôi đo lường hiệu suất của ClarifyGPT với số lượng minh chứng khác nhau để điều tra tính mạnh mẽ của prompt trong ClarifyGPT.

4.2 Các LLM được nghiên cứu

Có nhiều LLM có sẵn cho việc sinh mã. Tuy nhiên, bối cảnh cụ thể của công trình này đòi hỏi các LLM phải có một mức độ năng lực giao tiếp nhất định, tức là khả năng hiểu hướng dẫn của con người và xây dựng câu hỏi làm rõ. Do đó, các LLM không có instruction tuning (ví dụ như InCoder[13] và CodeGen[33]) không phù hợp làm mô hình cơ sở được áp dụng

cho framework ClarifyGPT. Trong công trình này, chúng tôi chọn hai chat-LLM đại diện (tức là ChatGPT và GPT4) làm mô hình cơ sở để đánh giá framework ClarifyGPT.

•ChatGPT [34] là một trong những mô hình chat mạnh mẽ nhất được phát triển bởi OpenAI. Nó được huấn luyện sử dụng một phương pháp mới có tên Reinforcement Learning from Human Feedback (RLHF), tích hợp liền mạch học tăng cường và phản hồi con người. Cụ thể, ChatGPT đầu tiên được huấn luyện với lượng lớn văn bản ngôn ngữ tự nhiên và tệp mã. Sau đó, nó được tinh chỉnh thông qua học tăng cường, cho phép nó hiểu và thực hiện hướng dẫn của con người một cách thành thạo. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng API của OpenAI để truy cập mô hình ChatGPT, tức là gpt-3.5-turbo.

•GPT-4 [35] là LLM tiên tiến nhất của OpenAI, có thể nhận đầu vào hình ảnh và văn bản, phát ra đầu ra văn bản. Nó cũng được huấn luyện với học tăng cường và học cách theo hướng dẫn của con người. GPT-4 đã chứng minh khả năng hiểu ngôn ngữ được cải thiện, cho phép nó hiểu các ngữ cảnh phức tạp và sắc thái, làm cho nó cực kỳ hiệu quả trong nhiều nhiệm vụ downstream, bao gồm tóm tắt văn bản, dịch thuật và sinh mã[6]. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng API của OpenAI để truy cập mô hình GPT-4, tức là gpt-4-turbo.

4.3 Bộ chuẩn

Theo công trình trước đây[7,11,17,26], chúng tôi tiến hành thí nghiệm trên bốn bộ chuẩn sinh mã công khai: HumanEval[8], MBPP-sanitized[4], cùng với các phiên bản test case mở rộng của chúng (tức là HumanEval-ET và MBPP-ET [10]). Thống kê của những bộ chuẩn này được hiển thị trong Bảng 1.

•HumanEval [8] là một bộ dữ liệu giải quyết vấn đề được viết tay được tạo ra sau ngày cắt của bộ dữ liệu huấn luyện Codex, bao gồm 164 bài toán lập trình Python. Các bài toán lập trình trong HumanEval liên quan đến hiểu ngôn ngữ, thuật toán và toán học. Mỗi bài toán bao gồm một chữ ký hàm, một yêu cầu ngôn ngữ tự nhiên và một số unit test. Một bài toán được coi là được giải quyết bởi code-LLM khi tất cả unit test đều được vượt qua.

•MBPP-sanitized [4] là một tập con được xác minh thủ công của bộ dữ liệu MBPP (Mostly Basic Programming Problems), chứa 427 bài toán lập trình Python được thu thập từ cộng đồng, liên quan đến thao tác số, chức năng thư viện chuẩn và nhiều hơn nữa. Mỗi bài toán chứa một chữ ký hàm, một yêu cầu người dùng và ba test case.

•HumanEval-ET và MBPP-ET [10] là hai phiên bản mở rộng của bộ chuẩn HumanEval và MBPP với trung bình 100+ test case bổ sung mỗi bài toán. Để cải thiện độ tin cậy của đánh giá mã được sinh ra, chúng thu thập nhiều test case góc không được bao gồm trong các bộ chuẩn ban đầu.

4.4 Số liệu đánh giá

Chúng tôi đánh giá độ chính xác của mã được sinh ra bằng số liệu Pass@k[22]. Số liệu này phục vụ như một ước lượng khả năng sinh dưới một ngân sách cụ thể, được sử dụng rộng rãi trong

các nghiên cứu liên quan đến LLM trước đây[7,23,51]. Đối với mỗi bài toán trong các bộ chuẩn, chúng tôi sinh ra k giải pháp mã, và nếu bất kỳ giải pháp mã nào trong k giải pháp mã vượt qua tất cả các test, bài toán này được coi là được giải quyết. Trong các tình huống phát triển thế giới thực, việc sinh ra k mã sẽ đặt gánh nặng lên các nhà phát triển, tức là họ cần đọc và hiểu k mã khác nhau và chọn một làm mã mục tiêu. Do đó, trong bài báo này, k được đặt là 1, thỏa mãn hầu hết các tình huống mà các nhà phát triển chỉ xem xét mã được sinh ra đơn lẻ[11,17]. Để tránh phương sai cao và tính ngẫu nhiên, chúng tôi chạy mỗi phương pháp ba lần và báo cáo kết quả trung bình làm kết quả cuối cùng.

4.5 Baseline so sánh

•Default LLM: lấy các yêu cầu ban đầu trực tiếp từ các bộ chuẩn làm đầu vào để prompt LLM sinh mã.

•CoT (Chain-of-Thought) [47]: sinh ra một chuỗi các bước lý luận cho mỗi yêu cầu bằng cách sử dụng prompt CoT và sau đó sinh ra mã tương ứng. Để đảm bảo tính công bằng của so sánh, baseline CoT có cùng số lượng minh chứng (tức là ba minh chứng) và seed minh chứng.

•GPT-Engineer²: là một kho GitHub mã nguồn mở gần đây. Nó sử dụng các hướng dẫn được thiết kế thủ công để khơi gợi LLM đặt câu hỏi làm rõ cho các yêu cầu đầu vào của người dùng và sau đó sinh ra các đoạn mã dựa trên phản hồi của người dùng.

4.6 Chi tiết triển khai

Các chi tiết triển khai về xây dựng prompt và cấu hình mô hình trong ClarifyGPT như sau.

Xây dựng Prompt. Vì bốn bộ chuẩn không có tập huấn luyện, theo công trình trước đây[44,47], chúng tôi chọn ba bài toán đầu tiên từ mỗi bộ chuẩn và trích xuất các yêu cầu người dùng từ những bài toán này làm seed minh chứng. Sau đó, chúng tôi tạo thủ công các minh chứng khác biệt cho các prompt khác nhau, như được minh họa trong Hình 3. Cần lưu ý rằng lý do chúng tôi chỉ tạo ba minh chứng cho mỗi prompt là do giới hạn độ dài đầu vào của LLM.

Cấu hình Mô hình. Chúng tôi coi hai LLM được sử dụng trong thí nghiệm như các trình sinh hộp đen và chỉ đặt một số tham số giao diện mà chúng cung cấp mà không truy cập các tham số nội bộ. Đối với tất cả LLM, chúng tôi đặt top p là 0.95, frequency_penalty là 0. Max_tokens đại diện cho số lượng token tối đa được sinh ra, được đặt là 800 cho prompt đặt câu hỏi làm rõ và 300 cho các prompt khác. Cụ thể, chúng tôi đặt temperature là 0, ngoại trừ khi lấy mẫu các giải pháp mã, temperature được đặt là 0.8. Chúng tôi theo Chen et al.[8] để cắt bớt nội dung được sinh ra trong HumanEval và MBPP bằng năm chuỗi dừng: "\nclass", "\ndef", "\n#", "\nif", và "\nprint".

5 KẾT QUẢ VÀ PHÂN TÍCH

5.1 RQ1: ClarifyGPT hoạt động như thế nào khi nhận phản hồi thực tế của người dùng so với các phương pháp baseline?

Thiết lập. Trong RQ này, chúng tôi khám phá ClarifyGPT hoạt động như thế nào trong các tình huống thế giới thực, tức là liệu ClarifyGPT có thể đạt được hiệu suất cao hơn các baseline sinh mã hiện có khi nhận phản hồi thực tế của người dùng hay không. Cụ thể, chúng tôi áp dụng ClarifyGPT vào mô hình GPT-4. Vì bộ chuẩn MBPP-ET chia sẻ cùng yêu cầu người dùng với MBPP-sanitized, chúng tôi chỉ áp dụng ClarifyGPT cho các phiên bản gốc của bộ chuẩn (tức là MBPP-sanitized) và báo cáo hiệu suất của ClarifyGPT trên hai bộ chuẩn này bằng cách sử dụng các unit test tương ứng. ClarifyGPT đầu tiên lấy yêu cầu người dùng của mỗi bài toán trong các bộ chuẩn làm đầu vào và xác định chúng là mơ hồ hoặc không mơ hồ.

Sau đó, nó sinh ra câu hỏi làm rõ cho các yêu cầu mơ hồ (như được hiển thị trong Hình 1). Tổng cộng, chúng tôi thu được 140 bài toán có yêu cầu mơ hồ từ bộ chuẩn MBPP-sanitized. Số lượng câu hỏi làm rõ trung bình mỗi bài toán là 2.85. Chúng tôi tạo ra ba bảng câu hỏi giống hệt nhau cho mỗi bài toán, đảm bảo rằng mỗi bài toán sẽ được đánh giá bởi ba người tham gia khác nhau. Mỗi bảng câu hỏi bao gồm ba yếu tố: (1) yêu cầu (mơ hồ) của bài toán, mô tả ý định của bài toán; (2) các unit test case chứa các ví dụ đầu vào-đầu ra mong đợi, hỗ trợ người tham gia hiểu ý định của bài toán; (3) các câu hỏi làm rõ được sinh ra, mà người tham gia được yêu cầu trả lời.

Chúng tôi tuyển dụng mười người tham gia, bao gồm ba nghiên cứu sinh tiến sĩ, hai sinh viên thạc sĩ, hai nhà nghiên cứu cao cấp và ba nhà phát triển ngành. Không ai trong số họ là đồng tác giả của bài báo này. Tất cả người tham gia đều có ít nhất ba năm kinh nghiệm phát triển Python, với sáu người trong số họ có hơn năm năm kinh nghiệm. Người tham gia ban đầu được cung cấp mô tả nhiệm vụ và các bảng câu hỏi ví dụ chứa câu trả lời thích hợp. Sau khi hoàn thành bài tập huấn luyện, chúng tôi giao 42 bài toán cho mỗi người tham gia và yêu cầu họ trả lời các câu hỏi làm rõ dựa trên thông tin được cung cấp trong các bảng câu hỏi. Mỗi bài toán sẽ được giải quyết bởi ba người tham gia.

Chúng tôi thu thập các câu trả lời được cung cấp bởi người tham gia và đưa chúng vào ClarifyGPT để sinh ra các giải pháp mã cuối cùng. Như đã đề cập trước đây, chúng tôi đánh giá tính đúng đắn của mã được sinh ra trên hai bộ chuẩn bằng cách sử dụng các unit test case. Vì các câu hỏi làm rõ của mỗi bài toán được trả lời bởi ba người tham gia, chúng tôi báo cáo kết quả Pass@1 trung bình.

Kết quả. Kết quả so sánh giữa hiệu suất của ClarifyGPT và các baseline khác được mô tả trong Bảng 2. Các giá trị màu đỏ là các cải thiện tương đối của ClarifyGPT so với baseline Default.

Chúng ta có thể thấy rằng ClarifyGPT (Human Feedback) đạt được hiệu suất cao nhất trên tất cả bốn bộ chuẩn. So với Default, ClarifyGPT (Human Feedback) chứng minh hiệu suất vượt trội với số liệu Pass@1, đạt được mức tăng 13.87% trên MBPP-sanitized và 16.83% trên MBPP-ET. Hơn nữa, khi so sánh với các baseline có hiệu suất tốt nhất (tức là CoT hoặc GPT-Engineer), ClarifyGPT (Human Feedback) cũng cải thiện hiệu suất Pass@1 thêm 9.53% trên MBPP-sanitized và 9.52% trên MBPP-ET. Điều này chủ yếu là do ClarifyGPT có thể thành thạo xác định các yêu cầu mơ hồ và đặt ra các câu hỏi làm rõ có mục tiêu. Người dùng dễ dàng làm rõ ý định của họ bằng cách trả lời những câu hỏi này, do đó tạo thuận lợi cho việc sinh ra mã chính xác hơn bởi LLM. Điều này chỉ ra rằng ClarifyGPT, như một framework sinh mã tương tác, có thể hỗ trợ các nhà phát triển viết mã trong các bối cảnh phát triển thế giới thực.

Trả lời RQ1: Trong đánh giá con người, ClarifyGPT nâng cao hiệu suất (Pass@1) của GPT-4 trên MBPP-sanitized từ 70.96% lên 80.8%; nâng cao hiệu suất của nó trên MBPP-ET từ 51.52% lên 60.19%. Cải thiện tương đối trung bình là 15.35%, vượt trội hơn các baseline.

5.2 RQ2: ClarifyGPT hoạt động như thế nào khi nhận phản hồi người dùng được mô phỏng so với các phương pháp baseline tiên tiến?

Thiết lập. Do sự tham gia của người tham gia, việc đánh giá framework sinh mã tương tác ClarifyGPT rất tốn kém và khó tái tạo. Một giải pháp tương đối đơn giản là tiến hành đánh giá offline[3]. Tuy nhiên, điều này giới hạn hệ thống trong việc chọn câu hỏi làm rõ từ một tập hợp các câu hỏi được xác định trước hoặc được gán nhãn, điều này không chuyển đổi tốt cho môi trường phát triển thực tế. Trong RQ này, chúng tôi áp dụng phương pháp User Simulation for Evaluation[15,39] để tạo thuận lợi cho việc đánh giá tự động của ClarifyGPT trên các LLM và bộ chuẩn khác nhau, loại bỏ nhu cầu sự tham gia trực tiếp của người dùng.

Khía cạnh quan trọng nhất của việc mô phỏng phản hồi người dùng là đảm bảo rằng phản hồi người dùng được tạo ra giống với phản hồi thực tế mà người dùng sẽ cung cấp trong cùng môi trường. Các mô phỏng độ tin cậy thấp có thể dẫn đến ClarifyGPT nhận phản hồi khó gặp trong thực tế, từ đó cho ra kết quả sai lệch và ảnh hưởng đến đánh giá hiệu suất của ClarifyGPT. Do đó, chúng tôi đề xuất một phương pháp mô phỏng người dùng độ tin cậy cao tận dụng LLM để sinh phản hồi của người dùng bằng cách cung cấp cho LLM các câu hỏi làm rõ và test case thực tế. Hiểu biết chính của chúng tôi là các test case thực tế chứa các ví dụ đầu vào-đầu ra mong đợi, phản ánh chức năng mong muốn mà người dùng tìm kiếm. Việc trao cho LLM kiến thức tiên nghiệm này tạo thuận lợi cho việc hiểu ý định người dùng và cho phép sinh ra phản hồi người dùng mô phỏng độ tin cậy cao. Để hướng dẫn LLM giải quyết nhiệm vụ này, chúng tôi thiết kế một prompt (như được hiển thị trong Hình 3), cũng bao gồm ba phần: (1) một hướng dẫn, mô tả nhiệm vụ (tức là mô phỏng phản hồi người dùng) chúng tôi muốn LLM giải quyết; (2) các bộ bốn <yêu cầu, test thực tế, câu hỏi làm rõ, câu trả lời> few-shot làm minh chứng, giúp LLM hiểu và giải quyết nhiệm vụ; (3) một truy vấn, chứa yêu cầu người dùng và test thực tế của nó, được cung cấp cho LLM để sinh phản hồi mô phỏng.

Chúng tôi áp dụng ba baseline (Mục 4.5) và ClarifyGPT của chúng tôi cho hai SOTA LLM (Mục 4.2). Chúng tôi đánh giá chúng trên bốn bộ chuẩn (Mục 4.3) và so sánh hiệu suất của chúng bằng cách tính toán số liệu Pass@1 (Mục 4.4). Để so sánh công bằng, tất cả baseline đều áp dụng cùng thiết lập thí nghiệm với ClarifyGPT của chúng tôi.

Kết quả. Bảng 3 trình bày kết quả so sánh giữa hiệu suất của ClarifyGPT (Simulated Feedback) và các baseline khác về mặt sinh mã. Các giá trị màu đỏ là các cải thiện tương đối của ClarifyGPT (Simulated Feedback) so với baseline Default.

Tổng thể, ClarifyGPT (Simulated Feedback) có thể cải thiện đáng kể hiệu suất sinh mã, đạt được những tiến bộ trên các LLM và bộ dữ liệu khác nhau. Đối với mô hình GPT-4, so với baseline Default, ClarifyGPT (Simulated Feedback) chứng minh những cải thiện đáng chú ý trong hiệu suất Pass@1, đạt được mức tăng 11.34% trên bộ dữ liệu HumanEval, 10.35% trên HumanEval-ET, 10.89% trên MBPP-sanitized và 13.49% trên MBPP-ET. Đối với mô hình ChatGPT, khi so sánh với baseline Default, ClarifyGPT (Simulated Feedback) cải thiện hiệu suất Pass@1 lần lượt 15.10%, 13.12%, 12.98% và 19.07% trên bốn bộ chuẩn. Kết quả chứng minh rằng ClarifyGPT, trao quyền cho LLM tự động sinh ra câu hỏi làm rõ và tinh chỉnh yêu cầu người dùng dựa trên phản hồi của người dùng, tạo thuận lợi cho người dùng làm rõ ý định của họ, từ đó nâng cao hiệu suất sinh mã bằng cách nắm bắt ý định người dùng.

Chúng tôi cũng lưu ý rằng, so với baseline liên quan nhất (tức là GPT-Engineer), thể hiện hiệu suất vượt trội với số liệu Pass@1, đạt được cải thiện trung bình

11.45%, 8.65%, 6.95% và 8.56% trên bốn bộ chuẩn. Chúng tôi quy những cải thiện này cho các kỹ thuật mới của chúng tôi, tức là xác định yêu cầu mơ hồ và sinh câu hỏi làm rõ. Việc đặt câu hỏi làm rõ cho mọi yêu cầu người dùng dẫn đến các tương tác LLM-Con người không cần thiết trên các yêu cầu không mơ hồ, điều này đặt gánh nặng bổ sung lên người dùng và làm tổn hại hiệu suất sinh mã khi tạo ra các câu hỏi ngoài chủ đề. Trong khi ClarifyGPT có thể xác định hiệu quả các yêu cầu mơ hồ mà không cần bất kỳ huấn luyện có giám sát nào bằng cách tiến hành kiểm tra tính nhất quán mã. Các đoạn mã không nhất quán được lấy làm đầu vào để giúp ClarifyGPT xây dựng các câu hỏi có mục tiêu hướng dẫn người dùng làm rõ tính mơ hồ.

Bên cạnh đó, chúng tôi quan sát thấy rằng hiệu suất của ClarifyGPT (Human Feedback) cao hơn một chút so với ClarifyGPT (Simulated Feedback). Điều này cho thấy phương pháp mô phỏng người dùng của chúng tôi có thể sinh ra phản hồi người dùng không thỏa mãn ý định của người dùng. Tuy nhiên, cả hai phương pháp đều có thể cải thiện đáng kể hiệu suất sinh mã và đạt được những tiến bộ nhất quán trên các LLM và bộ chuẩn khác nhau, chứng minh tính tin cậy của kết quả đánh giá phương pháp mô phỏng của chúng tôi.

Trả lời RQ2: ClarifyGPT (Simulated Feedback) cải thiện hiệu suất trung bình (Pass@1) của GPT-4 trên bốn bộ chuẩn từ 68.02% lên 75.75%, cải thiện hiệu suất trung bình của ChatGPT trên bốn bộ chuẩn từ 58.55% lên 67.22%. Cải thiện tương đối của chúng lần lượt là 11.52% và 15.07%, và cải thiện trung bình là 13.27%.

5.3 RQ3: Số lượng minh chứng trong một prompt ảnh hưởng như thế nào đến hiệu suất của ClarifyGPT?

Thiết lập. Trong RQ này, chúng tôi điều tra xem việc tăng hoặc giảm số lượng minh chứng có ảnh hưởng đến hiệu suất của ClarifyGPT (Simulated Feedback) trong nhiệm vụ sinh mã hay không. Cụ thể, do giới hạn độ dài đầu vào của LLM, chúng tôi thay đổi số lượng minh chứng trong prompt từ không đến ba. Sau đó, chúng tôi áp dụng hai LLM vào ClarifyGPT và các biến thể của nó, và đánh giá hiệu suất của chúng trên bốn bộ chuẩn. Chúng tôi chạy những phương pháp này ba lần và báo cáo kết quả Pass@1 trung bình làm báo cáo cuối cùng.

Kết quả. Bảng 4 trình bày so sánh hiệu suất giữa ClarifyGPT và các biến thể của nó. Tổng thể, ClarifyGPT chứng minh tính mạnh mẽ với số lượng minh chứng trong các prompt. Khi thay đổi số lượng minh chứng từ không đến ba, ClarifyGPT liên tục vượt trội hơn baseline Default trên hai LLM và bốn bộ chuẩn.

Chúng ta có thể quan sát thấy rằng, như mong đợi, hiệu suất của ClarifyGPT tăng theo số lượng minh chứng. Cụ thể, khi số lượng minh chứng trong prompt được tăng từ không đến ba, đối với ChatGPT, ClarifyGPT đạt được mức tăng hiệu suất trung bình từ 59.77% lên 67.22% trên bốn bộ chuẩn. Đối với mô hình GPT-4, hiệu suất trung bình của ClarifyGPT tăng từ 68.59% lên 75.75%. Điều này chủ yếu là do nhiều minh chứng hơn có thể cung cấp nhiều tình huống và thông tin khác nhau cho LLM, cho phép chúng hiểu rõ hơn ngữ cảnh của vấn đề và giải pháp cần thiết. Hơn nữa, LLM có thể học cách tổng quát hóa tốt hơn thông qua các minh chứng, tức là suy ra giải pháp cho tình huống mới từ minh chứng đã biết. Điều này cho phép LLM thích nghi tốt hơn với các đầu vào và yêu cầu khác nhau.

Chúng tôi cũng thấy rằng hiệu suất của ClarifyGPT trong thiết lập zero-shot có cải thiện nhẹ so với baseline Default, trong khi hiệu suất của nó trong thiết lập one-shot được tăng cường đáng kể so với baseline Default. Chúng tôi quy sự khác biệt này cho thực tế là trong thiết lập zero-shot, ClarifyGPT được mong đợi sinh ra phản hồi có ý nghĩa mà không có bất kỳ minh chứng nào, điều này có thể đặc biệt thách thức đối với các nhiệm vụ phức tạp (ví dụ, yêu cầu LLM sinh ra các câu hỏi làm rõ có mục tiêu). Hơn nữa, prompting zero-shot chỉ dựa vào kiến thức được huấn luyện trước của LLM và cách diễn đạt của các prompt được đưa ra, có thể không cung cấp đủ hướng dẫn hoặc ràng buộc để LLM tạo ra phản hồi chính xác hoặc phù hợp với ngữ cảnh. Ngược lại, hiệu suất của ClarifyGPT với thiết lập one-shot cao hơn đáng kể so với thiết lập zero-shot và gần với hiệu suất của ClarifyGPT với thiết lập three-shot. Điều này chỉ ra rằng ClarifyGPT có hiệu suất tổng quát hóa mạnh khi chỉ cung cấp một minh chứng. Chúng tôi tin rằng trong các tình huống sử dụng thực tế, việc sử dụng ClarifyGPT trong thiết lập one-shot có thể phục vụ như một sự đánh đổi giữa hiệu quả và hiệu suất.

Trả lời RQ3: Tổng thể, ClarifyGPT chứng minh tính mạnh mẽ với số lượng minh chứng trong các prompt. Khi thay đổi số lượng minh chứng từ không đến ba, ClarifyGPT liên tục vượt trội hơn baseline Default trên hai LLM và bốn bộ chuẩn.

6 THẢO LUẬN

6.1 Nghiên cứu trường hợp

Để đánh giá thêm hiệu quả của phương pháp chúng tôi, chúng tôi tiến hành phân tích định tính. Như được hiển thị trong Hình 4, chúng tôi chọn hai ví dụ đại diện từ hai bộ chuẩn sinh mã phổ biến

(tức là HumanEval và MBPP). Mỗi yêu cầu đầu vào bao gồm một chữ ký hàm và một mô tả NL. Chúng tôi lấy ChatGPT[34] làm mô hình cơ sở và sử dụng hai baseline (tức là Default và GPT-Engineer) và ClarifyGPT để sinh ra giải pháp mã cho mỗi yêu cầu đầu vào.

Đối với ví dụ đầu tiên được lấy từ MBPP-sa, mô tả "write a function to sort a list of elements" không chỉ định liệu hàm này nên được sắp xếp theo thứ tự tăng dần hay giảm dần. ChatGPT mặc định trực tiếp sinh ra giải pháp mã sắp xếp danh sách đã cho theo thứ tự giảm dần, không vượt qua được các test case thực tế. GPT-Engineer đặt năm câu hỏi làm rõ và sinh ra giải pháp mã chính xác dựa trên phản hồi của người dùng. Tuy nhiên, một số trong những câu hỏi đó không cung cấp thông tin và có thể được trả lời bằng thông tin trong yêu cầu đã cho. Ví dụ, câu trả lời cho câu hỏi thứ ba "Are there any specific constraints for the sorting algorithm?" có thể được suy ra từ tên hàm comb_sort được đề cập trong chữ ký hàm. Câu hỏi thứ năm "Are there any preferred programming language?" cũng có vẻ tầm thường, vì chúng ta có thể dễ dàng biết rằng hàm nên được triển khai bằng Python dựa trên cú pháp của chữ ký hàm. Việc trả lời những câu hỏi này không thể thu được thông tin bổ sung; thay vào đó, nó tạo ra các đối thoại thừa có hại cho trải nghiệm người dùng. Hơn nữa, nó dẫn đến sự gia tăng số lượng token cho cả đầu vào và đầu ra của LLM, do đó làm tăng chi phí hoạt động. Ngược lại, ClarifyGPT có thể xác định các điểm mơ hồ trong yêu cầu bằng cách so sánh các triển khai mã khác nhau, từ đó đặt câu hỏi làm rõ có mục tiêu. Kết quả là, ClarifyGPT chỉ đặt một câu hỏi "Should the sorting be in ascending or descending order?" và sinh ra giải pháp mã chính xác.

Đối với ví dụ thứ hai, yêu cầu người dùng được định nghĩa rõ ràng. ChatGPT mặc định sinh ra giải pháp chính xác, trong khi GPT-Engineer tạo ra giải pháp mã không chính xác. Sự khác biệt này chủ yếu phát sinh từ việc GPT-Engineer không thể xác định xem một yêu cầu có mơ hồ hay không. Do đó, ngay cả đối với yêu cầu không mơ hồ này, GPT-Engineer vẫn đặt ba câu hỏi, hóa ra là không cung cấp thông tin. Hơn nữa, những câu hỏi này góp phần làm cho prompt đã tinh chỉnh trở nên quá dài, có thể gây nhầm lẫn cho LLM. Ngược lại, ClarifyGPT của chúng tôi có thể xác định xem một yêu cầu có cần làm rõ hay không bằng cách tiến hành kiểm tra tính nhất quán mã. Vì vậy chúng ta có thể thấy ClarifyGPT không đặt bất kỳ câu hỏi nào cho yêu cầu này mà thay vào đó trực tiếp tạo ra giải pháp chính xác.

6.2 Lợi ích và hạn chế

Trong phần này, chúng tôi thảo luận về một số lợi ích và hạn chế tiềm năng của ClarifyGPT của chúng tôi.

Lợi ích. (1) Trái ngược với các phương pháp sinh mã dựa trên LLM phổ biến[7,23,27] tận dụng các kỹ thuật hậu xử lý để lấy mẫu một pool lớn các mã ứng viên và sau đó chọn một, ClarifyGPT nhằm trực tiếp làm rõ yêu cầu đầu vào bằng cách đặt câu hỏi làm rõ. Do đó, framework của chúng tôi góp phần vào việc tăng cường khả năng diễn giải trong mã được sinh ra bởi LLM. Bằng cách làm rõ các chi tiết cụ thể trong yêu cầu hoặc thêm kiến thức bổ sung vào chúng, người dùng có thể dễ dàng nhận biết các thay đổi tương ứng trong mã kết quả. Điều này góp phần cung cấp cho người dùng hướng dẫn về cách xây dựng yêu cầu để cải thiện việc sinh mã, từ đó tạo thuận lợi cho việc hiểu rõ hơn về mã được sinh ra. (2) ClarifyGPT của chúng tôi cải thiện kỹ năng tương tác của LLM bằng cách trao quyền cho chúng khả năng tự động đặt câu hỏi làm rõ cho các yêu cầu mơ hồ. Theo cách này, nó phục vụ để tạo thuận lợi cho người dùng trong việc xác định các mơ hồ trong yêu cầu và cung cấp hướng dẫn trong việc làm rõ ý định của họ mà không yêu cầu người dùng ban đầu sinh mã và sau đó đọc và phân tích mã để tinh chỉnh yêu cầu. Do đó, ClarifyGPT nâng cao trải nghiệm người dùng và hiệu suất sản xuất.

Hạn chế. (1) Lý tưởng, framework của chúng tôi có thể áp dụng cho tất cả LLM. Tuy nhiên, ClarifyGPT đòi hỏi các LLM phải có một mức độ năng lực giao tiếp nhất định, tức là khả năng hiểu hướng dẫn của con người và xây dựng câu hỏi làm rõ. Do đó, các LLM có thể áp dụng cho framework của chúng tôi bị hạn chế, tức là các LLM không có instruction tuning (ví dụ như InCoder[13] và CodeGen[33]) không phù hợp làm mô hình cơ sở được áp dụng cho framework ClarifyGPT. (2) Do sử dụng kiểm tra tính nhất quán mã để xác định xem một yêu cầu có cần làm rõ hay không, ClarifyGPT được yêu cầu sinh đầu vào thử nghiệm cho yêu cầu và so sánh đầu ra thử nghiệm của các giải pháp được lấy mẫu. Do đó, ClarifyGPT không phù hợp để sinh mã với đầu vào phức tạp (ví dụ như hình ảnh hoặc tệp). Ngoài ra, đối với một số mã không trả về giá trị đầu ra (ví dụ như các chương trình deep learning), việc sử dụng ClarifyGPT cũng có thể gặp một số hạn chế.

6.3 Các mối đe dọa đối với tính hợp lệ

Mối đe dọa đầu tiên đối với tính hợp lệ là tiềm năng rò rỉ dữ liệu. Vì những LLM này được huấn luyện trên các kho mã nguồn mở, có thể một số bộ chuẩn công khai đã được bao gồm trong dữ liệu huấn luyện của chúng. Điều này có thể làm thiên lệch đánh giá của chúng tôi về phương pháp được đề xuất, vì một số đầu ra mô hình có thể bị ảnh hưởng bởi việc tiếp xúc trước với các bộ chuẩn này. Để giảm thiểu mối đe dọa này, chúng tôi cẩn thận chọn HumanEval[8], MBPP-sanitized[4] và các phiên bản mở rộng tương ứng của chúng cho đánh giá. HumanEval là một bộ dữ liệu giải quyết vấn đề được tạo thủ công, được giới thiệu bởi OpenAI để đánh giá hiệu suất của Codex. MBPP-sanitized, mặt khác, là một tập con được xác minh thủ công của bộ dữ liệu MBPP, bao gồm 427 bài toán Python đã trải qua xác minh từ cộng đồng. Những bộ dữ liệu này đã trải qua xem xét thủ công tỉ mỉ và đã được sử dụng rộng rãi trong các nghiên cứu trước đây [7, 23, 46].

Mối đe dọa thứ hai đối với tính hợp lệ là mô phỏng người dùng để đánh giá. Do sự tham gia của người tham gia, việc đánh giá ClarifyGPT, một framework sinh mã tương tác, rất tốn kém và khó tái tạo. Do đó, chúng tôi đề xuất một phương pháp mô phỏng người dùng để tạo thuận lợi cho việc đánh giá tự động của ClarifyGPT trên các LLM và bộ chuẩn khác nhau. Tuy nhiên, các mô phỏng độ tin cậy thấp có thể dẫn đến ClarifyGPT nhận phản hồi khó gặp trong thực tế, từ đó cho ra kết quả sai lệch và ảnh hưởng đến đánh giá hiệu suất của ClarifyGPT. Để giảm thiểu mối đe dọa này, chúng tôi thiết kế một prompt đặc biệt để cung cấp cho LLM các câu hỏi làm rõ và test case thực tế. Bằng cách trao cho LLM kiến thức tiên nghiệm này, ClarifyGPT tạo thuận lợi cho việc hiểu ý định người dùng của LLM và cho phép sinh ra phản hồi người dùng mô phỏng độ tin cậy cao. Kết quả cho thấy hiệu suất của ClarifyGPT (Simulated Feedback) rất gần với ClarifyGPT (Human Feedback), chứng minh rằng phương pháp mô phỏng được đề xuất của chúng tôi có thể phục vụ như một proxy tốt cho việc đánh giá tự động của ClarifyGPT, loại bỏ nhu cầu sự tham gia trực tiếp của người dùng.

Mối đe dọa thứ ba liên quan đến tính tổng quát hóa của kết quả thí nghiệm của chúng tôi. Để giải quyết vấn đề này, một mặt, chúng tôi đã cẩn thận chọn hai chat LLM đại diện (ChatGPT và GPT-4) làm mô hình cơ sở và bốn bộ dữ liệu phổ biến làm đối tượng đánh giá. Chúng tôi áp dụng hai LLM vào ClarifyGPT của chúng tôi và đánh giá hiệu suất của chúng trên bốn bộ dữ liệu này. Mặt khác, xem xét tính nhạy cảm vốn có của LLM đối với prompt, chúng tôi chạy baseline và ClarifyGPT ba lần để giúp giảm thiểu phương sai cao và tính ngẫu nhiên. Chúng tôi báo cáo kết quả trung bình làm kết quả cuối cùng. Kết quả cho thấy ClarifyGPT của chúng tôi có thể cải thiện đáng kể hiệu suất của tất cả LLM, đạt được những tiến bộ nhất quán trên các bộ dữ liệu khác nhau. Do đó, chúng tôi tin rằng ClarifyGPT có tính tổng quát hóa tốt và có thể hoạt động hiệu quả trong nhiều bối cảnh liên quan.

7 KẾT LUẬN

Trong bài báo này, được thúc đẩy bởi quan sát rằng các nhà phát triển con người thường đặt câu hỏi làm rõ khi họ đối mặt với các yêu cầu mơ hồ, chúng tôi cho rằng việc trao quyền cho LLM khả năng tự động làm rõ các yêu cầu mơ hồ có thể cải thiện việc sinh mã. Để đạt được mục tiêu này, chúng tôi đề xuất ClarifyGPT, một framework sinh mã cho phép LLM xác định các yêu cầu mơ hồ và sinh ra các câu hỏi làm rõ có mục tiêu. Cụ thể, ClarifyGPT bao gồm bốn giai đoạn chính, tức là sinh đầu vào thử nghiệm, kiểm tra tính nhất quán mã, sinh câu hỏi dựa trên lý luận và sinh mã nâng cao. Đối với một yêu cầu đã cho, ClarifyGPT đầu tiên sinh ra các đầu vào thử nghiệm chất lượng cao bằng cách sử dụng các kỹ thuật prompting và biến đổi heuristic. Sau đó, nó sử dụng các đầu vào thử nghiệm đã sinh ra để tiến hành đánh giá tính nhất quán và xác định các yêu cầu mơ hồ. Tiếp theo, ClarifyGPT xây dựng các câu hỏi làm rõ có mục tiêu cho các yêu cầu mơ hồ đã xác định bằng cách prompting LLM tham gia vào lý luận trung gian. Cuối cùng, nó kết hợp các câu hỏi làm rõ và phản hồi của chúng để tinh chỉnh yêu cầu ban đầu và sinh ra giải pháp mã cuối cùng dựa trên prompt đã tinh chỉnh. Trong phần đánh giá, chúng tôi đầu tiên áp dụng GPT-4 vào ClarifyGPT và tuyển mười người tham gia để đánh giá hiệu suất của nó trên hai bộ chuẩn công khai. Kết quả đánh giá con người cho thấy ClarifyGPT đạt được cải thiện tương đối lên đến 16.83% trong Pass@1 so với baseline Default. Ngoài ra, để tự động hóa việc đánh giá ClarifyGPT, chúng tôi giới thiệu một phương pháp mô phỏng độ tin cậy cao để mô phỏng phản hồi của người dùng. Chúng tôi tiến hành các thí nghiệm toàn diện trên bốn bộ chuẩn (tức là HumanEval, HumanEval-ET, MBPP-sanitized và MBPP-ET) sử dụng hai LLM (tức là GPT-4 và ChatGPT). Kết quả mở rộng minh họa rằng ClarifyGPT cải thiện hiệu suất trung bình của GPT-4 trên bốn bộ chuẩn từ 68.02% lên 75.75%, và cải thiện hiệu suất trung bình của ChatGPT trên bốn bộ chuẩn từ 58.55% lên 67.22%. Do đó, chúng tôi tin rằng ClarifyGPT có thể tạo thuận lợi đáng kể cho việc ứng dụng thực tế của LLM trong môi trường phát triển thế giới thực.

TÀI LIỆU THAM KHẢO

[1] 2023. website. https://github.com/ClarifyGPT/ClarifyGPT.
[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. 2021. Unified Pre-training for Program Understanding and Generation. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. 2655–2668. https://doi.org/10.18653/v1/2021.naacl-main.211
[3] Mohammad Aliannejadi, Hamed Zamani, Fabio Crestani, và W Bruce Croft. 2019. Asking clarifying questions in open-domain information-seeking conversations. Trong Proceedings of the 42nd international acm sigir conference on research and development in information retrieval. 475–484.
[4] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, và Charles Sutton. 2021. Program Synthesis with Large Language Models. CoRR abs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732
[5] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, và Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR abs/2204.06745 (2022). https://doi.org/10.48550/arXiv.2204.06745 arXiv:2204.06745
[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).
[7] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, và Weizhu Chen. 2022. CodeT: Code Generation with Generated Tests. CoRR abs/2207.10397 (2022). https://doi.org/10.48550/arXiv.2207.10397 arXiv:2207.10397
[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374 https://arxiv.org/abs/2107.03374
[9] Kaustubh D. Dhole. 2020. Resolving Intent Ambiguities by Retrieving Discriminative Clarifying Questions. CoRR abs/2008.07559 (2020). arXiv:2008.07559 https://arxiv.org/abs/2008.07559

[10] Yihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li, Ge Li, và Zhi Jin. 2023. CodeScore: Evaluating Code Generation by Learning Code Execution. CoRR abs/2301.09043 (2023). https://doi.org/10.48550/arXiv.2301.09043 arXiv:2301.09043
[11] Yihong Dong, Xue Jiang, Zhi Jin, và Ge Li. 2023. Self-collaboration Code Generation via ChatGPT. CoRR abs/2304.07590 (2023). https://doi.org/10.48550/arXiv.2304.07590 arXiv:2304.07590
[12] Zachary Eberhart và Collin McMillan. 2022. Generating Clarifying Questions for Query Refinement in Source Code Search. Trong IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2022, Honolulu, HI, USA, March 15-18, 2022. IEEE, 140–151. https://doi.org/10.1109/SANER53432.2022.00028
[13] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, và Mike Lewis. 2022. InCoder: A Generative Model for Code Infilling and Synthesis. CoRR abs/2204.05999 (2022). https://doi.org/10.48550/arXiv.2204.05999 arXiv:2204.05999
[14] Shuzheng Gao, Xin-Cheng Wen, Cuiyun Gao, Wenxuan Wang, và Michael R Lyu. 2023. Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study. arXiv preprint arXiv:2304.07575 (2023).
[15] Michael D Gordon. 1990. Evaluating the effectiveness of information retrieval systems using simulated queries. Journal of the American Society for Information Science 41, 5 (1990), 313–323.
[16] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, và Ge Li. 2023. Self-planning code generation with large language model. arXiv preprint arXiv:2303.06689 (2023).
[17] Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, và Ge Li. 2023. Self-planning Code Generation with Large Language Model. CoRR abs/2303.06689 (2023). https://doi.org/10.48550/arXiv.2303.06689 arXiv:2303.06689
[18] Kimiya Keyvan và Jimmy Xiangji Huang. 2022. How to approach ambiguous queries in conversational search: A survey of techniques, approaches, tools, and challenges. Comput. Surveys 55, 6 (2022), 1–40.
[19] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners. CoRR abs/2205.11916 (2022). https://doi.org/10.48550/arXiv.2205.11916 arXiv:2205.11916
[20] Dmitrii Krasheninnikov, Egor Krasheninnikov, và David Krueger. 2022. Assistance with large language models. Trong NeurIPS ML Safety Workshop.
[21] Lorenz Kuhn, Yarin Gal, và Sebastian Farquhar. 2023. CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models. (2023).
[22] Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, và Percy Liang. 2019. SPoC: Search-based Pseudocode to Code. Trong Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada. 11883–11894. https://proceedings.neurips.cc/paper/2019/hash/7298332f04ac004a0ca44cc69ecf6f6b-Abstract.html
[23] Shuvendu K. Lahiri, Aaditya Naik, Georgios Sakkas, Piali Choudhury, Curtis von Veh, Madanlal Musuvathi, Jeevana Priya Inala, Chenglong Wang, và Jianfeng Gao. 2022. Interactive Code Generation via Test-Driven User-Intent Formalization. CoRR abs/2208.05950 (2022). https://doi.org/10.48550/arXiv.2208.05950 arXiv:2208.05950
[24] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, và Siddhartha Sen. 2023. CODAMOSA: Escaping coverage plateaus in test generation with pre-trained large language models. Trong International conference on software engineering (ICSE).
[25] Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, và Iryna Gurevych. 2023. Python Code Generation by Asking Clarification Questions. Trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023. Association for Computational Linguistics, 14287–14306. https://doi.org/10.18653/v1/2023.acl-long.799
[26] Jia Li, Ge Li, Yongmin Li, và Zhi Jin. 2023. Enabling Programming Thinking in Large Language Models Toward Code Generation. arXiv preprint arXiv:2305.06599 (2023).
[27] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, và Oriol Vinyals. 2022. Competition-Level Code Generation with AlphaCode. CoRR abs/2203.07814 (2022). https://doi.org/10.48550/arXiv.2203.07814 arXiv:2203.07814
[28] Chao Liu, Xuanlin Bao, Hongyu Zhang, Neng Zhang, Haibo Hu, Xiaohong Zhang, và Meng Yan. 2023. Improving ChatGPT Prompt for Code Generation. arXiv preprint arXiv:2305.08360 (2023).
[29] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, và Lingming Zhang. 2023. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. CoRR abs/2305.01210 (2023). https://doi.org/10.48550/arXiv.2305.01210 arXiv:2305.01210
[30] Sewon Min, Julian Michael, Hannaneh Hajishirzi, và Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. arXiv preprint arXiv:2004.10645 (2020).
[31] Noor Nashid, Mifta Sintaha, và Ali Mesbah. 2023. Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. Trong Proceedings of the 45th International Conference on Software Engineering (ICSE'23).

[32] Feng Nie, Meixi Chen, Zhirui Zhang, và Xu Cheng. 2022. Improving few-shot performance of language models via nearest neighbor calibration. arXiv preprint arXiv:2212.02216 (2022).
[33] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022).
[34] OpenAI. 2022. ChatGPT. https://openai.com/blog/chatgpt/.
[35] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). https://doi.org/10.48550/arXiv.2303.08774 arXiv:2303.08774
[36] Anton Osika. 2023. GPT-Engineer. https://github.com/AntonOsika/gpt-engineer/.
[37] Sudha Rao và Hal Daumé III. 2019. Answer-based Adversarial Training for Generating Clarification Questions. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 143–155. https://doi.org/10.18653/v1/n19-1013
[38] Max Schäfer, Sarah Nadi, Aryaz Eghbali, và Frank Tip. 2023. Adaptive test generation using a large language model. arXiv preprint arXiv:2302.06527 (2023).
[39] Ivan Sekulić, Mohammad Aliannejadi, và Fabio Crestani. 2022. Evaluating mixed-initiative conversational search systems via user simulation. Trong Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. 888–896.
[40] Disha Shrivastava, Hugo Larochelle, và Daniel Tarlow. 2022. Repository-Level Prompt Generation for Large Language Models of Code. CoRR abs/2206.12839 (2022). https://doi.org/10.48550/arXiv.2206.12839 arXiv:2206.12839
[41] Jan Trienes và Krisztian Balog. 2019. Identifying unclear questions in community question answering websites. Trong Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14–18, 2019, Proceedings, Part I 41. Springer, 276–289.
[42] Vasudev Vikram, Caroline Lemieux, và Rohan Padhye. 2023. Can Large Language Models Write Good Property-Based Tests? arXiv preprint arXiv:2307.04346 (2023).
[43] Jian Wang và Wenjie Li. 2021. Template-guided Clarifying Question Generation for Web Search Clarification. Trong CIKM '21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021. ACM, 3468–3472. https://doi.org/10.1145/3459637.3482199
[44] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, và Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).
[45] Yue Wang, Weishi Wang, Shafiq R. Joty, và Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021. Association for Computational Linguistics, 8696–8708. https://doi.org/10.18653/v1/2021.emnlp-main.685
[46] Zhiruo Wang, Shuyan Zhou, Daniel Fried, và Graham Neubig. 2022. Execution-Based Evaluation for Open-Domain Code Generation. CoRR abs/2212.10481 (2022). https://doi.org/10.48550/arXiv.2212.10481 arXiv:2212.10481
[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, và Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. CoRR abs/2201.11903 (2022). arXiv:2201.11903 https://arxiv.org/abs/2201.11903
[48] Frank F. Xu, Uri Alon, Graham Neubig, và Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. Trong MAPS@PLDI 2022: 6th ACM SIGPLAN International Symposium on Machine Programming, San Diego, CA, USA, 13 June 2022. ACM, 1–10. https://doi.org/10.1145/3520312.3534862
[49] Michal Zalewski. 2018. American fuzzing lop. https://lcamtuf.coredump.cx/afl/.
[50] Lingming Zhang, Darko Marinov, Lu Zhang, và Sarfraz Khurshid. 2011. An Empirical Study of JUnit Test-Suite Reduction. Trong IEEE 22nd International Symposium on Software Reliability Engineering, ISSRE 2011, Hiroshima, Japan, November 29 - December 2, 2011, Tadashi Dohi và Bojan Cukic (Eds.). IEEE Computer Society, 170–179. https://doi.org/10.1109/ISSRE.2011.26
[51] Tianyi Zhang, Tao Yu, Tatsunori B. Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, và Sida I. Wang. 2022. Coder Reviewer Reranking for Code Generation. CoRR abs/2211.16490 (2022). https://doi.org/10.48550/arXiv.2211.16490 arXiv:2211.16490
