# 2211.12821.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: ./2211.12821.pdf
# Kích thước file: 451648 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Giải thích các Mô hình Mã nguồn dựa trên Transformer:
Chúng Học được Gì? Khi nào Chúng Không Hoạt động?
1stAhmad Haji Mohammadkhani
University of Calgary
Calgary, Canada
ahmad.hajimohammadkh@ucalgary.ca2ndChakkrit Tantithamthavorn
Monash University
Melbourne, Australia
Chakkrit@monash.edu3rdHadi Hemmatif
York University
Toronto, Canada
hemmati@yorku.ca

Tóm tắt —Trong những năm gần đây, đã có sự quan tâm rộng rãi đến việc thiết kế các mô hình dựa trên mạng nơ-ron sâu để tự động hóa các tác vụ kỹ thuật phần mềm hạ nguồn trên mã nguồn, như tạo tài liệu mã, tìm kiếm mã và sửa chữa chương trình. Mặc dù mục tiêu chính của các nghiên cứu này là cải thiện hiệu quả của tác vụ hạ nguồn, nhiều nghiên cứu chỉ cố gắng sử dụng mô hình mạng nơ-ron tốt nhất tiếp theo, mà không có phân tích sâu thích hợp về lý do tại sao một giải pháp cụ thể hoạt động hoặc không hoạt động, trên các tác vụ hoặc kịch bản cụ thể. Trong bài báo này, sử dụng một phương pháp AI có thể giải thích (XAI) ví dụ (cơ chế chú ý), chúng tôi nghiên cứu hai mô hình ngôn ngữ lớn (LLM) gần đây cho mã (CodeBERT và GraphCodeBERT) trên một tập hợp các tác vụ kỹ thuật phần mềm hạ nguồn: tạo tài liệu mã (CDG), tinh chỉnh mã (CR) và dịch mã (CT). Thông qua các nghiên cứu định lượng và định tính, chúng tôi xác định những gì CodeBERT và GraphCodeBERT học được (tập trung chú ý cao nhất vào, theo loại token mã nguồn), trên các tác vụ này. Chúng tôi cũng chỉ ra một số mẫu phổ biến khi mô hình không hoạt động như mong đợi (hoạt động kém ngay cả trên những vấn đề dễ) và đề xuất các khuyến nghị có thể làm giảm bớt những thách thức đã quan sát được.

Từ khóa chỉ mục —AI có thể giải thích (XAI), LLM, Mô hình Mã, Có thể diễn giải, Chú ý, Transformer.

I. GIỚI THIỆU

Các Mô hình Ngôn ngữ Lớn (LLM) cho mã (tóm tắt: mô hình mã) được đề xuất để phân tích các kho dữ liệu lớn về mã nguồn và ngôn ngữ tự nhiên được thu thập từ các nền tảng mã nguồn mở (ví dụ: GitHub và StackOverflow). Các mô hình mã được đào tạo trước như vậy đã được sử dụng để tự động hóa các tác vụ liên quan đến mã nguồn khác nhau, ví dụ: hiểu mã, tạo mã, phát hiện bản sao mã [1], phát hiện lỗi [2], [3], và tóm tắt mã [4]. Việc tự động hóa các tác vụ kỹ thuật phần mềm như vậy đã được chứng minh là cải thiện đáng kể năng suất của các nhà phát triển phần mềm và giảm chi phí phát triển phần mềm.

Các nghiên cứu gần đây đã đề xuất các mô hình mã dựa trên Transformer, ví dụ: CodeBERT [5], GraphCodeBERT [6], CodeGPT [7], CodeT5 [8]. Tuy nhiên, hầu hết các nghiên cứu này thường tập trung vào việc cải thiện độ chính xác của nó—mà không xem xét khía cạnh khả năng giải thích của nó. Do đó, khi triển khai các mô hình như vậy trong thực tế, các nhà thực hành vẫn không biết tại sao các mô hình như vậy lại cung cấp một khuyến nghị hoặc đề xuất nhất định.

Hãy xem xét một đoạn mã Python nhất định của thuật toán sắp xếp nổi bọt. Một mô hình tóm tắt mã có thể có thể tóm tắt chính xác rằng đoạn mã nhất định là một thuật toán sắp xếp nổi bọt. Tuy nhiên, các nhà phát triển có thể không tin tưởng vào các mô hình nếu các mô hình tạo ra văn bản tự nhiên chính xác dựa trên thụt lề, khoảng trắng, hoặc dấu ngoặc của đoạn mã Python, thay vì thông tin ngữ nghĩa có ý nghĩa (tức là sắp xếp nổi bọt). Do đó, các dự đoán chính xác được tạo ra bởi các mô hình không đảm bảo rằng các mô hình được học một cách chính xác.

Vì vậy, sự thiếu khả năng giải thích của các mô hình mã lớn và phức tạp có thể dẫn đến thiếu sự áp dụng trong thực tế.

Trong bài báo này, chúng tôi tiến hành một nghiên cứu thực nghiệm để phân tích các mô hình mã thông qua lăng kính của AI có thể giải thích. Đặc biệt, chúng tôi tập trung vào hai mô hình mã nổi tiếng, tức là CodeBERT và GraphCodeBERT với ba tác vụ hạ nguồn cụ thể về hiểu và tạo, tức là Tóm tắt Mã (Mã →Văn bản), Chuyển đổi Mã (Mã →Mã), và Dịch Mã (Mã →Mã). Để giải thích các dự đoán của các mô hình này, chúng tôi tận dụng cơ chế chú ý bên trong kiến trúc Transformer, đây là một phương pháp AI có thể giải thích nội tại. Cơ chế chú ý cho phép chúng tôi hiểu những token nào quan trọng nhất trong chuỗi đầu vào góp phần nhiều nhất vào các token trong chuỗi đầu ra. Cụ thể, chúng tôi nhằm giải quyết hai câu hỏi nghiên cứu sau:

Các mô hình mã được đào tạo trước học được gì?
Kết quả: Phân tích điểm số chú ý và phân phối của chúng trên các loại token khác nhau cho thấy các mô hình học cách tập trung vào các loại token cụ thể cho mỗi tác vụ hạ nguồn. Trong CDG, các mô hình học cách tập trung vào chữ ký phương thức (tức là tên phương thức và tham số đầu vào). Trong khi đó trong CT, cú pháp, tức là các token liên quan đến ngôn ngữ lập trình, thu hút nhiều chú ý hơn. CR có một nền tảng trung gian so với hai tác vụ kia và có phân phối chú ý cân bằng hơn. Ngoài ra, được chỉ ra rằng GraphCodeBERT chú ý nhiều hơn đến các phần cấu trúc của mã nguồn, thay vì CodeBERT, điều này có thể là kết quả của bước bổ sung của GraphCodeBERT để phân tích mã và tận dụng luồng dữ liệu của mã. Những quan sát này phù hợp với những gì được mong đợi từ một mô hình mã, điều này khiến chúng tôi kết luận rằng hai mô hình đã nghiên cứu thực sự đang học (trong hầu hết trường hợp) những gì chúng được cho là phải học.

Khi nào các mô hình mã được đào tạo trước không hoạt động?
Kết quả. Phát hiện của chúng tôi cho thấy có những tình huống nhất định khiến các mô hình hoạt động kém trên các tác vụ khác nhau. Ví dụ, các mô hình không hoạt động tốt với các mẫu có mã nguồn dài hoặc phức tạp và/hoặc câu trả lời dài mong đợi (đầu ra của mô hình). Chúng tôi cũng cho thấy rằng hiệu suất kém của mô hình thường được phản ánh trong phân phối chú ý của nó.

--- TRANG 2 ---

Nói cách khác, bất cứ khi nào mô hình không thể đạt được đầu ra tốt cho một mô hình, nó cũng đã thất bại trong việc chú ý đủ đến các loại token tương ứng cho tác vụ hạ nguồn tương ứng. Chúng tôi cũng đã cung cấp một số khuyến nghị về cách có thể giảm thiểu những điểm yếu này trong bài báo.

Những phát hiện này khiến chúng tôi kết luận rằng mặc dù các mô hình được đào tạo trước đã cho thấy kết quả tuyệt vời trên các tác vụ kỹ thuật phần mềm; không có mô hình nào trong số chúng có thể được coi là một vấn đề đã đóng và có những khía cạnh nhất định của các mô hình này cần tập trung nhiều hơn thông qua các nghiên cứu tiếp theo. Việc giải thích các mô hình này có thể làm sáng tỏ những điểm yếu của chúng và cung cấp hướng dẫn cho nghiên cứu tương lai.

Khoa học Mở. Để thúc đẩy sáng kiến khoa học mở, chúng tôi đã công khai gói tái tạo tại GitHub.¹

II. BỐI CẢNH & CÔNG TRÌNH LIÊN QUAN

Khả năng giải thích hiện đang trở thành một mối quan tâm quan trọng trong kỹ thuật phần mềm. Nhiều nhà nghiên cứu thường sử dụng các kỹ thuật AI/ML để dự đoán lỗi, phát hiện phần mềm độc hại và ước tính nỗ lực. Mặc dù các kỹ thuật AI/ML này có thể cải thiện đáng kể năng suất của nhà phát triển, chất lượng phần mềm và trải nghiệm người dùng cuối, các nhà thực hành vẫn không hiểu tại sao các mô hình AI/ML như vậy lại đưa ra những dự đoán đó [9], [10], [11], [12], [13], [2]. Để giải quyết thách thức này, các nhà nghiên cứu đề xuất các phương pháp khác nhau để tạo ra các giải thích ở hai cấp độ:

(1) Các giải thích toàn cục có thể được tạo ra bằng cách sử dụng các kỹ thuật học máy có thể diễn giải (ví dụ: cây quyết định, quy tắc quyết định và kỹ thuật hồi quy logistic) hoặc các kỹ thuật nội tại cụ thể của mô hình (ví dụ: ANOVA, tầm quan trọng của biến) để toàn bộ quá trình dự đoán và khuyến nghị trở nên minh bạch và có thể hiểu được. Tuy nhiên, các kỹ thuật nội tại cụ thể của mô hình như vậy nhằm cung cấp khả năng giải thích toàn cục, mà không cung cấp giải thích cho các dự đoán riêng lẻ.

(2) Các giải thích cục bộ, mặt khác, có thể được tạo ra bằng cách sử dụng một số kỹ thuật (ví dụ: LIME, SHAP) để giải thích các dự đoán của các mô hình AI/ML hộp đen phức tạp (ví dụ: mạng nơ-ron, rừng ngẫu nhiên). Các kỹ thuật như vậy có thể cung cấp giải thích cho từng dự đoán riêng lẻ (tức là một thể hiện cần được giải thích), cho phép người dùng hiểu rõ hơn tại sao dự đoán được đưa ra.

Trong kỹ thuật phần mềm, AI có thể giải thích gần đây đã được nghiên cứu trong lĩnh vực dự đoán lỗi (tức là một mô hình phân loại để dự đoán liệu một file/class/method có bị lỗi trong tương lai hay không). Cụ thể, nghiên cứu khảo sát của Jiarpakdee và cộng sự [11] phát hiện rằng việc giải thích các dự đoán cũng quan trọng và hữu ích như việc cải thiện độ chính xác của dự đoán lỗi. Tuy nhiên, đánh giá tài liệu của họ phát hiện rằng 91% (81/96) các nghiên cứu dự đoán lỗi chỉ tập trung vào việc cải thiện độ chính xác dự đoán, mà không xem xét việc giải thích các dự đoán, trong khi chỉ 4% trong số 96 nghiên cứu này tập trung vào việc giải thích các dự đoán.

Mặc dù XAI vẫn là một chủ đề được nghiên cứu rất ít trong cộng đồng kỹ thuật phần mềm, rất ít nghiên cứu XAI hiện có đã cho thấy một số cách sử dụng thành công, ví dụ: trong dự đoán lỗi. Trong một ví dụ, Wattanakriengkrai và cộng sự [14] và Pornprasit và Tantithamthavorn [15] đã sử dụng các kỹ thuật bất khả tri mô hình (ví dụ: LIME) để dự đoán lỗi cấp độ dòng (ví dụ: dự đoán dòng nào sẽ bị lỗi trong tương lai), giúp các nhà phát triển định vị các dòng lỗi một cách hiệu quả về chi phí. Trong một ví dụ khác, Jiarpakdee và cộng sự [12] và Khanan và cộng sự [16] đã sử dụng các kỹ thuật bất khả tri mô hình (ví dụ: LIME) để giải thích các mô hình dự đoán lỗi, giúp các nhà phát triển hiểu rõ hơn tại sao một file được dự đoán là có lỗi. Rajapaksha và cộng sự [13] và Pornprasit và cộng sự [15] đã đề xuất các kỹ thuật bất khả tri mô hình dựa trên quy tắc cục bộ để tạo ra hướng dẫn có thể thực hiện được nhằm giúp các nhà quản lý lập ra các kế hoạch cải thiện chất lượng hiệu quả nhất.

A. Khoảng Trống Nghiên Cứu

Trong khi có những nỗ lực nghiên cứu về khả năng giải thích của các tác vụ phân loại trong các lĩnh vực SE (ví dụ: dự đoán lỗi), ít nghiên cứu tập trung vào các mô hình mã được đào tạo trước dựa trên transformer. Đặc biệt, các nhà thực hành thường nêu ra các mối quan tâm, ví dụ: tại sao mã nguồn này được tạo ra? tại sao token mã này được sửa đổi? Sự thiếu khả năng giải thích của các mô hình mã có thể dẫn đến thiếu niềm tin, cản trở việc áp dụng trong thực tế.

Để giải quyết thách thức này, bài báo này nhằm giải quyết các câu hỏi nghiên cứu sau: (RQ1) Các mô hình mã được đào tạo trước học được gì? và (RQ2) Khi nào các mô hình mã được đào tạo trước không hoạt động?

III. THIẾT LẬP THỰC NGHIỆM

A. Lựa chọn Mô hình Mã được Đào tạo Trước

Các mô hình mã được đào tạo trước, như transformers, là các mô hình học sâu được đào tạo trên các tập dữ liệu mở rộng (ví dụ: các dự án GitHub, bài đăng StackOverflow) để hiểu và tạo mã nguồn. Các mô hình này, còn được gọi là mô hình ngôn ngữ của mã, sử dụng các kỹ thuật tự giám sát, bao gồm các kiến trúc dựa trên BERT như Mô hình Ngôn ngữ Có Mặt nạ (MLM) và Dự đoán Câu Tiếp theo (NSP). Việc đào tạo này trên các kho dữ liệu lớn cho phép chúng nắm bắt các biểu diễn phổ quát của cả mã nguồn và ngôn ngữ tự nhiên cụ thể của lập trình. Các mô hình này cung cấp lợi ích có giá trị cho các tác vụ hạ nguồn đa dạng, loại bỏ nhu cầu xây dựng các mô hình mới từ đầu và nâng cao khả năng tái sử dụng. Đáng chú ý, những tiến bộ gần đây đã tạo ra các mô hình mã được đào tạo trước dựa trên Transformer khác nhau (ví dụ: CodeBERT, GraphCodeBERT, CodeGPT, CodeT5). Bài báo này tập trung vào hai mô hình transformer cụ thể: CodeBERT và GraphCodeBERT.

CodeBERT [5] là một mô hình được đào tạo trước đa phương thức đa ngôn ngữ cho ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL). Nó có một mã hóa Transformer đa lớp, được đào tạo trên Mô hình Ngôn ngữ Có Mặt nạ (MLM) và Phát hiện Token Thay thế (RTD) với cả NL và PL làm đầu vào. Mô hình có kiến trúc tương tự như BERT [17] và cho thấy kết quả đầy hứa hẹn trên nhiều tác vụ hạ nguồn như Dịch Mã, Phát hiện Bản sao, Phát hiện Lỗi, v.v. [5]. CodeBERT đã được chọn là một trong các mô hình mã của chúng tôi do tính phổ biến của nó tại thời điểm tiến hành nghiên cứu này và nhiều công trình liên quan

--- TRANG 3 ---

đã đề xuất một kỹ thuật dựa trên nó hoặc sử dụng nó làm đường cơ sở so sánh [18].

GraphCodeBERT [6] là một mô hình được đào tạo trước tương tự như CodeBERT nhưng cũng xem xét cấu trúc cấp độ ngữ nghĩa của mã. Nó sử dụng luồng dữ liệu trong giai đoạn đào tạo trước và sử dụng MLM, cùng với Dự đoán Cạnh và Căn chỉnh Nút làm các tác vụ đào tạo trước. Với tính năng này được bao gồm, mô hình có thể cải thiện kết quả trên các tác vụ chuẩn của nó so với CodeBERT, nhưng như chúng tôi sẽ chỉ ra trong các thí nghiệm của mình, trong các tác vụ như Tạo Tài liệu Mã, nó cho thấy một nhược điểm lớn. GraphCodeBERT đã được chọn là một trong các mô hình mã của chúng tôi vì ít nhất về mặt lý thuyết, nó là một bước tiến so với CodeBERT với thông tin được thêm vào mô hình từ chính mã. Nói cách khác, nó thuộc về một danh mục khác của các mô hình mã, điều này giúp với tính tổng quát của các phát hiện của chúng tôi.

B. Tinh chỉnh Mô hình Mã được Đào tạo Trước trên Ba Tác vụ Hạ nguồn

Các mô hình mã được đào tạo trước hiện có đã được sử dụng cho các tác vụ kỹ thuật phần mềm hạ nguồn khác nhau, có thể được phân loại thành bốn loại: (1) Văn bản →Văn bản (ví dụ: dịch ngôn ngữ của tài liệu mã [7], cải cách truy vấn [19]); (2) Văn bản →Mã (ví dụ: tìm kiếm mã [20], [21]); (3) Mã→Văn bản (ví dụ: tóm tắt mã [22], tạo thông điệp commit [23], [24]); và (4) Mã →Mã (ví dụ: sửa chữa chương trình tự động [25], [26], [27], dịch ngôn ngữ lập trình [28], hoàn thành mã [29]). Trong bài báo này, chúng tôi sẽ tập trung vào ba tác vụ hạ nguồn sau.

Tạo Tài liệu Mã (CDG) hoặc Tóm tắt Mã (Mã →Văn bản) là một tác vụ NLP được thiết kế để tạo ra các bình luận ngôn ngữ tự nhiên cho một mã nguồn nhất định, có thể giúp các nhà phát triển hiểu rõ hơn các mã trong các dự án phần mềm với các bình luận sai hoặc thiếu và giảm thời gian bổ sung cần dành để đọc mã nguồn. Ví dụ, cho một phương thức Python (" def sum(x,y): ... "), mô hình NLP sẽ tạo ra các bình luận ngôn ngữ tự nhiên như ("Đây là một hàm tính tổng").

Tinh chỉnh Mã (Mã→Mã) là một tác vụ NLP được thiết kế để tạo ra mã nguồn được tinh chỉnh (ví dụ: một phiên bản đã sửa) cho một mã nguồn nhất định (ví dụ: một phiên bản có lỗi). Tinh chỉnh mã đã được nghiên cứu rộng rãi trong bối cảnh đánh giá mã [30], [31], [32], giúp các nhà phát triển nhận được mã được tinh chỉnh có khả năng được phê duyệt mà không cần đợi phản hồi từ người đánh giá.

Dịch Mã (Mã→Mã) là một tác vụ NLP được thiết kế để tạo ra mã nguồn trong một ngôn ngữ (ví dụ: Java) cho một mã nguồn nhất định trong một ngôn ngữ khác (ví dụ: C#).

C. Cài đặt Siêu tham số

Trong quá trình đào tạo mô hình, chúng tôi sử dụng các giá trị tham số mặc định như sau: độ dài nguồn tối đa là 256 và độ dài đích tối đa là 128 với tốc độ học 5e-4, với kích thước lô là 16 và đào tạo nó trong 100 epoch.

Cả hai mô hình đều tuân theo các bước tương tự để tạo ra đầu ra. Sau khi đào tạo một mô hình trên một tác vụ hạ nguồn trên tập dữ liệu đào tạo tương ứng, mô hình tạo ra đầu ra cho mỗi mục dữ liệu kiểm tra, từng token một. Tức là, trong thời gian suy luận, trong

BẢNG I: Thống kê Tập dữ liệu.
Tác vụ Đầu vào→Đầu ra Đào tạo Xác thực Kiểm tra Tổng
Dịch Mã Java →C# 10,300 500 1,000 11,800
Tạo Tài liệu Mã Java→NL 164,923 5,183 10,955 181,061
Python→NL 251,820 13,914 14,918 280,652
Tinh chỉnh Mã Java →Java 52,364 6,545 6,545 65,454

mỗi bước, một số token (tùy thuộc vào kích thước beam, được đặt trong mô hình) được chọn từ các ứng viên dự đoán tiềm năng, và quá trình này được lặp lại (các token mới được thêm vào chuỗi đầu ra ứng viên) cho đến khi mô hình tạo ra token kết thúc câu, cho biết kết thúc dự đoán.

D. Đánh giá Độ chính xác Mô hình

Để đảm bảo rằng các giải thích được tạo ra từ các mô hình của chúng tôi là đáng tin cậy, các mô hình phải chính xác. Để đánh giá độ chính xác của mô hình, chúng tôi sử dụng điểm BLEU-4 làm mịn được sử dụng trong nghiên cứu gốc của CodeBERT [5] và thường được sử dụng bởi các kỹ thuật tạo tài liệu chuẩn [33]. Đối với các tác vụ khác, chúng tôi sử dụng điểm BLEU-4. BLEU-4 là thước đo đánh giá duy nhất chúng tôi sử dụng trong bài báo này và từ bây giờ, trừ khi chúng tôi nói rõ ràng khác khi chúng tôi sử dụng điểm BLEU, chúng tôi đang đề cập đến điểm BLEU-4. Điểm BLEU [34] tính toán sự chồng chéo n-gram của dự đoán và tài liệu hoặc đoạn mã vàng. Vì trong CDG, các câu được tạo ra thường ngắn, và n-gram cấp cao hơn không có khả năng có sự chồng chéo, CodeBERT sử dụng một phiên bản làm mịn [35] để bù đắp điều này, bằng cách đưa ra số lượng bổ sung cho các sự chồng chéo n-gram cấp cao hơn.

Đối với tác vụ tạo tài liệu mã, CodeBERT đạt được điểm BLEU tổng thể là 17.83 (19.06 và 17.65 cho Python và Java, tương ứng), trong khi GraphCodeBERT đạt được điểm BLEU tổng thể là 5.3 và 4.21 cho Java và Python, tương ứng. Đối với tác vụ tinh chỉnh mã, CodeBERT đạt được điểm BLEU là 91.07 với khớp chính xác là 5.16, trong khi GraphCodeBERT đạt được điểm BLEU là 91.31 với khớp chính xác là 9.1. Đối với tác vụ dịch mã (Java sang C#), CodeBERT đạt được điểm BLEU là 79.92 với khớp chính xác là 59.0, trong khi GraphCodeBERT đạt được điểm BLEU là 80.58 với khớp chính xác là 59.4.

E. Giải thích Mô hình thông qua Điểm số Chú ý

Các mô hình Transformer có thể hiểu các phụ thuộc dài giữa các từ trong một câu (hoặc các token trong một đoạn mã) bằng cách hưởng lợi từ cơ chế chú ý. Cơ chế chú ý về cơ bản hoạt động với một khóa (k), truy vấn (q), và giá trị (v), và dk biểu thị chiều của khóa. Ở dạng đơn giản nhất, nó tính toán sự tương tự giữa truy vấn, khóa và giá trị như:

Attention(q, k, v) = softmax(qk^T/√dk)V

trong đó khóa và truy vấn là các phần tử trong chuỗi. Tính toán tất cả các tích vô hướng của qi.kj sẽ tạo ra một ma trận trong đó mỗi hàng đại diện cho trọng số chú ý của một phần tử cụ thể i đối với tất cả các phần tử khác trong chuỗi. Sau đó,

--- TRANG 4 ---

một lớp softmax và phép nhân với vector giá trị sẽ được áp dụng cho ma trận để có được trung bình có trọng số. Điều này có nghĩa là mọi phụ thuộc giữa mỗi hai phần tử sẽ được xem xét trong đầu ra cuối cùng.

Chú ý là một phương pháp XAI hợp lý để giải thích CodeBERT và GraphCodeBERT. Để tính toán chú ý cho mỗi token, chúng tôi cần trọng số cho các lớp chú ý bộ mã hóa-giải mã. Vì chúng tôi có sáu lớp giải mã Transformer được xếp chồng, chúng tôi có sáu lớp chú ý. Thay vì tổng hợp các giá trị chú ý của 6 lớp này thành một thước đo nào đó, chúng tôi quyết định giữ dữ liệu của tất cả các lớp và phân tích vai trò của các lớp khác nhau trong việc giải thích các đầu ra. Trọng số chú ý có sẵn bên trong mô hình, nhưng thư viện Transformers được sử dụng bởi CodeBERT và GraphCodeBERT không cung cấp chúng theo mặc định. Do đó, chúng tôi đã thay đổi việc triển khai mô hình để thu thập chúng. Lưu ý rằng điểm số chú ý cho mỗi token là đầu ra của một lớp softmax, do đó nó có giá trị từ 0 đến 1.

IV. KẾT QUẢ THỰC NGHIỆM

A. (RQ1) Các mô hình mã được đào tạo trước học được gì?

Phương pháp. Để trả lời RQ này, chúng tôi phân tích trọng số chú ý cho mỗi loại token chứ không phải các token riêng lẻ. Để làm như vậy, trước tiên chúng tôi cần xác định danh sách các loại token. Đây là một quyết định chủ quan về những loại token nào là mối quan tâm cho nghiên cứu của chúng tôi. Chúng tôi lựa chọn một tập hợp bảy loại token bao gồm tất cả token và nhóm chúng theo sự liên quan ngữ nghĩa của chúng, như sau:

Tên phương thức: Tên của phương thức đang được nghiên cứu có thể là một trong những yếu tố quyết định chính trong việc nhận thức những gì một phương thức làm, đây là một bước rất quan trọng đối với mô hình, đặc biệt là trong một số tác vụ như CDG. Điều này chỉ bao gồm tên phương thức chính trong mỗi mẫu (nhắc nhở rằng mỗi mẫu đầu vào là mã nguồn cho một phương thức) chứ không phải các phương thức được gọi trong thân của phương thức chính.

Định danh loại: Danh mục này đại diện cho tất cả các từ khóa được sử dụng để xác định loại token trong các ngôn ngữ nguồn của chúng tôi là Python và Java. Để biết thêm thông tin về các token này, hãy xem "định danh loại" và "* loại" trong tree-sitter cho mỗi ngôn ngữ.

Từ khóa ngôn ngữ: Các token lệnh điều khiển luồng đều được gộp lại với nhau cho mỗi ngôn ngữ trong danh mục này. Các token này như sau: Đối với Python: {False, None, True, and, as, assert, async, await, break, class, continue, def, del, elif, else, except, finally, for, from, global, if, import, in, is, lambda, nonlocal, not, or, pass, raise, return, try, while, with, yield} và đối với Java: {if, else, switch, case, while, class, enum, interface, annotation, public, protected, private, static, abstract, final, native, synchronized, transient, volatile, strictfp, assert, return, throw, try, catch, finally, default, super, do, for, break, continue, super, void, import, extends, implements, import, instanceof, new, null, package, this, throws}.

Gọi phương thức: Danh mục này bao gồm tất cả các token là tên của các phương thức được gọi trong thân của phương thức đang được nghiên cứu. Chúng cũng có thể hữu ích trong việc mô tả những gì phương thức đang làm và có thể chứa lỗi cần sửa.

BẢNG II: Số lượng token trong mỗi danh mục (1: CT, 2: CDG Java, 3: CDG Python, 4: CR) cho mỗi tác vụ

Tên phương thức | Biến đầu vào | Gọi phương thức | Biến | Định danh loại | Từ khóa ngôn ngữ | Khác | Tổng
1 | 1,033 | 2,859 | 2,065 | 4,289 | 2,815 | 3,408 | 21,731 | 38,200
2 | 12,991 | 44,175 | 42,285 | 101,352 | 63,143 | 73,644 | 446,419 | 784,009
3 | 16,633 | 110,630 | 19,567 | 197,878 | 0 | 77,662 | 487,860 | 910,230
4 | 7,761 | 19,316 | 33,717 | 70,750 | 52,969 | 36,408 | 319,544 | 540,465

Biến cục bộ: Ở đây chúng tôi xem xét tất cả các biến chỉ được sử dụng trong thân của phương thức đang được nghiên cứu. Tức là, các tham số đầu vào của phương thức bị loại trừ.

Biến đầu vào: Danh mục này chỉ chứa các tham số đầu vào của phương thức đang được nghiên cứu. Chúng tôi đã tách nó khỏi danh mục Biến cục bộ.

Khác: Danh mục này đại diện cho tất cả các token không được bao gồm trong bất kỳ danh mục nào ở trên. Chủ yếu là các token như dấu câu, giá trị hằng số, dấu ngoặc, v.v.

Lưu ý rằng mặc dù các loại token này được chọn một cách chủ quan, kết quả sẽ chứng minh sự lựa chọn thiết kế này bằng cách cho thấy rằng chúng nằm trong số những token quan trọng nhất và không có nhiều token đóng góp được để lại cho danh mục "Khác". Chúng tôi cũng cần nhấn mạnh rằng mức độ trừu tượng về những gì tạo thành một "danh mục token" tùy thuộc vào người dùng XAI. Ví dụ, chúng tôi quyết định tách danh mục Tham số Đầu vào khỏi danh mục Tên Biến, để phân tích tốt hơn tác động của chúng một cách riêng lẻ, nhưng việc hợp nhất hai danh mục là một lựa chọn thiết kế hợp lệ (chỉ ở mức độ trừu tượng khác).

Đối với mỗi mẫu trong dữ liệu kiểm tra, chúng tôi tuân theo các bước này để tìm phân phối điểm số chú ý trên các danh mục khác nhau:

Đối với mỗi token được tạo ra (mỗi bước), chúng tôi lấy trọng số chú ý của nó đối với các token đầu vào và tìm loại tương ứng của chúng. Sau đó, chúng tôi tích lũy điểm số chú ý của tất cả các token trong mỗi danh mục để có được tổng điểm cho danh mục đó trong mẫu đó. Các danh mục của chúng tôi bao gồm tất cả token nên tổng của tất cả điểm số cho mỗi bước bằng một. Chúng tôi trải qua cùng một quá trình cho tất cả các token đầu ra trong tất cả các đoạn mã của tập dữ liệu kiểm tra và thu thập sự tích lũy của điểm số chú ý cho mỗi danh mục.

Đáng chú ý rằng kích thước của các danh mục token khá mất cân bằng. Ví dụ, chỉ có một tên phương thức cho mỗi mẫu nhưng có nhiều token trong danh mục "khác". Do đó, chúng tôi chuẩn hóa tổng điểm của mỗi danh mục, theo dân số của nó như trong Bảng II. Điều này cho chúng tôi điểm chú ý cho mỗi token cho mỗi danh mục. Cuối cùng, chúng tôi chuẩn hóa điểm số của tất cả các danh mục, từ 0 đến 100 với mục đích so sánh dễ dàng hơn.

Trong số các danh mục được xác định này, "Tên phương thức", "Biến đầu vào", và "Biến cục bộ" đại diện hơn cho các khía cạnh đặt tên của mã nguồn. Vì vậy, chúng tôi nhóm chúng trong danh mục cấp cao hơn là "Đặt tên", trong khi "Gọi phương thức", "Định danh loại", và "Từ khóa ngôn ngữ" liên quan nhiều hơn đến cấu trúc của mã. Do đó, chúng tôi xem xét chúng là danh mục cấp cao hơn của "Cấu trúc". Cũng lưu ý rằng chúng tôi chỉ báo cáo điểm trung bình của tất cả sáu lớp cho mỗi tác vụ và mô hình ở đây, vì việc báo cáo tất cả kết quả cho mỗi lớp sẽ quá

--- TRANG 5 ---

BẢNG III: Điểm số chú ý được chuẩn hóa của hai danh mục token cấp cao, cho các mô hình mã và tác vụ khác nhau. Kết quả là trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ | Mô hình | Đặt tên | Cấu trúc | Khác
Dịch Mã | CodeXGLUE | 42.36% | 51.38% | 6.27%
| GraphCodeBERT | 42.60% | 51.30% | 6.09%
CDG java | CodeXGLUE | 63.31% | 29.19% | 7.50%
| GraphCodeBERT | 64.51% | 28.19% | 7.30%
CDG python | CodeXGLUE | 67.97% | 23.75% | 8.28%
| GraphCodeBERT | 74.63% | 17.83% | 7.54%
Tinh chỉnh Mã | CodeXGLUE | 56.46% | 37.96% | 5.58%
| GraphCodeBERT | 55.49% | 38.86% | 5.65%

dài và cũng kết quả của RQ này khá tương tự trên các lớp khác nhau và tất cả chúng đều tuân theo các mẫu tương tự.

Kết quả. Trong ba tác vụ hạ nguồn đang được nghiên cứu, chúng tôi mong đợi các điểm số chú ý được chuẩn hóa khác nhau cho mỗi danh mục cấp cao, như sau: (a) CT là một tác vụ dựa nhiều vào cấu trúc, vì mô hình phải học cấu trúc ngôn ngữ nguồn và tạo ra cấu trúc tương đương trong ngôn ngữ đích. (b) Trong CDG, cấu trúc ít quan trọng hơn (các khối lồng nhau và cây cú pháp ít liên quan đến tài liệu đầu ra). Mặt khác, tên rất quan trọng trong tác vụ này, vì chúng về cơ bản mô tả chức năng của mã nguồn. (c) Cuối cùng, chúng tôi mong đợi tinh chỉnh mã ở giữa hai đầu này, vì cả tên và cấu trúc đều quan trọng trong việc gỡ lỗi mã.

Bảng III cho thấy tổng điểm số chú ý được chuẩn hóa của mỗi danh mục cấp cao và xác nhận giả thuyết của chúng tôi. Tạo Tài liệu Mã, như một tác vụ dựa nhiều vào đặt tên, có điểm số chú ý được chuẩn hóa cao đáng kể hơn 63% cho danh mục Đặt tên, trong khi nó chú ý ít hơn nhiều đến danh mục token loại Cấu trúc, so với các tác vụ khác. Nó cũng có con số cao hơn cho danh mục Khác có thể hiểu được xem xét thực tế rằng các bình luận NL trong mã cũng là một phần của danh mục này. Dịch Mã, mặt khác, là tác vụ duy nhất có điểm số chú ý được chuẩn hóa hơn 50% cho token Cấu trúc và ít hơn bất kỳ tác vụ nào cho danh mục Đặt tên. Tinh chỉnh Mã trong so sánh này giữ vị trí trung gian giữa hai tác vụ đã đề cập trong cả hai danh mục.

Trong Bảng IV, chúng tôi có phân tích chi tiết hơn cho mỗi danh mục của chúng tôi. Chúng tôi thấy rằng cả hai mô hình đều chú ý nhiều hơn đến các token cấu trúc cho CT. Đi vào chi tiết hơn, kết quả cho thấy rằng sự chú ý này tập trung nhiều hơn vào Gọi phương thức và Định danh loại thay vì Từ khóa ngôn ngữ, có khoảng 6% điểm số được chuẩn hóa. Rất thú vị là khi nghiên cứu riêng lẻ, danh mục Tên phương thức có điểm cao thứ hai sau Gọi phương thức. Mặc dù trong việc dịch mã, tên phương thức thường không thay đổi, điều này cho thấy rằng các mô hình sử dụng đáng kể tên của phương thức để hiểu chức năng của nó.

Trong tác vụ CDG, chúng tôi quan sát thấy sự phụ thuộc lớn vào các danh mục Đặt tên, kết quả cho thấy rằng danh mục Tên phương thức đóng vai trò quan trọng nhất. Nó luôn có điểm số được chuẩn hóa gần 40% hoặc cao hơn. Trong trường hợp không có Định danh loại, trong GraphCodeBERT CDG python, danh mục này có điểm cao nhất từ trước đến nay trong tất cả các loại token trên tất cả các tác vụ/mô hình. Trực giác, mức độ quan trọng này có thể biện minh được, vì ngay cả con người cũng dựa nhiều vào tên phương thức để hiểu chức năng của chúng. Ngoài ra, trong ba trong số bốn thí nghiệm khác nhau cho tác vụ này, Biến đầu vào có điểm số chú ý được chuẩn hóa cao thứ hai (từ 11% đến 16%). Điều này về cơ bản có nghĩa là các mô hình đã học được rằng khi tạo tài liệu cho một phương thức, phần chính và thú vị nhất là chữ ký của phương thức chứ không phải thân.

Tinh chỉnh Mã giữ vị trí trung gian giữa hai tác vụ khác có điểm số rất gần nhau cho bốn danh mục Tên phương thức, Biến đầu vào, Gọi phương thức và Biến. Vì trong tác vụ này mô hình được cho là tìm lỗi và cố gắng sửa chúng, có vẻ như các mô hình đã học được rằng ít lỗi hơn xảy ra trong các danh mục như Định danh loại, Từ khóa ngôn ngữ và Khác. Điều này có ý nghĩa vì chúng tôi biết rằng cơ sở dữ liệu cho tác vụ này được thu thập từ các dự án công cộng và chúng có thể không có lỗi cú pháp. Tên phương thức cũng ít có khả năng có lỗi, nhưng như chúng tôi thấy trong các tác vụ khác, danh mục này luôn có sức hấp dẫn tối thiểu đối với các mô hình.

Thú vị là theo bảng, Định danh loại là danh mục liên quan hoàn toàn đến cú pháp của mã và không bao gồm bất kỳ đặt tên nào, có đóng góp nhiều nhất cho CT với khoảng cách so với những danh mục khác. Nó có điểm 20.67% và 21.66% cho CodeBERT và GraphCodeBERT tương ứng trong khi điểm của nó trong các danh mục khác dưới 14%. Cũng đáng đề cập rằng GraphCodeBERT sử dụng luồng dữ liệu của mã để nắm bắt cấu trúc của nó; luôn có điểm cao hơn cho danh mục này so với CodeBERT.

Các mẫu tương tự xuất hiện hợp lệ cho tên Biến và cho CR. Điểm của danh mục này cho CR là 17.85% và 18.93%, trong khi trong các tác vụ khác điểm luôn dưới 12%.

Tương tự, danh mục Tên phương thức có điểm cao hơn hẳn trong CDG. Đối với tác vụ này trong python, danh mục này có điểm 39.44% và 41.04%, và trong Java, nó có 46.17% và 54.21% cho CodeBERT và GraphCodeBERT, tương ứng.

Có tất cả những quan sát này, chúng tôi có thể thấy một mẫu quan trọng so sánh các tác vụ khác nhau với nhau. Tên phương thức và biến đầu vào (về cơ bản là dòng đầu tiên của các mẫu mã) là các danh mục quan trọng nhất cho CDG; Gọi phương thức và biến cục bộ đóng vai trò quan trọng nhất đối với tinh chỉnh mã, cùng với tên phương thức với tầm quan trọng thấp hơn một chút. Mặt khác, dịch mã quan tâm đến định danh loại và từ khóa ngôn ngữ, hơn bất kỳ tác vụ nào khác, trong khi vẫn quan tâm đến một số danh mục đặt tên. Trong Bảng III, chúng tôi đã tổng hợp các con số cho hai danh mục chính của chúng tôi và chúng tôi có thể thấy một mẫu mà chúng tôi mong đợi. Token đặt tên quan trọng cho tất cả các tác vụ, nhưng ít hơn cho dịch mã mà thay vào đó, quan tâm nhiều hơn đến các token cấu trúc so với các tác vụ khác.

--- TRANG 6 ---

BẢNG IV: Điểm số chú ý được chuẩn hóa của các loại token khác nhau, cho các mô hình mã và tác vụ khác nhau. Kết quả là trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ/Mô hình | Tên phương thức | Biến đầu vào | Gọi phương thức | Biến | Định danh loại | Từ khóa ngôn ngữ | Khác
CT-Code XGLUE | 21.36% | 9.63% | 24.26% | 11.36% | 20.67% | 6.45% | 6.27%
CT-Graph CodeBERT | 22.78% | 7.89% | 23.46% | 11.93% | 21.66% | 6.18% | 6.09%
CDG Java-Code XGLUE | 39.44% | 13.88% | 10.49% | 10.00% | 13.07% | 5.63% | 7.50%
CDG Java-Graph CodeBERT | 41.04% | 15.10% | 8.44% | 8.38% | 13.13% | 6.62% | 7.30%
CDG Python-Code XGLUE | 46.17% | 11.96% | 16.00% | 9.83% | 0.00% | 7.76% | 8.28%
CDG Python-Graph CodeBERT | 54.21% | 12.79% | 10.79% | 7.64% | 0.00% | 7.03% | 7.54%
CR-Code XGLUE | 22.01% | 16.60% | 20.33% | 17.85% | 9.82% | 7.81% | 5.58%
CR-Graph CodeBERT | 19.36% | 17.19% | 21.15% | 18.93% | 10.02% | 7.69% | 5.65%

B. (RQ2) Khi nào các mô hình mã được đào tạo trước không hoạt động?

Phương pháp. Để cung cấp giải thích về khi nào CodeBERT và GraphCodeBert hoạt động tốt và khi nào chúng thất bại, trong RQ này, chúng tôi bắt đầu bằng phân tích định tính một số dự đoán mẫu. Sau đó, chúng tôi đưa ra giả thuyết dựa trên quan sát của chúng tôi và cuối cùng xác minh chúng một cách định lượng trên toàn bộ tập dữ liệu. Để xác định hiệu suất mạnh và yếu của các mô hình, chúng tôi không thể chỉ dựa vào các giá trị tuyệt đối của thước đo đánh giá (BLEU). Vì độ lớn của điểm BLEU một phần phụ thuộc vào mức độ khó hoặc dễ của tác vụ tạo tài liệu, cho mỗi mã mẫu. Do đó, chúng tôi cần đo lường mức độ khó của tác vụ tạo tài liệu nào đó, khi cho một mã nguồn.

Để giải quyết câu hỏi nghiên cứu này, chúng tôi tạo ra các thước đo để đánh giá độ phức tạp của mẫu và đánh giá hiệu suất mô hình tương ứng.

Trong bối cảnh CR và CT, nơi việc sao chép token chiếm ưu thế, Khoảng cách Levenshtein (LD) giữa đầu vào và đầu ra mong đợi ('đầu ra vàng') phục vụ như một thước đo độ phức tạp phù hợp. Bằng cách tính toán LD cho tất cả các mẫu tập dữ liệu, chúng tôi xác định một phần ba dễ nhất dựa trên LD thấp hơn.

Đối với tác vụ CDG, mặc dù sự khác biệt đầu vào-đầu ra trong ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL), phương pháp của chúng tôi vẫn tương tự. Chúng tôi xác định độ khó bằng cách giao các token được xử lý trước trong tài liệu đầu ra vàng cho mỗi phương thức với các token trong mã nguồn của phương thức.

Để tìm sự chồng chéo giữa mã nguồn và token đầu ra, chúng tôi tuân theo các bước tiền xử lý này: Đầu tiên, chúng tôi loại bỏ dấu câu và token ngắn hơn ba ký tự, trong tài liệu đầu ra. Sau đó, lemmatize các token đó bằng cách sử dụng lemmatizer Wordnet [36] chuẩn, được cung cấp trong gói NLTK. Tiếp theo, chúng tôi tokenize mã nguồn bằng cách sử dụng parser cho ngôn ngữ tương ứng. Lưu ý rằng do tokenization của CodeBERT và GraphCodeBERT, có thể chia một từ có nghĩa thành nhiều token, chúng tôi không sử dụng tokenization của chúng cho phân tích này. Cuối cùng, chúng tôi tạo ra một tập hợp các token không phân biệt chữ hoa chữ thường nằm ở giao điểm của đầu ra đã xử lý và token mã nguồn.

Bây giờ một tác vụ tạo tài liệu dễ/khó là khi sự chồng chéo giữa hai tập hợp cao/thấp. Do đó, giống như các ngưỡng cắt cho CR và CT, chúng tôi xem xét một phần ba đầu tiên các mẫu với sự chồng chéo cao nhất là dễ, và một phần ba trường hợp với sự chồng chéo ít nhất là các vấn đề khó, và bỏ qua phần còn lại (mức độ khó trung bình).

Quá trình trên cho chúng tôi biết mẫu nào được coi là khó và mẫu nào được coi là dễ. Bây giờ chúng tôi cần đo lường hiệu suất của các mô hình. Để làm như vậy, chúng tôi sử dụng điểm BLEU vì nó là điểm được chấp nhận và đáng tin cậy nhất được áp dụng trong các tác vụ này trong tài liệu. Đối với độ chính xác, chúng tôi cũng lấy một phần ba các mẫu với điểm BLEU cao nhất làm Cao và các mẫu với điểm BLEU ít nhất một phần ba làm Thấp.

Có những định nghĩa này, sẽ có bốn danh mục (cho các tuple của <mức độ dễ, độ chính xác mô hình>) như dưới đây:

Dễ−Cao: Danh mục này chứa các mục dữ liệu kiểm tra là các vấn đề dễ (có nghĩa là độ tương tự cao giữa đầu vào và dữ liệu vàng) mà các mô hình đã đạt được điểm BLEU Cao trên chúng.

Khó−Cao: Danh mục này chứa các mẫu được gắn nhãn là Khó và Cao. Điều này có nghĩa là ngay cả với sự thiếu token chung, mô hình vẫn có thể đạt được kết quả thỏa đáng trong những trường hợp này.

Khó−Thấp: Danh mục này bao gồm các trường hợp khó lần nữa, và như mong đợi, kết thúc với độ chính xác Thấp cho dự đoán của mô hình.

Dễ−Thấp: Danh mục này là thú vị nhất trong bài báo này, vì nó có thể hiển thị các điểm yếu tiềm năng của mô hình và rất phù hợp để được phân tích và "giải thích". Các mẫu trong nhóm này, nằm trong số các mẫu có sự chồng chéo cao hơn trong tập dữ liệu tương ứng của chúng có nghĩa là mô hình đang có công việc khá dễ dàng để dự đoán. Tuy nhiên, điểm BLEU như chỉ số độ chính xác của mô hình đang cho thấy hiệu suất kém so với các mẫu khác.

Để thực hiện quan sát thủ công (nghiên cứu định tính) cho RQ này, sau khi nhóm tập dữ liệu kiểm tra của chúng tôi vào bốn danh mục này, chúng tôi chọn ngẫu nhiên 100 mẫu từ danh mục mục tiêu của chúng tôi (Dễ−Thấp), và phân tích thủ công các đầu ra và trọng số chú ý của chúng.

Đối với mỗi mẫu, chúng tôi ghi lại những phát hiện thú vị nhất để xác định các mẫu thường xuyên nhất. Bằng cách này, chúng tôi phát triển một số giả thuyết. Cuối cùng, chúng tôi cố gắng xác minh những giả thuyết này bằng cách nghiên cứu định lượng toàn bộ tập dữ liệu kiểm tra, liên quan đến các giả thuyết. Đầu ra của giai đoạn định lượng này ở dạng một số thống kê mô tả để xác nhận hoặc bác bỏ các quan sát được thực hiện dựa trên 100 mẫu.

Kết quả. Có dữ liệu được chia theo các nhóm đã xác định, Bảng V cho thấy tỷ lệ dân số danh mục mục tiêu với toàn bộ tập dữ liệu cho mỗi tác vụ-mô hình. Tiếp theo, chúng tôi sẽ giải thích các quan sát từ phân tích thủ công.

Quan sát 1: Các mô hình mã được đào tạo trước không hoạt động tốt, khi tài liệu đầu ra vàng dài.

--- TRANG 7 ---

BẢNG V: Tỷ lệ dân số danh mục Dễ-Thấp với toàn bộ tập dữ liệu, cho mỗi tác vụ-mô hình.

| CodeBERT | GraphCodeBERT
CT | 11.70% | 11.41%
CDG / Java | 5.12% | 5.59%
CDG / Python | 5.87% | 5.85%
CR | 2.49% | 4.37%

Hình 1: Điểm BLEU so với độ dài mã tài liệu vàng.

Quan sát đầu tiên của chúng tôi liên quan đến tác vụ CDG là trong các trường hợp có tài liệu vàng dài, điểm BLEU có xu hướng thấp! Chúng tôi vẽ phân phối điểm BLEU, theo độ dài của tài liệu vàng, trong Hình 1. Theo biểu đồ, hầu hết điểm BLEU cao xảy ra khi độ dài của tài liệu vàng ít hơn 50 ký tự.

Nói chung, ý tưởng của điểm BLEU là về việc đếm số lượng n-gram chung giữa tham chiếu và đầu ra. Các tài liệu được tạo bởi mô hình thường ngắn nên đối với các câu dài hơn, có ít cơ hội hơn để mô hình chọn cùng một cách diễn đạt và từ với cùng thứ tự. Một giải thích khả thi khác cho quan sát này là tài liệu dài hơn có nghĩa là phương thức thực hiện một tác vụ phức tạp hơn và do đó khó hơn cho mô hình để tạo ra tài liệu đúng cho phương thức phức tạp.

Một giải pháp tiềm năng cho vấn đề này là buộc mô hình tạo ra các chuỗi dài hơn như đầu ra sẽ tăng cơ hội có điểm BLEU cao, trong các trường hợp có tài liệu tham chiếu dài. Nhưng rõ ràng, vì đây thực sự không phải là lỗi của mô hình và trong những trường hợp này, điểm BLEU thấp không nhất thiết cho thấy dự đoán tồi (như ví dụ được hiển thị trong Hình 2), cách tốt nhất để xử lý vấn đề này là xem xét các thước đo đánh giá khác và lý tưởng nhất là những thước đo chủ quan hơn.

Quan sát 2: Các mô hình mã được đào tạo trước không hoạt động tốt, khi mã nguồn đầu vào phức tạp.

Một quan sát thú vị khác là về độ dài của mã nguồn. Kết quả cho thấy rằng trong các trường hợp có mã dài hơn, điểm BLEU thường thấp hơn. Chúng tôi bắt đầu phân tích ban đầu với dữ liệu CDG và kết quả, được tóm tắt trong Hình 3, cho thấy xu hướng giảm của điểm BLEU, bằng cách tăng độ dài mã nguồn. Ví dụ, điểm BLEU trung bình cho các trường hợp ngắn hơn và dài hơn 300 token lần lượt là 0.161 và 0.149.

Tài liệu vàng: Phân tích các tiêu đề kiểm soát cache trả về một từ điển với các giá trị cho các chỉ thị khác nhau.
Dự đoán tốt nhất: Phân tích một dict của headers
Điểm BLEU: 0.08
Chồng chéo: 0.54

Hình 2: Một phương thức mẫu được chia thành các token, trong đó các giá trị chú ý của lớp cuối cùng của CodeBERT cho token được tạo cuối cùng ("headers" trong ví dụ này) được tô sáng như các sắc thái màu xanh (càng tối, càng cao).

Hình 3: Điểm BLEU so với độ dài mã nguồn.

Có hai giải thích cho quan sát này: (1) độ dài cố định của đầu vào trong các mô hình, về cơ bản có nghĩa là nếu độ dài mã nguồn cao hơn một giá trị cố định (trong các thí nghiệm của chúng tôi, 256 token), thì đầu vào sẽ bị cắt ngắn. Điều này có nghĩa là một số token sẽ không đến được bộ giải mã, và do đó chúng tôi có sự suy giảm tiềm năng trong điểm cuối cùng. (2) độ phức tạp tăng lên của mã. Tương tự như Quan sát 1, mã nguồn dài hơn có nghĩa là logic phức tạp hơn, nhiều đối tượng hơn, và các chức năng cần xem xét cho mô hình có thể dẫn đến kết quả kém hơn.

Theo những lý do này, hai giải pháp cơ bản có thể được đề xuất: (a) tăng ngưỡng đầu vào và (b) giảm độ dài đầu vào. Ngưỡng đầu vào có thể dễ dàng được sửa đổi

--- TRANG 8 ---

trong quá trình đào tạo của mô hình và chỉ yêu cầu nhiều tài nguyên hơn. Tùy chọn thứ hai, tuy nhiên, là một giải pháp phức tạp hơn đã được triển khai một cách ngây thơ bởi việc cắt ngắn. Một lựa chọn tiềm năng khác là tái cấu trúc các phương thức dài thành nhiều phương thức nhỏ hơn, sau đó chuyển mỗi phương thức cho các mô hình để tạo tài liệu, và cuối cùng hợp nhất tất cả tài liệu đầu ra thành một tài liệu.

Tiếp theo, để mở rộng quan sát này trên tất cả các tác vụ-mô hình, và tìm thêm kết quả thống kê, chúng tôi đã sử dụng một số thước đo độ phức tạp mã phổ biến và tiến hành phân tích trên tất cả 8 mô hình-tác vụ để hiểu rõ hơn về nguyên nhân gốc rễ của kết quả kém cho các đoạn mã dài. Chúng tôi đã chọn 'số lượng token', 'độ phức tạp cyclomatic', 'độ sâu khối lồng nhau', 'số lượng biến' làm thước đo độ phức tạp. Để đo lường độ khó của tác vụ, chúng tôi cũng bao gồm cùng 'khoảng cách Levenshtein' cho CT và CR và 'chồng chéo' cho CDG, trong phân tích của chúng tôi.

Đối với mỗi tác vụ-mô hình, chúng tôi có năm thước đo khác nhau để nghiên cứu, vì vậy chúng tôi có một biểu đồ cho mỗi thước đo. Trong mỗi biểu đồ, phân phối các mẫu trong tập dữ liệu tương ứng, liên quan đến thước đo đó được hiển thị bằng thanh màu xanh, và cùng phân phối nhưng chỉ cho danh mục mục tiêu (Dễ-Thấp) được hiển thị bằng màu đỏ. Với việc trực quan hóa này, chúng tôi có thể xác định bất kỳ sự khác biệt nào về xu hướng trên một thước đo cụ thể trong danh mục mục tiêu so với toàn bộ tập dữ liệu.

Hình 4 cho thấy kết quả cho tinh chỉnh mã (CR) cho CodeBERT cho số lượng token và số lượng biến (tất cả các biểu đồ cho GraphCodeBERT và các thước đo khác cho CodeBERT có thể được tìm thấy trong repo công khai). Như được minh họa trong các biểu đồ, danh mục Dễ-Thấp có phân phối rất tương tự như toàn bộ tập dữ liệu, ngoại trừ xu hướng tăng nhẹ cho một số thước đo được báo cáo như số lượng token và số lượng biến. Điều này có nghĩa là mô hình có xu hướng đưa ra quyết định tồi, bất cứ khi nào mã nguồn trở nên phức tạp hơn về số lượng token và biến, ngay cả khi sự chồng chéo của token cao. Ví dụ, xem xét số lượng token, như một thước đo độ phức tạp mã, tỷ lệ các mẫu có hơn 100 token chủ yếu cao hơn trong danh mục Dễ-Thấp so với tỷ lệ các mẫu tương tự trong tổng số. Điều đó có nghĩa là các mẫu có nhiều token có nhiều khả năng được cho là "Dễ" trong các danh mục của chúng tôi (nhiều sự chồng chéo hơn giữa đầu vào và đầu ra) nhưng thực tế, chúng khó hơn cho mô hình hiểu (do độ dài dài của đoạn mã).

Hình 5 cho thấy kết quả cho dịch mã (CT) cho số lượng token, độ sâu khối lồng nhau, và độ phức tạp cyclomatic cho CodeBERT (tất cả các biểu đồ cho GraphCodeBERT và các thước đo khác cho CodeBERT có thể được tìm thấy trong repo công khai). Đối với tác vụ này, số lượng biến tuân theo cùng một mẫu như tác vụ CR, tức là danh mục Dễ-Thấp khó hơn dựa trên các thước đo đó. Mặt khác, xem xét số lượng token, độ sâu khối lồng nhau, và thậm chí độ phức tạp cyclomatic, có một kết nối ngược lại. Nói cách khác, các mẫu có giá trị nhỏ hơn của các thước đo này, có mật độ cao hơn trong danh mục Dễ-Thấp. Ví dụ, trong cả hai mô hình, các mẫu có token ít hơn 20, là khoảng 50% dân số danh mục mục tiêu, mặc dù chúng chiếm một phần rất nhỏ của tổng tập dữ liệu. Một giải thích khả thi là khi mã nguồn quá ngắn (số lượng token rất nhỏ và rất ít khối lồng nhau), mô hình thất bại trong việc dịch nó đúng cách, do thiếu đủ thông tin/bối cảnh.

(a)

(b)

Hình 4: Phân phối của toàn bộ tập dữ liệu (màu xanh) và danh mục Dễ-Thấp (màu đỏ), theo thước đo độ phức tạp mã, cho tinh chỉnh mã trên CodeBERT

Một giải thích khác là thực tế rằng khó hơn để duy trì điểm BLEU cao khi đầu vào rất ngắn.

Hình 6 và Hình 7 minh họa một số kết quả tương tự cho tạo tài liệu mã (CDG) cho CodeBERT (tất cả các biểu đồ cho GraphCodeBERT và các thước đo khác cho CodeBERT có thể được tìm thấy trong repo công khai). Trong tác vụ hạ nguồn này, chúng tôi có thể thấy các mẫu phụ thuộc nhiều hơn vào ngôn ngữ, thay vì mô hình. Trong tất cả các trường hợp, số lượng biến, số lượng token, và độ sâu khối lồng nhau tuân theo cùng một mẫu chung.

Trong những thí nghiệm này, chúng tôi thấy rằng cả hai mô hình đều gặp khó khăn với các mẫu có độ phức tạp thấp trong Java. Tuy nhiên, chúng cũng có vấn đề trong việc tìm ra các mẫu phức tạp hơn trong Python. Ví dụ trong Python, các mẫu có hơn 80 token hoặc 8 biến, luôn có mật độ cao hơn trong danh mục Dễ-Thấp so với tất cả tập dữ liệu. Cuối cùng, thú vị là xem xét độ phức tạp cyclomatic, cả hai mô hình trong cả hai ngôn ngữ đều gặp khó khăn với các mẫu có độ phức tạp cao hơn.

Vì vậy, tóm lại, có thể kết luận rằng các mô hình mã hoạt động kém trên các đoạn mã có giá trị cực đoan của các thước đo liên quan đến độ phức tạp ở cả hai hướng (tức là cả mã dài

--- TRANG 9 ---

(a)

(b)

(c)

Hình 5: Phân phối của toàn bộ tập dữ liệu (màu xanh) và danh mục Dễ-Thấp (màu đỏ), theo thước đo độ phức tạp mã, cho dịch mã trên CodeBERT

có nhiều khối lồng nhau và token và cũng mã rất ngắn chỉ có một vài biến và token).

Quan sát 3: Các mô hình mã được đào tạo trước không hoạt động tốt, khi các mô hình thất bại trong việc tập trung vào các danh mục quan trọng:

Cuối cùng, chúng tôi phân tích đóng góp của các danh mục token tương tự như RQ1, nhưng cụ thể cho danh mục mục tiêu của (Dễ−Thấp). Trong Bảng VI, chúng tôi có điểm số được chuẩn hóa của hai danh mục chính cho các mẫu mục tiêu. Chúng tôi quan tâm đến việc so sánh kết quả cho danh mục này và kết quả trước đó cho toàn bộ tập dữ liệu kiểm tra. Do đó, chúng tôi tính toán sự khác biệt giữa hai cái này từ Bảng VI và Bảng III, và kết quả được hiển thị trong Bảng VII. Các số âm

(a)

(b)

Hình 6: Phân phối của toàn bộ tập dữ liệu (xanh) và danh mục Dễ-Thấp (đỏ), dựa trên thước đo độ phức tạp mã, cho tạo tài liệu mã trên CodeBERT trên Java

trong bảng này chỉ ra sự giảm trong danh mục mục tiêu.

Như kết quả cho thấy, có sự giảm đáng kể trong điểm cho danh mục Cấu trúc trong CT. Trong khi trả lời RQ1, chúng tôi cho thấy rằng tác vụ này chủ yếu dựa vào nhóm token này IV-A. Tương tự, chúng tôi có sự giảm nhẹ trong điểm của danh mục Đặt tên trong CDG trong khi danh mục đặt tên cũng chứng minh là danh mục quan trọng hơn cho CDG.

Cả hai manh mối này, dẫn chúng tôi đến kết luận rằng kết quả kém thỏa mãn hơn, bất cứ khi nào mô hình thất bại trong việc chú ý đủ đến danh mục token quan trọng tương ứng cho một tác vụ cụ thể. Dựa trên quan sát này, các câu hỏi nghiên cứu tiềm năng để điều tra trong tương lai là: "Liệu mô hình sẽ hoạt động tốt hơn nếu chúng tôi giúp nó bằng cách gắn thẻ các loại token? Việc khuếch đại thủ công điểm chú ý của các danh mục cụ thể theo tác vụ có thể có lợi cho các mô hình mã hay không".

V. HẠN CHẾ

Một hạn chế của nghiên cứu này là các thí nghiệm của chúng tôi chỉ trên mã Java và Python. Việc bao gồm các tập dữ liệu khác (cũng như mã Python cho các tác vụ CT và CR) đòi hỏi rất nhiều tiền xử lý để trở nên nhất quán với thiết kế và yêu cầu của chúng tôi và sẽ vượt ra ngoài phạm vi và kích thước của một bài báo hội nghị. Chúng tôi dự định mở rộng những phân tích này sang các ngôn ngữ khác trong tương lai.

Một hạn chế khác là chúng tôi đã sử dụng điểm BLEU làm thước đo đánh giá của chúng tôi cho độ chính xác của mô hình, điều này

--- TRANG 10 ---

(a)

(b)

Hình 7: Phân phối của toàn bộ tập dữ liệu (xanh) và danh mục Dễ-Thấp (đỏ), dựa trên thước đo độ phức tạp mã, cho tạo tài liệu mã trên CodeBERT trên Python

BẢNG VI: Điểm số chú ý được chuẩn hóa của các mẫu Dễ-Thấp, trong ba danh mục token chung, cho các mô hình mã và tác vụ khác nhau. Kết quả là trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ | Mô hình | Đặt tên | Cấu trúc | Khác
CT | CodeXGLUE | 48.85% | 44.51% | 7.70%
| GraphCodeBERT | 49.51% | 43.49% | 7.57%
CDG java | CodeXGLUE | 60.08% | 33.28% | 8.89%
| GraphCodeBERT | 63.85% | 37.06% | 7.43%
CDG python | CodeXGLUE | 66.37% | 29.14% | 8.61%
| GraphCodeBERT | 73.68% | 24.16% | 7.87%
CR | CodeXGLUE | 56.64% | 46.29% | 5.63%
| GraphCodeBERT | 55.82% | 48.35% | 5.62%

thường được sử dụng trong các tác vụ tạo tài liệu hạ nguồn để giảm tính chủ quan của kết quả. Tuy nhiên, như chúng tôi đã đề cập trong bài báo, nó không phải là một thước đo toàn diện vì nó không thể tìm thấy các cách diễn đạt lại hoặc các trường hợp mà dự đoán không sai, nhưng không khớp chính xác với nhãn vàng.

Các quan sát chúng tôi đã thực hiện cũng chỉ giới hạn ở các mẫu chính mà chúng tôi đã quan sát được trong 100 mẫu, một cách thủ công. Mặc dù sau đó chúng tôi xác nhận chúng một cách định lượng, tất nhiên có thể có một số giải thích khác

BẢNG VII: Sự khác biệt của điểm số chú ý được chuẩn hóa cho các mẫu Dễ-Thấp và tất cả mẫu, trong hai danh mục token chung, cho các mô hình mã và tác vụ khác nhau. Kết quả là trung bình của tất cả sáu lớp cho mỗi tác vụ.

Tác vụ | Mô hình | Đặt tên | Cấu trúc | Khác
CT | CodeXGLUE | 6.49% | -7.92% | 1.43%
| GraphCodeBERT | 6.90% | -8.38% | 1.48%
CDG java | CodeXGLUE | -3.24% | 1.85% | 1.39%
| GraphCodeBERT | -0.66% | 0.53% | 0.13%
CDG python | CodeXGLUE | -1.59% | 1.27% | 0.33%
| GraphCodeBERT | -0.96% | 0.63% | 0.33%
CR | CodeXGLUE | 0.18% | -0.23% | 0.06%
| GraphCodeBERT | 0.33% | -0.30% | -0.03%

mà chúng tôi đã bỏ lỡ quan sát, do các mẫu chúng tôi đã chọn.

Ngoài ra, nghiên cứu chỉ giới hạn ở ba tác vụ hạ nguồn (CT, CDG, và CR) và hai mô hình mã (CodeBERT và GraphCodeBERT). Cần nhiều công việc hơn để tổng quát hóa các phát hiện cho các mô hình dựa trên Transformer khác, trong tương lai.

Đáng đề cập là việc triển khai tác vụ CDG trong khung GraphCodeBERT, được thực hiện sử dụng các cài đặt và siêu tham số giống hệt như các tác vụ và mô hình khác. Phương pháp thống nhất này có thể đóng góp vào độ chính xác thấp hơn được quan sát trong hiệu suất của tác vụ CDG trong mô hình.

Cuối cùng, nghiên cứu này được tiến hành trước khi ChatGPT được công bố. Vì vậy, một mở rộng rất liên quan của công việc này sẽ là bao gồm mô hình GPT-4 cả như một mô hình mã cũng như một phương pháp XAI để cung cấp giải thích về các quyết định của chính nó và các mô hình khác.

VI. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Bài báo này đề xuất một phương pháp giải thích các mô hình mã được đào tạo trước (ví dụ: CodeBert và GraphCodeBert), sử dụng cơ chế chú ý nội bộ end-to-end của chúng, như phương pháp XAI. Không giống như hầu hết nghiên cứu XAI, nơi giải thích được áp dụng cho các mô hình độ chính xác cao để đảm bảo rằng kết quả đáng tin cậy, chúng tôi đã sử dụng XAI trên cả kịch bản độ chính xác cao (để tìm hiểu những gì các mô hình học) và thấp (để xem khi nào chúng không hoạt động tốt). Các phát hiện của chúng tôi không chỉ cung cấp quan sát về những gì các mô hình dựa trên Transformer tiên tiến này học theo các danh mục loại token và tại sao chúng hoạt động kém trong một số kịch bản, mà còn đề xuất các khuyến nghị có thể thực hiện được, như sử dụng các thước đo đánh giá chủ quan hơn cho tác vụ CDG, cung cấp loại token như đầu vào bổ sung cho mô hình, và khuếch đại thủ công điểm chú ý cho các loại token cụ thể. Trong tương lai, chúng tôi dự định mở rộng công việc này bằng cách kiểm tra các tác vụ hạ nguồn, mô hình và phương pháp XAI khác. Ngoài ra, chúng tôi cũng dự định làm việc trên các mô hình mã được đào tạo trước bằng cách triển khai các khuyến nghị được đề xuất từ quan sát của chúng tôi. Cuối cùng, chúng tôi dự định sử dụng mô hình GPT-4 để cả truy cập kết quả của nó như mô hình mã và sử dụng nó như một kỹ thuật XAI để giải thích tại sao một đầu ra cụ thể được cung cấp bởi mô hình.

--- TRANG 11 ---

TÀI LIỆU THAM KHẢO

[1] G. Shobha, A. Rana, V. Kansal, and S. Tanwar, "Code clone detection—a systematic review," Emerging Technologies in Data Mining and Information Security, pp. 645–655, 2021.

[2] C. Pornprasit, C. Tantithamthavorn, J. Jiarpakdee, M. Fu, and P. Thongtanunam, "Pyexplainer: Explaining the predictions of just-in-time defect models," in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 407–418.

[3] J. Humphreys and H. K. Dam, "An explainable deep model for defect prediction," in 2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE). IEEE, 2019, pp. 49–55.

[4] A. LeClair, S. Haque, L. Wu, and C. McMillan, "Improved code summarization via a graph neural network," in Proceedings of the 28th international conference on program comprehension, 2020, pp. 184–195.

[5] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang et al., "Codebert: A pre-trained model for programming and natural languages," in Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536–1547.

[6] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu et al., "Graphcodebert: Pre-training code representations with data flow," arXiv preprint arXiv:2009.08366, 2020.

[7] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., "Codexglue: A machine learning benchmark dataset for code understanding and generation," arXiv preprint arXiv:2102.04664, 2021.

[8] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation," arXiv preprint arXiv:2109.00859, 2021.

[9] C. Tantithamthavorn, J. Jiarpakdee, and J. Grundy, "Actionable analytics: Stop telling me what it is; please tell me what to do," IEEE Software, vol. 38, no. 4, pp. 115–120, 2021.

[10] C. K. Tantithamthavorn and J. Jiarpakdee, "Explainable ai for software engineering," in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2021, pp. 1–2.

[11] J. Jiarpakdee, C. K. Tantithamthavorn, and J. Grundy, "Practitioners' perceptions of the goals and visual explanations of defect prediction models," in 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 2021, pp. 432–443.

[12] J. Jiarpakdee, C. K. Tantithamthavorn, H. K. Dam, and J. Grundy, "An empirical study of model-agnostic techniques for defect prediction models," IEEE Transactions on Software Engineering, vol. 48, no. 1, pp. 166–185, 2020.

[13] D. Rajapaksha, C. Tantithamthavorn, J. Jiarpakdee, C. Bergmeir, J. Grundy, and W. Buntine, "Sqaplanner: Generating data-informed software quality improvement plans," IEEE Transactions on Software Engineering, vol. 48, no. 8, pp. 2814–2835, 2021.

[14] S. Wattanakriengkrai, P. Thongtanunam, C. Tantithamthavorn, H. Hata, and K. Matsumoto, "Predicting defective lines using a model-agnostic technique," IEEE Transactions on Software Engineering, vol. 48, no. 5, pp. 1480–1496, 2020.

[15] C. Pornprasit and C. K. Tantithamthavorn, "Jitline: A simpler, better, faster, finer-grained just-in-time defect prediction," in 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). IEEE, 2021, pp. 369–379.

[16] C. Khanan, W. Luewichana, K. Pruktharathikoon, J. Jiarpakdee, C. Tantithamthavorn, M. Choetkiertikul, C. Ragkhitwetsagul, and T. Sunetnanta, "Jitbot: An explainable just-in-time defect prediction bot," in 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2020, pp. 1336–1339.

[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.

[18] C. Pan, M. Lu, and B. Xu, "An empirical study on software defect prediction using codebert model," Applied Sciences, vol. 11, no. 11, p. 4793, 2021.

[19] K. Cao, C. Chen, S. Baltes, C. Treude, and X. Chen, "Automated query reformulation for efficient search based on query logs from stack overflow," in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1273–1285.

[20] X. Gu, H. Zhang, and S. Kim, "Deep code search," in 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 2018, pp. 933–944.

[21] T. Nguyen, P. C. Rigby, A. T. Nguyen, M. Karanfil, and T. N. Nguyen, "T2api: Synthesizing api code usage templates from english texts with statistical translation," in Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, 2016, pp. 1013–1017.

[22] S. Haque, A. LeClair, L. Wu, and C. McMillan, "Improved automatic summarization of subroutines via attention to file context," in Proceedings of the 17th International Conference on Mining Software Repositories, 2020, pp. 300–310.

[23] S. Jiang, A. Armaly, and C. McMillan, "Automatically generating commit messages from diffs using neural machine translation," in 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2017, pp. 135–146.

[24] L. Liu, X. Hu, W. Song, R. Fu, T. Liu, and G. Hu, "Neural multitask learning for simile recognition," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 1543–1553.

[25] N. Jiang, T. Lutellier, and L. Tan, "Cure: Code-aware neural machine translation for automatic program repair," in 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE, 2021, pp. 1161–1173.

[26] Y. Li, S. Wang, and T. N. Nguyen, "Dlfix: Context-based code transformation learning for automated program repair," in Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, 2020, pp. 602–614.

[27] Z. Chen, S. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshyvanyk, and M. Monperrus, "Sequencer: Sequence-to-sequence learning for end-to-end program repair," IEEE Transactions on Software Engineering, vol. 47, no. 9, pp. 1943–1959, 2019.

[28] B. Roziere, M.-A. Lachaux, L. Chanussot, and G. Lample, "Unsupervised translation of programming languages," Advances in Neural Information Processing Systems, vol. 33, pp. 20 601–20 611, 2020.

[29] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, "Intellicode compose: Code generation using transformer," in Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, 2020, pp. 1433–1443.

[30] R. Tufano, S. Masiero, A. Mastropaolo, L. Pascarella, D. Poshyvanyk, and G. Bavota, "Using pre-trained models to boost code review automation," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 2291–2302.

[31] P. Thongtanunam, C. Pornprasit, and C. Tantithamthavorn, "Autotransform: Automated code transformation to support modern code review process," in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 237–248.

[32] Y. Liu, C. Tantithamthavorn, Y. Liu, P. Thongtanunam, and L. Li, "Autoupdate: Automatically recommend code updates for android apps," arXiv preprint arXiv:2209.07048, 2022.

[33] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, "Deep code comment generation," in 2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC). IEEE, 2018, pp. 200–20 010.

[34] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318.

[35] C.-Y. Lin and F. J. Och, "Orange: a method for evaluating automatic evaluation metrics for machine translation," in COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, 2004, pp. 501–507.

[36] E. Loper and S. Bird, "Nltk: The natural language toolkit," CoRR, vol. cs.CL/0205028, 2002. [Online]. Available: http://dblp.uni-trier.de/db/journals/corr/corr0205.html#cs-CL-0205028
