# Dự đoán Mức độ Nghiêm trọng của Lỗi ở Mức Phương thức sử dụng Các Metric Mã nguồn và LLMs

Tóm tắt — Trong vài thập kỷ qua, các nỗ lực nghiên cứu đáng kể được dành cho việc dự đoán lỗi phần mềm. Tuy nhiên, hầu hết các công trình hiện có trong lĩnh vực này đối xử với tất cả các lỗi như nhau, điều này không đúng trong thực tế. Điều quan trọng là một phương pháp dự đoán khiếm khuyết phải ước tính mức độ nghiêm trọng của các lỗi đã xác định để những lỗi nghiêm trọng hơn nhận được sự chú ý ngay lập tức. Trong nghiên cứu này, chúng tôi điều tra các metric mã nguồn, biểu diễn mã nguồn sử dụng các mô hình ngôn ngữ lớn (LLMs), và sự kết hợp của chúng trong việc dự đoán nhãn mức độ nghiêm trọng lỗi của hai bộ dữ liệu nổi bật. Chúng tôi tận dụng một số metric nguồn ở mức độ chi tiết phương thức để huấn luyện tám mô hình học máy khác nhau. Kết quả của chúng tôi cho thấy các mô hình Decision Tree và Random Forest vượt trội hơn các mô hình khác về một số metric đánh giá của chúng tôi. Sau đó chúng tôi sử dụng CodeBERT LLM đã được huấn luyện trước để nghiên cứu hiệu quả của biểu diễn mã nguồn trong việc dự đoán mức độ nghiêm trọng lỗi. Việc tinh chỉnh CodeBERT cải thiện kết quả dự đoán mức độ nghiêm trọng lỗi một cách đáng kể trong khoảng 29%-140% cho một số metric đánh giá, so với mô hình dự đoán cổ điển tốt nhất trên metric mã nguồn. Cuối cùng, chúng tôi tích hợp các metric mã nguồn vào CodeBERT như một đầu vào bổ sung, sử dụng hai kiến trúc được đề xuất của chúng tôi, cả hai đều tăng cường hiệu quả của mô hình CodeBERT.

Từ khóa chỉ mục — Dự đoán mức độ nghiêm trọng lỗi, Biểu diễn mã, Mô hình Ngôn ngữ Lớn (LLMs), CodeBERT, Metric mã.

I. GIỚI THIỆU

Bảo trì phần mềm bao gồm xử lý lỗi là một trong những phần thách thức nhất của chu kỳ phát triển phần mềm [2]. Các bước khác nhau trong xử lý lỗi như phát hiện lỗi, định vị lỗi, và sửa lỗi đòi hỏi tài nguyên đáng kể (ví dụ, đội ngũ kỹ thuật, thời gian, v.v.), và hầu hết các bước này đang được thực hiện thủ công hoặc bán tự động trong thực tế [3], [4]. Do đó, cả các nhà nghiên cứu và người thực hành đều cố gắng tự động hóa những nhiệm vụ tẻ nhạt này từ các góc độ ứng dụng khác nhau như dự đoán khiếm khuyết, định vị lỗi, tạo test, và sửa chữa chương trình [5]–[9].

Mặc dù đã có nhiều nghiên cứu về xử lý lỗi sử dụng các kỹ thuật khác nhau như dựa trên tìm kiếm [10], dựa trên mẫu [11], và kỹ thuật dựa trên ML [9], hầu hết chúng ngầm định rằng tất cả các lỗi đều có tầm quan trọng như nhau điều này không đúng trong thực tế [9], [12]–[14]. Mức độ nghiêm trọng lỗi là một trong những đặc điểm lỗi được gán thủ công bởi QA/developers trong các giai đoạn báo cáo lỗi hoặc phân loại. Mức độ nghiêm trọng lỗi cho biết cường độ tác động của lỗi đối với hoạt động hệ thống [15], vì vậy nó hướng dẫn các đội kỹ thuật điều chỉnh ưu tiên của họ về việc lỗi nên được sửa nhanh như thế nào [16].

Ước tính mức độ nghiêm trọng của một lỗi nhất định, một cách tự động và có hệ thống (không thiên vị theo ý kiến của người báo cáo lỗi), có thể khá hữu ích, không chỉ cho các đội kỹ thuật (bao gồm developers và đội QA) mà còn như một đầu vào quan trọng cho các công cụ kỹ thuật phần mềm tự động khác, như sửa chữa chương trình, để ưu tiên quy trình của họ, tương ứng.

Động lực: Theo hiểu biết tốt nhất của chúng tôi, tài liệu hiện tại trong lĩnh vực dự đoán mức độ nghiêm trọng lỗi chủ yếu giới hạn ở các nghiên cứu dựa trên mô tả báo cáo lỗi sử dụng các kỹ thuật NLP (Natural Language Processing) khác nhau [17], [18]. Những kỹ thuật này có một số hạn chế, như sau: (a) cho rằng các mức độ nghiêm trọng được dự đoán chỉ được ước tính dựa trên các mức độ nghiêm trọng đã báo cáo cho một lỗi mới, chúng có thể thiên vị theo ý kiến chủ quan của người báo cáo (không phải lúc nào cũng là chuyên gia); (b) Chúng bỏ qua ngữ cảnh (mã nguồn cơ bản). Tức là, một báo cáo lỗi tương tự cho mã nguồn khác nhau có thể có mức độ nghiêm trọng khác nhau; (c) Những phương pháp này chỉ giới hạn ở các dự án với báo cáo lỗi phong phú và thông tin không áp dụng được trong nhiều trường hợp; và (d) Không phải tất cả các lỗi được phát hiện đều có báo cáo lỗi được viết cho chúng, thậm chí trong các dự án có lịch sử lỗi phong phú. Nhiều lần, một lỗi mới có thể được phát hiện bởi một test case và không có báo cáo nào được gán cho nó để sử dụng cho dự đoán mức độ nghiêm trọng. Báo cáo sẽ được thêm vào cuối cùng sau khi lỗi được sửa hoặc được gán để sửa nhưng mức độ nghiêm trọng cần thiết trước đó.

Những thách thức trên thúc đẩy chúng tôi đề xuất một kỹ thuật cho dự đoán mức độ nghiêm trọng lỗi không yêu cầu báo cáo lỗi. Đây thực tế là một vấn đề khó giải quyết hơn so với các lựa chọn thay thế dựa trên báo cáo lỗi, vì (a) các công cụ thông thường trong tay để dự đoán (như transformers và LLMs) phù hợp hơn khi làm việc với ngôn ngữ tự nhiên so với mã nguồn và (b) khi một tester/developer/hoặc thậm chí người dùng viết báo cáo lỗi, họ đã cung cấp rất nhiều thông tin bổ sung về lỗi bao gồm thông tin có thể trực tiếp hoặc gián tiếp ngụ ý mức độ nghiêm trọng. Do đó, một phương pháp không giả định sự tồn tại của báo cáo lỗi đang giải quyết một vấn đề khó khăn hơn.

Cũng có một số công trình hạn chế về việc sử dụng các metric mã nguồn cho dự đoán mức độ nghiêm trọng lỗi [19]–[21] chỉ dựa trên các metric mã nguồn ở mức class. Tuy nhiên, các nhà nghiên cứu/developers thấy mức độ chi tiết class/module quá thô cho việc sử dụng thực tế [22]–[25] (kết quả ở mức class, ví dụ, class nào có lỗi rủi ro hơn, thường được developers biết trước). Ngoài ra, các estimators của những kỹ thuật này không phải là state of the art ngày nay (kỹ thuật từ ít nhất 10 năm trước).

Để khắc phục những hạn chế đã đề cập và nâng cao state of the art, trong bài báo này, chúng tôi cung cấp một phương pháp tự động tận dụng tiềm năng của 1) các metric mã nguồn ở mức phương thức, 2) biểu diễn mã nguồn (code embedding) được cung cấp bởi các mô hình ngôn ngữ lớn được huấn luyện trước (LLMs) cho mã, và 3) tích hợp biểu diễn mã nguồn và các metric mã nguồn để dự đoán giá trị mức độ nghiêm trọng lỗi. Gói tái tạo của chúng tôi được cung cấp trong kho lưu trữ trực tuyến [26].

Chúng tôi tận dụng học biểu diễn mã nguồn (còn được gọi là feature learning) vì chúng đã cho thấy cải thiện đáng kể so với các mô hình cổ điển bằng cách tự động khám phá các đặc trưng/biểu diễn của mẫu để thực hiện các nhiệm vụ downstream được yêu cầu. Trong các nghiên cứu gần đây, LLMs cho mã như CodeBERT [27] (một mô hình ngôn ngữ lớn dựa trên transformers) đã cho thấy kết quả hứa hẹn trên một số nhiệm vụ liên quan đến phần mềm bao gồm phát hiện lỗi, sửa chữa chương trình, và phát hiện clone [9], [28].

Đóng góp và Tính mới: Trong nghiên cứu này, chúng tôi tận dụng 19 dự án mã nguồn mở chứa 3,342 phương thức buggy. Chúng tôi đã huấn luyện tám mô hình học máy cổ điển (KNN, SVM, Naive Bayes, Decision Tree, Random Forest, Ada Boost, XGBoost, và MLP) trên các metric mã nguồn được trích xuất và đánh giá kết quả trên một số metric hiệu suất. Kết quả cho thấy Random Forest có hiệu suất tốt nhất. Kết quả thí nghiệm của chúng tôi trên mô hình CodeBERT, sử dụng biểu diễn mã nguồn, cho thấy cải thiện đáng kể (29%-140% trên các metric khác nhau) so với các phương pháp trước đây. Cuối cùng, chúng tôi đề xuất tích hợp các metric mã nguồn và biểu diễn mã nguồn bằng cách cung cấp hai kiến trúc, trong đó các metric mã nguồn được cung cấp cho mô hình như (a) một đầu vào bổ sung dạng văn bản, và (b) một vector số. Phương pháp này cho thấy cải thiện (2%-10% trên các metric khác nhau) so với hiệu quả của mô hình CodeBERT.

Tính mới chủ yếu của công trình này là sử dụng LLM cho dự đoán mức độ nghiêm trọng lỗi mà không cần báo cáo lỗi, kèm theo các template được đề xuất của chúng tôi về cách kết hợp mã nguồn và metric ở mức phương thức với LLM để có kết quả tốt nhất.

Tóm lại, những đóng góp chính của nghiên cứu này bao gồm:

• Điều tra tám mô hình học máy cổ điển được huấn luyện trên 10 metric mã nguồn ở mức phương thức để dự đoán mức độ nghiêm trọng lỗi của hai bộ dữ liệu phổ biến, như baseline.

• Khám phá một LLM cho mã (CodeBERT) trong việc dự đoán nhãn mức độ nghiêm trọng lỗi chỉ sử dụng mã nguồn phương thức buggy.

• Đề xuất một phương pháp mới (hai kiến trúc khác nhau) để tích hợp các metric mã nguồn với biểu diễn mã nguồn được cung cấp bởi CodeBERT.

II. KIẾN THỨC NỀN TẢNG

A. Metric Mã nguồn

Các metric mã nguồn đã được sử dụng cho các ứng dụng liên quan đến phần mềm khác nhau như phát hiện code smell [29], ước tính nỗ lực bảo trì [30] và phát hiện khiếm khuyết [31]. Các mức độ chi tiết khác nhau như mức package/class [32]–[34], mức method [23], [24], [35]–[38], và mức line [39] đã được áp dụng trong nghiên cứu trước đây. Mức độ chi tiết cao (package/class) thực tế ít hữu ích hơn cho developers [23], [35] vì nó đòi hỏi nỗ lực đáng kể để định vị lỗi ở các thành phần package/class. Ngoài ra, mức độ chi tiết line có thể gặp phải quá nhiều false positives, bởi vì nhiều dòng có thể giống nhau chỉ do tình cờ [40], [41]. Do đó, mức độ chi tiết method đã trở thành trọng tâm mới của cộng đồng [23], [24], [35]–[38], đặc biệt là cho các mô hình dự đoán lỗi, và một số nghiên cứu cho thấy kết quả tích cực và khuyến khích [35], [36], [38].

Do vấn đề đã đề cập, trong nghiên cứu này, chúng tôi sử dụng mức độ chi tiết method được mô tả trong phần này. Những metric mã này có các ưu điểm khác nhau (ví dụ, tính toán nhanh) và nhược điểm (ví dụ, yêu cầu parser đặc thù ngôn ngữ). Chúng tôi đảm bảo có hầu hết các metric đã được chứng minh hiệu quả trong việc dự đoán buginess ở mức method, trong các nghiên cứu trước đây. Những metric này được định nghĩa như sau:

Lines of Code (LC) còn được gọi là Size là metric mã phổ biến nhất, dễ đo lường nhất, và hiệu quả nhất để ước tính bảo trì phần mềm [37], [42], [43]. Trong nghiên cứu này, chúng tôi tính LC như các dòng mã nguồn không có comment và dòng trống, tương tự như [37], [44], [45] để ngăn chặn tác động của định dạng mã và comment, nằm ngoài phạm vi của nghiên cứu này.

McCabe (MA) còn được gọi là cyclomatic complexity [44], [46], là một metric phổ biến khác chỉ ra số lượng đường dẫn độc lập, và do đó là độ phức tạp logic của một chương trình. Trực quan, các thành phần có giá trị McCabe cao dễ bị lỗi hơn.

McClure (ML) được đề xuất như một cải tiến so với McCabe [2], [47]. Không giống như McCabe, McClure xem xét số lượng biến điều khiển, và số lượng so sánh trong một predicate, không được hỗ trợ bởi McCabe.

McCabe và McClure không xem xét độ sâu lồng nhau. Nested Block Depth (NBD) [48]–[50] đã được nghiên cứu cùng với chúng để giảm thiểu vấn đề này.

Vì các thước đo độ phức tạp kiểu McCabe yêu cầu parser đặc thù ngôn ngữ (để tìm predicates), Hindle et al. [51] đã đề xuất Proxy Indentation (PI) như một proxy cho các metric độ phức tạp kiểu McCabe.

FanOut (FO) tính tổng số phương thức được gọi bởi một phương thức nhất định. Điều này cung cấp ước tính về coupling, tức là phụ thuộc của một phương thức cụ thể vào các phương thức khác.

Readability (R) kết hợp các đặc trưng mã khác nhau để tính một giá trị duy nhất để ước tính khả năng đọc mã. Chúng tôi đã sử dụng metric readability được đề xuất bởi Buse et al. [52] tạo ra điểm readability cho một phương thức nhất định. Điểm readability nằm trong khoảng từ 0 đến 1 để xác định mã ít đọc được nhất đến mã đọc được nhất, tương ứng.

Halstead Metrics chứa bảy thước đo dựa trên số lượng toán tử và toán hạng trong một thành phần [53]. Vì tất cả các metric Halstead đều có tương quan cao với nhau, trong nghiên cứu này, chúng tôi chỉ xem xét hai trong số chúng: Difficulty (D) và Effort (E) sử dụng các metric Halstead khác trong công thức của chúng, như sau:

E=D∗V D =n1/2∗N2/n2
N=N1 +N2 n=n1 +n2
V(HalsteadVolume) =N∗log2(n)

Trong đó n1 và n2 là số lượng toán tử và toán hạng riêng biệt, và N1 và N2 là tổng số toán tử và toán hạng, tương ứng.

Maintainability Index (MI) đã được giới thiệu bởi Omran và Hagemeister [54] trong đó các tác giả đã định nghĩa các metric để đo lường khả năng bảo trì của một hệ thống phần mềm và kết hợp những metric đó thành một giá trị duy nhất. MI có thể được tính như:

MI= 171 −5.2×ln(HalsteadVolume)−
0.23×(McCabe)−16.2×ln(LC)

Trong đó HalsteadVolume và LC được định nghĩa trước đây trong phần này.

B. Representation Learning

CodeBERT [27] là một mô hình ngôn ngữ lớn được huấn luyện trước (LLM) hỗ trợ cả ngôn ngữ lập trình (PL) và ngôn ngữ tự nhiên (NL). Nó tuân theo kiến trúc của các mô hình BERT [55] và RoBERTa [56] và sử dụng một transformer đa lớp hai chiều [57]. CodeBERT có cùng kiến trúc mô hình với RoBERTa-base với 125M tham số. Nó đã được huấn luyện trên 6.4M cặp NL-PL của 6 ngôn ngữ lập trình như Python, Java, JavaScript, PHP, Ruby, và Go từ bộ dữ liệu CodeSearchNet [58]. Mô hình này đã được huấn luyện theo hai mục tiêu, masked language modeling (MLM) và replaced token detection (RTD). Định dạng đầu vào tuân theo việc nối hai đoạn (phần đầu chứa ngôn ngữ tự nhiên, và phần thứ hai chứa mã) với token phân cách: [CLS], w1, w2, ..., wn, [SEP], c1, c2, ..., cm, [EOS]. Đầu ra bao gồm biểu diễn vector ngữ cảnh của cả đoạn NL và PL, và biểu diễn [CLS] có thể được sử dụng như biểu diễn chuỗi tổng hợp cho các nhiệm vụ khác nhau như ranking hoặc classification.

CodeBERT đã được sử dụng cho các nhiệm vụ kỹ thuật phần mềm khác nhau như [9] trong đó các tác giả sử dụng mô hình CodeBERT để tạo ra các bản sửa cho các lỗi Java đơn giản với độ chính xác trong khoảng 19-72%. Pan et al. [59] tận dụng CodeBERT cho phát hiện lỗi trong các thiết lập cross-version và cross-project và họ cũng điều tra tác động của các mẫu dự đoán khác nhau. Zhang et al. [60] đã sử dụng CodeBERT để tăng cường hiệu suất của nhiệm vụ tạo tiêu đề câu hỏi bằng cách sử dụng thông tin bi-modal tồn tại trong nội dung câu hỏi. Họ đề xuất rằng phương pháp này vượt trội hơn các mô hình trước đây dưới các thiết lập khác nhau trong khoảng 4%-14%.

III. THIẾT LẬP NGHIÊN CỨU

A. Thu thập Dữ liệu

Chúng tôi sử dụng các bộ dữ liệu Defects4J [61] và Bugs.jar [62] chứa các lỗi thực từ các dự án Java mã nguồn mở phổ biến khác nhau. Defects4J được chọn do tính đa dạng của nó bằng cách có các dự án trong các lĩnh vực khác nhau và cũng do sự phổ biến của nó trong các lĩnh vực nghiên cứu kỹ thuật phần mềm tự động khác nhau như Fault Localization [14], Program Repair [63], và Test Generation [12]. Bugs.jar là một bộ dữ liệu phổ biến khác chứa các dự án mã nguồn mở lớn đã được sử dụng rộng rãi trong Program Repair [64].

Phiên bản mới nhất của Defects4J (2.0.0) chứa 835 lỗi từ 17 dự án Java mã nguồn mở. Sau khi trích xuất các lỗi có nhãn mức độ nghiêm trọng, chúng tôi kết luận với 510 lỗi từ các dự án: Chart, Cli, Closure, Codec, Collections, Compress, Csv, JxPath, Lang, Math, và Time. Bộ dữ liệu Bugs.jar chứa 1,158 lỗi từ tám dự án Java mã nguồn mở lớn và phổ biến: Accumulo, Camel, Commons-math, Flink, Jackrabbit-oak, Logging-log4j2, Maven, và Wicket.

Nghiên cứu của chúng tôi yêu cầu các phương thức buggy cùng với nhãn mức độ nghiêm trọng của chúng. Do đó, chúng tôi thu thập nhãn mức độ nghiêm trọng của mỗi lỗi từ các hệ thống quản lý issue tương ứng. Bộ dữ liệu defects4J chứa các lỗi từ Jira, Google Code Archive, và Source Forge, nhưng bộ dữ liệu Bugs.jar chỉ chứa các lỗi từ Jira. Để trích xuất nhãn mức độ nghiêm trọng, chúng tôi trích xuất id issue của một lỗi nhất định (mỗi lỗi trong cả hai bộ dữ liệu đều có id issue duy nhất), và sau đó lấy nhãn mức độ nghiêm trọng của nó từ các hệ thống quản lý issue tương ứng. Để trích xuất nhãn mức độ nghiêm trọng từ Jira, chúng tôi sử dụng "Python Jira library" [65], cho Source Forge và Google Code Archive chúng tôi phân tích trang web chứa issue và trích xuất nhãn mức độ nghiêm trọng. Mã liên quan đến thu thập dữ liệu có sẵn trong gói tái tạo được cung cấp.

Chúng tôi thống nhất tất cả các nhãn mức độ nghiêm trọng phân loại và số thành định dạng số trong khoảng từ 0 đến 3 (0 nghiêm trọng nhất, 3 ít nghiêm trọng nhất) để sử dụng chúng như nhãn mục tiêu trong mục đích huấn luyện của chúng tôi theo Bảng I. Chúng tôi cũng hợp nhất hai bộ dữ liệu và sau đó xáo trộn các mẫu để có số lượng mẫu tương đối lớn hơn (tốt hơn cho việc huấn luyện các mô hình của chúng tôi) và cũng đạt được một mô hình có khả năng xử lý các lỗi đa dạng từ các dự án khác nhau và các lĩnh vực khác nhau thay vì có một mô hình được huấn luyện cụ thể cho mỗi bộ dữ liệu/dự án.

BẢNG I: Thống nhất nhãn mức độ nghiêm trọng được trích xuất từ các hệ thống quản lý issue thành bốn mức độ nghiêm trọng số.

Nhãn Mức độ Nghiêm trọng Được Lấy | Nhãn Lớp
Critical, Blocker | 0
Major, High | 1
Medium | 2
Low, Trivial, Minor | 3

Mỗi dự án buggy trong các bộ dữ liệu đã đề cập thể hiện một lỗi được sửa trong phiên bản sửa tương ứng của nó. Vì trọng tâm nghiên cứu của chúng tôi là mức độ chi tiết method, chúng tôi xem xét tất cả các phương thức bị ảnh hưởng trong quá trình patch sửa lỗi như phương thức buggy, tương tự như các nghiên cứu trước đây (ví dụ, [23], [37], [38]). Cuối cùng, sau khi hợp nhất hai bộ dữ liệu này và loại bỏ các instance trùng lặp (các phương thức buggy giống nhau xuất hiện trong một vài dự án/bộ dữ liệu) chúng tôi đạt được 3,342 phương thức buggy. Cho quá trình huấn luyện, chúng tôi chia bộ dữ liệu của mình ngẫu nhiên thành ba phần train, validation, và test khác nhau sử dụng tỷ lệ 70%, 15%, và 15% tương ứng. Lưu ý rằng trong trường hợp của chúng tôi, việc chia ngẫu nhiên không được coi là rò rỉ dữ liệu, vì mức độ nghiêm trọng lỗi không tương quan theo thời gian (biết một nhãn mức độ nghiêm trọng trong tương lai sẽ không đưa ra gợi ý bổ sung để dự đoán một nhãn mức độ nghiêm trọng hiện tại so với biết một nhãn mức độ nghiêm trọng trong quá khứ). Tuy nhiên, trong thực tế, người ta chỉ có thể sử dụng dữ liệu quá khứ để huấn luyện (chúng tôi không thể thực hiện thí nghiệm theo cách này vì không phải tất cả các mục dữ liệu của chúng tôi đều có time stamps). Thống kê bộ dữ liệu của chúng tôi bao gồm số lượng instance trong mỗi dự án, các phần train, validation, và test liên quan đến nhãn mức độ nghiêm trọng được cung cấp trong Bảng II.

BẢNG II: Thống kê bộ dữ liệu bao gồm số lượng mẫu với nhãn mức độ nghiêm trọng khác nhau trong mỗi dự án, toàn bộ bộ dữ liệu, các phần train, validation, và test.

Dự án | Lớp 0 | Lớp 1 | Lớp 2 | Lớp 3
Chart | 0 | 0 | 5 | 3
Cli | 2 | 36 | 0 | 12
Closure | 1 | 28 | 269 | 5
Codec | 1 | 16 | 0 | 3
Collections | 0 | 4 | 0 | 0
Compress | 2 | 47 | 0 | 15
CSV | 0 | 11 | 0 | 3
JxPath | 1 | 35 | 0 | 4
Lang | 7 | 56 | 0 | 23
Math | 11 | 87 | 0 | 34
Time | 0 | 1 | 17 | 3
Accumulo | 54 | 146 | 0 | 115
Camel | 8 | 183 | 0 | 111
Commons-math | 24 | 233 | 0 | 66
Flink | 61 | 82 | 0 | 24
Jackrabbit-oak | 82 | 518 | 0 | 96
Logging-log4j | 5 | 84 | 0 | 19
Maven | 7 | 56 | 0 | 15
Wicket | 9 | 459 | 0 | 143
Tất cả | 275 | 2082 | 291 | 694
Train | 198 | 1524 | 203 | 489
Validation | 30 | 264 | 40 | 92
Test | 47 | 294 | 48 | 113

B. Tính toán Metric Mã nguồn

Chúng tôi thấy rằng nhiều phương thức buggy trong bộ dữ liệu chứa comment mã. Vì việc nghiên cứu tác động của comment mã đối với chủ đề nghiên cứu của chúng tôi nằm ngoài phạm vi của nghiên cứu này, chúng tôi loại bỏ bất kỳ comment nào (như JavaDoc hoặc định dạng inline) tồn tại trong tất cả các mẫu của chúng tôi.

Hơn nữa, sau khi tính toán tất cả các metric mã nguồn, để loại bỏ vấn đề scaling tiềm ẩn của dữ liệu số, chúng tôi sử dụng RobustScaler [66]. Chúng tôi đã thực hiện bước tiền xử lý này vì các giá trị metric mã nguồn được tính toán của chúng tôi không có cùng phạm vi, vì vậy chúng tôi chuẩn hóa chúng bằng cách sử dụng thuật toán RobustScaler, chuẩn hóa dữ liệu đầu vào và cũng mạnh mẽ chống lại outliers. Thuật toán này tuân theo thuật toán tương tự như scale MinMax, nhưng nó sử dụng khoảng interquartile thay vì min-max [66].

C. Metric Đánh giá

Vì bộ dữ liệu của chúng tôi không cân bằng (số lượng lỗi trong mỗi danh mục mức độ nghiêm trọng không bằng nhau), giống như nhiều bộ dữ liệu thực tế, chúng tôi không thể chỉ dựa vào metric accuracy. Do đó, chúng tôi đánh giá thí nghiệm của mình bằng cách sử dụng một số metric mạnh mẽ chống lại bộ dữ liệu không cân bằng như sau:

Các metric đầu tiên để báo cáo là Precision và Recall. Vì chúng tôi có các classifier đa lớp (các lớp mục tiêu của chúng tôi là 0,1,2,3 được cung cấp trong Bảng I nên các metric đánh giá được cung cấp như precision, recall, và ROC-AUC được tính toán sử dụng phiên bản "weighted" thay vì "binary" mặc định. Phương pháp này tính toán metric cho mỗi nhãn và sau đó tìm trung bình có trọng số của chúng bằng support (số lượng instance thực cho mỗi nhãn) có tính đến sự mất cân bằng nhãn.

F1-Weighted: F-measure hoặc F-score là metric đánh giá phổ biến trong các nhiệm vụ classification xem xét cả precision và recall. F1 là dạng phổ biến nhất của F-measure, lấy trung bình hòa của precision và recall. Tuy nhiên, chúng tôi không thể sử dụng điểm F1 trực tiếp cho các vấn đề đa lớp, và vì bộ dữ liệu của chúng tôi cũng không cân bằng, metric F1-weighted được sử dụng. Nó được tính bằng cách lấy trung bình của tất cả điểm F1 mỗi lớp và xem xét số lượng mẫu của mỗi lớp.

ROC Curve Plot: Receiver Operator Characteristic (ROC) là một đường cong vẽ True Positive Rate (TPR) so với False Positive Rate (FPR) ở các giá trị ngưỡng khác nhau. FPR được định nghĩa là FP chia cho FP+TN.

Sử dụng biểu đồ này, một mô hình hoàn hảo được đại diện bởi một đường từ dưới trái đến trên trái và sau đó qua phần trên phải của biểu đồ. Một mô hình có kỹ năng được đại diện bởi một đường cong cong lên phần trên trái của biểu đồ. Tuy nhiên, một classifier ngẫu nhiên (không có kỹ năng) được đại diện bởi một đường chéo từ dưới trái đến trên phải của biểu đồ.

AUC-ROC: Area Under the Curve (AUC) là thước đo khả năng của một classifier trong việc phân biệt giữa các lớp mục tiêu. Classifier hoàn hảo có điểm AUC là 1 có nghĩa là nó phân biệt tốt giữa các lớp positive và negative trong khi classifier với điểm AUC là 0.5 có nghĩa là nó hoạt động như một classifier ngẫu nhiên. Điểm AUC là 0 có nghĩa là các dự đoán của classifier sai 100%.

Precision-Recall Curve Plot: Đường cong precision-Recall cho thấy sự đánh đổi giữa precision và recall cho các ngưỡng khác nhau. Điểm ở phần trên phải cho thấy một classifier hoàn hảo trong khi classifier không có kỹ năng (ngẫu nhiên) sẽ là một đường ngang trên biểu đồ với precision tỷ lệ thuận với số lượng ví dụ positive [67].

MCC: Matthews Correlation Coefficient (MCC) [68] đánh giá hiệu suất của một classifier bằng cách tận dụng tất cả các giá trị có sẵn trong confusion matrix giúp nó hữu ích cho các bộ dữ liệu không cân bằng. Nó được tính như dưới đây:

TP∗TN−FP∗FN/√((TP+FP)∗(TP+FN)∗(TN+FP)∗(TN+FN))

Giá trị MCC nằm giữa -1 và +1 trong đó +1 đại diện cho dự đoán hoàn hảo, 0 đại diện cho dự đoán ngẫu nhiên (không có kỹ năng), và -1 đại diện cho dự đoán nghịch đảo.

Lưu ý rằng tất cả các metric đánh giá bao gồm các phiên bản weighted được triển khai sử dụng thư viện metric tiêu chuẩn của Scikit-learn, trong đó công thức chính xác cho mỗi metric cũng có thể được tìm thấy trong tài liệu của nó [69]

IV. THÍ NGHIỆM

A. RQ1: Các metric mã nguồn ở mức phương thức có thể ước tính mức độ nghiêm trọng lỗi không?

Mục tiêu: Trong RQ này, chúng tôi nhằm nghiên cứu các metric mã nguồn được chọn của chúng tôi, đã cho thấy kết quả đáng kể trong các nhiệm vụ phát hiện khiếm khuyết trong các nghiên cứu trước đây, và khả năng của chúng trong việc dự đoán mức độ nghiêm trọng lỗi sử dụng một số mô hình học máy. Theo hiểu biết tốt nhất của chúng tôi, không có nghiên cứu trước đây về dự đoán nhãn mức độ nghiêm trọng lỗi trên các bộ dữ liệu Defects4J và Bugs.jar (các bộ dữ liệu tương đối nhỏ của các dự án thực tế mã nguồn mở) sử dụng các metric mã nguồn ở mức phương thức. Động lực của chúng tôi đằng sau RQ này là đánh giá khả năng của các mô hình học máy trên tập hợp các dự án đa dạng sử dụng mức độ chi tiết method (vì nó hiệu quả hơn cho người thực hành để xử lý lỗi ở mức này so với mức độ chi tiết class/package và dự đoán khiếm khuyết ở mức line chưa đủ chín).

Thiết kế: Chúng tôi huấn luyện tám mô hình học máy như KNN, SVM, naive Bayes, Decision Tree, RandomForest, XgBoost, AdaBoost, và MLP bằng cách sử dụng các metric mã nguồn của chúng tôi được mô tả trong phần II-A. Bằng cách chọn tám mô hình này, chúng tôi bao phủ một tập hợp đa dạng các thuật toán classification đã cho thấy kết quả hứa hẹn trong nghiên cứu/thực hành cho các nhiệm vụ classification khác nhau. Vì việc điều chỉnh hyper-parameter nằm ngoài phạm vi của nghiên cứu này, chúng tôi giữ các hyper-parameter mặc định (có sẵn trong triển khai thư viện Scikit-learn [70]) cho tất cả các mô hình này.

Lưu ý rằng chúng tôi coi RQ1 là baseline so sánh cho hai RQ tiếp theo. Mặc dù áp dụng chính xác các mô hình giống nhau trên chính xác các bộ dữ liệu giống nhau, không được thực hiện trước đây và là đóng góp của chúng tôi, nhưng các phương pháp cổ điển như vậy đã được sử dụng rộng rãi trong dự đoán khiếm khuyết và trong các trường hợp hạn chế trong dự đoán mức độ nghiêm trọng lỗi ở mức class, trong quá khứ. Do đó, việc coi chúng là baseline so sánh của chúng tôi là công bằng.

Kết quả: Bảng III cho thấy kết quả của các mô hình nghiên cứu của chúng tôi liên quan đến các metric đánh giá khác nhau (do giới hạn không gian trong bài báo này, chúng tôi cung cấp confusion matrix, ROC, và đường cong Precision-Recall của tám mô hình này trong kho lưu trữ mã được cung cấp). Trong nghiên cứu này, chúng tôi chỉ báo cáo F1 cho mỗi lớp (F1 Perclass) cho thấy hiệu quả của mô hình cho mỗi lớp mục tiêu (các nhóm mức độ nghiêm trọng khác nhau) riêng biệt, nhưng cung cấp các phiên bản "weighted" của Precision, Recall, AUC, và MCC vì phương pháp "weighted" tính đến tất cả các lớp mục tiêu (các nhóm mức độ nghiêm trọng khác nhau) và cung cấp ước tính tốt hơn về hiệu quả của mô hình trong việc dự đoán các nhóm mức độ nghiêm trọng khác nhau. Vì tất cả các mô hình đều có giá trị AUC >0.5, chúng tôi có thể nói rằng chúng hoạt động tốt hơn một classifier ngẫu nhiên. Tuy nhiên, như được hiển thị trong bảng (được làm nổi bật bằng kiểu in đậm), mô hình Random Forest có hiệu quả tốt nhất trong việc dự đoán nhãn mức độ nghiêm trọng lỗi bằng cách có kết quả tốt hơn sử dụng tất cả các metric đánh giá được cung cấp. Ngược lại, mô hình SVM có một trong những hiệu suất yếu nhất trong việc dự đoán giá trị mức độ nghiêm trọng, dựa trên bất kỳ metric nào. Theo giá trị MCC của nó, mô hình này hoạt động như classifier không có kỹ năng (ngẫu nhiên) (ít nhất với các tham số mặc định của thư viện Scikit-learn). Metric F1 per-class cho thấy rằng mô hình SVM chỉ có thể dự đoán "lớp 1 (Major, High) severity" chính xác và bỏ lỡ các nhóm mức độ nghiêm trọng khác. Hơn nữa, giá trị precision thấp của nó (0.34) cho thấy rằng nó trả về rất nhiều false positives. Một lý do có thể đằng sau kết quả tương đối kém của các mô hình SVM và Naive Bayes là nhu cầu vốn có của chúng về việc có một tập hợp đặc trưng mạnh mẽ và đủ mẫu. Vì kích thước bộ dữ liệu của chúng tôi tương đối nhỏ để huấn luyện một mô hình từ đầu, nên việc tái tạo nghiên cứu này sử dụng các bộ dữ liệu/đặc trưng khác có thể cải thiện kết quả của các mô hình này.

Như đã đề cập, các nghiên cứu trước đây [19]–[21] đã tận dụng một số metric mã nguồn hướng đối tượng khác với các metric mã nguồn ở mức phương thức của chúng tôi. Tuy nhiên, chúng tôi không thể đơn giản so sánh kết quả RQ1 của chúng tôi với các công trình đã công bố này vì (1) chúng không ở mức phương thức, là mức thực tế hơn từ góc độ người thực hành [23], [24], và (2) chúng dự đoán trên các mức độ nghiêm trọng khác nhau (thường chỉ là các mức "high" và "low").

Tóm tắt RQ1: Trong số các mô hình cổ điển, các mô hình Random Forest (MCC=0.21) và SVM (MCC=0) có hiệu suất tốt nhất và yếu nhất, tương ứng.

B. RQ2: CodeBERT có thể dự đoán mức độ nghiêm trọng lỗi tốt như thế nào?

Mục tiêu: Mục đích của RQ này là nghiên cứu hiệu quả của biểu diễn mã được cung cấp bởi một mô hình ngôn ngữ lớn được huấn luyện trước có tên CodeBERT trong việc dự đoán mức độ nghiêm trọng lỗi. Các động lực đằng sau RQ này là: 1) các mô hình này có khả năng xuất sắc trong một loạt các nhiệm vụ liên quan đến phần mềm bao gồm phát hiện lỗi, phát hiện clone, và sửa chữa chương trình được thể hiện thành công trong các nghiên cứu trước đây [28], 2) vì các mô hình này đã được huấn luyện trên hàng triệu mẫu, nó cho phép chúng tôi tinh chỉnh chúng cho các nhiệm vụ downstream khác nhau (ví dụ, các nhiệm vụ dự đoán lỗi của chúng tôi) với các bộ dữ liệu tương đối nhỏ (như bộ dữ liệu của chúng tôi) so với việc huấn luyện mô hình từ đầu đòi hỏi các bộ dữ liệu lớn, 3) các mô hình này tận dụng biểu diễn của dữ liệu đầu vào, vì vậy không cần trích xuất đặc trưng từ dữ liệu đầu vào (như các metric mã được trích xuất trong RQ1), đây là một nhiệm vụ khó trong dữ liệu và ứng dụng phức tạp, và cuối cùng, 4) vì các mô hình này đã được huấn luyện trên một số ngôn ngữ lập trình, nghiên cứu của chúng tôi có thể dễ dàng được áp dụng cho các ngôn ngữ lập trình khác (được hỗ trợ bởi CodeBERT) mà không cần thay đổi gì.

Hơn nữa, CodeBERT hỗ trợ cả đầu vào ngôn ngữ lập trình và ngôn ngữ tự nhiên, vì vậy các đầu vào bổ sung như mô tả lỗi có thể được cung cấp như đầu vào bổ sung cho nhiệm vụ nghiên cứu của chúng tôi có thể cải thiện kết quả hơn nữa, có thể được nghiên cứu như một công trình tương lai tiềm năng.

Thiết kế: Chúng tôi chọn mô hình "codebert-base" từ Hugging Face [71] được khởi tạo với "roberta-base" và được huấn luyện trên dữ liệu bi-modal (tài liệu & mã) với các mục tiêu MLM (masked language modeling) và RTD (replaced token detection). Vì chúng tôi chỉ sử dụng mã nguồn làm đầu vào, chúng tôi không tận dụng phần NL (Natural Language) của đầu vào mô hình này.

Vì mô hình CodeBERT là một encoder, chúng tôi nối thêm một lớp RobertaClassificationHead, để làm cho nó có thể áp dụng cho nhiệm vụ classification của chúng tôi. Chúng tôi thêm lớp này vào CodeBERT để tuân theo kiến trúc có tên RobertaForSequenceClassification, được cung cấp bởi Hugging Face như một mô hình classification. Chúng tôi sử dụng lớp Softmax trong kiến trúc này vì chúng tôi có vấn đề classification đa lớp và cũng sử dụng CrossEntropyLoss để tính toán loss trong quá trình huấn luyện.

Để tinh chỉnh mô hình CodeBERT, trước tiên chúng tôi đóng băng các lớp của nó để ngăn chặn việc thay đổi trọng số của các lớp hiện có một cách mạnh mẽ và chỉ huấn luyện lớp classification được thêm vào của chúng tôi bằng một số lượng lớn epochs (40 epochs). Trong quá trình huấn luyện, chúng tôi sử dụng kỹ thuật early stopping với patience = 3 để ngăn chặn vấn đề over-fitting bằng cách xem xét giá trị loss của mô hình sử dụng tập validation. Trong bước tiếp theo, chúng tôi mở đóng băng các lớp của mô hình CodeBERT và tinh chỉnh toàn bộ kiến trúc bằng cách sử dụng learning rate nhỏ hơn và ít epochs hơn để điều chỉnh trọng số của tất cả các lớp. Trong bước này, chúng tôi sử dụng năm epochs, như được đề xuất trong nghiên cứu trước đây [55] để sử dụng một số lượng nhỏ epochs để ngăn chặn vấn đề over-fitting và không thay đổi trọng số mô hình được huấn luyện trước, một cách mạnh mẽ.

Trong toàn bộ quá trình huấn luyện, chúng tôi sử dụng phần train để huấn luyện mô hình và phần validation để đánh giá hiệu quả của mô hình, và chúng tôi cũng lưu checkpoint của mô hình với điểm F1-weighted tốt nhất. Cuối cùng, chúng tôi sử dụng phần test (không bao giờ được nhìn thấy trong quá trình huấn luyện) để đánh giá cuối cùng của mô hình.

Vì kích thước khối 512 là giá trị tối đa được hỗ trợ bởi CodeBERT, chúng tôi đệm đầu vào của mình với padding token bất cứ khi nào nó ngắn hơn 512 tokens, và bất cứ khi nào độ dài mẫu lớn hơn 512, chúng tôi cắt bớt nó bằng cách giữ 512 tokens đầu tiên.

Kết quả: Các đường cong ROC và Precision-Recall của mô hình CodeBERT được cung cấp trong Hình 1. Những biểu đồ này cho thấy rằng mô hình CodeBERT dự đoán nhãn mức độ nghiêm trọng "Lớp 2 (Medium)" rất tốt vì các đường cong ROC và Precision-Recall của lớp này rất gần với đường cong classifier hoàn hảo. Kết quả này có vẻ thú vị vì, mặc dù số lượng thấp của các instance "Lớp 2 (Medium)" trong bộ dữ liệu của chúng tôi (489 trong phần train), mô hình đã dự đoán hầu hết các instance lớp này (42 trong số 48) trong phần test một cách chính xác.

Bảng IV cho thấy kết quả của mô hình này liên quan đến các metric đánh giá của chúng tôi. Những kết quả này cho thấy hiệu quả hứa hẹn của CodeBERT trong việc dự đoán mức độ nghiêm trọng lỗi do các giá trị tương đối cao của tất cả các metric đánh giá. Bằng cách so sánh kết quả này với kết quả của tám mô hình khác nhau được cung cấp trong RQ1 (Bảng III), chúng tôi có thể nói rằng mô hình CodeBERT có hiệu quả tốt hơn đáng kể vì nó tăng các giá trị F1-Weighted, AUC, và MCC lần lượt là 0.16 (29%), 0.22 (33%), và 0.30 (140%) so với mô hình Random Forest (mô hình tốt nhất trong số tất cả các mô hình RQ1).

Hơn nữa, chúng tôi đã cung cấp confusion matrix của mô hình CodeBERT trong Hình 2 trong đó các nhãn thực tế và các nhãn được dự đoán được hiển thị cho mỗi lớp. Hình này cho thấy rằng CodeBERT đã gán nhãn sai 18 lỗi với mức độ nghiêm trọng "lớp 0"; nó dự đoán 15 trong số chúng là lỗi mức độ nghiêm trọng "lớp 1", có nghĩa là nó phát hiện các lỗi critical như các lỗi major severity, nhưng nó chỉ gán mức độ nghiêm trọng thấp hơn (medium, low) cho ba lỗi. Đối với các lỗi có nhãn "lớp 1", nó dự đoán mức độ nghiêm trọng chính xác trong 87% trường hợp. Nó cũng phân loại tất cả các lỗi với nhãn "lớp 2" một cách chính xác, ngoại trừ sáu lỗi mà nó gán giá trị mức độ nghiêm trọng cao hơn. Các lỗi với nhãn "lớp 3" được dự đoán có giá trị mức độ nghiêm trọng cao hơn trong nhiều trường hợp. Những phân loại sai này có thể tăng thời gian điều tra của đội developers/QA trong thực tế, nhưng nó không tăng rủi ro sử dụng kỹ thuật này trái ngược với các tình huống mà mô hình gán nhãn mức độ nghiêm trọng thấp hơn cho các lỗi thực tế (ví dụ, coi các lỗi critical như các lỗi low-severity). Nó cũng vẫn hoạt động tốt hơn phương pháp cổ điển tốt nhất (Random Forest) thậm chí về mặt phân loại sai cho lớp này.

Kết quả của chúng tôi cho thấy rằng biểu diễn mã có khả năng cao hơn trong việc dự đoán mức độ nghiêm trọng lỗi của các bộ dữ liệu đã nghiên cứu so với việc sử dụng các metric mã nguồn được chọn của chúng tôi, đơn thuần. Lý do tiềm năng đầu tiên là sức mạnh của ngữ nghĩa đã học đằng sau các embeddings mã nguồn và biểu diễn mã so với các metric mã nguồn được trích xuất có thể không có tương quan cao với mức độ nghiêm trọng lỗi, bất kể mô hình dự đoán. Lý do tiềm năng thứ hai là, trong các trường hợp mà kích thước bộ dữ liệu tương đối nhỏ, việc huấn luyện mô hình từ đầu có thể không dẫn đến kết quả tốt, nhưng việc sử dụng các mô hình được huấn luyện trước và tinh chỉnh cho nhiệm vụ downstream được yêu cầu hoạt động tốt vì nó đã được huấn luyện trên các bộ dữ liệu lớn trong giai đoạn huấn luyện trước.

Tóm tắt RQ2: CodeBERT vượt trội hơn Random Forest (mô hình tốt nhất của RQ1) lần lượt là 29%, 33%, và 140% cho F1-Weighted, AUC, và MCC.

C. RQ3: Việc cung cấp metric mã nguồn như các đầu vào bổ sung cho CodeBERT có cải thiện dự đoán mức độ nghiêm trọng lỗi của nó không?

Mục tiêu: Trong RQ này, chúng tôi nghiên cứu tác động của việc tích hợp các metric mã nguồn với các mô hình CodeBERT. Vì chúng tôi thấy rằng các metric mã nguồn đơn thuần không phải là chỉ báo tốt về mức độ nghiêm trọng lỗi trong các bộ dữ liệu của chúng tôi, động lực đằng sau RQ này là tìm hiểu khả năng của chúng như một đầu vào bổ sung (vì chúng thể hiện các đặc trưng quan trọng liên quan đến mã nguồn như độ phức tạp, khả năng đọc, khả năng bảo trì, v.v.) cho mô hình CodeBERT để cải thiện hiệu quả của mô hình.

Thiết kế: Vì CodeBERT đã được huấn luyện trên mã nguồn, nó có thể đơn giản hiểu đầu vào mã nguồn trong khi các metric mã nguồn được tính toán của chúng tôi là một số giá trị số (ví dụ, 1, 2) vì vậy thách thức của chúng tôi là tìm cách tích hợp những giá trị số này với đầu vào mã nguồn của chúng tôi theo cách mà CodeBERT có thể hiểu và tận dụng những dữ liệu này. Chúng tôi cung cấp hai kiến trúc khác nhau có tên "ConcatInline" và "ConcatCLS".

Đối với kiến trúc ConcatInline, chúng tôi chuyển đổi các đặc trưng metric mã nguồn của chúng tôi sang định dạng văn bản và cung cấp nó cùng với đầu vào mã nguồn cho mô hình. Có nhiều cách tiềm năng để thêm các đặc trưng số/phân loại vào mô hình CodeBERT. Ý tưởng của chúng tôi dựa trên nghiên cứu trước đây trong NLP trong đó các đặc trưng số được thêm vào BERT [55] cho classification văn bản ngắn [72]. Vì mô hình CodeBERT đã được huấn luyện trên dữ liệu văn bản và cũng xem xét ngữ cảnh xung quanh mỗi từ trong các giai đoạn huấn luyện/suy luận, chúng tôi cung cấp ngữ cảnh cho mỗi đặc trưng mã nguồn để làm cho toàn bộ đầu vào phong phú và thông tin. Theo cách này, thay vì cung cấp trực tiếp các giá trị metric mã thô, chúng tôi cung cấp một đoạn văn đơn giản chứa mô tả (dựa trên văn bản template) cho mỗi metric mã và cũng các giá trị metric mã số. Chúng tôi không chuyển đổi các giá trị metric mã sang định dạng chữ cái (ví dụ sử dụng "one" thay vì "1") vì định dạng chữ cái tăng độ dài văn bản đầu vào một cách mạnh mẽ và mô hình CodeBERT có hạn chế về kích thước đầu vào. Một ví dụ về phương pháp này được hiển thị trong Bảng V. Lưu ý rằng việc sử dụng các template khác cho ngữ cảnh có thể thay đổi kết quả, và phân tích độ nhạy thích hợp với template ngữ cảnh dành cho công việc tương lai của chúng tôi.

Sau khi tạo đoạn văn mô tả metric mã và các giá trị của chúng, chúng tôi đặt nó trong phần đầu tiên của đầu vào mô hình CodeBERT (trước token SEP) và sau đó cung cấp mã nguồn chứa phương thức buggy trong phần thứ hai của đầu vào mô hình (sau token SEP). Cấu trúc đầu vào cuối cùng cho CodeBERT trong kiến trúc này là: [CLS], CreatedParagraph, [SEP], SourceCode, [EOS]

Vì mô hình CodeBERT có thể hỗ trợ đầu vào lên đến 512 tokens, trong trường hợp đầu vào dài, chúng tôi giữ toàn bộ phần NL (ngôn ngữ tự nhiên chứa đoạn văn được tạo của metric mã) và cắt bớt phần PL (phần ngôn ngữ lập trình chứa mã nguồn phương thức buggy) từ cuối để giảm toàn bộ kích thước đầu vào xuống 512 tokens. Ngược lại, nếu độ dài của đầu vào cuối cùng nhỏ hơn 512, chúng tôi đệm đầu vào đến cuối với các token Pad.

Đối với kiến trúc ConcatCLS, chúng tôi tích hợp các vector metric mã nguồn số với token CLS đầu ra của mô hình CodeBERT được hiển thị trong Hình 3. Nói cách khác, trong kiến trúc này, chúng tôi nối đầu ra của mô hình CodeBERT (token CLS thể hiện biểu diễn mã nguồn) và các vector số của chúng tôi (đại diện cho metric mã nguồn) và đưa vector kết quả vào một lớp dense. Có kiến trúc này, trong giai đoạn back-propagation của quá trình tinh chỉnh, trọng số mô hình CodeBERT không chỉ được cập nhật theo biểu diễn mã nguồn mà chúng cũng được cập nhật dựa trên đóng góp của metric mã nguồn.

Kết quả: Các đường cong ROC và Precision-Recall của các thí nghiệm của chúng tôi sử dụng các kiến trúc ConcatInline và ConcatCLS được hiển thị trong Hình 4. Ngoài ra, kết quả liên quan đến các metric đánh giá khác được cung cấp trong Bảng VI. Kết quả của chúng tôi cho thấy rằng việc tích hợp metric mã nguồn trong định dạng văn bản (ConcatInline) tăng hiệu quả trong việc dự đoán các nhãn mức độ nghiêm trọng khác nhau của mô hình CodeBERT lần lượt là 0.03, 0.01, và 0.04 cho các metric F1-Weighted, AUC, và MCC. Ngoài ra, các đường cong ROC được cung cấp của kiến trúc ConcatInline cho thấy rằng việc thêm metric mã nguồn tăng khả năng của mô hình CodeBERT để dự đoán nhãn mức độ nghiêm trọng "lớp 0 (Critical, Blocker)" lên 0.02 so với kết quả RQ2. Sự cải thiện này cho thấy tác động quyết định của metric mã nguồn của chúng tôi trong việc dự đoán giá trị mức độ nghiêm trọng lỗi, đặc biệt là cho các lỗi nghiêm trọng cao.

Kiến trúc ConcatCLS đã tăng hiệu quả của mô hình CodeBERT lần lượt là 0.04, và 0.05 cho các metric F1-Weighted và MCC, nhưng giá trị AUC không được cải thiện. Ngoài ra, từ giá trị metric F1-Perclass, chúng tôi thấy rằng kết quả được cải thiện trong tất cả các lớp. Hình 5 cho thấy confusion matrix của cả hai kiến trúc ConcatInline và ConcatCLS. Bằng cách so sánh những hình này với confusion matrix của mô hình CodeBERT (Hình IV) chúng tôi thấy rằng mô hình ConcatInline hoạt động tốt hơn mô hình CodeBERT cho tất cả các lớp ngoại trừ nhãn "lớp 0" trong đó số lượng phân loại chính xác giảm một. Mô hình ConcatCLS đã cải thiện số lượng phân loại chính xác trong tất cả các lớp (lần lượt là 1, 1, 5, 7, cho lớp 0, đến 3) so với mô hình CodeBERT.

Kết quả thú vị từ các confusion matrices là kiến trúc ConcatCLS đã cải thiện hiệu quả dự đoán cho "lớp 3 (Low, Trivial, Minor)" vì nó đã giảm số lượng phân loại sai của lớp này thành "lớp 2 (Medium)" (vấn đề này đã được thảo luận trong RQ2) từ 70 xuống 64 (8.5%).

Để so sánh các kiến trúc ConcatCLS và ConcatInline chúng tôi xem xét confusion matrix và các giá trị metric đánh giá. Chúng tôi xem xét confusion matrix vì nó cung cấp thông tin tốt hơn về mỗi nhãn riêng biệt và cũng các metric đánh giá khác như MCC, F1-perclass, và F1-weighted là các metric tốt cho các nhiệm vụ classification, đặc biệt là giá trị MCC hoạt động tốt trên các bộ dữ liệu không cân bằng, như bộ dữ liệu của chúng tôi. Dựa trên các hình và bảng được cung cấp hiển thị kết quả, chúng tôi có thể nói rằng kiến trúc ConcatCLS vượt trội hơn mô hình ConcatInline. Cuối cùng, chúng tôi kết luận rằng việc tích hợp metric mã nguồn như một đầu vào bổ sung trong các kiến trúc ConcatInline và ConcatCLS cải thiện hiệu quả của mô hình CodeBERT.

Tóm tắt RQ3: Tích hợp metric mã với biểu diễn mã, trong định dạng vector (ConcatCLS) cải thiện hiệu quả (MCC) của CodeBERT lên 5% và giảm tỷ lệ phân loại sai 8.5%.

D. Các mối đe dọa đối với Tính hợp lệ

Tính hợp lệ kết luận: Để đối phó với tính ngẫu nhiên tiềm ẩn của kết quả, trước tiên chúng tôi loại bỏ các mẫu trùng lặp tiềm ẩn từ bộ dữ liệu của mình, và sau đó xáo trộn và chia nó thành ba phần train, validation, và test sử dụng tỷ lệ 70%, 15%, 15%. Chúng tôi không thực hiện kỹ thuật cross-validation để giữ cho nghiên cứu của mình phù hợp với nghiên cứu đã được nghiên cứu trước đây như CodeXGLUE và các thí nghiệm CodeBERT được cung cấp. Tuy nhiên, việc chạy các kỹ thuật cross-validation như K-Fold có thể ảnh hưởng đến kết quả thu được của chúng tôi có thể được điều tra trong công việc tương lai.

Tính hợp lệ nội bộ: Một trong những mối đe dọa tính hợp lệ nội bộ đối với nghiên cứu này là cách chúng tôi phải đối phó với những hạn chế của các công cụ như độ dài ngữ cảnh trong LLMs. Kích thước 512 token của CodeBert đã được xử lý bằng cách cắt bớt các mẫu lớn xuống 512 tokens đầu tiên. Điều này có thể tác động đến kết quả được báo cáo. Công việc tương lai có thể thử LLMs với kích thước ngữ cảnh lớn hơn để hiểu tác động của hạn chế này. Một vấn đề khác sẽ là quá trình tích hợp các đặc trưng số vào mô hình CodeBERT có thể ảnh hưởng đến kết quả của chúng tôi, vì vậy để không thay đổi cấu trúc mô hình CodeBERT baseline một cách mạnh mẽ, chúng tôi không thêm nhiều lớp phức tạp để làm cho việc so sánh công bằng. Tuy nhiên, có thể tận dụng các đặc trưng số và tích hợp chúng với mô hình CodeBERT theo cách phức tạp hơn có thể cải thiện kết quả. Chúng tôi cũng trích xuất các metric mã nguồn nổi tiếng sử dụng thư viện tiêu chuẩn, để giảm bất kỳ yếu tố gây nhiễu nào trong những phép đo này.

Tính hợp lệ cấu trúc: Để giảm tác động của các mối đe dọa tính hợp lệ cấu trúc, chúng tôi đã sử dụng bảy metric đánh giá. Phân bố lớp lệch vốn có trong các bộ dữ liệu không cân bằng có thể dẫn đến các mô hình thể hiện bias dự đoán đối với các lớp đa số, do đó việc đánh giá các mô hình này là một nhiệm vụ thách thức. Vì vậy chúng tôi sử dụng định dạng "Weighted" của các metric đánh giá của chúng tôi để giảm tác động bias. Những metric này được tính toán bằng cách lấy trung bình của tất cả điểm mỗi lớp và cũng xem xét số lượng mẫu của mỗi lớp. Nói cách khác, đầu ra được tính toán đã tính đến đóng góp của mỗi lớp bằng cách xem xét số lượng instances trong lớp đã cho đó. Chúng tôi cũng báo cáo tất cả dữ liệu thô như confusion matrices.

Tính hợp lệ bên ngoài: Mặc dù chúng tôi đã tiến hành một thí nghiệm lớn với một số dự án trong hai bộ dữ liệu riêng biệt, tất cả các dự án của chúng tôi đều từ lĩnh vực mã nguồn mở và công việc tương lai cần thiết để tái tạo nghiên cứu trên các ứng dụng thương mại. Ngoài ra, mặc dù tại thời điểm thiết kế nghiên cứu này CodeBERT là LLM state of the art cho mã, cộng đồng đã tiến xa từ đó đến bây giờ bằng cách giới thiệu các mô hình tiên tiến và lớn hơn như GPT-4. Các công trình tương lai cần thiết để so sánh kết quả khi các LLMs tiên tiến hơn được thuê. Tuy nhiên, chúng tôi chỉ mong đợi kết quả tốt hơn hoặc ít nhất bằng nhau khi sử dụng các LLMs tiên tiến hơn.

V. CÔNG TRÌNH LIÊN QUAN

Các phương pháp dựa trên báo cáo lỗi tận dụng mô tả báo cáo lỗi và các kỹ thuật ngôn ngữ tự nhiên để dự đoán mức độ nghiêm trọng lỗi. Tian et al. [73] sử dụng classifier nearest neighbors và đo lường sự tương tự giữa các báo cáo lỗi trước đây và các báo cáo lỗi đã cho để dự đoán nhãn mức độ nghiêm trọng lỗi đã cho. Các thí nghiệm mở rộng của họ trên hàng nghìn báo cáo lỗi cho thấy kết quả hứa hẹn bằng cách có lên đến 76.3% cho metric F-measure. Phân tích cảm xúc là một kỹ thuật khác được tận dụng bởi Ramay et al [74] trong đó các tác giả sử dụng mô hình deep learning để phân tích các báo cáo lỗi đã cho để dự đoán mức độ nghiêm trọng lỗi. Các mô hình được đề xuất của họ có F-measures lên đến 88.01%. Lamkanfi et al [75] đã đề xuất dự đoán mức độ nghiêm trọng lỗi sử dụng kỹ thuật Topic Modeling. Đánh giá của họ về 30,000 báo cáo lỗi được trích xuất từ các dự án Eclipse, Mozilla, và Netbeans cho thấy hiệu quả của phương pháp của họ bằng cách báo cáo F-measure từ 65% đến 75% trên các báo cáo Bugzilla. Tan et al [18] đã tận dụng các cặp câu hỏi-và-trả lời có sẵn trên trang web Stack Overflow và kết hợp chúng với các báo cáo lỗi liên quan để tạo ra một phiên bản cải tiến của các báo cáo lỗi. Sự kết hợp này làm cho báo cáo lỗi phong phú dẫn đến cải thiện khoảng 23% của metric F-measure trung bình.

Công trình gần đây nhất về dự đoán mức độ nghiêm trọng dựa trên báo cáo lỗi [76], thực tế, sử dụng các mô hình Transformer tương tự như CodeBERT của chúng tôi. Tuy nhiên, nghiên cứu của họ khác với công trình của chúng tôi vì các lý do sau: a) họ vẫn chỉ dựa trên báo cáo lỗi và không phải mã nguồn, và (b) họ có classification nhị phân trên "nhãn ưu tiên" là một lĩnh vực khác với "mức độ nghiêm trọng" trong các hệ thống theo dõi issue. Mặc dù có thể có các tương quan, nhưng mức độ nghiêm trọng thường vốn có đối với lỗi (do đó ở một mức độ có thể được ước tính chỉ bằng mã) nhưng "ưu tiên" được gán một phần dựa trên lịch trình của dự án. Nói cách khác, người ta có thể gắn thẻ một lỗi không nghiêm trọng như ưu tiên cao vì ví dụ, theo lịch trình của developers, tốt hơn là sửa ngay bây giờ thay vì tháng tới, do đó có ưu tiên cao hơn.

Như đã giải thích trong phần giới thiệu, không giống như tất cả các bài báo trong danh mục này, chúng tôi không làm việc trên các báo cáo issue mà chỉ trên mã nguồn. Vì giả định rằng tất cả các lỗi trong mã đều có báo cáo issue được gán cho chúng là sai và có nhiều lỗi được phát hiện bằng cách chạy test cases không có báo cáo lỗi (hoặc ít nhất là chưa có). Vì lý do tương tự, chúng tôi không so sánh các nghiên cứu như vậy với phương pháp của chúng tôi.

Các phương pháp dựa trên metric mã nguồn trích xuất metric mã từ mã buggy và dự đoán nhãn mức độ nghiêm trọng dựa trên chúng. Các metric thiết kế hướng đối tượng ở mức class được sử dụng để dự đoán các lỗi Severity cao và thấp trên bộ dữ liệu NASA [19] trong đó các tác giả thấy rằng các metric thiết kế này có thể dự đoán các lỗi mức độ nghiêm trọng thấp trong các lớp dễ bị lỗi tốt hơn các lỗi mức độ nghiêm trọng cao. Trong một nghiên cứu khác, các metric hướng đối tượng ở mức class khác nhau được sử dụng để huấn luyện các kỹ thuật logistic regression và neural network để dự đoán mức độ nghiêm trọng lỗi của ba phiên bản Mozilla Firefox [21]. Các tác giả nói rằng neural network được đề xuất của họ dự đoán các lỗi mức độ nghiêm trọng cao và trung bình chính xác hơn các lỗi mức độ nghiêm trọng thấp. Một số metric hướng đối tượng ở mức class được sử dụng để điều tra xu hướng lỗi lớp trong sự phát triển sau phát hành của hệ thống bằng cách sử dụng ba phiên bản của dự án Eclipse [20].

Không giống như hầu hết các công trình trước đây, chúng tôi tập trung vào dự đoán mức độ nghiêm trọng lỗi "ở mức phương thức" trong thế giới thực thực tế hơn nhiều vì nó có thể được tính toán hiệu quả và cũng mức độ chi tiết class/module quá thô cho người thực hành để hành động [22]–[25]. Chúng tôi cũng mang biểu diễn mã như một phương pháp state-of-the-art để tối đa hóa sức mạnh dự đoán từ mã và metric.

VI. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Trong bài báo này, chúng tôi đề xuất một phương pháp sử dụng LLM cho mã để dự đoán mức độ nghiêm trọng của một lỗi chỉ sử dụng phương thức buggy làm đầu vào. Nó tích hợp các metric mã nguồn ở mức phương thức được trích xuất và mã của phương thức buggy làm đầu vào và sử dụng CodeBERT để dự đoán nhãn mức độ nghiêm trọng đa lớp. Thông qua một nghiên cứu thực nghiệm quy mô lớn trên 3,342 phương thức buggy từ 19 dự án của các dự án Java mã nguồn mở, chúng tôi cho thấy rằng việc sử dụng codeBERT chỉ trên mã nguồn có thể cải thiện dự đoán sử dụng các phương pháp cổ điển trên metric mã lên đến 140%, cho các metric đánh giá khác nhau. Ngoài ra, chúng tôi đã chỉ ra rằng việc cung cấp các metric mã nguồn được thu thập làm đầu vào bổ sung cho CodeBERT có thể cải thiện kết quả hơn nữa. Thú vị là, nó cũng giảm tỷ lệ false positive lên đến 8.5% cho các mức độ nghiêm trọng khác nhau, điều này cho thấy rằng các mô hình được huấn luyện trước phải được hướng dẫn (ví dụ, bằng cách đưa thông tin/ngữ cảnh bổ sung và tinh chỉnh) cho một nhiệm vụ cụ thể để cung cấp kết quả tốt nhất. Các hướng nghiên cứu tiềm năng tương lai của nghiên cứu này là thêm các comment mã có sẵn và cấu trúc vốn có của mã (ví dụ, data flow, control flow, v.v.) như một đầu vào bổ sung vào mô hình CodeBERT (hoặc các LLMs gần đây hơn như GPT-4) có thể nâng cao kết quả.

TÀI LIỆU THAM KHẢO

[1] E. Mashhadi, EhsanMashhadi/ISSRE2023-BugSeverityPrediction: 0.1.0, phiên bản 0.1.0, Tháng 8. 2023. DOI: 10.5281/zenodo.8267597. [Trực tuyến]. Có sẵn: https://doi.org/10.5281/zenodo.8267597.

[2] D. Kafura và G. R. Reddy, "The use of software complexity metrics in software maintenance," IEEE Transactions on Software Engineering, số 3, trang 335–343, 1987.

[3] J. Börstler và B. Paech, "The role of method chains and comments in software readability and comprehension—an experiment," IEEE Transactions on Software Engineering, tập 42, số 9, trang 886–898, 2016.

[4] K. H. Bennett và V. T. Rajlich, "Software maintenance and evolution: A roadmap," trong Proceedings of the Conference on the Future of Software Engineering, 2000, trang 73–87.

[5] M. Kondo, D. M. German, O. Mizuno, và E.-H. Choi, "The impact of context metrics on just-in-time defect prediction," Empirical Software Engineering, tập 25, số 1, trang 890–939, 2020.

[6] Y. Shin, A. Meneely, L. Williams, và J. A. Osborne, "Evaluating complexity, code churn, and developer activity metrics as indicators of software vulnerabilities," IEEE transactions on software engineering, tập 37, số 6, trang 772–787, 2010.

[7] A. Tosun, A. Bener, B. Turhan, và T. Menzies, "Practical considerations in deploying statistical methods for defect prediction: A case study within the turkish telecommunications industry," Information and Software Technology, tập 52, số 11, trang 1242–1257, 2010.

[8] Y. Zhou, B. Xu, và H. Leung, "On the ability of complexity metrics to predict fault-prone classes in object-oriented systems," Journal of Systems and Software, tập 83, số 4, trang 660–674, 2010.

[9] E. Mashhadi và H. Hemmati, "Applying codebert for automated program repair of java simple bugs," trong 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), IEEE, 2021, trang 505–509.

[10] C. Le Goues, T. Nguyen, S. Forrest, và W. Weimer, "Genprog: A generic method for automatic software repair," Ieee transactions on software engineering, tập 38, số 1, trang 54–72, 2011.

[11] F. Long và M. Rinard, "Automatic patch generation by learning correct code," trong Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, 2016, trang 298–312.

[12] S. Shamshiri, R. Just, J. M. Rojas, G. Fraser, P. McMinn, và A. Arcuri, "Do automatically generated unit tests find real faults? an empirical study of effectiveness and challenges (t)," trong 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), IEEE, 2015, trang 201–211.

[13] W. E. Wong, R. Gao, Y. Li, R. Abreu, và F. Wotawa, "A survey on software fault localization," IEEE Transactions on Software Engineering, tập 42, số 8, trang 707–740, 2016.

[14] S. Pearson, J. Campos, R. Just, và cộng sự, "Evaluating and improving fault localization," trong 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE), IEEE, 2017, trang 609–620.

[15] B. S. Neysiani, S. M. Babamir, và M. Aritsugi, "Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems," Information and Software Technology, tập 126, trang 106 344, 2020.

[16] G. Yang, T. Zhang, và B. Lee, "Towards semi-automatic bug triage and severity prediction based on topic model and multi-feature of bug reports," trong 2014 IEEE 38th Annual Computer Software and Applications Conference, IEEE, 2014, trang 97–106.

[17] K. K. Chaturvedi và V. Singh, "Determining bug severity using machine learning techniques," trong 2012 CSI sixth international conference on software engineering (CONSEG), IEEE, 2012, trang 1–6.

[18] Y. Tan, S. Xu, Z. Wang, T. Zhang, Z. Xu, và X. Luo, "Bug severity prediction using question-and-answer pairs from stack overflow," Journal of Systems and Software, tập 165, trang 110 567, 2020.

[19] Y. Zhou và H. Leung, "Empirical analysis of object-oriented design metrics for predicting high and low severity faults," IEEE Transactions on software engineering, tập 32, số 10, trang 771–789, 2006.

[20] R. Shatnawi và W. Li, "The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process," Journal of systems and software, tập 81, số 11, trang 1868–1882, 2008.

[21] S. Singh, P. Mittal, và K. S. Kahlon, "Empirical model for predicting high, medium and low severity faults using object oriented metrics in mozilla firefox," International journal of computer applications in technology, tập 47, số 2-3, trang 110–124, 2013.

[22] E. Shihab, A. E. Hassan, B. Adams, và Z. M. Jiang, "An industrial study on the risk of software changes," trong Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, 2012, trang 1–11.

[23] L. Pascarella, F. Palomba, và A. Bacchelli, "On the performance of method-level bug prediction: A negative result," Journal of Systems and Software, tập 161, trang 110 493, 2020.

[24] F. Grund, S. Chowdhury, N. C. Bradley, B. Hall, và R. Holmes, "Codeshovel: Constructing method-level source code histories," trong 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), IEEE, 2021, trang 1510–1522.

[25] H. Hata, O. Mizuno, và T. Kikuno, "Bug prediction based on fine-grained module histories," trong 2012 34th international conference on software engineering (ICSE), IEEE, 2012, trang 200–210.

[26] Gói tái tạo của bài báo, [Trực tuyến; truy cập 08-Tháng 8-2023]. [Trực tuyến]. Có sẵn: https://github.com/EhsanMashhadi/ISSRE2023-BugSeverityPrediction.

[27] Z. Feng, D. Guo, D. Tang, và cộng sự, "Codebert: A pre-trained model for programming and natural languages," arXiv preprint arXiv:2002.08155, 2020.

[28] S. Lu, D. Guo, S. Ren, và cộng sự, "Codexglue: A machine learning benchmark dataset for code understanding and generation," arXiv preprint arXiv:2102.04664, 2021.

[29] M. Tufano, F. Palomba, G. Bavota, và cộng sự, "When and why your code starts to smell bad," trong 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, IEEE, tập 1, 2015, trang 403–414.

[30] M. Polo, M. Piattini, và F. Ruiz, "Using code metrics to predict maintenance of legacy programs: A case study," trong Proceedings IEEE International Conference on Software Maintenance. ICSM 2001, IEEE, 2001, trang 202–208.

[31] R. Ferenc, D. Bán, T. Grósz, và T. Gyimóthy, "Deep learning in static, metric-based bug prediction," Array, tập 6, trang 100 021, 2020.

[32] A. Okutan và O. T. Yıldız, "Software defect prediction using bayesian networks," Empirical software engineering: an international journal, tập 19, số 1, trang 154–181, 2014.

[33] A. Koru và H. Liu, "Building effective defect-prediction models in practice," IEEE Software, tập 22, số 6, trang 23–29, 2005.

[34] T. Zimmermann, R. Premraj, và A. Zeller, "Predicting defects for eclipse," trong Third International Workshop on Predictor Models in Software Engineering (PROMISE'07: ICSE Workshops 2007), IEEE, 2007, trang 9–9.

[35] E. Giger, M. D'Ambros, M. Pinzger, và H. C. Gall, "Method-level bug prediction," trong Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement, IEEE, 2012, trang 171–180.

[36] R. Ferenc, P. Gyimesi, G. Gyimesi, Z. Tóth, và T. Gyimóthy, "An automatically created novel bug dataset and its validation in bug prediction," The Journal of systems and software, tập 169, trang 110 691, 2020.

[37] S. Chowdhury, G. Uddin, và R. Holmes, "An empirical study on maintainable method size in java," trong 19th International Conference on Mining Software Repositories, 2022.

[38] R. Mo, S. Wei, Q. Feng, và Z. Li, "An exploratory study of bug prediction at the method level," Information and software technology, tập 144, trang 106 794, 2022.

[39] S. Wattanakriengkrai, P. Thongtanunam, C. Tantithamthavorn, H. Hata, và K. Matsumoto, "Predicting defective lines using a model-agnostic technique," arXiv preprint arXiv:2009.03612, 2020.

[40] D. Steidl, B. Hummel, và E. Juergens, "Incremental origin analysis of source code files," trong Proceedings Working Conference on Mining Software Repositories (MSR), 2014, trang 42–51.

[41] F. Servant và J. A. Jones, "Fuzzy fine-grained code-history analysis," trong Proceedings of the International Conference on Software Engineering (ICSE), 2017, trang 746–757.

[42] Y. Gil và G. Lalouche, "On the correlation between size and metric validity," Empirical Software Engineering, tập 22, số 5, trang 2585–2611, Tháng 10. 2017.

[43] K. El Emam, S. Benlarbi, N. Goel, và S. N. Rai, "The confounding effect of class size on the validity of object-oriented metrics," IEEE Transactions on Software Engineering, tập 27, số 7, trang 630–650, 2001.

[44] D. Landman, A. Serebrenik, và J. Vinju, "Empirical analysis of the relationship between cc and sloc in a large corpus of java methods," trong 2014 IEEE International Conference on Software Maintenance and Evolution, IEEE, 2014, trang 221–230.

[45] P. Ralph và E. Tempero, "Construct validity in software engineering research and software metrics," trong Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018, 2018, trang 13–23.

[46] T. J. McCabe, "A complexity measure," IEEE Transactions on software Engineering, số 4, trang 308–320, 1976.

[47] C. L. McClure, "A model for program complexity analysis," trong Proceedings of the 3rd international conference on Software engineering, 1978, trang 149–157.

[48] N. Kasto và J. Whalley, "Measuring the difficulty of code comprehension tasks using software metrics," trong Proceedings of the Fifteenth Australasian Computing Education Conference-Volume 136, 2013, trang 59–65.

[49] M. Alenezi, M. Akour, và H. Al Sghaier, "The impact of co-evolution of code production and test suites through software releases in open source software systems," International Journal of Innovative Technology and Exploring Engineering (IJITEE), tập 9, số 1, trang 2737–2739, 2019.

[50] K. K. Zaw, H. W. Hnin, K. Y. Kyaw, và N. Funabiki, "Software quality metrics calculations for java programming learning assistant system," trong 2020 IEEE Conference on Computer Applications (ICCA), IEEE, 2020, trang 1–6.

[51] A. Hindle, M. W. Godfrey, và R. C. Holt, "Reading beside the lines: Indentation as a proxy for complexity metric," trong 2008 16th IEEE International Conference on Program Comprehension, IEEE, 2008, trang 133–142.

[52] R. P. Buse và W. R. Weimer, "Learning a metric for code readability," IEEE Transactions on software engineering, tập 36, số 4, trang 546–558, 2009.

[53] M. H. Halstead, Elements of Software Science (Operating and programming systems series). Elsevier Science Inc., 1977.

[54] P. Oman và J. Hagemeister, "Metrics for assessing a software system's maintainability," trong Proceedings Conference on Software Maintenance 1992, IEEE Computer Society, 1992, trang 337–338.

[55] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.

[56] Y. Liu, M. Ott, N. Goyal, và cộng sự, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.

[57] A. Vaswani, N. Shazeer, N. Parmar, và cộng sự, "Attention is all you need," Advances in neural information processing systems, tập 30, 2017.

[58] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, và M. Brockschmidt, "Codesearchnet challenge: Evaluating the state of semantic code search," arXiv preprint arXiv:1909.09436, 2019.

[59] C. Pan, M. Lu, và B. Xu, "An empirical study on software defect prediction using codebert model," Applied Sciences, tập 11, số 11, trang 4793, 2021.

[60] F. Zhang, X. Yu, J. Keung, và cộng sự, "Improving stack overflow question title generation with copying enhanced codebert model and bi-modal information," Information and Software Technology, tập 148, trang 106 922, 2022.

[61] R. Just, D. Jalali, và M. D. Ernst, "Defects4j: A database of existing faults to enable controlled testing studies for java programs," trong Proceedings of the 2014 International Symposium on Software Testing and Analysis, 2014, trang 437–440.

[62] R. K. Saha, Y. Lyu, W. Lam, H. Yoshida, và M. R. Prasad, "Bugs.jar: A large-scale, diverse dataset of real-world java bugs," trong Proceedings of the 15th international conference on mining software repositories, 2018, trang 10–13.

[63] M. Martinez, T. Durieux, R. Sommerard, J. Xuan, và M. Monperrus, "Automatic repair of real bugs in java: A large-scale experiment on the defects4j dataset," Empirical Software Engineering, tập 22, số 4, trang 1936–1964, 2017.

[64] R. K. Saha, Y. Lyu, H. Yoshida, và M. R. Prasad, "Elixir: Effective object-oriented program repair," trong 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE), IEEE, 2017, trang 648–659.

[65] Jira, [Trực tuyến; truy cập 08-Tháng 8-2023]. [Trực tuyến]. Có sẵn: https://github.com/pycontribs/jira.

[66] Robustscaler, [Trực tuyến; truy cập 20-Tháng 4-2022]. [Trực tuyến]. Có sẵn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html.

[67] Classification metrics, [Trực tuyến; truy cập 20-Tháng 4-2022]. [Trực tuyến]. Có sẵn: https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/.

[68] B. W. Matthews, "Comparison of the predicted and observed secondary structure of t4 phage lysozyme," Biochimica et Biophysica Acta (BBA)-Protein Structure, tập 405, số 2, trang 442–451, 1975.

[69] Sklearn evaluation metrics, [Trực tuyến; truy cập 08-Tháng 8-2023]. [Trực tuyến]. Có sẵn: https://scikit-learn.org/0.15/modules/model_evaluation.html.

[70] L. Buitinck, G. Louppe, M. Blondel, và cộng sự, "Api design for machine learning software: Experiences from the scikit-learn project," arXiv preprint arXiv:1309.0238, 2013.

[71] Hugging face, [Trực tuyến; truy cập 20-Tháng 4-2022]. [Trực tuyến]. Có sẵn: https://huggingface.co.

[72] Y. Hu, J. Ding, Z. Dou, và H. Chang, "Short-text classification detector: A bert-based mental approach," Computational Intelligence and Neuroscience, tập 2022, 2022.

[73] Y. Tian, D. Lo, và C. Sun, "Information retrieval based nearest neighbor classification for fine-grained bug severity prediction," trong 2012 19th Working Conference on Reverse Engineering, IEEE, 2012, trang 215–224.

[74] W. Y. Ramay, Q. Umer, X. C. Yin, C. Zhu, và I. Illahi, "Deep neural network-based severity prediction of bug reports," IEEE Access, tập 7, trang 46 846–46 857, 2019.

[75] T. Zhang, J. Chen, G. Yang, B. Lee, và X. Luo, "Towards more accurate severity prediction and fixer recommendation of software bugs," Journal of Systems and Software, tập 117, trang 166–184, 2016.

[76] M. Izadi, K. Akbari, và A. Heydarnoori, "Predicting the objective and priority of issue reports in software repositories," Empirical Software Engineering, tập 27, số 2, trang 50, 2022.
