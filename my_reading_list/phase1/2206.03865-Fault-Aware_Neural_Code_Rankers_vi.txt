# Bộ Xếp Hạng Mã Nguồn Thần Kinh Nhận Biết Lỗi

Jeevana Priya Inala Chenglong Wang Mei Yang Andres Codas
Mark Encarnación Shuvendu K Lahiri Madanlal Musuvathi Jianfeng Gao
Microsoft Research
{jinala,chenwang,meiyang,andres.codas,markenc,
shuvendu,madanm,jfgao}@microsoft.com

## Tóm tắt

Các mô hình ngôn ngữ lớn (LLMs) đã chứng minh khả năng ấn tượng trong việc tạo ra mã nguồn cho các tác vụ lập trình khác nhau. Trong nhiều trường hợp, LLMs có thể tạo ra một chương trình đúng cho một tác vụ khi được cho nhiều lần thử. Do đó, một xu hướng gần đây là thực hiện lấy mẫu quy mô lớn các chương trình bằng cách sử dụng một mô hình và sau đó lọc/xếp hạng các chương trình dựa trên việc thực thi chương trình trên một số lượng nhỏ các unit test đã biết để chọn một giải pháp ứng cử viên. Tuy nhiên, các phương pháp này giả định rằng các unit test được cung cấp và giả định khả năng thực thi an toàn các chương trình được tạo ra (có thể thực hiện các hoạt động nguy hiểm tùy ý như thao tác tập tin). Cả hai giả định trên đều không thực tế trong phát triển phần mềm thực tế. Trong bài báo này, chúng tôi đề xuất CODERANKER, một bộ xếp hạng thần kinh có thể dự đoán tính đúng đắn của một chương trình được lấy mẫu mà không cần thực thi nó. CODERANKER của chúng tôi có khả năng nhận biết lỗi, tức là nó được huấn luyện để dự đoán các loại thông tin thực thi khác nhau như dự đoán loại lỗi biên dịch/runtime chính xác (ví dụ: IndexError hoặc TypeError). Chúng tôi cho thấy rằng CODERANKER có thể tăng đáng kể độ chính xác pass@1 của các mô hình tạo mã khác nhau (bao gồm Codex [11], GPT-Neo, GPT-J) trên các tập dữ liệu APPS [25], HumanEval [11] và MBPP [3].

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn dựa trên transformer có khả năng ấn tượng [19,7,16], bao gồm khả năng tạo mã nguồn [11,3,28,39,33]. Nhiệm vụ ở đây là lấy mô tả ngôn ngữ tự nhiên hoặc ngữ cảnh mã nguồn trước đó làm đầu vào và tạo ra toàn bộ chương trình bằng ngôn ngữ lập trình có mục đích chung như Python hoặc C++. Tạo ra toàn bộ chương trình là một nhiệm vụ đầy thử thách vì nó bao gồm việc hiểu nhiệm vụ, tìm ra cách thực hiện nó, và viết mã nguồn mà không có lỗi cú pháp/runtime nào. Trên các bài toán lập trình khó hơn như các bài toán thi lập trình, các mô hình hiện tại đạt độ chính xác rất thấp đặc biệt nếu ngân sách lấy mẫu trong thời gian suy luận thấp. Ví dụ, trên tập dữ liệu APPS [25], một mô hình tạo mã nguồn tiên tiến, Codex [11], đạt độ chính xác 4% nếu nó chỉ được phép lấy mẫu một chương trình cho mỗi nhiệm vụ (gọi là pass@1), nhưng đạt độ chính xác 24% nếu nó được phép 100 mẫu cho mỗi nhiệm vụ (gọi là pass@100—ít nhất một chương trình đúng trong 100 mẫu) (xem Bảng 7). Quan sát này dẫn chúng ta đến một vấn đề nghiên cứu quan trọng về việc khám phá các phương pháp xếp hạng các chương trình được lấy mẫu để thu hẹp khoảng cách giữa hiệu suất pass@1 và pass@100.

Khi phân tích các chương trình được lấy mẫu thu được bằng cách sử dụng LLM, chúng tôi thấy rằng một số trong số chúng có lỗi cú pháp và lỗi runtime, và một số khác thực thi để tạo ra đầu ra không mong muốn. Do đó, các nghiên cứu trước [11,28] tập trung vào việc xếp hạng các chương trình bằng cách thực thi chúng trên một tập nhỏ các unit test (thường được giả định là được cung cấp như một phần của mô tả nhiệm vụ). Tuy nhiên, có một số lưu ý đối với phương pháp này: Thứ nhất, ngay cả khi một chương trình vượt qua các test đã cho, nó vẫn có thể thất bại trên các unit test chưa biết. Thứ hai, có gánh nặng cho người dùng trong việc cung cấp unit test cho mọi nhiệm vụ suy luận. Thứ ba, để thực thi mã nguồn, tất cả các dependency phải được cài đặt đúng cách. Điều này đặc biệt có vấn đề trong các tình huống mà người dùng muốn nhận gợi ý mã nguồn trong một dự án với nhiều tập tin và dependency (như sử dụng CoPilot trong môi trường VS Code [1]). Ngay cả khi những dependency này được thỏa mãn, nhiều tình huống lập trình thực tế liên quan đến mã nguồn chưa hoàn thiện đang được phát triển tích cực nơi việc thực thi chỉ đơn giản là không khả thi. Cuối cùng, mã nguồn được tạo ra bởi LLM có thể tiềm ẩn rủi ro bảo mật (như xóa tập tin trên đĩa) và do đó, việc thực thi mã nguồn như vậy cần các cơ chế cô lập nặng như môi trường sandbox.

Để giảm bớt các vấn đề trên khi dựa vào việc thực thi mã nguồn, một số nghiên cứu gần đây [18,36] đã đề xuất sử dụng bộ xếp hạng dựa trên mạng thần kinh¹ để xếp hạng các chương trình được lấy mẫu từ LLM. Mô hình xếp hạng về cơ bản là một bộ phân loại lấy mô tả nhiệm vụ và một chương trình được lấy mẫu làm đầu vào và dự đoán xác suất rằng chương trình đó đúng so với mô tả nhiệm vụ. Khi được cho nhiều chương trình (thu được bằng cách lấy mẫu), bộ xếp hạng sắp xếp lại chúng theo thứ tự giảm dần của xác suất dự đoán chương trình đó đúng. Bộ xếp hạng về cơ bản đang cố gắng mô phỏng việc thực thi chương trình trên một số unit test mà không thực sự thực thi mã nguồn trong quá trình suy luận. Dữ liệu huấn luyện được thu thập bằng cách thực thi cả chương trình đúng và sai được lấy mẫu từ chính mô hình tạo mã nguồn. Do đó, chúng ta chỉ cần unit test và khả năng thực thi trong bước tạo tập dữ liệu thay vì trong quá trình suy luận như trong các phương pháp trước.

Trong khi các phương pháp trước nhắm vào các bài toán toán học để làm cho bài toán học tập trở nên dễ giải quyết, trong bài báo này, chúng tôi thiết kế CODERANKER cho các tác vụ lập trình tổng quát phức tạp hơn trong Python. Một chương trình Python được lấy mẫu có thể thất bại theo nhiều cách khác nhau. Ví dụ, một chương trình khi được thực thi trên một unit test có thể dẫn đến lỗi biên dịch/runtime và có thể tạo ra nhiều loại đầu ra khác nhau như số nguyên, chuỗi, danh sách và từ điển có thể tạo ra sự không khớp kiểu. Ngược lại trong lĩnh vực toán học, có ít chỗ cho lỗi biên dịch/runtime và đầu ra thường chỉ là một số.

Phương pháp CODERANKER của chúng tôi dựa trên ý tưởng rằng một bộ xếp hạng thần kinh được huấn luyện để phân biệt giữa các chế độ thất bại khác nhau có thể có hiểu biết tốt hơn về chương trình và nhiệm vụ, và do đó có thể làm tốt hơn trong việc xếp hạng các chương trình. Do đó, chúng tôi thiết kế CODERANKER nhận biết lỗi và chúng tôi điều tra tác động của nó đến hiệu suất xếp hạng. Mỗi bộ xếp hạng nhận biết lỗi là một bộ phân loại được huấn luyện để dự đoán một/hai nhãn đa lớp được trích xuất từ thông tin phong phú thu được bằng cách thực thi các chương trình.

Chúng tôi đã sử dụng tập dữ liệu APPS để tinh chỉnh/huấn luyện các mô hình tạo mã nguồn và CODERANKER. Trên tập dữ liệu này, chúng tôi cho thấy rằng CODERANKER đã cải thiện hiệu suất pass@1 của Codex (được sử dụng theo cách few-shot) từ 26% lên 39.6% trên tập validation và từ 3.8% lên 4.5% trên tập test. Chúng tôi cũng thấy rằng các bộ xếp hạng của chúng tôi có thể chuyển giao sang tập dữ liệu khác mà không cần huấn luyện bổ sung. Trên tập dữ liệu HumanEval, chúng tôi đã cải thiện pass@1 của Codex từ 26% lên 32% và trên tập dữ liệu MBPP, chúng tôi đã cải thiện từ 36% lên 42%. Chúng tôi thấy hiệu suất tăng tương tự với các mô hình tạo mã nguồn khác như GPT-J và GPT-Neo. So với bộ xếp hạng dựa trên bộ phân loại nhị phân đơn giản, CODERANKER nhận biết lỗi của chúng tôi đạt hiệu suất xếp hạng tốt hơn. Cuối cùng, chúng tôi điều tra hiệu ứng của việc trộn tập dữ liệu xếp hạng từ nhiều mô hình tạo mã nguồn giúp chúng tôi có thêm sự tăng hiệu suất.

## 2 Kiến thức cơ bản

### 2.1 Tạo mã nguồn

**Nhiệm vụ**: Một nhiệm vụ tạo mã nguồn G là một prompt, được biểu diễn dưới dạng một chuỗi các token, chỉ định nhiệm vụ đang thực hiện. G thường là sự kết hợp của ngôn ngữ tự nhiên, các ví dụ đầu vào-đầu ra, và mã nguồn khởi tạo. Một giải pháp S cho nhiệm vụ tạo mã nguồn là một chuỗi các token cùng nhau tạo thành một chương trình để giải quyết nhiệm vụ đã cho. Các tập dữ liệu hiện có bổ sung chứa một tập hợp các cặp đầu vào-đầu ra được sử dụng để kiểm tra tính đúng đắn của chương trình được tạo ra. Hình 1 đưa ra một ví dụ nhiệm vụ tạo mã nguồn, giải pháp tham chiếu tương ứng, và các unit test được lấy từ tập dữ liệu APPS [25].

**Mô hình**: Có một số LLM được huấn luyện trước hiện có trong văn liệu phù hợp để tạo mã nguồn theo cách few-shot hoặc sau khi tinh chỉnh. Một mô hình tạo mã nguồn F cung cấp cách để chúng ta lấy mẫu các chương trình cho một nhiệm vụ G đã cho dưới dạng Si ~ PF(S|G) trong đó PF biểu thị phân phối xác suất được tạo ra bởi mô hình tạo mã nguồn F.

Trong bài báo này, chúng tôi nghiên cứu bốn mô hình tạo mã nguồn khác nhau—(i) Codex, (ii) GPT-J (6B), (iii) GPT-Neo 1.3B và (iv) GPT-Neo 125M (Bảng 1). Codex là mô hình tạo mã nguồn tiên tiến lớn nhất có sẵn công khai để truy vấn thông qua API và đã cho thấy hiệu suất ấn tượng trong tạo mã nguồn [11]. Nó được xây dựng dựa trên kiến trúc mô hình ngôn ngữ GPT-3 và được huấn luyện trên 180 GB dữ liệu GitHub. GPT-Neo và GPT-J là các mô hình mã nguồn mở với số lượng tham số từ 125M đến 6B. Các mô hình này được huấn luyện trước trên tập dữ liệu Pile (800 GB corpus ngôn ngữ tự nhiên với 8% dữ liệu GitHub). Vì các mô hình này là mã nguồn mở, có thể tinh chỉnh các mô hình này trên tập dữ liệu downstream như tập dữ liệu APPS. Ngoài các mô hình trên, còn có các mô hình tạo mã nguồn khác như AlphaCode [28] và mô hình của Google [3]. Trong khi phương pháp của chúng tôi có thể áp dụng cho bất kỳ mô hình nào trong số này, trong nghiên cứu này, chúng tôi xác nhận hiệu quả của CODERANKER trên một tập hợp các mô hình tạo mã nguồn tiên tiến có sẵn công khai, để có thể tái tạo.

**Số liệu**: Các mô hình tạo mã nguồn được đánh giá dựa trên tính đúng đắn về chức năng thay vì khớp chính xác/mờ với chương trình tham chiếu. Điều này là do các số liệu dựa trên khớp không thể tính đến không gian lớn và phức tạp của các chương trình tương đương về chức năng với chương trình tham chiếu. Tính đúng đắn về chức năng được ước tính bằng cách kiểm tra xem chương trình được lấy mẫu có vượt qua một tập hợp các unit test hay không. Các phương pháp trước đánh giá tính đúng đắn về chức năng sử dụng số liệu pass@k; cho k mẫu chương trình được tạo ra cho mỗi nhiệm vụ, một nhiệm vụ được coi là đã giải quyết nếu bất kỳ mẫu nào vượt qua các unit test. Số liệu pass@k đo lường tổng tỷ lệ các nhiệm vụ được giải quyết. Ngoài ra, chúng tôi định nghĩa số liệu exec@k tính toán tỷ lệ các nhiệm vụ có ít nhất một chương trình trong k mẫu thực thi mà không có lỗi biên dịch/runtime nào, tức là tạo ra giá trị không lỗi cho mỗi đầu vào test, nhưng có thể khớp hoặc không khớp với đầu ra mong muốn.

### 2.2 Xếp hạng mã nguồn

Cho n chương trình được lấy mẫu sử dụng mô hình tạo mã nguồn, S1, ..., Sn ~ PF(S|G), mục tiêu của CODERANKER là tìm thứ tự của các chương trình So1, ..., Son sao cho để tính toán ranked pass@k cho k ≤ n, một bài toán được coi là đã giải quyết nếu bất kỳ chương trình nào trong tập hợp {So1, ..., Sok} vượt qua các unit test. Mô hình CODERANKER R nhận mô tả nhiệm vụ tạo mã nguồn G và chương trình được lấy mẫu Si làm đầu vào và đưa ra điểm số si. Thứ tự xếp hạng của các chương trình được lấy mẫu được cho bởi So1, ..., Son sao cho so1 > so2 > ... > son.

## 3 Bộ Xếp Hạng Mã Nguồn Thần Kinh Nhận Biết Lỗi

Mô hình CODERANKER là một bộ phân loại được huấn luyện để phân loại một cặp ⟨G, Si⟩ là CORRECT hoặc không, trong đó CORRECT có nghĩa là Si thỏa mãn nhiệm vụ G đối với các unit test của nó. Điểm số si được tính toán dưới dạng si = PR(CORRECT|G, Si) trong đó PR là xác suất theo mô hình xếp hạng R. Xác suất này được trích xuất sử dụng các giá trị thực từ lớp cuối cùng trước lớp SoftMax.

### 3.1 Tập dữ liệu Xếp hạng Mã nguồn

Để huấn luyện mô hình CODERANKER, chúng ta cần một tập dữ liệu có cả chương trình CORRECT và WRONG (tức là không đúng). Để thu thập những chương trình này, chúng tôi sử dụng các mô hình tạo mã nguồn để lấy mẫu n = 100 chương trình cho mỗi nhiệm vụ trong tập dữ liệu huấn luyện. Sau đó chúng tôi thực thi các chương trình được lấy mẫu trên các unit test tương ứng để tạo ra các nhãn phân loại. Theo các quan sát từ [18], người ta phải cẩn thận để không over-train các mô hình tạo mã nguồn cơ sở nhằm đảm bảo tính đa dạng trong các chương trình được lấy mẫu. Do đó, chúng tôi chỉ tinh chỉnh các mô hình tạo mã nguồn cơ sở trong tối đa 2 epoch và chọn checkpoint dẫn đến validation loss thấp nhất (điều này không áp dụng cho mô hình Codex, được sử dụng theo cách few-shot). Bảng 2 cho thấy phân phối của các tập dữ liệu xếp hạng thu được bằng cách sử dụng 4 mô hình tạo mã nguồn khác nhau cho các nhiệm vụ trong tập dữ liệu APPS². Như có thể mong đợi, tập dữ liệu xếp hạng có sự mất cân bằng cao với khoảng 5X đến 40X dữ liệu điểm WRONG nhiều hơn so với các điểm dữ liệu CORRECT và tỷ lệ này cao hơn đối với các mô hình nhỏ hơn như mô hình GPT-Neo.

**Tập dữ liệu xếp hạng nhận biết lỗi**: Một bộ xếp hạng đơn giản là một bộ được huấn luyện để dự đoán nhãn nhị phân CORRECT hoặc WRONG. Tuy nhiên, một chương trình thất bại vì nhiều lý do khác nhau và biết tại sao một chương trình có thể thất bại là rất quan trọng để dự đoán xem chương trình có CORRECT hay không. Do đó, chúng tôi đã thiết kế tập dữ liệu xếp hạng nhận biết lỗi. Khi chúng tôi thực thi một chương trình trên một tập hợp các unit test, thông báo compiler nhiều hơn chỉ một bit thông tin. Thực tế, khi unit test thất bại, chúng ta biết nó thất bại vì lỗi biên dịch/runtime (mà chúng tôi gọi là lỗi thực thi) hoặc vì chương trình tạo ra đầu ra sai cho một đầu vào cụ thể (mà chúng tôi gọi là lỗi ý định). Bảng 2 cũng cho thấy phân phối của lỗi ý định và lỗi thực thi trong các tập dữ liệu xếp hạng. Một quan sát thú vị là tỷ lệ lỗi thực thi giảm đối với các mô hình tạo mã nguồn lớn hơn.

Có thể phân tích thêm các điểm dữ liệu WRONG. Trong lớp lỗi thực thi, thông báo compiler cho chúng ta biết chính xác loại lỗi thực thi (như IndexError hoặc TypeError hoặc TimeOutError) và dòng trong chương trình gây ra lỗi này. Bằng cách phân tích thông báo lỗi từ compiler Python, chúng tôi đã rút ra 10 lớp lỗi thực thi thường gặp nhất như được hiển thị trong Bảng 4 và Hình 2. Tương tự, đối với lớp lỗi ý định, chúng ta biết đầu ra được tạo ra khác với đầu ra mong đợi như thế nào (như loại sai hoặc độ dài sai của đầu ra mảng). Chúng tôi đã thiết kế thủ công 9 lớp lỗi ý định thường gặp nhất bằng cách xem xét các đầu ra mong đợi và được tạo ra khác nhau từ các ví dụ huấn luyện (xem Bảng 5 và Hình 3). Những nhãn chi tiết dựa trên thực thi này tạo thành tập dữ liệu nhận biết lỗi của chúng tôi. Bảng 3 cho thấy một vài mục dữ liệu với tất cả các nhãn. Hình 6 trong Phụ lục cho thấy phân phối của các lớp lỗi thực thi và lỗi ý định khác nhau cho tập dữ liệu xếp hạng thu được bằng cách sử dụng mô hình Codex.

### 3.2 Nhiệm vụ Xếp hạng Mã nguồn

Bây giờ chúng tôi mô tả các nhiệm vụ phân loại khác nhau được rút ra từ tập dữ liệu trên. Đối với mỗi nhiệm vụ phân loại, đầu vào là một cặp đặc tả nhiệm vụ tạo mã nguồn G và chương trình được tạo Si và đầu ra là một hoặc nhiều nhãn trong đó mỗi nhãn thuộc về tập hợp các lớp được xác định trước. Chúng tôi mô tả các nhãn/lớp khác nhau của các nhiệm vụ khác nhau dưới đây. Những nhiệm vụ này được thiết kế để khám phá sự đánh đổi giữa việc có các lớp thất bại trừu tượng so với việc có các lớp thất bại chi tiết.

**Nhị phân (B)**: Đầu ra là một nhãn nhị phân duy nhất với hai lớp {CORRECT, WRONG}.

**Tam phân (T)**: Nhiệm vụ tam phân chia lớp WRONG thành lớp lỗi ý định và lỗi thực thi: điều này tạo thành nhiệm vụ phân loại ba lớp với nhãn đầu ra {CORRECT, lỗi ý định, lỗi thực thi}.

**Nhận biết Lỗi Ý định (I)**: Nhiệm vụ nhận biết lỗi ý định chia lớp lỗi ý định trong nhiệm vụ tam phân thành 9 lớp con khác nhau, do đó có tổng cộng 11 lớp cho nhãn đầu ra.

**Nhận biết Lỗi Thực thi (E)**: Nhiệm vụ nhận biết lỗi thực thi tương tự như nhiệm vụ nhận biết lỗi ý định nhưng thay vì chia lớp lỗi ý định, giờ chúng ta chia lớp lỗi thực thi thành 10 lớp con khác nhau, do đó có tổng cộng 12 lớp cho nhãn đầu ra.

**Nhận biết Lỗi Thực thi + Dòng Lỗi (E+L)**: Đây là nhiệm vụ phân loại đa lớp và đa nhãn kết hợp hai nhiệm vụ phân loại. Thứ nhất là nhiệm vụ nhận biết lỗi thực thi được mô tả ở trên. Nhiệm vụ thứ hai là dự đoán dòng mã nguồn tương ứng với lỗi thực thi. Các nhãn cho số dòng lỗi cũng bao gồm -1 để biểu thị không có lỗi thực thi.

### 3.3 Mô hình Xếp hạng Mã nguồn

Chúng tôi đã triển khai các mô hình CODERANKER nhận biết lỗi bằng cách tinh chỉnh mô hình CodeBERT [21] được huấn luyện trước.

**CodeBERT**: Đây là mô hình hiểu mã nguồn kiểu BERT tiên tiến được huấn luyện trước trên tập dữ liệu CodeSearchNet [26] sử dụng sự kết hợp của các mục tiêu masked language modeling và replaced token detection [17]. Nó nhận đầu vào là sự nối của hai phân đoạn với token tách biệt đặc biệt, cụ thể là [CLS]; w1; w2; ::wn; [SEP]; c1; c2; :::; cm; [EOS]. Thường thì một phân đoạn là văn bản ngôn ngữ tự nhiên, và phân đoạn khác là mã nguồn. [CLS] là token đặc biệt, có thể coi biểu diễn ẩn cuối cùng của nó như biểu diễn tổng hợp chuỗi cho các nhiệm vụ phân loại hoặc xếp hạng downstream.

**Thêm đầu phân loại**: Chúng tôi thêm một đầu phân loại lên trên mô hình CodeBERT cơ sở bằng cách kết nối một lớp tuyến tính và một lớp softmax với biểu diễn ẩn của token đặc biệt [CLS]. Cho C ∈ R^H là vector ẩn cuối cùng tương ứng với token [CLS] và cho W ∈ R^(K×H) là trọng số của lớp phân loại mới được thêm vào trong đó K là số lượng lớp trong nhiệm vụ phân loại. Các logit cho đầu ra phân loại được tính toán là softmax(CW^T) và chúng tôi sử dụng hàm mất mát cross-entropy tiêu chuẩn để tinh chỉnh tất cả các trọng số.

**Thêm đầu dự đoán dòng**: Để dự đoán dòng tương ứng với lỗi thực thi, chúng tôi giới thiệu một vector dòng lỗi S ∈ R^H trong quá trình tinh chỉnh. Xác suất của token xuống dòng ("\n") thứ i là dòng lỗi được tính toán như tích vô hướng giữa Ti và S theo sau bởi softmax trên tất cả các token xuống dòng trong mã nguồn, tức là Pi = e^(S^T Ti) / ∑_j e^(S^T Tj) trong đó Ti là vector ẩn cuối cùng tương ứng với token xuống dòng thứ i. Chúng tôi bao gồm một token xuống dòng ở đầu và cuối đầu vào để chỉ ra trường hợp không có dòng lỗi trong mã nguồn (tức là mã nguồn không dẫn đến lỗi thực thi) và để chỉ ra trường hợp dòng lỗi vượt quá những gì được mã hóa trong đầu vào (điều này xảy ra nếu ngữ cảnh nhiệm vụ+mã nguồn không thể vừa trong giới hạn 512 token của CodeBERT), tương ứng.

## 4 Đánh giá

Tiếp theo chúng tôi đánh giá phương pháp CODERANKER. Chúng tôi điều tra (1) các CODERANKER nhận biết lỗi có thể cải thiện các mô hình tạo mã nguồn khác nhau trên các tập dữ liệu mã nguồn khác nhau như thế nào, (2) tác động của các nhiệm vụ xếp hạng khác nhau, và (3) hiệu ứng của việc trộn tập dữ liệu xếp hạng được tạo ra bởi các mô hình tạo mã nguồn khác nhau.

### 4.1 Thiết lập Thí nghiệm

**Tập dữ liệu tạo mã nguồn**: Chúng tôi xem xét ba tập dữ liệu tạo mã nguồn hiện có cho đánh giá của chúng tôi: (1) APPS [25]: một tập hợp 5000 nhiệm vụ huấn luyện và 5000 nhiệm vụ test được thu thập từ các cuộc thi lập trình và bài toán phỏng vấn, (2) HumanEval [11]: một tập hợp 164 nhiệm vụ test, và (3) MBPP [3]: một tập hợp 974 nhiệm vụ lập trình Python cơ bản với 474 bài toán huấn luyện và 500 bài toán test.

Trong các thí nghiệm của chúng tôi, chúng tôi chỉ sử dụng tập dữ liệu APPS để tinh chỉnh các mô hình tạo mã nguồn và các mô hình CODERANKER (vì nó là tập dữ liệu lớn nhất). Nhưng chúng tôi đánh giá các mô hình này trên tất cả ba tập nhiệm vụ test. Tập dữ liệu APPS không đi kèm với tập dữ liệu validation, vì vậy chúng tôi đã sử dụng một tập hợp 600 nhiệm vụ từ tập dữ liệu huấn luyện gốc cho validation; sau đó chúng được loại trừ khỏi tập dữ liệu huấn luyện. Vì chúng tôi quan tâm đến việc đánh giá rõ ràng khả năng của bộ xếp hạng để phân biệt mã nguồn CORRECT với mã nguồn WRONG, chúng tôi đã chọn tập validation chỉ bao gồm các bài toán mà mô hình Codex³ (theo cách few-shot) có thể tạo ra ít nhất một chương trình đúng trong 100 mẫu của nó. Để tạo điều kiện chuyển giao các mô hình GPT-J và GPT-Neo được tinh chỉnh trên tập dữ liệu HumanEval và MBPP, chúng tôi thực hiện chuyển đổi lập trình nhỏ của các mô tả nhiệm vụ để khớp với phong cách APPS.

**Số liệu**: Chúng tôi sử dụng các số liệu pass@1, pass@5, exec@1, ranked pass@1, ranked pass@5, và ranked exec@1 (giá trị càng cao càng tốt). Xem Phần 2 để biết định nghĩa của chúng. Chúng tôi cũng cho thấy số liệu pass@100 để minh họa giá trị tối đa có thể cho các số liệu pass@k và ranked pass@k. Những số liệu này được đo lường sử dụng một bộ ước lượng không thiên vị từ 100 mẫu như được đề xuất bởi [11].

**Thiết lập huấn luyện và siêu tham số**: Chúng tôi tinh chỉnh các mô hình tạo mã nguồn GPT-J và GPT-Neo trên tập dữ liệu huấn luyện APPS trong 2 epoch với batch size 256 và learning rate 1e-5, và chọn checkpoint có validation loss thấp nhất. Đối với suy luận, chúng tôi sử dụng temperature sampling với T = 0.8 cho mô hình Codex và T = 0.9 cho các mô hình GPT-J và GPT-Neo trừ khi được chỉ định khác. Chúng tôi chọn những temperature này để tối đa hóa tính đa dạng trong 100 mẫu, nhưng chúng tôi cũng tiến hành thử nghiệm với temperature thấp hơn trong Bảng 11. Đối với mỗi chương trình, chúng tôi lấy mẫu 512 token mới và cắt bớt chương trình được tạo ra bằng chuỗi dừng đặc biệt mà chúng tôi đã sử dụng trong các prompt few-shot/tinh chỉnh.

Chúng tôi tinh chỉnh các mô hình CODERANKER trong 30 epoch với batch size 512 và learning rate 1e-4, và chọn checkpoint dẫn đến số liệu ranked pass@1 tốt nhất trên tập dữ liệu validation. Chúng tôi sử dụng trọng số lớp để cân bằng các lớp khác nhau trong khi huấn luyện các bộ xếp hạng. Tất cả các thí nghiệm được tiến hành trên GPU V100-32GB.

**Ký hiệu**: Chúng tôi sử dụng ký hiệu R^Y_DX để biểu thị mô hình CODERANKER được huấn luyện trên tập dữ liệu thu được bằng cách sử dụng mô hình tạo mã nguồn X từ Bảng 2 và trên một trong năm nhiệm vụ xếp hạng Y từ Phần 3.2.

### 4.2 Kết quả Chính: CODERANKER cải thiện các mô hình tạo mã nguồn

**Tập dữ liệu validation APPS**: Trước tiên, chúng tôi phân tích kết quả trên tập dữ liệu validation APPS gồm 600 nhiệm vụ. Bảng 6 cho thấy hiệu suất trên các số liệu khác nhau cho 4 mô hình tạo mã nguồn khác nhau. Những kết quả này sử dụng mô hình CODERANKER tốt nhất cho mỗi mô hình tạo mã nguồn được hiển thị trong Bảng 10. Từ Bảng 6, chúng tôi thấy rằng các CODERANKER cải thiện tất cả các số liệu cho tất cả các mô hình tạo mã nguồn mặc dù có kích thước khác nhau. Hiệu suất pass@1 tăng từ 5.1% đến 13.6% với CODERANKER và các mô hình có thể giải quyết khoảng 30 đến 80 nhiệm vụ nhiều hơn khi nó phải chọn chỉ một chương trình từ 100 mẫu. Một quan sát thú vị khác là mô hình GPT Neo 125M khi kết hợp với CODERANKER (một mô hình 125M khác) đánh bại mô hình GPT-J (với số tham số nhiều hơn 50 lần). Những kết quả này cho thấy hiệu quả của CODERANKER trong việc cải thiện các mô hình tạo mã nguồn.

**Tập dữ liệu test APPS**: Kết quả của chúng tôi trên tập dữ liệu test APPS gồm 5000 nhiệm vụ được hiển thị trong Bảng 7. Các bài toán test khó hơn so với những bài trong tập validation, điều mà chúng ta có thể thấy qua các số pass@100 và pass@1 nhỏ hơn. Do đó, sự cải thiện từ CODERANKER nhỏ hơn về quy mô, nhưng vẫn là một cải thiện đáng kể; pass@1 của Codex tăng từ 3.8% lên 4.7% (35 bài toán bổ sung).

**Tập dữ liệu HumanEval và MBPP**: Chúng tôi đo lường khả năng chuyển giao của CODERANKER trên hai tập dữ liệu khác nhau–HumanEval (kết quả trong Bảng 8) và MBPP (kết quả trong Bảng 9). Chúng ta lại có thể thấy rằng các CODERANKER cải thiện hiệu suất của tất cả các mô hình tạo mã nguồn trên tất cả các số liệu cho cả hai tập dữ liệu (một ngoại lệ là pass@5 cho GPT-J trên MBPP). Hiệu suất pass@1 của mô hình Codex tăng 6% trên cả hai tập dữ liệu. Những kết quả này cho thấy khả năng của CODERANKER để chuyển giao ngoài phân phối và cũng chứng minh rằng các lỗi được tạo ra bởi các mô hình tạo mã nguồn trên các nhiệm vụ khác nhau là phổ quát.

**Exec@1 so với pass@1**: Trong tất cả các kết quả trên, các cải thiện trong số liệu exec@1 cao hơn so với các cải thiện trong số liệu pass@1; điều này cho thấy rằng CODERANKER tốt hơn trong việc xác định lỗi thực thi trong mã nguồn so với lỗi ý định, điều này được mong đợi vì thực thi mã nguồn được chứng minh là một nhiệm vụ khó về cơ bản đối với các mô hình ngôn ngữ [3, 34].

### 4.3 Thí nghiệm Phân tích

**Hiệu ứng của temperature lấy mẫu**: Trên tập dữ liệu HumanEval và MBPP, chúng tôi bổ sung thử nghiệm với các temperature khác nhau để lấy mẫu 100 chương trình (xem Bảng 11). Như mong đợi, chúng tôi nhận thấy rằng pass@100 giảm với temperature thấp hơn, nhưng pass@1 tăng. Chúng tôi thấy rằng CODERANKER tiếp tục tăng hiệu suất pass@1 cho 3 trong số 4 thiết lập và đạt 42.7% ranked pass@1 trên tập dữ liệu HumanEval—kết quả tốt nhất được biết đến cho đến nay trên tập dữ liệu này⁴ [16]. Trên tập dữ liệu MBPP, trong thiết lập lấy mẫu temperature thấp, chúng tôi thấy rằng CODERANKER làm giảm nhẹ hiệu suất pass@1—điều này chúng tôi quy cho sự khác biệt nhỏ giữa các số liệu pass@1 và pass@100, khiến cho việc học bộ xếp hạng đánh bại lược đồ xếp hạng ngẫu nhiên trở nên khó khăn.

**Phân tích các nhiệm vụ xếp hạng khác nhau**: Hình 4 cho thấy 4 đường cong huấn luyện; một cho mỗi mô hình tạo mã nguồn X, thể hiện các đường cong ranked pass@1 validation cho mô hình X khi kết hợp với 5 bộ xếp hạng khác nhau R^Y_DX. Kết quả ở định dạng bảng có thể được tìm thấy trong Phụ lục. Từ các đường cong, chúng ta có thể nhận thấy rằng đối với các mô hình lớn hơn như Codex và GPT-J, các bộ xếp hạng được huấn luyện trên nhiệm vụ phân loại tam phân R^T hoạt động tốt nhất, và đối với các mô hình nhỏ hơn như GPT-Neo 1.3B và GPT-Neo 125M, bộ xếp hạng được huấn luyện trên nhiệm vụ phân loại nhận biết ý định R^I và bộ xếp hạng được huấn luyện trên nhiệm vụ phân loại nhận biết lỗi thực thi + dòng lỗi R^{E+L} hoạt động tốt nhất, tương ứng. Chúng ta cũng có thể nhận thấy rằng các bộ xếp hạng nhị phân R^B hoạt động tệ hơn đáng kể đặc biệt với các mô hình nhỏ hơn. Những kết quả này gợi ý rằng khi huấn luyện bộ xếp hạng trên tập dữ liệu có nhiều mã nguồn WRONG hơn, các nhiệm vụ phân loại khó hơn hoạt động như bộ điều chỉnh tốt hơn. Hình 7 trong Phụ lục cho thấy các đường cong huấn luyện tương tự cho số liệu ranked exec@1; ở đây, chúng ta có thể thấy rằng R^E và R^{E+L} luôn đạt ranked exec@1 tốt nhất vì chúng được huấn luyện để xác định lỗi thực thi và R^B và R^I có hiệu suất tệ nhất.

**Phân tích các tập dữ liệu xếp hạng khác nhau**: Trong thí nghiệm này, chúng tôi đo lường tác động của việc sử dụng các bộ xếp hạng được huấn luyện trên dữ liệu từ một mô hình tạo mã nguồn trên mô hình tạo mã nguồn khác và hiệu ứng của việc trộn các tập dữ liệu xếp hạng này. Chúng tôi có 4 mô hình tạo mã nguồn khác nhau trong thí nghiệm này và do đó, 4 tập dữ liệu xếp hạng khác nhau. Chúng tôi phân tích hai tập dữ liệu hỗn hợp khác nhau—(i) mixed-small lấy mẫu ngẫu nhiên 25% các tập dữ liệu xếp hạng trên và kết hợp chúng, và (ii) mixed-large kết hợp tất cả 4 tập dữ liệu xếp hạng thành một; cái trước đại diện cho một tập dữ liệu có kích thước gần như tương tự với các tập dữ liệu xếp hạng riêng lẻ để so sánh công bằng, trong khi cái sau sử dụng tất cả dữ liệu có sẵn. Hình 5 cho thấy ranked pass@1 của tất cả các kết hợp 4x6 dưới dạng bản đồ nhiệt. Mỗi cột đã được chuẩn hóa sao cho các giá trị cho cùng một mô hình là 1 (tức là các giá trị đường chéo là 1). Từ hình, trong số các tập dữ liệu xếp hạng riêng lẻ, chúng tôi nhận thấy rằng các bộ xếp hạng được huấn luyện với dữ liệu từ cùng mô hình tạo mã nguồn là tốt nhất và các bộ xếp hạng được huấn luyện với dữ liệu từ các mô hình tạo mã nguồn có sự khác biệt lớn về kích thước là tệ nhất. Cuối cùng, các bộ xếp hạng được huấn luyện trên tập dữ liệu mixed-small hoạt động hơi tệ hơn (trừ mô hình GPT-J code gen.) so với việc sử dụng tập dữ liệu cùng mô hình, nhưng các bộ xếp hạng được huấn luyện trên tập dữ liệu mixed-large có hiệu suất tốt nhất tổng thể. Những kết quả này gợi ý rằng trong khi thường tốt hơn khi sử dụng cùng mô hình để tạo ra tập dữ liệu xếp hạng, chúng ta có thể sử dụng các mô hình nhỏ hơn (mô hình ít tốn kém hơn) để tăng cường tập dữ liệu xếp hạng nhằm cải thiện hiệu suất hơn nữa.

Phân tích định tính/định lượng bổ sung về phương pháp CODERANKER có thể được tìm thấy trong Phụ lục.

## 5 Nghiên cứu Liên quan

**Nhiệm vụ/mô hình tạo mã nguồn**: Có một số mô hình tạo mã nguồn được khám phá trong văn liệu, từ các kiến trúc chỉ decoder [11,3,39,6,22] đến các kiến trúc encoder-decoder [28,33,2,38] với các kích thước khác nhau. Tương tự, có một số tập dữ liệu nhiệm vụ từ nhiều lĩnh vực, bao gồm các bài toán từ toán học [3,18], tạo cell Jupyter notebook [10], các nhiệm vụ lập trình thông thường [3,11,35] và các bài toán lập trình cấp độ thi đấu [28,25]. Trong bài báo này, chúng tôi đánh giá khả năng của CODERANKER trong việc cải thiện hiệu suất của bốn mô hình tạo mã nguồn dựa trên decoder trên ba tập dữ liệu lập trình. Phương pháp của chúng tôi không phụ thuộc vào kiến trúc mô hình tạo mã nguồn miễn là chúng tạo ra đầu ra bằng cách lấy mẫu và phương pháp của chúng tôi có thể dễ dàng mở rộng sang các lĩnh vực khác nhau.

**Nhiệm vụ/mô hình hiểu mã nguồn**: Bên cạnh các nhiệm vụ tạo mã nguồn, có một số nhiệm vụ hiểu mã nguồn bao gồm tìm kiếm mã nguồn [4,8,23,26], phát hiện bản sao [37,30], tóm tắt mã nguồn [26], dịch mã nguồn [13,27,32] và phát hiện khuyết tật [9,5,15,40,5]. Các mô hình hiểu mã nguồn chỉ encoder [29,21,2,38,24] được phát triển cho các nhiệm vụ này. Nhiệm vụ CODERANKER của chúng tôi có thể được xem như một nhiệm vụ hiểu mã nguồn mới, và mô hình xếp hạng của chúng tôi được tinh chỉnh từ CodeBERT [21]. Các tập dữ liệu phát hiện khuyết tật có liên quan chặt chẽ với chúng tôi; sự khác biệt chính là công trình trước tập trung vào việc tìm lỗ hổng trong mã nguồn do con người viết, trong khi chúng tôi tập trung vào việc phát hiện lỗi trong mã nguồn do mô hình tạo ra.

**Lọc/xếp hạng cho các mô hình tạo mã nguồn**: Các nghiên cứu trước như [11,28] sử dụng thực thi để lọc bỏ các completion mã nguồn trong quá trình suy luận. Trong khi [11] chỉ sử dụng các unit test được cung cấp như một phần của mô tả nhiệm vụ, [28] bổ sung sử dụng mô hình thần kinh để tạo ra các đầu vào cho các unit test và sử dụng thực thi để lọc bỏ các chương trình tạo ra cùng đầu ra trên những đầu vào đó. Tuy nhiên, những công trình này yêu cầu thực thi mã nguồn có khả năng dễ bị tổn thương cho mọi nhiệm vụ suy luận. Phương pháp của chúng tôi bỏ qua việc thực thi tại thời điểm suy luận với bộ xếp hạng thần kinh. Tương tự như nghiên cứu của chúng tôi, [18,36] đề xuất bộ xếp hạng/veriﬁer dựa trên mạng thần kinh cho các mô hình tạo mã nguồn; sự khác biệt chính là lĩnh vực (các nhiệm vụ lập trình có mục đích chung trong trường hợp của chúng tôi so với các bài toán toán học trong công trình trước) và chúng tôi huấn luyện các bộ xếp hạng thần kinh để học tại sao/làm thế nào mã nguồn thất bại thay vì chỉ dự đoán nhãn nhị phân. Hơn nữa, [18,36] sử dụng các mô hình tạo sinh làm cơ sở xếp hạng của họ; trong nghiên cứu của chúng tôi, chúng tôi cho thấy lợi ích của các bộ xếp hạng sử dụng mô hình chỉ encoder đơn giản với chỉ 125M tham số ([18] sử dụng các mô hình với ít nhất 3B tham số) (xem Phần A.2.3 trong Phụ lục để xem thử nghiệm phân tích của chúng tôi về các kiến trúc xếp hạng khác nhau).

**Sử dụng thực thi/phân tích tĩnh trong các ngữ cảnh khác**: Đã có các công trình khác về việc sử dụng thực thi để hướng dẫn tạo mã nguồn bằng cách điều kiện hóa việc tạo ra trên một biểu diễn của các trạng thái chương trình [12,20]. Cũng có những nỗ lực để thay thế quá trình thực thi bằng mô hình thần kinh trong những trường hợp này [14,34]. Một công trình liên quan khác là [31], cải thiện các mô hình tạo mã nguồn bằng cách sử dụng phân tích tĩnh để tăng cường đầu vào của mô hình và đã được chứng minh là làm giảm đáng kể số lượng lỗi thực thi được tạo ra bởi các chương trình được tạo ra. Bài báo này có một phương pháp khác bằng cách sử dụng mô hình xếp hạng thần kinh để lọc bỏ các chương trình sai tại thời điểm suy luận.

## 6 Kết luận và Hướng Phát triển Tương lai

Chúng tôi đã trình bày CODERANKER xếp hạng các chương trình được tạo ra bởi mô hình tạo mã nguồn mà không cần thực thi rõ ràng các chương trình. Các bộ xếp hạng của chúng tôi có khả năng nhận biết lỗi, tức là được huấn luyện để dự đoán các lớp chi tiết của các chế độ thất bại và chúng tôi đã cho thấy hiệu quả của các bộ xếp hạng nhận biết lỗi trong việc cải thiện các số liệu pass@k và exec@k cho các mô hình tạo mã nguồn và nhiệm vụ khác nhau.

Một trong những hạn chế chính là phương pháp CODERANKER không chính xác hoàn toàn, tức là bộ xếp hạng có thể phân loại chương trình đúng là sai và ngược lại. Ngoài ra, chúng tôi phải chịu thêm thời gian suy luận để có hiệu suất tốt hơn, vì giờ chúng ta phải tạo ra n >> k chương trình để lọc k chương trình để hiển thị cho người dùng. Phương pháp hiện tại của chúng tôi cũng dựa vào việc lấy mẫu chương trình đầy đủ từ mô hình tạo mã nguồn trước khi sử dụng CODERANKER. Trong tương lai, chúng tôi muốn điều tra việc xếp hạng/phân loại các chương trình một phần, có thể lần lượt giảm thời gian suy luận bằng cách loại bỏ các chương trình sai sớm. Một hướng phát triển tương lai khác cho công trình của chúng tôi là điều tra các kiến trúc mô hình xếp hạng khác như những mô hình có thể tận dụng tốt hơn cấu trúc mã nguồn và khám phá các nhiệm vụ xếp hạng khác như tạo ra thông báo lỗi đầy đủ. Cuối cùng, sẽ thú vị khi điều tra khả năng chuyển giao của phương pháp CODERANKER sang các nhiệm vụ lập trình tổng quát (thay vì chỉ lập trình thi đấu). Một thách thức chính ở đây là thiếu tập dữ liệu lập trình tổng quát cần được thu thập bằng cách crawl các nguồn công cộng như GitHub. Các thí nghiệm sơ bộ của chúng tôi trên tập dữ liệu lập trình tổng quát như vậy cho thấy rằng các mô hình tạo mã nguồn hiện tại thường tạo ra nhiều chương trình có lỗi thực thi hơn là chương trình có lỗi ý định. Quan sát này kết hợp với kết quả trong bài báo này cho thấy CODERANKER thường tốt hơn trong việc xác định lỗi thực thi so với lỗi ý định, chúng tôi rất lạc quan rằng phương pháp CODERANKER nhận biết lỗi cũng sẽ cải thiện việc tạo mã nguồn cho các nhiệm vụ lập trình tổng quát.

**Lời cảm ơn**: Chúng tôi cảm ơn Todd Mytkowicz, Piali Choudhury, Rahee Gosh Peshwaria, Curtis von Veh, và Xiaodong Liu vì những thảo luận hữu ích về công trình này.
