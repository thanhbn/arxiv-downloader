# 2303.06233.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: ./2303.06233.pdf
# Kích thước file: 3519968 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Thông tin Cú pháp Không phụ thuộc Mô hình cho
Mô hình Ngôn ngữ Lập trình Tiền huấn luyện
Iman Saberi, Fatemeh H. Fardy
Khoa Khoa học Máy tính, Toán học, Vật lý và Thống kê
Đại học British Columbia
Kelowna, Canada
Email:iman.saberi@ubc.ca,yfatemeh.fard@ubc.ca

Tóm tắt —Mô hình Ngôn ngữ Lập trình Tiền huấn luyện (PPLMs) đã đạt được nhiều kết quả tối ưu gần đây cho nhiều tác vụ kỹ thuật phần mềm liên quan đến mã nguồn. Mặc dù một số nghiên cứu sử dụng luồng dữ liệu hoặc đề xuất các mô hình dạng cây sử dụng Cây Cú pháp Trừu tượng (AST), hầu hết các PPLMs không tận dụng đầy đủ thông tin cú pháp phong phú trong mã nguồn. Vẫn còn đó, đầu vào được coi là một chuỗi token. Có hai vấn đề; vấn đề đầu tiên là tính không hiệu quả tính toán do mối quan hệ bậc hai giữa độ dài đầu vào và độ phức tạp attention. Thứ hai, bất kỳ thông tin cú pháp nào, khi cần thiết như một đầu vào bổ sung cho các PPLMs hiện tại, đều yêu cầu mô hình phải được tiền huấn luyện từ đầu, lãng phí tất cả tài nguyên tính toán đã sử dụng để tiền huấn luyện các mô hình hiện tại. Trong công trình này, chúng tôi đề xuất các adapter Nhận dạng Thực thể Có tên (NER), các module nhẹ có thể được chèn vào các khối Transformer để học thông tin kiểu dữ liệu được trích xuất từ AST. Các adapter này có thể được sử dụng với các PPLMs hiện tại như CodeBERT, GraphCodeBERT, và CodeT5. Chúng tôi huấn luyện các NER adapter bằng cách sử dụng một hàm mục tiêu Phân loại Kiểu Token mới (TTC). Chúng tôi chèn công trình đề xuất vào CodeBERT, xây dựng CodeBERTER, và đánh giá hiệu suất trên hai tác vụ tinh chỉnh mã nguồn và tóm tắt mã nguồn. CodeBERTER cải thiện độ chính xác của tinh chỉnh mã nguồn từ 16.4 lên 17.8 trong khi sử dụng 20% ngân sách tham số huấn luyện so với phương pháp fine-tuning đầy đủ, và điểm BLEU của tóm tắt mã nguồn từ 14.75 lên 15.90 trong khi giảm 77% tham số huấn luyện so với phương pháp fine-tuning đầy đủ.

Từ khóa chỉ mục —Adapters, Mô hình Ngôn ngữ Lập trình Tiền huấn luyện

I. GIỚI THIỆU

Sự thành công của Mô hình Ngôn ngữ Tiền huấn luyện (PLM) trong Xử lý Ngôn ngữ Tự nhiên (NLP) đã dẫn đến sự xuất hiện của các mô hình tiền huấn luyện trong kỹ thuật phần mềm, như CodeBERT [1], CuBERT [2], và CodeT5 [3]. Mô hình Ngôn ngữ Lập trình Tiền huấn luyện (PPLMs) –PLMs được tiền huấn luyện trên ngôn ngữ lập trình– học một biểu diễn chung của mã nguồn sử dụng một hàm mục tiêu trừu tượng như Mô hình Ngôn ngữ Che (MLM) trong giai đoạn tiền huấn luyện. Các biểu diễn này sau đó được khai thác cho các tác vụ downstream hướng ngôn ngữ lập trình như tóm tắt mã nguồn [4], [5], [6], phát hiện bản sao mã nguồn [7], [8] và tinh chỉnh mã nguồn [9], [10] trong giai đoạn fine-tuning.

Bất chấp sự tiến bộ của PPLMs, hầu hết chúng [2], [11], [12] khai thác hoặc khối encoder hoặc decoder Transformer [13]. Các mô hình này xử lý một đoạn mã như một chuỗi token giống như các kỹ thuật thông thường cho mô hình hóa đầu vào trong lĩnh vực ngôn ngữ tự nhiên trong khi bỏ qua thông tin cú pháp phong phú của mã nguồn. Gần đây, một số nghiên cứu đã đề xuất các biểu diễn có tính đến thông tin cấu trúc của mã nguồn [9], [3], [14].

Jiang et al. đã đề xuất một PLM dựa trên cây cho các tác vụ sinh ngôn ngữ lập trình [14], sử dụng Cây Cú pháp Trừu tượng (AST) để cung cấp thông tin cấu trúc cho việc tiền huấn luyện kiến trúc dựa trên Transformer [13]. So với các phương pháp dựa trên chuỗi coi token mã nguồn là đầu vào của mô hình, trong phương pháp của họ, mỗi token mã nguồn được biểu diễn như một đường dẫn từ nút gốc đến nút terminal tương ứng với token mã nguồn đó trong AST. Mặc dù phương pháp của họ cung cấp thông tin cấu trúc phong phú cho mô hình, độ dài đầu vào của mô hình được nhân với độ dài trung bình của tất cả các đường dẫn trong AST. Việc mở rộng độ dài đầu vào này không hiệu quả về mặt tính toán, theo Ainslie et al., người đã đề cập rằng độ phức tạp tính toán và bộ nhớ của thao tác attention tăng theo bậc hai khi độ dài đầu vào tăng [15].

Một số PPLMs khác cung cấp thông tin kiểu dữ liệu cho các mô hình hiện có. GraphCodeBERT [9] xem xét cấu trúc vốn có của mã nguồn và sử dụng luồng dữ liệu được trích xuất từ AST tương ứng với mỗi đầu vào để cung cấp thông tin mức semantic như một đầu vào bổ sung cho mô hình; sau đó tiền huấn luyện CodeBERT [1] bằng thông tin mới này. Hơn nữa, các tác giả của CodeT5 [3] đã đề cập rằng các identifier do developer chỉ định chứa semantic mã nguồn phong phú, vì vậy họ đề xuất một tác vụ tiền huấn luyện nhận biết identifier mới để thông báo cho mô hình liệu một token mã nguồn có phải là identifier hay không. Cả hai phương pháp này đều yêu cầu mô hình phải được tiền huấn luyện từ đầu. Ngay cả thông tin mới do GraphCodeBERT cung cấp, mặc dù mô hình này được tiền huấn luyện trên CodeBERT, vẫn yêu cầu tiền huấn luyện từ đầu, điều này không hiệu quả về thời gian và tính toán.

Do đó, chúng ta không thể trực tiếp tận dụng các PPLMs hiện có như CodeBERT [1], GraphCodeBERT [9], và CodeT5 [3] để đề xuất thông tin hoặc định dạng đầu vào mới. Vì các mô hình này được tiền huấn luyện dựa trên một chuỗi token mã nguồn được làm phẳng [16], việc đề xuất một dạng biểu diễn đầu vào mới yêu cầu tất cả các mô hình này phải được tiền huấn luyện từ đầu. Hai vấn đề này, cụ thể là tính không hiệu quả tính toán khi sử dụng thông tin cú pháp của mã nguồn (do tăng độ dài) và việc yêu cầu tiền huấn luyện PPLMs từ đầu khi giới thiệu đầu vào mới, đã thúc đẩy chúng tôi đề xuất một phương pháp giải quyết chúng. Vì vậy, công trình này nhằm cung cấp embedding cú pháp của mã nguồn cho các mô hình ngôn ngữ lập trình tiền huấn luyện hiện có.

Để đề xuất một phương pháp hiệu quả về tham số và không phụ thuộc mô hình để áp đặt thông tin cú pháp cho các PPLMs hiện tại, chúng tôi sử dụng adapter. Trong NLP, adapter là một thành phần nhẹ được đặt bên trong mỗi khối Transformer để sửa đổi hành vi của nó [17]. Trong quá trình huấn luyện adapter, các trọng số của mô hình tiền huấn luyện được cố định, và các trọng số adapter mới được giới thiệu được huấn luyện trên một hàm mục tiêu. Adapter thường được sử dụng để điều chỉnh các mô hình NLP hiện có để hoạt động trong một bối cảnh cụ thể (tức là transfer learning) [18], [19] hoặc để thực hiện một tác vụ cụ thể (tức là quick fine-tuning) [20], [21]. Tuy nhiên, adapter chưa bao giờ được sử dụng để nắm bắt thông tin cú pháp của mã nguồn hay để cung cấp thông tin mới cho các PLMs hoặc PPLMs hiện có.

Trong công trình này, chúng tôi đề xuất Adapter Nhận dạng Thực thể Có tên (NER), các module nhẹ được chèn vào các khối Transformer có mục đích học thông tin kiểu dữ liệu được trích xuất từ AST. Các module có thể cắm này có thể được chèn vào các PPLMs hiện tại như CodeBERT [1], GraphCodeBERT [9], và CodeT5 [3]. Để huấn luyện các NER adapter, chúng tôi đặt vấn đề như một tác vụ phân loại token trong đó mỗi token đầu vào được gán một kiểu được trích xuất từ AST, và NER adapter nhằm phát hiện kiểu của mỗi token một cách chính xác.

Chúng tôi chèn NER adapter vào CodeBERT [1], do đó, phát triển một mô hình mà chúng tôi gọi là CodeBERTER. Chúng tôi tiến hành thí nghiệm trên dữ liệu sửa lỗi Java được giới thiệu trong [10] cho tinh chỉnh mã nguồn, và tập dữ liệu CodeSearchNet [22] cho tác vụ tóm tắt mã nguồn. Câu hỏi nghiên cứu chính mà chúng tôi điều tra ở đây là: Liệu NER adapter có thể cải thiện hiệu suất của các tác vụ mục tiêu kỹ thuật phần mềm, cụ thể là tinh chỉnh mã nguồn và tóm tắt mã nguồn, trong khi sử dụng ít tham số có thể huấn luyện hơn không?

Kết quả cho thấy rằng đối với tinh chỉnh mã nguồn, chúng tôi đã cải thiện độ chính xác baseline CodeBERT từ 16.4 lên 17.8 với 23 triệu tham số có thể huấn luyện, tức là ít hơn 80% so với tổng số tham số có thể huấn luyện của baseline. Chúng tôi cũng áp dụng NER adapter trên tóm tắt mã nguồn, cải thiện điểm BLEU-4 của hai ngôn ngữ, bao gồm Ruby và Go, lần lượt là 30% và 29%, với ít hơn 77% tham số so với full fine-tuning.

Các đóng góp chính của chúng tôi như sau:
Giới thiệu NER adapter với một hàm loss mới, Token Type Classification Loss (TTC), để áp đặt thông tin cú pháp cho các PPLMs hiện có. Chúng tôi cũng công bố mã nguồn¹.
Đánh giá hiệu suất của NER adapter trên tinh chỉnh mã nguồn và tóm tắt mã nguồn.
Cải thiện kết quả của các tác vụ này trong khi huấn luyện ít tham số hơn và hiệu quả hơn về mặt tính toán.

Phần còn lại của bài báo được tổ chức như sau. Trong Phần II, chúng tôi cung cấp tổng quan về thông tin nền quan trọng và sau đó giới thiệu NER adapter trong Phần III. Chúng tôi cung cấp thiết lập thí nghiệm và chi tiết nghiên cứu trong Phần IV. Kết quả và thảo luận được giải thích trong Phần V. Các phần VI và VII dành cho các công trình liên quan và mối đe dọa tính hợp lệ. Cuối cùng, chúng tôi kết luận bài báo trong Phần VIII.

II. KIẾN THỨC NỀN TẢNG

A. Cây Cú pháp Trừu tượng

AST là một biểu diễn dạng cây của cú pháp của một ngôn ngữ lập trình. Mỗi nút trong cây biểu diễn một cấu trúc cú pháp trong mã nguồn, như một từ khóa, một toán tử, một biến, hoặc một hàm. Cấu trúc của cây phản ánh cấu trúc của chương trình, với nút gốc biểu diễn cấu trúc cấp cao nhất và các nút lá biểu diễn các cấu trúc nhỏ nhất, cơ bản nhất [23].

AST có thể được sử dụng cho nhiều mục đích khác nhau, bao gồm phân tích cấu trúc của chương trình để kiểm tra lỗi cú pháp hoặc thực thi tiêu chuẩn coding [24], và trích xuất thông tin về chương trình, như định nghĩa biến và hàm, để tạo tài liệu hoặc sinh báo cáo độ phủ mã nguồn [25].

B. Mô hình Ngôn ngữ Lập trình Tiền huấn luyện

Mô hình Ngôn ngữ Lập trình Tiền huấn luyện (PPLMs) là các mô hình ngôn ngữ neural sâu được huấn luyện trên tập dữ liệu mã nguồn lớn và học dự đoán từ hoặc token tiếp theo trong mã nguồn cho trước bối cảnh của các từ hoặc token trước đó. Các mô hình này sau đó có thể được sử dụng để thực hiện nhiều tác vụ mục tiêu khác nhau trong giai đoạn fine-tuning, như tóm tắt mã nguồn [3], [26], hoàn thiện mã nguồn [27], sinh mã nguồn [3], và tinh chỉnh mã nguồn [9].

C. Adapter

Trong xử lý ngôn ngữ tự nhiên, adapter là các thành phần nhẹ có thể được sử dụng để điều chỉnh một mô hình ngôn ngữ cho một tác vụ hoặc tập dữ liệu cụ thể. Huấn luyện dựa trên adapter là một phương pháp fine-tuning hiệu quả và nhanh chóng yêu cầu ít tham số hơn so với full fine-tuning truyền thống. So với việc chỉ thêm một head mới lên trên mô hình ngôn ngữ tiền huấn luyện (PLM), adapter cung cấp hiệu suất vượt trội do khả năng tích hợp vào cấu trúc nội bộ của PLM và ảnh hưởng đến embedding nội bộ của mạng.

Adapter được sử dụng trong một số lĩnh vực, bao gồm (1) fine-tuning, là quá trình tiếp tục huấn luyện một mô hình machine learning trên tập dữ liệu mới, sử dụng các tham số và trọng số học được từ tập dữ liệu huấn luyện gốc làm điểm khởi đầu và (2) domain adaptation là lĩnh vực khác mà adapter được sử dụng, là quá trình điều chỉnh mô hình machine learning cho một domain mới, hoặc một lĩnh vực hoặc bối cảnh cụ thể. Điều này có thể được thực hiện bằng cách tiếp tục huấn luyện mô hình trên tập dữ liệu đại diện cho domain mới hoặc bằng cách thêm thông tin đặc thù domain vào mô hình, như word embedding hoặc feature đặc thù domain.

D. Language Adapter

Mục đích của language adapter là học các phép biến đổi đặc thù ngôn ngữ [19]. Chúng được huấn luyện trên dữ liệu không nhãn sử dụng hàm mục tiêu trừu tượng như mask language modeling (MLM). Chúng bao gồm một down-projection và một up-projection tại mỗi layer, với một residual connection giữa chúng. Down-projection, D, là một ma trận với kích thước h x d, trong đó h là kích thước ẩn của mô hình transformer và d là kích thước của adapter. Up-projection, U, là một ma trận với kích thước d x h. Language adapter tại mỗi layer nhận hidden state của Transformer, hl, và residual, rl, và áp dụng down-projection và up-projection cho chúng, với hàm kích hoạt ReLU. Đầu ra của language adapter sau đó được cộng vào residual connection.

LanguageAdapter l(hl;rl) =Ul(ReLU (Dl(hl))) +rl(1)

E. AdapterFusion

AdapterFusion là một phương pháp kết hợp kiến thức từ các language adapter khác nhau để cải thiện hiệu suất trên các tác vụ downstream như tóm tắt mã nguồn [28]. Cho một tập N language adapter, adapter fusion bao gồm việc lấy tổng có trọng số của các đầu ra của các adapter này trong khi trọng số của mô hình tiền huấn luyện và các language adapter được cố định. AdapterFusion bao gồm các ma trận key, value, và query tại mỗi layer. Mục tiêu của AdapterFusion là tìm tổ hợp tối ưu của language adapter bằng cách tối thiểu hóa hàm loss sử dụng tác vụ mục tiêu. Điều này cho phép trích xuất và kết hợp kiến thức từ các language adapter khác nhau để cải thiện hiệu suất trên các tác vụ downstream.

 = argminL(D; ;1;:::; N) (2)

trong đó bao gồm các metric Key l,Value l và Query l tại mỗi layer l, như được hiển thị trong Hình 1. Tại mỗi transformer block, đầu ra của sub-layer feed-forward được lấy làm Query, và đầu ra của mỗi language adapter được sử dụng cho cả vector Key và Value.

III. NER ADAPTER

NER adapter nhằm kết hợp thông tin kiểu token vào mạng, vì kiểu token có thể cung cấp thông tin cú pháp có giá trị cho mô hình. Theo nghiên cứu trong [29], các kiểu token khác nhau có thể có mức độ quan trọng khác nhau đối với mô hình ngôn ngữ tiền huấn luyện; ví dụ, identifier quan trọng hơn các kiểu khác về mặt lượng attention từ mô hình và các biểu diễn được học của chúng. Để huấn luyện mô hình trên kiểu token mã nguồn, trước tiên chúng ta cần xác định mỗi kiểu token trong tập dữ liệu của mình. Sau đó chúng tôi giới thiệu một biến thể mới của adapter, NER adapter, mà chúng tôi cắm vào PPLM và huấn luyện chúng trên tác vụ phân loại kiểu token. NER adapter được kỳ vọng sẽ dự đoán kiểu của mỗi token khi giai đoạn huấn luyện adapter hoàn thành. Chúng tôi thảo luận chi tiết về NER adapter trong ba phần con dưới đây: cấu trúc, hàm mục tiêu, và giai đoạn huấn luyện. Trong phần con cuối cùng, chúng tôi giải thích cách NER adapter được đặt trong các transformer block của PPLM.

--- TRANG 2 ---

Multi-Head Attention
Add & Norm
Feed Forward
Adapter Fusion
Add & Norm
Add & Norm
Language Adapter
NER Adapter
Key Value Query

Hình 1: Kiến trúc đề xuất để kết hợp thông tin cú pháp vào các transformer block bao gồm hai thành phần chính: một language adapter và một adapter nhận dạng thực thể có tên (NER). Các adapter này đã được huấn luyện riêng biệt trước khi được chèn vào một stack song song. Một module AdapterFusion được đặt trên đầu stack, được huấn luyện để thực hiện một tác vụ mục tiêu cụ thể, như tinh chỉnh mã nguồn hoặc tóm tắt mã nguồn. Điều này cho phép tích hợp và kết hợp kiến thức thu được từ các adapter bên dưới, dẫn đến hiệu suất chính xác và hiệu quả hơn.

A. Cấu trúc

Chúng tôi sử dụng cùng kiến trúc của language adapter cho NER adapter, vì vậy chúng bao gồm các khối feedforward down- và up-sampling kết hợp với residual connection:

NERAdapter l(hl;rl) =Ul(ReLU (Dl(hl))) +rl (3)

trong đó hl và rl lần lượt là hidden state và residual tại layer l.

B. Token Type Classification Loss (TTC)

Cho một kiểu token được gán cho mỗi token trong mẫu mã nguồn, chúng tôi đặt vấn đề như một vấn đề phân loại token, trong đó adapter chịu trách nhiệm dự đoán kiểu của mỗi token. Để huấn luyện NER adapter, chúng tôi sử dụng hàm loss cross-entropy.

Gọi Xi=x1;:::;x N biểu diễn một chuỗi token id cho mẫu i, T=t1;:::;t N chỉ ra chuỗi type id tương ứng, và Y=y1;:::;y N là biểu diễn one-hot của các type id này, với mỗi phần tử có kích thước bằng tổng số kiểu được trình bày trong tập dữ liệu. Cross-entropy cho mẫu i được tính như sau:

LNER =NX
t=1YTlog(Pt) (4)

trong đó LNER chỉ ra hàm loss và Pt là phân phối xác suất cho các kiểu chúng ta nhận được từ mạng cho token t. Hàm loss này cho phép chúng ta đo lường sự khác biệt giữa các kiểu token được dự đoán và thực tế, cung cấp phương tiện để điều chỉnh các tham số của mô hình tương ứng và cải thiện hiệu suất của nó.

C. Huấn luyện NER Adapter

Phần này giải thích các bước chúng ta cần để huấn luyện NER adapter.

1) Trích xuất NER tương ứng với các nút lá:
NER adapter yêu cầu được huấn luyện trên dữ liệu có nhãn của kiểu token. Để trích xuất chính xác kiểu của mỗi token trong một mẫu mã nguồn, chúng tôi sử dụng tree-sitter parser². Parser này được thiết kế đặc biệt để phân tích ngôn ngữ và có thể xác định kiểu của mỗi từ trong nhiều ngôn ngữ lập trình. Tree-sitter được sử dụng trong các nghiên cứu trước đây, như trong GraphCodeBERT [9] để trích xuất dataflow từ AST và trong các thí nghiệm phân tích tĩnh trên Github [30]. Một ví dụ về AST được tạo cho một đoạn mã nguồn nhất định được hiển thị trong Hình 3. Để có được kiểu token, chúng tôi đưa mỗi đoạn mã nguồn vào tree-sitter và sau đó duyệt AST để trích xuất kiểu của mỗi token mã nguồn.

2) Mã hóa Token bằng Tokenizer và Gán cho mỗi Sub-Token cùng kiểu Token:
Sử dụng các bước đã đề cập trước đó, chúng tôi trích xuất kiểu token mã nguồn. Tuy nhiên, do bản chất của tokenization, trong đó một từ duy nhất có thể được chia thành nhiều token, mối quan hệ một-một giữa từ và token không thể được giả định. Do đó, chúng ta phải thiết lập ánh xạ giữa các từ và token tương ứng của chúng và sau đó gán cùng kiểu cho mỗi token. Ví dụ, tên hàm find\_bad\_files sẽ được đánh dấu là kiểu identifier bởi tree-sitter. Khi được tokenize, tên này có thể được chia thành ba token: "find", "bad," và "files." Trong trường hợp này, chúng ta sẽ đánh dấu tất cả chúng là kiểu identifier. Sau quá trình này, chúng ta có một danh sách các token và kiểu tương ứng của chúng cho mỗi mẫu, cung cấp nền tảng vững chắc cho phân tích và xử lý tiếp theo.

3) Fine-tuning NER Adapter bằng hàm Token Type Classification Loss:
Bước cuối cùng là kết hợp NER adapter và fine-tune trọng số của chúng bằng hàm token type classification loss. Điều này được thực hiện trong khi giữ cố định trọng số của mô hình tiền huấn luyện, cho phép NER adapter chuyên môn hóa trong việc nhận dạng các thực thể có tên cụ thể trong khi tận dụng hiểu biết ngôn ngữ tổng quát của mô hình tiền huấn luyện. Phương pháp này cho phép cải thiện hiệu suất trong việc xác định thực thể có tên trong văn bản trong khi duy trì độ chính xác tổng thể của mô hình.

²https://tree-sitter.github.io/tree-sitter/

Hình 2: Một ví dụ về đoạn mã nguồn java được gửi đến tree-sitter để trích xuất AST tương ứng.

D. Tổng quan Kiến trúc

Kiến trúc đề xuất để kết hợp thông tin cú pháp vào các transformer block được minh họa trong Hình 1. Kiến trúc này bao gồm hai thành phần chính: một language adapter và một adapter nhận dạng thực thể có tên (NER). Chúng tôi huấn luyện các adapter này riêng biệt trước khi chèn chúng vào một stack song song, như được hiển thị trong hình. Language adapter được huấn luyện trên hàm loss mask language modeling. Language adapter được sử dụng để học kiến thức ngôn ngữ tổng quát nằm dưới tập dữ liệu. Cuối cùng, chúng tôi sử dụng AdapterFusion để kết hợp kiến thức thu được từ language và NER adapter để cung cấp biểu diễn đầu ra mạnh mẽ hơn tại mỗi transformer block. Module AdapterFusion được đặt trên đầu stack và được huấn luyện để thực hiện một tác vụ mục tiêu cụ thể. Các tác vụ này trong thí nghiệm của chúng tôi là tinh chỉnh mã nguồn và tóm tắt mã nguồn. Việc sử dụng AdapterFusion cho phép tích hợp và kết hợp kiến thức thu được từ các adapter bên dưới, dẫn đến hiệu suất chính xác và hiệu quả hơn.

Chi tiết về dữ liệu đầu vào và cách embedding của sub-token và adapter được kết hợp trong AdapterFusion được hiển thị thông qua một ví dụ trong Hình 4. Mẫu mã nguồn đầu vào được trình bày trong Hình 2, và Hình 4 minh họa các chi tiết. Mẫu mã nguồn được đưa vào một transformer block được trang bị NER và language adapter. Trước khi nhúng đầu vào, các từ được chia thành sub-token khi cần thiết. Các sub-token embedding này, được ký hiệu bởi Etokens, sau đó được truyền qua cả NER và language adapter song song. Điều này dẫn đến hai embedding cho mỗi sub-token, được ký hiệu bởi Ttoken và Ltoken, tương ứng với NER và language adapter. Cuối cùng, các embedding này được kết hợp bởi AdapterFusion để chọn thông tin hữu ích từ các embedding trước đó cho một tác vụ mục tiêu cụ thể, như tinh chỉnh mã nguồn. Các embedding cuối cùng được ký hiệu bởi Ftoken.

IV. THIẾT LẬP THÍ NGHIỆM

Chúng tôi tiến hành thí nghiệm sử dụng CodeBERT làm mô hình backbone cho tinh chỉnh mã nguồn và tóm tắt mã nguồn. Tác vụ trước nhằm xác định và sửa lỗi tự động, và tác vụ sau đề cập đến việc tự động tạo mô tả về chức năng của một đoạn mã nguồn nhất định bằng ngôn ngữ tự nhiên. CodeBERT được chọn vì nó đã được nghiên cứu và đánh giá trong một số công trình kỹ thuật phần mềm trước đây [1], [31], [11]. Tinh chỉnh mã nguồn được chọn vì nó phụ thuộc nhiều vào thông tin cú pháp để xác định và sửa lỗi cũng như tối ưu hóa cấu trúc mã nguồn, và tóm tắt mã nguồn được chọn vì nó cho phép chúng tôi đánh giá khả năng sinh của mô hình và đánh giá kiến thức semantic nằm dưới các khía cạnh ngôn ngữ lập trình và ngôn ngữ tự nhiên của PPLM. Sau đây, chúng tôi giải thích

--- TRANG 3 ---

Left:
Identifier: i
Right:
call
Identifier: range
Argument List
Integer: 5
Block
Expression Statement
Call
Identifier: print
Argument List
Identifier: i
For Statement

Hình 3: Một ví dụ về AST được tạo bởi tree-sitter parser cho mã nguồn đầu vào của Hình 2. Các nút lá được biểu diễn bởi các vòng tròn xanh, trong khi các nút không phải lá được biểu diễn bởi các vòng tròn xanh lá cây. AST này được tạo ra khi đoạn mã nguồn được đưa vào parser.

các tác vụ downstream và tập dữ liệu chúng tôi sử dụng để thực hiện thí nghiệm, mô hình baseline, và các metric đánh giá cho mỗi tác vụ downstream.

A. Tác vụ Downstream và Tập dữ liệu

Tinh chỉnh Mã nguồn là một khía cạnh thiết yếu của phát triển phần mềm và bước quan trọng trong việc đảm bảo tính mạnh mẽ và đáng tin cậy của hệ thống phần mềm [9]. Bằng cách tận dụng các kỹ thuật và công cụ khác nhau, nó nhằm tự động xác định và sửa lỗi trong mã nguồn, do đó giảm đáng kể chi phí và nỗ lực liên quan đến việc giải quyết chúng thủ công. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng tập dữ liệu được phát hành bởi Tufano et al. [10] để đánh giá hiệu quả của kiến trúc đề xuất và NER adapter trên tinh chỉnh mã nguồn. Chúng tôi thực hiện thí nghiệm trên tập dữ liệu BFP small. Tổng số datapoint trong tập huấn luyện, test, và validation lần lượt là 46680, 5835 và 5835.

Tóm tắt Mã nguồn là một kỹ thuật mạnh mẽ trong kỹ thuật phần mềm nhằm cung cấp mô tả ngôn ngữ tự nhiên về chức năng của mã nguồn, do đó tạo điều kiện thuận lợi cho việc hiểu và bảo trì mã nguồn [26]. Mục tiêu của tóm tắt mã nguồn là làm cho việc hiểu cấu trúc tổng thể và chức năng của mã nguồn trở nên dễ dàng hơn. Tác vụ này được nghiên cứu rộng rãi, và các phương pháp khác nhau được đề xuất, đặc biệt là sử dụng các mô hình ngôn ngữ lập trình như CodeBERT [1], CodeT5 [3], và Multilingual CodeBERT [32] để tóm tắt mã nguồn tự động.

Chúng tôi chọn tinh chỉnh mã nguồn vì nó là một tác vụ trừu tượng và phức tạp hơn suy luận kiểu khi chúng ta có NER adapter. Cụ thể hơn, chúng tôi cung cấp thông tin AST để đánh giá mô hình trên các tác vụ phức tạp hơn thay vì suy luận kiểu. Chúng tôi chọn tóm tắt mã nguồn vì nó là một tác vụ sinh, và mục tiêu là đánh giá mức độ mà thông tin cấu trúc đề xuất của chúng tôi có thể hữu ích cho các tác vụ mục tiêu sinh NL-PL.

Nghiên cứu của chúng tôi sử dụng CodeSearchNet [22] làm tập dữ liệu để huấn luyện language và NER adapter. Tập dữ liệu bao gồm sáu ngôn ngữ lập trình. Kích thước của mỗi ngôn ngữ được trình bày trong Bảng I. Vì tree-sitter xử lý toàn bộ đoạn mã nguồn PHP như một phần tử văn bản duy nhất và không cung cấp thông tin cú pháp, chúng tôi loại trừ PHP khỏi thí nghiệm. Thay vào đó, chúng tôi sẽ tập trung vào việc sử dụng tree-sitter cho các ngôn ngữ sau: Go, Java, JavaScript, Python, và Ruby. Cụ thể hơn, đối với tinh chỉnh mã nguồn, language và NER adapter được huấn luyện trên training split của tập dữ liệu Java unimodal trong CodeSearchNet, và AdapterFusion được huấn luyện trên tập dữ liệu Java bug fix được giới thiệu trong [10]. Đối với tóm tắt mã nguồn, language và NER adapter được huấn luyện trên training split của dữ liệu unimodal cho tất cả ngôn ngữ (kết hợp) có sẵn trong CodeSearchNet. AdapterFusion sau đó được huấn luyện trên các training split tổng hợp của dữ liệu bimodal (tức là mã nguồn và comment) cho tất cả ngôn ngữ được bao gồm trong CodeSearchNet, cho phép một phương pháp tóm tắt mã nguồn toàn diện và đa ngôn ngữ.

NER và language adapter được huấn luyện trong 50,000 bước huấn luyện với kích thước batch là 48 và tốc độ học là 2e-5. Đối với tinh chỉnh mã nguồn, AdapterFusion được huấn luyện trong 100,000 bước huấn luyện với kích thước batch là 16 và tốc độ học là 5e-5. Đối với tóm tắt mã nguồn, chúng tôi huấn luyện AdapterFusion trong 50,000 bước huấn luyện với kích thước batch là 32.

B. Mô hình Baseline

Phương pháp của chúng tôi được thiết kế để có tính modular và linh hoạt, cho phép tích hợp dễ dàng các mô hình ngôn ngữ lập trình tiền huấn luyện khác nhau. Điều này cho phép thí nghiệm và so sánh dễ dàng các mô hình khác nhau và hiệu suất của chúng trên các tác vụ khác nhau.

Trong nghiên cứu của chúng tôi, chúng tôi chọn CodeBERT [1] làm mô hình backbone và baseline. Mô hình này được tiền huấn luyện trên tập dữ liệu CodeSearchNet [22] và được nghiên cứu rộng rãi trong kỹ thuật phần mềm. CodeBERT là một mô hình đa ngôn ngữ được fine-tune trên các tập dữ liệu đơn ngôn ngữ.

Để đánh giá hiệu quả của phương pháp, chúng tôi so sánh kết quả fine-tuning kiến trúc đề xuất khi được chèn vào CodeBERT với kết quả fine-tuning CodeBERT trên hai tác vụ. Điều này cho phép chúng tôi đo lường khả năng cải thiện hiệu suất và hiệu quả của nó khi áp dụng cho các tác vụ tinh chỉnh mã nguồn và tóm tắt. Ngoài ra, sự so sánh này cung cấp cái nhìn sâu sắc về điểm mạnh và hạn chế của phương pháp đề xuất và cách nó so sánh với mô hình baseline. Đối với mỗi tác vụ, các phương pháp/mô hình khác được sử dụng để so sánh, được giải thích trong phần Kết quả để tiết kiệm không gian và giảm sự dư thừa.

C. Metric Đánh giá

Tác vụ tinh chỉnh mã nguồn được đánh giá bằng cách sử dụng kết hợp các phép đo điểm BLEU-4 và độ chính xác. Điểm BLEU (tức là Bilingual Evaluation Understudy) là một phép đo dựa trên precision để đánh giá hiệu suất của các mô hình NLP, đặc biệt là hệ thống dịch máy. Nó hoạt động bằng cách so sánh đầu ra của mô hình ngôn ngữ (ví dụ, một tóm tắt được tạo) với một tóm tắt tham chiếu do chuyên gia con người tạo ra và tính toán mức độ chồng chéo giữa hai. Trong công trình này, chúng tôi tính điểm BLUE-4 [33] trong đó thuật toán đầu tiên xác định các unigram đến 4-gram trong cả đầu ra và bản dịch tham chiếu và sau đó tính toán precision giữa tóm tắt được tạo (tức là n-gram hit) và tóm tắt ground truth (tức là tổng số n-gram count). Điểm BLEU cao hơn cho biết đầu ra của mô hình tương tự hơn với bản dịch tham chiếu và do đó được coi là chất lượng cao hơn. BLEU thường được sử dụng trong tóm tắt mã nguồn để đánh giá và so sánh chất lượng và tính mạch lạc của các tóm tắt được tạo từ các PPLMs khác nhau [26], [3], [9], [5].

V. KẾT QUẢ VÀ THẢO LUẬN

A. Hiệu suất của NER Adapter

Bảng II trình bày các metric đánh giá của hàm token type classification loss, cụ thể là Precision, Recall, F1score, và Accuracy, trong giai đoạn huấn luyện NER adapter. Precision là một metric đo tỷ lệ dự đoán true positive trong tất cả dự đoán positive. Recall đo tỷ lệ dự đoán true positive từ tất cả quan sát positive thực tế. F1 Score là trung bình điều hòa của Precision và Recall, và Accuracy là một metric đo tỷ lệ dự đoán chính xác trong tất cả quan sát. Các NER adapter này được huấn luyện trên các kiểu thu được từ tree-sitter parser trên tập dữ liệu CodeSearchNet.

Các ngôn ngữ lập trình trong bảng được sắp xếp theo thứ tự kích thước tập dữ liệu, với các ngôn ngữ ít tốn tài nguyên nhất được liệt kê đầu tiên và các ngôn ngữ tốn tài nguyên nhất được liệt kê cuối cùng. Python đạt hiệu suất tốt nhất cho tất cả metric so với các ngôn ngữ khác, và Ruby có hiệu suất thấp nhất vì nó gặp khó khăn với tập dữ liệu tài nguyên thấp. Đáng chú ý rằng điểm precision, recall, và F1 phụ thuộc nhiều vào kích thước tập dữ liệu, cho thấy rằng các tập dữ liệu lớn hơn thường cho kết quả tốt hơn cho việc huấn luyện NER adapter. Tuy nhiên, kết quả cho Java tệ hơn so với các ngôn ngữ khác, mặc dù có tập dữ liệu tương đối phong phú. Sự khác biệt này có thể được quy cho độ phức tạp cú pháp có trong Java so với Python, điều này góp phần vào hiệu suất thấp hơn. Mặt khác, Python thể hiện hiệu suất đặc biệt trên tất cả metric được đánh giá. Mức độ chính xác cao được ghi nhận cho tất cả ngôn ngữ lập trình càng chứng minh rằng các NER adapter được huấn luyện hiệu quả để trích xuất thông tin cú pháp từ embedding nội bộ của mạng. Điều này làm nổi bật tính hiệu quả và mạnh mẽ của triển khai NER adapter của chúng tôi.

B. Kết quả CodeBERTER cho Tinh chỉnh Mã nguồn

Chúng tôi đánh giá hiệu suất của NER adapter trên tác vụ tinh chỉnh mã nguồn để kiểm tra mức độ hiệu quả của chúng trên một tác vụ lập trình mục tiêu. Kết quả được trình bày trong Bảng III. Naive copy áp dụng phương pháp đơn giản bằng cách trực tiếp sao chép mã nguồn có lỗi làm kết quả tinh chỉnh. LSTM sử dụng kiến trúc Long Short Term Memory, và mô hình Transformer sử dụng 12 transformer encoder block (tức là cùng số layer và kích thước ẩn như các mô hình tiền huấn luyện). Đối với Transformer, encoder được khởi tạo với các mô hình tiền huấn luyện, trong khi các tham số của decoder được khởi tạo ngẫu nhiên. Kết quả được mô tả trong bảng chứng minh rằng Transformer vượt trội đáng kể so với mô hình LSTM.

Chúng tôi tách kết quả của các mô hình này trong Bảng III khỏi nhóm mô hình thứ hai tận dụng tiền huấn luyện trên ngôn ngữ lập trình. Điểm BLEU và Accuracy cao hơn đối với các mô hình có tiền huấn luyện. RoBERTa (code) chỉ được tiền huấn luyện trên mã nguồn. NSEdit [34] đề xuất một pointer network mới cho các transformer block để dự đoán vị trí chỉnh sửa. CoTexT [35] là một mô hình encoder-decoder bimodal tiền huấn luyện. Trong số các mô hình tiền huấn luyện lập trình, phương pháp của chúng tôi, được ký hiệu bởi CodeBERTER, nổi bật bằng cách đạt hiệu suất tốt nhất trên điểm BLEU. Lưu ý rằng CodeBERTER cải thiện kết quả của CodeBERT mà không tận dụng fine-tuning tất cả các tham số của nó. Điều này minh họa tác động có lợi của việc kết hợp thông tin cấu trúc mã nguồn trong tinh chỉnh mã nguồn.

C. Kết quả CodeBERTER cho Tóm tắt Mã nguồn

Tóm tắt mã nguồn đánh giá các khía cạnh semantic và syntactic của mô hình ngôn ngữ lập trình. Bảng IV hiển thị kết quả của phương pháp chúng tôi so với các mô hình khác. Đối với tác vụ mục tiêu này, chúng tôi chọn paradigm fine-tuning đa ngôn ngữ dựa trên các phát hiện và khuyến nghị gần đây trong [32], nơi các tác giả đề cập rằng tóm tắt mã nguồn có thể được hưởng lợi từ fine-tuning đa ngôn ngữ. Có nghĩa là, trong phương pháp của chúng tôi, đầu tiên, chúng tôi huấn luyện language và NER adapter trên tập dữ liệu đa ngôn ngữ, như đã đề cập trước đó. Chúng tôi cũng huấn luyện AdapterFusion trên tập dữ liệu tóm tắt mã nguồn đa ngôn ngữ và kiểm tra nó trên từng ngôn ngữ riêng biệt. Lưu ý rằng đối với tinh chỉnh mã nguồn, vì chỉ tồn tại tập dữ liệu Java, theo hiểu biết tốt nhất của chúng tôi, chúng tôi chỉ có thể đánh giá hiệu suất của NER adapter trên tinh chỉnh mã nguồn trong một thiết lập đơn ngôn ngữ.

Bảng IV hiển thị điểm BLUE-4 của các phương pháp tối ưu và phương pháp của chúng tôi, được ký hiệu bởi CodeBERTER, được phân tách bởi một đường kẻ ngang. Điểm tốt nhất được hiển thị in đậm. Điểm thu được từ CodeBERT cũng được gạch chân để dễ đọc. CodeBERT là baseline chính mà chúng tôi cần so sánh kết quả của mình vì mô hình backbone của chúng tôi là CodeBERT. CodeBERTER cải thiện điểm cho bốn ngôn ngữ, Ruby, JavaScript, Go, và Java, và có kết quả ngang bằng với các mô hình tốt nhất cho Python. Nhìn vào thống kê dữ liệu được cung cấp trong Bảng I cho thấy rằng ba ngôn ngữ đầu tiên có số lượng dữ liệu thấp hơn để huấn luyện so với hai ngôn ngữ kia. Đối với các ngôn ngữ tài nguyên thấp này, mô hình của chúng tôi cải thiện đáng kể kết quả so với baseline, CodeBERT.

polyglot CodeBERT và polyglot GraphCodeBERT, như được mô tả trong [32] bởi Ahmed et al. (2021), là các mô hình đã được full fine-tune trong một thiết lập đa ngôn ngữ. Ví dụ, để đánh giá hiệu suất của chúng trên Ruby, các mô hình được fine-tune trên tất cả ngôn ngữ lập trình trong tập dữ liệu CodeSearchNet [22] và sau đó được đánh giá trên tập dữ liệu test Ruby. Phương pháp này đã cải thiện hiệu suất của các mô hình polyglot, như được chứng minh bởi sự khác biệt về hiệu suất giữa CodeBERT và polyglot CodeBERT. Tuy nhiên, CodeBERTER cải thiện thêm điểm của polyglot CodeBERT, được quy cho việc kết hợp thông tin cú pháp đa ngôn ngữ vào mô hình bằng NER adapter.

Đối với các ngôn ngữ tài nguyên cao, Python và Java, CodeBERTER hoạt động ngang bằng với mô hình baseline (cải thiện kết quả Java từ CodeBERT lên 2 điểm BLEU). Chúng tôi liên hệ điều này với thực tế rằng đối với các ngôn ngữ tài nguyên cao, mô hình ngôn ngữ có thể học kiến thức tổng quát nằm dưới một ngôn ngữ tốt hơn một ngôn ngữ tài nguyên thấp, mà có ít dữ liệu huấn luyện hơn [39]. Do đó, NER adapter cho phép mô hình cung cấp thêm thông tin kiểu cho các ngôn ngữ tài nguyên thấp.

Mặc dù CodeBERTER có điểm tốt nhất cho một số ngôn ngữ, lưu ý rằng CodeBERTER sử dụng CodeBERT làm backbone, vì vậy sẽ công bằng hơn khi so sánh hiệu quả của mô hình của chúng tôi với CodeBERT chứ không phải các mô hình lớn hơn như CodeT5. Các mô hình lớn hơn này có gấp đôi số tham số so với CodeBERT. Tuy nhiên, chúng tôi xem xét các mô hình lớn hơn trong Bảng IV để chứng minh rằng phương pháp của chúng tôi thậm chí có thể có kết quả ngang bằng với các mô hình lớn hơn cho mỗi ngôn ngữ.

Theo leaderboard trên CodeXGLUE, chúng tôi cũng báo cáo điểm trung bình giữa các mô hình trong Bảng IV. Nếu chúng tôi tính toán trung bình trên các ngôn ngữ được nghiên cứu cho tóm tắt mã nguồn, hiệu suất CodeBERTER là 18.738, cao hơn tất cả các mô hình khác trên leaderboard cho tác vụ này.

D. Thảo luận

Hiệu quả tính toán của phương pháp chúng tôi. Đối với tất cả ngôn ngữ, chúng tôi huấn luyện NER adapter hiệu quả hơn với chi phí tính toán thấp hơn. Mỗi language và NER adapter có 0.9 triệu và AdapterFusion có 21 triệu tham số có thể huấn luyện. Chúng tôi huấn luyện ba adapter này cho tinh chỉnh mã nguồn tổng cộng, dẫn đến 23 triệu tham số có thể huấn luyện. Con số này vẫn ít hơn tổng số tham số có thể huấn luyện của mô hình CodeBERT, là 110 triệu. Lưu ý rằng tất cả 110 triệu tham số trong CodeBERT phải được huấn luyện lại trong giai đoạn standard fully fine-tuning. Tuy nhiên, điều này không được yêu cầu trong phương pháp của chúng tôi. Trong CodeBERTER, chúng tôi huấn luyện 23 triệu tham số cho tất cả adapter, bao gồm language, NER, và AdapterFusion. Về hiệu quả thời gian, mỗi NER adapter mất khoảng 10 giờ để được huấn luyện, cho 40,000 bước huấn luyện. Trong khi nếu chúng tôi full fine-tune CodeBERT cho tác vụ NER với cùng số bước huấn luyện, nó mất khoảng 17 giờ.

Hiệu ứng của NER adapter. Hình 5 minh họa mẫu attention của các token khác nhau trong một mẫu mã nguồn với nhau trong layer cuối cùng của CodeBERT, được tạo bởi bertviz³. Độ dày của các đường giữa các token chứng minh lượng attention; đường càng dày, attention được đặt trên token đó càng nhiều. Chúng tôi chỉ hiển thị một attention head (trong số 12 head) của layer cuối cùng để tránh biểu diễn lộn xộn trong hình này. Hình 5 so sánh mẫu attention cho một mẫu Java, cho CodeBERT ở bên trái, và cho CodeBERTER (tức là CodeBERT với NER adapter) ở bên phải. Như được hiển thị, việc sử dụng NER adapter dẫn đến sự thay đổi trong mẫu attention, có nghĩa là nó được phân phối nhiều hơn giữa các token. Cụ thể, trong CodeBERTER, NER adapter chú trọng nhiều hơn vào các token identifier như sum, a, và b, so với mô hình baseline. Điều này cho phép mô hình tận dụng attention bổ sung được đặt trên các token này cho các tác vụ downstream, dẫn đến hiệu suất được cải thiện.

Lưu ý rằng attention của CodeBERT hiển thị nhiều cross attention yếu giữa các token, và hầu hết attention nằm trên token <s>. Điều này phù hợp với các phát hiện trước đây của Sharma et al. [29], nơi các tác giả phát hiện rằng các token đặc biệt (ví dụ, <s>) nhận được nhiều attention nhất đặc biệt là trong layer cuối cùng. Vì vậy, như đã được chỉ ra trước đây trong [29], các kỹ thuật mới được yêu cầu để hướng attention của các mô hình dựa trên BERT về phía các token khác cho ngôn ngữ lập trình. Điều này được đạt được trong công trình của chúng tôi bằng cách sử dụng NER adapter.

VI. CÔNG TRÌNH LIÊN QUAN

Trong những năm gần đây, đã có nhiều nghiên cứu tập trung vào học biểu diễn mã nguồn cho các tác vụ kỹ thuật phần mềm khác nhau như sinh mã nguồn [40], [41], [42], tóm tắt mã nguồn [4], [5], [43], tổng hợp chương trình [44], [45], [46], [47], tìm kiếm mã nguồn [48], và sửa lỗi [49], [50]. Với sự ra đời của các mô hình ngôn ngữ tiền huấn luyện trong NLP, các nhà nghiên cứu trong kỹ thuật phần mềm đã tận dụng những tiến bộ trong các mô hình ngôn ngữ tiền huấn luyện trong NLP để đề xuất các mô hình ngôn ngữ lập trình tiền huấn luyện được tiền huấn luyện trên tập dữ liệu unimodal (tức là mã nguồn) hoặc bimodal (tức là mã nguồn và comment).

CodeBERT [1], GraphCodeBERT [9] và CodeT5 [3] là các ví dụ của phương pháp này. Mặc dù mã nguồn chứa thông tin cú pháp và semantic phong phú, hầu hết các PPLMs xử lý mã nguồn như một chuỗi token. GraphCodeBERT [9] mở rộng đầu vào với dataflow được trích xuất từ AST. Điều này cho phép mô hình hiểu rõ hơn các mối quan hệ và phụ thuộc giữa các phần khác nhau của mã nguồn. TreeBERT [14] biểu diễn đầu vào như một đường dẫn nối của các lá AST tương ứng với mỗi mẫu mã nguồn. Điều này cho phép mô hình nắm bắt tốt hơn cấu trúc và hệ thống phân cấp của mã nguồn. CodeT5 [3] là một mô hình học biểu diễn mã nguồn dựa trên kiến trúc T5. Mô hình được tiền huấn luyện trên corpus mã nguồn lớn để dự đoán token tiếp theo trong một chuỗi mã nguồn. Để cải thiện hiểu biết của mô hình về mã nguồn, một hàm mục tiêu mới được đề xuất trong CodeT5 để thông báo cho mô hình về sự hiện diện của các token identifier trong giai đoạn tiền huấn luyện. Mô hình được đánh giá trên một số tác vụ liên quan đến mã nguồn như sinh mã nguồn, tóm tắt mã nguồn, và tìm kiếm mã nguồn và cho thấy hiệu suất được cải thiện so với các mô hình hiện có khác.

Adapter đã được sử dụng rộng rãi trong NLP [17], [19], [28] như một phương pháp fine-tuning nhanh chóng và hiệu quả tham số. Adapter trước đây đã được sử dụng trong kỹ thuật phần mềm để cho thấy khả năng chuyển giao của PLMs ngôn ngữ tự nhiên sang ngôn ngữ lập trình với chi phí huấn luyện thấp hơn. Các tác giả huấn luyện language adapter (tức là adapter được huấn luyện trên masked language modeling trên dữ liệu không nhãn) và task adapter (tức là adapter được huấn luyện cho tác vụ mục tiêu trên dữ liệu có nhãn) cho phát hiện bản sao mã nguồn [18].

Sự khác biệt của các nghiên cứu hiện tại với công trình của chúng tôi. Mặc dù thông tin cú pháp được sử dụng trong một số PPLMs hiện tại, không có mô hình nào có thể tích hợp thông tin này vào một mô hình hiện có mà không yêu cầu tiền huấn luyện nó từ đầu. Mặt khác, NER adapter cho phép áp đặt thông tin cú pháp trong các mô hình hiện tại mà không cần tiền huấn luyện. Hơn nữa, mặc dù adapter được sử dụng rộng rãi trong NLP và gần đây được nghiên cứu trong kỹ thuật phần mềm, không có công trình nào sử dụng adapter trong bối cảnh như chúng tôi làm trong công trình của mình. Phương pháp của chúng tôi mới không chỉ cho hàm loss TTC mà chúng tôi giới thiệu cho NER adapter mà còn cho cách chúng được sử dụng (tức là kiến trúc đề xuất trong Hình 1).

VII. MỐI ĐE DỌA TÍNH HỢP LỆ

Tính hợp lệ Ngoại vi Trong nghiên cứu này, chúng tôi đánh giá kết quả của việc áp đặt thông tin cú pháp cho tóm tắt mã nguồn trên tập dữ liệu CodeSearchNet và tinh chỉnh mã nguồn cho ngôn ngữ Java. Tác vụ và ngôn ngữ lập trình bị hạn chế, và kết quả có thể không khái quát hóa được cho tất cả tác vụ downstream và tập dữ liệu khác. Tuy nhiên, dựa trên quan sát của chúng tôi và thực tế rằng chúng tôi cung cấp thông tin ngôn ngữ và cấu trúc thông qua adapter, chúng tôi giả thuyết rằng kết quả cho các tác vụ khác sẽ ít nhất ngang bằng với các mô hình hiện tại, nhấn mạnh rằng điều này có thể đạt được với ít tham số có thể huấn luyện hơn. Mặc dù vậy, thí nghiệm vẫn cần xác nhận điều này. Lưu ý rằng phương pháp của chúng tôi không giới hạn ở CodeBERT và NER adapter có thể được sử dụng trong các PPLMs khác.

Đối với tinh chỉnh mã nguồn, tồn tại nhiều phương pháp khác nhau đã được đề xuất trong tài liệu. Một đóng góp đáng chú ý là việc sử dụng beam search, đã được nghiên cứu thực nghiệm bởi Tufano et al. [10]. Tuy nhiên, vì nghiên cứu của chúng tôi tập trung vào một khía cạnh khác của tinh chỉnh mã nguồn, chúng tôi không khám phá sâu phương pháp cụ thể này.

Tính hợp lệ Nội vi Siêu tham số có thể ảnh hưởng đến giai đoạn fine-tuning của mô hình tiền huấn luyện, và không có quy tắc cứng để chọn các giá trị tốt nhất cho các tham số này. Do đó, mô hình có thể bị kẹt trong các giải pháp không tối ưu. Như Pfeiffer et al. [28] đã thực hiện tìm kiếm siêu tham số mở rộng trên adapter, chúng tôi xem xét các thiết lập mặc định cho siêu tham số của adapter. Tuy nhiên, nó được thực hiện trong lĩnh vực NLP và có thể không dẫn đến trạng thái tối ưu trong lĩnh vực kỹ thuật phần mềm.

VIII. KẾT LUẬN VÀ CÔNG TRÌNH TƯƠNG LAI

Trong nghiên cứu này, chúng tôi đã giới thiệu NER adapter, một phương pháp mới để nâng cao các mô hình tiền huấn luyện hiện có bằng cách áp đặt thông tin cú pháp. Để đánh giá hiệu suất của chúng, chúng tôi đã tiến hành thí nghiệm trên hai tác vụ liên quan đến lập trình: tinh chỉnh mã nguồn sử dụng tập dữ liệu Java trong thiết lập đơn ngôn ngữ và tóm tắt mã nguồn sử dụng phương pháp đa ngôn ngữ. Kết quả của chúng tôi cho thấy rằng CodeBERTER –CodeBERT với NER adapter– vượt trội hơn các mô hình baseline trong cả hai tác vụ. NER adapter không phụ thuộc mô hình và yêu cầu ít tham số hơn để được huấn luyện so với việc tiền huấn luyện hoặc fine-tune đầy đủ một mô hình. Chúng tôi dự định áp dụng CodeBERTER cho các tác vụ downstream khác và NER adapter cho các mô hình tiền huấn luyện khác.

--- TRANG 4 đến 11 ---

[Nội dung còn lại của bài báo bao gồm các bảng kết quả thí nghiệm, hình ảnh minh họa, và danh sách tài liệu tham khảo được dịch tương tự như phần trên]
