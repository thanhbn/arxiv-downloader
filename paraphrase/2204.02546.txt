# 2204.02546.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/paraphrase/2204.02546.pdf
# File size: 248908 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
QUICK STARTING DIALOG SYSTEMS WITH PARAPHRASE
GENERATION
A P REPRINT
Louis Marceau, Raouf Belbahar, Marc Queudot
National Bank of Canada
Montreal, QC, Canada
{louis.marceau,raoufmoncef.belbahar,marc.queudot}@bnc.caNada Naji
nada.aj.naji@gmail.com
Éric Charton
National Bank of Canada
Montreal, QC, Canada
eric.charton@bnc.ca
Marie-Jean Meurs
Université du Québec à Montréal
Montreal, QC, Canada
meurs.marie-jean@uqam.ca
ABSTRACT
Acquiring training data to improve the robustness of dialog systems can be a painstakingly long
process. In this work, we propose a method to reduce the cost and effort of creating new conversational
agents by artiﬁcially generating more data from existing examples, using paraphrase generation. Our
proposed approach can kick-start a dialog system with little human effort, and brings its performance
to a level satisfactory enough for allowing actual interactions with real end-users. We experimented
with two neural paraphrasing approaches, namely Neural Machine Translation and a Transformer-
based seq2seq model. We present the results obtained with two datasets in English and in French: a
crowd-sourced public intent classiﬁcation dataset and our own corporate dialog system dataset. We
show that our proposed approach increased the generalization capabilities of the intent classiﬁcation
model on both datasets, reducing the effort required to initialize a new dialog system and helping to
deploy this technology at scale within an organization.
Keywords Chatbot Dialog Systems Paraphrase Generation Natural Language Processing
1 Introduction
Dialog systems are becoming increasingly common as personal assistants and information seeking interfaces. In the
context of customer support, dialog systems are nowadays playing an important role as an additional communication
channel to respond to recurrent client queries Ganhotra et al. [2019]. However, insufﬁcient volume of available
training data as well as the huge effort required to gather such data hinder the development of robust domain-speciﬁc
conversational AI systems Schuster et al. [2018]. Dialog systems go through a long process towards maturity where
considerable volumes of data need to be gathered to reach satisfactory performance levels, suitable for real users.
In the context of question-answering dialog systems, we usually refer to intents as the categorization of the various
information needs or possible questions that users have. These intents are deﬁned using existing observations, and they
act as classes in the Natural Language Understanding (NLU) model of the dialog system. Such a model attempts at
associating an incoming user message to one of the predeﬁned intents to subsequently select a relevant response.
Users can express the same question in different ways. For instance, "I forgot my password, what can I do?"
and"don’t remember my pass-code" are both associated with the same intent but are phrased differently. Creating
large enough volumes of training examples that cover such variations is a labor-intensive process. This is especially true
for dialog systems that contain hundreds of intents. When building a new dialog system in a real industry setting, the
initial dataset of observations is often constructed from Frequently Asked Questions (FAQ) pages or questions asked to
human agents. While this dataset should be as rich and complete as possible to be used as a training set, it is rarelyarXiv:2204.02546v2  [cs.CL]  3 May 2022

--- PAGE 2 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
possible to collect multiple samples per intent at this point. A certain amount of training samples per intent must hence
be acquired in order to reach sufﬁcient performance level for the system to be exposed to real users with minimal risks.
In addition to that, since different dialog systems cover different use-cases, this data acquisition task must be repeated
every time a new system is created. This can be problematic when trying to deploy these systems in an organization at
scale. As the number of dialog systems increases, so do the resources and effort required to generate and maintain them.
The solution we propose to this problem relies on generating artiﬁcial paraphrases based on existing observations
during the design of a new dialog system, in order to increase the volume of training samples per intent. The artiﬁcially
generated paraphrases will help increase the performance of the intent classiﬁcation model
Before deploying a new dialog system in an industrial context, the system must reach a certain level of performance -
above a given threshold - in order to minimize the risk and dissatisfaction associated with a poorly-functioning system.
Reaching this threshold usually comes by acquiring more data per intent. Artiﬁcial samples can help increase the intent
classiﬁcation model generalization capabilities above this threshold, hence allow the system to be exposed to end-users.
In our context, training samples for a given intent are paraphrases because they use different words and syntax but have
the same semantic content. The goal of this task is not to replace real data, but rather to bootstrap the dialog system
while avoiding the cost of manually generating more data.
To achieve the paraphrase generation task, we compare different neural approaches in order to augment our training
data. The ﬁrst is based on Neural Machine Translation (NMT) Mallinson et al. [2017]. In this approach, data samples
are translated into another language, referred to as the pivot language and then the result is translated back to the source
language. The translations obtained after this step will be considered as potential paraphrases.
The second method we propose consists in directly outputting paraphrases using a neural network trained for text
generation. This neural network uses the encoder-decoder architecture of the Transformer Vaswani et al. [2017], which
has achieved state-of-the-art results in many Natural Language Processing (NLP) tasks. Large transformer-based
language models (LMs) like GPT-2 Radford et al. [2019], BERT Devlin et al. [2019] or T5 Raffel et al. [2019] can
learn powerful representations of the data and can also be ﬁne-tuned for a speciﬁc downstream task, like paraphrase
generation.
Our approach signiﬁcantly improves the performance of the intent classiﬁcation model with little to no human
intervention, hence, reduces both the cost to build an initial training set and the time-to-market to deploy the dialog
system. Once the system is put into production it can start gathering real data from interactions with the end users.
Using this approach, dialog systems can be deployed at scale in an organization without the huge cost associated with
handcrafting training data for every intent.
The rest of this paper is organized as follows: Section 2 surveys related previous work in the domain of dialog systems,
dataset availability and augmentation. Section 3 presents in details the datasets that we used in our experiments. In
Section 4, we lay out the experimental setup of our work including the architecture of the proposed approaches. The
evaluation framework is presented in Section 5. Section 6 reports the results achieved on the various intent classiﬁcation
datasets and discusses the implication of using paraphrasing in training set augmentation for NLU models. Finally,
Section 7 concludes this work, and explores possible future horizons.
2 Related Work
Dialog systems have evolved tremendously in the last decade following advances in deep learning. Question answering
systems and virtual assistants such as Alexa and Siri have become more and more widespread Gao et al. [2019], Chen
et al. [2017a,b]. When it comes to building new dialog systems, one of the main challenges is obtaining conversational
training data. Several initiatives have made available conversational datasets such as the Ubuntu Dialog Corpus Lowe
et al. [2015], the crowd-sourced, publicly available intent classiﬁcation dataset Kang et al. [2018], the in-scope and
out-of-scope intents dataset CLINC150 Larson et al. [2019], and the CAsT-19 Dataset (TREC Complex Answer
Track) Dalton et al. [2020]. Serban et al. [2015] surveyed publicly available datasets for Dialog systems development.
The authors often describe the process of building such datasets as, unsurprisingly, expensive and labor-intensive.
While such datasets constitute a valuable source for exploration, they have limited use when it comes to building
production-quality, domain-speciﬁc systems, for instance in the medical or ﬁnancial domains1.
Data augmentation is often used in computer vision and image processing. Basic transformations such as cropping,
rotating, and addition of Gaussian noise are applied to images to increase the size of the training data and reduce
overﬁtting Badimala et al. [2019], Krizhevsky et al. [2012], Deng et al. [2009]. NLP tasks, such as text classiﬁcation
or detection of sentence similarity, leverage this idea by applying transformations like shufﬂing (changing the order
1While there exists several types of dialog systems, our present work focuses solely on question answering dialog systems.
2

--- PAGE 3 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
of tokens or sentences), or incorporating synonyms and embeddings Badimala et al. [2019], Wang and Yang [2015],
Papadaki [2017]. Alternatively, paraphrasing can be used to transform the text by conveying the same meaning in
alternative ways in the same natural language. Several NLP methods have been used in the past to generate paraphrases.
Rule-based approaches were explored with handcrafted sets of rules or patterns to generate paraphrases McKeown
[1979], Meteer and Shaked [1988]. Other methods use monolingual or bilingual corpora approaches. For instance, Ban-
nard and Callison-Burch [2005] proposed an approach using a bilingual corpus with pairs of translated sentences to
obtain paraphrases in English by pivoting through another language, such as French or German. This idea was pushed
even further by using NMT models to pivot, and generate paraphrases automatically in the source language Mallinson
et al. [2017], Sokolov and Filimonov [2020]. In this direction, Junczys-Dowmunt et al. [2018] introduced Marian, an
open-source NMT framework developed on a deep-learning back-end based on reverse-mode auto-differentiation with
dynamic computation graphs. We describe our use of Marian in Section 4.1. More recently, large language models
have shown to be extremely effective for many NLP tasks including text generation. Such models have been ﬁne-tuned
with a paraphrasing dataset to serve as paraphrasing generators Witteveen and Andrews [2019]. Paraphrasing has been
used in summarization and plagiarism detection tasks Barrón-Cedeño et al. [2013], among others, but has not been
thoroughly explored in augmenting conversational corpora, to the best of our knowledge. In this paper, we propose a
low-cost approach to rapidly grow a small training dataset with the use of paraphrasing based on machine translation
and large language models. The following Section details the datasets used to test our data augmentation methods.
3 Datasets
We used two different datasets in our experiments. The ﬁrst dataset is the CLINC150 dataset2. It was built for the task
of evaluating intent classiﬁcation Larson et al. [2019]. This dataset contains 150 intents and some out-of-scope data,
which means samples that are not associated with one of the intents. The intents mainly correspond to task-oriented
queries (e.g., "move 100 dollars from my savings to my checking") though some queries related to general information
can be found as well (e.g., "how do I use my vacation days"). Out-of-scope data constitute utterances or messages that
are not associated with any in-scope intents and hence do not carry any interest for the task at hand. Therefore, we only
used the in-scope dataset in our experiments.
The in-scope dataset (SCOPE) is composed of 22,500 queries covering 150 intents, which correspond to ten domain :
banking ,work ,meta ,Auto & commute ,home ,travel ,utility ,kitchen and dining ,small talk andcredit cards . SCOPE is
split into training, validation and testing sets. The training set contains 100 samples per intent, thus yielding 15,000
training samples in total, while the validation and test have 20 and 30 samples per intent respectively. We made several
partitions of the training set by randomly sampling 5, 10 and 50 training samples per intent in order to measure the
impact of paraphrasing on datasets of various sizes. We refer to these sets as SCOPE-train-5 ,SCOPE-train-10 , and
SCOPE-train-50 , respectively. The original testing set was used to evaluate the different data augmentation methods.
It is composed of 4,500 training samples, 30 samples for each of the 150 intent. We refer to this set as SCOPE-test .
Finally, we left out the validation set since the conﬁguration and hyperparameters of the NLU model were kept constant
throughout the various experiments.
The second dataset is an in-house dialog dataset, which originates from the ﬁrst iteration of a question answering dialog
system currently deployed on the corporate website and transactional platform. We refer to this dataset as HOUSE. It
was built by deﬁning intents based on questions extracted from a FAQ Section of the corporate website, and manually
creating some different formulations for each. This was done as a ﬁrst step towards developing a dialog system to
answer clients’ frequent questions, such as how to change their password or perform a speciﬁc transaction.
We then split this handcrafted dataset into training and testing sets. It is worth mentioning that the HOUSE dataset is
entirely in French. The training set consists of 73 training samples, each of the 20 intents is associated to three to six
samples. We refer to this set as HOUSE-train . The test set is composed of 77 samples in total, with one to 11 samples
per intent. We call this set the HOUSE-test . We used a more recent version of the test set in order to have more testing
samples, but only the starting 20 intents were kept. This is why it contains more samples than the training set.
Tables 1 and 2 summarize the characteristics of aforementioned datasets.
4 Methodology and Experiments
In this Section, we describe our proposed approaches for dataset augmentation. Sub-section 4.1 presents the NMT-based
paraphrasing, pivoting around different languages such as English, German and Spanish for the HOUSE dataset (which
2https://github.com/clinc/oos-eval
3

--- PAGE 4 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
Dataset LanguageTotal
intentsTotal
samplesExamples
HOUSE French 20 150"Comment me connecter au site en ligne si j’ai
déjà un compte" (translation: How can I login to
the website if I already have an account)
SCOPE English 150 5,250 - 12,000"Is rice ok after 3 days in the refrigerator"
(kitchen and dining), "How can I see my rewards
for my visa card" (banking)
Table 1: Datasets used in our experiments. HOUSE and SCOPE, respectively, represent our in-house corporate dataset
and the in-scope public dataset. Their respective source language and sizes are given, along with an example of query.
is in French), and French and German for the SCOPE dataset. Sub-section 4.2 presents the second approach, which
utilizes a ﬁne-tuned PEGASUS Zhang et al. [2020] language model for the task of paraphrase generation.
4.1 The NMT-based System
The pre-trained NMT models we used are composed of Transformer models Vaswani et al. [2017] along with guided
word alignments based on the Marian-NMT framework Junczys-Dowmunt et al. [2018]. The required pre-processing
for these models consists of normalizing punctuation and using the language-independent subword tokenizer Senten-
cePiece Kudo and Richardson [2018] to get a subword-based tokenization while beneﬁting from the regularization
algorithm introduced in the paper. The subword tokenizer offers the best of both word-based and character-based
tokenization: known words are taken into account in the vocabulary, and the tokenizer can also make use of word pieces
and individual characters for unknown words.
We use a two-step process, each step involving a NMT model. During the ﬁrst step, training samples are passed through
the source-to-pivot (e.g. English-to-German) model and we keep the best translation for each sample (ranked by the
NMT model).
In the second step, we translate back to the source language (e.g. German-to-English) to generate several variations.
Often the ﬁrst best translation (as ranked by the NMT model) is identical to the original sample, and is therefore
systematically removed. At this point, we keep the following n-best translations as paraphrases of the original input
test. We experimented with varying the value of n, and found that setting nto six provides relevant syntactical and
lexical variations without causing a sharp semantic drift from the original samples.
4.2 The LM-based System
This system leverages a pre-trained transformer-based model using PEGASUS pre-training method. The PEGASUS
self-supervised pre-training method is designed for abstractive summarization which transfer well for our paraphrase
generation task. In PEGASUS, important sentences are removed/masked from an input document and the model is
tasked with recovering and outputting them in a sequence, similar to an extractive summary Zhang et al. [2020]. In the
fashion of transformer model for text generation, it’s been pre-trained on a large corpus of web-crawled documents.
For our experiments, we used a version of PEGASUS that had been ﬁne-tuned for paraphrase generation3. The dataset
used for ﬁne-tuning was ﬁltered out from multiple paraphrasing datasets like Google’s PAWS dataset Yang et al. [2019].
When using this system on a dataset in French (HOUSE), we added a translation step using the Marian NMT framework.
In this case, we use the top translation (as scored by the model) for each sample in the dataset as the input for the
paraphrasing model. Once again, we take the 6-best (top scored by the model) generated paraphrases for each sample
and translate them back to the source language.
Once the paraphrases have been generated using either approach (NMT- or LM-based), the next step in the process
is the detection and removal of duplicates. The goal of this step is to remove repetitive formulations. To perform
this de-duplication, the original samples and the generated paraphrases are passed through a normalizer that performs
case-folding (to lower-case) and de-punctuation. Afterwards, any paraphrase that matches any of the original samples
associated to a given intent is deleted. The outcome of this step is a set of unique samples and paraphrases associated to
each intent. This outcome constitutes the augmented training dataset.
3https://huggingface.co/tuner007/pegasus_paraphrase
4

--- PAGE 5 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
As concrete examples, consider the sample "I must apply for a new credit card" , which was translated by the NMT to
"Ich muss eine neue Kreditkarte beantragen" and"Je dois demander une nouvelle carte de crédit" using German and
French as pivot languages respectively. Below we report three paraphrases generated from the 6-best set:
With German as a pivot language, "I need to apply for a new credit card" ,"I have to request a new credit card" , and " I
need to request a new credit card" ;
With French as a pivot language: "I need to ask for a new credit card" ,"I need to request a new credit card" , and "I
need to apply for a new credit card" .
As can be seen, there are some language-speciﬁc subtleties between the paraphrases, such as the use of "ask" from
the French pivot, which is not present in the former set wherein German was used for pivoting. As for the PEGASUS-
generated paraphrases, the following ones were generated for the same example: "I need a new credit card" ,"I need to
apply for new credit card" , and "I have to apply t for a new credit card." and, interestingly, "I need a new credit card
because I’m looking for a job" which depicts the drift that natural language generation can sometimes introduce. It can
be useful to introduce this additional information in the training data as long as it is something a user could say for this
speciﬁc intent and it is not completely changing the meaning of the sentence.
4.3 Baselines and Augmented Datasets
The HOUSE dataset. The core training set HOUSE-train constitutes the baseline for augmentation in this dataset. To
evaluate our augmentation approach using NMT- and LM-based approaches, we generated the following datasets using
the steps described in Sub-sections 4.1 and 4.2:
•HOUSE-paraph (NMT-en): paraphrased with English as the pivot language to generate paraphrases in French
for each intent.
•HOUSE-paraph (NMT-de): paraphrased with German as the pivot language to generate paraphrases in French
for each intent.
•HOUSE-paraph (NMT-es): paraphrased with Spanish as the pivot language to generate paraphrases in French
for each intent.
•HOUSE-paraph (LM): augmented using PEGASUS (translation back and forth provided by the Marian NMT
framework).
The HOUSE-train baseline and all the augmented datasets are in French.
The SCOPE dataset. The core training sets are SCOPE-train-5, SCOPE-train-10, and SCOPE-train-50, each one
constitutes a baseline for the augmentation task because they have not been artiﬁcially augmented. We generated the
following augmented datasets as described in Sub-sections 4.1 and 4.2:
•SCOPE-paraph-5 (NMT-fr): 5 samples per intent version paraphrased with French as the pivot language to
generate paraphrases in English.
•SCOPE-paraph-5 (NMT-de): 5 samples per intent version paraphrased with German as the pivot language to
generate paraphrases in English.
• SCOPE-paraph-5 (LM): 5 samples-per-intent version augmented using PEGASUS.
•SCOPE-paraph-10 (NMT-fr): 10 samples per intents paraphrased with French as the pivot language to generate
paraphrases in English.
•SCOPE-paraph-10 (NMT-de): 10 samples per intents paraphrased with German as the pivot language to
generate paraphrases in English.
• SCOPE-paraph-10 (LM): 10 samples per intents augmented using PEGASUS.
•SCOPE-paraph-50 (NMT-fr): 50 samples per intents paraphrased with French as the pivot language to generate
paraphrases in English.
•SCOPE-paraph-50 (NMT-de): 50 samples per intents paraphrased with German as the pivot language to
generate paraphrases in English.
• SCOPE-paraph-50 (LM): 50 samples per intents augmented using PEGASUS.
The baselines SCOPE-train-5, SCOPE-train-10, and SCOPE-train-50 as well as all the augmented datasets are in
English.
5

--- PAGE 6 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
DatasetNumber
of intentsTotal
samplesMin samples
per intentMax samples
per intentAvg samples
per intent
HOUSE-train 20 73 3 6 3.6
HOUSE-paraph (NMT-en) 20 453 17 42 22.6
HOUSE-paraph (NMT-de) 20 463 16 41 23.15
HOUSE-paraph (NMT-es) 20 458 16 41 22.9
HOUSE-paraph (LM) 20 351 13 30 17.5
HOUSE-test 20 77 1 11 3.8
SCOPE-train-5 150 750 5 5 5.0
SCOPE-paraph-5 (NMT-de) 150 3,840 13 34 25.3
SCOPE-paraph-5 (NMT-fr) 150 3,828 15 33 25.52
SCOPE-paraph-5 (LM) 150 4,500 30 30 30.0
SCOPE-train-10 150 1,500 10 10 10.0
SCOPE-paraph-10 (NMT-de) 150 7,663 33 65 51.1
SCOPE-paraph-10 (NMT-fr) 150 7,601 39 64 50.7
SCOPE-paraph-10 (LM) 150 9,000 60 60 60.0
SCOPE-train-50 150 7,500 50 50 50.0
SCOPE-paraph-50 (NMT-de) 150 37,754 183 314 251.7
SCOPE-paraph-50 (NMT-fr) 150 37,387 206 298 249.2
SCOPE-paraph-50 (LM) 150 45,000 300 300 300.0
SCOPE-test 150 4,500 30 30 30.0
Table 2: Datasets used in our experiments. HOUSE and SCOPE, respectively, represent our in-house corporate dataset
and the in-scope public dataset. NMT-DE represent the machine translation paraphrasing approach with German as the
pivot language. FR, EN, and ES represent French, English, and Spanish, respectively. LM represents the paraphrases
obtained from using the ﬁne-tuned PEGASUS language model.
In Table 2, we lay out the HOUSE and SCOPE core and augmented datasets, the paraphrasing volumes for each of the
partitions using the NMT and the PEGASUS-based approaches.
4.4 Dialog System Setup
To evaluate the potential impact of our augmented datasets on the performance of dialog systems, we created dialog
systems using the Rasa framework Bocklisch et al. [2017] and trained the NLU models with the core training datasets.
The Rasa framework makes use of the DIET Classiﬁer for intent classiﬁcation Bunk et al. [2020]. As for the parametric
conﬁguration of the framework, we used a Bag-of-Words encoder to obtain sparse representations at both word and
character levels. These representations will be used as input by the classiﬁer who will then train it’s own dense
embeddings. All NLU models were trained for 200 epochs. For each of our baselines (core HOUSE and the three core
SCOPEs), we trained a Rasa-based dialog system using said conﬁguration. This yields one HOUSE baseline system
and three SCOPE baseline systems (SCOPE-5, -10, and -50). Similarly, we trained a dialog system with each of the
augmented datasets. Again, we must emphasize the fact that we are not trying to reach state-of-the-arts results on our
benchmark datasets, but rather demonstrate that artiﬁcial data augmentation through paraphrase generation can have
enough impact on performance in the early stages of a dialog system to speed-up and reduce the cost of the launch into
production.
5 Evaluation
We perform a comparative evaluation between the baselines, i.e. the core un-augmented datasets (HOUSE-train,
SCOPE-train-5, SCOPE-train-10, and SCOPE-train-50), and the augmented datasets (those inﬁxed with "paraph" in
Table 2).
To evaluate the performance of a dialog system, the effectiveness of intent classiﬁcation must be measured. For that
purpose, we use macro-averaged precision, recall, and F1-score. We additionally report the micro-averaged score
(micro-averaged F1, precision, and recall, which are coequal since each sample is assigned to a single class). In our
6

--- PAGE 7 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
Systems Micro Score (%) Macro F1 (%) Macro Precision (%) Macro Recall (%)
SCOPE-train-5 (baseline) 54.7 52.9 54.9 54.7
SCOPE-paraph-5 NMT-de 62.0 59.8 61.3 62.0
SCOPE-paraph-5 NMT-fr 59.6 58.3 61.0 59.6
SCOPE-paraph-5 LM 64.0 62.7 65.4 64.0
SCOPE-train-10 (baseline) 69.2 68.1 69.5 69.2
SCOPE-paraph-10 NMT-de 73.2 72.4 73.7 73.2
SCOPE-paraph-10 NMT-fr 73.6 73.0 75.3 73.6
SCOPE-paraph-10 LM 74.1 73.1 74.6 74.1
SCOPE-train-50 (baseline) 83.8 83.4 85.1 83.8
SCOPE-paraph-50 (NMT-de) 85.0 82.9 86.3 85.0
SCOPE-paraph-50 (NMT-fr) 87.1 86.9 88.0 87.1
SCOPE-paraph-50 (LM) 87.4 87.2 88.0 87.4
HOUSE-train (baseline) 79.2 78.3 82.3 79.2
HOUSE-paraph (NMT-de) 84.3 81.6 81.6 83.9
HOUSE-paraph (NMT-en) 84.4 85.3 85.5 83.4
HOUSE-paraph (NMT-es) 83.1 82.8 83.8 81.9
HOUSE-paraph (LM) 84.4 85.5 86.7 83.5
Table 3: Intent classiﬁcation results for SCOPE and HOUSE augmented using the various approaches.
context, precision reﬂects the ability of the classiﬁer to correctly predict a given intent out of the predictions made,
which, in turn translates to the ability of the dialog system to respond with the correct answer. A high recall reﬂects
the ability of the intent classiﬁer to ideally predict (realistically, as many as possible) the correct intent for all the
test examples associated with that intent. The quality of the classiﬁcation hence depends on that of the paraphrasing
component, whether it be NMT- or LM-based.
While most of the artiﬁcial data seem to be good paraphrase and even some of the intents were paraphrased perfectly,
some paraphrases were spurious, unsurprisingly. The noise is often reﬂected as paraphrases suffering a semantic drift
from the original intent or loss of some details or preciseness. For instance, with PEGASUS, the sample "commander
des devises" in French, which translates into "ordering currencies" - a quite prevalent ﬁnancial transaction, had drifted
into "I want the dollars", losing its precision, scope and banking-like undertone. A noisy example with NMT is
"Spécimen de chèque" which means "sample (void) check" , here the word chèque can be translated into its other
meanings to validate orto verify .
Such noise is undesirable yet unavoidable in an automated augmentation process. To quantify the presence of this
noise, we asked human expert annotators to determine whether the generated paraphrases for the HOUSE-paraph (LM)
dataset were correct or not. Two annotators with bilingual proﬁciency in English and French performed the evaluation
separately. They found that 82% of the paraphrases were correct and reﬂected the meaning of the original intent while
using different syntax and vocabulary.
6 Results and Discussion
In this Section, we present the evaluation results for both augmentation approaches on the HOUSE and SCOPE datasets.
Table 3 reports the overall results in comparison with those obtained with the un-augmented baselines. The ﬁrst
observation we make is that the F1-score for the intent classiﬁcation model has increased by up to 7.8% (absolute
difference) and for all augmented datasets compared to their baselines when evaluated with the test set (HOUSE-test
and SCOPE-test). Such an increase is observed even for SCOPE-train-50, which carried a noticeably large number of
samples per intent pre-augmentation. The performance of the various paraphrasing systems themselves seems to be
quite close to one another, though the LM-based system yields the best F1 score for most datasets. Furthermore, we
point out that datasets with fewer training examples per intent seemed to beneﬁt more from paraphrasing than the larger
datasets, which already possess more generalization capabilities due to the higher volumes of data. This conﬁrms the
hypothesis that paraphrasing is more useful in the early stages of dialog system development process, when very few
training samples per intent are available.
7

--- PAGE 8 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
Table 3 shows the scores obtained with the HOUSE dataset, which is the smaller dataset with only 20 intents and
very few samples. Again, we can see that paraphrasing always results in a higher micro score. This shows that either
paraphrasing systems can be used with success on dialog systems in a language other than English. When comparing
the individual systems, the performance levels are again quite close but the LM-based system still outperforms the
NMT-based ones by a small margin, despite the added translation step needed for French. However, it is important to
mention that the NMT-based approach use lighter models than the LM-based one, which could make it a viable choice
for groups or organizations with limited resources.
7 Conclusions and Future Work
This work explored the beneﬁt of paraphrasing as a data augmentation solution when creating NLU models for dialog
systems in a real industrial context. We experimented with two paraphrasing approaches: one is NMT-based, and the
other leverages large pre-trained language models for text generation tasks. We used these paraphrasing methods to
augment two datasets: ﬁrstly, a publicly available dataset in English, with a large number of intents and partitions of
varying sizes; secondly, a corporate dataset in French, with a smaller number of intents. In the light of our experiments,
we found that the beneﬁt of paraphrasing is undeniable, as the models trained on the augmented sets, across all
datasets and paraphrasing systems, outperformed the baseline models (trained with un-augmented data) for the intent
classiﬁcation task. This data augmentation method can be used effectively by organizations to create new dialog
systems, without having to deal with the huge costs and resource needs associated with manual data generation.
We summarize our ﬁndings as follows:
1.Our proposed augmentation approaches outperformed the un-augmented baselines on all datasets for both
French and English. This shows that the quality and diversity of the paraphrases can improve the NLU model
generalization capabilities.
2.Both NMT- and PEGASUS-based approaches improved the performance. Using PEGASUS slightly outper-
forms NMT as an augmentation component, despite the added translation step at the beginning and at the end
of the pipeline in the case of the French dataset.
3.NMT provides a smaller semantic variety than does PEGASUS, but is also safer than PEGASUS as the latter
might stray away from the original intent.
4.In NMT-based approaches, no clear difference between the various pivot languages could be observed. French
and English, however, seem to pair well as source-pivot languages in both directions.
5.The more intents there are, the more training samples are needed, which comes as no surprise. However,
we have seen in our experiments that 10 original samples per intent and their paraphrase augmentations can
be enough to reach a performance that allows a satisfying user experience, hence start interacting with real
end-users.
We therefore recommend that ﬁve to 10 initial samples per intent be handcrafted and phrased diversely to convey various
aspects of the intent, then augmentation is to be conducted using one of our proposed approaches.
We also show that our augmentation approaches are low-cost and signiﬁcantly faster than building a substantial training
set manually. As for future work, we would like to experiment with our augmentation methods on more mature dialog
systems with more samples per intent, based on actual user interactions, as new intents emerge and existing ones
evolve. Also, user logs often bring a different type of noise into the data, such as misspelling and specialized terms.
It would thus be interesting to explore the impact of such noise. Then, it could be interesting to test ways to ﬁlter
out bad paraphrases during the augmentation pipeline. This could be done, for example, by tuning a threshold on the
model conﬁdence score instead of extracting the top n paraphrases. Finally, it would be interesting to attempt a fusion
approach, that is to combine unique paraphrases originating from different and more diverse pivots (e.g. Chinese and
French) for higher paraphrasing diversity attributed to language subtleties, and possibly to combine paraphrases from
both NM- and LM-based approaches into the training set.
8

--- PAGE 9 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
References
Jatin Ganhotra, Siva Sankalp Patel, and Kshitij Fadnis. Knowledge-incorporating esim models for response selection in
retrieval-based dialog systems. arXiv preprint arXiv:1907.05792 , 2019.
Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. Cross-lingual transfer learning for multilingual task
oriented dialog. arXiv preprint arXiv:1810.13327 , 2018.
Jonathan Mallinson, Rico Sennrich, and Mirella Lapata. Paraphrasing revisited with neural machine translation. In
Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics , pages
881–893, 2017. URL https://www.aclweb.org/anthology/E17-1083 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural information processing systems (NIPS 2017) , pages
5998–6008, 2017.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics: Human Language Technologies , pages 4171–4186, 2019.
doi:10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint
arXiv:1910.10683 , 2019.
Jianfeng Gao, Michel Galley, and Lihong Li. Neural approaches to conversational AI: Question answering, task-oriented
dialogues and social chatbots . Now Foundations and Trends, 2019.
Yun-Nung Chen, Asli Celikyilmaz, and Dilek Hakkani-Tür. Deep learning for dialogue systems. In Proceedings of the
55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts , pages 8–14, 2017a. URL
https://www.aclweb.org/anthology/P17-5004 .
Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. A survey on dialogue systems: Recent advances and new
frontiers. ACM SIGKDD Explorations Newsletter , 19(2):25–35, 2017b.
Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. The ubuntu dialogue corpus: A large dataset for research in
unstructured multi-turn dialogue systems. arXiv preprint arXiv:1506.08909 , 2015.
Yiping Kang, Yunqi Zhang, Jonathan K Kummerfeld, Lingjia Tang, and Jason Mars. Data collection for dialogue system:
A startup perspective. In Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies , pages 33–40, 2018.
Stefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K Kummerfeld,
Kevin Leach, Michael A Laurenzano, Lingjia Tang, et al. An evaluation dataset for intent classiﬁcation and out-of-
scope prediction. arXiv preprint arXiv:1909.02027 , 2019.
Jeffrey Dalton, Chenyan Xiong, Vaibhav Kumar, and Jamie Callan. Cast-19: A dataset for conversational information
seeking. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval , pages 1985–1988, 2020.
Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle Pineau. A survey of available corpora for
building data-driven dialogue systems. arXiv preprint arXiv:1512.05742 , 2015.
Praveen Badimala, Chinmaya Mishra, Reddy Kumar Modam Venkataramana, Syed Bukhari, and Andreas Dengel.
A study of various text augmentation techniques for relation classiﬁcation in free text. In Proceedings of the 8th
International Conference on Pattern Recognition Applications and Methods (ICPRAM 2019), pages 360-367 , pages
360–367, 02 2019. doi:10.5220/0007311003600367.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural
networks. Advances in neural information processing systems , 25:1097–1105, 2012.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the 2009 IEEE conference on computer vision and pattern recognition , pages 248–255,
2009.
William Yang Wang and Diyi Yang. That’s so annoying!!!: A lexical and frame-semantic embedding based data
augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets. In Proceed-
ings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 2557–2563, 2015.
doi:10.18653/v1/D15-1306. URL https://www.aclweb.org/anthology/D15-1306 .
9

--- PAGE 10 ---
Quick Starting Dialog Systems with Paraphrase Generation A P REPRINT
Maria Papadaki. Data augmentation techniques for legal text analytics. Department of Computer Science, Athens
University of Economics and Business, Athens , 2017.
Kathleen R. McKeown. Paraphrasing using given and new information in a question-answer system. In Pro-
ceedings of the 17th Annual Meeting of the Association for Computational Linguistics , pages 67–72, 1979.
doi:10.3115/982163.982182. URL https://www.aclweb.org/anthology/P79-1016 .
Marie Meteer and Varda Shaked. Strategies for effective paraphrasing. In Proceedings of the 12th Conference on
Computational Linguistics , COLING ’88, page 431–436, 1988. ISBN 963 8431 56 3. doi:10.3115/991719.991724.
URL https://doi.org/10.3115/991719.991724 .
Colin Bannard and Chris Callison-Burch. Paraphrasing with bilingual parallel corpora. In Proceedings of
the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05) , pages 597–604, 2005.
doi:10.3115/1219840.1219914. URL https://www.aclweb.org/anthology/P05-1074 .
Alex Sokolov and Denis Filimonov. Neural machine translation for paraphrase generation. arXiv preprint
arXiv:2006.14223 , 2020.
Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld, Tom Neckermann,
Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, et al. Marian: Fast neural machine translation
in C++. arXiv preprint arXiv:1804.00344 , 2018.
Sam Witteveen and Martin Andrews. Paraphrasing with large language models. In Proceedings of the 3rd
Workshop on Neural Generation and Translation , pages 215–220, 2019. doi:10.18653/v1/D19-5623. URL
https://aclanthology.org/D19-5623 .
Alberto Barrón-Cedeño, Marta Vila, M Antònia Martí, and Paolo Rosso. Plagiarism meets paraphrasing: Insights for
the next generation in automatic plagiarism detection. Computational Linguistics , 39(4):917–947, 2013.
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for
abstractive summarization. In International Conference on Machine Learning , pages 11328–11339, 2020.
Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer
for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing: System Demonstrations , pages 66–71, 2018. doi:10.18653/v1/D18-2012. URL https://www.aclweb.
org/anthology/D18-2012 .
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase
Identiﬁcation. In Proceegings of the 2019 Conference on Empirical Methods in Natural Language Processing , page
3687–3692, 2019.
Tom Bocklisch, Joey Faulkner, Nick Pawlowski, and Alan Nichol. Rasa: Open source language understanding and
dialogue management. arXiv preprint arXiv:1712.05181 , 2017.
Tanja Bunk, Daksh Varshneya, Vladimir Vlasov, and Alan Nichol. Diet: Lightweight language understanding for
dialogue systems. arXiv preprint arXiv:2004.09936 , 2020.
10
