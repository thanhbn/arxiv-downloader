# Block-State Transformers
Mahan Fathi123∗Jonathan Pilault124∗
Orhan Firat1Christopher Pal24Pierre-Luc Bacon23Ross Goroshin1
1Google DeepMind2Mila3Université de Montréal4Polytechnique Montréal

## Tóm tắt
Các mô hình không gian trạng thái (SSMs) đã cho thấy kết quả ấn tượng trên các tác vụ đòi hỏi mô hình hóa các phụ thuộc tầm xa và mở rộng hiệu quả cho các chuỗi dài nhờ vào độ phức tạp thời gian chạy dưới bậc hai. Ban đầu được thiết kế cho tín hiệu liên tục, SSMs đã cho thấy hiệu suất vượt trội trên nhiều tác vụ, trong thị giác và âm thanh; tuy nhiên, SSMs vẫn kém hơn hiệu suất Transformer trong các tác vụ Mô hình hóa Ngôn ngữ. Trong công trình này, chúng tôi đề xuất một lớp lai có tên Block-State Transformer (BST), nội bộ kết hợp một lớp con SSM cho việc ngữ cảnh hóa tầm xa, và một lớp con Block Transformer cho biểu diễn ngắn hạn của các chuỗi. Chúng tôi nghiên cứu ba biến thể khác nhau và hoàn toàn có thể song song hóa, tích hợp SSMs và attention theo khối. Chúng tôi cho thấy mô hình của chúng tôi vượt trội hơn các kiến trúc dựa trên Transformer tương tự về độ phức tạp mô hình hóa ngôn ngữ và tổng quát hóa cho các chuỗi dài hơn. Ngoài ra, Block-State Transformer thể hiện tốc độ tăng hơn mười lần ở cấp độ lớp so với Block-Recurrent Transformer khi áp dụng song song hóa mô hình.

## 1 Giới thiệu
Transformers đã cho thấy hiệu suất ấn tượng trên một loạt rộng các tác vụ xử lý ngôn ngữ tự nhiên (NLP). Mặc dù chúng chủ yếu được sử dụng cho mô hình hóa ngôn ngữ, kiến trúc Transformer [40] cũng đã được áp dụng thành công cho các tác vụ khác ngoài NLP và phần lớn đã thay thế Mạng Nơ-ron Hồi quy (RNNs). Một số yếu tố góp phần vào thành công này, bao gồm hiệu quả tính toán và các bias quy nạp kiến trúc phù hợp với việc huấn luyện trên các tác vụ ngôn ngữ tự nhiên ở quy mô lớn. Về mặt tính toán, Transformers có thể xử lý các token của một chuỗi đầu vào nhất định song song, tận dụng tối đa phần cứng gia tốc hiện đại. Hơn nữa, cơ chế attention cho phép Transformers tìm ra các mối quan hệ trong các chuỗi dài hơn bằng cách cung cấp quyền truy cập sẵn sàng vào tất cả thông tin được trích xuất từ các token trước đó khi suy luận token tiếp theo. So với RNNs và LSTMs [19], lợi ích của self-attention là hai mặt: (i) khả năng lưu trữ và truy cập trực tiếp như ngữ cảnh được tăng mạnh, và (ii) huấn luyện trên các chuỗi dài hơn ổn định hơn [18, 23].

Xét đến những thành tựu đáng kể của Transformers trong các tác vụ mô hình hóa ngôn ngữ, và hiệu suất cải thiện của chúng ở quy mô lớn trên các tác vụ NLP khó như lý luận và trả lời câu hỏi [2,39,6], nhu cầu triển khai các mạng thậm chí sâu hơn và lớn hơn lớn hơn bao giờ hết. Một chiều mở rộng trực giao, có thể thậm chí còn quan trọng hơn, là kích thước của chuỗi đầu vào. Mặc dù có một số ưu điểm của Transformers so với RNNs, việc mở rộng độ dài chuỗi đầu vào vẫn là vấn đề, một lần nữa vì cả lý do hiệu suất tính toán và chất lượng. Hơn nữa, thời gian chạy của Transformer là bậc hai đối với độ dài chuỗi đầu vào, điều này khiến việc huấn luyện các mô hình này ngày càng đắt đỏ. Hơn nữa, Transformers với attention, có thể là local [8], sparse [4,43,36], xấp xỉ low-rank [41] hoặc tuyến tính hóa qua các phương pháp kernel [5,22], nổi tiếng gặp khó khăn trên các tác vụ phân loại đầu vào dài [37]. Vanilla transformers có thể không ổn định khi được huấn luyện trên các chuỗi dài [26] và tầm quan trọng của token tập trung trong một trường tiếp nhận cục bộ khoảng 50 token xung quanh bước thời gian hiện tại [35].

Một nhóm nghiên cứu mới nổi gợi ý rằng Mô hình Không gian Trạng thái (SSMs) có thể phục vụ như một thay thế cho Transformers vì chúng có thể nắm bắt các phụ thuộc trong các chuỗi cực dài, trong khi hiệu quả tính toán hơn và có thể song song hóa [14]. Mặc dù vẫn thuộc danh mục các mô hình chuỗi tự hồi quy, hệ thống động tuyến tính bất biến thời gian cơ bản của SSMs cho phép xử lý hiệu quả các chuỗi sử dụng các toán tử tích chập song song hóa được với Biến đổi Fourier Nhanh (FFT) [7], với độ phức tạp O(LlogL), trong đó L là độ dài của chuỗi. Hơn nữa, việc giữ lại thông tin quá khứ trên các chuỗi dài, lên đến hàng nghìn bước, có thể được đảm bảo bằng cách dẫn xuất các quy tắc cập nhật hồi quy bằng cách mượn ý tưởng từ xấp xỉ hàm trực tuyến [3,12]. SSMs gần đây đã vượt trội hơn Transformers trên các benchmark phụ thuộc tầm xa với biên độ lớn [37]. Mặc dù thành công trên các tác vụ phân loại tầm xa, SSMs vẫn chưa hoàn toàn phù hợp với Transformers như một mô hình chuỗi sẵn sàng cho các tác vụ mô hình hóa ngôn ngữ tổng quát [10]. Các phát hiện gần đây cho thấy Transformers và SSMs là các mô hình bổ sung cho mục đích mô hình hóa ngôn ngữ [28]. Trong công trình này, chúng tôi đề xuất một kiến trúc tích hợp bias quy nạp attention-based địa phương mạnh với khả năng mô hình hóa ngữ cảnh dài hạn của SSMs vào một lớp duy nhất, mà chúng tôi gọi là Block-State Transformer (BST). Mô hình của chúng tôi có thể xử lý các chuỗi đầu vào dài, trong khi vẫn tích hợp cơ chế attention để dự đoán các token tiếp theo. BST hoàn toàn có thể song song hóa, mở rộng cho các chuỗi dài hơn nhiều, và cung cấp tăng tốc 10× so với các lớp dựa trên Transformer tương đương.

Trong mỗi lớp BST, một SSM nhận toàn bộ chuỗi làm đầu vào và ánh xạ nó thành một chuỗi "ngữ cảnh" có cùng độ dài. Lớp con SSM tận dụng các tích chập dựa trên FFT. Chuỗi ngữ cảnh này sau đó được chia thành các khối có kích thước bằng nhau, tức là độ dài cửa sổ (W), và mỗi khối ngữ cảnh sau đó được đưa vào lớp Block Transformer, attention đến các chuỗi con có kích thước W như được định nghĩa trong [21]. Khối embeddings token đầu vào sau đó được cross-attend đến khối tương ứng của các trạng thái ngữ cảnh; xem Hình 1. Lưu ý rằng bằng cách giới thiệu SSMs như một phương tiện ngữ cảnh hóa, chúng tôi hoàn toàn loại bỏ nhu cầu cho các hồi quy tuần tự và chúng tôi có thể chạy lớp lai SSM-Transformer hoàn toàn song song. Độ phức tạp thời gian chạy kết quả có thể được biểu diễn như tổng của O(W2) + O(LlogL), trong đó số hạng đầu tiên biểu diễn độ phức tạp thời gian của lớp con Transformer, trong khi số hạng thứ hai biểu diễn độ phức tạp thời gian của lớp con SSM. Đây là một cải thiện lớn so với O(LW) của Block-Recurrent Transformer, miễn là có sẵn phần cứng để hỗ trợ tính toán song song. Hơn nữa, do các hạn chế được áp đặt bởi phần cứng, độ phức tạp thời gian chạy của SSM trên một chuỗi đầy đủ có thể so sánh với của Block Transformer trên một khối token, điều này tiếp tục ngụ ý sự vắng mặt của một nút thắt cổ chai tốc độ trong lớp BST, được xác nhận thực nghiệm cho các chuỗi chứa hàng trăm nghìn token. Điều này rõ ràng bằng cách quan sát rằng hai dòng dưới cùng ở bên trái của Hình 4 gần như chồng lên nhau.

## 2 Công trình liên quan
Công trình này chủ yếu liên quan đến hai nhánh nghiên cứu gần đây: (i) kết hợp attention cục bộ với mạng hồi quy để mở rộng khả năng nắm bắt các phụ thuộc tầm xa, vượt quá độ dài của kích thước cửa sổ attention, và (ii) Mô hình Không gian Trạng thái (SSMs) mô tả các chuỗi qua hệ thống động tuyến tính có đầu ra có thể được tính toán song song. Block-Recurrent Transformer (BRECT) [21] sử dụng cơ chế bộ nhớ hồi quy để mở rộng độ dài ngữ cảnh lý thuyết của Transformer. Trong đơn vị hồi quy của cell BRECT, các cập nhật được thực hiện cho "vector trạng thái hồi quy," được trích xuất bằng cách sử dụng cơ chế cross-attention trên một khối/cửa sổ của embeddings token đầu vào. Khác với công trình của họ, chúng tôi sử dụng các mô hình không gian trạng thái tuyến tính thay vì các cell hồi quy để duy trì các trạng thái ngữ cảnh. Chúng tôi cũng tiến hành một khám phá rộng rãi hơn về việc duy trì và cập nhật các trạng thái ngữ cảnh. Các công trình trước đó tăng cường transformers với bộ nhớ ngoại vi không khả vi bao gồm Memorizing Transformer [42]. Transformer-XL [8] là công trình sớm kết hợp bộ nhớ hồi quy với Transformers. Công trình của chúng tôi có thể được coi như một sự tiến hóa tiếp tục của những mô hình đó tích hợp các mô hình bộ nhớ hồi quy tiên tiến được lấy cảm hứng từ SSMs.

Các mô hình không gian trạng thái có thể được coi như RNNs tuyến tính [12]. Sự đơn giản này tạo điều kiện cho việc phân tích chúng và thậm chí cho phép dẫn xuất phân tích các trọng số hồi quy để biểu diễn tối ưu các chuỗi dài tùy ý. Tính chất tuyến tính cũng cho phép hồi quy được mở ra và song song hóa trong quá trình huấn luyện và suy luận [14]. Công trình của chúng tôi kết hợp những mô hình tiên tiến này, cho phép Transformers tận dụng ngữ cảnh vô hạn về mặt lý thuyết.

Các công trình khác đã cố gắng thay thế Transformers, và cơ chế attention của chúng bằng SSMs [28,27,10,30], tuy nhiên mặc dù có tiến bộ gần đây, hiệu suất đạt được bởi kiến trúc Transformer vẫn vô song trong ngôn ngữ. Tuy nhiên, SSMs có thể nắm bắt các phụ thuộc tầm xa hơn Transformers cả về lý thuyết và thực hành, đồng thời cũng rất có thể song song hóa [7,11]. Do đó chúng tôi chọn kết hợp các khía cạnh tốt nhất của SSMs và Transformers vào một mô hình duy nhất.

Ý tưởng giao tiếp qua các khối, tương tự như GSS [28], sau đó được triển khai bởi MEGA [27], thông qua quy tắc cập nhật Trung bình Di chuyển Theo cấp số nhân (EMA) thay vì SSMs². Tuy nhiên, cả GSS và MEGA đều sử dụng Đơn vị Attention Có cổng (GAU) một head duy nhất [20]. MEGA tiếp tục trộn đầu vào lớp, đầu ra GAU và đầu ra EMA qua hai cơ chế gating. Phương pháp của chúng tôi sử dụng kiến trúc đơn giản hơn để trộn tín hiệu từ attention cục bộ và đầu ra SSM qua cross-attention, cho phép chúng tôi cắm bất kỳ SSMs hoặc lớp attention sẵn có nào. Hơn nữa, chúng tôi điều tra ba cách để trộn tín hiệu SSM với attention như được nêu trong Phần 3.3.

## 3 Phương pháp
Chúng tôi xem xét vấn đề dự đoán token tiếp theo qua mô hình ngôn ngữ chỉ giải mã. Tác vụ tiền tố có vẻ đơn giản này đã dẫn đến tiến bộ ngoạn mục trong hiểu biết ngôn ngữ [9,2,29]. Trong quá trình huấn luyện, bộ giải mã nhận một chuỗi độ dài L của embeddings token và được giao nhiệm vụ tạo ra token tiếp theo ở mỗi bước trong chuỗi.

Chúng tôi bắt đầu bằng một đánh giá ngắn gọn về SSMs cần thiết để hiểu lớp Block-State Transformer (3.1). Kiến trúc Block-State Transformer đầy đủ của chúng tôi được nêu trong Phần 3.2. Phần 3.3 mô tả ba cách tiếp cận để tích hợp các trạng thái SSM vào cơ chế attention. Các chi tiết triển khai quan trọng được mô tả trong Phần 3.4.

### 3.1 Kiến thức cơ bản về Không gian Trạng thái
Các mô hình không gian trạng thái có thể được chia thành hai loại:

**Không gian Trạng thái: Kernels Có cấu trúc** S4[14], S5[34], S4D[15], DSS[16], theo một khởi tạo có cấu trúc của kernel tích chập bằng cách mở ra một hệ thống động tuyến tính bất biến thời gian (LTI) có dạng sau:

xₖ = Axₖ₋₁ + Buₖ,
yₖ = Cxₖ + Duₖ.                    (1)

Hệ thống được tham số hóa bởi ma trận trạng thái A∈ℝᴺˣᴺ, vectors B∈ℝᴺˣ¹, C∈ℝ¹ˣᴺ, và D∈ℝ¹ˣ¹, SSM ánh xạ tín hiệu đầu vào 1-D uₖ, thành tín hiệu đầu ra 1-D yₖ. Nội bộ, SSM chiếu tín hiệu đầu vào lên biểu diễn trạng thái N-D xₖ, trước khi ánh xạ nó xuống scalar sử dụng ma trận C. Số hạng Duₖ có thể được coi như kết nối tắt và sẽ được bỏ qua cho phần còn lại của thảo luận để thuận tiện. Đầu ra của phương trình hồi quy trên, yₖ, có thể được tính như một tích chập rời rạc, bằng cách nhận ra rằng hồi quy có thể được mở ra một cách rõ ràng:

Đặt x₋₁ := 0⃗,
yₖ = Σⱼ₌₀ᵏ CAʲB · uₖ₋ⱼ.                (2)

Các mục CAᵏB được thu thập để tạo kernel SSM K∈ℝᴸ, và tích chập có thể được biểu diễn như:

K = (CB, CAB, ..., CAᴸ⁻¹B),
yₖ = Σⱼ₌₀ᵏ Kⱼ · uₖ₋ⱼ, y = K * u.        (3)

Cho một chuỗi đầu vào u∈ℝᴸ, có thể tính đầu ra y∈ℝᴸ tuần tự thông qua hồi quy trong Phương trình (1). Mặc dù tính chất này hữu ích cho giải mã tự hồi quy, tính toán tuần tự quá chậm để huấn luyện với đầu vào dài và, thay vào đó, tích chập từ Phương trình (3) có thể được sử dụng để tính tất cả các phần tử của y song song. Điều này được thực hiện qua Biến đổi Fourier Nhanh (FFT) [7], với điều kiện chúng ta đã tính K.

Các bias quy nạp bổ sung đã được áp đặt lên SSMs bằng cách dẫn xuất phân tích các biểu thức dạng đóng cho các ma trận A và B sử dụng framework HiPPO [12]. Trong framework này, trạng thái xₜ biểu diễn các hệ số của đa thức xấp xỉ chuỗi uₜ.

**Bộ lọc Được tham số hóa Rõ ràng** Trái ngược với kernels có cấu trúc, người ta có thể tham số hóa kernel tích chập, như trọng số có thể huấn luyện và tối ưu hóa chúng, K̄∈ℝᴸ. Tuy nhiên, điều này sẽ dẫn đến hiệu suất kém trừ khi áp dụng một số loại regularization cho kernel. [11] đơn giản sử dụng việc nén các trọng số kernel, và sau đó áp dụng kỹ thuật làm mượn. Kernels có thể huấn luyện cũng được sử dụng trong các mô hình thay thế attention-free cho Transformers, như Hyena [30], bao gồm việc giảm theo cấp số nhân các trọng số dọc theo kernel:

K̄ₜ = e⁻ᵅᵗ · FFN◦PositionalEncoding(t),    (4)

trong đó K̄ₜ là một mục trong bộ lọc tại vị trí t, và FFN là mạng feed-forward được sử dụng để tách rời số lượng tham số khỏi độ dài chuỗi.

### 3.2 Lớp Block-State Transformer (BST)
Bây giờ chúng tôi giới thiệu lớp Block-State Transformer, kết hợp SSMs với Block Transformers. Tại mỗi lần lặp huấn luyện, một chuỗi L token, được lấy mẫu từ một tài liệu dài hơn. Các token sau đó được nhúng và đưa vào mô hình. Mô hình của chúng tôi bao gồm một chồng các lớp Block-State Transformer. Mỗi lớp BST tùy chọn bao gồm một lớp con SSM chịu trách nhiệm cung cấp ngữ cảnh tầm xa cho lớp Block Transformer, hoạt động tương tự như cell Block-Recurrent Transformer (BRECT). Lớp con SSM nhận chuỗi embeddings token từ lớp trước làm đầu vào, và tạo ra một chuỗi có cùng độ dài L làm đầu ra.

Đầu ra của SSM được mã hóa ngữ cảnh, có nghĩa là các mục tại mỗi bước thời gian, có thể bao gồm thông tin về tất cả các bước thời gian đứng trước các phần tử trong chuỗi. Chúng tôi thu thập một số "trạng thái ngữ cảnh," S, từ chuỗi ngữ cảnh, và chúng tôi đặt S ≪ L. Để ngăn mô hình truy cập thông tin tương lai, chúng tôi chỉ cho phép mô hình truy cập các trạng thái ngữ cảnh đứng trước token hiện tại. Các cách khác nhau để thu thập trạng thái ngữ cảnh từ chuỗi ngữ cảnh được thảo luận chi tiết trong phần 3.3.

Các trạng thái ngữ cảnh được đưa vào Block Transformer, thay cho những gì được gọi là "vector trạng thái hồi quy" trong Block-Recurrent Transformer [21]. Các hoạt động tiếp theo, được hiển thị ở phía bên phải của Hình 1, được giữ nguyên, ngoại trừ chúng tôi không còn cần chạy đơn vị hồi quy của cell BRECT vì chúng tôi duy trì ngữ cảnh qua SSM. Ngoài các trạng thái ngữ cảnh, Block Transformer cũng nhận một khối/cửa sổ có độ dài W của embeddings token làm đầu vào, được cross-attend đến các trạng thái ngữ cảnh. Đầu ra của hoạt động cross-attention sau đó được nối với đầu ra của self-attention trên embeddings đầu vào, theo sau là một phép chiếu đơn giản.

Ngoài khả năng của SSMs giữ lại thông tin trên các chân trời thời gian dài hơn so với Transformers và RNNs, sử dụng SSM để duy trì các trạng thái ngữ cảnh như thay thế cho các cell hồi quy tạo ra một lớp hiệu quả tính toán hơn. Loại bỏ hồi quy bằng cách tích hợp SSMs vào các lớp Transformer, cho phép lớp Block-State Transformer hoàn toàn có thể song song hóa, trong khi kiến trúc Block-Recurrent xử lý các khối token tuần tự sử dụng vòng lặp for.

### 3.3 Trạng thái Ngữ cảnh
Mặc dù đầu ra SSM mới nhất về mặt kỹ thuật chứa thông tin về toàn bộ chuỗi, việc truy xuất các token riêng lẻ chỉ từ trạng thái cuối cùng có thể không khả thi. Để bù đắp, chúng tôi nối một chuỗi các trạng thái, tương ứng với khối token mới nhất. Điều này cũng tương tự với cách tiếp cận được BRECT áp dụng. Biểu diễn này đảm bảo khả năng truy xuất và dễ truy cập, thông qua redundancy. Nó redundant vì các trạng thái liền kề có tương quan cao, tuy nhiên điều này cũng khiến có thể dễ dàng khôi phục khối token hiện tại, nếu cần thiết.

Trong cách tiếp cận của chúng tôi, các trạng thái ngữ cảnh được xây dựng từ đầu ra của SSM và đưa vào các head attention của Transformer. Các trạng thái ngữ cảnh này có thể được xây dựng theo nhiều cách khác nhau. Để hướng dẫn các quyết định thiết kế này, chúng tôi xem xét mỗi scheme được đề xuất dưới đây như giới thiệu retrievability với chi phí của redundancy. Shape của đầu ra của một lớp SSM đơn là (B×L×D), trong đó B là kích thước batch, L là số token được xử lý, và D là chiều embedding. Khi thực hiện cross-attention trong cell Transformer với H head khác nhau, tensor này cần được biến đổi thành tensor ngữ cảnh có shape (B×S×D×H), trong đó S là số trạng thái ngữ cảnh; chúng tôi thường đặt S ≪ L và S = W tương tự như Block-Recurrent Transformers (BRECT).

Bây giờ chúng tôi thảo luận ba cách tiếp cận khác nhau mà chúng tôi đánh giá để tạo tensor ngữ cảnh cho mỗi chuỗi khối:

**SH: Single-Head** Cách tiếp cận đầu tiên xây dựng tensor ngữ cảnh bằng cách nối tuần tự S trạng thái từ SSM với một bộ lọc duy nhất (mỗi có kích thước D). Lưu ý rằng vì SSM nắm bắt thông tin từ các khối trước đó, trạng thái ngữ cảnh cũng nắm bắt thông tin về các khối đứng trước khối hiện tại. Vector ngữ cảnh kết quả có tính retrievable và redundant cao, như được định nghĩa ở trên. Như trong Transformers điển hình, các lớp kết nối đầy đủ được sử dụng để chiếu mỗi vector ngữ cảnh đến H head khác nhau có kích thước D. Lưu ý rằng trong hoạt động cross-attention, các trạng thái ngữ cảnh tương ứng với các token tương lai từ khối hiện tại cần được masked out causally. Trong trường hợp này chúng tôi đặt S = W, và chúng tôi chọn cửa sổ đầu ra SSM tương ứng với khối hiện tại, và mask tam giác được sử dụng để triển khai causal masking của các trạng thái ngữ cảnh. Cách tiếp cận này được hiển thị trong Hình 1.

**MH: Multi-Head** Cách tiếp cận này khác với Single-Head (SH) ở chỗ ở đây SSM được giao nhiệm vụ tạo ra đầu ra riêng biệt cho các head khác nhau. Chúng tôi sử dụng các ma trận [C₁, C₂, ..., Cₕ] riêng biệt, để tạo ra các trạng thái ngữ cảnh được đưa vào các head attention. Điều này cho phép SSM trích xuất các đặc trưng bổ sung từ lịch sử được tóm tắt. Sự khác biệt về khái niệm là ma trận C, từ Phương trình (1), có quyền truy cập trực tiếp vào trạng thái bộ nhớ đầy đủ của SSM (xₖ), về mặt lý thuyết có thể được coi như biểu diễn compact của lịch sử, trước khi nó được ánh xạ xuống scalar. Cách tiếp cận Multi-Head (MH) được minh họa ở phía bên trái của Hình 2. Vì H ma trận C khác nhau có thể trích xuất thông tin bổ sung, vector ngữ cảnh được xây dựng bằng phương pháp này về mặt lý thuyết ít redundant hơn so với phương pháp single-head được mô tả ở trên.

**MF: Multi-Filter** Trong cách tiếp cận này, lớp con SSM tạo ra S trạng thái ngữ cảnh, mà chúng tôi đặt độc lập với W. Điều này được thực hiện bằng cách tích chập chuỗi embeddings với S kernel/filter khác nhau. Đầu ra của mỗi hoạt động tích chập, tương ứng với một bộ lọc cụ thể, là tensor có shape (B×L×D). Sau khi tích chập đầu vào với tất cả các bộ lọc, các trạng thái ngữ cảnh có kích thước D tương ứng với token cuối cùng từ cửa sổ trước được xếp chồng lại với nhau để tạo tensor (B×S×D). Các mạng feed forward sau đó được sử dụng để nâng tensor này lên các head khác nhau, (B×S×D×H). Khác với hai cách tiếp cận trước, ngữ cảnh được hình thành bằng cách chỉ lấy S trạng thái ngữ cảnh cuối cùng, từ cửa sổ trước, được xuất ra bởi S SSMs. Ngữ cảnh ít redundant hơn vì nó không còn bao gồm các trạng thái SSM liền kề. Vì ngữ cảnh được lấy từ các mục của cửa sổ trước, cross-attention masking không còn cần thiết, như được hiển thị ở phía bên phải của Hình 2.

Các trạng thái bộ nhớ của cách tiếp cận Multi-Filter (MF) có redundancy thấp nhất, trong khi Multi-Head (MH) đạt được điểm giữa, và Single-Head (SH) có redundancy nhiều nhất. Việc tích hợp redundancy trong những cách tiếp cận này nhằm tạo điều kiện cho retrievability của ngữ cảnh gần đây nhất được nắm bắt bởi SSM, mặc dù với chi phí của việc sử dụng dung lượng mạng có thể không hiệu quả. Cách tiếp cận cuối cùng đạt được utilization cao nhất, vì cross-attention được thực hiện trong không gian của các đặc trưng duy nhất được trích xuất bởi các bộ lọc chuyên biệt.

### 3.4 Chi tiết Triển khai
**Context IDs & Positional Embedding** Để cho phép phân biệt giữa các mục được cung cấp cho cơ chế attention, một positional embedding thường được thêm vào đầu vào. Khi sử dụng cách tiếp cận Multi-Filter (MF), các trạng thái ngữ cảnh được thu thập tương ứng với các đặc trưng khác nhau được trích xuất từ chuỗi, do đó chúng tôi thêm một tập hợp "context IDs" được học duy nhất vào các trạng thái ngữ cảnh, trước khi sử dụng chúng làm đầu vào cho cross-attention. Tuy nhiên, trong các trường hợp các trạng thái ngữ cảnh tương ứng với các bước thời gian khác nhau dọc theo chuỗi, cụ thể là các cách tiếp cận Single-Head (SH) và Multi-Head (MH), positional encoding vốn có được tích hợp vào các trạng thái ngữ cảnh, do tính chất tăng dần của tích chập; do đó, chúng tôi thấy việc thêm context IDs là không cần thiết. Chúng tôi cũng nhận ra rằng chúng tôi không cần thêm global positional bias vào embeddings token, và sử dụng relative position bias kiểu T5 [32] thay thế, vì SSM cũng mã hóa thông tin vị trí vào ngữ cảnh.

**Down-sampling** Phù hợp với các phát hiện trong [28], chúng tôi thấy các hoạt động FFT là nguồn chính của nút thắt cổ chai khi huấn luyện SSMs trên TPUs. Chúng tôi chiếu embeddings đầu vào xuống không gian chiều thấp hơn, bằng một phần tư kích thước embedding trong thí nghiệm của chúng tôi, điều này giảm tổng số FFTs cần thiết đi một hệ số 4. Đầu ra của SSM, tức là các trạng thái ngữ cảnh, sau đó được nâng lên kích thước embedding ban đầu trước khi được truyền đến Block Transformer.

## 4 Kết quả
Kết quả của chúng tôi được trình bày trong Bảng 1. Chúng tôi tiến hành thí nghiệm với BST trên ba bộ dữ liệu khác nhau, PG19, arXiv và GitHub, cho phép chúng tôi kiểm tra phương pháp của chúng tôi trên một bộ tài liệu có độ dài khác nhau bao gồm văn bản tiếng Anh, các bài báo khoa học latex và mã nguồn.

Bộ dữ liệu **PG19** là từ bộ sưu tập lớn các cuốn sách toàn bộ từ Project Gutenberg [31]. Tất cả 28,602 cuốn sách được trích xuất đều được xuất bản trước năm 1919 và chứa 6,966,499 từ tiếng Anh. Khi được tokenize, mỗi cuốn sách PG19 có từ 50k-100k token. PG19 đã trở thành benchmark phổ biến để đo lường tiến bộ về hiệu suất mô hình hóa ngôn ngữ tầm xa. Chúng tôi báo cáo hiệu suất đánh giá split "test".

Bộ dữ liệu **arXiv** là corpus chứa các bài báo khoa học và kỹ thuật về chủ đề Toán học [42]. Bộ dữ liệu arXiv chứa mã nguồn latex cũng như các mục như định lý, trích dẫn, định nghĩa được tham chiếu và thảo luận trên các phạm vi dài của văn bản. Sử dụng cùng từ vựng như trong [42] và [21] để so sánh công bằng, nhiều ký tự đặc biệt được chia thành các subword nhỏ. Kết quả là, số token mỗi bài báo trong bộ dữ liệu arXiv xấp xỉ bằng số token mỗi cuốn sách trong PG19. Chúng tôi báo cáo perplexity trên split "test".

Bộ dữ liệu **GitHub** [42] là lớn nhất trong ba bộ dữ liệu và được lắp ráp bằng cách trích xuất các repository mã GitHub với giấy phép mã nguồn mở. Các tệp được lọc để chỉ chứa các ngôn ngữ lập trình sau: C, C++, Java, Python, Go và Typescript. Mặc dù các tệp mã tương đối nhỏ, có nhiều phụ thuộc import giữa mỗi tệp. Bằng cách duyệt cây thư mục và nối tất cả các tệp mã dọc theo đường dẫn, một tài liệu duy nhất bảo tồn cấu trúc và phụ thuộc của repository được tạo ra. Chúng tôi báo cáo hiệu suất trên split "validation".

Để so sánh công bằng với các baseline, chúng tôi giữ từ vựng nhất quán như được sử dụng bởi [21] và [28]. Cụ thể, chúng tôi sử dụng vocab T5 được pretrain với 32k token cho PG19 [33] và vocab LaMDA với 32k token [39] cho cả bộ dữ liệu arXiv và GitHub. Do thời gian huấn luyện dài và số lượng thí nghiệm lớn, chúng tôi chỉ cung cấp error bar cho các mô hình PG19 ∼200M tham số bằng cách chạy mô hình của chúng tôi với ba random seed khác nhau. Error bar BRECT:FIXED:SKIP từ [21].

### 4.1 So sánh Baselines và Mô hình của chúng tôi
Chúng tôi thí nghiệm ba loại mô hình Block-State Transformer (BST) khác nhau: BST-SH, BST-MH và BST-MF như được mô tả trong Phần 3.3. Các mô hình của chúng tôi không sử dụng global learned positional embeddings mà mã hóa nhận thức vị trí với SSM ở lớp đầu tiên, ngay sau lớp word embedding. Chúng tôi tổ chức các mô hình thành hai nhóm: (i) kích thước cửa sổ cố định có kích thước cửa sổ huấn luyện 512 hoặc 2048 token; và (ii) số lượng tham số cố định có tổng số tham số ∼200M hoặc ∼400M. Chúng tôi chạy thí nghiệm với hai loại SSMs:

**BST:{SH,MH,MF}:S4** mã hóa ngữ cảnh dài sử dụng Structured State Space Model (S4) [16]. Như được mô tả trong Phương trình (3), kernel matrix K của S4 được biên dịch từ các ma trận A, B và C và độc lập với độ dài của độ dài chuỗi đánh giá đầu vào. Chúng tôi cho thấy tham số hóa có cấu trúc của K cho phép các mô hình BST của chúng tôi tổng quát hóa cho độ dài dài hơn. Chúng tôi tham khảo độc giả đến phần 4.2 cho kết quả về tổng quát hóa độ dài. Chúng tôi chỉ chạy một BST:MH sử dụng S4 vì mô hình đòi hỏi 8% tham số nhiều hơn trong khi hiệu suất ngang bằng với biến thể BST:SH nhanh hơn. BST:MF cũng có 8% tham số nhiều hơn nhưng hoạt động tốt hơn trên arXiv và GitHub so với SH. Thú vị là, SH hoạt động tốt hơn MF trên PG19, bộ dữ liệu mà ngữ cảnh cục bộ quan trọng hơn để dự đoán token tiếp theo so với arXiv và GitHub. Chúng tôi đặt giả thuyết rằng điều này có thể do khả năng của mô hình SH trong việc truy xuất ngữ cảnh gần đây nhất được nắm bắt bởi SSM.

**BST:{SH,MF}:UNSTRUCT** dựa trên các bộ lọc tích chập được tham số hóa không có cấu trúc, lấy cảm hứng từ kernel tích chập Hyena Hierarchies [30]. Chúng tôi loại trừ việc sử dụng cơ chế gating nhân được sử dụng trong Hyena Hierarchies và chỉ áp dụng các regularization được triển khai trên kernel được tham số hóa, được ký hiệu là K̄ trong Phương trình (4). Công thức này có hai ưu điểm quan trọng so với S4: (1) kernel K̄ không cần được biên dịch lại, cho phép tăng tốc khi sử dụng nhiều bộ lọc; (2) K̄ có nhiều tham số tự do hơn vì nó không còn bị hạn chế bởi các ma trận A, B trong phương trình 3, có thể cung cấp biểu diễn phong phú hơn có thể giải thích điểm perplexity cải thiện so với các biến thể S4. Tuy nhiên, kernel UNSTRUCT K̄ dựa vào learned positional encoding khiến phương pháp ít có thể mở rộng đến các chuỗi độ dài lớn hơn khi suy luận.

Chúng tôi so sánh Block-State Transformer với bốn baseline khác nhau:

**TRSF-XL:2048** [8] là Transformer với kích thước cửa sổ huấn luyện 2048. Như mong đợi, tăng kích thước cửa sổ cải thiện perplexity, đặc biệt trên các bộ dữ liệu arXiv và GitHub. Tuy nhiên, mô hình này hoạt động tệ hơn BST:SH:HYENA trên PG19 và chậm hơn nhiều, bị nút thắt cổ chai bởi lớp attention trên độ dài chuỗi cao hơn.

**SLIDE:12L** [21] Mô hình này gần như giống hệt TRSF-XL:2048. Tuy nhiên nó sử dụng cửa sổ trượt kích thước 512 trên một đoạn 4096 token. Cửa sổ trượt có thể vi phân qua hai khối, trong khi TRSF-XL không backpropagate qua các key và value được cache từ cửa sổ trước. Baseline đơn giản này gần nhất về mặt tốc độ huấn luyện với BST:SH. Điểm perplexity cho thấy tích hợp biểu diễn của quá khứ, như với BRECT và BST, tác động tích cực đến hiệu suất LM.

**BRECT:FIXED:SKIP** [21] là kiến trúc Block-Recurrent Transformer hiệu suất mạnh nhất và nhanh nhất trong [21]. Kiến trúc này rất tương tự SLIDE:12L. Tuy nhiên có cấu hình "skip" hồi quy tuần tự, cơ chế gating lớp tuyến tính đơn giản kết hợp biểu diễn ẩn khối hiện tại với thông tin quá khứ từ các khối trước.

**GSS-HYBRID-L** [28] là mô hình lai SSM-Transformer gần nhất được kiểm tra trên các tác vụ mô hình hóa ngôn ngữ tầm xa. GSS-HYBRID-L dựa trên Diagonal State Space (DSS) [16]. DSS và S4 tương tự về hiệu suất và kiến trúc, chỉ khác nhau về khởi tạo của kernel K [15]. [16] tiếp tục cải thiện DSS cho các tác vụ LM bằng cách giới thiệu phiên bản Gated State Space gọi là GSS, hoạt động tốt hơn trên PG19, arXiv và GitHub. Khác với phương pháp của chúng tôi, GSS-HYBRID-L không tích hợp trực tiếp các trạng thái SSMs vào cơ chế attention mà chỉ xen kẽ 32 lớp GSS với các lớp Transformer. Cần lưu ý rằng điểm GSS-HYBRID-L được thu được sau khi grid search trên bốn learning rate {6.4, 3.2, 1.6, 0.8} × 10⁻³ và sử dụng learning rate và weight decay khác nhau cho lớp SSM và lớp Transformer để tránh bất ổn huấn luyện. Trong thí nghiệm của chúng tôi, chúng tôi không sử dụng grid search và sử dụng cùng learning rate cho tất cả các lớp. Kết quả BST chứng minh rằng tích hợp các trạng thái SSM vào Transformer attention cung cấp lợi ích lớn hơn việc xen kẽ các lớp SSM và attention như trong GSS-HYBRID-L.

**Ngân sách tính toán cố định.** Như thấy trong Bảng 1, chúng tôi theo dõi lượng tính toán chính xác theo giờ TPUv4 được dành cho huấn luyện mỗi mô hình. Giờ TPUv4 huấn luyện cho SLIDE:12L, TRSF-XL:2048, BRECT:FIXED:SKIP và GSS-HYBRID-L được lấy từ [28]. Metric giờ TPUv4 đo chi phí tính toán của việc huấn luyện mô hình. Cho các thí nghiệm của chúng tôi, chúng tôi căn chỉnh thời gian huấn luyện với GSS-HYBRID-L để so sánh công bằng. Các mô hình tham số nhỏ hơn đều có 12 lớp, 8 head kích thước 128, vector embedding kích thước 1024, MLP với kích thước lớp ẩn 4096 với hàm kích hoạt ReLU. Cho các mô hình BST lớn hơn, chúng tôi tăng gấp đôi kích thước lớp trung gian từ 4096 lên 8192 và tăng số head attention lên 12.

**Chi tiết huấn luyện** Chúng tôi sử dụng cùng thiết lập huấn luyện như [21] và chúng tôi thực hiện thí nghiệm sử dụng thư viện Meliad³ trong JAX/Flax [1,17]. Chúng tôi sử dụng optimizer Adam [25] và batch size 32 và độ dài chuỗi L = 4k cho huấn luyện. Sử dụng hồi quy của SSM có cấu trúc (như S4) trong lớp đầu tiên cho phép chúng tôi mở rộng positional encoding đến các độ dài khác nhau khi suy luận. Các mô hình BST nhỏ hơn có lớp Block-State được tích hợp trong các lớp Transformer {1, 7, 9} và các mô hình BST lớn hơn tại các lớp {1, 5, 7, 9}. Vì các bộ dữ liệu của chúng tôi chứa tài liệu dài, có thể huấn luyện trên độ dài chuỗi L lớn hơn. Huấn luyện trên độ dài chuỗi 4k cho phép chúng tôi kiểm tra tổng quát hóa độ dài vì kernel tích chập K trong Phương trình (3) có thể được mở rộng đến bất kỳ độ dài chuỗi L nào. Tuy nhiên, vì chúng tôi cho thấy trong Phần 4.2 rằng mô hình của chúng tôi hoạt động tốt khi mở rộng đến độ dài chưa thấy, chúng tôi không thấy cần thiết chạy các thí nghiệm đắt đỏ với độ dài chuỗi cao hơn. Cho các biến thể mô hình MF, chúng tôi giảm chiều trạng thái SSM D thêm một hệ số hai để cải thiện hiệu quả FFT. Việc giảm chiều trạng thái có tác động không đáng kể đến perplexity. Các mô hình MF có S = 32 bộ lọc trong khi các mô hình MF lớn hơn có S = 64 bộ lọc.

### 4.2 Đánh giá khả năng Tổng quát hóa Độ dài
Chúng tôi trình bày phân tích tổng quát hóa độ dài và báo cáo perplexity trong Hình 3. Các mô hình và baseline của chúng tôi đều có ∼400M tham số, được huấn luyện trên độ dài chuỗi 4k và kiểm tra trên các chuỗi với độ dài thấp hơn và cao hơn {512, 16k, 65k}.

Chúng tôi nhận thấy tất cả các mô hình có perplexity tương tự cho độ dài chuỗi 512. Cả BST:SH:S4-L và GSS-HYBRID-L đều tổng quát hóa tốt trên độ dài chuỗi 16k và 65k cho PG19 và GitHub. Cho arXiv, perplexity của GSS-HYBRID-L và BST:MF:UNSTRUCT-L tăng mạnh, có thể do nhiễu trong bộ dữ liệu arXiv (như được chỉ ra bởi biến thiên trong metric perplexity theo thời gian). [28] cũng báo cáo rằng các mô hình GSS lớn hơn gặp khó khăn tổng quát hóa đến độ dài cao hơn. Thú vị là, cho arXiv một lần nữa, BRECT:FIXED:SKIP-L hoạt động rất tốt ở độ dài chuỗi cao hơn. Chúng tôi đặt giả thuyết rằng quyền truy cập của mô hình Block-Recurrent vào toàn bộ quá khứ trong quá trình huấn luyện, qua cache không khả vi của biểu diễn qua các chuỗi, giúp giữ lại "bộ nhớ" của các phụ thuộc giữa các mục chính trong bài báo arXiv cho phép mô hình truy cập các ký hiệu, định nghĩa, định lý hoặc phương trình quá khứ vượt quá độ dài chuỗi huấn luyện 4k. Chúng tôi cũng lưu ý rằng BST:MF:UNSTRUCT-L và BRECT:FIXED:SKIP-L vượt trội hơn các phương pháp khác trên PG19 lên đến độ dài chuỗi 16K. Hiệu suất perplexity trên PG19 có lẽ ít phụ thuộc vào các mối quan hệ dài hạn giữa các token, có thể giải thích hiệu suất của các mô hình không có cơ chế tích hợp rõ ràng cho tổng quát hóa độ dài.

Phân tích cũng cho phép chúng tôi rút ra sự phân biệt rõ ràng giữa SSMs có cấu trúc và không có cấu trúc được tích hợp trong các kiến trúc lai. Như đã đề cập trước đó trong Phần 3.1, SSMs như DSS và S4 sử dụng kernel có cấu trúc K, được xây dựng từ các ma trận được học A, B và C cho bất kỳ độ dài chuỗi L nào trong Phương trình 3. Vì K có thể mở rộng đến bất kỳ độ dài chuỗi L tùy ý, cả BST:SH:S4-L và GSS-HYBRID-L đều có cơ chế tích hợp cho tổng quát hóa độ dài mà mô hình BST:MF:UNSTRUCT-L không có cấu trúc không có. BST:MF:UNSTRUCT-L hoạt động tốt nhất trên chuỗi huấn luyện 4K và ngang bằng cho 512 với perplexity tăng cho độ dài chuỗi chưa thấy 16K và 65K. BST:SH:S4-L có perplexity tốt nhất cho độ dài chuỗi 65K trên PG19, GitHub và arXiv. Tương tự như [21], chúng tôi cũng nhận thấy rằng perplexity cải thiện khi chúng tôi mở rộng cửa sổ ngữ cảnh (độ dài chuỗi) cho PG19 và GitHub.

### 4.3 Hiệu quả
Cải thiện so với Block-Recurrent Transformers, với độ phức tạp thời gian O((W² + S² + 2SW) · L/W) ≈ O(L · W), theo sau từ khả năng chạy các cell của Block Transformer song song. Độ phức tạp thời gian của lớp Block-State Transformer bao gồm độ phức tạp thời gian của lớp con mô hình không gian trạng thái, O(D · LlogL), cộng với độ phức tạp thời gian cần thiết để thực thi Transformer trên các chunk ngữ cảnh nhất định (khối) song song, O(W²).

Mặc dù tăng trưởng siêu tuyến tính của lớp con SSM, các thí nghiệm của chúng tôi cho thấy rằng cải thiện hiệu suất đáng kể, lên đến hệ số 6, vẫn rõ ràng cho các chuỗi dài tới 65k token, điểm mà bão hòa phần cứng bắt đầu xảy ra. Khi sử dụng SSM có cấu trúc, độ phức tạp tính toán liên quan chặt chẽ đến kích thước trạng thái bộ nhớ nội bộ của SSM, N – chi tiết cụ thể có thể khác nhau tùy thuộc vào loại SSM chính xác. Chúng tôi đặt N = 16 khi báo cáo hiệu suất. Phía bên trái của Hình 4 cho thấy kết quả benchmark forward-pass của lớp Block-State Transformer trên GPU. Lớp được đề xuất của chúng tôi chạy gần như nhanh hơn 6-11× so với Block-Recurrent Transformers (bao gồm các đơn vị hồi quy), và mang lại hiệu suất tương đương với lớp SLIDE:12L, tức là BRECT không có hồi quy. Ở độ dài chuỗi 4k, chủ yếu được sử dụng trong quá trình huấn luyện, lớp BRECT chạy gần như chậm hơn 15× so với SLIDE:12L với cùng kích thước cửa sổ. Chúng tôi quản lý để giảm khoảng cách này xuống dưới 2× với lớp BST. Để phản ánh mô hình thực tế, cho các thí nghiệm này chúng tôi sử dụng độ dài cửa sổ cố định 128, kích thước trạng thái nội bộ 16 cho SSM, và 16 head. Hơn nữa, để làm nổi bật các lợi ích hiệu suất chỉ do song song hóa được tạo ra bởi framework của chúng tôi, chúng tôi sử dụng cùng kích thước embedding làm đầu vào cho SSM, là 512. Lưu ý rằng chúng tôi sử dụng triển khai vanilla của các hoạt động FFT và inverse FFT được cung cấp bởi JAX [1]. Tuy nhiên, chúng tôi tin rằng tốc độ của phương pháp chúng tôi có thể được cải thiện hơn nữa với các triển khai I/O-aware specific-hardware gần đây và nhanh hơn được giới thiệu trong các framework auto-diff khác.

## 5 Kết luận
Chúng tôi đã giới thiệu một mô hình kết hợp cơ chế attention của Transformers với cơ chế bộ nhớ tầm xa, và tính song song được cung cấp bởi Mô hình Không gian Trạng thái. Chúng tôi khám phá một số biến thể trạng thái bộ nhớ tạo ra các sự đánh đổi khác nhau giữa redundancy và retrievability. Các thí nghiệm cho thấy mô hình của chúng tôi có thể giảm thiểu perplexity ngang bằng và thường cải thiện so với các baseline cạnh tranh gần đây, trong khi đạt được tăng tốc lên đến hơn 10× ở cấp độ lớp, với điều kiện có hỗ trợ phần cứng để tận dụng đầy đủ tính song song. Đây là một tính chất hấp dẫn để mở rộng BST khiến việc thêm SSMs vào Transformers trở nên hấp dẫn về mặt tính toán. Chúng tôi cho thấy tích hợp các trạng thái SSM vào Transformer attention cung cấp lợi ích lớn hơn việc đơn giản xen kẽ các lớp SSM và attention. Cuối cùng, chúng tôi cho thấy mô hình tổng quát hóa đến các chuỗi dài hơn so với những gì nó được huấn luyện.

## Lời cảm ơn
Chúng tôi muốn cảm ơn Caglar Gulcehre và Albert Gu vì các cuộc thảo luận hữu ích và hỗ trợ với codebase S4. Chúng tôi cũng muốn bày tỏ lòng biết ơn đến Delesley Hutchins vì đã cung cấp hướng dẫn quý báu trong suốt dự án, cũng như Xavier Garcia và Courtney Paquette vì việc đánh giá cẩn thận bản thảo, nơi họ đã xác định và khắc phục một số lỗi.

## Tài liệu tham khảo
[1] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018.

[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020.

[3] T.S. Chihara. An Introduction to Orthogonal Polynomials. Dover Books on Mathematics. Dover Publications, 2011.

[4] Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019.

[5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, và Adrian Weller. Rethinking attention with performers. CoRR, abs/2009.14794, 2020.

[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.

[7] James Cooley và John Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of Computation, 19(90):297–301, 1965.

[8] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.

[10] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, và Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models, 2023.

[11] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, và Christopher Ré. Simple hardware-efficient long convolutions for sequence modeling, 2023.

[12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, và Christopher Re. Hippo: Recurrent memory with optimal polynomial projections, 2020.

[13] Albert Gu, Karan Goel, Ankit Gupta, và Christopher Ré. On the parameterization and initialization of diagonal state space models. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

[14] Albert Gu, Karan Goel, và Christopher Ré. Efficiently modeling long sequences with structured state spaces, 2022.

[15] Albert Gu, Ankit Gupta, Karan Goel, và Christopher Ré. On the parameterization and initialization of diagonal state space models, 2022.

[16] Ankit Gupta, Albert Gu, và Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022.

[17] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, và Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023.

[18] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(2):107–116, 1998.

[19] Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997.

[20] Weizhe Hua, Zihang Dai, Hanxiao Liu, và Quoc Le. Transformer quality in linear time. Trong Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, và Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9099–9117. PMLR, 17–23 Jul 2022.

[21] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, và Behnam Neyshabur. Block-recurrent transformers. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. Trong Hal Daumé III và Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5156–5165. PMLR, 13–18 Jul 2020.

[23] Urvashi Khandelwal, He He, Peng Qi, và Dan Jurafsky. Sharp nearby, fuzzy far away: How neural language models use context. CoRR, abs/1805.04623, 2018.

[24] Diederik Kingma và Jimmy Ba. Adam: A method for stochastic optimization. Trong International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.

[25] Diederik P. Kingma và Jimmy Ba. Adam: A method for stochastic optimization. Trong Yoshua Bengio và Yann LeCun, editors, ICLR (Poster), 2015.

[26] Conglong Li, Minjia Zhang, và Yuxiong He. The stability-efficiency dilemma: Investigating sequence length warmup for training GPT models. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.

[27] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, và Luke Zettlemoyer. Mega: Moving average equipped gated attention, 2023.

[28] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, và Behnam Neyshabur. Long range language modeling via gated state spaces. Trong The Eleventh International Conference on Learning Representations, 2023.

[29] OpenAI. Gpt-4 technical report, 2023.

[30] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, và Christopher Ré. Hyena hierarchy: Towards larger convolutional language models, 2023.

[31] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, và Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.

[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019.

[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67, 2020.

[34] Jimmy T. H. Smith, Andrew Warrington, và Scott W. Linderman. Simplified state space layers for sequence modeling, 2023.

[35] Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, và Y-Lan Boureau. Multi-scale transformer language models. CoRR, abs/2005.00581, 2020.

[36] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, và Da-Cheng Juan. Sparse Sinkhorn attention. Trong Hal Daumé III và Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 9438–9447. PMLR, 13–18 Jul 2020.

[37] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient transformers, 2020.

[38] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient transformers. Trong International Conference on Learning Representations, 2021.

[39] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, và Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022.

[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.

[41] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020.

[42] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, và Christian Szegedy. Memorizing transformers. Trong International Conference on Learning Representations, 2022.

[43] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020.

## A Hạn chế
Mặc dù lớp SSM của BST cho phép mô hình mở ra và song song hóa hồi quy mô hình ngữ cảnh dài hạn giữa các khối token, các biến thể SSM phụ thuộc vào các hoạt động FFT hiệu quả. Chúng tôi đã phát hiện rằng hoạt động FFT là nút thắt cổ chai tốc độ quan trọng trên TPUs cần được giải quyết để mở rộng BST tốt hơn đến nhiều lớp và mô hình lớn hơn. Mặc dù chúng tôi vẫn đang điều tra lý do, chúng tôi phát hiện rằng JAX FFT nhanh hơn 4× trên GPUs. Hơn nữa, các biến thể SSM mới như S5[34] bỏ qua các hoạt động FFT sử dụng toán tử kết hợp nhị phân⁴. Triển khai của chúng tôi đủ modular để chúng tôi có thể đơn giản cắm S5 hoặc sử dụng các triển khai FFT khác.

Một trong những giả định của chúng tôi là lớp SSM của BST có thể nắm bắt các phụ thuộc dài hạn phù hợp cho mỗi khối. Hồi quy SSM tại bước T = t cung cấp biểu diễn tóm tắt của các bước trước đó cho T = 0 đến T = t. Tuy nhiên, biểu diễn vector đơn có thể không đủ mạnh để hỗ trợ tất cả các phụ thuộc dài hạn quan trọng. Mặc dù có cải thiện perplexity trên các tác vụ mô hình hóa ngôn ngữ tầm xa, giả định này cần được kiểm tra trên các tác vụ phân loại tầm xa khác như Long Range Arena [37] cũng vậy. Có thể mô hình của chúng tôi có thể hoạt động tốt hơn nếu chúng tôi đưa vào lớp attention k = W biểu diễn SSM được chọn bởi hoạt động truy xuất top-k, tương tự như trong Memorizing Transformer [42].

## B So sánh chi tiết hơn với các baseline hiện có
Phần này cung cấp cho độc giả so sánh sâu hơn với các kiến trúc tương tự. Chúng tôi cover BRECT[21] trong Phần B.1 và GSS-HYBRID[28] trong Phần B.2.

### B.1 So sánh với Block Recurrent Transformer (BRECT)
Lớp con Block Transformer (tức là SLIDE:12L) xử lý keys và values từ cửa sổ trước được lưu trữ trong cache có thể vi phân. Điều này được triển khai tương tự như sliding window attention pattern được gợi ý trong [21] và ban đầu được giới thiệu bởi Transformer-XL [8]. Sử dụng causal mask, tại mỗi bước suy luận token, cơ chế attention được áp dụng cho các khối token có kích thước W và được mở rộng một phần đến các keys và values được cache từ khối trước với sliding window.

BRECT, như được giải thích trong [21], sử dụng cache không khả vi được mang từ một chuỗi có kích thước L đến chuỗi tiếp theo⁵. Các trạng thái hồi quy cuối cùng của một chuỗi được lưu trữ trong cache không khả vi và đưa vào bước huấn luyện tiếp theo trên chuỗi tiếp theo trong tài liệu như warm-start. Chúng tôi không truyền biểu diễn như vậy, vì để tính đầu ra của tích chập, chúng tôi cần quyền truy cập vào toàn bộ chuỗi. Chúng tôi tin rằng đây là một ưu điểm mà BRECT có so với phương pháp của chúng tôi, đặc biệt cho các ví dụ rất dài chia thành các chuỗi có thứ tự có độ dài L, vì cache được mang từ một chuỗi đến chuỗi tiếp theo có thể cung cấp thông tin tầm xa rất hữu ích và quyền truy cập (yếu) vào toàn bộ quá khứ. Vì chúng tôi cần toàn bộ chuỗi để tính các trạng thái SSM, lịch sử vượt quá L có thể bị mất trong quá trình. Chúng tôi tin rằng BST có thể được cải thiện hơn nữa bằng cách thêm cache chuỗi không khả vi cho các tài liệu rất dài.

Trong khi các kiến trúc khác, lịch sử giữa các khối token không được mô hình hóa, cả BST và BRECT đều sử dụng cơ chế để mô hình hóa ngữ cảnh khối trước. Các tác giả của BRECT thí nghiệm với các cơ chế gating tuần tự khác nhau để cô đọng thông tin từ các khối quá khứ. Với BST, chúng tôi sử dụng SSM để cung cấp ngữ cảnh từ các khối trước đó cho khối hiện tại như được giải thích trong Phần 3.2.

### B.2 So sánh với Transformer GSS-HYBRID
GSS-HYBRID [28] là kiến trúc lai SSM-Transformer mà chúng tôi mô tả đầu tiên trong Phần 4.1. Kiến trúc khác đáng kể so với BST. GSS-HYBRID chủ yếu bao gồm các lớp Gated State Space (GSS) và có một vài lớp Transformer xen kẽ tại mỗi lớp thứ 4 bắt đầu với lớp thứ 2. BST mặt khác chủ yếu bao gồm các lớp Block Transformer và có các lớp Block-State Transformer tại các vị trí {1, 7, 9} cho mô hình ∼200M và {1, 5, 7, 9} cho mô hình ∼400M. Lai của chúng tôi không xếp chồng các lớp SSM và Transformer như GSS-HYBRID mà thay thế hồi quy trong BRECT bằng SSM như S4. Trong BST, SSM tạo ra các trạng thái cho mỗi biểu diễn Block Transformer và chúng tôi sau đó sử dụng cross-attention để trộn các trạng thái và đầu ra self-attention. Các tác giả trong [28] ban đầu xây dựng GSS, phiên bản gated của DSS [16], để (1) giảm chiều tham số SSM, (2) ổn định huấn luyện SSM và (3) cho phép tổng quát hóa độ dài tốt hơn. Tuy nhiên, khi thí nghiệm với SSMs như S4 hoặc DSS, chúng tôi phát hiện rằng gating không cần thiết để đạt được cả ba mục tiêu nêu trên. Chúng tôi quyết định rằng sử dụng Gated Attention Unit của GSS [20] do đó không cần thiết khi tích hợp các trạng thái SSM vào cơ chế attention. Chúng tôi cũng nhắc lại rằng các tác giả trong [28] sử dụng tìm kiếm siêu tham số để có được hiệu suất tốt nhất trong khi chúng tôi không.

## C Thí nghiệm Mở rộng
Trong phần này, chúng tôi so sánh cách BST mở rộng so với Transformer-XL với cửa sổ lớn gấp 4 lần và BRECT. Trong Hình 5, chúng tôi thấy rằng ở quy mô thấp hơn, từ 80M đến 200M, BRECT và BST có hiệu suất rất tương tự. Vượt quá 200M, khoảng cách phần trăm hiệu suất perplexity giữa BRECT và BST tăng từ 2.5% ở 200M tham số lên 4.0% ở 1.3B tham số. Khoảng cách phần trăm hiệu suất perplexity giữa BRECT và TRSF-XL thậm chí còn rõ rệt hơn khi nó bắt đầu ở 7.6% ở 200M tham số lên 10.6% ở 1.3B tham số.

## D Thí nghiệm Long Range Arena
Mặc dù trọng tâm chính của nghiên cứu chúng tôi là chứng minh rằng các mô hình lai Transformer-SSM hiệu quả và hoạt động tốt trên LM tự hồi quy ngữ cảnh dài, chúng tôi cũng đánh giá phương pháp của chúng tôi trên tác vụ phân loại tiêu chuẩn nơi các phụ thuộc tầm xa trong một chuỗi quan trọng để nắm bắt. Trong Bảng 2, chúng tôi trình bày kết quả của chúng tôi trên benchmark Long Range Arena (LRA) [38] bao gồm ba phương thức khác nhau bao gồm văn bản, hình ảnh và biểu thức toán học. Bộ dữ liệu LRA cũng kiểm tra các mô hình trên các độ dài chuỗi khác nhau từ 1K đến 16K.

BST:SH:S4 bao gồm bốn lớp BST (không có lớp BRT xen kẽ) và hai lớp S4 ở trên. Chúng tôi sử dụng độ dài khối tiêu chuẩn 512 cho BST và BRT. Tuy nhiên, chúng tôi huấn luyện BST và BRT trên các chuỗi đầy đủ (lên đến 16K cho Path-X). Chúng tôi sử dụng AdamW làm optimizer [24] với warmup cho learning rate, nơi chúng tôi bắt đầu từ giá trị 1e−7 và tăng learning rate tuyến tính lên một giá trị cụ thể ∈ {1e−3, 2e−3, 4e−3} trong 10% đầu của huấn luyện. Điều này được theo sau bởi cosine annealing cho phần còn lại của huấn luyện xuống giá trị 1e−7. Tất cả các lớp đều hai chiều, bao gồm lớp S4 trong BST:SH:S4 như được mô tả trong [13]. Weight decay của chúng tôi được chọn từ {0, 0.05, 0.1, 0.15} và dropout của chúng tôi được chọn từ {0, 0.1}. Ngoại trừ thí nghiệm Path-X, chúng tôi sử dụng weight decay ∈ {0.03, 0.05, 0.07} cho tất cả các tham số ngoại trừ ma trận S4D A và B. Cũng vậy, cho Path-X, phạm vi khởi tạo của bước thời gian discretization Δ cho PathX được giảm từ (Δmin, Δmax) = (0.001, 0.1) xuống (Δmin, Δmax) = (0.0001, 0.01).

Kết quả của chúng tôi trên LRA rất hứa hẹn và cho thấy rằng, so với các phương pháp tiên tiến khác chia chuỗi thành các khối, BST có thể mô hình hóa các phụ thuộc tầm xa. Ví dụ, BST vượt trội hơn MEGA-CHUNK [27] trên bốn trong sáu tác vụ LRA và 1.5% trên điểm trung bình. Tuy nhiên, BST vẫn cần cải thiện (có lẽ bằng cách mở rộng kích thước khối) để bắt kịp MEGA (không có chunks).

## E Nghiên cứu Ablation
Trong phần sau, chúng tôi thực hiện ablation để điều tra (1) vị trí của một lớp SSM đơn trong Bảng 3 trong kiến trúc tổng thể, (2) tác động của số lớp SSM được thêm vào trong Bảng 4, và (3) kích thước D của trạng thái SSM trong Bảng 5. Cho các ablation, chúng tôi sử dụng BST:SH:S4 ∼200M tham số, vì đây là mô hình nhanh nhất, và đánh giá các cấu hình khác nhau trên PG19.

Trong Bảng 3, chúng tôi thí nghiệm thêm một lớp BST đơn tại các chỉ số lớp 3, 6, 9, 12. Chúng tôi nhận thấy rằng một lớp BST đơn với kích thước trạng thái D = 16 nằm gần giữa của toàn bộ chồng Block Transformer, tại index = 9, có tác động lớn nhất đến perplexity. Phát hiện này phù hợp với các phát hiện trong công trình trước [42, 21].

Trong Bảng 4, chúng tôi kiểm tra xem việc thêm nhiều lớp BST có mang lại cải thiện hiệu suất không. Chúng tôi bắt đầu với các lớp BST với kích thước trạng thái D = 16 tại các chỉ số 0, 9. Chúng tôi tiếp tục bằng cách thêm một lớp BST khác tại chỉ số 7 cho tổng cộng ba lớp BST và sau đó một lớp khác tại chỉ số 5, theo sau bởi một lớp khác tại chỉ số 12. Thêm nhiều lớp BST giảm perplexity. Tuy nhiên, kết quả dường như đạt đỉnh ở 5 lớp BST. Chúng tôi cũng lưu ý rằng có tăng 3.5% thời gian bước huấn luyện cho mỗi lớp được thêm vào.

Trong Bảng 5, chúng tôi huấn luyện các mô hình của chúng tôi với các kích thước trạng thái D khác nhau. Cho ablation kích thước trạng thái, chúng tôi sử dụng ba lớp BST tại các chỉ số 0, 7, 9. Chúng tôi thấy rằng tăng D cải thiện perplexity với chi phí của tốc độ huấn luyện (thời gian bước). Vì lý do này, chúng tôi chọn D = 16 cho kết quả BST Bảng 1.

## F Triển khai JAX của BST
Pseudocode 1 chứa hàm triển khai tích chập của nhiều bộ lọc trên cùng chuỗi đầu vào sử dụng FFT và các hoạt động inverse FFT. Pseudocodes 2, 3 và 4 tương ứng triển khai thu thập trạng thái ngữ cảnh của các biến thể BST: Single-Head (SH), Multi-Head (MH) và Multi-Filter (MF). Cuối cùng, Pseudocode 5 chạy lớp con Block Transformer song song bằng cách đưa các trạng thái ngữ cảnh vào khối tương ứng của chúng.

```python
"""Unstructured filters and convolutions."""
import jax
from jax import numpy as jnp
from einops import rearrange

win_length = 512 # (w)
seq_length = 4096 # (l)

def get_filters_unstruct(channels):
    """Returns trainable filters and biases.
    Args:
        channels: number of filters.
    Returns:
        h: filter of shape (seq_length, channels, dim)
        b: bias of shape (channels, dim)
    """
    t = jnp.linspace(0.0, 1.0, seq_length)
    h = jnp.exp(- alpha * t) * dense(positional_emb(t))
    b = get_bias()
    return h, b

def multichannel_convolution(u, h, b):
    """Multichannel convolution function.
    Args:
        u: input of shape (seq_length, dim)
        h: filters of shape (seq_length, channels, dim)
        b: bias of shape (channels, dim)
    """
    h = rearrange(h, "l c d -> c d l")
    fft_size = seq_length * 2
    u_f = jnp.fft.rfft(x, n=fft_size)
    h_f = jnp.fft.rfft(h, n=fft_size)
    y = jnp.fft.irfft(h_f * x_f, n=fft_size, norm="forward")[
        ..., :seq_length] # (c, d, l)
    y = y + x * b[..., None] # (c, d, l)
    y = rearrange(y, "c d l -> l d c")
    return y
```

Pseudocode 1: Bộ lọc không có cấu trúc và tích chập.

```python
"""Context state collection for BST-SH variant."""
num_heads = 8 # (h)
num_states = 32 # (s)

# (SH): Single-Head
def SH_context_states(u):
    """Single-Head Context Collection."""
    h, b = get_filters_[unstruct/s4](channels=1)
    y_1 = multichannel_convolution(u, h, b) # y_1: (l, d, 1)
    # lift to multiple heads
    y_h = dense(y_1) # y_h: (l, d, h)
    context_states = jnp.split(
        y_h, seq_length // win_length, axis=0)
    return context_states # (l/w, w, d, h)
```

Pseudocode 2: Thu thập trạng thái ngữ cảnh cho các biến thể BST-SH.

```python
"""Context state collection for BST-MH variant."""
# (MH): Multi-Head
def MH_context_states(u):
    """Multi-Head Context Collection."""
    h, b = get_filters_[unstruct/s4](channels=num_heads)
    y_h = multichannel_convolution(u, h, b) # y_h: (l, d, h)
    context_states = jnp.split(
        y_h, seq_length // win_length, axis=0)
    return context_states # (l/w, w, d, h)
```

Pseudocode 3: Thu thập trạng thái ngữ cảnh cho các biến thể BST-MH.

```python
"""Context state collection for BST-MF variant."""
# (MF): Multi-Filter
def MF_context_states(u):
    """Multi-Filter Context Collection."""
    h, b = get_filters_[unstruct/s4](channels=num_states)
    y_s = multichannel_convolution(u, h, b) # y_s: (l, d, s)
    context_states = jnp.split(
        y_s, seq_length // win_length, axis=0)
    # context_states: (l/w, w, d, s)
    # collect the last context states
    context_states = context_states[:, -1, ...] # (l/w, d, s)
    context_states = rearrange(
        context_states, "lw d s -> lw s d")
    # shift context states corresponding to windows
    context_states = jnp.roll(context_states, 1, axis=1)
    # replace the initial window with trainable weights
    init_context = get_init_context(num_states) # (d, s)
    context_states[0] = init_context
    # lift to multiple heads
    context_states = dense(context_states)
    return context_states # (l/w, s, d, h)
```

Pseudocode 4: Thu thập trạng thái ngữ cảnh cho các biến thể BST-MF.

```python
"""Block-State Transformer Layer."""
# Block Transformers are non-recurrent and parallelizable.
block_transformer = jax.vmap(BRecT.nonrecurrent_cell)

def BST(u):
    """Block-State Transformer Layer."""
    global MF # True if Multi-Filter, False otherwise (SH/MH)
    # split inputs into windows (l/w, w, d)
    u = jnp.split(u, seq_length // win_length, axis=0)
    # collect context states from SSM outputs
    context_states = [SH/MH/MF]_context_states(u)
    # pass the contexts in place of recurrent states
    y = block_transformer(
        token_embeddings=u,
        recurrent_state=context_states,
        use_cross_attn_causal_mask=not MF,
        use_cross_positional_emb=MF, # context IDs
    )
    return rearrange(y, "lw w d -> (lw w) d") # (l, d)
```

Pseudocode 5: Lớp Block-State Transformer.
