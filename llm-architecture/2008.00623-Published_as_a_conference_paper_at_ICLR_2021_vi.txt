# 2008.00623.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/llm-architecture/2008.00623.pdf
# Kích thước tệp: 909389 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
DELIGHT: TRANSFORMER SÂU VÀ NHẸ
Sachin Mehta1, Marjan Ghazvininejad2, Srinivasan Iyer2,
Luke Zettlemoyer1, 2, và Hannaneh Hajishirzi1,3
1Đại học Washington2Facebook AI Research3Viện Allen về AI
TÓM TẮT
Chúng tôi giới thiệu một transformer sâu và nhẹ, DeLighT, cung cấp hiệu suất tương tự
hoặc tốt hơn so với các mô hình dựa trên transformer tiêu chuẩn với ít tham số hơn đáng kể. DeLighT phân bổ tham số hiệu quả hơn cả (1) trong mỗi khối Transformer bằng cách sử dụng biến đổi DeLighT, một biến đổi sâu và nhẹ và (2) giữa các khối bằng cách sử dụng thu phóng theo khối, cho phép các khối DeLighT nông và hẹp hơn gần đầu vào và các khối DeLighT rộng và sâu hơn gần đầu ra. Nhìn chung, mạng DeLighT sâu gấp 2.5 đến 4 lần so với các mô hình transformer tiêu chuẩn nhưng có ít tham số và phép toán hơn. Các thí nghiệm trên các tác vụ dịch máy và mô hình hóa ngôn ngữ chuẩn cho thấy DeLighT phù hợp hoặc cải thiện hiệu suất của Transformer cơ sở với ít hơn 2 đến 3 lần tham số trung bình.

1 GIỚI THIỆU
Các mạng transformer dựa trên attention (Vaswani et al., 2017) được sử dụng rộng rãi cho các tác vụ mô hình hóa chuỗi, bao gồm mô hình hóa ngôn ngữ và dịch máy. Để cải thiện hiệu suất, các mô hình thường được thu phóng để rộng hơn, bằng cách tăng chiều của các lớp ẩn, hoặc sâu hơn, bằng cách xếp chồng nhiều khối transformer hơn. Ví dụ, T5 (Raffel et al., 2019) sử dụng một chiều 65K và GPT-3 (Brown et al., 2020) sử dụng 96 khối transformer. Tuy nhiên, việc thu phóng như vậy làm tăng số lượng tham số mạng đáng kể (ví dụ, T5 và GPT-3 có lần lượt 11 tỷ và 175 tỷ tham số), và làm phức tạp việc học, tức là các mô hình này hoặc yêu cầu kho dữ liệu huấn luyện rất lớn (Raffel et al., 2019; Devlin et al., 2019; Brown et al., 2020) hoặc điều chuẩn cẩn thận (Hinton et al., 2012; Wan et al., 2013; Merity et al., 2018a). Trong bài báo này, chúng tôi giới thiệu một kiến trúc dựa trên attention hiệu quả tham số mới có thể dễ dàng thu phóng để vừa rộng vừa sâu.

Kiến trúc Transformer Sâu và Nhẹ của chúng tôi, DeLighT, mở rộng kiến trúc transformer của Vaswani et al. (2017) và cung cấp hiệu suất tương tự hoặc tốt hơn với ít tham số và phép toán hơn đáng kể. Trọng tâm của DeLighT là biến đổi DeLighT sử dụng các biến đổi tuyến tính nhóm (GLT) của Mehta et al. (2018) với chiến lược mở rộng-thu gọn để thay đổi chiều rộng và độ sâu của khối DeLighT một cách hiệu quả. Vì GLT có tính cục bộ theo bản chất, biến đổi DeLighT sử dụng xáo trộn đặc trưng, tương tự như xáo trộn kênh trong mạng tích chập (Zhang et al., 2018), để chia sẻ thông tin giữa các nhóm khác nhau. Những biểu diễn rộng và sâu như vậy tạo điều kiện thay thế các lớp attention đa đầu và feed-forward trong transformer với attention đơn đầu và các lớp feed-forward nhẹ, giảm tổng tham số và phép toán mạng. Quan trọng là, không giống như transformer, biến đổi DeLighT tách rời độ sâu và chiều rộng khỏi kích thước đầu vào, cho phép chúng ta phân bổ tham số hiệu quả hơn giữa các khối bằng cách sử dụng các khối DeLighT nông và hẹp hơn gần đầu vào và các khối DeLighT sâu và rộng hơn gần đầu ra.

Chúng tôi chứng minh rằng các mô hình DeLighT đạt được hiệu suất tương tự hoặc tốt hơn so với các mô hình transformer với ít tham số và phép toán hơn đáng kể, trên hai tác vụ mô hình hóa chuỗi phổ biến, (i) dịch máy và (ii) mô hình hóa ngôn ngữ. Trên tập dữ liệu dịch máy WMT'16 En-Ro tài nguyên thấp, DeLighT đạt được hiệu suất transformer sử dụng ít hơn 2:8 tham số. Trên tập dữ liệu WMT'14 En-Fr tài nguyên cao, DeLighT cung cấp hiệu suất tốt hơn (+0.4 điểm BLEU) với ít hơn 1:8 tham số so với transformer cơ sở. Tương tự, về mô hình hóa ngôn ngữ, DeLighT phù hợp với hiệu suất của Transformer-XL (Dai et al., 2019) với ít hơn 1:5 tham số

1arXiv:2008.00623v2  [cs.LG]  11 Feb 2021

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
trên tập dữ liệu WikiText-103. Mã nguồn của chúng tôi là mã nguồn mở và có sẵn tại: https://github.com/
sacmehta/delight

2 CÔNG TRÌNH LIÊN QUAN
Cải thiện transformer: Một số phương pháp đã được giới thiệu để cải thiện kiến trúc transformer. Hướng nghiên cứu đầu tiên giải quyết thách thức tính toán self attention trên các chuỗi đầu vào dài (Child et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). Các phương pháp này có thể được kết hợp với kiến trúc của chúng tôi. Hướng nghiên cứu thứ hai tập trung vào việc giải thích multi-head attention (Raganato và Tiedemann, 2018; Brunner et al., 2020). Họ cho thấy rằng việc tăng số lượng đầu transformer có thể dẫn đến các biểu diễn dư thừa (Voita et al., 2019a; Michel et al., 2019) và việc sử dụng các đầu attention cố định với các mẫu được định trước (Raganato et al., 2020) hoặc ma trận attention tổng hợp (Tay et al., 2020) cải thiện hiệu suất. Hướng nghiên cứu thứ ba tập trung vào việc cải thiện transformer bằng cách học các biểu diễn tốt hơn (Wu et al., 2019; 2020; So et al., 2019). Các công trình này nhằm cải thiện khả năng biểu đạt của transformer bằng cách sử dụng các biến đổi khác nhau – ví dụ, sử dụng tích chập (Wu et al., 2019; Gehring et al., 2017), đơn vị tuyến tính có cổng (Dauphin et al., 2017), hoặc bộ trích xuất đặc trưng đa nhánh (So et al., 2019; Wu et al., 2020). Công trình của chúng tôi thuộc về danh mục này. Không giống như các công trình trước đây, chúng tôi cho thấy rằng có thể phân bổ tham số hiệu quả cả ở cấp độ khối bằng cách sử dụng biến đổi DeLighT và giữa các khối bằng cách sử dụng thu phóng theo khối.

Thu phóng mô hình: Thu phóng mô hình là một phương pháp tiêu chuẩn để cải thiện hiệu suất của các mô hình chuỗi (Vaswani et al., 2017; Raffel et al., 2019; Lan et al., 2020; Devlin et al., 2019; Shoeybi et al., 2019; Tan và Le, 2019; Brown et al., 2020). Các chiều mô hình được tăng trong thu phóng theo chiều rộng (Vaswani et al., 2017; Devlin et al., 2019) trong khi nhiều khối hơn (ví dụ, các khối Transformer) được xếp chồng trong thu phóng theo chiều sâu (Shoeybi et al., 2019; Brown et al., 2020; Wang et al., 2019). Trong cả hai trường hợp (và sự kết hợp của chúng), các tham số bên trong mỗi khối của mạng đều giống nhau, điều này có thể dẫn đến một giải pháp không tối ưu. Để cải thiện thêm hiệu suất của các mô hình chuỗi, bài báo này giới thiệu thu phóng theo khối cho phép các khối có kích thước thay đổi và phân bổ tham số hiệu quả trong mạng. Kết quả của chúng tôi cho thấy rằng (1) các khối DeLighT nông và hẹp hơn gần đầu vào và các khối DeLighT sâu và rộng hơn gần đầu ra mang lại hiệu suất tốt nhất, và (2) các mô hình với thu phóng theo khối kết hợp với thu phóng mô hình đạt được hiệu suất tốt hơn so với chỉ thu phóng mô hình. Chúng tôi lưu ý rằng các mạng nơ-ron tích chập (CNN) cũng học các biểu diễn nông và hẹp hơn gần đầu vào và các biểu diễn sâu và rộng hơn gần đầu ra. Không giống như CNN (ví dụ, ResNet của He et al. 2016) thực hiện một số phép toán cố định tại mỗi lớp tích chập, thu phóng theo khối được đề xuất sử dụng một số phép toán thay đổi trong mỗi lớp và khối.

Cải thiện các mô hình chuỗi: Cũng có công trình gần đây đáng kể về các phương pháp liên quan khác để cải thiện các mô hình chuỗi, bao gồm (1) cải thiện độ chính xác bằng cách sử dụng các biểu diễn cấp token tốt hơn – ví dụ, sử dụng BPE (Sennrich et al., 2016), đầu vào thích ứng (Baevski và Auli, 2019) và đầu ra (Grave et al., 2017a), và DeFINE (Mehta et al., 2020), và (2) cải thiện hiệu quả – ví dụ, sử dụng nén (Chen et al., 2018; Sun et al., 2020), cắt tỉa (Han et al., 2016; Voita et al., 2019b), và chưng cất (Hinton et al., 2015; Sanh et al., 2019). Gần nhất với công trình của chúng tôi là biến đổi DeFINE, cũng học các biểu diễn bằng cách sử dụng chiến lược mở rộng-thu gọn. Sự khác biệt chính giữa biến đổi DeFINE (Hình 1c) và biến đổi DeLighT (Hình 1d) là biến đổi DeLighT phân bổ tham số hiệu quả hơn trong các lớp mở rộng và thu gọn. Không giống như DeFINE, sử dụng ít nhóm hơn trong các biến đổi tuyến tính nhóm để học các biểu diễn rộng hơn, biến đổi DeLighT sử dụng nhiều nhóm hơn để học các biểu diễn rộng hơn với ít tham số hơn. Biến đổi DeLighT đạt được hiệu suất tương đương với biến đổi DeFINE nhưng với ít tham số hơn đáng kể.

3 DELIGHT: TRANSFORMER SÂU VÀ NHẸ
Một khối transformer tiêu chuẩn (Hình 1a) bao gồm multi-head attention sử dụng phép phân tách query-key-value để mô hình hóa mối quan hệ giữa các token chuỗi, và một mạng feed forward (FFN) để học các biểu diễn rộng hơn. Multi-head attention thu được query Q, key K, và value V bằng cách áp dụng ba phép chiếu lên đầu vào, mỗi phép bao gồm h lớp tuyến tính (hoặc đầu) ánh xạ đầu vào dm-chiều vào không gian dh-chiều, trong đó dh=dm=h là chiều đầu. FFN bao gồm hai lớp tuyến tính, trong đó lớp đầu tiên mở rộng các chiều từ dm thành df và

2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Keydm
dhQuerydm
dhValuedm
dh
Attention
Concat
dm
dm
Add
dm
df=4dm
dm
AddMulti-head Attention Feed Forward Network (FFN)Attention ops:
O(dmn2)
FFN params:
8d2
m
Depth = 4
(a) Khối transformerdm
Nb
(Eq. 4)wb
mdm
Keydo
do=dm
2Querydo
doValuedo
do
Attention
do
dm
Add
dm
dm=4
dm
AddBiến đổi DeLighT với Single-head Attention Light-weight FFNAttention ops:
O(don2)
FFN params:
d2
m
2
Depth = 4 + Nb
(b)Khối DeLighTĐầu vào ( dm-chiều)
Đầu ra ( do-chiều)Mở rộng Thu gọnSố lớp (độ sâu) = N
(c) Biến đổi DeFINE

Đầu vào ( dm-chiều)
Đầu ra ( do-chiều)Mở rộng Thu gọnSố lớp (độ sâu) = N
(d)Biến đổi DeLighT

Hình 1: (a, b) So sánh theo khối giữa khối transformer tiêu chuẩn của Vaswani et al. (2017) và khối DeLighT. Trong biến đổi DeLighT, số phép toán trong việc tính toán attention được giảm đi một nửa trong khi số tham số (và phép toán) trong FFN được giảm 16 lần. Các biến đổi với tham số có thể học (Linear và DeLighT) được hiển thị bằng màu. Hình dạng của các biến đổi tuyến tính cho biết hoạt động của chúng (mở rộng, thu gọn, v.v.). (c, d) so sánh biến đổi DeFINE (Mehta et al., 2020) với biến đổi DeLighT. So với biến đổi DeFINE, biến đổi DeLighT sử dụng các biến đổi tuyến tính nhóm (GLT) với nhiều nhóm hơn để học các biểu diễn rộng hơn với ít tham số hơn. Các màu khác nhau được sử dụng để hiển thị các nhóm trong GLT. Để đơn giản, xáo trộn đặc trưng không được hiển thị trong (d).

lớp thứ hai giảm các chiều từ df thành dm. Độ sâu của một khối transformer là 4, bao gồm (1) ba nhánh song song cho queries, keys, và values, (2) một lớp fusion kết hợp đầu ra của nhiều đầu, và (3) hai lớp tuyến tính tuần tự trong FFN. Nhìn chung, các mạng dựa trên transformer xếp chồng tuần tự các khối transformer để tăng dung lượng và độ sâu mạng.

Bài báo này mở rộng kiến trúc transformer và giới thiệu một transformer sâu và nhẹ, DeLighT. Mô hình của chúng tôi sử dụng một biến đổi mở rộng-thu gọn sâu và nhẹ, biến đổi DeLighT (Phần 3.1), cho phép học các biểu diễn rộng hơn một cách hiệu quả. Nó cũng cho phép thay thế các lớp multi-head attention và feed forward network (FFN) với single-head attention và một FFN nhẹ (Phần 3.2). Biến đổi DeLighT tách rời các chiều attention khỏi độ sâu và chiều rộng, cho phép chúng ta học các biểu diễn hiệu quả bằng cách sử dụng thu phóng theo khối thay vì xếp chồng đồng nhất các khối transformer (Phần 3.3).

3.1 BIẾN ĐỔI DELIGHT
Biến đổi DeLighT ánh xạ một vector đầu vào dm-chiều vào một không gian chiều cao (mở rộng) và sau đó giảm nó xuống thành một vector đầu ra do-chiều (thu gọn) bằng cách sử dụng N lớp của các biến đổi nhóm của Mehta et al. (2018), như được hiển thị trong Hình 1d. Trong các giai đoạn mở rộng và thu gọn này, biến đổi DeLighT sử dụng các biến đổi tuyến tính nhóm (GLT) vì chúng học các biểu diễn cục bộ bằng cách thu được đầu ra từ một phần cụ thể của đầu vào và hiệu quả hơn so với các biến đổi tuyến tính. Để học các biểu diễn toàn cục, biến đổi DeLighT chia sẻ thông tin giữa các nhóm khác nhau trong biến đổi tuyến tính nhóm bằng cách sử dụng xáo trộn đặc trưng, tương tự như xáo trộn kênh trong mạng tích chập (Zhang et al., 2018).

Một cách tiếp cận tiêu chuẩn để tăng khả năng biểu đạt và dung lượng của transformer là tăng các chiều đầu vào, dm. Tuy nhiên, việc tăng dm tuyến tính cũng tăng số phép toán trong multi-head attention (O(n2dm), trong đó n là độ dài chuỗi) trong một khối transformer tiêu chuẩn (Hình 1a). Ngược lại, để tăng khả năng biểu đạt và dung lượng của khối DeLighT, chúng ta tăng độ sâu và chiều rộng của các biến đổi DeLighT trung gian bằng cách sử dụng các giai đoạn mở rộng và thu gọn. Điều này cho phép chúng ta sử dụng các chiều nhỏ hơn để tính toán attention, yêu cầu ít phép toán hơn.

Chính thức, biến đổi DeLighT được điều khiển bởi năm tham số cấu hình: (1) số lớp GLT N, (2) hệ số nhân chiều rộng wm, (3) chiều đầu vào dm, (4) chiều đầu ra do, và

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
GLT(Groups = 1)Feature ShuffleInput mixerGLT(Groups = 2)GLT(Groups = 4)Input mixerĐầu vào
Đầu ra

Hình 2: Ví dụ minh họa giai đoạn mở rộng trong biến đổi DeLighT sử dụng GLT, xáo trộn đặc trưng, và kết nối input mixer, để học các biểu diễn sâu và rộng hơn một cách hiệu quả. Cho mục đích minh họa, chúng tôi đã sử dụng cùng chiều đầu vào và đầu ra.

(5) nhóm tối đa gmax trong một GLT. Trong giai đoạn mở rộng, biến đổi DeLighT chiếu đầu vào dm-chiều lên một không gian chiều cao, dmax=wmdm, tuyến tính bằng cách sử dụng ⌊N/2⌋ lớp. Trong giai đoạn thu gọn, biến đổi DeLighT chiếu vector dmax-chiều lên một không gian do-chiều bằng cách sử dụng N-⌊N/2⌋ lớp GLT còn lại. Về mặt toán học, chúng ta định nghĩa đầu ra Y tại mỗi lớp GLT l như:

Yl=
F(X;Wl;bl; gl); l = 1
F(H(X;Yl−1);Wl;bl; gl);Ngược lại(1)

trong đó Wl={Wl1,...,Wlgl} và bl={bl1,...,blgl} là các trọng số và bias có thể học của biến đổi tuyến tính nhóm F với gl nhóm tại lớp l-th. Tóm lại, hàm F nhận đầu vào X hoặc H(X;Yl−1) và chia thành gl nhóm không chồng lấn sao cho X={X1,...,Xgl}. Hàm F sau đó biến đổi tuyến tính mỗi Xi với trọng số Wli và bias bli để tạo ra đầu ra Yli=XiWli+bli. Các đầu ra của mỗi nhóm Yli sau đó được nối để tạo ra đầu ra Yl. Hàm H đầu tiên xáo trộn đầu ra của mỗi nhóm trong Yl−1 và sau đó kết hợp nó với đầu vào X bằng cách sử dụng kết nối input mixer của Mehta et al. (2020) để tránh các vấn đề gradient biến mất. Hình 2 trực quan hóa giai đoạn mở rộng trong biến đổi DeLighT với biến đổi tuyến tính nhóm, xáo trộn đặc trưng, và kết nối input mixer.

Số nhóm tại GLT l-th trong biến đổi DeLighT được tính như:
gl=
min(2l−1; gmax);1≤l≤⌊N/2⌋
gN−l; Ngược lại(2)

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng gmax=⌊dm/32⌋ để mỗi nhóm có ít nhất 32 phần tử đầu vào.

3.2 KHỐI DELIGHT
Hình 1b cho thấy cách chúng tôi tích hợp biến đổi DeLighT vào khối transformer để cải thiện hiệu quả của nó. Các đầu vào dm-chiều đầu tiên được đưa vào biến đổi DeLighT để tạo ra các đầu ra do-chiều, trong đó do< dm. Các đầu ra do-chiều này sau đó được đưa vào một single head attention, tiếp theo là một FFN nhẹ để mô hình hóa mối quan hệ của chúng.

Lớp DeLighT và single head attention: Giả sử chúng ta có một chuỗi n token đầu vào, mỗi token có chiều dm. Những n đầu vào dm-chiều này đầu tiên được đưa vào biến đổi DeLighT để tạo ra n đầu ra do-chiều, trong đó do< dm. Những n đầu ra do-chiều này sau đó được chiếu đồng thời bằng cách sử dụng ba lớp tuyến tính để tạo ra các queries Q, keys K, và values V do-chiều. Sau đó chúng ta mô hình hóa mối quan hệ ngữ cảnh giữa n token này bằng cách sử dụng scaled dot-product attention (Eq. 3). Để cho phép sử dụng các kết nối dư (He et al., 2016), các đầu ra do-chiều của phép toán attention này được chiếu tuyến tính vào một không gian dm-chiều.

Attention (K;Q;V) =softmax(QKT/√do)V (3)

Chúng tôi giả thuyết rằng khả năng của DeLighT trong việc học các biểu diễn rộng hơn cho phép chúng ta thay thế multi-head attention bằng single-head attention. Chi phí tính toán để tính attention trong

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Block-wise Uniform
Đầu vào
Đầu raĐầu vào
Đầu raBblocks N0=Nmin
NB−1=Nmax(xem Eq. 4)
NB−1=NN0=N
(a) Uniform vs. block-wise
B0B1B2B3B4B5B6B7
Decoder blocks400450500550600650700Số tham số 
 (nghìn)Block-wise
Uniform
B0B1B2B3B4B5B6B7
Decoder blocks90100110120130140150160Số phép toán 
 (triệu)Block-wise
Uniform (b) Phân bố tham số và phép toán trong mỗi khối

Hình 3: Thu phóng theo khối phân bổ tham số và phép toán hiệu quả giữa các khối, dẫn đến các khối DeLighT nông và hẹp hơn gần đầu vào và các khối DeLighT sâu và rộng hơn gần đầu ra. Trong (b), mạng DeLighT với cả thu phóng uniform (N=Nmin=Nmax=8) và block-wise (Nmin=4,Nmax=8) có khoảng 16.7 M tham số và thực hiện 3.5 B phép toán (tính cho độ dài chuỗi n=30), tuy nhiên, mạng DeLighT với thu phóng block-wise đã cung cấp perplexity tốt hơn 2 điểm.

transformer tiêu chuẩn và khối DeLighT lần lượt là O(dmn2) và O(don2), trong đó do< dm. Do đó, khối DeLighT giảm chi phí tính attention theo hệ số dm/do. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng do=dm/2, do đó yêu cầu ít hơn 2× phép toán nhân-cộng so với kiến trúc transformer.

FFN nhẹ: Tương tự như FFN trong transformer, khối này cũng bao gồm hai lớp tuyến tính. Vì khối DeLighT đã tích hợp các biểu diễn rộng hơn bằng cách sử dụng biến đổi DeLighT, nó cho phép chúng ta đảo ngược chức năng của các lớp FFN trong transformer. Lớp đầu tiên giảm chiều của đầu vào từ dm thành dm/r trong khi lớp thứ hai mở rộng chiều từ dm/r thành dm, trong đó r là hệ số giảm (xem Hình 1b). FFN nhẹ của chúng tôi giảm số tham số và phép toán trong FFN theo hệ số r×df/dm. Trong transformer tiêu chuẩn, các chiều FFN được mở rộng theo hệ số 4.1Trong các thí nghiệm của chúng tôi, chúng tôi đã sử dụng r=4. Do đó, FFN nhẹ giảm số tham số trong FFN 16 lần.

Độ sâu khối: Khối DeLighT xếp chồng (1) một biến đổi DeLighT với N GLT, (2) ba lớp tuyến tính song song cho key, query, và value, (3) một lớp chiếu, và (4) hai lớp tuyến tính của một FFN nhẹ. Do đó, độ sâu của khối DeLighT là N+4. So với khối transformer tiêu chuẩn (độ sâu là 4), khối DeLighT sâu hơn.

3.3 THU PHÓNG THEO KHỐI
Các phương pháp tiêu chuẩn để cải thiện hiệu suất của các mô hình chuỗi bao gồm tăng các chiều mô hình (thu phóng chiều rộng), xếp chồng nhiều khối hơn (thu phóng chiều sâu), hoặc cả hai. Tuy nhiên, việc thu phóng như vậy không hiệu quả lắm trên các tập dữ liệu nhỏ. Ví dụ, khi một mạng Transformer-Base (dm=512) được thay thế bằng Transformer-Large (dm=1024) trên kho văn bản WMT'16 En-Ro, số tham số tăng khoảng 4× trong khi hiệu suất không thay đổi đáng kể (BLEU: 34.28 vs. 34.35). Chúng tôi giả thuyết rằng điều này xảy ra vì việc thu phóng chiều rộng và chiều sâu mô hình phân bổ tham số đồng nhất giữa các khối, điều này có thể dẫn đến việc học các tham số dư thừa. Để tạo ra các mạng sâu và rộng, chúng ta mở rộng thu phóng mô hình xuống cấp độ khối (xem Hình 3).

Thu phóng khối DeLighT: Khối DeLighT học các biểu diễn sâu và rộng bằng cách sử dụng biến đổi DeLighT, có độ sâu và chiều rộng được điều khiển bởi hai tham số cấu hình: số lớp GLT N và hệ số nhân chiều rộng wm, tương ứng (Hình 3a). Các tham số cấu hình này cho phép chúng ta tăng số tham số có thể học bên trong khối DeLighT độc lập với các chiều đầu vào dm và đầu ra do. Việc hiệu chỉnh như vậy không thể thực hiện với khối transformer tiêu chuẩn vì khả năng biểu đạt và dung lượng của chúng là một hàm của đầu vào (chiều đầu vào = số đầu × chiều đầu). Ở đây, chúng tôi giới thiệu thu phóng theo khối tạo ra một mạng với các khối DeLighT có kích thước thay đổi, phân bổ các khối DeLighT nông và hẹp hơn gần đầu vào và các khối DeLighT sâu và rộng hơn gần đầu ra.

Để làm điều đó, chúng tôi giới thiệu hai tham số cấu hình trên toàn mạng: số GLT tối thiểu Nmin và tối đa Nmax trong một biến đổi DeLighT. Đối với khối DeLighT thứ b, chúng ta tính số GLT Nb và hệ số nhân chiều rộng wbm trong một biến đổi DeLighT bằng cách sử dụng thu phóng tuyến tính (Eq. 4). Với việc thu phóng này, mỗi khối DeLighT có độ sâu và chiều rộng khác nhau (Hình 3a).

Nb=Nmin+(Nmax−Nmin)b/(B−1); wbm=wm+(Nmax−Nmin)b/(Nmin(B−1));0≤b≤B−1(4)

Ở đây, B biểu thị số khối DeLighT trong mạng. Chúng ta thêm chỉ số trên b vào số lớp GLT N và hệ số nhân chiều rộng wm để chỉ ra rằng các tham số này dành cho khối thứ b.

Độ sâu mạng: Độ sâu của khối transformer là cố định, tức là 4. Do đó, các công trình trước đây (Raffel et al., 2019; Brown et al., 2020; Wang et al., 2019) đã liên kết độ sâu của các mạng dựa trên transformer với số khối transformer. Trong DeLighT, chúng tôi trình bày một góc nhìn khác để học các biểu diễn sâu hơn, trong đó mỗi khối có kích thước thay đổi. Để tính độ sâu mạng, chúng ta sử dụng định nghĩa tiêu chuẩn qua các lĩnh vực khác nhau, bao gồm thị giác máy tính (ví dụ, ResNet của He et al. 2016) và học máy lý thuyết (Telgarsky, 2016). Các công trình này đo độ sâu mạng như số lớp có thể học tuần tự (ví dụ, convolution, linear, hoặc group linear). Tương tự, độ sâu của mạng DeLighT và transformer với B khối lần lượt là ∑B−1b=0(Nb+4) và 4B.

4 KẾT QUẢ THỰC NGHIỆM
Chúng tôi đánh giá hiệu suất của DeLighT trên hai tác vụ mô hình hóa chuỗi tiêu chuẩn: (1) dịch máy (Phần 4.1) và (2) mô hình hóa ngôn ngữ (Phần 4.2).

4.1 DỊCH MÁY
Tập dữ liệu và đánh giá: Chúng tôi đánh giá DeLighT trên bốn tập dữ liệu: (1) IWSLT'14 German-English (De-En), (2) WMT'16 English-Romanian (En-Ro), (3) WMT'14 English-German (WMT'14 En-De), và (4) WMT'14 English-French (WMT'14 En-Fr). Đối với tập dữ liệu IWSLT'14 De-En, chúng tôi sao chép thiết lập của Wu et al. (2019) và Edunov et al. (2018), sử dụng 160K/7K/7K cặp câu cho huấn luyện, xác thực, và kiểm tra với từ vựng BPE chung khoảng 10K token, tương ứng. Đối với tập dữ liệu WMT'14 English-German (En-De), chúng tôi theo thiết lập của Vaswani et al. (2017). Tập dữ liệu có 3.9M/39K/3K cặp câu cho huấn luyện, xác thực, và kiểm tra tương ứng với kích thước từ vựng BPE chung là 44K.2Đối với tập dữ liệu WMT'14 English-French (En-Fr), chúng tôi sao chép thiết lập của Gehring et al. (2017), sử dụng 36M/27K/3K cặp câu cho huấn luyện, xác thực, và kiểm tra tương ứng với kích thước từ vựng BPE chung là 44K. Hiệu suất được đánh giá theo BLEU (Papineni et al., 2002) (cao hơn là tốt hơn) trên tập kiểm tra. Chúng tôi theo Wu et al. (2019) cho các siêu tham số liên quan đến tìm kiếm chùm.

Kiến trúc: Chúng tôi theo kiến trúc encoder-decoder đối xứng của Vaswani et al. (2017) với mã hóa vị trí sinusoidal. Cả encoder và decoder đều có B khối DeLighT. Các khối decoder giống hệt các khối encoder (Hình 1b), ngoại trừ chúng có thêm một đơn vị single-head attention source-target trước FFN nhẹ. Trong đơn vị single-head attention source-target, keys và values là các phép chiếu trên đầu ra encoder (chi tiết đầy đủ trong Phụ lục A). Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng wm=2, Nmin=4, và Nmax=8 cho WMT'16 En-Ro, WMT'14 En-De, và WMT'14 En-Fr; dẫn đến mạng DeLighT sâu 222 lớp. Đối với IWSLT'14 De-En, chúng tôi đã sử dụng wm=1, Nmin=3, và Nmax=9; dẫn đến mạng sâu 289 lớp. Để đơn giản, chúng tôi đặt B=Nmax. Chúng tôi sử dụng bảng tra cứu có thể học ánh xạ mỗi token trong từ vựng thành một vector 128-chiều. Chúng tôi triển khai các mô hình của mình bằng Fairseq (Ott et al., 2019) và sử dụng các script được cung cấp để tiền xử lý dữ liệu, huấn luyện, và đánh giá.

Huấn luyện: Đối với các mô hình IWSLT'14 De-En, chúng tôi theo thiết lập của Wu et al. (2019) và huấn luyện tất cả các mô hình trong 50K lần lặp với kích thước batch 4K token trên một GPU NVIDIA GTX 1080. Đối với WMT'16 En-Ro, chúng tôi theo thiết lập huấn luyện của Ghazvininejad et al. (2019) và huấn luyện các mô hình trong 100K lần lặp trên 16 GPU NVIDIA Tesla V100 với kích thước batch hiệu quả 64K token. Đối với WMT'14 En-De và WMT'14 En-Fr, chúng tôi theo thiết lập huấn luyện của Wu et al. (2019) và huấn luyện các mô hình trên 16 GPU V100 trong 30K và 50K lần lặp, tương ứng. Chúng tôi sử dụng Adam (Kingma và Ba, 2015) để tối thiểu hóa mất mát entropy chéo với giá trị làm mượt nhãn 0.1 trong huấn luyện. Để so sánh công bằng, chúng tôi đã huấn luyện các mô hình transformer cơ sở bằng cùng thiết lập huấn luyện.

2Chúng tôi sử dụng dữ liệu huấn luyện và xác thực tương thích với thư viện Tensor2Tensor (Vaswani et al., 2018) để có các so sánh công bằng với các công trình gần đây (ví dụ, Evolved Transformer).

6

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
IWSLT'14 De-En WMT'16 En-Ro
Mô hình # Tham số Tỷ lệ BLEU ΔBLEU # Tham số Tỷ lệ BLEU ΔBLEU
Transformer (Vaswani et al., 2017) – – 34.4y– 62 M – 34.3z–
Transformer (Triển khai của chúng tôi) 42 M 1:0 34.3 – 62 M 1:0 34.3 –
DeLighT 14 M 0:3 33.8 -0.5 22 M 0:35 34.3 0.0
DeLighT 30 M 0:7 35.3 +1.0 53 M 0:85 34.7 +0.4
(a) Kết quả trên kho văn bản nhỏ

WMT'14 En-De WMT'14 En-Fr
Mô hình # Tham số Tỷ lệ BLEU ΔBLEU # Tham số Tỷ lệ BLEU ΔBLEU
Transformer (Vaswani et al., 2017) 62 M – 27.3 – – 62 M 38.1 –
Transformer (Triển khai của chúng tôi) 67 M 1:0 27.7 – 67 M 1:0 39.2 –
DeLighT 37 M 0:55 27.6 -0.1 37 M 0:55 39.6 +0.4
DeLighT 54 M 0:80 28.0 +0.3 54 M 0:80 40.5 +1.3
(b) Kết quả trên kho văn bản lớn

Bảng 1: So sánh với transformer cơ sở trên kho văn bản dịch máy. Các mô hình DeLighT yêu cầu ít tham số hơn đáng kể để đạt được hiệu suất tương tự. Ở đây, y và z biểu thị transformer cơ sở tốt nhất được báo cáo từ Wu et al. (2019) và Ghazvininejad et al. (2019), tương ứng.

Độ sâu # Tham số # MAC BLEU
Transformer 60 67 M 11.1 B 39.2
DeLighT 222 37 M 5.6 B 39.6
DeLighT 222 54 M 8.1 B 40.5

Bảng 2: Mạng DeLighT sâu, nhẹ và hiệu quả so với transformer. Điểm BLEU được báo cáo trên tập dữ liệu WMT'14 En-Fr. Để tính độ sâu mạng, chúng tôi đếm số lớp tuần tự trong mạng (Phần 3.3). Chúng tôi sử dụng 20 token nguồn và 20 token đích để tính các phép toán nhân-cộng (MAC). Xem Phụ lục C để biết chi tiết.

[THIS IS FIGURE: Graph showing comparison of DeLighT with Transformers and Evolved Transformers on WMT'14 En-De corpus, with parameters on x-axis and BLEU score on y-axis]

Hình 4: So sánh DeLighT với Transformer và Evolved Transformer ở hai thiết lập khác nhau, trên kho văn bản WMT'14 En-De: (1) số tham số giống nhau và (2) hiệu suất giống nhau.

4.1.1 KẾT QUẢ
So sánh với transformer cơ sở: Bảng 1 so sánh hiệu suất của DeLighT với transformer cơ sở của Vaswani et al. (2017) trên các kho văn bản khác nhau. DeLighT cung cấp hiệu suất tốt hơn với ít tham số hơn so với transformer, trên các kho văn bản khác nhau. Cụ thể, trên kho văn bản tài nguyên thấp (WMT'16 En-Ro) và tài nguyên cao (WMT'14 En-De & WMT'14 En-Fr), DeLighT cung cấp hiệu suất tương tự hoặc tốt hơn với ít hơn 2:8× và 1:8× tham số, tương ứng. Khi số tham số được tăng lên, DeLighT vượt trội hơn transformer. Ví dụ, trên tập dữ liệu WMT'14 En-Fr, DeLighT sâu hơn 3:7× so với transformer và cải thiện điểm BLEU 1.3 điểm nhưng với ít hơn 13 triệu tham số và ít hơn 3 tỷ phép toán (xem Bảng 2).

Đặc biệt thú vị là các so sánh hiệu suất của DeLighT với transformer cơ sở của Vaswani et al. (2017) và biến thể tìm kiếm thần kinh của nó, tức là Evolved Transformer của So et al. (2019), ở hai thiết lập tham số khác nhau trên kho văn bản WMT'14 En-De trong Hình 4. Đối với các mô hình nhỏ (< 10 M tham số), các mô hình DeLighT cung cấp hiệu suất tốt hơn và để đạt được cùng hiệu suất như các mô hình này, các mô hình DeLighT yêu cầu ít tham số hơn.

So sánh với các phương pháp tiên tiến: Hầu hết các phương pháp tiên tiến đã đánh giá hiệu suất trên WMT'14 En-De trong khi một số cũng đã đánh giá trên IWSLT'14 De-En. Bảng 3 so sánh hiệu suất của DeLighT với các phương pháp tiên tiến trên hai kho văn bản này. DeLighT cung cấp hiệu suất tương tự hoặc tốt hơn so với các phương pháp hiện có. Quan trọng là cần lưu ý rằng các phương pháp hiện có đã cải thiện transformer cơ sở với các lựa chọn thiết kế khác nhau – ví dụ, cấu trúc encoder-decoder bất đối xứng (Wang et al., 2019) và tìm kiếm kiến trúc thần kinh (So et al., 2019). Chúng tôi tin rằng DeLighT, trong tương lai, cũng sẽ hưởng lợi từ các lựa chọn thiết kế như vậy.

Thu phóng các mô hình DeLighT: Hình 5 cho thấy hiệu suất của các mô hình DeLighT cải thiện với việc tăng tham số mạng; gợi ý khả năng học biểu diễn của chúng trên các kho văn bản khác nhau, bao gồm cả tài nguyên thấp.

7

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Mô hình # Tham số BLEU
Transformer (Vaswani et al., 2017) 42 M 34.3
Variational Attention (Deng et al., 2018) – 33.1
Dynamic convolutions (Vaswani et al., 2017) 43 M 35.2
Lite Transformerz(Wu et al., 2020) – 33.6
DeLighT (Của chúng tôi) 30 M 35.3
(a) IWSLT'14 De-En

Mô hình # Tham số BLEU
Transformer (Vaswani et al., 2017) 62 M 27.3
DLCL (Wang et al., 2019) 62 M 27.3
Evolved Transformery(So et al., 2019) 46 M 27.7
Lite Transformerz(Wu et al., 2020) – 26.5
DeLighT (Của chúng tôi) 37 M 27.6
(b) WMT'14 En-De

Bảng 3: So sánh với các phương pháp tiên tiến trên kho văn bản dịch máy. DeLighT cung cấp hiệu suất tương tự hoặc tốt hơn so với các mô hình tiên tiến với ít tham số hơn. Ở đây, y biểu thị mạng sử dụng tìm kiếm kiến trúc thần kinh (NAS) và z biểu thị tham số mạng đầy đủ không được báo cáo.

[THIS IS FIGURE: Four graphs showing scaling up DeLighT models across different datasets (IWSLT'14 De-En, WMT'16 En-Ro, WMT'14 En-De, WMT'14 En-Fr) with parameters on x-axis and BLEU scores on y-axis]

Hình 5: Thu phóng các mô hình DeLighT. Hiệu suất của DeLighT cải thiện với việc tăng số tham số mạng, trên các kho văn bản khác nhau, bao gồm cả tài nguyên thấp (WMT'16 En-Ro).

4.2 MÔ HÌNH HÓA NGÔN NGỮ
Tập dữ liệu và đánh giá: Chúng tôi đánh giá trên tập dữ liệu WikiText-103 (Merity et al., 2017) có 103M/217K/245K token cho huấn luyện, xác thực, và kiểm tra. Nó có từ vựng cấp từ khoảng 260K token. Theo các công trình gần đây (Baevski và Auli, 2019; Dai et al., 2019), chúng tôi báo cáo hiệu suất theo perplexity (thấp hơn là tốt hơn) trên tập kiểm tra.

Kiến trúc: Chúng tôi sử dụng kiến trúc decoder dựa trên transformer của Baevski và Auli (2019) với B khối DeLighT. Chúng tôi sử dụng wm=2, Nmin=4, và Nmax=12. Chúng tôi thu phóng dm bằng các giá trị {384;512;784;1024} để tăng tham số mạng. Để đơn giản, chúng tôi đặt B=Nmax. Theo thực hành tiêu chuẩn, chúng tôi sử dụng adaptive input (Baevski và Auli, 2019) như một bảng tra cứu và adaptive output (Grave et al., 2017a) như lớp phân loại với một đầu (chiều đầu là 128) và hai đuôi (chiều đuôi là 64 và 32). Chúng tôi cũng chia sẻ trọng số giữa các lớp đầu vào và đầu ra.

Huấn luyện: Chúng tôi theo thiết lập huấn luyện của Baevski và Auli (2019), ngoại trừ việc chúng tôi huấn luyện các mô hình trên 8 GPU NVIDIA Tesla V100 trong 100K lần lặp với độ dài ngữ cảnh 512 và kích thước batch hiệu quả 64K token. Chúng tôi sử dụng Adam trong huấn luyện và sử dụng độ dài ngữ cảnh 480 trong kiểm tra.

Kết quả: Bảng 4b so sánh hiệu suất của DeLighT với các phương pháp trước đây trên WikiText-103. Bảng 4a vẽ biểu đồ sự thay đổi của perplexity theo số tham số cho DeLighT và Transformer-XL (Dai et al., 2019) – vượt trội hơn các triển khai dựa trên transformer khác (ví dụ, Baevski và Auli 2019). Cả hai bảng đều cho thấy DeLighT cung cấp hiệu suất tốt hơn so với các phương pháp tiên tiến (bao gồm Transformer-XL) và thực hiện điều này bằng cách sử dụng độ dài ngữ cảnh nhỏ hơn và ít tham số hơn đáng kể, gợi ý rằng biến đổi DeLighT giúp học các mối quan hệ ngữ cảnh mạnh mẽ.

5 PHÂN TÍCH VÀ THẢO LUẬN VỀ HIỆU QUẢ TÍNH TOÁN
Thời gian huấn luyện và tiêu thụ bộ nhớ: Bảng 5 so sánh thời gian huấn luyện và tiêu thụ bộ nhớ của DeLighT với transformer cơ sở. Để so sánh ngang bằng, chúng tôi đã triển khai đơn vị Transformer mà không có CUDA kernel chuyên dụng của NVIDIA, và huấn luyện cả mạng transformer và DeLighT độ chính xác đầy đủ trong 30K lần lặp trên 16 GPU NVIDIA V100. Các mô hình transformer và DeLighT mất khoảng 37 và 23 giờ để huấn luyện và tiêu thụ khoảng 12.5 GB và 14.5 GB bộ nhớ GPU, tương ứng (R1 vs R2). Khi chúng tôi kích hoạt CUDA kernel chuyên dụng được cung cấp bởi thư viện APEX3 cho multi-head attention trong Transformer, thời gian huấn luyện của

3https://github.com/NVIDIA/apex

8

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
[THIS IS CHART: Line graph showing relationship between parameters and perplexity for DeLighT vs Transformer-XL]

(a)DeLighT vs. Transformer-XL

[THIS IS TABLE: Comparison table showing Method, Network Depth, Context Length, # Params, and Perplexity for various models including LSTM, QRNN, Transformer-XL, and DeLighT]

(b) So sánh với các phương pháp hiện có

Bảng 4: Kết quả trên tập dữ liệu WikiText-103. So với Transformer-XL, DeLighT cung cấp hiệu suất tương tự hoặc tốt hơn (perplexity thấp hơn) với ít tham số hơn. yĐối với Transformer-XL, chúng tôi tái tạo kết quả bằng mã nguồn chính thức. Để đánh giá Transformer-XL với độ dài ngữ cảnh 480, chúng tôi đặt siêu tham số mem_len thành 480 trong các script đánh giá chính thức.

[THIS IS TABLE: Performance comparison showing Model, # Params, BLEU, Training time, and Memory for different transformer variants]

Bảng 5: So sánh với transformer cơ sở về tốc độ huấn luyện và tiêu thụ bộ nhớ. Trong R4, chúng tôi đã triển khai CUDA kernel cho các hàm grouping và ungrouping (xem Phụ lục E). Chúng tôi mong đợi DeLighT hiệu quả hơn với một CUDA kernel duy nhất và chuyên dụng cho grouping, transformation, feature shuffling, và ungrouping. Tiêu thụ bộ nhớ được đo trên một GPU NVIDIA GP100 duy nhất (bộ nhớ 16 GB) với tối đa 4096 token mỗi batch và không có bất kỳ tích lũy gradient nào.

[THIS IS TABLE: Simple comparison showing Model, Dropout, and BLEU scores for Transformer and DeLighT]

Bảng 6: DeLighT yêu cầu ít điều chuẩn hơn so với transformer cơ sở (Tập dữ liệu: WMT'14 En-De).

mô hình transformer giảm từ 37 xuống 16 giờ trong khi chúng tôi không quan sát thấy bất kỳ thay đổi đáng kể nào trong tiêu thụ bộ nhớ. Được thúc đẩy bởi quan sát này, chúng tôi đã triển khai CUDA kernel chuyên dụng cho các hàm grouping và ungrouping trong GLT (xem Phụ lục E). Với những thay đổi này, thời gian huấn luyện và tiêu thụ bộ nhớ GPU của DeLighT giảm khoảng 4 giờ và 3 GB, tương ứng. Chúng tôi nhấn mạnh rằng grouping, linear transformation, feature shuffling, và ungrouping, có thể được triển khai hiệu quả bằng một CUDA kernel duy nhất. Trong tương lai, chúng tôi mong đợi một CUDA kernel chuyên dụng cho các phép toán này sẽ giảm thêm tiêu thụ bộ nhớ cũng như thời gian huấn luyện/suy luận.

Điều chuẩn: Bảng 6 cho thấy DeLighT cung cấp hiệu suất tương tự với transformer cơ sở, nhưng với ít tham số hơn và ít điều chuẩn hơn. Điều này gợi ý rằng việc học biểu diễn với các hàm biến đổi tốt hơn làm giảm nhu cầu dropout.

6 KẾT LUẬN
Bài báo này giới thiệu một kiến trúc transformer sâu và nhẹ, DeLighT, phân bổ tham số hiệu quả cả trong khối DeLighT và giữa các khối DeLighT. So với các mô hình transformer tiên tiến, các mô hình DeLighT (1) sâu và nhẹ và (2) cung cấp hiệu suất tương tự hoặc tốt hơn. Trong tương lai, chúng tôi dự định áp dụng DeLighT cho các tác vụ khác, bao gồm tiền huấn luyện mô hình ngôn ngữ, trả lời câu hỏi, và sinh ngôn ngữ.

Lời cảm ơn: Nghiên cứu này được hỗ trợ bởi ONR N00014-18-1-2826, DARPA N66001-19-2-403, NSF (IIS-1616112, IIS1252835), và giải thưởng Allen Distinguished Investigator. Các tác giả cũng muốn cảm ơn các thành viên của UW-NLP và H2Lab tại Đại học Washington vì phản hồi và nhận xét quý báu của họ.

9

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
TÀI LIỆU THAM KHẢO
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
và Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, trang
5998–6008, 2017.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, và Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv
preprint arXiv:1910.10683, 2019.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models
are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
and Short Papers), 2019.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, và Ruslan R Salakhutdinov. Improving
neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, và Rob Fergus. Regularization of neural networks using
dropconnect. In International conference on machine learning, trang 1058–1066, 2013.

Stephen Merity, Nitish Shirish Keskar, và Richard Socher. Regularizing and optimizing LSTM language
models. In International Conference on Learning Representations, 2018a. URL https://openreview.net/
forum?id=SyyGPP0TZ.

Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, và Hannaneh Hajishirzi. Pyramidal recurrent unit
for language modeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, 2018.

Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, và Jian Sun. Shufﬂenet: An extremely efﬁcient convolutional
neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern
recognition, trang 6848–6856, 2018.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov. Transformer-xl:
Attentive language models beyond a ﬁxed-length context. In Association for Computational Linguistics, 2019.

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse transformers.
arXiv preprint arXiv:1904.10509, 2019.

Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: The efﬁcient transformer. In International
Conference on Learning Representations, 2020.

Iz Beltagy, Matthew E. Peters, và Arman Cohan. Longformer: The long-document transformer.
arXiv:2004.05150, 2020.

Alessandro Raganato và Jörg Tiedemann. An analysis of encoder representations in transformer-based machine
translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP, Tháng 11 2018.

Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, và Roger Wattenhofer.
On identiﬁability in transformers. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=BJg1f6EFDB.

Elena Voita, Rico Sennrich, và Ivan Titov. The bottom-up evolution of representations in the transformer: A
study with machine translation and language modeling objectives. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), 2019a.

Paul Michel, Omer Levy, và Graham Neubig. Are sixteen heads really better than one? In Advances in Neural
Information Processing Systems, trang 14014–14024, 2019.

10

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Alessandro Raganato, Yves Scherrer, và Jörg Tiedemann. Fixed encoder self-attention patterns in transformer-
based machine translation. arXiv preprint arXiv:2002.10260, 2020.

Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, và Che Zheng. Synthesizer: Rethinking
self-attention in transformer models. arXiv preprint arXiv:2005.00743, 2020.

Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, và Michael Auli. Pay less attention with lightweight
and dynamic convolutions. In International Conference on Learning Representations, 2019.

Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, và Song Han. Lite transformer with long-short range attention.
In International Conference on Learning Representations, 2020.

David So, Quoc Le, và Chen Liang. The evolved transformer. In Proceedings of the 36th International
Conference on Machine Learning, trang 5877–5886, 2019.

Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, và Yann N Dauphin. Convolutional sequence to
sequence learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
trang 1243–1252. JMLR. org, 2017.

Yann N Dauphin, Angela Fan, Michael Auli, và David Grangier. Language modeling with gated convolutional
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, trang
933–941. JMLR. org, 2017.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. Albert:
A lite bert for self-supervised learning of language representations. In International Conference on Learning
Representations, 2020.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, và Bryan Catanzaro.
Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint
arXiv:1909.08053, 2019.

Mingxing Tan và Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In
Kamalika Chaudhuri và Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, 2019.

Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, và Lidia S. Chao. Learning deep
transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 2019.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016.

Rico Sennrich, Barry Haddow, và Alexandra Birch. Neural machine translation of rare words with subword
units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), Tháng 8 2016.

Alexei Baevski và Michael Auli. Adaptive input representations for neural language modeling. In International
Conference on Learning Representations, 2019.

Édouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, và Hervé Jégou. Efﬁcient softmax
approximation for GPUs. In International Conference on Machine Learning, 2017a.

Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, và Hannaneh Hajishirzi. DeFINE: Deep
Factorized Input Token Embeddings for Neural Sequence Modeling. In International Conference on Learning
Representations, 2020.

Patrick Chen, Si Si, Yang Li, Ciprian Chelba, và Cho-Jui Hsieh. Groupreduce: Block-wise low-rank ap-
proximation for neural language model shrinking. In Advances in Neural Information Processing Systems,
2018.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, và Denny Zhou. Mobilebert: a compact
task-agnostic bert for resource-limited devices. In Association for Computational Linguistics (ACL), 2020.

Song Han, Huizi Mao, và William J Dally. Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. In International Conference for Representation Learning, 2016.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, và Ivan Titov. Analyzing multi-head self-attention:
Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, 2019b.

11

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep
Learning and Representation Learning Workshop, 2015.

Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, a distilled version of bert: smaller,
faster, cheaper and lighter. In 5th Workshop on Energy Efﬁcient Machine Learning and Cognitive Computing -
NeurIPS, 2019.

Matus Telgarsky. Beneﬁts of depth in neural networks. COLT, 2016.

Sergey Edunov, Myle Ott, Michael Auli, David Grangier, và Marc'Aurelio Ranzato. Classical structured
prediction losses for sequence to sequence learning. In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume
1 (Long Papers), 2018.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion
Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, và Jakob Uszkoreit.
Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.
07416.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics,
trang 311–318. Association for Computational Linguistics, 2002.

Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, và Michael
Auli. Fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019:
Demonstrations, 2019.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, và Luke Zettlemoyer. Mask-predict: Parallel decoding of
conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), trang 6114–6123, 2019.

Diederik P Kingma và Jimmy Ba. Adam: A method for stochastic optimization. In International Conference
on Learning Representations, 2015.

Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, và Alexander Rush. Latent alignment and variational
attention. In Advances in Neural Information Processing Systems, trang 9712–9724, 2018.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel mixture models. In
International Conference on Learning Representations, 2017.

Edouard Grave, Armand Joulin, và Nicolas Usunier. Improving neural language models with a continuous
cache. In International Conference on Learning Representations, 2017b.

Stephen Merity, Nitish Shirish Keskar, và Richard Socher. An analysis of neural language modeling at multiple
scales. arXiv preprint arXiv:1803.08240, 2018b.

12

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
A KIẾN TRÚC DELIGHT CHO MÔ HÌNH HÓA NGÔN NGỮ VÀ DỊCH MÁY

Kiến trúc DeLighT cho mô hình hóa ngôn ngữ và dịch máy được hiển thị trong Hình 6. Đối với mô hình hóa ngôn ngữ, chúng tôi theo kiến trúc trong Baevski và Auli (2019) trong khi đối với dịch máy, chúng tôi theo kiến trúc trong Vaswani et al. (2017).

Mô hình hóa ngôn ngữ: Hình 6a hiển thị kiến trúc cho mô hình hóa ngôn ngữ. Kiến trúc xếp chồng B khối DeLighT, cấu hình của mỗi khối được xác định bằng thu phóng theo khối. Mỗi khối có ba lớp con. Lớp đầu tiên là biến đổi DeLighT học biểu diễn trong không gian chiều cao. Lớp thứ hai là single-head attention mã hóa mối quan hệ ngữ cảnh. Lớp thứ ba là mạng feed-forward nhẹ theo vị trí. Tương tự như Vaswani et al. (2017), chúng tôi sử dụng kết nối dư (He et al., 2016). Tương tự như các công trình trước đây (Baevski và Auli, 2019; Dai et al., 2019), chúng tôi sử dụng adaptive input liên kết (Baevski và Auli, 2019) và adaptive softmax (Grave et al., 2017a) để ánh xạ token thành vector và vector thành token, tương ứng.

Dịch máy: Hình 6b hiển thị kiến trúc cho dịch máy. Encoder xếp chồng B khối DeLighT, cấu hình của mỗi khối được xác định bằng thu phóng theo khối. Tương tự như mô hình hóa ngôn ngữ, mỗi khối encoder có ba lớp con. Lớp đầu tiên là biến đổi DeLighT học biểu diễn trong không gian chiều cao. Lớp thứ hai là single-head attention mã hóa mối quan hệ ngữ cảnh. Lớp thứ ba là mạng feed-forward nhẹ theo vị trí. Tương tự như Vaswani et al. (2017), chúng tôi sử dụng kết nối dư (He et al., 2016). Chúng tôi sử dụng bảng tra cứu có thể học để ánh xạ token

Đầu vào (shifted right)Adaptive
InputsPositional
Encoding
Embedding
LayerMasked Single-
head AttentionAdd & NormLight-weight
FFNAdd & NormAdaptive
SoftmaxLogits
B
Trọng số đầu vào và đầu ra được liên kết
(a) Mô hình hóa ngôn ngữ

ĐầuvàoLook-up
TablePositional
Encoding
Embedding
LayerSingle-head
AttentionAdd & NormLight-weight
FFNAdd & NormB

Đầu ra (shifted right)Look-up
TablePositional
Encoding
Embedding
LayerMasked Single-
head AttentionAdd & NormSingle-head
AttentionAdd & NormLight-weight
FFNAdd & NormLinearSoftmaxLogits
B
Trọng số đầu vào và đầu ra được liên kết
(b) Dịch máy

Hình 6: Mô hình hóa chuỗi với DeLighT. Ở đây, hình lục giác màu xanh lá cây đại diện cho biến đổi DeLighT.

13

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
thành vector. Tương tự như encoder, decoder cũng xếp chồng B khối. Các khối decoder giống hệt các khối encoder, ngoại trừ chúng có thêm một đơn vị single-head attention source-target trước FFN nhẹ. Key và value trong đơn vị single-head attention source-target là các phép chiếu trên đầu ra encoder. Chúng tôi sử dụng bảng tra cứu có thể học tiêu chuẩn để ánh xạ token thành vector và lớp phân loại tuyến tính để ánh xạ vector thành token.

B BIẾN ĐỔI TUYẾN TÍNH NHÓM VỚI KẾT NỐI INPUT-MIXER
Biến đổi tuyến tính nhóm (GLT) F chia một đầu vào dm-chiều X thành g nhóm không chồng lấn sao cho X=Concat(X1,...,Xg), trong đó Xi là một vector dm/g-chiều. Xi's sau đó được biến đổi đồng thời bằng g biến đổi tuyến tính Wi∈R^(dm/g×do/g) để tạo ra g đầu ra Yi=XiWi. Yi's sau đó được nối để tạo ra đầu ra cuối cùng do-chiều Y=Concat(Y1,...,Yg).

Hình 7a hiển thị một ví dụ về GLT trong giai đoạn mở rộng của biến đổi DeLighT. Cho mục đích minh họa, chúng tôi đã sử dụng cùng chiều trong ví dụ này. Nhớ lại rằng khi chúng ta đi sâu hơn trong giai đoạn mở rộng, số nhóm tăng lên. Trong ví dụ này, lớp đầu tiên có một nhóm, lớp thứ hai có hai nhóm và lớp thứ ba có bốn nhóm. GLT học biểu diễn cụ thể theo nhóm và mang tính cục bộ. Để cho phép GLT học biểu diễn toàn cục, chúng tôi sử dụng xáo trộn đặc trưng. Một ví dụ về GLT với xáo trộn đặc trưng được hiển thị trong Hình 7b. Hơn nữa, huấn luyện mạng nơ-ron sâu bằng cách chỉ xếp chồng tuyến tính hoặc tuyến tính nhóm (có hoặc không có xáo trộn đặc trưng) là thách thức vì vấn đề gradient biến mất. Kết nối dư được giới thiệu bởi He et al. (2016) giảm thiểu vấn đề này và giúp huấn luyện mạng nơ-ron sâu. Tuy nhiên, các kết nối như vậy không thể được sử dụng khi chiều đầu vào và đầu ra không giống nhau (ví dụ, trong các giai đoạn mở rộng và thu gọn trong biến đổi DeLighT). Để ổn định quá trình huấn luyện và học biểu diễn sâu hơn, chúng tôi sử dụng kết nối input-mixer của Mehta et al. (2020). Hình 7c hiển thị một ví dụ về GLT với xáo trộn đặc trưng và kết nối input mixer.

Groups = 1Groups = 2Groups = 4
(a) GLT

Groups = 1Groups = 2Groups = 4Feature Shuffle (b) GLT với xáo trộn đặc trưng

Groups = 1Groups = 2Groups = 4Feature ShuffleInput mixer
Input mixer(c) GLT với xáo trộn đặc trưng và
kết nối mixture đầu vào

Hình 7: Hình này trực quan hóa các biến thể khác nhau của biến đổi tuyến tính nhóm được sử dụng trong biến đổi DeLighT.

C PHÉP TOÁN NHÂN-CỘNG TRONG DELIGHT
Khối DeLighT được xây dựng bằng cách sử dụng biến đổi tuyến tính, GLT, và scaled dot-product attention. Tổng số phép toán nhân-cộng (MAC) trong một mạng là sự tích lũy của các phép toán riêng lẻ này. Gọi n biểu thị số token nguồn, m biểu thị số token đích, dm biểu thị chiều đầu vào, do biểu thị chiều đầu ra, và g biểu thị số nhóm trong GLT. Quy trình đếm MAC cho mỗi phép toán này được mô tả dưới đây.

Biến đổi tuyến tính nhóm (GLT): GLT F có g ma trận có thể học Wi∈R^(dm/g×do/g). Do đó, GLT học dmdo/g tham số và thực hiện dmdo/g MAC để biến đổi đầu vào dm-chiều thành đầu ra do-chiều. Theo thực hành tiêu chuẩn, ví dụ, ResNet của He et al. (2016), chúng tôi đếm phép cộng và nhân như một phép toán thay vì hai vì các phép toán này có thể được hợp nhất trong phần cứng gần đây.

14

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Quan trọng là, khi g=1, GLT giống như biến đổi tuyến tính.

Self-attention trong DeLighT: Scaled dot-product self-attention trong DeLighT được định nghĩa như:
Attention (K;Q;V) =softmax(QK^T/√do)V (5)
trong đó Q∈R^(n×do), K∈R^(n×do), V∈R^(n×do) biểu thị query, key, và value, tương ứng.

Phép toán attention bao gồm hai tích vô hướng. Tích vô hướng đầu tiên giữa Q và K trong khi tích vô hướng thứ hai là giữa đầu ra của tích vô hướng đầu tiên và V. Cả hai tích vô hướng đều yêu cầu don² MAC. Do đó, tổng số MAC trong việc tính scaled dot-product self-attention là 2don².

Trong trường hợp source-target attention (như trong dịch máy), K's và V's đến từ nguồn (encoder) và Q's được giải mã tăng dần (từng token một). Do đó, số MAC cần thiết để giải mã m token đích cho n token nguồn là m∑k=1^m 2kndo.

D ABLATION TRÊN TẬP DỮ LIỆU WIKITEXT-103
Bảng 7 nghiên cứu tác động của các tham số khối DeLighT trên tập dữ liệu WikiText-103, cụ thể là (1) số GLT tối thiểu Nmin, (2) số GLT tối đa Nmax, (3) hệ số nhân chiều rộng wm, và (4) chiều mô hình dm (xem Hình 1b). Hình 8, Hình 9, và Hình 10 hiển thị tác động của biến đổi DeLighT, xáo trộn đặc trưng, và FFN nhẹ. Bảng 8 hiển thị hiệu quả của vị trí biến đổi DeLighT trong khối DeLighT trong khi Hình 12 hiển thị hiệu quả của thu phóng mạng DeLighT. Chúng tôi chọn tập dữ liệu WikiText-103 cho ablation vì nó có từ vựng rất lớn so với các tập dữ liệu khác (267K vs. 30-40K), cho phép chúng tôi kiểm tra khả năng dưới kích thước từ vựng lớn. Hiệu suất được báo cáo theo perplexity (thấp hơn là tốt hơn) trên tập xác thực. Trong các nghiên cứu ablation, chúng tôi sử dụng cùng thiết lập huấn luyện như trong Phần 4.2 ngoại trừ việc chúng tôi chỉ huấn luyện trong 50K lần lặp.

Khối DeLighT: Nhìn chung, Bảng 7 cho thấy rằng thu phóng độ sâu và chiều rộng bằng cách sử dụng biến đổi DeLighT và thu phóng theo khối cải thiện hiệu suất. Chúng tôi đưa ra các quan sát sau:

a)Thu phóng theo khối (R4, R5) cung cấp hiệu suất tốt hơn so với thu phóng đồng nhất (R1-R3). Ví dụ, DeLighT với Nmin=4 và Nmax=8 (R4) nông hơn 1:25× so với DeLighT với Nmin=8 và Nmax=8 (R2), nhưng cung cấp hiệu suất tốt hơn với số tham số và phép toán tương tự. Thu phóng wm cải thiện hiệu suất (R2 vs. R3), tuy nhiên, sự cải thiện thấp hơn đáng kể so với mô hình với thu phóng theo khối (R3 vs. R5). Điều này gợi ý rằng phân bố không đồng nhất tham số giữa các khối cho phép mạng học biểu diễn tốt hơn.

b)Các tỷ lệ khác nhau giữa Nmax và Nmin mang lại kết quả khác nhau. Chúng tôi quan sát thấy cải thiện hiệu suất đáng kể khi tỷ lệ lớn hơn hoặc bằng hai. Ví dụ, khi chúng ta thu phóng Nmax/Nmin từ 2 thành 3 (R6 vs. R8), perplexity cải thiện 5 điểm với chỉ tăng vừa phải trong tham số mạng. Mặt khác, khi Nmax/Nmin gần 1 (R6 vs. R7), hiệu suất không thay đổi đáng kể. Điều này có thể vì việc phân bổ tham số giữa các khối gần như đồng nhất (Eq. 4). Điều này phù hợp với quan sát trước đây của chúng tôi.

c)Học biểu diễn nông và hẹp hơn gần đầu vào và biểu diễn sâu và rộng hơn gần đầu ra đạt được hiệu suất tốt hơn. Ví dụ, khi chúng ta thu phóng Nmax từ 8 thành 12 cho Nmin=4 (R6, R8), DeLighT cung cấp hiệu suất tốt hơn với số tham số tương tự so với mô hình với Nmin=6 (R7, R9). Điều này có thể vì tỷ lệ của Nmax và Nmin cao hơn khi Nmin=4, giúp phân bổ tham số mỗi khối hiệu quả hơn.

d)Biểu diễn sâu và rộng hơn gần đầu vào và biểu diễn nông và hẹp hơn gần đầu ra làm giảm hiệu suất (R13 vs. R16).

e)Thu phóng chiều rộng bằng cách sử dụng wm và dm cải thiện hiệu suất (R10-R15), tuy nhiên, tác động của chúng khác nhau. Ví dụ, khi chúng ta thu phóng wm và dm gấp hai, tốc độ tăng số tham số và phép toán nhanh hơn với dm so với wm. Khả năng của DeLighT trong việc học biểu diễn rộng hơn theo nhiều cách khác nhau có thể hữu ích trong việc lựa chọn các mô hình cụ thể theo ứng dụng.

Tác động của biến đổi DeLighT: Chúng tôi thay thế biến đổi DeLighT trong khối DeLighT (Hình 1b) bằng (1) biến đổi DeFINE và (2) một chồng các lớp tuyến tính. Hình 8 cho thấy biến đổi DeLighT cung cấp hiệu suất tương tự với ít tham số hơn đáng kể so với đơn vị DeFINE

15

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Dòng # Nmin Nmax wm dm Độ sâu Tham số MAC Perplexity
Thu phóng đồng nhất vs. theo khối
R1 4 4 2 256 43 14.1 M 2.96 B 56.19
R2 8 8 2 256 115 16.6 M 3.49 B 48.58
R3 8 8 4 256 115 22.1 M 4.64 B 45.10
R4 4 8 2 256 92 16.7 M 3.51 B 46.30
R5 4 12 2 256 158 21.0 M 4.41 B 41.18
Thay đổi độ sâu (Nmin và Nmax (Eq. 4)
R6 4 8 2 256 92 16.7 M 3.51 B 46.30
R7 6 8 2 256 102 16.5 M 3.46 B 46.68
R8 4 12 2 256 158 21.0 M 4.41 B 41.18
R9 6 12 2 256 172 20.0 M 4.20 B 42.26
Thay đổi chiều rộng biến đổi DeLighT wm (Eq. 4)
R10 4 12 2 256 158 21.0 M 4.41 B 41.18
R11 4 12 3 256 158 23.8 M 4.99 B 39.92
R12 4 12 4 256 158 27.1 M 5.69 B 39.10
Thay đổi chiều rộng mô hình dm
R13 4 12 2 256 158 21.0 M 4.41 B 41.18
R14 4 12 2 384 158 29.9 M 6.28 B 35.14
R15 4 12 2 512 158 43.8 M 9.20 B 30.81
Sâu và rộng hơn gần Đầu vào
R16 12 4 2 256 158 21.0 M 4.41 B 43.10

Bảng 7: Ablation về các khía cạnh khác nhau của khối DeLighT, bao gồm thu phóng đồng nhất vs. theo khối, thu phóng độ sâu, và thu phóng chiều rộng. Các dòng được tô sáng một phần có cùng cấu hình (lặp lại để minh họa kết quả). Thiết lập thực nghiệm của chúng tôi tương tự như Phần 4, ngoại trừ việc chúng tôi huấn luyện các mô hình trong 50K lần lặp. Phép toán nhân và cộng (MAC) được tính cho 20 bước thời gian.

và các lớp tuyến tính. Trong các thí nghiệm này, thiết lập giống như R13-R15 (Bảng 7), ngoại trừ, Nmax=8, vì các mô hình với chồng các lớp tuyến tính học quá nhiều tham số.

Xáo trộn đặc trưng: Hình 9 cho thấy xáo trộn đặc trưng cải thiện hiệu suất của DeLighT 1-2 điểm perplexity. Ở đây, chúng tôi sử dụng cùng thiết lập như trong R13-R15 (Bảng 7).

FFN nhẹ: Hình 10 hiển thị tác động của việc thay đổi hệ số giảm r trong FFN nhẹ. Chúng tôi sử dụng cùng thiết lập như trong R13 (Bảng 7). Chúng tôi không quan sát thấy bất kỳ sự giảm hiệu suất đáng kể nào cho đến r=4. Vượt quá r=4, chúng ta thấy sự giảm hiệu suất (perplexity tăng 2 điểm). Trong các trường hợp như vậy, các chiều bên trong của FFN nhẹ rất nhỏ và làm giảm hiệu suất. Đáng chú ý là FFN nhẹ với r=2^2 cung cấp cùng hiệu suất như r=2^2, nhưng với ít hơn 1:28× tham số mạng. Tại r=2^2, FFN nhẹ giống như FFN trong Vaswani et al. (2017). Điều này gợi ý rằng khả năng của

[THIS IS FIGURE 8: Graph showing impact of different transformations comparing DeFINE, DeLighT, and Linear methods]

[THIS IS FIGURE 9: Graph showing impact of feature shuffling with and without shuffle]

Hình 8: Tác động của các biến đổi khác nhau. Biến đổi DeLighT hiệu quả tham số hơn so với biến đổi DeFINE và tuyến tính. Giá trị perplexity thấp hơn có nghĩa là hiệu suất tốt hơn.

Hình 9: Tác động của xáo trộn đặc trưng. Xáo trộn đặc trưng cho phép chúng ta học biểu diễn từ thông tin toàn cục và cải thiện hiệu suất. Giá trị perplexity thấp hơn có nghĩa là hiệu suất tốt hơn.

16

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
[THIS IS FIGURE 10: Graph showing reduction factor (r) vs perplexity, with parameters shown]

Hình 10: Tác động của hệ số giảm r trong FFN nhẹ. Khả năng của biến đổi DeLighT trong việc học biểu diễn trong không gian chiều cao một cách hiệu quả cho phép chúng ta giảm gánh nặng tính toán trên FFN. Giá trị perplexity thấp hơn có nghĩa là hiệu suất tốt hơn.

[THIS IS FIGURE 11: Two-part figure showing (a) comparison between block-wise and uniform scaling, and (b) performance comparison graph]

Hình 11: Thu phóng đồng nhất vs. theo khối. (a) tương phản các phương pháp thu phóng đồng nhất và theo khối. (b) so sánh kết quả của DeLighT với các phương pháp thu phóng đồng nhất và theo khối trên tập dữ liệu WikiText-103. Mạng DeLighT với thu phóng theo khối cung cấp hiệu suất tốt hơn trên các thiết lập khác nhau. Giá trị perplexity thấp hơn có nghĩa là hiệu suất tốt hơn.

biến đổi DeLighT trong việc học biểu diễn trong không gian chiều cao một cách hiệu quả cho phép chúng ta giảm gánh nặng tính toán trên FFN.

Chúng tôi cũng đã thử nghiệm loại bỏ FFN nhẹ và trong khi nó giảm tham số 0.5-1 M, hiệu suất giảm khoảng 2-3 điểm perplexity trên các thiết lập tham số khác nhau.

Thu phóng đồng nhất vs. theo khối: Hình 11 so sánh hiệu suất của DeLighT với thu phóng đồng nhất và theo khối. Đối với một chiều mô hình dm cho trước, các mô hình DeLighT với thu phóng theo khối cung cấp hiệu suất tốt hơn.

Vị trí của biến đổi DeLighT: Chúng tôi nghiên cứu ba cấu hình cho biến đổi DeLighT trên tập xác thực WikiText-103 (Bảng 8): (1) biến đổi DeLighT theo sau bởi single-headed attention và FFN nhẹ, (2) single-headed attention theo sau bởi biến đổi DeLighT, và (3) single-headed attention theo sau bởi biến đổi DeLighT và FFN nhẹ. Với số tham số tương tự, chúng tôi thấy rằng (2) và (3) giảm hiệu suất của (1) đáng kể trên các thiết lập tham số khác nhau. Điều này gợi ý rằng biểu diễn sâu và rộng hơn giúp học biểu diễn ngữ cảnh tốt hơn; cho phép chúng ta thay thế multi-headed attention bằng single-headed attention.

Thu phóng DeLighT: Hình 12 hiển thị kết quả của các mô hình DeLighT thu được sau khi thay đổi các tham số cấu hình của biến đổi DeLighT (Nmin={4, 6}, Nmax={8, 12}, wm={2, 3, 4}, và dm={256, 384, 512}). Chúng ta có thể thấy rằng thu phóng một tham số cấu hình (ví dụ, dm) trong khi giữ các tham số cấu hình khác không đổi (ví dụ, Nmin, Nmax, và wm) liên tục cải thiện hiệu suất.

17

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Tham số Cấu hình Perplexity
Biến đổi DeLighT + Single-head attention + FFN nhẹ 31 M 34.20
Single-head attention + Biến đổi DeLighT 30 M 39.02
Single-head attention + Biến đổi DeLighT + FFN nhẹ 31 M 39.43
Biến đổi DeLighT + Single-head attention + FFN nhẹ 99 M 23.16
Single-head attention + Biến đổi DeLighT 96 M 28.33
Single-head attention + Biến đổi DeLighT + FFN nhẹ 99 M 27.94

Bảng 8: Hiệu quả của vị trí biến đổi DeLighT. Giá trị perplexity thấp hơn có nghĩa là hiệu suất tốt hơn.

[THIS IS FIGURE: Four bar charts labeled (a) through (d) showing scaling up DeLighT with different parameters (Nmin=4,Nmax=8), (Nmin=6,Nmax=8), (Nmin=4,Nmax=12), and (Nmin=6,Nmax=12). Each chart shows model dimension (dm) on x-axis and perplexity on y-axis with different wm values.]

Hình 12: Thu phóng DeLighT. Thu phóng một tham số cấu hình (ví dụ, dm) trong khi giữ các tham số cấu hình khác không đổi (ví dụ, Nmin, Nmax, và wm) liên tục cải thiện hiệu suất. Các số trên đầu mỗi thanh đại diện cho tham số mạng (triệu). Giá trị perplexity thấp hơn có nghĩa là hiệu suất tốt hơn.

Công trình này nghiên cứu mối quan hệ giữa Nmin, Nmax, wm, và dm, một cách thủ công. Chúng tôi tin rằng một cách tiếp cận có nguyên tắc hơn, như thu phóng hợp chất của Tan và Le (2019), thiết lập mối quan hệ giữa các tham số này sẽ tạo ra các mô hình hiệu quả và chính xác hơn.

E MÃ NGUỒN CHO BIẾN ĐỔI TUYẾN TÍNH NHÓM
Mã nguồn để triển khai biến đổi tuyến tính nhóm (GLT) trong PyTorch được hiển thị trong Listing 1. Mã nguồn để triển khai hiệu quả hàm grouping trong GLT được hiển thị trong Listing 2. Vì kernel ungrouping tương tự như kernel grouping, chúng tôi không hiển thị nó ở đây.

Các phép toán reshape và transpose trong triển khai PyTorch thô để grouping và ungrouping được thay thế bằng các CUDA kernel chuyên dụng, dẫn đến giảm dung lượng bộ nhớ và huấn luyện nhanh hơn.

18

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2021
Listing 1: "Triển khai thô của GLT trong Pytorch"
import torch
def glt_function(x, n_groups, weights, bias=None):
    '''
    :param x: Tensor đầu vào có kích thước [B x N], trong đó B là kích thước batch và N
    là chiều đầu vào
    :param n_groups: số nhóm trong GLT
    :param weights: trọng số glt [g x N/g x M/g]
    :param bias: GLT bias (tùy chọn) có kích thước [g x 1 x M/g]
    :return: tensor đầu ra có kích thước [B x M]
    '''
    bsz = x.size(0)
    ## HÀM GROUPING: Chuyển đổi tensor [B x N] thành [g x B x N/g] ##
    # [B x N] --> [B x g x N/g]
    x = x.contiguous().view(bsz, n_groups, -1)
    # [B x g x N/g] --> [g x B x N/g]
    x = x.transpose(0, 1) # chuyển vị để nhóm đứng trước
    ## HÀM BIẾN ĐỔI: Biến đổi từ không gian N/g-chiều thành
    không gian M/g-chiều ##
    # [g x B x N/g] x [g x N/g x M/g] --> [g x B x M/g]
    x = torch.bmm(x, weights) # nhân với Trọng số
    # thêm bias
    if bias is not None:
        x = torch.add(x, bias)
    ## HÀM REGROUPING: Chuyển đổi tensor [g x B x M/g] thành [B x M] ##
    # [g x B x M/g] --> [B x g x M/g]
    x = x.transpose(0, 1) # chuyển vị để batch đứng trước
    # [B x g x M/g] --> [B x M]
    x = x.contiguous().view(bsz, -1)
    return x

Listing 2: "Kernel grouping trong CUDA"
/*Kernel Grouping: Biến đổi đầu vào từ [B x N] thành [g x B x N/g] */
template<typename scalar_t>
__global__ void grouping_kernel_forward(const scalar_t *input,
                                      const int groups, const int total_elements,
                                      const int input_features, const int group_features,
                                      const int batch_size, scalar_t *output){
    const int index = IMUL(blockIdx.x, blockDim.x) + threadIdx.x;
    if (index >= total_elements){
        return;
    }
    const int b_idx = index / group_features;
    const int g_f_idx = (index % group_features);
    int in_offset, out_offset;
    #pragma unroll
    for(int g=0; g < groups; g++){
        in_offset = (b_idx *input_features) + (g *group_features) +
        g_f_idx;
        out_offset = ((g *batch_size + b_idx) *group_features) + g_f_idx;
        output[out_offset] = input[in_offset];
    }
}

19
