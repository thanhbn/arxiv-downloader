# Block-State Transformers
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/llm-architecture/2306.09539.pdf
# File size: 3812582 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Block-State Transformers
Mahan Fathi123∗Jonathan Pilault124∗
Orhan Firat1Christopher Pal24Pierre-Luc Bacon23Ross Goroshin1
1Google DeepMind2Mila3Université de Montréal4Polytechnique Montréal
Tóm tắt
Các mô hình không gian trạng thái (SSM) đã cho thấy kết quả ấn tượng trên các nhiệm vụ yêu cầu
mô hình hóa các phụ thuộc tầm xa và mở rộng hiệu quả đến các chuỗi dài nhờ vào độ phức tạp thời gian
chạy dưới bậc hai. Ban đầu được thiết kế cho tín hiệu liên tục, SSM đã cho thấy hiệu suất vượt trội trên
nhiều nhiệm vụ, trong thị giác và âm thanh; tuy nhiên, SSM vẫn kém hơn hiệu suất Transformer trong
các nhiệm vụ Mô hình hóa Ngôn ngữ. Trong nghiên cứu này, chúng tôi đề xuất một lớp kết hợp có tên
Block-State Transformer (BST), kết hợp nội bộ một sublayer SSM cho ngữ cảnh hóa tầm xa, và một
sublayer Block Transformer cho biểu diễn ngắn hạn của chuỗi. Chúng tôi nghiên cứu ba biến thể khác
nhau và hoàn toàn có thể song song hóa, tích hợp SSM và attention theo từng khối. Chúng tôi cho thấy
mô hình của chúng tôi vượt trội hơn các kiến trúc dựa trên Transformer tương tự về perplexity mô hình
hóa ngôn ngữ và tổng quát hóa đến các chuỗi dài hơn. Ngoài ra, Block-State Transformer thể hiện tốc
độ tăng hơn mười lần ở cấp độ lớp so với Block-Recurrent Transformer khi sử dụng song song hóa mô hình.

1 Giới thiệu
Transformer đã cho thấy hiệu suất ấn tượng trên một loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên
(NLP). Mặc dù chúng chủ yếu được sử dụng cho mô hình hóa ngôn ngữ, kiến trúc Transformer
[40] cũng đã được áp dụng thành công cho các nhiệm vụ khác ngoài NLP và phần lớn đã thay thế
Mạng Neural Hồi quy (RNN). Một số yếu tố góp phần vào thành công này, bao gồm hiệu quả tính
toán và các bias quy nạp kiến trúc phù hợp cho huấn luyện trên các nhiệm vụ ngôn ngữ tự nhiên
ở quy mô lớn. Về mặt tính toán, Transformer có thể xử lý các token của một chuỗi đầu vào nhất định
song song, tận dụng tối đa phần cứng accelerator hiện đại. Hơn nữa, cơ chế attention cho phép
Transformer tìm ra mối quan hệ trong các chuỗi dài hơn bằng cách cung cấp khả năng truy cập sẵn
sàng đến tất cả thông tin được trích xuất từ các token trước đó khi suy luận token tiếp theo. So với
RNN và LSTM [19], lợi ích của self-attention có hai mặt: (i) khả năng lưu trữ và truy cập trực tiếp
làm ngữ cảnh được tăng lên đáng kể, và (ii) huấn luyện trên các chuỗi dài hơn ổn định hơn [18, 23].

Với những thành tựu đáng chú ý của Transformer trong các nhiệm vụ mô hình hóa ngôn ngữ, và hiệu
suất cải thiện của chúng ở quy mô lớn trên các nhiệm vụ NLP khó như lý luận và trả lời câu hỏi [2,39,6],
nhu cầu triển khai các mạng thậm chí sâu hơn và lớn hơn lớn hơn bao giờ hết. Một chiều mở rộng trực
giao, có thể còn quan trọng hơn, là kích thước của chuỗi đầu vào. Bất chấp một số ưu điểm của
Transformer so với RNN, việc mở rộng độ dài chuỗi đầu vào vẫn là vấn đề, một lần nữa vì cả lý do
hiệu suất tính toán và chất lượng. Hơn nữa, thời gian chạy của Transformer là bậc hai theo độ dài
chuỗi đầu vào, làm cho việc huấn luyện các mô hình này ngày càng đắt đỏ. Ngoài ra, Transformer với
attention cục bộ [8], thưa thớt [4,43,36], xấp xỉ low-rank [41] hoặc tuyến tính hóa qua phương pháp
kernel [5,22], nổi tiếng gặp khó khăn trên các nhiệm vụ phân loại đầu vào dài [37]. Transformer
vanilla có thể không ổn định khi được huấn luyện
∗Đóng góp Bằng nhau.
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2023).arXiv:2306.09539v4 [cs.CL] 30 Oct 2023

--- TRANG 2 ---
Hình 1: Lớp Block-State Transformer. Lớp BST-SH được minh họa ở bên trái, và bao gồm một mô hình không gian trạng thái (SSM, màu xanh lá) và Block Transformers (màu đỏ). Để minh họa, chuỗi được chia thành 3 khối trong hình. Chi tiết của sublayer Block Transformer ở bên phải. *TRF = Transformer.

trên các chuỗi dài [26] và tầm quan trọng của token tập trung trong một trường tiếp nhận cục bộ khoảng 50 token xung quanh bước thời gian hiện tại [35].

Một nhóm nghiên cứu mới nổi cho thấy rằng Mô hình Không gian Trạng thái (SSM) có thể phục vụ như một sự thay thế cho Transformer vì chúng có thể nắm bắt các phụ thuộc trong các chuỗi cực dài, trong khi hiệu quả tính toán hơn và có thể song song hóa [14]. Mặc dù vẫn thuộc về danh mục các mô hình chuỗi tự hồi quy, hệ thống động tuyến tính bất biến thời gian cơ bản của SSM cho phép xử lý hiệu quả các chuỗi bằng cách sử dụng các toán tử tích chập có thể song song hóa với Biến đổi Fourier Nhanh (FFT) [7], với độ phức tạp O(LlogL), trong đó L là độ dài của chuỗi. Hơn nữa, việc giữ lại thông tin quá khứ trên các chuỗi dài, lên đến hàng nghìn bước, có thể được đảm bảo bằng cách suy ra các quy tắc cập nhật hồi quy bằng cách mượn ý tưởng từ xấp xỉ hàm trực tuyến [3,12].

SSM gần đây đã vượt trội hơn Transformer trên các benchmark phụ thuộc tầm xa với biên độ lớn [37]. Bất chấp thành công của chúng trên các nhiệm vụ phân loại tầm xa, SSM chưa hoàn toàn phù hợp với Transformer như một mô hình chuỗi sẵn có cho các nhiệm vụ mô hình hóa ngôn ngữ chung [10]. Các phát hiện gần đây cho thấy rằng Transformer và SSM là các mô hình bổ sung cho mục đích mô hình hóa ngôn ngữ [28]. Trong nghiên cứu này, chúng tôi đề xuất một kiến trúc tích hợp bias quy nạp attention cục bộ mạnh với khả năng mô hình hóa ngữ cảnh dài hạn của SSM thành một lớp duy nhất, mà chúng tôi gọi là Block-State Transformer (BST). Mô hình của chúng tôi có thể xử lý các chuỗi đầu vào dài, trong khi vẫn kết hợp cơ chế attention để dự đoán token tiếp theo. BST hoàn toàn có thể song song hóa, mở rộng đến các chuỗi dài hơn nhiều, và cung cấp tốc độ tăng 10× so với các lớp dựa trên Transformer tương đương.

Trong mỗi lớp BST, một SSM nhận toàn bộ chuỗi làm đầu vào và ánh xạ nó thành một chuỗi "ngữ cảnh" có cùng độ dài. Sublayer SSM tận dụng các tích chập dựa trên FFT. Chuỗi ngữ cảnh này sau đó được chia thành các khối có kích thước bằng nhau, tức là độ dài cửa sổ (W), và mỗi khối ngữ cảnh sau đó được đưa vào một lớp Block Transformer, attend đến các subsequence có kích thước W như được định nghĩa trong [21]. Khối embedding token đầu vào sau đó được cross-attend đến khối trạng thái ngữ cảnh tương ứng; xem Hình 1. Lưu ý rằng bằng cách giới thiệu SSM như một phương tiện ngữ cảnh hóa, chúng tôi hoàn toàn loại bỏ nhu cầu đối với các recurrence tuần tự và chúng tôi có thể chạy lớp SSM-Transformer kết hợp của chúng tôi hoàn toàn song song. Độ phức tạp thời gian chạy kết quả có thể được biểu diễn như tổng của O(W2) + O(LlogL), trong đó số hạng đầu tiên đại diện cho độ phức tạp thời gian của sublayer Transformer, trong khi số hạng thứ hai đại diện cho độ phức tạp thời gian của sublayer SSM. Đây là một cải tiến lớn so với O(LW) của Block-Recurrent Transformer, miễn là có phần cứng hỗ trợ tính toán song song. Hơn nữa, do các hạn chế áp đặt bởi phần cứng, độ phức tạp thời gian chạy của SSM trên một chuỗi đầy đủ có thể so sánh với Block Transformer trên một khối token, điều này ngụ ý thêm sự vắng mặt của bottleneck tốc độ trong lớp BST, được xác thực thực nghiệm cho các chuỗi chứa hàng trăm nghìn token. Điều này rõ ràng bằng cách quan sát rằng hai đường dưới cùng ở bên trái của Hình 4 gần như chồng lên nhau.

2

--- TRANG 3 ---
2 Nghiên cứu liên quan
Nghiên cứu này chủ yếu liên quan đến hai nhánh nghiên cứu gần đây: (i) kết hợp attention cục bộ với mạng hồi quy để mở rộng khả năng nắm bắt các phụ thuộc tầm xa của chúng, vượt ra ngoài độ dài kích thước cửa sổ attention, và (ii) Mô hình Không gian Trạng thái (SSM) mô tả các chuỗi thông qua hệ thống động tuyến tính có đầu ra có thể được tính toán song song. Block-Recurrent Transformer (BRECT) [21] sử dụng cơ chế bộ nhớ hồi quy để mở rộng độ dài ngữ cảnh lý thuyết của Transformer. Trong đơn vị hồi quy của cell BRECT, các cập nhật được thực hiện đối với "các vector trạng thái hồi quy" được trích xuất bằng cách sử dụng cơ chế cross-attention trên một khối/cửa sổ của token embedding đầu vào. Khác với nghiên cứu của họ, chúng tôi sử dụng các mô hình không gian trạng thái tuyến tính thay vì các cell hồi quy để duy trì trạng thái ngữ cảnh. Chúng tôi cũng tiến hành khám phá rộng rãi hơn về việc duy trì và cập nhật trạng thái ngữ cảnh. Các nghiên cứu trước đó tăng cường transformer với bộ nhớ ngoài không thể vi phân bao gồm Memorizing Transformer [42]. Transformer-XL [8] là một nghiên cứu sớm kết hợp bộ nhớ hồi quy với Transformer. Nghiên cứu của chúng tôi có thể được xem như một sự tiến hóa tiếp tục của những mô hình đó kết hợp các mô hình bộ nhớ hồi quy hiện đại lấy cảm hứng từ SSM.

Các mô hình không gian trạng thái có thể được coi là RNN tuyến tính [12]. Sự đơn giản này tạo điều kiện cho việc phân tích chúng và thậm chí cho phép suy ra phân tích các trọng số hồi quy để biểu diễn tối ưu các chuỗi dài tùy ý. Tính chất tuyến tính cũng cho phép recurrence được mở ra và song song hóa trong quá trình huấn luyện và suy luận [14]. Nghiên cứu của chúng tôi kết hợp các mô hình hiện đại này, cho phép Transformer tận dụng ngữ cảnh vô hạn về mặt lý thuyết.

Các nghiên cứu khác đã cố gắng thay thế Transformer và cơ chế attention của chúng bằng SSM [28,27,10,30], tuy nhiên bất chấp tiến bộ gần đây, hiệu suất đạt được bởi kiến trúc Transformer vẫn không có gì sánh được trong ngôn ngữ. Tuy nhiên, SSM có thể nắm bắt các phụ thuộc tầm xa hơn so với Transformer trong cả lý thuyết và thực tế, đồng thời cũng có tính song song cao [7,11]. Do đó chúng tôi chọn kết hợp các khía cạnh tốt nhất của SSM và Transformer thành một mô hình duy nhất.

Ý tưởng giao tiếp qua các khối, tương tự như GSS [28], sau đó được triển khai bởi MEGA [27], thông qua quy tắc cập nhật Exponentially Moving Average (EMA) thay vì SSM2. Tuy nhiên, cả GSS và MEGA đều sử dụng Gated Attention Unit (GAU) đơn head [20]. MEGA tiếp tục trộn đầu vào lớp, đầu ra GAU và đầu ra EMA thông qua hai cơ chế gating. Phương pháp của chúng tôi sử dụng kiến trúc đơn giản hơn để trộn tín hiệu từ attention cục bộ và đầu ra SSM thông qua cross-attention, cho phép chúng tôi cắm bất kỳ SSM hoặc lớp attention sẵn có nào. Hơn nữa, chúng tôi điều tra ba cách để trộn tín hiệu SSM với attention như được nêu trong Phần 3.3.

3 Phương pháp
Chúng tôi xem xét bài toán dự đoán token tiếp theo thông qua mô hình ngôn ngữ chỉ decoder. Nhiệm vụ pretext đơn giản có vẻ này đã dẫn đến tiến bộ ngoạn mục trong hiểu biết ngôn ngữ [9,2,29]. Trong quá trình huấn luyện, decoder nhận một chuỗi có độ dài L của token embedding và được giao nhiệm vụ tạo ra token tiếp theo ở mỗi bước trong chuỗi.

Chúng tôi bắt đầu bằng một đánh giá ngắn gọn về SSM rất cần thiết để hiểu lớp Block-State Transformer (3.1). Kiến trúc Block-State Transformer đầy đủ của chúng tôi được nêu trong Phần 3.2. Phần 3.3 mô tả ba phương pháp để tích hợp trạng thái SSM vào cơ chế attention. Các chi tiết triển khai quan trọng được mô tả trong Phần 3.4.

3.1 Kiến thức cơ bản về Không gian Trạng thái
Các mô hình không gian trạng thái có thể được chia thành hai loại:

Không gian Trạng thái: Structured Kernels S4[14], S5[34], S4D [15], DSS [16], tuân theo khởi tạo có cấu trúc của kernel tích chập bằng cách mở ra một hệ thống động tuyến tính bất biến thời gian (LTI) có dạng sau:

xk=Axk−1+Buk,
yk=Cxk+Duk.(1)

2Các tác giả trong [27] cho thấy một dạng toán học của EMA có chuyển đổi trạng thái và cũng suy ra một kernel tích chập để tính toán EMA hiệu quả tương tự như S4.

3

--- TRANG 4 ---
Hệ thống được tham số hóa bởi ma trận trạng thái A∈RN×N, vector B∈RN×1, C∈R1×N, và D∈R1×1, SSM ánh xạ tín hiệu đầu vào 1-D uk, thành tín hiệu đầu ra 1-D yk. Bên trong, SSM chiếu tín hiệu đầu vào thành trạng thái biểu diễn N-D xk, trước khi ánh xạ nó xuống thành một số vô hướng sử dụng ma trận C. Số hạng Duk có thể được coi như một kết nối skip và sẽ được bỏ qua cho phần còn lại của thảo luận để tiện lợi. Đầu ra của phương trình hồi quy trên, yk, có thể được tính toán như một tích chập rời rạc, bằng cách nhận ra rằng recurrence có thể được mở ra một cách rõ ràng:

Đặt x−1:=⃗0,
yk=kX
j=0CAjB·uk−j.(2)

Các entry CAkB được thu thập để tạo ra kernel SSM K∈RL, và tích chập có thể được biểu diễn như:

K= (CB,CAB , . . . ,CAL−1B),
yk=kX
j=0Kj·uk−j, y =K∗u .(3)

Cho một chuỗi đầu vào u∈RL, có thể tính toán đầu ra y∈RL tuần tự thông qua recurrence trong Phương trình (1). Mặc dù tính chất này hữu ích cho giải mã tự hồi quy, tính toán tuần tự chậm một cách đáng kiểm soát để huấn luyện với đầu vào dài và, thay vào đó, tích chập từ Phương trình (3) có thể được sử dụng để tính toán tất cả các phần tử của y song song. Điều này được thực hiện thông qua Biến đổi Fourier Nhanh (FFT) [7], với điều kiện chúng ta đã tính toán K.

Các bias quy nạp bổ sung đã được áp đặt lên SSM bằng cách suy ra phân tích các biểu thức dạng đóng cho các ma trận A và B sử dụng framework HiPPO [12]. Trong framework này, trạng thái xt đại diện cho các hệ số của đa thức xấp xỉ chuỗi ut.

Bộ lọc Tham số hóa Rõ ràng Trái ngược với structured kernel, người ta có thể tham số hóa kernel tích chập, như các trọng số có thể huấn luyện và tối ưu hóa chúng, ¯K∈RL. Tuy nhiên, điều này sẽ dẫn đến hiệu suất kém trừ khi áp dụng các loại regularization nhất định lên kernel. [11] đơn giản sử dụng việc nén các trọng số kernel, và sau đó áp dụng kỹ thuật làm mượn. Kernel có thể huấn luyện cũng được sử dụng trong các mô hình thay thế attention cho Transformer, như Hyena [30], bao gồm việc giảm mũ các trọng số dọc theo kernel:

¯Kt=e−αt·FFN◦PositionalEncoding(t), (4)

trong đó ¯Kt là một entry trong bộ lọc tại vị trí t, và FFN là mạng feed-forward được sử dụng để tách biệt số lượng tham số khỏi độ dài chuỗi.

3.2 Lớp Block-State Transformer (BST)
Bây giờ chúng tôi giới thiệu lớp Block-State Transformer, kết hợp SSM với Block Transformer. Tại mỗi lần lặp huấn luyện, một chuỗi L token, được lấy mẫu từ một tài liệu dài hơn. Các token sau đó được nhúng và đưa vào mô hình. Mô hình của chúng tôi bao gồm một chồng các lớp Block-State Transformer. Mỗi lớp BST tùy chọn bao gồm một sublayer SSM chịu trách nhiệm cung cấp ngữ cảnh tầm xa cho lớp Block Transformer, hoạt động tương tự như một cell Block-Recurrent Transformer (BRECT). Sublayer SSM nhận chuỗi token embedding từ lớp trước làm đầu vào, và tạo ra một chuỗi có cùng độ dài L làm đầu ra.

Đầu ra của SSM được mã hóa ngữ cảnh, có nghĩa là các entry tại mỗi bước thời gian, có thể bao gồm thông tin về tất cả các bước thời gian đi trước các phần tử trong chuỗi. Chúng tôi thu thập một số "trạng thái ngữ cảnh", S, từ chuỗi ngữ cảnh, và chúng tôi đặt S≪L. Để ngăn mô hình truy cập thông tin tương lai, chúng tôi chỉ cho phép mô hình truy cập các trạng thái ngữ cảnh đi trước token hiện tại. Các cách khác nhau để thu thập trạng thái ngữ cảnh từ chuỗi ngữ cảnh được thảo luận chi tiết trong phần 3.3.

Các trạng thái ngữ cảnh được đưa vào Block Transformer, thay cho những gì được gọi là "vector trạng thái hồi quy" trong Block-Recurrent Transformer [21]. Các hoạt động tiếp theo, được hiển thị ở phía bên phải của

4

--- TRANG 5 ---
Hình 2: Tóm tắt các phương pháp của chúng tôi. Phía bên trái cho thấy các trường hợp mà SSM được yêu cầu xuất ra ngữ cảnh Multi-Head (MH). Ở bên phải, phương pháp Multi-Filter (MF) được mô tả nơi các entry cuối cùng từ cửa sổ trước được nối thành một tập các trạng thái ngữ cảnh có kích thước S. Các đường đứt nét đại diện cho khối hiện tại.

Hình 1, được giữ nguyên, ngoại trừ chúng tôi không còn cần chạy đơn vị hồi quy của cell BRECT vì chúng tôi đang duy trì ngữ cảnh thông qua một SSM. Ngoài các trạng thái ngữ cảnh, Block Transformer cũng nhận một khối/cửa sổ có độ dài W của token embedding làm đầu vào, được cross-attend đến các trạng thái ngữ cảnh. Đầu ra của hoạt động cross-attention sau đó được nối với đầu ra của self-attention trên các embedding đầu vào, theo sau bởi một phép chiếu đơn giản.

Ngoài khả năng của SSM để giữ lại thông tin trong các khoảng thời gian dài hơn so với Transformer và RNN, việc sử dụng SSM để duy trì trạng thái ngữ cảnh như một sự thay thế cho các cell hồi quy làm cho lớp hiệu quả tính toán hơn. Loại bỏ recurrence bằng cách tích hợp SSM vào các lớp Transformer, cho phép lớp Block-State Transformer hoàn toàn có thể song song hóa, trong khi kiến trúc Block-Recurrent xử lý các khối token tuần tự bằng một vòng lặp for.

3.3 Trạng thái Ngữ cảnh
Mặc dù đầu ra SSM mới nhất về mặt kỹ thuật chứa thông tin về toàn bộ chuỗi, việc truy xuất các token riêng lẻ chỉ từ trạng thái cuối cùng có thể không khả thi. Để bù đắp, chúng tôi nối một chuỗi các trạng thái, tương ứng với khối token mới nhất. Điều này cũng tương tự với phương pháp được áp dụng bởi BRECT. Biểu diễn này đảm bảo khả năng truy xuất và dễ dàng truy cập, thông qua sự dư thừa. Nó dư thừa vì các trạng thái liền kề có tương quan cao, tuy nhiên điều này cũng làm cho việc dễ dàng khôi phục khối token hiện tại trở nên khả thi, nếu cần thiết.

Trong phương pháp của chúng tôi, các trạng thái ngữ cảnh được xây dựng từ đầu ra của SSM và đưa vào các head attention của Transformer. Các trạng thái ngữ cảnh này có thể được xây dựng theo nhiều cách khác nhau. Để hướng dẫn các quyết định thiết kế này, chúng tôi xem xét mỗi scheme được đề xuất dưới đây như việc giới thiệu khả năng truy xuất với chi phí của sự dư thừa. Hình dạng đầu ra của một lớp SSM đơn là (B×L×D), trong đó B là kích thước batch, L là số token được xử lý, và D là chiều embedding. Khi thực hiện cross-attention trong cell Transformer với H head khác nhau, tensor này cần được chuyển đổi thành tensor ngữ cảnh có hình dạng (B×S×D×H), trong đó S là số trạng thái ngữ cảnh; chúng tôi thường đặt S≪L và S=W tương tự như Block-Recurrent Transformer (BRECT).

Bây giờ chúng tôi thảo luận ba phương pháp khác nhau mà chúng tôi đánh giá để tạo ra tensor ngữ cảnh cho mỗi chuỗi khối:

SH: Single-Head Phương pháp đầu tiên xây dựng tensor ngữ cảnh bằng cách nối tuần tự S trạng thái từ SSM với một bộ lọc duy nhất (mỗi cái có kích thước D). Lưu ý rằng vì SSM nắm bắt thông tin từ các khối trước, trạng thái ngữ cảnh cũng nắm bắt thông tin về các khối đi trước khối hiện tại. Vector ngữ cảnh kết quả có khả năng truy xuất cao và dư thừa, như được định nghĩa ở trên. Như trong Transformer điển hình, các lớp kết nối đầy đủ được sử dụng để chiếu mỗi vector ngữ cảnh thành H head khác nhau có kích thước D. Lưu ý rằng trong hoạt động cross-attention, các trạng thái ngữ cảnh tương ứng với các token tương lai từ khối hiện tại cần được che causally. Trong trường hợp này chúng tôi đặt S=W, và chúng tôi chọn cửa sổ đầu ra SSM tương ứng với khối hiện tại, và một mask tam giác được sử dụng để triển khai causal masking của các trạng thái ngữ cảnh. Phương pháp này được hiển thị trong Hình 1.

MH: Multi-Head Phương pháp này khác với Single-Head (SH) ở chỗ ở đây SSM được giao nhiệm vụ tạo ra đầu ra riêng biệt cho các head khác nhau. Chúng tôi sử dụng các ma trận [C1,C2, ...,CH] riêng biệt, để tạo ra các trạng thái ngữ cảnh được đưa vào các head attention. Điều này cho phép SSM trích xuất các đặc trưng bổ sung từ lịch sử tóm tắt. Sự khác biệt khái niệm là ma trận C, từ Phương trình (1), có quyền truy cập trực tiếp vào trạng thái bộ nhớ đầy đủ của SSM (xk), mà về mặt lý thuyết có thể được coi như một biểu diễn compact của lịch sử, trước khi nó được ánh xạ xuống thành một số vô hướng. Phương pháp Multi-Head (MH) được minh họa ở phía bên trái của Hình 2. Vì H ma trận C khác nhau có thể trích xuất thông tin bổ sung, vector ngữ cảnh được xây dựng bởi phương pháp này về mặt lý thuyết ít dư thừa hơn so với phương pháp single-head được mô tả ở trên.

MF: Multi-Filter Trong phương pháp này, sublayer SSM tạo ra S trạng thái ngữ cảnh, mà chúng tôi đặt độc lập với W. Điều này được thực hiện bằng cách tích chập chuỗi embedding với S kernel/bộ lọc khác nhau. Đầu ra của mỗi hoạt động tích chập, tương ứng với một bộ lọc cụ thể, là tensor có hình dạng (B×L×D). Sau khi tích chập đầu vào với tất cả các bộ lọc, các trạng thái ngữ cảnh có kích thước D tương ứng với token cuối cùng từ cửa sổ trước được xếp chồng lại với nhau để tạo tensor (B×S×D). Mạng feed forward sau đó được sử dụng để nâng tensor này lên các head khác nhau, (B×S×D×H). Khác với hai phương pháp trước, ngữ cảnh được hình thành bằng cách chỉ lấy S trạng thái ngữ cảnh cuối cùng, từ cửa sổ trước, được xuất ra bởi S SSM. Ngữ cảnh ít dư thừa hơn vì nó không còn bao gồm các trạng thái SSM liền kề. Vì ngữ cảnh được lấy từ các entry của cửa sổ trước, cross-attention masking không còn cần thiết, như được hiển thị ở bên phải của Hình 2.

Trạng thái bộ nhớ của phương pháp Multi-Filter (MF) ít dư thừa nhất, trong khi Multi-Head (MH) tạo ra điểm giữa, và Single-Head (SH) có nhiều dư thừa nhất. Việc tích hợp sự dư thừa trong các phương pháp này nhằm tạo điều kiện cho khả năng truy xuất ngữ cảnh gần đây nhất được nắm bắt bởi SSM, mặc dù với chi phí có thể sử dụng không hiệu quả khả năng mạng. Phương pháp cuối cùng đạt được sử dụng cao nhất, vì cross-attention được thực hiện trong không gian của các đặc trưng duy nhất được trích xuất bởi các bộ lọc chuyên biệt.

3.4 Chi tiết Triển khai
Context ID & Positional Embedding Để cho phép phân biệt giữa các entry được cung cấp cho cơ chế attention, một positional embedding thường được thêm vào các đầu vào. Khi sử dụng phương pháp Multi-Filter (MF), các trạng thái ngữ cảnh được thu thập tương ứng với các đặc trưng khác nhau được trích xuất từ chuỗi, do đó chúng tôi thêm một tập các "context ID" học được duy nhất vào các trạng thái ngữ cảnh, trước khi sử dụng chúng làm đầu vào cho cross-attention. Tuy nhiên, trong các trường hợp mà các trạng thái ngữ cảnh tương ứng với các bước thời gian khác nhau dọc theo chuỗi, cụ thể là các phương pháp Single-Head (SH) và Multi-Head (MH), positional encoding vốn có được tích hợp vào các trạng thái ngữ cảnh, do tính chất tăng dần của tích chập; như vậy, chúng tôi thấy việc thêm context ID là không cần thiết. Chúng tôi cũng nhận ra rằng chúng tôi không cần thêm global positional bias vào token embedding, và sử dụng T5-style relative position bias [32] thay thế, vì SSM cũng mã hóa thông tin vị trí vào ngữ cảnh.

Down-sampling Phù hợp với các phát hiện trong [28], chúng tôi thấy các hoạt động FFT là nguồn chính của bottleneck khi huấn luyện SSM trên TPU. Chúng tôi chiếu các embedding đầu vào xuống không gian chiều thấp hơn, là một phần tư kích thước embedding trong các thí nghiệm của chúng tôi, điều này giảm tổng số FFT cần thiết đi một hệ số 4. Đầu ra của SSM, tức là các trạng thái ngữ cảnh, sau đó được nâng lên kích thước embedding ban đầu trước khi được truyền đến Block Transformer.

4 Kết quả
Kết quả của chúng tôi được trình bày trong Bảng 1. Chúng tôi tiến hành thí nghiệm với BST trên ba bộ dữ liệu khác nhau, PG19, arXiv và GitHub, cho phép chúng tôi kiểm tra phương pháp của chúng tôi trên một bộ các tài liệu có độ dài khác nhau bao gồm văn bản tiếng Anh, bài báo khoa học latex và mã nguồn.

Bộ dữ liệu PG19 từ một bộ sưu tập lớn các cuốn sách đầy đủ từ Project Gutenberg [31]. Tất cả 28,602 cuốn sách được trích xuất đều được xuất bản trước năm 1919 và chứa 6,966,499 từ tiếng Anh. Khi được tokenize, mỗi cuốn sách PG19 có từ 50k-100k token. PG19 đã trở thành một

6

--- TRANG 7 ---
Bảng 1: Perplexity của mỗi mô hình. Kết quả cho XL:2048, SLIDE:12L và BRECT:FIXED:SKIP từ [21] bằng cách chuyển đổi log2 của perplexity sang perplexity thô. Hiệu suất GSS-HYBRID-L được lấy từ [28]. Kết quả với ± là điểm trung bình và thanh lỗi của các lần chạy với ba seed ngẫu nhiên khác nhau. Với ngân sách tính toán nhỏ hơn, BST cung cấp cải thiện perplexity nhỏ so với BRECT trên PG19 và GitHub. Với cùng ngân sách tính toán, BST vượt trội hơn GSS-HYBRID-L trên các bộ dữ liệu từ 1.5% đến 4%.

[THIS IS TABLE: A detailed comparison table showing model performance across different datasets (PG19, arXiv, GitHub) with various parameters including sequence length, window length, number of parameters, TPUv4 hours, and perplexity scores for different model variants like SLIDE, TRSF-XL, BRECT, BST variants, and GSS-HYBRID]

benchmark phổ biến để đo lường tiến bộ về hiệu suất mô hình hóa ngôn ngữ tầm xa. Chúng tôi báo cáo hiệu suất đánh giá split "test".

Bộ dữ liệu arXiv là một corpus chứa các bài báo khoa học và kỹ thuật về chủ đề Toán học [42]. Bộ dữ liệu arXiv chứa mã nguồn latex cũng như các mục như định lý, trích dẫn, định nghĩa được tham chiếu và thảo luận trên các phạm vi dài của văn bản. Sử dụng cùng từ vựng như trong [42] và [21] để so sánh công bằng, nhiều ký tự đặc biệt được chia thành các subword nhỏ. Kết quả là, số token mỗi bài báo trong bộ dữ liệu arXiv xấp xỉ bằng số token mỗi cuốn sách trong PG19. Chúng tôi báo cáo perplexity trên split "test".

Bộ dữ liệu GitHub [42] là bộ dữ liệu lớn nhất trong ba bộ và được tập hợp bằng cách trích xuất các repository mã GitHub với giấy phép mã nguồn mở. Các file được lọc để chỉ chứa các ngôn ngữ lập trình sau: C, C++, Java, Python, Go và Typescript. Mặc dù các file mã tương đối nhỏ, có nhiều phụ thuộc import giữa mỗi file. Bằng cách duyệt cây thư mục và nối tất cả các file mã dọc theo đường dẫn, một tài liệu duy nhất bảo tồn cấu trúc và phụ thuộc của repository được tạo ra. Chúng tôi báo cáo hiệu suất trên split "validation".

Để so sánh công bằng với các baseline, chúng tôi giữ từ vựng nhất quán như được sử dụng bởi [21] và [28]. Cụ thể, chúng tôi sử dụng vocab T5 được pre-train với 32k token cho PG19 [33] và vocab LaMDA với 32k token [39] cho cả bộ dữ liệu arXiv và GitHub. Do thời gian huấn luyện dài và số lượng thí nghiệm lớn, chúng tôi chỉ cung cấp thanh lỗi cho các mô hình PG19 ∼200M tham số bằng cách chạy các mô hình của chúng tôi với ba seed ngẫu nhiên khác nhau. Thanh lỗi BRECT:FIXED:SKIP từ [21].

4.1 So sánh Baseline và Mô hình của chúng tôi
Chúng tôi thí nghiệm ba loại mô hình Block-State Transformer (BST) khác nhau: BST-SH, BST-MH và BST-MF như được mô tả trong Phần 3.3. Các mô hình của chúng tôi không sử dụng global learned positional embedding mà mã hóa nhận thức vị trí với một SSM ở lớp đầu tiên, ngay sau lớp word embedding. Chúng tôi tổ chức các mô hình thành hai nhóm: (i) kích thước cửa sổ cố định có kích thước cửa sổ huấn luyện 512 hoặc 2048 token; và (ii) số lượng tham số cố định có tổng cộng ∼200M hoặc ∼400M tham số. Chúng tôi chạy thí nghiệm với hai loại SSM:

BST:{SH,MH,MF}:S4 mã hóa ngữ cảnh dài bằng Structured State Space Model (S4) [16]. Như được mô tả trong Phương trình (3), ma trận kernel S4 K được biên dịch từ các ma trận A, B và C và độc lập với độ dài của chuỗi đánh giá đầu vào. Chúng tôi cho thấy rằng tham số hóa có cấu trúc của K cho phép các mô hình BST của chúng tôi tổng quát hóa đến độ dài dài hơn. Chúng tôi giới thiệu người đọc đến phần 4.2 cho kết quả về tổng quát hóa độ dài. Chúng tôi chỉ chạy một BST:MH sử dụng S4 vì mô hình yêu cầu 8% tham số nhiều hơn trong khi hiệu suất ngang bằng với biến thể BST:SH nhanh hơn. BST:MF cũng có 8% tham số nhiều hơn nhưng hoạt động tốt hơn trên arXiv và GitHub so với SH. Thú vị, SH hoạt động tốt hơn MF trên PG19, một bộ dữ liệu mà ngữ cảnh cục bộ quan trọng hơn để dự đoán token tiếp theo so với arXiv và GitHub. Chúng tôi cho rằng điều này có thể do khả năng của mô hình SH để truy xuất ngữ cảnh gần đây nhất được nắm bắt bởi SSM.

BST:{SH,MF}:UNSTRUCT dựa trên các bộ lọc tích chập tham số hóa không có cấu trúc, lấy cảm hứng từ kernel tích chập Hyena Hierarchies [30]. Chúng tôi loại trừ việc sử dụng cơ chế gating nhân được sử dụng trong Hyena Hierarchies và chỉ áp dụng các regularization được triển khai trên kernel tham số hóa, được ký hiệu là ¯K trong Phương trình (4). Công thức này có hai lợi thế quan trọng so với S4: (1) kernel ¯K không cần được biên dịch lại, cho phép tăng tốc khi sử dụng nhiều bộ lọc; (2) ¯K có nhiều tham số tự do hơn vì nó không còn bị hạn chế bởi các ma trận A, B trong phương trình 3, có thể cung cấp biểu diễn phong phú hơn có thể giải thích điểm perplexity cải thiện so với các biến thể S4. Tuy nhiên, kernel UNSTRUCT ¯K dựa vào positional encoding đã học làm cho phương pháp ít có thể mở rộng đến các chuỗi có độ dài lớn hơn tại thời điểm suy luận.

Chúng tôi so sánh Block-State Transformer với bốn baseline khác nhau:

TRSF-XL:2048 [8] là một Transformer với kích thước cửa sổ huấn luyện 2048. Như mong đợi, tăng kích thước cửa sổ cải thiện perplexity, đặc biệt trên các bộ dữ liệu arXiv và GitHub. Tuy nhiên, mô hình này hoạt động tệ hơn BST:SH:HYENA trên PG19 và chậm hơn nhiều, bị bottleneck bởi lớp attention trên độ dài chuỗi cao hơn.

SLIDE:12L [21] Mô hình này gần như giống hệt TRSF-XL:2048. Tuy nhiên nó sử dụng cửa sổ trượt có kích thước 512 trên một đoạn 4096 token. Cửa sổ trượt có thể vi phân qua hai khối, trong khi TRSF-XL không backpropagate qua các key và value được cache từ cửa sổ trước. Baseline đơn giản này gần nhất về tốc độ huấn luyện với BST:SH. Điểm perplexity cho thấy rằng việc tích hợp biểu diễn của quá khứ, như với BRECT và BST, tác động tích cực đến hiệu suất LM.

BRECT:FIXED:SKIP [21] là kiến trúc Block-Recurrent Transformer hoạt động mạnh nhất và nhanh nhất trong [21]. Kiến trúc này rất tương tự với SLIDE:12L. Tuy nhiên có cấu hình "skip" hồi quy tuần tự, một cơ chế gating lớp tuyến tính đơn giản kết hợp biểu diễn ẩn khối hiện tại với thông tin quá khứ từ các khối trước.

GSS-HYBRID-L[28] là mô hình kết hợp SSM-Transformer gần nhất được kiểm tra trên các nhiệm vụ mô hình hóa ngôn ngữ tầm xa. GSS-HYBRID-L dựa trên Diagonal State Space (DSS) [16]. DSS và S4 tương tự về hiệu suất và kiến trúc, chỉ khác nhau về khởi tạo kernel K[15]. [16] tiếp tục cải thiện DSS cho các nhiệm vụ LM bằng cách giới thiệu phiên bản Gated State Space gọi là GSS, hoạt động tốt hơn trên PG19, arXiv và GitHub. Không giống phương pháp của chúng tôi, GSS-HYBRID-L không tích hợp trực tiếp trạng thái SSM vào cơ chế attention mà chỉ xen kẽ 32 lớp GSS với các lớp Transformer. Cần lưu ý rằng điểm GSS-HYBRID-L được thu được sau khi tìm kiếm lưới trên bốn learning rate {6.4,3.2,1.6,0.8} ×10−3 và sử dụng learning rate và weight decay khác nhau cho lớp SSM và lớp Transformer để tránh bất ổn huấn luyện. Trong thí nghiệm của chúng tôi, chúng tôi không sử dụng tìm kiếm lưới và sử dụng cùng learning rate cho tất cả các lớp. Kết quả BST cho thấy rằng việc tích hợp trạng thái SSM vào attention Transformer cung cấp lợi ích lớn hơn so với việc xen kẽ các lớp SSM và attention như trong GSS-HYBRID-L.

Ngân sách tính toán cố định. Như được thấy trong Bảng 1, chúng tôi theo dõi chính xác lượng tính toán trong giờ TPUv4 đã được chi để huấn luyện mỗi mô hình. Giờ TPUv4 huấn luyện cho SLIDE:12L, TRSF-XL:2048, BRECT:FIXED:SKIP và GSS-HYBRID-L được lấy từ [28]. Metric giờ TPUv4 đo chi phí tính toán huấn luyện mô hình. Cho các thí nghiệm của chúng tôi, chúng tôi căn chỉnh thời gian huấn luyện của chúng tôi với GSS-HYBRID-L để so sánh công bằng. Các mô hình tham số nhỏ hơn đều có 12 lớp, 8 head kích thước 128, vector embedding kích thước 1024, một MLP với kích thước lớp ẩn 4096 với hàm kích hoạt ReLU. Cho các mô hình BST lớn hơn, chúng tôi nhân đôi kích thước lớp trung gian từ 4096 lên 8192 và tăng số head attention lên 12.

Chi tiết huấn luyện Chúng tôi sử dụng cùng thiết lập huấn luyện như [21] và chúng tôi thực hiện các thí nghiệm của mình bằng thư viện Meliad3 trong JAX/Flax [1,17]. Chúng tôi sử dụng optimizer Adam [25] và kích thước batch 32 và độ dài chuỗi L 4k cho huấn luyện. Sử dụng recurrence của SSM có cấu trúc (như S4) trong lớp đầu tiên cho phép chúng tôi mở rộng positional encoding đến các độ dài khác nhau tại thời điểm suy luận. Các mô hình BST nhỏ hơn có lớp Block-State tích hợp trong các lớp Transformer {1, 7, 9} và các mô hình BST lớn hơn tại các lớp {1, 5, 7, 9}. Vì các bộ dữ liệu của chúng tôi chứa các tài liệu dài, có thể huấn luyện trên độ dài chuỗi L lớn hơn. Huấn luyện trên độ dài chuỗi 4k cho phép chúng tôi kiểm tra tổng quát hóa độ dài vì kernel tích chập K trong Phương trình (3) có thể được mở rộng đến bất kỳ độ dài chuỗi L nào. Tuy nhiên, vì chúng tôi cho thấy trong Phần 4.2 rằng mô hình của chúng tôi hoạt động tốt khi được mở rộng đến các độ dài chưa thấy, chúng tôi không thấy cần thiết phải chạy các thí nghiệm đắt đỏ với độ dài chuỗi cao hơn. Cho các biến thể mô hình MF, chúng tôi giảm chiều trạng thái SSM D thêm một hệ số hai để cải thiện hiệu quả FFT. Việc giảm chiều trạng thái có tác động không đáng kể đến perplexity. Các mô hình MF có S= 32 bộ lọc trong khi các mô hình MF lớn hơn có S= 64 bộ lọc.

3https://github.com/google-research/meliad

8

--- TRANG 9 ---
4.2 Đánh giá khả năng Tổng quát hóa Độ dài
Chúng tôi trình bày phân tích tổng quát hóa độ dài và báo cáo perplexity trong Hình 3. Các mô hình và baseline của chúng tôi đều có ∼400M tham số, được huấn luyện trên độ dài chuỗi 4k và kiểm tra trên các chuỗi có độ dài thấp hơn và cao hơn {512, 16k, 65k}.

Chúng tôi nhận thấy rằng tất cả các mô hình có perplexity tương tự cho độ dài chuỗi 512. Cả BST:SH:S4-L và GSS-HYBRID-L đều tổng quát hóa tốt trên độ dài chuỗi 16k và 65k cho PG19 và GitHub. Đối với arXiv, perplexity của GSS-HYBRID-L và BST:MF:UNSTRUCT-L tăng mạnh, có thể do nhiễu trong bộ dữ liệu arXiv (như được chỉ ra bởi sự biến động trong metric perplexity theo thời gian). [28] cũng báo cáo rằng các mô hình GSS lớn hơn gặp khó khăn trong việc tổng quát hóa đến độ dài cao hơn. Thú vị, đối với arXiv một lần nữa, BRECT:FIXED:SKIP-L hoạt động rất tốt ở độ dài chuỗi cao hơn. Chúng tôi giả thuyết rằng việc truy cập của mô hình Block-Recurrent đến toàn bộ quá khứ trong quá trình huấn luyện, thông qua cache không thể vi phân của các biểu diễn qua các chuỗi, giúp giữ lại "bộ nhớ" của các phụ thuộc giữa các mục chính trong một bài báo arXiv cho phép mô hình truy cập các ký hiệu, định nghĩa, định lý hoặc phương trình quá khứ vượt ra ngoài độ dài chuỗi huấn luyện 4k. Chúng tôi cũng lưu ý rằng BST:MF:UNSTRUCT-L và BRECT:FIXED:SKIP-L vượt trội hơn các phương pháp khác trên PG19 lên đến độ dài chuỗi 16K. Hiệu suất perplexity trên PG19 có lẽ ít phụ thuộc vào mối quan hệ dài hạn giữa các token, điều này có thể giải thích hiệu suất của các mô hình không có cơ chế tích hợp rõ ràng cho tổng quát hóa độ dài.

Phân tích này cũng cho phép chúng tôi rút ra sự phân biệt rõ ràng giữa SSM có cấu trúc và không có cấu trúc được tích hợp trong các kiến trúc kết hợp. Như đã đề cập trước đó trong Phần 3.1, SSM như DSS và S4 sử dụng kernel có cấu trúc K, được xây dựng từ các ma trận học được A, B và C cho bất kỳ độ dài chuỗi L nào trong Phương trình 3. Vì K có thể mở rộng đến bất kỳ độ dài chuỗi L tùy ý nào, cả BST:SH:S4-L và GSS-HYBRID-L đều có cơ chế tích hợp sẵn cho tổng quát hóa độ dài mà mô hình BST:MF:UNSTRUCT-L không có cấu trúc không có. BST:MF:UNSTRUCT-L hoạt động tốt nhất trên chuỗi huấn luyện 4K và ngang bằng cho 512 với perplexity tăng cho độ dài chuỗi chưa thấy 16K và 65K. BST:SH:S4-L có perplexity tốt nhất cho độ dài chuỗi 65K trên PG19, GitHub và arXiv. Tương tự như [21], chúng tôi cũng nhận thấy rằng perplexity cải thiện khi chúng tôi mở rộng cửa sổ ngữ cảnh (độ dài chuỗi) cho PG19 và GitHub.

[THIS IS FIGURE/CHART: Hình 3 cho thấy ba biểu đồ so sánh tổng quát hóa độ dài cho các độ dài chuỗi {512, 16k, 65k} trên PG19 (trái), arXiv (giữa) và GitHub (phải). BST:SH:S4-L tổng quát hóa tốt hơn các baseline khác, bao gồm GSS-HYBRID-L sử dụng GSS, một SSM có cấu trúc.]

9

--- TRANG 10 ---
Hình 4: Trái: Thời gian tính toán forward-pass của một lớp BST được so sánh với một lớp BRECT và SLIDE:12L. Các thí nghiệm này được thực hiện trên GPU, để chứng minh và khai thác khả năng song song hóa của các lớp BST. BST:SH nhanh hơn 6-11× so với BRECT trong khi BST:MH nhanh hơn 3-4×. Phải: Perplexity của các mô hình được huấn luyện sử dụng các độ dài cửa sổ khác nhau. Hình cho thấy rằng việc tăng độ dài cửa sổ huấn luyện dẫn đến, như mong đợi, điểm perplexity tốt hơn. Tuy nhiên chúng tôi thấy rằng cả BST:MF:HYENA và BRECT:FIXED:SKIP đều ít bị ảnh hưởng nhất bởi việc giảm độ dài cửa sổ.

4.3 Hiệu quả
Cải thiện so với Block-Recurrent Transformer, với độ phức tạp thời gian O((W2+S2+2SW)·L/W)≈ O(L·W), xuất phát từ khả năng chạy các cell của Block Transformer song song. Độ phức tạp thời gian của lớp Block-State Transformer bao gồm độ phức tạp thời gian của sublayer mô hình không gian trạng thái, O(D·LlogL), cộng với độ phức tạp thời gian cần thiết để thực hiện Transformer trên các chunk ngữ cảnh nhất định (các khối) song song, O(W2).

Bất chấp sự tăng trưởng siêu tuyến tính của sublayer SSM, các thí nghiệm của chúng tôi chỉ ra rằng những cải thiện hiệu suất đáng kể, lên đến hệ số 6, vẫn rõ ràng đối với các chuỗi dài đến 65k token, điểm mà việc bão hòa phần cứng bắt đầu xảy ra. Khi sử dụng SSM có cấu trúc, độ phức tạp tính toán gắn chặt với kích thước trạng thái bộ nhớ nội bộ của SSM, N – chi tiết cụ thể có thể khác nhau tùy thuộc vào loại SSM chính xác. Chúng tôi đặt N= 16 khi báo cáo hiệu suất. Phía bên trái của Hình 4 cho thấy kết quả benchmark forward-pass của một lớp Block-State Transformer trên GPU. Lớp được đề xuất của chúng tôi chạy gần như nhanh hơn 6-11× so với Block-Recurrent Transformer (bao gồm các đơn vị hồi quy), và mang lại hiệu suất tương đương với lớp SLIDE:12L, tức là BRECT không có recurrence. Ở độ dài chuỗi 4k, chủ yếu được sử dụng trong quá trình huấn luyện, lớp BRECT chạy gần như chậm hơn 15× so với SLIDE:12L với cùng kích thước cửa sổ. Chúng tôi quản lý để giảm khoảng cách này xuống dưới 2× với lớp BST. Để phản ánh một mô hình thực tế, cho các thí nghiệm này chúng tôi sử dụng độ dài cửa sổ cố định 128, kích thước trạng thái nội bộ 16 cho SSM, và 16 head. Hơn nữa, để làm nổi bật những cải thiện hiệu suất chỉ do việc song song hóa được tạo ra bởi framework của chúng tôi, chúng tôi sử dụng cùng kích thước embedding làm đầu vào cho SSM, là 512. Lưu ý rằng chúng tôi sử dụng triển khai vanilla của các hoạt động FFT và inverse FFT được cung cấp bởi JAX [1]. Tuy nhiên, chúng tôi tin rằng tốc độ của phương pháp chúng tôi có thể được cải thiện hơn nữa với các triển khai I/O-aware cụ thể cho phần cứng gần đây và nhanh hơn được giới thiệu trong các framework auto-diff khác.

5 Kết luận
Chúng tôi đã giới thiệu một mô hình kết hợp cơ chế attention của Transformer với cơ chế bộ nhớ tầm xa, và khả năng song song được cung cấp bởi Mô hình Không gian Trạng thái. Chúng tôi khám phá một số biến thể trạng thái bộ nhớ đưa ra những đánh đổi khác nhau giữa sự dư thừa và khả năng truy xuất. Các thí nghiệm cho thấy rằng mô hình của chúng tôi có thể giảm thiểu perplexity ngang bằng và thường cải thiện so với các baseline cạnh tranh gần đây, trong khi đạt được tốc độ tăng hơn 10× ở cấp độ lớp, với điều kiện có hỗ trợ phần cứng để tận dụng đầy đủ khả năng song song. Đây là một tính chất hấp dẫn để mở rộng BST làm cho việc thêm SSM vào Transformer trở nên hấp dẫn về mặt tính toán. Chúng tôi cho thấy rằng việc tích hợp trạng thái SSM vào attention Transformer cung cấp lợi ích lớn hơn so với việc đơn giản xen kẽ các lớp SSM và attention. Cuối cùng, chúng tôi cho thấy rằng mô hình tổng quát hóa đến các chuỗi dài hơn so với những gì nó được huấn luyện.

10

--- TRANG 11 ---
Lời cảm ơn
Chúng tôi muốn cảm ơn Caglar Gulcehre và Albert Gu cho các thảo luận hữu ích và hỗ trợ với codebase S4. Chúng tôi cũng muốn bày tỏ lòng biết ơn đến Delesley Hutchins vì đã cung cấp hướng dẫn quý báu trong suốt dự án, cũng như Xavier Garcia và Courtney Paquette vì đã xem xét cẩn thận bản thảo, nơi họ đã xác định và sửa chữa một số lỗi.

Tài liệu tham khảo
[1]James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và Qiao
Zhang. JAX: các biến đổi có thể soạn được của các chương trình Python+NumPy, 2018.

[2]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, và Dario Amodei. Các mô hình ngôn ngữ là những người học ít mẫu. CoRR,
abs/2005.14165, 2020.

[3]T.S. Chihara. Giới thiệu về Đa thức Trực giao. Dover Books on Mathematics. Dover
Publications, 2011.

[4]Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Tạo ra các chuỗi dài với
transformer thưa thớt. CoRR, abs/1904.10509, 2019.

[5]Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Be-
langer, Lucy J. Colwell, và Adrian Weller. Suy nghĩ lại attention với performers. CoRR,
abs/2009.14794, 2020.

[6]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker
Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,
Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,
Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei
Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Mở rộng
mô hình hóa ngôn ngữ với pathways, 2022.

[7]James Cooley và John Tukey. Một thuật toán cho việc tính toán máy của chuỗi fourier phức.
Mathematics of Computation, 19(90):297–301, 1965.

[8]Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, và Ruslan Salakhutdinov.
Transformer-xl: Các mô hình ngôn ngữ chú ý vượt ra ngoài ngữ cảnh có độ dài cố định. arXiv preprint
arXiv:1901.02860, 2019.

[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training của
transformer hai chiều sâu cho hiểu biết ngôn ngữ. CoRR, abs/1810.04805, 2018.

[10] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, và Christopher Ré.
Hungry hungry hippos: Hướng tới mô hình hóa ngôn ngữ với các mô hình không gian trạng thái, 2023.

[11] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri
Rudra, và Christopher Ré. Tích chập dài đơn giản hiệu quả phần cứng cho mô hình hóa chuỗi,
2023.

[12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, và Christopher Re. Hippo: Bộ nhớ hồi quy
với phép chiếu đa thức tối ưu, 2020.

11

--- TRANG 12 ---
[13] Albert Gu, Karan Goel, Ankit Gupta, và Christopher Ré. Về tham số hóa và khởi tạo của các mô hình không gian trạng thái đường chéo. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, biên tập viên, Advances in Neural Information Processing Systems, 2022.

[14] Albert Gu, Karan Goel, và Christopher Ré. Mô hình hóa hiệu quả các chuỗi dài với không gian trạng thái có cấu trúc, 2022.

[15] Albert Gu, Ankit Gupta, Karan Goel, và Christopher Ré. Về tham số hóa và khởi tạo của các mô hình không gian trạng thái đường chéo, 2022.

[16] Ankit Gupta, Albert Gu, và Jonathan Berant. Không gian trạng thái đường chéo hiệu quả như không gian trạng thái có cấu trúc, 2022.

[17] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, và Marc van Zee. Flax: Thư viện mạng neural và hệ sinh thái cho JAX, 2023.

[18] Sepp Hochreiter. Vấn đề gradient biến mất trong quá trình học mạng neural hồi quy và các giải pháp vấn đề. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(2):107–116, 1998.

[19] Sepp Hochreiter và Jürgen Schmidhuber. Bộ nhớ ngắn hạn dài. Neural Computation, 9(8):1735–1780, 1997.

[20] Weizhe Hua, Zihang Dai, Hanxiao Liu, và Quoc Le. Chất lượng transformer trong thời gian tuyến tính. Trong Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, và Sivan Sabato, biên tập viên, Proceedings of the 39th International Conference on Machine Learning, tập 162 của Proceedings of Machine Learning Research, trang 9099–9117. PMLR, 17–23 Jul 2022.

[21] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, và Behnam Neyshabur. Block-recurrent transformers. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, biên tập viên, Advances in Neural Information Processing Systems, 2022.

[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers là RNNs: Transformer tự hồi quy nhanh với attention tuyến tính. Trong Hal Daumé III và Aarti Singh, biên tập viên, Proceedings of the 37th International Conference on Machine Learning, tập 119 của Proceedings of Machine Learning Research, trang 5156–5165. PMLR, 13–18 Jul 2020.

[23] Urvashi Khandelwal, He He, Peng Qi, và Dan Jurafsky. Sharp nearby, fuzzy far away: Cách các mô hình ngôn ngữ neural sử dụng ngữ cảnh. CoRR, abs/1805.04623, 2018.

[24] Diederik Kingma và Jimmy Ba. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. Trong International Conference on Learning Representations (ICLR), San Diega, CA, USA, 2015.

[25] Diederik P. Kingma và Jimmy Ba. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. Trong Yoshua Bengio và Yann LeCun, biên tập viên, ICLR (Poster), 2015.

[26] Conglong Li, Minjia Zhang, và Yuxiong He. Tình trạng khó xử ổn định-hiệu quả: Điều tra warmup độ dài chuỗi cho huấn luyện các mô hình GPT. Trong Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho, biên tập viên, Advances in Neural Information Processing Systems, 2022.

[27] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, và Luke Zettlemoyer. Mega: Moving average được trang bị gated attention, 2023.

[28] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, và Behnam Neyshabur. Mô hình hóa ngôn ngữ tầm xa thông qua không gian trạng thái có cổng. Trong The Eleventh International Conference on Learning Representations, 2023.

[29] OpenAI. Báo cáo kỹ thuật gpt-4, 2023.

[30] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, và Christopher Ré. Hyena hierarchy: Hướng tới các mô hình ngôn ngữ tích chập lớn hơn, 2023.

[31] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, và Timothy P. Lillicrap. Transformer nén cho mô hình hóa chuỗi tầm xa. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.

[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Khám phá giới hạn của transfer learning với một transformer văn bản thành văn bản thống nhất. CoRR, abs/1910.10683, 2019.

12

--- TRANG 13 ---
[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, và Peter J. Liu. Khám phá giới hạn của transfer learning với một
transformer văn bản thành văn bản thống nhất. J. Mach. Learn. Res., 21:140:1–140:67, 2020.

[34] Jimmy T. H. Smith, Andrew Warrington, và Scott W. Linderman. Các lớp không gian trạng thái đơn giản hóa cho mô hình hóa chuỗi, 2023.

[35] Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, và Y-Lan Boureau. Các mô hình ngôn ngữ transformer đa tỷ lệ. CoRR, abs/2005.00581, 2020.

[36] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, và Da-Cheng Juan. Sparse Sinkhorn attention.
Trong Hal Daumé III và Aarti Singh, biên tập viên, Proceedings of the 37th International Conference
on Machine Learning, tập 119 của Proceedings of Machine Learning Research, trang
9438–9447. PMLR, 13–18 Jul 2020.

[37] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: Một benchmark cho các
transformer hiệu quả, 2020.

[38] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: Một benchmark cho các
transformer hiệu quả. Trong International Conference on Learning Representations, 2021.

[39] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,
Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee,
Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,
Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,
Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,
Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,
Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben
Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi
Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen,
Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi,
và Quoc Le. Lamda: Các mô hình ngôn ngữ cho ứng dụng đối thoại. CoRR, abs/2201.08239, 2022.

[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong I. Guyon, U. Von Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett, biên tập viên, Advances in Neural
Information Processing Systems, tập 30. Curran Associates, Inc., 2017.

[41] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention
với độ phức tạp tuyến tính. CoRR, abs/2006.04768, 2020.

[42] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, và Christian Szegedy. Memorizing
transformers. Trong International Conference on Learning Representations, 2022.

[43] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird:
Transformers cho các chuỗi dài hơn. Advances in Neural Information Processing Systems, 33,
2020.

13

--- TRANG 14 ---
A Hạn chế
Trong khi lớp SSM của BST cho phép mô hình mở ra và song song hóa recurrence mô hình hóa ngữ cảnh dài hạn giữa các khối token, các biến thể SSM phụ thuộc vào các hoạt động FFT hiệu quả. Chúng tôi đã phát hiện rằng hoạt động FFT là một bottleneck tốc độ quan trọng trên TPU cần được giải quyết để mở rộng BST tốt hơn đến nhiều lớp và các mô hình lớn hơn. Mặc dù chúng tôi vẫn đang điều tra các lý do, chúng tôi phát hiện rằng JAX FFT nhanh hơn 4× trên GPU. Hơn nữa, các biến thể SSM mới như S5[34] bỏ qua các hoạt động FFT sử dụng một toán tử kết hợp nhị phân4. Triển khai của chúng tôi đủ modular để chúng tôi có thể đơn giản cắm S5 hoặc sử dụng các triển khai FFT khác.

Một trong những giả định của chúng tôi là lớp SSM của BST có thể nắm bắt các phụ thuộc dài hạn đúng cho mỗi khối. Recurrence SSM tại bước T=t cung cấp một biểu diễn tóm tắt của các bước trước đó cho T= 0 đến T=t. Tuy nhiên, một biểu diễn vector đơn có thể không đủ mạnh để hỗ trợ tất cả các phụ thuộc dài hạn quan trọng. Bất chấp những cải thiện perplexity trên các nhiệm vụ mô hình hóa ngôn ngữ tầm xa, giả định này cần được kiểm tra trên các nhiệm vụ phân loại tầm xa khác như Long Range Arena [37] cũng vậy. Có thể mô hình của chúng tôi có thể hoạt động tốt hơn nếu chúng tôi đưa vào lớp attention k=W biểu diễn SSM được chọn bởi một hoạt động truy xuất top-k, tương tự như trong Memorizing Transformer [42].

B So sánh chi tiết hơn với các baseline hiện có
Phần này cung cấp cho người đọc một so sánh sâu hơn với các kiến trúc tương tự. Chúng tôi đề cập đến BRECT[21] trong Phần B.1 và GSS-HYBRID [28] trong Phần B.2.

B.1 So sánh với Block Recurrent Transformer (BRECT)
Sublayer Block Transformer (tức là SLIDE:12L) xử lý key và value từ cửa sổ trước được lưu trữ trong cache có thể vi phân. Điều này được triển khai tương tự như mẫu attention cửa sổ trượt được đề xuất trong [21] và ban đầu được giới thiệu bởi Transformer-XL [8]. Sử dụng causal mask, tại mỗi bước suy luận token, cơ chế attention được áp dụng cho các khối token có kích thước W và được mở rộng một phần đến các key và value được cache từ khối trước với cửa sổ trượt.

BRECT, như được giải thích trong [21], sử dụng cache không thể vi phân được mang từ một chuỗi có kích thước L đến chuỗi tiếp theo5. Các trạng thái hồi quy cuối cùng của một chuỗi được lưu trữ trong cache không thể vi phân và đưa vào bước huấn luyện tiếp theo trên chuỗi tiếp theo trong tài liệu như một khởi đầu ấm. Chúng tôi không truyền biểu diễn như vậy, vì để tính toán đầu ra của tích chập, chúng tôi cần truy cập vào toàn bộ chuỗi. Chúng tôi tin rằng đây là một lợi thế mà BRECT có so với phương pháp của chúng tôi, đặc biệt cho các ví dụ rất dài được chia thành các chuỗi có thứ tự có độ dài L, vì cache được mang từ một chuỗi sang chuỗi tiếp theo có thể cung cấp thông tin tầm xa rất hữu ích và khả năng truy cập (yếu) đến toàn bộ quá khứ. Vì chúng tôi cần toàn bộ chuỗi để tính toán trạng thái SSM, lịch sử vượt ra ngoài L có thể bị mất trong quá trình. Chúng tôi tin rằng BST có thể được cải thiện hơn nữa bằng cách thêm cache chuỗi không thể vi phân cho các tài liệu rất dài.

Trong khi các kiến trúc khác, lịch sử giữa các khối token không được mô hình hóa, cả BST và BRECT đều sử dụng cơ chế để mô hình hóa ngữ cảnh khối trước. Các tác giả của BRECT thí nghiệm với các cơ chế gating tuần tự khác nhau để cô đọng thông tin từ các khối trước. Với BST, chúng tôi sử dụng SSM để cung cấp ngữ cảnh từ các khối trước đến khối hiện tại như được giải thích trong Phần 3.2.

B.2 So sánh với Transformer GSS-HYBRID
GSS-HYBRID [28] là một kiến trúc kết hợp SSM-Transformer mà chúng tôi mô tả đầu tiên trong Phần 4.1. Kiến trúc khác biệt đáng kể so với BST. GSS-HYBRID chủ yếu bao gồm các lớp Gated State Space (GSS) và có một vài lớp Transformer xen kẽ tại mỗi lớp thứ 4 bắt đầu với lớp thứ 2. BST mặt khác chủ yếu bao gồm các lớp Block Transformer và có các lớp Block-State Transformer tại các vị trí {1, 7, 9} cho mô hình ∼200M và {1, 5, 7, 9} cho mô hình ∼400M. Kết hợp của chúng tôi không xếp chồng các lớp SSM và Transformer như GSS-HYBRID mà thay thế recurrence trong BRECT bằng một SSM như S4. Trong BST, SSM tạo ra trạng thái cho mỗi biểu diễn Block Transformer và sau đó chúng tôi sử dụng cross-attention để trộn các trạng thái và đầu ra self-attention. Các tác giả trong [28] ban đầu xây dựng GSS, một phiên bản có cổng của DSS [16], để (1) giảm chiều tham số SSM, (2) ổn định huấn luyện SSM và (3) cho phép tổng quát hóa độ dài tốt hơn. Tuy nhiên, khi thí nghiệm với SSM như S4 hoặc DSS, chúng tôi thấy rằng gating không cần thiết để đạt được cả ba mục tiêu đã nêu ở trên. Chúng tôi quyết định rằng việc sử dụng Gated Attention Unit [20] của GSS do đó không cần thiết khi tích hợp trạng thái SSM vào cơ chế attention. Chúng tôi cũng nhắc lại rằng các tác giả trong [28] đã sử dụng tìm kiếm siêu tham số để có được hiệu suất tốt nhất trong khi chúng tôi thì không.

C Thí nghiệm Mở rộng
[THIS IS FIGURE/CHART: Biểu đồ cho thấy thuộc tính mở rộng trên PG-19. Vàng: (BST:SH:UNSTRUCT) 12-layer Block-State Transformer. Đỏ: (REC:FIXED:SKIP) 12-layer Block-Recurrent Transformer. Xanh: (TRSF-XL-2048) 13-layer Transformer-XL.]

Trong phần này, chúng tôi so sánh cách BST mở rộng so với Transformer-XL với cửa sổ gấp 4 lần và BRECT. Trong Hình 5, chúng tôi thấy rằng ở tỷ lệ thấp hơn, từ 80M đến 200M, BRECT và BST có hiệu suất rất tương tự. Vượt quá 200M, khoảng cách tỷ lệ phần trăm hiệu suất perplexity giữa BRECT và BST tăng từ 2.5% ở 200M tham số lên 4.0% ở 1.3B tham số. Khoảng cách tỷ lệ phần trăm hiệu suất perplexity giữa BRECT và TRSF-XL thậm chí còn rõ rệt hơn khi nó bắt đầu ở 7.6% ở 200M tham số lên 10.6% ở 1.3B tham số.

D Thí nghiệm Long Range Arena
[THIS IS TABLE: Bảng hiệu suất trên Long-Range Arena (LRA) cho thấy kết quả so sánh của các mô hình khác nhau trên 6 nhiệm vụ: LISTOPTS, TEXT, RETRIEVAL, IMAGE, PATHFINDER, PATH-X, và điểm trung bình. Các phương pháp với đầu vào chuỗi chunked được nhóm riêng.]

4Trong JAX, điều này tương đương với việc sử dụng jax.lax.associative_scan.
5Trong nghiên cứu của chúng tôi và trong [21], một tài liệu được chia thành nhiều chuỗi có kích thước L và mỗi chuỗi được chia thành nhiều khối có kích thước W

14

--- TRANG 15 ---
Trong khi trọng tâm chính của nghiên cứu của chúng tôi là chứng minh rằng các mô hình Transformer-SSM kết hợp hiệu quả và hoạt động tốt trên LM tự hồi quy ngữ cảnh dài, chúng tôi cũng đánh giá phương pháp của mình trên nhiệm vụ phân loại tiêu chuẩn nơi các phụ thuộc tầm xa trong một chuỗi quan trọng để nắm bắt. Trong Bảng 2, chúng tôi trình bày kết quả của chúng tôi trên benchmark Long Range Arena (LRA) [38] bao gồm ba phương thức khác nhau bao gồm văn bản, hình ảnh và biểu thức toán học. Bộ dữ liệu LRA cũng kiểm tra các mô hình trên các độ dài chuỗi khác nhau từ 1K đến 16K.

BST:SH:S4 bao gồm bốn lớp BST (không có lớp BRT xen kẽ) và hai lớp S4 ở trên. Chúng tôi sử dụng cùng độ dài khối tiêu chuẩn 512 cho BST và BRT. Tuy nhiên, chúng tôi huấn luyện BST và BRT trên các chuỗi đầy đủ (lên đến 16K cho Path-X). Chúng tôi sử dụng AdamW làm optimizer [24] với warmup cho learning rate, nơi chúng tôi bắt đầu từ giá trị 1e−7 và tăng learning rate tuyến tính lên một giá trị được chỉ định ∈ {1e−3,2e−3,4e−3} cho 10% đầu tiên của huấn luyện. Điều này được theo sau bởi cosine annealing cho phần còn lại của huấn luyện xuống giá trị 1e−7. Tất cả các lớp đều hai chiều, bao gồm lớp S4 trong BST:SH:S4 như được mô tả trong [13]. Weight decay của chúng tôi được chọn từ {0, 0.05, 0.1, 0.15} và dropout của chúng tôi được chọn từ {0, 0.1}. Ngoại trừ thí nghiệm Path-X, chúng tôi sử dụng weight decay ∈ {0.03,0.05,0.07} cho tất cả các tham số ngoại trừ ma trận S4D A và B. Ngoài ra, cho Path-X, phạm vi khởi tạo của bước thời gian rời rạc hóa ∆ cho PathX được giảm từ (∆min,∆max) = (0.001, 0.1) xuống (∆min,∆max) = (0.0001, 0.01).

Kết quả của chúng tôi trên LRA rất hứa hẹn và cho thấy rằng, so với các phương pháp hiện đại khác chunk chuỗi thành các khối, BST có thể mô hình hóa các phụ thuộc tầm xa. Ví dụ, BST vượt trội hơn MEGA-CHUNK [27] trên bốn trong sáu nhiệm vụ LRA và 1.5% trên điểm trung bình. Tuy nhiên, BST vẫn cần cải thiện (có lẽ bằng cách mở rộng kích thước khối) để bắt kịp MEGA (không có chunk).

E Nghiên cứu Ablation
Trong phần sau, chúng tôi thực hiện ablation để điều tra (1) vị trí của một lớp SSM đơn trong Bảng 3 trong kiến trúc tổng thể, (2) ảnh hưởng của số lượng lớp SSM được thêm vào trong Bảng 4, và (3) kích thước D của trạng thái SSM trong Bảng 5. Cho các ablation, chúng tôi sử dụng BST:SH:S4 ∼200M tham số, vì nó là mô hình nhanh nhất, và đánh giá các cấu hình khác nhau trên PG19.

[THIS IS TABLE: Three tables showing:
Table 3: A single BST at various layer indices
Table 4: Multiple BST layers at various locations  
Table 5: Increasing BST's S4 model state size D]

Trong Bảng 3, chúng tôi thí nghiệm thêm một lớp BST đơn tại các chỉ số lớp 3,6,9,12. Chúng tôi nhận thấy rằng một lớp BST đơn với kích thước trạng thái D= 16 nằm gần giữa toàn bộ stack Block Transformer, tại index = 9, có ảnh hưởng lớn nhất đến perplexity. Phát hiện này phù hợp với các phát hiện trong nghiên cứu trước [42, 21].

Trong Bảng 4, chúng tôi kiểm tra xem việc thêm nhiều lớp BST có mang lại cải thiện hiệu suất không. Chúng tôi bắt đầu với các lớp BST có kích thước trạng thái D= 16 tại các chỉ số 0,9. Chúng tôi tiếp tục bằng cách thêm một lớp BST khác tại chỉ số 7 cho tổng cộng ba lớp BST và sau đó một lớp khác tại chỉ số 5, tiếp theo là một lớp khác tại chỉ số 12. Thêm nhiều lớp BST làm giảm perplexity. Tuy nhiên, kết quả dường như đạt plateau ở 5 lớp BST. Chúng tôi cũng lưu ý rằng có sự tăng 3.5% thời gian bước huấn luyện cho mỗi lớp được thêm vào.

Trong Bảng 5, chúng tôi huấn luyện các mô hình của mình với các kích thước trạng thái D khác nhau. Cho ablation kích thước trạng thái, chúng tôi sử dụng ba lớp BST tại các chỉ số 0,7,9. Chúng tôi thấy rằng việc tăng D cải thiện perplexity với chi phí tốc độ huấn luyện (thời gian bước). Vì lý do này, chúng tôi chọn D= 16 cho kết quả BST Bảng 1.

16

--- TRANG 16 ---
F Triển khai JAX của BST
Pseudocode 1 chứa một hàm triển khai tích chập của nhiều bộ lọc trên cùng một chuỗi đầu vào sử dụng FFT và các hoạt động FFT nghịch đảo. Pseudocode 2, 3 và 4 tương ứng triển khai thu thập trạng thái ngữ cảnh của các biến thể BST: Single-Head (SH), Multi-Head (MH) và Multi-Filter (MF). Cuối cùng, Pseudocode 5 chạy sublayer Block Transformer song song bằng cách đưa các trạng thái ngữ cảnh vào khối tương ứng của chúng.

"""Bộ lọc và tích chập không có cấu trúc."""
import jax
from jax import numpy as jnp
from einops import rearrange

win_length = 512 # (w)
seq_length = 4096 # (l)

def get_filters_unstruct(channels):
    """Trả về các bộ lọc và bias có thể huấn luyện.
    Args:
        channels: số lượng bộ lọc.
    Returns:
        h: bộ lọc có hình dạng (seq_length, channels, dim)
        b: bias có hình dạng (channels, dim)
    """
    t = jnp.linspace(0.0, 1.0, seq_length)
    h = jnp.exp(- alpha * t) * dense(positional_emb(t))
    b = get_bias()
    return h, b

def multichannel_convolution(u, h, b):
    """Hàm tích chập đa kênh.
    Args:
        u: đầu vào có hình dạng (seq_length, dim)
        h: bộ lọc có hình dạng (seq_length, channels, dim)
        b: bias có hình dạng (channels, dim)
    """
    h = rearrange(h, "l c d -> c d l")
    fft_size = seq_length * 2
    u_f = jnp.fft.rfft(x, n=fft_size)
    h_f = jnp.fft.rfft(h, n=fft_size)
    y = jnp.fft.irfft(h_f * x_f, n=fft_size, norm="forward")[
        ..., :seq_length] # (c, d, l)
    y = y + x * b[..., None] # (c, d, l)
    y = rearrange(y, "c d l -> l d c")
    return y

Pseudocode 1: Bộ lọc và tích chập không có cấu trúc.

"""Thu thập trạng thái ngữ cảnh cho biến thể BST-SH."""
num_heads = 8 # (h)
num_states = 32 # (s)

# (SH): Single-Head
def SH_context_states(u):
    """Thu thập Ngữ cảnh Single-Head."""
    h, b = get_filters_[unstruct/s4](channels=1)

17

--- TRANG 17 ---
    y_1 = multichannel_convolution(u, h, b) # y_1: (l, d, 1)
    # nâng lên nhiều head
    y_h = dense(y_1) # y_h: (l, d, h)
    context_states = jnp.split(
        y_h, seq_length // win_length, axis=0)
    return context_states # (l/w, w, d, h)

Pseudocode 2: Thu thập trạng thái ngữ cảnh cho các biến thể BST-SH.

"""Thu thập trạng thái ngữ cảnh cho biến thể BST-MH."""
# (MH): Multi-Head
def MH_context_states(u):
    """Thu thập Ngữ cảnh Multi-Head."""
    h, b = get_filters_[unstruct/s4](channels=num_heads)
    y_h = multichannel_convolution(u, h, b) # y_h: (l, d, h)
    context_states = jnp.split(
        y_h, seq_length // win_length, axis=0)
    return context_states # (l/w, w, d, h)

Pseudocode 3: Thu thập trạng thái ngữ cảnh cho các biến thể BST-MH.

"""Thu thập trạng thái ngữ cảnh cho biến thể BST-MF."""
# (MF): Multi-Filter
def MF_context_states(u):
    """Thu thập Ngữ cảnh Multi-Filter."""
    h, b = get_filters_[unstruct/s4](channels=num_states)
    y_s = multichannel_convolution(u, h, b) # y_s: (l, d, s)
    context_states = jnp.split(
        y_s, seq_length // win_length, axis=0)
    # context_states: (l/w, w, d, s)
    # thu thập các trạng thái ngữ cảnh cuối cùng
    context_states = context_states[:, -1, ...] # (l/w, d, s)
    context_states = rearrange(
        context_states, "lw d s -> lw s d")
    # dịch chuyển các trạng thái ngữ cảnh tương ứng với các cửa sổ
    context_states = jnp.roll(context_states, 1, axis=1)
    # thay thế cửa sổ ban đầu bằng trọng số có thể huấn luyện
    init_context = get_init_context(num_states) # (d, s)
    context_states[0] = init_context
    # nâng lên nhiều head
    context_states = dense(context_states)
    return context_states # (l/w, s, d, h)

Pseudocode 4: Thu thập trạng thái ngữ cảnh cho các biến thể BST-MF.

"""Lớp Block-State Transformer."""
# Block Transformer không hồi quy và có thể song song hóa.
block_transformer = jax.vmap(BRecT.nonrecurrent_cell)

def BST(u):
    """Lớp Block-State Transformer."""

18

--- TRANG 18 ---
    global MF # True nếu Multi-Filter, False nếu không (SH/MH)
    # chia đầu vào thành các cửa sổ (l/w, w, d)
    u = jnp.split(u, seq_length // win_length, axis=0)
    # thu thập các trạng thái ngữ cảnh từ đầu ra SSM
    context_states = [SH/MH/MF]_context_states(u)
    # truyền các ngữ cảnh thay cho các trạng thái hồi quy
    y = block_transformer(
        token_embeddings=u,
        recurrent_state=context_states,
        use_cross_attn_causal_mask=not MF,
        use_cross_positional_emb=MF, # context ID
    )
    return rearrange(y, "lw w d -> (lw w) d") # (l, d)

Pseudocode 5: Lớp Block-State Transformer.

19
