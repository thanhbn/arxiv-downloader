# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/llm-architecture/2203.07852.pdf
# File size: 836079 bytes

===============================================
PDF FILE CONTENT
===============================================


--- TRANG 1 ---
Block-Recurrent Transformers
DeLesley Hutchins1, Imanol Schlag3y, Yuhuai Wu1, Ethan Dyer2, Behnam Neyshabur2
1Google Research2Google Research, Blueshift Team
3The Swiss AI Lab IDSIA, SUPSI & USI
{delesley, yuhuai, edyer, neyshabur}@google.com imanol@idsia.ch

Tóm tắt
Chúng tôi giới thiệu Block-Recurrent Transformer, áp dụng một lớp transformer theo cách hồi quy dọc theo một chuỗi, và có độ phức tạp tuyến tính so với độ dài chuỗi. Tế bào hồi quy của chúng tôi hoạt động trên các khối token thay vì token đơn lẻ trong quá trình huấn luyện, và tận dụng tính toán song song trong một khối để sử dụng hiệu quả phần cứng gia tốc. Chính tế bào này rất đơn giản một cách đáng kinh ngạc. Nó chỉ đơn thuần là một lớp transformer: nó sử dụng self-attention và cross-attention để tính toán hiệu quả một hàm hồi quy trên một tập hợp lớn các vector trạng thái và token. Thiết kế của chúng tôi được lấy cảm hứng một phần từ các tế bào LSTM, và nó sử dụng các cổng kiểu LSTM, nhưng nó mở rộng tế bào LSTM điển hình lên vài bậc độ lớn. Việc triển khai hồi quy của chúng tôi có cùng chi phí về thời gian tính toán và số lượng tham số như một lớp transformer thông thường, nhưng cung cấp perplexity được cải thiện đáng kể trong các tác vụ mô hình hóa ngôn ngữ trên các chuỗi rất dài. Mô hình của chúng tôi vượt trội hơn baseline Transformer XL tầm xa với biên độ lớn, trong khi chạy nhanh gấp đôi. Chúng tôi chứng minh tính hiệu quả của nó trên PG19 (sách), các bài báo arXiv, và mã nguồn GitHub. Mã của chúng tôi đã được công bố dưới dạng mã nguồn mở [1].

1 Giới thiệu
Transformer hầu hết đã thay thế các mạng neural hồi quy (RNN), như LSTM [2], trong các tác vụ liên quan đến dữ liệu tuần tự, đặc biệt là ngôn ngữ tự nhiên. Có nhiều lý do cho thành công của chúng. Đầu tiên, transformer xử lý tất cả các phần tử của chuỗi song song, và do đó nhanh hơn để huấn luyện trên phần cứng gia tốc hiện đại. Ngược lại, RNN phải xử lý token tuần tự, dẫn đến thời gian bước chậm trong quá trình huấn luyện, và kích thước batch lớn để bão hòa hoàn toàn GPU hoặc TPU.

Thứ hai, RNN phải tóm tắt và nén toàn bộ chuỗi trước đó thành một vector trạng thái duy nhất được truyền từ token này sang token khác. Kích thước của vector trạng thái giới hạn lượng thông tin mà RNN có thể mã hóa về các token trước đó trong chuỗi. Ngược lại, transformer có thể chú ý trực tiếp đến các token quá khứ, và không gặp phải hạn chế này.

Thứ ba, attention hoạt động hiệu quả trên khoảng cách dài hơn. Cổng forget trong LSTM loại bỏ thông tin di chuyển về phía trước, và gây ra gradient biến mất trong quá trình lan truyền ngược. Trong thực tế, điều này có nghĩa là LSTM gặp khó khăn trong việc gửi tín hiệu rõ ràng qua hơn vài trăm token, ít hơn nhiều so với kích thước điển hình của cửa sổ attention trong transformer [3].

Mặc dù có những ưu điểm này, transformer cũng có một nhược điểm. Độ phức tạp tính toán của self-attention là bậc hai so với độ dài chuỗi, đây là một yếu tố hạn chế khi cố gắng xử lý các tài liệu dài, như sách, bài báo kỹ thuật, hoặc kho mã nguồn. Hơn nữa, transformer không có bộ nhớ về ngữ cảnh quá khứ; bất kỳ token nào mà nó không thể chú ý đến đều "vô hình" đối với mô hình.

Đóng góp bằng nhau yTác việc được thực hiện một phần trong thời gian thực tập tại Google Research (Blueshift Team) và một phần được tài trợ bởi ERC Advanced grant số: 742870 cho J.Schmidhuber.

Hội nghị lần thứ 36 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2022).arXiv:2203.07852v3 [cs.LG] 2 Nov 2022

--- TRANG 2 ---
đầu vào token embeddingsđầu ra token embeddings
vector trạng thái hiện tạivector trạng thái tiếp theo
tuyến tính
Qsvtuyến tính
Ke Ve Qevsscross-
attnself-
attntuyến tính projection +MLP+
đầu vào token embeddingsvector trạng thái hiện tại
tuyến tính
Ke Vetuyến tính
Ks Vs Qsh
tuyến tính
Qehcross-
attnself-
attnM
LPtuyến tính projectioncổngcổng
+học được
state 
IDs+học được
state 
IDsđầu ra token embeddings
vector trạng thái tiếp theoHình 1: Minh họa tế bào hồi quy của chúng tôi. Phía bên trái mô tả hướng dọc (các lớp được xếp chồng theo cách thông thường) và phía bên phải mô tả hướng ngang (hồi quy). Lưu ý rằng hướng ngang chỉ đơn giản là xoay một lớp transformer thông thường 90°, và thay thế các kết nối dư thừa bằng các cổng.

Trong công trình này, chúng tôi mô tả một kiến trúc kết hợp những lợi ích của attention và hồi quy. Giống như các triển khai hồi quy trước đây, kiến trúc của chúng tôi xây dựng và duy trì một trạng thái có kích thước cố định, tóm tắt chuỗi mà mô hình đã thấy cho đến nay. Tuy nhiên, việc triển khai hồi quy của chúng tôi khác với công trình trước đây ở một số khía cạnh quan trọng mà cùng nhau giải quyết ba hạn chế đã đề cập ở trên.

Thay vì xử lý chuỗi từng token một, tế bào hồi quy của chúng tôi hoạt động trên các khối token; xem Hình 1. Trong một khối, tất cả token được xử lý song song, ít nhất là trong quá trình huấn luyện. Tế bào hồi quy cũng hoạt động trên một khối vector trạng thái thay vì một vector đơn lẻ. Điều này có nghĩa là kích thước của trạng thái hồi quy lớn hơn vài bậc độ lớn so với trong LSTM, điều này cải thiện đáng kể khả năng của mô hình trong việc nắm bắt quá khứ. Xử lý chuỗi theo khối cũng giúp truyền thông tin và gradient qua khoảng cách dài hơn, bởi vì số lượng bước hồi quy (và do đó số lần cổng forget được áp dụng) nhỏ hơn vài bậc độ lớn. Chúng tôi chỉ ra rằng Block-Recurrent Transformer có thể nhớ thông tin qua khoảng cách 60k token hoặc hơn.

Tế bào hồi quy bản thân rất đơn giản một cách đáng kinh ngạc. Phần lớn, nó bao gồm một lớp transformer thông thường được áp dụng theo cách hồi quy dọc theo độ dài chuỗi. Có một vài thủ thuật cần thiết để ổn định quá trình huấn luyện; xem Mục 3.2 và 3.4 để biết chi tiết. Chi phí của hồi quy, về cả thời gian tính toán và số lượng tham số, về cơ bản giống như việc chỉ đơn giản thêm một lớp nữa vào baseline transformer của chúng tôi. Chúng tôi chứng minh thực nghiệm rằng việc thêm một lớp hồi quy duy nhất dẫn đến cải thiện perplexity lớn hơn nhiều trên nhiều bộ dữ liệu so với việc thêm một lớp transformer thông thường, trong khi thời gian huấn luyện và việc sử dụng bộ nhớ tương đương. Hơn nữa, tế bào hồi quy của chúng tôi rất dễ triển khai vì nó chủ yếu sử dụng mã transformer hiện có. Do đó, kỹ thuật của chúng tôi là một cách rẻ và vui vẻ để cải thiện perplexity mô hình hóa ngôn ngữ trên các chuỗi dài.

2 Công trình liên quan
Chi phí bậc hai của attention được biết đến rộng rãi trong tài liệu, và đã có rất nhiều công trình về các cơ chế attention tầm xa hiệu quả; xem [4,5] cho các khảo sát gần đây. Các chiến lược thưa thớt như Big Bird [6], Routing Transformers [7], và Reformer [8] chỉ chọn một tập con token để chú ý đến. Các cơ chế phân cấp [9] kết hợp nhiều token thành cụm từ hoặc câu để giảm độ dài chuỗi. Expire-span [10] học cách loại bỏ các token xa mà mô hình đã gắn nhãn là "không quan trọng". Memorizing transformers [11] thay thế dense attention bằng k-nearest-neighbor lookup.

Còn một cách tiếp cận khác là giảm độ dài chuỗi bằng cách pooling, averaging, hoặc nén nó theo cách nào đó. Hierarchical 1D attention [12], và Combiner [13] áp dụng pooling hoặc averaging qua các token ở khoảng cách dài hơn. Linformer [14] áp dụng biến đổi tuyến tính lên các ma trận key và value để giảm độ dài chuỗi. Compressive transformers [15] và funnel transformers [16] áp dụng các lớp nén học được bổ sung để nén chuỗi.

2

--- TRANG 3 ---
key, value 
được cachekey, value 
hiện tại
query 
hiện tại
 key, value
sẽ được cachedHình 2: Cửa sổ trượt, với độ dài đoạn N = 16, kích thước cửa sổ/khối W = 8. Key và value cho W token được tô bóng đầu tiên đã được tính toán và cached trong bước huấn luyện trước; N token không được tô bóng còn lại là đoạn cho bước huấn luyện hiện tại. Thay vì một ma trận attention N×(W+N) duy nhất, attention được thực hiện trong hai tile có kích thước W×2W.

Phương trình cho attention là (gần đúng) softmax(QKT)V trong đó Q, K, và V là các ma trận query, key, và value của lớp attention. Nếu phép toán softmax được loại bỏ khỏi phương trình này hoặc được "tuyến tính hóa" theo cách nào đó, phương trình có thể được sắp xếp lại thành Q(KTV), trong đó (KTV) có thể được tính toán dần dần (tức là theo cách hồi quy) như một tổng tích lũy qua chuỗi [17]. Do đó, linearized attention có độ phức tạp tuyến tính thay vì bậc hai so với độ dài chuỗi. Theo lối suy luận này, đã có một số đề xuất xấp xỉ softmax [18,19] hoặc thay thế nó [20,21]. Linear transformers liên quan đến công trình trước đây về fast weight programmers [20] [22], và có thể được mở rộng với các dạng hồi quy khác [23].

Công trình của chúng tôi khác với tất cả các cơ chế trên, bởi vì chúng tôi chỉ dựa vào dense attention tiêu chuẩn với softmax.

Một vài dòng nghiên cứu khác đã kết hợp kiến trúc transformer với hồi quy theo cách nào đó. Feedback transformer [24] cho phép các lớp thấp hơn chú ý đến đầu ra của lớp cao nhất. Feedback có chi phí tối thiểu tại thời điểm suy luận, nhưng thật không may rất chậm để huấn luyện vì token phải được xử lý tuần tự. Simple Recurrent Units [25,26] sử dụng hàm hồi quy không liên quan đến phép nhân ma trận, và do đó nhanh hơn nhiều. RNMT+ kết hợp RNN và transformer trong kiến trúc encoder/decoder để cải thiện các tác vụ dịch thuật [27]. "Sandwich models" xen kẽ giữa các lớp transformer và RNN và vượt trội hơn cả transformer và RNN trong các tác vụ liên quan đến mã nguồn [28]. R-Transformer giới thiệu một RNN địa phương bổ sung có thể được tính toán song song để mô hình hóa cấu trúc tuần tự tốt hơn [29]. Kiến trúc Perceiver [30] có phần tương tự với chúng tôi; nó cũng áp dụng một lớp transformer theo cách lặp lại.

Theo hiểu biết tốt nhất của chúng tôi, ý tưởng thực hiện hồi quy trên các khối token còn ít được khám phá. Trong bối cảnh dịch thuật, [31] hoạt động trên câu thay vì token. Staircase Attention [32] cũng hoạt động trên các khối token; mỗi lớp nhận, làm đầu vào, các đầu ra của cùng lớp đó từ khối trước.

3 Phương pháp
Block-Recurrent Transformer dựa trên sliding-window attention [33], là một phần mở rộng của các ý tưởng từ Transformer-XL [34].

Một tài liệu dài, như một cuốn sách, bao gồm một chuỗi token. Do hạn chế bộ nhớ, thường không thể đưa toàn bộ chuỗi vào bộ nhớ thiết bị. Do đó, chuỗi được chia thành các đoạn có độ dài N (N = 4096 trong các thí nghiệm của chúng tôi), được xử lý tuần tự qua một số bước huấn luyện. Mỗi bước huấn luyện xử lý một đoạn.

Mẫu sliding window attention được minh họa trong Hình 2. Với một đoạn N token, cửa sổ trượt áp dụng mặt nạ nhân quả trong đó mỗi token chỉ có thể chú ý đến W token trước đó,

3

--- TRANG 4 ---
trong đó W là kích thước cửa sổ (W = 512 trong các thí nghiệm của chúng tôi). Do mặt nạ nhân quả, hầu hết các mục của ma trận attention N×N bị che (giả sử W << N). Do đó, việc tính toán attention có thể được tối ưu hóa bằng cách chia nó thành các tile nhỏ hơn dọc theo đường chéo. Đoạn N token được chia nhỏ thành các khối có kích thước W, và mỗi khối chú ý cục bộ đến chính nó và khối trước đó, vì vậy kích thước của mỗi ma trận attention cục bộ là W×2W. Sử dụng cơ chế này, attention là bậc hai so với kích thước cửa sổ W, nhưng tuyến tính so với độ dài đoạn N.

Mượn ý tưởng từ Transformer-XL, key và value từ khối cuối cùng trong mỗi đoạn được lưu trong cache không khả vi để sử dụng trong bước huấn luyện tiếp theo. Bằng cách sử dụng cache, khối đầu tiên trong đoạn tiếp theo có thể chú ý đến khối cuối cùng trong đoạn trước, điều này mở rộng cửa sổ trượt để bao phủ toàn bộ chuỗi (dài như sách). Cache triển khai một dạng truncated backpropagation through time [35] qua các tài liệu dài.

Lưu ý rằng nếu N = W, thì sliding window attention sẽ hoạt động chính xác giống như Transformer-XL; nó sẽ xử lý và cache một đoạn (tức là một khối) mỗi bước huấn luyện. Đặt N >> W không thay đổi độ dài ngữ cảnh của attention, nhưng nó cho phép gradient lan truyền ngược qua nhiều khối trong quá trình huấn luyện; chúng tôi chỉ ra rằng khả vi tính cải thiện mang lại lợi ích khiêm tốn cho perplexity so với Transformer-XL. Xem Phụ lục A để biết thêm chi tiết.

3.1 Tế bào hồi quy
Một lớp Block-Recurrent Transformer mở rộng cơ chế sliding-window attention bằng cách thêm một tập hợp các trạng thái hồi quy, được cập nhật ở cuối mỗi khối W token. Thiết kế của chúng tôi cho tế bào hồi quy được minh họa trong Hình 1, mô tả các phép toán được thực hiện trong một khối duy nhất của chuỗi đầu vào.

Tế bào hồi quy nhận hai tensor làm đầu vào: một tập hợp W token embedding, trong đó W là kích thước khối/cửa sổ, và một tập hợp S vector "trạng thái hiện tại". Tế bào tạo ra hai tensor làm đầu ra: một tập hợp W embedding đầu ra, cũng như một tập hợp S vector "trạng thái tiếp theo". Chúng tôi ký hiệu hàm từ token embedding đầu vào đến token embedding đầu ra là hướng dọc, và hàm từ vector trạng thái hiện tại đến vector trạng thái tiếp theo là hướng ngang.

Số lượng vector trạng thái S và kích thước cửa sổ W là các siêu tham số độc lập, nhưng chúng tôi đặt S = W = 512 trong các thí nghiệm để đơn giản hóa so sánh với baseline.

Hướng dọc của tế bào là một lớp transformer thông thường với phép toán cross-attention bổ sung, giống như một lớp decoder trong kiến trúc encoder-decoder tiêu chuẩn [36]. Nó thực hiện self-attention qua các token đầu vào, và cross-attend đến các trạng thái hồi quy. Không giống như một lớp decoder điển hình, chúng tôi thực hiện self-attention và cross-attention song song. Kết quả của cả hai dạng attention được nối lại với nhau và đưa vào một phép chiếu tuyến tính.

Hướng ngang của tế bào phản chiếu hướng tiến, ngoại trừ việc nó thực hiện self-attention qua các vector trạng thái hiện tại, và cross-attend đến các token đầu vào. Hướng hồi quy cũng thay thế các kết nối dư thừa bằng các cổng, cho phép mô hình "quên", một khả năng quan trọng cho các tác vụ thuật toán [37], hoặc khi xử lý các tài liệu dài, nơi nó đã trở thành trung tâm của thành công của LSTM [38].

Lưu ý rằng sự hiện diện của các cổng là lý do tại sao self-attention và cross-attention được thực hiện song song. Thực hiện chúng tuần tự, như thực hành tiêu chuẩn, sẽ giới thiệu một cổng thứ ba trong hướng ngang, dẫn đến perplexity tệ hơn trong các thí nghiệm của chúng tôi.

Hồi quy được tích hợp với cơ chế sliding window attention. Mặc dù không được hiển thị trong Hình 1, mỗi tế bào cũng nhận key và value từ khối trước làm đầu vào, chúng được nối với (Ke, Ve) từ khối hiện tại để triển khai sliding-window attention.

Một lớp Block-Recurrent Transformer xử lý các khối trong một đoạn tuần tự bằng cách xếp chồng các tế bào hồi quy theo chiều ngang, với đầu ra "trạng thái tiếp theo" của tế bào trước đưa vào đầu vào "trạng thái hiện tại" của tế bào tiếp theo. Trong mã, điều này được triển khai như một vòng lặp for đơn giản qua các khối. Nhiều lớp cũng có thể được xếp chồng theo chiều dọc theo cách thông thường. Các thí nghiệm của chúng tôi sử dụng một lớp hồi quy duy nhất, được đặt giữa một số lớp không hồi quy sử dụng sliding-window attention.

Tập hợp vector trạng thái cuối cùng từ khối cuối cùng trong đoạn được cached, cùng với key và value, và được sử dụng làm trạng thái ban đầu cho khối đầu tiên trong bước huấn luyện tiếp theo. Mỗi lớp trong stack (cả hồi quy và không hồi quy) có cache riêng của nó.

4

--- TRANG 5 ---
Chia sẻ key và value. Key và value được chia sẻ giữa hướng dọc và hướng ngang. Một tập hợp key và value (Ke; Ve) được tính toán từ token embedding đầu vào, và một tập hợp key và value khác (Ks; Vs) được tính toán từ vector trạng thái hồi quy. Query không được chia sẻ, vì vậy có bốn tập hợp query riêng biệt: Qv e và Qv s trong hướng dọc, và Qh s và Qh e trong hướng ngang.

3.2 State ID và Position Bias
Với số lượng lớn vector trạng thái, tổng kích thước của trạng thái hồi quy lớn hơn nhiều so với LSTM. Tuy nhiên, cùng một trọng số (ma trận chiếu và MLP) được áp dụng cho mỗi vector trạng thái. Nếu không có cách nào để phân biệt các trạng thái, mô hình sẽ tính toán cùng một kết quả cho mỗi vector trạng thái, do đó phủ nhận bất kỳ lợi thế nào từ việc có nhiều trạng thái. Để ngăn chặn chế độ lỗi này, chúng tôi thêm một tập hợp "state ID" đã học vào các vector trạng thái trước khi tính toán key, value, và query. Những "state ID" này cho phép mỗi vector trạng thái nhất quán đưa ra các query khác nhau đối với chuỗi đầu vào và đối với các trạng thái khác. State ID giống hệt với learned position embedding; chúng tôi sử dụng tên khác vì không có khái niệm "vị trí" giữa các trạng thái.

Chúng tôi không thêm global position embedding vào token, vì global position embedding không hoạt động tốt cho các chuỗi dài [34]. Thay vào đó, chúng tôi thêm relative position bias kiểu T5 [39] vào ma trận self-attention trong hướng dọc. (Mặc dù tương tự, relative position của T5 hơi khác so với relative position được sử dụng trong bài báo Transformer-XL [34].) Khi các trạng thái hồi quy cross-attend đến token đầu vào, không có position bias, vì khoảng cách tương đối giữa "trạng thái" và "token" không được định nghĩa.

Chúng tôi cũng chuẩn hóa query và key như được mô tả trong [40]; chúng tôi thấy rằng chuẩn hóa cải thiện tính ổn định của Transformer-XL khi được sử dụng với relative position bias.

3.3 Loại cổng
Chúng tôi thử nghiệm với hai cơ chế gating khác nhau cho tế bào hồi quy. Mỗi vector trạng thái có cổng riêng, nhưng tất cả vector trạng thái được cập nhật song song, sử dụng các phương trình dưới đây.

Cổng cố định. Cổng cố định sử dụng kết hợp lồi đã học, tương tự như highway networks [41].
zt = Wz ht + bz (1)
g = σ(bg) (2)
ct+1 = ct ⊙ g + zt ⊙ (1 − g) (3)

trong đó Wz là ma trận trọng số có thể huấn luyện, bz và bg là vector bias có thể huấn luyện, σ là hàm sigmoid, ct là trạng thái tế bào cho khối hiện tại (tức là trạng thái cho khối tại chỉ số t trong chuỗi các khối), ⊙ là phép nhân theo phần tử, và ht là đầu vào hiện tại vào cổng. Trong mô hình của chúng tôi, ht là đầu ra của attention, trong trường hợp đó Wz là phép chiếu tuyến tính đưa vào cổng, hoặc ht là đầu ra của lớp ẩn của MLP, trong trường hợp đó Wz là lớp cuối cùng của MLP.

Không giống như highway networks, bias bg là một vector đã học đơn giản có hình dạng Rd, được broadcast qua tất cả vector trạng thái, trong đó d là chiều embedding trạng thái. Giá trị của g không phụ thuộc vào giá trị hiện tại của vector trạng thái ct, hoặc đầu vào hiện tại ht, và do đó vẫn không đổi (tức là cố định) sau huấn luyện. Cổng cố định về cơ bản triển khai trung bình động mũ qua các khối trước.

Cổng LSTM. Cổng LSTM sử dụng kết hợp tiêu chuẩn của input gate và forget gate:
zt = tanh(Wz ht + bz) (4)
it = σ(Wi ht + bi + 1) (5)
ft = σ(Wf ht + bf + 1) (6)
ct+1 = ct ⊙ ft + zt ⊙ it (7)

trong đó Wz; Wi; Wf là các ma trận trọng số có thể huấn luyện, và bz; bi; bf là các vector bias có thể huấn luyện. Cổng LSTM biểu cảm hơn nghiêm ngặt, vì giá trị của ft và it phụ thuộc vào đầu vào hiện tại ht. Trong mô hình của chúng tôi, ht phụ thuộc vào ct, vì vậy cổng LSTM cũng phụ thuộc gián tiếp vào ct. Giá trị cổng LSTM do đó khác nhau cho mỗi vector trạng thái, và cho mỗi chỉ số khối t.

5

--- TRANG 6 ---
3.4 Khởi tạo cổng và tính ổn định huấn luyện
Chúng tôi quan sát thấy rằng tính ổn định huấn luyện khá nhạy cảm với cách các cổng được khởi tạo. Hồi quy có chế độ lỗi trong đó mô hình học cách hoàn toàn bỏ qua trạng thái hồi quy, trong trường hợp đó hiệu suất của nó trở về của transformer không hồi quy. Hơn nữa, tình huống này dường như là một tối ưu cục bộ; một khi mô hình đã đạt đến điểm này, nó không phục hồi. Chúng tôi ổn định huấn luyện bằng cách khởi tạo trọng số và bias thành các giá trị nhỏ nhưng khác không, và thêm hằng số -1 và +1 vào input gate và forget gate để thiên về "nhớ". Xem Phụ lục B để biết chi tiết.

3.5 Cấu hình cổng
Chúng tôi thử nghiệm với ba cấu hình cổng khác nhau.

Dual. Cấu hình dual gate là cấu hình được hiển thị trong Hình 1, trong đó cả hai kết nối dư thừa trong tế bào đều được thay thế bằng cổng. Nhược điểm của cấu hình này là có hai cổng, cả hai đều có thể quên.

Single. Cấu hình single gate loại bỏ phép chiếu tuyến tính và cổng gắn với nó. Thay vào đó, việc nối self-attention và cross-attention được đưa trực tiếp vào MLP.

Skip. Cấu hình skip loại bỏ MLP và cổng gắn với nó. Cấu hình này tương tự như phiên bản single-gate, ngoại trừ việc nó yếu hơn nghiêm ngặt. Thay vì MLP hai lớp với lớp ẩn rất lớn, nó sử dụng phép chiếu tuyến tính không có tính phi tuyến.

3.6 Vị trí của hồi quy và chi phí tính toán
Lớp hồi quy đơn lẻ. Phiên bản cơ bản của Block-Recurrent Transformer sử dụng một lớp hồi quy duy nhất được đặt giữa một số lớp transformer không hồi quy với sliding attention. Chúng tôi sử dụng mô hình 12 lớp với hồi quy ở lớp 10. Tất cả các lớp có cache kiểu Transformer-XL.

Chi phí của hồi quy. Trong quá trình huấn luyện, Block-Recurrent Transformer 12 lớp có chi phí tính toán gần như chính xác, về cả tham số và FLOP, như mô hình Transformer-XL 13 lớp không có hồi quy. Hai mô hình tương đương vì tế bào hồi quy thực hiện gần như các phép toán giống như một lớp transformer thông thường, chỉ khác ở hướng ngang thay vì hướng dọc.

Chi phí suy luận cho autoregressive decoding cũng gần như giống hệt, vì cùng lý do. Hồi quy thêm một phép toán attention bổ sung mỗi token, chi phí của nó giống như self-attention trong lớp thứ 13.

4 Kết quả
Chúng tôi đã thử nghiệm Block-Recurrent Transformer trên ba bộ dữ liệu khác nhau của các tài liệu dài: PG19, arXiv, và GitHub. Bộ dữ liệu PG19 [42] chứa các cuốn sách đầy đủ được viết trước năm 1919 từ dự án Gutenberg. Bộ dữ liệu arXiv [11] là một tập hợp các bài báo kỹ thuật được tải xuống qua arXiv Bulk Data Access1, và được lọc để chỉ bao gồm các bài báo được gắn nhãn là "Mathematics" và có nguồn LATEX. Bộ dữ liệu GitHub [11] là một tập hợp mã nguồn từ các kho GitHub khác nhau với giấy phép mã nguồn mở. Tất cả các tệp trong mỗi kho GitHub được nối lại với nhau để tạo thành một tài liệu dài.

Tác vụ là mô hình hóa ngôn ngữ tự hồi quy, mục tiêu là dự đoán token tiếp theo trong chuỗi. Chúng tôi báo cáo số bits-per-token (tức là log2 perplexity; thấp hơn là tốt hơn) cho tất cả mô hình. Chi tiết huấn luyện thêm cho từng bộ dữ liệu có thể được tìm thấy trong Phụ lục C.

4.1 Baseline
Chúng tôi so sánh Block-Recurrent Transformer với năm baseline khác nhau. Baseline đầu tiên, XL:512, thiết lập một điểm tham chiếu để so sánh các cải tiến khác. Đó là

1https://arxiv.org/help/bulk_data

6

--- TRANG 7 ---
Bảng 1: Trung bình bits-per-token (log2 perplexity) của mỗi mô hình. Các mô hình hồi quy (được đặt tên Rec:gate:config) có cùng chi phí tính toán như baseline Slide:13L, nhưng perplexity tốt hơn nhiều. Chúng thậm chí vượt trội hơn baseline XL:2048, trong khi chạy nhanh hơn gấp đôi. Thanh lỗi đo được trên PG19 thấp, giữa 0.002 và 0.007, nhưng được làm tròn lên 0.01 để phù hợp với độ chính xác của kết quả trong bảng. Thời gian bước cho một bước huấn luyện duy nhất (thấp hơn là tốt hơn). Đối với PG19, chúng tôi huấn luyện cả mô hình character-level (byte) và token-level.

Mô hình      độ dài đoạn  độ dài cửa sổ  thời gian bước   PG19        arXiv  GitHub
                                     (tương đối)    byte   token   token  token
XL:512       512         512        0.88        1.01   3.62±0.01  1.45   1.21
XL:1024      1024        1024       1.20        0.997  3.59±0.01  1.37   1.08
XL:2048      2048        2048       2.11        0.990  3.58±0.01  1.31   1.01
Slide:12L    4096        512        0.93        0.989  3.60       1.43   1.19
Slide:13L                           1.00        0.989  3.58±0.01  1.42   1.17
Rec:lstm:dual 4096       512        1.06        0.985  3.54±0.01  1.26   1.01
Rec:lstm:single                     1.05        0.962  3.54±0.01  1.29   1.03
Rec:lstm:skip                       1.00        0.969  3.56±0.01  1.31   1.10
Rec:fixed:dual                      1.01        0.957  3.52±0.01  1.27   0.991
Rec:fixed:single                    1.02        0.966  3.58±0.01  1.25   1.00
Rec:fixed:skip                      0.99        0.952  3.53±0.01  1.24   0.976
Feedback:lstm:single  4096  512     1.40        0.977  3.50       1.22   -
Feedback:fixed:skip               1.35        0.935  3.49       1.24   -
Memorizing Trans.     64k   512     1.94        0.950  3.53       1.22   -

mô hình Transformer-XL 12 lớp với kích thước cửa sổ 512, và 150 triệu tham số. Nó có 8 head có kích thước 128, vector embedding có kích thước 1024, MLP với lớp ẩn có kích thước 4096, và hàm phi tuyến relu. Nó sử dụng cache kiểu Transformer-XL, nhưng không có cửa sổ trượt, vì vậy độ dài đoạn giống với kích thước cửa sổ, tức là, nó được huấn luyện trên các đoạn 512 token.

XL:1024 và XL:2048 tương tự, nhưng có kích thước cửa sổ lần lượt là 1024 và 2048. Như mong đợi, tăng kích thước cửa sổ cải thiện perplexity, đặc biệt trên bộ dữ liệu arXiv. Tuy nhiên, hai mô hình này vẫn có perplexity tệ hơn mô hình hồi quy, cũng như chậm hơn nhiều.

Slide:12L là transformer 12 lớp với kích thước cửa sổ 512, nhưng sử dụng cửa sổ trượt qua một đoạn 4096 token. Mô hình này gần như giống hệt với XL:512; sự khác biệt duy nhất là cửa sổ trượt có khả vi qua nhiều khối, trong khi cache Transformer-XL thì không.

Slide:13L thêm lớp thứ 13, và có thể so sánh trực tiếp với các mô hình hồi quy về mặt chi phí tính toán (FLOP hoặc step-time), số lượng tham số, và độ dài đoạn. Lưu ý rằng việc thêm một lớp khác với nhiều tham số hơn mang lại cải thiện nhỏ hơn nhiều so với việc thêm hồi quy.

Chi phí tương đối. Tất cả năm baseline, và tất cả 6 mô hình hồi quy, có số lượng tham số gần như giống nhau: giữa 151 triệu (12 lớp) và 164 triệu (13 lớp hoặc hồi quy). Tốc độ huấn luyện (tức là thời gian bước) của mỗi mô hình được hiển thị trong Bảng 1 (thấp hơn là tốt hơn). Vì thời gian bước thô phụ thuộc vào phần cứng và trình biên dịch, chúng tôi báo cáo số liệu tương đối so với baseline Slide:13L.

Kích thước batch. Chúng tôi điều chỉnh kích thước batch để mỗi mô hình xử lý cùng số lượng token (và do đó cùng lượng dữ liệu huấn luyện) mỗi bước huấn luyện. Do đó, XL:512 (độ dài đoạn 512) chạy với kích thước batch 256 (8 mỗi replica), trong khi Slide:12L (độ dài đoạn 4096) chạy với kích thước batch 32 (1 mỗi replica) trên PG19.

4.2 Lợi ích của hồi quy
Chúng tôi so sánh 5 baseline với tất cả sáu cấu hình cổng cho Block-Recurrent Transformer. Mô hình hồi quy vượt trội đáng tin cậy hơn tất cả năm baseline. Cấu hình tốt nhất tổng thể là Rec:fixed:skip, vượt trội hơn các mô hình khác trong 3 trên 4 trường hợp, và nằm trong biên sai số trong trường hợp còn lại. Điều này đặc biệt đáng chú ý vì nó cũng là cấu hình nhanh nhất, có thời gian bước thấp hơn một chút và ít tham số hơn Slide:13L, vì nó không có

7

--- TRANG 8 ---
Hình 3: Scaling của Block-Recurrent Transformer 12 lớp so với Transformer-XL 13 lớp trên PG19. FLOP giống nhau giữa hai mô hình ở một số lượng tham số nhất định. Ở kích thước lớn hơn, việc thêm hồi quy tương đương với việc tăng gấp đôi số lượng tham số. Chi tiết trong Phụ lục F.

MLP. Nó tốt hơn baseline 13 lớp với biên độ lớn, và thậm chí còn tốt hơn mô hình Transformer-XL với kích thước cửa sổ 2048, chạy chậm hơn 2 lần.

Các cấu hình cổng khác cũng vượt trội hơn baseline 13 lớp, nhưng thứ hạng tương đối của chúng thay đổi theo bộ dữ liệu. Mặc dù mạnh hơn về lý thuyết, cổng LSTM có xu hướng tụt hậu so với cổng cố định trong tất cả các thí nghiệm của chúng tôi.

Scaling lên. Hình 3 hiển thị hiệu ứng của việc thêm hồi quy khi mô hình transformer được mở rộng lên và xuống về kích thước. Chúng tôi đã huấn luyện sáu mô hình khác nhau trên PG19, có kích thước từ 40M tham số đến 1.3B tham số. Đối với bốn mô hình nhỏ hơn, chúng tôi so sánh Block-Recurrent Transformer 12 lớp với baseline Transformer-XL 13 lớp, trong khi đối với hai mô hình lớn hơn, chúng tôi so sánh Block-Recurrent Transformer 24 lớp, với hồi quy ở lớp 10 và 20, với baseline Transformer-XL 26 lớp. Thí nghiệm này sử dụng learning rate cosine-decay như được mô tả trong [43], và từ vựng SentencePiece 32k tùy chỉnh [44]. Chi tiết thêm trong Phụ lục F.

Các thí nghiệm của chúng tôi cho thấy rằng hồi quy mang lại lợi ích nhất quán trên tất cả các quy mô. Cải thiện tương đối thực sự dường như tăng theo số lượng tham số; ở kích thước lớn hơn, hồi quy mang lại lợi ích lớn hơn việc tăng gấp đôi số lượng tham số.

4.3 Ablation
Nhiều lớp hồi quy. Việc thêm hai lớp hồi quy ngay cạnh nhau trong stack (lớp 9 và 10) không cải thiện perplexity mô hình. Việc thêm hai lớp cách xa nhau trong stack (lớp 4 và 10) đã mang lại cải thiện, nhưng cải thiện không tốt hơn việc chỉ đơn giản thêm một lớp không hồi quy khác vào stack. Công trình trước đây về Memorizing Transformers [11] cho thấy hiệu ứng tương tự. Trong nghiên cứu định tính của chúng tôi, chúng tôi thấy rằng mô hình dường như sử dụng hồi quy chủ yếu cho việc tra cứu tên tầm xa, giống như bộ nhớ. Chúng tôi kết luận rằng một lớp hồi quy là đủ để mô hình trích xuất hầu hết các lợi ích, mặc dù chúng tôi đã sử dụng hai lớp cho các mô hình lớn nhất.

Số lượng vector trạng thái hồi quy. Chúng tôi đã huấn luyện mô hình với số lượng vector trạng thái khác nhau, từ 128 đến 2048. Tăng số lượng trạng thái tạo ra cải thiện nhỏ nhưng có thể đo được đến 1024, nhưng mô hình làm tệ hơn với 2048 (xem Phụ lục D). Chúng tôi giả thuyết rằng mô hình gặp khó khăn trong việc học sử dụng trạng thái hồi quy hiệu quả nếu không gian trạng thái phát triển quá lớn.

Giảm kích thước cửa sổ. Giảm kích thước cửa sổ trượt làm giảm perplexity đáng kể đối với Transformer-XL, vì nó giảm lượng ngữ cảnh mà transformer có thể chú ý đến. Giảm kích thước cửa sổ trong transformer hồi quy có hiệu ứng nhỏ hơn, vì mô hình có thể sử dụng hồi quy để bù đắp (xem Phụ lục D).

4.4 Block feedback
Lấy cảm hứng từ feedback transformer [24], cho phép tất cả các lớp chú ý đến lớp cao nhất, chúng tôi đã triển khai một biến thể trong đó mọi lớp của transformer (không chỉ lớp hồi quy) có thể cross-attend đến các vector trạng thái trong lớp hồi quy. Biến thể này cải thiện thêm perplexity, nhưng

8

--- TRANG 9 ---
Bảng 2: So sánh với công trình đã xuất bản khác trên PG19. Các trường được đánh dấu - là không biết.

Mô hình                        Lớp  perplexity  tham số   kích thước từ vựng
                                              word-level
Compressive Transformer [15]    36   33.6       -         32k
Routing Transformer [7]         22   33.2       490M      198k
Perceiver AR [45]              60   28.9       974.6M    132k
Block-Recurrent Transformer    24   28.46      650M      32k
Block-Recurrent Transformer    24   26.50      1.3B      32k

với một chi phí; thời gian bước tăng khoảng 35-40%, và các query bổ sung cũng tăng số lượng tham số. Kết quả được hiển thị trong Bảng 1, và được mô tả thêm trong Phụ lục E.

4.5 So sánh với công trình đã xuất bản trước đây
Tập test PG19 chứa 6,966,499 từ [15], được chia thành 10,229,476 token sử dụng từ vựng SentencePiece, được huấn luyện trên PG19. Mô hình 24 lớp 1.3B tham số của chúng tôi đạt 3.22 bit mỗi token, và do đó đạt word-level perplexity state-of-the-art mới là 26.50 (Bảng 2). Tuy nhiên, chúng tôi lưu ý rằng số perplexity thô không nhất thiết là cách có ý nghĩa để so sánh kiến trúc, vì chúng phụ thuộc vào nhiều yếu tố khác, như số lượng tham số, từ vựng, lịch trình learning rate, kích thước batch, v.v.; một thảo luận chi tiết hơn trong Phụ lục C.3.

Chúng tôi có thể chạy so sánh công bằng (từ vựng, cấu hình và siêu tham số giống hệt) của Block-Recurrent Transformer với Memorizing Transformer [11], với bộ nhớ kích thước 64k (Bảng 1). Memorizing transformer được xây dựng tương tự như mô hình của chúng tôi; nó có một lớp đã được tăng cường với cơ chế cho phép nó chú ý qua khoảng cách dài hơn nhiều. Chúng tôi thấy rằng Block-Recurrence làm gần như tốt bằng Memorizing Transformer trên arXiv, và làm tốt bằng nhau trên PG19, nhưng huấn luyện nhanh gần gấp đôi. Tuy nhiên, có nhiều cách triển khai k-nearest-neighbor lookup xấp xỉ, vì vậy tốc độ tương đối sẽ rất phụ thuộc vào triển khai; triển khai của chúng tôi chạy trên TPU, và không sử dụng kernel CUDA tùy chỉnh.

4.6 Phân tích định tính
Công trình trước đây về transformer ngữ cảnh dài [42,11] đã phát hiện ra rằng attention ở tầm xa thường được sử dụng để tra cứu tên riêng, như nhân vật hoặc địa danh. Chúng tôi thực hiện phân tích định tính nhằm xác định liệu mô hình của chúng tôi có đang sử dụng hồi quy theo cách tương tự hay không. Chúng tôi chọn ngẫu nhiên 5 cuốn sách từ tập test PG19, chạy cả Block-Recurrent Transformer và Transformer-XL 13 lớp trên mỗi cuốn sách, và sau đó so sánh cross-entropy loss cho tất cả token. Chúng tôi sắp xếp kết quả, và kiểm tra 4 token hàng đầu từ mỗi cuốn sách với sự khác biệt lớn nhất: các token mà dự đoán của mô hình hồi quy có cải thiện lớn nhất so với baseline.

Trong 17/20 trường hợp, mô hình hồi quy dự đoán một tên riêng, thường với xác suất tương đối cao, mà Transformer-XL không thể dự đoán. Trong 2 trường hợp nó dự đoán tiêu đề chương (đã thấy trước đó trong mục lục), và trong trường hợp cuối cùng, nó dự đoán một từ ngoại ngữ độc nhất cho cuốn sách đó. Trong 19/20 trường hợp, từ được dự đoán không ở đâu trong cửa sổ attention, vì vậy nó phải được lưu trữ trong trạng thái hồi quy (chi tiết trong phụ lục, Mục G).

Trong nghiên cứu thứ hai, chúng tôi so sánh mô hình hồi quy, chạy bình thường, với một biến thể trong đó trạng thái hồi quy được xóa ở cuối mỗi đoạn 4096 token, thay vì được cached. Xóa trạng thái làm suy giảm khả năng dự đoán các phụ thuộc ở tầm xa hơn độ dài đoạn của mô hình; các dự đoán sai điển hình một lần nữa bao gồm tên riêng và tiêu đề chương. Thú vị là, nghiên cứu này cũng cho thấy rằng mô hình hồi quy có thể nhớ tiêu đề và tác giả của một cuốn sách (là một phần của boilerplate Gutenberg ở đầu và cuối mỗi cuốn sách) qua toàn bộ độ dài của cuốn sách - hơn 60,000 token. Xem Phụ lục G.1.

So sánh định lượng thêm về cross-entropy mỗi token giữa Transformer-XL và Block-Recurrent Transformer được đưa ra trong Phụ lục H.

1Thông tin cá nhân.

9

--- TRANG 10 ---
5 Thảo luận
Việc triển khai hồi quy của chúng tôi được lấy cảm hứng từ cách con người dường như xử lý các chuỗi dài. Khi một người đọc một cuốn tiểu thuyết, họ không cố gắng nhớ từng từ đơn lẻ trong cuốn sách. Thay vào đó, một độc giả sẽ xây dựng một mô hình tinh thần, hoặc đồ thị kiến thức, tóm tắt câu chuyện cho đến nay, tức là tên của các nhân vật chính, mối quan hệ giữa họ, và bất kỳ điểm cốt truyện chính nào. Khi một người đọc một đoạn văn bản, họ sẽ phân tích thông tin trong đoạn văn, xử lý và diễn giải thông tin bằng kiến thức nền từ mô hình tinh thần của họ, và cuối cùng cập nhật mô hình tinh thần của họ với thông tin mới. Kiến trúc hồi quy của chúng tôi mô phỏng quá trình này một cách lỏng lẻo. Nó lấy một khối văn bản, và phân tích nó bằng cách chạy nó qua một stack transformer thông thường. Token trong văn bản chú ý đến các trạng thái hồi quy (tức là mô hình tinh thần), và các trạng thái, đến lượt mình, được cập nhật bằng cách chú ý đến văn bản.

Dựa trên phân tích định tính của chúng tôi, dường như mô hình thực sự đang sử dụng trạng thái hồi quy để tóm tắt một số thông tin về các nhân vật và địa danh thường xuất hiện. Tuy nhiên, nó dường như không thực hiện nhiều lý luận phức tạp, được chứng minh bởi thực tế là mô hình hoạt động tốt nhất của chúng tôi là cấu hình fixed:skip. Cấu hình này không sử dụng cổng kiểu LSTM phức tạp, chọn nhớ hoặc quên dựa trên trạng thái hiện tại và đầu vào; thay vào đó, nó chỉ đơn giản tính toán trung bình động mũ, không khác nhiều so với một số dạng attention tầm xa xấp xỉ khác.

Hơn nữa, cấu hình skip cắt bỏ MLP lớn từ lớp transformer hồi quy. Trong kiến trúc transformer vanilla, việc loại bỏ MLP từ tất cả các lớp sẽ làm suy giảm nghiêm trọng mô hình [46]; những MLP lớn đó đang tính toán điều gì đó quan trọng. Trong một lớp hồi quy, việc loại bỏ MLP tạo ra ít sự khác biệt; nó dường như không tính toán bất cứ điều gì hữu ích. Chúng tôi kết luận rằng việc huấn luyện lớp hồi quy để sử dụng đầy đủ khả năng trích xuất kiến thức và tóm tắt của nó sẽ đòi hỏi những tiến bộ hơn nữa.

5.1 Đạo đức
Các tác động xã hội tiêu cực tiềm ẩn từ công trình này tương tự như bất kỳ tiến bộ nào khác trong mô hình hóa ngôn ngữ. Các mô hình ngôn ngữ lớn có thể được sử dụng để tạo ra thông tin sai lệch và tin tức giả, cung cấp năng lượng cho chatbot độc hại, hoặc tạo ra spam. Block-Recurrent Transformer có thể tạo ra các tài liệu dài hơn so với trước đây, do đó mở rộng phạm vi ứng dụng mà các tác động tiêu cực này có thể xảy ra. Cách tốt nhất để giảm thiểu những rủi ro này là huấn luyện các mô hình có thể lý luận về văn bản, và gắn cờ thông tin sai lệch hoặc nội dung độc hại.

6 Kết luận
Chúng tôi đã chỉ ra rằng khi huấn luyện mô hình ngôn ngữ trên các tài liệu dài, Block-Recurrent Transformer mang lại lợi ích lớn hơn với chi phí thấp hơn so với việc mở rộng mô hình transformer theo các cách khác. Việc thêm hồi quy vào một lớp duy nhất có chi phí gần như giống với việc thêm một lớp không hồi quy bổ sung, nhưng dẫn đến cải thiện perplexity lớn hơn nhiều. Chúng tôi cũng đã chỉ ra rằng hồi quy mang lại lợi ích lớn hơn việc chỉ đơn giản tăng kích thước cửa sổ attention, hoặc tăng số lượng tham số. Mô hình kích thước trung bình của chúng tôi có perplexity thấp hơn mô hình Transformer-XL với kích thước cửa sổ gấp 4 lần, nhưng chạy nhanh gấp đôi, và mô hình lớn hơn của chúng tôi vượt trội hơn mô hình Transformer-XL với số lượng tham số gấp đôi.

Hơn nữa, trái với một số biến thể transformer được đề xuất gần đây khác, Recurrent Transformer rất dễ triển khai, vì nó chủ yếu bao gồm các thành phần transformer thông thường và cổng RNN. Không cần kernel CUDA tùy chỉnh. Mã của chúng tôi đã được công bố dưới dạng mã nguồn mở [1].

Đánh giá block-recurrent transformer trên các tác vụ downstream là một hướng quan trọng cho công trình tương lai. Chúng tôi tin rằng Block-Recurrent Transformer sẽ hữu ích nhất trong các tình huống đòi hỏi ngữ cảnh tầm xa; ví dụ về các ứng dụng tiềm năng bao gồm viết báo cáo sách, tóm tắt bài báo dài, hoàn thành mã, hoặc hỏi đáp trên các tác phẩm dài như sách. Có một số benchmark mới và đang nổi lên kiểm tra hiệu suất tầm xa [47,48,4]. Các nghiên cứu trước đây đã tìm thấy mối tương quan mạnh mẽ giữa mô hình hóa ngôn ngữ và các tác vụ downstream đa dạng [49, 50].

Mặc dù có những thành công ban đầu, chúng tôi cũng tin rằng kiến trúc hồi quy mà chúng tôi trình bày ở đây chưa đạt được tiềm năng đầy đủ của nó, và có cơ hội cho nghiên cứu tương lai và cải tiến hơn nữa trong lĩnh vực này.

10

--- TRANG 11 ---
Tài liệu tham khảo
[1] D. Hutchins, M. Rabe, Y. Wu, I. Schlag, và C. Staats, "Meliad." Github source code repository. https://github.com/google-research/meliad, 2022.

[2] S. Hochreiter và J. Schmidhuber, "Long short-term memory," Neural Computation, vol. 9, no. 8, 1997.

[3] U. Khandelwal, H. He, P. Qi, và D. Jurafsky, "Sharp nearby, fuzzy far away: How neural language models use context," trong Association for Computational Linguistics, 2018.

[4] Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, và D. Metzler, "Long range arena: A benchmark for efficient transformers," trong International Conference on Learning Representations, 2021.

[5] Y. Tay, M. Dehghani, D. Bahri, và D. Metzler, "Efficient transformers: A survey," arXiv preprint arXiv:2009.06732, 2020.

[6] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontañón, P. Pham, A. Ravula, Q. Wang, L. Yang, và A. Ahmed, "Big bird: Transformers for longer sequences," trong NeurIPS, 2020.

[7] A. Roy, M. Saffar, A. Vaswani, và D. Grangier, "Efficient content-based sparse attention with routing transformers," Transactions of the Association for Computational Linguistics, vol. 9, pp. 53–68, 2021.

[8] N. Kitaev, L. Kaiser, và A. Levskaya, "Reformer: The efficient transformer," trong International Conference on Learning Representations, 2020.

[9] J. Ainslie, S. Ontañón, C. Alberti, V. Cvicek, Z. Fisher, P. Pham, A. Ravula, S. Sanghai, Q. Wang, và L. Yang, "ETC: encoding long and structured inputs in transformers," trong EMNLP, 2020.

[10] S. Sukhbaatar, D. Ju, S. Poff, S. Roller, A. Szlam, J. Weston, và A. Fan, "Not all memories are created equal: Learning to forget by expiring," trong ICML, 2021.

[11] Y. Wu, M. Rabe, D. Hutchins, và C. Szegedy, "Memorizing transformers," trong ICLR, 2022.

[12] Z. Zhu và R. Soricut, "H-transformer-1d: Fast one-dimensional hierarchical attention for sequences," trong ACL (C. Zong, F. Xia, W. Li, và R. Navigli, eds.), 2021.

[13] H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, và B. Dai, "Combiner: Full attention transformer with sparse computation cost," trong Advances in Neural Information Processing Systems (A. Beygelzimer, Y. Dauphin, P. Liang, và J. W. Vaughan, eds.), 2021.

[14] S. Wang, B. Z. Li, M. Khabsa, H. Fang, và H. Ma, "Linformer: Self-attention with linear complexity," CoRR, vol. abs/2006.04768, 2020.

[15] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, và T. P. Lillicrap, "Compressive transformers for long-range sequence modelling," trong ICLR, 2020.

[16] Z. Dai, G. Lai, Y. Yang, và Q. Le, "Funnel-transformer: Filtering out sequential redundancy for efficient language processing," trong NeurIPS, 2020.

[17] A. Katharopoulos, A. Vyas, N. Pappas, và F. Fleuret, "Transformers are RNNs: Fast autoregressive transformers with linear attention," trong ICML, 2020.

[18] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlós, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, và A. Weller, "Rethinking attention with performers," trong ICLR, 2021.

[19] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, và L. Kong, "Random feature attention," trong ICLR, 2021.

[20] I. Schlag, K. Irie, và J. Schmidhuber, "Linear Transformers are secretly fast weight programmers," trong ICML, 2021.

11

--- TRANG 12 ---
[21] W. Hua, Z. Dai, H. Liu, và Q. V. Le, "Transformer quality in linear time," arXiv preprint arXiv:2202.10447, 2022.

[22] J. Schmidhuber, "Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets," trong International Conference on Artificial Neural Networks (ICANN), 1993.

[23] K. Irie, I. Schlag, R. Csordás, và J. Schmidhuber, "Going beyond linear transformers with recurrent fast weight programmers," trong confNEU, 2021.

[24] A. Fan, T. Lavril, E. Grave, A. Joulin, và S. Sukhbaatar, "Addressing some limitations of transformers with feedback memory," arXiv preprint arXiv:2002.09402, 2020.

[25] T. Lei, Y. Zhang, S. I. Wang, H. Dai, và Y. Artzi, "Simple recurrent units for highly parallelizable recurrence," trong EMNLP, 2018.

[26] T. Lei, "When attention meets fast recurrence: Training language models with reduced compute," trong EMNLP, 2021.

[27] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. F. Foster, L. Jones, M. Schuster, N. Shazeer, N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, Z. Chen, Y. Wu, và M. Hughes, "The best of both worlds: Combining recent advances in neural machine translation," trong ACL, 2018.

[28] V. J. Hellendoorn, P. Maniatis, R. Singh, C. Sutton, và D. Bieber, "Global relational models of source code," trong ICLR, 2020.

[29] Z. Wang, Y. Ma, Z. Liu, và J. Tang, "R-transformer: Recurrent neural network enhanced transformer," arXiv preprint arXiv:1907.05572, 2019.

[30] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, và J. Carreira, "Perceiver: General perception with iterative attention," trong icml, 2021.

[31] A. Al Adel và M. S. Burtsev, "Memory transformer with hierarchical attention for long document processing," trong 2021 International Conference Engineering and Telecommunication (EnT), 2021.

[32] D. Ju, S. Roller, S. Sukhbaatar, và J. Weston, "Staircase attention for recurrent processing of sequences," arXiv preprint arXiv:2106.04279, 2021.

[33] I. Beltagy, M. E. Peters, và A. Cohan, "Longformer: The long-document transformer," arXiv preprint arXiv:2004.05150, 2020.

[34] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, và R. Salakhutdinov, "Transformer-XL: Attentive language models beyond a fixed-length context," trong ACL, 2019.

[35] R. J. Williams và J. Peng, "An efficient gradient-based algorithm for on-line training of recurrent network trajectories," Neural Computation, 1990.

[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Attention is all you need," trong Advances in neural information processing systems, 2017.

[37] R. Csordás, K. Irie, và J. Schmidhuber, "The neural data router: Adaptive control flow in transformers improves systematic generalization," trong International Conference on Learning Representations, 2022.

[38] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, và J. Schmidhuber, "LSTM: A search space odyssey," IEEE transactions on neural networks and learning systems, vol. 28, no. 10, 2016.

[39] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, và P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," Journal of Machine Learning Research, 2020.

12

--- TRANG 13 ---
[40] A. Henry, P. R. Dachapally, S. S. Pawar, và Y. Chen, "Query-key normalization for transformers," trong EMNLP, 2020.

[41] R. K. Srivastava, K. Greff, và J. Schmidhuber, "Training very deep networks," trong NIPS (C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, và R. Garnett, eds.), 2015.

[42] S. Sun, K. Krishna, A. Mattarella-Micke, và M. Iyyer, "Do long-range language models actually use long-range context?," trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 807–822, Association for Computational Linguistics, Nov. 2021.

[43] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al., "Training compute-optimal large language models," arXiv preprint arXiv:2203.15556, 2022.

[44] T. Kudo và J. Richardson, "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing," trong EMNLP, 2018.

[45] C. Hawthorne, A. Jaegle, C. Cangea, S. Borgeaud, C. Nash, M. Malinowski, S. Dieleman, O. Vinyals, M. Botvinick, I. Simon, et al., "General-purpose, long-context autoregressive modeling with perceiver ar," arXiv preprint arXiv:2202.07765, 2022.

[46] Y. Dong, J. Cordonnier, và A. Loukas, "Attention is not all you need: pure attention loses rank doubly exponentially with depth," trong ICML (M. Meila và T. Zhang, eds.), 2021.

[47] U. Shaham, E. Segal, M. Ivgi, A. Efrat, O. Yoran, A. Haviv, A. Gupta, W. Xiong, M. Geva, J. Berant, và O. Levy, "Scrolls: Standardized comparison over long language sequences," 2022.

[48] A. Wang, R. Y. Pang, A. Chen, J. Phang, và S. R. Bowman, "Squality: Building a long-document summarization dataset the hard way," arXiv preprint arXiv:2205.11465, 2022.

[49] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei, "Language models are few-shot learners," trong NeurIPS, 2020.

[50] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," arXiv preprint arXiv:2206.04615, 2022.

[51] N. Shazeer và M. Stern, "Adafactor: Adaptive learning rates with sublinear memory cost," trong ICML, 2018.

[52] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., "Lamda: Language models for dialog applications," arXiv preprint arXiv:2201.08239, 2022.

[53] O. Press, N. Smith, và M. Lewis, "Train short, test long: Attention with linear biases enables input length extrapolation," trong International Conference on Learning Representations, 2022.

[54] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., "Towards a human-like open-domain chatbot," arXiv preprint arXiv:2001.09977, 2020.

13

--- TRANG 14 ---
Phụ lục

Phụ lục A Phân tích thêm.

Độ dài ngữ cảnh của một mô hình ngôn ngữ tự hồi quy đề cập đến số lượng token trước đó mà mô hình có thể sử dụng khi dự đoán token tiếp theo. Một transformer vanilla hoạt động trên các đoạn có độ dài N có độ dài ngữ cảnh là 0 cho token đầu tiên, và N-1 cho token cuối cùng, và do đó có độ dài ngữ cảnh trung bình là N/2. Chất lượng dự đoán cho transformer vanilla không đồng đều trên toàn đoạn; nó đưa ra dự đoán kém cho các token gần đầu đoạn do thiếu ngữ cảnh, và dự đoán tốt hơn gần cuối.

Với sliding window attention, mỗi lớp chú ý đến lớp trước đó trong một cửa sổ W token từ vị trí hiện tại. Do đó độ dài ngữ cảnh cho một lớp duy nhất là W, bất kể token xuất hiện ở đâu trong đoạn. Trường hợp đơn giản khi W = N tương ứng với Transformer-XL. Mô hình Transformer-XL đạt được cải thiện lớn trong perplexity trung bình so với mô hình vanilla, chỉ đơn giản vì nó có thể đưa ra dự đoán tốt hơn nhiều cho các token ở đầu mỗi đoạn.

Trong mô hình có nhiều lớp, trường nhận lý thuyết (TRF) được định nghĩa là khoảng cách tối đa mà thông tin có thể lan truyền qua mô hình. Định nghĩa này tương tự như "độ dài ngữ cảnh", nhưng TRF là "lý thuyết", vì mô hình có thể gặp khó khăn trong việc thực sự học cách sử dụng nhiều ngữ cảnh như vậy trong thực tế. Ví dụ, TRF của LSTM là vô hạn, nhưng trong thực tế LSTM gặp khó khăn trong việc truyền thông tin qua hơn vài trăm token. Độ dài ngữ cảnh hiệu quả của LSTM ít hơn nhiều so với TRF có thể gợi ý.

Đối với mô hình sliding-window (hoặc Transformer-XL), TRF là W×L, trong đó L là số lượng lớp. Tuy nhiên, mô hình vẫn chỉ có thể chú ý trực tiếp đến W token trước đó. Mặc dù TRF cao hơn nhiều, việc sử dụng ngữ cảnh bổ sung đòi hỏi nhiều "bước nhảy" attention, điều này khó học hơn cho mô hình. Vấn đề này đặc biệt nghiêm trọng trong Transformer-XL (N = W), vì "bước nhảy" attention đầu tiên sẽ đến key và value trong cache, không khả vi. Mô hình sliding window của chúng tôi (N >> W) có TRF giống hệt với Transformer-XL, nhưng nó có thể vi phân qua nhiều khối, mang lại độ dài ngữ cảnh hiệu quả cao hơn trong thực tế, và do đó perplexity tốt hơn.

TRF của block-recurrent transformer là vô hạn. Chúng tôi cũng chỉ ra rằng độ dài ngữ cảnh hiệu quả cũng dường như khá lớn trong thực tế, vì chúng tôi quan sát các trường hợp mà mô hình có thể dự đoán chính xác thông tin qua khoảng cách hơn 60k token.

A.1 Độ phức tạp tính toán

Độ phức tạp tính toán của attention trong một lớp hồi quy là O((W² + S² + 2SW)N/W), trong đó N là độ dài đoạn, W là độ dài cửa sổ, và S là số lượng trạng thái. N/W là số lượng khối, và mỗi khối thực hiện self attention W², state self-attention S², và attention giữa token/trạng thái và trạng thái/token 2SW.

Độ phức tạp của sliding-window attention là O(W²N/W) = O(WN).

A.2 So sánh giữa ngữ cảnh, hồi quy, và bộ nhớ

Bộ dữ liệu arXiv chứa mã latex, với rất nhiều cú pháp phức tạp có thể hưởng lợi từ attention tầm xa (ví dụ: định lý, thuật ngữ chuyên môn, trích dẫn). Do đó, việc thêm bộ nhớ, hồi quy, hoặc chỉ tăng kích thước cửa sổ của baseline Transformer-XL mang lại lợi ích lớn hơn trên arXiv so với PG19.

Cũng có vẻ như có sự khác biệt định tính trong cách các mô hình sử dụng attention trên các bộ dữ liệu này. Chúng tôi giả thuyết rằng việc xử lý cú pháp phức tạp đòi hỏi attention trực tiếp (một "bước nhảy"), đó là lý do tại sao mô hình XL:2048 hoạt động tốt trên arXiv. Ngược lại, tiểu thuyết ngôn ngữ tự nhiên không có cú pháp phức tạp, nhưng có thể hưởng lợi từ hiểu biết tốt hơn về mối quan hệ tinh tế giữa các từ (ví dụ: từ "cô ấy" đề cập đến nhân vật nào). Những tương tác tinh tế này đòi hỏi attention đa lớp, đa "bước nhảy", dễ huấn luyện hơn trong mô hình Slide:13L có khả vi cao hơn. Trên PG19, Slide:13L thực sự làm tốt hơn XL:2048, mặc dù có kích thước cửa sổ ngắn hơn nhiều, trong khi trên arXiv, tình huống ngược lại.

14

--- TRANG 15 ---
Có thể có mối quan hệ tương tự giữa hồi quy và bộ nhớ kNN. Memorizing Transformer thực hiện attention trực tiếp, một "bước nhảy", sử dụng kNN lookup. Hồi quy, ngược lại, tóm tắt và nén văn bản thành một tập hợp các trạng thái hồi quy.

Cả Memorizing Transformer và Block-Recurrent Transformer đều đạt perplexity rất tương tự, và dựa trên các nghiên cứu định tính, chúng dường như sử dụng bộ nhớ hoặc trạng thái bổ sung chủ yếu cho việc tra cứu tên tầm xa. Tuy nhiên, hồi quy có thể tốt hơn trong việc nắm bắt thông tin tầm xa tinh tế hơn, như phong cách viết, trong khi bộ nhớ tốt hơn trong việc tra cứu chính xác các sự kiện, như trích dẫn. Thực tế là Memorizing Transformer làm tốt hơn Block-Recurrent transformer trên arXiv, nhưng không phải PG19, dường như hỗ trợ giả thuyết này, nhưng cần thêm thí nghiệm trên các tác vụ downstream để xác nhận giả thuyết này.

A.3 So sánh với Longformer

Ý tưởng về sliding window attention được phổ biến bởi Longformer [33], nhưng Longformer đầy đủ là một mô hình phức tạp hơn nhiều so với mô hình Slide mà chúng tôi trình bày ở đây.

Longformer đầy đủ sử dụng một số mẫu attention khác nhau, và sliding-window attention chỉ là một trong số đó. Longformer cũng sử dụng dilated attention và sparse global attention, cả hai đều được triển khai với kernel CUDA tùy chỉnh. Hơn nữa, LongFormer sử dụng kích thước cửa sổ khác nhau trong mỗi lớp, và nó sử dụng chế độ huấn luyện đa giai đoạn của pre-training và fine-tuning, theo một chương trình giảng dạy tăng dần kích thước cửa sổ và độ dài chuỗi. Chúng tôi không làm bất kỳ điều nào trong số này.

Phụ lục B Khởi tạo cổng và tính ổn định huấn luyện

Chúng tôi quan sát thấy rằng tính ổn định huấn luyện khá nhạy cảm với cách các cổng được khởi tạo. Hồi quy có chế độ lỗi trong đó mô hình học cách hoàn toàn bỏ qua trạng thái hồi quy, trong trường hợp đó hiệu suất của nó trở về của transformer không hồi quy. Hơn nữa, tình huống này dường như là một tối ưu cục bộ; một khi mô hình đã đạt đến điểm này, nó không phục hồi.

Giả thuyết của chúng tôi là việc học hàm chuyển đổi hồi quy là một nhiệm vụ khó khăn hơn nhiều so với việc học chú ý trực tiếp đến các token đầu vào. Do đó, hướng dọc huấn luyện nhanh hơn nhiều so với hướng ngang, đặc biệt là sớm trong quá trình huấn luyện. Điều này có thể dẫn đến tình huống mà các trạng thái hồi quy ít thông tin hơn nhiều so với các token đầu vào, và mô hình học cách bỏ qua chúng.

Để tránh chế độ lỗi này, khởi tạo cổng đòi hỏi sự chăm sóc đặc biệt. Hơn nữa, khởi tạo cổng phù hợp phụ thuộc vào optimizer. Chúng tôi sử dụng optimizer Adafactor [51], chuẩn hóa gradient so với phương sai của chúng, và sau đó nhân chúng với quy mô bản địa của tham số cơ bản. Do đó, nếu một bias term được khởi tạo thành 0, quy mô bản địa của nó sẽ là 0, các cập nhật gradient sẽ rất nhỏ, và bias sẽ có xu hướng vẫn nhỏ trong suốt quá trình huấn luyện. Nếu một bias term được khởi tạo thành 1 (báo cho forget gate "nhớ", và là thực hành tiêu chuẩn trong LSTM) thì các cập nhật ban đầu sẽ lớn, và mô hình sẽ học cách bỏ qua các trạng thái hồi quy trước khi chúng có cơ hội học bất cứ điều gì hữu ích.

Chúng tôi thỏa hiệp bằng cách khởi tạo các bias term của cổng thành các giá trị nhỏ nhưng khác không, sử dụng phân phối chuẩn với mean 0 và độ lệch chuẩn 0.1. Các ma trận trọng số của cổng cũng được khởi tạo thành các giá trị nhỏ, sử dụng phân phối chuẩn cắt ngắn với độ lệch chuẩn √(0.1/fin) trong đó fin là chiều của ht.

Chúng tôi thêm hằng số -1 và +1 vào input gate và forget gate (xem Eq. 4) để ban đầu thiên về cổng "nhớ" mà không ảnh hưởng đến kích thước của các cập nhật mà Adafactor sẽ áp dụng. Sử dụng thủ thuật khởi tạo này, tế bào hồi quy đáng tin cậy học cách sử dụng trạng thái hồi quy.

Phụ lục C Chi tiết huấn luyện

Đối với PG19, chúng tôi thực hiện cả mô hình hóa token-level và character-level. Trong các thí nghiệm ban đầu, chúng tôi sử dụng từ vựng sentencepiece 32k đã được huấn luyện trước từ T5 [39] cho mô hình hóa token-level. Chúng tôi sử dụng optimizer Adafactor [51], lịch trình learning rate với inverse square root decay, 1000 bước warmup, và tỷ lệ dropout 0.05. Learning rate là 1.0; khi kết hợp với warmup và lịch trình decay, điều này mang lại learning rate áp dụng là 0.03, giảm xuống 0.0014. Learning rate và lịch trình này được mượn từ các mô hình ngôn ngữ khác; chúng tôi không cố gắng thực hiện quét siêu tham số để xác định learning rate và lịch trình tối ưu. Chúng tôi huấn luyện trong 500k bước với 32 replica trên Google V4 TPU; huấn luyện mất khoảng 48 giờ. Kết quả báo cáo là cho split "test".

Đối với thí nghiệm scaling sau này trên PG19, chúng tôi chuyển lịch trình learning rate sang cosine decay, như được khuyến nghị trong [43], với tỷ lệ tối đa 0.01, và tối thiểu 0.001. Chúng tôi đã thực hiện một thí nghiệm ngắn với learning rate 0.02 và 0.005, trước khi quyết định 0.01. Thay đổi trong lịch trình learning rate dẫn đến cải thiện đáng kể. Chúng tôi cũng chuyển từ từ vựng T5 32k sang từ vựng sentencepiece 32k tùy chỉnh được huấn luyện trên PG19. Từ vựng tùy chỉnh của chúng tôi có bits-per-token cao hơn, nhưng ít token hơn, và do đó có word-level perplexity hơi tốt hơn.

Đối với arXiv, chúng tôi sử dụng từ vựng 32k đã được huấn luyện trước từ LaMDA [52]. Do số lượng lớn ký hiệu toán học trong LaTeX, nhiều token chỉ có một ký tự, vì vậy số bits-per-token thấp hơn so với PG19. Chúng tôi giảm learning rate xuống 0.5 sau khi quan sát một số bất ổn khi huấn luyện trên độ dài đoạn dài hơn (4096). Kết quả báo cáo là cho split "test".

Bộ dữ liệu GitHub lớn hơn nhiều so với PG19, và có phương sai rất cao, do thực tế là nó chứa mã được viết bằng nhiều ngôn ngữ lập trình và phong cách mã hóa khác nhau. Do đó, có rất nhiều nhiễu trong kết quả, khiến việc so sánh mô hình chính xác trở nên khó khăn. Chúng tôi giảm nhiễu bằng cách sử dụng kích thước batch lớn gấp 4 lần so với PG19. Như với nghiên cứu scaling PG19, chúng tôi sử dụng lịch trình learning rate cosine decay, nhưng LR tối đa là 0.005; đây là một nửa LR được sử dụng cho PG19. Do kích thước batch tăng, các mô hình này chỉ chạy trong 250k bước. Thí nghiệm GitHub sử dụng từ vựng 32k đã được huấn luyện trước từ LaMDA. Kết quả báo cáo là cho split "validation".

Do số lượng lớn thí nghiệm, chúng tôi không có tài nguyên tính toán để chạy tất cả thí nghiệm nhiều lần, và do đó không cung cấp thanh lỗi cho tất cả thí nghiệm. Tuy nhiên, đối với các số tiêu đề trên PG19-tokens, chúng tôi chạy mỗi thí nghiệm 3 lần, với cả seed ngẫu nhiên khác nhau và với dataset shuffling. Thanh lỗi đo được thực tế trên PG19 rất thấp, giữa 0.002 và 0.007. Các số trong Bảng 1 được làm tròn đến 0.01 gần nhất, có nghĩa là thanh lỗi phải được làm tròn lên để phù hợp với độ chính xác của kết quả báo cáo. Ví dụ: đối với Rec:fixed:skip trên PG19-tokens, trung bình của 3 lần chạy có mean 3.525 và độ lệch chuẩn 0.0047; chúng tôi làm tròn lên thành 3.53±0.01. Lưu ý rằng không thể có được thanh lỗi thực sự chính xác từ số lần chạy nhỏ như vậy; bằng cách làm tròn lỗi lên, chúng tôi cung cấp ước tính bảo thủ của lỗi thực tế.

C.1 Cấp phép bộ dữ liệu và các vấn đề khác.

PG19 bao gồm các tác phẩm trong phạm vi công cộng, và do đó nó là một bộ dữ liệu công cộng có sẵn miễn phí cho các nhà nghiên cứu khác. Do tuổi của các văn bản, một số cuốn sách có chứa tài liệu có thể gây khó chịu.

Bộ dữ liệu arXiv bao gồm các tài liệu mà tác giả đã cho phép rõ ràng để tác phẩm của họ được phân phối bởi arxiv.org. Tuy nhiên, vì tác giả vẫn giữ bản quyền, những bài báo này không nhất thiết có thể được phân phối lại dưới dạng bộ dữ liệu công cộng, cũng không sẽ chúng tôi xuất bản một mô hình đã được pre-trained trên dữ liệu này. Chúng tôi có được quyền truy cập vào bộ dữ liệu này qua các kênh riêng.

Bộ dữ liệu Github bao gồm mã với giấy phép mã nguồn mở, cho phép mã được tải xuống, biên dịch, hoặc sửa đổi. Tương tự như arXiv, tuy nhiên, vì các tác giả giữ bản quyền, mã này không nhất thiết có thể được phân phối lại như một bộ dữ liệu công cộng, cũng không sẽ chúng tôi xuất bản một mô hình đã được pre-trained trên dữ liệu này. Chúng tôi có được quyền truy cập vào bộ dữ liệu này qua các kênh riêng.

C.2 Lựa chọn bộ dữ liệu

Có một bộ dữ liệu tiêu chuẩn hóa quan trọng cho mục đích so sánh kết quả đã xuất bản, và trong lịch sử, các bài báo về mô hình hóa ngôn ngữ ngữ cảnh dài đã sử dụng enwik8 hoặc wiki-103 làm benchmark [34][33]. Tuy nhiên, những bộ dữ liệu này không phải là benchmark đặc biệt tốt cho mục đích của chúng tôi.

Mục đích của các thí nghiệm của chúng tôi là xem liệu block recurrence có thể truyền thông tin qua khoảng cách rất dài: chúng tôi chỉ ra việc truy xuất qua 60k+ token. Chúng tôi chọn PG19 cụ thể vì chúng tôi tin rằng nó là một bộ dữ liệu tốt cho những loại thí nghiệm này. Nó chỉ bao gồm các tác phẩm dài, dài như sách, nó lớn hơn nhiều so với enwik8 hoặc wiki-103, nó có sẵn công khai, và đã được trích dẫn trong công trình đã xuất bản khác. Arxiv và github (thật đáng buồn) không công khai, nhưng chúng tương tự có các tài liệu dài trong phạm vi 50k+ token.

16

--- TRANG 17 ---
Enwik8 không phải là một corpus của các bài báo dài. Thực tế, nó thậm chí không chia văn bản thành các bài báo riêng biệt; nó chỉ là một dump văn bản duy nhất nối nhiều bài báo ngắn không liên quan lại với nhau. Cố gắng chia nó theo ranh giới bài báo cho ra một bộ dữ liệu trong đó phần lớn "bài báo" chỉ đơn thuần là stub, với boilerplate HTML và không có văn bản thực tế. Enwik8 là một benchmark tốt cho nén dữ liệu, đó là mục đích ban đầu mà nó được dự định, nhưng nó ít lý tưởng hơn cho mô hình hóa ngôn ngữ tầm xa.

Wiki-103 tốt hơn đáng kể, vì nó chia văn bản thành các bài báo, và nó loại bỏ boilerplate, nhưng độ dài trung bình vẫn chỉ là 3.6k token mỗi bài báo, ít hơn độ dài đoạn được sử dụng trong các thí nghiệm của chúng tôi, và ngắn hơn nhiều so với 50k-100k token của PG19.

C.3 So sánh với kết quả đã xuất bản trước đây.

Người ta biết rằng perplexity transformer tỷ lệ với số lượng tham số. Tuy nhiên, việc lựa chọn từ vựng, lịch trình learning rate, kích thước batch, số bước huấn luyện, và optimizer cũng tạo ra sự khác biệt lớn đến số perplexity tiêu đề cuối cùng. Nghiên cứu scaling Chinchilla [43] chứng minh rằng thay đổi lịch trình learning rate có thể có hiệu ứng lớn; chúng tôi cũng quan sát thấy cải thiện từ thay đổi từ vựng. Không phải tất cả từ vựng đều được tạo ra như nhau, ngay cả đối với từ vựng có cùng kích thước, và được huấn luyện chủ yếu trên văn bản tiếng Anh. Số perplexity đã xuất bản giữa các mô hình khác nhau không thể được so sánh có ý nghĩa trừ khi tất cả các biến khác được kiểm soát nghiêm ngặt.

Phụ lục D Kích thước cửa sổ và số lượng trạng thái hồi quy

Kết quả ablation cho kích thước cửa sổ được hiển thị trong Bảng 3 (a). Giảm kích thước cửa sổ dẫn đến perplexity tệ hơn trong cả mô hình hồi quy và Transformer-XL, nhưng hình phạt nhỏ hơn đối với mô hình hồi quy.

Kết quả ablation cho số lượng trạng thái hồi quy được hiển thị trong Bảng 3 (b). Tăng số lượng trạng thái hồi quy tạo ra cải thiện nhỏ nhưng có thể đo được đến 1024 trạng thái, nhưng tệ hơn ở 2048 trạng thái. Kích thước cửa sổ là 512 cho thí nghiệm này.

Bảng 3: Thay đổi kích thước cửa sổ (a) và số lượng trạng thái hồi quy (b) trên PG19.

Kích thước cửa sổ  Rec:fixed:skip  Slide:12L
128              3.58            3.69
256              3.54            3.64
512              3.53            3.60

Số lượng trạng thái  Rec:fixed:skip
128                  3.54
256                  3.535
512                  3.53
1024                 3.51
2048                 3.55

Phụ lục E Block-Feedback

Trong biến thể block-feedback của mô hình, toàn bộ stack 12 lớp được áp dụng cho khối token đầu tiên. Trạng thái hồi quy cho khối đó sau đó được trích xuất từ lớp hồi quy, và trạng thái được broadcast đến tất cả các lớp khác khi xử lý khối tiếp theo. Vì lớp hồi quy được đặt cao trong stack, điều này có nghĩa là các lớp thấp hơn của transformer có thể cross-attend đến một lớp cao hơn, mạnh hơn về mặt tính toán, giống như feedback transformer [24]. Kết quả được hiển thị trong Bảng 1.

Hồi quy với feedback đắt hơn đáng kể so với phiên bản không feedback, vì giờ đây tất cả 12 lớp có mô-đun cross-attention, thay vì chỉ lớp hồi quy. Trong các thí nghiệm của chúng tôi, feedback tăng thời gian bước khoảng 35-40%, và các query bổ sung cũng tăng số lượng tham số.

Thêm feedback cải thiện perplexity trong hầu hết trường hợp, nhưng cải thiện dường như phụ thuộc vào bộ dữ liệu. Hiệu ứng của feedback cũng phụ thuộc vào cấu hình cổng. Đặc biệt, block feedback cải thiện đáng kể hiệu suất của cổng LSTM. Điều này có thể là vì các trạng thái hồi quy, và do đó cổng, nhận được gradient từ tất cả các lớp của transformer, thay vì chỉ một.

17

--- TRANG 18 ---
Phụ lục F Chi tiết biểu đồ scaling

Bảng 4: Bit mỗi token trên PG19 ở các quy mô mô hình khác nhau. Dữ liệu trong bảng này được sử dụng cho biểu đồ scaling trong Hình 3.

            40.6M  81.2M  162M   325M   650M   1.3B
Rec:fixed:skip  3.96   3.79   3.59   3.44   3.29   3.22
XL:512:13-layer 4.03   3.86   3.67   3.51   3.39   3.31

Hiệu suất trên các bộ dữ liệu văn bản lớn, như PG-19, có tương quan cao với số lượng tham số có thể huấn luyện; mô hình lớn hơn có xu hướng hoạt động tốt hơn. Tuy nhiên, việc huấn luyện mô hình lớn có thể đắt đỏ, và không phải tất cả nhà nghiên cứu đều có quyền truy cập vào lượng tính toán cần thiết để đánh bại state of the art mới của chúng tôi, hoặc thậm chí để tái tạo kết quả của chúng tôi.

Bảng 4 cung cấp kết quả số từ nghiên cứu scaling của chúng tôi, bao gồm các quy mô từ mô hình 40M tham số nhỏ có thể dễ dàng huấn luyện trên một máy duy nhất, đến mô hình 1.3B tham số lớn nhất của chúng tôi. Tệp cấu hình cho tất cả quy mô được cung cấp trong bản phát hành mã nguồn mở. Chiến lược scaling của chúng tôi là tăng các chiều của các phần khác nhau của transformer: kích thước embedding, kích thước lớp ẩn MLP, số lượng head, kích thước head, và số lượng lớp. Mỗi lần tăng gấp đôi số lượng tham số mở rộng một hoặc hai trong số các chiều này.

Phụ lục G Kết quả phân tích định tính

Hình 4: Sự khác biệt trong cross-entropy loss mỗi token.

Dưới đây là các đoạn trích từ nghiên cứu định tính của chúng tôi. Chúng tôi chọn ngẫu nhiên năm cuốn sách từ tập test PG19, và chạy hai mô hình khác nhau trên mỗi cuốn sách. Mô hình đầu tiên là Transformer-XL 13 lớp, và mô hình thứ hai là cấu hình Rec:fixed:skip của Block-Recurrent Transformer. Đối với mỗi token, chúng tôi tính toán sự khác biệt giữa cross-entropy loss (tức là negative log likelihood (NLL)) được xuất ra bởi cả hai mô hình, và sau đó sắp xếp kết quả.

Hình 4 hiển thị một ví dụ về sự khác biệt NLL mỗi token giữa hai mô hình trên cuốn sách đầu tiên; trục x là chỉ số của token. Trung bình, mô hình hồi quy làm hơi tốt hơn Transformer-XL, nhưng nó không nhất thiết đưa ra dự đoán tốt hơn cho bất kỳ token cá nhân nào.

Các đoạn trích sau đây hiển thị 4 token hàng đầu mà Block-Recurrent Transformer đưa ra dự đoán tốt hơn Transformer-XL; những token này tương ứng với các đỉnh trong Hình 4. Chúng tôi hiển thị số token, NLL được trả về bởi mô hình hồi quy, NLL được trả về bởi Transformer-XL, và một đoạn trích văn bản, với chính token được đánh dấu bằng |token|. Hầu hết các token hàng đầu là tên riêng của nhân vật và địa danh. Trong tất cả trường hợp trừ một, tên được dự đoán sai không xuất hiện trong cửa sổ attention của 512 token trước đó. Những tên này do đó vô hình đối với Transformer-XL, nhưng hiển thị đối với mô hình hồi quy.

Lưu ý rằng đây không phải là các ví dụ được chọn lọc; năm cuốn sách được chọn ngẫu nhiên. Hơn nữa, cùng một mẫu vẫn đúng nếu tìm kiếm được mở rộng đến 40 token hàng đầu cho mỗi cuốn sách. Thực tế, ngay cả các tên cũng thường giống nhau; Transformer-XL thường dường như dự đoán sai cùng những tên lặp đi lặp lại; đây có thể là tên của các nhân vật chính.

Sắp xếp theo cách khác, để hiển thị các token hàng đầu mà Transformer-XL làm tốt hơn mô hình hồi quy, không hiển thị cùng một mẫu. Vẫn có nhiều tên riêng, nhưng thường là các trường hợp mà cả hai mô hình đều không dự đoán được tên. Hơn nữa, các tên được trộn lẫn với các từ thông thường hơn.

Memoirs of Marie Antoinette Queen Of France Vol. 7.
(30555, 0.3696011, 12.688916)

18

--- TRANG 19 ---
physician's house to make inquiries as to the cause of so long an absence.
G|omin| and Larne had not yet ventured to follow this advice, when next
(32043, 0.20177796, 10.513703)
with the autopsy arrived at the outer gate of the Temple. These were
Dum|ang|in, head physician of the Hospice de l'Unite; Pelletan,
(34896, 1.3752508, 11.534926)
who had acted as courier to Louis XVI. during the flight to Varennes,
and Tur|gi|, who had waited on the Princesses in the Temple. It was
(34896, 1.3752508, 11.534926)
. In all the evidence there appeared but two serious facts, attested
by Latour-|du|-Pin and Valaze, who deposed to them because they could

Ghi chú: "Gomin" xuất hiện nhiều lần trong 40 kết quả hàng đầu.

An Unsentimental Journey through Cornwall
(983, 0.80441666, 11.626528)
OUGH CORNWALL [Illustration: FALMOUTH, FROM FLUSHING.]
|DAY| THE FIRST I believe in holidays. Not in a frantic rushing
(23606, 0.655162, 11.265186)
there was not the slightest use in getting up, I turned round and took
another sleep. |DAY| THE FIFTH "Hope for the best, and be
(11297, 0.405902, 10.426135)
ball. "Ma'am, if you go slow and steady, with me before and Cur|gen|ven
behind, you'll _not_ fall." Nor did I. I record it
(38021, 1.3457009, 11.167879)
our journey; going over the same ground which we had traversed already,
and finding Praden|ack| Down as bleak and beautiful as ever. Our first

Ghi chú: Từ "DAY" xuất hiện trước đó trong mục lục, nhưng không trong 512 token trước đó. "Curgenven" xuất hiện nhiều lần trong 40 kết quả hàng đầu.

The Good Hope by Herman Heijermans Jr.
(3975, 0.7425603, 15.8969145)
to us." It matters nothing that this gospel of Life has often been
preached. He|ij|ermans has caught the spirit of it as well as the letter.
(7385, 0.26241615, 15.000762)
must scratch the stones. CLEM. Tomorrow afternoon, then. COB. T|ja|!
I'll be here, then. Good day, Miss. [To Barend.] Good
(42638, 0.73628587, 15.407666)
hundred guilders. Bejour. [Rings off; at the last words Kne|irt|je has
entered.] KNEIRTJE. [Absently.] I----[
(25798, 0.12310324, 14.402218)
They exeunt, dragging Barend.] KNEIR. Oh, oh---- TRU|US|. [With
anxious curiosity, at side door.] What was the matter, Kneir?

Ghi chú: Từ "Tja" không phải là tên riêng, nhưng là một từ tiếng Hà Lan có thể độc nhất cho cuốn sách cụ thể này. Nó xuất hiện nhiều lần trong 40 kết quả hàng đầu. Tên "Truus" xuất hiện nhiều lần trong 40 kết quả hàng đầu.

19

--- TRANG 20 ---
Travels in Morocco Volume 2 by James Richardson

Ghi chú: Ví dụ thứ hai trong danh sách này không tốt, vì cả hai mô hình đều dự đoán sai tên. Do đó chúng tôi bao gồm một ví dụ thứ năm. Từ "Toser" xuất hiện thường xuyên trong 40 kết quả hàng đầu, và cũng là trường hợp duy nhất mà tên riêng xuất hiện trong cửa sổ attention 512 token.

(61049, 2.0328524, 18.696453)
, some of them as black as [Redacted]s. Many people in T|oser| have sore
eyes, and several with the loss of one eye, or nearly so; opthal
(63453, 9.254061, 24.413633)
Tunis;" but the restrictive system established by the Turks during late
years at Ghad|umes|, has greatly damaged the trade between the Jereed and
(66149, 0.8768588, 14.105822)
enterprizing fellow, worthy of imitation. He calculated the distance from
Ghabs to T|oser| at 200 miles. There are a number of towns in the
(64161, 1.0309317, 14.152167)
and ourselves went to Wedyen, a town and date-wood about eight miles
from T|oser|, to the left. The date-grove is extensive, and there are
(27282, 0.04982663, 12.551973)
, is a very ancient city, situate upon the right bank of the river
Boura|gra|g, and near its mouth. This place was captured in 1263, by

India's Love Lyrics by Laurence Hope
(15694, 0.07477246, 11.22447)
rieving A wasted chance. Fate knows no tears. Verses:
Fa|iz|Ulla Just in the hush before dawn A little wistful wind is
(3135, 0.0126608405, 9.759871)
afloat In the little noontide of love's delights Between
two Nights. Val|go|vind's Boat Song Waters glisten and
sunbeams quiver,
(23183, 0.1419289, 9.481476)
sleep, the touch of your lips on my mouth. His Rubies: Told by
Val|go|vind Along the hot and endless road, Calm and erect,
with haggard eyes,'
(26886, 0.13428347, 9.46193)
off sheath, And find there is no Rival like the Past. Verse
by T|aj|Mahomed When first I loved, I gave my very soul Utterly

G.1 Xóa trạng thái hồi quy

Hình 5: Sự khác biệt trong cross-entropy loss mỗi token với việc xóa trạng thái.

Nghiên cứu định tính thứ hai của chúng tôi được cấu trúc tương tự như nghiên cứu đầu tiên, ngoại trừ việc thay vì so sánh hai mô hình khác nhau, chúng tôi so sánh hai lần chạy khác nhau của cùng một mô hình: cấu hình Rec:fixed:skip. Lần chạy đầu tiên xử lý cuốn sách bình thường, trong khi lần chạy thứ hai xóa các trạng thái hồi quy ở đầu mỗi đoạn 4096 token. Trong lần chạy thứ hai, mô hình có thể sử dụng hồi quy trong một đoạn để nhìn xa hơn cửa sổ attention cục bộ 512 token, nhưng nó không thể sử dụng hồi quy để mang thông tin từ đoạn này sang đoạn tiếp theo.

20

--- TRANG 21 ---
Thí nghiệm này phần nào sạch hơn thí nghiệm đầu tiên, vì cả hai lần chạy đều được thực hiện với cùng một mô hình đã được huấn luyện trước, có cùng tham số. Hình 5 hiển thị sự khác biệt trong cross-entropy loss mỗi token cho cuốn sách đầu tiên. Không có sự khác biệt giữa hai lần chạy cho đoạn đầu tiên, và những sự khác biệt lớn nhất trong các đoạn tiếp theo thường tập trung gần đầu mỗi đoạn mới.

Mẫu tổng thể rất tương tự với thí nghiệm định tính đầu tiên: hầu hết các token liên quan đến tên riêng. Chúng tôi xác nhận rằng trong hầu hết trường hợp, tên được dự đoán sai không chỉ không xuất hiện trong cửa sổ attention 512 token, mà còn không xuất hiện trong đoạn 4096 token. Ngoài tên riêng, tiêu đề chương và chú thích hình minh họa xuất hiện thường xuyên trong 40 kết quả hàng đầu; mô hình hồi quy dường như nhớ những thứ này từ lần xuất hiện trước đó trong mục lục.

Có lẽ thú vị nhất, trong hai cuốn sách, một trong những dự đoán sai được xếp hạng cao nhất là tiêu đề và tác giả của chính cuốn sách. Dự án Gutenberg chèn boilerplate ở cả đầu và cuối mỗi cuốn sách; tiêu đề và tác giả được liệt kê nhiều lần ở đầu, và một lần ở cuối. Thí nghiệm này do đó cho thấy rằng mô hình có thể "nhớ" thông tin này trong trạng thái hồi quy, qua khoảng cách 60,000 token hoặc hơn.

Baby Mine, by Margaret Mayo

Lưu ý rằng dự đoán sai thứ 2 là tiêu đề của chính cuốn sách, tại token số 71,244.

(60153, 0.00438364, 12.798895)
nerves, but you needn't worry, I've got everything fixed.
Donneg|hey| sent a special officer over with me. He's outside
(71244, 2.3727102, 12.660636)
sunlight and shadows of his ample, well kept grounds. End of the
Project Gutenberg EBook of| Baby| Mine, by Margaret Mayo ***
(63536, 4.132567, 13.688847)
Alfred, with the air of a connoisseur. "She sure is," admitted
Don|neg|hey, more and more disgruntled as he felt his reputation for
(26947, 3.7521973, 12.516711)
with a sigh of thanksgiving he hurried upstairs to his unanswered mail.
CHAPTER XIII When Alfred| Hardy| found himself on the train bound for

The 'Patriotes' of '37 by Alfred D. Decelles
(38759, 0.85374373, 11.140007)
24, 125, 126. Cote, Dr Cyri|le|, 89, 108, 118, 120;
(40181, 0.35475963, 10.291676)
17-26, 129-30. Nelson, Dr Wolf|red|, a follower of Papineau, 37, 60
(28756, 0.0010796335, 9.256147)
on a well-reasoned plan of action. Most of the leaders--Wolf|red| Nelson,
Thomas Storrow Brown, Robert Bouchette, and Amury Girod--were
(28772, 1.7782648, 10.611834)
leaders--Wolfred Nelson, Thomas Storrow Brown, Robert Bouchette, and
Am|ury| Girod--were strangers to the men under their command; and none of

Another Brownie Book, by Palmer Cox

Trong trường hợp này, 4 kết quả hàng đầu bao gồm tác giả của cuốn sách. Cuốn sách này có nhiều hình minh họa được liệt kê trong mục lục, và tạo nên nhiều kết quả khác trong 40 kết quả hàng đầu.

(16442, 0.087840006, 8.706101)

21

--- TRANG 22 ---
[Illustration] [Illustration] [Illustration] THE BROWNIES
AND THE TUG|BO|AT. [Illustration] While Brownies strayed along a
(24225, 0.04784866, 8.266858)
[Illustration] End of the Project Gutenberg EBook of Another
Brownie Book, by Palmer| Cox| ***
(24224, 5.651471, 11.840027)
Illustration] [Illustration] End of the Project Gutenberg
EBook of Another Brownie Book, by| Palmer| Cox ***
(4460, 1.0682098, 6.6397715)
To secret haunts without delay. [Illustration] [Illustration]
THE BROWNIES AT| AR|CHERY. [Illustration] [Illustration]

The Life Of Thomas Paine Vol. II. (of II) by Moncure Daniel

Đến cuối cuốn sách, mô hình hồi quy khá chắc chắn rằng "Paine" là một nhân vật chính, nhưng không nếu bộ nhớ của nó tiếp tục bị xóa.

(170457, 0.98941976, 12.209277)
they could. The scandal branched into variants. Twenty-five years later
pious Grant| Thor|burn promulgated that Paine had run off from Paris
(120592, 0.6974209, 10.396527)
my friends and accept the same to yourself." As the Commissioners did
not leave when they expected,| Paine| added several other letters to
(169152, 0.18432029, 9.516743)
whose composition the farrier no doubt supposed he had paid the editor
with stories borrowed from "Old|ys|," or not actionable. Cheetham
(139870, 0.53406477, 9.646917)
nor was any letter received from him. This was probably the most important
allusion in a letter of| Paine|, dated New York, March 1, 1804, to "C

Phụ lục H Cross-entropy cấp token

Ngoài các nghiên cứu định tính so sánh một tài liệu duy nhất, chúng tôi cũng so sánh bits-per-token trung bình trên tất cả tài liệu trong tập test PG19. Người ta đã quan sát thấy rằng trong kiến trúc transformer vanilla, cross-entropy theo token này thường phân kỳ ở độ dài đoạn dài [53].

Trong Hình 6 chúng tôi vẽ biểu đồ cross-entropy tích lũy, là bits-per-token trung bình (tức là log 2 perplexity) được tính trung bình đến độ dài nhất định, và chúng tôi so sánh Block Recurrent Transformer với baseline Transformer-XL. Hiệu suất của hai kiến trúc tương đương trong vài nghìn token đầu tiên, nhưng kiến trúc hồi quy rõ ràng vượt trội hơn Transformer-XL ở độ dài tài liệu dài hơn.

Phụ lục I Thí nghiệm ablation từ vựng

Các tác giả của bộ dữ liệu PG-19 đề xuất word-level perplexity (WLP) như một thước đo hiệu suất mô hình [15]. Tuy nhiên, chỉ có thể tính WLP nếu có số đếm chính xác số từ. Số đếm từ đã xuất bản được cung cấp cho bộ dữ liệu PG19 [15], nhưng số đếm từ không có sẵn cho các bộ dữ liệu khác trong bài báo này, như arXiv và github. Thậm chí không có định nghĩa rõ ràng về cái gì tạo thành một "từ" trong mã nguồn hoặc tài liệu LATEX, có nhiều ký hiệu. Do đó, chúng tôi không cung cấp WLP cho các bộ dữ liệu này.

WLP cũng bỏ qua một số chi tiết về tokenization; đặc biệt, số đếm từ bỏ qua khoảng trắng. SentencePiece [44] thường được quảng cáo là tokenizer subword bảo toàn khoảng trắng, nhưng trong

22

--- TRANG 23 ---
10³ 10⁴ 10⁵
Token 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 Bits-per-token trung bình
Transformer-XL
Block Recurrent
Số lượng tài liệu ở độ dài. 0 20 40 60 80 100
Số lượng tài liệu PG19 Cross-entropy -- trung bình tích lũy

Hình 6: Cross-entropy tích lũy trên PG19 của Transformer-XL 13 lớp và mô hình Block Recurrent. Mặc dù tương đương ở vài nghìn token đầu tiên, mô hình hồi quy hoạt động tốt hơn ở các chuỗi dài hơn. Màu đỏ chúng tôi hiển thị số lượng tài liệu ở một độ dài token nhất định.

thực tế điều đó chỉ đúng một phần. Tùy thuộc vào cách nó được cấu hình, SentencePiece có thể chuẩn hóa văn bản bằng cách ánh xạ nhiều ký tự khoảng trắng (ví dụ: tab) thành space, hợp nhất các ký tự khoảng trắng nội bộ, hoặc loại bỏ tất cả newline khỏi văn bản.

Do đó, tùy thuộc vào từ vựng, một mô hình được huấn luyện với tokenizer SentencePiece có thể hoặc không dự đoán khoảng trắng và newline. Nếu nó dự đoán chúng, thì khoảng trắng đó không được thu thập bởi số đếm từ, có thể ảnh hưởng đến số WLP.

Đo bits-per-character (BPC) thay vì WLP là một so sánh đơn giản và công bằng hơn có thể hoạt động với bất kỳ bộ dữ liệu và tokenizer nào, nhưng nó gặp phải cùng vấn đề với khoảng trắng. Nếu tokenizer hợp nhất hoặc loại bỏ khoảng trắng khỏi văn bản đầu vào, như nhiều từ vựng SentencePiece làm, thì không thể có được số đếm ký tự nhất quán trên các từ vựng khác nhau.

I.1 Hiệu ứng của từ vựng lên perplexity mô hình

Chính từ vựng cũng có thể tạo ra sự khác biệt đáng kể. Chúng tôi lập luận rằng tokenizer và từ vựng nên được coi là một phần của mô hình, và các so sánh sử dụng WLP hoặc BPC chỉ hợp lệ giữa các mô hình sử dụng cùng tokenizer và từ vựng. Sử dụng WLP để đánh giá các đổi mới mô hình mà không tính đến các khác biệt khác có thể dẫn đến kết luận sai. Chúng tôi đưa ra một ví dụ thực nghiệm về cách sử dụng từ vựng khác nhau có thể dẫn đến sự khác biệt đáng kể trong WLP.

Tổng cross-entropy loss LS = ∑ᴺˢₜ₌₀ log(pt|p<t) trong đó S là sơ đồ tokenization được chọn và NS là số lượng token trong dữ liệu khi được tokenize bởi S. Các mô hình sử dụng cùng sơ đồ tokenization có thể được so sánh bởi token-level perplexity PPLS = exp(LS/NS) trong đó LS/NS là loss trung bình. Perplexity thấp hơn là tốt hơn.

23

--- TRANG 24 ---
WLP được định nghĩa là exp(LS/NW) trong đó NW là số "từ" như được định nghĩa bởi các tác giả của bộ dữ liệu PG-19. Đối với tập test PG-19, NW = 6,966,499 [15]. Do đó, WLP chỉ đơn giản là một tái scaling của loss trung bình, tức là WLP = exp(LS/NS × NS/NW), trong đó chúng tôi sẽ gọi NS/NW là hệ số quy mô – số token mỗi từ.

I.1.1 Kích thước từ vựng

Kích thước của từ vựng có ba hiệu ứng rõ ràng lên mô hình. Đầu tiên, từ vựng lớn hơn có nhiều tham số hơn, và do đó có thể lưu trữ nhiều thông tin hơn trong bảng embedding.

Thứ hai, từ vựng lớn hơn thường sẽ có những từ dài hơn và hiếm hơn, vì vậy độ dài trung bình của mỗi token (ký tự mỗi token) cũng sẽ dài hơn, và hệ số quy mô (token mỗi từ) sẽ nhỏ hơn. Độ dài ngữ cảnh của mô hình theo token thường cố định, vì vậy có token dài hơn có nghĩa là mô hình có thể chú ý qua khoảng cách dài hơn trong văn bản đầu vào.

Thứ ba, có token dài hơn cũng có nghĩa là mô hình sẽ xử lý nhiều dữ liệu huấn luyện hơn trong mỗi bước huấn luyện (một lần nữa vì số token mỗi bước cố định), và do đó hoàn thành nhiều epoch hơn qua bộ dữ liệu.

I.1.2 Chất lượng từ vựng

Ngay cả khi hai từ vựng có cùng kích thước, có thể có sự khác biệt về chất lượng từ vựng. Ví dụ, nếu từ vựng đa ngôn ngữ được sử dụng trên văn bản tiếng Anh, thì chỉ một phần dung lượng tổng của nó sẽ được sử dụng để thu thập từ tiếng Anh, và nó sẽ hoạt động giống như một từ vựng nhỏ hơn.

Vấn đề tinh tế hơn là có nhiều cách mà văn bản có thể được tokenize. Một tokenizer thực hiện tốt việc thu thập phân đoạn ngữ nghĩa tự nhiên thành subword có thể được mong đợi hoạt động tốt hơn một tokenizer không làm vậy, ví dụ: " walk|ed " so với " wal|ked ".

I.2 Kết quả thực nghiệm

Chúng tôi huấn luyện mô hình Rec:fixed:skip với độ dài đoạn 4096 trong 500k bước sử dụng lịch trình inverse square root decay trong khi chỉ thay đổi lựa chọn từ vựng (tất cả siêu tham số khác như trong Mục 4).

Từ vựng byte-level không sử dụng tokenizer, và hoạt động trên văn bản ASCII thô. Từ vựng T5 [39] được huấn luyện trên dữ liệu đa ngôn ngữ từ Common Crawl (commoncrawl.org), bao gồm 32k token, và không bảo toàn khoảng trắng. Chúng tôi cũng thử nghiệm trên phiên bản 32k của từ vựng LaMDA [54]. Để kiểm tra hiệu ứng của kích thước từ vựng, chúng tôi cũng huấn luyện một số mô hình SentencePiece trên tập huấn luyện PG19 với kích thước từ 512 đến 128k.

Kết quả được hiển thị trong Bảng 5 và Hình 7. Đối với từ vựng SentencePiece được huấn luyện trên PG19, chúng tôi quan sát thấy rằng từ vựng lớn hơn có hiệu ứng đáng kể lên hệ số quy mô, mặc dù hiệu ứng bắt đầu giảm với hơn 32k mục (xem Hình 7). Bits trung bình mỗi token tăng khi token trở nên dài hơn.

24

--- TRANG 25 ---
Bảng 5: Test perplexity của mô hình Rec:bias:skip khi sử dụng tokenizer khác nhau. Kết quả được đưa ra theo bits-per-token, vì vậy WLP là 2^(bits-per-token×scale factor). STE là viết tắt của SubwordTextEncoder deprecated của Tensorflow như được sử dụng bởi các baseline khác; SPM là viết tắt của SentencePiece model [44].

kích thước từ vựng   số đếm token    hệ số quy mô   bits-per-token trung bình   WPL
T5 [39]       32k   10,523,460      1.5106        3.525                      40.08
LaMDA [52]    32k   12,053,681      1.7302        3.121                      42.23
PG19 STE      32k   11,097,364      1.5930        3.352                      40.49
PG19 SPM      32k   10,229,476      1.4684        3.647                      40.93
PG19 SPM     128k    9,638,472      1.3835        3.830                      39.37
PG19 SPM      96k    9,711,259      1.3940        3.801                      39.36
PG19 SPM      64k    9,856,687      1.4149        3.766                      40.18
PG19 SPM      32k   10,229,476      1.4684        3.647                      40.93
PG19 SPM       8k   11,559,455      1.6593        3.289                      43.94
PG19 SPM       4k   12,622,101      1.8118        3.021                      44.43
PG19 SPM       1k   15,865,499      2.2774        2.477                      49.91
PG19 SPM      512   18,124,332      2.6016        2.125                      46.16
bytes         256   41,288,269      5.9267        0.960                      51.61

Hình 7: Hệ số quy mô word-perplexity và bits trung bình mỗi token cho từ vựng sentencepiece được huấn luyện trên PG19 với kích thước khác nhau.

25
