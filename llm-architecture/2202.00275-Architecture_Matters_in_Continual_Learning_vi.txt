# 2202.00275.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/llm-architecture/2202.00275.pdf
# Kích thước file: 1179112 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Kiến Trúc Quan Trọng trong Học Liên Tục
Seyed Iman Mirzadeh∗1, Arslan Chaudhry2, Dong Yin2,
Timothy Nguyen2, Razvan Pascanu2, Dilan Gorur2, và Mehrdad Farajtabar†2
1Washington State University,2DeepMind

Tóm tắt
Một lượng lớn nghiên cứu trong học liên tục tập trung vào việc khắc phục sự quên lãng thảm khốc của mạng nơ-ron bằng cách thiết kế các thuật toán mới có khả năng chống chịu trước sự dịch chuyển phân phối. Tuy nhiên, phần lớn các công trình này chỉ tập trung nghiêm ngặt vào phần "thuật toán" của học liên tục cho một "kiến trúc mạng nơ-ron cố định", và các tác động của việc sử dụng các kiến trúc khác nhau hầu như bị bỏ qua. Thậm chí một số ít phương pháp học liên tục hiện có có sửa đổi mô hình cũng giả định một kiến trúc cố định và nhằm phát triển thuật toán sử dụng mô hình hiệu quả trong suốt quá trình học. Tuy nhiên, trong công trình này, chúng tôi cho thấy rằng việc lựa chọn kiến trúc có thể tác động đáng kể đến hiệu suất học liên tục, và các kiến trúc khác nhau dẫn đến sự đánh đổi khác nhau giữa khả năng nhớ các tác vụ trước đó và học các tác vụ mới. Hơn nữa, chúng tôi nghiên cứu tác động của các quyết định kiến trúc khác nhau, và những phát hiện của chúng tôi đưa ra các thực hành tốt nhất và khuyến nghị có thể cải thiện hiệu suất học liên tục.

1 Giới thiệu
Học liên tục (CL) (Ring, 1995; Thrun, 1995) là một nhánh của học máy trong đó mô hình được tiếp xúc với một chuỗi các tác vụ với hy vọng khai thác kiến thức hiện có để thích ứng nhanh chóng với các tác vụ mới. Nghiên cứu trong học liên tục đã có sự bùng nổ trong những năm gần đây với trọng tâm rõ ràng là phát triển các thuật toán có thể giảm thiểu sự quên lãng thảm khốc (McCloskey và Cohen, 1989)—theo đó mô hình đột ngột quên thông tin của quá khứ khi được huấn luyện trên các tác vụ mới.

Trong khi phần lớn nghiên cứu trong học liên tục tập trung vào việc phát triển thuật toán học, có thể hoạt động tốt hơn so với việc tinh chỉnh ngây thơ trên một luồng dữ liệu, vai trò của kiến trúc mô hình, theo hiểu biết của chúng tôi, không được nghiên cứu một cách rõ ràng trong bất kỳ công trình hiện có nào. Thậm chí lớp các phương pháp dựa trên cách ly tham số hoặc mở rộng, ví dụ (Rusu et al., 2016; Yoon et al., 2018), chỉ có sự tập trung sơ bộ vào kiến trúc mô hình ở mức độ họ giả định một kiến trúc cụ thể và cố gắng tìm một thuật toán hoạt động trên kiến trúc đó. Trực giao với hướng thiết kế thuật toán này, động lực của chúng tôi là các bias quy nạp được tạo ra bởi các thành phần kiến trúc khác nhau quan trọng đối với học liên tục. Chúng tôi tìm cách đặc tả các tác động của các lựa chọn kiến trúc khác nhau.

Để thúc đẩy, xem xét một mô hình ResNet-18 (He et al., 2016) trên Split CIFAR-100, trong đó tập dữ liệu CIFAR-100 (Krizhevsky et al., 2009) được chia thành 20 tập rời rạc—một kiến trúc và điểm chuẩn phổ biến trong các công trình học liên tục hiện có. Hình 1a cho thấy rằng các thuật toán CL được thiết kế rõ ràng, EWC (Kirkpatrick et al., 2017) (một phương pháp dựa trên điều hòa tham số) và experience replay (Riemer et al., 2018) (một thuật toán CL dựa trên bộ nhớ) thực sự cải thiện so với việc tinh chỉnh ngây thơ. Tuy nhiên, có thể thấy rằng hiệu suất tương tự hoặc tốt hơn có thể đạt được trên điểm chuẩn này bằng cách đơn giản loại bỏ lớp pooling trung bình toàn cục từ ResNet-18 và thực hiện việc tinh chỉnh ngây thơ. Điều này rõ ràng chứng minh nhu cầu hiểu biết tốt hơn về kiến trúc mạng trong học liên tục.

Tóm lại, trong công trình này, chúng tôi nghiên cứu kỹ lưỡng các tác động của các quyết định kiến trúc trong học liên tục. Các thí nghiệm của chúng tôi cho thấy rằng các thành phần khác nhau của mạng nơ-ron hiện đại có tác động khác nhau đến các chỉ số học liên tục liên quan—cụ thể là độ chính xác trung bình, sự quên lãng và độ chính xác học tập (cf. Sec. 2.1.2)—đến mức mà việc tinh chỉnh thông thường với các thành phần được sửa đổi có thể đạt được hiệu suất tương tự hoặc tốt hơn so với các phương pháp CL được thiết kế cụ thể trên một kiến trúc cơ sở mà không tăng đáng kể số lượng tham số.

Chúng tôi tóm tắt các đóng góp chính như sau:
•Chúng tôi so sánh cả khả năng học tập và giữ gìn của các kiến trúc phổ biến. Theo hiểu biết của chúng tôi, tầm quan trọng của kiến trúc trong học liên tục chưa được khám phá trước đây.
•Chúng tôi nghiên cứu vai trò của các quyết định kiến trúc riêng lẻ (ví dụ, chiều rộng và chiều sâu, batch normalization, skip-connections và các lớp pooling) và cách chúng có thể tác động đến hiệu suất học liên tục.
•Chúng tôi cho thấy rằng, trong một số trường hợp, việc đơn giản sửa đổi kiến trúc có thể đạt được hiệu suất tương tự hoặc tốt hơn so với các thuật toán CL được thiết kế cụ thể (trên một kiến trúc cơ sở).
•Ngoài các điểm chuẩn CL tiêu chuẩn, Rotated MNIST và Split CIFAR-100, chúng tôi báo cáo kết quả trên điểm chuẩn Split ImageNet-1K quy mô lớn, hiếm khi được sử dụng trong tài liệu CL, để đảm bảo kết quả của chúng tôi đúng trong các cài đặt phức tạp hơn.
•Được lấy cảm hứng từ những phát hiện của chúng tôi, chúng tôi đưa ra các gợi ý thực tế có chi phí tính toán rẻ và có thể cải thiện hiệu suất của các kiến trúc khác nhau trong học liên tục.

Chúng tôi nhấn mạnh rằng mục tiêu chính của công trình này là minh họa tầm quan trọng của các quyết định kiến trúc trong học liên tục, và điều này không ngụ ý rằng khía cạnh thuật toán không thiết yếu. Thực tế, người ta có thể tận hưởng những cải tiến ở cả hai khía cạnh, như chúng tôi sẽ thảo luận trong Phụ lục B. Cuối cùng, chúng tôi lưu ý rằng mục tiêu phụ của công trình này là trở thành bước đệm khuyến khích nghiên cứu thêm về khía cạnh kiến trúc của học liên tục bằng cách tập trung vào chiều rộng hơn là chiều sâu của một số chủ đề trong công trình này. Chúng tôi tin rằng công trình của chúng tôi cung cấp nhiều hướng thú vị cần phân tích sâu hơn ngoài phạm vi của bài báo này nhưng có thể cải thiện đáng kể hiểu biết của chúng ta về học liên tục.

--- TRANG 2 ---
2 So sánh các Kiến trúc
2.1 Thiết lập Thí nghiệm
Ở đây, để ngắn gọn, chúng tôi giải thích thiết lập thí nghiệm của chúng tôi nhưng hoãn thông tin chi tiết hơn (ví dụ, siêu tham số, chi tiết kiến trúc, lý do đằng sau các lựa chọn thiết lập thí nghiệm, v.v.) đến Phụ lục A.

2.1.1 Thiết lập Học Liên tục
Chúng tôi sử dụng ba điểm chuẩn học liên tục cho các thí nghiệm của mình. Split CIFAR-100 bao gồm 20 tác vụ trong đó mỗi tác vụ có dữ liệu của 5 lớp (rời rạc), và chúng tôi huấn luyện trên mỗi tác vụ trong 10 epoch. Split ImageNet-1K bao gồm 10 tác vụ trong đó mỗi tác vụ bao gồm 100 lớp của ImageNet-1K và chúng tôi huấn luyện trên mỗi tác vụ trong 60 epoch. Cuối cùng, đối với một vài thí nghiệm, chúng tôi sử dụng điểm chuẩn Rotated MNIST nhỏ với 5 tác vụ trong đó tác vụ đầu tiên là tập dữ liệu MNIST tiêu chuẩn, và mỗi tác vụ tiếp theo thêm 22,5 độ xoay vào hình ảnh của tác vụ trước đó. Lý do đằng sau thiết lập MNIST của chúng tôi là bằng cách tăng độ xoay, sự dịch chuyển giữa các tác vụ tăng lên và làm cho điểm chuẩn thử thách hơn (Mirzadeh et al., 2021b). Chúng tôi lưu ý rằng các điểm chuẩn Split CIFAR-100 và Split ImageNet-1K sử dụng lớp phân loại đa đầu, trong khi điểm chuẩn MNIST sử dụng lớp phân loại đơn đầu. Do đó, Split CIFAR-100 và Split ImageNet-1K thuộc về cài đặt học tăng dần tác vụ, trong khi Rotated MNIST thuộc về học tăng dần miền (Hsu et al., 2018).

Chúng tôi làm việc với các kiến trúc phổ biến nhất trong tài liệu (học liên tục hoặc khác). Chúng tôi ký hiệu mỗi kiến trúc bằng một mô tả. MLP-N đại diện cho mạng được kết nối đầy đủ với các lớp ẩn có chiều rộng N. Mạng nơ-ron tích chập (CNN) được biểu diễn bằng CNN N trong đó N là hệ số nhân số kênh trong mỗi lớp. Trừ khi có quy định khác, CNN chỉ có các lớp tích chập (với stride là 2), theo sau là lớp feed-forward dày đặc cho phân loại. Đối với các thí nghiệm CIFAR-100, chúng tôi sử dụng ba lớp tích chập, và đối với các thí nghiệm ImageNet-1K, chúng tôi sử dụng sáu lớp tích chập. Hơn nữa, bất cứ khi nào chúng tôi thêm các lớp pooling, chúng tôi thay đổi stride của lớp tích chập thành 1 để giữ nguyên kích thước đặc trưng. ResNet tiêu chuẩn (He et al., 2016) có độ sâu D được ký hiệu bằng ResNet-D và WideResNets (WRN) (Zagoruyko và Komodakis, 2016) được ký hiệu bằng WRN-D-N trong đó D và N tương ứng là độ sâu và chiều rộng. Cuối cùng, chúng tôi cũng sử dụng Vision Transformers (ViT) được đề xuất gần đây (Dosovitskiy et al., 2021). Đối với các thí nghiệm ImageNet-1K, chúng tôi tuân theo quy ước đặt tên trong bài báo gốc (Dosovitskiy et al., 2021). Tuy nhiên, đối với các thí nghiệm Split CIFAR-100, chúng tôi sử dụng các phiên bản nhỏ hơn của ViT trong đó ViT N/M đại diện cho một vision transformer 4 lớp với kích thước ẩn là N và kích thước MLP là M.

Đối với mỗi kiến trúc, chúng tôi tìm kiếm trên một lưới lớn các siêu tham số (tham khảo Phụ lục A) và báo cáo kết quả tốt nhất. Hơn nữa, chúng tôi tính trung bình kết quả trên 5 lần khởi tạo ngẫu nhiên khác nhau, cho các siêu tham số tốt nhất tương ứng, và báo cáo trung bình và độ lệch chuẩn. Cuối cùng, đối với các điểm chuẩn Split CIFAR-100 và Split ImageNet-1K, chúng tôi xáo trộn ngẫu nhiên các nhãn trong mỗi lần chạy, cho 5 lần chạy, để đảm bảo rằng kết quả không bị thiên vị về một thứ tự nhãn cụ thể.

2.1.2 Chỉ số
Chúng tôi quan tâm đến việc so sánh các kiến trúc khác nhau từ hai khía cạnh: (1) một kiến trúc có thể học một tác vụ mới tốt như thế nào tức là khả năng học tập của chúng và (2) một kiến trúc có thể bảo tồn kiến thức trước đó tốt như thế nào tức là khả năng giữ gìn của chúng. Đối với khía cạnh đầu tiên, chúng tôi ghi lại độ chính xác trung bình, độ chính xác học tập và độ chính xác kết hợp/đa tác vụ, trong khi đối với khía cạnh sau, chúng tôi đo lường sự quên lãng trung bình của mô hình. Bây giờ chúng tôi định nghĩa các chỉ số này.

(1) Độ chính xác trung bình ∈ [0,100] (càng cao càng tốt): Độ chính xác xác thực trung bình sau khi mô hình đã được huấn luyện liên tục cho T tác vụ được định nghĩa là: AT = 1/T ∑Ti=1 aT,i, trong đó, at,i là độ chính xác xác thực trên tập dữ liệu của tác vụ i sau khi mô hình hoàn thành học tác vụ t.

(2) Độ chính xác học tập ∈ [0,100] (càng cao càng tốt): Độ chính xác cho mỗi tác vụ ngay sau khi nó được học. Độ chính xác học tập cung cấp một biểu diễn tốt về tính dẻo dai của mô hình và có thể được tính toán bằng: LAT = 1/T ∑Ti=1 ai,i. Lưu ý rằng đối với cả hai điểm chuẩn Split CIFAR-100 và ImageNet-1K, vì các tác vụ bao gồm hình ảnh với nhãn rời rạc, độ lệch chuẩn của chỉ số này có thể cao. Hơn nữa, vì tất cả các mô hình được huấn luyện từ đầu, độ chính xác học tập của vài tác vụ đầu tiên thường nhỏ hơn so với các tác vụ sau này.

(3) Độ chính xác kết hợp ∈ [0,100] (càng cao càng tốt): Độ chính xác của mô hình khi được huấn luyện trên dữ liệu của tất cả các tác vụ cùng nhau.

(4) Sự quên lãng trung bình ∈ [-100,100] (càng thấp càng tốt): Sự quên lãng trung bình được tính toán là sự khác biệt giữa độ chính xác đỉnh và độ chính xác cuối cùng của mỗi tác vụ, sau khi trải nghiệm học liên tục kết thúc. Đối với một điểm chuẩn học liên tục với T tác vụ, nó được định nghĩa là: F = 1/(T-1) ∑T-1i=1 max t∈{1,...,T-1}(at,i - aT,i).

2.2 Kết quả
Chúng tôi đầu tiên so sánh các kiến trúc khác nhau trên các điểm chuẩn Split CIFAR-100 và Split ImageNet-1K. Trong khi phần này tập trung rộng rãi vào khả năng học tập và giữ gìn của các kiến trúc khác nhau, các giải thích đằng sau khoảng cách hiệu suất giữa các kiến trúc khác nhau và phân tích các thành phần kiến trúc khác nhau được đưa ra trong phần tiếp theo.

Bảng 1 liệt kê hiệu suất của các kiến trúc khác nhau trên điểm chuẩn Split CIFAR-100. Có thể đưa ra một số quan sát từ bảng. Đầu tiên, các CNN rất đơn giản, không phải là công nghệ tiên tiến trong các tác vụ phân loại hình ảnh đơn lẻ, vượt trội đáng kể so với cả ResNets, WRNs và ViTs về độ chính xác trung bình và sự quên lãng. Quan sát này đúng với các kích thước chiều rộng và chiều sâu khác nhau trong tất cả các kiến trúc. Một xu hướng tổng thể tương tự, trong đó với số lượng tham số nhất định, CNN đơn giản vượt trội hơn các kiến trúc khác, cũng có thể được thấy trong Hình 1b và 1c.

Thứ hai, việc tăng đơn thuần số lượng tham số, trong hoặc giữa các kiến trúc, không nhất thiết dẫn đến việc tăng hiệu suất trong học liên tục. Ví dụ, ResNet-18 và ResNet-34 có hiệu suất gần như giống nhau mặc dù số lượng tham số trong cái sau gần gấp đôi. Tương tự, WRN-10-10 vượt trội hơn WRN-16-10 và WRN-28-10 mặc dù có số lượng tham số ít hơn đáng kể. Lưu ý rằng chúng tôi không rút ra nguyên tắc chung rằng việc tham số hóa quá mức không hữu ích trong học liên tục. Thực tế, trong một số trường hợp, nó thực sự hữu ích như có thể thấy trong việc cải thiện hiệu suất toàn diện khi ResNet-34 được so sánh với ResNet-50 hoặc khi WRN-10-2 được so sánh với WRN-10-10. Trong phần tiếp theo, chúng tôi phân tích khi nào việc tham số hóa quá mức có thể giúp hiệu suất trong học liên tục.

Cuối cùng, so sánh rõ ràng khả năng học tập và giữ gìn của các kiến trúc khác nhau, có thể thấy từ bảng rằng ResNets và WRNs có độ chính xác học tập cao hơn cho thấy chúng tốt hơn trong việc học một tác vụ mới. Điều này cũng giải thích việc sử dụng thường xuyên của chúng trong các cài đặt tác vụ đơn lẻ. Tuy nhiên, về mặt giữ gìn, CNN và ViTs tốt hơn nhiều, như được chứng minh bởi số lượng quên lãng thấp hơn của chúng. Điều này được chứng minh thêm trong Hình 2a (CIFAR-100) và Hình 2b (ImageNet-1K), nơi có thể thấy rằng ResNets và WRNs học từng tác vụ cá nhân tốt hơn nhiều dẫn đến độ chính xác trung bình cao hơn cho vài tác vụ đầu tiên. Tuy nhiên, khi số lượng tác vụ tăng lên, CNN vượt trội hơn các kiến trúc khác do sự quên lãng nhỏ hơn của chúng, cuối cùng dẫn đến đường cong độ chính xác trung bình phẳng hơn tổng thể.

--- TRANG 3 ---
Bảng 1: Split CIFAR-100: khả năng học tập và giữ gìn có thể khác nhau đáng kể giữa các kiến trúc khác nhau.

Mô hình | Tham số (M) | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập
---|---|---|---|---
CNN x1 | 0.3 | 62.2 ± 1.35 | 12.6 ± 1.14 | 74.1 ± 7.72
CNN x2 | 0.8 | 66.3 ± 1.12 | 10.1 ± 0.98 | 75.8 ± 7.2
CNN x4 | 2.3 | 68.1 ± 0.5 | 8.7 ± 0.21 | 76.4 ± 6.92
CNN x8 | 7.5 | 69.9 ± 0.62 | 8.0 ± 0.71 | 77.5 ± 6.78
CNN x16 | 26.9 | 76.8 ± 0.76 | 4.7 ± 0.84 | 81.0 ± 6.97
ResNet-18 | 11.2 | 45.0 ± 0.63 | 36.8 ± 1.08 | 74.9 ± 3.98
ResNet-34 | 21.3 | 44.8 ± 2.34 | 39.9 ± 2.28 | 72.6 ± 6.34
ResNet-50 | 23.6 | 56.2 ± 0.88 | 9.5 ± 0.38 | 67.8 ± 5.09
ResNet-101 | 42.6 | 56.8 ± 1.62 | 9.2 ± 0.89 | 65.7 ± 5.42
WRN-10-2 | 0.3 | 50.5 ± 2.65 | 36.5 ± 2.74 | 84.5 ± 5.04
WRN-10-10 | 7.7 | 56.8 ± 2.03 | 31.7 ± 1.34 | 86.7 ± 4.94
WRN-16-2 | 0.7 | 44.6 ± 2.81 | 41.4 ± 1.43 | 82.4 ± 6.09
WRN-16-10 | 17.3 | 51.3 ± 1.47 | 37.6 ± 2.22 | 86.9 ± 3.96
WRN-28-2 | 5.9 | 46.6 ± 2.27 | 39.5 ± 2.29 | 82.5 ± 6.26
WRN-28-10 | 36.7 | 49.3 ± 2.02 | 35.8 ± 2.56 | 82.5 ± 6.26
ViT-512/1024 | 8.8 | 51.7 ± 1.4 | 21.9 ± 1.3 | 71.4 ± 5.52
ViT-1024/1546 | 30.7 | 60.4 ± 1.56 | 12.2 ± 1.12 | 67.4 ± 5.57

Bảng 2: Split ImageNet-1K: khả năng học tập và giữ gìn có thể khác nhau đáng kể giữa các kiến trúc khác nhau.

Mô hình | Tham số (M) | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập
---|---|---|---|---
CNN x3 | 9.1 | 63.3 ± 0.68 | 5.4 ± 0.93 | 71.6 ± 2.31
CNN x6 | 24.2 | 66.7 ± 0.62 | 3.9 ± 0.86 | 70.1 ± 3.21
CNN x12 | 72.4 | 67.8 ± 1.04 | 2.8 ± 0.7 | 70.3 ± 2.82
ResNet-34 | 21.8 | 62.7 ± 0.53 | 17.3 ± 0.58 | 78.4 ± 2.57
ResNet-50 | 25.5 | 66.1 ± 0.69 | 19.0 ± 0.67 | 83.3 ± 1.57
ResNet-101 | 44.5 | 64.1 ± 0.72 | 18.9 ± 1.32 | 81.1 ± 2.89
WRN-50-2 | 68.9 | 63.2 ± 1.61 | 21.7 ± 1.73 | 85.8 ± 1.65
ViT-Base | 86.1 | 58.3 ± 0.65 | 15.9 ± 1.11 | 72.8 ± 2.25
ViT-Large | 307.4 | 60.7 ± 1.31 | 10.6 ± 1.1 | 73.2 ± 2.12

Xu hướng tương tự như CIFAR-100 cũng có thể thấy trong điểm chuẩn ImageNet-1K như được hiển thị trong Bảng 2. Tuy nhiên, sự khác biệt hiệu suất, được đo bằng độ chính xác trung bình, giữa CNN và các kiến trúc khác nhỏ hơn so với CIFAR-100. Chúng tôi tin rằng điều này là do độ chính xác học tập rất cao của các kiến trúc khác so với CNN trên điểm chuẩn này, và do đó việc sử dụng thường xuyên của chúng trong các cài đặt tác vụ đơn lẻ, dẫn đến việc cải thiện độ chính xác trung bình cuối cùng. Sự quên lãng trung bình của CNN vẫn nhỏ hơn nhiều so với các kiến trúc khác.

Tổng thể, từ cả hai bảng, chúng tôi kết luận rằng ResNets và WRNs có khả năng học tập tốt hơn trong khi CNN và ViTs có khả năng giữ gìn tốt hơn. Trong các thí nghiệm của chúng tôi, CNN đơn giản đạt được sự đánh đổi tốt nhất giữa học tập và giữ gìn.

--- TRANG 4 ---
3 Vai trò của các Thành phần Kiến trúc

Bây giờ chúng tôi nghiên cứu các thành phần riêng lẻ trong các kiến trúc khác nhau để hiểu cách chúng tác động đến hiệu suất học liên tục. Chúng tôi bắt đầu bằng các thuộc tính cấu trúc chung trong tất cả các kiến trúc như chiều rộng và chiều sâu (cf. Sec. 3.1), và cho thấy rằng khi chiều rộng tăng, sự quên lãng giảm. Trong Sec. 3.2, chúng tôi nghiên cứu tác động của batch normalization và quan sát rằng nó có thể cải thiện đáng kể độ chính xác học tập trong học liên tục. Sau đó, trong Sec. 3.3, chúng tôi thấy rằng việc thêm skip connections (hoặc shortcuts) vào CNN không nhất thiết cải thiện hiệu suất CL trong khi các lớp pooling (cf. Sec. 3.4 và Sec. 3.5) có thể có tác động đáng kể đến độ chính xác học tập và sự quên lãng. Hơn nữa, chúng tôi nghiên cứu ngắn gọn tác động của các attention heads trong ViT trong Sec. 3.6. Cuối cùng, dựa trên các quan sát mà chúng tôi đưa ra trong các phần đã nêu, trong Sec. 3.7, chúng tôi cung cấp tóm tắt các sửa đổi có thể cải thiện các kiến trúc khác nhau trên cả hai điểm chuẩn Split CIFAR-100 và ImageNet-1K.

3.1 Chiều rộng và Chiều sâu
Gần đây, Mirzadeh et al. (2021a) đã chỉ ra rằng các mạng nơ-ron rộng quên ít thảm khốc hơn bằng cách nghiên cứu tác động của chiều rộng và chiều sâu trong các mô hình MLP và WideResNet. Chúng tôi mở rộng nghiên cứu của họ bằng cách xác nhận kết luận của họ trên nhiều kiến trúc và điểm chuẩn hơn.

Bảng 3: Vai trò của chiều rộng và chiều sâu: tăng số lượng tham số (bằng cách tăng chiều rộng) làm giảm sự quên lãng và do đó tăng độ chính xác trung bình. Tuy nhiên, tăng chiều sâu không nhất thiết cải thiện hiệu suất, và do đó, điều quan trọng là phải phân biệt giữa việc mở rộng các mô hình bằng cách làm cho chúng sâu hơn và rộng hơn.

Điểm chuẩn | Mô hình | Chiều sâu | Tham số (M) | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập
---|---|---|---|---|---|---
Rot MNIST | MLP-128 | 2 | 0.1 | 70.8 ± 0.68 | 31.5 ± 0.92 | 96.0 ± 0.90
Rot MNIST | MLP-128 | 8 | 0.2 | 68.9 ± 1.07 | 35.4 ± 1.34 | 97.3 ± 0.76
Rot MNIST | MLP-256 | 2 | 0.3 | 71.1 ± 0.43 | 31.4 ± 0.48 | 96.1 ± 0.82
Rot MNIST | MLP-256 | 8 | 0.7 | 70.4 ± 0.61 | 32.1 ± 0.75 | 96.3 ± 0.77
Rot MNIST | MLP-512 | 2 | 0.7 | 72.6 ± 0.27 | 29.6 ± 0.36 | 96.4 ± 0.73
CIFAR-100 | CNN x4 | 3 | 2.3 | 68.1 ± 0.5 | 8.7 ± 0.21 | 76.4 ± 6.92
CIFAR-100 | CNN x4 | 6 | 5.4 | 62.9 ± 0.86 | 12.4 ± 1.62 | 77.7 ± 5.49
CIFAR-100 | CNN x8 | 3 | 7.5 | 69.9 ± 0.62 | 8.0 ± 0.71 | 77.5 ± 6.78
CIFAR-100 | CNN x8 | 6 | 19.9 | 66.5 ± 1.01 | 10.7 ± 1.19 | 76.6 ± 4.78
CIFAR-100 | ViT 512/1024 | 2 | 4.6 | 56.4 ± 1.14 | 15.9 ± 0.95 | 68.1 ± 7.15
CIFAR-100 | ViT 512/1024 | 4 | 8.8 | 51.7 ± 1.4 | 21.9 ± 1.3 | 71.4 ± 5.52

Bảng 3 cho thấy rằng trên tất cả các kiến trúc, việc tham số hóa quá mức thông qua việc tăng chiều rộng có lợi trong việc cải thiện hiệu suất học liên tục như được chứng minh bởi sự quên lãng thấp hơn và số độ chính xác trung bình cao hơn. Đối với MLP, khi chiều rộng tăng từ 128 lên 512, hiệu suất trong tất cả các thước đo đều cải thiện. Tuy nhiên, đối với cả MLP-128 và MLP-256 khi chiều sâu tăng từ 2 lên 8, độ chính xác trung bình giảm, và sự quên lãng trung bình tăng với một mức tăng nhỏ trong độ chính xác học tập. Cuối cùng, lưu ý rằng MLP-256 với 8 lớp có khoảng cùng số lượng tham số như MLP-512 với 2 lớp. Tuy nhiên, mạng rộng hơn trong hai mạng này có hiệu suất học liên tục tốt hơn.

Một phân tích tương tự cho ResNets và WideResNets được thể hiện trong Bảng 1. ResNet-50 và ResNet-101 rộng gấp bốn lần so với ResNet-18 và ResNet-34, và từ bảng, có thể thấy rằng chiều rộng này dẫn đến những cải thiện mạnh mẽ trong độ chính xác trung bình và sự quên lãng. Tương tự, ResNet-34 và ResNet-101 là các phiên bản sâu hơn của ResNet-18 và ResNet-50, tương ứng. Chúng ta có thể quan sát rằng việc tăng chiều sâu không hữu ích trong trường hợp này. Cuối cùng, WRN-10-10, WRN-16-10, và WRN-28-10 rộng hơn vượt trội hơn WRN-10-2, WRN-10-10, WRN-28-10 hẹp hơn, tương ứng. Trong khi nếu chúng ta cố định chiều rộng, việc tăng chiều sâu không hữu ích. Tổng thể, chúng tôi kết luận rằng việc tham số hóa quá mức thông qua chiều rộng có lợi trong học liên tục, trong khi một tuyên bố tương tự không thể được đưa ra cho chiều sâu.

Một giải thích có thể cho điều này là thông qua chế độ huấn luyện lười biếng, trực giao hóa gradient và sparse hóa được tạo ra bởi việc tăng chiều rộng (Mirzadeh et al., 2021a). Được thúc đẩy bởi quan sát này, sau đó, chúng tôi cho thấy tại sao các lớp global average pooling làm tổn hại hiệu suất học liên tục và cách chúng ta có thể giảm thiểu điều đó.

3.2 Batch Normalization
Batch Normalization (BN) (Ioffe và Szegedy, 2015) là một sơ đồ chuẩn hóa được chứng minh là tăng tốc độ hội tụ của mạng do các lợi ích tối ưu hóa và tổng quát hóa của nó (Santurkar et al., 2018; Bjorck et al., 2018). Một lợi thế khác của lớp BN là khả năng giảm vấn đề covariate shift có liên quan cụ thể đến học liên tục nơi phân phối dữ liệu có thể thay đổi từ tác vụ này sang tác vụ khác.

Có tương đối ít công trình nghiên cứu BN trong bối cảnh của học liên tục. Mirzadeh et al. (2020) đã phân tích BN trong học liên tục thông qua góc độ tổng quát hóa. Đồng thời với công trình này, Pham et al. (2022) nghiên cứu các sơ đồ chuẩn hóa trong học liên tục và chỉ ra rằng BN cho phép cải thiện việc học của mỗi tác vụ. Ngoài ra, các tác giả chỉ ra rằng trong sự hiện diện của bộ đệm replay của các tác vụ trước đó, BN tạo điều kiện cho việc chuyển giao kiến thức tốt hơn so với các sơ đồ chuẩn hóa khác như Group Normalization (Wu và He, 2018).

Tuy nhiên, theo trực giác, người ta có thể nghĩ rằng vì các thống kê BN đang thay đổi giữa các tác vụ, do phân phối dữ liệu tiến hóa trong học liên tục, và người ta không giữ thống kê của mỗi tác vụ, BN sẽ góp phần vào việc tăng sự quên lãng. Điều này không đúng trong một số thí nghiệm mà chúng tôi đã tiến hành. Tương tự như kết quả trong Mirzadeh et al. (2020); Pham et al. (2022), chúng tôi thấy BN tạo điều kiện cho độ chính xác học tập trong Split CIFAR-100 và split ImageNet-1K (cf. Bảng 4 và Bảng 7). Chúng tôi tin rằng điều này có thể là do các thống kê BN tương đối không thay đổi giữa các tác vụ trong các tập dữ liệu này. Để xác minh điều này, trong Phụ lục B.2, chúng tôi vẽ đồ thị các thống kê BN của lớp đầu tiên của CNN 4 trên tập dữ liệu Split CIFAR-100, và chúng tôi cho thấy rằng các thống kê BN ổn định trong suốt trải nghiệm học liên tục.

Tuy nhiên, nếu giả thuyết này đúng, điều ngược lại - một điểm chuẩn nơi các thống kê BN thay đổi nhiều giữa các tác vụ, như Permuted MNIST - sẽ làm tổn hại hiệu suất học liên tục. Trong Phụ lục B.3, chúng tôi vẽ đồ thị các thống kê BN của lớp đầu tiên của MLP-128 trên Permuted MNIST. Có thể thấy từ hình rằng thực sự các thống kê BN đang thay đổi trong điểm chuẩn này. Kết quả là, việc thêm BN vào điểm chuẩn này làm tổn hại đáng kể hiệu suất, như được chứng minh bởi việc tăng sự quên lãng trong Bảng 9.

Tổng thể, chúng tôi kết luận rằng hiệu ứng của lớp batchnorm phụ thuộc vào dữ liệu. Trong các thiết lập nơi phân phối đầu vào tương đối ổn định, như Split CIFAR-100 hoặc Split ImageNet-1K, lớp BN cải thiện hiệu suất học liên tục bằng cách tăng khả năng học tập của các mô hình. Tuy nhiên, đối với các thiết lập nơi phân phối đầu vào thay đổi nhiều giữa các tác vụ, như Permuted MNIST, lớp BN có thể làm tổn hại hiệu suất học liên tục bằng cách tăng sự quên lãng.

--- TRANG 5 ---
Bảng 4: Vai trò của các thành phần khác nhau cho điểm chuẩn Split CIFAR-100: Trong khi việc thêm skip connections không có tác động đáng kể đến hiệu suất, batch normalization và max pooling có thể cải thiện đáng kể độ chính xác học tập của CNN.

Mô hình | Tham số (M) | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập | Độ chính xác kết hợp
---|---|---|---|---|---
CNN x4 | 2.3 | 68.1 ± 0.5 | 8.7 ± 0.21 | 76.4 ± 6.92 | 73.4 ± 0.89
CNN x4 + Skip | 2.4 | 68.2 ± 0.56 | 8.9 ± 0.72 | 76.6 ± 7.07 | 73.8 ± 0.47
CNN x4 + BN | 2.3 | 74.0 ± 0.56 | 8.1 ± 0.35 | 81.7 ± 6.68 | 80.2 ± 0.16
CNN x4 + AvgPool | 2.3 | 68.5 ± 0.6 | 8.3 ± 0.57 | 76.3 ± 7.63 | 73.6 ± 0.83
CNN x4 + MaxPool | 2.3 | 74.4 ± 0.34 | 9.3 ± 0.47 | 83.3 ± 6.1 | 79.9 ± 0.53
CNN x4 + All | 2.4 | 77.7 ± 0.77 | 6.5 ± 0.58 | 83.7 ± 6.31 | 81.6 ± 0.77
CNN x8 | 7.5 | 69.9 ± 0.62 | 8.0 ± 0.71 | 77.5 ± 6.78 | 74.1 ± 0.83
CNN x8 + Skip | 7.8 | 70.7 ± 0.31 | 6.8 ± 0.91 | 77.1 ± 6.87 | 74.4 ± 0.35
CNN x8 + BN | 7.5 | 76.1 ± 0.3 | 5.9 ± 0.16 | 81.7 ± 6.83 | 80.5 ± 0.27
CNN x8 + AvgPool | 7.5 | 71.2 ± 0.5 | 8.3 ± 0.35 | 79.0 ± 7.05 | 74.0 ± 1.02
CNN x8 + MaxPool | 7.5 | 77.2 ± 0.53 | 7.1 ± 0.33 | 84.0 ± 5.81 | 80.6 ± 0.35
CNN x8 + All | 7.8 | 78.1 ± 1.15 | 5.7 ± 0.36 | 83.3 ± 6.27 | 81.9 ± 0.51
CNN x16 | 26.9 | 76.8 ± 0.76 | 4.7 ± 0.84 | 81.0 ± 6.97 | 79.1 ± 0.86
CNN x16 + All | 27.9 | 78.9 ± 0.27 | 4.5 ± 0.36 | 82.9 ± 6.48 | 82.1 ± 0.46

3.3 Skip Connections
Skip connections (Cho et al., 2011), được đề xuất ban đầu cho các mô hình tích chập bởi He et al. (2016), là rất quan trọng trong kiến trúc ResNet được sử dụng rộng rãi. Chúng cũng được sử dụng trong nhiều kiến trúc khác như transformers (Vaswani et al., 2017). Nhiều công trình đã được thực hiện để giải thích tại sao skip connections hữu ích: Hardt và Ma (2016) chỉ ra rằng skip connection có xu hướng loại bỏ các điểm tối ưu cục bộ giả; Bartlett et al. (2018b) nghiên cứu tính biểu đạt hàm của kiến trúc dư; Bartlett et al. (2018a) chỉ ra rằng gradient descent có thể chứng minh học được các phép biến đổi tuyến tính trong ResNets; Jastrzebski et al. (2017) chỉ ra rằng skip connections có xu hướng tinh chỉnh lặp đi lặp lại các biểu diễn đã học. Tuy nhiên, các công trình này chủ yếu tập trung vào việc học một tác vụ duy nhất. Trong các vấn đề học liên tục, do sự hiện diện của sự dịch chuyển phân phối, không rõ ràng liệu những lợi ích này của skip connections vẫn có tác động đáng kể đến hiệu suất mô hình, như sự quên lãng và độ chính xác trung bình trên các tác vụ.

Ở đây, chúng tôi nghiên cứu thực nghiệm tác động của skip connection đến các vấn đề học liên tục. Thật thú vị, như được minh họa trong Bảng 4, việc thêm skip connections vào CNN đơn giản không thay đổi hiệu suất đáng kể, và kết quả rất gần nhau (trong độ lệch chuẩn) của nhau. Do đó, chúng tôi kết luận rằng skip connection có thể không có tác động tích cực hoặc tiêu cực đáng kể đến hiệu suất mô hình trong các điểm chuẩn học liên tục của chúng tôi.

3.4 Lớp Pooling
Các lớp pooling là nền tảng của hiệu suất cải thiện của CNN trước ResNets. Các lớp pooling không chỉ thêm tính bất biến dịch chuyển cục bộ, giúp ích trong các ứng dụng như phân loại đối tượng (Krizhevsky et al., 2017; Dai et al., 2021), mà còn giảm độ phân giải không gian của các đặc trưng mạng, dẫn đến việc giảm chi phí tính toán. Vì một họ kiến trúc mà chúng tôi nghiên cứu là CNN hoàn toàn tích chập, chúng tôi xem xét lại vai trò của các lớp pooling trong các kiến trúc này trong thiết lập học liên tục. Về điều này, chúng tôi so sánh mạng không có pooling, CNN N, với những mạng có lớp pooling ('CNN N + AvgPool' hoặc 'CNN N + MaxPool') trong Bảng 4. Để giữ kích thước đặc trưng cố định, chúng tôi đặt stride tích chập từ 2 về 1 khi pooling được sử dụng. Chúng tôi đưa ra các quan sát sau từ bảng.

Đầu tiên, average pooling (+AvgPool) không có tác động đáng kể nào đến các chỉ số học liên tục. Thứ hai, max pooling (+MaxPool) cải thiện đáng kể khả năng học tập của mạng, được đo bằng việc cải thiện độ chính xác học tập. Thứ ba, về mặt giữ gìn, các lớp pooling không có tác động đáng kể như được đo bằng các số lượng quên lãng tương tự. Tất cả mọi thứ xem xét, chúng ta thấy rằng max pooling đạt được hiệu suất tốt nhất về độ chính xác trung bình, do khả năng học tập vượt trội của nó.

Khả năng của max pooling đạt được hiệu suất tốt hơn trong thiết lập học liên tục có thể được quy cho một quan sát thực nghiệm nổi tiếng bởi Springenberg et al. (2015), nơi chỉ ra rằng max pooling với stride 1 vượt trội hơn CNN với stride 2 và không có pooling. Hơn nữa, chúng tôi tin rằng max pooling có thể đã trích xuất các đặc trưng mức thấp, như cạnh, tốt hơn, dẫn đến việc cải thiện học tập trong tập dữ liệu như CIFAR-100 bao gồm hình ảnh tự nhiên. Có một số bằng chứng trong tài liệu rằng max pooling cung cấp các đặc trưng thưa thớt hơn và định vị chính xác (Zhou et al., 2016). Điều này có thể đã chuyển sang thiết lập học liên tục mà chúng tôi xem xét. Tuy nhiên, nó vẫn là một hướng tương lai thú vị để nghiên cứu thêm những lợi ích mà max pooling mang lại trong cả thiết lập học tiêu chuẩn và liên tục.

3.5 Lớp Global Pooling
Các lớp global average pooling (GAP) thường được sử dụng trong các mạng tích chập ngay trước lớp phân loại cuối cùng để giảm số lượng tham số trong bộ phân loại. Hệ quả của việc thêm lớp GAP là giảm chiều rộng của bộ phân loại cuối cùng. Được lập luận trong Sec. 3.1 rằng các mạng rộng hơn quên ít hơn. Do đó, các kiến trúc có lớp GAP có thể chịu sự quên lãng tăng.

Bảng 5 chứng minh thực nghiệm trực giác này. Từ bảng có thể thấy rằng việc áp dụng lớp GAP làm tăng đáng kể sự quên lãng dẫn đến độ chính xác trung bình thấp hơn. Trong phần trước, chúng tôi đã chứng minh rằng average pooling không dẫn đến giảm hiệu suất miễn là kích thước đặc trưng không gian giống nhau. Để chứng minh rằng không có gì vốn có đối với lớp GAP, và nó chỉ là hệ quả của việc giảm chiều rộng của bộ phân loại cuối cùng, chúng tôi xây dựng một đường cơ sở khác bằng cách nhân số lượng kênh trong lớp tích chập cuối cùng với 16 và sau đó áp dụng GAP. Mạng này được ký hiệu là "CNN x4 (16x)" trong Bảng 5 và nó có cùng chiều rộng bộ phân loại như mạng không có GAP. Có thể thấy kiến trúc này có sự quên lãng nhỏ hơn đáng kể cho thấy GAP ảnh hưởng đến học liên tục thông qua chiều rộng của bộ phân loại cuối cùng.

Bảng 5: Vai trò của Global Average Pooling (GAP) cho Split CIFAR-100: liên quan đến các lập luận của chúng tôi trong Sec. 3.1, việc thêm GAP vào CNN làm tăng đáng kể sự quên lãng. Sau đó, chúng tôi cho thấy rằng việc loại bỏ GAP khỏi ResNets cũng có thể giảm đáng kể sự quên lãng.

Mô hình | Tham số (M) | Chiều rộng Pre-Classification | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập | Độ chính xác kết hợp
---|---|---|---|---|---|---
CNN x4 | 2.3 | 8192 | 68.1 ± 0.5 | 8.7 ± 0.21 | 76.4 ± 6.92 | 73.4 ± 0.89
CNN x4 + GAP | 1.5 | 512 | 60.1 ± 0.43 | 14.3 ± 0.8 | 66.1 ± 7.76 | 76.9 ± 0.81
CNN x4 (16x) + GAP | 32.3 | 8192 | 73.6 ± 0.39 | 5.2 ± 0.66 | 75.6 ± 4.77 | 77.9 ± 0.37
CNN x8 | 7.5 | 16384 | 69.9 ± 0.62 | 8.0 ± 0.71 | 77.5 ± 6.78 | 74.1 ± 0.83
CNN x8 + GAP | 6.1 | 1024 | 63.1 ± 2.0 | 14.7 ± 1.68 | 70.1 ± 7.18 | 78.3 ± 0.97
CNN x16 | 26.9 | 32768 | 76.8 ± 0.76 | 4.7 ± 0.84 | 81.0 ± 6.97 | 74.6 ± 0.86
CNN x16 + GAP | 23.8 | 2048 | 66.3 ± 0.82 | 12.2 ± 0.65 | 72.3 ± 6.02 | 78.9 ± 0.27

Được lấy cảm hứng từ quan sát này, trong Sec. 3.7 chúng tôi cho thấy rằng hiệu suất của ResNets trong học liên tục có thể được cải thiện đáng kể bằng cách loại bỏ lớp GAP hoặc sử dụng các lớp average pooling nhỏ hơn thay vì GAP.

--- TRANG 6 ---
3.6 Attention Heads
Cơ chế attention là một thành phần nổi bật trong các kiến trúc dựa trên transformer đã có thành công lớn trong xử lý ngôn ngữ tự nhiên và gần đây hơn là các nhiệm vụ thị giác máy tính. Đối với trường hợp sau, các heads của vision transformers (ViTs) được chỉ ra là chú ý đến cả đặc trưng cục bộ và toàn cục trong hình ảnh Dosovitskiy et al. (2021). Chúng tôi nhân đôi số lượng heads trong khi cố định chiều rộng để đảm bảo rằng bất kỳ thay đổi nào trong kết quả không phải do kích thước biểu diễn tăng. Trong Bảng 6, có thể thấy rằng thậm chí việc nhân đôi số lượng heads trong ViTs chỉ cải thiện nhẹ trong việc tăng độ chính xác học tập và giảm sự quên lãng. Điều này cho thấy rằng việc tăng số lượng attention heads có thể không phải là một cách tiếp cận hiệu quả để cải thiện hiệu suất học liên tục trong ViTs. Tuy nhiên, chúng tôi lưu ý rằng phù hợp với các quan sát khác trong tài liệu (Paul và Chen, 2022), ViTs cho thấy khả năng chống chịu đầy hứa hẹn trước sự dịch chuyển phân phối như được chứng minh bởi số lượng quên lãng thấp hơn, cho cùng một lượng tham số, trên điểm chuẩn Split CIFAR-100 (cf. Hình 1c).

Bảng 6: Vai trò của attention heads: đối với điểm chuẩn Split CIFAR-100, việc tăng số lượng attention heads (trong khi cố định tổng chiều rộng), không tác động đáng kể đến hiệu suất.

Mô hình | Heads | Tham số (M) | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập
---|---|---|---|---|---
ViT 512/1024 | 4 | 8.8 | 50.9 ± 0.73 | 23.8 ± 1.3 | 72.8 ± 6.13
ViT 512/1024 | 8 | 8.8 | 51.7 ± 1.4 | 21.9 ± 1.3 | 71.4 ± 5.52
ViT 1024/1536 | 4 | 30.7 | 57.4 ± 1.59 | 14.4 ± 1.96 | 66.0 ± 5.89
ViT 1024/1536 | 8 | 30.7 | 60.4 ± 1.56 | 12.2 ± 1.12 | 67.4 ± 5.57

3.7 Cải thiện Kiến trúc
Bây giờ chúng tôi cung cấp các gợi ý thực tế để cải thiện hiệu suất của các kiến trúc, được rút ra từ các thí nghiệm của chúng tôi trong các phần trước.

Đối với CNN, chúng tôi thêm BatchNormalization và MaxPooling, vì cả hai đều được chỉ ra là cải thiện khả năng học tập của mô hình và skip connections giúp bài toán tối ưu hóa dễ dàng hơn. Bảng 7 cho thấy kết quả trên cả CIFAR-100 và ImageNet-1K. Có thể thấy từ bảng rằng việc thêm các thành phần này cải thiện đáng kể hiệu suất CNN, như được chứng minh bởi việc cải thiện gần như tất cả các thước đo, bao gồm độ chính xác trung bình.

Đối với ResNets, chúng tôi loại bỏ GAP, như trường hợp với CIFAR-100, hoặc average cục bộ các đặc trưng trong lớp gần cuối bằng bộ lọc 4x4, như trường hợp với ImageNet-1K. Lý do không loại bỏ hoàn toàn average pooling khỏi ImageNet-1K là sự tăng lớn kết quả trong số lượng tham số trong lớp phân loại. Có thể thấy từ Bảng 7 rằng việc loại bỏ hoàn toàn hoặc một phần GAP, CIFAR-100 và ImageNet-1K tương ứng, cải thiện rất nhiều khả năng giữ gìn của ResNets, được chỉ ra bởi sự quên lãng thấp hơn của chúng.

--- TRANG 7 ---
Bảng 7: Cải thiện kiến trúc CNN và ResNets trên cả hai điểm chuẩn Split CIFAR-100 và ImageNet-1K.

Mô hình | Điểm chuẩn | Tham số (M) | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập
---|---|---|---|---|---
CNN x4 | CIFAR-100 | 2.3 | 68.1 ± 0.5 | 8.7 ± 0.21 | 76.4 ± 6.92
CNN x4 + BN + MaxPool + Skip | CIFAR-100 | 1.5 | 77.7 ± 0.77 | 6.5 ± 0.58 | 83.7 ± 6.31
CNN x8 | CIFAR-100 | 7.5 | 69.9 ± 0.62 | 8.0 ± 0.71 | 77.5 ± 6.78
CNN x8 + BN + MaxPool + Skip | CIFAR-100 | 6.1 | 78.1 ± 1.15 | 5.7 ± 0.36 | 83.3 ± 6.27
CNN x3 | ImageNet-1K | 9.1 | 63.3 ± 0.68 | 5.8 ± 0.93 | 71.6 ± 2.31
CNN x3 + BN + MaxPool + Skip | ImageNet-1K | 9.1 | 66.4 ± 0.47 | 5.4 ± 0.3 | 74.7 ± 2.1
CNN x6 | ImageNet-1K | 24.2 | 66.7 ± 0.62 | 3.9 ± 0.86 | 70.1 ± 3.21
CNN x6 + BN + MaxPool + Skip | ImageNet-1K | 24.3 | 72.1 ± 0.41 | 4.0 ± 0.22 | 75.7 ± 2.57
ResNet-18 | CIFAR-100 | 11.2 | 45.0 ± 0.63 | 36.8 ± 1.08 | 74.9 ± 3.98
ResNet-18 w/o GAP | CIFAR-100 | 11.9 | 67.4 ± 0.76 | 11.2 ± 1.98 | 74.2 ± 4.79
ResNet-50 | CIFAR-100 | 23.6 | 56.2 ± 0.88 | 9.5 ± 0.38 | 67.8 ± 5.09
ResNet-50 w/o GAP | CIFAR-100 | 26.7 | 71.4 ± 0.29 | 6.6 ± 0.12 | 73.0 ± 5.18
ResNet-34 | ImageNet-1K | 21.8 | 62.7 ± 0.53 | 19.0 ± 0.67 | 80.4 ± 2.57
ResNet-34 w 4x4 AvgPool | ImageNet-1K | 23.3 | 66.0 ± 0.24 | 4.2 ± 0.16 | 70.2 ± 3.87
ResNet-50 | ImageNet-1K | 25.5 | 66.1 ± 0.69 | 17.3 ± 0.58 | 83.3 ± 1.57
ResNet-50 w 4x4 AvgPool | ImageNet-1K | 31.7 | 67.2 ± 0.13 | 3.5 ± 0.35 | 72.8 ± 3.27

4 Công trình Liên quan
Như chúng tôi đã thảo luận trước đây, công trình của chúng tôi tập trung vào kiến trúc trong học liên tục hơn là khía cạnh thuật toán. Do đó, chúng tôi bắt đầu với một tổng quan ngắn gọn về các thuật toán học liên tục, và sau đó chúng tôi tập trung chủ yếu vào phần của tài liệu có liên quan nhiều hơn đến kiến trúc.

4.1 Thuật toán
Về mặt thuật toán của nghiên cứu, các phương pháp khác nhau đã được đề xuất để giảm thiểu vấn đề quên lãng. (Zenke et al., 2017; Kirkpatrick et al., 2017; Mirzadeh et al., 2021b; Yin et al., 2020) xác định kiến thức thiết yếu từ quá khứ, thường dưới dạng tham số hoặc đặc trưng, và sửa đổi mục tiêu huấn luyện của tác vụ mới để bảo tồn kiến thức đã học. Tuy nhiên, trong thực tế, các phương pháp replay có vẻ hiệu quả hơn nhiều vì chúng giữ một tập con nhỏ dữ liệu đã thấy trước đó và sử dụng chúng trực tiếp khi học từ dữ liệu mới (ví dụ, experience replay) (Chaudhry et al., 2019; Rolnick et al., 2019; Riemer et al., 2018) hoặc kết hợp chúng như một phần của quá trình tối ưu hóa (ví dụ, gradient projection) (Farajtabar et al., 2020; Chaudhry et al., 2018; Balaji et al., 2020), hoặc học một mô hình sinh (Shin et al., 2017; Kirichenko et al., 2021) hoặc kernel (Derakhshani et al., 2021) để làm như vậy.

Có lẽ khía cạnh thuật toán liên quan nhất đến công trình của chúng tôi là các phương pháp cách ly tham số nơi các phần khác nhau của mô hình được dành riêng cho một tác vụ cụ thể (Yoon et al., 2018; Mallya và Lazebnik, 2018; Wortsman et al., 2020). Thông thường, các thuật toán này bắt đầu với một mô hình có kích thước cố định, và thuật toán nhằm phân bổ một tập con của mô hình cho mỗi tác vụ sao cho chỉ một phần cụ thể của mô hình chịu trách nhiệm cho kiến thức của tác vụ đó. Ví dụ, PackNet (Mallya và Lazebnik, 2018) sử dụng pruning lặp để giải phóng không gian cho các tác vụ tương lai, trong khi trong (Wortsman et al., 2020) các tác giả đề xuất cố định với một mạng được khởi tạo ngẫu nhiên và tham số hóa quá mức để tìm một mask nhị phân cho mỗi tác vụ. Tuy nhiên, mặc dù trọng tâm của các phương pháp này là mô hình và các tham số của nó, mục tiêu chính là có một thuật toán có thể sử dụng mô hình hiệu quả nhất có thể. Do đó, tầm quan trọng của kiến trúc thường bị bỏ qua.

Chúng tôi tin rằng công trình của chúng tôi trực giao với khía cạnh thuật toán của nghiên cứu học liên tục vì bất kỳ phương pháp nào được đề cập ở trên đều có thể sử dụng một kiến trúc khác nhau cho thuật toán được đề xuất của chúng, như chúng tôi đã chỉ ra trong Hình 1a và Bảng 8.

4.2 Kiến trúc
Đã có một số nỗ lực trong việc áp dụng tìm kiếm kiến trúc vào học liên tục (Xu và Zhu, 2018; Huang et al., 2019; Gao et al., 2020; Li et al., 2019; Lee et al., 2021). Tuy nhiên, khi nói đến tìm kiếm kiến trúc, trọng tâm hiện tại của tài liệu học liên tục chủ yếu là quyết định cách chia sẻ hoặc mở rộng mô hình hiệu quả bằng cách ủy thác những quyết định này cho bộ điều khiển. Ví dụ, cả Lee et al. (2021) và Lee et al. (2021) các tác giả sử dụng mô hình ResNet làm mô hình cơ sở và đề xuất một thuật toán tìm kiếm quyết định liệu các tham số nên được sử dụng lại hay cần tham số mới cho tác vụ mới, và các tác động của các quyết định kiến trúc (ví dụ, chiều rộng của mô hình, tác động của chuẩn hóa, v.v.) lên các chỉ số học liên tục chưa được thảo luận. Chúng tôi tin rằng các công trình học liên tục tương lai tập trung vào tìm kiếm kiến trúc có thể hưởng lợi từ công trình của chúng tôi bằng cách thiết kế không gian tìm kiếm tốt hơn bao gồm các thành phần khác nhau mà chúng tôi đã chỉ ra là quan trọng trong học liên tục.

Ngoài các phương pháp tìm kiếm kiến trúc, có một số công trình tập trung vào khía cạnh phi thuật toán của CL. Liên quan đến kiến trúc, Mirzadeh et al. (2020) nghiên cứu tác động của chiều rộng và chiều sâu lên sự quên lãng và chỉ ra rằng khi chiều rộng tăng, sự quên lãng sẽ giảm. Công trình của chúng tôi mở rộng phân tích của họ cho điểm chuẩn ImageNet-1K quy mô lớn hơn và các kiến trúc khác nhau. Gần đây, Ramasesh et al. (2022) chỉ ra rằng trong các thiết lập pre-training, quy mô của mô hình và tập dữ liệu có thể cải thiện hiệu suất CL. Trong công trình này, thay vì tập trung vào các thiết lập pre-training, chúng tôi nghiên cứu khả năng học tập và giữ gìn của các kiến trúc khác nhau khi các mô hình được huấn luyện từ đầu. Tuy nhiên, chúng tôi chỉ ra rằng trong khi việc mở rộng mô hình có thể hữu ích, tác động của các quyết định kiến trúc quan trọng hơn. Ví dụ, như được chỉ ra trong Bảng 1 và Bảng 2b, các kiến trúc có hiệu suất tốt nhất không phải là những kiến trúc có số lượng tham số cao nhất.

Cuối cùng, trong khi trong Sec. 3 chúng tôi đã thảo luận các công trình liên quan đến từng thành phần kiến trúc cụ thể, chúng tôi tin rằng công trình của chúng tôi vẽ nên một bức tranh toàn diện về các khía cạnh khác nhau của kiến trúc trong học liên tục và đặt ra những hướng thú vị cho các công trình tương lai.

5 Thảo luận
Trong công trình này, chúng tôi đã nghiên cứu vai trò của kiến trúc trong học liên tục. Thông qua các thí nghiệm rộng rãi trên nhiều điểm chuẩn, chúng tôi đã chỉ ra rằng các kiến trúc mạng nơ-ron khác nhau có khả năng học tập và giữ gìn khác nhau, và một thay đổi nhỏ trong kiến trúc có thể dẫn đến một thay đổi đáng kể trong hiệu suất trong thiết lập học liên tục.

Công trình của chúng tôi có thể được mở rộng theo nhiều hướng. Chúng tôi đã chứng minh thực nghiệm rằng các thành phần kiến trúc khác nhau, như chiều rộng, chiều sâu, chuẩn hóa và lớp pooling, có những tác động rất cụ thể trong thiết lập học liên tục. Người ta có thể khám phá lý thuyết các thành phần này vis-à-vis học liên tục. Tương tự, việc thiết kế các thành phần kiến trúc mã hóa các bias quy nạp mà người ta hy vọng có trong một mô hình, như một thước đo về hoặc khả năng chống chịu trước sự dịch chuyển phân phối, một cảm giác liên quan của các tác vụ khác nhau, hoặc thích ứng nhanh với các tác vụ mới, có thể là một hướng tương lai rất thú vị. Chúng tôi hy vọng rằng cộng đồng sẽ tiếp nhận những câu hỏi này và trả lời chúng trong các công trình tương lai.

Lời cảm ơn
Chúng tôi xin cảm ơn Huiyi Hu, Alexandre Galashov, và Yee Whye Teh vì những thảo luận hữu ích và phản hồi về dự án này.

--- TRANG 8 ---
Tài liệu tham khảo
Balaji, Y., Farajtabar, M., Yin, D., Mott, A., và Li, A. (2020). Hiệu quả của memory replay trong học liên tục quy mô lớn. arXiv preprint arXiv:2010.02418.

Bartlett, P., Helmbold, D., và Long, P. (2018a). Gradient descent với khởi tạo đồng nhất học hiệu quả các phép biến đổi tuyến tính xác định dương bằng mạng dư sâu. In International Conference on Machine Learning, pages 521–530. PMLR.

Bartlett, P. L., Evans, S. N., và Long, P. M. (2018b). Biểu diễn các hàm mượt mà như các hợp thành của các hàm gần đồng nhất với những tác động đối với tối ưu hóa mạng sâu. arXiv preprint arXiv:1804.05012.

Bjorck, N., Gomes, C. P., Selman, B., và Weinberger, K. Q. (2018). Hiểu batch normalization. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.

Chaudhry, A., Ranzato, M., Rohrbach, M., và Elhoseiny, M. (2018). Học suốt đời hiệu quả với a-gem. In International Conference on Learning Representations.

Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P. K., Torr, P. H. S., và Ranzato, M. (2019). Về bộ nhớ tập hợp nhỏ trong học liên tục. arXiv preprint arXiv:1902.10486.

Cho, K., Raiko, T., và Ilin, A. (2011). Gradient nâng cao và tỷ lệ học thích ứng để huấn luyện restricted boltzmann machines. In Getoor, L. và Scheffer, T., editors, Proceedings of the 28th International Conference on Machine Learning, ICML, pages 105–112. Omnipress.

Dai, Z., Liu, H., Le, Q. V., và Tan, M. (2021). Coatnet: Kết hợp convolution và attention cho mọi kích thước dữ liệu. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS.

Derakhshani, M. M., Zhen, X., Shao, L., và Snoek, C. (2021). Học liên tục kernel. In Proceedings of the 38th International Conference on Machine Learning, ICML, volume 139 of Proceedings of Machine Learning Research, pages 2621–2631. PMLR.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2021). Một hình ảnh đáng giá 16x16 từ: Transformers cho nhận dạng hình ảnh ở quy mô. In 9th International Conference on Learning Representations, ICLR, 2021.

Farajtabar, M., Azizan, N., Mott, A., và Li, A. (2020). Gradient descent trực giao cho học liên tục. In International Conference on Artificial Intelligence and Statistics, pages 3762–3773. PMLR.

Gao, Q., Luo, Z., và Klabjan, D. (2020). Tìm kiếm kiến trúc hiệu quả cho học liên tục. CoRR, abs/2006.04027.

Goodfellow, I. J., Mirza, M., Da, X., Courville, A. C., và Bengio, Y. (2014). Một cuộc điều tra thực nghiệm về quên lãng thảm khốc trong mạng nơ-ron dựa trên gradient. In 2nd International Conference on Learning Representations, ICLR.

Hardt, M. và Ma, T. (2016). Đồng nhất quan trọng trong học sâu. arXiv preprint arXiv:1611.04231.

He, K., Zhang, X., Ren, S., và Sun, J. (2016). Học dư sâu cho nhận dạng hình ảnh. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778.

Hsu, Y.-C., Liu, Y.-C., Ramasamy, A., và Kira, Z. (2018). Đánh giá lại các kịch bản học liên tục: Một phân loại và trường hợp cho đường cơ sở mạnh. arXiv preprint arXiv:1810.12488.

Huang, S., François-Lavet, V., và Rabusseau, G. (2019). Tìm kiếm kiến trúc mạng nơ-ron cho học tăng dần lớp. CoRR, abs/1909.06686.

Ioffe, S. và Szegedy, C. (2015). Batch normalization: Tăng tốc huấn luyện mạng sâu bằng cách giảm internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 448–456. JMLR.org.

Jastrzebski, S., Arpit, D., Ballas, N., Verma, V., Che, T., và Bengio, Y. (2017). Kết nối dư khuyến khích suy luận lặp. arXiv preprint arXiv:1710.04773.

Kirichenko, P., Farajtabar, M., Rao, D., Lakshminarayanan, B., Levine, N., Li, A., Hu, H., Wilson, A. G., và Pascanu, R. (2021). Học liên tục không phụ thuộc tác vụ với mô hình xác suất lai. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models.

Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., và Hadsell, R. (2017). Vượt qua quên lãng thảm khốc trong mạng nơ-ron. Proceedings of the National Academy of Sciences, 114(13):3521–3526.

Krizhevsky, A., Hinton, G., et al. (2009). Học nhiều lớp đặc trưng từ hình ảnh nhỏ.

Krizhevsky, A., Sutskever, I., và Hinton, G. E. (2017). Phân loại imagenet với mạng nơ-ron tích chập sâu. Commun. ACM, 60(6).

Lee, S., Behpour, S., và Eaton, E. (2021). Chia sẻ ít hơn là nhiều hơn: Học suốt đời trong mạng sâu với chuyển giao lớp có chọn lọc. In Meila, M. và Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 6065–6075. PMLR.

Li, X., Zhou, Y., Wu, T., Socher, R., và Xiong, C. (2019). Học để phát triển: Một khung học cấu trúc liên tục để vượt qua quên lãng thảm khốc. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, volume 97 of Proceedings of Machine Learning Research, pages 3925–3934.

Mallya, A. và Lazebnik, S. (2018). Packnet: Thêm nhiều tác vụ vào một mạng duy nhất bằng pruning lặp. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, pages 7765–7773. Computer Vision Foundation / IEEE Computer Society.

McCloskey, M. và Cohen, N. J. (1989). Giao thoa thảm khốc trong mạng kết nối: Vấn đề học tuần tự. Psychology of Learning and Motivation, 24:109–165.

Mirzadeh, S. I., Chaudhry, A., Hu, H., Pascanu, R., Gorur, D., và Farajtabar, M. (2021a). Mạng nơ-ron rộng quên ít thảm khốc hơn. ArXiv, abs/2110.11526.

Mirzadeh, S. I., Farajtabar, M., Gorur, D., Pascanu, R., và Ghasemzadeh, H. (2021b). Kết nối chế độ tuyến tính trong học đa tác vụ và liên tục. In International Conference on Learning Representations.

Mirzadeh, S. I., Farajtabar, M., Pascanu, R., và Ghasemzadeh, H. (2020). Hiểu vai trò của chế độ huấn luyện trong học liên tục. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., và Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 7308–7320.

Paul, S. và Chen, P.-Y. (2022). Vision transformers là học viên mạnh mẽ. AAAI.

Pham, Q., Liu, C., và HOI, S. (2022). Chuẩn hóa liên tục: Suy nghĩ lại batch normalization cho học liên tục trực tuyến. In International Conference on Learning Representations.

Ramasesh, V. V., Lewkowycz, A., và Dyer, E. (2022). Ảnh hưởng của quy mô lên quên lãng thảm khốc trong mạng nơ-ron. In International Conference on Learning Representations.

Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., và Tesauro, G. (2018). Học để học mà không quên bằng cách tối đa hóa chuyển giao và tối thiểu hóa giao thoa. In International Conference on Learning Representations.

Ring, M. B. (1995). Học liên tục trong môi trường củng cố. PhD thesis, University of Texas at Austin, TX, USA.

Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T. P., và Wayne, G. (2019). Experience replay cho học liên tục. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019.

Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., và Hadsell, R. (2016). Mạng nơ-ron tiến bộ. arXiv preprint arXiv:1606.04671.

Santurkar, S., Tsipras, D., Ilyas, A., và Madry, A. (2018). Batch normalization giúp tối ưu hóa như thế nào? In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.

Shin, H., Lee, J. K., Kim, J., và Kim, J. (2017). Học liên tục với deep generative replay. In Advances in Neural Information Processing Systems, pages 2990–2999.

Springenberg, J. T., Dosovitskiy, A., Brox, T., và Riedmiller, M. A. (2015). Phấn đấu cho sự đơn giản: Mạng hoàn toàn tích chập. CoRR, abs/1412.6806.

Thrun, S. (1995). Một góc nhìn học suốt đời cho điều khiển robot di động. In Intelligent robots and systems, pages 201–214. Elsevier.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., và Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008.

Wortsman, M., Ramanujan, V., Liu, R., Kembhavi, A., Rastegari, M., Yosinski, J., và Farhadi, A. (2020). Supermasks in superposition. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS.

Wu, Y. và He, K. (2018). Group normalization. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, volume 11217 of Lecture Notes in Computer Science, pages 3–19. Springer.

Xu, J. và Zhu, Z. (2018). Học liên tục được tăng cường. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, NeurIPS, pages 907–916.

Yin, D., Farajtabar, M., Li, A., Levine, N., và Mott, A. (2020). Tối ưu hóa và tổng quát hóa học liên tục dựa trên điều hòa: một quan điểm xấp xỉ mất mát. arXiv preprint arXiv:2006.10974.

Yoon, J., Yang, E., Lee, J., và Hwang, S. J. (2018). Học suốt đời với mạng có thể mở rộng động. In Sixth International Conference on Learning Representations. ICLR.

Zagoruyko, S. và Komodakis, N. (2016). Wide residual networks. In Proceedings of the British Machine Vision Conference 2016, BMVC 2016, York, UK, September 19-22, 2016. BMVA Press.

Zenke, F., Poole, B., và Ganguli, S. (2017). Học liên tục thông qua trí thông minh synapse. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3987–3995. JMLR.

Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., và Torralba, A. (2016). Học các đặc trưng sâu cho định vị phân biệt. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2921–2929.

--- TRANG 9 ---
Phụ lục
Tổng quan
Trong tài liệu này, chúng tôi cung cấp chi tiết bổ sung về công trình chính. Cụ thể hơn:
• Đầu tiên, trong Sec. A chúng tôi bao gồm các chi tiết về thiết lập thí nghiệm, lựa chọn thiết kế, chi tiết kiến trúc và siêu tham số.
• Thứ hai, trong Sec. B chúng tôi cung cấp các thí nghiệm và kết quả bổ sung và một phiên bản chi tiết hơn của một số thí nghiệm.

A Chi tiết Thiết lập Thí nghiệm
A.1 Kiến trúc
A.1.1 CNN
Trừ khi có quy định khác, các mô hình CNN trong công trình này chỉ bao gồm các lớp tích chập, theo sau là một lớp feed-forward duy nhất cho phân loại. Tất cả CNN sử dụng hàm kích hoạt ReLU và sử dụng kernel 3x3 với stride là 2.

Đối với các thí nghiệm CIFAR-100, CNN cơ sở chứa 3 lớp tích chập với 32, 64, và 128 kênh tương ứng, và CNN×N đề cập đến mô hình CNN cơ sở nơi số lượng kênh trong mỗi lớp được nhân với N. Do đó, CNN×4 đề cập đến 3 lớp tích chập với 128, 256, và 512 kênh, theo sau là một lớp feed-forward phân loại. Trong các thí nghiệm Split ImageNet-1K, CNN cơ sở chứa 6 lớp tích chập với 64 kênh cho bốn lớp đầu tiên và 128 kênh cho hai lớp cuối cùng. Tương tự như thiết lập cho các thí nghiệm Split CIFAR-100, tất cả các mô hình có một lớp feed-forward đơn lớp cho phân loại và sử dụng kích thước kernel là 3, với stride là 2.

Khi chúng tôi sử dụng các lớp pooling trong CNN (ví dụ, Sec. 3.4), để giữ kích thước đặc trưng giống với các CNN không có lớp pooling, chúng tôi sử dụng stride là 1 cho các lớp tích chập. Nói cách khác, mỗi lớp tích chập dẫn đến bản đồ đặc trưng giảm theo hệ số 2: hoặc convolution có stride 2 (ví dụ, CNN trong Bảng 1) hoặc nó có stride 1, theo sau là lớp pooling (ví dụ, CNN+(Avg/Max)Pool trong Bảng 4 và Bảng 7). Cuối cùng, đối với các thí nghiệm có skip-connections được thêm vào, chúng tôi thêm 2 skip-connections (với projections) cho CNN 3 lớp trên Split CIFAR100 thêm shortcuts từ lớp 1 đến đầu ra của lớp 2 và lớp 2 đến lớp 3. Đối với điểm chuẩn Split ImageNet-1K cho CNN 6 lớp, chúng tôi thêm 3 shortcuts bỏ qua mỗi lớp khác.

A.1.2 ResNets
Các ResNets mà chúng tôi sử dụng trong thí nghiệm ImageNet là các mô hình tiêu chuẩn, và chúng tôi không sửa đổi chúng. Tuy nhiên, các ResNets trong các thí nghiệm CIFAR-100 sử dụng kernel 3×3 với stride là 1, thay vì kernel 7×7 với stride là 2. Đây là một thực hành phổ biến cho các mô hình ResNet đối với hình ảnh CIFAR có kích thước thấp vì nó không giảm kích thước đầu vào đáng kể. Ngoài sửa đổi này cho các thí nghiệm CIFAR-100, phần còn lại của kiến trúc ResNet được giữ nguyên.

A.1.3 WideResNets
Các mô hình WideResNets (WRN) trên cả hai điểm chuẩn CIFAR-100 và ImageNet tuân theo việc triển khai gốc của các mô hình WRN. Đối với tất cả các thí nghiệm, chúng tôi sử dụng hệ số dropout là 0.1 vì chúng tôi quan sát thực nghiệm rằng việc tăng dropout không cải thiện hiệu suất đáng kể.

A.1.4 Vision Transformers
Các mô hình Vision Transformer (ViT) trong thí nghiệm ImageNet của chúng tôi tuân theo cùng kiến trúc như các vision transformers gốc. Tương tự như các mô hình ViT gốc, chúng tôi sử dụng kích thước patch là 16 cho cả hai mô hình ViT trong các thí nghiệm ImageNet-1K của chúng tôi.

Tuy nhiên, vì bài báo vision transformer gốc không cung cấp chi tiết cho các thực hành tốt nhất cho điểm chuẩn CIFAR, chúng tôi đã sử dụng các phiên bản nhỏ hơn của ViT để phù hợp với chi phí huấn luyện của các mô hình khác. Trong những thí nghiệm đó, ViT 512-1024 đề cập đến một mô hình ViT với 4 lớp, với chiều rộng là 512 và kích thước MLP là 1024. Tương tự, ViT 1024-1536 có chiều rộng là 1024 với kích thước MLP ẩn là 1536. Tất cả các mô hình sử dụng kích thước patch là 4 (tức là 64 patch cho hình ảnh CIFAR), nhưng chúng tôi quan sát thực nghiệm rằng việc tăng kích thước patch lên 8 không tác động đáng kể đến kết quả.

A.2 Siêu tham số
Trong phần này, chúng tôi báo cáo các siêu tham số mà chúng tôi sử dụng cho các thí nghiệm. Chúng tôi bao gồm siêu tham số được chọn cho mỗi kiến trúc trong ngoặc đơn.

A.3 Rotated MNIST
Chúng tôi tuân theo thiết lập trong (Mirzadeh et al., 2021a) cho các thí nghiệm MNIST của chúng tôi trong Sec. 3.1.

tỷ lệ học: [0.001, 0.01 (MLP), 0.05, 0.1]
momentum: [0.0 (MLP), 0.8]
weight decay: [0.0 (MLP), 0.0001]
batch size: [16, 32, 64 (MLP)]

A.4 Split CIFAR-100
Chúng tôi sử dụng lưới siêu tham số sau đây cho các thí nghiệm CIFAR-100:

tỷ lệ học: [0.001, 0.005, 0.01 (ViT 1024, ResNet 50/101), 0.05 (CNNs, ResNet 18/34, ViT 512, WRNs), 0.1]
learning rate decay: [1.0 (ResNet 50/101), 0.8 (CNN, ViTs, ResNet 18/34, WRNs)]
momentum: [0.0 (CNN, ViTs, ResNet 50/101), 0.8 (ResNet 18/34, WRNs)]
weight decay: [0.0 (CNNs, ViTs), 0.0001 (ResNets, WRNs)]
batch size: [8 (CNNs, ResNet18/34), 16 (WRNs), 64 (ResNet 50/101, ViT 512), 128 (ViT 1024)]

Chúng tôi lưu ý rằng learning rate decay được áp dụng sau mỗi tác vụ.

A.5 Split ImageNet-1K
Do ngân sách tính toán, chúng tôi sử dụng lưới nhỏ hơn cho các thí nghiệm ImageNet-1K. Tuy nhiên, chúng tôi đảm bảo rằng lưới đủ đa dạng để bao gồm các họ kiến trúc khác nhau.

tỷ lệ học: [0.005, 0.01, 0.05, 0.1 (Tất cả các mô hình)]
learning rate decay: [1.0, 0.75 (Tất cả các mô hình)]
momentum: [0.0, 0.8 (Tất cả các mô hình)]
weight decay: [0.0 (CNNs), 0.0001 (ResNets, WRN, ViTs)]
batch size: [64 (Tất cả các mô hình), 256]

B Kết quả Bổ sung
B.1 Thuật toán vs. Kiến trúc
Trong Hình 1a, chúng tôi đã cung cấp kết quả cho mô hình ResNet-18. Ở đây, chúng tôi cung cấp độ chính xác trung bình và sự quên lãng trung bình cho các mô hình và thuật toán khác nhau trên split CIFAR-100 với 20 tác vụ.

Bảng 8: Các Thuật toán và Kiến trúc Khác nhau
Thuật toán | Tham số (M) | Kiến trúc | Độ chính xác trung bình | Sự quên lãng trung bình
---|---|---|---|---
Finetune | 11.2 | ResNet-18 | 45.0 ± 0.63 | 36.8 ± 1.08
EWC | 11.2 | ResNet-18 | 50.6 ± 0.70 | 26.6 ± 2.53
AGEM (Mem = 1000) | 11.2 | ResNet-18 | 61.8 ± 0.45 | 22.9 ± 1.59
ER (Mem = 1000) | 11.2 | ResNet-18 | 64.3 ± 0.99 | 19.7 ± 1.26
Finetune | 11.9 | ResNet-18 (w/o GAP) | 67.4 ± 0.76 | 11.2 ± 1.98
AGEM (Mem = 1000) | 11.9 | ResNet-18 (w/o GAP) | 72.8 ± 1.33 | 5.8 ± 0.39
ER (Mem = 1000) | 11.9 | ResNet-18 (w/o GAP) | 74.4 ± 0.33 | 4.6 ± 0.54
Finetune | 2.3 | CNN x4 | 68.1 ± 0.50 | 8.7 ± 0.21
ER (Mem = 1000) | 2.3 | CNN x4 | 74.4 ± 0.27 | 2.4 ± 0.12

Chúng tôi nhắc nhở người đọc rằng mục tiêu của công trình chúng tôi không phải là làm giảm tầm quan trọng của các thuật toán học liên tục. Ngược lại, chúng tôi đánh giá cao những cải tiến thuật toán gần đây trong tài liệu học liên tục. Mục tiêu của công trình này là chỉ ra rằng vai trò của kiến trúc là đáng kể, và một kiến trúc tốt có thể bổ sung cho một thuật toán tốt trong học liên tục.

B.2 Thống kê Batchnorm trên Split CIFAR-100
Chúng tôi hiển thị thống kê BN của lớp đầu tiên cho CNN 4 trong Hình 3 sử dụng ước lượng mật độ kernel với kernel Gaussian. Như được minh họa, thống kê batch và các tham số BN đã học không thay đổi đáng kể giữa các tác vụ khác nhau. Ở đây, để đơn giản, chúng tôi chỉ hiển thị bốn tác vụ từ đầu, giữa và cuối trải nghiệm học. Hơn nữa, chúng tôi tập trung vào thống kê lớp đầu tiên, là lớp đầu tiên của mô hình hoạt động trên dữ liệu.

B.3 Thống kê Batchnorm trên Permuted MNIST
Trong Sec. 3.2, chúng tôi đã thảo luận rằng nếu thống kê batch thay đổi đáng kể giữa các tác vụ, batch normalization có thể tăng sự quên lãng. Để làm điều này, chúng tôi thiết kế một thí nghiệm nơi chúng tôi huấn luyện hai mạng MLP-128 (có và không có BN) trên điểm chuẩn Permuted MNIST (Goodfellow et al., 2014) với năm tác vụ. Trong khi Permuted MNIST không phải là một điểm chuẩn rất thực tế, nó phù hợp với yêu cầu của chúng tôi cho sự dịch chuyển phân phối tổng hợp (tức là xáo trộn pixel).

Trong khi Bảng 9 chứng minh lợi ích của việc thêm BN (tức là cải thiện độ chính xác học tập), chúng ta có thể quan sát một sự tăng đáng kể trong sự quên lãng trung bình. Để điều tra thêm, chúng tôi trực quan hóa thống kê BN trong Hình 4 nơi chúng ta có thể thấy so với Hình 3, thống kê thay đổi nhiều hơn, xác nhận giả thuyết của chúng tôi trong Sec. 3.2.

Bảng 9: Permuted MNIST: MLP với BN có độ chính xác học tập cao hơn một chút, nhưng sự quên lãng cũng cao hơn đáng kể.

Mô hình | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập
---|---|---|---
MLP-128 | 86.8 ± 0.95 | 10.9 ± 0.88 | 95.5 ± 0.33
MLP-128 + BN | 73.2 ± 0.82 | 32.5 ± 0.72 | 97.8 ± 0.45

B.4 Số epoch nhiều hơn trên Split CIFAR-100
Trong khi trong văn bản chính, chúng tôi đã sử dụng 10 epoch cho Split CIFAR-100, ở đây trong Bảng 10 chúng tôi cho thấy rằng các kết luận chính vẫn đúng ngay cả khi chúng tôi huấn luyện các mô hình lâu hơn.

Bảng 10: So sánh tác động của các thành phần trong hai cài đặt khác nhau: Các thành phần có lợi trong thời gian huấn luyện ngắn (ví dụ, loại bỏ lớp GAP, thêm lớp pooling hoặc batch norm), cũng có lợi khi thời gian huấn luyện dài hơn.

Epochs = 10 | Epochs = 50
---|---
Mô hình | Tham số (M) | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập | Độ chính xác trung bình | Sự quên lãng trung bình | Độ chính xác học tập
ResNet-18 | 11.2 | 45.0±0.63 | 36.8±1.08 | 74.9±3.98 | 37.1±0.59 | 48.9±1.35 | 82.4±4.83
ResNet-18 w/o GAP | 11.9 | 67.4±0.76 | 11.2±1.98 | 74.2±4.79 | 66.1±0.44 | 17.3±1.43 | 80.2±4.77
ResNet-50 | 23.6 | 56.2±0.88 | 9.5±0.38 | 67.8±5.09 | 53.4±0.29 | 14.5±0.64 | 75.3±5.65
ResNet-50 w/o GAP | 26.7 | 71.4±0.29 | 6.6±0.12 | 73.0±5.18 | 71.2±0.18 | 7.3±0.22 | 76.5±4.87
CNN x4 | 2.3 | 68.1±0.5 | 8.7±0.21 | 76.4±6.92 | 62.6±0.4 | 14.4±0.62 | 75.2±6.25
CNN x4 + BN | 2.3 | 74.0±0.56 | 8.1±0.35 | 81.7±6.68 | 68.9±0.93 | 13.8±0.68 | 80.7±5.83
CNN x4 + Maxpool | 2.3 | 74.4±0.34 | 9.3±0.47 | 83.3±6.1 | 69.3±0.79 | 13.5±0.85 | 81.9±5.47
CNN x8 | 7.5 | 69.9±0.62 | 8.0±0.71 | 77.5±6.78 | 64.3±0.82 | 13.2±1.01 | 78.8±6.61
CNN x8 + BN | 7.5 | 76.1±0.3 | 5.9±0.16 | 81.7±6.83 | 71.7±0.79 | 11.5±0.85 | 82.4±6.18
CNN x8 + Maxpool | 7.5 | 77.2±0.53 | 7.1±0.33 | 84.0±5.81 | 73.6±2.25 | 12.9±1.07 | 84.4±5.06
