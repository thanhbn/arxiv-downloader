# SVD-LLM: PHÂN TÍCH GIÁ TRỊ ĐƠN NHẤT NHẬN THỨC CẮT XÉN CHO NÉN MÔ HÌNH NGÔN NGỮ LỚN

Xin Wang¹ Yu Zheng² Zhongwei Wan¹ Mi Zhang¹
¹Đại học Bang Ohio ²Đại học Bang Michigan
https://github.com/AIoT-MLSys-Lab/SVD-LLM

## TÓM TẮT

Những tiến bộ trong Mô hình Ngôn ngữ Lớn (LLMs) đã bị cản trở bởi kích thước đáng kể của chúng, điều này đòi hỏi các phương pháp nén LLM để triển khai thực tế. Phân tích Giá trị Đơn nhất (SVD) cung cấp một giải pháp đầy hứa hẹn cho việc nén LLM. Tuy nhiên, các phương pháp nén LLM dựa trên SVD tiên tiến nhất có hai hạn chế chính: việc cắt xén các giá trị đơn nhất nhỏ hơn có thể dẫn đến mất mát nén cao hơn, và thiếu cập nhật trên các trọng số đã nén sau khi cắt xén SVD. Trong nghiên cứu này, chúng tôi đề xuất SVD-LLM, một phương pháp nén LLM sau huấn luyện dựa trên SVD nhằm giải quyết các hạn chế của các phương pháp hiện có. SVD-LLM kết hợp kỹ thuật làm trắng dữ liệu nhận thức cắt xén để đảm bảo ánh xạ trực tiếp giữa các giá trị đơn nhất và mất mát nén. Hơn nữa, SVD-LLM áp dụng cập nhật tham số với xấp xỉ hạng thấp tuần tự để bù đắp cho việc suy giảm độ chính xác sau khi nén SVD. Chúng tôi đánh giá SVD-LLM trên 10 bộ dữ liệu và bảy mô hình từ ba họ LLM khác nhau ở ba quy mô khác nhau. Kết quả của chúng tôi chứng minh sự vượt trội của SVD-LLM so với các phương pháp tiên tiến, đặc biệt ở tỷ lệ nén mô hình cao.

## 1 GIỚI THIỆU

Các Mô hình Ngôn ngữ Lớn (LLMs) đã thể hiện khả năng đáng kinh ngạc trong nhiều nhiệm vụ như hiểu và tạo sinh ngôn ngữ tự nhiên (Zhao et al., 2023; Gozalo-Brizuela và Garrido-Merchán, 2023). Mặc dù có những khả năng như vậy, việc dân chủ hóa LLMs chủ yếu bị hạn chế bởi nhu cầu tài nguyên đáng kể của chúng, điều này thúc đẩy việc thiết kế các phương pháp nén LLM (Wan et al., 2024a; Wang et al., 2024; Zhu et al., 2024; Zhou et al., 2024). Các phương pháp này thường được thực hiện theo cách sau huấn luyện mà không cần huấn luyện lại từ đầu. Các phương pháp nén LLM sau huấn luyện dựa trên lượng tử hóa (Yuan et al., 2024; Huang et al., 2024), cắt xén không có cấu trúc (Frantar và Alistarh, 2023), và cắt xén có cấu trúc (Ma et al., 2023; Ashkboos et al., 2024; Zhong et al., 2024) đã được nghiên cứu chuyên sâu. Mặc dù thành công, các phương pháp này có những hạn chế nhất định, như phụ thuộc vào phần cứng cụ thể và tăng tốc suy luận thấp. Ngược lại, các phương pháp nén dựa trên xấp xỉ hạng thấp, như Phân tích Giá trị Đơn nhất (SVD) không bị giới hạn bởi những ràng buộc đó. Hơn nữa, bộ nhớ cache KV của LLMs được nén thông qua SVD trong thời gian chạy cũng có thể được giảm.

Mặc dù có những ưu điểm này, tiềm năng của SVD cho việc nén LLM vẫn chưa được khám phá đầy đủ. Một số phương pháp nén LLM dựa trên SVD, như FWSVD (Hsu et al., 2022) và ASVD (Yuan et al., 2023) đã được đề xuất. Tuy nhiên, các phương pháp này thể hiện sự suy giảm hiệu suất nghiêm trọng khi tỷ lệ nén mô hình¹ tăng. Hạn chế như vậy có thể được quy cho hai vấn đề cơ bản liên quan đến cách tiếp cận của chúng. ❶ Sự không phù hợp giữa cắt xén SVD và mất mát nén: cả FWSVD và ASVD đều không thiết lập được mối quan hệ trực tiếp giữa các giá trị đơn nhất và mất mát nén mô hình. Kết quả là, việc cắt xén các giá trị đơn nhất nhỏ hơn trong SVD có thể dẫn đến mất mát nén cao hơn. ❷ Thiếu cập nhật tham số mô hình sau khi cắt xén SVD: khi tỷ lệ nén mô hình tăng, số lượng các giá trị đơn nhất cần được cắt xén trong SVD cũng tăng theo. Để bù đắp cho việc suy giảm độ chính xác do cắt xén một số lượng lớn hơn các giá trị đơn nhất, việc cập nhật các tham số còn lại của mô hình đã nén trở nên cần thiết. Thật không may, các phương pháp nén LLM dựa trên SVD hiện có không kết hợp việc cập nhật như vậy trong thiết kế của chúng, và do đó không thể bù đắp cho việc suy giảm độ chính xác đặc biệt dưới tỷ lệ nén mô hình cao.

Trong bài báo này, chúng tôi đề xuất một phương pháp nén LLM sau huấn luyện dựa trên SVD, SVD-LLM, giải quyết hiệu quả hai vấn đề cơ bản của các phương pháp nén LLM dựa trên SVD hiện có. SVD-LLM khác với chúng ở hai khía cạnh chính. ❶ Làm trắng Dữ liệu Nhận thức Cắt xén: được hỗ trợ bởi chứng minh lý thuyết, SVD-LLM kết hợp kỹ thuật làm trắng dữ liệu nhận thức cắt xén đảm bảo ánh xạ trực tiếp giữa các giá trị đơn nhất và mất mát nén mô hình. Bằng cách đó, kỹ thuật làm trắng dữ liệu nhận thức cắt xén được đề xuất có thể xác định các giá trị đơn nhất nào nên được cắt xén để gây ra mất mát nén mô hình tối thiểu. ❷ Cập nhật Tham số với Xấp xỉ Hạng thấp Tuần tự: để bù đắp cho việc suy giảm độ chính xác sau khi nén, SVD-LLM tinh chỉnh tuần tự các ma trận hạng thấp đã phân tách để phục hồi độ chính xác toàn cục.

Chúng tôi so sánh SVD-LLM với cả các phương pháp nén LLM dựa trên SVD tiên tiến nhất cũng như các phương pháp nén LLM dựa trên cắt xén và lượng tử hóa. Để chứng minh tính tổng quát của SVD-LLM, chúng tôi tiến hành đánh giá trên tổng cộng 10 bộ dữ liệu và bảy mô hình từ ba họ LLM khác nhau (LLaMA, OPT, và Mistral) ở ba quy mô khác nhau (7B, 13B, 30B), và đánh giá hiệu suất của SVD-LLM trên cả GPU và CPU. Chúng tôi nêu bật ba phát hiện của mình:

• SVD-LLM vượt trội các phương pháp nén LLM dựa trên SVD tiên tiến nhất FWSVD và ASVD trên tất cả 10 bộ dữ liệu, ba họ LLM ở ba quy mô với biên độ lớn.

• SVD-LLM cũng vượt trội các phương pháp nén LLM dựa trên cắt xén có cấu trúc tiên tiến nhất, bao gồm LLM-Pruner, SliceGPT, BlockPruner cũng như các phương pháp nén LLM dựa trên lượng tử hóa sau huấn luyện 1-bit tiên tiến nhất, bao gồm PB-LLM và BiLLM. Quan trọng hơn, khi kết hợp với lượng tử hóa sau huấn luyện 2-bit, SVD-LLM vượt trội phương pháp nén LLM dựa trên lượng tử hóa yêu cầu huấn luyện 1-bit tiên tiến nhất OneBit, trình bày một cách mới để đạt được hiệu suất nén tiên tiến mà không gây ra việc huấn luyện lại đắt đỏ.

• LLMs được nén bởi SVD-LLM có thể đạt được tăng tốc suy luận và giảm bộ nhớ khi triển khai trên phần cứng thực, bao gồm cả GPU và CPU. Đồng thời, SVD-LLM có thể giảm bộ nhớ cache KV trong thời gian chạy mà không có sự sụt giảm độ chính xác bổ sung.

## 2 NGHIÊN CỨU LIÊN QUAN

Nén Mô hình Ngôn ngữ Lớn: LLMs nói chung chứa các tham số quy mô tỷ. Việc áp dụng các phương pháp nén mô hình thông thường cho LLMs là không khả thi vì chúng đòi hỏi việc huấn luyện lại tốn nhiều tài nguyên. Do đó, các phương pháp sau huấn luyện tránh việc huấn luyện lại trong quá trình nén đã được đề xuất. Nói chung, các phương pháp này có thể được nhóm thành bốn loại: cắt xén không có cấu trúc, cắt xén có cấu trúc, lượng tử hóa, và xấp xỉ hạng thấp. Cụ thể, các phương pháp cắt xén không có cấu trúc (Frantar và Alistarh, 2023) đặt các trọng số cá nhân của LLM về zero mà không thay đổi hình dạng của nó. Tuy nhiên, việc thưa thớt không đều của cắt xén không có cấu trúc khó đạt được tăng tốc hoặc tiết kiệm bộ nhớ mong muốn. Khác với cắt xén không có cấu trúc, các phương pháp cắt xén có cấu trúc (Ma et al., 2023; Ashkboos et al., 2024; Zhong et al., 2024) loại bỏ toàn bộ các kênh hoặc các thành phần có cấu trúc khác từ LLMs, làm cho chúng dễ triển khai hơn trên phần cứng. Một đóng góp đáng chú ý là LLM-Pruner (Ma et al., 2023), nhóm các ma trận trọng số dựa trên sự phụ thuộc của chúng và gán tỷ lệ cắt xén cho mỗi nhóm dựa trên tầm quan trọng ước tính. Các phương pháp lượng tử hóa (Lin et al., 2024) nén các mô hình bằng cách giảm độ chính xác của các ma trận trọng số của LLM. Tuy nhiên, tương tự như cắt xén không có cấu trúc, lượng tử hóa cũng khó đạt được tăng tốc suy luận mong muốn do thiếu hỗ trợ phần cứng và các kernel hiệu quả cho tính toán độ chính xác thấp (Lin et al., 2024). Các nghiên cứu gần đây bao gồm PB-LLM (Yuan et al., 2024) và BiLLM (Huang et al., 2024) đẩy ranh giới đến lượng tử hóa 1-bit. Tuy nhiên, các cách tiếp cận này thường dẫn đến suy giảm độ chính xác nghiêm trọng.

SVD cho Nén Mô hình Ngôn ngữ: Phân tích Giá trị Đơn nhất (SVD) là một kỹ thuật xấp xỉ hạng thấp được sử dụng rộng rãi để giảm kích thước ma trận bằng cách xấp xỉ một ma trận với hai ma trận hạng thấp nhỏ hơn (Golub et al., 1987). Do đó, SVD thường được sử dụng cho việc nén mô hình. Ví dụ, DRONE (Chen et al., 2021) đạt được nén SVD tối ưu cho các mô hình ngôn ngữ nhỏ như BERT. Tuy nhiên, trong quá trình nén SVD, DRONE lưu trữ tất cả các kích hoạt đầu vào, làm cho việc nén LLM trở nên khó khăn do sử dụng bộ nhớ quá mức. Đối với LLMs, việc áp dụng trực tiếp SVD trên ma trận trọng số mà không xem xét tầm quan trọng của các trọng số dẫn đến mất mát nén lớn. Để giải quyết vấn đề này, Hsu et al. (2022) đề xuất FWSVD, giới thiệu thông tin Fisher để cân nhắc tầm quan trọng của các tham số. Tuy nhiên, FWSVD yêu cầu tính toán gradient phức tạp đòi hỏi tài nguyên tính toán và bộ nhớ đáng kể cho việc nén LLM. Một vấn đề khác của việc áp dụng trực tiếp SVD là phân phối của kích hoạt có thể ảnh hưởng đến mất mát nén. Để giải quyết điều này, Yuan et al. (2023) đề xuất ASVD, tỷ lệ ma trận trọng số bằng một ma trận đường chéo chuẩn hóa tác động của các kênh đầu vào lên các trọng số. Tuy nhiên, cả FWSVD và ASVD đều không thiết lập mối quan hệ trực tiếp giữa các giá trị đơn nhất và mất mát nén. Kết quả là, việc cắt xén các giá trị đơn nhất nhỏ hơn có thể dẫn đến mất mát nén cao hơn. Hơn nữa, khi tỷ lệ nén tăng, việc cập nhật các trọng số đã nén để phục hồi độ chính xác trở nên cần thiết. Tuy nhiên, cả FWSVD và ASVD đều không xem xét điều này, và do đó gây ra suy giảm độ chính xác nghiêm trọng dưới tỷ lệ nén cao.

## 3 SVD-LLM

Hình 1 cung cấp tổng quan về SVD-LLM. Ở mức độ cao, SVD-LLM là một phương pháp nén LLM sau huấn luyện dựa trên SVD. Cụ thể, theo quy trình tiêu chuẩn của các phương pháp nén LLM sau huấn luyện (Frantar và Alistarh, 2023; Yuan et al., 2023; Xiao et al., 2023), SVD-LLM sử dụng một tập hợp ngẫu nhiên các câu làm dữ liệu hiệu chuẩn để tạo kích hoạt cho làm trắng dữ liệu nhận thức cắt xén. Với kích hoạt được tạo, SVD-LLM suy ra ma trận làm trắng S thông qua phân tách Cholesky, và sau đó thực hiện SVD để cắt xén phép nhân của các ma trận trọng số W và ma trận làm trắng S để nén LLM. Sau khi cắt xén, SVD-LLM cập nhật các tham số mô hình còn lại với xấp xỉ hạng thấp tuần tự để phục hồi độ chính xác. Trong phần tiếp theo, chúng tôi mô tả chi tiết cả làm trắng dữ liệu nhận thức cắt xén và cập nhật tham số với xấp xỉ hạng thấp tuần tự. Mã giả của SVD-LLM được cung cấp trong Phụ lục A.1.

### 3.1 LÀM TRẮNG DỮ LIỆU NHẬN THỨC CẮT XÉN

Động lực: Do phương sai cao của kích hoạt đầu vào, việc đơn giản áp dụng SVD thông thường cho việc nén LLM dẫn đến suy giảm độ chính xác nghiêm trọng (Yuan et al., 2023). Để giải quyết vấn đề này, các phương pháp hiện có (Yuan et al., 2023; Hsu et al., 2022) xây dựng việc nén LLM như một bài toán tối ưu với mục tiêu sau:

O = min(||WX - W'X||_F)                                                   (1)

trong đó W là ma trận trọng số của LLM gốc, X là kích hoạt của W, W' là ma trận trọng số đã nén, và ||WX - W'X||_F là mất mát nén dưới dạng mất mát Frobenius.

Mặc dù các phương pháp hiện có cố gắng giảm mất mát nén này trong quá trình cắt xén SVD của chúng, tất cả đều không thiết lập được mối quan hệ trực tiếp giữa các giá trị đơn nhất và mất mát nén. Kết quả là, việc cắt xén các giá trị đơn nhất nhỏ hơn trong SVD có thể dẫn đến mất mát nén đáng kể. Lấy ASVD (Yuan et al., 2023) làm ví dụ, ASVD trích xuất một ma trận đường chéo S₀ từ X trong đó mỗi phần tử trong đường chéo là giá trị trung bình tuyệt đối của mỗi kênh. Sau đó nó sử dụng S₀ để chuẩn hóa X và chuyển đổi WX thành (WS₀)(S₀⁻¹X). Tiếp theo, SVD được thực hiện trên WS₀ để thu được các ma trận phân tách U₀, Σ₀, và V₀. Cuối cùng, ASVD cắt xén các giá trị đơn nhất nhỏ nhất trong Σ₀ để thu được ma trận trọng số đã nén W'₀ = U₀ × Trunc.(Σ₀) × V₀ × S₀⁻¹.

Mặc dù việc chuẩn hóa kích hoạt cải thiện hiệu suất, ASVD không thiết lập mối quan hệ trực tiếp giữa các giá trị đơn nhất và mất mát nén (một chứng minh chi tiết được bao gồm trong Phụ lục A.2). Để minh họa điểm này tốt hơn, chúng tôi trình bày hai ví dụ cụ thể trong Hình 2(a). Trong ví dụ ❶ chỉ cắt xén một giá trị đơn nhất, việc cắt xén giá trị đơn nhất nhỏ nhất 0.1 dẫn đến mất mát nén cao hơn (mất mát = 1.1) so với việc cắt xén giá trị đơn nhất nhỏ thứ hai 0.9 (mất mát = 0.7). Trong ví dụ ❷ cắt xén nhiều giá trị đơn nhất, việc cắt xén hai giá trị đơn nhất nhỏ nhất 0.9 và 0.1 cũng dẫn đến mất mát cao hơn (mất mát = 1.9) so với việc cắt xén 2.4 và 0.1 (mất mát = 1.7). Do đó, việc cắt xén các giá trị đơn nhất nhỏ nhất không dẫn đến mất mát tối thiểu.

Thiết kế Chính: Ý tưởng chính của SVD-LLM là kết hợp kỹ thuật làm trắng dữ liệu nhận thức cắt xén đảm bảo ánh xạ trực tiếp giữa các giá trị đơn nhất và mất mát nén. Để đạt được điều này, SVD-LLM buộc kích hoạt đã làm trắng S⁻¹X phải trực chuẩn sao cho mỗi kênh độc lập với nhau, tức là (S⁻¹X)(S⁻¹X)ᵀ = S⁻¹XXᵀ(S⁻¹)ᵀ = I, trong đó S được suy ra thông qua phân tách Cholesky (Meyer, 2000). SVD sau đó được thực hiện trên WS để thu được các ma trận phân tách U, Σ, V, trong đó U = [u₁, u₂, u₃, ..., uᵣ], Σ = diag(σ₁, σ₂, σ₃, ···, σᵣ), và V = [v₁, v₂, v₃, ..., vᵣ]. Cuối cùng, các giá trị đơn nhất nhỏ nhất trong Σ được cắt xén (ký hiệu bởi Trunc.(Σ)) để thu được hai ma trận hạng thấp W'ᵤ = U × [Trunc.(Σ)]^(1/2), W'ᵥ = [Trunc.(Σ)]^(1/2) × Vᵀ × S⁻¹ và ma trận trọng số đã nén W' = W'ᵤ × W'ᵥ = U × Trunc.(Σ) × Vᵀ × S⁻¹.

Hình 2(b) minh họa hiệu ứng của phương pháp làm trắng dữ liệu nhận thức cắt xén được đề xuất. Trong ví dụ ❶ chỉ cắt xén một giá trị đơn nhất, mất mát nén bằng với giá trị đơn nhất bị cắt xén. Trong ví dụ ❷, mất mát nén của việc cắt xén nhiều giá trị đơn nhất bằng căn bậc hai của tổng bình phương của chúng. Do đó, dưới kỹ thuật làm trắng dữ liệu nhận thức cắt xén được đề xuất, việc cắt xén các giá trị đơn nhất nhỏ nhất dẫn đến mất mát nén tối thiểu.

Trong phần tiếp theo, chúng tôi cung cấp chứng minh lý thuyết về tại sao kỹ thuật làm trắng dữ liệu nhận thức cắt xén được đề xuất đảm bảo ánh xạ trực tiếp giữa các giá trị đơn nhất và mất mát nén trong trường hợp một giá trị đơn nhất (Định lý 3.2) và nhiều giá trị đơn nhất (Hệ quả 3.3), tương ứng.

Bổ đề 3.1. Chuẩn Frobenius của ma trận A với chiều m×n có thể được suy ra thành căn bậc hai của vết của ma trận gram của nó, đó là:

‖A‖_F ≜ [∑ⱼ₌₁ⁿ ∑ᵢ₌₁ᵐ |aᵢⱼ|²]^(1/2) = [Trace(AᵀA)]^(1/2)     (2)

Cho SVD(WS) ký hiệu nén SVD trên ma trận WS. Ma trận trọng số đã nén W' có thể được biểu diễn như W' = SVD(WS)S⁻¹. Sử dụng Bổ đề 3.1, chúng ta thu được mất mát nén Lᵢ khi cắt xén giá trị đơn nhất thứ i của WS để giảm hạng của nó cho việc nén:

Lᵢ = ‖WX - W'X‖_F = ‖WSS⁻¹X - SVD(WS)S⁻¹X‖_F = ‖(WS - SVD(WS))S⁻¹X‖_F
   = ‖σᵢuᵢvᵢᵀS⁻¹X‖_F = σᵢ[Trace(uᵢvᵢᵀS⁻¹XXᵀ(S⁻¹)ᵀvᵢuᵢᵀ)]^(1/2)     (3)

Vì cả U = [u₁, u₂, u₃, ..., uᵣ] và V = [v₁, v₂, v₃, ..., vᵣ] đều là các ma trận trực chuẩn, chúng ta có:

vᵢᵀvᵢ = uᵢᵀuᵢ = 1; vᵢᵀvⱼ = uᵢᵀuⱼ = 0, ∀i ≠ j; Trace(vᵢvᵢᵀ) = Trace(uᵢuᵢᵀ) = 1     (4)

Định lý 3.2. Nếu S là phân tách Cholesky của XXᵀ, mất mát nén Lᵢ bằng σᵢ.

Chứng minh. Vì ma trận làm trắng S là phân tách Cholesky của XXᵀ, chúng ta có SSᵀ = XXᵀ. Chúng ta có thể suy luận thêm Phương trình (3) để thu được:

Lᵢ = σᵢ[Trace(uᵢvᵢᵀvᵢuᵢᵀ)]^(1/2) = σᵢ[Trace(uᵢ(vᵢᵀvᵢ)uᵢᵀ)]^(1/2) = σᵢ[Trace(uᵢuᵢᵀ)]^(1/2) = σᵢ     (5)

Do đó, Lᵢ của việc cắt xén σᵢ bằng với chính giá trị đơn nhất σᵢ.

Hệ quả 3.3. Nếu S là phân tách Cholesky của XXᵀ, việc cắt xén các giá trị đơn nhất nhỏ nhất dẫn đến mất mát L thấp nhất so với việc cắt xén các giá trị khác.

Chứng minh. Nếu chúng ta cắt xén σₘ₊₁, σₘ₊₂, σₘ₊₃, ..., σᵣ trong Σ để nén, bình phương của mất mát L là:

L² = ‖∑ᵢ₌ₘ₊₁ʳ σᵢuᵢvᵢᵀS⁻¹X‖²_F = ∑ⱼ₌ₘ₊₁ʳ ∑ᵢ₌ₘ₊₁ʳ σᵢσⱼTrace(uᵢvᵢᵀS⁻¹XXᵀ(S⁻¹)ᵀvⱼuⱼᵀ)
   = ∑ᵢ₌ₘ₊₁ʳ σᵢ²Trace(uᵢvᵢᵀS⁻¹XXᵀ(S⁻¹)ᵀvᵢuᵢᵀ) = ∑ᵢ₌ₘ₊₁ʳ (Lᵢ)² = ∑ᵢ₌ₘ₊₁ʳ (σᵢ)²     (6)

Mất mát bình phương L² bằng tổng của các giá trị đơn nhất bình phương (Suy dẫn chi tiết hơn trong Phụ lục A.3). Việc cắt xén các giá trị đơn nhất nhỏ nhất đạt được mất mát nén thấp nhất.

Do kỹ thuật làm trắng dữ liệu nhận thức cắt xén được đề xuất của chúng tôi được xây dựng dựa trên SVD, có tính khả dụng phụ thuộc vào phân phối giá trị đơn nhất nhất định, chúng tôi tiến hành thêm phân tích phổ của các giá trị đơn nhất thu được bằng kỹ thuật của chúng tôi. Chúng tôi tham chiếu độc giả đến Phụ lục A.4 để biết chi tiết.

Cũng cần lưu ý rằng kỹ thuật làm trắng dữ liệu nhận thức cắt xén được đề xuất của chúng tôi không chỉ đảm bảo ánh xạ trực tiếp giữa các giá trị đơn nhất và mất mát nén, mà còn có thể đạt được cùng mất mát nén tối ưu lý thuyết như DRONE (Chen et al., 2021). Tuy nhiên, trong quá trình nén SVD, DRONE lưu trữ tất cả kích hoạt đầu vào trong bộ nhớ, điều này gây ra thách thức cho việc nén LLM do tiêu thụ bộ nhớ cao. Ngược lại, SVD-LLM cập nhật tăng dần ma trận XXᵀ của nó bằng cách thêm xxᵀ của mỗi đầu vào mới x, đáng kể nhỏ hơn so với kích hoạt đầu vào đầy đủ. Trong Phụ lục A.5, chúng tôi cung cấp chứng minh của chúng tôi về SVD-LLM đạt được cùng mất mát nén tối ưu lý thuyết như DRONE và thảo luận về những ưu điểm của SVD-LLM so với DRONE trong tiết kiệm bộ nhớ, tốc độ nén, và tính ổn định số chi tiết.

### 3.2 CẬP NHẬT THAM SỐ VỚI XẤP XỈ HẠNG THẤP TUẦN TỰ

Động lực: Mặc dù kỹ thuật làm trắng dữ liệu nhận thức cắt xén được đề xuất giúp bảo tồn độ chính xác trong quá trình nén, khi tỷ lệ nén tăng, độ chính xác của LLM đã nén có thể suy giảm do nhiều giá trị đơn nhất lớn hơn sẽ bị cắt xén bởi nén SVD. Do đó, việc cập nhật các tham số còn lại trong LLM đã nén là cần thiết.

Thiết kế Chính: SVD-LLM đề xuất một biến thể của tinh chỉnh LoRA để cập nhật các tham số trọng số còn lại của LLM đã nén để phục hồi độ chính xác. Cụ thể, với hai ma trận hạng thấp W'ᵤ, W'ᵥ được tạo bởi làm trắng dữ liệu nhận thức cắt xén, thay vì áp dụng trực tiếp tinh chỉnh LoRA cho ma trận trọng số đã nén W' = W'ᵤ × W'ᵥ như LoRA tiêu chuẩn làm, chúng tôi đề xuất áp dụng LoRA lên W'ᵤ và W'ᵥ riêng biệt để bảo tồn cấu trúc hạng thấp của chúng như sau:

W'ᵤ ← W'ᵤ + BᵤAᵤ, W'ᵥ ← W'ᵥ + BᵥAᵥ     (7)

trong đó Aᵤ, Bᵤ, Aᵥ, và Bᵥ là các ma trận được sử dụng cho tinh chỉnh LoRA.

Việc tinh chỉnh đồng thời W'ᵤ và W'ᵥ sẽ không đảm bảo sự giảm trong mất mát tinh chỉnh. Điều này là do các đạo hàm của W'ᵤ và W'ᵥ phụ thuộc lẫn nhau trong quá trình tinh chỉnh, trong đó tối ưu hóa một ma trận có thể can thiệp vào việc tối ưu hóa ma trận kia. Do đó, như được hiển thị trong Hình 1, chúng tôi đề xuất chiến lược xấp xỉ hạng thấp tuần tự để tinh chỉnh W'ᵤ và W'ᵥ theo cách tuần tự. Cụ thể, chúng tôi đầu tiên đóng băng ma trận W'ᵥ và tinh chỉnh W'ᵤ với LoRA. Sau đó chúng tôi thực hiện tinh chỉnh LoRA vòng thứ hai trên ma trận W'ᵥ trong khi đóng băng ma trận trọng số đã cập nhật W'ᵤ. Cuối cùng, chúng tôi thêm các ma trận Bᵤ×Aᵤ và Bᵥ×Aᵥ vào W'ᵤ và W'ᵥ để thu được các ma trận trọng số đã nén cuối cùng.

## 4 THỰC NGHIỆM VÀ PHÂN TÍCH

Các Phương pháp Cơ sở. Chúng tôi so sánh SVD-LLM với ba nhóm phương pháp: (1) SVD thông thường và các phương pháp nén LLM dựa trên SVD tiên tiến nhất: FWSVD (Hsu et al., 2022), ASVD (Yuan et al., 2023) (Mục 4.1) và FLAP (Phụ lục A.6). (2) Các loại phương pháp nén LLM khác. Bao gồm ba phương pháp nén LLM dựa trên cắt xén tiên tiến nhất: LLM-Pruner (Ma et al., 2023), SliceGPT (Ashkboos et al., 2024), và BlockPruner (Zhong et al., 2024), và ba phương pháp nén LLM dựa trên lượng tử hóa tiên tiến nhất: PB-LLM (Yuan et al., 2024), BiLLM (Huang et al., 2024), và OneBit (Xu et al., 2024) (Mục 4.4). (3) LLM nhỏ hơn StableLM-3B (Tow et al.) được huấn luyện từ đầu (Phụ lục A.7).

Mô hình và Bộ dữ liệu. Để chứng minh tính tổng quát của SVD-LLM, chúng tôi đánh giá hiệu suất của SVD-LLM trên bảy mô hình từ ba họ LLM khác nhau ở ba quy mô khác nhau (LLaMA-7B, 13B, 30B, LLaMA2-7B (Touvron et al., 2023), OPT-6.7B (Zhang et al., 2022), Vicuna-7B (Chiang et al., 2023) và Mistral-7B (Jiang et al., 2023)) và 10 bộ dữ liệu bao gồm hai bộ dữ liệu mô hình hóa ngôn ngữ (WikiText-2 (Merity et al., 2017), và C4 (Raffel et al., 2020)), sáu bộ dữ liệu phân loại (OpenbookQA (Mihaylov et al., 2018), WinoGrande (Sakaguchi et al., 2020), HellaSwag (Zellers et al., 2019), Arc_e (Clark et al., 2018), PIQA (Bisk et al., 2020), MathQA (Amini et al., 2019)), và hai bộ dữ liệu tạo sinh (TruthfulQA (Lin et al., 2022) và GSM8K (Cobbe et al., 2021)) với khung LM-Evaluation-Harness (Gao et al., 2023).

Chi tiết Triển khai. Để đảm bảo so sánh công bằng, chúng tôi theo ASVD (Yuan et al., 2023) để chọn ngẫu nhiên 256 mẫu từ WikiText-2 làm dữ liệu hiệu chuẩn. Chúng tôi theo cùng cấu hình được sử dụng trong LLM-Pruner (Ma et al., 2023) để sử dụng bộ dữ liệu Alpaca (Taori et al., 2023) với 50K mẫu cho cập nhật tham số trong SVD-LLM. Thực nghiệm hiệu quả suy luận được tiến hành trên cả GPU NVIDIA A100 và CPU AMD EPYC 7643 trong khi các thực nghiệm khác được tiến hành trên GPU NVIDIA A100.

### 4.1 SO SÁNH VỚI CÁC PHƯƠNG PHÁP NÉN LLM DỰA TRÊN SVD TIÊN TIẾN NHẤT

Đầu tiên, chúng tôi so sánh hiệu suất của SVD-LLM với các phương pháp nén LLM dựa trên SVD tiên tiến nhất từ bốn khía cạnh: (1) hiệu suất dưới các tỷ lệ nén khác nhau, (2) hiệu suất trên các LLMs khác nhau, (3) hiệu suất trên LLMs với quy mô lớn hơn, và (4) tốc độ nén (Phụ lục A.8). Do tất cả các phương pháp cơ sở dựa trên SVD không kết hợp tinh chỉnh LoRA, để đảm bảo so sánh công bằng, chúng tôi cũng so sánh với SVD-LLM chỉ với làm trắng dữ liệu nhận thức cắt xén (ký hiệu là SVD-LLM(W)).

Hiệu suất dưới Các Tỷ lệ Nén Khác nhau. Chúng tôi đầu tiên đánh giá hiệu suất của LLaMA-7B được nén bởi SVD-LLM, SVD-LLM(W), và các phương pháp cơ sở dựa trên SVD dưới tỷ lệ nén từ 20% đến 80% trên tất cả 10 bộ dữ liệu. Kết quả được tóm tắt trong Bảng 1. Cả SVD-LLM và SVD-LLM(W) đều nhất quán vượt trội SVD thông thường, FWSVD và ASVD trên tất cả các tỷ lệ nén. Đặc biệt, khi tỷ lệ nén là 40% và cao hơn, SVD-LLM giảm độ bối rối hơn 99% trên hai bộ dữ liệu mô hình hóa ngôn ngữ và đạt được hơn 400% độ chính xác trung bình cao hơn trên sáu bộ dữ liệu phân loại. Quan trọng hơn, kết quả trên hai bộ dữ liệu tạo sinh (TruthfulQA, GSM8K) của tất cả ba phương pháp cơ sở khi tỷ lệ nén là 60% và cao hơn đều bằng zero, có nghĩa là các LLMs đã nén hoàn toàn mất khả năng tạo sinh. Ngược lại, SVD-LLM vẫn tạo ra kết quả tốt ngay cả dưới tỷ lệ nén 80%. Ví dụ nội dung được tạo bởi các LLMs đã nén được bao gồm trong Phụ lục A.9.

Hiệu suất trên Các LLMs Khác nhau. Để kiểm tra tính tổng quát của SVD-LLM và SVD-LLM(W) trên các LLMs khác nhau, chúng tôi so sánh hiệu suất giữa SVD-LLM và các phương pháp cơ sở trên bốn mô hình khác nhau từ ba họ LLM khác nhau – OPT-6.7B (họ OPT), LLaMA 2-7B (họ LLaMA), Mistral-7B (họ Mistral), và Vicuna-7B (họ LLaMA) – dưới tỷ lệ nén 20% trên WikiText-2 và sáu bộ dữ liệu phân loại. Như được hiển thị trong Bảng 2, cả SVD-LLM và SVD-LLM(W) đều nhất quán vượt trội các phương pháp cơ sở trên tất cả bốn LLMs, và thể hiện hiệu suất ổn định hơn trên các LLMs khác nhau, đặc biệt so với SVD thông thường và FWSVD.

Hiệu suất trên LLMs với Quy mô Lớn hơn. Để kiểm tra tính tổng quát của SVD-LLM và SVD-LLM(W) trên LLMs với quy mô lớn hơn, chúng tôi so sánh hiệu suất giữa SVD-LLM và các phương pháp cơ sở trên LLaMA-13B và LLaMA-30B dưới tỷ lệ nén 20%. Như được hiển thị trong Bảng 3, cả SVD-LLM và SVD-LLM(W) đều nhất quán vượt trội SVD thông thường, FWSVD, và ASVD trên cả hai kích thước mô hình.

### 4.2 HIỆU QUẢ SUY LUẬN CỦA SVD-LLM

Phân tích Lý thuyết về Hiệu quả Suy luận. Giả sử SVD-LLM nén ma trận trọng số W ∈ R^(d×n) thành hai ma trận hạng thấp W'ᵤ ∈ R^(d×r), W'ᵥ ∈ R^(r×n). Tỷ lệ nén sau đó được tính như Rw = 1 - (d+n)r/(dn).

(1) Phân tích Độ phức tạp Tính toán: Với đầu vào X ∈ R^(n×d), thay vì tính toán lại ma trận trọng số đầy đủ W' = W'ᵤ × W'ᵥ và sau đó tính đầu ra W' × X, SVD-LLM tính trạng thái trung gian M = W'ᵥ × X và sau đó tính đầu ra Y = W'ᵤ × M. Bằng cách này, độ phức tạp tính toán được giảm từ O(d²n) xuống O(d²r + rnd). Lấy tỷ lệ nén Rw = 50% làm ví dụ, vì Rw = 1 - (d+n)r/(dn), chúng ta có r = dn/(2(d+n)). Khi đó độ phức tạp tính toán là O(d²r + rnd) = O(rd(d+n)) = O(d²n/2) = 1/2 O(d²n), giảm 50%. Nói chung, với bất kỳ tỷ lệ nén Rw nào, độ phức tạp tính toán được giảm xuống 1-Rw lần so với ban đầu.

(2) Phân tích Bộ nhớ Suy luận: Vì SVD-LLM không tính toán lại trọng số đầy đủ W' = W'ᵤ × W'ᵥ, bộ nhớ trọng số được giảm xuống 1-Rw lần so với ban đầu trong quá trình suy luận. Như một ưu điểm khác, SVD-LLM có thể giảm bộ nhớ cache KV trong thời gian chạy (Wan et al., 2024b; 2025; Shen et al., 2024). Cụ thể, thay vì giữ W'ᵤ × W'ᵥ × X trong cache KV, SVD-LLM cung cấp tùy chọn lưu trữ kết quả trung gian M = W'ᵥ × X trong cache KV và tính toán lại các trạng thái key và value gốc với ma trận trọng số đã phân tách W'ᵤ nếu cần. Do đó, cache KV trong thời gian chạy được giảm xuống r/d = (1-Rw) × d/(n+d) lần so với ban đầu. Hơn nữa, vì W'ᵤ đã được lưu trữ như ma trận trọng số trong LLM đã phân tách, ma trận trạng thái trung gian gốc vẫn có thể được phục hồi bởi W'ᵤ × M mà không có sụt giảm độ chính xác. Do đó, SVD-LLM cung cấp giải pháp thống nhất cho phép nén mô hình và nén cache KV đồng thời.

Tăng tốc Suy luận trên Phần cứng. Để định lượng tăng tốc suy luận đạt được bởi SVD-LLM trên phần cứng thực, chúng tôi đo số lượng token mà LLaMA-7B và phiên bản đã nén của nó bởi SVD-LLM tạo ra mỗi giây (tức là thông lượng) dưới các kích thước batch và độ dài chuỗi khác nhau trên một GPU NVIDIA A100 và một CPU AMD EPYC 7643, tương ứng. Kết quả được hiển thị trong Hình 3. Chúng tôi có ba quan sát. (1) Dưới kích thước batch hoặc độ dài chuỗi cụ thể, tăng tốc đạt được bởi SVD-LLM liên quan đến mô hình gốc tăng khi tỷ lệ nén tăng. (2) Dưới tỷ lệ nén cụ thể, tăng tốc đạt được bởi SVD-LLM liên quan đến mô hình gốc trở nên đáng kể hơn khi kích thước batch tăng hoặc khi độ dài chuỗi giảm. (3) Hai quan sát trên đều hợp lệ cho cả GPU và CPU.

Giảm Bộ nhớ Suy luận trên Phần cứng. Cuối cùng, chúng tôi đánh giá việc tiết kiệm bộ nhớ suy luận, bao gồm cả tiết kiệm bộ nhớ trọng số và bộ nhớ cache KV trong thời gian chạy trên một GPU A100. Cụ thể, chúng tôi đo dấu chân bộ nhớ đỉnh trong quá trình suy luận khi tạo 128 token với kích thước batch 32 sử dụng LLaMA-7B được nén bởi SVD-LLM dưới các tỷ lệ nén khác nhau có/không xem xét giảm cache KV. Kết quả được minh họa trong Hình 4 trong đó việc giảm bộ nhớ từ đường đứt nét đến các thanh xanh chủ yếu đến từ nén trọng số mô hình và việc giảm bộ nhớ từ các thanh xanh đến các thanh vàng chủ yếu đến từ nén cache KV. Như được hiển thị, cả tiết kiệm bộ nhớ trọng số và tiết kiệm bộ nhớ cache KV trong thời gian chạy do SVD-LLM mang lại đều gần tuyến tính với tỷ lệ nén.

### 4.3 NGHIÊN CỨU LOẠI BỎ

Nghiên cứu Độ nhạy Mô-đun. Chúng tôi tiến hành các nghiên cứu loại bỏ để đánh giá các đóng góp riêng biệt của hai thành phần chính (tức là làm trắng dữ liệu nhận thức cắt xén và cập nhật tham số với xấp xỉ hạng thấp tuần tự) của SVD-LLM. Cho SVD-LLM(W) ký hiệu phiên bản của SVD-LLM chỉ với làm trắng dữ liệu nhận thức cắt xén; SVD-LLM(U) ký hiệu phiên bản của SVD-LLM với cắt xén SVD bình thường và cập nhật tham số với xấp xỉ hạng thấp tuần tự. Như được hiển thị trong Bảng 4, chúng tôi có ba quan sát. (1) SVD-LLM(W), SVD-LLM(U) và SVD-LLM nhất quán vượt trội ASVD trên tất cả các tỷ lệ nén. Đáng chú ý, khi tỷ lệ nén ở và trên 40%, tất cả đều giảm độ bối rối hơn 99% so với ASVD. (2) SVD-LLM nhất quán vượt trội cả SVD-LLM(U) và SVD-LLM(W) trên tất cả các tỷ lệ nén. Kết quả này chứng minh đóng góp độc đáo từ mỗi hai thành phần chính và tầm quan trọng của việc kết hợp cả hai thành phần để đạt được hiệu suất tốt nhất. (3) So sánh giữa SVD-LLM(W) và SVD-LLM(U), SVD-LLM(W) đạt được độ bối rối thấp hơn so với SVD-LLM(U) trên tất cả các tỷ lệ nén. Kết quả này cho thấy làm trắng dữ liệu nhận thức cắt xén đóng vai trò quan trọng hơn so với cập nhật tham số với xấp xỉ hạng thấp tuần tự.

Tác động của Dữ liệu Hiệu chuẩn. Tiếp theo, chúng tôi kiểm tra tác động của dữ liệu hiệu chuẩn lên SVD-LLM. Hình 5 và Bảng 6 tóm tắt hiệu suất của LLaMA-7B đã nén khi thay đổi ba đặc điểm chính của dữ liệu hiệu chuẩn: (1) số lượng dữ liệu hiệu chuẩn, (2) seed được sử dụng để lấy mẫu ngẫu nhiên dữ liệu hiệu chuẩn, và (3) bộ dữ liệu từ đó dữ liệu hiệu chuẩn được lấy mẫu. Như được hiển thị, những thay đổi của ba đặc điểm chính trên dữ liệu hiệu chuẩn gây ra không quá 3% cho hiệu suất cuối cùng, cho thấy độ nhạy của SVD-LLM đối với dữ liệu hiệu chuẩn là hạn chế.

Tác động của Thứ tự Cập nhật. Cuối cùng, chúng tôi kiểm tra tác động của thứ tự cập nhật trong thành phần cập nhật tham số với xấp xỉ hạng thấp tuần tự đến hiệu suất cuối cùng của LLM đã nén. Bảng 5 hiển thị hiệu suất của LLaMA-7B đã nén dưới tỷ lệ nén 20% đến 80% trên WikiText-2 với các thứ tự cập nhật khác nhau. Như được hiển thị, chỉ có sự khác biệt nhỏ về hiệu suất cuối cùng giữa việc cập nhật ma trận W'ᵤ trước và cập nhật ma trận W'ᵥ trước. Kết quả này cho thấy SVD-LLM không nhạy cảm với thứ tự cập nhật.

Các nghiên cứu loại bỏ khác được bao gồm trong Phụ lục A.10.

### 4.4 SO SÁNH VỚI CÁC LOẠI PHƯƠNG PHÁP NÉN LLM KHÁC

SVD-LLM trực giao với các phương pháp nén LLM sau huấn luyện khác bao gồm cắt xén và lượng tử hóa (Wan et al., 2024a; Shen et al., 2025). Cuối cùng, chúng tôi so sánh hiệu suất của SVD-LLM với các phương pháp nén LLM dựa trên cắt xén có cấu trúc và dựa trên lượng tử hóa tiên tiến nhất. Như được thảo luận trong Mục 2, vì các phương pháp cắt xén không có cấu trúc khó đạt được hiệu quả của nó trên phần cứng, chúng tôi không thực hiện so sánh với chúng trong thực nghiệm này.

So sánh với Cắt xén Có cấu trúc. Đầu tiên, chúng tôi so sánh SVD-LLM với ba phương pháp nén LLM dựa trên cắt xén có cấu trúc tiên tiến nhất: LLM-Pruner (Ma et al., 2023), SliceGPT (Ashkboos et al., 2024) và BlockPruner (Zhong et al., 2024) dưới cùng ngân sách bộ nhớ, từ 10 GB đến 7 GB trên LLaMA-7B sử dụng bộ dữ liệu WikiText-2. Như được hiển thị trong Bảng 7, SVD-LLM vượt trội tất cả ba phương pháp nén LLM dựa trên cắt xén có cấu trúc. Đặc biệt, SVD-LLM đạt được giảm độ bối rối lên đến 56% dưới ngân sách bộ nhớ 7G.

So sánh với Lượng tử hóa. Cuối cùng, chúng tôi so sánh SVD-LLM với ba phương pháp nén LLM dựa trên lượng tử hóa tiên tiến nhất: BiLLM (Huang et al., 2024), PB-LLM (Yuan et al., 2024), và OneBit (Xu et al., 2024), đẩy ranh giới đến lượng tử hóa 1-bit. Trong số đó, cả BiLLM và PB-LLM đều là các phương pháp sau huấn luyện, và OneBit yêu cầu huấn luyện. Kết quả trên LLaMA-7B sử dụng bộ dữ liệu WikiText-2 được hiển thị trong Bảng 8. Chúng tôi có ba quan sát. (1) So với các phương pháp sau huấn luyện PB-LLM và BiLLM, SVD-LLM đạt được hiệu suất tốt nhất. (2) Phương pháp yêu cầu huấn luyện OneBit vượt trội SVD-LLM. Kết quả này được mong đợi vì OneBit liên quan đến việc huấn luyện lại tốn nhiều tài nguyên sử dụng bộ dữ liệu quy mô lớn để tăng độ chính xác sau khi nén. (3) Cuối cùng, chúng tôi kết hợp SVD-LLM với phương pháp nén LLM sau huấn luyện dựa trên lượng tử hóa 2-bit QuIP# (Tseng et al., 2024). Điều này được thực hiện bằng cách đầu tiên áp dụng SVD-LLM cho LLM dưới tỷ lệ nén 40%, và sau đó áp dụng QuIP# cho lượng tử hóa 2-bit trên mô hình đã nén (W'ᵤ và W'ᵥ) được tạo từ SVD-LLM. Như được hiển thị trong Bảng 8, SVD-LLM vượt trội phương pháp yêu cầu huấn luyện 1-bit tiên tiến nhất OneBit mà không liên quan đến việc huấn luyện lại tốn nhiều tài nguyên. Kết quả này chứng minh tiềm năng của cách tiếp cận lai – tích hợp các kỹ thuật nén dựa trên SVD và dựa trên lượng tử hóa – để đẩy ranh giới của nén LLM sau huấn luyện.

## 5 KẾT LUẬN

Trong nghiên cứu này, chúng tôi trình bày SVD-LLM, một phương pháp nén LLM sau huấn luyện dựa trên SVD. SVD-LLM đề xuất kỹ thuật làm trắng dữ liệu nhận thức cắt xén để hướng dẫn các giá trị đơn nhất nào nên được cắt xén với mất mát nén tối thiểu. Nó cũng giới thiệu chiến lược xấp xỉ hạng thấp tuần tự để bù đắp cho việc suy giảm độ chính xác do cắt xén giá trị đơn nhất. Chúng tôi đánh giá SVD-LLM trên 10 bộ dữ liệu và bảy mô hình từ ba họ LLM ở ba quy mô. Kết quả của chúng tôi chứng minh sự vượt trội của SVD-LLM so với các phương pháp tiên tiến, đặc biệt ở tỷ lệ nén mô hình cao.

## 6 LỜI CẢM ƠN

Nghiên cứu này được hỗ trợ một phần bởi Giải thưởng NSF NeTS-2312675.

## TÀI LIỆU THAM KHẢO

[Danh sách đầy đủ các tài liệu tham khảo được giữ nguyên như trong bản gốc]

## A PHỤ LỤC

### A.1 MÃ GIẢ CỦA SVD-LLM

Thuật toán 1 hiển thị mã giả của SVD-LLM. Trước khi nén, SVD-LLM thu thập ngẫu nhiên một lượng nhỏ câu làm dữ liệu hiệu chuẩn C, sau đó chạy quá trình làm trắng dữ liệu nhận thức cắt xén như được hiển thị trong Thuật toán 2 để thu được tập hợp ma trận làm trắng SetS cho trọng số cần nén. Sau đó, nó chạy SVD và cắt xén với SetS trên mỗi ma trận trọng số trong LLM. Thay vì hoàn tất trực tiếp toàn bộ quá trình nén, nó lưu trữ các ma trận đã phân tách và tiếp tục sử dụng các ma trận này để chạy cập nhật tham số với xấp xỉ hạng thấp tuần tự như được hiển thị trong Thuật toán 3.

[Các thuật toán và phần còn lại của phụ lục được dịch tương tự...]

### A.2 MẤT MÁT NÉN CỦA ASVD

ASVD giới thiệu ma trận tỷ lệ đường chéo S₀ sửa đổi ma trận trọng số để phản ánh tầm quan trọng khác nhau của các kênh đầu vào khác nhau. Lớp tuyến tính được xây dựng như Y = (WS₀)S₀⁻¹X. Việc nén được thực hiện bằng cách giữ m giá trị đơn nhất lớn nhất của WS₀:

WS₀ ≈ ∑ᵢ₌₁ᵐ σ'ᵢu'ᵢv'ᵢᵀ

Kích hoạt kết quả được biểu diễn như:

Y ≈ ∑ᵢ₌₁ᵐ σ'ᵢu'ᵢv'ᵢᵀS₀⁻¹X

Lỗi nén L = ||(WS₀ - ∑ᵢ₌₁ᵐ σ'ᵢu'ᵢv'ᵢᵀ)S₀⁻¹X||_F được chứng minh dưới đây:

[Công thức toán học được giữ nguyên...]

đây vẫn là một hàm phức tạp liên quan đến kích hoạt X, ma trận đường chéo S₀, vector đơn nhất v'ᵢ và giá trị đơn nhất σ'ᵢ. Kết quả là, lỗi nén không liên quan trực tiếp đến giá trị đơn nhất, và việc nén SVD thông thường bằng cách cắt xén các giá trị đơn nhất nhỏ nhất có thể dẫn đến lỗi nén không tối ưu.

### A.3 MẤT MÁT NÉN CỦA SVD-LLM

Trong SVD-LLM, chúng tôi cũng xây dựng lớp tuyến tính như Y = (WS)S⁻¹X, trong đó S⁻¹XXᵀ(S⁻¹)ᵀ = I. Việc nén được thực hiện bằng cách giữ m giá trị đơn nhất lớn nhất trong tổng số r giá trị đơn nhất của WS. Mất mát nén L được chứng minh như sau:

[Công thức toán học chi tiết được dịch...]

Do đó, mất mát bình phương L² bằng tổng của các giá trị đơn nhất bình phương. Do đó, việc cắt xén các giá trị đơn nhất nhỏ nhất đạt được mất mát nén thấp nhất.

### A.4 PHÂN TÍCH PHỔ CỦA CÁC GIÁ TRỊ ĐƠN NHẤT ĐƯỢC PHÂN TÁCH BỞI SVD-LLM

Nói chung, SVD hữu ích cho việc nén khi ma trận cần được nén cho thấy sự suy giảm mạnh của các giá trị đơn nhất. Vì SVD-LLM phân tách phép nhân của ma trận trọng số W và ma trận làm trắng tương ứng S thay vì ma trận trọng số gốc W, khác với việc phân tách trọng số trong nghiên cứu trước (Yuan et al., 2023; Hsu et al., 2022), để nghiên cứu xem liệu việc nén SVD có áp dụng được trong SVD-LLM hay không, chúng tôi chọn các ma trận trọng số Query (WQ) và Key (WK) và hiển thị phổ của các giá trị đơn nhất của phép nhân của chúng với các ma trận làm trắng tương ứng SQ và SK. Như được hiển thị trong Hình 6, hầu hết các giá trị đơn nhất đều nhỏ hơn hoặc khoảng 100 với chỉ một số ít giá trị cực lớn, cho thấy SVD có thể áp dụng trong SVD-LLM.

### A.5 SO SÁNH VỚI DRONE

[Phần này được dịch chi tiết về so sánh với DRONE, bao gồm các ưu điểm về bộ nhớ, tốc độ và tính ổn định số...]

### A.6 SO SÁNH VỚI FLAP

Nghiên cứu gần đây FLAP (An et al., 2024) cũng là một phương pháp cắt xén có cấu trúc sau huấn luyện. Dưới đây chúng tôi so sánh độ bối rối của SVD-LLM và FLAP trên WikiText-2 dưới các tỷ lệ nén khác nhau khi nén LLaMA-7B. Như được hiển thị trong Bảng 11, SVD-LLM nhất quán vượt trội FLAP, đặc biệt dưới tỷ lệ nén cao.

### A.7 SO SÁNH VỚI LLM NHỎ HƠN ĐƯỢC HUẤN LUYỆN TỪ ĐẦU

Để so sánh hiệu suất giữa SVD-LLM và huấn luyện từ đầu, theo thiết kế thực nghiệm trước (Ma et al., 2023), chúng tôi nén LLaMA-7B xuống kích thước 3B tham số với SVD-LLM và chọn StableLM-3B (Tow et al.) làm phương pháp cơ sở để so sánh. Như được hiển thị trong Bảng 12, LLaMA-3B được nén từ LLaMA-7B bởi SVD-LLM đạt được độ chính xác tốt hơn trong tất cả các bộ dữ liệu, cho thấy SVD-LLM thậm chí có thể đạt được độ chính xác tốt hơn một số phương pháp huấn luyện từ đầu. Hơn nữa, SVD-LLM đảm bảo thông lượng cao hơn và tiêu thụ bộ nhớ thấp hơn so với StableLM-3B như được hiển thị trong bảng, điều này cũng đáp ứng phân tích và thảo luận hiệu quả của chúng tôi trong Mục 4.2.

### A.8 ĐÁNH GIÁ TỐC ĐỘ NÉN

Ngoài hiệu suất nén, chúng tôi cũng đánh giá tốc độ nén của SVD-LLM và các phương pháp cơ sở. Cụ thể, chúng tôi đo số giờ GPU được sử dụng cho SVD-LLM và ASVD khi nén LLaMA-7B dưới tỷ lệ nén 20% trên GPU A100. Kết quả được hiển thị trong Bảng 13. Như được hiển thị, ASVD mất khoảng 5.5 giờ, trong khi SVD-LLM hoàn thành quá trình nén trong 3.5 giờ, nhanh hơn 36%. Khi phân tích thời gian, hầu hết thời gian tiêu thụ bởi ASVD được dành cho việc tìm kiếm tỷ lệ nén cụ thể cho mỗi ma trận trọng số dựa trên điểm quan trọng được tính toán của nó. Ngược lại, SVD-LLM duy trì tỷ lệ nén nhất quán trong tất cả các ma trận trọng số và do đó loại bỏ quá trình tìm kiếm tốn thời gian.

### A.9 NỘI DUNG ĐƯỢC TẠO TỪ LLM ĐƯỢC NÉN BỞI SVD-LLM VÀ ASVD

Một số ví dụ về câu được tạo bởi LLaMA-7B được nén với SVD-LLM và ASVD được hiển thị trong Bảng 14. Các câu được tạo bởi mô hình được nén bởi SVD-LLM thể hiện tính trôi chảy, liên quan và thông tin tốt hơn so với những câu được nén bởi ASVD. Quan trọng hơn, khi tỷ lệ nén tăng lên 40%, phương pháp tiên tiến trước đây ASVD hoàn toàn mất khả năng tạo sinh. Ngược lại, ngay cả khi tỷ lệ nén lên đến 80%, SVD-LLM vẫn có khả năng tạo ra các câu hoàn chỉnh.

### A.10 CÁC NGHIÊN CỨU LOẠI BỎ KHÁC

SVD-LLM + Tinh chỉnh LoRA Bình thường so với SVD-LLM + Tinh chỉnh LoRA Tuần tự. Để minh họa sự vượt trội của cập nhật tham số được thiết kế với xấp xỉ hạng thấp tuần tự trong SVD-LLM, đây là một loại chiến lược tinh chỉnh LoRA tuần tự so với chiến lược tinh chỉnh LoRA bình thường, chúng tôi so sánh hiệu suất nén của SVD-LLM bằng cách áp dụng một trong hai chiến lược tinh chỉnh này. Cho SVD-LLM(SFT) ký hiệu SVD-LLM bằng cách áp dụng tinh chỉnh LoRA tuần tự và SVD-LLM(NFT) ký hiệu SVD-LLM bằng cách áp dụng tinh chỉnh LoRA bình thường. Như được hiển thị trong Bảng 15, SVD-LLM(SFT) nhất quán vượt trội SVD-LLM(NFT), điều này cũng khẳng định lại phân tích của chúng tôi trong Mục 3.2 rằng việc tối ưu hóa cả hai ma trận hạng thấp Wu, Wv cùng một lúc không ổn định và có thể dẫn đến hiệu suất tinh chỉnh kém.

ASVD + Tinh chỉnh LoRA Tuần tự so với SVD-LLM + Tinh chỉnh LoRA Tuần tự. Mặc dù chiến lược tinh chỉnh LoRA tuần tự được thiết kế cũng có thể được áp dụng trong các phương pháp nén LLM dựa trên SVD khác, hiệu suất của các phương pháp khác vẫn kém hơn SVD-LLM ngay cả khi được tích hợp với chiến lược này để nâng cao. Để minh họa điều này, chúng tôi so sánh hiệu suất của phương pháp tiên tiến trước đây ASVD khi được áp dụng với tinh chỉnh LoRA tuần tự với SVD-LLM. Cho SVD-LLM(SFT) ký hiệu SVD-LLM bằng cách áp dụng tinh chỉnh LoRA tuần tự và ASVD(SFT) ký hiệu ASVD bằng cách áp dụng tinh chỉnh LoRA tuần tự. Như được hiển thị trong Bảng 15, SVD-LLM(SFT) nhất quán vượt trội ASVD(SFT) dưới các tỷ lệ nén khác nhau.

### A.11 HẠN CHẾ

Có ba hạn chế của SVD-LLM, được để lại cho nghiên cứu tương lai.

(1) Độ chính xác nén vẫn cần được cải thiện dưới tỷ lệ nén cao. Mặc dù SVD-LLM đã đạt được hiệu suất tiên tiến so với các nghiên cứu trước như FWSVD và ASVD, độ chính xác nén của nó vẫn bị suy giảm, đặc biệt dưới tỷ lệ nén cao. Để nâng cao tính thực tiễn của SVD-LLM trong tình huống thực tế, độ chính xác của nó ít nhất nên có thể so sánh với phương pháp lượng tử hóa, bao gồm cả lượng tử hóa bit thấp và bit cao, thay vì được kết hợp với các phương pháp lượng tử hóa để sử dụng.

(2) Độ trễ của SVD-LLM cần được tối ưu hóa khi được sử dụng để nén cache KV. Như đã thảo luận trong Mục 4.2, SVD-LLM cũng có thể được áp dụng để nén cache KV để tiết kiệm bộ nhớ trong quá trình suy luận. Tuy nhiên, lợi ích này không miễn phí. Trên thực tế, do tính toán bổ sung do phục hồi các trạng thái key và value gốc, tốc độ suy luận sẽ bị ảnh hưởng bởi việc nén cache KV với SVD-LLM, điều này nên được tối ưu hóa trong tương lai.

(3) SVD-LLM nên được hướng dẫn tốt hơn cho việc tạo sinh chất lượng cao. Mặc dù SVD-LLM đạt được độ bối rối thấp, như được chứng minh trong Bảng 1, vẫn có thể tạo ra nội dung chất lượng thấp, như từ lặp lại ngay cả dưới tỷ lệ nén thấp. Hiện tượng này của LLM đã nén cũng đã được đề cập trong nghiên cứu trước (Ma et al., 2023) và nên được loại bỏ trong tương lai bằng cách hướng dẫn LLM đã nén để tạo ra kết quả chất lượng cao.
