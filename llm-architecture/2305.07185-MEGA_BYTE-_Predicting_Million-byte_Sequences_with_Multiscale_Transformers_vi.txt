MEGA BYTE: Dự đoán Chuỗi Triệu Byte với Transformers Đa Quy Mô

Tài liệu tham khảo

Baines, M., Bhosale, S., Caggiano, V., Goyal, N., Goyal, S., Ott, M., Lefaudeux, B., Liptchinsky, V., Rabbat, M., Sheiffer, S., Sridhar, A., và Xu, M. FairScale: Một thư viện PyTorch mô-đun mục đích chung cho huấn luyện hiệu suất cao và quy mô lớn. https://github.com/facebookresearch/fairscale, 2021.

Beltagy, I., Peters, M. E., và Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–1901, 2020.

Child, R., Gray, S., Radford, A., và Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.

Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Clark, J. H., Garrette, D., Turc, I., và Wieting, J. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:73–91, 2022.

Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., và Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context, 2019. URL https://arxiv.org/abs/1901.02860.

Didolkar, A., Gupta, K., Goyal, A., Gundavarapu, N. B., Lamb, A., Ke, N. R., và Bengio, Y. Temporal latent bottleneck: Synthesis of fast and slow processing mechanisms in sequence learning, 2022.

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

Edman, L., Toral, A., và van Noord, G. Subword-delimited downsampling for better character-level translation, 2022.

Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., và Leahy, C. The pile: An 800gb dataset of diverse text for language modeling, 2020.

Gu, A., Goel, K., và Ré, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.

Hawthorne, C., Jaegle, A., Cangea, C., Borgeaud, S., Nash, C., Malinowski, M., Dieleman, S., Vinyals, O., Botvinick, M., Simon, I., et al. General-purpose, long-context autoregressive modeling with perceiver ar. In International Conference on Machine Learning, pp. 8535–8558. PMLR, 2022.

Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., và Mohamed, A. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451–3460, 2021.

Hutchins, D., Schlag, I., Wu, Y., Dyer, E., và Neyshabur, B. Block-recurrent transformers. arXiv preprint arXiv:2203.07852, 2022.

Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., và Carreira, J. Perceiver: General perception with iterative attention. In International conference on machine learning, pp. 4651–4664. PMLR, 2021.

Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., van den Oord, A., Dieleman, S., và Kavukcuoglu, K. Efficient neural audio synthesis. CoRR, abs/1802.08435, 2018. URL http://arxiv.org/abs/1802.08435.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., và Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

Katharopoulos, A., Vyas, A., Pappas, N., và Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020.

Kingma, D. P. và Ba, J. Adam: A method for stochastic optimization. In ICLR, 2015.

--- TRANG 9 ---
MEGA BYTE: Dự đoán Chuỗi Triệu Byte với Transformers Đa Quy Mô

Kitaev, N., Kaiser, Ł., và Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020.

Kudo, T. và Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.

Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., và Zettlemoyer, L. Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022.

Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.

Oord, A. v. d., Kalchbrenner, N., và Kavukcuoglu, K. Pixel Recurrent Neural Networks. ICML, 4:2611–2620, 1 2016. doi: 10.48550/arxiv.1601.06759. URL https://arxiv.org/abs/1601.06759v3.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.

Press, O., Smith, N. A., và Lewis, M. Shortformer: Better language modeling using shorter inputs. arXiv preprint arXiv:2012.15832, 2020.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., và Sutskever, I. Language models are unsupervised multitask learners. 2019.

Rae, J. W., Potapenko, A., Jayakumar, S. M., và Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019a.

Rae, J. W., Potapenko, A., Jayakumar, S. M., và Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019b.

Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., và Sutskever, I. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.

Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., và Dai, B. Combiner: Full attention transformer with sparse computation cost, 2021. URL https://arxiv.org/abs/2107.05768.

Roy, A., Saffar, M., Vaswani, A., và Grangier, D. Efficient content-based sparse attention with routing transformers, 2020. URL https://arxiv.org/abs/2003.05997.

Salimans, T., Karpathy, A., Chen, X., và Kingma, D. P. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. CoRR, abs/1701.05517, 2017. URL http://arxiv.org/abs/1701.05517.

Schlag, I., Irie, K., và Schmidhuber, J. Linear transformers are secretly fast weight programmers. In Meila, M. và Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9355–9366. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/schlag21a.html.

Sennrich, R., Haddow, B., và Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.

Sharami, J., Shterionov, D., và Spronck, P. A systematic analysis of vocabulary and bpe settings for optimal fine-tuning of nmt: A case study of in-domain translation. arXiv preprint arXiv:2303.00722, 2023.

Smith, J. T., Warrington, A., và Linderman, S. W. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022.

Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., và Liu, Y. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

Sun, S., Krishna, K., Mattarella-Micke, A., và Iyyer, M. Do long-range language models actually use long-range context? arXiv preprint arXiv:2109.09115, 2021.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

Trinh, T. H. và Le, Q. V. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018.

van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. W., và Kavukcuoglu, K. Wavenet: A generative model for raw audio. CoRR, abs/1609.03499, 2016. URL http://arxiv.org/abs/1609.03499.

van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., van den Driessche, G., Lockhart, E., Cobo, L. C., Stimberg, F., Casagrande, N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H., Graves, A., King, H., Walters, T., Belov, D., và Hassabis, D. Parallel wavenet: Fast high-fidelity speech synthesis. CoRR, abs/1711.10433, 2017. URL http://arxiv.org/abs/1711.10433.

--- TRANG 10 ---
MEGA BYTE: Dự đoán Chuỗi Triệu Byte với Transformers Đa Quy Mô

Wu, Y., Rabe, M. N., Hutchins, D., và Szegedy, C. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, S., Sridhar, A., Wang, T., Zettlemoyer, L., và Ai, M. OPT: Open Pre-trained Transformer Language Models. 5 2022a. doi: 10.48550/arxiv.2205.01068. URL https://arxiv.org/abs/2205.01068v4.

Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022b.

--- TRANG 11 ---
MEGA BYTE: Dự đoán Chuỗi Triệu Byte với Transformers Đa Quy Mô

A. Phụ lục

A.1. Chi tiết Huấn luyện
Để đảm bảo huấn luyện ổn định, chúng tôi áp dụng gradient clipping với norm tối đa 1.0 và sử dụng optimizer Adam với β₁ = 0.9, β₂ = 0.98 (Kingma & Ba, 2015). Chúng tôi sử dụng bộ lập lịch learning rate polynomial decay tích hợp trong MetaSeq với 500 update warmup và end learning rate được đặt là 0. Tất cả các mô hình được huấn luyện với pre-norm và sử dụng kích hoạt ReLU. Chúng tôi áp dụng dropout 0.1 xuyên suốt, nhưng chúng tôi không áp dụng bất kỳ dropout nào cho embedding. Chúng tôi cũng sử dụng weight decay 0.1. Để khởi tạo trọng số, chúng tôi sử dụng một biến thể dựa trên codebase Megatron-LM, bao gồm việc sử dụng phân phối chuẩn với mean bằng không và độ lệch chuẩn 0.006. Chúng tôi cắt phân phối chuẩn này trong hai độ lệch chuẩn và quan sát được cải thiện đáng kể trong cả tính ổn định huấn luyện và hiệu suất.

A.2. Chi tiết Mô hình
Như đã thảo luận trong Phần 4.1, chúng tôi tiến hành các thí nghiệm sử dụng ngân sách tính toán và dữ liệu cố định trên tất cả các mô hình để tập trung so sánh của chúng tôi hoàn toàn vào kiến trúc mô hình thay vì tài nguyên huấn luyện. Để đạt được điều này, chúng tôi điều chỉnh các siêu tham số mô hình trong mỗi kiến trúc sao cho thời gian cho một update duy nhất được khớp và sau đó huấn luyện tất cả các mô hình cho cùng số update. Chúng tôi liệt kê tất cả chi tiết mô hình trong Bảng 11 và Bảng 12.

[Bảng 11 được giữ nguyên với cấu trúc]

Bảng 11. Chi tiết kiến trúc mô hình thông thường theo kích thước. Đối với mỗi kích thước mô hình, chúng tôi cho thấy số lớp (#L), kích thước embedding (dmodel), số attention head (#H), chiều của mỗi attention head (dhead).

--- TRANG 12 ---
MEGA BYTE: Dự đoán Chuỗi Triệu Byte với Transformers Đa Quy Mô

[Bảng 12 được giữ nguyên với toàn bộ cấu trúc và dữ liệu]

Bảng 12. Chi tiết kiến trúc mô hình. Chúng tôi báo cáo kích thước mô hình, kích thước embedding (D), số lớp (L), tổng kích thước batch (BS), learning rate (LR), và độ dài ngữ cảnh. Khi chúng tôi thay đổi số lớp mô hình từ lượng tiêu chuẩn cho kích thước nhất định (Bảng 11), chúng tôi ghi chú điều này tương ứng. Đối với các mô hình PerceiverAR, chúng tôi ghi chú số latent được sử dụng, và đối với các mô hình MEGABYTE chúng tôi ghi chú kích thước patch.

B. Pseudocode

Danh sách 1. Pseudocode của mô hình Megabyte
```python
class MegaByteDecoder:
    def __init__(
        self,
        global_args,
        local_args,
        patch_size,
    ):
        self.pad = 0
        self.patch_size = patch_size
        self.globalmodel = TransformerDecoder(global_args)
        self.localmodel = TransformerDecoder(local_args)
```

--- TRANG 13 ---
MEGA BYTE: Dự đoán Chuỗi Triệu Byte với Transformers Đa Quy Mô

```python
    def forward(
        self,
        bytes,
    ):
        bytes_global, bytes_local = self.prepare_input(bytes)
        global_bytes_embedded = self.globalmodel.embed(bytes_global)
        global_in = rearrange(
            global_bytes_embedded,
            "b (t p) e -> b t (p e)",
            p=self.patch_size,
        )
        global_output = self.globalmodel(global_in)
        global_output_reshaped = rearrange(
            global_output,
            "b t (p e) -> (b t) p e",
            p=self.patch_size,
        )
        local_bytes_embedded = self.localmodel.embed(bytes_local)
        local_in = local_bytes_embedded + global_output_reshaped
        local_output = self.localmodel(local_in)
        batch_size = bytes_global.shape[0]
        x = rearrange(local_output, "(b t) l v -> b (t l) v", b=batch_size)
        return x

    def prepare_input(self, bytes):
        padding_global = bytes.new(bytes.shape[0], self.patch_size).fill_(self.pad)
        bytes_global = torch.cat((padding_global, bytes[:, : -self.patch_size]), -1)
        bytes_input = rearrange(bytes, "b (t p) -> (b t) p", p=self.patch_size)
        padding_local = bytes_input.new(bytes_input.shape[0], 1).fill_(self.pad)
        bytes_local = torch.cat((padding_local, bytes_input[:, :-1]), -1)
        return bytes_global, bytes_local
```

C. Triển khai PerceiverAR
Để tái tạo PerceiverAR trong thiết lập kiểm soát tính toán, chúng tôi mở rộng triển khai transformer tiêu chuẩn trong metaseq với một lớp cross attention bổ sung để tính toán các latent và khớp với kiến trúc của PerceiverAR. Chúng tôi huấn luyện mô hình bằng cách lấy mẫu các span ngẫu nhiên từ mỗi văn bản, khớp với quy trình được sử dụng trong codebase PerceiverAR. Để nhất quán với công việc gốc, chúng tôi sử dụng đánh giá sliding window với stride numlatents/2 trừ khi có ghi chú khác. Trong một số trường hợp, chúng tôi đã sử dụng triển khai metaseq tiêu chuẩn thay vì các kỹ thuật cụ thể được báo cáo trong bài báo gốc: 1) chúng tôi sử dụng standard attention dropout thay vì cross-attention dropout 2) Chúng tôi không triển khai chunked attention. Chúng tôi xác thực triển khai của mình bằng cách tái tạo các thí nghiệm "Standard Ordering" trong Bảng 5 của bài báo Perceiver AR. Sau khi khớp cẩn thận kích thước ngữ cảnh, số latent, lượng dữ liệu và bước huấn luyện được sử dụng và learning rate, chúng tôi đạt được 3.53 bpb so với 3.54 được báo cáo trong bài báo gốc.

D. Thêm kết quả

D.1. Triển khai Patch scan
Hình ảnh có cấu trúc tự nhiên, chứa một lưới n×n pixel mỗi pixel gồm 3 byte (tương ứng với các kênh màu). Chúng tôi khám phá hai cách chuyển đổi hình ảnh thành chuỗi để mô hình hóa (xem Hình 6). Đầu tiên, raster scan trong đó các pixel được tuyến tính hóa thành 3 byte và nối từng hàng. Thứ hai, patch scan trong đó chúng tôi tạo các patch có hình dạng p×p×3 byte trong đó p = ∛(P/3), và sau đó sử dụng raster scan cả trong và giữa các patch. Trừ khi có quy định khác, các mô hình MEGABYTE sử dụng patch scan cho dữ liệu hình ảnh.

--- TRANG 14 ---
MEGA BYTE: Dự đoán Chuỗi Triệu Byte với Transformers Đa Quy Mô

[Hình 6 được giữ nguyên với chú thích]

Hình 6. Hai cách để mô hình hóa dữ liệu 2D tuần tự. Trái, raster scan, bằng cách lấy byte từng hàng và từ trái sang phải; phải, patch scan, trong đó chúng tôi đầu tiên chia một hình ảnh thành các patch, và thực hiện raster scan trên các patch và trong một patch. (T=36, K=9, P=4).

D.2. Patch scan vs Raster scan
Phương pháp patch scan được lấy cảm hứng từ các công việc gần đây trong Vision Transformers (Dosovitskiy et al., 2020), và nó hiệu quả hơn raster scan để mô hình hóa sequencing hình ảnh. Chúng tôi thấy nó cải thiện cả M EGABYTE và Perceiver AR.

[Bảng 13 được giữ nguyên]

Bảng 13. Hiệu suất ImageNet256 với patch scan vs raster scan cho M EGABYTE và Perceiver AR.

D.3. Mô hình hóa chuỗi dài hơn
Đối với thí nghiệm mở rộng pg19 của chúng tôi, chúng tôi cũng sử dụng độ dài ngữ cảnh dài hơn cho MEGABYTE. Kết quả được hiển thị trong Bảng 14. Với chuỗi dài hơn, chúng tôi không quan sát thêm cải thiện, nhất quán với các phát hiện trong Hawthorne et al. (2022). Chúng tôi nghĩ rằng chúng tôi sẽ được hưởng lợi nhiều hơn từ chuỗi dài hơn khi chúng tôi mở rộng thêm kích thước mô hình và dữ liệu.

[Bảng 14 được giữ nguyên]

Bảng 14. Chuỗi dài hơn cho dataset PG19. Đối với cả hai thí nghiệm, chúng tôi đặt mô hình global là 1.3b, mô hình local là 350m, và kích thước patch MEGABYTE là 8.
