# 2401.14242.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/llm-architecture/2401.14242.pdf
# File size: 999452 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Improving Natural Language Capability of Code Large Language Model
Wei Li1,2∗, Daoguang Zan1,2∗†, Bei Guan2,4†, Ailun Yu3, Xiaolin Chen1,2, Yongji Wang2,4
1Cooperative Innovation Center, Institute of Software, Chinese Academy of Sciences
2University of Chinese Academy of Sciences;3Peking University;
4Integrative Innovation Center, Institute of Software, Chinese Academy of Sciences
{liwei@,daoguang@,guanbei@,chenxiaolin2019@,ywang@itechs.}iscas.ac.cn
Abstract
Code large language models (Code LLMs)
have demonstrated remarkable performance
in code generation. Nonetheless, most exist-
ing works focus on boosting code LLMs from
the perspective of programming capabilities,
while their natural language capabilities re-
ceive less attention. To fill this gap, we thus
propose a novel framework, comprising two
modules: AttentionExtractor, which is respon-
sible for extracting key phrases from the user’s
natural language requirements, and Attention-
Coder, which leverages these extracted phrases
to generate target code to solve the requirement.
This framework pioneers an innovative idea by
seamlessly integrating code LLMs with tradi-
tional natural language processing tools. To
validate the effectiveness of the framework, we
craft a new code generation benchmark, called
MultiNL-H, covering five natural languages.
Extensive experimental results demonstrate the
effectiveness of our proposed framework.
1 Introduction
Code Large Language Models (code LLMs) have
demonstrated remarkable proficiency in code gen-
eration tasks (Zan et al., 2023; Zhang et al., 2023b;
Zheng et al., 2023b), which can transform users’
natural language requirements into target code us-
ing various programming languages. Numerous
methods have emerged to enhance the performance
of code LLMs, achieving significant advancements
by constructing high-quality training data (Luo
et al., 2023; Shen et al., 2023; Name, 2023) or well-
designed prompt (Wei et al., 2023; Shinn et al.,
2023). However, previous works predominantly
put efforts into enhancing code LLMs from the per-
spective of programming capabilities, rarely direct-
ing attention to the natural language understanding
abilities. The precondition for generating correct
code is that the model can accurately comprehend
∗Equal contribution
†Corresponding authors
/uni00000028/uni00000051/uni0000004a/uni0000004f/uni0000004c/uni00000056/uni0000004b /uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048 /uni00000036/uni00000053/uni00000044/uni00000051/uni0000004c/uni00000056/uni0000004b /uni00000029/uni00000055/uni00000048/uni00000051/uni00000046/uni0000004b /uni0000002d/uni00000044/uni00000053/uni00000044/uni00000051/uni00000048/uni00000056/uni00000048/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni00000033/uni00000044/uni00000056/uni00000056/uni00000023/uni00000014/uni0000000b/uni00000008/uni0000000cFigure 1: Pass @1of OpenAI’s GPT-3.5-turbo evaluated
on HumanEval across multiple natural languages.
the programming task, which commonly involves
natural language understanding. The practice of
training code LLMs on a corpus comprising both
code and natural language training instances, which
leads to a powerful model CodeLlama (Rozière
et al., 2023), implies the important role of natu-
ral language understanding abilities in code LLMs.
Therefore, this paper will delve into how to help
code LLMs comprehend natural language context
more accurately, thereby boosting the code genera-
tion capabilities of code LLMs.
After conducting preliminary experiments on
HumanEval (Chen et al., 2021) across multiple nat-
ural languages, we find that code LLMs exhibit
notable variations between different natural lan-
guages in code generation performance. Figure 1
demonstrates a significant drop in code generation
performance for other languages compared to En-
glish. This highlights the need for more exploration
of code generation in various natural languages. To
meet this challenge, we propose a framework to
guide code LLMs to focus on critical information
embedded in the given natural language context,
helping code LLMs to comprehend the task well.
From the perspective of cognitive science, the cog-
nitive load theory (Sweller, 1988, 2011; Kalyuga,
2011) shows that humans inherently prioritize key
information during handling new scenarios. In-
spired by this theory, our framework highlights
important words, phrases, and sentences in the taskarXiv:2401.14242v1  [cs.CL]  25 Jan 2024

--- PAGE 2 ---
description for code LLMs to facilitate the mod-
els in more easily capturing the key information
in given programming requirements. This frame-
work comprises two modules: AttentionExtrac-
tor, which identifies pivotal segments of natural-
language task description as Attention (e.g., words,
phrases, or sentences), and AttentionCoder, which
utilizes the extracted Attention to generate the tar-
get code by harnessing the capabilities of off-the-
shelf pre-trained models.
In order to comprehensively evaluate our ap-
proach, we craft a multi-lingual benchmark,
namely MultiNL-H, adapted from the Hu-
manEval (Chen et al., 2021) which describes code
tasks only in English. MultiNL-H comprises
code generation tasks described with multiple natu-
ral languages including English, Chinese, French,
Japanese, and Spanish. On MultiNL-H, we con-
duct extensive experiments to validate whether our
framework can boost code LLMs in task under-
standing across different natural languages. The
results show that our approach exhibits obvious
advantages over its baselines. For example, by
equipping code LLMs with AttentionExtractor and
AttentionCoder, we boost OpenAI’s GPT-3.5-turbo
in code generation for Chinese tasks by more than
an absolute improvement of 10% on pass @1.
In summary, the contributions of this work are
threefold:
•We propose a simple yet effective framework,
composed of the AttentionExtractor and At-
tentionCoder modules, to boost the code gen-
eration capabilities of code LLMs across mul-
tiple natural languages. This framework can
inspire researchers to solve tasks by integrat-
ing code LLMs with traditional natural lan-
guage processing (NLP) analysis tools.
•We craft a new benchmark, namely MultiNL-
H, which extends the HumanEval (English)
benchmark into four natural languages. It can
evaluate the code generation capabilities of
code LLMs from the perspective of natural
languages.
•We perform extensive experiments to prove
the effectiveness and superiority of our ap-
proach, as well as to derive some valuable in-
sights. Our work including all data and code
can be seen on https://github.com/
NL2Code/AttentionCoder .
Attention A Context x
Target 
Code yAttentionCoder
EnglishChineseSpanish
AttentionExtractorMC
ME
......Figure 2: The overview of our proposed framework.
2 Framework
We would define the code generation task. Given
acontext that contains the programming problem,
this task aims to output the target code to solve
it. This paper leverages code LLM (defined as M)
to address this task. Formally, let xrepresent the
context. Given x, the code generation task can be
formulated as y=M(x), where yrepresents the
target code.
In this paper, we propose a framework to solve
this task, and it includes two modules: Atten-
tionExtractor ( ME) and AttentionCoder ( MC),
as depicted in Figure 2. AttentionExtractor first
identifies and extracts key information A(e.g.,
words, phrases, or sentences) from the program-
ming problem in the given context x. Subsequently,
AttentionCoder focuses on generating the target
code yby leveraging Aand x. Therefore, the
code generation task can be also reformulated as
y=M(x,A). Specifically, the entire process of
our framework can be formalized as A=ME(x)
andy=MC(x,A). Note that Ais called “ Atten-
tion” in our experiments.
3 Methodology
This section will implement our proposed frame-
work to enhance the multiple natural language ca-
pabilities of code LLMs, including AttentionEx-
tractor and AttentionCoder.
3.1 AttentionExtractor
The goal of this module is to extract key informa-
tion (alias “ Attention ”) from natural language de-
scription. In the code generation scenario, as shown
in Figure 3, AttentionExtractor first extracts the
code comment in the given context xusing regular
expression. Subsequently, AttentionExtractor tok-
enizes the extracted code comment and performs
part-of-speech(POS) tagging using spaCy model1.
1https://spacy.io

--- PAGE 3 ---
def count_upper(s):
    """
    给定字符串 s，统计偶数下标中大写元音的个
数。
    """Context x
AttentionExtractor MEAttention AAttentionCoder MC
Natural Language 
DescriptionPrompt
Templatedef count_upper(s):
    count = 0
    for i in range(len(s)):
        if i % 2 == 0 and s[i] in ['A', 'E', 'I', 'O', 'U']:
            count += 1
    return countTarget Code y
Traditional NLP 
Analysis Tools大写元音
字符串偶数下标Code
LLMs
Below is an instruction that describes a task. Write 
a response that appropriately completes the 
request.
### Instruction:
{Context x}
### KeyWords:
Pay attention to these below KeyWords in 
instruction:
{Attention  A}
### Response:
relevance 
Prompt TemplateFigure 3: The implementation details of our framework: AttentionExtractor and AttentionCoder.
Based on these outcomes, AttentionExtractor em-
ploys the key phrase extraction algorithm such as
TextRank (Mihalcea and Tarau, 2004) implemented
by PKE package2to extract multiple phrases3as
Attention and ranks them according to their rele-
vance in the text network. For instance, Attention
extracted from the code comment are three Chi-
nese phases in Figure 3. These extracted Attention
will aid AttentionCoder in generating more accu-
rate code. Besides using the automatic extraction
method above, we also implement AttentionExtrac-
tor by inviting five participants to manually extract
key phrases for each problem.
3.2 AttentionCoder
AttentionExtractor has successfully extracted At-
tention for a programming problem, and then At-
tentionCoder aims to leverage these Attention to
generate target code. We make use of the most
straightforward way for AttentionCoder: consider-
ing both these extracted Attention Aand the context
xwhen prompting models. Formally, the Attention-
Coder can be written as y=MC(T(x,A)), where
T(,)denotes the prompt template that can concate-
nate xandA. The detailed templates can be found
in Figure 3. In our experiments, the off-the-shelf
code generation models, such as OpenAI’s GPT-
4 (OpenAI et al., 2023), GPT-3.5-turbo (Ouyang
et al., 2022), WizardCoder (Luo et al., 2023), and
2https://github.com/boudinfl/pke.git
3The POS patterns designed for PKE package is
“NP: <NOUN|PROPN|ADJ> *<NOUN|PROPN>; VP:
<VERB><NP> “ defined by using regular expression grammar
and POS tag.CodeLlama (Rozière et al., 2023), can be applied
directly to implement our AttentionCoder.
4 Benchmark Construction
To comprehensively evaluate the efficacy of our
framework, we carefully adapt HumanEval (Chen
et al., 2021) from English to four other natu-
ral languages, including Chinese, French, Span-
ish, and Japanese, and craft a new benchmark
called MultiNL-H ( Multi pleNatural Language
HumanEval). Specifically, we invite four native
speakers to translate the code comment from En-
glish into four other languages. After careful check-
ing, we ensure the coherence and fluency of the
translated content, as well as its consistency with
the original content. Furthermore, we also invite
four Python developers who are native speakers
of the corresponding natural language to solve the
programming problems in translated HumanEval.
Upon their feedback, we further polish and refine
this benchmark. Overall, our meticulous transla-
tion process ensures a high-quality benchmark for
multi-natural-language-oriented code generation.
We hope that this benchmark will contribute to the
rapid advancement of the field.
5 Experiments
In this section, we evaluate our framework by an-
swering three research questions (RQs):
•RQ1 (Effectiveness Validation) : How is the
performance of our framework on multiple-
natural-languages-oriented code generation?

--- PAGE 4 ---
•RQ2 (Ablation Study) : What factors influ-
ence the effectiveness of our framework?
•RQ3 (Transferability Evaluation) : Whether
our framework is transferable to other tasks in-
volving natural-language context, by reusing
AttentionExtractor?
We answer RQ1 by applying our framework to
various models and comparing their performance
before and after the application of Attention . To
address RQ2, we conduct extensive ablation stud-
ies to examine the impact of various factors on
our framework’s effectiveness. We answer RQ3
by extending our framework to other downstream
tasks, including multiple-programming-languages-
oriented code generation, code translation, and
mathematical reasoning. Specifically, we reuse
the task-agnostic AttentionExtractor to adapt to
different downstream tasks.
5.1 Experimental Setup
Benchmark We evaluate our framework on our
newly proposed MultiNL-H as described in Sec-
tion 4. For each evaluation instance, MultiNL-H
provides five different language versions of the
code prompt (English, Chinese, French, Spanish,
and Japanese) as well as the corresponding canoni-
cal solution and test cases.
Baselines We choose several existing code LLMs
as our base models, including close-source (Ope-
nAI’s GPT-4-0613 and GPT-3.5-turbo-0613) and
open-source ones (WizardCoder 1B,3B,7B, and
15B; CodeLlama 7B and 13B). We directly prompt
these models with different natural languages pro-
vided by MultiNL-H, obtaining their performance
without Attention as baseline results. Additionally,
we compare other competitive prompt engineering-
based approaches:
•Chain of Thought (CoT) (Wei et al., 2023;
Li et al., 2023a): CoT improves the capability
of code LLMs by prompting them to think
step by step.
•Reflexion (Shinn et al., 2023): Reflexion aug-
ments code LLMs by fostering reflective anal-
ysis in prompts, thereby enhancing task com-
prehension and code generation precision.
Evaluation Metrics Following the approach out-
lined by Codex (Chen et al., 2021), we employ
the pass @1metric to assess the performance of allmodels. Each model is tasked with generating a
single code solution to every programming prob-
lem using greedy decoding. The generated solution
is then tested against given test cases. A program-
ming problem is deemed to be successfully solved
only if the generated code solution passes all the
test cases. Under this, pass @1is defined as|Ps|
|P|,
where |P|signifies the total count of programming
problems, and |Ps|indicates the number of chal-
lenges successfully resolved. Thus, the pass @1
metric also can be served as the accuracy in our
evaluation.
Implementation Details In the implementa-
tion of AttentionExtractor, we treat all extracted
noun and verb phrases as Attention . We im-
plement AttentionCoder using existing models,
employ the greedy decoding strategy, and set
max_new_tokens to1024 . Furthermore, post-
processing is applied to extract relevant code from
chat-style responses to ensure a fair evaluation.
For details on reproducing the CoT and Reflexion,
please see Appendix B.
5.2 Effectiveness Validation
To find out whether our framework can boost
code LLMs on code generation in multiple-natural-
language-oriented code generation scenario, we
compare the performance of the base models on
MultiNL-H in the three following settings: (1)
No Attention : prompting models without the in-
corporation of extracted key phrases ( Attention ),
(2)AttentionCoder auto: prompting models with
key phrases extracted automatically through tradi-
tional NLP analysis tools, (3) AttentionCoder human :
prompting models with key phrases extracted man-
ually by human experts.
Table 1 shows the performance of baseline mod-
els on MultiNL-H under the aforementioned three
settings. By comparing the results under No At-
tention setting and the other two settings, it can
be deduced that prompting Attention on existing
code LLMs is capable of leading to better perfor-
mance. Particularly, with Chinese code comment,
theAttentionCoder autosetting of GPT-3.5 achieves
a large absolute improvement of 7.93% on pass @1,
compared with the No Attention setting. This ob-
servation suggests that highlighting the extracted
Attention in prompt can unlock the potential of code
LLMs to better understand programming problem.
Furthermore, by comparing the results under the
AttentionCoder autoand AttentionCoder human set-

--- PAGE 5 ---
Model Settings English Chinese French Spanish Japanese
WizardCoder 1BNo Attention 23.78 22.56 17.68 14.63 17.07
AttentionCode auto 23.78+0.0021.34-1.2217.68+0.0014.63+0.0016.46-0.61
AttentionCoder human 23.78+0.0022.56+0.0017.68+0.00 15.24+0.6116.46-0.61
WizardCoder 3BNo Attention 32.32 23.17 23.78 21.95 25.61
AttentionCode auto 34.76+2.4426.83+3.6625.00+1.2220.73-1.2225.00-0.61
AttentionCoder human 34.14+1.8226.22+3.0525.00+1.2220.73-1.2225.00-0.61
WizardCoder 7BNo Attention 51.22 41.46 39.02 38.41 35.37
AttentionCode auto 52.44+1.2246.34+4.8840.85+1.8340.85+2.4441.46+6.09
AttentionCoder human 52.44+1.2243.90+2.4440.85+1.8340.24+1.8342.07+6.70
WizardCoder 15BNo Attention 56.10 46.34 43.29 40.85 42.07
AttentionCode auto 59.15+3.0552.44+6.1045.73+2.4442.68+1.8343.29+1.22
AttentionCoder human 59.15+3.0551.83+5.4945.73+2.4443.29+2.4442.68+0.61
CodeLlama 7BNo Attention 32.93 28.05 25.00 23.78 21.95
AttentionCode auto 37.20+4.2732.93+4.8732.93+7.9332.93+9.1528.66+6.71
AttentionCoder human 37.80+4.8733.54+5.4932.93+7.9333.54+9.7626.22+4.27
CodeLlama 13BNo Attention 40.85 39.02 35.37 34.15 32.32
AttentionCode auto 44.51+3.6642.07+3.0537.80+2.4338.41+4.2634.15+1.83
AttentionCoder human 45.12+4.2742.07+3.0537.80+2.4337.80+3.6635.37+3.05
GPT-3.5No Attention 71.34 66.46 62.20 64.02 62.20
CoT Prompting 68.29-3.0560.98-5.4856.71-5.4964.63+0.6155.49-6.71
Reflexion 65.17-6.1760.37-6.0955.44-6.7656.10-7.9259.15-2.05
AttentionCode auto 75.00+3.6674.39+7.9366.46+4.2667.68+3.6662.80+0.60
AttentionCoder human 79.27+7.9376.83+10.3770.12+7.9270.73+6.7169.21+7.01
GPT-4No Attention 81.20 76.82 70.73 70.19 73.17
CoT Prompting 84.15+2.9581.10+4.2875.61+4.8875.00+4.8173.17+0.00
Reflexion 79.53-1.6780.49+3.6775.00+4.2772.56+2.3773.78+0.61
AttentionCode auto 82.32+1.1278.05+1.2376.83+6.1075.00+4.8176.83+3.66
AttentionCoder human 82.32+1.1278.05+1.2377.44+6.7175.00+4.8176.83+3.66
Table 1: Pass @1(%) results on the MultiNL-H: blue background is No Attention setting (baseline).
ting, it can be observed that humans in the loop
can usually further improve the code generation
performance of code LLMs. For example, for GPT-
3.5, the human setting can lead to an absolute im-
provement of 10.37% on pass @1compared to its
baseline under the Chinese scenario. A plausible
explanation for this observation is that humans, un-
like traditional NLP tools, excel in identifying and
extracting pivotal phrases from natural language,
thereby enabling code LLMs to better grasp the
essence of the given natural language input. We
also compare two prompt engineering approaches
including CoT (Wei et al., 2023; Li et al., 2023a)
and Reflexion (Shinn et al., 2023) with our frame-
work on GPT-3.5 and GPT-4. And we observe that
our approach consistently surpasses them acrossall languages. We hypothesize that this is primar-
ily due to the erroneous cascading within CoT and
Reflexion, further leading to a worse performance
than our framework. Interestingly, we observe that
our baselines, CoT and Reflexion, show a declin-
ing trend on GPT-3.5 while exhibiting an upward
trend on the more powerful GPT-4 model. This
suggests that they demand more stringent founda-
tional capabilities from the models. Overall, all
the above results prove the effectiveness of our ap-
proach to promote model understanding of code
requirements.
5.3 Ablation Study
To delve into the effectiveness of our framework,
we are keen on what factors can influence its per-

--- PAGE 6 ---
/uni0000003a/uni00000052/uni00000055/uni00000047 /uni00000033/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048 /uni00000036/uni00000048/uni00000051/uni00000057/uni00000048/uni00000051/uni00000046/uni00000048/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni0000001a/uni00000018/uni00000033/uni00000044/uni00000056/uni00000056/uni00000023/uni00000014/uni0000000b/uni00000008/uni0000000c
/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000002a/uni00000055/uni00000044/uni00000051/uni00000058/uni0000004f/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000005c/uni00000003/uni0000002f/uni00000048/uni00000059/uni00000048/uni0000004f/uni00000056/uni00000037/uni00000048/uni0000005b/uni00000057
/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000037/uni00000052/uni00000053/uni0000004c/uni00000046
/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni00000033/uni00000044/uni00000055
/uni00000057/uni0000004c/uni00000057/uni00000048/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000036/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000048
/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000033/uni00000052/uni00000056/uni0000004c/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000035/uni00000044/uni00000051/uni0000004e/uni0000002f/uni0000002f/uni00000030/uni00000056
/uni00000028/uni0000005b/uni00000057/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000019/uni00000013/uni00000011/uni00000013/uni00000019/uni00000015/uni00000011/uni00000018/uni00000019/uni00000018/uni00000011/uni00000013/uni00000019/uni0000001a/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000015/uni00000011/uni00000018/uni0000001a/uni00000018/uni00000011/uni00000013/uni00000033/uni00000044/uni00000056/uni00000056/uni00000023/uni00000014/uni0000000b/uni00000008/uni0000000c
/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004e/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000024/uni0000004f/uni0000004a/uni00000052/uni00000055/uni0000004c/uni00000057/uni0000004b/uni00000050/uni00000056/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000028/uni00000051/uni0000004a/uni0000004f/uni0000004c/uni00000056/uni0000004b /uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048 /uni00000029/uni00000055/uni00000048/uni00000051/uni00000046/uni0000004b /uni00000036/uni00000053/uni00000044/uni00000051/uni0000004c/uni00000056/uni0000004b /uni0000002d/uni00000044/uni00000053/uni00000044/uni00000051/uni00000048/uni00000056/uni00000048
/uni00000037/uni00000052/uni00000053/uni00000010/uni00000013 /uni00000037/uni00000052/uni00000053/uni00000010/uni00000014 /uni00000037/uni00000052/uni00000053/uni00000010/uni00000016 /uni00000037/uni00000052/uni00000053/uni00000010/uni00000018 /uni00000024/uni0000004f/uni0000004f/uni00000019/uni00000013/uni00000011/uni00000013/uni00000019/uni00000015/uni00000011/uni00000018/uni00000019/uni00000018/uni00000011/uni00000013/uni00000019/uni0000001a/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000015/uni00000011/uni00000018/uni0000001a/uni00000018/uni00000011/uni00000013/uni00000033/uni00000044/uni00000056/uni00000056/uni00000023/uni00000014/uni0000000b/uni00000008/uni0000000c
/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048
/uni0000000b/uni00000046/uni0000000c/uni00000003/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057Figure 4: The result of influence factors on GPT-3.5: Attention Granularity Levels, Ranking Algorithms, Attention
Count; “Top- x” in (c) means providing LLMs with xkey phrases extracted by AttentionExtractor, “Top-0” donates
baseline (No Attention), “All” means providing LLMs all extracted Attention .
formance. We initially explore the impact of differ-
entAttention granularity levels on performance,
including words, phrases, and sentences. Fol-
lowing that, we investigate the impact of various
ranking algorithms on performance, encompass-
ing TextRank (Mihalcea and Tarau, 2004), Top-
icRank (Bougouin et al., 2013), MultiPartiteR-
ank (Boudin, 2018), PositionRank (Florescu and
Caragea, 2017), SingleRank (Wan and Xiao, 2008),
and LLMs extraction. Subsequently, we analyze
the impact of extracted phrase count on perfor-
mance. Finally, we assess the effectiveness of our
framework on models with varying parameter sizes.
Owing to space limitations, more detailed analysis
are provided in Appendix C.
Attention Granularity Levels Technically, atten-
tion can be applied at different levels of granularity,
including words, phrases, or sentences. Therefore,
we compare these levels and the results are pre-
sented in Figure 4 (a). It is observed that the per-
formance of phrase-based attention surpasses that
of word-based and sentence-based attention. A rea-
sonable conjecture is that phrases more effectively
highlight the key information, in contrast to words
and sentences which may miss these focal points
due to being overly brief or excessively lengthy.
Consequently, for our default experiment, we adopt
phrase-based Attention .
Ranking Algorithms Extracting key phrases
from natural language and ranking them in impor-
tance is a very important step in our framework.
Therefore, we explore the effect of different rank-
ing algorithms on MultiNL-H. Specifically, these
include TextRank, TopicRank, MultiPartiteRank,
SingleRank, and PositionRank. The results of these
algorithms are shown in Figure 4 (b). We find thatthese rank algorithms perform almost the same.
This shows that our framework is insensitive to
ranking algorithms. In addition to using traditional
NLP analysis tools, we also use OpenAI’s GPT-3.5
to automatically extract and rank the attention. De-
tails about the prompt can be seen in Appendix A.2.
The results show that LLMs extraction is less effec-
tive, further demonstrating the necessity and good
prospect of combining with traditional NLP tools
for LLMs.
Attention Count We would like to explore how
many phrases should be prompted to LLMs for ap-
propriate Attention , and the results are displayed in
Figure 4 (c). The results show that for almost all
languages, performance initially declines and then
increases as the count of phrases increases, peaking
when all phrases are prompted. This suggests that
prompting with too few phrases might prevent the
LLM from grasping the key information. Under
this context, we are intrigued to determine whether
using the entire programming problem as Attention ,
without extracting phrases, could improve perfor-
mance. Results in Figure 4 (a) show that this setting
(“Sentence”) leads to a decrease in performance,
further underscoring the necessity of using phrases
asAttention .
Model Size We display the performance of Wiz-
ardCoder and CodeLlama of various sizes in Fig-
ure 5. We observe that our framework has a thresh-
old for model size. For instance, WizardCoder 1B
achieves no improvement in pass @1across all lan-
guages, while WizardCoder 7B brought a 3.4% av-
erage improvement. This suggests that by prompt-
ingAttention , the powerful models can obtain ob-
vious improvement, while small models may not.

--- PAGE 7 ---
/uni00000014/uni00000025 /uni00000016/uni00000025 /uni0000001a/uni00000025 /uni00000014/uni00000016/uni00000025 /uni00000014/uni00000018/uni00000025
/uni00000030/uni00000052/uni00000047/uni00000048/uni0000004f/uni00000003/uni00000036/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni00000006/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000004c/uni00000051/uni00000003/uni00000045/uni0000004c/uni0000004f/uni0000004f/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000000c/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000033/uni00000044/uni00000056/uni00000056/uni00000023/uni00000014/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000031/uni00000052/uni00000003/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051
/uni0000000b/uni0000003a/uni0000004c/uni0000005d/uni00000044/uni00000055/uni00000047/uni00000026/uni00000052/uni00000047/uni00000048/uni00000055/uni0000000c
/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000026/uni00000052/uni00000047/uni00000048/uni00000055
/uni0000000b/uni0000003a/uni0000004c/uni0000005d/uni00000044/uni00000055/uni00000047/uni00000026/uni00000052/uni00000047/uni00000048/uni00000055/uni0000000c
/uni00000031/uni00000052/uni00000003/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051
/uni0000000b/uni00000026/uni00000052/uni00000047/uni00000048/uni0000002f/uni0000004f/uni00000044/uni00000050/uni00000044/uni0000000c
/uni00000024/uni00000057/uni00000057/uni00000048/uni00000051/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000026/uni00000052/uni00000047/uni00000048/uni00000055
/uni0000000b/uni00000026/uni00000052/uni00000047/uni00000048/uni0000002f/uni0000004f/uni00000044/uni00000050/uni00000044/uni0000000cFigure 5: The results of model size: ordinate value is
the average of five natural language results on Pass @1.
5.4 Transferability Evaluation
Intuitively, as a means to assist the model in un-
derstanding natural language tasks, our framework
has the potential to excel in various tasks involv-
ing natural-language context. Consequently, we
are curious about whether Attention helps in dif-
ferent tasks across various domains. To declare
our framework’s transferability, we extend it to
three other tasks: (1) code generation of multi-
ple programming languages, evaluated using the
MultiPL-E (Cassano et al., 2022) benchmark. (2)
code translation, evaluated using the HumanEval-
X (Zheng et al., 2023a) benchmark, from which
we specifically select two sub-tasks related to code
translation: py2java and py2c++. (3) mathemati-
cal reasoning, evaluated using the GSM8k (Cobbe
et al., 2021) benchmark. For these tasks, we imple-
ment our framework by reusing the AttentionEx-
tractor and landing AttentionCoder with versatile
OpenAI’s GPT-3.5. As for the three different down-
stream tasks, we utilize different templates to im-
portAttention and prompt the model. More details
about these templates can be seen in Appendix A.3.
The evaluation results on the three tasks are pre-
sented in Table 2. For code generation of multi-
ple programming languages, our framework im-
plemented based on GPT-3.5 obtains different de-
grees of improvement on different programming
languages. For example, it leads to an absolute
improvement of 6.2%on pass @1of the JavaScript
code generation tasks compared to the performance
of GPT-3.5 without our framework. This indicates
the efficiency of our framework in code genera-
tion of multiple programming languages. For code
translation tasks, with our framework, GPT-3.5
achieves a comprehensive enhancement on both
of the tasks, which further shows the effectiveness
of our framework. For instance, our framework pro-
duces an absolute pass @1improvement of 3.05%
for py2java and that of 11.58% for py2c++. It im-Evaluation Setting Baseline AttentionCoder
MultiPL-Ego 27.92 31.17
js 50.93 57.14
java 48.10 49.37
HumanEval-Xpy2java 76.83 79.88
py2c++ 51.83 63.41
GSM8K - 74.60 74.90
Table 2: Pass @1(%) results on GPT-3.5 of different
tasks.
plies that the improvement brought about by our
framework in code translation depends greatly on
the involved programming languages. Actually,
this observation is also consistent with the results
on code generation of multiple programming lan-
guages, where the improvement also varies accord-
ing to the programming language used to gener-
ate codes. For mathematical reasoning, our frame-
work only derives a tiny improvement on GSM8K.
One possible reason is that GSM8K’s task descrip-
tions, characterized by structured and simple sen-
tences, differ significantly in complexity from those
in HumanEval, MultiNL-H, HumanEval-X, and
MultiPL-E, which tend to be more intricate. Thus,
our framework may excel in handling relatively
complex natural language scenarios, with less pro-
nounced advantages in simpler tasks. In summary,
our framework consistently demonstrates advan-
tages across various tasks, showing the potential of
transferability.
Of note, the experiments to evaluate the transfer-
ability of our framework only focus on the English
version. However, we believe the improvement
brought about by our framework could be extended
to other natural languages, especially considering
the conclusions already drawn from RQ1.
5.5 Case Study
After extensive experiments, we can conclude that
introducing Attention can help LLMs to better un-
derstand the programming context. One practical
case is displayed in Figure 6. For the baseline with-
outAttention , the generated code cannot pass all
test cases. The primary reason is that it ignores the
“an array of the words ” specified in the
requirements. But when we add this phase into the
prompt to highlight, code LLMs could pay more
attention to the return type, thereby, generating the
correct code.

--- PAGE 8 ---
def words_string(s):
    """
     你会得到一串用逗号或空格分隔的单词。你的任务是
将字符串拆分为单词并返回单词数组。
(You will be given a string of words separated by commas 
or spaces. Your task is\n    to split the string into words 
and return an array of the words)
    """Baseline (GPT-3.5)
Code:
def words_string(s):
    words = s.split(",") if "," in s else s.split(" ")
    words = [word.strip() for word in words]
    return words
Test Cases:
def check(candidate):
    assert True, "This prints if this assert fails 1 (good for 
debugging!)"
    assert candidate("Hi, my name is John") == ["Hi", 
"my", "name", "is", "John"]
    assert candidate("One, two, three, four, five, six") == 
["One", "two", "three", "four", "five", "six"]
    assert candidate("Hi, my name") == ["Hi", "my", 
"name"]def words_string(s):
    """
    你会得到一串用逗号或空格分隔的单词。你的任务是
将字符串拆分为单词并返回单词数组。
(You will be given a string of words separated by commas 
or spaces. Your task is\n    to split the string into words 
and return an array of the words)
    """AttentionCoder (GPT-3.5)
Code:
def words_string(s):
    words = s.split(",") 
    res = []
    for word in words:
        res.extend(word.split())  
    return res
Test Cases:
def check(candidate):
    assert True, "This prints if this assert fails 1 (good for 
debugging!)"
    assert candidate("Hi, my name is John") == ["Hi", "my", 
"name", "is", "John"]
    assert candidate("One, two, three, four, five, six") == 
["One", "two", "three", "four", "five", "six"]
    assert candidate("Hi, my name") == ["Hi", "my", 
"name"]Attention : 单词数组 ( an array of the words)
result must be 
an array of the 
words!
Failed: result may 
not be an array of 
the words!
Figure 6: One concrete case from the MultiNL-H benchmark.
6 Related Work
6.1 Code Large Language Model
Code large language models (Chen et al., 2023; Zan
et al., 2022b; Liu et al., 2023; Zan et al., 2022a;
Zhang et al., 2023a) have attracted widespread at-
tention in both industry and academia. As a mile-
stone, Codex (Chen et al., 2021) demonstrates great
code generation abilities. After that, a plenty of
code LLMs released, such as PolyCoder (Xu et al.,
2022), CodeParrot (Huggingface, 2021), Alpha-
Code (Li et al., 2022), PaLM-Coder (Christopoulou
et al., 2022), StarCoder (Li et al., 2023b), CodeL-
lama (Rozière et al., 2023), WizardCoder (Luo
et al., 2023). Some efforts (Zheng et al., 2023a;
Cassano et al., 2022) have evaluated code LLMs
from the perspective of multiple programming lan-
guages. However, few works focus on multiple-
natural-language-oriented code generation. In this
paper, we would like to explore the natural lan-
guage capabilities of code LLMs by proposing a
prompt engineering framework to unleash the po-
tential of code LLMs.
6.2 Phrase Extraction
Phrase extraction is a traditional NLP task, which
involves extracting key phrases (Turney, 2000)
from given natural language text. Many approacheshave been proposed, which can be divided into
two types: supervised manners, such as deep
key phrase generation (Meng et al., 2017) and
LTR (Burges et al., 2005), and unsupervised ones,
such as TextRank (Mihalcea and Tarau, 2004),
TopicRank (Bougouin et al., 2013), MultiPartit-
eRank (Boudin, 2018), SingleRank (Wan and Xiao,
2008), and PositionRank (Florescu and Caragea,
2017). In this paper, we regard these unsuper-
vised methods are easy yet effective to extract key
phrases from given natural language code descrip-
tions. The extracted key phrases can be prompted
to code LLMs to enhance natural language compre-
hension.
7 Conclusion
This paper proposes a simple yet effective frame-
work, including AttentionExtractor and Attention-
Coder, for improving the natural language capabili-
ties of code LLMs. The essence of this framework
lies in its seamless integration of LLMs with tra-
ditional NLP tools. To thoroughly evaluate our
framework, we craft a multiple-natural-language-
oriented benchmark, namely MultiNL-H. Finally,
extensive experiments demonstrate the effective-
ness and transferability of our framework. In future
work, we would like to investigate how to imple-
ment our framework during the pre-training and

--- PAGE 9 ---
fine-tuning phase, not just at inference.
Limitations
Our framework also encompasses some limitations:
(1) Technically, our framework can be applied to
any task requiring natural language understand-
ing. However, validating our framework across all
downstream tasks is impractical. Therefore, in Sec-
tion 5.4, we select three popular downstream tasks
for detailed analysis. Similarly, our framework
is designed to process various natural languages.
However, it is impractical to craft a dataset for ev-
ery natural language in the world, considering a
total of upwards to 7,0004. Hence, we choose five
representative languages (English, Chinese, French,
Spanish, and Japanese) for which we create bench-
marks in code generation. (2) Our framework is
based on prompt engineering, and it costs a large
amount of computing resources. Therefore, we will
open source all our code and data to alleviate this
issue.
Acknowledgments
This research was supported by the National Key
Research and Development Program of China, un-
der Grant No. 2022ZD0120201 - “Unified Repre-
sentation and Knowledge Graph Construction for
Science Popularization Resources”.
References
Florian Boudin. 2018. Unsupervised keyphrase ex-
traction with multipartite graphs. arXiv preprint
arXiv:1803.08721 .
Adrien Bougouin, Florian Boudin, and Béatrice Daille.
2013. Topicrank: Graph-based topic ranking for
keyphrase extraction. In International joint con-
ference on natural language processing (IJCNLP) ,
pages 543–551.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning , pages 89–96.
Federico Cassano, John Gouwar, Daniel Nguyen,
Sy Duy Nguyen, Luna Phipps-Costin, Donald Pinck-
ney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane An-
derson, Molly Q. Feldman, Arjun Guha, Michael
Greenberg, and Abhinav Jangda. 2022. A scalable
and extensible approach to benchmarking nl2code for
18 programming languages. ArXiv , abs/2208.08227.
4https://en.wikipedia.org/wiki/Lists_
of_languagesBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,
Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023.
CodeT: Code generation with generated tests. In
The Eleventh International Conference on Learning
Representations .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde, Jared Kaplan, Harrison Ed-
wards, Yura Burda, Nicholas Joseph, Greg Brockman,
Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov, Alethea Power, Lukasz Kaiser, Moham-
mad Bavarian, Clemens Winter, Philippe Tillet, Fe-
lipe Petroski Such, David W. Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-V oss, William H. Guss, Alex Nichol, Igor
Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew M. Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. ArXiv ,
abs/2107.03374.
Fenia Christopoulou, Gerasimos Lampouras, Milan
Gritta, Guchun Zhang, Yinpeng Guo, Zhong-Yi Li,
Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu,
Li yu Yan, Pingyi Zhou, Xin Wang, Yu Ma, Igna-
cio Iacobacci, Yasheng Wang, Guangtai Liang, Jia
Wei, Xin Jiang, Qianxiang Wang, and Qun Liu. 2022.
PanGu-Coder: Program synthesis with function-level
language modeling. ArXiv , abs/2207.11280.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Corina Florescu and Cornelia Caragea. 2017. Posi-
tionrank: An unsupervised approach to keyphrase
extraction from scholarly documents. In Proceed-
ings of the 55th annual meeting of the association for
computational linguistics (volume 1: long papers) ,
pages 1105–1115.
Huggingface. 2021. Training CodeParrot from
Scratch. https://huggingface.co/blog/
codeparrot .
Yongmin Li Jia Li, Ge Li and Zhi Jin. 2023. Structured
chain-of-thought prompting for code generation.
Slava Kalyuga. 2011. Cognitive load theory: How many
types of load does it really need? Educational Psy-
chology Review , 23:1–19.
Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023a. Struc-
tured chain-of-thought prompting for code genera-
tion. arXiv preprint arXiv:2305.06599 .
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc

--- PAGE 10 ---
Marone, Christopher Akiki, Jia Li, Jenny Chim,
Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,
Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko,
Nicolas Gontier, Nicholas Meade, Armel Zebaze,
Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu,
Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo
Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp
Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey,
Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo
Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel
Romero, Tony Lee, Nadav Timor, Jennifer Ding,
Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri
Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
Carolyn Jane Anderson, Brendan Dolan-Gavitt, Dan-
ish Contractor, Siva Reddy, Daniel Fried, Dzmitry
Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
Sean Hughes, Thomas Wolf, Arjun Guha, Leandro
von Werra, and Harm de Vries. 2023b. StarCoder:
may the source be with you!
Yujia Li, David H. Choi, Junyoung Chung, Nate Kush-
man, Julian Schrittwieser, Rémi Leblond, Tom, Ec-
cles, James Keeling, Felix Gimeno, Agustin Dal
Lago, Thomas Hubert, Peter Choy, Cyprien de,
Masson d’Autume, Igor Babuschkin, Xinyun Chen,
Po-Sen Huang, Johannes Welbl, Sven Gowal,
Alexey, Cherepanov, James Molloy, Daniel Jaymin
Mankowitz, Esme Sutherland Robson, Pushmeet
Kohli, Nando de, Freitas, Koray Kavukcuoglu, and
Oriol Vinyals. 2022. Competition-level code genera-
tion with alphacode. Science , 378:1092 – 1097.
Yan Liu, Xiaokang Chen, Yan Gao, Zhe Su, Fengji
Zhang, Daoguang Zan, Jian-Guang Lou, Pin-Yu
Chen, and Tsung-Yi Ho. 2023. Uncovering and
quantifying social biases in code generation. arXiv
preprint arXiv:2305.15377 .
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, and Daxin Jiang. 2023. WizardCoder:
Empowering code large language models with evol-
instruct. arXiv preprint arXiv:2306.08568 .
Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He,
Peter Brusilovsky, and Yu Chi. 2017. Deep keyphrase
generation. arXiv preprint arXiv:1704.06879 .
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into text. In Proceedings of the 2004 con-
ference on empirical methods in natural language
processing , pages 404–411.
Author’s Name. 2023. Code llama beats gpt-4: A deep
dive. Accessed: 2023-10-31.
OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agar-
wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,Jake Berdine, Gabriel Bernadett-Shapiro, Christo-
pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-
laine Boyd, Anna-Luisa Brakman, Greg Brockman,
Tim Brooks, Miles Brundage, Kevin Button, Trevor
Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
Chelsea Carlson, Rory Carmichael, Brooke Chan,
Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,
Ruby Chen, Jason Chen, Mark Chen, Ben Chess,
Chester Cho, Casey Chu, Hyung Won Chung, Dave
Cummings, Jeremiah Currier, Yunxing Dai, Cory
Decareaux, Thomas Degry, Noah Deutsch, Damien
Deville, Arka Dhar, David Dohan, Steve Dowl-
ing, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko
Felix, Simón Posada Fishman, Juston Forte, Is-
abella Fulford, Leo Gao, Elie Georges, Christian
Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,
Rapha Gontijo-Lopes, Jonathan Gordon, Morgan
Grafstein, Scott Gray, Ryan Greene, Joshua Gross,
Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse
Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-
hannes Heidecke, Chris Hesse, Alan Hickey, Wade
Hickey, Peter Hoeschele, Brandon Houghton, Kenny
Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu
Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger
Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie
Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,
Ali Kamali, Ingmar Kanitscheider, Nitish Shirish
Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook
Kim, Christina Kim, Yongjik Kim, Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,

--- PAGE 11 ---
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil
Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-
ston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2023. Gpt-4 technical report.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke E. Miller, Maddie Simens, Amanda Askell, Pe-
ter Welinder, Paul Francis Christiano, Jan Leike, and
Ryan J. Lowe. 2022. Training language models to
follow instructions with human feedback. ArXiv ,
abs/2203.02155.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-
han Xiong, Alexandre Défossez, Jade Copet, Faisal
Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,
Thomas Scialom, and Gabriel Synnaeve. 2023. Code
Llama: Open foundation models for code.
Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan,
Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan
Ji, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang.
2023. PanGu-Coder2: Boosting large language mod-
els for code with ranking feedback.
Noah Shinn, Federico Cassano, Edward Berman, Ash-
win Gopinath, Karthik Narasimhan, and Shunyu Yao.
2023. Reflexion: Language agents with verbal rein-
forcement learning.
John Sweller. 1988. Cognitive load during problem
solving: Effects on learning. Cognitive science ,
12(2):257–285.
John Sweller. 2011. Cognitive load theory. In Psychol-
ogy of learning and motivation , volume 55, pages
37–76. Elsevier.
Peter D Turney. 2000. Learning algorithms for
keyphrase extraction. Information retrieval , 2:303–
336.
Xiaojun Wan and Jianguo Xiao. 2008. Single document
keyphrase extraction using neighborhood knowledge.
InAAAI , volume 8, pages 855–860.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2023. Chain-of-thought prompting elic-
its reasoning in large language models.
Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J.
Hellendoorn. 2022. A systematic evaluation of large
language models of code. Proceedings of the 6th
ACM SIGPLAN International Symposium on Ma-
chine Programming .
Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji
Wang, and Jian-Guang Lou. 2022a. When language
model meets private library. In Conference on Em-
pirical Methods in Natural Language Processing .
Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin,
Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen,
and Jian-Guang Lou. 2022b. CERT: Continual pre-
training on sketches for library-oriented code genera-
tion. In International Joint Conference on Artificial
Intelligence .
Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu,
Bingchao Wu, Bei Guan, Wang Yongji, and Jian-
Guang Lou. 2023. Large language models meet
NL2Code: A survey. In Proceedings of the 61st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 7443–
7464, Toronto, Canada. Association for Computa-
tional Linguistics.
Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang
Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.
2023a. RepoCoder: Repository-level code comple-
tion through iterative retrieval and generation.
Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao,
Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023b.
Unifying the perspectives of nlp and software engi-
neering: A survey on language models for code.
Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan-
shan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen,
Andi Wang, Yang Li, Teng Su, Zhilin Yang, and
Jie Tang. 2023a. CodeGeeX: A pre-trained model
for code generation with multilingual evaluations on
humaneval-x. ArXiv , abs/2303.17568.
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen
Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen.
2023b. A survey of large language models for code:
Evolution, benchmarking, and future trends.
A Prompt Template
A.1 AttentionCoder
The prompt template of AttentionCoder can be di-
vided into two types: one-chat and two-chat style.
One-chat style is aimed to prompt LLMs by di-
rectly concatenating code context xand extracted
Attention Ain one chat, while two-chat style means
initially prompting the model to generate a solution

--- PAGE 12 ---
forxin the first chat round, and then prompting
Ain the subsequent chat round to refine the initial
response. These two types of templates are listed
in Table 3.
A.2 LLMs Extraction
We apply LLMs to extract key phrases of the code
description to compare the extraction ability be-
tween LLMs and traditional NLP analysis tools.
We conduct this experiment on GPT-3.5 with the
prompt shown in Figure 7. We post-process the
generated result and acquire the extracted Atten-
tion.
A.3 Three other tasks of transferability
evaluation
We apply our framework to other tasks includ-
ing multiple-programming-language-oriented code
generation, code translation, and mathematical
reasoning. The prompt template for the task
of multiple-programming-language-oriented code
generation is the same as the two-chat style tem-
plate of the multiple-natural-language-oriented
code generation task, as shown in Table 3. Prompt
Templates of code translation and mathematical rea-
soning are shown in Figure 9 and Figure 10, which
are created by doing some task-related tweaks
based on two-chat style prompt templates of code
generation. It is worth noting that we extracted the
function call as the Attention for the origin code
in the code translation task since it doesn’t have a
natural language description.
B CoT and Reflexion Reproduction
The Chain of Thought (CoT) method was initially
proposed for mathematical reasoning and has sub-
sequently been extended to encompass various
prompt engineering tasks. We reproduce CoT
method exactly based on the implementation pro-
cess of Jia Li and Jin (2023). A concrete case of
generating and leveraging solving process proce-
dures is respectively shown in Figure 11 and 12.
We reproduce the Reflexion, using the open source
project5of Shinn et al. (2023). We follow the
instructions of the project and do not change its
source code. We only set the benchmark path to
MultiNL-H to get the result of the framework run-
ning on multiple natural languages.
5https://github.com/noahshinn/
reflexionC Part of Speech of Attention
The part of speech of Attention can be a noun or
a verb. We analyze the performance of these two
parts of speech and the mixed setting (noun and
verb) in Figure 8. The results show that noun
phrases are better than verb phrases. After care-
ful analysis, we find that the main reason is that
noun phrases encompass more details that models
are prone to err on and need to pay attention to. For
example, given “Given a string s, count the number
of uppercase vowels in even indices.”, Attention-
Coder can extract verb phrases (“given a string s”,
“count the number”) and noun phrases (“uppercase
vowels”, “even indices”). Moreover, from the re-
sults, we can find that the mixed setting is better
than verb or noun since either verb or noun is mean-
ingful for solving different problems. Therefore,
by default, this paper regards the mixed setting as
Attention to prompt code LLMs.

--- PAGE 13 ---
Type Prompt Template Example
One-chat Below is an instruction that describes
a task. Write a response that appropri-
ately completes the request.
### Instruction:
Create a Python script for this problem:
{prompt with Attention putted into code
comment }
### Response:Below is an instruction that describes a task, Write a response that
appropriately completes the request.
### Instruction:
Create a Python script for this problem:
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -
bool:
" Check if in given list of numbers, are any two numbers closer
to each other than
given threshold.
Key Words: any two numbers, given list, given threshold
has_close_elements([1.0, 2.0, 3.0], 0.5) False
has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True
"
### Response:
Two-chat Question:
{origin prompt}
Answer:
{generated code}
Question:
1. Check out if the target code gener-
ated before is correct to match the Code
Description
2. If not, pay attention to these Key
Words in Code Description and rewrite
the code to be correct
### Key Words
{Attention }
### Code Description
{code comment}Question:
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -
bool:
" Check if in given list of numbers, are any two numbers closer
to each other than
given threshold.
Key Words: any two numbers, given list, given threshold
has_close_elements([1.0, 2.0, 3.0], 0.5) False
has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True
"
Answer:
{generated code}
Question:
1. Check out if the target code generated before is correct to match
the Code Description
2. If not, pay attention to these Key Words in Code Description and
rewrite the code to be correct
### Key Words
any two numbers, given list, given threshold
### Code Description
Check if in given list of numbers, are any two numbers closer to
each other than given threshold.
Table 3: Prompt Templates of AttentionCoder.
For the following code problem description, extract the key phrases that need to be 
considered in focus to solve the problem and rank them by relevance of problem:
Check if in given list of numbers, are any two numbers closer to each other than given 
threshold.    
Figure 7: A specific prompt for extracting Attention using LLMs: code comment of context is in gray background

--- PAGE 14 ---
/uni00000039/uni00000048/uni00000055/uni00000045
/uni00000033/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000031/uni00000052/uni00000058/uni00000051
/uni00000033/uni0000004b/uni00000055/uni00000044/uni00000056/uni00000048/uni00000025/uni00000052/uni00000057/uni0000004b
/uni00000037/uni0000005c/uni00000053/uni00000048/uni00000056/uni00000018/uni0000001a/uni00000011/uni00000018/uni00000019/uni00000013/uni00000011/uni00000013/uni00000019/uni00000015/uni00000011/uni00000018/uni00000019/uni00000018/uni00000011/uni00000013/uni00000019/uni0000001a/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000013/uni0000001a/uni00000015/uni00000011/uni00000018/uni0000001a/uni00000018/uni00000011/uni00000013/uni00000033/uni00000044/uni00000056/uni00000056/uni00000023/uni00000014/uni0000000b/uni00000008/uni0000000c
/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048
/uni00000028/uni00000051/uni0000004a/uni0000004f/uni0000004c/uni00000056/uni0000004b/uni00000029/uni00000055/uni00000048/uni00000051/uni00000046/uni0000004b
/uni00000036/uni00000053/uni00000044/uni00000051/uni0000004c/uni00000056/uni0000004b/uni0000002d/uni00000044/uni00000053/uni00000044/uni00000051/uni00000048/uni00000056/uni00000048Figure 8: The result of Part of Speech of Attention : verb
phrases, noun phrases, and both types.

--- PAGE 15 ---
1. Check out if the target code generated before is correct when transform Origin Code 
into another programming language of Target Function
2. If not, pay attention to these Key Parts in Origin Code and complete the Target 
Function of corresponding programming language correctly
### Key Parts
has_close_elements, enumerate, abs
### Origin Code
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    for idx, elem in enumerate(numbers):
        for idx2, elem2 in enumerate(numbers):
            if idx != idx2:
                distance = abs(elem - elem2)
                if distance < threshold:
                    return True
    return False
### Target Function
#include<stdio.h>
#include<vector>
#include<math.h>
using namespace std;
#include<algorithm>
#include<stdlib.h>
bool has_close_elements(vector<float> numbers, float threshold){Figure 9: One example of code translation on python2c++.
1. Check out if the answer is right for the Math Problem before
2. If not, pay attention to these KeyWords in Math Problem and rewrite the right answer
### KeyWords
any two numbers, given list, given threshold
### Math Problem
Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes 
muffins for her friends every day with four. She sells the remainder at the farmers' 
market daily for $2 per fresh duck egg. How much in dollars does she make every day at 
the farmers' market?
Figure 10: One concrete prompt of mathematical reasoning.

--- PAGE 16 ---
Question:
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer to each other than
    given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
Please understand the requirement and write a rough solving process. The necessary 
details should be written in natural languages.
Answer:
1. Define a function named `has_close_elements` that takes two parameters: `numbers` 
(a list of floats) and `threshold` (a float).
2. Initialize a variable `n` to store the length of the `numbers` list.
3. Iterate through the range of `n` using a for loop with the variable `i`.
4. Inside the loop, iterate through the range of `n` again using a nested for loop with the 
variable `j`.
5. Inside the nested loop, check if `i` is not equal to `j` to avoid comparing the same 
number with itself.
6. Calculate the absolute difference between `numbers[i]` and `numbers[j]` using the 
`abs()` function and store it in a variable `diff`.
7. Check if `diff` is less than the given `threshold`.
8. If the condition is satisfied, return True.
9. After the loops, return False to indicate that no pair of numbers satisfy the condition.Figure 11: A case of generating a rough solving process in natural language: the solving process description is in
gray background.
from typing import List
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer to each other than
    given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    1. Define a function named `has_close_elements` that takes two parameters: `numbers` (a list 
of floats) and `threshold` (a float).
    2. Initialize a variable `n` to store the length of the `numbers` list.
    3. Iterate through the range of `n` using a for loop with the variable `i`.
    4. Inside the loop, iterate through the range of `n` again using a nested for loop with the 
variable `j`.
    5. Inside the nested loop, check if `i` is not equal to `j` to avoid comparing the same number 
with itself.
    6. Calculate the absolute difference between `numbers[i]` and `numbers[j]` using the `abs()` 
function and store it in a variable `diff`.
    7. Check if `diff` is less than the given `threshold`.
    8. If the condition is satisfied, return True.
    9. After the loops, return False to indicate that no pair of numbers satisfy the condition.
    """
    # Please understand the requirement and write a rough solving process. The necessary details 
should be written in natural languages.
Figure 12: A case of generating target code using CoT method: the solving process description is in gray background.
