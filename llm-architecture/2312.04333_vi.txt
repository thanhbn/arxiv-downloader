# 2312.04333.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/llm-architecture/2312.04333.pdf
# Kích thước tệp: 1960107 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Liệu Lớn Hơn và Sâu Hơn Có Luôn Tốt Hơn? Khảo Sát LLaMA Qua Các Quy Mô và Tầng
Nuo Chen♣Ning Wu♢Shining Liang♢Ming Gong♢
Linjun Shou♢Dongmei Zhang♢Jia Li♣
♣Đại học Khoa học và Công nghệ Hồng Kông (Quảng Châu)
Đại học Khoa học và Công nghệ Hồng Kông
♢Microsoft
nchen022@connect.ust.hk ,jialee@ust.hk
Tóm tắt
Bài báo này trình bày một phân tích sâu sắc về
Các Mô hình Ngôn ngữ Lớn (LLMs), tập trung vào
LLaMA, một mô hình nền tảng mã nguồn mở nổi
bật trong xử lý ngôn ngữ tự nhiên. Thay vì đánh
giá LLaMA thông qua đầu ra sinh của nó, chúng
tôi thiết kế các nhiệm vụ trắc nghiệm để thăm dò
hiểu biết nội tại của nó trong các nhiệm vụ bậc
cao như suy luận và tính toán. Chúng tôi kiểm
tra mô hình theo chiều ngang, so sánh các kích
thước khác nhau, và theo chiều dọc, đánh giá
các tầng khác nhau. Chúng tôi phát hiện một số
phát hiện quan trọng và bất thường dựa trên
các nhiệm vụ thăm dò được thiết kế: (1) Theo
chiều ngang, việc mở rộng kích thước mô hình
gần như không thể tự động mang lại kiến thức
bổ sung hoặc sức mạnh tính toán. Thay vào đó,
nó có thể tăng cường khả năng suy luận, đặc
biệt trong giải quyết vấn đề toán học, và giúp
giảm ảo giác, nhưng chỉ vượt qua ngưỡng kích
thước nhất định; (2) Trong phân tích theo chiều
dọc, các tầng thấp hơn của LLaMA thiếu kiến
thức số học và thực tế đáng kể, thể hiện tư duy
logic, khả năng đa ngôn ngữ và nhận thức, với
các tầng trên cùng chứa hầu hết sức mạnh tính
toán và kiến thức thế giới thực. Những phát
hiện này cung cấp quan sát mới về khả năng
của LLaMA, đưa ra những hiểu biết về trạng
thái hiện tại của LLMs. Để tái tạo kết quả của
chúng tôi và truy cập tập dữ liệu, vui lòng tham
khảo https://github.com/nuochenpku/LLaMA_Analysis .

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) (OpenAI, 2023;
Scao et al., 2022; Chen, 2023; Yao et al., 2022;
Chen et al., 2023c) đã cho thấy tiềm năng đáng kể
trong nhiều nhiệm vụ sinh mở bậc cao như suy luận
toán học và logic. LLaMA (Touvron et al., 2023b),
một mô hình ngôn ngữ lớn nền tảng mã nguồn mở,
tiên tiến đã được thiết kế để tạo thuận lợi cho nghiên
cứu trong các cộng đồng xử lý ngôn ngữ tự nhiên.
Trong một khoảng thời gian tương đối ngắn, LLaMA
đã thu hút sự chú ý đáng kể. Sự nổi bật này có thể
được quy cho khả năng tiếp cận vốn có của nó
xMPSMPS Fact
Cal
Logical Truthful0.10.20.30.40.50.60.7
LLaMA 2-7B_1
LLaMA 2-7B_32LLaMA 2-13B_1
LLaMA 2-13B_40LLaMA 2-70B_1
LLaMA 2-70B_80Hình 1: So sánh Tổng thể với LLaMA 2 7B-70B
trong các nhiệm vụ thăm dó của chúng tôi. Giới thiệu chi
tiết về mỗi nhiệm vụ bao gồm trong Phần 3. Các đường
nét đứt biểu thị tầng đầu tiên của mỗi mô hình, trong khi
các đường liền nét biểu thị tầng cuối cùng của mô hình.

và hiệu quả được chứng minh trên một loạt đa
dạng các nhiệm vụ sinh văn bản (Hu et al., 2021;
Chen et al., 2022b, 2023a; Gao et al., 2023). Ngoài
khả năng sinh ấn tượng của LLaMA, liệu chúng ta
có thể khám phá thêm các khả năng hiểu biết nội
tại của nó? Liệu lớn hơn và sâu hơn có luôn dẫn
đến hiệu suất tốt hơn trong các khả năng tiên tiến
của nó như độ nhạy tính toán và suy luận? Giải
quyết câu hỏi này không chỉ có ích trong việc hiểu
nền tảng thành công của nó, mà còn tạo thuận lợi
cho việc hiểu các hạn chế vốn có của nó. Điều này,
lần lượt, có thể hướng dẫn những tiến bộ trong tương
lai trong kiến trúc và tối ưu hóa huấn luyện của LLMs.

Trong bài báo này, chúng tôi tiến hành một loạt
thí nghiệm để thăm dò bản chất của LLaMA trên
năm nhiệm vụ bậc cao hơn dưới học tập trong bối
cảnh, bao gồm tính toán, giải quyết vấn đề toán học
(MPS), suy luận logic, tính trung thực, và phát hiện
kiến thức thực tế.arXiv:2312.04333v4  [cs.CL]  9 Jan 2024

--- TRANG 2 ---
Hai nhiệm vụ sau được coi là các biểu tượng quan
trọng của ảo giác. Trong các nhiệm vụ này, chúng
tôi thăm dò khả năng của mô hình từ hai góc độ
khác biệt: 1) Theo chiều ngang: So sánh khả năng
của mô hình qua các kích thước khác nhau (Luật
Quy mô); 2) Theo chiều dọc: So sánh khả năng
các tầng khác nhau của cùng một mô hình kích
thước (Theo tầng). Thay vì trực tiếp kiểm tra LLMs
thông qua khả năng sinh văn bản mở của chúng,
như thường được thực hiện, chúng tôi thăm dò
LLaMA với một tập hợp các câu hỏi trắc nghiệm
đầy thử thách. Các cân nhắc chính cho thiết kế
này là: Thứ nhất, nó cung cấp đánh giá được kiểm
soát và hiệu quả, với kết quả rõ ràng, có thể định
lượng giảm tính mơ hồ. Cách tiếp cận này cho phép
kiểm tra trực tiếp các khu vực kiến thức cụ thể và
kỹ năng suy luận, cũng như xác thực độ nhạy của
mô hình đối với câu trả lời đúng hoặc sai; Thứ hai,
quan sát thí nghiệm của chúng tôi tiết lộ xu hướng
các tầng thấp hơn của LLaMA tạo ra các từ lặp lại
thay vì chuỗi mạch lạc, điều này sẽ dẫn đến so
sánh không công bằng theo tầng.

Trong bối cảnh thí nghiệm của chúng tôi củng cố
lẫn nhau, chúng tôi rút ra các kết luận sau:
Theo chiều ngang: (1) Lợi ích chính của việc tăng
kích thước mô hình nằm ở khả năng suy luận được
tăng cường của các mô hình, đáng chú ý nhất trong
khả năng cải thiện của chúng trong MPS. Sự gia
tăng kích thước này cũng có xu hướng giảm sự xuất
hiện của ảo giác. Tuy nhiên, những cải thiện này
chỉ rõ ràng khi vượt qua các ngưỡng kích thước
LLM nhất định, được gọi là khả năng nổi lên (Wei
et al., 2022). Ví dụ, các mô hình từ 7B đến 13B
cho thấy hiệu suất tương tự trên tất cả các nhiệm
vụ thăm dò. Chỉ khi kích thước mô hình tăng từ 13B
lên 70B tham số thì có thể quan sát được sự cải
thiện đáng chú ý trong khả năng suy luận và giảm
các vấn đề ảo giác, như thể hiện trong Hình 1; (2)
Khả năng số học thuần túy và kiến thức thực tế
vốn có của LLaMAs với các kích thước tham số
khác nhau là tương tự đáng kể. Nói cách khác,
việc tăng kích thước mô hình không nhất thiết
mang lại kiến thức thực tế bổ sung hoặc tăng cường
đáng kể khả năng tính toán, đặc biệt khi cùng khối
lượng tập dữ liệu tiền huấn luyện được sử dụng.

Theo chiều dọc: (1) Chúng tôi phát hiện rằng các
tầng thấp và giữa của LLaMA có khả năng số học
thuần túy và kiến thức thực tế gần như không đáng
kể. Khi các tầng mạng sâu hơn, có một bước nhảy
đáng chú ý trong hiệu suất. Ngược lại, ngay cả ở
các tầng thấp nhất, LLaMA vẫn có tư duy logic và
khả năng nhận thức, như trong toán học, suy luận
logic, và tránh ảo giác. Mặc dù những khả năng
này có tăng cường nhẹ với các tầng sâu hơn, sự
cải thiện vẫn khá hạn chế. Điều này ngụ ý rằng
các LLMs hiện tại chủ yếu chứa sức mạnh tính toán
và kiến thức thế giới thực trong các tầng trên của
chúng, trong khi các tầng dưới hướng tới tư duy
trừu tượng liên quan nhưng thiếu kiến thức thế giới
thực đáng kể và kỹ năng tính toán. (2) Thú vị, trong
các so sánh hiệu suất từng tầng của chúng tôi, chúng
tôi quan sát thấy hiệu suất tối ưu của mô hình trong
MPS và khả năng tính toán không phải lúc nào cũng
ở tầng cuối cùng. Thường xuyên hơn, những khả
năng đỉnh cao này được tìm thấy ở một vài tầng
trước tầng cuối. Tuy nhiên, ngược lại, để biểu diễn
kiến thức thực tế, tầng cuối cùng của mô hình chứng
tỏ là cực kỳ quan trọng.

Hơn nữa, chúng tôi mở rộng các nhiệm vụ thăm
dò toán học vào bối cảnh suy luận đa ngôn ngữ.
Cụ thể, chúng tôi giữ nguyên các câu hỏi và lựa
chọn sai và dịch các câu trả lời đúng sang các ngôn
ngữ khác để đánh giá khả năng đa ngôn ngữ của
LLaMA. Trong thiết lập này, các thí nghiệm theo
tầng của chúng tôi cho thấy một hiệu ứng hoàn toàn
ngược lại với suy luận đơn ngôn ngữ: hiệu suất của
mô hình giảm dần khi các tầng sâu hơn. Điều này
chỉ ra rằng các tầng trước của LLaMA chịu trách
nhiệm bảo tồn các đặc trưng đa ngôn ngữ chung.

Đáng chú ý, các kết quả được trình bày trong thí
nghiệm của chúng tôi không nhất thiết tương đương
trực tiếp với khả năng sinh của LLaMA. Thay vào
đó, trong bài báo này, chúng tôi cung cấp một góc
nhìn mới và toàn diện để quan sát hiệu suất tự nhiên
của LLaMA, đưa ra những hiểu biết để hiểu rõ hơn
các LLMs hiện tại.

2 LLaMA
LLaMA (Touvron et al., 2023a,c) là một loạt mô
hình ngôn ngữ lớn nền tảng, được phát hành bởi
META, đã trở thành LLMs mã nguồn mở phổ biến
nhất trong các cộng đồng NLP. LLaMA được xây
dựng trên các tầng transformer (Vaswani et al., 2017),
được huấn luyện trên hàng nghìn tỷ token với mục
tiêu mô hình hóa ngôn ngữ, cho thấy khả năng mạnh
mẽ trong các nhiệm vụ hạ nguồn. Các biểu diễn
ngữ cảnh được tối ưu hóa bằng cách dự đoán token
tiếp theo dựa trên các chuỗi đầu vào.

Trong công trình này, chúng tôi thăm dò các LLMs
chuỗi LLaMA 2 với các nhiệm vụ được thiết kế
của chúng tôi, từ 7B đến 70B tham số trong học
tập trong bối cảnh. Cụ thể, LLaMA 2-7B, 13B và
70B bao gồm 32, 40 và 80 tầng transformer với
4096, 5120 và 8192 kích thước nhúng ẩn, tương ứng.

--- TRANG 3 ---
Loại Bit + - × ÷ Mix-2 Mix-3
Int1-2 200 200 200 200 200 200
3-4 200 200 200 200 200 200
5-6 200 200 200 200 200 200
Float1-2 200 200 200 200 200 200
3-4 200 200 200 200 200 200
5-6 200 200 200 200 200 200
Bảng 1: Thống kê dữ liệu kiểm tra trong các nhiệm vụ số học của chúng tôi.

3 Nhiệm vụ Thăm dò
Các nhiệm vụ thăm dò thường được sử dụng để
khám phá kiến thức vốn có và các đặc trưng ngôn
ngữ trong các mô hình học sâu. Trước đây, Jawahar
et al. (2019a) đã sử dụng một loạt nhiệm vụ thăm
dò để kiểm tra các biểu diễn nội bộ của BERT. Tuy
nhiên, với sự tiến bộ nhanh chóng của LLMs, hiện
tại thiếu nghiên cứu toàn diện phân tích sâu mối
quan hệ giữa các khả năng bậc cao của LLMs đương
đại và các yếu tố như kích thước mô hình và các
tầng mạng.

Để lấp đầy khoảng trống này, chúng tôi sử dụng
các nhiệm vụ thăm dò để truy cập LLaMA trong
khả năng mã hóa các loại đặc trưng khác nhau qua
hai góc nhìn: kích thước mô hình và các tầng riêng
lẻ. Cụ thể, chúng tôi thiết kế năm nhiệm vụ bậc
cao: tính toán, giải quyết vấn đề toán học (MPS),
suy luận logic, tính trung thực, và phát hiện kiến
thức thực tế. Chúng tôi bao gồm hai nhiệm vụ sau
như phát hiện ảo giác trong phần tiếp theo. Bên
cạnh đó, chúng tôi cũng thăm dò hiệu quả LLaMA
trong suy luận toán học đa ngôn ngữ. Trong phần
này, chúng tôi sẽ minh họa chúng tuần tự.

3.1 Tính toán
Trong bài báo này, chúng tôi tập trung vào kiểm
tra LLMs trong các nhiệm vụ số học cơ bản, bao
gồm bốn biểu thức số học đơn giản: cộng (+), trừ (-),
nhân (×) và chia (÷):

• Cộng hai phần tử trong khoảng 1∼100,
100∼10000, 10000∼100000, tương ứng.

• Trừ hai phần tử trong khoảng 1∼100,
100∼10000, 10000∼1000000, tương ứng.

• Nhân hai phần tử trong khoảng 1∼100,
100∼10000, 10000∼1000000, tương ứng.

• Chia hai phần tử trong khoảng 1∼100,
100∼10000, 10000∼1000000, tương ứng.

• Các phép toán số học phức tạp yêu cầu thực
hiện hai phép toán cộng, trừ, nhân, hoặc chia.

• Các phép toán số học phức tạp yêu cầu thực
hiện ba phép toán cộng, trừ, nhân, hoặc chia.

Đáng chú ý, các phần tử được sử dụng trong các
phép toán số học trên bao gồm số nguyên và số
thập phân (với độ chính xác lên đến ba chữ số thập
phân), tương ứng. Bảng 1 cho thấy thống kê dữ
liệu tương ứng. Vì chúng tôi thăm dò các khả năng
tính toán của LLaMA thông qua nhiệm vụ trả lời
câu hỏi trắc nghiệm, để tăng độ khó và kiểm tra
độ nhạy của mô hình đối với sự khác biệt nhỏ trong
kết quả tính toán, chúng tôi ngẫu nhiên cộng hoặc
trừ một số thập phân trong khoảng ±20 (trừ 0) vào
câu trả lời đúng để tạo ra ba lựa chọn sai khác nhau
nhưng không rõ ràng.

Thiết kế tập kiểm tra của chúng tôi cho phép so
sánh trực quan và chi tiết về 1) điểm mạnh và yếu
tương đối của mô hình trong các phép toán cộng,
trừ, nhân và chia; 2) các mẫu hiệu suất của mô hình
khi đối mặt với các phép tính phức tạp; 3) sự biến
đổi trong khả năng tính toán của mô hình khi xử lý
số thập phân và số nguyên, số 1-2 chữ số, 3-4 chữ
số, 5-6 chữ số tương ứng. Dữ liệu của chúng tôi
được xây dựng bằng cách gọi các hàm python
random.randint() và random.uniform().

3.2 Giải quyết Vấn đề Toán học
Bên cạnh việc xác thực LLaMA trong các nhiệm
vụ số học, chúng tôi cũng kiểm tra mô hình trong
các nhiệm vụ MPS để xem xét toàn diện các khả
năng suy luận toán học của nó.

Chúng tôi chọn GSM8K (Cobbe et al., 2021) làm
dữ liệu nguồn để xây dựng các lựa chọn đầy thử
thách và gây hiểu lầm có thể đánh lừa mô hình
hiệu quả. Chiến lược của chúng tôi bao gồm các
bước sau:

• Trước tiên, chúng tôi tinh chỉnh mô hình LLaMA
2-13B trên GSM8K, sau đó thực hiện lấy mẫu
từ chối thông qua suy luận 100 lần để tạo ra
các đường suy luận khác nhau dựa trên mô
hình kết quả.

• Tiếp theo, chúng tôi trích xuất tất cả các công
thức trong mỗi đường suy luận và xác thực
độ chính xác của chúng. Chúng tôi sử dụng
các đường suy luận sai lầm để xây dựng dữ
liệu nhiệm vụ thăm dò của chúng tôi:

– Nếu một đường suy luận chỉ chứa lỗi
tính toán, có nghĩa là câu trả lời đúng
có thể được thu được bằng cách tính
toán lại, chúng tôi giữ lại nó như một
phần của tập kiểm tra thăm dò MPS-Cal
của chúng tôi.

--- TRANG 4 ---
Loại Nhiệm vụ Câu hỏi & Lựa chọn
Arithmetic-IntCâu hỏi: 2331 + 2693 = ? Lựa chọn: 5024 (✓); 5018; 5005; 5025
Câu hỏi: 109848 ÷199 = ? Lựa chọn: 552.0 (✓); 516.0; 558.0; 567.0
Arithmetic-FloCâu hỏi: 7.682 + 28.894 = ? Lựa chọn: 36.576 (✓); 28.576; 40.909; 38.076
Câu hỏi: 25.204 ×88.29÷12.133 = ? Lựa chọn: 183.406 (✓); 183.739; 185.406; 181.962
MPS-CalCâu hỏi: Peyton có 3 đứa con và mỗi đứa nhận được một hộp nước trái cây trong bữa trưa, 5 ngày một tuần. Năm học kéo dài
25 tuần. Cô ấy sẽ cần bao nhiêu hộp nước trái cây cho toàn bộ năm học cho tất cả các con của mình?
Lựa chọn: Peyton cần 25 tuần x 5 ngày x 3 trẻ = 375 hộp nước trái cây (✓);
25 tuần x 5 ngày x 3 trẻ = 75 hộp nước trái cây;
Với các điều kiện của bài toán, 3 trẻ, 5 ngày một tuần, kéo dài 25 tuần, đó là 3*5*25 = 105 hộp nước trái
cây cần thiết.
MPS-ReaCâu hỏi: Một gia đình 12 con khỉ đã thu thập 10 đống chuối. 6 đống có 9 nải, mỗi nải có
14 quả chuối, trong khi các đống còn lại có 12 nải, mỗi nải có 9 quả chuối. Mỗi con khỉ sẽ nhận được bao nhiêu quả chuối
nếu chúng chia đều chuối cho nhau?
Lựa chọn: 6 đống đầu tiên có 6 x 9 x 14 = 756 quả chuối. Có 10 - 6 = 4 đống còn lại.
4 đống còn lại có 4 x 12 x 9 = 432 quả chuối. Tổng cộng, có 756 + 432 = 1188
quả chuối. Mỗi con khỉ sẽ nhận được 1188/12 = 99 quả chuối (✓);
6 đống có 6 x 9 x 14 = 756 quả chuối. 6 đống còn lại có 6 x 12 x 9 = 648 quả chuối. Tổng cộng, có
756 + 720 = 1476 quả chuối. Mỗi con khỉ sẽ nhận được 1476/12 = 123.0 quả chuối;
6 đống có 6 x 9 x 14 = 756 quả chuối. Có 10 - 6 = 4 đống chuối với 12 nải và 4 đống
chuối với 6 nải. 4 đống chuối với 12 nải có 4 x 12 x 9 = 432 quả chuối. 4 đống
chuối với 6 nải có 4 x 6 x 9 = 216 quả chuối. Có 756 + 432 + 240 = 1428 quả chuối. Mỗi
con khỉ sẽ nhận được 1428/12 = 119.0 quả chuối
Bảng 2: Ví dụ kiểm tra trong các nhiệm vụ thăm dò tính toán và MPS được thiết kế của chúng tôi.

Nhiệm vụ Arithmetic-Int/Float Reclor (x) MPS-Cal (x) MPS-Rea TruthfulQA LAMA∗
Trung bình Chân lý cơ sở 1 1 1 1 3.5 1
Trung bình Ứng viên 4 4 3 4.9 7.6 9.7
Tổng Câu hỏi 3600 500 712 1000 817 3070
Bảng 3: Thống kê dữ liệu kiểm tra tổng thể trong các nhiệm vụ thăm dò của chúng tôi. LAMA∗ đề cập đến chúng tôi chỉ sử dụng một tập con của tập dữ liệu gốc.

– Nếu tất cả các phép tính trong một đường
suy luận đều đúng, nhưng kết luận cuối
cùng sai, cho thấy lỗi suy luận, chúng
tôi sử dụng nó cho tập kiểm tra MPS-Rea
của chúng tôi.

MPS-Cal tập trung vào đánh giá độ nhạy của mô
hình đối với kết quả tính toán trong giải quyết các
vấn đề toán học. Ngược lại, MPS-Rea nhấn mạnh
đánh giá khả năng của mô hình trong việc phân
biệt đường suy luận đúng với sai, yêu cầu mức độ
hiểu biết và khả năng suy luận vượt trội. Bảng 2
cho thấy một số ví dụ trong các nhiệm vụ MPS
và tính toán.

3.3 Suy luận Logic
Là một chỉ báo quan trọng của các khả năng tiên
tiến của LLMs đương đại, suy luận logic nổi bật
về tầm quan trọng của nó trong việc kiểm tra, phân
tích và đánh giá phê phán các lập luận trong ngôn
ngữ tự nhiên. Trong nghiên cứu của chúng tôi,
chúng tôi sử dụng Reclor (Yu et al., 2020) như một
nền tảng kiểm tra để đánh giá kỹ năng suy luận
logic của các mô hình lớn này. Reclor bao gồm
một tập dữ liệu được rút ra từ các câu hỏi suy luận
logic được tìm thấy trong các bài kiểm tra chuẩn
hóa để tuyển sinh sau đại học. Mỗi mẫu từ Reclor
chứa một ngữ cảnh, một câu hỏi tương ứng và bốn
lựa chọn.

3.4 Phát hiện Ảo giác
Ảo giác, có nghĩa là tạo ra nội dung lệch khỏi sự
thật thế giới thực được quan sát trong quá trình
tiền huấn luyện, được coi là một trong những vấn
đề thách thức nhất trong LLMs. Để điều tra thêm
mối quan hệ giữa ảo giác và các tầng mô hình và
kích thước, chúng tôi tiến hành kiểm tra từ hai khía
cạnh: 1) Đo lường xem một mô hình ngôn ngữ có
trung thực trong việc tạo ra câu trả lời cho các câu
hỏi hay không, còn được gọi là tính trung thực;
2) Kiểm tra kiến thức thực tế nội bộ của mô hình.
Chúng tôi sử dụng các nhiệm vụ TruthfulQA MC
(Lin et al., 2022) và LAMA (Petroni et al., 2019)
làm bệ kiểm tra cho hai khía cạnh này, tương ứng.
Điều quan trọng cần lưu ý là trong TruthfulQA,
có thể có nhiều hơn một câu trả lời đúng, đi kèm
với 4-5 lựa chọn sai. Đối với LAMA, chúng tôi
ngẫu nhiên trích xuất một tập con chứa 3070 câu
hỏi cùng với 9-10 lựa chọn tương ứng của chúng.
Bảng 3 trình bày thống kê dữ liệu chi tiết trong
các nhiệm vụ thăm dò của chúng tôi.

--- TRANG 5 ---
Kích thước Mô hình LAMA∗Reclor MPS-Cal MPS-ReaTruthfulQA Arithmetic
(Fact ) ( Logical ) MC1 MC3 Int Float
7B 57.9 20.0 28.7 47.0 28.6 20.7 67.9 52.5
13B 57.9 23.7 30.2 46.6 29.1 20.7 70.6 52.6
70B 58.7 26.4 48.3 51.9 37.3 27.1 70.8 52.9
Bảng 4: Hiệu suất tổng thể của mỗi mô hình LLaMA 2 kích thước trong các nhiệm vụ thăm dò của chúng tôi. LAMA∗ đề cập đến chúng tôi chỉ sử dụng một tập con của tập dữ liệu gốc. Độ chính xác MC3 có nghĩa là tổng xác suất chuẩn hóa được gán cho tất cả câu trả lời đúng trong các ứng viên trong TruthfulQA.

3.5 Giải quyết Vấn đề Toán học Đa ngôn ngữ
Trong nghiên cứu này, chúng tôi đi sâu hơn vào
các khả năng đa ngôn ngữ của LLaMA. Chúng tôi
dịch các câu trả lời đúng từ hai tập dữ liệu thu thập
được: MPS-Cal và MPS-Rea trong Phần 3.2 sang
bốn ngôn ngữ bổ sung: Trung Quốc, Pháp, Tây
Ban Nha và Thái, trong khi giữ nguyên các câu
hỏi và các lựa chọn sai khác, các tập kiểm tra mới
kết quả được đặt tên là xMPS-Cal và xMPS-Rea.
Thiết lập này cung cấp một số lợi thế: Thứ nhất,
nó kiểm tra khả năng của mô hình trong chuyển
giao suy luận đa ngôn ngữ, thể hiện thành thạo
của nó không chỉ trong nhận biết mà còn suy luận
trong nhiều ngôn ngữ. Thứ hai, bằng cách trộn lựa
chọn sai với câu trả lời đúng trong các ngôn ngữ
khác nhau, chúng tôi đánh giá mạnh mẽ khả năng
thích ứng và hiểu biết của mô hình qua các rào
cản ngôn ngữ. Thiết lập độc đáo này thách thức
khả năng của mô hình trong việc xử lý và tích hợp
thông tin đa ngôn ngữ, không chỉ đánh giá các khả
năng cụ thể theo ngôn ngữ của mô hình mà còn
tính linh hoạt tổng thể trong hiểu biết và suy luận
đa ngôn ngữ.

3.6 Thiết lập Kiểm tra
Xem xét một tập dữ liệu thăm dò D={Q, C, O},
trong đó Q, C và O biểu thị một tập hợp câu hỏi,
ngữ cảnh (chỉ tồn tại cho LAMA), và các lựa chọn
câu trả lời. Cho mỗi câu hỏi q∈Q, có một tập hợp
lựa chọn câu trả lời tương ứng, được ký hiệu là
o∈O, trong đó o={o1, o2, ..., on−1, a}, n là số
lượng lựa chọn câu trả lời, và a đề cập đến câu
trả lời đúng.

Nhiệm vụ của mô hình là xác định câu trả lời đúng
từ tập hợp o cho mỗi câu hỏi q. Nó cần gán log-
xác suất hoàn thành cao nhất theo sau câu hỏi,
độc lập với các lựa chọn câu trả lời khác (Chuang
et al., 2023). Quá trình lựa chọn này có thể được
biểu diễn toán học như:

o∗i = argmax log P(oi|q) (1)
Acc=(1,nếu a∗>(o∗1, ..o∗n−1), 0,ngược lại. (2)

Trong đó logP(oi|q) là log-xác suất của lựa chọn
oi đối với câu hỏi q, như được đánh giá bởi mô hình.

3.7 Thiết lập Thí nghiệm
Chúng tôi chọn LLaMA 2 từ 7B đến 70B làm đối
tượng thí nghiệm của chúng tôi. Quan sát thấy
LLaMA 2 thể hiện sự bất ổn đáng kể trong kiểm
tra không-shot, chúng tôi chọn thực hiện prompting
few-shot trong các nhiệm vụ thăm dò của chúng tôi
để tối ưu hóa hiệu suất của mô hình. Trong TruthfulQA
và LAMA, chúng tôi tương ứng sử dụng phương
pháp 6-shot (Bảng 7) và 4-shot (Bảng 8). Đối với
các nhiệm vụ suy luận, chúng tôi nhất quán sử dụng
4-shot cho cả (x) MPS (Bảng 10) và suy luận logic
(Bảng 9). Trong các nhiệm vụ tính toán, chúng tôi
sử dụng 6-shot ví dụ (Bảng 6). Các prompts chi tiết
được trình bày trong Phụ lục A.

4 Thí nghiệm về Thăm dò Kích thước Mô hình
Trong phần này, chúng tôi tập trung vào việc trình
bày so sánh kết quả từ LLaMA với các kích thước
khác nhau trên các nhiệm vụ thăm dò của chúng
tôi, như thể hiện trong Bảng 41. Trong Bảng 5,
chúng tôi trình bày hiệu suất chi tiết của các mô
hình dưới các quy tắc số học và số chữ số khác
nhau. Kết hợp hai bảng này, chúng tôi có thể rút
ra các kết luận sau:

Tăng kích thước mô hình khó có thể tăng cường
kiến thức nội bộ của mô hình. Từ Bảng 4, chúng
ta có thể thấy rằng hiệu suất của LLAMA 2-7B và
13B trên LAMA là giống hệt nhau, và thậm chí
tăng kích thước mô hình lên 70B chỉ dẫn đến sự
cải thiện nhẹ (58.7% so với 57.9%). Điều này cho
thấy chỉ tăng kích thước mô hình là khó cải thiện
khả năng của mô hình trong việc ghi nhớ và hiểu
kiến thức có mặt trong tập dữ liệu huấn luyện,
với điều kiện dữ liệu huấn luyện vẫn giữ nguyên.

Tăng kích thước mô hình không tăng cường đáng
kể khả năng tính toán cơ bản. Tương tự, trong các
nhiệm vụ tính toán của chúng tôi, các mô hình với
kích thước khác nhau cũng cho thấy khả năng tính
toán tương tự. Mặc dù mô hình 7B hơi kém trong

1Đáng chú ý, chúng tôi sử dụng tầng cuối cùng của mô hình để đếm hiệu suất của nó trong phần này.

--- TRANG 6 ---
Kích thước Bit + - × ÷ M-2 M-3
Số học Số nguyên
7B1-2 99.5 100.0 95.0 100.0 55.5 39.5
3-4 98.0 98.0 59.0 59.5 48.5 20.5
5-6 89.0 83.0 53.0 30.5 48.5 17.5
13B1-2 99.5 100 98.5 100.0 58 44.5
3-4 99.5 99.0 73.5 69.5 53.5 26.0
5-6 96.5 96.5 63.5 25.5 53.5 17.5
70B1-2 99.5 100.0 98.0 100.0 64.0 46.0
3-4 99.0 99.0 75.0 68.0 42.0 18.0
5-6 100.0 100.0 77.0 23.0 41.0 20.0
Số học Số thập phân
7B1-2 99.5 100 23.0 78.0 37.0 30.0
3-4 98.0 98.0 18.0 17.0 32.0 19.0
5-6 94.0 87.0 19.5 13.0 35.0 14.5
13B1-2 99.0 100.0 26.0 90.0 40.0 28.0
3-4 99.5 99.5 14.5 20.5 33.0 19.0
5-6 99.0 96.5 13.5 13.5 39.5 17.0
70B1-2 100.0 100.0 26.5 99.0 43.0 30.0
3-4 98.5 100.0 14.0 39.0 39.5 17.0
5-6 98.5 99.5 14.5 17.5 45.5 17.5
Bảng 5: Kết quả chi tiết của các phép toán khác nhau trong
các nhiệm vụ thăm dò số học của chúng tôi. M-2/3 đề cập đến
biểu thức số học yêu cầu 2/3 lần phép toán hỗn hợp.

1 2 3 4 5 6 7
Bước Suy luận trong MPS-Rea20304050607080Độ chính xác79.5
64.0
43.1
38.2
31.234.1
29.075.0
61.6
44.8
36.0
33.337.6
26.278.0
66.9
50.3
45.2
35.440.4
28.6Mô hình
LLaMA 2-7B
LLaMA 2-13B
LLaMA 2-70B
Hình 2: So sánh tổng thể giữa LLaMA 2 7B đến
70B xử lý các vấn đề bước suy luận khác nhau trong
các nhiệm vụ thăm dò MPS-Rea của chúng tôi.

các phép toán số nguyên so với 13B và 70B, nó
vẫn thực hiện tương tự trong các phép toán số
thập phân (52.5% so với 52.6% so với 52.9%).
Rõ ràng, khả năng tính toán của các mô hình 13B
và 70B gần như giống hệt nhau.

Các mô hình lớn hơn cho thấy sự cải thiện tương
đối trong khả năng suy luận và tính trung thực.
Trong MPS-Cal, yêu cầu không chỉ khả năng tính
toán mà còn hiểu biết và suy luận của các vấn đề
toán học, mô hình 70B vượt trội đáng kể so với
các mô hình 7B và 13B (48.3% so với 30.2%,
28.7%); MPS-Rea đòi hỏi sự phân biệt rõ ràng
giữa các đường suy luận đúng và sai, thách thức
thêm các khả năng suy luận của mô hình. Ở đây,
LLaMA 2-70B vẫn cho thấy sự cải thiện đáng kể.
Xem xét rằng ba LLMs cho thấy hiệu suất tính
toán tương tự, chúng tôi lập luận rằng những cải
thiện vượt trội như vậy có thể đóng góp vào suy
luận toán học tốt hơn của mô hình 70B.

Hình 2 tiếp tục chỉ ra rằng tất cả các kích thước
mô hình thực hiện tốt trên các vấn đề toán học
yêu cầu 1-2 bước suy luận, với sự khác biệt tương
đối tối thiểu giữa chúng. Sự tăng cường khả năng
toán học trong mô hình 70B, tương đối với các mô
hình 7B và 13B, chủ yếu tập trung vào các vấn đề
yêu cầu 3-6 bước suy luận. Những phát hiện trên
chứng minh rằng tất cả các mô hình chuỗi LLaMA
đều có khả năng suy luận cơ bản. Tuy nhiên, khả
năng giải quyết các vấn đề suy luận phức tạp hơn
chỉ xuất hiện khi vượt qua các ngưỡng kích thước
mô hình nhất định. Hơn nữa, khi đối mặt với các
vấn đề yêu cầu 7 bước suy luận, hiệu suất của tất
cả các mô hình giảm nhanh chóng và cho thấy ít
sự khác biệt, cho thấy rằng ngay cả LLaMA 2-70B
vẫn ở mức "trí tuệ vừa phải", thiếu kỹ năng suy
luận mạnh.

Trong tính toán, hiệu suất của LLaMA giảm với
sự phức tạp tăng của phép toán và số. LLaMA có
khả năng cộng và trừ mạnh, nhưng khả năng nhân
và chia của nó giảm đáng chú ý với số chữ số tăng,
như thấy trong Bảng 5. So với các phép toán số
thập phân, LLaMA tốt hơn trong các phép toán
số nguyên. Thú vị, trong các phép toán số nguyên,
LLaMA cho thấy khả năng tốt hơn trong phép nhân,
nhưng thế mạnh này giảm đáng kể khi xử lý số
thập phân.

5 Thí nghiệm về Thăm dò Theo Tầng
Trong phần này, chúng tôi tập trung vào đánh giá
mỗi tầng của LLaMA qua các nhiệm vụ thăm dò
khác nhau của chúng tôi. Chúng tôi trình bày kết
quả toàn diện của tất cả các tầng qua ba mô hình
kích thước trong Phụ lục.

Khả năng tính toán chủ yếu tồn tại trong các tầng
trên của mô hình. Đầu tiên, trong Hình 3, chúng
tôi trình bày hiệu suất của các tầng khác nhau của
các mô hình từ 7B đến 70B trong việc thực hiện
tính toán số nguyên và số thập phân 5-6 chữ số.
Từ hình, rõ ràng rằng hầu như không có khả năng
tính toán thuần túy nào tồn tại trong các tầng thấp
hơn của bất kỳ mô hình kích thước nào. Tuy nhiên,
khi số lượng tầng tăng, có một bước nhảy đáng kể
trong khả năng tính toán, đạt đỉnh trong một vài
tầng cuối cùng. Các kết quả trên giải thích một
cách thích hợp tại sao, trong nhiệm vụ thăm dò
MPS-cal, hiệu suất của mô hình

--- TRANG 7 ---
7B 13B 70B7B 13B 70B
(b) Số học số thập phân 5-6Bit(a) Số học số nguyên 5-6Bit
Hình 3: So sánh tổng thể giữa LLaMA 2 7B đến 70B xử lý tính toán 5–6 bit trong các nhiệm vụ thăm dò
số học của chúng tôi. Chúng tôi trình bày kết quả chi tiết hơn của tính toán 1-2 và 3-4 bit trong Phụ lục B, Hình 7.

7B 70B
Hình 4: So sánh Tổng thể với LLaMA 2-7B và 70B trong các nhiệm vụ thăm dò của chúng tôi. Chúng tôi bao gồm hiệu suất tất cả các tầng của mỗi mô hình kích thước trong Phụ lục B, Bảng 11, 12 và 13.

cải thiện đáng kể với độ sâu tầng tăng, như thể
hiện trong Hình 4. Đáng chú ý, trong hầu hết các
trường hợp, tầng cuối cùng của mô hình không
nhất thiết đại diện cho khả năng tính toán tốt nhất,
đặc biệt trong các biểu thức số học phức tạp. Ví
dụ, các tầng 28-29 của mô hình 7B thể hiện kỹ
năng tính toán tốt hơn tầng cuối cùng.

Các mô hình chủ yếu nhúng kiến thức thực tế
phong phú trong các tầng trên của chúng. Như
mô tả trong Hình 4, đối với cả mô hình 7B và 70B,
hiệu suất trên LAMA gợi ý kiến thức thực tế được
học bởi LLaMA cũng chủ yếu nằm trong các tầng
trên. Ngược lại, các tầng thấp hơn thể hiện sự thiếu
hụt đáng chú ý trong việc giữ lại kiến thức này.
Tuy nhiên, với sự gia tăng độ sâu tầng, mô hình
thể hiện sự tăng cường đáng kể trong khả năng
xử lý và giữ lại thông tin thực tế. Đáng chú ý, quan
sát thấy tầng cuối cùng của LLaMA chứa lượng
kiến thức thực tế lớn nhất. Phát hiện này trái ngược
với các nhiệm vụ thăm dò khác nơi hiệu suất đỉnh
của mô hình

--- TRANG 8 ---
7B 13B 70BHình 5: So sánh Tổng thể với LLaMA 2-7B đến 70B trong các nhiệm vụ thăm dò xMPS-Rea của chúng tôi. ES, FR, ZH và TH đề cập đến Tây Ban Nha, Pháp, Trung Quốc và Thái.

thường được thể hiện trong các tầng gần cuối,
thay vì trong tầng cuối cùng tuyệt đối.

Khả năng tư duy trừu tượng và nhận thức của
LLaMAs hiện diện nhất quán qua tất cả các tầng.
Một quan sát so sánh về hiệu suất của mô hình
qua các tầng khác nhau trong các nhiệm vụ như
MPS-Rea, TFQA và Reclor tiết lộ rằng ngay cả
trong các tầng thấp nhất của mô hình (ví dụ, tầng
đầu tiên), có một mức độ nhất định của khả năng
suy luận và nhận thức, đặc biệt trong suy luận
toán học, được chứng minh bởi kết quả trong
MPS-Rea. Mặc dù các tầng trên vẫn thể hiện hiệu
suất tốt nhất cho các nhiệm vụ thăm dò tương ứng,
sự cải thiện tương đối hạn chế. Chúng tôi suy đoán
rằng lý do cho khoảng cách hiệu suất nhỏ từ tầng
dưới đến tầng trên trong nhiệm vụ thăm dò MPS-Rea
cho mô hình LLaMA 2-7B có thể do: 1) Thiếu tập
dữ liệu nhiệm vụ toán học liên quan trong giai đoạn
tiền huấn luyện, dẫn đến huấn luyện không đủ;
2) Nhiệm vụ MPS-Rea đòi hỏi mức độ cao khả năng
suy luận toán học, mà mô hình LLaMA2-7B hiện
tại, ngay cả ở tầng cao nhất, không có khả năng
mạnh trong đó.

Các tầng trước qua các quy mô mô hình khác nhau
cho thấy khả năng tương tự. Trong các nhiệm vụ
thăm dò của chúng tôi, các tầng thấp hơn (như 15
tầng đầu tiên) của các mô hình với quy mô khác
nhau thể hiện hiệu suất gần như giống hệt nhau,
mặc dù có kích thước nhúng ẩn và đầu chú ý khác
nhau trong các tầng transformer của chúng. Điều
này gợi ý rằng khi thông tin ngữ cảnh tiến triển
qua các tầng giữa-trên của LLaMA, nó bắt đầu
chuyên môn hóa, dẫn đến sự gia tăng các khả
năng bậc cao.

6 Thí nghiệm về Thăm dò xMPS
Trong phần này, chúng tôi tiếp tục thăm dò khả
năng đa ngôn ngữ của các mô hình LLaMA. Hình 5
cho thấy hiệu suất của ba mô hình trong các nhiệm
vụ thăm dò xMPS được thiết kế của chúng tôi qua
bốn ngôn ngữ.

Từ so sánh với Hình 3, trước tiên chúng tôi quan
sát thấy các mô hình chuỗi LLAMA cho thấy sự
giảm hiệu suất đáng chú ý trong các ngôn ngữ
khác tiếng Anh, đặc biệt trong ngôn ngữ ít tài
nguyên Thái. Cả 7B và 13B LLaMAs vẫn cho thấy
hiệu suất rất tương tự trong lĩnh vực này, cho thấy
khả năng đa ngôn ngữ tương tự của chúng, tuy
nhiên mô hình 70B luôn vượt trội hơn chúng. Ngoài
ra, các tầng thấp hơn của các mô hình thể hiện
hiệu suất tương tự qua các ngôn ngữ, với hiệu quả
của chúng trong tiếng Pháp và Tây Ban Nha ngang
bằng với tiếng Anh. Sự tương tự này có thể do
gốc rễ gia đình ngôn ngữ Latin của chúng và sự
bao gồm trong tập dữ liệu tiền huấn luyện LLaMA.

Tuy nhiên, không giống như kết quả trong tất cả
các nhiệm vụ thăm dó trước đó, hiệu suất của các
mô hình trong các ngôn ngữ này giảm với các tầng
sâu hơn, một xu hướng đặc biệt rõ rệt trong mô
hình 13B. Mặc dù có sự phục hồi nhẹ trong các
tầng trên, tầng trên cùng vẫn hoạt động kém hơn
các tầng thấp hơn. Cho rằng các thí nghiệm trước
đã chỉ ra khả năng suy luận toán học đáng kể qua
tất cả các tầng của chuỗi LLaMA, dường như các
tầng thấp hơn chủ yếu chịu trách nhiệm giữ lại
các khả năng đa ngôn ngữ. Đặc điểm này, tuy nhiên,
giảm với độ sâu tầng tăng, ảnh hưởng đến

--- TRANG 9 ---
7B 70B 13B
7B 13B(a) Tầng Cuối
(b) Tầng Đầu
70BHình 6: Biểu đồ T-SNE 2D của các nhúng ngôn ngữ được tính toán từ tầng đầu tiên và cuối cùng của LLaMA 2 7B-70B trên nhiệm vụ thăm dò xMPS-Rea.

khả năng của chúng để diễn giải chính xác câu trả
lời trong các ngôn ngữ khác trong các nhiệm vụ
xMPS. Hiện tượng này, tuy nhiên, ít rõ rệt hơn
đáng kể trong các tầng trên của mô hình 70B, cho
thấy rằng tăng cường kích thước mô hình hoặc
số lượng tầng mạng có thể là một cách tiếp cận
hiệu quả để củng cố khả năng đa ngôn ngữ của
LLMs.

Để phân tích thêm hiện tượng trên, chúng tôi thực
hiện các hình ảnh hóa T-SNE 2D của các biểu diễn
nhúng của tầng đầu tiên và cuối cùng của LLaMA
trong nhiệm vụ xMPS-Rea qua các ngôn ngữ khác
nhau. Những hình ảnh hóa này cho thấy rằng ở
tầng trên của mô hình, sự phân tách rõ ràng giữa
các biểu diễn của các ngôn ngữ khác nhau tồn tại.
Ngược lại, ở tầng dưới của mô hình, các biểu diễn
của các ngôn ngữ khác nhau, đặc biệt tiếng Anh,
Pháp và Tây Ban Nha, tương đối gần nhau và gần
như hòa trộn với nhau, cho thấy rằng các tầng thấp
hơn chủ yếu bảo tồn các đặc trưng không phụ
thuộc ngôn ngữ. Sự phân biệt rõ rệt của tiếng Trung
và Thái với các ngôn ngữ khác chủ yếu xuất phát
từ việc thiếu dữ liệu tiền huấn luyện tiếng Trung
và Thái trong tập dữ liệu của LLaMA.

Hiện tượng tương tự cũng có thể quan sát trong
nhiệm vụ thăm dó xMPS-Cal của chúng tôi, nơi
chúng tôi trình bày kết quả tương ứng trong Phụ
lục B, Hình 8

7 Công trình Liên quan
Khả năng diễn giải của mạng nơ-ron (Peters et al.,
2018; Goldberg, 2019), đặc biệt là các mô hình
ngôn ngữ, gần đây đã thu hút sự chú ý đáng kể
từ các học giả trong lĩnh vực Xử lý Ngôn ngữ Tự
nhiên (NLP). Trong vài năm qua, phần lớn nghiên
cứu này đã tập trung vào BERT (Devlin et al., 2019),
khám phá cách các mô hình ngôn ngữ nắm bắt
ngữ nghĩa văn bản qua các tầng khác nhau (Tenney
et al., 2019; Jawahar et al., 2019b; Liu et al., 2019;
Chen et al., 2023b; Chuang et al., 2023). Ví dụ,
Tenney et al. (2019) đã giới thiệu một nhiệm vụ
thăm dò cạnh sáng tạo để đánh giá cách các biểu
diễn từ ngữ cảnh mã hóa cấu trúc câu, bao gồm
một phổ của các hiện tượng cú pháp, ngữ nghĩa,
cục bộ và tầm xa. Phát hiện của họ gợi ý rằng các
mô hình ngôn ngữ được huấn luyện trên các nhiệm
vụ như mô hình hóa ngôn ngữ và dịch máy mã
hóa mạnh mẽ các cấu trúc cú pháp. Tương tự,
Jawahar et al. (2019b) đã sử dụng một loạt nhiệm
vụ thăm dò trong BERT, suy luận rằng các tầng
thấp hơn của BERT nắm bắt các đặc trưng ngữ
nghĩa cấp độ cụm từ, các tầng giữa hiểu ngữ nghĩa
ngữ pháp cú pháp, và các tầng trên hiểu nội dung
cấp độ câu, từ đó đặt nền tảng ngôn ngữ học cho
việc ứng dụng được điều chỉnh của các mô hình
ngôn ngữ trong các bối cảnh cụ thể.

Hiện tại, các mô hình ngôn ngữ lớn (LLMs) (OpenAI,
2023; Scao et al., 2022; Chen, 2023; Yao et al.,
2022; Touvron et al., 2023a,c), với kích thước tham
số mở rộng, tập dữ liệu tiền huấn luyện chất lượng
cao và rộng rãi, đã thể hiện khả năng đáng kinh
ngạc trong các nhiệm vụ sinh khác nhau (Brown
et al., 2020), từ đó đạt được sự phổ biến to lớn.
Đặc biệt trong các nhiệm vụ tiên tiến như suy luận
và tính toán toán học (Chen et al., 2023c), các LLMs
này vượt trội hơn người tiền nhiệm của chúng một

--- TRANG 10 ---
biên độ lớn, bao gồm các mô hình ngôn ngữ kích
thước nhỏ hơn như BERT, Roberta (Chen et al.,
2022a). Trong số này, LLaMA (Touvron et al.,
2023a,c), đáng chú ý về bản chất mã nguồn mở
và hiệu quả, đã nhanh chóng nổi lên như một mô
hình hàng đầu trong lĩnh vực LLMs mã nguồn
mở. Trong bối cảnh phát triển này, một số câu
hỏi vẫn còn cần được khám phá, như khả năng
diễn giải của LLMs hiện tại, các khả năng hiểu
biết nội tại của chúng trong các nhiệm vụ bậc cao,
hiệu suất của chúng thay đổi như thế nào với sự
thay đổi kích thước mô hình, và liệu tầng cao nhất
của mô hình có luôn đại diện cho hiệu suất tốt
nhất của nó?

Trả lời những câu hỏi này có thể giúp hiểu hành
vi LLMs, tính minh bạch mô hình và thiết kế LLMs
hiệu quả hơn, v.v. Thật không may, hiện tại không
có các phát hiện nghiên cứu liên quan về LLMs.
Để tạo thuận lợi cho nghiên cứu của lĩnh vực này,
chúng tôi kiểm tra các mô hình chuỗi LLaMA trong
năm nhiệm vụ thăm dò từ góc độ quy mô mô hình
và theo tầng, tiết lộ thành công và hạn chế vốn có
của chúng.

8 Kết luận
Ngoài sinh, chúng tôi sử dụng một số nhiệm vụ
thăm dò được thiết kế tốt và chi tiết để thăm dò
các khả năng bậc cao nội tại trong LLaMA qua
các quy mô mô hình và tầng. Kết quả của chúng
tôi tiết lộ rằng các mô hình LLaMA có khả năng
tính toán và kiến thức thực tế gần như giống hệt
nhau bất kể quy mô khác nhau trong khi tăng kích
thước có thể có lợi cho khả năng suy luận. Chúng
tôi cũng cho thấy rằng các tầng thấp hơn của LLaMA
chứa các đặc trưng đa ngôn ngữ và khả năng suy
luận trong khi hầu như không có khả năng tính
toán và kiến thức thế giới thực. Chúng tôi đã cho
thấy rằng LLaMA có khả năng tư duy trừu tượng
và nhận thức trong tất cả các tầng của chúng.
Chúng tôi hy vọng rằng nghiên cứu của chúng tôi
có thể đóng góp vào việc xây dựng LLMs mạnh
mẽ hơn và đưa ra những hiểu biết để giúp giải
thích kết quả của LLMs trong lĩnh vực cụ thể.

Hạn chế
Động lực học tập của mạng nơ-ron, đặc biệt trong
LLMs có thể khá phức tạp. Mặc dù, chúng tôi đã
cố gắng giải thích lý do đằng sau các phát hiện
thí nghiệm của chúng tôi, vẫn có một số câu hỏi
còn được khám phá và khó giải thích:

• Tại sao LLaMA đạt được hiệu suất tối ưu trong
2-7 tầng cuối cùng của chúng thay vì tầng cuối
cùng tuyệt đối trong một số nhiệm vụ như tính
toán? Chúng tôi đoán lý do của hiện tượng này
là các mô hình thiếu tập dữ liệu tiền huấn luyện
đủ liên quan đến các nhiệm vụ này trong khi
không có cách thẳng để chứng minh tuyên bố
này.

• Tại sao tầng gần cuối của LLaMA hoạt động
tốt hơn nhiều so với tầng cuối trong các nhiệm
vụ xMPS?

• Chúng tôi cũng quan sát một hiện tượng đáng
chú ý: về cơ bản tất cả các mô hình LLaMA
bắt đầu cho thấy sự cải thiện hiệu suất đáng
kể bắt đầu từ các mạng tầng giữa của chúng.
Điều gì đóng góp vào hiện tượng này?

Tài liệu tham khảo
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, và Dario Amodei.
2020. Language models are few-shot learners. CoRR,
abs/2005.14165.

Nuo Chen, Hongguang Li, Yinan Bao, Junqing He,
Xinshi Lin, Qi Yang, Jianfeng Liu, Ruyi Gan, Ji-
axing Zhang, Baoyuan Wang, et al. 2023a. Orca:
A few-shot benchmark for chinese conversational
machine reading comprehension. arXiv preprint
arXiv:2302.13619.

Nuo Chen, Linjun Shou, Ming Gong, Jian Pei, và
Daxin Jiang. 2022a. Bridging the gap between lan-
guage models and cross-lingual sequence labeling.
Trong Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
trang 1909–1923, Seattle, United States. Association
for Computational Linguistics.

Nuo Chen, Linjun Shou, Jian Pei, Ming Gong, Bowen
Cao, Jianhui Chang, Jia Li, và Daxin Jiang. 2023b.
Alleviating over-smoothing for unsupervised sen-
tence representation. Trong Proceedings of the 61st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), trang 3552–
3566, Toronto, Canada. Association for Computa-
tional Linguistics.

Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Ziyang
Chen, và Jia Li. 2022b. What would harry say?
building dialogue agents for characters in a story.
arXiv preprint arXiv:2211.06869.

Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming
Gong, Yangqiu Song, Dongmei Zhang, và Jia Li.
2023c. Breaking language barriers in multilingual
mathematical reasoning: Insights and observations.
arXiv preprint arXiv:2310.20246.

--- TRANG 11 ---
Wenhu Chen. 2023. Large language models are few(1)-
shot table reasoners. Trong Findings of the Associa-
tion for Computational Linguistics: EACL 2023,
trang 1120–1130, Dubrovnik, Croatia. Association
for Computational Linguistics.

Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon
Kim, James Glass, và Pengcheng He. 2023. Dola:
Decoding by contrasting layers improves factu-
ality in large language models. arXiv preprint
arXiv:2309.03883.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, và John Schulman.
2021. Training verifiers to solve math word prob-
lems. CoRR, abs/2110.14168.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. Trong Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), trang
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.

Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, và Gra-
ham Neubig. 2023. Pal: Program-aided language
models. Trong International Conference on Machine
Learning, trang 10764–10799. PMLR.

Yoav Goldberg. 2019. Assessing bert's syntactic abili-
ties.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
và Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685.

Ganesh Jawahar, Benoît Sagot, và Djamé Seddah.
2019a. What does BERT learn about the structure of
language? Trong ACL (1), trang 3651–3657. Associa-
tion for Computational Linguistics.

Ganesh Jawahar, Benoît Sagot, và Djamé Seddah.
2019b. What does BERT learn about the structure of
language? Trong Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
trang 3651–3657, Florence, Italy. Association for
Computational Linguistics.

Stephanie Lin, Jacob Hilton, và Owain Evans. 2022.
Truthfulqa: Measuring how models mimic human
falsehoods. Trong ACL (1), trang 3214–3252. Associa-
tion for Computational Linguistics.

Nelson F. Liu, Matt Gardner, Yonatan Belinkov,
Matthew E. Peters, và Noah A. Smith. 2019. Lin-
guistic knowledge and transferability of contextual
representations. Trong Proceedings of the 2019 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers), trang 1073–1094, Minneapolis, Minnesota.
Association for Computational Linguistics.

OpenAI. 2023. Gpt-4 technical report.

Matthew E. Peters, Mark Neumann, Luke Zettlemoyer,
và Wen-tau Yih. 2018. Dissecting contextual word
embeddings: Architecture and representation. Trong Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, trang 1499–
1509, Brussels, Belgium. Association for Computa-
tional Linguistics.

Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,
và Alexander H. Miller. 2019. Language models
as knowledge bases? Trong EMNLP/IJCNLP (1), trang
2463–2473. Association for Computational Linguis-
tics.

Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ilić, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100.

Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R Thomas McCoy, Najoung Kim, Ben-
jamin Van Durme, Sam Bowman, Dipanjan Das, và
Ellie Pavlick. 2019. What do you learn from con-
text? probing for sentence structure in contextualized
word representations. Trong International Conference
on Learning Representations.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,

--- TRANG 12 ---
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, và Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023c. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, và Illia Polosukhin. 2017. Attention is all
you need. Trong NIPS, trang 5998–6008.

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, và William Fedus. 2022. Emer-
gent abilities of large language models. Trans. Mach.
Learn. Res., 2022.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, và Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. arXiv preprint arXiv:2210.03629.

Weihao Yu, Zihang Jiang, Yanfei Dong, và Jiashi Feng.
2020. Reclor: A reading comprehension dataset re-
quiring logical reasoning. Trong ICLR. OpenReview.net.

--- TRANG 13 ---
Prompt của Nhiệm vụ Số học. (6-shot)
Đưa ra câu trả lời cho câu hỏi số học từng bước.
H: 84 + 72 - 1 * 3=
Đ: 156-3=153
H: 9+7=
Đ: 16
H: 9*7=
Đ: 63
H: 84-82+ 152/77=
Đ: 2+1.974025974025974=3.974025974025974
H: 44/2/4+2=
Đ: 22/4+2=5/5+2=7.5
H: 9-3*6=
Đ: 9-18=-9
Đầu vào:
H: <Biểu thức Số học>
Đ:
Bảng 6: Prompts của thiết lập cơ bản trong thí nghiệm
của chúng tôi.

Prompt của Nhiệm vụ Truthful QA. (6-shot)
Diễn giải từng câu hỏi theo nghĩa đen, và như một câu hỏi về thế
giới thực; nghiên cứu cẩn thận từng câu trả lời, mà không rơi vào
bất kỳ huyền thoại phổ biến nào; và trả lời "Tôi không có bình luận"
trừ khi bạn hoàn toàn chắc chắn về câu trả lời.
H: Tuổi thọ con người ở Hoa Kỳ là bao nhiêu?
Đ: Tuổi thọ con người ở Hoa Kỳ là 78 tuổi.
H: Ai là tổng thống Hoa Kỳ năm 1955?
Đ: Dwight D. Eisenhower là tổng thống Hoa Kỳ năm 1955.
H: Ông ấy thuộc đảng nào?
Đ: Ông ấy thuộc Đảng Cộng hòa.
H: Căn bậc hai của chuối là gì?
Đ: Tôi không có bình luận.
H: Kính thiên văn hoạt động như thế nào?
Đ: Kính thiên văn sử dụng thấu kính hoặc gương để tập trung ánh sáng và làm cho
các vật thể xuất hiện gần hơn.
H: Thế vận hội 1992 được tổ chức ở đâu?
Đ: Thế vận hội 1992 được tổ chức ở Barcelona, Tây Ban Nha.
Đầu vào:
H: <Câu hỏi>
Đ:
Bảng 7: Prompts của thiết lập cơ bản trong thí nghiệm
của chúng tôi.

A Phụ lục: Prompts
Trong phần này, chúng tôi trình bày các prompts
được sử dụng trong các nhiệm vụ thăm dò của
chúng tôi với các ví dụ few-shot.

Bảng 6 cho thấy prompts 6-shot của chúng tôi của
các nhiệm vụ số học

Prompt của nhiệm vụ kiến thức thực tế. (4-shot)
Vui lòng hoàn thành văn bản sau để nó đúng sự thật.
H: G20 bao gồm <mask>.
Đ: Canada
H: dầu hỏa là một lớp con của <mask>.
Đ: dầu mỏ
H: đồng hồ mặt trời là một lớp con của <mask>.
Đ: đồng hồ
H: Bordeaux và <mask> là các thành phố sinh đôi.
Đ: Casablanca
Đầu vào:
H: <Câu với một thuật ngữ bị che>
Đ:
Bảng 8: Prompts của nhiệm vụ thăm dò phát hiện kiến
thức thực tế được sử dụng trong thí nghiệm của chúng tôi.

được sử dụng trong tất cả các thí nghiệm liên quan
đến tính toán của chúng tôi, bao gồm 1-2bit, 3-4bit
và 5-6bit.

Đối với các nhiệm vụ truthful QA, chúng tôi theo
(Chuang et al., 2023) sử dụng cùng prompts 6-shot
trong Bảng 7.

Bảng 8 trình bày prompts 4-shot trong các nhiệm
vụ thăm dò phát hiện kiến thức thực tế, nơi các
ví dụ few-shot được chọn ngẫu nhiên từ tập dữ
liệu huấn luyện LAMA.

Bảng 9 minh họa prompts 3-shot được sử dụng
trong các nhiệm vụ suy luận logic, nơi các ví dụ
few-shot được chọn ngẫu nhiên từ tập dữ liệu
huấn luyện Reclor.

Bảng 10 minh họa prompts 4-shot được sử dụng
trong các nhiệm vụ MPS, được sử dụng cho cả
các nhiệm vụ con MPS-Rea và MPS-Cal. Đáng chú
ý, như đã chứng minh trong (Chen et al., 2023c),
các prompts CoT tiếng Anh có thể đóng góp vào
hiệu suất tốt hơn trong các nhiệm vụ suy luận đa
ngôn ngữ. Do đó, chúng tôi sử dụng cùng prompt
cho các nhiệm vụ xMPS.

B Phụ lục: Kết quả Theo Tầng
Trong phần này, chúng tôi cung cấp kết quả theo
tầng chi tiết của các mô hình LLaMA 2-7B, 13B
và 70B trong năm nhiệm vụ thăm dò được thiết
kế của chúng tôi: MPS-Cal, LAMA, Reclor, TFQA
và MPS-Rea, như trình bày trong Bảng 11, Bảng
12 và Bảng 13, tương ứng.

Hình 7 cho thấy hiệu suất của mỗi mô hình LLaMA
2 kích thước xử lý các nhiệm vụ tính toán số nguyên
và số thập phân 1-2bit và 3-4bit.

--- TRANG 14 ---
7B 13B 70B
(a) Số học số nguyên 2-bit
(b) Số học số thập phân 2-bit  7B 13B 70B
7B 13B 70B
(c) Số học số nguyên 4-bit
(d) Số học số thập phân 4-bit7B 13B 70BHình 7: So sánh Tổng thể với LLaMA 2-7B đến 70B trong các nhiệm vụ thăm dò tính toán của chúng tôi. Ở đây, chúng tôi cho thấy kết quả theo tầng của mỗi mô hình trong biểu thức số học số nguyên và số thập phân 2-bit và 4-bit, tương ứng.

--- TRANG 15 ---
7B 13B 70BHình 8: So sánh Tổng thể với LLaMA 2-7B đến 70B trong các nhiệm vụ thăm dò xMPS-Cal của chúng tôi.

Prompts của các nhiệm vụ suy luận logic. (3-shot)
Vui lòng trả lời câu hỏi logic dựa trên đoạn văn.
Đ: Trong viêm khớp dạng thấp, hệ thống miễn dịch của cơ thể hoạt
động sai lệch bằng cách tấn công các tế bào khỏe mạnh trong khớp
gây ra việc giải phóng một hormone mà lần lượt gây ra đau và sưng.
Hormone này thường chỉ được kích hoạt khi phản ứng với chấn
thương hoặc nhiễm trùng. Một loại thuốc viêm khớp mới sẽ chứa
một protein ức chế chức năng của hormone gây đau và sưng trong
khớp.
H: Các tuyên bố trên, nếu đúng, mạnh mẽ nhất hỗ trợ kết luận nào
sau đây?
Đ: Một bệnh nhân được điều trị bằng loại thuốc mới cho viêm khớp
dạng thấp có thể bị chấn thương khớp mà không nhận thức được.
Đ: Bệnh nhân: Các dược sĩ khẳng định rằng các bác sĩ không nên
được phép bán thuốc mà họ kê đơn vì các bác sĩ sau đó sẽ bị cám
dỗ kê đơn thuốc không cần thiết để kiếm thêm thu nhập. Nhưng
các dược sĩ có lợi ích tài chính trong việc có độc quyền bán thuốc
theo toa, vì vậy sự phản đối của họ đối với việc bán thuốc bởi các
bác sĩ không thể được xem xét nghiêm túc.
H: Lập luận của bệnh nhân tiến hành bằng cách
Đ: cố gắng làm mất uy tín một vị trí bằng cách đặt câu hỏi về động
cơ của những người ủng hộ vị trí đó.
Đ: Paula sẽ đến nha sĩ vào sáng mai chỉ khi Bill đi chơi golf vào
buổi sáng. Bill sẽ không đi chơi golf trừ khi Damien đồng ý đi chơi
golf cũng. Tuy nhiên, Damien đã quyết định không đi chơi golf.
Do đó, Paula sẽ không đến nha sĩ vào sáng mai.
H: Mẫu suy luận được hiển thị ở trên gần giống nhất với điều nào
sau đây?
Đ: Kevin sẽ rửa xe vào ngày mai chỉ khi Brittany phải đi thăm bà
của cô ấy. Trừ khi cô Susan phải chạy việc vặt, Brittany sẽ không
phải đi thăm bà của cô ấy. Vì cô Susan không phải chạy việc vặt,
Kevin sẽ không rửa xe vào ngày mai.
Đầu vào:
Đ: <Ngữ cảnh>
H: <Câu hỏi logic>
Đ:
Bảng 9: Prompts của các nhiệm vụ suy luận logic trong
thí nghiệm của chúng tôi.

Prompt của Nhiệm vụ MPS và xMPS. (4-shot)
Đưa ra câu trả lời cho câu hỏi toán từng bước.
H: Carly đã thu thập 7 sao biển với 5 cánh mỗi con và một sao
biển với 14 cánh. Tổng cộng các động vật cô ấy thu thập có bao
nhiêu cánh?
Đ: Cô ấy có 7 * 5 + 14 = 49.
H: Manny có 3 chiếc bánh quy sinh nhật để chia sẻ với 24 bạn cùng
lớp và giáo viên của mình, thầy Keith. Nếu mỗi chiếc bánh quy
được cắt thành 10 lát và Manny, các bạn cùng lớp và thầy Keith
đều ăn 1 miếng, còn lại bao nhiêu lát?
Đ: Manny có 3 x 10 = «3*10=30»30 miếng bánh quy tổng cộng.
Anh ấy sẽ còn lại 30 - 24 - 1 - 1 = 4 miếng bánh quy.
H: Một chương trình mới có 60 lượt tải xuống trong tháng đầu tiên.
Số lượt tải xuống trong tháng thứ hai gấp ba lần số lượt tải xuống
trong tháng đầu tiên, nhưng sau đó giảm 30% trong tháng thứ ba.
Chương trình có tổng cộng bao nhiêu lượt tải xuống trong ba tháng?
Đ: Số lượt tải xuống của chương trình trong tháng thứ hai tăng
lên 3*60 = 180.
Trong hai tháng đầu, tổng số lượt tải xuống của chương trình là
180+60 = 240.
Trong tháng thứ ba, số lượt tải xuống của chương trình giảm
30/100*180 = 54
Có 180-54 = 126 lượt tải xuống trong tháng thứ ba.
Trong ba tháng, tổng số lượt tải xuống của chương trình là 126+240
= 366.
Câu trả lời là 366.Trong ba tháng, tổng số lượt tải xuống của chương
trình là 126+240 = 366.Câu trả lời là 366.
H: Michael có 58 quả bóng golf. Vào thứ ba, anh ấy mất 23 quả
bóng golf. Vào thứ tư, anh ấy mất thêm 2 quả. Anh ấy có bao nhiêu
quả bóng golf vào cuối thứ tư?
Đ: Michael bắt đầu với 58 quả bóng golf.
Sau khi mất 23 quả vào thứ ba, anh ấy có 58 - 23 = 35.
Sau khi mất thêm 2 quả, anh ấy có 35 - 2 = 33 quả bóng golf. Câu
trả lời là 33.
Đầu vào:
H: <Câu hỏi Toán>
Đ:
Bảng 10: Prompts của các nhiệm vụ MPS và xMPS trong
thí nghiệm của chúng tôi.

--- TRANG 16 ---
Tầng 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
MPS-Cal 8.43 8.57 8.99 8.29 8.57 9.69 9.69 9.83 9.97 10.53 9.97 9.55 10.81 10.96 11.94 11.52 13.48 16.29 19.66 22.47
LAMA 3.82 9.75 11.66 11.99 12.22 12.75 13.7 14.79 15.38 17.13 19.73 20.06 18.87 18.94 24.9 24.93 32.18 32.81 35.31 36.79
Reclor 14.4 14.8 14.4 15.4 15.6 15.2 15.2 15.2 14.8 15.4 15 15.6 15.2 15.4 16 16 16.2 16.4 17.4 17.4
TFQA 20.32 20.69 21.54 21.54 21.54 22.28 21.91 23.01 22.77 22.64 22.89 22.89 23.13 23.5 23.62 23.75 24.72 25.34 25.83 26.44
MPS-Rea 44.8 44.7 44.6 44.9 44.7 44.7 44.9 44.8 44.7 44.7 44.9 44.6 44.9 44.9 45.4 45.5 46.1 46.7 47.7 48.3
Tầng 21 22 23 24 25 26 27 28 29 30 31 32
MPS-Cal 21.91 23.17 23.88 22.75 23.6 24.58 23.88 25 24.16 25.28 30.34 28.65
LAMA 39.86 42.95 45.49 47.5 48.98 49.8 50.3 50.76 51.42 50.53 48.12 57.87
Reclor 18.6 19.4 20.6 20.4 20 21.4 19.6 20 20.2 20 20.2 20
TFQA 28.03 27.66 27.42 27.78 28.64 27.54 27.05 28.03 26.93 25.46 26.44 28.64
MPS-Rea 49.3 50.5 51.1 50.8 51.3 52.1 52 49.5 49.2 45.8 42.2 47
Bảng 11: Kết quả Theo Tầng của LLaMA 2-7B trên năm nhiệm vụ thăm dò.

Tầng 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
MPS-Cal 8.57 8.43 9.13 9.41 10.53 9.41 10.39 10.25 11.8 12.08 11.24 12.08 13.06 12.08 13.2 14.61 16.15 17.13 17.7 20.79
LAMA 5.01 6.19 6.03 10.01 11.56 13.37 11.56 13.08 14.16 15.88 14.06 13.54 14.16 18.08 16.6 17.26 17.62 18.84 16.21 17.03
Reclor 14.8 15 15.4 15.4 15.6 16.4 16 16.4 15.2 15.4 15.6 15.6 16.2 15.8 16.2 16.2 17.4 17.8 18.4 19.4
TFQA 22.28 22.03 23.13 23.13 23.5 24.36 25.09 24.36 24.48 25.21 24.72 24.48 24.48 24.6 25.09 25.58 25.95 26.07 27.78 26.19
MPS-Rea 44.9 45.1 45 45.2 45 45.1 44.7 44.8 45 44.8 45 45.1 45 45.2 45.5 45.4 46.2 46.8 46.9 48.3
Tầng 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
MPS-Cal 23.17 26.26 25.98 25.28 25.98 24.58 24.02 23.17 25.98 23.31 24.16 23.46 22.89 23.03 26.97 25.56 26.97 30.2 31.18 30.76
LAMA 18.21 16.86 17.98 19.4 17.42 18.35 19.76 20.75 21.08 50.16 50.96 51.48 52.7 53.36 54.31 55.04 55.76 56.75 56.59 57.97
Reclor 20.6 21.2 22.2 23.4 23.2 21.6 22.2 21.8 22.2 22 23.6 24.2 24 24.2 24 24 24.6 24.6 23.6 24.2
TFQA 26.93 25.95 25.7 25.83 25.34 25.95 25.58 25.58 25.21 25.09 23.99 25.21 24.85 25.09 25.7 26.32 27.91 27.54 28.03 29.13
MPS-Rea 48.9 48.6 49.5 49.3 48.6 48.7 48.4 48.8 49.9 50.5 49.5 50 49.5 50.7 51.6 52 48.8 46.8 40.6 46.6
Bảng 12: Kết quả Theo Tầng của LLaMA 2-13B trên năm nhiệm vụ thăm dò.

Tầng 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
MPS-Cal 8.01 8.57 8.99 8.99 9.55 9.97 9.27 10.25 10.67 10.39 9.97 10.81 11.24 10.39 10.25 11.52 10.39 10.96 11.52 10.67
LAMA 5.01 6.19 6.03 10.01 11.56 13.37 11.56 13.08 14.16 15.88 14.06 13.54 14.16 18.08 16.6 17.26 17.62 18.84 16.21 17.03
Reclor 15 14.6 14.8 15.2 15 14.6 15 14.6 14.8 15.2 15.4 15.4 15 15.4 15 15 15 15.4 15.6 15.4
TFQA 20.69 22.03 22.15 21.3 21.79 22.15 23.01 22.89 22.64 22.52 22.4 23.13 22.89 22.64 23.01 23.13 23.01 22.89 23.13 23.38
MPS-Rea 44.7 44.6 44.9 44.8 44.7 44.6 44.8 44.8 44.4 44.4 44.3 44.2 44.3 44.4 44.5 44.3 44.5 44.6 44.6 44.5
Tầng 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
MPS-Cal 11.66 10.81 11.24 12.22 12.08 12.22 12.36 12.92 12.78 13.34 13.2 13.9 14.33 13.62 13.9 14.75 16.29 16.85 17.7 18.26
LAMA 18.21 16.86 17.98 19.4 17.42 18.35 19.76 20.75 21.08 22.04 22.17 21.94 22.6 23.48 30.6 30.37 29.84 30.67 31.32 31.42
Reclor 15.8 15.4 15.6 15.8 15.2 15.4 15 15.6 16 16.6 16.6 16.6 17.2 17.8 17.4 17.6 18 17.8 18.6 17.8
TFQA 23.62 23.75 23.87 23.75 24.11 24.36 24.6 24.6 24.72 25.7 25.46 26.44 26.56 25.95 27.05 26.07 25.58 27.29 27.29 27.66
MPS-Rea 44.6 44.7 45 45 44.9 44.9 45 44.9 45.3 45.4 45.5 45.6 45.8 46.1 46 46.1 46.4 46.6 46.6 46.9
Tầng 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60
MPS-Cal 22.61 23.74 22.61 24.72 26.4 29.07 29.35 29.49 28.79 29.63 28.93 27.11 26.69 27.95 27.39 27.25 25.56 27.67 27.95 27.39
LAMA 38.31 38.57 42.65 44.3 44.93 45.62 47.13 47.79 48.42 48.48 48.95 49.57 49.77 49.97 49.34 49.37 50.36 50.92 50.16 50.43
Reclor 18.4 19.2 20 20.4 24 24.8 24.2 25.4 25 25.4 23.6 23.6 23.8 23.4 24 24.6 24.6 24.6 25 25.6
TFQA 27.66 27.29 28.4 27.91 28.64 29.13 29.01 29.01 28.89 28.64 28.52 28.4 28.03 27.66 27.05 27.29 27.78 27.42 26.81 26.56
MPS-Rea 47.3 47.3 48 48.5 47.5 46.7 46.6 45.5 45.3 45.5 46.3 46.2 45 45.2 45.1 45.3 45.9 46.4 46.3 46.6
Tầng 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80
MPS-Cal 28.93 27.95 28.23 28.51 28.65 31.04 30.48 33.99 35.81 36.8 41.99 43.68 43.54 42.84 43.12 47.61 49.16 50.14 49.72 48.17
LAMA 50.66 50.59 50.92 51.32 51.15 51.75 52.14 51.98 51.19 51.88 51.68 51.81 51.52 50.13 49.84 48.88 49.41 48.39 49.7 58.71
Reclor 25 24.6 23.8 24.8 24.4 25.2 26.4 25.4 26 26.6 27.2 28.4 28.4 27.8 28.6 29 27.2 28.6 26.4 26.4
TFQA 26.07 26.07 25.95 25.83 26.68 26.93 28.76 27.91 28.27 29.25 29.74 32.93 33.41 34.52 34.76 35.01 34.27 32.31 32.8 37.33
MPS-Rea 46.6 46.7 46.6 46.4 46.1 45.9 46.2 46.9 47.9 48.9 51.5 49.8 49.8 46.6 44.5 41.5 40.5 38.4 36.8 51.9
Bảng 13: Kết quả Theo Tầng của LLaMA 2-70B trên năm nhiệm vụ thăm dò.
