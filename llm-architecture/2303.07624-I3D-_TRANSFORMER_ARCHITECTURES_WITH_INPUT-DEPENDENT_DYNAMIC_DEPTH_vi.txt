Based on the PDF content provided, I'll translate this academic paper to Vietnamese while maintaining the exact structure:

# 2303.07624.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/llm-architecture/2303.07624.pdf
# Kích thước tệp: 383130 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
I3D: KIẾN TRÚC TRANSFORMER VỚI ĐỘ SÂU ĐỘNG PHỤ THUỘC ĐẦU VÀO
CHO NHẬN DẠNG GIỌNG NÓI

Yifan Peng1, Jaesong Lee2, Shinji Watanabe1
1Đại học Carnegie Mellon 2NA VER Corporation

TÓM TẮT
Nhận dạng giọng nói đầu cuối đến cuối dựa trên Transformer đã đạt được thành công lớn. Tuy nhiên, chi phí lưu trữ lớn và tải tính toán làm cho việc triển khai các mô hình này trong một số ứng dụng thực tế trở nên khó khăn. Các kỹ thuật nén mô hình có thể giảm kích thước mô hình và tăng tốc suy luận, nhưng mô hình nén có kiến trúc cố định có thể không tối ưu. Chúng tôi đề xuất một bộ mã hóa Transformer mới với Độ sâu Động phụ thuộc Đầu vào (I3D) để đạt được sự cân bằng hiệu suất-hiệu quả mạnh mẽ. Với số lượng lớp tương tự tại thời điểm suy luận, các mô hình dựa trên I3D vượt trội hơn Transformer gốc và mô hình cắt tỉa tĩnh thông qua cắt tỉa lớp lặp. Chúng tôi cũng trình bày phân tích thú vị về xác suất cổng và sự phụ thuộc đầu vào, giúp chúng ta hiểu rõ hơn về các bộ mã hóa sâu.

Từ khóa chỉ mục — Độ sâu động, transformer, nhận dạng giọng nói

1. GIỚI THIỆU
Gần đây, nhận dạng giọng nói tự động đầu cuối đến cuối (ASR) đã trở nên phổ biến. Các framework tiêu biểu bao gồm Phân loại Thời gian Kết nối (CTC) [1], Bộ mã hóa-Giải mã dựa trên Chú ý (AED) [2–4], và Bộ chuyển đổi Mạng nơ-ron Hồi quy (RNN-T) [5]. Nhiều loại mạng có thể được sử dụng làm bộ mã hóa trong các framework này, như Mạng nơ-ron Tích chập (CNNs), RNNs, Transformers [6] và sự kết hợp của chúng [7–9]. Transformers đã đạt được thành công lớn trong các benchmark khác nhau [10]. Tuy nhiên, chúng thường chứa nhiều khối nối tiếp và do đó có tính toán cao, điều này cản trở việc triển khai trong một số ứng dụng thực tế với tài nguyên hạn chế. Để giảm tính toán và tăng tốc suy luận, các nhà nghiên cứu đã điều tra các phương pháp khác nhau.

Một phương pháp phổ biến là nén một mô hình lớn được huấn luyện trước bằng cách sử dụng chưng cất [11–13], cắt tỉa [14–16], và lượng tử hóa [15]. Tuy nhiên, mô hình nén có kiến trúc cố định cho tất cả các loại đầu vào, có thể không tối ưu. Ví dụ, mô hình cố định này có thể quá tốn kém cho các phát ngôn rất dễ nhưng không đủ cho những phát ngôn khó. Để cân bằng tốt hơn hiệu suất và tính toán, các nghiên cứu trước đây đã khám phá các mô hình động [17], có thể thích ứng kiến trúc của chúng với các đầu vào khác nhau. Các mô hình động đã được chứng minh là hiệu quả trong thị giác máy tính [18–23], chủ yếu dựa trên CNNs. Đối với xử lý giọng nói, [24] huấn luyện hai bộ mã hóa RNN có kích thước khác nhau và chuyển đổi động giữa chúng được hướng dẫn bởi phát hiện từ khóa. [25] đề xuất một bộ chuyển đổi mã hóa động dựa trên dropout lớp và học tập hợp tác. [26] áp dụng hai bộ mã hóa RNN để xử lý giọng nói gần và xa. [27] cũng thiết kế hai bộ mã hóa RNN được nén ở các mức độ khác nhau và chuyển đổi giữa chúng trên cơ sở từng khung hình. [28] mở rộng ý tưởng này cho Transformer-transducers và xem xét các mạng con linh hoạt hơn, nhưng nó vẫn tập trung vào ASR streaming và kiến trúc vẫn được xác định trên cơ sở từng khung hình, đòi hỏi thiết kế đặc biệt cho các hoạt động key và query chi tiết trong self-attention. Đối với ASR không streaming (hoặc streaming dựa trên chunk), dự đoán cấp khung hình có thể tốn kém và không tối ưu, vì nó chỉ nắm bắt các đặc trưng cục bộ cấp khung hình.

Chúng tôi đề xuất một bộ mã hóa Transformer với Độ sâu Động phụ thuộc Đầu vào (I3D) cho ASR đầu cuối đến cuối. Thay vì sử dụng các hoạt động chi tiết được thiết kế cẩn thận trong các mô-đun phụ như attention, I3D dự đoán liệu có bỏ qua toàn bộ khối self-attention hoặc toàn bộ mạng feed-forward thông qua một loạt các bộ dự đoán cổng cục bộ hoặc một bộ dự đoán cổng toàn cục duy nhất. Dự đoán được thực hiện ở cấp phát ngôn (hoặc cấp chunk nếu mở rộng cho trường hợp streaming), dễ thực hiện hơn và giảm chi phí bổ sung. Nó cũng nắm bắt thống kê toàn cục của đầu vào. Như được phân tích trong Phần 3.4, độ dài của một phát ngôn ảnh hưởng đến kiến trúc suy luận. Một số khối có thể hữu ích cho các đầu vào dài hơn. Kết quả cho thấy các mô hình I3D liên tục vượt trội hơn Transformers được huấn luyện từ đầu và các mô hình cắt tỉa tĩnh thông qua cắt tỉa lớp lặp [29], khi sử dụng số lượng lớp tương tự cho suy luận. Chúng tôi cũng thực hiện phân tích thú vị về xác suất cổng dự đoán và sự phụ thuộc đầu vào, giúp chúng ta hiểu rõ hơn hành vi của các bộ mã hóa sâu.

2. PHƯƠNG PHÁP

2.1. Bộ mã hóa Transformer

Một lớp bộ mã hóa Transformer [6] chứa một mô-đun self-attention đa đầu (MHA) và một mạng feed-forward (FFN), được kết hợp tuần tự. Hàm của lớp thứ l như sau:

Y(l)=X(l−1)+MHA(l)(X(l−1)); (1)
X(l)=Y(l)+FFN(l)(Y(l)); (2)

trong đó X(l) là đầu ra của lớp Transformer thứ l và X(l−1) do đó là đầu vào của lớp thứ l. Y(l) là đầu ra của MHA tại lớp thứ l, cũng là đầu vào của FFN tại lớp thứ l. Các chuỗi này đều có độ dài T và kích thước đặc trưng d.

2.2. Kiến trúc tổng thể của các bộ mã hóa I3D

Hình 1a cho thấy kiến trúc tổng thể của các bộ mã hóa I3D. Một dạng sóng đầu tiên được chuyển đổi thành một chuỗi đặc trưng bởi một frontend, và được xử lý thêm và downsampling bởi một CNN, sau đó các embedding vị trí được thêm vào. Sau đó, chuỗi được xử lý bởi một stack gồm N lớp bộ mã hóa I3D để tạo ra các đặc trưng cấp cao. Thiết kế tổng thể này tuân theo của Transformer. Tuy nhiên, Transformer luôn sử dụng kiến trúc cố định bất kể đầu vào. I3D của chúng tôi chọn các kết hợp khác nhau của MHA và FFN tùy thuộc vào phát ngôn đầu vào. Để xác định liệu một mô-đun có nên được thực thi hay bỏ qua, một cổng nhị phân được giới thiệu cho mỗi mô-đun MHA hoặc FFN. Hàm

arXiv:2303.07624v1 [cs.CL] 14 Mar 2023

--- TRANG 2 ---

Frontend CNN Positional Embedding Audio Waveform I3D Encoder Layer×𝑁 Encoder Output Sequence Encoder Input Sequence (a) Kiến trúc bộ mã hóa tổng thể.

Multi-Head Self-Attention Feed-Forward Network …Mean Local Gate Predictor I3D Encoder Layer (b) Lớp bộ mã hóa I3D với bộ dự đoán cổng cục bộ.

Multi-Head Self-Attention Feed-Forward Network …Mean Global Gate Predictor Layer 1 Layer 𝑁×𝑁… Encoder Input Sequence Encoder Output Sequence (c) Bộ mã hóa I3D với bộ dự đoán cổng toàn cục.

Hình 1: Kiến trúc của các bộ mã hóa I3D được đề xuất của chúng tôi.

của lớp thứ l (xem Eqs. (1) và (2) cho Transformer gốc) bây giờ trở thành:

Y(l)=X(l−1)+g(l)MHA MHA(l)(X(l−1)); (3)
X(l)=Y(l)+g(l)FFN FFN(l)(Y(l)); (4)

trong đó g(l)MHA; g(l)FFN ∈ {0;1} là các cổng phụ thuộc đầu vào. Nếu một cổng được dự đoán là 0, thì mô-đun tương ứng sẽ bị bỏ qua, điều này có hiệu quả giảm tính toán. Tổng loss huấn luyện là:

Ltotal=LASR+λLutility; (5)
Lutility=1/2N ∑N l=1 (g(l)MHA+g(l)FFN); (6)

trong đó LASR là loss ASR tiêu chuẩn và Lutility là loss regularization đo tỷ lệ sử dụng của tất cả các mô-đun MHA và FFN. λ > 0 là một siêu tham số để cân bằng độ chính xác nhận dạng và chi phí tính toán.¹ Lưu ý rằng loss utility trong Eq. (6) được định nghĩa cho một phát ngôn cá nhân nên chỉ số phát ngôn bị bỏ qua. Trong thực tế, một mini-batch được sử dụng và loss được trung bình hóa trên các phát ngôn.

Một vấn đề chính với mục tiêu huấn luyện này là các cổng nhị phân không khả vi. Để giải quyết vấn đề này, chúng tôi áp dụng mẹo Gumbel-Softmax [30, 31], cho phép rút mẫu cứng (hoặc mềm) từ một phân phối rời rạc. Xem xét một biến ngẫu nhiên rời rạc Z với xác suất P(Z=k) ∝ πk cho bất kỳ k = 1,...,K nào. Để rút một mẫu từ phân phối này, chúng ta có thể đầu tiên rút K mẫu i.i.d. {gk}Kk=1 từ phân phối Gumbel tiêu chuẩn và sau đó chọn chỉ số có xác suất log nhiễu loạn lớn nhất:

z = arg max k∈{1,...,K} log πk + gk; (7)

Argmax không khả vi, có thể được thư giãn thành softmax. Đã biết rằng bất kỳ mẫu nào từ phân phối rời rạc có thể được ký hiệu là một vector one-hot, trong đó chỉ số của entry khác không duy nhất là mẫu mong muốn. Với ký hiệu dựa trên vector này, chúng ta có thể rút một mẫu mềm như sau:

z = softmax((log π + g)/τ); (8)

trong đó π = (π1,...,πK), g = (g1,...,gK), và τ là một hằng số nhiệt độ. Eq. (8) là một xấp xỉ của quá trình sampling

¹ Phương pháp của chúng tôi có thể được mở rộng để xem xét các chi phí khác nhau của MHA và FFN. Trong Eq. (6), chúng ta có thể sử dụng trung bình có trọng số của hai loại cổng, trong đó các trọng số phụ thuộc vào chi phí tính toán của chúng. Sau đó, việc huấn luyện sẽ tối thiểu hóa tổng tính toán thay vì đơn giản là số lượng lớp.

gốc, nhưng nó khả vi w.r.t. π và do đó phù hợp cho tối ưu hóa dựa trên gradient. Khi τ → 0, xấp xỉ trở nên gần hơn với phiên bản rời rạc. Chúng tôi sử dụng τ = 1 trong các thí nghiệm của mình.

Đối với MHA thứ l, một phân phối xác suất rời rạc p(l)MHA ∈ R² trên hai giá trị cổng có thể (0 và 1) được dự đoán, trong đó 0 có nghĩa là bỏ qua mô-đun này và 1 có nghĩa là thực thi nó. Sau đó, một mẫu mềm được rút từ phân phối rời rạc này sử dụng Eq. (8), được sử dụng làm cổng trong Eq. (3) trong quá trình huấn luyện. Tương tự, đối với FFN thứ l, một phân phối p(l)FFN ∈ R² được dự đoán, từ đó một cổng mềm được rút và sử dụng trong Eq. (4). Các phân phối cổng được tạo ra bởi một bộ dự đoán cổng dựa trên các đặc trưng đầu vào, như được định nghĩa trong Phần 2.3.

2.3. Bộ dự đoán cổng cục bộ và toàn cục

Chúng tôi đề xuất hai loại bộ dự đoán cổng, cụ thể là bộ dự đoán cổng cục bộ và bộ dự đoán cổng toàn cục. Chúng tôi sử dụng một perceptron đa lớp (MLP) với một lớp ẩn duy nhất có kích thước 32 trong tất cả các thí nghiệm, có ít overhead tính toán.

Bộ dự đoán cổng cục bộ (LocalGP hoặc LGP) được liên kết với một lớp bộ mã hóa I3D cụ thể, như được minh họa trong Hình 1b. Mỗi lớp có bộ dự đoán cổng riêng của nó với các tham số độc lập. Xem xét lớp bộ mã hóa thứ l với một chuỗi đầu vào X(l−1) ∈ RT×d. Chuỗi này đầu tiên được chuyển đổi thành một vector d-chiều x(l−1) ∈ Rd thông qua average pooling dọc theo chiều thời gian. Sau đó, vector được pooling này được biến đổi thành hai vector xác suất 2-chiều cho cổng MHA và cổng FFN, tương ứng:

p(l)MHA, p(l)FFN = LGP(l)(x(l−1)); (9)

trong đó p(l)MHA, p(l)FFN ∈ R² được giới thiệu trong Phần 2.2, và LGP(l) là bộ dự đoán cổng cục bộ tại lớp thứ l. Với công thức này, quyết định thực thi hoặc bỏ qua bất kỳ mô-đun MHA hoặc FFN nào phụ thuộc vào đầu vào của lớp hiện tại, điều này phụ thuộc thêm vào quyết định được đưa ra tại lớp trước đó. Do đó, các quyết định được đưa ra tuần tự từ lớp dưới đến lớp trên. Trong quá trình suy luận, một ngưỡng cố định θ ∈ [0,1] được sử dụng để tạo ra một cổng nhị phân cho mỗi mô-đun:

g(l)MHA = 1 if (p(l)MHA)₁ > θ else 0; (10)
g(l)FFN = 1 if (p(l)FFN)₁ > θ else 0; (11)

trong đó (p(l)MHA)₁ là xác suất thực thi MHA và (p(l)FFN)₁ là xác suất thực thi FFN. Chúng tôi sử dụng θ = 0.5 theo mặc định, nhưng cũng có thể điều chỉnh chi phí suy luận bằng cách thay đổi θ.

--- TRANG 3 ---

18 20 22 24 26 28 30 32 34 36 11:5 12:0 12:5 13:0 13:5
Số lượng lớp trung bình % WER (≈#) Transformer LayerDrop I3D-LocalGP
I3D-GlobalGP I3D-GlobalGP (θ khác nhau)

(a) LibriSpeech test clean

18 20 22 24 26 28 30 32 34 36 26:0 27:0 28:0 29:0 30:0
Số lượng lớp trung bình % WER (≈#)

(b) LibriSpeech test other

Hình 2: Tỷ lệ lỗi từ (%) của các mô hình dựa trên CTC so với số lượng lớp trung bình được sử dụng cho suy luận trên các bộ test LibriSpeech. θ là ngưỡng để tạo cổng nhị phân như được định nghĩa trong Eqs. (10) (11).

18 20 22 24 26 28 30 32 34 36 11:4 11:6 11:8 12:0 12:2 12:4
Số lượng lớp trung bình % WER (≈#) Transformer
LayerDrop
I3D-GlobalGP

(a) LibriSpeech test clean

18 20 22 24 26 28 30 32 34 36 26:0 26:5 27:0 27:5
Số lượng lớp trung bình % WER (≈#) Transformer
LayerDrop
I3D-GlobalGP

(b) LibriSpeech test other

Hình 3: Tỷ lệ lỗi từ (%) của các mô hình dựa trên InterCTC so với số lượng lớp trung bình được sử dụng cho suy luận trên các bộ test LibriSpeech.

Bộ dự đoán cổng toàn cục (GlobalGP hoặc GGP), mặt khác, được định nghĩa cho toàn bộ bộ mã hóa I3D, như được hiển thị trong Hình 1c. Nó dự đoán các phân phối cổng cho tất cả các lớp dựa trên đầu vào của bộ mã hóa, cũng là đầu vào của lớp đầu tiên: X = X(0) ∈ RT×d. Cụ thể, chuỗi được biến đổi thành một vector duy nhất x = x(0) ∈ Rd bằng average pooling. Sau đó, nó được ánh xạ tới hai tập hợp phân phối xác suất cho tất cả N cổng MHA và FFN, tương ứng:

{p(l)MHA}Nl=1, {p(l)FFN}Nl=1 = GGP(x); (12)

trong đó p(l)MHA, p(l)FFN ∈ R² là các phân phối xác suất cổng tại lớp thứ l, và bộ mã hóa I3D có tổng cộng N lớp. Ở đây, các quyết định thực thi hoặc bỏ qua các mô-đun được đưa ra ngay lập tức sau khi nhìn thấy đầu vào của bộ mã hóa, có overhead tính toán thấp hơn LocalGP và cho phép kiểm soát linh hoạt hơn đối với kiến trúc suy luận. Trong quá trình suy luận, chúng ta vẫn có thể sử dụng một ngưỡng cố định θ ∈ [0,1] để tạo cổng nhị phân như trong Eqs. (10) và (11).

3. THÍ NGHIỆM

3.1. Thiết lập thí nghiệm

Chúng tôi sử dụng PyTorch [32] và tuân theo các recipes ASR trong ESPnet [33] để huấn luyện tất cả các mô hình. Chúng tôi chủ yếu sử dụng framework CTC trên LibriSpeech

18 20 22 24 26 28 30 32 34 36 10 11 12 13
Số lượng lớp trung bình % WER (≈#) Transformer I3D-LocalGP I3D-GlobalGP

Hình 4: Tỷ lệ lỗi từ (%) của các mô hình dựa trên CTC so với số lượng lớp trung bình được sử dụng cho suy luận trên bộ test Tedlium2.

Bảng 1: Tỷ lệ lỗi từ (%) và số lượng lớp suy luận trung bình của các mô hình dựa trên AED trên LibriSpeech 100h.

Model | dev clean | test clean
---|---|---
 | Ave #layers | WER (≈#) | Ave #layers | WER (≈#)
Transformer | 36 | 7.8 | 36 | 8.0
 | 27 | 8.2 | 27 | 8.5
I3D-LGP-36 | 27.3 | 7.9 | 27.1 | 8.3
I3D-GGP-36 | 27.2 | 7.8 | 27.1 | 8.2

100h [34]. Trong Phần 3.5, chúng tôi cũng cho thấy rằng I3D có thể được áp dụng cho AED và một corpus khác, Tedlium2 [35]. Các bộ mã hóa I3D của chúng tôi có tổng cộng 36 lớp. Chúng được khởi tạo với các Transformers tiêu chuẩn đã được huấn luyện và fine-tuned với tỷ lệ học giảm (≈ 1e−3) và các λ khác nhau (thường dao động từ 1 đến 13) trong Eq. (5) để cân bằng WER và tính toán. Các epoch fine-tuning cho LibriSpeech 100h và Tedlium2 lần lượt là 50 và 35. Chúng tôi so sánh I3D với hai baseline. Đầu tiên, chúng tôi huấn luyện các Transformers tiêu chuẩn với số lượng lớp giảm. Thứ hai, chúng tôi huấn luyện một Transformer 36 lớp với LayerDrop [36, 37] hoặc Intermediate CTC (InterCTC) [38] và thực hiện cắt tỉa lớp lặp [29] sử dụng bộ validation để có được nhiều mô hình với kiến trúc nhỏ hơn và cố định. Baseline này được ký hiệu là "LayerDrop" trong Hình 2 và 3. Chúng ta có thể so sánh I3D, có các lớp được giảm động dựa trên đầu vào, với các mô hình cắt tỉa tĩnh.

3.2. Kết quả chính

Hình 2 so sánh các mô hình I3D của chúng tôi với hai baseline. Chúng tôi huấn luyện các mô hình I3D-CTC với λ khác nhau trong Eq. (5) để điều chỉnh điểm hoạt động. Chúng tôi tính số lượng lớp là trung bình của số lượng khối MHA và số lượng khối FFN. Cả I3D-LocalGP và I3D-GlobalGP đều vượt trội hơn Transformer tiêu chuẩn và phiên bản cắt tỉa sử dụng cắt tỉa lớp lặp [29]. Chúng ta có thể giảm số lượng lớp trung bình xuống khoảng 20 trong khi vẫn khớp với Transformer được huấn luyện từ đầu. LocalGP đạt được hiệu suất tương tự như GlobalGP, nhưng GlobalGP chỉ có một bộ dự đoán cổng, có thể hiệu quả hơn cho suy luận. Lý do tại sao LocalGP không tốt hơn GlobalGP có thể là LocalGP quyết định có thực thi hay bỏ qua một khối dựa trên đầu vào của lớp hiện tại, phụ thuộc vào các quyết định tại các lớp trước đó. Thủ tục tuần tự này có thể dẫn đến lan truyền lỗi nghiêm trọng hơn. Chúng tôi cũng cho thấy rằng có thể điều chỉnh chi phí tính toán của một mô hình I3D đã được huấn luyện bằng cách thay đổi θ (xem Eqs. (10) (11)) tại thời điểm suy luận. Ba mô hình I3D-GlobalGP được giải mã với θ khác nhau. Khi θ giảm, nhiều khối được sử dụng hơn, và WER thường được cải thiện.

Hình 3 cho thấy kết quả sử dụng InterCTC [38]. Các WER thấp hơn so với trong Hình 2, nhờ vào loss CTC phụ trợ giúp regularize việc huấn luyện. Một lần nữa, I3D luôn tốt hơn Transformer được huấn luyện từ đầu và mô hình cắt tỉa.

3.3. Phân tích các phân phối cổng

Hình 5 cho thấy mean và độ lệch chuẩn (std) của các xác suất cổng được tạo ra bởi một mô hình I3D-GlobalGP sử dụng CTC trên Lib-

--- TRANG 4 ---

1 6 11 16 21 26 31 36 0 0:5 1
Chỉ số lớp Xác suất

(a) Xác suất cổng MHA.

1 6 11 16 21 26 31 36 0 0:5 1
Chỉ số lớp Xác suất

(b) Xác suất cổng FFN.

Hình 5: Xác suất cổng dự đoán (mean và std) tại các lớp khác nhau của một mô hình I3D-GlobalGP trên LibriSpeech test other. Xác suất cao hơn có nghĩa là lớp có khả năng được thực thi cao hơn.

1 6 11 16 21 26 31 36 0 0:5 1
Chỉ số lớp Xác suất

(a) Xác suất cổng MHA (được huấn luyện với InterCTC).

1 6 11 16 21 26 31 36 0 0:5 1
Chỉ số lớp Xác suất

(b) Xác suất cổng FFN (được huấn luyện với InterCTC).

Hình 6: Xác suất cổng dự đoán (mean và std) tại các lớp khác nhau của một mô hình I3D-GlobalGP với InterCTC trên LibriSpeech test other. Xác suất cao hơn có nghĩa là lớp có khả năng được thực thi cao hơn.

riSpeech test-other. Hầu hết các lớp có xác suất ổn định. Một số lớp có biến thiên lớn hơn tùy thuộc vào đầu vào. Đối với cả MHA và FFN, các lớp trên được thực thi với xác suất cao trong khi các lớp dưới có xu hướng bị bỏ qua, điều này phù hợp với [28].

Chúng tôi cũng cho thấy xác suất cổng từ một mô hình I3D-GlobalGP được huấn luyện với InterCTC [38] trong Hình 6. Thú vị thay, xu hướng tổng thể rất khác so với Hình 5. Bây giờ, các lớp trên hầu như bị bỏ qua trong khi các lớp dưới được thực thi với xác suất rất cao, cho thấy rằng các lớp dưới của bộ mã hóa này có thể học các biểu diễn mạnh mẽ cho tác vụ ASR. Điều này có thể là do các loss CTC phụ trợ được chèn vào các lớp trung gian có thể tạo điều kiện cho việc lan truyền gradient đến các phần dưới của bộ mã hóa sâu, điều này hiệu quả cải thiện khả năng của nó và cả hiệu suất cuối cùng.

Chúng tôi tin rằng phân tích cổng này có thể cung cấp một cách để diễn giải hành vi theo lớp của các mạng sâu.

3.4. Phân tích sự phụ thuộc đầu vào

Đã được chứng minh rằng các mô hình I3D của chúng tôi có thể điều chỉnh động độ sâu bộ mã hóa dựa trên các đặc điểm của một phát ngôn đầu vào, điều này đạt được hiệu suất mạnh mẽ ngay cả với tính toán giảm. Nhưng vẫn chưa rõ các đặc trưng nào quan trọng để bộ dự đoán cổng xác định các mô-đun được sử dụng trong quá trình suy luận. Chúng tôi đã phát hiện ra rằng độ dài giọng nói nói chung ảnh hưởng đến kiến trúc suy luận.

Hình 7 cho thấy phân phối độ dài giọng nói được phân loại theo số lượng khối MHA hoặc FFN được sử dụng bởi một mô hình I3D-GlobalGP trong quá trình suy luận. Chúng tôi quan sát thấy rằng các phát ngôn sử dụng nhiều khối hơn có xu hướng dài hơn. Điều này có thể là do các phát ngôn dài hơn chứa thông tin phức tạp hơn và phụ thuộc tầm xa hơn giữa các khung hình, đòi hỏi nhiều khối hơn (đặc biệt là MHA) để xử lý.

Chúng tôi cũng xem xét hai yếu tố khác có thể ảnh hưởng đến suy luận

0 5 10 15 20 25 0 0:2 0:4
Độ dài giọng nói đầu vào tính bằng giây Tần suất 18 khối 19 khối
20 khối 21 khối

(a) MHA

0 5 10 15 20 25 0 0:1
Độ dài giọng nói đầu vào tính bằng giây Tần suất 19 khối 20 khối
21 khối 22 khối

(b) FFN

Hình 7: Phân phối độ dài giọng nói được phân loại theo số lượng khối MHA hoặc FFN được sử dụng cho suy luận. Đây là một mô hình I3D-GlobalGP được đánh giá trên LibriSpeech test other. Các phát ngôn sử dụng nhiều khối hơn có xu hướng dài hơn.

kiến trúc, cụ thể là độ khó của các phát ngôn được đo bằng WERs, và chất lượng âm thanh được đo bằng điểm DNSMOS [39]. Tuy nhiên, nói chung, chúng tôi không quan sát thấy mối quan hệ rõ ràng giữa các chỉ số này và số lượng lớp được sử dụng cho suy luận.

3.5. Khả năng tổng quát

Chúng tôi chứng minh rằng các bộ mã hóa I3D được đề xuất có thể được áp dụng trực tiếp cho các bộ dữ liệu và frameworks ASR khác. Hình 4 cho thấy kết quả của các mô hình dựa trên CTC trên Tedlium2. Các mô hình I3D của chúng tôi liên tục đạt được WER thấp hơn so với Transformer tiêu chuẩn với số lượng lớp tương tự hoặc thậm chí ít hơn trong quá trình suy luận.² Chúng tôi tiếp tục áp dụng I3D cho framework attention-based encoder-decoder (AED). Chỉ có bộ mã hóa được thay đổi trong khi bộ giải mã vẫn là bộ giải mã Transformer tiêu chuẩn. Bảng 1 trình bày kết quả trên LibriSpeech 100h. Với khoảng 27 lớp trung bình trong quá trình suy luận, các mô hình I3D của chúng tôi vượt trội hơn Transformer 27 lớp được huấn luyện từ đầu trên cả bộ dev clean và test clean. I3D với bộ dự đoán cổng toàn cục tốt hơn một chút so với với bộ dự đoán cổng cục bộ.

4. KẾT LUẬN

Trong công trình này, chúng tôi đề xuất I3D, một bộ mã hóa dựa trên Transformer điều chỉnh động độ sâu của nó dựa trên các đặc điểm của các phát ngôn đầu vào để cân bằng hiệu suất và hiệu quả. Chúng tôi thiết kế hai loại bộ dự đoán cổng và cho thấy rằng các mô hình dựa trên I3D liên tục vượt trội hơn Transformer gốc được huấn luyện từ đầu và mô hình cắt tỉa tĩnh. I3D có thể được áp dụng cho các frameworks và corpus ASR đầu cuối đến cuối khác nhau. Chúng tôi cũng trình bày phân tích thú vị về các xác suất cổng dự đoán và sự phụ thuộc đầu vào để diễn giải tốt hơn hành vi của các bộ mã hóa sâu và hiệu ứng của các kỹ thuật regularization loss trung gian. Trong tương lai, chúng tôi dự định áp dụng phương pháp này cho các mô hình được huấn luyện trước lớn. Chúng tôi sẽ khám phá việc chỉ fine-tuning các bộ dự đoán cổng để giảm đáng kể chi phí huấn luyện.

5. LỜI CẢM ơN

Công trình này sử dụng Bridges2 tại PSC và Delta tại NCSA thông qua phân bổ CIS210014 từ chương trình Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS), được hỗ trợ bởi các khoản tài trợ của National Science Foundation #2138259, #2138286, #2138307, #2137603, và #2138296.

² Chúng tôi cũng đã đánh giá I3D trên LibriSpeech 960h. Các quan sát phù hợp với LibriSpeech 100h và Tedlium2.

--- TRANG 5 ---

6. TÀI LIỆU THAM KHẢO

[1] A. Graves, S. Fernández, et al., "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks," in Proc. ICML, 2006.

[2] K. Cho, B. Merrienboer, et al., "Learning phrase representations using RNN encoder-decoder for statistical machine translation," in Proc. EMNLP, 2014.

[3] D. Bahdanau, K. Cho, et al., "Neural machine translation by jointly learning to align and translate," in Proc. ICLR, 2015.

[4] W. Chan, N. Jaitly, et al., "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition," in Proc. ICASSP, 2016.

[5] A. Graves, "Sequence transduction with recurrent neural networks," arXiv:1211.3711, 2012.

[6] A. Vaswani, N. Shazeer, N. Parmar, et al., "Attention is all you need," in Proc. NeurIPS, 2017.

[7] A. Gulati, J. Qin, C.-C. Chiu, et al., "Conformer: Convolution-augmented Transformer for Speech Recognition," in Proc. Interspeech, 2020.

[8] Y. Peng, S. Dalmia, et al., "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," in Proc. ICML, 2022.

[9] K. Kim, F. Wu, Y. Peng, et al., "E-branchformer: Branchformer with enhanced merging for speech recognition," arXiv:2210.00077, 2022.

[10] S. Karita, N. Chen, T. Hayashi, et al., "A comparative study on transformer vs rnn in speech applications," in Proc. ASRU, 2019.

[11] G. Hinton, O. Vinyals, J. Dean, et al., "Distilling the knowledge in a neural network," arXiv:1503.02531, 2015.

[12] H. Chang, S. Yang, and H. Lee, "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert," in Proc. ICASSP, 2022.

[13] R. Wang, Q. Bai, et al., "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT," in Proc. Interspeech, 2022.

[14] P. Dong, S. Wang, et al., "RTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition," in ACM/IEEE Design Automation Conference (DAC), 2020.

[15] K. Tan and D.L. Wang, "Compressing deep neural networks for efficient speech enhancement," in Proc. ICASSP, 2021.

[16] C. J. Lai, Y. Zhang, et al., "Parp: Prune, adjust and re-prune for self-supervised speech recognition," in Proc. NeurIPS, 2021.

[17] Y. Han, G. Huang, S. Song, et al., "Dynamic neural networks: A survey," IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 11, pp. 7436–7456, 2022.

[18] E. Bengio, P. Bacon, et al., "Conditional computation in neural networks for faster models," arXiv:1511.06297, 2015.

[19] A. Veit and S. Belongie, "Convolutional networks with adaptive inference graphs," in Proc. ECCV, 2018.

[20] X. Wang, F. Yu, et al., "Skipnet: Learning dynamic routing in convolutional networks," in Proc. ECCV, 2018.

[21] Z. Wu, T. Nagarajan, et al., "Blockdrop: Dynamic inference paths in residual networks," in Proc. CVPR, 2018.

[22] J. Shen, Y. Wang, et al., "Fractional skipping: Towards finer-grained dynamic cnn inference," in Proc. AAAI, 2020.

[23] C. Li, G. Wang, et al., "Dynamic slimmable network," in Proc. CVPR, 2021.

[24] J. Macoskey, G. P. Strimel, and A. Rastrow, "Bifocal neural asr: Exploiting keyword spotting for inference optimization," in Proc. ICASSP, 2021.

[25] Y. Shi, V. Nagaraja, C. Wu, et al., "Dynamic encoder transducer: a flexible solution for trading off accuracy for latency," arXiv:2104.02176, 2021.

[26] F. Weninger, M. Gaudesi, R. Leibold, R. Gemello, and P. Zhan, "Dual-encoder architecture with encoder selection for joint close-talk and far-talk speech recognition," in Proc. ASRU, 2021.

[27] J. Macoskey, G. P. Strimel, J. Su, and A. Rastrow, "Amortized neural networks for low-latency speech recognition," arXiv:2108.01553, 2021.

[28] Y. Xie, J. J. Macoskey, et al., "Compute Cost Amortized Transformer for Streaming ASR," in Proc. Interspeech, 2022.

[29] J. Lee, J. Kang, and S. Watanabe, "Layer pruning on demand with intermediate CTC," in Proc. Interspeech, 2021.

[30] E. Jang, S. Gu, and B. Poole, "Categorical reparameterization with gumbel-softmax," in Proc. ICLR, 2017.

[31] C. J. Maddison, A. Mnih, and Y. Teh, "The concrete distribution: A continuous relaxation of discrete random variables," in Proc. ICLR, 2017.

[32] A. Paszke, S. Gross, F. Massa, et al., "Pytorch: An imperative style, high-performance deep learning library," in Proc. NeurIPS, 2019.

[33] S. Watanabe, T. Hori, S. Karita, et al., "ESPnet: End-to-End Speech Processing Toolkit," in Proc. Interspeech, 2018.

[34] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: An ASR corpus based on public domain audio books," in Proc. ICASSP, 2015.

[35] A. Rousseau, P. Deléglise, Y. Esteve, et al., "Enhancing the ted-lium corpus with selected data for language modeling and more ted talks.," in Proc. LREC, 2014.

[36] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger, "Deep networks with stochastic depth," in Proc. ECCV, 2016.

[37] A. Fan, E. Grave, and A. Joulin, "Reducing transformer depth on demand with structured dropout," in Proc. ICLR, 2020.

[38] J. Lee and S. Watanabe, "Intermediate loss regularization for ctc-based speech recognition," in Proc. ICASSP, 2021.

[39] C. K. Reddy, V. Gopal, and R. Cutler, "Dnsmos p.835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors," in Proc. ICASSP, 2022.
