# 2305.13048.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/llm-architecture/2305.13048.pdf
# File size: 1590242 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
RWKV: Reinventing RNNs for the Transformer Era
Bo Peng1,2∗Eric Alcaide2,3,4∗Quentin Anthony2,5∗
Alon Albalak2,6Samuel Arcadinho2,7Stella Biderman2,8Huanqi Cao9Xin Cheng10
Michael Chung11Xingjian Du1Matteo Grella12Kranthi Kiran GV2,13Xuzheng He2
Haowen Hou14Jiaju Lin1Przemysław Kazienko15Jan Koco ´n15Jiaming Kong16
Bartłomiej Koptyra15Hayden Lau2Krishna Sri Ipsit Mantri17Ferdinand Mom18,19
Atsushi Saito2,20Guangyu Song21Xiangru Tang22Bolun Wang23Johan S. Wind24
Stanisław Wo´ zniak15Ruichong Zhang9Zhenyuan Zhang2Qihang Zhao25,26
Peng Zhou23Qinghua Zhou5Jian Zhu27Rui-Jie Zhu28,29
1Generative AI Commons2EleutherAI3U. of Barcelona4Charm Therapeutics5Ohio State U.6U. of C., Santa Barbara
7Zendesk8Booz Allen Hamilton9Tsinghua University10Peking University11Storyteller.io12Crisis2413New York U.
14National U. of Singapore15Wroclaw U. of Science and Technology16Databaker Technology17Purdue U.18Criteo AI Lab
19Epita20Nextremer21Moves22Yale U.23RuoxinTech24U. of Oslo25U. of Science and Technology of China
26Kuaishou Technology27U. of British Columbia28U. of C., Santa Cruz29U. of Electronic Science and Technology of China
Abstract
Transformers have revolutionized almost all
natural language processing (NLP) tasks but
suffer from memory and computational com-
plexity that scales quadratically with sequence
length. In contrast, recurrent neural networks
(RNNs) exhibit linear scaling in memory and
computational requirements but struggle to
match the same performance as Transformers
due to limitations in parallelization and scala-
bility. We propose a novel model architecture,
Receptance Weighted Key Value (RWKV), that
combines the efficient parallelizable training
of transformers with the efficient inference of
RNNs.
Our approach leverages a linear attention mech-
anism and allows us to formulate the model as
either a Transformer or an RNN, thus paralleliz-
ing computations during training and maintains
constant computational and memory complex-
ity during inference. We scale our models as
large as 14 billion parameters, by far the largest
dense RNN ever trained, and find RWKV per-
forms on par with similarly sized Transformers,
suggesting future work can leverage this archi-
tecture to create more efficient models. This
work presents a significant step towards rec-
onciling trade-offs between computational ef-
ficiency and model performance in sequence
processing tasks.1
1 Introduction
Deep learning has greatly advanced artificial intelli-
gence, impacting a range of scientific and industrial
uses. These often involve complex sequential data
∗Equal first authorship. Others listed alphabetically.
1Code at: https://github.com/BlinkDL/RWKV-LMprocessing tasks such as natural language under-
standing, conversational AI, time-series analysis,
and indirectly sequential formats like images and
graphs (Brown et al., 2020; Ismail Fawaz et al.,
2019; Wu et al., 2020; Albalak et al., 2022). Pre-
dominant among these techniques include RNNs
and Transformers (Vaswani et al., 2017), each with
specific benefits and drawbacks. RNNs require less
memory, particularly for handling long sequences.
However, they suffer from the vanishing gradient
problem and non-parallelizability in the time di-
mension during training, limiting their scalability
(Hochreiter, 1998; Le and Zuidema, 2016).
102103
Compute (exaFLOP)40455055Accuracy
BLOOM
Pythia
OPT
RWKV
Figure 1: Average performance of RWKV models com-
pared to transformers across twelve NLP tasks. For
further details, see section 5.
Transformers emerged as a powerful alternative,
adept at managing local and long-range dependen-
cies and supporting parallelized training (Tay et al.,
2022). Models such as GPT-3 (Brown et al., 2020),
ChatGPT (OpenAI, 2022; Koco ´n et al., 2023),arXiv:2305.13048v2  [cs.CL]  11 Dec 2023

--- PAGE 2 ---
Model Time Space
Transformer O(T2d) O(T2+Td)
Reformer O(TlogTd)O(TlogT+Td)
Performer O(Td2logd)O(Tdlogd+d2logd)
Linear Transformers O(Td2) O(Td+d2)
AFT-full O(T2d) O(Td)
AFT-local O(Tsd) O(Td)
MEGA O(cTd) O(cd)
RWKV (ours) O(Td) O(d)
Table 1: Inference complexity comparison with different
Transformers. Here Tdenotes the sequence length,
dthe feature dimension, cis MEGA’s chunk size of
quadratic attention, and sis the size of a local window
for AFT.
LLaMA (Touvron et al., 2023), and Chinchilla
(Hoffmann et al., 2022) showcase the potential of
Transformers in NLP. However, the self-attention
mechanism’s quadratic complexity makes it compu-
tationally and memory intensive for tasks involving
long sequences and constrained resources. This
has stimulated research to enhance Transformers’
scalability, sometimes sacrificing some of their ef-
fectiveness (Wang et al., 2020; Zaheer et al., 2020;
Dao et al., 2022a).
To tackle these challenges, we introduce the Re-
ceptance Weighted Key Value ( RWKV ) model,
combining the strengths of RNNs and Transform-
ers while circumventing key drawbacks. RWKV
alleviates memory bottleneck and quadratic scaling
associated with Transformers (Katharopoulos et al.,
2020) with efficient linear scaling, while maintain-
ing the expressive properties of the Transformer,
such as parallelized training and robust scalability.
RWKV reformulates the attention mechanism with
a variant of linear attention, replacing traditional
dot-product token interaction with more effective
channel-directed attention. This implementation,
without approximation , offers the lowest computa-
tional and memory complexity; see Table 1.
The motivation behind RWKV is to balance com-
putational efficiency with expressive capacity in
neural networks. It offers a solution for handling
large-scale models with billions of parameters, ex-
hibiting competitive performance at a reduced com-
putational cost. Experiments suggest RWKV ad-
dresses scaling and deployment challenges in AI,
especially for sequential data processing, pointing
towards more sustainable and efficient AI models.
Our contributions in this paper are as follows:
•The introduction of RWKV , a novel architec-ture combining RNNs and Transformer advan-
tages while mitigating their limitations.
•Detailed experiments demonstrating RWKV’s
performance and efficiency on benchmark
datasets for large-scale models.
•The release of pretrained models, from 169
million to 14 billion parameters, trained on
the Pile (Gao et al., 2020; Biderman et al.,
2022).2
2 Background
Here we briefly review the fundamentals of RNNs
and Transformers.
2.1 Recurrent Neural Networks (RNNs)
Popular RNN architectures such as LSTM (Hochre-
iter and Schmidhuber, 1997) and GRU (Chung
et al., 2014) are characterized by the following for-
mulation (shown for LSTM, others can be reasoned
similarly):
ft=σg(Wfxt+Ufht−1+bf), (1)
it=σg(Wixt+Uiht−1+bi), (2)
ot=σg(Woxt+Uoht−1+bo), (3)
˜ct=σc(Wcxt+Ucht−1+bc), (4)
ct=ft⊙ct−1+it⊙˜ct, (5)
ht=ot⊙σh(ct). (6)
Although RNNs can be factored into two linear
blocks ( WandU) and an RNN-specific block (1)–
(6), as noted by Bradbury et al. (2017), the data de-
pendency relying on previous time steps prohibits
parallelizing these typical RNNs.
2.2 Transformers and AFT
Introduced by Vaswani et al. (2017), Transformers
are a class of neural networks that have become
the dominant architecture for several NLP tasks.
Instead of operating on sequences step-by-step like
RNNs, Transformers rely on attention mechanisms
to capture relationships between all input and all
output tokens:
Attn( Q, K, V ) = softmax( QK⊤)V, (7)
where the multi-headness and scaling factor1√dkis
omitted for convenience. The core QK⊤multipli-
cation is an ensemble of pairwise attention scores
2https://huggingface.co/RWKV

--- PAGE 3 ---
between each token in a sequence, which can be
decomposed as vector operations:
Attn( Q, K, V )t=PT
i=1eq⊤
tki⊙viPT
i=1eq⊤
tki.(8)
AFT (Zhai et al., 2021), alternately formulates
Attn+(W, K, V )t=Pt
i=1ewt,i+ki⊙viPt
i=1ewt,i+ki,(9)
where {wt,i} ∈RT×Tis the learned pair-wise po-
sition biases, and each wt,iis a scalar.
Inspired by AFT, RWKV takes a similar ap-
proach. However, for simplicity, it modifies the
interaction weights so that it can be transformed
into an RNN. Each wt,iin RWKV is a channel-
wise time decay vector multiplied by the relative
position and traced backward from current time as
it decays:
wt,i=−(t−i)w, (10)
where w∈(R≥0)d, with dthe number of chan-
nels. We require wto be non-negative to ensure
thatewt,i≤1and the per-channel weights decay
backwards in time.
3 RWKV
The RWKV model architecture is defined by four
fundamental elements that are intrinsic to the time-
mixing and channel-mixing blocks:
•R: The Receptance vector acts as the receiver
of past information.
•W: The Weight signifies the positional
weight decay vector, a trainable parameter
within the model.
•K: The Key vector performs a role analogous
toKin traditional attention mechanisms.
•V: The Value vector functions similarly to V
in conventional attention processes.
These core elements interact multiplicatively at
each timestep, as depicted in Figure 2.
3.1 Architecture
The RWKV model is composed of stacked residual
blocks. Each block consists of a time-mixing and
a channel-mixing sub-block, embodying recurrent
structures to leverage past information.
This model uses a unique attention-like score
update process, which includes a time-dependent
Time 
Mixing
LayerNorm
LayerNorm
?
Channel 
Mixing
?
?'
Out
WKV
V
K
R
?
LayerNorm
R'
K'
V'
RWKV-LM 
HeadInput 
Embedding
LayerNorm
LayerNorm
Output 
Probabilities
LayerNorm
LayerNorm
Channel 
Mixing
Time 
Mixing
Out
SoftmaxFigure 2: Elements within an RWKV block (left) and
the complete RWKV residual block, equipped with a
final head for language modeling (right).
softmax operation improving numerical stability
and mitigating vanishing gradients (for rigorous
proof, see Appendix H). It ensures that the gra-
dient is propagated along the most relevant path.
Additionally, layer normalization (Ba et al., 2016)
incorporated within the architecture aids in stabi-
lizing the gradients, effectively addressing both
vanishing and exploding gradient issues. These
design elements not only enhance the training dy-
namics of deep neural networks but also facilitate
the stacking of multiple layers, leading to superior
performance over conventional RNN models by
capturing complex patterns across different levels
of abstraction (see also Appendix I).
3.1.1 Token Shift
In this architecture, all linear projection vectors
(R,K,Vin time-mixing, and R′,K′in channel-
mixing) involved in computations are produced by
linear interpolation between current and previous
timestep inputs, facilitating a token shift.
The vectors for time-mixing computation are
linear projections of linear combinations of the
current and previous inputs of the block:
rt=Wr·(µr⊙xt+ (1−µr)⊙xt−1),(11)
kt=Wk·(µk⊙xt+ (1−µk)⊙xt−1),(12)
vt=Wv·(µv⊙xt+ (1−µv)⊙xt−1),(13)

--- PAGE 4 ---
Token
shiftToken
shiftStates StatesLayer Norm
MyLayer Normname
Layer NormTime MixLM Head
Layer Norm
isLayer NormBob
Layer NormChannel Mix
Time MixLM Head
Layer Norm
nameLayer Normis
Layer NormChannel Mix
Time MixLM Head
Token
shiftToken
shiftChannel MixFigure 3: RWKV architecture for language modeling.
as are the channel-mixing inputs:
r′
t=W′
r·(µ′
r⊙xt+ (1−µ′
r)⊙xt−1),(14)
k′
t=W′
k·(µ′
k⊙xt+ (1−µ′
k)⊙xt−1).(15)
The token shift is implemented as a simple off-
set in the temporal dimension at each block us-
ing the PyTorch (Paszke et al., 2019) library as
nn.ZeroPad2d((0,0,1,-1)) .
3.1.2 WKV Operator
The computation of the WKV operator in our
model parallels the method used in Attention Free
Transformer (AFT) (Zhai et al., 2021). However,
unlike AFT where Wis a pairwise matrix, our
model treats Was a channel-wise vector that is
modified by relative position. In our model, this
recurrent behavior is defined by the time-dependent
update of the WKV vectors, formalized in the fol-
lowing equation:
wkv t=Pt−1
i=1e−(t−1−i)w+ki⊙vi+eu+kt⊙vtPt−1
i=1e−(t−1−i)w+ki+eu+kt.
(16)
To circumvent any potential degradation of W,
we introduce a vector Uthat separately attends to
the current token. More information about this can
be found in Appendix I.
3.1.3 Output Gating
Output gating is implemented in both time-mixing
and channel-mixing blocks using the sigmoid ofthe receptance, σ(r). The output vector otpost the
WKV operator is given by:
ot=Wo·(σ(rt)⊙wkv t). (17)
In the channel-mixing block, a similar operation
is performed:
o′
t=σ(r′
t)⊙(W′
v·max( k′
t,0)2), (18)
where we adopt the squared ReLU activation func-
tion (So et al., 2021).
3.2 Transformer-like Training
RWKV can be efficiently parallelized using a tech-
nique called time-parallel mode , reminiscent of
Transformers. The time complexity of processing
a batch of sequences in a single layer is O(BTd2),
primarily consisting of matrix multiplications Wλ,
where λ∈ {r, k, v, o }(assuming Bsequences, T
maximum tokens, and dchannels). In contrast, up-
dating attention scores wkv tinvolves a serial scan
(see Appendix D for more detail) and has complex-
ityO(BTd).
The matrix multiplications can be parallelized
similarly to Wλ, where λ∈ {Q, K, V, O }in con-
ventional Transformers. The element-wise WKV
computation is time-dependent but can be readily
parallelized along the other two dimensions (Lei
et al., 2018)3.
3.3 RNN-like Inference
Recurrent networks commonly utilize the output
at state tas input at state t+ 1. This usage is
also observed in the autoregressive decoding infer-
ence of language models, where each token must
be computed before being passed to the next step.
RWKV takes advantage of this RNN-like structure,
known as time-sequential mode . In this context,
RWKV can be conveniently formulated recursively
for decoding during inference, as demonstrated in
Appendix D.
3.4 Additional Optimizations
Custom Kernels To address inefficiencies in the
WKV computation arising from the sequential na-
ture of the task when using standard deep learning
frameworks, we have developed a custom CUDA
3For extremely long sequences, more sophisticated meth-
ods such as Martin and Cundy (2017) that parallelize over
sequence length could be used.

--- PAGE 5 ---
kernel. This kernel enables the execution of a sin-
gle compute kernel on training accelerators, while
all other parts of the model, such as matrix multi-
plications and point-wise operations, are already
inherently parallelizable and efficient.
Small Init Embedding During the initial stage
of training a transformer model (Vaswani et al.,
2017), we observe that the embedding matrix un-
dergoes slow changes, presenting a challenge for
the model to move away from its initial noisy em-
bedding state. To address this issue, we propose
an approach that involves initializing the embed-
ding matrix with small values and subsequently
applying an additional LayerNorm operation. This
accelerates and stabilizes the training process, al-
lowing for the training of deep architectures with
post-LN components. The effectiveness of this
approach is demonstrated in Figure 9, illustrating
improved convergence by enabling the model to
quickly transition away from the initially small em-
bedding. This is achieved through small changes
occurring in a single step, which subsequently lead
to substantial alterations in directions and further
notable changes after the LayerNorm operation.
Custom Initialization Building on principles
from previous works (He et al., 2016; Jumper et al.,
2021), we adopt an initialization strategy where
parameters are set to values resembling an identity
mapping while breaking symmetry to establish a
clear information flow. The majority of weights are
initialized to zero, and linear layers do not employ
biases. Detailed formulas are given in Appendix E.
We observe that the choice of initialization plays a
crucial role in both the speed and quality of conver-
gence (refer to Appendix F for further details).
3.5 Implementation
RWKV is implemented using the PyTorch Deep
Learning Library (Paszke et al., 2019). We inte-
grate additional optimization strategies inspired by
DeepSpeed (Rasley et al., 2020) into the system,
improving its efficiency and scalability.
The model begins with an embedding layer, as
detailed in Section 3.4. Following this are sev-
eral identical residual blocks arranged sequentially.
These are depicted in Figures 2 and 3 and adheres
to the principles outlined in Section 3.1.1. After
the last block, a simple output projection head, con-
sisting of a LayerNorm (Ba et al., 2016) and a
linear projection, is employed for logits generationfor next-token prediction and computation of the
cross-entropy loss during training.
4 Trained Models and Computing Costs
To demonstrate the scalability of RWKV , we train
six models ranging from 169 million to 14 billion
parameters as shown in Table 2. All models are
trained for one epoch (330 billion tokens) on the
Pile (Gao et al., 2020; Biderman et al., 2022).
Name Layers Model Dimension Parameters FLOP per token
169 M 12 768 1.693×1082.613×108
430 M 24 1024 4.304×1087.573×108
1.5 B 24 2048 1.515×1092.823×109
3 B 32 2560 2.985×1095.710×109
7 B 32 4096 7.393×1091.437×1010
14 B 40 5120 1.415×10102.778×1010
Table 2: RWKV model architectures and FLOP counts.
Further details of these hyperparameters are elaborated
upon in Appendix G.
The number of parameters for each model is
computed using the formula: # parameters =
2V D+ 13D2L+D(11L+ 4) where V= 50277
is the vocabulary size, Drepresents the Model Di-
mension and Lcorresponds to the number of lay-
ers. FLOPs is for a forward pass for one token. It
was calculated as 2(2V D+ 13D2L), which is the
twice (add and multiply) the number of parameters
in linear layers. The backwards pass FLOPs can
be approximated as twice that of the forward pass,
giving a total of 6(2V D+ 13D2L)FLOP per to-
ken. Notably, this matches the standard formula
for FLOP calculations in transformers Kaplan et al.
(2020): FLOP = 6·[# tokens ]·[# parameters ].
4.1 Additional Training Details
For training, we use the standard Adam optimizer
without weight decay, use bfloat16 precision, and
train with a context length of 1024 tokens. Fur-
ther details on hyperparameters are in Appendix G.
Diverting from standard practice for transformers,
we apply exponential decay to our learning rate.
We also incorporate the auxiliary loss introduced
by PaLM (Chowdhery et al., 2022), supplement-
ing the standard cross-entropy loss function. This
auxiliary loss encourages the softmax normalizer
to approximate zero closely. As for the learning
rate schedule, it remains constant for the initial
iterations, and subsequently decays exponentially.

--- PAGE 6 ---
4.2 Scaling Laws
Scaling laws (Kaplan et al., 2020; Henighan et al.,
2020; Hoffmann et al., 2022; Muennighoff et al.,
2023) in language models refer to the mathemati-
cal relationships that describe how the performance
of a language model changes with respect to var-
ious factors. These factors can include the model
size ( N), dataset size ( D), or the optimally allo-
cated compute budget ( Cmin). Scaling laws are
important for two primary reasons: they allow us
to make predictions and plans regarding the costs
and performance of large models before they are
trained via interpolation and extrapolation (Black
et al., 2022; Le Scao et al., 2022) and the contexts
in which they fail provides rich feedback on im-
portant areas for future research (Wei et al., 2022a;
Biderman et al., 2023a).
Previous work on scaling laws for RNNs has
claimed that LSTMs do not strictly follow the same
log-log linear scaling that transformers do (Kaplan
et al., 2020). We train 45 RWKV models for a
variety of pairs (dataset, parameters) and find that
RWKV does follow the same general form of the
scaling law that is well established for transformers.
Figure 4 shows our results for loss as a function of
compute, with the linear fit to the Pareto optimal
points holding an r2value of 0.994. Even when
we extrapolate our curve an additional order of
magnitude (blue), we find an extremely good fit
with an r2of0.875.
101
100101102103
Compute (exaFLOP)2×1003×100LossNon-Optimal
Optimal
T est
Trend Line
Figure 4: Scaling laws curves for RWKV models
5 Evaluations
Having demonstrated the scalability of RWKV
models in the previous section, we now turn our
attention to their competitiveness with traditional
transformers. We focus on two questions:Competitiveness Is RWKV competitive against
quadratic transformer architectures with the same
amount of compute?
Long Context Does increasing the context
length of RWKV yield better language modeling
loss when RWKV models are trained for context
lengths that most open-sourced quadratic trans-
formers cannot efficiently process?
5.1 NLP Evaluations
To demonstrate that RWKV is competitive with
traditional transformers at NLP tasks, we compare
with similarly sized models trained for a similar
number of tokens (Pythia (Biderman et al., 2023b),
OPT (Zhang et al., 2022) and BLOOM (Scao et al.,
2022)). All RWKV models were trained for one
epoch on the Pile (330B tokens), which is close but
not identical to the amount of tokens the Pythia,
OPT, and BLOOM models were trained for. Con-
sequently, we compare our models on a FLOP-
matched basis . We avoid comparing with model
trained in the Chinchilla-optimal regime (Hoff-
mann et al., 2022) or the overtrained regime (Tou-
vron et al., 2023) to ensure the most equitable com-
parison.
We report results on ARC (both Easy and Chal-
lenge) (Clark et al., 2018), BoolQ (Clark et al.,
2019), COPA (Roemmele et al., 2018), HeadQA
(Vilares and Gómez-Rodríguez, 2019), HellaSwag
(Zellers et al., 2019), LAMBADA (Paperno et al.,
2016), OpenBookQA (Mihaylov et al., 2018),
PIQA (Bisk et al., 2020), ReCoRD (Zhang et al.,
2018), SciQ (Johannes Welbl Nelson F. Liu, 2017),
and Winogrande (Zellers et al., 2020). Figure 1
shows the average results across all benchmarks.
Some individual benchmarks are shown in Fig 5,
with the rest in Appendix J.
Additionally, we carried out comparative studies
on RWKV and ChatGPT / GPT-4, see Appendix
L. They revealed that RWKV is very sensitive to
prompt engineering. When the prompts were ad-
justed (re-ordered) from the ones used for GPT to
more suitable for RWKV , the performance (F1) in-
creased even from 44.2% to 74.8%. For sarcasm
detection, RWKV outperformed ChatGPT, but was
still slightly worse than the SOTA solution.
5.2 Extended Context Finetuning
Unlike transformers, RNNs do not have a pre-
defined sequences length when they are created.
However in order to efficient make use of compute

--- PAGE 7 ---
102103
Compute (exaFLOP)17.520.022.525.027.530.032.535.037.5Accuracy
BLOOM
Pythia
OPT
RWKV(a) ARC (Challenge)
102103
Compute (exaFLOP)3035404550Accuracy
 (b) HellaSwag
102103
Compute (exaFLOP)203040506070Accuracy
BLOOM
Pythia
OPT
RWKV (c) LAMBADA (OpenAI)
102103
Compute (exaFLOP)12.515.017.520.022.525.027.530.0Accuracy
(d) OpenBookQA
102103
Compute (exaFLOP)505560657075808590Accuracy
BLOOM
Pythia
OPT
RWKV (e) ReCoRD
102103
Compute (exaFLOP)505254565860626466Accuracy
 (f) Winogrande
Figure 5: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. Additional
plots can be found in Appendix J.
we nevertheless need to preprocess the training data
into contexts of the same length. We find that we
are able to teach the model how to efficiently handle
substantially larger batch sizes by finetuning with
progressively increasing sequence length. Specif-
ically, we first double the sequence length from
1024 to 2048 and finetune for 10B tokens from the
original pretraining corpus, then we double again
to 4096 for 100B tokens from the same corpus, and
finally double to 8192 tokens for another 100B to-
kens from the same corpus. In Fig. 6 we show that
increasing context length leads to lower test loss
on the Pile, an indication that RWKV can make
effective use of long contextual information.
2123252729211
Context Length2122Pile test loss
7B 8k
14B 8k
Figure 6: RWKV shows decreasing mean test loss as a
function of context length on the Pile (Gao et al., 2020)5.3 Long Context Benchmarks
Additionally, we evaluate our model’s ability to
handle very long sequences by comparing to state-
of-the-art long sequence models on the Long-
Range Arena (LRA) benchmark (Tay et al., 2021).
LRA is designed to assess the performance of mod-
els in handling lengthy context situations. It in-
cludes a collection of tasks with sequences rang-
ing from 1,000 to 16,000 tokens, covering various
types of data like text, natural language, synthetic
images, and mathematical expressions. We apply
RWKV on the LRA benchmark and the results are
in Appendix J.2. The results show that RWKV per-
forms second only to the S4 model in five datasets.
6 Inference Experiments
We benchmark inference requirements according to
size and family. Specifically, we evaluate text gen-
eration speed and memory requirements on typical
compute platforms including CPU (x86) and GPU
(NVIDIA A100 80 GB). For all of our inference
experiments we use float32 precision and the Hug-
gingFace Transformers (Wolf et al., 2020). We in-
clude all model parameters in the parameter count,
including both embedding and non-embedding lay-
ers. Performance under different quantization se-
tups is left to further work. See Appendix K for

--- PAGE 8 ---
more results.
Figure 7: Cumulative time on text generation for LLMs.
Unlike transformers, RWKV exhibits linear scaling.
7 Future Work
There are several promising directions for future
work on the RWKV architecture. Work can be
done to increase model expressivity by enhancing
the time-decay formulations and exploring initial
model states while maintaining efficiency.
The RWKV computational efficiency can be
further improved by applying a parallel scan in
thewkv tstep to reduce the computational cost to
O(Blog(T)d).
The mechanisms used in RWKV can be applied
to encoder-decoder architectures, potentially re-
placing the cross-attention mechanism. This could
be applicable in seq2seq or multimodal settings,
thereby enhancing efficiency during both training
and inference.
RWKV’s state (or context ) can be leveraged for
interpretability, predictability in sequence data, and
safety. Manipulating the hidden state could also
guide behavior and allow greater customizability
through prompt tuning.
The RWKV architecture is not perfect, and can
be improved via many aspects, such as modifying
the formulae or implementing larger internal states.
Larger states can enhance the model’s memory to
previous context and improve performance over
various tasks.
8 Conclusions
We introduced RWKV , a new approach to RNN
models exploiting the potential of time-based mix-
ing components. RWKV introduces several key
strategies that allow it to capture locality and long-
range dependencies while addressing limitations of
current architectures by: (1) replacing the quadratic
QK attention with a scalar formulation at linearcost, (2) reformulating recurrence and sequential
inductive biases to enable efficient training paral-
lelization and efficient inference, and (3) enhancing
training dynamics using custom initializations.
We benchmark the proposed architecture in a
wide variety of NLP tasks and show comparable
performance to SoTA with reduced cost. Further
experiments on expressivity, interpretability, and
scaling showcase the model capabilities and draw
parallels in behavior between RWKV and other
LLMs.
RWKV opens a new route for scalable and ef-
ficient architectures to model complex relation-
ships in sequential data. While many alternatives
to Transformers have been proposed with similar
claims, ours is the first to back up those claims with
pretrained models with tens of billions of parame-
ters.
9 Limitations
While our proposed RWKV model has demon-
strated promising results regarding training and
memory efficiency during inference, some limita-
tions should be acknowledged and addressed in
future work.
First, the linear attention of RWKV leads to sig-
nificant efficiency gains but still, it may also limit
the model’s performance on tasks that require re-
calling minutiae information over very long con-
texts. This is due to the funneling of informa-
tion through a single vector representation over
many time steps, compared with the full informa-
tion maintained by the quadratic attention of stan-
dard Transformers. In other words, the model’s
recurrent architecture inherently limits its ability to
“look back” at previous tokens, as opposed to tra-
ditional self-attention mechanisms. While learned
time decay helps prevent the loss of information,
it is mechanistically limited compared to full self-
attention.
Another limitation of this work is the increased
importance of prompt engineering in comparison to
standard Transformer models. The linear attention
mechanism used in RWKV limits the information
from the prompt that will be carried over to the
model’s continuation. As a result, carefully de-
signed prompts may be even more crucial for the
model to perform well on tasks.
The above RWKV property was confirmed by
studies on prompt engineering presented in Ap-
pendix L. By changing the order of the information

--- PAGE 9 ---
pieces, we were even able to almost double the
RWKV performance for some tasks.
10 Ethics Statement
In this paper, we present a novel architecture for se-
quential data processing and prove its effectiveness
by building a series of LLMs trained on publicly re-
leased pretraining data (Gao et al., 2020; Biderman
et al., 2022) and later fine-tuned on publicly avail-
able instructions (Taori et al., 2023; Chaudhary,
2023; Cheung, 2023; Anand et al., 2023; Anony-
mous, 2023; Yang, 2023; Ji et al., 2023a,b).
As a novel architecture for sequential data,
RWKV has the potential to improve sequence-
based models across different applications ranging
from natural language processing to biomedical
data processing or climate modelling. Since the
training code is released open source, RWKV con-
tributes to the democratization of AI, levels the
playing field, and empowers members of the Open
Source community to inspect, study, and finetune
RWKV in particular tasks. Moreover, it contributes
to advancing the understanding of LLMs capabili-
ties and limitations. A significant amount of work
has been devoted to increasing the efficiency of
RWKV training so as to minimize its cost and pro-
mote accessibility.
As LLMs trained on public data, RWKV’s lower
inference cost compared to Transformer alterna-
tives makes it more suitable for deployment in con-
sumer and edge hardware, which is a step towards
the democratization and distribution of LLMs to
the general public, creating better privacy and own-
ership incentives. It also lowers the resource bar-
rier to Chat assistants and text generation for small
and/or underrepresented communities. PreTrained
model weights for different sizes ranging from 0.1B
to 14B parameters trained on multiple languages
are released to increase ease of adoption and allow
for the study of emergent phenomena.
On the other hand, with lower resource barri-
ers, the spreading of AI-generated text might be-
come more prevalent. Current RWKV LLMs may
exhibit and/or reproduce biases and potentially
harmful content present in the data used for train-
ing. Nonetheless, mitigation and finetuning strate-
gies discussed for other, large Transformer models
should be applicable to RWKV as well.Acknowledgements
We thank StabilityAI for the compute used to train
our models and for technical support in develop-
ment of RWKV . We also thank the members of the
RWKV and EleutherAI Discord servers for their
help and work on further extending the applicabil-
ity of RWKV to different domains.
References
Mohammad Mahmudul Alam, Edward Raff, Stella Bi-
derman, Tim Oates, and James Holt. 2023. Recasting
self-attention with holographic reduced representa-
tions. arXiv preprint arXiv:2305.19534 .
Alon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor
Pryor, Luke Yoffe, Deepak Ramachandran, Lise
Getoor, Jay Pujara, and William Yang Wang. 2022.
FETA: A benchmark for few-sample task transfer
in open-domain dialogue. In Proceedings of the
2022 Conference on Empirical Methods in Natu-
ral Language Processing , pages 10936–10953, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Yuvanesh Anand, Zach Nussbaum, Brandon Duder-
stadt, Benjamin Schmidt, and Andriy Mulyar. 2023.
Gpt4all: Training an assistant-style chatbot with large
scale data distillation from gpt-3.5-turbo. https:
//github.com/nomic-ai/gpt4all .
Anonymous. 2023. Sharegpt_vicuna_unfiltered.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-
ton. 2016. Layer normalization.
Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. TweetEval:
Unified benchmark and comparative evaluation for
tweet classification. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
1644–1650, Online. Association for Computational
Linguistics.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv:2004.05150 .
Stella Biderman, Kieran Bicheno, and Leo Gao.
2022. Datasheet for the pile. arXiv preprint
arXiv:2201.07311 .
Stella Biderman, USVSN Sai Prashanth, Lintang
Sutawika, Hailey Schoelkopf, Quentin Anthony,
Shivanshu Purohit, and Edward Raf. 2023a. Emer-
gent and predictable memorization in large language
models. arXiv preprint arXiv:2304.11158 .
Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, et al. 2023b. Pythia: A suite
for analyzing large language models across training
and scaling. arXiv preprint arXiv:2304.01373 .

--- PAGE 10 ---
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
Gao, and Yejin Choi. 2020. Piqa: Reasoning about
physical commonsense in natural language. In Thirty-
Fourth AAAI Conference on Artificial Intelligence .
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and
Stella Biderman. 2021. Gpt-neo: Large scale autore-
gressive language modeling with mesh-tensorflow.
URL: https://doi. org/10.5281/zenodo , 5297715.
Sidney Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al.
2022. Gpt-neox-20b: An open-source autoregres-
sive language model. In Proceedings of BigScience
Episode \# 5–Workshop on Challenges & Perspec-
tives in Creating Large Language Models , pages 95–
136.
James Bradbury, Stephen Merity, Caiming Xiong, and
Richard Socher. 2017. Quasi-recurrent neural net-
works. In ICLR .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev.
2023. Scaling transformer to 1m tokens and beyond
with rmt.
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
2022. Recurrent memory transformer. Advances
in Neural Information Processing Systems , 35:11079–
11091.
Sahil Chaudhary. 2023. Code alpaca: An instruction-
following llama model for code generation. https:
//github.com/sahil280114/codealpaca .
Joseph Cheung. 2023. Guanacodataset.
Krzysztof Choromanski, Valerii Likhosherstov, David
Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
Lukasz Kaiser, David Belanger, Lucy Colwell, and
Adrian Weller. 2020. Rethinking attention with per-
formers.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CoRR , abs/2204.02311.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In NIPS 2014 Deep Learning and Representa-
tion Learning Workshop .
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surprising
difficulty of natural yes/no questions. arXiv preprint
arXiv:1905.10044 .
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. In
arXiv:1803.05457 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Ja-
cob Hilton, Reiichiro Nakano, Christopher Hesse,
and John Schulman. 2021. Training verifiers to
solve math word problems. In arXiv , volume
abs/2110.14168.
Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and
Christopher Re. 2022a. Flashattention: Fast and
memory-efficient exact attention with IO-awareness.
InAdvances in Neural Information Processing Sys-
tems.
Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W
Thomas, Atri Rudra, and Christopher Ré. 2022b.
Hungry hungry hippos: Towards language mod-
eling with state space models. arXiv preprint
arXiv:2212.14052 .
Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo
Ko, Alan S. Cowen, Gaurav Nemade, and Sujith Ravi.
2020. Goemotions: A dataset of fine-grained emo-
tions. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020 , pages 4040–4054.
Association for Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2020.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 .
Albert Gu, Karan Goel, and Christopher Ré. 2021. Effi-
ciently modeling long sequences with structured state
spaces. arXiv preprint arXiv:2111.00396 .

--- PAGE 11 ---
Albert Gu, Karan Goel, and Christopher Ré. 2022. Effi-
ciently modeling long sequences with structured state
spaces. In The International Conference on Learning
Representations (ICLR) .
Mandy Guo, Joshua Ainslie, David C Uthus, Santiago
Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei
Yang. 2022. Longt5: Efficient text-to-text trans-
former for long sequences. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022 ,
pages 724–736.
Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Di-
agonal state spaces are as effective as structured state
spaces. Advances in Neural Information Processing
Systems , 35:22982–22994.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Identity mappings in deep residual net-
works.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen,
Christopher Hesse, Jacob Jackson, Heewoo Jun,
Tom B Brown, Prafulla Dhariwal, Scott Gray, et al.
2020. Scaling laws for autoregressive generative
modeling. arXiv preprint arXiv:2010.14701 .
Sepp Hochreiter. 1998. The vanishing gradient problem
during learning recurrent neural nets and problem so-
lutions. International Journal of Uncertainty, Fuzzi-
ness and Knowledge-Based Systems , 6(02):107–116.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation , 9(8):1735–
1780.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes
Welbl, Aidan Clark, Tom Hennigan, Eric Noland,
Katie Millican, George van den Driessche, Bogdan
Damoc, Aurelia Guy, Simon Osindero, Karen Si-
monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,
and Laurent Sifre. 2022. Training compute-optimal
large language models.
Hassan Ismail Fawaz, Germain Forestier, Jonathan We-
ber, Lhassane Idoumghar, and Pierre-Alain Muller.
2019. Deep learning for time series classification:
a review. Data mining and knowledge discovery ,
33(4):917–963.
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol
Vinyals, Andrew Zisserman, and Joao Carreira. 2021.
Perceiver: General perception with iterative atten-
tion. In International conference on machine learn-
ing, pages 4651–4664. PMLR.
Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee,
and Jangwoo Kim. 2019. Mnnfast: A fast and scal-
able system architecture for memory-augmented neu-
ral networks. In Proceedings of the 46th Interna-
tional Symposium on Computer Architecture , pages
250–263.Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang
Niu, Baochang Ma, and Xiangang Li. 2023a. Belle:
Be everyone’s large language model engine. https:
//github.com/LianjiaTech/BELLE .
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang
Niu, Lei Zhang, Baochang Ma, and Xiangang Li.
2023b. Exploring the impact of instruction data
scaling on large language models: An empirical
study on real-world use cases. arXiv preprint
arXiv:2303.14742 .
Matt Gardner Johannes Welbl Nelson F. Liu. 2017.
Crowdsourcing multiple choice science questions.
InDOI:10.18653/v1/W17-4413 .
John Jumper, Richard Evans, Alexander Pritzel, Tim
Green, Michael Figurnov, Olaf Ronneberger, Kathryn
Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna
Potapenko, and et al. 2021. Highly accurate pro-
tein structure prediction with alphafold. Nature ,
596(7873):583–589.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models. arXiv
preprint arXiv:2001.08361 .
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
rnns: Fast autoregressive transformers with linear
attention. In International Conference on Machine
Learning , pages 5156–5165. PMLR.
Nikita Kitaev, L. Kaiser, and Anselm Levskaya.
2020. Reformer: The efficient transformer. ArXiv ,
abs/2001.04451.
Jan Koco ´n, Igor Cichecki, Oliwier Kaszyca, Mateusz
Kochanek, Dominika Szydło, Joanna Baran, Julita
Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil
Kanclerz, Anna Koco ´n, Bartłomiej Koptyra, Wik-
toria Mieleszczenko-Kowszewicz, Piotr Miłkowski,
Marcin Oleksy, Maciej Piasecki, Łukasz Radli ´nski,
Konrad Wojtasik, Stanisław Wo´ zniak, and Prze-
mysław Kazienko. 2023. Chatgpt: Jack of all trades,
master of none. Information Fusion , page 101861.
Jan Koco ´n, Piotr Miłkowski, and Monika Za ´sko-
Zieli ´nska. 2019. Multi-level sentiment analysis of
polemo 2.0: Extended corpus of multi-domain con-
sumer reviews. In Proceedings of the 23rd Confer-
ence on Computational Natural Language Learning
(CoNLL) , pages 980–991.
Phong Le and Willem Zuidema. 2016. Quantifying
the vanishing gradient and long distance dependency
problem in recursive neural networks and recursive
lstms. In Proceedings of the 1st Workshop on Repre-
sentation Learning for NLP , pages 87–93.
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile
Saulnier, Stas Bekman, M Saiful Bari, Stella Bider-
man, Hady Elsahar, Jason Phang, Ofir Press, et al.

--- PAGE 12 ---
2022. What language model to train if you have one
million gpu hours? In Proceedings of BigScience
Episode #5–Workshop on Challenges & Perspectives
in Creating Large Language Models .
Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav
Artzi. 2018. Simple recurrent units for highly paral-
lelizable recurrence. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing , pages 4470–4481, Brussels, Belgium.
Association for Computational Linguistics.
Hanxiao Liu, Zihang Dai, David R. So, and Quoc V . Le.
2021. Pay attention to mlps.
Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,
Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021.
Luna: Linear unified nested attention. Advances
in Neural Information Processing Systems , 34:2441–
2453.
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,
Liangke Gui, Graham Neubig, Jonathan May, and
Luke Zettlemoyer. 2023. Mega: Moving average
equipped gated attention. In ICLR .
Eric Martin and Chris Cundy. 2017. Parallelizing linear
recurrent neural nets over sequence length. ArXiv ,
abs/1709.04057.
Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual asso-
ciations in GPT. Advances in Neural Information
Processing Systems , 36.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question answer-
ing. In EMNLP .
Niklas Muennighoff, Alexander M Rush, Boaz Barak,
Teven Le Scao, Aleksandra Piktus, Nouamane Tazi,
Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
2023. Scaling data-constrained language models.
arXiv preprint arXiv:2305.16264 .
OpenAI. 2022. Introducing chatgpt. https://openai.
com/blog/chatgpt .
Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan
Fernando, Caglar Gulcehre, Razvan Pascanu, and
Soham De. 2023. Resurrecting recurrent neu-
ral networks for long sequences. arXiv preprint
arXiv:2303.06349 .
Denis Paperno, Germán Kruszewski, Angeliki Lazari-
dou, Ngoc Quan Pham, Raffaella Bernardi, Sandro
Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
Fernandez. 2016. The LAMBADA dataset: Word
prediction requiring a broad discourse context. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 1525–1534, Berlin, Germany.
Association for Computational Linguistics.Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zach DeVito, Martin Raison, Alykhan Tejani,
Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-
jie Bai, and Soumith Chintala. 2019. Pytorch: An
imperative style, high-performance deep learning li-
brary.
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y
Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-
fano Ermon, and Christopher Ré. 2023. Hyena hierar-
chy: Towards larger convolutional language models.
arXiv preprint arXiv:2302.10866 .
Ilan Price, Jordan Gifford-Moore, Jory Flemming, Saul
Musker, Maayan Roichman, Guillaume Sylvain,
Nithum Thain, Lucas Dixon, and Jeffrey Sorensen.
2020. Six attributes of unhealthy conversations. In
Proceedings of the Fourth Workshop on Online Abuse
and Harms , pages 114–124, Online. Association for
Computational Linguistics.
Markus N. Rabe and Charles Staats. 2022. Self-
attention does not need o(n2)memory.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He. 2020. Deepspeed: System opti-
mizations enable training deep learning models with
over 100 billion parameters. In Proceedings of the
26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining , KDD ’20,
page 3505–3506, New York, NY , USA. Association
for Computing Machinery.
Melissa Roemmele, Cosmin Adrian Bejan, , and An-
drew S. Gordon. 2018. Choice of plausible alterna-
tives: An evaluation of commonsense causal reason-
ing. In AAAI .
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
Ramsha Siddiqui. 2019. SARCASMANIA: Sarcasm
Exposed! http://www.kaggle.com/rmsharks4/
sarcasmania-dataset . [Online; accessed 02-
February-2023].
David R. So, Wojciech Manke, Hanxiao Liu, Zihang
Dai, Noam Shazeer, and Quoc V . Le. 2021. Primer:
Searching for efficient transformers for language
modeling. CoRR , abs/2109.08668.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,
Zhe Zhao, and Che Zheng. 2020. Synthesizer: Re-
thinking self-attention in transformer models.

--- PAGE 13 ---
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. 2021. Long
range arena : A benchmark for efficient transformers.
InInternational Conference on Learning Representa-
tions .
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-
zler. 2022. Efficient transformers: A survey. ACM
Computing Surveys , 55(6):1–28.
Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov,
Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-
sica Yung, Andreas Steiner, Daniel Keysers, Jakob
Uszkoreit, Mario Lucic, and Alexey Dosovitskiy.
2021. Mlp-mixer: An all-mlp architecture for vi-
sion. CoRR , abs/2105.01601.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
David Vilares and Carlos Gómez-Rodríguez. 2019.
Head-qa: A healthcare dataset for complex reasoning.
InACL.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel Bowman. 2019. Superglue: A stickier
benchmark for general-purpose language understand-
ing systems. In Advances in Neural Information
Processing Systems , volume 32. Curran Associates,
Inc.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. 2018. GLUE:
A multi-task benchmark and analysis platform for nat-
ural language understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing
and Interpreting Neural Networks for NLP , pages
353–355, Brussels, Belgium. Association for Com-
putational Linguistics.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,
and Hao Ma. 2020. Linformer: Self-attention with
linear complexity.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf-
fel, Barret Zoph, Sebastian Borgeaud, Dani Yo-
gatama, Maarten Bosma, Denny Zhou, Donald Met-
zler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals,
Percy Liang, Jeff Dean, and William Fedus. 2022a.
Emergent abilities of large language models. ArXiv ,
abs/2206.07682.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Perric
Cistac, Clara Ma, Yacine Jernite, Julien Plu, Can-
wen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush.
2020. Transformers: State-of-the-Art Natural Lan-
guage Processing. pages 38–45. Association for
Computational Linguistics.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong
Long, Chengqi Zhang, and S Yu Philip. 2020. A com-
prehensive survey on graph neural networks. IEEE
transactions on neural networks and learning sys-
tems, 32(1):4–24.
Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.
Ex machina: Personal attacks seen at scale. In Pro-
ceedings of the 26th International Conference on
World Wide Web, WWW 2017, Perth, Australia, April
3-7, 2017 , pages 1391–1399. ACM.
Jianxin Yang. 2023. Firefly. https://github.com/
yangjianxin1/Firefly .
Manzil Zaheer, Guru Guruganesh, Kumar Avinava
Dubey, Joshua Ainslie, Chris Alberti, Santiago On-
tanon, Philip Pham, Anirudh Ravula, Qifan Wang,
Li Yang, et al. 2020. Big bird: Transformers for
longer sequences. Advances in Neural Information
Processing Systems , 33.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machine really finish your sentence? In ACL.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2020. Winogrande: An
adversarial winograd schema challenge at scale. In
ACL.
Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen
Huang, Hanlin Goh, Ruixiang Zhang, and Josh
Susskind. 2021. An attention free transformer.
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng
Gao, Kevin Duh, and Benjamin Van Durme. 2018.
Record: Bridging the gap between human and ma-
chine commonsense reading comprehension. In
arXiv:1810.12885 .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .

--- PAGE 14 ---
A Author Contributions
All authors contributed to the drafting of this paper. Eric Alcaide and Quentin Anthony organized the
paper and its experiments and were involved in all phases of the development process.
Model Design and Development Bo Peng (lead), Matteo Grella, Xuzheng He, Haowen Hou, Jiaming
Kong, Johan S. Wind
Model Training Bo Peng
Scaling Laws Analysis Stella Biderman, Bo Peng
Benchmark Evaluations Stella Biderman (lead), Kranthi Kiran GV , Krishna Sri Ipsit Mantri, Atsushi
Saito, Qihang Zhao, Peng Zhou, Rui-Jie Zhuåç
Long Context Experiments Xingjian Du, Rui-Jie Zhu, Bolun Wang, Ruichong Zhang, Jian Zhu,
Rui-Jie Zhu
Inference Speed Experiments Samuel Arcadinho, Przemysław Kazienko, Qinghua Zhou
Information Flow Experiments Huanqi Cao, Michael Chung, Matteo Grella, Ferdinand Mom,
Zhenyuan Zhang
Chat Experiments Jan Koco ´n (lead), Przemysław Kazienko, Bartłomiej Koptyra, Hayden Lau, Xiangru
Tang, Stanisław Wo´ zniak, Zhenyuan Zhang
Ethics and Broader Impacts Stella Biderman, Guangyu Song

--- PAGE 15 ---
B Author Contributions
Bo Peng Original RWKV idea, original code, performance optimizations, original experiments, and
trained RWKV models from 0.1B to 14B.
Eric Alcaide Manuscript (initial draft sections 1, C; sections 3, 7 and 8; revision and proofreading; final
version ). Figures (2, 3, 3, 8). Experiments section 6. Appendices E, K. Contributions to Appendix M.
Quentin Anthony Manuscript (organization, initial draft sections 1, C, 2; revision and proofreading;
final version).
Alon Albalak Manuscript (abstract and sections 1, 9, and 7; proofreading and revision).
Samuel Arcadinho Contributions to Figures 7, 13, and 14. Contributions to Appendix K.
Stella Biderman Performed the scaling laws analysis and evaluated competitor models on benchmark
tasks.
Huanqi Cao Manuscript (contributions to 3.2 and 3.3; proofreading and revision). Experiments for
Appendix I.
Xin Cheng Manuscript (proofreading and revision). Contributions to Appendix M, J.
Michael Chung Manuscript (contributions to section I; proofreading and revision).
Xingjian Du Evaluation on Long Range Arena Benchmark (TBD until 5.31).
Matteo Grella Manuscript (sections H, I, 8; contributions to sections 1, 7 and 9; proofreading and
revision). Contributions to Appendix D.
Kranthi Kiran GV Manuscript (sections C and 5; contributions to section 2; revision and proofreading).
Tables K and K. Appendix 4.
Xuzheng He Manuscript (contributions to section 2; proofreading and revision). Contributions to
Figure8. Appendix I. Contributions to appendix H.
Haowen Hou Figure 9. Appendix F.
Jiaju Lin RWKV on LRA benchmarking
Przemysław Kazienko Manuscript (proofreading and revision). Contributions to Section 6, 9, and
Appendix L.
Jan Kocon Manuscript (Section 1; proofreading and revision). Contributions to Appendix L.
Jiaming Kong Manuscript (revision and proofreading). Appendix H.
Bartłomiej Koptyra Manuscript (revision and proofreading) Contributions to Appendix L.
Hayden Lau Manuscript (contributions to section 1 and 9; proofreading and revision). Contributions to
Appendix M.
Krishna Sri Ipsit Mantri Figure 12
Ferdinand Mom Manuscript (contributions to section 1, C, 3.3, I; proofreading and revision). Contribu-
tions to Appendix D.
Atsushi Saito Manuscript (sections 2 and 5; contributions to section C). Contributions to Appendix J
Guangyu Song Manuscript (rewrote section 3; final version). Initial draft Ethics Statement).
Xiangru Tang Manuscript (sections C and 2; contributions to abstract; revision and proofreading).
Contributions to Appendix M.

--- PAGE 16 ---
Bolun Wang Contributions to Tables 1.
Johan S. Wind RWKV performance optimizations (CUDA), Contributions to Appendix 4.
Stanisław Wo´ zniak Contributions to Appendix L.
Ruichong Zhang Manuscript (proofreading and revision); Contributions to Figure 6 and Appendix M.
Zhenyuan Zhang Manuscript (revision and proofreading). Figure 3. Experiments Appendix I. Contri-
butions to Appendices D and M.
Qihang Zhao Manuscript (proofreading and revision). Contributions to Table 5.
Peng Zhou Contributions to Tables 1 and Table 5.
Qinghua Zhou Manuscript (Proofreading and revision of section 3; Add missing citations in 3.3).
Revision of Figures 2 and 12.
Jian Zhu Manuscript (section C; proofreading and revision). Figures 3 and 6.
Rui-Jie Zhu Tables 1 and 5. Experiments for table 5.
C Additional Related Work
Recently, a number of techniques have been proposed to address the limitations of transformers.
Optimizing Attention Mechanism Many transformer variants (“x-formers”) have been introduced to
reduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020;
Kitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al.,
2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other
efficient methods (Katharopoulos et al., 2020; Jaegle et al., 2021).
Some recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022; Jang et al.,
2019) share similarities with RWKV’s chunked computation scheme. Despite being memory-efficient,
their time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV
achieves better space and time complexity during inference by formulating a linear attention as an RNN.
Attention Free Models Another line of research replaces the attention mechanism with other modules to
scale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021; Liu et al., 2021) propose replacing
attention by Multi-Layer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer
(AFT) (Zhai et al., 2021) and HrrFormer (Alam et al., 2023) replaces dot-product self-attention with a
computationally efficient alternative. None of these models have been successfully scaled to the point
where drawing comparisons with transformer-based large language models makes sense.
There has also been substantial research into state space models (SSM) (Gu et al., 2021) and its variants
(Dao et al., 2022b; Gupta et al., 2022; Poli et al., 2023). In contrast to the preceding models, SSM and
its successors have shown substantial progress towards efficient scaling. Simultaneously with this work,
Poli et al. (2023) train SSM-based models with 125 million and 355 million parameters and show that the
performance is on-par with a transformer that uses a mix of local and global attention (Black et al., 2021).
Advances in RNNs Inspired by the success of transformers, RNN-style (Hochreiter and Schmidhuber,
1997; Chung et al., 2014) recursive components have also been modified to increase context length, such
as the Recurrent Memory Transformer (Bulatov et al., 2022, 2023) and Linear Recurrent Units (Orvieto
et al., 2023). Most similar to our work, the Quasi-Recurrent neural network (QRNN) (Bradbury et al.,
2017) uses both convolutional layers and recurrent pooling functions across timesteps and channels.
While QRNN utilizes convolutional filters with fixed sizes, RWKV employs a time-mixing module as
an attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN,
RWKV includes a parametrized channel-mixing module that is parallelizable.

--- PAGE 17 ---
D Time-Mixing Block as an RNN Cell
As stated in 3.3, the RWKV time-mixing block can be formulated as an RNN, as the WKV computation
can be written in such a recursive form:
a0, b0= 0, (19)
wkv t=at−1+eu+kt⊙vt
bt−1+eu+kt, (20)
at=e−w⊙at−1+ekt⊙vt, (21)
bt=e−w⊙bt−1+ekt. (22)
1 2
e3
Figure 8: RWKV time-mixing block formulated as an
RNN cell. Color codes: yellow ( µ) denotes the token
shift, red (1) denotes the denominator, blue (2) denotes the
numerator, and pink (3) denotes the fraction computations
in 16. hdenotes the numerator-denominator tuple.The dataflow of the RNN-like time-mixing is
shown in Fig. 8, where the hidden states his
the numerator-denominator tuple (a, b). To avoid
overflow in calculating ekt, a numerical trick is
used in the official implementation. Noticing
thata1=ek1⊙v1andb1=ek1, we set a′
1=
v1, b′
1= 1, p1=k1, where ptstores the shared
exponents of atandbt. Now the above recursion
can be converted into a numerical safe version,
for each time step t >1:
q:= max( pt−1, u+kt), (23)
wkv t=ept−1−q⊙a′
t−1+eu+kt−q⊙vt
ept−1−q⊙b′
t−1+eu+kt−q.(24)
The update to a′
t, b′
t, and their shared exponent is
also carried out in a similar fashion:
q′:= max( pt−1−w, kt), (25)
a′
t=ept−1−w−q′⊙a′
t−1+ekt−q′⊙vt, (26)
b′
t=ept−1−w−q′⊙b′
t−1+ekt−q′, (27)
pt=q′. (28)
The RWKV model has an internal state that stores some previous information. In each layer, the internal
state consists five parts, each of which is a vector with Dnumbers, where Dis the model dimension. The
five parts are:
• The current input of the Time-mix block xt;
• The current input of the Channel-mix block yt;
• The numerator of the WKV value a′
t, as defined in equation (26);
• The denominator of the WKV value b′
t, as defined in equation (27);
•An auxiliary state ptin(28), which is used for WKV computation to maintain numerical precision.
Which yields a total size of 5DLparameters. It is worth noting that in an algebraic context with infinite
precision, the helper state ptcan be ignored, and the WKV numerator and denominator can be computed
directly using equations (21) and (22), reducing the size of the internal state to 4DL.
E Parameter initializations
We describe the specific parameter initializations below and motivate the design choices. Parameters
belonging to residual blocks are often adjusted by layer depth and total number of layers. Let #denote
the vocabulary size, sdenote the embedding dimension, ddenote the hidden size (we use d= 4s),Lthe
number of layers, lthe layer index (from 0 to L−1), we use the following initializations:

--- PAGE 18 ---
• Embeddings are initialized to U(±1×10−4) as explained in 3.4
•For the time-mixing blocks (11, 12, 13), initializations are µki= (i
s)1−l
L,µvi= (i
s)1−l
L+0.3l
L−1and
µri=1
2·(i
s)1−l
L
• For the channel-mixing blocks (14, 15), µkiandµriare initialized to (i
s)1−l
L
•wi(16), also known as “time decay”, is initialized to −5 + 8·(i
d−1)0.7+1.3l
L−1. Intuitively, it is the
discount factor applied to previous tokens over time.
•ui(16), also known as “bonus”, is set to 0.5·(((i+ 1) mod 3) −1) + log 0 .3. It is the special
weighting applied to the current token in equation 16. The alternating zigzag pattern initially
creates subtle variations in the tensor elements, which are intended to help the model treat different
dimensions of the embedding distinctively.
•Wo(17) (time-mixing) and Wv(channel-mixing) are initialized to N(0,q
d
s= 2)
•All other Wr, Wk, Wvweights are initialized to 0 so the model can start learning from the beginning
without noisy signals.
• All LayerNorm weights start from 1 and biases from 0.
F Small Init Embedding
This section presents the experimental validation of small initialization embedding. The experimental
setup is as follows. In the baseline configuration, the parameters are initialized using a normal distribution
with a mean of 0.0 and a standard deviation of 0.02, which is a commonly used initialization method
in models like BERT and GPT. On the other hand, in the small initialization of the embedding (small
init emb) experiment, the parameters are initialized using a uniform distribution with a range of 1e-4,
which is slightly different from RWKV where a normal distribution with a standard deviation of 1e-4
is used. However, this difference is negligible and does not affect our conclusions. The experiments
were conducted with a batch size of 400. As depicted in Figure 9, the loss curve for the small init emb
exhibits a faster rate of decrease and convergence compared to the traditional initialization using a normal
distribution.
0 10000 20000 30000 40000 50000
Step4567891011LossBaseline
Small Init Emb
Figure 9: Effect of small initialization embedding.
G Hyperparameters
To train the models mentioned, we use ϵ= (0.9,0.99)without weight decay for the Adam optimizer, and
switch batch size dynamically between 128 or 256 sequences, each of 1024 tokens. We further organize

--- PAGE 19 ---
Model 169M 430M 1.5B 3B 7B 14B
Init LR 0.0006 0.0004 0.0003 0.00015 0.00015 0.0001
Warmup Mini-Epochs 361 411 443 451 465 544
End LR 0.00001 0.00001 0.00001 0.00001 0.00001 0.000007
Table 3: Hyperparameters for our learning rate (LR) schedule of the pretrained models.
the training into multiple mini-epochs, each of 40320 samples, to guide our learning rate schedule. The
training process takes 8043 mini-epochs to make one pass over the Pile. The initial warming up mini-
epochs have a constant learning rate of “Init LR”. After the warming up mini-epochs, the learning rate
exponentially decays until in the last mini-epoch, in which the model finishes training on the entire Pile,
the learning rate arrives at the “End LR”. The related hyperparameters are shown in Table 3.
H Gradient Stability in RWKV
In this section, we present a mathematical description of the gradient stability property in RWKV , focusing
specifically on the time-mixing block. By gradient stability we mean that if the inputs xtare bounded and
the model parameters are fixed, then the gradients with respect to WkandWvare uniformly bounded for
allT(thus not exploding). Consequently, we can control the amount each xtcontributes to the gradient at
Tin a naturally decaying fashion by the weight decay mechanism w(thus not vanishing unless desired).
First, we make the simplification that there are no token shifts, this will not affect the final conclusion.
In this scenario, wkv Tcan be written as
wkv T=PT
t=1Ke
t⊙vtPT
t=1Ke
t= E(vt) =S(vt)
S(1), (29)
where
vt=Wvxt,∂(vt)i
∂(Wv)i,j= (xt)j,
Ke
t=eWkxt+wT,t,∂(Ke
t)i
∂(Wk)i,j= (xt)j(Ke
t)i,
andS(·)andE(·)are shorthand for denoting sums and averages over weights Ke
t.
The loss function at position Tcan be written as
LT=l(f(wkv T), yT). (30)
Because wkv Trelates to (Wk)i,jand(Wv)i,jonly through the i-th channel (wkv T)i, we have
∂LT
∂(Wv)i,j=∂LT
∂(wkv T)i∂(wkv T)i
∂(Wv)i,j. (31)
The first part of the above equation contains trivial operations like output layers, and other layers of
time-mixing, which can be proven inductively. The second part of the above equation can be bounded as
∂(wkv T)i
∂(Wv)i,j=∂Ei[(vt)i]
∂(Wv)i,j
=|Ei[(xt)j]| ≤max
t|(xt)j|, (32)

--- PAGE 20 ---
which is irrelevant to T. Similarly,
∂(wkv T)i
∂(Wk)i,j=∂Si[(vt)i]
Si(1)/∂(Wk)i,j
=Si[(xt)j(vt)i]
Si(1)−Si[(xt)j]Si[(vt)i]
Si(1)2
= E i[(xt)j(vt)i]−Ei[(xt)j]Ei[(vt)i]
= cov i((xt)j,(vt)i) (33)
can also be bounded. Note that wkv’s softmax operation contains at least two non-zero terms ( uandw),
so the above “covariance” will not degenerate into 0.
I Model Behavior Visualization
0 100 200 300 400 500 600 700 800
Channel0.00.20.40.60.81.0Time DecayTime decay (sorted along channel axis)
Layer 1
Layer 2
Layer 3
Layer 4
Layer 5
Layer 6
Layer 7
Layer 8
Layer 9
Layer 10
Layer 11
Layer 12
Figure 10: Model behavior visualizations of RWKV .The right plot illustrates the time decays ( e−w)
in each layer of the RWKV-169M model, sorted
along the channel axis. Notably, several decays
in the last layers are very close or equal to one,
implying that certain information is preserved
and propagated throughout the model’s temporal
context. Meanwhile, many decays in the initial
layer are close to zero, which corresponds to lo-
cal operations in wkv (16), likely to be associated
with tasks such as text parsing or lexical analy-
sis. (Note that the local operations in wkv are
due to the extra parameter u, when e−wis de-
generated into 0.) These patterns of time decays
are partly learned, but also come from parameter
initialization as it speeds up training.
The plot below shows the information retrieval
and propagation path in the RWKV-430M model. The experiment follows the causal trace method
introduced by Meng et al. (2022), where we
1. Run the model once, and record all states and activation of each layer during the computation;
2. Corrupt the input embeddings of the subject using noise (“The Eiffel Tower” in this example);
3.Restore the states and activation of a certain layer at a certain token during the computation, and
record the log-probability of the model outputting the correct answer (“Paris”).
Unlike transformers, RWKV relies on the recursive propagation of information in the time dimension.
In this case, the fact that the Eiffel Tower is located in Paris is retrieved in layer 4 just after the model
sees “The Eiffel”. It is then passed down to the subsequent layers. In layer 20, mostly, the information is
propagated through time until reaching where it is needed. Finally, at the token “of”, it is passed down to
the last layer for outputting the answer.
J Additional Evaluations
J.1 Further details on NLP tasks
We evaluate on the following tasks:
ARC (Clark et al., 2018) A dataset designed for multiple-choice question answering, encompassing
science exam questions ranging from third grade to ninth grade. It has Easy and Challenge subsets that
we report results on separately.
BoolQ (Clark et al., 2019) A binary yes/no question answering benchmark.

--- PAGE 21 ---
1 6 11 16 21
LayerThe
 E
iff
el
 T ower
 is
 located
 in
 the
 city
 ofInformation propagation path
7
6
5
4
3
2
1
Log-probability of "Paris"Figure 11: Model behavior visualizations of the RWKV model.
COPA (Roemmele et al., 2018) A dataset to evaluate achievement in open-domain commonsense
causal reasoning.
HeadQA (Vilares and Gómez-Rodríguez, 2019) A benchmark consisting of graduate-level questions
encompassing various fields such as medicine, nursing, biology, chemistry, psychology, and pharmacology.
HellaSwag (Zellers et al., 2019) A novel benchmark for commonsense Natural Language Inference
(NLI) which is build by adversarial filtering against transformer models.
LAMBADA (Paperno et al., 2016) A benchmark dataset that evaluates the model’s contextual reasoning
and language comprehension abilities by presenting context-target pairs, where the objective is to predict
the most probable target token. We follow standard practice and use the untokenized version created by
OpenAI (Brown et al., 2020).
OpenBookQA (Mihaylov et al., 2018) A QA dataset to evaluate human comprehension of a subject by
incorporating open book facts, scientific knowledge, and perceptual common sense, drawing inspiration
from open book exams.
PIQA (Bisk et al., 2020) A benchmark for the task of physical common sense reasoning, which consists
of a binary choice task that can be better understood as a set of two pairs, namely (Goal, Solution).
ReCoRD (Zhang et al., 2018) A benchmark for evaluating commonsense reasoning in reading
comprehension by generating queries from CNN/Daily Mail news articles and requiring text span answers
from corresponding summarizing passages.
SciQ (Johannes Welbl Nelson F. Liu, 2017) A multiple-choice QA dataset which was created using
an innovative approach to gather well-crafted multiple-choice questions that are focused on a specific
domain.
Winogrande (Zellers et al., 2020) A dataset designed to evaluate the acquisition of common sense
reasoning by neural language models, aiming to determine whether we are accurately assessing the true
capabilities of machine common sense.

--- PAGE 22 ---
102103
Compute (exaFLOP)020406080100Accuracy
BLOOM
Pythia
OPT
RWKV(a) ARC (Challenge)
102103
Compute (exaFLOP)020406080100Accuracy
 (b) ARC (Easy)
102103
Compute (exaFLOP)020406080100Accuracy
 (c) BoolQ
102103
Compute (exaFLOP)020406080100Accuracy
BLOOM
Pythia
OPT
RWKV
(d) COPA
102103
Compute (exaFLOP)020406080100Accuracy
 (e) HeadQA
102103
Compute (exaFLOP)020406080100Accuracy
 (f) HellaSwag
102103
Compute (exaFLOP)020406080100Accuracy
BLOOM
Pythia
OPT
RWKV
(g) LAMBADA (OpenAI)
102103
Compute (exaFLOP)020406080100Accuracy
 (h) OpenBookQA
102103
Compute (exaFLOP)020406080100Accuracy
 (i) PiQA
102103
Compute (exaFLOP)020406080100Accuracy
BLOOM
Pythia
OPT
RWKV
(j) ReCoRD
102103
Compute (exaFLOP)020406080100Accuracy
 (k) SciQ
102103
Compute (exaFLOP)020406080100Accuracy
 (l) Winogrande
Figure 12: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks.

--- PAGE 23 ---
J.2 Evaluation on Long Range Arena
The Long-Range Arena (LRA) benchmark (Tay et al., 2021) is designed to assess the performance of
models in handling lengthy context situations. It includes a collection of tasks with sequences ranging
from 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images,
and mathematical expressions. We apply RWKV on the LRA benchmark and the report results are in
Table 4. Other models’ performances are directly cited from Gu et al. (2022); Alam et al. (2023).
Table 4: Evaluation on Long Range Arena. Other models reported in the literature (Gu et al., 2022; Alam et al.,
2023). Bolded values are the best.
MODEL LISTOPS TEXT RETRIEVAL IMAGE PATHFINDER PATH-X A VG
Transformer 36.37 64.27 57.46 42.44 71.40 ✗ 53.66
Reformer 37.27 56.10 53.40 38.07 68.50 ✗ 50.56
BigBird 36.05 64.02 59.29 40.83 74.87 ✗ 54.17
Linear Trans. 16.13 65.90 53.09 42.34 75.30 ✗ 50.46
Performer 18.01 65.40 53.82 42.77 77.05 ✗ 51.18
FNet 35.33 65.11 59.61 38.67 77.80 ✗ 54.42
Nyströmformer 37.15 65.52 79.56 41.58 70.94 ✗ 57.46
Luna-256 37.25 64.57 79.29 47.38 77.72 ✗ 59.37
Hrrformer 39.98 65.38 76.15 50.45 72.17 ✗ 60.83
S4 59.60 86.82 90.90 88.65 94.20 96.35 86.09
RWKV 55.88 86.04 88.34 70.53 58.42 ✗ 72.07
The results show that RWKV performs second only to the S4 model in five datasets. While RWKV
substantially underpreforms S4 on Image, Pathfinder, and Path-X, on the problems related to natural
language and computer code processing RWKV performs on par with S4 or nearly so.
J.3 Enwik8 Perplexity
We also evaluate our model in terms of perplexity on the Enwik8 dataset. Baseline comparisons are
made with Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020) (the best performing dense
version), Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020). L, d,
andTdenote the number of blocks (network depth), dimension of features, and sequence length, re-
spectively. Both Linear Transformer and Performer are implemented with customized CUDA kernels
(github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch.1No weight
decay nor dropout was used.2Trained with AdamW and weight decay set to 0.1, dropout of 0.1, batch
size of 16, and initial learning rate of 6e-4.
Method L d T Train bpc Test bpc Time Complexity Space Complexity
Transformer 12 512 1024 0.977 1.137 O(T2d) O(T2+Td)
Transformer 24 256 1024 1.039 1.130 O(T2d) O(T2+Td)
Reformer 12 512 1024 1.040 1.195 O(TlogTd) O(TlogT+Td)
Synthesizer 12 512 1024 0.994 1.298 O(T2d) O(T2+Td)
Linear Transformer 12 512 1024 0.981 1.207 O(Td2) O(Td+d2)
Performer 12 512 1024 1.002 1.199 O(Td2logd) O(Tdlogd+d2logd)
AFT-simple 12 512 1024 1.046 1.209 O(Td) O(Td)
RWKV-RNN16 512 1024 0.720 - O(Td) O(d)
RWKV-RNN212 512 1024 1.010 1.178 O(Td) O(d)
Table 5: Enwik8 results, measured in bits per character (bpc).
K Inference results
Figures 13 and 14 illustrate, respectively, the results on time (s) and memory (RAM, VRAM) requirements
for LLM inference in float32 precision. We benchmark the following model families and sizes:

--- PAGE 24 ---
•RWKV : 169m, 430m, 1.4b, 3b, 7b, 14b
•Bloom (Scao et al., 2022): 560m, 1b, 3b
•OPT (Zhang et al., 2022): 125m, 350m, 1.3b, 2.7b, 6.7b, 13b
•GPT-Neo (Black et al., 2021): 125m, 1.3b, 2.7b
•Pythia (Biderman et al., 2023b): 160m, 410m, 1.4b, 2.8b, 6.7b, 12b
Figure 13: Text generation inference memory (CPU RAM, GPU VRAM) for LLMs. Model parameters are not
accounted.
Figure 14: Text generation inference time for LLMs.

--- PAGE 25 ---
Task Name Measure ChatGPT GPT-4 RWKV-GPT RWKV-adapted SOTA
RTE F1 Macro 88.1 91.3 44.2 74.8 92.1
WNLI Accuracy 81.7 91.6 47.9 49.3 97.9
GoEmotions F1 Macro 25.6 23.1 7.9 7.9 52.8
PolEmo2 F1 Macro 44.1 41.0 38.2 40.9 76.4
Table 6: ChatGPT, GPT-4 and RWKV-4-Raven-14B reasoning performance comparison in RTE (Wang et al., 2019),
WNLI (Wang et al., 2018), GoEmotions (Demszky et al., 2020), and PolEmo2 (Koco ´n et al., 2019) benchmarks.
RWKV GPT prompts were primarily used for ChatGPT in (Koco ´n et al., 2023). SOTA is provided as a supplementary
reference.
Task Name Measure ChatGPT RWKV-adapted SOTA
Aggression F1 Macro 69.10 56.66 74.45
MathQA Accuracy 71.40 5.43 83.20
Sarcasm F1 Macro 49.88 50.96 53.57
TweetSent F1 Macro 63.32 52.50 72.07
Unhealthy F1 Macro 45.21 43.30 50.96
Table 7: ChatGPT and RWKV-4-Raven-14B performance comparison in Aggresion (Wulczyn et al., 2017), Sarcasm
(Siddiqui, 2019), Unhealthy (Price et al., 2020), MathQA (Cobbe et al., 2021), and TweetSent (Barbieri et al., 2020)
benchmarks. SOTA is provided as a supplementary reference.
L Importance of prompt construction and comparison to GPT models
Inspired by Koco ´n et al. (2023), we compared the zero-shot performance of the RWKV-4-Raven-14B with
ChatGPT (access in February 2023) and GPT-4 using several known NLP tasks, i.e., recognizing textual
entailment (RTE), Winograd Natural Language Inference (WNLI), and recognizing emotions elicited
in readers (GoEmotions and PolEmo2). Each model got the same prompts manually chosen to receive
proper responses from the ChatGPT model. As shown in Tab. 6, RWKV performs significantly worse
than ChatGPT and GPT-4 in several specific tasks. We suspect that this disparity is likely caused by the
choice of prompts used to generate the answers since the prompts are written in natural language and do
not take into account that RWKV , as an RNN, is unable to look back inside an instruction.
When the instruction style was adapted (re-ordered) to respect that RNNs are not capable of "retro-
spective processing", the quality may significantly change, e.g., for RTE (Wang et al., 2019) F1 Macro
increased from 44.2% to 74.8%. We hypothesize that RWKV models are more sensitive to the position of
the components in the context, as RNN-based architectures cannot look back and readjust the weight of
previous information. For better performance, the desired information should be placed after the main
question.
An example ChatGPT prompt for recognizing textual entailment (RTE)
Having premise <here is a premise> judge if the following hypothesis <here is a hypothesis> is
logically connected with the premise. Answer "entailment" if yes, or "not_entailment" if no.
A re-ordered RWKV prompt for RTE taking into account the nature of the RNN
Can you tell me if the hypothesis is entailment or is not entailment to the premise?
premise: <here is a premise>
hypothesis: <here is a hypothesis>
While separating the instruction from the input is relatively easy to do, some other aspects of prompt
engineering are harder to quantify. For that purpose, we also tested the approach of stating the input after

--- PAGE 26 ---
the question on multiple other tasks, i.e., aggression and sarcasm detection, classification of unhealthy
(offensive) texts, mathematical Q&A, and sentiment analysis, see Tab. 7. The results suggest that better
prompts might reduce the disparity between models. Raven achieves comparable results to ChatGPT on
unhealthy conversation detection and even surpasses it on the sarcasm detection dataset. While such an
approach to prompting looks necessary, it is not enough in itself to replace the capability of having free
access to the whole context. Therefore, prompt engineering seems to be significantly more important for
the RNN models rather than for standard transformers. It is entirely possible that good prompts to RNN
models do not mean additional restrictions, but should simply be constructed using completely different
guidelines. The authors of the aforementioned paper (Koco ´n et al., 2023)4perform chain-of-thought
to improve results on the MathQA dataset. Even including this approach, the Raven model achieved a
very low accuracy of 5.43%. Without it, the model performed even worse, performing only very basic
and simple calculations and achieving 4.13% accuracy. Raven struggled with questions that required
intermediate results. It is likely that the order of information presented in the math questions inside the
dataset poses a challenge for the RWKV model. It is yet to be seen if prompt engineering can address this
issue. This further emphasizes the importance of the order of information the model receives.
Template used to prompt the Raven model in MathQA with chain-of-thought
Write the reasoning and highlight the answer to the question at the end in the format: "Answer: ".
The question is: <here is a question>
Template used to prompt the Raven model in MathQA without chain-of-thought
Write the answer to the math question in the format: "Answer: ".
The question is: <here is a question>
M Cases
In this part, we present a few instances of outputs produced by RWKV model using a Chat interface56.
RWKV-4-Raven-14B
Tell me about ravens.
Ravens are large, black birds with a distinctive white head and neck. They are found in most parts
of the world, including North America, Europe, Asia, and Australia. Ravens are known for their
intelligence and problem-solving abilities. They are also considered to be symbols of death and
transformation in many cultures. Ravens are often associated with the afterlife or death because
they have been known to eat carrion or even other birds. In some cultures, ravens are also believed
to be messengers of the gods or guardians of treasure.
4This is in line with the idea discussed in (Wei et al., 2022b)
5ttps://github.com/BlinkDL/ChatRWKV
6https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio

--- PAGE 27 ---
RWKV-4-Raven-14B
Explain the concept of machine learning to a 10-year-old.
Machine learning is like a super-smart computer that can learn on its own. It can recognize patterns
and make decisions based on what it has learned. Just like how you might remember something
from the past, the computer can remember what it’s learned and use it to make predictions or
decisions in the future. It’s like having a smart friend who always knows what to do!
RWKV-4-Raven-14B
Write a short poem describing nature’s beauty in springtime.
Nature’s beauty in springtime is rare,
A symphony of colors, a gentle surprise.
The flowers bloom, the birds sing and sing,
As the sun shines down, warming our skin.
The trees sway in the gentle breeze,
As we take in their beauty with ease.
The sky is blue, the clouds are white,
A sight to behold, it’s truly divine.

--- PAGE 28 ---
RWKV-4-Raven-14B
Below is the numerical stable formula for updating the state variable in RWKV:
a∗
t=ep−qat−1+eu+kt−qvt
b∗
t=ep−qbt−1+eu+kt−q
wkv t=a∗
t
b∗
t
Please write out the corresponding pytorch code to do so, where both atandbtare
torch.Tensor([n])
Here is the PyTorch code to update the state variable in RWKV using the given formula:
import t o r c h
def u p d a t e _ s t a t e ( a , b , u , v , p , q ) :
a _ t = t o r c h . exp ( p − q ) *a + t o r c h . exp ( u + k − q ) *v
b _ t = t o r c h . exp ( p − q ) *b + t o r c h . exp ( u + k − q )
return a_t , b _ t
a , b = t o r c h . randn ( 2 , 3)
u = t o r c h . randn ( 2 )
v = t o r c h . randn ( 2 )
wkv = t o r c h . randn ( 2 )
a_t , b _ t = u p d a t e _ s t a t e ( a , b , u , v , wkv , t o r c h . randn ( 2 ) )
p r i n t ( " a _ t : " , a _ t )
p r i n t ( " b _ t : " , b _ t )
This code defines a function update_state that takes in the state variables a and b, the control
variables u and v, and the parameters wkv and torch.randn. It then uses the PyTorch torch.exp
function to compute the exponential of the control variables and the log of the normalization factor.
Finally, it uses these values to update the state.
RWKV-4-Raven-14B
Explain the following metaphor: Life is like cats.
The metaphor "Life is like cats" suggests that life can be unpredictable and full of unexpected
twists and turns, just like a cat’s behavior. It implies that we cannot fully control or predict the
outcome of our lives, just as we cannot control the movements of a cat. The metaphor also implies
that life is complex and multifaceted, with many different variables and factors at play. Just as a
cat can have multiple personalities or moods, life can have many twists and turns that are difficult
to predict or understand. Overall, the metaphor suggests that life is complex and unpredictable,
much like a cat’s behavior.

--- PAGE 29 ---
RWKV-4-Raven-14B
Test the convergence ofP∞
i=02−i.
The given series is a geometric series with the first term a0= 2and common ratio r=1
2. We can
find the sum of this series using the formula for a geometric series:
∞X
i=0ai=a0
1−r=1
2
1−1
2=1
1−1
2=2
1= 2
So, the sum of the given series is 2.
RWKV-4-Raven-14B
I would like to cook some Chinese food at home. Do you have any suggestions on the meal I
could choose?
Yes, of course! If you are not very experienced in cooking Chinese food, you could start with
making noodles or rice, then make a dish using one of those. I also think making stir-fry with
some meat is an excellent idea.
I want to make something more special. What about red braised pork belly?
Yes, red braised pork belly is also very popular and tasty.
Tell me the steps to cook.
You will need a few ingredients such as pork belly, star anise, sugar, ginger, scallions and wine.
Put all of these in a pot, cook on medium heat and simmer until the meat is tender. After that,
remove the meat from the pot and reserve the sauce. Slice the meat and add some ginger slices
into it, then boil until it’s almost done.
What do I need to be careful with the most during the cooking process?
To cook pork belly properly, it is very important to monitor the heat of the stove and be patient
with the cooking process. Overcooking it can cause the pork to be tough, dry and difficult to chew.
You can add a few ingredients like soy sauce or black beans to give the pork more flavor.
Tell me more about the proper heat and timing.
If the temperature of the water is too high, it can damage the collagen and connective tissues,
resulting in tough pork belly. It is essential to control the temperature carefully, especially
when braising the pork. Additionally, keep the water simmering but not boiling to maintain a
low simmer, as high heat will dry out the pork belly and result in tough and unpalatable meat.
Lastly, monitor the heat during cooking and make sure the heat is medium, as this is what is most
appropriate for slow cooking pork belly.
I see. Thank you for your advice.
You’re welcome! It was a pleasure chatting with you. Let me know if you need any further
assistance in the future.

--- PAGE 30 ---
RWKV-4-Raven-14B
If all apples are fruits and all fruits are sweet, are all apples sweet? Please explain your
answer in detail.
Yes, all apples are fruits, and all fruits are sweet. Therefore, it is safe to assume that all apples are
sweet.
If birds can fly. Penguins are birds. Can penguins fly? Please explain your answer in detail.
No, penguins cannot fly. Penguins are birds, but they are not capable of flight like other birds.
They have wings and can glide for short distances, but they cannot take off or fly long distances
like other birds.
The day before two days after the day before tomorrow is Saturday. What day is it today?
Please explain your answer in detail.
Today is Friday. Two days after tomorrow is Sunday. Tomorrow is Saturday.
If five cats can catch five mice in five minutes, how long will it take one cat to catch one
mouse? Please explain your answer in detail.
If five cats can catch five mice in five minutes, it means that each cat can catch one mouse in one
minute. To find out how long it will take one cat to catch one mouse, we can use the concept of
proportionality.
Let’s assume that it takes x minutes for a cat to catch a mouse. Then, the number of mice caught
by a cat in one minute is equal to the number of cats multiplied by the number of mice caught by
each cat: x = 5 * 1 = 5
So, it will take 5 minutes for one cat to catch one mouse.
RWKV-4-Raven-14B
Summarize the following paragraph.
<The Abstract of this paper goes here>
The paper presents a novel architecture, Receptance Weighted Key Value (RWKV), that combines
the efficient parallelizable training of Transformers with the efficient inference of RNNs. The
model is able to scale to tens of billions of parameters and exhibits linear computational complexity
during training and inference, making it a promising alternative to Transformers for sequence
processing tasks.
Please further summarize it in one sentence.
The researchers have developed a new architecture that combines the efficient parallelizable
training of Transformers with the efficient inference of RNNs, which allows for linear scaling in
memory and computational requirements.
