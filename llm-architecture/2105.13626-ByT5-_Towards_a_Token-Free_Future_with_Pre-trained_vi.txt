# ByT5: Hướng tới Tương lai Không Token với Các Mô hình Byte-to-Byte Được Tiền huấn luyện

Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou,
Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel
Google Research
{lintingx, adityabarua, nconstant, rmyeid, sharannarang, mihirkale, adarob}
@google.com, craffel@gmail.com

## Tóm tắt

Hầu hết các mô hình ngôn ngữ được tiền huấn luyện được sử dụng rộng rãi hoạt động trên các chuỗi token tương ứng với các đơn vị từ hoặc từ con. So với đó, các mô hình không token hoạt động trực tiếp trên văn bản thô (byte hoặc ký tự) có nhiều lợi ích: chúng có thể xử lý văn bản bằng bất kỳ ngôn ngữ nào ngay lập tức, chúng mạnh mẽ hơn với nhiễu, và chúng giảm thiểu nợ kỹ thuật bằng cách loại bỏ các đường ống tiền xử lý văn bản phức tạp và dễ gây lỗi. Vì các chuỗi byte hoặc ký tự dài hơn các chuỗi token, các nghiên cứu trước đây về mô hình không token thường đã giới thiệu các kiến trúc mô hình mới được thiết kế để phân bổ chi phí hoạt động trực tiếp trên văn bản thô. Trong bài báo này, chúng tôi chỉ ra rằng một kiến trúc Transformer tiêu chuẩn có thể được sử dụng với những sửa đổi tối thiểu để xử lý các chuỗi byte. Chúng tôi mô tả các đánh đổi về số lượng tham số, FLOP huấn luyện và tốc độ suy luận, và chỉ ra rằng các mô hình cấp byte có khả năng cạnh tranh với các đối tác cấp token của chúng. Chúng tôi cũng chứng minh rằng các mô hình cấp byte mạnh mẽ hơn đáng kể với nhiễu và hoạt động tốt hơn trên các tác vụ nhạy cảm với chính tả và phát âm. Như một phần của đóng góp của chúng tôi, chúng tôi phát hành một bộ mô hình Transformer cấp byte được tiền huấn luyện mới dựa trên kiến trúc T5, cũng như tất cả mã và dữ liệu được sử dụng trong các thí nghiệm của chúng tôi.

## 1 Giới thiệu

Một cân nhắc quan trọng khi thiết kế các mô hình NLP là cách biểu diễn văn bản. Một lựa chọn phổ biến là gán một ID token duy nhất cho mỗi từ trong một từ vựng hữu hạn cố định. Một đoạn văn bản nhất định do đó được chuyển đổi thành một chuỗi token bởi một tokenizer trước khi được đưa vào mô hình để xử lý. Một vấn đề với việc sử dụng từ vựng từ cố định là không có cách nào rõ ràng để xử lý một đoạn văn bản chứa từ ngoài từ vựng. Một cách tiếp cận tiêu chuẩn là ánh xạ tất cả các từ không biết tới cùng một token <UNK>, nhưng điều này ngăn mô hình phân biệt giữa các từ ngoài từ vựng khác nhau.

Các tokenizer từ con (Sennrich et al., 2016; Wu et al., 2016; Kudo and Richardson, 2018) đưa ra một giải pháp thanh lịch cho vấn đề ngoài từ vựng. Thay vì ánh xạ mỗi từ tới một token duy nhất, các tokenizer từ con phân tích các từ thành các đơn vị từ con nhỏ hơn với mục tiêu giảm thiểu tổng chiều dài của các chuỗi token cho một kích thước từ vựng cố định. Ví dụ, một tokenizer từ con có thể tokenize từ doghouse thành cặp token dog và house ngay cả khi doghouse không có trong từ vựng từ con. Tính linh hoạt này đã khiến các tokenizer từ con trở thành cách tiêu chuẩn để tokenize văn bản trong vài năm qua.

Tuy nhiên, các tokenizer từ con vẫn thể hiện nhiều hành vi không mong muốn. Lỗi chính tả, biến thể trong chính tả và viết hoa, và thay đổi hình thái đều có thể làm cho biểu diễn token của một từ hoặc cụm từ thay đổi hoàn toàn, có thể dẫn đến dự đoán sai. Hơn nữa, các ký tự không biết (ví dụ từ một ngôn ngữ không được sử dụng khi xây dựng từ vựng từ con) thường ngoài từ vựng đối với mô hình từ con.

Một giải pháp tự nhiên hơn tránh được những cạm bẫy nêu trên sẽ là tạo ra các mô hình NLP không token không dựa vào từ vựng đã học để ánh xạ từ hoặc đơn vị từ con tới token. Các mô hình như vậy hoạt động trực tiếp trên văn bản thô. Chúng tôi không phải là những người đầu tiên lập luận cho các mô hình không token, và một nghiên cứu toàn diện hơn về các lợi ích khác nhau của chúng có thể được tìm thấy trong nghiên cứu gần đây của Clark et al. (2021). Trong nghiên cứu này, chúng tôi sử dụng thực tế là dữ liệu văn bản thường được lưu trữ dưới dạng một chuỗi byte. Do đó, việc đưa các chuỗi byte trực tiếp vào mô hình cho phép xử lý các chuỗi văn bản tùy ý. Cách tiếp cận này phù hợp với triết lý học từ đầu tới cuối, nỗ lực huấn luyện các mô hình để ánh xạ trực tiếp từ dữ liệu thô tới dự đoán. Nó cũng có lợi ích cụ thể về kích thước mô hình: các từ vựng lớn của các mô hình cấp từ hoặc từ con thường dẫn đến nhiều tham số được dành cho ma trận từ vựng. Ngược lại, một mô hình cấp byte theo định nghĩa chỉ yêu cầu 256 embedding. Việc di chuyển biểu diễn từ ra khỏi ma trận từ vựng thưa thớt và vào các lớp mạng dày đặc sẽ cho phép các mô hình khái quát hóa hiệu quả hơn qua các thuật ngữ liên quan (ví dụ book / books) và biến thể chính tả. Cuối cùng, từ quan điểm thực tế, các mô hình với từ vựng cố định có thể phức tạp hóa việc thích ứng với ngôn ngữ mới và thuật ngữ mới, trong khi theo định nghĩa, các mô hình không token có thể xử lý bất kỳ chuỗi văn bản nào.

Nhược điểm chính của các mô hình cấp byte là các chuỗi byte có xu hướng dài hơn đáng kể so với các chuỗi token. Vì chi phí tính toán của các mô hình học máy có xu hướng tỷ lệ thuận với chiều dài chuỗi, nhiều nghiên cứu trước đây về các mô hình cấp ký tự và byte đã khám phá cách xử lý các chuỗi dài một cách hiệu quả bằng cách sử dụng convolution với pooling (Zhang et al., 2015; Lee et al., 2017) hoặc thời gian tính toán thích ứng (Graves, 2016).

Trong nghiên cứu này, chúng tôi áp dụng một cách tiếp cận đơn giản hơn và chỉ ra rằng kiến trúc Transformer có thể được thích ứng một cách đơn giản để xử lý các chuỗi byte mà không tăng chi phí tính toán một cách bất lợi đáng kể. Chúng tôi tập trung vào framework T5 (Raffel et al., 2020), nơi tất cả các vấn đề NLP dựa trên văn bản được đúc thành định dạng văn bản-tới-văn bản. Cách tiếp cận này làm cho việc giải quyết bất kỳ tác vụ NLP nào trở nên đơn giản bằng cách tạo ra một chuỗi byte có điều kiện trên một số byte đầu vào. Kiến trúc ByT5 đề xuất của chúng tôi được mô tả trong phần 3. Thiết kế vẫn khá gần với mT5 (biến thể đa ngôn ngữ của T5 được giới thiệu bởi Xue et al. (2021)), với những khác biệt được minh họa trong hình 1. Thông qua các thí nghiệm mở rộng trên một tập hợp đa dạng các tác vụ tiếng Anh và đa ngôn ngữ (được trình bày trong phần 4), chúng tôi chỉ ra rằng ByT5 có khả năng cạnh tranh với baseline cấp từ con, mặc dù được tiền huấn luyện trên ít văn bản hơn 4 lần. Chúng tôi cũng xác nhận trong phần 5 rằng các mô hình cấp byte mạnh mẽ hơn với việc hỏng văn bản đầu vào. Xuyên suốt, chúng tôi mô tả các đánh đổi của các quyết định thiết kế về chi phí tính toán và số lượng tham số, được thảo luận chi tiết hơn trong các phần 6 và 7. Kết quả cuối cùng là một bộ mô hình ByT5 được tiền huấn luyện mà chúng tôi phát hành cùng với bài báo này.

## 2 Nghiên cứu Liên quan

Các mô hình ngôn ngữ neural đầu tiên của Sutskever et al. (2011) và Graves (2013) hoạt động trực tiếp trên các chuỗi ký tự. Tiền lệ này đã khiến nhiều người sử dụng mô hình ngôn ngữ cấp ký tự như một benchmark để đánh giá các kiến trúc neural (Kalchbrenner et al., 2016; Chung et al., 2017; Ha et al., 2017; Zilly et al., 2017; Melis et al., 2018; Al-Rfou et al., 2019). Choe et al. (2019) đã chỉ ra rằng các mô hình ngôn ngữ byte có thể bằng perplexity của các mô hình cấp từ khi được cấp cùng một ngân sách tham số. Tuy nhiên, thực hành tiêu chuẩn trong các tình huống thực tế vẫn là sử dụng các mô hình cấp từ hoặc từ con.

Một số kiến trúc nhận thức ký tự đã được đề xuất sử dụng các đặc trưng cấp ký tự nhưng vẫn dựa vào tokenizer để xác định ranh giới từ. Các cách tiếp cận này bao gồm ELMo (Peters et al., 2018), CharacterBERT (El Boukkouri et al., 2020) và nhiều cách khác (Ling et al., 2015; Chung et al., 2016; Kim et al., 2016; Józefowicz et al., 2016; Wang et al., 2020; Wei et al., 2021). Riêng biệt, một số nghiên cứu đã nỗ lực cải thiện các vấn đề với tokenization, ví dụ bằng cách thích ứng từ vựng với ngôn ngữ mới (Garcia et al., 2021) hoặc chọn ngẫu nhiên các phân đoạn từ con khác nhau để cải thiện tính mạnh mẽ trong các cài đặt tài nguyên thấp và ngoài miền (Kudo, 2018). Các phương pháp này không đáp ứng mục tiêu của chúng tôi là đơn giản hóa đường ống NLP bằng cách loại bỏ tiền xử lý văn bản.

Đã có một vài nỗ lực gần đây để phát triển các mô hình ngôn ngữ được tiền huấn luyện không token đa năng cho học chuyển giao. Akbik et al. (2018) chỉ ra kết quả mạnh mẽ về gán nhãn chuỗi với tiền huấn luyện cấp ký tự và phát hành các mô hình bao phủ bốn ngôn ngữ. Gần đây hơn, Clark et al. (2021) phát triển CANINE, cho thấy lợi ích so với BERT đa ngôn ngữ bằng cách làm việc với ký tự thay vì token word-piece, mặc dù mô hình "CANINE-S" vẫn sử dụng tokenizer trong quá trình tiền huấn luyện để định nghĩa mục tiêu cho tác vụ mô hình ngôn ngữ có mặt nạ. Công trình của chúng tôi khác với những nghiên cứu này ở chỗ (i) chúng tôi huấn luyện các mô hình encoder-decoder mở rộng tới các tác vụ sinh tạo, (ii) các mô hình của chúng tôi làm việc trực tiếp với byte UTF-8, và (iii) chúng tôi khám phá hiệu ứng của quy mô mô hình, huấn luyện các mô hình vượt quá 10 tỷ tham số.

## 3 Thiết kế ByT5

Mục tiêu của chúng tôi trong việc thiết kế ByT5 là lấy một mô hình dựa trên token hiện có và thực hiện tập hợp tối thiểu các sửa đổi để làm cho nó không token, do đó hạn chế các yếu tố gây nhiễu thí nghiệm. Chúng tôi dựa ByT5 trên mô hình mT5 gần đây (Xue et al., 2021), được huấn luyện trên mC4 (một corpus lớn dữ liệu văn bản đa ngôn ngữ không nhãn) và đạt state-of-the-art trên nhiều benchmark cộng đồng. Chúng tôi phát hành ByT5 với năm kích thước tương tự như T5 và mT5 (Small, Base, Large, XL, XXL). Chúng tôi nhằm mục đích ByT5 bao phủ các trường hợp sử dụng tương tự như mT5: nó là một mô hình văn bản-tới-văn bản được tiền huấn luyện đa năng bao phủ hơn 100 ngôn ngữ. Chúng tôi kỳ vọng ByT5 sẽ đặc biệt hữu ích cho các tác vụ hoạt động trên các chuỗi văn bản ngắn đến trung bình (một vài câu hoặc ít hơn), vì chúng sẽ chịu ít sự chậm lại hơn trong fine-tuning và suy luận.

### 3.1 Thay đổi từ mT5

So với mT5, chúng tôi thực hiện các thay đổi chính sau trong việc thiết kế ByT5. Đầu tiên và quan trọng nhất, chúng tôi loại bỏ từ vựng SentencePiece (Kudo and Richardson, 2018) và đưa byte UTF-8 trực tiếp vào mô hình mà không có bất kỳ tiền xử lý văn bản nào. Các byte được nhúng tới kích thước ẩn của mô hình bằng một từ vựng gồm 256 giá trị byte có thể. Thêm 3 ID được dành riêng cho các token đặc biệt: padding, end-of-sentence, và một token <UNK> không sử dụng mà chúng tôi bao gồm chỉ theo quy ước.

Thứ hai, chúng tôi sửa đổi tác vụ tiền huấn luyện. mT5 sử dụng mục tiêu tiền huấn luyện "span corruption" được đề xuất đầu tiên bởi Raffel et al. (2020) nơi các span token trong dữ liệu văn bản không nhãn được thay thế bằng một "sentinel" ID duy nhất và mô hình phải điền vào các span còn thiếu. Thay vì thêm 100 token mới cho các sentinel, chúng tôi thấy rằng việc tái sử dụng 100 byte ID cuối cùng là đủ. Trong khi mT5 sử dụng chiều dài span trung bình là 3 token từ con, chúng tôi thấy rằng việc che giấu các byte-span dài hơn là có giá trị. Cụ thể, chúng tôi đặt chiều dài span mask trung bình của chúng tôi là 20 byte, và chỉ ra các ablation của giá trị này trong phần 6.

Thứ ba, chúng tôi thấy rằng ByT5 hoạt động tốt nhất khi chúng tôi tách độ sâu của các stack encoder và decoder. Trong khi T5 và mT5 sử dụng kiến trúc "cân bằng", chúng tôi thấy các mô hình cấp byte hưởng lợi đáng kể từ encoder "nặng hơn". Cụ thể, chúng tôi đặt độ sâu encoder của chúng tôi bằng 3 lần độ sâu decoder. Một cách trực quan, encoder nặng hơn này làm cho mô hình giống hơn với các mô hình chỉ encoder như BERT. Bằng cách giảm khả năng decoder, người ta có thể kỳ vọng chất lượng sẽ xấu đi trên các tác vụ như tóm tắt yêu cầu sinh tạo văn bản trôi chảy. Tuy nhiên, chúng tôi thấy rằng đây không phải là trường hợp, với các mô hình byte encoder nặng hoạt động tốt hơn trên cả tác vụ phân loại và sinh tạo. Chúng tôi ablate hiệu ứng của sự cân bằng encoder/decoder trong phần 6.

Vì không phải tất cả các chuỗi byte đều hợp pháp theo tiêu chuẩn UTF-8, chúng tôi loại bỏ bất kỳ byte bất hợp pháp nào trong đầu ra của mô hình (mặc dù chúng tôi chưa bao giờ quan sát thấy các mô hình của chúng tôi dự đoán các chuỗi byte bất hợp pháp trong thực tế). Ngoài các thay đổi trên, chúng tôi tuân theo mT5 trong tất cả các cài đặt. Giống như mT5, chúng tôi đặt chiều dài chuỗi của chúng tôi là 1024 (byte thay vì token), và huấn luyện trong 1 triệu bước trên các batch 2^20 token.

### 3.2 So sánh các Mô hình

Mục tiêu của chúng tôi trong bài báo này là chỉ ra rằng các sửa đổi đơn giản đối với kiến trúc Transformer có thể cho phép xử lý cấp byte trong khi chịu các đánh đổi hợp lý về chi phí. Việc mô tả các đánh đổi này yêu cầu một định nghĩa rõ ràng về ý nghĩa của "chi phí", vì có nhiều trục mà có thể đo lường kích thước mô hình và yêu cầu tính toán.

Các mô hình sử dụng từ vựng từ hoặc từ con thường bao gồm một ma trận từ vựng lưu trữ biểu diễn vector của mỗi token trong từ vựng. Chúng cũng bao gồm một ma trận tương tự trong lớp softmax đầu ra. Đối với các từ vựng lớn (ví dụ những cái trong các mô hình đa ngôn ngữ), các ma trận này có thể chiếm một tỷ lệ đáng kể tham số của mô hình. Ví dụ, các ma trận từ vựng và softmax đầu ra trong mô hình mT5-Base chiếm 256 triệu tham số, hoặc khoảng 66% tổng số tham số. Chuyển sang mô hình cấp byte cho phép phân bổ các tham số này ở nơi khác trong mô hình, ví dụ bằng cách thêm lớp hoặc làm cho các lớp hiện có "rộng hơn". Để bù đắp cho việc giảm tổng số tham số do thay đổi từ mô hình dựa trên token sang không token, chúng tôi điều chỉnh kích thước ẩn mô hình ByT5 (d_model) và chiều feed-forward (d_ff) để khớp tham số với mT5, trong khi duy trì tỷ lệ khoảng 2:5 giữa d_ff và d_model, như được khuyến nghị bởi Kaplan et al. (2020). Bảng 1 cho thấy các kiến trúc mô hình kết quả qua tất cả năm kích thước mô hình.

Riêng biệt, như đã đề cập trước đây, việc thay đổi từ chuỗi từ hoặc từ con sang chuỗi byte sẽ tăng chiều dài chuỗi (được tokenize) của một đoạn văn bản nhất định. Cơ chế self-attention ở trung tâm của kiến trúc Transformer phổ biến (Vaswani et al., 2017) có độ phức tạp thời gian và không gian bậc hai trong chiều dài chuỗi, vì vậy các chuỗi byte có thể dẫn đến chi phí tính toán cao hơn đáng kể. Trong khi các mạng neural tái phát và các cơ chế attention đã sửa đổi (Tay et al., 2020) có thể tuyên bố độ phức tạp tính toán tốt hơn trong chiều dài chuỗi, chi phí vẫn luôn tăng lên khi các chuỗi trở nên dài hơn.

Cho đến nay, chúng tôi đã thảo luận về các đại lượng dễ đo như số lượng tham số và FLOP. Tuy nhiên, không phải tất cả FLOP đều bằng nhau, và chi phí thực tế của một mô hình cụ thể cũng sẽ phụ thuộc vào phần cứng mà nó chạy trên. Một sự phân biệt quan trọng là xác định các thao tác có thể dễ dàng song song hóa (ví dụ xử lý có thể song song hóa hoàn toàn của encoder) và những thao tác không thể (ví dụ lấy mẫu tự hồi quy trong decoder trong quá trình suy luận). Đối với các mô hình encoder-decoder cấp byte, nếu decoder đặc biệt lớn, lấy mẫu tự hồi quy có thể trở nên tương đối đắt đỏ nhờ chiều dài tăng của các chuỗi byte.

Liên quan, việc ánh xạ token đầu vào tới biểu diễn vector tương ứng trong ma trận từ vựng về cơ bản là "miễn phí" về FLOP vì nó có thể được triển khai bằng cách địa chỉ hóa một hàng cụ thể trong bộ nhớ. Do đó, việc tái phân bổ tham số từ ma trận từ vựng tới phần còn lại của mô hình thường sẽ dẫn đến một mô hình yêu cầu nhiều FLOP hơn để xử lý một chuỗi đầu vào nhất định (xem phần 7 để so sánh chi tiết).

Cuối cùng, chúng tôi lưu ý rằng một metric quan trọng khác là hiệu quả dữ liệu, tức là cần bao nhiều dữ liệu để mô hình đạt được một giải pháp tốt. Đối với các vấn đề NLP, điều này có thể được đo bằng số lượng token hoặc lượng văn bản thô được nhìn thấy trong quá trình huấn luyện. Cụ thể, một mô hình cấp byte được huấn luyện trên cùng số lượng token như một mô hình cấp từ hoặc từ con sẽ được huấn luyện trên ít dữ liệu văn bản hơn. Trong Hình 2, chúng tôi chỉ ra tỷ lệ nén của tokenization SentencePiece mT5, được đo là tỷ lệ byte UTF-8 trên token trong mỗi phân chia ngôn ngữ của corpus mC4 được sử dụng trong tiền huấn luyện. Tỷ lệ này dao động từ 2.5 (Malta) đến 9.0 (Khmer). Khi xem xét corpus mC4 như một tổng thể, được lấy mẫu theo tỷ lệ trộn tiền huấn luyện mT5, chúng tôi có tỷ lệ nén tổng thể là 4:1 byte trên token SentencePiece. Một mặt, việc dài ra 4 lần này có thể được xem là một lợi thế cho ByT5: với các chuỗi dài hơn, mô hình có nhiều tính toán hơn để dành cho việc mã hóa một đoạn văn bản nhất định. Mặt khác, với chiều dài chuỗi đầu vào cố định và số bước huấn luyện, mô hình sẽ được tiếp xúc với khoảng 4 lần ít văn bản thực tế hơn trong quá trình tiền huấn luyện.

Với những yếu tố này trong tâm trí, chúng tôi chọn tập trung vào các biện pháp hiệu quả sau trong các thí nghiệm của chúng tôi: số lượng tham số, thời gian suy luận, và hiệu quả tiền huấn luyện. Số lượng tham số là một đại lượng đơn giản và dễ đo liên quan trực tiếp đến lượng bộ nhớ cần thiết để sử dụng mô hình. Thời gian suy luận là một phép đo thực tế của chi phí tính toán của mô hình đại diện cho phép đo "trường hợp xấu nhất" cho các mô hình cấp byte do chi phí tiềm năng bổ sung của lấy mẫu tự hồi quy. Cuối cùng, hiệu quả tiền huấn luyện cho phép chúng tôi đo liệu các mô hình cấp byte có thể học được một giải pháp tốt sau khi nhìn thấy ít dữ liệu tiền huấn luyện hơn hay không.

## 4 Kết quả Cốt lõi

Trong phần này, chúng tôi so sánh ByT5 với mT5 trên một loạt rộng các tác vụ. Chúng tôi chỉ ra rằng ByT5 có khả năng cạnh tranh với mT5 trên các benchmark NLP tiếng Anh và đa ngôn ngữ tiêu chuẩn và vượt trội hơn mT5 ở các kích thước mô hình nhỏ. Ngoài ra, ByT5 xuất sắc trên các tác vụ sinh tạo tự do và tác vụ cấp từ.

Đối với mỗi tác vụ downstream, chúng tôi fine-tune các mô hình mT5 và ByT5 trong 262,144 bước, sử dụng tốc độ học tập không đổi là 0.001 và tỷ lệ dropout là 0.1. Chúng tôi sử dụng kích thước batch 2^17 token theo mặc định, nhưng tăng lên 2^20 cho một số tác vụ với tập huấn luyện lớn hơn (GLUE, SuperGLUE, XNLI, TweetQA), và giảm xuống 2^16 cho tác vụ Dakshina. Trong tất cả các trường hợp, chúng tôi chọn checkpoint mô hình tốt nhất dựa trên hiệu suất tập validation.

### 4.1 Tác vụ Phân loại Tiếng Anh

Trên các benchmark phân loại văn bản GLUE (Wang et al., 2019b) và SuperGLUE (Wang et al., 2019a) được áp dụng rộng rãi, chúng tôi thấy ByT5 đánh bại mT5 ở kích thước Small và Base, nhưng mT5 có lợi thế ở các kích thước lớn hơn, như được hiển thị trong bảng 2. Hiệu suất mạnh mẽ của ByT5 ở các kích thước nhỏ hơn có thể xuất phát từ sự gia tăng lớn các tham số dày đặc so với mT5. Trong khi các mô hình tổng thể được khớp tham số, hầu hết các tham số mT5 Small và Base được "khóa" trong các ma trận liên quan đến vocab và chỉ được truy cập khi có một token cụ thể. Chúng tôi nghi ngờ rằng việc thay thế chúng bằng các tham số "dày đặc" được kích hoạt qua tất cả các ví dụ khuyến khích sử dụng tham số hiệu quả và chia sẻ hơn.

### 4.2 Tác vụ Sinh tạo Tiếng Anh

Chúng tôi cũng so sánh ByT5 với mT5 trên ba tác vụ sinh tạo tiếng Anh. XSum (Narayan et al., 2018) là một tác vụ tóm tắt trừu tượng yêu cầu các mô hình tóm tắt một bài báo tin tức trong một câu duy nhất. Để so sánh tốt hơn với nghiên cứu gần đây, chúng tôi áp dụng phiên bản của tác vụ được định nghĩa trong benchmark GEM (Gehrmann et al., 2021). TweetQA (Xiong et al., 2019) là một tác vụ trả lời câu hỏi trừu tượng được xây dựng từ các tweet được đề cập trong các bài báo tin tức. Điều này kiểm tra hiểu biết về ngôn ngữ "lộn xộn" và không chính thức của phương tiện truyền thông xã hội. Cuối cùng, DROP (Dua et al., 2019) là một tác vụ đọc hiểu thách thức yêu cầu lý luận số học.

Bảng 3 cho thấy ByT5 vượt trội hơn mT5 trên mỗi tác vụ sinh tạo qua tất cả các kích thước mô hình. Trên GEM-XSum, ByT5 gần (15.3 so với 17.0) điểm số tốt nhất được báo cáo bởi Gehrmann et al. (2021), một mô hình PEGASUS (Zhang et al., 2020) được tiền huấn luyện đặc biệt cho tóm tắt. Trên TweetQA, ByT5 vượt trội (72.0 so với 67.3) baseline BERT của Xiong et al. (2019). Trên DROP, ByT5 gần (EM 78.5 so với 84.1) kết quả tốt nhất từ Chen et al. (2020), một mô hình QDGAT (RoBERTa) với mô-đun lý luận số học chuyên biệt.

### 4.3 Benchmark Đa ngôn ngữ

Các thay đổi đối với từ vựng và tokenization có khả năng ảnh hưởng đến các ngôn ngữ khác nhau theo những cách khác nhau. Để kiểm tra hiệu ứng của việc chuyển sang mô hình cấp byte đối với hiểu biết đa ngôn ngữ, chúng tôi so sánh các mô hình ByT5 và mT5 khớp tham số trên các tác vụ từ bộ benchmark XTREME phổ biến (Hu et al., 2020). Cụ thể, chúng tôi đánh giá trên cùng sáu tác vụ như Xue et al. (2021). Những tác vụ này bao gồm hai tác vụ phân loại: XNLI (Conneau et al., 2018) và PAWS-X (Yang et al., 2019), ba tác vụ QA trích xuất: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020) và TyDiQA (Clark et al., 2020), và một tác vụ dự đoán có cấu trúc: WikiAnn NER (Pan et al., 2017).

Bảng 4 cho thấy ByT5 khá cạnh tranh tổng thể. Trên cài đặt trong ngôn ngữ thực tế nhất, nơi có một số dữ liệu huấn luyện vàng trong tất cả các ngôn ngữ, ByT5 vượt qua state-of-art mT5 trước đây trên tất cả các tác vụ và kích thước mô hình. Trên cài đặt translate-train, ByT5 đánh bại mT5 ở các kích thước nhỏ hơn, nhưng kết quả hỗn hợp ở các kích thước lớn hơn. Chúng tôi báo cáo kết quả zero-shot để hoàn thiện, nhưng nhấn mạnh rằng cài đặt này ít phù hợp với các ứng dụng thực tế, vì dịch máy có sẵn rộng rãi.

Chúng tôi khám phá phân tích theo từng ngôn ngữ trên hai tác vụ để xem các ngôn ngữ khác nhau bị ảnh hưởng như thế nào bởi việc chuyển sang xử lý cấp byte. Người ta có thể kỳ vọng các ngôn ngữ có hình thái biến tố phong phú (ví dụ tiếng Thổ Nhĩ Kỳ) sẽ hưởng lợi nhiều nhất từ việc rời khỏi từ vựng cố định. Chúng tôi cũng tò mò xem liệu có bất kỳ mẫu nào xuất hiện liên quan đến họ ngôn ngữ (ví dụ Romance so với Slavic), chữ viết (ví dụ Latin so với non-Latin), kích thước bộ ký tự, hoặc tính sẵn có của dữ liệu (tài nguyên cao so với thấp).

Hình 3 cho thấy khoảng cách theo từng ngôn ngữ giữa ByT5-Large và mT5-Large trên TyDiQA-GoldP và XNLI zero-shot. Một xu hướng đáng chú ý là khoảng cách khá ổn định qua các ngôn ngữ. Ví dụ, ByT5 tốt hơn trong mỗi ngôn ngữ trên TyDiQA-GoldP, trong khi mT5 luôn tốt hơn trên XNLI. So sánh qua các ngôn ngữ, chúng tôi quan sát thấy các ngôn ngữ có tỷ lệ nén token SentencePiece cao hơn (ví dụ tiếng Thái và Telugu) có xu hướng ủng hộ mT5, trong khi những ngôn ngữ có tỷ lệ nén thấp hơn (ví dụ tiếng Indonesia và Việt Nam) có xu hướng ủng hộ ByT5. Chúng tôi không quan sát thấy bất kỳ xu hướng mạnh mẽ nào liên quan đến độ phức tạp hình thái, họ ngôn ngữ, chữ viết, kích thước bộ ký tự, hoặc tính sẵn có của dữ liệu.

### 4.4 Tác vụ Cấp Từ

Do truy cập trực tiếp vào tín hiệu văn bản "thô", chúng tôi kỳ vọng ByT5 sẽ phù hợp với các tác vụ nhạy cảm với chính tả hoặc phát âm của văn bản. Trong phần này, chúng tôi kiểm tra giả thuyết này trên ba benchmark cấp từ: (i) chuyển tự, (ii) grapheme-to-phoneme, và (iii) biến tố hình thái.

Đối với chuyển tự, chúng tôi sử dụng benchmark Dakshina (Roark et al., 2020), bao phủ 12 ngôn ngữ Nam Á truyền thống được viết bằng chữ Brahmic hoặc Perso-Arabic nhưng cũng có thể được viết bằng ký tự Latin trong các bối cảnh không chính thức. Tác vụ chuyển tự từ đơn yêu cầu mô hình "dịch" một từ từ chữ Latin sang chữ bản địa và đo tỷ lệ lỗi ký tự. Các tác vụ còn lại là các shared task SIGMORPHON 2020. Chuyển đổi grapheme-to-phoneme đa ngôn ngữ (Gorman et al., 2020) bao phủ 15 ngôn ngữ và yêu cầu ánh xạ một từ tới phát âm của nó dưới dạng phoneme (ví dụ cat→/kæt/). Biến tố hình thái đa dạng về mặt loại hình (Vylomova et al., 2020) bao phủ 90 ngôn ngữ và yêu cầu tạo ra một biến tố cụ thể của một từ (ví dụ eat+PAST→ate).

Chúng tôi fine-tune các mô hình mT5 và ByT5 cho mỗi tác vụ. Để đơn giản, chúng tôi huấn luyện một mô hình đa ngôn ngữ cho mỗi tác vụ, với một tiền tố chỉ ra ngôn ngữ đang được đề cập. Bảng 5 cho thấy ByT5 vượt trội hơn mT5 với biên độ lớn trên toàn bộ. Mặc dù không đáng ngạc nhiên khi các mô hình "nhận thức ký tự" xuất sắc trên các tác vụ xung quanh hiện tượng nội từ, chúng tôi muốn nhấn mạnh rằng các tác vụ NLP cốt lõi này thường bị bỏ qua trong việc đánh giá các mô hình NLP đa năng.

## 5 Thí nghiệm về Nhiễu Tổng hợp

Văn bản trên các nền tảng kỹ thuật số hiện đại có nhiễu và thể hiện các hiện tượng phức tạp cấp ký tự như lỗi chính tả, lặp lại ký tự, và thay đổi chữ hoa không tiêu chuẩn (Caswell et al., 2020). Ngoài những điều này, lỗi có thể được đưa vào bởi các hệ thống NLP như phương pháp nhập dự đoán và nhận dạng giọng nói tự động. Chúng tôi đã thấy hiệu suất ByT5 mạnh mẽ trên văn bản "lộn xộn" trong TweetQA. Trong phần này, chúng tôi chuyển sang văn bản có nhiễu hơn và khám phá hiệu suất mô hình trên các đầu vào đã bị hỏng với nhiễu nhân tạo của các loại khác nhau. Qua một loạt các lược đồ làm nhiễu, chúng tôi thấy ByT5 vượt trội hơn mT5, chứng minh tính mạnh mẽ cao hơn với nhiễu qua các tác vụ và ngôn ngữ.

Chúng tôi thí nghiệm với năm lược đồ làm nhiễu:
(1) Drop: Mỗi ký tự (tức là codepoint Unicode) có 10% cơ hội bị loại bỏ. (2) Repetitions: Mỗi ký tự có 20% cơ hội được chọn để lặp lại. Nếu được chọn, 1-3 lần lặp (với khả năng bằng nhau) được thêm vào sau ký tự gốc. (3) Antspeak: Mỗi ký tự được viết hoa và đệm bằng khoảng trắng, vì vậy "an owl" trở thành " A N O W L ". (4) Uppercase: Mỗi ký tự được chuyển thành chữ hoa. (5) Random case: Mỗi ký tự được đặt thành chữ hoa ngẫu nhiên (hoa hoặc thường). Đối với hai loại nhiễu cuối cùng, chúng tôi giới hạn ở các ngôn ngữ có chữ viết phân biệt chữ hoa.

Đầu tiên chúng tôi xem xét cài đặt dễ hơn của nhiễu có thể học, nơi nhiễu được áp dụng trong cả fine-tuning và đánh giá. Chúng tôi đánh giá trên XNLI zero-shot và TyDiQA-GoldP. Đối với XNLI, cả premise và hypothesis đều bị làm nhiễu, và mô hình dự đoán nhãn entailment như thường lệ. Đối với TyDiQA, chúng tôi thêm nhiễu vào câu hỏi và bối cảnh, nhưng để câu trả lời không thay đổi. Do đó, trong nhiều trường hợp, mô hình cần phải đầu tiên định vị câu trả lời có nhiễu, và sau đó "hoàn tác" nhiễu để tạo ra mục tiêu. Chúng tôi fine-tune tất cả các mô hình trong 30,000 bước theo thủ tục trong phần 4.

Bảng 6 cho thấy khả năng khác nhau của ByT5 và mT5 để thích ứng với nhiễu có thể học. Chúng tôi đo sự suy giảm của metric tác vụ giữa các cài đặt sạch và có nhiễu. Chúng tôi quan sát thấy mT5 suy giảm nhiều hơn khi có nhiễu so với ByT5, qua tất cả các điều kiện nhiễu. Trong sự tương phản cực đoan nhất, rANdOm CaSE (thường được sử dụng như một thiết bị cảm xúc trên phương tiện truyền thông xã hội) cực kỳ có hại cho mT5, với tổn thất 25.7 và 14.3 điểm, trong khi ByT5 chỉ giảm 1.5 và 0.2 điểm. ByT5 cũng khá mạnh mẽ với UPPERCASE và repetitions.

Chúng tôi cũng kiểm tra tính mạnh mẽ với nhiễu không được nhìn thấy trong huấn luyện nhưng được đưa vào trong đánh giá. Điều này có liên quan trong việc làm cho các mô hình chống tương lai hơn cũng như có khả năng phục hồi hơn với các lỗi chính tả tình cờ hoặc đối nghịch (Pruthi et al., 2019; Sun et al., 2020). Chúng tôi chỉ đánh giá XNLI và bỏ qua TyDiQA-GoldP trong cài đặt này, vì không hợp lý khi kỳ vọng một mô hình sinh tạo được fine-tune để luôn sao chép các span từ bối cảnh sẽ tự nhiên "hoàn tác" các hỏng hóc và dự đoán các span mới. Cột ngoài cùng bên phải của bảng 6 cho thấy trong cài đặt thách thức hơn này, ByT5 một lần nữa có khả năng phục hồi hơn với nhiễu. Trong khi một số loại nhiễu không nhìn thấy như A N T S P E A K rất có hại, ByT5 chỉ thấy suy giảm nhỏ cho nhiễu chữ hoa.

Các phát hiện của chúng tôi lặp lại kết quả của Durrani et al. (2019), những người thấy rằng các mô hình cấp ký tự mạnh mẽ hơn với nhiễu thực và tổng hợp so với các mô hình dựa trên BPE hoặc từ, qua một loạt các tác vụ gắn thẻ hình thái, cú pháp và ngữ nghĩa. Kết luận tổng quát hơn xuất hiện là các mô hình không token mạnh mẽ hơn với nhiễu qua nhiều tác vụ.

## 6 Nghiên cứu Ablation

Để hiểu rõ hơn tầm quan trọng của các lựa chọn thiết kế khác nhau, chúng tôi huấn luyện các mô hình ablation và so sánh chúng với các baseline của chúng tôi trên ba tác vụ: XNLI zero-shot, TyDiQA-GoldP và GEM-XSum. Các baseline và ablation của chúng tôi được liệt kê trong bảng 7. Các baseline là các mô hình ByT5-Large và mT5-Large khớp tham số được thảo luận ở trên.

### 6.1 Kích thước Lớp Transformer Khớp

Mô hình (a) ByT5-36/12-668M giống hệt ByT5-Large ngoại trừ d_model và d_ff được khớp với mT5-Large, tạo ra một mô hình với 668 triệu tham số, 54% kích thước của ByT5-Large và mT5-Large. Như được thấy trong bảng 8, mô hình này vẫn có khả năng cạnh tranh, và vượt trội hơn mT5-Base có kích thước tương tự với biên độ lớn (cf. bảng 4). Đây là bằng chứng rằng giá trị của ByT5 không chỉ đến từ việc sử dụng các lớp transformer rộng hơn.

### 6.2 Cân bằng Encoder/Decoder

Để điều tra hiệu ứng của việc tách độ sâu encoder và decoder, chúng tôi huấn luyện hai mô hình ByT5 bổ sung với d_model và d_ff khớp với mT5-Large: (b) ByT5-24/24-718M, một mô hình "cân bằng" với 24/24 lớp encoder/decoder, và (c) ByT5-12/36-768M, một mô hình "decoder nặng". Vì các lớp decoder có tham số bổ sung được sử dụng cho attention decoder-encoder, các mô hình này lớn hơn thiết lập encoder nặng mặc định của chúng tôi. Tuy nhiên, mặc dù có tham số bổ sung, các cấu hình này hoạt động kém hơn trên tất cả các tác vụ, kể cả tác vụ sinh tạo GEM-XSum mà chúng tôi có thể kỳ vọng sẽ hưởng lợi từ decoder mạnh hơn.

Để kiểm tra liệu encoder nặng hơn có lợi cho mT5 hay không, chúng tôi huấn luyện (d) mT5-36/12-1.18B, một mô hình với cùng cấu hình như mT5-Large, nhưng chuyển sang 36/12 lớp encoder/decoder. Như với ByT5, chúng tôi quan sát thấy lợi ích qua tất cả ba tác vụ. Tuy nhiên, các lợi ích (+0.4, +1.8, +0.7) nhỏ hơn nhiều so với ByT5 (+2.9, +4.8, +5.2). Chúng tôi nghi ngờ encoder nặng có thể đặc biệt quan trọng trong các mô hình không từ vựng vì stack encoder phải thay thế cho ma trận embedding token có dung lượng cao bị thiếu, cho phép mô hình học "từ điển mềm" bao phủ có thể hàng triệu ánh xạ đặc thù từ dạng từ tới ý nghĩa. Trong nghiên cứu đồng thời, Wies et al. (2021) cũng quan sát thấy rằng các mô hình với từ vựng nhỏ hưởng lợi từ độ sâu bổ sung. Một lý do decoder có thể không cần nhiều dung lượng là trong suy luận, decoder được chạy tự hồi quy, sử dụng một lần forward đầy đủ cho mỗi dự đoán token. Với độ phân giải tăng của các chuỗi byte, điều này có nghĩa là các dự đoán ByT5 sẽ hưởng lợi từ 2-9 lần nhiều lần truyền qua stack decoder tùy thuộc vào ngôn ngữ (xem hình 2), so với mT5. Theo quan điểm này, ngay cả một decoder byte nông hơn có thể đủ để cạnh tranh với decoder từ con lớn hơn.

### 6.3 Chiều dài Span Bị che

Siêu tham số chiều dài span trung bình T5 kiểm soát chiều dài trung bình của các span bị che được sử dụng trong mục tiêu tiền huấn luyện không giám sát. Đối với T5 và mT5, điều này là 3 token SentencePiece. Đối với ByT5, chúng tôi giả thuyết rằng việc dự đoán các byte-span ngắn như vậy sẽ là một tác vụ quá dễ, vì điều này thường chỉ yêu cầu tái tạo một phần của một từ duy nhất (bất kể ngôn ngữ). Các mô hình ByT5 cuối cùng của chúng tôi sử dụng chiều dài span trung bình là 20 byte, dẫn đến các tác vụ tái tạo thách thức hơn.

Chúng tôi cũng chỉ ra các ablation (e-f) với chiều dài span 3 và 40. Bảng 8 cho thấy baseline của chúng tôi với chiều dài 20 hoạt động tốt nhất trên tác vụ phân loại XNLI, trong khi chiều dài 40 hoạt động tốt hơn trên TyDiQA-GoldP và GEM-XSum, cả hai đều yêu cầu tạo ra đầu ra văn bản ngôn ngữ tự nhiên.

### 6.4 Từ vựng Ký tự

Từ vựng cấp ký tự phục vụ như một điểm trung gian giữa từ vựng từ con lớn và từ vựng byte nhỏ. Như một điểm so sánh, chúng tôi huấn luyện (g) CharT5-36/12-1.23B: một mô hình với từ vựng gồm 47,198 ký tự, cùng tỷ lệ encoder/decoder như ByT5, và cùng số lượng tham số tổng thể như ByT5-Large và mT5-Large. Để đạt được số lượng tham số khớp này, chúng tôi đặt d_model=1376 và d_ff=3840. Tỷ lệ kết quả của các tham số liên quan đến vocab là 11% (so với 42% cho mT5-Large và 0.06% cho ByT5-Large). Từ vựng bản thân được triển khai bằng thư viện SentencePiece, nhưng với hạn chế bổ sung rằng token chỉ có thể đại diện cho các ký tự đơn. Các ký tự bao phủ tất cả những cái được nhìn thấy trong một mẫu 4 triệu tài liệu lấy từ corpus tiền huấn luyện mC4, trộn ngôn ngữ với tỷ lệ được sử dụng trong quá trình tiền huấn luyện. Chúng tôi sử dụng cơ chế fallback cấp byte, vì vậy không có ký tự nào ngoài từ vựng.

Bảng 8 cho thấy CharT5 khá cạnh tranh, nhưng hoạt động hơi kém hơn ByT5 trên tất cả ba tác vụ. Chúng tôi nghi ngờ điều này có thể do hai yếu tố: (i) CharT5 dành dung lượng cho các ký tự hiếm, và những tham số này sẽ được phân bổ tốt hơn trong các lớp transformer, và (ii) việc sử dụng byte UTF-8 tăng chiều dài chuỗi cho văn bản không ASCII, dẫn đến ngân sách tính toán bổ sung cho việc mã hóa và giải mã các ngôn ngữ với chữ viết không Latin.

## 7 So sánh Tốc độ

Bảng 9 so sánh FLOP tiền huấn luyện của ByT5 so với mT5, cũng như tốc độ tiền huấn luyện trên phần cứng cố định, như chuỗi mỗi giây với chiều dài chuỗi 1024. Qua tất cả các kích thước mô hình, ByT5 yêu cầu 1.2× nhiều thao tác hơn, dẫn đến 0.75× ít chuỗi mỗi giây.

Bảng 10 so sánh tốc độ suy luận của ByT5 và mT5 bằng cách đo số lượng dự đoán suy luận trung bình mỗi giây qua bốn tác vụ. Trên các tác vụ cấp từ, ByT5 khá cạnh tranh: trên SIGMORPHON 2020 Grapheme-to-Phoneme, nơi mục tiêu được viết bằng Bảng chữ cái Phiên âm Quốc tế, ByT5 và mT5 có tốc độ suy luận tương tự; trên chuyển tự Dakshina, ByT5 chậm hơn 1.5 đến 2.6 lần. Trên các tác vụ với chuỗi đầu vào dài hơn, sự chậm lại rõ rệt hơn: trên GEM-XSum (tóm tắt tài liệu), ByT5 chậm hơn 3.7 đến 6.4 lần so với mT5, trong khi trên phân loại XNLI zero-shot nó chậm hơn 6.4 đến 9.5 lần. Tổng quát hơn, chúng tôi quan sát thấy rằng - như mong đợi do encoder sâu hơn và decoder nông hơn - ByT5 đạt được tốc độ suy luận cạnh tranh hơn (so với mT5) trên các tác vụ với đầu vào ngắn và/hoặc mục tiêu dài. Theo quan điểm này, XNLI đại diện cho điều gì đó như trường hợp xấu nhất, nơi đầu vào là cặp câu và nhãn là các chữ số đơn {0, 1, 2}.

Thời gian cần thiết cho fine-tuning cũng thay đổi qua các tác vụ. Khi giữ kích thước batch không đổi ở một số lượng token cố định, chúng tôi thấy ByT5 thường cần nhiều bước fine-tuning hơn mT5 để đạt hiệu suất tối ưu trên tập holdout. Ví dụ, ByT5-Large cần 1.2× nhiều bước như mT5-Large để đạt hiệu suất validation đỉnh trên XNLI zero-shot, 2.6× nhiều bước cho TyDiQA-GoldP, và 4.5× cho GEM-XSum. Xu hướng tổng thể này được mong đợi, vì ít ví dụ được gắn nhãn hơn phù hợp với mỗi batch fine-tuning ByT5. Tuy nhiên, trên các tác vụ ủng hộ mạnh mẽ biểu diễn cấp byte, ByT5 đạt hiệu suất đỉnh trong ít bước fine-tuning hơn, gợi ý rằng mô hình có thể khái quát hóa tốt hơn từ một số lượng nhỏ ví dụ huấn luyện. Ví dụ, ByT5-Large cần 2.5× ít bước hơn mT5-Large để đạt hiệu suất đỉnh trên Dakshina.

Tổng thể, chúng tôi tin rằng chi phí tiền huấn luyện bổ sung (khoảng +33% thời gian wall) và chi phí fine-tuning bổ sung (cho một số tác vụ) được biện minh trong các ứng dụng không nhạy cảm với độ trễ bởi các lợi ích của độ phức tạp hệ thống giảm, tính mạnh mẽ tốt hơn với nhiễu, và hiệu suất tác vụ cải thiện trên nhiều benchmark.

## 8 Kết luận

Trong nghiên cứu này, chúng tôi đã trình bày ByT5, một biến thể không token của T5 đa ngôn ngữ (Xue et al., 2021) đơn giản hóa đường ống NLP bằng cách loại bỏ xây dựng từ vựng, tiền xử lý văn bản và tokenization. Về chất lượng tác vụ downstream, ByT5 có khả năng cạnh tranh với các mô hình mT5 khớp tham số dựa vào từ vựng SentencePiece. Cụ thể, ByT5 vượt trội hơn mT5 trong bất kỳ tình huống nào sau đây: (1) ở kích thước mô hình dưới 1 tỷ tham số, (2) trên các tác vụ sinh tạo, (3) trên các tác vụ đa ngôn ngữ với nhãn trong ngôn ngữ, (4) trên các tác vụ cấp từ nhạy cảm với chính tả và/hoặc phát âm, và (5) trong sự hiện diện của các loại nhiễu khác nhau.

Mặc dù đánh bại mT5 trong nhiều trường hợp, ByT5 hoạt động hơi kém trong các điều kiện nhất định - đáng chú ý nhất, trên các tác vụ phân loại tiếng Anh cho kích thước mô hình trên 1 tỷ tham số. Trong nghiên cứu tương lai, cũng sẽ quan trọng để đánh giá các cách tiếp cận không token trên một tập hợp đa dạng hơn các tác vụ, đặc biệt là những tác vụ mà các mô hình dựa trên ký tự truyền thống đã gặp khó khăn. Những tác vụ này bao gồm các tác vụ tương tự từ (Hiebert et al., 2018), các tác vụ gắn thẻ cú pháp và ngữ nghĩa (Durrani et al., 2019), và dịch máy từ nguồn không phải tiếng Anh sang tiếng Anh (Shaham and Levy, 2021).

Thông qua các ablation, chúng tôi đã chỉ ra rằng các mô hình encoder-decoder cấp byte hưởng lợi từ encoder "nặng hơn" (tách độ sâu encoder và decoder), và tác vụ tiền huấn luyện hưởng lợi từ việc che giấu các chuỗi ID dài hơn. Chúng tôi cũng chỉ ra rằng với số lượng tham số cố định, các mô hình cấp ký tự cho kết quả tương tự nhưng hơi kém hơn.

Thú vị là, các lợi ích chúng tôi quan sát với ByT5 được đạt được mặc dù mô hình được tiền huấn luyện trên ít văn bản hơn 4× so với mT5. Điều này gợi ý rằng các mô hình cấp byte có thể là những người học hiệu quả dữ liệu hơn. Những lợi ích này trong tính đơn giản thiết kế, chất lượng tác vụ và hiệu quả dữ liệu có được với chi phí tính toán bổ sung. Cách tiếp cận "không can thiệp" của chúng tôi trong việc đưa byte UTF-8 thô trực tiếp vào Transformer tốn +33% thời gian tiền huấn luyện, cũng như thời gian suy luận dài hơn (chậm hơn tới 10× trong trường hợp xấu nhất). Do đó, có không gian đáng kể để cải thiện. Chúng tôi tin rằng các kỹ thuật như hash embedding, attention cục bộ và down-sampling (Clark et al., 2021), cũng như tính toán thưa thớt (Fedus et al., 2021) có thể giúp giải quyết các vấn đề độ trễ, loại bỏ những rào cản còn lại đối với tương lai không token.

## Lời cảm ơn

Chúng tôi cảm ơn Jon Clark và Dan Garrette vì thảo luận xung quanh các cách tiếp cận không token và Noam Shazeer vì giúp đỡ xung quanh song song hóa mô hình trong T5. Chúng tôi cũng cảm ơn Jon Clark và các reviewer TACL cũng như action editor vì những bình luận hữu ích về bản thảo trước đó.
