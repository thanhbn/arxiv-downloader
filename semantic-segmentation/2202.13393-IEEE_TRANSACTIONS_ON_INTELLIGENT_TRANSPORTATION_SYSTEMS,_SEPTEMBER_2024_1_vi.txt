# 2202.13393.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/semantic-segmentation/2202.13393.pdf
# Kích thước tệp: 17837217 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, THÁNG 9 2024 1
TransKD: Chưng cất Tri thức Transformer cho
Phân đoạn Ngữ nghĩa Hiệu quả
Ruiping Liu, Kailun Yang∗, Alina Roitberg, Jiaming Zhang, Kunyu Peng, Huayao Liu,
Yaonan Wang, và Rainer Stiefelhagen

Tóm tắt —Các tiêu chuẩn đánh giá phân đoạn ngữ nghĩa trong lĩnh vực lái xe tự động được thống trị bởi các transformer lớn đã được tiền huấn luyện, tuy nhiên việc áp dụng rộng rãi của chúng bị cản trở bởi chi phí tính toán đáng kể và thời gian huấn luyện kéo dài. Để vượt qua hạn chế này, chúng tôi xem xét phân đoạn ngữ nghĩa hiệu quả từ góc độ chưng cất tri thức toàn diện và nhằm mục đích thu hẹp khoảng cách giữa việc trích xuất tri thức đa nguồn và embedding patch đặc trưng của transformer. Chúng tôi đề xuất khung Chưng cất Tri thức dựa trên Transformer (TransKD) để học các transformer học sinh nhỏ gọn bằng cách chưng cất cả bản đồ đặc trưng và embedding patch từ các transformer giáo viên lớn, bỏ qua quá trình tiền huấn luyện dài và giảm FLOPs hơn 85.0%. Cụ thể, chúng tôi đề xuất hai module cơ bản để thực hiện chưng cất bản đồ đặc trưng và chưng cất embedding patch tương ứng: (1) Cross Selective Fusion (CSF) cho phép chuyển giao tri thức giữa các đặc trưng cross-stage thông qua channel attention và chưng cất bản đồ đặc trưng trong các transformer phân cấp; (2) Patch Embedding Alignment (PEA) thực hiện biến đổi chiều trong quá trình patchifying để tạo điều kiện cho việc chưng cất embedding patch. Hơn nữa, chúng tôi giới thiệu hai module tối ưu hóa để tăng cường chưng cất embedding patch từ các góc độ khác nhau: (1) Global-Local Context Mixer (GL-Mixer) trích xuất cả thông tin toàn cục và cục bộ của một embedding đại diện; (2) Embedding Assistant (EA) hoạt động như một phương pháp embedding để kết nối liền mạch các mô hình giáo viên và học sinh với số lượng kênh của giáo viên. Các thí nghiệm trên các bộ dữ liệu Cityscapes, ACDC, NYUv2 và Pascal VOC2012 cho thấy TransKD vượt trội hơn các khung chưng cất tri thức tiên tiến và sánh ngang với phương pháp tiền huấn luyện tốn thời gian. Mã nguồn được công khai tại https://github.com/RuipingL/TransKD.

Thuật ngữ chỉ mục —Chưng cất Tri thức, Phân đoạn Ngữ nghĩa, Phân tích Cảnh, Vision Transformer, Hiểu biết Cảnh.

I. GIỚI THIỆU

Công việc này được hỗ trợ một phần bởi kinh phí từ chương trình thí điểm Core-Informatics của Hiệp hội Helmholtz (HGF), một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62473139), một phần bởi Công ty TNHH Công nghệ SurImage Hàng Châu, một phần bởi Bộ Khoa học, Nghiên cứu và Nghệ thuật Baden-Württemberg (MWK) thông qua Trường Sau đại học Hợp tác Khả năng tiếp cận thông qua Công nghệ Hỗ trợ dựa trên AI (KATE) theo Chương trình BW6-03, và một phần bởi Quỹ Sáng kiến và Kết nối của Hiệp hội Helmholtz trên phân vùng HAICORE@KIT và HOREKA@KIT.

R. Liu, J. Zhang, K. Peng, và R. Stiefelhagen thuộc Viện Anthropomatics và Robotics, Viện Công nghệ Karlsruhe, 76131 Karlsruhe, Đức.

K. Yang và Y. Wang thuộc Trường Robotics và Phòng thí nghiệm Kỹ thuật Quốc gia về Công nghệ Nhận thức và Điều khiển Thị giác Robot, Đại học Hunan, Changsha 410082, Trung Quốc.

A. Roitberg thuộc Viện Trí tuệ Nhân tạo, Đại học Stuttgart, 70569 Stuttgart, Đức.

J. Zhang cũng thuộc Viện Tính toán Thị giác, ETH Zurich, 8092 Zurich, Thụy Sĩ.

H. Liu thuộc NIO, Shanghai 201804, Trung Quốc.
∗Tác giả liên hệ (E-Mail: kailun.yang@hnu.edu.cn.)

[Hình 1: Các ví dụ khó cho phân đoạn ngữ nghĩa với các phương pháp chưng cất tri thức (KD). So với các phương pháp KD dựa trên Response [1] và dựa trên Feature [2], khung của chúng tôi, TransKD, cho phép mô hình dự đoán xe tải và hàng rào chính xác hơn bằng cách khám phá tri thức từ cả bản đồ đặc trưng và embedding patch.]

[Hình 2: So sánh với các khung chưng cất tri thức. Các TransKD của chúng tôi bù đắp hiệu quả khoảng cách hiệu suất giữa các mô hình không tiền huấn luyện và có tiền huấn luyện trong khi chỉ thêm số lượng tham số không đáng kể. Các mức tăng trên thanh chỉ ra số lượng tham số và hiệu suất khác nhau của TransKD. ImN: tiền huấn luyện trên ImageNet [3]. SKD: Structured Knowledge Distillation [4]. KR: Knowledge Review [2].]

PHÂN ĐOẠN ngữ nghĩa gán nhãn danh mục ở cấp độ pixel (xem ví dụ trong Hình 1) và là một công cụ quan trọng trong nhiều ứng dụng như lái xe tự động [5], [6], [7], [8] và hỗ trợ điều hướng cho người dùng đường bộ dễ bị tổn thương [9], [10]. Được thúc đẩy bởi những thành tựu của deep learning, phân đoạn ngữ nghĩa đã trao quyền cho việc hiểu cảnh chính xác trong bối cảnh lái xe tự động, như được chứng minh bởi độ chính xác đáng chú ý [11], [12]. Lái xe tự động đòi hỏi hoạt động tốc độ cao và khả năng phản ứng từ những khoảng cách đáng kể để phát hiện các mối nguy tiềm ẩn. Tuy nhiên, tài nguyên tính toán có sẵn trên các nền tảng di động xe hơi thường không đủ để hỗ trợ các mô hình phân đoạn ngữ nghĩa lớn và hiệu quả, và các mô hình nhỏ gọn nhưng kém hiệu quả thường không thể phân biệt các chi tiết xa. Hơn nữa, đã có sự thiếu sót đáng chú ý trong việc giải quyết các hạn chế tính toán quan trọng cho các ứng dụng thực tế, bao gồm tốc độ suy luận và huấn luyện, cũng như dung lượng bộ nhớ.

Trong khi Mạng Neural Tích chập (CNNs) đã là những người dẫn đầu trong nhận dạng thị giác gần một thập kỷ, chúng gặp khó khăn trong việc nắm bắt các tương tác quy mô lớn do tập trung vào các hoạt động lân cận cục bộ và trường tiếp nhận hạn chế. Vision transformers [11], [12], [13] vượt qua thách thức này thông qua khái niệm self-attention [14], học cách tăng cường hoặc giảm bớt các phần của đầu vào ở quy mô toàn cục. Chất lượng xuất sắc này trong việc mô hình hóa các phụ thuộc tầm xa đã đặt transformers ở vị trí hàng đầu của hầu như tất cả các tiêu chuẩn phân đoạn [11], [12], [13]. Thật không may, transformers thường rất lớn và không tổng quát hóa tốt khi được huấn luyện trên lượng dữ liệu không đủ do thiếu inductive bias so với CNNs [13], chẳng hạn như translation equivariance và locality, điều này có liên quan đến nhiệm vụ phân đoạn ngữ nghĩa. Do đó, vision transformers thường cho thấy hiệu suất kém thỏa mãn hơn trên các bộ dữ liệu cụ thể nhiệm vụ nhỏ hơn, và quá trình tiền huấn luyện tốn thời gian trên bộ dữ liệu lớn là cần thiết [11], [13], ví dụ, ≥100 epochs trên ImageNet [3]. Chi phí tính toán và huấn luyện cao tạo thành một nút thắt cổ chai đáng kể cho các ứng dụng thực tế, nơi các mô hình cồng kềnh không thể được triển khai trên thiết bị di động, trong khi các mô hình nhỏ hơn gặp khó khăn với các dự đoán không chính xác [4], [15].

Một cách phổ biến để giảm chi phí tính toán là chưng cất tri thức [17], cung cấp cơ chế để chuyển giao tri thức hiệu quả giữa mô hình giáo viên lớn và mạng học sinh nhỏ hơn và đã được nghiên cứu rộng rãi cho deep CNNs [4], [1], [2]. Gou et al. [16] phân biệt ba loại tri thức cần được chưng cất (xem Hình 3a-c): tri thức dựa trên response [4], [1], [17], tri thức dựa trên feature [4], [18], [19], và tri thức dựa trên relation [2], [20], [21]. Chưng cất tri thức dựa trên response và feature sử dụng đầu ra của một lớp cụ thể duy nhất của mô hình giáo viên (lớp cuối hoặc lớp trung gian tương ứng), trong khi chưng cất tri thức dựa trên relation theo dõi mối quan hệ giữa nhiều lớp đồng thời. Các paradigm này chủ yếu được thiết kế với việc chuyển giao CNN-to-CNN trong tâm trí [4], [1], [2] và không bao quát đầy đủ việc chưng cất tri thức từ các khối trung gian đặc trưng của transformer. Phương pháp patch embedding là một module đặc trưng của transformer để phân chia hình ảnh đầu vào hoặc bản đồ đặc trưng thành một chuỗi các patch. Nó thường được thực hiện thông qua các hoạt động tích chập với các tham số có thể học, thay vì bằng cách reshape. Theo vanilla vision transformer [13], việc đơn giản phân chia hình ảnh đầu vào hoặc bản đồ đặc trưng thành các patch lớn không chồng lên nhau là không thực tế để học các quan hệ không gian. Hạn chế này được giải quyết với các phương pháp embedding tối ưu, ví dụ như learnable positional embeddings [13], conditional positional embeddings [22], [23], shifted windows [24], và overlapping patch embeddings [25], [26]. Nhận thấy rằng có khoảng cách lớn giữa hiệu suất của các transformers với quá trình patchifying đơn giản và các phương pháp embedding tối ưu, chúng tôi nhận ra tầm quan trọng của thông tin tuần tự trong patch embeddings, được xử lý bởi module embedding biến đổi bản đồ đặc trưng thành các embeddings này. Do đó, chúng tôi coi patch embeddings như một nguồn tri thức đặc trưng của transformer.

Để đạt được phân đoạn ngữ nghĩa hiệu quả, chúng tôi đề xuất Chưng cất Tri thức dựa trên Transformer (TransKD), khung đầu tiên được thiết kế để chưng cất tri thức từ các transformers phân đoạn ngữ nghĩa quy mô lớn. Chúng tôi tập trung vào việc chưng cất toàn diện từ các nguồn tri thức khác nhau trong các khối xây dựng đặc trưng của transformer (ý tưởng chính của chúng tôi được minh họa trong Hình 3d). TransKD chưng cất thông tin từ cả (1) bản đồ đặc trưng và (2) patch embeddings đặc trưng của transformer. Như đã đề cập trước đó, patch embeddings khác với bản đồ đặc trưng không chỉ về hình dạng và chiều khác biệt mà còn về tri thức vốn có thu được thông qua quá trình patchifying. Do đó, chúng tôi cho rằng patch embeddings đặc trưng của transformer nên được khai thác đầy đủ để tối đa hóa tiềm năng của việc chưng cất tri thức transformer-transformer. Bằng cách kết hợp tri thức từ bản đồ đặc trưng và patch embeddings, cả mối quan hệ không gian và tuần tự có thể được thu hoạch, từ đó thúc đẩy phân đoạn ngữ nghĩa hiệu quả thông qua chưng cất.

Cụ thể, chúng tôi thiết kế hai module cơ bản và hai module tối ưu hóa trong khung TransKD. (1) Để chưng cất bản đồ đặc trưng, chúng tôi xây dựng một sơ đồ dựa trên relation trên Knowledge Review [2] và giới thiệu module Cross Selective Fusion (CSF) để hợp nhất các bản đồ đặc trưng cross-stage thông qua channel attention và xây dựng các luồng chưng cất bản đồ đặc trưng trong các transformers phân cấp. (2) Để chưng cất patch embedding, chúng tôi giới thiệu module cơ bản Patch Embedding Alignment (PEA) để thực hiện biến đổi chiều của patch embeddings dọc theo chiều kênh để tạo điều kiện cho các luồng chưng cất patch embedding đa giai đoạn. (3) Nhận ra rằng cả mối quan hệ lân cận và tầm xa đều quan trọng cho phân đoạn ngữ nghĩa [26], [27], Global-Local Context Mixer (GL-Mixer) của chúng tôi trích xuất cả thông tin toàn cục và cục bộ của một embedding đại diện để chưng cất. (4) Hơn nữa, xem xét khoảng cách lớn giữa kích thước của học sinh và giáo viên ảnh hưởng tiêu cực đến kết quả chưng cất tri thức [28], [29], chúng tôi trình bày module Embedding Assistant (EA), hoạt động như một embedder với số lượng kênh của giáo viên. EA giúp xây dựng mô hình trợ lý giáo viên giả bằng cách kết hợp các khối transformer của học sinh và kết nối liền mạch các mô hình giáo viên và học sinh.

Chúng tôi chứng minh lợi ích của phương pháp của chúng tôi cho các ứng dụng di động thực tế trên bốn tiêu chuẩn: bộ dữ liệu cảnh đường phố chung (Cityscapes [30]), bộ dữ liệu cảnh đường phố bất lợi (ACDC [31]), bộ dữ liệu hiểu biết trong nhà (NYUv2 [32]), và bộ dữ liệu tập trung vào đối tượng (Pascal VOC2012 [33]). Các thí nghiệm toàn diện cho thấy khung của chúng tôi vượt trội hơn các đối tác KD tiên tiến với khoảng cách lớn. So với mô hình giáo viên cồng kềnh, TransKD giảm các hoạt động dấu phẩy động (FLOPs) hơn 85.0%, trong khi duy trì độ chính xác cạnh tranh. Bằng cách thống nhất chưng cất patch embedding và bản đồ đặc trưng, TransKD tăng cường độ mạnh mẽ của việc phân đoạn các ví dụ khó nơi các paradigm trước đó gặp khó khăn, như được hiển thị trong Hình 1. Được đo chuẩn so với phương pháp chỉ sử dụng bản đồ đặc trưng Knowledge Review [2], TransKD-Base tăng cường hiệu suất chưng cất 5.18% trong mean Intersection over Union (mIoU) trong khi chỉ thêm 0.21M tham số không đáng kể trong giai đoạn huấn luyện, như được hiển thị trong Hình 2. Trên Cityscapes, TransKD cải thiện mIoU của SegFormer-B0 [26] không tiền huấn luyện 13.12% và của phiên bản có tiền huấn luyện 2.09%. Chúng tôi tiếp tục xác thực các mô hình transformer khác nhau [25], [26], [34] và xác nhận rằng TransKD liên tục thúc đẩy độ chính xác của các segmenter nhỏ gọn. Cuối cùng, TransKD sánh ngang với hiệu suất của phương pháp tiền huấn luyện tốn thời gian nổi tiếng. Mô hình tốt nhất của chúng tôi đạt 75.74% mIoU chỉ với 3.72M tham số.

Nhìn tổng quát, công việc này mang lại những đóng góp sau:
• Chúng tôi là những người đầu tiên đề xuất khung chưng cất tri thức transformer-to-transformer trong phân đoạn ngữ nghĩa và đầu tiên sử dụng patch embedding đặc trưng của transformer như một trong những nguồn tri thức.
• Chúng tôi cung cấp góc nhìn mới về việc chưng cất tri thức từ patch embeddings đặc trưng của transformer. Khung TransKD của chúng tôi thống nhất việc chưng cất bản đồ đặc trưng và patch embedding.
• Để đạt được mục đích này, chúng tôi thiết kế hai module cơ bản, Cross Selective Fusion (CSF) và Patch Embedding Alignment (PEA), và hai module tối ưu hóa, Global-Local Context Mixer (GL-Mixer) và Embedding Assistant (EA).
• Các thí nghiệm chi tiết xác thận lợi thế của TransKD trên các bộ dữ liệu Cityscapes, ACDC, NYUv2 và Pascal VOC2012, chứng minh những cải thiện đáng kể so với các phương pháp chưng cất tri thức hiện có.

--- TRANG 2 ---
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, THÁNG 9 2024 2

Giáo viên Học sinh
Giáo viênGiáo viên
Giáo viênHọc sinhHọc sinh
Học sinh(a) (b)
(c) (d)

Hình 3: (a)-(c) Chưng cất tri thức trong thị giác máy tính được chia thành ba loại [16]: chưng cất tri thức dựa trên response, chưng cất tri thức dựa trên feature, và chưng cất tri thức dựa trên relation. (d) TransKD trích xuất tri thức dựa trên relation của bản đồ đặc trưng và tri thức patch embedding đặc trưng của transformer tại mỗi giai đoạn.

II. CÔNG VIỆC LIÊN QUAN

A. Từ Phân đoạn Ngữ nghĩa Chính xác đến Hiệu quả

Phân đoạn ngữ nghĩa đã chứng kiến tiến bộ to lớn kể từ khi Fully Convolutional Networks (FCNs) [35] đầu tiên xem xét vấn đề phân loại pixel từ góc độ end-to-end. Nhiều mạng tiếp theo đã theo sơ đồ này, thường nâng cao độ chính xác thông qua kiến trúc encoder-decoder [36] hoặc tổng hợp ngữ cảnh đa tỷ lệ [27], [37]. Một nhóm phương pháp quan trọng tận dụng non-local self-attention [14], [38] để nắm bắt các phụ thuộc ngữ cảnh tầm xa và thúc đẩy lý luận toàn cục [39], [40]. Thành công gần đây của vision transformers [13], [41] dẫn đến sự phát triển của nhiều kiến trúc dựa trên transformer cho phân đoạn ngữ nghĩa [11], [12] từ đó trở thành tiên tiến trên các tiêu chuẩn chính [30], [32], [31], nhờ khả năng nắm bắt các phụ thuộc toàn cục từ các lớp sớm thông qua các module trộn token như self-attention [14] hoặc các khối Multi-Layer Perceptron (MLP) [42]. Tuy nhiên, chi phí tính toán và quá trình huấn luyện dài vẫn là điểm yếu của nghiên cứu transformer hướng đến độ chính xác.

Mặt khác, nhiều công việc giải quyết cụ thể phân đoạn ngữ nghĩa hiệu quả, nhằm tạo ra sự cân bằng tốt giữa tốc độ và độ chính xác. Dòng nghiên cứu này làm nhỏ gọn kiến trúc thông qua các kỹ thuật như early downsampling [43], filter factorization [5], [15], kiến trúc đa nhánh [44], [45], và ladder-style upsampling [46], [47]. Các backbone phân loại nhẹ như MobileNets [48] và ShuffleNets [49] cũng đã được điều chỉnh để đẩy nhanh phân đoạn hình ảnh. Hơn nữa, SegFormer [26] giới thiệu khung phân đoạn ngữ nghĩa nhẹ và hiệu quả thống nhất các bản đồ đặc trưng đầu ra của backbone với bộ giải mã MLP đơn giản. Khác với tất cả các công việc này, TransKD của chúng tôi xem các transformers phân đoạn tiết kiệm tài nguyên từ góc nhìn mới về chưng cất tri thức toàn diện. Chúng tôi giới thiệu khung đồng thời trích xuất tri thức từ các khối khác nhau của vision transformers quy mô lớn và tái sử dụng nó cho các transformers học sinh nhỏ gọn và nhanh.

B. Vision Transformer cho Dense Prediction

Kiến trúc Transformer đã trở thành tiêu chuẩn de-facto trong Xử lý Ngôn ngữ Tự nhiên (NLP), chủ yếu do hiệu quả của self-attention xếp chồng và khả năng nắm bắt các mối quan hệ tầm xa trong dữ liệu. Thành công của các mô hình dựa trên self-attention cũng đã mở rộng sang lĩnh vực nhận dạng thị giác, bắt đầu với việc giới thiệu Vision Transformer (ViT) [13] – transformer đầu tiên mang lại kết quả mạnh mẽ trên ImageNet [3]. Các phát triển tiếp theo đã tối ưu hóa thêm hiệu suất của vision transformer, kết hợp các tính năng như mạng đa tỷ lệ [24], [26], [50], tăng độ sâu [51], [52], chưng cất tri thức [53], [54] và sự pha trộn của attention toàn cục và cục bộ [22], [55].

Đồng thời, nhiều vision transformers nhẹ [54], [56] đã xuất hiện. Ví dụ, LeViT [54] là mô hình hybrid tái sử dụng các giai đoạn ResNet trong kiến trúc transformer trong khi MobileViT [57] tận dụng các khối MobileNetV2 [48] dọc theo đường downsampling và MobileFormer [58] thiết lập kiến trúc dual-stream bằng cách kết nối MobileNet và nhánh transformer của nó. Thật không may, các nghiên cứu gần đây [57] cho thấy MobileViT và các mô hình dựa trên ViT khác vẫn chưa đủ hiệu quả cho xử lý thời gian thực trên thiết bị di động. Trong công việc này, chúng tôi phát triển khung chưng cất tri thức dựa trên vision transformers, liên tục cải thiện hiệu suất của các transformers dự đoán dense và phân đoạn ngữ nghĩa nhỏ gọn [26], [25], [34], dẫn đến sự đánh đổi tốt hơn nhiều về độ chính xác, chi phí tính toán và yêu cầu tiền huấn luyện.

C. Chưng cất Tri thức cho Phân đoạn Ngữ nghĩa

Hinton et al. [17] giới thiệu khái niệm Chưng cất Tri thức (KD), bao gồm việc chuyển giao tri thức từ mô hình quy mô lớn sang mô hình nhỏ gọn hơn. Hai mô hình này hoạt động như giáo viên và học sinh, và paradigm này được sử dụng rộng rãi trong thị giác máy tính [16], [59]. Trong công việc này, mục tiêu của chúng tôi là nghiên cứu và thiết kế khung KD cụ thể cho phân đoạn ngữ nghĩa với các mô hình dựa trên transformer.

Các phương pháp KD đã được phân loại [16] thành ba loại dựa trên hình thức tri thức: KD dựa trên response [4], [1], [18], KD dựa trên feature [1], [18], [19], [60], [61], [62], và KD dựa trên relation [2], [20], [21]. Liu et al. [4] đầu tiên giới thiệu ý tưởng chưng cất tri thức có cấu trúc, tận dụng học đối kháng để căn chỉnh bản đồ phân đoạn được tạo ra bởi mạng học sinh nhỏ gọn với mạng giáo viên cồng kềnh. Knowledge Review (KR) [2] lần đầu tiên đề xuất chưng cất tri thức sử dụng các đường kết nối cross-stage. Double Similarity Distillation (DSD) [20] chuyển giao cả các phụ thuộc không gian chi tiết và các tương quan danh mục toàn cục. Khác với các công việc trước đây chủ yếu dành cho việc chưng cất tri thức spatial-wise, một số công việc gần đây tập trung vào việc chưng cất phân phối channel-wise. Channel Distillation (CD) [1] nhấn mạnh các phân phối mềm của kênh và chú ý đến các phần nổi bật nhất của bản đồ channel-wise.

Ngoài ra, việc thích nghi tri thức được đề cập trong [63] bằng cách tối ưu hóa chưng cất affinity. Adaptive Perspective Distillation (APD) [64] khai thác các tín hiệu ngữ cảnh chi tiết từ mỗi mẫu huấn luyện, trong khi [65] sử dụng phương pháp chưng cất phân cấp và cho phép khớp không gian một-đến-tất cả. Tất cả các công việc này được dành cho việc chưng cất tri thức CNN-to-CNN hoặc CNN-to-transformer. Ngược lại, mô hình TransKD được đề xuất của chúng tôi khám phá khái niệm này theo cách transformer-to-transformer mới, bằng cách xem xét rõ ràng tri thức đa nguồn trong kiến trúc transformer và tính đến patch embeddings đặc trưng của transformer.

III. KHUNG ĐỀ XUẤT: TRANSKD

A. Tổng quan

Trung tâm của công việc này là việc chuyển giao tri thức transformer-to-transformer cho phân đoạn ngữ nghĩa hiệu quả. Để tạo ra sự cân bằng tốt hơn giữa độ chính xác, chi phí tính toán và lượng tiền huấn luyện cần thiết, chúng tôi lần đầu tiên xem xét phân đoạn ngữ nghĩa thông qua lăng kính chưng cất tri thức đa nguồn trong visual transformers. Trong khi chưng cất tri thức transformer-to-transformer được khám phá trong xử lý ngôn ngữ tự nhiên [66], [67], [68], nó đã bị bỏ qua khá nhiều trong phân đoạn ngữ nghĩa và vision transformers nói chung, nơi hầu hết các mô hình vẫn tập trung xung quanh việc chưng cất tri thức dựa trên CNN. Việc áp dụng các phương pháp như vậy để phù hợp với visual transformers không đơn giản do các khối xây dựng đặc trưng kiến trúc khác nhau phát sinh, ví dụ từ quá trình patchifying.

Các transformers phân đoạn ngữ nghĩa hiện có thường theo cấu trúc isotropic [11], [69], [70] hoặc cấu trúc phân cấp bốn giai đoạn [25], [26], [34], [50], [71]. Vì cấu trúc sau tốt hơn trong việc mô hình hóa thông tin tầm xa đa tỷ lệ [24], [50], nó chứng minh linh hoạt hơn trong các nhiệm vụ dự đoán dense như phân đoạn ngữ nghĩa pixel-wise. Do đó, trong công việc này, việc chưng cất tri thức được thực hiện trên các transformers bốn giai đoạn, như được minh họa trong Hình 4.

Bước đầu tiên của pipeline vision transformer chung là "patchify" đầu vào I (một hình ảnh hoặc bản đồ đặc trưng):
E = PatchEmbed(I). (1)

"Patchification" này là thành phần chính của vision transformers vì nó cho phép chúng ta đổi các nhiệm vụ dựa trên thị giác thành các vấn đề sequence-to-sequence mà loại mô hình này ban đầu được xây dựng [14]. Module patch embedding chia hình ảnh đầu vào thành các patches. Ở đây, H, W và C đại diện cho chiều cao, chiều rộng và số lượng kênh của hình ảnh đầu vào hoặc bản đồ đặc trưng của module embedding tương ứng. Module sau đó biến đổi các patches này (chủ yếu thông qua hoạt động tích chập) thành một chuỗi các patch embeddings đại diện E ∈ R^(N×C'), trong đó N là số lượng patches và C' là số lượng kênh. Như thường được nhấn mạnh trong nghiên cứu trước đây, các patch embeddings kết quả truyền tải thông tin quan trọng về các mối quan hệ tuần tự tầm xa [11], [72], location priors [11], [13], [69] và local continuity cues [25], [26], [71]. Cả ngữ cảnh toàn cục và thông tin cục bộ có trong patch embeddings đều có liên quan đến phân đoạn ngữ nghĩa.

Tiếp theo, các patch embeddings được truyền đến một ngăn xếp các lớp mã hóa transformer bao gồm multi-head self-attention và position-wise fully connected feed-forward network [13], [14]. Các đầu ra của các lớp này sau đó được reshape thành bản đồ đặc trưng F:
F = TransformerEncode(E), (2)

trong đó bản đồ đặc trưng F ∈ R^(C'×H'×W') đánh dấu đầu ra của giai đoạn transformer hiện tại và do đó là đầu vào cho lớp tiếp theo. Một hệ quả tích cực của self-attention khám phá tất cả các patch tokens mà không bị hạn chế trong một trường tiếp nhận nhất định là ngữ cảnh toàn cục cũng được bao phủ trong bản đồ đặc trưng kết quả. Chưng cất tri thức từ cả bản đồ đặc trưng và patch embeddings cho phép chúng ta theo dõi thông tin bổ sung trong transformer, như các mối quan hệ không gian và tuần tự.

Được thúc đẩy bởi điều này, chúng tôi đề xuất Chưng cất Tri thức dựa trên Transformer (TransKD) – một khung chưng cất tri thức transformer-to-transformer mới với mục tiêu tổng thể xây dựng các transformers phân đoạn ngữ nghĩa nhỏ nhưng chính xác ít phụ thuộc vào tiền huấn luyện tốn thời gian. TransKD tập trung xung quanh hai loại chuyển giao tri thức: (1) chưng cất patch embedding và (2) chưng cất bản đồ đặc trưng. Chưng cất patch embedding được thực hiện tại mỗi giai đoạn transformer trong số bốn giai đoạn, trong khi chưng cất bản đồ đặc trưng tận dụng chưng cất tri thức dựa trên relation để thu thập thông tin từ cả bản đồ đặc trưng within-stage và cross-stage. Tổng quan về pipeline cốt lõi này được mô tả trong Hình 4(a).

Các loss chưng cất thu được từ mỗi giai đoạn được thêm vào loss cross-entropy gốc L_CE trong quá trình huấn luyện. Tổng loss KD sau đó trở thành:
L = L_CE + Σ(m=1 to M) α_m L^m_embd + Σ(m=1 to M) β_m L^m_fm, (3)

trong đó L^m_embd và L^m_fm đề cập đến patch embedding loss và feature map loss giữa giai đoạn thứ m của giáo viên và học sinh. Như đã đề cập trước đây, chúng tôi khám phá transformer bốn giai đoạn điển hình trong công việc này, tức là M = 4 trong trường hợp của chúng tôi. Các biến α_m và β_m là phần tử thứ m của α và β, kiểm soát trọng số của patch embedding loss và feature map loss tại mỗi giai đoạn tương ứng. Các vector α và β được đặt là [0.1, 0.1, 0.5, 1] và [1, 1, 1, 1]. Trong các mô tả phương pháp sau, các ma trận với chỉ số trên in và out đề cập đến đầu vào và đầu ra của các module được đề xuất, trong khi những ma trận với chỉ số trên mid đề cập đến các ma trận được xử lý trong các module.

Trong Sec. III-B, chúng tôi trình bày thiết kế của sơ đồ chưng cất patch embedding, và trong Sec. III-C, chúng tôi mô tả phương pháp chưng cất bản đồ đặc trưng. Cuối cùng, trong Sec. III-D, chúng tôi mô tả các cấu hình của các biến thể khác nhau của khung TransKD.

B. Chưng cất Patch Embedding

Như đã thảo luận trước đây, patch embedding kết hợp thông tin vị trí giúp tăng cường bản đồ đặc trưng. Để tối đa hóa tiềm năng của việc chưng cất tri thức transformer-to-transformer, chúng tôi cho rằng patch embeddings đặc trưng của transformer nên được khai thác đầy đủ. Mặc dù có các thiết kế chuyên biệt, các mô hình giáo viên cồng kềnh liên tục sở hữu nhiều khối transformer hơn để trích xuất đặc trưng và các kênh bổ sung để lưu giữ tri thức. Để căn chỉnh chiều giữa patch embeddings của học sinh và giáo viên để tính toán hàm loss và giới thiệu các tham số có thể huấn luyện để học tri thức channel-wise, chúng tôi đề xuất module cơ bản, Patch Embedding Alignment (PEA), để tạo điều kiện cho việc chưng cất patch embedding đơn giản nhất. Ngoài ra, chúng tôi thiết kế Global-Local Context Mixer (GL-Mixer) và Embedding Assistant (EA) để tối ưu hóa chưng cất patch embedding.

Căn chỉnh patch embedding. Chúng tôi thực hiện Patch Embedding Alignment (PEA) thông qua phép chiếu tuyến tính để biến đổi chiều. Tiếp theo, Mean Squared Error (MSE) loss giữa patch embeddings của học sinh và giáo viên được tính toán, như được mô tả trong Phương trình (4):
L^m_embd = MSE(E^S_m W_e, E^T_m), (4)

trong đó E^S_m và E^T_m là chuỗi patch embeddings tại giai đoạn thứ m của học sinh và giáo viên. Một ma trận có thể học W_e ∈ R^(C^S_m × C^T_m) được áp dụng để chống lại sự không khớp chiều và chuyển giao tri thức channel-wise từ patch embeddings giáo viên đến học sinh. C^S_m và C^T_m biểu thị số lượng kênh trong các mô hình giáo viên và học sinh tương ứng tại giai đoạn thứ m. PEA kết nối các transformers giáo viên và học sinh và tạo điều kiện cho các luồng patch embedding đa giai đoạn, trong khi chỉ thêm độ phức tạp tính toán cận biên.

Global-local context mixer. Patch embeddings cung cấp các mẫu đa dạng [13], [26]. Do đó, các gợi ý ngữ cảnh toàn cục và các đặc trưng cục bộ tinh tế, cả hai đều cần thiết cho phân đoạn ngữ nghĩa, không thể được chưng cất đầy đủ chỉ bằng phép chiếu tuyến tính. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên giới thiệu thiết kế hai nhánh vào lĩnh vực chưng cất tri thức trong phân đoạn ngữ nghĩa. Ngữ cảnh toàn cục được trích xuất thông qua multi-head attention (nhánh trái trong Hình 5), và thông tin cục bộ được trích xuất thông qua lớp tích chập (nhánh phải trong Hình 5). Một hoạt động tích chập khác, theo sau bởi hàm sigmoid σ(·), được sử dụng để kiểm soát luồng thông tin. Với chuỗi patch embedding tại giai đoạn thứ m E^in_m, việc xử lý hai nhánh của các gợi ý toàn cục và cục bộ bên trong GL-Mixer được hình thức hóa trong Phương trình (5):

GL-Mixer(E^in_m) = MultiHeadAttn(E^in_m) + E^in_m * W + a ⊗ σ(E^in_m * V + b), (5)

trong đó W, V, a và b đề cập đến các tham số có thể học cho các hoạt động tích chập. Như được hiển thị trong Hình 5, các hoạt động tích chập được thực hiện thông qua các tích chập 3×3, và toàn bộ GL-Mixer được thiết kế để nhẹ. Theo cách này, với một chuỗi patch embedding đại diện trong một transformer dự đoán dense, cả ngữ cảnh toàn cục giàu ngữ nghĩa và các đặc trưng cục bộ giàu chi tiết có thể được thu hoạch để chưng cất.

Embedding assistant. Chưng cất tri thức gặp vấn đề khi khoảng cách kích thước giữa học sinh và giáo viên là đáng kể [28], [29]. Nếu giáo viên trở nên rất phức tạp, học sinh cuối cùng thiếu khả năng hoặc cơ chế đầy đủ để bắt chước hành vi của nó mặc dù nhận được các gợi ý đa giai đoạn. Mirzadeh et al. [28] giới thiệu Teacher Assistant (TA) có kích thước trung gian để thu hẹp khoảng cách kích thước. TA hoạt động như người trung gian giữa giáo viên và học sinh: nó được chưng cất từ mô hình giáo viên và sau đó truyền tri thức cho học sinh. TA theo cùng cấu trúc với học sinh và giáo viên, ngoại trừ số lượng hidden states và transformer layers [68]. Số lượng hidden states được áp dụng từ học sinh, trong khi số lượng transformer layers tương ứng với số lượng của giáo viên. Tuy nhiên, việc thực hiện Teacher Assistant cho chưng cất tri thức là quá trình hai bước. Một mô hình trợ lý bổ sung TA cần được huấn luyện, lớn hơn mô hình học sinh, do đó gây ra chi phí tính toán bổ sung và thời gian học.

Chúng tôi rút cảm hứng từ khái niệm TA nhưng giải quyết nhược điểm đã đề cập và đề xuất module Embedding Assistant (EA) hoạt động như người trung gian giữa giáo viên và học sinh mà không cần chưng cất hai giai đoạn. Tổng quan về module EA được đề xuất được đưa ra trong Hình 4(c). EA là một module embedding có cấu trúc giống hệt với module embedding của học sinh và giáo viên và với số lượng kênh C tương ứng với số lượng của giáo viên. Với cùng kích thước kernel của hoạt động tích chập để thực hiện patch embedding, các đầu ra của EA, PEA và module embedding của giáo viên có cùng số lượng patches. Như được hiển thị trong Hình 4(c) và Hình 6, kết quả của EA và PEA được hợp nhất thông qua phép cộng element-wise từng giai đoạn.

Cho rằng một khối transformer của giáo viên bao gồm L-transformer layers với kích thước embedding d_e, và một khối transformer của học sinh bao gồm M-transformer layers với kích thước embedding d'_e, việc kết hợp các module EA và các khối transformer của học sinh có thể được xem như một mô hình trợ lý giả với khối transformer gồm M-transformer layers và kích thước embedding d_e. Do đó, mô hình trợ lý giả kích thước trung gian, tức là sự kết hợp của tất cả bốn EA giai đoạn và các khối transformer học sinh, giúp thu hẹp khoảng cách kích thước học sinh-giáo viên và cung cấp thông tin bổ sung. Vì phương pháp embedding của các mô hình khác nhau rất khác nhau, khung chưng cất tri thức sử dụng các EA được đề xuất không đạt được tính linh hoạt plug-and-play hoàn toàn. Tuy nhiên, so với teacher assistant [28], chúng tôi tăng cường việc chưng cất học sinh-giáo viên mà không cần quá trình chưng cất tri thức hai bước tốn kém.

C. Chưng cất Bản đồ Đặc trưng

Nhiều phương pháp chưng cất tri thức triển khai sơ đồ dựa trên feature một giai đoạn cho phân đoạn ngữ nghĩa [1], [18], [19], tức là thông tin được trao đổi tại một độ sâu mạng duy nhất. Cân nhắc tiếp theo của chúng tôi là việc chưng cất tri thức dựa trên relation cross-stage quan trọng cho các transformers phân đoạn hiệu quả, vì nó cho phép chuyển giao thông tin qua các độ sâu mạng khác nhau. Các ý tưởng tương tự đã cho thấy thành công trong quá khứ, ví dụ, các transformers dự đoán dense phân cấp sử dụng bản đồ đặc trưng cross-stage [26], [50].

Chúng tôi xem xét lại Knowledge Review [2], nghiên cứu các kết nối qua các cấp độ khác nhau của các mô hình CNN giáo viên và học sinh. Tuy nhiên, Attention Based Fusion (ABF), module hợp nhất bản đồ đặc trưng gốc trong Knowledge Review, tổng hợp động các bản đồ đặc trưng thông qua bản đồ attention không gian, nhưng bỏ qua thông tin channel-wise. Chúng tôi lập luận rằng các mô hình giáo viên thường có nhiều khối hơn để trích xuất đặc trưng và nhiều kênh hơn để lưu giữ thông tin, nhưng chúng không khác biệt về độ phân giải không gian so với các mô hình học sinh. Do đó, chúng tôi nhằm mục đích thu hẹp sự khác biệt channel-wise giữa các bản đồ đặc trưng của các mô hình học sinh và giáo viên trong khi kết hợp thông tin không gian thông qua thiết kế của module lựa chọn kênh bổ sung. Thiết kế của chúng tôi được thúc đẩy bởi hai khía cạnh sau: 1) Các gợi ý channel-wise của bản đồ đặc trưng cung cấp priors thông tin cho các bộ trích xuất đặc trưng. Nhiều công việc trước đây [73], [74], tập trung vào việc hợp nhất bản đồ đặc trưng, tìm cách hợp nhất thông tin không gian và kênh thông qua channel attention. Hơn nữa, Shu et al. [1] chỉ ra rằng việc tối thiểu hóa phân kỳ Kullback-Leibler (KL) channel-wise của các bản đồ đặc trưng và dự đoán hiệu quả hơn so với việc sử dụng đối tác không gian của nó. 2) Vì việc chưng cất patch embedding theo dõi các phụ thuộc giữa các vị trí không gian xa thông qua học sequence-wise, việc chưng cất bản đồ đặc trưng nên tham gia tri thức không gian toàn diện hơn trong các kênh. Do đó, điều quan trọng là làm nổi bật các kênh từ các giai đoạn nhất định thông qua tái trọng số và tính đến các phụ thuộc kênh cross-stage. Để đạt được điều này, chúng tôi đề xuất module Cross Selective Fusion (CSF) để hợp nhất thông tin một cách thích ứng qua các giai đoạn thông qua channel attention.

Cross selective fusion. Trong module này, các bản đồ đặc trưng từ giai đoạn học sinh hiện tại và CSF tiếp theo đầu tiên được thay đổi kích thước thành chiều thống nhất và sau đó được hợp nhất thông qua attention channel-wise. Như đã được chứng minh trước đây trong [73], các trích xuất đặc trưng với các kích thước kernel khác nhau hoặc modality có thể được hợp nhất theo cách thích ứng. Tương tự, chúng tôi đề xuất hợp nhất các bản đồ đặc trưng cross-stage cho việc chưng cất dựa trên relation. Như được hiển thị trong Hình 7, chúng tôi đầu tiên thực hiện tích chập 1×1 để thực hiện biến đổi channel-wise của bản đồ đặc trưng đầu vào F^in_m và thực hiện nội suy để thực hiện biến đổi không gian của bản đồ đặc trưng F^mid_{m+1} từ CSF của giai đoạn tiếp theo. Chiều cao và chiều rộng của chúng giống với bản đồ đặc trưng của giai đoạn thứ m F^in_m. Các kết quả ẽF^in_m và ẽF^mid_{m+1} được cộng element-wise, như được mô tả trong Phương trình (6):

F_m = ẽF^in_m + ẽF^mid_{m+1}. (6)

Thông tin toàn cục sau đó được nhúng thông qua global average pooling, trong đó s_c biểu thị thống kê channel-wise, như được hiển thị trong Phương trình (7):

s_c = F_{gp}(F_m) = 1/(H×W) Σ_{i=1}^H Σ_{j=1}^W F_m(i,j). (7)

Sau đó, một đặc trưng nhỏ gọn z ∈ R^{d×1} trong Phương trình (8) được tạo ra cho các lựa chọn chính xác và thích ứng:

z = F_{fc}(s_c) = δ(B(φ(s_c))), d = max(C/r, L), (8)

trong đó δ(·) biểu thị hàm ReLU, B(·) biểu thị Batch Normalization, và φ(·) biểu thị tích chập 1×1. d là chiều của vector z, được kiểm soát bởi tỉ lệ giảm r. L được chọn là giá trị tối thiểu của d.

Soft attention qua các kênh sau đó được sử dụng để lựa chọn thông tin một cách thích ứng từ các nhánh khác nhau:

a_c = e^{A_c z}/(e^{A_c z} + e^{B_c z}), b_c = e^{B_c z}/(e^{A_c z} + e^{B_c z}), (9)

F^{mid}_{m,c} = a_c · ẽF^in_m + b_c · ẽF^mid_{m+1}, a_c + b_c = 1, (10)

trong đó A, B ∈ R^{C×d} là hai ma trận có thể học, và A_c, B_c ∈ R^{1×d} là các phần tử thứ c của chúng. a_c, b_c và F^{mid}_{m,c} là các phần tử thứ c của a, b và F^{mid}_m. a và b biểu thị vector soft attention cho ẽF^in_m và ẽF^mid_{m+1} tương ứng. Hơn nữa, chúng tôi thực hiện tích chập 3×3 để biến đổi F^{mid}_m thành F^{out}_m, đảm bảo nó có cùng chiều với bản đồ đặc trưng của giáo viên.

Để huấn luyện, chúng tôi tính toán hierarchical context loss (HCL) [2] giữa các đầu ra của CSF và khối transformer giáo viên tại mỗi giai đoạn, như được hiển thị trong Hình 4. Bản đồ đặc trưng thu được từ CSF tại mỗi giai đoạn, với hình dạng [n,c,h,w], được chia thành thông tin ngữ cảnh 4 cấp độ thông qua adaptive average pooling. Chiều cao của các bản đồ đặc trưng đa cấp độ kết quả tại một giai đoạn là [h,4,2,1], chỉ ra rằng các hình dạng của bốn bản đồ đặc trưng trừu tượng là [n,c,h,w], [n,c,4,4], [n,c,2,2] và [n,c,1,1]. Tại mỗi giai đoạn, các khoảng cách L2 được sử dụng để chưng cất tri thức giữa các cấp độ ngữ cảnh. Loss HCL cuối cùng giữa các bản đồ đặc trưng tại giai đoạn thứ m được tính toán như:

L^m_{fm} = 1/(1 + Σ_l 1/2^l) Σ_{l=0}^3 1/2^l MSE(F^{out}_{m,l}, F^T_{m,l}), (11)

trong đó l đề cập đến số lượng cấp độ tại một giai đoạn. Mục đích của việc áp dụng loss HCL sau mỗi giai đoạn là khuyến khích việc chưng cất tri thức ở các cấp độ trừu tượng khác nhau.

D. Cấu hình

Cốt lõi của khung TransKD của chúng tôi bao gồm hai chiến lược chưng cất tri thức: chưng cất patch embedding (trong Sec. III-B) và chưng cất bản đồ đặc trưng (trong Sec. III-C). Để thực hiện thiết kế này, chúng tôi giới thiệu bốn module: các module PEA, GL-Mixer và EA hướng dẫn việc chưng cất patch embedding, trong khi module hợp nhất bản đồ đặc trưng CSF tăng cường việc chưng cất bản đồ đặc trưng dựa trên relation cross-stage.

Nhìn chung, CSF và PEA có thể được xem như các thành phần cơ bản của TransKD. GL-Mixer hoạt động trên embedding đại diện sâu nhất (tức là giai đoạn thứ tư cuối cùng), tạo ra các mẫu đa dạng cao [26]. Nó tăng cường việc chưng cất patch embedding tại giai đoạn cuối bằng cách thu hoạch thông tin ngữ cảnh toàn cục và cục bộ thông qua hai nhánh. Các module EA được đặt tại mỗi giai đoạn và hoạt động như phương pháp embedding để cung cấp các luồng chưng cất bổ sung bằng cách sử dụng phương pháp embedding giống hệt với phương pháp của giáo viên và học sinh, với số lượng kênh của giáo viên. Cần lưu ý rằng, vì các kiến trúc transformer khác nhau có các phương pháp embedding khác nhau, khung tận dụng EA không mang lại tính linh hoạt plug-and-play hoàn toàn.

Chúng tôi thực hiện và kiểm tra các biến thể khác nhau của khung được đề xuất. Lưu ý rằng chúng tôi không thể tách biệt và xác minh hiệu quả của từng module khi tất cả các module được kết hợp với nhau, vì việc tích hợp các thành phần này có nghĩa là nhiều ràng buộc và có thể dẫn đến overfitting. Do đó, chúng tôi tập trung vào ba cấu hình của việc chưng cất bản đồ đặc trưng và chưng cất patch embedding được cung cấp trong Bảng I, mà chúng tôi gọi là TransKD-Base, TransKD-GL và TransKD-EA. Cấu trúc của ba biến thể này được cung cấp trong Hình 4.

BẢNG I: Cấu hình của các biến thể TransKD. FM: Bản đồ Đặc trưng. PE: Patch Embedding.

Khung | Chưng cất FM | Chưng cất PE | Plug&Play
---|---|---|---
TransKD-Base | CSF | PEA | ✓
TransKD-GL | CSF | PEA + GL-Mixer | ✓
TransKD-EA | CSF | PEA + EA | ✗

IV. CÁC BỘ DỮ LIỆU VÀ THIẾT LẬP THÍ NGHIỆM

A. Bộ dữ liệu và Metric

Cityscapes [30] là bộ dữ liệu quy mô lớn, chứa các chuỗi cảnh đường phố từ 50 thành phố khác nhau. Cityscapes bao gồm 2975 hình ảnh huấn luyện và 500 hình ảnh xác thực với chú thích dense cũng như 1525 hình ảnh test. Nó được biết đến với việc đánh giá hiệu suất của các thuật toán thị giác cho hiểu biết cảnh ngữ nghĩa đô thị. Có 19 lớp ngữ nghĩa.

ACDC [31] là bộ dữ liệu được sử dụng để giải quyết phân đoạn ngữ nghĩa trong các điều kiện bất lợi, ví dụ như sương mù, ban đêm, mưa và tuyết. Mỗi điều kiện bất lợi bao gồm 400 hình ảnh huấn luyện, 100 hình ảnh xác thực và 500 hình ảnh test, ngoại trừ điều kiện ban đêm bao gồm 106 hình ảnh xác thực. Các tình huống khó khăn và toàn diện hơn là đặc trưng của bộ dữ liệu ACDC, vì mục tiêu của nó là xác thực chất lượng phân đoạn ngữ nghĩa trong các điều kiện bất lợi với tập nhãn giống hệt 19 lớp như Cityscapes.

NYU Depth V2 [32] là bộ dữ liệu trong nhà được chụp bởi cả camera RGB và depth. Nó chứa các cảnh văn phòng, cửa hàng và phòng trong nhà. Có nhiều đối tượng bị che khuất với ánh sáng không đều. Bộ dữ liệu bao gồm 1449 hình ảnh RGB-depth, được chia thành 795 hình ảnh huấn luyện và 654 hình ảnh test. Các hình ảnh được chú thích với 40 danh mục ngữ nghĩa. Chúng tôi sử dụng hình ảnh RGB trong các thí nghiệm nghiên cứu chưng cất tri thức.

Pascal VOC2012 [33] là bộ dữ liệu cho các cuộc thi nhận dạng đối tượng. Các hình ảnh được chú thích với 20 danh mục đối tượng foreground và một lớp background. Chúng tôi áp dụng chiến lược augmentation trong [75]. Bộ dữ liệu quy mô lớn đã augmented chứa 10582/1449/1456 hình ảnh cho huấn luyện/xác thực/test.

Metric đánh giá. Mean Intersection over Union (mIoU) trên tất cả các lớp là metric đánh giá phổ biến cho các nhiệm vụ phân đoạn ngữ nghĩa. Tất cả các thí nghiệm được đánh giá dựa trên mIoU, được tính toán thông qua Phương trình (12):

mIoU = 1/(k+1) Σ_{i=0}^k p_{ii}/(Σ_{j=0}^k p_{ij} + Σ_{j=0}^k (p_{ji} - p_{ii})), (12)

trong đó k là số lượng lớp, p_{ij} là số lượng pixel thuộc lớp i và được phân loại là lớp j. Để đánh giá hiệu quả của TransKD, chúng tôi sử dụng MMCV để tính toán Floating point operations per second (FLOPs) và số lượng tham số của các mô hình. Ngoài ra, chúng tôi sử dụng mã nguồn để test Frames Per Second (FPS) của các mô hình trên GPU NVIDIA GeForce GTX 1080 Ti.

B. Chi tiết Triển khai

Các mô hình transformer giáo viên và học sinh. Đối với phần chính của các thí nghiệm phân đoạn ngữ nghĩa, chúng tôi nghiên cứu chưng cất tri thức để nén SegFormer B2 lớn đã được tiền huấn luyện thành mô hình SegFormer B0 nhỏ hơn mà không cần tiền huấn luyện (trừ khi được chỉ định), có số lượng tham số tương tự như ResNets được sử dụng trong các công việc chưng cất trước đây [4], [1], [2]. SegFormer B0 [26] đạt tốc độ 35.06 Frames Per Second (FPS) khi chạy ở độ phân giải 512×1024 trên NVIDIA GTX 1080Ti và do đó rất phù hợp cho các ứng dụng thời gian thực. Chúng tôi cũng đã kiểm tra khung của chúng tôi với PVTv2 [25] và LVT [34] để đánh giá tính tổng quát của phương pháp.

Thiết lập huấn luyện và siêu tham số. Khi đánh giá hiệu suất của các khung chưng cất tri thức, chúng tôi thay đổi kích thước hình ảnh thành 512×1024 cho Cityscapes [30], 512×910 cho ACDC [31] và 512×512 cho Pascal VOC2012 [33], trong khi duy trì tỷ lệ hình ảnh gốc 480×640 cho bộ dữ liệu NYUv2 [32]. Chúng tôi sử dụng một GPU NVIDIA GTX 1080Ti khi thí nghiệm với các kích thước đầu vào đã đề cập và bốn GPU NVIDIA A100 cho các kích thước đầu vào lớn hơn. Số epoch được đặt là 1000 trên các bộ dữ liệu Cityscapes, ACDC và Pascal VOC2012, và 1500 trên bộ dữ liệu NYUv2. Theo cấu trúc của SegFormer [26], chiều cao và chiều rộng của đầu ra và bản đồ phân đoạn mục tiêu đều được rescale với tỉ lệ 1/4 so với hình ảnh đầu vào. Hiệu suất của mô hình SegFormer baseline có sự suy giảm nhẹ, điều này hợp lý do kích thước nhỏ hơn của hình ảnh đầu vào. Các mô hình trong công việc của chúng tôi được huấn luyện bằng optimizer AdamW [76] với learning rate 6×10^-5, và epsilon mặc định 1×10^-8 và betas (0.9, 0.999). Learning rate được điều chỉnh bằng polynomial learning rate decay scheduler [77] với hệ số mặc định 1.0. End learning rate được đặt là 0.0 và số bước decay tối đa được đặt ở 1500. Chúng tôi áp dụng batch size 8 mỗi GPU trên VOC2012 và 2 trên các bộ dữ liệu khác. Ngoài ra, số lượng kênh của bản đồ đặc trưng trong module hợp nhất CSF C được đặt là 64.

V. KẾT QUẢ VÀ PHÂN TÍCH THÍ NGHIỆM

A. Kết quả định lượng

Kết quả trên bộ dữ liệu Cityscapes. Trong Bảng II, chúng tôi so sánh hiệu suất của phương pháp TransKD mà không tiền huấn luyện mô hình học sinh trên ImageNet, với nhiều phương pháp chưng cất tri thức đã công bố trước đây, trên tập xác thực Cityscapes. Chúng tôi đầu tiên xác thực tính hữu ích của khung chúng tôi so với baseline được huấn luyện mà không có chưng cất tri thức và thấy rằng mô hình học sinh SegFormer-B0 được tối ưu hóa với khung chưng cất TransKD-EA của chúng tôi đạt được cải thiện +13.12% (55.86% vs. 68.98%) trong mIoU.

TransKD-EA cũng vượt trội hơn các khung chưng cất khác với khoảng cách đáng kể, ví dụ +5.58% so với phương pháp chưng cất chỉ sử dụng bản đồ đặc trưng Knowledge Review (KR) [2], tiết lộ lợi ích của việc thống nhất chưng cất patch embedding và bản đồ đặc trưng. Hơn nữa, mô hình học sinh được chưng cất bằng TransKD-EA mang lại kết quả có thể so sánh với mô hình học sinh có tiền huấn luyện dài bổ sung. Mặc dù TransKD của chúng tôi có vẻ phức tạp, chúng không nặng như vậy so với SKD [4] và đạt được chưng cất tri thức plug-and-play. Tuy nhiên, biến thể nhẹ của khung chúng tôi TransKD-Base đủ để thực hiện chưng cất, mang lại mức tăng đáng ngạc nhiên +5.18% so với KR [2] trong khi chỉ thêm 0.21M tham số cho chưng cất patch embedding.

Để khám phá hai nguồn tri thức, bản đồ đặc trưng và patch embeddings, một cách bình đẳng, chúng tôi thực hiện Knowledge Review trên hai vị trí tương ứng. Kết quả cho thấy việc chưng cất tri thức từ patch embeddings có mức tăng khá tốt so với việc chưng cất từ bản đồ đặc trưng (63.40% vs. 62.41%). Tri thức từ patch embeddings có thể bổ sung nhưng không thể thay thế những tri thức từ bản đồ đặc trưng. Bên cạnh đó, hóa ra cơ chế review có thể không hoàn toàn phù hợp cho thông tin vị trí.

So sánh với các phương pháp chưng cất trước đây. So với chưng cất tri thức truyền thống dựa trên response [1], [17], dựa trên feature [4], [1] và dựa trên relation [2], việc kết hợp chưng cất truyền thống và chưng cất patch embedding cải thiện đáng kể hiệu quả của việc chuyển giao transformer-transformer. Bên cạnh đó, so với mô hình giáo viên dựa trên SegFormer-B2, mô hình học sinh với backbone SegFormer-B0 giảm >85% GFLOPs. Hơn nữa, TransKD-EA sánh ngang với hiệu suất của phương pháp tiền huấn luyện tốn thời gian nổi tiếng. Lưu ý rằng số lượng tham số và GFLOPs được liệt kê trong Bảng II bao gồm của bản thân mô hình và của khung chưng cất.

Tuy nhiên, tại thời điểm test, độ phức tạp tính toán của mô hình học sinh được tăng cường không bị ảnh hưởng bởi phương pháp chưng cất và giống hệt với mô hình học sinh nhỏ gọn gốc (3.72M tham số và 13.67 GFLOPs), do đó duy trì hiệu quả cao cần thiết trong các ứng dụng thực tế.

Phân tích độ chính xác theo lớp trên Cityscapes. Tiếp theo, chúng tôi điều tra hiệu suất của khung chúng tôi cho các danh mục riêng lẻ trong Bảng III, so sánh các biến thể TransKD với baseline chưng cất chỉ sử dụng bản đồ đặc trưng Knowledge Review (KR) [2]. TransKD-GL tốt hơn trong việc nhận dạng các cơ sở hạ tầng nhỏ, như đèn giao thông, biển báo giao thông và cột đèn, trong khi TransKD-EA có hiệu suất tốt hơn trên các danh mục như tường và tàu. Cả hai biến thể đều vượt trội khung KR cho tất cả các lớp với khoảng cách đáng chú ý, đặc biệt là cho tường, cột đèn, người, người lái xe, xe tải và tàu. Trong Hình 8(a), chúng tôi tiếp tục hiển thị độ chính xác theo lớp của mô hình học sinh gốc, khung KR và các phương pháp TransKD của chúng tôi. Lợi ích của TransKD đặc biệt lớn đối với các danh mục xe cộ ít thường xuyên nhưng khó khăn như xe buýt, tàu và xe tải.

Hiệu quả cho các kiến trúc khác nhau. Sau đó chúng tôi xác minh liệu hiệu quả của phương pháp chúng tôi có nhất quán cho các kiến trúc khác nhau hay không. Với mục đích này, chúng tôi so sánh phương pháp TransKD-Base với khung Knowledge Review (KR) [2] cho các backbone phân đoạn khác nhau trên bộ dữ liệu Cityscapes. Kết quả trong Bảng IV chỉ ra rằng phương pháp của chúng tôi không gắn liền chặt chẽ với một mô hình cụ thể, mà có lợi ích nhất quán cho các backbone phân đoạn khác nhau. Cụ thể, phương pháp TransKD-Base của chúng tôi đạt được cải thiện +5.18%, +2.18% và +2.71% so với phương pháp KR chỉ sử dụng bản đồ đặc trưng, khi sử dụng mô hình SegFormer-B0, PVTv2-B0 [25] và Lite Vision Transformer (LVT) [34] làm học sinh tương ứng. Đối với PVTv2-B0, chúng tôi sử dụng PVTv2-B2 [25] làm mô hình giáo viên. Trong thiết lập sử dụng SegFormer-B2 làm giáo viên và LVT làm học sinh (có cấu trúc khá khác biệt), khung TransKD được đề xuất của chúng tôi cũng rõ ràng cải thiện chất lượng phân đoạn. Khi so sánh với segmenter học sinh gốc mà không có bất kỳ chưng cất nào, các mức tăng rõ ràng hơn với 12.72%, 10.34% và 11.05% trong mIoU. Những kết quả này cho thấy khung TransKD của chúng tôi có thể giải phóng tiềm năng của các mô hình transformer hiệu quả, mang lại cải thiện đủ của các mô hình nhỏ gọn.

Hơn nữa, chúng tôi đánh giá hiệu quả của các kiến trúc transformer khác nhau trong cả giai đoạn huấn luyện và suy luận. Trong quá trình huấn luyện, độ phức tạp của các transformers giáo viên và học sinh được nhúng trong các khung chưng cất tri thức được đo bằng số lượng tham số và GFLOPs. Ngược lại, trong giai đoạn suy luận, tốc độ khung hình mỗi giây (FPS) được đánh giá bằng các transformers thuần túy không được trang bị khung chưng cất tri thức trên GPU giống hệt, vì các khung chủ yếu được sử dụng để chuyển giao tri thức chỉ trong quá trình huấn luyện. Các transformers học sinh được sử dụng trong phân tích này có kích thước nhỏ hơn đáng kể, từ 7.35 đến 3.86 lần nhỏ hơn so với các mô hình giáo viên. Hơn nữa, việc kết hợp khung chưng cất chỉ gây ra mức tăng tối thiểu về số lượng tham số (khoảng 0.84 triệu tham số) và chi phí tính toán (thêm 2.97 GFLOPs), bất kể loại mô hình. Đáng chú ý, tốc độ suy luận của các mô hình học sinh được tăng cường bởi hệ số 2 đến 3.5 lần so với các mô hình giáo viên. Nhìn chung, các transformers học sinh cung cấp suy luận nhanh hơn với chi phí bổ sung tối thiểu và kích thước giảm.

Kết quả trên bộ dữ liệu ACDC. Bảng V tóm tắt kết quả chưng cất của CD [1], KR [2] và các phương pháp TransKD của chúng tôi trên bộ dữ liệu xác thực ACDC. Ngoài điểm mIoU tổng thể, chúng tôi kiểm tra các mô hình trên bốn điều kiện bất lợi khác nhau được ghi lại trong bộ dữ liệu ACDC, cụ thể là sương mù, ban đêm, mưa và tuyết. Phương pháp TransKD-EA của chúng tôi đạt được hiệu suất tốt nhất với 59.09% mIoU trên benchmark All-ACDC, và điểm số hàng đầu 66.13%, 44.99% và 57.24% mIoU cho các tập con sương mù, ban đêm và mưa tương ứng. Thú vị, một biến thể khác của khung chúng tôi, TransKD-GL, mang lại kết quả tốt nhất 60.61% trên tập con tuyết của bộ dữ liệu ACDC. Chúng tôi đưa ra giả thuyết rằng đối với các cảnh tuyết, các gợi ý cục bộ rất quan trọng cho việc phân đoạn và TransKD-GL xuất sắc, do việc trích xuất thông tin chi tiết từ embedding cao cấp đại diện. So với Knowledge Review, cả ba biến thể TransKD của chúng tôi đều có mức tăng hiệu suất tương ứng +3.66%, +3.23% và +4.19%. Những cải thiện trong các tập con thời tiết đa dạng và ban đêm này xác nhận hiệu quả của TransKD cho các diện mạo cảnh khác nhau.

Phân tích độ chính xác theo lớp trên ACDC. Chúng tôi cũng cung cấp kết quả phân đoạn theo lớp của các khung khác nhau trên bộ dữ liệu ACDC trong Hình 8(b). So với học sinh gốc và học sinh được tăng cường KR, các mô hình được tối ưu hóa với phương pháp TransKD của chúng tôi có mức tăng rõ ràng trong hầu hết các lớp. Đặc biệt, TransKD có những cải thiện đáng kể trên một số danh mục chiếm ít không gian (và do đó khó phân đoạn hơn) nhưng rất quan trọng cho lái xe tự động, như cột đèn, đèn giao thông và biển báo giao thông.

Kết quả trên bộ dữ liệu NYUv2. Lĩnh vực điều tra tiếp theo của chúng tôi là kết quả phân đoạn cho các cảnh trong nhà được bao phủ bởi bộ dữ liệu NYUv2 [32]. Mô hình học sinh gốc với SegFormer-B0 đạt hiệu suất tương đối thấp 18.19% mIoU trên NYUv2. Trong khi các khung CD và KR chỉ mang lại cải thiện hạn chế, phương pháp TransKD-GL của chúng tôi đạt điểm mIoU tốt nhất 24.20%. So với KR, các khung chưng cất TransKD-Base, -GL và -EA của chúng tôi đạt mức tăng mIoU +0.81%, +1.40% và +1.09% tương ứng. Điều này chỉ ra rằng các phương pháp TransKD của chúng tôi cũng có lợi cho hiểu biết cảnh trong nhà.

BẢNG VI: Phân tích độ chính xác trên bộ dữ liệu NYUv2 [32].

Mạng | #Params (M) | GFLOPs | mIoU (%)
---|---|---|---
Giáo viên (B2) | 27.38 | 66.73 | 44.11
Học sinh (B0) | 3.72 | 13.85 | 18.19
+CD [1] | 3.72 | 13.85 | 20.83
+Knowledge Review [2] | 4.35 | 16.35 | 22.80
TransKD-Base (của chúng tôi) | 4.57 | 16.64 | 23.61 (+0.81)
TransKD-GL (của chúng tôi) | 5.22 | 16.98 | 24.20 (+1.40)
TransKD-EA (của chúng tôi) | 5.54 | 18.02 | 23.89 (+1.09)

Kết quả trên bộ dữ liệu Pascal VOC2012. Vì cả Pascal VOC2012 và ImageNet đều là các bộ dữ liệu tập trung vào đối tượng với bố cục tương tự, việc huấn luyện với TransKD của chúng tôi được giả định là không thể có được mô hình chung từ quá trình tiền huấn luyện trên ImageNet. Tuy nhiên, chưng cất tri thức thông qua TransKD của chúng tôi bù đắp thành công 81.1% khoảng cách giữa các mô hình không tiền huấn luyện và có tiền huấn luyện, có nghĩa là TransKD-EA của chúng tôi đạt được mức tăng +19.3% so với mô hình học sinh. Cả ba biến thể, TransKD-Base, TransKD-GL và TransKD-EA đều đạt được những cải thiện nổi bật +4.17%, +4.49% và +4.68% mIoU so với giải pháp baseline Knowledge Review [2].

B. Nghiên cứu Ablation và So sánh

Hiệu quả của module hợp nhất bản đồ đặc trưng và hàm loss. Thành phần chưng cất bản đồ đặc trưng của TransKD (Sec. III-C) bao gồm các module hợp nhất bản đồ đặc trưng và các hàm loss. Để chưng cất bản đồ đặc trưng mạnh mẽ, chúng tôi đã giới thiệu Cross Selective Fusion (CSF), để thay thế module hợp nhất đặc trưng gốc ABF trong Knowledge Review [2]. CSF dựa trên cơ chế channel attention cho việc chưng cất bản đồ đặc trưng transformer. Để xác minh hiệu quả của CSF, chúng tôi so sánh CSF và ABF khi sử dụng hàm loss HCL trên Cityscapes [30] và ACDC [31] trong Bảng VIII. Rõ ràng rằng CSF vượt trội hơn ABF rõ ràng với khoảng cách độ chính xác 2.54% và 0.45% trên hai bộ dữ liệu. Phát hiện này cho thấy tính hữu ích của CSF và tầm quan trọng của các phụ thuộc channel-wise cho việc chưng cất tri thức dựa trên transformer.

Chúng tôi tiếp tục nhằm mục đích tách biệt vai trò của hàm loss bản đồ đặc trưng trong một tập hợp các thí nghiệm riêng biệt, với kết quả được tóm tắt trong Bảng IX. Loss phân kỳ Kullback-Leibler (KL) được sử dụng rộng rãi trong chưng cất tri thức và cho phép học sinh bắt chước các phân phối của giáo viên, tức là dự đoán mô hình [1], [4], [17] hoặc bản đồ đặc trưng [1]. Do đó, chúng tôi chọn các hàm loss phân kỳ KL channel-wise và spatial làm nhóm đối chứng để xác nhận hiệu quả của HCL (phiên bản phân cấp của loss MSE). Kết quả chứng minh rằng HCL vượt trội hơn cả hai loss phân kỳ KL hơn 1.5% khi chưng cất các bản đồ đặc trưng trung gian.

Hiệu quả của chưng cất patch embedding. Trong bài báo này, chưng cất patch embedding (Sec. III-B) là lần đầu tiên được giới thiệu trong lĩnh vực phân đoạn ngữ nghĩa. Bảng X tóm tắt các thí nghiệm của chúng tôi nhắm đến hiệu ứng của việc chưng cất patch embedding cho các giai đoạn khác nhau. Khi chưng cất một giai đoạn patch embedding, patch embedding loss được thêm trực tiếp vào feature map loss, tức là α_m = 1. Chưng cất patch embedding tại bất kỳ giai đoạn nào đều hiệu quả, vì nó rõ ràng thúc đẩy hiệu suất so với các đối tác không có trao đổi tri thức ở cấp độ patch embedding. Nói chung, giai đoạn càng sâu, càng nhiều kênh của patch embedding được sử dụng và hiệu ứng chưng cất mà khung có thể cung cấp càng tốt. Lưu ý rằng Knowledge Review chỉ sử dụng bản đồ đặc trưng gốc không sử dụng bất kỳ chưng cất patch embedding nào và đạt kết quả thấp hơn nhiều là 63.40%. Các mô hình mang lại hiệu suất tốt nhất với việc chưng cất patch embedding tất cả giai đoạn, ví dụ 67.89% mIoU cho Knowledge Review (được trang bị các module PEA của chúng tôi) và 68.58% cho TransKD.

Ngoài ra, ở đây chúng tôi sử dụng CSF+HCL để đại diện cho phần chưng cất bản đồ đặc trưng của chúng tôi trong TransKD-Base và so sánh với Knowledge Review kết hợp với PEA. CSF+HCL của chúng tôi cải thiện kết quả 1.2%∼2.5% so với Knowledge Review với các cấu hình PEA khác nhau (cả khi không được kết hợp hoặc kết hợp với PEA tại các giai đoạn khác nhau), xác minh lợi ích của module hợp nhất bản đồ đặc trưng CSF được đề xuất. Khi các khung chưng cất bản đồ đặc trưng tận dụng PEA tại tất cả các giai đoạn với trọng số α = [0.1, 0.1, 0.5, 1], CSF+HCL đạt mIoU tốt nhất 68.58% vẫn vượt trội biến thể Knowledge Review được tăng cường với các khối chưng cất patch embedding của chúng tôi 0.69%.

Hơn nữa, chúng tôi tóm tắt hiệu suất của TransKD trên các bộ dữ liệu khác nhau trong Bảng XI để phân tích hiệu quả của các module tối ưu hóa, GL-Mixer và EA. Chúng tôi quan sát thấy các module tối ưu hóa liên tục cải thiện hiệu suất của phiên bản cơ bản của TransKD hơn +0.5%. Tuy nhiên, trong khi TransKD được tăng cường bởi GL-Mixer có hiệu suất kém hơn phiên bản cơ bản trên bộ dữ liệu ACDC, TransKD-GL đạt kết quả tiên tiến trong điều kiện tuyết, như được chứng minh trong Bảng V.

Ảnh hưởng của kích thước đầu vào và trọng số tiền huấn luyện. Theo công việc gốc [26], việc sử dụng hình ảnh đầu vào lớn cải thiện đáng kể kết quả phân đoạn của SegFormer. Bên cạnh đó, tiền huấn luyện ImageNet là cách phổ biến để các phương pháp dựa trên transformer bù đắp cho việc thiếu inductive bias và tăng tốc hội tụ. Trong Bảng XII, chúng tôi kiểm tra ảnh hưởng của kích thước đầu vào và trọng số tiền huấn luyện bằng cách so sánh kết quả của Knowledge Review [2] và TransKD-Base cho các cấu hình khác nhau.

Chúng tôi đầu tiên xem xét các mạng mà không có khởi tạo trọng số thông qua tiền huấn luyện (tức là không có "+ImN" trong Bảng XII). Trong trường hợp này, tỷ lệ hình ảnh lớn tạo điều kiện cho hiệu quả của mô hình phân đoạn ngữ nghĩa tổng thể trong khi ảnh hưởng đến mức tăng của TransKD so với Knowledge Review và học sinh. Ở kích thước hình ảnh 512×1024, học sinh được chưng cất TransKD nâng cao độ chính xác 12.72% so với học sinh không tiền huấn luyện gốc. Với kích thước đầy đủ 1024×2048, TransKD-Base của chúng tôi đạt hiệu suất 71.59% với mức tăng độ chính xác ít hơn nhưng vẫn đáng kể 9.73% mIoU.

Như mong đợi, kích thước đầu vào lớn hơn dẫn đến kết quả tốt hơn cho cả mạng có tiền huấn luyện và không tiền huấn luyện. Chúng tôi tin rằng, vì tiền huấn luyện trên bộ dữ liệu lớn được cho là bổ sung inductive bias, và việc chưng cất patch embedding được đề xuất có cùng mục đích, việc chưng cất patch embedding có thể bù đắp đầy đủ khoảng cách giữa các transformers không tiền huấn luyện và có tiền huấn luyện, nhưng không cho thấy cải thiện nổi bật so với transformer có tiền huấn luyện. Do đó, TransKD-Base vượt trội Knowledge Review với khoảng cách nhỏ hơn khi các học sinh được tiền huấn luyện. Không ngạc nhiên, chất lượng phân đoạn tốt nhất đạt được khi fine-tuning học sinh được tiền huấn luyện trên ImageNet với phương pháp chưng cất tri thức TransKD-Base của chúng tôi, mang lại 75.74% mIoU.

So sánh với các segmenter tiên tiến. Trong Bảng XIII, chúng tôi tiếp tục so sánh TransKD-Base với các mạng phân đoạn ngữ nghĩa tiên tiến và các phương pháp chưng cất tri thức trước đây. Trong khi các mô hình nặng hơn như CCNet [79] và SETR [11] đạt điểm mIoU cao, chúng tốn nhiều tài nguyên tính toán, điều này thường loại bỏ việc triển khai của chúng trong các hệ thống di động và thời gian thực. Các segmenter chính xác như HANet [80] và SegFormer-B5 [26] đòi hỏi >1000 GFLOPs để mang lại dự đoán độ phân giải cao chính xác. Chúng tôi tiếp tục xem xét các segmenter ngữ nghĩa hiệu quả hơn. Các mô hình nhỏ gọn, như CGNet [84] và Fast-SCNN [85], rất nhẹ nhưng đi kèm với chi phí độ chính xác cao, và chất lượng phân đoạn giảm. Một số mô hình có độ phức tạp tính toán vừa phải như ERFNet [5] và BiSeNet [45] có thể đạt độ chính xác cao hơn nhưng cũng bao gồm nhiều tham số mô hình hơn.

Các thí nghiệm trước đây của chúng tôi trong Sec. V-A đã kiểm tra các phương pháp chưng cất tri thức khác nhau với cùng backbone học sinh. Trong phần này, chúng tôi thực hiện nhiều so sánh hơn với các phương pháp đã công bố trước đây với các con số từ các bài báo tương ứng của họ cho học sinh được tăng cường sau chưng cất (Bảng XIII). Hầu hết các công việc trước đây đã sử dụng PSPNet18 [27] làm mô hình học sinh. Trong nhóm các phương pháp chưng cất và học sinh không tiền huấn luyện, TransKD-Base của chúng tôi mang lại hiệu suất hàng đầu 71.59% mIoU, thậm chí gần với kết quả của một số học sinh có tiền huấn luyện, ví dụ MD [87] (71.90%). Trong nhóm các phương pháp chưng cất và học sinh có tiền huấn luyện, TransKD đạt điểm mIoU cao nhất mặc dù là mô hình nhẹ nhất chỉ với 3.72M tham số và 54.70 GFLOPs. Hình 9 trực quan hóa việc so sánh các mô hình phân đoạn hiệu quả đại diện và các phương pháp chưng cất tri thức. Chúng ta rõ ràng thấy rằng TransKD nổi bật ở phía trước của các mô hình tiên tiến trong quá khứ trong cả hai nhóm và tạo ra sự đánh đổi xuất sắc giữa hiệu quả và độ chính xác cho hiểu biết cảnh ngữ nghĩa.

C. Kết quả Định tính

Phân tích trực quan hóa t-SNE. Tiếp theo chúng tôi xem xét góc độ đại diện và đánh giá khả năng phân biệt của các embeddings đã học trong không gian tiềm ẩn thông qua trực quan hóa t-SNE [88] được đưa ra trong Hình 10, trong đó Hình 10(a) minh họa các embeddings học sinh gốc, và Hình 10(b) và Hình 10(c) là các đại diện học sinh đã học với KR [2] và TransKD-Base của chúng tôi tương ứng.

Mà không có bất kỳ chưng cất tri thức nào, ranh giới quyết định cho hầu hết các danh mục đều mơ hồ và khó phân biệt, dẫn đến việc ra quyết định khó khăn hơn cho bộ phân loại. Huấn luyện với Knowledge Review giúp phân biệt hầu hết các danh mục, nhưng biển báo giao thông và đèn giao thông, rất quan trọng cho xe tự động, dễ bị nhầm lẫn. Cuối cùng, những vấn đề này được giải quyết tốt hơn bằng cách sử dụng chưng cất tri thức TransKD của chúng tôi. So với Hình 10(a) và Hình 10(b), chúng ta quan sát thấy ranh giới inter-category rõ ràng hơn trong Hình 10(c), điều này chỉ ra lợi ích của các phương pháp được đề xuất thống nhất chưng cất patch embedding và feature khi nó tăng cường khả năng phân biệt trong không gian tiềm ẩn.

Phân tích hiệu ứng phân đoạn cảnh. Kết quả định tính của phân đoạn ngữ nghĩa trong các kịch bản đường phố dưới các loại điều kiện thời tiết khác nhau được mô tả trong Hình 11, trong đó thời tiết bình thường, sương mù, ban đêm, mưa và tuyết được xem xét từ trái sang phải trong mỗi hàng. Các mẫu đến từ Cityscapes (thời tiết bình thường) và ACDC (thời tiết bất lợi) tương ứng. Chúng tôi so sánh TransKD với Knowledge Review (KR) chỉ sử dụng bản đồ đặc trưng [2] và Channel-wise Distillation (CD) [1]. Các phần khó khăn của mỗi ví dụ được làm nổi bật với các hình chữ nhật nét đứt màu vàng. Đối với ví dụ có thời tiết bình thường nhưng cảnh phức tạp (xe buýt bị che khuất bởi biển báo giao thông), một số lượng lớn dự đoán sai trên các đối tượng nhỏ xuất hiện trong kết quả của học sinh mà không có chưng cất, trong khi các bản đồ phân đoạn không thỏa mãn tương tự cũng có thể được tìm thấy trong kết quả của CD và KR, ví dụ một phần xe buýt bị phân loại sai thành tòa nhà. May mắn thay, những vấn đề này được giải quyết tốt hơn bởi TransKD, dẫn đến phân đoạn hoàn chỉnh và rõ ràng hơn nhiều và chứng minh hiệu quả của nó trong việc xử lý chi tiết.

Những phát hiện này cũng được xác nhận trong các thí nghiệm của chúng tôi có các điều kiện thời tiết khác nhau của benchmark ACDC. Trong các cảnh sương mù, ban đêm và mưa, TransKD cho phép phát hiện tốt hơn các đối tượng nhỏ khó khăn như người đi bộ quan trọng cho an toàn, cũng như đèn giao thông và biển báo giao thông. Trong cảnh tuyết, TransKD cũng có thể phân tích thực vật mặc dù lớp này có kết cấu tương tự như tòa nhà nền, nhờ vào việc chưng cất embedding chuyển giao các phụ thuộc tầm xa. Điều này một lần nữa làm nổi bật tiềm năng của việc liên kết chưng cất patch embedding và bản đồ đặc trưng thông qua TransKD của chúng tôi, tạo ra tri thức thông tin hơn so với phương pháp chưng cất chỉ ở cấp độ đặc trưng KR.

Hạn chế và công việc tương lai. Mặc dù TransKD cho phép các mô hình học sinh không tiền huấn luyện hoạt động tương đương với các đối tác có tiền huấn luyện của chúng, vẫn còn một khoảng cách hiệu suất nhỏ (0.77% trên Cityscapes) giữa chúng. Trong tương lai, chúng tôi dự đoán rằng các nguồn tri thức, như patch embeddings và bản đồ đặc trưng, đã chứng minh hiệu quả trong công việc của chúng tôi, có thể được tận dụng tốt hơn để đóng khoảng cách hiệu suất này thông qua các thiết kế khung chưng cất tri thức tinh tế hơn. Hơn nữa, TransKD đã được thực hiện độc quyền trên các transformers pyramid, chủ yếu được sử dụng trong phân đoạn ngữ nghĩa. Chúng tôi tin rằng khung TransKD của chúng tôi cũng có thể được điều chỉnh cho các vision transformers isotropic.

VI. KẾT LUẬN

Trong công việc này, trọng tâm của chúng tôi nằm ở việc tăng cường hiệu quả phân đoạn ngữ nghĩa trong bối cảnh lái xe tự động thông qua chưng cất tri thức dựa trên transformer. Chúng tôi giới thiệu kiến trúc TransKD, một khung chưng cất tri thức transformer-to-transformer tiên phong tận dụng patch embeddings đặc trưng của transformer như một nguồn tri thức quan trọng. TransKD học các segmenter nhỏ gọn hiệu quả thông qua chưng cất bản đồ đặc trưng và chưng cất patch embedding, với hai module cơ bản, tức là CSF và PEA, và hai kỹ thuật tối ưu hóa, tức là GL-Mixer và EA. Để minh họa hiệu suất của phương pháp TransKD được đề xuất, các đánh giá chi tiết được thực hiện trên các bộ dữ liệu Cityscapes, ACDC, NYUv2 và Pascal VOC2012 với TransKD mang lại hiệu suất tiên tiến so với các phương pháp chưng cất tri thức hiện có. Các thí nghiệm mang lại bằng chứng thuyết phục rằng các module cấp độ patch embedding đặc trưng của transformer được đề xuất hoạt động như những nguồn mạnh mẽ cho việc chưng cất tri thức trong phân đoạn ngữ nghĩa, cho thấy tiềm năng cao như nền tảng cho các mô hình tiết kiệm tài nguyên nhưng chính xác trong lĩnh vực hiểu biết cảnh tự động.

TÀI LIỆU THAM KHẢO
[1] C. Shu, Y. Liu, J. Gao, Z. Yan, và C. Shen, "Channel-wise knowledge distillation for dense prediction," trong ICCV, 2021.
[2] P. Chen, S. Liu, H. Zhao, và J. Jia, "Distilling knowledge via knowledge review," trong CVPR, 2021.
[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei, "ImageNet: A large-scale hierarchical image database," trong CVPR, 2009.
[4] Y. Liu, K. Chen, C. Liu, Z. Qin, Z. Luo, và J. Wang, "Structured knowledge distillation for semantic segmentation," trong CVPR, 2019.
[5] E. Romera, J. M. Alvarez, L. M. Bergasa, và R. Arroyo, "ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation," T-ITS, vol. 19, no. 1, pp. 263–272, 2018.
[6] J. Zhang, H. Liu, K. Yang, X. Hu, R. Liu, và R. Stiefelhagen, "CMX: cross-modal fusion for RGB-X semantic segmentation with transformers," T-ITS, vol. 24, no. 12, pp. 14 679–14 694, 2023.
[7] Y. Zheng, F. Zhou, S. Liang, W. Song, và X. Bai, "Semantic segmentation in thermal videos: A new benchmark and multi-granularity contrastive learning-based framework," T-ITS, 2023.
[8] K. Muhammad et al., "Vision-based semantic segmentation in scene understanding for autonomous driving: Recent achievements, challenges, and outlooks," T-ITS, vol. 23, no. 12, pp. 22 694–22 715, 2022.
[9] J. Zhang, K. Yang, A. Constantinescu, K. Peng, K. Müller, và R. Stiefelhagen, "Trans4Trans: Efficient transformer for transparent object and semantic scene segmentation in real-world navigation assistance," T-ITS, vol. 23, no. 10, pp. 19 173–19 186, 2022.
[10] J. Zhang, K. Yang, và R. Stiefelhagen, "Exploring event-driven dynamic context for accident scene segmentation," T-ITS, vol. 23, no. 3, pp. 2606–2622, 2022.
[11] S. Zheng et al., "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers," trong CVPR, 2021.
[12] R. Strudel, R. Garcia, I. Laptev, và C. Schmid, "Segmenter: Transformer for semantic segmentation," trong ICCV, 2021.
[13] A. Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale," trong ICLR, 2021.
[14] A. Vaswani et al., "Attention is all you need," trong NeurIPS, 2017.
[15] S. Mehta, M. Rastegari, A. Caspi, L. Shapiro, và H. Hajishirzi, "ESPNet: Efficient spatial pyramid of dilated convolutions for semantic segmentation," trong ECCV, 2018.
[16] J. Gou, B. Yu, S. J. Maybank, và D. Tao, "Knowledge distillation: A survey," IJCV, vol. 129, no. 6, pp. 1789–1819, 2021.
[17] G. Hinton, O. Vinyals, và J. Dean, "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, 2015.
[18] Y. Wang, W. Zhou, T. Jiang, X. Bai, và Y. Xu, "Intra-class feature variation distillation for semantic segmentation," trong ECCV, 2020.
[19] D. Ji, H. Wang, M. Tao, J. Huang, X.-S. Hua, và H. Lu, "Structural and statistical texture knowledge distillation for semantic segmentation," trong CVPR, 2022.
[20] Y. Feng, X. Sun, W. Diao, J. Li, và X. Gao, "Double similarity distillation for semantic image segmentation," TIP, vol. 30, pp. 5363–5376, 2021.
[21] S. An, Q. Liao, Z. Lu, và J.-H. Xue, "Efficient semantic segmentation via self-attention and self-distillation," T-ITS, vol. 23, no. 9, pp. 15 256–15 266, 2022.
[22] X. Chu et al., "Twins: Revisiting the design of spatial attention in vision transformers," trong NeurIPS, 2021.
[23] K. Li et al., "UniFormer: Unifying convolution and self-attention for visual recognition," TPAMI, vol. 45, no. 10, pp. 12 581–12 600, 2023.
[24] Z. Liu et al., "Swin transformer: Hierarchical vision transformer using shifted windows," trong ICCV, 2021.
[25] W. Wang et al., "PVT v2: Improved baselines with pyramid vision transformer," CVM, vol. 8, no. 3, pp. 415–424, 2022.
[26] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, và P. Luo, "SegFormer: Simple and efficient design for semantic segmentation with transformers," trong NeurIPS, 2021.
[27] H. Zhao, J. Shi, X. Qi, X. Wang, và J. Jia, "Pyramid scene parsing network," trong CVPR, 2017.
[28] S. I. Mirzadeh, M. Farajtabar, A. Li, N. Levine, A. Matsukawa, và H. Ghasemzadeh, "Improved knowledge distillation via teacher assistant," trong AAAI, 2020.
[29] T. Huang, S. You, F. Wang, C. Qian, và C. Xu, "Knowledge distillation from a stronger teacher," trong NeurIPS, 2022.
[30] M. Cordts et al., "The cityscapes dataset for semantic urban scene understanding," trong CVPR, 2016.
[31] C. Sakaridis, D. Dai, và L. Van Gool, "ACDC: The adverse conditions dataset with correspondences for semantic driving scene understanding," trong ICCV, 2021.
[32] N. Silberman, D. Hoiem, P. Kohli, và R. Fergus, "Indoor segmentation and support inference from RGBD images," trong ECCV, 2012.
[33] M. Everingham, L. V. Gool, C. K. I. Williams, J. M. Winn, và A. Zisserman, "The pascal visual object classes (VOC) challenge," IJCV, vol. 88, pp. 303–338, 2010.
[34] C. Yang et al., "Lite vision transformer with enhanced self-attention," trong CVPR, 2022.
[35] J. Long, E. Shelhamer, và T. Darrell, "Fully convolutional networks for semantic segmentation," trong CVPR, 2015.
[36] V. Badrinarayanan, A. Kendall, và R. Cipolla, "SegNet: A deep convolutional encoder-decoder architecture for image segmentation," TPAMI, vol. 39, no. 12, pp. 2481–2495, 2017.
[37] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, và H. Adam, "Encoder-decoder with atrous separable convolution for semantic image segmentation," trong ECCV, 2018.
[38] X. Wang, R. Girshick, A. Gupta, và K. He, "Non-local neural networks," trong CVPR, 2018.
[39] J. Fu et al., "Dual attention network for scene segmentation," trong CVPR, 2019.
[40] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, và J. Wang, "OCNet: Object context for semantic segmentation," IJCV, vol. 129, no. 8, pp. 2375–2398, 2021.
[41] W. Yu et al., "MetaFormer is actually what you need for vision," trong CVPR, 2022.
[42] I. O. Tolstikhin et al., "MLP-mixer: An all-MLP architecture for vision," trong NeurIPS, 2021.
[43] A. Paszke, A. Chaurasia, S. Kim, và E. Culurciello, "ENet: A deep neural network architecture for real-time semantic segmentation," arXiv preprint arXiv:1606.02147, 2016.
[44] H. Zhao, X. Qi, X. Shen, J. Shi, và J. Jia, "ICNet for real-time semantic segmentation on high-resolution images," trong ECCV, 2018.
[45] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, và N. Sang, "BiSeNet: Bilateral segmentation network for real-time semantic segmentation," trong ECCV, 2018.
[46] M. Orsic, I. Kreso, P. Bevandic, và S. Segvic, "In defense of pre-trained ImageNet architectures for real-time semantic segmentation of road-driving images," trong CVPR, 2019.
[47] H. Li, P. Xiong, H. Fan, và J. Sun, "DFANet: Deep feature aggregation for real-time semantic segmentation," trong CVPR, 2019.
[48] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, và L.-C. Chen, "MobileNetV2: Inverted residuals and linear bottlenecks," trong CVPR, 2018.
[49] N. Ma, X. Zhang, H.-T. Zheng, và J. Sun, "ShuffleNet V2: Practical guidelines for efficient CNN architecture design," trong ECCV, 2018.
[50] W. Wang et al., "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions," trong ICCV, 2021.
[51] D. Zhou et al., "DeepViT: Towards deeper vision transformer," arXiv preprint arXiv:2103.11886, 2021.
[52] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, và H. Jégou, "Going deeper with image transformers," trong ICCV, 2021.
[53] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, và H. Jégou, "Training data-efficient image transformers & distillation through attention," trong ICML, 2021.
[54] B. Graham et al., "LeViT: a vision transformer in ConvNet's clothing for faster inference," trong ICCV, 2021.
[55] W. Wang et al., "CrossFormer: A versatile vision transformer hinging on cross-scale attention," trong ICLR, 2022.
[56] K. Wu et al., "TinyViT: Fast pretraining distillation for small vision transformers," trong ECCV, 2022.
[57] S. Mehta và M. Rastegari, "MobileViT: Light-weight, general-purpose, and mobile-friendly vision transformer," trong ICLR, 2022.
[58] Y. Chen et al., "Mobile-Former: Bridging MobileNet and transformer," trong CVPR, 2022.
[59] L. Wang và K.-J. Yoon, "Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks," TPAMI, vol. 44, no. 6, pp. 3048–3068, 2022.
[60] L. Liu et al., "Exploring inter-channel correlation for diversity-preserved knowledge distillation," trong ICCV, 2021.
[61] C. Yang, H. Zhou, Z. An, X. Jiang, Y. Xu, và Q. Zhang, "Cross-image relational knowledge distillation for semantic segmentation," trong CVPR, 2022.
[62] Z. Zhang, C. Zhou, và Z. Tu, "Distilling inter-class distance for semantic segmentation," trong IJCAI, 2022.
[63] T. He, C. Shen, Z. Tian, D. Gong, C. Sun, và Y. Yan, "Knowledge adaptation for efficient semantic segmentation," trong CVPR, 2019.
[64] Z. Tian et al., "Adaptive perspective distillation for semantic segmentation," TPAMI, 2022.
[65] S. Lin et al., "Knowledge distillation via the target-aware transformer," trong CVPR, 2022.
[66] X. Jiao et al., "TinyBERT: Distilling BERT for natural language understanding," trong EMNLP, 2020.
[67] C. Dong, G. Wang, H. Xu, J. Peng, X. Ren, và X. Liang, "EfficientBERT: Progressively searching multilayer perceptron via warm-up knowledge distillation," trong EMNLP, 2021.
[68] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, và M. Zhou, "MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers," trong NeurIPS, 2020.
[69] D. Zhou et al., "Understanding the robustness in vision transformers," trong ICML, 2022.
[70] Z. Chen et al., "Vision transformer adapter for dense predictions," trong ICLR, 2023.
[71] Y. Lee, J. Kim, J. Willette, và S. J. Hwang, "MPViT: Multi-path vision transformer for dense prediction," trong CVPR, 2022.
[72] X. Dong et al., "CSWin transformer: A general vision transformer backbone with cross-shaped windows," trong CVPR, 2022.
[73] X. Li, W. Wang, X. Hu, và J. Yang, "Selective kernel networks," trong CVPR, 2019.
[74] J. Hu, L. Shen, và G. Sun, "Squeeze-and-excitation networks," trong CVPR, 2018.
[75] B. Hariharan, P. Arbeláez, L. Bourdev, S. Maji, và J. Malik, "Semantic contours from inverse detectors," trong ICCV, 2011.
[76] I. Loshchilov và F. Hutter, "Decoupled weight decay regularization," trong ICLR, 2019.
[77] P. Mishra và K. Sarawadekar, "Polynomial learning rate policy with warm restart for deep neural network," trong TENCON, 2019.
[78] H. Zhang et al., "Context encoding for semantic segmentation," trong CVPR, 2018.
[79] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, và W. Liu, "CCNet: Criss-cross attention for semantic segmentation," trong ICCV, 2019.
[80] S. Choi, J. T. Kim, và J. Choo, "Cars can't fly up in the sky: Improving urban-scene segmentation via height-driven attention networks," trong CVPR, 2020.
[81] Y. Yuan, X. Chen, và J. Wang, "Object-contextual representations for semantic segmentation," trong ECCV, 2020.
[82] B. Cheng, A. Schwing, và A. Kirillov, "Per-pixel classification is not all you need for semantic segmentation," trong NeurIPS, 2021.
[83] R. P. Poudel, U. Bonde, S. Liwicki, và C. Zach, "ContextNet: Exploring context and detail for semantic segmentation in real-time," trong BMVC, 2018.
[84] T. Wu, S. Tang, R. Zhang, J. Cao, và Y. Zhang, "CGNet: A light-weight context guided network for semantic segmentation," TIP, vol. 30, pp. 1169–1179, 2021.
[85] R. P. K. Poudel, S. Liwicki, và R. Cipolla, "Fast-SCNN: Fast semantic segmentation network," trong BMVC, 2019.
[86] M. Tan và Q. Le, "EfficientNet: Rethinking model scaling for convolutional neural networks," trong ICML, 2019.
[87] J. Xie, B. Shuai, J.-F. Hu, J. Lin, và W.-S. Zheng, "Improving fast segmentation with teacher-student learning," trong BMVC, 2018.
[88] L. Van der Maaten và G. Hinton, "Visualizing data using t-SNE," JMLR, vol. 9, no. 11, 2008.

PHỤ LỤC A
CÁC HÀM LOSS CHƯNG CẤT TRI THỨC

Hàm loss bản đồ đặc trưng. Cả patch embedding và feature map losses thu được từ tất cả bốn giai đoạn đều được trọng số và thêm vào loss cross-entropy gốc. Để chưng cất bản đồ đặc trưng, chúng tôi tính toán loss HCL [2] giữa các đầu ra của CSF và khối transformer giáo viên tại mỗi giai đoạn.

Bản đồ đặc trưng thu được từ CSF tại mỗi giai đoạn với hình dạng [n,c,h,w] được chia thành thông tin ngữ cảnh 4 cấp độ thông qua adaptive average pooling. Chiều cao của các bản đồ đặc trưng đa cấp độ kết quả tại một giai đoạn là [h,4,2,1], điều này biểu thị rằng các hình dạng của bốn bản đồ đặc trưng trừu tượng là [n,c,h,w], [n,c,4,4], [n,c,2,2] và [n,c,1,1]. Tại mỗi giai đoạn, các khoảng cách L2 được sử dụng để chưng cất tri thức giữa các cấp độ ngữ cảnh. Loss HCL cuối cùng giữa các bản đồ đặc trưng tại giai đoạn thứ m được tính toán như:

L^m_{fm} = 1/(1 + Σ_l 1/2^l) Σ_{l=0}^3 1/2^l MSE(F^{out}_{m,l}, F^T_{m,l}), (13)

trong đó l đề cập đến số lượng cấp độ tại một giai đoạn. Mục đích của việc đặt loss HCL sau mỗi giai đoạn là khuyến khích chưng cất tri thức ở các cấp độ trừu tượng khác nhau.

PHỤ LỤC B
THÊM CÁC PHÂN TÍCH HIỆU SUẤT

Phân tích cải thiện IoU theo lớp trên các kiến trúc khác nhau. Để tiếp tục điều tra hiệu quả của việc chưng cất patch embedding trên các kiến trúc khác nhau, Hình 12 minh họa các cải thiện IoU theo lớp của TransKD của chúng tôi so với Knowledge Review với tất cả ba cặp giáo viên-học sinh, tức là SegFormer B2-B0 [26], PVT B2-B0 [25], SegFormer B2-LVT [34]. So với Knowledge Review, TransKD giúp các mô hình nhỏ gọn nhận dạng các lớp xe cộ ít thường xuyên chính xác hơn, như xe tải, xe buýt và tàu.

PHỤ LỤC C
THÊM CÁC PHÂN TÍCH ĐỊNH TÍNH

Phân tích bản đồ đặc trưng trung gian. Chúng tôi cung cấp giải thích trực quan về các quy trình quyết định nội bộ của mạng bằng cách trực quan hóa các bản đồ đặc trưng đến từ giai đoạn thứ 4 của mô hình trong Hình 13, với các ví dụ đại diện từ tập xác thực Cityscapes. So với học sinh mà không có bất kỳ thành phần TransKD nào, các mô hình được tăng cường TransKD trình bày các phản ứng có thể nhận dạng về mặt ngữ nghĩa. TransKD giúp chú ý nhiều hơn đến các đối tượng ô tô và xe tải, như được hiển thị trong các vùng màu đỏ trên bản đồ đặc trưng, trong khi duy trì ranh giới rõ ràng với nền kích hoạt phản ứng bản đồ đặc trưng yếu hơn nhiều (các vùng xanh dương và xanh lá), dẫn đến kết quả phân đoạn ngữ nghĩa tốt hơn. Ngoài ra, TransKD-Base có xu hướng tập trung vào các đối tượng lớn, ví dụ trên xe tải, trong khi TransKD-GL cho thấy phản ứng rõ ràng hơn trên ô tô có kích thước tương đối nhỏ hơn, mà chúng tôi cho là do việc trích xuất cả gợi ý cục bộ và toàn cục được khuyến khích bởi phương pháp này. Hơn nữa, TransKD-EA làm nổi bật tốt hơn các đối tượng xa camera, ví dụ ô tô trong hàng đầu. Những kết quả trực quan này tiếp tục minh họa hiệu quả của TransKD để học các đại diện trung gian nhận thức ngữ nghĩa.

Phân tích phân đoạn ngữ nghĩa ngoài trời. Ở đây chúng tôi so sánh các đầu ra phân đoạn của khung chưng cất dựa trên response hiệu quả nhất CD [1], khung chưng cất dựa trên feature KR [2] và tất cả ba phiên bản của TransKD. Các thí nghiệm phân đoạn ngữ nghĩa ngoài trời được thực hiện trên Cityscapes [30] (cảnh bình thường) và bộ dữ liệu ACDC [31] (cảnh bất lợi). Theo kết quả định lượng, TransKD-GL cho thấy hiệu suất thỏa mãn nhất trên thời tiết tuyết, trong khi TransKD-EA vượt trội tất cả các đối tác trên thời tiết bình thường và bất lợi khác. Như được hiển thị trong Hình 14, học sinh SegFormer B0 mà không có tiền huấn luyện, và được chưng cất với CD và KR, có thể nhận dạng thô các cảnh, như đường, vỉa hè và thực vật, nhưng transformer học sinh nhỏ gọn vẫn không thể phân đoạn các chi tiết, ví dụ đèn giao thông và người đi bộ ở xa và xe buýt bị che khuất. Ngược lại, TransKD trích xuất cả quan hệ không gian và tuần tự thông qua sự kết hợp của chưng cất bản đồ đặc trưng và patch embedding, từ đó cải thiện tính nhất quán của xe buýt. Hơn nữa, TransKD có thể phân biệt hiệu quả người, người lái xe và xe đạp trong thời tiết bất lợi, ngay cả khi chúng ở xa.

Phân tích phân đoạn ngữ nghĩa trong nhà. Trong nghiên cứu cuối cùng của chúng tôi, chúng tôi chuyển sang kết quả phân đoạn ngữ nghĩa trong nhà và minh họa các kết quả phân đoạn trên bộ dữ liệu NYUv2 [32] trong Hình 15. So với kết quả phân đoạn được tạo ra bởi học sinh mà không có bất kỳ chưng cất nào, ranh giới phân đoạn của TransKD rõ ràng hơn nhiều, và hầu hết các chi tiết của cảnh trong nhà được bảo tồn, đặc biệt là cho cảnh phòng ngủ, ví dụ trên gối. Đối với ví dụ phòng tắm, kết quả tương tự có thể được tìm thấy, trong đó, ví dụ, khăn tắm được nhận dạng tốt hơn bởi các phương pháp của chúng tôi. Ngoài các đối tượng có kích thước tương đối nhỏ, các đối tượng có ít gợi ý kết cấu có thể được phân đoạn hoàn toàn, ví dụ tường trong phòng ngủ. Những ví dụ định tính này liên tục xác nhận khả năng tổng quát hóa tốt của TransKD qua các môi trường đa dạng (trong nhà và ngoài trời, các điều kiện thời tiết khác nhau, v.v.) cũng như lợi ích của việc chuyển giao tri thức từ cả patch embeddings và bản đồ đặc trưng trong chưng cất tri thức transformer-to-transformer.

--- TRANG 19 ---
IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, THÁNG 9 2024 19

Hình ảnh Sự thật nền Học sinh
TransKD-Base TransKD-GL TransKD-EA

Hình 15: Kết quả phân đoạn ngữ nghĩa cảnh trong nhà định tính của các phương pháp TransKD của chúng tôi trên bộ dữ liệu NYUv2 [32].
