# BPKD: Chưng cất Tri thức Ưu tiên Biên giới
# Cho Phân đoạn Ngữ nghĩa

# 2306.08075.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/semantic-segmentation/2306.08075.pdf
# Kích thước tệp: 25155884 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
BPKD: Chưng cất Tri thức Ưu tiên Biên giới
Cho Phân đoạn Ngữ nghĩa
Liyang Liu1 Zihan Wang2 3 Minh Hieu Phan1 Bowen Zhang1 Jinchao Ge1 Yifan Liu1*
1Đại học Adelaide, Úc 2Đại học Queensland, Úc
3CSIRO's Data61, Úc

Tóm tắt
Các phương pháp chưng cất tri thức hiện tại trong phân đoạn ngữ nghĩa có xu hướng áp dụng một cách tiếp cận tổng thể đối xử công bằng với tất cả các vị trí không gian. Tuy nhiên, đối với dự đoán dày đặc, dự đoán của học sinh trên các vùng biên có độ không chắc chắn cao do rò rỉ thông tin ngữ cảnh, đòi hỏi tri thức độ nhạy không gian cao hơn so với các vùng thân. Để giải quyết thách thức này, bài báo này đề xuất một phương pháp mới được gọi là chưng cất tri thức ưu tiên biên giới (BPKD). BPKD chưng cất tri thức của thân và biên của mô hình giáo viên một cách riêng biệt cho mô hình học sinh nhỏ gọn. Cụ thể, chúng tôi sử dụng hai hàm mất mát khác biệt: (i) mất mát biên, nhằm phân biệt giữa các lớp mơ hồ ở mức pixel trong các vùng biên; (ii) mất mát thân, sử dụng các ràng buộc hình dạng và chọn lọc tập trung vào các vùng ngữ nghĩa bên trong. Các thí nghiệm của chúng tôi chứng minh rằng phương pháp BPKD được đề xuất cung cấp các cải thiện và tổng hợp mở rộng cho các vùng biên và thân. Ngoài ra, phương pháp này đạt được hiệu suất chưng cất tốt nhất cho phân đoạn ngữ nghĩa trên ba bộ dữ liệu chuẩn phổ biến, làm nổi bật tính hiệu quả và khả năng tổng quát của nó. BPKD cho thấy những cải thiện nhất quán trên một loạt đa dạng các cấu trúc phân đoạn nhẹ, bao gồm cả CNN và transformer, nhấn mạnh khả năng thích ứng không phụ thuộc kiến trúc của nó. Mã nguồn có sẵn tại https://github.com/AkideLiu/BPKD.

1. Giới thiệu
Phân đoạn ngữ nghĩa là một nhiệm vụ thị giác máy tính phức tạp bao gồm việc gán các danh mục duy nhất cho từng pixel của một hình ảnh đầu vào. Trong những năm gần đây, các mô hình học sâu với số lượng tham số lớn đã đạt được hiệu suất đáng kể trong phân đoạn ngữ nghĩa [9, 10, 18, 39, 66, 67, 69]. Tuy nhiên, những mô hình như vậy không thực tế cho các thiết bị hạn chế tài nguyên như thiết bị di động và robot do độ phức tạp tính toán cao [53,58,64]. Để giải quyết vấn đề này, các mô hình cơ sở nhẹ như MobileNet [22], ShuffleNet [41], và EfficientNet [50] đã được sử dụng cho phân đoạn ngữ nghĩa thời gian thực.

*Tác giả liên hệ, Email: yifan.liu04@adelaide.edu.au.

A)Không có Chưng cất
BPKD(Của chúng tôi) Hình ảnh Gốc
CWD
B)(a) Hình ảnh Gốc

A)Không có Chưng cất
BPKD(Của chúng tôi) Hình ảnh Gốc
CWD
B) (b) Học sinh không có KD

A)Không có Chưng cất
BPKD(Của chúng tôi) Hình ảnh Gốc
CWD
B)
(c) CWD

A)Không có Chưng cất
BPKD(Của chúng tôi) Hình ảnh Gốc
CWD
B) (d)BPKD(Của chúng tôi)

Entropy Lớp đầu tiên (trung bình) Độ chính xác Pixel Biên (%) Raw SKDS[34] CIRKD[57] CWD[47] BPKD(Của chúng tôi) 49.2 48.0 12 13 14 15 16 17 18
46 48 50 52 54 56 58
50.2 52.9 56.7 Entropy Lớp đầu tiên (trung bình) Độ chính xác Pixel Biên Entropy Giáo viên

(e)

Hình 1: Minh họa rò rỉ thông tin ngữ cảnh.
Trên) Các bản đồ không chắc chắn được tạo bằng cách tính entropy trung bình từ các bản đồ đặc trưng lớp đầu tiên, sử dụng các chiến lược chưng cất khác nhau như học sinh thô, CWD, và BPKD. Màu sáng hơn cho thấy độ chắc chắn cao hơn.
Dưới) Chứng minh mối tương quan nghịch đảo giữa entropy trung bình và độ chính xác mức pixel dọc theo các biên. Khi học các pixel biên, mô hình tổng hợp thông tin ngữ cảnh từ các danh mục lớp kế cận. Các phương pháp chưng cất toàn cảnh trước đây bị ảnh hưởng bởi rò rỉ thông tin ngữ cảnh, dẫn đến độ không chắc chắn cao trong các đặc trưng mức thấp biên và độ chính xác pixel biên thấp.

--- TRANG 2 ---
Thiết kế các kỹ thuật nén và tăng tốc cho các mạng nhỏ gọn là thách thức nhưng quan trọng. Các phương pháp chưng cất tri thức, như những phương pháp được giới thiệu trong [19,20,28,65], huấn luyện một mạng học sinh nhỏ hơn để bắt chước mạng giáo viên phức tạp bằng cách tối thiểu hóa khoảng cách xác suất mềm, thường được đo bằng divergence Kullback–Leibler (KL), giữa học sinh và giáo viên. Trong [31,62,68], các tác giả đã cố gắng chưng cất tri thức ẩn bằng cách sử dụng các mối quan hệ mạng và dữ liệu, với trọng tâm vào các nhiệm vụ phân loại, đạt được kết quả ấn tượng.

Các phương pháp chưng cất tri thức tiên phong cho phân đoạn ngữ nghĩa [35, 49, 56, 60] tập trung nhiều hơn vào việc nắm bắt thông tin tương quan giữa các pixel, kênh và hình ảnh. Liu et al. [35] gợi ý rằng tri thức ẩn trong phân đoạn ngữ nghĩa được xây dựng bởi biểu diễn có cấu trúc. Tri thức có cấu trúc phù hợp hơn cho việc giảm độ tương tự theo cặp và chưng cất tổng thể. IFVD [56] đề xuất mã hóa tri thức theo các mặt nạ ngữ nghĩa. Trong CWD [49], các tác giả tinh chỉnh chưng cất bằng cách nhấn mạnh việc căn chỉnh vùng nổi bật nhất của mỗi kênh giữa giáo viên và học sinh.

So với các nghiên cứu trước đây, bao gồm nhiều nghiên cứu khác nhau [3, 26, 35, 49, 56, 60, 61], chủ yếu tập trung vào việc chuyển giao các biểu diễn tri thức trên toàn bộ hình ảnh, tầm quan trọng của các biểu diễn tri thức khác biệt tại các vị trí không gian khác nhau đã bị bỏ qua. Khi học các đặc trưng biên, mô hình tổng hợp thông tin ngữ cảnh giữa các danh mục lớp kế cận, dẫn đến rò rỉ thông tin ngữ cảnh. Như được hiển thị trong Hình. 1 b, c, d), các phương pháp chưng cất toàn cảnh hiện tại thể hiện mức độ không chắc chắn cao, cũng như mức entropy cao hơn, tại các vùng biên. Theo các công trình trước đây [1], chúng tôi định lượng độ không chắc chắn bằng cách tính entropy trung bình của các đặc trưng lớp đầu tiên, nắm bắt các biểu diễn văn bản mức thấp. Hình. 1 e) cho thấy các phương pháp hiện tại gặp phải độ không chắc chắn cao hơn và độ chính xác thấp hơn tại các pixel biên, cho thấy hiện tượng rò rỉ thông tin ngữ cảnh. Khả năng thấp của các mạng học sinh nhỏ gọn làm trầm trọng thêm hiện tượng này, làm giảm chất lượng phân đoạn chi tiết trên các ranh giới, đặc biệt đối với phân đoạn đối tượng nhỏ. Tuy nhiên, việc vạch ra ranh giới của đối tượng là bắt buộc đối với các ứng dụng thực tế như định vị ranh giới đường cho điều hướng tự động [44] hoặc phân đoạn khối u cho lập kế hoạch điều trị [34].

Để giải quyết vấn đề học thông tin ngữ cảnh trong các phương pháp hiện có, chúng tôi đề xuất một phương pháp mới, được gọi là Chưng cất Tri thức Ưu tiên Biên giới (BPKD). Chúng tôi chia quá trình chưng cất tri thức thành hai phần phụ: phần chưng cất biên và phần chưng cất thân. Phương pháp BPKD được đề xuất của chúng tôi tăng cường rõ ràng chất lượng của các vùng biên và ranh giới đối tượng bằng cách tách rời chưng cất tri thức và sử dụng nhãn mềm giáo viên. Mất mát chưng cất biên bao gồm căn chỉnh xác suất không gian và tổng hợp thông tin ngữ cảnh để tinh chỉnh các ranh giới. Hơn nữa, các ranh giới cung cấp tri thức tiên nghiệm về hình dạng của các vùng bên trong của đối tượng, và vùng thân có thể khai thác tri thức này để loại bỏ các mẫu ranh giới có độ không chắc chắn cao và làm mượt các đường cong học. Do đó, chúng tôi quan sát thấy rằng trung tâm đối tượng nhận được sự chú ý lớn hơn do các ràng buộc hình dạng ngầm định, cải thiện thêm phân đoạn trong vùng thân.

Thông qua phân tích thực nghiệm, chúng tôi đã chứng minh rằng phương pháp được đề xuất hiệu quả hướng dẫn mạng học sinh học từ tri thức của mạng giáo viên, dẫn đến hiệu suất phân đoạn được cải thiện. Chúng tôi đánh giá phương pháp của mình trên các kiến trúc phổ biến trên ba bộ dữ liệu chuẩn phân đoạn: Cityscapes [13], ADE20K [71], và Pascal Context [15]. Kết quả thí nghiệm cho thấy BPKD vượt trội hơn các phương pháp chưng cất tiên tiến khác. Cụ thể, chúng tôi giảm sự khác biệt về hiệu suất giữa các mạng học sinh và giáo viên và thể hiện kết quả cạnh tranh so với các phương pháp phân đoạn thời gian thực chuyên biệt [16].

Các đóng góp chính của chúng tôi được tóm tắt như sau:
• Chúng tôi cho thấy rằng các phương pháp chưng cất hiện tại gặp phải vấn đề rò rỉ thông tin ngữ cảnh bằng cách phân tích độ không chắc chắn đặc trưng mức thấp, dẫn đến hiệu suất phân đoạn không tối ưu tại các ranh giới. Theo hiểu biết của chúng tôi, đây là bài báo đầu tiên xác định vấn đề quan trọng này trong tài liệu chưng cất tri thức cho phân đoạn ngữ nghĩa.

• Chúng tôi đề xuất một phương pháp chưng cất tri thức mới tập trung riêng biệt vào việc chưng cất thông tin liên quan đến thân và biên của đối tượng. Hàm mất mát biên chuyên biệt của chúng tôi tăng cường đáng kể chất lượng của các lát biên, đồng thời áp đặt các ràng buộc hình dạng mạnh mẽ trên các vùng thân. Phương pháp này hiệu quả tối thiểu hóa độ không chắc chắn ngăn chặn rò rỉ thông tin ngữ cảnh trong quá trình chưng cất và khuếch đại sự tập trung vào vùng bên trong.

• Phương pháp của chúng tôi đạt được kết quả tiên tiến trên ba bộ dữ liệu chuẩn phổ biến. Chúng tôi báo cáo mức tăng trong giao điểm trung bình trên hợp (mIoU) lên đến 4.02% khi so sánh với CWD SOTA trước đây. Ngoài ra, chúng tôi quan sát thấy sự cải thiện đáng kể về chất lượng dự đoán trong cả vùng biên và thân, chứng minh thêm tính hiệu quả của chúng tôi.

2. Công trình Liên quan
Phân đoạn Ngữ nghĩa. Các phương pháp tiên tiến gần đây trong phân đoạn ngữ nghĩa chủ yếu tận dụng Mạng Tích chập Hoàn toàn (FCN) [33,40,67]. Các mô hình đáng chú ý như PSPNet [70] và loạt DeepLab [4–7] sử dụng các kỹ thuật tiên tiến như mô-đun gộp kim tự tháp (PPM) và gộp kim tự tháp không gian atrous (ASPP) để nắm bắt ngữ cảnh đa tỷ lệ. HRNet [53] tiếp tục đổi mới với một xương sống song song để duy trì đặc trưng độ phân giải cao. Mặc dù hiệu suất của chúng, những mô hình này rất tốn kém về mặt tính toán, hạn chế khả năng ứng dụng của chúng trong các tình huống thời gian thực và thiết bị biên. Do đó, các mô hình nhẹ như ENet [45], SqueezeNet [25], và ESPNet [42] đã thu hút sự chú ý. Những mô hình này sử dụng các chiến lược như lấy mẫu xuống sớm, phân tích bộ lọc, và kim tự tháp không gian hiệu quả để giảm chi phí tính toán. Các biến thể MobileNet [22,23,48] cũng hiệu quả cho phân đoạn hiệu quả.

Phát hiện Biên. Các thuật toán phát hiện biên cổ điển như Canny [2], Sobel [27], và Prewitt [46] đã được trang bị lại vào các kiến trúc học sâu hiện đại để đạt được phân đoạn chi tiết. Các phương pháp phát hiện biên được giám sát sâu như HED [59] và RCF [37] giới thiệu thông tin biên đa tỷ lệ trực tiếp vào các đường ống phân đoạn. Tương tự, các mô hình như CASENet [63] đã tiến bộ theo nghệ thuật tiên tiến bằng cách hợp nhất các biên cụ thể theo lớp vào các thuật toán phân đoạn, cung cấp lợi ích kép của biểu diễn ranh giới chi tiết và phân biệt lớp. Song song, các kỹ thuật như mô hình chú ý biên [54] kết hợp thông tin biên bằng cách cân nhắc các đặc trưng dựa trên tầm quan trọng ranh giới của chúng, cho phép lập bản đồ đường viền tinh vi hơn trong phân đoạn ngữ nghĩa.

Chưng cất Tri thức. Chưng cất tri thức (KD) nhằm cô đặc các học tập từ một hoặc nhiều mô hình giáo viên mở rộng thành một mô hình học sinh hợp lý [17, 20]. Chủ yếu được sử dụng trong các nhiệm vụ thị giác cơ bản, các kỹ thuật KD có thể được phân loại thành các mô hình dựa trên phản hồi, dựa trên đặc trưng, và dựa trên mối quan hệ. Các phương pháp dựa trên phản hồi, chủ yếu được khởi xướng bởi Hinton et al. [20], tối thiểu hóa divergence Kullback-Leibler để truyền đạt tri thức ngầm định, có giá trị cao [19, 28, 65]. Các phương pháp dựa trên đặc trưng như FitNet [47] căn chỉnh các kích hoạt đặc trưng bên trong giữa giáo viên và học sinh, trong khi các phương pháp dựa trên mối quan hệ [31, 62, 68] đi sâu vào các mối quan hệ giữa các lớp hoặc giữa các mẫu. Tuy nhiên, KD truyền thống phần lớn thiên về phân loại hình ảnh, cung cấp tiện ích hạn chế trong các nhiệm vụ phân đoạn mức pixel.

Những tiến bộ gần đây đã thấy các phương pháp KD được điều chỉnh cho phân đoạn ngữ nghĩa. Các chiến lược như chưng cất tri thức cấu trúc [35, 36] định nghĩa phân đoạn như một nhiệm vụ dự đoán có cấu trúc, sử dụng các độ tương tự theo cặp và các cải tiến đối kháng tổng thể cho chuyển giao tri thức. Chưng cất theo kênh [49] tập trung vào các vùng kênh nổi bật. Các đổi mới bổ sung bao gồm chưng cất biến thiên đặc trưng trong lớp [56], kết hợp biến thiên mức pixel và theo lớp, và chưng cất mối quan hệ liên hình ảnh [60], tối ưu hóa các kết nối ngữ nghĩa toàn cầu. Chưng cất Tạo sinh Có mặt nạ [61] tận dụng hướng dẫn giáo viên để phục hồi đặc trưng. Công trình gần đây [3] gợi ý tương quan Pearson như một thay thế khả thi cho divergence KL. Xác thực thực nghiệm chứng thực hiệu quả của những kỹ thuật KD chuyên biệt này trong việc tăng cường hiệu suất phân đoạn ngữ nghĩa.

Trong những năm gần đây, một số kỹ thuật chưng cất tri thức trong phân đoạn ngữ nghĩa đã được đề xuất, mỗi kỹ thuật tập trung vào các khía cạnh khác nhau của vấn đề. Tuy nhiên, những phương pháp này thường bỏ qua tầm quan trọng của sự khác biệt đặc trưng vùng và tầm quan trọng tương đối của chúng. Để giải quyết hạn chế này, phương pháp của chúng tôi bao gồm việc chia tách các đường ống chưng cất tri thức và cải thiện hiệu suất.

3. Phương pháp
Trong phần này, trước tiên chúng tôi cung cấp tổng quan về quy trình làm việc của khung Chưng cất Tri thức Ưu tiên Biên giới (BPKD) (Phần 3.1), tiếp theo là mô tả chi tiết về hai khía cạnh triển khai chính của phương pháp chúng tôi. Cụ thể, Phần 3.2 phác thảo quá trình chưng cất tri thức biên, bao gồm lọc trước mặt nạ và lọc sau mặt nạ. Phần 3.3 giới thiệu mất mát chưng cất cho cải tiến thân.

3.1. Khung BPKD
Các kỹ thuật chưng cất đặc trưng hiện có [49, 60] chuyển giao các biểu diễn toàn cảnh từ giáo viên trong khi bỏ qua các hiệu ứng của các đặc trưng biên nhiễu trên quá trình chưng cất. Trong khung này, chúng tôi cẩn thận xem xét độ nhạy của các biểu diễn biên và giới thiệu chưng cất tri thức ưu tiên biên giới (BPKD) mới chuyển giao tri thức trong các vùng thân và biên một cách riêng biệt. Chưng cất các vùng biên riêng lẻ tăng cường chất lượng ranh giới đối tượng một cách rõ ràng. Hơn nữa, mất mát chưng cất biên cung cấp tri thức hình dạng tiên nghiệm cho các vùng bên trong của đối tượng. Ví dụ, với ràng buộc ranh giới của một chiếc xe, mô hình có thể dễ dàng xác định các danh mục pixel cho vùng bên trong của nó. Mất mát chưng cất thân có hai lợi ích chính từ tri thức ranh giới tiên nghiệm: (1) giảm khó khăn học bằng cách bắt chước phân phối xác suất logit của giáo viên vì các ranh giới có độ không chắc chắn cao được loại bỏ, và (2) tận dụng sự chú ý cao hơn vào trung tâm đối tượng thông qua các ràng buộc hình dạng ngầm định.

Phương pháp của chúng tôi sử dụng một kỹ thuật phát hiện biên để tạo mặt nạ biên ME cho mỗi lớp bằng cách xử lý sự thật cơ bản và bản đồ logit phân đoạn. Cho Z ∈ RH×W×C biểu thị bản đồ logit của một mạng, trong đó C tương ứng với số kênh và H×W đại diện cho độ phân giải không gian. Các mặt nạ biên ME được áp dụng để tách bản đồ logit Z thành hai thành phần: thành phần thân ZB và thành phần biên ZE, tuân theo quy tắc cộng, được ký hiệu bởi Z=ZB+ZE. Khung BPKD của chúng tôi chuyển giao riêng biệt tri thức biên và thân được mã hóa trong hai thành phần này cho học sinh. Vì các lát biên có ít lượng biểu diễn tri thức hơn, chúng tôi giới thiệu nhận thức được phân loại để cân bằng tầm quan trọng của các quan điểm đặc biệt khác nhau. Cùng nhau, những kỹ thuật này đóng vai trò quan trọng trong việc cải thiện hiệu suất tổng thể của mô hình.

Trong nghiên cứu này, chúng tôi đề xuất một phương pháp mới để phân tách mất mát chưng cất thành hai thành phần riêng biệt, cụ thể là mất mát thân ℓB và mất mát biên ℓE, như được biểu thị bởi Phương trình 2. Chúng tôi bao gồm trọng số mất mát thân λb và trọng số mất mát biên λe để kiểm soát các đóng góp của mỗi số hạng mất mát. Chiến lược tách rời này cho phép chúng tôi kiểm tra độ nhạy của việc học biên trong quá trình chưng cất tri thức, điều này đã bị bỏ qua bởi tài liệu. Mục tiêu mất mát được định nghĩa là:
ℓ=λb·ℓB(ZS_B,ZT_B)+λe·ℓE(ZS_E,ZT_E). (1)

3.2. Biểu diễn Tri thức Biên.
Khung của chúng tôi tối thiểu hóa sự khác biệt giữa các đặc trưng của giáo viên và học sinh trên các khu vực biên. Để đạt được điều này, chúng tôi trích xuất một biểu diễn tri thức biên bằng cách sử dụng các mặt nạ biên mềm. Mặt nạ biên này được áp dụng trên bản đồ logit để tạo ra một biểu diễn đặc trưng có mặt nạ cho các vùng biên của giáo viên và học sinh. Bản đồ biên ME được tạo thông qua hai giai đoạn: Lọc Trước Mặt nạ (PRM), nắm bắt sự khác biệt biên cho tất cả các lớp, và Lọc Sau Mặt nạ (POM), trích xuất sự khác biệt biên cho từng lớp riêng lẻ. Phương pháp này cho phép một mô hình trích xuất biểu diễn tri thức biên chính xác và chính xác hơn, dẫn đến hiệu suất được cải thiện trong phân loại chi tiết.

Trong quá trình phát hiện biên, chúng tôi sử dụng thuật toán Trimap có thể điều chỉnh [52] để trích xuất các biểu diễn biên được ký hiệu bởi ZE từ sự thật cơ bản (GT). Mặc dù dự đoán của giáo viên có thể phục vụ như một nguồn thay thế để tạo mặt nạ biên, chúng cung cấp độ chính xác hơi giảm so với việc sử dụng GT trực tiếp. Để tạo mặt nạ biên nhị phân GT edge, chúng tôi tính sự khác biệt giữa các phép toán giãn nở và xói mòn được áp dụng cho GT, được biểu thị chính thức là GT edge=dilation (GT)−erosion (GT). Mặt nạ nhị phân kết quả GT edge∈RC×H×W trải qua gộp trung bình để tạo ra ME∈RC×H′×W′, có cùng hình dạng với dự đoán logits Zpred. Các kích thước của ME được điều chỉnh bởi bước đầu ra S của mạng phân đoạn, cụ thể là W′=W/S.

•Lọc Trước Mặt nạ (PRM). Để có được bản đồ logits, chúng tôi áp dụng ME cho cả logits học sinh và giáo viên: ZE= Zpred·ME. Cụ thể, chúng tôi áp dụng mặt nạ biên cho mỗi kênh C để có thể tập trung logits cho các vùng biên chồng chéo. Một ví dụ trực quan là nếu có một khung hiển thị một con chó và một con mèo đứng gần nhau, chỉ có kích hoạt logits của lớp chó và mèo sẽ được xem xét và tất cả các kích hoạt khác sẽ bị ngăn chặn. Phép toán như vậy buộc học sinh tập trung nhiều hơn vào các mối tương quan giữa các lớp mơ hồ kề cận. Mất mát divergence KL mức không gian được áp dụng cho các logits được lọc ZS_E và ZT_E:

φ(ZT_E,ZS_E) = ∑C_c=1 ϕ(ZT_E,i)·log(ϕ(ZT_E,i)/ϕ(ZS_E,i)), (2)

trong đó ϕ là phép toán softmax cho mỗi pixel.
φ(ZT_E,ZS_E) đại diện cho các khoảng cách KL có mặt nạ biên cho tất cả các vị trí không gian.

•Lọc Sau Mặt nạ (POM). Chúng tôi tiếp tục tách mất mát biên cho từng lớp và thực hiện chuẩn hóa dựa trên diện tích biên bằng Lọc Sau Mặt nạ (POM). Cho ZT_E,i,c và ZS_E,i,c biểu thị các logits cho lớp thứ c tại pixel i trong các mô hình giáo viên và học sinh, tương ứng. Cho ME,c là mặt nạ mềm thu được bằng gộp trung bình mặt nạ biên nhị phân sự thật cơ bản cho lớp thứ c, và nc biểu thị số pixel khác không trong ME,c cho lớp này. Số hạng POM của chúng tôi có thể được công thức hóa như sau:

ℓE=∑C_c=1 (αc/nc) · (1/(W·H)) ∑W·H_i=1 φ(ZT_E,i,c,ZS_E,i,c)·ME,c, (3)

Bằng cách cân lại mất mát dựa trên diện tích biên của mỗi lớp, chúng tôi ưu tiên trung tâm của biên, nơi thông tin quan trọng nhất thường được định vị. Phương pháp này đảm bảo rằng mô hình học sinh tập trung vào việc học các vị trí và hình dạng biên chính xác cho mỗi lớp.

Các Mặt nạ Biên Mềm ME đóng vai trò quan trọng trong Mất mát Biên, và phương pháp tạo ra chúng của chúng tôi bao gồm hai thiết kế chuyên biệt: 1) chuyển đổi GTE nhị phân thành không gian rời rạc có trọng số, và 2) tạo mặt nạ cho mỗi kênh thay vì mặt nạ thống nhất. Việc áp dụng trực tiếp mặt nạ nhị phân có thể bao gồm thiên vị không tự tin, vì vậy chúng tôi sử dụng gộp trung bình để tạo các mặt nạ mềm hơn. Chúng tôi cũng cẩn thận xem xét các mặt nạ chồng chéo để tối thiểu hóa nhiễu và độ không chắc chắn. Thiết kế mặt nạ của chúng tôi nhằm bao gồm riêng biệt thiên vị không tự tin để tối thiểu hóa các phân phối tri thức.

Tóm lại, các giai đoạn PRM và POM được đề xuất trong vùng biên tinh chỉnh quá trình chưng cất tri thức bằng cách xác định các khác biệt biên cho mỗi lớp và áp dụng cân lại cho mất mát dựa trên diện tích biên. Phương pháp này đảm bảo rằng mô hình học sinh học các vị trí và hình dạng biên chính xác cho mỗi lớp, và cung cấp tri thức hình dạng tiên nghiệm cho biểu diễn tri thức thân.

3.3. Biểu diễn Tri thức Thân.
Phần này điều tra chưng cất tri thức thân. Các công trình trước đây xem xét chưng cất toàn cảnh, làm loãng tri thức thân với biểu diễn nhiễu trên biên. Để vượt qua những thách thức này, chúng tôi sử dụng mặt nạ biên nhị phân đảo ngược để trích xuất mặt nạ thân. Bằng cách loại bỏ vùng biên, chúng tôi khai thác các ràng buộc hình dạng ngầm định và giảm độ không chắc chắn, cho phép mất mát thân tập trung vào việc gán các vùng bên trong lớn của đối tượng cho các danh mục tương ứng của chúng. Để đạt được điều này, chúng tôi đề xuất một phương pháp căn chỉnh vùng tổng hợp các kích hoạt mức kênh để có được các phần giàu ngữ nghĩa. Như chúng tôi đã định nghĩa trước rằng Z=ZB+ZE. Logits thân được thu được bởi ZB=Z × (1−ME). Như được hiển thị trong công trình trước đây [35, 49], mất mát mức pixel cho vùng thân sẽ mang lại nhiễu không mong muốn do các ràng buộc cứng. Do đó, chúng tôi sử dụng ràng buộc lỏng lẻo của chưng cất theo kênh [49] cho phần thân. Mất mát cải tiến thân (BEL) được định nghĩa là:

ℓB=T2/C ∑C_c=1 ∑W·H_i=1 ϕ(ZT_B,c,i)·log[ϕ(ZT_B,c,i)/ϕ(ZS_B,c,i)], (4)

4. Thí nghiệm
4.1. Thiết lập Thí nghiệm
Bộ dữ liệu. Chúng tôi tiến hành các thí nghiệm trên ba bộ dữ liệu chuẩn cho phân đoạn ngữ nghĩa: Cityscapes [13], Pascal Context 2010 [15], và ADE20K [71].

ADE20K [71] chứa 20k/2k/3k hình ảnh cho train/val/test với 150 lớp ngữ nghĩa. Nó được xây dựng như chuẩn cho phân tích cảnh và phân đoạn thể hiện.

Cityscapes [13] là một bộ dữ liệu phân tích cảnh đô thị chứa 2975/500/1525 hình ảnh được chú thích tinh vi được sử dụng cho train/val/test. Hiệu suất được đánh giá trong 19 lớp.

Pascal Context [15] cung cấp các chú thích dày đặc, chứa 4998/5105/9637 hình ảnh train/val/test. Chúng tôi sử dụng 59 danh mục đối tượng cho huấn luyện và kiểm tra. Kết quả của chúng tôi được báo cáo trên tập xác thực.

Chi tiết Triển khai. Triển khai của chúng tôi dựa trên hộp công cụ mã nguồn mở MMSegmentation [11, 12] với PyTorch 1.11.0. Chúng tôi sử dụng tăng cường dữ liệu tiêu chuẩn, bao gồm lật ngẫu nhiên, cắt, và tỷ lệ trong phạm vi [0.5, 2]. Tất cả các thí nghiệm được tối ưu hóa bởi SGD với động lượng 0.9, và kích thước batch 16. Chúng tôi sử dụng cắt 512 ×512, 512 ×1024, và 480 ×480 cho ADE20k, Cityscapes, và Pascal Context, tương ứng. Chúng tôi sử dụng tỷ lệ học ban đầu 0.01 cho ADE20K và Cityscapes. Ngoài ra, chúng tôi sử dụng tỷ lệ học ban đầu 0.004 cho Pascal Context. Số lần lặp huấn luyện tổng cộng là 80K. Theo các phương pháp trước đây [7,70], chúng tôi sử dụng chính sách tỷ lệ học poly và báo cáo kết quả kiểm tra đơn tỷ lệ. Chúng tôi tiến hành tất cả các thí nghiệm trên 4 GPU NVIDIA A100. Tất cả các phương pháp chưng cất được huấn luyện với cùng cấu hình.

Chỉ số. Chúng tôi thiết lập so sánh công bằng bằng cách gán các tham số giống hệt cho mỗi phương pháp với cùng bộ dữ liệu. Giao điểm trên Hợp trung bình (mIoU), Trimap mIoU và độ chính xác trung bình pixel (mAcc) được sử dụng như các chỉ số đánh giá chính. GFLOPs, FPS và Số Tham số cũng được báo cáo cho các mạng học sinh khác nhau mà chúng tôi đã kiểm tra. Tất cả chi phí tính toán được báo cáo được đo bằng fvcore.¹

4.2. So sánh với Các Phương pháp Tiên tiến
Để đảm bảo so sánh công bằng, chúng tôi đã triển khai lại một số phương pháp chưng cất tri thức được đề xuất trước đây, bao gồm những phương pháp của [35, 49, 56, 60]. Sau đó, chúng tôi đánh giá phương pháp BPKD của mình với các mạng nhỏ gọn khác nhau, như PSPNet với xương sống ResNet18 [70], HRNet-W18 [53], Deeplab-V3+ với xương sống MobileNetV2 [48], ISANet với ResNet18 [24], Swin Transformers [38] với UPerNet [57] và DeiT [51]-Adapter [8] với UPerNet [57].

¹https://github.com/facebookresearch/fvcore

Bảng 2: So sánh hiệu suất của kiến trúc dựa trên transformer với các chiến lược chưng cất khác nhau. mIoU tiêu chuẩn và Trimap mIoU của Swin Transformers [38] và DeiT [51] với ViT Adapter (DeiT-Ada) [8] trên ADE20K với bộ giải mã UPerNet [57] cho 80K lần lặp. Tốc độ chuyển tiếp chưng cất (DFS.), thời gian huấn luyện (TT.) và dấu chân bộ nhớ GPU (GMem.) cho thấy phương pháp của chúng tôi có chi phí tính toán không đáng kể. (DFS.) và (TT.) được ước tính trên DeiT-Adapter với kích thước batch = 16 với 4 GPU và (GMem.) tiêu chuẩn cho phân bổ bộ nhớ video mỗi mẫu.

DFS.(S) ↑ TT.(H) ↓ GMem.(G) ↓ Swin ↑ Trimap ↑ DeiT-Ada. ↑ Trimap ↑
T:Base 9.52 11.26 8.32 50.13 40.10 48.80 39.72
S:Tiny 12.80 8.44 3.87 43.57 32.78 41.10 32.15
SKDS 8.72 11.36 4.45 43.58 33.04 41.90 32.25
IFVD 6.06 16.45 8.97 43.75 32.90 41.16 32.11
CIRKD 7.70 16.35 10.70 43.32 32.68 41.64 32.23
CWD 8.76 11.15 4.45 44.99 33.73 44.25 33.49
BPKD 7.84 13.49 5.49 46.13 38.11 45.25 37.05

--- TRANG 7 ---
Phương pháp
Giáo viên: PSP-ResNet101 79.74%
Học sinh: PSP-ResNet18 Tiêu chuẩn: 68.99% Trimap: 55.34%
Chưng cất Theo Kênh [49] Tiêu chuẩn: 74.29% Trimap: 57.34%
Chưng cất Theo Pixel [35] Tiêu chuẩn: 69.33% Trimap: 53.82%

Thân(C) Thân(P) Biên(P) Biên(C) Của chúng tôi
mIoU(%) Tiêu chuẩn 74.17 ( ▲5.18) 72.70 ( ▲3.71) 71.63 ( ▲2.64) 66.83 ( ▼2.16) 75.94 ( ▲6.95)
Trimap 56.20 ( ▲0.86) 54.12 ( ▼1.22) 61.37 ( ▲6.03) 51.76 ( ▼3.58) 62.91 ( ▲7.57)

Bảng 3: Tính hiệu quả của việc tách biệt biểu diễn tri thức toàn cảnh. Kết quả cho thấy biểu diễn tri thức cho các vị trí không gian khác nhau nên được xem xét riêng biệt. C và P biểu thị chưng cất tri thức theo kênh và theo pixel, tương ứng.

Hiệu suất. Bảng 1 báo cáo hiệu suất của phương pháp chúng tôi trên tập xác thực ADE20K, nơi BPKD được đề xuất đạt được hiệu suất tiên tiến (SOTA) trên nhiều mạng học sinh. Quá trình chưng cất tăng cường Giao điểm trên Hợp trung bình (mIoU) cho các mạng học sinh lên đến 24.33%. Đáng chú ý, BPKD liên tục vượt trội hơn SOTA hiện tại, CWD, với 3.87% trên tất cả các kiến trúc mạng được đánh giá. Kết quả bổ sung trên tập xác thực Pascal Context cho thấy mức tăng hiệu suất trung bình 2% so với các phương pháp SOTA. Các thí nghiệm tiếp theo được trình bày trong Bảng 2 tiết lộ hiệu quả của phương pháp trên các kiến trúc Transformer phổ biến như Swin và DeiT, nơi nó vượt trội hơn CWD lên đến 2.53%. Những phát hiện này nhấn mạnh bản chất không phụ thuộc kiến trúc của phương pháp chúng tôi.

Về hiệu quả tính toán, BPKD đạt được tốc độ chuyển tiếp chưng cất lên đến 29.3%, giảm thời gian huấn luyện 21.2% và 21.9%, và giảm tiêu thụ bộ nhớ (GMem.) 94.8% và 63.4%, so với các phương pháp trước đây như CIRKD và IFVD. Ngoài ra, chúng tôi ghi nhận cải thiện 13% trong chỉ số Trimap so với CWD trên kiến trúc Swin, nhấn mạnh tính hiệu quả về chi phí của phương pháp với hiệu suất SOTA.

4.3. Nghiên cứu Loại bỏ
Trong phần này, chúng tôi đánh giá toàn diện BPKD của chúng tôi dưới các cài đặt khác nhau. Tất cả các thí nghiệm loại bỏ được tiến hành trên bộ dữ liệu Cityscapes với T: PSPNet-R101 và S: PSPNet-R18. Để giảm chi phí tính toán, chúng tôi áp dụng cấu hình huấn luyện hợp lý, bao gồm kích thước cắt giảm xuống 512 ×512, và lịch huấn luyện đến 40k lần lặp. Thêm kết quả thí nghiệm được hiển thị trong phụ lục.

Tính Hiệu quả của Tri thức Tách biệt. Để xác minh tính hiệu quả của phương pháp chưng cất tri thức được đề xuất, chúng tôi đánh giá hiệu suất phân đoạn trong vùng biên trong Bảng 3. Chúng tôi đánh giá chỉ số Trimap mIoU [7] khi sử dụng chuẩn hóa theo kênh và theo pixel cho chưng cất mạng. Chuẩn hóa theo kênh dẫn đến suy giảm cả mIoU tiêu chuẩn 2.16% và Trimap mIoU 3.58%, cho thấy độ nhạy của nó trong các vùng biên. Ngược lại, chưng cất theo pixel tăng cường Trimap mIoU 6.03%, hưởng lợi từ thiết kế mất mát Biên chuyên biệt của chúng tôi tinh chỉnh chất lượng ranh giới [29, 30]. Hàm mất mát Thân cho thấy mức tăng mIoU tiêu chuẩn 5.18% khi sử dụng theo kênh và 3.71% khi theo không gian, nhấn mạnh tính hiệu quả của nó cho các vùng thân. Tuy nhiên, nó có tác động hạn chế đến hiệu suất của Trimap. Tóm lại, phương pháp tối ưu của chúng tôi, BPKD, đạt được điểm mIoU tốt nhất 75.94(+6.95)% và 62.91(+7.59)% trên đánh giá tiêu chuẩn và Trimap, tương ứng. Điều này làm nổi bật khả năng của BPKD trong việc tinh chỉnh các ranh giới ngữ nghĩa và vùng thân thông qua căn chỉnh mức pixel và tổng hợp ngữ cảnh.

Phương pháp mIoU(%) IMP.(%) mAcc(%)
Giáo viên 79.74 - 86.56
ResNet18 68.99 - 75.19
+ PRM 70.37 ▲1.98 76.95
+ POM 71.63 ▲2.64 78.47
+ BEL 74.17 ▲5.18 80.47
+ PRM + BEL 74.81 ▲5.92 81.52
+ POM + BEL 74.62 ▲5.63 80.98
+ PRM + POM + BEL 75.94 ▲6.95 82.62

PRM : Lọc Trước Mặt nạ BEL : Mất mát Thân POM : Lọc Sau Mặt nạ

Bảng 4: Các vị trí khác nhau áp dụng mặt nạ trong phương pháp được đề xuất. (IMP.) tham chiếu đến cải thiện đạt được bởi mạng học sinh.

So sánh Vị trí Bộ lọc Biên. Từ Bảng 4, kết quả số cho thấy tính hiệu quả của phương pháp được đề xuất của chúng tôi. Chúng tôi tiếp tục phân tích tác động của việc áp dụng bộ lọc biên cho các vị trí khác nhau. Áp dụng bộ lọc Trước Mặt nạ, hiệu suất cải thiện nhẹ 1.98% so với học sinh không có chưng cất. Ngược lại, lọc Sau Mặt nạ cải thiện hiệu suất 2.64%, bởi vì POM trích xuất sự khác biệt biên cụ thể cho từng loại lớp. Mô-đun Cải tiến Thân chăm sóc thông tin không phải biên trong cài đặt chưng cất của chúng tôi, và hiệu suất được nâng lên 5.18%. Sau đó, chúng tôi khám phá hiệu suất bằng cách áp dụng PRM hoặc POM cho BEL. Sự kết hợp của ba số hạng đạt được mIoU tốt nhất 75.94% trên tập xác thực Cityscapes.

--- TRANG 8 ---
Độ rộng Biên mIoU IMP. mAcc
3 73.89 ▲4.90 81.24
5 74.26 ▲5.27 82.00
7 75.94 ▲6.95 82.62
10 74.70 ▲5.71 82.13
15 74.36 ▲5.37 82.02

Bảng 5: So sánh hiệu suất cho các độ rộng biên khác nhau với PSPNet-R18 trên tập xác thực Cityscapes, để so sánh công bằng, chúng tôi chạy lại cùng cài đặt 3 lần và đo mIoU trung bình để đánh giá.

5 10 15 20 25
(a) Trọng số Mất mát Thân lb 73:0 73:5 74:0 74:5 75:0 75:5 76:0 76:5 mIoU ( %) trên Cityscapes val

30 40 50 60 70
(b) Trọng số Mất mát Biên le 73:0 73:5 74:0 74:5 75:0 75:5 76:0 76:5

1 2 3 4 5
(c) Trọng số Bên trong Mất mát Biên a 73:0 73:5 74:0 74:5 75:0 75:5 76:0 76:5

Hình 3: Tác động của (a) trọng số Mất mát Thân λb và (b) trọng số Mất mát Biên λe và (c) trọng số Bên trong Mất mát Biên α trên Cityscape val. Chúng tôi tìm thấy sự kết hợp tối ưu bằng nghiên cứu phạm vi rộng rằng λb= 20, λe= 50, α= 2.

Tác động của Các Độ rộng Biên Khác nhau. Độ rộng biên là then chốt trong Mô-đun Phát hiện Biên, ảnh hưởng phức tạp đến cả chất lượng và yêu cầu tính toán của quá trình phát hiện biên. Cụ thể, một đơn vị biên lớn hơn dẫn đến biên rộng hơn, kết hợp nhiều pixel hơn vào mất mát biên. Mặc dù sự gia tăng này có thể tăng cường độ mạnh mẽ của phát hiện biên, nó cũng đưa ra một sự đánh đổi: biên rộng hơn bao gồm pixel từ vùng thân, có thể pha loãng độ chính xác của các đặc trưng cụ thể biên. Chúng tôi phân tích hệ thống tác động của các độ rộng biên khác nhau đến hiệu suất chưng cất, như được tóm tắt trong Bảng 5. Phân tích của chúng tôi tiết lộ rằng độ rộng biên 7 đơn vị là tối ưu, dẫn đến cải thiện hiệu suất đáng kể 6.95%.

Tác động của Siêu tham số. Trong quá trình tối ưu hóa, các tham số λb và λe phục vụ như các yếu tố cân nhắc cho các hàm mất mát thân và biên, tương ứng. Những siêu tham số này là then chốt trong việc cân bằng sự tập trung giữa tinh chỉnh ranh giới ngữ nghĩa và độ chính xác vùng thân tổng thể. Giá trị λe cao hơn đặt nhiều nhấn mạnh hơn vào chất lượng biên, ngược lại, giá trị λb lớn hơn làm cho mô hình chú ý hơn đến các vùng thân lớn hơn và có thể tăng cường mIoU tiêu chuẩn. Mất mát biên kết hợp tham số bên trong α, phục vụ như một yếu tố cân bằng theo lớp. Tham số này đặc biệt có công cụ trong việc giảm thiểu mất cân bằng lớp bằng cách điều chỉnh bên trong đóng góp của mỗi lớp cho mất mát tổng thể. Như được hiển thị trong Hình 3, chúng tôi điều tra tác động của trọng số mất mát trong BPKD của chúng tôi và tìm thấy λb= 20, λe= 50 và α= 2 là lựa chọn tốt nhất.

Phân tích Trực quan hóa CAM. Hình 4 minh họa việc tinh chỉnh rõ ràng các ranh giới ngữ nghĩa trên nhiều đối tượng. Các ràng buộc hình dạng rõ ràng, như sự chú ý mạnh mẽ được dành cho đường viền gối. Mặc dù chưng cất BPKD, mạng học sinh không thể phân đoạn hoàn hảo con ngựa ở hàng thứ hai, nhưng nó đã chú ý cao đến silhouette xương. Điều này cho thấy rằng BPKD đã cố gắng hết mình để chưng cất tri thức cho mạng học sinh, nhưng do kích thước và khả năng hạn chế của nó, học sinh chỉ có thể học khả năng mức bề mặt của giáo viên. Các phần thân đã được liên kết mượt mà với một danh mục duy nhất, giảm biên có độ không chắc chắn cao và kết hợp tri thức hình dạng tiên nghiệm từ áp lực mất mát biên. Thêm kết quả phân đoạn định tính trong phụ lục trực quan chứng minh tính hiệu quả của BPKD của chúng tôi cho cả đối tượng nhỏ và lớn với cải tiến ranh giới rõ ràng.

(a) Hình ảnh (b) Giáo viên (c) Không có chưng cất (d) BPKD(Của chúng tôi)

Hình 4: So sánh trực quan hóa CAM giữa (b) mô hình giáo viên, (c) mô hình học sinh không có chưng cất, và (d) mô hình BPKD. Bản đồ kích hoạt được trích xuất từ khối cuối cùng của các xương sống ResNet tương ứng sử dụng HiResCAM [14]. Kết quả cho thấy rằng BPKD cho thấy sự tinh chỉnh ranh giới vượt trội và sự chú ý cao hơn đến các thân ngữ nghĩa. Để trực quan hóa tốt hơn, nên phóng to.

--- TRANG 9 ---
5. Kết luận
Công trình này trình bày một phương pháp chưng cất tri thức ưu tiên biên giới (BPKD) mới cho phân đoạn ngữ nghĩa, chuyển giao tri thức thân và biên của mô hình giáo viên cồng kềnh đến mô hình học sinh nhỏ gọn, một cách riêng biệt. Các thí nghiệm mở rộng chứng minh rằng biểu diễn tri thức trong các vùng thân và biên nên được xem xét khác nhau. Do các tính chất nội tại khác nhau, vùng biên cần tập trung vào việc phân biệt giữa các lớp không chắc chắn cho mỗi pixel trong khi vùng thân cần tập trung nhiều hơn vào định vị và kết nối các cấu trúc đối tượng. Kết quả thí nghiệm cho thấy rằng phương pháp chưng cất được đề xuất liên tục vượt trội hơn các phương pháp tiên tiến trên các bộ dữ liệu chuẩn công cộng khác nhau. Cả mIoU tổng thể và hiệu suất trong vùng biên đều được cải thiện với biên độ lớn.

6. Lời cảm ơn
Y. Liu thừa nhận sự hỗ trợ của tài trợ khởi động từ Đại học Adelaide cho sự tham gia của họ trong công trình này. Chúng tôi bày tỏ lòng biết ơn đối với Dịch vụ Máy tính Hiệu suất Cao Đại học Adelaide vì đã cung cấp Tài nguyên Máy tính GPU, và đối với ông Wang Hui và Tiến sĩ Fabien Voisin vì sự hỗ trợ kỹ thuật có giá trị của họ. Chúng tôi muốn cảm ơn ông Hanwen Wang vì đã cung cấp bộ sưu tập trực quan hóa được sử dụng trong phụ lục.

Tài liệu tham khảo
[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. Một đánh giá về định lượng độ không chắc chắn trong học sâu: Kỹ thuật, ứng dụng và thách thức. Information Fusion, 76:243–297, 2021. 2

[2] John Canny. Một phương pháp tính toán để phát hiện biên. IEEE Transactions on pattern analysis and machine intelligence, (6):679–698, 1986. 3

[3] Weihan Cao, Yifan Zhang, Jianfei Gao, Anda Cheng, Ke Cheng, và Jian Cheng. Pkd: Khung chưng cất tổng quát cho các bộ phát hiện đối tượng thông qua hệ số tương quan Pearson. arXiv preprint arXiv:2207.02039, 2022. 2, 3

[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, và Alan L Yuille. Phân đoạn hình ảnh ngữ nghĩa với mạng tích chập sâu và crf kết nối đầy đủ. arXiv preprint arXiv:1412.7062, 2014. 2

[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, và Alan L Yuille. Deeplab: Phân đoạn hình ảnh ngữ nghĩa với mạng tích chập sâu, tích chập atrous, và crf kết nối đầy đủ. IEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017. 2

[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, và Hartwig Adam. Suy nghĩ lại tích chập atrous cho phân đoạn hình ảnh ngữ nghĩa. arXiv preprint arXiv:1706.05587, 2017. 2, 6

[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, và Hartwig Adam. Bộ mã hóa-giải mã với tích chập tách biệt atrous cho phân đoạn hình ảnh ngữ nghĩa. Trong Proceedings of the European conference on computer vision (ECCV), trang 801–818, 2018. 2, 5, 7

[8] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, và Yu Qiao. Bộ điều hợp transformer thị giác cho dự đoán dày đặc. arXiv preprint arXiv:2205.08534, 2022. 6

[9] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, và Rohit Girdhar. Transformer mặt nạ chú ý mặt nạ cho phân đoạn hình ảnh toàn cầu. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 1290–1299, 2022. 1

[10] Bowen Cheng, Alex Schwing, và Alexander Kirillov. Phân loại theo pixel không phải tất cả những gì bạn cần cho phân đoạn ngữ nghĩa. Advances in Neural Information Processing Systems, 34:17864–17875, 2021. 1

[11] MMSegmentation Contributors. MMSegmentation: Hộp công cụ phân đoạn ngữ nghĩa openmmlab và chuẩn. https://github.com/open-mmlab/mmsegmentation, 2020. 5

[12] MMRazor Contributors. Hộp công cụ nén mô hình openmmlab và chuẩn. https://github.com/open-mmlab/mmrazor, 2021. 5

[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, và Bernt Schiele. Bộ dữ liệu cityscapes để hiểu cảnh đô thị ngữ nghĩa. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 3213–3223, 2016. 2, 5, 6, 13, 16

[14] Rachel Lea Draelos và Lawrence Carin. Hirescam: Biểu diễn vị trí trung thực trong sự chú ý thị giác cho phân loại hình ảnh y tế 3d có thể giải thích được. arXiv preprint arXiv:2011.08891, 2020. 8

[15] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, và Andrew Zisserman. Thử thách các lớp đối tượng thị giác pascal (voc). International journal of computer vision, 88(2):303–338, 2010. 2, 5, 6, 13

[16] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, và Xiaolin Wei. Suy nghĩ lại bisenet cho phân đoạn ngữ nghĩa thời gian thực. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 9716–9725, 2021. 2

[17] Jianping Gou, Baosheng Yu, Stephen J Maybank, và Dacheng Tao. Chưng cất tri thức: Một khảo sát. International Journal of Computer Vision, 129(6):1789–1819, 2021. 3

[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học biểu diễn dư sâu cho nhận dạng hình ảnh. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016. 1

[19] Byeongho Heo, Minsik Lee, Sangdoo Yun, và Jin Young Choi. Chuyển giao tri thức thông qua chưng cất các ranh giới kích hoạt được hình thành bởi các neuron ẩn. Trong Proceedings of the AAAI Conference on Artificial Intelligence, tập 33, trang 3779–3787, 2019. 2, 3

[20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Chưng cất tri thức trong một mạng neural. 2015. 2, 3

[21] Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, và Chen Change Loy. Chưng cất ái lực liên vùng cho phân đoạn đánh dấu đường. Trong CVPR, trang 12486–12495, 2020. 6, 12

[22] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Tìm kiếm mobilenetv3. Trong Proceedings of the IEEE/CVF international conference on computer vision, trang 1314–1324, 2019. 1, 3

[23] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, và Hartwig Adam. Mobilenets: Mạng tích chập hiệu quả cho ứng dụng thị giác di động. arXiv preprint arXiv:1704.04861, 2017. 3

[24] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, và Jingdong Wang. Tự chú ý thưa thớt đan xen cho phân đoạn ngữ nghĩa. arXiv preprint arXiv:1907.12273, 2019. 6

[25] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, và Kurt Keutzer. Squeezenet: Độ chính xác mức alexnet với ít hơn 50x tham số và< 0.5 mb kích thước mô hình. arXiv preprint arXiv:1602.07360, 2016. 3

[26] Deyi Ji, Haoran Wang, Mingyuan Tao, Jianqiang Huang, Xian-Sheng Hua, và Hongtao Lu. Chưng cất tri thức kết cấu và thống kê cho phân đoạn ngữ nghĩa. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 16876–16885, 2022. 2

[27] Nick Kanopoulos, Nagesh Vasanthavada, và Robert L Baker. Thiết kế bộ lọc phát hiện biên hình ảnh sử dụng toán tử sobel. IEEE Journal of solid-state circuits, 23(2):358–367, 1988. 3

[28] Jangho Kim, SeongUk Park, và Nojun Kwak. Diễn giải mạng phức tạp: Nén mạng thông qua chuyển giao yếu tố. Advances in neural information processing systems, 31, 2018. 2, 3

[29] Pushmeet Kohli, Philip HS Torr, et al. Thế năng bậc cao mạnh mẽ để thực thi tính nhất quán nhãn. International Journal of Computer Vision, 82(3):302–324, 2009. 7

[30] Philipp Krähenbühl và Vladlen Koltun. Suy luận hiệu quả trong crf kết nối đầy đủ với thế năng biên Gaussian. Advances in neural information processing systems, 24, 2011. 7

[31] Seung Hyun Lee, Dae Ha Kim, và Byung Cheol Song. Chưng cất tri thức tự giám sát sử dụng phân tách giá trị đơn. Trong Proceedings of the European Conference on Computer Vision (ECCV), trang 335–350, 2018. 2, 3

[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. Microsoft coco: Các đối tượng chung trong ngữ cảnh. Trong European conference on computer vision, trang 740–755. Springer, 2014. 14

[33] Akide Liu và Zihan Wang. Cv 3315 là tất cả những gì bạn cần: Cuộc thi phân đoạn ngữ nghĩa. arXiv preprint arXiv:2206.12571, 2022. 2

[34] Xiaofeng Liu, Fangxu Xing, Georges El Fakhri, và Jonghye Woo. Thích ứng đường viền ngữ nghĩa tự cho phân đoạn khối u não liên phương thức. Trong IEEE International Symposium on Biomedical Imaging, trang 1–5. IEEE, 2022. 2

[35] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, và Jingdong Wang. Chưng cất tri thức có cấu trúc cho phân đoạn ngữ nghĩa. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 2604–2613, 2019. 2, 3, 5, 6, 7, 12

[36] Yifan Liu, Changyong Shu, Jingdong Wang, và Chunhua Shen. Chưng cất tri thức có cấu trúc cho dự đoán dày đặc. IEEE transactions on pattern analysis and machine intelligence, 2020. 3

[37] Yueming Liu, Xiaomei Yang, Zhihua Wang, Chen Lu, Zhi Li, và Fengshuo Yang. Trích xuất khu vực nuôi trồng thủy sản và đánh giá tính dễ bị tổn thương trong sanduao dựa trên mô hình mạng đặc trưng tích chập phong phú hơn. Journal of Oceanology and Limnology, 37(6):1941–1954, 2019. 3

[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin transformer: Transformer thị giác phân cấp sử dụng cửa sổ dịch chuyển. Trong Proceedings of the IEEE/CVF international conference on computer vision, trang 10012–10022, 2021. 6

[39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, và Saining Xie. Một convnet cho những năm 2020. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 11976–11986, 2022. 1

[40] Jonathan Long, Evan Shelhamer, và Trevor Darrell. Mạng tích chập hoàn toàn cho phân đoạn ngữ nghĩa. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 3431–3440, 2015. 2

[41] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, và Jian Sun. Shufflenet v2: Hướng dẫn thực tế cho thiết kế kiến trúc cnn hiệu quả. Trong Proceedings of the European conference on computer vision (ECCV), trang 116–131, 2018. 1

[42] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Shapiro, và Hannaneh Hajishirzi. Espnet: Kim tự tháp không gian hiệu quả của các tích chập pha loãng cho phân đoạn ngữ nghĩa. Trong Proceedings of the european conference on computer vision (ECCV), trang 552–568, 2018. 3

[43] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, và Alan Yuille. Vai trò của ngữ cảnh để phát hiện đối tượng và phân đoạn ngữ nghĩa trong tự nhiên. Trong IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014. 17

[44] Shivam K Panda, Yongkyu Lee, và M Khalid Jawed. Agronav: Khung điều hướng tự động cho robot và xe nông nghiệp sử dụng phân đoạn ngữ nghĩa và phát hiện đường ngữ nghĩa. Trong IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 6271–6280, 2023. 2

[45] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, và Eugenio Culurciello. Enet: Một kiến trúc mạng neural sâu cho phân đoạn ngữ nghĩa thời gian thực. arXiv preprint arXiv:1606.02147, 2016. 3

[46] Judith MS Prewitt et al. Cải tiến và trích xuất đối tượng. Picture processing and Psychopictorics, 10(1):15–19, 1970. 3

[47] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, và Yoshua Bengio. Fitnets: Gợi ý cho mạng sâu mỏng. arXiv preprint arXiv:1412.6550, 2014. 3

[48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. Mobilenetv2: Dư nghịch đảo và cổ chai tuyến tính. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4510–4520, 2018. 3, 6

[49] Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, và Chunhua Shen. Chưng cất tri thức theo kênh cho dự đoán dày đặc. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 5311–5320, 2021. 2, 3, 5, 6, 7, 12, 15, 16, 17

[50] Mingxing Tan và Quoc Le. Efficientnet: Suy nghĩ lại tỷ lệ mô hình cho mạng tích chập. Trong International conference on machine learning, trang 6105–6114. PMLR, 2019. 1

[51] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Huấn luyện transformer hình ảnh hiệu quả dữ liệu & chưng cất thông qua chú ý. Trong International conference on machine learning, trang 10347–10357. PMLR, 2021. 6

[52] Jue Wang, Michael F Cohen, et al. Matting hình ảnh và video: một khảo sát. Foundations and Trends® in Computer Graphics and Vision, 3(2):97–175, 2008. 4

[53] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Học biểu diễn độ phân giải cao sâu để nhận dạng thị giác. IEEE transactions on pattern analysis and machine intelligence, 43(10):3349–3364, 2020. 1, 3, 6

[54] Xiaolong Wang, Ross Girshick, Abhinav Gupta, và Kaiming He. Mạng neural không cục bộ. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 7794–7803, 2018. 3

[55] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, và Chunhua Shen. Solov2: Phân đoạn thể hiện động và nhanh. Advances in Neural information processing systems, 33:17721–17732, 2020. 14

[56] Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, và Yongchao Xu. Chưng cất biến thiên đặc trưng trong lớp cho phân đoạn ngữ nghĩa. Trong European Conference on Computer Vision, trang 346–362. Springer, 2020. 2, 3, 6, 12

[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, và Jian Sun. Phân tích cảm nhận thống nhất để hiểu cảnh. Trong Proceedings of the European conference on computer vision (ECCV), trang 418–434, 2018. 6

[58] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, và Ping Luo. Segformer: Thiết kế đơn giản và hiệu quả cho phân đoạn ngữ nghĩa với transformer. Advances in Neural Information Processing Systems, 34:12077–12090, 2021. 1

[59] Saining Xie và Zhuowen Tu. Phát hiện biên lồng tổng thể. Trong Proceedings of the IEEE international conference on computer vision, trang 1395–1403, 2015. 3

[60] Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, và Qian Zhang. Chưng cất tri thức mối quan hệ liên hình ảnh cho phân đoạn ngữ nghĩa. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 12319–12328, 2022. 2, 3, 6, 12

[61] Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, và Chun Yuan. Chưng cất tạo sinh có mặt nạ. Trong Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XI, trang 53–69. Springer, 2022. 2, 3

[62] Junho Yim, Donggyu Joo, Jihoon Bae, và Junmo Kim. Món quà từ chưng cất tri thức: Tối ưu hóa nhanh, tối thiểu hóa mạng và học chuyển giao. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 4133–4141, 2017. 2, 3

[63] Zhiding Yu, Chen Feng, Ming-Yu Liu, và Srikumar Ramalingam. Casenet: Phát hiện biên ngữ nghĩa nhận thức danh mục sâu. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 5964–5973, 2017. 3

[64] Yuhui Yuan, Xiaokang Chen, Xilin Chen, và Jingdong Wang. Transformer phân đoạn: Biểu diễn ngữ cảnh đối tượng cho phân đoạn ngữ nghĩa. arXiv preprint arXiv:1909.11065, 2019. 1

[65] Sergey Zagoruyko và Nikos Komodakis. Chú ý nhiều hơn đến sự chú ý: Cải thiện hiệu suất của mạng tích chập thông qua chuyển giao chú ý. arXiv preprint arXiv:1612.03928, 2016. 2, 3

[66] Bowen Zhang, Liyang Liu, Minh Hieu Phan, Zhi Tian, Chunhua Shen, và Yifan Liu. Segvitv2: Khám phá phân đoạn ngữ nghĩa hiệu quả và liên tục với transformer thị giác đơn giản. IJCV, 2023. 1

[67] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, và Yifan Liu. Segvit: Phân đoạn ngữ nghĩa với transformer thị giác đơn giản. arXiv preprint arXiv:2210.05844, 2022. 1, 2

[68] Chenrui Zhang và Yuxin Peng. Tốt hơn và nhanh hơn: chuyển giao tri thức từ nhiều nhiệm vụ học tự giám sát thông qua chưng cất đồ thị cho phân loại video. arXiv preprint arXiv:1804.10069, 2018. 2, 3

[69] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Mạng chú ý chia tách. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 2736–2746, 2022. 1

[70] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, và Jiaya Jia. Mạng phân tích cảnh kim tự tháp. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 2881–2890, 2017. 2, 5, 6, 12, 15, 16, 17

[71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, và Antonio Torralba. Phân tích cảnh thông qua bộ dữ liệu ade20k. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 633–641, 2017. 2, 5, 6, 13, 15

--- TRANG 12 ---
Trong tài liệu bổ sung, chúng tôi bổ sung các lập luận cốt lõi của bài báo chính với các phân tích bổ sung, chi tiết thí nghiệm mở rộng, và trực quan hóa nâng cao. Ban đầu, chúng tôi tiến hành so sánh hiệu suất sử dụng lịch trình giảm trên ba bộ dữ liệu duy nhất, được chọn cụ thể cho các nghiên cứu loại bỏ. Những kết quả so sánh này được tóm gọn trong Bảng 7. Để tạo điều kiện tái tạo, chúng tôi cung cấp hướng dẫn triển khai chi tiết, cùng với mã tương thích PyTorch cho Mất mát Biên chuyên biệt của chúng tôi. Những hiểu biết phân tích bổ sung về IoU trimap theo lớp có sẵn trong Hình 6 và Bảng 8. Cuối cùng, do hạn chế không gian trong bản thảo chính, chúng tôi bao gồm cả thí nghiệm trực quan và định lượng trên ba bộ dữ liệu riêng biệt trong phần bổ sung này.

A. Hiệu suất Trên Lịch trình Giảm
Để chứng minh hiệu quả của việc tận dụng nhãn mềm từ giáo viên để tăng tốc độ hội tụ, chúng tôi tiến hành thí nghiệm so sánh với lịch trình giảm, huấn luyện 40k lần lặp, và độ phân giải 512×512. Kết quả của thí nghiệm này, cũng như so sánh với các thuật toán tiên tiến khác, được báo cáo trong Bảng 7. Để đảm bảo so sánh công bằng, khung chưng cất tri thức được đề xuất của chúng tôi được áp dụng cho các giáo viên khác nhau. Học sinh của chúng tôi có thể học tri thức từ mạng giáo viên, dẫn đến lợi ích hiệu suất đáng kể và đạt được kết quả tiên tiến trên nhiều bộ dữ liệu. Như được chứng minh trong Bảng 7, trên ADE20K phương pháp của chúng tôi vượt trội hơn chưng cất theo kênh cơ sở mạnh mẽ với 2.34%, 1.46%, 1.66%, 1.57%, và 1.59%, tương ứng. Hơn nữa, các phương pháp của chúng tôi có thể tăng cường mạng học sinh nhẹ mà không tăng khả năng tính toán, cải thiện hiệu suất với 6.45%, 4.13%, 6.08%, và 8.49% so với học sinh thô. Trên Cityscape phương pháp của chúng tôi vượt trội hơn chưng cất theo kênh cơ sở mạnh mẽ với 2.22%, 1.35%, 4.40%, 1.52%, tương ứng. Hơn nữa, các phương pháp của chúng tôi có thể tăng cường mạng học sinh nhẹ mà không tăng khả năng tính toán, cải thiện hiệu suất với 10.07%, 3.78%, 8.64%, và 1.75% so với học sinh thô. Trên Pascal Context phương pháp của chúng tôi vượt trội hơn chưng cất theo kênh cơ sở mạnh mẽ với 2.34%, 1.46%, 1.66%, 1.57%, và 1.59%, tương ứng. Hơn nữa, các phương pháp của chúng tôi có thể tăng cường mạng học sinh nhẹ mà không tăng khả năng tính toán, cải thiện hiệu suất với 10.07%, 3.78%, 6.08%, và 8.49% so với học sinh thô. Những kết quả số này gợi ý rằng phương pháp của chúng tôi không phụ thuộc vào cấu trúc mô hình cụ thể, và nó tạo ra lợi ích hiệu suất đáng kể với mạng học sinh thuần túy, ngay cả khi không có Pre-train ImageNet như được hiển thị trong bảng 6. Để chứng minh thêm tính hiệu quả của phương pháp chúng tôi, kết quả phân đoạn định tính được trực quan hóa trong Hình 9.

Hình 5: BPKD (trái) tách biệt thông tin biên và thân từ hình ảnh đầu vào và tạo hai đường ống chưng cất song song so với các công trình tiên phong, như CWD [49], SKDS [35], IFVD [56], CIRKD [60]. Mất mát tổ hợp được đề xuất buộc học sinh học từng phần riêng biệt. Kết quả cho thấy lợi ích hiệu suất rõ ràng.

Bảng 6: So sánh Hiệu suất với các phương pháp chưng cất tiên tiến trên bộ dữ liệu ADE20K, xương sống học sinh không được huấn luyện trước trên ImageNet.

Phương pháp mIoU mAcc(%)
T:PSPNet-R101 [70] 44.39 54.74
S:PSPnet-R18 [70] 17.11 22.99
SKDS [35] 20.79 27.74
IFVD [21] 20.75 27.6
CIRKD [60] 22.90 30.68
CWD [49] 24.79 31.44
BPKD(Của chúng tôi) 27.46 36.10

B. Chi tiết Triển khai của Mất mát Biên
Phần này trình bày chi tiết triển khai của Mất mát Biên trong PyTorch, nhằm tạo điều kiện tái tạo. Triển khai của chúng tôi bao gồm các thành phần được chú thích rõ ràng được kết hợp vào hệ thống chưng cất của chúng tôi. Để bắt đầu, phép toán Bộ lọc Trước Mặt nạ được áp dụng cho các logit thu được từ cả giáo viên và học sinh. Sau đó, một sự sắp xếp lại chiều được thực hiện để tối ưu hóa quá trình. Divergence KL phục vụ như thành phần cốt lõi để ước tính khoảng cách giữa các phân phối xác suất học sinh và giáo viên, thiết lập tham chiếu phôi thai để tính toán mất mát. Bộ lọc Sau Mặt nạ lấy đầu vào từ tiêu chí không giảm và tổng hợp khoảng cách trên chiều kênh. Nó được theo sau bởi một mở rộng không gian bổ sung lặp lại thông tin không gian dựa trên các kênh đã cho. Cuối cùng, vector trọng số bên trong được áp dụng cho các danh mục tương ứng để tăng cường khả năng học cho các mẫu biên khó. Các số hạng mất mát EDM sau đó được hoàn thiện bằng thích ứng trọng số tổng thể và giảm trung bình. Hơn nữa, Mô-đun Phát hiện Biên là một thành phần phụ quan trọng khác sử dụng mặt nạ biên đa cấp bằng cách cung cấp đầu vào và nhãn sự thật cơ bản. Chúng tôi sử dụng giãn nở và xói mòn để lấy các biên, với siêu tham số, kích thước kernel, kiểm soát độ rộng biên.

Để giải quyết áp lực tính toán phát sinh từ kernel lớn, compute iters được giới thiệu để tối ưu hóa bộ nhớ GPU. Ngoài ra, mô-đun phát hiện biên sử dụng gộp trung bình như chính sách lấy mẫu xuống, xem xét tầm quan trọng giảm dần từ ranh giới bên trong đến đường viền.

C. Hiệu suất trimap Phân loại
Phần này trình bày đánh giá Edge IoU phân loại sử dụng kiến trúc bộ mã hóa PSPnet và xương sống Resnet18 trên tập xác thực Cityscape. Hình 6 và Bảng 8 minh họa kết quả. Các phương pháp được đề xuất của chúng tôi chứng minh cải thiện đáng kể trong hầu hết các lớp dựa trên các yếu tố biên so với mô hình học sinh thô và phương pháp chưng cất theo kênh cơ sở mạnh mẽ. Chúng tôi cũng quan sát thấy cải thiện rõ ràng cho các danh mục khó phân biệt bằng ranh giới. Ví dụ, chúng tôi đạt được cải thiện 10.31% cho danh mục tường và cải thiện 6.69% cho danh mục xe buýt.

D. Phân đoạn thể hiện
Chúng tôi tiến hành thí nghiệm sử dụng SOLOv2 trên bộ dữ liệu COCO để chứng minh khả năng thích ứng tổng quát của phương pháp chúng tôi. Cụ thể, chúng tôi chọn SOLOV2 [55] X-101(DCN) làm giáo viên và Light SOLOV2 [55] R-18 làm học sinh. Bảng 9 trình bày kết quả, cho thấy phương pháp của chúng tôi cải thiện học sinh thô với 6.5%, 9.5%, 7.5%, 6.9%, 8.8%, và 4.8% trên các chỉ số tương ứng. Kết quả chứng minh rằng phương pháp chưng cất của chúng tôi có thể dễ dàng thích ứng với các nhiệm vụ phân đoạn thể hiện và vượt trội hơn các phương pháp trước đây trong cài đặt tập huấn luyện quy mô nhỏ. Mặc dù các nhiệm vụ phân đoạn thể hiện và phân đoạn ngữ nghĩa có sự khác biệt riêng biệt, chúng chia sẻ các tính chất tương tự ở chỗ chúng dự đoán mặt nạ cho các giác quan mục tiêu và cho các chú thích mức pixel. Trong các thí nghiệm, chúng tôi áp dụng các phương pháp chưng cất tri thức nhiều lần trên các logit phân loại kim tự tháp và biểu diễn có mặt nạ, tiếp theo là tính toán trung bình trên tất cả các cấp độ của các số hạng phụ để có được mất mát chưng cất. Từ kết quả số, các chỉ số AP tăng rõ ràng cho các đối tượng nhỏ và trung bình. Tuy nhiên, không có cải thiện hiệu suất cho các đối tượng lớn, cho thấy phương pháp của chúng tôi có chỗ để cải thiện trong các nhiệm vụ phân đoạn thể hiện. Như một triển vọng tương lai, chúng tôi nhằm điều chỉnh phương pháp hiện tại và thiết kế một số hạng mất mát chuyên biệt cho chưng cất tri thức mức thể hiện.

Phương pháp AP AP 50 AP75 APS APM APL
T:SOLOv2-X101 41.7 63.2 45.1 18.0 45.0 61.6
S:SOLOv2-R18* 26.7 44.1 27.5 6.50 27.2 45.8
CWD 28.4 47.1 29.6 9.90 30.3 44.2
BPKD(Của chúng tôi) 33.2 53.6 35.0 13.4 36.0 50.6

Bảng 9: Kết quả phân đoạn thể hiện được trình bày trong báo cáo này được thu được trên tập xác thực COCO [32] sử dụng kết quả mô hình đơn. Tất cả các phương pháp chưng cất và cơ sở mạng học sinh được huấn luyện sử dụng lịch trình 1x với huấn luyện đa tỷ lệ bị vô hiệu hóa. Bảng dưới đây chứng minh rằng phương pháp chưng cất của chúng tôi có thể dễ dàng thích ứng với các nhiệm vụ phân đoạn thể hiện và vượt trội hơn các phương pháp trước đây trong cài đặt huấn luyện quy mô nhỏ.

--- TRANG 15 ---
(a)Hình ảnh (b)Không có chưng cất (c)CWD (d)BPKD(Của chúng tôi) (e)Giáo viên

Hình 7: Kết quả định tính trên tập xác thực ADE20K [71] được tạo bởi PSPNet [70] và kiến trúc mạng ResNet18: (a) Hình ảnh ban đầu, (b) sơ đồ không có chưng cất, (c) phương pháp tiên tiến chưng cất theo kênh [49], (d) BPKD phương pháp của chúng tôi, (e) giáo viên. Hình này cho thấy rằng các phương pháp của chúng tôi phân đoạn các đối tượng phức tạp nhỏ với ranh giới rõ ràng. Phóng to để xem tốt hơn. Trong hàng thứ hai, BPKD chứng minh kết quả phân đoạn vượt trội; ví dụ, đường viền của chiếc ghế rõ ràng hơn và độ chính xác tổng thể được tăng cường. Trong hàng thứ ba, thứ tư, và cuối cùng, các đường viền của những chiếc gối được vạch ra rõ ràng, tiến gần hơn đến kết quả được tạo bởi Giáo viên. Trong hàng thứ bảy, bàn cà phê nằm giữa hai chiếc ghế được thể hiện với độ rõ nét lớn hơn, từ đó gợi ý rằng phương pháp của chúng tôi đạt được hiệu suất đáng khen ngợi trong việc giải quyết sự mơ hồ giữa nhiều đối tượng gần kề. Những kết quả trực quan này cho thấy rằng phương pháp của chúng tôi không chỉ cải thiện ranh giới ngữ nghĩa mà còn tận dụng tri thức tiên nghiệm về đường viền và hình dạng để tạo ra kết quả phân đoạn xuất sắc.

--- TRANG 16 ---
(a)Hình ảnh (b)Không có chưng cất (c)CWD (d)BPKD(Của chúng tôi) (e)Sự thật cơ bản

Hình 8: Kết quả định tính trên tập xác thực Cityscapes [13] được tạo bởi PSPNet [70] và kiến trúc mạng ResNet18: (a) Hình ảnh ban đầu, (b) không có chưng cất, (c) phương pháp tiên tiến chưng cất theo kênh [49], (d) BPKD phương pháp của chúng tôi, (e) Sự thật cơ bản. Hình này cho thấy rằng các phương pháp của chúng tôi phân đoạn các đối tượng phức tạp nhỏ với ranh giới rõ ràng. Phóng to để xem tốt hơn. Trong hàng đầu tiên, BPKD hiệu quả giải quyết vấn đề kính chắn gió phía trước của xe buýt bị phân đoạn sai thành nhiều lớp. Trong hàng thứ hai, phương pháp tăng cường đường viền của các phần trên và dưới của biển báo đường. Hàng thứ ba trình bày một kết quả đặc biệt nổi bật trong phân đoạn của một đoàn tàu; thông qua giám sát và chưng cất khác biệt của các biên và thân chính, đoàn tàu được tách biệt rõ ràng khỏi các chướng ngại vật. Trong hàng thứ tư, những người đi bộ xa xôi đưa ra một trường hợp thách thức cho phân đoạn và nhận dạng do góc máy quay xa và che khuất đáng kể; mặc dù những phức tạp này, BPKD vẫn quản lý để tạo ra các biên ổn định xung quanh hình người. Trong những hình ảnh còn lại, BPKD liên tục cho thấy những cải thiện, đạt được kết quả phân đoạn đáng khen ngợi trên cả lối đi bộ và đường bộ.

--- TRANG 17 ---
(a)Hình ảnh (b)Không có chưng cất (c)CWD (d)BPKD(Của chúng tôi) (e)Giáo viên

Hình 9: Kết quả định tính trên tập xác thực Pascal-Context [43] được tạo bởi PSPNet [70] và kiến trúc mạng ResNet18: (a) Hình ảnh ban đầu, (b) sơ đồ không có chưng cất, (c) phương pháp tiên tiến chưng cất theo kênh [49], (d) BPKD phương pháp của chúng tôi, (e) giáo viên. Hình này cho thấy rằng các phương pháp của chúng tôi phân đoạn các đối tượng phức tạp nhỏ với ranh giới rõ ràng. Phóng to để xem tốt hơn. Trong hàng đầu tiên, BPKD thể hiện phân đoạn biên chi tiết hơn so với mạng học sinh không chưng cất. Trong hàng thứ hai, cả mạng học sinh và CWD đều không quản lý để đạt được phân đoạn hiệu quả của chiếc cốc, do các yếu tố góc độ phức tạp và môi trường. Tuy nhiên, phương pháp của chúng tôi hoàn thành phân đoạn chính xác bằng cách tận dụng tri thức tiên nghiệm về ranh giới của chiếc cốc và học có giám sát trên thân chính của chiếc cốc. Trong những ví dụ còn lại, BPKD chứng minh kết quả phân đoạn phức tạp và tiên tiến hơn, từ đó tiết lộ rằng dưới ảnh hưởng của mất mát biên, không gian hội tụ của toàn bộ mạng hưởng lợi từ các tiền đề hình dạng và không gian trên các lớp khác nhau. Điều này, đến lượt mình, ngầm định cấp giám sát thân chính tốt hơn và hỗ trợ, cuối cùng đạt được kết quả chưng cất tiên tiến (SOTA) trong phân đoạn.
