# 2306.08075.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/semantic-segmentation/2306.08075.pdf
# File size: 25155884 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
BPKD: Boundary Privileged Knowledge Distillation
For Semantic Segmentation
Liyang Liu1Zihan Wang2 3Minh Hieu Phan1Bowen Zhang1Jinchao Ge1Yifan Liu1*
1The University of Adelaide, Australia2The University of Queensland, Australia
3CSIRO’s Data61, Australia
Abstract
Current knowledge distillation approaches in semantic
segmentation tend to adopt a holistic approach that treats
all spatial locations equally. However, for dense predic-
tion, students’ predictions on edge regions are highly un-
certain due to contextual information leakage, requiring
higher spatial sensitivity knowledge than the body regions.
To address this challenge, this paper proposes a novel
approach called boundary-privileged knowledge distilla-
tion (BPKD). BPKD distills the knowledge of the teacher
model’s body and edges separately to the compact student
model. Specifically, we employ two distinct loss functions:
(i) edge loss, which aims to distinguish between ambigu-
ous classes at the pixel level in edge regions; (ii) body loss,
which utilizes shape constraints and selectively attends to
the inner-semantic regions. Our experiments demonstrate
that the proposed BPKD method provides extensive refine-
ments and aggregation for edge and body regions. Ad-
ditionally, the method achieves state-of-the-art distillation
performance for semantic segmentation on three popular
benchmark datasets, highlighting its effectiveness and gen-
eralization ability. BPKD shows consistent improvements
across a diverse array of lightweight segmentation struc-
tures, including both CNNs and transformers, underscoring
its architecture-agnostic adaptability. The code is available
athttps://github.com/AkideLiu/BPKD .
1. Introduction
Semantic segmentation is a complex computer vision
task that involves assigning unique categories to each pixel
of an input image. In recent years, deep learning models
with large numbers of parameters have achieved remarkable
performance in semantic segmentation [9, 10, 18, 39, 66, 67,
69]. However, such models are impractical for resource-
constrained devices like mobile devices and robotics due to
their high computational complexity [53,58,64]. To address
this issue, lightweight base models such as MobileNet [22],
ShuffleNet [41], and EfficientNet [50] have been used for
*Corresponding author, Email: yifan.liu04@adelaide.edu.au.
A)W/o Distillation
BPKD(Ours)Original Image
CWD
B)(a) Original Image
A)W/o Distillation
BPKD(Ours)Original Image
CWD
B) (b) Student w/o KD
A)W/o Distillation
BPKD(Ours)Original Image
CWD
B)
(c) CWD
A)W/o Distillation
BPKD(Ours)Original Image
CWD
B) (d)BPKD(Ours)
First-layerFeatureMapEntropy(mean)EdgePixelAccuracy(%)RawSKDS[34]CIRKD[57]CWD[47]BPKD(Ours)49.248.012131415161718
46485052545658
50.252.956.7First-layerFeatureMapEntropy(mean)EdgePixelAccuracyTeacherEntropy
(e)
Figure 1: Illustration of contextual information leakage.
Above) The uncertainty maps are generated by computing
the mean entropy from first-layer feature maps, employ-
ing different distillation strategies such as the raw student,
CWD, and BPKD. Brighter colors indicate higher certainty.
Below) Demonstrates the inverse correlation between mean
entropy and pixel-level accuracy along edges. When learn-
ing edge pixels, the model aggregates contextual informa-
tion from adjacent class categories. Previous whole-view
distillation methods are affected by contextual information
leakage, leading to high uncertainty in edge low-level fea-
tures and low edge pixel accuracy.
1arXiv:2306.08075v2  [cs.CV]  1 Sep 2023

--- PAGE 2 ---
real-time semantic segmentation.
Designing compression and acceleration techniques for
compact networks is challenging but crucial. Knowl-
edge distillation approaches, such as those introduced in
[19,20,28,65], train a smaller student network to mimic the
complex teacher network by minimizing the soft probabili-
ties distance, typically measured by Kullback–Leibler (KL)
divergence, between the student and teacher. In [31,62,68],
authors have attempted to distill hidden knowledge by uti-
lizing network and data relations, with a focus on classifi-
cation tasks, achieving impressive results.
Pioneering knowledge distillation methods for semantic
segmentation [35, 49, 56, 60] focus more on capturing the
correlational information among pixels, channels, and im-
ages. Liu et al. [35] suggest that hidden knowledge in se-
mantic segmentation is constructed by structured represen-
tation. Structured knowledge is more suitable for pair-wise
similarity reduction and holistic distillation. IFVD [56] pro-
posed to encode the knowledge according to the semantic
masks. In CWD [49], authors refine distillation by empha-
sizing aligning the most salient region of each channel be-
tween the teacher and student.
In comparison to prior studies, which encompassed a va-
riety of studies including [3, 26, 35, 49, 56, 60, 61], that pre-
dominantly concentrated on transferring knowledge repre-
sentations across the entire image, the importance of dis-
tinct knowledge representations at different spatial loca-
tions has been neglected. When learning edge features, the
model aggregates contextual information between adjacent
class categories, leading to contextual information leakage .
As shown in Fig. 1 b, c, d), current whole-view distilla-
tion methods exhibit high levels of uncertainty, as well as
higher levels of entropy, at edge regions. Following prior
works [1], we quantify the uncertainty by computing a mean
entropy of first-layer features, capturing low-level textual
representations. Fig. 1 e) shows that current methods suf-
fer from higher uncertainties and lower accuracy at edge
pixels, indicating the phenomenon of contextual informa-
tion leakage. The low capacity of compact student networks
further exacerbates this phenomenon, degrading segmenta-
tion details on the boundaries, especially for small object
segmentation. However, delineating object’s boundaries is
mandatory for real-life applications such as localizing road
boundaries for autonomous navigation [44] or segmenting
tumors for treatment planning [34].
To tackle the issue of contextual information learning
in existing methods, we propose a novel approach, termed
Boundary Privileged Knowledge Distillation (BPKD). We
divide the knowledge distillation process into two subsec-
tions: the edge distillation and the body distillation sec-
tions. Our proposed BPKD approach explicitly enhances
the quality of edge regions and object boundaries by de-
coupling knowledge distillation and using teacher soft la-bels. The edge distillation loss involves spatial probability
alignment and aggregation of contextual information to re-
fine the boundaries. Furthermore, boundaries provide prior
knowledge of the shape of an object’s inner regions, and
the body region can exploit this knowledge to eliminate
high-uncertainty boundary samples and smooth the learn-
ing curves. Consequently, we observed that the object cen-
ter received greater attention due to the implicit shape con-
straints, further improving segmentation in the body area.
Through empirical analysis, we have demonstrated that
our proposed approach effectively guides the student net-
work to learn from the teacher network’s knowledge, re-
sulting in improved segmentation performance. We eval-
uate our method over popular architectures on three seg-
mentation benchmark datasets: Cityscapes [13], ADE20K
[71], and Pascal Context [15]. Experimental results indi-
cate that BPKD outperforms other state-of-the-art distil-
lation approaches. Specifically, we reduce the disparity
in performance between the student and teacher networks
and exhibit competitive results in comparison to specialized
real-time segmentation methods [16].
Our main contributions are summarized as follows:
• We show that current distillation methods suffer from
contextual information leakage problems by analysing
low-level feature uncertainty, leading to non-optimal
segmentation performance at boundaries. To the best
of our knowledge, this is the first paper identifying this
critical problem within knowledge distillation litera-
ture for semantic segmentation.
• We propose a novel knowledge distillation method that
separately focuses on distilling information related to
the body and edge of objects. Our specialized edge
loss function significantly enhances the quality of edge
slices, while simultaneously imposing strong shape
constraints on the body regions. This approach effec-
tively minimizes the uncertainty prevents contextual
information leakage in the distillation process and am-
plifies the focus on the inner region.
• Our method achieves state-of-the-art results on three
popular benchmark datasets. We report an increase
in the mean Intersection over Union (mIoU) by up to
4.02% when compared to the previous SOTA CWD.
Additionally, we observe a remarkable enhancement
in prediction quality in both edge and body regions,
further demonstrating our effectiveness.
2. Related Work
Semantic Segmentation. Recent state-of-the-art ap-
proaches in semantic segmentation primarily leverage Fully
Convolutional Networks (FCNs) [33,40,67]. Notable mod-
els like PSPNet [70] and DeepLab series [4–7] employ ad-
vanced techniques such as pyramid pooling modules (PPM)
2

--- PAGE 3 ---
and atrous spatial pyramid pooling (ASPP) to capture multi-
scale contexts. HRNet [53] further innovates with a paral-
lel backbone for high-resolution feature maintenance. De-
spite their performance, these models are computationally
intensive, limiting their applicability in real-time and edge-
device scenarios. Consequently, lightweight models like
ENet [45], SqueezeNet [25], and ESPNet [42] have gained
traction. These models use strategies such as early down-
sampling, filter factorization, and efficient spatial pyra-
mids to reduce computational overhead. MobileNet vari-
ants [22,23,48] are also effective for efficient segmentation.
Edge Detection. Classical edge detection algorithms
like Canny [2], Sobel [27], and Prewitt [46] have been
retrofitted into modern deep learning architectures to
achieve fine-grained segmentation. Deeply-supervised edge
detection methods such as HED [59] and RCF [37] intro-
duce multi-scale edge information directly into the segmen-
tation pipelines. Similarly, models like CASENet [63] have
advanced the state-of-the-art by fusing class-specific edges
into segmentation algorithms, providing a dual benefit of
detailed boundary representation and class differentiation.
In parallel, techniques like edge-attention models [54] in-
corporate edge information by weighting features based on
their boundary importance, enabling finer contour mapping
in semantic segmentation.
Knowledge Distillation. Knowledge distillation (KD)
aims to condense the learnings from one or more ex-
pansive teacher models into a streamlined student model
[17, 20]. Predominantly employed in basic vision tasks,
KD techniques can be taxonomized into response-based,
feature-based, and relation-based paradigms. Response-
based methods, chiefly initiated by Hinton et al. [20], mini-
mize Kullback-Leibler divergence to convey implicit, high-
value knowledge [19, 28, 65]. Feature-based approaches
like FitNet [47] align internal feature activations between
teacher and student, whereas relation-based methods [31,
62, 68] delve into inter-layer or inter-sample relationships.
Nonetheless, traditional KD is largely skewed towards im-
age classification, offering limited utility in pixel-level seg-
mentation tasks.
Recent advancements have seen KD methods tailor-fit
for semantic segmentation. Strategies such as structural
knowledge distillation [35, 36] define segmentation as a
structured prediction task, employing pair-wise similarities
and holistic adversarial enhancements for knowledge trans-
fer. Channel-wise distillation [49] concentrates on salient
channel regions. Additional innovations include intra-class
feature variation distillation [56], which amalgamates pixel-
level and class-wise variation, and Cross-Image Relational
distillation [60], which optimizes global semantic intercon-
nections. Masked Generative Distillation [61] leverages
teacher guidance for feature recovery. Recent work [3] sug-
gests Pearson correlation as a viable KL divergence alterna-tive. Empirical validation corroborates the efficacy of these
specialized KD techniques in boosting semantic segmenta-
tion performance.
In recent years, several techniques for knowledge distil-
lation in semantic segmentation have been proposed, each
focusing on different aspects of the problem. However,
these methods have often neglected the significance of re-
gional feature differences and their relative importance. In
order to address this limitation, our approach involves split-
ting the knowledge distillation pipelines and improving the
performance.
3. Methods
In this section, we first provide an overview of the work-
flow of the Boundary Privileged Knowledge Distillation
(BPKD) framework (Section 3.1), followed by a detailed
description of two key implementation aspects of our ap-
proach. Specifically, Section 3.2 outlines the edge knowl-
edge distillation process, which involves pre-mask filtering
and post-mask filtering. Section 3.3 introduces the distilla-
tion loss for body enhancements.
3.1. BPKD Framework
Existing feature distillation techniques [49, 60] transfer
the whole-view representations from the teacher while over-
looking the effects of noisy edge features on the distilla-
tion process. In this framework, we carefully consider the
sensitivity of edge representations and introduce the novel
boundary-privileged knowledge distillation (BPKD) that
transfers the knowledge in the body and the edge regions
separately. Distilling edge regions individually enhances
the quality of object boundaries explicitly. Furthermore, the
edge distillation loss provides prior shape knowledge for
the object’s inner regions. For instance, given a vehicle’s
boundary constraint, the model can easily determine pixel
categories for its inner region. The body distillation loss
has two key benefits from the prior boundary knowledge:
(1) reducing learning difficulty by mimicking the teacher’s
logit probability distribution since high-uncertainty bound-
aries are removed, and (2) leveraging higher attention on the
object center through implicit shape constraints.
Our approach uses an edge detection technique to gener-
ate edge masks MEfor each class by processing the ground
truth and the segmentation logit map. Let Z ∈RH×W×C
denote a network’s logit map, where Ccorresponds to the
number of channels and H×Wrepresents the spatial res-
olution. The edge masks MEare applied to separate the
logit map Zinto two components: the body component ZB
and the edge component ZE, which adhere to an additive
rule, denoted by Z=ZB+ZE. Our BPKD framework
separately transfers the edge and body knowledge encoded
in these two components to the student. As the edge slices
have less amount of knowledge representations, we intro-
3

--- PAGE 4 ---
(b)Distillation LossDecoderEncoderInput Image
ℒ!"#(𝑌$%"&,𝐺𝑇)
Ζ'(&)*Ζ'(&)+
(a)DecoupleDetect EdgeGTFilterData FlowLossCriterion 
GTEdgeEdge LossBody LossBPKD : Boundary Privileged KnowledgeDistillation Framework 
Student NetInput Image
DecoderEncoderTeacher Net
Ζ!"#$%[𝐶×𝑊×𝐻]GT(C∗𝑊∗𝐻)Post-Mask Filteringℒ!
Pre-Mask FilteringSoftMax(C)KL divergenceSum(C)Repeat(C)Sum(W×H)Sum / Mask PixelsApply 𝛼[C][C×𝑊×𝐻]
Ζ!"#$&EdgeDetectionOne-HotEncodingDilation –ErosionGT Mask (1/0)AvgPool2DEdge Mask [1…0]
(c)EdgeLossΖ,&#"+Ζ,&#"*[1×𝑊×𝐻][C×𝑊×𝐻][C×𝑊×𝐻]
[1×𝑊×𝐻][C×𝑊×𝐻][C×𝑊×𝐻]𝑀!	Figure 2: Illustration of our proposed Boundary Privileged Knowledge Distillation framework and architecture. (a) demon-
strates the decoupling process that involves the edge detection on the ground truth to generate GTEdge Mask, followed by
applying the mask filter to obtain the Teacher and Student logits masks. This step ensures that the information from the
boundary region is isolated and appropriately conveyed to the Student. (b) shows that distillation comprises two terms: body
loss and edge loss. The body loss term captures the categorized similarities, whereas the edge loss term concentrates on the
boundary regions’ transfer. ZEandZBis short terms for ZEdge andZBody . InZS,T,SandTstandard for student and
teacher, respectively. (c) shows edge loss calculation is performed in two stages: pre-mask filtering and post-mask filtering.
The pre-mask filtering step shapes the probability distribution to contain only edge information. Subsequently, the post-mask
filtering step aggregates contextual information between adjacent categories to produce the final edge loss.
duce a categorized awareness to balance the importance of
different special perspectives. Together, these techniques
play a crucial role in improving the overall performance of
the model.
In this study, we propose a novel approach for decom-
posing the distillation loss into two distinct components,
namely the body loss ℓBand the edge loss ℓE, as expressed
by Equation 2. We include the body loss weight λband
edge loss weight λeto control the contributions of each loss
term. This decoupling strategy allows us to examine the
sensitivity of edge learning in the knowledge distillation
process, which has been overlooked by the literature. The
loss objective is defined as:
ℓ=λb·ℓB 
ZS
B,ZT
B
+λe·ℓE 
ZS
E,ZT
E
. (1)
3.2. Edge Knowledge Representation.
Our framework minimizes the discrepancy between the
teacher’s and the student’s features on the edge areas. To
achieve this, we extract an edge knowledge representation
by using soft edge masks. This edge mask is applied on the
logit map to produce a masked feature representation for
teachers’ and students’ edge regions. The edge map ME
is created through two stages: Pre-Mask Filtering (PRM),
which captures edge discrepancy for all classes, and Post-Mask Filtering (POM), which extracts edge discrepancy for
each individual class. This approach allows a model to
extract a more accurate and precise representation of the
edge knowledge, leading to improved performance in de-
tails classification.
During the edge detection process, we employ an adjustable
Trimap algorithm [52] to extract edge representations de-
noted by ZEfrom the ground truth (GT). Although the
teacher’s predictions could serve as an alternative source for
generating an edge mask, they offer slightly reduced accu-
racy compared to using GT directly. To generate the binary
edge mask GT edge, we compute the difference between the
dilation and erosion operations applied to the GT, formally
expressed as GT edge=dilation (GT)−erosion (GT). The
resultant binary mask GT edge∈RC×H×Wundergoes av-
erage pooling to produce ME∈RC×H′×W′, which shares
the same shape as the logits prediction Zpred. The dimen-
sions of MEare governed by the output stride Sof the seg-
mentation network, specifically W′=W/S .
•Pre Mask Filtering (PRM) . To obtain the logits map,
we apply MEto both student and teacher logits: ZE=
Zpred·ME. Specifically, we apply the edge mask for
each channel Cso that we can concentrate the logits
for the overlap edge regions. An intuitive example is if
4

--- PAGE 5 ---
there is a frame that displays a dog and a cat standing
nearby, only the logits activation of the dog and the cat
class will be considered and all other activation will
be suppressed. Such an operation forces the student
to focus more on the correlations between the adjacent
ambiguous classes. A spatial-level KL divergence loss
is applied to the filtered logits ZS
EandZT
E:
φ(ZT
E,ZS
E) =CX
c=1ϕ(ZT
E,i)·logϕ(ZT
E,i)
ϕ(ZS
E,i),(2)
where ϕis the softmax operation for each pixel.
φ(ZT
E,ZS
E)represents the edge-masked KL distances
for all spatial locations.
•Post Mask Filtering (POM) . We further separate the
edge loss for each class and perform normalization
based on the edge area by Post Mask Filtering (POM).
LetZT
E,i,c andZS
E,i,c denote the logits for the c-th class
at pixel iin the teacher and student models, respec-
tively. Let ME,cbe the soft mask obtained by average-
pooling the ground truth binary edge mask for the c-th
class, and ncdenotes the number of non-zero pixels in
ME,cfor this class. Our POM term can be formulated
as follows:
ℓE=CX
c=1αc
ncW·HX
i=1φ(ZT
E,i,c,ZS
E,i,c)·ME,c,(3)
By re-weighing the loss based on the edge area of
each class, we prioritize the center of the edge, where
the most important information is often located. This
approach ensures that the student model focuses on
learning the correct edge positions and shapes for each
class.
The Soft Edge Masks MEplay a critical role in the Edge
Loss, and our approach to generating them involves two
specialized designs: 1) converting binary GTEinto a
weighted discrete space, and 2) generating masks per chan-
nel instead of a unified mask. Directly applying binary
masks may include unconfident bias, so we use average
pooling to generate softer masks. We also carefully con-
sider overlapping masks to minimize noise and uncertainty.
Our mask design aims to exclusively include unconfident
bias for minimizing knowledge distributions.
In summary, the proposed PRM and POM stages in the edge
region refine the knowledge distillation process by identi-
fying edge discrepancies for each class and applying a re-
weighting to the loss based on the edge area. This method
guarantees that the student model learns the correct edge
positions and shapes for each class, and provides the shape
prior knowledge for body knowledge representation.3.3. Body Knowledge Representation.
This section investigates the body knowledge distilla-
tion. Prior works consider whole-view distillation, which
dilutes body knowledge with noisy representation on the
edge. To overcome these challenges, we utilize the reversed
edge binary mask to extract body masks. By removing the
edge region, we exploit implicit shape constraints and re-
duce uncertainty, which allows the body loss to focus on
assigning the large inner regions of objects to their corre-
sponding categories. To achieve this, we propose a region
alignment approach that synthesizes channel-level activa-
tions to obtain semantically rich sections. As we prede-
fined that Z=ZB+ZE. The body logits is obtained
byZB=Z × (1−ME). As shown in the previous
work [35, 49], a pixel-wise loss for the body region will
bring in unexpected noise due to the hard constraints. Thus,
we employ a loose constraint of the channel-wise distilla-
tion [49] for the body part. Body enhancement loss (BEL)
is defined as:
ℓB=T2
CCX
c=1W·HX
i=1ϕ(ZT
B,c,i)·loghϕ(ZT
B,c,i)
ϕ(ZS
B,c,i)i
,(4)
4. Experiments
4.1. Experimental Setup
Dataset. We conduct the experiments on three benchmark
datasets for semantic segmentation: Cityscapes [13], Pascal
Context 2010 [15], and ADE20K [71].
ADE20K [71] contains 20k/2k/3k images for train/val/test
with 150 semantic classes. It is constructed as the bench-
mark for scene parsing and instance segmentation.
Cityscapes [13] is an urban scene parsing dataset that
contains 2975/500/1525 finely annotated images used for
train/val/test. The performance is evaluated in 19 classes.
Pascal Context [15] provides dense annotations, which
contain 4998/5105/9637 train/val/test images. We use 59
object categories for training and testing. Our results are
reported on the validation set.
Implementation Details. Our implementation is based on
the open-source toolbox MMSegmentation [11, 12] with
PyTorch 1.11.0. We employ the standard data augmenta-
tion, including random flipping, cropping, and scaling in
the range of [0.5, 2]. All experiments are optimized by
SGD with a momentum of 0.9, and a batch size of 16. We
use the crop of 512 ×512, 512 ×1024, and 480 ×480
for ADE20k, Cityscapes, and Pascal Context, correspond-
ingly. We use an initial learning rate of 0.01 for ADE20K
and Cityscapes. In addition, we use an initial learning rate
of 0.004 for Pascal Context. The number of total training it-
erations is 80K. Following the previous methods [7,70], we
use the poly learning rate policy and report the single-scale
5

--- PAGE 6 ---
Table 1: Performance comparison of different distillation methods with state-of-the-art techniques. We test these methods on
various segmentation networks for both student and teacher models, using datasets including Cityscapes [13], ADE20K [71],
and Pascal Context [15]. The FLOPs and FPS are obtained on 512×512resolutions. Our BPKD outperforms all previous
methods in large margins across multiple datasets and network architectures. DLab refers to Deeplab architecture. HRV2P
refers to HRNetV2p. MV2 refers to MobileNet v2.
ADE20K Cityscapes Pascal Context 59
Methods FLOPs(G) Param(M) FPS(S) 80k 512*512 80k 1024*512 80k 480*480
mIoU(%) mAcc(%) mIoU(%) mAcc(%) mIoU(%) mAcc(%)
T: PSPNet-R101 [70] 256.89 68.07 2.68 44.39 54.75 79.74 86.56 52.47 63.15
S:PSPnet-R18 [70] 54.53 12.82 15.71 33.30 42.58 74.23 81.45 43.79 54.46
SKDS [35] 54.53 12.82 15.71 34.49( ▲1.19) 44.28 76.13( ▲1.9) 82.58 45.08( ▲1.29) 55.56
IFVD [21] 54.53 12.82 15.71 34.54( ▲1.24) 44.26 75.35( ▲1.12) 82.86 45.97( ▲2.18) 56.6
CIRKD [60] 54.53 12.82 15.71 35.07( ▲1.77) 45.38 76.03( ▲1.80) 82.56 45.62( ▲1.83) 56.15
CWD [49] 54.53 12.82 15.71 37.02( ▲3.72) 46.33 76.26( ▲2.03) 83.04 45.99( ▲2.20) 55.56
BPKD(Ours) 54.53 12.82 15.71 38.51( ▲5.21) 47.70 77.57( ▲3.34) 84.47 46.82( ▲3.03) 56.29
T:HRV2P-W48 [53] 95.64 65.95 6.42 42.02 53.52 80.65 87.39 51.12 61.39
S:HRV2P-W18S [53] 10.49 3.97 23.74 31.38 41.39 75.31 83.71 40.62 51.43
SKDS [35] 10.49 3.97 23.74 32.57( ▲1.19) 43.22 77.27( ▲1.96) 84.77 41.54( ▲0.92) 52.18
IFVD [21] 10.49 3.97 23.74 32.66( ▲1.28) 43.23 77.18( ▲1.87) 84.74 41.55( ▲0.93) 52.24
CIRKD [60] 10.49 3.97 23.74 33.06( ▲1.68) 44.30 77.36( ▲2.05) 84.97 42.02( ▲1.40) 52.88
CWD [49] 10.49 3.97 23.74 34.00( ▲2.62) 42.76 77.87( ▲2.56) 84.98 42.89( ▲2.27) 53.37
BPKD(Ours) 10.49 3.97 23.74 35.31( ▲3.93) 46.11 78.58( ▲3.27) 85.78 43.96( ▲3.34) 54.51
T:DLabV3P-R101 [6] 255.67 62.68 2.60 45.47 56.41 80.98 88.70 53.20 64.04
S:DLabV3P-MV2 [48] 69.60 15.35 8.40 31.56 45.14 75.29 83.11 41.01 52.92
SKDS [35] 69.60 15.35 8.40 32.49( ▲0.93) 46.47 76.05( ▲0.76) 84.14 42.07( ▲1.06) 55.06
IFVD [21] 69.60 15.35 8.40 32.11( ▲0.55) 46.07 76.97( ▲1.68) 84.85 41.73( ▲0.72) 54.34
CIRKD [60] 69.60 15.35 8.40 32.24( ▲0.68) 46.09 77.71( ▲2.42) 85.33 42.25( ▲1.24) 55.12
CWD [49] 69.60 15.35 8.40 35.12( ▲3.56) 49.76 77.97( ▲2.68) 86.68 43.74( ▲2.73) 56.37
BPKD(Ours) 69.60 15.35 8.40 35.49( ▲3.93) 53.84 78.59( ▲3.30) 86.45 46.23( ▲5.22) 58.12
T:ISANet-R101 [24] 228.21 56.80 2.35 43.80 54.39 80.61 88.29 53.41 64.04
S:ISANet-R18 [24] 54.33 12.46 17.34 31.15 41.21 73.62 80.36 44.05 54.67
SKDS [35] 54.33 12.46 17.34 32.16( ▲1.01) 41.80 74.99( ▲1.37) 82.61 45.69( ▲1.64) 56.27
IFVD [21] 54.33 12.46 17.34 32.78( ▲1.63) 42.61 75.35( ▲1.73) 82.86 46.75( ▲2.70) 56.4
CIRKD [60] 54.33 12.46 17.34 32.82( ▲1.67) 42.71 75.41( ▲1.79) 82.92 45.83( ▲1.78) 56.11
CWD [49] 54.33 12.46 17.34 37.56( ▲6.41) 45.79 75.43( ▲1.81) 82.64 46.76( ▲2.71) 56.48
BPKD(Ours) 54.33 12.46 17.34 38.73( ▲7.58) 47.92 75.72( ▲2.10) 83.65 47.25( ▲3.20) 56.81
testing result. We conduct all experiments on 4 NVIDIA
A100 GPUs. All the distillation methods are trained with
the same configurations.
Metrics. We set up a fair comparison by assigning identical
parameters for each method with the same dataset. Mean
Intersection-over-Union (mIoU), Trimap mIoU and pixel
mean accuracy (mAcc) are employed as the main evalua-
tion metrics. GFLOPs, FPS and No. Parameters are also
reported for various student networks that we tested. All re-
ported computational costs are measured using the fvcore.1
4.2. Compare with State-of-the-arts Methods
To ensure a fair comparison, we have re-implemented
a number of previously proposed knowledge distillation
methods, including those by [35, 49, 56, 60]. Subse-
quently, we benchmarked our BPKD method against var-
ious compact networks, such as PSPNet with ResNet18
backbone [70], HRNet-W18 [53], Deeplab-V3+ with Mo-
1https://github.com/facebookresearch/fvcoreTable 2: Performance comparison of transformers-based
architecture vs. different distillation strategies. Standard
mIoU and Trimap mIoU of Swin Transformers [38] and
DeiT [51] with ViT Adapter (DeiT-Ada) [8] on ADE20K
with UPerNet [57] decoder for 80K iterations. Distillation
forward speed (DFS.), training time (TT.) and GPU mem-
ory footprint (GMem.) light our method have negligible
computational cost. (DFS.) and (TT.) estimated on DeiT-
Adapter with batch size = 16 with 4 GPUs and (GMem.)
standard for per sample video memory allocation.
DFS.(S) ↑TT.(H) ↓GMem.(G) ↓Swin ↑Trimap ↑DeiT-Ada. ↑Trimap ↑
T:Base 9.52 11.26 8.32 50.13 40.10 48.80 39.72
S:Tiny 12.80 8.44 3.87 43.57 32.78 41.10 32.15
SKDS 8.72 11.36 4.45 43.58 33.04 41.90 32.25
IFVD 6.06 16.45 8.97 43.75 32.90 41.16 32.11
CIRKD 7.70 16.35 10.70 43.32 32.68 41.64 32.23
CWD 8.76 11.15 4.45 44.99 33.73 44.25 33.49
BPKD 7.84 13.49 5.49 46.13 38.11 45.25 37.05
bileNetV2 backbone [48], ISANet with ResNet18 [24],
Swin Transformers [38] with UPerNet [57] and DeiT [51]-
Adapter [8] with UPerNet [57].
6

--- PAGE 7 ---
Method
Teacher: PSP-ResNet101 79.74%
Student: PSP-ResNet18 Standard: 68.99% Trimap: 55.34%
Channel Wise Distillation [49] Standard: 74.29% Trimap: 57.34%
Pixel Wise Distillation [35] Standard: 69.33% Trimap: 53.82%
Body(C) Body(P) Edge(P) Edge(C) Ours
mIoU(%)Standard 74.17 ( ▲5.18) 72.70 ( ▲3.71) 71.63 ( ▲2.64) 66.83 ( ▼2.16) 75.94 ( ▲6.95)
Trimap 56.20 ( ▲0.86) 54.12 ( ▼1.22) 61.37 ( ▲6.03) 51.76 ( ▼3.58) 62.91 ( ▲7.57)
Table 3: The effectiveness of the decoupling whole-view knowledge representation. The results show knowledge representa-
tion for different spatial locations should be considered, separately. C and P denote channel-wise and pixel-wise knowledge
distillation, respectively.
Performance. Table 1 reports our method’s performance
on the ADE20K validation set, where the proposed BPKD
achieves state-of-the-art (SOTA) performance across multi-
ple student networks. The distillation process enhances the
mean Intersection over Union (mIoU) for student networks
by up to 24.33%. Notably, BPKD consistently outper-
forms the current SOTA, CWD, by 3.87% across all eval-
uated network architectures. Additional results on the Pas-
cal Context validation set indicate an average performance
increase of 2% over SOTA methods. Further experiments
presented in Table 2 reveal the method’s efficacy on pop-
ular Transformer architectures like Swin and DeiT, where
it outperforms CWD by up to 2.53%. These findings un-
derscore the architecture-agnostic nature of our approach.
In terms of computational efficiency, BPKD achieves a dis-
tillation forward speed up to 29.3%, training duration re-
duction of 21.2%and21.9%, and a memory consumption
(GMem.) reduction of 94.8%and63.4%, compared to pre-
vious methods such as CIRKD and IFVD. Additionally, we
register a 13% improvement in Trimap metrics over CWD
on the Swin architecture, emphasizing the method’s cost-
effectiveness with SOTA performance.
4.3. Ablation Study
In this section, we comprehensively evaluate our BPKD
under different settings. All ablation experiments are con-
ducted on the Cityscapes dataset with T: PSPNet-R101 and
S: PSPNet-R18. To decrease computational costs, we adopt
a streamlined training configuration, including crop size re-
duced to 512 ×512, and training schedule to 40k iterations.
More experiment results are shown in the supplementary.
Effectiveness of Decoupled Knowledge. To verify the
effectiveness of the proposed knowledge distillation ap-
proach, we evaluate the segmentation performance in the
edge region in Table 3. We evaluated the Trimap mIoU met-
ric [7] when using channel-wise and pixel-wise normaliza-
tion for network distillation. Channel-wise normalization
led to a decline in both standard mIoU by 2.16% and Trimap
mIoU by 3.58%, indicating its sensitivity in edge regions.
Conversely, pixel-wise distillation enhanced Trimap mIoUby6.03%, benefiting from our specialized Edge loss design
that refines boundary quality [29, 30]. The Body loss func-
tion shows an increase in standard mIoU by 5.18% when
uses channel-wise and 3.71% when spatial-wise, emphasiz-
ing its effectiveness for body regions. However, it had a
limited impact on Trimap’s performance. In summary, our
optimal approach, BPKD, achieved the best mIoU scores
of75.94(+6 .95)% and62.91(+7 .59)% on standard and
Trimap evaluations, respectively. This highlights BPKD’s
capability to refine semantic boundaries and body regions
through pixel-level alignment and context aggregation.
Method mIoU(%) IMP.(%) mAcc(%)
Teacher 79.74 - 86.56
ResNet18 68.99 - 75.19
+ PRM 70.37 ▲1.98 76.95
+ POM 71.63 ▲2.64 78.47
+ BEL 74.17 ▲5.18 80.47
+ PRM + BEL 74.81 ▲5.92 81.52
+ POM + BEL 74.62 ▲5.63 80.98
+ PRM + POM + BEL 75.94 ▲6.95 82.62
PRM : Pre-Mask Filtering BEL : Body Loss POM : Post-Mask Filter-
ing
Table 4: The different locations apply the mask in the pro-
posed method. (IMP.) refers to the improvement achieved
by the student network.
Compare Edge Filter Locations. From Table 4, the nu-
merical results demonstrated the effectiveness of our pro-
posed method. We further analyzed the impact of apply-
ing the edge filter for different locations. Applying the Pre
Mask filter, the performance slightly improved by 1.98%
compared to the student without distillation. In contrast,
Post Mask filtering improves the performance by 2.64%,
because POM extracts edge discrepancy specifically for
each type of class. The Body Enhancement Module takes
care of non-edge information during our distillation setting,
and the performance is raised by 5.18 %. Afterwards, we ex-
plore the performance by applying PRM or POM to BEL.
The combination of three terms archives best mIoU that
75.94% on the Cityscapes validation set.
7

--- PAGE 8 ---
Edge Width mIoU IMP. mAcc
3 73.89 ▲4.90 81.24
5 74.26 ▲5.27 82.00
7 75.94 ▲6.95 82.62
10 74.70 ▲5.71 82.13
15 74.36 ▲5.37 82.02
Table 5: Performance comparison for various
different edge width with PSPNet-R18 on the
Cityscapes validation set, for a fair comparison,
we rerun the same setting 3 times and measure
mean mIoU for evaluation.
5 10 15 20 25
(a) Body Loss Weight lb73:073:574:074:575:075:576:076:5mIoU ( %) on Cityscapes val
30 40 50 60 70
(b) Edge Loss Weight le73:073:574:074:575:075:576:076:5
1 2 3 4 5
(c) Edge Loss Inner a73:073:574:074:575:075:576:076:5
Figure 3: Impact of the (a) Body Loss weight λband (b) Edge Loss
weight λeand (c) Edge Loss Inner weight αon Cityscape val. We
found the optimal combination by board range study that λb= 20, λe=
50, α= 2.
Impact of Different Edge Widths. Edge width is pivotal
in the Edge Detection Module, intricately influencing both
the quality and the computational demands of the edge de-
tection process. Specifically, a larger edge unit results in a
wider edge, which incorporates more pixels into the edge
loss. While this increase can potentially augment the ro-
bustness of edge detection, it also presents a trade-off: the
(a) Image(b) Teacher(c) W/odistillation(d) BPKD(Ours)
Figure 4: Comparison of CAM visualizations among (b) the
teacher model, (c) the student model without distillation,
and (d) the BPKD model. Activation maps were extracted
from the last block of the corresponding ResNet backbones
using HiResCAM [14]. The results indicate that BPKD
shows superior refinement of boundaries and higher atten-
tion to semantic bodies. For better visualization, zoom-in is
recommended.wider the edge includes pixels from the body region, which
could dilute the precision of edge-specific features. We sys-
tematically analyze the impact of varying edge widths on
the distillation performance, as summarized in Table 5. Our
analysis reveals that an edge width of 7 units is optimal,
leading to a significant performance improvement of 6.95%.
Impact of Hyperparameters. In the optimization process,
the parameters λbandλeserve as weighting factors for the
body and edge loss functions, respectively. These hyperpa-
rameters are pivotal in balancing the focus between seman-
tic boundary refinement and overall body region accuracy.
A higher value of λeplaces more emphasis on edge quality,
conversely, a larger λbvalue makes the model more attuned
to the larger body regions and potentially boosts the stan-
dard mIoU. The edge loss incorporates an internal parame-
terα, serving as a class-wise balancing factor. This parame-
ter is particularly instrumental in mitigating class imbalance
by intra-adjusting the contribution of each class to the over-
all loss. As shown in Fig 3, we investigate the impact of
loss weights in our BPKD and find λb= 20 ,λe= 50 and
α= 2is the best choice.
CAM Visualization Analysis. Figure 4 illustrates the ex-
plicit refinement of semantic boundaries on multiple ob-
jects. The shape constraints are evident, such as the strong
attention given to the pillow outlines. Despite BPKD dis-
tillation, the student network cannot perfectly segment the
horse in the second row, but it has highly attended to the
bone silhouette. This shows that BPKD has tried its best
to distill knowledge to the student network, but due to its
limited size and capacity, the student can only learn the
surface-level capacity of the teacher. The body sections
have been smoothly affiliated to a single category, reducing
high-uncertainty edge and incorporating shape prior knowl-
edge from the edge loss pressure. More qualitative seg-
mentation results in supplement visually demonstrate our
BPKD’s effectiveness for both tiny and large objects with
explicit boundaries enhancement.
8

--- PAGE 9 ---
5. Conclusion
This work presents a novel boundary-privileged knowl-
edge distillation (BPKD) method for semantic segmenta-
tion, which transfers the cumbersome teacher model’s body
and edge knowledge to the compact student model, sepa-
rately. Extensive experiments demonstrate that the knowl-
edge representation in the body and the edge regions should
be considered differently. Due to different intrinsic prop-
erties, the edge region needs to focus on distinguishing be-
tween uncertain classes for each pixel while the body re-
gion needs to focus more on localizing and connecting ob-
ject structures. Experimental results show that the proposed
distillation method consistently outperforms state-of-the-art
methods on various public benchmark datasets. The over-
all mIoU and the performance in the edge region are both
improved by a large margin.
6. Acknowledgments
Y . Liu acknowledges the support of start-up funding
from The University of Adelaide for their participation in
this work. We express our gratitude to The University of
Adelaide High-Performance Computing Services for pro-
viding the GPU Compute Resources, and to Mr. Wang Hui
and Dr. Fabien V oisin for their valuable technical support.
We would like to thank Mr. Hanwen Wang for providing
the visualization collection used in the supplementary.
References
[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana
Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul
Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra
Acharya, et al. A review of uncertainty quantification in deep
learning: Techniques, applications and challenges. Informa-
tion Fusion , 76:243–297, 2021. 2
[2] John Canny. A computational approach to edge detection.
IEEE Transactions on pattern analysis and machine intelli-
gence , (6):679–698, 1986. 3
[3] Weihan Cao, Yifan Zhang, Jianfei Gao, Anda Cheng, Ke
Cheng, and Jian Cheng. Pkd: General distillation framework
for object detectors via pearson correlation coefficient. arXiv
preprint arXiv:2207.02039 , 2022. 2, 3
[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Semantic image segmen-
tation with deep convolutional nets and fully connected crfs.
arXiv preprint arXiv:1412.7062 , 2014. 2
[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE transactions on pattern
analysis and machine intelligence , 40(4):834–848, 2017. 2
[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for seman-
tic image segmentation. arXiv preprint arXiv:1706.05587 ,
2017. 2, 6[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
Proceedings of the European conference on computer vision
(ECCV) , pages 801–818, 2018. 2, 5, 7
[8] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong
Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for
dense predictions. arXiv preprint arXiv:2205.08534 , 2022.
6
[9] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 1290–1299, 2022. 1
[10] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmen-
tation. Advances in Neural Information Processing Systems ,
34:17864–17875, 2021. 1
[11] MMSegmentation Contributors. MMSegmentation:
Openmmlab semantic segmentation toolbox and bench-
mark. https://github.com/open-mmlab/
mmsegmentation , 2020. 5
[12] MMRazor Contributors. Openmmlab model compres-
sion toolbox and benchmark. https://github.com/
open-mmlab/mmrazor , 2021. 5
[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3213–3223, 2016. 2, 5, 6, 13, 16
[14] Rachel Lea Draelos and Lawrence Carin. Hirescam:
Faithful location representation in visual attention for ex-
plainable 3d medical image classification. arXiv preprint
arXiv:2011.08891 , 2020. 8
[15] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International journal of computer
vision , 88(2):303–338, 2010. 2, 5, 6, 13
[16] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei,
Zhenhua Chai, Junfeng Luo, and Xiaolin Wei. Rethinking
bisenet for real-time semantic segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 9716–9725, 2021. 2
[17] Jianping Gou, Baosheng Yu, Stephen J Maybank, and
Dacheng Tao. Knowledge distillation: A survey. Interna-
tional Journal of Computer Vision , 129(6):1789–1819, 2021.
3
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 1
[19] Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin Young
Choi. Knowledge transfer via distillation of activation
boundaries formed by hidden neurons. In Proceedings of
the AAAI Conference on Artificial Intelligence , volume 33,
pages 3779–3787, 2019. 2, 3
9

--- PAGE 10 ---
[20] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling
the knowledge in a neural network. 2015. 2, 3
[21] Yuenan Hou, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, and
Chen Change Loy. Inter-region affinity distillation for road
marking segmentation. In CVPR , pages 12486–12495, 2020.
6, 12
[22] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-
bilenetv3. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 1314–1324, 2019. 1,
3
[23] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017. 3
[24] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang,
Xilin Chen, and Jingdong Wang. Interlaced sparse
self-attention for semantic segmentation. arXiv preprint
arXiv:1907.12273 , 2019. 6
[25] Forrest N Iandola, Song Han, Matthew W Moskewicz,
Khalid Ashraf, William J Dally, and Kurt Keutzer.
Squeezenet: Alexnet-level accuracy with 50x fewer pa-
rameters and¡ 0.5 mb model size. arXiv preprint
arXiv:1602.07360 , 2016. 3
[26] Deyi Ji, Haoran Wang, Mingyuan Tao, Jianqiang Huang,
Xian-Sheng Hua, and Hongtao Lu. Structural and statistical
texture knowledge distillation for semantic segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 16876–16885, 2022. 2
[27] Nick Kanopoulos, Nagesh Vasanthavada, and Robert L
Baker. Design of an image edge detection filter using the so-
bel operator. IEEE Journal of solid-state circuits , 23(2):358–
367, 1988. 3
[28] Jangho Kim, SeongUk Park, and Nojun Kwak. Paraphras-
ing complex network: Network compression via factor trans-
fer.Advances in neural information processing systems , 31,
2018. 2, 3
[29] Pushmeet Kohli, Philip HS Torr, et al. Robust higher or-
der potentials for enforcing label consistency. International
Journal of Computer Vision , 82(3):302–324, 2009. 7
[30] Philipp Kr ¨ahenb ¨uhl and Vladlen Koltun. Efficient inference
in fully connected crfs with gaussian edge potentials. Ad-
vances in neural information processing systems , 24, 2011.
7
[31] Seung Hyun Lee, Dae Ha Kim, and Byung Cheol Song. Self-
supervised knowledge distillation using singular value de-
composition. In Proceedings of the European Conference
on Computer Vision (ECCV) , pages 335–350, 2018. 2, 3
[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision , pages 740–755.
Springer, 2014. 14
[33] Akide Liu and Zihan Wang. Cv 3315 is all you
need: Semantic segmentation competition. arXiv preprint
arXiv:2206.12571 , 2022. 2[34] Xiaofeng Liu, Fangxu Xing, Georges El Fakhri, and Jonghye
Woo. Self-semantic contour adaptation for cross modality
brain tumor segmentation. In IEEE International Symposium
on Biomedical Imaging , pages 1–5. IEEE, 2022. 2
[35] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo,
and Jingdong Wang. Structured knowledge distillation for
semantic segmentation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2604–2613, 2019. 2, 3, 5, 6, 7, 12
[36] Yifan Liu, Changyong Shu, Jingdong Wang, and Chunhua
Shen. Structured knowledge distillation for dense predic-
tion. IEEE transactions on pattern analysis and machine
intelligence , 2020. 3
[37] Yueming Liu, Xiaomei Yang, Zhihua Wang, Chen Lu, Zhi
Li, and Fengshuo Yang. Aquaculture area extraction and
vulnerability assessment in sanduao based on richer convo-
lutional features network model. Journal of Oceanology and
Limnology , 37(6):1941–1954, 2019. 3
[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012–10022, 2021. 6
[39] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 11976–11986,
2022. 1
[40] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 3431–3440, 2015. 2
[41] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufflenet v2: Practical guidelines for efficient cnn architec-
ture design. In Proceedings of the European conference on
computer vision (ECCV) , pages 116–131, 2018. 1
[42] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda
Shapiro, and Hannaneh Hajishirzi. Espnet: Efficient spatial
pyramid of dilated convolutions for semantic segmentation.
InProceedings of the european conference on computer vi-
sion (ECCV) , pages 552–568, 2018. 3
[43] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and
semantic segmentation in the wild. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2014. 17
[44] Shivam K Panda, Yongkyu Lee, and M Khalid Jawed.
Agronav: Autonomous navigation framework for agricul-
tural robots and vehicles using semantic segmentation and
semantic line detection. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 6271–6280,
2023. 2
[45] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eu-
genio Culurciello. Enet: A deep neural network architec-
ture for real-time semantic segmentation. arXiv preprint
arXiv:1606.02147 , 2016. 3
10

--- PAGE 11 ---
[46] Judith MS Prewitt et al. Object enhancement and extraction.
Picture processing and Psychopictorics , 10(1):15–19, 1970.
3
[47] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou,
Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550 ,
2014. 3
[48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4510–4520, 2018. 3, 6
[49] Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, and
Chunhua Shen. Channel-wise knowledge distillation for
dense prediction. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 5311–5320,
2021. 2, 3, 5, 6, 7, 12, 15, 16, 17
[50] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In International
conference on machine learning , pages 6105–6114. PMLR,
2019. 1
[51] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In International conference on machine learning ,
pages 10347–10357. PMLR, 2021. 6
[52] Jue Wang, Michael F Cohen, et al. Image and video matting:
a survey. Foundations and Trends® in Computer Graphics
and Vision , 3(2):97–175, 2008. 4
[53] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,
Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui
Tan, Xinggang Wang, et al. Deep high-resolution repre-
sentation learning for visual recognition. IEEE transactions
on pattern analysis and machine intelligence , 43(10):3349–
3364, 2020. 1, 3, 6
[54] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 7794–7803, 2018. 3
[55] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-
hua Shen. Solov2: Dynamic and fast instance segmenta-
tion. Advances in Neural information processing systems ,
33:17721–17732, 2020. 14
[56] Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, and
Yongchao Xu. Intra-class feature variation distillation for se-
mantic segmentation. In European Conference on Computer
Vision , pages 346–362. Springer, 2020. 2, 3, 6, 12
[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understand-
ing. In Proceedings of the European conference on computer
vision (ECCV) , pages 418–434, 2018. 6
[58] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and
efficient design for semantic segmentation with transform-
ers. Advances in Neural Information Processing Systems ,
34:12077–12090, 2021. 1[59] Saining Xie and Zhuowen Tu. Holistically-nested edge de-
tection. In Proceedings of the IEEE international conference
on computer vision , pages 1395–1403, 2015. 3
[60] Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang,
Yongjun Xu, and Qian Zhang. Cross-image relational knowl-
edge distillation for semantic segmentation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 12319–12328, 2022. 2, 3, 6, 12
[61] Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan
Yuan, and Chun Yuan. Masked generative distillation. In
Computer Vision–ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XI ,
pages 53–69. Springer, 2022. 2, 3
[62] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A
gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4133–4141, 2017. 2, 3
[63] Zhiding Yu, Chen Feng, Ming-Yu Liu, and Srikumar Rama-
lingam. Casenet: Deep category-aware semantic edge detec-
tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 5964–5973, 2017. 3
[64] Yuhui Yuan, Xiaokang Chen, Xilin Chen, and Jingdong
Wang. Segmentation transformer: Object-contextual rep-
resentations for semantic segmentation. arXiv preprint
arXiv:1909.11065 , 2019. 1
[65] Sergey Zagoruyko and Nikos Komodakis. Paying more at-
tention to attention: Improving the performance of convolu-
tional neural networks via attention transfer. arXiv preprint
arXiv:1612.03928 , 2016. 2, 3
[66] Bowen Zhang, Liyang Liu, Minh Hieu Phan, Zhi Tian,
Chunhua Shen, and Yifan Liu. Segvitv2: Exploring efficient
and continual semantic segmentation with plain vision trans-
formers. IJCV , 2023. 1
[67] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xi-
aolin Wei, Chunhua Shen, and Yifan Liu. Segvit: Semantic
segmentation with plain vision transformers. arXiv preprint
arXiv:2210.05844 , 2022. 1, 2
[68] Chenrui Zhang and Yuxin Peng. Better and faster: knowl-
edge transfer from multiple self-supervised learning tasks
via graph distillation for video classification. arXiv preprint
arXiv:1804.10069 , 2018. 2, 3
[69] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu,
Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller,
R Manmatha, et al. Resnest: Split-attention networks. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2736–2746, 2022. 1
[70] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 2881–2890, 2017. 2, 5, 6, 12, 15,
16, 17
[71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Scene parsing through
ade20k dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 633–641,
2017. 2, 5, 6, 13, 15
11

--- PAGE 12 ---
In the supplementary materials, we augment the core ar-
guments of our main paper with additional analyses, ex-
panded experimental details, and enhanced visualizations.
Initially, we conduct a performance comparison using re-
duced schedules across three unique datasets, specifically
chosen for ablation studies. These comparative results are
encapsulated in Table 7. To facilitate reproducibility, we
provide an in-depth guide to our implementation, along
with PyTorch-compatible code for our specialized Edge
Loss. Additional analytical insights into class-trimap IoU
are available in Figure 6 and Table 8. Lastly, due to space
constraints in the main manuscript, we include both vi-
sual and quantitative experiments on three distinct datasets
within this supplementary section.
A. Performance On Reduced Schedules
In order to demonstrate the efficacy of leveraging soft-labels
from teachers to accelerate convergence speed, we con-
ducted a comparison experiment with reduced schedules,
40k iteration training, and a 512×512resolution. The re-
sults of this experiment, as well as comparisons with other
state-of-the-art algorithms, are reported in Table 7. To en-
sure a fair comparison, our proposed knowledge distillation
framework was applied to different teachers. Our students
were able to learn knowledge from the teacher network, re-
sulting in significant performance gains and achieving state-
of-the-art results on multiple datasets. As demonstrated
Figure 5: BPKD (left) decouples the edge and body infor-
mation from the input image and creates two parallel dis-
tillation pipelines compared to pioneering works, such as
CWD [49], SKDS [35], IFVD [56], CIRKD [60]. The pro-
posed compositional loss forces the student to learn each
part separately. The result shows the explicit performance
gain.in Table 7, on ADE20K our method outperformed strong
baseline channel-wise distillation by 2.34%, 1.46%, 1.66%,
1.57%, and 1.59%, respectively. Furthermore, our meth-
ods were able to enhance the lightweight student network
without increasing computational capacity, improving per-
formance by 6.45%, 4.13%, 6.08%, and 8.49% compared
to raw students. On Cityscape our method outperformed
strong baseline channel-wise distillation by 2.22%, 1.35%,
4.40%, 1.52%, respectively. Furthermore, our methods
were able to enhance the lightweight student network with-
out increasing computational capacity, improving perfor-
mance by 10.07%, 3.78%, 8.64%, and 1.75% compared to
raw students. On Pascal Context our method outperformed
strong baseline channel-wise distillation by 2.34%, 1.46%,
1.66%, 1.57%, and 1.59%, respectively. Furthermore, our
methods were able to enhance the lightweight student net-
work without increasing computational capacity, improving
performance by 10.07%, 3.78%, 6.08%, and 8.49% com-
pared to raw students. These numerical results suggest that
our method is not dependent on a specific model struc-
ture, and that it produces significant performance gains with
the pure student network, even without ImageNet Pre-train
shows in table 6. To further demonstrate the effectiveness of
our method, qualitative segmentation results are visualized
in Figure 9.
Table 6: Performance Comparison with state-of-the-art dis-
tillation methods on ADE20K dataset, the student backbone
is not pre-trained on ImageNet.
Methods mIoU mAcc(%)
T:PSPNet-R101 [70] 44.39 54.74
S:PSPnet-R18 [70] 17.11 22.99
SKDS [35] 20.79 27.74
IFVD [21] 20.75 27.6
CIRKD [60] 22.90 30.68
CWD [49] 24.79 31.44
BPKD(Ours) 27.46 36.10
B. Implementation Details of Edge Loss
This section presents the implementation details of the Edge
Loss in PyTorch , aimed at facilitating reproducibility. Our
implementation consists of well-annotated components that
are incorporated into our distillation system. To begin with,
the Pre Mask Filter operation is applied to the logits ob-
tained from both teachers and students. Subsequently, a di-
mensional rearrangement is performed to optimize the pro-
cess. The KL divergence serves as the core component to
estimate the distance between the student and teacher prob-
ability distributions, which establishes an embryonic refer-
ence for calculating the loss. The Post Mask Filter takes
12

--- PAGE 13 ---
Table 7: Performance comparison of different distillation methods with state-of-the-art techniques in Reduced schedules . We
set a reduced training setting to reducethe computational cost, including crop size reduced to 512 × 512, and training schedule
to 40k iterations. We test these methods on various segmentation networks for both student and teacher models, using datasets
including Cityscapes [13], ADE20K [71], and Pascal Context [15]. The FLOPs are obtained on 512×512resolutions. Our
BPKD outperforms all previous methods in large margins across multiple datasets and network architectures.
Methods FLOPs(G) Parameters(M) ADE20K Cityscapes Pascal Context 59
40k 512*512 40k 512*512 40k 512*512
mIoU(%) mAcc(%) mIoU mAcc(%) mIoU mAcc(%)
T: PSPNet-R101 256.89 68.07 44.39 54.75 79.74 86.56 52.47 63.15
S:PSPnet-R18 54.53 12.82 29.42 38.48 68.99 75.19 43.07 53.79
SKDS 54.53 12.82 31.80 42.25 69.33 75.37 43.93 54.01
IFVD 54.53 12.82 32.15 42.53 71.08 77.46 44.75 54.99
CIRKD 54.53 12.82 32.25 43.02 72.23 78.79 44.83 55.3
CWD 54.53 12.82 33.53 41.71 74.29 80.95 45.92 55.50
BPKD(Ours) 54.53 12.82 35.87 45.42 75.94 82.62 47.16 57.61
T: HRNetV2P-W48 95.64 65.95 42.02 53.52 80.65 87.39 51.12 61.39
S:HRNetV2P-W18S 10.49 3.97 28.69 37.86 73.77 82.89 40.82 51.70
SKDS 10.49 3.97 30.49 40.19 74.75 83.23 42.91 53.63
IFVD 10.49 3.97 30.57 40.42 75.33 83.83 43.12 54.03
CIRKD 10.49 3.97 31.34 41.45 74.63 83.72 43.45 54.10
CWD 10.49 3.97 31.36 40.24 75.54 84.08 45.50 56.01
BPKD(Ours) 10.49 3.97 32.82 43.49 76.56 85.34 46.12 57.63
T:DeeplabV3P-R101 255.67 62.68 45.47 56.41 80.98 88.7 53.2 64.04
S:DeeplabV3P+MV2 69.6 15.35 22.38 31.71 70.49 80.11 37.16 49.1
SKDS 69.6 15.35 24.65 35.07 70.81 79.31 39.18 51.13
IFVD 69.6 15.35 24.53 35.13 71.82 80.88 38.8 50.79
CIRKD 69.6 15.35 25.21 36.17 72.39 81.84 39.99 52.66
CWD 69.6 15.35 26.89 35.79 73.35 82.41 42.52 53.24
BPKD(Ours) 69.6 15.35 28.46 41.45 76.58 84.14 44.32 56.04
T:ISANet-R101 228.21 56.8 43.8 54.39 80.61 88.29 52.94 63.52
S: ISANet-R18 54.33 12.46 27.68 36.92 71.45 78.65 41.08 50.62
SKDS 54.33 12.46 28.70 38.51 70.65 77.53 42.87 52.89
IFVD 54.33 12.46 29.66 38.80 70.30 77.79 43.19 53.46
CIRKD 54.33 12.46 29.79 40.48 72.00 79.32 43.49 53.89
CWD 54.33 12.46 34.58 43.04 71.61 80.02 44.63 55.01
BPKD(Ours) 54.33 12.46 36.17 45.26 72.72 81.50 45.50 56.55
Class road sidewalk building wall fence pole light sign vege. terrain
Baseline 80.88 62.44 68.72 24.82 35.91 59.73 58.39 63.58 69.64 42.58
CWD 81.84 65.36 70.14 29.22 37.98 60.72 60.43 65.12 70.91 45.28
BPKD(Ours) 86.29 70.73 76.45 35.13 42.12 63.96 66.25 71.09 77.65 49.87
Class sky person rider car truck bus train moto. bicycle average
Baseline 77.72 65.94 48.45 74.55 33.63 50.58 34.58 39.83 59.56 55.34
CWD 79.2 67.58 49.11 75.76 36.53 51.37 38.29 43.82 60.75 57.34
BPKD(Ours) 84.87 73.56 54.4 82.28 44.49 57.27 41.27 50.14 67.37 62.91
Table 8: Illustration of our proposed Boundary Privilege Knowledge Distillation schemes in terms of class Trimap IoU
metrics with PSPnet + Resnet18 network architecture over the Cityscapes validation set.
input from the unreduced criterion and aggregates the dis-
tance on the channel dimension. It is followed by an addi-tional spatial expansion that repeats the spatial information
based on the given channels. Finally, the inner weights vec-
13

--- PAGE 14 ---
Figure 6: Illustration of our proposed Boundary Privilege Knowledge Distillation schemes in terms of class Trimap IoU
metrics with PSPnet + Resnet18 network architecture over the Cityscapes validation set. It can be seen from the figure that
our method has different degrees of improvement for all categories, meanwhile , we have a significant improvement for
categories that are difficult to distinguish by boundaries.
tor is applied to corresponding categories to enhance the
learning capacity for hard edge samples. The EDM loss
terms are then finalized by overall weights adaption and
average reduction. Furthermore, the Edge Detection Mod-
ule is another crucial sub-component that employs multiple-
level edge masks by providing input and ground truth labels.
We utilize dilation and erosion to retrieve the edges, with the
hyperparameter, kernel size, controlling the edge width.
To address the computational pressure arising from a large
kernel, the compute iters is introduced for GPU mem-
ory optimization. Additionally, the edge detection module
employs average pooling as a downsampling policy, con-
sidering progressively decreasing importance from internal
boundaries to outlines.
C. Categorical trimap Performance
This section presents an evaluation of the categorical
Edge IoU using the PSPnet encoder and Resnet18 back-
bone architecture on the Cityscape validation set. Figure
6 and Table 8 illustrate the results. Our proposed meth-
ods demonstrated significant improvement in most classes
based on edge factors compared to the raw student model
and strong baseline channel-wise distillation method. We
also observed explicit improvement for categories that are
difficult to distinguish by boundaries. For instance, we
achieved a 10.31% improvement for the wall category and
a 6.69% improvement for the bus category.
D. Instance segmentation
We conducted experiments using SOLOv2 on the COCO
dataset to demonstrate the general adaptability of our
method. Specifically, we selected SOLOV2 [55] X-
101(DCN) as the teacher and Light SOLOV2 [55] R-18
as the student. Table 9 presents the results, which showthat our method improved the raw student by 6.5%, 9.5%,
7.5%, 6.9%, 8.8%, and 4.8% on the corresponding met-
rics. The results demonstrate that our distillation method
can easily adapt to instance segmentation tasks and out-
performs previous methods in the small-scale training set-
ting. While instance segmentation and semantic segmenta-
tion tasks have distinct differences, they share similar prop-
erties in that they predict masks for target senses and given
pixel-level annotations. During the experiments, we applied
knowledge distillation methods multiple times on pyramid
classification logits and masked representations, followed
by calculating the average across all levels of sub-terms to
obtain the distillation loss. From the numerical results, AP
metrics explicitly increased for small and medium objects.
However, there was no performance improvement for large
objects, indicating that our method has room for improve-
ment in instance segmentation tasks. As a future prospect,
we aim to adjust the current method and design a special-
ized loss term for instance-level knowledge distillation.
Methods AP AP 50AP75APSAPMAPL
T:SOLOv2-X101 41.7 63.2 45.1 18.0 45.0 61.6
S:SOLOv2-R18* 26.7 44.1 27.5 6.50 27.2 45.8
CWD 28.4 47.1 29.6 9.90 30.3 44.2
BPKD(Ours) 33.2 53.6 35.0 13.4 36.0 50.6
Table 9: The instance segmentation results presented in this
report were obtained on the COCO [32] validation set using
single-model results. All distillation methods and the stu-
dent network baseline were trained using a 1x schedule with
multiple-scale training disabled. The table below demon-
strates that our distillation method can easily adapt to in-
stance segmentation tasks and outperforms previous meth-
ods in a small-scale training setting.
14

--- PAGE 15 ---
(a)Image(b)W/odistillation(c)CWD(d)BPKD(Ours)(e)Teacher
Figure 7: Qualitative results on the ADE20K [71] validation set produced by PSPNet [70] and ResNet18 network architecture:
(a) Initial images, (b) w/o distillation scheme, (c) sate-of-the-art method channel-wise distillation [49], (d) BPKD our method,
(e) teacher. This figure shows that our methods segment the small complex objects with explicit boundaries. Zoom in for a
better view. In the second row, BPKD demonstrates superior segmentation results; for instance, the contour of the chair is
more distinct and the overall accuracy is enhanced. In the third, fourth, and last rows, the contours of the pillows are clearly
delineated, coming closer to the results generated by the Teacher. In the seventh row, the coffee table situated between the two
chairs is rendered with greater clarity, thereby suggesting that our method achieves commendable performance in resolving
the ambiguity among multiple proximate objects. These visual outcomes indicate that our approach not only improves
semantic boundaries but also leverages prior knowledge of contours and shapes to produce outstanding segmentation results.15

--- PAGE 16 ---
(a)Image(b)W/odistillation(c)CWD(d)BPKD(Ours)(e)Ground truth
Figure 8: Qualitative results on the Cityscapes [13] validation set produced by PSPNet [70] and ResNet18 network architec-
ture: (a) Initial images, (b) w/o distillation, (c) sate-of-the-art method channel-wise distillation [49], (d) BPKD our method,
(e) Ground truth. This figure shows that our methods segment the small complex objects with explicit boundaries. Zoom
in for a better view. In the first row, BPKD effectively addresses the issue of the front windshield of the bus being wrongly
segmented into multiple classes. In the second row, the approach enhances the contours of the upper and lower sections
of the road signs. The third row presents a particularly striking result in the segmentation of a train; through differentiated
supervision and distillation of the edges and main body, the train is clearly segregated from obstructions. In the fourth row,
distant pedestrians present a challenging case for segmentation and identification due to their far-off camera angle and sig-
nificant occlusions; despite these complexities, BPKD still manages to produce stable edges around the human figures. In
the remaining images, BPKD consistently shows improvements, achieving commendable segmentation outcomes on both
pedestrian paths and roadways.16

--- PAGE 17 ---
(a)Image(b)W/odistillation(c)CWD(d)BPKD(Ours)(e)Teacher
Figure 9: Qualitative results on the Pascal-Context [43] validation set produced by PSPNet [70] and ResNet18 network archi-
tecture: (a) Initial images, (b) w/o distillation scheme, (c) sate-of-the-art method channel-wise distillation [49] , (d) BPKD
our method, (e) teacher. This figure shows that our methods segment the small complex objects with explicit boundaries.
Zoom in for a better view. In the first row, BPKD exhibits more fine-grained edge segmentation compared to the non-distilled
student network. In the second row, neither the student network nor CWD manages to achieve effective segmentation of the
cup, owing to complex angles and environmental factors. Our approach, however, accomplishes accurate segmentation by
leveraging prior knowledge of the cup’s boundary and supervised learning on the main body of the cup. In the remaining
examples, BPKD demonstrates more intricate and advanced segmentation results, thereby revealing that under the influence
of edge loss, the entire network’s convergence space benefits from shape and spatial priors across different classes. This, in
turn, implicitly grants better main-body supervision and assistance, ultimately achieving state-of-the-art (SOTA) distillation
results in segmentation. 17
