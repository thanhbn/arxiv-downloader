# Tận dụng Mô hình Ngôn ngữ Lớn cho Hệ thống Gợi ý Đã được Tiền huấn luyện

Zhixuan Chu*1, Hongyan Hao*1, Xin Ouyang1, Simeng Wang1, Yan Wang1, Yue Shen1, Jinjie Gu1,
Qing Cui1, Longfei Li1, Siqiao Xue1, James Y Zhang1, Sheng Li2
1Ant Group
2University of Virginia
{chuzhixuan.czx, hongyanhao.hhy, xin.oyx, simeng.wsm, luli.wy, zhanying, jinjie.gujj, cuiqing.cq, longyao.llf, siqiao.xsq,
james.z }@antgroup.com, shengli@virginia.edu

## Tóm tắt

Những tiến bộ gần đây trong hệ thống gợi ý đã chuyển hướng sang việc đưa ra các gợi ý toàn diện và cá nhân hóa hơn bằng cách sử dụng mô hình ngôn ngữ lớn (LLM). Tuy nhiên, việc tích hợp hiệu quả kiến thức thường thức và khả năng lý luận của LLM vào hệ thống gợi ý vẫn là một vấn đề thách thức. Trong bài báo này, chúng tôi đề xuất RecSysLLM, một mô hình gợi ý đã được tiền huấn luyện mới dựa trên LLM. RecSysLLM duy trì khả năng lý luận và kiến thức của LLM trong khi tích hợp kiến thức miền gợi ý thông qua các thiết kế độc đáo về dữ liệu, huấn luyện và suy luận. Điều này cho phép RecSysLLM tận dụng các khả năng của LLM cho các tác vụ gợi ý trong một khung thống nhất, hiệu quả. Chúng tôi chứng minh hiệu quả của RecSysLLM trên các bộ dữ liệu chuẩn và các tình huống thực tế. RecSysLLM cung cấp một cách tiếp cận đầy hứa hẹn để phát triển các hệ thống gợi ý thống nhất bằng cách khai thác đầy đủ sức mạnh của các mô hình ngôn ngữ đã được tiền huấn luyện.

## Giới thiệu

Lĩnh vực gợi ý đã thu hút sự chú ý đáng kể trong những năm gần đây do khả năng thúc đẩy tăng trưởng kinh doanh và nâng cao sự tương tác của người dùng. Những tiến bộ gần đây trong hệ thống gợi ý đã chuyển hướng sang việc kết hợp thông tin đa dạng và phục vụ một loạt các tình huống ứng dụng rộng hơn, thay vì tập trung vào các kiến trúc chuyên biệt cho từng tác vụ cụ thể. Sự chuyển đổi này được thúc đẩy bởi nhu cầu về các gợi ý toàn diện và cá nhân hóa hơn, cũng như sự có sẵn của các nguồn dữ liệu và kiến thức mới (Geng et al. 2022; Chu et al. 2022; Hui et al. 2022; Sheu et al. 2021; Li and Zhao 2021; Jiang et al. 2022; Xue et al. 2021).

Thêm vào đó, với sự ra đời của Mô hình Ngôn ngữ Lớn (LLM) (Radford et al., 2019; Brown et al. 2020; Ouyang et al. 2022), chúng ta đã chứng kiến một sự bùng nổ chưa từng có trong các khả năng xử lý ngôn ngữ tự nhiên. Sức mạnh của LLM nằm ở khả năng hiểu và tạo ra ngôn ngữ giống con người. LLM cũng đã cho phép trích xuất kiến thức tiềm ẩn từ dữ liệu văn bản (Gu et al. 2023; Yoneda et al. 2023; Zhao et al. 2023). Khả năng mới tìm thấy này của LLM đã mở ra những con đường thú vị cho việc tích hợp thông tin ngữ nghĩa vào hệ thống gợi ý và cung cấp nhiều hiểu biết về sở thích và hành vi của người dùng (Shi et al. 2023; Zhao, Tan, and Mei 2022). Kết quả là, việc kết hợp LLM vào hệ thống gợi ý đã trở thành một bước quan trọng hướng tới việc cung cấp một mô hình mạnh mẽ và toàn diện cho các tác vụ gợi ý. Trong phần tiếp theo, chúng tôi sẽ thảo luận về các mô hình hệ thống gợi ý thế hệ mới từ hai hướng, tức là mô hình gợi ý đã được tiền huấn luyện thống nhất và sự kết hợp giữa LLM và mô hình gợi ý.

Một mặt, huấn luyện một mô hình gợi ý đã được tiền huấn luyện có thể giúp vượt qua những hạn chế của các phương pháp gợi ý hiện có đòi hỏi phải thiết kế các kiến trúc và mục tiêu huấn luyện chuyên biệt cho từng tác vụ. Các phương pháp gợi ý truyền thống đã tập trung vào một tác vụ duy nhất, như gợi ý sản phẩm cá nhân hóa, quảng cáo theo ngữ cảnh, phân khúc khách hàng, v.v., khiến chúng ít thích ứng với các tác vụ mới và hạn chế khả năng tổng quát hóa sang các miền mới. Bằng cách huấn luyện một mô hình gợi ý đã được tiền huấn luyện, chúng ta có thể tận dụng sức mạnh của các mô hình đã được tiền huấn luyện để học các biểu diễn có thể tổng quát hóa của hành vi người dùng và đặc điểm sản phẩm (Tsai et al. 2023; Zhao, Tan, and Mei 2022) có thể được áp dụng cho nhiều tác vụ gợi ý khác nhau. Nhìn chung, một mô hình gợi ý đã được tiền huấn luyện cung cấp một giải pháp linh hoạt và có thể mở rộng có thể được điều chỉnh cho nhiều tác vụ gợi ý khác nhau. Vì các tác vụ gợi ý thường chia sẻ một nhóm người dùng-mục chung, các tính năng, chuỗi hành vi và thông tin ngữ cảnh khác, chúng tôi tin rằng việc hợp nhất nhiều tác vụ gợi ý hơn nữa vào một khung thống nhất là đầy hứa hẹn để chúng có thể chuyển giao kiến thức một cách tiềm ẩn để hỗ trợ lẫn nhau và cho phép tổng quát hóa sang các tác vụ chưa thấy khác (Xie et al. 2022).

Mặt khác, việc tích hợp LLM vào hệ thống gợi ý có một số lợi thế đáng kể. Những lợi thế này liên quan đến các khả năng của LLM trong việc suy nghĩ, lý luận và khám phá các mối quan hệ tiềm ẩn trong dữ liệu văn bản dựa trên sự kéo theo của kiến thức nền phong phú và các chuỗi logic. (1) Bằng cách tận dụng thông tin ngữ nghĩa trong dữ liệu ngôn ngữ tự nhiên, LLM có thể giúp hệ thống gợi ý hiểu và suy luận mối quan hệ giữa các đặc điểm người dùng và chuỗi hành vi và giữa các thực thể trong chuỗi hành vi. Điều này cho phép hệ thống gợi ý hiểu nhu cầu và sở thích của người dùng một cách toàn diện hơn. (2) Một lợi ích khác của việc tích hợp LLM vào hệ thống gợi ý là khả năng tận dụng kiến thức tiềm ẩn được ẩn giấu trong các mô hình. LLM được huấn luyện trên lượng dữ liệu văn bản khổng lồ và có thể giúp hiểu các mối quan hệ giữa các khái niệm và ý tưởng khác nhau. Bằng cách kết hợp LLM vào hệ thống gợi ý, kiến thức tiềm ẩn này có thể được sử dụng để tạo ra các gợi ý đa dạng và logic hơn. Điều này có thể dẫn đến các gợi ý sáng tạo và bất ngờ hơn mà người dùng có thể chưa từng xem xét. (3) Bằng cách tận dụng các khả năng xử lý ngôn ngữ tự nhiên của LLM, các tác vụ gợi ý trước đây đòi hỏi các hệ thống chuyên biệt riêng biệt giờ đây có thể được tích hợp vào một khung thống nhất. Kiến thức đã được tiền huấn luyện và khả năng học vài mẫu của LLM cho phép các mô hình gợi ý được điều chỉnh nhanh chóng sang các miền mới với dữ liệu hạn chế. Nhìn chung, sức mạnh xử lý ngôn ngữ tự nhiên và tính linh hoạt của LLM có thể giúp hợp nhất nhiều tác vụ gợi ý hơn vào một khung thống nhất. Hơn nữa, một khảo sát toàn diện về gợi ý và LLM được cung cấp trong Phụ lục. Khảo sát này bao gồm động cơ thúc đẩy chúng, phát triển hiện tại và các thách thức.

Tuy nhiên, việc xây dựng một hệ thống gợi ý mạnh mẽ và tích hợp khai thác đầy đủ kiến thức to lớn và khả năng lý luận của các mô hình ngôn ngữ lớn đặt ra một số thách thức chính. Việc trực tiếp huấn luyện một mô hình gợi ý đã được tiền huấn luyện từ đầu không chỉ lãng phí thời gian và nỗ lực thu thập dữ liệu mà còn thiếu khả năng lý luận và kiến thức thường thức chung làm nền tảng cho các mô hình ngôn ngữ lớn hiện đại. Trong khi đó, việc trực tiếp tinh chỉnh một mô hình LLM đã được tiền huấn luyện trên dữ liệu gợi ý cũng có những nhược điểm. Dữ liệu gợi ý có những đặc điểm riêng biệt - như các thực thể cố định và hành vi người dùng tuần tự - khác với các tập văn bản thô được sử dụng để huấn luyện các mô hình ngôn ngữ. Do đó, việc tinh chỉnh có thể xóa bỏ nhiều khả năng cụ thể cho các tác vụ gợi ý. Vì vậy, chúng tôi đề xuất một mô hình gợi ý đã được tiền huấn luyện mới (RecSysLLM) dựa trên mô hình ngôn ngữ lớn đã được tiền huấn luyện thông qua các thiết kế độc đáo cho gợi ý trong ba giai đoạn, tức là giai đoạn dữ liệu, giai đoạn huấn luyện và giai đoạn suy luận. Mô hình của chúng tôi duy trì khả năng lý luận và kiến thức phong phú có trong các mô hình ngôn ngữ lớn trong khi tích hợp kiến thức cụ thể cho gợi ý. Nó trực tiếp kế thừa các tham số và khung của mô hình ngôn ngữ lớn gốc nhưng cũng thiết kế và mở rộng một số cơ chế trong giai đoạn dữ liệu (văn bản hóa và lấy mẫu), giai đoạn huấn luyện (che, vị trí và sắp xếp), và giai đoạn suy luận (điền vị trí động). Những sửa đổi này không loại bỏ việc token hóa, tham số, cấu trúc, hoặc kiến thức đã học trước đó trong LLM. Trên cơ sở này, dữ liệu gợi ý được sử dụng để tinh chỉnh nó. Lợi thế đáng kể của mô hình gợi ý đã được tiền huấn luyện này là nó có thể sử dụng khả năng lý luận và kiến thức phong phú của các mô hình ngôn ngữ lớn trong khi kết hợp kiến thức cụ thể miền của hệ thống gợi ý thông qua việc tinh chỉnh hiệu quả tham số của dữ liệu hồ sơ người dùng và chuỗi hành vi. Một lợi ích quan trọng khác của mô hình này là nó có thể được điều chỉnh dễ dàng cho các tác vụ con gợi ý downstream khác nhau. Chúng tôi đánh giá mô hình được đề xuất trên các bộ dữ liệu chuẩn rộng rãi và các tình huống thực tế. Kết quả thực nghiệm chứng minh hiệu quả của nó trong việc cải thiện chất lượng gợi ý. Nhìn chung, mô hình gợi ý đã được tiền huấn luyện được đề xuất của chúng tôi cung cấp một cách tiếp cận đầy hứa hẹn để xây dựng các hệ thống gợi ý hiệu quả, hiệu quả và thống nhất.

## Cơ chế Tiền huấn luyện RecSysLLM

Để tận dụng đầy đủ LLM và kiến thức miền trong các tác vụ gợi ý, chúng ta cần sửa đổi LLM và tinh chỉnh LLM hiện có để có được một mô hình gợi ý đã được tiền huấn luyện. Tuy nhiên, các mô hình ngôn ngữ lớn thông thường được huấn luyện trên kiến thức chung và tập văn bản mạch lạc, và khung của mô hình không được thiết kế cho dữ liệu chuỗi hành vi và các tác vụ gợi ý. Để giải quyết hai điểm này, chúng tôi thực hiện các sửa đổi từ ba giai đoạn, tức là giai đoạn dữ liệu, huấn luyện và suy luận, để chuyển đổi một mô hình ngôn ngữ đã được tiền huấn luyện thông thường thành một mô hình gợi ý đã được tiền huấn luyện. Toàn bộ khung được minh họa trong Hình 1. Mô hình gợi ý đã được tiền huấn luyện này đã được sử dụng trong các ứng dụng thực tế trong các tình huống tiếng Trung, vì vậy chúng tôi lấy GLM (Du et al. 2021) làm ví dụ để giới thiệu cơ chế tiền huấn luyện RecSysLLM, là song ngữ Trung-Anh. Mô hình của chúng tôi cũng có thể được điều chỉnh cho các mô hình ngôn ngữ lớn khác với những sửa đổi nhỏ.

### Giai đoạn Dữ liệu

Trong giai đoạn dữ liệu, việc văn bản hóa dữ liệu dạng bảng thường là cách tiếp cận dễ nhất và trực tiếp nhất để triển khai các mô hình ngôn ngữ lớn. Đối với việc tiền huấn luyện RecSysLLM, trước tiên chúng tôi văn bản hóa dữ liệu dạng bảng thông thường, như các đặc điểm người dùng được lưu trữ trong một bảng có hàng và cột thành văn bản. Vì các mô hình ngôn ngữ lớn ban đầu được huấn luyện trên dữ liệu văn bản, các đặc điểm dựa trên văn bản có thể dễ dàng được kết hợp với các chuỗi hành vi dựa trên văn bản và thông tin văn bản khác, điều này giúp mô hình của chúng tôi nắm bắt tốt hơn mối quan hệ giữa các đặc điểm và chuỗi hành vi. Ngoài ra, việc văn bản hóa dữ liệu dạng bảng cho phép linh hoạt hơn trong cách chúng được sử dụng trong các tác vụ tiếp theo.

So với các văn bản ngôn ngữ thông thường, các văn bản huấn luyện trong hệ thống gợi ý nên tính đến sở thích và ưu tiên của người dùng từ các thời kỳ khác nhau (Yu et al. 2019). Sở thích dài hạn thường ổn định và phản ánh sở thích chung của người dùng. Những sở thích này không thay đổi thường xuyên theo thời gian, nhưng chúng thiếu tính thời sự và có thể không phản ánh sở thích hiện tại. Mặt khác, sở thích ngắn hạn có xu hướng thay đổi thường xuyên theo thời gian và phản ánh sở thích hiện tại của người dùng nhiều hơn. Chúng tôi nhằm sử dụng các giai đoạn sở thích khác nhau để cung cấp các gợi ý chính xác và phù hợp cho người dùng, có thể cân bằng sở thích chung của người dùng với nhu cầu hiện tại của họ. Vì vậy, chúng tôi lấy mẫu các chuỗi hành vi trong sở thích dài hạn (~10%), sở thích trung hạn (~30%), và sở thích ngắn hạn (~60%). Sở thích dài hạn nắm bắt sở thích của người dùng đã duy trì nhất quán trong một thời gian dài, thường kéo dài qua nhiều tháng hoặc năm. Sở thích trung hạn nắm bắt sở thích của người dùng đã phát triển và thay đổi trong một thời gian ngắn hơn, thường kéo dài qua nhiều tuần hoặc tháng. Sở thích ngắn hạn có thể cải thiện độ chính xác gợi ý bằng cách cung cấp cho hệ thống sở thích gần đây nhất của người dùng, kéo dài qua nhiều ngày hoặc giờ.

### Giai đoạn Huấn luyện

Để nhất quán với kiến trúc của GLM, mô hình của chúng tôi vẫn được huấn luyện bằng cách tối ưu hóa một mục tiêu điền chỗ trống tự hồi quy dựa trên một văn bản đầu vào x = [x1,···, xn]. Khác với văn bản ngôn ngữ chung trong GLM, văn bản đầu vào của chúng tôi được cấu thành từ các đặc điểm người dùng và chuỗi hành vi. Mặc dù các đặc điểm người dùng và chuỗi hành vi đã được văn bản hóa cũng được cấu thành từ nhiều token, chúng thường đại diện cho một ý nghĩa hoàn chỉnh như một tổng thể. Nếu chúng được chia thành các phần khác nhau, như văn bản thông thường, chúng sẽ mất ý nghĩa độc đáo của mình. Ngoài ra, sức mạnh của LLM đến từ cách nó token hóa và xử lý văn bản. Nó đã được huấn luyện trên một lượng dữ liệu khổng lồ và đã học cách nhận ra các mẫu và mối quan hệ giữa các token, cho phép nó xác định các thực thể một cách chính xác và trích xuất thông tin. Nếu chúng ta tạo ra một phương pháp token hóa mới, chúng ta sẽ mất sức mạnh của LLM. Vì vậy, để duy trì sức mạnh của LLM và bổ sung kiến thức mới trong dữ liệu gợi ý, tốt nhất là tận dụng việc token hóa hiện có và tăng cường nó với thông tin và khả năng bổ sung thay vì tạo ra một token hóa mới. Trong phần tiếp theo, chúng tôi gọi các thuộc tính trong đặc điểm người dùng và các mục trong chuỗi hành vi là thực thể, có nghĩa là chúng là các đơn vị hoàn chỉnh và có ý nghĩa cố định. Vì vậy, như được hiển thị trong "Thực thể" của Hình 1, dữ liệu của chúng tôi được cấu thành từ văn bản ngôn ngữ đơn giản và thực thể, trong đó (x1, x2, và x3) đã được hợp nhất để tạo thành e1 và (x6 và x7) để tạo thành e2. x4 và x5 là các token riêng biệt.

**Cơ chế Che.** Để tiêm kiến thức mới của các tác vụ gợi ý dựa trên LLM gốc, chúng tôi tuân theo nguyên tắc trong LLM và thiết kế cơ chế che mới và các chiến lược vị trí. Tương tự như GLM (Du et al. 2021), nhiều đoạn văn bản {s1,···,sm} được lấy mẫu, trong đó mỗi đoạn si tương ứng với một chuỗi các token liên tiếp [si,1,···, si,li] trong x. Mỗi đoạn được thay thế bằng một token [MASK] duy nhất. Văn bản còn lại và [MASK] tạo thành một văn bản bị hỏng xcorrupt. Trong GLM, vì không có sự tồn tại của thực thể, các token có thể được lấy mẫu ngẫu nhiên thành các đoạn. Tuy nhiên, trong mô hình của chúng tôi, nhiều token liên tiếp tạo thành một thực thể không nên được chia thành các phần khác nhau. Nói cách khác, các token của một thực thể được coi như một tổng thể. Cơ chế [MASK] sẽ không phá vỡ các thực thể hoàn chỉnh, điều này sẽ làm nổi bật cấu trúc tổng thể của các thực thể và giúp nắm bắt mối quan hệ tương hỗ giữa các thực thể. Ví dụ, như được hiển thị trong "Mặt nạ" của Hình 1, x1, x2, và x3 tạo thành e1 được chặn như một tổng thể và token đơn x5 cũng được chặn. Vì vậy, chúng tôi tạo thành xcorrupt với [M], x4, [M], x6, và x7 trong "Chia" của Hình 1.

Tương thích với các tác vụ xử lý ngôn ngữ tự nhiên khác nhau, chúng tôi áp dụng thiết lập tiền huấn luyện đa tác vụ (Du et al. 2021) với [M] cấp thực thể, [sM] cấp câu, và [gM] cấp tài liệu. Cụ thể, cấp thực thể đề cập đến việc che ngẫu nhiên các đoạn token liên tiếp từ văn bản đầu vào, theo ý tưởng của tự mã hóa, nắm bắt sự phụ thuộc lẫn nhau giữa các thực thể. Cấp câu hạn chế rằng các đoạn bị che phải là câu đầy đủ. Cấp tài liệu là lấy mẫu một đoạn duy nhất có độ dài được lấy mẫu từ một phân phối đều trên 50%–100% độ dài gốc. Mục tiêu nhằm tạo văn bản dài.

**Thứ tự Đoạn.** Chúng tôi thực hiện mục tiêu điền chỗ trống tự hồi quy với các kỹ thuật sau. Đầu vào x được chia thành hai phần: một phần là văn bản bị hỏng xcorrupt, và phần khác bao gồm các đoạn bị che. Mô hình của chúng tôi tự động học một bộ mã hóa hai chiều cho phần đầu và một bộ giải mã một chiều cho phần thứ hai trong một mô hình thống nhất. Mô hình dự đoán các token bị thiếu trong các đoạn từ văn bản bị hỏng theo cách tự hồi quy, có nghĩa là khi dự đoán các token bị thiếu trong một đoạn, mô hình có quyền truy cập vào văn bản bị hỏng và các đoạn đã dự đoán trước đó. Thay vì hoán vị ngẫu nhiên thứ tự của các đoạn trong GLM gốc (Du et al. 2021), chúng tôi giữ tất cả các đoạn theo thứ tự thời gian để duy trì mối quan hệ tương hỗ giữa các thực thể khác nhau. Chính thức, chúng tôi định nghĩa mục tiêu tiền huấn luyện của một chuỗi chỉ số có độ dài m [1,2, ..., m] là:

$$\sum_{i=1}^{m} \log p(s_i | x_{corrupt}, s_1, ..., s_{i-1}; \theta)$$ (1)

**Mã hóa Vị trí.** Để cho phép tạo tự hồi quy, mỗi đoạn được đệm với các token đặc biệt [START] và [END], cho đầu vào và đầu ra, tương ứng. Để nhất quán với LLM gốc, chúng tôi không thể tùy ý sửa đổi, thêm hoặc giảm các chiến lược vị trí gốc. Vì vậy, chúng tôi mở rộng mã hóa vị trí 2D (Du et al. 2021) dựa trên thực thể. Cụ thể, mỗi token được mã hóa với hai id vị trí, tức là id vị trí giữa và trong.

Id vị trí giữa đại diện cho vị trí trong văn bản bị hỏng xcorrupt. Đối với các đoạn bị che, đó là vị trí của token [MASK] tương ứng. Đối với id vị trí trong, chúng tôi tuân theo ý nghĩa cốt yếu trong LLM gốc, vẫn đề cập đến vị trí trong. Thay vì phạm vi của toàn bộ đoạn, chúng tôi mở rộng nó thành độ chi tiết tốt hơn. Đối với các thực thể, nó đại diện cho mối quan hệ trong giữa các thực thể. Như được hiển thị trong Hình 1, đối với các token riêng biệt (không trong thực thể) trong phần mã hóa ([M], x4, [M]), id vị trí trong của chúng là 0. Đối với các token liên tiếp trong thực thể (x6 và x7), chúng được đánh số theo thứ tự thời gian. Đối với các token trong phần điền chỗ trống tự hồi quy, chúng dao động từ 1 đến độ dài của thực thể bao gồm [S], như (thực thể: [S], x1, x2, x3 → 1,2,3,4) và (token độc lập: [S], x5 → 1,2). Hai id vị trí được chiếu thành hai vector thông qua các bảng nhúng có thể học, cả hai đều được thêm vào các nhúng token đầu vào.

### Giai đoạn Suy luận

Vì mô hình đã được tiền huấn luyện của chúng tôi được thiết kế để phù hợp với các tác vụ downstream khác nhau, độ dài của văn bản được tạo nên không xác định trước và linh hoạt cho các tác vụ khác nhau. Hơn nữa, do sự tồn tại của thực thể, id vị trí trong đại diện cho vị trí tương đối của thực thể. Như được hiển thị trong "Giai đoạn Suy luận" của Hình 1, chúng ta không thể chỉ định id vị trí trong trước khi điền chỗ trống tự hồi quy. Do đó, chúng tôi đã thiết kế một cơ chế vị trí động cho các sửa đổi che và vị trí được thực hiện trong giai đoạn suy luận. Nó có thể tiến hành đánh giá tự hồi quy để xác định và bổ sung id vị trí trong từng cái một khi mỗi token được tạo trong quy trình tạo tự hồi quy. Cụ thể, chúng tôi thiết lập một nhóm thực thể trước, lưu trữ tất cả các token của các thực thể tồn tại trong tác vụ gợi ý của chúng tôi. Khi một token được tạo, nó sẽ được đánh giá là một phần của thực thể hay không. Chúng tôi sử dụng thuật toán Trie (Bodon và Ronyai 2003) để kiểm tra xem token được tạo và token trước có thuộc cùng một thực thể hay không, đây là cấu trúc dữ liệu cây được sử dụng để định vị các khóa cụ thể từ trong một tập hợp. Nếu chúng thuộc về một thực thể, id vị trí trong sẽ tiếp tục tăng. Ngược lại, nó sẽ bắt đầu lại từ 1. Quy trình chi tiết được minh họa trong Hình 2.

## Thí nghiệm

### Thiết lập Thí nghiệm

**Bộ dữ liệu.** Chúng tôi đánh giá phương pháp của mình trên ba bộ dữ liệu thương mại điện tử thực tế từ Amazon.com, bao gồm các danh mục Thể thao & Ngoài trời, Làm đẹp, và Đồ chơi & Trò chơi. Các bộ dữ liệu chứa đánh giá và nhận xét của người dùng từ năm 2019, cùng với hồ sơ giao dịch từ ngày 1 tháng 1 đến ngày 31 tháng 12 (Zhou et al. 2020; Xue et al. 2022, 2023). Thống kê chính của các bộ dữ liệu kết quả được cung cấp trong Bảng 1.

**Chỉ số.** Theo các thí nghiệm trong (Geng et al. 2022), chúng tôi bao gồm năm họ tác vụ khác nhau – đánh giá, gợi ý tuần tự, giải thích, nhận xét, và gợi ý trực tiếp để tạo điều kiện cho việc tiền huấn luyện đa tác vụ cho gợi ý. Đối với dự đoán đánh giá, chúng tôi áp dụng Căn bậc hai Trung bình Sai số (RMSE) và Sai số Tuyệt đối Trung bình (MAE) làm chỉ số đánh giá. Đối với các tác vụ gợi ý tuần tự và gợi ý trực tiếp, chúng tôi sử dụng Tỷ lệ Trúng top-k (HR@k) và Độ lợi Tích lũy Chiết khấu Chuẩn hóa (NDCG@k) để đánh giá hiệu suất và báo cáo HR@1, 5, 10 và NGCG@5, 10. Đối với tạo giải thích và tóm tắt nhận xét, chúng tôi đánh giá các phương pháp khác nhau với BLEU-4, ROUGE-1, ROUGE-2, và ROUGE-L. Giá trị thấp hơn của RMSE và MAE cho thấy hiệu suất tốt hơn, trong khi giá trị cao hơn được ưa thích cho tất cả các chỉ số khác. Trong tất cả các bảng kết quả, số in đậm đại diện cho hiệu suất tốt nhất, trong khi số gạch dưới đề cập đến hiệu suất tốt thứ hai.

**Đường cơ sở cho Nhiều Tác vụ** Để chứng minh năng lực trên một loạt các tác vụ liên quan đến gợi ý, chúng tôi áp dụng các phương pháp đại diện tương tự như (Geng et al. 2022) cho các tác vụ khác nhau, như Dự đoán Đánh giá (MF (Koren, Bell, và Volinsky 2009) và MLP (Cheng et al. 2016)), Gợi ý Trực tiếp (BPR-MF (Rendle et al. 2009), BPR-MLP (Cheng et al. 2016), và SimpleX (Mao et al. 2021)), Gợi ý Tuần tự (Caser (Tang và Wang 2018), HGN (Ma, Kang, và Liu 2019), GRU4Rec (Hidasi et al. 2016), BERT4Rec (Sun et al. 2019), FDSA (Zhang et al. 2019), SASRec (Kang và McAuley 2018), và S3-Rec (Zhou et al. 2020)), Tạo Giải thích (Attn2Seq (Dong et al. 2017), NRT (Li et al. 2017), PETER (Li, Zhang, và Chen 2021), và PETER+), và tóm tắt nhận xét (T0(Sanh et al. 2022) và GPT-2(Radford et al. 2019)). Các đường cơ sở chi tiết được cung cấp trong Phụ lục.

### Triển khai

Để tạo điều kiện cho việc tiền huấn luyện đa tác vụ dựa trên prompt cho gợi ý, Geng et al. (2022) đã tạo ra một bộ sưu tập các mẫu prompt cá nhân hóa. Bộ sưu tập bao gồm năm họ tác vụ khác nhau – đánh giá, gợi ý tuần tự, giải thích, nhận xét, và gợi ý trực tiếp. Các prompt bao gồm các trường cá nhân hóa cho người dùng và mục để giúp mô hình khám phá sở thích người dùng-mục. Đối với dự đoán đánh giá, prompt yêu cầu dự đoán đánh giá hoặc sở thích của người dùng cho một mục. Đối với gợi ý tuần tự, prompt yêu cầu dự đoán mục tiếp theo mà người dùng sẽ tương tác. Đối với giải thích, prompt yêu cầu tạo văn bản giải thích sở thích của người dùng. Đối với nhận xét, prompt tóm tắt hoặc dự đoán đánh giá từ nhận xét. Đối với gợi ý trực tiếp, prompt hỏi có nên gợi ý một mục cho người dùng hay không. Bộ sưu tập hoàn chỉnh các prompt cá nhân hóa với ví dụ được cung cấp trong Phụ lục của (Geng et al. 2022). Những prompt này cho phép xây dựng các ví dụ huấn luyện đa dạng từ dữ liệu thô cho việc pertaining đa tác vụ. Chúng tôi tiền huấn luyện RecSysLLM của mình với các ví dụ huấn luyện đa dạng với các mẫu prompt khác nhau từ tất cả năm họ tác vụ để xác minh khả năng học đa tác vụ của nó. Bên cạnh đó, chúng tôi áp dụng một phần prompt trong mỗi họ tác vụ cho đánh giá zero-shot trong khi tất cả các prompt còn lại được sử dụng cho việc tiền huấn luyện prompted đa tác vụ. Kết quả là, chúng tôi có thể không chỉ so sánh hiệu suất qua các tác vụ gợi ý khác nhau mà còn đánh giá khả năng tổng quát hóa zero-shot trên các prompt chưa thấy.

Mô hình RecSysLLM của chúng tôi cho các tác vụ tiếng Anh này tận dụng mô hình GLM-10B mạnh mẽ cho tiếng Anh (Du et al. 2021) làm nền tảng. GLM là một Mô hình Ngôn ngữ Chung được tiền huấn luyện với mục tiêu điền chỗ trống tự hồi quy và có thể được tinh chỉnh cho các tác vụ hiểu và tạo ngôn ngữ tự nhiên khác nhau. Cách tiếp cận của chúng tôi xây dựng trên nền tảng GLM-10B đã được tiền huấn luyện này bằng cách sử dụng một phương pháp tinh chỉnh hiệu quả tham số được gọi là LoRA (Thích ứng Thứ hạng Thấp) (Hu et al. 2021) để điều chỉnh mô hình cho các tác vụ gợi ý cụ thể của chúng tôi. LoRA cho phép tùy chỉnh hiệu quả mô hình GLM-10B khổng lồ cho các miền chuyên biệt bằng cách học một phân tách chiều thấp của việc cập nhật mô hình. Điều này cho phép chúng tôi khai thác kiến thức ngôn ngữ rộng của GLM-10B trong khi hiệu chỉnh nó cho các mục tiêu RecSysLLM của chúng tôi. Chúng tôi tiêm các ma trận phân tách thứ hạng có thể huấn luyện vào mỗi lớp query keyvalue, dense, dense hto4h và dense 4htoh của kiến trúc Transformer trong GLM-10B. Chúng tôi tiền huấn luyện RecSysLLM của mình trong tám epoch với tối ưu hóa AdamW (Loshchilov và Hutter 2017) trên bốn GPU NVIDIA RTX A100. Để đạt được việc sử dụng bộ nhớ hiệu quả và huấn luyện phân tán, chúng tôi sử dụng module DeepSpeed (Rasley et al. 2020). Kích thước batch được đặt là 32 trên mỗi GPU. Chúng tôi đặt tốc độ học tối đa là 1×10^-5 và sử dụng chiến lược warmup để điều chỉnh tốc độ học. Ngoài ra, chúng tôi đặt độ dài tối đa của token đầu vào là 1024.

### Hiệu suất

Chúng tôi tiền huấn luyện RecSysLLM của mình trên một tập hợp đa dạng các ví dụ huấn luyện sử dụng các mẫu prompt khác nhau qua tất cả năm họ tác vụ. Điều này là để xác minh kỹ lưỡng khả năng học đa tác vụ của nó. Kết quả trong Bảng 2-7 chứng minh rằng đối với các tác vụ với mẫu prompt đã thấy, mô hình của chúng tôi đạt được những kết luận tương tự như mô hình P5 và đạt hiệu suất tương đương hoặc vượt trội. Tuy nhiên, chúng tôi đã vui mừng khám phá ra rằng đối với các mẫu prompt chưa thấy theo cách zero-shot, mô hình của chúng tôi vượt trội đáng kể so với P5.

(1) Từ Bảng 2, đối với dự đoán đánh giá, RecSysLLM của chúng tôi có hiệu suất tương tự trên prompt trong tập dữ liệu huấn luyện, nhưng nó có RMSE và MAE tốt hơn trên cả ba bộ dữ liệu so với P5 trong thiết lập zero-shot. Điều này phản ánh rằng RecSysLLM của chúng tôi kế thừa khả năng hiểu ngữ nghĩa của LLM trên các prompt chưa thấy, điều này đáp ứng kỳ vọng của chúng tôi đối với LLM.

(2) Trong Bảng 4, đối với gợi ý tuần tự, RecSysLLM của chúng tôi vượt trội P5 trên Beauty và Toys. Nó có hiệu suất tốt hơn P5 trên các prompt chưa thấy theo cách zero-shot. Kết quả cho thấy RecSysLLM của chúng tôi thu được kiến thức giữa và trong thực thể và đưa ra dự đoán hợp lý hơn.

(3) Như được hiển thị trong Bảng 5, RecSysLLM của chúng tôi thể hiện hiệu suất vượt trội trong tác vụ tạo giải thích, cả có và không có gợi ý dựa trên tính năng. Những cải thiện lớn trong khả năng xử lý ngôn ngữ tự nhiên của LLM là nền tảng cho hiệu suất mạnh mẽ này. Hơn nữa, sự gia tăng đáng kể trong điểm số khi có gợi ý làm nổi bật vai trò quan trọng của kỹ thuật prompt trong việc khai thác đầy đủ các khả năng của các mô hình ngôn ngữ lớn. Thông qua thiết kế prompt và sức mạnh tạo sinh của LLM, hệ thống của chúng tôi đạt được kết quả tiên tiến nhất trong tác vụ thách thức này.

(4) Kết quả tóm tắt nhận xét tiếp tục chứng minh sự vượt trội của RecSysLLM, như được hiển thị trong Bảng 6. Mặc dù có ít tham số hơn T0 (7 tỷ so với 11 tỷ), mô hình của chúng tôi đạt hiệu suất cao hơn trên tất cả các chỉ số đánh giá. Những lợi ích này so với các đường cơ sở mạnh như T0 nhấn mạnh tính hiệu quả và hiệu quả của cách tiếp cận của chúng tôi. Khả năng tạo ra các bản tóm tắt chất lượng cao với ít tham số hơn làm nổi bật sức mạnh của phương pháp của chúng tôi, mang lại hiệu suất mạnh mà không cần các mô hình cực kỳ lớn.

(5) Đối với tác vụ gợi ý trực tiếp, chúng tôi đánh giá trên các prompt câu hỏi mở để kiểm tra khả năng gợi ý tạo sinh. Kết quả được minh họa trong Bảng 7. RecSysLLM của chúng tôi vượt trội P5 trên hầu hết các chỉ số đánh giá cho tác vụ này. Mô hình simpleX là một đường cơ sở lọc cộng tác mạnh, nhưng RecSysLLM đạt được xếp hạng mục top-1 tốt hơn so với simpleX.

Để phân tích thêm khoảng cách hiệu suất giữa mô hình P5 và phương pháp được đề xuất của chúng tôi, chúng tôi đã tiến hành kiểm tra sâu dữ liệu huấn luyện. Bảng 3 minh họa rằng trong mô hình P5, các mục chỉ đơn giản được đại diện bằng ID số dựa trên thứ tự xuất hiện của chúng trong bộ dữ liệu. Loại đại diện đơn giản này không thể nắm bắt thông tin ngữ nghĩa về các mục. Ngược lại, mô hình RecSysLLM của chúng tôi đại diện tất cả các mục dưới dạng chuỗi văn bản. Đại diện văn bản cho phép mô hình ngôn ngữ lớn của chúng tôi hiểu và nắm bắt các mối quan hệ tương hỗ tinh tế giữa các mục hiệu quả hơn nhiều. Chúng tôi tin rằng đây là lý do chính tại sao mô hình của chúng tôi vượt trội P5 trong hầu hết các trường hợp. Đại diện văn bản trong mô hình của chúng tôi trao quyền cho nó tiếp thu chi tiết ngữ nghĩa và xác định các kết nối có ý nghĩa không thể được suy ra chỉ từ ID.

### Ứng dụng trong bộ dữ liệu thực tế

**Bộ dữ liệu**

Dữ liệu được sử dụng trong công việc này được thu thập từ Alipay, một nền tảng thanh toán di động ở Trung Quốc. Chúng tôi trích xuất nhật ký hành vi người dùng, bao gồm hóa đơn, truy vấn tìm kiếm và lượt truy cập trang cho một số tác vụ gợi ý. Mỗi chuỗi người dùng bao gồm 500 tương tác gần đây nhất của người dùng, kéo dài hơn một năm lịch sử cho một số người dùng. Các chuỗi người dùng được sử dụng để mô hình hóa sở thích người dùng đang phát triển và nắm bắt cả sở thích dài hạn và ngắn hạn. Tập huấn luyện chứa 200.000 chuỗi, và tập kiểm tra chứa 10.000 chuỗi. Bộ dữ liệu thực tế quy mô lớn cho phép mô hình hóa hành vi và sở thích người dùng phức tạp cho các tác vụ gợi ý khác nhau. Các danh mục phân cấp và tương tác tuần tự cung cấp tín hiệu phong phú để hiểu sở thích người dùng.

**Chi tiết Triển khai**

Mô hình RecSysLLM của chúng tôi cho các tác vụ tiếng Trung tận dụng mô hình ChatGLM-6B mạnh mẽ (Du et al. 2021) làm nền tảng. ChatGLM-6B là một mô hình ngôn ngữ song ngữ mã nguồn mở với 6,2 tỷ tham số, được huấn luyện trên một tập văn liệu nghìn tỷ token bao gồm chủ yếu văn bản tiếng Trung với một số tiếng Anh. Kiến trúc mô hình dựa trên khung Mô hình Ngôn ngữ Chung (GLM). Tương tự, cách tiếp cận của chúng tôi xây dựng trên nền tảng ChatGLM-6B đã được tiền huấn luyện này bằng cách sử dụng LoRA để điều chỉnh mô hình cho các tác vụ hệ thống gợi ý cụ thể của chúng tôi. Chúng tôi đặt thứ hạng của Lora là 8, là một hệ số phù hợp được chọn bởi nghiên cứu ablation.

**Gợi ý Tuần tự.**

**Mô tả Tác vụ.** Trong phần này, chúng tôi tiến hành hai tác vụ gợi ý tuần tự để đánh giá hiệu suất của mô hình, tức là dự đoán mục tiếp theo và gợi ý ứng viên. Đối với dự đoán mục tiếp theo, mô hình trực tiếp dự đoán mục tiếp theo mà người dùng sẽ tương tác dựa trên tương tác lịch sử và hồ sơ của họ. Đối với gợi ý ứng viên, cho lịch sử tương tác của người dùng, hồ sơ và một danh sách các mục ứng viên trong đó chỉ có một là tích cực, mô hình chọn mục tiếp theo chính xác. Chúng tôi đã so sánh mô hình của mình trên các bộ dữ liệu Amazon Sports, Beauty và Toys và chứng minh các khả năng gợi ý vượt trội so với các hệ thống gợi ý cơ sở khác. Ở đây, chúng tôi so sánh RecSysLLM của mình với các mô hình tạo sinh mạnh mẽ ChatGPT và GPT-4 mới được công bố. Chúng tôi cũng so sánh phương pháp của mình với một cách tiếp cận tinh chỉnh cơ bản của ChatGLM trên các tác vụ gợi ý của chúng tôi. Điều này cho phép chúng tôi phân tích những cải thiện đạt được bởi các kỹ thuật chuyên biệt của chúng tôi được điều chỉnh cho các hệ thống gợi ý dựa trên LLM. Bằng cách đánh giá so với một đường cơ sở tinh chỉnh đơn giản, chúng tôi có thể định lượng lợi ích của cách tiếp cận được đề xuất của chúng tôi và chứng minh rằng các lựa chọn kiến trúc và phương pháp huấn luyện của chúng tôi mang lại lợi thế có ý nghĩa về hiệu suất gợi ý so với việc chỉ tinh chỉnh một mô hình ngôn ngữ lớn sẵn có.

**Dự đoán Mục Tiếp theo.** Kết quả trong Bảng 8 chứng minh rằng đối với dự đoán mục tiếp theo, RecSysLLM của chúng tôi đạt hiệu suất ngang bằng với ChatGPT, với cả hai vượt trội đáng kể so với việc tinh chỉnh ChatGLM ngây thơ và GPT-4. Đây là một kết quả đáng ngạc nhiên, vì chúng tôi mong đợi mô hình GPT-4 lớn hơn sẽ đạt hiệu suất vượt trội so với ChatGPT trong tác vụ gợi ý này do kích thước tham số lớn hơn và quy mô tiền huấn luyện. Tuy nhiên, GPT-4 không thể hiện kết quả mạnh đặc biệt và không vượt trội quyết định so với ChatGPT. Có một số giải thích tiềm năng cho việc tại sao GPT-4 kém hiệu suất so với kỳ vọng trong dự đoán mục tiếp theo. Thứ nhất, bộ dữ liệu và phương pháp đánh giá được sử dụng cho tác vụ này có thể không hoàn toàn khai thác điểm mạnh của GPT-4 trong các lĩnh vực như học vài mẫu và gợi lại kiến thức. Thứ hai, các khả năng tạo sinh mạnh mẽ hơn của GPT-4 có thể đã khiến nó phân kỳ quá xa khỏi các phân phối chặt chẽ của dữ liệu gợi ý. Có thể có sự không khớp giữa các kỹ năng tạo ngôn ngữ tự nhiên rộng của GPT-4 và dự đoán chuyên biệt được yêu cầu bởi tác vụ hệ thống gợi ý. Tóm lại, RecSysLLM chuyên biệt của chúng tôi chứng minh rằng việc chỉ sử dụng một mô hình ngôn ngữ được tiền huấn luyện lớn hơn không phải là con đường duy nhất để cải thiện hiệu suất gợi ý. Kiến trúc mô hình và mục tiêu tiền huấn luyện cũng đóng một vai trò quan trọng. Bằng cách thiết kế một mô hình cụ thể cho gợi ý, tập trung việc tiền huấn luyện vào dữ liệu gợi ý, và ràng buộc chặt chẽ việc tinh chỉnh cuối cùng, RecSysLLM của chúng tôi có thể bằng hoặc vượt hiệu suất của ngay cả các mô hình ngôn ngữ chung lớn hơn nhiều như GPT-4 cho dự đoán mục tiếp theo. Những kết quả này làm nổi bật tầm quan trọng của thiết kế mô hình chuyên biệt ngoài quy mô để thúc đẩy hệ thống gợi ý.

**Gợi ý Ứng viên.** Đối với gợi ý ứng viên trong Bảng 9, RecSysLLM của chúng tôi liên tục vượt trội cả ChatGPT và việc tinh chỉnh ChatGLM ngây thơ qua các chỉ số. Điều này chứng minh hiệu quả của cách tiếp cận chuyên biệt của chúng tôi cho tác vụ này. Trái ngược với kết quả mục tiếp theo, lần này, GPT-4 đạt hiệu suất tốt nhất tổng thể trong gợi ý ứng viên. Trong gợi ý ứng viên, cho lịch sử tương tác của người dùng, hồ sơ và một danh sách các mục ứng viên trong đó chỉ có một là tương tác tiếp theo thực tế, mô hình phải chọn mục chính xác từ các ứng viên. Với một tập hợp tùy chọn bị ràng buộc được cung cấp, GPT-4 có thể phát huy đầy đủ khả năng lý luận và suy luận mạnh mẽ của mình. Tập hợp lựa chọn hạn chế ngăn xu hướng tạo sinh của GPT-4 dẫn nó đi sai hướng. Kết quả là, GPT-4 có thể tận dụng quy mô và tiền huấn luyện của mình để đạt hiệu suất tốt nhất tổng thể trong gợi ý ứng viên. Tóm lại, bằng cách cung cấp cho GPT-4 một tập hợp ứng viên tập trung, chúng ta có thể khai thác điểm mạnh của nó trong lý luận logic trong khi tránh tạo quá mức. Điều này cho phép GPT-4 đạt kết quả tiên tiến nhất trong gợi ý ứng viên, thể hiện lợi ích của quy mô và tiền huấn luyện của nó. RecSysLLM chuyên biệt của chúng tôi vẫn vượt trội các mô hình ngôn ngữ chung trong tác vụ này, chứng minh giá trị của mô hình hóa cụ thể cho gợi ý. Nhưng những kết quả này làm nổi bật cách các LM tạo sinh lớn như GPT-4 có thể xuất sắc khi được thiết lập đúng cách.

## Kết luận

Trọng tâm của bài báo này là thiết kế một mô hình mới của việc tiền huấn luyện các mô hình gợi ý dựa trên các mô hình ngôn ngữ lớn. Chúng tôi giới thiệu một cơ chế che mới, thứ tự đoạn và mã hóa vị trí để tiêm kiến thức giữa và trong thực thể vào LLM. Mặc dù phương pháp của chúng tôi tuân theo kiến trúc của các mô hình ngôn ngữ tạo sinh (GLM) ở một mức độ nào đó, các ý tưởng cốt lõi của các thiết kế đặc biệt cho thực thể trong các tác vụ gợi ý có thể được mở rộng sang các mô hình ngôn ngữ lớn khác. Các thí nghiệm được tiến hành trên các bộ dữ liệu công cộng và công nghiệp chứng minh hiệu quả và tiềm năng của mô hình được đề xuất của chúng tôi trên hệ thống gợi ý và các ứng dụng liên quan. Kết quả cho thấy cải thiện so với các đường cơ sở mạnh, cho thấy rằng việc mã hóa các mối quan hệ thực thể trong quá trình tiền huấn luyện có thể cải thiện hiệu suất downstream một cách có ý nghĩa. Trong khi chúng tôi xác thực cách tiếp cận của mình trên một tập hợp bộ dữ liệu được chọn, các thí nghiệm thêm trên một loạt tác vụ rộng hơn sẽ tiết lộ tốt hơn điểm mạnh và hạn chế của phương pháp. Đặc biệt, việc đánh giá cách tiếp cận qua một tập hợp miền đa dạng hơn có thể làm sáng tỏ mức độ mạnh mẽ của các biểu diễn đã học. Ngoài ra, từ quan điểm suy luận nhân quả (Yao et al. 2021; Chu et al. 2023), có khả năng có những cải thiện thêm về cách các kết nối ngữ nghĩa giữa các thực thể được nắm bắt và tiêm vào mô hình.
