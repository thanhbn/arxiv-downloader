# 2405.15613.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/dataset-curation/2405.15613.pdf
# Kích thước tệp: 6045479 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Curation Dữ liệu Tự động cho Học Tự giám sát:
Một Phương pháp Dựa trên Phân cụm

Huy V. Vo1 Vasil Khalidov1 Timothée Darcet1,2 Théo Moutakanni1,3 Nikita Smetanin1
Marc Szafraniec1 Hugo Touvron1 Camille Couprie1 Maxime Oquab1 Armand Joulin4
Hervé Jégou1 Patrick Labatut1 Piotr Bojanowski1

1Meta Fundamental AI Research (FAIR) 2INRIA 3Université Paris Saclay 4Google
Code: https://github.com/facebookresearch/ssl-data-curation

Tóm tắt

Các đặc trưng tự giám sát là nền tảng của các hệ thống học máy hiện đại. Chúng thường được tiền huấn luyện trên các bộ sưu tập dữ liệu mà việc xây dựng và curation thường đòi hỏi nỗ lực con người đáng kể. Quy trình thủ công này có một số hạn chế tương tự như những gì gặp phải trong học có giám sát, ví dụ, việc lựa chọn dữ liệu từ cộng đồng là tốn kém và mất thời gian, ngăn cản việc mở rộng quy mô kích thước tập dữ liệu. Trong công trình này, chúng tôi xem xét vấn đề curation tự động các tập dữ liệu chất lượng cao cho tiền huấn luyện tự giám sát. Chúng tôi cho rằng các tập dữ liệu như vậy nên lớn, đa dạng và cân bằng, và đề xuất một phương pháp dựa trên phân cụm để xây dựng những tập dữ liệu thỏa mãn tất cả các tiêu chí này. Phương pháp của chúng tôi bao gồm các ứng dụng liên tiếp và phân cấp của k-means trên một kho dữ liệu lớn và đa dạng để có được các cụm phân bố đều giữa các khái niệm dữ liệu, theo sau là bước lấy mẫu phân cấp, cân bằng từ các cụm này. Các thí nghiệm mở rộng trên ba miền dữ liệu khác nhau bao gồm hình ảnh web, hình ảnh vệ tinh và văn bản cho thấy rằng các đặc trưng được huấn luyện trên các tập dữ liệu được curation tự động của chúng tôi vượt trội hơn những đặc trưng được huấn luyện trên dữ liệu không được curation trong khi ngang bằng hoặc tốt hơn những đặc trưng được huấn luyện trên dữ liệu được curation thủ công.

1 Giới thiệu

Học tự giám sát (SSL) là cốt lõi của các hệ thống học máy hiện đại tiên tiến. Các mô hình ngôn ngữ lớn (LLM) được tiền huấn luyện theo cách tự giám sát sử dụng mục tiêu mô hình hóa ngôn ngữ (Radford et al., 2019; Ouyang et al., 2022; Raffel et al., 2020; Touvron et al., 2023), và các bộ mã hóa thị giác nền tảng được huấn luyện với các hương vị khác nhau của học tương phản (Richemond et al., 2020; Chen et al., 2020; Caron et al., 2021; Oquab et al., 2023). LLM đạt được hiệu suất xuất sắc trên tất cả các nhiệm vụ xử lý ngôn ngữ tự nhiên thông thường, như phân tích cảm xúc, dịch thuật, tóm tắt, trả lời câu hỏi, hoặc đối thoại. Đối với biểu diễn hình ảnh, các mô hình gần đây đạt được độ chính xác trên 87% trên ImageNet (Oquab et al., 2023), chứng tỏ rằng khoảng cách với nghệ thuật tiên tiến có giám sát tuyệt đối đang thu hẹp đáng kể. Bên cạnh hiệu suất xuất sắc trên các điểm chuẩn tiêu chuẩn, những mô hình này cho thấy khả năng tổng quát hóa mạnh mẽ ngoài phân phối, mở ra những hướng nghiên cứu mới. SSL đã được áp dụng thành công vào các miền hẹp hơn, mở khóa những cải tiến mô hình đáng kể, như phân tích hình ảnh y tế (Azizi et al., 2021; Chen et al., 2024), học các biểu diễn kiểu hình của tế bào (Ucar et al., 2021), và ước tính độ cao tán lá để giám sát tăng trưởng rừng (Tolan et al., 2023) để kể tên một số ứng dụng.

SSL là không giám sát vì nó không yêu cầu chú thích của con người để huấn luyện mô hình. Vì vậy, SSL cho phép mở rộng quy mô cả mô hình và dữ liệu mà không có ràng buộc về chú thích dữ liệu. Tuy nhiên, nhiều nỗ lực trước đây trong việc mở rộng quy mô mô hình và kích thước dữ liệu huấn luyện đã mang lại kết quả không thỏa đáng. Các mô hình ngôn ngữ lớn được huấn luyện trên các nhóm lớn văn bản corpus được curation kém đã dẫn đến hiệu suất dưới mức trung bình trên các điểm chuẩn tiêu chuẩn (Zhang et al., 2022; Le Scao et al., 2023). Huấn luyện trên các bộ sưu tập ngẫu nhiên của hình ảnh internet

--- TRANG 2 ---

Bảng 1: Tác động của pipeline curation dữ liệu của chúng tôi trên ba miền dữ liệu khác nhau: hình ảnh web theo độ chính xác phân loại hoặc mAP xếp hạng (đối với "oxf-H"), văn bản theo khớp chính xác ("nq" và "tqa") hoặc độ chính xác ("arc-c" hoặc "hellaswag"), và hình ảnh vệ tinh theo điểm số R² khối. Phương pháp curation tự động của chúng tôi dẫn đến những cải thiện đáng kể trong các điểm chuẩn so với các tập dữ liệu thô. Kết quả tốt nhất được in đậm.

| curation | hình ảnh web | văn bản | hình ảnh vệ tinh |
|----------|-------------|---------|------------------|
| | in-val | in-A | sketch | cars | oxf-H | inat18 | arc-c | hellaswag | nq | tqa | neon | a-neon | ca | sao-paulo |
| ✗ | 82.8 | 46.9 | 54.0 | 71.7 | 14.3 | 65.9 | 35.5 | 51.9 | 19.1 | 41.3 | 0.54 | 0.34 | 0.76 | 0.41 |
| ours | 84.7 | 66.4 | 60.5 | 82.5 | 32.1 | 75.7 | 40.1 | 53.1 | 22.5 | 43.7 | 0.64 | 0.53 | 0.79 | 0.47 |

Hình 1: Tổng quan về pipeline curation dữ liệu. Nhóm dữ liệu lớn thường thể hiện phân phối đuôi dài của các khái niệm. Trên các kho hình ảnh web, các khái niệm như website hoặc dog có mặt nhiều hơn plunger. Chúng tôi áp dụng k-means phân cấp để có được các cụm phân bố đều trên các khái niệm. Các điểm dữ liệu sau đó được lấy mẫu từ các cụm để tạo thành một tập dữ liệu được curation có sự cân bằng khái niệm tốt hơn.

cũng liên tục dẫn đến sự giảm hiệu suất đáng kể (Doersch et al., 2015; Caron et al., 2019; Goyal et al., 2019; 2021; Tian et al., 2021). Hiệu suất kém này có thể là do phân phối đuôi dài của các khái niệm trong các tập dữ liệu không được curation (xem (Salakhutdinov et al., 2011; Zhu et al., 2014; Liu et al., 2019). Như được chỉ ra bởi Wenzek et al. (2019), dữ liệu web thể hiện phân phối không đồng đều cao của các ngôn ngữ, và việc nhận dạng ngôn ngữ và lọc phù hợp là cần thiết để có được dữ liệu văn bản đơn ngữ đáng tin cậy. Trong các bộ sưu tập hình ảnh, các danh mục đối tượng cụ thể thống trị phân phối và xuất hiện trong nhiều hình ảnh, trong khi những danh mục khác có mặt ít hơn đáng kể. Hình ảnh chứa plunger chiếm 0.1% ImageNet nhưng có thể sẽ ít phổ biến hơn trong hình ảnh trực tuyến. Sự mất cân bằng này dẫn đến thiên vị hướng tới một số danh mục đối tượng thống trị trong việc trình bày đã học. Chúng tôi lập luận rằng sự cân bằng là một thuộc tính cần thiết của các tập dữ liệu tiền huấn luyện. Chúng tôi điều tra các phương pháp để tự động cân bằng lại các tập dữ liệu với phân phối đuôi dài.

Tuy nhiên có nhiều ứng dụng SSL thành công gần đây ở quy mô lớn. LLM thường được huấn luyện trên một hỗn hợp dữ liệu được curation cẩn thận, thường được neo xung quanh các nguồn dữ liệu chất lượng cao như Wikipedia (Touvron et al., 2023). Để mở rộng số lượng token, dữ liệu internet thô được lọc để khớp với phân phối ngôn ngữ và chủ đề của Wikipedia. Đối với các mô hình hình ảnh nền tảng, các hình ảnh liên quan được truy xuất từ một nhóm hình ảnh web ngẫu nhiên dựa trên seed, thường là tập dữ liệu được gắn nhãn thủ công. Điều này đảm bảo sự cân bằng tương đối tốt

--- TRANG 3 ---

giữa các khái niệm thị giác (Oquab et al., 2023). Tính mạnh mẽ trong các nhiệm vụ dự đoán hạ nguồn được hưởng lợi đáng kể từ việc sử dụng các mô hình lớn được tiền huấn luyện trên các tập dữ liệu lớn. Trong khi các công trình được đề cập ở trên tạo thành những điểm chứng minh mạnh mẽ cho việc mở rộng quy mô SSL, các pipeline curation dữ liệu khá là đặc biệt. Trong công trình này, chúng tôi tập trung vào việc curation có nguyên tắc và tự động của dữ liệu không được curation quy mô lớn, mà chúng tôi tin rằng sẽ ngày càng quan trọng trong các pipeline huấn luyện tương lai. Để đẩy giới hạn của tiền huấn luyện không giám sát, việc thiết kế tự động các tập dữ liệu huấn luyện đáng tin cậy vẫn là một câu hỏi nghiên cứu mở. Trái ngược với quy trình curation được đề xuất bởi Oquab et al. (2023), chúng tôi muốn thiết kế một thuật toán curation tổng quát bất khả tri với các nhiệm vụ hạ nguồn. Một thuật toán curation có nguyên tắc và tổng quát cho phép khả năng suy luận các thuộc tính thú vị từ các nguồn dữ liệu hoàn toàn không được curation, độc lập với đặc thù của các ứng dụng đang có.

Chúng tôi tiếp cận vấn đề này từ các nguyên tắc cơ bản, và đặt câu hỏi về các đặc tính cần thiết của một tập dữ liệu tiền huấn luyện tốt. Chúng tôi đưa ra giả thuyết rằng các tập dữ liệu như vậy nên lớn, đa dạng và cân bằng. Tầm quan trọng của hai tiêu chí đầu tiên đã được chứng minh nhiều lần (Kaplan et al., 2020; Hoffmann et al., 2022). Việc có được dữ liệu lớn và đa dạng có thể thực hiện được bằng cách tận dụng các kho lưu trữ web quy mô lớn của Internet (Grave et al., 2018; Wenzek et al., 2019). Tuy nhiên, các tập dữ liệu được tập hợp theo cách đó thể hiện phân phối đuôi dài của các khái niệm, tức là một số khái niệm thống trị chiếm một phần lớn của tập dữ liệu, trong khi những khái niệm khác xuất hiện ít thường xuyên hơn. Phân phối lệch này dẫn đến các đặc trưng thiên vị hướng tới các khái niệm đầu trong khi bỏ qua những khái niệm ở xa hơn trong đuôi, ngăn cản mô hình học được các đặc trưng phổ quát. Do đó, chúng tôi khẳng định rằng việc cân bằng dữ liệu là thiết yếu để tránh những thiên vị như vậy. Trong phân tích của chúng tôi, chúng tôi sử dụng thuật ngữ "khái niệm" thay vì "danh mục" hoặc "lớp" vì sau này thường được định nghĩa kém, chủ quan và phụ thuộc vào bối cảnh. Hơn nữa, một điểm dữ liệu (một hình ảnh hoặc một đoạn văn bản) có thể thuộc về nhiều "lớp" như vậy. Ngược lại, "khái niệm" là một thuật ngữ trừu tượng hơn và cho phép chúng tôi có một cuộc thảo luận khách quan hơn. Chúng tôi không định nghĩa rõ ràng các khái niệm, và thay vào đó để dữ liệu định nghĩa nó. Một khái niệm xuất hiện như nội dung chung của một nhóm điểm dữ liệu tương tự theo nhận thức của con người. Trong sự hiện diện của - có thể là yếu - các nhãn, sự cân bằng giữa các khái niệm có thể được đạt được bằng cách giới hạn số lượng điểm dữ liệu tương ứng với mỗi khái niệm (Radford et al., 2021; Dehghani et al., 2023; Xu et al., 2024). Tuy nhiên, điều này rất thách thức trong một thiết lập không giám sát mà không có quyền truy cập siêu dữ liệu.

Để đạt được mục tiêu này, chúng tôi giới thiệu một kỹ thuật curation tự động để xây dựng các tập dữ liệu cân bằng rộng lớn từ một nguồn dữ liệu không được curation. Từ một nhóm dữ liệu lớn chứa phân phối đuôi dài của các khái niệm, phương pháp của chúng tôi nhằm cân bằng lại dữ liệu sao cho các khái niệm ít thường xuyên trở nên nổi bật hơn so với những khái niệm phổ biến. Chúng tôi xem xét một lớp đặc biệt của các tập dữ liệu cân bằng - những tập được lấy mẫu đều từ hỗ trợ của phân phối dữ liệu cơ bản, và tìm cách xây dựng một tập từ nhóm dữ liệu. Vì không có chú thích nào có sẵn, chúng tôi tận dụng các phương pháp dựa trên phân cụm để đạt mục tiêu này. Được cung cấp các nhúng của tất cả các điểm dữ liệu được tạo ra bởi một bộ trích xuất đặc trưng, ví dụ, DINOv2 (Oquab et al., 2023) cho hình ảnh hoặc SBERT (Reimers & Gurevych, 2019) cho văn bản, chúng tôi giới thiệu một phương pháp k-means phân cấp cho phép lấy mẫu các điểm từ một phân phối gần với phân phối đều trên hỗ trợ dữ liệu trong không gian nhúng (xem Hình 1 để tổng quan về phương pháp được đề xuất). Chúng tôi chỉ ra rằng các đặc trưng tự giám sát được huấn luyện trên các tập dữ liệu được curation bằng phương pháp của chúng tôi dẫn đến những cải thiện lớn trong các điểm chuẩn trong ba miền khác nhau: hình ảnh web, văn bản và hình ảnh vệ tinh (Bảng 1).

Phần còn lại của bài báo được tổ chức như sau. Chúng tôi thảo luận về các thuộc tính cần thiết của các tập dữ liệu tiền huấn luyện cho học tự giám sát trong Mục 3.1, sau đó mô tả việc sử dụng k-means hoặc k-means phân cấp được đề xuất của chúng tôi để xây dựng các tập dữ liệu với những thuộc tính này trong Mục 3.2 và 3.3. Chúng tôi chỉ ra bằng dữ liệu mô phỏng trong Mục 4.1 rằng phương pháp của chúng tôi làm phẳng phân phối dữ liệu một cách hiệu quả, do đó kéo xuống các vùng dày đặc tương ứng với dữ liệu dư thừa và tăng trọng số cho các mẫu đuôi dài. Các thí nghiệm trên hình ảnh tự nhiên web trong thế giới thực được hiển thị trong Mục 4.2 chứng minh rằng phương pháp của chúng tôi dẫn đến những cải thiện trên hầu hết các điểm chuẩn, đặc biệt là về tính mạnh mẽ, tổng quát hóa ngoài phân phối và các trường hợp đuôi dài. Để đánh giá tính tổng quát của phương pháp ngoài hình ảnh tự nhiên, chúng tôi áp dụng trong Mục 4.3 phương pháp của chúng tôi cho dữ liệu văn bản và hình ảnh vệ tinh, chỉ ra những cải thiện đáng kể trong cả hai miền. Những nghiên cứu này cho thấy rằng phương pháp của chúng tôi cho phép tận dụng hiệu quả dữ liệu thô để cải thiện học đặc trưng tự giám sát, giảm bớt đáng kể chi phí liên quan đến chú thích và curation thủ công của các tập dữ liệu.

--- TRANG 4 ---

2 Công trình liên quan

Học tự giám sát là cốt lõi của học máy hiện đại. Đối với xử lý ngôn ngữ tự nhiên, mô hình hóa ngôn ngữ là một nhiệm vụ cơ bản có tính chất tự giám sát. Huấn luyện các mô hình ngôn ngữ thần kinh bắt đầu với các kiến trúc tương đối đơn giản, như các mô hình feed-forward (Bengio et al., 2000), hoặc mạng thần kinh hồi quy thuần túy (Elman, 1990; Hochreiter & Schmidhuber, 1997; Mikolov et al., 2010). Tận dụng dữ liệu lớn hơn và huấn luyện các mô hình lớn hơn đã mở đường cho việc tận dụng các mô hình ngôn ngữ để học biểu diễn (Radford et al., 2017; 2018; Devlin et al., 2019). Tinh chỉnh các mô hình BERT trên nhiệm vụ đang có đã trở thành quy trình tiêu chuẩn mà hầu hết các chuyên gia NLP theo. Gần đây, đẩy mô hình mô hình hóa ngôn ngữ đến cực độ đã dẫn đến tiến bộ đáng kinh ngạc trong việc học các mô hình quy mô lớn (Chowdhery et al., 2022; Hoffmann et al., 2022; Ouyang et al., 2022; Achiam et al., 2023; Touvron et al., 2023), thay đổi cơ bản lĩnh vực nghiên cứu AI.

Đồng thời, học không giám sát các đặc trưng thị giác cũng nhận được nhiều quan tâm trong thị giác máy tính trong vài năm qua. Ban đầu, các phương pháp học tự giám sát dựa trên các nhiệm vụ pretext được thiết kế tốt. Ý tưởng là các đặc trưng thị giác tổng quát sẽ xuất hiện bằng cách huấn luyện một mạng thần kinh để giải quyết những nhiệm vụ đơn giản tùy ý này, yêu cầu mô hình hiểu và lý luận về các thuộc tính hình ảnh cụ thể. Song song, một số phương pháp dựa trên việc nhận ra mỗi hình ảnh như một lớp riêng đã được đề xuất (Dosovitskiy et al., 2014; Bojanowski & Joulin, 2017; Wu et al., 2018). Theo con đường này, nhiều hàm mất mát tự giám sát "tổng quát" thay thế đã được đề xuất, dẫn đến cái có thể được gọi là Kiến trúc Nhúng Chung (LeCun, 2022). Một khối lượng lớn công trình được dành riêng cho việc thiết kế những mất mát như vậy. Điều này bao gồm các phương pháp dựa trên tương phản (Oord et al., 2018; Chen et al., 2020; Hénaff et al., 2020), với các biến thể bao gồm hàng đợi động lượng (He et al., 2020) và khai thác đồ thị hàng xóm gần nhất (Dwibedi et al., 2021). Cùng với đó, một số mất mát dựa trên phân cụm (Caron et al., 2018; Asano et al., 2020; Caron et al., 2020; Assran et al., 2022), chưng cất (Grill et al., 2020; Caron et al., 2021), và tối đa hóa thông tin (Zbontar et al., 2021; Bardes et al., 2022) đã được đề xuất trong tài liệu. Sự tiến bộ nhanh chóng này trong lĩnh vực đã dẫn đến tiến bộ đáng kinh ngạc về sức mạnh biểu diễn của các mô hình SSL. Công trình này tập trung vào việc xây dựng các tập dữ liệu tiền huấn luyện chất lượng cao cho SSL với một phương pháp curation tự động. Chúng tôi đánh giá các tập dữ liệu được curation của chúng tôi với DINOv2 (Oquab et al., 2023), một phương pháp dựa trên chưng cất cho thấy những nỗ lực huấn luyện thành công trên các tập dữ liệu hình ảnh lớn, được curation. Đánh giá pipeline curation của chúng tôi với các phương pháp SSL khác nằm ngoài phạm vi của công trình này - chúng tôi giả định rằng kết luận của chúng tôi đúng với các thuật toán huấn luyện tương tự (SimCLR, MoCo, SwAV).

Curation dữ liệu. Dữ liệu chất lượng cao đã là một thành phần chủ chốt trong việc huấn luyện các mô hình tiên tiến, cả cho NLP và thị giác máy tính. Trong bối cảnh học tự giám sát, nơi không cần siêu dữ liệu, vẫn là thiết yếu để tận dụng khối lượng lớn dữ liệu chất lượng cao. Như một trong những ví dụ nổi bật đầu tiên, các vector từ được huấn luyện với word2vec (Mikolov et al., 2013) đã cực kỳ phổ biến với các chuyên gia. Chất lượng của chúng bị ảnh hưởng trực tiếp bởi việc sử dụng một tập dữ liệu được lựa chọn cẩn thận với hơn 1 tỷ từ. Để tạo ra các vector từ chất lượng cao trong nhiều ngôn ngữ hơn, Grave et al. (2018) đã tiếp tục đẩy mạnh hướng curation dữ liệu quy mô lớn. Bằng cách lọc dữ liệu Common Crawl, các tác giả đã quản lý để có được các tập dữ liệu lớn để huấn luyện các vector từ đáng tin cậy cho 157 ngôn ngữ. Gần đây, các LLM mã nguồn mở tiên tiến (Touvron et al., 2023) cũng tận dụng loại dữ liệu lớn được curation cẩn thận này từ web (Wenzek et al., 2019).

Hầu hết các mô hình thị giác tự giám sát thành công được huấn luyện trên tập dữ liệu ImageNet được curation (Deng et al., 2009) (không có nhãn). Đã có một số nỗ lực ban đầu trong việc huấn luyện trên các tập dữ liệu khác, thường được tạo ra từ các nguồn dữ liệu lớn không được curation. Doersch et al. (2015) chỉ ra rằng các phương pháp tự giám sát có thể được huấn luyện trên dữ liệu thị giác không được curation, và Caron et al. (2019); Goyal et al. (2019) chỉ ra cách nó có thể được mở rộng đến hàng trăm triệu hình ảnh không được curation. Tương tự, Goyal et al. (2021; 2022a) tận dụng hàng tỷ hình ảnh internet ngẫu nhiên để có được các đặc trưng tự giám sát chất lượng cao. Asano et al. (2021) đề xuất một tập dữ liệu kích thước ImageNet của các hình ảnh không được curation để tạo điều kiện cho nghiên cứu ngoài hình ảnh được curation. Với mục tiêu huấn luyện trên dữ liệu không được curation, Tian et al. (2021) đề xuất một giải pháp lấy cảm hứng từ thuật toán chia để trị. Trong thiết lập đó, tập dữ liệu không được curation được chia thành các phần nhất quán sử dụng phân cụm, và các mô hình riêng lẻ được huấn luyện với SSL trên những phần đó. Một mô hình lớn được có được bằng cách chưng cất kiến thức từ các mô hình cụ thể theo phần. Oquab et al. (2023) đề xuất truy xuất các hình ảnh gần nhất với một tập cố định các tập dữ liệu quan tâm theo cách không giám sát.

--- TRANG 5 ---

Nguyên tắc này chia sẻ một số điểm tương đồng với ý tưởng tổng quát về lan truyền nhãn cho học bán giám sát, được khám phá bởi Yalniz et al. (2019). Phương pháp này cho thấy kết quả tốt, nhưng việc lựa chọn hình ảnh dựa trên các truy vấn từ các tập dữ liệu cụ thể có thể bỏ qua một loạt rộng các khái niệm thị giác trong các kho dựa trên Internet. Trái ngược với công trình trước, chúng tôi không sử dụng nhãn hình ảnh trong pipeline curation của chúng tôi. Thay vào đó, chúng tôi dựa vào các phương pháp phân cụm để chọn một tập hình ảnh cân bằng nhưng đa dạng cho huấn luyện tự giám sát.

Cắt tỉa dữ liệu và học tích cực tìm cách giảm kích thước của tập dữ liệu huấn luyện để tiết kiệm tính toán và/hoặc chi phí chú thích. Cắt tỉa dữ liệu tìm và loại bỏ các điểm dữ liệu dư thừa hoặc có vấn đề khỏi tập huấn luyện để cải thiện việc học. Các phương pháp điển hình bao gồm xếp hạng các điểm dữ liệu theo một số thước đo cắt tỉa và loại bỏ những điểm xếp hạng thấp. Các thước đo đáng chú ý bao gồm khoảng cách đến nguyên mẫu (Sorscher et al., 2022), lỗi huấn luyện (Paul et al., 2021), điểm quên (Toneva et al., 2019) hoặc điểm ảnh hưởng (Feldman & Zhang, 2020). Hầu hết các phương pháp cắt tỉa dữ liệu đòi hỏi thông tin nhãn. Học tích cực xen kẽ giữa việc huấn luyện mô hình và lựa chọn các mẫu tốt nhất tiếp theo để chú thích cho đến khi chi phí ngân sách chú thích được đáp ứng (Settles, 2009a). Các mẫu được chọn để tối đa hóa hiệu suất của mô hình. Các chiến lược lựa chọn phổ biến bao gồm chọn các mẫu đại diện nhất (Geifman & El-Yaniv, 2017; Sener & Savarese, 2018) hoặc thông tin nhất (Brust et al., 2019; Choi et al., 2021) hoặc cả hai (Zhdanov, 2019; Ash et al., 2020; Vo et al., 2022). Khác với những công trình này, chúng tôi không tập trung vào việc giảm chi phí tài nguyên và không yêu cầu nhãn dữ liệu, thay vào đó chúng tôi tìm cách sửa chữa phân phối của dữ liệu tiền huấn luyện SSL với một pipeline tự động và không giám sát.

Phân cụm hoặc phân tích cụm nhằm tìm cấu trúc trong dữ liệu bằng cách chia nó thành các phần nhất quán, rời rạc. Các phương pháp phân cụm có thể dựa trên tâm như k-means (Arthur & Vassilvitskii, 2007; Lloyd, 1982) hoặc mean-shift (Cheng, 1995), dựa trên mật độ như DBSCAN (Ester et al., 1996; Schubert et al., 2017), dựa trên mô hình thống kê với Gaussian Mixture Model (Yang et al., 2012) hoặc phân cấp như agglomerative (Defays, 1977; Sibson, 1973). Nó đã tìm thấy các ứng dụng rộng rãi trong các lĩnh vực khoa học khác nhau, thường được sử dụng để đưa cấu trúc vào dữ liệu để dễ phân tích hơn. Trong thị giác máy tính, các nhà nghiên cứu đã áp dụng phân cụm cho phân đoạn hình ảnh (Achanta et al., 2012), lượng tử hóa (Jégou et al., 2011) hoặc trích xuất bag-of-visual-words (Lazebnik et al., 2006; Jégou et al., 2010). Việc sử dụng phân cụm k-means của chúng tôi gần với các phương pháp học tích cực (Settles, 2009b) hoặc cắt tỉa dữ liệu (Sorscher et al., 2022) như một phần của quá trình lựa chọn hoặc xếp hạng dữ liệu. Trái ngược với chúng, chúng tôi không sử dụng k-means vì nó không tối ưu cho mục đích của chúng tôi, và thay vào đó đề xuất k-means phân cấp để lấy mẫu các tập dữ liệu cân bằng. Ứng dụng phân cấp của k-means đã được xem xét trước đây bởi Nister & Stewenius (2006) để xây dựng cây từ vựng của các khái niệm thị giác. Theo cách từ trên xuống, k-means đầu tiên được sử dụng để chia tập dữ liệu thành nhiều cụm, sau đó một k-means riêng biệt được áp dụng lên mỗi cụm để có được các cụm tinh hơn mà quá trình tiếp tục đệ quy. Ngược lại, phương pháp của chúng tôi xây dựng cây theo cách từ dưới lên nơi k-means tiếp theo được áp dụng trên các tâm thu được với k-means trước đó. Như chúng ta sẽ thấy trong Mục 3, trái ngược với Nister & Stewenius (2006), phương pháp của chúng tôi được đảm bảo tạo ra các phân cụm gần như cân bằng. Gần đây hơn, Ma et al. (2024) cũng sử dụng phân cụm k-means hai bước để có được các cụm dữ liệu với độ chi tiết khác nhau để huấn luyện các chuyên gia dữ liệu CLIP (Ramesh et al., 2021).

3 Phương pháp

3.1 Một Tiêu chí để Tạo Tập dữ liệu Tiền huấn luyện

Sử dụng học tự giám sát, người ta có thể tiềm năng huấn luyện các mô hình để biểu diễn tất cả các khái niệm một cách phù hợp. Tuy nhiên, điều này chỉ có thể nếu dữ liệu tiền huấn luyện lớn, đa dạng và bao phủ đủ các khái niệm. Trước đây đã được chỉ ra rằng các tập dữ liệu lớn và đa dạng là thiết yếu để huấn luyện các mô hình lớn tạo ra các nhúng tốt hơn so với các đối tác nhỏ hơn (Caron et al., 2019; 2021; Ramesh et al., 2021). Các tập dữ liệu lớn và đa dạng cũng đã được sử dụng trong các phương pháp học tự giám sát gần đây và mang lại hiệu suất tốt hơn trên các nhiệm vụ hạ nguồn (Ramesh et al., 2021; Oquab et al., 2023) và các đặc trưng mạnh mẽ hơn (Goyal et al., 2022a). Chúng thường được có được bằng cách thu thập dữ liệu từ các kho dữ liệu trực tuyến và áp dụng một phương pháp heuristic cho curation dữ liệu.

Các bộ sưu tập dữ liệu web, tuy nhiên, thường có phân phối đuôi dài của các khái niệm (Reed, 2001; Liu et al., 2022). Một số khái niệm thống trị và chiếm một phần lớn của tập dữ liệu. Ngược lại, nhiều khái niệm còn lại xuất hiện ít thường xuyên hơn nhiều. Sự khác biệt này có thể thiên vị việc huấn luyện mô hình hướng tới các khái niệm thống trị, hoặc trong trường hợp tồi tệ nhất, ngăn cản các mô hình học được các trình bày có ý nghĩa. Chúng tôi tin rằng sự cân bằng, được định nghĩa là có khoảng cùng số lượng điểm dữ liệu cho mỗi khái niệm, là một tiêu chí quan trọng khác cho các tập dữ liệu học tự giám sát. Tiêu chí này chưa được nghiên cứu kỹ lưỡng,

--- TRANG 6 ---

Bảng 2: Độ chính xác phân loại ImageNet (Deng et al., 2009) của các đặc trưng tự giám sát được huấn luyện trên ImageNet và các biến thể mất cân bằng của nó. Mức độ mất cân bằng tăng với hệ số α. Tập dữ liệu ImageNet gốc tương ứng với α = 0.

| Hệ số mất cân bằng α | 0.0 | 0.5 | 1.0 | 2.0 |
|----------------------|-----|-----|-----|-----|
| Độ chính xác         | 82.7| 79.0| 74.2| 57.0|

một phần do việc sử dụng rộng rãi các tập dữ liệu seed cân bằng như Wikipedia hoặc ImageNet (Deng et al., 2009), nhưng cũng do tính chất thách thức của việc xây dựng một tập dữ liệu cân bằng trong thiết lập không giám sát. Chúng tôi chỉ ra tầm quan trọng của tiêu chí cân bằng bằng các kết quả thực nghiệm cho các biểu diễn hình ảnh. Trong Bảng 2, chúng tôi báo cáo hiệu suất của các mô hình được huấn luyện trên các tập dữ liệu với các hệ số mất cân bằng khác nhau. Chúng tôi tạo ra một cách nhân tạo các biến thể mất cân bằng của ImageNet bằng cách lấy mẫu lại tập dữ liệu này sao cho kích thước lớp tuân theo luật power với số mũ tỉ lệ α được lấy trong {0.5, 1, 2}. Có thể quan sát thấy rằng các tập dữ liệu mất cân bằng hơn (α lớn) dẫn đến độ chính xác tồi tệ hơn trong phân loại tuyến tính trên ImageNet.

Quan sát này dẫn chúng tôi đến mệnh đề sau: Tập dữ liệu cho học tự giám sát nên lớn, đa dạng và cân bằng. Vì vậy, curation dữ liệu cho SSL bao gồm việc xây dựng các tập dữ liệu với tất cả những thuộc tính này. Chúng tôi đề xuất xây dựng những tập dữ liệu như vậy bằng cách chọn các tập con cân bằng của các kho dữ liệu trực tuyến lớn. Vì các kho này đã bao phủ một tập đa dạng các khái niệm, một tập con cân bằng lớn thỏa mãn tất cả các tiêu chí.

Nếu tất cả các điểm dữ liệu trong kho được liên kết với các nhãn phân loại, curation sẽ đơn giản bao gồm việc lấy mẫu cùng số lượng điểm dữ liệu từ mỗi danh mục. Khi những nhãn như vậy không có sẵn, chúng ta có thể bắt chước quá trình này bằng cách chia nhóm dữ liệu thành các cụm sử dụng các phương pháp như k-means (Lloyd, 1982; Arthur & Vassilvitskii, 2007) và coi chỉ số cụm như một danh mục đại diện. Chúng tôi sẽ thảo luận về các hạn chế của phương pháp này trong mục tiếp theo. Trong công trình này, chúng tôi đề xuất một phương pháp tổng quát hơn: lấy mẫu các điểm dữ liệu từ phân phối đều trên hỗ trợ của phân phối dữ liệu. Một tập con thu được theo cách đó là cân bằng khái niệm một cách tiệm cận nếu không gian nhúng trong đó phân phối được thao tác được tổ chức tốt. Trong không gian như vậy, các điểm dữ liệu tương tự về mặt ngữ nghĩa nằm gần nhau, hay nói cách khác, khoảng cách metric được cảm ứng phản ánh "khoảng cách ngữ nghĩa". Ví dụ, trong không gian nhúng nơi các khái niệm được biểu diễn bởi các blob không chồng chéo có kích thước bằng nhau, phương pháp lấy mẫu được đề xuất của chúng tôi tương đương với việc lấy mẫu cùng số lượng điểm từ mỗi khái niệm.

Phát biểu bài toán. Cho P là phân phối dữ liệu từ một nguồn chúng ta có thể lấy mẫu từ, ví dụ, Internet. Cho X là một tập các mẫu được rút ra từ P. Chúng tôi giả định rằng dữ liệu được biểu diễn bởi các vector trong R^d sao cho X là một phần tử của R^{n×d}. Chúng tôi muốn chọn một tập con S của X như thể chúng tôi trực tiếp lấy mẫu nó từ U, phân phối đều trên hỗ trợ của P. Lưu ý rằng với các giả định hợp lý, U được định nghĩa tốt. Hãy ký hiệu bởi p mật độ của P. Chúng tôi giả định rằng P sống trong một compact trong R^d, tức là hỗ trợ của nó Ω = {x|p(x) > 0} bị giới hạn. Giả định này là hợp lý vì các điểm dữ liệu của chúng tôi là các đặc trưng được trích xuất từ mạng thần kinh, luôn bị giới hạn về số và thường có norm nhỏ. Hàm chỉ thị 1_Ω có thể đo được vì Ω có thể đo được và khả tích hữu hạn nhờ giả định compact. Chúng tôi có thể định nghĩa U như phân phối xác suất với mật độ u = (1/vol(Ω))1_Ω trong đó vol(Ω) = ∫1_Ω là thể tích của Ω.

Tiếp theo, chúng tôi thảo luận về việc sử dụng phân cụm k-means để lấy mẫu một tập con nhóm dữ liệu cân bằng và các hạn chế của nó. Chúng tôi chỉ ra rằng chúng tôi có thể giải quyết những hạn chế này bằng một lựa chọn hàm khoảng cách tốt hơn hoặc, đơn giản hơn, phương pháp được đề xuất của chúng tôi, dựa trên việc áp dụng liên tiếp, phân cấp của thuật toán k-means trên dữ liệu thô. Các tâm của các cụm thu được với phương pháp này tuân theo một phân phối gần với U. Lấy mẫu từ những cụm này do đó gần như tương đương với việc lấy mẫu trực tiếp từ U. Cuối cùng, chúng tôi thảo luận về một số phương pháp lựa chọn dữ liệu từ các cụm thu được.

3.2 Cân bằng lại các tập dữ liệu với k-means

Phân cụm K-means (Arthur & Vassilvitskii, 2007; Lloyd, 1982) là một kỹ thuật có thể chi trả về mặt tính toán để tìm cấu trúc nhất quán trong dữ liệu. Nó chia dữ liệu thành các nhóm sao cho các điểm dữ liệu trong cùng một

--- TRANG 7 ---

nhóm gần nhau, theo một khoảng cách nào đó, trong khi những điểm trong các nhóm khác nhau xa nhau. Cho x_i ∈ R^d là nhúng của điểm dữ liệu i, k là số lượng cụm, và m_{ij} là biến thành viên nhị phân chỉ ra liệu điểm dữ liệu i có thuộc cụm j hay không. K-means tìm cách tìm (m_{ij})_{1≤i≤n,1≤j≤k} để tối thiểu hóa tổng độ biến dạng trong cụm:

∑_{j=1}^k ∑_{i=1}^n m_{ij}d(x_i, c_j),     (1)

với c_j là tâm của cụm j, được chọn để tối thiểu hóa ∑_{i=1}^n m_{ij}d(x_i, c_j), và d là khoảng cách L2 bình phương.

Lưu ý rằng với lựa chọn khoảng cách này, c_j có dạng đóng. Nó là trung bình của tất cả các điểm trong cụm j. Như đã thảo luận trước đây, k-means có thể được sử dụng để cân bằng lại một tập dữ liệu không được curation. Người ta bắt đầu bằng cách chia tập dữ liệu thành nhiều cụm, và lấy một số cố định hình ảnh từ mỗi cụm để tạo thành một tập dữ liệu mới. Phương pháp này chỉ hiệu quả nếu mỗi khái niệm chiếm khoảng cùng số lượng cụm. Tuy nhiên, không tầm thường để đảm bảo điều kiện này trong thực tế. Các khái niệm thống trị thường chiếm nhiều cụm hơn đáng kể so với những khái niệm ít phổ biến hơn. Ví dụ, khi áp dụng k-means trên một nhóm dữ liệu hình ảnh web, chúng tôi quan sát thấy rằng 300 trong số 10.000 cụm đại diện cho "trang web" (xem Mục 4.2.2 và Hình 4 để biết thêm chi tiết). Hiện tượng này được giải thích bằng cách nhìn vào hàm mục tiêu trong Phương trình (1). Khi một khái niệm thống trị được biểu diễn bởi nhiều điểm dữ liệu, việc nhóm tất cả chúng trong một cụm duy nhất sẽ dẫn đến độ biến dạng trong cụm lớn. Việc chia khái niệm này thành nhiều cụm nhỏ hơn để giảm đáng kể mục tiêu là tốt hơn, và bù đắp cho sự tăng lên trong số lượng cụm bằng cách nhóm các khái niệm nhỏ, hiếm với sự tăng nhẹ trong mục tiêu. Như vậy, k-means có xu hướng chia các khái niệm thị giác thống trị lớn thành nhiều cụm. Để minh họa điều này, chúng tôi có thể xem xét một ví dụ mẫu trong R với k = 3 và một tập dữ liệu gồm 5000 điểm được phân bố đều trong [0.9, 1.1], 2 điểm tại x = 2 và 2 điểm tại x = 3. Theo trực giác, người ta sẽ chọn 3 tâm tại 1, 2 và 3, dẫn đến độ biến dạng là 16.7. Tuy nhiên, K-means sẽ chia cụm đầu tiên gồm 5000 điểm thành hai và chọn các tâm tại 0.95, 1.05 và 2.5 để có được độ biến dạng nhỏ hơn là 6.0.

Kết quả từ Zador (1982; 1964) cung cấp một giải thích khác cho hiện tượng này. Hóa ra trong d-chiều, các tâm k-means tiệm cận theo phân phối với mật độ tỷ lệ với p^{d/(d+2)}. Xem Gray & Neuhoff (1998) cho một góc nhìn lịch sử và Hình 2 cho một mô phỏng 1-D minh họa thuộc tính này. Trong chiều cao, phân phối của các tâm k-means do đó phụ thuộc vào, và gần với, phân phối dữ liệu P. Điều này có nghĩa là k-means tạo ra nhiều cụm hơn đáng kể trong các vùng mật độ cao hơn trong không gian nhúng, tương ứng với các khái niệm thống trị. Kết quả là, không thể cân bằng lại các tập dữ liệu bằng k-means đơn giản. Chúng ta sẽ thấy trong Mục 4.2.2 rằng đây cũng là hạn chế của các kỹ thuật phân cụm khác như Phân cụm Agglomerative (Defays, 1977; Sibson, 1973).

[Tiếp tục với phần còn lại của tài liệu...]

Chúng tôi có thể khuyến khích k-means tạo ra các cụm lớn, và do đó ít cụm hơn trong các vùng dày đặc, bằng cách sử dụng một hàm biến dạng khác d(x,y) = ||x-y||^s với s > 2 (xem Hình 2). Lựa chọn biến dạng này giữ cho việc gán cụm không thay đổi vì tâm gần nhất với một điểm theo d hoặc L2 là như nhau. Do đó nó tương thích với khoảng cách ngữ nghĩa được xấp xỉ bởi L2. Tuy nhiên, những hàm biến dạng mới này làm giảm trọng số các điểm gần tâm hơn. Có nhiều điểm này trong một cụm không làm tăng đáng kể độ biến dạng trong cụm, giảm tác động của kích thước cụm. Trong trường hợp cực đoan khi s → ∞, mục tiêu chỉ tính đến các điểm dữ liệu xa nhất từ tâm trong mỗi cụm và hoàn toàn bỏ qua kích thước cụm. Một nhược điểm là việc tính toán tâm của một cụm cho các thành viên của nó không còn tầm thường. Nó có thể được thực hiện một cách gần đúng với gradient descent ngẫu nhiên, nhưng việc tính toán tốn kém trong thiết lập quy mô lớn. Như được chỉ ra tiếp theo, chúng tôi có thể đạt được hiệu ứng tương tự với việc áp dụng liên tiếp đơn giản của thuật toán k-means gốc.

3.3 Cân bằng lại các tập dữ liệu với k-means phân cấp

Như đã nêu ở trên, các tâm của các cụm k-means tuân theo phân phối Q với mật độ không chuẩn hóa p^{d/(d+2)}, vẫn gần với phân phối dữ liệu P trong chiều cao. Tuy nhiên, chúng tôi quan sát thấy rằng Q di chuyển gần U hơn so với phân phối dữ liệu P. Điều này được chỉ ra bởi bổ đề dưới đây.

Bổ đề 1 Cho P là phân phối xác suất với hàm mật độ p, t là một vô hướng trong (0,1), Q là phân phối xác suất với mật độ q = (1/Z)p^t trong đó Z = ∫p^t, và U là phân phối xác suất đều trên hỗ trợ Ω của P với mật độ u = (1/vol(Ω))1_Ω. Bất đẳng thức sau đây đúng:

D_{KL}(Q||U) ≤ D_{KL}(P||U),     (2)

trong đó D_{KL} ký hiệu phân kỳ Kullback-Leibler. Hơn nữa, đẳng thức xảy ra khi và chỉ khi P = U.

Chúng tôi có thể chứng minh bổ đề này bằng một số biến đổi đơn giản, và sử dụng tính không âm của D_{KL}(P||Q) và D_{KL}(Q||P). Mở rộng D_{KL}(Q||U) như ∫q log q + log vol(Ω) và D_{KL}(P||U) như ∫p log p + log vol(Ω), chúng tôi quan sát thấy rằng Phương trình (2) tương đương với ∫q log q ≤ ∫p log p. Nhờ tính không âm của D_{KL}(Q||P), chúng tôi có ∫q log q ≥ ∫q log p. Mở rộng ∫q log q như t∫q log p - log Z và thay thế điều này vào bất đẳng thức trước, chúng tôi có ∫q log p ≤ -log Z/(1-t), và do đó ∫q log q = t∫q log p - log Z ≤ -log Z/(1-t) (*). Tương tự, chúng tôi có thể mở rộng ∫p log p như (1/t)∫p log q + (1/t)log Z và kết hợp với tính không âm của D_{KL}(P||Q) để có ∫p log p ≥ -log Z/(1-t) (**). Chúng tôi suy ra ∫q log q ≤ ∫p log p bằng cách kết hợp (*) và (**). Đẳng thức xảy ra khi và chỉ khi p = q, có nghĩa là p là hằng số, hoặc tương đương P = U.

Do đó chúng tôi đề xuất áp dụng k-means liên tiếp cho dữ liệu, theo cấu trúc phân cấp, để xấp xỉ U. Trong quá trình này, chúng tôi áp dụng k-means T lần, k-means thứ t nhóm tập C_{t-1} của các tâm của k-means thứ (t-1) thành k_t cụm. K-means đầu tiên được tính toán trên dữ liệu thô. Chúng tôi gọi quá trình này là k-means phân cấp vì nó xây dựng cấu trúc cây trên dữ liệu nơi các điểm dữ liệu gốc là lá, và các nút trong ở mức t đại diện cho các tâm, và tương đương các cụm, thu được với k-means thứ t. Gốc là một điểm ảo kết nối các nút ở mức T. Dựa trên kết quả từ Zador (1982), tiệm cận, các tâm của k-means thứ T tuân theo phân phối với mật độ không chuẩn hóa p^{(d/(d+2))^T}. Phân phối này hội tụ về U khi T → ∞.

Trong quá trình trên, số lượng điểm dữ liệu đầu vào cho k-means giảm theo cấp số nhân sau mỗi lần áp dụng, điều này giới hạn số lần nó có thể được áp dụng. Chúng tôi vượt qua vấn đề này bằng các bước lấy mẫu lại-phân cụm, hoặc đơn giản là lấy mẫu lại, ở mỗi mức. Được cung cấp các cụm ở mức t, lấy mẫu lại-phân cụm bao gồm đầu tiên chọn r_t điểm gần nhất với tâm từ mỗi cụm để tạo thành một tập con R của C_{t-1}. Chúng tôi chọn r_t nhỏ để các điểm trong R gần như tuân theo phân phối của các tâm, gần U hơn so với phân phối của C_{t-1}, như được chỉ ra bởi Bổ đề 1. Sau đó chúng tôi áp dụng k-means trên R thay vì C_{t-1} để tìm n_t tâm mới. Cuối cùng, chúng tôi tạo thành một phân cụm mới của C_{t-1} bằng cách gán các điểm cho những tâm mới này. Vì phân phối của các điểm trong R gần U hơn so với phân phối của các điểm trong C_{t-1}, chúng tôi mong đợi rằng phân phối của các tâm mới gần U hơn so với phân phối của những tâm trước đó. Trái ngược với k-means phân cấp đơn giản, lấy mẫu lại-phân cụm không giảm tập hợp các điểm đầu vào cho k-means tiếp theo, vì vậy chúng tôi có thể áp dụng lặp lại quá trình này để tiến gần hơn đến U. Chúng tôi sử dụng k-means phân cấp với lấy mẫu lại trong pipeline của chúng tôi, như được tóm tắt trong Thuật toán 1.

--- TRANG 8 ---

Thuật toán 1: Thuật toán k-means phân cấp với lấy mẫu lại.

Đầu vào: Dữ liệu X ∈ R^{n×d}, số lượng mức T, số lượng cụm mỗi mức (k_t)_{1≤t≤T}, số lượng lấy mẫu lại m, số lượng điểm được lấy mẫu lại mỗi cụm (r_t)_{1≤t≤T}.

Kết quả: Một cấu trúc phân cấp của các cụm trên dữ liệu: các tâm (C_t)_{1≤t≤T} và các cụm ((L_t^{(i)})_{1≤i≤k_t})_{1≤t≤T}.

1 for t = 1 to T do
2    if t = 1 then I ← X else I ← C_{t-1}    ▷ Lấy đầu vào I của mức t
3    C_t ← kmeans(I, k_t)    ▷ Tìm các tâm C_t với k-means
4    L_t ← assign(I, C_t)    ▷ Gán các cụm (L_t^{(i)})_{1≤i≤n_t} với k-means
5    # lấy mẫu lại-phân cụm
6    for s = 1 to m do
7        R ← ⋃_{i=1}^{k_t} resample(L_t^{(i)}, r_t)    ▷ Lấy mẫu r_t điểm từ mỗi cụm
8        C_t ← kmeans(R, k_t)    ▷ Tìm các tâm dựa trên tập được lấy mẫu lại
9        L_t ← assign(I, C_t)    ▷ Gán các cụm cho toàn bộ tập đầu vào của mức t
10   end
11 end

Lấy mẫu từ k-means phân cấp. Như đã thảo luận ở trên, các tâm ở mức cao nhất của phân cụm phân cấp phân bố đều trên hỗ trợ dữ liệu. Người ta có thể xây dựng một tập con cân bằng của nhóm dữ liệu bằng cách lấy mẫu đều một số cố định lá (điểm dữ liệu) từ các cây con tương ứng của các tâm. Với các cây con nhỏ có ít lá hơn cần thiết, chúng tôi lấy tất cả các lá mà không lấy mẫu quá mức. Chúng tôi đặt tên chiến lược lấy mẫu này là lấy mẫu phẳng. Thường xuyên nhất, chúng tôi có kích thước mục tiêu cho tập con. Trong trường hợp này, chúng tôi tìm số lượng điểm cần lấy mẫu từ mỗi cây con sao cho kích thước tập con thu được xấp xỉ tốt nhất mục tiêu. Cụ thể, được cung cấp kích thước mục tiêu N và kích thước cụm s_j (1 ≤ j ≤ k), chúng tôi tìm số nguyên n để tối thiểu hóa |N - ∑_{j=1}^k min(n, s_j)| với tìm kiếm nhị phân trong khoảng [0, N].

Thay vì lấy mẫu trực tiếp từ các cây con mức cao nhất, chúng tôi đề xuất một chiến lược khác lấy mẫu theo cấu trúc phân cấp theo cách từ trên xuống. Được cung cấp số lượng điểm cần lấy mẫu từ một cây con của mức T, chúng tôi tính toán số lượng điểm cần lấy mẫu từ các cây con riêng của nó với tìm kiếm nhị phân ở trên, và lặp lại quá trình này xuống mức 1. Một khi chúng tôi có những số này cho tất cả các cụm ở mức 1, chúng tôi lấy mẫu từ chúng để tạo thành tập con cân bằng. Chiến lược này, được đặt tên là lấy mẫu phân cấp, đảm bảo sự cân bằng giữa các khái niệm được biểu diễn bởi các cây con ở mức cao nhất (như động vật, phương tiện, thể thao, v.v.) và giữa các khái niệm phụ được biểu diễn bởi các nút trong ở mức thấp hơn (như chó, nhện, máy bay, xe scooter, bóng đá, vovinam, v.v.). Chúng tôi xem xét một số cách để lấy mẫu các điểm dữ liệu từ các cụm của mức 1 như lấy mẫu các điểm ngẫu nhiên ("r"), hoặc các điểm gần nhất ("c") hoặc xa nhất ("f") từ các tâm của chúng. Chúng tôi so sánh lấy mẫu phẳng và phân cấp, cũng như các phương pháp lấy mẫu "r", "c" và "f" trong Mục 4.2.2.

Lựa chọn số lượng cụm. Phân tích của chúng tôi không đưa ra một phương pháp để chọn số lượng cụm tối ưu ở các mức khác nhau của k-means phân cấp. Theo trực giác, với cấu trúc cây của tập dữ liệu được biểu diễn bởi phân cụm, các nút trong ở mức thấp biểu diễn các khái niệm phụ hoặc nhỏ trong khi những nút ở mức cao hơn biểu diễn các khái niệm hoặc khái niệm lớn. Với cách giải thích này, việc có các cụm lớn hơn ở mức thấp hơn (các khái niệm phụ có thể chứa nhiều hình ảnh) và các cụm nhỏ hơn ở mức cao hơn (các khái niệm lớn chứa một số khái niệm nhỏ hơn) là hợp lý. Lựa chọn số lượng cụm của chúng tôi được hướng dẫn bởi trực giác này và sự thuận tiện. Ví dụ, đối với một phân cụm phân cấp 4 mức trên một tập dữ liệu 743 triệu điểm dữ liệu, chúng tôi chọn k = 10M, 500k, 50k, 10k cho bốn mức, dẫn đến các cụm có kích thước trung bình 70, 50, 10 và 5 cụm từ mức thứ nhất đến mức thứ tư tương ứng.

4 Thí nghiệm

Chúng tôi nghiên cứu thực nghiệm thuật toán được đề xuất trong một số thiết lập. Chúng tôi bắt đầu với các thí nghiệm có kiểm soát trên dữ liệu mô phỏng để cung cấp một phân tích có thể diễn giải về thuật toán của chúng tôi. Tiếp theo, chúng tôi thực hiện các thí nghiệm mở rộng bằng cách huấn luyện một phương pháp học tự giám sát tiên tiến (DINOv2 (Oquab et al., 2023)) trên các tập dữ liệu được curation tỉ mỉ từ hình ảnh web. Cuối cùng, chúng tôi chỉ ra tính tổng quát của phương pháp bằng cách áp dụng cùng một thuật toán cho hai miền khác: curation dữ liệu văn bản để huấn luyện các mô hình ngôn ngữ lớn và curation hình ảnh vệ tinh để huấn luyện một mô hình dự đoán chiều cao tán lá.

4.1 Thí nghiệm trên dữ liệu mô phỏng

Chúng tôi đầu tiên minh họa tác động của k-means phân cấp trên dữ liệu mô phỏng trong mặt phẳng 2-D. Chúng tôi lấy mẫu 9000 điểm từ một hỗn hợp của 3 Gaussian và phân phối đều, bị giới hạn bởi hình vuông Ω = [-3,3] × [-3,3]. Chúng tôi khớp 300 cụm trên tập này với một số cấu hình của k-means phân cấp (1, 2, 3 mức với hoặc không có lấy mẫu lại). Chúng tôi cũng thử các phương pháp phân cụm khác như DBSCAN (Ester et al., 1996) hoặc Phân cụm Agglomerative (Defays, 1977). Sử dụng ước tính mật độ kernel, chúng tôi ước tính mật độ của các phân phối của các tâm thu được với những phương pháp này. Chúng tôi hiển thị các tâm cùng với sơ đồ Voronoi của các cụm trong Hình 3(a-b). Hình cho thấy rằng k-means, tương đương với k-means phân cấp 1 mức không có lấy mẫu lại, tạo ra nhiều cụm hơn đáng kể trong các vùng mật độ cao hơn. Các cụm được phân bố đều hơn trên hình vuông khi chúng tôi sử dụng k-means phân cấp với nhiều mức hơn (Hình 3(a)). K-means phân cấp 3 mức với lấy mẫu lại của chúng tôi mang lại các cụm phân bố gần như đều trên hỗ trợ, được chứng minh bởi mật độ gần như phẳng (Hình 3(b)). Ngoài ra, có thể quan sát thấy rằng Phân cụm Agglomerative có những hạn chế tương tự như k-means trong khi DBSCAN cũng thất bại trong việc tạo ra các cụm phân bố đều (Hình 3(b)).

Về mặt định lượng, chúng tôi tính toán phân kỳ Kullback-Leibler (KL) giữa mật độ kernel ước tính và phân phối đều U trên hỗ trợ dữ liệu Ω. Kết quả được hiển thị trong Hình 3(c). Chúng tôi quan sát thấy rằng phân phối của các tâm thu được với k-means phân cấp tiến gần U hơn khi thêm nhiều mức và các bước lấy mẫu lại. Phân kỳ KL của k-means phân cấp với ba mức và lấy mẫu lại gần với giới hạn dưới được cung cấp bởi phân kỳ KL giữa mật độ kernel ước tính trên 300 điểm ngẫu nhiên trong Ω và U. Những kết quả này xác nhận phân tích của chúng tôi trong Mục 3.

4.2 Học tự giám sát trên hình ảnh web

4.2.1 Dữ liệu huấn luyện, chi tiết triển khai và đánh giá

Chúng tôi áp dụng thuật toán curation của chúng tôi cho một nhóm hình ảnh web. Nó được tập hợp bằng cách theo các liên kết từ thẻ <img> trong các trang web của một kho lưu trữ dữ liệu web được thu thập công khai. Chúng tôi lọc ra các URL trỏ đến các miền không an toàn hoặc bị hạn chế và không tải xuống hình ảnh từ chúng. Tiếp theo, việc xử lý hậu kỳ bao gồm khử trùng dựa trên hash PCA, loại bỏ các hình ảnh có kích thước nhỏ nhất nhỏ hơn 112px hoặc lớn hơn 512px, lọc nội dung NSFW và loại bỏ các khuôn mặt có thể nhận dạng được áp dụng cho các hình ảnh đã tải xuống để loại bỏ nội dung có hại và bảo vệ quyền riêng tư. Sau những bước này, chúng tôi có được một tập ban đầu gồm 1.2 tỷ hình ảnh độc đáo. Sau đó, chúng tôi loại bỏ các hình ảnh gần trùng lặp trong tập này hoặc đối với các tập kiểm tra của các đánh giá được xem xét bên dưới với pipeline phát hiện sao chép của Pizzi et al. (2022). Điều này dẫn đến một nhóm dữ liệu cuối cùng gồm 743 triệu hình ảnh độc đáo.

Chúng tôi huấn luyện một ViT-L với DINOv2¹ trên ImageNet1k (Russakovsky et al., 2015) và sử dụng nó như bộ trích xuất đặc trưng cơ sở của chúng tôi. Để chạy k-means và khởi tạo k-means++ ở quy mô lớn, chúng tôi triển khai một phiên bản được hỗ trợ GPU phân tán của thuật toán này trong PyTorch (Paszke et al., 2019). Lần chạy chính của chúng tôi bao gồm k-means phân cấp 4 mức trên nhóm hình ảnh này với 10M, 500k, 50k và 10k cụm ở mức thứ nhất, thứ hai, thứ ba và thứ tư. Vì k-means mức đầu tiên tốn nhiều tính toán, vì lý do hiệu quả, chúng tôi áp dụng kỹ thuật lấy mẫu lại được mô tả trong Mục 3.3 10 lần chỉ trên ba mức trên cùng, với số lượng điểm (r_t) được lấy mẫu từ mỗi cụm là một nửa kích thước cụm trung bình ở mỗi mức. Để tạo thành các tập dữ liệu được curation từ phân cụm phân cấp, theo mặc định chúng tôi sử dụng kỹ thuật lấy mẫu phân cấp được trình bày trong Mục 3.3 với kích thước mục tiêu điển hình là 100M hình ảnh. Để so sánh các tập dữ liệu tiền huấn luyện khác nhau, chúng tôi huấn luyện một DINOv2-reg (Oquab et al., 2023; Darcet et al., 2024) với ViT-g với các siêu tham số gốc cho 625k vòng lặp. Vì hiệu quả, chúng tôi thực hiện tất cả các nghiên cứu ablation của chúng tôi với ViT-L. Chúng tôi đánh giá các đặc trưng được tiền huấn luyện trên các tập dữ liệu khác nhau trên một loạt rộng các điểm chuẩn hạ nguồn với linear probing, không có fine-tuning.

• Phân loại ImageNet: Chúng tôi báo cáo độ chính xác top-1 trên phân loại k-nn và tuyến tính trên 1000 lớp của ImageNet. Ngoài tập validation tiêu chuẩn, chúng tôi cũng xem xét các tập kiểm tra thay thế ImageNet-

¹https://github.com/facebookresearch/dinov2

--- TRANG 10 ---

V2 (Recht et al., 2019) và ImageNet-ReaL (Beyer et al., 2020). Những tập kiểm tra này đã được xem xét trước đây để tránh overfitting tập validation tiêu chuẩn.

•Các tập kiểm tra ImageNet ngoài phân phối: Chúng tôi báo cáo độ chính xác top-1 của bộ phân loại tuyến tính được huấn luyện trên ImageNet được mô tả ở trên, trên ImageNet-A (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019) và ObjectNet (Barbu et al., 2019). ImageNet-A chứa các ví dụ khó bị phân loại sai bởi các ResNet được huấn luyện (He et al., 2016). ImageNet-R bao gồm hình ảnh của các danh mục ImageNet với các thay đổi trong phong cách hình ảnh, độ mờ, vị trí địa lý, vận hành camera, v.v. ImageNet-Sketch bao gồm các bản phác thảo của các lớp ImageNet. ObjectNet hiển thị các đối tượng ImageNet trong các góc nhìn và nền mới. Những tập kiểm tra này được sử dụng để đánh giá tính mạnh mẽ của các đặc trưng được tiền huấn luyện trên các miền khác nhau.

--- TRANG 11 ---

•Điểm chuẩn đuôi dài: Chúng tôi báo cáo độ chính xác phân loại top-1 trên iNaturalist2018 (Van Horn et al., 2018) và iNaturalist2021 (Van Horn et al., 2021). Những tập dữ liệu này chứa hình ảnh của các danh mục tự nhiên chi tiết như chim, côn trùng, thực vật, v.v. Chúng thể hiện phân phối hình ảnh rất mất cân bằng giữa các danh mục, trình bày một nhiệm vụ thách thức.

•Truy xuất: Chúng tôi đánh giá các đặc trưng được tiền huấn luyện trên nhận dạng mốc địa danh ở mức instance trong các tập dữ liệu Oxford và Paris (Philbin et al., 2007; 2008). Chúng tôi sử dụng phiên bản đã được sửa đổi của Radenović et al. (2018). Chúng tôi xếp hạng hình ảnh dựa trên độ tương đồng cosine của các đặc trưng với truy vấn và báo cáo độ chính xác trung bình được tính toán dựa trên việc xếp hạng.

•Phân loại chi tiết: Theo Chen et al. (2020), chúng tôi báo cáo phân loại top-1 trên 12 điểm chuẩn nhỏ. Bao gồm Aircraft (Maji et al., 2013), Caltech (Fei-Fei et al., 2004), Cars (Krause et al., 2013), CIFAR (Krizhevsky & Hinton, 2009), CUB (Berg et al., 2014), DTD (Cimpoi et al., 2014), Flowers (Nilsback & Zisserman, 2008), Food (Bossard et al., 2014), Pets (Parkhi et al., 2012), SUN (Xiao et al., 2010) và Pascal VOC (Everingham et al., 2015).

•Dự đoán dày đặc: Chúng tôi xem xét ba điểm chuẩn phân đoạn ngữ nghĩa bao gồm ADE20K (Zhou et al., 2017), Cityscapes (Cordts et al., 2016) và Pascal VOC (Everingham et al., 2015), và ba điểm chuẩn ước tính độ sâu bao gồm KITTI (Geiger et al., 2013), NYU (Silberman et al., 2012) và SUN-RGBD (Song et al., 2015). Chúng tôi báo cáo thước đo mIoU trên phân đoạn ngữ nghĩa và thước đo RMSE trên các điểm chuẩn ước tính độ sâu. Tất cả kết quả được có được bằng cách huấn luyện các đầu tuyến tính trên các đặc trưng cấp patch đã đóng băng, theo quy trình được mô tả trong Oquab et al. (2023). Đối với ước tính độ sâu, chúng tôi sử dụng kết nối của 4 lớp từ backbone, trong khi chúng tôi chỉ sử dụng lớp cuối cùng cho phân đoạn ngữ nghĩa.

4.2.2 Nghiên cứu Ablation

Chúng tôi bắt đầu bằng cách điều tra thực nghiệm các thuộc tính của các biến thể khác nhau của k-means phân cấp. Đầu tiên, chúng tôi kiểm tra phân phối của các cụm mà chúng tạo ra (Hình 4). Thứ hai, chúng tôi so sánh hiệu suất của các đặc trưng được huấn luyện trên các tập dữ liệu được curation với chúng (Bảng 3). Chúng tôi đặt tên các tập dữ liệu được curation theo số lượng mức của phân cụm mà chúng được tạo ra và chiến lược lấy mẫu trong cụm. Một tập dữ liệu được curation được tạo thành bằng cách lấy mẫu từ một phân cụm phân cấp có T mức với các phương pháp "r", "c" và "f" được đặt tên là "Tr", "Tc" và "Tf" tương ứng. Chúng tôi bổ sung thêm các hậu tố để chỉ ra rằng lấy mẫu phẳng được sử dụng thay vì lấy mẫu phân cấp (xem Mục 3.3), rằng k-means được khởi tạo ngẫu nhiên thay vì với k-means++, hoặc để chỉ rõ số lượng cụm ở mức cao nhất, số lượng bước lấy mẫu lại hoặc loại nhúng cơ sở khi cần thiết. Trừ khi được đề cập khác, tất cả các lần chạy k-means phân cấp có 10.000 cụm ở mức cao nhất và mười bước lấy mẫu lại trong tất cả các mức ngoại trừ mức đầu tiên.

K-means phân cấp có dẫn đến các phân cụm cân bằng hơn không? Trong Mục 3 và 4.1, chúng tôi cung cấp một lập luận lý thuyết và mô phỏng cho thấy rằng thuật toán k-means phân cấp của chúng tôi dẫn đến các phân cụm cân bằng hơn. Ở đây, chúng tôi điều tra liệu điều này có áp dụng cho dữ liệu thế giới thực, như nhóm dữ liệu hình ảnh lớn của chúng tôi. Như đã thảo luận trong Mục 3.2, sự mất cân bằng mạnh thể hiện trong các khái niệm thống trị (ví dụ, "trang web") được chia thành nhiều cụm nhỏ. Chúng tôi nghiên cứu cách 1000 lớp của ImageNet (Deng et al., 2009) liên quan đến các cụm của chúng tôi. Chúng tôi liên kết mỗi cụm với một trong các danh mục ImageNet và kiểm tra số lượng cụm và kích thước trung bình của các cụm đại diện cho danh mục đó.

Chúng tôi xem xét k-means và k-means phân cấp với hai, ba và bốn mức và 10 bước lấy mẫu lại trong mỗi mức. Chúng tạo ra bốn phân cụm với 10.000 cụm ở mức cao nhất. Chúng tôi gán các cụm được tạo ra bởi mỗi phương pháp cho các lớp ImageNet với k-nn, sử dụng các tâm cụm. Chúng tôi trình bày một biểu đồ phân tán của hai đại lượng cho mỗi phân cụm như một hàm của tổng kích thước lớp. Hai đại lượng này là số lượng cụm liên kết với mỗi lớp và kích thước trung bình của cụm cho lớp đó. Chúng tôi hiển thị các biểu đồ phân tán trong Hình 4. Chúng tôi thấy rằng k-means tạo ra các cụm có kích thước tương đối không đổi và các lớp lớn hơn được chia thành nhiều cụm hơn. Ngược lại, k-means phân cấp với nhiều mức hơn có thể tạo thành các cụm lớn hơn cho các lớp lớn, dẫn đến các cụm được phân bố đều hơn giữa các lớp.

Ảnh hưởng của số lượng mức trong k-means phân cấp. Chúng tôi chỉ ra trong Mục 3 rằng k-means phân cấp với nhiều mức hơn dẫn đến các tập dữ liệu được curation cân bằng hơn, và lập luận rằng điều này có lợi cho việc học đặc trưng tự giám sát. Chúng tôi xác thực thực nghiệm điều này bằng cách so sánh hiệu suất của các tập dữ liệu "1r", "2r", "3r"

--- TRANG 12 ---

và "4r" trong Bảng 3a. Chúng được curation tương ứng từ các phân cụm thu được với k-means phân cấp đơn, hai, ba và bốn mức, có 1, 11, 21 và 31 ứng dụng k-means. Chúng tôi quan sát thấy rằng thêm nhiều mức hơn trong k-means phân cấp nói chung dẫn đến hiệu suất tốt hơn trên các điểm chuẩn hạ nguồn. Bước nhảy lớn nhất được quan sát khi chuyển từ 1 mức, tương đương với k-means vanilla, lên 2 mức với những cải thiện đáng kể trên tất cả các điểm chuẩn. Đặc biệt, những lợi ích lớn được quan sát trong các điểm chuẩn về tính mạnh mẽ, đuôi dài, truy xuất. Chuyển lên 3 mức dẫn đến những lợi ích thêm trong các nhiệm vụ đuôi dài và truy xuất. Thêm một mức nữa dẫn đến những lợi ích nhỏ trên tất cả các điểm chuẩn ngoại trừ một sự giảm nhỏ trong iNaturalist2018. Những kết quả này xác nhận giá trị của phương pháp curation k-means phân cấp được đề xuất của chúng tôi trong một thiết lập thực tế.

Ảnh hưởng của chiến lược lấy mẫu. Chúng tôi thảo luận trong Mục 3.3 hai chiến lược lấy mẫu, lấy mẫu phẳng cơ sở và lấy mẫu phân cấp được đề xuất của chúng tôi, để tạo thành các tập dữ liệu được curation từ một phân cụm phân cấp. Chúng tôi so sánh tác động của chúng trong Bảng 3b ("4r" vs. "4r-flat"). Có thể thấy rằng phương pháp trước vượt trội hơn phương pháp sau trong tất cả các điểm chuẩn, làm nổi bật tầm quan trọng của sự cân bằng giữa các khái niệm ở tất cả các mức, không chỉ mức cao nhất. Chúng tôi cũng so sánh các phương pháp lấy mẫu "r", "c" và "f" trong lấy mẫu phân cấp thông qua hiệu suất hạ nguồn của các đặc trưng được huấn luyện trên "4r", "4c" và "4f" tương ứng. Có thể quan sát thấy rằng lấy mẫu ngẫu nhiên hoạt động tốt nhất, vượt trội hơn các phương pháp khác trên hầu hết các điểm chuẩn. Nó được theo sát bởi lấy mẫu "c" trong khi lấy mẫu "f" tụt lại phía sau. Điều này có thể do thực tế rằng lấy mẫu "f" trả về các điểm dữ liệu gần với ranh giới cụm không được đảm bảo phân bố đều trong hỗ trợ dữ liệu. Ngoài ra, lấy mẫu "r" tạo ra một tập điểm đa dạng hơn so với lấy mẫu "c".

k-means++ vs. khởi tạo ngẫu nhiên. Phân cụm k-means được biết là nhạy cảm với việc khởi tạo tâm. Chọn ngẫu nhiên các điểm dữ liệu làm tâm ban đầu (ngẫu nhiên) đơn giản và không tốn kém, do đó thường được sử dụng trong quy mô lớn. Tuy nhiên nó có thể dẫn đến phân cụm tùy ý tồi tệ đối với hàm mục tiêu (Arthur & Vassilvitskii, 2007). Trong bối cảnh của chúng tôi, áp dụng khởi tạo ngẫu nhiên trên các tập dữ liệu không được curation có thể dẫn đến sự mất cân bằng quan trọng giữa các khái niệm thị giác. Mặt khác, khởi tạo k-means++ (Arthur & Vassilvitskii, 2007) được biết là tạo ra các cụm đa dạng hơn. Nó cũng có một đảm bảo gần đúng về tính tối ưu cho phép phân tích của chúng tôi dựa trên giải pháp tối ưu của k-means trong Mục 3.3. Chúng tôi so sánh trong các thí nghiệm của chúng tôi tác động của hai kỹ thuật khởi tạo này ('4r' vs. '4r--') trên hiệu suất đặc trưng được tiền huấn luyện trong Bảng 3c. Chúng tôi triển khai một phiên bản được hỗ trợ GPU phân tán

--- TRANG 13 ---

Bảng 3: Hiệu suất trên các nhiệm vụ hạ nguồn của các đặc trưng được huấn luyện trên các tập dữ liệu được curation với các biến thể khác nhau của k-means phân cấp được đề xuất của chúng tôi. Các tập dữ liệu được đặt tên theo số lượng mức phân cụm và phương pháp lấy mẫu trong các cụm, với các hậu tố chỉ ra chi tiết bổ sung trong cấu hình phân cụm. Tất cả việc tiền huấn luyện được thực hiện với kiến trúc ViT-L. Kết quả tốt nhất được in đậm, và tốt thứ hai được gạch dưới. Xem văn bản để biết thêm chi tiết.

(a) Ảnh hưởng của số lượng mức của k-means phân cấp.
| dataset | imagenet | ood | long-tailed | retrieval |
|---------|----------|-----|-------------|-----------|
|         | knn | val | in-A | sketch | inat18 | inat21 | oxf-H |
| raw     | 73.6 | 82.8 | 46.9 | 54.0 | 65.9 | 73.8 | 14.3 |
| 1r      | 76.6 | 83.9 | 58.0 | 57.0 | 70.1 | 77.7 | 16.3 |
| 2r      | 78.7 | 84.5 | 65.4 | 60.0 | 73.8 | 80.8 | 24.7 |
| 3r      | 78.7 | 84.5 | 64.7 | 60.1 | 76.2 | 82.3 | 29.7 |
| 4r      | 79.6 | 84.7 | 66.4 | 60.5 | 75.7 | 82.3 | 32.1 |

(b) Ảnh hưởng của phương pháp lấy mẫu.
| dataset | imagenet | ood | long-tailed | retrieval |
|---------|----------|-----|-------------|-----------|
|         | knn | val | in-A | sketch | inat18 | inat21 | oxf-H |
| 4r-flat | 78.9 | 84.4 | 64.6 | 59.2 | 74.6 | 81.5 | 18.3 |
| 4c      | 79.4 | 84.7 | 64.3 | 59.4 | 75.1 | 81.6 | 29.2 |
| 4f      | 75.6 | 83.4 | 54.3 | 53.5 | 68.4 | 76.4 | 16.4 |
| 4r      | 79.6 | 84.7 | 66.4 | 60.5 | 75.7 | 82.3 | 32.1 |

(c) Ảnh hưởng của phương pháp khởi tạo k-means.
| dataset | imagenet | ood | long-tailed | retrieval |
|---------|----------|-----|-------------|-----------|
|         | knn | val | in-A | sketch | inat18 | inat21 | oxf-H |
| 4r--    | 74.0 | 83.0 | 47.7 | 53.9 | 69.0 | 76.5 | 25.4 |
| 4r      | 79.6 | 84.7 | 66.4 | 60.5 | 75.7 | 82.3 | 32.1 |

(d) Độ nhạy cảm với số lượng cụm.
| dataset | imagenet | ood | long-tailed | retrieval |
|---------|----------|-----|-------------|-----------|
|         | knn | val | in-A | sketch | inat18 | inat21 | oxf-H |
| 4r-20k  | 80.0 | 84.8 | 65.3 | 60.2 | 76.7 | 82.7 | 31.3 |
| 4r      | 79.6 | 84.7 | 66.4 | 60.5 | 75.7 | 82.3 | 32.1 |

(e) Độ nhạy cảm với số lượng bước lấy mẫu lại.
| dataset | imagenet | ood | long-tailed | retrieval |
|---------|----------|-----|-------------|-----------|
|         | knn | val | in-A | sketch | inat18 | inat21 | oxf-H |
| 4r-0    | 77.0 | 84.4 | 63.3 | 59.0 | 74.0 | 80.9 | 26.2 |
| 4r-100  | 80.0 | 84.8 | 66.4 | 59.8 | 76.0 | 82.4 | 30.7 |
| 4r      | 79.6 | 84.7 | 66.4 | 60.5 | 75.7 | 82.3 | 32.1 |

(f) Ảnh hưởng của các nhúng cơ sở.
| dataset | imagenet | ood | long-tailed | retrieval |
|---------|----------|-----|-------------|-----------|
|         | knn | val | in-A | sketch | inat18 | inat21 | oxf-H |
| 4r-raw  | 71.5 | 82.2 | 39.6 | 51.7 | 63.0 | 72.0 | 28.4 |
| 4r-in22k| 79.2 | 84.9 | 69.1 | 61.7 | 76.5 | 82.5 | 30.1 |
| 4r      | 79.6 | 84.7 | 66.4 | 60.5 | 75.7 | 82.3 | 32.1 |

của cả hai kỹ thuật trong PyTorch (Paszke et al., 2019). Có thể thấy rằng trên tất cả các điểm chuẩn, k-means++ dẫn đến các đặc trưng hoạt động tốt hơn đáng kể so với những đặc trưng thu được trong một pipeline với khởi tạo ngẫu nhiên cho k-means. Phương pháp sau dẫn đến các đặc trưng mạnh mẽ hơn trong dữ liệu ngoài miền và đuôi dài, được chứng minh bởi những lợi ích lớn trên ImageNet-A (+18.7), ImageNet-Sketch (+6.6), iNaturalist2018 (+6.7) và iNaturalist2021 (+5.8). Những kết quả này nhấn mạnh tầm quan trọng của một việc khởi tạo phù hợp cho k-means, ngay cả trong quy mô lớn. Chúng tôi khởi tạo k-means với k-means++ theo mặc định trong các thí nghiệm của chúng tôi.

Độ nhạy cảm với số lượng cụm. Như đã thảo luận trong Mục 3.3, lựa chọn số lượng cụm của chúng tôi được hướng dẫn bởi tính đơn giản và trực giác rằng các cụm mức cao hơn nên có kích thước nhỏ hơn so với các cụm mức thấp hơn. Ngoài phân cụm mặc định của chúng tôi với k = 10M, 500k, 50k và 10k dẫn đến tập dữ liệu "4r", chúng tôi chạy một k-means phân cấp 4 mức khác với k = 20M, 800k, 80k và 20k. Phân cụm này dẫn đến tập dữ liệu "4r-20k". Quan sát thấy trong Bảng 3d rằng hai tập dữ liệu dẫn đến hiệu suất tương tự trung bình. Các đặc trưng được tiền huấn luyện trên "4r-20k" mang lại kết quả tốt hơn đáng kể trên ImageNet k-nn và iNaturalist trong khi những đặc trưng được huấn luyện trên "4r" hoạt động tốt hơn trên ImageNet-A và truy xuất Oxford. Kết quả tốt hơn với "4r-20k" trên iNaturalist có thể là do nhiều cụm hơn trong k-means phân cấp dẫn đến việc phân chia tập dữ liệu tinh hơn và tốt hơn. Tuy nhiên, cần lưu ý rằng nó cũng đi kèm với chi phí tính toán cao hơn.

Độ nhạy cảm với số lượng bước lấy mẫu lại. Chúng tôi chỉ ra trong Mục 3.3 và Hình 3 rằng các bước lấy mẫu lại cho phép nhiều ứng dụng k-means hơn, dẫn đến các cụm phân bố đều hơn trên hỗ trợ dữ liệu. Chúng tôi đánh giá ảnh hưởng của nó bằng cách so sánh trong Bảng 3e "4r" với "4r-0", một tập dữ liệu được curation từ một phân cụm được xây dựng không có lấy mẫu lại, và "4r-100", một tập dữ liệu được curation từ một phân cụm được xây dựng với 100 bước lấy mẫu lại ở mức 3 và 4. Lưu ý rằng chúng tôi có 10 bước lấy mẫu lại mỗi mức trong phân cụm mặc định của chúng tôi tạo ra "4r". Chúng tôi quan sát thấy rằng không có lấy mẫu lại, hiệu suất giảm đáng kể trên tất cả các điểm chuẩn, nhấn mạnh tầm quan trọng của nó. Với nhiều bước lấy mẫu lại hơn, hiệu suất cải thiện thêm một chút trên hầu hết các điểm chuẩn nhưng trung bình, sử dụng 10 hoặc 100 bước lấy mẫu lại mang lại kết quả tương đương.

Ảnh hưởng của các nhúng cơ sở. Chúng tôi tiến hành các thí nghiệm chính của chúng tôi với các nhúng được trích xuất từ một ViT-L được huấn luyện với DINOv2 (Oquab et al., 2023) trên ImageNet1k. Để điều tra ảnh hưởng của các nhúng, chúng tôi cũng huấn luyện một ViT-L với DINOv2 trên ImageNet22k (Russakovsky et al., 2015) hoặc nhóm dữ liệu thô của chúng tôi. Sau đó chúng tôi chạy pipeline curation của chúng tôi trên nhóm dữ liệu thô của chúng tôi với những nhúng này, dẫn đến hai tập dữ liệu mới "4r-raw" và "4r-in22k". Có thể thấy trong Bảng 3f rằng chất lượng của các tập dữ liệu được curation, như được hiển thị bởi hiệu suất của các đặc trưng được tiền huấn luyện trên chúng, phụ thuộc mạnh vào loại nhúng được sử dụng trong pipeline curation. Sử dụng các nhúng được tiền huấn luyện trên nhóm dữ liệu thô dẫn đến hiệu suất kém so với các nhúng được tiền huấn luyện trên ImageNet1k hoặc ImageNet22k. Các nhúng được tiền huấn luyện trên ImageNet22k dẫn đến hiệu suất tốt hơn so với những nhúng được huấn luyện trên ImageNet1k trên hầu hết các điểm chuẩn. Chúng tôi chọn các nhúng được tiền huấn luyện trên ImageNet1k để giới hạn sự hiện diện của công việc thủ công trong pipeline của chúng tôi.

So sánh với bộ trích xuất đặc trưng cơ sở Chúng tôi so sánh các đặc trưng được huấn luyện trên tập dữ liệu được curation "4r" của chúng tôi và các đặc trưng được tạo ra bởi bộ trích xuất cơ sở trong Bảng 4. Chúng tôi quan sát rằng mô hình được huấn luyện trên "4r" hoạt động tốt hơn bộ trích xuất cơ sở trên phân loại k-nn trên ImageNet. Điều này không đáng ngạc nhiên vì bộ trích xuất cơ sở được huấn luyện trên ImageNet, do đó học để phân biệt các lớp ImageNet tốt hơn. Ngược lại, việc huấn luyện các đặc trưng trên tập dữ liệu được curation của chúng tôi dẫn đến hiệu suất tốt hơn trên tất cả các điểm chuẩn khác, với khoảng cách lớn trên các điểm chuẩn ngoài phân phối, đuôi dài và truy xuất.

Bảng 4: Hiệu suất của các đặc trưng được huấn luyện trên tập dữ liệu được curation của chúng tôi và những đặc trưng được tạo ra bởi bộ trích xuất đặc trưng cơ sở trên các điểm chuẩn thông thường

| dataset | imagenet | ood | long-tailed | retrieval |
|---------|----------|-----|-------------|-----------|
|         | knn | val | in-A | sketch | inat18 | inat21 | oxf-H |
| base    | 81.3 | 83.0 | 38.8 | 34.7 | 64.1 | 71.6 | 14.9 |
| 4r      | 79.6 | 84.7 | 66.4 | 60.5 | 75.7 | 82.3 | 32.1 |

4.2.3 So sánh với các tập dữ liệu khác.

Chúng tôi so sánh chất lượng của các đặc trưng được huấn luyện trên tập dữ liệu được curation tự động "4r" của chúng tôi, các tập dữ liệu được curation thủ công ImageNet1k và ImageNet22k, ImageNet1k-ret – một tập dữ liệu 100M hình ảnh được hình thành bằng cách truy xuất các hàng xóm gần nhất của các hình ảnh ImageNet1k trong nhóm dữ liệu của chúng tôi, và nhóm dữ liệu thô trong Bảng 5. Kiến trúc ViT-g được sử dụng trong tất cả các thí nghiệm này.

So sánh trên các điểm chuẩn phân loại và truy xuất. Chúng tôi hiển thị hiệu suất của các đặc trưng được tiền huấn luyện trên ImageNet, ngoài phân phối, đuôi dài, truy xuất và các điểm chuẩn chi tiết trong Bảng 5a. Trên tất cả các điểm chuẩn, ngoại trừ truy xuất Oxford (Radenović et al., 2018), các đặc trưng được huấn luyện trên tập dữ liệu được curation "4r" của chúng tôi vượt trội đáng kể so với những đặc trưng được huấn luyện trên nhóm dữ liệu thô. Khoảng cách đặc biệt lớn trên các điểm chuẩn ngoài phân phối và đuôi dài, cho thấy rằng phương pháp curation của chúng tôi dẫn đến các đặc trưng mạnh mẽ hơn. Trên các điểm chuẩn ImageNet và chi tiết tiêu chuẩn, curation cũng mang lại cải thiện đáng kể, xác nhận giá trị của nó. Cuối cùng, mặc dù nhóm dữ liệu thô dẫn đến hiệu suất tốt hơn so với tập dữ liệu được curation trên truy xuất Oxford, phương pháp sau tạo ra các đặc trưng tốt hơn trên điểm chuẩn truy xuất Paris.

So với các đặc trưng được huấn luyện trên ImageNet22k, các đặc trưng được huấn luyện trên "4r" hoạt động hơi tồi tệ hơn trên ImageNet k-nn nhưng hai phương pháp ngang bằng trên các tập validation khác (val, V2 và ReaL). Điều này đáng kể vì ImageNet22k chứa ImageNet1k và được curation với nỗ lực con người đáng kể trong khi tập dữ liệu của chúng tôi được có được với một pipeline tự động. Trên iNaturalist (Van Horn et al., 2018) và các điểm chuẩn chi tiết, ImageNet22k vẫn dẫn đến hiệu suất hơi tốt hơn, nhưng trên các điểm chuẩn ngoài phân phối và truy xuất "4r" dẫn đến kết quả tốt hơn nhiều, với khoảng cách lớn trong ImageNet-R (Hendrycks & Dietterich, 2019), ImageNet-Sketch (Hendrycks et al., 2021a), ObjectNet (Barbu et al., 2019) và truy xuất Paris (Radenović et al., 2018). Điều này chứng minh rằng việc huấn luyện SSL trên tập dữ liệu được curation của chúng tôi tạo ra các đặc trưng mạnh mẽ hơn.

--- TRANG 15 ---

Bảng 5: So sánh với các đặc trưng SSL được tiền huấn luyện trên các tập dữ liệu thô và được curation thủ công. Trong tất cả các thí nghiệm, mô hình ViT-g được huấn luyện với DINOv2-reg (Darcet et al., 2024) để có được các đặc trưng SSL, và chúng tôi đánh giá chúng trên các nhiệm vụ hạ nguồn mà không fine-tuning.

(a) Hiệu suất trên các điểm chuẩn phân loại (độ chính xác) và truy xuất (mAP).
| dataset | curation | imagenet | out-of-distribution | long-tailed | retrieval | fine grained |
|---------|----------|----------|-------------------|-------------|-----------|-------------|
|         |          | knn | val | V2 | ReaL | in-A | in-R | sketch | objnet | inat18 | inat21 | oxf-H | par-H |  |
| IN1k    | man.     | 72.0 | 77.7 | 65.5 | 83.6 | 21.7 | 34.8 | 24.7 | 34.2 | 42.2 | 54.2 | 11.6 | 36.9 | 78.1 |
| IN22k   | man.     | 83.0 | 85.9 | 77.6 | 89.4 | 74.0 | 68.9 | 55.9 | 62.9 | 81.5 | 86.0 | 32.8 | 68.6 | 91.2 |
| IN1k-ret| man. + ret. | 83.4 | 86.1 | 78.6 | 89.6 | 75.3 | 79.1 | 62.5 | 65.1 | 75.3 | 82.4 | 24.8 | 65.7 | 90.7 |
| raw     | ✗        | 78.0 | 85.0 | 75.9 | 88.7 | 65.8 | 74.2 | 59.9 | 67.1 | 72.2 | 79.6 | 35.8 | 75.6 | 89.7 |
| 4r      | ours     | 81.5 | 85.7 | 78.0 | 89.2 | 75.4 | 79.0 | 64.1 | 69.3 | 80.6 | 85.5 | 33.2 | 79.5 | 90.9 |

(b) Hiệu suất trên các điểm chuẩn phân đoạn ngữ nghĩa (mIoU) và ước tính độ sâu (RMSE).
| dataset | curation | dense prediction |
|---------|----------|------------------|
|         |          | segmentation | depth ↓ |
|         |          | ade20k | voc | cityscapes | avg | kitti | nyu | sun-rgbd | avg |
| IN1k    | man.     | 0.398 | 0.799 | 0.656 | 0.618 | 2.905 | 0.417 | 0.472 | 1.265 |
| IN22k   | man.     | 0.481 | 0.830 | 0.688 | 0.666 | 2.642 | 0.329 | 0.369 | 1.113 |
| IN1k-ret| man. + ret. | 0.468 | 0.823 | 0.687 | 0.659 | 2.703 | 0.319 | 0.371 | 1.131 |
| raw     | ✗        | 0.500 | 0.837 | 0.701 | 0.679 | 2.556 | 0.312 | 0.361 | 1.076 |
| 4r      | ours     | 0.489 | 0.828 | 0.695 | 0.671 | 2.560 | 0.335 | 0.371 | 1.089 |

(c) Đánh giá về tính công bằng của các đặc trưng được tiền huấn luyện.
| dataset | curation | fairness |
|---------|----------|----------|
|         |          | income bucket | regions |
|         |          | low | medium | high | africa | asia | americas | europe |
| IN1k    | man.     | 48.6 | 68.3 | 79.1 | 55.4 | 66.3 | 72.9 | 80.0 |
| IN22k   | man.     | 65.1 | 82.6 | 89.4 | 72.1 | 80.3 | 86.6 | 89.2 |
| IN1k-ret| man. + ret. | 64.6 | 81.9 | 89.5 | 71.2 | 79.9 | 85.7 | 89.4 |
| raw     | ✗        | 65.4 | 82.8 | 89.8 | 72.3 | 80.7 | 86.2 | 89.8 |
| 4r      | ours     | 66.7 | 82.9 | 89.7 | 72.7 | 81.3 | 86.5 | 89.0 |

Trong số các tập dữ liệu tiền huấn luyện, ImageNet1k-ret mang lại các đặc trưng hoạt động tốt nhất trên phân loại ImageNet1k, nhưng điều này không đáng ngạc nhiên vì tập dữ liệu này tập trung xung quanh ImageNet. Sự lệch hướng ImageNet này cản trở khả năng tổng quát hóa của các đặc trưng sang các miền khác. Thật vậy, những khoảng cách hiệu suất đáng kể được quan sát trên ImageNet-Sketch, ObjectNet, iNaturalist, Oxford và Paris so với các đặc trưng được huấn luyện trên "4r". Những kết quả này làm nổi bật hạn chế về khả năng tổng quát hóa của các phương pháp curation dựa trên truy xuất. Cuối cùng, chúng tôi quan sát thấy rằng ImageNet1k dẫn đến kết quả kém trên tất cả các điểm chuẩn. Có thể là do kích thước nhỏ của nó đối với một mô hình có khả năng cao như ViT-g. Điều này một lần nữa xác nhận nhu cầu về các tập dữ liệu tiền huấn luyện SSL lớn.

Các nhiệm vụ dự đoán dày đặc. Chúng tôi trình bày hiệu suất của các đặc trưng được tiền huấn luyện trên phân đoạn ngữ nghĩa và ước tính độ sâu trong Bảng 5b. Có thể thấy rằng nhóm dữ liệu thô mang lại hiệu suất hơi tốt hơn so với tập dữ liệu được curation. Điều này có thể là do phân phối của nhóm dữ liệu của chúng tôi. Khi nhìn vào các cụm hình ảnh, chúng tôi quan sát thấy rằng trong số 50 cụm lớn nhất, có 9 cụm mô tả các khái niệm "phòng ngủ", "cảnh trong nhà" hoặc "tòa nhà" có liên quan trong các điểm chuẩn như ADE20K (Zhou et al., 2017), NYU (Silberman et al., 2012) hoặc SUN-RGBD (Song et al., 2015). Những cụm này chứa hơn 35 triệu hình ảnh tổng cộng, chiếm 5% nhóm dữ liệu của chúng tôi. Với lấy mẫu phân cấp, chỉ khoảng 600 nghìn trong số chúng được giữ lại trong tập dữ liệu được curation, tương ứng với chỉ 0.6% kích thước của nó. Kết quả là, các đặc trưng được huấn luyện trên nhóm dữ liệu thô biểu diễn tốt hơn các khái niệm trên và có được hiệu suất tốt hơn trên những điểm chuẩn này. Tuy nhiên, đáng chú ý rằng sự giảm hiệu suất do curation ở đây rất nhỏ so với những lợi ích đạt được trong các điểm chuẩn khác. Cuối cùng, so với các tập dữ liệu được curation thủ công hoặc dựa trên truy xuất, "4r" dẫn đến kết quả tốt hơn đáng kể trung bình.

Công bằng trên các khu vực địa lý. Theo Goyal et al. (2022b) và Oquab et al. (2023), chúng tôi đánh giá công bằng của các đặc trưng trên tập dữ liệu Dollar Street (De Vries et al., 2019). Tập dữ liệu này chứa hình ảnh mô tả các đối tượng khác nhau từ 289 hộ gia đình từ 54 quốc gia. Mô hình được huấn luyện để nhận ra 94 khái niệm thay đổi thị giác giữa các hộ gia đình dựa trên mức thu nhập và khu vực địa lý của họ. Kết quả trong Bảng 5c cho thấy rằng các đặc trưng được tiền huấn luyện trên các tập dữ liệu lớn mang lại khoảng cách hẹp hơn giữa các mức thu nhập và khu vực so với những đặc trưng được tiền huấn luyện trên ImageNet1k. Huấn luyện trên tập dữ liệu được curation của chúng tôi cũng dẫn đến khoảng cách nhỏ hơn so với huấn luyện trên ImageNet22k được curation thủ công, tập dữ liệu được curation dựa trên truy xuất và nhóm dữ liệu thô. Tuy nhiên, khoảng cách tương đối giữa các mức thu nhập (25.6%) và khu vực (18.3%) vẫn đáng kể. Điều này có thể do hạn chế của nhóm dữ liệu của chúng tôi. Thực hiện curation trên một nhóm dữ liệu thô lớn hơn và đa dạng hơn nhiều có khả năng dẫn đến công bằng tốt hơn trong các đặc trưng SSL.

4.2.4 Đánh giá định tính về phương pháp curation của chúng tôi

Chúng tôi minh họa trong Hình 5 một mẫu từ cấu trúc phân cấp của các khái niệm thu được trên nhóm hình ảnh web. Chúng tôi hiển thị ba cụm ở mức 3 đại diện cho các khái niệm food, motorbike và bedroom. Các hình chữ nhật đỏ đại diện cho các cụm ở mức 1, mỗi cụm được minh họa bằng hai hình ảnh đại diện. Có thể thấy rằng các cụm có tính nhất quán. Các cụm ở mức thấp hơn đại diện cho các khái niệm tinh hơn như danh mục phụ của đối tượng (món tráng miệng hoặc món chính cho thực phẩm), phong cách của đối tượng (giường với các phong cách khác nhau), nền (xe máy ở biển hoặc bên hồ), góc nhìn (góc nhìn phía trước hoặc phía sau của xe máy) hoặc độ sáng (tối hoặc sáng). Sự cân bằng của những khái niệm và khái niệm phụ này trong các tập dữ liệu được curation là quan trọng, như được hiển thị trong các thí nghiệm ở trên.

4.3 Ứng dụng cho các miền khác

Phương pháp được trình bày trong bài báo này là tổng quát và bất khả tri với nhiệm vụ hạ nguồn đang có. Chúng tôi có thể áp dụng thuật toán của chúng tôi miễn là chúng tôi có thể tính toán các đặc trưng tốt cho dữ liệu huấn luyện thô. Trong phần này, chúng tôi chứng minh tính mạnh mẽ của phương pháp của chúng tôi bằng cách áp dụng thành công cùng một phương pháp cho hai nhiệm vụ khác. Đầu tiên, chúng tôi nghiên cứu việc huấn luyện các mô hình ngôn ngữ lớn trên các corpus văn bản quy mô lớn dựa trên web. Thứ hai, chúng tôi điều tra curation dữ liệu để huấn luyện các biểu diễn của hình ảnh vệ tinh.

4.3.1 Huấn luyện Mô hình Ngôn ngữ Lớn

Đã được chỉ ra nhiều lần rằng các mô hình ngôn ngữ lớn (LLM) đòi hỏi một lượng lớn dữ liệu để huấn luyện. Ví dụ, chúng ta thấy sự cải thiện liên tục khi huấn luyện các mô hình trên nhiều token hơn (xem Llama 1 & 2 (Touvron et al., 2023)). Hoffmann et al. (2022) chỉ ra rằng việc sử dụng nhiều token hơn cải thiện đáng kể chất lượng của mô hình, bất kể số lượng tham số. Tuy nhiên, chất lượng dữ liệu đóng một vai trò thiết yếu và chưa được nghiên cứu kỹ

--- TRANG 17 ---

Hình 5: Cấu trúc phân cấp của các cụm thu được khi áp dụng k-means phân cấp được đề xuất của chúng tôi trên hình ảnh web. Chúng tôi hiển thị ở đây các cụm ở mức 1, 2 và 3 đại diện cho các khái niệm food, motorbike và bedroom. Các hình chữ nhật đỏ hiển thị các cụm ở mức 1 với 2 hình ảnh đại diện.

lưỡng. Phần đầu tiên của Llama (Touvron et al., 2023) được huấn luyện trên một hỗn hợp các tập dữ liệu, một số trong đó là các tập dữ liệu chất lượng cao từ các miền hẹp trong khi phần lớn dữ liệu huấn luyện là một biến thể của CCNET (Wenzek et al., 2019), một curation heuristic dựa trên Wikipedia được áp dụng cho văn bản từ Common Crawl. Chúng tôi điều tra hiệu quả của phương pháp tự động của chúng tôi để curation dữ liệu tiền huấn luyện LLM.

Để làm điều này, chúng tôi áp dụng pipeline curation của chúng tôi cho hai nhóm văn bản dựa trên Common Crawl. Nhóm dữ liệu thứ nhất ("ccnet-1") được có được bằng cách theo Touvron et al. (2023), sử dụng pipeline của Wenzek et al. (2019) theo sau là bộ lọc dựa trên Wikipedia. Tập dữ liệu này gồm 641M tài liệu đã được curation, vì vậy phân phối dữ liệu bị lệch hướng Wikipedia. Chúng tôi có được nhóm dữ liệu thứ hai ("ccnet-2") bằng cách chạy cùng pipeline, không có bộ lọc dựa trên Wikipedia từ LLaMa và giai đoạn lọc Language Model từ Wenzek et al. (2019). Làm như vậy giữ phân phối dữ liệu gốc gần với Common Crawl thô hơn Wikipedia. Tập dữ liệu này "thô" hơn "ccnet-1" và có 789M tài liệu.

Chúng tôi sử dụng mô hình all-mpnet-base-v2 từ SBERT (Reimers & Gurevych, 2019) để biểu diễn tài liệu. Chúng tôi áp dụng k-means phân cấp 3 mức với 10M, 500k và 50k cụm trong ba mức tương ứng, và lấy mẫu 200M tài liệu để tạo thành các tập dữ liệu được curation trên cả hai nhóm dữ liệu. Trên mỗi nhóm dữ liệu và tập dữ liệu được curation, chúng tôi huấn luyện một mô hình ngôn ngữ với 7B tham số trên lịch trình cho 210B token theo Touvron et al. (2023). Sau khi huấn luyện mô hình, chúng tôi đánh giá nó trên một số nhiệm vụ. Chúng tôi xem xét các điểm chuẩn bao gồm đánh giá 0-shot trên các nhiệm vụ lý luận thường thức như PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), Arc-challenge (Clark et al., 2018) và Hellaswag (Zellers et al., 2019), cũng như đánh giá 5-shot trên các nhiệm vụ kiến thức thế giới như NQ (Kwiatkowski et al., 2019) và TrivialQA (Joshi et al., 2017). Chúng tôi báo cáo thước đo độ chính xác trên các điểm chuẩn lý luận thường thức, trong khi trên các nhiệm vụ kiến thức thế giới, chúng tôi báo cáo thước đo khớp chính xác. Hiệu suất hạ nguồn trên những điểm chuẩn này được hiển thị trong Bảng 6.

Có thể thấy rằng phương pháp curation của chúng tôi cải thiện đáng kể hiệu suất trên tất cả các điểm chuẩn, cả cho ccnet-1 và ccnet-2, với những lợi ích lớn trên các tập dữ liệu Arc-challenge và NQ. Đáng chú ý là pipeline curation tự động của chúng tôi quản lý để cải thiện một nhóm dữ liệu đã được curation. Thật vậy, "ccnet-1" được lọc để loại bỏ các tài liệu sẽ rơi quá xa khỏi phân phối Wikipedia. Sự cải thiện nhất quán này đối với ccnet-1 có thể là do sự cân bằng khái niệm tốt hơn được mang lại bởi phương pháp của chúng tôi, một khía cạnh thường bị bỏ qua trong các pipeline dữ liệu hiện tại.

4.3.2 Ứng dụng cho hình ảnh vệ tinh

Tolan et al. (2023) trình bày một ứng dụng thú vị của học tự giám sát cho vấn đề ước tính chiều cao tán lá từ hình ảnh vệ tinh. Công trình này nhằm xây dựng một bản đồ chính xác cao về chiều cao cây ở quy mô toàn cầu. Những bản đồ như vậy hữu ích để giám sát tăng trưởng rừng hiệu quả và minh bạch hơn. Họ đề xuất một phương pháp hai bước. Đầu tiên, một backbone được huấn luyện sử dụng DINOv2 (Oquab et al., 2023) trên một tập dữ liệu quy mô lớn của hình ảnh vệ tinh. Sau đó, một decoder có giám sát được huấn luyện trên nó sử dụng dữ liệu chú thích chất lượng cao nhỏ hơn. Họ sử dụng một tập dữ liệu tiền huấn luyện gồm 18 triệu patch 256×256 của hình ảnh vệ tinh có độ phân giải khoảng 0.5 mét. Các hình ảnh được lấy mẫu trong các khu vực có sẵn các phép đo chiều cao từ vệ tinh GEDI, chủ yếu được chọn từ các mẫu có chứa thực vật. Decoder được mượn từ Dense Prediction Transformer (Ranftl et al., 2021). Nó được huấn luyện sử dụng hình ảnh vệ tinh kết hợp với bản đồ chiều cao tán lá ground truth từ tập dữ liệu NEON (National Ecological Observatory Network (NEON), 2022). Dữ liệu này bao phủ một số khu vực của Hoa Kỳ. Bộ ước tính chiều cao tán lá sau đó được đánh giá trên bốn tập kiểm tra. Chúng bao gồm tập kiểm tra NEON, chứa hình ảnh từ các địa điểm không có trong dữ liệu huấn luyện của decoder, tập dữ liệu California Brande (Brande, 2021), tập dữ liệu Sao Paulo (dos Santos et al., 2019), chứa cây cao hơn nhiều so với những cây trong NEON, và tập kiểm tra Aerial NEON chứa hình ảnh được chụp bởi drone thay vì vệ tinh.

Bảng 7: Hiệu suất của Tolan et al. (2023) trên các điểm chuẩn chiều cao tán lá khi sử dụng các backbone được tiền huấn luyện trên tập dữ liệu thô hoặc được curation của hình ảnh vệ tinh.

| dataset | neon | ca brande | sao paulo | aerial neon | avg |
|---------|------|-----------|-----------|-------------|-----|
|         | MAE↓ | r² | MAE↓ | r² | MAE↓ | r² | MAE↓ | r² | MAE↓ | r² |
| raw     | 3.1  | 0.54 | 0.6  | 0.76 | 5.2  | 0.41 | 3.3  | 0.34 | 3.0  | 0.51 |
| curated | 2.9  | 0.64 | 0.6  | 0.79 | 5.0  | 0.47 | 3.1  | 0.53 | 2.9  | 0.61 |

Đối với các thí nghiệm của chúng tôi, chúng tôi xây dựng một nhóm thô gồm 18 triệu hình ảnh theo cách tương tự như Tolan et al. (2023). Trên nhóm dữ liệu này, chúng tôi áp dụng k-means phân cấp 3 mức với 500k, 50k và 10k cụm ở mức thứ nhất, thứ hai và thứ ba. Sau đó chúng tôi lấy mẫu một tập dữ liệu được curation gồm 9 triệu hình ảnh. Chúng tôi sử dụng các nhúng DINOv2-reg ViT-L (Darcet et al., 2024) được huấn luyện trên nhóm dữ liệu thô để biểu diễn hình ảnh. Sau đó chúng tôi huấn luyện một DINOv2-reg ViT-L trên cả tập dữ liệu được curation và nhóm dữ liệu thô, và đánh giá các bộ ước tính chiều cao tán lá được huấn luyện với hai backbone này. Chúng tôi theo cùng giao thức đánh giá như Tolan et al. (2023) và báo cáo các thước đo Mean Average Error (MAE) và block R² (r²) trên các tập kiểm tra. Chúng tôi tóm tắt kết quả trong Bảng 7.

Huấn luyện backbone của tập dữ liệu được curation của chúng tôi dẫn đến những cải thiện đáng kể trên tất cả các điểm chuẩn, với cải thiện tương đối 20% trong thước đo r² trung bình. Sự khác biệt trong r² lớn nhất trên tập kiểm tra aerial neon, là tập ngoài phân phối nhất - công nghệ hình ảnh khác nhau (máy bay so với vệ tinh). Kết quả của chúng tôi chứng minh tiềm năng của pipeline curation của chúng tôi để cải thiện các hệ thống học trong các miền nơi các tập dữ liệu được curation quy mô lớn, chất lượng cao hiếm hoặc không có sẵn.

5 Kết luận

Chúng tôi đã trình bày một pipeline curation dữ liệu tự động tạo ra các tập dữ liệu huấn luyện lớn, đa dạng và cân bằng cho học đặc trưng tự giám sát. Phương pháp của chúng tôi bao gồm việc áp dụng liên tiếp phân cụm k-means trên các tập dữ liệu thô, kết hợp với các bước lấy mẫu lại-phân cụm cải thiện phân phối của các tâm k-means. Quy trình này dẫn đến các cụm phân bố đều hơn giữa các khái niệm. Thông qua các thí nghiệm mở rộng, chúng tôi đã chứng minh rằng pipeline của chúng tôi cho phép học các đặc trưng hiệu quả trong ba miền dữ liệu khác nhau bao gồm hình ảnh web, hình ảnh vệ tinh và văn bản. Pipeline của chúng tôi dẫn đến các đặc trưng mạnh mẽ hơn so với những đặc trưng được huấn luyện trên các tập dữ liệu được curation thủ công khi áp dụng cho hình ảnh web. Những đặc trưng này cũng hoạt động tốt trong một loạt rộng các nhiệm vụ hơn so với những đặc trưng được huấn luyện trên các tập dữ liệu được curation sử dụng truy xuất.

Mặc dù các tập dữ liệu được curation của chúng tôi mang lại các đặc trưng tốt hơn đáng kể so với các tập dữ liệu thô hoặc ImageNet1k, chúng vẫn hơi kém hơn ImageNet22k trên một số điểm chuẩn như ImageNet-1k, các tập dữ liệu phân loại chi tiết và iNaturalist. Tuy nhiên, đáng chú ý rằng ImageNet22k được curation với nỗ lực con người đáng kể hơn ImageNet1k. Những tập dữ liệu đánh giá này có tương quan rất cao với điểm chuẩn ImageNet, đã ảnh hưởng đến việc đánh giá thị giác máy tính trong hơn một thập kỷ. Hơn nữa, tập dữ liệu được curation của chúng tôi vẫn vượt trội trên các kiểm tra tính mạnh mẽ quan trọng (ImageNet Adversarial, Rendition và Sketch). Phương pháp của chúng tôi dẫn đến các mô hình hoạt động tốt hơn đáng kể so với những mô hình được huấn luyện trên dữ liệu thô cho văn bản và hình ảnh vệ tinh. Những kết quả này xác nhận tầm quan trọng của curation dữ liệu cho học đặc trưng tự giám sát và giá trị của phương pháp của chúng tôi. Áp dụng k-means phân cấp không giới hạn trong bối cảnh học tự giám sát. Nó nên được xem xét thay cho k-means vanilla trong các nhiệm vụ cần thiết các tập dữ liệu đa dạng và đại diện, như học tích cực hoặc cắt tỉa dữ liệu. Công trình tương lai sẽ hướng theo hướng này.

Hạn chế. Đầu tiên, công trình của chúng tôi đề xuất ba thuộc tính mong muốn của các tập dữ liệu tiền huấn luyện. Tuy nhiên, các yếu tố khác không được tính đến. Điều này bao gồm các yếu tố chủ quan và khó ước tính như chất lượng của các điểm dữ liệu riêng lẻ. Thứ hai, trong các thí nghiệm của chúng tôi trên hình ảnh web, chúng tôi vẫn dựa vào các đặc trưng được tiền huấn luyện sử dụng SSL trên một tập dữ liệu được tập hợp thủ công (ImageNet-1k). Cần có những điều tra thêm để loại bỏ thành phần thủ công này khỏi pipeline của chúng tôi. Cuối cùng, việc tận dụng các nhóm hình ảnh lớn hơn đáng kể sẽ cải thiện thêm hiệu suất của chúng tôi. Chúng tôi để lại bài tập mở rộng quy mô này cho công trình tương lai.

--- TRANG 19 ---

Tuyên bố về Tác động Rộng hơn. Việc xây dựng tập dữ liệu tự động nói chung đặt ra rủi ro củng cố thiên vị và vi phạm quyền riêng tư. Trong công trình của chúng tôi, chúng tôi giảm thiểu những lo ngại này bằng một số biện pháp an toàn. Ví dụ, chúng tôi đã sử dụng các mô hình mạnh để phát hiện và làm mờ tất cả các khuôn mặt con người trong nhóm dữ liệu hình ảnh web của chúng tôi. Hơn nữa, công trình của chúng tôi nhằm giảm thiểu thiên vị do đại diện quá mức một số khái niệm trong hình ảnh internet ngẫu nhiên, dẫn đến công bằng tốt hơn trong các nhiệm vụ hạ nguồn (Bảng 5c). Đồng thời, các chuyên gia có thể điều chỉnh các phương pháp curation tham số cho các mục tiêu cụ thể. Nếu các đánh giá công bằng như những đánh giá trong Mục 4.2.3 được thiết lập, người ta có thể giám sát hiệu suất hạ nguồn cùng với các chỉ số công bằng để chọn dữ liệu tối ưu. Người dùng cuối tạo ra tập dữ liệu nên kiểm tra các vấn đề công bằng.

Lời cảm ơn

Chúng tôi muốn cảm ơn đội M2C2 tại Meta FAIR đã chuẩn bị nhóm dữ liệu web.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo dài được giữ nguyên với định dạng tương tự...]
