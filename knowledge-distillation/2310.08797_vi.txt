# 2310.08797.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.08797.pdf
# Kích thước tệp: 516183 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Phân Tích So Sánh Các Phương Pháp Chưng Cất Không Phụ Thuộc Tác Vụ
để Nén Các Mô Hình Ngôn Ngữ Transformer
Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee
IBM Research AI
{takuma.udagawa@, aashka.trivedi@, mimerler@us., bhatta@us.}ibm.com
Tóm tắt
Các mô hình ngôn ngữ lớn đã trở thành một thành phần quan trọng trong NLP hiện đại, đạt được hiệu suất tối tân trong nhiều tác vụ khác nhau. Tuy nhiên, chúng thường không hiệu quả cho việc triển khai thực tế do chi phí suy luận đắt đỏ. Chưng cất kiến thức là một kỹ thuật đầy hứa hẹn để cải thiện hiệu quả của chúng trong khi vẫn duy trì hầu hết tính hiệu quả của chúng. Trong bài báo này, chúng tôi tái tạo, so sánh và phân tích một số phương pháp đại diện cho chưng cất không phụ thuộc tác vụ (mục đích chung) của các mô hình ngôn ngữ Transformer. Mục tiêu nghiên cứu của chúng tôi bao gồm chuyển giao Phân phối Đầu ra (OD), chuyển giao Trạng thái Ẩn (HS) với các chiến lược ánh xạ lớp khác nhau, và chuyển giao Chú ý Đa Đầu (MHA) dựa trên MiniLMv2. Thông qua các thí nghiệm mở rộng, chúng tôi nghiên cứu tính hiệu quả của từng phương pháp cho các kiến trúc học sinh khác nhau trong cả môi trường đơn ngữ (tiếng Anh) và đa ngữ. Nhìn chung, chúng tôi cho thấy chuyển giao MHA dựa trên MiniLMv2 thường là lựa chọn tốt nhất cho chưng cất và giải thích các lý do tiềm năng đằng sau thành công của nó. Hơn nữa, chúng tôi cho thấy chuyển giao HS vẫn là một đường cơ sở cạnh tranh, đặc biệt dưới chiến lược ánh xạ lớp tinh vi, trong khi chuyển giao OD luôn tụt hậu so với các cách tiếp cận khác. Những phát hiện từ nghiên cứu này đã giúp chúng tôi triển khai các mô hình học sinh hiệu quả nhưng hiệu suất cao cho các ứng dụng quan trọng về độ trễ.

1 Giới thiệu
Các mô hình ngôn ngữ lớn đã trở thành một thành phần quan trọng trong NLP hiện đại. Chúng đã đạt được hiệu suất xuất sắc trên các tác vụ hạ nguồn khác nhau (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020) và khả năng của chúng cho thấy sự cải thiện nhất quán với nhiều tính toán, dữ liệu và tham số mô hình hơn (Kaplan et al., 2020; Brown et al., 2020; Touvron et al., 2023). Về mặt tiêu cực, việc triển khai các mô hình như vậy trong môi trường thực tế đang trở nên ngày càng khó khăn do tính không hiệu quả của chúng, tức là chi phí tính toán, bộ nhớ, độ trễ và lưu trữ cao (Xu and McAuley, 2023).

Chưng cất kiến thức (Hinton et al., 2015) là một kỹ thuật đầy hứa hẹn để vượt qua thách thức này bằng cách chuyển giao kiến thức của mô hình gốc (giáo viên) cho một mô hình nhỏ hơn, hiệu quả hơn (học sinh). Điều này có thể được thực hiện theo cách cụ thể cho tác vụ (Turc et al., 2019; Jiao et al., 2020) hoặc không phụ thuộc tác vụ (Sanh et al., 2019; Wang et al., 2020). Cách thứ hai chỉ yêu cầu chưng cất một học sinh mục đích chung duy nhất có thể được tinh chỉnh trực tiếp trên bất kỳ tác vụ hạ nguồi nào. Do tính tiện lợi cao, chúng tôi tập trung vào cách tiếp cận thứ hai này trong nghiên cứu này.

Trong những năm gần đây, đã có nhiều phương pháp được đề xuất cho chưng cất không phụ thuộc tác vụ của các mô hình ngôn ngữ Transformer. Mục tiêu của bài báo này là tái tạo, so sánh và phân tích các phương pháp đại diện nhất trong lĩnh vực này. Chúng tôi thường tập trung vào chưng cất không phụ thuộc kiến trúc mà không áp đặt hoặc áp đặt tối thiểu hạn chế đối với kiến trúc học sinh¹: các phương pháp đại diện bao gồm chuyển giao Phân phối Đầu ra (OD) (Hinton et al., 2015), chuyển giao Trạng thái Ẩn (HS) dựa trên ánh xạ tuyến tính (Jiao et al., 2020; Mukherjee et al., 2021) và chuyển giao Chú ý Đa Đầu (MHA) dựa trên MiniLMv2 (Wang et al., 2021).

Đối với chuyển giao HS, chiến lược ánh xạ lớp giữa các lớp giáo viên và học sinh đóng vai trò quan trọng trong hiệu suất tổng thể, tuy nhiên, chiến lược tối ưu vẫn chưa biết hoặc gây tranh cái (Sun et al., 2019; Wu et al., 2020; Ko et al., 2023). Do đó, chúng tôi khám phá một loạt chiến lược đa dạng để đánh giá thực nghiệm từng kỹ thuật.

Đối với chuyển giao MHA, cách tiếp cận MiniLMv2 đã được chứng minh đạt hiệu suất tối tân, tuy nhiên, có tương đối ít hiểu biết đằng sau thành công của nó. Do đó, chúng tôi phát triển một biến thể mới có tên DirectMiniLM hữu ích để hiểu tính hiệu quả đằng sau MiniLMv2 cả về mặt lý thuyết và thực nghiệm.

¹Bằng không phụ thuộc kiến trúc, chúng tôi có nghĩa là học sinh và giáo viên có thể có các tham số kiến trúc khác nhau (ví dụ: số lớp, đầu chú ý, kích thước trạng thái ẩn, v.v.).

--- TRANG 2 ---
IBM Research
Lớp Feed-ForwardLớp Chú ý Đa ĐầuHọc sinhGiáo viên×LHọc sinhGiáo viênHọc sinhGiáo viên
Học sinhGiáo viênHọc sinhGiáo viênHọc sinh Giáo viên(a) Transformer LM
IBM Research
Lớp Feed-ForwardLớp Chú ý Đa ĐầuHọc sinhGiáo viên×LHọc sinhGiáo viênHọc sinhGiáo viên
Học sinhGiáo viênHọc sinhGiáo viênHọc sinh Giáo viên (b) Chuyển giao OD
IBM Research
Lớp Feed-ForwardLớp Chú ý Đa ĐầuHọc sinhGiáo viên×LHọc sinhGiáo viênHọc sinhGiáo viên
Học sinhGiáo viênHọc sinhGiáo viênHọc sinh Giáo viên (c) Chuyển giao HS
IBM Research
Lớp Feed-ForwardLớp Chú ý Đa ĐầuHọc sinhGiáo viên×LHọc sinhGiáo viênHọc sinhGiáo viên
Học sinhGiáo viênHọc sinhGiáo viênHọc sinh Giáo viên (d) Chuyển giao MHA
Hình 1: Minh họa cấp cao về (a) kiến trúc Transformer và (b-d) các phương pháp chưng cất đại diện.
(b-d) biểu thị chuyển giao Phân phối Đầu ra (OD), Trạng thái Ẩn (HS) và Chú ý Đa Đầu (MHA), tương ứng.
Các đường giữa học sinh và giáo viên mô tả mức độ thông tin nào được chuyển giao trong từng phương pháp.

Trái ngược với hầu hết các nghiên cứu trước đây, tất cả các phương pháp được tái tạo trên một codebase thống nhất duy nhất để so sánh công bằng và nhất quán. Chúng tôi cũng tiến hành chưng cất trên 4 kiến trúc học sinh khác nhau, giảm kích thước mô hình trong các chiều khác nhau để phù hợp với các ngân sách tham số và độ trễ khác nhau. Cuối cùng, tất cả các thí nghiệm được tiến hành trên cả môi trường đơn ngữ và đa ngữ, được chưng cất từ BERT mã nguồn mở (Devlin et al., 2019) và XLM-RoBERTa nội bộ (Conneau et al., 2020), tương ứng.

Thông qua các thí nghiệm mở rộng, chúng tôi phân tích phê phán tính hiệu quả của từng phương pháp chưng cất và cung cấp lời khuyên thực tế cho cả nhà nghiên cứu và người thực hành làm việc trong lĩnh vực này.

Tóm lại, những phát hiện chính của chúng tôi là:
•Chuyển giao MHA thường là lựa chọn tốt nhất cho các kiến trúc học sinh và môi trường ngôn ngữ khác nhau. Bằng so sánh với DirectMiniLM, chúng tôi cung cấp những hiểu biết mới về thành công cơ bản của nó.
•Mặc dù tính hiệu quả của chuyển giao HS phụ thuộc vào chiến lược ánh xạ lớp, nó vẫn là một đường cơ sở cạnh tranh. Chiến lược ánh xạ lớp tinh vi hơn có thể cung cấp sự tăng cường về hiệu suất, đặc biệt trong môi trường đa ngữ.
•Các phương pháp dựa vào chuyển giao OD luôn tụt hậu so với các phương pháp khác. Điều này cho thấy chưng cất OD cổ điển có thể kém hiệu quả hơn khi chưng cất các mô hình ngôn ngữ phức tạp trên mục tiêu mục đích chung.

2 Mô hình Ngôn ngữ Transformer
Trước tiên, chúng tôi xem xét ngắn gọn kiến trúc tiêu chuẩn của các mô hình ngôn ngữ Transformer (Vaswani et al., 2017; Devlin et al., 2019). Một Transformer bao gồm một chồng L lớp Transformer, trong đó mỗi lớp bao gồm hai lớp con: một lớp Chú ý Đa Đầu (MHA) tiếp theo là một lớp Feed-Forward (FF) được kết nối đầy đủ (Hình 1, (a)).

Chính thức, gọi x biểu thị chuỗi đầu vào, dh kích thước trạng thái ẩn, và Hi∈R|x|×dh trạng thái ẩn của lớp Transformer thứ i (H0 biểu thị embedding chuỗi đầu vào). Cho Hi, lớp MHA đầu tiên tính toán các ánh xạ truy vấn, khóa và giá trị Qi,a, Ki,a, Vi,a cho mỗi đầu chú ý a∈[1, Ah], được kết hợp để có được đầu ra đầu chú ý Oi,a:
Qi,a=HiWQ,i,a (1)
Ki,a=HiWK,i,a (2)
Vi,a=HiWV,i,a (3)
Oi,a= softmax(Qi,aKT
i,a√dk)Vi,a (4)

Ở đây, dk biểu thị kích thước đầu chú ý (thường được đặt thành dh/Ah) và WQ,i,a, WK,i,a, WV,i,a∈Rdh×dk là các ma trận trọng số đã học. Đầu ra của lớp MHA là sự nối tiếp của Oi,a, cụ thể là MHA(Hi) = ⊕Ah
a=1Oi,a.

Tiếp theo, đầu ra lớp MHA được theo sau bởi một lớp FF theo vị trí với kích thước trung gian df và một kích hoạt phi tuyến (chúng tôi sử dụng GELU (Hendrycks and Gimpel, 2016) trong tất cả mô hình). Trạng thái ẩn của lớp Transformer tiếp theo được tính như Hi+1= FF(MHA(Hi)).²

Cuối cùng, để dự đoán phân phối đầu ra trên toàn bộ từ vựng V, một lớp tuyến tính WO∈Rdh×|V| được áp dụng trên đỉnh trạng thái ẩn cuối cùng để tính toán logits z=HLWO∈R|x|×|V|. Phân phối đầu ra có thể được lấy bằng cách áp dụng hàm softmax trên z, ký hiệu là softmax(z).

Trong suốt bài báo này, chúng tôi giả định rằng cả học sinh và giáo viên đều là các mô hình ngôn ngữ Transformer với LS và LT lớp, tương ứng.

²Cả hai lớp con MHA và FF đều có kết nối dư (He et al., 2016) và được theo sau bởi chuẩn hóa lớp (Ba et al., 2016), được bỏ qua để ngắn gọn.

--- TRANG 3 ---
3 Phương pháp Chưng cất
Tiếp theo, chúng tôi giới thiệu các phương pháp chưng cất không phụ thuộc tác vụ đại diện được minh họa trong Hình 1, (b-d). Đối với chuyển giao Chú ý Đa Đầu (MHA), chúng tôi xem xét hai cách tiếp cận: MiniLMv2 và biến thể mới của nó DirectMiniLM. Để khảo sát các phương pháp và chủ đề nâng cao mà chúng tôi không thể đề cập trong nghiên cứu này, vui lòng tham khảo Phụ lục A.

Chuyển giao Phân phối Đầu ra (OD) Phân phối đầu ra của giáo viên chứa thông tin hữu ích về xác suất tương đối của các dự đoán hợp lý (ngay cả khi không chính xác) (Hinton et al., 2015). Trong chuyển giao OD, học sinh được huấn luyện để sao chép phân phối đầu ra của giáo viên. Điều này được đạt được bằng cách tối ưu hóa hàm mất mát sau, trong đó zS, zT biểu thị logits học sinh/giáo viên, CE(.) mất mát entropy chéo và T nhiệt độ đầu ra:

LOD=T²·CE(softmax(zT/T), softmax(zS/T)) (5)

Chuyển giao Trạng thái Ẩn (HS) Các mô hình ngôn ngữ Transformer dần dần học các tính năng hữu ích và có thể tổng quát hóa từng lớp một. Trong chuyển giao HS, học sinh được huấn luyện để dự đoán các tính năng hữu ích như vậy được biểu diễn trong trạng thái ẩn của giáo viên.

Chính thức, mỗi lớp học sinh được ánh xạ đến một tập hợp các lớp giáo viên cần dự đoán. Gọi φ(i) biểu thị tập hợp được ánh xạ từ lớp học sinh thứ i, trong đó ∅ ⊆ φ(i) ⊆ [1, LT]. Đối với mỗi j∈φ(i), trạng thái ẩn của lớp học sinh thứ i HSi∈R|x|×dSh được biến đổi tuyến tính để dự đoán trạng thái ẩn của lớp giáo viên thứ j HTj∈R|x|×dTh.³ Điều này được biểu diễn bởi hàm mất mát sau, trong đó Wji∈RdSh×dTh biểu thị trọng số biến đổi tuyến tính và MSE(.) mất mát lỗi bình phương trung bình:

LHS=∑LSi=1∑j∈φ(i)MSE(HSiWji, HTj) (6)

Một vấn đề mở trong cách tiếp cận này là lựa chọn chiến lược ánh xạ lớp φ. Chúng tôi tiến hành các thí nghiệm mở rộng để so sánh một loạt chiến lược đa dạng, sẽ được thảo luận trong §4.

MiniLMv2 Lớp MHA là một thành phần chính trong các mô hình ngôn ngữ Transformer điều khiển các phụ thuộc và tương tác tầm xa trong văn bản đầu vào. MiniLMv2 (Wang et al., 2021) là một phương pháp hiệu quả để chuyển giao sâu mô-đun này trong khi cho phép số lượng đầu chú ý khác nhau ASh và ATh cho học sinh và giáo viên. Ý tưởng chính của họ là chưng cất các ma trận quan hệ chú ý (Q-Q, K-K và V-V) có được bằng cách đầu tiên nối các ánh xạ truy vấn (Q), khóa (K) và giá trị (V) từ tất cả các đầu chú ý và chia lại chúng thành cùng số lượng đầu quan hệ chú ý Ar.

Chính thức, gọi ASQ,i,a, ASK,i,a, ASV,i,a∈R|x|×dSr biểu thị các truy vấn, khóa và giá trị được nối và chia lại cho lớp học sinh thứ i, trong đó a∈[1, Ar] và dSr=dSh/Ar. Ví dụ, ⊕ASha=1QSi,a=⊕Ara=1ASQ,i,a, tức là các truy vấn gốc từ ASh đầu chú ý được nối đơn giản và sau đó chia lại thành Ar ma trận. Chúng tôi sử dụng ký hiệu tương tự cho lớp giáo viên thứ j, ATQ,j,a, ATK,j,a, ATV,j,a∈R|x|×dTr, trong đó dTr=dTh/Ar. Sau đó, hàm mất mát của MiniLMv2 có thể được định nghĩa như sau:

LMHA = ∑α∈{Q,K,V}∑Ara=1CE(RTα,j,a, RSα,i,a) (7)

RTα,j,a= softmax(ATα,j,aATTα,j,a/√dTr) (8)

RSα,i,a= softmax(ASα,i,aASTα,i,a/√dSr) (9)

Ở đây, RTα,j,a, RSα,i,a∈R|x|×|x| biểu thị các ma trận quan hệ chú ý được tính toán dựa trên tích ma trận của ATα,i,a, ASα,i,a trong phương trình (8), (9), tương ứng. Trực quan, điều này nhằm chuyển giao các truy vấn (Q), khóa (K) và giá trị (V) của giáo viên theo cách gián tiếp thông qua tích ma trận của chúng (Q-Q, K-K và V-V).

Tuy nhiên, có rất ít biện minh cho lý do tại sao phương pháp này hoạt động hiệu quả. Cũng khó để so sánh trực tiếp phương pháp này với chuyển giao HS vì các mất mát được tính toán khác nhau. Để hiểu rõ hơn MiniLMv2, chúng tôi đề xuất biến thể mới có tên DirectMiniLM cho phân tích của chúng tôi.

DirectMiniLM Trong DirectMiniLM, chúng tôi nhằm chuyển giao các ánh xạ Q/K/V của giáo viên một cách trực tiếp hơn thông qua biến đổi tuyến tính của các ánh xạ của học sinh, giống như chúng tôi đã làm trong chuyển giao HS. Cụ thể, chúng tôi sử dụng hàm mất mát sau với biến đổi tuyến tính Wα,a∈RdSr×dTr:

LDirectMHA = ∑α∈{Q,K,V}∑Ara=1MSE(ASα,i,aWα,a, ATα,j,a) (10)

--- TRANG 4 ---
DirectMiniLM quan trọng trong hai khía cạnh. Thứ nhất, cách tiếp cận này có thể so sánh trực tiếp với chuyển giao HS dựa trên phương trình (6) với điểm khác biệt duy nhất là thông tin nào bạn chuyển giao: trạng thái ẩn HTi→HSj hoặc các ánh xạ Q/K/V ATα,i,a→ASα,j,a. Từ so sánh này, chúng ta có thể định lượng lợi thế chính xác của việc chuyển giao từng kiến thức theo cách so sánh táo với táo.

Thứ hai, DirectMiniLM cũng có liên quan chặt chẽ đến MiniLMv2: nếu chúng ta hạn chế Wα,a là trực giao (tức là Wα,aWTα,a=I) và lấy tích ma trận cho mỗi số hạng trong mất mát MSE trong phương trình (10), chúng ta có được hàm mất mát sau:

∑α∈{Q,K,V}∑Ara=1MSE(ASα,i,aASTα,i,a, ATα,j,aATTα,i,a) (11)

Mất mát này giống MiniLMv2 từ phương trình (7) với điểm khác biệt nhỏ là sử dụng mất mát MSE thay vì mất mát CE với softmax. Do đó, DirectMiniLM với một số ràng buộc tự nhiên tương ứng với MiniLMv2. Điểm khác biệt chính là ATα,i,a được chuyển giao trực tiếp (với các ánh xạ tuyến tính) hay gián tiếp (với các ma trận quan hệ): bằng cách so sánh hai cách tiếp cận này, chúng ta có thể định lượng chính xác lợi thế của từng kỹ thuật tối ưu hóa.

4 Thiết lập Thí nghiệm
Chúng tôi khám phá các phương pháp chưng cất kiến thức không phụ thuộc tác vụ dưới hai môi trường:⁴

1.Chưng cất Đơn ngữ: Chúng tôi huấn luyện học sinh tiếng Anh sử dụng BERT mã nguồn mở (Devlin et al., 2019) làm giáo viên. Các mô hình này được chưng cất trên cùng corpus được sử dụng để pretraining BERT, tức là Wikipedia tiếng Anh (Devlin et al., 2019) và BookCorpus (Zhu et al., 2015).

2.Chưng cất Đa ngữ: Chúng tôi huấn luyện học sinh đa ngữ sử dụng XLM-RoBERTa nội bộ (Conneau et al., 2020) làm giáo viên, và chưng cất trên tập dữ liệu CC100 (Conneau et al., 2020), bao gồm dữ liệu hơn 100 ngôn ngữ. Chúng tôi chỉ sử dụng một tập con nhỏ của corpus để tiến hành thí nghiệm trong ngân sách tính toán hợp lý trong khi duy trì phân phối theo ngôn ngữ.

Trong cả hai môi trường, chúng tôi sử dụng kiến trúc Base (12 lớp) cho giáo viên, như được hiển thị trong Bảng 1. Để biết thêm chi tiết về từng thiết lập chưng cất (ví dụ: siêu tham số), vui lòng tham khảo Phụ lục B.

⁴Lưu ý rằng chúng tôi giới hạn nghiên cứu của mình với các mô hình chỉ encoder và để chưng cất các mô hình chỉ decoder (Radford et al., 2019) hoặc encoder-decoder (Lewis et al., 2020) làm công việc tương lai.

Mô hình Học sinh Để tiến hành so sánh mạnh các phương pháp chưng cất kiến thức đại diện, chúng tôi huấn luyện 4 học sinh với kiến trúc khác nhau và ngân sách độ trễ/tham số. Tóm tắt về kiến trúc học sinh, với tham số và độ trễ suy luận của chúng, được hiển thị trong Bảng 1.

Học sinh lớn nhất của chúng tôi là mô hình 6 lớp tuân theo cùng kiến trúc như DistilBERT (Sanh et al., 2019). Chúng tôi cũng sử dụng mô hình 6 lớp được sử dụng trong Mukherjee et al. (2021), có kích thước ẩn nhỏ hơn giáo viên. Các học sinh 4 và 3 lớp nhỏ hơn của chúng tôi được lấy như khuyến nghị từ quá trình Tìm kiếm Kiến trúc Neural (Trivedi et al., 2023) để tìm kiến trúc học sinh tốt cho chưng cất từ giáo viên XLM-RoBERTa, có điều kiện để giảm thiểu độ trễ trên CPU. Vui lòng tham khảo Phụ lục C để biết thêm chi tiết.

Chiến lược Ánh xạ Lớp Chiến lược ánh xạ lớp φ là tham số cần được xem xét cho cả chuyển giao HS và MHA. Đối với chuyển giao HS, chúng tôi khám phá ba môi trường sau:

1.Ánh xạ Đơn: Chúng tôi chỉ chưng cất lớp giáo viên cuối cùng (LTth) vào lớp học sinh cuối cùng, điều này đã được chứng minh là một đường cơ sở đơn giản nhưng cạnh tranh (Ko et al., 2023).

2.Ánh xạ 1-đến-1: Công việc trước đây cho thấy việc ánh xạ không chỉ lớp cuối cùng mà còn các lớp trung gian cải thiện chưng cất (Sun et al., 2019). Trong ánh xạ 1-đến-1, chúng tôi chưng cất một lớp giáo viên vào mỗi lớp học sinh bằng cách chọn:
•LS lớp giáo viên cuối cùng, tức là φ(i) = {LT−LS+i}(i∈[1, LS]). Thực nghiệm, các lớp giáo viên cuối cùng nắm bắt kiến thức cấp cao hơn (ví dụ: ngữ nghĩa) trong biểu diễn của chúng (Tenney et al., 2019; Jawahar et al., 2019).
•Lựa chọn đồng đều các lớp giáo viên chọn mỗi lớp giáo viên thứ k, tức là φ(i) = {ki}, trong đó k=⌈LT/LS⌉.⁵ Phương pháp này cũng có thể chuyển giao các lớp giáo viên thấp hơn, theo thực nghiệm nắm bắt kiến thức cục bộ (ví dụ: cú pháp) (Tenney et al., 2019).

3.Ánh xạ 1-đến-N: Một số công việc thậm chí cho thấy việc ánh xạ mỗi lớp học sinh đến nhiều lớp giáo viên có thể tránh mất thông tin và tạo thuận lợi cho việc học của học sinh (Wu et al., 2020; Passban et al., 2021). Đối với Ánh xạ 1-đến-N, chúng tôi khám phá các lựa chọn lớp giáo viên sau:
•Lựa chọn đồng đều k lớp liên tiếp (Uniform-Cons.), tức là φ(i) = [k(i−1), ki], trong đó k=⌈LT/LS⌉. Điều này tránh mất thông tin vì tất cả các lớp giáo viên được ánh xạ đến ít nhất một lớp học sinh.
•Kết hợp các chiến lược Uniform và Last từ ánh xạ 1-đến-1 (Uniform+Last). Điều này chọn 2 lớp giáo viên cho mỗi lớp học sinh dựa trên mỗi chiến lược 1-đến-1, mong đợi lấy được điều tốt nhất từ cả hai cách tiếp cận.

Đối với chuyển giao MHA, chúng tôi luôn sử dụng chiến lược ánh xạ đơn và chưng cất một lớp giáo viên duy nhất vào lớp học sinh cuối cùng, theo Wang et al. (2021). Cụ thể, chúng tôi thí nghiệm với ba lớp giáo viên cuối cùng như một lựa chọn cho chưng cất cho cả MiniLMv2 và DirectMiniLM. Bảng 2 tóm tắt các tùy chọn lựa chọn lớp của chúng tôi.

⁵Chiến lược này được sử dụng trong DistilBERT (Sanh et al., 2019) và cũng được biết đến như chiến lược "skip" (Sun et al., 2019).

Mặc dù chuyển giao OD có thể được tiến hành từ đầu, chúng tôi thấy điều này hội tụ chậm và không hoạt động cạnh tranh.⁶ Do đó, chúng tôi sử dụng phong cách chưng cất đa giai đoạn (Mukherjee et al., 2021) và tiến hành chuyển giao OD sau chuyển giao HS, sử dụng checkpoint đã chưng cất từ chuyển giao HS. Cách tiếp cận này hội tụ nhanh hơn nhiều với hiệu suất cuối cùng tốt hơn, do đó chúng tôi sử dụng cách tiếp cận này như phương pháp chuyển giao OD đại diện.

⁶Học sinh 6L đơn ngữ của chúng tôi mất 49 giờ trên 30 GPU V100 để đạt hiệu suất chấp nhận được, trong khi cùng mô hình đạt điểm số tốt hơn chỉ trong 10,5 giờ khi khởi tạo từ checkpoint đã chuyển giao HS.

--- TRANG 5 ---
| Mô hình | Kiến trúc | Đơn ngữ | Đa ngữ | Độ trễ Đơn ngữ | Độ trễ Đa ngữ |
|---------|-----------|---------|---------|-------------|-------------|
|         |           | Params  | Params  | GPU | CPU | GPU | CPU |
| 6L-DistilBERT | 6, 12, 768, 3072 | 66 | 234 | 5.98 (0.03) | 33.28 (0.09) | 6.01 (0.06) | 34.02(0.06) |
| 6L | 6, 12, 384, 1536 | 23 | 106 | 5.69 (0.02) | 11.98 (0.07) | 5.99 (0.07) | 12.52 (0.06) |
| 4L | 4, 12, 576, 768 | 27 | 153 | 3.66 (0.01) | 9.53 (0.04) | 3.98 (0.02) | 9.66 (0.05) |
| 3L | 3, 12, 384, 1024 | 16 | 100 | 3.02 (0.01) | 5.41 (0.08) | 3.25 (0.01) | 6.01 (0.06) |
| Giáo viên | 12, 12, 768, 3072 | 110 | 277 | 8.69 (0.08) | 64.91 (0.61) | 9.47 (0.01) | 66.31 (0.57) |

Bảng 1: Kiến trúc Mô hình hiển thị như [L, Ah, dh, df]. Tất cả tham số tính bằng triệu, với sự khác biệt trong tham số đơn ngữ và đa ngữ do kích thước từ vựng (30K cho đơn ngữ và 252K cho đa ngữ). Tất cả độ trễ tính bằng milliseconds, đo trên 5 lần chạy, với độ lệch chuẩn trong ngoặc đơn.

| Phương pháp Chưng cất | Chiến lược Ánh xạ Lớp |
|----------------------|----------------------|
| Chuyển giao HS | Đơn: LTth |
|                | 1-đến-1: Last, Uniform |
|                | 1-đến-N: Uniform-Cons., Uniform+Last |
| Chuyển giao MHA | Đơn: LTth, (LT−1)th, (LT−2)th |

Bảng 2: Chiến lược ánh xạ lớp được khám phá trong từng phương pháp chưng cất. Cùng các chiến lược được khám phá cho MiniLMv2 và DirectMiniLM trong Chuyển giao MHA.

5 Đánh giá và Kết quả
Đối với cả mô hình đơn ngữ và đa ngữ, chúng tôi đo hiệu suất trên Benchmark GLUE tiếng Anh (Wang et al., 2019) và báo cáo điểm trung bình của tất cả các tác vụ (không có CoLA⁷). Đối với mô hình đa ngữ, chúng tôi cung cấp đánh giá trên tập dữ liệu XNLI (Conneau et al., 2018), một tập hợp các tác vụ suy luận đánh giá hiệu suất của mô hình trên 15 ngôn ngữ sau khi được tinh chỉnh chỉ trên dữ liệu huấn luyện tiếng Anh. Chúng tôi báo cáo điểm trung bình của tất cả ngôn ngữ cho XNLI.

Bảng 3 tóm tắt hiệu suất của từng phương pháp chưng cất trên 4 kiến trúc học sinh. Để đánh giá chi tiết từng phương pháp dựa trên cấu hình tốt nhất, vui lòng tham khảo Phụ lục D.

Chúng tôi cũng cung cấp so sánh với DistilBERT (Sanh et al., 2019), một phương pháp hạn chế kiến trúc đại diện, trong Phụ lục E.

Chuyển giao HS Từ Bảng 3, chúng ta có thể xác minh rằng hiệu suất của chuyển giao HS thay đổi với các chiến lược ánh xạ lớp khác nhau, và không có chiến lược nào thống trị các chiến lược khác trong tất cả môi trường. Trong môi trường đơn ngữ, chúng tôi thấy chiến lược ánh xạ đơn hoạt động cạnh tranh, phù hợp với phát hiện của Ko et al. (2023). Tuy nhiên, trong môi trường đa ngữ, các chiến lược 1-đến-N tinh vi hơn thường cho thấy sự vượt trội so với các đường cơ sở đơn giản hơn. Điều này chỉ ra rằng nhiều giám sát hơn từ giáo viên có thể hữu ích (và tệ nhất là vô hại), do đó chúng tôi ủng hộ việc áp dụng các chiến lược 1-đến-N, đặc biệt trong chưng cất đa ngữ đầy thử thách.

Chuyển giao OD Như đã đề cập trong §4, chúng tôi khởi tạo mô hình từ các checkpoint đã chuyển giao HS với mỗi chiến lược ánh xạ lớp. Thú vị, chúng ta thấy sự suy giảm nhẹ về hiệu suất trên các tác vụ hạ nguồi so với chỉ chuyển giao HS, với mất mát đáng kể quan sát được cho các học sinh nhỏ hơn. Điều này chỉ ra rằng việc học biểu diễn hiệu quả từ các tín hiệu phân phối đầu ra là khó khăn, đặc biệt đối với học sinh có khả năng thấp hơn. Hơn nữa, xét mức độ đắt đỏ về mặt tính toán của chuyển giao OD, chuyển giao HS là một lựa chọn thay thế rẻ hơn và hiệu quả hơn cho chuyển giao kiến thức.

⁷Các mô hình đã chưng cất thường hoạt động kém trên CoLA: Chúng tôi giả thuyết điều này là do CoLA là tác vụ cú pháp duy nhất trong benchmark trái ngược với các tác vụ ngữ nghĩa khác (Xu et al., 2022). Chúng tôi bao gồm kết quả của CoLA trong Phụ lục D.

Chuyển giao MHA Đối với cả MiniLMv2 và DirectMiniLM, chúng tôi thấy việc chưng cất lớp giáo viên giữa-trên, tức là chiến lược (LT−1)th hoặc (LT−2)th, dẫn đến hiệu suất tốt nhất, phù hợp với phát hiện gốc của Wang et al. (2021). Quan trọng, chúng tôi thấy cả hai phương pháp chuyển giao MHA thường vượt trội hơn chuyển giao HS, điều này chỉ ra lợi ích của việc chuyển giao kiến thức Q/K/V so với kiến thức trạng thái ẩn. Điều này phù hợp với nghiên cứu so sánh mới nhất của Wang et al. (2023), mặc dù họ chỉ đánh giá trên kiến trúc 6L-DistilBERT trong môi trường đơn ngữ.

Chúng tôi cũng lưu ý rằng MiniLMv2 và DirectMiniLM hoạt động tương đương, với ngoại lệ đáng chú ý trên XNLI. Chúng tôi quy điều này cho hai yếu tố:

1.MiniLMv2 chuyển giao biểu diễn quan hệ có điều kiện trên toàn bộ đầu vào, trong khi DirectMiniLM chuyển giao biểu diễn tuyệt đối theo vị trí. Cái trước có thể mang tính thông tin ngữ nghĩa hơn, vì biểu diễn ngữ cảnh thường thể hiện cấu trúc quan hệ phong phú (Park et al., 2021; Liu et al., 2022a).

2.DirectMiniLM yêu cầu học trọng số biến đổi tuyến tính Wα,a, trong khi MiniLMv2 không tạo ra bất kỳ tham số bổ sung nào.

Từ những quan sát này, chúng tôi thường mong đợi MiniLMv2 là phương pháp chưng cất tốt nhất và đã áp dụng nó trong các ứng dụng quan trọng về độ trễ của chúng tôi.⁸ Tuy nhiên, DirectMiniLM hoạt động tương đương và cung cấp những hiểu biết có ý nghĩa về lợi ích của từng kỹ thuật tối ưu hóa, có thể hữu ích cho việc gỡ lỗi và phân tích MiniLMv2. Do đó, chúng tôi khuyến nghị so sánh nó cho cả nhà nghiên cứu và người thực hành trong các nghiên cứu tương lai.

⁸Cụ thể, các học sinh 4L đơn ngữ và đa ngữ với tốc độ tăng 7 lần trên CPU đã được triển khai cho các ứng dụng NLP khác nhau, như trích xuất thực thể, phân loại tài liệu và phát hiện quan hệ, trong khi duy trì 93% hiệu suất trung bình của giáo viên (Trivedi et al., 2023).

6 Kết luận
Nghiên cứu này phân tích phê phán các phương pháp đại diện cho chưng cất không phụ thuộc tác vụ của các mô hình ngôn ngữ. Cụ thể, chúng tôi so sánh chuyển giao Phân phối Đầu ra (OD), Trạng thái Ẩn (HS) và Chú ý Đa Đầu (MHA) cho các kiến trúc học sinh, môi trường ngôn ngữ và chiến lược ánh xạ lớp khác nhau. Thông qua các thí nghiệm mở rộng, chúng tôi cho thấy chuyển giao MHA dựa trên MiniLMv2 là lựa chọn tốt nhất trong nhiều môi trường, tiếp theo là chuyển giao HS với các chiến lược ánh xạ 1-đến-N tinh vi. Trong khi đó, chúng tôi không thấy chuyển giao OD là một lựa chọn thay thế hiệu quả. Cuối cùng, chúng tôi đề xuất DirectMiniLM để làm sáng tỏ lợi thế chính xác của kỹ thuật tối ưu hóa gián tiếp (tức là dựa trên ma trận quan hệ) được đề xuất trong MiniLMv2. Nhìn chung, chúng tôi hy vọng nghiên cứu này sẽ là hướng dẫn hữu ích cho cả nhà nghiên cứu và người thực hành làm việc trong lĩnh vực này.

--- TRANG 6 ---
| Phương pháp Chưng cất | Chiến lược Ánh xạ Lớp | Avg. GLUE (Đơn ngữ) | Avg. GLUE (Đa ngữ) | Avg. XNLI (Đa ngữ) |
|---------------------|---------------------|---------------------|-------------------|-------------------|
|                     |                     | 6L-DistilBERT | 6L | 4L | 3L | 6L-DistilBERT | 6L | 4L | 3L | 6L-DistilBERT | 6L | 4L | 3L |
| Chuyển giao HS | LTth | 84.1 | 79.4 | 80.2 | 78.9 | 80.8 | 77.1 | 78.0 | 74.7 | 56.2 | 55.1 | 51.6 | 50.6 |
|                | Last | 83.2 | 80.4 | 79.3 | 77.7 | 81.7 | 77.0 | 78.3 | 72.6 | 63.1 | 61.0 | 60.3 | 54.4 |
|                | Uniform | 82.9 | 80.6 | 79.6 | 76.6 | 81.6 | 78.2 | 78.3 | 73.5 | 59.9 | 59.9 | 59.7 | 59.9 |
|                | Uniform-Cons. | 83.9 | 80.6 | 80.6 | 77.7 | 82.4 | 78.8 | 78.0 | 75.9 | 65.5 | 62.2 | 60.4 | 58.6 |
|                | Uniform+Last | 84.1 | 80.4 | 80.4 | 77.7 | 83.1 | 78.7 | 79.2 | 75.0 | 67.0 | 62.7 | 62.5 | 57.9 |
| Chuyển giao OD | LTth | 84.1 | 78.1 | 79.4 | 76.6 | 78.5 | 75.1 | 75.2 | 67.9 | 50.5 | 48.2 | 51.6 | 43.8 |
| (khởi tạo từ Chuyển giao HS) | Last | 83.1 | 80.4 | 79.3 | 76.4 | 80.7 | 76.9 | 76.1 | 69.8 | 62.6 | 57.0 | 54.1 | 42.7 |
|                | Uniform | 83.4 | 79.8 | 79.8 | 77.1 | 79.9 | 78.0 | 77.9 | 65.4 | 60.4 | 54.1 | 52.0 | 42.8 |
|                | Uniform-Cons. | 83.7 | 80.3 | 79.5 | 76.7 | 81.7 | 78.7 | 76.4 | 70.1 | 63.1 | 61.0 | 56.5 | 48.2 |
|                | Uniform+Last | 84.1 | 80.5 | 79.9 | 77.1 | 82.1 | 78.4 | 76.4 | 72.3 | 66.0 | 60.9 | 60.0 | 48.6 |
| MiniLMv2 | LTth | 84.2 | 81.9 | 79.9 | 77.6 | 82.3 | 80.1 | 79.3 | 74.4 | 67.0 | 66.7 | 63.1 | 59.3 |
|          | (LT−1)th | 84.2 | 82.5 | 80.0 | 78.2 | 83.1 | 81.0 | 80.2 | 75.8 | 69.1 | 67.5 | 65.6 | 62.0 |
|          | (LT−2)th | 84.4 | 82.2 | 80.7 | 78.3 | 82.9 | 80.5 | 78.3 | 73.4 | 67.5 | 66.9 | 63.5 | 61.5 |
| DirectMiniLM | LTth | 84.0 | 81.3 | 79.7 | 78.2 | 83.2 | 80.8 | 79.0 | 75.1 | 66.3 | 66.1 | 64.7 | 60.7 |
|              | (LT−1)th | 84.4 | 81.7 | 79.6 | 78.0 | 81.9 | 81.1 | 80.3 | 73.8 | 66.9 | 65.9 | 64.8 | 61.0 |
|              | (LT−2)th | 84.3 | 81.7 | 80.4 | 78.3 | 83.4 | 80.9 | 79.7 | 75.6 | 66.3 | 64.8 | 65.4 | 60.5 |
| Giáo viên |  | 85.5 |  |  |  | 84.8 |  |  |  | 70.9 |  |  |  |

Bảng 3: Hiệu suất của các phương pháp chưng cất đại diện được đánh giá trên avg. GLUE và XNLI. Kết quả dựa trên chiến lược ánh xạ lớp tốt nhất cho từng phương pháp được gạch chân, và kết quả tổng thể tốt nhất được hiển thị bằng chữ đậm.

--- TRANG 7 ---
Tài liệu tham khảo
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.

Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. 2021. BinaryBERT: Pushing the limit of BERT quantization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4334–4348, Online. Association for Computational Linguistics.

Matan Ben Noach and Yoav Goldberg. 2020. Compressing pre-trained language models by matrix decomposition. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884–889, Suzhou, China. Association for Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.

Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. 2021. Drone: Data-aware low-rank compression for large nlp models. In Advances in Neural Information Processing Systems, volume 34, pages 29321–29334. Curran Associates, Inc.

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale.

Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating cross-lingual sentence representations.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Angela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations.

Md Akmal Haidar, Nithin Anchuri, Mehdi Rezagholizadeh, Abbas Ghaddar, Philippe Langlais, and Pascal Poupart. 2022. RAIL-KD: RAndom intermediate layer mapping for knowledge distillation. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1389–1400, Seattle, United States. Association for Computational Linguistics.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.

Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.

Aref Jafari, Mehdi Rezagholizadeh, Pranav Sharma, and Ali Ghodsi. 2021. Annealing knowledge distillation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2493–2504, Online. Association for Computational Linguistics.

Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019. What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657, Florence, Italy. Association for Computational Linguistics.

Xiaoqi Jiao, Huating Chang, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2021. Improving task-agnostic bert distillation with layer mapping search. Neurocomputing, 461:194–203.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163–4174, Online. Association for Computational Linguistics.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.

Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2021. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506–5518. PMLR.

--- TRANG 8 ---
Jongwoo Ko, Seungjoon Park, Minchan Jeong, Sukjin Hong, Euijai Ahn, Du-Seong Chang, and Se-Young Yun. 2023. Revisiting intermediate layer distillation for compressing language models: An overfitting perspective. In Findings of the Association for Computational Linguistics: EACL 2023, pages 158–175, Dubrovnik, Croatia. Association for Computational Linguistics.

François Lagunas, Ella Charlaix, Victor Sanh, and Alexander Rush. 2021. Block pruning for faster transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10619–10629, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics.

Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min Yang, and Yaohong Jin. 2020. BERT-EMD: Many-to-many layer mapping for BERT compression with earth mover's distance. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3009–3018, Online. Association for Computational Linguistics.

Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and Lawrence Carin. 2021. Mix{kd}: Towards efficient distillation of large-scale language models. In International Conference on Learning Representations.

Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, and Bin He. 2021. A global past-future early exit method for accelerating inference of pre-trained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2013–2023, Online. Association for Computational Linguistics.

Chang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan Zhao. 2022a. Multi-granularity structural knowledge distillation for language model compression. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001–1011, Dublin, Ireland. Association for Computational Linguistics.

Chang Liu, Chongyang Tao, Jianxin Liang, Tao Shen, Jiazhan Feng, Quzhe Huang, and Dongyan Zhao. 2022b. Rethinking task-specific knowledge distillation: Contextualized corpus as better textbook. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10652–10658, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. 2020. FastBERT: a self-distilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6035–6044, Online. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.

Xinge Ma, Jin Wang, Liang-Chih Yu, and Xuejie Zhang. 2022. Knowledge distillation with reptile meta-learning for pretrained language model compression. In Proceedings of the 29th International Conference on Computational Linguistics, pages 4907–4917, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Quanlu Zhang, Yaming Yang, Yunhai Tong, and Jing Bai. 2020. LadaBERT: Lightweight adaptation of BERT through hybrid model compression. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3225–3234, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2020. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 5191–5198.

Subhabrata Mukherjee, Ahmed Hassan Awadallah, and Jianfeng Gao. 2021. Xtremedistiltransformers: Task transfer for task-agnostic distillation. arXiv preprint arXiv:2106.04563.

Geondo Park, Gyeongman Kim, and Eunho Yang. 2021. Distilling linguistic context for language model compression. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 364–378, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Peyman Passban, Yimeng Wu, Mehdi Rezagholizadeh, and Qun Liu. 2021. Alp-kd: Attention-based layer projection for knowledge distillation. In Proceedings of the AAAI Conference on artificial intelligence, volume 35, pages 13657–13665.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

--- TRANG 9 ---
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815–8821.

Wonchul Son, Jaemin Na, Junyong Choi, and Wonjun Hwang. 2021. Densely guided knowledge distillation using multiple teacher assistants. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9395–9404.

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4323–4332, Hong Kong, China. Association for Computational Linguistics.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. MobileBERT: a compact task-agnostic BERT for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2158–2170, Online. Association for Computational Linguistics.

Marzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Ghodsi, and Mehdi Rezagholizadeh. 2022. KroneckerBERT: Significant compression of pre-trained language models through kronecker decomposition and knowledge distillation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2116–2127, Seattle, United States. Association for Computational Linguistics.

Weiting Tan, Kevin Heffernan, Holger Schwenk, and Philipp Koehn. 2023. Multilingual representation distillation with contrastive learning. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1477–1490, Dubrovnik, Croatia. Association for Computational Linguistics.

Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601, Florence, Italy. Association for Computational Linguistics.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Aashka Trivedi, Takuma Udagawa, Michele Merler, Rameswar Panda, Yousef El-Kurdi, and Bishwaranjan Bhattacharjee. 2023. Neural architecture search for effective teacher-student knowledge transfer in language models. arXiv preprint arXiv:2303.09639.

Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.

Jue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley. 2022. SkipBERT: Efficient inference with shallow layer skipping. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7287–7301, Dublin, Ireland. Association for Computational Linguistics.

Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. 2021. MiniLMv2: Multi-head self-attention relation distillation for compressing pre-trained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2140–2151, Online. Association for Computational Linguistics.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Advances in Neural Information Processing Systems, volume 33, pages 5776–5788. Curran Associates, Inc.

Xinpeng Wang, Leonie Weissweiler, Hinrich Schütze, and Barbara Plank. 2023. How to distill your BERT: An empirical study on the impact of weight initialisation and distillation objectives. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1843–1852, Toronto, Canada. Association for Computational Linguistics.

Yimeng Wu, Peyman Passban, Mehdi Rezagholizadeh, and Qun Liu. 2020. Why skip if you can combine: A simple knowledge distillation technique for intermediate layers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1016–1021, Online. Association for Computational Linguistics.

Yimeng Wu, Mehdi Rezagholizadeh, Abbas Ghaddar, Md Akmal Haidar, and Ali Ghodsi. 2021. Universal-KD: Attention-based output-grounded intermediate

--- TRANG 10 ---
layer knowledge distillation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7649–7661, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022. Structured pruning learns compact and accurate models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513–1528, Dublin, Ireland. Association for Computational Linguistics.

Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. BERxiT: Early exiting for BERT with better fine-tuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 91–104, Online. Association for Computational Linguistics.

Canwen Xu and Julian McAuley. 2023. A survey on model compression and acceleration for pretrained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 10566–10575.

Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. BERT-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7859–7869, Online. Association for Computational Linguistics.

Dongkuan (DK) Xu, Subhabrata Mukherjee, Xiaodong Liu, Debadeepta Dey, Wenhui Wang, Xiang Zhang, Ahmed Awadallah, and Jianfeng Gao. 2022. Few-shot task-agnostic neural architecture search for distilling large language models. In Advances in Neural Information Processing Systems, volume 35, pages 28644–28656. Curran Associates, Inc.

Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36–39. IEEE.

Minjia Zhang, Niranjan Uma Naresh, and Yuxiong He. 2022. Adversarial data augmentation for task-specific knowledge distillation of pre-trained transformers. Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):11685–11693.

Wangchunshu Zhou, Canwen Xu, and Julian McAuley. 2022. BERT learns to teach: Knowledge distillation with meta learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7037–7049, Dublin, Ireland. Association for Computational Linguistics.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 19–27.

A Công việc Liên quan
MobileBERT (Sun et al., 2020) là một kỹ thuật hiệu quả để nén BERT thành một học sinh được thiết kế đặc biệt với kiến trúc nút thắt cổ chai. Trong BERT-of-Theseus (Xu et al., 2020), các mô-đun của giáo viên được thay thế dần dần bằng các mô-đun nhỏ hơn để cải thiện hiệu quả. Tuy nhiên, những cách tiếp cận này hạn chế kiến trúc của học sinh. Ngược lại, chúng tôi tập trung vào các phương pháp chưng cất không phụ thuộc kiến trúc để có tính linh hoạt tốt hơn.

Cải tiến về mục tiêu chưng cất cũng được thực hiện, ví dụ: chuyển giao biểu diễn quan hệ, cấu trúc hoặc toàn diện của các mô hình ngôn ngữ có thể cung cấp tín hiệu hữu ích hơn cho học sinh (Park et al., 2021; Liu et al., 2022a; Tan et al., 2023). Khi tập hợp chuyển giao bị hạn chế, các phương pháp khác nhau về tăng cường dữ liệu (Liang et al., 2021; Zhang et al., 2022; Liu et al., 2022b) có thể được áp dụng thành công. Để giảm bớt khoảng cách năng lực giữa giáo viên và học sinh, các công việc trước đây đã đề xuất ủ nhiệt có lịch trình trong chuyển giao OD (Jafari et al., 2021), chưng cất đa giai đoạn với trợ lý giáo viên kích thước trung gian (Mirzadeh et al., 2020; Son et al., 2021), và meta-learning để tối ưu hóa giáo viên cho chưng cất học sinh (Zhou et al., 2022; Ma et al., 2022). Chúng tôi để việc khám phá các kỹ thuật nâng cao như vậy cho công việc tương lai.

Các chiến lược ánh xạ lớp cho chuyển giao HS cũng đã được nghiên cứu rộng rãi. Jiao et al. (2021) đề xuất một quá trình tìm kiếm tiến hóa để có được ánh xạ lớp tối ưu cho các tác vụ hạ nguồn cụ thể. Li et al. (2020) áp dụng Earth Mover's Distance để ưu tiên các ánh xạ với chi phí nhỏ hơn (tức là mất mát chưng cất). Cơ chế chú ý cũng có thể được áp dụng để ánh xạ các lớp học sinh đến các lớp giáo viên tương tự, trong đó độ tương tự được tính toán dựa trên độ tương tự cosine (Passban et al., 2021) hoặc dự đoán của các bộ phân loại nội bộ (Wu et al., 2021). Cuối cùng, ánh xạ ngẫu nhiên đã được chứng minh hoạt động tốt một cách đáng ngạc nhiên, có thể hoạt động như một bộ chính quy để ngăn chặn overfitting (Haidar et al., 2022). Trong nghiên cứu này, chúng tôi thay vào đó tập trung vào các chiến lược heuristic được thiết kế cẩn thận và dễ áp dụng.

Cuối cùng, có những cách tiếp cận khác nhau để giảm chi phí suy luận của các mô hình ngôn ngữ lớn, như lượng tử hóa (Zafrir et al., 2019; Shen et al., 2020; Kim et al., 2021; Bai et al., 2021), cắt tỉa

--- TRANG 11 ---
(Fan et al., 2020; Lagunas et al., 2021; Xia et al., 2022), cơ chế thoát sớm (Liu et al., 2020; Xin et al., 2021; Liao et al., 2021; Wang et al., 2022), và phân rã ma trận (Ben Noach and Goldberg, 2020; Mao et al., 2020; Chen et al., 2021; Tahaei et al., 2022). Nhiều cách tiếp cận này bổ sung cho các phương pháp chưng cất của chúng tôi và có thể được kết hợp để có hiệu quả hơn nữa.

B Thiết lập Chưng cất
Chúng tôi huấn luyện học sinh đơn ngữ trên toàn bộ Wikipedia và BookCorpus sử dụng Trình tối ưu hóa AdamW (Loshchilov and Hutter, 2019) với β1= 0.9, β2= 0.98. Đối với chuyển giao HS và MHA, học sinh được huấn luyện trong 7 epoch với tốc độ học đỉnh (LR) là 5e−4. Đối với chuyển giao OD, chúng tôi huấn luyện trong 3 epoch với LR đỉnh là 3e−4 sau chuyển giao HS. Chúng tôi sử dụng khởi động LR tuyến tính trong 5% đầu của các bước huấn luyện và sau đó suy giảm tuyến tính. Chúng tôi sử dụng kích thước batch là 32 với độ dài chuỗi tối đa được đặt thành 256 và huấn luyện trên 30 GPU V100.

Đối với chưng cất đa ngữ, chúng tôi sử dụng một tập con nhỏ của CC-100 chứa 7M câu, mà chúng tôi thấy là đủ để phát triển học sinh cạnh tranh. Chúng tôi thường sử dụng cùng thiết lập như chưng cất đơn ngữ, ngoại trừ chúng tôi sử dụng LR đỉnh là 8e−4 cho chuyển giao MHA. Học sinh đa ngữ được huấn luyện trên 2 GPU A100-80GB.

Cuối cùng, các siêu tham số cụ thể cho phương pháp (§3) như sau. Đối với chuyển giao OD, chúng tôi đặt nhiệt độ đầu ra T thành giá trị mặc định là 1. Đối với MiniLMv2, chúng tôi sử dụng Ar> Ah để chuyển giao kiến thức tinh vi hơn trong các ánh xạ Q/K/V: cụ thể, chúng tôi đặt Ar= 48, cũng được sử dụng trong Wang et al. (2021). Đối với DirectMiniLM, chúng tôi thấy việc sử dụng Ar=Ah mà không có ràng buộc trực giao trên Wα,a dẫn đến hiệu suất tốt nhất và sử dụng thiết lập này trong suốt các thí nghiệm của chúng tôi.

C Tìm Mô hình Học sinh Nhỏ hơn
Các học sinh nhỏ nhất của chúng tôi, mô hình 4 lớp và 3 lớp, được lấy như khuyến nghị từ quá trình Tìm kiếm Kiến trúc Neural để tìm kiến trúc học sinh tốt cho chưng cất không phụ thuộc tác vụ từ giáo viên XLM-RoBERTa, có điều kiện để giảm thiểu độ trễ suy luận trên CPU. Cụ thể, chúng tôi tuân theo phương pháp KD-NAS của Trivedi et al. (2023) và sửa đổi phần thưởng để giảm mất mát chưng cất LHS được định nghĩa trong Phương trình (6), cùng với độ trễ CPU của học sinh (lat(S)) được chuẩn hóa bởi độ trễ của giáo viên (lat(T)):

reward(S) = (1 − LHS) × lat(S) / (0.6 × lat(T)) − 0.06   (12)

Vui lòng tham khảo bài báo gốc của họ để biết thêm chi tiết.

D Kết quả Đánh giá cho Mô hình Tốt nhất
Chúng tôi bao gồm kết quả chi tiết của từng phương pháp chưng cất cho cấu hình tốt nhất (tức là chiến lược ánh xạ lớp). Cụ thể, chúng tôi hiển thị kết quả của từng tác vụ GLUE cho chưng cất đơn ngữ và đa ngữ trong Bảng 5 và 6. Chúng tôi hiển thị hiệu suất theo ngôn ngữ trên XNLI trong Bảng 7. Tất cả các tác vụ hạ nguồi được đánh giá trên 3 seed ngẫu nhiên.

Để đánh giá hiệu quả, chúng tôi đã không tiến hành tìm kiếm lưới đắt đỏ cho các siêu tham số tinh chỉnh. Sau một số điều chỉnh thủ công, chúng tôi sử dụng cùng LR là 2e−5 và kích thước batch là 32 để tinh chỉnh tất cả mô hình trên tất cả tác vụ. Chúng tôi sử dụng 3 epoch tinh chỉnh cho các tác vụ GLUE (ngoại trừ CoLA, nơi chúng tôi sử dụng 6 và 10 epoch cho mô hình đơn ngữ và đa ngữ) và 5 epoch cho XNLI.

E Chưng cất Hạn chế Kiến trúc: DistilBERT
DistilBERT (Sanh et al., 2019) là một trong những đường cơ sở sớm nhất và được sử dụng rộng rãi nhất. Phương pháp này bao gồm (1) khởi tạo lớp từ các lớp giáo viên, (2) chuyển giao HS dựa trên mất mát độ tương tự cosine, và (3) chuyển giao OD. Hai kỹ thuật đầu hạn chế kiến trúc của mỗi lớp học sinh phải giống hệt với mô hình giáo viên, điều này giới hạn phân tích của chúng tôi với kiến trúc học sinh 6L-DistilBERT.

| 6L-DistilBERT | Giáo viên |
|---------------|-----------|
| Avg. GLUE (Đơn ngữ) | 82.9 (0.5) | 85.5 (0.6) |
| Avg. GLUE (Đa ngữ) | 79.7 (0.5) | 84.8 (0.3) |
| Avg. XNLI (Đa ngữ) | 61.8 (0.5) | 70.9 (0.8) |

Bảng 4: Hiệu suất DistilBERT. Điểm GLUE trung bình được báo cáo cho tất cả tác vụ không có CoLA. Điểm XNLI trung bình được báo cáo cho tất cả ngôn ngữ. Trung bình được lấy trên 3 seed ngẫu nhiên với độ lệch chuẩn trong ngoặc đơn.

Như được hiển thị trong kết quả của Bảng 4, hiệu suất của DistilBERT thường không cạnh tranh với các phương pháp chưng cất của chúng tôi từ Bảng 3, đặc biệt trong môi trường đa ngữ.

--- TRANG 12 ---
| Mô hình | Phương pháp Chưng cất | Chiến lược Tốt nhất | Hiệu suất GLUE Tốt nhất | Avg. | Avg. |
|---------|---------------------|------------------|------------------------|-------|-------|
|         |                     |                  | MNLI | QQP | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE | (-CoLA) |  |
| 6L-DistilBERT | Chuyển giao HS | Uniform+Last | 82.6 | 86.2 | 88.7 | 90.8 | 45.9 | 85.9 | 89.7 | 65.1 | 79.4 (0.5) | 84.1 (0.4) |
|               | Chuyển giao OD | Uniform+Last | 82.7 | 86.5 | 88.3 | 91.3 | 50.8 | 85.5 | 89.7 | 64.4 | 79.9 (0.3) | 84.1 (0.2) |
|               | MiniLMv2 | (LT−2)th | 83.0 | 86.6 | 90.1 | 91.6 | 53.1 | 86.7 | 89.0 | 64.2 | 80.5 (0.4) | 84.4 (0.3) |
|               | DirectMiniLM | (LT−1)th | 82.9 | 86.6 | 90.0 | 91.4 | 52.7 | 86.4 | 89.0 | 64.9 | 80.5 (0.5) | 84.4 (0.4) |
| 6L | Chuyển giao HS | Uniform-Cons. | 78.3 | 85.0 | 85.9 | 90.9 | 31.2 | 83.2 | 84.4 | 56.3 | 74.4 (0.4) | 80.6 (0.3) |
|    | Chuyển giao OD | Uniform+Last | 79.1 | 84.6 | 86.3 | 89.7 | 38.6 | 82.3 | 83.7 | 57.9 | 75.3 (0.6) | 80.5 (0.3) |
|    | MiniLMv2 | (LT−1)th | 80.8 | 84.9 | 88.0 | 90.3 | 36.2 | 84.5 | 86.2 | 62.5 | 76.7 (0.1) | 82.5 (0.1) |
|    | DirectMiniLM | (LT−1)th | 80.0 | 85.1 | 87.2 | 90.9 | 36.1 | 83.3 | 85.9 | 59.7 | 76.0 (0.2) | 81.7 (0.2) |
| 4L | Chuyển giao HS | Uniform-Cons. | 77.3 | 84.9 | 85.7 | 90.0 | 26.9 | 83.4 | 83.0 | 60.1 | 73.9 (0.4) | 80.6 (0.3) |
|    | Chuyển giao OD | Uniform+Last | 78.2 | 84.6 | 85.1 | 90.1 | 32.2 | 83.3 | 83.2 | 55.1 | 74.0 (0.2) | 79.9 (0.4) |
|    | MiniLMv2 | (LT−2)th | 78.8 | 83.8 | 86.0 | 90.8 | 30.9 | 83.0 | 84.3 | 58.2 | 74.5 (0.2) | 80.7 (0.3) |
|    | DirectMiniLM | (LT−2)th | 79.0 | 84.2 | 85.7 | 90.0 | 29.7 | 82.5 | 84.9 | 56.6 | 74.1 (0.4) | 80.4 (0.4) |
| 3L | Chuyển giao HS | LTth | 74.3 | 82.8 | 84.0 | 89.4 | 20.0 | 80.8 | 83.4 | 57.5 | 71.5 (0.1) | 78.9 (0.3) |
|    | Chuyển giao OD | Uniform+Last | 73.8 | 81.9 | 83.4 | 86.6 | 15.1 | 78.8 | 82.7 | 52.8 | 69.4 (0.3) | 77.1 (0.4) |
|    | MiniLMv2 | (LT−2)th | 75.1 | 81.9 | 84.8 | 87.3 | 13.3 | 81.6 | 82.0 | 55.1 | 70.1 (0.4) | 78.3 (0.2) |
|    | DirectMiniLM | (LT−2)th | 75.7 | 82.2 | 84.0 | 88.5 | 16.8 | 81.0 | 83.3 | 53.5 | 70.6 (0.2) | 78.3 (0.3) |
| Giáo viên |  |  | 84.4 | 88.0 | 91.5 | 92.9 | 57.4 | 88.0 | 89.0 | 64.8 | 82.0 (0.6) | 85.5 (0.6) |

Bảng 5: Hiệu suất GLUE Học sinh Đơn ngữ cho tất cả tác vụ. Mỗi hàng hiển thị hiệu suất dựa trên chiến lược ánh xạ lớp tốt nhất. Mỗi điểm được báo cáo như trung bình trên 3 seed ngẫu nhiên (độ lệch chuẩn trong ngoặc đơn).

| Mô hình | Phương pháp Chưng cất | Chiến lược Tốt nhất | Hiệu suất GLUE Tốt nhất | Avg. | Avg. |
|---------|---------------------|------------------|------------------------|-------|-------|
|         |                     |                  | MNLI | QQP | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE | (-CoLA) |  |
| 6L-DistilBERT | Chuyển giao HS | Uniform+Last | 80.8 | 86.8 | 87.9 | 90.2 | 32.3 | 84.7 | 88.5 | 62.6 | 76.7 (0.6) | 83.1 (0.3) |
|               | Chuyển giao OD | Uniform+Last | 80.1 | 86.4 | 86.2 | 89.8 | 33.1 | 84.1 | 87.5 | 60.5 | 76.0 (1.0) | 82.1 (0.5) |
|               | MiniLMv2 | (LT−1)th | 81.3 | 85.8 | 88.8 | 89.6 | 40.2 | 85.9 | 89.3 | 61.0 | 77.7 (0.5) | 83.1 (0.3) |
|               | DirectMiniLM | (LT−2)th | 81.0 | 86.4 | 89.2 | 89.8 | 37.8 | 85.9 | 90.1 | 61.7 | 77.7 (0.7) | 83.4 (0.6) |
| 6L | Chuyển giao HS | Uniform-Cons. | 75.0 | 82.8 | 83.0 | 86.7 | 16.9 | 80.8 | 84.6 | 58.5 | 71.1 (0.6) | 78.8 (0.4) |
|    | Chuyển giao OD | Uniform-Cons. | 76.2 | 83.7 | 83.6 | 87.5 | 16.9 | 78.1 | 85.0 | 55.9 | 71.1 (0.6) | 78.7 (0.5) |
|    | MiniLMv2 | (LT−1)th | 78.3 | 83.7 | 86.9 | 89.1 | 29.2 | 83.6 | 85.1 | 60.3 | 74.5 (0.5) | 81.0 (0.4) |
|    | DirectMiniLM | (LT−1)th | 78.3 | 84.3 | 86.1 | 89.4 | 25.5 | 84.5 | 86.9 | 58.0 | 74.1 (0.6) | 81.1 (0.5) |
| 4L | Chuyển giao HS | Uniform+Last | 75.6 | 83.7 | 83.8 | 87.8 | 18.3 | 81.2 | 83.3 | 59.0 | 71.6 (0.7) | 79.2 (0.5) |
|    | Chuyển giao OD | Uniform | 73.4 | 83.8 | 81.2 | 85.2 | 17.0 | 80.0 | 82.8 | 58.6 | 70.3 (0.7) | 77.9 (0.7) |
|    | MiniLMv2 | (LT−1)th | 76.8 | 83.4 | 85.2 | 87.6 | 17.1 | 83.9 | 86.0 | 58.1 | 72.3 (0.7) | 80.2 (0.5) |
|    | DirectMiniLM | (LT−1)th | 77.0 | 83.6 | 85.2 | 88.5 | 19.2 | 83.5 | 85.2 | 59.1 | 72.7 (0.6) | 80.3 (0.4) |
| 3L | Chuyển giao HS | Uniform-Cons. | 71.0 | 80.7 | 82.1 | 84.6 | 11.0 | 75.8 | 82.2 | 54.9 | 67.8 (0.4) | 75.9 (0.4) |
|    | Chuyển giao OD | Uniform+Last | 68.1 | 79.4 | 79.7 | 81.9 | 2.6 | 61.5 | 81.2 | 54.6 | 63.6 (0.5) | 72.3 (0.6) |
|    | MiniLMv2 | (LT−1)th | 72.7 | 80.6 | 83.2 | 84.6 | 9.7 | 70.6 | 81.7 | 57.4 | 67.6 (0.6) | 75.8 (0.5) |
|    | DirectMiniLM | (LT−2)th | 72.2 | 81.2 | 83.4 | 84.8 | 15.9 | 67.9 | 82.0 | 58.0 | 68.2 (1.1) | 75.6 (1.1) |
| Giáo viên |  |  | 84.1 | 87.9 | 90.2 | 91.9 | 51.7 | 86.6 | 91.4 | 61.4 | 80.6 (0.3) | 84.8 (0.3) |

Bảng 6: Hiệu suất GLUE Học sinh Đa ngữ cho tất cả tác vụ. Mỗi hàng hiển thị hiệu suất dựa trên chiến lược ánh xạ lớp tốt nhất. Mỗi điểm được báo cáo như trung bình trên 3 seed ngẫu nhiên (độ lệch chuẩn trong ngoặc đơn).

| Mô hình | Phương pháp Chưng cất | Chiến lược Tốt nhất | Hiệu suất XNLI Tốt nhất | Avg. |
|---------|---------------------|------------------|------------------------|-------|
|         |                     |                  | ar | bg | de | el | en | es | fr | hi | ru | sw | th | tr | ur | vi | zh |  |
| 6L-DistilBERT | Chuyển giao HS | Uniform+Last | 64.7 | 69.7 | 69.6 | 69.2 | 80.7 | 72.0 | 70.2 | 64.6 | 67.7 | 51.2 | 65.3 | 62.5 | 58.9 | 70.4 | 68.6 | 67.0 (0.4) |
|               | Chuyển giao OD | Uniform+Last | 63.7 | 69.1 | 69.4 | 67.0 | 78.6 | 70.7 | 68.9 | 60.0 | 69.0 | 51.2 | 65.4 | 61.9 | 57.9 | 68.5 | 68.8 | 66.0 (0.6) |
|               | MiniLMv2 | (LT−1)th | 65.5 | 71.6 | 72.1 | 71.5 | 81.4 | 75.0 | 73.5 | 65.3 | 70.6 | 58.1 | 65.1 | 67.1 | 60.9 | 69.7 | 69.3 | 69.1 (0.5) |
|               | DirectMiniLM | (LT−1)th | 63.8 | 69.4 | 69.3 | 68.5 | 79.2 | 73.2 | 71.2 | 64.1 | 67.2 | 55.1 | 63.9 | 65.6 | 59.7 | 66.6 | 67.0 | 66.9 (0.4) |
| 6L | Chuyển giao HS | Uniform+Last | 59.7 | 67.2 | 63.4 | 65.6 | 75.9 | 68.7 | 66.8 | 58.3 | 62.4 | 48.9 | 62.7 | 59.1 | 53.4 | 63.2 | 65.1 | 62.7 (0.4) |
|    | Chuyển giao OD | Uniform+Last | 55.7 | 62.6 | 63.7 | 59.2 | 76.5 | 66.9 | 63.7 | 54.1 | 62.0 | 45.7 | 57.9 | 56.3 | 51.0 | 62.8 | 62.2 | 61.0 (0.5) |
|    | MiniLMv2 | (LT−1)th | 65.0 | 69.7 | 70.4 | 68.8 | 80.3 | 73.1 | 71.5 | 62.9 | 69.3 | 53.8 | 65.0 | 65.7 | 59.6 | 69.2 | 68.0 | 67.5 (0.5) |
|    | DirectMiniLM | LTth | 63.2 | 68.8 | 70.1 | 68.1 | 78.4 | 70.5 | 70.0 | 62.2 | 66.6 | 52.4 | 64.6 | 64.0 | 59.1 | 66.2 | 66.9 | 66.1 (0.5) |
| 4L | Chuyển giao HS | Uniform+Last | 56.9 | 64.5 | 66.2 | 66.3 | 77.3 | 68.2 | 63.9 | 57.9 | 63.9 | 49.2 | 61.8 | 59.2 | 54.0 | 64.2 | 64.2 | 62.5 (0.5) |
|    | Chuyển giao OD | Uniform+Last | 55.7 | 62.6 | 63.7 | 59.2 | 76.5 | 66.9 | 63.7 | 54.1 | 62.0 | 45.7 | 57.9 | 56.3 | 51.0 | 62.8 | 62.2 | 60.0 (0.5) |
|    | MiniLMv2 | (LT−1)th | 62.9 | 67.5 | 67.8 | 68.2 | 77.8 | 70.7 | 68.2 | 62.4 | 67.0 | 51.0 | 63.6 | 64.7 | 57.7 | 67.2 | 67.4 | 65.6 (0.8) |
|    | DirectMiniLM | (LT−2)th | 63.2 | 68.3 | 67.9 | 67.6 | 78.3 | 69.7 | 69.6 | 63.1 | 64.9 | 49.0 | 64.2 | 62.4 | 58.6 | 67.2 | 66.3 | 65.4 (0.7) |
| 3L | Chuyển giao HS | Uniform | 58.3 | 63.4 | 60.5 | 60.6 | 74.1 | 65.6 | 61.6 | 56.6 | 61.4 | 46.7 | 57.3 | 55.9 | 51.8 | 61.1 | 63.1 | 59.9 (0.5) |
|    | Chuyển giao OD | Uniform+Last | 45.6 | 52.3 | 48.7 | 47.8 | 69.9 | 55.0 | 49.4 | 42.9 | 47.3 | 40.9 | 46.3 | 44.4 | 41.6 | 49.7 | 47.8 | 48.6 (0.5) |
|    | MiniLMv2 | (LT−1)th | 60.0 | 64.9 | 63.6 | 64.3 | 74.1 | 66.7 | 64.2 | 58.2 | 61.8 | 49.4 | 59.7 | 60.7 | 55.3 | 64.2 | 62.4 | 62.0 (0.8) |
|    | DirectMiniLM | (LT−1)th | 57.4 | 63.0 | 64.1 | 63.3 | 74.3 | 66.1 | 65.1 | 57.2 | 62.1 | 46.7 | 56.7 | 58.1 | 55.2 | 63.6 | 61.8 | 61.0 (0.4) |
| Giáo viên |  |  | 69.1 | 73.2 | 74.1 | 72.2 | 83.4 | 75.1 | 73.1 | 69.0 | 71.3 | 57.3 | 69.7 | 67.7 | 64.1 | 70.8 | 73.3 | 70.9 (0.8) |

Bảng 7: Hiệu suất XNLI Học sinh Đa ngữ cho 15 ngôn ngữ. Mỗi hàng hiển thị hiệu suất dựa trên chiến lược ánh xạ lớp tốt nhất. Mỗi điểm được báo cáo như trung bình trên 3 seed ngẫu nhiên (độ lệch chuẩn trong ngoặc đơn).
