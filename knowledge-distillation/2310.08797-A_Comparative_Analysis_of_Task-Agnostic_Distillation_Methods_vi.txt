# Phân tích So sánh các Phương pháp Chưng cất Bất khả tri Tác vụ
# để Nén các Mô hình Ngôn ngữ Transformer

Takuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee
IBM Research AI
{takuma.udagawa@, aashka.trivedi@, mimerler@us., bhatta@us.}ibm.com

## Tóm tắt

Các mô hình ngôn ngữ lớn đã trở thành thành phần quan trọng trong NLP hiện đại, đạt được hiệu suất tối ưu trong nhiều tác vụ khác nhau. Tuy nhiên, chúng thường không hiệu quả cho việc triển khai thực tế do chi phí suy luận đắt đỏ. Chưng cất tri thức là một kỹ thuật đầy hứa hẹn để cải thiện hiệu quả của chúng trong khi vẫn giữ lại hầu hết tính hiệu quả. Trong bài báo này, chúng tôi tái tạo, so sánh và phân tích một số phương pháp đại diện cho chưng cất bất khả tri tác vụ (mục đích chung) của các mô hình ngôn ngữ Transformer. Mục tiêu nghiên cứu của chúng tôi bao gồm chuyển giao Phân phối Đầu ra (OD), chuyển giao Trạng thái Ẩn (HS) với các chiến lược ánh xạ lớp khác nhau, và chuyển giao Chú ý Đa đầu (MHA) dựa trên MiniLMv2. Thông qua các thí nghiệm mở rộng, chúng tôi nghiên cứu hiệu quả của từng phương pháp cho các kiến trúc học sinh khác nhau trong cả môi trường đơn ngữ (tiếng Anh) và đa ngữ. Nhìn chung, chúng tôi cho thấy chuyển giao MHA dựa trên MiniLMv2 thường là lựa chọn tốt nhất cho chưng cất và giải thích các lý do tiềm tàng đằng sau thành công của nó. Hơn nữa, chúng tôi cho thấy chuyển giao HS vẫn là một đường cơ sở cạnh tranh, đặc biệt dưới chiến lược ánh xạ lớp tinh vi, trong khi chuyển giao OD luôn tụt hậu so với các phương pháp khác. Những phát hiện từ nghiên cứu này đã giúp chúng tôi triển khai các mô hình học sinh hiệu quả nhưng hiệu quả cho các ứng dụng quan trọng về độ trễ.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn đã trở thành thành phần quan trọng trong NLP hiện đại. Chúng đã đạt được hiệu suất vượt trội trên các tác vụ hạ lưu khác nhau (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020) và khả năng của chúng cho thấy sự cải thiện liên tục với nhiều tính toán, dữ liệu và tham số mô hình hơn (Kaplan et al., 2020; Brown et al., 2020; Touvron et al., 2023). Mặt tiêu cực, việc triển khai các mô hình như vậy trong môi trường thực tế ngày càng trở nên khó khăn do tính không hiệu quả của chúng, tức là chi phí tính toán, bộ nhớ, độ trễ và lưu trữ cao (Xu and McAuley, 2023).

Chưng cất tri thức (Hinton et al., 2015) là một kỹ thuật đầy hứa hẹn để khắc phục thách thức này bằng cách chuyển giao tri thức của mô hình gốc (giáo viên) sang một mô hình nhỏ hơn, hiệu quả hơn (học sinh). Điều này có thể được thực hiện theo cách cụ thể tác vụ (Turc et al., 2019; Jiao et al., 2020) hoặc bất khả tri tác vụ (Sanh et al., 2019; Wang et al., 2020). Cách thứ hai chỉ yêu cầu chưng cất một học sinh mục đích chung duy nhất có thể được tinh chỉnh trực tiếp cho bất kỳ tác vụ hạ lưu nào. Do tính tiện lợi cao, chúng tôi tập trung vào phương pháp sau này trong nghiên cứu này.

Trong những năm gần đây, đã có nhiều phương pháp được đề xuất cho chưng cất bất khả tri tác vụ của các mô hình ngôn ngữ Transformer. Mục tiêu của bài báo này là tái tạo, so sánh và phân tích các phương pháp đại diện nhất trong lĩnh vực này. Chúng tôi thường tập trung vào chưng cất bất khả tri kiến trúc không áp đặt hoặc áp đặt hạn chế tối thiểu đối với kiến trúc học sinh¹: các phương pháp đại diện bao gồm chuyển giao Phân phối Đầu ra (OD) (Hinton et al., 2015), chuyển giao Trạng thái Ẩn (HS) dựa trên ánh xạ tuyến tính (Jiao et al., 2020; Mukherjee et al., 2021) và chuyển giao Chú ý Đa đầu (MHA) dựa trên MiniLMv2 (Wang et al., 2021).

Đối với chuyển giao HS, chiến lược ánh xạ lớp giữa các lớp giáo viên và học sinh đóng vai trò quan trọng trong hiệu suất tổng thể, tuy nhiên, chiến lược tối ưu vẫn chưa được biết hoặc gây tranh cãi (Sun et al., 2019; Wu et al., 2020; Ko et al., 2023). Do đó, chúng tôi khám phá một loạt chiến lược đa dạng để đánh giá thực nghiệm từng kỹ thuật.

Đối với chuyển giao MHA, phương pháp MiniLMv2 đã được chứng minh đạt hiệu suất tối ưu, tuy nhiên, có tương đối ít hiểu biết đằng sau thành công của nó. Do đó, chúng tôi phát triển một biến thể mới có tên DirectMiniLM hữu ích để hiểu tính hiệu quả đằng sau MiniLMv2 cả về mặt lý thuyết và thực nghiệm.

¹Bằng bất khả tri kiến trúc, chúng tôi có nghĩa là học sinh và giáo viên có thể có các tham số kiến trúc khác nhau (ví dụ: số lớp, đầu chú ý, kích thước trạng thái ẩn, v.v.).

---

## 2 Mô hình Ngôn ngữ Transformer

Đầu tiên, chúng tôi xem xét ngắn gọn kiến trúc tiêu chuẩn của các mô hình ngôn ngữ Transformer (Vaswani et al., 2017; Devlin et al., 2019). Một Transformer bao gồm một ngăn xếp L lớp Transformer, trong đó mỗi lớp bao gồm hai lớp con: một lớp Chú ý Đa đầu (MHA) theo sau bởi một lớp Chuyển tiếp (FF) được kết nối đầy đủ (Hình 1, (a)).

Chính thức, gọi x là chuỗi đầu vào, dₕ là kích thước trạng thái ẩn, và Hᵢ ∈ R^|x|×dₕ là trạng thái ẩn của lớp Transformer thứ i (H₀ biểu thị nhúng chuỗi đầu vào). Cho Hᵢ, lớp MHA đầu tiên tính toán các ánh xạ truy vấn, khóa và giá trị Qᵢ,ₐ, Kᵢ,ₐ, Vᵢ,ₐ cho mỗi đầu chú ý a ∈ [1, Aₕ], được kết hợp để thu được đầu ra đầu chú ý Oᵢ,ₐ:

Qᵢ,ₐ = HᵢW^Q,i,a                    (1)
Kᵢ,ₐ = HᵢW^K,i,a                    (2)
Vᵢ,ₐ = HᵢW^V,i,a                    (3)
Oᵢ,ₐ = softmax(Qᵢ,ₐK^T_{i,a}/√dₖ)Vᵢ,ₐ  (4)

Ở đây, dₖ biểu thị kích thước đầu chú ý (thường được đặt thành dₕ/Aₕ) và W^Q,i,a, W^K,i,a, W^V,i,a ∈ R^{dₕ×dₖ} là các ma trận trọng số được học. Đầu ra của lớp MHA là phép nối của Oᵢ,ₐ, cụ thể là MHA(Hᵢ) = ⊕^{Aₕ}_{a=1} Oᵢ,ₐ.

Tiếp theo, đầu ra lớp MHA được theo sau bởi một lớp FF theo vị trí với kích thước trung gian df và một hàm kích hoạt phi tuyến (chúng tôi sử dụng GELU (Hendrycks and Gimpel, 2016) trong tất cả các mô hình). Trạng thái ẩn của lớp Transformer tiếp theo được tính như Hᵢ₊₁ = FF(MHA(Hᵢ)).²

Cuối cùng, để dự đoán phân phối đầu ra trên toàn bộ từ vựng V, một lớp tuyến tính W^O ∈ R^{dₕ×|V|} được áp dụng trên đỉnh trạng thái ẩn cuối để tính toán các logit z = H_LW^O ∈ R^{|x|×|V|}. Phân phối đầu ra có thể được thu được bằng cách áp dụng hàm softmax trên z, ký hiệu là softmax(z).

Trong suốt bài báo này, chúng tôi giả định rằng cả học sinh và giáo viên đều là mô hình ngôn ngữ Transformer với L^S và L^T lớp, tương ứng.

²Cả hai lớp con MHA và FF đều có kết nối dư (He et al., 2016) và được theo sau bởi chuẩn hóa lớp (Ba et al., 2016), được bỏ qua để ngắn gọn.

---

## 3 Phương pháp Chưng cất

Tiếp theo, chúng tôi giới thiệu các phương pháp chưng cất bất khả tri tác vụ đại diện được minh họa trong Hình 1, (b-d). Đối với chuyển giao Chú ý Đa đầu (MHA), chúng tôi xem xét hai phương pháp: MiniLMv2 và biến thể mới DirectMiniLM. Để khảo sát các phương pháp và chủ đề tiên tiến mà chúng tôi không thể đề cập trong nghiên cứu này, vui lòng tham khảo Phụ lục A.

**Chuyển giao Phân phối Đầu ra (OD)** Phân phối đầu ra của giáo viên chứa thông tin hữu ích về xác suất tương đối của các dự đoán hợp lý (ngay cả khi không chính xác) (Hinton et al., 2015). Trong chuyển giao OD, học sinh được huấn luyện để sao chép phân phối đầu ra của giáo viên. Điều này đạt được bằng cách tối ưu hóa hàm mất mát sau đây, trong đó z^S, z^T biểu thị logit học sinh/giáo viên, CE(.) là mất mát entropy chéo và T là nhiệt độ đầu ra:

L_{OD} = T² · CE(softmax(z^T/T), softmax(z^S/T))     (5)

**Chuyển giao Trạng thái Ẩn (HS)** Các mô hình ngôn ngữ Transformer học dần dần các đặc trưng hữu ích và có thể tổng quát hóa từng lớp một. Trong chuyển giao HS, học sinh được huấn luyện để dự đoán những đặc trưng hữu ích như vậy được biểu diễn trong các trạng thái ẩn của giáo viên.

Chính thức, mỗi lớp học sinh được ánh xạ đến một tập hợp các lớp giáo viên cần được dự đoán. Gọi φ(i) biểu thị tập hợp được ánh xạ từ lớp học sinh thứ i, trong đó ∅ ⊆ φ(i) ⊆ [1, L^T]. Đối với mỗi j ∈ φ(i), trạng thái ẩn của lớp học sinh thứ i H^S_i ∈ R^{|x|×d^S_h} được biến đổi tuyến tính để dự đoán trạng thái ẩn của lớp giáo viên thứ j H^T_j ∈ R^{|x|×d^T_h}.³ Điều này được biểu diễn bởi hàm mất mát sau đây, trong đó W^j_i ∈ R^{d^S_h×d^T_h} biểu thị trọng số biến đổi tuyến tính và MSE(.) là mất mát lỗi bình phương trung bình:

L_{HS} = ∑^{L^S}_{i=1} ∑_{j∈φ(i)} MSE(H^S_i W^j_i, H^T_j)     (6)

Một vấn đề mở trong phương pháp này là việc lựa chọn chiến lược ánh xạ lớp φ. Chúng tôi tiến hành thí nghiệm mở rộng để so sánh một loạt chiến lược đa dạng, sẽ được thảo luận trong §4.

**MiniLMv2** Lớp MHA là thành phần chính trong các mô hình ngôn ngữ Transformer kiểm soát các phụ thuộc tầm xa và tương tác trong văn bản đầu vào. MiniLMv2 (Wang et al., 2021) là một phương pháp hiệu quả để chuyển giao sâu mô-đun này trong khi cho phép số lượng đầu chú ý khác nhau A^S_h và A^T_h cho học sinh và giáo viên. Ý tưởng chính của họ là chưng cất các ma trận quan hệ chú ý (Q-Q, K-K và V-V) thu được bằng cách đầu tiên nối các ánh xạ truy vấn (Q), khóa (K) và giá trị (V) từ tất cả các đầu chú ý và chia lại chúng thành cùng số lượng đầu quan hệ chú ý A_r.

Chính thức, gọi A^S_{Q,i,a}, A^S_{K,i,a}, A^S_{V,i,a} ∈ R^{|x|×d^S_r} biểu thị các truy vấn, khóa và giá trị được nối và chia lại cho lớp học sinh thứ i, trong đó a ∈ [1, A_r] và d^S_r = d^S_h/A_r. Ví dụ, ⊕^{A^S_h}_{a=1} Q^S_{i,a} = ⊕^{A_r}_{a=1} A^S_{Q,i,a}, tức là các truy vấn gốc từ A^S_h đầu chú ý được đơn giản nối lại và sau đó chia lại thành A_r ma trận. Chúng tôi sử dụng ký hiệu tương tự cho lớp giáo viên thứ j, A^T_{Q,j,a}, A^T_{K,j,a}, A^T_{V,j,a} ∈ R^{|x|×d^T_r}, trong đó d^T_r = d^T_h/A_r. Sau đó, hàm mất mát của MiniLMv2 có thể được định nghĩa như sau:

L_{MHA} = ∑_{α∈{Q,K,V}} ∑^{A_r}_{a=1} CE(R^T_{α,j,a}, R^S_{α,i,a})     (7)

R^T_{α,j,a} = softmax(A^T_{α,j,a}A^{T^T}_{α,j,a}/√d^T_r)     (8)

R^S_{α,i,a} = softmax(A^S_{α,i,a}A^{S^T}_{α,i,a}/√d^S_r)     (9)

Ở đây, R^T_{α,j,a}, R^S_{α,i,a} ∈ R^{|x|×|x|} biểu thị các ma trận quan hệ chú ý được tính toán dựa trên tích ma trận của A^T_{α,i,a}, A^S_{α,i,a} trong eq. (8), (9), tương ứng. Trực quan, điều này nhằm chuyển giao các truy vấn (Q), khóa (K) và giá trị (V) của giáo viên theo cách gián tiếp qua tích ma trận của chúng (Q-Q, K-K và V-V).

Tuy nhiên, có rất ít biện minh cho tại sao phương pháp này hoạt động hiệu quả. Cũng khó để so sánh trực tiếp phương pháp này với chuyển giao HS vì các mất mát được tính toán khác nhau. Để hiểu rõ hơn MiniLMv2, chúng tôi đề xuất biến thể mới có tên DirectMiniLM cho phân tích của chúng tôi.

**DirectMiniLM** Trong DirectMiniLM, chúng tôi nhằm chuyển giao các ánh xạ Q/K/V của giáo viên một cách trực tiếp hơn thông qua biến đổi tuyến tính của các ánh xạ học sinh, giống như chúng tôi đã làm trong chuyển giao HS. Cụ thể, chúng tôi sử dụng hàm mất mát sau đây với biến đổi tuyến tính W_{α,a} ∈ R^{d^S_r×d^T_r}:

L^{Direct}_{MHA} = ∑_{α∈{Q,K,V}} ∑^{A_r}_{a=1} MSE(A^S_{α,i,a}W_{α,a}, A^T_{α,j,a})     (10)

³Lưu ý rằng d^S_h và d^T_h là kích thước trạng thái ẩn học sinh và giáo viên có thể nhận các giá trị khác nhau.

DirectMiniLM quan trọng trong hai khía cạnh. Đầu tiên, phương pháp này có thể so sánh trực tiếp với chuyển giao HS dựa trên eq. (6) với điểm khác biệt duy nhất là thông tin nào bạn chuyển giao: các trạng thái ẩn H^T_i → H^S_j hoặc các ánh xạ Q/K/V A^T_{α,i,a} → A^S_{α,j,a}. Từ so sánh này, chúng ta có thể định lượng lợi thế chính xác của việc chuyển giao từng tri thức theo cách so sánh trực tiếp.

Thứ hai, DirectMiniLM cũng có liên quan chặt chẽ với MiniLMv2: nếu chúng ta ràng buộc W_{α,a} là trực giao (tức là W_{α,a}W^T_{α,a} = I) và lấy tích ma trận cho mỗi hạng tử trong mất mát MSE trong eq. (10), chúng ta thu được hàm mất mát sau đây:

∑_{α∈{Q,K,V}} ∑^{A_r}_{a=1} MSE(A^S_{α,i,a}A^{S^T}_{α,i,a}, A^T_{α,j,a}A^{T^T}_{α,i,a})     (11)

Mất mát này rất giống với MiniLMv2 từ eq. (7) với điểm khác biệt nhỏ là sử dụng mất mát MSE thay vì mất mát CE với softmax. Do đó, DirectMiniLM với các ràng buộc nhất định tự nhiên tương ứng với MiniLMv2. Điểm khác biệt chính là A^T_{α,i,a} được chuyển giao trực tiếp (với ánh xạ tuyến tính) hay gián tiếp (với ma trận quan hệ): bằng cách so sánh hai phương pháp này, chúng ta có thể định lượng chính xác lợi thế của từng kỹ thuật tối ưu hóa.

---

## 4 Thiết lập Thí nghiệm

Chúng tôi khám phá các phương pháp chưng cất tri thức bất khả tri tác vụ dưới hai thiết lập:⁴

1. **Chưng cất Đơn ngữ**: Chúng tôi huấn luyện học sinh tiếng Anh sử dụng BERT mã nguồn mở (Devlin et al., 2019) làm giáo viên. Những mô hình này được chưng cất trên cùng một kho dữ liệu được sử dụng để huấn luyện trước BERT, tức là Wikipedia tiếng Anh (Devlin et al., 2019) và BookCorpus (Zhu et al., 2015).

2. **Chưng cất Đa ngữ**: Chúng tôi huấn luyện học sinh đa ngữ sử dụng XLM-RoBERTa nội bộ (Conneau et al., 2020) làm giáo viên, và chưng cất trên tập dữ liệu CC100 (Conneau et al., 2020), bao gồm dữ liệu trong hơn 100 ngôn ngữ. Chúng tôi chỉ sử dụng một tập con nhỏ của kho dữ liệu để tiến hành thí nghiệm trong ngân sách tính toán hợp lý trong khi duy trì phân phối theo ngôn ngữ.

Trong cả hai thiết lập, chúng tôi sử dụng kiến trúc Base (12 lớp) cho giáo viên, như được hiển thị trong Bảng 1. Để biết thêm chi tiết về từng thiết lập chưng cất (ví dụ: siêu tham số), vui lòng tham khảo Phụ lục B.

**Mô hình Học sinh** Để tiến hành so sánh mạnh mẽ các phương pháp chưng cất tri thức đại diện, chúng tôi huấn luyện 4 học sinh với kiến trúc và ngân sách độ trễ/tham số khác nhau. Tóm tắt về kiến trúc học sinh, với tham số và độ trễ suy luận của chúng, được hiển thị trong Bảng 1.

Học sinh lớn nhất của chúng tôi là mô hình 6 lớp tuân theo cùng kiến trúc như DistilBERT (Sanh et al., 2019). Chúng tôi cũng sử dụng mô hình 6 lớp được sử dụng trong Mukherjee et al. (2021), có kích thước ẩn nhỏ hơn so với giáo viên. Học sinh 4 và 3 lớp nhỏ hơn của chúng tôi được thu được như các khuyến nghị từ quy trình Tìm kiếm Kiến trúc Mạng thần kinh (Trivedi et al., 2023) để tìm kiến trúc học sinh tốt cho chưng cất từ giáo viên XLM-RoBERTa, với điều kiện tối thiểu hóa độ trễ trên CPU. Vui lòng tham khảo Phụ lục C để biết thêm chi tiết.

**Chiến lược Ánh xạ Lớp** Chiến lược ánh xạ lớp φ là một tham số cần được xem xét cho cả chuyển giao HS và MHA. Đối với chuyển giao HS, chúng tôi khám phá ba thiết lập sau:

1. **Ánh xạ Đơn**: Chúng tôi chỉ chưng cất lớp giáo viên cuối (L^T) vào lớp học sinh cuối, đã được chứng minh là một đường cơ sở đơn giản nhưng cạnh tranh (Ko et al., 2023).

2. **Ánh xạ 1-đến-1**: Nghiên cứu trước cho thấy ánh xạ không chỉ lớp cuối mà còn các lớp trung gian cải thiện chưng cất (Sun et al., 2019). Trong ánh xạ 1-đến-1, chúng tôi chưng cất một lớp giáo viên vào mỗi lớp học sinh bằng cách chọn:
   • **Cuối L^S** lớp giáo viên, tức là φ(i) = {L^T - L^S + i} (i ∈ [1, L^S]). Thực nghiệm, các lớp giáo viên cuối nắm bắt tri thức cấp cao hơn (ví dụ: ngữ nghĩa) trong biểu diễn của chúng (Tenney et al., 2019; Jawahar et al., 2019).
   • **Lựa chọn Đồng nhất** của các lớp giáo viên chọn mỗi lớp giáo viên thứ k, tức là φ(i) = {ki}, trong đó k = ⌈L^T/L^S⌉.⁵ Phương pháp này cũng có thể chuyển giao các lớp giáo viên thấp hơn, thực nghiệm nắm bắt tri thức cục bộ (ví dụ: cú pháp) (Tenney et al., 2019).

3. **Ánh xạ 1-đến-N**: Một số công trình thậm chí cho thấy ánh xạ mỗi lớp học sinh đến nhiều lớp giáo viên có thể tránh mất thông tin và tạo điều kiện cho học tập của học sinh (Wu et al., 2020; Passban et al., 2021). Đối với Ánh xạ 1-đến-N, chúng tôi khám phá các lựa chọn lớp giáo viên sau:
   • **Lựa chọn đồng nhất** của k lớp liên tiếp (Uniform-Cons.), tức là φ(i) = [k(i-1), ki], trong đó k = ⌈L^T/L^S⌉. Điều này tránh mất thông tin vì tất cả các lớp giáo viên được ánh xạ đến ít nhất một lớp học sinh.
   • **Kết hợp các chiến lược Uniform và Last** từ ánh xạ 1-đến-1 (Uniform+Last). Điều này chọn 2 lớp giáo viên cho mỗi lớp học sinh dựa trên mỗi chiến lược 1-đến-1, kỳ vọng lấy được điều tốt nhất từ cả hai phương pháp.

Đối với chuyển giao MHA, chúng tôi luôn áp dụng chiến lược ánh xạ đơn và chưng cất một lớp giáo viên duy nhất vào lớp học sinh cuối, theo Wang et al. (2021). Cụ thể, chúng tôi thí nghiệm với ba lớp giáo viên cuối làm lựa chọn cho chưng cất cho cả MiniLMv2 và DirectMiniLM. Bảng 2 tóm tắt các tùy chọn lựa chọn lớp của chúng tôi.

Trong khi chuyển giao OD có thể được tiến hành từ đầu, chúng tôi thấy điều này hội tụ chậm và không hoạt động cạnh tranh.⁶ Do đó, chúng tôi áp dụng kiểu chưng cất đa giai đoạn (Mukherjee et al., 2021) và tiến hành chuyển giao OD sau chuyển giao HS, sử dụng điểm kiểm tra đã chưng cất từ chuyển giao HS. Phương pháp này hội tụ nhanh hơn nhiều với hiệu suất cuối tốt hơn, do đó chúng tôi áp dụng phương pháp này làm phương pháp chuyển giao OD đại diện.

⁴Lưu ý rằng chúng tôi giới hạn nghiên cứu của mình với các mô hình chỉ mã hóa và để lại việc chưng cất các mô hình chỉ giải mã (Radford et al., 2019) hoặc mã hóa-giải mã (Lewis et al., 2020) như công việc tương lai.

⁵Chiến lược này được sử dụng trong DistilBERT (Sanh et al., 2019) và cũng được biết đến như chiến lược "bỏ qua" (Sun et al., 2019).

⁶Học sinh đơn ngữ 6L của chúng tôi mất 49 giờ trên 30 GPU V100 để đạt hiệu suất chấp nhận được, trong khi cùng mô hình đạt điểm số tốt hơn chỉ trong 10,5 giờ khi khởi tạo từ điểm kiểm tra được chuyển giao HS.

---

## 5 Đánh giá và Kết quả

Đối với cả mô hình đơn ngữ và đa ngữ của chúng tôi, chúng tôi đo hiệu suất trên Benchmark GLUE tiếng Anh (Wang et al., 2019) và báo cáo điểm trung bình của tất cả các tác vụ (không có CoLA⁷). Đối với các mô hình đa ngữ, chúng tôi cung cấp đánh giá trên tập dữ liệu XNLI (Conneau et al., 2018), một tập hợp các tác vụ suy luận đánh giá hiệu suất của mô hình trên 15 ngôn ngữ sau khi được tinh chỉnh chỉ trên dữ liệu huấn luyện tiếng Anh. Chúng tôi báo cáo điểm trung bình của tất cả các ngôn ngữ cho XNLI.

Bảng 3 tóm tắt hiệu suất của từng phương pháp chưng cất trên 4 kiến trúc học sinh. Để đánh giá chi tiết từng phương pháp dựa trên cấu hình tốt nhất, vui lòng tham khảo Phụ lục D. Chúng tôi cũng cung cấp so sánh với DistilBERT (Sanh et al., 2019), một phương pháp ràng buộc kiến trúc đại diện, trong Phụ lục E.

**Chuyển giao HS** Từ Bảng 3, chúng ta có thể xác minh rằng hiệu suất của chuyển giao HS thay đổi với các chiến lược ánh xạ lớp khác nhau, và không có chiến lược nào thống trị các chiến lược khác trong tất cả các thiết lập. Trong thiết lập đơn ngữ, chúng tôi thấy rằng chiến lược ánh xạ đơn hoạt động cạnh tranh, phù hợp với những phát hiện của Ko et al. (2023). Tuy nhiên, trong thiết lập đa ngữ, các chiến lược 1-đến-N tinh vi hơn thường cho thấy sự ưu việt so với các đường cơ sở đơn giản hơn. Điều này cho thấy rằng việc giám sát nhiều hơn từ giáo viên có thể hữu ích (và tệ nhất là vô hại), do đó chúng tôi ủng hộ việc áp dụng các chiến lược 1-đến-N, đặc biệt trong chưng cất đa ngữ đầy thách thức.

**Chuyển giao OD** Như đã đề cập trong §4, chúng tôi khởi tạo mô hình từ các điểm kiểm tra được chuyển giao HS với từng chiến lược ánh xạ lớp. Thú vị thay, chúng ta thấy sự suy giảm nhẹ về hiệu suất trên các tác vụ hạ lưu so với chỉ chuyển giao HS, với mất mát đáng kể được quan sát cho các học sinh nhỏ hơn. Điều này cho thấy rằng việc học các biểu diễn hiệu quả từ các tín hiệu phân phối đầu ra là khó khăn, đặc biệt đối với các học sinh có khả năng thấp hơn. Hơn nữa, xét đến việc chuyển giao OD có thể tốn kém về mặt tính toán, chuyển giao HS là một lựa chọn thay thế rẻ hơn và hiệu quả hơn cho chuyển giao tri thức.

**Chuyển giao MHA** Đối với cả MiniLMv2 và DirectMiniLM, chúng tôi thấy chưng cất lớp giáo viên giữa-trên, tức là chiến lược (L^T-1) hoặc (L^T-2), dẫn đến hiệu suất tốt nhất, phù hợp với những phát hiện ban đầu của Wang et al. (2021). Quan trọng, chúng tôi thấy rằng cả hai phương pháp chuyển giao MHA thường vượt trội so với chuyển giao HS, điều này chỉ ra lợi ích của việc chuyển giao tri thức Q/K/V so với tri thức trạng thái ẩn. Điều này phù hợp với nghiên cứu so sánh mới nhất của Wang et al. (2023), mặc dù họ chỉ đánh giá trên kiến trúc 6L-DistilBERT trong thiết lập đơn ngữ.

Chúng tôi cũng lưu ý rằng MiniLMv2 và DirectMiniLM hoạt động tương đương, với ngoại lệ đáng chú ý trên XNLI. Chúng tôi quy điều này cho hai yếu tố:

1. MiniLMv2 chuyển giao các biểu diễn quan hệ có điều kiện trên toàn bộ đầu vào, trong khi DirectMiniLM chuyển giao các biểu diễn tuyệt đối theo vị trí. Cái trước có thể mang nhiều thông tin ngữ nghĩa hơn, vì các biểu diễn ngữ cảnh thường thể hiện cấu trúc quan hệ phong phú (Park et al., 2021; Liu et al., 2022a).

2. DirectMiniLM yêu cầu học trọng số biến đổi tuyến tính W_{α,a}, trong khi MiniLMv2 không tạo ra bất kỳ tham số bổ sung nào.

Từ những quan sát này, chúng tôi thường kỳ vọng MiniLMv2 sẽ là phương pháp chưng cất tốt nhất và đã áp dụng nó trong các ứng dụng quan trọng về độ trễ của chúng tôi.⁸ Tuy nhiên, DirectMiniLM hoạt động tương đương và cung cấp những hiểu biết có ý nghĩa về lợi ích của từng kỹ thuật tối ưu hóa, có thể hữu ích cho việc gỡ lỗi và phân tích MiniLMv2. Do đó, chúng tôi khuyến nghị so sánh nó cho cả nhà nghiên cứu và thực hành trong các nghiên cứu tương lai.

⁷Các mô hình được chưng cất thường hoạt động kém trên CoLA: Chúng tôi giả định điều này là do CoLA là tác vụ cú pháp duy nhất trong benchmark trái ngược với các tác vụ ngữ nghĩa khác (Xu et al., 2022). Chúng tôi bao gồm kết quả của CoLA trong Phụ lục D.

⁸Cụ thể, các học sinh đơn ngữ và đa ngữ 4L với tăng tốc 7x trên CPU đã được triển khai cho các ứng dụng NLP khác nhau, chẳng hạn như trích xuất thực thể, phân loại tài liệu và phát hiện quan hệ, trong khi duy trì 93% hiệu suất của giáo viên trung bình (Trivedi et al., 2023).

---

## 6 Kết luận

Nghiên cứu này phân tích quan trọng các phương pháp đại diện cho chưng cất bất khả tri tác vụ của các mô hình ngôn ngữ. Cụ thể, chúng tôi so sánh chuyển giao Phân phối Đầu ra (OD), Trạng thái Ẩn (HS) và Chú ý Đa đầu (MHA) cho các kiến trúc học sinh khác nhau, thiết lập ngôn ngữ và chiến lược ánh xạ lớp. Thông qua các thí nghiệm mở rộng, chúng tôi cho thấy chuyển giao MHA dựa trên MiniLMv2 là lựa chọn tốt nhất trong nhiều thiết lập, theo sau bởi chuyển giao HS với các chiến lược ánh xạ 1-đến-N tinh vi. Trong khi đó, chúng tôi không thấy chuyển giao OD là một lựa chọn thay thế hiệu quả. Cuối cùng, chúng tôi đề xuất DirectMiniLM để làm rõ lợi thế chính xác của kỹ thuật tối ưu hóa gián tiếp (tức là dựa trên ma trận quan hệ) được đề xuất trong MiniLMv2. Nhìn chung, chúng tôi hy vọng nghiên cứu này sẽ là hướng dẫn hữu ích cho cả nhà nghiên cứu và thực hành làm việc trong lĩnh vực này.

---

[Bảng 1 - Bảng 7 và các tài liệu tham khảo được dịch tương tự với cấu trúc gốc...]
