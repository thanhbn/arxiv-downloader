# 2310.02421.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.02421.pdf
# Kích thước tệp: 148196 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2310.02421v1  [cs.LG]  3 Oct 2023Liệu một mô hình Ngôn ngữ Lớn học sinh có thể hoạt động tốt như
giáo viên của nó không?
Sia Gholami
Viện Kỹ sư Điện và Điện tử, Thành viên IEEE
gholami@ieee.org
Marwan Omar
Viện Công nghệ Illinois
momar3@iit.edu
Tóm tắt
Sự phức tạp ngày càng gia tăng của các mô hình học sâu đương đại, trong khi đạt được
độ chính xác vô song, đã vô tình đưa ra những thách thức triển khai
trong các môi trường hạn chế tài nguyên. Chưng cất kiến thức, một kỹ thuật nhằm
chuyển giao kiến thức từ một mô hình "giáo viên" có năng lực cao đến một
mô hình "học sinh" tinh gọn, nổi lên như một giải pháp đầy hứa hẹn cho tình thế khó khăn này. Bài báo này
cung cấp một cái nhìn tổng quan toàn diện về mô hình chưng cất kiến thức, nhấn mạnh
các nguyên tắc nền tảng của nó như tiện ích của nhãn mềm và ý nghĩa
của việc điều chỉnh nhiệt độ. Thông qua việc kiểm tra tỉ mỉ, chúng tôi làm sáng tỏ
các yếu tố quyết định quan trọng của việc chưng cất thành công, bao gồm kiến trúc của
mô hình học sinh, chất lượng của giáo viên, và sự cân bằng tinh tế của các siêu tham số.
Trong khi thừa nhận những lợi thế sâu sắc của nó, chúng tôi cũng đi sâu vào
các phức tạp và thách thức vốn có trong quá trình này. Việc khám phá của chúng tôi nhấn mạnh
tiềm năng của chưng cất kiến thức như một kỹ thuật then chốt trong việc tối ưu hóa sự đánh đổi
giữa hiệu suất mô hình và hiệu quả triển khai.
1 Giới thiệu
Trong những năm gần đây, bối cảnh học sâu đã được đặc trưng bởi các mô hình ngày càng
lớn và phức tạp. Trong khi các mô hình như vậy, thường có hàng tỷ tham số, liên tục thiết lập các
tiêu chuẩn mới về độ chính xác, cường độ tính toán của chúng đưa ra những thách thức triển khai, đặc biệt trong
các môi trường có tài nguyên tính toán hạn chế, chẳng hạn như các thiết bị biên [Tao et al., 2020]. Chưng cất kiến thức cung cấp một giải pháp khả thi cho tình thế nan giải này, tạo điều kiện cho việc chuyển giao kiến thức từ
một mô hình "giáo viên" phức tạp, có năng lực cao đến một mô hình "học sinh" nhỏ gọn hơn, nhằm mục đích giữ lại
càng nhiều hiệu suất càng tốt [Hinton et al., 2015].
Trung tâm của chưng cất kiến thức là nguyên tắc rằng việc học có thể được tăng cường khi các mô hình được
huấn luyện không chỉ trên các nhãn cứng mà còn trên các đầu ra xác suất phong phú hơn của mô hình giáo viên. Những
nhãn mềm này có thể được coi như nắm bắt phân phối độ tin cậy của giáo viên trên các lớp, cung cấp những hiểu biết tinh tế mà các nhãn cứng có thể bỏ qua [Bucilua et al., 2006].
Một thành phần quan trọng của phương pháp này là điều chỉnh nhiệt độ, điều này điều chỉnh độ chi tiết của
những nhãn mềm này. Tham số nhiệt độ, được giới thiệu bởi Hinton et al. [2015], đóng một vai trò then chốt
trong việc kiểm soát "độ sắc nét" của các phân phối đầu ra của giáo viên, do đó ảnh hưởng đến chất lượng của
thông tin được chuyển đến mô hình học sinh.
Bản thảo. Đang được xem xét.

--- TRANG 2 ---
Việc huấn luyện mô hình học sinh sau đó thường được hướng dẫn bởi một hàm mất mát có trọng số cân bằng
giữa mất mát entropy chéo thông thường và sự phân kỳ từ các đầu ra của giáo viên, thường được
đo bằng cách sử dụng phân kỳ Kullback-Leibler [Lopez-Paz et al., 2015].
Tuy nhiên, quá trình này không phải là không có phức tạp. Kiến trúc tối ưu của mô hình học sinh,
chất lượng của giáo viên, và sự cân bằng chính xác của các siêu tham số đều là những yếu tố quyết định trong
sự thành công của việc chưng cất [Polino et al., 2018]. Những phức tạp của các yếu tố này và sự tương tác của chúng
vẫn là một điểm tập trung của nghiên cứu đương đại.
Kết luận, chưng cất kiến thức nổi lên như một kỹ thuật chính trong bộ công cụ học sâu, bắc cầu
khoảng cách giữa hiệu suất tiên tiến và triển khai thực tế, hiệu quả. Việc tiếp tục khám phá của nó hứa hẹn sẽ tinh chỉnh và mở rộng thêm khả năng ứng dụng của nó trên các lĩnh vực đa dạng.
Để sử dụng chưng cất kiến thức cho việc tạo ra các transformer hiệu quả, quá trình thường bao gồm các
bước sau:
1. Huấn luyện một mô hình transformer lớn, phức tạp làm mô hình giáo viên trên nhiệm vụ quan tâm.
2. Tạo ra một tập dữ liệu các ví dụ cho nhiệm vụ, và sử dụng mô hình giáo viên để tạo ra các dự đoán
cho mỗi ví dụ.
3. Huấn luyện một mô hình transformer nhỏ hơn, đơn giản hơn làm mô hình học sinh trên cùng nhiệm vụ, sử dụng
các dự đoán của mô hình giáo viên làm mục tiêu.
4. Sử dụng một sự kết hợp của mất mát nhiệm vụ gốc và mất mát chưng cất để huấn luyện mô hình học sinh.
Mất mát chưng cất khuyến khích mô hình học sinh bắt chước các dự đoán của mô hình giáo viên,
thay vì chỉ cố gắng tối ưu hóa mất mát nhiệm vụ gốc.
Bằng cách sử dụng chưng cất kiến thức theo cách này, có thể tạo ra các mô hình transformer hiệu quả
nhỏ hơn và nhanh hơn mô hình gốc, trong khi vẫn đạt được hiệu suất có thể so sánh hoặc thậm chí tốt hơn trên nhiệm vụ quan tâm.
Có nhiều lợi ích khi sử dụng chưng cất kiến thức trong việc xây dựng các transformer hiệu quả:
1. Cải thiện hiệu quả: Chưng cất kiến thức cho phép bạn tạo ra các mô hình Transformer nhỏ hơn, hiệu quả hơn đòi hỏi ít tài nguyên tính toán hơn cho huấn luyện và suy luận.
Điều này cho phép xử lý nhanh hơn và giảm việc sử dụng bộ nhớ, làm cho việc triển khai các
mô hình trên các thiết bị hạn chế tài nguyên như điện thoại di động hoặc thiết bị biên trở nên dễ dàng hơn.
2. Giảm tiêu thụ năng lượng: Các mô hình nhỏ hơn được tạo ra thông qua chưng cất kiến thức
tiêu thụ ít năng lượng hơn trong quá trình suy luận, điều này rất quan trọng cho các thiết bị chạy bằng pin và
các giải pháp AI bền vững.
3. Suy luận nhanh hơn: Kích thước và độ phức tạp giảm của các mô hình chưng cất dẫn đến thời gian suy luận nhanh hơn, điều này rất cần thiết trong các tình huống đòi hỏi xử lý thời gian thực hoặc độ trễ thấp, chẳng hạn như hiểu ngôn ngữ tự nhiên trong trợ lý giọng nói hoặc dịch văn bản thời gian thực.
4. Tăng cường khái quát hóa: Chưng cất kiến thức chuyển giao kiến thức từ một mô hình lớn, hiệu suất cao đến một mô hình nhỏ hơn bằng cách huấn luyện trên các mục tiêu mềm. Những mục tiêu mềm này chứa thông tin phong phú hơn về các mối quan hệ giữa các lớp khác nhau, có thể giúp mô hình học sinh học các biểu diễn tốt hơn và khái quát hóa tốt hơn cho dữ liệu chưa thấy [Komodakis and Zagoruyko, 2017].
5. Giữ lại hiệu suất: Mặc dù giảm kích thước và độ phức tạp, các mô hình Transformer chưng cất có thể giữ lại phần lớn hiệu suất của các mô hình giáo viên lớn hơn. Điều này có nghĩa là bạn vẫn có thể đạt được kết quả mạnh mẽ trên các nhiệm vụ NLP trong khi hưởng lợi từ các cải thiện hiệu quả.
6. Triển khai tiết kiệm chi phí: Các yêu cầu tính toán giảm của các mô hình chưng cất có thể dẫn đến chi phí thấp hơn khi triển khai các giải pháp AI, đặc biệt trong các dịch vụ dựa trên đám mây
nơi chi phí tính toán được xác định bởi các tài nguyên tiêu thụ.
7. Phân phối và cập nhật dễ dàng hơn: Các mô hình nhỏ hơn dễ phân phối và cập nhật hơn, giảm
thời gian và băng thông cần thiết cho người dùng tải xuống và cài đặt cập nhật, điều này đặc biệt có lợi cho các ứng dụng trên thiết bị di động hoặc trong các khu vực có kết nối internet hạn chế.
2

--- TRANG 3 ---
Nhìn chung, chưng cất kiến thức cung cấp một kỹ thuật mạnh mẽ để xây dựng các transformer hiệu quả có thể đạt được độ chính xác cao, khái quát hóa tốt, và được triển khai trên các thiết bị hạn chế tài nguyên.
2 Các Công trình Liên quan
Xử lý Ngôn ngữ Tự nhiên (NLP) đã là một lĩnh vực nghiên cứu chính trong Trí tuệ Nhân tạo và
Học Máy kể từ những ngày đầu của khoa học máy tính [Voorhees et al., 1999, Moldovan et al.,
2000, Brill et al., 2002, Ferrucci et al., 2010, Gholami and Noori, 2021, 2022, Gholami et al., 2022,
Gholami and Khashe, 2022a,b, Brand et al., 2022, Gholami and Omar, 2023]. Có nhiều ví dụ về việc sử dụng chưng cất kiến thức để tạo ra các mô hình Transformer hiệu quả trong tài liệu. Dưới đây là một vài trường hợp đáng chú ý:
DistilBERT [Sanh et al., 2019] là một ví dụ phổ biến về việc áp dụng chưng cất kiến thức để tạo ra một
phiên bản nhỏ hơn của BERT, một mô hình Transformer được huấn luyện trước quy mô lớn cho các nhiệm vụ NLP. DistilBERT có ít hơn 40% tham số so với mô hình BERT gốc nhưng giữ lại 95% hiệu suất của nó trên
các nhiệm vụ đánh giá khác nhau.
Sun et al. [Sun et al., 2020] đã giới thiệu MobileBERT, một mô hình Transformer hiệu quả được tạo ra bằng cách sử dụng
một sự kết hợp của chưng cất kiến thức và tìm kiếm kiến trúc. MobileBERT được thiết kế cho các
nhiệm vụ NLP trên thiết bị và đạt được tốc độ suy luận nhanh hơn 4.3 lần và kích thước mô hình nhỏ hơn 2.4 lần so với
BERT-base, trong khi duy trì mức hiệu suất tương tự.
Jiao et al. [Jiao et al., 2019] đã trình bày TinyBERT, một ví dụ khác về việc áp dụng chưng cất kiến thức để tạo ra một phiên bản nhỏ hơn và nhanh hơn của BERT. TinyBERT bao gồm một quá trình chưng cất kiến thức hai bước: chưng cất tổng quát trên một tập dữ liệu quy mô lớn và chưng cất cụ thể cho nhiệm vụ trên một tập dữ liệu nhiệm vụ mục tiêu. Phương pháp này dẫn đến một mô hình nhỏ hơn 7.5 lần và nhanh hơn 9.4 lần so với BERT-base
trong khi duy trì hiệu suất cạnh tranh.
Touvron et al. [Touvron et al., 2021] đã đề xuất Data-efficient Image Transformers (DeiT), là
một mô hình Transformer cho phân loại hình ảnh. Mặc dù DeiT tập trung vào lĩnh vực thị giác, các
tác giả đã sử dụng chưng cất kiến thức từ một mô hình giáo viên mạng nơ-ron tích chập (CNN) để
cải thiện hiệu suất của mô hình học sinh Transformer. Điều này chứng minh tiềm năng của
chưng cất kiến thức đa phương thức trong việc tạo ra các mô hình Transformer hiệu quả.
Wang et al. [Wang et al., 2020] đã đề xuất MiniLM, một phiên bản chưng cất của mô hình BERT được huấn luyện trước.
MiniLM sử dụng một sự kết hợp của chưng cất kiến thức chú ý tự-bản thân và chưng cất kiến thức lớp trung gian, nhằm mục đích bảo tồn kiến thức ngôn ngữ và thông tin cấu trúc của mô hình gốc. MiniLM đạt được một sự giảm đáng kể về kích thước mô hình và suy luận nhanh hơn trong khi duy trì hiệu suất cạnh tranh trên các tiêu chuẩn NLP khác nhau.
Fan et al. [fan] đã đề xuất LayerDrop, một kỹ thuật điều chuẩn để huấn luyện và chưng cất hiệu quả các mô hình Transformer sâu. LayerDrop huấn luyện một mô hình đơn có thể được cắt tỉa hiệu quả tại thời điểm suy luận bằng cách bỏ các lớp, dẫn đến một họ các mô hình nhỏ hơn với các sự đánh đổi khác nhau
giữa hiệu suất và hiệu quả. Phương pháp này có thể được kết hợp với chưng cất kiến thức để
tạo ra các mô hình Transformer thậm chí hiệu quả hơn.
Gao et al. [Chakraborty et al., 2021] đã giới thiệu RocketQAv2, một phiên bản chưng cất của T5 (Text-to-Text
Transfer Transformer) cho các nhiệm vụ trả lời câu hỏi miền mở. RocketQAv2 dựa trên mô hình T5 nhưng sử dụng các kỹ thuật chưng cất kiến thức để tạo ra một mô hình nhỏ hơn hiệu quả hơn trong việc phục vụ các ứng dụng thế giới thực.
Những ví dụ này làm nổi bật hiệu quả của chưng cất kiến thức trong việc tạo ra các mô hình Transformer nhỏ hơn, nhanh hơn và
hiệu quả hơn trong khi duy trì hiệu suất cạnh tranh trên các nhiệm vụ NLP và thị giác khác nhau.
3 Phương pháp
Chưng cất kiến thức là một kỹ thuật được sử dụng trong học máy để chuyển giao kiến thức từ một
mô hình lớn hơn, phức tạp hơn (được gọi là mô hình giáo viên) đến một mô hình nhỏ hơn, đơn giản hơn (được gọi là mô hình học sinh). Mục tiêu là tạo ra một mô hình học sinh nhẹ, hiệu quả về mặt tính toán mà giữ lại
càng nhiều hiệu suất của mô hình giáo viên càng tốt. Điều này đặc biệt hữu ích cho việc triển khai
3

--- TRANG 4 ---
các mô hình học máy trên các thiết bị hạn chế tài nguyên hoặc trong các tình huống mà thời gian suy luận và
tiêu thụ năng lượng là quan trọng.
Phương pháp chưng cất kiến thức đòi hỏi việc huấn luyện một mô hình nhỏ hơn, tập trung hơn để sao chép
kết quả của một mô hình ngôn ngữ lớn hơn, rộng hơn, như GPT-3 [Brown et al., 2020]. Kiến thức tượng trưng cấp cao của mô hình lớn hơn nên được giảm xuống thành một mô hình nhỏ hơn, hiệu quả hơn có thể thực hiện chính xác các nhiệm vụ cụ thể. Phương pháp này bao gồm việc huấn luyện một mô hình Transformer học sinh sử dụng chưng cất kiến thức từ một mô hình giáo viên lớn hơn. Mô hình giáo viên cung cấp các nhãn mềm
cho dữ liệu huấn luyện, được sử dụng để huấn luyện mô hình học sinh. Điều này cho phép mô hình học sinh
học từ biểu diễn phong phú của mô hình giáo viên trong khi hiệu quả hơn do kích thước nhỏ hơn của nó [Freitag et al., 2017]. Trong các thí nghiệm của chúng tôi, chúng tôi đã sử dụng mô hình được giới thiệu bởi Gholami and Omar
[2023] (GPT-Efficio) làm giáo viên.
Ở đây chúng tôi nghiên cứu một phương pháp cụ thể được gọi là quá trình chưng cất ẩn dụ (FD) bao gồm ba
thành phần chính: một mô hình giáo viên, một mô hình học sinh, và một tập hợp các quy tắc. Mô hình giáo viên là
mô hình ngôn ngữ lớn hơn, tổng quát hơn, trong khi mô hình học sinh là mô hình nhỏ hơn, chuyên biệt hơn
đang được huấn luyện. Các quy tắc định nghĩa các ràng buộc và mối quan hệ giữa các khái niệm khác nhau trong
miền được mô hình hóa. Có ba bước trong quy trình FD. Mô hình học sinh được huấn luyện đầu tiên bằng cách sử dụng
một tập hợp các ví dụ huấn luyện được tạo ra bởi mô hình hướng dẫn. Để tăng cường hiệu suất của mô hình học sinh, một tập dữ liệu cụ thể cho nhiệm vụ được sử dụng để tinh chỉnh nó. Sau đó, đầu ra của mô hình học sinh được tinh chỉnh thêm bằng cách sử dụng các quy tắc tượng trưng để đảm bảo nó tuân thủ các hạn chế và mối quan hệ
được chỉ định bởi các quy tắc.
Cả học có giám sát và không giám sát đều có thể được bao gồm trong quá trình FD. Mô hình giáo viên
tạo ra một tập hợp các ví dụ được gắn nhãn trong môi trường có giám sát, sau đó được sử dụng để
huấn luyện mô hình học sinh. Trong một kịch bản không giám sát, mô hình học sinh được huấn luyện bằng các phương pháp
như học tương phản sử dụng một tập hợp các ví dụ không nhãn mà mô hình giáo viên đã tạo ra.
Mục tiêu của chưng cất ẩn dụ là học một mô hình nhỏ hơn, chuyên biệt hơn, fS, từ một tập dữ liệu
các cặp đầu vào/đầu ra, D= (x1,y1),...,(xn,yn), trong đó xi là một chuỗi đầu vào, và yi là một
chuỗi đầu ra tượng trưng. Một mô hình ngôn ngữ tổng quát được huấn luyện trước, fG, có thể thực hiện cùng nhiệm vụ như fG nhưng
với ít tham số hơn. Chúng tôi đề xuất một kỹ thuật chưng cất ẩn dụ để đạt được mục tiêu này,
bao gồm việc huấn luyện mô hình nhỏ hơn, fS, sử dụng một hỗn hợp của học có giám sát và không giám sát. Đối với
chưng cất ẩn dụ, hàm mất mát như sau:
Hàm mất mát cho chưng cất kiến thức, bao gồm một sự kết hợp của mất mát entropy chéo
với các nhãn thật và mất mát KL-divergence giữa các đầu ra của giáo viên và học sinh, thường được
viết như:
L=α×CE(y,Student (x))+(1−α)×T2×KL(Student(x
T)||Teacher(x
T)) (1)
Trong đó L là tổng hàm mất mát, α là một yếu tố trọng số xác định sự cân bằng giữa
mất mát gốc và mất mát chưng cất, CE là hàm mất mát entropy chéo, y là các nhãn thật,
Student(x) là các dự đoán của mô hình học sinh, T là tham số nhiệt độ được sử dụng để làm mềm xác suất,
KL là phân kỳ Kullback-Leibler, và Teacher(x) là các dự đoán của mô hình giáo viên.
KL-divergence và entropy chéo đều được đo trên tất cả các lớp, và chúng ta đang tổng
các phép đo này để có được một mất mát vô hướng.
Phân kỳ Kullback-Leibler (KL) là một thước đo về cách một phân phối xác suất phân kỳ
từ một phân phối xác suất thứ hai, được mong đợi. Đối với các phân phối xác suất rời rạc P và Q, phân kỳ KL được định nghĩa như:
KL(P||Q) =/summationdisplay
P(i)×log(P(i)
Q(i)) (2)
trong đó tổng là trên tất cả các sự kiện có thể i, P(i) là xác suất của sự kiện i dưới phân phối P,
và Q(i) là xác suất của sự kiện i dưới phân phối Q.
Đối với các phân phối liên tục, tổng được thay thế bằng một tích phân trên tất cả các kết quả có thể.
4

--- TRANG 5 ---
4 Thí nghiệm
Trong phần này chúng tôi trình bày kết quả của từng phương pháp của chúng tôi trong bối cảnh mô hình ngôn ngữ
(tức là các nhiệm vụ hoàn thành) và trả lời câu hỏi.
4.1 Kết quả
Phần này nghiên cứu các kỹ thuật chưng cất kiến thức để nén các mô hình lớn thành các mô hình nhỏ hơn
trong khi giữ lại một phần tốt của hiệu suất của mô hình gốc. Một số tác động của việc
sử dụng chưng cất kiến thức trên một mô hình transformer là:
1. Cải thiện Hiệu quả: Một mô hình chưng cất nhỏ hơn có ít tham số hơn và do đó đòi hỏi
ít tài nguyên tính toán hơn cho suy luận. Điều này làm cho việc triển khai mô hình trên
các thiết bị có tài nguyên hạn chế, như thiết bị di động, trở nên khả thi.
2. Tốc độ: Mô hình nhỏ hơn cũng nên nhanh hơn, cả về thời gian huấn luyện và suy luận,
so với mô hình lớn hơn ban đầu.
3. Hiệu suất: Nói chung, mô hình chưng cất sẽ hoạt động tệ hơn mô hình lớn hơn ban đầu,
nhưng tốt hơn một mô hình có kích thước tương tự được huấn luyện từ đầu [Du et al., 2017]. Mục tiêu là giữ lại càng nhiều hiệu suất càng tốt với các ràng buộc về kích thước mô hình.
4. Tính bền vững: Trong một số trường hợp, chưng cất kiến thức cũng có thể tăng tính bền vững của
mô hình và khả năng khái quát hóa của nó, vì nó học để bắt chước các dự đoán của giáo viên trên một
loạt rộng các mẫu và không chỉ sự thật nền tảng.
Tuy nhiên, điều quan trọng cần lưu ý là những lợi ích này phụ thuộc vào chi tiết cụ thể của nhiệm vụ, kiến trúc
của các mô hình giáo viên và học sinh, và quy trình huấn luyện được sử dụng. Chưng cất được cấu hình kém
có thể dẫn đến một mô hình hoạt động không tốt hơn hoặc thậm chí tệ hơn một mô hình có cùng kích thước được huấn luyện
từ đầu.
Bảng 1: Hiệu suất của phương pháp chưng cất kiến thức trên các nhiệm vụ hoàn thành
Mô hình nparams LAMBADA
(acc)LAMBADA
(ppl)StoryCloze
(acc)HellaSwag
(acc)
GPT-3 Zero-Shot 175B 76.2 3.00 83.2 78.9
GPT-3 One-Shot 175B 72.5 3.35 84.7 78.1
GPT-3 Few-Shot 175B 86.4 1.92 87.7 79.3
GPT-Efficio (teacher) 950M 67.1 9.2 80.5 72.6
GPT-Efficio (student) 320M 52.47 13.53 61.28 63.52
Bảng 1 chứng minh hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio so với
GPT-3.
Bảng 2: Hiệu suất của phương pháp chưng cất kiến thức trên các nhiệm vụ QA
Mô hình nparams NQ WebQ TriviaQA
GPT-3 Zero-Shot 175B 14.6 14.4 64.3
GPT-3 One-Shot 175B 23.0 25.3 68.0
GPT-3 Few-Shot 175B 29.9 41.5 71.2
GPT-Efficio (teacher) 950M 27.5 40.6 69.2
GPT-Efficio (student) 320M 19.61 30.52 53.61
Bảng 2 hiển thị hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio so với GPT-3.
5 Phân tích
Trong bối cảnh chưng cất kiến thức, có một số siêu tham số chính có thể tác động đến
hiệu suất của mô hình học sinh:
5

--- TRANG 6 ---
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot
GPT-Efficio (teacher) GPT-Efficio (student)00.20.40.60.81
0.760.730.86
0.67
0.520.83 0.850.88
0.81
0.610.79 0.78 0.79
0.73
0.64Độ chính xácLAMBADA acc StoryCloze acc HellaSwag acc
Hình 1: Hiệu suất của phương pháp chưng cất kiến thức trên các nhiệm vụ hoàn thành
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot
GPT-Efficio (teacher) GPT-Efficio (student)020406080100
14.62329.927.5
19.61
14.425.341.5 40.6
30.5264.36871.269.2
53.61Độ chính xácNQ WebQ TriviaQA
Hình 2: Hiệu suất của phương pháp chưng cất kiến thức trên các nhiệm vụ QA.
6

--- TRANG 7 ---
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot
GPT-Efficio (teacher) GPT-Efficio (student)204060Độ chính xácHiệu suất của phương pháp chưng cất kiến thức trên các nhiệm vụ QA
NQ
WebQ
TriviaQA
Hình 3: Hiệu suất của phương pháp chưng cất kiến thức trên các nhiệm vụ QA.
1. Nhiệt độ ( T): Nhiệt độ là một tham số trong hàm softmax được sử dụng để
"làm mềm" các đầu ra của các mô hình giáo viên và học sinh trong quá trình chưng cất.
Một nhiệt độ cao hơn dẫn đến một phân phối xác suất mềm hơn trên các lớp, làm cho
quá trình chưng cất hiệu quả hơn bằng cách làm nổi bật các mối quan hệ giữa các
lớp khác nhau. [Hinton et al., 2015]. Tuy nhiên, đặt nhiệt độ quá cao có thể dẫn đến
việc làm mềm quá mức và dẫn đến mất thông tin có giá trị.
2. Hệ số Chưng cất ( α): Đây là trọng số được cho cho mất mát gốc (thường
là mất mát entropy chéo với các nhãn thật) trong hàm mất mát kết hợp. Sự cân bằng
giữa mất mát gốc này và mất mát chưng cất (KL-divergence giữa các đầu ra của giáo viên
và học sinh) là rất quan trọng. Nếu α được đặt quá cao, học sinh có thể tập trung quá nhiều
vào việc khớp các nhãn thật và không đủ vào việc học từ các dự đoán của giáo viên. [Zagoruyko and Komodakis, 2016].
3. Kiến trúc Mô hình: Mặc dù không phải là một siêu tham số của quá trình chưng cất
chính nó, kiến trúc của mô hình học sinh có thể tác động đáng kể đến hiệu suất của nó. Nếu
mô hình học sinh quá nhỏ, nó có thể không có đủ khả năng để học từ giáo viên một cách hiệu quả. Ngược lại, nếu nó quá lớn, những lợi ích của chưng cất (như tăng hiệu quả
và tốc độ) có thể không được thực hiện.
4. Tốc độ Học và Các Tham số Huấn luyện Khác: Như với bất kỳ mô hình học máy nào, tốc độ học và các tham số huấn luyện khác có thể tác động đáng kể đến hiệu suất của
mô hình học sinh.
5. Số Epoch Chưng cất: Số epoch mà học sinh được huấn luyện để khớp với
các đầu ra của giáo viên cũng có thể ảnh hưởng đến hiệu suất. Quá ít epoch có thể dẫn đến underfitting,
trong khi quá nhiều có thể dẫn đến overfitting.
Như mọi khi, những siêu tham số này có thể cần được điều chỉnh tùy thuộc vào chi tiết cụ thể của nhiệm vụ,
tập dữ liệu, và kiến trúc của các mô hình giáo viên và học sinh.
Trong phần này chúng tôi nghiên cứu tác động của Nhiệt độ ( T) lên mô hình học sinh. Tham số nhiệt độ T trong chưng cất kiến thức đóng một vai trò quan trọng trong việc kiểm soát "độ sắc nét" của
phân phối xác suất đầu ra bởi mô hình giáo viên.
7

--- TRANG 8 ---
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-ShotGPT-Efficio T- GPT-Efficio T1 GPT-Efficio T2020406080100
76.2
72.5
86.4
67.1
52.47
49.483.2
84.7
87.7
80.5
61.28
59.2978.9
78.1
79.3
72.6
63.52
60.59Độ chính xácLAMBADA (acc) StoryCloze (acc) HellaSwag (acc)
Hình 4: Phân tích tác động của siêu tham số T trên các nhiệm vụ hoàn thành
Trong bối cảnh mô hình ngôn ngữ, nếu T thấp (gần 1 hoặc ít hơn), các xác suất đầu ra của mô hình giáo viên sẽ "sắc nét" hoặc "nhọn" hơn, có nghĩa là mô hình sẽ gán xác suất cao cho một vài từ được chọn và xác suất rất thấp cho phần còn lại. Điều này có thể làm cho việc học các hành vi tinh tế hơn từ mô hình giáo viên trở nên khó khăn cho mô hình học sinh vì gradient của hàm mất mát trở nên thưa thớt và việc học chậm lại.
Mặt khác, nếu T cao, các xác suất đầu ra của mô hình giáo viên trở nên đồng nhất hơn hoặc "mềm" hơn. Điều này có nghĩa là mô hình gán xác suất phân phối đều hơn trên một tập hợp từ lớn hơn. Điều này có thể có lợi trong các trường hợp có nhiều câu trả lời đúng, vì nó khuyến khích mô hình học sinh xem xét một loạt khả năng rộng hơn thay vì chỉ tập trung vào một câu trả lời đúng duy nhất. Về cơ bản, nó cung cấp một tập thông tin phong phú hơn cho mô hình học sinh trong quá trình huấn luyện.
Tuy nhiên, đặt T quá cao cũng có thể có nhược điểm. Nếu các xác suất đầu ra của giáo viên trở nên quá đồng nhất, mô hình học sinh có thể gặp khó khăn trong việc xác định các từ có khả năng cao hơn và ít khả năng hơn. Điều này có thể dẫn đến underfitting, nơi mô hình học sinh trở nên kém chính xác hơn vì nó ít tự tin hơn trong các dự đoán của mình.
Vì vậy, nhiệt độ T nên được đặt theo cách cân bằng nhu cầu cho mô hình học sinh học các hành vi tinh tế từ mô hình giáo viên đồng thời đảm bảo mô hình học sinh có thể phân biệt giữa các dự đoán có khả năng cao hơn và ít hơn. Việc tinh chỉnh tham số này có thể đòi hỏi một số thí nghiệm hoặc xác thực trên một tập dev riêng biệt.
Bảng 3: Phân tích tác động của siêu tham số T trên các nhiệm vụ hoàn thành
Mô hình T n params LAMBADA
(acc)LAMBADA
(ppl)StoryCloze
(acc)HellaSwag
(acc)
GPT-3 Zero-Shot - 175B 76.2 3.00 83.2 78.9
GPT-3 One-Shot - 175B 72.5 3.35 84.7 78.1
GPT-3 Few-Shot - 175B 86.4 1.92 87.7 79.3
GPT-Efficio (teacher) - 950M 67.1 9.2 80.5 72.6
GPT-Efficio (student) 1 320M 52.47 13.53 61.28 63.52
GPT-Efficio (student) 2 320M 49.40 14.69 59.29 60.59
Bảng 3 chứng minh hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio với các giá trị T khác nhau
so với GPT-3.
8

--- TRANG 9 ---
Bảng 4: Phân tích tác động của siêu tham số T trên các nhiệm vụ QA
Mô hình T n params NQ WebQ TriviaQA
GPT-3 Zero-Shot - 175B 14.6 14.4 64.3
GPT-3 One-Shot - 175B 23.0 25.3 68.0
GPT-3 Few-Shot - 175B 29.9 41.5 71.2
GPT-Efficio (teacher) - 950M 27.5 40.6 69.2
GPT-Efficio (student) 1 320M 19.61 30.52 53.61
GPT-Efficio (student) 2 320M 17.19 27.87 48.50
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-ShotGPT-Efficio T- GPT-Efficio T1 GPT-Efficio T2020406080100
14.6
23
29.9
27.5
19.61
17.1914.4
25.3
41.5
40.6
30.52
27.8764.3
68
71.2
69.2
53.61
48.5Độ chính xácNQ WebQ TriviaQA
Hình 5: Phân tích tác động của siêu tham số T trên các nhiệm vụ QA
Bảng 4 hiển thị hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio với các
giá trị T khác nhau so với GPT-3.
6 Hạn chế
Mặc dù chưng cất kiến thức là một công cụ mạnh mẽ, nó không phải là không có những hạn chế và thách thức. Dưới đây là một vài điều cần xem xét:
1. Khoảng cách Hiệu suất: Hiệu suất của mô hình học sinh chưng cất, mặc dù tốt hơn một
mô hình có kích thước tương tự được huấn luyện từ đầu, thường không đạt được mức hiệu suất
của mô hình giáo viên lớn hơn. Thường có một sự đánh đổi giữa kích thước mô hình và độ chính xác.
2. Phụ thuộc vào Mô hình Giáo viên Tốt: Hiệu quả của chưng cất kiến thức phụ thuộc nhiều
vào chất lượng của mô hình giáo viên. Mô hình học sinh chỉ có thể tốt như mô hình giáo viên cho phép. Nếu mô hình giáo viên không được huấn luyện tốt hoặc không hoạt động tốt, mô hình học sinh cũng không có khả năng hoạt động tốt.
3. Độ Nhạy Siêu tham số: Quá trình chưng cất kiến thức đòi hỏi việc điều chỉnh cẩn thận
một số siêu tham số, như tham số nhiệt độ và trọng số giữa
mất mát gốc và mất mát chưng cất. Việc tìm các cài đặt đúng có thể khó khăn và có thể
đòi hỏi nhiều thí nghiệm.
4. Chi phí Tính toán: Mặc dù mô hình học sinh nhỏ hơn và hiệu quả hơn,
quá trình chưng cất chính nó đòi hỏi mô hình giáo viên tạo ra các dự đoán cho dữ liệu,
có thể tốn kém về mặt tính toán, đặc biệt đối với các mô hình và tập dữ liệu lớn.
9

--- TRANG 10 ---
5. Quá trình Mờ ám: Quá trình chưng cất kiến thức phần nào mờ ám và khó
diễn giải. Không phải lúc nào cũng rõ ràng tại sao một mô hình học sinh nhất định học hiệu quả từ một mô hình giáo viên, hoặc tại sao một số siêu tham số hoạt động tốt hơn những cái khác.
6. Năng lực Mô hình Học sinh: Cũng có một giới hạn về việc một mô hình học sinh nhỏ hơn có thể
học được bao nhiều từ một mô hình giáo viên lớn. Nếu năng lực của mô hình học sinh quá nhỏ, nó có thể không
học hiệu quả kiến thức của giáo viên.
7. Nguy cơ Overfitting: Nếu mô hình giáo viên đã overfit với dữ liệu huấn luyện, có thể
mô hình học sinh có thể học những dự đoán overfit này, dẫn đến khái quát hóa kém cho dữ liệu mới.
Mặc dù có những hạn chế này, chưng cất kiến thức vẫn có thể là một kỹ thuật rất hữu ích, đặc biệt khi
đối phó với các ràng buộc về tài nguyên tính toán hoặc khi triển khai các mô hình trong các ứng dụng thế giới thực nơi hiệu quả là chìa khóa.
7 Công việc Tương lai
Có một số hướng mà công việc tương lai về chưng cất kiến thức có thể thực hiện để cải thiện thêm
kỹ thuật này và ứng dụng của nó trong các lĩnh vực khác nhau:
• Hiểu biết Cải thiện về Động lực Chưng cất: Nghiên cứu thêm cần thiết để hiểu
động lực của việc chuyển giao kiến thức trong quá trình chưng cất. Ví dụ, hiểu
những khía cạnh nào của kiến thức giáo viên được chuyển giao hiệu quả nhất và tại sao có thể
giúp tối ưu hóa quá trình.
• Điều chỉnh Siêu tham số Tự động: Với độ nhạy của quá trình chưng cất đối với các siêu tham số như nhiệt độ và trọng số giữa mất mát gốc và mất mát chưng cất, việc phát triển các phương pháp điều chỉnh siêu tham số tự động hoặc hiệu quả hơn có thể có lợi.
• Kỹ thuật Chưng cất Tiên tiến: Khám phá các kỹ thuật chưng cất tiên tiến ngoài
phương pháp tiêu chuẩn có thể dẫn đến kết quả tốt hơn. Điều này có thể bao gồm các hàm mất mát mới,
phương pháp huấn luyện, hoặc các loại mối quan hệ giáo viên-học sinh.
• Chưng cất Đa Giáo viên: Ý tưởng chưng cất kiến thức từ nhiều mô hình giáo viên
vào một mô hình học sinh duy nhất là một lĩnh vực thú vị để khám phá. Điều này có thể
kết hợp các điểm mạnh của các mô hình khác nhau vào một mô hình học sinh hiệu quả duy nhất.
• Thích ứng Cụ thể Miền: Thích ứng và tối ưu hóa các kỹ thuật chưng cất kiến thức
cho các miền hoặc nhiệm vụ cụ thể cũng có thể là một con đường có giá trị cho công việc tương lai. Các
nhiệm vụ khác nhau có thể hưởng lợi từ các chiến lược chưng cất khác nhau.
• Bảo mật và An toàn trong Chưng cất: Vì chưng cất bao gồm việc chuyển giao kiến thức từ
một mô hình giáo viên, có thể có những lo ngại về bảo mật và an toàn, đặc biệt khi
mô hình giáo viên đã được huấn luyện trên dữ liệu nhạy cảm. Công việc tương lai có thể xem xét cách đảm bảo
rằng chưng cất không rò rỉ thông tin nhạy cảm.
• Hiểu Hạn chế và Thất bại: Nghiên cứu thêm về khi nào và tại sao chưng cất kiến thức
thất bại có thể giúp phát triển các phương pháp chưng cất bền vững và đáng tin cậy hơn.
Lĩnh vực này đang phát triển nhanh chóng và tính liên quan của những hướng này có thể thay đổi khi có thêm nghiên cứu được
thực hiện và các kỹ thuật mới hơn được phát triển.
8 Kết luận
Sự gia tăng vượt bậc về độ sâu và phức tạp của các kiến trúc neural đã nhấn mạnh nhu cầu cấp thiết
cho các chiến lược triển khai hiệu quả trong các kịch bản thế giới thực. Chưng cất kiến thức đã được
chiếu sáng như một ngọn hải đăng trong quest này, cung cấp một phương pháp để khai thác sức mạnh của các mô hình tiên tiến trong những ranh giới dễ quản lý hơn, thân thiện với triển khai. Trong suốt bài báo này, chúng tôi đã phân tích
các khía cạnh phức tạp của kỹ thuật này, từ những tinh tế của việc sử dụng nhãn mềm đến vai trò then chốt
của điều chỉnh nhiệt độ, và làm nổi bật vô số yếu tố quyết định ảnh hưởng đến sự thành công của
quá trình chưng cất.
10

--- TRANG 11 ---
Tuy nhiên, như với nhiều giải pháp trong lĩnh vực học sâu, chưng cất kiến thức không phải là không có
thách thức. Sự tương tác chính xác của động lực giáo viên-học sinh, cài đặt siêu tham số tối ưu, và các phức tạp kiến trúc của mô hình học sinh nhấn mạnh bản chất đa chiều
của quá trình. Hơn nữa, sự đánh đổi giữa hiệu quả mô hình và hiệu suất, trong khi được giảm nhẹ,
vẫn là một yếu tố cần được điều hướng cẩn thận.
Nhìn về phía trước, rõ ràng là lĩnh vực chưng cất kiến thức cung cấp một tấm thảm nghiên cứu phong phú về
các con đường. Khi chúng ta tiếp tục đẩy ranh giới của hiệu suất mô hình, việc theo đuổi đồng thời
hiệu quả trở nên không thể thiếu. Chưng cất kiến thức, trong bối cảnh này, đứng như
vừa là một minh chứng cho những tiến bộ của chúng ta và vừa là một biên giới đầy hứa hẹn cho việc khám phá tương lai. Nó đóng gói
bản chất của nghiên cứu học sâu đương đại: cuộc kết hôn giữa hiệu suất và chủ nghĩa thực dụng,
nhằm mục đích đưa các giải pháp AI tiên tiến gần hơn với khả năng ứng dụng thế giới thực.
Tài liệu Tham khảo
Ryan Brand, Sia Gholami, Daniel Horowitz, Liutong Zhou, and Sourav Bhabesh. Text classification
for online conversations with machine learning on aws. AWS Machine Learning Blog, 2022.
Eric Brill, Susan Dumais, and Michele Banko. An analysis of the askmsr question-answering system.
In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing
(EMNLP 2002), pages 257–264, 2002.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 535–541, 2006.
Soham Chakraborty, Sourav Patel, and Murti V Salapaka. Recovery of power flow to critical infrastructures using mode-dependent droop-based inverters. arXiv preprint arXiv:2102.00046, 2021.
Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading
comprehension. arXiv preprint arXiv:1705.00106, 2017.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. Building watson: An overview
of the deepqa project. AI magazine, 31(3):59–79, 2010.
Markus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran. Ensemble distillation for neural machine translation. arXiv preprint arXiv:1702.01802, 2017.
Sia Gholami and Saba Khashe. Alexa, predict my flight delay. arXiv preprint arXiv:2208.09921,
2022a.
Sia Gholami and Saba Khashe. Flight delay prediction using deep learning and conversational voicebased agents. American Academic Scientific Research Journal for Engineering, Technology, and
Sciences, 89(1):60–72, 2022b.
Sia Gholami and Mehdi Noori. Zero-shot open-book question answering. arXiv preprint
arXiv:2111.11520, 2021.
Sia Gholami and Mehdi Noori. You don't need labeled data for open-book question answering.
Applied Sciences, 12(1):111, 2022.
Sia Gholami and Marwan Omar. Do generative large language models need billions of parameters?
arXiv preprint arXiv:2309.06589, 2023.
Sia Gholami, Danny Byrd, Francisco Calderon Rodriguez, Muhyun Kim, Yohei Nakayama, Mehdi
Noori, and Nathalie Rauschmayr. Create, train, and deploy a billion-parameter language model
on terabytes of data with tensorflow and amazon sagemaker. AWS Machine Learning Blog, 2022.
11

--- TRANG 12 ---
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351,
2019.
Nikos Komodakis and Sergey Zagoruyko. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017.
David Lopez-Paz, Léon Bottou, Bernhard Schölkopf, and Vladimir Vapnik. Unifying distillation
and privileged information. arXiv preprint arXiv:1511.03643, 2015.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Roxana Girju, Richard Goodrum,
and Vasile Rus. The structure and performance of an open-domain question answering system. In
Proceedings of the 38th annual meeting of the Association for Computational Linguistics, pages
563–570, 2000.
Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984,
2020.
Yudong Tao, Rui Ma, Mei-Ling Shyu, and Shu-Ching Chen. Challenges in energy-efficient deep
neural network training with fpga. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, pages 400–401, 2020.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efficient image transformers & distillation through attention. In
International conference on machine learning, pages 10347–10357. PMLR, 2021.
Ellen M Voorhees et al. The trec-8 question answering track report. In Trec, volume 99, pages
77–82. Citeseer, 1999.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776–5788, 2020.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer.arXiv preprint arXiv:1612.03928,
2016.
12

--- TRANG 13 ---
Hình này "Picture9.png" có sẵn trong định dạng "png"
từ:
http://arxiv.org/ps/2310.02421v1
