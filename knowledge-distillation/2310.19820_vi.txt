# 2310.19820.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.19820.pdf
# Kích thước tệp: 436892 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bài viết Chủ đề: tinyML
NetDistiller: Trao Quyền cho Học Sâu Nhỏ
thông qua Chưng Cất Tại Chỗ
Shunyao Zhang, Đại học Rice, Houston, TX, 77005, USA
Y onggan Fu, Viện Công nghệ Georgia, Atlanta, GA, 30332, USA
Shang Wu, Đại học Rice, Houston, TX, 77005, USA
Jyotikrishna Dass, Đại học Rice, Houston, TX, 77005, USA
Haoran Y ou, Viện Công nghệ Georgia, Atlanta, GA, 30332, USA
Yingyan (Celine) Lin, Viện Công nghệ Georgia, Atlanta, GA, 30332, USA
Tóm tắt—Tăng cường độ chính xác nhiệm vụ của các mạng nơ-ron nhỏ (TNNs) đã trở thành một thách thức cơ bản để cho phép triển khai TNNs trên các thiết bị biên bị hạn chế bởi những giới hạn nghiêm ngặt về bộ nhớ, tính toán, băng thông và nguồn cung cấp điện. Để đạt được mục tiêu này, chúng tôi đề xuất một khung làm việc được gọi là NetDistiller để tăng cường độ chính xác có thể đạt được của TNNs bằng cách xem chúng như các mạng con của một giáo viên chia sẻ trọng số được xây dựng bằng cách mở rộng số lượng kênh của TNN. Cụ thể, mô hình TNN mục tiêu được huấn luyện chung với mô hình giáo viên chia sẻ trọng số thông qua (1) phẫu thuật gradient để giải quyết xung đột gradient giữa chúng và (2) chưng cất nhận biết không chắc chắn để giảm thiểu hiện tượng overfitting của mô hình giáo viên. Các thí nghiệm mở rộng trên các nhiệm vụ đa dạng xác nhận hiệu quả của NetDistiller trong việc tăng cường độ chính xác có thể đạt được của TNNs so với các phương pháp tiên tiến nhất. Mã nguồn của chúng tôi có sẵn tại https:// github.com/ GATECH-EIC/ NetDistiller.
Hiệu suất phá kỷ lục gần đây của các mạng nơ-ron (NNs) đã thúc đẩy việc ứng dụng ngày càng tăng của chúng trên các ngành khoa học và kỹ thuật khác nhau. Song song với đó, dự kiến rằng số lượng thiết bị kết nối Internet của Vạn vật (IoT) trên toàn thế giới sẽ đạt 30,9 tỷ đơn vị vào năm 2025 [1]. Triển khai trí tuệ được hỗ trợ bởi NN trên nhiều thiết bị IoT có tầm quan trọng đáng kể, vì nó cho phép sử dụng dữ liệu thu thập tại biên cho các chức năng thông minh trên thiết bị khác nhau có thể cách mạng hóa cuộc sống con người. Nhu cầu tăng trưởng mạnh mẽ này đã làm nổi lên lĩnh vực các mạng nơ-ron nhỏ (TNNs) đã thu hút sự chú ý ngày càng tăng đáng kể. Điều này là do TNNs cho phép các thiết bị biên nhỏ và rẻ tiền làm việc trực tiếp trên dữ liệu cục bộ với chi phí điện năng và tính toán thấp hơn, dẫn đến cả việc giảm độ trễ và tăng cường quyền riêng tư vì nó làm giảm hoặc thậm chí loại bỏ sự cần thiết của kết nối internet để chia sẻ và tập trung hóa dữ liệu trên một
XXXX-XXX © 2023 IEEE
Digital Object Identifier 10.1109/XXX.0000.0000000máy chủ đám mây. Tuy nhiên, hiệu suất nhiệm vụ có thể đạt được của TNNs vẫn không thỏa đáng do khả năng mô hình hạn chế của chúng. Do đó, cải thiện hiệu suất nhiệm vụ của TNNs đã trở thành một thách thức cơ bản để cho phép việc áp dụng rộng rãi của chúng, điều này rất được mong muốn trong nhiều ứng dụng biên thực tế.
Để giải quyết thách thức nêu trên và do đó mở khóa tiềm năng của TNNs tại biên, đã có nỗ lực nghiên cứu ngày càng tăng nhằm tăng cường hiệu suất nhiệm vụ có thể đạt được của chúng. Đặc biệt, đã được chỉ ra rằng việc huấn luyện TNNs khác biệt cơ bản so với việc huấn luyện NNs lớn. Ví dụ, các tác giả của [2] xác định rằng TNNs gặp phải vấn đề under-fitting do khả năng mô hình hạn chế của chúng, trái ngược với NNs lớn, có xu hướng gặp phải over-fitting. Họ cũng quan sát thấy rằng các kỹ thuật tăng cường dữ liệu và regularization, vốn tăng cường độ chính xác ImageNet cho các NNs lớn over-parameterized như ResNet50, có tác động có hại đến độ chính xác của TNNs, chẳng hạn như MobileNetV2-Tiny [3], nhỏ hơn 174 lần so với ResNet50.
Lấy cảm hứng từ các nghiên cứu trước đó, chúng tôi đưa ra giả thuyết rằng việc tăng cường khả năng mô hình (ví dụ, kênh)
Tháng Mười Một Được xuất bản bởi IEEE Computer Society IEEE Micro 1arXiv:2310.19820v1  [cs.LG]  24 Oct 2023

--- TRANG 2 ---
tinyML
Giáo viên
CE Loss0    1    2    3    4 ...
KL Loss CE LossLựa chọn loss nhận biết
không chắc chắnLớpHọc sinhHình ảnh
Phẫu thuật
Gradient
Lớp 0    1    2    3    4 ...Suy luận
GradientsMô hình Giáo viên
Mô hình Học sinh
Gradient
Được chiếu Chia sẻ trọng số  
Lớp Conv Depth-wise
Giáo viên
Khối MobileNet-V2
(Học sinh)Khối MobileNet-V2
(Giáo viên)Chia sẻ trọng số  
Lớp Conv Point-wiseHọc sinh
HÌNH 1. Tổng quan về NetDistiller. TNN mục tiêu là một mô hình học sinh hoạt động như một mạng con trong một mô hình giáo viên chia sẻ trọng số, được xây dựng bằng cách mở rộng số lượng kênh của TNN mục tiêu. Các mô hình giáo viên và học sinh được huấn luyện đồng thời trong khi mô hình giáo viên được huấn luyện với nhãn ground truth và mục tiêu để huấn luyện mô hình học sinh được quyết định thích ứng đầu vào giữa một cơ chế chưng cất tại chỗ và nhãn ground truth dựa trên sự không chắc chắn đầu ra của nó. Để giảm thiểu vấn đề xung đột gradient quan sát được trong quá trình huấn luyện, các gradient của giáo viên được chỉnh sửa thông qua phẫu thuật gradient để loại bỏ các thành phần xung đột dựa trên các gradient của học sinh.

trong quá trình huấn luyện cho phép TNNs thu được kiến thức bổ sung, dẫn đến cải thiện độ chính xác nhiệm vụ. Trong chưng cất kiến thức thông thường [4], kiến thức được mã hóa bởi một mô hình lớn được chuyển giao cho một mô hình nhỏ hơn bằng cách huấn luyện mô hình nhỏ với các đầu ra và/hoặc activations của mô hình lớn. Kết quả là, mô hình nhỏ có thể đạt được độ chính xác cao hơn bằng cách bắt chước các hành vi của mô hình lớn. Trong công trình này, chúng tôi ủng hộ một lược đồ chưng cất kiến thức tại chỗ mới, song song với lược đồ thông thường, tiếp tục tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs. Chúng tôi có những đóng góp sau:
•Chúng tôi là những người đầu tiên chứng minh rằng việc tích hợp một supernet chia sẻ trọng số với chưng cất tại chỗ có thể phục vụ như một công thức huấn luyện hiệu quả để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs. Cụ thể, chúng tôi đề xuất một khung làm việc có tên NetDistiller, kết hợp TNN mục tiêu như một mô hình học sinh trong một supernet chia sẻ trọng số hoạt động như một mô hình giáo viên để tăng cường hiệu suất nhiệm vụ của các TNNs được huấn luyện mà không gây ra bất kỳ overhead suy luận nào.
•Chúng tôi xác định rằng chưng cất tại chỗ thông thường có thể gây ra xung đột gradient nghiêm trọng giữa giáo viên supernet và học sinh mạng con (tức là, TNN mục tiêu). Cụ thể, chúng tôi phát hiện rằng lên đến 50% các gradient trọng số trong mô hình học sinh có độ tương tự cosine âm với những gradient của mô hình giáo viên. Điều này dẫn đến sự hội tụ kém khi các gradient này tích lũy trên các trọng số chia sẻ của chúng. Hơn nữa, chưng cất tại chỗ thông thường có xu hướng gây ra overfitting trong mô hình giáo viên, do đó làm giảm hiệu quả của chưng cất tại chỗ của chúng tôi.
•Để giảm thiểu cả hai vấn đề đã xác định ở trên, NetDistiller đề xuất hai giải pháp: (1) loại bỏ các gradient xung đột bằng cách chiếu các gradient xung đột của giáo viên lên mặt phẳng pháp tuyến của các gradient của học sinh, và (2) tích hợp chưng cất nhận biết không chắc chắn để lựa chọn động hàm loss của học sinh giữa divergence Kullback–Leibler và loss cross-entropy dựa trên sự chắc chắn của đầu ra mô hình học sinh. Những cải tiến này cho phép NetDistiller giải phóng hiệu quả đầy hứa hẹn của chưng cất tại chỗ trong việc huấn luyện TNNs một cách thuận lợi hơn.
•Chúng tôi thực hiện các đánh giá và nghiên cứu ablation mở rộng để xác nhận hiệu quả của khung làm việc NetDistiller trong việc tăng cường độ chính xác có thể đạt được của TNNs khi so sánh với phương pháp tiên tiến nhất (SOTA). Ví dụ, chúng tôi quan sát thấy độ chính xác cao hơn 2,3% so với NetAug [2], khi huấn luyện mô hình MobileNet-V3-w0.35 trên tập dữ liệu ImageNet. Chúng tôi hiểu rằng NetDistiller đã mở ra một góc nhìn mới cho việc tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs và làm phong phú lĩnh vực chưng cất kiến thức.

CÔNG TRÌNH LIÊN QUAN
Mạng Nơ-ron Hiệu quả / Nhỏ. Tiến bộ đáng kể đã được thực hiện trong việc thiết kế các NNs hiệu quả và thân thiện với thiết bị di động. Ví dụ, MobileNets [5] sử dụng Depth-wise Separable Convolutions, thay thế một lớp convolution tiêu chuẩn bằng sự kết hợp của depthwise convolution và pointwise convolution, chứng minh tiềm năng giảm chi phí tính toán trong khi
2 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ Tháng Mười Một 2023

--- TRANG 3 ---
tinyML
duy trì độ chính xác nhiệm vụ. Song song với các mạng hiệu quả được thiết kế thủ công và các lược đồ nén, học máy tự động đã được sử dụng thành công thông qua tìm kiếm kiến trúc mạng nơ-ron [6], [7]. Trái ngược với các kỹ thuật trên, khung làm việc được đề xuất của chúng tôi nhằm cải thiện độ chính xác suy luận của TNNs thông qua chưng cất tại chỗ nơi kiến trúc TNN mục tiêu được sử dụng như một học sinh mạng con của một mô hình giáo viên supernet chia sẻ trọng số.

Chưng Cất Kiến Thức. Chưng Cất Kiến Thức (KD) [8] đề cập đến ý tưởng chuyển giao kiến thức được thu thập bởi một mô hình đã được huấn luyện trước và over-parameterized (giáo viên) cho một mô hình nhỏ (học sinh) phù hợp hơn cho việc triển khai tại biên. Cụ thể, mô hình nhỏ thường có khả năng không đủ để học một biểu diễn kiến thức ngắn gọn, và KD trao quyền cho mô hình học sinh để học hành vi chính xác của mô hình giáo viên bằng cách bắt chước các đầu ra của giáo viên ở mỗi cấp độ, tức là các nhãn mềm. Trong công trình này, chúng tôi ủng hộ một lược đồ chưng cất tại chỗ mới, cụ thể là NetDistiller để tăng cường hiệu suất nhiệm vụ của TNNs. Trái ngược với KD thông thường nơi mô hình giáo viên là một NN lớn được huấn luyện trước trên một tập dữ liệu khác và mô hình học sinh là một mô hình nhỏ riêng biệt, NetDistiller là một phương pháp trực giao kết hợp TNN như một mạng con học sinh trong một supernet chia sẻ trọng số hoạt động như mô hình giáo viên cho chưng cất tại chỗ.

Tăng Cường Mạng. Các tác giả trong [2] đề xuất tăng cường mạng, được biết đến như NetAug, để tăng cường độ chính xác của học sâu nhỏ bằng cách giảm thiểu vấn đề under-fitting. Cụ thể, NetAug tăng cường mạng một cách động trong quá trình huấn luyện, kết hợp mô hình nhỏ như một mô hình con trong một mô hình lớn hơn để giám sát phụ trợ ngoài chức năng độc lập của nó. Trái ngược, NetDistiller cung cấp một lược đồ thay thế để tăng cường hiệu suất của TNNs thông qua chưng cất kiến thức tại chỗ. Đặc biệt, TNN trong NetDistiller hoạt động như một mạng con (học sinh) trong một supernet chia sẻ trọng số tĩnh (giáo viên) được xây dựng bằng cách mở rộng các kênh của TNN mục tiêu. Trong phần kết quả thí nghiệm, chúng tôi cung cấp một nghiên cứu so sánh của NetDistiller được đề xuất với lược đồ SOTA NetAug và phát hiện rằng NetDistiller vượt trội hơn NetAug, ví dụ, đạt được độ chính xác cao hơn 2,3% khi huấn luyện mô hình MobileNet-V3-w0.35 trên tập dữ liệu ImageNet.

Khung Làm Việc NetDistiller
NetDistiller được đề xuất của chúng tôi là một công thức huấn luyện để tăng cường độ chính xác của TNNs bằng cách kết hợp TNN mục tiêu như một mô hình học sinh (mạng con) trong một supernet chia sẻ trọng số, hoạt động như một mô hình giáo viên. Thông qua chưng cất tại chỗ, NetDistiller chưng cất và chuyển giao kiến thức từ một mô hình giáo viên supernet đến mô hình học sinh mạng con là TNN mục tiêu của chúng tôi. Trong phần này, chúng tôi đầu tiên mô tả việc xây dựng supernet chia sẻ trọng số từ TNN tiếp theo là triển khai thực tế của chưng cất tại chỗ của chúng tôi. Tiếp theo, chúng tôi mô tả các kỹ thuật để giải quyết xung đột gradient giữa các mô hình giáo viên và học sinh, cũng như giảm thiểu vấn đề over-fitting trong mô hình giáo viên trong giai đoạn huấn luyện cuối thông qua chưng cất nhận biết không chắc chắn. Cuối cùng, chúng tôi thảo luận về overhead huấn luyện và suy luận gây ra bởi NetDistiller.

Yếu tố Kích hoạt 0 của NetDistiller: Xây dựng Mô hình Giáo viên Chia sẻ Trọng số
NetDistiller mở rộng các kênh của TNN mục tiêu để xây dựng một supernet chia sẻ trọng số hoạt động như mô hình giáo viên. Do đó, TNN mục tiêu hoạt động như một mô hình mạng con. Cả mô hình học sinh và giáo viên đều chia sẻ trọng số trên tất cả các lớp convolution trong khi duy trì các lớp Batch-Normalization tương ứng của chúng, chiếm các thống kê chạy khác nhau (tức là, means và variances) của các giá trị activation của chúng. Như một công thức huấn luyện mới để tăng cường khả năng của TNN mục tiêu để giảm thiểu các vấn đề under-fitting và tăng cường độ chính xác, NetDistiller xây dựng một mô hình giáo viên với số lượng kênh gấp 3 lần so với TNN mục tiêu. Hình 1 mô tả việc xây dựng mô hình giáo viên chia sẻ trọng số từ TNN mục tiêu được mô tả ở trên.

Yếu tố Kích hoạt 1 của NetDistiller: Chưng Cất Tại Chỗ
Mục tiêu của chưng cất tại chỗ của chúng tôi là ổn định việc huấn luyện supernet và cải thiện hiệu suất của các mạng con. Đặc biệt, TNNs có nhiều khả năng bị kẹt trong các cực tiểu cục bộ do khả năng không đủ, điều này hạn chế hiệu suất của chúng so với các NNs lớn over-parameterized [2]. Để giải quyết điều này, NetDistiller tích hợp TNN mục tiêu như một mô hình học sinh mạng con trong một mô hình giáo viên supernet chia sẻ trọng số được xây dựng bằng cách mở rộng các kênh của NN mục tiêu như được chứng minh trong Hình 1. Theo hiểu biết tốt nhất của chúng tôi, NetDistiller là đầu tiên chứng minh rằng việc áp dụng chưng cất tại chỗ cho một supernet chia sẻ trọng số [9] có thể phục vụ như một công thức huấn luyện hiệu quả để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs.

Cụ thể, chưng cất tại chỗ tận dụng các 'nhãn mềm' được dự đoán bởi supernet như các tín hiệu giám sát cho mô hình học sinh mạng con trong mỗi lần lặp huấn luyện trong khi sử dụng nhãn ground truth cho mô hình giáo viên. Chính thức, tại lần lặp huấn luyện n, tham số supernet W được cập nhật bởi

Wn←Wn−1+ηg(Wn−1),

Tháng Mười Một 2023 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ 3

--- TRANG 4 ---
tinyML
trong đó η là tốc độ học, và:
g(Wn−1) =∇W
LD(W)+
Lstu
[W,Wstu];Wn−1
W=Wn−1(1)
Ở đây, LD là loss cross-entropy của giáo viên supernet trên tập dữ liệu huấn luyện D, W và Wstu biểu thị các mô hình giáo viên và học sinh, tương ứng, và Lstu là loss học sinh được điều chế bởi chưng cất nhận biết không chắc chắn (được giới thiệu trong phần Yếu tố Kích hoạt 3). Ngoài ra, quá trình chưng cất trong NetDistiller là một lần, tức là nó được triển khai tại chỗ trong quá trình huấn luyện mà không có chi phí tính toán và bộ nhớ bổ sung, không giống như KD thông thường hai bước nơi một mô hình lớn phải được huấn luyện trước.

Yếu tố Kích hoạt 2 của NetDistiller: Phẫu thuật Gradient để Giải quyết Xung đột Gradient
Xem xét rằng các gradient từ cả mô hình học sinh và giáo viên tích lũy trên các trọng số chia sẻ, chúng tôi xác định rằng chưng cất tại chỗ thông thường có thể gây ra xung đột gradient nghiêm trọng giữa giáo viên supernet và học sinh mạng con (TNN mục tiêu). Cụ thể, chúng tôi phát hiện rằng lên đến 50% các gradient mô hình học sinh có độ tương tự cosine âm với những gradient của mô hình giáo viên. Lấy cảm hứng từ PCGrad [10] thực hiện phẫu thuật gradient cho học đa nhiệm vụ, NetDistiller giải quyết điều này bằng cách chiếu các gradient giáo viên xung đột lên mặt phẳng pháp tuyến của các gradient học sinh, do đó loại bỏ các thành phần xung đột trong các gradient giáo viên và cải thiện hiệu suất của TNNs. Cụ thể, gọi ∇lstu và ∇ltea biểu thị các gradient của mô hình học sinh và giáo viên, tương ứng. Chúng tôi định nghĩa φ là góc giữa hai gradient trên và g là gradient cuối cùng để cập nhật các trọng số. Để đảm bảo việc huấn luyện mô hình học sinh, chúng tôi chiếu gradient xung đột của giáo viên, proj(∇ltea), khi độ tương tự cosine cos(φ) =∇lstu.∇ltea
∥∇lstu∥∥∇ltea∥ là âm, được công thức hóa như sau:

g=∇lstu+proj(∇ltea), trong đó
proj(∇ltea) =

∇ltea−∇lT
tea∇lstu
∥∇lstu∥2∇lstu, if cos( φ)<0
∇ltea, otherwise(2)

Yếu tố Kích hoạt 3 của NetDistiller: Chưng Cất Nhận biết Không chắc chắn
Vì các mô hình học sinh và giáo viên chia sẻ trọng số được huấn luyện chung từ đầu và mô hình giáo viên over-parameterized hội tụ nhanh hơn mô hình học sinh, chúng tôi quan sát thấy rằng mô hình giáo viên supernet gặp phải over-fitting ở giai đoạn huấn luyện cuối. Ngoài ra, [11] ủng hộ rằng các mô hình lớn có cải thiện lớn nhất trên các mẫu nơi mô hình nhỏ không chắc chắn nhất. Đối với các ví dụ nhất định, ngay cả những nơi mô hình nhỏ không đặc biệt chính xác, các mô hình lớn thường không thể cải thiện. Dựa trên những hiểu biết này, chúng tôi đưa ra giả thuyết rằng mô hình giáo viên không phải lúc nào cũng là một giáo viên tốt trong suốt quá trình huấn luyện.

Dựa trên điều này, chúng tôi đề xuất một kỹ thuật được gọi là chưng cất nhận biết không chắc chắn (UD) để lựa chọn động các hàm loss học sinh giữa divergence Kullback–Leibler (KL) và loss cross-entropy dựa trên sự chắc chắn của đầu ra mô hình học sinh (xem Hình 1). Cụ thể, chúng tôi đo sự không chắc chắn thông qua entropy của các đầu ra học sinh. Khi entropy của đầu ra học sinh cao (tức là, không chắc chắn), học sinh được chưng cất bởi giáo viên chia sẻ trọng số thông qua loss KL divergence, ngược lại, học sinh được huấn luyện bởi nhãn ground truth thông qua loss cross-entropy. Chúng tôi công thức hóa quá trình này như sau:

Lstu=(
KL(Wstu(x),W(x)),uncertainty ≥T
CE(Wstu(x),y), otherwise(3)

Ở đây, T biểu thị ngưỡng không chắc chắn; Lstu biểu thị loss mô hình học sinh; KL() và CE() biểu thị loss KL divergence và loss cross-entropy, tương ứng; x và y biểu thị dữ liệu đầu vào và nhãn ground truth; và uncertainty biểu thị entropy của các đầu ra mô hình học sinh Wstu(x).

Phân tích Overhead Huấn luyện và Suy luận
Trái ngược với quá trình chưng cất hai bước trong KD thông thường, NetDistiller thực hiện chưng cất tại chỗ một lần kiến thức từ giáo viên supernet đến mô hình học sinh mạng con mà không có bất kỳ chi phí tính toán và bộ nhớ bổ sung nào. Tương tự như NetAug, NetDistiller không đưa ra overhead suy luận thêm vì chỉ TNN mục tiêu được sử dụng trong quá trình suy luận, cho phép việc triển khai TNNs khả thi trên các thiết bị biên bị hạn chế tài nguyên. Mặc dù mở rộng mô hình TNN mục tiêu lên 3×, chúng tôi quan sát thấy chỉ tăng 20% thời gian huấn luyện của NetDistiller so với TNNs thông thường.

KẾT QUẢ THỰC NGHIỆM

Thiết lập Thí nghiệm
Mô hình. Chúng tôi đánh giá NetDistiller với các phương pháp huấn luyện TNNs SOTA, ví dụ, NetAug [2], và KD [4], trên năm TNNs được chấp nhận rộng rãi [3], [2], bao gồm MobileNet-V2-Tiny, MobileNet-V2 (w0.35 và w1.0), MobileNet-V3, MCUNet (256kb-1mb), và ProxylessNAS (w0.35, w1.0). Đặc biệt, w0.35 chỉ ra các mô hình có

4 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ Tháng Mười Một 2023

--- TRANG 5 ---
tinyML
BẢNG 1. Đánh giá NetDistiller với các phương pháp SOTA để huấn luyện TNNs. r160: Độ phân giải hình ảnh đầu vào là 160×160. w0.35: Mô hình có 0.35 × số lượng kênh so với mô hình vanilla.

Mô hìnhMobileNet-V2-Tiny MCUNet MobileNet-V3, r160 ProxylessNAS, r160 MobileNet-V2, r160
r144 r176 w0.35 w0.35 w1.0 w0.35 w1.0
Tham số 0.75M 0.74M 2.2M 1.8M 4.1M 1.7M 3.5M
MACs 23.5M 81.8M 19.6M 35.7M 164.1M 30.9M 154.1M
Baseline 51.7% 61.5% 58.1% 59.1% 71.2% 56.3% 69.7%
NetAug [2] 53.3% 62.7% 60.3% 60.8% 71.9% 57.8% 70.6%
In-situ 54.1% 62.7% 62.1% 60.7% 71.2% 58.5% 71.2%
In-situ + PCGrad [10] 54.5% 63.4% 62.3% 61.3% 72.5% 59.0% 72.0%
NetDistiller (ours) 54.8% 64.2% 62.6% 61.5% 72.8% 59.3% 72.6%

BẢNG 2. Nghiên cứu ablation về tỷ lệ mở rộng kênh trên MobileNet-V2-w0.35 (MBV2-w0.35) và MobileNet-V3-w0.35 (MBV3-w0.35). Các kích thước giáo viên khác nhau trong hàng đầu tiên chỉ ra tỷ lệ mở rộng kênh. Xem xét sự cải thiện hạn chế (0.2%) giữa giáo viên ×4 và ×3 trên mô hình MobileNet-V2-w0.35 và hiệu quả huấn luyện, giáo viên với kích thước ×3 được chọn trong NetDistiller.

Kích thước Giáo viên Baseline ×2 ×3 ×4 ×5
MobileNet-V2-0.35 56.3% 58.0% 58.5% 58.7% 58.3%
MobileNet-V3-0.35 58.1% 61.3% 62.1% 61.8% 61.8%

0.35 lần số kênh so với vanilla (tức là, w1.0). Theo định nghĩa mô hình trong [2], các kênh của mô hình w0.35 được làm tròn thành bội số của 8.

Tập dữ liệu. Theo [2], chúng tôi xem xét tập dữ liệu ImageNet với độ phân giải đầu vào r144, r160, và r176 cho các TNNs mục tiêu khác nhau. Trong các thí nghiệm chưng cất kiến thức bên ngoài, độ phân giải đầu vào cho giáo viên bên ngoài được đặt thành r224 để phù hợp với cấu hình huấn luyện trước của nó. Các thí nghiệm phát hiện đối tượng được huấn luyện trên tập dữ liệu PASCAL VOC 2007+2012 và được đánh giá trên tập xác thực PASCAL VOC 2007 với độ phân giải đầu vào r416.

Các Chỉ số Đánh giá. Chúng tôi đánh giá NetDistiller và các phương pháp baseline về độ chính xác top-1 trên ImageNet và average precision tại IoU= 0.5(AP50) cho phát hiện đối tượng trên PASCAL VOC.

Thiết lập Huấn luyện. Theo thiết lập huấn luyện trong [2], chúng tôi huấn luyện TNNs trong 180 epochs sử dụng bộ tối ưu SGD với momentum 0.9 và tốc độ học ban đầu 0.4 với bộ lập lịch tốc độ học cosine. Chúng tôi áp dụng làm ấm tốc độ học trong 5 epochs và các gradient được cắt xuống 1.0 trong toàn bộ giai đoạn huấn luyện. Kỹ thuật làm mượt nhãn với hệ số 0.1 được áp dụng khi sử dụng nhãn ground truth. Đối với NetDistiller với chưng cất nhận biết không chắc chắn, chúng tôi sử dụng cùng công thức huấn luyện nhưng tăng epochs huấn luyện lên 360. Ngưỡng không chắc chắn T được đặt thành 3.75 dựa trên quan sát thực nghiệm của nghiên cứu ablation của chúng tôi. Tất cả các thí nghiệm ImageNet được chạy trên 8 GPU với kích thước batch 1024. Như một bài báo gần đây [2] đã phát hiện

BẢNG 3. Nghiên cứu ablation về phẫu thuật gradient trên mô hình MobileNet-V2-w0.35 và MobileNet-V3-w0.35 trong 360 epochs. Chúng tôi vô hiệu hóa phẫu thuật gradient và tính toán độ tương tự cosine giữa hai gradient (của giáo viên và học sinh) của mỗi lớp tích chập. Các giá trị phần trăm được hiển thị dưới các epochs khác nhau phản ánh tỷ lệ trung bình của số lớp có độ tương tự cosine âm (xung đột gradient) so với tổng số lớp trong mô hình.

Epoch 1 90 180 270 360
MobileNet-V2-w0.35 51.5% 40.1% 37.4% 39.4% 38.2%
MobileNet-V3-w0.35 50.1% 45.2% 34.7% 38.5% 37.4%

BẢNG 4. Nghiên cứu ablation về các ngưỡng chưng cất nhận biết không chắc chắn khác nhau trên MobileNet-V2-w0.35 và MobileNet-V2-w1.0. Hàng đầu tiên là các ngưỡng. Chưng cất nhận biết không chắc chắn chưng cất mô hình học sinh nếu entropy đầu ra của nó (không chắc chắn) cao hơn ngưỡng và huấn luyện mô hình học sinh với nhãn ground truth ngược lại.

Mô hìnhNgưỡng Không chắc chắn
2.5 3.75 5.0
MobileNet-V2-w0.35 59.1% 59.3% 58.9%
MobileNet-V2-w1.0 71.9% 72.6% 71.2%

rằng tăng cường dữ liệu và regularization có thể có hại cho việc huấn luyện TNN, chúng tôi chỉ sử dụng các tăng cường dữ liệu tiêu chuẩn (ví dụ. random flip, random crop) và vô hiệu hóa các phương pháp regularization như dropout và drop path. Đối với transfer learning trên nhiệm vụ phát hiện đối tượng, các mô hình MobileNet-V2-w0.35 và MobileNet-V3-w0.35 được kết nối với đầu YOLO-v4. Tất cả các thí nghiệm phát hiện đối tượng được huấn luyện thông qua bộ tối ưu SGD với momentum 0.9 và tốc độ học ban đầu 1e-4 giảm bởi bộ lập lịch tốc độ học cosine trong 100 epochs với kích thước batch 8.

Đánh giá với Các Phương pháp Huấn luyện TNN SOTA
Như được hiển thị trong Bảng 1, các quan sát của chúng tôi như sau: (1) NetDistiller cải thiện lên đến 4.5% độ chính xác so với tất cả baselines và đạt được độ chính xác cao hơn 2.3% so với baseline cạnh tranh nhất NetAug trên MobileNet-V3-

Tháng Mười Một 2023 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ 5

--- TRANG 6 ---
tinyML
BẢNG 5. Nghiên cứu ablation về kết hợp NetDistiller và chưng cất kiến thức bên ngoài. KD: Chưng cất TNN mục tiêu với một giáo viên bên ngoài (ResNet-50 được huấn luyện trước trên ImageNet). NetDistiller w/o UD: Chưng cất nhận biết không chắc chắn được tắt trong các thí nghiệm KD bên ngoài. NetDistiller+KD: Giáo viên bên ngoài chưng cất cả mô hình giáo viên và học sinh của NetDistiller.

Mô hình Baseline KD NetDistiller w/o UD NetDistiller+KD
MobileNet-V2-Tiny, r144 51.7% 53.7% (+2.0%) 55.5% (+3.8%) 56.1% (+4.4%)
MobileNet-V2-w0.35, r160 56.3% 58.4% (+2.1%) 59.0% (+2.7%) 59.5% (+3.2%)
MobileNet-V3-w0.35, r160 58.1% 61.6% (+3.5%) 62.3% (+4.2%) 62.5% (+4.4%)
ProxylessNAS-w0.35, r160 59.1% 60.8% (+1.7%) 61.3% (+2.2%) 61.9% (+2.8%)

w0.35. (2) Chưng cất tại chỗ đóng góp nhiều nhất: TNNs được huấn luyện chỉ thông qua chưng cất tại chỗ có thể phù hợp hoặc vượt qua phương pháp huấn luyện TNNs SOTA, NetAug [2]. (3) Trong tất cả các mô hình, TNNs đạt được sự tăng cường độ chính xác xấp xỉ 0.5% sau khi giới thiệu PCGrad [10] để giảm thiểu xung đột gradient. (4) NetDistiller được đề xuất của chúng tôi, kết hợp chưng cất tại chỗ với PCGrad và chưng cất nhận biết không chắc chắn, chứng minh là một phương pháp huấn luyện hiệu quả để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs.

Nghiên cứu Ablation của NetDistiller
Tỷ lệ Mở rộng Kênh của Mô hình Giáo viên. Vì NetDistiller mở rộng các kênh của TNNs để tạo ra một supernet chia sẻ trọng số như mô hình giáo viên, tỷ lệ mở rộng kênh của mô hình giáo viên đóng vai trò quan trọng trong việc xác định hiệu quả của cơ chế chưng cất tại chỗ. Điều này đặc biệt quan trọng xem xét rằng một mô hình giáo viên mỏng có thể có khả năng hạn chế, trong khi một giáo viên quá rộng có thể không chuyển giao thông tin hiệu quả. Để xác định tỷ lệ mở rộng kênh tối ưu, chúng tôi đánh giá NetDistiller với tỷ lệ mở rộng kênh ×2, ×3, ×4, và ×5 trên hai TNNs, MobileNet-V2-w0.35 và MobileNet-V3-w0.35, tương ứng. Như được hiển thị trong Bảng 2, chúng tôi quan sát thấy: (1) tất cả bốn mô hình giáo viên đều tăng cường độ chính xác của TNNs, chứng minh hiệu quả tổng thể của NetDistiller, và (2) MobileNet-V2-w0.35 và MobileNet-V3-w0.35 đạt được độ chính xác cao nhất khi tỷ lệ mở rộng kênh lần lượt là ×4 và ×3. Để giảm thiểu overhead huấn luyện gây ra bởi mô hình giáo viên mở rộng, chúng tôi mặc định đặt tỷ lệ mở rộng kênh thành ×3 trong NetDistiller.

Định lượng Xung đột Gradient. Do cơ chế chia sẻ trọng số, việc huấn luyện chung các mô hình học sinh và giáo viên dẫn đến việc tích lũy gradient trên cùng các trọng số, không thể tránh khỏi dẫn đến xung đột gradient. Để xác minh sự xuất hiện của xung đột gradient ở các giai đoạn huấn luyện khác nhau, chúng tôi tính toán tỷ lệ các lớp có độ tương tự cosine âm, trung bình trên tập xác thực, liên quan đến tổng số lớp trong mô hình trong suốt quá trình huấn luyện. Kết quả trong Bảng 3 chứng minh rằng các mô hình giáo viên và học sinh thực sự trải qua xung đột gradient, với nhiều đến 51.5% lớp thể hiện gradient xung đột. Để giải quyết vấn đề này, việc áp dụng phẫu thuật gradient dẫn đến cải thiện độ chính xác 0.5% và 0.2% cho MobileNet-V2-w0.35 và MobileNet-V3-w0.35, tương ứng, như được hiển thị trong Bảng 1.

Ngưỡng của Chưng Cất Nhận biết Không chắc chắn. Cơ chế chưng cất nhận biết không chắc chắn được đề xuất của chúng tôi lựa chọn động mục tiêu cho mô hình học sinh, lựa chọn giữa chưng cất tại chỗ hoặc cross-entropy dựa trên sự không chắc chắn của các đầu ra mô hình học sinh. Để quyết định ngưỡng không chắc chắn, chúng tôi xác thực các mô hình MobileNet-V2-w0.35 và MobileNet-V2-w1.0 với ngưỡng không chắc chắn 5.0, 3.75, và 2.5, xem xét entropy của các mô hình ImageNet nằm trong khoảng [1.5, 10] khi áp dụng hệ số làm mượt nhãn 0.1. Như được hiển thị trong Bảng 4, chúng tôi quan sát thấy cả hai mô hình đều đạt được độ chính xác cao nhất khi ngưỡng không chắc chắn được đặt thành 3.75, dẫn đến cải thiện độ chính xác 0.3% và 0.6%, tương ứng, so với NetDistiller không có chưng cất nhận biết không chắc chắn. Do đó, chúng tôi áp dụng ngưỡng không chắc chắn mặc định là 3.75 khi kích hoạt chưng cất nhận biết không chắc chắn, mà không có bất kỳ overhead bổ sung nào.

Chưng Cất Kiến Thức Bổ sung. Một baseline tự nhiên để đánh giá NetDistiller là chưng cất kiến thức tiêu chuẩn. Các quan sát gần đây đã gợi ý rằng khoảng cách đáng kể giữa các mô hình giáo viên và học sinh có thể dẫn đến hiệu suất chưng cất kiến thức không tối ưu [12]. Do đó, chúng tôi đưa ra giả thuyết rằng (a) chưng cất tại chỗ được đề xuất của chúng tôi phục vụ như một cơ chế hiệu quả hơn để cho phép huấn luyện TNN khi so sánh với chưng cất kiến thức, và (b) chưng cất kiến thức bổ sung cho phương pháp của chúng tôi và có thể được áp dụng đồng thời. Để xác nhận các giả thuyết này, chúng tôi chưng cất kiến thức từ một ResNet-50 được huấn luyện trước trên ImageNet cho cả mô hình giáo viên và học sinh của NetDistiller. Quá trình này được gọi là chưng cất bên ngoài để phân biệt với chưng cất tại chỗ của chúng tôi. Sau đó chúng tôi đánh giá phương pháp này so với (1) NetDistiller thông thường và (2) chưng cất kiến thức tiêu chuẩn. Như được hiển thị trong

6 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ Tháng Mười Một 2023

--- TRANG 7 ---
tinyML
-30% epochs+2.7% Acc
-44% epochs+4.2% Acc
Epoch Epoch Epoch EpochTop1 Acc  
Top1 Acc  
Top1 Acc  
Top1 Acc  MobileNet-V2-w0.35 T rain MobileNet-V2-w0.35 Eval  MobileNet-V3-w0.35 T rain MobileNet-V3-w0.35 Eval  
HÌNH 2. Hình dung quá trình huấn luyện của NetDistiller và các baselines cho mô hình MobileNet-V2-w0.35 và MobileNet-V3-w0.35. Nó tiết lộ rằng NetDistiller cải thiện đáng kể cả độ chính xác huấn luyện và đánh giá của TNNs. Điều này nổi bật tiềm năng đáng kể của NetDistiller trong việc tăng cường TNNs và giảm thiểu vấn đề underfitting.

Bảng 5, chúng tôi có thể quan sát rằng (1) NetDistiller thông thường có thể vượt trội hơn chưng cất kiến thức tiêu chuẩn, ví dụ, cải thiện độ chính xác 3.8% và 4.2% trên MobileNet-V2-Tiny và MobileNet-V3-w0.35 tương ứng, do đó xác nhận giả thuyết (a) của chúng tôi; và (2) chưng cất kiến thức trực giao với NetDistiller, vì việc áp dụng chưng cất kiến thức trên NetDistiller dẫn đến cải thiện độ chính xác 4.4% trên MobileNet-V2-Tiny và MobileNet-V3-w0.35, tương ứng, xác nhận giả thuyết (b) của chúng tôi.

Hình dung Quỹ đạo Huấn luyện
Chúng tôi trình bày các đường cong huấn luyện cho MobileNet-V2-w0.35 và MobileNet-V3-w0.35 trong suốt 180 epochs huấn luyện sử dụng NetDistiller, so với các baselines huấn luyện tiêu chuẩn tương ứng, trong Hình 2. Đáng chú ý, chúng tôi quan sát rằng cả độ chính xác huấn luyện và đánh giá của NetDistiller đều vượt trội hơn các baselines tương ứng ở cùng epoch huấn luyện. Ví dụ, NetDistiller đạt được cải thiện độ chính xác 2.7% cho MobileNet-V2-w0.35 và cải thiện độ chính xác ấn tượng 4.2% cho MobileNet-V3-w0.35 so với các baselines. Hơn nữa, để đạt được độ chính xác tương đương, NetDistiller yêu cầu ít epochs huấn luyện hơn, dẫn đến giảm 44% epochs huấn luyện, như được chỉ ra trong Hình 2.

Nghiên cứu Transfer Learning về Phát hiện Đối tượng
Để đánh giá tính tổng quát của các biểu diễn được học bởi NetDistiller, chúng tôi chuyển giao MobileNet-V2-w0.35 và MobileNet-V3-w0.35 được huấn luyện bởi NetDistiller đến một nhiệm vụ phát hiện đối tượng downstream. Sau đó chúng tôi so sánh hiệu suất của chúng với các mô hình được huấn luyện trước tiêu chuẩn, có hoặc không có chưng cất kiến thức, trên ImageNet. Cụ thể, chúng tôi thay thế các lớp pooling và linear cuối cùng trong MobileNet-V2-w0.35 và MobileNet-V3-w0.35 bằng đầu phát hiện đối tượng YOLO-v4. Như được hiển thị trong Bảng 6, NetDistiller liên tục có khả năng chuyển giao tốt hơn với Average Precision (AP) cao hơn

BẢNG 6. Transfer learning trên nhiệm vụ phát hiện đối tượng sử dụng tập dữ liệu PASCAL VOC 2007+2012 và mô hình MobileNet-V2-w0.35 (MBV2) hoặc MobileNet-V3-w0.35 (MBV3). Cả hai mô hình đều được kết nối với đầu phát hiện YOLO-v4. Mô hình được khởi tạo với trọng số được huấn luyện trước của NetDistiller trên ImageNet. Chúng tôi báo cáo Average Precision tại IoU= 0.5 (AP50).

Mô hình Baseline AP50 KD AP50 NetDistiller AP50
MobileNet-v2-w0.35 60.4% 61.1% 62.3%
MobileNet-v3-w0.35 63.6% 62.8% 65.2%

1.9% /1.6% trên MobileNet-V2-w0.35 / MobileNet-V3-w0.35 so với các baselines huấn luyện tiêu chuẩn. Lưu ý rằng mặc dù các đặc trưng được huấn luyện trước trên nhiệm vụ phân loại có thể không nhất thiết hữu ích cho các nhiệm vụ downstream [2], điều này cũng được phản ánh với kết quả của MobileNet-V3-w0.35 được huấn luyện trước KD trong Bảng 6, NetDistiller vẫn quản lý để cải thiện AP có thể đạt được lên đến 1.9% trên cả hai mô hình. Điều này nhấn mạnh khả năng ứng dụng rộng rãi của phương pháp chúng tôi trên các nhiệm vụ và tập dữ liệu đa dạng.

KẾT LUẬN
Tăng cường độ chính xác nhiệm vụ của TNNs là một thách thức quan trọng trong việc cho phép triển khai chúng trên các thiết bị IoT bị hạn chế tài nguyên. Khung làm việc NetDistiller được đề xuất của chúng tôi giải quyết thách thức này bằng cách xem TNNs như các mạng con của một mô hình giáo viên chia sẻ trọng số, đạt được bằng cách mở rộng số lượng kênh trong TNN. Bằng cách kết hợp phẫu thuật gradient để xử lý xung đột gradient và chưng cất nhận biết không chắc chắn để giảm thiểu overfitting mô hình giáo viên, NetDistiller cải thiện đáng kể độ chính xác có thể đạt được của TNNs. Các thí nghiệm mở rộng trên nhiều nhiệm vụ khác nhau chứng minh hiệu quả vượt trội của NetDistiller so với các lược đồ huấn luyện TNN SOTA. Tiến bộ này đánh dấu một bước quan trọng hướng tới việc hiện thực hóa tiềm năng đầy đủ của TNNs trong các ứng dụng IoT thực tế.

Tháng Mười Một 2023 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ 7

--- TRANG 8 ---
tinyML
Lời cảm ơn
Công trình này được hỗ trợ bởi Quỹ Khoa học Quốc gia (NSF) thông qua chương trình CCF (Số giải thưởng: 2211815) và bởi CoCoSys, một trong bảy trung tâm trong JUMP 2.0, một chương trình Tập đoàn Nghiên cứu Bán dẫn (SRC) được tài trợ bởi DARPA.

TÀI LIỆU THAM KHẢO
1.Lionel Sujay Vailshery. Iot and non-iot connections worldwide 2010-2025. https://www.statista.com/statistics/1101442/iot-number-of-connected-devices-worldwide/. Truy cập: 2021-03-08.

2.Han Cai, Chuang Gan, Ji Lin, and song han. Network augmentation for tiny deep learning. In International Conference onLearning Representations, 2022.

3.Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet: Tiny deep learning on iot devices. Advances inNeural Information Processing Systems , 33:11711–11722, 2020.

4.Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.

5.Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.

6.Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference onmachine learning , pages 6105–6114. PMLR, 2019.

7.Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Y anghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Y angqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In Proceedings oftheIEEE/CVF Conference onCVPR, pages 10734–10742, 2019.

8.Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances inNIPS , volume 27. Curran Associates, Inc., 2014.

9.Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, and Vikas Chandra. Alphanet: Improved training of supernets with alpha-divergence. In International Conference onMachine Learning , pages 10760–10771. PMLR, 2021.

10.Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances inNIPS , 33:5824–5836, 2020.

11.Taman Narayan, Heinrich Jiang, Sen Zhao, and Sanjiv Kumar. Predicting on the edge: Identifying where a larger model does better. arXiv preprint arXiv:2202.07652, 2022.

12.Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings of theIEEE/CVF ICCV, pages 4794–4802, 2019.

Shunyao Zhang là một sinh viên PhD tại Đại học Rice, Houston, USA. Anh ấy nhận được bằng thạc sĩ về Kỹ thuật Điện và Máy tính từ Đại học Carnegie Mellon, Pittsburgh, USA. Các lĩnh vực nghiên cứu của anh ấy là tiny ML và robustness đối kháng. Liên hệ với anh ấy tại sz74@rice.edu.

Yonggan Fu là một sinh viên PhD tại Viện Công nghệ Georgia. Trước đó, anh ấy nhận được bằng cử nhân từ Trường Các Tài năng Trẻ tại Đại học Khoa học và Công nghệ Trung Quốc. Trọng tâm và đam mê nghiên cứu của anh ấy là phát triển các thuật toán AI hiệu quả và mạnh mẽ và đồng thiết kế các bộ gia tốc phần cứng tương ứng hướng tới ba chiến thắng về độ chính xác, hiệu quả và robustness. Liên hệ với anh ấy tại yfu314@gatech.edu.

Shang Wu là một sinh viên thạc sĩ tại Đại học RICE nơi anh ấy chuyên ngành Kỹ thuật Điện và Máy tính. Anh ấy nhận được bằng cử nhân về khoa học máy tính tại Đại học George Washington. Các lĩnh vực nghiên cứu của anh ấy là ML hiệu quả, ML mạnh mẽ và AI tạo sinh. Liên hệ với anh ấy tại sw99@rice.edu.

Jyotikrishna Dass là một Nhà khoa học Nghiên cứu tại Đại học Rice và quản lý các hoạt động tại Chương trình Dữ liệu thành Kiến thức của Rice. Các lĩnh vực nghiên cứu của anh ấy là trong các hệ thống học máy phân tán và song song cho điện toán biên hiệu quả. Trước đó, anh ấy là một nghiên cứu sinh sau tiến sĩ tại phòng thí nghiệm của Tiến sĩ Yingyan Lin. Tiến sĩ Dass nhận được PhD về Kỹ thuật Máy tính từ Đại học Texas A&M. Liên hệ: jdass@rice.edu.

Haoran You hiện tại là một sinh viên PhD trong Khoa CS của Viện Công nghệ Georgia. Anh ấy nhận được bằng cử nhân trong lớp tiên tiến tại Đại học Khoa học và Công nghệ Hoa Trung và bằng thạc sĩ tại Đại học Rice. Anh ấy đang theo đuổi bằng tiến sĩ trong lĩnh vực học máy và kiến trúc máy tính. Các lĩnh vực nghiên cứu của anh ấy bao gồm nhưng không giới hạn trong học máy bị hạn chế tài nguyên, thị giác máy tính, học sâu, và đồng thiết kế thuật toán/bộ gia tốc. Liên hệ: haoran.you@gatech.edu.

8 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ Tháng Mười Một 2023

--- TRANG 9 ---
tinyML
Yingyan (Celine) Lin hiện tại là Phó Giáo sư tại Trường Khoa học Máy tính và là thành viên của Trung tâm Học Máy tại Viện Công nghệ Georgia. Cô ấy dẫn dắt Phòng thí nghiệm Điện toán Hiệu quả và Thông minh (EIC). Nghiên cứu của cô ấy tập trung vào phát triển các kỹ thuật học máy hiệu quả thông qua đổi mới liên lớp, trải dài từ các thuật toán trí tuệ nhân tạo (AI) hiệu quả đến bộ gia tốc phần cứng AI và thiết kế chip AI, và nhằm thúc đẩy AI xanh và trí thông minh được hỗ trợ bởi AI phổ biến. Cô ấy nhận được bằng PhD về Kỹ thuật Điện và Máy tính từ Đại học Illinois tại Urbana-Champaign vào năm 2017. Giáo sư Lin đã nhận được Giải thưởng CAREER của NSF, Giải thưởng Giảng viên IBM, Giải thưởng Nghiên cứu Giảng viên Facebook, và Giải thưởng Giảng viên Trẻ Xuất sắc ACM SIGDA. Cô ấy đã phục vụ trong Ủy ban Chương trình Kỹ thuật cho nhiều hội nghị khác nhau bao gồm DAC, ICCAD, MLSys, MICRO, và NeurIPS. Cô ấy hiện tại là Biên tập viên Phụ trách cho IEEE Transactions on Circuits and Systems II: Express Briefs. Liên hệ với cô ấy tại celine.lin@gatech.edu.

Tháng Mười Một 2023 NetDistiller: Trao Quyền cho Học Sâu Nhỏ thông qua Chưng Cất Tại Chỗ 9
