# BD-KD: Cân bằng các Phân kỳ cho Chưng cất Tri thức Trực tuyến
Ibtihel Amara1, Nazanin Sepahvand1, Brett H. Meyer1, Warren J. Gross1, và
James J. Clark1
Đại học McGill
Tóm tắt. Chúng tôi giải quyết thách thức tạo ra các mô hình nhỏ gọn đáng tin cậy và chính xác cho các thiết bị biên. Trong khi Chưng cất Tri thức (KD) đã cải thiện việc nén mô hình về mặt đạt được hiệu suất độ chính xác cao, việc hiệu chuẩn các mô hình nhỏ gọn này đã bị bỏ qua. Chúng tôi giới thiệu BD-KD (Chưng cất Tri thức Phân kỳ Cân bằng), một khung cho KD trực tuyến dựa trên logit. BD-KD tăng cường cả độ chính xác và hiệu chuẩn mô hình đồng thời, loại bỏ nhu cầu về các kỹ thuật hiệu chuẩn lại sau hoc, vốn thêm chi phí tính toán vào toàn bộ pipeline huấn luyện và làm giảm hiệu suất. Phương pháp của chúng tôi khuyến khích huấn luyện tập trung vào học sinh bằng cách điều chỉnh tổn thất chưng cất trực tuyến thông thường trên cả tổn thất học sinh và giáo viên, sử dụng trọng số theo từng mẫu của phân kỳ Kullback-Leibler thuận và nghịch. Chiến lược này cân bằng độ tin cậy của mạng học sinh và tăng hiệu suất. Các thí nghiệm trên các tập dữ liệu CIFAR10, CIFAR100, TinyImageNet và ImageNet, và các kiến trúc khác nhau cho thấy hiệu chuẩn và độ chính xác được cải thiện so với các phương pháp KD trực tuyến gần đây.
Từ khóa: Chưng cất Tri thức ·Huấn luyện tập trung học sinh ·Hiệu chuẩn Mô hình

1 Giới thiệu
Một yếu tố chính trong tính khả thi của nhiều ứng dụng biên quan trọng là sự hợp nhất của độ tin cậy thực sự với độ chính xác đặc biệt. Một minh họa thuyết phục về điều này có thể được chứng kiến trong lĩnh vực xe tự lái, một miền mà tính cấp thiết của sự nhỏ gọn là chính yếu. Trong những trường hợp như vậy, các mạng nơ-ron sâu chịu trách nhiệm cho các quá trình ra quyết định phức tạp phải được định cỡ tỉ mỉ để tích hợp liền mạch với các chip điện tử có sức mạnh tính toán hạn chế và dung lượng bộ nhớ giới hạn. Sự thành công của những ứng dụng như vậy đòi hỏi duy trì sự cân bằng giữa tính nhỏ gọn, độ chính xác và độ tin cậy dự đoán được hiệu chuẩn cho sự không chắc chắn. Nhiều nghiên cứu [3,5,8,9,11,12,34] đã tập trung vào việc nén các mô hình sâu lớn hoạt động tốt với hàng tỷ tham số. Trong số các kỹ thuật nén thành công hơn là chưng cất tri thức (KD), một paradigm huấn luyện giáo viên-học sinh. KD được biết đến với tính đơn giản và linh hoạt vì nó độc lập với kiến trúc, không có ràng buộc về loại kiến trúc của cả mạng giáo viên hoặc mạng học sinh. Tuy nhiên, các kỹ thuật KD đã bị cản trở bởi các vấn đề phát sinh khi có khoảng cách dung lượng lớn giữa các mạng giáo viên và các mạng học sinh nhỏ hơn, như đã được lưu ý lần đầu bởi [20].

Trọng tâm chủ yếu của nghiên cứu chưng cất tri thức (KD) đã phát triển xung quanh việc tăng cường độ chính xác của học sinh, sử dụng các nguyên tắc thiết kế phức tạp và các hàm mục tiêu đa diện. Tuy nhiên, sự chú ý không đủ đã được hướng tới việc cải thiện hiệu chuẩn trong mô hình học sinh nhỏ gọn kết quả. Hiệu chuẩn được liên kết với độ đáng tin cậy và độ tin cậy của các mạng [6]. Một mô hình được hiệu chuẩn tốt là mô hình mà các xác suất tin cậy ước tính phản ánh các xác suất chính thực của các kết quả liên quan. Ví dụ, nếu mô hình cung cấp một câu trả lời với ước tính tin cậy là 0.95, trong một mạng được hiệu chuẩn tốt, điều này sẽ biểu thị rằng sự kiện được dự đoán hiện thực hóa trong thực tế 95% thời gian. Sự liên kết này giữa dự đoán và thực tế đặt nền tảng cho việc ra quyết định mạnh mẽ và sáng suốt, do đó nhấn mạnh độ tin cậy và hiệu quả của mô hình.

Trong nghiên cứu của chúng tôi, chúng tôi xây dựng dựa trên các quan sát thực nghiệm gần đây [16,26,38] nhấn mạnh hiệu quả của chưng cất trực tuyến trong việc tăng cường độ chính xác của mô hình học sinh. Chúng tôi tập trung vào việc thu được các mô hình học sinh nhỏ gọn vừa hoạt động tốt vừa được hiệu chuẩn tốt hơn. Chúng tôi đề xuất một khung học tập đơn giản cho huấn luyện KD trực tuyến hiệu quả được gọi là BD-KD: Chưng cất Tri thức phân kỳ cân bằng. Cách tiếp cận của chúng tôi khai thác nguyên tắc nổi bật của huấn luyện hợp tác với trọng tâm đặc biệt vào quá trình học tập của học sinh, trong đó cả huấn luyện của mạng giáo viên và học sinh được sắp xếp để tham gia vào một trao đổi có đi có lại các tín hiệu phản hồi thông tin thông qua việc tích hợp tổn thất phân kỳ Kullback-Leibler (KL) của chúng.

Những đóng góp chính của chúng tôi là:
–Chúng tôi đề xuất một chiến lược huấn luyện KD trực tuyến với trọng tâm vào huấn luyện mô hình học sinh trong đó tổn thất KL thuận thông thường trong hàm mục tiêu học sinh được thay thế bằng trọng số theo từng mẫu của cả KL thuận và ngược. Chúng tôi chứng minh rằng sơ đồ trọng số này tăng cường đáng kể hiệu suất của học sinh thông qua ước tính không chắc chắn đầu ra và hiệu chuẩn tốt hơn.

–Phương pháp của chúng tôi vượt trội hơn các mô hình KD tiên tiến, cả trực tuyến và ngoại tuyến, về mặt độ chính xác, cho bốn tập dữ liệu thị giác máy tính khác nhau và trên nhiều kiến trúc mạng.
–Cách tiếp cận của chúng tôi cải thiện hiệu chuẩn mạng học sinh, so với cả các phương pháp trực tuyến và ngoại tuyến hiện có. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên báo cáo kết nối giữa chưng cất trực tuyến và hiệu chuẩn mô hình được cải thiện.
–Cách tiếp cận của chúng tôi có thể giảm thiểu vấn đề khoảng cách dung lượng để các mạng học sinh nhỏ gọn có lợi từ các giáo viên dung lượng lớn hơn mà không mất hiệu suất.

Tính mới lạ của bài báo chúng tôi thể hiện rõ nhất trong chưng cất tập trung vào học sinh, điều này tạo hình và tái định nghĩa cả paradigm nén mô hình và hiệu chuẩn. Bằng cách tận dụng cơ chế cân bằng tin cậy trong quá trình huấn luyện chưng cất từ một mạng giáo viên lớn không được hiệu chuẩn, chúng tôi đạt được hiệu suất đáng chú ý. Hơn nữa, khung của chúng tôi dễ dàng tích hợp với các hàm mục tiêu đa dạng cho các nhiệm vụ và ứng dụng khác nhau.

2 Các công trình liên quan
KD Trực tuyến tập trung vào Học sinh. Một đóng góp nổi bật trong lĩnh vực KD trực tuyến tập trung vào học sinh được giới thiệu bởi [27]. Phương pháp của họ, được gọi là SwitOKD, cố gắng tăng cường độ chính xác của mô hình học sinh bằng cách thu hẹp khoảng cách độ chính xác trong quá trình huấn luyện. Điều này được đảm bảo bằng cách chiến lược làm chậm quá trình huấn luyện của giáo viên, cho phép mạng học sinh đạt được những bước tiến đáng kể. Cách tiếp cận này chuyển đổi các giai đoạn KD ngoại tuyến và trực tuyến, truyền vào quá trình huấn luyện với trọng tâm tập trung vào học sinh. Tương tự, [16] đề xuất một paradigm huấn luyện giáo viên proxy, được gọi là SHAKE, trong đó các trọng số được cập nhật thông qua việc sử dụng mô hình giáo viên (KD ngoại tuyến) và học sinh trong quá trình huấn luyện KD trực tuyến. Họ tin rằng giáo viên proxy được khuyến khích nhận thức về khả năng của huấn luyện học sinh, điều này khuyến khích huấn luyện tập trung vào học sinh. Trong khi những phương pháp này thiết lập các tiêu chuẩn ấn tượng cho KD trực tuyến, chúng có một số hạn chế. SwitOKD kéo dài huấn luyện do thực tế chúng ta đang làm chậm huấn luyện của mạng giáo viên, và SHAKE thêm độ phức tạp thông qua việc sử dụng một giáo viên proxy và thiết kế tổng thể được đề xuất. Trong bài báo này, chúng tôi đề xuất một phương pháp chưng cất trực tuyến mới trong đó, giống như các phương pháp đã nêu, chúng tôi tập trung vào việc có huấn luyện KD trực tuyến tập trung vào học sinh với mục tiêu chính là đạt được hiệu suất cao với hiệu chuẩn được cải thiện. Cách tiếp cận mới của chúng tôi liên quan đến việc sửa đổi các hàm mục tiêu KD trực tuyến, trong đó cả tổn thất phân kỳ KL thuận và nghịch đều được sử dụng.

Hiểu biết KL Đối xứng. Việc kết hợp cả KL thuận và ngược đã chứng minh huấn luyện mạnh mẽ và hiệu suất đáng kể [1,35]. Một số cách tiếp cận KD ngoại tuyến không có dữ liệu đã chấp nhận phân kỳ Jensen-Shannon (JSD), khai thác các phân kỳ KL thuận và nghịch để tổng hợp các hình ảnh không dư thừa hiệu quả hơn. Cách tiếp cận này cũng hỗ trợ trong việc định lượng sự khác biệt giáo viên-học sinh trong các tình huống không có dữ liệu [1]. Lee và cộng sự [15] đã sử dụng KL đối xứng đơn giản bằng cách kết hợp cả các thành phần KL thuận và nghịch trong tự chưng cất ngoại tuyến được đề xuất của họ thông qua dropout. Không giống như các phương pháp dựa trên JSD, cách tiếp cận của chúng tôi cân bằng một cách độc đáo các phân kỳ KL thuận và ngược dựa trên sự không chắc chắn của học sinh-giáo viên, thích ứng với hành vi tìm kiếm mode và tìm kiếm mean. Sự khác biệt quan trọng này, điều mà làm cho công trình của chúng tôi khác biệt và giải quyết một cách động ước tính không chắc chắn học sinh-giáo viên dẫn đến cải thiện hiệu chuẩn và hiệu suất đáng kể.

Tái trọng số theo từng mẫu trong KD. Trong KD, tái trọng số theo từng mẫu của tổn thất KD đã được chứng minh là rất hiệu quả cho hiệu suất chưng cất hiệu quả. Tang và cộng sự [31] đã chỉ ra rằng việc điều chỉnh động trọng số mô hình học sinh trong quá trình huấn luyện, dựa trên một yếu tố tái thang logit được suy ra từ tin cậy dự đoán tương đối của giáo viên cho mỗi mẫu, cải thiện đáng kể hiệu suất tổng thể của học sinh. Tương tự, Zhou và cộng sự [41] đề xuất một phương pháp trọng số cho nhãn mềm, áp dụng trọng số nhỏ hơn nếu mô hình học sinh vượt trội hơn mô hình giáo viên cho một mẫu nhất định. Các thí nghiệm của họ chứng minh rằng tái trọng số theo từng mẫu tăng cường hiệu suất chưng cất. Su và cộng sự [30] đề xuất một chiến lược tái trọng số entropy, sử dụng entropy của giáo viên để tái trọng số tổn thất KD của học sinh.
Công trình của chúng tôi khác biệt đáng kể so với các kỹ thuật tái trọng số theo từng mẫu đã nêu. Hầu hết các kỹ thuật này được sử dụng trong thiết lập KD ngoại tuyến, sử dụng một thành phần duy nhất của phân kỳ KL, thường là KL thuận, và áp dụng yếu tố trọng số theo từng mẫu chỉ trên thành phần đó. Ngược lại, BD-KD hài hòa các đóng góp của cả phân kỳ KL thuận và nghịch trong tổn thất KD bằng cách sử dụng khoảng cách không chắc chắn (tức là khoảng cách entropy) giữa các mạng giáo viên lớn và học sinh nhỏ gọn như một yếu tố trọng số theo từng mẫu.

Hiệu chuẩn trong KD. Hiệu chuẩn mô hình đã thu hút sự chú ý của nhiều nghiên cứu [6,21,22,36,40] vì nó giải quyết sự không chắc chắn của các mạng sâu được đưa ra độ chính xác thực nghiệm của chúng. Có nhiều kỹ thuật hiệu chuẩn trong tài liệu. Một số tập trung vào việc tăng cường hiệu chuẩn thông qua tăng cường dữ liệu rộng rãi, chẳng hạn như mixup [32], trong khi những kỹ thuật khác tập trung vào các kỹ thuật post-hoc như thang đo nhiệt độ [6], vốn thêm chi phí tính toán vào huấn luyện và giảm hiệu suất. Nghiên cứu gần đây đã chỉ ra rằng các kỹ thuật chưng cất thông thường khuyến khích mô hình học sinh được hiệu chuẩn tốt hơn so với cùng một mô hình được huấn luyện từ đầu [29,36]. Các phương pháp hiện tại đang chuyển hướng tới việc thu được các mạng được hiệu chuẩn tốt thông qua KD. Trong công trình của chúng tôi, bằng trọng số thích ứng giữa KL nghịch và thuận để cân bằng tình trạng quá tin cậy và thiếu tin cậy của học sinh nhỏ gọn trong huấn luyện, làm cho huấn luyện chưng cất tập trung hơn vào học sinh, chúng tôi thu được các mô hình học sinh nhỏ gọn được hiệu chuẩn tốt hơn mà không ảnh hưởng đến hiệu suất tổng thể của chúng.

3 Phương pháp luận
Khung của chúng tôi được trình bày trong Hình 1. Trong phương pháp chưng cất ban đầu của [10], các nhãn mềm thu được từ một giáo viên được huấn luyện trước được sử dụng để huấn luyện một mạng học sinh. Trong hình thức KD này, hàm mục tiêu được sử dụng để hướng dẫn huấn luyện của học sinh bao gồm hai thành phần: (1) tổn thất cross-entropy cổ điển, LCE, tối thiểu hóa khoảng cách giữa dự đoán của học sinh và nhãn cứng, và (2) tổn thất chưng cất, bao gồm phân kỳ KL, LDistill=LKL, có mục tiêu là đưa các logit của học sinh gần với giáo viên hơn.

Loại phân kỳ KL được sử dụng trong hầu hết các cách tiếp cận KD là KL thuận:
KL(pt||ps) =P
xpt(x) log
pt(x)
ps(x)
, trong đó pt và ps là các xác suất softmax của giáo viên và học sinh, tương ứng. Một nhược điểm chính của phân kỳ KL thuận là nó có thể ước tính quá mức độ không chắc chắn của phân phối gốc [19,23]. Chi tiết thêm về hiểu biết về ước tính quá mức và thiếu hụt có thể được tìm thấy trong tài liệu bổ sung. Phân kỳ KL là không đối xứng. Một biến thể của KL thuận hoạt động theo hướng ngược lại, và được biết đến như KL nghịch: KL(ps||pt) =P
xps(x) log
ps(x)
pt(x)
. KL thuận được biết đến là thể hiện hành vi tìm kiếm mean [19,23]. Nhìn vào biểu thức của nó, lượng bên trong log cao khi pt(x) cao và ps(x) thấp. Để tránh giá trị cao này trong quá trình tối thiểu hóa, khối lượng của ps nên được phân tán ra bất cứ nơi nào pt(x) có một số khối lượng, điều này giải thích thuộc tính tìm kiếm mean. Ngược lại, KL nghịch được biết đến có thuộc tính tìm kiếm mode [19,23]. Để tối thiểu hóa thành phần này, khối lượng của ps nên bao phủ vùng mà pt(x) cao làm cho ps ưu tiên bao phủ các mode cao của pt và bỏ qua các mode khác, điều này giải thích thuộc tính tìm kiếm mode. Chúng tôi cung cấp trong tài liệu bổ sung chi tiết và ví dụ về hiểu biết các hiệu ứng của tìm kiếm mean và tìm kiếm mode liên quan đến sự không chắc chắn của học sinh. Cho các thuộc tính này của KL thuận và nghịch, và để đảm bảo liên kết tốt hơn và độ trung thực cao hơn giữa các phân phối softmax của học sinh và giáo viên, trong phương pháp đề xuất của chúng tôi, hai thay đổi chính được thực hiện trong tổn thất KD ban đầu: (1) thay vì sử dụng KL thuận thông thường trong hàm mục tiêu của học sinh, chúng tôi sử dụng cả KL thuận và nghịch, cùng với một cơ chế trọng số theo từng mẫu cân bằng sự không chắc chắn của mô hình học sinh và (2) phân kỳ KL nghịch được sử dụng cho hàm mục tiêu của giáo viên (xem Eq.3 và Eq.1). Các hàm tổn thất của học sinh và giáo viên cho cách tiếp cận của chúng tôi được hiển thị trong Eq. 1 và Eq. 3:

Ls=αsX
iLCE(ps
i, yi) +Ls
KD(1)

trong đó,

Ls
KD=τ2βsX
iδi,fKL(pt
i,τ||ps
i,τ) +τ2βsX
iδi,rKL(ps
i,τ||pt
i,τ)(2)

và,

Lt=αtX
iLCE(pt
i, yi) +Lt
KD(3)

trong đó,

Lt
KD=τ2βtX
iKL(ˆpt
i,τ||ˆps
i,τ) (4)

trong đó pt và ps là các dự đoán của giáo viên và học sinh, tương ứng và τ là nhiệt độ để làm mềm các xác suất này cho chưng cất. i đại diện cho mẫu thứ i trong tập huấn luyện. yi là nhãn sự thật cơ bản của mẫu i, được mã hóa one-hot. Chúng ta thấy từ Eq. 3 rằng tổn thất KL của giáo viên khác với những tổn thất được sử dụng trong các phương pháp trực tuyến khác. Trong phương pháp của chúng tôi, thành phần KL nghịch được sử dụng trong hàm mục tiêu của giáo viên hoạt động như một bộ điều hòa, điều này chống lại vấn đề quá tin cậy thường được quan sát trong các mạng lớn.

Cơ chế Trọng số Theo từng Mẫu. Giả sử fs và ft là các biểu diễn của học sinh và giáo viên tương ứng, trong đó logit của giáo viên, zt
i, cho mẫu thứ i (xi, yi), là: zit=ft(xi, yi). Tương tự, chúng ta định nghĩa logit của học sinh là zis=fs(xi, yi). Đầu ra của hai mạng là một phân phối dự đoán phân loại trên c lớp: ps(yi=j|xi) = σ(zs,τ
i) = exp(zs
i/τ)/Pc
j=1exp(zs
j/τ) và pt(yi=j|xi) =σ(zt,τ
i) = exp(zt
i/τ)/Pc
j=1exp(zt
j/τ). Entropy theo từng mẫu của phân phối dự đoán được cho là:

Ht(xi, yi) =−cX
j=0pt(yi=j|xi) log( pt(yi=j|xi))
Hs(xi, yi) =−cX
j=0ps(yi=j|xi) log( ps(yi=j|xi)).

Các giá trị entropy này được sử dụng để cân bằng các thành phần KL thuận và ngược của hàm mục tiêu của học sinh:

δi,r=1ifHs(xi, yi)−Ht(xi, yi)<0
votherwise
δi,f=vifHs(xi, yi)−Ht(xi, yi)<0
1otherwise

trong đó δf
i và δr
i lần lượt là các trọng số theo từng mẫu của tổn thất KL thuận và dự trữ trong Eq.1 và v là một siêu tham số với các giá trị lớn hơn một.
Thành phần Hs(xi, yi)−Ht(xi, yi) đo lường sự khác biệt giữa entropy của học sinh và giáo viên cho mỗi mẫu đầu vào, với các giá trị âm biểu thị một học sinh "tìm kiếm mode" (ước tính thấp sự không chắc chắn của giáo viên). Để giải quyết điều này, chúng tôi tăng trọng số trên KL thuận với siêu tham số δi,f được đặt thành v >> 1 và δi,r= 1. Ngược lại, các giá trị dương biểu thị một học sinh "tìm kiếm mean" với entropy cao hơn (ước tính quá mức sự không chắc chắn của giáo viên). Trong trường hợp này, chúng tôi tăng cường trọng số KL nghịch với δi,f= 1 và δi,r=v cho việc tái điều chỉnh.

Chúng tôi cung cấp một phân tích toàn diện và rõ ràng về thuật toán của chúng tôi, được trình bày theo định dạng kiểu PyTorch trong Thuật toán 1 của tài liệu bổ sung. Mã của chúng tôi có thể dễ dàng được tích hợp vào một loạt các khung chưng cất và trên các nhiệm vụ downstream đa dạng, tăng cường thêm khả năng thích ứng và tiện ích của nó. Chúng tôi dự định công khai mã liên quan đến bài báo của chúng tôi khi nó được chấp nhận để xuất bản.

7 Bảng 1: So sánh độ chính xác (%) trên các kỹ thuật chưng cất trực tuyến tiên tiến và kỹ thuật KD vanilla ngoại tuyến cơ bản. (*) là các giá trị được cung cấp trong [27]. Kết quả cho CIFAR-10, CIFAR-100, và Tiny-ImageNet được lấy trên ba hạt giống ngẫu nhiên. Chúng tôi báo cáo trung bình và độ lệch chuẩn. Chúng tôi quan sát thấy BD-KD cải thiện đáng kể cả mạng học sinh và giáo viên.

[Bảng với các kết quả cho CIFAR-10, CIFAR-100, Tiny-ImageNet và ImageNet được dịch giữ nguyên cấu trúc]

4 Thí nghiệm và Kết quả
4.1 Chi tiết Thí nghiệm
Trong phần này, chúng tôi thực hiện một loạt thí nghiệm để chứng minh hiệu quả của huấn luyện chưng cất trực tuyến BD-KD được đề xuất. Chúng tôi so sánh phương pháp của chúng tôi với các cách tiếp cận tiên tiến sử dụng các kiến trúc mạng khác nhau và các tập dữ liệu phân loại khác nhau.

Tập dữ liệu. Chúng tôi thực hiện các thí nghiệm trên bốn tập dữ liệu khác nhau: CIFAR10, CIFAR100 [13], Tiny-ImageNet [14], và ImageNet [4]. Chúng tôi sử dụng các tăng cường dữ liệu tiêu chuẩn như RandomCrop và RandomHorizontalFlip. Chúng tôi chuẩn hóa tất cả các hình ảnh theo kênh sử dụng mean và độ lệch chuẩn. Ngoài vanilla KD, chúng tôi so sánh phương pháp của chúng tôi với các công trình gần đây về chưng cất trực tuyến như DML [38], KDCL [7], SwitOKD [26], và SHAKE [16]. Chúng tôi cũng đối chiếu các phát hiện của chúng tôi với các phương pháp KD ngoại tuyến hiện tại, như CRD [33], DKD [39], FitNet [28], v.v.

Chi tiết Huấn luyện. Chúng tôi tuân theo quy trình huấn luyện của các công trình trước [26,33] và chúng tôi báo cáo hiệu suất trên nhiều tập dữ liệu. Chúng tôi chủ yếu sử dụng bộ tối ưu SGD với động lượng 0.9 cho tất cả các tập dữ liệu. Đối với CIFAR100, tổng số epoch được đặt thành 240. Tốc độ học ban đầu được đặt thành 0.01 cho hầu hết các mạng, và chúng tôi thực hiện một scheduler trên tốc độ học. Sau này được chia cho 10 mỗi epoch thứ 150, 180, và 210. Đối với CIFAR-10, tổng số epoch được đặt thành 300. Chúng tôi chia tốc độ học cho 10 mỗi epoch thứ 100, 150, và 200. Đối với Tiny-ImageNet, tổng số epoch được đặt thành 120 và tốc độ học ban đầu là 0.01. Chúng tôi đặt nhiệt độ τ cho KD thành 2. Giá trị được chọn cho v là 2 cho trọng số thích ứng theo từng mẫu. Chúng tôi đánh giá hiệu suất của các mô hình sử dụng độ chính xác phân loại.

Chúng tôi căn chỉnh các thí nghiệm của chúng tôi với các thực hành đánh giá điểm chuẩn phổ biến trong cộng đồng KD, đảm bảo một đánh giá công bằng và không thiên vị về phương pháp của chúng tôi. Tất cả các cặp giáo viên-học sinh được kiểm tra trong bài báo của chúng tôi đều được lấy từ các bài báo trước đó trong đó việc điều chỉnh siêu tham số toàn diện đã được tiến hành để đạt được kết quả tối ưu.

4.2 Kết quả
Kết quả trên CIFAR-10, 100, và Tiny-ImageNet. Chúng tôi hiển thị các kết quả thí nghiệm trên CIFAR-10, CIFAR-100, và Tiny ImageNet và so sánh BD-KD với các phương pháp KD trực tuyến và ngoại tuyến gần đây. Bảng 1 trình bày độ chính xác xác thực tính theo % của các phương pháp KD trực tuyến gần đây. Nhìn chung, cách tiếp cận của chúng tôi mang lại cải thiện trên tất cả các tập dữ liệu và tất cả các cặp học sinh-giáo viên. Với BD-KD, chúng tôi quan sát thấy sự tăng từ 1% - 3% so với baseline chưng cất trực tuyến DML [38] trên tất cả các tập dữ liệu và cặp giáo viên-học sinh. Thật vậy, một sự tăng đáng kể-cao về hiệu suất học sinh với kiến trúc mobileNetv2 được chưng cất từ WRN-16-2 trên CIFAR-100 có thể quan sát được. Chúng ta thấy một sự tăng hiệu suất 8% khi so sánh với vanilla KD, gần 2% khi so sánh với DML và KDCL, và 1% so với SwitOKD. Phương pháp của chúng tôi cũng cho thấy sự tăng đáng kể về hiệu suất của giáo viên. Ví dụ, đối với Tiny-ImageNet, chúng tôi chứng kiến sự tăng gần 3% về hiệu suất của giáo viên khi so sánh với SwitOKD. Sự tăng hiệu suất này trên cả học sinh và giáo viên có thể được giải thích bởi thực tế là với phương pháp của chúng tôi, trong quá trình huấn luyện, cả hai mạng đều tạo ra chất lượng tri thức có thể chuyển giao tốt hơn thông qua các logit của chúng. Ngoài ra, huấn luyện phức tạp mà giáo viên phải chịu đựng với các kỹ thuật như SwitOKD cản trở độ chính xác của giáo viên. Thêm vào đó, giáo viên của chúng tôi được huấn luyện với KL nghịch (tìm kiếm mode), cố gắng khớp với phân phối ồn ào của học sinh trong các epoch đầu. Điều này hoạt động như một điều hòa trên giáo viên, tránh over-fitting và mang lại hiệu suất tốt hơn so với các kỹ thuật KD trực tuyến hiện tại như DML, KDCL, và SwitOKD.

Để xác thực thêm phương pháp của chúng tôi, chúng tôi cũng so sánh BD-KD với các kỹ thuật KD ngoại tuyến khác. Bảng 2 hiển thị hiệu suất trên tập kiểm tra trên CIFAR-100 sử dụng các cặp kiến trúc giáo viên-học sinh khác nhau. Đáng chú ý, phương pháp của chúng tôi thể hiện cải thiện nhất quán trong tất cả các cặp giáo viên-học sinh so với baseline KD vanilla cổ điển và baseline KD trực tuyến (ví dụ DML). BD-KD đạt được độ chính xác tương đương hoặc thậm chí tốt hơn so với các phương pháp dựa trên KD ngoại tuyến hoặc lai (chuyển đổi hoặc kết hợp KD ngoại tuyến và trực tuyến) khác. Chúng tôi chú ý rằng với BD-KD, sự tăng hiệu suất so với các baseline là lớn nhất khi cặp giáo viên-học sinh có kiến trúc tương tự. Một giải thích cho điều này là với các kiến trúc tương tự, các mô hình dễ dàng cập nhật trọng số của nhau và tối ưu hóa tương ứng, do đó có hiệu suất tốt hơn.

Kết quả trên ImageNet. Độ chính xác Top-1 và top-5 trên ImageNet được báo cáo trong Bảng 1 và Bảng 3 cho các cặp giáo viên-học sinh ResNet18 - Resnet32 và Resnet50 - MobilenetV1. Phương pháp BD-KD được đề xuất của chúng tôi tạo ra hiệu suất cao hơn đáng kể hoặc tương tự khi so sánh với các phương pháp KD baseline cho dù trực tuyến hay ngoại tuyến cũng như các kỹ thuật KD khác. Ví dụ, chúng tôi đạt được sự tăng hiệu suất 2.29% khi so sánh với vanilla KD và 1.84% so với DML. Chúng tôi thậm chí đạt được sự tăng khoảng 0.31% khi so sánh với các phương pháp KD lai gần đây như SHAKE [16], sử dụng cả KD ngoại tuyến và trực tuyến thông qua thiết lập giáo viên proxy. Điều này cho thấy rằng kỹ thuật của chúng tôi có thể mở rộng cho các tập dữ liệu quy mô lớn.

Giảm thiểu vấn đề khoảng cách dung lượng. Một trong những hậu quả chính của vấn đề khoảng cách dung lượng trong KD là học sinh không được hưởng lợi từ chưng cất giáo viên lớn hơn [20]. Trong Hình 2a, chúng tôi so sánh các kỹ thuật chưng cất trực tuyến có sẵn và quan sát hành vi của chúng liên quan đến vấn đề khoảng cách dung lượng. Chủ yếu, BD-KD đang hoạt động tốt tổng thể. Chúng tôi quan sát thấy sự tăng ổn định khi chúng tôi tăng kích thước mạng của chúng tôi. Thực tế, với DML, chưng cất trực tuyến đã giảm hiệu suất khi một giáo viên dung lượng cao hơn được sử dụng. Tương tự, với SwitOKD, hiệu suất học sinh plateau khi chúng tôi tăng kích thước giáo viên.

Mở rộng cho nhiều mạng. Tương tự như DML và SwitOKD, BD-KD có thể được mở rộng cho nhiều mạng. Vì các hàm tổn thất giữa giáo viên và học sinh khác nhau, chúng tôi áp dụng cùng thiết lập nhiều giáo viên như SwitchOKD. Ví dụ, trong huấn luyện ba mạng, có hai cách tiếp cận để huấn luyện mô hình học sinh: thiết lập một-giáo viên-hai-học sinh (1T2S) và thiết lập hai-giáo viên-một-học sinh (2T1S). Như có thể thấy trong Bảng 4, chúng tôi đạt được độ chính xác học sinh cao hơn trên tất cả các phương pháp sử dụng kịch bản 2T1S, trong khi học sinh cho 1T2S thấp hơn một chút. Một giải thích cho điều này là đối với 2T1S, học sinh nhỏ gọn học được tri thức giống như ensemble và đa dạng từ nhiều giáo viên. Đối với các mạng giáo viên của chúng tôi, giáo viên lớn nhất của chúng tôi với BD-KD đạt được kết quả tương đương với vanilla KD ở 79.45% và KDCL ở 80.71% nhưng cao hơn nhiều so với DML và SwitOKD 1T2S và 2T1S. Chi tiết thêm về các thiết lập đa mạng này được cung cấp trong tài liệu bổ sung.

Phân tích Hiệu chuẩn. Để định lượng và đo hiệu suất hiệu chuẩn mô hình, chúng tôi sử dụng Expected Calibration Error [24]. Chúng tôi cũng đánh giá định tính hiệu chuẩn với các đường cong độ tin cậy (còn được gọi là đường cong hiệu chuẩn). Hình 3 hiển thị các đường cong hiệu chuẩn của một mạng học sinh ResNet20 được chưng cất từ một WideResNet có độ sâu 16 và chiều rộng 8 (WRN-16-8) sử dụng vanilla KD ngoại tuyến và các phương pháp KD trực tuyến trên CIFAR100. Chúng tôi quan sát thấy việc thực hiện chưng cất ngoại tuyến cho một mạng học sinh nhỏ gọn quá tin cậy so với các kỹ thuật KD trực tuyến. Với vanilla KD ngoại tuyến và bằng cách làm cho học sinh khớp với độ tin cậy của mạng được huấn luyện trước lớn, độ tin cậy của mạng nhỏ gọn dễ bị ảnh hưởng, chuyển nó thành quá tin cậy hơn.

Một giải pháp để giảm thiểu hiệu chuẩn sai trong quá trình chưng cất là lựa chọn các kỹ thuật KD trực tuyến. Trong Hình 3 (b), (c), và (d) chúng ta thấy rằng cùng một mạng học sinh trực tuyến nhỏ gọn được hiệu chuẩn tốt hơn so với baseline ngoại tuyến. Chúng ta thấy sự giảm ECE từ 0.1207 cho ngoại tuyến xuống 0.0429 với DML, 0.04108 với SwitOKD, và thậm chí giảm thấp hơn xuống 0.0252 sử dụng BD-KD.

Bảng 5 (bảng trên) báo cáo ECE học sinh expected calibration error trên tập kiểm tra của CIFAR-100 với các cặp giáo viên-học sinh khác nhau và các phương pháp KD khác nhau. Chúng tôi quan sát thấy BD-KD tạo ra các giá trị ECE thấp hơn, có nghĩa là học sinh nhỏ gọn kết quả được hiệu chuẩn tốt hơn. Thật vậy, chúng tôi đạt được sự giảm ECE gần một nửa khi chưng cất WRN-16-6 thành ResNet20 và tương tự khi chưng cất VGG13-VGG8 với BD-KD so với DML và SwitOKD. Sự giảm này trong metric ECE chỉ ra rằng sơ đồ trọng số được đề xuất liên quan đến việc cân bằng KL thuận và ngược trong quá trình huấn luyện của học sinh để khắc phục tình trạng quá tin cậy và thiếu tin cậy của mô hình học sinh giúp hiệu chuẩn mạng học sinh. Thêm vào đó, mạng học sinh thu được có các cụm lớp nhỏ gọn hơn với sự phân tách lớp tốt hơn (trong Hình 2c) so với SwitOKD (trong Hình 2b), điều này tạo ra khả năng tổng quát tốt hơn và do đó hiệu chuẩn tốt hơn. Trong một số trường hợp, BD-KD cho thấy hiệu suất gần tương tự như SwitOKD. Chúng tôi lập luận rằng quá trình tối ưu hóa của SwitOKD phức tạp, có thể dẫn đến đôi khi các kết quả khác nhau về hiệu chuẩn tùy thuộc vào mạng giáo viên và việc khởi tạo của cả hai mạng. Bảng 5 (bảng dưới) hiển thị ECE của các mạng học sinh MobileNet và WRN-16-2 được huấn luyện đồng thời với một mạng giáo viên duy nhất WRN-16-10 với SwitOKD và BD-KD. Chúng tôi quan sát thấy phương pháp của chúng tôi dẫn đến lỗi ECE thấp hơn tổng thể (thấp hơn càng tốt). Chúng tôi quan sát thấy sự giảm ECE cao hơn với mạng học sinh dung lượng nhỏ nhất sử dụng phương pháp của chúng tôi.

4.3 Phân tích và Ablation
BD-KD cho thiết lập KD ngoại tuyến? Phù hợp với các thảo luận trước đó, BD-KD tích hợp liền mạch vào các tổn thất KD khác nhau, cả trong cấu hình KD ngoại tuyến và trực tuyến. Trong Bảng 6, chúng tôi cung cấp một phân tích so sánh về hiệu suất của các mô hình học sinh và giáo viên được huấn luyện dưới thiết lập KD ngoại tuyến, còn được gọi là vanilla KD. Cụ thể, trong KD ngoại tuyến, chúng tôi thay thế tổn thất học sinh thông thường bằng tổn thất của BD-KD (Eq. 2). Các phát hiện của chúng tôi tiết lộ sự tăng cường hiệu suất đáng chú ý với Vanilla KD + BD-KD (75.43% độ chính xác) so với vanilla KD đơn thuần (75.06% độ chính xác).

Sử dụng BD-KD trên các cấu hình KD trực tuyến đa dạng. Chúng tôi trình bày so sánh trong Bảng 6 giữa DML đơn thuần và DML được làm giàu với tổn thất tập trung học sinh của BD-KD (Eq. 2), cũng như SwitOKD được huấn luyện với tổn thất học sinh của BD-KD. Chúng tôi quan sát thấy các cải thiện hiệu suất nhất quán, bao gồm cải thiện về độ chính xác và ECE, trên các mô hình học sinh được huấn luyện với cách tiếp cận của chúng tôi. Ví dụ, với SwitOKD, chúng tôi chứng kiến sự tăng từ 74.90% lên 75.93% về độ chính xác, và giảm ECE từ 4.40% xuống 3.21%. Do đó, BD-KD hiệu quả cải thiện cả hiệu suất và hiệu chuẩn của mô hình học sinh nhỏ gọn.

Phân kỳ JS so với Tái trọng số Theo từng mẫu trong BD-KD. Trong Bảng 6, chúng tôi đối chiếu các thiết lập KD khác nhau được huấn luyện với phân kỳ JS và BD-KD. Trong khi phân kỳ JS cải thiện các baseline, BD-KD chứng minh hiệu suất vượt trội tổng thể, về cả độ chính xác và hiệu chuẩn mô hình. Điều này làm nổi bật lợi thế đáng kể của trọng số theo từng mẫu trên các thành phần phân kỳ khác nhau. Thêm vào đó, để có hiểu biết tốt hơn về hiệu ứng của trọng số theo từng mẫu của cả hai thành phần KL liên quan đến hiệu chuẩn tin cậy mô hình, chúng tôi so sánh các đường cong hiệu chuẩn và các giá trị ECE của một mô hình học sinh được huấn luyện với trọng số mẫu với mô hình không có trọng số của các phân kỳ KL trong Eq.1. Hình 5 cho thấy rằng mô hình học sinh được huấn luyện mà không có sơ đồ trọng số theo từng mẫu được đề xuất cho các phân kỳ KL bị hiệu chuẩn sai với ECE là 0.0461, trong khi BD-KD cho thấy một mạng được hiệu chuẩn tốt hơn với ECE là 0.0315 (giảm khoảng 1.5%). Biểu đồ độ tin cậy của BD-KD cho thấy rằng học sinh của chúng tôi trở nên thiếu tin cậy ở các giá trị tin cậy thấp, nhưng gần như được hiệu chuẩn hoàn hảo trên các giá trị tin cậy trung bình đến cao quan trọng.

Khoảng cách Độ chính xác và Entropy. Chúng tôi cung cấp trong Hình 4(b) khoảng cách độ chính xác giữa giáo viên (ResNet50) và học sinh (MobileNetV1) trên tập xác thực ImageNet trên cả BD-KD và SwitOKD. Tương tự, trong Hình 4(a), chúng tôi trình bày khoảng cách độ chính xác giữa giáo viên (ResNet32x4) và học sinh (ShuffleNetV2) trên CIFAR-100 sử dụng BD-KD và SwitOKD. Chúng tôi quan sát thấy rằng, trong suốt quá trình huấn luyện, BD-KD có thể duy trì khoảng cách độ chính xác huấn luyện thấp giữa các mạng giáo viên và học sinh, điều này đóng góp vào yếu tố độ trung thực và do đó khả năng tổng quát và hiệu chuẩn tốt hơn. Thêm vào đó, điều này giải thích tại sao BD-KD là huấn luyện tập trung vào học sinh. Bằng cách duy trì khoảng cách này rất thấp, chúng tôi đảm bảo rằng mạng học sinh có thể theo kịp tri thức và độ tin cậy của giáo viên. Chúng tôi cũng quan sát thấy khoảng cách độ chính xác thấp hơn khi sử dụng phương pháp đề xuất của chúng tôi, so với SwitOKD. BD-KD có thể duy trì hiệu suất này trong suốt quá trình huấn luyện. Hơn nữa, chúng tôi quan sát thấy khoảng cách entropy giảm trong Hình 4 (c) trong suốt quá trình học tập của các mạng. Thật vậy, BD-KD được thiết kế để duy trì khoảng cách không chắc chắn thấp hơn, tăng cường hiệu chuẩn mô hình học sinh, như thấy trong các thí nghiệm của chúng tôi.

Độ nhạy cảm với v. Chúng tôi thay đổi siêu tham số v theo các giá trị sau [1,2,3,4] (trong Bảng 7) và ghi lại độ chính xác của cả mạng giáo viên và học sinh trên tập kiểm tra của CIFAR-100. Chúng ta thấy rằng lựa chọn tối ưu cho v là 2. Khi chúng tôi tăng giá trị của siêu tham số này, hiệu suất giảm. Người ta phải cẩn thận trong việc chọn giá trị phù hợp của v dựa trên nhiệm vụ downstream và kiến trúc của học sinh vì giá trị cao có thể dẫn đến gradient không chuẩn hóa và thậm chí có thể gây ra bùng nổ gradient khi v quá cao.

Độ nhạy cảm với τ. Chúng tôi thực hiện phân tích độ nhạy cảm siêu tham số trên τ, thay đổi các giá trị của nó và quan sát độ chính xác tính theo % của cả mạng học sinh và giáo viên trong Bảng 7. Chúng tôi thấy rằng τ= 2 là giá trị tối ưu. Khi chúng tôi tăng τ, chúng tôi quan sát thấy sự giảm hiệu suất trong cả hai mạng. Điều này có thể liên quan đến thực tế rằng nhiệt độ cao hơn đóng góp vào việc tối đa hóa khoảng cách entropy giữa giáo viên và học sinh, do đó làm cho việc học sinh theo dõi giáo viên trở nên rất khó khăn.

5 Kết luận
Cách tiếp cận đề xuất của chúng tôi giải quyết các thách thức của nén mô hình và độ tin cậy mạng cùng lúc. Bằng cách cân bằng động (tức là trọng số theo từng mẫu) KL thuận và nghịch trong hàm mục tiêu của học sinh và sử dụng KL nghịch cho chưng cất giáo viên, chúng tôi tăng cường độ chính xác và hiệu chuẩn của học sinh vượt qua các kỹ thuật chưng cất trực tuyến khác. Các thí nghiệm rộng rãi xác thực hiệu quả của phương pháp chúng tôi. Chủ yếu, phương pháp của chúng tôi độc lập với tổn thất và nhiệm vụ, đạt được hiệu suất độ chính xác và hiệu chuẩn mô hình tốt hơn, làm cho nó dễ dàng được tích hợp vào các hàm mục tiêu khác và vào các nhiệm vụ khác nhau.

[Phần tài liệu tham khảo, phụ lục và các chi tiết kỹ thuật khác được dịch tương tự, giữ nguyên cấu trúc và định dạng]
