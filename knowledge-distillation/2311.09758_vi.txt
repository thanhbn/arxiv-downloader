# 2311.09758.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2311.09758.pdf
# Kích thước tệp: 878319 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
OrchestraLLM: Điều phối hiệu quả các mô hình ngôn ngữ cho theo dõi trạng thái đối thoại
Chia-Hsuan Lee
Đại học Washington
chiahlee@uw.eduHao Cheng
Microsoft Research
chehao@microsoft.comMari Ostendorf
Đại học Washington
ostendor@uw.edu
Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) đã cách mạng hóa bối cảnh Xử lý Ngôn ngữ Tự nhiên, nhưng tốn kém về mặt tính toán.
Để giảm chi phí mà không hy sinh hiệu suất, các nghiên cứu trước đó đã khám phá nhiều phương pháp khác nhau để khai thác tiềm năng của các Mô hình Ngôn ngữ Nhỏ hơn (SLMs) như những lựa chọn thay thế hiệu quả về chi phí cho các đối tác lớn hơn của chúng. Được thúc đẩy bởi những phát hiện rằng SLMs và LLMs thể hiện những điểm mạnh bổ sung trong một nhiệm vụ trích xuất kiến thức có cấu trúc, công trình này trình bày một khung định tuyến SLM/LLM mới được thiết kế để cải thiện hiệu quả tính toán và nâng cao hiệu suất nhiệm vụ. Trong các nhiệm vụ theo dõi trạng thái đối thoại, khung định tuyến được đề xuất nâng cao hiệu suất đáng kể so với việc chỉ dựa vào LLMs, đồng thời giảm chi phí tính toán hơn 50%.
1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) đã trở thành các công cụ đa năng có khả năng giải quyết một loạt các nhiệm vụ chỉ với một vài ví dụ huấn luyện. Tuy nhiên, kích thước ngày càng mở rộng của chúng đã mang lại những yêu cầu tính toán leo thang. Ngược lại, các Mô hình Ngôn ngữ Nhỏ hơn (SLMs) hiệu quả hơn thường yêu cầu một lượng lớn dữ liệu tinh chỉnh để trở nên thực sự hiệu quả. Công trình này giải quyết các tình huống khi chỉ có dữ liệu cụ thể cho nhiệm vụ hạn chế có sẵn, làm cho các SLMs được tinh chỉnh kém đáng tin cậy hơn.
Mục tiêu của chúng tôi là phát triển một khung định tuyến điều phối SLMs và LLMs, nâng cao hiệu suất nhiệm vụ đồng thời giảm chi phí tính toán.
Đối thoại hướng nhiệm vụ là quan trọng đối với tương tác hiệu quả giữa con người và máy tính, cho phép các hệ thống hiểu và hỗ trợ với các nhiệm vụ cụ thể như đặt vé máy bay hoặc lên lịch họp. Các đối thoại hướng nhiệm vụ liên quan đến dữ liệu có cấu trúc thường dựa vào Theo dõi Trạng thái Đối thoại (DST), trong đó ý định của người dùng được trích xuất từ lịch sử đối thoại giữa người dùng và tác nhân dưới dạng các giá trị slot được liên kết với một lược đồ được định trước. Các SLMs được tinh chỉnh đã được sử dụng trong DST trong vài năm, bao gồm cả các LMs tự hồi quy (Ham et al., 2020; Hosseini-Asl et al., 2020; Peng et al., 2020) và các LMs chuỗi-sang-chuỗi (Lee et al., 2021; Su et al., 2022; Bang et al., 2023; Imrattanatrai and Fukuda, 2023; Wang et al., 2023). LLMs đã được sử dụng cho học trong ngữ cảnh với ít mẫu trong DST (Xie et al., 2022; Hudecek and Dušek, 2023; Hu et al., 2022; King and Flanigan, 2023a) nơi LLMs được nhắc với các mô tả nhiệm vụ do con người viết hoặc các ví dụ trong ngữ cảnh. Trong công việc của chúng tôi, chúng tôi tìm cách tận dụng hiệu quả của LLMs với một lượng nhỏ dữ liệu huấn luyện nhưng giảm chi phí.
Các chiến lược tận dụng cả SLMs và LLMs đã được phát triển để giảm thiểu những yêu cầu tính toán của LLMs. Các phương pháp dựa trên tầng nhảy hướng một truy vấn đến LLM khi nó không thể được giải quyết bởi SLM (Chen et al., 2023; Madaan et al., 2023). Các phương pháp này gây ra độ trễ và dư thừa tính toán vì chúng liên tục truy vấn SLMs. Các phương pháp khác sử dụng các bộ phân loại nhị phân để dự đoán LM phù hợp nhất để sử dụng (Kag et al., 2022; Šakota et al., 2023). Một hạn chế của các phương pháp dựa trên bộ phân loại là sự cần thiết phải huấn luyện lại khi giới thiệu các mô hình mới.
Trong công việc này, chúng tôi đề xuất một khung định tuyến động, OrchestraLLM (được minh họa trong Hình 1), tận dụng các chuyên gia LM nhỏ (được tinh chỉnh) và lớn. Giả thuyết rằng các ví dụ có các embedding ngữ nghĩa tương tự có cùng mức độ khó khăn, chúng tôi chọn một chuyên gia phù hợp dựa trên khoảng cách embedding giữa instance kiểm tra và các instance trong các pool chuyên gia. Các pool chuyên gia chứa các ví dụ đại diện cho các loại ngữ cảnh đối thoại nơi các LMs khác nhau cung cấp các câu trả lời đáng tin cậy hơn. Sau khi truy xuất k ví dụ gần nhất hàng đầu, một chuyên gia được chọn dựa trên đa số phiếu. Không giống như các phương pháp dựa trên tầng nhảy và dựa trên bộ phân loại, khung được đề xuất loại bỏ nhu cầu huấn luyện router, mặc dù dữ liệu được gán nhãn thủ công cần thiết để tạo các pool chuyên gia.
Ngoài ra, retriever có thể được tinh chỉnh với nhãn nhiệm vụ đích hoặc thông tin chuyên gia để đạt được định tuyến hiệu quả và chính xác hơn.
Tóm lại, đóng góp chính của công việc này là việc giới thiệu một mô hình chuyển đổi mới được thiết kế để giảm chi phí tính toán liên quan đến LLMs đồng thời nâng cao hiệu suất. Kết quả thử nghiệm trên hai benchmark DST đa miền khác nhau (MultiWOZ (Budzianowski et al., 2018; Ye et al., 2022) và SGD (Rastogi et al., 2020)) chứng minh rằng OrchestraLLM tận dụng các năng lực của các chuyên gia khác nhau, vượt trội hơn các hệ thống LLM đồng thời cũng đạt được việc giảm đáng kể hơn 50% chi phí tính toán.
2 Theo dõi Trạng thái Đối thoại
Trong công việc này, chúng tôi tập trung vào việc kết hợp LLMs đa năng và SLMs cụ thể cho nhiệm vụ để đạt được hiệu quả tốt hơn cho theo dõi trạng thái đối thoại (DST).
Trong phần sau, chúng tôi trước tiên cung cấp các thiết lập nhiệm vụ cần thiết và sau đó chi tiết hai mô hình DST đại diện sử dụng LLMs và SLMs tương ứng.
Một đối thoại hướng nhiệm vụ (TOD) bao gồm một chuỗi các cuộc trao đổi giữa hai bên, mỗi bên được khởi tạo bởi người dùng và được theo sau bởi phản hồi từ hệ thống. Ở đây, chúng tôi ký hiệu mỗi cuộc trao đổi như một lượt dẫn đến một chuỗi,
U1, A1, ..., UT, AT, trong đó Ut và At đại diện cho phát ngôn của người dùng và phản hồi của hệ thống, tương ứng. Đối với lượt thứ t, người dùng cung cấp một phát ngôn mới Ut, và tác nhân hệ thống phản hồi với phát ngôn At. Tại lượt t, ngữ cảnh đối thoại tương ứng là Ct={U1, A1, . . . , At−1, Ut}, loại trừ phản hồi hệ thống mới nhất At. Mục tiêu của DST là trích xuất thông tin liên quan đến nhiệm vụ như các biểu diễn có cấu trúc (trạng thái đối thoại) từ các phát ngôn người dùng-hệ thống để các yêu cầu của người dùng có thể được thực hiện tương ứng. Để tạo thuận lợi cho điều này, thường có một lược đồ cụ thể cho nhiệm vụ được cung cấp. Trong một tình huống đa miền được xem xét trong bài báo này, lược đồ chứa M miền D={d1, . . . , dM} và N slots S={s1, . . . , sN} để theo dõi. DSTt, trạng thái đối thoại tại lượt t, định nghĩa các ánh xạ hiện tại từ các cặp (dm,sn) thành một giá trị v dựa trên ngữ cảnh đối thoại Ct. Cụ thể,
DSTt={(dm, sn, vtmn)|vtij≠null},
chỉ chứa các slots không null được tích lũy cho đến nay. Thay vì trực tiếp dự đoán toàn bộ trạng thái đối thoại từ đầu, chúng tôi xây dựng các dự đoán trạng thái đối thoại dựa trên niềm tin cấp lượt (TLB) như Hu et al. (2022) đã làm, cho phép kết hợp linh hoạt hơn của LLMs và SLMs. Tại lượt t, mô hình DST chỉ dự đoán TLBt, trong đó các slots được biểu đạt mới hoặc các slots với giá trị được cập nhật được sử dụng để có được DSTt mới nhất thông qua việc tổng hợp tất cả các TLBs trước đó.1
Trong tài liệu, các mô hình DST dựa trên SLM cụ thể cho nhiệm vụ thường được tinh chỉnh với các cập nhật tham số đầy đủ trong khi các mô hình DST sử dụng LLMs được thực hiện thông qua học trong ngữ cảnh với ít mẫu. Chúng tôi thảo luận về hai mô hình DST khác nhau được xem xét dưới đây.
LLM DST. IC-DST (Hu et al., 2022) là một khung học trong ngữ cảnh (ICL) cho phép DST với ít mẫu với LLMs. Dự đoán là sự thay đổi trong mỗi cặp lượt thay vì các trạng thái đối thoại được tích lũy. Để có được các trạng thái đối thoại được tích lũy, các thay đổi lượt được tổng hợp qua các lượt.
Các trạng thái đối thoại của các lượt trước được sử dụng như một tóm tắt của lịch sử ngữ cảnh và nó cho phép phù hợp với nhiều mẫu hơn điều quan trọng đối với hiệu suất ICL. Cụ thể, cho bảng lược đồ, K mẫu trong ngữ cảnh, trạng thái đối thoại của lượt trước, và instance đầu vào (cặp phát ngôn tác nhân-người dùng gần đây nhất), LLM xuất ra
TLBt=LLM(T, E1:K, DSTt−1, At−1, Ut)(1)
trong đó T là bảng lược đồ cho tất cả các miền, Ek là các ví dụ của các cặp thay đổi lượt và các đầu ra liên quan.
SLM DST. Ở đây, chúng tôi phát triển một mô hình DST dựa trên prompt (ký hiệu là Prompt-DST) với SLM (T5 (Raffel et al., 2020)). Đầu vào của Prompt-DST tương tự như IC-DST, ngoại trừ việc các mẫu trong ngữ cảnh bị loại trừ. Cụ thể, cho đầu vào được tăng cường prompt lược đồ, mô hình xuất ra
TLBt=SLM(T, DSTt−1, At−1, Ut).(2)
Ở đây, mô hình được huấn luyện sử dụng mục tiêu học bằng cách tối đa hóa log-likelihood của các giá trị slot vt(dm, sn) cho TLB hiện tại, tức là,
max log P(TLBt|T, DSTt−1, At−1, Ut).(3)
Trong quá trình suy luận, một thủ tục giải mã tham lam được áp dụng trực tiếp, tức là, chỉ token có khả năng nhất trong từ vựng mô hình đã cho được dự đoán tại mỗi bước giải mã.
3 Phương pháp Định tuyến
Ở đây, chúng tôi trình bày phương pháp của chúng tôi cho định tuyến với OrchestraLLM được áp dụng cho nhiệm vụ DST. Khung tổng thể được minh họa trong Hình 1. Chúng tôi ký hiệu các mô hình DST khác nhau là các chuyên gia. Cho một instance đầu vào mới (bộ ba (DSTt−1, At−1, Ut)), OrchestraLLM trước tiên tính toán embedding ngữ nghĩa của nó, so sánh nó với các embedding mẫu của các bộ ba từ mỗi pool chuyên gia sử dụng khoảng cách cosine, và truy xuất k mẫu hàng đầu. Router gán đầu vào cho một chuyên gia dựa trên đa số phiếu. Trong khi phương pháp của chúng tôi được lấy cảm hứng từ công việc của Jang et al. (2023), điều quan trọng cần lưu ý là phương pháp của họ chủ yếu tập trung vào tối ưu hóa hiệu suất nhiệm vụ trong các tình huống chuyển giao nhiệm vụ zero-shot, trong khi sự nhấn mạnh của chúng tôi nằm ở việc cải thiện hiệu quả tính toán trong các thiết lập học với ít mẫu.
3.1 Xây dựng Pool Chuyên gia
Đối với mỗi đối thoại trong một tập nhỏ được giữ lại, các chuyên gia SLM và LLM được sử dụng để dự đoán TLB tại mỗi lượt người dùng (TLBt) một cách riêng lẻ. Nếu cả hai chuyên gia đều dự đoán chính xác TLB, bộ ba instance được bao gồm trong pool SLM. Khi chỉ một chuyên gia dự đoán chính xác TLB, instance được gán cho pool của chuyên gia đó. Các instance không được dự đoán chính xác không được sử dụng trong bất kỳ pool nào.
3.2 Học Biểu diễn Bộ ba
Tương tự như công việc gần đây về truy xuất dày đặc (Karpukhin et al., 2020), retriever sử dụng kiến trúc bi-encoder, mã hóa các đối thoại với nhãn và dự đoán vào không gian embedding. Xuyên suốt công việc, SenBERT (Reimers and Gurevych, 2019) được sử dụng như mô hình embedding xương sống. Bi-encoder được tinh chỉnh sử dụng một tập nhỏ các đối thoại, tương tự như tập được sử dụng để xây dựng các pool chuyên gia. Chúng tôi sử dụng một mất mát tương phản sao cho độ tương tự giữa một cặp ví dụ tích cực cao và độ tương tự giữa một cặp ví dụ tiêu cực thấp. Ba phương pháp khác nhau để xây dựng các cặp tích cực và tiêu cực được khám phá: nhận biết nhiệm vụ, nhận biết chuyên gia, và kết hợp của chúng.
Giám sát Nhận biết Nhiệm vụ xác định các cặp instance tích cực và tiêu cực cho huấn luyện bằng cách trước tiên tính toán độ tương tự theo cặp cho mỗi mẫu trong tập giữ lại. Sau đó, l cặp điểm cao nhất và thấp nhất được sử dụng làm ví dụ tích cực và tiêu cực, tương ứng. Hàm độ tương tự tận dụng các chú thích vàng của các đối thoại tập giữ lại. Cho hai instance, a và b, độ tương tự là một kết hợp có trọng số của độ tương tự slot-value của trạng thái trước (DST) và TLB hiện tại:
½SimDST+SimTLB.
Cho TLBx={(sx1, vx1),···,(sxm, vxm)} là TLB của instance x. Theo Hu et al. (2022), độ tương tự cặp slot-value là
Fslot-value=F({(sa1, va1),···,(sam, vam)},
{(sb1, vb1),···,(sbn, vbn)}).
và độ tương tự slot là
Fslot=F({sa1,···, sam},{sb1,···, sbn}).
trong đó F là định nghĩa tiêu chuẩn của điểm F1 tức là,
F=2PR/(P+R), trong đó P là precision, và R là recall.
Điểm độ tương tự giữa TLBa và TLBb là
Sim(TLBa, TLBb)=Fslot-value+Fslot−1
Độ tương tự lịch sử ngữ cảnh SimDST được định nghĩa theo cách tương tự.
Giám sát Nhận biết Chuyên gia trước tiên nhóm các instance trong tập giữ lại theo chuyên gia nào đưa ra dự đoán chính xác nhất. (Đối với hòa, SLM được chọn.) Sau đó chúng tôi tính toán độ tương tự bộ ba theo cặp sử dụng một embedder có sẵn (ví dụ, SenBERT). L cặp điểm cao nhất với cùng nhãn chuyên gia là ví dụ tích cực, và l cặp điểm thấp nhất với nhãn chuyên gia khác nhau là ví dụ tiêu cực.
Giám sát Nhận biết Nhiệm vụ+Chuyên gia đơn giản là gộp cả hai tập các cặp tích cực và tiêu cực.
Lưu ý rằng giám sát nhận biết nhiệm vụ không phụ thuộc vào các chuyên gia nào được sử dụng trong định tuyến, vì vậy mô hình embedding không cần được huấn luyện lại khi các chuyên gia được thêm hoặc cập nhật. Giám sát nhận biết chuyên gia sẽ yêu cầu cập nhật mô hình embedding nếu các chuyên gia thay đổi. Trong tất cả các trường hợp, các pool chuyên gia sẽ cần được cập nhật với các thay đổi đối với các chuyên gia.
4 Thử nghiệm
4.1 Tập dữ liệu
Chúng tôi sử dụng hai tập dữ liệu được chi tiết dưới đây cho các thử nghiệm.
Một tóm tắt của các tập dữ liệu DST được báo cáo trong Bảng 1.
MultiWOZ (Budzianowski et al., 2018) là một tập dữ liệu đối thoại hướng nhiệm vụ đa miền chứa hơn 10K đối thoại được viết bởi con người-con người qua 8 miền và đã là một trong những benchmark phổ biến nhất trong tài liệu DST. Sau khi xuất bản của Budzianowski et al. (2018), nhiều công việc cải thiện chất lượng nhãn, ví dụ, MultiWOZ 2.1 (Eric et al., 2020) và MultiWOZ 2.4 (Ye et al., 2021). Chúng tôi thử nghiệm sử dụng phiên bản gần đây nhất, MultiWOZ 2.4.
SGD (Rastogi et al., 2020) là một tập dữ liệu đối thoại hướng nhiệm vụ chứa hơn 16k cuộc hội thoại đa miền kéo dài 41 miền, có đặc trưng đánh giá ngoài miền. 15 trên 21 miền trong tập kiểm tra không có mặt trong tập huấn luyện và 77% các lượt đối thoại trong tập kiểm tra chứa ít nhất một miền không có mặt trong tập huấn luyện.
4.2 Thiết lập Thử nghiệm
Trong công việc này, chúng tôi xem xét một thiết lập với ít mẫu cho DST. Theo thiết lập thử nghiệm đa miền từ Wu et al. (2020), chúng tôi ngẫu nhiên lấy mẫu 5% dữ liệu huấn luyện từ MultiWOZ và SGD tương ứng để huấn luyện các mô hình chuyên gia.
Thiết lập Mô hình và Siêu tham số. Đối với Prompt-DST, chúng tôi sử dụng T5-base và T5-large làm mô hình xương sống cho MWOZ và SGD tương ứng, vì cái sau phức tạp hơn về mặt lược đồ và nhiều lượt đối thoại hơn. Đối với IC-DST, chúng tôi sử dụng ChatGPT làm mô hình xương sống2 với 10 mẫu trong ngữ cảnh. Chúng tôi khởi tạo retriever định tuyến từ SenBERT (all-mpnet-base-v2). Chúng tôi chạy suy luận trên 100 đối thoại được lấy mẫu ngẫu nhiên từ các tập validation của MWOZ và SGD làm các tập giữ lại.
Cùng 100 đối thoại được sử dụng để huấn luyện retriever. Đối với tất cả các thử nghiệm, l = 25 được sử dụng cho các ví dụ tích cực và tiêu cực cho học tương phản. Trong quá trình suy luận, chúng tôi ngẫu nhiên lấy mẫu 100 lượt từ các tập giữ lại để phục vụ như pool SLM và pool LLM tương ứng cho các thử nghiệm MWOZ và 300 lượt cho các thử nghiệm SGD.3 Chúng tôi để dành cho công việc tương lai việc tạo ra các chiến lược mới cho việc chọn instance pool chuyên gia. Chúng tôi sử dụng k = 10 cho đa số phiếu và phá vỡ hòa bằng cách ưu tiên SLM.
4.3 Đánh giá
4.3.1 Độ chính xác
Theo quy ước, các hệ thống DST được đánh giá bằng độ chính xác mục tiêu chung (JGA) trên các trạng thái đối thoại được tích lũy (Henderson et al., 2014). Metric này đánh giá tính đúng đắn của trạng thái đối thoại tại mỗi lượt và xem nó là chính xác chỉ khi tất cả các giá trị slot trong mỗi miền khớp chính xác với các giá trị sự thật cơ bản. Việc đánh giá chính xác hệ thống hoạt động tốt như thế nào trên các lượt đơn lẻ với DST JGA là khó khăn. Do đó chúng tôi cũng báo cáo niềm tin cấp lượt (TLB JGA) (Dey et al., 2022).
4.3.2 Hiệu quả
Các phép toán dấu phẩy động mỗi Giây (FLOPs) đại diện cho số lượng phép toán số học dấu phẩy động (phép cộng và phép nhân) mà một mô hình thực hiện trong một lần chạy. FLOPs thường được sử dụng để ước tính chi phí tính toán hoặc khối lượng công việc cần thiết cho huấn luyện hoặc suy luận. Huấn luyện một mô hình lớn yêu cầu một số lượng đáng kể các lần chạy ngược, đắt hơn các lần chạy xuôi, tuy nhiên suy luận là một quá trình liên tục xảy ra bất cứ khi nào mô hình được sử dụng, do đó tích lũy nhiều chi phí hơn theo thời gian. NVIDIA (Leopold, 2019) và Amazon (Barr, 2019) báo cáo khoảng 90% khối lượng công việc ML là xử lý suy luận trong các dịch vụ đám mây của họ. Do đó, chúng tôi chọn báo cáo FLOPs cho việc sử dụng thời gian suy luận.
Chúng tôi ước tính chi phí tính toán tổng hợp, được đo bằng TeraFLOPs, cần thiết để thực hiện suy luận trên toàn bộ tập dữ liệu kiểm tra. Điều quan trọng cần lưu ý là IC-DST dựa vào ChatGPT, một mô hình không thể truy cập công khai, do đó không cho phép đánh giá trực tiếp hiệu quả tính toán của nó. Dựa trên phỏng đoán phổ biến trong lĩnh vực công cộng, ChatGPT được cho là phiên bản được tinh chỉnh của mô hình GPT-3 với số lượng tham số đáng kể là 175 tỷ (Brown et al., 2020). Để ước tính các yêu cầu tính toán, chúng tôi tiến hành đo lường FLOPs trên mô hình GPT-2 (Radford et al.) và sau đó mở rộng các đo lường này theo sự khác biệt kích thước tham số giữa GPT-2 và ChatGPT. Chi phí tính toán của retriever, được đo bằng FLOPs, cho mỗi instance lượt, là khoảng 0.02 TeraFLOPs. Tải tính toán này trở nên không đáng kể khi được xem xét cùng với ChatGPT trong OrchestraLLM. ChatGPT yêu cầu khoảng 3000 TeraFLOPs cho mỗi instance lượt.
4.4 Baseline
Định tuyến Dựa trên Phân loại
Chúng tôi so sánh khung định tuyến của chúng tôi với các phương pháp dựa trên phân loại hiện có để chuyển đổi mô hình, như những phương pháp được đề xuất bởi Šakota et al. (2023) và Kag et al. (2022). Các phương pháp hiện có này thường huấn luyện một bộ phân loại nhị phân để phục vụ như router. Chúng tôi huấn luyện BERT (Devlin et al., 2019) (bert-base-cased) với các nhãn chuyên gia trong tập giữ lại của các đối thoại với một mục tiêu phân loại nhị phân để thực hiện định tuyến như một baseline. Định tuyến Dựa trên Tầng nhảy
Các phương pháp dựa trên tầng nhảy Chen et al. (2023); Madaan et al. (2023) thường truy vấn một SLM và chuyển hướng instance đến LLM nếu mô hình ngôn ngữ nhỏ hơn không đủ tự tin. Chúng tôi chọn sử dụng xác suất mức chuỗi được chuẩn hóa của đầu ra SLM làm thước đo tin cậy. Chúng tôi điều chỉnh ngưỡng xác suất trên tập giữ lại và sử dụng ngưỡng để xác định có nên chuyển hướng instance đến LLM trong quá trình suy luận.
4.5 Kết quả
4.5.1 MultiWOZ
Chúng tôi chứng minh các thử nghiệm MultiWOZ trong một thiết lập với ít mẫu trong Bảng 2. Chúng tôi sử dụng 5% đối thoại trong tập huấn luyện để tinh chỉnh Prompt-DST và retriever của IC-DST. Prompt-DST và IC-DST thực hiện suy luận trên 100 đối thoại khác từ tập validation để phục vụ như pool chuyên gia. Chúng tôi ngẫu nhiên chọn 100 lượt từ mỗi chuyên gia để phục vụ như pool chuyên gia cho định tuyến động.
Như mong đợi, IC-DST vượt trội hơn Prompt-DST trong thiết lập với ít mẫu, cho thấy LLM có khả năng tổng quát hóa hơn SLM được tinh chỉnh.
Router phân loại dựa trên BERT gặp khó khăn trong việc khai thác hiệu quả khả năng của cả hai mô hình. Để thiết lập một giới hạn hiệu suất trên cho router được học, chúng tôi giới thiệu router oracle, tổng hợp dự đoán từ cả LLM và SLM khi một trong hai mô hình đúng, với sự ưu tiên cho SLM khi có sẵn. Ngay cả với SenBERT vanilla làm retriever, OrchestraLLM vượt trội hơn IC-DST đồng thời tiết kiệm 60% lệnh gọi đến LLM, chứng minh hiệu quả của khung được đề xuất của chúng tôi. Tinh chỉnh thêm retriever với các ví dụ tương phản nhận biết nhiệm vụ được đề xuất định tuyến các ví dụ hiệu quả hơn và cải thiện DST JGA khoảng 3% so với IC-DST. Với huấn luyện nhận biết chuyên gia bổ sung của retriever, chúng tôi có thể tiết kiệm thêm khoảng 7% lưu lượng đến LLM với hiệu suất vượt trội so với IC-DST. Mặc dù kích thước nhỏ gọn, Prompt-DST được tinh chỉnh để phù hợp với kiến thức cụ thể trong miền và các artifact cụ thể cho nhiệm vụ (ví dụ, ràng buộc lược đồ và chiến lược gán nhãn tùy chỉnh). Ngược lại, IC-DST được làm giàu với một kho tàng rộng lớn kiến thức thu được trong giai đoạn pretraining của LLM, trao cho nó khả năng lý luận ngữ cảnh và nắm bắt kiến thức thường thức tăng cường (Phần 5.2). Vì hai mô hình này bổ sung cho nhau, một tích hợp hiệu quả có thể vượt qua hiệu suất của mô hình IC-DST.
6 Kết quả trên SGD
Để đánh giá hệ thống của chúng tôi trong các tình huống ngoài miền, chúng tôi hiển thị kết quả thử nghiệm trong một thiết lập với ít mẫu trên SGD trong Bảng 3. Chúng tôi sử dụng 5% đối thoại trong tập huấn luyện để tinh chỉnh Prompt-DST và retriever của IC-DST. Prompt-DST và IC-DST thực hiện suy luận trên 100 đối thoại khác trong tập validation để phục vụ như pool chuyên gia. Chúng tôi ngẫu nhiên chọn 300 lượt từ mỗi chuyên gia để phục vụ như pool chuyên gia cho định tuyến động.
Như chúng tôi quan sát trong MultiWOZ, việc kết hợp SenBERT có sẵn làm router cải thiện điểm TLB và cũng tiết kiệm khoảng 50% tính toán. Tinh chỉnh SenBERT với mục tiêu nhận biết nhiệm vụ cải thiện hiệu quả 5% và tăng cả điểm TLB và DST. Với giám sát nhận biết chuyên gia bổ sung, nhiều lượt được định tuyến đến SLM và cải thiện điểm TLB. Thiết lập này vượt trội hơn IC-DST hơn 4% TLB JGA và tiết kiệm 57% FLOPs, chứng minh rằng router của chúng tôi đủ phổ quát để hỗ trợ gán chéo miền và thành công cải thiện độ chính xác hệ thống.
5 Phân tích
5.1 Tổng quát hóa Chéo Miền
Ngoài Miền (OOD) trong SGD Để đánh giá hiệu quả của OrchestraLLM trong việc tổng quát hóa đến các miền chưa thấy, chúng tôi trình bày kết quả phân tích trên SGD trong Hình 2. Đầu tiên, chúng tôi quan sát rằng Prompt-DST hoạt động tốt hơn IC-DST trên các đối thoại trong miền nhưng thua kém IC-DST trên tất cả các loại đối thoại khác. Điều này cho thấy khả năng tổng quát hóa của các Mô hình Ngôn ngữ Lớn (LLMs) vượt trội so với các Mô hình Ngôn ngữ Nhỏ hơn (SLMs). Tất cả các biến thể của OrchestraLLM vượt trội hơn IC-DST trong các tình huống OOD, chứng minh khả năng của router trong việc điều phối hiệu quả các instance ngay cả khi chúng nằm ngoài miền.
Retriever Chéo Tập dữ liệu Chúng tôi tiếp tục đánh giá khung được đề xuất của chúng tôi trong một tình huống thách thức hơn nơi router và các mô hình xương sống được huấn luyện trong các tập dữ liệu khác nhau. Chúng tôi huấn luyện mô hình retriever trên các đối thoại tập giữ lại MWOZ và đánh giá khung trên các đối thoại kiểm tra SGD và ngược lại. Kết quả được hiển thị trong Bảng 4. Đáng chú ý, khung định tuyến của chúng tôi vẫn có thể điều phối hiệu quả hai LLMs với một retriever được huấn luyện với một tập dữ liệu khác nhau và vượt trội hơn IC-DST đồng thời cũng đạt được tiết kiệm chi phí tính toán khoảng 54% trên MultiWOZ và 43% trên SGD.
So với tinh chỉnh khớp tập dữ liệu, tinh chỉnh không khớp với một tập dữ liệu khác nhau chỉ làm tổn hại nhẹ độ chính xác và hiệu quả.
5.2 Chuyên môn của SLM và LLM
Để hiểu rõ hơn bản chất bổ sung của các LMs, chúng tôi kiểm tra các ví dụ để xác định chuyên môn của chúng. Chúng tôi cung cấp các ví dụ đại diện từ các pool chuyên gia trong Bảng 5. Một lỗi phổ biến được LLM mắc phải là không tuân thủ lược đồ. Trong ví dụ này, LLM đơn giản sao chép văn bản ("affordable") từ lượt làm dự đoán DST, trong khi SLM có khả năng gắn kết giá trị trong định dạng cụ thể lược đồ ("cheap"). Tuy nhiên, chúng tôi xác định hai điểm mạnh mà LLM sở hữu so với SLM. Thứ nhất, nó xuất sắc trong việc xử lý kiến thức thường thức, ví dụ, nó có thể suy ra số lượng khách đúng ở nhà khách từ ngữ cảnh ("me and my mum"). Thứ hai, nó thể hiện sự thành thạo trong lý luận ngữ cảnh dài. Khi có tham chiếu đến ngữ cảnh trước đó qua các miền, LLM liên tục đưa ra suy luận chính xác, trong khi SLM thường bỏ qua ngữ cảnh và tạo ra các giá trị ngẫu nhiên.
5.3 Định tuyến LM Mới
Để chứng minh tính linh hoạt của OrchestraLLM, chúng tôi cung cấp kết quả định tuyến khi tích hợp một LM mới, cụ thể là T5-3b được tinh chỉnh với phương pháp Prompt-DST trên dữ liệu huấn luyện MultiWOZ 5%. Chúng tôi áp dụng OrchestraLLM với SenBERT có sẵn để định tuyến giữa T5-base và T5-3b. Kết quả được hiển thị trong Bảng 6 nhấn mạnh khả năng thích ứng của OrchestraLLM trong việc định tuyến hiệu quả các ví dụ với một LM mới được giới thiệu, tất cả mà không yêu cầu bất kỳ huấn luyện bổ sung nào của retriever.
6 Công việc Liên quan
6.1 Suy luận Thích ứng Mẫu
Để phân bổ các mức độ tài nguyên tính toán khác nhau cho việc xử lý các mẫu đầu vào khác nhau, hai loại phương pháp chủ đạo đã xuất hiện trong lĩnh vực này: thoát sớm (Liu et al., 2020; Xin et al., 2021; Zhou et al., 2020) và bỏ token (Goyal et al., 2020; Guan et al., 2022; Kim and Cho, 2021). Salehi et al. (2023) cũng nghiên cứu để hướng các mẫu khác nhau đến các mạng con với các chiều rộng khác nhau. Khung định tuyến được đề xuất của chúng tôi cũng áp dụng suy luận thích ứng mẫu. Tuy nhiên, nó phân biệt bản thân bằng cách tận dụng không chỉ một mô hình đơn lẻ mà là sự kết hợp của các mô hình.
6.2 Chuyển đổi Mô hình
Đã có một số lượng ngày càng tăng các nghiên cứu tập trung vào khái niệm chuyển đổi mô hình, trong đó các ví dụ đầu vào được định tuyến một cách thông minh giữa các mô hình nhỏ và lớn dựa trên mức độ phức tạp cá nhân của chúng. Ví dụ, Madaan et al. (2023) đề xuất một phương pháp tận dụng một meta-verifier bên ngoài để xác định tính đúng đắn của các dự đoán được thực hiện bởi SLM và để xác định liệu một ví dụ có đáng để định tuyến đến LLM. Ngược lại, phương pháp của chúng tôi không cần thiết sử dụng các verifiers bổ sung. Một tập hợp các phương pháp liên quan khác, được minh họa bởi công việc của Šakota et al. (2023); Kag et al. (2022), liên quan đến việc huấn luyện các bộ phân loại nhị phân để phân loại các ví dụ là phù hợp cho xử lý SLM hoặc LLM. Phương pháp này yêu cầu router được huấn luyện trên dữ liệu được gán nhãn nơi các mô hình ngôn ngữ đã thực hiện dự đoán. Ngược lại, phương pháp của chúng tôi thể hiện khả năng tận dụng retriever có sẵn, tăng cường tính linh hoạt của nó.
6.3 Theo dõi trạng thái đối thoại Ít mẫu
Để giảm nhu cầu dữ liệu được gán nhãn trong DST, nhiều phương pháp được đề xuất cho DST ít mẫu (Li et al., 2021; Lin et al., 2021; Shin et al., 2022; Hu et al., 2022). Mô hình DST ít mẫu tiên tiến nhất là (King and Flanigan, 2023b), trong đó các tác giả tái cấu trúc DST như một nhiệm vụ lập trình Python và tận dụng Codex (Chen et al., 2021) làm LLM xương sống, không còn có thể truy cập được.
Ngoài ra, phương pháp của họ liên quan đến nhiều lần giải mã cho một lượt đơn và dựa vào điểm xác suất của tokens, có thể không luôn có sẵn một cách dễ dàng.
7 Kết luận
Chúng tôi giới thiệu OrchestraLLM, một khung định tuyến tích hợp liền mạch một SLM và một LLM, được điều phối bởi một router dựa trên truy xuất. Trong quá trình suy luận, một router động hướng dẫn các instance đến LM dựa trên khoảng cách embedding ngữ nghĩa của chúng với các mẫu LM được truy xuất, tận dụng chuyên môn của cả SLM và LLM. Đánh giá của chúng tôi trên DST chứng minh rằng OrchestraLLM vượt trội hơn các hệ thống dựa trên LLM đồng thời cũng đạt được tiết kiệm chi phí tính toán hơn 50%. Nghiên cứu này đại diện cho một bước quan trọng hướng tới sự hợp tác hiệu quả của các mô hình ngôn ngữ, đặc biệt trong một hệ thống tương tác con người-máy tính đa lượt như đối thoại hướng nhiệm vụ.
8 Hạn chế
Nghiên cứu của chúng tôi chứng minh lợi ích của việc kết hợp SLM và LLM để cải thiện hiệu suất nhiệm vụ đồng thời quản lý chi phí tính toán, đặc biệt trong bối cảnh các nhiệm vụ theo dõi trạng thái đối thoại. Tuy nhiên, điều quan trọng cần thừa nhận rằng khả năng áp dụng của phương pháp chúng tôi có thể không mở rộng một cách liền mạch đến tất cả các loại nhiệm vụ trong lĩnh vực NLP rộng lớn hơn.
Ngoài ra, trong khung hiện tại của chúng tôi, chúng tôi tập trung vào việc tận dụng một SLM và một LLM. Tuy nhiên, các ứng dụng thế giới thực thường liên quan đến một loạt rộng các nhiệm vụ đa dạng, mỗi nhiệm vụ có thể yêu cầu LMs với chuyên môn khác nhau. Như một hướng nghiên cứu tương lai, chúng tôi dự định khám phá việc điều phối nhiều LMs đồng thời.
Tài liệu tham khảo
Namo Bang, Jeehyun Lee, and Myoung-Wan Koo.
2023. Task-optimized adapters for an end-to-end
task-oriented dialogue system. In Findings of the As-
sociation for Computational Linguistics: ACL 2023,
pages 7355–7369, Toronto, Canada. Association for
Computational Linguistics.
Jeff Barr. 2019. Amazon ec2 update. Blog Post. Ac-
cessed: 2021-06-01.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang
Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-
madan, and Milica Gasic. 2018. Multiwoz-a large-
scale multi-domain wizard-of-oz dataset for task-
oriented dialogue modelling. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 5016–5026.
Lingjiao Chen, Matei Zaharia, and James Zou. 2023.
Frugalgpt: How to use large language models while
reducing cost and improving performance. arXiv
preprint arXiv:2305.05176.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-
tiplication for transformers at scale. arXiv preprint
arXiv:2208.07339.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. arXiv preprint arXiv:2305.14314.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association
for Computational Linguistics.
Suvodip Dey, Ramamohan Kummara, and Maunendra
Desarkar. 2022. Towards fair evaluation of dialogue
state tracking by flexible incorporation of turn-level
performances. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 318–324.
Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,
Sanchit Agarwal, Shuyang Gao, Adarsh Kumar,
Anuj Goyal, Peter Ku, and Dilek Hakkani-Tur. 2020.
Multiwoz 2.1: A consolidated multi-domain dia-
logue dataset with state corrections and state tracking
baselines. In Proceedings of the 12th Language Re-
sources and Evaluation Conference, pages 422–428.
Angela Fan, Edouard Grave, and Armand Joulin. 2019.
Reducing transformer depth on demand with struc-
tured dropout. In International Conference on Learn-
ing Representations.
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh
Raje, Venkatesan Chakaravarthy, Yogish Sabharwal,
and Ashish Verma. 2020. Power-bert: Accelerat-
ing bert inference via progressive word-vector elim-
ination. In International Conference on Machine
Learning, pages 3690–3699. PMLR.
Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin,
and Minyi Guo. 2022. Transkimmer: Transformer
learns to layer-wise skim. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 7275–
7286.
Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, and
Kee-Eung Kim. 2020. End-to-end neural pipeline
for goal-oriented dialogue systems using gpt-2. In
Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, pages 583–
592.
Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the 15th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), pages 263–272, Philadelphia,
PA, U.S.A. Association for Computational Linguis-
tics.
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,
Semih Yavuz, and Richard Socher. 2020. A sim-
ple language model for task-oriented dialogue. Ad-
vances in Neural Information Processing Systems,
33:20179–20191.
Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,
Noah A. Smith, and Mari Ostendorf. 2022. In-
context learning for few-shot dialogue state tracking.
In Findings of the Association for Computational
Linguistics: EMNLP 2022, pages 2627–2643, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Vojtěch Hudecek and Ondrej Dušek. 2023. Are large
language models all you need for task-oriented di-
alogue? In Proceedings of the 24th Meeting of the
Special Interest Group on Discourse and Dialogue,
pages 216–228.
Wiradee Imrattanatrai and Ken Fukuda. 2023. End-to-
end task-oriented dialogue systems based on schema.
In Findings of the Association for Computational Lin-
guistics: ACL 2023, pages 10148–10161, Toronto,
Canada. Association for Computational Linguistics.
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung
Kim, Lajanugen Logeswaran, Moontae Lee, Kyung-
jae Lee, and Minjoon Seo. 2023. Exploring the ben-
efits of training expert language models over instruc-
tion tuning. arXiv preprint arXiv:2302.03202.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.
Tinybert: Distilling bert for natural language under-
standing. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020, pages 4163–
4174.
Anil Kag, Igor Fedorov, Aditya Gangrade, Paul What-
mough, and Venkatesh Saligrama. 2022. Efficient
edge inference by selective query. In The Eleventh
International Conference on Learning Representa-
tions.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 6769–6781,
Online. Association for Computational Linguistics.
Gyuwan Kim and Kyunghyun Cho. 2021. Length-
adaptive transformer: Train once with length drop,
use anytime with search. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 6501–6511, Online. Associa-
tion for Computational Linguistics.
Brendan King and Jeffrey Flanigan. 2023a. Diverse
retrieval-augmented in-context learning for dialogue
state tracking. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 5570–
5585.
Brendan King and Jeffrey Flanigan. 2023b. Diverse
retrieval-augmented in-context learning for dialogue
state tracking. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 5570–
5585, Toronto, Canada. Association for Computa-
tional Linguistics.
François Lagunas, Ella Charlaix, Victor Sanh, and
Alexander M Rush. 2021. Block pruning for faster
transformers. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 10619–10629.
Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021.
Dialogue state tracking with a language model using
schema-driven prompting. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing, pages 4937–4949.
George Leopold. 2019. Aws to offer nvidia's t4 gpus
for ai inferencing. Online. Accessed: 2021-06-01.
Shuyang Li, Jin Cao, Mukund Sridhar, Henghui Zhu,
Shang-Wen Li, Wael Hamza, and Julian McAuley.
2021. Zero-shot generalization in dialog state track-
ing through generative question answering. In Pro-
ceedings of the 16th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Main Volume, pages 1063–1074, Online.
Association for Computational Linguistics.
Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A
Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,
Andrea Madotto, Eunjoon Cho, and Rajen Subba.
2021. Leveraging slot descriptions for zero-shot
cross-domain dialogue state tracking. In Proceed-
ings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
5640–5648.
Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,
Haotang Deng, and Qi Ju. 2020. FastBERT: a self-
distilling BERT with adaptive inference time. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 6035–
6044, Online. Association for Computational Lin-
guistics.
Aman Madaan, Pranjal Aggarwal, Ankit Anand, Sriv-
idya Pranavi Potharaju, Swaroop Mishra, Pei Zhou,
Aditya Gupta, Dheeraj Rajagopal, Karthik Kappa-
ganthu, Yiming Yang, et al. 2023. Automix: Auto-
matically mixing language models. arXiv preprint
arXiv:2310.12963.
Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? Advances
in neural information processing systems, 32.
Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-
deh, Lars Liden, and Jianfeng Gao. 2020. Soloist:
Few-shot task-oriented dialog with a single pre-
trained auto-regressive model.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. Language mod-
els are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research,
21(1):5485–5551.
Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,
Raghav Gupta, and Pranav Khaitan. 2020. Towards
scalable multi-domain conversational agents: The
schema-guided dialogue dataset. In Proceedings of
the AAAI conference on artificial intelligence, vol-
ume 34, pages 8689–8696.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics.
Marija Šakota, Maxime Peyrard, and Robert West.
2023. Fly-swat or cannon? cost-effective language
model choice via meta-modeling. arXiv preprint
arXiv:2308.06077.
Mohammadreza Salehi, Sachin Mehta, Aditya Kusu-
pati, Ali Farhadi, and Hannaneh Hajishirzi. 2023.
Sharcs: Efficient transformers through routing
with dynamic width sub-networks. arXiv preprint
arXiv:2310.12126.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108.
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, and Kurt
Keutzer. 2020. Q-bert: Hessian based ultra low
precision quantization of bert. In Proceedings of
the AAAI Conference on Artificial Intelligence, vol-
ume 34, pages 8815–8821.
Jamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea
Madotto, and Juneyoung Park. 2022. Dialogue sum-
maries as dialogue states (DS2), template-guided
summarization for few-shot dialogue state tracking.
In Findings of the Association for Computational
Linguistics: ACL 2022, pages 3824–3846, Dublin,
Ireland. Association for Computational Linguistics.
Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta,
Deng Cai, Yi-An Lai, and Yi Zhang. 2022. Multi-
task pre-training for plug-and-play task-oriented di-
alogue system. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 4661–4676.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2019. Well-read students learn better:
On the importance of pre-training compact models.
arXiv preprint arXiv:1908.08962.
Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan,
Zheng Lin, Shi Wang, Dacheng Tao, and Li Guo.
2023. Divide, conquer, and combine: Mixture of
semantic-independent experts for zero-shot dialogue
state tracking. arXiv preprint arXiv:2306.00434.
Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020.
Structured pruning of large language models. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 6151–6162.
Chien-Sheng Wu, Steven CH Hoi, and Caiming Xiong.
2020. Improving limited labeled dialogue state track-
ing with self-supervision. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020,
pages 4462–4472.
Mengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.
Structured pruning learns compact and accurate mod-
els. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1513–1528.
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,
Torsten Scholak, Michihiro Yasunaga, Chien-Sheng
Wu, Ming Zhong, Pengcheng Yin, Sida I Wang,
et al. 2022. Unifiedskg: Unifying and multi-tasking
structured knowledge grounding with text-to-text lan-
guage models. arXiv preprint arXiv:2201.05966.
Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.
2021. BERxiT: Early exiting for BERT with better
fine-tuning and extension to regression. In Proceed-
ings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics:
Main Volume, pages 91–104, Online. Association for
Computational Linguistics.
Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz.
2021. Multiwoz 2.4: A multi-domain task-oriented
dialogue dataset with essential annotation corrections
to improve state tracking evaluation. arXiv preprint
arXiv:2104.00773.
Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz.
2022. MultiWOZ 2.4: A multi-domain task-oriented
dialogue dataset with essential annotation corrections
to improve state tracking evaluation. In Proceedings
of the 23rd Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 351–360,
Edinburgh, UK. Association for Computational Lin-
guistics.
Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian
McAuley, Ke Xu, and Furu Wei. 2020. Bert loses
patience: Fast and robust inference with early exit.
Advances in Neural Information Processing Systems,
33:18330–18341.
A Công việc Liên quan
A.1 Suy luận Thích ứng Mẫu
Để cho phép các mức độ tài nguyên tính toán khác nhau được phân bổ cho việc xử lý các mẫu đầu vào khác nhau, hai loại phương pháp chủ đạo đã xuất hiện trong lĩnh vực này: thoát sớm và bỏ token. Các chiến lược thoát sớm thường kết hợp các bộ phân loại bổ sung tại các lớp trung gian trong một mô hình, xác định liệu một ví dụ đầu vào nhất định có nên chấm dứt xử lý của nó sớm và không lan truyền đến các lớp tiếp theo (Liu et al., 2020; Xin et al., 2021; Zhou et al., 2020). Mặt khác, các kỹ thuật bỏ token nhằm giảm động chiều dài của chuỗi đầu vào, đạt được bằng cách loại bỏ có chọn lọc một số token nhất định khỏi chuỗi đầu vào, sau đó không được chuyển đến các lớp tiếp theo trong mô hình (Goyal et al., 2020; Guan et al., 2022; Kim and Cho, 2021). Salehi et al. (2023) cũng nghiên cứu để hướng các mẫu khác nhau đến các mạng con với các chiều rộng khác nhau. Khung định tuyến được đề xuất của chúng tôi cũng áp dụng suy luận thích ứng mẫu. Tuy nhiên, nó phân biệt bản thân bằng cách tận dụng không chỉ một mô hình đơn lẻ mà là sự kết hợp của các mô hình.
A.2 Nén Mô hình
Nén mô hình chủ yếu rơi vào ba mô hình chính: cắt tỉa, chưng cất, và lượng tử hóa.
Các chiến lược cắt tỉa chủ yếu được thiết kế để giảm chi phí tính toán bằng cách chọn và giữ lại một mạng con trong một mô hình lớn hơn (Fan et al., 2019; Michel et al., 2019; Wang et al., 2020; Lagunas et al., 2021; Xia et al., 2022). Ngược lại, các kỹ thuật chưng cất bao gồm việc huấn luyện một mô hình học sinh nhỏ gọn, với mục tiêu truyền đạt kiến thức và hiệu suất của một mô hình giáo viên lớn hơn (Sanh et al., 2019; Turc et al., 2019; Jiao et al., 2020). Cuối cùng, các phương pháp lượng tử hóa nhằm giảm thiểu yêu cầu bộ nhớ bằng cách biểu diễn các tham số mô hình với ít bit hơn, do đó đánh đổi một mức độ chính xác để tăng cường hiệu quả (Shen et al., 2020; Dettmers et al., 2022, 2023). Lưu ý rằng các phương pháp nói trên được đặc trưng là "tĩnh" về bản chất, vì chúng chủ yếu tập trung vào việc tối ưu hóa các kiến trúc mô hình cố định cho mỗi điểm dữ liệu. Ngược lại, khung định tuyến được giới thiệu trong công việc này áp dụng một góc nhìn động.
